# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States.](http://arxiv.org/abs/2303.18242) | 引入了一种名为 $\infty$-Diff 的生成式扩散模型，可以处理无限分辨率的数据，不需要使用超网络进行潜在向量压缩或依赖于离散的组件，能够显著提高样本质量，并能够在保留细节的同时有效地扩展到比训练数据更高的分辨率。 |
| [^2] | [Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?.](http://arxiv.org/abs/2303.18240) | 该研究研究了使用预训练视觉表征来实现身体智能的最新进展。他们展示了最大、最全面的经验研究，发现没有一种表征是普遍优越的，并且数据集的大小和多样性并不能普遍改善性能。 |
| [^3] | [Physics and Chemistry from Parsimonious Representations: Image Analysis via Invariant Variational Autoencoders.](http://arxiv.org/abs/2303.18236) | 这篇论文介绍了如何使用不变分变自编码器（VAEs）解决在图像分析中的物理和化学问题，通过分离已知和未知的变量因素来发现物理和化学现象。 |
| [^4] | [Simple Sorting Criteria Help Find the Causal Order in Additive Noise Models.](http://arxiv.org/abs/2303.18211) | 文章探讨了加性噪声模型中找到因果顺序的方法。作者发现除了方差排序外，变量的决定系数$R^2$排序也可用于匹配已有方法的表现，且不受数据缩放的影响。 |
| [^5] | [SimTS: Rethinking Contrastive Representation Learning for Time Series Forecasting.](http://arxiv.org/abs/2303.18205) | 本文提出了一种不依赖于负对或特定时间序列特征假设的简单表示学习方法SimTS，通过学习在潜在空间中从过去预测未来来改进时间序列预测，并在多个数据集上表现出具有竞争力的性能。 |
| [^6] | [TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features.](http://arxiv.org/abs/2303.18201) | 本文提出了一种新的方法TPMCF，利用多源特征进行QoS预测。该方法利用带有注意力机制的编码器-解码器架构，并使用协作特征捕捉用户和服务之间的关系，有效地处理数据稀疏和异常值。 |
| [^7] | [PADME-SoSci: A Platform for Analytics and Distributed Machine Learning for the Social Sciences.](http://arxiv.org/abs/2303.18200) | PADME-SoSci是一个联邦学习平台，通过在原数据位置进行学习来实现数据所有权保护和跨位置数据分析。 |
| [^8] | [GVP: Generative Volumetric Primitives.](http://arxiv.org/abs/2303.18193) | 本文提出了Generative Volumetric Primitives (GVP)生成体素元素，是第一个能够实时采样和渲染512分辨率图像的纯3D生成模型，通过2D卷积网络高效地生成多个体积元素和它们的空间信息，能够捕捉三维空间中的稀疏性和对应性，并通过知识蒸馏技术实现高自由度的训练。实验表明，GVP优于以前的方法，从图像质量和多视图一致性两方面进行了验证。 |
| [^9] | [Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency.](http://arxiv.org/abs/2303.18191) | 本文提出了一种新的测试时间触发样本检测方法 TeCo，该方法只需要受害模型的硬标签输出，通过评估测试时间鲁棒性一致性来检测后门，不需要其他额外的信息，提高了实用性。 |
| [^10] | [Learning Spiking Neural Systems with the Event-Driven Forward-Forward Process.](http://arxiv.org/abs/2303.18187) | 该论文提出了一种基于事件驱动的前向-前向和预测式前向-前向学习过程的通用化方案，用于递归电路计算每个神经元的膜电位。与依赖反馈突触调整神经电活动的尖峰神经编码不同，该模型纯在线并且时间向前，是学习带有时间尖峰信号的感觉数据模式分布表示的有前途的一种途径。 |
| [^11] | [A Closer Look at Parameter-Efficient Tuning in Diffusion Models.](http://arxiv.org/abs/2303.18181) | 本文研究了大规模扩散模型的参数效率微调，通过插入小型可学习模块来实现。研究表明，适配器的输入位置是影响下游任务性能的关键因素，而将输入位置放在交叉注意力块之后可获得最佳性能。 |
| [^12] | [Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties.](http://arxiv.org/abs/2303.18178) | 本文提出了一种名为Party-wise Dropout的方法，该方法可以提高竖直联邦学习模型对被动方意外退出的鲁棒性，并提出了一种名为DIMIP的防御方法，用于保护活动方在部署阶段的知识产权。 |
| [^13] | [How Efficient Are Today's Continual Learning Algorithms?.](http://arxiv.org/abs/2303.18171) | 这篇论文研究了增量班级学习的最新方法，并指出许多方法在计算、内存和存储方面非常低效。为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。 |
| [^14] | [Constrained Optimization of Rank-One Functions with Indicator Variables.](http://arxiv.org/abs/2303.18158) | 本文提出了一种基于透视重构技术的紧凑扩展公式，用于解决涉及指标变量限制下决策变量支持集合的秩一凸函数的最小化问题。 |
| [^15] | [MAGNNETO: A Graph Neural Network-based Multi-Agent system for Traffic Engineering.](http://arxiv.org/abs/2303.18157) | 提出了一个基于分布式ML框架MAGNNETO，运用多智能体强化学习和图神经网络来解决ISP网络的TE优化问题，在OSPF中的链路权重调整方面表现良好。 |
| [^16] | [BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection.](http://arxiv.org/abs/2303.18138) | BERT4ETH是一个用于以太坊欺诈检测的预训练Transformer编码器，它通过三种实用有效策略来解决针对高度重复、偏斜分布和异构以太坊交易这些挑战，并且比现有技术更为优秀。 |
| [^17] | [Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids.](http://arxiv.org/abs/2303.18136) | 该论文提出了针对智能电网故障预测系统的机器学习对抗攻击的研究，证明智能电网中使用的深度神经网络方法容易受到对抗性攻击，并突出了目前在智能电网中的机器学习算法存在对各种对抗性攻击的弱点。 |
| [^18] | [A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks.](http://arxiv.org/abs/2303.18132) | 本文提出了一种基于脱同步的对抗措施，使激活函数的时序分析变得更加困难，以防止模型提取和神经网络侧信道分析攻击。实验结果表明，在4096个神经元的情况下，我们的方案仅产生不到1%的开销。 |
| [^19] | [AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking.](http://arxiv.org/abs/2303.18131) | 本文提出了一种新的方法AdvCheck，通过计算本地梯度检测对抗性样本，相较于其他最先进的检测方法具有更高的效率和更好的表现。 |
| [^20] | [A two-head loss function for deep Average-K classification.](http://arxiv.org/abs/2303.18118) | 本文提出了一种双头损失函数用于平均K分类，其中第二个头被用于多标签分类，模型采用了软阈值训练以保证平均返回K个类，实验在两个数据集上证明其优于softmax基线。 |
| [^21] | [The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR.](http://arxiv.org/abs/2303.18110) | 爱丁堡国际英语口音语料库（EdAcc）发布，包括英语的广泛多样性和每个说话者的语言背景概况。在 EdAcc 上训练的最佳模型明显优于所有现有模型，这突显了当前英语ASR模型的缺点。这一数据集的公开共享将有助于民主化英语语音识别研究和开发。 |
| [^22] | [Evaluation Challenges for Geospatial ML.](http://arxiv.org/abs/2303.18087) | 本文论述了地理空间机器学习评估中的独特挑战，提出了改进地理空间模型性能评估的具体方法。 |
| [^23] | [Solving morphological analogies: from retrieval to generation.](http://arxiv.org/abs/2303.18062) | 该论文提出了一个基于深度学习和条件变分自编码器的框架来解决基于类比的推理中的类比检测和解决两个任务，该框架可以生成之前不存在于数据集中的类比。 |
| [^24] | [NOSTROMO: Lessons learned, conclusions and way forward.](http://arxiv.org/abs/2303.18060) | 本文介绍了元建模对于空中交通管理研究的价值，并强调了其在实现欧洲ATM总体计划中的关键绩效指标方面所起的作用。 |
| [^25] | [Inferring networks from time series: a neural approach.](http://arxiv.org/abs/2303.18059) | 本论文提出了一种基于神经网络的快速计算方法，可以从时间序列数据中推断大型网络的相邻矩阵，并对不确定性进行量化，解决了网络推断问题的不足。 |
| [^26] | [Deep neural operator for learning transient response of interpenetrating phase composites subject to dynamic loading.](http://arxiv.org/abs/2303.18055) | 本文提出了一种使用深度神经算子代理物理有限元分析模型，学习交错相复合材料在动态加载下的瞬态响应，加速其力学性能的物理预测。 |
| [^27] | [Differentially Private Stochastic Convex Optimization in (Non)-Euclidean Space Revisited.](http://arxiv.org/abs/2303.18047) | 本文重新探讨了在欧几里得空间和一般的$\ell_p^d$空间中进行差分隐私随机凸优化（DP-SCO）的问题。针对凸损失函数和强凸损失函数，提出了方法，其输出能够实现(预期)过量种群风险，这只取决于约束集合的高斯宽度，对强凸函数，界限是最优的。同时，提出了针对重尾数据进行DP-SCO的算法，并在$1<p<2$和$2≤p≤∞$的两种情况下，提供了第一个理论结果。 |
| [^28] | [Scardina: Scalable Join Cardinality Estimation by Multiple Density Estimators.](http://arxiv.org/abs/2303.18042) | 本篇论文提出了一种基于多个密度估计器的可扩展连接基数估计方法，可以精确估计具有强相关的大型和复杂模式的数据的基数，有望在查询优化器中得到应用。 |
| [^29] | [Traffic Sign Recognition Dataset and Data Augmentation.](http://arxiv.org/abs/2303.18037) | 本论文介绍了解决交通标志识别中数据集不足的问题的方法——TSR数据集增强，以及在TT100K数据集上实现的成果，可以显著提高特定标志类别的性能并实现最先进的性能。 |
| [^30] | [Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization.](http://arxiv.org/abs/2303.18031) | 该论文评估了基于领域泛化的方法在开放领域泛化中的表现，证明了CORAL和MMD等简单DG方法在某些情况下的竞争力，提出了这些方法的简单扩展。 |
| [^31] | [Rapid prediction of lab-grown tissue properties using deep learning.](http://arxiv.org/abs/2303.18017) | 该文介绍了使用深度学习技术快速预测实验室培养的细胞/hydrogels的机械特性的理论证明，减少了物理实验成本。 |
| [^32] | [Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review.](http://arxiv.org/abs/2303.18005) | 通过对36篇文章的综述，该研究发现人工智能模型在卵巢癌的诊断和预后中显示出有希望的结果，但现有研究受到小样本量，潜在偏见和缺乏外部验证的限制。 |
| [^33] | [Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation.](http://arxiv.org/abs/2303.17995) | 该研究提出了一种新的熵估计方法NNetEn，用于有效地分离时间序列系统的混沌动态，并在分离混沌时间序列方面证明了其高效率。 |
| [^34] | [A fast Multiplicative Updates algorithm for Non-negative Matrix Factorization.](http://arxiv.org/abs/2303.17992) | 提出了一种快速的非负矩阵分解乘法更新算法，通过改进交替主体化最小化算法，实现了较快的求解和类似或更好的逼近精度结果。 |
| [^35] | [Federated Learning for Metaverse: A Survey.](http://arxiv.org/abs/2303.17987) | 本文综述了早期FL4M的研究进展，并探讨了FL对于保护元宇宙参与者的数据隐私和降低服务器计算和存储需求的重要性。 |
| [^36] | [Promoting Non-Cooperation Through Ordering.](http://arxiv.org/abs/2303.17971) | 本文研究了如何通过排序方式，在处罚的个人中激励非合作行为，对于中央管理部门能够显著增加总付款。 |
| [^37] | [HD-GCN:A Hybrid Diffusion Graph Convolutional Network.](http://arxiv.org/abs/2303.17966) | 介绍了一种名为HD-GCN的新框架，以解决邻接矩阵造成的信息扩散限制。利用扩散映射促进相邻节点之间的信息扩散，再利用图卷积进一步传播信息。通过使用扩散映射获得的扩散距离进行正则化和约束。 |
| [^38] | [Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States.](http://arxiv.org/abs/2303.17963) | 本文提出了一种面向未知具有潜在状态系统的学习优化控制方法，并给出了概率性能保证，同时提出了一种验证任意控制律性能的方法。 |
| [^39] | [Ensemble Methods for Multi-Organ Segmentation in CT Series.](http://arxiv.org/abs/2303.17956) | 本论文提出了利用单器官模型进行集成，解决数据稀缺限制下放射治疗器官风险分割的问题，并获得了令人兴奋的结果。 |
| [^40] | [FP8 versus INT8 for efficient deep learning inference.](http://arxiv.org/abs/2303.17951) | 本文比较了FP8和INT8在设备高效推理中的性能，展示了量化和量化感知训练的成果，为选择正确的数字格式提供了参考。 |
| [^41] | [Unsupervised Anomaly Detection and Localization of Machine Audio: A GAN-based Approach.](http://arxiv.org/abs/2303.17949) | 本文提出了一种基于生成对抗网络的无监督机器音频异常检测和定位方法AEGAN-AD，并在DCASE 2022 Challenge任务2数据集上获得了最先进的结果。 |
| [^42] | [Benchmarking FedAvg and FedCurv for Image Classification Tasks.](http://arxiv.org/abs/2303.17942) | 本研究分析了在图像分类任务中使用的FedAvg和FedCurv联邦学习算法。联邦学习可以解决隐私问题，但异质系统和非IID数据仍然是挑战。 |
| [^43] | [Comparing Adversarial and Supervised Learning for Organs at Risk Segmentation in CT images.](http://arxiv.org/abs/2303.17941) | 本文比较了使用GAN和监督学习方法进行CT扫描中器官风险（OAR）分割的表现。研究结果表明，所提出的基于GAN的方法与基于CNN的方法相似或优于其对应的CNN模型，尤其是在分割更具挑战性的目标时。 |
| [^44] | [Per-Example Gradient Regularization Improves Learning Signals from Noisy Data.](http://arxiv.org/abs/2303.17940) | 本文研究了每个示例的梯度正则化 (PEGR) 技术，证明其可以有效地学习信号并抑制噪音，从而提高测试误差和抗噪声扰动能力。 |
| [^45] | [Conflict-Averse Gradient Optimization of Ensembles for Effective Offline Model-Based Optimization.](http://arxiv.org/abs/2303.17934) | 本文提出了一种冲突回避的集成渐变优化方法，使用集成保证不受分布变化的影响，并使用辅助目标识别集成中无效梯度的区域，并自适应调整这些梯度的权重，以避免探索不可行的设计区域。实验结果表明，该方法在离线基于模型的优化中显着提高了性能。 |
| [^46] | [Learning-based Observer Evaluated on the Kinematic Bicycle Model.](http://arxiv.org/abs/2303.17933) | 本文提出了一种基于学习的观测器方法，能够在处理非线性、不确定、模型误差等问题方面胜过经典观测器。该方法在动力学自行车模型上已得到评价和应用。 |
| [^47] | [Beyond Multilayer Perceptrons: Investigating Complex Topologies in Neural Networks.](http://arxiv.org/abs/2303.17925) | 本研究探索了神经网络的复杂拓扑对其近似能力的影响，发现在高难度情况下，复杂拓扑相比于传统的多层感知器表现更优异，但代价是前向传播计算时间的增加和图形损伤的鲁棒性的降低。 |
| [^48] | [Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking.](http://arxiv.org/abs/2303.17907) | 本文提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向，实现面向全沉浸多用户虚拟现实技术的高效通信。 |
| [^49] | [Accelerating Wireless Federated Learning via Nesterov's Momentum and Distributed Principle Component Analysis.](http://arxiv.org/abs/2303.17885) | 本文提出了利用一次性分布式主成分分析（PCA）和 Nesterov 动量来加速无线联邦学习的算法，缓解了从工人到服务器的上传链路成为通信瓶颈的问题。PCA-AWFL 算法经过理论证明比 PCA-WFL 算法更快地收敛。数值实验验证了所提出算法的有效性。 |
| [^50] | [Fused Depthwise Tiling for Memory Optimization in TinyML Deep Neural Network Inference.](http://arxiv.org/abs/2303.17878) | 本文提出了一种新的融合深度分块瓦片（FDT）方法，可以在不引入任何运行时开销的情况下降低DNN推理的内存使用量，进一步提高了在嵌入式设备上进行的TinyML模型的内存利用率。 |
| [^51] | [CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer.](http://arxiv.org/abs/2303.17867) | 本文提出了一种全新的CAP-VSTNet框架，它保留了内容相关性，解决了多功能风格转换中的问题，并在定性和定量方面达到了更好的结果。 |
| [^52] | [Maximum Covariance Unfolding Regression: A Novel Covariate-based Manifold Learning Approach for Point Cloud Data.](http://arxiv.org/abs/2303.17852) | 本文提出了一种针对非结构化点云数据的新的协变量流形学习方法，用于建模和流程优化。本方法能够学习与解释性协变量相关性最高的点云的低维流形。 |
| [^53] | [A Benchmark Generative Probabilistic Model for Weak Supervised Learning.](http://arxiv.org/abs/2303.17841) | 本文提出一种基准生成性概率模型，在启发式标注的原始数据集上训练，生成伪标签作为一种准确、快速、经济的弱监督学习方法，在图像分类和自然语言处理中达到了最先进的表现。 |
| [^54] | [Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations.](http://arxiv.org/abs/2303.17839) | 本文提出了一种从教学视频及其解说中学习过程感知的视频表示方法，联合学习视频表示和深度概率模型可以增强过程推理的新功能，同时对个体步骤的识别也能得到加强。 |
| [^55] | [Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers.](http://arxiv.org/abs/2303.17836) | 提出了一种新的无特定输入显著性映射视角，它计算了模型对其输出所归属的高级特征，这种方法能够独立于输入进行模型解释，且鲁棒性较好。 |
| [^56] | [Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets.](http://arxiv.org/abs/2303.17824) | 本文重点研究使用隐式数值初值问题求解器模板的ODE-nets。使用展开的隐式方案对ODE-nets进行训练可以返回一个接近于反转修正微分方程(IMDE)的近似值，并且我们可以使用自适应算法加速训练并保持精度。 |
| [^57] | [An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response.](http://arxiv.org/abs/2303.17823) | 本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。 |
| [^58] | [An Efficient Off-Policy Reinforcement Learning Algorithm for the Continuous-Time LQR Problem.](http://arxiv.org/abs/2303.17819) | 本文提出了一种高效的离策略强化学习算法，使用特定的持续激励输入作为探索信号来解决连续时间LQR问题，并证明了算法在每次迭代中可收敛到最优控制输入，同时提高了求解的效率。 |
| [^59] | [Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare.](http://arxiv.org/abs/2303.17807) | 本研究评估了大型语言模型在应用传统韩医的潜力。其中，GPT-4在应用韩国国家中医医生执照考试中取得了57.29%的准确率，潜在应用价值高。 |
| [^60] | [On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks.](http://arxiv.org/abs/2303.17805) | 本文研究了具有不同尺度的非零权重的无限宽2层ReLU神经网络的正则化路径，展示该问题具有一个无限维度的凸对应，随着初始化的尺度在0到+∞范围内变化，关联路径在所谓的内核和丰富的区域之间连续插值。 |
| [^61] | [Time-series Anomaly Detection based on Difference Subspace between Signal Subspaces.](http://arxiv.org/abs/2303.17802) | 本文提出了一种新的基于信号子空间差异子空间的时间序列异常检测方法，通过捕获两个子空间的完整结构差异来提高性能，在公共时间序列数据集上证明了其有效性。 |
| [^62] | [A Slow-Shifting Concerned Machine Learning Method for Short-term Traffic Flow Forecasting.](http://arxiv.org/abs/2303.17782) | 该论文提出了一种关注缓慢变化的机器学习方法来解决交通流量预测中的时间漂移问题，该方法包括经验模态分解和长短期记忆网络等步骤，可缓解交通流量数据的非平稳性，提高交通流量预测的准确性。 |
| [^63] | [Learning Internal Representations of 3D Transformations from 2D Projected Inputs.](http://arxiv.org/abs/2303.17776) | 该论文提出了一种基于生成流形模型的计算模型，通过对移动的二维投影点进行深度推断，学习了三维旋转变换，为了理解生物视觉系统如何内部表示三维变换提供了思路。 |
| [^64] | [Domain Knowledge integrated for Blast Furnace Classifier Design.](http://arxiv.org/abs/2303.17769) | 本文设计了一种融合领域知识的分类模型框架，生成适用于工业应用的分类器，有效解决了安全和能源等不同学习目标下爆炉复杂系统的问题。 |
| [^65] | [Scalable Bayesian Meta-Learning through Generalized Implicit Gradients.](http://arxiv.org/abs/2303.17768) | 本研究提出了一种新颖的隐式贝叶斯元学习方法(iBaML)，通过将隐式梯度的优势应用到概率贝叶斯元学习中，显著缓解了可扩展性的瓶颈，并量化了相关的不确定性，具备良好的复杂性控制。 |
| [^66] | [Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness.](http://arxiv.org/abs/2303.17765) | 本文提出了两种算法，适应相似性结构并对异常值任务具有稳健性，适用于表示多任务学习和迁移学习设置。 |
| [^67] | [Towards Adversarially Robust Continual Learning.](http://arxiv.org/abs/2303.17764) | 该论文是关于在持续学习中提高对抗鲁棒性的研究，首次提出一种新方法“任务感知边界增强（TABA）”，并在CIFAR-10和CIFAR-100上进行了充分的实验验证其有效性。 |
| [^68] | [CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society.](http://arxiv.org/abs/2303.17760) | 本文介绍了一个名为角色扮演的新型交互式代理框架，用于实现语言模型之间的自主合作，并展示了其在生成对话数据方面的有效性。 |
| [^69] | [A Note On Nonlinear Regression Under L2 Loss.](http://arxiv.org/abs/2303.17745) | 研究者们发现对于传统的最小二乘问题存在一个可以使用凸非线性回归模型解决的方法，使得设计更复杂的系统和更易于训练的模型成为可能。 |
| [^70] | [Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network.](http://arxiv.org/abs/2303.17732) | 通过优化输入增益，可以显著提高前馈神经网络的性能，特别是在使用反向传播和隐藏权重优化等算法时。 |
| [^71] | [$\beta^{4}$-IRT: A New $\beta^{3}$-IRT with Enhanced Discrimination Estimation.](http://arxiv.org/abs/2303.17731) | 本文提出了一种新的名称为β4-IRT的项目反应理论模型，通过使用梯度下降方法估计模型参数来增强判别估计，解决了β3-IRT的对称性问题。 |
| [^72] | [BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware.](http://arxiv.org/abs/2303.17727) | BOLT是一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库，它提供了一个灵活的高级API，使用户可以构建模型并抽象掉稀疏网络训练的算法细节。 |
| [^73] | [Generating Adversarial Samples in Mini-Batches May Be Detrimental To Adversarial Robustness.](http://arxiv.org/abs/2303.17720) | 本文探究小批量在生成对抗样本中的作用，发现小批量大小的增加会导致生成的对抗样本效果下降，进一步提醒人们低估真实的对抗攻击强度并高估模型的鲁棒性。 |
| [^74] | [A Characterization of Online Multiclass Learnability.](http://arxiv.org/abs/2303.17716) | 在线多类学习问题中，使用Multiclass Littlestone维度可以刻画标签数目为无界情况下的可学习性。 |
| [^75] | [Analysis of Failures and Risks in Deep Learning Model Converters: A Case Study in the ONNX Ecosystem.](http://arxiv.org/abs/2303.17708) | 本文详细分析了深度学习模型转换器的故障情况，特别是对ONNX相关的转换器进行了首次故障分析，并详细报告了故障的症状，原因和位置以及随时间的趋势。 |
| [^76] | [Dual Cross-Attention for Medical Image Segmentation.](http://arxiv.org/abs/2303.17696) | 本文提出了一种双重交叉注意力模块，用于增强U-Net-based架构的医学图像分割模型。该模块通过捕获通道和空间依赖性，解决了编码器和解码器之间的语义鸿沟问题。 |
| [^77] | [Task Oriented Conversational Modelling With Subjective Knowledge.](http://arxiv.org/abs/2303.17695) | 本文提出了一种改进知识选择模块的实体检索方法，并探讨了一种潜在的关键字提取方法，以提高任务导向交互建模系统的性能。 |
| [^78] | [Exact Characterization of the Convex Hulls of Reachable Sets.](http://arxiv.org/abs/2303.17674) | 本文精确地刻画了具有有界扰动的非线性系统的可达集的凸包为一阶常微分方程的解的凸包，提出了一种低成本、高精度的估计算法，可用于过逼近可达集。 |
| [^79] | [Neural signature kernels as infinite-width-depth-limits of controlled ResNets.](http://arxiv.org/abs/2303.17671) | 通过控制ResNets的欧拉离散化，提出了一种新的家族限制核，称为神经签名核。在无限深度情况下，有限宽度的受控ResNets按分布收敛至神经CDE。 |
| [^80] | [MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries.](http://arxiv.org/abs/2303.17661) | 本论文提出了MetaEnhance，一个利用最先进的人工智能方法来提高电子学位论文关键字段质量的框架，并成功在500份样本中实现了高准确性的元数据错误检测和纠正。 |
| [^81] | [Progress towards an improved particle flow algorithm at CMS with machine learning.](http://arxiv.org/abs/2303.17657) | CMS实验引入机器学习粒子流算法（MLPF）并优化了其重构实现，首次以生成器/模拟级粒子信息为目标进行优化，这为提高检测器对物理量的响应铺平了道路。 |
| [^82] | [Self-Refine: Iterative Refinement with Self-Feedback.](http://arxiv.org/abs/2303.17651) | 自我反馈迭代精炼是一种无需监督学习或加强学习的LLMs初始输出优化方法，优于直接生成，被证实在7个不同任务中表现更好。 |
| [^83] | [Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning.](http://arxiv.org/abs/2303.17649) | 本文介绍了一种将英文GPT模型对齐到西班牙语的小封闭领域中的方法，该方法使用了奖励模型来改进答案的解码和生成，在问答任务中取得了良好的结果。 |
| [^84] | [Practical Policy Optimization with Personalized Experimentation.](http://arxiv.org/abs/2303.17648) | 本文提出了一个个性化试验框架PEX，利用HTE建模和序列决策策略优化，在用户级别上优化治疗组分配，同时优化多个短期和长期结果。 |
| [^85] | [An evaluation of time series forecasting models on water consumption data: A case study of Greece.](http://arxiv.org/abs/2303.17617) | 本文对希腊的用水量数据进行了时间序列预测模型的评估，揭示了各算法优缺点。 |
| [^86] | [Patterns Detection in Glucose Time Series by Domain Transformations and Deep Learning.](http://arxiv.org/abs/2303.17616) | 该论文介绍了一种基于血糖时间序列的领域转换和深度学习方法，以预测血糖水平的未来行为，帮助避免低血糖事件。 |
| [^87] | [Utilizing Reinforcement Learning for de novo Drug Design.](http://arxiv.org/abs/2303.17615) | 本文开发了一个统一的框架，利用强化学习生成预测具有活性的新药分子。在需要结构多样性的情况下，同时使用高分和低分子来更新策略是有利的。使用所有生成的分子可以提高性能稳定性，而off-policy算法有潜力提高生成分子的结构多样性。 |
| [^88] | [oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes.](http://arxiv.org/abs/2303.17612) | oBERTa是一组易于使用的语言模型，通过改进初始化、蒸馏、剪枝等技术，可以在不需要模型压缩方面的专业知识的情况下提高稀疏迁移学习的效率和准确性。 |
| [^89] | [Ensemble weather forecast post-processing with a flexible probabilistic neural network approach.](http://arxiv.org/abs/2303.17610) | 本论文提出了一种神经网络和归一化流相结合的方法，可联合预测所有位置和提前期，从而放宽了许多传统后处理方法的分布假设，并通过EUPPBench基准测试证明了其超越性能。 |
| [^90] | [Machine learning for discovering laws of nature.](http://arxiv.org/abs/2303.17607) | 模型基于达尔文自然选择，结合函数选择和运算符选择两个过程，通过从数据中学习构建理论，可自动发现和表示自然定律，成功应用于模拟多领域问题，并提供一种新方法解决描述自然定律的严格数学模型不足的问题。 |
| [^91] | [The G-invariant graph Laplacian.](http://arxiv.org/abs/2303.17001) | 本文提出了 G不变图拉普拉斯算子 用于处理数据集不仅在流形上，而且在一个连续群的作用下也是封闭的情形，相较于标准图拉普拉斯算子收敛速度更快。 |
| [^92] | [Severity classification of ground-glass opacity via 2-D convolutional neural network and lung CT scans: a 3-day exploration.](http://arxiv.org/abs/2303.16904) | 本研究探讨了使用二维卷积神经网络技术对肺部CT扫描进行严重程度分类，为诊断肺部疾病如COVID-19和肺炎提供新的方法。 |
| [^93] | [Neural Collapse Inspired Federated Learning with Non-iid Data.](http://arxiv.org/abs/2303.16066) | 本文提出了一种受神经衰竭启示的方案，通过将每个客户端优化向全局分类的最佳结构，解决了联邦学习中非独立同分布数据的挑战，并通过添加全局记忆向量来补救参数波动的问题。 |
| [^94] | [Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models.](http://arxiv.org/abs/2303.15999) | 本文开发了一种基于深度学习的半监督回归模型，可直接从图片中计算简绸纱线的线密度，以应用于古老油画，为艺术品保护和修复提供了可靠的线密度估计方法。 |
| [^95] | [Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach.](http://arxiv.org/abs/2303.13555) | 本研究提出了一种机器学习方法，可用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型的吸附动力学定律结构。 |
| [^96] | [ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization.](http://arxiv.org/abs/2303.11674) | 本文介绍了一种轻量级的类MLP架构ALOFT，它可使用动态低频变换用于域泛化。与CNN相比，该架构更好地捕捉全局表示，因此具有更好的泛化能力。 |
| [^97] | [Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning.](http://arxiv.org/abs/2303.09483) | 本文提出了一种辅助网络持续学习方法（ANCL），通过对流信息的控制，自然插值可塑性和稳定性之间的差异，有助于在神经网络中实现更好的稳定性-可塑性平衡。 |
| [^98] | [Rewarding Chatbots for Real-World Engagement with Millions of Users.](http://arxiv.org/abs/2303.06135) | 本文研究了如何通过利用用户反馈来提高聊天机器人的参与度，从而增强其留存能力。具体方法是使用自动伪标签来训练奖励模型，并使用平均对话长度一类的指标来衡量其效果。在试验中，该方法可将聊天机器人的平均对话长度提高70%。 |
| [^99] | [An ADMM Solver for the MKL-$L_{0/1}$-SVM.](http://arxiv.org/abs/2303.04445) | 该论文提出了一个快速ADMM求解器，能够有效处理带有$(0,1)$-损失函数的支持向量机问题，通过在合成平面数据上的实验，证明了该方法的潜力。 |
| [^100] | [BO-Muse: A human expert and AI teaming framework for accelerated experimental design.](http://arxiv.org/abs/2303.01684) | BO-Muse是一种新的人工智能和人类专家协作的优化方法，它让人类专家发挥主导作用，通过注入新颖性并发现弱点来打破过度开发，以加速实验设计。 |
| [^101] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^102] | [Partial Optimality in Cubic Correlation Clustering.](http://arxiv.org/abs/2302.04694) | 该论文关注于建立完全图和三次目标函数的特殊情况下的部分最优性条件，同时提出了测试这些条件的算法并在两个数据集上数值上检验了它们的效果。 |
| [^103] | [NeuKron: Constant-Size Lossy Compression of Sparse Reorderable Matrices and Tensors.](http://arxiv.org/abs/2302.04570) | NeuKron提出了一种可以将稀疏可重排矩阵和张量压缩至常数大小的方法，其使用了循环神经网络并能在线性时间内更新参数，同时可以在对数时间内检索每个条目的近似。 |
| [^104] | [A unified recipe for deriving (time-uniform) PAC-Bayes bounds.](http://arxiv.org/abs/2302.03421) | 该论文提出了一种用于推导PAC-Bayesian泛化界限的统一框架，不同于传统的固定样本大小方式，该框架适用于所有停止时间。同时，该论文还提出了新的边界方法，也可以应用于非平稳损失函数和非独立同分布的数据。 |
| [^105] | [Towards Verifying the Geometric Robustness of Large-scale Neural Networks.](http://arxiv.org/abs/2301.12456) | 本文提出了一种名为GeoRobust的黑盒鲁棒性分析器，旨在对大规模神经网络进行多个几何变换的鲁棒性验证，并且无论网络的体系结构、激活函数和神经元数量如何，GeoRobust都能够提供高精度的最坏情况的变换组合。 |
| [^106] | [On Batching Variable Size Inputs for Training End-to-End Speech Enhancement Systems.](http://arxiv.org/abs/2301.10587) | 本文系统地研究了不同批处理策略和批量大小对语音增强系统的训练效果的影响，以及如何在如何在实现高网络性能的同时最大化资源利用。 |
| [^107] | [Towards Flexibility and Interpretability of Gaussian Process State-Space Model.](http://arxiv.org/abs/2301.08843) | 本文提出了一种新的状态空间模型类TGPSSM，利用正规化流增加了标准GPSSM中的GP先验概率，从而增强模型的灵活性和表现力。同时提出了可扩展的变分推理算法，为潜在状态的变分分布提供灵活和最优的结构。 |
| [^108] | [Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.](http://arxiv.org/abs/2301.08243) | 本论文提出了一种非生成方法的自监督学习架构，即Image-based Joint-Embedding Predictive Architecture（I-JEPA），可以生成高度语义图像表示，通过联合嵌入预测架构和掩模策略达到这一目的。 |
| [^109] | [Efficient Online Learning with Memory via Frank-Wolfe Optimization: Algorithms with Bounded Dynamic Regret and Applications to Control.](http://arxiv.org/abs/2301.00497) | 本文介绍了基于Frank-Wolfe优化的高效内存在线学习算法，该算法以历史决策为基础，适应实时时变环境。该算法广泛应用于动态系统的在线控制，统计套利和时间序列预测等人工智能领域。 |
| [^110] | [Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data.](http://arxiv.org/abs/2301.00437) | 研究者证明对于均方误差和交叉熵损失，深度线性网络中出现的全局解在不同数据上都具有神经塌陷的特性，即最后一层特征会崩溃为类均值，而这些类均值是等角紧框架的顶点。 |
| [^111] | [HandsOff: Labeled Dataset Generation With No Additional Human Annotations.](http://arxiv.org/abs/2212.12645) | 介绍了一种名为HandsOff的技术，能够在现有少量标记图像的基础上生成无限数量的带标签的合成图像数据集，避免了其他方法中需要大量人工注释图像的问题，并在多个领域达到了最先进的性能水平。 |
| [^112] | [Estimating truncation effects of quantum bosonic systems using sampling algorithms.](http://arxiv.org/abs/2212.08546) | 本文提出了一种使用传统采样方法估计量子玻色系统截断误差的方法，该方法可用于估计实际量子模拟玻色理论所需的资源，并检查对应量子模拟的结果的有效性。 |
| [^113] | [Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation.](http://arxiv.org/abs/2212.06370) | 本文提出了一种双重精度质量驱动的神经网络，可以自动地学习基于回归的神经网络的预测区间，而非只提供传统的目标估计。该方法通过设计一种新颖的损失函数，最小化平均预测区间宽度以及提高覆盖概率来提高PI的质量和精度，且比最先进的方法更加具有计算效率。 |
| [^114] | [Editing Models with Task Arithmetic.](http://arxiv.org/abs/2212.04089) | 本文提出了一种使用任务向量进行模型编辑的新范式，任务向量可通过算术操作进行修改和组合，可以提高目标任务性能且对控制任务影响较小。 |
| [^115] | [A Unifying Theory of Distance from Calibration.](http://arxiv.org/abs/2211.16886) | 该论文提出了一种基于属性测试思想的严格框架来分析校准度量，提出了一个距离校准的基础真相，提供了三个一致性的校准度量，并在合成数据集和真实数据集的实证研究中证明了这些度量的表现如预期。 |
| [^116] | [Relative Sparsity for Medical Decision Problems.](http://arxiv.org/abs/2211.16566) | 本文提出了一种在医疗保健中使用的数据驱动决策策略的相对稀疏性方法，该方法帮助解释从当前的标准护理转变为新的护理政策的重要方面。 |
| [^117] | [DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata.](http://arxiv.org/abs/2211.11417) | 提出了一种名为 DyNCA 的实时可控动态纹理合成框架，可以合成无限长和任意大小的逼真视频纹理，提高了现有结果的逼真程度，并提供了多种实时视频控制功能。 |
| [^118] | [Implicit Graphon Neural Representation.](http://arxiv.org/abs/2211.03329) | 本文通过神经网络模型直接对图谱进行建模，得到了隐式图谱神经表示法，它具有可以表示任意分辨率的图谱、能够高效地生成任意大小具有所需结构的图表等优势。 |
| [^119] | [Unrolled Graph Learning for Multi-Agent Collaboration.](http://arxiv.org/abs/2210.17101) | 提出一种受人类协作启发的分布式多智能体学习模型，智能体可以自主检测适合的合作者，并参考合作者的模型以获得更好的性能，使用协作图实现成对协作关系指示，通过展开图学习网络以更灵活适应各种情况地学习潜在合作者之间的相似特征。 |
| [^120] | [Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection.](http://arxiv.org/abs/2210.16539) | 本文提出一种基于提示学习的预训练语言模型方法，加入不流畅特征提高阿尔茨海默病检测性能。实验结果表明该方法在基准数据集上取得了最佳表现，最高准确率达到95.1%。 |
| [^121] | [Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report.](http://arxiv.org/abs/2210.12113) | 本文提出了一种基于扩散概率模型的多任务脑肿瘤修复方法，可以同时修复缺失的脑肿瘤区域并为数据增加合成的脑肿瘤，相较于传统的生成对抗网络，该方法能够生成多样化的输出，且在公开数据集上取得了最先进的性能。 |
| [^122] | [Towards Global Neural Network Abstractions with Locally-Exact Reconstruction.](http://arxiv.org/abs/2210.12054) | GINNACER提出了一种新的全局区间神经网络抽象技术，可以在整个输入域产生准确的上估计边界，同时对于任何给定的局部输入保证精确重构。 |
| [^123] | [A Kernel Approach for PDE Discovery and Operator Learning.](http://arxiv.org/abs/2210.08140) | 本文提出了一种使用核方法的三步学习和求解偏微分方程的框架，能够近似解新的PDE，并展示了比其他算法更优的表现。 |
| [^124] | [How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization.](http://arxiv.org/abs/2210.06441) | 数据扩充针对多样性和不一致性能够比额外的训练数据更有价值，鼓励不变性的数据扩充可以比单一的不变性更有用，数据扩充在训练期间引入了额外的随机性并能够拉平损失函数的曲面。 |
| [^125] | [Improving Sample Quality of Diffusion Models Using Self-Attention Guidance.](http://arxiv.org/abs/2210.00939) | 该论文提出了一种利用自注意力指导的策略来提升扩散模型生成图像的稳定性和质量，具有较高的实用价值。 |
| [^126] | [Bounded Simplex-Structured Matrix Factorization: Algorithms, Identifiability and Applications.](http://arxiv.org/abs/2209.12638) | 提出了一种新的低秩矩阵分解模型BSSMF，它的矩阵W每列的元素属于给定的区间，而H的列是随机的，推广了NMF和SSMF，适用于矩阵元素属于给定区间的情况下，具有易于理解的分解和离散结构，适用于主题建模和社区检测等应用。 |
| [^127] | [Black-box Dataset Ownership Verification via Backdoor Watermarking.](http://arxiv.org/abs/2209.06015) | 本文提出了一种通过后门水印技术验证已发布数据集的所有权的方法，以检测其是否被用于训练（可疑的）第三方模型。 |
| [^128] | [Concatenated Classic and Neural (CCN) Codes: ConcatenatedAE.](http://arxiv.org/abs/2209.01701) | 本论文提出了连接传统和神经网络的长码方法，并设计出具有相同网络参数的NN用于多次进行One-hot编码，以扩展编码维度。实验证明，这种方法有效提高了纠错能力并具有较强的鲁棒性。 |
| [^129] | [Differentially Private Vertical Federated Clustering.](http://arxiv.org/abs/2208.01700) | 该论文提出了一种差分隐私的垂直联邦聚类算法，它可以在保护数据隐私的同时训练准确模型，是差分隐私垂直联邦k均值聚类的第一个实际解决方案。 |
| [^130] | [Supervised Training of Conditional Monge Maps.](http://arxiv.org/abs/2206.14262) | 本文提出了一种名为 CondOT 的多任务方法，它使用带有上下文标签 $c_i$ 的多个测量对 $\left(\mu_i, \nu_i\right)$ 来估计一组有条件的 OT 映射，以便将上下文因素引入 OT 估计中。 |
| [^131] | [On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice.](http://arxiv.org/abs/2206.06484) | 该论文研究了医学图像分割中常用的Accuracy和Dice测量方法在存在噪声标签的情况下的性能表现，并在探究中获得了结论：两种测量方法的最优解集体积可能显著偏离目标预期体积，Accuracy的解的体积始终小于等于Dice的解的体积，但当可行分割集被限制为体积等于目标预期体积的分割集时，两种指标的最优解是一致的。 |
| [^132] | [Continual evaluation for lifelong learning: Identifying the stability gap.](http://arxiv.org/abs/2205.13452) | 终身学习中，时间相关的数据生成分布对神经网络的梯度训练具有困难性。一些最先进的方法在开始学习新任务时会存在轻微的遗忘，随后会有一段性能恢复的阶段跟随，我们称之为“稳定性差距”。 |
| [^133] | [Domain Adversarial Graph Convolutional Network Based on RSSI and Crowdsensing for Indoor Localization.](http://arxiv.org/abs/2204.05184) | 本文提出了一种基于RSSI和众包感知的域对抗图卷积网络室内定位方法，可通过少量标记的站点调查数据和大量未标记的众包WiFi指纹进行训练，能够有效地捕捉数据的拓扑结构，并在基准数据集上实现了显著的定位精度改进。 |
| [^134] | [Continual Learning of Multi-modal Dynamics with External Memory.](http://arxiv.org/abs/2203.00936) | 本文提出了一种新的连续学习方法，通过在记忆中维护遇到序列模式的描述符来实现，能够有效处理新的行为模式的连续出现。 |
| [^135] | [VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning.](http://arxiv.org/abs/2202.10324) | 本文提出VRL3，一种数据驱动的框架，可用于解决具有挑战性的视觉深度强化学习任务。该框架包含三个阶段，并能在具有稀疏奖励和逼真视觉输入的手部操纵任务中显著提高样本效率。 |
| [^136] | [Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond.](http://arxiv.org/abs/2202.06861) | 本文介绍了Quantus，一个可解释的AI工具包，用于详尽迅速地评估神经网络预测的解释表现，并提高领域内的透明度和可重复性。 |
| [^137] | [The Schr\"odinger Bridge between Gaussian Measures has a Closed Form.](http://arxiv.org/abs/2202.05722) | 本文提出了高斯测度之间的Schr\"odinger桥的闭合形式表达式，解决了动态最优输运问题，在机器学习中具有重要的应用价值。 |
| [^138] | [Near-Optimal Learning of Extensive-Form Games with Imperfect Information.](http://arxiv.org/abs/2202.01752) | 本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。 |
| [^139] | [Factor-augmented tree ensembles.](http://arxiv.org/abs/2111.14000) | 本文提出了一种因子增强的树集合方法，能够处理多种不规则预测变量，为处理宏观金融问题提供一种可靠的方法。 |
| [^140] | [A Method for Evaluating Deep Generative Models of Images via Assessing the Reproduction of High-order Spatial Context.](http://arxiv.org/abs/2111.12577) | 本研究提出了一种评估图像深度生成模型的方法，通过检测经过训练的GAN生成的图像中高阶空间上下文的再现能力，并验证了多个客观测试以评估不同GAN的质量。 |
| [^141] | [One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer.](http://arxiv.org/abs/2110.10325) | 本论文提出了一种新的机器学习方法——一步式诱导式多目标学习与DiNS（OSAMTL-DiNS），以处理医学组织病理学全幻灯片图像分析中的复杂噪声标签。在乳腺癌肿瘤分割中得到了成功应用。 |
| [^142] | [Adaptive joint distribution learning.](http://arxiv.org/abs/2110.04829) | 该论文提出了一种自适应联合分布学习的框架，可以从大量数据点中估计低维、归一化和正的Radon-Nikodym导数模型，并在不同学习问题上取得了良好的结果。 |
| [^143] | [Transferring Knowledge Distillation for Multilingual Social Event Detection.](http://arxiv.org/abs/2108.03084) | 本研究提出了一种基于GNN的多语言社交事件检测方法，该方法利用了跨语言词嵌入技术，能够对多种语言的数据进行检测，对于较少使用的语言也有较好的应用效果。 |
| [^144] | [Learning Treatment Effects in Panels with General Intervention Patterns.](http://arxiv.org/abs/2106.02780) | 本文提出了一种针对面板数据的因果推断问题的解决方案，利用合成对照的方法进行处理。文章拓展了该框架，使其适用性更加广泛，并在计算实验中展现了它的优越性能。 |
| [^145] | [Optimal training of integer-valued neural networks with mixed integer programming.](http://arxiv.org/abs/2009.03825) | 本文介绍了一种新的混合整数规划方法来训练整数值神经网络，其中包括优化神经元数量的方法和鼓励NN更加稀疏的正则化项方法，可以在不使用GPU或复杂超参数调整的情况下提高训练效率和泛化性能。 |
| [^146] | [Adaptive Estimators Show Information Compression in Deep Neural Networks.](http://arxiv.org/abs/1902.09037) | 本文使用自适应估计技术研究深度网络中的信息压缩，发现相比使用饱和激活函数，非饱和激活函数的网络能实现可比的任务性能水平，但无法显示出信息压缩。 |

# 详细

[^1]: 无穷分辨率扩散模型

    $\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States. (arXiv:2303.18242v1 [cs.LG])

    [http://arxiv.org/abs/2303.18242](http://arxiv.org/abs/2303.18242)

    引入了一种名为 $\infty$-Diff 的生成式扩散模型，可以处理无限分辨率的数据，不需要使用超网络进行潜在向量压缩或依赖于离散的组件，能够显著提高样本质量，并能够在保留细节的同时有效地扩展到比训练数据更高的分辨率。

    

    我们引入了一种名为$\infty$-Diff的生成式扩散模型，可以处理无限分辨率的数据。通过在训练期间随机抽样数据的子集并学习去噪，我们可以学到一个连续的函数，允许任意分辨率的采样。与其他最近的无限分辨率生成式模型相比，我们的方法直接操作原始数据，不需要使用超网络进行潜在向量压缩或依赖于离散的组件。因此，我们的方法可以显著提高样本质量，例如降低FID分数，并能够在保留细节的同时有效地扩展到比训练数据更高的分辨率。

    We introduce $\infty$-Diff, a generative diffusion model which directly operates on infinite resolution data. By randomly sampling subsets of coordinates during training and learning to denoise the content at those coordinates, a continuous function is learned that allows sampling at arbitrary resolutions. In contrast to other recent infinite resolution generative models, our approach operates directly on the raw data, not requiring latent vector compression for context, using hypernetworks, nor relying on discrete components. As such, our approach achieves significantly higher sample quality, as evidenced by lower FID scores, as well as being able to effectively scale to higher resolutions than the training data while retaining detail.
    
[^2]: 寻找具有身体智能的人工视觉皮层在哪里？

    Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])

    [http://arxiv.org/abs/2303.18240](http://arxiv.org/abs/2303.18240)

    该研究研究了使用预训练视觉表征来实现身体智能的最新进展。他们展示了最大、最全面的经验研究，发现没有一种表征是普遍优越的，并且数据集的大小和多样性并不能普遍改善性能。

    

    我们提出了最大、最全面的预训练视觉表征（PVR）或视觉基础模型的经验研究，用于身体智能。首先，我们策划了 CortexBench，其中包括涵盖动力学、导航、熟练和移动操作的17种不同任务。接下来，我们系统评估了现有的PVR，发现没有一种是普遍优越的。为了研究预训练数据规模和多样性的影响，我们结合了来自7个不同来源的超过4000小时的自我中心视频（超过560万张图像）和ImageNet，使用切片数据的遮盖自编码（MAE）来训练不同大小的视觉变形器。与先前的工作推断相反，我们发现扩展数据集的规模和多样性并不能普遍改善性能（但平均性能有所提高）。我们最大的模型名为VC-1，平均表现超过所有先前的PVR，但也没有普遍优势。最后，我们展示了VC-1的特定于任务或领域的适应会带来实质性的改进。

    We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia
    
[^3]: 通过简约表示获取物理和化学信息：不变分变自编码器用于图像分析

    Physics and Chemistry from Parsimonious Representations: Image Analysis via Invariant Variational Autoencoders. (arXiv:2303.18236v1 [cs.LG])

    [http://arxiv.org/abs/2303.18236](http://arxiv.org/abs/2303.18236)

    这篇论文介绍了如何使用不变分变自编码器（VAEs）解决在图像分析中的物理和化学问题，通过分离已知和未知的变量因素来发现物理和化学现象。

    

    电子、光学和扫描探针显微镜技术正在产生越来越多的图像数据，这些数据包含原子和中等尺度结构和功能的信息。这需要开发机器学习方法，从数据中发现物理和化学现象，例如电子和扫描隧道显微镜图像中对称性破缺的表现，纳米颗粒的变异性。变分自编码器（VAE）正在成为一种强大的无监督数据分析范例，允许分解变量因素并发现最佳简洁表示。在这里，我们总结了VAEs的最新发展，涵盖了VAEs背后的基本原理和直觉。介绍了不变VAEs作为一种适应于成像数据中的尺度和平移不变性并将已知变化因素与待发现因素分开的方法。我们进一步描述了不变VAEs在解决图像分析问题方面的机会，包括发现对称性破缺现象和识别化学功能。

    Electron, optical, and scanning probe microscopy methods are generating ever increasing volume of image data containing information on atomic and mesoscale structures and functionalities. This necessitates the development of the machine learning methods for discovery of physical and chemical phenomena from the data, such as manifestations of symmetry breaking in electron and scanning tunneling microscopy images, variability of the nanoparticles. Variational autoencoders (VAEs) are emerging as a powerful paradigm for the unsupervised data analysis, allowing to disentangle the factors of variability and discover optimal parsimonious representation. Here, we summarize recent developments in VAEs, covering the basic principles and intuition behind the VAEs. The invariant VAEs are introduced as an approach to accommodate scale and translation invariances present in imaging data and separate known factors of variations from the ones to be discovered. We further describe the opportunities ena
    
[^4]: 简单的排序标准有助于在加性噪声模型中找到因果顺序。

    Simple Sorting Criteria Help Find the Causal Order in Additive Noise Models. (arXiv:2303.18211v1 [stat.ML])

    [http://arxiv.org/abs/2303.18211](http://arxiv.org/abs/2303.18211)

    文章探讨了加性噪声模型中找到因果顺序的方法。作者发现除了方差排序外，变量的决定系数$R^2$排序也可用于匹配已有方法的表现，且不受数据缩放的影响。

    

    加性噪声模型（ANM）是一种常见的功能假设，可以从观测数据中学习因果结构。由于缺乏符合假设的真实世界数据，合成ANM数据经常用于评估因果发现算法。Reisach等人（2021）表明，对于常见的模拟参数，按增大方差的顺序变量排列与因果顺序密切相关，并引入变异性可排序性来量化这种对齐程度。本文还表明，除了方差，还有变量的方差被所有其他变量解释的比例（由决定系数$R^2$捕获）倾向于沿着因果顺序增加。简单的基准算法可以使用$R^2$-sortability来匹配已有方法的性能。由于$R^2$可排序性不受数据缩放的影响，这些算法在标准化或重新缩放的数据上表现同样出色，解决了利用变异性可排序性的算法的一个关键限制。

    Additive Noise Models (ANM) encode a popular functional assumption that enables learning causal structure from observational data. Due to a lack of real-world data meeting the assumptions, synthetic ANM data are often used to evaluate causal discovery algorithms. Reisach et al. (2021) show that, for common simulation parameters, a variable ordering by increasing variance is closely aligned with a causal order and introduce var-sortability to quantify the alignment. Here, we show that not only variance, but also the fraction of a variable's variance explained by all others, as captured by the coefficient of determination $R^2$, tends to increase along the causal order. Simple baseline algorithms can use $R^2$-sortability to match the performance of established methods. Since $R^2$-sortability is invariant under data rescaling, these algorithms perform equally well on standardized or rescaled data, addressing a key limitation of algorithms exploiting var-sortability. We characterize and 
    
[^5]: SimTS:重新思考时间序列预测的对比表示学习

    SimTS: Rethinking Contrastive Representation Learning for Time Series Forecasting. (arXiv:2303.18205v1 [cs.LG])

    [http://arxiv.org/abs/2303.18205](http://arxiv.org/abs/2303.18205)

    本文提出了一种不依赖于负对或特定时间序列特征假设的简单表示学习方法SimTS，通过学习在潜在空间中从过去预测未来来改进时间序列预测，并在多个数据集上表现出具有竞争力的性能。

    

    对比学习方法在图像或时间序列分类中展现出了强大的学习意义的表现。但是，这些方法对于时间序列预测的效果不够明显，因为对实例鉴别的优化不能直接应用于从历史上下文中预测未来状态。此外，目前技术中正负示例对的构建强烈依赖于特定的时间序列特征，限制了其在各种类型的时间序列数据中的普适性。为解决这些局限性，我们提出了SimTS，这是一种简单的表示学习方法，通过学习在潜在空间中从过去预测未来来改进时间序列预测。SimTS不依赖于负对或特定时间序列特征的假设。我们在几个基准时间序列预测数据集上进行了大量实验，结果表明SimTS取得了竞争性的性能。

    Contrastive learning methods have shown an impressive ability to learn meaningful representations for image or time series classification. However, these methods are less effective for time series forecasting, as optimization of instance discrimination is not directly applicable to predicting the future state from the history context. Moreover, the construction of positive and negative pairs in current technologies strongly relies on specific time series characteristics, restricting their generalization across diverse types of time series data. To address these limitations, we propose SimTS, a simple representation learning approach for improving time series forecasting by learning to predict the future from the past in the latent space. SimTS does not rely on negative pairs or specific assumptions about the characteristics of the particular time series. Our extensive experiments on several benchmark time series forecasting datasets show that SimTS achieves competitive performance comp
    
[^6]: TPMCF: 使用多源协同特征进行时间QoS预测

    TPMCF: Temporal QoS Prediction using Multi-Source Collaborative Features. (arXiv:2303.18201v1 [cs.SE])

    [http://arxiv.org/abs/2303.18201](http://arxiv.org/abs/2303.18201)

    本文提出了一种新的方法TPMCF，利用多源特征进行QoS预测。该方法利用带有注意力机制的编码器-解码器架构，并使用协作特征捕捉用户和服务之间的关系，有效地处理数据稀疏和异常值。

    

    最近，随着服务API的快速部署，个性化的服务推荐在电子商务行业的增长中发挥了至关重要的作用。决定服务性能的服务质量(QoS)参数经常被用于推荐，但随时间波动。因此，QoS的预测对于在等价服务中识别合适的服务至关重要。当代的时间QoS预测方法由于各种限制而很难达到期望的精度，例如无法处理数据稀疏和异常值以及捕获用户-服务交互之间的高阶时间关系。虽然最近一些基于循环神经网络的体系结构可以建模QoS数据之间的时间关系，但由于缺乏其他特征（例如协作特征）来理解用户-服务交互之间的关系，预测精度会降低。本文通过提出一种解决方案TPMCF，来解决上述挑战。TPMCF利用多源特征（包括时间、用户和服务特征）进行QoS预测。具体地，它使用一个带有注意机制的新颖编码器解码器架构来利用用户-服务交互之间的高阶时间关系。此外，它使用协作特征来捕捉用户和服务之间的关系，并处理数据稀疏和异常值。对实际数据集进行的大量实验证明了TPMCF的有效性和优越性。

    Recently, with the rapid deployment of service APIs, personalized service recommendations have played a paramount role in the growth of the e-commerce industry. Quality-of-Service (QoS) parameters determining the service performance, often used for recommendation, fluctuate over time. Thus, the QoS prediction is essential to identify a suitable service among functionally equivalent services over time. The contemporary temporal QoS prediction methods hardly achieved the desired accuracy due to various limitations, such as the inability to handle data sparsity and outliers and capture higher-order temporal relationships among user-service interactions. Even though some recent recurrent neural-network-based architectures can model temporal relationships among QoS data, prediction accuracy degrades due to the absence of other features (e.g., collaborative features) to comprehend the relationship among the user-service interactions. This paper addresses the above challenges and proposes a s
    
[^7]: PADME-SoSci：社会科学中用于分析和分布式机器学习的平台

    PADME-SoSci: A Platform for Analytics and Distributed Machine Learning for the Social Sciences. (arXiv:2303.18200v1 [cs.CR])

    [http://arxiv.org/abs/2303.18200](http://arxiv.org/abs/2303.18200)

    PADME-SoSci是一个联邦学习平台，通过在原数据位置进行学习来实现数据所有权保护和跨位置数据分析。

    

    在社会数据科学中，数据隐私和所有权是非常重要的，并提出了法律和伦理问题。当不同方拥有数据的不同部分时，共享和分析数据变得困难。一种应对挑战的方法是在收集数据进行分析之前将数据应用去识别化或匿名化技术。然而，这样做可能会降低数据效用并增加重新识别的风险。为了解决这些局限性，我们提出了PADME，这是一个分布式分析工具，它联邦了模型实现和训练。PADME使用联邦方法，模型由所有方实现和部署，并逐步访问每个数据位置进行训练。这使得可以跨位置分析数据，同时仍允许像所有数据都在同一位置一样训练模型。在数据的原始位置上训练模型可以保留数据所有权。此外，只有在所有数据位置上的分析都完成后，才会提供结果。

    Data privacy and ownership are significant in social data science, raising legal and ethical concerns. Sharing and analyzing data is difficult when different parties own different parts of it. An approach to this challenge is to apply de-identification or anonymization techniques to the data before collecting it for analysis. However, this can reduce data utility and increase the risk of re-identification. To address these limitations, we present PADME, a distributed analytics tool that federates model implementation and training. PADME uses a federated approach where the model is implemented and deployed by all parties and visits each data location incrementally for training. This enables the analysis of data across locations while still allowing the model to be trained as if all data were in a single location. Training the model on data in its original location preserves data ownership. Furthermore, the results are not provided until the analysis is completed on all data locations to
    
[^8]: GVP: 生成体素元素

    GVP: Generative Volumetric Primitives. (arXiv:2303.18193v1 [cs.CV])

    [http://arxiv.org/abs/2303.18193](http://arxiv.org/abs/2303.18193)

    本文提出了Generative Volumetric Primitives (GVP)生成体素元素，是第一个能够实时采样和渲染512分辨率图像的纯3D生成模型，通过2D卷积网络高效地生成多个体积元素和它们的空间信息，能够捕捉三维空间中的稀疏性和对应性，并通过知识蒸馏技术实现高自由度的训练。实验表明，GVP优于以前的方法，从图像质量和多视图一致性两方面进行了验证。

    

    三维感知生成模型的进展推动了具有显式摄像机控制的图像合成边界。为了实现高分辨率图像合成，已经尝试设计了一些高效的发生器，例如具有3D和2D组件的混合架构。然而，这种设计会损害多视图一致性，而高分辨率的纯3D生成器的设计仍然是一个开放问题。本文提出了生成体素元素（GVP），这是第一个能够实时采样和渲染512分辨率图像的纯3D生成模型。GVP共同模拟了多个体积元素和它们的空间信息，这两个元素都可以通过2D卷积网络高效地生成。这些元素的混合自然地捕捉了三维空间中的稀疏性和对应性。这种高度自由度的发生器的训练是通过知识蒸馏技术实现的。在几个数据集上的实验证明，GVP在图像质量和多视图一致性方面的表现优于以前的方法。

    Advances in 3D-aware generative models have pushed the boundary of image synthesis with explicit camera control. To achieve high-resolution image synthesis, several attempts have been made to design efficient generators, such as hybrid architectures with both 3D and 2D components. However, such a design compromises multiview consistency, and the design of a pure 3D generator with high resolution is still an open problem. In this work, we present Generative Volumetric Primitives (GVP), the first pure 3D generative model that can sample and render 512-resolution images in real-time. GVP jointly models a number of volumetric primitives and their spatial information, both of which can be efficiently generated via a 2D convolutional network. The mixture of these primitives naturally captures the sparsity and correspondence in the 3D volume. The training of such a generator with a high degree of freedom is made possible through a knowledge distillation technique. Experiments on several datas
    
[^9]: 基于破坏鲁棒性一致性的推理阶段后门检测

    Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency. (arXiv:2303.18191v1 [cs.CR])

    [http://arxiv.org/abs/2303.18191](http://arxiv.org/abs/2303.18191)

    本文提出了一种新的测试时间触发样本检测方法 TeCo，该方法只需要受害模型的硬标签输出，通过评估测试时间鲁棒性一致性来检测后门，不需要其他额外的信息，提高了实用性。

    

    深度神经网络被证明容易受到后门攻击。在推理阶段检测触发样本，即测试时间触发样本检测，可以防止后门被触发。然而，现有的检测方法通常需要防御者对受害模型具有高度可访问性、额外的清洁数据或了解后门触发器的外观知识等，限制了它们的实用性。本文提出了一种新的测试时间触发样本检测方法 TeCo，该方法只需要受害模型的硬标签输出，不需要任何额外的信息。我们的研究从一项有趣的观察开始，即被感染的后门模型在对于干净图像的不同图像破坏方面具有相似的性能，但对于触发样本表现不一致。基于这一现象，我们设计了 TeCo 来评估测试时间鲁棒性一致性，通过计算预测结果的偏差来进行检测。

    Deep neural networks are proven to be vulnerable to backdoor attacks. Detecting the trigger samples during the inference stage, i.e., the test-time trigger sample detection, can prevent the backdoor from being triggered. However, existing detection methods often require the defenders to have high accessibility to victim models, extra clean data, or knowledge about the appearance of backdoor triggers, limiting their practicality. In this paper, we propose the test-time corruption robustness consistency evaluation (TeCo), a novel test-time trigger sample detection method that only needs the hard-label outputs of the victim models without any extra information. Our journey begins with the intriguing observation that the backdoor-infected models have similar performance across different image corruptions for the clean images, but perform discrepantly for the trigger samples. Based on this phenomenon, we design TeCo to evaluate test-time robustness consistency by calculating the deviation o
    
[^10]: 利用事件驱动的前向前向过程学习尖峰神经系统

    Learning Spiking Neural Systems with the Event-Driven Forward-Forward Process. (arXiv:2303.18187v1 [cs.NE])

    [http://arxiv.org/abs/2303.18187](http://arxiv.org/abs/2303.18187)

    该论文提出了一种基于事件驱动的前向-前向和预测式前向-前向学习过程的通用化方案，用于递归电路计算每个神经元的膜电位。与依赖反馈突触调整神经电活动的尖峰神经编码不同，该模型纯在线并且时间向前，是学习带有时间尖峰信号的感觉数据模式分布表示的有前途的一种途径。

    

    我们为使用尖峰神经元进行信息处理开发了一种新的学分分配算法，无需反馈突触。具体而言，我们提出了一种基于事件驱动的前向-前向和预测式前向-前向学习过程的通用化方案，用于迭代处理感觉输入。因此，递归电路会根据局部自下向上、自上而下和侧面的信号计算每层中每个神经元的膜电位，促进一种动态的、逐层并行的神经计算形式。与依赖反馈突触调整神经电活动的尖峰神经编码不同，我们的模型纯在线并且时间向前，这样就能够学习带有时间尖峰信号的感觉数据模式的分布表示。值得注意的是，我们在几个模式数据集上的实验结果表明，基于事件驱动的前向前向（ED-FF）框架工作正常。

    We develop a novel credit assignment algorithm for information processing with spiking neurons without requiring feedback synapses. Specifically, we propose an event-driven generalization of the forward-forward and the predictive forward-forward learning processes for a spiking neural system that iteratively processes sensory input over a stimulus window. As a result, the recurrent circuit computes the membrane potential of each neuron in each layer as a function of local bottom-up, top-down, and lateral signals, facilitating a dynamic, layer-wise parallel form of neural computation. Unlike spiking neural coding, which relies on feedback synapses to adjust neural electrical activity, our model operates purely online and forward in time, offering a promising way to learn distributed representations of sensory data patterns with temporal spike signals. Notably, our experimental results on several pattern datasets demonstrate that the even-driven forward-forward (ED-FF) framework works we
    
[^11]: 关于扩散模型参数效率微调的进一步探究

    A Closer Look at Parameter-Efficient Tuning in Diffusion Models. (arXiv:2303.18181v1 [cs.CV])

    [http://arxiv.org/abs/2303.18181](http://arxiv.org/abs/2303.18181)

    本文研究了大规模扩散模型的参数效率微调，通过插入小型可学习模块来实现。研究表明，适配器的输入位置是影响下游任务性能的关键因素，而将输入位置放在交叉注意力块之后可获得最佳性能。

    

    大规模扩散模型如Stable Diffusion在许多实际应用中都表现出足够的强大，然而在对这些模型进行个性化微调时却需要耗费大量内存和时间。受自然语言处理领域近期的进展启发，我们通过插入小型可学习模块(称作适配器)来研究大规模扩散模型的参数效率微调。具体来说，我们将适配器的设计空间分解为正交因子——输入位置、输出位置以及函数形式，并进行ANOVA分析，一种分析离散(设计选项)与连续变量(评估指标)之间相关性的经典统计方法。我们的分析表明，适配器的输入位置是影响下游任务性能的关键因素。接着，我们仔细研究了输入位置的选择，发现将输入位置放在交叉注意力块后可以使性能达到最佳。

    Large-scale diffusion models like Stable Diffusion are powerful and find various real-world applications while customizing such models by fine-tuning is both memory and time inefficient. Motivated by the recent progress in natural language processing, we investigate parameter-efficient tuning in large diffusion models by inserting small learnable modules (termed adapters). In particular, we decompose the design space of adapters into orthogonal factors -- the input position, the output position as well as the function form, and perform Analysis of Variance (ANOVA), a classical statistical approach for analyzing the correlation between discrete (design options) and continuous variables (evaluation metrics). Our analysis suggests that the input position of adapters is the critical factor influencing the performance of downstream tasks. Then, we carefully study the choice of the input position, and we find that putting the input position after the cross-attention block can lead to the bes
    
[^12]: 鲁棒性与知识产权保护的竖直联邦学习防止参与方意外退出

    Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties. (arXiv:2303.18178v1 [cs.CR])

    [http://arxiv.org/abs/2303.18178](http://arxiv.org/abs/2303.18178)

    本文提出了一种名为Party-wise Dropout的方法，该方法可以提高竖直联邦学习模型对被动方意外退出的鲁棒性，并提出了一种名为DIMIP的防御方法，用于保护活动方在部署阶段的知识产权。

    

    竖直联邦学习（VFL）使具有标记特征的服务提供商（即活动方）能够与拥有辅助特征的被动方合作，以提高模型性能。然而，现有的VFL方法存在两个主要漏洞，当被动方在VFL部署阶段意外退出时 - 严重的性能降低和活动方标签的知识产权泄露。本文提出了Party-wise Dropout来提高VFL模型对被动方意外退出的鲁棒性，以及一种名为DIMIP的防御方法，用于保护活动方在部署阶段的知识产权。我们对多个数据集以及不同的推理攻击对我们提出的方法进行了评估。结果表明，Party-wise Dropout可以在被动方退出后有效地保持模型性能，DIMIP可以成功地掩盖被动方特征提取器中的标签信息，从而减轻知识产权侵权。

    Vertical federated learning (VFL) enables a service provider (i.e., active party) who owns labeled features to collaborate with passive parties who possess auxiliary features to improve model performance. Existing VFL approaches, however, have two major vulnerabilities when passive parties unexpectedly quit in the deployment phase of VFL - severe performance degradation and intellectual property (IP) leakage of the active party's labels. In this paper, we propose \textbf{Party-wise Dropout} to improve the VFL model's robustness against the unexpected exit of passive parties and a defense method called \textbf{DIMIP} to protect the active party's IP in the deployment phase. We evaluate our proposed methods on multiple datasets against different inference attacks. The results show that Party-wise Dropout effectively maintains model performance after the passive party quits, and DIMIP successfully disguises label information from the passive party's feature extractor, thereby mitigating I
    
[^13]: 今天的迭代学习算法有多高效？

    How Efficient Are Today's Continual Learning Algorithms?. (arXiv:2303.18171v1 [cs.CV])

    [http://arxiv.org/abs/2303.18171](http://arxiv.org/abs/2303.18171)

    这篇论文研究了增量班级学习的最新方法，并指出许多方法在计算、内存和存储方面非常低效。为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。

    

    监督式迭代学习涉及从不断增长的带标签数据流中更新深度神经网络（DNN）。尽管大部分工作集中在克服灾难性遗忘上，但迭代学习背后的主要动机之一是能够有效地更新网络，而不是随着训练数据集随时间增长，从头开始重新训练。尽管最近的迭代学习方法基本上解决了灾难遗忘问题，但对这些算法的效率关注不足。在这里，我们研究了增量班级学习的最新方法，并表明许多方法在计算、内存和存储方面非常低效。有些方法甚至需要更多的计算资源才能完成训练！我们认为，为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。迭代学习不仅仅是缓解灾难性遗忘。

    Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forg
    
[^14]: 指标变量限制下秩一函数的约束优化

    Constrained Optimization of Rank-One Functions with Indicator Variables. (arXiv:2303.18158v1 [math.OC])

    [http://arxiv.org/abs/2303.18158](http://arxiv.org/abs/2303.18158)

    本文提出了一种基于透视重构技术的紧凑扩展公式，用于解决涉及指标变量限制下决策变量支持集合的秩一凸函数的最小化问题。

    

    在各种机器学习应用中，涉及到通过约束来建模决策变量支持集合的秩一凸函数的最小化的优化问题。这些问题通常采用指标变量来识别连续变量的支持。本文通过透视重构技术研究了这些问题的紧凑扩展公式。与大多数先前的研究依赖于支持函数参数和离散规划技术以提供凸包结果不同，我们提出了一种构造方法，利用透视函数引起的隐藏圆锥结构。为此，我们首先针对每个圆锥约束涉及独立连续变量的线性函数和一组二元变量的一般圆锥混合二进制集合建立了一个凸包结果。然后，我们展示了与应对epi相关的集合的扩展表示形式。

    Optimization problems involving minimization of a rank-one convex function over constraints modeling restrictions on the support of the decision variables emerge in various machine learning applications. These problems are often modeled with indicator variables for identifying the support of the continuous variables. In this paper we investigate compact extended formulations for such problems through perspective reformulation techniques. In contrast to the majority of previous work that relies on support function arguments and disjunctive programming techniques to provide convex hull results, we propose a constructive approach that exploits a hidden conic structure induced by perspective functions. To this end, we first establish a convex hull result for a general conic mixed-binary set in which each conic constraint involves a linear function of independent continuous variables and a set of binary variables. We then demonstrate that extended representations of sets associated with epi
    
[^15]: 基于图神经网络的交通工程多智能体系统MAGNNETO

    MAGNNETO: A Graph Neural Network-based Multi-Agent system for Traffic Engineering. (arXiv:2303.18157v1 [cs.NI])

    [http://arxiv.org/abs/2303.18157](http://arxiv.org/abs/2303.18157)

    提出了一个基于分布式ML框架MAGNNETO，运用多智能体强化学习和图神经网络来解决ISP网络的TE优化问题，在OSPF中的链路权重调整方面表现良好。

    

    当前网络优化的趋势是使用机器学习(ML)解决各种各样的网络优化任务。因此，已经做出了许多面向交通工程(Traffic Engineering, TE)的属于ML的解决方案，因为它是ISP网络中的核心问题。目前，最先进的TE优化器依赖于传统的优化技术，比如局部搜索(Local search)、约束程序设计(Constraint Programming)或线性程序设计(Linear programming)。在本文中，我们提出了MAGNNETO，这是一个基于分布式ML的框架，它利用多智能体强化学习(Multi-Agent Reinforcement Learning)和图神经网络(Graph Neural Networks)进行分布式TE优化。MAGNNETO在网络中部署了一组智能体，这些智能体通过邻近智能体之间的消息交换进行分布式学习和通信。特别地，我们将该框架应用于优化OSPF中的链路权重，以实现最小化网络拥塞的目标。在我们的评估中，我们将MAGNNETO与多个最新的TE优化器进行了比较，在超过75个拓扑结构(达到153个节点)上进行了测试。

    Current trends in networking propose the use of Machine Learning (ML) for a wide variety of network optimization tasks. As such, many efforts have been made to produce ML-based solutions for Traffic Engineering (TE), which is a fundamental problem in ISP networks. Nowadays, state-of-the-art TE optimizers rely on traditional optimization techniques, such as Local search, Constraint Programming, or Linear programming. In this paper, we present MAGNNETO, a distributed ML-based framework that leverages Multi-Agent Reinforcement Learning and Graph Neural Networks for distributed TE optimization. MAGNNETO deploys a set of agents across the network that learn and communicate in a distributed fashion via message exchanges between neighboring agents. Particularly, we apply this framework to optimize link weights in OSPF, with the goal of minimizing network congestion. In our evaluation, we compare MAGNNETO against several state-of-the-art TE optimizers in more than 75 topologies (up to 153 node
    
[^16]: BERT4ETH：用于以太坊欺诈检测的预训练Transformer模型

    BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection. (arXiv:2303.18138v1 [cs.CR])

    [http://arxiv.org/abs/2303.18138](http://arxiv.org/abs/2303.18138)

    BERT4ETH是一个用于以太坊欺诈检测的预训练Transformer编码器，它通过三种实用有效策略来解决针对高度重复、偏斜分布和异构以太坊交易这些挑战，并且比现有技术更为优秀。

    

    随着以太坊上各种欺诈行为的激增，保护易受攻击用户免受受害是非常必要的。虽然目前的研究仅依赖于基于图的欺诈检测方法，但有人认为它们可能不适合处理高度重复、偏斜分布和异构以太坊交易。为了应对这些挑战，我们提出了BERT4ETH，这是一个通用的预训练Transformer编码器，用于检测以太坊上的各种欺诈行为。BERT4ETH具有Transformer的优秀建模能力，能够捕捉以太坊交易中固有的动态顺序模式，并通过减少重复性、减轻偏斜和建模异构性等三种实用有效策略来解决为以太坊预训练BERT模型带来的挑战。我们的实证评估表明，BERT4ETH在欺诈检测方面优于现有技术。

    As various forms of fraud proliferate on Ethereum, it is imperative to safeguard against these malicious activities to protect susceptible users from being victimized. While current studies solely rely on graph-based fraud detection approaches, it is argued that they may not be well-suited for dealing with highly repetitive, skew-distributed and heterogeneous Ethereum transactions. To address these challenges, we propose BERT4ETH, a universal pre-trained Transformer encoder that serves as an account representation extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features the superior modeling capability of Transformer to capture the dynamic sequential patterns inherent in Ethereum transactions, and addresses the challenges of pre-training a BERT model for Ethereum with three practical and effective strategies, namely repetitiveness reduction, skew alleviation and heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH outperforms state-of-the-ar
    
[^17]: 智能电网故障预测系统的机器学习对抗攻击

    Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids. (arXiv:2303.18136v1 [cs.CR])

    [http://arxiv.org/abs/2303.18136](http://arxiv.org/abs/2303.18136)

    该论文提出了针对智能电网故障预测系统的机器学习对抗攻击的研究，证明智能电网中使用的深度神经网络方法容易受到对抗性攻击，并突出了目前在智能电网中的机器学习算法存在对各种对抗性攻击的弱点。

    

    在智能电网中，由于经济和关键性的原因，故障检测任务可能会对社会产生很大的影响。近年来，许多智能电网应用程序，如缺陷检测和负载预测，已经采用了数据驱动的方法。本研究的目的是研究智能电网情况下机器学习（ML）应用的安全性挑战。事实上，这些数据驱动算法的鲁棒性和安全性尚未与所有电网应用程序相关地进行广泛研究。我们首先证明了智能电网中使用的深度神经网络方法容易受到对抗性攻击。接着，我们突出展示了故障定位和类型分类方面的研究，说明了目前在智能电网中的机器学习算法对各种对抗性攻击的弱点。

    In smart electrical grids, fault detection tasks may have a high impact on society due to their economic and critical implications. In the recent years, numerous smart grid applications, such as defect detection and load forecasting, have embraced data-driven methodologies. The purpose of this study is to investigate the challenges associated with the security of machine learning (ML) applications in the smart grid scenario. Indeed, the robustness and security of these data-driven algorithms have not been extensively studied in relation to all power grid applications. We demonstrate first that the deep neural network method used in the smart grid is susceptible to adversarial perturbation. Then, we highlight how studies on fault localization and type classification illustrate the weaknesses of present ML algorithms in smart grids to various adversarial attacks
    
[^18]: 基于脱同步的神经网络侧信道攻击对抗措施

    A Desynchronization-Based Countermeasure Against Side-Channel Analysis of Neural Networks. (arXiv:2303.18132v1 [cs.CR])

    [http://arxiv.org/abs/2303.18132](http://arxiv.org/abs/2303.18132)

    本文提出了一种基于脱同步的对抗措施，使激活函数的时序分析变得更加困难，以防止模型提取和神经网络侧信道分析攻击。实验结果表明，在4096个神经元的情况下，我们的方案仅产生不到1%的开销。

    

    模型提取攻击已经被广泛应用，通常可以用于恢复多层神经网络的机密参数。最近，神经网络的侧信道分析即使对于具有多个深层网络的网络也可以进行参数提取，并且非常有效。因此，实现一定程度的保护以抵御这些攻击非常重要。在本文中，我们提出了一种基于脱同步的对抗措施，使激活函数的时序分析变得更加困难。我们分析了几种激活函数的时序特性，并设计了脱同步方式，以隐藏输入和激活类型的依赖性。我们在32位ARM Cortex-M4微控制器上实验验证了对抗措施的有效性，并使用t检验显示了侧信道信息泄漏。最终开销取决于完全连接层中神经元的数量，例如，对于4096个神经元的情况，我们的方案开销不到1%。

    Model extraction attacks have been widely applied, which can normally be used to recover confidential parameters of neural networks for multiple layers. Recently, side-channel analysis of neural networks allows parameter extraction even for networks with several multiple deep layers with high effectiveness. It is therefore of interest to implement a certain level of protection against these attacks. In this paper, we propose a desynchronization-based countermeasure that makes the timing analysis of activation functions harder. We analyze the timing properties of several activation functions and design the desynchronization in a way that the dependency on the input and the activation type is hidden. We experimentally verify the effectiveness of the countermeasure on a 32-bit ARM Cortex-M4 microcontroller and employ a t-test to show the side-channel information leakage. The overhead ultimately depends on the number of neurons in the fully-connected layer, for example, in the case of 4096
    
[^19]: AdvCheck：通过本地梯度检查表征对抗生成样本

    AdvCheck: Characterizing Adversarial Examples via Local Gradient Checking. (arXiv:2303.18131v1 [cs.CR])

    [http://arxiv.org/abs/2303.18131](http://arxiv.org/abs/2303.18131)

    本文提出了一种新的方法AdvCheck，通过计算本地梯度检测对抗性样本，相较于其他最先进的检测方法具有更高的效率和更好的表现。

    

    深度神经网络（DNN）容易受到对抗性样本攻击，在安全关键领域可能导致灾难。已经提出了各种检测方法来表征对抗生成样本的特征唯一性，或区分由对抗性样本触发的DNN的行为。基于特征的检测方法不能处理受到大扰动的对抗性样本，还需要大量的对抗性样本。另一个主流的基于模型的检测方法，通过模型行为表征输入属性，计算代价很高。为了解决这些问题，我们引入了本地梯度的概念，并揭示出对抗性样本的本地梯度较正常样本有更大的边界。我们受到这一观察的启发，利用本地梯度检测对抗性样本，并提出了一个通用的框架AdvCheck。具体地，通过从一些正常样本和添加噪声的杂项样本计算本地梯度，我们可以有效地区分对抗性样本和正常样本。我们进一步提出了一种名为AdvCheck-LIME的变体，通过引入局部性来处理本地梯度。广泛的实验在基准数据集上证明了我们提出的方法的优越性和效率，相较于其他最先进的检测方法。

    Deep neural networks (DNNs) are vulnerable to adversarial examples, which may lead to catastrophe in security-critical domains. Numerous detection methods are proposed to characterize the feature uniqueness of adversarial examples, or to distinguish DNN's behavior activated by the adversarial examples. Detections based on features cannot handle adversarial examples with large perturbations. Besides, they require a large amount of specific adversarial examples. Another mainstream, model-based detections, which characterize input properties by model behaviors, suffer from heavy computation cost. To address the issues, we introduce the concept of local gradient, and reveal that adversarial examples have a quite larger bound of local gradient than the benign ones. Inspired by the observation, we leverage local gradient for detecting adversarial examples, and propose a general framework AdvCheck. Specifically, by calculating the local gradient from a few benign examples and noise-added misc
    
[^20]: 一种用于深度平均K分类的双头损失函数

    A two-head loss function for deep Average-K classification. (arXiv:2303.18118v1 [cs.LG])

    [http://arxiv.org/abs/2303.18118](http://arxiv.org/abs/2303.18118)

    本文提出了一种双头损失函数用于平均K分类，其中第二个头被用于多标签分类，模型采用了软阈值训练以保证平均返回K个类，实验在两个数据集上证明其优于softmax基线。

    

    平均K分类是一种在输入图像不确定性影响下返回数量变化的K分类方法，但所有样本的返回结果需平均为K。解决这一问题的简单方法是用经过交叉熵损失训练的模型的softmax输出进行阈值处理。这种方法理论上证明渐近一致，但对于有限样本集合并不一定是最优选择。本文提出了一种新的损失函数，基于对经典softmax的多标签分类头而添加的第二个头。用软阈值对softmax的伪标签进行训练，保证平均返回K个类。我们展示了这种方法可以更好地捕捉类别之间的不确定性，并因此返回更一致的可能类别集。实验在两个已有数据集上证明了我们的方法胜过softmax基线。

    Average-K classification is an alternative to top-K classification in which the number of labels returned varies with the ambiguity of the input image but must average to K over all the samples. A simple method to solve this task is to threshold the softmax output of a model trained with the cross-entropy loss. This approach is theoretically proven to be asymptotically consistent, but it is not guaranteed to be optimal for a finite set of samples. In this paper, we propose a new loss function based on a multi-label classification head in addition to the classical softmax. This second head is trained using pseudo-labels generated by thresholding the softmax head while guaranteeing that K classes are returned on average. We show that this approach allows the model to better capture ambiguities between classes and, as a result, to return more consistent sets of possible classes. Experiments on two datasets from the literature demonstrate that our approach outperforms the softmax baseline,
    
[^21]: 爱丁堡国际英语口音语料库：迈向英语语音识别（ASR）的民主化

    The Edinburgh International Accents of English Corpus: Towards the Democratization of English ASR. (arXiv:2303.18110v1 [cs.CL])

    [http://arxiv.org/abs/2303.18110](http://arxiv.org/abs/2303.18110)

    爱丁堡国际英语口音语料库（EdAcc）发布，包括英语的广泛多样性和每个说话者的语言背景概况。在 EdAcc 上训练的最佳模型明显优于所有现有模型，这突显了当前英语ASR模型的缺点。这一数据集的公开共享将有助于民主化英语语音识别研究和开发。

    

    英语是世界上使用最广泛的语言，每天有数百万人使用英语作为第一或第二语言，在许多不同的语境中使用。因此，英语有许多变体。虽然过去几十年来英语自动语音识别（ASR）取得了许多进展，但通常基于测试数据集报告的结果未能代表今天全球使用的多样化英语。我们提出了首个爱丁堡国际英语口音语料库（EdAcc）的版本。此数据集试图更好地代表英语的广泛多样性，包括约40小时的朋友之间的二元视频通话。与其他数据集不同，EdAcc包括广泛的英语第一语言和第二语言变体以及每个人的语言背景概况。最新公共和商业模型的结果显示，EdAcc强调了当前英语ASR模型的缺点。在680个转录的EdAcc对话上训练的表现最佳的模型在这个数据集上明显优于所有现有模型。该数据集是公开的，我们希望它能为民主化英语ASR研究和开发做出贡献。

    English is the most widely spoken language in the world, used daily by millions of people as a first or second language in many different contexts. As a result, there are many varieties of English. Although the great many advances in English automatic speech recognition (ASR) over the past decades, results are usually reported based on test datasets which fail to represent the diversity of English as spoken today around the globe. We present the first release of The Edinburgh International Accents of English Corpus (EdAcc). This dataset attempts to better represent the wide diversity of English, encompassing almost 40 hours of dyadic video call conversations between friends. Unlike other datasets, EdAcc includes a wide range of first and second-language varieties of English and a linguistic background profile of each speaker. Results on latest public, and commercial models show that EdAcc highlights shortcomings of current English ASR models. The best performing model, trained on 680 t
    
[^22]: 地理空间机器学习的评估挑战

    Evaluation Challenges for Geospatial ML. (arXiv:2303.18087v1 [cs.LG])

    [http://arxiv.org/abs/2303.18087](http://arxiv.org/abs/2303.18087)

    本文论述了地理空间机器学习评估中的独特挑战，提出了改进地理空间模型性能评估的具体方法。

    

    随着地理空间机器学习模型和由其预测产生的地图在科学和政策的下游分析中越来越多地被使用，评估其准确性和适用性变得至关重要。地理空间机器学习与其他学习范例有着关键差异，因此，衡量空间机器学习输出性能的正确方法一直是争论的话题。本文阐述了全球或遥感数据下地理空间机器学习模型评估的独特挑战，并总结了改进地理空间模型性能评估的具体方法。

    As geospatial machine learning models and maps derived from their predictions are increasingly used for downstream analyses in science and policy, it is imperative to evaluate their accuracy and applicability. Geospatial machine learning has key distinctions from other learning paradigms, and as such, the correct way to measure performance of spatial machine learning outputs has been a topic of debate. In this paper, I delineate unique challenges of model evaluation for geospatial machine learning with global or remotely sensed datasets, culminating in concrete takeaways to improve evaluations of geospatial model performance.
    
[^23]: 解决形态学类比问题：从检索到生成

    Solving morphological analogies: from retrieval to generation. (arXiv:2303.18062v1 [cs.CL])

    [http://arxiv.org/abs/2303.18062](http://arxiv.org/abs/2303.18062)

    该论文提出了一个基于深度学习和条件变分自编码器的框架来解决基于类比的推理中的类比检测和解决两个任务，该框架可以生成之前不存在于数据集中的类比。

    

    类比推理是人类思维的一种非凡能力，并且已被用来解决难以理解的任务。 基于类比的推理（AR）受到了人工智能社区的越来越多的关注，并在多个机器学习任务中表现出其潜力，例如分类，决策和具有竞争性结果的推荐。 我们提出了一个基于深度学习（DL）的框架来解决AR中的两个关键任务：类比检测和解决。该框架在整个Siganalogies数据集上进行了全面测试，该数据集包含单词之间的形态学类比比例（APs），并且在许多语言中显示出优于符号方法的表现。 之前的工作已经探索了分类问题上的类比神经网络行为（ANNc）和检索问题上的类比神经网络行为（ANNr），以及自编码器（AE）在生成解决方案单词上的潜力。 在本文中，我们通过提出一个基于条件变分自编码器（CVAE）的统一框架来总结并扩展以前的工作，该框架可以共同解决两个任务。我们提出的框架可以生成在数据集中以前不存在的类比。

    Analogical inference is a remarkable capability of human reasoning, and has been used to solve hard reasoning tasks. Analogy based reasoning (AR) has gained increasing interest from the artificial intelligence community and has shown its potential in multiple machine learning tasks such as classification, decision making and recommendation with competitive results. We propose a deep learning (DL) framework to address and tackle two key tasks in AR: analogy detection and solving. The framework is thoroughly tested on the Siganalogies dataset of morphological analogical proportions (APs) between words, and shown to outperform symbolic approaches in many languages. Previous work have explored the behavior of the Analogy Neural Network for classification (ANNc) on analogy detection and of the Analogy Neural Network for retrieval (ANNr) on analogy solving by retrieval, as well as the potential of an autoencoder (AE) for analogy solving by generating the solution word. In this article we sum
    
[^24]: NOSTROMO: 教训、结论与未来方向

    NOSTROMO: Lessons learned, conclusions and way forward. (arXiv:2303.18060v1 [cs.LG])

    [http://arxiv.org/abs/2303.18060](http://arxiv.org/abs/2303.18060)

    本文介绍了元建模对于空中交通管理研究的价值，并强调了其在实现欧洲ATM总体计划中的关键绩效指标方面所起的作用。

    

    本白皮书旨在解释元建模对空中交通管理(ATM)研究的价值。它将定义元建模并探讨其能力和不能做到的事情。读者假定具有SESAR的基础知识：单一欧洲天空ATM研究项目。 SESAR的重要组成部分是带来改进，该项目是单一欧洲天空倡议的技术支柱，改进是通过特定关键绩效指标(KPIs)衡量的，并由所谓的SESAR“解决方案”系列来实施。这些“解决方案”是新的或改进的操作程序或技术，旨在满足欧洲ATM总体计划中描述的操作和性能改进。

    This White Paper sets out to explain the value that metamodelling can bring to air traffic management (ATM) research. It will define metamodelling and explore what it can, and cannot, do. The reader is assumed to have basic knowledge of SESAR: the Single European Sky ATM Research project. An important element of SESAR, as the technological pillar of the Single European Sky initiative, is to bring about improvements, as measured through specific key performance indicators (KPIs), and as implemented by a series of so-called SESAR 'Solutions'. These 'Solutions' are new or improved operational procedures or technologies, designed to meet operational and performance improvements described in the European ATM Master Plan.
    
[^25]: 从时间序列中推断网络结构的神经方法

    Inferring networks from time series: a neural approach. (arXiv:2303.18059v1 [cs.LG])

    [http://arxiv.org/abs/2303.18059](http://arxiv.org/abs/2303.18059)

    本论文提出了一种基于神经网络的快速计算方法，可以从时间序列数据中推断大型网络的相邻矩阵，并对不确定性进行量化，解决了网络推断问题的不足。

    

    网络结构是许多复杂现象的动态基础，包括基因调控、食物链、电力网络和社交媒体。然而，由于网络结构通常无法直接观测到，因此必须从其紧急动态的观测数据中推断它们的相互连接性。在本研究中，我们提出了一种快速计算方法，使用神经网络从时间序列数据中推断大型网络相邻矩阵。使用神经网络提供了预测的不确定性量化方法，反映了推断问题的非凸性和数据上的噪声。这是有用的，因为网络推断问题通常是欠定的，并且在网络推断方法中缺乏这个特征。我们通过从观测其响应断电的情况下推断英国电力网络的线路故障位置来展示我们的方法的能力。

    Network structures underlie the dynamics of many complex phenomena, from gene regulation and foodwebs to power grids and social media. Yet, as they often cannot be observed directly, their connectivities must be inferred from observations of their emergent dynamics. In this work we present a powerful and fast computational method to infer large network adjacency matrices from time series data using a neural network. Using a neural network provides uncertainty quantification on the prediction in a manner that reflects both the non-convexity of the inference problem as well as the noise on the data. This is useful since network inference problems are typically underdetermined, and a feature that has hitherto been lacking from network inference methods. We demonstrate our method's capabilities by inferring line failure locations in the British power grid from observations of its response to a power cut. Since the problem is underdetermined, many classical statistical tools (e.g. regressio
    
[^26]: 深度神经算子用于学习动态加载下交错相复合材料的瞬态响应

    Deep neural operator for learning transient response of interpenetrating phase composites subject to dynamic loading. (arXiv:2303.18055v1 [cs.LG])

    [http://arxiv.org/abs/2303.18055](http://arxiv.org/abs/2303.18055)

    本文提出了一种使用深度神经算子代理物理有限元分析模型，学习交错相复合材料在动态加载下的瞬态响应，加速其力学性能的物理预测。

    

    添加制造已被认为是制造业的技术革命，它允许直接从计算机辅助设计模型中制造具有复杂三维结构的材料。交错相复合材料（IPC）的力学性能，尤其是对动态加载的响应，高度依赖于它们的三维结构。为了加快对各种结构设计的IPC力学性能的物理预测，我们采用深度神经算子（DNO）来学习IPC在动态加载下的瞬态响应，作为基于物理的有限元分析（FEA）模型的代理。我们考虑了一个由两种Young模量比为2.7的金属组成的3D IPC梁，其中使用组分材料的随机块来展示它们的响应。

    Additive manufacturing has been recognized as an industrial technological revolution for manufacturing, which allows fabrication of materials with complex three-dimensional (3D) structures directly from computer-aided design models. The mechanical properties of interpenetrating phase composites (IPCs), especially response to dynamic loading, highly depend on their 3D structures. In general, for each specified structural design, it could take hours or days to perform either finite element analysis (FEA) or experiments to test the mechanical response of IPCs to a given dynamic load. To accelerate the physics-based prediction of mechanical properties of IPCs for various structural designs, we employ a deep neural operator (DNO) to learn the transient response of IPCs under dynamic loading as surrogate of physics-based FEA models. We consider a 3D IPC beam formed by two metals with a ratio of Young's modulus of 2.7, wherein random blocks of constituent materials are used to demonstrate the
    
[^27]: 差分隐私随机凸优化在（非）欧几里得空间中的再探讨

    Differentially Private Stochastic Convex Optimization in (Non)-Euclidean Space Revisited. (arXiv:2303.18047v1 [cs.LG])

    [http://arxiv.org/abs/2303.18047](http://arxiv.org/abs/2303.18047)

    本文重新探讨了在欧几里得空间和一般的$\ell_p^d$空间中进行差分隐私随机凸优化（DP-SCO）的问题。针对凸损失函数和强凸损失函数，提出了方法，其输出能够实现(预期)过量种群风险，这只取决于约束集合的高斯宽度，对强凸函数，界限是最优的。同时，提出了针对重尾数据进行DP-SCO的算法，并在$1<p<2$和$2≤p≤∞$的两种情况下，提供了第一个理论结果。

    

    本文重新探讨了在欧几里得空间和一般的$\ell_p^d$空间中进行差分隐私随机凸优化（DP-SCO）的问题。具体来说，我们关注三个仍未被充分理解的设置: (1) 在欧几里得空间限制且有界的（凸）集合上进行DP-SCO; (2) 在$\ell_p^d$空间上无限制的DP-SCO; (3) 在限制且有界的$\ell_p^d$空间中，使用重尾数据进行DP-SCO。对于问题（1），针对凸损失函数和强凸损失函数，我们提出了方法，其输出能够实现(预期)过量种群风险，这只取决于约束集合的高斯宽度，而不是空间维度。此外，我们还展示了对于强凸函数，界限是最优的，最多相差一个对数因子。对于问题（2）和（3），我们提出了几种新算法，并针对$1<p<2$和$2≤p≤∞$的两种情况，提供了第一个理论结果。

    In this paper, we revisit the problem of Differentially Private Stochastic Convex Optimization (DP-SCO) in Euclidean and general $\ell_p^d$ spaces. Specifically, we focus on three settings that are still far from well understood: (1) DP-SCO over a constrained and bounded (convex) set in Euclidean space; (2) unconstrained DP-SCO in $\ell_p^d$ space; (3) DP-SCO with heavy-tailed data over a constrained and bounded set in $\ell_p^d$ space. For problem (1), for both convex and strongly convex loss functions, we propose methods whose outputs could achieve (expected) excess population risks that are only dependent on the Gaussian width of the constraint set rather than the dimension of the space. Moreover, we also show the bound for strongly convex functions is optimal up to a logarithmic factor. For problems (2) and (3), we propose several novel algorithms and provide the first theoretical results for both cases when $1<p<2$ and $2\leq p\leq \infty$.
    
[^28]: 基于多个密度估计器的可扩展连接基数估计方法

    Scardina: Scalable Join Cardinality Estimation by Multiple Density Estimators. (arXiv:2303.18042v1 [cs.DB])

    [http://arxiv.org/abs/2303.18042](http://arxiv.org/abs/2303.18042)

    本篇论文提出了一种基于多个密度估计器的可扩展连接基数估计方法，可以精确估计具有强相关的大型和复杂模式的数据的基数，有望在查询优化器中得到应用。

    

    近年来，基于机器学习的基数估计方法正在取代传统方法。这一变化预计将有助于基数估计的最重要应用之一，即查询优化器，以加快查询处理速度。然而，当关系模式由许多表格组成，且表格/属性之间存在强相关性时，现有的方法没有精确估计基数。本文描述了多个密度估计器可以结合使用，有效地针对具有强相关的大型和复杂模式的数据进行基数估计。我们提出了Scardina，一种新的连接基数估计方法，该方法使用基于模式结构的多个分区模型。

    In recent years, machine learning-based cardinality estimation methods are replacing traditional methods. This change is expected to contribute to one of the most important applications of cardinality estimation, the query optimizer, to speed up query processing. However, none of the existing methods do not precisely estimate cardinalities when relational schemas consist of many tables with strong correlations between tables/attributes. This paper describes that multiple density estimators can be combined to effectively target the cardinality estimation of data with large and complex schemas having strong correlations. We propose Scardina, a new join cardinality estimation method using multiple partitioned models based on the schema structure.
    
[^29]: 交通标志识别数据集与数据增强

    Traffic Sign Recognition Dataset and Data Augmentation. (arXiv:2303.18037v1 [cs.CV])

    [http://arxiv.org/abs/2303.18037](http://arxiv.org/abs/2303.18037)

    本论文介绍了解决交通标志识别中数据集不足的问题的方法——TSR数据集增强，以及在TT100K数据集上实现的成果，可以显著提高特定标志类别的性能并实现最先进的性能。

    

    尽管有许多交通标志分类的数据集，但很少有收集用于交通标志识别的数据集，其中只有很少的数据集能够获取足够的实例来训练深度学习模型。 深度学习方法几乎是唯一的途径，可用于训练涵盖与传统方法（如通过颜色、形状等）相比高度相似类别的真实世界使用模型。同样，对于某些特定标志类别，它们的标志含义注定无法在数据集中获得足够的实例。为解决这个问题，我们提出了一种独特的交通标志识别数据集增强方法，利用了交通标志的标准，称之为TSR数据集增强。我们基于基准清华大学-腾讯100K（TT100K）数据集来验证该独特的数据增强方法。我们对基于TT100K数据集的四个主要迭代版本数据集执行该方法，实验结果表明我们的方法显著提高了特定标志类别的性能，并在TSR数据集上实现了最先进的性能。

    Although there are many datasets for traffic sign classification, there are few datasets collected for traffic sign recognition and few of them obtain enough instances especially for training a model with the deep learning method. The deep learning method is almost the only way to train a model for real-world usage that covers various highly similar classes compared with the traditional way such as through color, shape, etc. Also, for some certain sign classes, their sign meanings were destined to can't get enough instances in the dataset. To solve this problem, we purpose a unique data augmentation method for the traffic sign recognition dataset that takes advantage of the standard of the traffic sign. We called it TSR dataset augmentation. We based on the benchmark Tsinghua-Tencent 100K (TT100K) dataset to verify the unique data augmentation method. we performed the method on four main iteration version datasets based on the TT100K dataset and the experimental results showed our meth
    
[^30]: 简单的领域泛化方法是开放领域泛化的强大基准方法

    Simple Domain Generalization Methods are Strong Baselines for Open Domain Generalization. (arXiv:2303.18031v1 [cs.CV])

    [http://arxiv.org/abs/2303.18031](http://arxiv.org/abs/2303.18031)

    该论文评估了基于领域泛化的方法在开放领域泛化中的表现，证明了CORAL和MMD等简单DG方法在某些情况下的竞争力，提出了这些方法的简单扩展。

    

    在现实世界的应用中，机器学习模型需要处理开放集识别（OSR），即在推理过程中出现未知类别，以及领域漂移（domain shift），即训练和推理阶段之间数据分布不同的情况。领域泛化（DG）旨在处理推理阶段的目标领域在模型训练期间不可访问的情况下的领域漂移情况。开放领域泛化（ODG）同时考虑了DG和OSR。领域增强元学习（DAML）是一个面向ODG的方法，但其学习过程较为复杂。另一方面，尽管提出了各种DG方法，但它们尚未在ODG情况下进行评估。本文全面评估现有的DG方法在ODG中的表现，并展示了两种简单的DG方法，即CORrelation ALignment（CORAL）和Maximum Mean Discrepancy（MMD）在若干情况下与DAML具有竞争力。此外，我们通过引入一个小调整，提出了CORAL和MMD的简单扩展。

    In real-world applications, a machine learning model is required to handle an open-set recognition (OSR), where unknown classes appear during the inference, in addition to a domain shift, where the distribution of data differs between the training and inference phases. Domain generalization (DG) aims to handle the domain shift situation where the target domain of the inference phase is inaccessible during model training. Open domain generalization (ODG) takes into account both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a method targeting ODG but has a complicated learning process. On the other hand, although various DG methods have been proposed, they have not been evaluated in ODG situations. This work comprehensively evaluates existing DG methods in ODG and shows that two simple DG methods, CORrelation ALignment (CORAL) and Maximum Mean Discrepancy (MMD), are competitive with DAML in several cases. In addition, we propose simple extensions of CORAL and MMD by introducing th
    
[^31]: 深度学习快速预测实验室培养的组织机械特性

    Rapid prediction of lab-grown tissue properties using deep learning. (arXiv:2303.18017v1 [q-bio.TO])

    [http://arxiv.org/abs/2303.18017](http://arxiv.org/abs/2303.18017)

    该文介绍了使用深度学习技术快速预测实验室培养的细胞/hydrogels的机械特性的理论证明，减少了物理实验成本。

    

    细胞和细胞外基质之间的相互作用是组织自组织的关键。本文介绍了使用机器学习工具预测在固定模具中培养的细胞/hydrogels自组织中机械生物学作用的理论证明。我们通过CONTRACT network dipole orientation(CONDOR)模型在模具中模拟了6500个细胞/基质交互作用，并使用\texttt{pix2pix}深度学习模型中的实现来进行训练，将保留的740个未经训练的案例用于训练和验证。机器学习技术的预测结果与生物物理算法的保留预测之间的比较表明，机器学习模型可以高精度地快速预测细胞/hydrogels的机械特性，减少了耗时和昂贵的物理实验的需求。

    The interactions between cells and the extracellular matrix are vital for the self-organisation of tissues. In this paper we present proof-of-concept to use machine learning tools to predict the role of this mechanobiology in the self-organisation of cell-laden hydrogels grown in tethered moulds. We develop a process for the automated generation of mould designs with and without key symmetries. We create a large training set with $N=6500$ cases by running detailed biophysical simulations of cell-matrix interactions using the contractile network dipole orientation (CONDOR) model for the self-organisation of cellular hydrogels within these moulds. These are used to train an implementation of the \texttt{pix2pix} deep learning model, reserving $740$ cases that were unseen in the training of the neural network for training and validation. Comparison between the predictions of the machine learning technique and the reserved predictions from the biophysical algorithm show that the machine le
    
[^32]: 卵巢癌组织病理学中的人工智能：一项系统综述

    Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review. (arXiv:2303.18005v1 [eess.IV])

    [http://arxiv.org/abs/2303.18005](http://arxiv.org/abs/2303.18005)

    通过对36篇文章的综述，该研究发现人工智能模型在卵巢癌的诊断和预后中显示出有希望的结果，但现有研究受到小样本量，潜在偏见和缺乏外部验证的限制。

    

    目的-特征化和评估已发表的研究，评估利用组织病理学数据进行卵巢癌诊断或预后的人工智能（AI）方法的质量。方法-在2022年1月12日之前，对5个来源进行搜索。包括标准要求研究评估AI在卵巢癌的组织病理学图像上，对卵巢癌，包括输卵管卵巢和腹膜肿瘤的诊断或预后推断。排除评论和非英语文章。对每个包含的模型使用PROBAST评估偏倚风险。结果-共发现1434篇研究文章，其中36篇符合纳入标准。这些研究报告了62个感兴趣的模型，其中包括35个分类器，14个生存预测模型，7个分割模型和6个回归模型。使用1-1375张从1-664个卵巢癌患者中得到的幻灯片开发了这些模型。预测了广泛的结果，包括总体生存（9/62），组织学亚型（7/62）和淋巴结状态（6/62）。结论-基于可用的文献，AI模型在卵巢癌组织病理学的诊断和预后中显示出有希望的结果。但是，现有的研究受到样本量小、潜在的偏见和缺乏外部验证的限制。

    Purpose - To characterise and assess the quality of published research evaluating artificial intelligence (AI) methods for ovarian cancer diagnosis or prognosis using histopathology data. Methods - A search of 5 sources was conducted up to 01/12/2022. The inclusion criteria required that research evaluated AI on histopathology images for diagnostic or prognostic inferences in ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews and non-English language articles were excluded. The risk of bias was assessed for every included model using PROBAST. Results - A total of 1434 research articles were identified, of which 36 were eligible for inclusion. These studies reported 62 models of interest, including 35 classifiers, 14 survival prediction models, 7 segmentation models, and 6 regression models. Models were developed using 1-1375 slides from 1-664 ovarian cancer patients. A wide array of outcomes were predicted, including overall survival (9/62), histological subtypes (7
    
[^33]: 神经网络熵(NNetEn)：基于熵特征的脑电信号和混沌时间序列分离，用于NNetEn计算的Python包

    Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation. (arXiv:2303.17995v1 [cs.LG])

    [http://arxiv.org/abs/2303.17995](http://arxiv.org/abs/2303.17995)

    该研究提出了一种新的熵估计方法NNetEn，用于有效地分离时间序列系统的混沌动态，并在分离混沌时间序列方面证明了其高效率。

    

    熵测量是时间序列分类问题中有效的特征。传统的熵测量方法，例如香农熵，使用概率分布函数。然而，为了有效地分离时间序列，需要新的熵估计方法来表征系统的混沌动态。我们的神经网络熵(NNetEn)概念是基于特殊数据集(MNIST-10和SARS-CoV-2-RBV1)的分类，这些数据集与记录在LogNNet神经网络储层中的时间序列熵相关。NNetEn以原始方式估计时间序列的混沌动态。基于NNetEn算法，我们提出了两个新的分类度量：R2效率和皮尔逊效率。NNetEn的效率在使用离散分析(ANOVA)分离正弦映射的两个混沌时间序列方面得到验证。对于两个接近的动态时间序列 (r=1.1918和r=1.2243)，F比值达到了124的值，反映了高效率。

    Entropy measures are effective features for time series classification problems. Traditional entropy measures, such as Shannon entropy, use probability distribution function. However, for the effective separation of time series, new entropy estimation methods are required to characterize the chaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn) is based on the classification of special datasets (MNIST-10 and SARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in the reservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamics of time series in an original way. Based on the NNetEn algorithm, we propose two new classification metrics: R2 Efficiency and Pearson Efficiency. The efficiency of NNetEn is verified on separation of two chaotic time series of sine mapping using dispersion analysis (ANOVA). For two close dynamic time series (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124 and reflects high efficien
    
[^34]: 一种快速的非负矩阵分解乘法更新算法

    A fast Multiplicative Updates algorithm for Non-negative Matrix Factorization. (arXiv:2303.17992v1 [math.OC])

    [http://arxiv.org/abs/2303.17992](http://arxiv.org/abs/2303.17992)

    提出了一种快速的非负矩阵分解乘法更新算法，通过改进交替主体化最小化算法，实现了较快的求解和类似或更好的逼近精度结果。

    

    非负矩阵分解是一种重要的无监督机器学习工具，可以将数据矩阵分解为易于解释的部分。过去三十年中出现了许多算法，其中一种广为人知的方法是由李飞飞和才华横溢于2002年提出的乘法更新算法。该算法在许多领域表现良好，具有简单易实现和可适应流行变体的特点。本文建议通过为每个替代子问题制作更紧密的Hessian矩阵的上限来改进乘法更新算法，并将其视为交替主体化最小化算法。在合成数据和实际数据上实践中观察到，所提出的fastMU算法通常比原始的乘法更新算法快数倍，同时在逼近精度方面实现了类似或更好的结果，收敛仍然得到保证。

    Nonnegative Matrix Factorization is an important tool in unsupervised machine learning to decompose a data matrix into a product of parts that are often interpretable. Many algorithms have been proposed during the last three decades. A well-known method is the Multiplicative Updates algorithm proposed by Lee and Seung in 2002. Multiplicative updates have many interesting features: they are simple to implement and can be adapted to popular variants such as sparse Nonnegative Matrix Factorization, and, according to recent benchmarks, is state-of-the-art for many problems where the loss function is not the Frobenius norm. In this manuscript, we propose to improve the Multiplicative Updates algorithm seen as an alternating majorization minimization algorithm by crafting a tighter upper bound of the Hessian matrix for each alternate subproblem. Convergence is still ensured and we observe in practice on both synthetic and real world dataset that the proposed fastMU algorithm is often several
    
[^35]: 面向元宇宙的联邦学习：综述

    Federated Learning for Metaverse: A Survey. (arXiv:2303.17987v1 [cs.CR])

    [http://arxiv.org/abs/2303.17987](http://arxiv.org/abs/2303.17987)

    本文综述了早期FL4M的研究进展，并探讨了FL对于保护元宇宙参与者的数据隐私和降低服务器计算和存储需求的重要性。

    

    在元宇宙发展的过程中，数据采集和私人数据泄漏问题成为了制约其广泛应用的难题。联邦学习（FL）是一种分布式机器学习范式，具有隐私保护功能，专门设计用于大量边缘设备的训练任务。将FL应用于元宇宙不仅可以保护参与者的数据隐私，还可以减少服务器上高计算能力和高存储量的需求。本文回顾了针对FL4M的一些早期研究进展。

    The metaverse, which is at the stage of innovation and exploration, faces the dilemma of data collection and the problem of private data leakage in the process of development. This can seriously hinder the widespread deployment of the metaverse. Fortunately, federated learning (FL) is a solution to the above problems. FL is a distributed machine learning paradigm with privacy-preserving features designed for a large number of edge devices. Federated learning for metaverse (FL4M) will be a powerful tool. Because FL allows edge devices to participate in training tasks locally using their own data, computational power, and model-building capabilities. Applying FL to the metaverse not only protects the data privacy of participants but also reduces the need for high computing power and high memory on servers. Until now, there have been many studies about FL and the metaverse, respectively. In this paper, we review some of the early advances of FL4M, which will be a research direction with u
    
[^36]: 通过排序促进非合作行为

    Promoting Non-Cooperation Through Ordering. (arXiv:2303.17971v1 [cs.MA])

    [http://arxiv.org/abs/2303.17971](http://arxiv.org/abs/2303.17971)

    本文研究了如何通过排序方式，在处罚的个人中激励非合作行为，对于中央管理部门能够显著增加总付款。

    

    在很多现实情况下，例如大城市中的小交通违规事件，中央管理部门需要定期对大量个人进行惩罚。通常的做法是给每个个人一个机会，承担一小笔罚款，并保证免除可能会面临的较大处罚和法律程序。然而，由于违法者数量众多，中央管理部门能力有限，个人面临的风险通常很小，理性的个人将选择不支付罚款。本文展示了，如果中央管理部门按照已知的公开顺序处理违法者，就能适当地激励违法者支付罚款。我们在实验和理论分析中显示，我们的机制促进了非合作行为，激励个人支付罚款。而且，对于任意联盟也是适用的。我们量化了中央管理部门收到的预期总付款，并显示其显著增加。

    In many real world situations, like minor traffic offenses in big cities, a central authority is tasked with periodic administering punishments to a large number of individuals. Common practice is to give each individual a chance to suffer a smaller fine and be guaranteed to avoid the legal process with probable considerably larger punishment. However, thanks to the large number of offenders and a limited capacity of the central authority, the individual risk is typically small and a rational individual will not choose to pay the fine. Here we show that if the central authority processes the offenders in a publicly known order, it properly incentives the offenders to pay the fine. We show analytically and on realistic experiments that our mechanism promotes non-cooperation and incentives individuals to pay. Moreover, the same holds for an arbitrary coalition. We quantify the expected total payment the central authority receives, and show it increases considerably.
    
[^37]: HD-GCN: 一种混合扩散图卷积网络

    HD-GCN:A Hybrid Diffusion Graph Convolutional Network. (arXiv:2303.17966v1 [cs.LG])

    [http://arxiv.org/abs/2303.17966](http://arxiv.org/abs/2303.17966)

    介绍了一种名为HD-GCN的新框架，以解决邻接矩阵造成的信息扩散限制。利用扩散映射促进相邻节点之间的信息扩散，再利用图卷积进一步传播信息。通过使用扩散映射获得的扩散距离进行正则化和约束。

    

    GCN及其变种模型的信息扩散性能受邻接矩阵限制，这可能会降低它们的性能。因此，我们介绍了一种名为混合扩散图卷积网络（HD-GCN）的新框架，以解决邻接矩阵造成的信息扩散限制。在HD-GCN框架中，我们首先利用扩散映射来促进在特征空间中相邻节点之间的信息扩散。这允许在没有相邻关系的情况下，在相似点之间进行信息扩散。接下来，我们在扩散映射之后利用图卷积进一步传播相邻节点之间的信息，从而实现在图中相邻相似节点之间的信息传播。最后，我们采用通过使用扩散映射获得的扩散距离对训练标签的预测进行正则化和约束。

    The information diffusion performance of GCN and its variant models is limited by the adjacency matrix, which can lower their performance. Therefore, we introduce a new framework for graph convolutional networks called Hybrid Diffusion-based Graph Convolutional Network (HD-GCN) to address the limitations of information diffusion caused by the adjacency matrix. In the HD-GCN framework, we initially utilize diffusion maps to facilitate the diffusion of information among nodes that are adjacent to each other in the feature space. This allows for the diffusion of information between similar points that may not have an adjacent relationship. Next, we utilize graph convolution to further propagate information among adjacent nodes after the diffusion maps, thereby enabling the spread of information among similar nodes that are adjacent in the graph. Finally, we employ the diffusion distances obtained through the use of diffusion maps to regularize and constrain the predicted labels of trainin
    
[^38]: 面向未知具有潜在状态系统的学习优化控制方法

    Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])

    [http://arxiv.org/abs/2303.17963](http://arxiv.org/abs/2303.17963)

    本文提出了一种面向未知具有潜在状态系统的学习优化控制方法，并给出了概率性能保证，同时提出了一种验证任意控制律性能的方法。

    

    随着控制工程方法应用于越来越复杂的系统，数据驱动的系统辨识方法成为物理建模的有希望的替代方法。然而，许多这些方法依赖于状态测量的可用性，而复杂系统的状态通常不是直接可测量的。因此，可能需要同时估计动力学和潜在状态，从而更加具有挑战性地设计具有性能保证的控制器。本文提出了一种新方法，用于计算具有潜在状态的未知非线性系统的最优输入轨迹。对结果输入轨迹进行了概率性能保证，并提出了一种验证任意控制律性能的方法。本文在数值模拟中展示了所提出方法的有效性。

    As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
    
[^39]: CT系列中多器官分割的集成方法

    Ensemble Methods for Multi-Organ Segmentation in CT Series. (arXiv:2303.17956v1 [eess.IV])

    [http://arxiv.org/abs/2303.17956](http://arxiv.org/abs/2303.17956)

    本论文提出了利用单器官模型进行集成，解决数据稀缺限制下放射治疗器官风险分割的问题，并获得了令人兴奋的结果。

    

    在医学图像领域，语义分割是由医生执行的最重要、最困难和最耗时的任务之一。由于计算机视觉中深度学习模型的最近进展，自动化这种任务的承诺变得越来越现实。然而，仍然需要解决许多问题，例如数据的稀缺可用性以及高度专业化模型的效率难以扩展到一般场景。放射治疗计划的风险器官分割属于此类问题，因为有限的数据可用性会对开发通用模型的可能性产生负面影响。在这项工作中，我们专注于通过提出三种类型的单器官模型集成来解决这个问题，这些模型能够利用其组件的不同专业技能产生多器官掩模。所获得的结果是令人兴奋的，并证明了这是一种可能的解决方案，可找到有效的方法在CT系列中执行多器官分割。

    In the medical images field, semantic segmentation is one of the most important, yet difficult and time-consuming tasks to be performed by physicians. Thanks to the recent advancement in the Deep Learning models regarding Computer Vision, the promise to automate this kind of task is getting more and more realistic. However, many problems are still to be solved, like the scarce availability of data and the difficulty to extend the efficiency of highly specialised models to general scenarios. Organs at risk segmentation for radiotherapy treatment planning falls in this category, as the limited data available negatively affects the possibility to develop general-purpose models; in this work, we focus on the possibility to solve this problem by presenting three types of ensembles of single-organ models able to produce multi-organ masks exploiting the different specialisations of their components. The results obtained are promising and prove that this is a possible solution to finding effic
    
[^40]: FP8和INT8在高效深度学习推理中的比较

    FP8 versus INT8 for efficient deep learning inference. (arXiv:2303.17951v1 [cs.LG])

    [http://arxiv.org/abs/2303.17951](http://arxiv.org/abs/2303.17951)

    本文比较了FP8和INT8在设备高效推理中的性能，展示了量化和量化感知训练的成果，为选择正确的数字格式提供了参考。

    

    最近，使用FP8作为神经网络训练的数字格式的想法在深度学习世界中流传。鉴于目前大部分训练都是使用完整网络的FP32或者有时使用混合精度的FP16进行的，部分网络使用8位重量级的FP8可以加快深度学习中通常耗时昂贵的训练过程。这引发了人们对于此发展对于边缘设备高效推理的影响的自然问题。在高效推理设备中，工作负载通常在INT8中执行。有时为了保证效率，甚至低至INT4。在本文中，我们比较了FP8和INT格式在设备高效推理中的性能。我们理论上展示了神经网络中INT和FP格式之间的差异，并提供了大量的训练后量化和训练时量化结果来展示如何在不同格式下实现高效推理。

    Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how 
    
[^41]: 一种基于GAN的无监督机器音频异常检测与定位方法

    Unsupervised Anomaly Detection and Localization of Machine Audio: A GAN-based Approach. (arXiv:2303.17949v1 [cs.SD])

    [http://arxiv.org/abs/2303.17949](http://arxiv.org/abs/2303.17949)

    本文提出了一种基于生成对抗网络的无监督机器音频异常检测和定位方法AEGAN-AD，并在DCASE 2022 Challenge任务2数据集上获得了最先进的结果。

    

    机器学习中的自动异常检测仍然面临挑战。本文认为生成对抗网络（GAN）的能力适用于机器音频异常检测，并提出了一种完全无监督的方法AEGAN-AD，其中生成器（也是一个自编码器）被训练用于重构输入频谱图，而鉴别器经过重新设计，以在训练和检测阶段期间辅助生成器。在DCASE 2022 Challenge TASK 2数据集上的表现表明，AEGAN-AD在五种机器类型上获得了最先进的结果。作者还研究了一种新的异常定位方法。

    Automatic detection of machine anomaly remains challenging for machine learning. We believe the capability of generative adversarial network (GAN) suits the need of machine audio anomaly detection, yet rarely has this been investigated by previous work. In this paper, we propose AEGAN-AD, a totally unsupervised approach in which the generator (also an autoencoder) is trained to reconstruct input spectrograms. It is pointed out that the denoising nature of reconstruction deprecates its capacity. Thus, the discriminator is redesigned to aid the generator during both training stage and detection stage. The performance of AEGAN-AD on the dataset of DCASE 2022 Challenge TASK 2 demonstrates the state-of-the-art result on five machine types. A novel anomaly localization method is also investigated. Source code available at: www.github.com/jianganbai/AEGAN-AD
    
[^42]: 图像分类任务中的FedAvg和FedCurv基准测试

    Benchmarking FedAvg and FedCurv for Image Classification Tasks. (arXiv:2303.17942v1 [cs.LG])

    [http://arxiv.org/abs/2303.17942](http://arxiv.org/abs/2303.17942)

    本研究分析了在图像分类任务中使用的FedAvg和FedCurv联邦学习算法。联邦学习可以解决隐私问题，但异质系统和非IID数据仍然是挑战。

    

    经典的机器学习技术需要在单个数据湖中训练数据。然而，聚合来自不同所有者的数据并不总是方便的，原因包括安全、隐私和保密。数据具有一定的价值，与他人共享时可能会消失；避免共享数据的能力使得工业应用成为可能，其中安全和隐私至关重要，只需实现本地策略即可训练全局模型，这些策略可以独立运行甚至在离线数据中心运行。联邦学习（FL）是一种分布式机器学习方法，通过仅共享本地AI模型并保持数据分散来解决隐私问题，已成为解决隐私问题的有效方法。联邦学习的两个重要挑战是管理同一联合网络中的异构系统并处理实际数据，这些数据在客户端之间通常不是独立和同分布的（non-IID）。

    Classic Machine Learning techniques require training on data available in a single data lake. However, aggregating data from different owners is not always convenient for different reasons, including security, privacy and secrecy. Data carry a value that might vanish when shared with others; the ability to avoid sharing the data enables industrial applications where security and privacy are of paramount importance, making it possible to train global models by implementing only local policies which can be run independently and even on air-gapped data centres. Federated Learning (FL) is a distributed machine learning approach which has emerged as an effective way to address privacy concerns by only sharing local AI models while keeping the data decentralized. Two critical challenges of Federated Learning are managing the heterogeneous systems in the same federated network and dealing with real data, which are often not independently and identically distributed (non-IID) among the clients
    
[^43]: 对比对抗学习和监督学习在CT图像中器官风险分割方面的表现

    Comparing Adversarial and Supervised Learning for Organs at Risk Segmentation in CT images. (arXiv:2303.17941v1 [eess.IV])

    [http://arxiv.org/abs/2303.17941](http://arxiv.org/abs/2303.17941)

    本文比较了使用GAN和监督学习方法进行CT扫描中器官风险（OAR）分割的表现。研究结果表明，所提出的基于GAN的方法与基于CNN的方法相似或优于其对应的CNN模型，尤其是在分割更具挑战性的目标时。

    

    CT扫描中器官风险（OAR）分割是放疗治疗流程的关键组成部分。近年来，深度学习技术在自动化这个过程中表现出了非凡的潜力。本文探讨了生成对抗网络（GANs）与监督学习方法在分割OARs方面的表现。我们提出了三种基于GAN的模型，这些模型具有相同的生成器结构，但具有不同的鉴别器网络。使用包含50个已注释CT扫描的StructSeg数据集比较了这些模型与良好的CNN模型，如SE-ResUnet和DeepLabV3，这些扫描包含六个OAR的轮廓。我们的工作旨在提供有关在OAR分割环境中采用对抗性训练的优缺点的见解。结果非常有希望，表明所提出的基于GAN的方法与基于CNN的方法相似或优于其对应的CNN模型，尤其是在分割更具挑战性的目标时。

    Organ at Risk (OAR) segmentation from CT scans is a key component of the radiotherapy treatment workflow. In recent years, deep learning techniques have shown remarkable potential in automating this process. In this paper, we investigate the performance of Generative Adversarial Networks (GANs) compared to supervised learning approaches for segmenting OARs from CT images. We propose three GAN-based models with identical generator architectures but different discriminator networks. These models are compared with well-established CNN models, such as SE-ResUnet and DeepLabV3, using the StructSeg dataset, which consists of 50 annotated CT scans containing contours of six OARs. Our work aims to provide insight into the advantages and disadvantages of adversarial training in the context of OAR segmentation. The results are very promising and show that the proposed GAN-based approaches are similar or superior to their CNN-based counterparts, particularly when segmenting more challenging targe
    
[^44]: 每个示例的梯度正则化改进了从嘈杂数据中学习的信号

    Per-Example Gradient Regularization Improves Learning Signals from Noisy Data. (arXiv:2303.17940v1 [stat.ML])

    [http://arxiv.org/abs/2303.17940](http://arxiv.org/abs/2303.17940)

    本文研究了每个示例的梯度正则化 (PEGR) 技术，证明其可以有效地学习信号并抑制噪音，从而提高测试误差和抗噪声扰动能力。

    

    本文研究了每个示例的梯度正则化 (PEGR) 技术，并通过理论分析证明了其在提高测试误差和抗噪声扰动方面的有效性。具体来说，我们采用了 \citet {cao2022benign} 的信号噪音数据模型，并展示了 PEGR 可以有效地学习信号并抑制噪音。与传统的梯度下降方法不同，PEGR 可以区分信号和噪音，从而提高泛化性能。我们的分析揭示了 PEGR 惩罚模式学习的方差，从而有效地抑制了从训练数据中记忆噪声。

    Gradient regularization, as described in \citet{barrett2021implicit}, is a highly effective technique for promoting flat minima during gradient descent. Empirical evidence suggests that this regularization technique can significantly enhance the robustness of deep learning models against noisy perturbations, while also reducing test error. In this paper, we explore the per-example gradient regularization (PEGR) and present a theoretical analysis that demonstrates its effectiveness in improving both test error and robustness against noise perturbations. Specifically, we adopt a signal-noise data model from \citet{cao2022benign} and show that PEGR can learn signals effectively while suppressing noise. In contrast, standard gradient descent struggles to distinguish the signal from the noise, leading to suboptimal generalization performance. Our analysis reveals that PEGR penalizes the variance of pattern learning, thus effectively suppressing the memorization of noises from the training d
    
[^45]: 冲突回避的集成渐变优化，实现有效的离线基于模型的优化

    Conflict-Averse Gradient Optimization of Ensembles for Effective Offline Model-Based Optimization. (arXiv:2303.17934v1 [cs.LG])

    [http://arxiv.org/abs/2303.17934](http://arxiv.org/abs/2303.17934)

    本文提出了一种冲突回避的集成渐变优化方法，使用集成保证不受分布变化的影响，并使用辅助目标识别集成中无效梯度的区域，并自适应调整这些梯度的权重，以避免探索不可行的设计区域。实验结果表明，该方法在离线基于模型的优化中显着提高了性能。

    

    数据驱动的基于模型的离线优化是解决黑盒计算设计问题的实际方法，因为真实的目标函数未知且昂贵且难以查询。然而，标准方法优化设计与基础真实目标的学习代理模型可能会受到分布变化的影响。特别是在高维设计空间中，有效设计位于狭窄流形上，标准方法易于产生“误导”代理模型输出高值的越界无效设计。使用集合而不是单个模型作为代理模型有助于缓解分布的变化，但是从集合中组合梯度信息的朴素公式（例如最小和平均梯度）仍然是次优的并且经常受到不收敛的影响。在本文中，我们探索了从集合中组合梯度信息的替代方法，这些方法对分布变化有鲁棒性，利用最近的鲁棒优化和不确定性评估来提高性能。我们的方法涉及使用辅助“清洗”目标来识别设计空间的区域，其中集合可能产生无效梯度，然后自适应地减小优化期间的这些梯度的权重，以避免探索不可行的区域。我们在几个合成和实际的设计问题上展示了我们的方法的有效性，与标准的基于模型的离线优化方法相比，在优化性能方面获得了显着的改进。

    Data-driven offline model-based optimization (MBO) is an established practical approach to black-box computational design problems for which the true objective function is unknown and expensive to query. However, the standard approach which optimizes designs against a learned proxy model of the ground truth objective can suffer from distributional shift. Specifically, in high-dimensional design spaces where valid designs lie on a narrow manifold, the standard approach is susceptible to producing out-of-distribution, invalid designs that "fool" the learned proxy model into outputting a high value. Using an ensemble rather than a single model as the learned proxy can help mitigate distribution shift, but naive formulations for combining gradient information from the ensemble, such as minimum or mean gradient, are still suboptimal and often hampered by non-convergent behavior.  In this work, we explore alternate approaches for combining gradient information from the ensemble that are robu
    
[^46]: 基于学习的观测器在动力学自行车模型上的应用

    Learning-based Observer Evaluated on the Kinematic Bicycle Model. (arXiv:2303.17933v1 [cs.RO])

    [http://arxiv.org/abs/2303.17933](http://arxiv.org/abs/2303.17933)

    本文提出了一种基于学习的观测器方法，能够在处理非线性、不确定、模型误差等问题方面胜过经典观测器。该方法在动力学自行车模型上已得到评价和应用。

    

    车辆状态的知识是进行适当规划和控制的必要条件。这些量通常可以通过测量获得。控制理论提供了极其有用的方法-观测器-用于处理不能直接测量或具有噪声测量的数量。经典观察器是从模型中数学推导出来的。尽管它们成功地应用在如卡尔曼滤波器之类的情况下，但在系统表现出高度非线性、建模误差、高度不确定性或与环境交互困难（例如道路接触）时，它们表现出其局限性。在本文中，我们提出了一种建立在学习基础上的观测器方法，能够胜过经典的观测方法。我们比较了几种神经网络架构，并定义了用于训练它们的数据生成过程。该方法在动力学自行车模型上得到评价，这可以方便地生成用于训练和测试的数据。这个模型也被用于扩展卡尔曼滤波器（EKF）进行比较。

    The knowledge of the states of a vehicle is a necessity to perform proper planning and control. These quantities are usually accessible through measurements. Control theory brings extremely useful methods -- observers -- to deal with quantities that cannot be directly measured or with noisy measurements. Classical observers are mathematically derived from models. In spite of their success, such as the Kalman filter, they show their limits when systems display high non-linearities, modeling errors, high uncertainties or difficult interactions with the environment (e.g. road contact). In this work, we present a method to build a learning-based observer able to outperform classical observing methods. We compare several neural network architectures and define the data generation procedure used to train them. The method is evaluated on a kinematic bicycle model which allows to easily generate data for training and testing. This model is also used in an Extended Kalman Filter (EKF) for compa
    
[^47]: 超越多层感知器：研究神经网络中的复杂拓扑结构

    Beyond Multilayer Perceptrons: Investigating Complex Topologies in Neural Networks. (arXiv:2303.17925v1 [cs.NE])

    [http://arxiv.org/abs/2303.17925](http://arxiv.org/abs/2303.17925)

    本研究探索了神经网络的复杂拓扑对其近似能力的影响，发现在高难度情况下，复杂拓扑相比于传统的多层感知器表现更优异，但代价是前向传播计算时间的增加和图形损伤的鲁棒性的降低。

    

    在本研究中，我们探讨了神经网络的网络拓扑对其近似能力的影响，特别是复杂网络拓扑结构。我们提出了一种基于各种拓扑结构构建复杂神经网络的新方法，包括Barabasi-Albert，Erdos-Renyi，Watts-Strogatz和多层感知器（MLP）。我们使用从流形学习生成器产生的合成数据集对构建的网络进行评估，其中难度和噪声水平各不相同。我们的发现表明，与传统的MLP相比，复杂拓扑结构在高难度情况下表现优异。这种性能优势归因于复杂网络利用底层目标函数的复合性能力。然而，这种好处是以前向传播计算时间的增加和图形损伤的鲁棒性的降低为代价的。此外，我们还研究了各种拓扑属性之间的关系。

    In this study, we explore the impact of network topology on the approximation capabilities of artificial neural networks (ANNs), with a particular focus on complex topologies. We propose a novel methodology for constructing complex ANNs based on various topologies, including Barab\'asi-Albert, Erd\H{o}s-R\'enyi, Watts-Strogatz, and multilayer perceptrons (MLPs). The constructed networks are evaluated on synthetic datasets generated from manifold learning generators, with varying levels of task difficulty and noise. Our findings reveal that complex topologies lead to superior performance in high-difficulty regimes compared to traditional MLPs. This performance advantage is attributed to the ability of complex networks to exploit the compositionality of the underlying target function. However, this benefit comes at the cost of increased forward-pass computation time and reduced robustness to graph damage. Additionally, we investigate the relationship between various topological attribute
    
[^48]: 面向全沉浸多用户虚拟现实技术的预测上下文感知和重定向步行

    Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2303.17907v1 [cs.NI])

    [http://arxiv.org/abs/2303.17907](http://arxiv.org/abs/2303.17907)

    本文提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向，实现面向全沉浸多用户虚拟现实技术的高效通信。

    

    虚拟现实技术正朝着增强沉浸感、支持多用户体验和在虚拟体验中支持无限制的移动，而通过重定向步行将用户限制在专门的VR设置内。为了满足未来VR系统的极端数据速率和延迟要求，支持无线网络基础设施将在毫米波（mmWave）频率上运行，并通过波束成形和波束导向实现高度定向的通信。我们提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向。具体而言，我们认为通过短期预测多用户VR设置中用户的横向移动，可以利用用户方向上的直线视距（LoS）“跟踪”来优化发射端的波束成形和波束导向。

    Virtual Reality (VR) technology is being advanced along the lines of enhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and supporting unconstrained mobility of the users in their VEs, while constraining them within specialized VR setups through Redirected Walking (RDW). For meeting the extreme data-rate and latency requirements of future VR systems, supporting wireless networking infrastructures will operate in millimeter Wave (mmWave) frequencies and leverage highly directional communication in both transmission and reception through beamforming and beamsteering. We propose to leverage predictive context-awareness for optimizing transmitter and receiver-side beamforming and beamsteering. In particular, we argue that short-term prediction of users' lateral movements in multiuser VR setups with RDW can be utilized for optimizing transmitter-side beamforming and beamsteering through Line-of-Sight (LoS) "tracking" in the users' directions. At the same time, short-
    
[^49]: 利用 Nesterov 动量和分布式主成分分析来加速无线联邦学习

    Accelerating Wireless Federated Learning via Nesterov's Momentum and Distributed Principle Component Analysis. (arXiv:2303.17885v1 [cs.LG])

    [http://arxiv.org/abs/2303.17885](http://arxiv.org/abs/2303.17885)

    本文提出了利用一次性分布式主成分分析（PCA）和 Nesterov 动量来加速无线联邦学习的算法，缓解了从工人到服务器的上传链路成为通信瓶颈的问题。PCA-AWFL 算法经过理论证明比 PCA-WFL 算法更快地收敛。数值实验验证了所提出算法的有效性。

    

    本文研究了一种无线联邦学习系统，通过正交无线通道允许服务器和工人交换未编码信息。由于工人频繁通过带宽受限通道向服务器上传本地梯度，因此从工人到服务器的上传链路成为一个通信瓶颈。因此，利用一次性分布式主成分分析（PCA）来减小上传梯度的维度，从而缓解通信瓶颈。基于低维梯度和 Nesterov 动量提出了基于 PCA 的无线联邦学习（PCA-WFL）算法及其加速版本（即 PCA-AWFL）。对于非凸损失函数，进行了有限时间分析，量化了系统超参数对 PCA-WFL 和 PCA-AWFL 算法收敛的影响。PCA-AWFL 算法经过理论证明比 PCA-WFL 算法更快地收敛。此外，考虑无线信道衰落和随机工人模型的共存，数值实验验证了所提出算法的有效性。

    A wireless federated learning system is investigated by allowing a server and workers to exchange uncoded information via orthogonal wireless channels. Since the workers frequently upload local gradients to the server via bandwidth-limited channels, the uplink transmission from the workers to the server becomes a communication bottleneck. Therefore, a one-shot distributed principle component analysis (PCA) is leveraged to reduce the dimension of uploaded gradients such that the communication bottleneck is relieved. A PCA-based wireless federated learning (PCA-WFL) algorithm and its accelerated version (i.e., PCA-AWFL) are proposed based on the low-dimensional gradients and the Nesterov's momentum. For the non-convex loss functions, a finite-time analysis is performed to quantify the impacts of system hyper-parameters on the convergence of the PCA-WFL and PCA-AWFL algorithms. The PCA-AWFL algorithm is theoretically certified to converge faster than the PCA-WFL algorithm. Besides, the co
    
[^50]: 嵌入式深度神经网络中的融合深度分块瓦片技术提高内存利用率

    Fused Depthwise Tiling for Memory Optimization in TinyML Deep Neural Network Inference. (arXiv:2303.17878v1 [cs.LG])

    [http://arxiv.org/abs/2303.17878](http://arxiv.org/abs/2303.17878)

    本文提出了一种新的融合深度分块瓦片（FDT）方法，可以在不引入任何运行时开销的情况下降低DNN推理的内存使用量，进一步提高了在嵌入式设备上进行的TinyML模型的内存利用率。

    

    随着微型、低功耗控制器的出现，深度神经网络（DNN）推理的内存优化变得越来越重要。在这样的小型设备上运行DNN推理任务，如音频关键词检测或基于雷达的手势识别，由于DNN推理需要大量的中间运行时缓冲区来存储激活和其他中间数据，从而导致内存使用量高度限制。本文提出了一种新的融合深度分块瓦片（FDT）方法，用于DNN的内存优化。与现有的分块方法相比，FDT可以在不引入任何运行时开销的情况下降低内存使用量，并适用于比现有分块方法更多种类的网络层。FDT显著提高了TinyML的内存优化，减少了以前无法减少内存的模型，并为模型的替代设计提供了其他设计方案。

    Memory optimization for deep neural network (DNN) inference gains high relevance with the emergence of TinyML, which refers to the deployment of DNN inference tasks on tiny, low-power microcontrollers. Applications such as audio keyword detection or radar-based gesture recognition are heavily constrained by the limited memory on such tiny devices because DNN inference requires large intermediate run-time buffers to store activations and other intermediate data, which leads to high memory usage. In this paper, we propose a new Fused Depthwise Tiling (FDT) method for the memory optimization of DNNs, which, compared to existing tiling methods, reduces memory usage without inducing any run time overhead. FDT applies to a larger variety of network layers than existing tiling methods that focus on convolutions. It improves TinyML memory optimization significantly by reducing memory of models where this was not possible before and additionally providing alternative design points for models th
    
[^51]: CAP-VSTNet: 内容相关性保留的多功能风格转换

    CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer. (arXiv:2303.17867v1 [cs.CV])

    [http://arxiv.org/abs/2303.17867](http://arxiv.org/abs/2303.17867)

    本文提出了一种全新的CAP-VSTNet框架，它保留了内容相关性，解决了多功能风格转换中的问题，并在定性和定量方面达到了更好的结果。

    

    内容相关性的损失问题一直是引起照片逼真及视频风格转化中产生伪影的主要原因。本文提出了一种名为CAP-VSTNet的全新框架，它由全新的可逆残差网络和无偏的线性转换模块组成，用于实现多功能风格转换。这种可逆残差网络不仅能够保留内容相关性，而且不会像传统的可逆网络一样引入冗余信息，从而实现更好的风格化效果。在解决由线性转换引起的像素相关性损失问题的Matting Laplacian训练损失的支持下，所提出的框架在多功能风格转换中具有适用性和有效性。广泛的实验结果表明，CAP-VSTNet与现有的最先进方法相比，能够产生更好的定性和定量结果。

    Content affinity loss including feature and pixel affinity is a main problem which leads to artifacts in photorealistic and video style transfer. This paper proposes a new framework named CAP-VSTNet, which consists of a new reversible residual network and an unbiased linear transform module, for versatile style transfer. This reversible residual network can not only preserve content affinity but not introduce redundant information as traditional reversible networks, and hence facilitate better stylization. Empowered by Matting Laplacian training loss which can address the pixel affinity loss problem led by the linear transform, the proposed framework is applicable and effective on versatile style transfer. Extensive experiments show that CAP-VSTNet can produce better qualitative and quantitative results in comparison with the state-of-the-art methods.
    
[^52]: 一种针对点云数据的新颖协变量流形学习方法：极大协方差展开回归

    Maximum Covariance Unfolding Regression: A Novel Covariate-based Manifold Learning Approach for Point Cloud Data. (arXiv:2303.17852v1 [cs.LG])

    [http://arxiv.org/abs/2303.17852](http://arxiv.org/abs/2303.17852)

    本文提出了一种针对非结构化点云数据的新的协变量流形学习方法，用于建模和流程优化。本方法能够学习与解释性协变量相关性最高的点云的低维流形。

    

    点云数据被广泛应用于制造业的过程检测、建模、监控和优化。目前的张量回归技术已经有效地用于结构化点云数据的分析，其中在均匀网格上的测量可以形成一个张量。但是，这些技术不适用于常常形成流形的非结构化点云数据。因此，本文提出了一种非线性降维方法，名为极大协方差展开回归，能够学习具有最高与解释性协变量相关性的点云的低维流形。然后，这个低维流形用于基于过程变量的回归建模和流程优化。最后，通过模拟和钢制支架制造的案例研究评估了所提出的方法的性能并与基准方法进行了比较。

    Point cloud data are widely used in manufacturing applications for process inspection, modeling, monitoring and optimization. The state-of-art tensor regression techniques have effectively been used for analysis of structured point cloud data, where the measurements on a uniform grid can be formed into a tensor. However, these techniques are not capable of handling unstructured point cloud data that are often in the form of manifolds. In this paper, we propose a nonlinear dimension reduction approach named Maximum Covariance Unfolding Regression that is able to learn the low-dimensional (LD) manifold of point clouds with the highest correlation with explanatory covariates. This LD manifold is then used for regression modeling and process optimization based on process variables. The performance of the proposed method is subsequently evaluated and compared with benchmark methods through simulations and a case study of steel bracket manufacturing.
    
[^53]: 一种针对弱监督学习的基准生成性概率模型

    A Benchmark Generative Probabilistic Model for Weak Supervised Learning. (arXiv:2303.17841v1 [cs.LG])

    [http://arxiv.org/abs/2303.17841](http://arxiv.org/abs/2303.17841)

    本文提出一种基准生成性概率模型，在启发式标注的原始数据集上训练，生成伪标签作为一种准确、快速、经济的弱监督学习方法，在图像分类和自然语言处理中达到了最先进的表现。

    

    寻找相关高质量的数据集来训练机器学习模型对于实践者来说是一个主要 bottleneck。而且，为了解决野心勃勃实际应用场景下的问题，数据通常需要附带带有高质量注释的标签，以便于监督模型的训练。手动标记具有高质量标签的数据通常是一项耗时且具有挑战性的任务，往往成为机器学习项目的瓶颈。弱监督学习 (WSL) 方法已被开发出来，通过根据启发式、远程监视和知识库来赋予未标记数据大约标签 (伪标签) 的自动方式，从而减轻注释负担。我们应用概率生成隐变量模型 (PLVMs)，在启发式标注表示的原始数据集上进行训练，作为一种生成伪标签的准确、快速、经济的方式。我们展示了 PLVMs 在图像分类中的多个基准数据集上实现了最先进的表现，并展示了它们在自然语言处理中的事件检测任务上的多才多艺。

    Finding relevant and high-quality datasets to train machine learning models is a major bottleneck for practitioners. Furthermore, to address ambitious real-world use-cases there is usually the requirement that the data come labelled with high-quality annotations that can facilitate the training of a supervised model. Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project. Weak Supervised Learning (WSL) approaches have been developed to alleviate the annotation burden by offering an automatic way of assigning approximate labels (pseudo-labels) to unlabelled data based on heuristics, distant supervision and knowledge bases. We apply probabilistic generative latent variable models (PLVMs), trained on heuristic labelling representations of the original dataset, as an accurate, fast and cost-effective way to generate pseudo-labels. We show that the PLVMs achieve state-of-
    
[^54]: 从教学视频及其解说中学习过程感知的视频表示

    Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations. (arXiv:2303.17839v1 [cs.CV])

    [http://arxiv.org/abs/2303.17839](http://arxiv.org/abs/2303.17839)

    本文提出了一种从教学视频及其解说中学习过程感知的视频表示方法，联合学习视频表示和深度概率模型可以增强过程推理的新功能，同时对个体步骤的识别也能得到加强。

    

    互联网上教学视频及其解说的丰富资源为理解过程活动提供了令人兴奋的途径。本文提出了一种学习视频表示的方法，该表示对基于大规模网络教学视频及其解说的个体步骤及其时间顺序进行编码，而不使用人工注释。方法联合学习视频表示和深度概率模型，以捕获步骤的时间依赖关系和巨大个体差异。实验证明，学习时间排序不仅能够增强过程推理的新功能，还可以加强对个体步骤的识别。我们的模型在步骤分类（在COIN/EPIC-Kitchens上分别增加2.8% / 3.3%）和步骤预测（在COIN上增加7.4%）方面显著提高了最先进的结果。此外，我们的模型在步骤提取的零样本推理方面取得了有希望的结果。

    The abundance of instructional videos and their narrations over the Internet offers an exciting avenue for understanding procedural activities. In this work, we propose to learn video representation that encodes both action steps and their temporal ordering, based on a large-scale dataset of web instructional videos and their narrations, without using human annotations. Our method jointly learns a video representation to encode individual step concepts, and a deep probabilistic model to capture both temporal dependencies and immense individual variations in the step ordering. We empirically demonstrate that learning temporal ordering not only enables new capabilities for procedure reasoning, but also reinforces the recognition of individual steps. Our model significantly advances the state-of-the-art results on step classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting (+7.4% on COIN). Moreover, our model attains promising results in zero-shot inference for step c
    
[^55]: 重新思考解释：深度视觉分类器的无特定输入显著性映射

    Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers. (arXiv:2303.17836v1 [cs.CV])

    [http://arxiv.org/abs/2303.17836](http://arxiv.org/abs/2303.17836)

    提出了一种新的无特定输入显著性映射视角，它计算了模型对其输出所归属的高级特征，这种方法能够独立于输入进行模型解释，且鲁棒性较好。

    

    显著性方法通过将输入特征归属于模型输出，提供事后的模型解释。 当前的方法主要使用单个输入样本来实现这一点，因此无法回答有关模型的独立于输入的查询。 我们还表明，特定于输入的显著性映射本质上容易受到误导性特征归属的影响。试图使用“通用”输入特征来进行模型解释的现有尝试假定可以访问包含这些特征的数据集，这会导致解释的偏差。针对这一差距，我们提出了一种新的无特定输入显著性映射视角，该方法计算了模型对其输出所归属的高级特征。 这些特征是几何相关的，并通过积累模型相对于无限制数据分布的梯度信息来计算。 为了计算这些特征，我们将独立的数据点沿着人类可理解标签相关联的局部最小值推向模型损失面。 所提出的方法提供了一种新的解释深度分类器的方式，不依赖于特定输入信息，并且经过检验，在输入变化、噪声和对抗性攻击方面都很鲁棒。

    Saliency methods provide post-hoc model interpretation by attributing input features to the model outputs. Current methods mainly achieve this using a single input sample, thereby failing to answer input-independent inquiries about the model. We also show that input-specific saliency mapping is intrinsically susceptible to misleading feature attribution. Current attempts to use 'general' input features for model interpretation assume access to a dataset containing those features, which biases the interpretation. Addressing the gap, we introduce a new perspective of input-agnostic saliency mapping that computationally estimates the high-level features attributed by the model to its outputs. These features are geometrically correlated, and are computed by accumulating model's gradient information with respect to an unrestricted data distribution. To compute these features, we nudge independent data points over the model loss surface towards the local minima associated by a human-understa
    
[^56]: 隐式模板化ODE-nets的实现和(反转修正)误差分析

    Implementation and (Inverse Modified) Error Analysis for implicitly-templated ODE-nets. (arXiv:2303.17824v1 [math.NA])

    [http://arxiv.org/abs/2303.17824](http://arxiv.org/abs/2303.17824)

    本文重点研究使用隐式数值初值问题求解器模板的ODE-nets。使用展开的隐式方案对ODE-nets进行训练可以返回一个接近于反转修正微分方程(IMDE)的近似值，并且我们可以使用自适应算法加速训练并保持精度。

    

    本文着重研究使用基于隐式数值初值问题求解器模板的ODE-nets来学习数据中的隐含动力学。首先，我们使用展开的隐式方案对ODE-nets进行反转修正误差分析以方便解释。结果表明，使用展开的隐式方案对ODE-nets进行训练返回了一个接近于反转修正微分方程(IMDE)的近似值。此外，我们建立了针对训练此类ODE-nets进行超参数选择的理论基础，而当前的策略通常将ODE-nets的数值积分视为黑匣子。因此，我们制定了一种自适应算法，在训练过程中监测误差级别并调整(展开的)隐式解法的迭代次数，以使展开的近似误差小于当前的学习损失。这有助于加速训练，同时保持精度。我们进行了多个数值实验以展示该方法的优越性。

    We focus on learning hidden dynamics from data using ODE-nets templated on implicit numerical initial value problem solvers. First, we perform Inverse Modified error analysis of the ODE-nets using unrolled implicit schemes for ease of interpretation. It is shown that training an ODE-net using an unrolled implicit scheme returns a close approximation of an Inverse Modified Differential Equation (IMDE). In addition, we establish a theoretical basis for hyper-parameter selection when training such ODE-nets, whereas current strategies usually treat numerical integration of ODE-nets as a black box. We thus formulate an adaptive algorithm which monitors the level of error and adapts the number of (unrolled) implicit solution iterations during the training process, so that the error of the unrolled approximation is less than the current learning loss. This helps accelerate training, while maintaining accuracy. Several numerical experiments are performed to demonstrate the advantages of the pr
    
[^57]: 一种基于可解释神经网络的连续回应有序回归非比例赔率模型

    An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])

    [http://arxiv.org/abs/2303.17823](http://arxiv.org/abs/2303.17823)

    本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。

    

    本文提出了一种基于可解释神经网络的非比例赔率模型（N$^3$POM) 用于有序回归，其中反应变量不仅可以取离散值，也可以取连续值，而回归系数根据预测顺序反应也不同。与传统方法直接从离散反应估计线性系数不同，我们训练了一个非线性的神经网络，通过以反应为输入产生线性系数。由于神经网络的优势，N$^3$POM可以在保留传统有序回归的可解释性的同时具有灵活性。我们给出了充分的条件，使得在指定的用户区域内，预测的条件累积概率（CCP）满足局部单调性约束。我们还提供了一种保持单调性的随机（MPS）算法来充分训练神经网络。

    This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
    
[^58]: 一种用于连续时间LQR问题的高效离策略强化学习算法

    An Efficient Off-Policy Reinforcement Learning Algorithm for the Continuous-Time LQR Problem. (arXiv:2303.17819v1 [eess.SY])

    [http://arxiv.org/abs/2303.17819](http://arxiv.org/abs/2303.17819)

    本文提出了一种高效的离策略强化学习算法，使用特定的持续激励输入作为探索信号来解决连续时间LQR问题，并证明了算法在每次迭代中可收敛到最优控制输入，同时提高了求解的效率。

    

    本文设计了一种离策略强化学习算法，仅利用从系统测量到的输入状态数据来解决连续时间LQR问题。不同于文献中的其他算法，我们提出在数据收集步骤中使用特定的持续激励输入作为探索信号。我们随后展示，利用这种持续激励的数据，我们算法中矩阵方程的解保证每次迭代都存在且唯一。同时，算法收敛到最优控制输入也得到了证明。此外，我们将策略评估步骤制定为一个Sylvester转置方程的解，从而提高了其求解的效率。最后，我们提出了一种仅使用测量数据来确定稳定策略以初始化算法的方法。

    In this paper, an off-policy reinforcement learning algorithm is designed to solve the continuous-time LQR problem using only input-state data measured from the system. Different from other algorithms in the literature, we propose the use of a specific persistently exciting input as the exploration signal during the data collection step. We then show that, using this persistently excited data, the solution of the matrix equation in our algorithm is guaranteed to exist and to be unique at every iteration. Convergence of the algorithm to the optimal control input is also proven. Moreover, we formulate the policy evaluation step as the solution of a Sylvester-transpose equation, which increases the efficiency of its solution. Finally, a method to determine a stabilizing policy to initialize the algorithm using only measured data is proposed.
    
[^59]: 探索大型语言模型在传统韩医中的潜力：基于基础模型的文化适应保健方法

    Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare. (arXiv:2303.17807v1 [cs.CL])

    [http://arxiv.org/abs/2303.17807](http://arxiv.org/abs/2303.17807)

    本研究评估了大型语言模型在应用传统韩医的潜力。其中，GPT-4在应用韩国国家中医医生执照考试中取得了57.29%的准确率，潜在应用价值高。

    

    传统韩医注重个体化诊断和治疗，数据有限且过程隐性，使AI建模困难。GPT-3.5和GPT-4等大型语言模型尽管缺乏医学专业培训，但已显示出出色的医疗知识。本研究旨在评估GPT-3.5和GPT-4在应用韩国国家中医医生执照考试中的潜力。结果显示，GPT-3.5和GPT-4分别取得了42.06%和57.29%的准确率，其中GPT-4接近及格水平。

    Introduction: Traditional Korean medicine (TKM) emphasizes individualized diagnosis and treatment, making AI modeling difficult due to limited data and implicit processes. GPT-3.5 and GPT-4, large language models, have shown impressive medical knowledge despite lacking medicine-specific training. This study aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using the Korean National Licensing Examination for Korean Medicine Doctors. Methods: GPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questions from the 2022 examination across 12 subjects. Each question was independently evaluated five times in an initialized session. Results: GPT-3.5 and GPT-4 achieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passing performance. There were significant differences in accuracy by subjects, with 83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine (2). Both models showed high accuracy in recall-based and diagnosis-based questi
    
[^60]: 关于初始化的影响：2层神经网络的缩放路径

    On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks. (arXiv:2303.17805v1 [cs.LG])

    [http://arxiv.org/abs/2303.17805](http://arxiv.org/abs/2303.17805)

    本文研究了具有不同尺度的非零权重的无限宽2层ReLU神经网络的正则化路径，展示该问题具有一个无限维度的凸对应，随着初始化的尺度在0到+∞范围内变化，关联路径在所谓的内核和丰富的区域之间连续插值。

    

    在监督学习中，正则化路径有时被用作梯度下降的优化路径的方便理论代理。本文研究了具有不同尺度的非零权重的无限宽2层ReLU神经网络的正则化路径的修改。通过利用与不平衡最优输运理论的联系，我们表明，尽管2层网络训练的非凸性，这个问题具有一个无限维度的凸对应。我们制定了相应的函数优化问题并调查了其主要特性。特别地，我们展示了随着初始化的尺度在0到+∞范围内变化，关联路径在所谓的内核和丰富的区域之间连续插值。数值实验证实，在我们的设置中，缩放路径和优化路径的最终状态行为类似，甚至超越了这些范围。

    In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized with zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with non-zero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite dimensional convex counterpart. We formulate the corresponding functional optimization problem and investigate its main properties. In particular, we show that as the scale of the initialization ranges between $0$ and $+\infty$, the associated path interpolates continuously between the so-called kernel and rich regimes. The numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly even beyond these ex
    
[^61]: 基于信号子空间差异子空间的时间序列异常检测方法

    Time-series Anomaly Detection based on Difference Subspace between Signal Subspaces. (arXiv:2303.17802v1 [cs.LG])

    [http://arxiv.org/abs/2303.17802](http://arxiv.org/abs/2303.17802)

    本文提出了一种新的基于信号子空间差异子空间的时间序列异常检测方法，通过捕获两个子空间的完整结构差异来提高性能，在公共时间序列数据集上证明了其有效性。

    

    本文提出了一种基于奇异谱分析(SSA)和差异子空间概念的新型时间序列异常检测方法。其关键思想是通过监控过去和现在时间序列数据所对应的两个信号子空间之间轻微的时间变化来作为异常评分。这是传统SSA方法的自然推广，传统方法测量的是两个信号子空间之间的最小角度来表征变化的程度。通过用差异子空间替代最小角度，我们的方法在使用SSA框架的同时提高了性能，因为它可以捕捉到两个子空间之间的完整结构差异。我们通过对公共时间序列数据集的性能评估证明了方法的有效性。

    This paper proposes a new method for anomaly detection in time-series data by incorporating the concept of difference subspace into the singular spectrum analysis (SSA). The key idea is to monitor slight temporal variations of the difference subspace between two signal subspaces corresponding to the past and present time-series data, as anomaly score. It is a natural generalization of the conventional SSA-based method which measures the minimum angle between the two signal subspaces as the degree of changes. By replacing the minimum angle with the difference subspace, our method boosts the performance while using the SSA-based framework as it can capture the whole structural difference between the two subspaces in its magnitude and direction. We demonstrate our method's effectiveness through performance evaluations on public time-series datasets.
    
[^62]: 一种用于短期交通流量预测的关注缓慢变化的机器学习方法

    A Slow-Shifting Concerned Machine Learning Method for Short-term Traffic Flow Forecasting. (arXiv:2303.17782v1 [cs.LG])

    [http://arxiv.org/abs/2303.17782](http://arxiv.org/abs/2303.17782)

    该论文提出了一种关注缓慢变化的机器学习方法来解决交通流量预测中的时间漂移问题，该方法包括经验模态分解和长短期记忆网络等步骤，可缓解交通流量数据的非平稳性，提高交通流量预测的准确性。

    

    预测拥挤区域的交通流量变化对当局制定缓解拥堵的措施或安排基础设施建设至关重要。然而，交通流量预测面临一个关键挑战，即每日和每周峰值之间缓慢变化的时间漂移，导致交通流量信号的非平稳性，进而导致准确预测的困难。为解决这一挑战，我们提出了一种关注缓慢变化的机器学习方法来预测交通流量，其包括两个部分。首先，我们利用经验模态分解作为特征工程，以减轻交通流量数据的非平稳性，得到一系列平稳分量。其次，由于长短期记忆网络在捕捉时间特征方面的优越性，我们以平稳分量为特征训练一个先进的交通流量预测模型。

    The ability to predict traffic flow over time for crowded areas during rush hours is increasingly important as it can help authorities make informed decisions for congestion mitigation or scheduling of infrastructure development in an area. However, a crucial challenge in traffic flow forecasting is the slow shifting in temporal peaks between daily and weekly cycles, resulting in the nonstationarity of the traffic flow signal and leading to difficulty in accurate forecasting. To address this challenge, we propose a slow shifting concerned machine learning method for traffic flow forecasting, which includes two parts. First, we take advantage of Empirical Mode Decomposition as the feature engineering to alleviate the nonstationarity of traffic flow data, yielding a series of stationary components. Second, due to the superiority of Long-Short-Term-Memory networks in capturing temporal features, an advanced traffic flow forecasting model is developed by taking the stationary components as
    
[^63]: 从二维投影图像中学习三维变换的内部表示

    Learning Internal Representations of 3D Transformations from 2D Projected Inputs. (arXiv:2303.17776v1 [q-bio.NC])

    [http://arxiv.org/abs/2303.17776](http://arxiv.org/abs/2303.17776)

    该论文提出了一种基于生成流形模型的计算模型，通过对移动的二维投影点进行深度推断，学习了三维旋转变换，为了理解生物视觉系统如何内部表示三维变换提供了思路。

    

    在三维世界中交互时，人类必须从投影到二维视网膜图像中估计三维结构。研究表明，人类使用物体形状在运动引起的转换中的持久性作为提示来解决深度模糊问题。为了了解生物视觉系统如何内部表示三维变换，我们提出了一种基于生成流形模型的计算模型，该模型可以从二维点的运动中推断三维结构。我们的模型还可以学习变换的表示，提供了一个概念验证，说明人类如何在发育或演化的时间尺度上发展内部表示。我们的模型集中于旋转运动，展示了我们的模型如何从移动的二维投影点中推断深度，从2D训练刺激中学习三维旋转变换，并与人的心理表现进行比较。

    When interacting in a three dimensional world, humans must estimate 3D structure from visual inputs projected down to two dimensional retinal images. It has been shown that humans use the persistence of object shape over motion-induced transformations as a cue to resolve depth ambiguity when solving this underconstrained problem. With the aim of understanding how biological vision systems may internally represent 3D transformations, we propose a computational model, based on a generative manifold model, which can be used to infer 3D structure from the motion of 2D points. Our model can also learn representations of the transformations with minimal supervision, providing a proof of concept for how humans may develop internal representations on a developmental or evolutionary time scale. Focused on rotational motion, we show how our model infers depth from moving 2D projected points, learns 3D rotational transformations from 2D training stimuli, and compares to human performance on psych
    
[^64]: 爆炉分类器设计中融合领域知识

    Domain Knowledge integrated for Blast Furnace Classifier Design. (arXiv:2303.17769v1 [cs.LG])

    [http://arxiv.org/abs/2303.17769](http://arxiv.org/abs/2303.17769)

    本文设计了一种融合领域知识的分类模型框架，生成适用于工业应用的分类器，有效解决了安全和能源等不同学习目标下爆炉复杂系统的问题。

    

    爆炉建模和控制是工业领域中的重要问题，黑匣子模型是描述复杂爆炉系统的有效手段。为了满足工业应用中的安全和节能等不同学习目标，本文提出了一种框架，用于设计融合领域知识的分类模型，生成适用于工业应用的分类器。我们的知识融入学习方案允许用户更正确地创建一个分类器，以识别“重要样本”（其错误分类可能导致严重后果），同时保持恰当的分类精度。所提出的方法的有效性已经通过两个真实的爆炉数据集得到验证，这将指导操作人员更好地利用他们的先前经验来控制爆炉系统。

    Blast furnace modeling and control is one of the important problems in the industrial field, and the black-box model is an effective mean to describe the complex blast furnace system. In practice, there are often different learning targets, such as safety and energy saving in industrial applications, depending on the application. For this reason, this paper proposes a framework to design a domain knowledge integrated classification model that yields a classifier for industrial application. Our knowledge incorporated learning scheme allows the users to create a classifier that identifies "important samples" (whose misclassifications can lead to severe consequences) more correctly, while keeping the proper precision of classifying the remaining samples. The effectiveness of the proposed method has been verified by two real blast furnace datasets, which guides the operators to utilize their prior experience for controlling the blast furnace systems better.
    
[^65]: 可扩展的贝叶斯元学习：基于广义隐式梯度方法

    Scalable Bayesian Meta-Learning through Generalized Implicit Gradients. (arXiv:2303.17768v1 [cs.LG])

    [http://arxiv.org/abs/2303.17768](http://arxiv.org/abs/2303.17768)

    本研究提出了一种新颖的隐式贝叶斯元学习方法(iBaML)，通过将隐式梯度的优势应用到概率贝叶斯元学习中，显著缓解了可扩展性的瓶颈，并量化了相关的不确定性，具备良好的复杂性控制。

    

    元学习在处理数据有限的新兴任务时具有独特的效率和速度。将其视为双层优化问题揭示出它的广泛适用性。然而，当内层优化依赖于基于梯度的迭代时，其算法视角面临可扩展性问题。本研究通过将隐式梯度的优势应用到概率贝叶斯元学习中，显著缓解了可扩展性的瓶颈。新颖的隐式贝叶斯元学习（iBaML）方法不仅扩展了可学习先验的范围，还量化了相关的不确定性。此外，无论内层优化轨迹如何，最终复杂性均得到良好控制。分析误差界定证明了该方法的精确性。

    Meta-learning owns unique effectiveness and swiftness in tackling emerging tasks with limited data. Its broad applicability is revealed by viewing it as a bi-level optimization problem. The resultant algorithmic viewpoint however, faces scalability issues when the inner-level optimization relies on gradient-based iterations. Implicit differentiation has been considered to alleviate this challenge, but it is restricted to an isotropic Gaussian prior, and only favors deterministic meta-learning approaches. This work markedly mitigates the scalability bottleneck by cross-fertilizing the benefits of implicit differentiation to probabilistic Bayesian meta-learning. The novel implicit Bayesian meta-learning (iBaML) method not only broadens the scope of learnable priors, but also quantifies the associated uncertainty. Furthermore, the ultimate complexity is well controlled regardless of the inner-level optimization trajectory. Analytical error bounds are established to demonstrate the precisi
    
[^66]: 学习相似的线性表示：适应性、极小化、以及稳健性

    Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])

    [http://arxiv.org/abs/2303.17765](http://arxiv.org/abs/2303.17765)

    本文提出了两种算法，适应相似性结构并对异常值任务具有稳健性，适用于表示多任务学习和迁移学习设置。

    

    表示多任务学习和迁移学习在实践中取得了巨大的成功，然而对这些方法的理论理解仍然欠缺。本文旨在理解从具有相似但并非完全相同的线性表示的任务中学习，同时处理异常值任务。我们提出了两种算法，适应相似性结构并对异常值任务具有稳健性，适用于表示多任务学习和迁移学习设置，我们的算法在单任务或仅目标学习时表现优异。

    Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \textit{adaptive} to the similarity structure and \textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when
    
[^67]: 朝向对抗鲁棒的持续学习

    Towards Adversarially Robust Continual Learning. (arXiv:2303.17764v1 [cs.LG])

    [http://arxiv.org/abs/2303.17764](http://arxiv.org/abs/2303.17764)

    该论文是关于在持续学习中提高对抗鲁棒性的研究，首次提出一种新方法“任务感知边界增强（TABA）”，并在CIFAR-10和CIFAR-100上进行了充分的实验验证其有效性。

    

    最近的研究表明，经过持续学习训练的模型可以达到与标准监督学习相当的性能，并且持续学习模型的学习灵活性使得它们在实际应用中具有广泛的应用前景。然而，深度学习模型显示出对抗攻击的弱点。虽然在标准监督学习的情况下有许多关于模型鲁棒性的研究，但保护持续学习免受对抗攻击尚未受到研究。为了填补这一研究空白，我们是首次研究持续学习中的对抗鲁棒性，并提出一种名为任务感知边界增强（Task-Aware Boundary Augmentation，TABA）的新方法来提高持续学习模型的鲁棒性。通过在CIFAR-10和CIFAR-100上进行全面的实验，我们展示了对抗训练和TABA在防御对抗攻击方面的有效性。

    Recent studies show that models trained by continual learning can achieve the comparable performances as the standard supervised learning and the learning flexibility of continual learning models enables their wide applications in the real world. Deep learning models, however, are shown to be vulnerable to adversarial attacks. Though there are many studies on the model robustness in the context of standard supervised learning, protecting continual learning from adversarial attacks has not yet been investigated. To fill in this research gap, we are the first to study adversarial robustness in continual learning and propose a novel method called \textbf{T}ask-\textbf{A}ware \textbf{B}oundary \textbf{A}ugmentation (TABA) to boost the robustness of continual learning models. With extensive experiments on CIFAR-10 and CIFAR-100, we show the efficacy of adversarial training and TABA in defending adversarial attacks.
    
[^68]: CAMEL: 用于“心智”探索大规模语言模型社群的交互式代理

    CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])

    [http://arxiv.org/abs/2303.17760](http://arxiv.org/abs/2303.17760)

    本文介绍了一个名为角色扮演的新型交互式代理框架，用于实现语言模型之间的自主合作，并展示了其在生成对话数据方面的有效性。

    

    对话式语言模型的快速发展已取得了在复杂任务解决方面的显著进展。然而，它们的成功在很大程度上依赖于人类的指导，以引导对话，这可能是具有挑战性和耗时的。本文探讨了构建可扩展技术以促进交互式代理之间的自主合作并深入了解它们的“认知”过程的潜力。为了解决实现自主合作的挑战，我们提出了一个名为角色扮演的新型交互式代理框架。我们的方法涉及使用启动提示来引导聊天代理完成任务，同时保持与人类意图的一致性。我们展示了如何使用角色扮演来生成对话数据，以研究聊天代理的行为和能力，为研究对话式语言模型提供了有价值的资源。我们的贡献是介绍了一种新型的交互式代理框架，名为角色扮演，用于实现语言模型之间的自主合作，并展示了其在生成对话数据方面的有效性。

    The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framewor
    
[^69]: L2范数下的非线性回归注释

    A Note On Nonlinear Regression Under L2 Loss. (arXiv:2303.17745v1 [cs.LG])

    [http://arxiv.org/abs/2303.17745](http://arxiv.org/abs/2303.17745)

    研究者们发现对于传统的最小二乘问题存在一个可以使用凸非线性回归模型解决的方法，使得设计更复杂的系统和更易于训练的模型成为可能。

    

    我们研究了L2范数（平方损失）函数下的非线性回归问题。传统的非线性回归模型通常导致参数集的非凸优化问题。我们展示了一个针对传统最小二乘问题的凸非线性回归模型，可能有助于设计更复杂的系统和更易于训练的模型。

    We investigate the nonlinear regression problem under L2 loss (square loss) functions. Traditional nonlinear regression models often result in non-convex optimization problems with respect to the parameter set. We show that a convex nonlinear regression model exists for the traditional least squares problem, which can be a promising towards designing more complex systems with easier to train models.
    
[^70]: 最优输入增益：超级前馈神经网络所需的全部。

    Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network. (arXiv:2303.17732v1 [cs.LG])

    [http://arxiv.org/abs/2303.17732](http://arxiv.org/abs/2303.17732)

    通过优化输入增益，可以显著提高前馈神经网络的性能，特别是在使用反向传播和隐藏权重优化等算法时。

    

    输入的线性转换改变了等效的前馈网络的训练性能。然而，大多数线性变换被视为与实际训练分离的预处理操作。从等效网络开始，通过线性转换对输入进行预处理等效于在每次训练迭代中将负梯度矩阵与自相关矩阵相乘。提出了一个二阶方法，用于找到在给定迭代中最大化学习的自相关矩阵。当自相关矩阵为对角线矩阵时，该方法优化了输入增益。该最优输入增益（OIG）方法用于改进两个一阶二级训练算法，即反向传播（BP）和隐藏权重优化（HWO），这两种算法交替更新输入权重并解线性方程以确定输出权重。结果证明，所提出的OIG方法极大地提高了第一顺序算法的性能。

    Linear transformation of the inputs alters the training performance of feed-forward networks that are otherwise equivalent. However, most linear transforms are viewed as a pre-processing operation separate from the actual training. Starting from equivalent networks, it is shown that pre-processing inputs using linear transformation are equivalent to multiplying the negative gradient matrix with an autocorrelation matrix per training iteration. Second order method is proposed to find the autocorrelation matrix that maximizes learning in a given iteration. When the autocorrelation matrix is diagonal, the method optimizes input gains. This optimal input gain (OIG) approach is used to improve two first-order two-stage training algorithms, namely back-propagation (BP) and hidden weight optimization (HWO), which alternately update the input weights and solve linear equations for output weights. Results show that the proposed OIG approach greatly enhances the performance of the first-order al
    
[^71]: β4-IRT：一种具有增强判别力估计的新β3-IRT。

    $\beta^{4}$-IRT: A New $\beta^{3}$-IRT with Enhanced Discrimination Estimation. (arXiv:2303.17731v1 [cs.LG])

    [http://arxiv.org/abs/2303.17731](http://arxiv.org/abs/2303.17731)

    本文提出了一种新的名称为β4-IRT的项目反应理论模型，通过使用梯度下降方法估计模型参数来增强判别估计，解决了β3-IRT的对称性问题。

    

    项目反应理论旨在从不同难度等级的项目中推断出受试者未观察到的能力和特征。针对不同类型的任务，如二进制或概率响应、反应时间、多重响应等，已经提出了几种项目反应理论模型。本文提出了一个新版本的β3-IRT，称为β4-IRT，该版本使用梯度下降方法来估计模型参数。在β3-IRT中，能力和难度受到限制，因此我们采用链接函数将β4-IRT转变为无约束的梯度下降过程。原始的β3-IRT存在对称问题，即如果某个项目的判别力值符号错误（例如负值，而实际上是正的），拟合过程可能无法恢复该项目的正确判别力和难度值。

    Item response theory aims to estimate respondent's latent skills from their responses in tests composed of items with different levels of difficulty. Several models of item response theory have been proposed for different types of tasks, such as binary or probabilistic responses, response time, multiple responses, among others. In this paper, we propose a new version of $\beta^3$-IRT, called $\beta^{4}$-IRT, which uses the gradient descent method to estimate the model parameters. In $\beta^3$-IRT, abilities and difficulties are bounded, thus we employ link functions in order to turn $\beta^{4}$-IRT into an unconstrained gradient descent process. The original $\beta^3$-IRT had a symmetry problem, meaning that, if an item was initialised with a discrimination value with the wrong sign, e.g. negative when the actual discrimination should be positive, the fitting process could be unable to recover the correct discrimination and difficulty values for the item. In order to tackle this limita
    
[^72]: BOLT：一种用于在普通CPU硬件上训练和部署大规模神经网络的自动化深度学习框架。

    BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware. (arXiv:2303.17727v1 [cs.LG])

    [http://arxiv.org/abs/2303.17727](http://arxiv.org/abs/2303.17727)

    BOLT是一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库，它提供了一个灵活的高级API，使用户可以构建模型并抽象掉稀疏网络训练的算法细节。

    

    有效地在普通CPU硬件上进行大规模神经网络的训练和推理对于民主化深度学习能力具有巨大的实际意义。目前，由数十亿个参数组成的大规模模型的训练过程需要广泛使用专用硬件加速器（例如GPU），这些加速器仅限于少数具有相当财务资源的机构。此外，训练和部署这些模型通常会带来惊人的碳足迹。在本文中，我们通过引入BOLT，一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库来解决这些挑战。BOLT提供了一个灵活的高级API，用于构建模型，该API对于现有流行的深度学习框架的用户来说是熟悉的。通过自动调整专用超参数，BOLT也抽象掉了稀疏网络训练的算法细节。

    Efficient large-scale neural network training and inference on commodity CPU hardware is of immense practical significance in democratizing deep learning (DL) capabilities. Presently, the process of training massive models consisting of hundreds of millions to billions of parameters requires the extensive use of specialized hardware accelerators, such as GPUs, which are only accessible to a limited number of institutions with considerable financial resources. Moreover, there is often an alarming carbon footprint associated with training and deploying these models. In this paper, we address these challenges by introducing BOLT, a sparse deep learning library for training massive neural network models on standard CPU hardware. BOLT provides a flexible, high-level API for constructing models that will be familiar to users of existing popular DL frameworks. By automatically tuning specialized hyperparameters, BOLT also abstracts away the algorithmic details of sparse network training. We e
    
[^73]: 在小批量生成对抗样本可能会损害对抗性鲁棒性

    Generating Adversarial Samples in Mini-Batches May Be Detrimental To Adversarial Robustness. (arXiv:2303.17720v1 [cs.LG])

    [http://arxiv.org/abs/2303.17720](http://arxiv.org/abs/2303.17720)

    本文探究小批量在生成对抗样本中的作用，发现小批量大小的增加会导致生成的对抗样本效果下降，进一步提醒人们低估真实的对抗攻击强度并高估模型的鲁棒性。

    

    神经网络在计算机视觉中的效果得到了证明，但同时也容易受到对抗性攻击的影响。因此，随着神经网络的使用增加，对抗性攻击带来的威胁也随之增加。本文通过探索在对抗性样本生成过程中使用的小批量大小与生成的对抗性样本强度之间的关系，来解决对抗性鲁棒性的挑战。我们证明了小批量大小的增加会导致生成的对抗性样本效果的下降，并将这些观察结果与梯度消失现象联系起来。接下来，我们制定了损失函数，使得小批量大小不会降低对抗性样本强度。我们的研究结果突出了低估对抗性攻击真实（实际）强度和高估模型鲁棒性的潜在风险。为了提高研究的可重复性、模型的可用性和鲁棒性，我们分享了代码。

    Neural networks have been proven to be both highly effective within computer vision, and highly vulnerable to adversarial attacks. Consequently, as the use of neural networks increases due to their unrivaled performance, so too does the threat posed by adversarial attacks. In this work, we build towards addressing the challenge of adversarial robustness by exploring the relationship between the mini-batch size used during adversarial sample generation and the strength of the adversarial samples produced. We demonstrate that an increase in mini-batch size results in a decrease in the efficacy of the samples produced, and we draw connections between these observations and the phenomenon of vanishing gradients. Next, we formulate loss functions such that adversarial sample strength is not degraded by mini-batch size. Our findings highlight a potential risk for underestimating the true (practical) strength of adversarial attacks, and a risk of overestimating a model's robustness. We share 
    
[^74]: 在线多类可学习性的刻画。

    A Characterization of Online Multiclass Learnability. (arXiv:2303.17716v1 [cs.LG])

    [http://arxiv.org/abs/2303.17716](http://arxiv.org/abs/2303.17716)

    在线多类学习问题中，使用Multiclass Littlestone维度可以刻画标签数目为无界情况下的可学习性。

    

    我们考虑当标签数目是无界的时候在线多类学习问题。我们展示了Multiclass Littlestone维度，这个概念首次出现在\cite{DanielyERMprinciple}中，继续刻画了该场景下的在线可学习性。我们的结果补充了最近的工作，\cite{Brukhimetal2022}给出了当标签空间是无界的情况下批处理多类可学习性的刻画。

    We consider the problem of online multiclass learning when the number of labels is unbounded. We show that the Multiclass Littlestone dimension, first introduced in \cite{DanielyERMprinciple}, continues to characterize online learnability in this setting. Our result complements the recent work by \cite{Brukhimetal2022} who give a characterization of batch multiclass learnability when the label space is unbounded.
    
[^75]: 深度学习模型转换器中的故障和风险分析：以ONNX生态系统的案例研究为例

    Analysis of Failures and Risks in Deep Learning Model Converters: A Case Study in the ONNX Ecosystem. (arXiv:2303.17708v1 [cs.SE])

    [http://arxiv.org/abs/2303.17708](http://arxiv.org/abs/2303.17708)

    本文详细分析了深度学习模型转换器的故障情况，特别是对ONNX相关的转换器进行了首次故障分析，并详细报告了故障的症状，原因和位置以及随时间的趋势。

    

    软件工程师开发，优化和部署深度学习模型。他们在各种开发框架中使用和重新使用模型，并在各种运行时环境中部署它们。在这个多样化的生态系统中，工程师使用深度学习模型转换器将模型从框架移动到运行时环境。然而，转换器中的错误可能会影响模型质量并破坏部署。深度学习模型转换器的故障频率和故障模式尚不清楚。本文针对ONNX (Open Neural Network eXchange)相关的模型转换器进行了首次故障分析。具体而言，我们分析了ONNX转换器在两个重要的DL框架PyTorch和TensorFlow中的过去故障。还报告了故障（N=200个问题）的症状，原因和位置以及随时间的趋势。我们还通过转换8,797个模型（真实世界和人工生成的实例）来评估当今的故障。

    Software engineers develop, fine-tune, and deploy deep learning (DL) models. They use and re-use models in a variety of development frameworks and deploy them on a range of runtime environments. In this diverse ecosystem, engineers use DL model converters to move models from frameworks to runtime environments. However, errors in converters can compromise model quality and disrupt deployment. The failure frequency and failure modes of DL model converters are unknown.  In this paper, we conduct the first failure analysis on DL model converters. Specifically, we characterize failures in model converters associated with ONNX (Open Neural Network eXchange). We analyze past failures in the ONNX converters in two major DL frameworks, PyTorch and TensorFlow. The symptoms, causes, and locations of failures (for N=200 issues), and trends over time are also reported. We also evaluate present-day failures by converting 8,797 models, both real-world and synthetically generated instances. The consis
    
[^76]: 双重交叉注意力在医学图像分割中的应用

    Dual Cross-Attention for Medical Image Segmentation. (arXiv:2303.17696v1 [eess.IV])

    [http://arxiv.org/abs/2303.17696](http://arxiv.org/abs/2303.17696)

    本文提出了一种双重交叉注意力模块，用于增强U-Net-based架构的医学图像分割模型。该模块通过捕获通道和空间依赖性，解决了编码器和解码器之间的语义鸿沟问题。

    

    本文提出一种简单而有效的注意力模块——双重交叉注意力（DCA），它能够增强基于U-Net的医学图像分割模型中的跳跃连接。DCA通过依次捕获多尺度编码器特征中的通道和空间依赖性来解决编码器和解码器之间的语义鸿沟问题。DCA模块可以集成到任何带有跳跃连接的编码器-解码器架构中，比如U-Net及其变种。本文将DCA模块集成到六种U-Net-based架构中进行了测试。

    We propose Dual Cross-Attention (DCA), a simple yet effective attention module that is able to enhance skip-connections in U-Net-based architectures for medical image segmentation. DCA addresses the semantic gap between encoder and decoder features by sequentially capturing channel and spatial dependencies across multi-scale encoder features. First, the Channel Cross-Attention (CCA) extracts global channel-wise dependencies by utilizing cross-attention across channel tokens of multi-scale encoder features. Then, the Spatial Cross-Attention (SCA) module performs cross-attention to capture spatial dependencies across spatial tokens. Finally, these fine-grained encoder features are up-sampled and connected to their corresponding decoder parts to form the skip-connection scheme. Our proposed DCA module can be integrated into any encoder-decoder architecture with skip-connections such as U-Net and its variants. We test our DCA module by integrating it into six U-Net-based architectures such
    
[^77]: 面向任务的主观知识交互建模

    Task Oriented Conversational Modelling With Subjective Knowledge. (arXiv:2303.17695v1 [cs.CL])

    [http://arxiv.org/abs/2303.17695](http://arxiv.org/abs/2303.17695)

    本文提出了一种改进知识选择模块的实体检索方法，并探讨了一种潜在的关键字提取方法，以提高任务导向交互建模系统的性能。

    

    现有的对话模型都是基于数据库和API的系统来处理的。但是，用户的问题经常需要处理这些系统无法处理的信息。然而，这些问题的答案可以在客户评价和常见问题解答中找到。DSTC-11提出了一个由三个部分组成的管道，包括知识寻求回合检测、知识选择和响应生成，从而创建一个基于主观知识的交互式模型。本文着重于改进知识选择模块，以提高整个系统的性能。我们提出了一种实体检索方法，它可以实现准确和更快的知识搜索。我们提出的基于命名实体识别(NER)的实体检索方法比基线模型快了7倍。此外，我们还探讨了一种潜在的关键字提取方法，可以提高知识选择的准确性。初步结果显示了4\%的改进。

    Existing conversational models are handled by a database(DB) and API based systems. However, very often users' questions require information that cannot be handled by such systems. Nonetheless, answers to these questions are available in the form of customer reviews and FAQs. DSTC-11 proposes a three stage pipeline consisting of knowledge seeking turn detection, knowledge selection and response generation to create a conversational model grounded on this subjective knowledge. In this paper, we focus on improving the knowledge selection module to enhance the overall system performance. In particular, we propose entity retrieval methods which result in an accurate and faster knowledge search. Our proposed Named Entity Recognition (NER) based entity retrieval method results in 7X faster search compared to the baseline model. Additionally, we also explore a potential keyword extraction method which can improve the accuracy of knowledge selection. Preliminary results show a 4 \% improvement
    
[^78]: 可达集的凸包的精确刻画

    Exact Characterization of the Convex Hulls of Reachable Sets. (arXiv:2303.17674v1 [math.OC])

    [http://arxiv.org/abs/2303.17674](http://arxiv.org/abs/2303.17674)

    本文精确地刻画了具有有界扰动的非线性系统的可达集的凸包为一阶常微分方程的解的凸包，提出了一种低成本、高精度的估计算法，可用于过逼近可达集。

    

    本文研究了具有有界扰动的非线性系统的可达集的凸包。可达集在控制中起着至关重要的作用，但计算起来仍然非常具有挑战性，现有的过逼近工具往往过于保守或计算代价高昂。本文精确地刻画了可达集的凸包，将其表示成一阶常微分方程的解的凸包，这个有限维的刻画开启了一种紧密的估计算法，可用于过逼近可达集，且成本比现有方法更低、更精准。本文还提出了神经反馈环分析和鲁棒模型预测控制的应用。

    We study the convex hulls of reachable sets of nonlinear systems with bounded disturbances. Reachable sets play a critical role in control, but remain notoriously challenging to compute, and existing over-approximation tools tend to be conservative or computationally expensive. In this work, we exactly characterize the convex hulls of reachable sets as the convex hulls of solutions of an ordinary differential equation from all possible initial values of the disturbances. This finite-dimensional characterization unlocks a tight estimation algorithm to over-approximate reachable sets that is significantly faster and more accurate than existing methods. We present applications to neural feedback loop analysis and robust model predictive control.
    
[^79]: 神经签名核作为受控ResNets的无限宽度-深度极限。(arXiv:2303.17671v1 [math.DS])

    Neural signature kernels as infinite-width-depth-limits of controlled ResNets. (arXiv:2303.17671v1 [math.DS])

    [http://arxiv.org/abs/2303.17671](http://arxiv.org/abs/2303.17671)

    通过控制ResNets的欧拉离散化，提出了一种新的家族限制核，称为神经签名核。在无限深度情况下，有限宽度的受控ResNets按分布收敛至神经CDE。

    

    受沉积计算范例的启发，我们考虑由神经受控微分方程（神经CDE）的欧拉离散化定义的随机初始化受控ResNets。我们表明，在无限宽度-深度限制和适当的缩放下，这些架构弱收敛到一些连续路径空间上索引的高斯过程，并且具有满足根据激活函数的选择变化的某些偏微分方程（PDE）的核。在激活为恒等式的特殊情况下，我们表明该方程式简化为线性PDE，极限核与Salvi等人的签名核一致。在这种情况下，我们还表明宽度-深度极限是可交换的。我们将这种新的限制核家族称为神经签名核。最后，我们表明，在无限深度的情况下，有限宽度的受控ResNets按分布收敛到具有随机向量场的神经CDE，具体取决于w。

    Motivated by the paradigm of reservoir computing, we consider randomly initialized controlled ResNets defined as Euler-discretizations of neural controlled differential equations (Neural CDEs). We show that in the infinite-width-then-depth limit and under proper scaling, these architectures converge weakly to Gaussian processes indexed on some spaces of continuous paths and with kernels satisfying certain partial differential equations (PDEs) varying according to the choice of activation function. In the special case where the activation is the identity, we show that the equation reduces to a linear PDE and the limiting kernel agrees with the signature kernel of Salvi et al. (2021). In this setting, we also show that the width-depth limits commute. We name this new family of limiting kernels neural signature kernels. Finally, we show that in the infinite-depth regime, finite-width controlled ResNets converge in distribution to Neural CDEs with random vector fields which, depending on w
    
[^80]: MetaEnhance:大学图书馆电子学位论文元数据质量提升

    MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries. (arXiv:2303.17661v1 [cs.DL])

    [http://arxiv.org/abs/2303.17661](http://arxiv.org/abs/2303.17661)

    本论文提出了MetaEnhance，一个利用最先进的人工智能方法来提高电子学位论文关键字段质量的框架，并成功在500份样本中实现了高准确性的元数据错误检测和纠正。

    

    数字对象的元数据质量对于通过数字库界面进行发现非常重要。但由于各种原因，数字对象的元数据通常展示出不完整、不一致和不正确的值。我们研究了自动检测、纠正和规范学术元数据的方法，以电子学位论文的七个关键字段为案例研究。我们提出了MetaEnhance，这是一个利用最先进的人工智能方法来提高这些字段质量的框架。为了评估MetaEnhance，我们编制了一个元数据质量评估基准，其中包含500个电子学位论文，通过多个标准进行采样子集来组合。我们对这个基准测试MetaEnhance，结果发现所提出的方法几乎完美地实现了错误检测的F1分数以及五个字段中的错误纠正的F1分数在0.85到1.00之间。

    Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.
    
[^81]: 基于机器学习的CMS粒子流算法改进进展

    Progress towards an improved particle flow algorithm at CMS with machine learning. (arXiv:2303.17657v1 [physics.data-an])

    [http://arxiv.org/abs/2303.17657](http://arxiv.org/abs/2303.17657)

    CMS实验引入机器学习粒子流算法（MLPF）并优化了其重构实现，首次以生成器/模拟级粒子信息为目标进行优化，这为提高检测器对物理量的响应铺平了道路。

    

    粒子流（PF）算法是CMS实验在CERN LHC中事件重建的核心算法，它基于轨迹和量能器聚簇推断粒子，并在预计具有增加堆叠和探测器粒度的第二阶段运行条件中重点开发。近年来，基于机器学习的粒子流（MLPF）算法已在CMS中进行了探索，可能具有直接优化感兴趣的物理量、高度可重构到新条件和天然适合于异构加速器部署的优点。本文探讨了CMS对MLPF重构实现的改进进展，首次以生成器/模拟级粒子信息为目标进行优化。这为潜在地提高检测器对感兴趣的物理量的响应铺平了道路。我们描述了

    The particle-flow (PF) algorithm, which infers particles based on tracks and calorimeter clusters, is of central importance to event reconstruction in the CMS experiment at the CERN LHC, and has been a focus of development in light of planned Phase-2 running conditions with an increased pileup and detector granularity. In recent years, the machine learned particle-flow (MLPF) algorithm, a graph neural network that performs PF reconstruction, has been explored in CMS, with the possible advantages of directly optimizing for the physical quantities of interest, being highly reconfigurable to new conditions, and being a natural fit for deployment to heterogeneous accelerators. We discuss progress in CMS towards an improved implementation of the MLPF reconstruction, now optimized using generator/simulation-level particle information as the target for the first time. This paves the way to potentially improving the detector response in terms of physical quantities of interest. We describe the
    
[^82]: 自我反馈迭代精炼：一种无需监督学习或加强学习的LM改进框架

    Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])

    [http://arxiv.org/abs/2303.17651](http://arxiv.org/abs/2303.17651)

    自我反馈迭代精炼是一种无需监督学习或加强学习的LLMs初始输出优化方法，优于直接生成，被证实在7个不同任务中表现更好。

    

    鉴于语言模型(LLMs)不总是能在第一次良好地解决生成问题（如摘要、答案、解释等），我们引入自我反馈迭代精炼（SELF-REFINE）框架，通过迭代反馈和精炼相似地优化LLMs的初始输出。主要思想是：使用LLM生成输出，然后允许同一模型提供其自身输出的多方面反馈，最后利用反馈使相同模型精炼先前生成的输出。我们的迭代精炼框架与早期工作不同，无需监督训练数据或加强学习，并且可以与单个LLM一起使用。我们对七个不同的任务进行了实验，范围从评论重写到数学推理，表明我们的方法优于直接生成。在所有任务中，使用SELF-REFINE生成的输出被人类和自动化指标优先于使用GPT-3.5和GPT-4直接生成的输出，表现得更好。

    Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving
    
[^83]: 利用强化学习将一个中等大小的英文GPT模型对齐到西班牙语的小封闭领域中

    Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])

    [http://arxiv.org/abs/2303.17649](http://arxiv.org/abs/2303.17649)

    本文介绍了一种将英文GPT模型对齐到西班牙语的小封闭领域中的方法，该方法使用了奖励模型来改进答案的解码和生成，在问答任务中取得了良好的结果。

    

    本文提出了一种方法，将原本用于开放领域的中等大小英文GPT模型，对齐到西班牙语的小封闭领域。该模型被精细调整用于问答任务。为了实现这一目标，我们还需要训练和实现另一个神经网络（我们称之为奖励模型），以评分并确定答案是否适用于给定的问题。该组件有助于改进系统回答的解码和生成。 BLEU和perplexity等数字度量标准被用于评估模型，同时也使用人类判断来比较解码技术与其他技术。最终，结果支持了所提出的方法，并确定使用奖励模型来对齐生成回答是可行的。

    In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
    
[^84]: 个性化试验下的实用策略优化

    Practical Policy Optimization with Personalized Experimentation. (arXiv:2303.17648v1 [cs.LG])

    [http://arxiv.org/abs/2303.17648](http://arxiv.org/abs/2303.17648)

    本文提出了一个个性化试验框架PEX，利用HTE建模和序列决策策略优化，在用户级别上优化治疗组分配，同时优化多个短期和长期结果。

    

    许多组织通过实验平台来测量处理效果，以在全面部署之前评估产品变化的因果效应。然而，标准实验平台对表现出异质处理效应的最终用户人群（HTE）的效果并不理想。在这里，我们介绍了一个个性化试验框架，个性化试验（PEX），通过HTE建模和顺序决策策略优化，以在用户级别上优化治疗组分配，同时优化多个短期和长期结果。我们描述了一种从实践中证明成功并可使用开源软件轻松实现的端到端工作流程。

    Many organizations measure treatment effects via an experimentation platform to evaluate the casual effect of product variations prior to full-scale deployment. However, standard experimentation platforms do not perform optimally for end user populations that exhibit heterogeneous treatment effects (HTEs). Here we present a personalized experimentation framework, Personalized Experiments (PEX), which optimizes treatment group assignment at the user level via HTE modeling and sequential decision policy optimization to optimize multiple short-term and long-term outcomes simultaneously. We describe an end-to-end workflow that has proven to be successful in practice and can be readily implemented using open-source software.
    
[^85]: 基于希腊数据的用水量时间序列预测模型评估

    An evaluation of time series forecasting models on water consumption data: A case study of Greece. (arXiv:2303.17617v1 [cs.LG])

    [http://arxiv.org/abs/2303.17617](http://arxiv.org/abs/2303.17617)

    本文对希腊的用水量数据进行了时间序列预测模型的评估，揭示了各算法优缺点。

    

    近年来，城市化和工业化的加剧导致水资源需求不断上升，从而加大了供需之间的差距。适当的用水分配和用水量预测是缓解供需失衡、改善水资源运营、规划和管理的关键因素。为此，在本文中，我们评估了几种著名的时间序列预测算法，针对希腊的用水量数据进行了评估。这是一个拥有多元社会经济和城市化问题的国家。我们采用由希腊自来水和污水公司提供的真实数据集对预测算法进行了评估，揭示了每种算法及其应用的关键见解。

    In recent years, the increased urbanization and industrialization has led to a rising water demand and resources, thus increasing the gap between demand and supply. Proper water distribution and forecasting of water consumption are key factors in mitigating the imbalance of supply and demand by improving operations, planning and management of water resources. To this end, in this paper, several well-known forecasting algorithms are evaluated over time series, water consumption data from Greece, a country with diverse socio-economic and urbanization issues. The forecasting algorithms are evaluated on a real-world dataset provided by the Water Supply and Sewerage Company of Greece revealing key insights about each algorithm and its use.
    
[^86]: 基于领域转换和深度学习的血糖时间序列模式检测

    Patterns Detection in Glucose Time Series by Domain Transformations and Deep Learning. (arXiv:2303.17616v1 [cs.LG])

    [http://arxiv.org/abs/2303.17616](http://arxiv.org/abs/2303.17616)

    该论文介绍了一种基于血糖时间序列的领域转换和深度学习方法，以预测血糖水平的未来行为，帮助避免低血糖事件。

    

    糖尿病患者需要管理血糖水平以使其保持在适当范围内。预测未来的血糖值是否会超出健康阈值对于采取纠正措施以避免潜在健康损害至关重要。本文描述了我们的研究，旨在预测血糖水平的未来行为，以便可以预测低血糖事件。本文的方法是在血糖时间序列上应用变换函数，并将其用于卷积神经网络中。我们使用4位不同糖尿病患者的真实数据测试了我们的方法，取得了有希望的结果。

    People with diabetes have to manage their blood glucose level to keep it within an appropriate range. Predicting whether future glucose values will be outside the healthy threshold is of vital importance in order to take corrective actions to avoid potential health damage. In this paper we describe our research with the aim of predicting the future behavior of blood glucose levels, so that hypoglycemic events may be anticipated. The approach of this work is the application of transformation functions on glucose time series, and their use in convolutional neural networks. We have tested our proposed method using real data from 4 different diabetes patients with promising results.
    
[^87]: 利用强化学习进行de novo药物设计

    Utilizing Reinforcement Learning for de novo Drug Design. (arXiv:2303.17615v1 [q-bio.BM])

    [http://arxiv.org/abs/2303.17615](http://arxiv.org/abs/2303.17615)

    本文开发了一个统一的框架，利用强化学习生成预测具有活性的新药分子。在需要结构多样性的情况下，同时使用高分和低分子来更新策略是有利的。使用所有生成的分子可以提高性能稳定性，而off-policy算法有潜力提高生成分子的结构多样性。

    

    基于深度学习的药物设计方法在生成具有特定性质的新药分子方面表现出了强大的潜力。最近的研究利用强化学习实现了字符串生成新分子的显著性能提升。本文中，我们开发了一个统一的框架，利用强化学习进行de novo药物设计，系统地研究了各种on-policy和off-policy 强化学习算法和重播缓冲区，学习基于RNN的策略，生成预测对于多巴胺受体DRD2具有活性的新分子。我们的研究结果表明，在需要结构多样性的情况下，同时使用高分和低分分子来更新策略是有利的。对于on-policy算法，使用所有生成的分子可以提高性能稳定性。此外，当重放高、中和低分子时，off-policy算法有潜力提高生成分子的结构多样性。

    Deep learning-based approaches for generating novel drug molecules with specific properties have gained a lot of interest in the last years. Recent studies have demonstrated promising performance for string-based generation of novel molecules utilizing reinforcement learning. In this paper, we develop a unified framework for using reinforcement learning for de novo drug design, wherein we systematically study various on- and off-policy reinforcement learning algorithms and replay buffers to learn an RNN-based policy to generate novel molecules predicted to be active against the dopamine receptor DRD2. Our findings suggest that it is advantageous to use at least both top-scoring and low-scoring molecules for updating the policy when structural diversity is essential. Using all generated molecules at an iteration seems to enhance performance stability for on-policy algorithms. In addition, when replaying high, intermediate, and low-scoring molecules, off-policy algorithms display the pot
    
[^88]: oBERTa: 通过改进初始化、蒸馏和剪枝来提高稀疏迁移学习

    oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])

    [http://arxiv.org/abs/2303.17612](http://arxiv.org/abs/2303.17612)

    oBERTa是一组易于使用的语言模型，通过改进初始化、蒸馏、剪枝等技术，可以在不需要模型压缩方面的专业知识的情况下提高稀疏迁移学习的效率和准确性。

    

    本文介绍了oBERTa语言模型的范围，它是一组易于使用的语言模型，允许自然语言处理（NLP）从业者在不需要模型压缩方面的专业知识的情况下获得3.8到24.3倍的更快速的模型。oBERTa扩展了现有的剪枝、知识蒸馏和量化工作，并利用冻结的嵌入来改进知识蒸馏，并改进模型初始化，以在广泛的传递任务上提供更高的准确性。在生成oBERTa时，我们探索了高度优化的RoBERTa与BERT在预训练和微调期间剪枝方面的不同之处，并发现它在微调期间不太适合压缩。我们探索了oBERTa在七个具有代表性的NLP任务上的使用，并发现改进的压缩技术使得经过剪枝的oBERTa模型能够匹配BERTBASE的性能，并超过SQUAD V1.1问答数据的Prune OFA Large的性能。

    In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
    
[^89]: 采用灵活概率神经网络方法的集合天气预报后处理

    Ensemble weather forecast post-processing with a flexible probabilistic neural network approach. (arXiv:2303.17610v1 [cs.LG])

    [http://arxiv.org/abs/2303.17610](http://arxiv.org/abs/2303.17610)

    本论文提出了一种神经网络和归一化流相结合的方法，可联合预测所有位置和提前期，从而放宽了许多传统后处理方法的分布假设，并通过EUPPBench基准测试证明了其超越性能。

    

    集合预报后处理是生成准确概率预报的必要步骤。传统的后处理方法是根据每个位置或每个提前期估计参数统计分布。我们提出了一种基于神经网络的新方法，它可以联合预测所有位置和提前期。为了放宽许多后处理方法的分布假设，我们的方法采用归一化流作为灵活的参数分布估计器。这使我们能够以数学确切的方式模拟不同的预测分布。我们在EUPPBench基准测试中展示了我们方法的有效性，在该测试中，我们对西欧地区子区域的站点进行了温度预报后处理。我们展示了我们的新方法在基准测试中表现出卓越的性能，超越了我们之前的表现良好的成绩。此外，通过提供详细的比较，我们证明了我们方法的优越性。

    Ensemble forecast post-processing is a necessary step in producing accurate probabilistic forecasts. Conventional post-processing methods operate by estimating the parameters of a parametric distribution, frequently on a per-location or per-lead-time basis. We propose a novel, neural network-based method, which produces forecasts for all locations and lead times, jointly. To relax the distributional assumption of many post-processing methods, our approach incorporates normalizing flows as flexible parametric distribution estimators. This enables us to model varying forecast distributions in a mathematically exact way. We demonstrate the effectiveness of our method in the context of the EUPPBench benchmark, where we conduct temperature forecast post-processing for stations in a sub-region of western Europe. We show that our novel method exhibits state-of-the-art performance on the benchmark, outclassing our previous, well-performing entry. Additionally, by providing a detailed compariso
    
[^90]: 发现自然定律的机器学习

    Machine learning for discovering laws of nature. (arXiv:2303.17607v1 [cs.LG])

    [http://arxiv.org/abs/2303.17607](http://arxiv.org/abs/2303.17607)

    模型基于达尔文自然选择，结合函数选择和运算符选择两个过程，通过从数据中学习构建理论，可自动发现和表示自然定律，成功应用于模拟多领域问题，并提供一种新方法解决描述自然定律的严格数学模型不足的问题。

    

    微观粒子遵循量子力学的原理——那么宏观和微观世界之间的明确界限在哪里呢？正是这个“解释问题”促使薛定谔提出了他著名的思想实验（一只同时死亡和活着的猫），引发了关于量子测量问题的激烈争论，但至今仍没有令人满意的答案。这正是描述自然定律的严格数学模型的不足之处。我们提出了一个基于达尔文自然选择的计算模型来描述和理解自然定律。实际上，无论是宏观粒子、微观电子还是安全问题，它们都可以被认为是一个实体，这个实体随着时间的推移变化，可以用状态和值组成的数据序列来描述。观察者可以从这个数据序列中学习，构建理论（通常由函数和微分方程组成）。我们不再使用用户的经验或逻辑来建模，而是使用数据。计算模型的核心基于两个过程：函数选择和运算符选择。函数选择过程类似于达尔文的进化，允许具有优势特征的函数生存和繁殖；而运算符选择过程捕捉了自然定律的相互依存性，可以平衡自然界中不同函数的优势。该方法使我们能够从数据中自动发现和表示自然定律，并已成功应用于模拟量子力学、经典力学和系统生物学。

    A microscopic particle obeys the principles of quantum mechanics -- so where is the sharp boundary between the macroscopic and microscopic worlds? It was this "interpretation problem" that prompted Schr\"odinger to propose his famous thought experiment (a cat that is simultaneously both dead and alive) and sparked a great debate about the quantum measurement problem, and there is still no satisfactory answer yet. This is precisely the inadequacy of rigorous mathematical models in describing the laws of nature. We propose a computational model to describe and understand the laws of nature based on Darwin's natural selection. In fact, whether it's a macro particle, a micro electron or a security, they can all be considered as an entity, the change of this entity over time can be described by a data series composed of states and values. An observer can learn from this data series to construct theories (usually consisting of functions and differential equations). We don't model with the us
    
[^91]: G不变图拉普拉斯算子

    The G-invariant graph Laplacian. (arXiv:2303.17001v1 [cs.LG])

    [http://arxiv.org/abs/2303.17001](http://arxiv.org/abs/2303.17001)

    本文提出了 G不变图拉普拉斯算子 用于处理数据集不仅在流形上，而且在一个连续群的作用下也是封闭的情形，相较于标准图拉普拉斯算子收敛速度更快。

    

    基于图拉普拉斯算子的算法已经被证明在降维、聚类和去噪等领域对流形数据非常有效。本文考虑的数据集不仅在流形上，而且在一个连续群的作用下也是封闭的。这类数据集的一个例子是沿着低维流形传播的体积，其中每个体积可以在三维空间中旋转。我们介绍了G不变图拉普拉斯算子，通过考虑数据集上的群的作用来广义化图拉普拉斯算子。我们显示了与标准图拉普拉斯算子类似，G不变图拉普拉斯算子收敛于数据流形上的Laplace-Beltrami算子，但收敛速度显著提高。此外，我们还表明G不变图拉普拉斯算子的特征函数具有群元素和某些矩阵的特征向量的张量积形式，可以使用F高效地计算。

    Graph Laplacian based algorithms for data lying on a manifold have been proven effective for tasks such as dimensionality reduction, clustering, and denoising. In this work, we consider data sets whose data point not only lie on a manifold, but are also closed under the action of a continuous group. An example of such data set is volumes that line on a low dimensional manifold, where each volume may be rotated in three-dimensional space. We introduce the G-invariant graph Laplacian that generalizes the graph Laplacian by accounting for the action of the group on the data set. We show that like the standard graph Laplacian, the G-invariant graph Laplacian converges to the Laplace-Beltrami operator on the data manifold, but with a significantly improved convergence rate. Furthermore, we show that the eigenfunctions of the G-invariant graph Laplacian admit the form of tensor products between the group elements and eigenvectors of certain matrices, which can be computed efficiently using F
    
[^92]: 通过二维卷积神经网络和肺部CT扫描对地玻璃影进行严重程度分类：一个三天的探索

    Severity classification of ground-glass opacity via 2-D convolutional neural network and lung CT scans: a 3-day exploration. (arXiv:2303.16904v1 [eess.IV])

    [http://arxiv.org/abs/2303.16904](http://arxiv.org/abs/2303.16904)

    本研究探讨了使用二维卷积神经网络技术对肺部CT扫描进行严重程度分类，为诊断肺部疾病如COVID-19和肺炎提供新的方法。

    

    地玻璃影是许多肺部疾病的标志，包括COVID19和肺炎患者。本研究使用新建的虚拟环境，在2023年3月17日创建，通过实验探讨了各种预训练的二维卷积神经网络，如稠密神经网络、ResNet和视觉变换器，以及微调的程度。根据经验实验，我们选择使用ADAM优化算法对所有CNN架构进行标准学习率0.001的微调，并在验证损失达到平原时进行早停。为每个训练的CNN制定严重程度分类规则，并在三天内进行了实验测试。

    Ground-glass opacity is a hallmark of numerous lung diseases, including patients with COVID19 and pneumonia. This brief note presents experimental results of a proof-of-concept framework that got implemented and tested over three days as driven by the third challenge entitled "COVID-19 Competition", hosted at the AI-Enabled Medical Image Analysis Workshop of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023). Using a newly built virtual environment (created on March 17, 2023), we investigated various pre-trained two-dimensional convolutional neural networks (CNN) such as Dense Neural Network, Residual Neural Networks (ResNet), and Vision Transformers, as well as the extent of fine-tuning. Based on empirical experiments, we opted to fine-tune them using ADAM's optimization algorithm with a standard learning rate of 0.001 for all CNN architectures and apply early-stopping whenever the validation loss reached a plateau. For each trained CNN, th
    
[^93]: 受神经衰竭启示的非独立同分布数据联邦学习

    Neural Collapse Inspired Federated Learning with Non-iid Data. (arXiv:2303.16066v1 [cs.LG])

    [http://arxiv.org/abs/2303.16066](http://arxiv.org/abs/2303.16066)

    本文提出了一种受神经衰竭启示的方案，通过将每个客户端优化向全局分类的最佳结构，解决了联邦学习中非独立同分布数据的挑战，并通过添加全局记忆向量来补救参数波动的问题。

    

    联邦学习中的主要挑战之一是异构设备之间的非独立同分布（非iid）特性，导致本地更新存在显著差异，影响中央服务器的性能。尽管已经提出了许多研究来解决这个挑战，但它们只关注本地训练和聚合过程以平滑变化，并未在深度学习模型中实现高性能。受神经衰竭现象启发，我们强制每个客户端优化向全局分类的最佳结构。具体而言，我们将其初始化为随机的简单六角紧框架（ETF），并在本地更新期间将其作为所有客户端的单元优化目标进行固定。在确保所有客户端学习收敛于全局最优解之后，我们提出为每个类别添加全局记忆向量，以补救由于类内条件分布偏差引起的参数波动。

    One of the challenges in federated learning is the non-independent and identically distributed (non-iid) characteristics between heterogeneous devices, which cause significant differences in local updates and affect the performance of the central server. Although many studies have been proposed to address this challenge, they only focus on local training and aggregation processes to smooth the changes and fail to achieve high performance with deep learning models. Inspired by the phenomenon of neural collapse, we force each client to be optimized toward an optimal global structure for classification. Specifically, we initialize it as a random simplex Equiangular Tight Frame (ETF) and fix it as the unit optimization target of all clients during the local updating. After guaranteeing all clients are learning to converge to the global optimum, we propose to add a global memory vector for each category to remedy the parameter fluctuation caused by the bias of the intra-class condition dist
    
[^94]: 一种基于半监督回归深度学习模型的简绸纱线数目计算方法以应用于古老油画

    Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models. (arXiv:2303.15999v1 [cs.CV])

    [http://arxiv.org/abs/2303.15999](http://arxiv.org/abs/2303.15999)

    本文开发了一种基于深度学习的半监督回归模型，可直接从图片中计算简绸纱线的线密度，以应用于古老油画，为艺术品保护和修复提供了可靠的线密度估计方法。

    

    本文作者开发了一种基于深度学习的回归方法，以执行对简绸纱线密度的估计，以进行画布分析。该新方法可以直接从图像中计算线密度，并在训练过程中使用了专家知识。该方法在艺术保存和修复领域具有潜在应用。

    In this work the authors develop regression approaches based on deep learning to perform thread density estimation for plain weave canvas analysis. Previous approaches were based on Fourier analysis, that are quite robust for some scenarios but fail in some other, in machine learning tools, that involve pre-labeling of the painting at hand, or the segmentation of thread crossing points, that provides good estimations in all scenarios with no need of pre-labeling. The segmentation approach is time-consuming as estimation of the densities is performed after locating the crossing points. In this novel proposal, we avoid this step by computing the density of threads directly from the image with a regression deep learning model. We also incorporate some improvements in the initial preprocessing of the input image with an impact on the final error. Several models are proposed and analyzed to retain the best one. Furthermore, we further reduce the density estimation error by introducing a sem
    
[^95]: 非线性平流-扩散-吸附系统的高效混合建模和吸附模型探索：系统科学机器学习方法

    Efficient hybrid modeling and sorption model discovery for non-linear advection-diffusion-sorption systems: A systematic scientific machine learning approach. (arXiv:2303.13555v1 [cs.CE])

    [http://arxiv.org/abs/2303.13555](http://arxiv.org/abs/2303.13555)

    本研究提出了一种机器学习方法，可用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型的吸附动力学定律结构。

    

    本研究提出了一种系统化的机器学习方法，用于创建非线性平流-扩散-吸附系统的高效混合模型和发现吸附摄取模型。它演示了使用基于梯度的优化器、伴随灵敏度分析和JIT编译的向量雅各布乘积，结合空间离散和自适应积分器来训练这些复杂系统的有效方法。稀疏和符号回归被用来识别人工神经网络中缺失的函数。该方法的鲁棒性在一个模拟数据集上得到了测试，该数据集包含固定床吸附的噪声突破曲线观测结果，得出了拟合良好的混合模型。该研究成功地利用稀疏和符号回归重建了吸附摄取动力学，并利用确定的多项式准确地预测了突破曲线，突显了该框架发现吸附动力学定律结构的潜力。

    This study presents a systematic machine learning approach for creating efficient hybrid models and discovering sorption uptake models in non-linear advection-diffusion-sorption systems. It demonstrates an effective method to train these complex systems using gradientbased optimizers, adjoint sensitivity analysis, and JIT-compiled vector Jacobian products, combined with spatial discretization and adaptive integrators. Sparse and symbolic regression were employed to identify missing functions in the artificial neural network. The robustness of the proposed method was tested on an in-silico data set of noisy breakthrough curve observations of fixed-bed adsorption, resulting in a well-fitted hybrid model. The study successfully reconstructed sorption uptake kinetics using sparse and symbolic regression, and accurately predicted breakthrough curves using identified polynomials, highlighting the potential of the proposed framework for discovering sorption kinetic law structures.
    
[^96]: ALOFT：一种轻量化的类MLP架构，配合动态低频变换用于域泛化

    ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization. (arXiv:2303.11674v1 [cs.CV])

    [http://arxiv.org/abs/2303.11674](http://arxiv.org/abs/2303.11674)

    本文介绍了一种轻量级的类MLP架构ALOFT，它可使用动态低频变换用于域泛化。与CNN相比，该架构更好地捕捉全局表示，因此具有更好的泛化能力。

    

    域泛化旨在学习一个模型，它在不重新训练的情况下利用多个源域来很好地推广到看不见的目标域。大多数现有的域泛化工作都基于卷积神经网络（CNN）。然而，卷积核的局部操作使得模型过于关注局部表示（例如纹理），这从本质上使得模型更容易过拟合源域并阻碍其泛化能力。最近，几种基于MLP的方法通过学习图像不同块之间的全局交互，在监督学习任务中取得了有希望的结果。本文在此受到启发，首先分析了CNN和MLP方法在DG中的差异，并发现MLP方法表现出更好的泛化能力，因为它们可以比CNN方法更好地捕捉全局表示（例如结构）。然后，基于最近的一种轻量级MLP方法，我们获得了一个强大的基准线，其性能优于大多数统计学方法。

    Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most stat
    
[^97]: 辅助网络在持续学习中实现更好的稳定性-可塑性平衡

    Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning. (arXiv:2303.09483v1 [cs.LG])

    [http://arxiv.org/abs/2303.09483](http://arxiv.org/abs/2303.09483)

    本文提出了一种辅助网络持续学习方法（ANCL），通过对流信息的控制，自然插值可塑性和稳定性之间的差异，有助于在神经网络中实现更好的稳定性-可塑性平衡。

    

    与人类顺序学习新任务的自然能力相比，神经网络被认为容易出现灾难性遗忘，即模型在被优化为新任务后，在旧任务上的表现急剧下降。为此，持续学习（CL）社区提出了几种解决方案，旨在使神经网络具有学习当前任务（可塑性）的能力，同时在以前的任务上实现高精度（稳定性）。尽管取得了显着的进展，但稳定性-可塑性平衡还远未得到解决，其基本机制尚不清楚。在这项工作中，我们提出了一种新方法——辅助网络持续学习（ANCL），它将一个额外的辅助网络应用于主要关注稳定性的持续学习模型中，从而促进模型的可塑性。更具体地说，所提出的框架通过控制主要网络和辅助网络之间信息的流动来自然地插值可塑性和稳定性之间的差异。多个数据集的实验结果表明，ANCL在可塑性和稳定性方面优于现有持续学习方法，实现了更好的平衡。

    In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and st
    
[^98]: 基于百万用户的现实世界互动来奖励聊天机器人

    Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.06135](http://arxiv.org/abs/2303.06135)

    本文研究了如何通过利用用户反馈来提高聊天机器人的参与度，从而增强其留存能力。具体方法是使用自动伪标签来训练奖励模型，并使用平均对话长度一类的指标来衡量其效果。在试验中，该方法可将聊天机器人的平均对话长度提高70%。

    

    预先训练的大型语言模型的出现，导致部署了一系列的社交聊天机器人。虽然这些聊天机器人展示了其语言能力和流畅性，但它们并不能保证很有吸引力，很容易失去用户。本文研究了开发优先考虑用户参与度以增强留存的社交聊天机器人，具体探讨了使用人工反馈以高效地开发高度有吸引力的聊天机器人。提出的方法使用从用户交互中收集的自动伪标签来训练奖励模型，该模型可用于在推理时拒绝低得分的样本响应，以提高用户参与度。引入了直观的评估指标，例如平均对话长度（MCL），作为衡量已部署聊天机器人参与度水平的代理。在Chai Research平台上对每日的10,000个新聊天机器人用户进行A/B测试，结果表明，这种方法可使MCL增加70％，这相当于将留存时间延长1.5倍。

    The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
    
[^99]: 一个ADMM求解MKL-$L_{0/1}$-SVM的方法

    An ADMM Solver for the MKL-$L_{0/1}$-SVM. (arXiv:2303.04445v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.04445](http://arxiv.org/abs/2303.04445)

    该论文提出了一个快速ADMM求解器，能够有效处理带有$(0,1)$-损失函数的支持向量机问题，通过在合成平面数据上的实验，证明了该方法的潜力。

    

    我们针对带有臭名昭著的$(0,1)$-损失函数的支持向量机问题，制定了多核学习(MKL)问题。给出了一些一阶最优性条件，然后利用这些条件来开发一个快速的ADMM求解器，用于处理非凸和非光滑的优化问题。一个简单的数值实验表明，我们的MKL-$L_{0/1}$-SVM框架具有很好的前景。

    We formulate the Multiple Kernel Learning (abbreviated as MKL) problem for the support vector machine with the infamous $(0,1)$-loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver for the nonconvex and nonsmooth optimization problem. A simple numerical experiment on synthetic planar data shows that our MKL-$L_{0/1}$-SVM framework could be promising.
    
[^100]: BO-Muse：一种用于加速实验设计的人工智能和人类专家的协作框架

    BO-Muse: A human expert and AI teaming framework for accelerated experimental design. (arXiv:2303.01684v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01684](http://arxiv.org/abs/2303.01684)

    BO-Muse是一种新的人工智能和人类专家协作的优化方法，它让人类专家发挥主导作用，通过注入新颖性并发现弱点来打破过度开发，以加速实验设计。

    

    本文介绍了BO-Muse，一种新的人工智能和人类专家协作的方法，用于优化昂贵的黑盒函数。受到从专家知识中提取和蒸馏回AI模型的内在困难以及对真实世界实验设计中人类行为的观察的启发，我们的算法让人类专家在实验过程中发挥主导作用。人类专家可以充分利用他们的领域专业知识，而人工智能则扮演着灵感的角色，在寻找弱点的同时注入新颖性，从而打破由认知融入引起的过度开发。在温和的假设下，我们证明了我们的算法亚线性收敛，速度快于单独使用人工智能或人类专家。我们使用合成数据以及人类专家进行真实世界实验来验证我们的算法。

    In this paper we introduce BO-Muse, a new approach to human-AI teaming for the optimization of expensive black-box functions. Inspired by the intrinsic difficulty of extracting expert knowledge and distilling it back into AI models and by observations of human behavior in real-world experimental design, our algorithm lets the human expert take the lead in the experimental process. The human expert can use their domain expertise to its full potential, while the AI plays the role of a muse, injecting novelty and searching for areas of weakness to break the human out of over-exploitation induced by cognitive entrenchment. With mild assumptions, we show that our algorithm converges sub-linearly, at a rate faster than the AI or human alone. We validate our algorithm using synthetic data and with human experts performing real-world experiments.
    
[^101]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^102]: 三次相关聚类中的部分最优性

    Partial Optimality in Cubic Correlation Clustering. (arXiv:2302.04694v2 [cs.DM] UPDATED)

    [http://arxiv.org/abs/2302.04694](http://arxiv.org/abs/2302.04694)

    该论文关注于建立完全图和三次目标函数的特殊情况下的部分最优性条件，同时提出了测试这些条件的算法并在两个数据集上数值上检验了它们的效果。

    

    高阶相关聚类问题是一种表达力强的模型，最近针对几个应用提出了局部搜索启发式算法。然而，保证最优性是 NP-hard 的，而且在问题陈述的复杂性上已经受到实际阻碍。在这里，我们专注于建立完全图和三次目标函数的特殊情况下部分最优性条件。此外，我们定义和实现了测试这些条件的算法，并在两个数据集上数值上检验了它们的效果。

    The higher-order correlation clustering problem is an expressive model, and recently, local search heuristics have been proposed for several applications. Certifying optimality, however, is NP-hard and practically hampered already by the complexity of the problem statement. Here, we focus on establishing partial optimality conditions for the special case of complete graphs and cubic objective functions. In addition, we define and implement algorithms for testing these conditions and examine their effect numerically, on two datasets.
    
[^103]: NeuKron: 稀疏可重排矩阵和张量的常数大小有损压缩

    NeuKron: Constant-Size Lossy Compression of Sparse Reorderable Matrices and Tensors. (arXiv:2302.04570v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04570](http://arxiv.org/abs/2302.04570)

    NeuKron提出了一种可以将稀疏可重排矩阵和张量压缩至常数大小的方法，其使用了循环神经网络并能在线性时间内更新参数，同时可以在对数时间内检索每个条目的近似。

    

    许多真实世界的数据自然地被表示为稀疏可重排矩阵，其行和列可以任意排序(例如双分图的邻接矩阵)。将稀疏矩阵存储在传统的方式需要线性数量的空间来存储非零元素，而稀疏矩阵的有损压缩(例如截断SVD)通常需要与行和列的数量成线性关系的空间。本文提出了一种称为NeuKron的方法，用于将稀疏可重排矩阵压缩至常数大小的空间。NeuKron使用具有常数参数数量的循环神经网络泛化Kronecker乘积。NeuKron通过更新参数来近似给定矩阵的乘积，并重新排列矩阵的行和列以便于近似。更新所需的时间为输入矩阵中非零元素的线性时间，并且每个条目的近似可以在对数时间内检索。我们还将NeuKron扩展到可以压缩具有常数参数数量的稀疏可重排张量。

    Many real-world data are naturally represented as a sparse reorderable matrix, whose rows and columns can be arbitrarily ordered (e.g., the adjacency matrix of a bipartite graph). Storing a sparse matrix in conventional ways requires an amount of space linear in the number of non-zeros, and lossy compression of sparse matrices (e.g., Truncated SVD) typically requires an amount of space linear in the number of rows and columns. In this work, we propose NeuKron for compressing a sparse reorderable matrix into a constant-size space. NeuKron generalizes Kronecker products using a recurrent neural network with a constant number of parameters. NeuKron updates the parameters so that a given matrix is approximated by the product and reorders the rows and columns of the matrix to facilitate the approximation. The updates take time linear in the number of non-zeros in the input matrix, and the approximation of each entry can be retrieved in logarithmic time. We also extend NeuKron to compress sp
    
[^104]: 一种统一的方法推导（时间均匀的）PAC-Bayes界限

    A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03421](http://arxiv.org/abs/2302.03421)

    该论文提出了一种用于推导PAC-Bayesian泛化界限的统一框架，不同于传统的固定样本大小方式，该框架适用于所有停止时间。同时，该论文还提出了新的边界方法，也可以应用于非平稳损失函数和非独立同分布的数据。

    

    我们提出了一个框架，用于推导PAC-Bayesian泛化界限。与大多数关于此主题的文献不同，我们的界限是任何时间都有效的（即时间均匀的），这意味着它们适用于所有停止时间，而不仅仅是固定的样本大小。我们的方法按照以下顺序结合了四种工具：（a）非负超马丁格尔或反向亚马逊，（b）混合法，（c）Donsker-Varadhan公式（或其它凸性对偶原理）和（d）Ville不等式。我们的主要成果是一个PAC-Bayes定理，适用于广泛的离散随机过程类。我们展示了这个结果如何推出知名的经典PAC-Bayes界限，例如Seeger、McAllester、Maurer和Catoni的界限，以及许多最新的界限。我们还提出了几个新的界限。我们的框架还使我们能够放松传统的假设；特别地，我们考虑非平稳损失函数和非独立同分布的数据。

    We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivati
    
[^105]: 为验证大规模神经网络的几何鲁棒性迈进

    Towards Verifying the Geometric Robustness of Large-scale Neural Networks. (arXiv:2301.12456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12456](http://arxiv.org/abs/2301.12456)

    本文提出了一种名为GeoRobust的黑盒鲁棒性分析器，旨在对大规模神经网络进行多个几何变换的鲁棒性验证，并且无论网络的体系结构、激活函数和神经元数量如何，GeoRobust都能够提供高精度的最坏情况的变换组合。

    

    深度神经网络(DNNs)已知对抗性几何变换易受攻击。本文旨在通过可证明的保证来验证大规模DNNs对多个几何变换的鲁棒性。我们开发了GeoRobust，一个基于新型全局优化策略的黑盒子鲁棒性分析器，用于定位影响甚至改变网络输出的最坏变换组合。 GeoRobust可以根据Lipschitzian理论的最新进展，提供找到最坏情况组合的可证明保证。由于其黑盒子性质，GeoRobust可以部署在大规模DNNs上，无论它们的体系结构、激活函数和神经元数量如何。实际上，GeoRobust可以在几秒钟内以高精度定位ImageNet上ResNet50模型的最坏几何变换。

    Deep neural networks (DNNs) are known to be vulnerable to adversarial geometric transformation. This paper aims to verify the robustness of large-scale DNNs against the combination of multiple geometric transformations with a provable guarantee. Given a set of transformations (e.g., rotation, scaling, etc.), we develop GeoRobust, a black-box robustness analyser built upon a novel global optimisation strategy, for locating the worst-case combination of transformations that affect and even alter a network's output. GeoRobust can provide provable guarantees on finding the worst-case combination based on recent advances in Lipschitzian theory. Due to its black-box nature, GeoRobust can be deployed on large-scale DNNs regardless of their architectures, activation functions, and the number of neurons. In practice, GeoRobust can locate the worst-case geometric transformation with high precision for the ResNet50 model on ImageNet in a few seconds on average. We examined 18 ImageNet classifiers
    
[^106]: 批量处理变长语音增强系统的训练输入

    On Batching Variable Size Inputs for Training End-to-End Speech Enhancement Systems. (arXiv:2301.10587v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.10587](http://arxiv.org/abs/2301.10587)

    本文系统地研究了不同批处理策略和批量大小对语音增强系统的训练效果的影响，以及如何在如何在实现高网络性能的同时最大化资源利用。

    

    基于神经网络的语音增强系统的性能主要受到模型架构的影响，而训练时间和计算资源的利用则主要受到诸如批量大小之类的训练参数的影响。由于不同的噪声和混响语音组合可以具有不同的持续时间，在培训期间需要一种批处理策略来处理变长输入，特别是对于最先进的端到端系统。这些策略通常力求在零填充和数据随机化之间取得一种折衷，并且可以与动态批量大小相结合，以获得每个批次中一致的数据量。然而，这些策略对资源利用和更重要的网络性能的影响并没有得到很好的记录。本文系统地研究了不同批处理策略和批量大小对Conv-TasNet的训练统计数据和语音增强性能的影响，评估了配对和不匹配的情况下。

    The performance of neural network-based speech enhancement systems is primarily influenced by the model architecture, whereas training times and computational resource utilization are primarily affected by training parameters such as the batch size. Since noisy and reverberant speech mixtures can have different duration, a batching strategy is required to handle variable size inputs during training, in particular for state-of-the-art end-to-end systems. Such strategies usually strive for a compromise between zero-padding and data randomization, and can be combined with a dynamic batch size for a more consistent amount of data in each batch. However, the effect of these strategies on resource utilization and more importantly network performance is not well documented. This paper systematically investigates the effect of different batching strategies and batch sizes on the training statistics and speech enhancement performance of a Conv-TasNet, evaluated in both matched and mismatched co
    
[^107]: 面向高斯过程状态空间模型的灵活性和可解释性的探索

    Towards Flexibility and Interpretability of Gaussian Process State-Space Model. (arXiv:2301.08843v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08843](http://arxiv.org/abs/2301.08843)

    本文提出了一种新的状态空间模型类TGPSSM，利用正规化流增加了标准GPSSM中的GP先验概率，从而增强模型的灵活性和表现力。同时提出了可扩展的变分推理算法，为潜在状态的变分分布提供灵活和最优的结构。

    

    过去十年中，高斯过程状态空间模型（GPSSM）备受关注。然而，GPSSM的模型表达能力远非令人满意。大多数GPSSM研究依赖于标准高斯过程（GP）和预先设置的核心，例如平方指数（SE）核心或Matern核心，这限制了模型的表达能力和在复杂场景中的应用。针对这个问题，本文提出了一种新的概率状态空间模型类，称为TGPSSM。通过利用一个参数化的正规化流，TGPSSM增加了标准GPSSM中的GP先验概率，使状态空间模型更具灵活性和表现力。此外，我们提出了一种可扩展的变分推理算法，用于在TGPSSM中进行学习和推理，为潜在状态的变分分布提供灵活和最优的结构。由于GP的稀疏表示，该算法是可解释和计算高效的。

    The Gaussian process state-space model (GPSSM) has attracted much attention over the past decade. However, the model representation power of the GPSSM is far from satisfactory. Most GPSSM studies rely on the standard Gaussian process (GP) with a preliminary kernel, such as the squared exponential (SE) kernel or Mat\'{e}rn kernel, which limits the model representation power and its application in complex scenarios. To address this issue, this paper proposes a novel class of probabilistic state-space models, called TGPSSMs. By leveraging a parametric normalizing flow, the TGPSSMs enrich the GP priors in the standard GPSSM, rendering the state-space model more flexible and expressive. Additionally, we present a scalable variational inference algorithm for learning and inference in TGPSSMs, which provides a flexible and optimal structure for the variational distribution of latent states. The algorithm is interpretable and computationally efficient owing to the sparse representation of GP a
    
[^108]: 具有联合嵌入预测架构的图像自监督学习

    Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. (arXiv:2301.08243v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.08243](http://arxiv.org/abs/2301.08243)

    本论文提出了一种非生成方法的自监督学习架构，即Image-based Joint-Embedding Predictive Architecture（I-JEPA），可以生成高度语义图像表示，通过联合嵌入预测架构和掩模策略达到这一目的。

    

    本论文提出了一种在不依赖手工制作数据增强的情况下学习高度语义图像表示的方法。我们介绍了基于图像的联合嵌入预测架构（I-JEPA），这是一种从图像中进行自我监督学习的非生成方法。I-JEPA的核心设计选择是掩模策略，以引导I-JEPA产生语义表示。当与Vision Transformers结合使用时，证明I-JEPA具有高度可扩展性。

    This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting a
    
[^109]: 基于Frank-Wolfe优化的高效内存在线学习：具有有界动态遗憾的算法和控制应用

    Efficient Online Learning with Memory via Frank-Wolfe Optimization: Algorithms with Bounded Dynamic Regret and Applications to Control. (arXiv:2301.00497v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00497](http://arxiv.org/abs/2301.00497)

    本文介绍了基于Frank-Wolfe优化的高效内存在线学习算法，该算法以历史决策为基础，适应实时时变环境。该算法广泛应用于动态系统的在线控制，统计套利和时间序列预测等人工智能领域。

    

    投影操作是在线学习中的典型计算瓶颈。本文在在线凸优化的记忆框架中实现了无投影的在线学习。具体地，OCO-M反映了决策历史如何影响当前结果，并允许在线学习损失函数依赖于当前和过去的决策。特别地，我们引入了第一个具有内存的无投影基学习算法，该算法使动态遗憾最小化，即最小化与任何时变决策序列的次优性。本算法以在线Frank-Wolfe（OFW）和Hedge算法为基础，旨在解决人工智能应用中的问题，例如需要在实时中适应时变环境，并考虑过去决策对现在的影响。这些应用包括：动态系统的在线控制，统计套利和时间序列预测。

    Projection operations are a typical computation bottleneck in online learning. In this paper, we enable projection-free online learning within the framework of Online Convex Optimization with Memory (OCO-M) -- OCO-M captures how the history of decisions affects the current outcome by allowing the online learning loss functions to depend on both current and past decisions. Particularly, we introduce the first projection-free meta-base learning algorithm with memory that minimizes dynamic regret, i.e., that minimizes the suboptimality against any sequence of time-varying decisions. We are motivated by artificial intelligence applications where autonomous agents need to adapt to time-varying environments in real-time, accounting for how past decisions affect the present. Examples of such applications are: online control of dynamical systems; statistical arbitrage; and time series prediction. The algorithm builds on the Online Frank-Wolfe (OFW) and Hedge algorithms. We demonstrate how our 
    
[^110]: 深度线性网络中的神经塌陷:从平衡到不平衡的数据

    Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data. (arXiv:2301.00437v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00437](http://arxiv.org/abs/2301.00437)

    研究者证明对于均方误差和交叉熵损失，深度线性网络中出现的全局解在不同数据上都具有神经塌陷的特性，即最后一层特征会崩溃为类均值，而这些类均值是等角紧框架的顶点。

    

    现代深度神经网络在图像分类和自然语言处理等任务中表现出色，但令人惊讶的是，这些具有大量参数的复杂系统在训练到收敛时，它们的最后一层特征和分类器在经典数据集上表现出相同的结构性质。特别地，观察到最后一层特征会崩溃为类均值，并且这些类均值是等角紧框架(simplex Equiangular Tight Frame)的顶点。这种现象被称为神经塌陷(NC)。最近的论文理论上证明了在简化的“无约束特征模型”训练问题的全局最小值中出现了$\mathcal{NC}$。在这个语境下，我们进一步证明了在常用的均方误差(MSE)和交叉熵(CE)损失下，深度线性网络中也会发生$\mathcal{NC}$现象，表明全局解在不同数据上都具有$\mathcal{NC}$的特性。

    Modern deep neural networks have achieved impressive performance on tasks from image classification to natural language processing. Surprisingly, these complex systems with massive amounts of parameters exhibit the same structural properties in their last-layer features and classifiers across canonical datasets when training until convergence. In particular, it has been observed that the last-layer features collapse to their class-means, and those class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is known as Neural Collapse ($\mathcal{NC}$). Recent papers have theoretically shown that $\mathcal{NC}$ emerges in the global minimizers of training problems with the simplified ``unconstrained feature model''. In this context, we take a step further and prove the $\mathcal{NC}$ occurrences in deep linear networks for the popular mean squared error (MSE) and cross entropy (CE) losses, showing that global solutions exhibit $\mathcal{NC}$ properties across
    
[^111]: 无需额外人工注释的数据集生成技术——HandsOff

    HandsOff: Labeled Dataset Generation With No Additional Human Annotations. (arXiv:2212.12645v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.12645](http://arxiv.org/abs/2212.12645)

    介绍了一种名为HandsOff的技术，能够在现有少量标记图像的基础上生成无限数量的带标签的合成图像数据集，避免了其他方法中需要大量人工注释图像的问题，并在多个领域达到了最先进的性能水平。

    

    最近的一些工作利用生成对抗网络(GAN)的表现力生成带标签的合成数据集。这些数据集生成方法通常需要新的合成图像的注释，这迫使从业者寻找注释者，策划一组合成图像，并确保生成的标签的质量。我们引入了HandsOff框架，这是一种在少于50个现有标记图像上训练后能够产生无限数量的合成图像和相应标签的技术。我们的框架通过将GAN反演领域与数据集生成相结合避免了以前工作的实际缺点。我们在多个具有挑战性的领域生成具有丰富像素级标签的数据集，如人脸、汽车、全身姿势和城市驾驶场景。与先前的数据集生成方法和迁移学习相比，我们的方法在语义分割、关键点检测和深度估计方面取得了最先进的性能。

    Recent work leverages the expressive power of generative adversarial networks (GANs) to generate labeled synthetic datasets. These dataset generation methods often require new annotations of synthetic images, which forces practitioners to seek out annotators, curate a set of synthetic images, and ensure the quality of generated labels. We introduce the HandsOff framework, a technique capable of producing an unlimited number of synthetic images and corresponding labels after being trained on less than 50 pre-existing labeled images. Our framework avoids the practical drawbacks of prior work by unifying the field of GAN inversion with dataset generation. We generate datasets with rich pixel-wise labels in multiple challenging domains such as faces, cars, full-body human poses, and urban driving scenes. Our method achieves state-of-the-art performance in semantic segmentation, keypoint detection, and depth estimation compared to prior dataset generation approaches and transfer learning ba
    
[^112]: 使用采样算法估计量子玻色系统的截断效应

    Estimating truncation effects of quantum bosonic systems using sampling algorithms. (arXiv:2212.08546v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2212.08546](http://arxiv.org/abs/2212.08546)

    本文提出了一种使用传统采样方法估计量子玻色系统截断误差的方法，该方法可用于估计实际量子模拟玻色理论所需的资源，并检查对应量子模拟的结果的有效性。

    

    要在基于量子比特或量子位的量子计算机上模拟玻色子，必须通过将无限维局部希尔伯特空间截断为有限维来规范理论。在寻求实际量子应用的过程中，了解截断误差有多大非常重要。通常情况下，除非我们拥有好的量子计算机，否则很难估计误差。本文表明，传统的经典设备采样方法，具体而言是马尔科夫链蒙特卡罗方法可以用现有合理的计算资源解决这个问题。我们以二维格点上的标量场理论为例演示了这个想法，其大小超过了使用确切对角化方法所能达到的范围。这种方法可用于估计实际量子模拟玻色理论所需的资源，并检查对应量子模拟的结果的有效性。

    To simulate bosons on a qubit- or qudit-based quantum computer, one has to regularize the theory by truncating infinite-dimensional local Hilbert spaces to finite dimensions. In the search for practical quantum applications, it is important to know how big the truncation errors can be. In general, it is not easy to estimate errors unless we have a good quantum computer. In this paper we show that traditional sampling methods on classical devices, specifically Markov Chain Monte Carlo, can address this issue with a reasonable amount of computational resources available today. As a demonstration, we apply this idea to the scalar field theory on a two-dimensional lattice, with a size that goes beyond what is achievable using exact diagonalization methods. This method can be used to estimate the resources needed for realistic quantum simulations of bosonic theories, and also, to check the validity of the results of the corresponding quantum simulations.
    
[^113]: 双重精度质量驱动的神经网络用于生成预测区间

    Dual Accuracy-Quality-Driven Neural Network for Prediction Interval Generation. (arXiv:2212.06370v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06370](http://arxiv.org/abs/2212.06370)

    本文提出了一种双重精度质量驱动的神经网络，可以自动地学习基于回归的神经网络的预测区间，而非只提供传统的目标估计。该方法通过设计一种新颖的损失函数，最小化平均预测区间宽度以及提高覆盖概率来提高PI的质量和精度，且比最先进的方法更加具有计算效率。

    

    在深度学习模型在实际应用中，准确的不确定性量化对于提高其可靠性至关重要。对于回归任务，应该在深度学习模型的确定性预测之外提供预测区间(PIs)。只要Pis足够窄而且捕获了大部分的概率密度，这些Pis就是有用的或"高质量"的。本文提出了一种方法，可以自动地为回归神经网络学习预测区间，除了传统的目标预测之外。具体而言，我们训练两个伴侣神经网络：一个使用一个输出，目标估计，另一个使用两个输出，相应PI的上限和下限的值。我们的主要贡献是为生成PI的网络设计了一种新颖的损失函数，该函数考虑了目标估计网络的输出，并且具有两个优化目标：减小平均预测区间宽度和提高Pis的质量(通过其覆盖概率进行测量)。我们在几个回归数据集上评估了我们的方法，并证明了我们的方法可以产生比最先进的方法更准确且质量更高的预测区间，同时又具有计算效率。

    Accurate uncertainty quantification is necessary to enhance the reliability of deep learning models in real-world applications. In the case of regression tasks, prediction intervals (PIs) should be provided along with the deterministic predictions of deep learning models. Such PIs are useful or "high-quality" as long as they are sufficiently narrow and capture most of the probability density. In this paper, we present a method to learn prediction intervals for regression-based neural networks automatically in addition to the conventional target predictions. In particular, we train two companion neural networks: one that uses one output, the target estimate, and another that uses two outputs, the upper and lower bounds of the corresponding PI. Our main contribution is the design of a novel loss function for the PI-generation network that takes into account the output of the target-estimation network and has two optimization objectives: minimizing the mean prediction interval width and e
    
[^114]: 使用任务算术编辑模型

    Editing Models with Task Arithmetic. (arXiv:2212.04089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04089](http://arxiv.org/abs/2212.04089)

    本文提出了一种使用任务向量进行模型编辑的新范式，任务向量可通过算术操作进行修改和组合，可以提高目标任务性能且对控制任务影响较小。

    

    改变预训练模型的行为方式（比如提高其在下游任务上的表现或减轻预训练期间学习到的偏差）是开发机器学习系统时常见的做法。本文提出了一种围绕“任务向量”来引导神经网络行为的新范式。任务向量指定了一个方向，即预训练模型权重空间中的方向，沿着该方向移动可以提高任务的表现。我们通过从经过微调任务后的相同模型的权重中减去预训练模型的权重来构建任务向量。我们展示了这些任务向量可以通过否定和加法等算术操作进行修改和组合，从而引导生成模型的行为。否定任务向量会降低目标任务的性能，而对控制任务的模型行为影响不大。此外，将任务向量相加可以提高目标任务的性能和控制任务的模型行为。

    Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performanc
    
[^115]: 一种距离校准的统一理论

    A Unifying Theory of Distance from Calibration. (arXiv:2211.16886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16886](http://arxiv.org/abs/2211.16886)

    该论文提出了一种基于属性测试思想的严格框架来分析校准度量，提出了一个距离校准的基础真相，提供了三个一致性的校准度量，并在合成数据集和真实数据集的实证研究中证明了这些度量的表现如预期。

    

    我们研究了如何定义和度量概率预测器的距离校准的根本问题。虽然完美校准的概念是被理解的，但没有共识如何量化距离。文献中提出了许多校准度量，但不清楚它们彼此如何比较，许多流行的度量（如预期校准误差）无法满足连续性等基本属性。我们提出了一个严格的框架来分析校准度量，受到属性测试的文献启发。我们提出了一个距离校准的基础真相：到最接近完美校准预测器的$\ell_1$距离。我们定义了一种一致性校准度量，即与该距离多项式相关的度量。应用我们的框架，我们确定了三个一致性的校准度量，可以有效地进行估计：平滑校准、区间校准和最大校准。最后，我们通过合成数据集和真实数据集的实证研究表明，这些度量的表现如预期。

    We study the fundamental question of how to define and measure the distance from calibration for probabilistic predictors. While the notion of perfect calibration is well-understood, there is no consensus on how to quantify the distance from perfect calibration. Numerous calibration measures have been proposed in the literature, but it is unclear how they compare to each other, and many popular measures such as Expected Calibration Error (ECE) fail to satisfy basic properties like continuity.  We present a rigorous framework for analyzing calibration measures, inspired by the literature on property testing. We propose a ground-truth notion of distance from calibration: the $\ell_1$ distance to the nearest perfectly calibrated predictor. We define a consistent calibration measure as one that is polynomially related to this distance. Applying our framework, we identify three calibration measures that are consistent and can be estimated efficiently: smooth calibration, interval calibratio
    
[^116]: 医疗决策问题中的相对稀疏性

    Relative Sparsity for Medical Decision Problems. (arXiv:2211.16566v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2211.16566](http://arxiv.org/abs/2211.16566)

    本文提出了一种在医疗保健中使用的数据驱动决策策略的相对稀疏性方法，该方法帮助解释从当前的标准护理转变为新的护理政策的重要方面。

    

    现有的统计方法可以估计一个策略，或者一个从协变量到决策的映射，然后指导决策者（例如，基于协变量血压和心率是否应该进行低血压治疗）。在医疗保健中使用这些数据驱动的策略具有极大的兴趣。然而，通常重要的是向医疗保健提供者和患者解释如何从当前的标准护理转变为新的护理政策。为此，我们从信任域策略优化（TRPO）中借鉴了想法。然而，在我们的工作中，与TRPO不同的是，在将标准治疗转变为新的建议治疗时，需要强制将策略的方面（即血压和心率的参数）固定在相对较少的位置上，以帮助可解释性。这产生了“相对稀疏”，作为一个调整参数$\lambda$的函数，我们可以确定政策差异的位置和大小。

    Existing statistical methods can estimate a policy, or a mapping from covariates to decisions, which can then instruct decision makers (e.g., whether to administer hypotension treatment based on covariates blood pressure and heart rate). There is great interest in using such data-driven policies in healthcare. However, it is often important to explain to the healthcare provider, and to the patient, how a new policy differs from the current standard of care. This end is facilitated if one can pinpoint the aspects of the policy (i.e., the parameters for blood pressure and heart rate) that change when moving from the standard of care to the new, suggested policy. To this end, we adapt ideas from Trust Region Policy Optimization (TRPO). In our work, however, unlike in TRPO, the difference between the suggested policy and standard of care is required to be sparse, aiding with interpretability. This yields ``relative sparsity," where, as a function of a tuning parameter, $\lambda$, we can ap
    
[^117]: DyNCA：使用神经元自动机的实时动态纹理合成

    DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata. (arXiv:2211.11417v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11417](http://arxiv.org/abs/2211.11417)

    提出了一种名为 DyNCA 的实时可控动态纹理合成框架，可以合成无限长和任意大小的逼真视频纹理，提高了现有结果的逼真程度，并提供了多种实时视频控制功能。

    

    目前的动态纹理合成 (DyTS) 模型可以合成逼真的视频，但它们需要缓慢的迭代优化过程来合成单个固定大小的短视频，并且在合成过程中没有后期控制的功能。我们提出了动态神经元自动机 (DyNCA)，这是一个实时可控的动态纹理合成框架。我们的方法建立在最近引入的 NCA 模型之上，可以实时合成无限长和任意大小的逼真视频纹理。我们定量和定性地评估了我们的模型，并展示了我们合成的视频比现有结果更逼真。我们将 SOTA DyTS 的性能提高了 $2\sim 4$ 个数量级。此外，我们的模型提供了几个实时视频控制功能，包括运动速度、运动方向以及编辑刷工具。我们在一个在线交互式演示中展示了我们训练好的模型，该演示在本地硬件上运行，并可在个人电脑和移动设备上访问。

    Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic videos. However, they require a slow iterative optimization process to synthesize a single fixed-size short video, and they do not offer any post-training control over the synthesis process. We propose Dynamic Neural Cellular Automata (DyNCA), a framework for real-time and controllable dynamic texture synthesis. Our method is built upon the recently introduced NCA models and can synthesize infinitely long and arbitrary-sized realistic video textures in real time. We quantitatively and qualitatively evaluate our model and show that our synthesized videos appear more realistic than the existing results. We improve the SOTA DyTS performance by $2\sim 4$ orders of magnitude. Moreover, our model offers several real-time video controls including motion speed, motion direction, and an editing brush tool. We exhibit our trained models in an online interactive demo that runs on local hardware and is accessible on personal 
    
[^118]: 隐式图谱神经表示法

    Implicit Graphon Neural Representation. (arXiv:2211.03329v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03329](http://arxiv.org/abs/2211.03329)

    本文通过神经网络模型直接对图谱进行建模，得到了隐式图谱神经表示法，它具有可以表示任意分辨率的图谱、能够高效地生成任意大小具有所需结构的图表等优势。

    

    图谱是生成不同大小的图表的通用且强大的模型。本文提出使用神经网络直接对图谱进行建模，得到隐式图谱神经表示法 (IGNR)。现有工作在建模和重构图谱时常通过固定分辨率分段常数表示来逼近目标图谱。我们的IGNR具有一定的优势，可以表示任意分辨率的图谱，并在学习模型后能够自然高效地生成具有所需结构的任意大小的图表。此外，通过利用Gromov-Wasserstein距离，我们允许输入图数据不对齐且具有不同的大小。我们首先通过展示在图谱学习任务上的优异性能来证明我们模型的有效性。然后我们提出了IGNR的扩展，可以被整合到自编码器框架中，并在更一般的图谱学习场景下展示其良好的性能。

    Graphons are general and powerful models for generating graphs of varying size. In this paper, we propose to directly model graphons using neural networks, obtaining Implicit Graphon Neural Representation (IGNR). Existing work in modeling and reconstructing graphons often approximates a target graphon by a fixed resolution piece-wise constant representation. Our IGNR has the benefit that it can represent graphons up to arbitrary resolutions, and enables natural and efficient generation of arbitrary sized graphs with desired structure once the model is learned. Furthermore, we allow the input graph data to be unaligned and have different sizes by leveraging the Gromov-Wasserstein distance. We first demonstrate the effectiveness of our model by showing its superior performance on a graphon learning task. We then propose an extension of IGNR that can be incorporated into an auto-encoder framework, and demonstrate its good performance under a more general setting of graphon learning. We al
    
[^119]: 多智能体协作的展开图学习

    Unrolled Graph Learning for Multi-Agent Collaboration. (arXiv:2210.17101v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17101](http://arxiv.org/abs/2210.17101)

    提出一种受人类协作启发的分布式多智能体学习模型，智能体可以自主检测适合的合作者，并参考合作者的模型以获得更好的性能，使用协作图实现成对协作关系指示，通过展开图学习网络以更灵活适应各种情况地学习潜在合作者之间的相似特征。

    

    多智能体学习越来越受到关注，以应对数据交换受限的分布式机器学习场景。然而，现有的多智能体学习模型通常考虑在智能体之间固定强制性的协作关系下的数据融合，这不如人类协作那样灵活和自治。为弥补这一缺口，我们提出了一个受人类协作启发的分布式多智能体学习模型，在该模型中，智能体可以自主检测适合的合作者，并参考合作者的模型以获得更好的性能。为实现这种适应性协作，我们使用协作图来指示成对协作关系。协作图可以通过基于不同智能体之间的模型相似度的图学习技术来获得。由于模型相似性不能通过固定的图形优化来描述，我们设计了一个展开图学习网络，以更适应各种情况地学习潜在合作者之间的相似特征。

    Multi-agent learning has gained increasing attention to tackle distributed machine learning scenarios under constrictions of data exchanging. However, existing multi-agent learning models usually consider data fusion under fixed and compulsory collaborative relations among agents, which is not as flexible and autonomous as human collaboration. To fill this gap, we propose a distributed multi-agent learning model inspired by human collaboration, in which the agents can autonomously detect suitable collaborators and refer to collaborators' model for better performance. To implement such adaptive collaboration, we use a collaboration graph to indicate the pairwise collaborative relation. The collaboration graph can be obtained by graph learning techniques based on model similarity between different agents. Since model similarity can not be formulated by a fixed graphical optimization, we design a graph learning network by unrolling, which can learn underlying similar features among potent
    
[^120]: 基于预训练语言模型的提示学习在阿尔茨海默病检测中的应用研究

    Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection. (arXiv:2210.16539v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.16539](http://arxiv.org/abs/2210.16539)

    本文提出一种基于提示学习的预训练语言模型方法，加入不流畅特征提高阿尔茨海默病检测性能。实验结果表明该方法在基准数据集上取得了最佳表现，最高准确率达到95.1%。

    

    阿尔茨海默病（AD）的早期诊断对于促进预防性护理和延缓疾病进程非常关键，基于语音的自动AD筛查系统为其他临床筛查技术提供了一种非侵入性且更具扩展性的替代方案。预训练语言模型（PLM）如BERT产生的文本嵌入特征被广泛应用在这样的系统中。然而，PLM领域微调通常基于掩蔽词或句子预测成本，这与后端AD检测任务不一致。因此，本文研究了使用基于提示的PLM微调，这种微调一致地使用AD分类错误作为训练目标函数。在PLM微调期间，在提示短语中进一步加入了基于犹豫或暂停填充符令牌频率的不流畅特征。对于使用不同PLMs（BERT和RoBERTa）或使用不同微调范例（传统机器学习和提示学习）的系统，基于决策投票的组合进一步增强了AD检测的性能。在基准AD语音数据集上，所提出的框架达到了高达95.1%的准确率，表现处于同类研究的最前沿。

    Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating preventive care and to delay further progression. Speech based automatic AD screening systems provide a non-intrusive and more scalable alternative to other clinical screening techniques. Textual embedding features produced by pre-trained language models (PLMs) such as BERT are widely used in such systems. However, PLM domain fine-tuning is commonly based on the masked word or sentence prediction costs that are inconsistent with the back-end AD detection task. To this end, this paper investigates the use of prompt-based fine-tuning of PLMs that consistently uses AD classification errors as the training objective function. Disfluency features based on hesitation or pause filler token frequencies are further incorporated into prompt phrases during PLM fine-tuning. The decision voting based combination among systems using different PLMs (BERT and RoBERTa) or systems with different fine-tuning paradigms (conventional ma
    
[^121]: 基于扩散模型的多任务脑肿瘤修复：一份方法论报告

    Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report. (arXiv:2210.12113v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.12113](http://arxiv.org/abs/2210.12113)

    本文提出了一种基于扩散概率模型的多任务脑肿瘤修复方法，可以同时修复缺失的脑肿瘤区域并为数据增加合成的脑肿瘤，相较于传统的生成对抗网络，该方法能够生成多样化的输出，且在公开数据集上取得了最先进的性能。

    

    尽管越来越多的研究开始将深度学习模型应用于医学影像，但医学数据集的稀缺性和不平衡性可能会严重影响这些模型的性能。生成合成数据是一种解决这些问题的常用技术。修补算法是DL生成模型的子集，可以在匹配周围上下文和某些非成像输入条件的同时更改输入图像的一个或多个区域。虽然绝大多数针对医学影像数据的修补技术都使用生成对抗网络（GANs），但由于GAN的有限输出种类，这些算法的性能经常不尽人意。掩模扩散概率模型(DDPMs)是一种最近推出的生成网络系列，能够生成与GANs相当质量的结果，但具有多样化的输出，同时不需要鉴别器。在本文中，我们提出了一种基于DDPM的多任务脑肿瘤修复方法，可以同时修复缺失的脑肿瘤区域并为数据增加合成的脑肿瘤。我们在一个公开可得的脑肿瘤分割数据集上展示了我们提出的方法的有效性，并取得了最先进的性能。

    Despite the ever-increasing interest in applying deep learning (DL) models to medical imaging, the typical scarcity and imbalance of medical datasets can severely impact the performance of DL models. The generation of synthetic data that might be freely shared without compromising patient privacy is a well-known technique for addressing these difficulties. Inpainting algorithms are a subset of DL generative models that can alter one or more regions of an input image while matching its surrounding context and, in certain cases, non-imaging input conditions. Although the majority of inpainting techniques for medical imaging data use generative adversarial networks (GANs), the performance of these algorithms is frequently suboptimal due to their limited output variety, a problem that is already well-known for GANs. Denoising diffusion probabilistic models (DDPMs) are a recently introduced family of generative networks that can generate results of comparable quality to GANs, but with diver
    
[^122]: 到达全局神经网络抽象与本地精确重构

    Towards Global Neural Network Abstractions with Locally-Exact Reconstruction. (arXiv:2210.12054v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12054](http://arxiv.org/abs/2210.12054)

    GINNACER提出了一种新的全局区间神经网络抽象技术，可以在整个输入域产生准确的上估计边界，同时对于任何给定的局部输入保证精确重构。

    

    神经网络是一类强大的非线性函数。但是，它们的黑匣子性质使得解释它们的行为和验证它们的安全性变得困难。抽象技术通过将神经网络转换为更简单的、过度逼近的函数来解决这个挑战。然而，现有的抽象技术存在不足，限制了它们在输入域的小局部的适用性。本文提出了基于中心精确重构的全域区间神经网络抽象（GINNACER）。我们的新型抽象技术在整个输入域产生准确的过度逼近边界，同时对于任何给定的局部输入保证精确重构。我们的实验表明，GINNACER比最先进的全局抽象技术紧凑了几个数量级，同时与局部技术相竞争。

    Neural networks are a powerful class of non-linear functions. However, their black-box nature makes it difficult to explain their behaviour and certify their safety. Abstraction techniques address this challenge by transforming the neural network into a simpler, over-approximated function. Unfortunately, existing abstraction techniques are slack, which limits their applicability to small local regions of the input domain. In this paper, we propose Global Interval Neural Network Abstractions with Center-Exact Reconstruction (GINNACER). Our novel abstraction technique produces sound over-approximation bounds over the whole input domain while guaranteeing exact reconstructions for any given local input. Our experiments show that GINNACER is several orders of magnitude tighter than state-of-the-art global abstraction techniques, while being competitive with local ones.
    
[^123]: 用核方法探索偏微分方程和算子学习

    A Kernel Approach for PDE Discovery and Operator Learning. (arXiv:2210.08140v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.08140](http://arxiv.org/abs/2210.08140)

    本文提出了一种使用核方法的三步学习和求解偏微分方程的框架，能够近似解新的PDE，并展示了比其他算法更优的表现。

    

    本文介绍了一种使用核方法学习和解决偏微分方程（PDEs）的三步框架。给定一个训练集，包括网格上的噪声PDE解以及源项/边界项的对，利用核平滑技术去噪并近似解的导数。然后利用这些信息在核回归模型中学习PDE的代数形式。学习得到的PDE在基于核的求解器中被用来近似解新的源项/边界项的PDE，从而构成了一个算子学习框架。数值实验将该方法与最先进的算法进行了比较，并展示了其竞争性能。

    This article presents a three-step framework for learning and solving partial differential equations (PDEs) using kernel methods. Given a training set consisting of pairs of noisy PDE solutions and source/boundary terms on a mesh, kernel smoothing is utilized to denoise the data and approximate derivatives of the solution. This information is then used in a kernel regression model to learn the algebraic form of the PDE. The learned PDE is then used within a kernel based solver to approximate the solution of the PDE with a new source/boundary term, thereby constituting an operator learning framework. Numerical experiments compare the method to state-of-the-art algorithms and demonstrate its competitive performance.
    
[^124]: 数据扩充到底有多大作用？缩放定律、不变性及隐式正则化的研究

    How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization. (arXiv:2210.06441v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06441](http://arxiv.org/abs/2210.06441)

    数据扩充针对多样性和不一致性能够比额外的训练数据更有价值，鼓励不变性的数据扩充可以比单一的不变性更有用，数据扩充在训练期间引入了额外的随机性并能够拉平损失函数的曲面。

    

    尽管数据扩充有明显的性能优势，但我们知道的有关数据扩充为何如此有效的知识还很少。在这篇论文中，我们将分离出数据扩充发挥作用的几个关键机制。在增广数据和实际数据之间建立一个汇率，我们发现在分布外测试场景中，产生多样且与数据分布不一致的增广能够比额外的训练数据更有价值。此外，我们发现旨在鼓励不变性的数据扩充对于小型和中型训练集可能比单一的不变性更有价值。随着这一发现，我们展示了数据增广在训练期间引入额外的随机性，从而有效地拉平了损失函数的曲面。

    Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.
    
[^125]: 利用自注意力指导提高扩散模型的样本质量

    Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.00939](http://arxiv.org/abs/2210.00939)

    该论文提出了一种利用自注意力指导的策略来提升扩散模型生成图像的稳定性和质量，具有较高的实用价值。

    

    去噪扩散模型以其出色的生成质量和多样性受到关注。这种成功很大程度上归因于使用分类或文本条件的扩散指导方法，如分类器和无分类器指导。在本文中，我们提出了一个更全面的视角，超越了传统的指导方法。从这个广义的视角出发，我们引入了新的无条件和无监督的策略来提高生成图像的质量。作为一种简单的解决方案，模糊指导改善了中间样本的适用性，使得扩散模型能够以适度的指导尺度生成更高质量的样本。在此基础上，自注意力指导（SAG）利用了扩散模型的中间自注意力映射来增强它们的稳定性和效果。具体而言，SAG在每次迭代中仅对扩散模型关注的区域进行对抗性模糊处理。

    Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity. This success is largely attributed to the use of class- or text-conditional diffusion guidance methods, such as classifier and classifier-free guidance. In this paper, we present a more comprehensive perspective that goes beyond the traditional guidance methods. From this generalized perspective, we introduce novel condition- and training-free strategies to enhance the quality of generated images. As a simple solution, blur guidance improves the suitability of intermediate samples for their fine-scale information and structures, enabling diffusion models to generate higher quality samples with a moderate guidance scale. Improving upon this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy. Specifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteratio
    
[^126]: 有界单纯形结构矩阵分解：算法、可识别性和应用

    Bounded Simplex-Structured Matrix Factorization: Algorithms, Identifiability and Applications. (arXiv:2209.12638v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12638](http://arxiv.org/abs/2209.12638)

    提出了一种新的低秩矩阵分解模型BSSMF，它的矩阵W每列的元素属于给定的区间，而H的列是随机的，推广了NMF和SSMF，适用于矩阵元素属于给定区间的情况下，具有易于理解的分解和离散结构，适用于主题建模和社区检测等应用。

    

    本文提出了一种新的低秩矩阵分解模型，称为有界单纯形结构矩阵分解（BSSMF）。给定一个输入矩阵X和一个分解秩r，BSSMF在矩阵W中寻找具有r列的矩阵和在矩阵H中寻找具有r行的矩阵，使得X≈WH ，其中W的每列中的元素都是有界的，即它们属于给定的区间，而H的列属于概率单纯形，即H是列随机的。BSSMF推广了非负矩阵分解（NMF）和单纯形结构矩阵分解（SSMF）。BSSMF特别适用于输入矩阵X的元素属于给定区间的情况；例如，当X的行表示图像时，或者X是类似Netflix和MovieLens数据集中的评分矩阵时，其中X的元素属于区间[1,5]。单纯形结构矩阵H不仅可以提供易于理解的分解，从而对X的列空间进行软聚类，而且还赋予H的列离散结构，使其非常适合用于如主题建模和社区检测等应用。我们开发了有效的BSSMF优化算法，建立了其可识别性保证，并在合成和实际数据集上展示了BSSMF的有效性。

    In this paper, we propose a new low-rank matrix factorization model dubbed bounded simplex-structured matrix factorization (BSSMF). Given an input matrix $X$ and a factorization rank $r$, BSSMF looks for a matrix $W$ with $r$ columns and a matrix $H$ with $r$ rows such that $X \approx WH$ where the entries in each column of $W$ are bounded, that is, they belong to given intervals, and the columns of $H$ belong to the probability simplex, that is, $H$ is column stochastic. BSSMF generalizes nonnegative matrix factorization (NMF), and simplex-structured matrix factorization (SSMF). BSSMF is particularly well suited when the entries of the input matrix $X$ belong to a given interval; for example when the rows of $X$ represent images, or $X$ is a rating matrix such as in the Netflix and MovieLens datasets where the entries of $X$ belong to the interval $[1,5]$. The simplex-structured matrix $H$ not only leads to an easily understandable decomposition providing a soft clustering of the colu
    
[^127]: 通过后门水印的黑盒数据集所有权验证

    Black-box Dataset Ownership Verification via Backdoor Watermarking. (arXiv:2209.06015v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.06015](http://arxiv.org/abs/2209.06015)

    本文提出了一种通过后门水印技术验证已发布数据集的所有权的方法，以检测其是否被用于训练（可疑的）第三方模型。

    

    深度学习，特别是深度神经网络（DNNs），由于其高效性和高效性，在许多重要应用中被广泛且成功地采用。DNN的快速发展受益于一些高质量数据集（例如ImageNet）的存在，这些数据集允许研究人员和开发者轻松验证其方法的性能。目前，几乎所有现有的已发布数据集都要求它们仅能用于学术或教育目的而非商业目的，但仍然没有很好的方法来确保这一点。在本文中，我们将保护已发布数据集的形式化为验证它们是否被用于训练（可疑的）第三方模型，而防御者只能查询模型，而没有关于其参数和训练细节的信息。

    Deep learning, especially deep neural networks (DNNs), has been widely and successfully adopted in many critical applications for its high effectiveness and efficiency. The rapid development of DNNs has benefited from the existence of some high-quality datasets ($e.g.$, ImageNet), which allow researchers and developers to easily verify the performance of their methods. Currently, almost all existing released datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes without permission. However, there is still no good way to ensure that. In this paper, we formulate the protection of released datasets as verifying whether they are adopted for training a (suspicious) third-party model, where defenders can only query the model while having no information about its parameters and training details. Based on this formulation, we propose to embed external patterns via backdoor watermarking for the ownership verification to protect them. 
    
[^128]: 连接传统和神经网络的长码（CCN）：ConcatenatedAE

    Concatenated Classic and Neural (CCN) Codes: ConcatenatedAE. (arXiv:2209.01701v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2209.01701](http://arxiv.org/abs/2209.01701)

    本论文提出了连接传统和神经网络的长码方法，并设计出具有相同网络参数的NN用于多次进行One-hot编码，以扩展编码维度。实验证明，这种方法有效提高了纠错能力并具有较强的鲁棒性。

    

    研究表明，用于纠错的小型神经网络（NN）可以提高经典信道编码的效果，并解决信道模型变化的问题。我们使用同一组NN来进行一次性编码多个One-hot编码，并通过串联在外层的经典编码中来扩展任何这样结构的编码维度。我们设计了具有相同网络参数的NN，其中每个Reed-Solomon码字符都是不同NN的输入。相对于小型神经编码，显示了显着的块误码概率提高以及对信道模型变化的鲁棒性。

    Small neural networks (NNs) used for error correction were shown to improve on classic channel codes and to address channel model changes. We extend the code dimension of any such structure by using the same NN under one-hot encoding multiple times, then serially-concatenated with an outer classic code. We design NNs with the same network parameters, where each Reed-Solomon codeword symbol is an input to a different NN. Significant improvements in block error probabilities for an additive Gaussian noise channel as compared to the small neural code are illustrated, as well as robustness to channel model changes.
    
[^129]: 差分隐私垂直联邦聚类

    Differentially Private Vertical Federated Clustering. (arXiv:2208.01700v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2208.01700](http://arxiv.org/abs/2208.01700)

    该论文提出了一种差分隐私的垂直联邦聚类算法，它可以在保护数据隐私的同时训练准确模型，是差分隐私垂直联邦k均值聚类的第一个实际解决方案。

    

    在许多应用中，多个方有关同一组用户的私有数据，但在不同的属性集上，服务器希望利用这些数据来训练模型。为了在保护数据主体隐私的同时实现模型学习，我们需要垂直联邦学习（VFL）技术，其中数据方仅共享用于训练模型的信息，而不是私有数据。然而，确保共享的信息在学习准确模型的同时保持隐私非常具有挑战性。据我们所知，本文提出的算法是差分隐私垂直联邦k均值聚类的第一个实际解决方案，其中服务器可以获得具有可证明差分隐私保证的全局中心集。我们的算法假设不受信任的中央服务器从本地数据方聚合不同ially private的局部中心和成员编码。它构建一个加权网格作为全局的摘要。

    In many applications, multiple parties have private data regarding the same set of users but on disjoint sets of attributes, and a server wants to leverage the data to train a model. To enable model learning while protecting the privacy of the data subjects, we need vertical federated learning (VFL) techniques, where the data parties share only information for training the model, instead of the private data. However, it is challenging to ensure that the shared information maintains privacy while learning accurate models. To the best of our knowledge, the algorithm proposed in this paper is the first practical solution for differentially private vertical federated k-means clustering, where the server can obtain a set of global centers with a provable differential privacy guarantee. Our algorithm assumes an untrusted central server that aggregates differentially private local centers and membership encodings from local data parties. It builds a weighted grid as the synopsis of the global
    
[^130]: 有条件蒙热映射的监督式训练

    Supervised Training of Conditional Monge Maps. (arXiv:2206.14262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14262](http://arxiv.org/abs/2206.14262)

    本文提出了一种名为 CondOT 的多任务方法，它使用带有上下文标签 $c_i$ 的多个测量对 $\left(\mu_i, \nu_i\right)$ 来估计一组有条件的 OT 映射，以便将上下文因素引入 OT 估计中。

    

    最优传输（OT）理论描述了将一个概率测量映射到另一个概率测量的最有效方式的一般原则。在许多应用中，如预测细胞对治疗的反应，在定义最优传输问题的输入/输出数据测量对 $(\mu, \nu)$ 不是单独出现的，而是与某个上下文 $c$ 相关联，例如比较未经处理和处理的细胞群体时的治疗。为了将上下文考虑进 OT 估计中，我们引入了 CondOT，这是一种基于多任务的方法，可以通过多个带有上下文标签 $c_i$ 的测量对 $\left(\mu_i, \nu_i\right)$ 来估计一族有条件的 OT 映射。CondOT 学习了一个全局映射 $\mathcal{T}_\theta$，在上下文的控制下生成映射。

    Optimal transport (OT) theory describes general principles to define and select, among many possible choices, the most efficient way to map a probability measure onto another. That theory has been mostly used to estimate, given a pair of source and target probability measures $(\mu, \nu)$, a parameterized map $T_\theta$ that can efficiently map $\mu$ onto $\nu$. In many applications, such as predicting cell responses to treatments, pairs of input/output data measures $(\mu, \nu)$ that define optimal transport problems do not arise in isolation but are associated with a context $c$, as for instance a treatment when comparing populations of untreated and treated cells. To account for that context in OT estimation, we introduce CondOT, a multi-task approach to estimate a family of OT maps conditioned on a context variable, using several pairs of measures $\left(\mu_i, \nu_i\right)$ tagged with a context label $c_i$. CondOT learns a global map $\mathcal{T}_\theta$ conditioned on context th
    
[^131]: 关于带有噪声标签的图像分割问题：Accuracy和Dice的最优解的特性和体积属性的刻画

    On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice. (arXiv:2206.06484v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.06484](http://arxiv.org/abs/2206.06484)

    该论文研究了医学图像分割中常用的Accuracy和Dice测量方法在存在噪声标签的情况下的性能表现，并在探究中获得了结论：两种测量方法的最优解集体积可能显著偏离目标预期体积，Accuracy的解的体积始终小于等于Dice的解的体积，但当可行分割集被限制为体积等于目标预期体积的分割集时，两种指标的最优解是一致的。

    

    本文研究了医学图像分割中两种最流行的性能测量方法Accuracy和Dice在目标标签存在噪声的情况下的表现。对于这两种测量方法，我们证明了与最佳分割集的特性和体积属性有关的多个命题，并提供了相关实验。我们的主要见解是：（i）两种测量方法的解集的体积可能显著偏离目标的预期体积，（ii）Accuracy的解的体积始终小于等于Dice的解的体积，（iii）当可行分割集被限制为体积等于目标预期体积的分割集时，这两种指标的最优解是一致的。

    We study two of the most popular performance metrics in medical image segmentation, Accuracy and Dice, when the target labels are noisy. For both metrics, several statements related to characterization and volume properties of the set of optimal segmentations are proved, and associated experiments are provided. Our main insights are: (i) the volume of the solutions to both metrics may deviate significantly from the expected volume of the target, (ii) the volume of a solution to Accuracy is always less than or equal to the volume of a solution to Dice and (iii) the optimal solutions to both of these metrics coincide when the set of feasible segmentations is constrained to the set of segmentations with the volume equal to the expected volume of the target.
    
[^132]: 终身学习的持续评估：识别稳定性差距。

    Continual evaluation for lifelong learning: Identifying the stability gap. (arXiv:2205.13452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13452](http://arxiv.org/abs/2205.13452)

    终身学习中，时间相关的数据生成分布对神经网络的梯度训练具有困难性。一些最先进的方法在开始学习新任务时会存在轻微的遗忘，随后会有一段性能恢复的阶段跟随，我们称之为“稳定性差距”。

    

    时间相关的数据生成分布已被证明对神经网络的梯度训练是困难的，因为贪婪的更新会导致以前学习的知识灾难性地被遗忘。尽管终身学习领域已经取得了一定进展以克服这种遗忘，但我们发现一组常见的最先进的方法在开始学习新任务时仍然存在严重遗忘，只是这种遗忘是暂时的，会被一段性能恢复的阶段所跟随。我们将这种有趣但潜在有问题的现象称为稳定性差距。由于终身学习模型评估领域仅在每个任务之后进行评估的标准做法，因此稳定性差距可能仍然未被发现。而我们则建立了一个终身评估框架，使用每次迭代的评估，并定义了一组新的指标来量化最坏情况下的性能。实证结果显示，经验回放、基于约束的回放、知识_distillation和_expansion都存在稳定性差距。

    Time-dependent data-generating distributions have proven to be difficult for gradient-based training of neural networks, as the greedy updates result in catastrophic forgetting of previously learned knowledge. Despite the progress in the field of continual learning to overcome this forgetting, we show that a set of common state-of-the-art methods still suffers from substantial forgetting upon starting to learn new tasks, except that this forgetting is temporary and followed by a phase of performance recovery. We refer to this intriguing but potentially problematic phenomenon as the stability gap. The stability gap had likely remained under the radar due to standard practice in the field of evaluating continual learning models only after each task. Instead, we establish a framework for continual evaluation that uses per-iteration evaluation and we define a new set of metrics to quantify worst-case performance. Empirically we show that experience replay, constraint-based replay, knowledg
    
[^133]: 基于RSSI和众包感知的域对抗图卷积网络室内定位方法

    Domain Adversarial Graph Convolutional Network Based on RSSI and Crowdsensing for Indoor Localization. (arXiv:2204.05184v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2204.05184](http://arxiv.org/abs/2204.05184)

    本文提出了一种基于RSSI和众包感知的域对抗图卷积网络室内定位方法，可通过少量标记的站点调查数据和大量未标记的众包WiFi指纹进行训练，能够有效地捕捉数据的拓扑结构，并在基准数据集上实现了显著的定位精度改进。

    

    近年来，使用WiFi指纹进行室内定位越来越受欢迎，这主要是由于WiFi广泛可用和移动通信设备的普及。然而，许多现有的指纹数据集构建方法依赖于耗时费力的数据收集过程。此外，这些方法通常关注理想的实验室环境，而忽略了大型多层建筑的实际挑战。为了解决这些问题，我们提出了一种新的WiDAGCN模型，该模型可以使用少量标记的站点调查数据和大量未标记的众包WiFi指纹进行训练。通过基于测向信号强度指示器（RSSI）构建收发点和WiFi接入点（AP）之间的异构图，我们的模型能够有效地捕捉数据的拓扑结构。我们还结合了图卷积网络（GCNs）来提取图层次特征，并使用域对抗性训练来减少站点调查数据和众包数据之间的域差异。实验结果表明，与现有方法相比，我们提出的方法在基准数据集上实现了显著的定位精度改进。

    In recent years, the use of WiFi fingerprints for indoor positioning has grown in popularity, largely due to the widespread availability of WiFi and the proliferation of mobile communication devices. However, many existing methods for constructing fingerprint datasets rely on labor-intensive and time-consuming processes of collecting large amounts of data. Additionally, these methods often focus on ideal laboratory environments, rather than considering the practical challenges of large multi-floor buildings. To address these issues, we present a novel WiDAGCN model that can be trained using a small number of labeled site survey data and large amounts of unlabeled crowdsensed WiFi fingerprints. By constructing heterogeneous graphs based on received signal strength indicators (RSSIs) between waypoints and WiFi access points (APs), our model is able to effectively capture the topological structure of the data. We also incorporate graph convolutional networks (GCNs) to extract graph-level 
    
[^134]: 带外部记忆的多模态动态连续学习

    Continual Learning of Multi-modal Dynamics with External Memory. (arXiv:2203.00936v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00936](http://arxiv.org/abs/2203.00936)

    本文提出了一种新的连续学习方法，通过在记忆中维护遇到序列模式的描述符来实现，能够有效处理新的行为模式的连续出现。

    

    本文研究了在新的行为模式连续出现时，如何将模型拟合到动态环境中。学习模型能够意识到新的模式出现，但它没有访问单个训练序列的真实模式的信息。目前的连续学习方法无法处理这种情况，因为参数传递受到灾难性干扰的影响，而情节记忆设计需要知道序列的真实模式。我们设计了一种新的连续学习方法，通过在神经情节记忆中维护遇到的序列模式的描述符来克服这两个限制。我们在记忆的注意权重上使用Dirichlet过程先验，以促进模式描述符的有效存储。通过检索先前任务相似模式的描述符，并将此描述符馈入其转移中，我们的方法通过在任务之间传递知识来执行连续学习。

    We study the problem of fitting a model to a dynamical environment when new modes of behavior emerge sequentially. The learning model is aware when a new mode appears, but it does not have access to the true modes of individual training sequences. The state-of-the-art continual learning approaches cannot handle this setup, because parameter transfer suffers from catastrophic interference and episodic memory design requires the knowledge of the ground-truth modes of sequences. We devise a novel continual learning method that overcomes both limitations by maintaining a descriptor of the mode of an encountered sequence in a neural episodic memory. We employ a Dirichlet Process prior on the attention weights of the memory to foster efficient storage of the mode descriptors. Our method performs continual learning by transferring knowledge across tasks by retrieving the descriptors of similar modes of past tasks to the mode of a current sequence and feeding this descriptor into its transitio
    
[^135]: VRL3：一种用于视觉深度强化学习的数据驱动框架

    VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning. (arXiv:2202.10324v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.10324](http://arxiv.org/abs/2202.10324)

    本文提出VRL3，一种数据驱动的框架，可用于解决具有挑战性的视觉深度强化学习任务。该框架包含三个阶段，并能在具有稀疏奖励和逼真视觉输入的手部操纵任务中显著提高样本效率。

    

    本文提出了VRL3，这是一个采用简单设计解决具有挑战性的视觉深度强化学习（DRL）任务的强大数据驱动框架。作者分析了采用数据驱动方法的主要障碍，并提出了一系列的设计原则、新的发现和关于数据驱动视觉DRL的关键见解。该框架包括三个阶段：在第一阶段，作者利用非RL数据集（例如ImageNet）来学习任务无关的视觉表示；在第二阶段，作者利用离线RL数据（例如有限数量的专家演示）将任务无关的表示转化为更强大的任务特定的表示；在第三阶段，作者通过在线RL对智能体进行微调。在一组具有稀疏奖励和逼真视觉输入的挑战性手部操纵任务中，与之前的SOTA相比，VRL3的样本效率平均提高了780%。在最难的任务上，VRL3的样本效率提高了1220%（使用更宽的编码器，提高到2440%），并以超过SOTA的性能解决了该任务。

    We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with on
    
[^136]: Quantus: 一个可解释的AI工具包，用于负责任评估神经网络的解释和更多应用

    Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond. (arXiv:2202.06861v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06861](http://arxiv.org/abs/2202.06861)

    本文介绍了Quantus，一个可解释的AI工具包，用于详尽迅速地评估神经网络预测的解释表现，并提高领域内的透明度和可重复性。

    

    解释方法的评估是一个尚未深入探讨的研究课题，然而，由于可解释性被认为能增强人们对人工智能的信任，有必要系统地审核和比较解释方法以确认其正确性。到目前为止，还没有专注于XAI评估的工具，能够详尽迅速地让研究者评估神经网络预测解释的表现。为了增加领域内的透明度和可重复性，我们建立了Quantus，一个全面的Python评估工具包，其中包括一个不断增长的、良好组织的评估指标和解释方法评估的教程集合。该工具包经过了彻底的测试，可在PyPi下或https://github.com/understandable-machine-intelligence-lab/Quantus/上以开源许可证获得。

    The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus -- a comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).
    
[^137]: 高斯分布之间的Schr\"odinger桥具有闭合形式

    The Schr\"odinger Bridge between Gaussian Measures has a Closed Form. (arXiv:2202.05722v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05722](http://arxiv.org/abs/2202.05722)

    本文提出了高斯测度之间的Schr\"odinger桥的闭合形式表达式，解决了动态最优输运问题，在机器学习中具有重要的应用价值。

    

    高斯分布之间的静态最优输运问题（$\mathrm{OT}$）寻求恢复最优映射或更广义的耦合，将一个高斯分布变为另一个。已经有很多研究并应用于各种任务。本文关注OT的动态公式，也称为Schr\"odinger桥（SB）问题，由于与基于扩散的生成模型的联系，在机器学习中最近引起了兴趣。与静态设置相比，即使对于高斯分布，动态设置也知之甚少。在本文中，我们为高斯测度之间的SB提供了闭合形式表达式。与静态高斯OT问题不同，它可以简单地缩减为研究凸程序，我们求解SB的框架需要更多复杂的工具，如黎曼几何和生成器理论等。值得注意的是，我们建立起高斯测度之间SB的解本身是带有高斯过程的。

    The static optimal transport $(\mathrm{OT})$ problem between Gaussians seeks to recover an optimal map, or more generally a coupling, to morph a Gaussian into another. It has been well studied and applied to a wide variety of tasks. Here we focus on the dynamic formulation of OT, also known as the Schr\"odinger bridge (SB) problem, which has recently seen a surge of interest in machine learning due to its connections with diffusion-based generative models. In contrast to the static setting, much less is known about the dynamic setting, even for Gaussian distributions. In this paper, we provide closed-form expressions for SBs between Gaussian measures. In contrast to the static Gaussian OT problem, which can be simply reduced to studying convex programs, our framework for solving SBs requires significantly more involved tools such as Riemannian geometry and generator theory. Notably, we establish that the solutions of SBs between Gaussian measures are themselves Gaussian processes with 
    
[^138]: 不完美信息博弈中的近似最优学习

    Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01752](http://arxiv.org/abs/2202.01752)

    本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。

    

    本文解决了学习不完美信息广义博弈的近似最优算法设计的开放性问题。我们提出了第一种算法系列，仅需要 $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ 局游戏即可在两人零和博弈中找到一个 $\varepsilon$-近似纳什均衡，其中 $X,Y$ 是信息集的数量，$A,B$ 是两名玩家的行动数。这比已知的样本复杂度 $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ 有着 $\widetilde{\mathcal{O}}(\max\{X, Y\})$ 的巨大改进，并且在对数因子内与信息理论下限一致。我们通过两种新算法实现了这种样本复杂度：平衡在线镜面下降和平衡反事实后悔最小化。这两种算法都依赖于将“平衡探索策略”集成到它们的经典对手中的新方法。此外，我们还将我们的结果扩展到了更广泛的支持不完美信息博弈的二人博弈和多人博弈中。

    This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
    
[^139]: 因子增强的树集合方法

    Factor-augmented tree ensembles. (arXiv:2111.14000v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.14000](http://arxiv.org/abs/2111.14000)

    本文提出了一种因子增强的树集合方法，能够处理多种不规则预测变量，为处理宏观金融问题提供一种可靠的方法。

    

    本文提出了利用状态空间方法提取潜在稳态因子来扩展时间序列回归树信息集的方法。通过这样做，该方法将时间序列回归树的应用扩展到两个方面。第一，它可以处理测量误差、非平稳趋势、季节性和/或缺失观测等不规则的预测变量。第二，它提供了一种明确的利用领域专业理论来指导时间序列回归树的方法。实证结果表明，这些因子增强的树集合方法在宏观金融问题方面提供了一种可靠的方法。本文重点介绍了美国股票波动率与商业周期之间的先导滞后效应。

    This manuscript proposes to extend the information set of time-series regression trees with latent stationary factors extracted via state-space methods. In doing so, this approach generalises time-series regression trees on two dimensions. First, it allows to handle predictors that exhibit measurement error, non-stationary trends, seasonality and/or irregularities such as missing observations. Second, it gives a transparent way for using domain-specific theory to inform time-series regression trees. Empirically, ensembles of these factor-augmented trees provide a reliable approach for macro-finance problems. This article highlights it focussing on the lead-lag effect between equity volatility and the business cycle in the United States.
    
[^140]: 通过评估高阶空间上下文的再现性评估图像深度生成模型的方法

    A Method for Evaluating Deep Generative Models of Images via Assessing the Reproduction of High-order Spatial Context. (arXiv:2111.12577v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.12577](http://arxiv.org/abs/2111.12577)

    本研究提出了一种评估图像深度生成模型的方法，通过检测经过训练的GAN生成的图像中高阶空间上下文的再现能力，并验证了多个客观测试以评估不同GAN的质量。

    

    深度生成模型（DGM）可以改变诊断成像领域。生成对抗网络（GAN）是一种广泛使用的DGM。然而，将GAN和其他DGM应用于需要领域专业知识才能使用生成图像的任何应用程序时，普遍存在缺乏评估生成图像领域相关质量的充分或自动化手段的问题。本文展示了针对两种流行GAN架构输出的图像的几个客观测试。我们设计了几个随机上下文模型（SCM）来恢复在经过训练的GAN生成后可以恢复的不同的图像特征。其中一些特征是高阶算法像素排列规则，这些规则不易表达为协方差矩阵。我们设计并验证了统计分类器，以便检测已知排列规则的特定效应。然后，我们测试了两种不同GAN正确复现高阶空间上下文的比率。

    Deep generative models (DGMs) have the potential to revolutionize diagnostic imaging. Generative adversarial networks (GANs) are one kind of DGM which are widely employed. The overarching problem with deploying GANs, and other DGMs, in any application that requires domain expertise in order to actually use the generated images is that there generally is not adequate or automatic means of assessing the domain-relevant quality of generated images. In this work, we demonstrate several objective tests of images output by two popular GAN architectures. We designed several stochastic context models (SCMs) of distinct image features that can be recovered after generation by a trained GAN. Several of these features are high-order, algorithmic pixel-arrangement rules which are not readily expressed in covariance matrices. We designed and validated statistical classifiers to detect specific effects of the known arrangement rules. We then tested the rates at which two different GANs correctly rep
    
[^141]: 一步式诱导式多目标学习及其在乳腺癌肿瘤分割中的应用

    One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.10325](http://arxiv.org/abs/2110.10325)

    本论文提出了一种新的机器学习方法——一步式诱导式多目标学习与DiNS（OSAMTL-DiNS），以处理医学组织病理学全幻灯片图像分析中的复杂噪声标签。在乳腺癌肿瘤分割中得到了成功应用。

    

    近年来的研究表明，机器学习和逻辑推理的结合，包括数据驱动的逻辑推理、知识驱动的机器学习和诱导学习，在发明先进的人工智能技术方面具有很高的有效性。在医学组织病理学全幻灯片图像分析中，一步式诱导式多目标学习（OSAMTL），作为一种受诱导学习启发的方法，通过以一种平衡的方式简单地结合机器学习和逻辑推理，已经证明了其处理单个嘈杂标签的复杂噪声标签的有效性。但是，OSAMTL不适用于提供多种嘈杂样本（DiNS）的学习任务情况。在本文中，我们给出了DiNS的定义，并提出了一步式诱导式多目标学习与DiNS（OSAMTL-DiNS），以扩展原始的OSAMTL以处理DiNS的复杂噪声标签。将OSAMTL-DiNS应用于MHWSIA中的乳腺癌肿瘤分割中，我们展示了其有效性。

    Recent studies have demonstrated the effectiveness of the combination of machine learning and logical reasoning, including data-driven logical reasoning, knowledge driven machine learning and abductive learning, in inventing advanced artificial intelligence technologies. One-step abductive multi-target learning (OSAMTL), an approach inspired by abductive learning, via simply combining machine learning and logical reasoning in a one-step balanced way, has as well shown its effectiveness in handling complex noisy labels of a single noisy sample in medical histopathology whole slide image analysis (MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy samples (DiNS) are provided for a learning task. In this paper, giving definition of DiNS, we propose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels of DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show 
    
[^142]: 自适应联合分布学习

    Adaptive joint distribution learning. (arXiv:2110.04829v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.04829](http://arxiv.org/abs/2110.04829)

    该论文提出了一种自适应联合分布学习的框架，可以从大量数据点中估计低维、归一化和正的Radon-Nikodym导数模型，并在不同学习问题上取得了良好的结果。

    

    我们开发了一个新的框架，用于将联合概率分布嵌入张量积再生核希尔伯特空间（RKHS）中。我们的框架可以容纳一个低维、归一化和正的Radon-Nikodym导数模型，该模型可以从多达数百万个数据点的样本大小中进行估计，减轻了RKHS建模的固有限制。我们的方法自然产生了定义良好的归一化和正的条件分布。嵌入计算速度快且适用于从预测到分类的各种学习问题。我们的理论结果得到了有益的数值结果的支持。

    We develop a new framework for embedding joint probability distributions in tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized and positive model of a Radon-Nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. The embedding is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.
    
[^143]: 将知识蒸馏技术应用在多语言社交事件检测中的研究

    Transferring Knowledge Distillation for Multilingual Social Event Detection. (arXiv:2108.03084v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.03084](http://arxiv.org/abs/2108.03084)

    本研究提出了一种基于GNN的多语言社交事件检测方法，该方法利用了跨语言词嵌入技术，能够对多种语言的数据进行检测，对于较少使用的语言也有较好的应用效果。

    

    最近，基于图神经网络（GNN）的社交事件检测任务显示出了良好的性能。然而，大多数研究都是面向具有大量训练样本的语言的单语言数据，这导致了更常见的多语言环境和更少使用的语言的相对较少研究。因此，我们提出了一种GNN，该网络结合跨语言词嵌入以便在多语言数据流中检测事件。首先的突破在于让GNN能够处理多语言数据。为此，我们概述了一种构建策略，该策略在节点和语义级别上对不同语言中的消息进行了对齐。通过合并相同但在不同语言中表示的实体来建立消息之间的关系。非英语消息表示通过跨语言词嵌入转换为英语语义空间。然后，GNN模型统一编码生成结果消息图。

    Recently published graph neural networks (GNNs) show promising performance at social event detection tasks. However, most studies are oriented toward monolingual data in languages with abundant training samples. This has left the more common multilingual settings and lesser-spoken languages relatively unexplored. Thus, we present a GNN that incorporates cross-lingual word embeddings for detecting events in multilingual data streams. The first exploit is to make the GNN work with multilingual data. For this, we outline a construction strategy that aligns messages in different languages at both the node and semantic levels. Relationships between messages are established by merging entities that are the same but are referred to in different languages. Non-English message representations are converted into English semantic space via the cross-lingual word embeddings. The resulting message graph is then uniformly encoded by a GNN model. In special cases where a lesser-spoken language needs 
    
[^144]: 学习具有一般干预模式面板数据中的治疗效应

    Learning Treatment Effects in Panels with General Intervention Patterns. (arXiv:2106.02780v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.02780](http://arxiv.org/abs/2106.02780)

    本文提出了一种针对面板数据的因果推断问题的解决方案，利用合成对照的方法进行处理。文章拓展了该框架，使其适用性更加广泛，并在计算实验中展现了它的优越性能。

    

    面板数据中的因果推断问题是一个中心计量经济学问题。本文研究的是一个基本版本的面板数据因果推断问题：设$M^*$为低秩矩阵，$E$为零均值噪声矩阵。对于一个具有$\{0,1\}$值的“治疗”矩阵$Z$，我们观测到矩阵$O$，其中$O_{ij} := M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$，其中$\mathcal{T}_{ij}$是未知的异质性治疗效应。这个问题需要我们估计平均治疗效应$\tau^*:=\sum_{ij} \mathcal{T}_{ij} Z_{ij} / \sum_{ij} Z_{ij}$。合成对照范例提供了一种估计$\tau^*$的方法，当$Z$仅仅支持单个行时。本文将该框架扩展到允许一般的$Z$的速率最优恢复$\tau^*$，从而广泛扩展了它的适用性。我们的保证是在这个广泛的设置中第一次出现的。合成和真实数据上的计算实验表明，我们的估计器相对于竞争估计器具有重大优势。

    The problem of causal inference with panel data is a central econometric question. The following is a fundamental version of this problem: Let $M^*$ be a low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix $Z$ with entries in $\{0,1\}$ we observe the matrix $O$ with entries $O_{ij} := M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$ where $\mathcal{T}_{ij} $ are unknown, heterogenous treatment effects. The problem requires we estimate the average treatment effect $\tau^* := \sum_{ij} \mathcal{T}_{ij} Z_{ij} / \sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to estimating $\tau^*$ when $Z$ places support on a single row. This paper extends that framework to allow rate-optimal recovery of $\tau^*$ for general $Z$, thus broadly expanding its applicability. Our guarantees are the first of their type in this general setting. Computational experiments on synthetic and real-world data show a substantial advantage over competing estimators.
    
[^145]: 混合整数规划优化整数值神经网络的最佳训练方法

    Optimal training of integer-valued neural networks with mixed integer programming. (arXiv:2009.03825v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.03825](http://arxiv.org/abs/2009.03825)

    本文介绍了一种新的混合整数规划方法来训练整数值神经网络，其中包括优化神经元数量的方法和鼓励NN更加稀疏的正则化项方法，可以在不使用GPU或复杂超参数调整的情况下提高训练效率和泛化性能。

    

    最近的研究表明，使用混合整数规划求解器可以优化神经网络的某些方面。但是使用混合整数规划求解器进行神经网络训练的方法尚未得到广泛研究。目前的优化神经网络训练的方法通常基于梯度的方法，并需要大量数据、在GPU上进行计算和广泛的超参数调整。相比之下，使用混合整数规划求解器进行训练不需要GPU或繁琐的超参数调整，但目前只能处理少量的数据。本文在最近使用混合整数规划求解器训练二进制神经网络的进展基础上，提出了新的混合整数规划模型，使训练效率得到改善，并可以训练重要的整数值神经网络。我们提供了两种新方法来进一步发挥使用混合整数规划进行神经网络训练的潜在重要性，第一种方法在训练的同时优化NN中神经元的数量，这减少了在训练之前决定网络结构的需要，并可以节省大量的时间和产生更好的结果。第二种方法引入了一个正则化项，鼓励训练的NN更加稀疏，这可以提高泛化性能。

    Recent work has shown potential in using Mixed Integer Programming (MIP) solvers to optimize certain aspects of neural networks (NNs). However the intriguing approach of training NNs with MIP solvers is under-explored. State-of-the-art-methods to train NNs are typically gradient-based and require significant data, computation on GPUs, and extensive hyper-parameter tuning. In contrast, training with MIP solvers does not require GPUs or heavy hyper-parameter tuning, but currently cannot handle anything but small amounts of data. This article builds on recent advances that train binarized NNs using MIP solvers. We go beyond current work by formulating new MIP models which improve training efficiency and which can train the important class of integer-valued neural networks (INNs). We provide two novel methods to further the potential significance of using MIP to train NNs. The first method optimizes the number of neurons in the NN while training. This reduces the need for deciding on netwo
    
[^146]: 深度神经网络中的自适应估计器显示信息压缩

    Adaptive Estimators Show Information Compression in Deep Neural Networks. (arXiv:1902.09037v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1902.09037](http://arxiv.org/abs/1902.09037)

    本文使用自适应估计技术研究深度网络中的信息压缩，发现相比使用饱和激活函数，非饱和激活函数的网络能实现可比的任务性能水平，但无法显示出信息压缩。

    

    为了改善神经网络的功能，理解它们的学习过程至关重要。深度学习的信息瓶颈理论提出，神经网络通过压缩它们的表示来忽略与任务无关的信息，从而实现良好的泛化性能。然而，对于这个理论的经验证据是相互矛盾的，因为只有当网络使用饱和激活函数时才观察到压缩。相反，具有非饱和激活函数的网络实现了可比较的任务性能水平，但没有显示出压缩。在本文中，我们开发了更强大的互信息估计技术，适应于神经网络的隐藏活动，并产生更敏感的从所有函数中激活的测量结果，特别是无界函数。利用这些自适应估计技术，我们研究了带有不同激活函数的网络中的压缩情况。首先，我们使用了两种改进的估计方法，...

    To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we 
    

