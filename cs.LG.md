# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise](https://rss.arxiv.org/abs/2402.01567) | 本文通过在线学习的视角，揭示了Adam优化器的构成和算法组成的重要性，发现Adam实际上是伪装成FTRL的，研究了在线学习的角度对其算法组成的好处。 |
| [^2] | [Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum](https://rss.arxiv.org/abs/2402.01297) | 我们通过推导核矩阵的特征数界限，增强了核岭回归的测试误差界限。对于多项式谱衰减的核，我们恢复了先前的结果；对于指数谱衰减，我们提出了新的非平凡的界限。我们的研究表明，特征谱衰减多项式的核回归器具有良好的泛化能力，而特征谱指数衰减的核回归器则具有灾难性的过拟合。 |
| [^3] | [Simulation of Graph Algorithms with Looped Transformers](https://rss.arxiv.org/abs/2402.01107) | 本文研究了使用循环变压器网络模拟图算法的能力，证明了该结构可以模拟Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法，并展示了其具有图灵完整性的结果。 |
| [^4] | [Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning](https://rss.arxiv.org/abs/2402.01057) | 本文介绍了一种针对单演示模仿学习的新方法TDIL，通过引入基于转换鉴别器的替代奖励函数，鼓励代理向靠近专家状态的状态导航，有效解决了奖励信号稀疏的问题。 |
| [^5] | [A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours](https://arxiv.org/abs/2404.00618) | 提出了一种多分支径向基网络方法来成功预测复杂混沌行为，通过独特的神经网络架构并引入了注意机制，展示了先进机器学习算法在阐明中的潜力 |
| [^6] | [Croissant: A Metadata Format for ML-Ready Datasets](https://arxiv.org/abs/2403.19546) | Croissant是一种面向机器学习数据集的元数据格式，使数据集更易发现、可移植和互操作，有助于解决ML数据管理和负责任AI中的重要挑战。 |
| [^7] | [Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models](https://arxiv.org/abs/2403.17902) | Serpent提出了一种新的图像恢复架构，利用状态空间模型在全局感受野和计算效率之间取得平衡，实现了与最先进技术相当的重建质量，但计算量减少了数个数量级。 |
| [^8] | [Text clustering with LLM embeddings](https://arxiv.org/abs/2403.15112) | 研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率 |
| [^9] | [CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification](https://arxiv.org/abs/2403.11904) | 该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。 |
| [^10] | [Recurrent Drafter for Fast Speculative Decoding in Large Language Models](https://arxiv.org/abs/2403.09919) | 本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。 |
| [^11] | [Representing Molecules as Random Walks Over Interpretable Grammars](https://arxiv.org/abs/2403.08147) | 提出了一种新颖的分子表示模型，使用可解释的图文法描述分子的层次化设计空间，实现了在设计空间上的随机游走，从而提高了分子生成和属性预测的性能、效率和可合成性。 |
| [^12] | [On the Last-Iterate Convergence of Shuffling Gradient Methods](https://arxiv.org/abs/2403.07723) | 该论文证明了针对目标函数的洗牌梯度方法最后迭代的收敛速率，弥合了在不同设置中最后迭代的良好性能与现有理论之间的差距。 |
| [^13] | [Advantage-Aware Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2403.07262) | 介绍了一种新的适应优势的策略优化（A2PO）方法，用于离线学习，能够解决多行为策略收集的约束冲突问题，有效避免过拟合问题。 |
| [^14] | [Unity by Diversity: Improved Representation Learning in Multimodal VAEs](https://arxiv.org/abs/2403.05300) | 通过软约束取代硬约束，提出了一种新的专家混合先验，改善了多模态VAEs中的表示学习。 |
| [^15] | [Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records](https://arxiv.org/abs/2403.04086) | 该研究提出了一种名为AutoDP的自动化方法，用于在电子健康记录数据中为联合疾病预测搜索最佳配置。 |
| [^16] | [You Need to Pay Better Attention](https://arxiv.org/abs/2403.01643) | 提出了三种新的注意力机制，在效率和学习能力方面优于标准的多头注意力，提高了Transformer模型的性能和更广泛的部署能力。 |
| [^17] | [Near-optimal Per-Action Regret Bounds for Sleeping Bandits](https://arxiv.org/abs/2403.01315) | 该论文提出了针对睡眠臂决策问题的接近最优每次行动遗憾界，通过直接最小化每次行动遗憾，使用Generalized EXP3、EXP3-IX和Tsallis entropy下的FTRL方法，获得了较之现有方法更好的界。 |
| [^18] | [UniTS: Building a Unified Time Series Model](https://arxiv.org/abs/2403.00131) | UNITS是一种统一的时间序列模型，通过独特的统一网络骨干实现了通用任务规范，并成功支持多种任务，包括分类、预测、插补和异常检测。 |
| [^19] | [RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences](https://arxiv.org/abs/2402.17257) | RIME是一种针对嘈杂偏好的健壮PbRL算法，通过动态过滤去噪偏好和热启动奖励模型，极大增强了现有最先进PbRL方法的鲁棒性。 |
| [^20] | [Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees](https://arxiv.org/abs/2402.17106) | 该论文提出了一种针对数据集特性量身定制的近似公平性-准确性权衡曲线计算方法，能够有效减轻训练多个模型的计算负担并提供了严格的统计保证 |
| [^21] | [Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159) | 本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。 |
| [^22] | [Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2402.14800) | 引入了专家级稀疏化技术，提出了专家修剪和跳过的后训练方法，以提高MoE LLMs的部署效率，同时保持模型性能。 |
| [^23] | [Position Paper: Challenges and Opportunities in Topological Deep Learning](https://arxiv.org/abs/2402.08871) | 拓扑深度学习将拓扑特征引入深度学习模型，可作为图表示学习和几何深度学习的补充，给各种机器学习环境提供了自然选择。本文讨论了拓扑深度学习中的开放问题，并提出了未来的研究机会。 |
| [^24] | [Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees](https://arxiv.org/abs/2402.08090) | 本论文提出了扩展线性化收缩动力学（ELCD），是第一个具有全局收缩性保证的神经网络动力系统，通过参数化非线性向量场的扩展线性化实现。通过在数据空间和潜在空间之间训练微分同胚，并在潜在空间中强制收缩性，ELCD能在面对不确定性时保持全局稳定性和鲁棒性。 |
| [^25] | [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://arxiv.org/abs/2402.07865) | 本论文探索了视觉条件化语言模型（VLMs）设计的关键空间，并提供了一套标准化评估，同时还研究了预训练的视觉表示和权衡的问题。 |
| [^26] | [Thresholded Oja does Sparse PCA?](https://arxiv.org/abs/2402.07240) | 阈值和重新归一化Oja算法的输出可获得一个接近最优的错误率，与未经阈值处理的Oja向量相比，这大大减小了误差。 |
| [^27] | [Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss](https://arxiv.org/abs/2402.05928) | 本文研究了依赖学习理论中的尖锐率，主要是为了避免样本大小缩减对方差产生影响。当假设类别的拓扑结构符合某些条件时，经验风险最小化者的性能与类别的复杂性和二阶统计量有关。 |
| [^28] | [Interpretable classifiers for tabular data via discretization and feature selection](https://arxiv.org/abs/2402.05680) | 通过离散化和特征选择的方法，我们提出了一种从表格数据中计算出准确又易解释的分类器的方法。在实验证明该方法在准确度上与随机森林和XGBoost等现有方法相当，并且在多种情况下实际上超过了参考结果。 |
| [^29] | [Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains](https://arxiv.org/abs/2402.05140) | 本文介绍了一种将通用的LLMs应用于专业领域的方法，通过学习自定义的输入标签来对LLMs进行条件约束。通过明确将任务领域与任务功能分离，这种方法能够改善在专业领域中的任务求解能力。 |
| [^30] | [Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning](https://arxiv.org/abs/2402.03917) | 这篇论文解决了冷启动场景的无示例增量学习的问题，提出了一种弹性特征整合的方法，通过规范特征漂移并利用原型来减少任务新鲜度偏差。 |
| [^31] | [Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2402.03286) | 本文提出了一种无需训练的方法ConsiStory，通过共享预训练模型的内部激活，实现了一致的文本到图像生成。引入了主题驱动的共享注意力块和基于对应的特征注入，促进了图像之间的主题一致性，并采用了策略来保持布局多样性。 |
| [^32] | [Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models](https://arxiv.org/abs/2402.03271) | 通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。 |
| [^33] | [Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions](https://arxiv.org/abs/2402.02686) | 本研究提出了一种名为多区域马尔可夫高斯过程的方法，将高斯过程和线性动态系统相结合，有效地发现了多个脑区之间的方向性通讯。通过建立LDS与多输出GP之间的联系，该模型实现了线性推断并提供了可解释的低维表示。 |
| [^34] | [LQER: Low-Rank Quantization Error Reconstruction for LLMs](https://arxiv.org/abs/2402.02446) | LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。 |
| [^35] | [EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics](https://arxiv.org/abs/2402.02425) | EuLagNet提出了一种新的拉格朗日引导范式，通过跟踪多尺度关键粒子的运动来捕捉多尺度流体动力学。这种方法克服了由于欧拉观察而导致的流体动力学困难，为准确预测未来的流体提供了一种有效的方法。 |
| [^36] | [Defining Neural Network Architecture through Polytope Structures of Dataset](https://arxiv.org/abs/2402.02407) | 本文通过定义上下界确定神经网络宽度，与数据集的几何复杂性相关。同时开发了一种算法，可以从训练好的神经网络中推断数据集的多面体结构。 |
| [^37] | [APIServe: Efficient API Support for Large-Language Model Inferencing](https://arxiv.org/abs/2402.01869) | APIServe是针对API增强的大型语言模型推理的一个高效工具，它最大限度地减少了由 API 调用引起的 GPU 资源浪费，并提高了整体服务吞吐量。 |
| [^38] | [Drug Discovery with Dynamic Goal-aware Fragments](https://arxiv.org/abs/2310.00841) | 提出了一种名为GEAM的分子生成框架，用于药物发现。该框架通过构建目标感知片段词汇，识别贡献于所需目标特性的重要片段，并在生成过程中更新片段词汇。 |
| [^39] | [CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process.](http://arxiv.org/abs/2401.14535) | CaRiNG提出了一种基于可辨识性理论的方法，用于学习具有非可逆生成过程的时间因果表示。这种方法能够恢复独立的潜在组分，即使它们来自于非线性且非可逆的混合过程。 |
| [^40] | [Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction.](http://arxiv.org/abs/2401.10189) | 这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。 |
| [^41] | [GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting.](http://arxiv.org/abs/2401.09881) | GA-SmaAt-GNet是一种生成对抗架构，用于改进极端降水暂时预测的深度学习模型性能。它通过创建一个新的生成器和使用注意力增强鉴别器来利用降水掩码提供的额外信息，提高了预测质量。 |
| [^42] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^43] | [Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy.](http://arxiv.org/abs/2312.12728) | 本研究介绍了一种通用的推理加速框架，用于提高大型语言模型（LLMs）的推理速度，并在保持生成准确性的同时降低成本。该框架在支付宝的检索增强生成（RAG）系统中得到了应用。 |
| [^44] | [Absolute Policy Optimization.](http://arxiv.org/abs/2310.13230) | 这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。 |
| [^45] | [On the Representational Capacity of Recurrent Neural Language Models.](http://arxiv.org/abs/2310.12942) | 本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。 |
| [^46] | [Prompt Injection Attacks and Defenses in LLM-Integrated Applications.](http://arxiv.org/abs/2310.12815) | 本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。 |
| [^47] | [Provably Robust Cost-Sensitive Learning via Randomized Smoothing.](http://arxiv.org/abs/2310.08732) | 本研究通过随机平滑认证框架，为成本敏感的稳健分类器提供了严格的稳健性保证，并通过优化方案针对不同数据子组设计了细粒度认证半径，取得了优越的性能。 |
| [^48] | [Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making.](http://arxiv.org/abs/2310.03022) | Decision ConvFormer提出了一种新的动作序列预测器，通过使用本地卷积过滤来捕捉强化学习数据集中的局部关联，同时在各个标准RL基准上取得了最先进的性能。 |
| [^49] | [Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques.](http://arxiv.org/abs/2309.16733) | 这篇论文系统调查了深度学习应用对底层硬件故障的韧性，并提供了未来研究的方向。 |
| [^50] | [Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator.](http://arxiv.org/abs/2309.15769) | 本文研究了普通最小二乘插值器在高维环境中的代数和统计属性，并为最小l2范数OLS插值器提供了基本结果。这些结果对理解OLS插值器的泛化能力具有重要意义。 |
| [^51] | [Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification.](http://arxiv.org/abs/2309.10211) | 使用人生成的因果知识来改进数据表示可以提高神经网络在复杂分类任务中的性能，指示了改进机器学习系统开发实践的重要性。 |
| [^52] | [Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms.](http://arxiv.org/abs/2309.00591) | 本文提出了一种名为 ROBAI 的算法，旨在快速识别并选择最佳臂，并在一系列连续回合中最大化奖励。该算法在预定停止时间和自适应停止时间要求下均实现了渐进最优遗憾，并且在预定停止时间下仅需 $\mathcal{O}(\log T)$ 回合即可选择最佳臂，在自适应停止时间下仅需 $\mathcal{O}(\log^2 T)$ 回合即可选择最佳臂。 |
| [^53] | [Federated Learning of Causal Effects from Incomplete Observational Data.](http://arxiv.org/abs/2308.13047) | 我们提出了一种联邦学习的方法，可以从多个分布式和不完整的数据源中进行因果推断，估计因果效应并解决因为缺失值引入的偏差问题。 |
| [^54] | [Efficient Estimation of the Local Robustness of Machine Learning Models.](http://arxiv.org/abs/2307.13885) | 本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。 |
| [^55] | [FedNoisy: Federated Noisy Label Learning Benchmark.](http://arxiv.org/abs/2306.11650) | FedNoisy是第一个标准化的联合噪声标签学习基准测试，并提供20个基本设置和标准化的仿真流程，以帮助研究人员探索联合学习中噪声标签的影响。 |
| [^56] | [Better Generalization with Semantic IDs: A case study in Ranking for Recommendations.](http://arxiv.org/abs/2306.08121) | 本文提出使用语义ID解决推荐系统中的物品冷启动问题，这些ID是从内容嵌入中学习的，可以捕捉概念的层次关系，相较于完全消除ID特征的方法，语义ID能更好地提高推荐质量。 |
| [^57] | [Eliminating Spurious Correlations from Pre-trained Models via Data Mixing.](http://arxiv.org/abs/2305.14521) | 本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。 |
| [^58] | [HyFL: A Hybrid Framework For Private Federated Learning.](http://arxiv.org/abs/2302.09904) | HyFL是一种混合框架，它结合了安全多方计算技术和分层联合学习，并能够在分布式环境中保证数据和全局模型的隐私安全，有助于大规模部署。 |
| [^59] | [Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning.](http://arxiv.org/abs/2210.16299) | 本文研究了在线、实时解决逆强化学习中存在的多个解的挑战，提出了一种能够收敛到近似等价解的正则化历史堆栈观察器。通过开发新的数据丰富性条件，证明了该技术的有效性。 |
| [^60] | [Neural Optimal Transport with General Cost Functionals.](http://arxiv.org/abs/2205.15403) | 该论文介绍了一种新颖的神经网络算法，用于计算具有一般成本函数的最优传输方案。相比于常见的欧几里得成本，这种方法更灵活，并允许使用辅助信息构建传输映射。此外，该方法还解决了在高维空间下处理新数据点的挑战，并提供了理论误差分析。作为应用，该论文构造了一个能够在保留类别结构的同时映射数据分布的成本函数。 |
| [^61] | [Near-Optimal Algorithms for Private Online Learning in a Stochastic Environment.](http://arxiv.org/abs/2102.07929) | 本文提出了两种隐私在线随机学习的算法，包括差分隐私的随机赌博机算法和私有随机在线学习的完全信息版本。其中，我们提出的随时可用的基于UCB的算法达到了最优性能。 |

# 详细

[^1]: 通过在线学习更新理解Adam优化器：Adam是伪装成FTRL的

    Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise

    [https://rss.arxiv.org/abs/2402.01567](https://rss.arxiv.org/abs/2402.01567)

    本文通过在线学习的视角，揭示了Adam优化器的构成和算法组成的重要性，发现Adam实际上是伪装成FTRL的，研究了在线学习的角度对其算法组成的好处。

    

    尽管Adam优化器在实践中取得了成功，但对其算法组成的理论理解仍然有限。特别是，大多数现有的对Adam的分析仅显示了可以简单地通过非自适应算法如SGD实现的收敛速度。在本文中，我们提供了一种基于在线学习的不同视角，强调了Adam算法的重要性。受Cutkosky等人（2023）的启发，我们考虑了一个称为在线学习更新的框架，其中我们根据在线学习者选择优化器的更新。在这个框架下，设计一个好的优化器就等同于设计一个好的在线学习者。我们的主要观察是，Adam对应于一种被称为Follow-the-Regularized-Leader (FTRL)的原则性在线学习框架。基于这一观察，我们从在线学习的角度研究了其算法组成的好处。

    Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates, where we choose the updates of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective.
    
[^2]: 通过特征谱表征核岭回归的过拟合

    Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum

    [https://rss.arxiv.org/abs/2402.01297](https://rss.arxiv.org/abs/2402.01297)

    我们通过推导核矩阵的特征数界限，增强了核岭回归的测试误差界限。对于多项式谱衰减的核，我们恢复了先前的结果；对于指数谱衰减，我们提出了新的非平凡的界限。我们的研究表明，特征谱衰减多项式的核回归器具有良好的泛化能力，而特征谱指数衰减的核回归器则具有灾难性的过拟合。

    

    我们推导了核矩阵的条件数的新界限，然后利用这些界限增强了在固定输入维度的过参数化区域中核岭回归的现有非渐近测试误差界限。对于具有多项式谱衰减的核，我们恢复了先前工作的界限；对于指数衰减，我们的界限是非平凡和新颖的。我们对过拟合的结论是双重的：(i) 谱衰减多项式的核回归器必须在存在噪声标记的训练数据的情况下得到很好的泛化；这些模型表现出所谓的温和过拟合；(ii) 如果任何核岭回归器的特征谱指数衰减，则其泛化差，即表现出灾难性过拟合。这增加了核岭回归器表现出良性过拟合的可用特征谱衰减次多项式的极端情况的表征。我们的分析结合了新的随机矩阵理论(RMT)。

    We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) te
    
[^3]: 使用循环变压器模拟图算法

    Simulation of Graph Algorithms with Looped Transformers

    [https://rss.arxiv.org/abs/2402.01107](https://rss.arxiv.org/abs/2402.01107)

    本文研究了使用循环变压器网络模拟图算法的能力，证明了该结构可以模拟Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法，并展示了其具有图灵完整性的结果。

    

    最近，使用神经网络执行图算法引起了很大的兴趣，由于有了令人满意的实证进展。这促使我们进一步了解神经网络如何能够使用关系数据复制推理步骤。在这项工作中，我们从理论角度研究了变压器网络模拟图算法的能力。我们使用的架构是一个带额外注意力头和与图形交互的循环变压器。我们通过构造证明了这种架构能够模拟诸如Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法等算法。网络的宽度不随输入图的大小增加，这意味着网络可以模拟任何图上的上述算法。尽管有这个特性，我们展示了在我们的解决方案中有一个由于有限精度而受到限制的模拟极限。最后，我们展示了我们的解决方案具有图灵完整性的结果。

    The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture that we utilize is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth- and Depth-First Search, and Kosaraju's strongly connected components algorithm. The width of the network does not increase with the size of the input graph, which implies that the network can simulate the above algorithms for any graph. Despite this property, we show that there is a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness resu
    
[^4]: 专家接近性作为单演示模仿学习的替代奖励

    Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning

    [https://rss.arxiv.org/abs/2402.01057](https://rss.arxiv.org/abs/2402.01057)

    本文介绍了一种针对单演示模仿学习的新方法TDIL，通过引入基于转换鉴别器的替代奖励函数，鼓励代理向靠近专家状态的状态导航，有效解决了奖励信号稀疏的问题。

    

    本文关注单演示模仿学习（IL），这是一种在获取大量专家演示困难或不可行的实际应用中的实用方法。与 typicIL 设置中具有多个示范不同，单演示IL涉及代理只有一条专家轨迹的访问。我们强调在这种情况下奖励信号稀疏的问题，并提出通过我们提出的基于转换鉴别器的IL（TDIL）方法来缓解这个问题。TDIL是一种基于IRL的方法，旨在通过引入考虑环境动态的更密集的替代奖励函数来解决奖励稀疏性。这个替代奖励函数鼓励代理向靠近专家状态的状态导航。在实践中，TDIL训练一个过渡鉴别器来区分给定环境中的有效和非有效过渡以计算替代奖励。实验表明，TDIL优于现有方法。

    In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where obtaining numerous expert demonstrations is costly or infeasible. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existin
    
[^5]: 一种多分支径向基网络方法用于预测复杂混沌行为

    A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours

    [https://arxiv.org/abs/2404.00618](https://arxiv.org/abs/2404.00618)

    提出了一种多分支径向基网络方法来成功预测复杂混沌行为，通过独特的神经网络架构并引入了注意机制，展示了先进机器学习算法在阐明中的潜力

    

    在这项研究中，我们提出了一种多分支网络方法来预测由错综复杂和混沌行为特征的物理吸引子的动力学。我们引入了一种独特的神经网络架构，由径向基函数（RBF）层和旨在有效捕捉吸引子时间演变中非线性相互依赖关系的注意机制组成。我们的结果表明，通过使用包含大约28分钟活动的现实数据集的36,700个时间序列观测，我们成功预测了吸引子的轨迹的100次预测。为了进一步说明我们提出的技术的性能，我们提供了全面的可视化，展示了吸引子的原始和预测行为，并将观察到的与估计结果进行了定量比较。总体而言，这项工作展示了先进机器学习算法在阐明中的潜力

    arXiv:2404.00618v1 Announce Type: new  Abstract: In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucid
    
[^6]: Croissant：一种面向机器学习数据集的元数据格式

    Croissant: A Metadata Format for ML-Ready Datasets

    [https://arxiv.org/abs/2403.19546](https://arxiv.org/abs/2403.19546)

    Croissant是一种面向机器学习数据集的元数据格式，使数据集更易发现、可移植和互操作，有助于解决ML数据管理和负责任AI中的重要挑战。

    

    数据是机器学习（ML）的关键资源，但处理数据仍然是一个主要的摩擦点。本文介绍了Croissant，一种用于数据集的元数据格式，简化了数据被ML工具和框架使用的方式。Croissant使数据集更易发现、可移植和互操作，从而解决了ML数据管理和负责任AI中的重要挑战。Croissant已得到几个流行数据集库的支持，涵盖数十万个数据集，可以加载到最流行的ML框架中。

    arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
    
[^7]: Serpent：通过多尺度结构化状态空间模型实现可扩展高效的图像恢复

    Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models

    [https://arxiv.org/abs/2403.17902](https://arxiv.org/abs/2403.17902)

    Serpent提出了一种新的图像恢复架构，利用状态空间模型在全局感受野和计算效率之间取得平衡，实现了与最先进技术相当的重建质量，但计算量减少了数个数量级。

    

    有效图像恢复架构的计算建筑块领域，主要由卷积处理和各种注意机制的组合所主导。然而，卷积滤波器本质上是局部的，因此在建模图像的长距离依赖性方面存在困难。另一方面，注意机制擅长捕获任意图像区域之间的全局相互作用，但对图像尺寸的二次成本较高。在这项工作中，我们提出了Serpent，这是一种利用最近在状态空间模型（SSMs）方面的进展作为其核心计算模块的架构。SSMs最初用于序列建模，可以通过有利的输入尺寸的线性缩放来维持全局感受野。我们的初步结果表明，Serpent可以实现与最先进技术相当的重建质量，同时需要数量级的计算量较少（在FLOPS上高达150倍的减少）。

    arXiv:2403.17902v1 Announce Type: cross  Abstract: The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of convolutional processing and various attention mechanisms. However, convolutional filters are inherently local and therefore struggle at modeling long-range dependencies in images. On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension. In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block. SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size. Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) an
    
[^8]: 使用LLM嵌入进行文本聚类

    Text clustering with LLM embeddings

    [https://arxiv.org/abs/2403.15112](https://arxiv.org/abs/2403.15112)

    研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率

    

    文本聚类是组织不断增长的数字内容的重要方法，有助于结构化和发现未分类数据中的隐藏模式。在这项研究中，我们调查了不同文本嵌入（特别是大型语言模型LLMs中使用的）和聚类算法如何影响文本数据集的聚类方式。进行了一系列实验以评估嵌入是如何影响聚类结果的，以及通过摘要进行降维和嵌入大小调整的作用。结果显示，LLM嵌入在捕获结构化语言的细微差别方面表现出色，而BERT在性能上领先于轻量级选项。此外，我们发现增加嵌入维度和摘要技术并不一致地提高聚类效率，这表明这些策略需要仔细分析才能在实际模型中使用。这些结果突出了一种

    arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
    
[^9]: CICLe: 适应上下文的大规模多类食品风险分类学习

    CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification

    [https://arxiv.org/abs/2403.11904](https://arxiv.org/abs/2403.11904)

    该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。

    

    受污染或掺假食品对人类健康构成重大风险。在给定了用于训练的标记网络文本集的情况下，可以应用机器学习和自然语言处理来自动检测这种风险。我们发布了一个包含7,546个描述公共食品召回公告的短文本数据集。每个文本都经过手动标记，分为两个粒度级别（粗粒度和细粒度），用于表示召回对应的食品产品和危害。我们描述了数据集并对朴素、传统和Transformer模型进行了基准测试。基于我们的分析，基于tf-idf表示的逻辑回归在支持较低的类别上优于RoBERTa和XLM-R。最后，我们讨论了不同的提示策略，并提出了一种基于符合预测的LLM-in-the-loop框架，这可以提高基本分类器的性能，同时减少了与普通提示相比的能源消耗。

    arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
    
[^10]: 大型语言模型中用于快速推测解码的循环草稿机制

    Recurrent Drafter for Fast Speculative Decoding in Large Language Models

    [https://arxiv.org/abs/2403.09919](https://arxiv.org/abs/2403.09919)

    本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。

    

    在本文中，我们介绍一种改进的推测解码方法，旨在提高大型语言模型的效率。我们的方法利用了两种成熟技术的优势：经典的双模型推测解码方法和较新的单模型方法Medusa。从Medusa得到灵感，我们的方法采用了单模型策略进行推测解码。然而，我们的方法通过使用具有循环依赖设计的单个轻量级草稿头来区分自己，本质上类似于经典推测解码中使用的小型草稿模型，但避免了完整transformer架构的复杂性。由于循环依赖，我们可以使用波束搜索快速过滤出草稿头中不需要的候选项。其结果是一种结合了单模型设计简易性并避免了创建数据相关树依赖的方法。

    arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
    
[^11]: 将分子表示为可解释的文法上的随机游走

    Representing Molecules as Random Walks Over Interpretable Grammars

    [https://arxiv.org/abs/2403.08147](https://arxiv.org/abs/2403.08147)

    提出了一种新颖的分子表示模型，使用可解释的图文法描述分子的层次化设计空间，实现了在设计空间上的随机游走，从而提高了分子生成和属性预测的性能、效率和可合成性。

    

    最近分子探索领域的研究主要集中在小型、类似药物的分子上，导致许多在材料设计中同样重要的应用缺乏足够的技术支持。这些应用通常依赖于更复杂的分子结构，有更少的例子，是使用已知的亚结构精心设计的。我们提出了一种数据高效且可解释的模型，用于以图文法的形式表示和推理这些分子，明确描述了特征为设计基础的层次化设计空间。我们提出了一种新颖的表示形式，即在设计空间上进行随机游走，既有助于分子生成，又有助于属性预测。我们展示了相较于现有方法在性能、效率和预测分子可合成性方面的明显优势，并详细阐述了该方法的化学可解释性。

    arXiv:2403.08147v1 Announce Type: new  Abstract: Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability.
    
[^12]: 关于洗牌梯度方法的最后迭代收敛性

    On the Last-Iterate Convergence of Shuffling Gradient Methods

    [https://arxiv.org/abs/2403.07723](https://arxiv.org/abs/2403.07723)

    该论文证明了针对目标函数的洗牌梯度方法最后迭代的收敛速率，弥合了在不同设置中最后迭代的良好性能与现有理论之间的差距。

    

    洗牌梯度方法，也被称为无替换的随机梯度下降（SGD），在实践中被广泛应用，特别包括三种流行算法：Random Reshuffle（RR）、Shuffle Once（SO）和Incremental Gradient（IG）。与经验成功相比，长期以来对于洗牌梯度方法的理论保证并不充分了解。最近，只为凸函数的平均迭代和强凸问题的最后迭代（以平方距离为度量）建立了收敛速率。然而，当将函数值差作为收敛准则时，现有理论无法解释在不同设置中（例如受约束的优化）最后迭代的良好性能。为了弥合这种实践与理论之间的差距，我们针对目标函数证明了洗牌梯度方法最后迭代的收敛速率。

    arXiv:2403.07723v1 Announce Type: new  Abstract: Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objectiv
    
[^13]: 适应优势的策略优化用于离线强化学习

    Advantage-Aware Policy Optimization for Offline Reinforcement Learning

    [https://arxiv.org/abs/2403.07262](https://arxiv.org/abs/2403.07262)

    介绍了一种新的适应优势的策略优化（A2PO）方法，用于离线学习，能够解决多行为策略收集的约束冲突问题，有效避免过拟合问题。

    

    离线强化学习致力于利用离线数据集来制定有效的智能体策略，而无需在线交互，通过在行为策略的支持下施加适当的保守约束来解决分布外问题。本文引入了一种新的适应优势的策略优化（A2PO）方法，以明确构建针对混合质量数据集的离线学习优势感知策略约束。

    arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
    
[^14]: 多模态VAEs中的统一多样性：改进的表示学习

    Unity by Diversity: Improved Representation Learning in Multimodal VAEs

    [https://arxiv.org/abs/2403.05300](https://arxiv.org/abs/2403.05300)

    通过软约束取代硬约束，提出了一种新的专家混合先验，改善了多模态VAEs中的表示学习。

    

    多模态数据的变分自编码器在数据分析的许多任务中表现出潜力，如表示学习、有条件生成和填补。目前的架构要么跨模态共享编码器输出、解码器输入，要么两者都要学习共享表示。这样的架构对模型施加了严格约束。在这项工作中，我们展示了通过用软约束取代这些硬约束可以获得更好的潜在表示。我们提出了一种新的专家混合先验，软性地引导每个模态的潜在表示朝着共享的后验。这种方法导致了优秀的潜在表示，并允许每个编码保留来自其未压缩原始特征更好的信息。通过对多个基准数据集和一个具有挑战性的现实世界神经科学数据集进行的广泛实验，我们展示了改进的学习潜在表示和填补。

    arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
    
[^15]: 基于电子健康记录的自动化多任务学习用于联合疾病预测

    Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records

    [https://arxiv.org/abs/2403.04086](https://arxiv.org/abs/2403.04086)

    该研究提出了一种名为AutoDP的自动化方法，用于在电子健康记录数据中为联合疾病预测搜索最佳配置。

    

    在大数据和数字化医疗领域，电子健康记录（EHR）已成为一个丰富的信息来源，有潜力改善患者护理和医学研究。近年来，机器学习模型在分析EHR数据以预测患者未来健康状况方面得到了广泛应用。在其中，一些研究提倡使用多任务学习（MTL）来联合预测多个目标疾病，以提高单任务学习的预测性能。然而，由于当前EHR数据的MTL框架严重依赖于人工专家来识别用于联合训练的任务组和设计模型架构，存在显著局限性。为减少人为干预并改进框架设计，我们提出了一种名为AutoDP的自动化方法，可以同时搜索任务分组和架构的最佳配置。为了解决涵盖任务组合和架构的广泛联合搜索空间，

    arXiv:2403.04086v1 Announce Type: new  Abstract: In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures. To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing ta
    
[^16]: 您需要更好地关注付费

    You Need to Pay Better Attention

    [https://arxiv.org/abs/2403.01643](https://arxiv.org/abs/2403.01643)

    提出了三种新的注意力机制，在效率和学习能力方面优于标准的多头注意力，提高了Transformer模型的性能和更广泛的部署能力。

    

    我们引入了三种新的注意力机制，这些机制在效率和学习能力方面胜过标准的多头注意力，从而提高了Transformer模型的性能和更广泛的部署能力。我们的第一个贡献是优化注意力，其性能与标准注意力相似，但参数数量少了四分之三，每个头部少了一个矩阵乘法。接下来，我们引入了高效注意力，其性能与标准注意力相当，但参数数量减少了一半，每个头部减少了两个矩阵乘法，并且比标准注意力快两倍。最后，我们介绍了超级注意力，在视觉和自然语言处理任务中明显超越了标准注意力，同时具有更少的参数和矩阵乘法。除了提供严格的数学比较，我们在MN中评估了所提出的注意力机制

    arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
    
[^17]: 睡眠臂决策问题中接近最优的每次行动遗憾界

    Near-optimal Per-Action Regret Bounds for Sleeping Bandits

    [https://arxiv.org/abs/2403.01315](https://arxiv.org/abs/2403.01315)

    该论文提出了针对睡眠臂决策问题的接近最优每次行动遗憾界，通过直接最小化每次行动遗憾，使用Generalized EXP3、EXP3-IX和Tsallis entropy下的FTRL方法，获得了较之现有方法更好的界。

    

    我们推导了针对睡眠臂决策问题的接近最优每次行动遗憾界，其中敌手选择每轮可用臂的集合和它们的损失。在每轮至多有 $A$ 个可用臂的 $K$ 个总臂的情况下，已知的最好上界为 $O(K\sqrt{TA\ln{K}})$，通过间接最小化内部睡眠遗憾获得。与极小值 $\Omega(\sqrt{TA})$ 下界相比，这个上界包含额外的乘数因子 $K\ln{K}$。我们通过直接最小化每次行动遗憾，使用EXP3、EXP3-IX和带有Tsallis熵的FTRL的推广版本，从而获得了顺序为 $O(\sqrt{TA\ln{K}})$ 和 $O(\sqrt{T\sqrt{AK}})$ 的接近最优界。我们将结果扩展到了从睡眠专家获得建议的臂决策问题设置，同时推广了EXP4。这为现有的多个自适应和跟踪遗憾界的新证明铺平了道路。

    arXiv:2403.01315v1 Announce Type: new  Abstract: We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for 
    
[^18]: UniTS: 构建统一的时间序列模型

    UniTS: Building a Unified Time Series Model

    [https://arxiv.org/abs/2403.00131](https://arxiv.org/abs/2403.00131)

    UNITS是一种统一的时间序列模型，通过独特的统一网络骨干实现了通用任务规范，并成功支持多种任务，包括分类、预测、插补和异常检测。

    

    基础模型，特别是LLMs，正在深度学习中产生深远影响。我们可以通过少量提示或微调将单个预训练模型适应于许多任务，而不是训练许多特定任务的模型。然而，当前的基础模型适用于序列数据，但不适用于时间序列，因为时间序列具有独特的挑战，包括固有多样性和多领域时间序列数据集，预测、分类和其他类型任务之间的任务规范分歧，以及对任务专用模型的明显需求。我们开发了UNITS，一种支持通用任务规范的统一时间序列模型，可容纳分类、预测、插补和异常检测任务。这是通过一种新颖的统一网络骨干实现的，该骨干结合了序列和变量注意力以及动态线性算子，并作为统一模型进行训练。在38个多领域数据集上，UNITS展示

    arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate
    
[^19]: RIME: 具有嘈杂偏好的健壮偏好强化学习

    RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences

    [https://arxiv.org/abs/2402.17257](https://arxiv.org/abs/2402.17257)

    RIME是一种针对嘈杂偏好的健壮PbRL算法，通过动态过滤去噪偏好和热启动奖励模型，极大增强了现有最先进PbRL方法的鲁棒性。

    

    偏好强化学习（PbRL）通过利用人类偏好作为奖励信号，避免了对奖励设计的需求。然而，当前PbRL算法过度依赖来自领域专家的高质量反馈，导致缺乏鲁棒性。在本文中，我们提出了RIME，一种针对嘈杂偏好的健壮PbRL算法，用于有效地从嘈杂偏好中学习奖励。我们的方法结合了基于样本选择的鉴别器，动态过滤去噪偏好以进行健壮训练。为了减轻选择不正确造成的累积误差，我们提出热启动奖励模型，此外还能填补PbRL中从预训练到在线训练过渡时的性能差距。我们在机器人操纵和运动任务上的实验表明，RIME显著提升了当前最先进的PbRL方法的鲁棒性。消融研究进一步表明，热启动

    arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
    
[^20]: 数据集公平性：在您的数据上实现具有效用保证的公平性

    Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees

    [https://arxiv.org/abs/2402.17106](https://arxiv.org/abs/2402.17106)

    该论文提出了一种针对数据集特性量身定制的近似公平性-准确性权衡曲线计算方法，能够有效减轻训练多个模型的计算负担并提供了严格的统计保证

    

    在机器学习公平性中，训练能够最小化不同敏感群体之间差异的模型通常会导致准确性下降，这种现象被称为公平性-准确性权衡。这种权衡的严重程度基本取决于数据集的特性，如数据集的不均衡或偏见。因此，在数据集之间使用统一的公平性要求仍然值得怀疑，并且往往会导致效用极低的模型。为了解决这个问题，我们提出了一种针对单个数据集量身定制的近似公平性-准确性权衡曲线的计算效率高的方法，该方法支持严格的统计保证。通过利用You-Only-Train-Once（YOTO）框架，我们的方法减轻了在逼近权衡曲线时需要训练多个模型的计算负担。此外，我们通过在该曲线周围引入置信区间来量化我们近似值的不确定性，

    arXiv:2402.17106v1 Announce Type: cross  Abstract: In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offe
    
[^21]: 面向预训练大型语言模型的机器遗忘

    Machine Unlearning of Pre-trained Large Language Models

    [https://arxiv.org/abs/2402.15159](https://arxiv.org/abs/2402.15159)

    本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。

    

    本研究探讨了大型语言模型（LLMs）背景下“被遗忘权”的概念。我们以机器遗忘作为一个关键解决方案，重点关注预训练模型——一个明显缺乏研究的领域。我们在预训练LLMs中勾勒了一个全面的机器遗忘框架，包括对七种不同遗忘方法的批判性分析。通过使用来自arXiv、书籍和GitHub的策划数据集进行严格评估，我们建立了一个有力的机器遗忘性能基准，表明这些方法的计算效率比重新训练高出 $10^5$ 倍以上。我们的结果表明，在分布数据上将梯度上升与梯度下降结合可以改善超参数的鲁棒性。我们还提供了关于在遗忘过程中进行高效超参数调整的详细指南。我们的研究推动了有关伦理人工智能实践的讨论，提供了

    arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
    
[^22]: 并非所有专家都相等: 混合专家大型语言模型的高效专家修剪和跳过

    Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models

    [https://arxiv.org/abs/2402.14800](https://arxiv.org/abs/2402.14800)

    引入了专家级稀疏化技术，提出了专家修剪和跳过的后训练方法，以提高MoE LLMs的部署效率，同时保持模型性能。

    

    大型语言模型（LLMs）进展中的一个重要进展是混合专家（MoE）LLMs的出现。与传统的LLMs相比，MoE LLMs可以在更少的参数下实现更高的性能，但由于其巨大的参数大小，仍然很难部署它们。与先前依赖于专门设计的硬件的权重剪枝方法不同，本文主要旨在通过引入即插即用的专家级稀疏化技术来提高MoE LLMs的部署效率。具体而言，我们首次提出了针对任务不可知和任务特定的MoE LLMs专家修剪和跳过的后训练方法，旨在提高在广泛任务范围内保持模型性能的同时提高部署效率。大量实验证明，我们提出的方法可以同时减小模型大小并增加推断速度，同时保持饱和

    arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
    
[^23]: 位置论文：拓扑深度学习中的挑战与机遇

    Position Paper: Challenges and Opportunities in Topological Deep Learning

    [https://arxiv.org/abs/2402.08871](https://arxiv.org/abs/2402.08871)

    拓扑深度学习将拓扑特征引入深度学习模型，可作为图表示学习和几何深度学习的补充，给各种机器学习环境提供了自然选择。本文讨论了拓扑深度学习中的开放问题，并提出了未来的研究机会。

    

    拓扑深度学习是一个快速发展的领域，它利用拓扑特征来理解和设计深度学习模型。本文认为，通过融入拓扑概念，拓扑深度学习可以补充图表示学习和几何深度学习，并成为各种机器学习环境下的自然选择。为此，本文讨论了拓扑深度学习中的开放问题，涵盖了从实用益处到理论基础的各个方面。针对每个问题，它概述了潜在的解决方案和未来的研究机会。同时，本文也是对科学界的邀请，希望积极参与拓扑深度学习研究，开发这个新兴领域的潜力。

    arXiv:2402.08871v1 Announce Type: new Abstract: Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.
    
[^24]: 学习神经收缩动力学：扩展线性化和全局保证

    Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees

    [https://arxiv.org/abs/2402.08090](https://arxiv.org/abs/2402.08090)

    本论文提出了扩展线性化收缩动力学（ELCD），是第一个具有全局收缩性保证的神经网络动力系统，通过参数化非线性向量场的扩展线性化实现。通过在数据空间和潜在空间之间训练微分同胚，并在潜在空间中强制收缩性，ELCD能在面对不确定性时保持全局稳定性和鲁棒性。

    

    在学习的动态系统中，全局稳定性和鲁棒性保证对于确保系统在面对不确定性时的良好行为至关重要。我们提出了扩展线性化收缩动力学（ELCD），该系统是第一个具有任意度量下全局收缩性保证的基于神经网络的动力系统。ELCD的关键特性是非线性向量场扩展线性化的参数化。在其最基本形式下，ELCD保证全局指数稳定、平衡收缩以及在某些度量下全局收缩。为了实现在数据空间中相对于更一般度量的收缩，我们训练数据空间和潜在空间之间的微分同胚，并在潜在空间中强制收缩性，从而确保数据空间的全局收缩性。我们在2D、4D和8D的LASA数据集上展示了ELCD的性能。

    Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.
    
[^25]: 透视VLMs：探索视觉条件化语言模型的设计空间

    Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models

    [https://arxiv.org/abs/2402.07865](https://arxiv.org/abs/2402.07865)

    本论文探索了视觉条件化语言模型（VLMs）设计的关键空间，并提供了一套标准化评估，同时还研究了预训练的视觉表示和权衡的问题。

    

    视觉条件化语言模型（VLMs）在视觉对话、场景理解和机器人任务规划等应用中得到了越来越多的应用，这种应用促使了像LLaVa、InstructBLIP和PaLI-3等许多新模型的出现。尽管有这么多新的发布，但关于图像预处理、架构和优化的关键设计决策仍然未被充分探索，这使得我们很难理解模型性能的因素，这一挑战又因缺乏客观、一致的评估而变得更加复杂。为了填补这些空白，我们首先编制了一套标准化评估，涵盖了视觉问答、从语言中定位物体以及探索幻觉等属性的目标挑战集，这些评估可以提供关于VLM能力的精细、准确的见解。其次，我们对关键的设计轴进行了严格的研究，包括预训练的视觉表示和使用的权衡。

    Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
    
[^26]: 阈值Oja是否适用于稀疏PCA？

    Thresholded Oja does Sparse PCA?

    [https://arxiv.org/abs/2402.07240](https://arxiv.org/abs/2402.07240)

    阈值和重新归一化Oja算法的输出可获得一个接近最优的错误率，与未经阈值处理的Oja向量相比，这大大减小了误差。

    

    我们考虑了当比值$d/n \rightarrow c > 0$时稀疏主成分分析（PCA）的问题。在离线设置下，关于稀疏PCA的最优率已经有很多研究，其中所有数据都可以用于多次传递。相比之下，当人口特征向量是$s$-稀疏时，具有$O(d)$存储和$O(nd)$时间复杂度的流算法通常要求强初始化条件，否则会有次优错误。我们展示了一种简单的算法，对Oja算法的输出（Oja向量）进行阈值和重新归一化，从而获得接近最优的错误率。这非常令人惊讶，因为没有阈值，Oja向量的误差很大。我们的分析集中在限制未归一化的Oja向量的项上，这涉及将一组独立随机矩阵的乘积在随机初始向量上的投影。 这是非平凡且新颖的，因为以前的Oja算法分析没有考虑这一点。

    arXiv:2402.07240v2 Announce Type: cross  Abstract: We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \rightarrow c > 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's al
    
[^27]: 依赖学习理论中的尖锐率：避免样本大小缩减的平方损失

    Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss

    [https://arxiv.org/abs/2402.05928](https://arxiv.org/abs/2402.05928)

    本文研究了依赖学习理论中的尖锐率，主要是为了避免样本大小缩减对方差产生影响。当假设类别的拓扑结构符合某些条件时，经验风险最小化者的性能与类别的复杂性和二阶统计量有关。

    

    本文研究了具有依赖性（β-混合）数据和平方损失的统计学习，在一个假设类别Φ_p的子集F中，其中Φ_p是范数∥f∥_Φ_p≡sup_m≥1 m^{-1/p}∥f∥_L^m，其中p∈[2，∞]。我们的研究动机是在具有依赖性数据的学习中寻找尖锐的噪声交互项或方差代理。在没有任何可实现性假设的情况下，典型的非渐近结果显示出方差代理通过底层协变量过程的混合时间进行了乘积缩减。我们证明，只要在我们的假设类别F上，L^2和Φ_p的拓扑是可比较的，即Φ_p是一个弱亚高斯类别：∥f∥_Φ_p≲∥f∥_L^2^η，其中η∈(0，1]，经验风险最小化者在其主导项中只实现了一种只依赖于类别复杂性和二阶统计量的速率。我们的结果适用于许多依赖性数据模型。

    In this work, we study statistical learning with dependent ($\beta$-mixing) data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$ where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p} \|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \emph{multiplicatively} by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class $\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class: $\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether t
    
[^28]: 通过离散化和特征选择的可解释性表格数据分类器

    Interpretable classifiers for tabular data via discretization and feature selection

    [https://arxiv.org/abs/2402.05680](https://arxiv.org/abs/2402.05680)

    通过离散化和特征选择的方法，我们提出了一种从表格数据中计算出准确又易解释的分类器的方法。在实验证明该方法在准确度上与随机森林和XGBoost等现有方法相当，并且在多种情况下实际上超过了参考结果。

    

    我们引入了一种从表格数据中计算出具有解释性且准确的分类器的方法。所得到的分类器是简短的DNF公式，通过将原始数据离散化为布尔形式，然后使用特征选择结合非常快速的算法来产生最佳的布尔分类器。我们通过14个实验来演示该方法，得到的结果的准确度主要与随机森林、XGBoost以及文献中相同数据集的现有结果相似。在多种情况下，我们的方法实际上在准确度方面优于参考结果，尽管我们研究的主要目标是我们的分类器的即时可解释性。我们还证明了一个关于从现实数据中获得的分类器与来自数据背景分布的最佳分类器相对应的概率的新结果。

    We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.
    
[^29]: Tag-LLM: 将通用的LLM应用于专业领域的再利用

    Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains

    [https://arxiv.org/abs/2402.05140](https://arxiv.org/abs/2402.05140)

    本文介绍了一种将通用的LLMs应用于专业领域的方法，通过学习自定义的输入标签来对LLMs进行条件约束。通过明确将任务领域与任务功能分离，这种方法能够改善在专业领域中的任务求解能力。

    

    大型语言模型（LLMs）在理解和生成自然语言方面表现出了非凡的能力。然而，在专门领域中，如物理学和生物医学科学这样的预训练语料库中未充分涵盖的领域，它们的能力下降。本文探讨了如何将通用LLMs重新用于专业领域的有效任务解决方案。我们介绍了一种新颖的、与模型无关的框架，用于学习自定义的输入标签，这些标签被参数化为连续向量并附加到LLMs的嵌入层，以对LLMs进行条件约束。我们设计了两种类型的输入标签：领域标签用于限定专业表示（例如化学式）并提供领域相关的上下文；功能标签用于表示特定的功能（例如预测分子性质）并压缩功能解决指令。我们使用辅助数据和领域知识开发了一个包括三个阶段的学习这些标签的协议。通过明确将任务领域与任务功能分离，我们的方法能够改善在专业领域中的任务求解能力。

    Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
    
[^30]: 冷启动无示例增量学习的弹性特征整合

    Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning

    [https://arxiv.org/abs/2402.03917](https://arxiv.org/abs/2402.03917)

    这篇论文解决了冷启动场景的无示例增量学习的问题，提出了一种弹性特征整合的方法，通过规范特征漂移并利用原型来减少任务新鲜度偏差。

    

    无示例类别增量学习（EFCIL）旨在从一系列任务中学习，而不需要访问先前任务的数据。在本文中，我们考虑了具有挑战性的冷启动场景，在第一个任务中没有足够的数据来学习高质量的骨干网络。对于EFCIL来说，这是特别具有挑战性的，因为它需要高度的可塑性，这会导致特征漂移，在无示例的情况下很难进行补偿。为了解决这个问题，我们提出了一种简单而有效的方法，通过规范在与先前任务高度相关的方向上的漂移，并利用原型来减少任务新鲜度偏差，以整合特征表示。我们的方法被称为弹性特征整合（EFC），它利用基于经验特征矩阵（EFM）的可解二阶近似来处理特征漂移。EFM在特征空间中引入了伪度量，我们使用它来规范重要方向上的特征漂移，并更新高斯原型。

    Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prot
    
[^31]: 无需训练的一致性文本到图像生成

    Training-Free Consistent Text-to-Image Generation

    [https://arxiv.org/abs/2402.03286](https://arxiv.org/abs/2402.03286)

    本文提出了一种无需训练的方法ConsiStory，通过共享预训练模型的内部激活，实现了一致的文本到图像生成。引入了主题驱动的共享注意力块和基于对应的特征注入，促进了图像之间的主题一致性，并采用了策略来保持布局多样性。

    

    文本到图像模型通过自然语言引导图像生成过程，提供了一种新的创造性灵活性。然而，使用这些模型在多样化的提示下一致地描绘相同的主题仍然具有挑战性。现有方法通过优化模型来教授它描述特定用户提供主题的新词汇或者为模型添加图像条件。这些方法要求针对每个主题进行漫长的优化或进行大规模预训练。此外，它们在将生成的图像与文本提示对齐和描绘多个主题方面遇到困难。在这里，我们介绍了一种无训练方法ConsiStory，通过共享预训练模型的内部激活来实现一致的主题生成。我们引入了一个主题驱动共享注意力块和基于对应的特征注入，以促进图像之间的主题一致性。此外，我们开发了策略以鼓励布局多样性，同时保持主题一致性。

    Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
    
[^32]: 想法的不确定性：不确定性感知规划增强大型语言模型的信息搜索能力

    Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models

    [https://arxiv.org/abs/2402.03271](https://arxiv.org/abs/2402.03271)

    通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。

    

    在面对不确定性时，寻求信息的能力至关重要。在许多实际应用中，比如医学诊断和故障排除，解决任务所需的信息不是初始给定的，而需要通过询问后续问题来主动寻求（例如，医生向患者询问症状的更多细节）。在这项工作中，我们引入了思想的不确定性（UoT），一种算法将大型语言模型的能力与主动提问信息的能力相结合。UoT结合了1）不确定性感知仿真方法，使模型能够模拟可能的未来场景，并估计其发生的可能性；2）基于不确定性的奖励机制，激励模型寻求信息；3）奖励传播方案，以最大化预期奖励的方式选择最佳的问题提问方式。在医学诊断、故障排除和'20的实验中。

    In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
    
[^33]: 多区域马尔可夫高斯过程：一种发现多个脑区之间方向性通讯的高效方法

    Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions

    [https://arxiv.org/abs/2402.02686](https://arxiv.org/abs/2402.02686)

    本研究提出了一种名为多区域马尔可夫高斯过程的方法，将高斯过程和线性动态系统相结合，有效地发现了多个脑区之间的方向性通讯。通过建立LDS与多输出GP之间的联系，该模型实现了线性推断并提供了可解释的低维表示。

    

    研究不同脑区之间复杂的相互作用对神经科学至关重要。各种统计方法已经探索了多个脑区之间潜在的通讯。两个主要的类别是高斯过程（GP）和线性动态系统（LDS），每个方法都具有独特的优势。基于GP的方法有效地发现了潜在变量，如频带和通讯方向。相反，基于LDS的方法在计算效率上高，但在潜在表示方面缺乏强大的表达能力。在本研究中，我们通过创建一个与多输出GP相对应的LDS，即多区域马尔可夫高斯过程（MRM-GP），将这两种方法合二为一。我们的工作首次建立了LDS和多输出GP之间的联系，在神经记录的潜在空间中明确建模了频率和相位延迟。因此，该模型在时间点上实现了线性推断成本，并提供了可解释的低维表示。

    Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables such as frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional repre
    
[^34]: LQER: 低秩量化误差重建用于LLMs

    LQER: Low-Rank Quantization Error Reconstruction for LLMs

    [https://arxiv.org/abs/2402.02446](https://arxiv.org/abs/2402.02446)

    LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。

    

    大型语言模型（LLMs）的训练后量化是具有挑战性的。在这项工作中，我们介绍了低秩量化误差减少（LQER）方法，该方法结合了量化和低秩逼近来恢复模型的能力。LQER利用激活引起的尺度矩阵将量化误差的奇异值分布推向期望的分布，从而实现了在各种LLMs和下游任务上近乎无损的W4A8量化，无需知识蒸馏、网格搜索或基于梯度的迭代优化。与现有方法不同，LQER的计算模式消除了从不规则内存位置收集高精度权重所需的专用Scatter和Gather过程。我们的W4A8 LLMs在六个热门下游任务上实现了近乎无损的性能，同时使用的硬件资源比领先的最新方法少1.36倍。一旦论文被接受，我们将开源我们的框架。

    Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
    
[^35]: EuLagNet: 拉格朗日动力学的欧拉预测

    EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics

    [https://arxiv.org/abs/2402.02425](https://arxiv.org/abs/2402.02425)

    EuLagNet提出了一种新的拉格朗日引导范式，通过跟踪多尺度关键粒子的运动来捕捉多尺度流体动力学。这种方法克服了由于欧拉观察而导致的流体动力学困难，为准确预测未来的流体提供了一种有效的方法。

    

    准确预测未来的流体对气象学、海洋学和空气动力学等广泛领域至关重要。然而，由于流体通常从欧拉角度观察，其活跃和复杂的动力学在静止的网格中严重被掩盖和混淆，给预测带来了巨大挑战。本文引入了一种新的拉格朗日引导范式来解决复杂的流体动力学。我们提出了以拉格朗日动力学为导向的欧拉-拉格朗日双重递归网络（EuLagNet），通过跟踪自适应采样的多尺度关键粒子的运动并随时间积累动力学信息来捕捉多尺度流体动力学。具体地，我们提出了一个EuLag块，用于在每个时刻和尺度上传递学习到的欧拉和拉格朗日特征，其中跟踪粒子的运动是从欧拉观察中推断出来的，它们积累的动力学信息被纳入到预测模型中。

    Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into
    
[^36]: 通过数据集的多面体结构定义神经网络架构

    Defining Neural Network Architecture through Polytope Structures of Dataset

    [https://arxiv.org/abs/2402.02407](https://arxiv.org/abs/2402.02407)

    本文通过定义上下界确定神经网络宽度，与数据集的几何复杂性相关。同时开发了一种算法，可以从训练好的神经网络中推断数据集的多面体结构。

    

    当前神经网络的理论和实证研究表明，复杂的数据集需要大型网络架构进行彻底分类，然而这种关系的具体性质仍不清楚。本文通过定义神经网络宽度的上下界来解决这个问题，这些界限是由所讨论数据集的多面体结构所确定的。我们还深入探讨了这些原则在单纯复合体和特定多样曲面形状上的应用，解释了网络宽度需求如何根据数据集的几何复杂性而变化。此外，我们开发了一种算法来研究一种相反情况，即可以从相应的训练神经网络推断出数据集的多面体结构。通过我们的算法，我们确定了流行的数据集（如MNIST、Fashion-MNIST和CIFAR10）可以用只有少数面的两个多面体有效地表示。

    Current theoretical and empirical research in neural networks suggests that complex datasets require large network architectures for thorough classification, yet the precise nature of this relationship remains unclear. This paper tackles this issue by defining upper and lower bounds for neural network widths, which are informed by the polytope structure of the dataset in question. We also delve into the application of these principles to simplicial complexes and specific manifold shapes, explaining how the requirement for network width varies in accordance with the geometric complexity of the dataset. Moreover, we develop an algorithm to investigate a converse situation where the polytope structure of a dataset can be inferred from its corresponding trained neural networks. Through our algorithm, it is established that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be efficiently encapsulated using no more than two polytopes with a small number of faces.
    
[^37]: APIServe: 高效支持大型语言模型推理的API工具

    APIServe: Efficient API Support for Large-Language Model Inferencing

    [https://arxiv.org/abs/2402.01869](https://arxiv.org/abs/2402.01869)

    APIServe是针对API增强的大型语言模型推理的一个高效工具，它最大限度地减少了由 API 调用引起的 GPU 资源浪费，并提高了整体服务吞吐量。

    

    大型语言模型越来越多地与外部工具和API集成，如ChatGPT插件，以扩展其能力以外的语言中心任务。然而，当前的LLM推理系统是为独立的LLM设计的。它们将API调用视为新请求，导致不必要的重新计算已经计算过的上下文，这占了总模型前向时间的37-40%。本文提出了APIServe，这是针对API增强的LLM推理框架。APIServe最大限度地减少了由API调用引起的GPU资源浪费，并将节省的内存用于服务更多的请求。与现有的LLM推理系统相比，APIServe将整体服务吞吐量提升了1.6倍，每秒完成的请求增加了2倍。

    Large language models are increasingly integrated with external tools and APIs like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat API calls as new requests, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents APIServe, the first LLM inference framework targeting API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API calls and dedicates saved memory for serving more requests. APISERVE improves the overall serving throughput by 1.6x and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.
    
[^38]: 使用动态目标感知片段进行药物发现

    Drug Discovery with Dynamic Goal-aware Fragments

    [https://arxiv.org/abs/2310.00841](https://arxiv.org/abs/2310.00841)

    提出了一种名为GEAM的分子生成框架，用于药物发现。该框架通过构建目标感知片段词汇，识别贡献于所需目标特性的重要片段，并在生成过程中更新片段词汇。

    

    基于片段的药物发现是在庞大的化学空间中发现药物候选物的有效策略，并已广泛应用于分子生成模型中。然而，在这些模型中，许多现有的片段提取方法没有考虑目标化学性质，或者依赖于启发式规则。此外，现有的基于片段的生成模型不能使用生成过程中新发现的目标感知片段来更新片段词汇。为此，我们提出了一种用于药物发现的分子生成框架，称为目标感知片段提取、组装和修改（GEAM）。GEAM包含三个模块，分别负责目标感知片段提取、片段组装和片段修改。片段提取模块通过信息瓶颈原则识别对所需目标特性有贡献的重要片段，从而构建一个有效的目标感知片段词汇。

    Fragment-based drug discovery is an effective strategy for discovering drug candidates in the vast chemical space, and has been widely employed in molecular generative models. However, many existing fragment extraction methods in such models do not take the target chemical properties into account or rely on heuristic rules. Additionally, the existing fragment-based generative models cannot update the fragment vocabulary with goal-aware fragments newly discovered during the generation. To this end, we propose a molecular generative framework for drug discovery, named Goal-aware fragment Extraction, Assembly, and Modification (GEAM). GEAM consists of three modules, each responsible for goal-aware fragment extraction, fragment assembly, and fragment modification. The fragment extraction module identifies important fragments contributing to the desired target properties with the information bottleneck principle, thereby constructing an effective goal-aware fragment vocabulary. Moreover, GE
    
[^39]: CaRiNG: 在非可逆生成过程下学习时间因果表示

    CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process. (arXiv:2401.14535v1 [cs.LG])

    [http://arxiv.org/abs/2401.14535](http://arxiv.org/abs/2401.14535)

    CaRiNG提出了一种基于可辨识性理论的方法，用于学习具有非可逆生成过程的时间因果表示。这种方法能够恢复独立的潜在组分，即使它们来自于非线性且非可逆的混合过程。

    

    鉴别顺序数据中潜在的延迟时间因果过程对于把握时间动力学和进行下游推理至关重要。尽管最近的一些方法可以稳健地识别这些潜在的因果变量，但它们依赖于从潜在变量到观测数据的可逆生成过程的严格假设。然而，这些假设通常在包含信息损失的现实应用中难以满足。例如，视觉感知过程将3D空间转化为2D图像，或者视觉坚持现象在当前感知中融入历史数据。为了解决这个挑战，我们建立了一个可辨识性理论，允许在非线性和非可逆混合情况下恢复独立的潜在组分。在此理论基础上，我们提出了一种基于原则的方法，CaRiNG，用于学习具有可辨识性的非可逆生成时间数据的因果表示。

    Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the CAusal RepresentatIon of Non-invertible Generative temporal data with identifiability
    
[^40]: Chem-FINESE: 通过文本重构验证细粒度少样本实体提取

    Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])

    [http://arxiv.org/abs/2401.10189](http://arxiv.org/abs/2401.10189)

    这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。

    

    在化学领域中，细粒度少样本实体提取面临两个独特的挑战。首先，与一般领域的实体提取任务相比，化学论文中的句子通常包含更多的实体。此外，实体提取模型通常难以提取长尾类型的实体。在本文中，我们提出了一种新颖的基于序列到序列的少样本实体提取方法Chem-FINESE来解决这两个挑战。我们的Chem-FINESE包含两个组件：一个序列到序列的实体提取器用于从输入句子中提取命名实体，以及一个序列到序列的自我验证模块用于从提取的实体中重构原始输入句子。受到一个好的实体提取系统需要忠实提取实体的事实启发，我们的新自我验证模块利用实体提取结果来重构原始输入句子。此外，我们设计了一种新的对比损失来减少在提取过程中的过度复制。

    Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
    
[^41]: GA-SmaAt-GNet：用于极端降水暂时预测的生成对抗小型注意力GNet

    GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting. (arXiv:2401.09881v1 [cs.LG])

    [http://arxiv.org/abs/2401.09881](http://arxiv.org/abs/2401.09881)

    GA-SmaAt-GNet是一种生成对抗架构，用于改进极端降水暂时预测的深度学习模型性能。它通过创建一个新的生成器和使用注意力增强鉴别器来利用降水掩码提供的额外信息，提高了预测质量。

    

    近年来，数据驱动的建模方法在各种气象应用中获得了相当的关注，尤其是在天气预报领域。然而，这些方法在处理极端天气条件时常常面临挑战。鉴于此，我们提出了GA-SmaAt-GNet，这是一种新颖的生成对抗结构，利用了两种方法来增强深度学习模型在极端降水暂时预测中的性能。首先，它使用了一种基于成功的SmaAt-UNet结构构建的新型SmaAt-GNet作为生成器。该网络将降水掩码（二值化降水图）作为附加数据源，利用有价值的信息进行改进的预测。此外，GA-SmaAt-GNet还使用了一个灵感来自于著名的Pix2Pix结构的注意力增强鉴别器。另外，我们使用荷兰的实际降水数据集对GA-SmaAt-GNet的性能进行了评估。

    In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting. However, these approaches often encounter challenges when dealing with extreme weather conditions. In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator. This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture. Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherland
    
[^42]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^43]: Lookahead:一种用于具有无损生成准确性的大型语言模型的推理加速框架

    Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.12728](http://arxiv.org/abs/2312.12728)

    本研究介绍了一种通用的推理加速框架，用于提高大型语言模型（LLMs）的推理速度，并在保持生成准确性的同时降低成本。该框架在支付宝的检索增强生成（RAG）系统中得到了应用。

    

    随着大型语言模型（LLMs）在各种任务中取得了重大进展，如问答、翻译、文本摘要和对话系统，尤其是对于像支付宝这样为数十亿用户提供重要金融产品的需要准确信息的情况，信息的准确性变得至关重要。为了解决这个问题，支付宝开发了一种称为检索增强生成（RAG）系统的方法，该系统将LLMs与最准确和最新的信息相结合。然而，对于为数百万用户提供服务的真实产品来说，LLMs的推理速度成为一个关键因素，而不仅仅是一个实验性的模型。因此，本文提出了一种通用的推理加速框架，通过加速推理过程，实现了我们的RAG系统的速度大幅提升和成本降低，同时保持着无损的生成准确性。在传统的推理过程中，每个令牌都由LLMs按顺序生成，导致的时间消耗与生成的令牌数成正比。

    As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation (RAG) system that grounds LLMs on the most accurate and up-to-date information. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.  Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our RAG system, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this 
    
[^44]: 绝对策略优化

    Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])

    [http://arxiv.org/abs/2310.13230](http://arxiv.org/abs/2310.13230)

    这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。

    

    近年来，基于信任域的在线策略强化学习在解决复杂控制任务和游戏场景方面取得了令人瞩目的结果。然而，这一类别中现有的最先进算法主要强调对预期性能的改进，缺乏对最坏情况下性能结果的控制能力。为了解决这个限制，我们引入了一个新颖的目标函数；通过优化该函数，可以确保近乎总体性能样本的下界（绝对性能）呈现单调改进。考虑到这一具有突破性的理论进展，我们通过一系列的近似对这个理论基础算法进行了改进，得到了一种实用的解决方案称为绝对策略优化（APO）。我们的实验证明了我们的方法在具有挑战性的连续控制基准任务上的有效性，并将其适用性扩展到掌握Atari游戏。我们的发现表明，APO在提高性能的同时也显著改善了最坏情况下的性能结果。

    In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
    
[^45]: 关于循环神经网络语言模型的表示能力的研究

    On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.12942](http://arxiv.org/abs/2310.12942)

    本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。

    

    本研究调查了基于循环神经网络(RNNs)的语言模型(LMs)的计算表达性。Siegelmann和Sontag(1992)曾经展示了具有有理权重和隐藏状态以及无限计算时间的RNNs是图灵完备的。然而，LMs不仅定义了字符串上的加权，还定义了(非加权)语言成员关系，对RNN LMs（RLMs）的计算能力分析应该反映这一点。我们将图灵完备性结果扩展到概率情况，展示了如何使用有理权重的RLM和无限计算时间来模拟任何概率图灵机(PTM)。由于在实践中，RLMs实时工作，每个时间步骤处理一个符号，因此我们将上述结果作为RLMs表达性的上界。我们还通过展示在实时计算限制下，这些模型可以模拟确定性实时有理PTMs来提供下界。

    This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
    
[^46]: LLM-集成应用中的提示注入攻击和防御

    Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])

    [http://arxiv.org/abs/2310.12815](http://arxiv.org/abs/2310.12815)

    本文提出了一个通用框架来形式化提示注入攻击，并系统化防御这种类型的攻击。

    

    大型语言模型（LLMs）越来越多地用作各种称为LLM-集成应用的实际应用程序的后端。最近的多项研究表明，LLM-集成应用容易受到提示注入攻击的威胁，攻击者可以将恶意指令/数据注入这些应用程序的输入中，以达到攻击者的预期结果。然而，现有的研究仅限于案例研究，缺乏对提示注入攻击及其防御的系统理解。本论文旨在填补这一空白。我们提出了一个通用框架来形式化提示注入攻击，并将研究论文和博客文章中讨论的现有攻击视为我们框架的特例。我们的框架使我们能够通过组合现有攻击设计新的攻击方式。此外，我们还提出了一个系统化提示注入攻击防御的框架。利用我们的框架，我们可以预防和缓解这种类型的攻击。

    Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
    
[^47]: 可证保健康商务字体尹包通过随机平滑(译注)水。

    Provably Robust Cost-Sensitive Learning via Randomized Smoothing. (arXiv:2310.08732v1 [cs.LG])

    [http://arxiv.org/abs/2310.08732](http://arxiv.org/abs/2310.08732)

    本研究通过随机平滑认证框架，为成本敏感的稳健分类器提供了严格的稳健性保证，并通过优化方案针对不同数据子组设计了细粒度认证半径，取得了优越的性能。

    

    我们关注于在成本敏感的情景下学习对抗性稳健分类器，在这种情况下，不同类别的对抗性变换的潜在危害被编码在一个二进制成本矩阵中。现有的方法要么是经验性的，无法证明稳健性，要么存在固有的可扩展性问题。在这项工作中，我们研究了随机平滑，一种更可扩展的稳健性认证框架，是否可以用于证明成本敏感的稳健性。建立在一种成本敏感认证半径的概念之上，我们展示了如何调整标准的随机平滑认证流程，为任何成本矩阵产生严格的稳健性保证。此外，通过针对不同数据子组设计的细粒度认证半径优化方案，我们提出了一种算法，用于训练针对成本敏感稳健性优化的平滑分类器。在图像基准测试和真实的医学数据集上进行了大量实验，证明了我们方法的优越性。

    We focus on learning adversarially robust classifiers under a cost-sensitive scenario, where the potential harm of different classwise adversarial transformations is encoded in a binary cost matrix. Existing methods are either empirical that cannot certify robustness or suffer from inherent scalability issues. In this work, we study whether randomized smoothing, a more scalable robustness certification framework, can be leveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive certified radius, we show how to adapt the standard randomized smoothing certification pipeline to produce tight robustness guarantees for any cost matrix. In addition, with fine-grained certified radius optimization schemes specifically designed for different data subgroups, we propose an algorithm to train smoothed classifiers that are optimized for cost-sensitive robustness. Extensive experiments on image benchmarks and a real-world medical dataset demonstrate the superiority of our
    
[^48]: Decision ConvFormer: MetaFormer中的本地过滤对于决策制定已经足够了

    Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making. (arXiv:2310.03022v1 [cs.LG])

    [http://arxiv.org/abs/2310.03022](http://arxiv.org/abs/2310.03022)

    Decision ConvFormer提出了一种新的动作序列预测器，通过使用本地卷积过滤来捕捉强化学习数据集中的局部关联，同时在各个标准RL基准上取得了最先进的性能。

    

    Transformer在自然语言处理中的成功引发了其在各个领域的应用。在离线强化学习中，Decision Transformer（DT）作为一种基于Transformer的有前途的模型逐渐崭露头角。然而，我们发现DT的注意力模块不适合捕捉作为马尔科夫决策过程建模的强化学习轨迹中固有的局部依赖模式。为了解决DT的局限性，我们提出了一种基于MetaFormer架构的新型动作序列预测器，称为Decision ConvFormer（DC）。DC采用本地卷积过滤作为令牌混合器，能够有效捕捉RL数据集中固有的局部关联。在大量实验证明中，DC在各种标准RL基准上取得了最先进的性能，同时需要更少的资源。

    The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that 
    
[^49]: 深度学习应用的韧性：分析和加固技术的系统调查

    Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques. (arXiv:2309.16733v1 [cs.LG])

    [http://arxiv.org/abs/2309.16733](http://arxiv.org/abs/2309.16733)

    这篇论文系统调查了深度学习应用对底层硬件故障的韧性，并提供了未来研究的方向。

    

    机器学习（ML）目前被广泛应用于各种领域，是最有效的人工智能（AI）技术之一，如视觉、自主系统等。这一趋势促使人们对ML应用在底层硬件故障影响下的分析和设计做出了大量贡献。本文通过一次深入的回顾系统地调查了关于深度学习（ML技术之一）对抗硬件故障的韧性的已有知识，清晰地呈现了这一文献流的优点和缺点，并提出了未来的研究方向。文章基于2019年1月至2023年3月间发表的163篇科学论文，采用分类框架来解读和突出研究的相似之处和特点，从工作的主要范围、采用的故障和错误模型等多个参数进行分类。

    Machine Learning (ML) is currently being exploited in numerous applications being one of the most effective Artificial Intelligence (AI) technologies, used in diverse fields, such as vision, autonomous systems, and alike. The trend motivated a significant amount of contributions to the analysis and design of ML applications against faults affecting the underlying hardware. The authors investigate the existing body of knowledge on Deep Learning (among ML techniques) resilience against hardware faults systematically through a thoughtful review in which the strengths and weaknesses of this literature stream are presented clearly and then future avenues of research are set out. The review is based on 163 scientific articles published between January 2019 and March 2023. The authors adopt a classifying framework to interpret and highlight research similarities and peculiarities, based on several parameters, starting from the main scope of the work, the adopted fault and error models, to the
    
[^50]: 普通最小二乘插值器的代数和统计属性

    Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator. (arXiv:2309.15769v1 [math.ST])

    [http://arxiv.org/abs/2309.15769](http://arxiv.org/abs/2309.15769)

    本文研究了普通最小二乘插值器在高维环境中的代数和统计属性，并为最小l2范数OLS插值器提供了基本结果。这些结果对理解OLS插值器的泛化能力具有重要意义。

    

    深度学习研究揭示了对超参数化统计模型的良性过拟合现象，近年来引起了重大的理论兴趣。鉴于其简单性和实用性，普通最小二乘（OLS）插值器已成为获得对这种现象基础洞察力的关键所在。尽管OLS在经典环境中的性质已经得到了很好的建立，但在高维环境中的行为还没有像岭回归或套索回归那样被探索得那么透彻，尽管近年来已取得了显著进展。我们通过为最小l2范数OLS插值器提供基本的代数和统计结果来贡献于这一日益增长的文献。特别地，我们提供了（i）留-k-out残差公式的高维代数等价物，（ii） Cochran公式，以及（iii）Frisch-Waugh-Lovell定理。这些结果有助于理解OLS插值器的泛化能力并具有实质性的影响。

    Deep learning research has uncovered the phenomenon of benign overfitting for over-parameterized statistical models, which has drawn significant theoretical interest in recent years. Given its simplicity and practicality, the ordinary least squares (OLS) interpolator has become essential to gain foundational insights into this phenomenon. While properties of OLS are well established in classical settings, its behavior in high-dimensional settings is less explored (unlike for ridge or lasso regression) though significant progress has been made of late. We contribute to this growing literature by providing fundamental algebraic and statistical results for the minimum $\ell_2$-norm OLS interpolator. In particular, we provide high-dimensional algebraic equivalents of (i) the leave-$k$-out residual formula, (ii) Cochran's formula, and (iii) the Frisch-Waugh-Lovell theorem. These results aid in understanding the OLS interpolator's ability to generalize and have substantive implications for c
    
[^51]: 因果理论和结构化数据表示的改进效果研究

    Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification. (arXiv:2309.10211v1 [cs.LG])

    [http://arxiv.org/abs/2309.10211](http://arxiv.org/abs/2309.10211)

    使用人生成的因果知识来改进数据表示可以提高神经网络在复杂分类任务中的性能，指示了改进机器学习系统开发实践的重要性。

    

    我们考虑如何使用以人为中心的因果理论和来自动力系统文献的工具，来引导神经网络在复杂分类任务中训练数据的表示。具体而言，我们使用模拟数据来展示，使用将数据生成过程的不变结构因果特征明确显示在数据表示中的神经网络训练，相比于更为天真的数据表示方法，能够提高在分类任务中的超出分布（OOD）泛化性能。我们认为这些结果表明，利用人生成的因果知识来减少机器学习开发者的认识不确定性，可以导致更加明确规范的机器学习流程。这进而指示了通过改进机器学习系统开发实践来提高机器学习系统的鲁棒性和安全性的更广泛努力中的动力系统方法的实用性。

    We consider how human-centered causal theories and tools from the dynamical systems literature can be deployed to guide the representation of data when training neural networks for complex classification tasks. Specifically, we use simulated data to show that training a neural network with a data representation that makes explicit the invariant structural causal features of the data generating process of an epidemic system improves out-of-distribution (OOD) generalization performance on a classification task as compared to a more naive approach to data representation. We take these results to demonstrate that using human-generated causal knowledge to reduce the epistemic uncertainty of ML developers can lead to more well-specified ML pipelines. This, in turn, points to the utility of a dynamical systems approach to the broader effort aimed at improving the robustness and safety of machine learning systems via improved ML system development practices.
    
[^52]: 快速和遗憾最小的最佳臂识别：基本限制和低复杂度算法

    Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms. (arXiv:2309.00591v1 [cs.LG])

    [http://arxiv.org/abs/2309.00591](http://arxiv.org/abs/2309.00591)

    本文提出了一种名为 ROBAI 的算法，旨在快速识别并选择最佳臂，并在一系列连续回合中最大化奖励。该算法在预定停止时间和自适应停止时间要求下均实现了渐进最优遗憾，并且在预定停止时间下仅需 $\mathcal{O}(\log T)$ 回合即可选择最佳臂，在自适应停止时间下仅需 $\mathcal{O}(\log^2 T)$ 回合即可选择最佳臂。

    

    本文考虑具有双重目标的随机多臂老虎机(MAB)问题：(i) 快速识别并选择最佳臂，以及(ii) 在一系列T个连续回合中最大化奖励。尽管每个目标都已经得到了独立的深入研究，即(i)的最佳臂识别和(ii)的遗憾最小化，但是同时实现这两个目标仍然是一个开放的问题，尽管它在实践中非常重要。本文引入了“遗憾最小化的最佳臂识别”(ROBAI)，旨在实现这两个双重目标。为了解决具有预定停止时间和自适应停止时间要求的ROBAI，我们分别提出了$\mathsf{EOCP}$算法及其变体，不仅在高斯老虎机和一般老虎机中达到了渐进最优遗憾，而且在预定停止时间下，在$\mathcal{O}(\log T)$回合内选择了最佳臂，在自适应停止时间下，选择了最佳臂在$\mathcal{O}(\log^2 T)$回合内。

    This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping ti
    
[^53]: 不完整观测数据的联邦因果效应学习

    Federated Learning of Causal Effects from Incomplete Observational Data. (arXiv:2308.13047v1 [cs.LG])

    [http://arxiv.org/abs/2308.13047](http://arxiv.org/abs/2308.13047)

    我们提出了一种联邦学习的方法，可以从多个分布式和不完整的数据源中进行因果推断，估计因果效应并解决因为缺失值引入的偏差问题。

    

    分布式和不完整的数据源在实际应用中很常见，对因果推断提出了巨大挑战。由于隐私限制，这些数据源无法合并为一个实体，而其中的缺失值可能会引入偏差到因果估计中。我们提出了一种新的方法，可以从多个分布式和不完整的数据源中进行联邦因果推断，从而估计因果效应。我们的方法将损失函数拆分为多个组件，每个组件对应于具有缺失值的特定数据源。我们的方法在缺失随机假设下考虑了缺失数据，并估计了因果估计的高阶统计量。我们的方法从分散的数据源中恢复观察到的混淆变量的条件分布，以识别因果效应。我们的框架估计了异质的条件分布以应对不完整的数据源。

    Decentralized and incomplete data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints, and the presence of missing values within them can potentially introduce bias to the causal estimands. We introduce a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. Our approach disentangles the loss function into multiple components, each corresponding to a specific data source with missing values. Our approach accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. Our method recovers the conditional distribution of missing confounders given the observed confounders from the decentralized data sources to identify causal effects. Our framework estimates heterogeneou
    
[^54]: 机器学习模型的局部鲁棒性的高效估计

    Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])

    [http://arxiv.org/abs/2307.13885](http://arxiv.org/abs/2307.13885)

    本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    

    机器学习模型通常需要对噪声输入数据具有鲁棒性。现实世界中的噪声（通常是随机的）对模型预测的影响可以通过模型的局部鲁棒性来捕捉，即在输入周围的局部区域内模型预测的一致性。然而，基于蒙特卡罗采样的计算局部鲁棒性的朴素方法在统计上是低效的，对于大规模应用而言计算成本高昂。在这项工作中，我们通过局部线性函数逼近和多元正态CDF开发了首个分析估计器，以高效计算多类别判别模型的局部鲁棒性。通过这些估计器的推导，我们展示了局部鲁棒性与随机平滑和softmax概率等概念的联系。我们还通过实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
    
[^55]: FedNoisy: 分布式噪声标签学习基准测试

    FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11650](http://arxiv.org/abs/2306.11650)

    FedNoisy是第一个标准化的联合噪声标签学习基准测试，并提供20个基本设置和标准化的仿真流程，以帮助研究人员探索联合学习中噪声标签的影响。

    

    联合学习已经因为无需对来自客户端的敏感数据进行聚合而变得受欢迎。但是，数据隔离的分布式和孤立性可能会受到数据质量的复杂性的影响，使其更容易受到噪声标签的干扰。许多努力都致力于在集中式或联合式环境中防御噪声标签的负面影响。然而，缺乏一个全面考虑各种典型联合学习场景中噪声标签影响的基准测试。在这项工作中，我们提供了第一个标准化的基准测试，可以帮助研究人员充分探索潜在的联合噪声设置。此外，我们进行了全面的实验，探索这些数据设置的特性，并揭示了联合学习中的挑战性场景，这可能指导未来的方法开发。我们强调我们基准测试中提出的20个基本设置，适用于5个以上的数据集，并提供了标准化的仿真流程。

    Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
    
[^56]: 使用语义ID进行更好的泛化：推荐排名的案例研究

    Better Generalization with Semantic IDs: A case study in Ranking for Recommendations. (arXiv:2306.08121v1 [cs.IR])

    [http://arxiv.org/abs/2306.08121](http://arxiv.org/abs/2306.08121)

    本文提出使用语义ID解决推荐系统中的物品冷启动问题，这些ID是从内容嵌入中学习的，可以捕捉概念的层次关系，相较于完全消除ID特征的方法，语义ID能更好地提高推荐质量。

    

    在推荐模型中，训练好的物品表示是至关重要的。通常，一项商品会被分配一个唯一的随机生成的ID，并且通常会通过学习与随机ID值相对应的嵌入来表示。虽然这种方法被广泛使用，但在物品数量大且物品服从幂律分布的情况下——这是真实世界推荐系统的典型特征——会有一定局限性。这会导致物品冷启动问题，模型无法对尾部和以前未见过的物品进行可靠的推荐。完全消除这些ID特征及其学习的嵌入以解决冷启动问题会严重降低推荐质量。基于内容的物品嵌入更为可靠，但对于用户过去的物品交互序列来说，它们成本高且使用困难。本文中，我们使用语义ID来表示离散的物品，这些ID是通过使用RQ-VAE从内容嵌入中学习的，可以捕捉概念的层次关系。

    Training good representations for items is critical in recommender models. Typically, an item is assigned a unique randomly generated ID, and is commonly represented by learning an embedding corresponding to the value of the random ID. Although widely used, this approach have limitations when the number of items are large and items are power-law distributed -- typical characteristics of real-world recommendation systems. This leads to the item cold-start problem, where the model is unable to make reliable inferences for tail and previously unseen items. Removing these ID features and their learned embeddings altogether to combat cold-start issue severely degrades the recommendation quality. Content-based item embeddings are more reliable, but they are expensive to store and use, particularly for users' past item interaction sequence. In this paper, we use Semantic IDs, a compact discrete item representations learned from content embeddings using RQ-VAE that captures hierarchy of concep
    
[^57]: 通过数据混合消除预训练模型中的虚假相关性

    Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])

    [http://arxiv.org/abs/2305.14521](http://arxiv.org/abs/2305.14521)

    本文提出了一种通过数据混合来消除预训练模型中虚假相关性的方法，来提高模型对于新样本的预测能力。这种方法经过理论证明和多种任务实验验证，可以取得良好的效果。

    

    在大数据集上预训练的机器学习模型取得了显著的收敛性和鲁棒性。然而，这些模型往往利用了某些属性和标签之间的虚假相关性，在特定类别的大多数示例中普遍存在，但并不足以预测这些类别。学到的虚假相关性可能会在对新数据进行微调后仍然存在，这会降低模型对不展现虚假相关性的示例的性能。本文提出了一种简单而高效的方法，以消除预训练模型中的虚假相关性。我们方法的关键思想是利用一小组带有虚假属性的示例，并通过数据混合来平衡所有类别中的虚假属性。我们在理论上证实了我们的方法的有效性，并在各种视觉和NLP任务上进行了实证，包括消除虚假相关性。

    Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
    
[^58]: HyFL:一种用于私有联合学习的混合框架

    HyFL: A Hybrid Framework For Private Federated Learning. (arXiv:2302.09904v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09904](http://arxiv.org/abs/2302.09904)

    HyFL是一种混合框架，它结合了安全多方计算技术和分层联合学习，并能够在分布式环境中保证数据和全局模型的隐私安全，有助于大规模部署。

    

    联合学习已经成为分布式机器学习的有效方法，通过在客户端设备上保留训练数据来确保数据隐私。然而，最近的研究强调了FL中的漏洞，包括通过单个模型更新甚至整个全局模型泄漏敏感信息。虽然关注点已放在客户端数据隐私上，但有限的研究解决了全局模型隐私问题。此外，客户端本地训练为恶意客户端启动强大的模型污染攻击开辟了途径。不幸的是，目前没有现有工作提供全面解决所有这些问题的解决方案。因此，我们介绍了HyFL，这是一种混合框架，可实现数据和全局模型隐私，并促进大规模部署。HyFL的基础是安全多方计算技术和分层联合学习的独特组合。在HyFL的训练过程中，客户端模型在多个抽象层次上进行安全聚合，以在分布式环境中提供隐私保护。实验证明，HyFL在大规模数据集上实现良好性能，同时确保客户端数据和全局模型的强大隐私和安全保障。

    Federated learning (FL) has emerged as an efficient approach for large-scale distributed machine learning, ensuring data privacy by keeping training data on client devices. However, recent research has highlighted vulnerabilities in FL, including the potential disclosure of sensitive information through individual model updates and even the aggregated global model. While much attention has been given to clients' data privacy, limited research has addressed the issue of global model privacy. Furthermore, local training at the client's side has opened avenues for malicious clients to launch powerful model poisoning attacks. Unfortunately, no existing work has provided a comprehensive solution that tackles all these issues. Therefore, we introduce HyFL, a hybrid framework that enables data and global model privacy while facilitating large-scale deployments. The foundation of HyFL is a unique combination of secure multi-party computation (MPC) techniques with hierarchical federated learnin
    
[^59]: 基于观测器的逆强化学习中的非唯一性和等价解的收敛性研究

    Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.16299](http://arxiv.org/abs/2210.16299)

    本文研究了在线、实时解决逆强化学习中存在的多个解的挑战，提出了一种能够收敛到近似等价解的正则化历史堆栈观察器。通过开发新的数据丰富性条件，证明了该技术的有效性。

    

    在在线和实时解决确定性逆强化学习问题中，存在多个解的是一个关键挑战。非唯一性需要研究等价解的概念，即结果在不同的代价函数但相同的反馈矩阵，以及收敛到这些解的方法。尽管已经在文献中开发了离线算法以收敛到等价解，但尚未提供解决非唯一性的在线、实时技术。本文提出了一种能够收敛到逆强化学习问题的近似等价解的正则化历史堆栈观察器。发展了新的数据丰富性条件以促进分析，并通过模拟结果展示了所开发技术的有效性。

    A key challenge in solving the deterministic inverse reinforcement learning (IRL) problem online and in real-time is the existence of multiple solutions. Nonuniqueness necessitates the study of the notion of equivalent solutions, i.e., solutions that result in a different cost functional but same feedback matrix, and convergence to such solutions. While offline algorithms that result in convergence to equivalent solutions have been developed in the literature, online, real-time techniques that address nonuniqueness are not available. In this paper, a regularized history stack observer that converges to approximately equivalent solutions of the IRL problem is developed. Novel data-richness conditions are developed to facilitate the analysis and simulation results are provided to demonstrate the effectiveness of the developed technique.
    
[^60]: 具有一般成本函数的神经最优传输

    Neural Optimal Transport with General Cost Functionals. (arXiv:2205.15403v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15403](http://arxiv.org/abs/2205.15403)

    该论文介绍了一种新颖的神经网络算法，用于计算具有一般成本函数的最优传输方案。相比于常见的欧几里得成本，这种方法更灵活，并允许使用辅助信息构建传输映射。此外，该方法还解决了在高维空间下处理新数据点的挑战，并提供了理论误差分析。作为应用，该论文构造了一个能够在保留类别结构的同时映射数据分布的成本函数。

    

    我们引入了一种新颖的基于神经网络的算法，用于计算具有一般成本函数的最优传输方案。与常见的欧几里得成本（如$\ell^1$或$\ell^2$）不同，这种函数提供了更大的灵活性，并允许使用辅助信息（如类别标签）来构建所需的传输映射。现有的一般成本方法是离散的，并且在实践中存在限制，即它们不能提供样本外的估计。我们解决了针对一般成本设计连续的最优传输方法的挑战，该方法能够推广到高维空间（如图像）中的新数据点。此外，我们还对我们恢复的传输方案进行了理论误差分析。作为应用，我们构造了一个成本函数，用于在保留类别结构的同时映射数据分布。

    We introduce a novel neural network-based algorithm to compute optimal transport (OT) plans for general cost functionals. In contrast to common Euclidean costs, i.e., $\ell^1$ or $\ell^2$, such functionals provide more flexibility and allow using auxiliary information, such as class labels, to construct the required transport map. Existing methods for general costs are discrete and have limitations in practice, i.e. they do not provide an out-of-sample estimation. We address the challenge of designing a continuous OT approach for general costs that generalizes to new data points in high-dimensional spaces, such as images. Additionally, we provide the theoretical error analysis for our recovered transport plans. As an application, we construct a cost functional to map data distributions while preserving the class-wise structure.
    
[^61]: 隐私在线随机学习的近似最优算法。

    Near-Optimal Algorithms for Private Online Learning in a Stochastic Environment. (arXiv:2102.07929v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.07929](http://arxiv.org/abs/2102.07929)

    本文提出了两种隐私在线随机学习的算法，包括差分隐私的随机赌博机算法和私有随机在线学习的完全信息版本。其中，我们提出的随时可用的基于UCB的算法达到了最优性能。

    

    本文研究了两种私有随机在线学习的变体。第一种变体是差分隐私的随机赌博机算法。本文提出了一种随时可用的基于UCB的算法，可以达到最优性能。第二种变体是私有随机在线学习的完全信息版本。我们提出了一种新的算法，可同时获得隐私和性能的良好表现。

    We consider two variants of private stochastic online learning. The first variant is differentially private stochastic bandits. Previously, Sajed and Sheffet (2019) devised the DP Successive Elimination (DP-SE) algorithm that achieves the optimal $ O \biggl(\sum\limits_{1\le j \le K: \Delta_j >0} \frac{ \log T}{ \Delta_j} + \frac{ K\log T}{\epsilon} \biggr)$ problem-dependent regret bound, where $K$ is the number of arms, $\Delta_j$ is the mean reward gap of arm $j$, $T$ is the time horizon, and $\epsilon$ is the required privacy parameter. However, like other elimination style algorithms, it is not an anytime algorithm. Until now, it was not known whether UCB-based algorithms could achieve this optimal regret bound. We present an anytime, UCB-based algorithm that achieves optimality. Our experiments show that the UCB-based algorithm is competitive with DP-SE. The second variant is the full information version of private stochastic online learning. Specifically, for the problem of deci
    

