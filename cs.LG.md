# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Credal Learning Theory](https://rss.arxiv.org/abs/2402.00957) | 本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。 |
| [^2] | [Mitigating Reversal Curse via Semantic-aware Permutation Training](https://arxiv.org/abs/2403.00758) | 逆转诅咒问题是导致因果语言模型无法进行双向推理的根本原因之一，在这篇论文中，我们提出了通过语义感知的置换训练来缓解这一问题。 |
| [^3] | [A time-stepping deep gradient flow method for option pricing in (rough) diffusion models](https://arxiv.org/abs/2403.00746) | 提出了一种时间步进深度梯度流方法，用于处理（粗糙）扩散模型中的期权定价问题，保证了对大金额水平下期权价格的渐近行为和先验上下界。 |
| [^4] | [AtP*: An efficient and scalable method for localizing LLM behaviour to components](https://arxiv.org/abs/2403.00745) | AtP*是一种将LLM行为准确定位到组件的高效可扩展方法，通过解决Attribution Patching存在的显著假阴性问题，提供了显著改进以及进一步的性能提升。 |
| [^5] | [Subhomogeneous Deep Equilibrium Models](https://arxiv.org/abs/2403.00720) | 本文提出了一种新的分析方法，基于子齐次算子和非线性Perron-Frobenius理论，为隐式深度神经网络的固定点存在性和唯一性提供了更灵活的框架。 |
| [^6] | [Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds](https://arxiv.org/abs/2403.00715) | 通过引入竞争分析框架，我们提出了调整FTRL学习率的更新规则，使其在常数因子内达到最佳竞争比，并且展示了当惩罚项具有近似单调性时的竞争比特性。 |
| [^7] | [Defining Expertise: Applications to Treatment Effect Estimation](https://arxiv.org/abs/2403.00694) | 专家决策者的行动自然地编码了其领域知识的一部分，可以帮助在同一领域内进行推断，从而在治疗效果估计中利用专业知识作为归纳偏差可能是有益的。 |
| [^8] | [Scalable Learning of Item Response Theory Models](https://arxiv.org/abs/2403.00680) | 该论文提出了一种从大数据中学习项目反应理论模型中的潜在变量的方法，利用这些模型与逻辑回归之间的相似性来提高计算的效率和可伸缩性。 |
| [^9] | [Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate](https://arxiv.org/abs/2403.00675) | 通过重要性抽样在自然策略梯度中重用历史轨迹可提高收敛速率 |
| [^10] | [Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency](https://arxiv.org/abs/2403.00673) | 提出了快照强化学习（SnapshotRL）框架，通过简单改变环境来增强样本效率，而无需对算法和模型进行任何修改 |
| [^11] | [Advancing Additive Manufacturing through Deep Learning: A Comprehensive Review of Current Progress and Future Challenges](https://arxiv.org/abs/2403.00669) | 深度学习在增材制造领域显示出巨大潜力，能够克服高维数据的复杂挑战，推动该领域不断发展。 |
| [^12] | [Stability-Certified Learning of Control Systems with Quadratic Nonlinearities](https://arxiv.org/abs/2403.00646) | 本文主要关注开发一种方法，以便推断具有固有稳定性保证的二次控制动态系统。 |
| [^13] | [Rethinking The Uniformity Metric in Self-Supervised Learning](https://arxiv.org/abs/2403.00642) | 通过识别并满足现有均匀性度量未能达标的五个基本性质，本文引入了一个对维度崩溃敏感的新均匀性度量。 |
| [^14] | [Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency](https://arxiv.org/abs/2403.00625) | 该论文介绍了一种专门设计用于减轻新任务中偏见的高效且稳健的微调框架，通过中和对不同人口群体预测有影响力的权重，从而提升公平性和效率。 |
| [^15] | [Generalized User Representations for Transfer Learning](https://arxiv.org/abs/2403.00584) | 提出了一个在大规模推荐系统中有效表示用户口味的通用用户表示框架，结合表示学习和迁移学习，同时提出了管理生产模型中该框架部署的新颖解决方案。 |
| [^16] | [SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study](https://arxiv.org/abs/2403.00578) | 该研究分析了SINDy技术在非线性识别中的有效性，发现其在处理真实动态系统时面临处理未观察状态和非平滑动力学的困难，为了在这些挑战性背景下也能利用SINDy，提出了解决方案。 |
| [^17] | [Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms](https://arxiv.org/abs/2403.00574) | 本文研究通过采用新颖方法，从一系列轨迹中估计随机优化器的稳态分布，填补了深度学习优化中关于优化和泛化能力之间关系的理解空白。 |
| [^18] | [Rethinking cluster-conditioned diffusion models](https://arxiv.org/abs/2403.00570) | 通过结合最新的图片聚类和扩散模型技术，本文提出了一种在考虑最佳聚类粒度的情况下实现最先进FID并具有较强训练样本效率的聚类条件扩散模型，并提出了一种新颖方法来减少视觉组搜索空间。 |
| [^19] | [EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data](https://arxiv.org/abs/2403.00564) | EfficientZero V2在有限数据情况下通过一系列改进，在多个任务中超越了当前最先进水平，并且相比于通用算法DreamerV3有显著提升 |
| [^20] | [Indirectly Parameterized Concrete Autoencoders](https://arxiv.org/abs/2403.00563) | 本文提出了间接参数化CAEs（IP-CAEs）来解决具体自编码器（CAEs）在稳定联合优化方面的问题，IP-CAEs在多个数据集上表现出显著且一致的改进。 |
| [^21] | [Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking](https://arxiv.org/abs/2403.00550) | 本研究提出了模仿学习数据集，通过专家政策的组织、现成数据集和技术的提供以及常见模仿学习技术实现的分享，解决了模仿学习中缺乏可用数据和评估一致性的问题。 |
| [^22] | [Machine Learning Training Optimization using the Barycentric Correction Procedure](https://arxiv.org/abs/2403.00542) | 该研究提出了一种将机器学习算法与重心校正程序（BCP）结合的方法，旨在解决高维空间中长时间执行的问题，并在合成和真实数据中展示了该方法的显著优势。 |
| [^23] | [Epsilon-Greedy Thompson Sampling to Bayesian Optimization](https://arxiv.org/abs/2403.00540) | 将$\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。 |
| [^24] | [VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis](https://arxiv.org/abs/2403.00529) | VoxGenesis是一个无监督语音合成框架，可以在没有监督的情况下发现潜在说话者流形和有意义的语音编辑方向，为解决情感、语调和说话风格等难以获取准确标签的人声特征提供了新方法。 |
| [^25] | [Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning](https://arxiv.org/abs/2403.00514) | 实施超过60种不同的离策略代理，发现某些组合表现出稳健和优越的性能，揭示了特定正则化设置与任务的关联性约几维多多。 |
| [^26] | [Learning and Leveraging World Models in Visual Representation Learning](https://arxiv.org/abs/2403.00504) | 通过联合嵌入预测架构（JEPA）以及引入图像世界模型（IWM），本研究探讨了如何将JEPA预测任务概括为更广泛的数据损坏形式，并研究了学习性能良好的IWM的关键方面。此外，通过微调可以将IWM学到的预测世界模型用于解决各种任务，最终控制所学习表示的抽象级别。 |
| [^27] | [A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications](https://arxiv.org/abs/2403.00485) | 该论文综述了几何图神经网络的数据结构、模型和应用，通过提出具备不变性/等变性属性的几何GNN来更好地表征几何图的几何形状和拓扑，并提供了现有模型的统一视角。 |
| [^28] | [Autonomous Robotic Arm Manipulation for Planetary Missions using Causal Machine Learning](https://arxiv.org/abs/2403.00470) | 使用因果机器学习，在模拟行星环境中训练机器臂自主研究并互动物体，以揭示潜在的因果因素。 |
| [^29] | [Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for Discretionary Lane Change](https://arxiv.org/abs/2403.00446) | 本文首次将安全的混合行动强化学习引入到自主变道中，提出了新的算法PASAC-PIDLag，并进行了与不安全版本的比较分析。 |
| [^30] | [LoMOE: Localized Multi-Object Editing via Multi-Diffusion](https://arxiv.org/abs/2403.00437) | 通过多扩散过程实现零样本局部多对象编辑，赋予用户在图像中一次性添加、替换或编辑多对象的能力。 |
| [^31] | [HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding](https://arxiv.org/abs/2403.00425) | HALC是一种旨在减少大型视觉-语言模型中对象幻觉的新颖解码算法，通过局部的自动聚焦基准机制和全局的波束搜索算法，成功减少OH而保持文本生成质量，同时可以作为即插即用模块集成到任何LVLMs中。 |
| [^32] | [Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis](https://arxiv.org/abs/2403.00423) | 本研究探讨了ML-UQ校准统计量的使用问题，发现一些统计量对于生成分布的选择过于敏感，可能影响校准诊断。 |
| [^33] | [Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey](https://arxiv.org/abs/2403.00420) | 通过对抗性训练来改进DRL对条件变化的鲁棒性，研究者系统分析了当代对抗攻击方法，提供了详细见解。 |
| [^34] | [Provably Robust DPO: Aligning Language Models with Noisy Feedback](https://arxiv.org/abs/2403.00409) | 通过引入面向随机偏好翻转的策略优化通用框架，本研究旨在理解存在嘈杂反馈时的DPO算法，从而解决语言模型对齐人类兴趣中的挑战。 |
| [^35] | [Fractal interpolation in the context of prediction accuracy optimization](https://arxiv.org/abs/2403.00403) | 该研究提出了三种基于分形插值的数据增强策略，分别是“最接近Hurst策略”、“最接近值策略”和“公式策略”，并使用四个公共数据集和一组来自罗马尼亚布拉索夫市气象记录的私人数据集来验证这些策略。 |
| [^36] | [Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems](https://arxiv.org/abs/2403.00381) | 提出了一种基于结构化DNN的控制器，通过设计神经网络结构确保闭环稳定性，并进一步优化参数以实现改进的控制性能，同时提供了关于跟踪误差的明确上限。 |
| [^37] | [Invariant Test-Time Adaptation for Vision-Language Model Generalization](https://arxiv.org/abs/2403.00376) | 本文提出了一个测试时提示调优范式，通过优化可学习的提示，迫使模型利用真正的因果不变特征，以解决视觉-语言模型在特定任务需求上无法有效利用预训练特征的挑战。 |
| [^38] | [Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning](https://arxiv.org/abs/2403.00352) | 解缠表示对抽象视觉推理这一基本下游任务是不必要的，表示信息量的丰富程度更能影响下游性能。 |
| [^39] | [Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks](https://arxiv.org/abs/2403.00344) | 提出了一个框架，通过训练适应多样化护理接收者响应的鲁棒护理人员政策，以提高多智能体强化学习中政策的鲁棒性。 |
| [^40] | [Nonlinear Sheaf Diffusion in Graph Neural Networks](https://arxiv.org/abs/2403.00337) | 探索在图神经网络中引入非线性拉普拉斯的潜在益处，通过实验分析验证了模型在图相关任务中的实际有效性。 |
| [^41] | [Learning with Logical Constraints but without Shortcut Satisfaction](https://arxiv.org/abs/2403.00329) | 引入了逻辑连接词的双重变量来解决捷径满足问题，提出了一个新的学习逻辑约束的框架，实验证明其在模型的普适性和约束满足方面性能优越。 |
| [^42] | [Softened Symbol Grounding for Neuro-symbolic Systems](https://arxiv.org/abs/2403.00323) | 提出了一种软化符号接地过程，有效地桥接了神经网络训练和符号约束求解，形成了一个高效的神经符号学习框架 |
| [^43] | [DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things](https://arxiv.org/abs/2403.00321) | DEEP-IoT通过“更多监听，更少传输”的策略，挑战和转变了传统的物联网通信模型，大幅降低能耗并提高设备寿命。 |
| [^44] | [Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode](https://arxiv.org/abs/2403.00318) | 该论文介绍了一种基于深度强化学习的方法，旨在解决管理问题，如库存管理、动态定价和推荐，提出了一种基于变压器神经网络结构的大型管理模型，能够超越传统启发式方法解决管理任务，实现跨领域决策协调，证明了在复杂动态商业环境中DRL框架的有效性。 |
| [^45] | [Axe the X in XAI: A Plea for Understandable AI](https://arxiv.org/abs/2403.00315) | 论文辩护了采用“可理解的人工智能”标签作为替代“XAI”，以避免围绕XAI目标和目的的混乱，并主张采用更适合的实用理解概念。 |
| [^46] | [Universal Auto-encoder Framework for MIMO CSI Feedback](https://arxiv.org/abs/2403.00299) | 该论文提出了一个通用的自编码器框架，可以支持不同输入大小和多种压缩比，相比于朴素和最先进方法，在降低硬件复杂度的同时提供了可比的性能表现。 |
| [^47] | [Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification](https://arxiv.org/abs/2403.00293) | 提出了一个有效的适配器框架，旨在将自监督语音模型调整为说话者验证任务，通过并行适配器设计，允许在中间Transformer层调整潜在特征以及在所有Transformer层的输出嵌入 |
| [^48] | [Semantic Text Transmission via Prediction with Small Language Models: Cost-Similarity Trade-off](https://arxiv.org/abs/2403.00290) | 该研究通过使用小型语言模型进行预测，实现了在语义文本传输中的成本和相似度之间的权衡。 |
| [^49] | [Optimization of Array Encoding for Ultrasound Imaging](https://arxiv.org/abs/2403.00289) | 利用机器学习构建扫描序列，产生高质量超声B模式图像，并优化超声成像编码序列。 |
| [^50] | [A Survey of Route Recommendations: Methods, Applications, and Opportunities](https://arxiv.org/abs/2403.00284) | 基于城市计算的路线推荐综述对路线推荐研究中的传统机器学习和现代深度学习方法进行了分类，展示了与城市计算场景相关的新应用，并揭示了最新进展。 |
| [^51] | [Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2403.00282) | 提出了一种名为CoMOGA的约束多目标梯度聚合算法，通过将目标转换为约束，实现了对帕累托最优策略的求解，同时满足预定义约束。 |
| [^52] | [Shifted Interpolation for Differential Privacy](https://arxiv.org/abs/2403.00278) | 本文在统一框架下建立了“通过迭代实现隐私放大”现象，提高了先前分析的水平，并由此获得了其他差分隐私概念更紧密的隐私核算。 |
| [^53] | [Graph Construction with Flexible Nodes for Traffic Demand Prediction](https://arxiv.org/abs/2403.00276) | 本文提出一种新型的图构建方法，利用基于密度的聚类算法确定了图中节点的灵活定位，克服了传统算法的计算瓶颈，并从乘客数据中提取有价值信息用于初始化GNNs的边权重。 |
| [^54] | [ARED: Argentina Real Estate Dataset](https://arxiv.org/abs/2403.00273) | ARED是专为阿根廷市场设计的综合房地产价格预测数据集系列，尽管零版只包含短期信息，但展示了市场层面上的时间相关现象。 |
| [^55] | [Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval](https://arxiv.org/abs/2403.00272) | 通过提出一种基于注意力的双编码器架构和特殊设计的损失函数，本文实现了在训练过程中同时学习基于类别和基于对象身份的嵌入，从而显著提高了姿势不变的对象识别和检索性能。 |
| [^56] | [Parameter-Efficient Tuning of Large Convolutional Models](https://arxiv.org/abs/2403.00269) | 通过引入滤波器子空间和滤波器原子的概念，本研究提出了一种在微调大型卷积模型时仅调整少量参数来提取任务特定表示的方法。 |
| [^57] | [Deciphering diffuse scattering with machine learning and the equivariant foundation model: The case of molten FeO](https://arxiv.org/abs/2403.00259) | 用机器学习和等变基础模型成功地解读了熔融FeO的弥散散射，提供了实验和量子力学模型之间更好的一致性 |
| [^58] | ["Lossless" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach](https://arxiv.org/abs/2403.00258) | 本研究利用神经切线核和随机矩阵理论，提出了一种新颖的压缩方法，能够在高维度情形下对宽而全连接的深度神经网络进行有效压缩。 |
| [^59] | [Robust deep labeling of radiological emphysema subtypes using squeeze and excitation convolutional neural networks: The MESA Lung and SPIROMICS Studies](https://arxiv.org/abs/2403.00257) | 提出了一种利用压缩和激励卷积神经网络对肺CT图像进行sLTP和CTES分类的方法，实现了准确和可重现的肺部CT扫描sLTP分割。 |
| [^60] | [Cloud-based Federated Learning Framework for MRI Segmentation](https://arxiv.org/abs/2403.00254) | 提出了一个针对农村医疗环境的脑组织分割的新型框架，结合了深度强化学习环境和本地的精调模型，采用联邦学习进行模型训练，以维护数据隐私并增强模型泛化能力。 |
| [^61] | [EUROPA: A Legal Multilingual Keyphrase Generation Dataset](https://arxiv.org/abs/2403.00252) | 提出了一个用于法律领域多语关键词生成的数据集EUROPA，包含所有24种欧盟官方语言，表明在特定领域多语言语料库上仍有改进空间。 |
| [^62] | [Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance](https://arxiv.org/abs/2403.00236) | 通过使用FlanT5-XXL和SemEval 2016数据集，研究了零-shot立场检测在推特上的性能表现及其对提示和解码策略的敏感性，揭示了其能够匹敌或超越最先进基准测试的能力，并识别了其中的潜在偏见。 |
| [^63] | [Causal Bandits with General Causal Models and Interventions](https://arxiv.org/abs/2403.00233) | 该论文在三个方向上推进了因果赌博机的结果：在一般因果模型下进行干预设计，实现广义软干预以及提供一般的遗憾上下界。 |
| [^64] | [Diffraction and Scattering Aware Radio Map and Environment Reconstruction using Geometry Model-Assisted Deep Learning](https://arxiv.org/abs/2403.00229) | 该论文提出了一种利用几何结构辅助深度学习的方法，通过接收信号强度数据共同构建无线电地图和虚拟环境，同时开发了用于提取关键衍射特征和描述散射的模型。 |
| [^65] | [Robust Policy Learning via Offline Skill Diffusion](https://arxiv.org/abs/2403.00225) | 提出了一种新颖的离线技能学习框架DuSkill，通过引导扩散模型生成通用技能，从而增强不同领域任务的策略学习鲁棒性。 |
| [^66] | [Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale](https://arxiv.org/abs/2403.00222) | 该研究提出了SUB-SAMPLE-Q算法，通过对局部代理进行子采样，在指数级别的时间内计算出最佳策略，从而实现了与标准方法相比的指数加速。 |
| [^67] | [Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART](https://arxiv.org/abs/2403.00212) | 使用XLSR Wav2Vec2和mBART在自定义数据集上进行微调，实现了视频内容的多语言转录和翻译，为个性化语音提供了可访问的解决方案 |
| [^68] | [Improving Socratic Question Generation using Data Augmentation and Preference Optimization](https://arxiv.org/abs/2403.00199) | 通过数据增强和偏好优化，改进了苏格拉底提问生成方法，减轻教师繁重的工作量，防止生成无效问题。 |
| [^69] | [AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs](https://arxiv.org/abs/2403.00198) | AXOLOTL是一个新颖的后处理框架，通过零样本学习的三步过程，识别和解决偏见，指导模型自我去偏见其输出，从而实现公平性并保持模型性能。 |
| [^70] | [Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras](https://arxiv.org/abs/2403.00196) | 通过引入生成模型的方法解决了由于传感器帧率不匹配而导致数据缺失的问题，利用有条件的生成对抗网络（cGANs）中的pix2pix架构比CycleGAN表现更好，多视角输入风格特别是堆叠视图可以增强热图像生成的准确性 |
| [^71] | [Ask Your Distribution Shift if Pre-Training is Right for You](https://arxiv.org/abs/2403.00194) | 预训练有助于缓解较差的外推，但对数据集偏见无济于事。 |
| [^72] | [Impact of Decentralized Learning on Player Utilities in Stackelberg Games](https://arxiv.org/abs/2403.00188) | 研究了分散学习对斯塔克尔贝格博弈中玩家效用的影响，提出了一种放松遗憾基准来更好捕捉系统特征，并开发了实现近乎最优遗憾的算法。 |
| [^73] | [Entry-Specific Bounds for Low-Rank Matrix Completion under Highly Non-Uniform Sampling](https://arxiv.org/abs/2403.00184) | 该论文提出了针对高度非均匀采样条件下低秩矩阵完成问题的入口特定界限，通过定制每个条目的误差上界来匹配一定条件下的极小下界。 |
| [^74] | [Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems](https://arxiv.org/abs/2403.00178) | 提出了Causal Graph Ordinary Differential Equations (CAG-ODE) 模型，能够捕捉多智能体系统中处理的连续动态影响，利用图神经网络作为ODE函数。 |
| [^75] | [Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning](https://arxiv.org/abs/2403.00177) | 提出了一种使用物理知识的自监督学习算法，通过仅使用非侵入式患者健康数据识别数字孪生体模型参数，从而实现了非侵入式医学数字孪生体的构建。 |
| [^76] | [SoD$^2$: Statically Optimizing Dynamic Deep Neural Network](https://arxiv.org/abs/2403.00176) | 本文提出了SoD$^2$框架，用于静态优化动态深度神经网络，通过秩和维度传播（RDP）方法实现了操作符的形状静态确定，进而进行一系列优化，包括融合代码生成、执行计划和运行时内存分配计划生成。 |
| [^77] | [Go Beyond Black-box Policies: Rethinking the Design of Learning Agent for Interpretable and Verifiable HVAC Control](https://arxiv.org/abs/2403.00172) | 通过从现有热动力学模型和历史数据提取的决策树重新设计HVAC控制器，克服了可靠性瓶颈，并实现了更节能的策略。 |
| [^78] | [TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision](https://arxiv.org/abs/2403.00165) | 本文提出了一种最小监督的分层文本分类方法，利用每个节点的唯一类名作为唯一监督，同时结合大型语言模型（LLM）提高分类性能。 |
| [^79] | [Automated Efficient Estimation using Monte Carlo Efficient Influence Functions](https://arxiv.org/abs/2403.00158) | 该论文提出了一种名为蒙特卡洛高效影响函数（MC-EIF）的全自动技术，用于近似高效影响函数，能够实现针对广泛类别的模型和目标函数的统计估计，达到最优的收敛速度。 |
| [^80] | [Privacy-Preserving Distributed Optimization and Learning](https://arxiv.org/abs/2403.00157) | 差分隐私是最有前景的隐私保护方法，适用于现代学习应用中的高维优化，具有低的计算和通信复杂性。 |
| [^81] | [Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space](https://arxiv.org/abs/2403.00155) | 通过概率潜在空间提出了一个新的理论框架，解释了深度神经网络压缩的优化网络稀疏度，并探讨了网络层的AP3/AP2属性与性能之间的关系。 |
| [^82] | [Analysis of Kernel Mirror Prox for Measure Optimization](https://arxiv.org/abs/2403.00147) | 该论文研究了核镜像Prox算法用于度量优化问题的收敛性，提出了一种针对混合功能纳什均衡问题的算法，并在无限维空间中证明了其收敛速度为$O(1/N)。 |
| [^83] | [EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation](https://arxiv.org/abs/2403.00144) | 提出了一种集成方法EBBS，配合新颖的双层束搜索算法，能够优于直接和通过第三语言进行的翻译，并实现知识蒸馏来提高推理效率。 |
| [^84] | [Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging](https://arxiv.org/abs/2403.00143) | 通过树平均法构建集成解析器，稳定并提升无监督不连续成分句法分析性能，实验结果表明该方法在所有指标上均优于基准线 |
| [^85] | [UniTS: Building a Unified Time Series Model](https://arxiv.org/abs/2403.00131) | UNITS是一种统一的时间序列模型，通过独特的统一网络骨干实现了通用任务规范，并成功支持多种任务，包括分类、预测、插补和异常检测。 |
| [^86] | [From Flies to Robots: Inverted Landing in Small Quadcopters with Dynamic Perching](https://arxiv.org/abs/2403.00128) | 通过模仿蝇类的颠倒降落行为，开发了适用于小型四轴飞行器的控制策略，使用强化学习来优化在不同天花板接近条件下的动态停泊。 |
| [^87] | [Federated Linear Contextual Bandits with Heterogeneous Clients](https://arxiv.org/abs/2403.00116) | 提出了一种适用于异构客户的联邦赌臂学习方法，通过在联邦学习设置下为客户进行聚类，实现了非平凡次线性后悔和通信成本的优化 |
| [^88] | [Longitudinal Counterfactuals: Constraints and Opportunities](https://arxiv.org/abs/2403.00105) | 本文提出使用纵向数据来评估和改善反事实中的可信度，并开发了一种比较纵向变化与反事实差异的度量标准，从而生成可信的反事实。 |
| [^89] | [On Robustness and Generalization of ML-Based Congestion Predictors to Valid and Imperceptible Perturbations](https://arxiv.org/abs/2403.00103) | 本研究研究了基于机器学习的EDA工具中的健壮性问题，提出了针对VLSI布局的新颖不可察觉性概念。 |
| [^90] | [Scaling up Dynamic Edge Partition Models via Stochastic Gradient MCMC](https://arxiv.org/abs/2403.00044) | 该论文通过引入Dirichlet先验规范和Dirichlet马尔可夫链构建，扩展了边缘划分模型（EPM）以适应动态环境，并提出了一个简单的Gibbs采样器来处理后验计算。 |
| [^91] | [Global and Local Prompts Cooperation via Optimal Transport for Federated Learning](https://arxiv.org/abs/2403.00041) | 提出了联邦提示合作 via Optimal Transport（FedOTP）方法，通过最优输运实现全局和本地提示的合作，针对数据异质性设计了高效的协作提示学习策略。 |
| [^92] | [Influencing Bandits: Arm Selection for Preference Shaping](https://arxiv.org/abs/2403.00036) | 该论文考虑了在非静态多臂赌博机中，通过观察奖励来积极和消极地强化人群偏好，并提出了用于最大化支持预定手臂的人口比例的算法。对于不同意见动态，提出了不同的策略并分析了后悔，最后讨论了多个推荐系统共存的情况。 |
| [^93] | [Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network](https://arxiv.org/abs/2403.00033) | 通过结合动态内在功能网络和LSTM技术，使用高阶注意力模块进行信息融合和消息传递，提出了HOGAB模型，对慢性大麻用户的静息态fMRI数据进行分析，提高了多图分类的准确性。 |
| [^94] | [GraphPub: Generation of Differential Privacy Graph with High Availability](https://arxiv.org/abs/2403.00030) | 提出了一种名为GraphPub的新型图边保护框架，通过反向学习和编码器-解码器机制，在保护图拓扑结构的同时保证数据的可用性基本不变。 |
| [^95] | [Lower Bounds for Differential Privacy Under Continual Observation and Online Threshold Queries](https://arxiv.org/abs/2403.00028) | 该论文展示了针对差分隐私在持续观测和在线阈值查询情形下关于时间步长和事件数量的新下界结果。 |
| [^96] | [A Quick Framework for Evaluating Worst Robustness of Complex Networks](https://arxiv.org/abs/2403.00027) | 研究引入了Most Destruction Attack（MDA）概念，用于评估网络的最差稳健性，以确定在最坏情况下系统的极限鲁棒性。 |
| [^97] | [Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2403.00026) | 提出了蒙特利尔容量车辆路径问题的基础模型（FM-MCVRP），将MCVRP视为类似自然语言处理任务，并利用Transformer架构嵌入大型语言模型框架进行训练。 |
| [^98] | [On the Challenges and Opportunities in Generative AI](https://arxiv.org/abs/2403.00025) | 现代生成人工智能范例中存在关键的未解决挑战，如何解决这些挑战将进一步增强它们的能力、多功能性和可靠性，并为研究方向提供有价值的见解。 |
| [^99] | [FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class Classification in Flow Cytometry Benchmarking](https://arxiv.org/abs/2403.00024) | 本文介绍了FlowCyt项目，提出了针对流式细胞术数据中多类别单细胞分类的全面基准测试，其中图神经网络（GNNs）通过利用图编码数据中的空间关系展现出卓越的性能。 |
| [^100] | [Auditable Homomorphic-based Decentralized Collaborative AI with Attribute-based Differential Privacy](https://arxiv.org/abs/2403.00023) | 提出了一种名为AerisAI的可审计同态协作人工智能框架，利用同态加密和细粒度差分隐私提高安全性，通过基于区块链的智能合约直接聚合加密参数，消除了对于模型性能的负面影响 |
| [^101] | [Transformer-based Parameter Estimation in Statistics](https://arxiv.org/abs/2403.00019) | 提出了一种基于Transformer的参数估计方法，相较于传统方法，不需要封闭形式解或数学推导，也不需要概率密度函数，仅需经过训练的Transformer模型进行一次推断即可估计潜在分布的参数。 |
| [^102] | [Towards Interpreting Multi-Objective Feature Associations](https://arxiv.org/abs/2403.00017) | 提出一种使用多标签的客观特征交互设计，结合全局敏感性分析，以在农业环境中找到最佳组合的新方法。 |
| [^103] | [Deep Sensitivity Analysis for Objective-Oriented Combinatorial Optimization](https://arxiv.org/abs/2403.00016) | 本研究提出了一种深度敏感性分析方法，结合神经网络反馈和全局敏感性分析，以在家禽管理中最优化降低多种病原体水平。 |
| [^104] | [GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion](https://arxiv.org/abs/2403.00014) | 本文提出了GIN-SD框架，通过位置编码和关注融合解决了在图中检测具有不完整节点的来源的挑战。 |
| [^105] | [Prioritizing Informative Features and Examples for Deep Learning from Noisy Data](https://arxiv.org/abs/2403.00013) | 提出了一个系统框架，通过优先选择信息特征和样本来增强深度学习中的开发过程 |
| [^106] | [PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling](https://arxiv.org/abs/2403.00012) | 提出了基于预排序GNN的定时预测方法，包括全局电路预训练、局部时延学习和注意力单元建模，以解决大规模工业电路中的信号衰减和误差累积问题。 |
| [^107] | [Introducing User Feedback-based Counterfactual Explanations (UFCE)](https://arxiv.org/abs/2403.00011) | 本研究引入了一种名为基于用户反馈的反事实解释（UFCE）的新方法，旨在解决当前反事实解释算法的局限性，并增强提供的解释的可信度。 |
| [^108] | [Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach](https://arxiv.org/abs/2402.19062) | 该研究提出了一种全面的方法，通过图卷积神经网络实现了自动超声心动图视图识别，结合了3D心脏网格重建和后续任务如分割和姿势估计。 |
| [^109] | [Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos](https://arxiv.org/abs/2402.18888) | 提出了一种基于不确定性的可拓展编码本的联邦学习框架，用于应对异构数据孤岛中模型适应新分布的挑战 |
| [^110] | [ICE-SEARCH: A Language Model-Driven Feature Selection Approach](https://arxiv.org/abs/2402.18609) | ICE-SEARCH是首个将语言模型与进化算法相结合用于特征选择任务的方法，在医学预测分析应用中取得了State-of-the-Art(SOTA)表现。 |
| [^111] | [SuperdropNet: a Stable and Accurate Machine Learning Proxy for Droplet-based Cloud Microphysics](https://arxiv.org/abs/2402.18354) | SuperdropNet是一种机器学习代理，旨在模拟液滴的云微物理，并通过使用多步自回归预测、物理约束和精细控制随机性来提高准确性和稳定性 |
| [^112] | [Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures](https://arxiv.org/abs/2402.17992) | 本研究提出了一种物理启发的机器学习方法，将科学原理和物理定律融入深度神经网络，用于建模非线性结构的地震响应。 |
| [^113] | [Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2402.17978) | 提出了一种名为Imagine, Initialize, and Explore (IIE)的新方法，利用变压器模型在复杂场景中实现多智能体的有效探索。 |
| [^114] | [Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning](https://arxiv.org/abs/2402.15997) | Cieran是一个允许数据分析师在Jupyter笔记本中设计图表时快速找到质量配色方案的工具，通过主动偏好学习范式进行排序和创建新的配色方案，帮助新手分析师定制配色方案以适应其数据背景。 |
| [^115] | [Spatially-Aware Transformer Memory for Embodied Agents](https://arxiv.org/abs/2402.15160) | 本文探讨了利用包含空间信息的面向空间感知变压器模型，以改善记忆利用效率。 |
| [^116] | [What's in a Name? Auditing Large Language Models for Race and Gender Bias](https://arxiv.org/abs/2402.14875) | 调查发现，大型语言模型存在种族和性别偏见，尤其对与黑人女性相关的名字表现最不利。审计在模型部署和实施时的重要性得到强调。 |
| [^117] | [Edge Caching Based on Deep Reinforcement Learning and Transfer Learning](https://arxiv.org/abs/2402.14576) | 本文提出了一种基于双深度Q学习的缓存方法，通过半马尔可夫决策过程（SMDP）适应现实场景中随机请求到达的特性，综合考虑各种文件特征。 |
| [^118] | [Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization](https://arxiv.org/abs/2402.14270) | 通过硬样本加权持续训练的方法，该研究提出了IR-DRO框架，通过动态优先考虑训练中信息丰富的样本，以改善LLM泛化能力。 |
| [^119] | [Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records](https://arxiv.org/abs/2402.14042) | 本研究使用GANs生成了时间序列合成痴呆患者医疗记录的数据集，并比较了不同GAN模型在生成合成数据方面的质量，实现了在不涉及隐私问题的情况下保护用户数据并延伸数据应用。 |
| [^120] | [E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series](https://arxiv.org/abs/2402.14041) | E2USD提出了一种有效的无监督多元时间序列状态检测方法，利用了快速傅里叶变换和双视图嵌入模块进行编码，以及通过对抗学习方法消除假阴性，从而实现了SOTA准确性并显著降低了计算开销。 |
| [^121] | [BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2402.13918) | 本文通过对遥感图像中的云进行分割，旨在提高卫星图像分析的精度和效率，应用于环境监测、资源管理和灾害响应。 |
| [^122] | [How to validate average calibration for machine learning regression tasks ?](https://arxiv.org/abs/2402.10043) | 本文提出了两种验证机器学习回归任务平均校准性的方法，将校准误差与平均绝对误差之间的差值和将平均平方z-分数与1进行比较。研究发现，前者对不确定性分布敏感，而后者在该方面提供了最可靠的方法。 |
| [^123] | [Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting](https://arxiv.org/abs/2402.05956) | 本文提出了一种名为Pathformer的多尺度自适应路径的Transformer模型，用于时间序列预测。通过整合时间分辨率和时间距离进行多尺度建模，并使用自适应路径来优化建模过程，可以提高预测准确性和泛化能力。 |
| [^124] | [Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation](https://arxiv.org/abs/2402.03268) | 本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。 |
| [^125] | [Measuring Moral Inconsistencies in Large Language Models](https://arxiv.org/abs/2402.01719) | 本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。 |
| [^126] | [Killer Apps: Low-Speed, Large-Scale AI Weapons](https://arxiv.org/abs/2402.01663) | 本文研究了AI武器的概念、部署、检测和潜在对策，强调了在信息领域内基于AI的心理操纵的潜力，以及其对全球个人、组织和社会的威胁。 |
| [^127] | [Messenger RNA Design via Expected Partition Function and Continuous Optimization](https://arxiv.org/abs/2401.00037) | 将RNA设计问题转化为连续优化，并提出了基于期望分区函数的通用优化框架，将候选序列的分布逐步优化为单一序列。 |
| [^128] | [On Rate-Optimal Partitioning Classification from Observable and from Privatised Data](https://arxiv.org/abs/2312.14889) | 研究了在放宽条件下的分区分类方法的收敛速率，提出了绝对连续分量的新特性，计算了分类错误概率的精确收敛率 |
| [^129] | [Towards an end-to-end artificial intelligence driven global weather forecasting system](https://arxiv.org/abs/2312.12462) | 提出了一种端到端基于人工智能的全球天气预报系统，通过将AI技术应用于数据同化和天气预报模型，实现了从数据处理到预测全过程的自动化。 |
| [^130] | [Distributional Bellman Operators over Mean Embeddings](https://arxiv.org/abs/2312.07358) | 提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架，推导出新算法并展示可与深度强化学习结合，提高表现。 |
| [^131] | [Improving the performance of weak supervision searches using transfer and meta-learning](https://arxiv.org/abs/2312.06152) | 通过转移学习和元学习，本研究通过在模拟数据上训练神经网络来提高弱监督搜索的性能，从而减少了在实验数据上需要的信号量。 |
| [^132] | [Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use](https://arxiv.org/abs/2312.04455) | 本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。 |
| [^133] | [Imitation Bootstrapped Reinforcement Learning](https://arxiv.org/abs/2311.02198) | 提出了一种模仿引导式强化学习（IBRL）的框架，用于高效的样本-efficient RL，通过先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。 |
| [^134] | [Proving Linear Mode Connectivity of Neural Networks via Optimal Transport](https://arxiv.org/abs/2310.19103) | 证明了通过最优输运的方法，使用随机梯度下降训练的两层神经网络可以线性连接，同时提供了每层神经元权重独立的深度神经网络线性连接的上下界。 |
| [^135] | [Enhancing Group Fairness in Online Settings Using Oblique Decision Forests](https://arxiv.org/abs/2310.11401) | 提出了Aranyani，一种斜裁集成的方法，用于解决在在线环境中优化群体公平性目标所面临的挑战 |
| [^136] | [Tree Cross Attention](https://arxiv.org/abs/2309.17388) | 提出了一种基于交叉注意力的树形模块，能够仅从对数数量的标记中检索信息进行推断。 |
| [^137] | [Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks](https://arxiv.org/abs/2308.06960) | 设计更好的微调策略以在下游任务中更好地利用转移的知识并提高性能。 |
| [^138] | [Neural Additive Models for Location Scale and Shape: A Framework for Interpretable Neural Regression Beyond the Mean](https://arxiv.org/abs/2301.11862) | 提出了神经加性模型具有位置尺度和形状的神经加性模型(NAMLSS)框架，结合了经典深度学习模型的预测能力和分布回归的优势 |
| [^139] | [Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees](https://arxiv.org/abs/2210.01282) | 本论文提出了一种单循环估计算法，具有有限时间保证，能够处理高维状态空间的马尔可夫决策过程的结构估计问题，而不会损害奖励估计精度。 |
| [^140] | [Distributed Influence-Augmented Local Simulators for Parallel MARL in Large Networked Systems](https://arxiv.org/abs/2207.00288) | 本文展示了如何将大型网络系统分解为多个局部组件，构建独立并并行运行的模拟器，有效监测影响，并在几小时内训练大型多智能体系统，减轻同时学习的负面影响。 |
| [^141] | [Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning](https://arxiv.org/abs/2203.16464) | 本研究提出了一个新颖的框架，利用对抗逆强化学习，可以为强化学习模型所作出的决策提供全局解释，并捕捉直观的倾向。 |
| [^142] | [Memory-Efficient Sequential Pattern Mining with Hybrid Tries](https://arxiv.org/abs/2202.06834) | 提出了一种基于混合trie的内存高效序列模式挖掘方法，在内存消耗和计算时间方面相比现有技术有显著改善，且是唯一一个能够处理256GB系统内存下大数据集的方法。 |
| [^143] | [InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification](https://arxiv.org/abs/2109.07319) | 提出了一种轻量级框架InceptionXML，通过在embedding维度上重新分配卷积操作，应对短文本查询中的单词顺序缺失，同时提出了InceptionXML+框架，通过同步标签筛选器和极端分类器，改进了动态硬负采样技术。 |
| [^144] | [Deep learning insights into cosmological structure formation](https://arxiv.org/abs/2011.10577) | 通过构建深度学习框架，研究了各向异性信息在初始条件中如何影响暗物质暗团的最终质量，并发现各向异性添加了一些额外信息量。 |
| [^145] | [Bayesian Robust Optimization for Imitation Learning](https://arxiv.org/abs/2007.12315) | 提出了基于贝叶斯鲁棒优化的模仿学习方法，以在状态不确定性下同时考虑激进和保守的策略优化。 |
| [^146] | [Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control](https://arxiv.org/abs/1909.12077) | 通过设计哈密顿动力学与控制，Symplectic ODE-Net (SymODEN)可以透明地学习潜在动力学，从而揭示系统的相关物理方面。 |
| [^147] | [Finetuning Large Language Models for Vulnerability Detection.](http://arxiv.org/abs/2401.17010) | 本文优化了大规模语言模型用于源代码中的漏洞检测任务，通过微调最先进的代码语言模型WizardCoder并改进其训练过程和策略，实现了对漏洞数据集的分类性能的提升。 |
| [^148] | [Polynomial time auditing of statistical subgroup fairness for Gaussian data.](http://arxiv.org/abs/2401.16439) | 这篇论文研究了使用统计子组不公平性概念对分类器进行审计的问题，并给出了对高斯分布的审计结果。他们提供了一种替代方法来利用无偏学习的进展。 |
| [^149] | [The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations.](http://arxiv.org/abs/2401.13662) | 本文提供了一个深度强化学习中策略梯度算法的综合指南，包括理论基础、实际实现和比较结果，并对正则化的益处进行了讨论。 |
| [^150] | [Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response.](http://arxiv.org/abs/2401.10726) | 本研究通过优化聚合灵活性提供策略和评估HVAC系统的分散灵活性提供，为聚合器在可再生能源不确定性下实现需求响应提供了实用工具，从而实现了稳健的脱碳和增强能源系统的韧性。 |
| [^151] | [Characterising Gradients for Unsupervised Accuracy Estimation under Distribution Shift.](http://arxiv.org/abs/2401.08909) | 本文研究了在分布偏移下，利用梯度信息对真实测试准确性进行预测的方法。通过分析分类层梯度范数，我们发现在无法泛化到测试数据集时，调整模型以获得更大的梯度范数是有效的。 |
| [^152] | [Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting.](http://arxiv.org/abs/2401.06040) | 本论文提出了一种基于小波启发的多尺度图卷积循环网络，用于交通预测。该方法将多尺度分析方法和深度学习方法相结合，对交通数据中的多尺度结构进行建模，并展现了较好的性能。 |
| [^153] | [ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization.](http://arxiv.org/abs/2312.02277) | 本文提出了一种名为ALEXR的高效算法，用于解决凸有限和耦合组成随机优化问题。此算法在解决平滑和非平滑问题时具有优越的收敛速度，并且可应用于多个领域，包括组分布鲁棒优化、不平衡数据学习、强化学习和排序学习。 |
| [^154] | [Optimal Budgeted Rejection Sampling for Generative Models.](http://arxiv.org/abs/2311.00460) | 该论文提出了一种最佳限制预算拒绝采样方法（OBRS），可显著改善生成模型的样本质量和多样性。通过将采样方案与训练过程相结合，该方法在给定采样预算情况下，对于任何真实分布和拒绝后分布之间的f-散度都是最优的。 |
| [^155] | [Optimal Transport for Measures with Noisy Tree Metric.](http://arxiv.org/abs/2310.13653) | 本文提出了一种针对树度量有噪声的优化传输方法，通过引入新的不确定性集合，解决了实际应用中树结构扰动的问题。 |
| [^156] | [SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation.](http://arxiv.org/abs/2310.12508) | 这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。 |
| [^157] | [A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model.](http://arxiv.org/abs/2310.11143) | 本研究提出了一种基于机器学习的概率暴露模型，可以更准确地估计德国室内氡气分布，并具有更高的空间分辨率。 |
| [^158] | [Fast Graph Condensation with Structure-based Neural Tangent Kernel.](http://arxiv.org/abs/2310.11046) | 本文提出了一种以数据为中心的解决方案，将大型图数据集压缩为较小的集合而不会损失GNN的预测性能。通过将图结构压缩问题转化为核岭回归任务，利用基于结构的神经切线内核来捕捉图的拓扑结构。 |
| [^159] | [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation.](http://arxiv.org/abs/2310.02304) | 本文提出了一种自学优化器（STOP），通过递归自我改进的代码生成，使用融合了语言模型的脚手架程序来改进自身，从而生成性能更好的程序。 |
| [^160] | [Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data.](http://arxiv.org/abs/2310.00290) | 这篇论文研究了储备计算在自回归时间序列数据中的数学结构，并揭示了其隐藏的权重矩阵结构，以实现对AR类型时间序列数据的完美预测。 |
| [^161] | [Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities.](http://arxiv.org/abs/2309.16739) | 本文探讨了将大型语言模型(LLMs)部署在6G边缘的潜力和挑战。我们介绍了由LLMs支持的关键应用，并从响应时间、带宽成本和数据隐私等方面分析了云端部署面临的问题。我们提出了6G移动边缘计算(MEC)系统可能解决这些问题的方案，并讨论了边缘训练和边缘推理的创新技术。 |
| [^162] | [Implicit regularization of deep residual networks towards neural ODEs.](http://arxiv.org/abs/2309.01213) | 本文建立了深度残差网络向神经常微分方程的隐式正则化，通过对用梯度流训练的非线性网络的研究，证明了在网络以神经常微分方程的离散化形式初始化后，这种离散化将在整个训练过程中保持不变，并提供了收敛性的条件。 |
| [^163] | [EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE.](http://arxiv.org/abs/2308.11971) | 本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。 |
| [^164] | [Backdoor Federated Learning by Poisoning Backdoor-Critical Layers.](http://arxiv.org/abs/2308.04466) | 该论文研究了后门联邦学习中后门关键层的存在，并提出了一种针对这些层的新型后门攻击方法，旨在在各种防御策略下实现攻击效果和隐蔽性之间的平衡。 |
| [^165] | [Spectral Ranking Inferences based on General Multiway Comparisons.](http://arxiv.org/abs/2308.02918) | 本文研究了使用光谱方法在广义多维比较中估计和量化未观察到的比较实体的偏好分数的性能，并揭示了光谱估计量与最大似然估计量之间的关系。 |
| [^166] | [SABRE: Robust Bayesian Peer-to-Peer Federated Learning.](http://arxiv.org/abs/2308.02747) | SABRE是一种强鲁棒性变分贝叶斯点对点联邦学习框架，通过新的聚合方法克服了现有框架的局限性，在非IID环境中表现良好，对数据/模型攻击具备鲁棒性，在图像分类数据上优于现有框架。 |
| [^167] | [$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces.](http://arxiv.org/abs/2306.17366) | 这项研究提出了一种$\lambda$-AC算法，通过学习连续状态空间中的潜在决策感知模型，实现了决策驱动的强化学习。通过理论和实证研究，确定了决策感知强化学习模型的必要组成部分，并展示了设计选择对算法性能的重要影响。 |
| [^168] | [Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects.](http://arxiv.org/abs/2306.10125) | 自监督学习（SSL）在时间序列分析中的应用取得了显著性能，通过减少对标注数据的依赖，即使只有少量标注数据，也能实现高性能。 |
| [^169] | [Training generative models from privatized data.](http://arxiv.org/abs/2306.09547) | 介绍了一种在隐私化数据上训练GAN的框架，使用熵正则化Wasserstein距离去噪可以缓解正则化偏差和隐私化噪声的影响，提高模型有效性。 |
| [^170] | [Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training.](http://arxiv.org/abs/2306.08173) | 本文提出了一种差分隐私的CLIP模型（Dp-CLIP），旨在保护多模态AI任务中的数据隐私，同时保持模型准确性。该方法在基准数据集上得到了验证，并表明其与标准非私有CLIP模型相比具有同等的性能。 |
| [^171] | [Survey on Sociodemographic Bias in Natural Language Processing.](http://arxiv.org/abs/2306.08158) | 本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。 |
| [^172] | [Global universal approximation of functional input maps on weighted spaces.](http://arxiv.org/abs/2306.03303) | 本文提出了功能性输入神经网络，可以在带权重空间上完成全局函数逼近。这一方法适用于连续函数的推广，还可用于路径空间函数的逼近，同时也可以逼近线性函数签名。 |
| [^173] | [Federated Domain Generalization: A Survey.](http://arxiv.org/abs/2306.01334) | 该论文调查了联邦领域泛化的研究领域，提到了联邦学习和领域泛化技术的优势，以及在泛化联邦模型时面临的挑战。 |
| [^174] | [The Risks of Recourse in Binary Classification.](http://arxiv.org/abs/2306.00497) | 研究发现，在二分分类中提供追索权会增加错误率，导致更多错误的发生。提供算法追索权可能也会在系统级别上给予不利。 |
| [^175] | [Bures-Wasserstein Means of Graphs.](http://arxiv.org/abs/2305.19738) | 该论文提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。实验结果表明，在各种任务中都有很好的表现。 |
| [^176] | [Escaping mediocrity: how two-layer networks learn hard single-index models with SGD.](http://arxiv.org/abs/2305.18502) | 研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。 |
| [^177] | [Hierarchical clustering with dot products recovers hidden tree structure.](http://arxiv.org/abs/2305.15022) | 本文发现一种基于点积的层次聚类算法，可以通过最大平均点积合并聚类，并且输出的树结构可用于准确估计数据的生成层次结构，树形恢复性能优于现有方法。 |
| [^178] | [CoinEM: Tuning-Free Particle-Based Variational Inference for Latent Variable Models.](http://arxiv.org/abs/2305.14916) | 本文提出了两种无需调参的基于粒子的变分推断算法，其中一种是通过考虑边缘最大似然估计为自由能泛函最小化得到的，另一种是用于优化该问题的算法，完全无需调参。 |
| [^179] | [Text Conditional Alt-Text Generation for Twitter Images.](http://arxiv.org/abs/2305.14779) | 本文针对Twitter上分享的图像提出了一种文本条件下的替代文本生成方法。通过CLIP前缀模型，该模型结合图像和推文中的文本信息，生成关于图像的上下文相关的替代文本。 |
| [^180] | [The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning.](http://arxiv.org/abs/2304.09914) | 本文使用机器学习的算法分析了来自15个不同国家的220个政治领袖的YouTube视频，总结了政治领袖面部情感表达的差异。 |
| [^181] | [ASPEST: Bridging the Gap Between Active Learning and Selective Prediction.](http://arxiv.org/abs/2304.03870) | 本文提出了一种新的学习范式——主动选择性预测（ASPEST），它可以在转移目标领域中学习查询更多有信息的样本，从而实现减少人工标注工作的同时增加准确性和覆盖率。 |
| [^182] | [A Transformer-Based Deep Learning Approach for Fairly Predicting Post-Liver Transplant Risk Factors.](http://arxiv.org/abs/2304.02780) | 本研究提出了一个基于Transformer的深度学习框架模型，可同时预测五种移植后风险并实现更好的性能。 |
| [^183] | [$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States.](http://arxiv.org/abs/2303.18242) | 引入了一种名为 $\infty$-Diff 的生成式扩散模型，可以处理无限分辨率的数据，不需要使用超网络进行潜在向量压缩或依赖于离散的组件，能够显著提高样本质量，并能够在保留细节的同时有效地扩展到比训练数据更高的分辨率。 |
| [^184] | [Policy Gradient Methods for Discrete Time Linear Quadratic Regulator With Random Parameters.](http://arxiv.org/abs/2303.16548) | 本文使用增强学习技术中的策略梯度方法解决了带随机参数的离散时间线性二次调节器的最优控制问题，并建立了全局线性收敛保证。 |
| [^185] | [Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation.](http://arxiv.org/abs/2303.16521) | 该论文提出了一种不需要数据增强的在线深度聚类方法，通过加强正则化来避免崩溃，相比于其他方法，具有更高的稳定性。 |
| [^186] | [DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks.](http://arxiv.org/abs/2303.04878) | DeepGD是一种用于深度神经网络的多目标黑盒测试选择方法，通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。 |
| [^187] | [Global Convergence Rate of Deep Equilibrium Models with General Activations.](http://arxiv.org/abs/2302.05797) | 该论文研究了具有一般激活函数的深度平衡模型（DEQ）的全局收敛速度，证明了梯度下降以线性收敛速度收敛到全局最优解，并解决了限制平衡点Gram矩阵最小特征值的挑战。 |
| [^188] | [SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits.](http://arxiv.org/abs/2301.12357) | 本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。 |
| [^189] | [Graph Topology Learning Under Privacy Constraints.](http://arxiv.org/abs/2301.06662) | 在隐私约束下，我们提出了一个框架，联合学习为本地客户定制的个性化图以及共识图，以推断潜在图拓扑，同时在保护隐私的情况下处理分布式客户端的数据。 |
| [^190] | [CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning.](http://arxiv.org/abs/2211.08229) | 本文分析了现有数据污染后门攻击对对比学习的局限性，并提出了一种名为CorruptEncoder的新型攻击方法，通过理论导向的方式创建优化的污染输入，大幅提高攻击效果。实验证明，CorruptEncoder是首个仅需要少量图像和污染比例即可达到90%以上攻击成功率的攻击方法。同时，本文提出了一种名为局部裁剪的防御策略来应对数据污染后门攻击。 |
| [^191] | [ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks.](http://arxiv.org/abs/2210.00108) | ImpNet是一种在编译神经网络中添加的不可察觉和黑盒不可检测的后门攻击。这些后门可以绕过数据准备和模型训练阶段的保护措施，并且只能在插入阶段可靠地检测到，移除它们很具有挑战性。 |
| [^192] | [Making SGD Parameter-Free.](http://arxiv.org/abs/2205.02160) | 该论文提出了一种无参数化的随机梯度下降法，能够在一定程度上适应未知梯度范数、平滑性和强凸性，并在收敛速度方面具有高概率保证。 |
| [^193] | [Interpretable Learning in Multivariate Big Data Analysis for Network Monitoring.](http://arxiv.org/abs/1907.02677) | 本文扩展了多变量大数据分析（MBDA）方法，提出了一种自动推导特征的解决方案，结合可解释性和交互式模型的优势以及并行处理的能力，应用于网络监测和诊断，最终在UGR'16和Dartmouth'18两个数据集上取得成功。 |

# 详细

[^1]: 信任学习理论

    Credal Learning Theory

    [https://rss.arxiv.org/abs/2402.00957](https://rss.arxiv.org/abs/2402.00957)

    本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。

    

    统计学习理论是机器学习的基础，为从未知概率分布中学习到的模型的风险提供理论边界。然而，在实际部署中，数据分布可能会变化，导致领域适应/泛化问题。在本文中，我们建立了一个“信任”学习理论的基础，使用概率的凸集（信任集）来建模数据生成分布的变异性。我们认为，这样的信任集可以从有限样本的训练集中推断出来。对于有限假设空间（无论是否可实现）和无限模型空间，推导出界限，这直接推广了经典结果。

    Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
    
[^2]: 通过语义感知置换训练来缓解逆转诅咒

    Mitigating Reversal Curse via Semantic-aware Permutation Training

    [https://arxiv.org/abs/2403.00758](https://arxiv.org/abs/2403.00758)

    逆转诅咒问题是导致因果语言模型无法进行双向推理的根本原因之一，在这篇论文中，我们提出了通过语义感知的置换训练来缓解这一问题。

    

    大型语言模型（LLM）在各种任务中取得了令人印象深刻的表现，然而最近的研究表明，因果关系的LLM遭遇了“逆转诅咒”。一个典型的例子是，模型知道“A的父亲是B”，但无法推理出“B的孩子是A”。这一局限性对人工通用智能（AGI）的进展构成了挑战，因为它暗示了模型在理解和应用双向推理方面存在差距。本文首先进行了大量评估，并确定了逆转诅咒的根本原因在于训练和推断阶段之间的词序不同，即因果语言模型在训练数据中预测先行词的能力不足。因此，考虑到在训练数据上进行排列可以被视为潜在解决方案，因为这可以使模型预测先行词或标记。然而，先前的排列方法可能受到截断影响。

    arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
    
[^3]: 一种针对（粗糙）扩散模型中期权定价的时间步进深度梯度流方法

    A time-stepping deep gradient flow method for option pricing in (rough) diffusion models

    [https://arxiv.org/abs/2403.00746](https://arxiv.org/abs/2403.00746)

    提出了一种时间步进深度梯度流方法，用于处理（粗糙）扩散模型中的期权定价问题，保证了对大金额水平下期权价格的渐近行为和先验上下界。

    

    我们开发了一种新颖的深度学习方法，用于在扩散模型中定价欧式期权，可以高效处理由于粗糙波动率模型的马尔可夫逼近而导致的高维问题。期权定价的偏微分方程被重新表述为能量最小化问题，该问题通过深度人工神经网络以时间步进的方式进行近似。所提出的方案符合期权价格在大金额水平上的渐近行为，并遵守期权价格的先验已知上下界。通过一系列数值示例评估了所提方法的准确性和效率，特别关注了提升Heston模型。

    arXiv:2403.00746v1 Announce Type: cross  Abstract: We develop a novel deep learning approach for pricing European options in diffusion models, that can efficiently handle high-dimensional problems resulting from Markovian approximations of rough volatility models. The option pricing partial differential equation is reformulated as an energy minimization problem, which is approximated in a time-stepping fashion by deep artificial neural networks. The proposed scheme respects the asymptotic behavior of option prices for large levels of moneyness, and adheres to a priori known bounds for option prices. The accuracy and efficiency of the proposed method is assessed in a series of numerical examples, with particular focus in the lifted Heston model.
    
[^4]: AtP*：一种将LLM行为定位到组件的高效可扩展方法

    AtP*: An efficient and scalable method for localizing LLM behaviour to components

    [https://arxiv.org/abs/2403.00745](https://arxiv.org/abs/2403.00745)

    AtP*是一种将LLM行为准确定位到组件的高效可扩展方法，通过解决Attribution Patching存在的显著假阴性问题，提供了显著改进以及进一步的性能提升。

    

    Activation Patching是一种直接计算行为因果归因于模型组件的方法。然而，要全面应用该方法，需要进行一次成本随模型组件数量线性增加的扫描，这可能对SoTA大型语言模型（LLMs）来说成本过高。我们研究了Attribution Patching（AtP），这是对Activation Patching的一种快速基于梯度的近似方法，并发现了两类导致AtP出现显著假阴性的故障模式。我们提出了AtP*的变体，通过两种改变来解决这些故障模式，同时保持可扩展性。我们首次系统研究了AtP及其他快速激活修补方法的对比，并表明AtP明显优于所有其他研究方法，而AtP*进一步提供了显著改进。最后，我们提供了一种方法来限制AtP*估计的假阴性剩余概率。

    arXiv:2403.00745v1 Announce Type: cross  Abstract: Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.
    
[^5]: 子齐次深度平衡模型

    Subhomogeneous Deep Equilibrium Models

    [https://arxiv.org/abs/2403.00720](https://arxiv.org/abs/2403.00720)

    本文提出了一种新的分析方法，基于子齐次算子和非线性Perron-Frobenius理论，为隐式深度神经网络的固定点存在性和唯一性提供了更灵活的框架。

    

    隐式深度神经网络近年来在各种应用中作为传统网络的强大替代方案而发展壮大。然而，这些模型通常缺乏存在性和唯一性保证，引发稳定性、性能和可复现性问题。本文基于子齐次算子和非线性Perron-Frobenius理论，提出了一种用于隐式深度神经网络的固定点存在性和唯一性的新分析。与先前类似分析相比，我们的理论允许对参数矩阵提出更弱的假设，从而为定义良好的隐式网络提供更灵活的框架。我们在前馈、卷积和图神经网络示例上展示了由此产生的子齐次网络的性能。

    arXiv:2403.00720v1 Announce Type: new  Abstract: Implicit-depth neural networks have grown as powerful alternatives to traditional networks in various applications in recent years. However, these models often lack guarantees of existence and uniqueness, raising stability, performance, and reproducibility issues. In this paper, we present a new analysis of the existence and uniqueness of fixed points for implicit-depth neural networks based on the concept of subhomogeneous operators and the nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our theory allows for weaker assumptions on the parameter matrices, thus yielding a more flexible framework for well-defined implicit networks. We illustrate the performance of the resulting subhomogeneous networks on feed-forward, convolutional, and graph neural network examples.
    
[^6]: 自适应学习率的FTRL算法的竞争比分析和最佳方案研究

    Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds

    [https://arxiv.org/abs/2403.00715](https://arxiv.org/abs/2403.00715)

    通过引入竞争分析框架，我们提出了调整FTRL学习率的更新规则，使其在常数因子内达到最佳竞争比，并且展示了当惩罚项具有近似单调性时的竞争比特性。

    

    Follow-The-Regularized-Leader (FTRL)被认为是在线学习中一种有效且多功能的方法，其中学习率的恰当选择对于减小后悔是至关重要的。为此，我们将调整FTRL学习率的问题构建为一个顺序决策问题，并引入竞争分析框架。我们建立了竞争比的下界，并提出了学习率的更新规则，使其在一个常数因子内达到下界的上界。具体地，我们说明了最优竞争比是由惩罚项的组成部分的（近似）单调性所决定的，表明如果惩罚项的组成部分形成单调非增序列，则可以实现常数竞争比，并推导出了在惩罚项$\xi$近似单调非增时的紧密竞争比。我们提出的更新规则被称为...

    arXiv:2403.00715v1 Announce Type: new  Abstract: Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL's learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotonically non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our proposed update rule, referred to a
    
[^7]: 定义专业知识：在治疗效果估计中的应用

    Defining Expertise: Applications to Treatment Effect Estimation

    [https://arxiv.org/abs/2403.00694](https://arxiv.org/abs/2403.00694)

    专家决策者的行动自然地编码了其领域知识的一部分，可以帮助在同一领域内进行推断，从而在治疗效果估计中利用专业知识作为归纳偏差可能是有益的。

    

    决策者通常是其领域的专家，并基于其领域知识采取行动。本文讨论了在治疗效果估计领域中专业知识的重要性，以及利用专业知识作为归纳偏差的潜力。

    arXiv:2403.00694v1 Announce Type: cross  Abstract: Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help make inferences within the same domain: Knowing doctors try to prescribe the best treatment for their patients, we can tell treatments prescribed more frequently are likely to be more effective. Yet in machine learning, the fact that most decision-makers are experts is often overlooked, and "expertise" is seldom leveraged as an inductive bias. This is especially true for the literature on treatment effect estimation, where often the only assumption made about actions is that of overlap. In this paper, we argue that expertise - particularly the type of expertise the decision-makers of a domain are likely to have - can be informative in designin
    
[^8]: 可扩展的项目反应理论模型学习

    Scalable Learning of Item Response Theory Models

    [https://arxiv.org/abs/2403.00680](https://arxiv.org/abs/2403.00680)

    该论文提出了一种从大数据中学习项目反应理论模型中的潜在变量的方法，利用这些模型与逻辑回归之间的相似性来提高计算的效率和可伸缩性。

    

    项目反应理论（IRT）模型旨在评估 $n$ 名考生的潜在能力以及 $m$ 个测验项目的隐含难度特征，这些项目是从表明其对应答案质量的分类数据中得出的。传统的心理测量评估基于相对较少的考生和项目，例如一个由 $200$ 名学生解决包含 $10$ 道题目的考试的班级。而近年来的全球大规模评估，如PISA，或互联网研究，可能导致参与者数量显著增加。此外，在机器学习领域，算法扮演考生角色，数据分析问题扮演项目角色，$n$ 和 $m$ 都可能变得非常大，挑战计算的效率和可伸缩性。为了从大数据中学习IRT模型中的潜在变量，我们利用这些模型与逻辑回归之间的相似性，后者可以使用s准确地近似。

    arXiv:2403.00680v1 Announce Type: new  Abstract: Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using s
    
[^9]: 通过重要性抽样在自然策略梯度中重用历史轨迹：收敛性和收敛速率

    Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate

    [https://arxiv.org/abs/2403.00675](https://arxiv.org/abs/2403.00675)

    通过重要性抽样在自然策略梯度中重用历史轨迹可提高收敛速率

    

    强化学习提供了一个学习控制的数学框架，其成功在很大程度上取决于它可以利用的数据量。有效利用先前策略得到的历史轨迹对于加快策略优化至关重要。实证证据表明基于重要性抽样的策略梯度方法效果良好。然而，现有文献往往忽视了不同迭代之间轨迹的相互依赖性，且良好的实证表现缺乏严格的理论证明。本文研究了一种通过重要性抽样重新利用历史轨迹的自然策略梯度方法的变体。我们表明了所提梯度估计器的偏差渐近可忽略，得到的算法是收敛的，并且重用过去的轨迹有助于提高收敛速率。我们进一步将所提估计器应用于

    arXiv:2403.00675v1 Announce Type: new  Abstract: Reinforcement learning provides a mathematical framework for learning-based control, whose success largely depends on the amount of data it can utilize. The efficient utilization of historical trajectories obtained from previous policies is essential for expediting policy optimization. Empirical evidence has shown that policy gradient methods based on importance sampling work well. However, existing literature often neglect the interdependence between trajectories from different iterations, and the good empirical performance lacks a rigorous theoretical justification. In this paper, we study a variant of the natural policy gradient method with reusing historical trajectories via importance sampling. We show that the bias of the proposed estimator of the gradient is asymptotically negligible, the resultant algorithm is convergent, and reusing past trajectories helps improve the convergence rate. We further apply the proposed estimator to 
    
[^10]: 快照强化学习：利用先前轨迹提高效率

    Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency

    [https://arxiv.org/abs/2403.00673](https://arxiv.org/abs/2403.00673)

    提出了快照强化学习（SnapshotRL）框架，通过简单改变环境来增强样本效率，而无需对算法和模型进行任何修改

    

    深度强化学习（DRL）算法需要大量样本和计算资源才能实现更高的性能，这限制了它们的实际应用并对进一步发展构成挑战。鉴于资源有限的约束，利用现有的计算工作（例如学习策略、样本）来增强样本效率和减少DRL算法的计算资源消耗至关重要。以前利用现有计算工作的研究需要对现有算法和模型进行干扰性修改，专门为特定算法设计，缺乏灵活性和通用性。本文提出了快照强化学习（SnapshotRL）框架，通过简单改变环境来增强样本效率，而无需对算法和模型进行任何修改。

    arXiv:2403.00673v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) algorithms require substantial samples and computational resources to achieve higher performance, which restricts their practical application and poses challenges for further development. Given the constraint of limited resources, it is essential to leverage existing computational work (e.g., learned policies, samples) to enhance sample efficiency and reduce the computational resource consumption of DRL algorithms. Previous works to leverage existing computational work require intrusive modifications to existing algorithms and models, designed specifically for specific algorithms, lacking flexibility and universality. In this paper, we present the Snapshot Reinforcement Learning (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models. By allowing student agents to choose states in teacher trajectories as the initi
    
[^11]: 通过深度学习推动增材制造：当前进展和未来挑战的综合评述

    Advancing Additive Manufacturing through Deep Learning: A Comprehensive Review of Current Progress and Future Challenges

    [https://arxiv.org/abs/2403.00669](https://arxiv.org/abs/2403.00669)

    深度学习在增材制造领域显示出巨大潜力，能够克服高维数据的复杂挑战，推动该领域不断发展。

    

    增材制造（AM）已经被证明是广泛使用的减少制造的潜在替代品，因为其在最小材料浪费的情况下制造高度定制产品的能力。然而，由于包括复杂和动态过程相互作用在内的一些主要固有挑战，即使使用传统的机器学习，有时也难以完全理解，因为涉及到高维数据，如图像、点云和体素。然而，最近出现的深度学习（DL）在克服许多这些挑战方面显示出了巨大的潜力，因为DL能够自动从高维数据中捕捉复杂关系，而无需手工制作特征提取。因此，AM和DL交叉领域的研究量每年呈指数增长，这可能会将增材制造推向更广阔的应用领域。

    arXiv:2403.00669v1 Announce Type: new  Abstract: Additive manufacturing (AM) has already proved itself to be the potential alternative to widely-used subtractive manufacturing due to its extraordinary capacity of manufacturing highly customized products with minimum material wastage. Nevertheless, it is still not being considered as the primary choice for the industry due to some of its major inherent challenges, including complex and dynamic process interactions, which are sometimes difficult to fully understand even with traditional machine learning because of the involvement of high-dimensional data such as images, point clouds, and voxels. However, the recent emergence of deep learning (DL) is showing great promise in overcoming many of these challenges as DL can automatically capture complex relationships from high-dimensional data without hand-crafted feature extraction. Therefore, the volume of research in the intersection of AM and DL is exponentially growing each year which ma
    
[^12]: 具有二次非线性控制系统的稳定性认证学习

    Stability-Certified Learning of Control Systems with Quadratic Nonlinearities

    [https://arxiv.org/abs/2403.00646](https://arxiv.org/abs/2403.00646)

    本文主要关注开发一种方法，以便推断具有固有稳定性保证的二次控制动态系统。

    

    本文主要关注一种运算推断方法，旨在基于关于结构的先验假设构建低维动力模型，这些假设通常受到已建立的物理学或专家见解的启发。稳定性是动力系统的基本属性，然而，并非总是可以在通过推断得出的模型中保证稳定性。我们的主要目标是开发一种方法，以便推断具有固有稳定性保证的二次控制动态系统。为此，我们研究了具有能量保持非线性的控制系统的稳定性特征，从而确定这类系统在哪些条件下是有界输入有界状态稳定的。这些见解随后应用于学习过程，产生了从设计上固有稳定的推断模型。我们提出的框架的有效性通过一些数值例子加以证明。

    arXiv:2403.00646v1 Announce Type: new  Abstract: This work primarily focuses on an operator inference methodology aimed at constructing low-dimensional dynamical models based on a priori hypotheses about their structure, often informed by established physics or expert insights. Stability is a fundamental attribute of dynamical systems, yet it is not always assured in models derived through inference. Our main objective is to develop a method that facilitates the inference of quadratic control dynamical systems with inherent stability guarantees. To this aim, we investigate the stability characteristics of control systems with energy-preserving nonlinearities, thereby identifying conditions under which such systems are bounded-input bounded-state stable. These insights are subsequently applied to the learning process, yielding inferred models that are inherently stable by design. The efficacy of our proposed framework is demonstrated through a couple of numerical examples.
    
[^13]: 重新审视自监督学习中的均匀性度量

    Rethinking The Uniformity Metric in Self-Supervised Learning

    [https://arxiv.org/abs/2403.00642](https://arxiv.org/abs/2403.00642)

    通过识别并满足现有均匀性度量未能达标的五个基本性质，本文引入了一个对维度崩溃敏感的新均匀性度量。

    

    均匀性在评估学习表示方面起着至关重要的作用，有助于更深入理解自监督学习。之前的一项开创性工作引入了一个均匀性度量，定量衡量学习表示的崩溃程度。直接优化这一度量与对齐一起，被证明能够有效地防止不断崩溃。然而，我们提出理论和实证证据表明这一度量缺乏对维度崩溃的敏感性，凸显了其局限性。为了解决这一局限性并设计一个更有效的均匀性度量，本文确定了五个基本性质，其中现有的均匀性度量未能满足其中的一些。我们随后引入了一个新的均匀性度量，满足所有这些期望，并且对维度崩溃具有敏感性。

    arXiv:2403.00642v1 Announce Type: cross  Abstract: Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various 
    
[^14]: 在微调预训练模型以提升公平性和效率的偏见缓解方法

    Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency

    [https://arxiv.org/abs/2403.00625](https://arxiv.org/abs/2403.00625)

    该论文介绍了一种专门设计用于减轻新任务中偏见的高效且稳健的微调框架，通过中和对不同人口群体预测有影响力的权重，从而提升公平性和效率。

    

    细调预训练模型是许多现实世界应用中广泛采用的技术。然而，在新任务上微调这些模型可能导致不公平的结果。这是因为无论原始预训练模型是否考虑了公平性，都没有公平性属性的泛化保证。为了解决这个问题，我们引入了一种专门设计用于减轻新任务中偏见的高效且稳健的微调框架。我们的实证分析表明，影响不同人口群体预测的预训练模型中的参数是不同的，基于这一观察，我们采用一种转移学习策略，该策略通过使用跨人口群体之间的Fisher信息确定的这些有影响力的权重来中和这些有影响力的权重的重要性。此外，我们将这种权重重要性中和策略与矩阵因子分解技术结合起来。

    arXiv:2403.00625v1 Announce Type: new  Abstract: Fine-tuning pre-trained models is a widely employed technique in numerous real-world applications. However, fine-tuning these models on new tasks can lead to unfair outcomes. This is due to the absence of generalization guarantees for fairness properties, regardless of whether the original pre-trained model was developed with fairness considerations. To tackle this issue, we introduce an efficient and robust fine-tuning framework specifically designed to mitigate biases in new tasks. Our empirical analysis shows that the parameters in the pre-trained model that affect predictions for different demographic groups are different, so based on this observation, we employ a transfer learning strategy that neutralizes the importance of these influential weights, determined using Fisher information across demographic groups. Additionally, we integrate this weight importance neutralization strategy with a matrix factorization technique, which pro
    
[^15]: 推荐系统中的通用用户表示的迁移学习

    Generalized User Representations for Transfer Learning

    [https://arxiv.org/abs/2403.00584](https://arxiv.org/abs/2403.00584)

    提出了一个在大规模推荐系统中有效表示用户口味的通用用户表示框架，结合表示学习和迁移学习，同时提出了管理生产模型中该框架部署的新颖解决方案。

    

    我们提出了一个新颖的框架，用于大规模推荐系统中的用户表示，旨在以通用方式有效表示多样化的用户口味。我们的方法采用两阶段方法，结合表示学习和迁移学习。表示学习模型使用自动编码器将各种用户特征压缩成表示空间。在第二阶段，下游任务特定模型通过迁移学习利用用户表示，而不是单独策划用户特征。我们进一步在表示的输入特征上增强这种方法，以增加灵活性，并实现对用户事件（包括新用户体验）的几乎实时反应。此外，我们提出了一个新颖的解决方案，用于管理该框架在生产模型中的部署，允许下游模型独立工作。我们通过严格的线下验证了我们框架的性能。

    arXiv:2403.00584v1 Announce Type: cross  Abstract: We present a novel framework for user representation in large-scale recommender systems, aiming at effectively representing diverse user taste in a generalized manner. Our approach employs a two-stage methodology combining representation learning and transfer learning. The representation learning model uses an autoencoder that compresses various user features into a representation space. In the second stage, downstream task-specific models leverage user representations via transfer learning instead of curating user features individually. We further augment this methodology on the representation's input features to increase flexibility and enable reaction to user events, including new user experiences, in Near-Real Time. Additionally, we propose a novel solution to manage deployment of this framework in production models, allowing downstream models to work independently. We validate the performance of our framework through rigorous offl
    
[^16]: SINDy与硬非线性和隐藏动力学：基准研究

    SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study

    [https://arxiv.org/abs/2403.00578](https://arxiv.org/abs/2403.00578)

    该研究分析了SINDy技术在非线性识别中的有效性，发现其在处理真实动态系统时面临处理未观察状态和非平滑动力学的困难，为了在这些挑战性背景下也能利用SINDy，提出了解决方案。

    

    在这项工作中，我们分析了稀疏识别非线性动力学（SINDy）技术在三个非线性识别基准数据集上的有效性，以更好地了解其在处理真实动态系统时的适用性。尽管SINDy可以是一种追求基于物理的学习的吸引策略，但我们的分析突显了在处理未观察到的状态和非平滑动力学方面存在困难。由于这些特征在一般实际系统和特别是控制应用中无处不在，我们补充我们的分析，通过实践方法来解决这些问题，以便在这些具有挑战性的背景下也利用SINDy。

    arXiv:2403.00578v1 Announce Type: cross  Abstract: In this work we analyze the effectiveness of the Sparse Identification of Nonlinear Dynamics (SINDy) technique on three benchmark datasets for nonlinear identification, to provide a better understanding of its suitability when tackling real dynamical systems. While SINDy can be an appealing strategy for pursuing physics-based learning, our analysis highlights difficulties in dealing with unobserved states and non-smooth dynamics. Due to the ubiquity of these features in real systems in general, and control applications in particular, we complement our analysis with hands-on approaches to tackle these issues in order to exploit SINDy also in these challenging contexts.
    
[^17]: 深度学习的单模型观点的发展：优化与随机优化算法的泛化能力

    Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms

    [https://arxiv.org/abs/2403.00574](https://arxiv.org/abs/2403.00574)

    本文研究通过采用新颖方法，从一系列轨迹中估计随机优化器的稳态分布，填补了深度学习优化中关于优化和泛化能力之间关系的理解空白。

    

    尽管关于深度学习优化的文献内容很丰富，但我们对于什么使优化算法有效的理解仍然零散。特别是，我们不太清楚增强的优化是否会转化为更好的泛化能力。目前的研究忽视了随机梯度下降（SGD）及其变体固有的随机性，导致缺乏全面的基准测试和对它们统计性能的洞察。本文旨在通过采用一种新颖的方法来填补这一差距。我们不仅仅评估单个优化轨迹的终点，而是从一系列轨迹中汲取，以估计随机优化器的稳态分布。我们的研究涵盖了广泛的技术，包括SGD及其变体、平坦最小值优化器以及我们在Basin Hopping框架下提出的新算法。通过我们的评估，我们发现...

    arXiv:2403.00574v1 Announce Type: new  Abstract: Despite an extensive body of literature on deep learning optimization, our current understanding of what makes an optimization algorithm effective is fragmented. In particular, we do not understand well whether enhanced optimization translates to improved generalizability. Current research overlooks the inherent stochastic nature of stochastic gradient descent (SGD) and its variants, resulting in a lack of comprehensive benchmarking and insight into their statistical performance. This paper aims to address this gap by adopting a novel approach. Rather than solely evaluating the endpoint of individual optimization trajectories, we draw from an ensemble of trajectories to estimate the stationary distribution of stochastic optimizers. Our investigation encompasses a wide array of techniques, including SGD and its variants, flat-minima optimizers, and new algorithms we propose under the Basin Hopping framework. Through our evaluation, which 
    
[^18]: 重新思考基于聚类条件的扩散模型

    Rethinking cluster-conditioned diffusion models

    [https://arxiv.org/abs/2403.00570](https://arxiv.org/abs/2403.00570)

    通过结合最新的图片聚类和扩散模型技术，本文提出了一种在考虑最佳聚类粒度的情况下实现最先进FID并具有较强训练样本效率的聚类条件扩散模型，并提出了一种新颖方法来减少视觉组搜索空间。

    

    我们针对使用聚类分配的图片级条件扩散模型进行了全面的实验研究。我们阐明了关于图片聚类的个别组件如何影响三个数据集上的图片合成。通过结合图片聚类和扩散模型的最新进展，我们展示了，在考虑到图片合成（视觉组）的最佳簇粒度的情况下，通过聚类条件可以实现最先进的FID（即在CIFAR10和CIFAR100上分别为1.67和2.17），同时实现了较强的训练样本效率。最后，我们提出了一种新颖的方法，通过仅使用基于特征的聚类来推导减少视觉组搜索空间的上限簇边界。与现有方法不同，我们发现聚类与基于聚类的图片生成之间没有显著联系。代码和聚类分配将会发布。

    arXiv:2403.00570v1 Announce Type: cross  Abstract: We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.
    
[^19]: 高效Zero V2：在有限数据下掌握离散和连续控制

    EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data

    [https://arxiv.org/abs/2403.00564](https://arxiv.org/abs/2403.00564)

    EfficientZero V2在有限数据情况下通过一系列改进，在多个任务中超越了当前最先进水平，并且相比于通用算法DreamerV3有显著提升

    

    强化学习在现实世界任务中的样本效率仍然是一个关键挑战。虽然最近的算法在提高样本效率方面取得了显著进展，但没有一个能在不同领域中一直表现出优越性能。在本文中，我们介绍了EfficientZero V2，这是一个专为高效RL算法设计的通用框架。我们将EfficientZero的性能扩展到多个领域，涵盖连续和离散行动，以及视觉和低维输入。通过一系列我们提出的改进，EfficientZero V2在有限数据设置下在各种任务中大幅超越了当前的最先进水平（SOTA）。EfficientZero V2在多个基准测试中表现出明显的进步，比如Atari 100k，Proprio Control等中，在66个评估任务中有50个取得了优越的结果。

    arXiv:2403.00564v1 Announce Type: cross  Abstract: Sample efficiency remains a crucial challenge in applying Reinforcement Learning (RL) to real-world tasks. While recent algorithms have made significant strides in improving sample efficiency, none have achieved consistently superior performance across diverse domains. In this paper, we introduce EfficientZero V2, a general framework designed for sample-efficient RL algorithms. We have expanded the performance of EfficientZero to multiple domains, encompassing both continuous and discrete actions, as well as visual and low-dimensional inputs. With a series of improvements we propose, EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a significant margin in diverse tasks under the limited data setting. EfficientZero V2 exhibits a notable advancement over the prevailing general algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks across diverse benchmarks, such as Atari 100k, Proprio Control, an
    
[^20]: 间接参数化具体自编码器

    Indirectly Parameterized Concrete Autoencoders

    [https://arxiv.org/abs/2403.00563](https://arxiv.org/abs/2403.00563)

    本文提出了间接参数化CAEs（IP-CAEs）来解决具体自编码器（CAEs）在稳定联合优化方面的问题，IP-CAEs在多个数据集上表现出显著且一致的改进。

    

    特征选择在数据高维或获取完整特征集成本高昂的情况下至关重要。最近基于神经网络的嵌入式特征选择的发展在广泛应用中表现出有希望的结果。具体自编码器（CAEs）被认为是嵌入式特征选择中的最先进技术，但可能难以实现稳定的联合优化，从而影响其训练时间和泛化能力。本文发现这种不稳定性与CAE学习重复选择有关。为了解决这个问题，我们提出了一种简单有效的改进：间接参数化CAEs（IP-CAEs）。IP-CAEs学习一个嵌入和从它到Gumbel-Softmax分布参数的映射。尽管实现简单，IP-CAE在多个数据集上的重构和分类任务中均表现出显著且一致的改进，无论是在泛化还是训练时间上。

    arXiv:2403.00563v1 Announce Type: new  Abstract: Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classifi
    
[^21]: 模仿学习数据集：创建数据集、训练智能体和进行基准测试的工具包

    Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking

    [https://arxiv.org/abs/2403.00550](https://arxiv.org/abs/2403.00550)

    本研究提出了模仿学习数据集，通过专家政策的组织、现成数据集和技术的提供以及常见模仿学习技术实现的分享，解决了模仿学习中缺乏可用数据和评估一致性的问题。

    

    模仿学习领域需要专家数据来训练智能体完成任务。然而，这种学习方法常常因缺乏可用数据而备受困扰，导致技术测试都是基于自有数据。创建数据集是一个繁琐的过程，需要研究人员从头开始训练专家智能体、记录它们的交互并用新创建的数据测试每种基准方法。此外，为每种新技术创建新数据集会导致评估过程缺乏一致性，因为每个数据集在状态和动作分布上都可能大不相同。为了应对这些问题，本研究旨在创建模仿学习数据集，提供一个工具包，可以：(i) 提供经过精心策划的专家策略，并支持多线程以加快数据集的创建过程；(ii) 提供现成的数据集和技术以及精确的测量；以及(iii) 分享常用模仿学习技术的实现。

    arXiv:2403.00550v1 Announce Type: cross  Abstract: Imitation learning field requires expert data to train agents in a task. Most often, this learning approach suffers from the absence of available data, which results in techniques being tested on its dataset. Creating datasets is a cumbersome process requiring researchers to train expert agents from scratch, record their interactions and test each benchmark method with newly created data. Moreover, creating new datasets for each new technique results in a lack of consistency in the evaluation process since each dataset can drastically vary in state and action distribution. In response, this work aims to address these issues by creating Imitation Learning Datasets, a toolkit that allows for: (i) curated expert policies with multithreaded support for faster dataset creation; (ii) readily available datasets and techniques with precise measurements; and (iii) sharing implementations of common imitation learning techniques. Demonstration li
    
[^22]: 使用重心校正程序优化机器学习训练

    Machine Learning Training Optimization using the Barycentric Correction Procedure

    [https://arxiv.org/abs/2403.00542](https://arxiv.org/abs/2403.00542)

    该研究提出了一种将机器学习算法与重心校正程序（BCP）结合的方法，旨在解决高维空间中长时间执行的问题，并在合成和真实数据中展示了该方法的显著优势。

    

    机器学习（ML）算法是具有许多人类影响应用的具有预测竞争力的算法。然而，在高维空间中，长时间执行仍然是文献中尚未解决的问题。本研究提出将ML算法与一种称为重心校正程序（BCP）的高效方法相结合，以解决这一问题。本研究使用合成数据和来自私立大学的教育数据集展示了所提出方法的好处。研究发现，当实例数和维度增加时，该组合可以在合成和真实数据中提供与时间相关的显著好处，而不会失去准确性。此外，在高维空间中，经验证BCP和线性支持向量分类（LinearSVC）在估计的特征映射高斯径向基函数（RBF）核后，在计算时间和准确性方面是不可行的。

    arXiv:2403.00542v1 Announce Type: new  Abstract: Machine learning (ML) algorithms are predictively competitive algorithms with many human-impact applications. However, the issue of long execution time remains unsolved in the literature for high-dimensional spaces. This study proposes combining ML algorithms with an efficient methodology known as the barycentric correction procedure (BCP) to address this issue. This study uses synthetic data and an educational dataset from a private university to show the benefits of the proposed method. It was found that this combination provides significant benefits related to time in synthetic and real data without losing accuracy when the number of instances and dimensions increases. Additionally, for high-dimensional spaces, it was proved that BCP and linear support vector classification (LinearSVC), after an estimated feature map for the gaussian radial basis function (RBF) kernel, were unfeasible in terms of computational time and accuracy.
    
[^23]: Epsilon-Greedy Thompson Sampling用于贝叶斯优化

    Epsilon-Greedy Thompson Sampling to Bayesian Optimization

    [https://arxiv.org/abs/2403.00540](https://arxiv.org/abs/2403.00540)

    将$\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。

    

    Thompson采样（TS）被认为是解决贝叶斯优化中开发-探索困境的解决方案。 虽然它通过随机生成和最大化高斯过程（GP）后验的样本路径来优先进行探索，但TS在每次执行探索后通过收集关于真实目标函数的信息来弱化其开发功能。 本研究将在TS中引入$\varepsilon$-greedy策略，这是一种在强化学习中被广泛应用的选择策略，以改进其开发功能。 我们首先描述了TS应用于BO的两个极端，即通用TS和样本平均TS。前者和后者分别提倡探索和开发。 然后我们使用$\varepsilon$-greedy策略在两个极端之间随机切换。 $\varepsilon \in (0,1)$的小值优先考虑开发，反之亦然。 我们实证表明$\varepsilon$-greedy T

    arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy T
    
[^24]: VoxGenesis: 无监督发现潜在说话者流形用于语音合成

    VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis

    [https://arxiv.org/abs/2403.00529](https://arxiv.org/abs/2403.00529)

    VoxGenesis是一个无监督语音合成框架，可以在没有监督的情况下发现潜在说话者流形和有意义的语音编辑方向，为解决情感、语调和说话风格等难以获取准确标签的人声特征提供了新方法。

    

    arXiv:2403.00529v1 公告类型: 跨越 摘要: 在人工智能中，实现对人声的微妙和精准模拟一直是一个长期目标。尽管近年取得了重大进展，但当前主流的语音合成模型仍然依赖于监督式说话者建模和明确的参考语句。然而，人声有许多方面，例如情感、语调和说话风格，很难获得准确的标签。本文提出了 VoxGenesis，一种新颖的无监督语音合成框架，可以在没有监督的情况下发现潜在说话者流形和有意义的语音编辑方向。VoxGenesis 在概念上很简单。它不是将语音特征确定地映射到波形，而是将高斯分布转换为由语义标记条件和对准的语音分布。这迫使模型学习一个与语义内容解耦的说话者分布。

    arXiv:2403.00529v1 Announce Type: cross  Abstract: Achieving nuanced and accurate emulation of human voice has been a longstanding goal in artificial intelligence. Although significant progress has been made in recent years, the mainstream of speech synthesis models still relies on supervised speaker modeling and explicit reference utterances. However, there are many aspects of human voice, such as emotion, intonation, and speaking style, for which it is hard to obtain accurate labels. In this paper, we propose VoxGenesis, a novel unsupervised speech synthesis framework that can discover a latent speaker manifold and meaningful voice editing directions without supervision. VoxGenesis is conceptually simple. Instead of mapping speech features to waveforms deterministically, VoxGenesis transforms a Gaussian distribution into speech distributions conditioned and aligned by semantic tokens. This forces the model to learn a speaker distribution disentangled from the semantic content. During
    
[^25]: Actor-Critic中的过度估计、过拟合和可塑性：强化学习的苦涩教训

    Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning

    [https://arxiv.org/abs/2403.00514](https://arxiv.org/abs/2403.00514)

    实施超过60种不同的离策略代理，发现某些组合表现出稳健和优越的性能，揭示了特定正则化设置与任务的关联性约几维多多。

    

    最近，强化学习（RL）中离策略的进展显著提高了样本效率，主要是由于各种形式的正则化的应用，使其比传统的代理更能进行梯度更新步骤。然而，许多这些技术都在有限的情景下进行了测试，通常只在单个仿真基准任务上测试，与众所周知的算法相比，而不是与一系列正则化方法相比。这限制了我们对推动RL改进的具体机制的理解。为了解决这个问题，我们实现了超过60种不同的离策略代理，每个代理都整合了最近最先进算法中的已建立的正则化技术。我们在来自2个仿真基准的14个不同任务上测试了这些代理。我们的研究结果表明，尽管特定的正则化设置的效果因任务而异，但某些组合始终表现出稳健和优越的性能。

    arXiv:2403.00514v1 Announce Type: new  Abstract: Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior perform
    
[^26]: 学习和利用视觉表示学习中的世界模型

    Learning and Leveraging World Models in Visual Representation Learning

    [https://arxiv.org/abs/2403.00504](https://arxiv.org/abs/2403.00504)

    通过联合嵌入预测架构（JEPA）以及引入图像世界模型（IWM），本研究探讨了如何将JEPA预测任务概括为更广泛的数据损坏形式，并研究了学习性能良好的IWM的关键方面。此外，通过微调可以将IWM学到的预测世界模型用于解决各种任务，最终控制所学习表示的抽象级别。

    

    通过联合嵌入预测架构（JEPA），我们探讨了如何将JEPA预测任务概括为更广泛的数据损坏形式。我们引入了图像世界模型（IWM），这种方法超越了遮罩图像建模，学会在潜在空间中预测全局光度变换的影响。我们研究了学习性能良好的IWM的关键方面：条件、预测困难度和容量。此外，我们表明通过微调可以将IWM学到的预测世界模型用于解决各种任务；经过微调的IWM世界模型的性能与甚至超过以往的自监督方法。最后，我们表明通过IWM学习可以控制所学习表示的抽象级别。

    arXiv:2403.00504v1 Announce Type: cross  Abstract: Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned represe
    
[^27]: 几何图神经网络综述：数据结构、模型和应用

    A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications

    [https://arxiv.org/abs/2403.00485](https://arxiv.org/abs/2403.00485)

    该论文综述了几何图神经网络的数据结构、模型和应用，通过提出具备不变性/等变性属性的几何GNN来更好地表征几何图的几何形状和拓扑，并提供了现有模型的统一视角。

    

    几何图是一种具有几何特征的特殊图形，对于建模许多科学问题至关重要。与一般图不同，几何图通常具有平移、旋转和反射等物理对称性，这使它们难以被当前的图神经网络（GNN）有效处理。为了解决这个问题，研究人员提出了各种具备不变性/等变性属性的几何图神经网络，以更好地表征几何图的几何形状和拓扑。鉴于该领域的当前进展，有必要对与几何GNN相关的数据结构、模型和应用进行全面调查。在本文中，基于必要但简洁的数学基础知识，我们从几何消息传递的角度提供了现有模型的统一视角。此外，我们总结了应用程序以及相关数据集，以促进以后的研究。

    arXiv:2403.00485v1 Announce Type: new  Abstract: Geometric graph is a special kind of graph with geometric features, which is vital to model many scientific problems. Unlike generic graphs, geometric graphs often exhibit physical symmetries of translations, rotations, and reflections, making them ineffectively processed by current Graph Neural Networks (GNNs). To tackle this issue, researchers proposed a variety of Geometric Graph Neural Networks equipped with invariant/equivariant properties to better characterize the geometry and topology of geometric graphs. Given the current progress in this field, it is imperative to conduct a comprehensive survey of data structures, models, and applications related to geometric GNNs. In this paper, based on the necessary but concise mathematical preliminaries, we provide a unified view of existing models from the geometric message passing perspective. Additionally, we summarize the applications as well as the related datasets to facilitate later 
    
[^28]: 使用因果机器学习进行行星任务的自主机器臂操作

    Autonomous Robotic Arm Manipulation for Planetary Missions using Causal Machine Learning

    [https://arxiv.org/abs/2403.00470](https://arxiv.org/abs/2403.00470)

    使用因果机器学习，在模拟行星环境中训练机器臂自主研究并互动物体，以揭示潜在的因果因素。

    

    自主机器臂操作具有潜力使行星探测和原位资源利用任务更加高效和有成效，因为机器臂可以自行处理物体并执行特定目标的动作。我们在模拟行星环境中训练机器臂自主研究其没有先前知识的物体，例如行星岩石。机器臂与物体互动，并根据不同的因果因素对其进行分类，这些因素是诸如质量或摩擦系数之类的参数，确定其互动结果的因果关系。通过强化学习，机器臂学会以揭示潜在因果因素的方式互动。我们展示了即使没有任何先前关于物体的知识或以前收集的训练数据，这种方法也可以运作。我们在行星探索环境中进行训练。

    arXiv:2403.00470v1 Announce Type: cross  Abstract: Autonomous robotic arm manipulators have the potential to make planetary exploration and in-situ resource utilization missions more time efficient and productive, as the manipulator can handle the objects itself and perform goal-specific actions. We train a manipulator to autonomously study objects of which it has no prior knowledge, such as planetary rocks. This is achieved using causal machine learning in a simulated planetary environment. Here, the manipulator interacts with objects, and classifies them based on differing causal factors. These are parameters, such as mass or friction coefficient, that causally determine the outcomes of its interactions. Through reinforcement learning, the manipulator learns to interact in ways that reveal the underlying causal factors. We show that this method works even without any prior knowledge of the objects, or any previously-collected training data. We carry out the training in planetary expl
    
[^29]: 安全的混合行动强化学习决策与控制策略在自由变道中的应用

    Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for Discretionary Lane Change

    [https://arxiv.org/abs/2403.00446](https://arxiv.org/abs/2403.00446)

    本文首次将安全的混合行动强化学习引入到自主变道中，提出了新的算法PASAC-PIDLag，并进行了与不安全版本的比较分析。

    

    自主变道是先进驾驶辅助系统的重要特性，可以提高交通效率，减少事故发生率。然而，在复杂环境中保证自动驾驶车辆的安全驾驶仍具有挑战性。本文首次将安全的混合行动强化学习引入到自主变道中，并提出了基于PID拉格朗日的参数化软演员评论家算法（PASAC-PIDLag）。此外，我们对参数化软演员评论家算法（PASAC）进行了比较分析，后者是PASAC-PIDLag的不安全版本。这两种算法被应用于训练自动驾驶车辆的变道策略，输出离散的变道决策和纵向车辆加速度。

    arXiv:2403.00446v1 Announce Type: cross  Abstract: Autonomous lane-change, a key feature of advanced driver-assistance systems, can enhance traffic efficiency and reduce the incidence of accidents. However, safe driving of autonomous vehicles remains challenging in complex environments. How to perform safe and appropriate lane change is a popular topic of research in the field of autonomous driving. Currently, few papers consider the safety of reinforcement learning in autonomous lane-change scenarios. We introduce safe hybrid-action reinforcement learning into discretionary lane change for the first time and propose Parameterized Soft Actor-Critic with PID Lagrangian (PASAC-PIDLag) algorithm. Furthermore, we conduct a comparative analysis of the Parameterized Soft Actor-Critic (PASAC), which is an unsafe version of PASAC-PIDLag. Both algorithms are employed to train the lane-change strategy of autonomous vehicles to output discrete lane-change decision and longitudinal vehicle acceler
    
[^30]: LoMOE: 通过多扩散实现局部多对象编辑

    LoMOE: Localized Multi-Object Editing via Multi-Diffusion

    [https://arxiv.org/abs/2403.00437](https://arxiv.org/abs/2403.00437)

    通过多扩散过程实现零样本局部多对象编辑，赋予用户在图像中一次性添加、替换或编辑多对象的能力。

    

    扩散模型领域的最新发展展示了生成高质量基于提示条件的图像编辑的卓越能力。然而，先前的方法主要依赖于文本提示进行图像编辑，当对场景中包含单个/多个对象的特定对象或细粒度区域进行精确编辑时往往不太有效。我们引入了一种新颖的框架，通过多扩散过程实现零样本局部多对象编辑，以克服这一挑战。该框架赋予用户在图像中对对象执行各种操作的能力，例如在一个复杂场景中一次性添加、替换或编辑$\textbf{多}$对象。我们的方法利用前景 mask 和对应的简单文本提示对目标区域施加局部影响，实现高保真度图像编辑。通过跨注意力和背景

    arXiv:2403.00437v1 Announce Type: cross  Abstract: Recent developments in the field of diffusion models have demonstrated an exceptional capacity to generate high-quality prompt-conditioned image edits. Nevertheless, previous approaches have primarily relied on textual prompts for image editing, which tend to be less effective when making precise edits to specific objects or fine-grained regions within a scene containing single/multiple objects. We introduce a novel framework for zero-shot localized multi-object editing through a multi-diffusion process to overcome this challenge. This framework empowers users to perform various operations on objects within an image, such as adding, replacing, or editing $\textbf{many}$ objects in a complex scene $\textbf{in one pass}$. Our approach leverages foreground masks and corresponding simple text prompts that exert localized influences on the target regions resulting in high-fidelity image editing. A combination of cross-attention and backgrou
    
[^31]: 通过自适应焦点对比解码减少对象幻觉：HALC

    HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding

    [https://arxiv.org/abs/2403.00425](https://arxiv.org/abs/2403.00425)

    HALC是一种旨在减少大型视觉-语言模型中对象幻觉的新颖解码算法，通过局部的自动聚焦基准机制和全局的波束搜索算法，成功减少OH而保持文本生成质量，同时可以作为即插即用模块集成到任何LVLMs中。

    

    在解释多模态环境方面，大型视觉-语言模型（LVLMs）展现了令人印象深刻的能力，但它们不可避免地会受到对象幻觉（OH）的困扰。我们介绍了HALC，这是一种新颖的解码算法，旨在减少LVLMs中的OH。HALC利用视觉-语言任务中独特的细粒度最佳视觉信息，并同时在局部和全局上操作。具体来说，HALC集成了一个强大的自动聚焦基准机制（局部），在运行时纠正产生幻觉的标记，以及一种专门的波束搜索算法（全局），以显着减少OH，同时保持文本生成质量。此外，HALC可以作为即插即用模块集成到任何LVLMs中，无需额外训练。大量实验研究证明了HALC在减少OH方面的有效性，优于四个基准测试中的现有技术。

    arXiv:2403.00425v1 Announce Type: cross  Abstract: While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.
    
[^32]: 使用模拟参考值验证ML-UQ校准统计量：一项敏感性分析

    Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis

    [https://arxiv.org/abs/2403.00423](https://arxiv.org/abs/2403.00423)

    本研究探讨了ML-UQ校准统计量的使用问题，发现一些统计量对于生成分布的选择过于敏感，可能影响校准诊断。

    

    一些流行的机器学习不确定性量化（ML-UQ）校准统计量没有预定义的参考值，主要用于比较研究。因此，校准几乎从不被验证，诊断留给读者的判断。提出了基于实际不确定性导出的合成校准数据集的模拟参考值，以弥补这一问题。由于用于模拟合成误差的生成概率分布通常没有约束，所以模拟参考值对生成分布选择的敏感性可能会成为问题，对校准诊断产生怀疑。本研究探讨了这一问题的各个方面，并显示一些统计量对于用于验证时生成分布的选择过于敏感，当生成分布未知时。例如，

    arXiv:2403.00423v1 Announce Type: cross  Abstract: Some popular Machine Learning Uncertainty Quantification (ML-UQ) calibration statistics do not have predefined reference values and are mostly used in comparative studies. In consequence, calibration is almost never validated and the diagnostic is left to the appreciation of the reader. Simulated reference values, based on synthetic calibrated datasets derived from actual uncertainties, have been proposed to palliate this problem. As the generative probability distribution for the simulation of synthetic errors is often not constrained, the sensitivity of simulated reference values to the choice of generative distribution might be problematic, shedding a doubt on the calibration diagnostic. This study explores various facets of this problem, and shows that some statistics are excessively sensitive to the choice of generative distribution to be used for validation when the generative distribution is unknown. This is the case, for instan
    
[^33]: 经由对抗攻击和训练的稳健深度强化学习：一项调查

    Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey

    [https://arxiv.org/abs/2403.00420](https://arxiv.org/abs/2403.00420)

    通过对抗性训练来改进DRL对条件变化的鲁棒性，研究者系统分析了当代对抗攻击方法，提供了详细见解。

    

    深度强化学习（DRL）是一种训练自主代理在各种复杂环境中的方法。尽管在众所周知的环境中表现出色，但它仍然容易受到轻微条件变化的影响，引发了人们对其在现实应用中可靠性的担忧。为了提高可用性，DRL必须展示出可信度和鲁棒性。通过对抗性训练提高DRL对条件变化的鲁棒性是一种改进方式，通过训练代理针对环境动态的适当对抗性攻击。我们的工作致力于解决这一关键问题，对当代对抗攻击方法进行了深入分析，系统地对其进行分类，并比较它们的目标和操作机制。这种分类为我们提供了对对抗性攻击如何有效评估DRL代理的恢复力的详细见解，从而为开辟DRL在实际应用中的道路奠定了基础。

    arXiv:2403.00420v1 Announce Type: cross  Abstract: Deep Reinforcement Learning (DRL) is an approach for training autonomous agents across various complex environments. Despite its significant performance in well known environments, it remains susceptible to minor conditions variations, raising concerns about its reliability in real-world applications. To improve usability, DRL must demonstrate trustworthiness and robustness. A way to improve robustness of DRL to unknown changes in the conditions is through Adversarial Training, by training the agent against well suited adversarial attacks on the dynamics of the environment. Addressing this critical issue, our work presents an in-depth analysis of contemporary adversarial attack methodologies, systematically categorizing them and comparing their objectives and operational mechanisms. This classification offers a detailed insight into how adversarial attacks effectively act for evaluating the resilience of DRL agents, thereby paving the 
    
[^34]: 可证明鲁棒的DPO: 用有噪反馈对齐语言模型

    Provably Robust DPO: Aligning Language Models with Noisy Feedback

    [https://arxiv.org/abs/2403.00409](https://arxiv.org/abs/2403.00409)

    通过引入面向随机偏好翻转的策略优化通用框架，本研究旨在理解存在嘈杂反馈时的DPO算法，从而解决语言模型对齐人类兴趣中的挑战。

    

    最近，从基于喜好反馈学习作为一种与人类兴趣对齐的有前景方法已经引起了广泛关注。虽然这些对齐的生成模型在各种任务中展示出令人印象深刻的能力，但它们对高质量人类喜好数据的依赖在实际应用中构成了瓶颈。具体来说，数据集中有噪（不正确和模糊）的偏好对可能会限制语言模型准确捕捉人类意图。虽然从业者最近提出了启发式方法来减轻噪声偏好的影响，但对它们的工作完整理论理解仍然难以捉摸。在这项工作中，我们旨在通过引入一个面向在随机偏好翻转存在的策略优化的通用框架来弥合这一差距。我们特别关注直接偏好优化（DPO）算法，因为它假设偏好遵循 Bradley-Te

    arXiv:2403.00409v1 Announce Type: cross  Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Te
    
[^35]: 在预测准确性优化的背景下的分形插值

    Fractal interpolation in the context of prediction accuracy optimization

    [https://arxiv.org/abs/2403.00403](https://arxiv.org/abs/2403.00403)

    该研究提出了三种基于分形插值的数据增强策略，分别是“最接近Hurst策略”、“最接近值策略”和“公式策略”，并使用四个公共数据集和一组来自罗马尼亚布拉索夫市气象记录的私人数据集来验证这些策略。

    

    本文关注使用分形插值技术优化时间序列预测的假设。一般来说，机器学习模型预测的准确性与使用数据的质量和数量方面密切相关，遵循“垃圾进，垃圾出”的原则。为了定量和定性增加数据集，数据科学家最关注的问题之一是生成合成数据，这些数据应尽可能地紧随原始数据的实际模式。

    arXiv:2403.00403v1 Announce Type: new  Abstract: This paper focuses on the hypothesis of optimizing time series predictions using fractal interpolation techniques. In general, the accuracy of machine learning model predictions is closely related to the quality and quantitative aspects of the data used, following the principle of \textit{garbage-in, garbage-out}. In order to quantitatively and qualitatively augment datasets, one of the most prevalent concerns of data scientists is to generate synthetic data, which should follow as closely as possible the actual pattern of the original data.   This study proposes three different data augmentation strategies based on fractal interpolation, namely the \textit{Closest Hurst Strategy}, \textit{Closest Values Strategy} and \textit{Formula Strategy}. To validate the strategies, we used four public datasets from the literature, as well as a private dataset obtained from meteorological records in the city of Brasov, Romania. The prediction resul
    
[^36]: 基于结构化深度神经网络的拉格朗日系统反步轨迹跟踪控制

    Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems

    [https://arxiv.org/abs/2403.00381](https://arxiv.org/abs/2403.00381)

    提出了一种基于结构化DNN的控制器，通过设计神经网络结构确保闭环稳定性，并进一步优化参数以实现改进的控制性能，同时提供了关于跟踪误差的明确上限。

    

    深度神经网络（DNN）越来越多地被用于学习控制器，因为其出色的逼近能力。然而，它们的黑盒特性对闭环稳定性保证和性能分析构成了重要挑战。在本文中，我们引入了一种基于结构化DNN的控制器，用于采用反推技术实现拉格朗日系统的轨迹跟踪控制。通过适当设计神经网络结构，所提出的控制器可以确保任何兼容的神经网络参数实现闭环稳定性。此外，通过进一步优化神经网络参数，可以实现更好的控制性能。此外，我们提供了关于跟踪误差的明确上限，这允许我们通过适当选择控制参数来实现所需的跟踪性能。此外，当系统模型未知时，我们提出了一种改进的拉格朗日神经网络。

    arXiv:2403.00381v1 Announce Type: cross  Abstract: Deep neural networks (DNN) are increasingly being used to learn controllers due to their excellent approximation capabilities. However, their black-box nature poses significant challenges to closed-loop stability guarantees and performance analysis. In this paper, we introduce a structured DNN-based controller for the trajectory tracking control of Lagrangian systems using backing techniques. By properly designing neural network structures, the proposed controller can ensure closed-loop stability for any compatible neural network parameters. In addition, improved control performance can be achieved by further optimizing neural network parameters. Besides, we provide explicit upper bounds on tracking errors in terms of controller parameters, which allows us to achieve the desired tracking performance by properly selecting the controller parameters. Furthermore, when system models are unknown, we propose an improved Lagrangian neural net
    
[^37]: 视觉-语言模型泛化的不变测试时适应性

    Invariant Test-Time Adaptation for Vision-Language Model Generalization

    [https://arxiv.org/abs/2403.00376](https://arxiv.org/abs/2403.00376)

    本文提出了一个测试时提示调优范式，通过优化可学习的提示，迫使模型利用真正的因果不变特征，以解决视觉-语言模型在特定任务需求上无法有效利用预训练特征的挑战。

    

    arXiv:2403.00376v1 公告类型: 交叉摘要: 视觉-语言基础模型在大量图像-文本配对数据集上的可扩展性使其在众多下游任务中展现出卓越成功。然而，这些模型在应用于长尾任务（如细粒度图像分类）时显示出明显局限，这是由于“决策捷径”导致了它们的泛化能力受限。本文发现CLIP模型具有丰富的特征集，涵盖了既有的\textit{期望不变因果特征}又有的\textit{不希望的决策捷径}。此外，CLIP在下游任务中的表现不佳源自其无法有效利用预训练特征以符合特定任务要求。为解决这一挑战，本文引入一种测试时提示调优范式，优化一个可学习的提示，从而促使模型利用真正的因果不变特征。

    arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
    
[^38]: 重新审视下游任务中的解缠：对抽象视觉推理必要性的研究

    Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning

    [https://arxiv.org/abs/2403.00352](https://arxiv.org/abs/2403.00352)

    解缠表示对抽象视觉推理这一基本下游任务是不必要的，表示信息量的丰富程度更能影响下游性能。

    

    在表示学习中，解缠表示尤为理想，因为它以可分离且紧凑的模式编码数据的生成因子。研究人员倡导利用解缠表示完成下游任务，并提供了令人鼓舞的实证证据。本文进一步探讨了解缠表示在下游应用中的必要性。具体而言，我们展示了在基本的下游任务——抽象视觉推理中，基于维度的解缠表示是不必要的。我们提供了大量的实证证据，反驳了解缠的必要性，涵盖了多个数据集、表示学习方法和下游网络架构。此外，我们的发现表明，表示信息量的丰富程度是下游性能的更好指标，而不是解缠。最后，我们的研究还发现，表示信息量与解缠之间存在积极的相关性。

    arXiv:2403.00352v1 Announce Type: cross  Abstract: In representation learning, a disentangled representation is highly desirable as it encodes generative factors of data in a separable and compact pattern. Researchers have advocated leveraging disentangled representations to complete downstream tasks with encouraging empirical evidence. This paper further investigates the necessity of disentangled representation in downstream applications. Specifically, we show that dimension-wise disentangled representations are unnecessary on a fundamental downstream task, abstract visual reasoning. We provide extensive empirical evidence against the necessity of disentanglement, covering multiple datasets, representation learning methods, and downstream network architectures. Furthermore, our findings suggest that the informativeness of representations is a better indicator of downstream performance than disentanglement. Finally, the positive correlation between informativeness and disentanglement e
    
[^39]: 采用多样化的合作行为和对抗样式采样提高多智能体强化学习中政策的鲁棒性用于辅助任务

    Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks

    [https://arxiv.org/abs/2403.00344](https://arxiv.org/abs/2403.00344)

    提出了一个框架，通过训练适应多样化护理接收者响应的鲁棒护理人员政策，以提高多智能体强化学习中政策的鲁棒性。

    

    个人携带动作障碍者的自主协助是自主机器人系统最有前景的应用之一。最近的研究报道了在医疗领域使用深度强化学习（RL）取得了令人鼓舞的结果。先前的研究表明，辅助任务可以被制定为多智能体强化学习，其中有两个智能体：护理人员和护理接收者。然而，多智能体强化学习训练的政策往往对其他智能体的政策敏感。在这种情况下，训练有素的护理人员政策可能不适用于不同的护理接收者。为了缓解这个问题，我们提出了一个通过训练适应多样化护理接收者响应的鲁棒护理人员政策的框架。在我们的框架中，多样化的护理接收者响应是通过试验和错误自主学习的。此外，为了增强护理者政策的鲁棒性，我们提出了一种对护理接收者响应进行对抗采样的策略。

    arXiv:2403.00344v1 Announce Type: cross  Abstract: Autonomous assistance of people with motor impairments is one of the most promising applications of autonomous robotic systems. Recent studies have reported encouraging results using deep reinforcement learning (RL) in the healthcare domain. Previous studies showed that assistive tasks can be formulated as multi-agent RL, wherein there are two agents: a caregiver and a care-receiver. However, policies trained in multi-agent RL are often sensitive to the policies of other agents. In such a case, a trained caregiver's policy may not work for different care-receivers. To alleviate this issue, we propose a framework that learns a robust caregiver's policy by training it for diverse care-receiver responses. In our framework, diverse care-receiver responses are autonomously learned through trials and errors. In addition, to robustify the care-giver's policy, we propose a strategy for sampling a care-receiver's response in an adversarial mann
    
[^40]: 图神经网络中的非线性层拓扑扩散

    Nonlinear Sheaf Diffusion in Graph Neural Networks

    [https://arxiv.org/abs/2403.00337](https://arxiv.org/abs/2403.00337)

    探索在图神经网络中引入非线性拉普拉斯的潜在益处，通过实验分析验证了模型在图相关任务中的实际有效性。

    

    本文旨在探讨将非线性拉普拉斯引入Sheaf神经网络以提升其在图相关任务中的潜在益处。主要目标是理解这种非线性对扩散动力学、信号传播以及离散时间设置下神经网络架构性能的影响。研究主要强调实验分析，使用真实世界和合成数据集验证模型不同版本的实际有效性。这种方法将焦点从最初的理论探索转向展示所提出模型的实际效用。

    arXiv:2403.00337v1 Announce Type: new  Abstract: This work focuses on exploring the potential benefits of introducing a nonlinear Laplacian in Sheaf Neural Networks for graph-related tasks. The primary aim is to understand the impact of such nonlinearity on diffusion dynamics, signal propagation, and performance of neural network architectures in discrete-time settings. The study primarily emphasizes experimental analysis, using real-world and synthetic datasets to validate the practical effectiveness of different versions of the model. This approach shifts the focus from an initial theoretical exploration to demonstrating the practical utility of the proposed model.
    
[^41]: 在不满足捷径的情况下学习逻辑约束

    Learning with Logical Constraints but without Shortcut Satisfaction

    [https://arxiv.org/abs/2403.00329](https://arxiv.org/abs/2403.00329)

    引入了逻辑连接词的双重变量来解决捷径满足问题，提出了一个新的学习逻辑约束的框架，实验证明其在模型的普适性和约束满足方面性能优越。

    

    最近的神经符号学习研究探讨了通过将逻辑知识编码为额外的损失函数将逻辑约束整合到深度学习中。然而，现有方法往往通过捷径虚假地满足了逻辑约束，未能充分利用知识。本文提出了一个新的学习逻辑约束的框架。具体而言，我们通过引入逻辑连接词的双重变量来解决捷径满足问题，对约束的满足方式进行编码。我们进一步提出了一个变分框架，其中编码的逻辑约束被表达为一个分布损失，与模型的原始训练损失兼容。理论分析表明，所提出的方法具有显著的特性，实验评估显示其在模型的普适性和约束满足方面性能优越。

    arXiv:2403.00329v1 Announce Type: new  Abstract: Recent studies in neuro-symbolic learning have explored the integration of logical knowledge into deep learning via encoding logical constraints as an additional loss function. However, existing approaches tend to vacuously satisfy logical constraints through shortcuts, failing to fully exploit the knowledge. In this paper, we present a new framework for learning with logical constraints. Specifically, we address the shortcut satisfaction issue by introducing dual variables for logical connectives, encoding how the constraint is satisfied. We further propose a variational framework where the encoded logical constraint is expressed as a distributional loss that is compatible with the model's original training loss. The theoretical analysis shows that the proposed approach bears salient properties, and the experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction.
    
[^42]: 神经符号系统的软化符号接地

    Softened Symbol Grounding for Neuro-symbolic Systems

    [https://arxiv.org/abs/2403.00323](https://arxiv.org/abs/2403.00323)

    提出了一种软化符号接地过程，有效地桥接了神经网络训练和符号约束求解，形成了一个高效的神经符号学习框架

    

    神经符号学习通常包括两个独立世界，即神经网络训练和符号约束求解，其成功取决于符号接地，这是人工智能中的一个基本问题。本文提出了一种新颖的软化符号接地过程，弥合了两个世界之间的差距，从而形成了一个有效且高效的神经符号学习框架。技术上，该框架具有以下特点：(1)将符号解状态建模为Boltzmann分布，避免了昂贵的状态搜索，并促进了网络训练和符号推理之间的互惠互利交互；(2)利用投影和SMT解算器的新型MCMC技术，高效地从断开的符号解空间中采样；(3)一种退火机制，可以摆脱陷入次优符号接地的困境。对三个代表性的神经符号学习任务进行的实验表明，该框架具有显著提高

    arXiv:2403.00323v1 Announce Type: new  Abstract: Neuro-symbolic learning generally consists of two separated worlds, i.e., neural network training and symbolic constraint solving, whose success hinges on symbol grounding, a fundamental problem in AI. This paper presents a novel, softened symbol grounding process, bridging the gap between the two worlds, and resulting in an effective and efficient neuro-symbolic learning framework. Technically, the framework features (1) modeling of symbol solution states as a Boltzmann distribution, which avoids expensive state searching and facilitates mutually beneficial interactions between network training and symbolic reasoning;(2) a new MCMC technique leveraging projection and SMT solvers, which efficiently samples from disconnected symbol solution spaces; (3) an annealing mechanism that can escape from %being trapped into sub-optimal symbol groundings. Experiments with three representative neuro symbolic learning tasks demonstrate that, owining 
    
[^43]: DEEP-IoT: 下行增强型高效能物联网

    DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things

    [https://arxiv.org/abs/2403.00321](https://arxiv.org/abs/2403.00321)

    DEEP-IoT通过“更多监听，更少传输”的策略，挑战和转变了传统的物联网通信模型，大幅降低能耗并提高设备寿命。

    

    本文介绍了DEEP-IoT，这是一种具有革命意义的通信范例，旨在重新定义物联网设备之间的通信方式。通过开创性的“更多监听，更少传输”的策略，DEEP-IoT挑战和转变了传统的发送方（物联网设备）为中心的通信模型，将接收方（接入点）作为关键角色，从而降低能耗并延长设备寿命。我们不仅概念化了DEEP-IoT，还通过在窄带系统中集成深度学习增强的反馈信道编码来实现它。模拟结果显示，IoT单元的运行寿命显著提高，比使用Turbo和Polar编码的传统系统提高了最多52.71%。这一进展标志着一种变革。

    arXiv:2403.00321v1 Announce Type: cross  Abstract: At the heart of the Internet of Things (IoT) -- a domain witnessing explosive growth -- the imperative for energy efficiency and the extension of device lifespans has never been more pressing. This paper presents DEEP-IoT, a revolutionary communication paradigm poised to redefine how IoT devices communicate. Through a pioneering "listen more, transmit less" strategy, DEEP-IoT challenges and transforms the traditional transmitter (IoT devices)-centric communication model to one where the receiver (the access point) play a pivotal role, thereby cutting down energy use and boosting device longevity. We not only conceptualize DEEP-IoT but also actualize it by integrating deep learning-enhanced feedback channel codes within a narrow-band system. Simulation results show a significant enhancement in the operational lifespan of IoT cells -- surpassing traditional systems using Turbo and Polar codes by up to 52.71%. This leap signifies a paradi
    
[^44]: 深度强化学习用于解决管理问题: 迈向大型管理模式

    Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode

    [https://arxiv.org/abs/2403.00318](https://arxiv.org/abs/2403.00318)

    该论文介绍了一种基于深度强化学习的方法，旨在解决管理问题，如库存管理、动态定价和推荐，提出了一种基于变压器神经网络结构的大型管理模型，能够超越传统启发式方法解决管理任务，实现跨领域决策协调，证明了在复杂动态商业环境中DRL框架的有效性。

    

    我们引入了一种深度强化学习（DRL）方法，用于解决包括库存管理、动态定价和推荐在内的管理问题。该DRL方法有潜力基于特定的变压器神经网络结构，开发出一个大型管理模型，从而形成适用于各种管理任务的人工通用智能范式。我们试图在一个统一框架中解决问题，考虑到不同任务之间的相互关系。我们的方法的核心是通过生成式决策进行基础决策模型的开发，协调跨不同领域的决策。我们的实验结果证实了我们基于DRL的框架在复杂和动态的商业环境中的有效性。

    arXiv:2403.00318v1 Announce Type: new  Abstract: We introduce a deep reinforcement learning (DRL) approach for solving management problems including inventory management, dynamic pricing, and recommendation. This DRL approach has the potential to lead to a large management model based on certain transformer neural network structures, resulting in an artificial general intelligence paradigm for various management tasks. Traditional methods have limitations for solving complex real-world problems, and we demonstrate how DRL can surpass existing heuristic approaches for solving management tasks. We aim to solve the problems in a unified framework, considering the interconnections between different tasks. Central to our methodology is the development of a foundational decision model coordinating decisions across the different domains through generative decision-making. Our experimental results affirm the effectiveness of our DRL-based framework in complex and dynamic business environments.
    
[^45]: 切掉XAI中的X: 为可理解的人工智能辩护

    Axe the X in XAI: A Plea for Understandable AI

    [https://arxiv.org/abs/2403.00315](https://arxiv.org/abs/2403.00315)

    论文辩护了采用“可理解的人工智能”标签作为替代“XAI”，以避免围绕XAI目标和目的的混乱，并主张采用更适合的实用理解概念。

    

    在最近的一篇论文中，Erasmus等人(2021)辩护了在可解释的人工智能(XAI)中术语“解释”存在的歧义可以通过采用哲学科学中四种不同的现存解释模式之一来解决：演绎-规范、归纳-统计、因果机械和新机制主义模式。在本章中，我展示了作者声称这些模式可以像对待任何自然现象一样应用于深度神经网络的说法是错误的。我还提出了更一般的论点，说明了XAI文献中目前使用的可解释性概念与传统科学解释概念几乎没有相似之处。更有成效的做法是使用“可理解的人工智能”标签，以避免围绕XAI目标和目的的困惑。在章节的后半部分，我主张采用更适合扮演核心角色的实用理解概念。

    arXiv:2403.00315v1 Announce Type: new  Abstract: In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity of the term "explanation" in explainable AI (XAI) can be solved by adopting any of four different extant accounts of explanation in the philosophy of science: the Deductive Nomological, Inductive Statistical, Causal Mechanical, and New Mechanist models. In this chapter, I show that the authors' claim that these accounts can be applied to deep neural networks as they would to any natural phenomenon is mistaken. I also provide a more general argument as to why the notion of explainability as it is currently used in the XAI literature bears little resemblance to the traditional concept of scientific explanation. It would be more fruitful to use the label "understandable AI" to avoid the confusion that surrounds the goal and purposes of XAI. In the second half of the chapter, I argue for a pragmatic conception of understanding that is better suited to play the centra
    
[^46]: 用于MIMO CSI反馈的通用自编码器框架

    Universal Auto-encoder Framework for MIMO CSI Feedback

    [https://arxiv.org/abs/2403.00299](https://arxiv.org/abs/2403.00299)

    该论文提出了一个通用的自编码器框架，可以支持不同输入大小和多种压缩比，相比于朴素和最先进方法，在降低硬件复杂度的同时提供了可比的性能表现。

    

    现有基于自编码器（AE）的信道状态信息（CSI）框架专注于特定配置的用户设备（UE）和基站（BS），因此AE的输入和输出大小是固定的。然而，在实际情况下，输入和输出大小可能会根据BS和UE的天线数量以及在频率维度上分配的资源块数量而变化。支持不同输入和输出大小的一种简单方法是使用多个AE模型，但由于UE的有限硬件资源，这种方法是不切实际的。在本文中，我们提出了一个通用AE框架，可以支持不同的输入大小和多种压缩比。所提出的AE框架在提供与朴素和最先进方法相比可比的压缩比-失真平衡性能的同时大大降低了硬件复杂度。

    arXiv:2403.00299v1 Announce Type: cross  Abstract: Existing auto-encoder (AE)-based channel state information (CSI) frameworks have focused on a specific configuration of user equipment (UE) and base station (BS), and thus the input and output sizes of the AE are fixed. However, in the real-world scenario, the input and output sizes may vary depending on the number of antennas of the BS and UE and the allocated resource block in the frequency dimension. A naive approach to support the different input and output sizes is to use multiple AE models, which is impractical for the UE due to the limited HW resources. In this paper, we propose a universal AE framework that can support different input sizes and multiple compression ratios. The proposed AE framework significantly reduces the HW complexity while providing comparable performance in terms of compression ratio-distortion trade-off compared to the naive and state-of-the-art approaches.
    
[^47]: 为自动说话者验证效率调整预训练语音模型的适配器

    Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification

    [https://arxiv.org/abs/2403.00293](https://arxiv.org/abs/2403.00293)

    提出了一个有效的适配器框架，旨在将自监督语音模型调整为说话者验证任务，通过并行适配器设计，允许在中间Transformer层调整潜在特征以及在所有Transformer层的输出嵌入

    

    带有出色泛化能力的自监督语音模型在预训练和微调范式中在各种下游语音任务上展现出令人印象深刻的性能。然而，随着预训练模型规模的增长，微调变得不可行，因为需要巨大的计算和存储开销，以及过拟合的风险。适配器是轻量级模块，插入到预训练模型中以促进参数高效的调整。本文提出了一个有效的适配器框架，旨在将自监督语音模型调整为说话者验证任务。通过并行适配器设计，我们的框架将两种类型的适配器插入到预训练模型中，允许在中间Transformer层中调整潜在特征以及在所有Transformer层中的输出嵌入。我们进行了全面实验来验证该方法的效率和有效性。

    arXiv:2403.00293v1 Announce Type: cross  Abstract: With excellent generalization ability, self-supervised speech models have shown impressive performance on various downstream speech tasks in the pre-training and fine-tuning paradigm. However, as the growing size of pre-trained models, fine-tuning becomes practically unfeasible due to heavy computation and storage overhead, as well as the risk of overfitting. Adapters are lightweight modules inserted into pre-trained models to facilitate parameter-efficient adaptation. In this paper, we propose an effective adapter framework designed for adapting self-supervised speech models to the speaker verification task. With a parallel adapter design, our proposed framework inserts two types of adapters into the pre-trained model, allowing the adaptation of latent features within intermediate Transformer layers and output embeddings from all Transformer layers. We conduct comprehensive experiments to validate the efficiency and effectiveness of t
    
[^48]: 基于小型语言模型的语义文本传输：成本-相似度权衡

    Semantic Text Transmission via Prediction with Small Language Models: Cost-Similarity Trade-off

    [https://arxiv.org/abs/2403.00290](https://arxiv.org/abs/2403.00290)

    该研究通过使用小型语言模型进行预测，实现了在语义文本传输中的成本和相似度之间的权衡。

    

    我们考虑在无噪音和字符擦除通道上从源到目的地传输自然语言文本。我们利用语言的固有相关性和可预测性，通过允许目的地预测或补全与源文本可能不相似的单词来限制传输成本。我们的目标是获得可实现的$(\bar{c}, \bar{s})$对，其中$\bar{c}$是源头的平均传输成本，$\bar{s}$是通过余弦相似度测量的源头词向量和目的地预测/补全词向量之间的平均语义相似度。我们使用神经语言模型和基于一阶马尔可夫链的小型语言模型(SLM)进行预测，为传输使用了阈值策略，即如果单词与目的地预测/补全的单词的余弦相似度低于阈值，则传输该单词。

    arXiv:2403.00290v1 Announce Type: cross  Abstract: We consider the communication of natural language text from a source to a destination over noiseless and character-erasure channels. We exploit language's inherent correlations and predictability to constrain transmission costs by allowing the destination to predict or complete words with potential dissimilarity with the source text. Concretely, our objective is to obtain achievable $(\bar{c}, \bar{s})$ pairs, where $\bar{c}$ is the average transmission cost at the source and $\bar{s}$ is the average semantic similarity measured via cosine similarity between vector embedding of words at the source and those predicted/completed at the destination. We obtain $(\bar{c}, \bar{s})$ pairs for neural language and first-order Markov chain-based small language models (SLM) for prediction, using both a threshold policy that transmits a word if its cosine similarity with that predicted/completed at the destination is below a threshold, and a peri
    
[^49]: 超声成像阵列编码的优化

    Optimization of Array Encoding for Ultrasound Imaging

    [https://arxiv.org/abs/2403.00289](https://arxiv.org/abs/2403.00289)

    利用机器学习构建扫描序列，产生高质量超声B模式图像，并优化超声成像编码序列。

    

    目标：合成孔径成像的发射编码模型是理解超声成像重建的声学传输效应的稳健而灵活的框架。我们的目标是利用机器学习（ML）构建由时间延迟和簧应权参数化的扫描序列，以产生高质量的B模式图像。方法：我们使用PyTorch中的ML模型，并从Field II生成模拟RF数据，探究可能编码序列空间，以找到能够最小化描述图像质量的损失函数的序列。这种方法通过一种新颖的延迟-求和波束成形导数的公式化，在计算上变得可行。我们在金属丝靶和组织模拟幻像上通过实验证实了这些结果。主要结果：根据给定的成像参数（成像域，硬件限制）训练后，我们的ML成像模型产生了优化的编码序列。

    arXiv:2403.00289v1 Announce Type: cross  Abstract: Objective: The transmit encoding model for synthetic aperture imaging is a robust and flexible framework for understanding the effect of acoustic transmission on ultrasound image reconstruction. Our objective is to use machine learning (ML) to construct scanning sequences, parameterized by time delays and apodization weights, that produce high quality B-mode images. Approach: We use an ML model in PyTorch and simulated RF data from Field II to probe the space of possible encoding sequences for those that minimize a loss function that describes image quality. This approach is made computationally feasible by a novel formulation of the derivative for delay-and-sum beamforming. We demonstrate these results experimentally on wire targets and a tissue-mimicking phantom. Main Results: When trained according to a given set of imaging parameters (imaging domain, hardware restrictions), our ML imaging model produces optimized encoding sequences
    
[^50]: 路线推荐综述：方法、应用和机会

    A Survey of Route Recommendations: Methods, Applications, and Opportunities

    [https://arxiv.org/abs/2403.00284](https://arxiv.org/abs/2403.00284)

    基于城市计算的路线推荐综述对路线推荐研究中的传统机器学习和现代深度学习方法进行了分类，展示了与城市计算场景相关的新应用，并揭示了最新进展。

    

    现今，随着先进的信息技术部署在整个城市，大量数据和强大的计算资源正在使现代城市发展智能化。作为智能交通的重要组成部分，路线推荐及其应用被广泛使用，直接影响市民的出行习惯。基于大数据（可能是多模式）开发智能高效的出行路线已成为路线推荐研究的核心挑战。我们的综述对基于城市计算的路线推荐工作进行了全面回顾。它分为以下三个部分：1）方法论。我们对大量传统机器学习和现代深度学习方法进行分类。同时，我们讨论它们的历史关系并揭示最新进展。2）应用方面。我们展示了大量与城市计算场景中路线推荐相关的新应用。3）我们迪

    arXiv:2403.00284v1 Announce Type: new  Abstract: Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens` travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: 1) Methodology-wise. We categorize a large volume of traditional machine learning and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. 2) Application\-wise. We present numerous novel applications related to route commendation within urban computing scenarios. 3) We di
    
[^51]: 尺度不变梯度聚合用于受约束多目标强化学习

    Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning

    [https://arxiv.org/abs/2403.00282](https://arxiv.org/abs/2403.00282)

    提出了一种名为CoMOGA的约束多目标梯度聚合算法，通过将目标转换为约束，实现了对帕累托最优策略的求解，同时满足预定义约束。

    

    多目标强化学习(MORL)的目标是找到一组帕累托最优策略，以涵盖各种偏好。然而，在实际应用中应用MORL，找到的策略不仅要帕累托最优，还要满足预定义的安全约束。为此，我们提出了一种名为约束多目标梯度聚合器(Constrained Multi-Objective Gradient Aggregator, CoMOGA)的约束MORL(CMORL)算法。CoMOGA意识到同时处理多个目标和约束的困难，通过将目标转换为额外的约束，将原始CMORL问题放松成一个约束优化问题。这种新颖的转换过程确保转换后的约束对目标尺度不变，同时具有与原始目标相同的效果。我们展示了所提方法收敛到一个局部帕累托最优策略，同时满足预定义约束。

    arXiv:2403.00282v1 Announce Type: new  Abstract: Multi-objective reinforcement learning (MORL) aims to find a set of Pareto optimal policies to cover various preferences. However, to apply MORL in real-world applications, it is important to find policies that are not only Pareto optimal but also satisfy pre-defined constraints for safety. To this end, we propose a constrained MORL (CMORL) algorithm called Constrained Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of handling multiple objectives and constraints concurrently, CoMOGA relaxes the original CMORL problem into a constrained optimization problem by transforming the objectives into additional constraints. This novel transformation process ensures that the converted constraints are invariant to the objective scales while having the same effect as the original objectives. We show that the proposed method converges to a local Pareto optimal policy while satisfying the predefined constraints. Empirical eva
    
[^52]: 差分隐私的平移插值

    Shifted Interpolation for Differential Privacy

    [https://arxiv.org/abs/2403.00278](https://arxiv.org/abs/2403.00278)

    本文在统一框架下建立了“通过迭代实现隐私放大”现象，提高了先前分析的水平，并由此获得了其他差分隐私概念更紧密的隐私核算。

    

    喧嚣的梯度下降及其变种是差分隐私机器学习中主导的算法。量化它们的隐私泄漏是一个基本问题，然而即使在凸损失的基础设置中，紧致的表征仍然是开放的。本文通过在$f$-差分隐私的统一框架下建立（和改进）“通过迭代实现隐私放大”现象，提高了先前分析的水平--这种方法紧紧捕捉了隐私损失的所有方面，并立即获得了其他差分隐私概念（如$(\varepsilon,\delta)$-DP和Renyi DP）更紧密的隐私核算。我们的关键技术见解是构建了揭示了流行的平移散度论证的平移插值过程，使得超越基于散度的差分隐私放宽的泛化成为可能。值得注意的是，这导致了在强凸基础设置中的第一个精确隐私分析。

    arXiv:2403.00278v1 Announce Type: new  Abstract: Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the "privacy amplification by iteration" phenomenon in the unifying framework of $f$-differential privacy--which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex
    
[^53]: 具有灵活节点的图构建用于交通需求预测

    Graph Construction with Flexible Nodes for Traffic Demand Prediction

    [https://arxiv.org/abs/2403.00276](https://arxiv.org/abs/2403.00276)

    本文提出一种新型的图构建方法，利用基于密度的聚类算法确定了图中节点的灵活定位，克服了传统算法的计算瓶颈，并从乘客数据中提取有价值信息用于初始化GNNs的边权重。

    

    图神经网络（GNNs）已被广泛应用于交通需求预测中，交通模式可分为基于站点的模式和自由漂浮交通模式。现有的交通图构建研究主要依赖于地图匹配，基于道路网络构建图。然而，在自由漂浮交通需求预测中，数据分布的复杂性和不均匀性使得道路网络匹配不够灵活。为了解决这些挑战，本文提出了一种针对自由漂浮交通模式量身定制的新型图构建方法。我们提出了一种新颖的基于密度的聚类算法（HDPC-L）来确定图中节点的灵活定位，克服传统聚类算法的计算瓶颈，实现对大规模数据集的有效处理。此外，我们从乘客数据中提取有价值的信息，以初始化GNNs的边权重。

    arXiv:2403.00276v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have been widely applied in traffic demand prediction, and transportation modes can be divided into station-based mode and free-floating traffic mode. Existing research in traffic graph construction primarily relies on map matching to construct graphs based on the road network. However, the complexity and inhomogeneity of data distribution in free-floating traffic demand forecasting make road network matching inflexible. To tackle these challenges, this paper introduces a novel graph construction method tailored to free-floating traffic mode. We propose a novel density-based clustering algorithm (HDPC-L) to determine the flexible positioning of nodes in the graph, overcoming the computational bottlenecks of traditional clustering algorithms and enabling effective handling of large-scale datasets. Furthermore, we extract valuable information from ridership data to initialize the edge weights of GNNs. Comprehen
    
[^54]: ARED: 阿根廷房地产数据集

    ARED: Argentina Real Estate Dataset

    [https://arxiv.org/abs/2403.00273](https://arxiv.org/abs/2403.00273)

    ARED是专为阿根廷市场设计的综合房地产价格预测数据集系列，尽管零版只包含短期信息，但展示了市场层面上的时间相关现象。

    

    阿根廷房地产市场是一个独特的案例研究，其特点是过去几十年间不稳定且迅速变化的宏观经济环境。尽管存在一些用于价格预测的数据集，但缺乏专门针对阿根廷的混合多模态数据集。本文介绍了ARED的第一版，这是专为阿根廷市场设计的综合房地产价格预测数据集系列。这个版本仅包含2024年1月至2月的信息。尽管这个零版只捕获了短短的时间范围（44天），但时间相关的现象主要发生在市场层面上（整个市场）。然而，未来版本的数据集很可能会包含历史数据。ARED中的每个列表都包含描述性特征和可变长度的图像集。

    arXiv:2403.00273v1 Announce Type: new  Abstract: The Argentinian real estate market presents a unique case study characterized by its unstable and rapidly shifting macroeconomic circumstances over the past decades. Despite the existence of a few datasets for price prediction, there is a lack of mixed modality datasets specifically focused on Argentina. In this paper, the first edition of ARED is introduced. A comprehensive real estate price prediction dataset series, designed for the Argentinian market. This edition contains information solely for Jan-Feb 2024. It was found that despite the short time range captured by this zeroth edition (44 days), time dependent phenomena has been occurring mostly on a market level (market as a whole). Nevertheless future editions of this dataset, will most likely contain historical data. Each listing in ARED comprises descriptive features, and variable-length sets of images.
    
[^55]: 双重姿势不变嵌入：学习用于识别和检索的类别和对象特定的判别性表示

    Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval

    [https://arxiv.org/abs/2403.00272](https://arxiv.org/abs/2403.00272)

    通过提出一种基于注意力的双编码器架构和特殊设计的损失函数，本文实现了在训练过程中同时学习基于类别和基于对象身份的嵌入，从而显著提高了姿势不变的对象识别和检索性能。

    

    在姿势不变的对象识别和检索的背景下，我们证明在训练过程中同时学习基于类别和基于对象身份的嵌入是可能的，并且可以显著提高性能。本文提出了一种基于注意力的双编码器架构，配合特别设计的损失函数，同时优化两个不同嵌入空间中的类间和类内距离，一个用于类别嵌入，另一个用于对象级嵌入。

    arXiv:2403.00272v1 Announce Type: cross  Abstract: In the context of pose-invariant object recognition and retrieval, we demonstrate that it is possible to achieve significant improvements in performance if both the category-based and the object-identity-based embeddings are learned simultaneously during training. In hindsight, that sounds intuitive because learning about the categories is more fundamental than learning about the individual objects that correspond to those categories. However, to the best of what we know, no prior work in pose-invariant learning has demonstrated this effect. This paper presents an attention-based dual-encoder architecture with specially designed loss functions that optimize the inter- and intra-class distances simultaneously in two different embedding spaces, one for the category embeddings and the other for the object-level embeddings. The loss functions we have proposed are pose-invariant ranking losses that are designed to minimize the intra-class d
    
[^56]: 大型卷积模型的参数高效调整

    Parameter-Efficient Tuning of Large Convolutional Models

    [https://arxiv.org/abs/2403.00269](https://arxiv.org/abs/2403.00269)

    通过引入滤波器子空间和滤波器原子的概念，本研究提出了一种在微调大型卷积模型时仅调整少量参数来提取任务特定表示的方法。

    

    为了解决微调大型预训练模型所需的高计算和参数复杂性，研究人员开发了参数高效的方法，仅更新下游任务的部分参数。然而，这些工作通常忽视了卷积核的独特属性，而卷积核仍然是许多大型模型的基本元素，比如Stable Diffusion。在本研究中，我们首先通过在每个网络层内分解卷积核到一小组滤波器子空间元素，即滤波器原子，引入了滤波器子空间。然后，我们通过仅调整滤波器原子（通常为几百个参数）对这些模型进行微调，以提取任务特定的表示。为了潜在地扩展调整的参数空间，我们进一步展示了一种简单的方法，通过递归地将每个筛选原子分解到另一组筛选原子来生成一个过完备的滤波器子空间。

    arXiv:2403.00269v1 Announce Type: cross  Abstract: To address the high computational and parameter complexity associated with fine-tuning large pre-trained models, researchers have developed parameter-efficient methods, where only partial parameters are updated for downstream tasks. However, these works often overlook the distinct properties of convolutional kernels, which still remain essential elements in many large models, such as Stable Diffusion. In this study, we first introduce filter subspace by decomposing convolutional kernels within each network layer over a small set of filter subspace elements, referred to as filter atoms. We then fine-tune these models to extract task-specific representation by only adapting the filter atoms, a few hundred parameters typically. To potentially expand the parameter space for tuning, we further show a simple approach to generate an overcomplete filter subspace by recursively decomposing each filter atom over another set of filter atoms. The 
    
[^57]: 用机器学习和等变基础模型解读弥散散射：以熔融FeO为例

    Deciphering diffuse scattering with machine learning and the equivariant foundation model: The case of molten FeO

    [https://arxiv.org/abs/2403.00259](https://arxiv.org/abs/2403.00259)

    用机器学习和等变基础模型成功地解读了熔融FeO的弥散散射，提供了实验和量子力学模型之间更好的一致性

    

    用机器学习和等变基础模型解读弥散散射的过程对于将弥散X射线或中子散射测量和原子-原子对势预测结构在无序材料中的衔接一直是凝聚态物理学中长期面临的挑战。本文概述了过去几十年来采用的传统方法，即使用将三维结构模型与测量的结构因子及其相关的原子对分布函数相关联的近似原子间对势。近年来，机器学习的原子间势的使用已经增长，并且在离子和氧化物系统的情况下取得了特别的成功。大规模采样的最新进展，以及将散射测量直接整合到模型开发中，为实验和用量子力学精度计算的大规模模型之间提供了改善的一致性。然而，关于l

    arXiv:2403.00259v1 Announce Type: cross  Abstract: Bridging the gap between diffuse x-ray or neutron scattering measurements and predicted structures derived from atom-atom pair potentials in disordered materials, has been a longstanding challenge in condensed matter physics. This perspective gives a brief overview of the traditional approaches employed over the past several decades. Namely, the use of approximate interatomic pair potentials that relate 3-dimensional structural models to the measured structure factor and its associated pair distribution function. The use of machine learned interatomic potentials has grown in the past few years, and has been particularly successful in the cases of ionic and oxide systems. Recent advances in large scale sampling, along with a direct integration of scattering measurements into the model development, has provided improved agreement between experiments and large-scale models calculated with quantum mechanical accuracy. However, details of l
    
[^58]: “无损”压缩深度神经网络：一种高维神经切线核方法

    "Lossless" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach

    [https://arxiv.org/abs/2403.00258](https://arxiv.org/abs/2403.00258)

    本研究利用神经切线核和随机矩阵理论，提出了一种新颖的压缩方法，能够在高维度情形下对宽而全连接的深度神经网络进行有效压缩。

    

    现代深度神经网络（DNNs）非常强大；然而，这是以增加深度并使每一层的参数更多为代价的，这使得它们的训练和推断变得更具挑战性。为了应对这一关键限制，人们致力于对这些大规模机器学习模型进行压缩（如稀疏化和/或量化），以便它们可以部署在低功耗的物联网设备上。本文基于神经切线核（NTK）和随机矩阵理论（RMT）的最新进展，提出了一种新颖的压缩方法，适用于宽而全连接的\emph{深}神经网络。具体而言，我们展示了在高维度情形下，当数据点的数量$n$和它们的维度$p$都很大，并且数据遵循高斯混合模型时，对于一大类DNN，NTK矩阵之间存在\emph{渐近谱等价}。

    arXiv:2403.00258v1 Announce Type: cross  Abstract: Modern deep neural networks (DNNs) are extremely powerful; however, this comes at the price of increased depth and having more parameters per layer, making their training and inference more computationally challenging. In an attempt to address this key limitation, efforts have been devoted to the compression (e.g., sparsification and/or quantization) of these large-scale machine learning models, so that they can be deployed on low-power IoT devices. In this paper, building upon recent advances in neural tangent kernel (NTK) and random matrix theory (RMT), we provide a novel compression approach to wide and fully-connected \emph{deep} neural nets. Specifically, we demonstrate that in the high-dimensional regime where the number of data points $n$ and their dimension $p$ are both large, and under a Gaussian mixture model for the data, there exists \emph{asymptotic spectral equivalence} between the NTK matrices for a large family of DNN m
    
[^59]: 利用压缩和激励卷积神经网络对放射性肺气肿亚型进行强大的深度标记：MESA Lung和SPIROMICS研究

    Robust deep labeling of radiological emphysema subtypes using squeeze and excitation convolutional neural networks: The MESA Lung and SPIROMICS Studies

    [https://arxiv.org/abs/2403.00257](https://arxiv.org/abs/2403.00257)

    提出了一种利用压缩和激励卷积神经网络对肺CT图像进行sLTP和CTES分类的方法，实现了准确和可重现的肺部CT扫描sLTP分割。

    

    肺气肿是肺组织进行性不可逆性的损失，在肺病理学和肺部计算机断层扫描（CT）图像上通常被分类为三个亚型。最近的研究导致了在肺部CT上无监督学习了十个空间信息的肺部纹理图案（sLTP），表示基于肺气肿肺实质的纹理外观和肺部内的空间位置的不同模式，并聚合成6个稳健且可重复的CT肺气肿亚型（CTES）。然而，现有的sLTP分割方法对CT获取协议的变化非常敏感且运行缓慢。在这项工作中，我们提出了一个用于在肺部CT上进行sLTP和CTES有监督分类的稳健3D压缩和激励CNN。我们的结果表明，该模型在两个独立队列上实现了准确和可重现的肺部CT扫描sLTP分割，独立于扫描仪制造商。

    arXiv:2403.00257v1 Announce Type: cross  Abstract: Pulmonary emphysema, the progressive, irreversible loss of lung tissue, is conventionally categorized into three subtypes identifiable on pathology and on lung computed tomography (CT) images. Recent work has led to the unsupervised learning of ten spatially-informed lung texture patterns (sLTPs) on lung CT, representing distinct patterns of emphysematous lung parenchyma based on both textural appearance and spatial location within the lung, and which aggregate into 6 robust and reproducible CT Emphysema Subtypes (CTES). Existing methods for sLTP segmentation, however, are slow and highly sensitive to changes in CT acquisition protocol. In this work, we present a robust 3-D squeeze-and-excitation CNN for supervised classification of sLTPs and CTES on lung CT. Our results demonstrate that this model achieves accurate and reproducible sLTP segmentation on lung CTscans, across two independent cohorts and independently of scanner manufactu
    
[^60]: 基于云的MRI分割联邦学习框架

    Cloud-based Federated Learning Framework for MRI Segmentation

    [https://arxiv.org/abs/2403.00254](https://arxiv.org/abs/2403.00254)

    提出了一个针对农村医疗环境的脑组织分割的新型框架，结合了深度强化学习环境和本地的精调模型，采用联邦学习进行模型训练，以维护数据隐私并增强模型泛化能力。

    

    在当代农村医疗环境中，诊断脑部图像的主要挑战是数据稀缺，因为大多数现有的深度学习模型需要大量的训练数据来优化其性能，这需要中心化处理方法，可能会损害数据隐私。本文提出了一个针对农村医疗设施的脑组织分割的新颖框架。该框架同时使用了深度强化学习（DRL）环境和在农村医疗网站本地部署的精调模型（RM）。所提出的DRL模型具有较少的参数数量，并且适用于在分布式农村站点上实现。为了维护数据隐私并增强模型的泛化能力，我们采用联邦学习（FL）进行合作模型训练。我们通过训练网络来展示我们方法的有效性

    arXiv:2403.00254v1 Announce Type: cross  Abstract: In contemporary rural healthcare settings, the principal challenge in diagnosing brain images is the scarcity of available data, given that most of the existing deep learning models demand extensive training data to optimize their performance, necessitating centralized processing methods that potentially compromise data privacy. This paper proposes a novel framework tailored for brain tissue segmentation in rural healthcare facilities. The framework employs a deep reinforcement learning (DRL) environment in tandem with a refinement model (RM) deployed locally at rural healthcare sites. The proposed DRL model has a reduced parameter count and practicality for implementation across distributed rural sites. To uphold data privacy and enhance model generalization without transgressing privacy constraints, we employ federated learning (FL) for cooperative model training. We demonstrate the efficacy of our approach by training the network wi
    
[^61]: EUROPA：一个法律多语关键词生成数据集

    EUROPA: A Legal Multilingual Keyphrase Generation Dataset

    [https://arxiv.org/abs/2403.00252](https://arxiv.org/abs/2403.00252)

    提出了一个用于法律领域多语关键词生成的数据集EUROPA，包含所有24种欧盟官方语言，表明在特定领域多语言语料库上仍有改进空间。

    

    关键词生成主要在学术研究文章的背景下进行探索，特别侧重于科学领域和英语。 在这项工作中，我们提出了EUROPA，一个用于法律领域多语关键词生成的数据集。 它源自欧洲法院的法律判决，并包含了所有24种欧盟官方语言中的实例。 我们在我们的语料库上运行多语言模型并分析结果，展示了在像我们提出的特定领域多语言语料库上有改进空间。

    arXiv:2403.00252v1 Announce Type: cross  Abstract: Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.
    
[^62]: 使用FlanT5-XXL进行零-shot立场检测的基准测试：从训练数据、提示和解码策略中探讨其接近SOTA的表现

    Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance

    [https://arxiv.org/abs/2403.00236](https://arxiv.org/abs/2403.00236)

    通过使用FlanT5-XXL和SemEval 2016数据集，研究了零-shot立场检测在推特上的性能表现及其对提示和解码策略的敏感性，揭示了其能够匹敌或超越最先进基准测试的能力，并识别了其中的潜在偏见。

    

    我们研究了基于LLM的零-shot立场检测在推特上的表现。使用FlanT5-XXL，一个经过调整指令的开源LLM，在SemEval 2016任务6A、6B和P-Stance数据集上，我们研究了在不同提示和解码策略下的表现及其变化，以及模型的潜在偏见。我们展示零-shot方法可以匹敌甚至胜过最先进的基准测试，包括微调模型。我们提供了对其表现的各种见解，包括对指令和提示的敏感性，解码策略，提示的困惑度，以及提示中存在的否定和反对。最后，我们确保LLM没有在测试数据集上进行训练，并确定了一种可能部分解释解码策略之间表现差异的积极偏见。

    arXiv:2403.00236v1 Announce Type: cross  Abstract: We investigate the performance of LLM-based zero-shot stance detection on tweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the SemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and its variations under different prompts and decoding strategies, as well as the potential biases of the model. We show that the zero-shot approach can match or outperform state-of-the-art benchmarks, including fine-tuned models. We provide various insights into its performance including the sensitivity to instructions and prompts, the decoding strategies, the perplexity of the prompts, and to negations and oppositions present in prompts. Finally, we ensure that the LLM has not been trained on test datasets, and identify a positivity bias which may partially explain the performance differences across decoding strategie
    
[^63]: 具有一般因果模型和干预的因果赌博机

    Causal Bandits with General Causal Models and Interventions

    [https://arxiv.org/abs/2403.00233](https://arxiv.org/abs/2403.00233)

    该论文在三个方向上推进了因果赌博机的结果：在一般因果模型下进行干预设计，实现广义软干预以及提供一般的遗憾上下界。

    

    本文考虑了因果赌博机（CBs）用于因果系统中干预的顺序设计。其目标是通过最小化与事后最佳干预序列相比的累积遗憾度量来优化奖励函数。本文将CBs的结果推进了三个方向。首先，假设结构因果模型（SCMs）未知，并且任意从Lipschitz连续函数类$\mathcal{F}$中绘制。现有结果通常集中在（广义）线性SCMs上。其次，假设干预是广义软干预，具有任意所需粒度的水平，导致无限数量的可能干预。相比之下，现有文献通常采用原子和硬干预。第三，我们提供了关于遗憾的一般上下界。上界包含（并改进）特殊情况的已知界限。下界是gene

    arXiv:2403.00233v1 Announce Type: cross  Abstract: This paper considers causal bandits (CBs) for the sequential design of interventions in a causal system. The objective is to optimize a reward function via minimizing a measure of cumulative regret with respect to the best sequence of interventions in hindsight. The paper advances the results on CBs in three directions. First, the structural causal models (SCMs) are assumed to be unknown and drawn arbitrarily from a general class $\mathcal{F}$ of Lipschitz-continuous functions. Existing results are often focused on (generalized) linear SCMs. Second, the interventions are assumed to be generalized soft with any desired level of granularity, resulting in an infinite number of possible interventions. The existing literature, in contrast, generally adopts atomic and hard interventions. Third, we provide general upper and lower bounds on regret. The upper bounds subsume (and improve) known bounds for special cases. The lower bounds are gene
    
[^64]: 利用几何模型辅助深度学习的衍射和散射感知无线电地图与环境重建

    Diffraction and Scattering Aware Radio Map and Environment Reconstruction using Geometry Model-Assisted Deep Learning

    [https://arxiv.org/abs/2403.00229](https://arxiv.org/abs/2403.00229)

    该论文提出了一种利用几何结构辅助深度学习的方法，通过接收信号强度数据共同构建无线电地图和虚拟环境，同时开发了用于提取关键衍射特征和描述散射的模型。

    

    arXiv:2403.00229v1 公告类型: 交叉摘要: 机器学习（ML）有助于5G及以上无线通信系统的快速信道建模。许多现有的ML技术利用城市地图构建无线电地图；然而，并非始终可用更新的城市地图。本文提出利用接收信号强度（RSS）数据共同构建无线电地图和虚拟环境，通过利用环境的几何结构。与许多现有的缺乏环境模型的ML方法不同，我们开发了一个虚拟障碍模型，并表征了传播路径与虚拟障碍物之间的几何关系。采用多屏幕刀刃模型提取关键的衍射特征，并将这些特征输入神经网络（NN）进行衍射表示。为了描述散射，与大多数现有方法直接输入整个城市地图不同，我们的模型侧重于几何

    arXiv:2403.00229v1 Announce Type: cross  Abstract: Machine learning (ML) facilitates rapid channel modeling for 5G and beyond wireless communication systems. Many existing ML techniques utilize a city map to construct the radio map; however, an updated city map may not always be available. This paper proposes to employ the received signal strength (RSS) data to jointly construct the radio map and the virtual environment by exploiting the geometry structure of the environment. In contrast to many existing ML approaches that lack of an environment model, we develop a virtual obstacle model and characterize the geometry relation between the propagation paths and the virtual obstacles. A multi-screen knife-edge model is adopted to extract the key diffraction features, and these features are fed into a neural network (NN) for diffraction representation. To describe the scattering, as oppose to most existing methods that directly input an entire city map, our model focuses on the geometry st
    
[^65]: 通过离线技能扩散实现稳健策略学习

    Robust Policy Learning via Offline Skill Diffusion

    [https://arxiv.org/abs/2403.00225](https://arxiv.org/abs/2403.00225)

    提出了一种新颖的离线技能学习框架DuSkill，通过引导扩散模型生成通用技能，从而增强不同领域任务的策略学习鲁棒性。

    

    基于技能的强化学习方法在解决长时域任务中表现出了相当大的潜力，尤其是通过分层结构。这些技能是从离线数据集中无关任务地学习的，可以加快针对新任务的策略学习过程。然而，由于这些技能在不同领域中的应用仍受限于对数据集的固有依赖，当尝试通过强化学习为不同于数据集领域的目标领域学习基于技能的策略时，这一挑战就变得困难。在本文中，我们提出了一个新颖的离线技能学习框架DuSkill，它采用了引导扩散模型来生成从数据集中有限技能扩展出的通用技能，从而增强了不同领域任务的策略学习鲁棒性。具体来说，我们设计了一个引导扩散技能解码器，结合分层编码，以解开技能嵌入。

    arXiv:2403.00225v1 Announce Type: new  Abstract: Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embeddi
    
[^66]: 存在大规模局部代理的全局决策高效强化学习

    Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale

    [https://arxiv.org/abs/2403.00222](https://arxiv.org/abs/2403.00222)

    该研究提出了SUB-SAMPLE-Q算法，通过对局部代理进行子采样，在指数级别的时间内计算出最佳策略，从而实现了与标准方法相比的指数加速。

    

    我们研究了存在许多局部代理的全局决策的强化学习问题，其中全局决策者做出影响所有局部代理的决策，目标是学习一个最大化全局和局部代理奖励的策略。在这种情况下，可扩展性一直是一个长期存在的挑战，因为状态/动作空间的大小可能会随代理数量指数增长。本文提出了SUB-SAMPLE-Q算法，在此算法中，全局代理对$k\leq n$个局部代理进行子采样以在仅指数于$k$的时间内计算出最佳策略，从而提供了与指数于$n$的标准方法相比的指数加速。我们展示了随着子采样代理数$k$的增加，学到的策略将收敛于顺序为$\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$的最优策略。

    arXiv:2403.00222v1 Announce Type: new  Abstract: We study reinforcement learning for global decision-making in the presence of many local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the rewards of both the global and the local agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state/action space which can be exponential in the number of agents. This work proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\leq n$ local agents to compute an optimal policy in time that is only exponential in $k$, providing an exponential speedup from standard methods that are exponential in $n$. We show that the learned policy converges to the optimal policy in the order of $\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$ as the number of sub-sampled agents $k$ increases, 
    
[^67]: 使用在自定义数据集上微调的XLSR Wav2Vec2和mBART进行视频的转录和翻译

    Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART

    [https://arxiv.org/abs/2403.00212](https://arxiv.org/abs/2403.00212)

    使用XLSR Wav2Vec2和mBART在自定义数据集上进行微调，实现了视频内容的多语言转录和翻译，为个性化语音提供了可访问的解决方案

    

    这项研究解决了使用极少数据训练个性化语音识别模型的挑战。仅利用来自YouTube视频的14分钟自定义音频，我们采用基于检索的语音转换（RVC）创建了一个自定义的Common Voice 16.0语料库。随后，在这个数据集上对一个跨语言自监督表示（XLSR）Wav2Vec2模型进行了微调。开发的基于Web的GUI可以高效地转录和翻译输入的印地语视频。通过集成XLSR Wav2Vec2和mBART，该系统将翻译后的文本与视频时间轴对齐，为个性化语音的多语言视频内容转录和翻译提供了可访问的解决方案。

    arXiv:2403.00212v1 Announce Type: new  Abstract: This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.
    
[^68]: 利用数据增强和偏好优化改进苏格拉底提问生成

    Improving Socratic Question Generation using Data Augmentation and Preference Optimization

    [https://arxiv.org/abs/2403.00199](https://arxiv.org/abs/2403.00199)

    通过数据增强和偏好优化，改进了苏格拉底提问生成方法，减轻教师繁重的工作量，防止生成无效问题。

    

    苏格拉底方法是一种引导学生独立解决问题而不直接揭示问题解决方案的方法。本文提出一种通过数据增强和偏好优化改进苏格拉底提问生成的方法，用于增强巨大语言模型自动生成苏格拉底问题，以减轻教师的繁重工作量。研究表明，现有涉及提示这些巨大语言模型的方法有时会产生无效的输出，例如直接揭示问题解决方案或提供无关或过早的问题。为了解决这一问题，本研究首先提出一种数据增强方法，以丰富现有的苏格拉底提问数据集；其次，提出一种方法来优化开源巨大语言模型，例如LLama 2，以更倾向于地面真值问题。

    arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
    
[^69]: AXOLOTL：通过辅助自我去偏见大型语言模型输出实现公平性

    AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs

    [https://arxiv.org/abs/2403.00198](https://arxiv.org/abs/2403.00198)

    AXOLOTL是一个新颖的后处理框架，通过零样本学习的三步过程，识别和解决偏见，指导模型自我去偏见其输出，从而实现公平性并保持模型性能。

    

    预训练的大型语言模型（LLMs）极大地推动了自然语言处理能力，但容易受其训练数据中存在的偏见影响，导致在各种应用中出现不公平结果。尽管已经提出了许多策略来减轻偏见，但它们通常需要大量计算资源，可能会损害模型性能。在这项工作中，我们介绍了AXOLOTL，这是一个新颖的后处理框架，能够独立于任务和模型运行，在不直接访问内部参数的情况下利用公共API与LLMs交互。通过类似零样本学习的三步过程，AXOLOTL识别偏见，提出解决方案，并指导模型自我去偏见其输出。这种方法最小化了计算成本，并保持了模型性能，使AXOLOTL成为一个具有广泛适用性和易用性的去偏见LLM输出的有前景工具。

    arXiv:2403.00198v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications. While numerous strategies have been proposed to mitigate bias, they often require extensive computational resources and may compromise model performance. In this work, we introduce AXOLOTL, a novel post-processing framework, which operates agnostically across tasks and models, leveraging public APIs to interact with LLMs without direct access to internal parameters. Through a three-step process resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs. This approach minimizes computational costs and preserves model performance, making AXOLOTL a promising tool for debiasing LLM outputs with broad applicability and ease of use.
    
[^70]: 利用合成数据增强学习找到缺失视频帧：一个通用框架及在使用RGB摄像头生成热成像中的应用

    Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras

    [https://arxiv.org/abs/2403.00196](https://arxiv.org/abs/2403.00196)

    通过引入生成模型的方法解决了由于传感器帧率不匹配而导致数据缺失的问题，利用有条件的生成对抗网络（cGANs）中的pix2pix架构比CycleGAN表现更好，多视角输入风格特别是堆叠视图可以增强热图像生成的准确性

    

    高级驾驶辅助系统（ADAS）在智能车辆中依赖于车辆车厢内的准确驾驶员感知，通常利用各种传感模式的结合。然而，这些模式以不同的速率操作，对于实时、全面的驾驶员状态监测提出了挑战。本文解决了由于传感器帧率不匹配导致数据缺失的问题，引入了一种生成模型方法来创建合成但逼真的热成像。我们提出使用有条件的生成对抗网络（cGANs），具体比较了pix2pix和CycleGAN架构。实验结果表明pix2pix优于CycleGAN，并且利用多视角输入风格，特别是堆叠视图，增强了热图像生成的准确性。此外，该研究评估了模型在不同主体之间的泛化能力，揭示了个性化训练对于最佳性能的重要性。

    arXiv:2403.00196v1 Announce Type: cross  Abstract: Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on accurate driver perception within the vehicle cabin, often leveraging a combination of sensing modalities. However, these modalities operate at varying rates, posing challenges for real-time, comprehensive driver state monitoring. This paper addresses the issue of missing data due to sensor frame rate mismatches, introducing a generative model approach to create synthetic yet realistic thermal imagery. We propose using conditional generative adversarial networks (cGANs), specifically comparing the pix2pix and CycleGAN architectures. Experimental results demonstrate that pix2pix outperforms CycleGAN, and utilizing multi-view input styles, especially stacked views, enhances the accuracy of thermal image generation. Moreover, the study evaluates the model's generalizability across different subjects, revealing the importance of individualized training for optimal pe
    
[^71]: 询问您的分布转移是否适合进行预训练

    Ask Your Distribution Shift if Pre-Training is Right for You

    [https://arxiv.org/abs/2403.00194](https://arxiv.org/abs/2403.00194)

    预训练有助于缓解较差的外推，但对数据集偏见无济于事。

    

    预训练是一种广泛使用的方法，用于开发对分布转移具有鲁棒性的模型。然而，在实践中，其有效性因情况而异：在某些情况下，微调预训练模型可以显著改善鲁棒性，但在其他情况下则完全不同（与从头开始训练相比）。在这项工作中，我们试图描述预训练可以和不能解决的失败模式。特别是，我们关注模型在分布转移下可能出现的两种失败模式：较差的外推（例如，它们无法推广到不同领域）和训练数据中的偏见（例如，它们依赖于虚假特征）。我们的研究表明，作为一个经验准则，预训练可以帮助缓解较差的外推，但不能缓解数据集偏见。在提供了这一发现的理论动机和实证证据后，我们探讨了开发鲁棒模型的两个潜在含义：（1）预训练和旨在防止利用偏见的干预措施。

    arXiv:2403.00194v1 Announce Type: new  Abstract: Pre-training is a widely used approach to develop models that are robust to distribution shifts. However, in practice, its effectiveness varies: fine-tuning a pre-trained model improves robustness significantly in some cases but not at all in others (compared to training from scratch). In this work, we seek to characterize the failure modes that pre-training can and cannot address. In particular, we focus on two possible failure modes of models under distribution shift: poor extrapolation (e.g., they cannot generalize to a different domain) and biases in the training data (e.g., they rely on spurious features). Our study suggests that, as a rule of thumb, pre-training can help mitigate poor extrapolation but not dataset biases. After providing theoretical motivation and empirical evidence for this finding, we explore two of its implications for developing robust models: (1) pre-training and interventions designed to prevent exploiting bi
    
[^72]: 分散学习对斯塔克尔贝格博弈中玩家效用的影响

    Impact of Decentralized Learning on Player Utilities in Stackelberg Games

    [https://arxiv.org/abs/2403.00188](https://arxiv.org/abs/2403.00188)

    研究了分散学习对斯塔克尔贝格博弈中玩家效用的影响，提出了一种放松遗憾基准来更好捕捉系统特征，并开发了实现近乎最优遗憾的算法。

    

    当学习代理（如推荐系统或聊天机器人）在现实世界中部署时，通常会随时间反复与另一个学习代理（如用户）交互。在许多这样的双代理系统中，每个代理单独学习，而两个代理的奖励并不完全一致。为了更好地理解这类情况，我们研究了两代理系统的学习动态以及对每个代理目标的影响。我们将这些系统建模为具有分散学习的斯塔克尔贝格博弈，并展示标准遗憾基准（如斯塔克尔贝格均衡回报）导致至少有一名玩家出现最坏情况的线性遗憾。为了更好地捕捉这些系统，我们构建了一种对代理的学习误差容忍的放松遗憾基准。我们展示了标准学习算法未能提供次线性遗憾，并开发了算法，实现了对于双方玩家而言与理想$O(T^{2/3})$遗憾接近最优。

    arXiv:2403.00188v1 Announce Type: new  Abstract: When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent's objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal $O(T^{2/3})$ regret for both players with respect to 
    
[^73]: 低秩矩阵完成在高度非均匀采样下的入口特定界限

    Entry-Specific Bounds for Low-Rank Matrix Completion under Highly Non-Uniform Sampling

    [https://arxiv.org/abs/2403.00184](https://arxiv.org/abs/2403.00184)

    该论文提出了针对高度非均匀采样条件下低秩矩阵完成问题的入口特定界限，通过定制每个条目的误差上界来匹配一定条件下的极小下界。

    

    低秩矩阵完成涉及使用稀疏一组观测条目来估计矩阵中未观察到的条目的问题。我们考虑观测到的条目是用高度变化的概率进行采样的非均匀设置，可能具有不同的渐近标度。我们证明在结构化采样概率下，通常情况下以及有时在较小的子矩阵上运行估计算法比在整个矩阵上更好，甚至是最佳的。特别是，我们证明了定制到每个条目的误差上界，这些上界在某些条件下与极小下界相匹配。我们的界限以局部采样概率的函数的形式刻画了估计每个条目的难度。我们提供了数值实验，证实了我们的理论发现。

    arXiv:2403.00184v1 Announce Type: cross  Abstract: Low-rank matrix completion concerns the problem of estimating unobserved entries in a matrix using a sparse set of observed entries. We consider the non-uniform setting where the observed entries are sampled with highly varying probabilities, potentially with different asymptotic scalings. We show that under structured sampling probabilities, it is often better and sometimes optimal to run estimation algorithms on a smaller submatrix rather than the entire matrix. In particular, we prove error upper bounds customized to each entry, which match the minimax lower bounds under certain conditions. Our bounds characterize the hardness of estimating each entry as a function of the localized sampling probabilities. We provide numerical experiments that confirm our theoretical findings.
    
[^74]: 因果图ODE：多智能体动态系统中的连续处理效应建模

    Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems

    [https://arxiv.org/abs/2403.00178](https://arxiv.org/abs/2403.00178)

    提出了Causal Graph Ordinary Differential Equations (CAG-ODE) 模型，能够捕捉多智能体系统中处理的连续动态影响，利用图神经网络作为ODE函数。

    

    实际世界中的多智能体系统通常是动态和连续的，其中智能体共同演化，并随时间改变其轨迹和相互作用。例如，美国的COVID-19传播可以看作是一个多智能体系统，其中各州充当智能体，它们之间的日常人口流动就是相互作用。在这种系统中估计反事实结果可以实现准确的未来预测和有效的决策制定，例如制定COVID-19政策。然而，现有方法未能有效地模拟处理对结果的连续动态影响，尤其是当同时应用多种处理（例如“居家隔离”和“接种疫苗”政策）时。为了解决这一挑战，我们提出了因果图普通微分方程（CAG-ODE），这是一种能够使用图神经网络（GNN）作为ODE函数捕捉智能体之间连续相互作用的新颖模型。

    arXiv:2403.00178v1 Announce Type: cross  Abstract: Real-world multi-agent systems are often dynamic and continuous, where the agents co-evolve and undergo changes in their trajectories and interactions over time. For example, the COVID-19 transmission in the U.S. can be viewed as a multi-agent system, where states act as agents and daily population movements between them are interactions. Estimating the counterfactual outcomes in such systems enables accurate future predictions and effective decision-making, such as formulating COVID-19 policies. However, existing methods fail to model the continuous dynamic effects of treatments on the outcome, especially when multiple treatments (e.g., "stay-at-home" and "get-vaccine" policies) are applied simultaneously. To tackle this challenge, we propose Causal Graph Ordinary Differential Equations (CAG-ODE), a novel model that captures the continuous interaction among agents using a Graph Neural Network (GNN) as the ODE function. The key innovat
    
[^75]: 使用物理知识的自监督学习构建非侵入式医学数字孪生体

    Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning

    [https://arxiv.org/abs/2403.00177](https://arxiv.org/abs/2403.00177)

    提出了一种使用物理知识的自监督学习算法，通过仅使用非侵入式患者健康数据识别数字孪生体模型参数，从而实现了非侵入式医学数字孪生体的构建。

    

    数字孪生体是实现实际物理现象的虚拟复制品，利用数学建模来表征和模拟其定义特征。通过为疾病过程构建数字孪生体，我们可以进行仿真，模拟患者在虚拟环境中的健康状况和在假设干预下的对照结果。这消除了侵入性程序或不确定治疗决策的需求。本文提出了一种仅利用非侵入式患者健康数据来识别数字孪生体模型参数的方法。我们将数字孪生体建模看作一个复合逆问题，并观察到其结构类似于自监督学习中的预训练和微调。利用这一点，我们引入了一种基于物理知识的自监督学习算法，这种算法首先在解决物理模型方程的假定任务上对神经网络进行预训练。随后，该模型被训练以...

    arXiv:2403.00177v1 Announce Type: new  Abstract: A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of solving the physical model equations. Subsequently, the model is trained to rec
    
[^76]: SoD$^2$: 静态优化动态深度神经网络

    SoD$^2$: Statically Optimizing Dynamic Deep Neural Network

    [https://arxiv.org/abs/2403.00176](https://arxiv.org/abs/2403.00176)

    本文提出了SoD$^2$框架，用于静态优化动态深度神经网络，通过秩和维度传播（RDP）方法实现了操作符的形状静态确定，进而进行一系列优化，包括融合代码生成、执行计划和运行时内存分配计划生成。

    

    虽然近年来已开发了许多针对DNN的编译和运行时系统，但主要集中在静态DNN上。动态DNN，其中张量形状和大小甚至使用的操作符集取决于输入和/或执行，正在变得常见。本文提出了SoD$^2$，一个用于优化动态DNN的综合框架。我们方法的基础是对构成DNN的常见操作符进行分类，并利用这一分类方法来实现秩和维度传播（RDP）方法。该框架静态确定操作符的形状为已知常量、符号常量或这些操作的运算。接下来，使用RDP我们实现一系列优化，如融合代码生成、执行（顺序）计划，甚至运行时内存分配计划生成。通过在 10 个新兴动态DNN 上评估该框架，并将其与几个现有系统进行比较，我们展示了

    arXiv:2403.00176v1 Announce Type: cross  Abstract: Though many compilation and runtime systems have been developed for DNNs in recent years, the focus has largely been on static DNNs. Dynamic DNNs, where tensor shapes and sizes and even the set of operators used are dependent upon the input and/or execution, are becoming common. This paper presents SoD$^2$, a comprehensive framework for optimizing Dynamic DNNs. The basis of our approach is a classification of common operators that form DNNs, and the use of this classification towards a Rank and Dimension Propagation (RDP) method. This framework statically determines the shapes of operators as known constants, symbolic constants, or operations on these. Next, using RDP we enable a series of optimizations, like fused code generation, execution (order) planning, and even runtime memory allocation plan generation. By evaluating the framework on 10 emerging Dynamic DNNs and comparing it against several existing systems, we demonstrate both 
    
[^77]: 超越黑盒策略：重新思考可解释和可验证HVAC控制学习代理的设计

    Go Beyond Black-box Policies: Rethinking the Design of Learning Agent for Interpretable and Verifiable HVAC Control

    [https://arxiv.org/abs/2403.00172](https://arxiv.org/abs/2403.00172)

    通过从现有热动力学模型和历史数据提取的决策树重新设计HVAC控制器，克服了可靠性瓶颈，并实现了更节能的策略。

    

    近期的研究表明，基于模型的强化学习（MBRL）有潜力提高暖通空调（HVAC）系统的能效。然而，现有方法依赖于黑盒热动力学模型和随机优化器，缺乏可靠性保证并对居住者健康构成风险。在本工作中，我们通过重新设计使用现有热动力学模型和历史数据提取的决策树的HVAC控制器，克服了可靠性瓶颈。我们的决策树策略是确定性的、可验证的、可解释的，并且比当前的MBRL方法更节能。首先，我们引入了基于领域知识的RL代理在HVAC控制中的新颖验证标准。其次，我们开发了一个生成可验证决策树策略的策略提取过程。我们发现，热动力学模型输入的高维度阻碍了效率。

    arXiv:2403.00172v1 Announce Type: cross  Abstract: Recent research has shown the potential of Model-based Reinforcement Learning (MBRL) to enhance energy efficiency of Heating, Ventilation, and Air Conditioning (HVAC) systems. However, existing methods rely on black-box thermal dynamics models and stochastic optimizers, lacking reliability guarantees and posing risks to occupant health. In this work, we overcome the reliability bottleneck by redesigning HVAC controllers using decision trees extracted from existing thermal dynamics models and historical data. Our decision tree-based policies are deterministic, verifiable, interpretable, and more energy-efficient than current MBRL methods. First, we introduce a novel verification criterion for RL agents in HVAC control based on domain knowledge. Second, we develop a policy extraction procedure that produces a verifiable decision tree policy. We found that the high dimensionality of the thermal dynamics model input hinders the efficiency 
    
[^78]: TELEClass: 税务学丰富和LLM增强的最小监督分层文本分类

    TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision

    [https://arxiv.org/abs/2403.00165](https://arxiv.org/abs/2403.00165)

    本文提出了一种最小监督的分层文本分类方法，利用每个节点的唯一类名作为唯一监督，同时结合大型语言模型（LLM）提高分类性能。

    

    分层文本分类旨在将每个文档分类为标签Taxonomy中的一组类别。本文旨在研究使用最少监督：仅使用每个节点的唯一类名作为监督来进行分层文本分类。最近，大型语言模型（LLM）通过零提示在各种任务上表现出竞争性能，但这种方法在分层设置中表现较差，因为在提示中包含大而结构化的标签空间是无效的。另一方面，以前的弱监督分层文本分类方法仅利用原始的Taxonomy骨架，忽略了文本语料库中隐藏的丰富信息，这些信息可以用作额外的类别指示信息。

    arXiv:2403.00165v1 Announce Type: new  Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative 
    
[^79]: 使用蒙特卡洛高效影响函数实现自动化高效估计

    Automated Efficient Estimation using Monte Carlo Efficient Influence Functions

    [https://arxiv.org/abs/2403.00158](https://arxiv.org/abs/2403.00158)

    该论文提出了一种名为蒙特卡洛高效影响函数（MC-EIF）的全自动技术，用于近似高效影响函数，能够实现针对广泛类别的模型和目标函数的统计估计，达到最优的收敛速度。

    

    许多实际问题涉及使用高维模型和数据集估计低维统计量。几种方法基于影响函数理论来解决这些估计任务，例如去偏/双重极大似然或有针对性最小损失估计。本文介绍了一种全自动的技术，即\textit{蒙特卡洛高效影响函数} (MC-EIF)，用于逼近高效影响函数，与现有的可微概率编程系统无缝集成。MC-EIF可自动化广泛类别的模型和目标函数的高效统计估计，这些估计以前需要严格的自定义分析。我们证明MC-EIF是一致的，并且使用MC-EIF的估计器达到了最优的$\sqrt{N}$收敛速度。我们经验性地展示，使用MC-EIF的估计器与使用解析EIF的估计器相当。最后，我们展示一个新颖的顶点示例

    arXiv:2403.00158v1 Announce Type: cross  Abstract: Many practical problems involve estimating low dimensional statistical quantities with high-dimensional models and datasets. Several approaches address these estimation tasks based on the theory of influence functions, such as debiased/double ML or targeted minimum loss estimation. This paper introduces \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully automated technique for approximating efficient influence functions that integrates seamlessly with existing differentiable probabilistic programming systems. MC-EIF automates efficient statistical estimation for a broad class of models and target functionals that would previously require rigorous custom analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF achieve optimal $\sqrt{N}$ convergence rates. We show empirically that estimators using MC-EIF are at parity with estimators using analytic EIFs. Finally, we demonstrate a novel capstone exa
    
[^80]: 隐私保护的分布式优化与学习

    Privacy-Preserving Distributed Optimization and Learning

    [https://arxiv.org/abs/2403.00157](https://arxiv.org/abs/2403.00157)

    差分隐私是最有前景的隐私保护方法，适用于现代学习应用中的高维优化，具有低的计算和通信复杂性。

    

    最近，由于在传感器网络、智能电网、机器学习等领域的广泛应用，分布式优化和学习引起了广泛关注。尽管分布式优化和学习算法迅速发展，但现有算法要求每个代理与其邻居交换消息，这可能会泄露敏感信息并引起重大隐私问题。在这篇综述性文章中，我们概述了隐私保护的分布式优化和学习方法。我们首先讨论了密码学、差分隐私和其他可用于隐私保护的技术，并指出它们在分布式优化和学习中的优缺点。我们认为，在这些方法中，差分隐私是最具有前景的，因为它具有低计算和通信复杂性，这对于具有高维优化的现代学习应用极具吸引力。

    arXiv:2403.00157v1 Announce Type: new  Abstract: Distributed optimization and learning has recently garnered great attention due to its wide applications in sensor networks, smart grids, machine learning, and so forth. Despite rapid development, existing distributed optimization and learning algorithms require each agent to exchange messages with its neighbors, which may expose sensitive information and raise significant privacy concerns. In this survey paper, we overview privacy-preserving distributed optimization and learning methods. We first discuss cryptography, differential privacy, and other techniques that can be used for privacy preservation and indicate their pros and cons for privacy protection in distributed optimization and learning. We believe that among these approaches, differential privacy is most promising due to its low computational and communication complexities, which are extremely appealing for modern learning based applications with high dimensions of optimizati
    
[^81]: 通过概率潜在空间解释深度神经网络压缩

    Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space

    [https://arxiv.org/abs/2403.00155](https://arxiv.org/abs/2403.00155)

    通过概率潜在空间提出了一个新的理论框架，解释了深度神经网络压缩的优化网络稀疏度，并探讨了网络层的AP3/AP2属性与性能之间的关系。

    

    尽管深度神经网络（DNNs）表现出色，但它们的计算复杂性和存储空间消耗导致了网络压缩的概念。尽管已广泛研究了诸如修剪和低秩分解等DNN压缩技术，但对它们的理论解释仍未受到足够关注。本文提出了一个利用DNN权重的概率潜在空间并利用信息理论分歧度量解释最佳网络稀疏性的新理论框架。我们为DNN引入了新的类比投影模式（AP2）和概率中的类比投影模式（AP3）概念，并证明网络中层的AP3/AP2特性与其性能之间存在关系。此外，我们提供了一个理论分析，解释了压缩网络的训练过程。这些理论结果是从实证实验

    arXiv:2403.00155v1 Announce Type: new  Abstract: Despite the impressive performance of deep neural networks (DNNs), their computational complexity and storage space consumption have led to the concept of network compression. While DNN compression techniques such as pruning and low-rank decomposition have been extensively studied, there has been insufficient attention paid to their theoretical explanation. In this paper, we propose a novel theoretical framework that leverages a probabilistic latent space of DNN weights and explains the optimal network sparsity by using the information-theoretic divergence measures. We introduce new analogous projected patterns (AP2) and analogous-in-probability projected patterns (AP3) notions for DNNs and prove that there exists a relationship between AP3/AP2 property of layers in the network and its performance. Further, we provide a theoretical analysis that explains the training process of the compressed network. The theoretical results are empirica
    
[^82]: 核镜像Prox用于度量优化的分析

    Analysis of Kernel Mirror Prox for Measure Optimization

    [https://arxiv.org/abs/2403.00147](https://arxiv.org/abs/2403.00147)

    该论文研究了核镜像Prox算法用于度量优化问题的收敛性，提出了一种针对混合功能纳什均衡问题的算法，并在无限维空间中证明了其收敛速度为$O(1/N)。

    

    通过选择适当的函数空间作为非负度量锥的对偶，我们在一个统一的框架下研究了一类功能鞍点优化问题，我们将其称为混合功能纳什均衡(MFNE)，这是一些现有机器学习算法的基础，如隐式生成模型、分布鲁棒优化(DRO)和Wasserstein质心。 当函数空间选择为再生核希尔伯特空间(RKHS)时，我们将鞍点优化动态建模为相互作用的Fisher-Rao-RKHS梯度流。作为离散时间对应物，我们提出了一种原始-对偶核镜像Prox(KMP)算法，该算法在RKHS中使用对偶步骤和原始熵镜像Prox步骤。 然后，我们在无限维设置下为这类MFNE问题提供了KMP的统一收敛分析，这在确定性情况下建立了一个$O(1/N)$的收敛速度。

    arXiv:2403.00147v1 Announce Type: cross  Abstract: By choosing a suitable function space as the dual to the non-negative measure cone, we study in a unified framework a class of functional saddle-point optimization problems, which we term the Mixed Functional Nash Equilibrium (MFNE), that underlies several existing machine learning algorithms, such as implicit generative models, distributionally robust optimization (DRO), and Wasserstein barycenters. We model the saddle-point optimization dynamics as an interacting Fisher-Rao-RKHS gradient flow when the function space is chosen as a reproducing kernel Hilbert space (RKHS). As a discrete time counterpart, we propose a primal-dual kernel mirror prox (KMP) algorithm, which uses a dual step in the RKHS, and a primal entropic mirror prox step. We then provide a unified convergence analysis of KMP in an infinite-dimensional setting for this class of MFNE problems, which establishes a convergence rate of $O(1/N)$ in the deterministic case and
    
[^83]: EBBS: 一个具有双层束搜索的集成方法用于零翻译机器翻译

    EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation

    [https://arxiv.org/abs/2403.00144](https://arxiv.org/abs/2403.00144)

    提出了一种集成方法EBBS，配合新颖的双层束搜索算法，能够优于直接和通过第三语言进行的翻译，并实现知识蒸馏来提高推理效率。

    

    当我们用特定的翻译方向训练多语言模型时，零翻译的能力就会出现；模型可以直接在未见过的方向进行翻译。另外，零翻译也可以通过第三种语言（例如英语）来实现。在我们的工作中，我们发现直接和通过第三种语言进行的翻译都存在噪音，并且表现不尽如人意。我们提出了EBBS，一个具有新颖的双层束搜索算法的集成方法，其中每个集成组件在下层逐步探索自己的预测，但它们通过上层的“软投票”机制进行同步。在两个流行的多语言翻译数据集上的结果表明，EBBS始终优于直接和通过第三种语言进行的翻译，以及现有的集成技术。此外，我们可以将集成的知识传回到多语言模型中，以提高推理效率；值得注意的是，我们的E

    arXiv:2403.00144v1 Announce Type: cross  Abstract: The ability of zero-shot translation emerges when we train a multilingual model with certain translation directions; the model can then directly translate in unseen directions. Alternatively, zero-shot translation can be accomplished by pivoting through a third language (e.g., English). In our work, we observe that both direct and pivot translations are noisy and achieve less satisfactory performance. We propose EBBS, an ensemble method with a novel bi-level beam search algorithm, where each ensemble component explores its own prediction step by step at the lower level but they are synchronized by a "soft voting" mechanism at the upper level. Results on two popular multilingual translation datasets show that EBBS consistently outperforms direct and pivot translations as well as existing ensemble techniques. Further, we can distill the ensemble's knowledge back to the multilingual model to improve inference efficiency; profoundly, our E
    
[^84]: 基于集成的无监督不连续成分句法分析：树平均法

    Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging

    [https://arxiv.org/abs/2403.00143](https://arxiv.org/abs/2403.00143)

    通过树平均法构建集成解析器，稳定并提升无监督不连续成分句法分析性能，实验结果表明该方法在所有指标上均优于基准线

    

    我们解决了无监督不连续成分句法分析的问题，在这个问题中我们观察到先前唯一模型的性能存在高方差。我们提出通过对现有不连续解析器的不同运行构建一个集成，并通过平均预测树来稳定和提升性能。首先，我们针对不同的二元性和连续性设置提供了全面的树平均计算复杂度分析（以P和NP完全为单位）。然后，我们开发了一种高效的精确算法来处理这一任务，在我们的实验中对所有样本运行时间均合理。在三个数据集上的结果显示我们的方法在所有指标上均优于所有基准线，我们还对我们的方法进行了深入分析。

    arXiv:2403.00143v1 Announce Type: cross  Abstract: We address unsupervised discontinuous constituency parsing, where we observe a high variance in the performance of the only previous model. We propose to build an ensemble of different runs of the existing discontinuous parser by averaging the predicted trees, to stabilize and boost performance. To begin with, we provide comprehensive computational complexity analysis (in terms of P and NP-complete) for tree averaging under different setups of binarity and continuity. We then develop an efficient exact algorithm to tackle the task, which runs in a reasonable time for all samples in our experiments. Results on three datasets show our method outperforms all baselines in all metrics; we also provide in-depth analyses of our approach.
    
[^85]: UniTS: 构建统一的时间序列模型

    UniTS: Building a Unified Time Series Model

    [https://arxiv.org/abs/2403.00131](https://arxiv.org/abs/2403.00131)

    UNITS是一种统一的时间序列模型，通过独特的统一网络骨干实现了通用任务规范，并成功支持多种任务，包括分类、预测、插补和异常检测。

    

    基础模型，特别是LLMs，正在深度学习中产生深远影响。我们可以通过少量提示或微调将单个预训练模型适应于许多任务，而不是训练许多特定任务的模型。然而，当前的基础模型适用于序列数据，但不适用于时间序列，因为时间序列具有独特的挑战，包括固有多样性和多领域时间序列数据集，预测、分类和其他类型任务之间的任务规范分歧，以及对任务专用模型的明显需求。我们开发了UNITS，一种支持通用任务规范的统一时间序列模型，可容纳分类、预测、插补和异常检测任务。这是通过一种新颖的统一网络骨干实现的，该骨干结合了序列和变量注意力以及动态线性算子，并作为统一模型进行训练。在38个多领域数据集上，UNITS展示

    arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate
    
[^86]: 从蝇类到机器人：小型四轴飞行器进行动态停泊逆向降落

    From Flies to Robots: Inverted Landing in Small Quadcopters with Dynamic Perching

    [https://arxiv.org/abs/2403.00128](https://arxiv.org/abs/2403.00128)

    通过模仿蝇类的颠倒降落行为，开发了适用于小型四轴飞行器的控制策略，使用强化学习来优化在不同天花板接近条件下的动态停泊。

    

    颠倒降落是许多飞行动物常见的行为。然而，对于机器飞行器来说，掌握这项技能尤为具有挑战性，特别是要执行快速身体旋转（或翻转）并在重力作用下着陆的动态停泊。蝇类的颠倒降落表明，光流感应与精确触发和控制导致各种成功着陆行为的身体翻转是密切相关的。在这一基础上，我们旨在通过开发一个适用于任意天花板接近条件的控制策略，来复制蝇类的着陆行为在小型四轴飞行器上。首先，我们在模拟环境中采用强化学习来优化整个光流空间的多个天花板接近速度和方向下的离散感知-运动对。接下来，我们将感知-运动对转换为在连续增强光流空间中的两阶段控制策略。这个控制策略包含了

    arXiv:2403.00128v1 Announce Type: cross  Abstract: Inverted landing is a routine behavior among a number of animal fliers. However, mastering this feat poses a considerable challenge for robotic fliers, especially to perform dynamic perching with rapid body rotations (or flips) and landing against gravity. Inverted landing in flies have suggested that optical flow senses are closely linked to the precise triggering and control of body flips that lead to a variety of successful landing behaviors. Building upon this knowledge, we aimed to replicate the flies' landing behaviors in small quadcopters by developing a control policy general to arbitrary ceiling-approach conditions. First, we employed reinforcement learning in simulation to optimize discrete sensory-motor pairs across a broad spectrum of ceiling-approach velocities and directions. Next, we converted the sensory-motor pairs to a two-stage control policy in a continuous augmented-optical flow space. The control policy consists o
    
[^87]: 具有异构客户的联邦线性上下文赌臂

    Federated Linear Contextual Bandits with Heterogeneous Clients

    [https://arxiv.org/abs/2403.00116](https://arxiv.org/abs/2403.00116)

    提出了一种适用于异构客户的联邦赌臂学习方法，通过在联邦学习设置下为客户进行聚类，实现了非平凡次线性后悔和通信成本的优化

    

    由于分布式系统产生的数据量不断增加，对跨多个代理进行协同和私密的赌臂学习的需求正在增长。联邦赌臂学习已经成为一种有前途的框架，用于私密、高效和去中心化的在线学习。然而，几乎所有先前的工作都依赖于客户同质性的强假设，即所有参与客户都应共享相同的赌臂模型；否则，它们将遭受线性后悔。这严重限制了联邦赌臂学习在实践中的应用。在这项工作中，我们为异构客户引入了一种新的联邦赌臂方法，该方法在联邦学习环境下对客户进行聚类，用于协同赌臂学习。我们提出的算法实现了对所有客户而言的非平凡次线性后悔和通信成本，符合联邦学习中的通信协议，在任何时候只有一个模型

    arXiv:2403.00116v1 Announce Type: cross  Abstract: The demand for collaborative and private bandit learning across multiple agents is surging due to the growing quantity of data generated from distributed systems. Federated bandit learning has emerged as a promising framework for private, efficient, and decentralized online learning. However, almost all previous works rely on strong assumptions of client homogeneity, i.e., all participating clients shall share the same bandit model; otherwise, they all would suffer linear regret. This greatly restricts the application of federated bandit learning in practice. In this work, we introduce a new approach for federated bandits for heterogeneous clients, which clusters clients for collaborative bandit learning under the federated learning setting. Our proposed algorithm achieves non-trivial sub-linear regret and communication cost for all clients, subject to the communication protocol under federated learning that at anytime only one model c
    
[^88]: 纵向反事实：限制与机会

    Longitudinal Counterfactuals: Constraints and Opportunities

    [https://arxiv.org/abs/2403.00105](https://arxiv.org/abs/2403.00105)

    本文提出使用纵向数据来评估和改善反事实中的可信度，并开发了一种比较纵向变化与反事实差异的度量标准，从而生成可信的反事实。

    

    反事实解释是一种为数据主体提供补救措施的常见方法。然而，当前的方法可能会产生数据主体无法实现的反事实，使得在实践中难以证明反事实适用于补救。尽管在使用反事实进行算法补救时，认为可信度是一种重要品质，但地面真实可信度仍然难以量化。本文提出使用纵向数据来评估和改善反事实中的可信度。具体来说，我们开发了一种度量标准，比较了纵向差异与反事实差异，从而允许我们评估一个反事实与先前观察到的变化有多相似。此外，我们使用该度量标准生成可信的反事实。最后，我们讨论了使用反事实进行补救的固有困难。

    arXiv:2403.00105v1 Announce Type: new  Abstract: Counterfactual explanations are a common approach to providing recourse to data subjects. However, current methodology can produce counterfactuals that cannot be achieved by the subject, making the use of counterfactuals for recourse difficult to justify in practice. Though there is agreement that plausibility is an important quality when using counterfactuals for algorithmic recourse, ground truth plausibility continues to be difficult to quantify. In this paper, we propose using longitudinal data to assess and improve plausibility in counterfactuals. In particular, we develop a metric that compares longitudinal differences to counterfactual differences, allowing us to evaluate how similar a counterfactual is to prior observed changes. Furthermore, we use this metric to generate plausible counterfactuals. Finally, we discuss some of the inherent difficulties of using counterfactuals for recourse.
    
[^89]: 对基于机器学习的拥塞预测器的健壮性和泛化性的研究

    On Robustness and Generalization of ML-Based Congestion Predictors to Valid and Imperceptible Perturbations

    [https://arxiv.org/abs/2403.00103](https://arxiv.org/abs/2403.00103)

    本研究研究了基于机器学习的EDA工具中的健壮性问题，提出了针对VLSI布局的新颖不可察觉性概念。

    

    在电子计算机辅助设计(CAD)流程中，对基于机器学习（ML）技术的使用引起了极大的兴趣，尤其是基于深度学习的方法。然而，尽管深度学习方法在多个应用中取得了最先进的性能，最近的研究表明神经网络通常对其输入的微小、精心选择的扰动（例如图像中的单个像素更改）很容易受攻击。在这项工作中，我们探讨了ML-based EDA工具的健壮性问题--尤其是对拥塞预测。据我们所知，我们是第一个在ML-based EDA背景下探索这个概念的团队。我们首先描述了一个针对VLSI布局问题特别设计的不可察觉性概念，该问题定义在元件列表和单元放置上。我们的不可察觉性定义的特点是保证布局的扰动不会改变其

    arXiv:2403.00103v1 Announce Type: new  Abstract: There is substantial interest in the use of machine learning (ML)-based techniques throughout the electronic computer-aided design (CAD) flow, particularly methods based on deep learning. However, while deep learning methods have achieved state-of-the-art performance in several applications, recent work has demonstrated that neural networks are generally vulnerable to small, carefully chosen perturbations of their input (e.g. a single pixel change in an image). In this work, we investigate robustness in the context of ML-based EDA tools -- particularly for congestion prediction. As far as we are aware, we are the first to explore this concept in the context of ML-based EDA.   We first describe a novel notion of imperceptibility designed specifically for VLSI layout problems defined on netlists and cell placements. Our definition of imperceptibility is characterized by a guarantee that a perturbation to a layout will not alter its global 
    
[^90]: 通过随机梯度MCMC扩展动态边缘划分模型

    Scaling up Dynamic Edge Partition Models via Stochastic Gradient MCMC

    [https://arxiv.org/abs/2403.00044](https://arxiv.org/abs/2403.00044)

    该论文通过引入Dirichlet先验规范和Dirichlet马尔可夫链构建，扩展了边缘划分模型（EPM）以适应动态环境，并提出了一个简单的Gibbs采样器来处理后验计算。

    

    arXiv:2403.00044v1宣布类型:cross 摘要: 边缘划分模型(EPM)是一种从静态图结构数据中提取重叠社区结构的生成模型。 在EPM中，采用Gamma过程（GaP）先验来推断合适的潜在社区数量，并且每个顶点被赋予一个Gamma分布的正成员向量。 尽管具有许多吸引人的特性，EPM中的推理通常使用马尔可夫链蒙特卡罗（MCMC）方法执行，这阻止了其应用于大规模网络数据。 在本文中，我们通过使用Dirichlet先验规范表示每个顶点的正成员向量来将EPM泛化以考虑动态环境，并通过Dirichlet马尔可夫链构造捕获顶点的时间演变行为。 提出了一种简单实施的Gibbs采样器，使用负二项增强技术执行后验计算。 对于大网络资料

    arXiv:2403.00044v1 Announce Type: cross  Abstract: The edge partition model (EPM) is a generative model for extracting an overlapping community structure from static graph-structured data. In the EPM, the gamma process (GaP) prior is adopted to infer the appropriate number of latent communities, and each vertex is endowed with a gamma distributed positive memberships vector. Despite having many attractive properties, inference in the EPM is typically performed using Markov chain Monte Carlo (MCMC) methods that prevent it from being applied to massive network data. In this paper, we generalize the EPM to account for dynamic enviroment by representing each vertex with a positive memberships vector constructed using Dirichlet prior specification, and capturing the time-evolving behaviour of vertices via a Dirichlet Markov chain construction. A simple-to-implement Gibbs sampler is proposed to perform posterior computation using Negative- Binomial augmentation technique. For large network d
    
[^91]: 通过最优输运实现全局和本地提示的合作，用于联邦学习

    Global and Local Prompts Cooperation via Optimal Transport for Federated Learning

    [https://arxiv.org/abs/2403.00041](https://arxiv.org/abs/2403.00041)

    提出了联邦提示合作 via Optimal Transport（FedOTP）方法，通过最优输运实现全局和本地提示的合作，针对数据异质性设计了高效的协作提示学习策略。

    

    预训练的视觉-语言模型中的提示学习在各种下游任务中表现出了卓越的灵活性。最近的研究尝试将这种强大的预训练模型整合到联邦学习框架中，以同时降低通信成本并促进对数据不足的局部训练。为了应对当前联邦提示学习方法在系统化解决严重的数据异质性方面的不足，即涉及标签和特征转移的数据分布，我们提出了通过最优输运实现联邦提示合作（FedOTP），它引入了高效的协作提示学习策略，以在每个客户端基础上捕捉不同的类别特征。具体而言，对于每个客户端，我们学习一个全局提示来提取客户端之间的共识知识，还学习一个本地提示来捕获特定客户端的特征。

    arXiv:2403.00041v1 Announce Type: cross  Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific
    
[^92]: 影响Bandits：用于形塑偏好的手臂选择

    Influencing Bandits: Arm Selection for Preference Shaping

    [https://arxiv.org/abs/2403.00036](https://arxiv.org/abs/2403.00036)

    该论文考虑了在非静态多臂赌博机中，通过观察奖励来积极和消极地强化人群偏好，并提出了用于最大化支持预定手臂的人口比例的算法。对于不同意见动态，提出了不同的策略并分析了后悔，最后讨论了多个推荐系统共存的情况。

    

    我们考虑一个非静态多臂赌博机，在这其中人群的偏好受到观察到的奖励的积极和消极强化。算法的目标是塑造人群的偏好，以最大化支持预定手臂的人口比例。对于二元意见的情况，考虑了两种意见动态 -- 递减弹性（建模为具有增加球数的Polya采样）和常量弹性（使用投票者模型）。对于第一种情况，我们描述了一种探索-然后-承诺策略和一种Thompson采样策略，并分析了每种策略的后悔。然后，我们展示了这些算法及其分析可推广到常弹性情况。我们还描述了一种基于Thompson采样的算法，用于当存在两种以上类型的意见情况。最后，我们讨论了存在多个推荐系统的情况引发的情况。

    arXiv:2403.00036v1 Announce Type: cross  Abstract: We consider a non stationary multi-armed bandit in which the population preferences are positively and negatively reinforced by the observed rewards. The objective of the algorithm is to shape the population preferences to maximize the fraction of the population favouring a predetermined arm. For the case of binary opinions, two types of opinion dynamics are considered -- decreasing elasticity (modeled as a Polya urn with increasing number of balls) and constant elasticity (using the voter model). For the first case, we describe an Explore-then-commit policy and a Thompson sampling policy and analyse the regret for each of these policies. We then show that these algorithms and their analyses carry over to the constant elasticity case. We also describe a Thompson sampling based algorithm for the case when more than two types of opinions are present. Finally, we discuss the case where presence of multiple recommendation systems gives ris
    
[^93]: 通过高阶注意力大脑网络分析大麻使用者的静息态fMRI数据

    Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network

    [https://arxiv.org/abs/2403.00033](https://arxiv.org/abs/2403.00033)

    通过结合动态内在功能网络和LSTM技术，使用高阶注意力模块进行信息融合和消息传递，提出了HOGAB模型，对慢性大麻用户的静息态fMRI数据进行分析，提高了多图分类的准确性。

    

    大麻的持续使用明显影响人们的生活和健康。在这项研究中，我们提出了一个可解释的新框架，命名为HOGAB（High-Order Attention Graph Attention神经网络）模型，以分析两个数据集中慢性大麻用户的局部异常脑活动。HOGAB将动态内在功能网络与LSTM技术相结合，捕捉大麻用户fMRI时间序列中的时间模式。此外，我们使用高阶注意力模块来对邻域节点进行信息融合和消息传递，增强长期大麻用户的社区聚类分析。此外，我们通过融入注意力机制提高了模型的整体学习能力，在多图分类中实现了85.1%的AUC和80.7%的准确性。此外，我们比较了线性机器学习方法，并评估了我们提出的HODAB模型的有效性。

    arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
    
[^94]: GraphPub: 具有高可用性的差分隐私图生成

    GraphPub: Generation of Differential Privacy Graph with High Availability

    [https://arxiv.org/abs/2403.00030](https://arxiv.org/abs/2403.00030)

    提出了一种名为GraphPub的新型图边保护框架，通过反向学习和编码器-解码器机制，在保护图拓扑结构的同时保证数据的可用性基本不变。

    

    近年来，随着图神经网络（GNN）的快速发展，越来越多的图数据集被用于GNN任务。然而，当上游数据所有者发布图数据时，往往会存在许多隐私问题，因为许多现实世界的图数据包含像个人的朋友列表等敏感信息。差分隐私（DP）是一种常用的保护隐私的方法，但由于图数据的复杂拓扑结构，将DP应用在图上往往会影响GNN模型的消息传递和聚合，导致模型准确性下降。本文提出了一种新颖的图边保护框架GraphPub，可以保护图拓扑结构同时确保数据的可用性基本不变。通过反向学习和编码器-解码器机制，我们搜索一些对节点特征聚合没有太大负面影响的虚假边。

    arXiv:2403.00030v1 Announce Type: cross  Abstract: In recent years, with the rapid development of graph neural networks (GNN), more and more graph datasets have been published for GNN tasks. However, when an upstream data owner publishes graph data, there are often many privacy concerns, because many real-world graph data contain sensitive information like person's friend list. Differential privacy (DP) is a common method to protect privacy, but due to the complex topological structure of graph data, applying DP on graphs often affects the message passing and aggregation of GNN models, leading to a decrease in model accuracy. In this paper, we propose a novel graph edge protection framework, graph publisher (GraphPub), which can protect graph topology while ensuring that the availability of data is basically unchanged. Through reverse learning and the encoder-decoder mechanism, we search for some false edges that do not have a large negative impact on the aggregation of node features, 
    
[^95]: 在持续观测和在线阈值查询下的差分隐私下界

    Lower Bounds for Differential Privacy Under Continual Observation and Online Threshold Queries

    [https://arxiv.org/abs/2403.00028](https://arxiv.org/abs/2403.00028)

    该论文展示了针对差分隐私在持续观测和在线阈值查询情形下关于时间步长和事件数量的新下界结果。

    

    这是翻译过的论文摘要。在这个问题中，我们的目标是在隐瞒每一事件的存在的同时，跟踪随时间发生的事件数量。具体地，在每个时间步长t∈[T]中，我们在线学习到Δt≥0个新事件已发生，并且必须回应一个估计值nt≈∑_{j=1}^t Δj。隐私要求是，所有输出在所有时间步长上一起满足事件级别的差分隐私。主要问题是我们的误差如何依赖于总时间步长T和总事件数量n。Dwork等人（2015）展示了O(log(T)+log^2(n))的上界，而Henzinger等人（2023）展示了Ω(min{log n, log T})的下界。我们展示了一个新的下界...

    arXiv:2403.00028v1 Announce Type: cross  Abstract: One of the most basic problems for studying the "price of privacy over time" is the so called private counter problem, introduced by Dwork et al. (2010) and Chan et al. (2010). In this problem, we aim to track the number of events that occur over time, while hiding the existence of every single event. More specifically, in every time step $t\in[T]$ we learn (in an online fashion) that $\Delta_t\geq 0$ new events have occurred, and must respond with an estimate $n_t\approx\sum_{j=1}^t \Delta_j$. The privacy requirement is that all of the outputs together, across all time steps, satisfy event level differential privacy. The main question here is how our error needs to depend on the total number of time steps $T$ and the total number of events $n$. Dwork et al. (2015) showed an upper bound of $O\left(\log(T)+\log^2(n)\right)$, and Henzinger et al. (2023) showed a lower bound of $\Omega\left(\min\{\log n, \log T\}\right)$. We show a new lo
    
[^96]: 评估复杂网络最差稳健性的快速框架

    A Quick Framework for Evaluating Worst Robustness of Complex Networks

    [https://arxiv.org/abs/2403.00027](https://arxiv.org/abs/2403.00027)

    研究引入了Most Destruction Attack（MDA）概念，用于评估网络的最差稳健性，以确定在最坏情况下系统的极限鲁棒性。

    

    鲁棒性对于理解、设计、优化和修复网络至关重要，模拟攻击一直是主流的评估方法。然而，模拟攻击往往耗时甚至不切实际，更为关键且持续被忽视的缺点是任何攻击策略仅提供了一个潜在的瓦解范式。关键问题是：在最坏情况下或面临最严重的攻击时，对于给定系统，“最差稳健性”，即所谓的极限鲁棒性是多少？了解系统的最差稳健性对于把握其可靠性极限、准确评估保护能力以及确定相关设计和安全维护成本至关重要。为了解决这些挑战，我们引入了基于知识堆叠思想的“最大破坏攻击”（MDA）的概念。MDA用于评估网络的最差稳健性，随后进行

    arXiv:2403.00027v1 Announce Type: cross  Abstract: Robustness is pivotal for comprehending, designing, optimizing, and rehabilitating networks, with simulation attacks being the prevailing evaluation method. Simulation attacks are often time-consuming or even impractical, however, a more crucial yet persistently overlooked drawback is that any attack strategy merely provides a potential paradigm of disintegration. The key concern is: in the worst-case scenario or facing the most severe attacks, what is the limit of robustness, referred to as ``Worst Robustness'', for a given system? Understanding a system's worst robustness is imperative for grasping its reliability limits, accurately evaluating protective capabilities, and determining associated design and security maintenance costs. To address these challenges, we introduce the concept of Most Destruction Attack (MDA) based on the idea of knowledge stacking. MDA is employed to assess the worst robustness of networks, followed by the 
    
[^97]: 学习交付：蒙特利尔容量车辆路径问题的基础模型

    Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem

    [https://arxiv.org/abs/2403.00026](https://arxiv.org/abs/2403.00026)

    提出了蒙特利尔容量车辆路径问题的基础模型（FM-MCVRP），将MCVRP视为类似自然语言处理任务，并利用Transformer架构嵌入大型语言模型框架进行训练。

    

    在本文中，我们提出了蒙特利尔容量车辆路径问题（MCVRP）的基础模型（FM-MCVRP），这是一个新颖的深度学习（DL）模型，用于近似解决一个变体的容量车辆路径问题（CVRP），该问题描述了许多现实世界的应用场景。MCVRP首次由Bengio等人（2021）正式描述，定义在一个固定有限的图上，类似于一个城市。每个MCVRP实例本质上是连接固定图中随机抽样节点的子图，这些节点代表给定日期实际交付问题中潜在地址集合。我们利用这个问题结构，将MCVRP构建成类似的自然语言处理（NLP）任务。具体来说，我们利用Transformer架构嵌入到大型语言模型（LLM）框架中，来训练我们的模型。

    arXiv:2403.00026v1 Announce Type: cross  Abstract: In this paper, we present the Foundation Model for the Montreal Capacitated Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that approximates high-quality solutions to a variant of the Capacitated Vehicle Routing Problem (CVRP) that characterizes many real-world applications. The so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally described by Bengio et al. (2021), is defined on a fixed and finite graph, which is analogous to a city. Each MCVRP instance is essentially the sub-graph connecting a randomly sampled subset of the nodes in the fixed graph, which represent a set of potential addresses in a real-world delivery problem on a given day. Our work exploits this problem structure to frame the MCVRP as an analogous Natural Language Processing (NLP) task. Specifically, we leverage a Transformer architecture embedded in a Large Language Model (LLM) framework to train our model in a superv
    
[^98]: 关于生成人工智能中的挑战与机遇

    On the Challenges and Opportunities in Generative AI

    [https://arxiv.org/abs/2403.00025](https://arxiv.org/abs/2403.00025)

    现代生成人工智能范例中存在关键的未解决挑战，如何解决这些挑战将进一步增强它们的能力、多功能性和可靠性，并为研究方向提供有价值的见解。

    

    深度生成建模领域近年来增长迅速而稳定。随着海量训练数据的可用性以及可扩展的无监督学习范式的进步，最近的大规模生成模型展现出合成高分辨率图像和文本以及结构化数据（如视频和分子）的巨大潜力。然而，我们认为当前大规模生成人工智能模型没有充分解决若干基本问题，限制了它们在各个领域的广泛应用。在本工作中，我们旨在确定现代生成人工智能范例中的关键未解决挑战，以进一步增强它们的能力、多功能性和可靠性。通过识别这些挑战，我们旨在为研究人员提供有价值的见解，探索有益的研究方向，从而促进更加强大和可访问的生成人工智能的发展。

    arXiv:2403.00025v1 Announce Type: cross  Abstract: The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI so
    
[^99]: FlowCyt: 流式细胞术中多类别分类的深度学习方法比较研究

    FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class Classification in Flow Cytometry Benchmarking

    [https://arxiv.org/abs/2403.00024](https://arxiv.org/abs/2403.00024)

    本文介绍了FlowCyt项目，提出了针对流式细胞术数据中多类别单细胞分类的全面基准测试，其中图神经网络（GNNs）通过利用图编码数据中的空间关系展现出卓越的性能。

    

    本文介绍了FlowCyt，这是第一个针对流式细胞术数据中多类别单细胞分类的全面基准测试。数据集包括来自30名患者的骨髓样本，每个细胞由十二个标记特征。地面真实标签识别了五种血液细胞类型：T淋巴细胞、B淋巴细胞、单核细胞、肥大细胞和造血干/祖细胞（HSPCs）。实验利用了有监督的归纳学习和半监督的转导学习，每名患者最多使用了100万个细胞。基线方法包括高斯混合模型、XGBoost、随机森林、深度神经网络和图神经网络（GNNs）。GNNs通过利用图编码数据中的空间关系展现出卓越的性能。该基准测试允许标准化评估临床相关的分类任务，以及探索性分析以获得对血液细胞表型的洞察。这代表了一项重要的工作。

    arXiv:2403.00024v1 Announce Type: new  Abstract: This paper presents FlowCyt, the first comprehensive benchmark for multi-class single-cell classification in flow cytometry data. The dataset comprises bone marrow samples from 30 patients, with each cell characterized by twelve markers. Ground truth labels identify five hematological cell types: T lymphocytes, B lymphocytes, Monocytes, Mast cells, and Hematopoietic Stem/Progenitor Cells (HSPCs). Experiments utilize supervised inductive learning and semi-supervised transductive learning on up to 1 million cells per patient. Baseline methods include Gaussian Mixture Models, XGBoost, Random Forests, Deep Neural Networks, and Graph Neural Networks (GNNs). GNNs demonstrate superior performance by exploiting spatial relationships in graph-encoded data. The benchmark allows standardized evaluation of clinically relevant classification tasks, along with exploratory analyses to gain insights into hematological cell phenotypes. This represents th
    
[^100]: 具有基于属性的差分隐私的可审计同态协作人工智能

    Auditable Homomorphic-based Decentralized Collaborative AI with Attribute-based Differential Privacy

    [https://arxiv.org/abs/2403.00023](https://arxiv.org/abs/2403.00023)

    提出了一种名为AerisAI的可审计同态协作人工智能框架，利用同态加密和细粒度差分隐私提高安全性，通过基于区块链的智能合约直接聚合加密参数，消除了对于模型性能的负面影响

    

    在最近几年，联邦学习（FL）的概念引领了具有隐私保护的分布式人工智能（AI）新范式。然而，大多数当前的FL系统因为需要受信任的第三方而存在数据隐私问题。尽管一些先前的工作引入差分隐私来保护数据，但这可能会显著恶化模型性能。为了解决这些问题，我们提出了一种新颖的去中心化协作AI框架，名为具有基于同态加密和细粒度差分隐私的可审计的分布式协作AI（AerisAI）。我们提出的AerisAI直接使用基于区块链的智能合约来聚合加密参数，摆脱了需要受信任的第三方的需求。我们还提出了一个全新的概念，用于消除差分隐私对于模型性能的负面影响。

    arXiv:2403.00023v1 Announce Type: cross  Abstract: In recent years, the notion of federated learning (FL) has led to the new paradigm of distributed artificial intelligence (AI) with privacy preservation. However, most current FL systems suffer from data privacy issues due to the requirement of a trusted third party. Although some previous works introduce differential privacy to protect the data, however, it may also significantly deteriorate the model performance. To address these issues, we propose a novel decentralized collaborative AI framework, named Auditable Homomorphic-based Decentralised Collaborative AI (AerisAI), to improve security with homomorphic encryption and fine-grained differential privacy. Our proposed AerisAI directly aggregates the encrypted parameters with a blockchain-based smart contract to get rid of the need of a trusted third party. We also propose a brand-new concept for eliminating the negative impacts of differential privacy for model performance. Moreove
    
[^101]: 基于Transformer的统计学参数估计

    Transformer-based Parameter Estimation in Statistics

    [https://arxiv.org/abs/2403.00019](https://arxiv.org/abs/2403.00019)

    提出了一种基于Transformer的参数估计方法，相较于传统方法，不需要封闭形式解或数学推导，也不需要概率密度函数，仅需经过训练的Transformer模型进行一次推断即可估计潜在分布的参数。

    

    参数估计是统计学中最重要的任务之一，有助于帮助人们理解观测样本背后的分布。传统上，参数估计通过封闭形式解（例如，高斯分布的最大似然估计）或通过迭代数值方法（例如，Beta分布时封闭形式解不存在的情况下使用Newton-Raphson方法）来完成。本文提出了基于Transformer的参数估计方法。与现有解决方案相比，我们的方法不需要封闭形式解或任何数学推导。它甚至不需要知道概率密度函数，这是数值方法所需的。训练了Transformer模型后，仅需要进行一次推断，即可基于观测样本估计潜在分布的参数。在实证研究中，我们比较了我们的方法。

    arXiv:2403.00019v1 Announce Type: new  Abstract: Parameter estimation is one of the most important tasks in statistics, and is key to helping people understand the distribution behind a sample of observations. Traditionally parameter estimation is done either by closed-form solutions (e.g., maximum likelihood estimation for Gaussian distribution), or by iterative numerical methods such as Newton-Raphson method when closed-form solution does not exist (e.g., for Beta distribution).   In this paper we propose a transformer-based approach to parameter estimation. Compared with existing solutions, our approach does not require a closed-form solution or any mathematical derivations. It does not even require knowing the probability density function, which is needed by numerical methods. After the transformer model is trained, only a single inference is needed to estimate the parameters of the underlying distribution based on a sample of observations. In the empirical study we compared our ap
    
[^102]: 朝着解释多目标特征关联的方向

    Towards Interpreting Multi-Objective Feature Associations

    [https://arxiv.org/abs/2403.00017](https://arxiv.org/abs/2403.00017)

    提出一种使用多标签的客观特征交互设计，结合全局敏感性分析，以在农业环境中找到最佳组合的新方法。

    

    理解多个特征如何相关并对特定目标的贡献是与理解每个特征如何对特定结果贡献同等重要的。在预测中解释单个特征可以通过多种方式处理；然而，在多目标预测中，难以获得特征值组合的可解释性。为了解决这个问题，我们提出了一种使用多标签的客观特征交互设计，以找到农业环境中特征的最佳组合。该设计的一项新颖之处是确定了一种方法，该方法将特征解释与全局敏感性分析相结合，以确保在多目标环境中进行组合优化。我们在初步实验中证明，可以找到近似的特征值组合以实现期望的结果，该结果使用了两个农业数据集。

    arXiv:2403.00017v1 Announce Type: cross  Abstract: Understanding how multiple features are associated and contribute to a specific objective is as important as understanding how each feature contributes to a particular outcome. Interpretability of a single feature in a prediction may be handled in multiple ways; however, in a multi-objective prediction, it is difficult to obtain interpretability of a combination of feature values. To address this issue, we propose an objective specific feature interaction design using multi-labels to find the optimal combination of features in agricultural settings. One of the novel aspects of this design is the identification of a method that integrates feature explanations with global sensitivity analysis in order to ensure combinatorial optimization in multi-objective settings. We have demonstrated in our preliminary experiments that an approximate combination of feature values can be found to achieve the desired outcome using two agricultural datas
    
[^103]: 面向目标的组合优化的深度敏感性分析

    Deep Sensitivity Analysis for Objective-Oriented Combinatorial Optimization

    [https://arxiv.org/abs/2403.00016](https://arxiv.org/abs/2403.00016)

    本研究提出了一种深度敏感性分析方法，结合神经网络反馈和全局敏感性分析，以在家禽管理中最优化降低多种病原体水平。

    

    饲养环境企业代码：2403.00016v1 公告类型：交叉 摘要：病原体控制是现代家禽养殖的关键方面，对公共健康和生产力都具有重要意义。有效的家禽管理措施可以降低家禽群体中的病原体水平，从而通过降低食源性疾病的风险来促进食品安全。同时，它们通过预防能够迅速传播并影响群体生长、蛋产量和整体健康的传染病来支持动物健康和福利。本研究将寻找最佳管理实践以最小化多种病原体存在视为一个组合优化问题。具体来说，我们将各种管理设置的可能组合建模为一个解决空间，可通过高效探索以识别最优降低病原体水平的配置。该设计融入了一种基于神经网络反馈的方法，结合特征解释和全局敏感性分析，以确保组合优化

    arXiv:2403.00016v1 Announce Type: cross  Abstract: Pathogen control is a critical aspect of modern poultry farming, providing important benefits for both public health and productivity. Effective poultry management measures to reduce pathogen levels in poultry flocks promote food safety by lowering risks of food-borne illnesses. They also support animal health and welfare by preventing infectious diseases that can rapidly spread and impact flock growth, egg production, and overall health. This study frames the search for optimal management practices that minimize the presence of multiple pathogens as a combinatorial optimization problem. Specifically, we model the various possible combinations of management settings as a solution space that can be efficiently explored to identify configurations that optimally reduce pathogen levels. This design incorporates a neural network feedback-based method that combines feature explanations with global sensitivity analysis to ensure combinatorial
    
[^104]: GIN-SD: 通过位置编码和关注融合在图中检测有不完整节点的来源

    GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion

    [https://arxiv.org/abs/2403.00014](https://arxiv.org/abs/2403.00014)

    本文提出了GIN-SD框架，通过位置编码和关注融合解决了在图中检测具有不完整节点的来源的挑战。

    

    在图中检测源已经在谣言源识别领域展现出强大的效力。尽管最近的解决方案通过利用深度神经网络提高了性能，但通常要求完整的用户数据。在本文中，我们解决了一个更具挑战性的任务，即通过不完整的用户数据进行谣言源检测，并提出了一个新颖的框架GIN-SD，通过位置编码和关注融合来解决这一挑战。具体来说，我们的方法利用位置嵌入模块来区分不完整的节点，并采用自注意机制来关注具有更大信息传输能力的节点。为了缓解由于源节点和非源节点数量之间显著差异导致的预测偏差，我们还引入了一个类平衡机制。大量实验证实了GIN-SD及其优越性。

    arXiv:2403.00014v1 Announce Type: cross  Abstract: Source detection in graphs has demonstrated robust efficacy in the domain of rumor source identification. Although recent solutions have enhanced performance by leveraging deep neural networks, they often require complete user data. In this paper, we address a more challenging task, rumor source detection with incomplete user data, and propose a novel framework, i.e., Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion (GIN-SD), to tackle this challenge. Specifically, our approach utilizes a positional embedding module to distinguish nodes that are incomplete and employs a self-attention mechanism to focus on nodes with greater information transmission capacity. To mitigate the prediction bias caused by the significant disparity between the numbers of source and non-source nodes, we also introduce a class-balancing mechanism. Extensive experiments validate the effectiveness of GIN-SD and its su
    
[^105]: 从嘈杂数据中为深度学习优先选择信息特征和样本

    Prioritizing Informative Features and Examples for Deep Learning from Noisy Data

    [https://arxiv.org/abs/2403.00013](https://arxiv.org/abs/2403.00013)

    提出了一个系统框架，通过优先选择信息特征和样本来增强深度学习中的开发过程

    

    在这篇论文中，我们提出了一个系统框架，可以优先选择信息特征和样本，以增强开发过程的每个阶段。具体而言，我们优先选择信息特征和样本，并提高特征学习、数据标记和数据选择的性能。我们首先提出一种方法，通过使用辅助的分布数据提取只与解决目标任务相关的信息特征。我们通过使用辅助分布数据中的信息特征来去除目标分布中的噪声特征。接下来，我们引入一种方法，从无标签的嘈杂数据中优先选择信息样本，以降低主动学习的标记成本。为了解决纯度-信息困境，即尝试选择信息样本会导致选择许多噪声样本的问题，我们提出了一个元模型，找到最佳纯度和信息性之间的平衡。

    arXiv:2403.00013v1 Announce Type: new  Abstract: In this dissertation, we propose a systemic framework that prioritizes informative features and examples to enhance each stage of the development process. Specifically, we prioritize informative features and examples and improve the performance of feature learning, data labeling, and data selection. We first propose an approach to extract only informative features that are inherent to solving a target task by using auxiliary out-of-distribution data. We deactivate the noise features in the target distribution by using that in the out-of-distribution data. Next, we introduce an approach that prioritizes informative examples from unlabeled noisy data in order to reduce the labeling cost of active learning. In order to solve the purity-information dilemma, where an attempt to select informative examples induces the selection of many noisy examples, we propose a meta-model that finds the best balance between purity and informativeness. Lastl
    
[^106]: 基于预排序GNN的定时预测：全局电路预训练，局部时延学习和注意力单元建模

    PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling

    [https://arxiv.org/abs/2403.00012](https://arxiv.org/abs/2403.00012)

    提出了基于预排序GNN的定时预测方法，包括全局电路预训练、局部时延学习和注意力单元建模，以解决大规模工业电路中的信号衰减和误差累积问题。

    

    预路由定时预测最近被研究用于评估芯片设计中候选单元布局的质量。它直接估计引脚级（余量、斜率）和边级（网延迟、单元延迟）的定时指标，而无需耗时路由。然而，在大规模工业电路中，由于长时延路径，它经常遭受信号衰减和误差累积的困扰。为了解决这些挑战，我们提出了一个两阶段方法。首先，我们提出了全局电路训练来预训练一个图自动编码器，从电路网表中学习全局图嵌入。其次，我们使用一种新颖的节点更新方案进行GCN上的消息传递，遵循学习到的图嵌入和电路图的拓扑排序序列。这个方案在更新序列中残留地建模了相邻引脚之间的局部时间延迟，并通过一个注意力单元提取了每个单元内部的查找表信息。

    arXiv:2403.00012v1 Announce Type: new  Abstract: Pre-routing timing prediction has been recently studied for evaluating the quality of a candidate cell placement in chip design. It involves directly estimating the timing metrics for both pin-level (slack, slew) and edge-level (net delay, cell delay), without time-consuming routing. However, it often suffers from signal decay and error accumulation due to the long timing paths in large-scale industrial circuits. To address these challenges, we propose a two-stage approach. First, we propose global circuit training to pre-train a graph auto-encoder that learns the global graph embedding from circuit netlist. Second, we use a novel node updating scheme for message passing on GCN, following the topological sorting sequence of the learned graph embedding and circuit graph. This scheme residually models the local time delay between two adjacent pins in the updating sequence, and extracts the lookup table information inside each cell via a ne
    
[^107]: 引入基于用户反馈的反事实解释（UFCE）

    Introducing User Feedback-based Counterfactual Explanations (UFCE)

    [https://arxiv.org/abs/2403.00011](https://arxiv.org/abs/2403.00011)

    本研究引入了一种名为基于用户反馈的反事实解释（UFCE）的新方法，旨在解决当前反事实解释算法的局限性，并增强提供的解释的可信度。

    

    机器学习模型在实际应用中被广泛使用。然而，它们的复杂性常常使得解释其决策背后的原因成为具有挑战性的任务。反事实解释（CEs）已经成为可行的解决方案，用于在可解释的人工智能（XAI）中生成可理解的解释。CE提供给用户关于如何通过最小的输入修改实现所期望的结果的可操作信息。然而，当前的CE算法通常在优化变化以避免不期望的结果时在整个特征空间内运行，忽视了对结果的主要贡献者的识别，并忽视了建议变化的实际可行性。在这项研究中，我们介绍了一种新的方法，被命名为基于用户反馈的反事实解释（UFCE），该方法解决了这些限制，并旨在增强对所提供解释的信心。UFCE允许t

    arXiv:2403.00011v1 Announce Type: cross  Abstract: Machine learning models are widely used in real-world applications. However, their complexity makes it often challenging to interpret the rationale behind their decisions. Counterfactual explanations (CEs) have emerged as a viable solution for generating comprehensible explanations in eXplainable Artificial Intelligence (XAI). CE provides actionable information to users on how to achieve the desired outcome with minimal modifications to the input. However, current CE algorithms usually operate within the entire feature space when optimizing changes to turn over an undesired outcome, overlooking the identification of key contributors to the outcome and disregarding the practicality of the suggested changes. In this study, we introduce a novel methodology, that is named as user feedback-based counterfactual explanation (UFCE), which addresses these limitations and aims to bolster confidence in the provided explanations. UFCE allows for t
    
[^108]: 自动超声心动图视图识别的图卷积神经网络：一种全面方法

    Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach

    [https://arxiv.org/abs/2402.19062](https://arxiv.org/abs/2402.19062)

    该研究提出了一种全面的方法，通过图卷积神经网络实现了自动超声心动图视图识别，结合了3D心脏网格重建和后续任务如分割和姿势估计。

    

    为了促进对心脏超声波（US）的诊断，临床实践已经建立了心脏的几种标准视图，这些视图作为诊断测量的参考点，并定义了图像获取的视口。自动视图识别涉及将这些图像分组为标准视图的类别。尽管深度学习技术在实现这一点方面取得了成功，但仍然存在在验证图像是否适合特定测量方面的困难，原因包括心脏结构的正确位置、姿势和可能的遮挡。我们的方法超越了视图分类，并融合了对心脏的三维网格重建，从而实现了更多后续任务，如分割和姿势估计。在这项工作中，我们探讨了通过图卷积学习3D心脏网格，使用类似的技术来学习自然图像中的3D网格，例如人体姿势估计。

    arXiv:2402.19062v1 Announce Type: cross  Abstract: To facilitate diagnosis on cardiac ultrasound (US), clinical practice has established several standard views of the heart, which serve as reference points for diagnostic measurements and define viewports from which images are acquired. Automatic view recognition involves grouping those images into classes of standard views. Although deep learning techniques have been successful in achieving this, they still struggle with fully verifying the suitability of an image for specific measurements due to factors like the correct location, pose, and potential occlusions of cardiac structures. Our approach goes beyond view classification and incorporates a 3D mesh reconstruction of the heart that enables several more downstream tasks, like segmentation and pose estimation. In this work, we explore learning 3D heart meshes via graph convolutions, using similar techniques to learn 3D meshes in natural images, such as human pose estimation. As the 
    
[^109]: 基于不确定性的离散异构数据孤岛中可拓展编码本的联邦学习

    Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos

    [https://arxiv.org/abs/2402.18888](https://arxiv.org/abs/2402.18888)

    提出了一种基于不确定性的可拓展编码本的联邦学习框架，用于应对异构数据孤岛中模型适应新分布的挑战

    

    旨在利用广泛分布的数据集进行联邦学习(FL)面临着一个关键挑战：不同孤岛间数据的异构性。我们发现从FL导出的模型在应用于具有陌生分布的数据孤岛时会表现出明显增加的不确定性。因此，我们提出了一种创新而简单的迭代框架，称为基于不确定性的可拓展编码本联邦学习(UEFL)。该框架动态地将潜在特征映射到可训练的离散向量，评估不确定性，并针对表现出高不确定性的孤岛特别地扩展离散化词典或编码本。

    arXiv:2402.18888v1 Announce Type: new  Abstract: Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance a
    
[^110]: ICE-SEARCH: 一种基于语言模型驱动的特征选择方法

    ICE-SEARCH: A Language Model-Driven Feature Selection Approach

    [https://arxiv.org/abs/2402.18609](https://arxiv.org/abs/2402.18609)

    ICE-SEARCH是首个将语言模型与进化算法相结合用于特征选择任务的方法，在医学预测分析应用中取得了State-of-the-Art(SOTA)表现。

    

    本研究揭示了In-Context Evolutionary Search (ICE-SEARCH)方法，这是首个将语言模型(LMs)与进化算法相结合用于特征选择(FS)任务的工作，并展示了其在医学预测分析(MPA)应用中的有效性。ICE-SEARCH利用语言模型中固有的交叉和突变能力，在一个进化框架内显着改进特征选择，通过模型的全面世界知识和其适应各种角色的能力。我们对该方法的评估涵盖了三个关键的MPA任务：中风、心血管疾病和糖尿病，在这些任务中ICE-SEARCH在确定医学应用的关键特征方面优于传统的FS方法。ICE-SEARCH在中风预测和糖尿病预测中实现了领先水平；决策随机化ICE-SEARCH在心血管疾病预测中排名为领先水平。我们的结果不仅证明了

    arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
    
[^111]: SuperdropNet: 一种稳定准确的适用于基于液滴的云微物理的机器学习代理

    SuperdropNet: a Stable and Accurate Machine Learning Proxy for Droplet-based Cloud Microphysics

    [https://arxiv.org/abs/2402.18354](https://arxiv.org/abs/2402.18354)

    SuperdropNet是一种机器学习代理，旨在模拟液滴的云微物理，并通过使用多步自回归预测、物理约束和精细控制随机性来提高准确性和稳定性

    

    云微物理对气候和天气现象具有重要影响，不准确的表示可能会限制预测的准确性。本文介绍了一种名为SuperdropNet的机器学习代理，用于模拟基于液滴的云微物理，旨在提高准确性和稳定性。

    arXiv:2402.18354v1 Announce Type: cross  Abstract: Cloud microphysics has important consequences for climate and weather phenomena, and inaccurate representations can limit forecast accuracy. While atmospheric models increasingly resolve storms and clouds, the accuracy of the underlying microphysics remains limited by computationally expedient bulk moment schemes based on simplifying assumptions. Droplet-based Lagrangian schemes are more accurate but are underutilized due to their large computational overhead. Machine learning (ML) based schemes can bridge this gap by learning from vast droplet-based simulation datasets, but have so far struggled to match the accuracy and stability of bulk moment schemes. To address this challenge, we developed SuperdropNet, an ML-based emulator of the Lagrangian superdroplet simulations. To improve accuracy and stability, we employ multi-step autoregressive prediction during training, impose physical constraints, and carefully control stochasticity in
    
[^112]: 物理启发的机器学习用于预测非线性钢框架结构的地震响应

    Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures

    [https://arxiv.org/abs/2402.17992](https://arxiv.org/abs/2402.17992)

    本研究提出了一种物理启发的机器学习方法，将科学原理和物理定律融入深度神经网络，用于建模非线性结构的地震响应。

    

    由于传统数值模拟的大量计算成本，人们越来越关注利用机器学习（ML）方法进行结构元模型建模。现有的数据驱动策略显示出模型稳健性和可解释性以及丰富数据依赖性的潜在限制。为了解决这些挑战，本文提出了一种新颖的物理启发机器学习（PiML）方法，将科学原理和物理定律融入深度神经网络中，用于建模非线性结构的地震响应。基本概念是将ML模型的解空间约束在已知的物理范围内。这是通过三个主要特点实现的，即模型降阶、长短期记忆（LSTM）网络和牛顿第二定律（例如，运动方程）。模型降阶对处理具有固有冗余性和增强性的结构系统至关重要。

    arXiv:2402.17992v1 Announce Type: cross  Abstract: There is a growing interest in utilizing machine learning (ML) methods for structural metamodeling due to the substantial computational cost of traditional numerical simulations. The existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data. To address these challenges, this paper presents a novel physics-informed machine learning (PiML) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures. The basic concept is to constrain the solution space of the ML model within known physical bounds. This is made possible with three main features, namely, model order reduction, a long short-term memory (LSTM) networks, and Newton's second law (e.g., the equation of motion). Model order reduction is essential for handling structural systems with inherent redundancy and enh
    
[^113]: 想象、初始化和探索：多智能体强化学习中的有效探索方法

    Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2402.17978](https://arxiv.org/abs/2402.17978)

    提出了一种名为Imagine, Initialize, and Explore (IIE)的新方法，利用变压器模型在复杂场景中实现多智能体的有效探索。

    

    有效的探索对于在复杂协调任务中发现多智能体强化学习（MARL）的最佳策略至关重要。现有方法主要利用内在奖励来实现承诺的探索，或者使用基于角色的学习来分解联合动作空间，而不是直接在整个动作-观察空间中进行集体搜索。然而，在长时间跨度任务中，他们往往面临获取特定联合动作序列以达到成功状态的挑战。为解决这一局限性，我们提出了一种称为Imagine, Initialize, and Explore (IIE)的新方法，为复杂场景中的高效多智能体探索提供了一个有前途的解决方案。IIE利用一个变压器模型来想象智能体如何达到可以影响彼此转移函数的关键状态。然后，在探索阶段之前，我们通过模拟器在这个状态下初始化环境。我们制定了实现这种想象的方法。

    arXiv:2402.17978v1 Announce Type: cross  Abstract: Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imag
    
[^114]: 通过基于现场主动偏好学习设计顺序配色方案的Cieran

    Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning

    [https://arxiv.org/abs/2402.15997](https://arxiv.org/abs/2402.15997)

    Cieran是一个允许数据分析师在Jupyter笔记本中设计图表时快速找到质量配色方案的工具，通过主动偏好学习范式进行排序和创建新的配色方案，帮助新手分析师定制配色方案以适应其数据背景。

    

    优质的配色方案可以帮助传达重要的数据模式。然而，要为特定情景找到看起来“恰到好处”的美观配色方案，需要相当多的设计和技术专业知识。我们引入了Cieran，这是一个工具，允许任何数据分析师在设计Jupyter笔记本中的图表时迅速找到优质的配色方案。我们的系统采用了一种主动的偏好学习范式，通过两两比较来对专家设计的配色方案进行排序，允许在色彩设计方面是新手的分析师根据其数据背景定制配色方案。我们通过将配色方案的设计视为在CIELAB颜色空间中的路径规划问题，并使用上下文特定的奖励模型来实现这一目标。在与十二名科学家的评估中，我们发现Cieran有效地模拟了用户的偏好来排名配色方案，并利用这一模型创建了新的优质设计。我们的工作展示了主动偏好学习的潜力。

    arXiv:2402.15997v1 Announce Type: cross  Abstract: Quality colormaps can help communicate important data patterns. However, finding an aesthetically pleasing colormap that looks "just right" for a given scenario requires significant design and technical expertise. We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks. Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context. We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model. In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs. Our work shows the potential of active preferenc
    
[^115]: 面向空间感知的变压器记忆体用于体验代理

    Spatially-Aware Transformer Memory for Embodied Agents

    [https://arxiv.org/abs/2402.15160](https://arxiv.org/abs/2402.15160)

    本文探讨了利用包含空间信息的面向空间感知变压器模型，以改善记忆利用效率。

    

    情节记忆在各种认知过程中起着至关重要的作用，比如能够在头脑中回忆过去事件的能力。虽然认知科学强调空间上下文在情节记忆的形成和检索中的重要性，但当前实现人工智能系统中情节记忆的主要方法是通过存储时间顺序体验的变压器，这忽略了空间维度。因此，目前尚不清楚如何将基础结构扩展到除了仅有时间顺序之外的空间轴，并由此能够获得哪些好处。为了解决这个问题，本文探讨了利用包含空间信息的面向空间感知变压器模型。这些模型使得可以创建考虑时间和空间维度的场所中心情节记忆。采用这种方法，我们证明记忆利用效率可以得到提高，导致增强

    arXiv:2402.15160v1 Announce Type: cross  Abstract: Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanc
    
[^116]: 名字的含义是什么？审计大型语言模型中的种族和性别偏见

    What's in a Name? Auditing Large Language Models for Race and Gender Bias

    [https://arxiv.org/abs/2402.14875](https://arxiv.org/abs/2402.14875)

    调查发现，大型语言模型存在种族和性别偏见，尤其对与黑人女性相关的名字表现最不利。审计在模型部署和实施时的重要性得到强调。

    

    我们采用审计设计来调查最先进的大型语言模型中的偏见，包括GPT-4。在我们的研究中，我们引发模型在各种情景下为个人提供建议，比如在购车谈判或选举结果预测过程中。我们发现该建议系统性地对与种族少数群体和女性常见相关的名字产生不利影响。与黑人女性相关的名字得到的结果最不利。这些偏见在42个提示模板和多个模型中都是一致的，表明这是一个系统性问题，而不是孤立事件。在提示中提供数值、与决策相关的锚点可以成功抵消偏见，而定性细节的影响并不一致，甚至可能会加剧差异。我们的研究结果强调了在语言模型部署和实施时进行审计的重要性，以减轻其潜在影响。

    arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
    
[^117]: 基于深度强化学习和迁移学习的边缘缓存

    Edge Caching Based on Deep Reinforcement Learning and Transfer Learning

    [https://arxiv.org/abs/2402.14576](https://arxiv.org/abs/2402.14576)

    本文提出了一种基于双深度Q学习的缓存方法，通过半马尔可夫决策过程（SMDP）适应现实场景中随机请求到达的特性，综合考虑各种文件特征。

    

    本文讨论了网络中冗余数据传输日益挑战的问题。流量激增已经使中继链路和骨干网络承压，促使对边缘路由器的缓存解决方案进行探索。现有工作主要依赖于马尔可夫决策过程（MDP）处理缓存问题，假设固定时间间隔的决策；然而，现实场景涉及随机请求到达，尽管各种文件特征在确定最佳缓存策略方面起着至关重要的作用，但相关的现有工作并未考虑所有这些文件特征来形成缓存策略。在本文中，首先我们利用半马尔可夫决策过程（SMDP）来建模缓存问题，以适应现实场景的连续时间特性，允许在文件请求时随机进行缓存决策。然后，我们提出了一种基于双深度Q学习的缓存方法，全面考虑了不同文件特征的影响。

    arXiv:2402.14576v1 Announce Type: cross  Abstract: This paper addresses the escalating challenge of redundant data transmission in networks. The surge in traffic has strained backhaul links and backbone networks, prompting the exploration of caching solutions at the edge router. Existing work primarily relies on Markov Decision Processes (MDP) for caching issues, assuming fixed-time interval decisions; however, real-world scenarios involve random request arrivals, and despite the critical role of various file characteristics in determining an optimal caching policy, none of the related existing work considers all these file characteristics in forming a caching policy. In this paper, first, we formulate the caching problem using a semi-Markov Decision Process (SMDP) to accommodate the continuous-time nature of real-world scenarios allowing for caching decisions at random times upon file requests. Then, we propose a double deep Q-learning-based caching approach that comprehensively accou
    
[^118]: 牛头角：硬样本加权持续训练改善LLM泛化能力

    Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization

    [https://arxiv.org/abs/2402.14270](https://arxiv.org/abs/2402.14270)

    通过硬样本加权持续训练的方法，该研究提出了IR-DRO框架，通过动态优先考虑训练中信息丰富的样本，以改善LLM泛化能力。

    

    在大语言模型（LLMs）快速发展的领域中，一个关键挑战是在高质量训练数据短缺的情况下增强它们的能力。我们的研究从一个轻量级持续训练LLMs的经验策略开始，使用它们的原始预训练数据集，重点关注导致中等损失的样本的选择性保留。这些样本被认为是信息丰富的，并且有助于模型的改进，与最高损失的样本形成对比，后者将由于与数据噪声和复杂性的相关性而被丢弃。然后，我们将这一策略形式化为基于实例加权的分布鲁棒优化（IR-DRO）的原则框架。IR-DRO旨在通过实例重新加权机制动态优先考虑训练的重点样本，由一个封闭形式解决方案简化，以便轻松整合到已建立的训练协议中。

    arXiv:2402.14270v1 Announce Type: new  Abstract: In the rapidly advancing arena of large language models (LLMs), a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data. Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses. These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity. We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protoco
    
[^119]: 使用GANs生成合成数据延伸与保护——基于时间序列医疗记录

    Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records

    [https://arxiv.org/abs/2402.14042](https://arxiv.org/abs/2402.14042)

    本研究使用GANs生成了时间序列合成痴呆患者医疗记录的数据集，并比较了不同GAN模型在生成合成数据方面的质量，实现了在不涉及隐私问题的情况下保护用户数据并延伸数据应用。

    

    arXiv:2402.14042v1 公告类型:交叉摘要: 保护私人用户数据对于高质量体验(QoE)和可接受性至关重要，尤其是对于处理敏感数据的服务，如基于IT的健康服务。尽管已经显示匿名化技术容易被数据重新识别，但合成数据生成逐渐取代了匿名化，因为它相对耗时和资源耗费较少，并且更能抵抗数据泄漏。生成对抗网络(GANs)已被用于生成合成数据集，特别是遵循差分隐私现象的GAN框架。本研究比较了用于生成时间序列合成痴呆患者医疗记录的最新GAN基模型，这些数据可以在不涉及隐私问题的情况下分发。 预测建模、自相关性和分布分析被用来评估生成数据的生成质量(QoG)。

    arXiv:2402.14042v1 Announce Type: cross  Abstract: Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. T
    
[^120]: E2USD：用于多元时间序列的高效而有效的无监督状态检测

    E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series

    [https://arxiv.org/abs/2402.14041](https://arxiv.org/abs/2402.14041)

    E2USD提出了一种有效的无监督多元时间序列状态检测方法，利用了快速傅里叶变换和双视图嵌入模块进行编码，以及通过对抗学习方法消除假阴性，从而实现了SOTA准确性并显著降低了计算开销。

    

    我们提出了E2USD方法，能够实现高效而准确的无监督多元时间序列状态检测。E2USD利用基于快速傅立叶变换的时间序列压缩器(FFTCompress)和分解的双视图嵌入模块(DDEM)，一起以低计算开销对输入的多元时间序列进行编码。此外，我们提出了一种假阴性取消对比学习方法(FNCCLearning)，以抵消假阴性的影响，并实现更友好的簇嵌入空间。为了在流式设置中进一步减少计算开销，我们引入了自适应阈值检测(ADATD)。通过使用六个基线模型和六个数据集进行全面实验，我们证明E2USD能够在显著降低计算开销的情况下达到SOTA的准确性。我们的代码可在https://github.com/AI4CTS/E2Usd 找到。

    arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
    
[^121]: BenchCloudVision: 云检测和分割中深度学习方法的基准分析

    BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery

    [https://arxiv.org/abs/2402.13918](https://arxiv.org/abs/2402.13918)

    本文通过对遥感图像中的云进行分割，旨在提高卫星图像分析的精度和效率，应用于环境监测、资源管理和灾害响应。

    

    配备光学传感器的卫星捕获高分辨率图像，为各种环境现象提供了宝贵的见解。近年来，针对遥感中一些挑战的研究激增，从不同景观中的水检测到山地和地形的分割。正在进行的研究旨在提高卫星图像分析的精度和效率。尤其是，越来越多的重点放在开发准确的水体检测、雪和云的方法上，这些对环境监测、资源管理和灾害响应至关重要。在这个背景下，本文专注于从遥感图像中分割云。由于光学传感器应用中云的存在，准确的遥感数据分析可能具有挑战性。由此产生的产品（如应用和研究）的质量直接受到影响。

    arXiv:2402.13918v1 Announce Type: cross  Abstract: Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directl
    
[^122]: 如何验证机器学习回归任务的平均校准性？

    How to validate average calibration for machine learning regression tasks ?

    [https://arxiv.org/abs/2402.10043](https://arxiv.org/abs/2402.10043)

    本文提出了两种验证机器学习回归任务平均校准性的方法，将校准误差与平均绝对误差之间的差值和将平均平方z-分数与1进行比较。研究发现，前者对不确定性分布敏感，而后者在该方面提供了最可靠的方法。

    

    机器学习回归任务的平均校准性可以通过两种方式进行测试。一种方式是将校准误差（CE）估计为平均绝对误差（MSE）与平均方差（MV）或平均平方不确定性之间的差值。另一种方式是将平均平方z-分数或缩放误差（ZMS）与1进行比较。两种方法可能得出不同的结论，正如来自最近的机器学习不确定性量化文献中的数据集集合所示。研究表明，CE对不确定性分布非常敏感，特别是对于离群不确定性的存在，因此无法可靠地用于校准测试。相比之下，ZMS统计量不具有这种敏感性问题，在这种情况下提供了最可靠的方法。文章还讨论了对条件校准验证的影响。

    arXiv:2402.10043v1 Announce Type: cross  Abstract: Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.
    
[^123]: Pathformer: 多尺度自适应路径的时间序列预测模型

    Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting

    [https://arxiv.org/abs/2402.05956](https://arxiv.org/abs/2402.05956)

    本文提出了一种名为Pathformer的多尺度自适应路径的Transformer模型，用于时间序列预测。通过整合时间分辨率和时间距离进行多尺度建模，并使用自适应路径来优化建模过程，可以提高预测准确性和泛化能力。

    

    基于Transformer的模型在时间序列预测中取得了一些成功。现有的方法主要从有限或固定尺度对时间序列进行建模，这使得捕捉跨多个尺度的不同特征变得具有挑战性。本文提出了一种多尺度自适应路径（Pathformer）的Transformer模型。该模型同时整合了时间分辨率和时间距离进行多尺度建模。多尺度划分运用不同大小的数据块将时间序列分割成不同的时间分辨率。基于每个尺度的划分，对这些数据块进行双重注意力机制，以捕捉全局相关性和局部细节作为时间依赖关系。我们进一步通过自适应路径来丰富多尺度Transformer，该路径可以根据输入时间序列中不断变化的时间动态调整多尺度建模过程，提高Pathformer的预测准确性和泛化能力。在11个真实数据集上进行了大量实验。

    Transformer-based models have achieved some success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on eleven rea
    
[^124]: 从推理路径聚合的角度理解语言模型的推理能力

    Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation

    [https://arxiv.org/abs/2402.03268](https://arxiv.org/abs/2402.03268)

    本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。

    

    预训练的语言模型能够在没有明确微调的情况下执行复杂的推理。为了理解预训练与下一个标记预测目标的关系如何促使推理能力的出现，我们提出可以将语言模型视为在预训练时通过聚合间接的推理路径来得出新结论。我们发现，这个视角在逻辑推理和数学推理等关键情况下非常有效。具体而言，我们将推理路径形式化为在知识/推理图上的随机游走路径。对学习的语言模型分布的分析表明，相关随机游走路径概率的加权和是解释语言模型推理的合理方式。对多个知识图谱和数学问题数据集进行的实验和分析揭示了训练对随机游走路径的影响，并表明增加无标签的随机游走推理路径可以提高现实世界的多步推理能力。

    Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
    
[^125]: 在大型语言模型中测量道德不一致性

    Measuring Moral Inconsistencies in Large Language Models

    [https://arxiv.org/abs/2402.01719](https://arxiv.org/abs/2402.01719)

    本研究提出了一种新的信息论度量方法，称为语义图熵（SGE），用于测量道德情景中大型语言模型（LLM）的一致性。与现有的一致性度量方法相比，SGE在五个LLMs上与人类判断更好地相关，为研究LLM不一致性的根本原因提供了新的思路。

    

    如果语义等价的提示产生语义等价的响应，那么大型语言模型(LLM)被认为是一致的。尽管最近的进展展示了LLMs在对话系统中令人印象深刻的能力，但我们表明即使是最先进的LLMs在生成方面也存在高度不一致性，这对它们的可靠性提出了质疑。先前的研究尝试用任务特定的准确度来衡量这一点。然而，这种方法对于没有“正确”答案的道德情景（例如，道路交运问题）是不合适的。为了解决这个问题，我们提出了一种新的信息论度量方法，称为语义图熵（SGE），来衡量LLM在道德情景中的一致性。我们利用“经验法则”（RoTs）来解释模型的决策策略，并进一步增强我们的度量方法。与现有的一致性度量方法相比，SGE与人类判断在五个LLMs上更好地相关。在未来，我们的目标是调查LLM不一致性的根本原因。

    A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
    
[^126]: 杀手级应用：低速大规模AI武器

    Killer Apps: Low-Speed, Large-Scale AI Weapons

    [https://arxiv.org/abs/2402.01663](https://arxiv.org/abs/2402.01663)

    本文研究了AI武器的概念、部署、检测和潜在对策，强调了在信息领域内基于AI的心理操纵的潜力，以及其对全球个人、组织和社会的威胁。

    

    人工智能（AI）和机器学习（ML）的不断进步，特别是由OpenAI、Meta和Anthropic等组织开发的尖端生成式预训练转换器（GPT）模型的发展，给战争和安全带来了新的挑战和机会。目前关注的主要是AI在武器系统中的整合以及在动能冲突中快速决策中的作用。然而，同样重要但经常被忽视的一个方面是在信息领域中基于AI的心理操纵在互联网规模内的潜力。这些能力可能对全球个人、组织和社会造成重大威胁。本文探讨了AI武器的概念、部署、检测和潜在对策。

    The accelerating advancements in Artificial Intelligence (AI) and Machine Learning (ML), highlighted by the development of cutting-edge Generative Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and Anthropic, present new challenges and opportunities in warfare and security. Much of the current focus is on AI's integration within weapons systems and its role in rapid decision-making in kinetic conflict. However, an equally important but often overlooked aspect is the potential of AI-based psychological manipulation at internet scales within the information domain. These capabilities could pose significant threats to individuals, organizations, and societies globally. This paper explores the concept of AI weapons, their deployment, detection, and potential countermeasures.
    
[^127]: 通过期望分区函数和连续优化设计信使RNA

    Messenger RNA Design via Expected Partition Function and Continuous Optimization

    [https://arxiv.org/abs/2401.00037](https://arxiv.org/abs/2401.00037)

    将RNA设计问题转化为连续优化，并提出了基于期望分区函数的通用优化框架，将候选序列的分布逐步优化为单一序列。

    

    RNA设计的任务是离散优化问题，有几个版本的这些问题是NP难题。为了替代常用的局部搜索方法，我们将这些问题表述为连续优化，并基于一种称为"期望分区函数"的经典分区函数的泛化开发了一个通用框架。基本思想是从所有可能的候选序列中开始一个分布，并将目标函数从一个序列扩展到一个分布。然后我们使用基于梯度下降的优化方法改善扩展的目标函数，分布将逐渐收缩到一个独热序列（即一个序列）。作为案例研究，我们考虑了在疫苗和治疗领域有着广泛应用的mRNA设计的重要问题。

    arXiv:2401.00037v2 Announce Type: replace-cross  Abstract: The tasks of designing RNAs are discrete optimization problems, and several versions of these problems are NP-hard. As an alternative to commonly used local search methods, we formulate these problems as continuous optimization and develop a general framework for this optimization based on a generalization of classical partition function which we call "expected partition function". The basic idea is to start with a distribution over all possible candidate sequences, and extend the objective function from a sequence to a distribution. We then use gradient descent-based optimization methods to improve the extended objective function, and the distribution will gradually shrink towards a one-hot sequence (i.e., a single sequence). As a case study, we consider the important problem of mRNA design with wide applications in vaccines and therapeutics. While the recent work of LinearDesign can efficiently optimize mRNAs for minimum free
    
[^128]: 论从可观测和私密数据中实现速率最优分区分类

    On Rate-Optimal Partitioning Classification from Observable and from Privatised Data

    [https://arxiv.org/abs/2312.14889](https://arxiv.org/abs/2312.14889)

    研究了在放宽条件下的分区分类方法的收敛速率，提出了绝对连续分量的新特性，计算了分类错误概率的精确收敛率

    

    在这篇论文中，我们重新审视了分区分类的经典方法，并研究了在放宽条件下的收敛速率，包括可观测（非私密）和私密数据。我们假设特征向量$X$取值于$\mathbb{R}^d$，其标签为$Y$。之前关于分区分类器的结果基于强密度假设，这种假设限制较大，我们通过简单的例子加以证明。我们假设$X$的分布是绝对连续分布和离散分布的混合体，其中绝对连续分量集中于一个$d_a$维子空间。在这里，我们在更宽松的条件下研究了这个问题：除了标准的Lipschitz和边际条件外，我们还引入了绝对连续分量的一个新特性，通过该特性计算了分类错误概率的精确收敛率，对于...

    arXiv:2312.14889v2 Announce Type: replace-cross  Abstract: In this paper we revisit the classical method of partitioning classification and study its convergence rate under relaxed conditions, both for observable (non-privatised) and for privatised data. Let the feature vector $X$ take values in $\mathbb{R}^d$ and denote its label by $Y$. Previous results on the partitioning classifier worked with the strong density assumption, which is restrictive, as we demonstrate through simple examples. We assume that the distribution of $X$ is a mixture of an absolutely continuous and a discrete distribution, such that the absolutely continuous component is concentrated to a $d_a$ dimensional subspace. Here, we study the problem under much milder assumptions: in addition to the standard Lipschitz and margin conditions, a novel characteristic of the absolutely continuous component is introduced, by which the exact convergence rate of the classification error probability is calculated, both for the
    
[^129]: 实现端到端人工智能驱动的全球天气预报系统

    Towards an end-to-end artificial intelligence driven global weather forecasting system

    [https://arxiv.org/abs/2312.12462](https://arxiv.org/abs/2312.12462)

    提出了一种端到端基于人工智能的全球天气预报系统，通过将AI技术应用于数据同化和天气预报模型，实现了从数据处理到预测全过程的自动化。

    

    天气预报系统对科学和社会至关重要，在将人工智能（AI）应用于中期天气预报方面取得了重大成就。然而，现有的基于AI的天气预报模型依赖于传统数值天气预报（NWP）系统的分析或再分析产品作为预测的初始条件。初始状态通常由传统数据同化组件生成，这是计算昂贵且耗时。本文提出了一种基于AI的数据同化模型，即Adas，用于全球天气变量。我们将Adas与先进的基于AI的天气预报模型（即FengWu）结合起来，构建了第一个端到端基于AI的全球天气预报系统：FengWu-Adas。我们证明了Adas能够同化稀疏的全球观测数据，产生高质量的分析结果，使系统能够稳定运行。

    arXiv:2312.12462v2 Announce Type: replace-cross  Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from the traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. Initial states are typically generated by traditional data assimilation component, which is computational expensive and time-consuming. Here we present an AI-based data assimilation model, i.e., Adas, for global weather variables. And we combine Adas with the advanced AI-based weather forecasting model (i.e., FengWu) to construct the first end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate sparse global observations to produce high-quality analysis, enabling the system operate stably 
    
[^130]: 基于均值嵌入的分布式贝尔曼算子

    Distributional Bellman Operators over Mean Embeddings

    [https://arxiv.org/abs/2312.07358](https://arxiv.org/abs/2312.07358)

    提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架，推导出新算法并展示可与深度强化学习结合，提高表现。

    

    我们提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架。 我们基于这一框架推导出了几种新的动态规划和时序差分学习算法，提供了渐近收敛理论，并对这些算法在一系列表格任务上的实证表现进行了检验。此外，我们展示了这种方法可以与深度强化学习直接结合，得到一种新的深度强化学习代理，该代理在 Arcade Learning Environment 上优于基线分布式方法。

    arXiv:2312.07358v2 Announce Type: replace-cross  Abstract: We propose a novel algorithmic framework for distributional reinforcement learning, based on learning finite-dimensional mean embeddings of return distributions. We derive several new algorithms for dynamic programming and temporal-difference learning based on this framework, provide asymptotic convergence theory, and examine the empirical performance of the algorithms on a suite of tabular tasks. Further, we show that this approach can be straightforwardly combined with deep reinforcement learning, and obtain a new deep RL agent that improves over baseline distributional approaches on the Arcade Learning Environment.
    
[^131]: 通过转移学习和元学习提高弱监督搜索的性能

    Improving the performance of weak supervision searches using transfer and meta-learning

    [https://arxiv.org/abs/2312.06152](https://arxiv.org/abs/2312.06152)

    通过转移学习和元学习，本研究通过在模拟数据上训练神经网络来提高弱监督搜索的性能，从而减少了在实验数据上需要的信号量。

    

    弱监督搜索原则上具有能够在实验数据上训练和学习独特信号特性的优势。然而，这种搜索的实际适用性受到成功通过弱监督训练神经网络可能需要大量信号的限制。在这项工作中，我们试图通过使用转移学习和元学习创建可以从较少实验信号中学习的神经网络。总的想法是首先在模拟数据上训练神经网络，从而学习可以重复使用的概念或成为更有效的学习者。然后，神经网络将在实验数据上进行训练，并且由于先前的训练而需要较少的信号。我们发现转移学习和元学习可以显著提高弱监督搜索的性能。

    arXiv:2312.06152v2 Announce Type: replace-cross  Abstract: Weak supervision searches have in principle the advantages of both being able to train on experimental data and being able to learn distinctive signal properties. However, the practical applicability of such searches is limited by the fact that successfully training a neural network via weak supervision can require a large amount of signal. In this work, we seek to create neural networks that can learn from less experimental signal by using transfer and meta-learning. The general idea is to first train a neural network on simulations, thereby learning concepts that can be reused or becoming a more efficient learner. The neural network would then be trained on experimental data and should require less signal because of its previous training. We find that transfer and meta-learning can substantially improve the performance of weak supervision searches.
    
[^132]: 强化关注力中最短的支柱：增强大型语言模型的上下文意识，以实现有效的工具使用

    Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use

    [https://arxiv.org/abs/2312.04455](https://arxiv.org/abs/2312.04455)

    本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    

    在本文中，我们证明了大型语言模型(LLMs)中关注分配中的内在波形模式显著影响它们在需要高度上下文意识的任务中的性能，例如利用LLMs进行工具使用。具体而言，当关键信息在上下文中位于关注波形的低谷区域时，模型可能会忽视该信息，导致性能下降。为了解决这个问题，我们提出了一种名为“Attention Buckets”的新型推理方法。它允许LLMs通过多个并行过程处理输入。每个过程使用不同的基准角度进行旋转位置嵌入，从而创建出一个独特的关注波形。通过用一个过程的关注低谷补偿另一个过程的关注高峰，我们的方法增强了LLM对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
    
[^133]: 模仿引导式强化学习

    Imitation Bootstrapped Reinforcement Learning

    [https://arxiv.org/abs/2311.02198](https://arxiv.org/abs/2311.02198)

    提出了一种模仿引导式强化学习（IBRL）的框架，用于高效的样本-efficient RL，通过先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。

    

    尽管强化学习（RL）具有相当大的潜力，但机器人控制任务主要依赖模仿学习（IL）是因为其更好的样本效率。然而，收集能使IL推广到所有可能场景的全面专家演示是昂贵的，任何分布的转变都需要重新收集数据进行微调。因此，如果RL可以建立在IL的基础上作为一种高效的自我改进程序，那么它将具有吸引力。我们提出了一种模仿引导式强化学习（IBRL）的新框架，用于具有示范的高效抽样RL，首先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。与先前过度采样示范或用额外的模仿损失对RL进行正则化的工作相比，IBRL能够利用来自IL的高质量动作。

    arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p
    
[^134]: 通过最优输运证明神经网络的线性模态连通性

    Proving Linear Mode Connectivity of Neural Networks via Optimal Transport

    [https://arxiv.org/abs/2310.19103](https://arxiv.org/abs/2310.19103)

    证明了通过最优输运的方法，使用随机梯度下降训练的两层神经网络可以线性连接，同时提供了每层神经元权重独立的深度神经网络线性连接的上下界。

    

    高维非凸优化问题的能量景观对于理解现代深度神经网络架构的有效性至关重要。最近的研究实验证明，在随机训练的两次运行之后找到的两个不同解通常可以通过非常简单的连续路径（如线性路径）相连，除了权重的排列。本文提供了一个理论框架，用于理论上解释这一经验观察。基于经验度量的Wasserstein距离的收敛速率，我们展示了在很高的概率下，使用随机梯度下降训练的两个足够宽的两层神经网络之间是线性连接的。此外，我们还表达了具有独立神经元权重的两个深度神经网络每层宽度线性连接的上下界。最后，我们通过展示如何通过保持维度来实证我们的方法的有效性。

    arXiv:2310.19103v2 Announce Type: replace  Abstract: The energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures. Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a framework theoretically explaining this empirical observation. Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension 
    
[^135]: 利用斜裁决策森林增强在线环境中的群体公平性

    Enhancing Group Fairness in Online Settings Using Oblique Decision Forests

    [https://arxiv.org/abs/2310.11401](https://arxiv.org/abs/2310.11401)

    提出了Aranyani，一种斜裁集成的方法，用于解决在在线环境中优化群体公平性目标所面临的挑战

    

    公平性，特别是群体公平性，在机器学习系统中是一个重要的考虑因素。目前最常见的群体公平性增强技术是通过在训练过程中依赖公平目标（例如人口统计平等）和任务特定目标（例如交叉熵）的混合方法。然而，在数据以在线方式一次一个实例到达时，优化这样的公平性目标面临着几个挑战。特别是，群体公平性目标是通过不同人口统计群体的预测期望来定义的。在在线环境中，算法每次只能访问一个实例，估计群体公平性目标需要额外的存储和比任务特定目标更多的计算（例如前向/后向传递）在每个时间步上。

    arXiv:2310.11401v2 Announce Type: replace  Abstract: Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of obliq
    
[^136]: 树形交叉注意力

    Tree Cross Attention

    [https://arxiv.org/abs/2309.17388](https://arxiv.org/abs/2309.17388)

    提出了一种基于交叉注意力的树形模块，能够仅从对数数量的标记中检索信息进行推断。

    

    交叉注意力是一种用于从一组上下文标记中检索信息以进行预测的流行方法。在推断时，对于每个预测，交叉注意力会扫描全部的 $\mathcal{O}(N)$ 个标记。然而，在实践中，通常只需一小部分标记即可实现良好的性能。方法如Perceiver IO在推断时廉价，因为它将信息提炼到一个规模较小的潜在标记集合 $L < N$ 上，然后再应用交叉注意力，仅导致 $\mathcal{O}(L)$ 的复杂性。然而，在实践中，随着输入标记的数量和提炼信息的增加，所需的潜在标记数量也会显著增加。在这项工作中，我们提出了一种基于交叉注意力的模块，称为树形交叉注意力 (TCA) - 该模块仅从对数数量的标记中检索信息进行推断。TCA以树结构组织数据

    arXiv:2309.17388v2 Announce Type: replace  Abstract: Cross Attention is a popular method for retrieving information from a set of context tokens for making predictions. At inference time, for each prediction, Cross Attention scans the full set of $\mathcal{O}(N)$ tokens. In practice, however, often only a small subset of tokens are required for good performance. Methods such as Perceiver IO are cheap at inference as they distill the information to a smaller-sized set of latent tokens $L < N$ on which cross attention is then applied, resulting in only $\mathcal{O}(L)$ complexity. However, in practice, as the number of input tokens and the amount of information to distill increases, the number of latent tokens needed also increases significantly. In this work, we propose Tree Cross Attention (TCA) - a module based on Cross Attention that only retrieves information from a logarithmic $\mathcal{O}(\log(N))$ number of tokens for performing inference. TCA organizes the data in a tree structu
    
[^137]: 为图级任务调优预训练图神经网络的搜索

    Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks

    [https://arxiv.org/abs/2308.06960](https://arxiv.org/abs/2308.06960)

    设计更好的微调策略以在下游任务中更好地利用转移的知识并提高性能。

    

    最近，图神经网络（GNNs）在许多与图相关的任务中展现了前所未有的成功。然而，与其他神经网络一样，GNN面临着标签稀缺的问题。因此，最近的工作尝试在大规模未标记的图上预训练GNN，并将从未标记的图中获得的知识适应到目标下游任务中。该适应通常通过使用有限数量的标记数据对预训练的GNN进行微调来实现。尽管微调的重要性，当前的GNN预训练工作往往忽视了设计一个良好的微调策略以更好地利用转移的知识并提高下游任务的性能。只有少数工作开始研究预训练GNN的更好微调策略。但他们的设计要么有很强的假设，要么忽视了各种下游数据集的数据感知问题。因此，我们的目标是为预训练的GNN设计更好的微调策略。

    arXiv:2308.06960v2 Announce Type: replace-cross  Abstract: Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs t
    
[^138]: 具有位置尺度和形状的神经加性模型：超越平均值的可解释神经回归框架

    Neural Additive Models for Location Scale and Shape: A Framework for Interpretable Neural Regression Beyond the Mean

    [https://arxiv.org/abs/2301.11862](https://arxiv.org/abs/2301.11862)

    提出了神经加性模型具有位置尺度和形状的神经加性模型(NAMLSS)框架，结合了经典深度学习模型的预测能力和分布回归的优势

    

    深度神经网络(DNNs)已被证明在各种任务中非常有效，使其成为需要高级预测能力的问题的首选方法。尽管取得成功，但DNNs的内部运作通常不透明，使其难以解释或理解。这种缺乏可解释性导致近年来对固有可解释性神经网络的研究不断增加。神经加性模型（NAMs）等模型通过将经典统计方法与DNNs相结合实现了视觉可解释性。然而，这些方法仅集中于平均响应预测，忽略了底层数据响应分布的其他特性。我们提出了具有位置尺度和形状的神经加性模型(NAMLSS)，这是一个建模框架，将经典深度学习模型的预测能力与分布回归的固有优势相结合。

    arXiv:2301.11862v2 Announce Type: replace-cross  Abstract: Deep neural networks (DNNs) have proven to be highly effective in a variety of tasks, making them the go-to method for problems requiring high-level predictive power. Despite this success, the inner workings of DNNs are often not transparent, making them difficult to interpret or understand. This lack of interpretability has led to increased research on inherently interpretable neural networks in recent years. Models such as Neural Additive Models (NAMs) achieve visual interpretability through the combination of classical statistical methods with DNNs. However, these approaches only concentrate on mean response predictions, leaving out other properties of the response distribution of the underlying data. We propose Neural Additive Models for Location Scale and Shape (NAMLSS), a modelling framework that combines the predictive power of classical deep learning models with the inherent advantages of distributional regression while
    
[^139]: 高维状态空间中马尔可夫决策过程的结构估计与有限时间保证

    Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees

    [https://arxiv.org/abs/2210.01282](https://arxiv.org/abs/2210.01282)

    本论文提出了一种单循环估计算法，具有有限时间保证，能够处理高维状态空间的马尔可夫决策过程的结构估计问题，而不会损害奖励估计精度。

    

    我们考虑基于可观测的行为历史和访问状态来估计人类代理动态决策的结构模型的任务。问题具有固有的嵌套结构：在内部问题中，确定给定奖励函数的最优策略，而在外部问题中，最大化适合度度量。已经提出了几种方法来减轻这种嵌套循环结构的计算负担，但当状态空间要么是具有大基数的离散空间，要么是高维连续空间时，这些方法仍然面临高复杂度的问题。逆强化学习(IRL)文献中的其他方法强调策略估计，但却以降低奖励估计精度为代价。在本文中，我们提出了一种具有有限时间保证的单循环估计算法，适用于处理高维状态空间而不会损害奖励。

    arXiv:2210.01282v3 Announce Type: replace-cross  Abstract: We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising rewar
    
[^140]: 分布式增强影响的局部模拟器用于大型网络系统中的并行多智能体强化学习

    Distributed Influence-Augmented Local Simulators for Parallel MARL in Large Networked Systems

    [https://arxiv.org/abs/2207.00288](https://arxiv.org/abs/2207.00288)

    本文展示了如何将大型网络系统分解为多个局部组件，构建独立并并行运行的模拟器，有效监测影响，并在几小时内训练大型多智能体系统，减轻同时学习的负面影响。

    

    由于其高样本复杂性，模拟在今天对于成功应用强化学习至关重要。然而，许多现实世界问题表现出过于复杂的动态性，这使得它们的全尺度模拟在计算上变得缓慢。本文展示了如何将许多智能体的大型网络系统分解为多个局部组件，从而我们可以构建独立并并行运行的分开模拟器。为了监测不同局部组件相互施加的影响，这些模拟器每一个都配备了一个周期性训练在真实轨迹上的学习模型。我们的实证结果表明，将模拟分配给不同的进程不仅可以在几小时内训练大型多智能体系统，还有助于减轻同时学习的负面影响。

    arXiv:2207.00288v2 Announce Type: replace  Abstract: Due to its high sample complexity, simulation is, as of today, critical for the successful application of reinforcement learning. Many real-world problems, however, exhibit overly complex dynamics, which makes their full-scale simulation computationally slow. In this paper, we show how to decompose large networked systems of many agents into multiple local components such that we can build separate simulators that run independently and in parallel. To monitor the influence that the different local components exert on one another, each of these simulators is equipped with a learned model that is periodically trained on real trajectories. Our empirical results reveal that distributing the simulation among different processes not only makes it possible to train large multi-agent systems in just a few hours but also helps mitigate the negative effects of simultaneous learning.
    
[^141]: 通过逆强化学习实现可解释的深度强化学习模型

    Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning

    [https://arxiv.org/abs/2203.16464](https://arxiv.org/abs/2203.16464)

    本研究提出了一个新颖的框架，利用对抗逆强化学习，可以为强化学习模型所作出的决策提供全局解释，并捕捉直观的倾向。

    

    人工智能，尤其是近年来深度学习的最新进展，在自然语言处理和计算机视觉等领域的许多任务中取得了出色的表现。除了令人满意的评估指标外，这些模型通常需要高度可解释性才能被可靠地利用。因此，提供能够深入了解模型如何将其输入映射到输出的解释成为人们迫切需要的。不幸的是，目前机器学习模型的黑盒特性仍然是一个尚未解决的问题，这种特性阻碍了研究人员学习和提供对模型行为和最终预测的解释描述。

    arXiv:2203.16464v3 Announce Type: replace-cross  Abstract: Artificial intelligence, particularly through recent advancements in deep learning, has achieved exceptional performances in many tasks in fields such as natural language processing and computer vision. In addition to desirable evaluation metrics, a high level of interpretability is often required for these models to be reliably utilized. Therefore, explanations that offer insight into the process by which a model maps its inputs onto its outputs are much sought-after. Unfortunately, the current black box nature of machine learning models is still an unresolved issue and this very nature prevents researchers from learning and providing explicative descriptions for a model's behavior and final predictions. In this work, we propose a novel framework utilizing Adversarial Inverse Reinforcement Learning that can provide global explanations for decisions made by a Reinforcement Learning model and capture intuitive tendencies that th
    
[^142]: 基于混合Tries的内存高效序列模式挖掘

    Memory-Efficient Sequential Pattern Mining with Hybrid Tries

    [https://arxiv.org/abs/2202.06834](https://arxiv.org/abs/2202.06834)

    提出了一种基于混合trie的内存高效序列模式挖掘方法，在内存消耗和计算时间方面相比现有技术有显著改善，且是唯一一个能够处理256GB系统内存下大数据集的方法。

    

    随着现代数据集的指数级增长，对于能够处理如此庞大数据集的高效挖掘算法的需求变得日益迫切。本文提出了一种内存高效的方法用于序列模式挖掘（SPM），这是知识发现中的一个基本主题，面临着针对大数据集的已知内存瓶颈。我们的方法涉及一种新颖的混合trie数据结构，利用重复模式紧凑地存储内存中的数据集; 以及一个相应的挖掘算法，旨在有效地从此紧凑表示中提取模式。对真实测试实例的数值结果显示，与最先进技术相比，对于小到中等大小的数据集，内存消耗平均提高了88％，计算时间提高了41％。此外，我们的算法是唯一一个在系统内存为256GB的情况下能够处理大数据集的SPM方法。

    arXiv:2202.06834v2 Announce Type: replace-cross  Abstract: As modern data sets continue to grow exponentially in size, the demand for efficient mining algorithms capable of handling such large data sets becomes increasingly imperative. This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on real-life test instances show an average improvement of 88% in memory consumption and 41% in computation time for small to medium-sized data sets compared to the state of the art. Furthermore, our algorithm stands out as the only capable SPM approach for large data sets within 256GB of system memory.
    
[^143]: InceptionXML：一种带有同步负采样的轻量级框架，用于短文本极端分类

    InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification

    [https://arxiv.org/abs/2109.07319](https://arxiv.org/abs/2109.07319)

    提出了一种轻量级框架InceptionXML，通过在embedding维度上重新分配卷积操作，应对短文本查询中的单词顺序缺失，同时提出了InceptionXML+框架，通过同步标签筛选器和极端分类器，改进了动态硬负采样技术。

    

    短文本数据对大量目标标签进行自动注释，被称为短文本极端分类，已经在许多应用中得到应用，包括相关搜索预测和产品推荐任务。本文提出了一种卷积架构InceptionXML，其轻量但功能强大，并且能够应对搜索和推荐任务中短文本查询中固有的缺乏单词顺序的特点。我们通过将卷积的操作沿着嵌入维度重新构建，而不是像传统CNNs一样沿着单词维度进行文本分类，证明了应用卷积的有效性。为了将我们的模型扩展到具有数百万标签的数据集，我们还提出了InceptionXML+框架，通过同步标签筛选器和极端分类器，改进了最近提出的动态硬负采样技术在标签筛选中的缺陷。

    arXiv:2109.07319v3 Announce Type: replace-cross  Abstract: Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classification, has found numerous applications including prediction of related searches and product recommendation tasks. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries encountered in search and recommendation tasks. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs for text classification. Towards scaling our model to datasets with millions of labels, we also propose InceptionXML+ framework which improves upon the shortcomings of the recently proposed dynamic hard-negative mining technique for label shortlisting by synchronizing the label-shortlister and extreme classifier. 
    
[^144]: 深度学习对宇宙结构形成的洞见

    Deep learning insights into cosmological structure formation

    [https://arxiv.org/abs/2011.10577](https://arxiv.org/abs/2011.10577)

    通过构建深度学习框架，研究了各向异性信息在初始条件中如何影响暗物质暗团的最终质量，并发现各向异性添加了一些额外信息量。

    

    通过宇宙模拟可以计算早期宇宙中的线性初始条件如何演变成为后来的暗物质扩展暗团。然而，对于这一复杂过程的理论理解仍然难以捉摸；特别是，初始条件中各向异性信息如何确定最终暗物质暗团质量的角色仍然是一个长期存在的难题。在本文中，我们构建了一个深度学习框架来探讨这个问题。我们训练了一个三维卷积神经网络（CNN）来从初始条件预测暗物质暗团的质量，并全面量化了初始密度场的各向同性和各向异性方面关于最终暗团质量的信息量。我们发现，各向异性确实比密度场的球面平均值所包含的信息量略微增加了一些，尽管在统计上是显著的。

    arXiv:2011.10577v3 Announce Type: replace-cross  Abstract: The evolution of linear initial conditions present in the early universe into extended halos of dark matter at late times can be computed using cosmological simulations. However, a theoretical understanding of this complex process remains elusive; in particular, the role of anisotropic information in the initial conditions in establishing the final mass of dark matter halos remains a long-standing puzzle. Here, we build a deep learning framework to investigate this question. We train a three-dimensional convolutional neural network (CNN) to predict the mass of dark matter halos from the initial conditions, and quantify in full generality the amounts of information in the isotropic and anisotropic aspects of the initial density field about final halo masses. We find that anisotropies add a small, albeit statistically significant amount of information over that contained within spherical averages of the density field about final 
    
[^145]: 基于贝叶斯鲁棒优化的模仿学习

    Bayesian Robust Optimization for Imitation Learning

    [https://arxiv.org/abs/2007.12315](https://arxiv.org/abs/2007.12315)

    提出了基于贝叶斯鲁棒优化的模仿学习方法，以在状态不确定性下同时考虑激进和保守的策略优化。

    

    模仿学习中的一个主要挑战是确定在演示的状态分布之外时代理应采取什么行动。逆强化学习（IRL）可以通过学习参数化奖励函数实现对新状态的泛化，但这些方法仍然面临对真实奖励函数和相应最优策略的不确定性。现有基于IRL的安全模仿学习方法使用maxmin框架处理这种不确定性，该框架在假设有一个对抗性奖励函数的情况下优化策略，而风险中立的IRL方法则优化均值或MAP奖励函数的策略。完全忽视风险可能会导致过于激进和不安全的策略，而完全以对抗性方式优化也是有问题的，因为它可能导致表现不佳的过度保守策略。为了在这两个极端之间建立桥梁，我们提出了一种基于贝叶斯鲁棒优化的方法，该方法在状态的置信区间内对策略进行优化。

    arXiv:2007.12315v4 Announce Type: replace  Abstract: One of the main challenges in imitation learning is determining what action an agent should take when outside the state distribution of the demonstrations. Inverse reinforcement learning (IRL) can enable generalization to new states by learning a parameterized reward function, but these approaches still face uncertainty over the true reward function and corresponding optimal policy. Existing safe imitation learning approaches based on IRL deal with this uncertainty using a maxmin framework that optimizes a policy under the assumption of an adversarial reward function, whereas risk-neutral IRL approaches either optimize a policy for the mean or MAP reward function. While completely ignoring risk can lead to overly aggressive and unsafe policies, optimizing in a fully adversarial sense is also problematic as it can lead to overly conservative policies that perform poorly in practice. To provide a bridge between these two extremes, we p
    
[^146]: Symplectic ODE-Net: 使用控制学习哈密顿动力学

    Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control

    [https://arxiv.org/abs/1909.12077](https://arxiv.org/abs/1909.12077)

    通过设计哈密顿动力学与控制，Symplectic ODE-Net (SymODEN)可以透明地学习潜在动力学，从而揭示系统的相关物理方面。

    

    在本文中，我们介绍了Symplectic ODE-Net（SymODEN），这是一个深度学习框架，可以从观测到的状态轨迹中推断出由普通微分方程（ODE）给定的物理系统的动力学。为了在更少的训练样本上实现更好的泛化，SymODEN通过以物理为基础的方式设计相关的计算图，引入适当的归纳偏差。特别地，我们通过设计哈密顿动力学与控制以透明的方式学习潜在动力学，从而可以利用这种动力学来揭示系统的相关物理方面，如质量和势能。此外，我们提出了一种参数化方法，可以在广义坐标数据嵌入到高维空间或者只能访问速度数据而不是广义动量时，也能强制执行这种哈密顿形式。这一框架通过提供可解释的、与物理一致的信息

    arXiv:1909.12077v5 Announce Type: replace  Abstract: In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consisten
    
[^147]: 优化大规模语言模型用于漏洞检测

    Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])

    [http://arxiv.org/abs/2401.17010](http://arxiv.org/abs/2401.17010)

    本文优化了大规模语言模型用于源代码中的漏洞检测任务，通过微调最先进的代码语言模型WizardCoder并改进其训练过程和策略，实现了对漏洞数据集的分类性能的提升。

    

    本文介绍了对大规模语言模型进行微调，并将其用于源代码中的漏洞检测的结果。我们利用最先进的语言模型StarCoder的改进版本WizardCoder，并通过进一步微调将其适应于漏洞检测任务。为了加速训练，我们修改了WizardCoder的训练过程，并探究了最佳的训练策略。针对负样本远多于正样本的不平衡数据集，我们还尝试了不同的技术来提高分类性能。微调后的WizardCoder模型在平衡和不平衡的漏洞数据集上在ROC AUC和F1度量上实现了改进，证明了将预训练的语言模型用于源代码中的漏洞检测的有效性。主要贡献包括对最先进的代码语言模型WizardCoder进行微调，提高其训练速度而不影响性能，并对训练过程和策略进行了优化。

    This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
    
[^148]: 多项式时间下对高斯数据进行统计子组不公平性审计的研究

    Polynomial time auditing of statistical subgroup fairness for Gaussian data. (arXiv:2401.16439v1 [cs.LG])

    [http://arxiv.org/abs/2401.16439](http://arxiv.org/abs/2401.16439)

    这篇论文研究了使用统计子组不公平性概念对分类器进行审计的问题，并给出了对高斯分布的审计结果。他们提供了一种替代方法来利用无偏学习的进展。

    

    我们研究了使用统计子组不公平性概念对分类器进行审计的问题。Kearns等人（2018）已经表明，审计组合子组公平性的问题与无偏学习一样困难。尽管对于这个问题没有已知的高效算法，但几乎所有解决对子组的统计歧视度量的工作都假设可以访问此问题的预言机。如果我们假设数据分布是高斯分布，甚至仅是寻常对数凹曲分布，那么最近的一系列工作已经发现了高效的无偏学习算法来学习半空间。不幸的是，Kearns等人给出的提升风格的规约要求无偏学习算法在可能不是寻常对数凹的重新加权分布上成功，即使原始数据分布是寻常对数凹的。在这项工作中，我们对高斯分布的审计结果给出了正面和负面的结果：在正面方面，我们提供了一个替代方法来利用这些无偏学习的进展。

    We study the problem of auditing classifiers with the notion of statistical subgroup fairness. Kearns et al. (2018) has shown that the problem of auditing combinatorial subgroups fairness is as hard as agnostic learning. Essentially all work on remedying statistical measures of discrimination against subgroups assumes access to an oracle for this problem, despite the fact that no efficient algorithms are known for it. If we assume the data distribution is Gaussian, or even merely log-concave, then a recent line of work has discovered efficient agnostic learning algorithms for halfspaces. Unfortunately, the boosting-style reductions given by Kearns et al. required the agnostic learning algorithm to succeed on reweighted distributions that may not be log-concave, even if the original data distribution was. In this work, we give positive and negative results on auditing for the Gaussian distribution: On the positive side, we an alternative approach to leverage these advances in agnostic l
    
[^149]: 深度强化学习中策略梯度的终极指南：理论、算法和实现

    The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations. (arXiv:2401.13662v1 [cs.LG])

    [http://arxiv.org/abs/2401.13662](http://arxiv.org/abs/2401.13662)

    本文提供了一个深度强化学习中策略梯度算法的综合指南，包括理论基础、实际实现和比较结果，并对正则化的益处进行了讨论。

    

    最近几年，在深度强化学习中提出了各种强大的策略梯度算法。虽然所有这些算法都建立在策略梯度定理的基础上，但具体的设计选择在算法之间有很大的差异。我们提供了一个整体的视角来概述在线策略梯度算法，以便理解它们的理论基础和实际实现。在这个概述中，我们包括了连续版本的策略梯度定理的详细证明、收敛结果和对实际算法的全面讨论。我们比较了连续控制环境中最重要的算法，并对正则化的益处提供了深入的见解。所有的代码都可以在https://github.com/Matt00n/PolicyGradientsJax获得。

    In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax.
    
[^150]: 用基于数据驱动的实用工具增强聚合器能力：利用聚合与分散的灵活性实现需求响应

    Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response. (arXiv:2401.10726v1 [eess.SY])

    [http://arxiv.org/abs/2401.10726](http://arxiv.org/abs/2401.10726)

    本研究通过优化聚合灵活性提供策略和评估HVAC系统的分散灵活性提供，为聚合器在可再生能源不确定性下实现需求响应提供了实用工具，从而实现了稳健的脱碳和增强能源系统的韧性。

    

    本研究探讨了在可再生能源带来不确定性的情况下，聚合器和建筑物居住者通过需求响应（DR）方案激活灵活性的关键相互作用，着重于实现稳健的脱碳和增强能源系统的韧性。首先，引入了一种在数据有限的环境中优化聚合灵活性提供策略的方法，利用离散傅里叶变换（DFT）和聚类技术识别建筑物居民的活动模式。其次，研究评估了DR事件期间供热通风空调（HVAC）系统的分散灵活性提供，采用机器学习和优化技术进行精确的设备级分析。第一种方法为聚合器在整个建筑物消费仅有一个智能电表的环境中提供灵活性服务提供了一条非侵入性途径。

    This study explores the crucial interplay between aggregators and building occupants in activating flexibility through Demand Response (DR) programs, with a keen focus on achieving robust decarbonization and fortifying the resilience of the energy system amidst the uncertainties presented by Renewable Energy Sources (RES). Firstly, it introduces a methodology of optimizing aggregated flexibility provision strategies in environments with limited data, utilizing Discrete Fourier Transformation (DFT) and clustering techniques to identify building occupant's activity patterns. Secondly, the study assesses the disaggregated flexibility provision of Heating Ventilation and Air Conditioning (HVAC) systems during DR events, employing machine learning and optimization techniques for precise, device-level analysis. The first approach offers a non-intrusive pathway for aggregators to provide flexibility services in environments of a single smart meter for the whole building's consumption, while t
    
[^151]: 无监督准确性估计下分布偏移的梯度特征化研究

    Characterising Gradients for Unsupervised Accuracy Estimation under Distribution Shift. (arXiv:2401.08909v1 [cs.LG])

    [http://arxiv.org/abs/2401.08909](http://arxiv.org/abs/2401.08909)

    本文研究了在分布偏移下，利用梯度信息对真实测试准确性进行预测的方法。通过分析分类层梯度范数，我们发现在无法泛化到测试数据集时，调整模型以获得更大的梯度范数是有效的。

    

    在变化的测试环境下，无法访问真实测试标签的情况下估计测试准确性是机器学习算法安全部署中一个具有挑战性但极其重要的问题。现有的方法依赖于神经网络的输出或提取特征的信息来建立与真实测试准确性相关的估计分数。本文通过实证和理论研究探讨了梯度信息如何在分布偏移下对真实测试准确性进行预测。具体而言，我们使用从经过一次梯度步长的交叉熵损失函数后反向传播的分类层梯度范数来进行研究。我们的关键思想是，在模型在分布偏移下无法泛化到测试数据集时，应当调整模型以获得更大的梯度范数。我们提供理论见解，突出了这种方法的主要要素。

    Estimating test accuracy without access to the ground-truth test labels under varying test environments is a challenging, yet extremely important problem in the safe deployment of machine learning algorithms. Existing works rely on the information from either the outputs or the extracted features of neural networks to formulate an estimation score correlating with the ground-truth test accuracy. In this paper, we investigate--both empirically and theoretically--how the information provided by the gradients can be predictive of the ground-truth test accuracy even under a distribution shift. Specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our key idea is that the model should be adjusted with a higher magnitude of gradients when it does not generalize to the test dataset with a distribution shift. We provide theoretical insights highlighting the main ingredients of such an approach en
    
[^152]: 基于小波启发的多尺度图卷积循环网络用于交通预测

    Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting. (arXiv:2401.06040v1 [cs.LG])

    [http://arxiv.org/abs/2401.06040](http://arxiv.org/abs/2401.06040)

    本论文提出了一种基于小波启发的多尺度图卷积循环网络，用于交通预测。该方法将多尺度分析方法和深度学习方法相结合，对交通数据中的多尺度结构进行建模，并展现了较好的性能。

    

    交通预测是智能交通系统的基础。时空图神经网络在交通预测中展现出了最先进的性能。然而，这些方法并没有明确地对交通数据中的某些自然特征进行建模，比如包含不同粒度或尺度上的空间和时间变化的多尺度结构。为此，我们提出了一种基于小波启发的图卷积循环网络（WavGCRN），将多尺度分析（MSA）方法和深度学习（DL）方法相结合。在WavGCRN中，交通数据通过离散小波变换（DWT）被分解为时频分量，构建了多流输入结构；然后使用图卷积循环网络（GCRNs）作为编码器对每个流进行特征提取，提取不同尺度的时空特征；最后，可学习的逆DWT和GCRN被结合为解码器，融合信息。

    Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the inf
    
[^153]: ALEXR:一种用于凸有限和耦合组成随机优化的最优单循环算法

    ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization. (arXiv:2312.02277v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2312.02277](http://arxiv.org/abs/2312.02277)

    本文提出了一种名为ALEXR的高效算法，用于解决凸有限和耦合组成随机优化问题。此算法在解决平滑和非平滑问题时具有优越的收敛速度，并且可应用于多个领域，包括组分布鲁棒优化、不平衡数据学习、强化学习和排序学习。

    

    本文重新审视了一类具有多个应用的凸有限和耦合组成随机优化（cFCCO）问题，包括组分布鲁棒优化（GDRO），不平衡数据学习，强化学习和排序学习。为了更好地解决这些问题，我们引入了一个高效的单循环原始-对偶块坐标近端算法，称为ALEXR。该算法利用块坐标随机镜像上升更新对偶变量和随机近端梯度下降更新原始变量。我们在平滑和非平滑函数条件下建立了ALEXR在凸和强凸情况下的收敛速度，这不仅改进了以前在平滑cFCCO问题上的最佳速度，还扩展了cFCCO的范围，用于解决更具挑战性的非平滑问题，如GDRO的对偶形式。最后，我们提供了较低的复杂性下界，以证明算法具有很强的效率。

    This paper revisits a class of convex Finite-Sum Coupled Compositional Stochastic Optimization (cFCCO) problems with many applications, including group distributionally robust optimization (GDRO), learning with imbalanced data, reinforcement learning, and learning to rank. To better solve these problems, we introduce an efficient single-loop primal-dual block-coordinate proximal algorithm, dubbed ALEXR. This algorithm leverages block-coordinate stochastic mirror ascent updates for the dual variable and stochastic proximal gradient descent updates for the primal variable. We establish the convergence rates of ALEXR in both convex and strongly convex cases under smoothness and non-smoothness conditions of involved functions, which not only improve the best rates in previous works on smooth cFCCO problems but also expand the realm of cFCCO for solving more challenging non-smooth problems such as the dual form of GDRO. Finally, we present lower complexity bounds to demonstrate that the con
    
[^154]: 生成模型的最佳限制预算拒绝采样方法

    Optimal Budgeted Rejection Sampling for Generative Models. (arXiv:2311.00460v1 [cs.LG])

    [http://arxiv.org/abs/2311.00460](http://arxiv.org/abs/2311.00460)

    该论文提出了一种最佳限制预算拒绝采样方法（OBRS），可显著改善生成模型的样本质量和多样性。通过将采样方案与训练过程相结合，该方法在给定采样预算情况下，对于任何真实分布和拒绝后分布之间的f-散度都是最优的。

    

    最近提出了拒绝采样方法来改善基于鉴别器的生成模型的性能。然而，这些方法只在无限采样预算下是最优的，并且通常应用于与拒绝过程独立训练的生成器。我们首先提出了一种Optimal Budgeted Rejection Sampling (OBRS)方案，该方案在给定采样预算情况下，对于真实分布和拒绝后分布之间的任何f-散度证明是最优的。其次，我们提出了一种端到端方法，将采样方案融入训练过程，进一步提高模型的整体性能。通过实验证明和支持的理论，我们展示了这些方法在显著提高样本质量和多样性方面的有效性。

    Rejection sampling methods have recently been proposed to improve the performance of discriminator-based generative models. However, these methods are only optimal under an unlimited sampling budget, and are usually applied to a generator trained independently of the rejection procedure. We first propose an Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal with respect to \textit{any} $f$-divergence between the true distribution and the post-rejection distribution, for a given sampling budget. Second, we propose an end-to-end method that incorporates the sampling scheme into the training procedure to further enhance the model's overall performance. Through experiments and supporting theory, we show that the proposed methods are effective in significantly improving the quality and diversity of the samples.
    
[^155]: 采用有噪声树度量的优化传输方法

    Optimal Transport for Measures with Noisy Tree Metric. (arXiv:2310.13653v1 [stat.ML])

    [http://arxiv.org/abs/2310.13653](http://arxiv.org/abs/2310.13653)

    本文提出了一种针对树度量有噪声的优化传输方法，通过引入新的不确定性集合，解决了实际应用中树结构扰动的问题。

    

    本研究探讨了在树度量空间上支持的概率测度的优化传输（OT）问题。已知这种OT问题（即树-瓦瓦斯坦（TW））具有闭合形式表达式，但基本上取决于输入测度支持上的底层树结构。然而，在实际操作中，由于噪声或对抗性测量，给定的树结构可能会被扰动。为了缓解这个问题，我们采取了最大-最小鲁棒OT方法，该方法考虑了在一个树度量的不确定性集合上两个输入测度之间的最大可能距离。总体上说，由于其非凸性和非光滑性，这种方法很难计算，即便是在支持为1维空间的测度情况下，这妨碍了它的实际应用，特别是在大规模情景下。在本文中，我们从边缘删除/添加的角度提出了一种新颖的树度量的不确定性集合，这个集合在一个优雅的框架下涵盖了多样的树结构。

    We study optimal transport (OT) problem for probability measures supported on a tree metric space. It is known that such OT problem (i.e., tree-Wasserstein (TW)) admits a closed-form expression, but depends fundamentally on the underlying tree structure over supports of input measures. In practice, the given tree structure may be, however, perturbed due to noisy or adversarial measurements. In order to mitigate this issue, we follow the max-min robust OT approach which considers the maximal possible distances between two input measures over an uncertainty set of tree metrics. In general, this approach is hard to compute, even for measures supported in $1$-dimensional space, due to its non-convexity and non-smoothness which hinders its practical applications, especially for large-scale settings. In this work, we propose \emph{novel uncertainty sets of tree metrics} from the lens of edge deletion/addition which covers a diversity of tree structures in an elegant framework. Consequently, 
    
[^156]: SalUn：通过基于梯度的权重显著性增强机器遗忘在图像分类和生成中的效果

    SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])

    [http://arxiv.org/abs/2310.12508](http://arxiv.org/abs/2310.12508)

    这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。

    

    随着数据法规的不断发展，机器遗忘（MU）已成为增强当前AI模型的信任和安全性的重要工具。然而，现有的MU方法通常在遗忘精度、稳定性和跨领域适用性方面存在局限。为了解决这些挑战，我们引入了MU中的“权重显著性”概念，借鉴了模型解释中的输入显著性。这一创新将MU的关注点从整个模型引导到了具体的模型权重上，提高了其效果和效率。我们称之为显著性遗忘（SalUn）的方法将其与“精确”遗忘（在删除遗忘数据集后从头开始重新训练模型）的性能差距缩小。据我们所知，SalUn是第一个能够在图像分类和生成中有效消除遗忘数据、类别或概念影响的有原则的MU方法。例如，SalUn可在图片分类和生成任务中擦除遗忘数据、类别或概念。

    With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
    
[^157]: 一种基于机器学习的概率暴露模型的德国高分辨率室内氡气地图

    A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model. (arXiv:2310.11143v1 [stat.ML])

    [http://arxiv.org/abs/2310.11143](http://arxiv.org/abs/2310.11143)

    本研究提出了一种基于机器学习的概率暴露模型，可以更准确地估计德国室内氡气分布，并具有更高的空间分辨率。

    

    室内氡气是一种致癌的放射性气体，可以在室内积累。通常情况下，全国范围内的室内氡暴露是基于广泛的测量活动估计得来的。然而，样本的特征往往与人口特征不同，这是由于许多相关因素，如地质源氡气的可用性或楼层水平。此外，样本大小通常不允许以高空间分辨率进行暴露估计。我们提出了一种基于模型的方法，可以比纯数据方法更加现实地估计室内氡分布，并具有更高的空间分辨率。我们采用了两阶段建模方法：1）应用分位数回归森林，使用环境和建筑数据作为预测因子，估计了德国每个住宅楼的每个楼层的室内氡概率分布函数；2）使用概率蒙特卡罗抽样技术使它们组合和。

    Radon is a carcinogenic, radioactive gas that can accumulate indoors. Indoor radon exposure at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sample often differ from the characteristics of the population due to the large number of relevant factors such as the availability of geogenic radon or floor level. Furthermore, the sample size usually does not allow exposure estimation with high spatial resolution. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. We applied a two-stage modelling approach: 1) a quantile regression forest using environmental and building data as predictors was applied to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany; (2) a probabilistic Monte Carlo sampling technique enabled the combination and
    
[^158]: 基于结构的神经切线内核的快速图结构压缩

    Fast Graph Condensation with Structure-based Neural Tangent Kernel. (arXiv:2310.11046v1 [cs.LG])

    [http://arxiv.org/abs/2310.11046](http://arxiv.org/abs/2310.11046)

    本文提出了一种以数据为中心的解决方案，将大型图数据集压缩为较小的集合而不会损失GNN的预测性能。通过将图结构压缩问题转化为核岭回归任务，利用基于结构的神经切线内核来捕捉图的拓扑结构。

    

    互联网技术的快速发展造成了大量的图结构数据。图神经网络（GNN）作为一种有效的图挖掘方法，在处理大规模图数据时会导致大量的计算资源开销。本文提出了一种以数据为中心的解决方案，将大型图数据集压缩为较小的集合，而不会损失GNN的预测性能。然而，现有的方法通过计算密集型的双层优化架构来压缩图结构数据，同样也会带来巨大的计算开销。本文将图结构压缩问题改为核岭回归任务，而不是在双层优化的内循环中迭代训练GNN。具体来说，本文提出了一种新的图数据集压缩框架（GC-SNTK），其中开发了一种基于结构的神经切线内核（SNTK）来捕捉图的拓扑结构。

    The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and s
    
[^159]: 自学优化器（STOP）：递归自我改进的代码生成

    Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])

    [http://arxiv.org/abs/2310.02304](http://arxiv.org/abs/2310.02304)

    本文提出了一种自学优化器（STOP），通过递归自我改进的代码生成，使用融合了语言模型的脚手架程序来改进自身，从而生成性能更好的程序。

    

    最近几年的人工智能系统（例如思维树和程序辅助语言模型）取得了一些重要进展，通过提供一个“脚手架”程序来解决问题，该程序构建了多次调用语言模型以生成更好的输出。脚手架程序通常使用Python等编程语言编写。在这项工作中，我们使用了一个融合了语言模型的脚手架程序来改进自身。我们从一个种子“改进器”开始，通过多次查询语言模型并返回最佳解决方案，根据给定的效用函数来改进输入程序。然后，我们运行这个种子改进器来改进自身。在一系列细分任务中，得到的改进改进器生成的程序在性能上明显优于种子改进器。随后，我们对语言模型提出的各种自我改进策略进行了分析，包括波束搜索、遗传算法和模拟退火。由于语言模型本身没有改变，这并不是一种增长领域。

    Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
    
[^160]: 完美预测储备计算在自回归时间序列数据中的数学结构

    Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data. (arXiv:2310.00290v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00290](http://arxiv.org/abs/2310.00290)

    这篇论文研究了储备计算在自回归时间序列数据中的数学结构，并揭示了其隐藏的权重矩阵结构，以实现对AR类型时间序列数据的完美预测。

    

    储备计算（RC）是一种递归神经网络（RNN），毫无疑问，RC将越来越广泛地用于构建时间序列数据的未来预测模型，具有低训练成本、高速度和高计算能力。然而，对于RC神经网络的数学结构的研究直到最近才开始。Bollt（2021）阐明了自回归（AR）模型对于理解RC神经网络的数学结构的必要性，并指出Wold分解定理是理解这些结构的里程碑。在铭记这一著名结果的基础上，本文阐明了RC神经网络中输入和循环权重矩阵的隐藏结构，并展示了这些结构对于AR类型的时间序列数据实现了完美预测。

    Reservoir Computing (RC) is a type of recursive neural network (RNN), and there can be no doubt that the RC will be more and more widely used for building future prediction models for time-series data, with low training cost, high speed and high computational power. However, research into the mathematical structure of RC neural networks has only recently begun. Bollt (2021) clarified the necessity of the autoregressive (AR) model for gaining the insight into the mathematical structure of RC neural networks, and indicated that the Wold decomposition theorem is the milestone for understanding of these. Keeping this celebrated result in mind, in this paper, we clarify hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures attain perfect prediction for the AR type of time series data.
    
[^161]: 将大型语言模型推至6G边缘：视野、挑战和机遇

    Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])

    [http://arxiv.org/abs/2309.16739](http://arxiv.org/abs/2309.16739)

    本文探讨了将大型语言模型(LLMs)部署在6G边缘的潜力和挑战。我们介绍了由LLMs支持的关键应用，并从响应时间、带宽成本和数据隐私等方面分析了云端部署面临的问题。我们提出了6G移动边缘计算(MEC)系统可能解决这些问题的方案，并讨论了边缘训练和边缘推理的创新技术。

    

    大型语言模型(LLMs)展示了显著的能力，正在改变人工智能的发展并有可能塑造我们的未来。然而，由于LLMs的多模态特性，当前的基于云的部署面临着一些关键挑战：1) 响应时间长；2) 高带宽成本；以及3) 违反数据隐私。6G移动边缘计算(MEC)系统可能解决这些迫切问题。本文探讨了在6G边缘部署LLMs的潜力。我们首先介绍了由多模态LLMs提供支持的关键应用，包括机器人技术和医疗保健，以突出在终端用户附近部署LLMs的需求。然后，我们确定了在边缘部署LLMs时面临的关键挑战，并设想了适用于LLMs的6G MEC架构。此外，我们深入探讨了两个设计方面，即LLMs的边缘训练和边缘推理。在这两个方面，考虑到边缘的固有资源限制，我们讨论了各种前沿技术。

    Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
    
[^162]: 深度残差网络的隐式正则化与神经常微分方程的关联

    Implicit regularization of deep residual networks towards neural ODEs. (arXiv:2309.01213v1 [stat.ML])

    [http://arxiv.org/abs/2309.01213](http://arxiv.org/abs/2309.01213)

    本文建立了深度残差网络向神经常微分方程的隐式正则化，通过对用梯度流训练的非线性网络的研究，证明了在网络以神经常微分方程的离散化形式初始化后，这种离散化将在整个训练过程中保持不变，并提供了收敛性的条件。

    

    残差神经网络是先进的深度学习模型。它们的连续深度模拟称为神经常微分方程（ODE），也被广泛使用。尽管它们取得了成功，但离散模型与连续模型之间的联系仍缺乏坚实的数学基础。在本文中，我们通过建立一个针对用梯度流训练的非线性网络的深度残差网络向神经常微分方程的隐式正则化来朝着这个方向迈出了一步。我们证明，如果网络的初始化是神经常微分方程的离散化，则这种离散化在整个训练过程中保持不变。我们的结果对于有限的训练时间和训练时间趋于无穷大都成立，只要网络满足Polyak-Lojasiewicz条件。重要的是，这个条件适用于一个残差网络家族，其中残差是两层感知机，在宽度上只是线性超参数化，并且暗示了梯度流的收敛性。

    Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this article, we take a step in this direction by establishing an implicit regularization of deep residual networks towards neural ODEs, for nonlinear networks trained with gradient flow. We prove that if the network is initialized as a discretization of a neural ODE, then such a discretization holds throughout training. Our results are valid for a finite training time, and also as the training time tends to infinity provided that the network satisfies a Polyak-Lojasiewicz condition. Importantly, this condition holds for a family of residual networks where the residuals are two-layer perceptrons with an overparameterization in width that is only linear, and implies the convergence of gradient flow to
    
[^163]: EVE: 使用遮蔽预测和模态感知的高效视觉-语言预训练

    EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])

    [http://arxiv.org/abs/2308.11971](http://arxiv.org/abs/2308.11971)

    本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。

    

    在本文中，我们介绍了一种名为EVE的高效视觉-语言基础模型，它是由一种统一的Transformer进行预训练的统一多模态模型。EVE通过在图像-文本对上进行遮蔽信号建模来统一视觉和语言的预训练任务，以重建可见信号，即图像像素和文本标记。通过集成模态感知的稀疏专家混合模块，EVE在一个共享的Transformer网络中编码了视觉和语言，并通过选择性地切换到不同的专家来捕捉模态特定信息。

    Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t
    
[^164]: 后门联邦学习：通过污染后门关键层

    Backdoor Federated Learning by Poisoning Backdoor-Critical Layers. (arXiv:2308.04466v1 [cs.CR])

    [http://arxiv.org/abs/2308.04466](http://arxiv.org/abs/2308.04466)

    该论文研究了后门联邦学习中后门关键层的存在，并提出了一种针对这些层的新型后门攻击方法，旨在在各种防御策略下实现攻击效果和隐蔽性之间的平衡。

    

    联邦学习（FL）已被广泛应用于在分布式设备上进行敏感数据的机器学习训练。然而，分散式学习范式和FL的异质性进一步扩展了后门攻击的攻击面。现有的FL攻击和防御方法通常会关注整个模型，但没有一个方法意识到后门关键（BC）层的存在，后门关键层是指控制模型漏洞的一小部分层。攻击BC层可以达到攻击整个模型的效果，但被最先进的防御手段发现的机会要小得多。本文提出了一个从攻击者的角度识别和验证BC层的普适性方法。基于识别出的BC层，我们精心设计了一种新的后门攻击方法，根据不同的防御策略自适应地寻求攻击效果和隐蔽性之间的平衡。大量实验表明，

    Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that 
    
[^165]: 基于广义多维比较的光谱排名推断

    Spectral Ranking Inferences based on General Multiway Comparisons. (arXiv:2308.02918v1 [stat.ME])

    [http://arxiv.org/abs/2308.02918](http://arxiv.org/abs/2308.02918)

    本文研究了使用光谱方法在广义多维比较中估计和量化未观察到的比较实体的偏好分数的性能，并揭示了光谱估计量与最大似然估计量之间的关系。

    

    本文研究了在一个非常普遍和更加真实的情景中，使用光谱方法对未观察到的比较实体的偏好分数进行估计和不确定性量化的性能。在这种情况下，比较图由可能具有异构大小的超边组成，对于给定的超边，比较数量可能仅为1。这种设置在实际应用中普遍存在，避免了需要指定图的随机性以及在常用的Bradley-Terry-Luce (BTL)或Plackett-Luce (PL)模型中施加的限制性均匀采样假设。此外，在适用BTL或PL模型的情况下，我们揭示了光谱估计量与最大似然估计量（MLE）之间的关系。我们发现，通过应用从等权重传统光谱方法估计得到的最佳加权，可以实现与MLE相同的渐近效率的双步光谱方法。考虑到渐近情况，

    This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptot
    
[^166]: SABRE:强鲁棒贝叶斯点对点联邦学习

    SABRE: Robust Bayesian Peer-to-Peer Federated Learning. (arXiv:2308.02747v1 [cs.LG])

    [http://arxiv.org/abs/2308.02747](http://arxiv.org/abs/2308.02747)

    SABRE是一种强鲁棒性变分贝叶斯点对点联邦学习框架，通过新的聚合方法克服了现有框架的局限性，在非IID环境中表现良好，对数据/模型攻击具备鲁棒性，在图像分类数据上优于现有框架。

    

    我们引入了SABRE，一种新颖的强鲁棒性变分贝叶斯点对点联邦学习框架。我们分析了已知的变分贝叶斯点对点联邦学习框架（BayP2PFL）的抗攻击性，随后证明了BayP2PFL在面对这些攻击时不具备鲁棒性。然后，我们提出了SABRE聚合方法来克服现有框架的局限性。SABRE在非IID环境中表现良好，不要求大多数良性节点多于受损节点，并且在良性环境下甚至优于基准算法。我们从去中心化线性回归设置中在理论上证明了我们算法对数据/模型攻击的鲁棒性。对基准图像分类数据进行的概念证明评估显示了SABRE在各种攻击下优于现有框架的卓越性能。

    We introduce SABRE, a novel framework for robust variational Bayesian peer-to-peer federated learning. We analyze the robustness of the known variational Bayesian peer-to-peer federated learning framework (BayP2PFL) against poisoning attacks and subsequently show that BayP2PFL is not robust against those attacks. The new SABRE aggregation methodology is then devised to overcome the limitations of the existing frameworks. SABRE works well in non-IID settings, does not require the majority of the benign nodes over the compromised ones, and even outperforms the baseline algorithm in benign settings. We theoretically prove the robustness of our algorithm against data / model poisoning attacks in a decentralized linear regression setting. Proof-of-Concept evaluations on benchmark data from image classification demonstrate the superiority of SABRE over the existing frameworks under various poisoning attacks.
    
[^167]: $\lambda$-AC：学习连续状态空间强化学习中的潜在决策感知模型

    $\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])

    [http://arxiv.org/abs/2306.17366](http://arxiv.org/abs/2306.17366)

    这项研究提出了一种$\lambda$-AC算法，通过学习连续状态空间中的潜在决策感知模型，实现了决策驱动的强化学习。通过理论和实证研究，确定了决策感知强化学习模型的必要组成部分，并展示了设计选择对算法性能的重要影响。

    

    决策感知模型学习的思想，在模型驱动的强化学习中变得越来越重要，即模型在决策制定时应该是准确的。尽管已经建立了一些有希望的理论结果，但是在连续控制问题中，利用决策感知损失的算法的实际性能仍然不足。本文研究了决策感知强化学习模型所需的必要组成部分，并展示了能够实现良好算法性能的设计选择。为此，我们对该领域的重要算法思想进行了理论和实证研究。我们强调，在MuZero系列工作中所建立的经验性设计决策对于相关算法的良好性能至关重要，并展示了在随机环境中，不同的价值感知算法实例之间行为差异。在这些见解的基础上，我们提出了潜在模型驱动决策的算法，称为$\lambda$-AC。

    The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
    
[^168]: 自监督学习在时间序列分析中的应用：分类、进展和前景

    Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10125](http://arxiv.org/abs/2306.10125)

    自监督学习（SSL）在时间序列分析中的应用取得了显著性能，通过减少对标注数据的依赖，即使只有少量标注数据，也能实现高性能。

    

    自监督学习（SSL）最近在各种时间序列任务上取得了令人瞩目的性能。SSL最突出的优势是减少对标注数据的依赖。基于预训练和微调策略，即使只有少量标注数据，也可以实现高性能。与许多关于计算机视觉和自然语言处理的自监督学习综述相比，目前还缺乏针对时间序列SSL的综述。为了填补这一空白，本文回顾了当前时间序列数据中的自监督学习（SSL）方法的最新研究进展。为此，我们首先全面回顾了与自监督学习（SSL）和时间序列相关的现有综述，然后通过总结从生成型、对比型和对抗型三个角度对现有时间序列自监督学习方法进行了新的分类。这些方法进一步细分为十个子类，详细回顾和讨论了它们的关键直觉、主要框架、优势和限制。

    Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
    
[^169]: 从隐私化数据中训练生成模型

    Training generative models from privatized data. (arXiv:2306.09547v1 [cs.LG])

    [http://arxiv.org/abs/2306.09547](http://arxiv.org/abs/2306.09547)

    介绍了一种在隐私化数据上训练GAN的框架，使用熵正则化Wasserstein距离去噪可以缓解正则化偏差和隐私化噪声的影响，提高模型有效性。

    

    本文介绍了一种在差分隐私化数据上训练生成式对抗网络（GAN）的框架，其中采用了常见的加性噪声机制（如拉普拉斯噪声和高斯噪声）对数据进行差分隐私保护。我们展示了当使用熵正则化Wasserstein距离来去噪数据分布时，这种方法可以唯一地缓解正则化偏差和隐私化噪声的影响，从而提高模型的有效性。同时，我们还分析了所提出的方法并提供了样本复杂度结果和实验证据以支持其可靠性。

    Local differential privacy (LDP) is a powerful method for privacy-preserving data collection. In this paper, we develop a framework for training Generative Adversarial Networks (GAN) on differentially privatized data. We show that entropic regularization of the Wasserstein distance -- a popular regularization method in the literature that has been often leveraged for its computational benefits -- can be used to denoise the data distribution when data is privatized by common additive noise mechanisms, such as Laplace and Gaussian. This combination uniquely enables the mitigation of both the regularization bias and the effects of privatization noise, thereby enhancing the overall efficacy of the model. We analyse the proposed method, provide sample complexity results and experimental evidence to support its efficacy.
    
[^170]: 在多模态人工智能中保护数据：一种差分隐私方法用于CLIP训练

    Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training. (arXiv:2306.08173v1 [cs.LG])

    [http://arxiv.org/abs/2306.08173](http://arxiv.org/abs/2306.08173)

    本文提出了一种差分隐私的CLIP模型（Dp-CLIP），旨在保护多模态AI任务中的数据隐私，同时保持模型准确性。该方法在基准数据集上得到了验证，并表明其与标准非私有CLIP模型相比具有同等的性能。

    

    多模态人工智能的成功引发了视觉和语言任务中数据隐私的关注。虽然CLIP通过对图像和文本的联合训练彻底改变了多模态学习，但其可能无意中披露敏感信息的潜力需要集成保护隐私的机制。我们引入了对比语言-图像预训练（CLIP）模型的差分隐私改进，有效地解决了隐私问题，同时保持准确性。我们提出的方法Dp-CLIP在包括图像分类和视觉问答等多样的视觉和语言任务的基准数据集上进行了严格评估。我们证明了我们的方法保持了与标准的非私有CLIP模型同等的性能。此外，我们在线性表示设置下分析了我们提出的算法。我们推导了算法的收敛速度，并展示了在梯度被剪辑时实用性和隐私之间的权衡。

    The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks. While CLIP has revolutionized multimodal learning through joint training on images and text, its potential to unintentionally disclose sensitive information necessitates the integration of privacy-preserving mechanisms. We introduce a differentially private adaptation of the Contrastive Language-Image Pretraining (CLIP) model that effectively addresses privacy concerns while retaining accuracy. Our proposed method, Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse vision-and-language tasks such as image classification and visual question answering. We demonstrate that our approach retains performance on par with the standard non-private CLIP model. Furthermore, we analyze our proposed algorithm under linear representation settings. We derive the convergence rate of our algorithm and show a trade-off between utility and privacy when gradients are clipped pe
    
[^171]: 自然语言处理中社会人口统计偏见的调查

    Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])

    [http://arxiv.org/abs/2306.08158](http://arxiv.org/abs/2306.08158)

    本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。

    

    深度神经网络在训练过程中往往会学习到非预期的偏见，这在实际应用中可能会产生有害的影响。本文对209篇关于NLP模型中偏见的论文进行了调查，其中大部分论文涉及社会人口统计偏见。为了更好地理解偏见与真实世界的危害之间的区别，我们借鉴心理学和行为经济学的思想，提出了社会人口统计偏见的定义。我们确定了NLP偏见研究的三个主要类别：偏见类型、量化偏见和去偏见。我们认为当前对于量化偏见的方法存在可靠性问题，许多偏见度量并不涉及真实世界中的偏见，当前的去偏见技术是表面的，只是隐藏了偏见，而不是真正去除它。最后，我们提供了未来工作的建议。

    Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
    
[^172]: 带权重空间上功能性输入映射的全局普适逼近

    Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])

    [http://arxiv.org/abs/2306.03303](http://arxiv.org/abs/2306.03303)

    本文提出了功能性输入神经网络，可以在带权重空间上完成全局函数逼近。这一方法适用于连续函数的推广，还可用于路径空间函数的逼近，同时也可以逼近线性函数签名。

    

    我们引入了所谓的功能性输入神经网络，定义在可能是无限维带权重空间上，其值也在可能是无限维的输出空间中。为此，我们使用一个加性族作为隐藏层映射，以及一个非线性激活函数应用于每个隐藏层。依靠带权重空间上的Stone-Weierstrass定理，我们可以证明连续函数的推广的全局普适逼近结果，超越了常规紧集逼近。这特别适用于通过功能性输入神经网络逼近（非先见之明的）路径空间函数。作为带权Stone-Weierstrass定理的进一步应用，我们证明了线性函数签名的全局普适逼近结果。我们还在这个设置中引入了高斯过程回归的观点，并展示了签名内核的再生核希尔伯特空间是某些高斯过程的Cameron-Martin空间。

    We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
    
[^173]: 联邦领域泛化：一项调查研究

    Federated Domain Generalization: A Survey. (arXiv:2306.01334v1 [cs.LG])

    [http://arxiv.org/abs/2306.01334](http://arxiv.org/abs/2306.01334)

    该论文调查了联邦领域泛化的研究领域，提到了联邦学习和领域泛化技术的优势，以及在泛化联邦模型时面临的挑战。

    

    机器学习通常依赖于一个假设，即训练和测试的分布是相同的，数据是集中存储供训练和测试之用。然而，在现实场景中，分布可能存在显著差异，并且数据通常分布在不同的设备、组织或边缘节点上。因此，必须开发能够有效泛化到未见过的分布，并且数据分布在不同领域的模型。为了应对这一挑战，近年来出现了对联邦领域泛化 (FDG) 的极大兴趣。FDG 结合了联邦学习 (FL) 和领域泛化 (DG) 技术的优点，使多个源领域能够协作学习一个能够直接泛化到未见过的领域而又保持数据隐私的模型。然而，在领域转移下泛化联邦模型是一个技术上具有挑战性的问题，目前还没有得到充分的关注。

    Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research 
    
[^174]: 二分分类中追索权的风险

    The Risks of Recourse in Binary Classification. (arXiv:2306.00497v1 [cs.LG])

    [http://arxiv.org/abs/2306.00497](http://arxiv.org/abs/2306.00497)

    研究发现，在二分分类中提供追索权会增加错误率，导致更多错误的发生。提供算法追索权可能也会在系统级别上给予不利。

    

    算法追索权提供解释，以帮助用户推翻机器学习系统的不利决策。但到目前为止，很少有人关注提供追索权是否有益。我们引入了一个抽象的学习理论框架，比较了具有和没有算法追索权的分类的风险（即期望损失）。这使我们能够回答在整个人群水平上提供追索权何时有益或有害的问题。令人惊讶的是，我们发现在许多可信的情况下，提供追索权反而会有害，因为它将用户推向更高类别不确定性的区域，因此会导致更多的错误。我们进一步研究了部署分类器的一方是否有动机针对提供追索权的情况进行策略规划，我们发现有时候确实存在这种现象，这对他们的用户不利。因此，提供算法追索权在系统级别上可能也是有害的。

    Algorithmic recourse provides explanations that help users overturn an unfavorable decision by a machine learning system. But so far very little attention has been paid to whether providing recourse is beneficial or not. We introduce an abstract learning-theoretic framework that compares the risks (i.e. expected losses) for classification with and without algorithmic recourse. This allows us to answer the question of when providing recourse is beneficial or harmful at the population level. Surprisingly, we find that there are many plausible scenarios in which providing recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty and therefore leads to more mistakes. We further study whether the party deploying the classifier has an incentive to strategize in anticipation of having to provide recourse, and we find that sometimes they do, to the detriment of their users. Providing algorithmic recourse may therefore also be harmful at the systemic level
    
[^175]: 图的Bures-Wasserstein平均值

    Bures-Wasserstein Means of Graphs. (arXiv:2305.19738v1 [stat.ML])

    [http://arxiv.org/abs/2305.19738](http://arxiv.org/abs/2305.19738)

    该论文提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。实验结果表明，在各种任务中都有很好的表现。

    

    在机器学习和统计学中，找到采样数据的平均值是一项基本任务。然而，在数据样本为图对象的情况下，定义平均值是一项困难的任务。我们提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。通过在此嵌入空间中找到平均值，我们可以恢复保留结构信息的平均图。我们确定了新的图平均值的存在和唯一性，并提供了一个迭代算法来计算它。为了展示我们的框架作为机器学习中的一个有价值的工具，我们在各种任务中进行了评估，包括结构化图的k-means聚类、功能性脑网络的分类以及多层图的半监督节点分类。我们的实验结果表明，我们的方法实现了一致的p。

    Finding the mean of sampled data is a fundamental task in machine learning and statistics. However, in cases where the data samples are graph objects, defining a mean is an inherently difficult task. We propose a novel framework for defining a graph mean via embeddings in the space of smooth graph signal distributions, where graph similarity can be measured using the Wasserstein metric. By finding a mean in this embedding space, we can recover a mean graph that preserves structural information. We establish the existence and uniqueness of the novel graph mean, and provide an iterative algorithm for computing it. To highlight the potential of our framework as a valuable tool for practical applications in machine learning, it is evaluated on various tasks, including k-means clustering of structured graphs, classification of functional brain networks, and semi-supervised node classification in multi-layer graphs. Our experimental results demonstrate that our approach achieves consistent p
    
[^176]: 逃离平庸：双层神经网络如何在 SGD 下学习困难的单指标模型

    Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. (arXiv:2305.18502v1 [stat.ML])

    [http://arxiv.org/abs/2305.18502](http://arxiv.org/abs/2305.18502)

    研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。

    

    本研究探讨了在随机梯度下降（SGD）下双层神经网络学习单指数目标函数的样本复杂度问题，重点关注在初始化时存在许多平坦方向的挑战性情况。已经有研究表明，这种情况下通常需要 $n=O(d\log{d})$ 个样本。但是，我们提供了在高维度和不同宽度情况下的前置因子的精确结果。值得注意的是，我们的发现表明，在这个问题类中，过参数化只会增加一定因子的收敛性。这些见解基于 SGD 动态的低维度随机过程模型，其中逃离平庸等同于计算出站出时间。然而，我们证明这个过程的确定性近似足以代表逃逸时间，这意味着在这种情况下随机性的作用可能很小。

    This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario.
    
[^177]: 基于点积的层次聚类可以恢复隐藏的树形结构

    Hierarchical clustering with dot products recovers hidden tree structure. (arXiv:2305.15022v1 [stat.ML])

    [http://arxiv.org/abs/2305.15022](http://arxiv.org/abs/2305.15022)

    本文发现一种基于点积的层次聚类算法，可以通过最大平均点积合并聚类，并且输出的树结构可用于准确估计数据的生成层次结构，树形恢复性能优于现有方法。

    

    本文提供了一个对于已有凝聚聚类算法的新视角，专注于层次结构的恢复。我们建议一种简单的标准算法变体，其中聚类是通过最大平均点积而不是最小距离或簇内方差来合并的。我们证明了此算法输出的树可以作为数据生成层次结构的可靠估计。关键技术创新在于理解模型中的层次信息如何转化为可从数据中恢复的树形几何信息，并同时增长样本大小和数据维数的好处。我们在真实数据上展示了优于现有方法（如UPGMA、Ward's方法和HDBSCAN）的树形恢复性能。

    In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN.
    
[^178]: CoinEM：无需调参的基于粒子的潜变量模型变分推断方法

    CoinEM: Tuning-Free Particle-Based Variational Inference for Latent Variable Models. (arXiv:2305.14916v1 [stat.ML])

    [http://arxiv.org/abs/2305.14916](http://arxiv.org/abs/2305.14916)

    本文提出了两种无需调参的基于粒子的变分推断算法，其中一种是通过考虑边缘最大似然估计为自由能泛函最小化得到的，另一种是用于优化该问题的算法，完全无需调参。

    

    本文提出两种基于粒子的新型算法，用于通过边际最大似然估计学习潜变量模型，其中一种完全无需调参。我们的方法基于将边际最大似然估计视为优化问题的角度：即将其视为自由能泛函的最小化。解决这个问题的一种方法是考虑自由能关联的梯度流的离散化。我们研究了一种类似于流行的 Stein 变分梯度下降算法的方法。特别地，我们为此算法建立了下降引理，保证了自由能在每次迭代中下降。但此方法和其他由梯度流的离散化得到的方法都必须依赖于学习率，该学习率必须由从业者仔细调整，以确保以合适的速率收敛。为此，我们还提出了另一种算法用于优化这个问题，该算法是完全无需调参的。

    We introduce two new particle-based algorithms for learning latent variable models via marginal maximum likelihood estimation, including one which is entirely tuning-free. Our methods are based on the perspective of marginal maximum likelihood estimation as an optimization problem: namely, as the minimization of a free energy functional. One way to solve this problem is to consider the discretization of a gradient flow associated with the free energy. We study one such approach, which resembles an extension of the popular Stein variational gradient descent algorithm. In particular, we establish a descent lemma for this algorithm, which guarantees that the free energy decreases at each iteration. This method, and any other obtained as the discretization of the gradient flow, will necessarily depend on a learning rate which must be carefully tuned by the practitioner in order to ensure convergence at a suitable rate. With this in mind, we also propose another algorithm for optimizing the
    
[^179]: Twitter图像的文本条件下的替代文本生成

    Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])

    [http://arxiv.org/abs/2305.14779](http://arxiv.org/abs/2305.14779)

    本文针对Twitter上分享的图像提出了一种文本条件下的替代文本生成方法。通过CLIP前缀模型，该模型结合图像和推文中的文本信息，生成关于图像的上下文相关的替代文本。

    

    本文提出了一种针对社交媒体特别是Twitter上分享的图像生成替代文本（或alt-text）描述的方法。与图像的字幕不同，文本替换文本更加直白描述和上下文特定。此外，关键是，发布到Twitter上的图像通常是由用户编写的文本附加的，尽管这些文本不一定描述图像，但可能提供有用的上下文信息，如果正确利用可以提供信息，例如推文可能会命名图片中模型之前没有见过的不常见的对象。我们通过一个CLIP前缀模型来解决这个问题，该模型提取图像的嵌入并将其传递给映射网络，该网络输出单词嵌入空间中的短序列，或称为“前缀”，我们将推文本身的文本也连接到其中。这样，模型就可以在文章中条件化视觉和文本信息。然后将合并的多模式前缀作为预训练的语言模型的提示输入。

    In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
    
[^180]: 柿子政治的面孔：使用机器学习比较政治领袖面部情感表达的差异

    The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])

    [http://arxiv.org/abs/2304.09914](http://arxiv.org/abs/2304.09914)

    本文使用机器学习的算法分析了来自15个不同国家的220个政治领袖的YouTube视频，总结了政治领袖面部情感表达的差异。

    

    网络媒体已经彻底改变了政治信息在全球范围内的传播和消费方式，这种转变促使政治人物采取新的策略来捕捉和保持选民的注意力。这些策略往往依赖于情感说服和吸引。随着虚拟空间中视觉内容越来越普遍，很多政治沟通也被标志着唤起情感的视频内容和图像。本文提供了一种新的分析方法。我们将基于现有训练好的卷积神经网络架构提供的Python库fer，应用一种基于深度学习的计算机视觉算法，对描绘来自15个不同国家的政治领袖的220个YouTube视频样本进行分析。该算法返回情绪分数，每一帧都代表6种情绪状态（愤怒，厌恶，恐惧，快乐，悲伤和惊讶）和一个中性表情。

    Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
    
[^181]: ASPEST：主动学习和选择预测之间的弥合

    ASPEST: Bridging the Gap Between Active Learning and Selective Prediction. (arXiv:2304.03870v1 [cs.LG])

    [http://arxiv.org/abs/2304.03870](http://arxiv.org/abs/2304.03870)

    本文提出了一种新的学习范式——主动选择性预测（ASPEST），它可以在转移目标领域中学习查询更多有信息的样本，从而实现减少人工标注工作的同时增加准确性和覆盖率。

    

    选择性预测旨在学习一个可靠的模型，当模型不确定性很高时，可以避免进行预测。随后，可以将这些预测推迟给人类专家进行进一步评估。然而，在许多实际场景中，测试数据的分布与训练数据不同。这导致更不准确的预测，需要增加人工标注，这在许多场景中都是困难和昂贵的。主动学习通过仅查询最信息量丰富的示例来避免这种困难，并且在多个案例中已被证明可以降低总体的标注工作。在这项工作中，我们弥合了选择性预测和主动学习之间的差距，提出了一种新的学习范式，称为主动选择性预测（active selective prediction），它可以在增加准确性和覆盖率的同时在转移目标领域中学习查询更多有信息的样本。对于这个新问题，我们提出了一个简单但有效的解决方案ASPEST，它训练模型快照的集合。

    Selective prediction aims to learn a reliable model that abstains from making predictions when the model uncertainty is high. These predictions can then be deferred to a human expert for further evaluation. In many real-world scenarios, however, the distribution of test data is different from the training data. This results in more inaccurate predictions, necessitating increased human labeling, which is difficult and expensive in many scenarios. Active learning circumvents this difficulty by only querying the most informative examples and, in several cases, has been shown to lower the overall labeling effort. In this work, we bridge the gap between selective prediction and active learning, proposing a new learning paradigm called active selective prediction which learns to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new problem, we propose a simple but effective solution, ASPEST, that trains ensembles of model snapshots
    
[^182]: 基于Transformer的深度学习方法公正预测肝移植后风险因素

    A Transformer-Based Deep Learning Approach for Fairly Predicting Post-Liver Transplant Risk Factors. (arXiv:2304.02780v1 [cs.LG])

    [http://arxiv.org/abs/2304.02780](http://arxiv.org/abs/2304.02780)

    本研究提出了一个基于Transformer的深度学习框架模型，可同时预测五种移植后风险并实现更好的性能。

    

    肝移植是对于晚期肝病患者的一项拯救性手术，而该手术存在两个主要挑战：为供体寻找最佳匹配的受体，以及在不同亚群体之间确保移植的公平性。传统的MELD评分系统只能评估在90天内未接受器官移植的患者的死亡风险。然而，供受体匹配也应该考虑到移植后的风险因素，如心血管疾病、慢性排异等，这些都是移植后常见的并发症。准确预测这些风险分数仍然是一个重大挑战。本研究提出了一个深度学习框架模型，通过将其制定为多任务学习问题，在数据上训练了提出的深度神经网络，可同时预测五种移植后风险并实现更好的性能。

    Liver transplantation is a life-saving procedure for patients with end-stage liver disease. There are two main challenges in liver transplant: finding the best matching patient for a donor and ensuring transplant equity among different subpopulations. The current MELD scoring system evaluates a patient's mortality risk if not receiving an organ within 90 days. However, the donor-patient matching should also take into consideration post-transplant risk factors, such as cardiovascular disease, chronic rejection, etc., which are all common complications after transplant. Accurate prediction of these risk scores remains a significant challenge. In this study, we will use predictive models to solve the above challenge. We propose a deep learning framework model to predict multiple risk factors after a liver transplant. By formulating it as a multi-task learning problem, the proposed deep neural network was trained on this data to simultaneously predict the five post-transplant risks and ach
    
[^183]: 无穷分辨率扩散模型

    $\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States. (arXiv:2303.18242v1 [cs.LG])

    [http://arxiv.org/abs/2303.18242](http://arxiv.org/abs/2303.18242)

    引入了一种名为 $\infty$-Diff 的生成式扩散模型，可以处理无限分辨率的数据，不需要使用超网络进行潜在向量压缩或依赖于离散的组件，能够显著提高样本质量，并能够在保留细节的同时有效地扩展到比训练数据更高的分辨率。

    

    我们引入了一种名为$\infty$-Diff的生成式扩散模型，可以处理无限分辨率的数据。通过在训练期间随机抽样数据的子集并学习去噪，我们可以学到一个连续的函数，允许任意分辨率的采样。与其他最近的无限分辨率生成式模型相比，我们的方法直接操作原始数据，不需要使用超网络进行潜在向量压缩或依赖于离散的组件。因此，我们的方法可以显著提高样本质量，例如降低FID分数，并能够在保留细节的同时有效地扩展到比训练数据更高的分辨率。

    We introduce $\infty$-Diff, a generative diffusion model which directly operates on infinite resolution data. By randomly sampling subsets of coordinates during training and learning to denoise the content at those coordinates, a continuous function is learned that allows sampling at arbitrary resolutions. In contrast to other recent infinite resolution generative models, our approach operates directly on the raw data, not requiring latent vector compression for context, using hypernetworks, nor relying on discrete components. As such, our approach achieves significantly higher sample quality, as evidenced by lower FID scores, as well as being able to effectively scale to higher resolutions than the training data while retaining detail.
    
[^184]: 离散时间线性二次调节器中带随机参数的策略梯度方法

    Policy Gradient Methods for Discrete Time Linear Quadratic Regulator With Random Parameters. (arXiv:2303.16548v1 [math.OC])

    [http://arxiv.org/abs/2303.16548](http://arxiv.org/abs/2303.16548)

    本文使用增强学习技术中的策略梯度方法解决了带随机参数的离散时间线性二次调节器的最优控制问题，并建立了全局线性收敛保证。

    

    本文研究了一个离散时间的线性系统和二次准则的无限时域最优控制问题，其中参数是独立同分布于时间的随机变量。在这种一般情况下，我们应用增强学习技术中的策略梯度方法来寻找最优控制，而不需要了解参数的统计信息。我们研究了状态过程的次高斯性质，并根据比现有结果更弱且更易验证的假设，建立了此方法的全局线性收敛保证。我们进行了数值实验，以说明我们的结果。

    This paper studies an infinite horizon optimal control problem for discrete-time linear system and quadratic criteria, both with random parameters which are independent and identically distributed with respect to time. In this general setting, we apply the policy gradient method, a reinforcement learning technique, to search for the optimal control without requiring knowledge of statistical information of the parameters. We investigate the sub-Gaussianity of the state process and establish global linear convergence guarantee for this approach based on assumptions that are weaker and easier to verify compared to existing results. Numerical experiments are presented to illustrate our result.
    
[^185]: 在不使用数据增强的情况下加强正则化来防止在线深度聚类中的崩溃

    Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation. (arXiv:2303.16521v1 [cs.LG])

    [http://arxiv.org/abs/2303.16521](http://arxiv.org/abs/2303.16521)

    该论文提出了一种不需要数据增强的在线深度聚类方法，通过加强正则化来避免崩溃，相比于其他方法，具有更高的稳定性。

    

    在线深度聚类是指联合使用特征提取网络和聚类模型，以将每个新数据点或批处理分配到聚类标签中。尽管比离线方法更快速和更灵活，但在线聚类很容易达到崩溃解，其中编码器将所有输入映射到同一点，并将所有输入放入单个聚类中。现有成功模型采用了各种技术来避免这个问题，其中大多数需要数据增强或旨在使数据集中每个聚类的平均软分配相同。我们提出了一种不需要数据增强的方法，与现有方法不同，它对硬分配进行了规则化。我们使用贝叶斯框架，导出一个直观的优化目标，可以直接包含在编码器网络的训练中。在四个图像数据集上进行测试，我们证明它比其他方法更加稳定地避免了崩溃。

    Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, we show that it consistently avoids collapse more robustly than other method
    
[^186]: DeepGD: 一种用于深度神经网络的多目标黑盒测试选择方法

    DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04878](http://arxiv.org/abs/2303.04878)

    DeepGD是一种用于深度神经网络的多目标黑盒测试选择方法，通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。

    

    深度神经网络（DNN）被广泛应用于图像处理、语音识别和自然语言处理等各个应用领域。然而，由于其输入域的复杂性和规模，测试DNN模型可能具有挑战性。特别地，测试DNN模型通常需要生成或探索大规模的未标记数据集。在实践中，DNN测试神谕通常需要昂贵的人工工作来标记测试数据，可能涉及多个专家以确保标记的正确性。在本文中，我们提出了DeepGD，一种用于DNN模型的黑盒多目标测试选择方法。它通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。

    Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by s
    
[^187]: 具有一般激活函数的深度平衡模型的全局收敛速度

    Global Convergence Rate of Deep Equilibrium Models with General Activations. (arXiv:2302.05797v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05797](http://arxiv.org/abs/2302.05797)

    该论文研究了具有一般激活函数的深度平衡模型（DEQ）的全局收敛速度，证明了梯度下降以线性收敛速度收敛到全局最优解，并解决了限制平衡点Gram矩阵最小特征值的挑战。

    

    在最近的一篇论文中，Ling等人研究了具有ReLU激活函数的过参数化深度平衡模型（DEQ）。他们证明了对于二次损失函数，梯度下降方法以线性收敛速度收敛到全局最优解。本文表明，对于具有任何具有有界一阶和二阶导数的激活函数的DEQ，该事实仍然成立。由于新的激活函数通常是非线性的，限制平衡点的Gram矩阵的最小特征值尤其具有挑战性。为了完成这个任务，我们需要创建一个新的总体Gram矩阵，并开发一种具有Hermite多项式展开的新形式的双重激活函数。

    In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-linear, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion.
    
[^188]: SPEED: 线性异方差 Bandit 策略评估的实验设计

    SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits. (arXiv:2301.12357v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12357](http://arxiv.org/abs/2301.12357)

    本文提出了一种在线性 Bandit 环境下针对包含异方差奖励噪声的策略评估，使用最优数据收集策略的新算法 SPEED，该算法可实现带有均方误差比较小的策略评估。

    

    本文研究了线性 Bandit 下策略评估的最优数据收集问题。在策略评估中，我们需要估计多臂赌博机环境中执行目标策略将获得的期望收益。本文是首个专注于解决线性 Bandit 环境下包含异方差奖励噪声的策略评估的最优数据收集策略的工作。我们首先在线性 Bandit 环境下制定了加权最小二乘估计的最优设计，以减少目标策略价值的均方误差。接着，我们使用该设计来推导出数据收集期间每个动作的最优样本分配。然后，我们引入了一种名为 SPEED（Structured Policy Evaluation Experimental Design）的新算法，该算法跟踪最优设计，并计算其与最优设计的遗憾。最后，我们通过实验证明 SPEED 可以实现带有均方误差比较小的策略评估。

    In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error compa
    
[^189]: 在隐私约束下的图拓扑学习

    Graph Topology Learning Under Privacy Constraints. (arXiv:2301.06662v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06662](http://arxiv.org/abs/2301.06662)

    在隐私约束下，我们提出了一个框架，联合学习为本地客户定制的个性化图以及共识图，以推断潜在图拓扑，同时在保护隐私的情况下处理分布式客户端的数据。

    

    我们考虑在数据分布于分布式客户端且具有隐私敏感性的新颖实际场景中，通过平滑图信号推断潜在图拓扑的问题。这个任务的主要困难在于如何在隐私约束下利用所有独立客户端的潜在异构数据。为了解决这个问题，我们提出了一个框架，通过联合学习为本地客户端定制的个性化图以及共识图。个性化图匹配本地数据分布，从而减轻数据的异质性，而共识图捕捉全局信息。我们接下来设计了一个定制的算法来解决引入的问题，同时不违反隐私约束，即所有的私有数据都在本地处理。为了进一步增强隐私保护，我们将差分隐私（DP）引入到所提算法中，在传输模型更新时抵御隐私攻击。理论上，我们建立了可证明收敛的分析。

    We consider the problem of inferring the underlying graph topology from smooth graph signals in a novel but practical scenario where data are located in distributed clients and are privacy-sensitive. The main difficulty of this task lies in how to utilize the potentially heterogeneous data of all isolated clients under privacy constraints. Towards this end, we propose a framework where personalized graphs for local clients as well as a consensus graph are jointly learned. The personalized graphs match local data distributions, thereby mitigating data heterogeneity, while the consensus graph captures the global information. We next devise a tailored algorithm to solve the induced problem without violating privacy constraints, i.e., all private data are processed locally. To further enhance privacy protection, we introduce differential privacy (DP) into the proposed algorithm to resist privacy attacks when transmitting model updates. Theoretically, we establish provable convergence analy
    
[^190]: CorruptEncoder：基于数据污染的对比学习后门攻击

    CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning. (arXiv:2211.08229v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.08229](http://arxiv.org/abs/2211.08229)

    本文分析了现有数据污染后门攻击对对比学习的局限性，并提出了一种名为CorruptEncoder的新型攻击方法，通过理论导向的方式创建优化的污染输入，大幅提高攻击效果。实验证明，CorruptEncoder是首个仅需要少量图像和污染比例即可达到90%以上攻击成功率的攻击方法。同时，本文提出了一种名为局部裁剪的防御策略来应对数据污染后门攻击。

    

    对比学习使用无标签的预训练数据集对通用编码器进行预训练，包括图像或图像-文本对。对比学习容易受到基于数据污染的后门攻击（DPBA）的攻击，攻击者通过向预训练数据集中注入被污染的输入来后门化编码器。然而，现有的DPBA的效果有限。本文首先分析现有攻击的局限性，并提出了一种名为CorruptEncoder的新型DPBA来对抗对比学习。CorruptEncoder使用理论导向的方法创建最优的污染输入以最大限度地提高攻击效果。实验结果表明，CorruptEncoder在攻击效果上明显优于现有的DPBA。尤其是，CorruptEncoder是首个仅需要少量（3个）参考图像和小规模污染比例（0.5%）即可达到90%以上的攻击成功率的DPBA。此外，本文还提出了一种名为局部裁剪的防御策略来抵御DPBA。实验结果表明，我们的防御策略能有效抵御DPBA。

    Contrastive learning (CL) pre-trains general-purpose encoders using an unlabeled pre-training dataset, which consists of images or image-text pairs. CL is vulnerable to data poisoning based backdoor attacks (DPBAs), in which an attacker injects poisoned inputs into the pre-training dataset so the encoder is backdoored. However, existing DPBAs achieve limited effectiveness. In this work, we take the first step to analyze the limitations of existing attacks and propose new DPBAs called CorruptEncoder to CL. CorruptEncoder uses a theory-guided method to create optimal poisoned inputs to maximize attack effectiveness. Our experiments show that CorruptEncoder substantially outperforms existing DPBAs. In particular, CorruptEncoder is the first DPBA that achieves more than 90% attack success rates with only a few (3) reference images and a small poisoning ratio (0.5%). Moreover, we also propose a defense, called localized cropping, to defend against DPBAs. Our results show that our defense ca
    
[^191]: ImpNet: 编译神经网络中不可察觉和黑盒不可检测的后门攻击

    ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks. (arXiv:2210.00108v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00108](http://arxiv.org/abs/2210.00108)

    ImpNet是一种在编译神经网络中添加的不可察觉和黑盒不可检测的后门攻击。这些后门可以绕过数据准备和模型训练阶段的保护措施，并且只能在插入阶段可靠地检测到，移除它们很具有挑战性。

    

    早期的后门攻击引起了攻防开发中的一场竞争。防御手段已经出现，并展示了在模型中检测和移除后门的能力。这些防御手段通过检查训练数据、模型或训练过程的完整性来工作。在这项工作中，我们展示了后门可以在编译过程中添加，绕过了数据准备和模型训练阶段的任何保护措施。攻击者不仅可以在编译过程中插入已有的基于权重的后门，还可以插入一种新的与权重无关的后门，例如ImpNet。这些后门在训练或数据准备过程中是不可检测的，因为它们尚不存在。接下来，我们展示了一些后门，包括ImpNet，只能在插入它们的阶段可靠地检测到，而在其他任何地方移除它们都是一个重大挑战。我们得出结论，机器学习模型的安全性需要保证。

    Early backdoor attacks against machine learning set off an arms race in attack and defence development. Defences have since appeared demonstrating some ability to detect backdoors in models or even remove them. These defences work by inspecting the training data, the model, or the integrity of the training procedure. In this work, we show that backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages. The attacker can not only insert existing weight-based backdoors during compilation, but also a new class of weight-independent backdoors, such as ImpNet. These backdoors are impossible to detect during the training or data preparation processes, because they are not yet present. Next, we demonstrate that some backdoors, including ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge. We conclude that ML model security requires assurance of 
    
[^192]: 使随机梯度下降法无参数化

    Making SGD Parameter-Free. (arXiv:2205.02160v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2205.02160](http://arxiv.org/abs/2205.02160)

    该论文提出了一种无参数化的随机梯度下降法，能够在一定程度上适应未知梯度范数、平滑性和强凸性，并在收敛速度方面具有高概率保证。

    

    我们开发了一种无参数随机凸优化（SCO）算法，其收敛速度仅比对应的已知参数设置的最优速度多一个双对数因子。相比之下，先前已知的无参数SCO的最佳速度是基于在线无参数后悔界的，与已知参数的对应方法相比包含不可避免的额外对数项。我们的算法具有概念上的简单性，具有高概率保证，并且部分适应未知梯度范数、平滑性和强凸性。我们的成果的核心是SGD步长选择的新型无参数证书，以及假设在SGD迭代上没有先验界限的时间一致集中结果。

    We develop an algorithm for parameter-free stochastic convex optimization (SCO) whose rate of convergence is only a double-logarithmic factor larger than the optimal rate for the corresponding known-parameter setting. In contrast, the best previously known rates for parameter-free SCO are based on online parameter-free regret bounds, which contain unavoidable excess logarithmic terms compared to their known-parameter counterparts. Our algorithm is conceptually simple, has high-probability guarantees, and is also partially adaptive to unknown gradient norms, smoothness, and strong convexity. At the heart of our results is a novel parameter-free certificate for SGD step size choice, and a time-uniform concentration result that assumes no a-priori bounds on SGD iterates.
    
[^193]: 多变量大数据分析中的可解释性学习用于网络监测

    Interpretable Learning in Multivariate Big Data Analysis for Network Monitoring. (arXiv:1907.02677v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/1907.02677](http://arxiv.org/abs/1907.02677)

    本文扩展了多变量大数据分析（MBDA）方法，提出了一种自动推导特征的解决方案，结合可解释性和交互式模型的优势以及并行处理的能力，应用于网络监测和诊断，最终在UGR'16和Dartmouth'18两个数据集上取得成功。

    

    开发新的数据驱动模型以评估通信网络性能越来越受到关注。对于许多应用程序，比如网络监测和故障排除，如果不能被人类操作员解释，数据模型就没多大用处。在本文中，我们提出了多变量大数据分析（MBDA）方法的扩展，这是一种近期提出的可解释性数据分析工具。在这个扩展中，我们提出了自动推导特征的解决方案，这是当数据量庞大时应用MBDA的重要步骤。所得到的网络监测方法允许我们检测和诊断不同的网络异常，采用一种将可解释性和交互式模型的优势与并行处理的能力相结合的数据分析工作流。我们将扩展的MBDA应用于两个案例研究：UGR'16，用于异常检测的基准流量实际数据集，以及Dartmouth'18，最长和最具挑战性的数据集之一。

    There is an increasing interest in the development of new data-driven models useful to assess the performance of communication networks. For many applications, like network monitoring and troubleshooting, a data model is of little use if it cannot be interpreted by a human operator. In this paper, we present an extension of the Multivariate Big Data Analysis (MBDA) methodology, a recently proposed interpretable data analysis tool. In this extension, we propose a solution to the automatic derivation of features, a cornerstone step for the application of MBDA when the amount of data is massive. The resulting network monitoring approach allows us to detect and diagnose disparate network anomalies, with a data-analysis workflow that combines the advantages of interpretable and interactive models with the power of parallel processing. We apply the extended MBDA to two case studies: UGR'16, a benchmark flow-based real-traffic dataset for anomaly detection, and Dartmouth'18, the longest and l
    

