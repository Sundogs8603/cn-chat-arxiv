# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [One-Step Image Translation with Text-to-Image Models](https://arxiv.org/abs/2403.12036) | 通过引入单步扩散模型并通过对抗学习目标调整到新任务和领域，我们提出了一种解决现有条件扩散模型局限性的通用方法，适用于无配对和配对设置下的图像翻译任务。 |
| [^2] | [VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models](https://arxiv.org/abs/2403.12034) | 利用预训练的视频扩散模型，本文提出了一个可生成大规模3D数据集的VFusion3D模型。 |
| [^3] | [ROUTERBENCH: A Benchmark for Multi-LLM Routing System](https://arxiv.org/abs/2403.12031) | 提出了ROUTERBENCH，一个用于评估LLM路由系统性能的基准测试框架，包括超过405k推理结果的数据集，以支持路由策略的开发。 |
| [^4] | [Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning](https://arxiv.org/abs/2403.12030) | 提出了一种基于预训练模型的增量类别学习方法，通过训练每个新任务的轻量级适配器模块来创建任务特定的子空间，实现了模型更新而不损害先前知识。 |
| [^5] | [Align and Distill: Unifying and Improving Domain Adaptive Object Detection](https://arxiv.org/abs/2403.12029) | 引入了统一的基准测试和实现框架ALDI以及新的DAOD基准数据集CFC-DAOD，解决了领域自适应目标检测中的基准问题，并支持未来方法的发展。 |
| [^6] | [FlexCap: Generating Rich, Localized, and Flexible Captions in Images](https://arxiv.org/abs/2403.12026) | FlexCap模型能够生成图像中具有不同长度的区域描述，在密集字幕任务和视觉问答系统中表现出优越性能。 |
| [^7] | [A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models](https://arxiv.org/abs/2403.12025) | 提出了用于揭示大型语言模型中健康公平危害和偏见的资源和方法，进行了实证案例研究，并提出了用于人类评估LLM生成答案偏见的多因子框架以及EquityMedQA数据集。 |
| [^8] | [Supervised Fine-Tuning as Inverse Reinforcement Learning](https://arxiv.org/abs/2403.12017) | 本论文提出将逆强化学习和模仿学习的见解结合，探讨了使用演示数据集对齐大语言模型的方法，并对不同方法的性能进行了分析。 |
| [^9] | [EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents](https://arxiv.org/abs/2403.12014) | EnvGen提出了一种新的框架，利用LLMs的推理能力自适应创建训练环境，帮助小型具身体RL代理在弱点方面学习有用技能。 |
| [^10] | [Convergence of Kinetic Langevin Monte Carlo on Lie groups](https://arxiv.org/abs/2403.12012) | 提出了一个基于Lie群的动力学Langevin Monte Carlo采样算法，通过添加噪声和精细离散化实现了Lie群结构的保持，并在W2距离下证明了连续动力学和离散采样器的指数收敛性。 |
| [^11] | [Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions](https://arxiv.org/abs/2403.12007) | 本研究旨在定义通过数字行为变化干预支持癌症患者提高生活质量的有效参与方式，发现医生处方显著增加患者对移动数字行为变化干预的持续参与，同时指出每周参与一次已足以维持福祉，但内在动机可能需要更高水平的参与。 |
| [^12] | [Visualization for Trust in Machine Learning Revisited: The State of the Field in 2023](https://arxiv.org/abs/2403.12005) | 2023年的研究显示，可解释和可信赖的机器学习可视化仍然是一个重要且不断发展的领域，为各种领域提供了趋势、见解和挑战。 |
| [^13] | [Learning Useful Representations of Recurrent Neural Network Weight Matrices](https://arxiv.org/abs/2403.11998) | 提出了机械主义和功能主义两种方法以学习递归神经网络(RNN)权重的有用表示，并发展了框架来生成有助于确定RNN行为的丰富表示 |
| [^14] | [Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning](https://arxiv.org/abs/2403.11996) | 利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。 |
| [^15] | [Diffusion Denoising as a Certified Defense against Clean-label Poisoning](https://arxiv.org/abs/2403.11981) | 提出了一种扩散去噪作为针对干净标签中毒的认证防御，能将攻击成功率降低到0-16%，同时几乎不影响测试准确性，为未来开发更强干净标签攻击和利用该防御措施作为强有力基础提供了重要启示。 |
| [^16] | [Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory](https://arxiv.org/abs/2403.11968) | 本文揭示了无分类器引导的条件扩散模型的尖锐统计理论，提出了适应数据分布平滑度的样本复杂度界限，并展示了新颖的扩散泰勒逼近技术在理论发展中的重要性。 |
| [^17] | [Informed Spectral Normalized Gaussian Processes for Trajectory Prediction](https://arxiv.org/abs/2403.11966) | 该论文提出了一种用于轨迹预测的新颖正则化持续学习方法，利用通知式先验知识，提高了模型性能和数据效率。 |
| [^18] | [Probabilistic Calibration by Design for Neural Network Regression](https://arxiv.org/abs/2403.11964) | 提出了一种称为Quantile Recalibration Training的新型端到端模型训练过程，将后处理校准直接整合到训练过程中，无需额外参数，展示出在神经网络回归中提高校准性能的方法。 |
| [^19] | [Transfer Learning Beyond Bounded Density Ratios](https://arxiv.org/abs/2403.11963) | 低次多项式估计类上的迁移学习，证明了在非常温和的假设下，对于低次多项式来说非平凡的迁移学习是可能的，超越了$dQ/dP$有界的经典假设 |
| [^20] | [Enhanced Event-Based Video Reconstruction with Motion Compensation](https://arxiv.org/abs/2403.11961) | 通过运动补偿来增强重建质量，提出将输入帧和稀疏编码进行变换，并将流网络与CISTA-LSTC集成，形成CISTA-Flow网络，使系统仅依赖事件。 |
| [^21] | [CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation](https://arxiv.org/abs/2403.11960) | CASPER提出了一种因果关系感知的方法来处理时空时间序列数据插补问题，避免过度利用非因果关系，提高数据分析的准确性。 |
| [^22] | [Learning Dynamical Systems Encoding Non-Linearity within Space Curvature](https://arxiv.org/abs/2403.11948) | 引入一种方法以提高学习动力学系统的复杂性，同时不影响训练效率或稳定性保证。 |
| [^23] | [Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees](https://arxiv.org/abs/2403.11947) | 引入强化学习和可解释性决策树相结合的方法，实现了在家庭能源管理中有效管理能耗的控制器。 |
| [^24] | [Multistep Inverse Is Not All You Need](https://arxiv.org/abs/2403.11940) | 本研究考虑了控制问题中的观测空间到简化控制相关变量空间的编码器学习，AC-State方法是一个多步反向方法。 |
| [^25] | [State space representations of the Roesser type for convolutional layers](https://arxiv.org/abs/2403.11938) | 从控制理论的角度，提供了Roesser类型的2-D卷积层状态空间表示，具有最小化的状态数量，在$c_\mathrm{in}=c_\mathrm{out}$的情况下证明了这一点，并进一步实现了扩张、跨越和N-D卷积的状态空间表示。 |
| [^26] | [Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic](https://arxiv.org/abs/2403.11925) | 通过多级Actor-Critic框架和多级蒙特卡罗梯度估计器，本研究成功解决了平均奖励MDPs全局收敛中对混合时间预测的依赖性，展现出最严格的依赖关系。 |
| [^27] | [Single-Agent Actor Critic for Decentralized Cooperative Driving](https://arxiv.org/abs/2403.11914) | 提出了一种新颖的单Agent Actor Critic模型，旨在利用单Agent强化学习学习自主车辆的去中心化合作驾驶策略，并通过对各种交通场景的广泛评估展现了改善道路系统内不同瓶颈位置交通流量的巨大潜力。 |
| [^28] | [Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers](https://arxiv.org/abs/2403.11907) | 可解释性强化学习在能源应用控制器中的创新是通过提出可微分决策树来解决数据驱动控制中的可解释性和住宅资产硬件能力受限等挑战。 |
| [^29] | [CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification](https://arxiv.org/abs/2403.11904) | 该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。 |
| [^30] | [Larimar: Large Language Models with Episodic Memory Control](https://arxiv.org/abs/2403.11901) | Larimar提出了一种大脑启发的架构，通过分布式情节记忆增强LLMs，实现了动态、一次性的知识更新，无需昂贵的重新训练或微调，且在速度和灵活性上表现出色。 |
| [^31] | [Visuo-Tactile Pretraining for Cable Plugging](https://arxiv.org/abs/2403.11898) | 本文研究了如何将触觉信息纳入模仿学习平台以在复杂任务中提高性能，通过训练机器人代理插拔USB电缆，实现了在微细操纵任务中的进展。 |
| [^32] | [From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?](https://arxiv.org/abs/2403.11894) | 该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。 |
| [^33] | [KnFu: Effective Knowledge Fusion](https://arxiv.org/abs/2403.11892) | FKD是一个新的联邦学习范式，基于知识蒸馏概念，尝试在解决FL中的模型异构性和梯度反转攻击方面取得进展。 |
| [^34] | [SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules](https://arxiv.org/abs/2403.11887) | SuperLoRA提出了一个统一且高度灵活的框架，通过引入不同的技巧扩展了不同的LoRA变体，在极少参数情况下特别优异。 |
| [^35] | [Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids](https://arxiv.org/abs/2403.11877) | 提出了一种针对第四代区域供热网格的基于学习的热功率流高效训练方法，通过生成代理分布覆盖相关供需值来加速训练，避免求解热网格方程的迭代过程。 |
| [^36] | [Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping](https://arxiv.org/abs/2403.11876) | 该论文提出了一种深度贝叶斯未来融合的方法，通过自监督的方式实现高分辨率越野地图的制作，为长程预测提供更好的支持。 |
| [^37] | [NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction](https://arxiv.org/abs/2403.11872) | NuGraph2 是一种用于液氩时间投影室探测器中模拟中微子相互作用低级重建的图神经网络，通过多头注意力传递机制实现了高效的背景过滤和语义标记。 |
| [^38] | [The Real Tropical Geometry of Neural Networks](https://arxiv.org/abs/2403.11871) | 将二元分类器定义为热带有理函数的符号，发现ReLU神经网络的参数空间含于其内，提出了基于参数空间的两种不同细分方法，并描述了0/1损失函数的子水平集以及分类风扇的几何和组合特性。 |
| [^39] | [Complete and Efficient Graph Transformers for Crystal Material Property Prediction](https://arxiv.org/abs/2403.11857) | 提出了一种利用晶体单位胞的周期模式建立晶格表示进行晶体高效图表示的新方法，同时设计了适用于晶体材料的SE(3) transformer，包括iComFormer和eComFormer两个变体。 |
| [^40] | [Near-Optimal Solutions of Constrained Learning Problems](https://arxiv.org/abs/2403.11844) | 本文研究了在非凸设置中，通过表征与最优对偶变量相关的Lagrange最小化器的约束违反来弥合实践与理论之间的差距。 |
| [^41] | [Fuzzy Rough Choquet Distances for Classification](https://arxiv.org/abs/2403.11843) | 本文提出了一种基于模糊粗糙集合度量的新型Choquet距离，用于捕捉数据中的非线性关系，使得距离度量更加灵活与准确。 |
| [^42] | [Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data](https://arxiv.org/abs/2403.11841) | 提出了一种新的策略学习算法，PESCAL，利用基于前门标准的中介变量消除混杂偏差，并采用悲观原则处理候选策略引起的分布变化。 |
| [^43] | [Multi-Criteria Comparison as a Method of Advancing Knowledge-Guided Machine Learning](https://arxiv.org/abs/2403.11840) | 提出了一种通用的模型评估方法，该方法可以比较不同类型和结构的候选模型在多个科学、理论和实践标准上的表现。 |
| [^44] | [Towards Understanding the Relationship between In-context Learning and Compositional Generalization](https://arxiv.org/abs/2403.11834) | 强制模型进行上下文学习可能有助于促进神经网络模型的构成概括能力 |
| [^45] | [SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator](https://arxiv.org/abs/2403.11833) | 本文介绍了一种名为SSCAE的对抗攻击模型，用于生成语义、句法和上下文感知的自然语言对抗样本，提出了动态阈值和本地贪婪搜索以生成高质量对抗样本。 |
| [^46] | [Sound Event Detection and Localization with Distance Estimation](https://arxiv.org/abs/2403.11827) | 本文将声音事件检测和定位任务扩展为具有距离估计的3D SELD，探讨了两种集成距离估计的方法，并在Ambisonic和双耳版本的声音场景下进行了实验。 |
| [^47] | [CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution](https://arxiv.org/abs/2403.11826) | 引入胶囊层的架构修改，配合图神经网络，实现了将受物理启发的特征整合进分析，为高级对象标记提供了新思路。 |
| [^48] | [Low-Cost Privacy-Aware Decentralized Learning](https://arxiv.org/abs/2403.11795) | ZIP-DL是一种低成本的隐私感知去中心化学习算法，通过向每个模型更新添加相关噪声，在保护隐私的同时实现了较高的模型准确性，具有较好的收敛速度和隐私保证。 |
| [^49] | [A tutorial on learning from preferences and choices with Gaussian Processes](https://arxiv.org/abs/2403.11782) | 提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。 |
| [^50] | [Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt](https://arxiv.org/abs/2403.11780) | 提出了Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的唱歌声音合成方法，采用了基于解码器的变压器模型架构和范围旋律解耦的音高表示方法。 |
| [^51] | [Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms](https://arxiv.org/abs/2403.11778) | 该研究评估了在实时通信平台中使用静态深度伪造音频检测模型的可行性，并通过开发两个基于Resnet和LCNN架构的模型，实现了在通信平台中的实时深度伪造音频检测，为提升模型性能和确保通信安全提供了策略和框架。 |
| [^52] | [S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention](https://arxiv.org/abs/2403.11772) | 本文介绍了一项关于使用联合嵌入预测架构（JEPAs）实现脑电信号无缝跨数据集转移的探索性研究，提出了Signal-JEPA用于表示脑电记录，并展示了其在精确下游分类中的重要性。 |
| [^53] | [Efficient Feature Extraction and Late Fusion Strategy for Audiovisual Emotional Mimicry Intensity Estimation](https://arxiv.org/abs/2403.11757) | 本文提出了用于音频视觉情感模仿强度估计的高效特征提取和延迟融合策略 |
| [^54] | [Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs](https://arxiv.org/abs/2403.11755) | 提出了Meta-Prompting for Visual Recognition (MPVR)方法，通过仅需少量信息即可自动化零样本识别中的提示生成过程。 |
| [^55] | [PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks](https://arxiv.org/abs/2403.11743) | 通过引入转导的概念，提出了PARMESAN，一种用于解决密集预测任务的无参数内存搜索和转导方法，实现了灵活性和无需连续训练的学习。 |
| [^56] | [LSKNet: A Foundation Lightweight Backbone for Remote Sensing](https://arxiv.org/abs/2403.11735) | LSKNet是一种轻量级的大型选择核网络骨干，能动态调整其较大的空间感受野，以更好地模拟遥感场景中各种对象的远程上下文。 |
| [^57] | [Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$](https://arxiv.org/abs/2403.11734) | 该研究提出了一种参数化版本的关系GNNs，通过在$t$为无穷大时仅使用二次空间的嵌入来近似$3$-GNNs，对于较低的$t$值，通过交换较少的消息实现弱的近似，同时通常产生了几个规划领域中所需的$C_3$特征。 |
| [^58] | [PITA: Physics-Informed Trajectory Autoencoder](https://arxiv.org/abs/2403.11728) | 提出了物理信息轨迹自动编码器（PITA）架构，通过将物理动力学模型纳入损失函数，使得轨迹更加平滑且具有物理合理性。 |
| [^59] | [Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation](https://arxiv.org/abs/2403.11722) | 提出了一种使用四元值神经网络和四元反向传播进行时间序列压缩的方法，在保留特征之间关系的同时在故障分类中展现出潜力。 |
| [^60] | [Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models](https://arxiv.org/abs/2403.11706) | 本文将多源扩散模型（MSDM）推广到任意时间域扩散模型，并引入文本嵌入条件，实现了不需要分离数据训练，可以参数化任意数量源，并实现丰富语义控制的音乐生成模型。 |
| [^61] | [Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach](https://arxiv.org/abs/2403.11705) | 提出了一种机器学习框架来模拟在三角格子中稳定手征磁性领域的复杂电子介导的自旋-自旋相互作用，研究了磁性领域在热淬后的粗化过程 |
| [^62] | [Generalization error of spectral algorithms](https://arxiv.org/abs/2403.11696) | 本研究考虑了使用光谱算法来训练核，推导出泛化误差函数，提供了完整的损失渐近行为，展示了损失在特定频谱尺度上的局部化。 |
| [^63] | [Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates](https://arxiv.org/abs/2403.11687) | 在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。 |
| [^64] | [Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding](https://arxiv.org/abs/2403.11686) | Crystalformer是一种用于晶体结构的Transformer-based编码器，利用无限连接注意力进行无限的原子间势求和，具有较低的参数需求。 |
| [^65] | [Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes](https://arxiv.org/abs/2403.11678) | 通过3D感知的潜空间和跨场景共享信息，实现了NeRFs有效学习大量语义相似场景，并显著降低了训练时间和内存消耗 |
| [^66] | [HDLdebugger: Streamlining HDL debugging with Large Language Models](https://arxiv.org/abs/2403.11671) | 提出了一个LLM辅助的HDL调试框架 HDLdebugger，通过逆向工程方法生成HDL调试数据，提供检索增强生成的搜索引擎以及检索增强LLM微调的方法，以简化HDL调试任务。 |
| [^67] | [Diffusion-Based Environment-Aware Trajectory Prediction](https://arxiv.org/abs/2403.11643) | 该论文提出了一种基于扩散的生成模型，用于多智能体轨迹预测，能够准确捕捉交通参与者与环境之间的复杂相互作用，优于多种传统方法，在预测准确性方面表现出色。 |
| [^68] | [Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring](https://arxiv.org/abs/2403.11642) | 通过考虑运行时的时间约束，本研究通过调整基于遗传算法的技术，在预测性流程监控中引入了时间背景知识来生成反事实解释。 |
| [^69] | [The Value of Reward Lookahead in Reinforcement Learning](https://arxiv.org/abs/2403.11637) | 分析了在强化学习中利用部分未来奖励先知的价值，通过竞争性分析得出了最坏情况下奖励期望的精确比率。 |
| [^70] | [Dual-Channel Multiplex Graph Neural Networks for Recommendation](https://arxiv.org/abs/2403.11624) | 该研究提出了一种名为双通道多重图神经网络（DCMGNN）的新型推荐框架，能够有效解决现有推荐方法中存在的多通路关系行为模式建模和对目标关系影响忽略的问题。 |
| [^71] | [Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report)](https://arxiv.org/abs/2403.11603) | 本文提出了针对智能物联网系统的多人合作多臂老虎机模型，设计了分布式合作赌博算法DC-ULCB，能够在最大化数据速率的同时保持选择的公平性，通过分析和验证，证明在奖励和公平性方面优于现有算法。 |
| [^72] | [A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs](https://arxiv.org/abs/2403.11591) | 提出了一种物理信息神经网络方法，用于发现快/慢动力学ODE系统的慢不变流形，能够同时分解矢量场为快慢组分并以闭合形式提供下层SIM的泛函。 |
| [^73] | [Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines](https://arxiv.org/abs/2403.11585) | Linguacodus是一种创新框架，通过部署动态流水线和精细调整的大型语言模型，实现了将自然语言任务描述转换为代码的自动化过程，极大地推进了机器学习应用的发展。 |
| [^74] | [Offline Multitask Representation Learning for Reinforcement Learning](https://arxiv.org/abs/2403.11574) | 通过研究离线多任务低秩RL，提出了一种名为MORL的新算法，证明了在强化学习中应用学习到的表示的优势。 |
| [^75] | [Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization](https://arxiv.org/abs/2403.11565) | 该论文介绍了一种名为DSM的统一框架，用于分析去中心化随机次梯度方法的全局收敛性，证明了在温和条件下的全局收敛性，并展示其涵盖了各种现有高效的去中心化次梯度方法。 |
| [^76] | [Advancing Neuromorphic Computing: Mixed-Signal Design Techniques Leveraging Brain Code Units and Fundamental Code Units](https://arxiv.org/abs/2403.11563) | 该论文介绍了一种创新性的数字神经形态架构，通过混合信号设计方法将脑编码单元（BCU）和基本编码单元（FCU）集成在一起，提升神经形态系统的计算效率、准确性和适应性。 |
| [^77] | [RL en Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model](https://arxiv.org/abs/2403.11544) | 在局部访问模型下，通过引入Lin-Confident-FTRL算法，可以学习到具有更优精度界限和更好扩展性的均值平衡算法。 |
| [^78] | [Semantic Prompting with Image-Token for Continual Learning](https://arxiv.org/abs/2403.11537) | 提出了一种基于图像令牌的语义提示方法，称为I-Prompt，旨在消除任务预测，通过语义提示匹配和图像令牌级提示来选择提示。 |
| [^79] | [OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System](https://arxiv.org/abs/2403.11536) | 引入外部模态引导的数据挖掘框架以解决自动光学检验在工业制造中面临的模型部署挑战和准确性问题。 |
| [^80] | [Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)](https://arxiv.org/abs/2403.11532) | 本研究提出使用符合预测来评估分布外（OOD）检测中效率的新方法，并定义了新的符合AUROC和符合FRP@TPR95指标，为OOD和异常检测基准提供了概率保守性保证。 |
| [^81] | [LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers](https://arxiv.org/abs/2403.11522) | LOOPer是针对多面体编译器的学习型自动代码优化器，通过机器学习建立成本模型来指导多面体优化搜索，突破了传统编译器在选择代码转换方面的限制。 |
| [^82] | [A Data-driven Approach for Rapid Detection of Aeroelastic Modes from Flutter Flight Test Based on Limited Sensor Measurements](https://arxiv.org/abs/2403.11521) | 本研究提出了一种基于时延嵌入动态模态分解技术的数据驱动方法，以及稳健主成分分析和促进稀疏度准则，能够自动和最优地选择稀疏模态，加快气动弹性模态识别和分析过程。 |
| [^83] | [State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards](https://arxiv.org/abs/2403.11520) | 提出了分离状态SARSA（SS-SARSA）算法，针对恢复老虎机场景设计，通过将轮数视为状态，降低Q-learning/SARSA所需的状态组合数量，实现有效学习，并在温和假设下证明了渐近收敛至最优策略。 |
| [^84] | [Covid-19 detection from CT scans using EfficientNet and Attention mechanism](https://arxiv.org/abs/2403.11505) | 开发了一个基于深度学习模型的管道，结合EfficientNet和注意力机制，用于从肺部CT扫描图像中检测COVID-19，并在竞赛数据集验证集上表现优异 |
| [^85] | [MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning](https://arxiv.org/abs/2403.11504) | MLVICX是一种用于胸部X射线自监督表征学习的方法，通过多级方差和协方差探索策略来捕获丰富的表征。 |
| [^86] | [Do CLIPs Always Generalize Better than ImageNet Models?](https://arxiv.org/abs/2403.11497) | CLIP模型在面对分布转移时表现出良好的泛化能力，作者设计了CounterAnimal数据集来探究模型对虚假特征的依赖性。 |
| [^87] | [Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics](https://arxiv.org/abs/2403.11495) | 提出了一种名为Toast的新框架，以及增强版DyToast，用于学习路网的通用表示，并增强了时间动态的整合，以提高各种时间敏感下游任务的性能。 |
| [^88] | [Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting](https://arxiv.org/abs/2403.11491) | 提出了一种高效的抗遗忘测试时间适应（EATA）方法，通过开发主动样本选择标准和引入Fisher正则化约束重要模型参数，实现了不会忘记的不确定性校准测试时间模型适应。 |
| [^89] | [Open-World Semi-Supervised Learning for Node Classification](https://arxiv.org/abs/2403.11483) | 方差不平衡可能对模型性能产生负面影响，提出一种不依赖预训练图编码器的有效方法 |
| [^90] | [SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction](https://arxiv.org/abs/2403.11482) | 提出了一种适用于3D地震数据的新型扩散模型重建框架，可以在处理复杂缺失模式时提高重建性能 |
| [^91] | [Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs](https://arxiv.org/abs/2403.11477) | 该研究提出了对于弱通信MDPs的样本复杂度界限为 $\tilde{O}(SA\frac{H}{\epsilon^2})$，改进了现有工作，是在所有参数上最小最优的。 |
| [^92] | [Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training](https://arxiv.org/abs/2403.11472) | 通过开发一种新技术，我们能够加速学习索引结构的重新训练过程，解决了字符串键在学习索引中性能瓶颈的问题。 |
| [^93] | [FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update](https://arxiv.org/abs/2403.11464) | 提出了一种具有随机参数更新机制的个性化联邦学习方法，以解决资源受限设备在非iid数据场景下的性能下降问题。 |
| [^94] | [Graph Partial Label Learning with Potential Cause Discovering](https://arxiv.org/abs/2403.11449) | 提出了一种具有潜在因果发现功能的图部分标签学习方法，可在部分标记学习环境中有效学习区分信息。 |
| [^95] | [LLM Guided Evolution - The Automation of Models Advancing Models](https://arxiv.org/abs/2403.11446) | 该研究提出了一种新的"引导进化"（GE）框架，利用大型语言模型（LLMs）直接修改代码，采用自我维持的反馈循环增强模型进化决策制定。 |
| [^96] | [Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making](https://arxiv.org/abs/2403.11432) | 本研究致力于研究基于注意力的DRL框架的可解释性，在自主车辆决策中，通过在开源AV仿真环境中添加多头注意力框架，提高了模型的解释性。 |
| [^97] | [Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure](https://arxiv.org/abs/2403.11425) | 使用大型语言模型结合新颖的叙述特征，能够有效识别癌症患者患心力衰竭的风险，表现优于传统机器学习模型和深度学习模型。 |
| [^98] | [Neural network representation of quantum systems](https://arxiv.org/abs/2403.11420) | 利用神经网络的通用逼近定理，我们提出了一个新颖的映射方法，将广泛类的量子力学系统表示为神经网络形式，从而可以对Feynman路径积分中的任意路径进行统计求和。 |
| [^99] | [Variational Sampling of Temporal Trajectories](https://arxiv.org/abs/2403.11418) | 本文介绍了一种通过在函数空间中显式参数化过渡函数来学习轨迹分布的机制，实现了对轨迹的高效合成和推断。 |
| [^100] | [DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation](https://arxiv.org/abs/2403.11415) | DreamSampler框架通过整合反向扩散采样和分数蒸馏，提供了模型无关的图像处理方法，解决了分数蒸馏易崩溃的问题，并在图像编辑和重构中展现了竞争力。 |
| [^101] | [Layer-diverse Negative Sampling for Graph Neural Networks](https://arxiv.org/abs/2403.11408) | 提出了一种层多样的负采样方法，通过在消息传递传播中选择性采样负样本，并且在多层GNNs中实现了逐层负样本多样性，实验证明其有效性。 |
| [^102] | [Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors](https://arxiv.org/abs/2403.11407) | 通过利用去噪扩散模型先验结构，提出了一种分布式分隔后验采样方法，相比先前的方法具有更低的逼近误差。 |
| [^103] | [Automated data processing and feature engineering for deep learning and big data applications: a survey](https://arxiv.org/abs/2403.11395) | 现代人工智能方法旨在设计能够直接从数据中学习的算法，自动化数据处理任务的兴起驱动了机器学习和大数据应用中利用大量复杂数据的发展。 |
| [^104] | [Investigating the Benefits of Projection Head for Representation Learning](https://arxiv.org/abs/2403.11391) | 投影头技术能够通过逐层渐进的特征加权和更为归一化的表示，提高表示学习效果。 |
| [^105] | [Stochastic approach for elliptic problems in perforated domains](https://arxiv.org/abs/2403.11385) | 提出了一种基于神经网络的无网格方法，用于解决多孔域中的椭圆问题，该方法结合了随机表示方法和Feynman-Kac公式，能够有效捕捉多尺度性质并处理界面条件。 |
| [^106] | [Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction](https://arxiv.org/abs/2403.11375) | Path-GPTOmic框架通过调节模型嵌入空间和提出梯度调整模型，解决了癌症生存预测中存在的病理图像和基因组学数据不平衡的问题。 |
| [^107] | [JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning](https://arxiv.org/abs/2403.11366) | 提出了用于检索增强微调的JAX张量并行LoRA库，通过PEFT兼容微调Llama-2模型，利用分布式训练和JAX的即时编译和张量分片实现了资源高效管理，加速微调并降低内存需求，提高了微调大型语言模型在复杂RAG应用中的可扩展性和可行性。 |
| [^108] | [IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight](https://arxiv.org/abs/2403.11363) | IGANN Sparse是一种新颖的机器学习模型，通过训练过程中的非线性特征选择促进稀疏性，确保在不影响预测性能的情况下提高模型的可解释性。 |
| [^109] | [Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies](https://arxiv.org/abs/2403.11353) | 该论文提出了一种利用迭代自我训练方法来训练深度学习模型，从而解决二维核磁共振（2D NMR）预测中的挑战，弥补了缺乏标注NMR训练数据集的不足。 |
| [^110] | [An SDP-based Branch-and-Cut Algorithm for Biclustering](https://arxiv.org/abs/2403.11351) | 提出了一个基于SDP的分支定界算法，用于解决$k$-最密不相交双团问题。 |
| [^111] | [Robustness of the data-driven approach in limited angle tomography](https://arxiv.org/abs/2403.11350) | 数据驱动方法在有限角度层析成像中相较于传统方法能更稳定地重构更多信息 |
| [^112] | [COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits](https://arxiv.org/abs/2403.11348) | 通过概率电路，提出了COLEP框架，实现了可证实鲁棒学习推理一致性预测，其特点在于训练统计模型学习不同语义概念，并利用概率电路实现精确高效推理 |
| [^113] | [Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective](https://arxiv.org/abs/2403.11345) | 本文从均场视角研究了独立强化学习在合作竞争代理中的应用，提出了一种可实现纳什均衡的线性二次结构RL方法，并通过考虑无限代理数量的情况来解决有限人口环境中的非稳态性问题。 |
| [^114] | [Federated Transfer Learning with Differential Privacy](https://arxiv.org/abs/2403.11343) | 本文提出了具有差分隐私的联邦迁移学习框架，通过利用多个异构源数据集的信息来增强对目标数据集的学习，同时考虑隐私约束。 |
| [^115] | [Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans](https://arxiv.org/abs/2403.11338) | 该研究利用最新的CNN-based分割架构PDAtt-Unet，结合3D CNN骨干网络和集成方法，针对Covid-19检测和Covid-19领域自适应挑战进行了研究。 |
| [^116] | [Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects](https://arxiv.org/abs/2403.11332) | 提出了一种结合图神经网络和双机器学习的新方法，能够准确和高效地估计直接和同行效应，处理网络混杂因素，并一致地估计所需的因果效应 |
| [^117] | [Potential of Domain Adaptation in Machine Learning in Ecology and Hydrology to Improve Model Extrapolability](https://arxiv.org/abs/2403.11331) | 本研究讨论了在生态和水文学领域中使用领域自适应技术来改善模型外推能力的潜力，弥补了目前模型普遍存在的地理外推问题。 |
| [^118] | [Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback](https://arxiv.org/abs/2403.11330) | 通过将全局明确标注拆解成本地隐式多模态反馈，提出了一种改进对话代理的方法，并在各种对话度量方面展现出一致的改进 |
| [^119] | [Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts](https://arxiv.org/abs/2403.11314) | 这项研究探讨了如何在Transformer模型中进行逻辑推理，通过在数据集中引入证明来训练两种模型，成功避免了伪相关性和推理捷径。 |
| [^120] | [SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant](https://arxiv.org/abs/2403.11299) | 本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。 |
| [^121] | [A Modified Word Saliency-Based Adversarial Attack on Text Classification Models](https://arxiv.org/abs/2403.11297) | 本文介绍了一种修改的基于词显著性的对抗攻击方法，通过对模型最具影响力的词进行修改，旨在欺骗分类模型且保持语义连贯性，在提高攻击效果的同时避开了分类系统的检测。 |
| [^122] | [Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction](https://arxiv.org/abs/2403.11292) | 提出了一种名为GOOD的图神经网络模型，设计用于解决领域外链接预测问题，采用一种新颖的多关系嵌入聚合设计概念。 |
| [^123] | [Advanced Knowledge Extraction of Physical Design Drawings, Translation and conversion to CAD formats using Deep Learning](https://arxiv.org/abs/2403.11291) | 通过利用深度学习方法，本研究提出了一种创新的方法，利用物体检测模型和边缘检测算法提取物理设计图纸的信息，将其转换为CAD格式。 |
| [^124] | [Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation](https://arxiv.org/abs/2403.11265) | 通过引入合成示例的数据增强方法，可以改善在作者验证任务中对抗性攻击下的分类器预测。 |
| [^125] | [Understanding Diffusion Models by Feynman's Path Integral](https://arxiv.org/abs/2403.11262) | 通过费曼的路径积分引入了扩散模型的新形式，对基于分数的生成模型提供了全面描述，并识别出连接随机和确定性采样方案的插值参数。 |
| [^126] | [A Lie Group Approach to Riemannian Batch Normalization](https://arxiv.org/abs/2403.11261) | 本论文建立了一个Lie群上的统一框架，为黎曼批量归一化（RBN）技术提供了理论保证，并推广了现有的Lie群到对称正定流形上的三类参数化Lie群结构。 |
| [^127] | [A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty](https://arxiv.org/abs/2403.11259) | 通过机器学习模型将用户请求分配给服务器，以解决在移动边缘计算中应用部署问题的二阶段随机规划。 |
| [^128] | [Simple 2D Convolutional Neural Network-based Approach for COVID-19 Detection](https://arxiv.org/abs/2403.11230) | 提出了一种针对CT扫描图像的高级空间切片特征学习框架，通过过滤异常数据和减少数据冗余，提高了分析效果。 |
| [^129] | [Cheap Ways of Extracting Clinical Markers from Texts](https://arxiv.org/abs/2403.11227) | 该论文研究了从文本中提取临床标记的廉价方法，比较了传统机器学习方法和大型语言模型对于提取亮点和生成摘要的效果。 |
| [^130] | [CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations](https://arxiv.org/abs/2403.11220) | 提出了一种用于未知退化下目标检测的链式思维驱动自适应增强器CPA-Enhancer，并将其集成到通用检测器中，有效提升受损图像的检测性能 |
| [^131] | [CBR - Boosting Adaptive Classification By Retrieval of Encrypted Network Traffic with Out-of-distribution](https://arxiv.org/abs/2403.11206) | 本文提出了一种基于ANN的自适应分类方法CBR，可以在不重新训练模型的情况下有效识别新的和现有的类别。 |
| [^132] | [Partitioned Neural Network Training via Synthetic Intermediate Labels](https://arxiv.org/abs/2403.11204) | 该研究提出了一种通过将模型分区到不同GPU上，并生成合成中间标签来训练各个部分的方法，以缓解大规模神经网络训练中的内存和计算压力。 |
| [^133] | [Graph Unitary Message Passing](https://arxiv.org/abs/2403.11199) | 提出了一种名为GUMP的图单元消息传递方法，通过应用单元邻接矩阵来缓解图神经网络中的过度压缩问题。 |
| [^134] | [usfAD Based Effective Unknown Attack Detection Focused IDS Framework](https://arxiv.org/abs/2403.11180) | 提出了两种基于半监督学习的IDS策略，以应对IDS在检测零日或未知攻击方面的不足 |
| [^135] | [Prior-dependent analysis of posterior sampling reinforcement learning with function approximation](https://arxiv.org/abs/2403.11175) | 该研究提出了首个先验依赖性贝叶斯遗憾上界，并对后验抽样强化学习进行了改进分析，提出了一个新的上界结果，实现了对先前基准的方法论提升。 |
| [^136] | [Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks](https://arxiv.org/abs/2403.11173) | 提出了一种多目标进化神经架构搜索方法，针对循环神经网络设计，填补了现代NAS方法中忽视的领域。 |
| [^137] | [Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption](https://arxiv.org/abs/2403.11166) | Pencil是第一个解决协作神经网络训练数据隐私挑战的框架，兼顾模型和数据隐私，不依赖非共谋假设。 |
| [^138] | [A Selective Review on Statistical Methods for Massive Data Computation: Distributed Computing, Subsampling, and Minibatch Techniques](https://arxiv.org/abs/2403.11163) | 该论文选择性综述了大规模数据计算的统计方法，主要集中在分布式计算、子采样以及小批量梯度技术这三类统计计算方法。 |
| [^139] | [CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion](https://arxiv.org/abs/2403.11162) | 该方法提出了一种通过对比梯度反转实现扩散模型的数字版权认证的新方法，通过利用预训练模型和微调模型之间的概念差异来恢复图像的缺失细节。 |
| [^140] | [Is Mamba Effective for Time Series Forecasting?](https://arxiv.org/abs/2403.11144) | Mamba模型作为一种状态空间模型在时间序列预测中具有捕捉复杂依赖关系、近线性复杂度以及性能优势的潜力。 |
| [^141] | [Machine learning-based system reliability analysis with Gaussian Process Regression](https://arxiv.org/abs/2403.11125) | 本文提出了基于高斯过程回归的机器学习系统可靠性分析方法，并通过几个定理探讨了最优学习策略，包括考虑和忽略样本之间的相关性以及顺序多个训练样本增益的理论最优策略。 |
| [^142] | [Phasic Diversity Optimization for Population-Based Reinforcement Learning](https://arxiv.org/abs/2403.11114) | 引入了Phasic Diversity Optimization (PDO)算法，采用种群训练框架，将奖励和多样性训练分为不同阶段，并在辅助阶段实现激进的多样性优化。 |
| [^143] | [Self-Supervised Quantization-Aware Knowledge Distillation](https://arxiv.org/abs/2403.11106) | 提出了一种自监督量化感知知识蒸馏(SQAKD)框架，可以在不需要标记的监督情况下，同时最小化全精度和低比特模型之间的KL损失以及量化的离散化误差，从而避免了繁琐的超参数调整和复杂的训练过程。 |
| [^144] | [ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models](https://arxiv.org/abs/2403.11103) | 通过让大语言模型自省特定领域，生成领域相关属性并创建属性丰富的训练数据，同时绕过复杂结构的挑战，实现了生成命名实体识别数据集的创新策略。 |
| [^145] | [Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance](https://arxiv.org/abs/2403.11100) | 图扩展的性质是该研究的关键，研究发现在修剪的循环神经网络层中保持图的扩展性能可以维持RNN和LSTM的分类准确性。 |
| [^146] | [Learning-Based Pricing and Matching for Two-Sided Queues](https://arxiv.org/abs/2403.11093) | 设计定价和匹配算法以最大化平台利润，在未知需求和供应函数下，保持顾客和服务器队列长度低于阈值 |
| [^147] | [Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via NN-Driven Traffic Analysis at Line-Speed](https://arxiv.org/abs/2403.11090) | 本文介绍了一种名为BoS的方法，通过在数据平面上实现神经网络驱动的流量分析，以推动智能网络数据平面的发展。 |
| [^148] | [Incorporating Higher-order Structural Information for Graph Clustering](https://arxiv.org/abs/2403.11087) | 该论文提出了一种新颖的图聚类网络，利用图的高阶结构信息，并设计了一个图互信息极大化模块，有效地最大化了图级和节点级表示之间的互信息。 |
| [^149] | [RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082) | RobustSentEmbed是一个自监督句子嵌入框架，通过对抗对比学习提高了文本表示任务中的泛化性和稳健性，实现了在对抗攻击中的优越表现。 |
| [^150] | [Bridging Expert Knowledge with Deep Learning Techniques for Just-In-Time Defect Prediction](https://arxiv.org/abs/2403.11079) | 提出了一种模型融合框架，结合了基于专家知识的简单模型和基于深度学习技术的复杂模型，以实现即时缺陷预测的更好性能。 |
| [^151] | [A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization](https://arxiv.org/abs/2403.11062) | 提出了一种简单的混合策略参数化方法，通过整合风险中性策略和可调整策略，提高了CVaR优化的样本效率。 |
| [^152] | [JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks](https://arxiv.org/abs/2403.11048) | 本研究提出了JustQ框架，用于在NISQ计算机上部署公平准确的量子神经网络，包括NISQ错误模型、基于强化学习的部署和同时考虑公平性和准确性的灵活优化目标。实验结果表明JustQ优于先前方法，在准确性和公平性方面表现更优，为未来的研究铺平了道路。 |
| [^153] | [Regulating Chatbot Output via Inter-Informational Competition](https://arxiv.org/abs/2403.11046) | 本文通过探讨信息间竞争，提出利用信息市场本身作为有效减轻AI聊天机器人输出风险的可能性，并指出监管者在面对新技术不确定性时往往过度谨慎，需重新评估监管策略。 |
| [^154] | [Advancing multivariate time series similarity assessment: an integrated computational approach](https://arxiv.org/abs/2403.11044) | 提出了一种名为MTASA的集成计算方法，旨在优化多变量时间序列对齐，并通过多处理引擎提高计算资源利用率，解决了评估多变量时间序列数据相似性时所面临的挑战 |
| [^155] | [FAGH: Accelerating Federated Learning with Approximated Global Hessian](https://arxiv.org/abs/2403.11041) | 提出了一种FL方法FAGH，利用近似全局Hessian和全局梯度的一阶矩来加速全局模型训练，降低通信次数。 |
| [^156] | [FH-TabNet: Multi-Class Familial Hypercholesterolemia Detection via a Multi-Stage Tabular Deep Learning](https://arxiv.org/abs/2403.11032) | 该论文介绍了FH-TabNet，是一个多阶段表格深度学习模型，用于家族性高胆固醇血症的多类检测任务。 |
| [^157] | [Accelerating prototype selection with spatial abstraction](https://arxiv.org/abs/2403.11020) | 提出了一种利用空间抽象加速样本选择过程的方法，可以有效地减少计算资源需求，并在广泛认可的数据集上进行了测试。 |
| [^158] | [Improved Algorithm and Bounds for Successive Projection](https://arxiv.org/abs/2403.11013) | 提出了伪点SPA算法，利用极值理论推导误差界限，相比原始SPA具有更快的收敛速度和更好的数值性能 |
| [^159] | [Forward Learning of Graph Neural Networks](https://arxiv.org/abs/2403.11004) | 图神经网络的成功依赖于反向传播算法，但其存在一些限制，为此提出了前向正向算法作为一种替代方法。 |
| [^160] | [Topologically faithful multi-class segmentation in medical images](https://arxiv.org/abs/2403.11001) | 提出了一种用于医学图像的拓扑保真多类别分割的通用损失函数，通过将N类别分割问题分解为N个单类别分割任务，实现了对神经网络的训练，验证了在四个医学数据集上的有效性 |
| [^161] | [N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields](https://arxiv.org/abs/2403.10997) | 利用Nested Neural Feature Fields (N2F2) 实现了层次化监督学习，提供了对物理维度或语义维度等不同粒度的场景属性全面和细致的理解。 |
| [^162] | [A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2403.10996) | 提出了一个可持续的多智能体深度强化学习框架，利用分散的学习架构，来解决交通路口穿越和自主赛车等问题 |
| [^163] | [Edge Private Graph Neural Networks with Singular Value Perturbation](https://arxiv.org/abs/2403.10995) | 提出了一种新的隐私保护GNN训练算法Eclipse，通过观察图结构中邻接矩阵的低秩行为，实现了在保护边缘隐私的同时保持模型良好效用。 |
| [^164] | [IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning](https://arxiv.org/abs/2403.10984) | 介绍了一种名为\carb 的端到端建模工具，用于在物联网-启用深度学习中精确估算碳足迹，展示了与实际测量值相比最大$\pm21\%$的碳足迹差异。 |
| [^165] | [Enhancing IoT Security Against DDoS Attacks through Federated Learning](https://arxiv.org/abs/2403.10968) | 通过联邦学习，利用物联网设备的集体智慧构建全局模型，保护数据隐私和最小化通信开销，增强物联网对抗DDoS攻击的安全性。 |
| [^166] | [Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization](https://arxiv.org/abs/2403.10967) | 学习上下文世界模型有助于提高在未知上下文下的零样点泛化能力。 |
| [^167] | [Energy-Based Models with Applications to Speech and Language Processing](https://arxiv.org/abs/2403.10961) | 基于能量的模型（EBMs）是一类重要的概率模型，在语音和语言处理等领域吸引了越来越多的关注，因其显著的理论和算法进展。 |
| [^168] | [SelfIE: Self-Interpretation of Large Language Model Embeddings](https://arxiv.org/abs/2403.10949) | 提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。 |
| [^169] | [The Fallacy of Minimizing Local Regret in the Sequential Task Setting](https://arxiv.org/abs/2403.10946) | 研究了强化学习中在序列任务设置下最小化局部遗憾的谬误，揭示了近视地最小化遗憾在实际应用中的复杂性。 |
| [^170] | [ViSaRL: Visual Reinforcement Learning Guided by Human Saliency](https://arxiv.org/abs/2403.10940) | ViSaRL提出了Visual Saliency-Guided Reinforcement Learning（受视觉显著性引导的强化学习）方法，通过学习视觉表示来显著提高RL代理在不同任务上的成功率、样本效率和泛化性能。 |
| [^171] | [Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR](https://arxiv.org/abs/2403.10937) | 通过最小化增强语言模型进行初步解码，以提高低资源ASR中晶格重新打分的语音识别准确性，相对减少了泰卢固语21.8%和卡纳达语41.8%的词误差率，同时仅消耗1/8内存。 |
| [^172] | [Function-space Parameterization of Neural Networks for Sequential Learning](https://arxiv.org/abs/2403.10929) | 提出了一种神经网络的函数空间参数化方法，能够在序列学习中有效整合新数据并保留先前知识，同时在不重新训练的情况下合并新数据。 |
| [^173] | [Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC](https://arxiv.org/abs/2403.10927) | 提出了用于空地协同 MEC 的分布式多目标动态卸载调度方案，结合了MORL和核方法，通过使用n步回报来处理积压中的波动。 |
| [^174] | [Interpretable Machine Learning for TabPFN](https://arxiv.org/abs/2403.10923) | TabPFN模型在低数据情况下实现了良好的分类性能，并能够以秒级速度生成后验预测分布，我们提出了几种专为TabPFN设计的可解释性方法的改进，实现了更高效的计算。 |
| [^175] | [Automatic location detection based on deep learning](https://arxiv.org/abs/2403.10912) | 该研究利用深度学习技术实现了针对印度城市图像的自动位置检测系统，通过两种方法（普通CNN和VGG16模型）获得了高准确度，并突出了优势和改进潜力。 |
| [^176] | [Graph Regularized NMF with L20-norm for Unsupervised Feature Learning](https://arxiv.org/abs/2403.10910) | 引入L20范数约束的图正则化NMF用于无监督特征学习，旨在增强特征稀疏性和减轻噪声影响。 |
| [^177] | [DTOR: Decision Tree Outlier Regressor to explain anomalies](https://arxiv.org/abs/2403.10903) | DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。 |
| [^178] | [List Sample Compression and Uniform Convergence](https://arxiv.org/abs/2403.10889) | 研究在列表学习中均匀收敛和样本压缩原则的适用性，证明了在列表PAC学习中均匀收敛仍然等价于可学习性 |
| [^179] | [Probabilistic World Modeling with Asymmetric Distance Measure](https://arxiv.org/abs/2403.10875) | 学习非对称相似性函数使得我们能够将概率世界动态的几何抽象嵌入到表征空间中，并实现多向概率推理。 |
| [^180] | [stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation](https://arxiv.org/abs/2403.10863) | stMCDI是一种新颖的条件扩散模型，通过利用空间定位转录组数据中的空间位置信息来填补缺失值，同时保持整体数据分布。 |
| [^181] | [FedQNN: Federated Learning using Quantum Neural Networks](https://arxiv.org/abs/2403.10861) | FedQNN框架融合了量子机器学习与经典联邦学习原则，在保护数据隐私的分布式环境中实现了合作学习，具有高准确率。 |
| [^182] | [Neural-Kernel Conditional Mean Embeddings](https://arxiv.org/abs/2403.10859) | 结合深度学习和CME的神经网络方法，解决了核条件均值嵌入面临的可伸缩性和表现力挑战，并在条件密度估计任务和强化学习中展现出卓越性能。 |
| [^183] | [Reinforcement Learning with Options](https://arxiv.org/abs/2403.10855) | 本论文提出了使用选项的分层强化学习方法，通过构建Hierarchical Policy learning来解决高维复杂环境中学习的问题。 |
| [^184] | [Just Say the Name: Online Continual Learning with Category Names Only via Data Generation](https://arxiv.org/abs/2403.10853) | 提出了在线连续学习框架G-NoCL，采用生成数据并利用DIverSity和COmplexity enhancing ensemBlER（DISCOBER）进行数据融合，展示了其在在线连续学习基准测试中的优越性能。 |
| [^185] | [Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process](https://arxiv.org/abs/2403.10842) | 本研究提出一种新颖的双Transformer模型，结合门控动态可学习注意机制，用于田纳西伊斯曼过程的故障检测与诊断，提高性能通过独立处理输入数据和提取多样化信息，以及动态学习适应性调整注意策略。 |
| [^186] | [SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation](https://arxiv.org/abs/2403.10834) | SF(DA)$^2$是一种新颖的无源域自适应方法，通过在特征空间中构建增强图并利用谱邻域聚类来识别分区，实现了数据增强的好处而避免了挑战。 |
| [^187] | [LookALike: Human Mimicry based collaborative decision making](https://arxiv.org/abs/2403.10824) | 提出了一种新颖的方法，通过在LLM代理之间进行知识蒸馏，实现实时的人类角色扮演，保留独特上下文，而不依赖任何存储数据或预训练，并展示出在模拟真实世界任务中表现优于现有技术。 |
| [^188] | [Incentivized Exploration of Non-Stationary Stochastic Bandits](https://arxiv.org/abs/2403.10819) | 提出了针对非平稳随机赌博机的激励探索算法，实现了随时间的子线性遗憾和补偿 |
| [^189] | [FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning](https://arxiv.org/abs/2403.10807) | FlyKD提出了一种名为飞行中的知识蒸馏，通过结合课程学习，能够生成几乎无限数量的伪标签，极大缓解了在嘈杂伪标签上的优化过程，并显示出优于基准KD和局部结构保持图卷积网络的性能。 |
| [^190] | [Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion](https://arxiv.org/abs/2403.10803) | 提出了一种名为MLOD的新框架，利用多测试过程在不同级别的特征中识别测试样本中的分布偏移，无需修改预训练模型结构。 |
| [^191] | [Anomaly Detection Based on Isolation Mechanisms: A Survey](https://arxiv.org/abs/2403.10802) | 基于隔离机制的异常检测是一种新颖有效的方法，具有低计算复杂性、低内存使用、高可伸缩性、对噪声和无关特征的稳健性，以及不需要先验知识或繁重参数调整。 |
| [^192] | [Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders](https://arxiv.org/abs/2403.10800) | 模型重新编程方法 Reprogrammer 在文本图像编码器中的应用优于传统微调方法，能够提高下游模型在分布内和分布外数据中的性能表现 |
| [^193] | [Efficient Pruning of Large Language Model with Adaptive Estimation Fusion](https://arxiv.org/abs/2403.10799) | 提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。 |
| [^194] | [From Words to Routes: Applying Large Language Models to Vehicle Routing](https://arxiv.org/abs/2403.10795) | 该研究探索了应用大型语言模型解决车辆路径规划问题的能力，提出了基于自然语言任务描述的基本提示范例并提出一种使模型进行改进的框架。 |
| [^195] | [Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games](https://arxiv.org/abs/2403.10794) | 该研究提出了一种在对抗性多智能体游戏中应用扩散-强化学习的分层运动规划方法，通过整合高级扩散模型和低级RL算法，实现比基准方法更高效率的运动规划。 |
| [^196] | [QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines](https://arxiv.org/abs/2403.10790) | QuantumLeak是一种有效准确的方法，用于从基于云的NISQ机器中提取QNN模型，并且相较于现有的经典模型提取技术，可以在不同数据集和VQC架构下提高本地VQC准确率。 |
| [^197] | [Time Series Representation Learning with Supervised Contrastive Temporal Transformer](https://arxiv.org/abs/2403.10787) | 提出了一种名为SCOTT的具有监督对比变换器的时间序列表示学习模型，结合了Transformer和Temporal Convolutional Networks以学习全局和局部特征，并简化了用于标记时间序列数据的监督对比损失。 |
| [^198] | [ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models](https://arxiv.org/abs/2403.10786) | ContourDiff是一种新颖的框架，利用图像的领域不变解剖轮廓表示，旨在帮助准确翻译医学图像并保持其解剖准确性。 |
| [^199] | [From Melting Pots to Misrepresentations: Exploring Harms in Generative AI](https://arxiv.org/abs/2403.10776) | 探索生成式AI中的社会有害影响，提出对多样性和公平性的关切，并引领讨论在这些模型中的重要性，同时提出未来研究方向。 |
| [^200] | [A Probabilistic Approach for Alignment with Human Comparisons](https://arxiv.org/abs/2403.10771) | 通过提出的两阶段“监督微调+人类比较”框架，本文研究了如何有效利用人类比较来改善AI模型的对齐，特别是在面对嘈杂数据和高维模型时。 |
| [^201] | [ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference](https://arxiv.org/abs/2403.10766) | 本文提出了一种在纵向设置中使用封闭形式常微分方程（ODE）的解决方案，相较于传统的神经网络推断，具有更好的可解释性和不规则性。 |
| [^202] | [A Primal-Dual Algorithm for Faster Distributionally Robust Optimization](https://arxiv.org/abs/2403.10763) | 这种原始-对偶算法在分布鲁棒优化问题中实现了最先进的线性收敛速度。 |
| [^203] | [Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning](https://arxiv.org/abs/2403.10761) | 本文提出了一个通过深度强化学习对无人机和移动充电器进行调度的方法，以解决无人机在观察任务中的航线规划和充电优化问题。 |
| [^204] | [A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling](https://arxiv.org/abs/2403.10748) | Latent Space Dynamics Identification (LaSDI)框架能将高保真数据转换为简单的ODEs，并且每个构件可以根据应用灵活调节，为简约建模带来新的可能性。 |
| [^205] | [Horizon-Free Regret for Linear Markov Decision Processes](https://arxiv.org/abs/2403.10738) | 该论文提出了第一个适用于线性马尔可夫决策过程的无视规划时Horizon的界限，与先前的方法相比，直接估计值函数和置信区间，避免显式估计转换模型和计算不同时间步长的非齐次值函数。 |
| [^206] | [Variance-Dependent Regret Bounds for Non-stationary Linear Bandits](https://arxiv.org/abs/2403.10732) | 提出利用奖励分布方差和变化预算的算法，可以实现更紧的遗憾上限界限。 |
| [^207] | [Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation](https://arxiv.org/abs/2403.10731) | 提出了一种改进条件人类图像生成的两阶段方法，首先训练手部生成器产生手部图像和分割掩模，在第二阶段使用改进的 ControlNet 模型绘制生成手部周围的身体。 |
| [^208] | [Counterfactual Analysis of Neural Networks Used to Create Fertilizer Management Zones](https://arxiv.org/abs/2403.10730) | 本研究提出了一种基于肥料响应性的管理区聚类方法，利用神经网络生成氮响应曲线并进行特征化分析。 |
| [^209] | [Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency](https://arxiv.org/abs/2403.10717) | 通过利用比例预测一致性技术，本研究提出了一种在被毒化数据集中自动识别后门数据的方法，无需额外干净数据或手动定义后门检测阈值。 |
| [^210] | [Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns](https://arxiv.org/abs/2403.10707) | 本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。 |
| [^211] | [PERL: Parameter Efficient Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2403.10704) | 使用低秩适应（LoRA）方法进行参数高效强化学习（PERL），能够在与传统RLHF设置相当的性能下，实现更快的训练和更少的内存占用。 |
| [^212] | [On the low-shot transferability of [V]-Mamba](https://arxiv.org/abs/2403.10696) | [V]-Mamba相对于ViTs在利用线性探查进行迁移时展现出更优秀或等效的少样本学习能力，但在采用视觉提示作为迁移方法时表现出较弱或类似的表现。 |
| [^213] | [MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling](https://arxiv.org/abs/2403.10691) | MYTE是一种基于形态学的字节编码范式，通过使用具有一致大小的片段来实现跨不同语言的信息编码，为99种语言提供了更短的编码，特别是对非欧洲语言和非拉丁文字的改进最为显著。 |
| [^214] | [Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning](https://arxiv.org/abs/2403.10689) | 该研究提出了一种视觉到触觉-音频跨模态迁移学习方法，实现了潜在物体特征的识别。 |
| [^215] | [AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs](https://arxiv.org/abs/2403.10686) | AutoHLS提出了一个集成深度神经网络和贝叶斯优化的框架，用于加速HLS硬件设计优化，实现高达70倍的探索时间加速 |
| [^216] | [Evaluation of GlassNet for physics-informed machine learning of glass stability and glass-forming ability](https://arxiv.org/abs/2403.10682) | 对GlassNet模型在预测玻璃稳定性参数方面的应用进行了评估，探索了使用这些参数来估计玻璃的形成能力的可行性。 |
| [^217] | [Riemannian Flow Matching Policy for Robot Motion Learning](https://arxiv.org/abs/2403.10672) | RFMP是一种新颖的模型，利用流匹配的优势在机器人视觉运动策略中具有高效训练和推断能力，并通过融合黎曼流形上的几何意识，提供更平滑的动作轨迹。 |
| [^218] | [Hessian-Free Laplace in Bayesian Deep Learning](https://arxiv.org/abs/2403.10671) | 提出了一种无Hessian计算和求逆的Hessian-Free Laplace近似框架，通过对数后验和网络预测的曲率来估计后验的方差。 |
| [^219] | [Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data](https://arxiv.org/abs/2403.10663) | 通过使用多视角数据为深度神经网络添加水印，可以有效防御对源模型功能的窃取攻击 |
| [^220] | [InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning](https://arxiv.org/abs/2403.10658) | InterLUDE提出了一种新的半监督学习方法，通过两部分相互作用来增强SSL，包括嵌入融合和基于一致性正则化的新损失函数，实验证明该方法在图像分类和医学任务上取得显著改进。 |
| [^221] | [Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization](https://arxiv.org/abs/2403.10652) | 使用子群阈值优化（STO）技术，在不改动训练数据和底层机器学习算法的情况下，优化各个子群的分类阈值，以最小化整体歧视分数，从而提高信贷借贷模型的公平性。 |
| [^222] | [PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation](https://arxiv.org/abs/2403.10650) | 本研究通过对模型预测不确定性的量化来选择需要进一步适应的层，从而克服了持续测试时间自适应方法中由于伪标签引起的不准确性困扰。 |
| [^223] | [A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks](https://arxiv.org/abs/2403.10646) | 本研究调查了用于基于机器学习的网络安全任务的源代码表示方法，发现基于图的表示是最受欢迎的，而分词器和抽象语法树是最流行的两种表示方法。 |
| [^224] | [Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs](https://arxiv.org/abs/2403.10642) | 通过集成多个神经算子来提高对区域外学习的不确定性估计，从而解决现有方法在OOD测试输入上的失败 |
| [^225] | [A resource-constrained stochastic scheduling algorithm for homeless street outreach and gleaning edible food](https://arxiv.org/abs/2403.10638) | 该研究针对无家可归者和食品银行的资源受限外展问题，提出了一种基于Thompson抽样与马尔可夫链恢复的算法，显著优于基线算法。 |
| [^226] | [Limits of Approximating the Median Treatment Effect](https://arxiv.org/abs/2403.10618) | 估计中位数差异比估计中位数处理效应更容易的问题是因果推断的基本问题 |
| [^227] | [DiPaCo: Distributed Path Composition](https://arxiv.org/abs/2403.10616) | DiPaCo提出了一种协同模块化架构和训练方法，可以通过路径分发计算，实现机器学习模型的训练，并在推断时无需模型压缩。 |
| [^228] | [LightIt: Illumination Modeling and Control for Diffusion Models](https://arxiv.org/abs/2403.10615) | LightIt提出了一种用于图像生成的显式照明控制方法，通过条件生成来实现对图像生成的照明控制，同时训练了一个身份保持的重照模型。 |
| [^229] | [Sequential Monte Carlo for Inclusive KL Minimization in Amortized Variational Inference](https://arxiv.org/abs/2403.10610) | 用顺序蒙特卡洛采样器估计inclusive KL散度梯度，提出了三种梯度估计器，解决了现有方法的偏差梯度和高度集中变分分布问题。 |
| [^230] | [SurvRNC: Learning Ordered Representations for Survival Prediction using Rank-N-Contrast](https://arxiv.org/abs/2403.10603) | 提出了SurvRNC方法，通过引入损失函数作为正则化器来获得基于生存时间的有序表示，能处理被截尾数据，并可整合到任何生存模型中。 |
| [^231] | [From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction](https://arxiv.org/abs/2403.10586) | 机器学习技术在非肌层侵袭性膀胱癌复发预测中具有潜在作用，可以提高准确性，降低治疗成本，并有效规划治疗方案 |
| [^232] | [Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint](https://arxiv.org/abs/2403.10585) | 提出了一种通过后验抽样解决一般性噪声逆问题的方法，采用策略梯度视角，通过扩散策略梯度（DPG）精确估计指导评分函数，实现对多种线性和非线性逆任务的鲁棒性解决，提高了图像恢复质量。 |
| [^233] | [How Suboptimal is Training rPPG Models with Videos and Targets from Different Body Sites?](https://arxiv.org/abs/2403.10582) | 许多rPPG模型是使用面部视频和手指接触PPG测量作为目标进行训练的，但不同身体部位的PPG信号具有不同的形态特征，这样训练面部视频的rPPG模型可能是次优的。 |
| [^234] | [Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction](https://arxiv.org/abs/2403.10581) | 提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。 |
| [^235] | [Generative Modelling of Stochastic Rotating Shallow Water Noise](https://arxiv.org/abs/2403.10578) | 本文提出一种用于校准流体动力学随机偏微分方程中噪声的通用方法，使用生成模型技术取代了以往的PCA技术，这能够避免对增量施加额外约束。 |
| [^236] | [Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain](https://arxiv.org/abs/2403.10576) | 利用非语言元素进行网络安全领域的预训练，提出了新的预训练方法并在网络安全领域中取得了优越表现 |
| [^237] | [Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking](https://arxiv.org/abs/2403.10573) | 引入医学数据中的难以察觉噪声来保护数据，防止未经授权的训练，尤其适用于生物医学数据领域。 |
| [^238] | [Discovering Invariant Neighborhood Patterns for Heterophilic Graphs](https://arxiv.org/abs/2403.10572) | 本文提出了一种新颖的不变邻域模式学习方法，通过自适应邻域传播模块和不变非同源图学习模块，解决了非同源图上邻域模式分布偏移问题。 |
| [^239] | [JaxDecompiler: Redefining Gradient-Informed Software Design](https://arxiv.org/abs/2403.10571) | JaxDecompiler是一种将JAX函数转换为可编辑Python代码的工具，简化了基于梯度信息的软件开发中的反向工程、理解、定制和互操作过程。 |
| [^240] | [Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment](https://arxiv.org/abs/2403.10569) | 通过在Xception上实施高效的参数缩减策略，该研究在资源受限的边缘环境中实现了DNN的帕累托最优性，提高了模型的准确性，减少了内存利用，且在Caltech-101图像分类中表现优于原始Xception和轻量级模型。 |
| [^241] | [MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts](https://arxiv.org/abs/2403.10568) | 本文提出了MoPE技术，通过解开提示以自适应捕获数据集级和实例级特征，引入了混合Prompt专家来增强表达能力，并且在多模态融合中表现出更大的表达能力和可扩展性。 |
| [^242] | [Uncertainty estimation in spatial interpolation of satellite precipitation with ensemble learning](https://arxiv.org/abs/2403.10567) | 引入九种集成学习器并利用新颖特征工程策略，结合多种分位数回归算法，填补了空间插值中集成学习的不确定性估计领域的研究空白 |
| [^243] | [Cooling-Guide Diffusion Model for Battery Cell Arrangement](https://arxiv.org/abs/2403.10566) | 导入冷却引导扩散模型的生成式AI方法优化了电池单元布局，显著降低了单元的最高温度，并在冷却效率方面具有独特优势。 |
| [^244] | [Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks](https://arxiv.org/abs/2403.10562) | 无状态策略通过评估反样本对抗黑盒查询，有效引入了防御者有利的不对称性。这种防御方法能够欺骗攻击者寻找对抗性示例、保持模型在合法输入上的准确性，且适用于多种攻击类型。 |
| [^245] | [A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024](https://arxiv.org/abs/2403.10561) | AAAI 2024年人本主义表示学习研讨会的被接受论文集合。部分论文选择退出。 |
| [^246] | [Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI](https://arxiv.org/abs/2403.10559) | 生成模型与联网自动驾驶车辆的整合有望提升自动车辆的预测建模、模拟精度和决策流程，对交通行业的安全和创新具有潜在推动作用。 |
| [^247] | [Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack](https://arxiv.org/abs/2403.10558) | 本文提出了一种针对模型反演攻击的自适应混合遮盖算法，通过在频域中使用MixUp策略对人脸图像进行遮盖，以在隐私保护和准确性之间取得平衡。 |
| [^248] | [Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models](https://arxiv.org/abs/2403.10557) | 本论文通过二阶信息（Hessian）的视角重新审视了大型语言模型的机器遗忘问题，提出了遗忘算法，具有较高的鲁棒性。 |
| [^249] | [KARINA: An Efficient Deep Learning Model for Global Weather Forecast](https://arxiv.org/abs/2403.10555) | KARINA模型通过结合ConvNext、SENet和Geocyclic填充，在2.5°分辨率下提升天气预测准确性，只需较少的计算资源，展示出与更高分辨率模型相当的预测精度。 |
| [^250] | [Learning to Watermark LLM-generated Text via Reinforcement Learning](https://arxiv.org/abs/2403.10553) | 通过将信号嵌入LLM的权重中，我们设计一种模型级水印，有效追踪生成文本的滥用情况，并提出基于强化学习的协同训练框架，使水印更准确、更稳健且更适应新攻击。 |
| [^251] | [Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer](https://arxiv.org/abs/2403.10552) | 通过师生无数据知识传输，实现在未知陌生地点上训练自定位模型，不仅可以处理各种类型的开放式老师，还有效避免依赖于师生私人数据可用性。 |
| [^252] | [Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows](https://arxiv.org/abs/2403.10550) | 通过生成伪异常样本和使用双向归一化流模块，实现对异常数据的半监督检测。 |
| [^253] | [On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems](https://arxiv.org/abs/2403.10549) | 在低功耗极端边缘嵌入式系统上，提出了一个完全设备端域自适应系统，能够实现高达14%的准确性提升，仅需少于10 kB的内存和100个标记的话语，在适应复杂的语音噪声后能够恢复准确性。 |
| [^254] | [Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing](https://arxiv.org/abs/2403.10547) | 研究了在强污染模型中寻找SOSP的问题，提出了一般框架以\emph{独立于维度}的精度保证高效地找到近似SOSP，具有对抗异常值的鲁棒性，同时将该框架应用于低秩矩阵感知问题，发展了能够容忍数据破坏的高效且可证明鲁棒性的算法。 |
| [^255] | [Distinguishing Neighborhood Representations Through Reverse Process of GNNs for Heterophilic Graphs](https://arxiv.org/abs/2403.10543) | 通过GNN的反向传播过程，可以显著改善异质图中节点表示的区分度，并在许多情况下提高分类性能。 |
| [^256] | [MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications](https://arxiv.org/abs/2403.10538) | 该论文介绍了MATADOR，一个用于边缘应用的自动化片上Tsetlin机设计生成系统，实现了将ML模型转换为SoC-FPGA解决方案的高效方法。 |
| [^257] | [A survey of synthetic data augmentation methods in computer vision](https://arxiv.org/abs/2403.10075) | 该论文调查了计算机视觉中合成数据增强方法，涵盖了基于逼真3D图形建模、神经风格转移、差分神经渲染和生成的数据合成方法。 |
| [^258] | [A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage](https://arxiv.org/abs/2403.09701) | 混合强化学习算法中，通过将离线数据集包含在在线算法的经验重放缓冲区中进行启动，可以实现类似于基于离线数据分布引导在线探索的可证明收益，即使离线数据集没有单一策略可集中性。 |
| [^259] | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力 |
| [^260] | [Optimistic Verifiable Training by Controlling Hardware Nondeterminism](https://arxiv.org/abs/2403.09603) | 提出了一种方法，结合了在比目标模型更高精度下进行训练、在中间计算步骤后进行四舍五入，并基于自适应阈值存储四舍五入决策，以应对硬件非确定性对训练过程的影响。 |
| [^261] | [LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection](https://arxiv.org/abs/2403.09209) | 该论文的贡献是提出了一个名为LAN的框架，能够实时在活动级别进行内部威胁检测，并学习活动序列内的时间依赖关系和活动之间的关系。 |
| [^262] | [AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning](https://arxiv.org/abs/2403.09113) | AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。 |
| [^263] | [Fast Inference of Removal-Based Node Influence](https://arxiv.org/abs/2403.08333) | 提出了一种评估节点影响的新方法，通过测量训练好的图神经网络模型在移除节点后的预测变化，以实现快速推断。 |
| [^264] | [Deep Submodular Peripteral Network](https://arxiv.org/abs/2403.08199) | 引入了深度子模逆点网络（DSPNs），并提出了一种使用对比学习启发的GPC-ready策略进行训练的方法，以应对子模函数学习中的两大挑战。 |
| [^265] | [EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech](https://arxiv.org/abs/2403.08164) | 提出了一种基于深度卷积神经网络的轻量级TTS系统，采用两阶段训练而非递归单元，可以显著减少训练时间和经济成本 |
| [^266] | [Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings](https://arxiv.org/abs/2403.07454) | 使用结构混合概率分布提供了准确的后验推断，同时具有更小的计算占用量，相较于现有的基于神经网络的SBI方法。 |
| [^267] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^268] | [AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation](https://arxiv.org/abs/2403.07030) | 提出了一种名为AuG-KD的方法，通过利用基于锚点的混合生成，解决了无数据知识蒸馏中的知识转移挑战。 |
| [^269] | [Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning](https://arxiv.org/abs/2403.06880) | 研究探讨了幼儿启发的奖励转换如何影响强化学习任务的样本效率和成功率，特别是发现了幼儿启发的稀疏转密集（S2D）转换的有效性。 |
| [^270] | [ALaRM: Align Language Models via Hierarchical Rewards Modeling](https://arxiv.org/abs/2403.06754) | ALaRM是第一个从人类反馈中建模分层奖励的框架，通过整合整体奖励与特定方面的奖励，改善了大型语言模型与人类偏好的对齐性，尤其在复杂文本生成任务中表现出更精确和一致的指导。 |
| [^271] | [Ricci flow-based brain surface covariance descriptors for Alzheimer disease](https://arxiv.org/abs/2403.06645) | 本文首次提出了一种基于Ricci流的大脑表面协方差描述符的流水线，可以用于诊断阿尔茨海默病。 |
| [^272] | [TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic Analysis and Generation](https://arxiv.org/abs/2403.05822) | TrafficGPT 是一个深度学习模型，旨在突破令牌长度限制，实现高效的长时间流量分析和生成，解决了网络流量分析和生成中依赖标记数据和生成符合实际模式的流量样本的难题。 |
| [^273] | [MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process](https://arxiv.org/abs/2403.05751) | 提出了一种新颖的MG-TSD模型，利用数据内在粒度水平作为目标来引导学习过程，实现了状态-of-the-art的预测性能 |
| [^274] | [Effectiveness Assessment of Recent Large Vision-Language Models](https://arxiv.org/abs/2403.04306) | 本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。 |
| [^275] | [Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning](https://arxiv.org/abs/2403.03835) | Cobweb是一种类似人类类别学习系统，采用类别效用度量构建分层组织的类似树状结构，能够捕捉心理效应并在单一模型中展现出实例和原型学习的灵活性，为将来研究人类类别学习提供了基础。 |
| [^276] | [DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models](https://arxiv.org/abs/2403.03768) | DeepCRE是一种新型的计算模型，在患者级别CRE性能上平均提高了17.7％，在指示级别CRE增加了5倍，并成功确定了六个具有显着优势的药物候选者。 |
| [^277] | [Non-Convex Stochastic Composite Optimization with Polyak Momentum](https://arxiv.org/abs/2403.02967) | 本文研究了具有Polyak动量的随机近端梯度方法，在非凸复合优化问题中实现了最佳收敛速度，无论批量大小如何。 |
| [^278] | [A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends](https://arxiv.org/abs/2403.02292) | 通过分析谷歌应用商店上1200万条隐私相关评论，研究了十年间隐私评论的大规模趋势，发现隐私评论呈现持续增长，探讨了热门和逐渐减少的隐私话题，以及不同国家用户对隐私问题看法的差异。 |
| [^279] | [TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models](https://arxiv.org/abs/2403.02221) | TPLLM提出了基于预训练大语言模型的交通预测框架，能够在历史交通数据有限的地区实现准确预测和良好泛化能力 |
| [^280] | [SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction](https://arxiv.org/abs/2403.01570) | 提出SERVAL，一个协同学习流水线，可以通过相互增强，实现LLMs和小模型的垂直能力无监督开发，从而改善领域特定垂直问题的零-shot预测能力。 |
| [^281] | [Polynormer: Polynomial-Expressive Graph Transformer in Linear Time](https://arxiv.org/abs/2403.01232) | Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。 |
| [^282] | [ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework](https://arxiv.org/abs/2403.00781) | 这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。 |
| [^283] | [Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network](https://arxiv.org/abs/2403.00033) | 通过结合动态内在功能网络和LSTM技术，使用高阶注意力模块进行信息融合和消息传递，提出了HOGAB模型，对慢性大麻用户的静息态fMRI数据进行分析，提高了多图分类的准确性。 |
| [^284] | [Learning-Based Algorithms for Graph Searching Problems](https://arxiv.org/abs/2402.17736) | 本研究提出了针对未知图的图搜索问题的基于学习的算法，首次在未知加权图上建立了形式保证，并设计算法在预测误差上具有最优或几乎最佳依存关系。 |
| [^285] | [Enhancing Continuous Domain Adaptation with Multi-Path Transfer Curriculum](https://arxiv.org/abs/2402.16681) | 本文提出了一种新颖的CDA方法W-MPOT，通过构建基于Wasserstein距离的传递课程，严格解决了领域排序和错误累积问题，实现了将源模型通过多条有效路径转移到目标域。 |
| [^286] | [Truly No-Regret Learning in Constrained MDPs](https://arxiv.org/abs/2402.15776) | 本文首次肯定回答了一个开放问题，即是否可以在不允许错误抵消的情况下，通过将一种常见的安全约束模型扩展到具有多个约束的CMDPs，提出了一种可以实现次线性后悔的新方法。 |
| [^287] | [OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2402.15321) | 提供了OpenSUN3D研讨会上针对开放词汇3D场景理解的挑战概述，包括挑战数据集、评估方法和获胜方法的简要描述 |
| [^288] | [Learning solution operators of PDEs defined on varying domains via MIONet](https://arxiv.org/abs/2402.15097) | 通过MIONet学习定义在不同域上的PDE的解算子，实现了解映射的学习，包括各种参数的变化，结果为进一步处理度量空间的逼近理论提供了洞见。 |
| [^289] | [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](https://arxiv.org/abs/2402.14899) | 该研究评估了多模态LLMs在采用串联推理时的对抗鲁棒性，发现串联推理在一定程度上提高了对抗性鲁棒性，但引入了一种新的停止推理攻击技术成功规避了这种增强。 |
| [^290] | [A Temporal Bias Correction using a Machine Learning Attention model](https://arxiv.org/abs/2402.14169) | 本论文提出了一种新颖的偏差校正方法，将校准视为概率模型而不是算法流程，利用机器学习概率注意力模型来适配偏差校正任务，可准确校正具有长期时间属性的气候统计数据，提高了在这些数据上进行可靠影响研究的准确性。 |
| [^291] | [GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence](https://arxiv.org/abs/2402.12566) | GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。 |
| [^292] | [Generative Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2402.11887) | 提出了一种用于半监督图异常检测的生成式方法，通过生成模拟异常节点来训练判别性单类分类器，以更好地利用图中的已知正常节点。 |
| [^293] | [Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2402.11702) | 大型语言模型在代码生成方面表现出显著能力，但目前主要用于展示概念或提供示例，需要进一步改进才能实现生产就绪代码。 |
| [^294] | [Can we soft prompt LLMs for graph learning tasks?](https://arxiv.org/abs/2402.10359) | 引入了GraphPrompter框架，通过软提示将图信息与LLMs对齐，以进一步探究LLMs理解图信息的潜力。 |
| [^295] | [HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments](https://arxiv.org/abs/2402.10228) | HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。 |
| [^296] | [Revisiting Recurrent Reinforcement Learning with Memory Monoids](https://arxiv.org/abs/2402.09900) | 这篇论文重新审视了使用内存单子的循环强化学习方法。通过定义新颖的内存单子框架并提出一种新的批处理方法，改进了样本效率、增加了回报并简化了实现过程。 |
| [^297] | [EcoVal: An Efficient Data Valuation Framework for Machine Learning](https://arxiv.org/abs/2402.09288) | EcoVal是一种高效的机器学习数据估值框架，通过估计每个数据的内在和外在价值，实现了快速实用地估算机器学习模型数据的价值。 |
| [^298] | [Cartesian atomic cluster expansion for machine learning interatomic potentials](https://arxiv.org/abs/2402.07472) | 本论文提出了一种改进的机器学习原子间势模型，使用基于笛卡尔坐标的原子密度展开来替代传统的原子团簇展开方法，并结合低维嵌入和原子间消息传递。该模型在不同系统中表现出良好的准确性、稳定性和普适性。 |
| [^299] | [A Novel Gaussian Min-Max Theorem and its Applications](https://arxiv.org/abs/2402.07356) | 本文介绍了一个新的高斯最小最大定理，扩展了经典定理对于独立但非恒定分布的情况。此外，该定理在高维统计学、机器学习、非光滑优化和信号处理等领域有广泛的应用。 |
| [^300] | [Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2402.05808) | 本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。 |
| [^301] | [Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations](https://arxiv.org/abs/2402.05713) | 该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。 |
| [^302] | [The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents](https://arxiv.org/abs/2402.03220) | 该论文研究了在两层神经网络中学习多指数目标函数时，重复使用批次的梯度下降（GD）的训练动态。研究发现，与单次GD相比，多次GD能够克服目标函数的限制，仅需两个时间步骤就能实现网络与目标子空间的重叠，展示了在有限时间内有效学习的广泛函数类。这些结果基于动力平均场理论（DMFT）的分析。 |
| [^303] | [How Free is Parameter-Free Stochastic Optimization?](https://arxiv.org/abs/2402.03126) | 这个论文研究了无参随机优化的问题，提出了一种完全无参的方法，通过简单的超参数搜索技术在非凸和凸设置下都能取得优于先进算法的性能。同时，论文还建立了一个下界，指出完全无参的方法在某些情况下无法实现。 |
| [^304] | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544) | LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。 |
| [^305] | [Multimodal Co-orchestration for Exploring Structure-Property Relationships in Combinatorial Libraries via Multi-Task Bayesian Optimization](https://arxiv.org/abs/2402.02198) | 本研究提出并实现了一种多模态协同编排的方法，通过多任务贝叶斯优化来探索组合库中的结构-性质关系。该方法利用变分自动编码器的降维和表示学习来控制复杂可观测量的测量。 |
| [^306] | [Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction](https://arxiv.org/abs/2401.15603) | 该论文提出了一种特征值修正策略，可以提升谱图神经网络的表达能力，使多项式滤波器摆脱重复特征值输入的限制，并增强了特征值的均匀分布。 |
| [^307] | [Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D](https://arxiv.org/abs/2401.12046) | 提出了一种利用双SE(d)xSE(d)对称性的Fourier Transporter（FourTran）方法，用于实现在3D环境中更高的样本效率，在RLbench基准测试中取得了最先进的结果。 |
| [^308] | [Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise](https://arxiv.org/abs/2312.14567) | 填补了理论空白，建立了一种非渐近收敛界，证明了在各向异性梯度噪声条件下，随机重球方法在二次目标上可以提供$\tilde{\mathcal{O}}(\sqrt{\kappa})$的加速收敛。 |
| [^309] | [Diffusion Reward: Learning Rewards via Conditional Video Diffusion](https://arxiv.org/abs/2312.14134) | 通过条件视频扩散学习奖励，解决复杂视觉强化学习问题，有效提高了任务成功率，超越了基线方法。 |
| [^310] | [Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances](https://arxiv.org/abs/2312.12741) | 提出了一种在双臂高斯赌臂机器人中解决具有未知方差情况下局部最优固定预算最佳臂识别问题的策略。 |
| [^311] | [Multi-Objective Reinforcement Learning-based Approach for Pressurized Water Reactor Optimization](https://arxiv.org/abs/2312.10194) | 提出了一种新的多目标强化学习方法PEARL，通过学习单一策略解决多目标问题，避免了需要多个神经网络解决简单子问题的需求。 |
| [^312] | [CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize with Binary Linear Programs](https://arxiv.org/abs/2312.07718) | CaVE提出了一种新的端到端训练方法，即预测然后优化，通过将预测的成本向量与真实最优解的法线锥对齐，实现了决策感知型学习模型的训练。 |
| [^313] | [Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2312.05320) | 本研究使用去噪扩散概率模型为气动翼流场模拟训练了不确定性感知代理模型，成功捕捉整个解分布并准确估计模拟不确定性。 |
| [^314] | [Language Model Knowledge Distillation for Efficient Question Answering in Spanish](https://arxiv.org/abs/2312.04193) | 通过知识蒸馏，我们开发了SpanishTinyRoBERTa，一个基于RoBERTa的西班牙语压缩语言模型，用于提高西班牙语问答的效率。 |
| [^315] | [A Novel Federated Learning-Based IDS for Enhancing UAVs Privacy and Security](https://arxiv.org/abs/2312.04135) | 本论文引入了基于联邦学习的入侵检测系统(FL-IDS)，旨在解决FANETs中集中式系统所遇到的挑战，降低了计算和存储成本，适合资源受限的无人机。 |
| [^316] | [A Comprehensive Evaluation of Augmentations for Robust OOD Self-Supervised Contrastive Phonocardiogram Representation Learning](https://arxiv.org/abs/2312.00502) | 本研究通过对比自监督学习应用于1D心音图样本中异常检测，进行了广泛的音频增强方法比较评估和在多个数据集上训练分类器的研究。 |
| [^317] | [Dataset Distillation via the Wasserstein Metric](https://arxiv.org/abs/2311.18531) | 通过引入Wasserstein距离及其重心，我们提出一种有效的数据集精炼方法，利用先验知识提高分布匹配效果，实现了新的最先进性能。 |
| [^318] | [Multinomial belief networks](https://arxiv.org/abs/2311.16909) | 提出了一种深度生成模型，用于处理具有多项式计数数据的分析需求，并能够从数据中完全自动提取出生物意义的元签名。 |
| [^319] | [Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention](https://arxiv.org/abs/2311.16834) | 该论文提出了一种新颖的模块化神经网络模型，用于多变量时间序列预测，通过循环神经网络学习时间依赖关系，并使用基于注意力的特征选择组件实现解释性，实验结果表明其优于当前最先进的解释性神经加性模型（NAM）及其变体。 |
| [^320] | [Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation](https://arxiv.org/abs/2311.16199) | Symphony提出了一种新颖的$E(3)$-等变自回归生成模型，通过使用点对称球形谐波信号来高效建模分子的3D几何结构。 |
| [^321] | [UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition](https://arxiv.org/abs/2311.15599) | 提出了四项用于设计大卷积神经网络的架构指南，并展示了在多模态领域具有通用感知能力 |
| [^322] | [Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning](https://arxiv.org/abs/2311.15487) | 通过几何调整的梯度下降，在深度学习中以均匀指数速率实现全局$\mathcal{L}^2$最小化，这一方法在过参数化情况下具有明确自然的不变几何含义。 |
| [^323] | [CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning](https://arxiv.org/abs/2311.14272) | 提出了CRISP，一种结合精细N:M结构稀疏性和粗粒度块稀疏性的混合结构稀疏模式，旨在增强基于类别的模型剪枝效率。 |
| [^324] | [Molecular Identification and Peak Assignment: Leveraging Multi-Level Multimodal Alignment on NMR](https://arxiv.org/abs/2311.13817) | 本文提出了一种新颖的解决方案，即多级多模态对齐（K-M3AID），通过在分子图和NMR光谱之间建立对应关系，采用知识引导的实例级对比学习，以解决分子检索、异构体识别和峰归属等任务中的挑战。 |
| [^325] | [Inverse Problems with Learned Forward Operators](https://arxiv.org/abs/2311.12528) | 逆问题中使用的具有学习正演算子的方法分为两种：一种完全不考虑正演算子，学习其在训练数据子空间上的限制，另一种使用简化的物理模型并依赖于训练数据来学习模型修正。这两种方法都强调对正演算子及其共轭算子的训练数据的重要性。 |
| [^326] | [Looped Transformers are Better at Learning Learning Algorithms](https://arxiv.org/abs/2311.12424) | 提出了循环变压器架构及其训练方法，通过将迭代特征融入变压器架构中，实现了在解决数据拟合问题方面与标准变压器性能可比的效果，同时参数数量少于10%。 |
| [^327] | [Revealing behavioral impact on mobility prediction networks through causal interventions](https://arxiv.org/abs/2311.11749) | 通过因果干预框架，评估移动因素对神经网络的影响，产生具有不同移动行为的位置序列，有助于理解和解释预测网络。 |
| [^328] | [Inherently Interpretable Time Series Classification via Multiple Instance Learning](https://arxiv.org/abs/2311.10049) | 通过多例学习提出了一个新框架MILLET，使现有的深度学习时间序列分类模型变得内在可解释，同时不影响甚至改进预测性能，并在多个数据集上展示出比其他方法更高质量的稀疏解释。 |
| [^329] | [Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey](https://arxiv.org/abs/2311.07914) | 调查综合审查了LLMs中基于知识图谱的增强技术，重点关注其在减轻幻觉方面的效果。 |
| [^330] | [Learning to design protein-protein interactions with enhanced generalization](https://arxiv.org/abs/2310.18515) | 本研究的三个主要贡献是构建了最大和非冗余的3D蛋白质-蛋白质相互作用数据集PPIRef，提出了可泛化跨不同蛋白质结合剂变体的PPIformer模型，并通过热力学调整预训练损失函数实现对蛋白质-蛋白质相互作用突变影响的预测，最终展示了该新方法在增强泛化能力方面的优势。 |
| [^331] | [PubDef: Defending Against Transfer Attacks From Public Models](https://arxiv.org/abs/2310.17645) | 本文提出了一个新的实用威胁模型，其中对手依赖于通过公开可用的替代模型的迁移攻击，提出了一种基于博弈论视角的专门防御方法，并在多个公共模型和攻击算法下进行了评估。 |
| [^332] | [Dual-Encoders for Extreme Multi-Label Classification](https://arxiv.org/abs/2310.10636) | 提出了一种解耦softmax损失的方法来克服现有对比损失在极端多标签分类任务上的局限性，并拓展到soft top-k 操作符。 |
| [^333] | [Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World](https://arxiv.org/abs/2310.10207) | Bongard-OpenWorld基准旨在评估机器视觉中对真实世界中的自由形式视觉概念进行少样本推理，并且提出了开放世界自由形式概念和真实世界图像两项新挑战。 |
| [^334] | [Spike-based Neuromorphic Computing for Next-Generation Computer Vision](https://arxiv.org/abs/2310.09692) | 基于脉冲的神经形态计算作为对今天主导视觉领域的深度卷积神经网络的可行替代，承诺能够实现数量级的能量效率提升。 |
| [^335] | [Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches](https://arxiv.org/abs/2310.06790) | 本文介绍了一种改进的EDMD-DL方法，利用自动微分同时确定可观测字典和Koopman算子的近似值，并评估了其他替代方法的性能。 |
| [^336] | [Robust-GBDT: GBDT with Nonconvex Loss for Tabular Classification in the Presence of Label Noise and Class Imbalance](https://arxiv.org/abs/2310.05067) | Robust-GBDT结合了梯度提升决策树（GBDT）和非凸损失函数，在处理标签噪声和类别不平衡方面展现出了前所未有的稳健性和泛化能力。 |
| [^337] | [Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift](https://arxiv.org/abs/2310.04971) | 本研究发现多模态对比学习对抗分布转移具有鲁棒性的两种机制，即类内对比和类间特征共享，这有助于防止训练数据中的虚假特征干扰核心特征，从而在零样本分类准确度方面表现优越。 |
| [^338] | [Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations](https://arxiv.org/abs/2310.04566) | 本论文介绍了一种自监督学习框架，利用Transformer神经网络使机器人能够从整齐排列的示范中理解和复制整洁的概念，从而实现整理物品的功能。 |
| [^339] | [Compressing LLMs: The Truth is Rarely Pure and Never Simple](https://arxiv.org/abs/2310.01382) | 本研究重新评估了现有最先进的压缩LLM方法对稠密LLM的有效性，并引入了一个新的压缩LLM基准来重新定义评估协议。 |
| [^340] | [Deep Neural Networks Tend To Extrapolate Predictably](https://arxiv.org/abs/2310.00873) | 我们的工作表明，与传统观点不同，在面对超出分布范围的输入时，高维输入的神经网络预测往往会趋向于一个恒定值，且这个值通常能够接近最优恒定解。 |
| [^341] | [Analyzing and Mitigating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2310.00754) | 提出了一种名为LVLM Hallucination Revisor（LURE）的算法，通过重新构建较少具有幻觉性的描述，来事后纠正大型视觉语言模型中的物体幻觉问题。 |
| [^342] | [Can LLM-Generated Misinformation Be Detected?](https://arxiv.org/abs/2309.13788) | LLM生成的虚假信息可能比人类撰写的虚假信息更难以检测，具有更具欺骗性的风格，可能造成更多危害。 |
| [^343] | [A Unifying Generator Loss Function for Generative Adversarial Networks](https://arxiv.org/abs/2308.07233) | 引入了一个统一的生成器损失函数，称为$\mathcal{L}_\alpha$-GAN，通过最小化Jensen-$f_\alpha$-散度来优化生成器，可以恢复出文献中的多个GAN问题。 |
| [^344] | [Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space](https://arxiv.org/abs/2306.02574) | 本文研究了马尔可夫决策过程中具有可数无限状态空间的最优控制问题，提出了基于汤普森采样和动态大小片段的算法进行贝叶斯学习，解决了在这些模型上的挑战。 |
| [^345] | [Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo](https://arxiv.org/abs/2305.18246) | 通过Langevin Monte Carlo直接采样Q函数后验分布，避免了高斯逼近，实现了在强化学习中高效的探索策略。 |
| [^346] | [Distributional Reinforcement Learning with Dual Expectile-Quantile Regression](https://arxiv.org/abs/2305.16877) | 提出了一种用双期望分位回归的分布式强化学习方法，能够更高效地学习任意回报分布 |
| [^347] | [Hang-Time HAR: A Benchmark Dataset for Basketball Activity Recognition using Wrist-Worn Inertial Sensors](https://arxiv.org/abs/2305.13124) | 提出了一个用于篮球活动识别的基准数据集，记录了来自美国和德国两支篮球队的24名球员在训练和比赛中佩戴腕部惯性传感器的数据，具有不同国家文化差异和参与者篮球经验异质性等特点 |
| [^348] | [Phased Data Augmentation for Training a Likelihood-Based Generative Model with Limited Data](https://arxiv.org/abs/2305.12681) | 分阶段数据增强是一种新颖技术，能够优化有限数据训练生成模型的方法，通过限制增强强度，提高了模型学习能力，保持了模型的保真度，在整合PixelCNNs与VQ-VAE-2的模型中取得了卓越性能。 |
| [^349] | [BCQQ: Batch-Constraint Quantum Q-Learning with Cyclic Data Re-uploading](https://arxiv.org/abs/2305.00905) | 本文提出了一种 BCQ 算法，将 VQC 作为函数逼近器，并通过循环移位数据编码层中输入变量的顺序来进行全新的数据重新上传方案，以提高批量 RL 的效率。 |
| [^350] | [Tangent Bundle Convolutional Learning: from Manifolds to Cellular Sheaves and Back](https://arxiv.org/abs/2303.11323) | 本文介绍了一种在黎曼流形的切丛上进行卷积操作的方法，并基于此定义了切丛滤波器和切丛神经网络（TNNs），这为连续架构提供了新颖的操作方式，最后证明了离散化后的架构收敛于连续架构。 |
| [^351] | [EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models](https://arxiv.org/abs/2303.05656) | 该研究探索了使用扩散模型在电子健康记录（EHR）数据合成方面的潜力，挑战传统基于生成对抗网络（GAN）的方法。 |
| [^352] | [A low latency attention module for streaming self-supervised speech representation learning](https://arxiv.org/abs/2302.13451) | 本文提出了用于流式自监督语音表示学习的低延迟注意力模块，实现了在低延迟的情况下进行实时推断。 |
| [^353] | [Copula-based transferable models for synthetic population generation](https://arxiv.org/abs/2302.09193) | 提出了一种基于Copula的新框架，利用不同人口样本以及相似边际依赖性，引入空间组件并考虑多种信息源，用于生成合成但现实的目标人口表示。 |
| [^354] | [Predicting the long-term collective behaviour of fish pairs with deep learning](https://arxiv.org/abs/2302.06839) | 该研究通过引入深度学习模型对Hemigrammus rhodostomus鱼类的社会互动进行评估，并展示了机器学习模型在再现实验可观测量方面与分析模型的竞争力，强调了跨不同时间尺度进行一致验证的必要性。 |
| [^355] | [Improving Domain Generalization with Domain Relations](https://arxiv.org/abs/2302.02609) | 通过利用领域关系对训练领域特定函数重新加权，D$^3$G方法能够改进领域泛化能力。 |
| [^356] | [Representation Deficiency in Masked Language Modeling](https://arxiv.org/abs/2302.02060) | 掩码语言建模中的 $\texttt{[MASK]} $符号会导致模型维度过度分配，造成真实标记的表示不足，本文提出了MAE-LM来解决这一问题 |
| [^357] | [Provably Bounding Neural Network Preimages](https://arxiv.org/abs/2302.01404) | 提出了INVPROP算法用于验证神经网络输出集的前像上的属性，结合分支界限以增加精度，并且实现了GPU加速，避免了线性规划求解器的需求。 |
| [^358] | [Curriculum Learning for ab initio Deep Learned Refractive Optics](https://arxiv.org/abs/2302.01089) | 通过Curriculum Learning，提出了一种DeepLens设计方法，能够从随机初始化表面从头开始学习复合透镜的光学设计，克服了对良好初始设计的需求，并实现了自动设计经典成像透镜和大视场延伸景深计算透镜。 |
| [^359] | [Agnostic Visual Recommendation Systems: Open Challenges and Future Directions](https://arxiv.org/abs/2302.00569) | 本文讨论了无偏见的视觉推荐系统领域的挑战和未来方向，特别是系统不依赖人类规则而自主学习任务的新方法。 |
| [^360] | [Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning](https://arxiv.org/abs/2211.08559) | 使用跨域自监督深度学习方法，以医学图像作为输入进行疾病预后建模，提高了预测阿尔茨海默病进展的准确性。 |
| [^361] | [Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection](https://arxiv.org/abs/2210.06891) | 提出了一种基于任务驱动特征选择的多通道成像实验设计方法，通过优化设计和训练机器学习模型执行用户指定的图像分析任务。 |
| [^362] | [Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity](https://arxiv.org/abs/2210.05279) | 本文提出了一种新的随机零阶梯度硬阈值（SZOHT）算法，通过新颖的随机支持采样，解决了$\ell_0$约束下梯度计算困难的问题。 |
| [^363] | [MechProNet: Machine Learning Prediction of Mechanical Properties in Metal Additive Manufacturing](https://arxiv.org/abs/2209.12605) | 通过机器学习方法预测金属增材制造中的机械性能，建立了一个全面的框架，包括来自90多篇文章和140个数据表的大量实验数据。 |
| [^364] | [A review of predictive uncertainty estimation with machine learning](https://arxiv.org/abs/2209.08307) | 该综述回顾了机器学习中利用概率分布进行预测不确定性估计的主题，为评估概率预测提供了相关度量，从经典统计方法到现代机器学习算法进行了梳理。 |
| [^365] | [Statistical Properties of the log-cosh Loss Function Used in Machine Learning](https://arxiv.org/abs/2208.04564) | 分析log-cosh损失函数的统计特性，比较它与柯西分布的性质，并研究MLE的偏差、方差和置信区间，同时提供了与其他损失函数的鲁棒估计器比较。 |
| [^366] | [Digital Twin Data Modelling by Randomized Orthogonal Decomposition and Deep Learning](https://arxiv.org/abs/2206.08659) | 本论文引入了一种通过结合Krylov动态模态分解和适当正交分解优势的新算法，来创建高效的流体流动数字孪生模型，并证明了随机正交分解算法相对于SVD经验正交分解方法具有数个优势 |
| [^367] | [Accelerating Asynchronous Federated Learning Convergence via Opportunistic Mobile Relaying](https://arxiv.org/abs/2206.04742) | 通过机会性移动中继，提出了FedMobile算法，实现了异步联邦学习收敛速率为$O(\frac{1}{\sqrt{NT}})$。 |
| [^368] | [Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning](https://arxiv.org/abs/2205.12532) | 代理通过学习一组足够的技能原语，可以在环境中实现所有高层目标，并能够在任何常规语言中逻辑和时间地组合这些技能，以确切实现时间逻辑规范。 |
| [^369] | [PyGOD: A Python Library for Graph Outlier Detection](https://arxiv.org/abs/2204.12095) | PyGOD是一个开源Python库，支持多种领先的基于图的异常检测方法，提供了易于使用的API和丰富的实用程序函数，同时采用了最佳的代码可靠性和可维护性实践。 |
| [^370] | [Adaptive Rational Activations to Boost Deep Reinforcement Learning](https://arxiv.org/abs/2102.09407) | 本研究提出了利用有理数作为适应性激活函数来改进深度强化学习，并展示了这种方法在Atari游戏中取得了一致的改进，特别是将简单的DQN提升为一个稳健的方法。 |
| [^371] | [Shift Aggregate Extract Networks](https://arxiv.org/abs/1703.05537) | 通过深层分层分解的架构，提出了一种有效学习大型图表示的方法，能够在大型社交网络数据集上胜过当前最先进的图分类方法，同时在小型化学生物基准数据集上具有竞争力。 |
| [^372] | [EdgeOL: Efficient in-situ Online Learning on Edge Devices.](http://arxiv.org/abs/2401.16694) | 本文提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率，在边缘设备上实现了显著的性能提升。 |
| [^373] | [A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data.](http://arxiv.org/abs/2401.16497) | 本研究提出了一个名为LDGD的判别贝叶斯高斯过程潜变量模型，能够有效地从高维数据中提取信息，并具有较高的预测准确性和鲁棒性。 |
| [^374] | [Understanding Domain Generalization: A Noise Robustness Perspective.](http://arxiv.org/abs/2401.14846) | 本文通过研究域泛化算法和经验风险最小化算法在标签噪声下的表现，发现域泛化算法在有限样本训练中显示出隐性的标签噪声鲁棒性，有助于减轻假冗余相关和提高泛化能力，但在真实世界基准数据集上的实验证明，标签噪声鲁棒性并不一定意味着相比ERM具有更好的性能。 |
| [^375] | [Ricci flow-guided autoencoders in learning time-dependent dynamics.](http://arxiv.org/abs/2401.14591) | 利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。 |
| [^376] | [Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities.](http://arxiv.org/abs/2401.14405) | 本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。 |
| [^377] | [Towards 3D Molecule-Text Interpretation in Language Models.](http://arxiv.org/abs/2401.13923) | 提出了一个名为3D-MoLM的模型，通过给语言模型配备一个3D分子编码器，实现了对3D分子-文本的解释和分析，此模型在下游任务上显著优于现有基线。 |
| [^378] | [Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors.](http://arxiv.org/abs/2401.13652) | 本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。 |
| [^379] | [Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret Analysis.](http://arxiv.org/abs/2401.10383) | 本文提出了一种解决多智能体图形赌博机问题的算法Multi-G-UCB，并通过数值实验验证了其有效性。 |
| [^380] | [Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation.](http://arxiv.org/abs/2401.10253) | 本文提出了一种基于图神经网络的混合任务元学习算法，用于可扩展和可转移的带宽分配。通过引入GNN和HML算法，该方法在不同的通信场景下具有较好的性能和采样效率。 |
| [^381] | [Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification.](http://arxiv.org/abs/2401.09493) | 本研究利用线性变分编码器-解码器来学习云辐射反馈对早期热带气旋强化的影响，发现内核深对流和浅云的长波辐射强迫都对强化起到贡献，其中深对流的影响最大。 |
| [^382] | [Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs.](http://arxiv.org/abs/2401.07961) | 这项研究将概率Lambert问题与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程等领域连接起来，从而解决了概率Lambert问题的解的存在和唯一性，并提供了数值求解的方法。 |
| [^383] | [Identifying Policy Gradient Subspaces.](http://arxiv.org/abs/2401.06604) | 本文研究了两种深度策略梯度方法在不同模拟基准任务上的评估结果，发现尽管数据分布不断变化，但存在低维且缓慢变化的梯度子空间，这有助于未来更高效的强化学习工作。 |
| [^384] | [Evaluating Language Model Agency through Negotiations.](http://arxiv.org/abs/2401.04536) | 本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。 |
| [^385] | [Large Language Models as Visual Cross-Domain Learners.](http://arxiv.org/abs/2401.03253) | 本研究提出了大型语言模型作为视觉跨领域学习器（LLaVO），通过将图像转换为文本描述，使用大型语言模型进行训练和微调，实现了在跨领域任务中减少领域转移的效果。 |
| [^386] | [The Power of Training: How Different Neural Network Setups Influence the Energy Demand.](http://arxiv.org/abs/2401.01851) | 本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。 |
| [^387] | [Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review.](http://arxiv.org/abs/2401.01519) | 本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。 |
| [^388] | [Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning.](http://arxiv.org/abs/2312.05720) | 本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。 |
| [^389] | [Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization.](http://arxiv.org/abs/2311.03351) | Uni-O4提出了统一的离线和在线深度强化学习方法，通过对齐目标实现了无缝传递，增强了学习范式的灵活性。在离线阶段，Uni-O4利用多样的集合策略解决了估计行为策略和离线数据集不匹配的问题。 |
| [^390] | [Retrieval-Based Reconstruction For Time-series Contrastive Learning.](http://arxiv.org/abs/2311.00519) | 本文提出了一种基于检索重建的时间序列对比学习方法（REBAR），通过检索信息和重建子序列来构建正样本对，从而解决了时间序列中使用数据增强创建正样本对的挑战。实验证明，REBAR误差可以作为正/负标记器，并且在对比学习框架中集成REBAR方法可以学习具有有用信息的嵌入表示。 |
| [^391] | [Flooding Regularization for Stable Training of Generative Adversarial Networks.](http://arxiv.org/abs/2311.00318) | 本文在生成对抗网络中直接对对抗损失函数进行正则化，通过应用泛化方法防止判别器的损失过分降低，并通过实验证实了其稳定性。 |
| [^392] | [Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data.](http://arxiv.org/abs/2311.00136) | Neuroformer是一个多模态和多任务的生成预训练模型，旨在处理系统神经科学中大规模的多模态数据。模型经过训练后能准确预测神经回路活动并推断神经回路连接性，同时能用于预测行为。 |
| [^393] | [Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning.](http://arxiv.org/abs/2310.18247) | 该论文提出了一种人工引导的数据增强框架（GuDA）用于提高演示学习模型的性能。 |
| [^394] | [Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates.](http://arxiv.org/abs/2310.17786) | 本文研究了在稀疏奖励任务中，动力学不变的数据增强函数对模型无关的强化学习更新的影响。实验结果表明，增加状态-动作覆盖率可以提高学习效果。 |
| [^395] | [The Expressive Power of Low-Rank Adaptation.](http://arxiv.org/abs/2310.17513) | 本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。 |
| [^396] | [Causal Modeling with Stationary Diffusions.](http://arxiv.org/abs/2310.17405) | 本文提出了一种新颖的因果推断方法，使用随机微分方程建模系统行为，不需要因果图的形式化。在多种情况下，该方法比传统方法更好地推广到未见干预的变量。 |
| [^397] | [Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes.](http://arxiv.org/abs/2310.16597) | 本文扩展了之前的研究，将证明的范围从独立同分布权重扩展到了更大的权重分布类别(PSEUDO-IID)，包括低秩和稀疏设置。作者发现使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。这些结果可以帮助我们识别更广泛的神经网络的边界混沌状态，并进行性能调优。 |
| [^398] | [MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits.](http://arxiv.org/abs/2310.12359) | MARVEL是一个多智能体强化学习框架，可以实现利用常见数据在高速公路走廊上进行大规模可变速限控制。它通过奖励结构和智能体之间的协调，提高交通安全性和流动性。与无控制情况相比，MARVEL可以提高交通安全性63.4%，提高交通流动性14.6%。 |
| [^399] | [From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique.](http://arxiv.org/abs/2310.09362) | 这项研究开发了一个能够发出声音的波斯语聊天机器人，用于指导用户进行基于依恋理论的自我依恋技术。通过使用规则和分类模块，聊天机器人可以理解用户输入并推荐适当的自我依恋练习。该研究还开发了一种准确率超过92%的情感分析模块，以识别用户情感。这项工作有助于在后疫情时代提供数字心理疗法的替代方案。 |
| [^400] | [Tokenizer Choice For LLM Training: Negligible or Crucial?.](http://arxiv.org/abs/2310.08754) | 在LLM训练中，分词器的选择对模型的后续性能、成本有着显著影响，常见的分词器评估指标不一定预测模型的性能。 |
| [^401] | [Unit Commitment Predictor With a Performance Guarantee: A Support Vector Machine Classifier.](http://arxiv.org/abs/2310.08601) | 本文提出了一个带有性能保证的机组启停预测器，通过学习和预测常规机组的启停决策，系统运营商可以在求解器中使用预热启动并显著加速计算。对于预测，使用了适当正则化的核化支持向量机分类器，能将计算时间减少1.7倍。 |
| [^402] | [Memory-Consistent Neural Networks for Imitation Learning.](http://arxiv.org/abs/2310.06171) | 本文介绍了一种内存一致的神经网络模型，在模仿学习中使用专家演示训练策略。该模型通过对输出结果进行硬约束，避免了错误的累积现象，保证了策略效果的上界。 |
| [^403] | [Generalization in diffusion models arises from geometry-adaptive harmonic representation.](http://arxiv.org/abs/2310.02557) | 通过分析基于分数的反向扩散算法生成的高质量样本的研究结果，我们发现尽管存在维度灾难，但为了降噪而训练的深度神经网络可以学习到高维密度。此外，我们展示了在训练集的非重叠子集上训练的网络可以学习到相同的密度，从而证明了DNN架构和训练算法中的归纳偏差与数据分布的一致性。 |
| [^404] | [L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation.](http://arxiv.org/abs/2310.02003) | L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。 |
| [^405] | [Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks.](http://arxiv.org/abs/2310.01820) | 本文研究了评估图神经网络解释性的鲁棒度的方法，并指出了现有度量方法的局限性。 |
| [^406] | [PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels.](http://arxiv.org/abs/2310.01655) | 本文通过使用多项式函数和多项式草图，实现了一个快速注意力机制PolySketchFormer，以突破Transformer架构中注意力机制的二次复杂性难题，无需假设注意力矩阵具有稀疏结构，并提出了高效的基于块的算法。 |
| [^407] | [SmartPlay : A Benchmark for LLMs as Intelligent Agents.](http://arxiv.org/abs/2310.01557) | SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。 |
| [^408] | [Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients.](http://arxiv.org/abs/2310.01012) | 本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。 |
| [^409] | [Learning to Make Adherence-Aware Advice.](http://arxiv.org/abs/2310.00817) | 本文提出了一种顺序决策模型，考虑了人的依从程度和机器提供建议的时机，并提供了学习算法来学习最佳的建议策略。 |
| [^410] | [Combining Spatial and Temporal Abstraction in Planning for Better Generalization.](http://arxiv.org/abs/2310.00229) | Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。 |
| [^411] | [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing.](http://arxiv.org/abs/2309.16883) | 本文提出了一个增强随机平滑的方法，通过研究随机平滑引入的方差与分类器的Lipschitz常数和边界之间的关系，以及采用单纯形投影技术来增加认证鲁棒半径。 |
| [^412] | [From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity.](http://arxiv.org/abs/2309.16512) | 本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。 |
| [^413] | [Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems.](http://arxiv.org/abs/2309.12128) | 本研究通过探索连接理论和实践，提供了无监督神经网络在解决逆问题中的收敛和恢复性能保证。同时，我们还得出了对于两层具有平滑激活函数的深度逆先验网络的超参数化界限，该网络将从我们的保证中受益。 |
| [^414] | [DreamLLM: Synergistic Multimodal Comprehension and Creation.](http://arxiv.org/abs/2309.11499) | DreamLLM是一种学习框架，实现了多模态理解与创作的协同效应。通过直接采样生成语言和图像的生成模型，避免了信息损失，并获得了更全面的多模态理解。此外，DreamLLM能够生成自由形式交织内容，展现了其在零样本多模态学习任务中的卓越性能。 |
| [^415] | [Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization.](http://arxiv.org/abs/2309.10370) | 本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。 |
| [^416] | [Primal-Dual $\ell_0$-Constrained Sparse Index Tracking.](http://arxiv.org/abs/2309.10152) | 本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，可以轻松控制投资组合中资产数量的上限。 |
| [^417] | [Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets.](http://arxiv.org/abs/2309.09258) | 本文首次证明了在深度为2的神经网络上，适当正则化的逻辑回归代价函数通过随机梯度下降（SGD）能够收敛到全局极小值，这适用于任意数据和具有充分平滑且有界激活函数。同时，我们还证明了连续时间SGD的指数级快速收敛速度，该结果也适用于光滑无界的激活函数。 |
| [^418] | [Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights.](http://arxiv.org/abs/2309.08731) | 本文提出了一种深度学习方法，通过学习到的ICP权重优化雷达-激光雷达的定位，从而改善了雷达测量对激光雷达地图的定位效果。这一方法在保持高质量地图定位性能的同时，提高了在降水和大雾等恶劣天气条件下的定位准确性。 |
| [^419] | [Deep Nonnegative Matrix Factorization with Beta Divergences.](http://arxiv.org/abs/2309.08249) | 本文提出了一种使用Beta散度的深度非负矩阵分解方法，应用于面部特征提取、文档主题识别和高光谱图像材料识别。 |
| [^420] | [Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support.](http://arxiv.org/abs/2309.07183) | 本研究利用声音数据训练多个机器学习模型来对呼吸疾病进行分类。方法采用经验模态分解和频谱分析来提取与心血管和呼吸模式相关的生理信号，并通过特征提取和预测建模实现快速筛查和诊断支持。 |
| [^421] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^422] | [Compact: Approximating Complex Activation Functions for Secure Computation.](http://arxiv.org/abs/2309.04664) | Compact提出了一种逼近复杂激活函数的方法，可以与MPC技术高效配合使用，并且几乎不会损失模型准确性。 |
| [^423] | [Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets.](http://arxiv.org/abs/2308.14969) | 重新审视彩票式转移的高效可靠性，研究了线性探测（LP）和视觉提示/重新编程（VP）方法在数据稀疏性和模型稀疏性方面的能力，并发现彩票式转移并非通用的重新编程器。 |
| [^424] | [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models.](http://arxiv.org/abs/2308.13137) | OmniQuant是一种用于大型语言模型的全向校准量化技术，通过优化各种量化参数实现了良好的性能，并保持了计算效率。 |
| [^425] | [Quantifying the biomimicry gap in biohybrid systems.](http://arxiv.org/abs/2308.08978) | 这项工作中，研究人员针对生物混合系统中的仿生差距进行了量化分析。通过使用真实鱼类和仿生诱饵进行实验，以及对鱼类对的模拟，研究人员证明了他们的系统可以生成与真实鱼类完全相似的社会互动。 |
| [^426] | [Temporal Interest Network for Click-Through Rate Prediction.](http://arxiv.org/abs/2308.08487) | 本文提出了时间兴趣网络（TIN），用于捕捉行为与目标之间的四重语义和时间相关性，以预测点击率的效果和已有方法对这种相关性的学习程度尚不清楚。 |
| [^427] | [Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks.](http://arxiv.org/abs/2308.07553) | 本论文提出了一种改进的点对点认证方法，通过利用差分隐私和采样高斯机制，能够确保对有限数量中毒样本的预测具有不变性，提供更大的对抗鲁棒性保证。 |
| [^428] | [Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges.](http://arxiv.org/abs/2308.06668) | 传统农业系统中的机器学习和深度学习模型存在局限性，而基础模型在语言和视觉任务中表现出了显著的成功。本研究旨在探索基础模型在智能农业领域的潜力和应用。 |
| [^429] | [DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System.](http://arxiv.org/abs/2308.06378) | 本文介绍了一种新的深度卷积神经模糊推理系统（DCNFIS），它通过将模糊逻辑和深度学习模型相结合，实现了提高透明度而不损失准确性的目标。DCNFIS在准确性上与现有卷积神经网络相当，并且胜过了最先进的深度模糊系统。通过模糊规则提取的解释可以提高模型的可解释性。 |
| [^430] | [Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods.](http://arxiv.org/abs/2308.01938) | 本文提出了两种新的在线多任务学习方法，分别基于递归最小二乘和递归核方法。与基于梯度下降或不精确逼近的方法不同，我们的方法在每个实例的代价上具有二次复杂度。我们将这些方法应用于风力短期预测挑战，并与其他竞争者进行了比较。 |
| [^431] | [More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes.](http://arxiv.org/abs/2308.01313) | 本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。 |
| [^432] | [Online learning in bandits with predicted context.](http://arxiv.org/abs/2307.13916) | 本文研究了一种在预测上下文中的在线学习问题，通过将经典统计学中的测量误差模型推广到在线决策设置中，我们提出了第一个具有次线性后悔的在线算法。 |
| [^433] | [Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses.](http://arxiv.org/abs/2307.11714) | 本论文研究了使用切片Wasserstein损失训练神经网络时，随机梯度下降算法的收敛性，并证明了在特定条件下，SGD轨迹逼近了梯度流方程的集合。 |
| [^434] | [Properties of Discrete Sliced Wasserstein Losses.](http://arxiv.org/abs/2307.10352) | 本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。 |
| [^435] | [In-context Autoencoder for Context Compression in a Large Language Model.](http://arxiv.org/abs/2307.06945) | 在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。 |
| [^436] | [Deep learning for dynamic graphs: models and benchmarks.](http://arxiv.org/abs/2307.06104) | 本文对深度学习动态图领域进行了调查，总结了学习时间和空间信息的最新优势，并对最流行的方法进行了公平的性能比较，为评估新架构和方法建立了一个可靠的基准模型。 |
| [^437] | [The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks.](http://arxiv.org/abs/2306.16922) | ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。 |
| [^438] | [Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration.](http://arxiv.org/abs/2306.13769) | 提出了一种基于功能团的扩散模型D3FG，用于口袋特异性分子生成和扩展。基于刚性体的功能团和质点的连接器可以共同形成增强配体-蛋白质相互作用的复杂片段，能够生成高质量的分子。 |
| [^439] | [Protein Discovery with Discrete Walk-Jump Sampling.](http://arxiv.org/abs/2306.12360) | 本文提出了一种离散行走跳跃采样的方法，通过学习平滑能量函数、使用 Langevin Markov 链蒙特卡罗 (MCMC) 算法和一步去噪的投射技术，可以解决离散生成模型的采样难题。同时在抗体蛋白质的生成建模上进行了应用和测试，并引入了分布一致性得分作为基准测试标准。 |
| [^440] | [MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction.](http://arxiv.org/abs/2306.10060) | MUBen评估不同骨干和UQ模型组合对分子不确定性估计和属性预测的性能，以解决预训练模型微调中的过拟合校准问题。 |
| [^441] | [Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX.](http://arxiv.org/abs/2306.09884) | Jumanji是JAX中一套可扩展的强化学习系统，提供了一系列高度可定制的环境，具有快速、灵活、可扩展和模块化特点，利用硬件加速器赋能更有能力的代理人。 |
| [^442] | [MOFI: Learning Image Representations from Noisy Entity Annotated Images.](http://arxiv.org/abs/2306.07952) | MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。 |
| [^443] | [Proximity-Informed Calibration for Deep Neural Networks.](http://arxiv.org/abs/2306.04590) | 该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。 |
| [^444] | [Improving Offline RL by Blending Heuristics.](http://arxiv.org/abs/2306.00321) | HUBL是一种用于基于值函数回溯的广泛类离线强化学习算法的简单性能提升技术。我们证明了HUBL可通过调整奖励和折扣因子来简单实现，并且实验结果表明HUBL能够在提高性能的同时降低复杂度。 |
| [^445] | [Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.](http://arxiv.org/abs/2305.15852) | 本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。 |
| [^446] | [Differentially Private Latent Diffusion Models.](http://arxiv.org/abs/2305.15759) | 本文提出使用差分隐私训练潜在扩散模型(LDMs)，通过预训练自编码器将高维像素空间转变为低维潜在空间实现更高效快速的DMs训练，并且通过只微调注意力模块减少了可训练参数的数量。 |
| [^447] | [FEDORA: Flying Event Dataset fOr Reactive behAvior.](http://arxiv.org/abs/2305.14392) | FEDORA是一个飞行事件数据集，解决了现有数据集缺少完整数据和时间分辨率的问题，旨在帮助在资源受限环境下实现基于视觉的自主导航和避障。 |
| [^448] | [Expressive Losses for Verified Robustness via Convex Combinations.](http://arxiv.org/abs/2305.13991) | 通过基于凸组合的表达性损失，可以提高网络的对抗鲁棒性，最新的算法可以获得最先进的结果；这种方法通过对抗性攻击和IBP边界之间的简单凸组合进行实现。 |
| [^449] | [Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees.](http://arxiv.org/abs/2305.11997) | 本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。 |
| [^450] | [Where does a computer vision model make mistakes? Using interactive visualizations to find where and how CV models can improve.](http://arxiv.org/abs/2305.11927) | 研究使用交互式可视化工具在创建计算机视觉分类和检测模型时帮助用户识别和改进模型上的问题，有效减少设计师的工作量。 |
| [^451] | [LLM Itself Can Read and Generate CXR Images.](http://arxiv.org/abs/2305.11490) | 该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。 |
| [^452] | [The Update Equivalence Framework for Decision-Time Planning.](http://arxiv.org/abs/2304.13138) | 该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。 |
| [^453] | [Sheaf Neural Networks for Graph-based Recommender Systems.](http://arxiv.org/abs/2304.09097) | 基于Sheaf神经网络的模型提出了一种新的向量空间表示方法，使得其在基准推荐任务上获得最先进的性能表现。 |
| [^454] | [Energy-guided Entropic Neural Optimal Transport.](http://arxiv.org/abs/2304.06094) | 本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。 |
| [^455] | [Interpretable statistical representations of neural population dynamics and geometry.](http://arxiv.org/abs/2304.03376) | 该论文提出了一种基于统计分布的几何深度学习框架，用于表示非线性动态系统的几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。利用该方法，能够解释神经动力学的嵌入，在灵长类似任务中取得了最先进的准确性。 |
| [^456] | [DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps.](http://arxiv.org/abs/2304.00133) | DeforestVis提供了一种可视化分析工具，通过提供代理决策树，总结了复杂机器学习模型的行为，以帮助用户探索复杂性。 |
| [^457] | [Magnushammer: A Transformer-based Approach to Premise Selection.](http://arxiv.org/abs/2303.04488) | Magnushammer是一种基于Transformer的前提选择方法，通过在PISA基准上的测试表明，它可以大幅度超越传统符号系统，并将先前最先进的证明率从57.0％提高到71.0％。 |
| [^458] | [Koopman-based generalization bound: New aspect for full-rank weights.](http://arxiv.org/abs/2302.05825) | 我们提出了一种使用Koopman算子对全秩神经网络权重进行泛化的新界限，当权重矩阵的条件数较小时，该界限比现有基于范数的界限更紧。我们的界限不与现有界限相矛盾，而是对现有界限进行的补充。此外，我们的界限可以与现有界限结合以得到更紧的界限。该研究结果为理解全秩权重神经网络的泛化提供了新的视角，同时也为算子理论分析和神经网络泛化之间提供了连接。 |
| [^459] | [Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization.](http://arxiv.org/abs/2302.04552) | 本论文研究了乐观的在线镜像下降算法在Stochastically Extended Adversarial (SEA)模型中的理论保证，对于凸和平滑的函数，其遗憾界限为O(sqrt(σ_{1:T}^2) + sqrt(Σ_{1:T}^2))，对于强凸和平滑的函数，其界限为O(sqrt(σ_{\max}^2) + sqrt(Σ_{\max}^2))。 |
| [^460] | [Multi-Source Diffusion Models for Simultaneous Music Generation and Separation.](http://arxiv.org/abs/2302.02257) | 本研究提出了一种基于扩散模型的生成模型，能够同时进行音乐合成和源分离，并于部分生成和分离任务上实现有竞争力的定量结果。 |
| [^461] | [Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging.](http://arxiv.org/abs/2302.01622) | 本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。 |
| [^462] | [Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence.](http://arxiv.org/abs/2301.11476) | 这篇论文通过研究广义的Tsallis KL散度，扩展了Munchausen强化学习算法，并提供了一种将KL正则化纳入实际算法的方法。对于Tsallis KL，当$q > 1$时，可以获得新的策略优化选项。 |
| [^463] | [On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach.](http://arxiv.org/abs/2211.16059) | 本文提出了两种针对分布式环境的多重检验方法：比例匹配和贪婪聚合。贪心聚合方法有效地近似了每个节点的最优拒绝区域，且具有计算效率性质。 |
| [^464] | [Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization.](http://arxiv.org/abs/2211.11656) | 本文提出一种名为知情联合消除（IFU）的新颖联邦优化方法，可实现有效且可量化的客户端消除请求，实验结果表明其效率较基本方法和最先进的FU方法更高。 |
| [^465] | [Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies.](http://arxiv.org/abs/2209.14568) | 本文提出了一种概率框架，为每个观测值提供稀疏的局部反事实规则，并将这些规则聚合成区域反事实规则，以适应不稳定的实现环境，并产生稳健的救济措施。 |
| [^466] | [Graph Neural Modeling of Network Flows.](http://arxiv.org/abs/2209.05208) | 本文提出了一种新颖的网络流问题图学习架构 PEW，相较于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。 |

# 详细

[^1]: 一步图像翻译与文本到图像模型

    One-Step Image Translation with Text-to-Image Models

    [https://arxiv.org/abs/2403.12036](https://arxiv.org/abs/2403.12036)

    通过引入单步扩散模型并通过对抗学习目标调整到新任务和领域，我们提出了一种解决现有条件扩散模型局限性的通用方法，适用于无配对和配对设置下的图像翻译任务。

    

    在这项工作中，我们解决了现有条件扩散模型的两个局限性：由于迭代去噪过程而导致的推断速度慢以及对配对数据进行模型微调的依赖。为了解决这些问题，我们引入了一种通过对抗学习目标将单步扩散模型调整到新任务和领域的通用方法。具体而言，我们将传统的潜在扩散模型各模块整合到一个具有小可训练权重的端到端生成器网络中，增强了其保留输入图像结构的能力，同时减少了过拟合。我们展示了，在无配对设置下，我们的模型 CycleGAN-Turbo 在各种场景翻译任务上优于现有的基于GAN和基于扩散的方法，如日夜转换以及添加/移除雾、雪和雨等天气效果。我们将我们的方法扩展到配对设置，我们的模型 pix2pix-Turbo 的性能

    arXiv:2403.12036v1 Announce Type: cross  Abstract: In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on 
    
[^2]: VFusion3D: 从视频扩散模型中学习可扩展的3D生成模型

    VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models

    [https://arxiv.org/abs/2403.12034](https://arxiv.org/abs/2403.12034)

    利用预训练的视频扩散模型，本文提出了一个可生成大规模3D数据集的VFusion3D模型。

    

    本文提出了一种新颖的范式，利用预训练的视频扩散模型构建可扩展的3D生成模型。构建基础3D生成模型的主要障碍是3D数据的有限可用性。与图像、文本或视频不同，3D数据不容易获取且难以获得，这导致与其他类型数据的数量相比存在显着的规模差异。为了解决这个问题，我们提出使用一个通过大量文本、图像和视频训练的视频扩散模型作为3D数据的知识源。通过微调解锁其多视角生成能力，我们生成一个大规模的合成多视角数据集来训练前馈3D生成模型。

    arXiv:2403.12034v1 Announce Type: cross  Abstract: This paper presents a novel paradigm for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compare
    
[^3]: ROUTERBENCH：用于多LLM路由系统的基准测试

    ROUTERBENCH: A Benchmark for Multi-LLM Routing System

    [https://arxiv.org/abs/2403.12031](https://arxiv.org/abs/2403.12031)

    提出了ROUTERBENCH，一个用于评估LLM路由系统性能的基准测试框架，包括超过405k推理结果的数据集，以支持路由策略的开发。

    

    随着大型语言模型（LLMs）的应用范围不断扩大，对有效的服务解决方案的需求变得日益关键。尽管LLMs具有多样性，但没有单一模型可以最优地解决所有任务和应用，特别是在平衡性能和成本之间。为了弥补这一限制，发展了LLM路由系统，这些系统结合了各种模型的优势，以克服单个LLMs的约束。然而，缺乏用于评估LLM路由器性能的标准化基准测试，阻碍了这一领域的进展。为弥合这一差距，我们提出了ROUTERBENCH，这是一个新颖的评估框架，旨在系统评估LLM路由系统的功效，以及一个包括来自代表性LLMs的超过405k推理结果的全面数据集，以支持路由策略的开发。我们进一步提出了一个LLM路由的理论框架，以及...

    arXiv:2403.12031v1 Announce Type: cross  Abstract: As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and del
    
[^4]: 基于预训练模型的增量类别学习的可扩展子空间集成

    Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning

    [https://arxiv.org/abs/2403.12030](https://arxiv.org/abs/2403.12030)

    提出了一种基于预训练模型的增量类别学习方法，通过训练每个新任务的轻量级适配器模块来创建任务特定的子空间，实现了模型更新而不损害先前知识。

    

    arXiv:2403.12030v1 发表类型：交叉摘要：类增量学习（CIL）要求学习系统不断学习新类别而不会遗忘旧知识。尽管预训练模型在CIL中表现出色，但一个关键问题仍然存在：学习新类别通常会导致旧类别的覆盖。网络过度修改会导致遗忘，而最小调整会导致新类别拟合不足。因此，希望找到一种有效的模型更新方式，既不损害先前知识。在本文中，我们提出了适用于基于PTM的CIL的Extended Subspace Ensemble（EASE）。为了使模型更新不冲突，我们为每个新任务训练一个独特的轻量级适配器模块，旨在创建任务特定的子空间。这些适配器跨越高维特征空间，实现跨多个子空间的联合决策。随着数据的演变，不断扩展的子空间使旧类别分类器与新类别不兼容。

    arXiv:2403.12030v1 Announce Type: cross  Abstract: Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones. Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes. As a result, it is desired to figure out a way of efficient model updating without harming former knowledge. In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces. These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces. As data evolves, the expanding subspaces render the old class classifiers incompatible with new
    
[^5]: 对齐与提炼：统一和改进领域自适应目标检测

    Align and Distill: Unifying and Improving Domain Adaptive Object Detection

    [https://arxiv.org/abs/2403.12029](https://arxiv.org/abs/2403.12029)

    引入了统一的基准测试和实现框架ALDI以及新的DAOD基准数据集CFC-DAOD，解决了领域自适应目标检测中的基准问题，并支持未来方法的发展。

    

    目标检测器通常表现不佳于与其训练集不同的数据。最近，领域自适应目标检测（DAOD）方法已经展示了在应对这一挑战上的强大结果。遗憾的是，我们发现了系统化的基准测试陷阱，这些陷阱对过去的结果提出质疑并阻碍了进一步的进展：（a）由于基线不足导致性能高估，（b）不一致的实现实践阻止了方法的透明比较，（c）由于过时的骨干和基准测试缺乏多样性，导致缺乏普遍性。我们通过引入以下问题来解决这些问题：（1）一个统一的基准测试和实现框架，Align and Distill（ALDI），支持DAOD方法的比较并支持未来发展，（2）一个公平且现代的DAOD训练和评估协议，解决了基准测试的陷阱，（3）一个新的DAOD基准数据集，CFC-DAOD，能够在多样化的真实环境中进行评估。

    arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
    
[^6]: FlexCap：在图像中生成丰富、本地化和灵活的标题

    FlexCap: Generating Rich, Localized, and Flexible Captions in Images

    [https://arxiv.org/abs/2403.12026](https://arxiv.org/abs/2403.12026)

    FlexCap模型能够生成图像中具有不同长度的区域描述，在密集字幕任务和视觉问答系统中表现出优越性能。

    

    我们介绍了一种多功能的$\textit{灵活字幕}$视觉-语言模型（VLM），能够生成长度不同的特定区域描述。该模型FlexCap经过训练，可为输入的边界框生成长度条件的字幕，从而可以控制其输出的信息密度，描述范围从简洁的对象标签到详细的字幕。为了实现这一点，我们从带字幕的图像开始创建了大规模的图像区域描述训练数据集。这种灵活的字幕功能有几个宝贵的应用。首先，FlexCap在Visual Genome数据集上的密集字幕任务中表现出优越性能。其次，可以通过采用FlexCap生成本地化描述作为大型语言模型的输入来构建视觉问答（VQA）系统。由此产生的系统在许多VQ上实现了最新技术的零样本性能。

    arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
    
[^7]: 一个用于揭示大型语言模型中健康公平危害和偏见的工具箱

    A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models

    [https://arxiv.org/abs/2403.12025](https://arxiv.org/abs/2403.12025)

    提出了用于揭示大型语言模型中健康公平危害和偏见的资源和方法，进行了实证案例研究，并提出了用于人类评估LLM生成答案偏见的多因子框架以及EquityMedQA数据集。

    

    大型语言模型（LLMs）有着为复杂的健康信息需求提供服务的巨大潜力，但同时也有可能引入危害并加剧健康不平等。可靠地评估与公平相关的模型失灵是发展促进健康公平系统的关键步骤。在这项工作中，我们提出了用于揭示可能导致LLM生成的长篇答案中的公平相关危害的偏见的资源和方法，并使用Med-PaLM 2进行了一项实证案例研究，这是迄今为止在该领域进行的最大规模的人类评估研究。我们的贡献包括用于人类评估LLM生成答案偏见的多因子框架，以及EquityMedQA，一个包含七个新发布数据集的收集，其中既包括手动策划又包括LLM生成的问题，丰富了对抗性查询。我们的人类评估框架和数据集设计过程都根植于实际

    arXiv:2403.12025v1 Announce Type: cross  Abstract: Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounde
    
[^8]: 监督微调作为逆强化学习

    Supervised Fine-Tuning as Inverse Reinforcement Learning

    [https://arxiv.org/abs/2403.12017](https://arxiv.org/abs/2403.12017)

    本论文提出将逆强化学习和模仿学习的见解结合，探讨了使用演示数据集对齐大语言模型的方法，并对不同方法的性能进行了分析。

    

    大语言模型（LLMs）对齐的主流方法通常依赖于人类或AI反馈，并假设可以访问特定类型的偏好数据集。在我们的工作中，我们质疑这些数据集的有效性，并探讨了各种情景下与专家演示对齐更为现实的方法。我们构建了一个顺序决策框架，以演示数据集为基础来规划对齐LLMs的问题。借鉴逆强化学习和模仿学习的见解，我们引入了各种方法来最小化LLM对齐任务中的差异。我们的分析突出了这些不同方法的覆盖率和寻找模式行为。此外，我们考察了经典监督微调方法的利弊，并详细阐述了不同方法表现突出的情景。

    arXiv:2403.12017v1 Announce Type: cross  Abstract: The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.
    
[^9]: EnvGen: 通过LLMs生成和调整环境以训练具身体的代理

    EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents

    [https://arxiv.org/abs/2403.12014](https://arxiv.org/abs/2403.12014)

    EnvGen提出了一种新的框架，利用LLMs的推理能力自适应创建训练环境，帮助小型具身体RL代理在弱点方面学习有用技能。

    

    最近有关通过互动进行具身体学习的最新方法直接采用大型语言模型（LLMs）作为代理，以确定环境中的下一步。LLM代理由于其世界知识和推理能力，比基于强化学习（RL）的以往较小的代理表现更强；但频繁调用LLMs速度慢且昂贵。我们提出EnvGen，一个处理这个问题的新框架。首先，我们提示一个LLM生成训练环境，使代理可以快速并行学习不同任务。具体而言，LLM获得任务描述和模拟器目标，然后被要求生成一组环境配置。

    arXiv:2403.12014v1 Announce Type: cross  Abstract: Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g
    
[^10]: 基于Lie群的动力学Langevin Monte Carlo算法的收敛性

    Convergence of Kinetic Langevin Monte Carlo on Lie groups

    [https://arxiv.org/abs/2403.12012](https://arxiv.org/abs/2403.12012)

    提出了一个基于Lie群的动力学Langevin Monte Carlo采样算法，通过添加噪声和精细离散化实现了Lie群结构的保持，并在W2距离下证明了连续动力学和离散采样器的指数收敛性。

    

    最近，基于变分优化和左平凡化等技术构建了一个明确的、基于动量的动力学系统，用于优化定义在Lie群上的函数。我们适当地为优化动力学添加可处理的噪声，将其转化为采样动力学，利用动量变量是欧几里得的这一有利特性，尽管潜在函数存在于流形上。然后，我们通过精心离散化导致的动力学采样动力学提出了一个Lie群MCMC采样器。这种离散化完全保持了Lie群结构。在W2距离下，分别对连续动力学和离散采样器证明了指数收敛性，其中只需要Lie群的紧致性和潜在函数的测地L-光滑性。据我们所知，这是对动力学Langevin算法的第一个收敛性结果。

    arXiv:2403.12012v1 Announce Type: cross  Abstract: Explicit, momentum-based dynamics for optimizing functions defined on Lie groups was recently constructed, based on techniques such as variational optimization and left trivialization. We appropriately add tractable noise to the optimization dynamics to turn it into a sampling dynamics, leveraging the advantageous feature that the momentum variable is Euclidean despite that the potential function lives on a manifold. We then propose a Lie-group MCMC sampler, by delicately discretizing the resulting kinetic-Langevin-type sampling dynamics. The Lie group structure is exactly preserved by this discretization. Exponential convergence with explicit convergence rate for both the continuous dynamics and the discrete sampler are then proved under W2 distance. Only compactness of the Lie group and geodesically L-smoothness of the potential function are needed. To the best of our knowledge, this is the first convergence result for kinetic Langev
    
[^11]: 确定通过移动数字行为变化干预提高癌症患者福祉的有效参与方式

    Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions

    [https://arxiv.org/abs/2403.12007](https://arxiv.org/abs/2403.12007)

    本研究旨在定义通过数字行为变化干预支持癌症患者提高生活质量的有效参与方式，发现医生处方显著增加患者对移动数字行为变化干预的持续参与，同时指出每周参与一次已足以维持福祉，但内在动机可能需要更高水平的参与。

    

    数字行为变化干预（DBCI）正在支持新健康行为的发展。评估它们的有效性对于改进它们和理解成功因素至关重要。然而，特别是在受伦理限制的小规模研究中，开发者的全面指导仍然有限。本研究基于CAPABLE项目，旨在定义通过DBCI支持癌症患者提高生活质量的有效参与方式。我们确定了衡量参与度的指标，探讨了患者和临床医生对DBCI的兴趣，并提出了在这种背景下评估DBCI影响的假设。我们的研究结果表明，医生的处方显着增加了患者对移动DBCI的持续参与。此外，尽管每周一次参与DBCI就足以维持福祉，但从外在动机向内在动机的转变可能需要更高水平的参与。

    arXiv:2403.12007v1 Announce Type: cross  Abstract: Digital Behavior Change Interventions (DBCIs) are supporting development of new health behaviors. Evaluating their effectiveness is crucial for their improvement and understanding of success factors. However, comprehensive guidance for developers, particularly in small-scale studies with ethical constraints, is limited. Building on the CAPABLE project, this study aims to define effective engagement with DBCIs for supporting cancer patients in enhancing their quality of life. We identify metrics for measuring engagement, explore the interest of both patients and clinicians in DBCIs, and propose hypotheses for assessing the impact of DBCIs in such contexts. Our findings suggest that clinician prescriptions significantly increase sustained engagement with mobile DBCIs. In addition, while one weekly engagement with a DBCI is sufficient to maintain well-being, transitioning from extrinsic to intrinsic motivation may require a higher level o
    
[^12]: 2023年机器学习中信任可视化的最新进展

    Visualization for Trust in Machine Learning Revisited: The State of the Field in 2023

    [https://arxiv.org/abs/2403.12005](https://arxiv.org/abs/2403.12005)

    2023年的研究显示，可解释和可信赖的机器学习可视化仍然是一个重要且不断发展的领域，为各种领域提供了趋势、见解和挑战。

    

    可解释和可信赖的机器学习可视化仍然是信息可视化和视觉分析领域中最重要和深入研究的领域之一，涉及医学、金融和生物信息学等各种应用领域。在我们2020年的最新报告中，包括了200种技术，我们坚持收集同行评审的文章，描述可视化技术，根据先前建立的包含119个类别的分类模式对其进行分类，并在在线调查浏览器中提供了542种技术的结果集。在本调查文章中，我们介绍了截至2023年秋季关于这一数据集的新分析结果，并讨论了在机器学习中使用可视化的趋势、见解和八个开放挑战。我们的结果证实了可视化技术在增加对机器学习模型的信任方面呈快速增长的趋势。

    arXiv:2403.12005v1 Announce Type: cross  Abstract: Visualization for explainable and trustworthy machine learning remains one of the most important and heavily researched fields within information visualization and visual analytics with various application domains, such as medicine, finance, and bioinformatics. After our 2020 state-of-the-art report comprising 200 techniques, we have persistently collected peer-reviewed articles describing visualization techniques, categorized them based on the previously established categorization schema consisting of 119 categories, and provided the resulting collection of 542 techniques in an online survey browser. In this survey article, we present the updated findings of new analyses of this dataset as of fall 2023 and discuss trends, insights, and eight open challenges for using visualizations in machine learning. Our results corroborate the rapidly growing trend of visualization techniques for increasing trust in machine learning models in the p
    
[^13]: 学习递归神经网络权重矩阵的有用表示

    Learning Useful Representations of Recurrent Neural Network Weight Matrices

    [https://arxiv.org/abs/2403.11998](https://arxiv.org/abs/2403.11998)

    提出了机械主义和功能主义两种方法以学习递归神经网络(RNN)权重的有用表示，并发展了框架来生成有助于确定RNN行为的丰富表示

    

    递归神经网络(RNNs)是通用的并行串行计算机。 RNN的程序是其权重矩阵。 如何学习有助于RNN分析以及下游任务的RNN权重的有用表示？ 尽管机械主义方法直接查看一些RNN的权重来预测其行为，功能主义方法分析其整体功能--具体来说是其输入输出映射。 我们考虑了几种适用于RNN权重的机械主义方法，并为RNN引入了置换等变的深度权重空间层。我们的两种新颖的功能主义方法通过“询问”输入而从RNN权重中提取信息。 我们开发了一个理论框架，证明了功能主义方法能够产生有助于确定RNN行为的丰富表示的条件。 我们创建并发布了第一个两个“模型动物园”数据集，用于RNN权重表示

    arXiv:2403.11998v1 Announce Type: new  Abstract: Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation 
    
[^14]: 利用生成式知识提取、基于图的表示和多模态智能图推理加速科学发现

    Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning

    [https://arxiv.org/abs/2403.11996](https://arxiv.org/abs/2403.11996)

    利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。

    

    利用生成式人工智能，我们将一组涉及生物材料领域的1,000篇科学论文转化为详细的本体知识图，揭示了它们固有的无标度特性。通过基于节点相似性和介数中心性的组合排名，探测不同概念之间的图遍历路径，我们揭示了深入的跨学科关系，可用于回答查询，识别知识中的空白，并提出前所未见的材料设计及其行为。一项比较揭示了生物材料和贝多芬第九交响曲之间的详细结构相似之处，突显了通过同构映射共享复杂性模式。该算法进一步创建了一种创新的基于分级菌丝体的复合材料，将图采样的联合合成原理与康定斯基《第七组成》中提取的原则相结合

    arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
    
[^15]: 扩散去噪作为一种针对干净标签中毒的认证防御

    Diffusion Denoising as a Certified Defense against Clean-label Poisoning

    [https://arxiv.org/abs/2403.11981](https://arxiv.org/abs/2403.11981)

    提出了一种扩散去噪作为针对干净标签中毒的认证防御，能将攻击成功率降低到0-16%，同时几乎不影响测试准确性，为未来开发更强干净标签攻击和利用该防御措施作为强有力基础提供了重要启示。

    

    我们提出了一种针对干净标签中毒攻击的认证防御方法。这些攻击通过向训练数据中注入少量的毒害样本（例如1%），其中包含$p$-范数受限的对抗性扰动，从而诱导对测试输入的目标误分类。受到$去噪平滑$实现的对抗鲁棒性的启发，我们展示了如何使用一个现成的扩散模型对篡改的训练数据进行消毒。我们广泛测试了我们的防御措施对七种干净标签中毒攻击的防护效果，并且将它们的攻击成功率降低到0-16%，同时测试准确性几乎没有下降。我们将我们的防御与现有的针对干净标签中毒的对策进行比较，显示出我们的防御效果最佳，并提供最佳的模型效用。我们的结果凸显了未来需要开展更强大的干净标签攻击并使用我们的认证但实用的防御作为稳固基础的必要性。

    arXiv:2403.11981v1 Announce Type: cross  Abstract: We present a certified defense to clean-label poisoning attacks. These attacks work by injecting a small number of poisoning samples (e.g., 1%) that contain $p$-norm bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by $denoised$ $smoothing$, we show how an off-the-shelf diffusion model can sanitize the tampered training data. We extensively test our defense against seven clean-label poisoning attacks and reduce their attack success to 0-16% with only a negligible drop in the test time accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong base
    
[^16]: 揭示无分类器引导的条件扩散模型：一个尖锐的统计理论

    Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory

    [https://arxiv.org/abs/2403.11968](https://arxiv.org/abs/2403.11968)

    本文揭示了无分类器引导的条件扩散模型的尖锐统计理论，提出了适应数据分布平滑度的样本复杂度界限，并展示了新颖的扩散泰勒逼近技术在理论发展中的重要性。

    

    arXiv:2403.11968v1 公告类型: 新 具有分类条件的扩散模型是现代图像合成的基础，在计算生物学和强化学习等领域得到广泛应用。在这些应用中，条件扩散模型结合各种条件信息，如提示输入，以引导样本生成到所需的属性。尽管经验上取得成功，但条件扩散模型的理论在很大程度上尚不完备。本文通过展示使用条件扩散模型进行分布估计的尖锐统计理论来弥合这一差距。我们的分析得出一个适应数据分布平滑度并匹配极小极值下限的样本复杂度界限。我们理论发展的关键在于条件评分函数的逼近结果，依赖于一种新颖的扩散泰勒逼近技术。此外，我们展示了我们的统计理论在...

    arXiv:2403.11968v1 Announce Type: new  Abstract: Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning. In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional diffusion models is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models. Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in 
    
[^17]: 通知谱归一化高斯过程用于轨迹预测

    Informed Spectral Normalized Gaussian Processes for Trajectory Prediction

    [https://arxiv.org/abs/2403.11966](https://arxiv.org/abs/2403.11966)

    该论文提出了一种用于轨迹预测的新颖正则化持续学习方法，利用通知式先验知识，提高了模型性能和数据效率。

    

    先前的参数分布为通知式学习提供了一种优雅的方式，以表示先验专家和世界知识。以前的工作表明，使用这样的信息先验来规范概率深度学习（DL）模型会增加它们的性能和数据效率。然而，用于概率DL模型的常用基于采样的近似方法可能计算昂贵，需要多次推理和更长的训练时间。计算高效的最后一层核逼近如谱归一化高斯过程（SNGPs）是有希望的替代方案。我们提出了一种针对SNGPs的基于新颖正则化的持续学习方法，它可以使用代表先前任务学习的先验知识的通知先验。我们的提议建立在成熟方法的基础上，不需要记忆或参数扩展。我们将我们的通知SNGP模型应用于轨迹预测问题。

    arXiv:2403.11966v1 Announce Type: cross  Abstract: Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning. Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency. However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times. Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs). We propose a novel regularization-based continual learning method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks. Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion. We apply our informed SNGP model to the trajectory prediction proble
    
[^18]: 神经网络回归的设计概率校准

    Probabilistic Calibration by Design for Neural Network Regression

    [https://arxiv.org/abs/2403.11964](https://arxiv.org/abs/2403.11964)

    提出了一种称为Quantile Recalibration Training的新型端到端模型训练过程，将后处理校准直接整合到训练过程中，无需额外参数，展示出在神经网络回归中提高校准性能的方法。

    

    为了在许多真实世界应用中进行最佳决策，为回归问题生成经过校准且精确的神经网络预测分布至关重要。为解决神经网络的误校准问题，提出了各种改善校准的方法，包括在训练后调整预测的后处理方法和在训练过程中进行操作的正则化方法。虽然与正则化方法相比，后处理方法在校准方面表现出更好的改进，但后处理步骤与模型训练完全独立。我们引入了一种称为Quantile Recalibration Training的新型端到端模型训练过程，将后处理校准直接整合到训练过程中，无需额外的参数。我们还提出了一个统一的算法，将我们的方法和其他后处理方法以及正则化方法作为特殊情况包含在内。我们展示了我们的方法在一个大规模问题上的性能。

    arXiv:2403.11964v1 Announce Type: new  Abstract: Generating calibrated and sharp neural network predictive distributions for regression problems is essential for optimal decision-making in many real-world applications. To address the miscalibration issue of neural networks, various methods have been proposed to improve calibration, including post-hoc methods that adjust predictions after training and regularization methods that act during training. While post-hoc methods have shown better improvement in calibration compared to regularization methods, the post-hoc step is completely independent of model training. We introduce a novel end-to-end model training procedure called Quantile Recalibration Training, integrating post-hoc calibration directly into the training process without additional parameters. We also present a unified algorithm that includes our method and other post-hoc and regularization methods, as particular cases. We demonstrate the performance of our method in a large
    
[^19]: 超越有界密度比的迁移学习

    Transfer Learning Beyond Bounded Density Ratios

    [https://arxiv.org/abs/2403.11963](https://arxiv.org/abs/2403.11963)

    低次多项式估计类上的迁移学习，证明了在非常温和的假设下，对于低次多项式来说非平凡的迁移学习是可能的，超越了$dQ/dP$有界的经典假设

    

    我们研究了迁移学习的基本问题，即学习算法从某个源分布$P$收集数据，但需要在不同的目标分布$Q$上表现良好。标准的测度变换论证表明，当密度比$dQ/dP$有界时发生迁移学习。然而，Kpotufe和Martinet(2018年COLT)以及Hanneke和Kpotufe(2019年NeurIPS)之前引人深思的作品展示了一些情况，其中比率$dQ/dP$是无界的，但迁移学习是可能的。在这项工作中，我们专注于在低次多项式估计类上进行迁移学习。我们的主要结果是在定义域$\mathbb{R}^n$上的一般迁移不等式，证明了在非常温和的假设下，对于低次多项式来说非平凡的迁移学习是可能的，远远超出了$dQ/dP$被有界的经典假设。例如，如果$Q$是对数凹测度，则始终适用。

    arXiv:2403.11963v1 Announce Type: new  Abstract: We study the fundamental problem of transfer learning where a learning algorithm collects data from some source distribution $P$ but needs to perform well with respect to a different target distribution $Q$. A standard change of measure argument implies that transfer learning happens when the density ratio $dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio $dQ/dP$ is unbounded, but transfer learning is possible.   In this work, we focus on transfer learning over the class of low-degree polynomial estimators. Our main result is a general transfer inequality over the domain $\mathbb{R}^n$, proving that non-trivial transfer learning for low-degree polynomials is possible under very mild assumptions, going well beyond the classical assumption that $dQ/dP$ is bounded. For instance, it always applies if $Q$ is a log-concave measur
    
[^20]: 通过运动补偿增强事件驱动视频重建

    Enhanced Event-Based Video Reconstruction with Motion Compensation

    [https://arxiv.org/abs/2403.11961](https://arxiv.org/abs/2403.11961)

    通过运动补偿来增强重建质量，提出将输入帧和稀疏编码进行变换，并将流网络与CISTA-LSTC集成，形成CISTA-Flow网络，使系统仅依赖事件。

    

    深度神经网络用于事件驱动视频重建通常缺乏可解释性，并且具有高内存需求。最近引入了一种轻量级网络CISTA-LSTC，表明通过系统设计其架构可以实现高质量的重建。然而，其建模假设输入信号和输出重建帧共享相同的稀疏表示，忽视了运动导致的位移。为了解决这一问题，我们提出对输入强度帧和稀疏编码进行变换以增强重建质量。通过将流网络与CISTA-LSTC集成，构建了一个CISTA-Flow网络用于运动补偿。系统仅依赖事件，其中预测的流有助于重建，然后重建的帧用于促进流估计。我们还为该组合系统引入了一个迭代训练框架。结果表明

    arXiv:2403.11961v1 Announce Type: cross  Abstract: Deep neural networks for event-based video reconstruction often suffer from a lack of interpretability and have high memory demands. A lightweight network called CISTA-LSTC has recently been introduced showing that high-quality reconstruction can be achieved through the systematic design of its architecture. However, its modelling assumption that input signals and output reconstructed frame share the same sparse representation neglects the displacement caused by motion. To address this, we propose warping the input intensity frames and sparse codes to enhance reconstruction quality. A CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC for motion compensation. The system relies solely on events, in which predicted flow aids in reconstruction and then reconstructed frames are used to facilitate flow estimation. We also introduce an iterative training framework for this combined system. Results demonstrate tha
    
[^21]: CASPER：因果关系感知时空图神经网络用于时空时间序列插补

    CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation

    [https://arxiv.org/abs/2403.11960](https://arxiv.org/abs/2403.11960)

    CASPER提出了一种因果关系感知的方法来处理时空时间序列数据插补问题，避免过度利用非因果关系，提高数据分析的准确性。

    

    arXiv:2403.11960v1 公告类型：新 提要：时空时间序列是理解人类活动及其影响的基础，通常通过放置在不同位置的监测传感器收集。收集到的数据通常包含由于各种故障而导致的缺失值，这对数据分析有重要影响。为了填补缺失值，已经提出了许多方法。在恢复特定数据点时，大多数现有方法倾向于考虑与该点相关的所有信息，无论它们是否具有因果关系。在数据收集过程中，包括一些未知混杂因素是不可避免的，例如时间序列中的背景噪声和构建的传感器网络中的非因果快捷边。这些混杂因素可能在输入和输出之间开辟反向路径，换句话说，它们建立了输入和输出之间的非因果相关性。

    arXiv:2403.11960v1 Announce Type: new  Abstract: Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations. The collected data usually contains missing values due to various failures, which have significant impact on data analysis. To impute the missing values, a lot of methods have been introduced. When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output. Over-exploiting these non-causa
    
[^22]: 学习编码空间曲率内非线性的动力学系统

    Learning Dynamical Systems Encoding Non-Linearity within Space Curvature

    [https://arxiv.org/abs/2403.11948](https://arxiv.org/abs/2403.11948)

    引入一种方法以提高学习动力学系统的复杂性，同时不影响训练效率或稳定性保证。

    

    动力学系统（DS）是塑造机器人控制的高级策略的有效和强大手段。它们提供了稳健和反应灵敏的控制，同时保证了驱动向量场的稳定性。现实世界场景的日益复杂性需要具有更高程度非线性的DS，以及适应环境条件变化（如障碍物）的能力。目前DS的学习策略往往需要权衡，要么牺牲稳定性保证，要么牺牲离线计算效率，以增强所学DS的能力。在线对环境变化进行局部适应要么没有被考虑，要么被视为一个单独的问题。本文的目标是介绍一种方法，增强学习DS的复杂性，同时在训练期间不影响效率或稳定性保证。此外，我们的目标是提供一个统一的

    arXiv:2403.11948v1 Announce Type: cross  Abstract: Dynamical Systems (DS) are an effective and powerful means of shaping high-level policies for robotics control. They provide robust and reactive control while ensuring the stability of the driving vector field. The increasing complexity of real-world scenarios necessitates DS with a higher degree of non-linearity, along with the ability to adapt to potential changes in environmental conditions, such as obstacles. Current learning strategies for DSs often involve a trade-off, sacrificing either stability guarantees or offline computational efficiency in order to enhance the capabilities of the learned DS. Online local adaptation to environmental changes is either not taken into consideration or treated as a separate problem. In this paper, our objective is to introduce a method that enhances the complexity of the learned DS without compromising efficiency during training or stability guarantees. Furthermore, we aim to provide a unified 
    
[^23]: 使用可微分决策树的可解释强化学习家庭能源管理系统

    Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees

    [https://arxiv.org/abs/2403.11947](https://arxiv.org/abs/2403.11947)

    引入强化学习和可解释性决策树相结合的方法，实现了在家庭能源管理中有效管理能耗的控制器。

    

    随着能源转型的不断发展，需求侧灵活性成为现代电网的重要组成部分，为提供电网支持并促进可持续能源的进一步整合。除了传统能源来源外，住宅部门是另一个主要且尚未充分利用的灵活性来源，主要受到太阳能光伏、家庭电池和电动汽车的日益普及推动。然而，释放住宅这种能源灵活性具有挑战性，因为这需要一个可以有效管理家庭能源消耗并在保持用户舒适的同时能够在不同各种房屋之间容易扩展的控制框架。我们旨在解决这一具有挑战性的问题，并引入了一种使用可微分决策树的强化学习方法。该方法将数据驱动的强化学习的可伸缩性与可解释性的（可微分）决策树相结合。这样就产生了一个控制器，可以...

    arXiv:2403.11947v1 Announce Type: cross  Abstract: With the ongoing energy transition, demand-side flexibility has become an important aspect of the modern power grid for providing grid support and allowing further integration of sustainable energy sources. Besides traditional sources, the residential sector is another major and largely untapped source of flexibility, driven by the increased adoption of solar PV, home batteries, and EVs. However, unlocking this residential flexibility is challenging as it requires a control framework that can effectively manage household energy consumption, and maintain user comfort while being readily scalable across different, diverse houses. We aim to address this challenging problem and introduce a reinforcement learning-based approach using differentiable decision trees. This approach integrates the scalability of data-driven reinforcement learning with the explainability of (differentiable) decision trees. This leads to a controller that can be e
    
[^24]: 多步反向不是你所需要的

    Multistep Inverse Is Not All You Need

    [https://arxiv.org/abs/2403.11940](https://arxiv.org/abs/2403.11940)

    本研究考虑了控制问题中的观测空间到简化控制相关变量空间的编码器学习，AC-State方法是一个多步反向方法。

    

    在真实世界的控制设置中，观测空间通常是不必要的高维且受到时间相关噪声的影响。然而，系统的可控动态通常远比原始观测数据的动态简单。因此，学习一个编码器将观测空间映射到一个包含控制相关变量的简化空间是可取的。本文考虑了由Efroni等人（2022年）首次提出的Ex-BMDP模型，该模型将观测可以分解为依赖于动作的潜在状态和独立于动作的时间相关噪声，并形式化了能够解决控制问题的场景。Lamb等人（2022年）提出了“AC-State”方法，用于学习一个编码器，从这些问题中的观测中提取包含完整依赖于动作的潜在状态表示。AC-State是一个多步反向方法，它使用路径中第一个和最后一个状态的编码来预测

    arXiv:2403.11940v1 Announce Type: new  Abstract: In real-world control settings, the observation space is often unnecessarily high-dimensional and subject to time-correlated noise. However, the controllable dynamics of the system are often far simpler than the dynamics of the raw observations. It is therefore desirable to learn an encoder to map the observation space to a simpler space of control-relevant variables. In this work, we consider the Ex-BMDP model, first proposed by Efroni et al. (2022), which formalizes control problems where observations can be factorized into an action-dependent latent state which evolves deterministically, and action-independent time-correlated noise. Lamb et al. (2022) proposes the "AC-State" method for learning an encoder to extract a complete action-dependent latent state representation from the observations in such problems. AC-State is a multistep-inverse method, in that it uses the encoding of the the first and last state in a path to predict the 
    
[^25]: Roesser类型的状态空间表示用于卷积层

    State space representations of the Roesser type for convolutional layers

    [https://arxiv.org/abs/2403.11938](https://arxiv.org/abs/2403.11938)

    从控制理论的角度，提供了Roesser类型的2-D卷积层状态空间表示，具有最小化的状态数量，在$c_\mathrm{in}=c_\mathrm{out}$的情况下证明了这一点，并进一步实现了扩张、跨越和N-D卷积的状态空间表示。

    

    从控制理论的角度看，卷积层（神经网络的）是2-D（或N-D）线性时不变动态系统。卷积层通常通过卷积核表示，对应于动态系统通过其脉冲响应表示。然而，许多控制理论的分析工具，例如涉及线性矩阵不等式的工具，需要一个状态空间表示。因此，我们明确提供了Roesser类型的2-D卷积层状态空间表示，具有$c_\mathrm{in}r_1+c_\mathrm{out}r_2$个状态，其中$c_\mathrm{in}/c_\mathrm{out}$是层的输入/输出通道数，$r_1/r_2$ 表示卷积核的宽度/长度。对于$c_\mathrm{in}=c_\mathrm{out}$，已经证明这种表示是最小的。我们进一步构建了扩张、跨越和N-D卷积的状态空间表示。

    arXiv:2403.11938v1 Announce Type: cross  Abstract: From the perspective of control theory, convolutional layers (of neural networks) are 2-D (or N-D) linear time-invariant dynamical systems. The usual representation of convolutional layers by the convolution kernel corresponds to the representation of a dynamical system by its impulse response. However, many analysis tools from control theory, e.g., involving linear matrix inequalities, require a state space representation. For this reason, we explicitly provide a state space representation of the Roesser type for 2-D convolutional layers with $c_\mathrm{in}r_1 + c_\mathrm{out}r_2$ states, where $c_\mathrm{in}$/$c_\mathrm{out}$ is the number of input/output channels of the layer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel. This representation is shown to be minimal for $c_\mathrm{in} = c_\mathrm{out}$. We further construct state space representations for dilated, strided, and N-D convolutions.
    
[^26]: 在平均奖励强化学习中实现全局最优性而无需混合时间预测：基于多级Actor-Critic方法

    Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic

    [https://arxiv.org/abs/2403.11925](https://arxiv.org/abs/2403.11925)

    通过多级Actor-Critic框架和多级蒙特卡罗梯度估计器，本研究成功解决了平均奖励MDPs全局收敛中对混合时间预测的依赖性，展现出最严格的依赖关系。

    

    在平均奖励强化学习的背景下，对于混合时间的预测的oracle知识要求，即度量马尔可夫链在固定策略下达到其稳态分布所需的时间，对于策略梯度方法的全球收敛构成了重大挑战。为了解决这一限制，我们考虑了多级Actor-Critic（MAC）框架，该框架结合了多级蒙特卡洛（MLMC）梯度估计器。通过我们的方法，实现了对混合时间知识的依赖性的有效减轻，这是平均奖励MDPs全局收敛的首次尝试。此外，我们的方法展现出最严格的$\mathcal{O}$依赖关系。

    arXiv:2403.11925v1 Announce Type: new  Abstract: In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\mathcal{O
    
[^27]: 单Agent Actor Critic用于去中心化合作驾驶

    Single-Agent Actor Critic for Decentralized Cooperative Driving

    [https://arxiv.org/abs/2403.11914](https://arxiv.org/abs/2403.11914)

    提出了一种新颖的单Agent Actor Critic模型，旨在利用单Agent强化学习学习自主车辆的去中心化合作驾驶策略，并通过对各种交通场景的广泛评估展现了改善道路系统内不同瓶颈位置交通流量的巨大潜力。

    

    主动交通管理结合自主车辆（AVs）承诺未来拥有减少拥堵和增强交通流量。然而，为实际应用开发算法需要解决连续交通流量和部分可观察性带来的挑战。为了弥合这一差距，推动主动交通管理领域朝着更大程度的去中心化发展，我们介绍了一个新颖的不对称actor-critic模型，旨在利用单Agent强化学习学习自主车辆的去中心化合作驾驶策略。我们的方法采用具有掩码的注意力神经网络来处理实际交通流量的动态特性和部分可观察性。通过在各种交通场景中针对基线控制器的广泛评估，我们的模型显示出在道路系统内不同瓶颈位置改善交通流量的巨大潜力。

    arXiv:2403.11914v1 Announce Type: new  Abstract: Active traffic management incorporating autonomous vehicles (AVs) promises a future with diminished congestion and enhanced traffic flow. However, developing algorithms for real-world application requires addressing the challenges posed by continuous traffic flow and partial observability. To bridge this gap and advance the field of active traffic management towards greater decentralization, we introduce a novel asymmetric actor-critic model aimed at learning decentralized cooperative driving policies for autonomous vehicles using single-agent reinforcement learning. Our approach employs attention neural networks with masking to handle the dynamic nature of real-world traffic flow and partial observability. Through extensive evaluations against baseline controllers across various traffic scenarios, our model shows great potential for improving traffic flow at diverse bottleneck locations within the road system. Additionally, we explore t
    
[^28]: Distill2Explain: 可解释强化学习在能源应用控制器中的可微分决策树

    Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers

    [https://arxiv.org/abs/2403.11907](https://arxiv.org/abs/2403.11907)

    可解释性强化学习在能源应用控制器中的创新是通过提出可微分决策树来解决数据驱动控制中的可解释性和住宅资产硬件能力受限等挑战。

    

    需求侧灵活性作为能源过渡进程中的关键要素日益重要。作为全球最终能源消耗的约25％，住宅部门是一个重要的（潜在的）能源灵活性来源。然而，发掘这种灵活性需要开发一个控制框架，该框架在不同房屋之间容易扩展，易于维护，并且对终端用户简单易懂。针对这一任务的潜在控制框架是数据驱动控制，特别是无模型强化学习（RL）。这种基于RL的控制器通过与环境交互学习良好的控制策略，完全基于数据学习，并且人类干预最小。然而，它们缺乏可解释性，这妨碍了用户接受。此外，住宅资产的有限硬件能力构成了阻碍（例如使用深度神经网络）。为了克服这两个挑战，我们提出了一种...

    arXiv:2403.11907v1 Announce Type: cross  Abstract: Demand-side flexibility is gaining importance as a crucial element in the energy transition process. Accounting for about 25% of final energy consumption globally, the residential sector is an important (potential) source of energy flexibility. However, unlocking this flexibility requires developing a control framework that (1) easily scales across different houses, (2) is easy to maintain, and (3) is simple to understand for end-users. A potential control framework for such a task is data-driven control, specifically model-free reinforcement learning (RL). Such RL-based controllers learn a good control policy by interacting with their environment, learning purely based on data and with minimal human intervention. Yet, they lack explainability, which hampers user acceptance. Moreover, limited hardware capabilities of residential assets forms a hurdle (e.g., using deep neural networks). To overcome both those challenges, we propose a no
    
[^29]: CICLe: 适应上下文的大规模多类食品风险分类学习

    CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification

    [https://arxiv.org/abs/2403.11904](https://arxiv.org/abs/2403.11904)

    该论文提出了CICLe框架，基于适应上下文学习的方式，在大规模多类食品风险分类中取得了较好的效果，提出了基于符合预测的LLM-in-the-loop框架，可以提高基本分类器的性能，并减少能源消耗。

    

    受污染或掺假食品对人类健康构成重大风险。在给定了用于训练的标记网络文本集的情况下，可以应用机器学习和自然语言处理来自动检测这种风险。我们发布了一个包含7,546个描述公共食品召回公告的短文本数据集。每个文本都经过手动标记，分为两个粒度级别（粗粒度和细粒度），用于表示召回对应的食品产品和危害。我们描述了数据集并对朴素、传统和Transformer模型进行了基准测试。基于我们的分析，基于tf-idf表示的逻辑回归在支持较低的类别上优于RoBERTa和XLM-R。最后，我们讨论了不同的提示策略，并提出了一种基于符合预测的LLM-in-the-loop框架，这可以提高基本分类器的性能，同时减少了与普通提示相比的能源消耗。

    arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
    
[^30]: Larimar: 具有情节记忆控制的大型语言模型

    Larimar: Large Language Models with Episodic Memory Control

    [https://arxiv.org/abs/2403.11901](https://arxiv.org/abs/2403.11901)

    Larimar提出了一种大脑启发的架构，通过分布式情节记忆增强LLMs，实现了动态、一次性的知识更新，无需昂贵的重新训练或微调，且在速度和灵活性上表现出色。

    

    本文提出了Larimar - 一种新颖的、受大脑启发的架构，用于增强大型语言模型(LLMs)的分布式情节记忆。 Larimar的记忆允许动态、一次性更新知识，无需进行计算昂贵的重新训练或微调。在多个事实编辑基准测试上的实验结果表明，Larimar在速度方面表现优异 - 根据基础LLM的不同，速度提升为4-10倍，并且由于提出的架构简单、不依赖于LLM，因此具有良好的灵活性和通用性。我们进一步提供了选择性事实遗忘和输入上下文长度概括机制，并展示了它们的有效性。

    arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
    
[^31]: 视觉-触觉预训练用于插拔电缆

    Visuo-Tactile Pretraining for Cable Plugging

    [https://arxiv.org/abs/2403.11898](https://arxiv.org/abs/2403.11898)

    本文研究了如何将触觉信息纳入模仿学习平台以在复杂任务中提高性能，通过训练机器人代理插拔USB电缆，实现了在微细操纵任务中的进展。

    

    触觉信息是进行精细操纵的关键工具。作为人类，我们在很大程度上依赖触觉信息来理解周围的物体以及如何与其互动。我们不仅使用触摸来执行操纵任务，还用它来学习如何执行这些任务。因此，为了创建能够学习以人类或超人类水平完成操纵任务的机器人代理，我们需要正确地将触觉信息融入技能执行和技能学习中。本文研究了如何将触觉信息纳入模仿学习平台以提高复杂任务的性能。为此，我们着手解决插拔USB电缆的挑战，这是一项依赖于微观视觉-触觉协作的熟练操纵任务。通过将触觉信息纳入模仿学习框架，我们能够训练一个机器人代理插拔USB电缆。

    arXiv:2403.11898v1 Announce Type: cross  Abstract: Tactile information is a critical tool for fine-grain manipulation. As humans, we rely heavily on tactile information to understand objects in our environments and how to interact with them. We use touch not only to perform manipulation tasks but also to learn how to perform these tasks. Therefore, to create robotic agents that can learn to complete manipulation tasks at a human or super-human level of performance, we need to properly incorporate tactile information into both skill execution and skill learning. In this paper, we investigate how we can incorporate tactile information into imitation learning platforms to improve performance on complex tasks. To do this, we tackle the challenge of plugging in a USB cable, a dexterous manipulation task that relies on fine-grain visuo-tactile serving. By incorporating tactile information into imitation learning frameworks, we are able to train a robotic agent to plug in a USB cable - a firs
    
[^32]: 从可解释到可解释的深度学习在医疗自然语言处理中的应用：现实有多远？

    From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?

    [https://arxiv.org/abs/2403.11894](https://arxiv.org/abs/2403.11894)

    该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。

    

    深度学习（DL）通过解决各种自然语言处理（NLP）任务，极大地增强了医疗保健研究。然而，基于DL的NLP方法日益复杂，需要透明的模型解释性，或至少是可解释性，以进行可靠的决策制定。本文对医疗健康NLP中的可解释和可解释的DL进行了彻底的范围审查。引入了术语“XIAI”（eXplainable和Interpretable Artificial Intelligence）以区分XAI和IAI。方法根据其功能（模型、输入、输出为基础）和范围（局部、全局）进一步分类。我们的分析表明，注意机制是最主要的新兴IAI。此外，IAI越来越多地用于对抗XAI。确定的主要挑战是大多数XIAI不探索“全局”建模过程，缺乏最佳实践，并且需要系统评估和基准测试。

    arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
    
[^33]: KnFu: 有效的知识融合

    KnFu: Effective Knowledge Fusion

    [https://arxiv.org/abs/2403.11892](https://arxiv.org/abs/2403.11892)

    FKD是一个新的联邦学习范式，基于知识蒸馏概念，尝试在解决FL中的模型异构性和梯度反转攻击方面取得进展。

    

    arXiv:2403.11892v1 公告类型：新  摘要：联邦学习（FL）已经成为传统集中学习方法的一种突出替代方案。一般来说，FL是一种去中心化方法，允许跨多个本地节点协作训练机器学习（ML）模型，确保数据隐私和安全，同时利用各种多样化的数据集。然而，传统的FL容易受到梯度反转攻击，强制在本地模型上实施统一架构，并且由于非独立同分布的本地数据集，导致模型异构性（模型漂移）。为了减轻其中一些挑战，出现了联邦知识蒸馏（FKD）的新范式。FKD基于知识蒸馏（KD）的概念开发，其中涉及从经验丰富并训练良好的大型教师模型中提取和转移知识到轻量级学生模型。然而，FKD仍然面临模型漂移问题。直观地说，并不是所有的知识都是统一的。

    arXiv:2403.11892v1 Announce Type: new  Abstract: Federated Learning (FL) has emerged as a prominent alternative to the traditional centralized learning approach. Generally speaking, FL is a decentralized approach that allows for collaborative training of Machine Learning (ML) models across multiple local nodes, ensuring data privacy and security while leveraging diverse datasets. Conventional FL, however, is susceptible to gradient inversion attacks, restrictively enforces a uniform architecture on local models, and suffers from model heterogeneity (model drift) due to non-IID local datasets. To mitigate some of these challenges, the new paradigm of Federated Knowledge Distillation (FKD) has emerged. FDK is developed based on the concept of Knowledge Distillation (KD), which involves extraction and transfer of a large and well-trained teacher model's knowledge to lightweight student models. FKD, however, still faces the model drift issue. Intuitively speaking, not all knowledge is univ
    
[^34]: SuperLoRA: 多层注意力模块参数高效统一适应

    SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules

    [https://arxiv.org/abs/2403.11887](https://arxiv.org/abs/2403.11887)

    SuperLoRA提出了一个统一且高度灵活的框架，通过引入不同的技巧扩展了不同的LoRA变体，在极少参数情况下特别优异。

    

    低秩自适应（LoRA）及其变体被广泛应用于微调大型模型，包括自然语言处理的大型语言模型和计算机视觉的扩散模型。本文提出了一个名为SuperLoRA的通用框架，统一并扩展了不同的LoRA变体，可以在不同的超参数设置下实现。通过引入分组、折叠、洗牌、投影和张量因子化，SuperLoRA相比其他LoRA变体提供了更高的灵活性，在非常少的参数范围内特别在传递学习任务中表现出卓越性能。

    arXiv:2403.11887v1 Announce Type: cross  Abstract: Low-rank adaptation (LoRA) and its variants are widely employed in fine-tuning large models, including large language models for natural language processing and diffusion models for computer vision. This paper proposes a generalized framework called SuperLoRA that unifies and extends different LoRA variants, which can be realized under different hyper-parameter settings. Introducing grouping, folding, shuffling, projecting, and tensor factoring, SuperLoRA offers high flexibility compared with other LoRA variants and demonstrates superior performance for transfer learning tasks especially in the extremely few-parameter regimes.
    
[^35]: 针对第四代区域供热网格的基于学习的热功率流效率训练

    Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids

    [https://arxiv.org/abs/2403.11877](https://arxiv.org/abs/2403.11877)

    提出了一种针对第四代区域供热网格的基于学习的热功率流高效训练方法，通过生成代理分布覆盖相关供需值来加速训练，避免求解热网格方程的迭代过程。

    

    热功率流（TPF）是第四代区域供热网格中的一个重要任务，该网格具有多个分散的热源和网格结构。计算TPF即确定网格状态（包括温度、压力和质量流）以满足给定供需值的控制目的，通常通过求解非线性热网格方程来实现，但可以通过使用神经网络等学习模型来加速几个数量级。我们提出了一种新颖高效的方案，用于生成涵盖相关供需值的足够大的训练数据集。我们的方法不是从采样的供需值中生成训练示例，而是从代理分布生成训练示例，该代理分布覆盖发电机和消费者质量流，省略了解决热网格方程所需的迭代过程。确切但略有不同的训练示例可以加权表示原始训练分布。

    arXiv:2403.11877v1 Announce Type: new  Abstract: Thermal power flow (TPF) is an important task for various control purposes in 4 Th generation district heating grids with multiple decentral heat sources and meshed grid structures. Computing the TPF, i.e., determining the grid state consisting of temperatures, pressures, and mass flows for given supply and demand values, is classically done by solving the nonlinear heat grid equations, but can be sped up by orders of magnitude using learned models such as neural networks. We propose a novel, efficient scheme to generate a sufficiently large training data set covering relevant supply and demand values. Instead of sampling supply and demand values, our approach generates training examples from a proxy distribution over generator and consumer mass flows, omitting the iterations needed for solving the heat grid equations. The exact, but slightly different, training examples can be weighted to represent the original training distribution. We
    
[^36]: 深度贝叶斯未来融合用于自监督、高分辨率、越野地图制作

    Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping

    [https://arxiv.org/abs/2403.11876](https://arxiv.org/abs/2403.11876)

    该论文提出了一种深度贝叶斯未来融合的方法，通过自监督的方式实现高分辨率越野地图的制作，为长程预测提供更好的支持。

    

    资源受限的越野车辆的传感器分辨率有限，这给可靠的越野自主性带来了巨大挑战。为了克服这一局限性，我们提出了一个基于融合未来信息（即未来融合）进行自监督的通用框架。最近的方法利用未来信息以及手工制作的启发式方法来直接监督目标下游任务（例如可穿越性估计）。然而，在本文中，我们选择了一个更为通用的发展方向 - 通过未来融合以自监督的方式时间高效地完成最高分辨率（即每像素2厘米）BEV地图，可用于任何下游任务以获得更好的长程预测。为此，首先，我们创建了一个高分辨率未来融合数据集，其中包含（RGB / 高度）原始稀疏噪音输入和基于地图的密集标签的成对数据。接下来，为了适应传感器的噪声和稀疏性

    arXiv:2403.11876v1 Announce Type: cross  Abstract: The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a self-supervised manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sens
    
[^37]: NuGraph2：用于中微子物理事件重建的图神经网络

    NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction

    [https://arxiv.org/abs/2403.11872](https://arxiv.org/abs/2403.11872)

    NuGraph2 是一种用于液氩时间投影室探测器中模拟中微子相互作用低级重建的图神经网络，通过多头注意力传递机制实现了高效的背景过滤和语义标记。

    

    arXiv:2403.11872v1 公告类型：跨领域 摘要：液氩时间投影室（LArTPC）探测器技术提供了丰富的高分辨率粒子相互作用信息，充分利用这些信息需要先进的自动重建技术。本文描述了NuGraph2，一种用于LArTPC探测器中模拟中微子相互作用低级重建的图神经网络（GNN）。MicroBooNE探测器几何形状中的模拟中微子相互作用被描述为异质图，每个探测器平面上的能量沉积形成平面子图上的节点。该网络利用多头注意力传递机制对这些图节点执行背景过滤和语义标记，以98.0\%的效率识别与主要物理相互作用相关联的节点，并以94.9\%的效率根据粒子类型将其标记。该网络直接在探测器可观察量上运行。

    arXiv:2403.11872v1 Announce Type: cross  Abstract: Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a wealth of high-resolution information on particle interactions, and leveraging that information to its full potential requires sophisticated automated reconstruction techniques. This article describes NuGraph2, a Graph Neural Network (GNN) for low-level reconstruction of simulated neutrino interactions in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE detector geometry are described as heterogeneous graphs, with energy depositions on each detector plane forming nodes on planar subgraphs. The network utilizes a multi-head attention message-passing mechanism to perform background filtering and semantic labelling on these graph nodes, identifying those associated with the primary physics interaction with 98.0\% efficiency and labelling them according to particle type with 94.9\% efficiency. The network operates directly on detector observables
    
[^38]: 神经网络的实际热带几何学

    The Real Tropical Geometry of Neural Networks

    [https://arxiv.org/abs/2403.11871](https://arxiv.org/abs/2403.11871)

    将二元分类器定义为热带有理函数的符号，发现ReLU神经网络的参数空间含于其内，提出了基于参数空间的两种不同细分方法，并描述了0/1损失函数的子水平集以及分类风扇的几何和组合特性。

    

    我们考虑将二元分类器定义为热带有理函数的符号，即两个凸分段线性函数的差。ReLU神经网络的参数空间被包含在热带有理函数参数空间的半代数集中。我们启动了对参数空间的两种不同细分的研究：一种细分为半代数集，其中决策边界的组合类型是固定的，另一种细分为一个多面体风扇，捕捉数据集的分区组合。0/1损失函数的子水平集出现为这种分类风扇的子风扇，我们展示了水平集不一定是连接的。我们描述分类风扇i) 几何上，作为激活多面体的法线风扇，以及ii) 组合上，通过关联二分图的性质列表，类比于有向性的线性代数公理。

    arXiv:2403.11871v1 Announce Type: cross  Abstract: We consider a binary classifier defined as the sign of a tropical rational function, that is, as the difference of two convex piecewise linear functions. The parameter space of ReLU neural networks is contained as a semialgebraic set inside the parameter space of tropical rational functions. We initiate the study of two different subdivisions of this parameter space: a subdivision into semialgebraic sets, on which the combinatorial type of the decision boundary is fixed, and a subdivision into a polyhedral fan, capturing the combinatorics of the partitions of the dataset. The sublevel sets of the 0/1-loss function arise as subfans of this classification fan, and we show that the level-sets are not necessarily connected. We describe the classification fan i) geometrically, as normal fan of the activation polytope, and ii) combinatorially through a list of properties of associated bipartite graphs, in analogy to covector axioms of orient
    
[^39]: 完备高效的用于晶体材料性质预测的图卷积神经网络

    Complete and Efficient Graph Transformers for Crystal Material Property Prediction

    [https://arxiv.org/abs/2403.11857](https://arxiv.org/abs/2403.11857)

    提出了一种利用晶体单位胞的周期模式建立晶格表示进行晶体高效图表示的新方法，同时设计了适用于晶体材料的SE(3) transformer，包括iComFormer和eComFormer两个变体。

    

    晶体结构由沿着整个三维空间定期重复的原胞内的原子基团特征化。晶体的周期性和无限性质为几何图表示学习提出了独特的挑战。具体来说，构建能够有效捕获晶体完整几何信息并处理手性晶体的图形仍然是一个未解决且具有挑战性的问题。在本文中，我们引入了一种新颖的方法，利用单位胞的周期模式为每个原子建立基于晶格的表示，实现了晶体的高效和富有表现力的图表示。此外，我们提出了ComFormer，一种专门针对晶体材料设计的SE(3)变压器。ComFormer包括两个变体，即使用不变的欧几里得距离和角度的iComFormer以及利用等变矢量表示的eComFormer。

    arXiv:2403.11857v1 Announce Type: new  Abstract: Crystal structures are characterized by atomic bases within a primitive unit cell that repeats along a regular lattice throughout 3D space. The periodic and infinite nature of crystals poses unique challenges for geometric graph representation learning. Specifically, constructing graphs that effectively capture the complete geometric information of crystals and handle chiral crystals remains an unsolved and challenging problem. In this paper, we introduce a novel approach that utilizes the periodic patterns of unit cells to establish the lattice-based representation for each atom, enabling efficient and expressive graph representations of crystals. Furthermore, we propose ComFormer, a SE(3) transformer designed specifically for crystalline materials. ComFormer includes two variants; namely, iComFormer that employs invariant geometric descriptors of Euclidean distances and angles, and eComFormer that utilizes equivariant vector representa
    
[^40]: 受限制学习问题的近似最优解决方案

    Near-Optimal Solutions of Constrained Learning Problems

    [https://arxiv.org/abs/2403.11844](https://arxiv.org/abs/2403.11844)

    本文研究了在非凸设置中，通过表征与最优对偶变量相关的Lagrange最小化器的约束违反来弥合实践与理论之间的差距。

    

    随着机器学习系统的广泛应用，限制它们行为的需求变得日益明显。最近的进展表明，对满足鲁棒性、安全性和公平性要求的模型的开发已经引起了广泛关注。这些要求可以通过制定受限制的学习问题并通过对偶上升算法来解决，以实现泛化保证。然而，即使在非凸设置中，这些算法收敛于目标值，也无法保证其结果是可行的。为了做到这一点，需要在所有迭代上进行随机化，这在任何现代应用中几乎是不切实际的。尽管如此，实践中观察到的最终迭代表现良好。在这项工作中，我们通过表征与最优对偶变量相关的Lagrange最小化器的约束违反，尽管缺乏凸性，来解决实践与理论之间的差距。

    arXiv:2403.11844v1 Announce Type: new  Abstract: With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent. This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and fairness requirements. These requirements can be imposed (with generalization guarantees) by formulating constrained learning problems that can then be tackled by dual ascent algorithms. Yet, though these algorithms converge in objective value, even in non-convex settings, they cannot guarantee that their outcome is feasible. Doing so requires randomizing over all iterates, which is impractical in virtually any modern applications. Still, final iterates have been observed to perform well in practice. In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity. To do this, we le
    
[^41]: 模糊粗糙Choquet距离用于分类

    Fuzzy Rough Choquet Distances for Classification

    [https://arxiv.org/abs/2403.11843](https://arxiv.org/abs/2403.11843)

    本文提出了一种基于模糊粗糙集合度量的新型Choquet距离，用于捕捉数据中的非线性关系，使得距离度量更加灵活与准确。

    

    本文介绍了一种基于模糊粗糙集合度量的新型Choquet距离。所提出的距离度量结合了从模糊粗糙集理论中获得的属性信息和Choquet积分的灵活性。这种方法旨在适应性地捕捉数据中的非线性关系，认识到条件属性与决策属性之间的相互作用，并实现更灵活与准确的距离度量。我们探讨了它在机器学习中的应用，特别强调基于距离的分类方法（例如k最近邻）。本文研究了两种基于正区域的模糊粗糙集度量，并探讨了两种根据模糊粗糙集理论导出的用于适用Choquet积分的单调化程序，以及它们之间的差异。

    arXiv:2403.11843v1 Announce Type: cross  Abstract: This paper introduces a novel Choquet distance using fuzzy rough set based measures. The proposed distance measure combines the attribute information received from fuzzy rough set theory with the flexibility of the Choquet integral. This approach is designed to adeptly capture non-linear relationships within the data, acknowledging the interplay of the conditional attributes towards the decision attribute and resulting in a more flexible and accurate distance. We explore its application in the context of machine learning, with a specific emphasis on distance-based classification approaches (e.g. k-nearest neighbours). The paper examines two fuzzy rough set based measures that are based on the positive region. Moreover, we explore two procedures for monotonizing the measures derived from fuzzy rough set theory, making them suitable for use with the Choquet integral, and investigate their differences.
    
[^42]: 基于中介因素的悲观因果强化学习用于混杂的离线数据

    Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data

    [https://arxiv.org/abs/2403.11841](https://arxiv.org/abs/2403.11841)

    提出了一种新的策略学习算法，PESCAL，利用基于前门标准的中介变量消除混杂偏差，并采用悲观原则处理候选策略引起的分布变化。

    

    在现实场景中，由随机实验收集的数据集往往受到时间和预算限制而规模有限。因此，利用大规模的观测数据集成为实现高质量策略学习更具吸引力的选择。然而，大多数现有的离线强化学习（RL）方法依赖于两个关键假设-- 非混杂性和正性-- 这两个假设在观测数据环境中经常不成立。鉴于这些挑战，我们提出了一种新颖的策略学习算法，称为悲观因果学习（PESCAL）。我们利用基于前门标准的中介变量来消除混杂偏差；此外，我们采用悲观原则来解决由候选策略引起的动作分布与生成观测数据的行为策略之间的分布变化。我们的关键观察是，通过融合辅助变量

    arXiv:2403.11841v1 Announce Type: cross  Abstract: In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget. As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning. However, most existing offline reinforcement learning (RL) methods depend on two key assumptions--unconfoundedness and positivity--which frequently do not hold in observational data contexts. Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data. Our key observation is that, by incorporating auxiliary variable
    
[^43]: 将多标准比较作为推动知识引导机器学习的方法

    Multi-Criteria Comparison as a Method of Advancing Knowledge-Guided Machine Learning

    [https://arxiv.org/abs/2403.11840](https://arxiv.org/abs/2403.11840)

    提出了一种通用的模型评估方法，该方法可以比较不同类型和结构的候选模型在多个科学、理论和实践标准上的表现。

    

    本文描述了一种通用的模型评估方法，可以适应评估AI/ML模型在包括核心科学原则和更实际结果在内的多个标准上。该方法源于心理学和决策科学中的预测竞赛，通过对多个不同类型和结构的候选模型在多个科学、理论和实践标准上的评估，使用计算社会选择领域的投票规则来评估标准得分的序数排名，从而比较多样化措施和类型的模型在整体评估中的效果。还讨论了额外的优势和应用。

    arXiv:2403.11840v1 Announce Type: new  Abstract: This paper describes a generalizable model evaluation method that can be adapted to evaluate AI/ML models across multiple criteria including core scientific principles and more practical outcomes. Emerging from prediction competitions in Psychology and Decision Science, the method evaluates a group of candidate models of varying type and structure across multiple scientific, theoretic, and practical criteria. Ordinal ranking of criteria scores are evaluated using voting rules from the field of computational social choice and allow the comparison of divergent measures and types of models in a holistic evaluation. Additional advantages and applications are discussed.
    
[^44]: 探索上下文学习与构成概括之间的关系

    Towards Understanding the Relationship between In-context Learning and Compositional Generalization

    [https://arxiv.org/abs/2403.11834](https://arxiv.org/abs/2403.11834)

    强制模型进行上下文学习可能有助于促进神经网络模型的构成概括能力

    

    根据构成概括原则，复杂表达的含义可以理解为其部分含义及它们如何组合的函数。这一原则对于人类语言处理至关重要，同时，可以说对于面对超出分布数据的NLP模型也是重要的。然而，许多神经网络模型，包括Transformer，在构成概括方面表现不佳。本文假设强制模型进行上下文学习可以提供归纳偏见以促进构成概括。为了验证这一假设，我们在一个使普通学习非常困难的环境中训练了一个因果Transformer：我们向其提供不同排序的训练实例并洗牌实例标签。这相当于在数据集中训练模型解决所有可能的少样本学习问题。模型可以解决任务，然而，通过利用

    arXiv:2403.11834v1 Announce Type: new  Abstract: According to the principle of compositional generalization, the meaning of a complex expression can be understood as a function of the meaning of its parts and of how they are combined. This principle is crucial for human language processing and also, arguably, for NLP models in the face of out-of-distribution data. However, many neural network models, including Transformers, have been shown to struggle with compositional generalization. In this paper, we hypothesize that forcing models to in-context learn can provide an inductive bias to promote compositional generalization. To test this hypothesis, we train a causal Transformer in a setting that renders ordinary learning very difficult: we present it with different orderings of the training instance and shuffle instance labels. This corresponds to training the model on all possible few-shot learning problems attainable from the dataset. The model can solve the task, however, by utilizi
    
[^45]: SSCAE -- 语义、句法和上下文感知自然语言对抗样本生成器

    SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator

    [https://arxiv.org/abs/2403.11833](https://arxiv.org/abs/2403.11833)

    本文介绍了一种名为SSCAE的对抗攻击模型，用于生成语义、句法和上下文感知的自然语言对抗样本，提出了动态阈值和本地贪婪搜索以生成高质量对抗样本。

    

    机器学习模型容易受到恶意制作的对抗样本（AEs）的影响。用AEs来训练机器学习模型可以提高其对抗攻击的稳健性和稳定性。在自然语言处理（NLP）领域开发高质量AEs的模型比计算机视觉等领域要慢得多。本文介绍了一种名为SSCAE的实用和高效的对抗攻击模型，用于语义、句法和上下文感知自然语言AE生成器。SSCAE识别重要单词并使用掩码语言模型生成早期替换集。接着，使用两个著名的语言模型来评估初始集合的语义和句法特性。我们引入了（1）动态阈值来捕获更高效的扰动以及（2）本地贪婪搜索以生成高质量

    arXiv:2403.11833v1 Announce Type: new  Abstract: Machine learning models are vulnerable to maliciously crafted Adversarial Examples (AEs). Training a machine learning model with AEs improves its robustness and stability against adversarial attacks. It is essential to develop models that produce high-quality AEs. Developing such models has been much slower in natural language processing (NLP) than in areas such as computer vision. This paper introduces a practical and efficient adversarial attack model called SSCAE for \textbf{S}emantic, \textbf{S}yntactic, and \textbf{C}ontext-aware natural language \textbf{AE}s generator. SSCAE identifies important words and uses a masked language model to generate an early set of substitutions. Next, two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics. We introduce (1) a dynamic threshold to capture more efficient perturbations and (2) a local greedy search to generate high-qualit
    
[^46]: 具有距离估计的声音事件检测和定位

    Sound Event Detection and Localization with Distance Estimation

    [https://arxiv.org/abs/2403.11827](https://arxiv.org/abs/2403.11827)

    本文将声音事件检测和定位任务扩展为具有距离估计的3D SELD，探讨了两种集成距离估计的方法，并在Ambisonic和双耳版本的声音场景下进行了实验。

    

    声音事件检测和定位(SELD)是识别声音事件及其对应到达方向(DOA)的综合任务。尽管这一任务在近年来已得到广泛研究并具有许多应用，但它未能提供有关声源位置的完整信息。本文通过将任务扩展为具有距离估计的声音事件检测、定位(3D SELD)来克服这一问题。我们研究了两种集成距离估计在SELD核心中的方法 - 一种是多任务方法，通过单独模型输出来处理问题，另一种是通过将多ACCDOA方法扩展以包括距离信息而获得的单任务方法。我们对Ambisonic和双耳版本的STARSS23：Sony-TAU Realistic Spatial Soundscapes 2023开展了研究。此外，我们的研究涉及与距离估计部分相关的损失函数的实验。

    arXiv:2403.11827v1 Announce Type: cross  Abstract: Sound Event Detection and Localization (SELD) is a combined task of identifying sound events and their corresponding direction-of-arrival (DOA). While this task has numerous applications and has been extensively researched in recent years, it fails to provide full information about the sound source position. In this paper, we overcome this problem by extending the task to Sound Event Detection, Localization with Distance Estimation (3D SELD). We study two ways of integrating distance estimation within the SELD core - a multi-task approach, in which the problem is tackled by a separate model output, and a single-task approach obtained by extending the multi-ACCDOA method to include distance information. We investigate both methods for the Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial Soundscapes 2023. Moreover, our study involves experiments on the loss function related to the distance estimation part. Our resu
    
[^47]: CapsLorentzNet: 将受物理启发的特征与图卷积相整合

    CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution

    [https://arxiv.org/abs/2403.11826](https://arxiv.org/abs/2403.11826)

    引入胶囊层的架构修改，配合图神经网络，实现了将受物理启发的特征整合进分析，为高级对象标记提供了新思路。

    

    随着先进机器学习技术的出现，提升对象标记已经取得了显著进展。本文通过引入与各种图神经网络（GNN）架构兼容的新颖架构修改，进一步推动了这一领域的发展。我们的方法主张在标准GNN中替换传统解码块以集成胶囊层。这些胶囊是具有向量激活的神经元组。这些向量的方向表示被研究对象的重要属性，其大小表征被研究对象是否属于由胶囊代表的类别。此外，胶囊网络结合了一种通过重构机制进行正则化，促进了专家设计的高级特征无缝融入分析。我们已经研究了我们的架构与LorentzNet架构在夸克胶子方面的实用性。

    arXiv:2403.11826v1 Announce Type: cross  Abstract: With the advent of advanced machine learning techniques, boosted object tagging has witnessed significant progress. In this article, we take this field further by introducing novel architectural modifications compatible with a wide array of Graph Neural Network (GNN) architectures. Our approach advocates for integrating capsule layers, replacing the conventional decoding blocks in standard GNNs. These capsules are a group of neurons with vector activations. The orientation of these vectors represents important properties of the objects under study, with their magnitude characterizing whether the object under study belongs to the class represented by the capsule. Moreover, capsule networks incorporate a regularization by reconstruction mechanism, facilitating the seamless integration of expert-designed high-level features into the analysis. We have studied the usefulness of our architecture with the LorentzNet architecture for quark-glu
    
[^48]: 低成本隐私感知去中心化学习

    Low-Cost Privacy-Aware Decentralized Learning

    [https://arxiv.org/abs/2403.11795](https://arxiv.org/abs/2403.11795)

    ZIP-DL是一种低成本的隐私感知去中心化学习算法，通过向每个模型更新添加相关噪声，在保护隐私的同时实现了较高的模型准确性，具有较好的收敛速度和隐私保证。

    

    本文介绍了一种新颖的隐私感知去中心化学习（DL）算法ZIP-DL，该算法依赖于在模型训练过程中向每个模型更新添加相关噪声。这种技术确保了由于其相关性，在聚合过程中添加的噪声几乎相互抵消，从而最小化对模型准确性的影响。此外，ZIP-DL不需要多次通信轮进行噪声抵消，解决了隐私保护与通信开销之间的常见权衡。我们为收敛速度和隐私保证提供了理论保证，从而使ZIP-DL可应用于实际场景。我们的广泛实验研究表明，ZIP-DL在易受攻击性和准确性之间取得了最佳权衡。特别是，与基线DL相比，ZIP-DL（i）将可追踪攻击的有效性降低了多达52个点，（ii）准确性提高了高达37个百分点。

    arXiv:2403.11795v1 Announce Type: new  Abstract: This paper introduces ZIP-DL, a novel privacy-aware decentralized learning (DL) algorithm that relies on adding correlated noise to each model update during the model training process. This technique ensures that the added noise almost neutralizes itself during the aggregation process due to its correlation, thus minimizing the impact on model accuracy. In addition, ZIP-DL does not require multiple communication rounds for noise cancellation, addressing the common trade-off between privacy protection and communication overhead. We provide theoretical guarantees for both convergence speed and privacy guarantees, thereby making ZIP-DL applicable to practical scenarios. Our extensive experimental study shows that ZIP-DL achieves the best trade-off between vulnerability and accuracy. In particular, ZIP-DL (i) reduces the effectiveness of a linkability attack by up to 52 points compared to baseline DL, and (ii) achieves up to 37 more accuracy
    
[^49]: 使用高斯过程从偏好和选择中学习的教程

    A tutorial on learning from preferences and choices with Gaussian Processes

    [https://arxiv.org/abs/2403.11782](https://arxiv.org/abs/2403.11782)

    提供了一个使用高斯过程进行偏好学习的框架，能够将理性原则融入学习过程，涵盖了多种偏好学习模型。

    

    偏好建模位于经济学、决策理论、机器学习和统计学的交叉点。通过理解个体的偏好及其选择方式，我们可以构建更接近他们期望的产品，为跨领域的更高效、个性化应用铺平道路。此教程的目标是提供一个连贯、全面的偏好学习框架，使用高斯过程演示如何将理性原则（来自经济学和决策理论）无缝地纳入学习过程中。通过合适地定制似然函数，这一框架使得能够构建涵盖随机效用模型、辨识限制和对象和标签偏好的多重冲突效用情景的偏好学习模型。

    arXiv:2403.11782v1 Announce Type: new  Abstract: Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals' preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducin
    
[^50]: Prompt-Singer: 带自然语言提示的可控唱歌声音合成

    Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt

    [https://arxiv.org/abs/2403.11780](https://arxiv.org/abs/2403.11780)

    提出了Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的唱歌声音合成方法，采用了基于解码器的变压器模型架构和范围旋律解耦的音高表示方法。

    

    近期的唱歌声音合成(SVS)方法取得了显著的音频质量和自然度，然而它们缺乏显式控制合成唱歌风格属性的能力。我们提出Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的SVS方法。我们采用基于仅解码器的变压器模型架构，具有多尺度层次结构，并设计了一个分离音高表示的范围旋律解耦的方法，从而实现了基于文本的音域控制同时保持了旋律准确性。此外，我们探索了各种实验设置，包括不同类型的文本表示，文本编码器微调，以及引入语音数据以减轻数据稀缺性，旨在促进进一步研究。实验证明，我们的模型具有良好的控制能力和音频质量。音频示例可访问 http://prompt-singer.

    arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
    
[^51]: 在通信平台中开发实时深度伪造音频检测系统的研究

    Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms

    [https://arxiv.org/abs/2403.11778](https://arxiv.org/abs/2403.11778)

    该研究评估了在实时通信平台中使用静态深度伪造音频检测模型的可行性，并通过开发两个基于Resnet和LCNN架构的模型，实现了在通信平台中的实时深度伪造音频检测，为提升模型性能和确保通信安全提供了策略和框架。

    

    深度伪造音频在通信平台中构成不断上升的威胁，因此需要实时检测以确保音频流的完整性。本研究评估了在实时通信平台中使用静态深度伪造音频检测模型的可行性，开发了可在不同平台上运行的可执行软件，实现了实时执行。基于Resnet和LCNN架构的两个深度伪造音频检测模型使用ASVspoof 2019数据集进行实现，相较于ASVspoof 2019挑战基准取得了优越性能。该研究提出了增强这些模型的策略和框架，为在通信平台中实现实时深度伪造音频检测铺平了道路。这项工作促进了音频流安全性的发展，确保在动态的实时通信场景中具有强大的检测能力。

    arXiv:2403.11778v1 Announce Type: cross  Abstract: Deepfake audio poses a rising threat in communication platforms, necessitating real-time detection for audio stream integrity. Unlike traditional non-real-time approaches, this study assesses the viability of employing static deepfake audio detection models in real-time communication platforms. An executable software is developed for cross-platform compatibility, enabling real-time execution. Two deepfake audio detection models based on Resnet and LCNN architectures are implemented using the ASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof 2019 challenge baselines. The study proposes strategies and frameworks for enhancing these models, paving the way for real-time deepfake audio detection in communication platforms. This work contributes to the advancement of audio stream security, ensuring robust detection capabilities in dynamic, real-time communication scenarios.
    
[^52]: S-JEPA：通过动态空间注意力实现无缝跨数据集转移

    S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention

    [https://arxiv.org/abs/2403.11772](https://arxiv.org/abs/2403.11772)

    本文介绍了一项关于使用联合嵌入预测架构（JEPAs）实现脑电信号无缝跨数据集转移的探索性研究，提出了Signal-JEPA用于表示脑电记录，并展示了其在精确下游分类中的重要性。

    

    受脑电信号处理中无缝跨数据集转移挑战的启发，本文介绍了关于使用联合嵌入预测架构（JEPAs）的探索性研究。近年来，自监督学习已经成为各个领域中迁移学习的一个有前途的方法。然而，它在脑电信号中的应用仍然是一个未被充分探索的领域。本文介绍了用于表示脑电记录的Signal-JEPA，其中包括一种新颖的领域特定空间块掩蔽策略和三种新颖的用于下游分类的架构。该研究在一个54个受试者数据集上进行，模型的下游性能在三种不同的BCI范式上进行了评估：运动想象、ERP和SSVEP。我们的研究为JEPAs在脑电信号编码中的潜力提供了初步证据。值得注意的是，我们的结果突出了空间滤波对准确下游分类的重要性。

    arXiv:2403.11772v1 Announce Type: cross  Abstract: Motivated by the challenge of seamless cross-dataset transfer in EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs). In recent years, self-supervised learning has emerged as a promising approach for transfer learning in various domains. However, its application to EEG signals remains largely unexplored. In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding. Notably, our results highlight the importance of spatial filtering for accurate downstream classific
    
[^53]: 音频视觉情感模仿强度估计的高效特征提取和延迟融合策略

    Efficient Feature Extraction and Late Fusion Strategy for Audiovisual Emotional Mimicry Intensity Estimation

    [https://arxiv.org/abs/2403.11757](https://arxiv.org/abs/2403.11757)

    本文提出了用于音频视觉情感模仿强度估计的高效特征提取和延迟融合策略

    

    在本文中，我们提出了解情感模仿强度（EMI）估计挑战的解决方案，该挑战是第六届面向野外情感行为分析（ABAW）竞赛的一部分。EMI估计挑战任务旨在通过从一组预定义的情绪类别（即，“崇拜”，“娱乐”，“决心”，“共情性疼痛”，“兴奋”和“喜悦”）中评估它们来评估种子视频的情感强度。

    arXiv:2403.11757v1 Announce Type: cross  Abstract: In this paper, we present the solution to the Emotional Mimicry Intensity (EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis in-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to evaluate the emotional intensity of seed videos by assessing them from a set of predefined emotion categories (i.e., "Admiration," "Amusement," "Determination," "Empathic Pain," "Excitement," and "Joy").
    
[^54]: 使用元提示自动化LLMs进行零样本视觉识别

    Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs

    [https://arxiv.org/abs/2403.11755](https://arxiv.org/abs/2403.11755)

    提出了Meta-Prompting for Visual Recognition (MPVR)方法，通过仅需少量信息即可自动化零样本识别中的提示生成过程。

    

    大型语言模型（LLM）生成的类别特定提示的提示集成已经被证明是增强视觉语言模型（VLMs）零样本识别能力的有效方法。为了获得这些类别特定提示，现有方法依赖于手工为LLMs设计提示，以生成下游任务的VLM提示。然而，这需要手动编写这些任务特定提示，而且它们可能无法涵盖与感兴趣类别相关的各种视觉概念和任务特定风格。为了有效地将人类排除在循环之外，并完全自动化零样本识别的提示生成过程，我们提出了用于视觉识别的元提示（MPVR）。仅以目标任务的少量自然语言描述形式以及一系列相关类别标签作为输入，MPVR自动产生一个多样化的类别提示集。

    arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
    
[^55]: PARMESAN: 用于密集预测任务的无参数内存搜索与转导

    PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks

    [https://arxiv.org/abs/2403.11743](https://arxiv.org/abs/2403.11743)

    通过引入转导的概念，提出了PARMESAN，一种用于解决密集预测任务的无参数内存搜索和转导方法，实现了灵活性和无需连续训练的学习。

    

    在这项工作中，我们通过转导推理来解决深度学习中的灵活性问题。我们提出了PARMESAN（无参数内存搜索与转导），这是一种可扩展的转导方法，利用内存模块来解决密集预测任务。在推断过程中，内存中的隐藏表示被搜索以找到相应的示例。与其他方法不同，PARMESAN通过修改内存内容学习，而无需进行任何连续训练或微调可学习参数。我们的方法与常用的神经结构兼容。

    arXiv:2403.11743v1 Announce Type: new  Abstract: In this work we address flexibility in deep learning by means of transductive reasoning. For adaptation to new tasks or new data, existing methods typically involve tuning of learnable parameters or even complete re-training from scratch, rendering such approaches unflexible in practice. We argue that the notion of separating computation from memory by the means of transduction can act as a stepping stone for solving these issues. We therefore propose PARMESAN (parameter-free memory search and transduction), a scalable transduction method which leverages a memory module for solving dense prediction tasks. At inference, hidden representations in memory are being searched to find corresponding examples. In contrast to other methods, PARMESAN learns without the requirement for any continuous training or fine-tuning of learnable parameters simply by modifying the memory content. Our method is compatible with commonly used neural architecture
    
[^56]: LSKNet：一种用于遥感的轻量级基础架构

    LSKNet: A Foundation Lightweight Backbone for Remote Sensing

    [https://arxiv.org/abs/2403.11735](https://arxiv.org/abs/2403.11735)

    LSKNet是一种轻量级的大型选择核网络骨干，能动态调整其较大的空间感受野，以更好地模拟遥感场景中各种对象的远程上下文。

    

    遥感图像由于其固有的复杂性对下游任务提出了独特的挑战。尽管已经有大量研究致力于遥感分类、目标检测和语义分割，但其中大多数研究都忽视了嵌入在遥感场景中的宝贵先验知识。这些先验知识可能会很有用，因为在没有参考足够长程上下文的情况下，遥感对象可能会被错误识别，而这可以因不同对象而异。本文考虑了这些先验知识，并提出了一种轻量级的大型选择核网络（LSKNet）骨干网络。LSKNet可以动态调整其较大的空间感受野，以更好地模拟遥感场景中各种对象的远距离上下文。据我们所知，先前尚未在遥感图像中探索过大型和选择性核机制。我们的轻量级方法没有太多复杂性。

    arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
    
[^57]: 学习古典规划领域的通用策略：超越$C_2$

    Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$

    [https://arxiv.org/abs/2403.11734](https://arxiv.org/abs/2403.11734)

    该研究提出了一种参数化版本的关系GNNs，通过在$t$为无穷大时仅使用二次空间的嵌入来近似$3$-GNNs，对于较低的$t$值，通过交换较少的消息实现弱的近似，同时通常产生了几个规划领域中所需的$C_3$特征。

    

    基于GNN的方法用于学习跨规划领域的通用策略受到$C_2$表达能力的限制，即一阶逻辑只能包含两个变量和计数。这种限制可以通过转向$k$-GNNs，其中$k=3$，其中物体嵌入被三元组嵌入所替换，来克服。然而，尽管$3$-GNNs具有$C_3$的表达能力，但不同于受限于$C_2$的$1$-和$2$-GNNs，它们需要四次时间进行消息交换和三次空间进行嵌入，使它们变得不切实际。在这项工作中，我们引入了一个参数化版本的关系GNNs。当$t$为无穷大时，R-GNN[$t$]仅使用二次空间的嵌入来近似$3$-GNNs。对于较低的$t$值，例如$t=1$和$t=2$，R-GNN[$t$]通过交换较少的消息实现了更弱的近似，但有趣的是，通常产生了在几个规划领域中所需的$C_3$特征。此外，新的R-GNN[$t$] ar

    arXiv:2403.11734v1 Announce Type: new  Abstract: GNN-based approaches for learning general policies across planning domains are limited by the expressive power of $C_2$, namely; first-order logic with two variables and counting. This limitation can be overcomed by transitioning to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$- and $2$-GNNs that are confined to $C_2$, they require quartic time for message exchange and cubic space for embeddings, rendering them impractical. In this work, we introduce a parameterized version of relational GNNs. When $t$ is infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the $C_3$ features required in several planning domains. Furthermore, the new R-GNN[$t$] ar
    
[^58]: PITA: 物理信息轨迹自动编码器

    PITA: Physics-Informed Trajectory Autoencoder

    [https://arxiv.org/abs/2403.11728](https://arxiv.org/abs/2403.11728)

    提出了物理信息轨迹自动编码器（PITA）架构，通过将物理动力学模型纳入损失函数，使得轨迹更加平滑且具有物理合理性。

    

    验证在安全关键应用中的机器人系统需要在许多场景中进行测试，包括罕见的边缘情况，这些情况不太可能发生，需要通过在仿真环境中测试来补充现实世界的测试。生成模型可用于通过在学习的潜在空间中进行采样，将真实世界数据集与生成数据相结合，从而生成边缘情况场景。自动编码器可以通过学习从较低维度的中间表示重建输入数据来学习特定领域的潜在表示。然而，生成的轨迹不一定是物理上合理的，而通常包含输入轨迹中没有的噪声。为解决此问题，我们提出了新颖的物理信息轨迹自动编码器（PITA）架构，将物理动力学模型纳入自动编码器的损失函数中。这导致平滑轨迹，不仅重建了输入

    arXiv:2403.11728v1 Announce Type: new  Abstract: Validating robotic systems in safety-critical appli-cations requires testing in many scenarios including rare edgecases that are unlikely to occur, requiring to complement real-world testing with testing in simulation. Generative models canbe used to augment real-world datasets with generated data toproduce edge case scenarios by sampling in a learned latentspace. Autoencoders can learn said latent representation for aspecific domain by learning to reconstruct the input data froma lower-dimensional intermediate representation. However, theresulting trajectories are not necessarily physically plausible, butinstead typically contain noise that is not present in the inputtrajectory. To resolve this issue, we propose the novel Physics-Informed Trajectory Autoencoder (PITA) architecture, whichincorporates a physical dynamics model into the loss functionof the autoencoder. This results in smooth trajectories that notonly reconstruct the input 
    
[^59]: 使用四元值神经网络和四元反向传播进行时间序列压缩

    Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation

    [https://arxiv.org/abs/2403.11722](https://arxiv.org/abs/2403.11722)

    提出了一种使用四元值神经网络和四元反向传播进行时间序列压缩的方法，在保留特征之间关系的同时在故障分类中展现出潜力。

    

    我们提出了一种新颖的四元数时间序列压缩方法，将长时间序列划分为数据段，提取这些块的最小值、最大值、均值和标准差作为代表性特征，并将它们封装在四元数中，得到一个四元数值时间序列。这个时间序列使用四元数值神经网络层进行处理，我们旨在通过使用哈密顿积来保留这些特征之间的关系。为了训练这个四元数神经网络，我们推导出使用GHR微积分的四元数反向传播，这对于四元数空间中的有效乘积和链规则是必需的。此外，我们研究了推导更新规则与自动微分之间的联系。我们将我们提出的压缩方法应用于Tennessee Eastman数据集，在两个设置中使用压缩数据进行故障分类：一个完全监督的设置和另一个半监督的设置。

    arXiv:2403.11722v1 Announce Type: new  Abstract: We propose a novel quaternionic time-series compression methodology where we divide a long time-series into segments of data, extract the min, max, mean and standard deviation of these chunks as representative features and encapsulate them in a quaternion, yielding a quaternion valued time-series. This time-series is processed using quaternion valued neural network layers, where we aim to preserve the relation between these features through the usage of the Hamilton product. To train this quaternion neural network, we derive quaternion backpropagation employing the GHR calculus, which is required for a valid product and chain rule in quaternion space. Furthermore, we investigate the connection between the derived update rules and automatic differentiation. We apply our proposed compression method on the Tennessee Eastman Dataset, where we perform fault classification using the compressed data in two settings: a fully supervised one and i
    
[^60]: 通用文本条件音乐扩散模型的多源推断

    Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models

    [https://arxiv.org/abs/2403.11706](https://arxiv.org/abs/2403.11706)

    本文将多源扩散模型（MSDM）推广到任意时间域扩散模型，并引入文本嵌入条件，实现了不需要分离数据训练，可以参数化任意数量源，并实现丰富语义控制的音乐生成模型。

    

    多源扩散模型（MSDM）用于音乐生成任务：生成一组连贯的源，创建伴奏以及执行源分离。本文将MSDM推广到条件于文本嵌入的任意时间域扩散模型。这些模型不需要分离的数据，可以对任意数量的源进行参数化，并允许丰富的语义控制。我们提出了一种推理过程，可以实现源和伴奏的连贯生成。此外，我们调整了MSDM的Dirac分隔器以执行源分离。我们实验了在Slakh2100和MTG-Jamendo上训练的扩散模型，展示了有竞争力的性能。

    arXiv:2403.11706v1 Announce Type: cross  Abstract: Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings. These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive gene
    
[^61]: 在传递电子磁体中手征领域的粗化：一种机器学习力场方法

    Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach

    [https://arxiv.org/abs/2403.11705](https://arxiv.org/abs/2403.11705)

    提出了一种机器学习框架来模拟在三角格子中稳定手征磁性领域的复杂电子介导的自旋-自旋相互作用，研究了磁性领域在热淬后的粗化过程

    

    受挫传递磁体通常表现出复杂的非共线或非共面磁序，支持拓扑电子结构。一个典型例子是由三角格子上的电子自旋相互作用稳定的手征自旋序的异常量子霍尔态。我们提出了一种可扩展的机器学习（ML）框架，用于模拟在三角格子中稳定手征磁性领域的复杂的电子介导的自旋-自旋相互作用。利用ML力场模型进行的大规模动力学模拟研究了热淬后手征领域的粗化。虽然手征相由破缺的$Z_2$ Ising型对称性描述，但我们发现特征尺寸

    arXiv:2403.11705v1 Announce Type: cross  Abstract: Frustrated itinerant magnets often exhibit complex noncollinear or noncoplanar magnetic orders which support topological electronic structures. A canonical example is the anomalous quantum Hall state with a chiral spin order stabilized by electron-spin interactions on a triangular lattice. While a long-range magnetic order cannot survive thermal fluctuations in two dimensions, the chiral order which results from the breaking of a discrete Ising symmetry persists even at finite temperatures. We present a scalable machine learning (ML) framework to model the complex electron-mediated spin-spin interactions that stabilize the chiral magnetic domains in a triangular lattice. Large-scale dynamical simulations, enabled by the ML force-field models, are performed to investigate the coarsening of chiral domains after a thermal quench. While the chiral phase is described by a broken $Z_2$ Ising-type symmetry, we find that the characteristic siz
    
[^62]: 光谱算法的泛化误差

    Generalization error of spectral algorithms

    [https://arxiv.org/abs/2403.11696](https://arxiv.org/abs/2403.11696)

    本研究考虑了使用光谱算法来训练核，推导出泛化误差函数，提供了完整的损失渐近行为，展示了损失在特定频谱尺度上的局部化。

    

    近期关注核方法泛化的渐近精确估计，源于神经网络及其相关核之间的类比。然而，以往的研究是通过核岭回归（KRR）推导出这些估计值，而神经网络通常是通过梯度下降（GD）进行训练。本研究考虑了使用由配置文件$h(\lambda)$指定的一系列“光谱算法”来训练核，其中包括KRR和GD作为特例。然后，我们推导出关于两种数据模型的学习配置文件$h(\lambda)$的泛化误差函数：高维高斯模型和低维平移不变模型。在对核和目标的频谱进行幂律假设的情况下，我们利用我们的框架来(i)为有噪和无噪观测提供完整的损失渐近行为；(ii)展示损失出现在某些频谱尺度上的局部化，

    arXiv:2403.11696v1 Announce Type: new  Abstract: The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of $\textit{spectral algorithms}$ specified by profile $h(\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. Under power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, givi
    
[^63]: 非光滑隐式微分：确定性和随机收敛速率

    Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates

    [https://arxiv.org/abs/2403.11687](https://arxiv.org/abs/2403.11687)

    在非光滑设置下，提出了用于计算具有内映射的外映射固定点的隐式导数的新方法NSID，并提供了确定性情况下迭代微分（ITD）和近似隐式微分（AID）的改进线性收敛速率。

    

    我们研究了有效计算参数化不可微收缩映射固定点导数的问题。这个问题在机器学习中有广泛的应用，包括超参数优化、元学习和数据污染攻击。我们分析了两种流行的方法：迭代微分（ITD）和近似隐式微分（AID）。在非光滑设置中的一个关键挑战是链规则不再成立。在Bolte等人（2022）最近的工作基础上，他们证明了不可微分ITD的线性收敛，我们提供了确定性情况下ITD和AID的改进线性收敛速率。我们进一步介绍了NSID，一种新的方法，用于在固定点被定义为只通过随机无偏估计器访问的外映射和内映射的组合时计算隐式导数。我们建立了该方法的收敛速率。

    arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of 
    
[^64]: Crystalformer：用于周期结构编码的无限连接注意力

    Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding

    [https://arxiv.org/abs/2403.11686](https://arxiv.org/abs/2403.11686)

    Crystalformer是一种用于晶体结构的Transformer-based编码器，利用无限连接注意力进行无限的原子间势求和，具有较低的参数需求。

    

    材料科学中的一个基本问题是从它们的晶体结构预测材料的物理性质。在预测分子性质等边缘领域，全连接注意力网络已被证明是成功的。然而，与这些有限原子排列不同，晶体结构是无限重复的，周期性的原子排列，其全连接注意力导致无限连接注意力。在这项工作中，我们展示了这种无限连接注意力可以导致一个可计算的公式形式，解释为神经势求和，在一个深度学习特征空间中执行无限的原子间势求和。然后，我们提出了一种简单而有效的基于Transformer的晶体结构编码器架构，称为Crystalformer。与现有的基于Transformer的模型相比，所提出的模型仅需要29.4%的参数数量，wit

    arXiv:2403.11686v1 Announce Type: new  Abstract: Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space. We then propose a simple yet effective Transformer-based encoder architecture for crystal structures called Crystalformer. Compared to an existing Transformer-based model, the proposed model requires only 29.4% of the number of parameters, wit
    
[^65]: 探索基于3D感知的潜空间，以有效学习多个场景

    Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes

    [https://arxiv.org/abs/2403.11678](https://arxiv.org/abs/2403.11678)

    通过3D感知的潜空间和跨场景共享信息，实现了NeRFs有效学习大量语义相似场景，并显著降低了训练时间和内存消耗

    

    我们提出了一种方法，通过结合两种技术来改善每个场景所需的训练时间和内存成本，从而实现将NeRFs无缝扩展到学习大量语义相似的场景。首先，我们学习了一个3D感知的潜空间，在其中训练三平面场景表示，从而降低了学习场景的分辨率。此外，我们提出了一种跨场景共享通用信息的方式，从而减少了学习特定场景的模型复杂性。当训练1000个场景时，我们的方法将每个场景的内存成本降低了44%，时间成本降低了86%。我们的项目页面位于https://3da-ae.github.io。

    arXiv:2403.11678v1 Announce Type: cross  Abstract: We present a method enabling the scaling of NeRFs to learn a large number of semantically-similar scenes. We combine two techniques to improve the required training time and memory cost per scene. First, we learn a 3D-aware latent space in which we train Tri-Plane scene representations, hence reducing the resolution at which scenes are learned. Moreover, we present a way to share common information across scenes, hence allowing for a reduction of model complexity to learn a particular scene. Our method reduces effective per-scene memory costs by 44% and per-scene time costs by 86% when training 1000 scenes. Our project page can be found at https://3da-ae.github.io .
    
[^66]: HDLdebugger: 利用大语言模型简化HDL调试

    HDLdebugger: Streamlining HDL debugging with Large Language Models

    [https://arxiv.org/abs/2403.11671](https://arxiv.org/abs/2403.11671)

    提出了一个LLM辅助的HDL调试框架 HDLdebugger，通过逆向工程方法生成HDL调试数据，提供检索增强生成的搜索引擎以及检索增强LLM微调的方法，以简化HDL调试任务。

    

    在芯片设计领域，硬件描述语言（HDLs）发挥着至关重要的作用。然而，由于HDL的复杂语法和在线资源有限的问题，即使是经验丰富的工程师，调试HDL代码仍然是一项困难且耗时的任务。因此，迫切需要开发自动化HDL代码调试模型，以减轻硬件工程师的负担。尽管大语言模型（LLMs）在生成、完成和调试软件代码方面具有强大的能力，但它们在专门领域的HDL调试中的利用受到了限制，并且迄今为止尚未产生令人满意的结果。在本文中，我们提出了一个LLM辅助的HDL调试框架，即HDLdebugger，它包括通过逆向工程方法生成HDL调试数据，用于检索增强生成的搜索引擎，以及用于检索增强LLM微调的方法。

    arXiv:2403.11671v1 Announce Type: cross  Abstract: In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through 
    
[^67]: 基于扩散的环境感知轨迹预测

    Diffusion-Based Environment-Aware Trajectory Prediction

    [https://arxiv.org/abs/2403.11643](https://arxiv.org/abs/2403.11643)

    该论文提出了一种基于扩散的生成模型，用于多智能体轨迹预测，能够准确捕捉交通参与者与环境之间的复杂相互作用，优于多种传统方法，在预测准确性方面表现出色。

    

    预测交通参与者未来轨迹的能力对于自动驾驶汽车的安全和高效运行至关重要。本文提出了一种基于扩散的生成模型，用于多智能体轨迹预测。该模型能够捕捉交通参与者与环境之间的复杂相互作用，准确学习数据的多模态特性。通过在模型输出上引入微分运动约束，我们展示了我们的模型能够生成多样化且逼真的未来轨迹。通过使用一个感知交互引导信号，我们进一步证明了该模型可以适应预测不太合作代理的行为。

    arXiv:2403.11643v1 Announce Type: cross  Abstract: The ability to predict the future trajectories of traffic participants is crucial for the safe and efficient operation of autonomous vehicles. In this paper, a diffusion-based generative model for multi-agent trajectory prediction is proposed. The model is capable of capturing the complex interactions between traffic participants and the environment, accurately learning the multimodal nature of the data. The effectiveness of the approach is assessed on large-scale datasets of real-world traffic scenarios, showing that our model outperforms several well-established methods in terms of prediction accuracy. By the incorporation of differential motion constraints on the model output, we illustrate that our model is capable of generating a diverse set of realistic future trajectories. Through the use of an interaction-aware guidance signal, we further demonstrate that the model can be adapted to predict the behavior of less cooperative agen
    
[^68]: 引导基于时间背景知识生成可解释性反事实解释来进行预测性流程监控

    Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring

    [https://arxiv.org/abs/2403.11642](https://arxiv.org/abs/2403.11642)

    通过考虑运行时的时间约束，本研究通过调整基于遗传算法的技术，在预测性流程监控中引入了时间背景知识来生成反事实解释。

    

    反事实解释指出了修改输入实例以改变人工智能系统结果应该有什么不同。然而，在预测性流程监控领域处理反事实解释时，必须仔细考虑事件之间的控制流关系。确实，一个反事实不应违反活动之间的控制流关系（即时间背景知识）。在预测性流程监控的可解释性领域中，已经有一系列关于基于结果的预测的反事实解释的作品。然而，其中没有一个在生成这些反事实时考虑了时间背景知识的包含。在这项工作中，我们改进了基于遗传算法的最先进技术，以考虑一系列运行时的时间约束来生成反事实。我们假定这种时间背景知识

    arXiv:2403.11642v1 Announce Type: new  Abstract: Counterfactual explanations suggest what should be different in the input instance to change the outcome of an AI system. When dealing with counterfactual explanations in the field of Predictive Process Monitoring, however, control flow relationships among events have to be carefully considered. A counterfactual, indeed, should not violate control flow relationships among activities (temporal background knowledege). Within the field of Explainability in Predictive Process Monitoring, there have been a series of works regarding counterfactual explanations for outcome-based predictions. However, none of them consider the inclusion of temporal background knowledge when generating these counterfactuals. In this work, we adapt state-of-the-art techniques for counterfactual generation in the domain of XAI that are based on genetic algorithms to consider a series of temporal constraints at runtime. We assume that this temporal background knowle
    
[^69]: 强化学习中未来奖励先知的价值

    The Value of Reward Lookahead in Reinforcement Learning

    [https://arxiv.org/abs/2403.11637](https://arxiv.org/abs/2403.11637)

    分析了在强化学习中利用部分未来奖励先知的价值，通过竞争性分析得出了最坏情况下奖励期望的精确比率。

    

    在强化学习（RL）中，代理们与不断变化的环境进行顺序交互，旨在最大化获得的奖励。通常情况下，奖励仅在行动后被观察到，因此目标是最大化预期累积奖励。然而，在许多实际场景中，奖励信息是提前观察到的 -- 交易前观察到价格；了解部分附近交通信息；经常在互动之前为代理分配目标。在这项工作中，我们旨在通过竞争性分析的视角，定量分析这种未来奖励信息的价值。特别地，我们测量了标准RL代理的价值与具有部分未来奖励先知的代理之间的比率。我们刻画了最坏情况下的奖励分布，并推导出最坏情况下奖励期望的精确比率。令人惊讶的是，结果比率与离线RL和r中已知的数量有关。

    arXiv:2403.11637v1 Announce Type: new  Abstract: In reinforcement learning (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards. Usually, rewards are observed only after acting, and so the goal is to maximize the expected cumulative reward. Yet, in many practical settings, reward information is observed in advance -- prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction. In this work, we aim to quantifiably analyze the value of such future reward information through the lens of competitive analysis. In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead. We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations. Surprisingly, the resulting ratios relate to known quantities in offline RL and r
    
[^70]: 双通道多重图神经网络用于推荐

    Dual-Channel Multiplex Graph Neural Networks for Recommendation

    [https://arxiv.org/abs/2403.11624](https://arxiv.org/abs/2403.11624)

    该研究提出了一种名为双通道多重图神经网络（DCMGNN）的新型推荐框架，能够有效解决现有推荐方法中存在的多通路关系行为模式建模和对目标关系影响忽略的问题。

    

    高效的推荐系统在准确捕捉反映个人偏好的用户和项目属性方面发挥着至关重要的作用。一些现有的推荐技术已经开始将重点转向在真实世界的推荐场景中对用户和项目之间的各种类型交互关系进行建模，例如在线购物平台上的点击、标记收藏和购买。然而，这些方法仍然面临两个重要的缺点：(1) 不足的建模和利用用户和项目之间多通路关系形成的各种行为模式对表示学习的影响，以及(2) 忽略了行为模式中不同关系对推荐系统场景中目标关系的影响。在本研究中，我们介绍了一种新颖的推荐框架，即双通道多重图神经网络（DCMGNN），该框架解决了上述挑战。

    arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
    
[^71]: 在智能物联网系统中针对网络的公平分布式合作赌博学习（技术报告）

    Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report)

    [https://arxiv.org/abs/2403.11603](https://arxiv.org/abs/2403.11603)

    本文提出了针对智能物联网系统的多人合作多臂老虎机模型，设计了分布式合作赌博算法DC-ULCB，能够在最大化数据速率的同时保持选择的公平性，通过分析和验证，证明在奖励和公平性方面优于现有算法。

    

    在智能物联网（IoT）系统中，网络内的边缘服务器与其邻居交换信息并从传感器收集数据以完成交付的任务。本文提出了一个多人多臂老虎机模型，旨在为智能IoT系统的数据收集提供便利，并纳入公平考虑。在我们的模型中，我们建立了一个有效的通信协议，帮助服务器与其邻居合作。然后我们设计了一个分布式合作赌博算法，DC-ULCB，使服务器能够协作选择传感器，以最大化数据速率同时保持选择的公平性。我们对DC-ULCB的奖励遗憾和公平遗憾进行了分析，并证明两种遗憾均具有对数实例相关的上界。此外，通过大量的模拟实验，我们验证了DC-ULCB在最大化奖励和确保公平性方面优于现有算法。

    arXiv:2403.11603v1 Announce Type: cross  Abstract: In intelligent Internet of Things (IoT) systems, edge servers within a network exchange information with their neighbors and collect data from sensors to complete delivered tasks. In this paper, we propose a multiplayer multi-armed bandit model for intelligent IoT systems to facilitate data collection and incorporate fairness considerations. In our model, we establish an effective communication protocol that helps servers cooperate with their neighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB, enabling servers to collaboratively select sensors to maximize data rates while maintaining fairness in their choices. We conduct an analysis of the reward regret and fairness regret of DC-ULCB, and prove that both regrets have logarithmic instance-dependent upper bounds. Additionally, through extensive simulations, we validate that DC-ULCB outperforms existing algorithms in maximizing reward and ensuring fairness.
    
[^72]: 一种物理信息神经网络方法用于近似一般类别的刚性ODE系统的慢不变流形

    A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs

    [https://arxiv.org/abs/2403.11591](https://arxiv.org/abs/2403.11591)

    提出了一种物理信息神经网络方法，用于发现快/慢动力学ODE系统的慢不变流形，能够同时分解矢量场为快慢组分并以闭合形式提供下层SIM的泛函。

    

    我们提出了一种物理信息神经网络（PINN）方法，用于发现快/慢动力学ODE系统的慢不变流形（SIMs）。与其他利用简单回归构建降阶黑盒代理模型的机器学习（ML）方法不同，或者需要对快慢变量具有先验知识的方法不同，我们的方法同时将矢量场分解为快慢组分，并以闭合形式提供下层SIM的泛函。通过找到状态变量到快慢变量的转换来实现分解，从而能够推导出显式的、以快变量为基础的SIM泛函。后者通过使用具有符号微分的单层前向神经网络在几何奇异摄动理论（GSPT）中解决与不变性方程相对应的PDE而获得。

    arXiv:2403.11591v1 Announce Type: cross  Abstract: We present a physics-informed neural network (PINN) approach for the discovery of slow invariant manifolds (SIMs), for the most general class of fast/slow dynamical systems of ODEs. In contrast to other machine learning (ML) approaches that construct reduced order black box surrogate models using simple regression, and/or require a priori knowledge of the fast and slow variables, our approach, simultaneously decomposes the vector field into fast and slow components and provides a functional of the underlying SIM in a closed form. The decomposition is achieved by finding a transformation of the state variables to the fast and slow ones, which enables the derivation of an explicit, in terms of fast variables, SIM functional. The latter is obtained by solving a PDE corresponding to the invariance equation within the Geometric Singular Perturbation Theory (GSPT) using a single-layer feedforward neural network with symbolic differentiation.
    
[^73]: Linguacodus：一种在机器学习流水线中进行变革性代码生成的协同框架

    Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines

    [https://arxiv.org/abs/2403.11585](https://arxiv.org/abs/2403.11585)

    Linguacodus是一种创新框架，通过部署动态流水线和精细调整的大型语言模型，实现了将自然语言任务描述转换为代码的自动化过程，极大地推进了机器学习应用的发展。

    

    在不断发展的机器学习领域中，将自然语言描述无缝转化为可执行代码仍然是一个巨大的挑战。本文介绍了Linguacodus，这是一个创新性框架，旨在通过部署一个动态流水线，通过高级数据塑形指令，将自然语言任务描述迭代地转换为代码来应对这一挑战。Linguacodus的核心是一个经过精细调整的大型语言模型（LLM），能够评估各种问题的多样解决方案，并为特定任务选择最合适的解决方案。本文详细介绍了精细调整过程，并阐明了如何将自然语言描述转化为功能性代码。Linguacodus代表了自动化代码生成的重大飞跃，有效地弥合了任务描述和可执行代码之间的差距。它对推进跨不同领域的机器学习应用具有巨大潜力。

    arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
    
[^74]: 离线多任务表示学习用于强化学习

    Offline Multitask Representation Learning for Reinforcement Learning

    [https://arxiv.org/abs/2403.11574](https://arxiv.org/abs/2403.11574)

    通过研究离线多任务低秩RL，提出了一种名为MORL的新算法，证明了在强化学习中应用学习到的表示的优势。

    

    我们研究了强化学习中的离线多任务表示学习，其中学习者被提供来自共享通用表示的不同任务的离线数据集，并被要求学习共享表示。我们从理论上对离线多任务低秩RL进行了研究，并提出了一种名为MORL的新算法，用于离线多任务表示学习。此外，我们在奖励免费、离线和在线场景中研究了下游RL，其中向代理引入了一个新任务，该任务与上游离线任务共享相同的表示。我们的理论结果表明，使用从上游离线任务中学到的表示的好处，而不是直接学习低秩模型的表示。

    arXiv:2403.11574v1 Announce Type: new  Abstract: We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.
    
[^75]: 基于去中心化随机次梯度法的非平滑非凸优化问题

    Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization

    [https://arxiv.org/abs/2403.11565](https://arxiv.org/abs/2403.11565)

    该论文介绍了一种名为DSM的统一框架，用于分析去中心化随机次梯度方法的全局收敛性，证明了在温和条件下的全局收敛性，并展示其涵盖了各种现有高效的去中心化次梯度方法。

    

    在这篇论文中，我们关注具有非凸和非平滑目标函数的去中心化优化问题，特别是关注非平滑神经网络的去中心化训练。我们提出了一个统一的框架，称为DSM，用于分析去中心化随机次梯度法的全局收敛性。我们证明了在温和条件下，我们提出的框架的全局收敛性，通过建立生成序列渐近逼近其关联微分包含的轨迹。此外，我们证明了我们提出的框架涵盖了各种现有高效的去中心化次梯度方法，包括去中心化随机次梯度下降（DSGD），具有梯度跟踪技术的DSGD（DSGD-T）和带动量的DSGD（DSGDm）。此外，我们引入SignSGD，采用符号映射来正则化DSGDm中的更新方向，并表明它被包含在我们的提议中。

    arXiv:2403.11565v1 Announce Type: cross  Abstract: In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks. We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods. We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion. Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our propos
    
[^76]: 推进神经形态计算：利用脑编码单元和基本编码单元的混合信号设计技术

    Advancing Neuromorphic Computing: Mixed-Signal Design Techniques Leveraging Brain Code Units and Fundamental Code Units

    [https://arxiv.org/abs/2403.11563](https://arxiv.org/abs/2403.11563)

    该论文介绍了一种创新性的数字神经形态架构，通过混合信号设计方法将脑编码单元（BCU）和基本编码单元（FCU）集成在一起，提升神经形态系统的计算效率、准确性和适应性。

    

    本文介绍了一种开创性的数字神经形态架构，创新地利用了混合信号设计方法将脑编码单元（Brain Code Unit，BCU）和基本编码单元（Fundamental Code Unit，FCU）集成在一起。利用开源数据集和最新的材料科学进展，我们的研究着重于提升神经形态系统的计算效率、准确性和适应性。我们方法的核心在于将数字系统的精度和可扩展性与模拟处理的稳健性和能量效率相结合。通过实验，我们展示了我们的系统在各个指标上的有效性。BCU实现了88.0%的准确率和20.0 GOP/s/W的功耗效率，而FCU记录了86.5%的准确率和18.5 GOP/s/W的功耗效率。我们的混合信号设计方法显著改善了延迟和吞吐量，实现了低至0.75毫秒的延迟和高达213的吞吐量。

    arXiv:2403.11563v1 Announce Type: cross  Abstract: This paper introduces a groundbreaking digital neuromorphic architecture that innovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU) using mixedsignal design methodologies. Leveraging open-source datasets and the latest advances in materials science, our research focuses on enhancing the computational efficiency, accuracy, and adaptability of neuromorphic systems. The core of our approach lies in harmonizing the precision and scalability of digital systems with the robustness and energy efficiency of analog processing. Through experimentation, we demonstrate the effectiveness of our system across various metrics. The BCU achieved an accuracy of 88.0% and a power efficiency of 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power efficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly improved latency and throughput, achieving a latency as low as 0.75 ms and throughput up to 213 
    
[^77]: 独立函数逼近下的强化学习与马尔可夫博弈：在局部访问模型下改进的样本复杂度界限

    RL en Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model

    [https://arxiv.org/abs/2403.11544](https://arxiv.org/abs/2403.11544)

    在局部访问模型下，通过引入Lin-Confident-FTRL算法，可以学习到具有更优精度界限和更好扩展性的均值平衡算法。

    

    在一般和Markov博弈中有效地学习具有大状态和动作空间的均值平衡，同时克服多方代理的困境是一个具有挑战性的问题。最近的研究尝试通过使用独立线性函数类来逼近每个智能体的边际$Q$值来解决这个问题。然而，在这样一个框架下现有的样本复杂度界限对所需精度$\varepsilon$或动作空间具有次优依赖性。在本工作中，我们引入了一种新算法，Lin-Confident-FTRL，用于学习具有局部对模拟器的访问权限的粗粒度相关均衡（CCE），即可以与访问状态的基础环境进行交互。在对状态空间大小进行对数相关性的同时，Lin-Confident-FTRL学习$\epsilon$-CCE，并获得具有备份的最佳精度界限$O(\epsilon^{-2})，同时消除了对动作空间的线性依存关系，同时以多项式方式扩展

    arXiv:2403.11544v1 Announce Type: new  Abstract: Efficiently learning equilibria with large state and action spaces in general-sum Markov games while overcoming the curse of multi-agency is a challenging problem. Recent works have attempted to solve this problem by employing independent linear function classes to approximate the marginal $Q$-value for each agent. However, existing sample complexity bounds under such a framework have a suboptimal dependency on the desired accuracy $\varepsilon$ or the action space. In this work, we introduce a new algorithm, Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local access to the simulator, i.e., one can interact with the underlying environment on the visited states. Up to a logarithmic dependence on the size of the state space, Lin-Confident-FTRL learns $\epsilon$-CCE with a provable optimal accuracy bound $O(\epsilon^{-2})$ and gets rids of the linear dependency on the action space, while scaling polynomially with 
    
[^78]: 基于图像令牌的语义提示用于持续学习

    Semantic Prompting with Image-Token for Continual Learning

    [https://arxiv.org/abs/2403.11537](https://arxiv.org/abs/2403.11537)

    提出了一种基于图像令牌的语义提示方法，称为I-Prompt，旨在消除任务预测，通过语义提示匹配和图像令牌级提示来选择提示。

    

    持续学习旨在在保留先前任务知识的同时，为新任务微调模型参数。最近，基于提示的学习已经出现，利用预训练模型提示学习后续任务，而不依赖于重复缓冲区。尽管这种方法取得了出色的结果，但现有方法取决于前置任务选择过程来选择适当的提示。但是，在任务选择中的不完美可能会对性能产生负面影响，尤其是在任务数量较大或任务分布不均衡的情况下。为解决这一问题，我们引入了I-Prompt，这是一种关注图像令牌视觉语义信息以消除任务预测的无关方法。我们的方法包括语义提示匹配，根据令牌之间的相似性确定提示，以及图像令牌级提示，应用提示直接进行。

    arXiv:2403.11537v1 Announce Type: cross  Abstract: Continual learning aims to refine model parameters for new tasks while retaining knowledge from previous tasks. Recently, prompt-based learning has emerged to leverage pre-trained models to be prompted to learn subsequent tasks without the reliance on the rehearsal buffer. Although this approach has demonstrated outstanding results, existing methods depend on preceding task-selection process to choose appropriate prompts. However, imperfectness in task-selection may lead to negative impacts on the performance particularly in the scenarios where the number of tasks is large or task distributions are imbalanced. To address this issue, we introduce I-Prompt, a task-agnostic approach focuses on the visual semantic information of image tokens to eliminate task prediction. Our method consists of semantic prompt matching, which determines prompts based on similarities between tokens, and image token-level prompting, which applies prompts dire
    
[^79]: OCR即为所需：将多模态性引入基于图像的缺陷检测系统

    OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System

    [https://arxiv.org/abs/2403.11536](https://arxiv.org/abs/2403.11536)

    引入外部模态引导的数据挖掘框架以解决自动光学检验在工业制造中面临的模型部署挑战和准确性问题。

    

    arXiv:2403.11536v1 公告类型: 跨领域 摘要: 自动光学检验（AOI）在制造过程中起着关键作用，主要利用高分辨率成像仪器进行扫描。它通过分析图像纹理或图案来检测异常，从而成为工业制造和质量控制中的重要工具。尽管它的重要性，用于AOI的模型部署经常面临挑战，包括有限的样本量阻碍了有效特征学习，源领域之间的差异以及在成像过程中光照和摄像机位置变化对其敏感性。这些因素共同影响模型预测的准确性。传统的AOI通常无法充分利用来自机器或图像内部的丰富机制参数信息，包括经常有益于AOI分类的统计参数。为了解决这个问题，我们引入了一个基于外部模态引导的数据挖掘框架，主要根植于...

    arXiv:2403.11536v1 Announce Type: cross  Abstract: Automatic optical inspection (AOI) plays a pivotal role in the manufacturing process, predominantly leveraging high-resolution imaging instruments for scanning purposes. It detects anomalies by analyzing image textures or patterns, making it an essential tool in industrial manufacturing and quality control. Despite its importance, the deployment of models for AOI often faces challenges. These include limited sample sizes, which hinder effective feature learning, variations among source domains, and sensitivities to changes in lighting and camera positions during imaging. These factors collectively compromise the accuracy of model predictions. Traditional AOI often fails to capitalize on the rich mechanism-parameter information from machines or inside images, including statistical parameters, which typically benefit AOI classification. To address this, we introduce an external modality-guided data mining framework, primarily rooted in o
    
[^80]: 应该使用符合预测进行分布外检测（反之亦然？）

    Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)

    [https://arxiv.org/abs/2403.11532](https://arxiv.org/abs/2403.11532)

    本研究提出使用符合预测来评估分布外（OOD）检测中效率的新方法，并定义了新的符合AUROC和符合FRP@TPR95指标，为OOD和异常检测基准提供了概率保守性保证。

    

    关于分布外（OOD）检测的研究主要集中在构建有效区分OOD数据和分布内（ID）数据的分数上。另一方面，符合预测（CP）使用非一致性分数构建具有概率覆盖保证的预测集。在这项工作中，我们提出使用CP更好地评估OOD分数的效率。具体而言，我们强调在标准OOD基准设置中，由于测试数据集的有限样本大小，评估指标可能过于乐观。基于（Bates等人，2022）的工作，我们定义了新的符合AUROC和符合FRP@TPR95指标，这些修正提供了关于这些指标变异性的概率保守性保证。我们展示了这些修正对两个参考OOD和异常检测基准OpenOOD（Yang等人，2022）和AD-Bench（Han等人，2022）的影响。我们还展示了我们的作用的好处

    arXiv:2403.11532v1 Announce Type: cross  Abstract: Research on Out-Of-Distribution (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data. On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use CP to better assess the efficiency of OOD scores. Specifically, we emphasize that in standard OOD benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal AUROC and conformal FRP@TPR95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference OOD and anomaly detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022). We also show that the benefits of us
    
[^81]: LOOPer: 一个针对多面体编译器的学习型自动代码优化器

    LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers

    [https://arxiv.org/abs/2403.11522](https://arxiv.org/abs/2403.11522)

    LOOPer是针对多面体编译器的学习型自动代码优化器，通过机器学习建立成本模型来指导多面体优化搜索，突破了传统编译器在选择代码转换方面的限制。

    

    虽然多面体编译器在实现高级代码转换方面已经取得成功，但在选择能够带来最佳加速的最有利转换方面仍然面临挑战。这促使使用机器学习构建成本模型来引导多面体优化的搜索。最先进的多面体编译器已经展示了这种方法的可行性概念验证。虽然这种概念验证显示出了希望，但仍然存在显著限制。使用深度学习成本模型的最先进多面体编译器只支持少量仿射变换的子集，限制了它们应用复杂代码变换的能力。它们还只支持具有单个循环嵌套和矩形迭代域的简单程序，限制了它们对许多程序的适用性。这些限制显著影响了这样的编译器和自动调度器的通用性

    arXiv:2403.11522v1 Announce Type: cross  Abstract: While polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. This has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. While such a proof-of-concept has shown promise, it still has significant limitations. State-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. They also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. These limitations significantly impact the generality of such compilers and autoschedulers and put in
    
[^82]: 基于有限传感器测量的快速检测气动弹性振动模态的数据驱动方法

    A Data-driven Approach for Rapid Detection of Aeroelastic Modes from Flutter Flight Test Based on Limited Sensor Measurements

    [https://arxiv.org/abs/2403.11521](https://arxiv.org/abs/2403.11521)

    本研究提出了一种基于时延嵌入动态模态分解技术的数据驱动方法，以及稳健主成分分析和促进稀疏度准则，能够自动和最优地选择稀疏模态，加快气动弹性模态识别和分析过程。

    

    阵振飞行试验涉及通过对飞机升降面施加人工激励来评估机体的气动弹性稳定性。随后捕获和分析响应以提取系统的频率和阻尼特性。然而，噪声污染、湍流、模式非最佳激励以及一个或多个传感器的传感器故障导致该过程耗时并破坏提取过程。为了加快识别和分析气动弹性模态的过程，本研究实施了一种时延嵌入动态模态分解技术。这种方法辅之以稳健主成分分析方法和一个稀疏性促进标准，从而实现了稀疏模态的自动和最佳选择。本研究论文的第五作者提供的匿名化阵振飞行试验数据在此实施中被利用。

    arXiv:2403.11521v1 Announce Type: cross  Abstract: Flutter flight test involves the evaluation of the airframes aeroelastic stability by applying artificial excitation on the aircraft lifting surfaces. The subsequent responses are captured and analyzed to extract the frequencies and damping characteristics of the system. However, noise contamination, turbulence, non-optimal excitation of modes, and sensor malfunction in one or more sensors make it time-consuming and corrupt the extraction process. In order to expedite the process of identifying and analyzing aeroelastic modes, this study implements a time-delay embedded Dynamic Mode Decomposition technique. This approach is complemented by Robust Principal Component Analysis methodology, and a sparsity promoting criterion which enables the automatic and optimal selection of sparse modes. The anonymized flutter flight test data, provided by the fifth author of this research paper, is utilized in this implementation. The methodology assu
    
[^83]: 分离状态SARSA: 一种具有恢复奖励的实用序贯决策算法

    State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards

    [https://arxiv.org/abs/2403.11520](https://arxiv.org/abs/2403.11520)

    提出了分离状态SARSA（SS-SARSA）算法，针对恢复老虎机场景设计，通过将轮数视为状态，降低Q-learning/SARSA所需的状态组合数量，实现有效学习，并在温和假设下证明了渐近收敛至最优策略。

    

    虽然许多多臂老虎机算法假设所有臂的奖励在各轮之间保持不变，但在许多现实场景中，这种假设并不成立。本文考虑了恢复老虎机的设置，其中奖励取决于自上次拉动臂以来经过的轮数。我们提出了一种新的适用于这种情况的强化学习（RL）算法，名为分离状态SARSA（SS-SARSA）算法，其中将各轮视为状态。 SS-SARSA算法通过减少Q-learning/SARSA所需的状态组合数量来实现高效学习，这在大规模RL问题中经常遇到组合问题。此外，它对奖励结构进行最少假设并提供较低的计算复杂度。此外，我们证明在温和假设下渐近收敛至最优策略。模拟研究证明

    arXiv:2403.11520v1 Announce Type: new  Abstract: While many multi-armed bandit algorithms assume that rewards for all arms are constant across rounds, this assumption does not hold in many real-world scenarios. This paper considers the setting of recovering bandits (Pike-Burke & Grunewalder, 2019), where the reward depends on the number of rounds elapsed since the last time an arm was pulled. We propose a new reinforcement learning (RL) algorithm tailored to this setting, named the State-Separate SARSA (SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm achieves efficient learning by reducing the number of state combinations required for Q-learning/SARSA, which often suffers from combinatorial issues for large-scale RL problems. Additionally, it makes minimal assumptions about the reward structure and offers lower computational complexity. Furthermore, we prove asymptotic convergence to an optimal policy under mild assumptions. Simulation studies demonstrate the
    
[^84]: 使用EfficientNet和注意力机制从CT扫描中检测Covid-19

    Covid-19 detection from CT scans using EfficientNet and Attention mechanism

    [https://arxiv.org/abs/2403.11505](https://arxiv.org/abs/2403.11505)

    开发了一个基于深度学习模型的管道，结合EfficientNet和注意力机制，用于从肺部CT扫描图像中检测COVID-19，并在竞赛数据集验证集上表现优异

    

    通过深度学习模型，我们开发了一个基于管道的COVID-19检测方法，用于从肺部CT扫描图像中检测COVID-19。我们的方法结合了EfficientNet和注意力机制，并取得了比去年竞赛数据集验证集上的其他团队更好的结果。

    arXiv:2403.11505v1 Announce Type: cross  Abstract: Manual diagnosis and analysis of COVID-19 through the examination of lung Computed Tomography (CT) scan images by physicians tends to result in inefficiency, especially with high patient volumes and numerous images per patient. We address the need for automation by developing a deep learning model-based pipeline for COVID-19 detection from CT scan images of the lungs. The Domain adaptation, Explainability, and Fairness in AI for Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D) provides an opportunity to assess our designed pipeline for COVID-19 detection from CT scan images. The proposed pipeline incorporates EfficientNet with an Attention mechanism with a pre-processing step. Our pipeline outperforms last year's teams on the validation set of the competition dataset.
    
[^85]: MLVICX: 用于胸部X射线自监督表征学习的多级方差-协方差探索

    MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning

    [https://arxiv.org/abs/2403.11504](https://arxiv.org/abs/2403.11504)

    MLVICX是一种用于胸部X射线自监督表征学习的方法，通过多级方差和协方差探索策略来捕获丰富的表征。

    

    自监督学习（SSL）在减少手动注释需求、使深度学习模型可用于医学图像分析任务方面具有潜在用途。通过利用从未标记数据中学到的表示，自监督模型在无需或只需少量微调的任务上表现良好。然而，对于具有复杂解剖结构和多样临床病情的医学图像，如胸部X射线，需要一种能够编码细粒度细节并保留更广泛上下文信息的表征学习技术。在这种情况下，我们介绍了MLVICX（用于胸部X射线自监督表征学习的多级方差-协方差探索），一种从胸部X射线图像中捕获富表示形式的方法。我们方法的核心是一种新颖的多级方差和协方差探索策略，使其更有能力

    arXiv:2403.11504v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks. By leveraging the representations learned from unlabeled data, self-supervised models perform well on tasks that require little to no fine-tuning. However, for medical images, like chest X-rays, which are characterized by complex anatomical structures and diverse clinical conditions, there arises a need for representation learning techniques that can encode fine-grained details while preserving the broader contextual information. In this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning), an approach to capture rich representations in the form of embeddings from chest X-ray images. Central to our approach is a novel multi-level variance and covariance exploration strategy that empowers t
    
[^86]: CLIP总是比ImageNet模型泛化更好吗？

    Do CLIPs Always Generalize Better than ImageNet Models?

    [https://arxiv.org/abs/2403.11497](https://arxiv.org/abs/2403.11497)

    CLIP模型在面对分布转移时表现出良好的泛化能力，作者设计了CounterAnimal数据集来探究模型对虚假特征的依赖性。

    

    大型视觉语言模型，例如CLIP，已经彻底改变了现代机器学习。CLIP展示了在分布转移下的良好泛化能力，得到了越来越多的文献支持。然而，CLIP的评估数据集主要是为ImageNet基准而设计的变种，可能不能完全反映CLIP在LAION等上进行预训练时对虚假相关性的稳健性。为了弥补这一差距，我们收集了一个真实世界数据集，名为CounterAnimal，其中包含动物照片中发现的现实虚假特征。CounterAnimal包括a）常见组：包括常见背景的动物，并且 b) 对照组：包括在不寻常背景下的动物。从常见组到对照组的性能下降量化了模型对虚假特征（即背景）预测动物的依赖性。我们发现，在LAION或OpenAI数据上进行训练的CLIP即没有

    arXiv:2403.11497v1 Announce Type: cross  Abstract: Large vision language models, such as CLIPs, have revolutionized modern machine learning. CLIPs have demonstrated great generalizability under distribution shifts, supported by an increasing body of literature. However, the evaluation datasets for CLIPs are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos. CounterAnimal consists of a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual backgrounds. The performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit no
    
[^87]: 具有时间动态的路网语义增强表示学习

    Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics

    [https://arxiv.org/abs/2403.11495](https://arxiv.org/abs/2403.11495)

    提出了一种名为Toast的新框架，以及增强版DyToast，用于学习路网的通用表示，并增强了时间动态的整合，以提高各种时间敏感下游任务的性能。

    

    在这项研究中，我们引入了一个名为Toast的新颖框架，用于学习路网的通用表示，以及其增强版DyToast，旨在增强时间动态的整合，提高各种时间敏感下游任务的性能。具体来说，我们提出编码路网固有的两个关键语义特征：交通模式和行驶语义。为实现此目的，我们通过纳入旨在预测与目标路段相关的交通上下文的辅助目标，改进了skip-gram模块。此外，我们利用轨迹数据，并设计基于Transformer的预训练策略，以在路网上提炼行驶语义。DyToast通过使用具有益处特性的统一三角函数进一步增进了该框架，使其能够捕获路网的时间演变和动态性质。

    arXiv:2403.11495v1 Announce Type: cross  Abstract: In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road netwo
    
[^88]: 不会忘却的不确定性校准测试时间模型适应

    Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting

    [https://arxiv.org/abs/2403.11491](https://arxiv.org/abs/2403.11491)

    提出了一种高效的抗遗忘测试时间适应（EATA）方法，通过开发主动样本选择标准和引入Fisher正则化约束重要模型参数，实现了不会忘记的不确定性校准测试时间模型适应。

    

    测试时间适应（TTA）旨在通过根据任何测试样本调整给定模型，以应对训练和测试数据之间的潜在分布偏移。尽管最近的TTA表现出有希望的性能，但我们仍然面临两个关键挑战：1）先前的方法对每个测试样本执行反向传播，导致许多应用程序无法承受的优化成本；2）虽然现有的TTA可以显著改善在分布数据上的测试性能，但它们经常在TTA后在分布数据上遭受严重性能下降（即所谓的遗忘）。为此，我们提出了一种高效的抗遗忘测试时间适应（EATA）方法，该方法开发了一个主动样本选择标准，以识别可靠且非冗余的样本进行在测试时间的熵最小化。为了缓解遗忘，EATA引入了从测试样本中估计的Fisher正则化项，以约束重要的模型参数免于急剧变化。

    arXiv:2403.11491v1 Announce Type: new  Abstract: Test-time adaptation (TTA) seeks to tackle potential distribution shifts between training and test data by adapting a given model w.r.t. any test sample. Although recent TTA has shown promising performance, we still face two key challenges: 1) prior methods perform backpropagation for each test sample, resulting in unbearable optimization costs to many applications; 2) while existing TTA can significantly improve the test performance on out-of-distribution data, they often suffer from severe performance degradation on in-distribution data after TTA (known as forgetting). To this end, we have proposed an Efficient Anti-Forgetting Test-Time Adaptation (EATA) method which develops an active sample selection criterion to identify reliable and non-redundant samples for test-time entropy minimization. To alleviate forgetting, EATA introduces a Fisher regularizer estimated from test samples to constrain important model parameters from drastic c
    
[^89]: 开放世界半监督学习用于节点分类

    Open-World Semi-Supervised Learning for Node Classification

    [https://arxiv.org/abs/2403.11483](https://arxiv.org/abs/2403.11483)

    方差不平衡可能对模型性能产生负面影响，提出一种不依赖预训练图编码器的有效方法

    

    开放世界半监督学习 (Open-world SSL) 用于节点分类，在图形社区中是一个实用但未被充分探索的问题，它将未标记的节点分类为已见类或多个新颖类。根据经验和理论分析，我们发现方差不平衡可能对模型性能产生负面影响。预训练特征编码器可以通过为新颖类生成紧凑表示来缓解这个问题。然而，为各种类型的图形数据创建通用预训练编码器被证明是具有挑战性的。因此，需要一种不依赖预训练图编码器的有效方法。

    arXiv:2403.11483v1 Announce Type: cross  Abstract: Open-world semi-supervised learning (Open-world SSL) for node classification, that classifies unlabeled nodes into seen classes or multiple novel classes, is a practical but under-explored problem in the graph community. As only seen classes have human labels, they are usually better learned than novel classes, and thus exhibit smaller intra-class variances within the embedding space (named as imbalance of intra-class variances between seen and novel classes). Based on empirical and theoretical analysis, we find the variance imbalance can negatively impact the model performance. Pre-trained feature encoders can alleviate this issue via producing compact representations for novel classes. However, creating general pre-trained encoders for various types of graph data has been proven to be challenging. As such, there is a demand for an effective method that does not rely on pre-trained graph encoders. In this paper, we propose an IMbalanc
    
[^90]: SeisFusion: 带有输入指导的受限扩散模型用于3D地震数据插值和重构

    SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction

    [https://arxiv.org/abs/2403.11482](https://arxiv.org/abs/2403.11482)

    提出了一种适用于3D地震数据的新型扩散模型重建框架，可以在处理复杂缺失模式时提高重建性能

    

    地理、物理或经济约束通常导致地震数据中存在缺失的痕迹，使得重建完整的地震数据成为地震数据处理中的关键步骤。传统的地震数据重建方法需要选择多个经验参数，并且难以处理大规模连续缺失数据。随着深度学习的发展，各种神经网络展示出强大的重建能力。然而，这些卷积神经网络代表了一种点对点的重建方法，可能无法覆盖整个数据集的分布。因此，当处理具有复杂缺失模式的地震数据时，这些网络可能会经历不同程度的性能下降。针对这一挑战，我们提出了一种针对3D地震数据量身定制的新颖扩散模型重建框架。

    arXiv:2403.11482v1 Announce Type: new  Abstract: Geographical, physical, or economic constraints often result in missing traces within seismic data, making the reconstruction of complete seismic data a crucial step in seismic data processing. Traditional methods for seismic data reconstruction require the selection of multiple empirical parameters and struggle to handle large-scale continuous missing data. With the development of deep learning, various neural networks have demonstrated powerful reconstruction capabilities. However, these convolutional neural networks represent a point-to-point reconstruction approach that may not cover the entire distribution of the dataset. Consequently, when dealing with seismic data featuring complex missing patterns, such networks may experience varying degrees of performance degradation. In response to this challenge, we propose a novel diffusion model reconstruction framework tailored for 3D seismic data. To constrain the results generated by the
    
[^91]: 弱通信和一般平均奖赏MDPs的基于跨度的最佳样本复杂度

    Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs

    [https://arxiv.org/abs/2403.11477](https://arxiv.org/abs/2403.11477)

    该研究提出了对于弱通信MDPs的样本复杂度界限为 $\tilde{O}(SA\frac{H}{\epsilon^2})$，改进了现有工作，是在所有参数上最小最优的。

    

    我们研究了在生成模型下学习平均奖赏马尔可夫决策过程（MDP）中$\epsilon$-最佳策略的样本复杂度。对于弱通信MDPs，我们建立了复杂度界限为$\tilde{O}(SA\frac{H}{\epsilon^2})$，其中$H$是最优策略的偏差函数的跨度，$SA$是状态-动作空间的基数。我们的结果是在所有参数$S,A,H$和$\epsilon$上（最多对数因子）最小最优的，改进了现有工作，现有工作要么假设所有策略的混合时间均匀有界，要么对参数有次优的依赖。我们进一步研究一般（非弱通信）平均奖赏MDPs中的样本复杂度。我们认为需要一个新的瞬态时间参数$B$，建立了一个$\tilde{O}(SA\frac{B+H}{\epsilon^2})$的复杂度界限，并证明了匹配的（最多对数因子）最小最优下界。这两个结果都是基于减少

    arXiv:2403.11477v1 Announce Type: new  Abstract: We study the sample complexity of learning an $\epsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\tilde{O}(SA\frac{H}{\epsilon^2})$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\epsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We further investigate sample complexity in general (non-weakly-communicating) average-reward MDPs. We argue a new transient time parameter $B$ is necessary, establish an $\tilde{O}(SA\frac{B+H}{\epsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the
    
[^92]: 通过基于记忆的增量训练加速字符串键的学习索引结构

    Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training

    [https://arxiv.org/abs/2403.11472](https://arxiv.org/abs/2403.11472)

    通过开发一种新技术，我们能够加速学习索引结构的重新训练过程，解决了字符串键在学习索引中性能瓶颈的问题。

    

    学习索引使用机器学习模型来学习键和它们在键-值索引中对应位置之间的映射。这些索引使用映射信息作为训练数据。学习索引需要频繁重新训练它们的模型以整合更新查询引入的更改。为了有效重新训练模型，现有的学习索引系统经常利用一种线性代数QR分解技术来执行矩阵分解。这种因子化方法在每次重新训练过程中处理所有键-位置对，导致随着键和它们长度的总数线性增长的计算操作。因此，重新训练会造成严重的性能瓶颈，尤其是对于可变长度的字符串键，而重新训练对于保持高预测准确性以及确保低查询服务延迟至关重要。

    arXiv:2403.11472v1 Announce Type: new  Abstract: Learned indexes use machine learning models to learn the mappings between keys and their corresponding positions in key-value indexes. These indexes use the mapping information as training data. Learned indexes require frequent retrainings of their models to incorporate the changes introduced by update queries. To efficiently retrain the models, existing learned index systems often harness a linear algebraic QR factorization technique that performs matrix decomposition. This factorization approach processes all key-position pairs during each retraining, resulting in compute operations that grow linearly with the total number of keys and their lengths. Consequently, the retrainings create a severe performance bottleneck, especially for variable-length string keys, while the retrainings are crucial for maintaining high prediction accuracy and in turn, ensuring low query service latency.   To address this performance problem, we develop an 
    
[^93]: FedSPU：具有随机参数更新的资源受限设备个性化联邦学习

    FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update

    [https://arxiv.org/abs/2403.11464](https://arxiv.org/abs/2403.11464)

    提出了一种具有随机参数更新机制的个性化联邦学习方法，以解决资源受限设备在非iid数据场景下的性能下降问题。

    

    个性化的联邦学习（PFL）被广泛应用于物联网应用中，用于处理大量的非iid客户端数据，同时确保数据隐私。然而，客户拥有的异构边缘设备可能施加不同程度的资源约束，给PFL造成计算和通信瓶颈。联邦Dropout已成为应对这一挑战的一种流行策略，其中仅在客户端设备上训练全局模型的一个子模型，从而降低计算和通信开销。然而，基于Dropout的模型修剪策略可能引入偏差，特别是对非iid本地数据。当有偏见的子模型吸收来自其他客户端的高度分散参数时，性能下降是不可避免的。为了应对这一情况，我们提出了具有随机参数更新的联邦学习（FedSPU）。与专门为小型本地子模型定制全局模型的Dropout不同，FedSPU引入了个性化的随机参数更新机制，以在保持数据隐私的同时降低计算和通信开销。

    arXiv:2403.11464v1 Announce Type: new  Abstract: Personalized Federated Learning (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy. However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL. Federated Dropout has emerged as a popular strategy to address this challenge, wherein only a subset of the global model, i.e. a \textit{sub-model}, is trained on a client's device, thereby reducing computation and communication overheads. Nevertheless, the dropout-based model-pruning strategy may introduce bias, particularly towards non-iid local data. When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable. In response, we propose federated learning with stochastic parameter update (FedSPU). Unlike dropout that tailors the global model to small-size local sub-model
    
[^94]: 具有潜在因果发现功能的图部分标签学习

    Graph Partial Label Learning with Potential Cause Discovering

    [https://arxiv.org/abs/2403.11449](https://arxiv.org/abs/2403.11449)

    提出了一种具有潜在因果发现功能的图部分标签学习方法，可在部分标记学习环境中有效学习区分信息。

    

    图神经网络（GNNs）因其在解决各领域复杂图结构数据挑战中的潜力而受到广泛关注。然而，准确标注图数据以进行训练由于图的固有复杂性和相互关联性而困难。为了解决这个问题，我们提出了一种新颖的图表示学习方法，使得GNN模型能够有效地学习区分信息，即使在部分标记学习（PLL）的环境中存在噪声标签。 PLL是一个重要的弱监督学习问题，其中每个训练实例与一组候选标签相关联，包括真实标签和额外的噪声标签。我们的方法利用潜在因果提取来获取具有更高因果关系可能性的图数据。通过结合基于提取的图的辅助训练，

    arXiv:2403.11449v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have gained considerable attention for their potential in addressing challenges posed by complex graph-structured data in diverse domains. However, accurately annotating graph data for training is difficult due to the inherent complexity and interconnectedness of graphs. To tackle this issue, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL). PLL is a critical weakly supervised learning problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels. Our approach leverages potential cause extraction to obtain graph data that exhibit a higher likelihood of possessing a causal relationship with the labels. By incorporating auxiliary training based on the extracted gra
    
[^95]: LLM引导进化-模型推进模型的自动化

    LLM Guided Evolution - The Automation of Models Advancing Models

    [https://arxiv.org/abs/2403.11446](https://arxiv.org/abs/2403.11446)

    该研究提出了一种新的"引导进化"（GE）框架，利用大型语言模型（LLMs）直接修改代码，采用自我维持的反馈循环增强模型进化决策制定。

    

    在机器学习领域，传统的模型发展和自动化方法（如AutoML）通常依赖于层层抽象，如基于树或Cartesian遗传规划。我们的研究介绍了一种新颖的框架“引导进化”（GE），它不同于这些方法，利用大型语言模型（LLMs）直接修改代码。GE利用LLMs进行更智能的、有监督的进化过程，引导变异和交叉。我们独特的“思想进化”（EoT）技术进一步增强了GE，使LLMs能够反思和从先前变异的结果中学习。这导致了一个自我维持的反馈循环，增强了模型进化中的决策制定。通过利用LLMs生成对精心制作的提示的多样响应和调节模型温度来维持遗传多样性，GE维持着对进化算法至关重要的遗传多样性。这不仅加速了进化

    arXiv:2403.11446v1 Announce Type: cross  Abstract: In the realm of machine learning, traditional model development and automated approaches like AutoML typically rely on layers of abstraction, such as tree-based or Cartesian genetic programming. Our study introduces "Guided Evolution" (GE), a novel framework that diverges from these methods by utilizing Large Language Models (LLMs) to directly modify code. GE leverages LLMs for a more intelligent, supervised evolutionary process, guiding mutations and crossovers. Our unique "Evolution of Thought" (EoT) technique further enhances GE by enabling LLMs to reflect on and learn from the outcomes of previous mutations. This results in a self-sustaining feedback loop that augments decision-making in model evolution. GE maintains genetic diversity, crucial for evolutionary algorithms, by leveraging LLMs' capability to generate diverse responses from expertly crafted prompts and modulate model temperature. This not only accelerates the evolution
    
[^96]: 深度强化学习驱动的自主车辆决策的揭秘

    Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making

    [https://arxiv.org/abs/2403.11432](https://arxiv.org/abs/2403.11432)

    本研究致力于研究基于注意力的DRL框架的可解释性，在自主车辆决策中，通过在开源AV仿真环境中添加多头注意力框架，提高了模型的解释性。

    

    随着强化学习领域中通用函数逼近器的出现，利用深度强化学习（DRL）的实际应用数量激增。自动驾驶任务中的决策制定已成为其中一项主要应用，将传感器数据或高阶运动学变量作为输入，并提供离散选择或连续控制输出。然而，模型的黑盒特性限制了DRL在自主车辆中的实际部署。因此，在这项研究工作中，我们关注基于注意力的DRL框架的可解释性。我们在开源AV仿真环境中使用了基于连续近端策略优化的DRL算法作为基线模型，并添加了一个多头注意力框架。我们提供了一些分析技术来讨论训练模型的可解释性。

    arXiv:2403.11432v1 Announce Type: cross  Abstract: With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded. Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output. However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs). Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework. We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment. We provide some analytical techniques for discussing the interpretability of the trained mode
    
[^97]: 叙事特征还是结构特征？研究大型语言模型以识别患心力衰竭风险的癌症患者

    Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure

    [https://arxiv.org/abs/2403.11425](https://arxiv.org/abs/2403.11425)

    使用大型语言模型结合新颖的叙述特征，能够有效识别癌症患者患心力衰竭的风险，表现优于传统机器学习模型和深度学习模型。

    

    癌症治疗已知会引入心毒性，对预后和生存率产生负面影响。识别患心力衰竭（HF）风险的癌症患者对于改善癌症治疗结果和安全性至关重要。本研究使用来自电子健康记录（EHRs）的机器学习（ML）模型，包括传统ML、时间感知长短期记忆（T-LSTM）和使用从结构化医学代码衍生的新颖叙述特征的大型语言模型（LLMs）来识别患HF风险的癌症患者。我们从佛罗里达大学健康中心识别了一组包括12,806名肺癌、乳腺癌和结直肠癌患者的癌症队列，其中1,602人在癌症后发展为HF。LLM GatorTron-3.9B取得了最佳的F1分数，比传统支持向量机高出39%，比T-LSTM深度学习模型高出7%，比广泛使用的Transformer模型BERT高出5.6%。

    arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
    
[^98]: 神经网络表示量子系统

    Neural network representation of quantum systems

    [https://arxiv.org/abs/2403.11420](https://arxiv.org/abs/2403.11420)

    利用神经网络的通用逼近定理，我们提出了一个新颖的映射方法，将广泛类的量子力学系统表示为神经网络形式，从而可以对Feynman路径积分中的任意路径进行统计求和。

    

    已经提出，接近高斯过程的随机宽神经网络是围绕高斯固定点的量子场论。在本文中，我们提供了一种新颖的映射，通过该映射，可以将一大类量子力学系统表达为具有对网络参数的统计求和形式的神经网络。我们的简单思想是利用神经网络的通用逼近定理生成费曼路径积分中的任意路径。这种映射可以应用于相互作用的量子系统/场论，即使远离高斯极限。我们的发现使机器学习与量子世界更加接近。

    arXiv:2403.11420v1 Announce Type: cross  Abstract: It has been proposed that random wide neural networks near Gaussian process are quantum field theories around Gaussian fixed points. In this paper, we provide a novel map with which a wide class of quantum mechanical systems can be cast into the form of a neural network with a statistical summation over network parameters. Our simple idea is to use the universal approximation theorem of neural networks to generate arbitrary paths in the Feynman's path integral. The map can be applied to interacting quantum systems / field theories, even away from the Gaussian limit. Our findings bring machine learning closer to the quantum world.
    
[^99]: 时间轨迹的变分采样

    Variational Sampling of Temporal Trajectories

    [https://arxiv.org/abs/2403.11418](https://arxiv.org/abs/2403.11418)

    本文介绍了一种通过在函数空间中显式参数化过渡函数来学习轨迹分布的机制，实现了对轨迹的高效合成和推断。

    

    一个确定性的时间过程可以通过其轨迹来确定，该轨迹是(a)初始条件$z_0 \in \mathcal{Z}$和(b)过渡函数$f:(\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$的乘积空间中的元素，往往受底层动力系统控制的影响。现有方法通常将过渡函数建模为微分方程或循环神经网络。尽管它们在预测未来测量方面很有效，但很少有结果成功建立使用神经网络进行轨迹采样和统计推断的方法，部分原因是参数化方面的约束。在这项工作中，我们引入了一种机制，通过将过渡函数$f$显式地参数化为函数空间中的一个元素来学习轨迹的分布。我们的框架允许有效地合成新颖的轨迹，同时还直接提供了一个方便的工具来进行推断。

    arXiv:2403.11418v1 Announce Type: cross  Abstract: A deterministic temporal process can be determined by its trajectory, an element in the product space of (a) initial condition $z_0 \in \mathcal{Z}$ and (b) transition function $f: (\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$ often influenced by the control of the underlying dynamical system. Existing methods often model the transition function as a differential equation or as a recurrent neural network. Despite their effectiveness in predicting future measurements, few results have successfully established a method for sampling and statistical inference of trajectories using neural networks, partially due to constraints in the parameterization. In this work, we introduce a mechanism to learn the distribution of trajectories by parameterizing the transition function $f$ explicitly as an element in a function space. Our framework allows efficient synthesis of novel trajectories, while also directly providing a convenient tool for inferen
    
[^100]: DreamSampler：统一扩散采样和分数蒸馏以用于图像处理

    DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation

    [https://arxiv.org/abs/2403.11415](https://arxiv.org/abs/2403.11415)

    DreamSampler框架通过整合反向扩散采样和分数蒸馏，提供了模型无关的图像处理方法，解决了分数蒸馏易崩溃的问题，并在图像编辑和重构中展现了竞争力。

    

    反向采样和分数蒸馏已成为最近几年使用潜在扩散模型（LDMs）进行图像处理的主要工具。虽然反向扩散采样通常需要调整LDM架构或特征工程，分数蒸馏提供了一种简单而强大的与模型无关的方法，但往往容易发生模式崩溃。为了解决这些局限性并利用这两种方法的优势，我们引入了一个称为DreamSampler的新颖框架，通过正则化潜在优化的视角无缝地整合了这两种不同的方法。类似于分数蒸馏，DreamSampler是一种适用于任何LDM架构的模型无关方法，但它允许在图像编辑和重构中进行蒸馏和反向采样，并提供额外的指导。通过涉及图像编辑、SVG重构等实验，我们展示了竞争力

    arXiv:2403.11415v1 Announce Type: cross  Abstract: Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs). While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive pe
    
[^101]: 图神经网络的层多样负采样

    Layer-diverse Negative Sampling for Graph Neural Networks

    [https://arxiv.org/abs/2403.11408](https://arxiv.org/abs/2403.11408)

    提出了一种层多样的负采样方法，通过在消息传递传播中选择性采样负样本，并且在多层GNNs中实现了逐层负样本多样性，实验证明其有效性。

    

    图神经网络(GNNs)是各种结构学习应用的强大解决方案，因为它们对图数据的表示能力强大。然而，传统的GNNs依赖于仅从一阶邻居（即正样本）收集信息的消息传递机制，可能导致过度平滑和过度压缩等问题。为了缓解这些问题，我们提出了一种用于消息传递传播的层多样负采样方法。该方法在确定性点过程中使用了一个采样矩阵，将候选集转换为一个空间，并从该空间选择性地对其进行采样以生成负样本。为了在每个前向传播过程中进一步增强负样本的多样性，我们开发了一种空间压缩方法，以实现多层GNNs中的逐层多样性。各种真实世界图数据集上的实验证明了我们方法的有效性。

    arXiv:2403.11408v1 Announce Type: new  Abstract: Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approac
    
[^102]: 分布式分隔后验采样用于去噪扩散先验

    Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors

    [https://arxiv.org/abs/2403.11407](https://arxiv.org/abs/2403.11407)

    通过利用去噪扩散模型先验结构，提出了一种分布式分隔后验采样方法，相比先前的方法具有更低的逼近误差。

    

    近年来，对于使用去噪扩散模型（DDM）作为逆贝叶斯问题求解的先验引起了极大的兴趣。然而，从结果后验分布中抽样是一个挑战。为了解决这个问题，先前的研究提出了近似方法来偏置扩散的漂移项。在本工作中，我们采取了一种不同的方法，并利用DDM先验的特定结构来定义一组中间和更简单的后验抽样问题，相比先前的方法，这些方法具有更低的逼近误差。我们通过使用合成例子和各种图像恢复任务来实证地展示我们方法对于一般线性逆问题的重构能力。

    arXiv:2403.11407v1 Announce Type: cross  Abstract: Interest in the use of Denoising Diffusion Models (DDM) as priors for solving inverse Bayesian problems has recently increased significantly. However, sampling from the resulting posterior distribution poses a challenge. To solve this problem, previous works have proposed approximations to bias the drift term of the diffusion. In this work, we take a different approach and utilize the specific structure of the DDM prior to define a set of intermediate and simpler posterior sampling problems, resulting in a lower approximation error compared to previous methods. We empirically demonstrate the reconstruction capability of our method for general linear inverse problems using synthetic examples and various image restoration tasks.
    
[^103]: 用于深度学习和大数据应用的自动化数据处理和特征工程：一项调查

    Automated data processing and feature engineering for deep learning and big data applications: a survey

    [https://arxiv.org/abs/2403.11395](https://arxiv.org/abs/2403.11395)

    现代人工智能方法旨在设计能够直接从数据中学习的算法，自动化数据处理任务的兴起驱动了机器学习和大数据应用中利用大量复杂数据的发展。

    

    现代人工智能（AI）的方法旨在设计能够直接从数据中学习的算法。这种方法取得了令人印象深刻的成果，并在AI的发展中做出了重要贡献，特别是在监督深度学习领域。它也简化了机器学习系统的设计，因为学习过程是高度自动化的。然而，并非所有传统深度学习流程中的数据处理任务都已自动化。在大多数情况下，数据必须在可以用于训练之前经过手动收集、预处理并通过数据增强进一步扩展。最近，出现了用于自动化这些任务的特殊技术。数据处理任务的自动化驱动力是利用大量复杂、异构数据进行机器学习和大数据应用。如今，基于自动化机器学习（A

    arXiv:2403.11395v1 Announce Type: cross  Abstract: Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (A
    
[^104]: 探究投影头对表示学习的益处

    Investigating the Benefits of Projection Head for Representation Learning

    [https://arxiv.org/abs/2403.11391](https://arxiv.org/abs/2403.11391)

    投影头技术能够通过逐层渐进的特征加权和更为归一化的表示，提高表示学习效果。

    

    一种获取高质量表示的有效技术是在训练过程中在编码器顶部添加一个投影头，然后丢弃它并使用预投影表示。尽管该技术被证明在实践中有效，但其成功背后的原因尚不明确。在这项工作中，我们对这个问题提供了严谨的理论回答。我们首先研究了使用自监督对比损失训练的线性模型。我们揭示了训练算法的隐式偏差导致了逐层渐进的特征加权，随着层深入，特征变得越来越不均衡。因此，较低层往往具有更多归一化和更少专门化表示。我们从理论上表征了这种表示更为适用的情况。

    arXiv:2403.11391v1 Announce Type: new  Abstract: An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations. Despite its proven practical effectiveness, the reason behind the success of this technique is poorly understood. The pre-projection representations are not directly optimized by the loss function, raising the question: what makes them better? In this work, we provide a rigorous theoretical answer to this question. We start by examining linear models trained with self-supervised contrastive loss. We reveal that the implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers. Consequently, lower layers tend to have more normalized and less specialized representations. We theoretically characterize scenarios where such representations are more 
    
[^105]: 用于多孔域中椭圆问题的随机方法

    Stochastic approach for elliptic problems in perforated domains

    [https://arxiv.org/abs/2403.11385](https://arxiv.org/abs/2403.11385)

    提出了一种基于神经网络的无网格方法，用于解决多孔域中的椭圆问题，该方法结合了随机表示方法和Feynman-Kac公式，能够有效捕捉多尺度性质并处理界面条件。

    

    科学与工程中的广泛应用涉及带有孔隙的域中的PDE模型，例如多孔金属或空气过滤器。解决这类多孔域问题受到与解决孔隙几何尺度相关的计算挑战。我们提出了一种基于神经网络的无网格方法，用于处理多孔域问题。该方法在捕捉各种配置尺度方面具有鲁棒性和高效性，包括涉及由小孔引起的多尺度性质的解的平均宏观行为。新方法结合了使用随机表示或Feynman-Kac公式的无导数损失方法。特别地，我们实现了用于处理域与孔隙之间界面的无导数损失方法的Neumann边界条件。提供了一套严格的数值测试来支持所提出的方法。

    arXiv:2403.11385v1 Announce Type: cross  Abstract: A wide range of applications in science and engineering involve a PDE model in a domain with perforations, such as perforated metals or air filters. Solving such perforated domain problems suffers from computational challenges related to resolving the scale imposed by the geometries of perforations. We propose a neural network-based mesh-free approach for perforated domain problems. The method is robust and efficient in capturing various configuration scales, including the averaged macroscopic behavior of the solution that involves a multiscale nature induced by small perforations. The new approach incorporates the derivative-free loss method that uses a stochastic representation or the Feynman-Kac formulation. In particular, we implement the Neumann boundary condition for the derivative-free loss method to handle the interface between the domain and perforations. A suite of stringent numerical tests is provided to support the proposed
    
[^106]: Path-GPTOmic：一种用于生存结果预测的平衡多模态学习框架

    Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction

    [https://arxiv.org/abs/2403.11375](https://arxiv.org/abs/2403.11375)

    Path-GPTOmic框架通过调节模型嵌入空间和提出梯度调整模型，解决了癌症生存预测中存在的病理图像和基因组学数据不平衡的问题。

    

    为了预测癌症生存结果，临床研究中的标准方法通常基于两种主要模态：用于观察细胞形态特征的病理图像，以及用于量化基因表达的基因组学（如批量RNA测序）。然而，现有的病理基因组多模态算法面临着重大挑战：（1）有价值的关于基因和基因间相互作用的生物学见解经常被忽视；（2）一个模态通常主导优化过程，导致对另一个模态的训练不足。在本文中，我们引入了一种新的多模态“Path-GPTOmic”框架用于癌症生存预测。首先，为了提取有价值的生物学见解，我们调节基础模型scGPT的嵌入空间，该模型最初是在单细胞RNA测序数据上训练的，使其适用于批量RNA测序数据。其次，为了解决模态间不平衡问题，我们提出了一个梯度调整模型提供更好的路径遗传学技术（一种转导技术）。

    arXiv:2403.11375v1 Announce Type: cross  Abstract: For predicting cancer survival outcomes, standard approaches in clinical research are often based on two main modalities: pathology images for observing cell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene expressions. However, existing pathology-genomic multi-modal algorithms face significant challenges: (1) Valuable biological insights regarding genes and gene-gene interactions are frequently overlooked; (2) one modality often dominates the optimization process, causing inadequate training for the other modality. In this paper, we introduce a new multi-modal ``Path-GPTOmic" framework for cancer survival outcome prediction. First, to extract valuable biological insights, we regulate the embedding space of a foundation model, scGPT, initially trained on single-cell RNA-seq data, making it adaptable for bulk RNA-seq data. Second, to address the imbalance-between-modalities problem, we propose a gradient modula
    
[^107]: JORA: 用于检索增强微调的JAX张量并行LoRA库

    JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning

    [https://arxiv.org/abs/2403.11366](https://arxiv.org/abs/2403.11366)

    提出了用于检索增强微调的JAX张量并行LoRA库，通过PEFT兼容微调Llama-2模型，利用分布式训练和JAX的即时编译和张量分片实现了资源高效管理，加速微调并降低内存需求，提高了微调大型语言模型在复杂RAG应用中的可扩展性和可行性。

    

    《JORA: JAX张量并行LoRA库用于检索增强微调》通过介绍一种新的框架，提供了一种适用于检索增强生成（RAG）任务的PEFT兼容微调Llama-2模型的方法，利用分布式训练，独特地利用了JAX的即时编译（JIT）和张量分片，实现了资源的高效管理，从而实现了加速微调并降低内存需求。这一进展显著提高了微调大型语言模型（LLMs）用于复杂RAG应用的可扩展性和可行性，甚至在GPU资源有限的系统上也能取得显著改进。

    arXiv:2403.11366v1 Announce Type: cross  Abstract: The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences. Current open-source libraries support full-model inference and fine-tuning across multiple GPUs but fall short of accommodating the efficient parameter distribution required for retrieved context. Addressing this gap, we introduce a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, thereby enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources. Our experiments show more than 1
    
[^108]: IGANN Sparse: 用非线性洞察力连接稀疏性和可解释性

    IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight

    [https://arxiv.org/abs/2403.11363](https://arxiv.org/abs/2403.11363)

    IGANN Sparse是一种新颖的机器学习模型，通过训练过程中的非线性特征选择促进稀疏性，确保在不影响预测性能的情况下提高模型的可解释性。

    

    特征选择是预测性分析中的关键组成部分，显著影响模型的预测准确性和解释性。本文提出了IGANN Sparse，这是一种新颖的机器学习模型，它来自广义加法模型家族，通过训练过程中的非线性特征选择促进稀疏性，从而确保通过改进模型稀疏性来实现可解释性而不影响预测性能。

    arXiv:2403.11363v1 Announce Type: cross  Abstract: Feature selection is a critical component in predictive analytics that significantly affects the prediction accuracy and interpretability of models. Intrinsic methods for feature selection are built directly into model learning, providing a fast and attractive option for large amounts of data. Machine learning algorithms, such as penalized regression models (e.g., lasso) are the most common choice when it comes to in-built feature selection. However, they fail to capture non-linear relationships, which ultimately affects their ability to predict outcomes in intricate datasets. In this paper, we propose IGANN Sparse, a novel machine learning model from the family of generalized additive models, which promotes sparsity through a non-linear feature selection process during training. This ensures interpretability through improved model sparsity without sacrificing predictive performance. Moreover, IGANN Sparse serves as an exploratory tool
    
[^109]: 溶剂感知的2D核磁共振预测：利用多任务训练和迭代自训练策略

    Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies

    [https://arxiv.org/abs/2403.11353](https://arxiv.org/abs/2403.11353)

    该论文提出了一种利用迭代自我训练方法来训练深度学习模型，从而解决二维核磁共振（2D NMR）预测中的挑战，弥补了缺乏标注NMR训练数据集的不足。

    

    核磁共振（NMR）光谱在各个科学领域中起着关键作用，提供了有关分子的结构信息、电子性质和动态行为的见解。准确的NMR光谱预测能够高效地生成候选分子，使化学家能够将它们与实际实验光谱进行比较。该过程有助于确认分子结构或指出差异，引导进一步的研究。机器学习（ML）已经成为一种有前途的替代方法，用于根据分子结构预测分子的原子NMR化学位移。虽然在预测一维（1D）NMR方面已经取得了显著进展，但通过机器学习进行二维（2D）NMR预测仍然是一项挑战，因为缺乏用于训练的标注的NMR数据集。为了解决这一差距，我们提出了一种迭代自训练（IST）方法，用于训练深度学习模型，以预测原子2DNMR位移。

    arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
    
[^110]: 基于SDP的二分图聚类分支定界算法

    An SDP-based Branch-and-Cut Algorithm for Biclustering

    [https://arxiv.org/abs/2403.11351](https://arxiv.org/abs/2403.11351)

    提出了一个基于SDP的分支定界算法，用于解决$k$-最密不相交双团问题。

    

    二分图聚类，也称为共聚类、块聚类或双向聚类，涉及将数据矩阵的行和列同时聚类成不同的组，使得同一组内的行和列显示出相似的模式。作为二分图聚类的模型问题，我们考虑$k$-最密不相交双团问题，其目标是在给定加权完全二分图中识别 $k$ 个不相交的完全二部子图（称为双团），使它们的密度之和最大化。为了解决这个问题，我们提出了一个定制的分支定界算法。对于上界例程，我们考虑半定规划放松并提出了用于加强界限的有效不等式。我们使用一种一阶方法以切平面方式解决这个放松问题。对于下界，我们设计了一个利用解决方案的最大权匹配舍入过程。

    arXiv:2403.11351v1 Announce Type: cross  Abstract: Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite graph such that the sum of their densities is maximized. To address this problem, we present a tailored branch-and-cut algorithm. For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound. We solve this relaxation in a cutting-plane fashion using a first-order method. For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved
    
[^111]: 有限角度层析成像中数据驱动方法的鲁棒性

    Robustness of the data-driven approach in limited angle tomography

    [https://arxiv.org/abs/2403.11350](https://arxiv.org/abs/2403.11350)

    数据驱动方法在有限角度层析成像中相较于传统方法能更稳定地重构更多信息

    

    有限角度Radon变换由于其不逆问题而闻名于世。在这项工作中，我们给出了一个数学解释，即基于深度神经网络的数据驱动方法相较于传统方法可以以更稳定的方式重构更多信息。

    arXiv:2403.11350v1 Announce Type: cross  Abstract: The limited angle Radon transform is notoriously difficult to invert due to the ill-posedness. In this work, we give a mathematical explanation that the data-driven approach based on deep neural networks can reconstruct more information in a stable way compared to traditional methods.
    
[^112]: COLEP: 通过概率电路实现可证实鲁棒学习推理一致性预测

    COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits

    [https://arxiv.org/abs/2403.11348](https://arxiv.org/abs/2403.11348)

    通过概率电路，提出了COLEP框架，实现了可证实鲁棒学习推理一致性预测，其特点在于训练统计模型学习不同语义概念，并利用概率电路实现精确高效推理

    

    参考类型：交叉  摘要：一致性预测已经显示出在为任意黑匣子机器学习模型构建统计严谨的预测集方面表现出色，假设数据是可交换的。然而，即使在推理过程中进行微小的对抗性扰动也可能违反可交换性假设，挑战覆盖率保证，并导致后续实证覆盖率的下降。在这项工作中，我们提出了一个通过概率电路实现可证实鲁棒学习推理的一致性预测框架（COLEP），其中包括一个数据驱动的学习组件，用于训练统计模型以学习不同的语义概念，以及一个用于编码知识并表征训练模型之间关系的推理组件。为了实现精确和高效的推理，我们在推理组件内部使用了概率电路（PCs）。在理论上，我们提供了完整的预测认证

    arXiv:2403.11348v1 Announce Type: cross  Abstract: Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage. In this work, we propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a reasoning component that encodes knowledge and characterizes the relationships among the trained models for logic reasoning. To achieve exact and efficient reasoning, we employ probabilistic circuits (PCs) within the reasoning component. Theoretically, we provide end-to-end certification of predict
    
[^113]: 独立强化学习用于合作竞争Agent：均场视角

    Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective

    [https://arxiv.org/abs/2403.11345](https://arxiv.org/abs/2403.11345)

    本文从均场视角研究了独立强化学习在合作竞争代理中的应用，提出了一种可实现纳什均衡的线性二次结构RL方法，并通过考虑无限代理数量的情况来解决有限人口环境中的非稳态性问题。

    

    在本文中，我们研究了分成团队的代理之间的强化学习（RL），每个团队内部存在合作，但不同团队之间存在非零和的竞争。为了开发一种可以明确实现纳什均衡的RL方法，我们专注于线性二次结构。此外，为了解决有限人口环境中由多智能体交互引起的非稳态性，我们考虑每个团队内代理数量无限的情况，即均场设置。这导致了一个广义和的LQ均场类型博弈（GS-MFTGs）。我们在标准逆可逆条件下表征了GS-MFTG的纳什均衡（NE）。然后证明了这个MFTG NE在有限人口博弈中为$\mathcal{O}(1/M)$-NE，其中$M$是每个团队中代理数量的下界。这些结构性结果推动了一个名为多玩家递进式自然Pol的算法。

    arXiv:2403.11345v1 Announce Type: cross  Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Pol
    
[^114]: 具有差分隐私的联邦迁移学习

    Federated Transfer Learning with Differential Privacy

    [https://arxiv.org/abs/2403.11343](https://arxiv.org/abs/2403.11343)

    本文提出了具有差分隐私的联邦迁移学习框架，通过利用多个异构源数据集的信息来增强对目标数据集的学习，同时考虑隐私约束。

    

    联邦学习越来越受到欢迎，数据异构性和隐私性是两个突出的挑战。在本文中，我们在联邦迁移学习框架内解决了这两个问题，旨在通过利用来自多个异构源数据集的信息来增强对目标数据集的学习，同时遵守隐私约束。我们严格制定了\textit{联邦差分隐私}的概念，为每个数据集提供隐私保证，而无需假设有一个受信任的中央服务器。在这个隐私约束下，我们研究了三个经典的统计问题，即单变量均值估计、低维线性回归和高维线性回归。通过研究极小值率并确定这些问题的隐私成本，我们展示了联邦差分隐私是已建立的局部和中央模型之间的一种中间隐私模型。

    arXiv:2403.11343v1 Announce Type: new  Abstract: Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of 
    
[^115]: 基于集成和测试增强的3D CT扫描进行Covid-19检测和Covid-19领域自适应

    Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans

    [https://arxiv.org/abs/2403.11338](https://arxiv.org/abs/2403.11338)

    该研究利用最新的CNN-based分割架构PDAtt-Unet，结合3D CNN骨干网络和集成方法，针对Covid-19检测和Covid-19领域自适应挑战进行了研究。

    

    自2019年底Covid-19出现以来，利用人工智能（AI）进行医学图像分析已经成为一个关键的研究领域，尤其是利用CT扫描成像进行疾病诊断。本文致力于第4届COV19D竞赛，重点关注Covid-19检测和Covid-19领域适应挑战。我们的方法集中在肺部分割和Covid-19感染分割，使用最近的基于CNN的分割架构PDAtt-Unet，同时分割肺部区域和感染区域。与传统方法不同，我们将输入切片（灰度图像）与分割的肺部和感染部位连接起来，生成类似于彩色通道的三个输入通道。此外，我们采用三种3D CNN骨干网络Customized Hybrid-DeCoVNet，以及预训练的3D-Resnet-18和3D-Resnet-50模型来训练两个挑战的Covid-19识别。此外，我们还探讨了集成方法。

    arXiv:2403.11338v1 Announce Type: cross  Abstract: Since the emergence of Covid-19 in late 2019, medical image analysis using artificial intelligence (AI) has emerged as a crucial research area, particularly with the utility of CT-scan imaging for disease diagnosis. This paper contributes to the 4th COV19D competition, focusing on Covid-19 Detection and Covid-19 Domain Adaptation Challenges. Our approach centers on lung segmentation and Covid-19 infection segmentation employing the recent CNN-based segmentation architecture PDAtt-Unet, which simultaneously segments lung regions and infections. Departing from traditional methods, we concatenate the input slice (grayscale) with segmented lung and infection, generating three input channels akin to color channels. Additionally, we employ three 3D CNN backbones Customized Hybrid-DeCoVNet, along with pretrained 3D-Resnet-18 and 3D-Resnet-50 models to train Covid-19 recognition for both challenges. Furthermore, we explore ensemble approaches 
    
[^116]: 基于图神经网络的网络因果效应双机器学习估计器

    Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects

    [https://arxiv.org/abs/2403.11332](https://arxiv.org/abs/2403.11332)

    提出了一种结合图神经网络和双机器学习的新方法，能够准确和高效地估计直接和同行效应，处理网络混杂因素，并一致地估计所需的因果效应

    

    我们的论文解决了在社交网络数据中推断因果效应的挑战，这些数据具有个体之间复杂的相互依赖关系，导致单位之间不独立、干扰（单位的结果受邻居的处理影响）以及引入来自邻近单位的额外混杂因素等问题。我们提出了一种将图神经网络和双机器学习相结合的创新方法，能够使用单个观测社交网络准确高效地估计直接和同伴效应。我们的方法利用图同构网络与双机器学习相结合，有效调整网络混杂因素并一致地估计所需的因果效应。我们展示了我们的估计器既具有渐近正态性又半参数高效。我们对三个半合成状态下的四种最先进基线方法进行了全面评估

    arXiv:2403.11332v1 Announce Type: new  Abstract: Our paper addresses the challenge of inferring causal effects in social network data, characterized by complex interdependencies among individuals resulting in challenges such as non-independence of units, interference (where a unit's outcome is affected by neighbors' treatments), and introduction of additional confounding factors from neighboring units. We propose a novel methodology combining graph neural networks and double machine learning, enabling accurate and efficient estimation of direct and peer effects using a single observational social network. Our approach utilizes graph isomorphism networks in conjunction with double machine learning to effectively adjust for network confounders and consistently estimate the desired causal effects. We demonstrate that our estimator is both asymptotically normal and semiparametrically efficient. A comprehensive evaluation against four state-of-the-art baseline methods using three semi-synth
    
[^117]: 机器学习在生态和水文学中的领域自适应潜力以提高模型的外推能力

    Potential of Domain Adaptation in Machine Learning in Ecology and Hydrology to Improve Model Extrapolability

    [https://arxiv.org/abs/2403.11331](https://arxiv.org/abs/2403.11331)

    本研究讨论了在生态和水文学领域中使用领域自适应技术来改善模型外推能力的潜力，弥补了目前模型普遍存在的地理外推问题。

    

    由于生态和水文地面实际观测的全球分布的异质性，机器学习模型在应用于未知位置时可能具有有限的适应性，也称为弱外推能力。领域自适应技术已经广泛应用于机器学习领域，例如图像分类，通过调整训练集和测试集之间的领域分布的差异或不一致性，可以提高模型的泛化能力。然而，尽管这些模型经常因地理外推能力问题而受到质疑，但这种方法很少被明确地应用于全球范围内生态和水文学中的机器学习模型。本文简要描述了当前生态和水文学中的机器学习模型在全球观测分布的代表性和由此造成的局限性方面的缺陷。

    arXiv:2403.11331v1 Announce Type: cross  Abstract: Due to the heterogeneity of the global distribution of ecological and hydrological ground-truth observations, machine learning models can have limited adaptability when applied to unknown locations, which is referred to as weak extrapolability. Domain adaptation techniques have been widely used in machine learning domains such as image classification, which can improve the model generalization ability by adjusting the difference or inconsistency of the domain distribution between the training and test sets. However, this approach has rarely been used explicitly in machine learning models in ecology and hydrology at the global scale, although these models have often been questioned due to geographic extrapolability issues. This paper briefly describes the shortcomings of current machine learning models of ecology and hydrology in terms of the global representativeness of the distribution of observations and the resulting limitations of 
    
[^118]: 通过将一个全局明确标注拆解成本地隐式多模态反馈来改进对话代理

    Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback

    [https://arxiv.org/abs/2403.11330](https://arxiv.org/abs/2403.11330)

    通过将全局明确标注拆解成本地隐式多模态反馈，提出了一种改进对话代理的方法，并在各种对话度量方面展现出一致的改进

    

    我们描述了一种方法，通过全局（即，对话级）奖励对齐基于LLM的对话代理，同时考虑到自然发生的多模态信号。在高层次上，我们的方法（名为GELI）通过将人类提供的全局明确（GE）会话级奖励拆分，利用本地隐式（LI）多模态奖励信号来跨模态地塑造奖励分解步骤。然后将这种分解的奖励模型作为标准RHLF流程的一部分，来改进基于LLM的对话代理。我们进行了定量和定性的人类研究，评估了我们的GELI方法的性能，并发现与基线方法相比，它在各种对话度量方面都表现出一致的改进。

    arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
    
[^119]: 在Transformer中进行推理-减少伪相关性和推理捷径

    Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts

    [https://arxiv.org/abs/2403.11314](https://arxiv.org/abs/2403.11314)

    这项研究探讨了如何在Transformer模型中进行逻辑推理，通过在数据集中引入证明来训练两种模型，成功避免了伪相关性和推理捷径。

    

    Transformer语言模型是用于处理自然语言的神经网络，在许多需要逻辑推理的任务中被使用。然而，Transformer模型可能会轻易学习到数据中的伪模式，从而绕过实际推理过程。本文研究了Transformer在多大程度上可以被训练来a) 近似命题逻辑推理，同时b) 避免通过训练数据中的伪相关性引起的已知推理捷径。为此，我们使用了一个数据集，其中真实性与问题中规则数量等之间存在已知的伪相关性。我们通过证明增强了数据，并训练了两个模型：一个生成式Transformer，WP-BART，它在问题及其完整证明上进行训练；一个神经符号模型，SIP-BART，它在单个证明步骤上训练，并将生成式Transformer模型BART与符号推理检查器结合起来。我们发现SIP-BART成功地避免了推理捷径。

    arXiv:2403.11314v1 Announce Type: cross  Abstract: Transformer language models are neural networks used for a wide variety of tasks concerning natural language, including some that also require logical reasoning. However, a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning. In this paper we investigate to what extent transformers can be trained to a) approximate reasoning in propositional logic while b) avoiding known reasoning shortcuts via spurious correlations in the training data. To do so, we use a dataset with known spurious correlation between truth and e.g. the number of rules in the problem. We augment the data with proofs, and train two models: a generative transformer, WP-BART, trained on problems and their whole proofs, and a neuro-symbolic model, SIP-BART, trained on individual proof steps and combining the generative transformer model BART with a symbolic proof checker. We find that SIP-BART succeeds in avoiding reasoning 
    
[^120]: SQ-LLaVA：自问自答的大型视觉-语言助手

    SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant

    [https://arxiv.org/abs/2403.11299](https://arxiv.org/abs/2403.11299)

    本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。

    

    最近视觉-语言模型的发展在经过视觉指导调整后，在视觉-语言任务中展现出显着的泛化能力。然而，预训练视觉编码器和大型语言模型之间的鸿沟成为整个网络的瓶颈。为了改善跨模态对齐，现有的工作通常考虑涵盖更广泛的视觉任务范围的更多视觉指导数据，对模型进行微调以用于问答，但这种操作成本较高。然而，图像包含大量上下文信息，但这一方面一直鲜有人探索。本文首次尝试利用视觉指导数据内部被忽视的上下文，训练模型自我训练'学习'如何提出高质量问题。通过这种方式，我们引入了一个名为SQ-LLaVA的新颖框架：自问自答的大型视觉-语言助手。SQ-LLaVA在生成灵活且有意义的图像方面表现出高效性。

    arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
    
[^121]: 对文本分类模型的一种修改的基于词显著性的对抗攻击

    A Modified Word Saliency-Based Adversarial Attack on Text Classification Models

    [https://arxiv.org/abs/2403.11297](https://arxiv.org/abs/2403.11297)

    本文介绍了一种修改的基于词显著性的对抗攻击方法，通过对模型最具影响力的词进行修改，旨在欺骗分类模型且保持语义连贯性，在提高攻击效果的同时避开了分类系统的检测。

    

    本文介绍了一种新型的针对文本分类模型的对抗攻击方法，称为修改的基于词显著性的对抗攻击（MWSAA）。该技术基于词显著性的概念，通过战略性地扰乱输入文本，旨在误导分类模型同时保持语义连贯性。通过改进传统的对抗攻击方法，MWSAA显著增强了其规避分类系统检测的效果。该方法首先通过显著性估计过程识别输入文本中的显著词，这些词对模型的决策过程具有最大影响。随后，这些显著词经过精心设计的修改，引导语义相似性度量来确保改变的文本保持连贯并保留其原始含义。在不同的文本分类数据集上进行了实证评估。

    arXiv:2403.11297v1 Announce Type: new  Abstract: This paper introduces a novel adversarial attack method targeting text classification models, termed the Modified Word Saliency-based Adversarial At-tack (MWSAA). The technique builds upon the concept of word saliency to strategically perturb input texts, aiming to mislead classification models while preserving semantic coherence. By refining the traditional adversarial attack approach, MWSAA significantly enhances its efficacy in evading detection by classification systems. The methodology involves first identifying salient words in the input text through a saliency estimation process, which prioritizes words most influential to the model's decision-making process. Subsequently, these salient words are subjected to carefully crafted modifications, guided by semantic similarity metrics to ensure that the altered text remains coherent and retains its original meaning. Empirical evaluations conducted on diverse text classification datasets
    
[^122]: 多关系图神经网络用于领域外链接预测

    Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction

    [https://arxiv.org/abs/2403.11292](https://arxiv.org/abs/2403.11292)

    提出了一种名为GOOD的图神经网络模型，设计用于解决领域外链接预测问题，采用一种新颖的多关系嵌入聚合设计概念。

    

    动态多关系图是一种用于包含不同类型实体和关系的数据的表达形式，在这种数据中，关系允许随时间变化。解决这类数据上的预测任务需要能够找到捕捉所涉及关系多样性以及动态演变的结构嵌入的能力。在这项工作中，我们为动态多关系图建立了一类新颖且具有挑战性的任务，涉及领域外链接预测，其中待预测的关系在输入图中不可用。然后，我们引入了一种新颖的图神经网络模型，命名为GOOD，专门设计来解决领域外泛化问题。GOOD引入了一种新颖的多关系嵌入聚合设计概念，基于这样一个观点，即当能够解开不同关系混合比例时，嵌入才是好的表示。

    arXiv:2403.11292v1 Announce Type: cross  Abstract: Dynamic multi-relational graphs are an expressive relational representation for data enclosing entities and relations of different types, and where relationships are allowed to vary in time. Addressing predictive tasks over such data requires the ability to find structure embeddings that capture the diversity of the relationships involved, as well as their dynamic evolution. In this work, we establish a novel class of challenging tasks for dynamic multi-relational graphs involving out-of-domain link prediction, where the relationship being predicted is not available in the input graph. We then introduce a novel Graph Neural Network model, named GOOD, designed specifically to tackle the out-of-domain generalization problem. GOOD introduces a novel design concept for multi-relation embedding aggregation, based on the idea that good representations are such when it is possible to disentangle the mixing proportions of the different relatio
    
[^123]: 物理设计图纸的高级知识提取、翻译和转换为CAD格式的深度学习

    Advanced Knowledge Extraction of Physical Design Drawings, Translation and conversion to CAD formats using Deep Learning

    [https://arxiv.org/abs/2403.11291](https://arxiv.org/abs/2403.11291)

    通过利用深度学习方法，本研究提出了一种创新的方法，利用物体检测模型和边缘检测算法提取物理设计图纸的信息，将其转换为CAD格式。

    

    物理形式下设计图纸的维护、存档和使用在不同行业中长时间以来变得繁琐。简单扫描图纸很难提取信息。将它们转换为计算机辅助设计（CAD）等数字格式，并进行必要的知识提取可以解决这个问题。将这些机器图纸转换为数字形式是一个关键挑战，需要先进的技术。本研究提出了一种利用深度学习方法的创新方法。该方法采用物体检测模型，如Yolov7、Faster R-CNN，来检测图像中存在的物理绘图对象，随后使用边缘检测算法，如Canny滤波器，从图纸区域提取和精化识别的线条，以及曲线检测技术来检测圆。还提取图纸中的装饰物（复杂形状）。为确保全面性的转换

    arXiv:2403.11291v1 Announce Type: cross  Abstract: The maintenance, archiving and usage of the design drawings is cumbersome in physical form in different industries for longer period. It is hard to extract information by simple scanning of drawing sheets. Converting them to their digital formats such as Computer-Aided Design (CAD), with needed knowledge extraction can solve this problem. The conversion of these machine drawings to its digital form is a crucial challenge which requires advanced techniques. This research proposes an innovative methodology utilizing Deep Learning methods. The approach employs object detection model, such as Yolov7, Faster R-CNN, to detect physical drawing objects present in the images followed by, edge detection algorithms such as canny filter to extract and refine the identified lines from the drawing region and curve detection techniques to detect circle. Also ornaments (complex shapes) within the drawings are extracted. To ensure comprehensive convers
    
[^124]: 通过数据增强来改善作者验证的方法

    Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation

    [https://arxiv.org/abs/2403.11265](https://arxiv.org/abs/2403.11265)

    通过引入合成示例的数据增强方法，可以改善在作者验证任务中对抗性攻击下的分类器预测。

    

    作者验证（AV）是一个文本分类任务，关注的是推断候选文本是由一个特定作者撰写还是由其他人撰写。已经显示许多AV系统容易受到敌对攻击的影响，其中恶意作者积极尝试欺骗分类器，方法是隐藏他们的写作风格，或者模仿另一位作者的风格。本文研究了将分类器训练集与（负面的）合成示例进行增强的潜在好处。这些合成示例是为了模仿感兴趣的作者的风格而生成的。我们分析了这种增强对在敌对环境下的AV任务中带来的分类器预测改进。具体来说，我们尝试了三种不同的生成器架构（一种基于循环神经网络，另一种基于小规模transformers，另一种基于流行的GPT模型）。

    arXiv:2403.11265v1 Announce Type: cross  Abstract: Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT mod
    
[^125]: 通过费曼的路径积分理解扩散模型

    Understanding Diffusion Models by Feynman's Path Integral

    [https://arxiv.org/abs/2403.11262](https://arxiv.org/abs/2403.11262)

    通过费曼的路径积分引入了扩散模型的新形式，对基于分数的生成模型提供了全面描述，并识别出连接随机和确定性采样方案的插值参数。

    

    基于分数的扩散模型在图像生成方面已被证明具有高效性，并且得到了广泛的应用；然而，随机和确定性（即概率流ODEs）采样方案之间性能差异的潜在因素仍然不清楚。我们引入了使用费曼的路径积分的扩散模型的新形式，这是最初为量子物理学开发的一种形式。我们发现这种形式提供了对基于分数的生成模型的全面描述，并展示了反向随机微分方程和损失函数的推导。这种形式容纳了一个插值参数，连接了随机和确定性采样方案，我们确定这个参数就像是量子物理学中的普朗克常数的对应物。这种类比使我们能够应用温策-克拉墨-布里渊（WKB）展开，这是量子物理学中一个成熟的技术。

    arXiv:2403.11262v1 Announce Type: cross  Abstract: Score-based diffusion models have proven effective in image generation and have gained widespread usage; however, the underlying factors contributing to the performance disparity between stochastic and deterministic (i.e., the probability flow ODEs) sampling schemes remain unclear. We introduce a novel formulation of diffusion models using Feynman's path integral, which is a formulation originally developed for quantum physics. We find this formulation providing comprehensive descriptions of score-based generative models, and demonstrate the derivation of backward stochastic differential equations and loss functions.The formulation accommodates an interpolating parameter connecting stochastic and deterministic sampling schemes, and we identify this parameter as a counterpart of Planck's constant in quantum physics. This analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a well-established technique in quantum ph
    
[^126]: 一种Lie群方法应用于黎曼批量归一化

    A Lie Group Approach to Riemannian Batch Normalization

    [https://arxiv.org/abs/2403.11261](https://arxiv.org/abs/2403.11261)

    本论文建立了一个Lie群上的统一框架，为黎曼批量归一化（RBN）技术提供了理论保证，并推广了现有的Lie群到对称正定流形上的三类参数化Lie群结构。

    

    计算机视觉和机器学习中存在许多应用中的流形值测量。最近的研究将深度神经网络(DNNs)扩展到流形，并且同时，归一化技术也已经被应用到几个流形，称为黎曼归一化。然而，大多数现有的黎曼归一化方法是以临时方式推导出来的，仅适用于特定的流形。本文在Lie群上建立了一个统一的黎曼批量归一化（RBN）技术框架。我们的框架提供了控制黎曼平均值和方差的理论保证。在经验上，我们专注于具有三种不同类型Lie群结构的对称正定(SPD)流形。利用变形概念，我们将现有的SPD流形上的Lie群推广成三类参数化Lie群。

    arXiv:2403.11261v1 Announce Type: cross  Abstract: Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization l
    
[^127]: 移动边缘计算中应用部署问题的基于学习的解决方案

    A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty

    [https://arxiv.org/abs/2403.11259](https://arxiv.org/abs/2403.11259)

    通过机器学习模型将用户请求分配给服务器，以解决在移动边缘计算中应用部署问题的二阶段随机规划。

    

    在移动边缘计算服务器中放置应用程序是一个复杂的挑战，涉及许多服务器、用户及其请求。现有算法需要很长时间解决具有重大不确定性情景的高维问题。因此，需要一种有效的方法来最大化服务质量，同时考虑所有技术约束。其中一种方法是机器学习，它模拟了在边缘服务器中部署应用程序的最佳解决方案。机器学习模型预计将学习如何根据用户和服务器的空间位置将用户请求分配给服务器。本研究将问题构建为二阶段随机规划。通过变化参数如用户位置、请求速率和解决优化模型生成足够数量的训练记录。然后，基于每个用户距离可用服务器的距离特征，

    arXiv:2403.11259v1 Announce Type: cross  Abstract: Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and 
    
[^128]: 基于简单2D卷积神经网络的COVID-19检测方法

    Simple 2D Convolutional Neural Network-based Approach for COVID-19 Detection

    [https://arxiv.org/abs/2403.11230](https://arxiv.org/abs/2403.11230)

    提出了一种针对CT扫描图像的高级空间切片特征学习框架，通过过滤异常数据和减少数据冗余，提高了分析效果。

    

    本研究探讨了利用深度学习技术分析肺部计算机断层扫描（CT）图像的方法。传统的深度学习方法在处理CT图像中的不同切片计数和分辨率方面面临挑战，这种差异来自于使用各种扫描设备。通常情况下，对单个切片进行预测，然后将其组合以获得全面的结果。然而，这种方法并没有包含特定于每个切片的学习特征，导致效果上的妥协。为了解决这些挑战，我们提出了一种专门为CT扫描定制的高级空间切片特征学习（SSFL++）框架。它旨在过滤出整个CT扫描中的异常数据，通过减少70%的数据冗余，使我们能够选择进行分析的关键空间切片特征。此外，我们引入了一种基于核密度的切片采样（KDS）方法，以增强训练稳定性。

    arXiv:2403.11230v1 Announce Type: cross  Abstract: This study explores the use of deep learning techniques for analyzing lung Computed Tomography (CT) images. Classic deep learning approaches face challenges with varying slice counts and resolutions in CT images, a diversity arising from the utilization of assorted scanning equipment. Typically, predictions are made on single slices which are then combined for a comprehensive outcome. Yet, this method does not incorporate learning features specific to each slice, leading to a compromise in effectiveness. To address these challenges, we propose an advanced Spatial-Slice Feature Learning (SSFL++) framework specifically tailored for CT scans. It aims to filter out out-of-distribution (OOD) data within the entire CT scan, allowing us to select essential spatial-slice features for analysis by reducing data redundancy by 70\%. Additionally, we introduce a Kernel-Density-based slice Sampling (KDS) method to enhance stability during training a
    
[^129]: 从文字中提取临床标记的廉价方法

    Cheap Ways of Extracting Clinical Markers from Texts

    [https://arxiv.org/abs/2403.11227](https://arxiv.org/abs/2403.11227)

    该论文研究了从文本中提取临床标记的廉价方法，比较了传统机器学习方法和大型语言模型对于提取亮点和生成摘要的效果。

    

    本文描述了UniBuc考古团队为CLPsych 2024共享任务所进行的工作，该任务涉及在文本中寻找支持分配的自杀风险水平的证据。需要两种类型的证据：亮点（提取文本中的相关片段）和摘要（将证据聚合成综合）。我们的工作重点是评估大型语言模型（LLM），而不是一种更节省内存和资源的替代方法。第一种方法采用了传统的机器学习（GOML）管道，包括tf-idf向量化器和逻辑回归分类器，其代表性特征用于提取相关的亮点。第二种更消耗资源的方法使用LLM生成摘要，并由思维链指导提供指示临床标记的文本序列。

    arXiv:2403.11227v1 Announce Type: new  Abstract: This paper describes the work of the UniBuc Archaeology team for CLPsych's 2024 Shared Task, which involved finding evidence within the text supporting the assigned suicide risk level. Two types of evidence were required: highlights (extracting relevant spans within the text) and summaries (aggregating evidence into a synthesis). Our work focuses on evaluating Large Language Models (LLM) as opposed to an alternative method that is much more memory and resource efficient. The first approach employs a good old-fashioned machine learning (GOML) pipeline consisting of a tf-idf vectorizer with a logistic regression classifier, whose representative features are used to extract relevant highlights. The second, more resource intensive, uses an LLM for generating the summaries and is guided by chain-of-thought to provide sequences of text indicating clinical markers.
    
[^130]: CPA-Enhancer：链式思维驱动自适应增强器用于未知退化下的目标检测

    CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations

    [https://arxiv.org/abs/2403.11220](https://arxiv.org/abs/2403.11220)

    提出了一种用于未知退化下目标检测的链式思维驱动自适应增强器CPA-Enhancer，并将其集成到通用检测器中，有效提升受损图像的检测性能

    

    目前，已经广泛研究了在已知单一退化情况下的目标检测方法。然而，现有方法需要先验知识来确定退化类型，并为每种类型训练一个单独的模型，从而限制了它们在不可预测环境中的实际应用。为了解决这一挑战，我们提出了一种链式思维（CoT）驱动的自适应增强器CPA-Enhancer，用于未知退化情况下的目标检测。具体而言，CPA-Enhancer在CoT提示的逐步指导下逐步调整其增强策略，这些提示编码了与退化相关的信息。据我们所知，这是首个利用CoT提示进行目标检测任务的工作。总的来说，CPA-Enhancer是一个即插即用的增强模型，可以集成到任何通用检测器中，在不事先知道退化类型的情况下，在受损图像上实现显著提升。实验结果表明，CPA-E

    arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
    
[^131]: CBR - 通过检索加密网络流量增强自适应分类的方法

    CBR - Boosting Adaptive Classification By Retrieval of Encrypted Network Traffic with Out-of-distribution

    [https://arxiv.org/abs/2403.11206](https://arxiv.org/abs/2403.11206)

    本文提出了一种基于ANN的自适应分类方法CBR，可以在不重新训练模型的情况下有效识别新的和现有的类别。

    

    加密网络流量分类从不同的角度和不同的目标解决问题。其中一个常见的方法是使用基于机器学习或深度学习的解决方案来处理固定数量的类别，导致当输入一个未知类别时发生错误分类。处理未知类别的一个解决方案是重新训练模型，但每次模型变得过时都重新训练既耗费资源又耗时。因此，迫切需要让分类模型能够动态地检测和适应新类别，而无需重新训练，而是能够使用少量样本学习来检测新类别。在本文中，我们介绍了一种新颖的基于检索的自适应分类方法CBR，用于加密网络流量分类。我们的新方法基于基于人工神经网络的方法，可以有效地识别新的和现有的类别，而无需重新训练模型。

    arXiv:2403.11206v1 Announce Type: new  Abstract: Encrypted network traffic Classification tackles the problem from different approaches and with different goals. One of the common approaches is using Machine learning or Deep Learning-based solutions on a fixed number of classes, leading to misclassification when an unknown class is given as input. One of the solutions for handling unknown classes is to retrain the model, however, retraining models every time they become obsolete is both resource and time-consuming. Therefore, there is a growing need to allow classification models to detect and adapt to new classes dynamically, without retraining, but instead able to detect new classes using few shots learning [1]. In this paper, we introduce Adaptive Classification By Retrieval CBR, a novel approach for encrypted network traffic classification. Our new approach is based on an ANN-based method, which allows us to effectively identify new and existing classes without retraining the model
    
[^132]: 通过合成中间标签进行分区神经网络训练

    Partitioned Neural Network Training via Synthetic Intermediate Labels

    [https://arxiv.org/abs/2403.11204](https://arxiv.org/abs/2403.11204)

    该研究提出了一种通过将模型分区到不同GPU上，并生成合成中间标签来训练各个部分的方法，以缓解大规模神经网络训练中的内存和计算压力。

    

    大规模神经网络架构的普及，特别是深度学习模型，对资源密集型训练提出了挑战。 GPU 内存约束已经成为训练这些庞大模型的一个明显瓶颈。现有策略，包括数据并行、模型并行、流水线并行和完全分片数据并行，提供了部分解决方案。 特别是模型并行允许将整个模型分布在多个 GPU 上，但随后的这些分区之间的数据通信减慢了训练速度。此外，为在每个 GPU 上存储辅助参数所需的大量内存开销增加了计算需求。 本研究主张不使用整个模型进行训练，而是将模型分区到 GPU 上，并生成合成中间标签来训练各个部分。 通过随机过程生成的这些标签减缓了训练中的内存和计算压力。

    arXiv:2403.11204v1 Announce Type: cross  Abstract: The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training. GPU memory constraints have become a notable bottleneck in training such sizable models. Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions. Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training. Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands. Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments. These labels, produced through a random process, mitigate me
    
[^133]: 图单元消息传递

    Graph Unitary Message Passing

    [https://arxiv.org/abs/2403.11199](https://arxiv.org/abs/2403.11199)

    提出了一种名为GUMP的图单元消息传递方法，通过应用单元邻接矩阵来缓解图神经网络中的过度压缩问题。

    

    消息传递机制是图神经网络在各种应用中取得成功的原因，但也带来了过度压缩的问题。最近的研究通过改善图谱的重连技术、破坏图中的结构偏见来抵制过度压缩，然而在过度压缩度量方面对过度压缩的改进有所限制。受到单元RNN的启发，我们提出了图单元消息传递（GUMP），通过应用单元邻接矩阵进行消息传递来缓解图神经网络中的过度压缩问题。为设计GUMP，首先提出了一种转换方法，使普通图具有单元邻接矩阵并保持其结构偏差。然后，通过利用单元邻接矩阵的固有结构实现单位化投影算法获得单元邻接矩阵，并允许GUMP是置换等变的。实验结果表明了GUMP在改善各种应用任务上性能的有效性。

    arXiv:2403.11199v1 Announce Type: cross  Abstract: Message passing mechanism contributes to the success of GNNs in various applications, but also brings the oversquashing problem. Recent works combat oversquashing by improving the graph spectrums with rewiring techniques, disrupting the structural bias in graphs, and having limited improvement on oversquashing in terms of oversquashing measure. Motivated by unitary RNN, we propose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs by applying unitary adjacency matrix for message passing. To design GUMP, a transformation is first proposed to make general graphs have unitary adjacency matrix and keep its structural bias. Then, unitary adjacency matrix is obtained with a unitary projection algorithm, which is implemented by utilizing the intrinsic structure of unitary adjacency matrix and allows GUMP to be permutation-equivariant. Experimental results show the effectiveness of GUMP in improving the performance on vari
    
[^134]: 基于usfAD的有效未知攻击检测为重点的IDS框架

    usfAD Based Effective Unknown Attack Detection Focused IDS Framework

    [https://arxiv.org/abs/2403.11180](https://arxiv.org/abs/2403.11180)

    提出了两种基于半监督学习的IDS策略，以应对IDS在检测零日或未知攻击方面的不足

    

    现今各种网络系统的快速扩张，包括物联网（IoT）和工业物联网（IIoT），导致了越来越多的网络威胁。确保对抗这些威胁的稳健保护需要实施一种有效的入侵检测系统（IDS）。研究人员十多年来一直深入研究监督机器学习技术，以开发IDS来对正常和攻击流量进行分类。然而，使用监督学习构建有效的IDS模型需要大量良性和攻击样本。从现实场景中收集足够数量的攻击样本是不可能的，因为网络攻击偶尔发生。此外，训练和测试于已知数据集上的IDS无法检测到零日或未知攻击，原因是攻击模式的迅速演变。为了解决这一挑战，我们提出了两种基于半监督学习的IDS策略。

    arXiv:2403.11180v1 Announce Type: cross  Abstract: The rapid expansion of varied network systems, including the Internet of Things (IoT) and Industrial Internet of Things (IIoT), has led to an increasing range of cyber threats. Ensuring robust protection against these threats necessitates the implementation of an effective Intrusion Detection System (IDS). For more than a decade, researchers have delved into supervised machine learning techniques to develop IDS to classify normal and attack traffic. However, building effective IDS models using supervised learning requires a substantial number of benign and attack samples. To collect a sufficient number of attack samples from real-life scenarios is not possible since cyber attacks occur occasionally. Further, IDS trained and tested on known datasets fails in detecting zero-day or unknown attacks due to the swift evolution of attack patterns. To address this challenge, we put forth two strategies for semi-supervised learning based IDS wh
    
[^135]: 先验依赖性分析基于函数逼近的后验抽样强化学习

    Prior-dependent analysis of posterior sampling reinforcement learning with function approximation

    [https://arxiv.org/abs/2403.11175](https://arxiv.org/abs/2403.11175)

    该研究提出了首个先验依赖性贝叶斯遗憾上界，并对后验抽样强化学习进行了改进分析，提出了一个新的上界结果，实现了对先前基准的方法论提升。

    

    这项研究在对线性混合MDPs建模的函数逼近强化学习（RL）中推进了随机探索。我们为具有函数逼近的RL建立了首个先验依赖性贝叶斯遗憾上界；并且改进了用于后验抽样强化学习（PSRL）的贝叶斯遗憾分析，提出了一个上界为${\mathcal{O}}(d\sqrt{H^3 T \log T})$的结果，其中$d$表示转移核的维度，$H$表示规划视野，$T$表示总交互次数。 这表示通过优化$\mathcal{O}(\sqrt{\log T})$因子，我们在之前针对线性混合MDPs的基准（Osband和Van Roy，2014）上取得了方法论上的提升。我们的方法，利用价值定向模型学习的视角，引入了一种解耦论证和方差缩减技术，超越了传统分析依赖于置信区间和集中不等式的限制。

    arXiv:2403.11175v1 Announce Type: cross  Abstract: This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\mathcal{O}(\sqrt{\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to f
    
[^136]: 多目标进化神经架构搜索用于循环神经网络

    Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks

    [https://arxiv.org/abs/2403.11173](https://arxiv.org/abs/2403.11173)

    提出了一种多目标进化神经架构搜索方法，针对循环神经网络设计，填补了现代NAS方法中忽视的领域。

    

    人工神经网络（NN）架构设计是一项不容易且耗时的任务，通常需要高水平的人类专业知识。神经架构搜索（NAS）旨在自动化NN架构的设计，并已证明在自动查找性能优于人工设计的NN架构方面是成功的。NN架构性能可以基于多个目标进行量化，其中包括模型准确度和一些NN架构复杂性目标等。大多数考虑多个目标进行NN架构性能评估的现代NAS方法关注于自动化前馈NN架构设计，而忽略了多目标自动化循环神经网络（RNN）架构设计。RNN对于建模序列数据集非常重要，在自然语言处理领域内占有显著地位。

    arXiv:2403.11173v1 Announce Type: cross  Abstract: Artificial neural network (NN) architecture design is a nontrivial and time-consuming task that often requires a high level of human expertise. Neural architecture search (NAS) serves to automate the design of NN architectures and has proven to be successful in automatically finding NN architectures that outperform those manually designed by human experts. NN architecture performance can be quantified based on multiple objectives, which include model accuracy and some NN architecture complexity objectives, among others. The majority of modern NAS methods that consider multiple objectives for NN architecture performance evaluation are concerned with automated feed forward NN architecture design, which leaves multi-objective automated recurrent neural network (RNN) architecture design unexplored. RNNs are important for modeling sequential datasets, and prominent within the natural language processing domain. It is often the case in real 
    
[^137]: 铅笔：无需非共谋假设的私密可扩展协作学习

    Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption

    [https://arxiv.org/abs/2403.11166](https://arxiv.org/abs/2403.11166)

    Pencil是第一个解决协作神经网络训练数据隐私挑战的框架，兼顾模型和数据隐私，不依赖非共谋假设。

    

    随着对数据隐私的关注不断提升，协作神经网络训练面临着重大挑战，其中数据所有权以及模型训练/部署责任分属于不同实体。我们的社区在解决这一挑战方面做出了重大贡献，提出了各种方法，如联合学习（FL）和基于同态加密（HE）和安全多方计算（MPC）等密码构造的隐私保护机器学习。然而，FL完全忽视了模型隐私，HE的可扩展性有限（仅限于一家数据提供者）。尽管最先进的MPC框架提供了合理的吞吐量并同时确保了模型/数据隐私，但它们依赖于在计算服务器上假设的关键非共谋假设，放松这一假设仍然是一个未解之谜。在本文中，我们提出了Pencil，这是第一个用于协作训练的私密训练框架。

    arXiv:2403.11166v1 Announce Type: cross  Abstract: The escalating focus on data privacy poses significant challenges for collaborative neural network training, where data ownership and model training/deployment responsibilities reside with distinct entities. Our community has made substantial contributions to addressing this challenge, proposing various approaches such as federated learning (FL) and privacy-preserving machine learning based on cryptographic constructs like homomorphic encryption (HE) and secure multiparty computation (MPC). However, FL completely overlooks model privacy, and HE has limited extensibility (confined to only one data provider). While the state-of-the-art MPC frameworks provide reasonable throughput and simultaneously ensure model/data privacy, they rely on a critical non-colluding assumption on the computing servers, and relaxing this assumption is still an open problem.   In this paper, we present Pencil, the first private training framework for collabora
    
[^138]: 面向大规模数据计算的统计方法选择性综述：分布式计算、子采样和小批量技术

    A Selective Review on Statistical Methods for Massive Data Computation: Distributed Computing, Subsampling, and Minibatch Techniques

    [https://arxiv.org/abs/2403.11163](https://arxiv.org/abs/2403.11163)

    该论文选择性综述了大规模数据计算的统计方法，主要集中在分布式计算、子采样以及小批量梯度技术这三类统计计算方法。

    

    本文提供了对大规模数据分析的统计计算方法进行选择性综述。过去几十年中，已经迅速发展了大量用于大规模数据计算的统计方法。在本工作中，我们着重研究了三类统计计算方法：（1）分布式计算，（2）子采样方法，和（3）小批量梯度技术。第一类文献关于分布式计算，重点是在数据集太大无法被单台计算机轻松处理的情况下。在这种情况下，必须使用一个具有多台计算机的分布式计算系统。第二类文献关于子采样方法，关注的是样本数据集的大小足够小，可以放在一台计算机上，但太大以至于无法整体处理其内存。最后一类文献研究了小批量梯度方法...

    arXiv:2403.11163v1 Announce Type: cross  Abstract: This paper presents a selective review of statistical computation methods for massive data analysis. A huge amount of statistical methods for massive data computation have been rapidly developed in the past decades. In this work, we focus on three categories of statistical computation methods: (1) distributed computing, (2) subsampling methods, and (3) minibatch gradient techniques. The first class of literature is about distributed computing and focuses on the situation, where the dataset size is too huge to be comfortably handled by one single computer. In this case, a distributed computation system with multiple computers has to be utilized. The second class of literature is about subsampling methods and concerns about the situation, where the sample size of dataset is small enough to be placed on one single computer but too large to be easily processed by its memory as a whole. The last class of literature studies those minibatch g
    
[^139]: CGI-DM：通过对比梯度反转进行扩散模型的数字版权认证

    CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion

    [https://arxiv.org/abs/2403.11162](https://arxiv.org/abs/2403.11162)

    该方法提出了一种通过对比梯度反转实现扩散模型的数字版权认证的新方法，通过利用预训练模型和微调模型之间的概念差异来恢复图像的缺失细节。

    

    扩散模型（DMs）已经发展成为先进的图像生成工具，特别是对于少样本生成，在这种情况下，一个预训练模型被微调到一小组图像上以捕捉特定风格或对象。尽管它们取得了成功，但人们对潜在的版权侵犯问题表示担忧，这主要源于在此过程中使用未经授权的数据。为了应对这一问题，我们提出了通过对比梯度反转进行扩散模型（CGI-DM），这是一种新颖的方法，具有生动的视觉图像，用于数字版权认证。我们的方法涉及删除图像的部分信息，并通过利用预训练模型和微调模型之间的概念差异恢复缺失的细节。我们将两个模型的潜在变量之间的差异构建为给定相同输入图像时的KL散度，可以通过蒙特卡罗采样和投影梯度下降（PGD）进行最大化。原图像和

    arXiv:2403.11162v1 Announce Type: cross  Abstract: Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and
    
[^140]: Mamba在时间序列预测中的有效性如何？

    Is Mamba Effective for Time Series Forecasting?

    [https://arxiv.org/abs/2403.11144](https://arxiv.org/abs/2403.11144)

    Mamba模型作为一种状态空间模型在时间序列预测中具有捕捉复杂依赖关系、近线性复杂度以及性能优势的潜力。

    

    在时间序列预测（TSF）领域中，由于Transformer模型能够聚焦全局环境，有效捕捉时间序列中长距离依赖关系以及辨别多变量之间的相关性，因此它一直展现出强大的性能。然而，由于Transformer模型的低效率和关于其捕捉依赖关系能力的质疑，对Transformer架构的不断完善工作仍在进行中。最近，状态空间模型（SSMs）如Mamba因其能够像Transformer一样捕捉序列中的复杂依赖关系，同时又保持近线性的复杂度而备受推崇。在文本和图像任务中，基于Mamba的模型可以提高性能并节约成本，实现双赢局面。这引起了我们对探索SSM在TSF任务中潜力的兴趣。在本文中，我们介绍了两种基于SSM的简单模型，S-Mamba和......

    arXiv:2403.11144v1 Announce Type: new  Abstract: In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and 
    
[^141]: 基于高斯过程回归的机器学习系统可靠性分析

    Machine learning-based system reliability analysis with Gaussian Process Regression

    [https://arxiv.org/abs/2403.11125](https://arxiv.org/abs/2403.11125)

    本文提出了基于高斯过程回归的机器学习系统可靠性分析方法，并通过几个定理探讨了最优学习策略，包括考虑和忽略样本之间的相关性以及顺序多个训练样本增益的理论最优策略。

    

    arXiv:2403.11125v1 公告类型: 交叉 摘要: 基于机器学习的可靠性分析方法在计算效率和准确性方面取得了巨大进展。最近，已经提出许多有效的学习策略来增强计算性能。然而，其中很少有人探讨了理论上的最优学习策略。在这篇文章中，我们提出了几个定理来促进这种探索。具体来说，详细阐述了考虑和忽略候选设计样本之间相关性的情况。此外，我们证明了众所周知的 U 学习函数可以重新制定为在忽略 Kriging 相关性的情况下的最优学习函数。此外，还通过带有相应损失函数的贝叶斯估计数学上探讨了顺序多个训练样本增益的理论上最优学习策略。模拟结果表明最优学习策略……

    arXiv:2403.11125v1 Announce Type: cross  Abstract: Machine learning-based reliability analysis methods have shown great advancements for their computational efficiency and accuracy. Recently, many efficient learning strategies have been proposed to enhance the computational performance. However, few of them explores the theoretical optimal learning strategy. In this article, we propose several theorems that facilitates such exploration. Specifically, cases that considering and neglecting the correlations among the candidate design samples are well elaborated. Moreover, we prove that the well-known U learning function can be reformulated to the optimal learning function for the case neglecting the Kriging correlation. In addition, the theoretical optimal learning strategy for sequential multiple training samples enrichment is also mathematically explored through the Bayesian estimate with the corresponding lost functions. Simulation results show that the optimal learning strategy consid
    
[^142]: 种群强化学习的相位多样性优化

    Phasic Diversity Optimization for Population-Based Reinforcement Learning

    [https://arxiv.org/abs/2403.11114](https://arxiv.org/abs/2403.11114)

    引入了Phasic Diversity Optimization (PDO)算法，采用种群训练框架，将奖励和多样性训练分为不同阶段，并在辅助阶段实现激进的多样性优化。

    

    本文介绍了Phasic Diversity Optimization (PDO)算法，这是一个基于种群的训练框架，将奖励和多样性训练分为不同的阶段，而不是优化多目标函数。在辅助阶段，表现较差的agent通过决策者进行多样化，不会取代存档中更好的agent。奖励和多样性的解耦使我们能够在辅助阶段使用激进的多样性优化，而不会降低性能。

    arXiv:2403.11114v1 Announce Type: cross  Abstract: Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degra
    
[^143]: 自监督量化感知知识蒸馏

    Self-Supervised Quantization-Aware Knowledge Distillation

    [https://arxiv.org/abs/2403.11106](https://arxiv.org/abs/2403.11106)

    提出了一种自监督量化感知知识蒸馏(SQAKD)框架，可以在不需要标记的监督情况下，同时最小化全精度和低比特模型之间的KL损失以及量化的离散化误差，从而避免了繁琐的超参数调整和复杂的训练过程。

    

    遗憾地，现有工作将知识蒸馏应用于量化感知训练(QAT)需要繁琐的超参数调整来平衡不同损失项的权重，假定有标记的训练数据可用，并且需要复杂、计算密集的训练程序以获得良好的性能。为了解决这些限制，本文提出了一种新颖的自监督量化感知知识蒸馏(SQAKD)框架。SQAKD首先统一了各种量化函数的前向和反向动态，使其可以灵活地整合各种QAT工作。然后，它将QAT形式化为一个联合优化问题，同时最小化了用于KD的全精度模型和低比特模型之间的KL损失，以及用于量化的离散化误差，而无需来自标签的监督。

    arXiv:2403.11106v1 Announce Type: cross  Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. However, existing works applying KD to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating various QAT works. Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for KD and the discretization error for quantization, without supervision from labels. A comprehensive e
    
[^144]: ProgGen:通过自反大语言模型逐步生成命名实体识别数据集

    ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models

    [https://arxiv.org/abs/2403.11103](https://arxiv.org/abs/2403.11103)

    通过让大语言模型自省特定领域，生成领域相关属性并创建属性丰富的训练数据，同时绕过复杂结构的挑战，实现了生成命名实体识别数据集的创新策略。

    

    虽然大语言模型（LLMs）在跨领域上表现出卓越的适应性，但这些模型在结构化知识提取任务（如命名实体识别NER）方面经常表现不佳。本文探讨了一种创新的、具有成本效益的策略，利用具有适度NER能力的LLMs来生成优秀的NER数据集。我们的方法不同于基本的类条件提示，而是指导LLMs对特定领域进行自我反思，从而生成具有领域相关属性（例如影评的类别和情感）的属性丰富的训练数据。此外，我们预先生成实体术语，然后围绕这些实体开发NER上下文数据，有效规避了LLMs对复杂结构的挑战。我们在通用和专业领域展开的实验显示，相对于传统数据生成方法，性能得到了显著提升，同时成本更低。

    arXiv:2403.11103v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being mor
    
[^145]: 在修剪的循环神经网络层中保持性能的图扩展

    Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance

    [https://arxiv.org/abs/2403.11100](https://arxiv.org/abs/2403.11100)

    图扩展的性质是该研究的关键，研究发现在修剪的循环神经网络层中保持图的扩展性能可以维持RNN和LSTM的分类准确性。

    

    图的扩展性质指的是其强连通性和稀疏性。已经报告称可以对深度神经网络进行修剪，使其在保持性能的同时具有高度的稀疏性。这种修剪对于在资源受限平台上使用循环神经网络执行实时序列学习任务至关重要。我们对诸如RNN和LSTM的循环网络进行修剪，保持底层图的较大谱间隔，并确保它们的逐层扩展性质。我们还从其二分图层面研究时间展开的循环网络图的性质。针对基准序列MNIST、CIFAR-10和Google语音命令数据的实验结果表明，扩展器图形属性是保持RNN和LSTM分类准确性的关键。

    arXiv:2403.11100v1 Announce Type: new  Abstract: Expansion property of a graph refers to its strong connectivity as well as sparseness. It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance. Such pruning is essential for performing real time sequence learning tasks using recurrent neural networks in resource constrained platforms. We prune recurrent networks such as RNNs and LSTMs, maintaining a large spectral gap of the underlying graphs and ensuring their layerwise expansion properties. We also study the time unfolded recurrent network graphs in terms of the properties of their bipartite layers. Experimental results for the benchmark sequence MNIST, CIFAR-10, and Google speech command data show that expander graph properties are key to preserving classification accuracy of RNN and LSTM.
    
[^146]: 基于学习的双边队列定价和匹配

    Learning-Based Pricing and Matching for Two-Sided Queues

    [https://arxiv.org/abs/2403.11093](https://arxiv.org/abs/2403.11093)

    设计定价和匹配算法以最大化平台利润，在未知需求和供应函数下，保持顾客和服务器队列长度低于阈值

    

    我们考虑一个具有多种类型顾客和服务器的动态系统。每种等待的顾客或服务器加入一个单独的队列，形成一个具有顾客队列和服务器队列的二部图。平台可以匹配服务器和顾客，如果它们的类型是兼容的。匹配的对将离开系统。平台将根据顾客的类型收取一个价格，当它们到达时，并根据其类型向服务器支付一个价格。每个队列的到达率取决于某些未知的需求或供应函数按价格确定。我们的目标是设计定价和匹配算法，以最大化平台在未知需求和供应函数下的利润，同时保持顾客和服务器的队列长度低于预定阈值。这个系统可以用来建模像乘车共享市场这样的双边市场，有乘客和司机。挑战在于

    arXiv:2403.11093v1 Announce Type: cross  Abstract: We consider a dynamic system with multiple types of customers and servers. Each type of waiting customer or server joins a separate queue, forming a bipartite graph with customer-side queues and server-side queues. The platform can match the servers and customers if their types are compatible. The matched pairs then leave the system. The platform will charge a customer a price according to their type when they arrive and will pay a server a price according to their type. The arrival rate of each queue is determined by the price according to some unknown demand or supply functions. Our goal is to design pricing and matching algorithms to maximize the profit of the platform with unknown demand and supply functions, while keeping queue lengths of both customers and servers below a predetermined threshold. This system can be used to model two-sided markets such as ride-sharing markets with passengers and drivers. The difficulties of the pr
    
[^147]: 大脑开关：通过基于神经网络驱动的线速流量分析实现先进智能网络数据平面

    Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via NN-Driven Traffic Analysis at Line-Speed

    [https://arxiv.org/abs/2403.11090](https://arxiv.org/abs/2403.11090)

    本文介绍了一种名为BoS的方法，通过在数据平面上实现神经网络驱动的流量分析，以推动智能网络数据平面的发展。

    

    新兴的可编程网络引发了对智能网络数据平面（INDP）的重要研究，实现了基于学习的线速流量分析。先前的INDP重点研究在数据平面上部署树/森林模型。我们观察到基于树的INDP方法存在根本局限性：虽然在数据平面上可以表示更大的树/森林表，但数据平面上可计算的流特征受硬件限制。本文介绍了BoS，通过实现神经网络（NN）驱动的线速流量分析来推动INDP的边界。许多类型的NN（如循环神经网络（RNN）和transformers）专为处理序列数据而设计，相对于基于树的模型具有优势，因为它们可以将原始网络数据作为输入，而无需即时进行复杂的特征计算。

    arXiv:2403.11090v1 Announce Type: cross  Abstract: The emerging programmable networks sparked significant research on Intelligent Network Data Plane (INDP), which achieves learning-based traffic analysis at line-speed. Prior art in INDP focus on deploying tree/forest models on the data plane. We observe a fundamental limitation in tree-based INDP approaches: although it is possible to represent even larger tree/forest tables on the data plane, the flow features that are computable on the data plane are fundamentally limited by hardware constraints. In this paper, we present BoS to push the boundaries of INDP by enabling Neural Network (NN) driven traffic analysis at line-speed. Many types of NNs (such as Recurrent Neural Network (RNN), and transformers) that are designed to work with sequential data have advantages over tree-based models, because they can take raw network data as input without complex feature computations on the fly. However, the challenge is significant: the recurrent
    
[^148]: 结合高阶结构信息进行图聚类

    Incorporating Higher-order Structural Information for Graph Clustering

    [https://arxiv.org/abs/2403.11087](https://arxiv.org/abs/2403.11087)

    该论文提出了一种新颖的图聚类网络，利用图的高阶结构信息，并设计了一个图互信息极大化模块，有效地最大化了图级和节点级表示之间的互信息。

    

    聚类在数据挖掘中具有深远意义。近年来，图卷积网络(GCN)作为一种强大的深度聚类工具崭露头角，集成了图结构信息和节点属性。然而，大多数现有方法忽略了图的高阶结构信息。显然，位于同一聚类中的节点可以建立远距连接。此外，最近的深度聚类方法通常应用自监督模块来监控模型的训练过程，仅关注节点属性而忽略图结构。本文提出了一种新颖的图聚类网络，充分利用了图的结构信息。为了捕捉高阶结构信息，我们设计了一个图互信息极大化模块，有效地最大化了图级和节点级表示之间的互信息，并采用了一个包含三元自监督模块的模型。

    arXiv:2403.11087v1 Announce Type: new  Abstract: Clustering holds profound significance in data mining. In recent years, graph convolutional network (GCN) has emerged as a powerful tool for deep clustering, integrating both graph structural information and node attributes. However, most existing methods ignore the higher-order structural information of the graph. Evidently, nodes within the same cluster can establish distant connections. Besides, recent deep clustering methods usually apply a self-supervised module to monitor the training process of their model, focusing solely on node attributes without paying attention to graph structure. In this paper, we propose a novel graph clustering network to make full use of graph structural information. To capture the higher-order structural information, we design a graph mutual infomax module, effectively maximizing mutual information between graph-level and node-level representations, and employ a trinary self-supervised module that includ
    
[^149]: RobustSentEmbed：使用对抗自监督对比学习的稳健句子嵌入

    RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning

    [https://arxiv.org/abs/2403.11082](https://arxiv.org/abs/2403.11082)

    RobustSentEmbed是一个自监督句子嵌入框架，通过对抗对比学习提高了文本表示任务中的泛化性和稳健性，实现了在对抗攻击中的优越表现。

    

    预训练语言模型（PLMs）在各种自然语言处理任务中展现出了出色的性能。然而，尽管它们在未见数据上取得成功，但目前基于PLM的表示通常在对抗设置中表现出较差的稳健性。本文引入了RobustSentEmbed，这是一个自监督句子嵌入框架，旨在改善不同文本表示任务中的泛化性和稳健性，以及对抗各种对抗攻击。通过生成高风险的对抗扰动并将其用于新颖的目标函数中，RobustSentEmbed巧妙地学习高质量和稳健的句子嵌入。我们的实验证实了RobustSentEmbed优于最先进表示方法的优越性。具体来说，我们的框架显著降低了各种对抗攻击的成功率，特别是降低了

    arXiv:2403.11082v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing
    
[^150]: 将专家知识与深度学习技术相结合，实现即时缺陷预测

    Bridging Expert Knowledge with Deep Learning Techniques for Just-In-Time Defect Prediction

    [https://arxiv.org/abs/2403.11079](https://arxiv.org/abs/2403.11079)

    提出了一种模型融合框架，结合了基于专家知识的简单模型和基于深度学习技术的复杂模型，以实现即时缺陷预测的更好性能。

    

    即时缺陷预测旨在自动预测提交是否存在缺陷，近年来已得到广泛研究。大多数研究可以分为两类：1)使用传统机器学习分类器和手工设计特征的简单模型，2)使用深度学习技术自动提取提交内容特征的复杂模型。为了充分利用专家知识和语义意义，我们提出了一种模型融合框架，旨在在特征级别上同时采用早期融合和...

    arXiv:2403.11079v1 Announce Type: cross  Abstract: Just-In-Time (JIT) defect prediction aims to automatically predict whether a commit is defective or not, and has been widely studied in recent years. In general, most studies can be classified into two categories: 1) simple models using traditional machine learning classifiers with hand-crafted features, and 2) complex models using deep learning techniques to automatically extract features from commit contents. Hand-crafted features used by simple models are based on expert knowledge but may not fully represent the semantic meaning of the commits. On the other hand, deep learning-based features used by complex models represent the semantic meaning of commits but may not reflect useful expert knowledge. Simple models and complex models seem complementary to each other to some extent. To utilize the advantages of both simple and complex models, we propose a model fusion framework that adopts both early fusions on the feature level and la
    
[^151]: 用于提高CVaR优化样本效率的简单混合策略参数化

    A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization

    [https://arxiv.org/abs/2403.11062](https://arxiv.org/abs/2403.11062)

    提出了一种简单的混合策略参数化方法，通过整合风险中性策略和可调整策略，提高了CVaR优化的样本效率。

    

    利用策略梯度(PG)优化条件值风险(CVaR)的强化学习算法在提高样本效率方面面临着重大挑战，限制了它们的实际应用。为了解决这些挑战，我们提出了一种简单的混合策略参数化方法。这种方法将风险中性策略与可调整策略整合为一个风险厌恶策略。通过采用这种策略，所有收集到的轨迹都可以用于策略更新，并且通过风险中性组件刺激更高的回报，从而提升尾部并防止扁平化。我们的实证研究表明，这种混合参数化是非常有效的。

    arXiv:2403.11062v1 Announce Type: new  Abstract: Reinforcement learning algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications. This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat. To address these challenges, we propose a simple mixture policy parameterization. This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy. By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness. Our empirical study reveals that this mixture parameterization is uniquely effectiv
    
[^152]: JustQ：公平准确的量子神经网络的自动部署

    JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks

    [https://arxiv.org/abs/2403.11048](https://arxiv.org/abs/2403.11048)

    本研究提出了JustQ框架，用于在NISQ计算机上部署公平准确的量子神经网络，包括NISQ错误模型、基于强化学习的部署和同时考虑公平性和准确性的灵活优化目标。实验结果表明JustQ优于先前方法，在准确性和公平性方面表现更优，为未来的研究铺平了道路。

    

    尽管量子神经网络（QNNs）在决策系统中取得了成功，但其公平性仍未得到探索，因为主要关注点在于准确性。本文进行了设计空间探索，揭示了QNN的不公平性，并突显了QNN部署和量子噪声对准确性和公平性的显著影响。为了有效地遍历广阔的QNN部署设计空间，我们提出了JustQ，这是一个在NISQ计算机上部署公平准确QNN的框架。它包括完整的NISQ错误模型、基于强化学习的部署以及一个灵活的优化目标，同时考虑公平性和准确性。实验结果表明，JustQ优于先前的方法，实现了更高的准确性和公平性。本研究开创了NISQ计算机上公平QNN设计的先河，为未来的研究铺平了道路。

    arXiv:2403.11048v1 Announce Type: cross  Abstract: Despite the success of Quantum Neural Networks (QNNs) in decision-making systems, their fairness remains unexplored, as the focus primarily lies on accuracy. This work conducts a design space exploration, unveiling QNN unfairness, and highlighting the significant influence of QNN deployment and quantum noise on accuracy and fairness. To effectively navigate the vast QNN deployment design space, we propose JustQ, a framework for deploying fair and accurate QNNs on NISQ computers. It includes a complete NISQ error model, reinforcement learning-based deployment, and a flexible optimization objective incorporating both fairness and accuracy. Experimental results show JustQ outperforms previous methods, achieving superior accuracy and fairness. This work pioneers fair QNN design on NISQ computers, paving the way for future investigations.
    
[^153]: 通过信息间竞争调控聊天机器人输出

    Regulating Chatbot Output via Inter-Informational Competition

    [https://arxiv.org/abs/2403.11046](https://arxiv.org/abs/2403.11046)

    本文通过探讨信息间竞争，提出利用信息市场本身作为有效减轻AI聊天机器人输出风险的可能性，并指出监管者在面对新技术不确定性时往往过度谨慎，需重新评估监管策略。

    

    ChatGPT的出现引发了一年多的监管狂潮。然而，少数现有研究严格质疑了这样一个假设：如果不经规范，AI聊天机器人的输出会对人类事务造成实质且严重的伤害。大多数研究人员忽视了信息市场本身可以有效减轻这些风险的关键可能性，并因此倾向于使用监管工具直接解决问题。本文通过关注各种渠道之间的信息竞争，发展了一个重新评估AI相关内容风险和相应监管提议的标准。长达数十年的信息和通信技术监管史表明，监管者在面对新技术带来的不确定性时往往过于谨慎，并在提出过度的监管措施时犯错误。事实上，丰富的经验数据支持了信息市场机制在信息监管方面的作用。

    arXiv:2403.11046v1 Announce Type: cross  Abstract: The advent of ChatGPT has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empiric
    
[^154]: 推进多变量时间序列相似性评估：一种集成计算方法

    Advancing multivariate time series similarity assessment: an integrated computational approach

    [https://arxiv.org/abs/2403.11044](https://arxiv.org/abs/2403.11044)

    提出了一种名为MTASA的集成计算方法，旨在优化多变量时间序列对齐，并通过多处理引擎提高计算资源利用率，解决了评估多变量时间序列数据相似性时所面临的挑战

    

    Data mining在提取复杂系统见解和支持不同领域的决策中发挥着关键作用，而多变量时间序列数据的分析尤为重要。然而，评估多变量时间序列数据的相似性存在着一些挑战，包括处理大型数据集、解决时间上的错位问题以及需要高效全面的分析框架。为了解决所有这些挑战，我们提出了一种名为多变量时间序列对齐和相似性评估（MTASA）的新型集成计算方法。MTASA建立在一个混合方法学的基础上，旨在优化时间序列对齐，辅以一个提高计算资源利用率的多处理引擎。这种集成方法包括四个关键组件，每个组件都解决了时间序列相似性评估的基本方面，从而提供了全面的分析框架。

    arXiv:2403.11044v1 Announce Type: new  Abstract: Data mining, particularly the analysis of multivariate time series data, plays a crucial role in extracting insights from complex systems and supporting informed decision-making across diverse domains. However, assessing the similarity of multivariate time series data presents several challenges, including dealing with large datasets, addressing temporal misalignments, and the need for efficient and comprehensive analytical frameworks. To address all these challenges, we propose a novel integrated computational approach known as Multivariate Time series Alignment and Similarity Assessment (MTASA). MTASA is built upon a hybrid methodology designed to optimize time series alignment, complemented by a multiprocessing engine that enhances the utilization of computational resources. This integrated approach comprises four key components, each addressing essential aspects of time series similarity assessment, thereby offering a comprehensive f
    
[^155]: FAGH：利用近似全局Hessian加速联邦学习

    FAGH: Accelerating Federated Learning with Approximated Global Hessian

    [https://arxiv.org/abs/2403.11041](https://arxiv.org/abs/2403.11041)

    提出了一种FL方法FAGH，利用近似全局Hessian和全局梯度的一阶矩来加速全局模型训练，降低通信次数。

    

    在联邦学习（FL）中，由于全局模型收敛速度慢而导致的显着通信开销构成了一项重大挑战。本文提出了一种名为FAGH的FL方法，该方法利用近似全局Hessian的一阶矩和全局梯度的一阶矩来训练全局模型，通过利用全局Hessian的曲率，加速全局模型训练的收敛，减少了通信次数。

    arXiv:2403.11041v1 Announce Type: new  Abstract: In federated learning (FL), the significant communication overhead due to the slow convergence speed of training the global model poses a great challenge. Specifically, a large number of communication rounds are required to achieve the convergence in FL. One potential solution is to employ the Newton-based optimization method for training, known for its quadratic convergence rate. However, the existing Newton-based FL training methods suffer from either memory inefficiency or high computational costs for local clients or the server. To address this issue, we propose an FL with approximated global Hessian (FAGH) method to accelerate FL training. FAGH leverages the first moment of the approximated global Hessian and the first moment of the global gradient to train the global model. By harnessing the approximated global Hessian curvature, FAGH accelerates the convergence of global model training, leading to the reduced number of communicati
    
[^156]: FH-TabNet：多阶段表格深度学习用于多类家族性高胆固醇血症检测

    FH-TabNet: Multi-Class Familial Hypercholesterolemia Detection via a Multi-Stage Tabular Deep Learning

    [https://arxiv.org/abs/2403.11032](https://arxiv.org/abs/2403.11032)

    该论文介绍了FH-TabNet，是一个多阶段表格深度学习模型，用于家族性高胆固醇血症的多类检测任务。

    

    家族性高胆固醇血症（FH）是一种遗传疾病，其特征是低密度脂蛋白（LDL）胆固醇或其相关基因水平升高。早期和准确分类FH对于及时干预以减轻危及生命的状况风险具有重要意义。然而，传统的诊断方法复杂、昂贵，并且即使对于经验丰富的临床医生来说也是一个具有挑战性的解释任务，导致高的漏诊率。尽管近年来对于使用机器学习（ML）模型进行早期FH检测产生了兴趣激增，但现有解决方案仅考虑使用传统ML模型进行二进制分类任务。尽管其重要性，但由于底层临床数据的分类性质，因此针对FH检测应用深度学习（DL）的研究处于起步阶段。该论文通过引入FH-TabNet来填补这一空白，FH-TabNet是一个多阶段表格DL模型。

    arXiv:2403.11032v1 Announce Type: new  Abstract: Familial Hypercholesterolemia (FH) is a genetic disorder characterized by elevated levels of Low-Density Lipoprotein (LDL) cholesterol or its associated genes. Early-stage and accurate categorization of FH is of significance allowing for timely interventions to mitigate the risk of life-threatening conditions. Conventional diagnosis approach, however, is complex, costly, and a challenging interpretation task even for experienced clinicians resulting in high underdiagnosis rates. Although there has been a recent surge of interest in using Machine Learning (ML) models for early FH detection, existing solutions only consider a binary classification task solely using classical ML models. Despite its significance, application of Deep Learning (DL) for FH detection is in its infancy, possibly, due to categorical nature of the underlying clinical data. The paper addresses this gap by introducing the FH-TabNet, which is a multi-stage tabular DL 
    
[^157]: 利用空间抽象加速样本选择过程

    Accelerating prototype selection with spatial abstraction

    [https://arxiv.org/abs/2403.11020](https://arxiv.org/abs/2403.11020)

    提出了一种利用空间抽象加速样本选择过程的方法，可以有效地减少计算资源需求，并在广泛认可的数据集上进行了测试。

    

    工业和社会的数字化增长导致了可供处理和利用的数据不断增长。然而，大量的数据需要相当多的计算资源来应用机器学习方法。样本选择技术已经被应用来降低这些技术所需的计算资源。本文提出了一种加速现有样本选择技术的方法。它利用空间划分的概念构建数据集的抽象表示。第二步利用这种抽象表示有效地剪枝搜索空间并选择一组候选原型。然后，通过我们的方法选择的候选样本可以应用一些传统的样本选择算法。我们的方法与五种传统的样本选择算法集成，并在14个广为认可的数据集上进行了测试。

    arXiv:2403.11020v1 Announce Type: new  Abstract: The increasing digitalization in industry and society leads to a growing abundance of data available to be processed and exploited. However, the high volume of data requires considerable computational resources for applying machine learning approaches. Prototype selection techniques have been applied to reduce the requirements of computational resources that are needed by these techniques. In this paper, we propose an approach for speeding up existing prototype selection techniques. It builds an abstract representation of the dataset, using the notion of spatial partition. The second step uses this abstract representation to prune the search space efficiently and select a set of candidate prototypes. After, some conventional prototype selection algorithms can be applied to the candidates selected by our approach. Our approach was integrated with five conventional prototype selection algorithms and tested on 14 widely recognized datasets 
    
[^158]: 提升连续投影算法和界限

    Improved Algorithm and Bounds for Successive Projection

    [https://arxiv.org/abs/2403.11013](https://arxiv.org/abs/2403.11013)

    提出了伪点SPA算法，利用极值理论推导误差界限，相比原始SPA具有更快的收敛速度和更好的数值性能

    

    在$d$维空间中给定一个包含$K$个顶点的单纯形，假设我们在单纯形上测量$n$个带有噪声的点（因此，一些观测点落在单纯形之外）。顶点搜索是估计单纯形的$K$个顶点的问题。一个常用的顶点搜索算法是连续投影算法（SPA）。然而，在强噪声或异常值下，观察到SPA表现不佳。我们提出了伪点SPA（pp-SPA）。它使用投影步骤和去噪步骤生成伪点，并将它们输入到SPA中进行顶点搜索。我们基于（可能是）高维随机向量的极值理论推导了pp-SPA的误差界限。结果表明，pp-SPA比SPA具有更快的收敛速度和更好的数值性能。我们的分析包括对原始SPA的改进非渐近界限，这具有独立的价值。

    arXiv:2403.11013v1 Announce Type: new  Abstract: Given a $K$-vertex simplex in a $d$-dimensional space, suppose we measure $n$ points on the simplex with noise (hence, some of the observed points fall outside the simplex). Vertex hunting is the problem of estimating the $K$ vertices of the simplex. A popular vertex hunting algorithm is successive projection algorithm (SPA). However, SPA is observed to perform unsatisfactorily under strong noise or outliers. We propose pseudo-point SPA (pp-SPA). It uses a projection step and a denoise step to generate pseudo-points and feed them into SPA for vertex hunting. We derive error bounds for pp-SPA, leveraging on extreme value theory of (possibly) high-dimensional random vectors. The results suggest that pp-SPA has faster rates and better numerical performances than SPA. Our analysis includes an improved non-asymptotic bound for the original SPA, which is of independent interest.
    
[^159]: 图神经网络的前向学习

    Forward Learning of Graph Neural Networks

    [https://arxiv.org/abs/2403.11004](https://arxiv.org/abs/2403.11004)

    图神经网络的成功依赖于反向传播算法，但其存在一些限制，为此提出了前向正向算法作为一种替代方法。

    

    图神经网络（GNNs）在推荐系统、药物发现和问答等领域取得了显著的成功。在GNNs的成功背后，是反向传播（BP）算法，这是训练深度神经网络（NNs）的事实标准。然而，尽管BP的有效性，它还是存在一些限制，不仅在生物上不合理，而且限制了学习NNs的可扩展性、并行性和灵活性。为了解决这些限制，最近在图像分类领域提出了前向正向（FF）算法作为BP的替代方法，通过在正负数据上执行两次前向传递来训练NNs。

    arXiv:2403.11004v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we 
    
[^160]: 医学图像中拓扑保真的多类别分割

    Topologically faithful multi-class segmentation in medical images

    [https://arxiv.org/abs/2403.11001](https://arxiv.org/abs/2403.11001)

    提出了一种用于医学图像的拓扑保真多类别分割的通用损失函数，通过将N类别分割问题分解为N个单类别分割任务，实现了对神经网络的训练，验证了在四个医学数据集上的有效性

    

    在医学图像分割中，拓扑精度是一个非常重要的属性，对于下游应用如网络分析和血管或细胞计数中的流模拟至关重要。最近，重要的方法论进步将代数拓扑中扎实的概念带到了二值分割中。然而，在多类别分割场景中，这些方法很少被探索，拓扑错误很常见。我们提出了一个通用损失函数，用于拓扑保真的多类别分割，扩展了最近基于持久条码的Betti匹配概念。我们将N类别分割问题投影到N个单类别分割任务，这使得我们能够使用一参数持久同调，从而使神经网络的训练变得可行。我们在一组包含高度不同拓扑特征的四个医学数据集上验证了我们的方法。

    arXiv:2403.11001v1 Announce Type: cross  Abstract: Topological accuracy in medical image segmentation is a highly important property for downstream applications such as network analysis and flow modeling in vessels or cell counting. Recently, significant methodological advancements have brought well-founded concepts from algebraic topology to binary segmentation. However, these approaches have been underexplored in multi-class segmentation scenarios, where topological errors are common. We propose a general loss function for topologically faithful multi-class segmentation extending the recent Betti matching concept, which is based on induced matchings of persistence barcodes. We project the N-class segmentation problem to N single-class segmentation tasks, which allows us to use 1-parameter persistent homology making training of neural networks computationally feasible. We validate our method on a comprehensive set of four medical datasets with highly variant topological characteristic
    
[^161]: 嵌套神经特征场的层次场景理解

    N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields

    [https://arxiv.org/abs/2403.10997](https://arxiv.org/abs/2403.10997)

    利用Nested Neural Feature Fields (N2F2) 实现了层次化监督学习，提供了对物理维度或语义维度等不同粒度的场景属性全面和细致的理解。

    

    在计算机视觉中，理解多层抽象的复杂场景仍然是一个巨大挑战。为了解决这个问题，我们引入了嵌套神经特征场 (N2F2)，这是一种新颖的方法，利用分层监督来学习单个特征场，在同一高维特征中的不同维度编码不同粒度的场景属性。我们的方法允许灵活定义层次，可以根据物理维度、语义维度或两者均匹配，从而实现对场景的全面和细致理解。我们利用2D类别无关分割模型在图像空间的任意尺度提供语义有意义的像素分组，并查询CLIP视觉编码器，为这些段落中的每个部分获得与语言对齐的嵌入。我们提出的分层监督方法将不同的嵌套特征场维度分配给提取C

    arXiv:2403.10997v1 Announce Type: cross  Abstract: Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the C
    
[^162]: 一个可扩展且可并行化的数字孪生框架，用于多智能体强化学习系统可持续Sim2Real转换

    A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems

    [https://arxiv.org/abs/2403.10996](https://arxiv.org/abs/2403.10996)

    提出了一个可持续的多智能体深度强化学习框架，利用分散的学习架构，来解决交通路口穿越和自主赛车等问题

    

    本工作提出了一个可持续的多智能体深度强化学习框架，能够选择性地按需扩展并行化训练工作负载，并利用最少的硬件资源将训练好的策略从模拟环境转移到现实世界。我们引入了AutoDRIVE生态系统作为一个启动数字孪生框架，用于训练、部署和转移合作和竞争的多智能体强化学习策略从模拟环境到现实世界。具体来说，我们首先探究了4台合作车辆(Nigel)在单智能体和多智能体学习环境中共享有限状态信息的交叉遍历问题，采用了一种通用策略方法。然后，我们使用个体策略方法研究了2辆车(F1TENTH)的对抗性自主赛车问题。在任何一组实验中，我们采用了去中心化学习架构，这允许对策略进行有力的训练和测试。

    arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
    
[^163]: 具有奇异值扰动的边缘私有图神经网络

    Edge Private Graph Neural Networks with Singular Value Perturbation

    [https://arxiv.org/abs/2403.10995](https://arxiv.org/abs/2403.10995)

    提出了一种新的隐私保护GNN训练算法Eclipse，通过观察图结构中邻接矩阵的低秩行为，实现了在保护边缘隐私的同时保持模型良好效用。

    

    图神经网络（GNNs）在从图结构化数据中学习表示方面发挥关键作用，并被证明在许多应用中非常有用。然而，GNN训练流程已被显示容易受到节点特征泄漏和边提取攻击的影响。本文研究了一种情景，其中攻击者旨在从训练过的GNN模型中恢复私有边缘信息。先前的研究采用差分隐私（DP）直接向邻接矩阵或紧凑图表示添加噪声。添加的扰动导致图结构被大幅改变，降低了模型的效用。我们提出了一种新的保护隐私的GNN训练算法Eclipse，该算法在提供强大的隐私保护的同时保持了良好的模型效用。Eclipse基于两个关键观察结果。第一，图结构中的邻接矩阵表现出低秩行为。因此，Eclipse使用低秩的方式训练GNNs。

    arXiv:2403.10995v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) play a key role in learning representations from graph-structured data and are demonstrated to be useful in many applications. However, the GNN training pipeline has been shown to be vulnerable to node feature leakage and edge extraction attacks. This paper investigates a scenario where an attacker aims to recover private edge information from a trained GNN model. Previous studies have employed differential privacy (DP) to add noise directly to the adjacency matrix or a compact graph representation. The added perturbations cause the graph structure to be substantially morphed, reducing the model utility. We propose a new privacy-preserving GNN training algorithm, Eclipse, that maintains good model utility while providing strong privacy protection on edges. Eclipse is based on two key observations. First, adjacency matrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains GNNs with a low-r
    
[^164]: IoTCO2：评估物联网-启用深度学习的端到端碳足迹

    IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning

    [https://arxiv.org/abs/2403.10984](https://arxiv.org/abs/2403.10984)

    介绍了一种名为\carb 的端到端建模工具，用于在物联网-启用深度学习中精确估算碳足迹，展示了与实际测量值相比最大$\pm21\%$的碳足迹差异。

    

    为了提高隐私性和确保服务质量（QoS），深度学习（DL）模型越来越多地部署在物联网（IoT）设备上进行数据处理，极大地增加了与IoT上DL相关的碳足迹，涵盖了操作和实体方面。现有的操作能量预测器经常忽略了量化的DL模型和新兴的神经处理单元（NPUs），而实体碳足迹建模工具忽略了IoT设备中常见的非计算硬件组件，导致了物联网DL准确碳足迹建模工具的差距。本文介绍了\textit{\carb}，一种用于精确估算物联网DL中碳足迹的端到端建模工具，展示了与各种DL模型的实际测量值相比最大$\pm21\%$的碳足迹差异。此外，\carb的实际应用通过多个用户案例展示。

    arXiv:2403.10984v1 Announce Type: cross  Abstract: To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \textit{\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating a maximum $\pm21\%$ deviation in carbon footprint values compared to actual measurements across various DL models. Additionally, practical applications of \carb are showcased through multiple user case studies.
    
[^165]: 通过联邦学习增强对抗 DDoS 攻击的物联网安全

    Enhancing IoT Security Against DDoS Attacks through Federated Learning

    [https://arxiv.org/abs/2403.10968](https://arxiv.org/abs/2403.10968)

    通过联邦学习，利用物联网设备的集体智慧构建全局模型，保护数据隐私和最小化通信开销，增强物联网对抗DDoS攻击的安全性。

    

    物联网的迅速普及引入了物理设备与数字世界之间的变革性连接。然而，分布式拒绝服务（DDoS）攻击不断升级，危及物联网网络的完整性和可靠性。传统的DDoS缓解方法无法处理物联网生态系统的复杂性，可能 compromise 数据隐私。本文介绍了一种创新策略，通过利用联邦学习的力量，增强物联网网络对抗DDoS攻击的安全性，允许多个物联网设备或边缘节点协作构建全局模型，同时保护数据隐私并最小化通信开销。该研究旨在探究联邦学习在检测和缓解物联网中DDoS攻击中的有效性。我们提出的框架利用物联网设备的集体智慧进行实时攻击检测。

    arXiv:2403.10968v1 Announce Type: cross  Abstract: The rapid proliferation of the Internet of Things (IoT) has ushered in transformative connectivity between physical devices and the digital realm. Nonetheless, the escalating threat of Distributed Denial of Service (DDoS) attacks jeopardizes the integrity and reliability of IoT networks. Conventional DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT ecosystems, potentially compromising data privacy. This paper introduces an innovative strategy to bolster the security of IoT networks against DDoS attacks by harnessing the power of Federated Learning that allows multiple IoT devices or edge nodes to collaboratively build a global model while preserving data privacy and minimizing communication overhead. The research aims to investigate Federated Learning's effectiveness in detecting and mitigating DDoS attacks in IoT. Our proposed framework leverages IoT devices' collective intelligence for real-time attack det
    
[^166]: 梦想中的许多世界：学习上下文世界模型有助于零样点泛化

    Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization

    [https://arxiv.org/abs/2403.10967](https://arxiv.org/abs/2403.10967)

    学习上下文世界模型有助于提高在未知上下文下的零样点泛化能力。

    

    零样点泛化（Zero-shot generalization，ZSG）到未见过的动态对于创建具有普遍能力的体系代理是一个重大挑战。为了解决更广泛的挑战，我们从上下文强化学习（contextual reinforcement learning，cRL）的简单设置开始，假设可观察到参数化系统动态变化的上下文值，如机器人的质量或尺寸，而不对马尔可夫状态的可观察性做进一步简化假设。为了实现对未知上下文变化的ZSG目标，我们提出了上下文循环状态空间模型（contextual recurrent state-space model，cRSSM），它对Dreamer（v3）（Hafner等人，2023年）的世界模型进行了修改，使得世界模型可以融入上下文以从观察中推断潜在的马尔可夫状态并建模潜在动态。我们的实验表明，这种系统性地将上下文纳入其中提高了在“梦境”训练的策略的ZSG能力。

    arXiv:2403.10967v1 Announce Type: cross  Abstract: Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams
    
[^167]: 基于能量的模型及其在语音和语言处理中的应用

    Energy-Based Models with Applications to Speech and Language Processing

    [https://arxiv.org/abs/2403.10961](https://arxiv.org/abs/2403.10961)

    基于能量的模型（EBMs）是一类重要的概率模型，在语音和语言处理等领域吸引了越来越多的关注，因其显著的理论和算法进展。

    

    基于能量的模型（EBMs）是一类重要的概率模型，也称为随机场和无向图模型。EBMs是非规范化的，因此与其他流行的自归一化概率模型（如隐马尔可夫模型（HMMs）、自回归模型、生成对抗网络（GANs）和变分自编码器（VAEs））有根本的不同。近年来，由于重要的理论和算法进展，EBMs不仅吸引了核心机器学习社区的越来越多关注，还吸引了应用领域（如语音、视觉、自然语言处理（NLP）等）的兴趣。语音和语言的顺序性质也提供了特殊挑战，需要与处理固定维度数据（例如图像）有所不同的处理方法。因此，本专著旨在系统介绍基于能量的模型，包括算法进展。

    arXiv:2403.10961v1 Announce Type: cross  Abstract: Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as speech, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of speech and language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images). Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progr
    
[^168]: SelfIE：大型语言模型嵌入的自我解释

    SelfIE: Self-Interpretation of Large Language Model Embeddings

    [https://arxiv.org/abs/2403.10949](https://arxiv.org/abs/2403.10949)

    提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。

    

    arXiv:2403.10949v1 公告类型：交叉摘要：大型语言模型（LLMs）如何获得答案？解释和控制LLM的推理过程对于可靠性、透明度和未来模型发展至关重要。我们提出了SelfIE（嵌入的自我解释），这是一个框架，能够利用LLMs响应关于给定段落的查询的能力，以自然语言解释它们自己的嵌入。SelfIE能够解释隐藏嵌入中的开放世界概念，在案例中揭示LLM的内部推理，如做出道德决策、内化提示注入和回想有害知识。SelfIE对隐藏嵌入的文本描述也开辟了控制LLM推理的新途径。我们提出了监督控制，它允许编辑开放式概念，而只需要计算单个层的梯度。我们将RLHF扩展到隐藏的嵌入，并提出了强化控制来消除有害知识。

    arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
    
[^169]: 在序列任务设置中最小化局部遗憾的谬误

    The Fallacy of Minimizing Local Regret in the Sequential Task Setting

    [https://arxiv.org/abs/2403.10946](https://arxiv.org/abs/2403.10946)

    研究了强化学习中在序列任务设置下最小化局部遗憾的谬误，揭示了近视地最小化遗憾在实际应用中的复杂性。

    

    在强化学习领域，在线强化学习经常被概念化为一个优化问题，其中算法与未知环境交互以最小化累积遗憾。在静态设置中，可以获得强大的理论保证，如次线性（$\sqrt{T}$）遗憾界限，通常意味着收敛到最优策略并停止探索。然而，这些理论设置通常过分简化了真实世界强化学习实现中遇到的复杂性，其中任务按顺序到达，任务之间有重大变化，并且算法可能不允许在某些任务中进行自适应学习。我们研究超出结果分布的变化，涵盖奖励设计（从结果到奖励的映射）和允许的策略空间的变化。我们的结果揭示了在每个任务中近视地最小化遗憾的谬误：获得最优遗憾r

    arXiv:2403.10946v1 Announce Type: cross  Abstract: In the realm of Reinforcement Learning (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret. In a stationary setting, strong theoretical guarantees, like a sublinear ($\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration. However, these theoretical setups often oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks. We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces. Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret r
    
[^170]: ViSaRL：受人类显著性引导的视觉强化学习

    ViSaRL: Visual Reinforcement Learning Guided by Human Saliency

    [https://arxiv.org/abs/2403.10940](https://arxiv.org/abs/2403.10940)

    ViSaRL提出了Visual Saliency-Guided Reinforcement Learning（受视觉显著性引导的强化学习）方法，通过学习视觉表示来显著提高RL代理在不同任务上的成功率、样本效率和泛化性能。

    

    使用强化学习（RL）从高维像素输入培训机器人执行复杂控制任务在样本效率上是低效的，因为图像观察主要由与任务无关的信息组成。相比之下，人类能够在视觉上关注与任务相关的对象和区域。基于这一观察，我们引入了受视觉显著性引导的强化学习（ViSaRL）。使用ViSaRL学习视觉表示显着提高了RL代理在不同任务上，包括DeepMind控制基准、仿真中的机器人操作和真实机器人上的成功率、样本效率和泛化性能。我们提出了将显著性整合到基于CNN和Transformer的编码器中的方法。我们展示使用ViSaRL学习的视觉表示对各种视觉扰动，包括感知噪声和场景变化，都具有鲁棒性。ViSaRL在真实环境中成功率几乎翻了一番。

    arXiv:2403.10940v1 Announce Type: cross  Abstract: Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-
    
[^171]: 通过最小化增强语言模型进行初步解码以改进低资源ASR中的晶格重新打分

    Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR

    [https://arxiv.org/abs/2403.10937](https://arxiv.org/abs/2403.10937)

    通过最小化增强语言模型进行初步解码，以提高低资源ASR中晶格重新打分的语音识别准确性，相对减少了泰卢固语21.8%和卡纳达语41.8%的词误差率，同时仅消耗1/8内存。

    

    本文解决了在基线语言模型无法生成全面晶格的低资源语言中通过晶格重新打分改善语音识别准确性的问题。我们通过将基线语言模型最小程度地增强为目标语言更大文本语料库中出现但基线语言模型中不存在的词的词频来解决这一问题。通过使用这种增强的基线语言模型进行解码生成的晶格更加全面。我们的方法使词误差率相对减少了21.8%（泰卢固语）和41.8%（卡纳达语）。与使用完整维基百科文本增强语言模型解码相比，我们的方法在减少词误差率方面相当，而我们的方法只消耗1/8的内存。

    arXiv:2403.10937v1 Announce Type: cross  Abstract: This paper addresses the problem of improving speech recognition accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent f
    
[^172]: 神经网络的函数空间参数化用于序列学习

    Function-space Parameterization of Neural Networks for Sequential Learning

    [https://arxiv.org/abs/2403.10929](https://arxiv.org/abs/2403.10929)

    提出了一种神经网络的函数空间参数化方法，能够在序列学习中有效整合新数据并保留先前知识，同时在不重新训练的情况下合并新数据。

    

    由于在梯度下降深度学习中难以整合新数据并保留先前知识，顺序学习范式提出了挑战。虽然高斯过程优雅地解决了这些问题，但在处理诸如图像之类的丰富输入和伸缩性方面存在困难。为了解决这些问题，我们引入了一种将神经网络从权重空间转换到函数空间的技术，即双参数化。我们的参数化提供了：(i) 通过稀疏化将函数空间方法扩展到大数据集的途径，(ii) 在访问过去数据受限的情况下保留先前知识，以及(iii) 在不重新训练的情况下合并新数据的机制。我们的实验表明，我们可以在持续学习中保留知识，并有效地合并新数据。我们进一步展示了其在不确定性量化和引导基于模型的RL中探索的优点。

    arXiv:2403.10929v1 Announce Type: cross  Abstract: Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge. While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images. To address these issues, we introduce a technique that converts neural networks from weight space to function space, through a dual parameterization. Our parameterization offers: (i) a way to scale function-space methods to large data sets via sparsification, (ii) retention of prior knowledge when access to past data is limited, and (iii) a mechanism to incorporate new data without retraining. Our experiments demonstrate that we can retain knowledge in continual learning and incorporate new data efficiently. We further show its strengths in uncertainty quantification and guiding exploration in model-based RL. Further information and code is available on the
    
[^173]: 分布式多目标动态卸载调度用于空地协同 MEC

    Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC

    [https://arxiv.org/abs/2403.10927](https://arxiv.org/abs/2403.10927)

    提出了用于空地协同 MEC 的分布式多目标动态卸载调度方案，结合了MORL和核方法，通过使用n步回报来处理积压中的波动。

    

    利用无人机（UAV）与边缘服务器协助地面移动边缘计算（MEC）已经引起了极大关注。然而，基于确定性优化或单目标强化学习（RL）的最先进方案无法在高度动态网络环境中减少任务比特的积压，并同时提高能效，设计问题等同于一个顺序决策问题。为了解决上述问题，并且处理增加的地面用户数量引入的维度诅咒，本文提出了一个分布式多目标（MO）动态轨迹规划和卸载调度方案，整合了MORL和核方法。n步回报的设计也被应用于减少积压中的波动。数值结果显示n步回报可以使得所提出的核方法受益。

    arXiv:2403.10927v1 Announce Type: cross  Abstract: Utilizing unmanned aerial vehicles (UAVs) with edge server to assist terrestrial mobile edge computing (MEC) has attracted tremendous attention. Nevertheless, state-of-the-art schemes based on deterministic optimizations or single-objective reinforcement learning (RL) cannot reduce the backlog of task bits and simultaneously improve energy efficiency in highly dynamic network environments, where the design problem amounts to a sequential decision-making problem. In order to address the aforementioned problems, as well as the curses of dimensionality introduced by the growing number of terrestrial terrestrial users, this paper proposes a distributed multi-objective (MO) dynamic trajectory planning and offloading scheduling scheme, integrated with MORL and the kernel method. The design of n-step return is also applied to average fluctuations in the backlog. Numerical results reveal that the n-step return can benefit the proposed kernel-b
    
[^174]: TabPFN的可解释机器学习

    Interpretable Machine Learning for TabPFN

    [https://arxiv.org/abs/2403.10923](https://arxiv.org/abs/2403.10923)

    TabPFN模型在低数据情况下实现了良好的分类性能，并能够以秒级速度生成后验预测分布，我们提出了几种专为TabPFN设计的可解释性方法的改进，实现了更高效的计算。

    

    最近开发的Prior-Data Fitted Networks（PFNs）已经显示出在低数据情况下具有非常有希望的应用结果。TabPFN模型是PFN的一种特殊情况，适用于表格数据，在不需要学习参数或超参数调整的情况下，能够在短短几秒钟内实现多种分类任务的最先进性能，并且能够生成后验预测分布。TabPFN因此成为了许多领域应用中非常吸引人的选择。然而，该方法的一个主要缺点是缺乏可解释性。因此，我们提出了几种针对TabPFN专门设计的流行解释性方法的改进。通过利用该模型的独特性质，我们的改进允许比现有实现更高效的计算。特别是，我们展示了通过避免...

    arXiv:2403.10923v1 Announce Type: cross  Abstract: The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how in-context learning facilitates the estimation of Shapley values by avoid
    
[^175]: 基于深度学习的自动位置检测

    Automatic location detection based on deep learning

    [https://arxiv.org/abs/2403.10912](https://arxiv.org/abs/2403.10912)

    该研究利用深度学习技术实现了针对印度城市图像的自动位置检测系统，通过两种方法（普通CNN和VGG16模型）获得了高准确度，并突出了优势和改进潜力。

    

    数字图像的普及和深度学习的进步为各个领域带来了创新解决方案，尤其在图像分类领域。我们的项目对针对印度城市图像进行识别和分类的图像分类系统进行了深入研究和实施。利用大量数据集，我们的模型将图像分类为五个主要的印度城市：艾哈迈达巴德、德里、喀拉拉、加尔各答和孟买，以识别每个城市/邦的独特特征和特性。为了实现高精度和召回率，我们采用了两种方法。首先是一个普通的卷积神经网络(CNN)，然后我们探索了通过利用VGG16模型来实现迁移学习的效力。普通CNN实现了可观的准确度，而VGG16模型实现了63.6%的测试准确度。评估突出了优势和改进潜力的领域。

    arXiv:2403.10912v1 Announce Type: cross  Abstract: The proliferation of digital images and the advancements in deep learning have paved the way for innovative solutions in various domains, especially in the field of image classification. Our project presents an in-depth study and implementation of an image classification system specifically tailored to identify and classify images of Indian cities. Drawing from an extensive dataset, our model classifies images into five major Indian cities: Ahmedabad, Delhi, Kerala, Kolkata, and Mumbai to recognize the distinct features and characteristics of each city/state. To achieve high precision and recall rates, we adopted two approaches. The first, a vanilla Convolutional Neural Network (CNN) and then we explored the power of transfer learning by leveraging the VGG16 model. The vanilla CNN achieved commendable accuracy and the VGG16 model achieved a test accuracy of 63.6%. Evaluations highlighted the strengths and potential areas of improvement
    
[^176]: 具有L20范数的图正则化NMF用于无监督特征学习

    Graph Regularized NMF with L20-norm for Unsupervised Feature Learning

    [https://arxiv.org/abs/2403.10910](https://arxiv.org/abs/2403.10910)

    引入L20范数约束的图正则化NMF用于无监督特征学习，旨在增强特征稀疏性和减轻噪声影响。

    

    非负矩阵分解（NMF）是机器学习和数据挖掘领域广泛应用的技术。图正则化非负矩阵分解（GNMF）是NMF的一种扩展，它包含了图正则化约束。GNMF在聚类和降维方面表现出色，有效地发现了嵌入高维空间中的内在低维结构。然而，GNMF对噪声的敏感性限制了它在实际应用中的稳定性和鲁棒性。为了增强特征的稀疏性并减轻噪声对其影响，同时在数据中挖掘行稀疏模式以进行有效的特征选择，我们将L20范数约束引入作为GNMF的稀疏约束。我们提出了一种基于GNMF\_$\ell_{20}$的无监督特征学习框架，并设计了基于PALM及其加速版本的算法来解决这个问题。

    arXiv:2403.10910v1 Announce Type: new  Abstract: Nonnegative Matrix Factorization (NMF) is a widely applied technique in the fields of machine learning and data mining. Graph Regularized Non-negative Matrix Factorization (GNMF) is an extension of NMF that incorporates graph regularization constraints. GNMF has demonstrated exceptional performance in clustering and dimensionality reduction, effectively discovering inherent low-dimensional structures embedded within high-dimensional spaces. However, the sensitivity of GNMF to noise limits its stability and robustness in practical applications. In order to enhance feature sparsity and mitigate the impact of noise while mining row sparsity patterns in the data for effective feature selection, we introduce the $\ell_{2,0}$-norm constraint as the sparsity constraints for GNMF. We propose an unsupervised feature learning framework based on GNMF\_$\ell_{20}$ and devise an algorithm based on PALM and its accelerated version to address this prob
    
[^177]: DTOR：决策树异常值回归器用于解释异常

    DTOR: Decision Tree Outlier Regressor to explain anomalies

    [https://arxiv.org/abs/2403.10903](https://arxiv.org/abs/2403.10903)

    DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。

    

    解释异常值的出现以及其产生机制在各种领域中可能非常重要。故障、欺诈、威胁等问题，除了被正确识别之外，通常需要有效的解释以有效执行可操作的对抗措施。越来越广泛地使用复杂的机器学习方法来识别异常值，使得这样的解释更具挑战性。我们提出了决策树异常值回归器（DTOR），这是一种通过估计异常检测模型生成的异常分数来为单个数据点生成基于规则的解释的技术。这是通过首先应用决策树回归器来计算估计分数，然后提取与数据点分数相关联的相对路径来实现的。我们的结果表明，即使在具有大量特征的数据集中，DTOR的鲁棒性也得到了证实。此外，与其他基于规则的方法相比

    arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
    
[^178]: 列表样本压缩和均匀收敛

    List Sample Compression and Uniform Convergence

    [https://arxiv.org/abs/2403.10889](https://arxiv.org/abs/2403.10889)

    研究在列表学习中均匀收敛和样本压缩原则的适用性，证明了在列表PAC学习中均匀收敛仍然等价于可学习性

    

    列表学习是监督分类的一个变种，在这种学习中，学习器为每个实例输出多个可能的标签，而不仅仅是一个。我们研究了与列表学习上的泛化相关的经典原则。我们的主要目标是确定在列表PAC学习领域，PAC设置中的经典原则是否保留其适用性。我们重点关注均匀收敛（这是经验风险最小化的基础）和样本压缩（这是Occam's Razor的一个强大体现）。在经典PAC学习中，均匀收敛和样本压缩都满足一种“完备性”形式：每当一个类是可学习的时候，也可以通过遵循这些原则的学习规则来学习它。我们探讨在列表学习环境中是否也存在相同的完备性。我们表明在列表PAC学习环境中，均匀收敛仍然等价于可学习性。

    arXiv:2403.10889v1 Announce Type: new  Abstract: List learning is a variant of supervised classification where the learner outputs multiple plausible labels for each instance rather than just one. We investigate classical principles related to generalization within the context of list learning. Our primary goal is to determine whether classical principles in the PAC setting retain their applicability in the domain of list PAC learning. We focus on uniform convergence (which is the basis of Empirical Risk Minimization) and on sample compression (which is a powerful manifestation of Occam's Razor). In classical PAC learning, both uniform convergence and sample compression satisfy a form of `completeness': whenever a class is learnable, it can also be learned by a learning rule that adheres to these principles. We ask whether the same completeness holds true in the list learning setting.   We show that uniform convergence remains equivalent to learnability in the list PAC learning setting
    
[^179]: 用非对称距离度量进行概率世界建模

    Probabilistic World Modeling with Asymmetric Distance Measure

    [https://arxiv.org/abs/2403.10875](https://arxiv.org/abs/2403.10875)

    学习非对称相似性函数使得我们能够将概率世界动态的几何抽象嵌入到表征空间中，并实现多向概率推理。

    

    表征学习是机器学习中的一个基本任务，旨在从数据中揭示结构以便促进后续任务。然而，在随机世界中计划和推理的好的表征是一个未解之谜。在这项工作中，我们认为学习距离函数对于在表征空间中进行计划和推理是至关重要的。我们展示了通过非对称对比学习可以将概率世界动态的几何抽象嵌入到表征空间中。与之前关注学习相互相似性或兼容性度量的方法不同，我们学习了一个反映状态达到性和允许多向概率推理的非对称相似性函数。此外，通过以一个共同的参考状态（例如观察者的当前状态）为条件，所学习的表征空间使我们能够发现几何上显著的状态。

    arXiv:2403.10875v1 Announce Type: new  Abstract: Representation learning is a fundamental task in machine learning, aiming at uncovering structures from data to facilitate subsequent tasks. However, what is a good representation for planning and reasoning in a stochastic world remains an open problem. In this work, we posit that learning a distance function is essential to allow planning and reasoning in the representation space. We show that a geometric abstraction of the probabilistic world dynamics can be embedded into the representation space through asymmetric contrastive learning. Unlike previous approaches that focus on learning mutual similarity or compatibility measures, we instead learn an asymmetric similarity function that reflects the state reachability and allows multi-way probabilistic inference. Moreover, by conditioning on a common reference state (e.g. the observer's current state), the learned representation space allows us to discover the geometrically salient state
    
[^180]: stMCDI: 使用图神经网络的条件掩码扩散模型用于空间转录组数据填补

    stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation

    [https://arxiv.org/abs/2403.10863](https://arxiv.org/abs/2403.10863)

    stMCDI是一种新颖的条件扩散模型，通过利用空间定位转录组数据中的空间位置信息来填补缺失值，同时保持整体数据分布。

    

    空间定位转录组学通过提供基因表达数据及其相应的物理位置，为单细胞分析带来了重大进展。然而，这种高度的空间分辨率却带来了一个缺点，即由于缺失值的高发生率，导致了细胞水平上的空间转录组数据明显受到困扰。此外，大多数现有的填补方法要么忽视不同点之间的空间信息，要么牺牲整体基因表达数据分布。为解决这些挑战，我们的主要关注点是有效利用空间定位转录组数据中的空间位置信息来填补缺失值，同时保持整体数据分布。我们引入了一种新颖的条件扩散模型stMCDI，利用使用随机掩码数据部分作为指导来训练去噪网络。

    arXiv:2403.10863v1 Announce Type: cross  Abstract: Spatially resolved transcriptomics represents a significant advancement in single-cell analysis by offering both gene expression data and their corresponding physical locations. However, this high degree of spatial resolution entails a drawback, as the resulting spatial transcriptomic data at the cellular level is notably plagued by a high incidence of missing values. Furthermore, most existing imputation methods either overlook the spatial information between spots or compromise the overall gene expression data distribution. To address these challenges, our primary focus is on effectively utilizing the spatial location information within spatial transcriptomic data to impute missing values, while preserving the overall data distribution. We introduce \textbf{stMCDI}, a novel conditional diffusion model for spatial transcriptomics data imputation, which employs a denoising network trained using randomly masked data portions as guidance
    
[^181]: FedQNN: 使用量子神经网络的联邦学习

    FedQNN: Federated Learning using Quantum Neural Networks

    [https://arxiv.org/abs/2403.10861](https://arxiv.org/abs/2403.10861)

    FedQNN框架融合了量子机器学习与经典联邦学习原则，在保护数据隐私的分布式环境中实现了合作学习，具有高准确率。

    

    在这项研究中，我们探索了量子联邦学习（QFL）的创新领域，作为通过分布式网络训练量子机器学习（QML）模型的框架。我们提出的联邦量子神经网络（FedQNN）框架是一种尖端解决方案，将QML的独特特征与经典联邦学习的原则相结合。这项工作彻底调查了QFL，在分布式环境中保护数据处理的能力，并促进合作学习而无需直接共享数据。我们的研究通过在不同数据集（包括基因组学和医疗保健）上进行实验证实了这一概念，从而验证了我们的FedQNN框架的多功能性和高效性。结果在三个不同数据集上始终保持在86%以上的准确率。

    arXiv:2403.10861v1 Announce Type: cross  Abstract: In this study, we explore the innovative domain of Quantum Federated Learning (QFL) as a framework for training Quantum Machine Learning (QML) models via distributed networks. Conventional machine learning models frequently grapple with issues about data privacy and the exposure of sensitive information. Our proposed Federated Quantum Neural Network (FedQNN) framework emerges as a cutting-edge solution, integrating the singular characteristics of QML with the principles of classical federated learning. This work thoroughly investigates QFL, underscoring its capability to secure data handling in a distributed environment and facilitate cooperative learning without direct data sharing. Our research corroborates the concept through experiments across varied datasets, including genomics and healthcare, thereby validating the versatility and efficacy of our FedQNN framework. The results consistently exceed 86% accuracy across three distinct
    
[^182]: 神经核条件均值嵌入

    Neural-Kernel Conditional Mean Embeddings

    [https://arxiv.org/abs/2403.10859](https://arxiv.org/abs/2403.10859)

    结合深度学习和CME的神经网络方法，解决了核条件均值嵌入面临的可伸缩性和表现力挑战，并在条件密度估计任务和强化学习中展现出卓越性能。

    

    核条件均值嵌入（CME）为表示条件分布提供了强大的框架，但通常面临可伸缩性和表现力挑战。在这项工作中，我们提出了一种新方法，有效地结合了深度学习与CME，以解决这些挑战。具体而言，我们的方法利用端到端神经网络（NN）优化框架，使用基于核的目标函数。这种设计避免了当前CME方法所需的计算昂贵的Gram矩阵求逆。为进一步提高性能，我们提供了有效的策略来优化剩余的核超参数。在条件密度估计任务中，我们的NN-CME混合方法实现了竞争性能，并常常超过现有基于深度学习的方法。最后，我们展示了其在无缝集成到强化学习（RL）环境中的卓越多功能性。

    arXiv:2403.10859v1 Announce Type: cross  Abstract: Kernel conditional mean embeddings (CMEs) offer a powerful framework for representing conditional distribution, but they often face scalability and expressiveness challenges. In this work, we propose a new method that effectively combines the strengths of deep learning with CMEs in order to address these challenges. Specifically, our approach leverages the end-to-end neural network (NN) optimization framework using a kernel-based objective. This design circumvents the computationally expensive Gram matrix inversion required by current CME methods. To further enhance performance, we provide efficient strategies to optimize the remaining kernel hyperparameters. In conditional density estimation tasks, our NN-CME hybrid achieves competitive performance and often surpasses existing deep learning-based methods. Lastly, we showcase its remarkable versatility by seamlessly integrating it into reinforcement learning (RL) contexts. Building on 
    
[^183]: 使用选项的强化学习

    Reinforcement Learning with Options

    [https://arxiv.org/abs/2403.10855](https://arxiv.org/abs/2403.10855)

    本论文提出了使用选项的分层强化学习方法，通过构建Hierarchical Policy learning来解决高维复杂环境中学习的问题。

    

    这篇论文旨在探索强化学习领域，并建立在现有方法的基础上，提出改进的方法来解决在高维复杂环境中学习的问题。它通过以分层方式分解学习任务，即分层强化学习，来实现这些目标。第一章我们熟悉马尔可夫决策过程框架，并展示一些最近的技术。之后，我们构建了分层策略学习，以应对单个基本策略的局限性。这个层次结构由顶层的管理代理和底层的员工代理组成。在最后一章，也是本论文的核心部分，我们尝试独立于管理级别学习层次结构的低层元素，即所谓的“Eigenoption”。基于环境的图结构。

    arXiv:2403.10855v1 Announce Type: new  Abstract: The current thesis aims to explore the reinforcement learning field and build on existing methods to produce improved ones to tackle the problem of learning in high-dimensional and complex environments. It addresses such goals by decomposing learning tasks in a hierarchical fashion known as Hierarchical Reinforcement Learning.   We start in the first chapter by getting familiar with the Markov Decision Process framework and presenting some of its recent techniques that the following chapters use. We then proceed to build our Hierarchical Policy learning as an answer to the limitations of a single primitive policy. The hierarchy is composed of a manager agent at the top and employee agents at the lower level.   In the last chapter, which is the core of this thesis, we attempt to learn lower-level elements of the hierarchy independently of the manager level in what is known as the "Eigenoption". Based on the graph structure of the environm
    
[^184]: 只说名称：通过数据生成实现仅利用类别名称进行在线连续学习

    Just Say the Name: Online Continual Learning with Category Names Only via Data Generation

    [https://arxiv.org/abs/2403.10853](https://arxiv.org/abs/2403.10853)

    提出了在线连续学习框架G-NoCL，采用生成数据并利用DIverSity和COmplexity enhancing ensemBlER（DISCOBER）进行数据融合，展示了其在在线连续学习基准测试中的优越性能。

    

    在现实世界的场景中，由于成本过高，对于连续学习进行大量手动注释是不切实际的。虽然之前的研究受到大规模网络监督训练的影响，建议在连续学习中利用网络抓取的数据，但这带来了诸如数据不平衡、使用限制和隐私问题等挑战。为了解决连续网络监督训练的风险，我们提出了一种在线连续学习框架 - 仅使用名称的生成式连续学习（G-NoCL）。所提出的G-NoCL使用一组生成器G以及学习者。当遇到新概念（例如，类别）时，G-NoCL采用新颖的样本复杂性引导数据合成技术DIverSity and COmplexity enhancing ensemBlER（DISCOBER）从生成的数据中最优抽样训练数据。通过大量实验，我们展示了DISCOBER在G-NoCL在线连续学习基准测试中表现出的优越性能，涵盖了In-Distributi。

    arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
    
[^185]: 使用门控动态可学习注意机制的双Transformer在田纳西伊斯曼过程中进行故障检测与诊断

    Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process

    [https://arxiv.org/abs/2403.10842](https://arxiv.org/abs/2403.10842)

    本研究提出一种新颖的双Transformer模型，结合门控动态可学习注意机制，用于田纳西伊斯曼过程的故障检测与诊断，提高性能通过独立处理输入数据和提取多样化信息，以及动态学习适应性调整注意策略。

    

    故障检测和诊断（FDD）对于确保工业过程的安全性和效率至关重要。我们提出了一种新颖的FDD方法，适用于田纳西伊斯曼过程（TEP），这是化工过程控制中广泛使用的基准。该模型采用两个独立的Transformer分支，能够独立处理输入数据并提取多样化的信息。引入了一种新颖的注意机制，即门控动态可学习注意（GDLAttention），它集成了门控机制和动态学习能力。门控机制调节注意权重，使模型能够关注输入的最相关部分。动态学习方法在训练过程中调整注意策略，有可能提高性能。注意机制使用双线性相似性函数，提供更大的灵活性来捕捉查询和输入之间的复杂关系。

    arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
    
[^186]: SF(DA)$^2$: 无源域自适应在数据增强的视角下

    SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation

    [https://arxiv.org/abs/2403.10834](https://arxiv.org/abs/2403.10834)

    SF(DA)$^2$是一种新颖的无源域自适应方法，通过在特征空间中构建增强图并利用谱邻域聚类来识别分区，实现了数据增强的好处而避免了挑战。

    

    在深度学习模型面对域偏移的脆弱性时，提出了无源域自适应（SFDA）方法，旨在适应新的、未见的目标域，而无需访问源域数据。尽管将数据增强应用于SFDA的潜在好处具有吸引力，但也会带来一些挑战，比如依赖于保存类别变换的先验知识以及增加内存和计算需求。在本文中，我们提出了SF(DA)$^2$，这是一种新颖的方法，利用数据增强的好处，并避免了这些挑战。我们在预训练模型的特征空间中构建一个增强图，利用目标特征之间的邻居关系，并提出了谱邻域聚类来识别预测空间中的分区。

    arXiv:2403.10834v1 Announce Type: cross  Abstract: In the face of the deep learning model's vulnerability to domain shift, source-free domain adaptation (SFDA) methods have been proposed to adapt models to new, unseen target domains without requiring access to source domain data. Although the potential benefits of applying data augmentation to SFDA are attractive, several challenges arise such as the dependence on prior knowledge of class-preserving transformations and the increase in memory and computational requirements. In this paper, we propose Source-free Domain Adaptation Through the Lens of Data Augmentation (SF(DA)$^2$), a novel approach that leverages the benefits of data augmentation without suffering from these challenges. We construct an augmentation graph in the feature space of the pretrained model using the neighbor relationships between target features and propose spectral neighborhood clustering to identify partitions in the prediction space. Furthermore, we propose im
    
[^187]: LookALike: 基于人类模仿的协作决策

    LookALike: Human Mimicry based collaborative decision making

    [https://arxiv.org/abs/2403.10824](https://arxiv.org/abs/2403.10824)

    提出了一种新颖的方法，通过在LLM代理之间进行知识蒸馏，实现实时的人类角色扮演，保留独特上下文，而不依赖任何存储数据或预训练，并展示出在模拟真实世界任务中表现优于现有技术。

    

    人工通用智能在向其他系统传达特定角色细微差别方面存在不足。在构建能够相互沟通以解决现实世界问题的自主LLM代理时，这一不足更为明显。人类能够传达上下文和领域特定的微小差别以及知识，这导致了技能的进一步完善。在这项工作中，我们提出并评估了一种新颖的方法，实现了LLM代理之间的知识精炼，从而实现了保留独特上下文而无需依赖任何存储数据或预训练的实时人类角色扮演。我们还评估了我们的系统在模拟实际任务中相比最先进技术表现更好的情况。

    arXiv:2403.10824v1 Announce Type: cross  Abstract: Artificial General Intelligence falls short when communicating role specific nuances to other systems. This is more pronounced when building autonomous LLM agents capable and designed to communicate with each other for real world problem solving. Humans can communicate context and domain specific nuances along with knowledge, and that has led to refinement of skills. In this work we propose and evaluate a novel method that leads to knowledge distillation among LLM agents leading to realtime human role play preserving unique contexts without relying on any stored data or pretraining. We also evaluate how our system performs better in simulated real world tasks compared to state of the art.
    
[^188]: 非平稳随机赌博机的激励探索

    Incentivized Exploration of Non-Stationary Stochastic Bandits

    [https://arxiv.org/abs/2403.10819](https://arxiv.org/abs/2403.10819)

    提出了针对非平稳随机赌博机的激励探索算法，实现了随时间的子线性遗憾和补偿

    

    我们研究了多臂赌博机（MAB）问题中的激励探索，其中玩家通过探索除了贪婪选择之外的臂获得补偿，并且可能对奖励提供偏倚反馈。我们考虑了两种不同的非平稳环境：突变和持续变化，并提出了相应的激励探索算法。我们展示了所提出的算法实现了随时间的子线性遗憾和补偿，从而有效地激励了探索，尽管存在非平稳性和偏倚或漂移反馈。

    arXiv:2403.10819v1 Announce Type: cross  Abstract: We study incentivized exploration for the multi-armed bandit (MAB) problem with non-stationary reward distributions, where players receive compensation for exploring arms other than the greedy choice and may provide biased feedback on the reward. We consider two different non-stationary environments: abruptly-changing and continuously-changing, and propose respective incentivized exploration algorithms. We show that the proposed algorithms achieve sublinear regret and compensation over time, thus effectively incentivizing exploration despite the nonstationarity and the biased or drifted feedback.
    
[^189]: FlyKD: 飞行中的图知识蒸馏与课程学习

    FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning

    [https://arxiv.org/abs/2403.10807](https://arxiv.org/abs/2403.10807)

    FlyKD提出了一种名为飞行中的知识蒸馏，通过结合课程学习，能够生成几乎无限数量的伪标签，极大缓解了在嘈杂伪标签上的优化过程，并显示出优于基准KD和局部结构保持图卷积网络的性能。

    

    知识蒸馏（KD）旨在将更强大的教师模型的知识转移给更轻便的学生模型，以提高模型的效率，使其更快速和更易部署。然而，学生模型在优化过程中面临由教师模型生成的嘈杂伪标签的困难，而能够生成的伪标签量受到内存不足（OOM）错误的限制。在本文中，我们提出了 FlyKD（飞行中的知识蒸馏），它可以生成几乎无限数量的伪标签，结合课程学习大大缓解了对嘈杂伪标签的优化过程。实验证明，FlyKD的性能优于基准KD和著名的局部结构保持图卷积网络（LSPGCN）。最后，通过课程学习的成功，我们揭示了一个新的研究方向，即改善对嘈杂伪标签的优化。

    arXiv:2403.10807v1 Announce Type: new  Abstract: Knowledge Distillation (KD) aims to transfer a more capable teacher model's knowledge to a lighter student model in order to improve the efficiency of the model, making it faster and more deployable. However, the student model's optimization process over the noisy pseudo labels (generated by the teacher model) is tricky and the amount of pseudo labels one can generate is limited due to Out of Memory (OOM) error. In this paper, we propose FlyKD (Knowledge Distillation on the Fly) which enables the generation of virtually unlimited number of pseudo labels, coupled with Curriculum Learning that greatly alleviates the optimization process over the noisy pseudo labels. Empirically, we observe that FlyKD outperforms vanilla KD and the renown Local Structure Preserving Graph Convolutional Network (LSPGCN). Lastly, with the success of Curriculum Learning, we shed light on a new research direction of improving optimization over noisy pseudo label
    
[^190]: 利用基于多测试的逐层特征融合增强超出分布检测

    Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion

    [https://arxiv.org/abs/2403.10803](https://arxiv.org/abs/2403.10803)

    提出了一种名为MLOD的新框架，利用多测试过程在不同级别的特征中识别测试样本中的分布偏移，无需修改预训练模型结构。

    

    在开放环境中部署机器学习会遇到一个挑战，即遇到与训练数据显著不同的各种测试输入，这些超出分布的样本可能在局部或全局特征上与训练分布有所偏移。本文提出了一种新颖的框架，名为基于多测试的逐层超出分布（OOD）检测（MLOD），通过严格的多个测试过程在不同级别的特征中鉴别测试样本中的分布偏移。我们的方法不同于现有方法，因为它不需要修改预训练的深度神经网络的结构或微调。

    arXiv:2403.10803v1 Announce Type: cross  Abstract: Deploying machine learning in open environments presents the challenge of encountering diverse test inputs that differ significantly from the training data. These out-of-distribution samples may exhibit shifts in local or global features compared to the training distribution. The machine learning (ML) community has responded with a number of methods aimed at distinguishing anomalous inputs from original training data. However, the majority of previous studies have primarily focused on the output layer or penultimate layer of pre-trained deep neural networks. In this paper, we propose a novel framework, Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to identify distributional shifts in test samples at different levels of features through rigorous multiple testing procedure. Our approach distinguishes itself from existing methods as it does not require modifying the structure or fine-tuning of the pre-trained c
    
[^191]: 基于隔离机制的异常检测：一项调查

    Anomaly Detection Based on Isolation Mechanisms: A Survey

    [https://arxiv.org/abs/2403.10802](https://arxiv.org/abs/2403.10802)

    基于隔离机制的异常检测是一种新颖有效的方法，具有低计算复杂性、低内存使用、高可伸缩性、对噪声和无关特征的稳健性，以及不需要先验知识或繁重参数调整。

    

    异常检测是一个长期并且活跃的研究领域，在金融、安全和制造等领域有许多应用。然而，在大数据时代，异常检测算法的效率和性能受到大规模、高维和异构数据的挑战。基于隔离的无监督异常检测是一种识别数据中异常的新颖有效方法。该方法依赖于异常很少且与正常实例不同的理念，因此可以通过随机分区轻松隔离异常值。基于隔离的方法相对于现有方法具有一些优势，如低计算复杂性、低内存使用、高可伸缩性、对噪声和无关特征的稳健性，以及不需要先验知识或繁重参数调整。本调查中，我们回顾了最先进的基于隔离的异常检测方法，包括它们的数据p

    arXiv:2403.10802v1 Announce Type: new  Abstract: Anomaly detection is a longstanding and active research area that has many applications in domains such as finance, security, and manufacturing. However, the efficiency and performance of anomaly detection algorithms are challenged by the large-scale, high-dimensional, and heterogeneous data that are prevalent in the era of big data. Isolation-based unsupervised anomaly detection is a novel and effective approach for identifying anomalies in data. It relies on the idea that anomalies are few and different from normal instances, and thus can be easily isolated by random partitioning. Isolation-based methods have several advantages over existing methods, such as low computational complexity, low memory usage, high scalability, robustness to noise and irrelevant features, and no need for prior knowledge or heavy parameter tuning. In this survey, we review the state-of-the-art isolation-based anomaly detection methods, including their data p
    
[^192]: 模型重新编程优于在文本图像编码器中针对分布外数据进行微调

    Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders

    [https://arxiv.org/abs/2403.10800](https://arxiv.org/abs/2403.10800)

    模型重新编程方法 Reprogrammer 在文本图像编码器中的应用优于传统微调方法，能够提高下游模型在分布内和分布外数据中的性能表现

    

    在评估将预训练模型转移到下游任务的性能时，不仅需要评估下游模型的分布内（ID）准确性，还需要评估其泛化能力并识别分布外（OOD）样本。本文揭示了侵入性微调技术所带来的隐藏成本。具体来说，我们证明了常用的微调方法不仅扭曲了用于泛化到协变量转移的OOD样本（OOD泛化）所需的表示，还扭曲了用于检测在语义上转移的OOD样本（OOD检测）所需的表示。为了解决这些挑战，我们引入了一种新的模型重新编程方法用于微调，我们将其命名为重新编程器。重新编程器旨在提高下游模型在ID、OOD泛化和OOD检测任务中的整体性能。我们的实证证据显示重新编程器优于对抗性微调与原型微调方法，是一种通用、有效且强大的工具来应对OOD挑战。

    arXiv:2403.10800v1 Announce Type: new  Abstract: When evaluating the performance of a pre-trained model transferred to a downstream task, it is imperative to assess not only the in-distribution (ID) accuracy of the downstream model but also its capacity to generalize and identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden costs associated with intrusive fine-tuning techniques. Specifically, we demonstrate that commonly used fine-tuning methods not only distort the representations necessary for generalizing to covariate-shifted OOD samples (OOD generalization) but also distort the representations necessary for detecting semantically-shifted OOD samples (OOD detection). To address these challenges, we introduce a new model reprogramming approach for fine-tuning, which we name Reprogrammer. Reprogrammer aims to improve the holistic performance of the downstream model across ID, OOD generalization, and OOD detection tasks. Our empirical evidence reveals that Rep
    
[^193]: 使用自适应估计融合高效剪枝大型语言模型

    Efficient Pruning of Large Language Model with Adaptive Estimation Fusion

    [https://arxiv.org/abs/2403.10799](https://arxiv.org/abs/2403.10799)

    提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。

    

    大型语言模型（LLMs）已经成为许多生成性下游任务中至关重要的组成部分，这导致在资源受限设备上高效部署它们成为不可避免的趋势和重大挑战。结构化剪枝是解决这一挑战的广泛应用方法。然而，当处理多个解码器层的复杂结构时，通常的方法往往采用常见的估计方法进行剪枝。这些方法导致特定下游任务精度下降。本文介绍了一种简单而有效的方法，可自适应地模拟每个子结构的重要性。同时，它可以基于复杂和多层结构的结果，自适应地融合粗粒度和细粒度的估计。我们设计的所有方面都无缝集成到端到端的剪枝框架中。与主流数据集上的最先进方法相比，我们的实验结果表明

    arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
    
[^194]: 从单词到路径：将大型语言模型应用于车辆路径规划

    From Words to Routes: Applying Large Language Models to Vehicle Routing

    [https://arxiv.org/abs/2403.10795](https://arxiv.org/abs/2403.10795)

    该研究探索了应用大型语言模型解决车辆路径规划问题的能力，提出了基于自然语言任务描述的基本提示范例并提出一种使模型进行改进的框架。

    

    LLMs在机器人领域（例如操作和导航）中展示出令人印象深刻的进展，其利用自然语言任务描述。LLMs在这些任务中取得成功让我们思考：LLMs在使用自然语言任务描述解决车辆路径规划问题（VRPs）的能力如何？在这项工作中，我们分三步研究这个问题。首先，我们构建了一个包含21种单车或多车路径问题的数据集。其次，我们评估了LLMs在四种基本提示范例文本到代码生成的性能，每种包括不同类型的文本输入。我们发现，直接从自然语言任务描述生成代码的基本提示范例对于GPT-4效果最佳，实现了56%的可行性，40%的优化性和53%的效率。第三，基于观察到LLMs可能无法在初始尝试中提供正确解决方案的现象，我们提出了一个框架，使LLMs能够进行改进。

    arXiv:2403.10795v1 Announce Type: cross  Abstract: LLMs have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of LLMs in these tasks leads us to wonder: What is the ability of LLMs to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of LLMs across four basic prompt paradigms of text-to-code generation, each involving different types of text input. We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that LLMs may not be able to provide correct solutions at the initial attempt, we propose a framework that enables LLMs to refin
    
[^195]: 在对抗性多智能体游戏中的扩散-强化学习分层运动规划

    Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games

    [https://arxiv.org/abs/2403.10794](https://arxiv.org/abs/2403.10794)

    该研究提出了一种在对抗性多智能体游戏中应用扩散-强化学习的分层运动规划方法，通过整合高级扩散模型和低级RL算法，实现比基准方法更高效率的运动规划。

    

    强化学习（RL）-基于的运动规划最近展现出优于传统方法的潜力，从自主导航到机器人操作。在这项工作中，我们关注部分可观察多智能体对抗追逐逃避游戏（PEG）中对逃避目标的运动规划任务。这些追逐逃避问题与各种应用相关，例如搜索和救援行动以及监视机器人，其中机器人必须有效规划他们的行动来收集情报或完成任务，同时避免被侦查或被俘虏自己。我们提出了一种分层架构，该架构整合了一个高级扩散模型，用于规划对环境数据敏感的全局路径，同时低级RL算法推理闪避行为与全局路径跟随行为。通过利用扩散模型引导RL算法，我们的方法比基线提高了51.2％，实现更高效率。

    arXiv:2403.10794v1 Announce Type: cross  Abstract: Reinforcement Learning- (RL-)based motion planning has recently shown the potential to outperform traditional approaches from autonomous navigation to robot manipulation. In this work, we focus on a motion planning task for an evasive target in a partially observable multi-agent adversarial pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to various applications, such as search and rescue operations and surveillance robots, where robots must effectively plan their actions to gather intelligence or accomplish mission tasks while avoiding detection or capture themselves. We propose a hierarchical architecture that integrates a high-level diffusion model to plan global paths responsive to environment data while a low-level RL algorithm reasons about evasive versus global path-following behavior. Our approach outperforms baselines by 51.2% by leveraging the diffusion model to guide the RL algorithm for more efficien
    
[^196]: QuantumLeak：从基于云的NISQ机器中窃取量子神经网络

    QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines

    [https://arxiv.org/abs/2403.10790](https://arxiv.org/abs/2403.10790)

    QuantumLeak是一种有效准确的方法，用于从基于云的NISQ机器中提取QNN模型，并且相较于现有的经典模型提取技术，可以在不同数据集和VQC架构下提高本地VQC准确率。

    

    变分量子电路（VQC）已成为实现量子神经网络（QNN）的强大工具，解决了一系列复杂问题。精心训练的VQC作为有价值的知识资产托管在基于云的嘈杂中等规模量子（NISQ）计算机上，使其容易受到恶意VQC窃取攻击。本文引入了QuantumLeak，一种从基于云的NISQ机器中有效准确提取QNN模型的方法。与现有的用于经典机器学习模型的传统模型提取技术相比，在不同数据集和VQC架构下，QuantumLeak将本地VQC的准确率提高了4.99%\~7.35%。

    arXiv:2403.10790v1 Announce Type: cross  Abstract: Variational quantum circuits (VQCs) have become a powerful tool for implementing Quantum Neural Networks (QNNs), addressing a wide range of complex problems. Well-trained VQCs serve as valuable intellectual assets hosted on cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them susceptible to malicious VQC stealing attacks. However, traditional model extraction techniques designed for classical machine learning models encounter challenges when applied to NISQ computers due to significant noise in current devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN model extraction technique from cloud-based NISQ machines. Compared to existing classical model stealing techniques, QuantumLeak improves local VQC accuracy by 4.99\%$\sim$7.35\% across diverse datasets and VQC architectures.
    
[^197]: 具有监督对比时间变换器的时间序列表示学习

    Time Series Representation Learning with Supervised Contrastive Temporal Transformer

    [https://arxiv.org/abs/2403.10787](https://arxiv.org/abs/2403.10787)

    提出了一种名为SCOTT的具有监督对比变换器的时间序列表示学习模型，结合了Transformer和Temporal Convolutional Networks以学习全局和局部特征，并简化了用于标记时间序列数据的监督对比损失。

    

    找到时间序列数据的有效表示是一项有用但具有挑战性的任务。有些工作利用自监督或无监督学习方法来解决这个问题。然而，如何利用可用的标签信息来获得更好的表示仍然是一个悬而未决的问题。为了回答这个问题，我们利用时间序列和表示学习领域中的现有技术，开发了一个简单但新颖的融合模型，称为：\textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT)。我们首先研究了适用于各种类型时间序列数据的合适增强方法，以帮助学习具有变化不变性的表示。其次，我们以简单的方式结合了Transformer和Temporal Convolutional Networks，以有效地学习全局和局部特征。最后，我们简化了用于标记时间序列数据表示学习的监督对比损失。

    arXiv:2403.10787v1 Announce Type: cross  Abstract: Finding effective representations for time series data is a useful but challenging task. Several works utilize self-supervised or unsupervised learning methods to address this. However, there still remains the open question of how to leverage available label information for better representations. To answer this question, we exploit pre-existing techniques in time series and representation learning domains and develop a simple, yet novel fusion model, called: \textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT). We first investigate suitable augmentation methods for various types of time series data to assist with learning change-invariant representations. Secondly, we combine Transformer and Temporal Convolutional Networks in a simple way to efficiently learn both global and local features. Finally, we simplify Supervised Contrastive Loss for representation learning of labelled time series data. We p
    
[^198]: ContourDiff：带轮廓引导扩散模型的无配对图像翻译

    ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models

    [https://arxiv.org/abs/2403.10786](https://arxiv.org/abs/2403.10786)

    ContourDiff是一种新颖的框架，利用图像的领域不变解剖轮廓表示，旨在帮助准确翻译医学图像并保持其解剖准确性。

    

    准确地在不同模态之间翻译医学图像（例如从CT到MRI）对于许多临床和机器学习应用至关重要。本文提出了一种名为ContourDiff的新框架，该框架利用图像的领域不变解剖轮廓表示。这些表示易于从图像中提取，但对其解剖内容形成精确的空间约束。我们引入一种扩散模型，将来自任意输入领域的图像的轮廓表示转换为输出领域中的图像。

    arXiv:2403.10786v1 Announce Type: cross  Abstract: Accurately translating medical images across different modalities (e.g., CT to MRI) has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with images translated to the output domain. To address these challenges, we propose ContourDiff, a novel framework that leverages domain-invariant anatomical contour representations of images. These representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. We introduce a diffusion model that converts contour representations of images from arbitrary input domains into images in the output domain of in
    
[^199]: 从大熔炉到误传：探讨生成式AI中的有害影响

    From Melting Pots to Misrepresentations: Exploring Harms in Generative AI

    [https://arxiv.org/abs/2403.10776](https://arxiv.org/abs/2403.10776)

    探索生成式AI中的社会有害影响，提出对多样性和公平性的关切，并引领讨论在这些模型中的重要性，同时提出未来研究方向。

    

    随着Gemini和GPT等先进生成模型的广泛应用，这些模型被纳入AI作为服务（AIaaS）的社会技术系统中，跨越多个领域。尽管如此，对这些模型在各种社会人口维度上的歧视倾向仍存在关切，尤其是倾向于某些“多数群体”人口统计特征。尽管有关媒体呼吁多样化代表的广泛，但在AIaaS背景下，边缘化种族和民族群体仍然面临持续扭曲、刻板印象和忽视。本文对社会有害研究现状进行了关键总结，引领讨论重点放在它们的影响上。我们还提出了本讨论指引下的开放式研究问题，以帮助确定未来研究途径。

    arXiv:2403.10776v1 Announce Type: cross  Abstract: With the widespread adoption of advanced generative models such as Gemini and GPT, there has been a notable increase in the incorporation of such models into sociotechnical systems, categorized under AI-as-a-Service (AIaaS). Despite their versatility across diverse sectors, concerns persist regarding discriminatory tendencies within these models, particularly favoring selected `majority' demographics across various sociodemographic dimensions. Despite widespread calls for diversification of media representations, marginalized racial and ethnic groups continue to face persistent distortion, stereotyping, and neglect within the AIaaS context. In this work, we provide a critical summary of the state of research in the context of social harms to lead the conversation to focus on their implications. We also present open-ended research questions, guided by our discussion, to help define future research pathways.
    
[^200]: 一种基于概率的人类比较对齐方法

    A Probabilistic Approach for Alignment with Human Comparisons

    [https://arxiv.org/abs/2403.10771](https://arxiv.org/abs/2403.10771)

    通过提出的两阶段“监督微调+人类比较”框架，本文研究了如何有效利用人类比较来改善AI模型的对齐，特别是在面对嘈杂数据和高维模型时。

    

    一个增长的趋势是将人类知识整合到学习框架中，利用微妙的人类反馈来完善AI模型。尽管取得了这些进展，但尚未开发出描述人类比较何时改善传统监督微调过程的特定条件的全面理论框架。为弥补这一差距，本文研究了有效利用人类比较来解决由嘈杂数据和高维模型引起的限制。我们提出了一个将机器学习与人类反馈通过概率二分方法联系起来的两阶段“监督微调+人类比较”（SFT+HC）框架。这两阶段框架首先通过SFT过程从带有噪声标记的数据中学习低维表示，然后利用人类比较来改进模型对齐。为了检验对齐阶段的效力，我们引入了一个新概念，称为“标签噪声到一致性”

    arXiv:2403.10771v1 Announce Type: new  Abstract: A growing trend involves integrating human knowledge into learning frameworks, leveraging subtle human feedback to refine AI models. Despite these advances, no comprehensive theoretical framework describing the specific conditions under which human comparisons improve the traditional supervised fine-tuning process has been developed. To bridge this gap, this paper studies the effective use of human comparisons to address limitations arising from noisy data and high-dimensional models. We propose a two-stage "Supervised Fine Tuning+Human Comparison" (SFT+HC) framework connecting machine learning with human feedback through a probabilistic bisection approach. The two-stage framework first learns low-dimensional representations from noisy-labeled data via an SFT procedure, and then uses human comparisons to improve the model alignment. To examine the efficacy of the alignment phase, we introduce a novel concept termed the "label-noise-to-co
    
[^201]: 横向异质性处理效应推断的ODE发现

    ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference

    [https://arxiv.org/abs/2403.10766](https://arxiv.org/abs/2403.10766)

    本文提出了一种在纵向设置中使用封闭形式常微分方程（ODE）的解决方案，相较于传统的神经网络推断，具有更好的可解释性和不规则性。

    

    在机器学习社区中，推断无偏处理效应已经受到广泛关注。近年来，我们的社区针对标准设置、高维处理设置甚至纵向设置提出了许多解决方案。虽然解决方案非常多样化，但大多仍然依赖于神经网络进行推断和同时纠正分配偏差。新方法通常是在之前的方法基础上提出新的（或改进的）架构和学习算法。然而，最终结果——基于神经网络的推断机器——尚未受到挑战。在本文中，我们在纵向设置中引入了一种不同类型的解决方案：一个封闭形式的常微分方程（ODE）。虽然我们仍然依赖于连续优化来学习ODE，但得到的推断机器不再是神经网络。这样做带来了几个优势，比如可解释性，不规则性。

    arXiv:2403.10766v1 Announce Type: new  Abstract: Inferring unbiased treatment effects has received widespread attention in the machine learning community. In recent years, our community has proposed numerous solutions in standard settings, high-dimensional treatment settings, and even longitudinal settings. While very diverse, the solution has mostly relied on neural networks for inference and simultaneous correction of assignment bias. New approaches typically build on top of previous approaches by proposing new (or refined) architectures and learning algorithms. However, the end result -- a neural-network-based inference machine -- remains unchallenged. In this paper, we introduce a different type of solution in the longitudinal setting: a closed-form ordinary differential equation (ODE). While we still rely on continuous optimization to learn an ODE, the resulting inference machine is no longer a neural network. Doing so yields several advantages such as interpretability, irregular 
    
[^202]: 一种用于更快分布鲁棒优化问题的原始-对偶算法

    A Primal-Dual Algorithm for Faster Distributionally Robust Optimization

    [https://arxiv.org/abs/2403.10763](https://arxiv.org/abs/2403.10763)

    这种原始-对偶算法在分布鲁棒优化问题中实现了最先进的线性收敛速度。

    

    我们考虑带有闭合、凸不确定性集的惩罚分布鲁棒优化（DRO）问题，这个设置包括了实践中使用的$f$-DRO、Wasserstein-DRO和谱/$L$-风险公式。我们提出了Drago，一种随机原始-对偶算法，在强凸-强凹DRO问题上实现了最先进的线性收敛速度。该方法将随机化和循环组件与小批量结合，有效处理了DRO中原始和对偶问题的独特不对称性质。我们通过分类和回归中的数值基准支持我们的理论结果。

    arXiv:2403.10763v1 Announce Type: cross  Abstract: We consider the penalized distributionally robust optimization (DRO) problem with a closed, convex uncertainty set, a setting that encompasses the $f$-DRO, Wasserstein-DRO, and spectral/$L$-risk formulations used in practice. We present Drago, a stochastic primal-dual algorithm that achieves a state-of-the-art linear convergence rate on strongly convex-strongly concave DRO problems. The method combines both randomized and cyclic components with mini-batching, which effectively handles the unique asymmetric nature of the primal and dual problems in DRO. We support our theoretical results with numerical benchmarks in classification and regression.
    
[^203]: 通过混合动作深度强化学习调度无人机和移动充电器

    Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning

    [https://arxiv.org/abs/2403.10761](https://arxiv.org/abs/2403.10761)

    本文提出了一个通过深度强化学习对无人机和移动充电器进行调度的方法，以解决无人机在观察任务中的航线规划和充电优化问题。

    

    最近，工业界和学术界对使用无线充电器延长无人机操作寿命的兴趣日益增长。本文考虑了一种受到充电器辅助的无人机应用：无人机部署到观察一组兴趣点，而充电器可以移动以为无人机充电。我们专注于无人机和移动充电器的航线和充电计划，以在尽可能短的时间内获得高观察效用，同时确保无人机在任务执行期间保持可操作状态。本文提出的无人机-充电器调度问题本质上是一个多阶段决策过程，在该过程中，无人机和移动充电器充当两个代理商合作完成任务。这两个代理商的离散连续混合动作空间在我们的问题中构成了一个重要挑战。为了解决这个问题，我们提出了一种混合动作d

    arXiv:2403.10761v1 Announce Type: new  Abstract: Recently there has been a growing interest in industry and academia, regarding the use of wireless chargers to prolong the operational longevity of unmanned aerial vehicles (commonly knowns as drones). In this paper we consider a charger-assisted drone application: a drone is deployed to observe a set points of interest, while a charger can move to recharge the drone's battery. We focus on the route and charging schedule of the drone and the mobile charger, to obtain high observation utility with the shortest possible time, while ensuring the drone remains operational during task execution. Essentially, this proposed drone-charger scheduling problem is a multi-stage decision-making process, in which the drone and the mobile charger act as two agents who cooperate to finish a task. The discrete-continuous hybrid action space of the two agents poses a significant challenge in our problem. To address this issue, we present a hybrid-action d
    
[^204]: 对于侵入性和非侵入性简约建模中潜空间动态识别算法的全面回顾

    A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling

    [https://arxiv.org/abs/2403.10748](https://arxiv.org/abs/2403.10748)

    Latent Space Dynamics Identification (LaSDI)框架能将高保真数据转换为简单的ODEs，并且每个构件可以根据应用灵活调节，为简约建模带来新的可能性。

    

    偏微分方程（PDEs）的数值求解器被广泛应用于模拟物理系统。然而，计算成本仍然是各种科学和工程应用中的主要瓶颈，这促使简化模型（ROMs）的发展。最近，基于机器学习的ROMs获得了显著的流行，并且对于解决传统ROM方法的一些局限性特别是对于对流主导系统而言是有希望的。在本章中，我们专注于一种称为潜空间动态识别（LaSDI）的特定框架，它将由PDE控制的高保真数据转换为由普通微分方程（ODEs）控制的更简单和低维的潜在空间数据。这些ODEs可以被学习，随后被插值以进行ROM预测。LaSDI的每个构件都可以根据应用程序轻松调节，这使得LaSDI成为了

    arXiv:2403.10748v1 Announce Type: cross  Abstract: Numerical solvers of partial differential equations (PDEs) have been widely employed for simulating physical systems. However, the computational cost remains a major bottleneck in various scientific and engineering applications, which has motivated the development of reduced-order models (ROMs). Recently, machine-learning-based ROMs have gained significant popularity and are promising for addressing some limitations of traditional ROM methods, especially for advection dominated systems. In this chapter, we focus on a particular framework known as Latent Space Dynamics Identification (LaSDI), which transforms the high-fidelity data, governed by a PDE, to simpler and low-dimensional latent-space data, governed by ordinary differential equations (ODEs). These ODEs can be learned and subsequently interpolated to make ROM predictions. Each building block of LaSDI can be easily modulated depending on the application, which makes the LaSDI fr
    
[^205]: 无视规划时Horizon的线性马尔可夫决策过程的遗憾

    Horizon-Free Regret for Linear Markov Decision Processes

    [https://arxiv.org/abs/2403.10738](https://arxiv.org/abs/2403.10738)

    该论文提出了第一个适用于线性马尔可夫决策过程的无视规划时Horizon的界限，与先前的方法相比，直接估计值函数和置信区间，避免显式估计转换模型和计算不同时间步长的非齐次值函数。

    

    一系列最近的研究表明在强化学习（RL）中的遗憾界限可以（几乎）独立于规划时Horizon，也就是无视Horizon的界限。然而，这些遗憾界限仅适用于允许对转换模型大小进行多项式依赖的设置，例如表格型马尔可夫决策过程（MDP）和线性混合MDP。我们为流行的线性MDP设置给出了第一个无视Horizon的界限，其中转换模型的大小可以是指数级大甚至不可数。与先前的工作不同，先前的工作明确估计转换模型并计算不同时间步长的非齐次值函数，我们直接估计值函数和置信区间。我们通过：（1）维护用于值函数的多个加权最小二乘估计器；以及（2）一个结构引理来获得无视Horizon的界限，该引理表明非齐次值函数的最大总变异是有界的。

    arXiv:2403.10738v1 Announce Type: new  Abstract: A recent line of works showed regret bounds in reinforcement learning (RL) can be (nearly) independent of planning horizon, a.k.a.~the horizon-free bounds. However, these regret bounds only apply to settings where a polynomial dependency on the size of transition model is allowed, such as tabular Markov Decision Process (MDP) and linear mixture MDP. We give the first horizon-free bound for the popular linear MDP setting where the size of the transition model can be exponentially large or even uncountable. In contrast to prior works which explicitly estimate the transition model and compute the inhomogeneous value functions at different time steps, we directly estimate the value functions and confidence sets. We obtain the horizon-free bound by: (1) maintaining multiple weighted least square estimators for the value functions; and (2) a structural lemma which shows the maximal total variation of the inhomogeneous value functions is bounde
    
[^206]: 针对非平稳线性赌博机的方差相关遗憾界限

    Variance-Dependent Regret Bounds for Non-stationary Linear Bandits

    [https://arxiv.org/abs/2403.10732](https://arxiv.org/abs/2403.10732)

    提出利用奖励分布方差和变化预算的算法，可以实现更紧的遗憾上限界限。

    

    我们研究了非平稳随机线性赌博机问题，其中奖励分布每一轮都在演变。现有算法通过总变化预算$B_K$来表征非平稳性，该预算是线性赌博机每$K$轮连续特征向量变化的总和。然而，这样的量只衡量了相对于奖励分布期望的非平稳性，这使得现有算法在一般非平稳分布情况下表现不佳。在这项工作中，我们提出了利用奖励分布方差以及$B_K$的算法，并展示它们可以实现更紧的遗憾上限界限。具体来说，我们介绍了两种新算法: 重新启动的加权$\text{OFUL}^+$和重新启动的$\text{SAVE}^+$。这些算法分别处理了奖励方差信息已知和未知的情况。

    arXiv:2403.10732v1 Announce Type: cross  Abstract: We investigate the non-stationary stochastic linear bandit problem where the reward distribution evolves each round. Existing algorithms characterize the non-stationarity by the total variation budget $B_K$, which is the summation of the change of the consecutive feature vectors of the linear bandits over $K$ rounds. However, such a quantity only measures the non-stationarity with respect to the expectation of the reward distribution, which makes existing algorithms sub-optimal under the general non-stationary distribution setting. In this work, we propose algorithms that utilize the variance of the reward distribution as well as the $B_K$, and show that they can achieve tighter regret upper bounds. Specifically, we introduce two novel algorithms: Restarted Weighted$\text{OFUL}^+$ and Restarted $\text{SAVE}^+$. These algorithms address cases where the variance information of the rewards is known and unknown, respectively. Notably, when
    
[^207]: 一种为扩散模型提供帮助的方法：改进条件人类图像生成的两阶段方法

    Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation

    [https://arxiv.org/abs/2403.10731](https://arxiv.org/abs/2403.10731)

    提出了一种改进条件人类图像生成的两阶段方法，首先训练手部生成器产生手部图像和分割掩模，在第二阶段使用改进的 ControlNet 模型绘制生成手部周围的身体。

    

    近年来，人类图像生成取得了显著进展，特别是在扩散模型的进步方面。然而，现有的扩散方法在生成一致的手部解剖结构时遇到挑战，并且生成的图像通常缺乏对手部姿势的精确控制。为了解决这一局限性，我们引入了一种新颖的方法来进行姿势条件的人类图像生成，将过程分为两个阶段：手的生成和随后围绕手部进行身体外部绘制。我们提出通过多任务设置训练手部生成器来生成手部图像及其对应的分割掩模，并在第一阶段中使用训练好的模型。然后在第二阶段使用适应的 ControlNet 模型来绘制周围的身体，生成最终结果。我们引入了一种新颖的混合技术，在第二阶段保留手部细节。

    arXiv:2403.10731v1 Announce Type: cross  Abstract: Recent years have seen significant progress in human image generation, particularly with the advancements in diffusion models. However, existing diffusion methods encounter challenges when producing consistent hand anatomy and the generated images often lack precise control over the hand pose. To address this limitation, we introduce a novel approach to pose-conditioned human image generation, dividing the process into two stages: hand generation and subsequent body out-painting around the hands. We propose training the hand generator in a multi-task setting to produce both hand images and their corresponding segmentation masks, and employ the trained model in the first stage of generation. An adapted ControlNet model is then used in the second stage to outpaint the body around the generated hands, producing the final result. A novel blending technique is introduced to preserve the hand details during the second stage that combines the
    
[^208]: 神经网络用于创建肥料管理区的反事实分析

    Counterfactual Analysis of Neural Networks Used to Create Fertilizer Management Zones

    [https://arxiv.org/abs/2403.10730](https://arxiv.org/abs/2403.10730)

    本研究提出了一种基于肥料响应性的管理区聚类方法，利用神经网络生成氮响应曲线并进行特征化分析。

    

    在精准农业中，考虑到田间变异性的管理区（MZs）的利用有助于有效的肥料管理。本文提出了一种基于肥料响应性的MZ聚类方法，利用卷积神经网络（CNN）为田地中的所有站点生成了氮（N）响应曲线，然后利用函数主成分分析对近似的N响应曲线进行特征化。

    arXiv:2403.10730v1 Announce Type: new  Abstract: In Precision Agriculture, the utilization of management zones (MZs) that take into account within-field variability facilitates effective fertilizer management. This approach enables the optimization of nitrogen (N) rates to maximize crop yield production and enhance agronomic use efficiency. However, existing works often neglect the consideration of responsivity to fertilizer as a factor influencing MZ determination. In response to this gap, we present a MZ clustering method based on fertilizer responsivity. We build upon the statement that the responsivity of a given site to the fertilizer rate is described by the shape of its corresponding N fertilizer-yield response (N-response) curve. Thus, we generate N-response curves for all sites within the field using a convolutional neural network (CNN). The shape of the approximated N-response curves is then characterized using functional principal component analysis. Subsequently, a counterf
    
[^209]: 揭示后门秘密：利用优化的比例预测一致性识别后门数据

    Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency

    [https://arxiv.org/abs/2403.10717](https://arxiv.org/abs/2403.10717)

    通过利用比例预测一致性技术，本研究提出了一种在被毒化数据集中自动识别后门数据的方法，无需额外干净数据或手动定义后门检测阈值。

    

    现代机器学习（ML）系统需要大量的训练数据，通常需要借助外部来源。然而，这种做法使它们容易受到后门毒化攻击。先前的后门防御策略主要集中在识别被植入后门的模型或被毒化数据的特征上，通常在假设可以访问干净数据的情况下运作。在这项工作中，我们探讨了一个相对不太被探索的挑战：在被毒化的数据集中自动识别后门数据，而且在现实条件下，即不需要额外的干净数据或不需要手动定义后门检测的阈值。我们从比例预测一致性（SPC）技术中汲取灵感，该技术利用被毒化数据对输入缩放因子的预测不变性。基于此，我们将后门数据识别问题建模为一个分层数据划分优化问题。

    arXiv:2403.10717v1 Announce Type: cross  Abstract: Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, i.e., without the need for additional clean data or without manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exploits the prediction invariance of poisoned data to an input scaling factor. Based on this, we pose the backdoor data identification problem as a hierarchical data splitting optimizatio
    
[^210]: 利用LLMs集成揭示社交媒体消息的潜在主题：气候运动案例研究

    Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns

    [https://arxiv.org/abs/2403.10707](https://arxiv.org/abs/2403.10707)

    本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。

    

    本文介绍了一种揭示和分析社交媒体消息主题的新方法。鉴于传统主题级分析的局限性，往往只捕捉到整体模式，本研究强调了对更精细、主题聚焦的探索的需求。传统的主题发现方法，涉及手动流程和人在循环中的方法，具有价值，但在伸缩性、一致性和资源强度方面面临挑战，涉及时间和成本。为了应对这些挑战，我们提出了一种利用大型语言模型（LLMs）先进功能的机器在循环中方法。这种方法允许更深入地调查社交媒体话语的主题方面，使我们能够揭示多样的主题，每个主题具有独特的特征和相关性，从而提供对更广泛主题内有的微妙细节的全面理解。

    arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
    
[^211]: PERL: 从人类反馈中实现参数高效强化学习

    PERL: Parameter Efficient Reinforcement Learning from Human Feedback

    [https://arxiv.org/abs/2403.10704](https://arxiv.org/abs/2403.10704)

    使用低秩适应（LoRA）方法进行参数高效强化学习（PERL），能够在与传统RLHF设置相当的性能下，实现更快的训练和更少的内存占用。

    

    强化学习从人类反馈（RLHF）已被证明是一种将预训练的大型语言模型（LLMs）与人类偏好对齐的有效方法。然而，使用RLHF训练模型计算成本高昂，且整个过程复杂。在本研究中，我们研究了RLHF，其中基础模型使用胡等人提出的低秩适应（LoRA）的参数高效方法进行训练。我们探讨了“参数高效强化学习”（PERL）的设置，在其中我们使用LoRA进行奖励模型训练和强化学习。我们将PERL与传统的微调（全调）在包括2个新数据集在内的7个基准测试中的奖励建模和强化学习方面的各种配置进行了比较。我们发现，PERL的性能与传统的RLHF设置相当，同时训练速度更快，内存占用更少。这使得RLHF具有很高的性能，同时减少了计算成本。

    arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of "Parameter Efficient Reinforcement Learning" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational 
    
[^212]: 论[V]-Mamba的低样本迁移性

    On the low-shot transferability of [V]-Mamba

    [https://arxiv.org/abs/2403.10696](https://arxiv.org/abs/2403.10696)

    [V]-Mamba相对于ViTs在利用线性探查进行迁移时展现出更优秀或等效的少样本学习能力，但在采用视觉提示作为迁移方法时表现出较弱或类似的表现。

    

    现代大规模神经网络的优势在于它们能够有效地适应少量示例的新任务。尽管大量研究已经探讨了Vision Transformers（ViTs）在不同约束下用于各种下游任务的可迁移性，但本研究将重点转移到了[V]-Mamba的迁移学习潜力。我们比较了其在不同少样本数据预算和高效迁移方法下的性能。我们的分析得出了三个关于[V]-Mamba在少样本迁移性能方面的关键见解：(a) 在利用线性探查（LP）进行迁移时，[V]-Mamba相较于ViTs展现出了更优秀或等效的少样本学习能力，(b) 相反，当采用视觉提示（VP）作为迁移方法时，[V]-Mamba展示出较弱或类似于ViTs的少样本学习性能，(c) 我们观察到在迁移性能差距方面存在着微弱的正相关性

    arXiv:2403.10696v1 Announce Type: cross  Abstract: The strength of modern large-scale neural networks lies in their ability to efficiently adapt to new tasks with few examples. Although extensive research has investigated the transferability of Vision Transformers (ViTs) to various downstream tasks under diverse constraints, this study shifts focus to explore the transfer learning potential of [V]-Mamba. We compare its performance with ViTs across different few-shot data budgets and efficient transfer methods. Our analysis yields three key insights into [V]-Mamba's few-shot transfer performance: (a) [V]-Mamba demonstrates superior or equivalent few-shot learning capabilities compared to ViTs when utilizing linear probing (LP) for transfer, (b) Conversely, [V]-Mamba exhibits weaker or similar few-shot learning performance compared to ViTs when employing visual prompting (VP) as the transfer method, and (c) We observe a weak positive correlation between the performance gap in transfer vi
    
[^213]: MYTE：形态学驱动的字节编码，用于更好、更公平的多语言语言建模

    MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling

    [https://arxiv.org/abs/2403.10691](https://arxiv.org/abs/2403.10691)

    MYTE是一种基于形态学的字节编码范式，通过使用具有一致大小的片段来实现跨不同语言的信息编码，为99种语言提供了更短的编码，特别是对非欧洲语言和非拉丁文字的改进最为显著。

    

    多语言语言建模中的一个主要考虑因素是如何最好地表示具有不同词汇和文字的语言。尽管当代文本编码方法涵盖了大多数世界文字系统，但它们存在偏向于全球西方高资源语言的问题。因此，少数语言的文本往往被分割为一长串在语言学上毫无意义的单元。为了解决这种不平等，我们引入了一种新的范式，用跨不同语言具有一致大小的片段来编码相同的信息。我们的编码约定（MYTE）基于形态素，因为它们的库存在各种语言中比字符更平衡，而以前的方法使用字符。我们展示MYTE为所有99种分析语言产生了更短的编码，其中非欧洲语言和非拉丁文字的改进最为显著。这进而改善了多语言语言建模的性能。

    arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
    
[^214]: 具有视觉到触觉-音频跨模态迁移学习的潜在物体特征识别

    Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning

    [https://arxiv.org/abs/2403.10689](https://arxiv.org/abs/2403.10689)

    该研究提出了一种视觉到触觉-音频跨模态迁移学习方法，实现了潜在物体特征的识别。

    

    识别机器人处理物体特征对于调整动作以确保与容器稳定有效地交互至关重要。这项工作的目标是识别潜在的不可见物体特征，通过视觉到触觉-音频的跨模态迁移学习方法，将视觉学习到的潜在空间转移到仅使用触觉-音频和运动数据训练的第二模块。

    arXiv:2403.10689v1 Announce Type: cross  Abstract: Recognising the characteristics of objects while a robot handles them is crucial for adjusting motions that ensure stable and efficient interactions with containers. Ahead of realising stable and efficient robot motions for handling/transferring the containers, this work aims to recognise the latent unobservable object characteristics. While vision is commonly used for object recognition by robots, it is ineffective for detecting hidden objects. However, recognising objects indirectly using other sensors is a challenging task. To address this challenge, we propose a cross-modal transfer learning approach from vision to haptic-audio. We initially train the model with vision, directly observing the target object. Subsequently, we transfer the latent space learned from vision to a second module, trained only with haptic-audio and motor data. This transfer learning framework facilitates the representation of object characteristics using in
    
[^215]: AutoHLS：学习加速HLS设计空间探索

    AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs

    [https://arxiv.org/abs/2403.10686](https://arxiv.org/abs/2403.10686)

    AutoHLS提出了一个集成深度神经网络和贝叶斯优化的框架，用于加速HLS硬件设计优化，实现高达70倍的探索时间加速

    

    高级综合（HLS）是一种设计流程，利用现代语言特性和灵活性，如复杂数据结构，继承，模板等，快速原型化硬件设计。然而，探索各种设计空间参数可能需要硬件工程师花费大量时间和精力，以满足特定设计规格。本文提出了一个名为AutoHLS的新颖框架，将深度神经网络（DNN）与贝叶斯优化（BO）集成，加速HLS硬件设计优化。我们的工具专注于HLS pragma探索和操作转换。它利用集成的DNN来预测在给定FPGA资源预算下的综合性。我们还探讨了量子神经网络（QNNs）的潜力，取代AutoHLS流水线中的经典DNNs。我们的实验结果表明，在探索时间上加速了多达70倍。

    arXiv:2403.10686v1 Announce Type: cross  Abstract: High-level synthesis (HLS) is a design flow that leverages modern language features and flexibility, such as complex data structures, inheritance, templates, etc., to prototype hardware designs rapidly. However, exploring various design space parameters can take much time and effort for hardware engineers to meet specific design specifications. This paper proposes a novel framework called AutoHLS, which integrates a deep neural network (DNN) with Bayesian optimization (BO) to accelerate HLS hardware design optimization. Our tool focuses on HLS pragma exploration and operation transformation. It utilizes integrated DNNs to predict synthesizability within a given FPGA resource budget. We also investigate the potential of emerging quantum neural networks (QNNs) instead of classical DNNs for the AutoHLS pipeline. Our experimental results demonstrate up to a 70-fold speedup in exploration time.
    
[^216]: 对GlassNet在玻璃稳定性和形成能力的物理启发式机器学习进行评估

    Evaluation of GlassNet for physics-informed machine learning of glass stability and glass-forming ability

    [https://arxiv.org/abs/2403.10682](https://arxiv.org/abs/2403.10682)

    对GlassNet模型在预测玻璃稳定性参数方面的应用进行了评估，探索了使用这些参数来估计玻璃的形成能力的可行性。

    

    玻璃构成了许多现代应用的基础，也在未来医疗和环境应用中具有巨大潜力。然而，它们的结构复杂性和庞大的组成空间使得对某些应用进行设计和优化具有挑战性。玻璃加工特别重要的是估计给定组成的玻璃成形能力（GFA）。然而，关于玻璃形成的物理机制仍存在许多未解之谜，特别是在氧化玻璃中。显而易见，用于估计GFA的代理属性将在玻璃加工和设计中非常有用，但寻找这样一个替代性属性已被证明是困难的。在这里，我们探讨了一个开源的预训练NN模型GlassNet的应用，该模型可以预测计算玻璃稳定性（GS）所需的特征温度，并评估使用这些物理启发式ML（PIML）预测的GS参数来估计形成能力的可行性。

    arXiv:2403.10682v1 Announce Type: cross  Abstract: Glasses form the basis of many modern applications and also hold great potential for future medical and environmental applications. However, their structural complexity and large composition space make design and optimization challenging for certain applications. Of particular importance for glass processing is an estimate of a given composition's glass-forming ability (GFA). However, there remain many open questions regarding the physical mechanisms of glass formation, especially in oxide glasses. It is apparent that a proxy for GFA would be highly useful in glass processing and design, but identifying such a surrogate property has proven itself to be difficult. Here, we explore the application of an open-source pre-trained NN model, GlassNet, that can predict the characteristic temperatures necessary to compute glass stability (GS) and assess the feasibility of using these physics-informed ML (PIML)-predicted GS parameters to estimat
    
[^217]: 用于机器人运动学习的黎曼流匹配策略

    Riemannian Flow Matching Policy for Robot Motion Learning

    [https://arxiv.org/abs/2403.10672](https://arxiv.org/abs/2403.10672)

    RFMP是一种新颖的模型，利用流匹配的优势在机器人视觉运动策略中具有高效训练和推断能力，并通过融合黎曼流形上的几何意识，提供更平滑的动作轨迹。

    

    我们引入了黎曼流匹配策略（RFMP），这是一种新颖的模型，用于学习和合成机器人视觉运动策略。RFMP利用了流匹配方法的高效训练和推断能力。通过设计，RFMP继承了流匹配的优势：能够编码高维度多模态分布，在机器人任务中常见，并且具有非常简单和快速的推断过程。我们展示了RFMP在基于状态和基于视觉的机器人运动策略中的适用性。值得注意的是，由于机器人状态存在于黎曼流形上，RFMP在本质上融合了几何意识，这对于现实机器人任务至关重要。为了评估RFMP，我们进行了两个概念验证实验，将其性能与扩散策略进行了比较。尽管这两种方法都成功地学习了所考虑的任务，但我们的结果表明RFMP提供了更平滑的动作轨迹，显著地提高了性能。

    arXiv:2403.10672v1 Announce Type: cross  Abstract: We introduce Riemannian Flow Matching Policies (RFMP), a novel model for learning and synthesizing robot visuomotor policies. RFMP leverages the efficient training and inference capabilities of flow matching methods. By design, RFMP inherits the strengths of flow matching: the ability to encode high-dimensional multimodal distributions, commonly encountered in robotic tasks, and a very simple and fast inference process. We demonstrate the applicability of RFMP to both state-based and vision-conditioned robot motion policies. Notably, as the robot state resides on a Riemannian manifold, RFMP inherently incorporates geometric awareness, which is crucial for realistic robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments, comparing its performance against Diffusion Policies. Although both approaches successfully learn the considered tasks, our results show that RFMP provides smoother action trajectories with signifi
    
[^218]: Bayesian深度学习中的无Hessian-Laplace

    Hessian-Free Laplace in Bayesian Deep Learning

    [https://arxiv.org/abs/2403.10671](https://arxiv.org/abs/2403.10671)

    提出了一种无Hessian计算和求逆的Hessian-Free Laplace近似框架，通过对数后验和网络预测的曲率来估计后验的方差。

    

    贝叶斯后验的Laplace近似（LA）是以最大后验估计为中心的高斯分布。它在贝叶斯深度学习中的吸引力源于能够在标准网络参数优化之后量化不确定性（即事后），从近似后验中抽样的便利性以及模型证据的解析形式。然而，LA的一个重要计算瓶颈是必须计算和求逆对数后验的Hessian矩阵。Hessian可以以多种方式近似，质量与网络、数据集和推断任务等多个因素有关。在本文中，我们提出了一个绕过Hessian计算和求逆的替代框架。无Hessian-Laplace（HFL）近似使用对数后验和网络预测的曲率来估计其方差。只需要两个点估计：最大后验估计和等价的曲率方向。

    arXiv:2403.10671v1 Announce Type: cross  Abstract: The Laplace approximation (LA) of the Bayesian posterior is a Gaussian distribution centered at the maximum a posteriori estimate. Its appeal in Bayesian deep learning stems from the ability to quantify uncertainty post-hoc (i.e., after standard network parameter optimization), the ease of sampling from the approximate posterior, and the analytic form of model evidence. However, an important computational bottleneck of LA is the necessary step of calculating and inverting the Hessian matrix of the log posterior. The Hessian may be approximated in a variety of ways, with quality varying with a number of factors including the network, dataset, and inference task. In this paper, we propose an alternative framework that sidesteps Hessian calculation and inversion. The Hessian-free Laplace (HFL) approximation uses curvature of both the log posterior and network prediction to estimate its variance. Only two point estimates are needed: the st
    
[^219]: 不仅改变标签，学习特征：使用多视角数据为深度神经网络添加水印

    Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data

    [https://arxiv.org/abs/2403.10663](https://arxiv.org/abs/2403.10663)

    通过使用多视角数据为深度神经网络添加水印，可以有效防御对源模型功能的窃取攻击

    

    随着机器学习作为服务（MLaaS）平台的日益普及，越来越多关注深度神经网络（DNN）水印技术。这些方法用于验证目标DNN模型的所有权以保护知识产权。本文首先从特征学习的角度引入了一种新颖的基于触发集的水印方法。具体来说，我们表明通过选择展示多个特征的数据，也被称为$\textit{多视角数据}$，可以有效地防御...

    arXiv:2403.10663v1 Announce Type: cross  Abstract: With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as $\textit{multi-view data}$, it becomes feasible to effectively defend 
    
[^220]: InterLUDE：标记数据与未标记数据间的相互作用以增强半监督学习

    InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning

    [https://arxiv.org/abs/2403.10658](https://arxiv.org/abs/2403.10658)

    InterLUDE提出了一种新的半监督学习方法，通过两部分相互作用来增强SSL，包括嵌入融合和基于一致性正则化的新损失函数，实验证明该方法在图像分类和医学任务上取得显著改进。

    

    半监督学习旨在通过在标记数据和未标记数据上进行训练来提高任务性能。当前主流的图像分类半监督学习方法大多优化损失函数，其中将监督分类目标与仅从未标记数据导出的正则化项相加。 这种表达方式忽略了标记和未标记图像之间相互作用的潜力。 本文介绍了InterLUDE，这是一种新的增强SSL方法，由两部分组成，每个部分都受益于标记-未标记交互。 第一部分是嵌入融合，它在标记和未标记的嵌入之间插值以改进表示学习。 第二部分是一种新的损失函数，基于一致性正则化原则，旨在最小化模型在标记与未标记输入之间的预测差异。 在标准闭集SSL基准测试和医学SSL任务上进行的实验表明，InterLUDE能取得显著改进。

    arXiv:2403.10658v1 Announce Type: cross  Abstract: Semi-supervised learning (SSL) seeks to enhance task performance by training on both labeled and unlabeled data. Mainstream SSL image classification methods mostly optimize a loss that additively combines a supervised classification objective with a regularization term derived solely from unlabeled data. This formulation neglects the potential for interaction between labeled and unlabeled images. In this paper, we introduce InterLUDE, a new approach to enhance SSL made of two parts that each benefit from labeled-unlabeled interaction. The first part, embedding fusion, interpolates between labeled and unlabeled embeddings to improve representation learning. The second part is a new loss, grounded in the principle of consistency regularization, that aims to minimize discrepancies in the model's predictions between labeled versus unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a medical SSL task with an uncurated u
    
[^221]: 使用子群阈值优化提高信贷借贷模型公平性

    Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization

    [https://arxiv.org/abs/2403.10652](https://arxiv.org/abs/2403.10652)

    使用子群阈值优化（STO）技术，在不改动训练数据和底层机器学习算法的情况下，优化各个子群的分类阈值，以最小化整体歧视分数，从而提高信贷借贷模型的公平性。

    

    为提高信贷借贷决策的准确性，许多金融机构现在使用机器学习模型的预测。然而，最近的研究表明，这些预测有可能对人口的某些子群体存在偏见和不公平。为了解决这个问题，引入了几种技术来消除偏见，提高预测的整体公平性。我们介绍了一种名为“Subgroup Threshold Optimizer”（简称STO）的新的公平性技术，它不需要对输入训练数据进行任何改动，也不需要对底层机器学习算法进行任何更改，因此可以与任何现有的机器学习流程一起使用。STO通过优化各个子群的分类阈值，以最小化它们之间的整体歧视分数。我们在一个真实世界的实验中

    arXiv:2403.10652v1 Announce Type: new  Abstract: In an effort to improve the accuracy of credit lending decisions, many financial intuitions are now using predictions from machine learning models. While such predictions enjoy many advantages, recent research has shown that the predictions have the potential to be biased and unfair towards certain subgroups of the population. To combat this, several techniques have been introduced to help remove the bias and improve the overall fairness of the predictions. We introduce a new fairness technique, called \textit{Subgroup Threshold Optimizer} (\textit{STO}), that does not require any alternations to the input training data nor does it require any changes to the underlying machine learning algorithm, and thus can be used with any existing machine learning pipeline. STO works by optimizing the classification thresholds for individual subgroups in order to minimize the overall discrimination score between them. Our experiments on a real-world 
    
[^222]: PALM：推进用于持续测试时间自适应的自适应学习率机制

    PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation

    [https://arxiv.org/abs/2403.10650](https://arxiv.org/abs/2403.10650)

    本研究通过对模型预测不确定性的量化来选择需要进一步适应的层，从而克服了持续测试时间自适应方法中由于伪标签引起的不准确性困扰。

    

    实际环境中的视觉模型面临领域分布的快速转变，导致识别性能下降。持续测试时间自适应（CTTA）直接根据测试数据调整预训练的源判别模型以适应这些不断变化的领域。一种高度有效的CTTA方法涉及应用逐层自适应学习率，并选择性地调整预训练层。然而，它受到领域转移估计不准确和由伪标签引起的不准确性所困扰。在这项工作中，我们旨在通过识别层来克服这些限制，通过对模型预测不确定性的量化来选择层，而无须依赖伪标签。我们利用梯度的大小作为一个度量标准，通过反向传播softmax输出与均匀分布之间的KL散度来计算，以选择需要进一步适应的层。随后，仅属于这些层的参数将被进一步适应。

    arXiv:2403.10650v1 Announce Type: cross  Abstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Continual test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains using test data. A highly effective CTTA method involves applying layer-wise adaptive learning rates, and selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. In this work, we aim to overcome these limitations by identifying layers through the quantification of model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these se
    
[^223]: 用于基于机器学习的网络安全任务的源代码表示调查

    A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks

    [https://arxiv.org/abs/2403.10646](https://arxiv.org/abs/2403.10646)

    本研究调查了用于基于机器学习的网络安全任务的源代码表示方法，发现基于图的表示是最受欢迎的，而分词器和抽象语法树是最流行的两种表示方法。

    

    arXiv:2403.10646v1 公告类型: 新的 摘要: 用于网络安全相关软件工程任务的机器学习技术越来越受欢迎。源代码的表示是该技术的关键部分，可以影响模型学习源代码特征的方式。随着越来越多的这些技术被开发，了解该领域的当前状态对于更好地理解已有内容和尚未存在的内容是有价值的。本文对这些现有的基于ML的方法进行了研究，展示了不同网络安全任务和编程语言所使用的表示类型。此外，我们研究了不同表示所使用的模型类型。我们发现基于图的表示是最受欢迎的表示类别，而分词器和抽象语法树（ASTs）是最受欢迎的表示方法。我们还发现网络安全领域最受欢迎的

    arXiv:2403.10646v1 Announce Type: new  Abstract: Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what's not there yet. This paper presents a study of these existing ML-based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that graph-based representations are the most popular category of representation, and Tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall. We also found that the most popular cybersecur
    
[^224]: 使用不确定性量化来表征和改进偏微分方程的区域外学习

    Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs

    [https://arxiv.org/abs/2403.10642](https://arxiv.org/abs/2403.10642)

    通过集成多个神经算子来提高对区域外学习的不确定性估计，从而解决现有方法在OOD测试输入上的失败

    

    存在于科学机器学习（SciML）领域中的现有工作表明，通过数据驱动学习解算符可以为经典数值偏微分方程（PDE）求解器提供一个快速的近似替代方案。在其中，神经算子（NOs）已经被认为尤为具有前景。我们观察到，对于区域外（OOD）测试输入，几种NOs的不确定性量化（UQ）方法甚至在模型对于域内任务的解近似良好时也会失败。为了解决这个限制，我们展示了集成几个NOs可以识别高误差区域，并提供良好与预测误差相关的不确定性估计。基于此，我们提出了一种经济有效的替代方案，DiverseNO，通过鼓励其最后前向传播层中的多个头部进行多样化预测来模拟集成的属性。然后，我们介绍了一种使用t

    arXiv:2403.10642v1 Announce Type: new  Abstract: Existing work in scientific machine learning (SciML) has shown that data-driven learning of solution operators can provide a fast approximate alternative to classical numerical partial differential equation (PDE) solvers. Of these, Neural Operators (NOs) have emerged as particularly promising. We observe that several uncertainty quantification (UQ) methods for NOs fail for test inputs that are even moderately out-of-domain (OOD), even when the model approximates the solution well for in-domain tasks. To address this limitation, we show that ensembling several NOs can identify high-error regions and provide good uncertainty estimates that are well-correlated with prediction errors. Based on this, we propose a cost-effective alternative, DiverseNO, that mimics the properties of the ensemble by encouraging diverse predictions from its multiple heads in the last feed-forward layer. We then introduce Operator-ProbConserv, a method that uses t
    
[^225]: 一种面向无家可归者街头外展和收集可食食物的资源受限随机调度算法

    A resource-constrained stochastic scheduling algorithm for homeless street outreach and gleaning edible food

    [https://arxiv.org/abs/2403.10638](https://arxiv.org/abs/2403.10638)

    该研究针对无家可归者和食品银行的资源受限外展问题，提出了一种基于Thompson抽样与马尔可夫链恢复的算法，显著优于基线算法。

    

    我们开发了一种通用的算法解决方案，解决了社会变革组织在资源受限外展中遇到的问题，这些组织拥有不同的使命和运营方式：Breaking Ground——一家帮助纽约无家可归者过渡至永久住房的组织和以色列全国食品银行Leket。具体地，我们针对部分观测的周期性不安定赌徒问题在$k$-步转移下开发了一种估计和优化方法。结果表明，我们提出的Thompson抽样与具有马尔可夫链恢复（通过Stein变分梯度下降）的算法在两个组织的问题上明显优于基线。我们以远景方式进行了这项工作，旨在设计一个足够灵活但也足够有用的解决方案，以帮助克服对数据可持续影响缺乏的问题。

    arXiv:2403.10638v1 Announce Type: new  Abstract: We developed a common algorithmic solution addressing the problem of resource-constrained outreach encountered by social change organizations with different missions and operations: Breaking Ground -- an organization that helps individuals experiencing homelessness in New York transition to permanent housing and Leket -- the national food bank of Israel that rescues food from farms and elsewhere to feed the hungry. Specifically, we developed an estimation and optimization approach for partially-observed episodic restless bandits under $k$-step transitions. The results show that our Thompson sampling with Markov chain recovery (via Stein variational gradient descent) algorithm significantly outperforms baselines for the problems of both organizations. We carried out this work in a prospective manner with the express goal of devising a flexible-enough but also useful-enough solution that can help overcome a lack of sustainable impact in da
    
[^226]: 近似中位数处理效应的极限

    Limits of Approximating the Median Treatment Effect

    [https://arxiv.org/abs/2403.10618](https://arxiv.org/abs/2403.10618)

    估计中位数差异比估计中位数处理效应更容易的问题是因果推断的基本问题

    

    平均处理效应（ATE）的估计是因果推断中一个经过深入研究的问题。然而，它并不一定能捕捉数据中的异质性，因此提出了几种方法来解决这个问题，包括估计分位处理效应。在包含$n$个个体的有限人群设置中，治疗和对照值用潜在结果向量$\mathbf{a}, \mathbf{b}$表示，大部分先前的工作侧重于估计median$(\mathbf{a}) -$ median$(\mathbf{b})$，其中median($\mathbf x$)表示向量$\mathbf x$中所有值排序后的中位数值。众所周知，估计中位数差异比估计中位数$(\mathbf{a-b})$更容易，即所谓的中位数处理效应（MTE）。因果推断的基本问题是对于每个个体$i$，我们只能观测到潜在结果值之一，即

    arXiv:2403.10618v1 Announce Type: cross  Abstract: Average Treatment Effect (ATE) estimation is a well-studied problem in causal inference. However, it does not necessarily capture the heterogeneity in the data, and several approaches have been proposed to tackle the issue, including estimating the Quantile Treatment Effects. In the finite population setting containing $n$ individuals, with treatment and control values denoted by the potential outcome vectors $\mathbf{a}, \mathbf{b}$, much of the prior work focused on estimating median$(\mathbf{a}) -$ median$(\mathbf{b})$, where median($\mathbf x$) denotes the median value in the sorted ordering of all the values in vector $\mathbf x$. It is known that estimating the difference of medians is easier than the desired estimand of median$(\mathbf{a-b})$, called the Median Treatment Effect (MTE). The fundamental problem of causal inference -- for every individual $i$, we can only observe one of the potential outcome values, i.e., either the
    
[^227]: DiPaCo: 分布式路径组合

    DiPaCo: Distributed Path Composition

    [https://arxiv.org/abs/2403.10616](https://arxiv.org/abs/2403.10616)

    DiPaCo提出了一种协同模块化架构和训练方法，可以通过路径分发计算，实现机器学习模型的训练，并在推断时无需模型压缩。

    

    机器学习（ML）领域的进展得益于扩展神经网络模型。这种扩展是通过不断壮举的工程努力实现的，以适应需要设备之间高带宽通信的并行ML方法。在这项工作中，我们提出了一个为ML模型设计的协同模块化架构和训练方法，称为DIstributed PAth COmposition（DiPaCo）。在训练过程中，DiPaCo通过一组共享模块的路径进行计算分发。结合一种受Local-SGD启发的优化方法（DiLoCo），该方法通过大幅减少通信来确保模块同步，促进了跨连接不佳且异构的工作节点的训练，其设计确保了对工作节点故障和抢占的稳健性。在推断时，每个输入只需要执行一条路径，无需任何模型压缩。我们认为这是一种首创性的方法。

    arXiv:2403.10616v1 Announce Type: cross  Abstract: Progress in machine learning (ML) has been fueled by scaling neural network models. This scaling has been enabled by ever more heroic feats of engineering, necessary for accommodating ML approaches that require high bandwidth communication between devices working in parallel. In this work, we propose a co-designed modular architecture and training approach for ML models, dubbed DIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes computation by paths through a set of shared modules. Together with a Local-SGD inspired optimization (DiLoCo) that keeps modules in sync with drastically reduced communication, Our approach facilitates training across poorly connected and heterogeneous workers, with a design that ensures robustness to worker failures and preemptions. At inference time, only a single path needs to be executed for each input, without the need for any model compression. We consider this approach as a first 
    
[^228]: LightIt：扩散模型的照明建模与控制

    LightIt: Illumination Modeling and Control for Diffusion Models

    [https://arxiv.org/abs/2403.10615](https://arxiv.org/abs/2403.10615)

    LightIt提出了一种用于图像生成的显式照明控制方法，通过条件生成来实现对图像生成的照明控制，同时训练了一个身份保持的重照模型。

    

    我们介绍了LightIt，这是一种用于图像生成的显式照明控制方法。最近的生成方法缺乏照明控制，而这对于图像生成的许多艺术方面至关重要，比如设置整体情绪或电影外观。为了克服这些限制，我们提出在生成过程中以遮蔽和法线图为条件。我们使用单次反射遮蔽来建模照明，包括投射阴影。我们首先训练一个遮蔽估计模块来生成真实世界图像和遮蔽对的数据集。然后，我们使用估计的遮蔽和法线作为输入训练控制网络。我们的方法展示了在许多场景中高质量的图像生成和照明控制。此外，我们使用我们生成的数据集来训练一个保持身份的重照模型，以图像和目标遮蔽为条件。我们的方法是第一个能够生成具有可控、一致性的图像的方法。

    arXiv:2403.10615v1 Announce Type: cross  Abstract: We introduce LightIt, a method for explicit illumination control for image generation. Recent generative methods lack lighting control, which is crucial to numerous artistic aspects of image generation such as setting the overall mood or cinematic appearance. To overcome these limitations, we propose to condition the generation on shading and normal maps. We model the lighting with single bounce shading, which includes cast shadows. We first train a shading estimation module to generate a dataset of real-world images and shading pairs. Then, we train a control network using the estimated shading and normals as input. Our method demonstrates high-quality image generation and lighting control in numerous scenes. Additionally, we use our generated dataset to train an identity-preserving relighting model, conditioned on an image and a target shading. Our method is the first that enables the generation of images with controllable, consisten
    
[^229]: 顺序蒙特卡洛在摊销变分推理中包容KL最小化的应用

    Sequential Monte Carlo for Inclusive KL Minimization in Amortized Variational Inference

    [https://arxiv.org/abs/2403.10610](https://arxiv.org/abs/2403.10610)

    用顺序蒙特卡洛采样器估计inclusive KL散度梯度，提出了三种梯度估计器，解决了现有方法的偏差梯度和高度集中变分分布问题。

    

    用来训练编码器网络执行摊销变分推理，从精确后验分布到其近似分布的KL散度，即包容或正向KL，是因其极小化值的区域覆盖性质而成为变分目标的越来越流行选择。然而，最小化这一目标是具有挑战性的。作为一种替代方法，我们提出了SMC-Wake，该过程用于拟合一种摊销变分近似，其使用调节似然的顺序蒙特卡罗采样器来估计包容KL散度的梯度。我们提出了三种梯度估计器，其中对迭代次数是渐近无偏的两种是强一致的。我们的方法交替使用随机梯度

    arXiv:2403.10610v1 Announce Type: new  Abstract: For training an encoder network to perform amortized variational inference, the Kullback-Leibler (KL) divergence from the exact posterior to its approximation, known as the inclusive or forward KL, is an increasingly popular choice of variational objective due to the mass-covering property of its minimizer. However, minimizing this objective is challenging. A popular existing approach, Reweighted Wake-Sleep (RWS), suffers from heavily biased gradients and a circular pathology that results in highly concentrated variational distributions. As an alternative, we propose SMC-Wake, a procedure for fitting an amortized variational approximation that uses likelihood-tempered sequential Monte Carlo samplers to estimate the gradient of the inclusive KL divergence. We propose three gradient estimators, all of which are asymptotically unbiased in the number of iterations and two of which are strongly consistent. Our method interleaves stochastic gr
    
[^230]: SurvRNC：使用Rank-N-Contrast学习有序表示来进行生存预测

    SurvRNC: Learning Ordered Representations for Survival Prediction using Rank-N-Contrast

    [https://arxiv.org/abs/2403.10603](https://arxiv.org/abs/2403.10603)

    提出了SurvRNC方法，通过引入损失函数作为正则化器来获得基于生存时间的有序表示，能处理被截尾数据，并可整合到任何生存模型中。

    

    预测生存的可能性对于被诊断为癌症的个体来说至关重要，因为它提供了关于早期预后的宝贵信息。这种知识使得制定有效的治疗方案成为可能，从而带来改善患者结局的结果。在过去几年中，深度学习模型为评估医学图像、电子健康记录和基因组数据以估计癌症风险分数提供了可行的解决方案。然而，这些模型通常没有充分发挥其潜力，因为它们很难学习具有回归意识的特征表示。在本研究中，我们提出Survival Rank-N Contrast（SurvRNC）方法，该方法引入了一种损失函数作为正则化器，以根据生存时间获得一个有序表示。该函数可以处理被截尾的数据，并可以被整合到任何生存模型中，以确保所学得的表示是顺序的。

    arXiv:2403.10603v1 Announce Type: cross  Abstract: Predicting the likelihood of survival is of paramount importance for individuals diagnosed with cancer as it provides invaluable information regarding prognosis at an early stage. This knowledge enables the formulation of effective treatment plans that lead to improved patient outcomes. In the past few years, deep learning models have provided a feasible solution for assessing medical images, electronic health records, and genomic data to estimate cancer risk scores. However, these models often fall short of their potential because they struggle to learn regression-aware feature representations. In this study, we propose Survival Rank-N Contrast (SurvRNC) method, which introduces a loss function as a regularizer to obtain an ordered representation based on the survival times. This function can handle censored data and can be incorporated into any survival model to ensure that the learned representation is ordinal. The model was extensi
    
[^231]: 从算法到结果：审视人工智能在非肌层侵袭性膀胱癌复发预测中的作用

    From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction

    [https://arxiv.org/abs/2403.10586](https://arxiv.org/abs/2403.10586)

    机器学习技术在非肌层侵袭性膀胱癌复发预测中具有潜在作用，可以提高准确性，降低治疗成本，并有效规划治疗方案

    

    膀胱癌是英国每天造成15人死亡的领先泌尿道癌症。这种癌症主要表现为非肌层侵袭性膀胱癌（NMIBC），其特点是肿瘤还未渗透到膀胱壁的肌肉层。 NMIBC的复发率非常高，达到70-80％，因此治疗成本最高。目前用于预测复发的工具使用评分系统来高估风险，并具有较低的准确性。对复发的不准确和延迟预测显著提高了死亡的可能性。因此，准确预测复发对于成本效益的管理和治疗计划至关重要。这就是机器学习（ML）技术出现的地方，通过利用分子和临床数据预测NMIBC复发，成为一种有前途的方法。本次审查对预测NMIBC复发的ML方法进行了全面分析。我们的系统评估使

    arXiv:2403.10586v1 Announce Type: cross  Abstract: Bladder cancer, the leading urinary tract cancer, is responsible for 15 deaths daily in the UK. This cancer predominantly manifests as non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very high recurrence rate of 70-80% and hence the costliest treatments. Current tools for predicting recurrence use scoring systems that overestimate risk and have poor accuracy. Inaccurate and delayed prediction of recurrence significantly elevates the likelihood of mortality. Accurate prediction of recurrence is hence vital for cost-effective management and treatment planning. This is where Machine learning (ML) techniques have emerged as a promising approach for predicting NMIBC recurrence by leveraging molecular and clinical data. This review provides a comprehensive analysis of ML approaches for predicting NMIBC recurrence. Our systematic evaluation de
    
[^232]: 通过后验抽样解决一般性噪声逆问题：一个策略梯度的视角

    Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint

    [https://arxiv.org/abs/2403.10585](https://arxiv.org/abs/2403.10585)

    提出了一种通过后验抽样解决一般性噪声逆问题的方法，采用策略梯度视角，通过扩散策略梯度（DPG）精确估计指导评分函数，实现对多种线性和非线性逆任务的鲁棒性解决，提高了图像恢复质量。

    

    解决图像逆问题（例如，超分辨率和修复）需要生成一个与给定输入（低分辨率图像或遮挡图像）相匹配的高保真图像。通过使用输入图像作为指导，我们可以利用预训练的扩散生成模型来解决各种图像逆任务，而无需针对特定任务微调模型。为了精确估计输入图像的指导评分函数，我们提出了扩散策略梯度（DPG），这是一种可行的计算方法，通过将中间噪声图像视为策略，将目标图像视为策略选择的状态。实验表明，我们的方法对多个线性和非线性逆任务上的高斯和泊松噪声退化都具有鲁棒性，从而在FFHQ、ImageNet和LSUN数据集上实现更高的图像恢复质量。

    arXiv:2403.10585v1 Announce Type: cross  Abstract: Solving image inverse problems (e.g., super-resolution and inpainting) requires generating a high fidelity image that matches the given input (the low-resolution image or the masked image). By using the input image as guidance, we can leverage a pretrained diffusion generative model to solve a wide range of image inverse tasks without task specific model fine-tuning. To precisely estimate the guidance score function of the input image, we propose Diffusion Policy Gradient (DPG), a tractable computation method by viewing the intermediate noisy images as policies and the target image as the states selected by the policy. Experiments show that our method is robust to both Gaussian and Poisson noise degradation on multiple linear and non-linear inverse tasks, resulting into a higher image restoration quality on FFHQ, ImageNet and LSUN datasets.
    
[^233]: 使用来自不同身体部位的视频和目标训练rPPG模型有多次优？

    How Suboptimal is Training rPPG Models with Videos and Targets from Different Body Sites?

    [https://arxiv.org/abs/2403.10582](https://arxiv.org/abs/2403.10582)

    许多rPPG模型是使用面部视频和手指接触PPG测量作为目标进行训练的，但不同身体部位的PPG信号具有不同的形态特征，这样训练面部视频的rPPG模型可能是次优的。

    

    通过光电容积描记法（rPPG）远程摄像头测量脉搏血容量是一种引人注目的技术，可用于可伸缩、低成本和易获取的心血管信息评估。神经网络目前为此任务提供了最先进的技术，而监督训练或微调是创建这些模型的重要步骤。然而，大多数当前的模型是使用来自手指的接触PPG测量作为目标/标签，通过面部视频进行训练的。迄今为止，很少有公共数据集融合了来自面部的接触PPG测量数据。然而，大量证据表明，不同身体部位的PPG信号具有非常不同的形态特征。使用最近发布的具有同步接触PPG和视频测量数据的独特数据集...

    arXiv:2403.10582v1 Announce Type: cross  Abstract: Remote camera measurement of the blood volume pulse via photoplethysmography (rPPG) is a compelling technology for scalable, low-cost, and accessible assessment of cardiovascular information. Neural networks currently provide the state-of-the-art for this task and supervised training or fine-tuning is an important step in creating these models. However, most current models are trained on facial videos using contact PPG measurements from the fingertip as targets/ labels. One of the reasons for this is that few public datasets to date have incorporated contact PPG measurements from the face. Yet there is copious evidence that the PPG signals at different sites on the body have very different morphological features. Is training a facial video rPPG model using contact measurements from another site on the body suboptimal? Using a recently released unique dataset with synchronized contact PPG and video measurements from both the hand and fa
    
[^234]: 大型语言模型指导的心力衰竭风险预测的ECG双注意力网络

    Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction

    [https://arxiv.org/abs/2403.10581](https://arxiv.org/abs/2403.10581)

    提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。

    

    心力衰竭（HF）由于全球死亡率不断上升而构成重大公共卫生挑战。通过早期诊断和预防来解决这一问题可显著减少疾病对社会的影响。本文引入了一种使用临床获取的12导联心电图（ECG）进行HF风险预测的方法。我们提出了一种新颖的、轻量级的双注意力ECG网络，旨在捕捉对早期HF预测至关重要的复杂心电图特征，尽管低风险和高风险组之间存在明显的不平衡。该网络具有一个跨导注意力模块和12个导联特定的时间注意力模块，以捕捉交叉导联交互作用和每个导联内的局部时间动态。为了防止模型过拟合于有限的训练数据，我们利用一个大型语言模型（LLM）与公共ECG-Report数据集进行预训练，用于进行ECG-报告对齐任务。然后对网络进行fine-tune以用于HF风险预测

    arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
    
[^235]: 随机旋转浅水噪声的生成建模

    Generative Modelling of Stochastic Rotating Shallow Water Noise

    [https://arxiv.org/abs/2403.10578](https://arxiv.org/abs/2403.10578)

    本文提出一种用于校准流体动力学随机偏微分方程中噪声的通用方法，使用生成模型技术取代了以往的PCA技术，这能够避免对增量施加额外约束。

    

    在最近的工作中，作者们开发了一种用于校准流体动力学随机偏微分方程中噪声的通用方法，其中随机性被引入以参数化次网格尺度过程。子网格尺度过程的随机参数化在天气和气候预测的不确定性估计中是必需的，以表示由子网格尺度波动引起的系统模型误差。以前的方法使用基于随机参数化增量为正态分布的假设的主成分分析（PCA）技术。在本文中，PCA技术被生成模型技术所取代。这使我们能够避免对增量施加额外约束。该方法在具有模型高程变量作为输入数据的随机旋转浅水模型上进行了测试。数值模拟表明

    arXiv:2403.10578v1 Announce Type: cross  Abstract: In recent work, the authors have developed a generic methodology for calibrating the noise in fluid dynamics stochastic partial differential equations where the stochasticity was introduced to parametrize subgrid-scale processes. The stochastic parameterization of sub-grid scale processes is required in the estimation of uncertainty in weather and climate predictions, to represent systematic model errors arising from subgrid-scale fluctuations. The previous methodology used a principal component analysis (PCA) technique based on the ansatz that the increments of the stochastic parametrization are normally distributed.   In this paper, the PCA technique is replaced by a generative model technique. This enables us to avoid imposing additional constraints on the increments. The methodology is tested on a stochastic rotating shallow water model with the elevation variable of the model used as input data. The numerical simulations show that
    
[^236]: 忽略我但不要替代我：利用非语言元素进行网络安全领域的预训练

    Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain

    [https://arxiv.org/abs/2403.10576](https://arxiv.org/abs/2403.10576)

    利用非语言元素进行网络安全领域的预训练，提出了新的预训练方法并在网络安全领域中取得了优越表现

    

    针对网络安全信息通常技术复杂且通过非结构化文本传递，使得自动化处理网络威胁情报变得极具挑战性。针对涉及高度专业知识的文本领域，基于领域语料库的预训练一直是语言模型获取领域专业知识的一种常见方法。然而，网络安全文本通常包含非语言元素（如URL和哈希值），这可能与现有的预训练方法不适用。先前在其他领域的工作中，已将此类文本视为噪音进行移除或过滤，但这些方法的有效性尚未得到调查，特别是在网络安全领域。我们提出了不同的预训练方法，并通过下游任务和探测任务评估了它们的有效性。我们提出的策略（选择性MLM和联合训练NLE标记分类）优于常用的替换非

    arXiv:2403.10576v1 Announce Type: cross  Abstract: Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We propose different pretraining methodologies and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy (selective MLM and jointly training NLE token classification) outperforms the commonly taken approach of replacing non-l
    
[^237]: 医学不可学习的示例：通过稀疏感知本地蒙版保护医学数据免受未经授权训练

    Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking

    [https://arxiv.org/abs/2403.10573](https://arxiv.org/abs/2403.10573)

    引入医学数据中的难以察觉噪声来保护数据，防止未经授权的训练，尤其适用于生物医学数据领域。

    

    随着人工智能在医疗保健领域的快速增长，敏感医学数据的生成和存储显著增加。这种数据的丰富量推动了医学人工智能技术的进步。然而，对于未经授权的数据利用，例如用于训练商业人工智能模型，常常使研究人员望而却步，因为他们不愿公开其宝贵的数据集。为了保护这些难以收集的数据，同时鼓励医疗机构分享数据，一个有前途的解决方案是向数据中引入难以察觉的噪声。这种方法旨在通过在模型泛化中引入退化来保护数据，防止未经授权的训练。尽管现有方法在一般领域显示出令人钦佩的数据保护能力，但当应用于生物医学数据时往往表现不佳，主要是因为它们未能考虑到稀疏性。

    arXiv:2403.10573v1 Announce Type: cross  Abstract: With the rapid growth of artificial intelligence (AI) in healthcare, there has been a significant increase in the generation and storage of sensitive medical data. This abundance of data, in turn, has propelled the advancement of medical AI technologies. However, concerns about unauthorized data exploitation, such as training commercial AI models, often deter researchers from making their invaluable datasets publicly available. In response to the need to protect this hard-to-collect data while still encouraging medical institutions to share it, one promising solution is to introduce imperceptible noise into the data. This method aims to safeguard the data against unauthorized training by inducing degradation in model generalization. Although existing methods have shown commendable data protection capabilities in general domains, they tend to fall short when applied to biomedical data, mainly due to their failure to account for the spar
    
[^238]: 发现异性图的不变邻域模式

    Discovering Invariant Neighborhood Patterns for Heterophilic Graphs

    [https://arxiv.org/abs/2403.10572](https://arxiv.org/abs/2403.10572)

    本文提出了一种新颖的不变邻域模式学习方法，通过自适应邻域传播模块和不变非同源图学习模块，解决了非同源图上邻域模式分布偏移问题。

    

    本文研究了非同源图上的分布偏移问题。大多数现有的图神经网络方法依赖于同源假设，即同一类别的节点更有可能被连接。然而，在现实世界的图中，这种同源性假设并不总是成立，这导致了在先前的方法中未能解释的更复杂的分布偏移。在非同源图上，邻域模式的分布偏移更加多样化。我们提出了一种新颖的不变邻域模式学习（INPL）方法，以缓解非同源图上的分布偏移问题。具体来说，我们提出了自适应邻域传播（ANP）模块来捕获自适应的邻域信息，这可以缓解非同源图上的邻域模式分布偏移问题。我们提出了不变非同源图学习（INHGL）模块，其约束了ANP并进行学习。

    arXiv:2403.10572v1 Announce Type: new  Abstract: This paper studies the problem of distribution shifts on non-homophilous graphs Mosting existing graph neural network methods rely on the homophilous assumption that nodes from the same class are more likely to be linked. However, such assumptions of homophily do not always hold in real-world graphs, which leads to more complex distribution shifts unaccounted for in previous methods. The distribution shifts of neighborhood patterns are much more diverse on non-homophilous graphs. We propose a novel Invariant Neighborhood Pattern Learning (INPL) to alleviate the distribution shifts problem on non-homophilous graphs. Specifically, we propose the Adaptive Neighborhood Propagation (ANP) module to capture the adaptive neighborhood information, which could alleviate the neighborhood pattern distribution shifts problem on non-homophilous graphs. We propose Invariant Non-Homophilous Graph Learning (INHGL) module to constrain the ANP and learn in
    
[^239]: JaxDecompiler：重新定义基于梯度信息的软件设计

    JaxDecompiler: Redefining Gradient-Informed Software Design

    [https://arxiv.org/abs/2403.10571](https://arxiv.org/abs/2403.10571)

    JaxDecompiler是一种将JAX函数转换为可编辑Python代码的工具，简化了基于梯度信息的软件开发中的反向工程、理解、定制和互操作过程。

    

    在能够计算梯度下降优化的数值库中，JAX以其提供更多功能而脱颖而出，其加速效果来自一个名为Jaxpr语言的中间表示。然而，直接编辑Jaxpr代码是不可能的。本文介绍了JaxDecompiler，一种将任何JAX函数转换为可编辑Python代码的工具，特别适用于编辑由梯度函数生成的JAX函数。JaxDecompiler简化了通过JAX开发的软件的反向工程、理解、定制和互操作的过程。我们突出了它的功能，强调了它在深度学习和更一般的基于梯度信息的软件中的实际应用，并且证明了反编译代码的速度性能与原始代码的相似性。

    arXiv:2403.10571v1 Announce Type: cross  Abstract: Among numerical libraries capable of computing gradient descent optimization, JAX stands out by offering more features, accelerated by an intermediate representation known as Jaxpr language. However, editing the Jaxpr code is not directly possible. This article introduces JaxDecompiler, a tool that transforms any JAX function into an editable Python code, especially useful for editing the JAX function generated by the gradient function. JaxDecompiler simplifies the processes of reverse engineering, understanding, customizing, and interoperability of software developed by JAX. We highlight its capabilities, emphasize its practical applications especially in deep learning and more generally gradient-informed software, and demonstrate that the decompiled code speed performance is similar to the original.
    
[^240]: 在资源受限的边缘环境中利用高效参数缩减实现DNN的帕累托最优性

    Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment

    [https://arxiv.org/abs/2403.10569](https://arxiv.org/abs/2403.10569)

    通过在Xception上实施高效的参数缩减策略，该研究在资源受限的边缘环境中实现了DNN的帕累托最优性，提高了模型的准确性，减少了内存利用，且在Caltech-101图像分类中表现优于原始Xception和轻量级模型。

    

    本文提出了对现有深度神经网络（DNN）进行优化，改善其硬件利用率，并为资源受限的边缘环境提供便于设备上训练的条件。我们在Xception上实施了高效的参数缩减策略，缩小模型大小而不损失准确性，从而减少训练过程中的内存利用。我们在两个实验中评估了我们的模型：Caltech-101图像分类和PCB缺陷检测，并将其性能与原始的Xception和轻量级模型EfficientNetV2B1和MobileNetV2进行了比较。Caltech-101图像分类的结果显示，我们的模型具有更好的测试准确度（76.21%），比Xception（75.89%）平均使用更少的内存（847.9MB比Xception的874.6MB），并且训练和推理时间更快。轻量级模型存在过拟合问题，EfficientNetV2B1的测试准确度为30.52%，MobileNetV2的测试准确度为58.11%。

    arXiv:2403.10569v1 Announce Type: cross  Abstract: This paper proposes an optimization of an existing Deep Neural Network (DNN) that improves its hardware utilization and facilitates on-device training for resource-constrained edge environments. We implement efficient parameter reduction strategies on Xception that shrink the model size without sacrificing accuracy, thus decreasing memory utilization during training. We evaluate our model in two experiments: Caltech-101 image classification and PCB defect detection and compare its performance against the original Xception and lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the Caltech-101 image classification show that our model has a better test accuracy (76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than Xception (874.6MB), and has faster training and inference times. The lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy and MobileNetV2 having a 58.11% test acc
    
[^241]: MoPE：通过Prompt专家混合实现参数高效和可扩展的多模态融合

    MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts

    [https://arxiv.org/abs/2403.10568](https://arxiv.org/abs/2403.10568)

    本文提出了MoPE技术，通过解开提示以自适应捕获数据集级和实例级特征，引入了混合Prompt专家来增强表达能力，并且在多模态融合中表现出更大的表达能力和可扩展性。

    

    Prompt调整已经证明在融合多模态任务的单模基础模型时具有参数效率性。然而，其有限的适应性和表达能力导致性能不佳与其他调整方法相比。本文通过将简单提示解开以自适应地捕获数据集级和实例级特征来解决这个问题。建立在这种解开的基础上，我们引入了Prompt专家的混合（MoPE）技术来增强表达能力。MoPE利用多模态配对先验在每个实例基础上路由最有效的提示。与简单提示相比，我们基于MoPE的条件提示对多模态融合具有更大的表达能力，在训练数据和可训练参数总数上具有更好的扩展性。我们还研究了一个专家路由的正则化项，导致专家的不断发展专长，不同专家专注于不同的特征。

    arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
    
[^242]: 集成学习中的卫星降水空间插值不确定性估计

    Uncertainty estimation in spatial interpolation of satellite precipitation with ensemble learning

    [https://arxiv.org/abs/2403.10567](https://arxiv.org/abs/2403.10567)

    引入九种集成学习器并利用新颖特征工程策略，结合多种分位数回归算法，填补了空间插值中集成学习的不确定性估计领域的研究空白

    

    arXiv:2403.10567v1 公告类型：新的 摘要：概率分布形式的预测对决策至关重要。分位数回归在空间插值设置中能够合并遥感和雨量数据，实现此目标。然而，在这种情境下，分位数回归算法的集成学习尚未被研究。本文通过引入九种基于分位数的集成学习器并将其应用于大型降水数据集来填补这一空白。我们采用了一种新颖的特征工程策略，将预测因子减少为相关位置的加权距离卫星降水，结合位置高程。我们的集成学习器包括六种堆叠方法和三种简单方法（均值、中位数、最佳组合器），结合了六种个体算法：分位数回归(QR)、分位数回归森林(QRF)、广义随机森林(GRF)、梯度提升机(GBM)、轻量级梯度提升机(LightGBM)和分位数回归神经网络

    arXiv:2403.10567v1 Announce Type: new  Abstract: Predictions in the form of probability distributions are crucial for decision-making. Quantile regression enables this within spatial interpolation settings for merging remote sensing and gauge precipitation data. However, ensemble learning of quantile regression algorithms remains unexplored in this context. Here, we address this gap by introducing nine quantile-based ensemble learners and applying them to large precipitation datasets. We employed a novel feature engineering strategy, reducing predictors to distance-weighted satellite precipitation at relevant locations, combined with location elevation. Our ensemble learners include six stacking and three simple methods (mean, median, best combiner), combining six individual algorithms: quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neur
    
[^243]: 电池单元布局的冷却引导扩散模型

    Cooling-Guide Diffusion Model for Battery Cell Arrangement

    [https://arxiv.org/abs/2403.10566](https://arxiv.org/abs/2403.10566)

    导入冷却引导扩散模型的生成式AI方法优化了电池单元布局，显著降低了单元的最高温度，并在冷却效率方面具有独特优势。

    

    我们的研究引入了一种生成式人工智能方法，利用冷却引导扩散模型来优化电池单元的布局，这是增强电池热管理系统冷却性能和效率的关键步骤。与传统设计流程相比，我们的创新方法使用参数去噪扩散概率模型（DDPM）与分类器和冷却引导相结合，生成具有增强冷却路径的优化单元布局，显著降低了单元的最高温度。通过结合基于位置的分类器引导，我们确保生成的布局的可行性。同时，冷却引导直接优化冷却效率，使我们的方法独具效果。与两种先进模型相比，我们的方法在提高效率上表现出独特优势。

    arXiv:2403.10566v1 Announce Type: cross  Abstract: Our study introduces a Generative AI method that employs a cooling-guided diffusion model to optimize the layout of battery cells, a crucial step for enhancing the cooling performance and efficiency of battery thermal management systems. Traditional design processes, which rely heavily on iterative optimization and extensive guesswork, are notoriously slow and inefficient, often leading to suboptimal solutions. In contrast, our innovative method uses a parametric denoising diffusion probabilistic model (DDPM) with classifier and cooling guidance to generate optimized cell layouts with enhanced cooling paths, significantly lowering the maximum temperature of the cells. By incorporating position-based classifier guidance, we ensure the feasibility of generated layouts. Meanwhile, cooling guidance directly optimizes cooling-efficiency, making our approach uniquely effective. When compared to two advanced models, the Tabular Denoising Diff
    
[^244]: Counter-Samples: 一种抵御黑盒对抗攻击的无状态策略

    Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks

    [https://arxiv.org/abs/2403.10562](https://arxiv.org/abs/2403.10562)

    无状态策略通过评估反样本对抗黑盒查询，有效引入了防御者有利的不对称性。这种防御方法能够欺骗攻击者寻找对抗性示例、保持模型在合法输入上的准确性，且适用于多种攻击类型。

    

    我们的论文提出了一种新型的抵御黑盒攻击的方法，攻击者利用受害者模型作为 Oracle 来构建他们的对抗性样本。与传统的预处理防御方法不同，我们的无状态策略实际上对抗攻击过程本身。我们针对每个查询都评估一个反样本，其中反样本是针对攻击者的目标进行优化的原始样本。通过用有针对性的白盒优化来对抗每一个黑盒查询，我们的策略有效地为防御者引入了对称性，使其处于有利地位。这种防御不仅能够有效地误导攻击者寻找对抗性示例，还能保留在合法输入上的模型准确性，并且适用于多种类型的攻击。我们证明，我们的方法对最先进的黑盒攻击非常有效，并且在 C 上胜过现有的防御方法。

    arXiv:2403.10562v1 Announce Type: cross  Abstract: Our paper presents a novel defence against black box attacks, where attackers use the victim model as an oracle to craft their adversarial examples. Unlike traditional preprocessing defences that rely on sanitizing input samples, our stateless strategy counters the attack process itself. For every query we evaluate a counter-sample instead, where the counter-sample is the original sample optimized against the attacker's objective. By countering every black box query with a targeted white box optimization, our strategy effectively introduces an asymmetry to the game to the defender's advantage. This defence not only effectively misleads the attacker's search for an adversarial example, it also preserves the model's accuracy on legitimate inputs and is generic to multiple types of attacks.   We demonstrate that our approach is remarkably effective against state-of-the-art black box attacks and outperforms existing defences for both the C
    
[^245]: AAAI 2024年人本主义表示学习研讨会的被接受论文集合

    A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024

    [https://arxiv.org/abs/2403.10561](https://arxiv.org/abs/2403.10561)

    AAAI 2024年人本主义表示学习研讨会的被接受论文集合。部分论文选择退出。

    

    arXiv:2403.10561v1 公告类型: 跨项  摘要: 这份非存档索引并不完整，因为一些被接受的论文选择了退出。所有被接受论文的列表可在研讨会网站上找到。

    arXiv:2403.10561v1 Announce Type: cross  Abstract: This non-archival index is not complete, as some accepted papers chose to opt-out of inclusion. The list of all accepted papers is available on the workshop website.
    
[^246]: 生成模型与联网自动驾驶车辆：探索交通和人工智能交叉领域的调查

    Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI

    [https://arxiv.org/abs/2403.10559](https://arxiv.org/abs/2403.10559)

    生成模型与联网自动驾驶车辆的整合有望提升自动车辆的预测建模、模拟精度和决策流程，对交通行业的安全和创新具有潜在推动作用。

    

    这份报告调查了生成模型和联网自动驾驶车辆（CAVs）两种推动技术和交通进步的突破性力量的历史和影响。通过关注生成模型在CAVs背景下的应用，该研究旨在揭示这种整合如何提升自动驾驶车辆的预测建模、模拟精度和决策流程。本文讨论了在交通领域整合生成模型和CAV技术的益处和挑战，旨在强调取得的进展、剩余的障碍以及在安全和创新方面的潜力。

    arXiv:2403.10559v1 Announce Type: cross  Abstract: This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, simulation accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.
    
[^247]: 针对模型反演攻击的隐私保护人脸识别自适应混合遮盖策略

    Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack

    [https://arxiv.org/abs/2403.10558](https://arxiv.org/abs/2403.10558)

    本文提出了一种针对模型反演攻击的自适应混合遮盖算法，通过在频域中使用MixUp策略对人脸图像进行遮盖，以在隐私保护和准确性之间取得平衡。

    

    arXiv:2403.10558v1 公告类型: 跨领域 摘要: 在训练人脸识别（FR）模型中利用个人敏感数据存在重大的隐私问题，因为对手可以利用模型反演攻击（MIA）推断出原始训练数据。现有的防御方法，如数据增强和差分隐私，已被用来减轻这一问题。然而，这些方法往往无法在隐私和准确性之间达到最佳平衡。为解决这一局限性，本文介绍了一种针对MIA的自适应混合遮盖算法。具体地，在频域中使用自适应的MixUp策略对人脸图像进行遮盖。与主要用于数据增强的传统MixUp算法不同，我们修改的方法融合了频域混合。先前的研究表明，增加MixUp中混合的图像数量可以增强隐私保护，但会降低人脸识别的准确性。为了解决这一问题，

    arXiv:2403.10558v1 Announce Type: cross  Abstract: The utilization of personal sensitive data in training face recognition (FR) models poses significant privacy concerns, as adversaries can employ model inversion attacks (MIA) to infer the original training data. Existing defense methods, such as data augmentation and differential privacy, have been employed to mitigate this issue. However, these methods often fail to strike an optimal balance between privacy and accuracy. To address this limitation, this paper introduces an adaptive hybrid masking algorithm against MIA. Specifically, face images are masked in the frequency domain using an adaptive MixUp strategy. Unlike the traditional MixUp algorithm, which is predominantly used for data augmentation, our modified approach incorporates frequency domain mixing. Previous studies have shown that increasing the number of images mixed in MixUp can enhance privacy preservation but at the expense of reduced face recognition accuracy. To ove
    
[^248]: 二阶信息很重要：重访大型语言模型的机器遗忘问题

    Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2403.10557](https://arxiv.org/abs/2403.10557)

    本论文通过二阶信息（Hessian）的视角重新审视了大型语言模型的机器遗忘问题，提出了遗忘算法，具有较高的鲁棒性。

    

    随着大型语言模型（LLMs）的快速发展，我们目睹了ChatGPT、LLaMa和Gemini等主要LLM产品之间的激烈竞争。然而，训练语料库的各种问题（如隐私泄露和版权侵犯）仍然未被充分探讨。以LLM从业者的视角来看，处理这些意外的隐私侵犯可能具有挑战性。之前的研究通过使用梯度信息解决了LLMs的“遗忘”问题，但它们大多引入了显著的开销，如数据预处理或缺乏鲁棒性。在本文中，与基于一阶信息的方法形成对比，我们通过二阶信息（Hessian）的视角重新审视了遗忘问题。受经典牛顿更新启发，我们的遗忘算法具有

    arXiv:2403.10557v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are 
    
[^249]: KARINA：一种用于全球天气预测的高效深度学习模型

    KARINA: An Efficient Deep Learning Model for Global Weather Forecast

    [https://arxiv.org/abs/2403.10555](https://arxiv.org/abs/2403.10555)

    KARINA模型通过结合ConvNext、SENet和Geocyclic填充，在2.5°分辨率下提升天气预测准确性，只需较少的计算资源，展示出与更高分辨率模型相当的预测精度。

    

    基于深度学习的数据驱动模型在气候研究中越来越普遍，特别是在全球天气预测方面。然而，以高分辨率训练全球天气数据需要大量的计算资源。因此，我们提出了一种名为KARINA的新模型，以克服该领域典型的巨大计算需求。该模型通过只需4个NVIDIA A100 GPU和不到12小时的训练时间，即可实现与更高分辨率对手相媲美的预测准确性，从而大大减少了计算资源的需求。KARINA结合了ConvNext、SENet和Geocyclic填充以增强以2.5°分辨率进行的天气预测，可以过滤掉高频噪声。地理周期填充保留了输入图像的侧边界像素，从而在球形地球中保持大气流的连续性。SENet动态改善特征响应，推进了大气过程建模。

    arXiv:2403.10555v1 Announce Type: cross  Abstract: Deep learning-based, data-driven models are gaining prevalence in climate research, particularly for global weather prediction. However, training the global weather data at high resolution requires massive computational resources. Therefore, we present a new model named KARINA to overcome the substantial computational demands typical of this field. This model achieves forecasting accuracy comparable to higher-resolution counterparts with significantly less computational resources, requiring only 4 NVIDIA A100 GPUs and less than 12 hours of training. KARINA combines ConvNext, SENet, and Geocyclic Padding to enhance weather forecasting at a 2.5{\deg} resolution, which could filter out high-frequency noise. Geocyclic Padding preserves pixels at the lateral boundary of the input image, thereby maintaining atmospheric flow continuity in the spherical Earth. SENet dynamically improves feature response, advancing atmospheric process modeling,
    
[^250]: 通过强化学习学习给LLM生成的文本添加水印

    Learning to Watermark LLM-generated Text via Reinforcement Learning

    [https://arxiv.org/abs/2403.10553](https://arxiv.org/abs/2403.10553)

    通过将信号嵌入LLM的权重中，我们设计一种模型级水印，有效追踪生成文本的滥用情况，并提出基于强化学习的协同训练框架，使水印更准确、更稳健且更适应新攻击。

    

    我们研究如何给LLM生成的文本添加水印，即将可以通过算法检测到的信号嵌入LLM生成的文本中，以追踪滥用情况。与当前主流方法不同，我们通过将LLM调优阶段纳入水印流程，扩展了水印设计空间。先前的研究侧重于在输出中嵌入信号的标记级水印，我们设计了一种模型级水印，将信号嵌入LLM的权重中，可以被配对的检测器检测到。我们提出了一个基于强化学习的协同训练框架，迭代地（1）训练一个检测器来检测生成的带水印文本，并（2）调整LLM以生成检测器轻松检测到但保持正常效用的文本。我们实验证明，我们的水印更准确、更稳健且更适应（应对新攻击）。它还允许水印模型开源。此外，如果与al一起使用

    arXiv:2403.10553v1 Announce Type: cross  Abstract: We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with al
    
[^251]: 通过师生无数据知识传输在未知陌生地点上训练自定位模型

    Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer

    [https://arxiv.org/abs/2403.10552](https://arxiv.org/abs/2403.10552)

    通过师生无数据知识传输，实现在未知陌生地点上训练自定位模型，不仅可以处理各种类型的开放式老师，还有效避免依赖于师生私人数据可用性。

    

    当一个机器人在一个一般的开放世界中运行时，目前自定位模型的一个典型假设是目标工作空间中有一个带标注的训练数据集。然而，在一般开放世界中，这种情况并不总是成立。本研究引入了一种新颖的开放世界分布式机器人系统训练方案。在我们的方案中，一个机器人（"学生"）可以向它在陌生地点遇到的其他机器人（"老师"）寻求指导。具体来说，从老师模型重建一个伪训练数据集，然后用于学生模型的持续学习。与典型的知识传输方案不同，我们的方案在老师模型上只引入了最小的假设，使其可以处理各种类型的开放式老师，包括不合作的、无法训练的（如图像检索引擎）和黑匣子老师（即数据隐私）。与现有方法中依赖于老师的私人数据可用性不同

    arXiv:2403.10552v1 Announce Type: cross  Abstract: A typical assumption in state-of-the-art self-localization models is that an annotated training dataset is available in the target workspace. However, this does not always hold when a robot travels in a general open-world. This study introduces a novel training scheme for open-world distributed robot systems. In our scheme, a robot ("student") can ask the other robots it meets at unfamiliar places ("teachers") for guidance. Specifically, a pseudo-training dataset is reconstructed from the teacher model and thereafter used for continual learning of the student model. Unlike typical knowledge transfer schemes, our scheme introduces only minimal assumptions on the teacher model, such that it can handle various types of open-set teachers, including uncooperative, untrainable (e.g., image retrieval engines), and blackbox teachers (i.e., data privacy). Rather than relying on the availability of private data of teachers as in existing methods
    
[^252]: 通过双向归一化流进行半监督学习的异常流量检测

    Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows

    [https://arxiv.org/abs/2403.10550](https://arxiv.org/abs/2403.10550)

    通过生成伪异常样本和使用双向归一化流模块，实现对异常数据的半监督检测。

    

    随着互联网的快速发展，各种类型的异常流量威胁着网络安全。本文考虑了异常网络流量检测的问题，并提出了一个仅使用正常流量的三阶段异常检测框架。我们的框架可以生成伪异常样本，无需先前对异常进行了解，从而实现对异常数据的检测。首先，我们采用重构方法来学习正常样本的深层表示。其次，利用双向流模块将这些表示归一化为标准正态分布。为了模拟异常样本，我们向归一化后的表示添加噪声，然后将其通过双向流模块的生成方向。最后，在潜在空间中训练一个简单的分类器来区分正常样本和伪异常样本。在推断过程中，我们的框架仅需要两个模块来进行检测。

    arXiv:2403.10550v1 Announce Type: cross  Abstract: With the rapid development of the Internet, various types of anomaly traffic are threatening network security. We consider the problem of anomaly network traffic detection and propose a three-stage anomaly detection framework using only normal traffic. Our framework can generate pseudo anomaly samples without prior knowledge of anomalies to achieve the detection of anomaly data. Firstly, we employ a reconstruction method to learn the deep representation of normal samples. Secondly, these representations are normalized to a standard normal distribution using a bidirectional flow module. To simulate anomaly samples, we add noises to the normalized representations which are then passed through the generation direction of the bidirectional flow module. Finally, a simple classifier is trained to differentiate the normal samples and pseudo anomaly samples in the latent space. During inference, our framework requires only two modules to detec
    
[^253]: 在低功耗极端边缘嵌入式系统上进行关键词检测的设备端域学习

    On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems

    [https://arxiv.org/abs/2403.10549](https://arxiv.org/abs/2403.10549)

    在低功耗极端边缘嵌入式系统上，提出了一个完全设备端域自适应系统，能够实现高达14%的准确性提升，仅需少于10 kB的内存和100个标记的话语，在适应复杂的语音噪声后能够恢复准确性。

    

    当神经网络暴露在嘈杂环境中时，关键词检测的准确性会下降。现场对之前未见噪声的自适应对于恢复准确性损失至关重要，并且需要设备端学习来确保自适应过程完全在边缘设备上进行。在这项工作中，我们提出了一个完全设备端域自适应系统，实现了高达14%的准确性增益，超过了已经稳健的关键词检测模型。我们使用不到10 kB的内存实现了设备端学习，仅使用100个标记的话语就能在适应复杂的语音噪声后提高5%的准确性。我们证明域自适应可以在超低功耗的微控制器上实现，仅需806 mJ，且在始终打开的、电池供电的设备上仅需14秒。

    arXiv:2403.10549v1 Announce Type: cross  Abstract: Keyword spotting accuracy degrades when neural networks are exposed to noisy environments. On-site adaptation to previously unseen noise is crucial to recovering accuracy loss, and on-device learning is required to ensure that the adaptation process happens entirely on the edge device. In this work, we propose a fully on-device domain adaptation system achieving up to 14% accuracy gains over already-robust keyword spotting models. We enable on-device learning with less than 10 kB of memory, using only 100 labeled utterances to recover 5% accuracy after adapting to the complex speech noise. We demonstrate that domain adaptation can be achieved on ultra-low-power microcontrollers with as little as 806 mJ in only 14 s on always-on, battery-operated devices.
    
[^254]: 坚韧的二阶非凸优化及其在低秩矩阵感知中的应用

    Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing

    [https://arxiv.org/abs/2403.10547](https://arxiv.org/abs/2403.10547)

    研究了在强污染模型中寻找SOSP的问题，提出了一般框架以\emph{独立于维度}的精度保证高效地找到近似SOSP，具有对抗异常值的鲁棒性，同时将该框架应用于低秩矩阵感知问题，发展了能够容忍数据破坏的高效且可证明鲁棒性的算法。

    

    寻找近似的二阶稳定点（SOSP）是随机非凸优化中一个经过深入研究的基本问题，具有许多在机器学习中的应用。然而，在存在异常值的情况下，这个问题理解不足，限制了现有非凸算法在对抗性环境中的使用。本文研究在强污染模型中寻找SOSPs的问题，解决了一定部分的数据点被任意破坏的情况下有效地寻找近似SOSP的一般框架，具有\emph{独立于维度}的精度保证，使用$\widetilde{O}({D^2}/{\epsilon})$个样本，其中$D$是环境维度，$\epsilon$是被破坏数据点的比例。

    arXiv:2403.10547v1 Announce Type: cross  Abstract: Finding an approximate second-order stationary point (SOSP) is a well-studied and fundamental problem in stochastic nonconvex optimization with many applications in machine learning. However, this problem is poorly understood in the presence of outliers, limiting the use of existing nonconvex algorithms in adversarial settings.   In this paper, we study the problem of finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted. We introduce a general framework for efficiently finding an approximate SOSP with \emph{dimension-independent} accuracy guarantees, using $\widetilde{O}({D^2}/{\epsilon})$ samples where $D$ is the ambient dimension and $\epsilon$ is the fraction of corrupted datapoints.   As a concrete application of our framework, we apply it to the problem of low rank matrix sensing, developing efficient and provably robust algorithms that can tolerate corruptions in both 
    
[^255]: 通过GNN的反向过程区分异质图中的邻居表示

    Distinguishing Neighborhood Representations Through Reverse Process of GNNs for Heterophilic Graphs

    [https://arxiv.org/abs/2403.10543](https://arxiv.org/abs/2403.10543)

    通过GNN的反向传播过程，可以显著改善异质图中节点表示的区分度，并在许多情况下提高分类性能。

    

    Graph Neural Network（GNN）类似于扩散过程，在堆叠许多层时导致学习表示过度平滑。因此，消息传递的反向过程可以通过反转正向消息传播来锐化节点表示。锐化后的表示可以帮助我们更好地区分具有不同标签的邻居节点，例如在异质图中。在这项工作中，我们将反向过程的设计原则应用于GNN的三个变体。通过在异质图数据上的实验，其中相邻节点需要具有不同表示才能成功分类，我们展示了反向过程在许多情况下显着提高了预测性能。进一步分析表明，反向机制可以减轻数百层上的过度平滑。

    arXiv:2403.10543v1 Announce Type: cross  Abstract: Graph Neural Network (GNN) resembles the diffusion process, leading to the over-smoothing of learned representations when stacking many layers. Hence, the reverse process of message passing can sharpen the node representations by inverting the forward message propagation. The sharpened representations can help us to better distinguish neighboring nodes with different labels, such as in heterophilic graphs. In this work, we apply the design principle of the reverse process to the three variants of the GNNs. Through the experiments on heterophilic graph data, where adjacent nodes need to have different representations for successful classification, we show that the reverse process significantly improves the prediction performance in many cases. Additional analysis reveals that the reverse mechanism can mitigate the over-smoothing over hundreds of layers.
    
[^256]: MATADOR：用于边缘应用的自动化片上Tsetlin机设计生成系统

    MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications

    [https://arxiv.org/abs/2403.10538](https://arxiv.org/abs/2403.10538)

    该论文介绍了MATADOR，一个用于边缘应用的自动化片上Tsetlin机设计生成系统，实现了将ML模型转换为SoC-FPGA解决方案的高效方法。

    

    System-on-Chip Field-Programmable Gate Array（SoC-FPGA）通过设计协处理器加速器系统为机器学习（ML）边缘推断应用提供了显著的吞吐量增益。然而，将ML模型训练并翻译为SoC-FPGA解决方案的设计工作可能是相当大的，需要专业知识来平衡模型性能、功耗、延迟和资源利用率之间的 trade-offs。与其他ML算法相反，Tsetlin机器（TM）通过从Tsetlin自动机（学习元素）和布尔输入特征之间形成逻辑命题来执行分类。经过训练的TM模型通常具有很高的稀疏性，并且这些逻辑命题在类别内部和类别之间都有相当大的重叠。因此，该模型可以使用极少量的AND和NOT门转换为RTL级的设计。本文介绍了MATADOR，一个自动化布尔到si

    arXiv:2403.10538v1 Announce Type: cross  Abstract: System-on-Chip Field-Programmable Gate Arrays (SoC-FPGAs) offer significant throughput gains for machine learning (ML) edge inference applications via the design of co-processor accelerator systems. However, the design effort for training and translating ML models into SoC-FPGA solutions can be substantial and requires specialist knowledge aware trade-offs between model performance, power consumption, latency and resource utilization. Contrary to other ML algorithms, Tsetlin Machine (TM) performs classification by forming logic proposition between boolean actions from the Tsetlin Automata (the learning elements) and boolean input features. A trained TM model, usually, exhibits high sparsity and considerable overlapping of these logic propositions both within and among the classes. The model, thus, can be translated to RTL-level design using a miniscule number of AND and NOT gates. This paper presents MATADOR, an automated boolean-to-si
    
[^257]: 计算机视觉中合成数据增强方法的调查

    A survey of synthetic data augmentation methods in computer vision

    [https://arxiv.org/abs/2403.10075](https://arxiv.org/abs/2403.10075)

    该论文调查了计算机视觉中合成数据增强方法，涵盖了基于逼真3D图形建模、神经风格转移、差分神经渲染和生成的数据合成方法。

    

    处理计算机视觉问题的标准方法是使用代表目标任务的大规模图像数据集训练深度卷积神经网络（CNN）模型。然而，在许多情况下，很难获取足够的目标任务图像数据。数据增强是缓解这一挑战的一种方法。一种常见做法是明确地以所需的方式转换现有图像，以创建实现良好泛化性能所需的训练数据的数量和变化性。在无法访问目标领域数据的情况下，一个可行的解决方法是从零开始合成训练数据--即合成数据增强。本文介绍了合成数据增强技术的广泛评估。它涵盖了基于逼真3D图形建模、神经风格转移（NST）、差分神经渲染和生成的数据合成方法。

    arXiv:2403.10075v1 Announce Type: cross  Abstract: The standard approach to tackling computer vision problems is to train deep convolutional neural network (CNN) models using large-scale image datasets which are representative of the target task. However, in many scenarios, it is often challenging to obtain sufficient image data for the target task. Data augmentation is a way to mitigate this challenge. A common practice is to explicitly transform existing images in desired ways so as to create the required volume and variability of training data necessary to achieve good generalization performance. In situations where data for the target domain is not accessible, a viable workaround is to synthesize training data from scratch--i.e., synthetic data augmentation. This paper presents an extensive review of synthetic data augmentation techniques. It covers data synthesis approaches based on realistic 3D graphics modeling, neural style transfer (NST), differential neural rendering, and gen
    
[^258]: 一种对有限覆盖的混合RL在线算法的自然扩展

    A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage

    [https://arxiv.org/abs/2403.09701](https://arxiv.org/abs/2403.09701)

    混合强化学习算法中，通过将离线数据集包含在在线算法的经验重放缓冲区中进行启动，可以实现类似于基于离线数据分布引导在线探索的可证明收益，即使离线数据集没有单一策略可集中性。

    

    混合强化学习（RL）结合在线和离线数据，近年来引起了广泛关注，但关于其可证明益处的研究仍然很少。许多现有的混合RL算法对离线数据集施加覆盖假设，但我们表明这是不必要的。一个设计良好的在线算法应该在离线数据集中“填补空白”，探索行为策略未探索的状态和动作。与先前侧重于估计离线数据分布以引导在线探索的方法不同，我们表明对标准乐观在线算法的一个自然扩展——通过将离线数据集包含在经验重放缓冲区中来启动它们——即使离线数据集没有单一策略可集中性，也可实现混合数据的类似可证明收益。我们完成

    arXiv:2403.09701v1 Announce Type: new  Abstract: Hybrid Reinforcement Learning (RL), leveraging both online and offline data, has garnered recent interest, yet research on its provable benefits remains sparse. Additionally, many existing hybrid RL algorithms (Song et al., 2023; Nakamoto et al., 2023; Amortila et al., 2024) impose coverage assumptions on the offline dataset, but we show that this is unnecessary. A well-designed online algorithm should "fill in the gaps" in the offline dataset, exploring states and actions that the behavior policy did not explore. Unlike previous approaches that focus on estimating the offline data distribution to guide online exploration (Li et al., 2023b), we show that a natural extension to standard optimistic online algorithms -- warm-starting them by including the offline dataset in the experience replay buffer -- achieves similar provable gains from hybrid data even when the offline dataset does not have single-policy concentrability. We accomplish
    
[^259]: Quiet-STaR: 语言模型可以自己学会思考后再说话

    Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking

    [https://arxiv.org/abs/2403.09629](https://arxiv.org/abs/2403.09629)

    Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力

    

    写作和交谈时，人们有时会停下来思考。尽管以推理为重点的作品通常将推理框定为回答问题或完成代理任务的方法，但推理几乎都隐含在所有书面文本中。例如，这适用于证明中未明确说明的步骤，以及支撑对话的心智理论。在自学习推理者（STaR，Zelikman等，2022）中，通过从少量示例中推断来自问答中有用的思考，并学习那些导致正确答案的思考。这是一个高度受限制的环境--理想情况下, 一个语言模型可以学会从任意文本中推断未明确说明的思考。我们提出Quiet-STaR，这是STaR的一个泛化版本，其中语言模型学会在每个标记处生成解释未来文本的思考过程，从而改善其预测。我们解决了一些关键挑战，包括1）生成连续的计算成本

    arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
    
[^260]: 控制硬件非确定性进行乐观可验证训练

    Optimistic Verifiable Training by Controlling Hardware Nondeterminism

    [https://arxiv.org/abs/2403.09603](https://arxiv.org/abs/2403.09603)

    提出了一种方法，结合了在比目标模型更高精度下进行训练、在中间计算步骤后进行四舍五入，并基于自适应阈值存储四舍五入决策，以应对硬件非确定性对训练过程的影响。

    

    AI系统日益增加的计算需求导致了为缺乏必要资源的客户进行模型训练的服务的出现。然而，确保训练的正确性并防范潜在的训练时攻击，例如数据毒化，都带来了挑战。现有的关于可验证训练的工作主要分为两类：基于证明的系统，由于需要加密技术而难以扩展，以及考虑到一个可信第三方审计员复制训练过程的“乐观”方法。 后者的一个关键挑战是，在训练期间GPU类型之间的硬件非确定性阻止审计员精确复制训练过程，因此这样的方案不够健壮。我们提出了一种方法，将训练在比目标模型更高的精度下进行，中间计算步骤后四舍五入，基于自适应阈值存储四舍五入决策。

    arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
    
[^261]: 学习自适应邻居以实时检测内部威胁

    LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection

    [https://arxiv.org/abs/2403.09209](https://arxiv.org/abs/2403.09209)

    该论文的贡献是提出了一个名为LAN的框架，能够实时在活动级别进行内部威胁检测，并学习活动序列内的时间依赖关系和活动之间的关系。

    

    企业和组织面临来自内部员工可能导致严重后果的潜在威胁。先前关于内部威胁检测（ITD）的研究主要集中在检测异常用户或异常时间段（例如，一周或一天）。然而，用户可能在日志中有数十万条活动，即使在一天内，一个用户也可能存在数千条活动，这需要高昂的调查预算来验证异常用户或活动。另一方面，现有作品主要是事后方法而不是实时检测，无法及时报告内部威胁在引起损失之前。在本文中，我们进行了针对活动级别实时ITD的第一项研究，并提出了一个细粒度和高效的框架LAN。具体而言，LAN同时学习活动序列内的时间依赖关系和活动之间的关系。

    arXiv:2403.09209v1 Announce Type: cross  Abstract: Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between ac
    
[^262]: AutoLoRA：基于元学习的自动调整矩阵秩在低秩适应中的应用

    AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning

    [https://arxiv.org/abs/2403.09113](https://arxiv.org/abs/2403.09113)

    AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。

    

    大规模预训练之后进行任务特定微调在各种自然语言处理任务中取得了巨大成功。然而，对于大型预训练模型的所有参数进行微调存在着巨大的计算和内存挑战，因此研发了几种高效的微调方法。其中，低秩适应（LoRA）通过在冻结的预训练权重之上微调低秩增量更新矩阵，被证明特别有效。然而，LoRA在所有层中均匀分配秩，并且依赖于穷举搜索来找到最佳秩，导致了高计算成本和微调性能不佳。为了解决这些限制，我们引入了AutoLoRA，这是一个基于元学习的框架，用于自动识别每个LoRA层的最佳秩。AutoLoRA将低秩更新矩阵中的每个秩为1的矩阵与选择变量相关联，该变量决定了秩为1的矩阵是否应该被...

    arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
    
[^263]: 快速推断基于移除的节点影响

    Fast Inference of Removal-Based Node Influence

    [https://arxiv.org/abs/2403.08333](https://arxiv.org/abs/2403.08333)

    提出了一种评估节点影响的新方法，通过测量训练好的图神经网络模型在移除节点后的预测变化，以实现快速推断。

    

    图神经网络（GNNs）被广泛用于捕获图中信息传播模式。虽然取得了显著的性能，但评估节点影响的新趋势日益受到关注。我们提出了一种评估节点影响的新方法，通过衡量训练好的GNN模型在移除节点后的预测变化。一个真实应用是，“在预测Twitter账户极性的任务中，如果移除特定账户，其他账户的极性会如何改变？”我们将GNN作为一个代理模型，其预测可以模拟移除节点引起的节点或边的变化。为了获得每个节点的影响，一种直接的方法是交替移除每个节点，并在修改后的图上应用训练好的GNN。这是可靠的但耗时，因此我们需要一种高效的方法。

    arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
    
[^264]: 深度子模逆点网络

    Deep Submodular Peripteral Network

    [https://arxiv.org/abs/2403.08199](https://arxiv.org/abs/2403.08199)

    引入了深度子模逆点网络（DSPNs），并提出了一种使用对比学习启发的GPC-ready策略进行训练的方法，以应对子模函数学习中的两大挑战。

    

    子模函数对各种应用至关重要，但通常缺乏实用的学习方法来获取它们。本文引入了深度子模逆点网络（DSPNs），一种新颖的子模函数参数化族，并提出了使用对比学习启发的GPC-ready策略对其进行训练的方法，以连接并解决上述两个挑战。

    arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style "peripteral" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate
    
[^265]: EM-TTS：高效训练的低资源蒙古语轻量级文本转语音系统

    EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech

    [https://arxiv.org/abs/2403.08164](https://arxiv.org/abs/2403.08164)

    提出了一种基于深度卷积神经网络的轻量级TTS系统，采用两阶段训练而非递归单元，可以显著减少训练时间和经济成本

    

    最近，基于深度学习的文本转语音（TTS）系统取得了高质量的语音合成结果。递归神经网络已成为TTS系统中序列数据的标准建模技术，并被广泛使用。然而，训练包含RNN组件的TTS模型需要强大的GPU性能并且时间长。相反，基于CNN的序列合成技术可以显著减少TTS模型的参数和训练时间，同时由于其高并行性，可以保证一定的性能，从而减轻这些训练的经济成本。本文提出了一种基于深度卷积神经网络的轻量级TTS系统，这是一个两阶段训练的端到端TTS模型，不使用任何递归单元。我们的模型包括两个阶段：Text2Spectrum 和 SSRN。前者用于将音素编码为粗糙的梅尔频谱图，后者用于合成。

    arXiv:2403.08164v1 Announce Type: cross  Abstract: Recently, deep learning-based Text-to-Speech (TTS) systems have achieved high-quality speech synthesis results. Recurrent neural networks have become a standard modeling technique for sequential data in TTS systems and are widely used. However, training a TTS model which includes RNN components requires powerful GPU performance and takes a long time. In contrast, CNN-based sequence synthesis techniques can significantly reduce the parameters and training time of a TTS model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training. In this paper, we propose a lightweight TTS system based on deep convolutional neural networks, which is a two-stage training end-to-end TTS model and does not employ any recurrent units. Our model consists of two stages: Text2Spectrum and SSRN. The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesi
    
[^266]: 使用高斯局部线性映射进行快速、准确和轻量级的顺序仿真推断

    Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings

    [https://arxiv.org/abs/2403.07454](https://arxiv.org/abs/2403.07454)

    使用结构混合概率分布提供了准确的后验推断，同时具有更小的计算占用量，相较于现有的基于神经网络的SBI方法。

    

    arXiv:2403.07454v1 公告类型: 跨领域 摘要: 针对具有难以处理的似然函数的复杂模型的贝叶斯推断可以使用多次调用计算模拟器的算法来解决。 这些方法被统称为“基于仿真的推断”（SBI）。 最近的SBI方法利用神经网络（NN）提供近似但表达丰富的构造，用于不可用的似然函数和后验分布。 然而，它们通常无法实现准确性和计算需求之间的最佳折衷。 在这项工作中，我们提出了一种提供似然函数和后验分布近似的替代方法，使用结构化的概率分布混合物。 相对于最先进的基于NN的SBI方法，我们的方法在产生准确的后验推断的同时，具有更小的计算占用量。 我们在SBI文献中的几个基准模型上展示了我们的结果。

    arXiv:2403.07454v1 Announce Type: cross  Abstract: Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as "simulation-based inference" (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several benchmark models from the SBI literature.
    
[^267]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^268]: AuG-KD: 基于锚点的混合生成用于领域之外知识蒸馏

    AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation

    [https://arxiv.org/abs/2403.07030](https://arxiv.org/abs/2403.07030)

    提出了一种名为AuG-KD的方法，通过利用基于锚点的混合生成，解决了无数据知识蒸馏中的知识转移挑战。

    

    由于隐私或专利问题，越来越多的大型模型发布时不提供其训练数据的访问权限，这使得将它们的知识转移变得低效且问题复杂。针对这一问题，出现了无数据知识蒸馏（DFKD）方法作为直接解决方案。然而，简单地采用从DFKD派生的模型用于实际应用会导致显著的性能下降，这是因为教师训练数据与实际场景（学生领域）之间存在巨大差异。这种性能下降源于教师知识中不适用于学生领域的部分，这些知识是特定于教师领域的，会削弱学生的性能。因此，在DFKD中，有选择地转移适用于学生领域的教师知识成为主要挑战。在本研究中，我们提出了一种简单而有效的方法AuG-KD。它利用一种基于不确定性和样本特定的锚点来对齐学生领域。

    arXiv:2403.07030v1 Announce Type: new  Abstract: Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers' training data and real-world scenarios (student domain). The degradation stems from the portions of teachers' knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students' performance. Hence, selectively transferring teachers' appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-doma
    
[^269]: 揭示受幼儿启发的奖励转换在目标导向强化学习中的重要性

    Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning

    [https://arxiv.org/abs/2403.06880](https://arxiv.org/abs/2403.06880)

    研究探讨了幼儿启发的奖励转换如何影响强化学习任务的样本效率和成功率，特别是发现了幼儿启发的稀疏转密集（S2D）转换的有效性。

    

    幼儿从稀疏反馈的自由探索逐渐发展为利用先前经验进行以目标为导向的学习，获得更密集奖励。受此幼儿启发的奖励转换的影响，我们探讨了将不同奖励转换纳入强化学习（RL）任务的意义。我们研究的重点是从稀疏到基于潜在的密集奖励的转换，这两者共享无论奖励变化均为最佳策略。通过包括以自我为中心的导航和机械臂操作任务在内的各种实验，我们发现适当的奖励转换显著影响样本效率和成功率。特别值得注意的是受幼儿启发的稀疏转密集（S2D）转换的有效性。除了这些性能指标外，使用交叉密度可视化技术，我们观察到转换，特别是S2D，使策略损失景观更加平滑，促进

    arXiv:2403.06880v1 Announce Type: cross  Abstract: Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting
    
[^270]: ALaRM: 通过分层奖励建模对齐语言模型

    ALaRM: Align Language Models via Hierarchical Rewards Modeling

    [https://arxiv.org/abs/2403.06754](https://arxiv.org/abs/2403.06754)

    ALaRM是第一个从人类反馈中建模分层奖励的框架，通过整合整体奖励与特定方面的奖励，改善了大型语言模型与人类偏好的对齐性，尤其在复杂文本生成任务中表现出更精确和一致的指导。

    

    我们介绍了ALaRM，第一个在强化学习中从人类反馈模型分层奖励的框架，旨在增强大型语言模型（LLMs）与人类偏好的对齐性。该框架解决了当前对齐方法的限制，这些方法通常难以处理人类监督信号的不一致性和稀疏性，通过将整体奖励与特定方面的奖励相结合。这种整合使得语言模型更加精确和一致地指导朝着期望的结果前进，尤其在复杂和开放的文本生成任务中。通过应用基于一致性的方法来过滤和组合多个奖励，该框架提供了一种可靠的机制来改善模型的对齐性。我们通过在长篇问题回答和机器翻译任务中使用gpt-3.5-turbo进行成对比较来验证我们的方法。

    arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
    
[^271]: 基于Ricci流的大脑表面协方差描述符用于阿尔茨海默病

    Ricci flow-based brain surface covariance descriptors for Alzheimer disease

    [https://arxiv.org/abs/2403.06645](https://arxiv.org/abs/2403.06645)

    本文首次提出了一种基于Ricci流的大脑表面协方差描述符的流水线，可以用于诊断阿尔茨海默病。

    

    从MRI大脑扫描中自动提取特征并诊断阿尔茨海默病是一个持续挑战。随着3D成像技术的进步，3D数据采集比其2D对应物更具可行性和效率。本文首次提出了一种流水线，从皮层表面利用Ricci能量优化提取新颖的基于协方差的描述符，而非使用基于特征的向量。这些协方差描述符是对称正定矩阵的非线性流形的组成部分，因此我们专注于使用高斯径向基函数将基于流形的分类应用于3D形状问题。将这一新颖特征应用于异常皮层脑形态学分析中，可以用于诊断阿尔茨海默病。在大约两百个3D MRI大脑模型上进行的实验研究，这些模型来自阿尔茨海默病神经影像学倡议 (ADNI) 数据集

    arXiv:2403.06645v1 Announce Type: cross  Abstract: Automated feature extraction from MRI brain scans and diagnosis of Alzheimer's disease are ongoing challenges. With advances in 3D imaging technology, 3D data acquisition is becoming more viable and efficient than its 2D counterpart. Rather than using feature-based vectors, in this paper, for the first time, we suggest a pipeline to extract novel covariance-based descriptors from the cortical surface using the Ricci energy optimization. The covariance descriptors are components of the nonlinear manifold of symmetric positive-definite matrices, thus we focus on using the Gaussian radial basis function to apply manifold-based classification to the 3D shape problem. Applying this novel signature to the analysis of abnormal cortical brain morphometry allows for diagnosing Alzheimer's disease. Experimental studies performed on about two hundred 3D MRI brain models, gathered from Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset dem
    
[^272]: TrafficGPT：突破令牌限制，实现高效长时间流量分析和生成

    TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic Analysis and Generation

    [https://arxiv.org/abs/2403.05822](https://arxiv.org/abs/2403.05822)

    TrafficGPT 是一个深度学习模型，旨在突破令牌长度限制，实现高效的长时间流量分析和生成，解决了网络流量分析和生成中依赖标记数据和生成符合实际模式的流量样本的难题。

    

    多年来，网络流量分析和生成取得了显著进步。从传统的统计方法，该领域发展到复杂的深度学习技术。这种进步提高了检测复杂模式和安全威胁的能力，以及测试和优化网络性能的能力。然而，仍然存在障碍，例如对标记数据进行分析的依赖以及生成遵循实际模式的流量样本的困难。预训练的深度神经网络已经成为解决这些问题的强大工具，通过从大型无标签数据集中学习健壮的数据表示来提供改进的性能。尽管存在益处，但现有的预训练模型面临令牌长度限制等挑战，这限制了它们在全面流量分析和实际流量生成中的有用性。为了应对这些挑战，我们介绍了TrafficGPT，一个深度学习

    arXiv:2403.05822v1 Announce Type: new  Abstract: Over the years, network traffic analysis and generation have advanced significantly. From traditional statistical methods, the field has progressed to sophisticated deep learning techniques. This progress has improved the ability to detect complex patterns and security threats, as well as to test and optimize network performance. However, obstacles persist, such as the dependence on labeled data for analysis and the difficulty of generating traffic samples that follow realistic patterns. Pre-trained deep neural networks have emerged as powerful tools to resolve these issues, offering improved performance by learning robust data representations from large unlabeled datasets. Despite their benefits, existing pre-trained models face challenges like token length limitation, which restricts their usefulness in comprehensive traffic analysis and realistic traffic generation. To address these challenges, we introduce TrafficGPT, a deep learning
    
[^273]: MG-TSD：具有引导学习过程的多粒度时间序列扩散模型

    MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process

    [https://arxiv.org/abs/2403.05751](https://arxiv.org/abs/2403.05751)

    提出了一种新颖的MG-TSD模型，利用数据内在粒度水平作为目标来引导学习过程，实现了状态-of-the-art的预测性能

    

    最近，扩散概率模型由于其生成高保真样本的显著能力而在生成式时间序列预测中引起关注。然而，由于其随机特性带来的不稳定性挑战，如何有效利用它们在概率时间序列预测任务中的强大建模能力仍然是一个悬而未决的问题。为了解决这一挑战，我们引入了一种新颖的多粒度时间序列扩散（MG-TSD）模型，通过利用数据内在的粒度水平作为中间扩散步骤的给定目标来指导扩散模型的学习过程，实现了最先进的预测性能。

    arXiv:2403.05751v1 Announce Type: cross  Abstract: Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the p
    
[^274]: 最近大型视觉-语言模型的有效性评估

    Effectiveness Assessment of Recent Large Vision-Language Models

    [https://arxiv.org/abs/2403.04306](https://arxiv.org/abs/2403.04306)

    本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。

    

    大型视觉-语言模型(LVLMs)的出现代表着迈向人工通用智能的重要进步。然而，它们在专业和通用任务中的有效性程度需要进一步调查。本文旨在评估流行的LVLMs在专业和通用任务中的能力，旨在提供对这些创新方法的全面理解。为了评估它们在专业任务中的有效性，我们量身定制了一个包含自然、医疗和工业三种不同场景的全面测试平台，涵盖六项具有挑战性的任务。这些任务包括显著、伪装和透明物体检测，以及息肉和皮肤病变检测，以及工业异常检测。我们检验了最近三种开源LVLMs--MiniGPT-v2、LLaVA-1.5和Shikra--在视觉识别和定位领域的表现。

    arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
    
[^275]: Cobweb：一种增量和分层式的人类类别学习模型

    Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning

    [https://arxiv.org/abs/2403.03835](https://arxiv.org/abs/2403.03835)

    Cobweb是一种类似人类类别学习系统，采用类别效用度量构建分层组织的类似树状结构，能够捕捉心理效应并在单一模型中展现出实例和原型学习的灵活性，为将来研究人类类别学习提供了基础。

    

    Cobweb是一种类似人类的类别学习系统，与其他增量分类模型不同的是，它利用类别效用度量构建分层组织的类似树状结构。先前的研究表明，Cobweb能够捕捉心理效应，如基本水平、典型性和扇形效应。然而，对Cobweb作为人类分类模型的更广泛评估仍然缺乏。本研究填补了这一空白。它确定了Cobweb与经典的人类类别学习效应的一致性。还探讨了Cobweb展现出在单一模型中既有实例又有原型学习的灵活性。这些发现为将来研究Cobweb作为人类类别学习的综合模型奠定了基础。

    arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
    
[^276]: DeepCRE：利用尖端计算模型改革药物研发

    DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models

    [https://arxiv.org/abs/2403.03768](https://arxiv.org/abs/2403.03768)

    DeepCRE是一种新型的计算模型，在患者级别CRE性能上平均提高了17.7％，在指示级别CRE增加了5倍，并成功确定了六个具有显着优势的药物候选者。

    

    arXiv:2403.03768v1 公告类型：新摘要：药物开发领域和治疗应用领域都面临着重大挑战。治疗领域需要更多的治疗选择，同时大量有前景的临床前药物在临床试验中失败。一个原因是在药物开发的后期阶段交叉药物反应评估（CRE）的不足。尽管计算机模拟的CRE模型为解决这一问题提供了一种解决方案，但现有方法学要么局限于早期开发阶段，要么缺乏对全面CRE分析的能力。在这里，我们介绍了一种名为DeepCRE的新型计算模型，并展示了DeepCRE在推动治疗发现和发展方面的潜力。DeepCRE通过实现患者级别CRE平均性能提高17.7\%，指示级别CRE增加了5倍，优于现有最佳模型。此外，DeepCRE已经确定了六个显示出明显更大优势的药物候选者。

    arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
    
[^277]: 具有Polyak动量的非凸随机复合优化

    Non-Convex Stochastic Composite Optimization with Polyak Momentum

    [https://arxiv.org/abs/2403.02967](https://arxiv.org/abs/2403.02967)

    本文研究了具有Polyak动量的随机近端梯度方法，在非凸复合优化问题中实现了最佳收敛速度，无论批量大小如何。

    

    随机近端梯度法是广泛使用的随机梯度下降（SGD）方法的一个强大泛化，在机器学习中已经被广泛应用。然而，众所周知，当随机噪声显著时（即仅使用小型或有界批量大小时），该方法在非凸环境中无法收敛。本文关注具有Polyak动量的随机近端梯度方法。我们证明了该方法对于非凸复合优化问题实现了最佳收敛速度，而批量大小大小无关。此外，我们对Polyak动量在复合优化环境中的方差减少效应进行了严格分析，并且我们证明了当近端步骤只能通过近似解来求解时，该方法也会收敛。最后，我们提供了数值实验来验证我们的理论结果。

    arXiv:2403.02967v1 Announce Type: cross  Abstract: The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.
    
[^278]: Android应用隐私相关评论的十年大规模趋势分析

    A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends

    [https://arxiv.org/abs/2403.02292](https://arxiv.org/abs/2403.02292)

    通过分析谷歌应用商店上1200万条隐私相关评论，研究了十年间隐私评论的大规模趋势，发现隐私评论呈现持续增长，探讨了热门和逐渐减少的隐私话题，以及不同国家用户对隐私问题看法的差异。

    

    我们展示了对谷歌应用商店上1200万条隐私相关评论的分析结果，这些评论跨越了10年时间。通过应用最先进的自然语言处理技术，我们能够在时间、国家、应用类型、不同隐私主题以及多种情感维度上检视用户对隐私问题的看法。我们发现隐私相关评论持续增长，并探究了一些热门话题（如数据删除和数据窃取），以及一些逐渐减少的话题（如涉及敏感权限的隐私相关评论）。尽管隐私评论来自200多个国家，但有33个国家提供了90%的隐私评论。我们通过检查每个国家用户评论的隐私主题分布来进行跨国家比较，发现地理接近并不意味着附近国家有类似的隐私观点。

    arXiv:2403.02292v1 Announce Type: new  Abstract: We present an analysis of 12 million instances of privacy-relevant reviews publicly visible on the Google Play Store that span a 10 year period. By leveraging state of the art NLP techniques, we can examine what users have been writing about privacy along multiple dimensions: time, countries, app types, diverse privacy topics, and even across a spectrum of emotions. We find consistent growth of privacy-relevant reviews, and explore topics that are trending (such as Data Deletion and Data Theft), as well as those on the decline (such as privacy-relevant reviews on sensitive permissions). We find that although privacy reviews come from more than 200 countries, 33 countries provide 90% of privacy reviews. We conduct a comparison across countries by examining the distribution of privacy topics a country's users write about, and find that geographic proximity is not a reliable indicator that nearby countries have similar privacy perspectives.
    
[^279]: TPLLM: 基于预训练大语言模型的交通预测框架

    TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models

    [https://arxiv.org/abs/2403.02221](https://arxiv.org/abs/2403.02221)

    TPLLM提出了基于预训练大语言模型的交通预测框架，能够在历史交通数据有限的地区实现准确预测和良好泛化能力

    

    arXiv:2403.02221v1 公告类型: 新摘要: 交通预测构成智能交通系统（ITS）范围内的一个关键方面，高精度预测对于有效交通管理具有深远意义。当前基于深度学习的交通预测模型的精度通常随着训练数据量的增加而呈上升趋势。然而，获取全面的交通时空数据往往面临诸多挑战，主要源自于数据收集和保存的高昂成本。因此，在历史交通数据有限的地区实现准确预测和良好泛化能力的模型开发是一个具有挑战性的问题。近年来迅速发展的预训练大语言模型（LLMs）在跨模态知识传输和少样本学习方面表现出卓越的能力。

    arXiv:2403.02221v1 Announce Type: new  Abstract: Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learn
    
[^280]: SERVAL：垂直模型和LLM之间的协同学习，实现零-shot级别的医学预测

    SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction

    [https://arxiv.org/abs/2403.01570](https://arxiv.org/abs/2403.01570)

    提出SERVAL，一个协同学习流水线，可以通过相互增强，实现LLMs和小模型的垂直能力无监督开发，从而改善领域特定垂直问题的零-shot预测能力。

    

    近期大型语言模型（LLMs）的发展展示出对通用和常识问题卓越的零-shot能力。然而，LLMs在领域特定垂直问题上的应用仍然落后，主要是由于垂直知识方面的问题和不足。此外，垂直数据注释过程通常需要劳动密集型的专家参与，因此增加了增强模型垂直能力的额外挑战。在本文中，我们提出了SERVAL，一个协同学习流水线，旨在通过相互增强，对LLMs和小模型的垂直能力进行无监督开发。具体来说，SERVAL利用LLMs的零-shot输出作为注释，利用其置信度来从头开始教授一个强大的垂直模型。反过来，训练有素的垂直模型引导LLM微调，以增强其零-shot能力，逐步改进两者。

    arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
    
[^281]: Polynormer: 多项式表达的线性时间图转换器

    Polynormer: Polynomial-Expressive Graph Transformer in Linear Time

    [https://arxiv.org/abs/2403.01232](https://arxiv.org/abs/2403.01232)

    Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。

    

    图转换器（GTs）已经成为一种有前途的架构，理论上它比消息传递图神经网络（GNNs）更具表现力。然而，典型的GT模型至少具有二次复杂度，因此无法扩展到大型图。虽然最近提出了几种线性GTs，但它们在几个热门图数据集上仍落后于GNN对应模型，这对于它们的实际表现力构成了一个重要关注点。为了平衡GTs的表现力和可扩展性之间的权衡，我们提出了Polynormer，一个具有线性复杂度的多项式表达GT模型。Polynormer构建在一个新颖的基础模型上，该模型在输入特征上学习高次多项式。为了使基础模型具有置换等变性，我们将其与图拓扑和节点特征分开集成，从而产生本地和全局等变关注模型。因此，Polynormer采用了线性的局部到全局关注方案。

    arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
    
[^282]: ChatDiet：通过LLM增强框架赋能个性化营养导向食品推荐聊天机器人

    ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework

    [https://arxiv.org/abs/2403.00781](https://arxiv.org/abs/2403.00781)

    这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。

    

    食物对健康的深远影响使得先进的营养导向食品推荐服务成为必要。传统方法往往缺乏个性化、可解释性和互动性等关键元素。虽然大型语言模型（LLMs）带来了解释性和可解释性，但它们单独的使用未能实现真正的个性化。本文介绍了ChatDiet，一种新颖的LLM驱动框架，专门设计用于个性化营养导向食品推荐聊天机器人。ChatDiet集成了个人和人群模型，辅以一个协调器，无缝检索和处理相关信息。其结果是动态提供个性化和可解释的食品推荐，根据个人用户喜好定制。我们对ChatDiet进行了评估，包括一个引人入胜的案例研究，在案例研究中建立了一个因果个人模型来估计个人营养效果。

    arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
    
[^283]: 通过高阶注意力大脑网络分析大麻使用者的静息态fMRI数据

    Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network

    [https://arxiv.org/abs/2403.00033](https://arxiv.org/abs/2403.00033)

    通过结合动态内在功能网络和LSTM技术，使用高阶注意力模块进行信息融合和消息传递，提出了HOGAB模型，对慢性大麻用户的静息态fMRI数据进行分析，提高了多图分类的准确性。

    

    大麻的持续使用明显影响人们的生活和健康。在这项研究中，我们提出了一个可解释的新框架，命名为HOGAB（High-Order Attention Graph Attention神经网络）模型，以分析两个数据集中慢性大麻用户的局部异常脑活动。HOGAB将动态内在功能网络与LSTM技术相结合，捕捉大麻用户fMRI时间序列中的时间模式。此外，我们使用高阶注意力模块来对邻域节点进行信息融合和消息传递，增强长期大麻用户的社区聚类分析。此外，我们通过融入注意力机制提高了模型的整体学习能力，在多图分类中实现了85.1%的AUC和80.7%的准确性。此外，我们比较了线性机器学习方法，并评估了我们提出的HODAB模型的有效性。

    arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
    
[^284]: 基于学习的图搜索问题算法

    Learning-Based Algorithms for Graph Searching Problems

    [https://arxiv.org/abs/2402.17736](https://arxiv.org/abs/2402.17736)

    本研究提出了针对未知图的图搜索问题的基于学习的算法，首次在未知加权图上建立了形式保证，并设计算法在预测误差上具有最优或几乎最佳依存关系。

    

    我们考虑了Banerjee等人（2022年）最近提出的具有预测的图搜索问题。在这个问题中，一个从某个顶点$r$出发的代理者必须在最小化总行程的同时遍历一个（潜在未知的）图$G$以找到隐藏的目标节点$g$。我们研究了一种设置，在这种设置中，在任意节点$v$处，代理者会接收到到$g$的距离的噪声估计。我们设计了针对未知图的这种搜索任务的算法。我们在未知加权图上建立了第一次形式保证，并提供了显示我们提出的算法在预测误差上具有最优或几乎最佳依存关系的下界。此外，我们进行了数值实验，证明我们的算法除了对抗性误差具有鲁棒性外，还在误差是随机的典型实例中表现良好。最后，我们提供了对Banerjee等人算法的替代简化性能界限。

    arXiv:2402.17736v1 Announce Type: cross  Abstract: We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et 
    
[^285]: 用多路径传递课程增强连续领域适应

    Enhancing Continuous Domain Adaptation with Multi-Path Transfer Curriculum

    [https://arxiv.org/abs/2402.16681](https://arxiv.org/abs/2402.16681)

    本文提出了一种新颖的CDA方法W-MPOT，通过构建基于Wasserstein距离的传递课程，严格解决了领域排序和错误累积问题，实现了将源模型通过多条有效路径转移到目标域。

    

    机器学习中长期存在的一个挑战是解决训练和测试数据之间的巨大分布差异，这导致了迁移学习和领域自适应等领域的出现。最近，连续领域适应（CDA）作为一种有效技术出现，通过利用一系列中间域来弥合这一差距。本文提出了一种新颖的CDA方法W-MPOT，严格解决了先前研究忽视的领域排序和错误累积问题。具体而言，我们基于Wasserstein距离在源域和中间域上构建了一个传递课程，这受到了对CDA的理论分析的启发。然后，我们通过一个修改版的连续最优传输，在课程中的多条有效路径上将源模型转移到目标域。引入了一个双向路径一致性约束，以减轻连续过程中映射错误累积的影响。

    arXiv:2402.16681v2 Announce Type: replace  Abstract: Addressing the large distribution gap between training and testing data has long been a challenge in machine learning, giving rise to fields such as transfer learning and domain adaptation. Recently, Continuous Domain Adaptation (CDA) has emerged as an effective technique, closing this gap by utilizing a series of intermediate domains. This paper contributes a novel CDA method, W-MPOT, which rigorously addresses the domain ordering and error accumulation problems overlooked by previous studies. Specifically, we construct a transfer curriculum over the source and intermediate domains based on Wasserstein distance, motivated by theoretical analysis of CDA. Then we transfer the source model to the target domain through multiple valid paths in the curriculum using a modified version of continuous optimal transport. A bidirectional path consistency constraint is introduced to mitigate the impact of accumulated mapping errors during contin
    
[^286]: 受限制MDP中的真正无悔学习

    Truly No-Regret Learning in Constrained MDPs

    [https://arxiv.org/abs/2402.15776](https://arxiv.org/abs/2402.15776)

    本文首次肯定回答了一个开放问题，即是否可以在不允许错误抵消的情况下，通过将一种常见的安全约束模型扩展到具有多个约束的CMDPs，提出了一种可以实现次线性后悔的新方法。

    

    受约束的马尔可夫决策过程（CMDPs）是在强化学习中建模安全约束的常见方式。目前用于高效解决CMDPs的最先进方法基于原始-对偶算法。对于这些算法，所有当前已知的后悔界都允许错误抵消——可以通过在一个回合中的约束违反来用严格的约束满足在另一个回合中。这使得在线学习过程不安全，因为它仅保证最终（混合）策略的安全性，而在学习过程中不保证安全。正如Efroni等人（2020年）指出的，原始-对偶算法是否可以在不允许错误抵消的情况下可证明地实现次线性后悔是一个开放问题。在本文中，我们给出了第一个肯定的答案。我们首先将关于正则化原始-对偶方案的最后迭代收敛性通用化到具有多个约束的CMDPs上。基于这一见解，我们提出了一种基于模型的原始

    arXiv:2402.15776v1 Announce Type: new  Abstract: Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for error cancellations -- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal
    
[^287]: OpenSUN3D: 开放词汇的3D场景理解第一次研讨会挑战

    OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding

    [https://arxiv.org/abs/2402.15321](https://arxiv.org/abs/2402.15321)

    提供了OpenSUN3D研讨会上针对开放词汇3D场景理解的挑战概述，包括挑战数据集、评估方法和获胜方法的简要描述

    

    这份报告概述了在2023年ICCV会议上举办的OpenSUN3D Workshop关于开放词汇3D场景理解的挑战。该研讨会系列的目标是为开放词汇3D场景理解任务提供探索和讨论平台，包括但不限于分割、检测和映射。我们提供了研讨会上举办的挑战概述，展示了挑战数据集、评估方法以及获胜方法的简要描述。更多详情请参阅https://opensun3d.github.io/index_iccv23.html。

    arXiv:2402.15321v1 Announce Type: cross  Abstract: This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.
    
[^288]: 通过MIONet学习定义在不同域上的PDE的解算子

    Learning solution operators of PDEs defined on varying domains via MIONet

    [https://arxiv.org/abs/2402.15097](https://arxiv.org/abs/2402.15097)

    通过MIONet学习定义在不同域上的PDE的解算子，实现了解映射的学习，包括各种参数的变化，结果为进一步处理度量空间的逼近理论提供了洞见。

    

    在这项工作中，我们提出了一种方法，通过MIONet学习定义在不同域上的PDE的解算子，并在理论上证明了这种方法。我们首先将MIONet的逼近理论扩展到进一步处理度量空间，建立MIONet可以在度量空间中逼近具有多个输入的映射。随后，我们构建了一个包含一些适当区域的集合，并为这个集合提供了一个度量，从而使其成为一个度量空间，满足MIONet的逼近条件。基于理论基础，我们能够学习PDE的解映射，其中包括各种参数的变化，包括微分算子的参数，右手边项，边界条件以及域。举例来说，我们对2D泊松方程进行了实验，其中域和右手边项是变化的。结果提供了洞见。

    arXiv:2402.15097v1 Announce Type: new  Abstract: In this work, we propose a method to learn the solution operators of PDEs defined on varying domains via MIONet, and theoretically justify this method. We first extend the approximation theory of MIONet to further deal with metric spaces, establishing that MIONet can approximate mappings with multiple inputs in metric spaces. Subsequently, we construct a set consisting of some appropriate regions and provide a metric on this set thus make it a metric space, which satisfies the approximation condition of MIONet. Building upon the theoretical foundation, we are able to learn the solution mapping of a PDE with all the parameters varying, including the parameters of the differential operator, the right-hand side term, the boundary condition, as well as the domain. Without loss of generality, we for example perform the experiments for 2-d Poisson equations, where the domains and the right-hand side terms are varying. The results provide insig
    
[^289]: 停止推理！当多模态LLMs与串联推理遇到对抗性图像

    Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images

    [https://arxiv.org/abs/2402.14899](https://arxiv.org/abs/2402.14899)

    该研究评估了多模态LLMs在采用串联推理时的对抗鲁棒性，发现串联推理在一定程度上提高了对抗性鲁棒性，但引入了一种新的停止推理攻击技术成功规避了这种增强。

    

    最近，多模态LLMs（MLLMs）展示了很强的理解图像的能力。然而，像传统视觉模型一样，它们仍然容易受到对抗性图像的攻击。与此同时，串联推理（CoT）已经被广泛应用在MLLMs上，不仅提高了模型的性能，而且通过提供中间推理步骤来增强模型的可解释性。然而，目前还缺乏关于MLLMs在CoT下的对抗鲁棒性的研究，以及在MLLMs用对抗性图像推断错误答案时推理的合理性。我们的研究评估了采用CoT推理时MLLMs的对抗鲁棒性，发现CoT在一定程度上提高了对抗性鲁棒性，抵抗了已有的攻击方法。此外，我们引入了一种新的停止推理攻击技术，可以有效地规避CoT引起的鲁棒性增强。最后，我们展示了CoT推理的变化。

    arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
    
[^290]: 使用机器学习注意力模型进行时间偏差校正

    A Temporal Bias Correction using a Machine Learning Attention model

    [https://arxiv.org/abs/2402.14169](https://arxiv.org/abs/2402.14169)

    本论文提出了一种新颖的偏差校正方法，将校准视为概率模型而不是算法流程，利用机器学习概率注意力模型来适配偏差校正任务，可准确校正具有长期时间属性的气候统计数据，提高了在这些数据上进行可靠影响研究的准确性。

    

    气候模型在与真实世界观测数据相比存在偏差，通常需要在影响研究之前进行校准。使校准成为可能的统计方法集合被称为偏差校正（BC）。然而，当前的BC方法在调整时间偏差方面存在困难，因为它们忽略了连续时间点之间的依赖关系。因此，具有长期时间属性的气候统计数据（如热浪持续时间和频率）无法准确校正，这使得在这些气候统计数据上进行可靠影响研究变得更加困难。本文提出了一种新颖的BC方法来校正时间偏差。这得益于将BC重新构想为概率模型而不是算法流程，并将最先进的机器学习（ML）概率关注模型调整到BC任务中。通过尼日利亚阿布贾的热浪持续时间统计案例研究...

    arXiv:2402.14169v1 Announce Type: new  Abstract: Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and
    
[^291]: GenAudit：利用证据修复语言模型输出中的事实错误

    GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence

    [https://arxiv.org/abs/2402.12566](https://arxiv.org/abs/2402.12566)

    GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。

    

    LLMs即使可以访问参考文档，也可能生成事实不准确的陈述。在高风险应用中（例如基于文档的医疗保健或金融问答），这样的错误可能具有危险性。我们提出了GenAudit -- 一个旨在帮助检查基于文档任务语言模型响应的工具。GenAudit通过修订或删除未被参考文档支持的声明，同时为看似被证据支持的事实提供来自参考文献的证据，来建议修改LLM响应。我们训练模型来执行这些任务，并设计了一个交互界面，向用户呈现建议的修改和证据。通过人工评分员的全面评估显示，GenAudit在总结不同领域文档时能够检测出8种不同的LLM输出中的错误。为确保系统能够标记大多数错误，我们提出了一种方法，可以提高错误召回率，同时最小化对预处理的影响。

    arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
    
[^292]: 生成式半监督图异常检测

    Generative Semi-supervised Graph Anomaly Detection

    [https://arxiv.org/abs/2402.11887](https://arxiv.org/abs/2402.11887)

    提出了一种用于半监督图异常检测的生成式方法，通过生成模拟异常节点来训练判别性单类分类器，以更好地利用图中的已知正常节点。

    

    这项工作考虑了一个实际情境下的半监督图异常检测（GAD），在这个情境中，图中的部分节点被知晓是正常的，与大多数GAD研究中使用完全未标记图的无监督情况形成对比。我们发现，可以利用这些正常节点有助于提升现有无监督GAD方法在半监督情境下的检测性能。然而，它们对这些正常节点的利用是有限的。在本文中，我们提出了一种新颖的用于半监督情境的生成式GAD方法（GGAD），以更好地利用这些正常节点。其关键思想是生成模拟异常节点的异常节点，它们融合了本地结构和节点表示，为训练判别型单类分类器提供有效的负面节点样本。

    arXiv:2402.11887v1 Announce Type: new  Abstract: This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and 
    
[^293]: ChatGPT是否能支持开发者？对大型语言模型用于代码生成的实证评估

    Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation

    [https://arxiv.org/abs/2402.11702](https://arxiv.org/abs/2402.11702)

    大型语言模型在代码生成方面表现出显著能力，但目前主要用于展示概念或提供示例，需要进一步改进才能实现生产就绪代码。

    

    大型语言模型（LLMs）已经展示出在代码生成方面的显著能力，许多先前的研究显示它们在各种开发场景中具有很大潜力。然而，这些研究主要提供了在研究环境中的评估，这在理解LLM在实际世界中能有效支持开发者方面留下了显著的空白。为了解决这个问题，我们对DevGPT中的对话进行了实证分析，这是从开发者与ChatGPT的对话中收集的数据集（通过GitHub等平台上的Share Link功能捕获）。我们的实证结果表明，目前使用LLM生成的代码的实践通常仅限于展示高层概念或提供文档中的示例，而不是作为可用于生产的代码。这些发现表明，在LLM代码生成方面还需要大量未来工作才能使其成为不可或缺的组成部分。

    arXiv:2402.11702v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts o
    
[^294]: 我们能否用软提示LLMs来进行图学习任务？

    Can we soft prompt LLMs for graph learning tasks?

    [https://arxiv.org/abs/2402.10359](https://arxiv.org/abs/2402.10359)

    引入了GraphPrompter框架，通过软提示将图信息与LLMs对齐，以进一步探究LLMs理解图信息的潜力。

    

    图在表示社交网络、生物数据和引用网络等现实世界应用中的复杂关系方面起着重要作用。最近，大型语言模型（LLMs）在各个领域取得了巨大成功，这使得将LLMs应用于图表格尤为诱人。然而，直接将LLMs应用于图表格形式存在独特挑战，因为图表格形式与文本形式之间存在差异和不匹配。因此，为了进一步探究LLMs理解图信息的潜力，我们引入了GraphPrompter，这是一个通过软提示来将图信息与LLMs对齐的新颖框架。具体而言，GraphPrompter包括两个主要组件：一个图神经网络用于编码复杂的图信息，以及一个能够有效处理文本信息的LLM。在不同基准数据集上进行了广泛实验，涵盖了节点分类和链接预测任务。

    arXiv:2402.10359v1 Announce Type: cross  Abstract: Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tas
    
[^295]: HyperAgent：一种简单、可扩展、高效且可证明用于复杂环境的强化学习框架

    HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments

    [https://arxiv.org/abs/2402.10228](https://arxiv.org/abs/2402.10228)

    HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    

    为了在资源约束下解决复杂任务，强化学习（RL）代理需要简单、高效、可扩展、具有大状态空间和不断积累的交互数据。我们提出了HyperAgent，这是一个具有超模型、索引抽样方案和增量更新机制的RL框架，可以在一般价值函数逼近中进行计算高效的顺序后验逼近和数据高效的动作选择，超越了共轭性。HyperAgent的实现简单，只需要在DDQN中添加一个模块和一行额外代码。在实践中，HyperAgent在大规模深度RL基准测试中表现出稳健的性能，无论是在数据还是计算方面都获得了显着的效率提升。在理论上，在实际可扩展的算法中，HyperAgent是第一个能够实现可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
    
[^296]: 重新审视带有内存单子的循环强化学习

    Revisiting Recurrent Reinforcement Learning with Memory Monoids

    [https://arxiv.org/abs/2402.09900](https://arxiv.org/abs/2402.09900)

    这篇论文重新审视了使用内存单子的循环强化学习方法。通过定义新颖的内存单子框架并提出一种新的批处理方法，改进了样本效率、增加了回报并简化了实现过程。

    

    在强化学习中，像RNN和transformers这样的记忆模型通过将轨迹映射到潜在的马尔可夫状态来处理部分可观察的马尔可夫决策过程（POMDPs）。这些模型对于长序列的规模化处理能力并不特别好，尤其是与一类新兴的记忆模型（有时称为线性循环模型）相比。我们发现这些模型的循环更新是一个单子，因此我们正式定义了一个新颖的内存单子框架。我们重新审视了循环强化学习中的传统批处理方法，突出了理论和实证上的不足之处。利用内存单子的特性，我们提出了一种新的批处理方法，改进了样本效率，增加了回报，并简化了循环丢失函数在强化学习中的实施。

    arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
    
[^297]: EcoVal:一种高效的机器学习数据估值框架

    EcoVal: An Efficient Data Valuation Framework for Machine Learning

    [https://arxiv.org/abs/2402.09288](https://arxiv.org/abs/2402.09288)

    EcoVal是一种高效的机器学习数据估值框架，通过估计每个数据的内在和外在价值，实现了快速实用地估算机器学习模型数据的价值。

    

    在机器学习工作流中量化数据的价值可以在机器学习倡议中做出更具战略意义的决策中起到关键作用。现有的基于Shapley值的机器学习数据估值框架在计算方面非常昂贵，因为需要大量重复训练模型才能获得Shapley值。在本文中，我们介绍了一种高效的数据估值框架EcoVal，以快速实用的方式估算机器学习模型数据的价值。我们不直接处理独立的数据样本，而是确定类似的数据点簇的价值。这个价值进一步在所有成员簇点之间传播。我们展示了可以通过估计每个数据的内在和外在价值来确定整体数据价值。这是通过将模型的性能建模为“生产函数”来实现的，这是一个非常重要的概念。

    arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
    
[^298]: 机器学习原子团簇展开在物质科学和化学中的应用

    Cartesian atomic cluster expansion for machine learning interatomic potentials

    [https://arxiv.org/abs/2402.07472](https://arxiv.org/abs/2402.07472)

    本论文提出了一种改进的机器学习原子间势模型，使用基于笛卡尔坐标的原子密度展开来替代传统的原子团簇展开方法，并结合低维嵌入和原子间消息传递。该模型在不同系统中表现出良好的准确性、稳定性和普适性。

    

    机器学习原子间势正在革新材料科学和化学中的大规模、准确的原子模拟。这些势函数通常使用原子团簇展开或变换消息传递与球谐基函数。然而，为了保持旋转对称性，依赖Clebsch-Gordan系数会导致计算效率低下和冗余。我们提出一种替代方法：基于笛卡尔坐标的原子密度展开。该方法在保持相互作用体系的同时提供了对原子环境的完整描述。此外，我们还整合了各种化学元素的低维嵌入和原子间消息传递。所得到的势函数被命名为Cartesian Atomic Cluster Expansion(CACE)，具有良好的准确性、稳定性和普适性。我们在不同系统中进行验证，包括大规模水、小分子和25种元素高熵合金。

    Machine learning interatomic potentials are revolutionizing large-scale, accurate atomistic modelling in material science and chemistry. These potentials often use atomic cluster expansion or equivariant message passing with spherical harmonics as basis functions. However, the dependence on Clebsch-Gordan coefficients for maintaining rotational symmetry leads to computational inefficiencies and redundancies. We propose an alternative: a Cartesian-coordinates-based atomic density expansion. This approach provides a complete description of atomic environments while maintaining interaction body orders. Additionally, we integrate low-dimensional embeddings of various chemical elements and inter-atomic message passing. The resulting potential, named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy, stability, and generalizability. We validate its performance in diverse systems, including bulk water, small molecules, and 25-element high-entropy alloys.
    
[^299]: 一个新的高斯最小最大定理及其应用

    A Novel Gaussian Min-Max Theorem and its Applications

    [https://arxiv.org/abs/2402.07356](https://arxiv.org/abs/2402.07356)

    本文介绍了一个新的高斯最小最大定理，扩展了经典定理对于独立但非恒定分布的情况。此外，该定理在高维统计学、机器学习、非光滑优化和信号处理等领域有广泛的应用。

    

    Gordon的一个著名结果允许比较两个高斯过程的最小最大行为，如果满足某些不等式条件。这个结果的结果包括高斯最小最大（GMT）和凸高斯最小最大（CGMT）定理，这些定理在高维统计学、机器学习、非光滑优化和信号处理方面有广泛的应用。目前为止，没有发现满足这些不等式的其他一对高斯过程。在本文中，我们确定了这样一对新的高斯过程。由此得到的定理将经典的GMT定理和CGMT定理从基本过程中的底层高斯矩阵具有iid行的情况扩展到具有独立但非恒定分布的情况。新的CGMT定理应用于多源高斯回归问题，以及属于的二元分类问题。

    A celebrated result by Gordon allows one to compare the min-max behavior of two Gaussian processes if certain inequality conditions are met. The consequences of this result include the Gaussian min-max (GMT) and convex Gaussian min-max (CGMT) theorems which have had far-reaching implications in high-dimensional statistics, machine learning, non-smooth optimization, and signal processing. Both theorems rely on a pair of Gaussian processes, first identified by Slepian, that satisfy Gordon's comparison inequalities. To date, no other pair of Gaussian processes satisfying these inequalities has been discovered. In this paper, we identify such a new pair. The resulting theorems extend the classical GMT and CGMT Theorems from the case where the underlying Gaussian matrix in the primary process has iid rows to where it has independent but non-identically-distributed ones. The new CGMT is applied to the problems of multi-source Gaussian regression, as well as to binary classification of genera
    
[^300]: 通过反向课程强化学习训练大型语言模型进行推理

    Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning

    [https://arxiv.org/abs/2402.05808](https://arxiv.org/abs/2402.05808)

    本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。

    

    在本文中，我们提出了R$^3$：通过反向课程强化学习（RL）进行推理的学习方法，该方法只使用结果监督来实现大型语言模型的过程监督的好处。将RL应用于复杂推理的核心挑战是确定一系列行动，以获得正向奖励并提供适当的优化监督。结果监督为最终结果提供了稀疏奖励，而不识别错误位置，而过程监督提供了逐步奖励，但需要大量手动注释。R$^3$通过学习正确演示来克服这些限制。具体而言，R$^3$将推理的起始状态从演示的结束滑动到开始，从而在所有阶段都促进了更容易的模型探索。因此，R$^3$建立了一个逐步的课程，使结果监督能够提供阶段级信号并精确定位错误。

    In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
    
[^301]: 明明就在眼前：对弱势患者群体进行不可检测的对抗性偏见攻击

    Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations

    [https://arxiv.org/abs/2402.05713](https://arxiv.org/abs/2402.05713)

    该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。

    

    人工智能在放射学中的广泛应用揭示了深度学习模型加剧对弱势患者群体的临床偏见的风险。虽然先前的文献主要关注训练的深度学习模型所展示的偏见的量化，但针对特定人口群体的对抗性偏见攻击以及其在临床环境中的影响仍然是一个未被充分研究的医学影像领域。在这项工作中，我们证明了针对人口统计学标签的毒化攻击可以向深度学习模型引入对抗性的诊断不足偏见，并在不影响整体模型性能的情况下降低对被低估群体的性能。此外，我们的结果在多个性能指标和人口群体（如性别、年龄以及其交叉子群）上表明，群体对于不可检测的对抗性偏见攻击的脆弱性与其在模型的训练数据中的表征直接相关。

    The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
    
[^302]: 重复使用批次在两层网络的梯度下降中的好处：打破信息和跳跃指数的诅咒

    The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents

    [https://arxiv.org/abs/2402.03220](https://arxiv.org/abs/2402.03220)

    该论文研究了在两层神经网络中学习多指数目标函数时，重复使用批次的梯度下降（GD）的训练动态。研究发现，与单次GD相比，多次GD能够克服目标函数的限制，仅需两个时间步骤就能实现网络与目标子空间的重叠，展示了在有限时间内有效学习的广泛函数类。这些结果基于动力平均场理论（DMFT）的分析。

    

    本研究探讨了学习多指数目标函数时，两层神经网络的训练动态。我们关注重复多次使用批次的多次梯度下降（GD），并展示它与单次梯度下降相比，显著改变了对于哪些函数是可学习的的结论。具体而言，我们发现具有有限步长的多次GD能够克服目标函数的信息指数（Ben Arous等人，2021）和跳跃指数（Abbe等人，2023）所给出的梯度流和单次GD的限制。我们发现，通过重复使用批次，网络仅需两个时间步骤就能与目标子空间达成重叠，即使函数不满足阶梯性质（Abbe等人，2021）。我们对能够在有限时间内有效学习的（广泛的）函数类进行了表征。我们的结果证明基于动力平均场理论（DMFT）的分析。我们进一步提供了动态的闭式描述。

    We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamica
    
[^303]: 无参随机优化的自由度有多高？

    How Free is Parameter-Free Stochastic Optimization?

    [https://arxiv.org/abs/2402.03126](https://arxiv.org/abs/2402.03126)

    这个论文研究了无参随机优化的问题，提出了一种完全无参的方法，通过简单的超参数搜索技术在非凸和凸设置下都能取得优于先进算法的性能。同时，论文还建立了一个下界，指出完全无参的方法在某些情况下无法实现。

    

    我们研究了无参随机优化的问题，探讨了在什么条件下可以存在完全无参的方法：这些方法可以达到与最优调参方法相竞争的收敛速度，而不需要对真实问题参数有很多知识。现有的无参方法只能被视为“部分”无参，因为它们需要对真实问题参数有一些非平凡的知识，比如随机梯度范数的上界、到最小值的距离的上界等。在非凸设置中，我们证明了一个简单的超参数搜索技术可以得到一个完全无参的方法，在性能上超过了更复杂的先进算法。在具有噪声函数值的凸设置下，在较小的噪声假设下，我们也提供了类似的结果。最后，假设只能访问随机梯度，我们建立了一个下界，使得完全无参的方法无法实现。

    We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free s
    
[^304]: LHRS-Bot：利用VGI增强的大型多模态语言模型赋能遥感领域

    LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model

    [https://arxiv.org/abs/2402.02544](https://arxiv.org/abs/2402.02544)

    LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    

    大型语言模型（LLMs）的革命性能力开创了多模态大型语言模型（MLLMs）并促进了在各个专业领域的多样化应用。然而，在遥感（RS）领域中，近期的MLLM努力未能充分考虑到遥感图像中多样的地理景观和物体。为了弥补这一差距，我们构建了一个大规模的RS图像-文本数据集LHRS-Align，以及一个信息丰富的RS特定指导数据集LHRS-Instruct，利用丰富的自愿地理信息（VGI）和全球可用的RS图像。在此基础上，我们引入了LHRS-Bot，一种针对RS图像理解的MLLM，通过一种新颖的多层次视觉-语言对齐策略和课程学习方法。全面的实验证明，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
    
[^305]: 多模态协同编排通过多任务贝叶斯优化探索组合库中的结构-性质关系

    Multimodal Co-orchestration for Exploring Structure-Property Relationships in Combinatorial Libraries via Multi-Task Bayesian Optimization

    [https://arxiv.org/abs/2402.02198](https://arxiv.org/abs/2402.02198)

    本研究提出并实现了一种多模态协同编排的方法，通过多任务贝叶斯优化来探索组合库中的结构-性质关系。该方法利用变分自动编码器的降维和表示学习来控制复杂可观测量的测量。

    

    自动化和自主仪器的快速发展带来了多模态工具的协同编排机会，这些工具配备了多种顺序检测方法，或者多个表征工具来同时探索相同样品。这可以以组合库为例，可以通过多个工具同时在多个位置探索，或者在自动合成系统中进行下游表征。在协同编排方法中，从一种模态中获得的信息应该加速其他模态的发现。相应地，协同编排代理应该根据预期的知识增益和测量成本来选择测量模态。在这里，我们提出并实施了一种用于进行具有复杂可观测量（如光谱或图像）的测量的协同编排方法。该方法依赖于变分自动编码器的降维和表示学习来控制潜在空间。

    The rapid growth of automated and autonomous instrumentations brings forth an opportunity for the co-orchestration of multimodal tools, equipped with multiple sequential detection methods, or several characterization tools to explore identical samples. This can be exemplified by the combinatorial libraries that can be explored in multiple locations by multiple tools simultaneously, or downstream characterization in automated synthesis systems. In the co-orchestration approaches, information gained in one modality should accelerate the discovery of other modalities. Correspondingly, the orchestrating agent should select the measurement modality based on the anticipated knowledge gain and measurement cost. Here, we propose and implement a co-orchestration approach for conducting measurements with complex observables such as spectra or images. The method relies on combining dimensionality reduction by variational autoencoders with representation learning for control over the latent space 
    
[^306]: 用特征值修正提升谱图神经网络的表达能力

    Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction

    [https://arxiv.org/abs/2401.15603](https://arxiv.org/abs/2401.15603)

    该论文提出了一种特征值修正策略，可以提升谱图神经网络的表达能力，使多项式滤波器摆脱重复特征值输入的限制，并增强了特征值的均匀分布。

    

    在最近几年中，特征为多项式滤波器的谱图神经网络越来越受到关注，在节点分类等任务中取得了显著的表现。这些模型通常假设规范化拉普拉斯矩阵的特征值彼此不同，因此期望多项式滤波器具有很高的拟合能力。然而，本文在实证上观察到规范化拉普拉斯矩阵经常具有重复的特征值。此外，我们从理论上建立了可辨认特征值的数量在确定谱图神经网络的表达能力方面起着关键作用。鉴于这一观察结果，我们提出了一种特征值修正策略，可以使多项式滤波器摆脱重复特征值输入的限制。具体而言，所提出的特征值修正策略增强了特征值的均匀分布，从而减轻了谱图神经网络的表达能力受限的问题。

    arXiv:2401.15603v2 Announce Type: replace  Abstract: In recent years, spectral graph neural networks, characterized by polynomial filters, have garnered increasing attention and have achieved remarkable performance in tasks such as node classification. These models typically assume that eigenvalues for the normalized Laplacian matrix are distinct from each other, thus expecting a polynomial filter to have a high fitting ability. However, this paper empirically observes that normalized Laplacian matrices frequently possess repeated eigenvalues. Moreover, we theoretically establish that the number of distinguishable eigenvalues plays a pivotal role in determining the expressive power of spectral graph neural networks. In light of this observation, we propose an eigenvalue correction strategy that can free polynomial filters from the constraints of repeated eigenvalue inputs. Concretely, the proposed eigenvalue correction strategy enhances the uniform distribution of eigenvalues, thus mit
    
[^307]: Fourier Transporter：三维双等变机器人操作

    Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D

    [https://arxiv.org/abs/2401.12046](https://arxiv.org/abs/2401.12046)

    提出了一种利用双SE(d)xSE(d)对称性的Fourier Transporter（FourTran）方法，用于实现在3D环境中更高的样本效率，在RLbench基准测试中取得了最先进的结果。

    

    许多复杂的机器人操作任务可以分解为一系列的拾取和放置动作。在3D环境中，训练机器人代理学习这个序列通常需要许多迭代或演示。在这项工作中，我们提出了傅立叶传输器（Fourier Transporter，FourTran），利用拾放问题中的双SE(d)xSE(d)对称性，实现了更高的样本效率。FourTran是一种使用专家演示训练的开环行为克隆方法，用于预测新环境下的拾放动作。FourTran受限于独立地合并拾放动作的对称性。我们的方法利用了一种纤维空间的傅立叶变换，可以实现高效的构造。我们在RLbench基准测试上测试了我们提出的网络，在各种任务中取得了最先进的结果。

    arXiv:2401.12046v2 Announce Type: replace-cross  Abstract: Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions. Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments. In this work, we propose Fourier Transporter (FourTran) which leverages the two-fold SE(d)xSE(d) symmetry in the pick-place problem to achieve much higher sample efficiency. FourTran is an open-loop behavior cloning method trained using expert demonstrations to predict pick-place actions on new environments. FourTran is constrained to incorporate symmetries of the pick and place actions independently. Our method utilizes a fiber space Fourier transformation that allows for memory-efficient construction. We test our proposed network on the RLbench benchmark and achieve state-of-the-art results across various tasks.
    
[^308]: 非各向同性梯度噪声下随机重球方法的加速收敛性

    Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise

    [https://arxiv.org/abs/2312.14567](https://arxiv.org/abs/2312.14567)

    填补了理论空白，建立了一种非渐近收敛界，证明了在各向异性梯度噪声条件下，随机重球方法在二次目标上可以提供$\tilde{\mathcal{O}}(\sqrt{\kappa})$的加速收敛。

    

    对于优化深度学习模型，带有递减学习率的重球动量经常与SGD一起使用。然而，与其在实践中的流行相比，对其理论性质的了解仍然非常有限，特别是在标准的二次回归问题的各向异性梯度噪声条件下。本文填补了这一理论空白，通过在二次目标下，建立了一种具有步长衰减调度器的随机重球方法的非渐近收敛界，而此条件下的梯度噪声是各向异性的。直接影响是，我们展示了重球动量可以提供$\tilde{\mathcal{O}}(\sqrt{\kappa})$的SGD偏差项加速收敛，同时实现接近最优的收敛率。

    arXiv:2312.14567v2 Announce Type: replace  Abstract: Heavy-ball momentum with decaying learning rates is widely used with SGD for optimizing deep learning models. In contrast to its empirical popularity, the understanding of its theoretical property is still quite limited, especially under the standard anisotropic gradient noise condition for quadratic regression problems. Although it is widely conjectured that heavy-ball momentum method can provide accelerated convergence and should work well in large batch settings, there is no rigorous theoretical analysis. In this paper, we fill this theoretical gap by establishing a non-asymptotic convergence bound for stochastic heavy-ball methods with step decay scheduler on quadratic objectives, under the anisotropic gradient noise condition. As a direct implication, we show that heavy-ball momentum can provide $\tilde{\mathcal{O}}(\sqrt{\kappa})$ accelerated convergence of the bias term of SGD while still achieving near-optimal convergence rat
    
[^309]: 通过条件视频扩散学习奖励

    Diffusion Reward: Learning Rewards via Conditional Video Diffusion

    [https://arxiv.org/abs/2312.14134](https://arxiv.org/abs/2312.14134)

    通过条件视频扩散学习奖励，解决复杂视觉强化学习问题，有效提高了任务成功率，超越了基线方法。

    

    从专家视频中学习奖励为强化学习任务指定预期行为提供了一种经济高效的解决方案。本文提出了Diffusion Reward，这是一个通过条件视频扩散模型从专家视频中学习奖励以解决复杂视觉强化学习问题的新框架。我们的关键见解是，在专家轨迹的条件下观察到较低的生成多样性。因此，Diffusion Reward被形式化为负的条件熵，鼓励专家式行为的有效探索。我们展示了我们的方法在MetaWorld和Adroit的10个机器人操纵任务中以视觉输入和稀疏奖励的有效性。此外，Diffusion Reward甚至能够成功有效地解决未见过的任务，大大超越了基线方法。项目页面和代码：https://diffusion-reward.github.io/。

    arXiv:2312.14134v2 Announce Type: replace  Abstract: Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is observed when conditioned on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert-like behaviors. We show the efficacy of our method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual input and sparse reward. Moreover, Diffusion Reward could even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io/.
    
[^310]: 在具有未知方差的双臂高斯赌臂机器人中局部最优固定预算最佳臂识别问题

    Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances

    [https://arxiv.org/abs/2312.12741](https://arxiv.org/abs/2312.12741)

    提出了一种在双臂高斯赌臂机器人中解决具有未知方差情况下局部最优固定预算最佳臂识别问题的策略。

    

    我们解决了双臂高斯赌臂机器人中带有固定预算的最佳臂识别（BAI）问题。 在BAI中，给定多个臂，我们通过自适应实验来找到具有最高期望奖励的最佳臂。 Kaufmann等人（2016年）为误识别最佳臂的概率提出了一个下界。 他们还提出了一种策略，假设奖励的方差已知，并表明随着预算无限接近，这种策略在概念上是最优的，即其误识别概率与下界匹配。 但是，当方差未知时，一种渐近优的策略尚未被发现。 针对这个未解决问题，我们提出了一种在自适应实验中估计方差并以估计标准差比例抽取臂的策略。 我们称此策略为Neyman分配（NA）-增强倒概率加权（AIPW）策略。

    arXiv:2312.12741v2 Announce Type: replace  Abstract: We address the problem of best arm identification (BAI) with a fixed budget for two-armed Gaussian bandits. In BAI, given multiple arms, we aim to find the best arm, an arm with the highest expected reward, through an adaptive experiment. Kaufmann et al. (2016) develops a lower bound for the probability of misidentifying the best arm. They also propose a strategy, assuming that the variances of rewards are known, and show that it is asymptotically optimal in the sense that its probability of misidentification matches the lower bound as the budget approaches infinity. However, an asymptotically optimal strategy is unknown when the variances are unknown. For this open issue, we propose a strategy that estimates variances during an adaptive experiment and draws arms with a ratio of the estimated standard deviations. We refer to this strategy as the Neyman Allocation (NA)-Augmented Inverse Probability weighting (AIPW) strategy. We then d
    
[^311]: 基于多目标强化学习的压水堆优化方法

    Multi-Objective Reinforcement Learning-based Approach for Pressurized Water Reactor Optimization

    [https://arxiv.org/abs/2312.10194](https://arxiv.org/abs/2312.10194)

    提出了一种新的多目标强化学习方法PEARL，通过学习单一策略解决多目标问题，避免了需要多个神经网络解决简单子问题的需求。

    

    一种新颖的方法，被称为带有强化学习的Pareto包络（PEARL），已被开发出来以解决多目标问题所带来的挑战，特别是在工程领域中，候选解的评估可能会耗费大量时间。PEARL通过学习单一策略来区别于传统基于策略的多目标强化学习方法，消除了需要多个神经网络独立解决简单子问题的需求。受深度学习和进化技术启发，已经开发了几个版本，以适应无约束和有约束的问题领域。课程学习被利用来有效管理这些版本中的约束。首先，PEARL的性能在经典多目标基准上进行了评估。此外，它还在两个实际的PWR堆芯装载方案优化问题上进行了测试，以展示其在实际应用中的可行性。

    arXiv:2312.10194v3 Announce Type: replace  Abstract: A novel method, the Pareto Envelope Augmented with Reinforcement Learning (PEARL), has been developed to address the challenges posed by multi-objective problems, particularly in the field of engineering where the evaluation of candidate solutions can be time-consuming. PEARL distinguishes itself from traditional policy-based multi-objective Reinforcement Learning methods by learning a single policy, eliminating the need for multiple neural networks to independently solve simpler sub-problems. Several versions inspired from deep learning and evolutionary techniques have been crafted, catering to both unconstrained and constrained problem domains. Curriculum Learning is harnessed to effectively manage constraints in these versions. PEARL's performance is first evaluated on classical multi-objective benchmarks. Additionally, it is tested on two practical PWR core Loading Pattern optimization problems to showcase its real-world applicab
    
[^312]: CaVE: 一种面向快速预测然后优化的锥对齐方法，用于二进制线性规划

    CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize with Binary Linear Programs

    [https://arxiv.org/abs/2312.07718](https://arxiv.org/abs/2312.07718)

    CaVE提出了一种新的端到端训练方法，即预测然后优化，通过将预测的成本向量与真实最优解的法线锥对齐，实现了决策感知型学习模型的训练。

    

    预测然后优化的端到端框架，也称为决策焦点学习，因其能够将优化集成到机器学习模型的训练过程中，并从上下文实例信息中预测未知成本（目标函数）系数而备受欢迎。在这个领域，大多数感兴趣的问题通常可以被描述为整数线性规划。在这项工作中，我们专注于二进制线性规划（BLPs），并提出了一种新的端到端训练方法，即预测然后优化。我们的方法，锥对齐向量估计（CaVE），将预测的成本向量与训练实例的真实最优解对应的法线锥进行对齐。当预测的成本向量位于锥内部时，二进制问题线性松弛的最优解也是最优的。这种对齐不仅能产生决策感知型学习模型，还能显著

    arXiv:2312.07718v2 Announce Type: replace  Abstract: The end-to-end predict-then-optimize framework, also known as decision-focused learning, has gained popularity for its ability to integrate optimization into the training procedure of machine learning models that predict the unknown cost (objective function) coefficients of optimization problems from contextual instance information. Naturally, most of the problems of interest in this space can be cast as integer linear programs. In this work, we focus on binary linear programs (BLPs) and propose a new end-to-end training method to predict-then-optimize. Our method, Cone-aligned Vector Estimation (CaVE), aligns the predicted cost vectors with the normal cone corresponding to the true optimal solution of a training instance. When the predicted cost vector lies inside the cone, the optimal solution to the linear relaxation of the binary problem is optimal. This alignment not only produces decision-aware learning models but also dramatic
    
[^313]: 使用去噪扩散概率模型的气动翼流场模拟的不确定性感知代理模型

    Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with Denoising Diffusion Probabilistic Models

    [https://arxiv.org/abs/2312.05320](https://arxiv.org/abs/2312.05320)

    本研究使用去噪扩散概率模型为气动翼流场模拟训练了不确定性感知代理模型，成功捕捉整个解分布并准确估计模拟不确定性。

    

    将神经网络作为湍流模拟的代理模型是一个备受关注的话题。同时，在代理模型的预测中体现模拟的固有不确定性仍然非常具有挑战性。本研究首次尝试使用去噪扩散概率模型（DDPMs）训练一个用于湍流模拟的不确定性感知代理模型。由于其普遍性，选择以各种形状、雷诺数和攻角的气动翼流场模拟作为学习目标。我们的结果表明，DDPMs能成功捕捉解的整个分布，从而准确估计模拟的不确定性。DDPMs的性能还与以贝叶斯神经网络和异方差模型形式的不同基准进行了比较。实验证明，DDPMs在性能上优于其他方法。

    arXiv:2312.05320v2 Announce Type: replace-cross  Abstract: Leveraging neural networks as surrogate models for turbulence simulation is a topic of growing interest. At the same time, embodying the inherent uncertainty of simulations in the predictions of surrogate models remains very challenging. The present study makes a first attempt to use denoising diffusion probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for turbulence simulations. Due to its prevalence, the simulation of flows around airfoils with various shapes, Reynolds numbers, and angles of attack is chosen as the learning objective. Our results show that DDPMs can successfully capture the whole distribution of solutions and, as a consequence, accurately estimate the uncertainty of the simulations. The performance of DDPMs is also compared with varying baselines in the form of Bayesian neural networks and heteroscedastic models. Experiments demonstrate that DDPMs outperform the other methods regardin
    
[^314]: 西班牙语问答中的语言模型知识蒸馏

    Language Model Knowledge Distillation for Efficient Question Answering in Spanish

    [https://arxiv.org/abs/2312.04193](https://arxiv.org/abs/2312.04193)

    通过知识蒸馏，我们开发了SpanishTinyRoBERTa，一个基于RoBERTa的西班牙语压缩语言模型，用于提高西班牙语问答的效率。

    

    最近发展的预训练西班牙语言模型的进展在许多自然语言处理(NLP)任务中取得了重大进展，如问答。然而，缺乏高效模型对这些模型在资源受限环境中的采用构成了一道障碍。因此，针对西班牙语的较小的蒸馏模型可能被证明是高度可扩展的，并促进它们在各种任务和场景中的进一步采用。在这项工作中，我们朝着这个方向迈出了一步，通过开发基于RoBERTa的西班牙语高效问答压缩语言模型SpanishTinyRoBERTa。为了实现这一目标，我们从一个大模型向一个更轻的模型进行知识蒸馏，这使得更广泛的实现成为可能，即使在计算资源有限的地区，也能实现可忽略的性能牺牲。我们的实验表明，这种密集的蒸馏模型能够实现限制的计算性能兼容。

    arXiv:2312.04193v2 Announce Type: replace  Abstract: Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained environments. Therefore, smaller distilled models for the Spanish language could be proven to be highly scalable and facilitate their further adoption on a variety of tasks and scenarios. In this work, we take one step in this direction by developing SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient question answering in Spanish. To achieve this, we employ knowledge distillation from a large model onto a lighter model that allows for a wider implementation, even in areas with limited computational resources, whilst attaining negligible performance sacrifice. Our experiments show that the dense distilled model can st
    
[^315]: 基于联邦学习的用于增强无人机隐私和安全的入侵检测系统

    A Novel Federated Learning-Based IDS for Enhancing UAVs Privacy and Security

    [https://arxiv.org/abs/2312.04135](https://arxiv.org/abs/2312.04135)

    本论文引入了基于联邦学习的入侵检测系统(FL-IDS)，旨在解决FANETs中集中式系统所遇到的挑战，降低了计算和存储成本，适合资源受限的无人机。

    

    无人机在飞行自组织网络(FANETs)中运行时会遇到安全挑战，因为这些网络具有动态和分布式的特性。先前的研究主要集中在集中式入侵检测上，假设一个中央实体负责存储和分析来自所有设备的数据。然而，这些方法面临计算和存储成本以及单点故障风险等挑战，威胁到数据隐私和可用性。数据在互连设备之间广泛分散的情况突显了去中心化方法的必要性。本文介绍了基于联邦学习的入侵检测系统(FL-IDS)，解决了FANETs中集中式系统遇到的挑战。FL-IDS在去中心化方式下运行，降低了客户端和中央服务器的计算和存储成本，这对于资源受限的无人机至关重要。

    arXiv:2312.04135v2 Announce Type: replace-cross  Abstract: Unmanned aerial vehicles (UAVs) operating within Flying Ad-hoc Networks (FANETs) encounter security challenges due to the dynamic and distributed nature of these networks. Previous studies predominantly focused on centralized intrusion detection, assuming a central entity responsible for storing and analyzing data from all devices.However, these approaches face challenges including computation and storage costs, along with a single point of failure risk, threatening data privacy and availability. The widespread dispersion of data across interconnected devices underscores the necessity for decentralized approaches. This paper introduces the Federated Learning-based Intrusion Detection System (FL-IDS), addressing challenges encountered by centralized systems in FANETs. FL-IDS reduces computation and storage costs for both clients and the central server, crucial for resource-constrained UAVs. Operating in a decentralized manner, F
    
[^316]: 对稳健的OOD自监督对比心音图表示学习增强方法的全面评估

    A Comprehensive Evaluation of Augmentations for Robust OOD Self-Supervised Contrastive Phonocardiogram Representation Learning

    [https://arxiv.org/abs/2312.00502](https://arxiv.org/abs/2312.00502)

    本研究通过对比自监督学习应用于1D心音图样本中异常检测，进行了广泛的音频增强方法比较评估和在多个数据集上训练分类器的研究。

    

    尽管近年来深度学习模型的研究活动有所增加，但在医学等多个现实世界环境中，这些模型尚未被广泛接受。高质量标记数据的短缺经常阻碍了开发稳健且具有一般性的模型，当面临新收集的超出分布（OOD）数据集时，这些模型不会因效果下降而受损。对比自监督学习（SSL）为标记数据稀缺性提供了潜在解决方案，因为它利用未标记数据增加模型的效能和稳健性。本研究中，我们提出将对比SSL应用于检测1D心音图（PCG）样本中的异常，通过学习信号的广义表示。具体来说，我们进行了一项广泛的比较评估，涉及多种基于音频的增强方法，评估了在不同下游任务的多个数据集上训练的分类器，最终

    arXiv:2312.00502v2 Announce Type: replace  Abstract: Despite the recent increase in research activity, deep-learning models have not yet been widely accepted in several real-world settings, such as medicine. The shortage of high-quality annotated data often hinders the development of robust and generalizable models, which do not suffer from degraded effectiveness when presented with newly-collected, out-of-distribution (OOD) datasets. Contrastive Self-Supervised Learning (SSL) offers a potential solution to labeled data scarcity, as it takes advantage of unlabeled data to increase model effectiveness and robustness. In this research, we propose applying contrastive SSL for detecting abnormalities in 1D phonocardiogram (PCG) samples by learning a generalized representation of the signal. Specifically, we perform an extensive comparative evaluation of a wide range of audio-based augmentations, evaluate trained classifiers on multiple datasets across different downstream tasks, and finall
    
[^317]: 通过Wasserstein度量进行数据集精炼

    Dataset Distillation via the Wasserstein Metric

    [https://arxiv.org/abs/2311.18531](https://arxiv.org/abs/2311.18531)

    通过引入Wasserstein距离及其重心，我们提出一种有效的数据集精炼方法，利用先验知识提高分布匹配效果，实现了新的最先进性能。

    

    数据集精炼（DD）作为一种强大的策略，将大型数据集的丰富信息封装为明显更小的合成等价物，从而在减少计算开销的同时保留模型性能。为实现这一目标，我们引入了Wasserstein距离，这是一种基于最优输运理论的度量，用于增强DD中的分布匹配。我们的方法利用Wasserstein重心提供了一种在量化分布差异和高效捕获分布集合中心的几何意义方法。通过在预训练分类模型的特征空间中嵌入合成数据，我们促进了有效的分布匹配，利用这些模型固有的先验知识。我们的方法不仅保持了基于分布匹配的技术的计算优势，而且在一系列任务中实现了新的最先进性能。

    arXiv:2311.18531v2 Announce Type: replace-cross  Abstract: Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a ran
    
[^318]: 多项式信念网络

    Multinomial belief networks

    [https://arxiv.org/abs/2311.16909](https://arxiv.org/abs/2311.16909)

    提出了一种深度生成模型，用于处理具有多项式计数数据的分析需求，并能够从数据中完全自动提取出生物意义的元签名。

    

    机器学习中的贝叶斯方法在需要量化不确定性、处理缺失观测、样本稀缺或数据稀疏时是具有吸引力的。为了满足这些分析需求，我们提出了一种用于多项式计数数据的深层生成模型，其中网络的权重和隐藏单元均服从狄利克雷分布。我们制定了一个利用一系列增广关系的吉布斯抽样过程，类似于周-丛-陈模型。我们将该模型应用于小规模手写数字和一个大型的DNA突变癌症实验数据集，并展示了该模型如何能够完全数据驱动地提取出生物意义的元签名。

    arXiv:2311.16909v2 Announce Type: replace-cross  Abstract: A Bayesian approach to machine learning is attractive when we need to quantify uncertainty, deal with missing observations, when samples are scarce, or when the data is sparse. All of these commonly apply when analysing healthcare data. To address these analytical requirements, we propose a deep generative model for multinomial count data where both the weights and hidden units of the network are Dirichlet distributed. A Gibbs sampling procedure is formulated that takes advantage of a series of augmentation relations, analogous to the Zhou--Cong--Chen model. We apply the model on small handwritten digits, and a large experimental dataset of DNA mutations in cancer, and we show how the model is able to extract biologically meaningful meta-signatures in a fully data-driven way.
    
[^319]: 模块化神经网络用于时间序列预测：使用注意力进行解释性和特征选择

    Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention

    [https://arxiv.org/abs/2311.16834](https://arxiv.org/abs/2311.16834)

    该论文提出了一种新颖的模块化神经网络模型，用于多变量时间序列预测，通过循环神经网络学习时间依赖关系，并使用基于注意力的特征选择组件实现解释性，实验结果表明其优于当前最先进的解释性神经加性模型（NAM）及其变体。

    

    多变量时间序列在医疗保健、气象学和生命科学等领域有许多应用。虽然深度学习模型在时间序列预测方面表现出色，但被批评为“黑盒”或无法解释。本文提出了一种新颖的用于多变量时间序列预测的模块化神经网络模型，其构造具有解释性。循环神经网络学习数据中的时间依赖关系，而基于注意力的特征选择组件选择最相关的特征并抑制在学习时间依赖性中使用的冗余特征。从选择的特征独立训练模块化深度网络，向用户展示特征如何影响结果，使模型具有解释性。实验结果表明，这种方法可以超过最先进的可解释性神经加性模型（NAM）及其变体。

    arXiv:2311.16834v3 Announce Type: replace-cross  Abstract: Multivariate time series have many applications, from healthcare and meteorology to life science. Although deep learning models have shown excellent predictive performance for time series, they have been criticised for being "black-boxes" or non-interpretable. This paper proposes a novel modular neural network model for multivariate time series prediction that is interpretable by construction. A recurrent neural network learns the temporal dependencies in the data while an attention-based feature selection component selects the most relevant features and suppresses redundant features used in the learning of the temporal dependencies. A modular deep network is trained from the selected features independently to show the users how features influence outcomes, making the model interpretable. Experimental results show that this approach can outperform state-of-the-art interpretable Neural Additive Models (NAM) and variations thereo
    
[^320]: Symphony: 对分子生成的点对称球形谐波的对称等变自回归模型

    Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation

    [https://arxiv.org/abs/2311.16199](https://arxiv.org/abs/2311.16199)

    Symphony提出了一种新颖的$E(3)$-等变自回归生成模型，通过使用点对称球形谐波信号来高效建模分子的3D几何结构。

    

    我们提出了Symphony，这是一个$E(3)$-等变的自回归生成模型，用于3D分子几何结构的构建，通过从分子碎片中迭代地构建分子。现有的自回归模型如G-SchNet和G-SphereNet用于分子的旋转不变特征来尊重分子的3D对称性。相反，Symphony使用带有更高次$E(3)$-等变特征的消息传递。这使得通过球谐信号有效地建模分子的3D几何的概率分布成为可能。我们展示了Symphony能够准确地从QM9数据集中生成小分子，优于现有的自回归模型，并接近扩散模型的性能。

    arXiv:2311.16199v2 Announce Type: replace  Abstract: We present Symphony, an $E(3)$-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet and G-SphereNet for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree $E(3)$-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.
    
[^321]: UniRepLKNet：一种用于音频、视频、点云、时间序列和图像识别的通用感知大卷积神经网络

    UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition

    [https://arxiv.org/abs/2311.15599](https://arxiv.org/abs/2311.15599)

    提出了四项用于设计大卷积神经网络的架构指南，并展示了在多模态领域具有通用感知能力

    

    大卷积神经网络(ConvNets)最近受到了广泛的研究关注，但仍有两个未解决的重要问题需要进一步研究。1) 现有大卷积神经网络的架构在很大程度上遵循传统ConvNets或transformers的设计原则，而大卷积神经网络的架构设计仍未得到解决。2) 随着transformers主导了多种模态，仍需探讨ConvNets在视觉以外领域是否具有强大的通用感知能力。在本文中，我们从两个方面做出了贡献。1) 我们提出了四项用于设计大卷积神经网络的架构指南，其中的核心是利用大卷积核的基本特征，区别于小卷积核-它们可以广泛地观察而无需深入。遵循这些指南，我们提出的大卷积神经网络表现出领先的性能

    arXiv:2311.15599v2 Announce Type: replace-cross  Abstract: Large-kernel convolutional neural networks (ConvNets) have recently received extensive research attention, but two unresolved and critical issues demand further investigation. 1) The architectures of existing large-kernel ConvNets largely follow the design principles of conventional ConvNets or transformers, while the architectural design for large-kernel ConvNets remains under-addressed. 2) As transformers have dominated multiple modalities, it remains to be investigated whether ConvNets also have a strong universal perception ability in domains beyond vision. In this paper, we contribute from two aspects. 1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep. Following such guidelines, our proposed large-kernel ConvNet shows leading performanc
    
[^322]: 深度学习中通过几何调整的梯度下降以均匀指数速率全局$\mathcal{L}^2$最小化

    Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning

    [https://arxiv.org/abs/2311.15487](https://arxiv.org/abs/2311.15487)

    通过几何调整的梯度下降，在深度学习中以均匀指数速率实现全局$\mathcal{L}^2$最小化，这一方法在过参数化情况下具有明确自然的不变几何含义。

    

    我们考虑在深度学习网络中广泛使用的用于最小化$\mathcal{L}^2$代价函数的梯度下降流，并引入两个改进版本；一个适用于过参数化设置，另一个适用于欠参数化设置。这两个版本都具有明确自然的不变几何含义，考虑到在过参数化设置中的拉回向量丛结构和在欠参数化设置中的推前向量丛结构。在过参数化情况下，我们证明，只要满足秩条件，改进的梯度下降的所有轨道将以均匀指数收敛速率将$\mathcal{L}^2$代价驱动到全局最小值；因此，对于任何预先指定的接近全局最小值的近似，我们可以得到先验停止时间。我们指出后者与次Riemann几何的关系。

    arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.
    
[^323]: CRISP：用于基于类别的模型剪枝的混合结构稀疏性

    CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning

    [https://arxiv.org/abs/2311.14272](https://arxiv.org/abs/2311.14272)

    提出了CRISP，一种结合精细N:M结构稀疏性和粗粒度块稀疏性的混合结构稀疏模式，旨在增强基于类别的模型剪枝效率。

    

    用于分类任务的机器学习流程通常会训练一个通用模型，以实现在各种类别上的准确性。然而，典型用户只会定期遇到有限的类别选择。这种不平衡提供了一个机会，通过定制模型重点关注用户特定类别来增强计算效率。现有作品依赖于不结构化的剪枝，这在模型中引入了随机分布的非零值，使其不适用于硬件加速。另外，一些方法使用结构化剪枝，如通道剪枝，但这些方法往往只提供最小的压缩，并可能导致模型准确性降低。在这项工作中，我们提出了CRISP，一种新颖的剪枝框架，利用了混合结构稀疏性模式，结合精细的N:M结构稀疏性和粗粒度的块稀疏性。我们的剪枝策略是基于梯度的基于类别的

    arXiv:2311.14272v2 Announce Type: replace-cross  Abstract: Machine learning pipelines for classification tasks often train a universal model to achieve accuracy across a broad range of classes. However, a typical user encounters only a limited selection of classes regularly. This disparity provides an opportunity to enhance computational efficiency by tailoring models to focus on user-specific classes. Existing works rely on unstructured pruning, which introduces randomly distributed non-zero values in the model, making it unsuitable for hardware acceleration. Alternatively, some approaches employ structured pruning, such as channel pruning, but these tend to provide only minimal compression and may lead to reduced model accuracy. In this work, we propose CRISP, a novel pruning framework leveraging a hybrid structured sparsity pattern that combines both fine-grained N:M structured sparsity and coarse-grained block sparsity. Our pruning strategy is guided by a gradient-based class-aware
    
[^324]: 分子鉴定与峰归属：在NMR上利用多级多模态对齐

    Molecular Identification and Peak Assignment: Leveraging Multi-Level Multimodal Alignment on NMR

    [https://arxiv.org/abs/2311.13817](https://arxiv.org/abs/2311.13817)

    本文提出了一种新颖的解决方案，即多级多模态对齐（K-M3AID），通过在分子图和NMR光谱之间建立对应关系，采用知识引导的实例级对比学习，以解决分子检索、异构体识别和峰归属等任务中的挑战。

    

    arXiv:2311.13817v2 公告类型：替换 摘要：核磁共振（NMR）光谱在解读分子结构和动态行为方面起着至关重要的作用。虽然基于AI的NMR预测模型具有潜力，但在分子检索、异构体识别和峰归属等任务中仍然存在挑战。为此，本文引入了一种新颖的解决方案，即具有知识引导的实例级对齐的多级多模态对齐（K-M3AID），该解决方案在两种异质模态之间建立对应关系：分子图和NMR光谱。K-M3AID采用了一个双协调对比学习架构，包含三个关键模块：图级对齐模块、节点级对齐模块和通信通道。值得注意的是，在节点级对齐模块中，K-M3AID引入了知识引导的实例级对比学习。此外，K-M3AID表明在节点级对齐过程中获得的技能

    arXiv:2311.13817v2 Announce Type: replace  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays an essential role in deciphering molecular structure and dynamic behaviors. While AI-enhanced NMR prediction models hold promise, challenges still persist in tasks such as molecular retrieval, isomer recognition, and peak assignment. In response, this paper introduces a novel solution, Multi-Level Multimodal Alignment with Knowledge-Guided Instance-Wise Discrimination (K-M3AID), which establishes correspondences between two heterogeneous modalities: molecular graphs and NMR spectra. K-M3AID employs a dual-coordinated contrastive learning architecture with three key modules: a graph-level alignment module, a node-level alignment module, and a communication channel. Notably, K-M3AID introduces knowledge-guided instance-wise discrimination into contrastive learning within the node-level alignment module. In addition, K-M3AID demonstrates that skills acquired during node-level alignment
    
[^325]: 具有学习正演算子的逆问题

    Inverse Problems with Learned Forward Operators

    [https://arxiv.org/abs/2311.12528](https://arxiv.org/abs/2311.12528)

    逆问题中使用的具有学习正演算子的方法分为两种：一种完全不考虑正演算子，学习其在训练数据子空间上的限制，另一种使用简化的物理模型并依赖于训练数据来学习模型修正。这两种方法都强调对正演算子及其共轭算子的训练数据的重要性。

    

    解决逆问题需要对正演算子有所了解，但准确的模型可能计算成本高昂，因此希望有不影响重建质量的更便宜的替代方案。本章回顾了采用具有学习正演算子的逆问题中的重建方法，遵循两种不同的范例。第一种对正演算子完全不可知，并学习其在由训练数据生成的子空间上的限制。然后使用投影正则化框架找到重建。第二种使用测量过程物理的简化模型，并仅依赖于训练数据来学习模型校正。我们介绍了这两种方法的理论并进行了数值比较。一个共同的主题出现：两种方法都需要或至少受益于训练数据不仅对于正演算子而且对于其共轭算子。

    arXiv:2311.12528v2 Announce Type: replace-cross  Abstract: Solving inverse problems requires the knowledge of the forward operator, but accurate models can be computationally expensive and hence cheaper variants that do not compromise the reconstruction quality are desired. This chapter reviews reconstruction methods in inverse problems with learned forward operators that follow two different paradigms. The first one is completely agnostic to the forward operator and learns its restriction to the subspace spanned by the training data. The framework of regularisation by projection is then used to find a reconstruction. The second one uses a simplified model of the physics of the measurement process and only relies on the training data to learn a model correction. We present the theory of these two approaches and compare them numerically. A common theme emerges: both methods require, or at least benefit from, training data not only for the forward operator, but also for its adjoint.
    
[^326]: 循环变压器在学习学习算法方面更有效

    Looped Transformers are Better at Learning Learning Algorithms

    [https://arxiv.org/abs/2311.12424](https://arxiv.org/abs/2311.12424)

    提出了循环变压器架构及其训练方法，通过将迭代特征融入变压器架构中，实现了在解决数据拟合问题方面与标准变压器性能可比的效果，同时参数数量少于10%。

    

    变压器已经证明在上下文中解决来自各种(潜在)模型的数据拟合问题方面非常有效，但是，变压器架构中缺乏固有的迭代结构在模拟迭代算法方面存在挑战，而传统机器学习方法通常采用迭代算法。为了解决这个问题，我们提出了循环变压器架构及其相关的训练方法，旨在将迭代特征融入变压器架构中。实验结果表明，循环变压器在解决各种数据拟合问题方面的性能与标准变压器可比，同时参数数量不到标准变压器的10%。

    arXiv:2311.12424v3 Announce Type: replace  Abstract: Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models, as reported by Garg et al. However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative algorithms, which are commonly employed in traditional machine learning methods. To address this, we propose the utilization of looped transformer architecture and its associated training methodology, with the aim of incorporating iterative characteristics into the transformer architectures. Experimental results suggest that the looped transformer achieves performance comparable to the standard transformer in solving various data-fitting problems, while utilizing less than 10% of the parameter count.
    
[^327]: 通过因果干预揭示行为对移动预测网络的影响

    Revealing behavioral impact on mobility prediction networks through causal interventions

    [https://arxiv.org/abs/2311.11749](https://arxiv.org/abs/2311.11749)

    通过因果干预框架，评估移动因素对神经网络的影响，产生具有不同移动行为的位置序列，有助于理解和解释预测网络。

    

    深度神经网络在移动性预测任务中被越来越广泛地使用，然而它们复杂的内部运作方式给可解释性带来挑战，特别是在理解各种移动行为如何影响预测方面。本研究引入了一个因果干预框架，评估与下一个位置预测相关的移动因素对神经网络的影响 -- 这是一项专注于预测个体即将到达位置的任务。为了实现这一目标，我们使用个体移动模型生成合成位置访问序列，并通过干预它们的数据生成过程来控制行为动态。我们使用移动指标评估干预位置序列，并将其输入到训练良好的网络中以分析性能变化。结果表明，产生具有不同移动行为的位置序列的有效性，从而促进了预测网络的理解和解释。

    arXiv:2311.11749v2 Announce Type: replace-cross  Abstract: Deep neural networks are increasingly utilized in mobility prediction tasks, yet their intricate internal workings pose challenges for interpretability, especially in comprehending how various aspects of mobility behavior affect predictions. This study introduces a causal intervention framework to assess the impact of mobility-related factors on neural networks designed for next location prediction -- a task focusing on predicting the immediate next location of an individual. To achieve this, we employ individual mobility models to generate synthetic location visit sequences and control behavior dynamics by intervening in their data generation process. We evaluate the interventional location sequences using mobility metrics and input them into well-trained networks to analyze performance variations. The results demonstrate the effectiveness in producing location sequences with distinct mobility behaviors, thereby facilitating t
    
[^328]: 通过多例学习实现内在可解释的时间序列分类

    Inherently Interpretable Time Series Classification via Multiple Instance Learning

    [https://arxiv.org/abs/2311.10049](https://arxiv.org/abs/2311.10049)

    通过多例学习提出了一个新框架MILLET，使现有的深度学习时间序列分类模型变得内在可解释，同时不影响甚至改进预测性能，并在多个数据集上展示出比其他方法更高质量的稀疏解释。

    

    传统的时间序列分类（TSC）方法通常是黑匣子，难以理解其决策过程。在这项工作中，我们利用多例学习（MIL）来解决这个问题，并提出了一个名为MILLET的新框架：用于本地可解释时间序列分类的多例学习。我们将MILLET应用于现有的深度学习TSC模型，并展示它们如何变得内在可解释，而不会影响（在某些情况下，甚至提高）预测性能。我们在85个UCR TSC数据集上评估了MILLET，并提出了一个特别设计的新颖合成数据集，以促进可解释性评估。在这些数据集上，我们展示MILLET能够快速产生比其他众所周知的可解释性方法更高质量的稀疏解释。

    arXiv:2311.10049v3 Announce Type: replace-cross  Abstract: Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/J
    
[^329]: 知识图谱是否可以减少LLMs中的幻觉？：一项调查

    Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey

    [https://arxiv.org/abs/2311.07914](https://arxiv.org/abs/2311.07914)

    调查综合审查了LLMs中基于知识图谱的增强技术，重点关注其在减轻幻觉方面的效果。

    

    当代LLMs很容易产生幻觉，主要源于模型内的知识空缺。为了解决这一关键限制，研究人员采用各种策略通过整合外部知识来增强LLMs，旨在减少幻觉并提升推理准确性。在这些策略中，利用知识图谱作为外部信息源已经显示出了良好的效果。在这项调查中，我们全面审查了LLMs中基于知识图谱的增强技术，重点关注它们在减轻幻觉方面的效果。我们将这些方法系统地归类为三个总体组，提供方法比较和性能评估。最后，本调查探讨了与这些技术相关的当前趋势和挑战，并概述了这一新兴领域未来研究的潜在方向。

    arXiv:2311.07914v2 Announce Type: replace  Abstract: The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.
    
[^330]: 学习设计具有增强泛化能力的蛋白质-蛋白质相互作用

    Learning to design protein-protein interactions with enhanced generalization

    [https://arxiv.org/abs/2310.18515](https://arxiv.org/abs/2310.18515)

    本研究的三个主要贡献是构建了最大和非冗余的3D蛋白质-蛋白质相互作用数据集PPIRef，提出了可泛化跨不同蛋白质结合剂变体的PPIformer模型，并通过热力学调整预训练损失函数实现对蛋白质-蛋白质相互作用突变影响的预测，最终展示了该新方法在增强泛化能力方面的优势。

    

    发现增强蛋白质-蛋白质相互作用（PPIs）的突变对于推进生物医学研究和开发改进的治疗药物至关重要。虽然机器学习方法在这一领域取得了实质进展，但在实际场景中往往难以在训练数据之外进行泛化。本文的贡献有三个方面。首先，我们构建了PPIRef，这是一个最大的非冗余的3D蛋白质-蛋白质相互作用数据集，可以有效进行大规模学习。其次，我们利用PPIRef数据集预训练了PPIformer，这是一个新的SE(3)-等变模型，可以在不同蛋白质结合剂变体之间进行泛化。我们通过对预训练损失函数进行热力学调整，对PPIformer进行微调，以预测突变对蛋白质-蛋白质相互作用的影响。最后，我们通过在新的、未知的数据上表现出超越其他最先进方法的增强泛化能力，展示了我们的新PPIformer方法的优势。

    arXiv:2310.18515v3 Announce Type: replace  Abstract: Discovering mutations enhancing protein-protein interactions (PPIs) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical scenarios. The contributions of this work are three-fold. First, we construct PPIRef, the largest and non-redundant dataset of 3D protein-protein interactions, enabling effective large-scale learning. Second, we leverage the PPIRef dataset to pre-train PPIformer, a new SE(3)-equivariant model generalizing across diverse protein-binder variants. We fine-tune PPIformer to predict effects of mutations on protein-protein interactions via a thermodynamically motivated adjustment of the pre-training loss function. Finally, we demonstrate the enhanced generalization of our new PPIformer approach by outperforming other state-of-the-art methods on new, no
    
[^331]: PubDef：防御来自公共模型的迁移攻击

    PubDef: Defending Against Transfer Attacks From Public Models

    [https://arxiv.org/abs/2310.17645](https://arxiv.org/abs/2310.17645)

    本文提出了一个新的实用威胁模型，其中对手依赖于通过公开可用的替代模型的迁移攻击，提出了一种基于博弈论视角的专门防御方法，并在多个公共模型和攻击算法下进行了评估。

    

    对抗性攻击一直是行业中一个令人担忧且未解决的威胁。然而，通过十年的鲁棒性评估文献历史，我们了解到发起强大或最优攻击是具有挑战性的，它需要机器学习和领域专业知识。换句话说，过去大多数文献坚定假设的白盒威胁模型是不现实的。在本文中，我们提出了一个新的实用威胁模型，在这个模型中，对手依赖于通过公开可用的替代模型的迁移攻击。我们认为这种设置将成为未来安全敏感应用中最普遍的情况。我们评估了在这种设置下的迁移攻击，并提出了基于博弈论视角的专门防御方法。这些防御方法在24个公共模型和11种攻击算法下进行了评估，涵盖三个数据集（CIFAR-10、CIFAR-100 和 ImageNet）。

    arXiv:2310.17645v2 Announce Type: replace-cross  Abstract: Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under thi
    
[^332]: 用于极端多标签分类的双编码器

    Dual-Encoders for Extreme Multi-Label Classification

    [https://arxiv.org/abs/2310.10636](https://arxiv.org/abs/2310.10636)

    提出了一种解耦softmax损失的方法来克服现有对比损失在极端多标签分类任务上的局限性，并拓展到soft top-k 操作符。

    

    双编码器（DE）模型广泛用于检索任务，通常在开放式QA基准上进行研究，这些基准通常以多类和有限的训练数据为特征。相比之下，在像极端多标签分类（XMC）这样的多标签和数据丰富的检索设置中，它们的性能尚未得到充分探讨。当前的实证证据表明，DE模型在XMC基准上明显不足，其中SOTA方法通过使用每类分类头将可学习参数数量与总类别数（语料库中的文档）线性扩展。为此，我们首先研究并强调现有的多标签对比训练损失不适用于在XMC任务上训练DE模型。我们提出了解耦的softmax损失 - 这是对InfoNCE损失的简单修改，克服了现有对比损失的局限性。我们进一步将我们的损失设计扩展到一个软top-k操作符

    arXiv:2310.10636v2 Announce Type: replace  Abstract: Dual-encoder (DE) models are widely used in retrieval tasks, most commonly studied on open QA benchmarks that are often characterized by multi-class and limited training data. In contrast, their performance in multi-label and data-rich retrieval settings like extreme multi-label classification (XMC), remains under-explored. Current empirical evidence indicates that DE models fall significantly short on XMC benchmarks, where SOTA methods linearly scale the number of learnable parameters with the total number of classes (documents in the corpus) by employing per-class classification head. To this end, we first study and highlight that existing multi-label contrastive training losses are not appropriate for training DE models on XMC tasks. We propose decoupled softmax loss - a simple modification to the InfoNCE loss - that overcomes the limitations of existing contrastive losses. We further extend our loss design to a soft top-k operato
    
[^333]: Bongard-OpenWorld: 在真实世界中进行自由形式视觉概念的少样本推理

    Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World

    [https://arxiv.org/abs/2310.10207](https://arxiv.org/abs/2310.10207)

    Bongard-OpenWorld基准旨在评估机器视觉中对真实世界中的自由形式视觉概念进行少样本推理，并且提出了开放世界自由形式概念和真实世界图像两项新挑战。

    

    我们介绍了Bongard-OpenWorld，这是一个用于评估机器视觉中真实世界少样本推理的新基准。 它源自经典的Bongard问题（BPs）：给定两组图像（正和负），模型需要通过诱导视觉概念来确定查询图像所属的图像集，这些概念仅由正集中的图像所描述。 我们的基准继承了原始BPs的少样本概念归纳，同时增加了两层新挑战：1）开放世界的自由形式概念，因为Bongard-OpenWorld中的视觉概念是从开放词汇表中独特组合的术语，范围从对象类别到抽象视觉属性和常识事实知识； 2）真实世界的图像，而不是许多类似物使用的合成图表。在我们的探索中，Bongard-OpenWorld已经对当前的少样本推理算法提出了重大挑战。我们还远

    arXiv:2310.10207v2 Announce Type: replace  Abstract: We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We furt
    
[^334]: 面向下一代计算机视觉的基于脉冲的神经形态计算

    Spike-based Neuromorphic Computing for Next-Generation Computer Vision

    [https://arxiv.org/abs/2310.09692](https://arxiv.org/abs/2310.09692)

    基于脉冲的神经形态计算作为对今天主导视觉领域的深度卷积神经网络的可行替代，承诺能够实现数量级的能量效率提升。

    

    神经形态计算承诺在能量效率上比传统的冯·诺依曼计算范式有数量级的改进。其目标是通过学习和模拟大脑功能来开发一种自适应、容错、低占地面积、快速、低能耗的智能系统，可以通过不同抽象层次的创新来实现，包括材料、器件、电路、架构和算法。由于在复杂视觉任务中的能量消耗因数据集增大而呈指数级增长，而资源受限的边缘设备变得越来越普遍，基于脉冲的神经形态计算方法可以作为今天主导视觉领域的深度卷积神经网络的可行替代方案。在本书章节中，我们介绍了神经形态计算，概述了设计堆栈的不同层面（器件、电路和算法）中的一些代表性示例，并得出结论。

    arXiv:2310.09692v2 Announce Type: replace-cross  Abstract: Neuromorphic Computing promises orders of magnitude improvement in energy efficiency compared to traditional von Neumann computing paradigm. The goal is to develop an adaptive, fault-tolerant, low-footprint, fast, low-energy intelligent system by learning and emulating brain functionality which can be realized through innovation in different abstraction layers including material, device, circuit, architecture and algorithm. As the energy consumption in complex vision tasks keep increasing exponentially due to larger data set and resource-constrained edge devices become increasingly ubiquitous, spike-based neuromorphic computing approaches can be viable alternative to deep convolutional neural network that is dominating the vision field today. In this book chapter, we introduce neuromorphic computing, outline a few representative examples from different layers of the design stack (devices, circuits and algorithms) and conclude w
    
[^335]: 用自动微分增强数据驱动动态建模中的预测能力：Koopman和神经ODE方法

    Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches

    [https://arxiv.org/abs/2310.06790](https://arxiv.org/abs/2310.06790)

    本文介绍了一种改进的EDMD-DL方法，利用自动微分同时确定可观测字典和Koopman算子的近似值，并评估了其他替代方法的性能。

    

    数据驱动的Koopman算子逼近在预测具有复杂动态特征的系统的时间演化方面具有很大潜力。在这些方法中，被称为扩展动态模式分解与字典学习（EDMD-DL）的方法引起了很大关注。本文介绍了一种修改的EDMD-DL方法，同时确定可观测字典和对应的Koopman算子的近似值。该创新利用自动微分来通过伪逆简化梯度下降计算。我们还讨论了几种替代方法的性能。我们评估了一个“纯”Koopman方法，它涉及在可观测空间内管理动态的线性、高维系统的直接时间积分。此外，我们探讨了一种修改的方法，其中系统在状态空间和可观测空间之间交替。

    arXiv:2310.06790v2 Announce Type: replace  Abstract: Data-driven approximations of the Koopman operator are promising for predicting the time evolution of systems characterized by complex dynamics. Among these methods, the approach known as extended dynamic mode decomposition with dictionary learning (EDMD-DL) has garnered significant attention. Here we present a modification of EDMD-DL that concurrently determines both the dictionary of observables and the corresponding approximation of the Koopman operator. This innovation leverages automatic differentiation to facilitate gradient descent computations through the pseudoinverse. We also address the performance of several alternative methodologies. We assess a 'pure' Koopman approach, which involves the direct time-integration of a linear, high-dimensional system governing the dynamics within the space of observables. Additionally, we explore a modified approach where the system alternates between spaces of states and observables at ea
    
[^336]: Robust-GBDT: 在标签噪声和类别不平衡情况下用非凸损失进行表格分类的GBDT

    Robust-GBDT: GBDT with Nonconvex Loss for Tabular Classification in the Presence of Label Noise and Class Imbalance

    [https://arxiv.org/abs/2310.05067](https://arxiv.org/abs/2310.05067)

    Robust-GBDT结合了梯度提升决策树（GBDT）和非凸损失函数，在处理标签噪声和类别不平衡方面展现出了前所未有的稳健性和泛化能力。

    

    处理表格分类任务中的标签噪声是机器学习中持久的挑战。虽然强化提升方法在二元分类中已经显示出潜力，但在复杂的多类情况下，它们的有效性通常有限。此外，不平衡数据集、缺失值和计算效率等问题进一步复杂化了它们的实际效用。本研究引入了Robust-GBDT，这是一种开创性方法，它将梯度提升决策树（GBDT）的强大能力与非凸损失函数对抗标签噪声的韧性相结合。通过利用特定区域内的局部凸性，Robust-GBDT展示了前所未有的稳健性，挑战了传统智慧。通过将先进的GBDT与针对类别不平衡量身定制的新型Robust Focal Loss进行无缝集成，Robust-GBDT显著增强了泛化能力，特别是在嘈杂和不平衡的数据集中。

    arXiv:2310.05067v2 Announce Type: replace  Abstract: Dealing with label noise in tabular classification tasks poses a persistent challenge in machine learning. While robust boosting methods have shown promise in binary classification, their effectiveness in complex, multi-class scenarios is often limited. Additionally, issues like imbalanced datasets, missing values, and computational inefficiencies further complicate their practical utility. This study introduces Robust-GBDT, a groundbreaking approach that combines the power of Gradient Boosted Decision Trees (GBDT) with the resilience of nonconvex loss functions against label noise. By leveraging local convexity within specific regions, Robust-GBDT demonstrates unprecedented robustness, challenging conventional wisdom. Through seamless integration of advanced GBDT with a novel Robust Focal Loss tailored for class imbalance, Robust-GBDT significantly enhances generalization capabilities, particularly in noisy and imbalanced datasets. 
    
[^337]: 理解多模态对比学习对分布转移的鲁棒性

    Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift

    [https://arxiv.org/abs/2310.04971](https://arxiv.org/abs/2310.04971)

    本研究发现多模态对比学习对抗分布转移具有鲁棒性的两种机制，即类内对比和类间特征共享，这有助于防止训练数据中的虚假特征干扰核心特征，从而在零样本分类准确度方面表现优越。

    

    最近，诸如CLIP之类的多模态对比学习（MMCL）方法在学习对抗分布转移且推广到新领域的表示方面取得了显著成功。尽管在实证方面取得了成功，但学习这种具有泛化性的表示背后的机理尚不清楚。在这项工作中，我们对这一问题进行了严格分析，并发现MMCL鲁棒性背后的两种机制：\emph{类内对比}，使模型能够学习具有高方差的特征；\emph{类间特征共享}，其中一个类别中的注释细节有助于更好地学习其他类别。这两种机制防止了训练数据中过度表示的虚假特征掩盖泛化性核心特征。在分布转移下，这产生了更优秀的零样本分类准确度。此外，我们从理论上证明了在鲁棒性方面使用丰富字幕的好处。

    arXiv:2310.04971v2 Announce Type: replace  Abstract: Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and uncover two mechanisms behind MMCL's robustness: \emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness a
    
[^338]: Knolling Bot: 从整洁的示范中学习机器人对象排列

    Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations

    [https://arxiv.org/abs/2310.04566](https://arxiv.org/abs/2310.04566)

    本论文介绍了一种自监督学习框架，利用Transformer神经网络使机器人能够从整齐排列的示范中理解和复制整洁的概念，从而实现整理物品的功能。

    

    地址：arXiv:2310.04566v2  公告类型：replace-cross  摘要：解决家庭空间中散乱物品的整理挑战受到整洁性的多样性和主观性的复杂性影响。正如人类语言的复杂性允许同一理念的多种表达一样，家庭整洁偏好和组织模式变化广泛，因此预设物体位置将限制对新物体和环境的适应性。受自然语言处理（NLP）的进展启发，本文引入一种自监督学习框架，使机器人能够从整洁布局的示范中理解和复制整洁的概念，类似于使用会话数据集训练大语言模型（LLM）。我们利用一个Transformer神经网络来预测后续物体的摆放位置。我们展示了一个“整理”系统，利用机械臂和RGB相机在桌子上组织不同大小和数量的物品。

    arXiv:2310.04566v2 Announce Type: replace-cross  Abstract: Addressing the challenge of organizing scattered items in domestic spaces is complicated by the diversity and subjective nature of tidiness. Just as the complexity of human language allows for multiple expressions of the same idea, household tidiness preferences and organizational patterns vary widely, so presetting object locations would limit the adaptability to new objects and environments. Inspired by advancements in natural language processing (NLP), this paper introduces a self-supervised learning framework that allows robots to understand and replicate the concept of tidiness from demonstrations of well-organized layouts, akin to using conversational datasets to train Large Language Models(LLM). We leverage a transformer neural network to predict the placement of subsequent objects. We demonstrate a ``knolling'' system with a robotic arm and an RGB camera to organize items of varying sizes and quantities on a table. Our 
    
[^339]: 压缩LLM：真相很少纯粹，也绝不简单

    Compressing LLMs: The Truth is Rarely Pure and Never Simple

    [https://arxiv.org/abs/2310.01382](https://arxiv.org/abs/2310.01382)

    本研究重新评估了现有最先进的压缩LLM方法对稠密LLM的有效性，并引入了一个新的压缩LLM基准来重新定义评估协议。

    

    尽管现代大型语言模型（LLMs）取得了显著成就，但面临着巨大的计算和内存占用。 最近，几项工作显示出在无需训练和数据的情况下对LLMs进行压缩（修剪和量化）取得了显著成功，达到了50-60％的稀疏度，并将位宽减小到每个权重3或4位，并且与未压缩基线相比，困惑度的降低可以忽略不计。 随着最近研究工作集中在开发越来越复杂的压缩方法上，我们的工作退一步重新评估了现有SoTA压缩方法的有效性，这些方法依赖于一种相当简单且广受质疑的度量标准，困惑度（即使对于稠密的LLMs）。 我们引入了知识密集型压缩的LLM基准（LLM-KICK），这是一个精心策划的任务集合，用于重新定义对压缩LLMs的评估协议，这些LLMs与其稠密对应物有显著的对齐性，

    arXiv:2310.01382v2 Announce Type: replace  Abstract: Despite their remarkable achievements, modern Large Language Models (LLMs) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to redefine the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts and 
    
[^340]: 深度神经网络倾向于可预测的外推

    Deep Neural Networks Tend To Extrapolate Predictably

    [https://arxiv.org/abs/2310.00873](https://arxiv.org/abs/2310.00873)

    我们的工作表明，与传统观点不同，在面对超出分布范围的输入时，高维输入的神经网络预测往往会趋向于一个恒定值，且这个值通常能够接近最优恒定解。

    

    传统观点认为，神经网络在面对超出分布范围（OOD）的输入时，预测往往是不可预测和过于自信的。我们的工作重新评估了具有高维输入的神经网络对这一假设的情况。我们观察到，与以往认为的任意外推不同，神经网络的预测往往在输入数据变得越来越OOD时趋向于一个恒定值。此外，我们发现这个值往往能够紧密地逼近最优恒定解（OCS），即在不观察输入的情况下最小化训练数据上的平均损失的预测。我们展示了在包括CIFAR10-C、ImageNet-R、S在内的8个数据集上以及不同分布转移（包括交叉熵、MSE和高斯NLL）和不同架构（CNN和变压器）下出现这种现象的结果。此外，我们对这种行为提出了一个解释，并首次验证了它。

    arXiv:2310.00873v2 Announce Type: replace  Abstract: Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate e
    
[^341]: 分析和减轻大型视觉语言模型中的物体幻觉

    Analyzing and Mitigating Object Hallucination in Large Vision-Language Models

    [https://arxiv.org/abs/2310.00754](https://arxiv.org/abs/2310.00754)

    提出了一种名为LVLM Hallucination Revisor（LURE）的算法，通过重新构建较少具有幻觉性的描述，来事后纠正大型视觉语言模型中的物体幻觉问题。

    

    大型视觉语言模型（LVLMs）在理解图像信息和人类语言方面显示出卓越的能力。然而，LVLMs 仍然存在物体幻觉问题，即生成包含图像中实际不存在的对象的描述。这可能对许多视觉语言任务产生负面影响，如视觉总结和推理。为解决这一问题，我们提出了一种简单而强大的算法，LVLM 幻觉修正器（LURE），通过重构较少具有幻觉性的描述来事后纠正LVLM 中的物体幻觉。LURE根据对导致物体幻觉的关键因素的严格统计分析，包括共现（图像中某些对象经常与其他对象一起出现）、不确定性（在LVLM解码过程中不确定性较高的对象）和对象位置（幻觉通常出现在生成描述的后部分）。

    arXiv:2310.00754v2 Announce Type: replace-cross  Abstract: Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the gen
    
[^342]: 能够检测到LLM生成的虚假信息吗?

    Can LLM-Generated Misinformation Be Detected?

    [https://arxiv.org/abs/2309.13788](https://arxiv.org/abs/2309.13788)

    LLM生成的虚假信息可能比人类撰写的虚假信息更难以检测，具有更具欺骗性的风格，可能造成更多危害。

    

    大型语言模型（LLMs）的出现产生了深远影响。然而，LLMs（如ChatGPT）可能被利用来生成虚假信息，这给在线安全和公众信任带来了严重关切。一个基本的研究问题是：LLM生成的虚假信息是否会比人类撰写的虚假信息造成更大危害?我们提出从检测难度的角度来探讨这个问题。我们首先建立了一个LLM生成的虚假信息分类法。然后，我们对利用LLMs生成虚假信息的潜在真实世界方法进行分类和验证。通过广泛的实证调查，我们发现与具有相同语义的人类撰写的虚假信息相比，LLM生成的虚假信息对人类和检测器来说更难检测，这表明它可能具有更具欺骗性的风格，潜在地造成更多危害。我们还讨论了我们发现的影响。

    arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
    
[^343]: 生成对抗网络的统一生成器损失函数

    A Unifying Generator Loss Function for Generative Adversarial Networks

    [https://arxiv.org/abs/2308.07233](https://arxiv.org/abs/2308.07233)

    引入了一个统一的生成器损失函数，称为$\mathcal{L}_\alpha$-GAN，通过最小化Jensen-$f_\alpha$-散度来优化生成器，可以恢复出文献中的多个GAN问题。

    

    引入了一个统一的$\alpha$参数化生成器损失函数，用于双目标生成对抗网络（GAN），该网络使用经典鉴别器损失函数，如原始GAN（VanillaGAN）系统中的损失函数。生成器损失函数基于对称类概率估计类型函数$\mathcal{L}_\alpha$，得到的GAN系统被称为$\mathcal{L}_\alpha$-GAN。在最佳鉴别器的情况下，表明生成器的优化问题包括最小化Jensen-$f_\alpha$-散度，这是Jensen-Shannon散度的自然泛化，其中$f_\alpha$是关于损失函数$\mathcal{L}_\alpha$的凸函数。还证明了这个$\mathcal{L}_\alpha$-GAN问题恢复了文献中一些GAN问题作为特殊情况，包括VanillaGAN、最小二乘GAN（LSGAN）、最小$k$阶GAN（L

    arXiv:2308.07233v2 Announce Type: replace  Abstract: A unifying $\alpha$-parametrized generator loss function is introduced for a dual-objective generative adversarial network (GAN), which uses a canonical (or classical) discriminator loss function such as the one in the original GAN (VanillaGAN) system. The generator loss function is based on a symmetric class probability estimation type function, $\mathcal{L}_\alpha$, and the resulting GAN system is termed $\mathcal{L}_\alpha$-GAN. Under an optimal discriminator, it is shown that the generator's optimization problem consists of minimizing a Jensen-$f_\alpha$-divergence, a natural generalization of the Jensen-Shannon divergence, where $f_\alpha$ is a convex function expressed in terms of the loss function $\mathcal{L}_\alpha$. It is also demonstrated that this $\mathcal{L}_\alpha$-GAN problem recovers as special cases a number of GAN problems in the literature, including VanillaGAN, Least Squares GAN (LSGAN), Least $k$th order GAN (L$
    
[^344]: 在具有可数无限状态空间的马尔可夫决策过程中的贝叶斯学习最优策略

    Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space

    [https://arxiv.org/abs/2306.02574](https://arxiv.org/abs/2306.02574)

    本文研究了马尔可夫决策过程中具有可数无限状态空间的最优控制问题，提出了基于汤普森采样和动态大小片段的算法进行贝叶斯学习，解决了在这些模型上的挑战。

    

    很多现实应用的模型，如通信网络或计算系统的排队模型，都具有可数无限状态空间。目前已经开发的算法和学习过程主要针对有限状态设置，并不能直接应用于这些模型。为了解决这个问题，本文研究了在一个未知参数θ∈Θ控制下的家族离散时间可数状态空间马尔可夫决策过程(MDP)的最优控制问题，该过程定义在可数无限状态空间Xυ={Z+}d上，具有有限动作空间Aυ以及无界成本函数。我们采用贝叶斯观点，将随机未知参数θ*作为给定先验分布在Θ上生成。为了最优地控制未知的MDP，我们提出了一种基于汤普森采样和动态大小片段的算法：在每个片段的开始，根据后验概率分布选择动作，并在每个片段结束时更新后验概率分布。

    Models of many real-life applications, such as queuing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter $\theta\in\Theta$, and defined on a countably-infinite state space $\mathcal X=\mathbb{Z}_+^d$, with finite action space $\mathcal A$, and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter $\boldsymbol{\theta}^*$ generated via a given fixed prior distribution on $\Theta$. To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior
    
[^345]: 可证明且实用：通过Langevin Monte Carlo在强化学习中的有效探索

    Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo

    [https://arxiv.org/abs/2305.18246](https://arxiv.org/abs/2305.18246)

    通过Langevin Monte Carlo直接采样Q函数后验分布，避免了高斯逼近，实现了在强化学习中高效的探索策略。

    

    我们提出了一种基于Thompson采样的可伸缩且有效的强化学习探索策略。现有Thompson采样算法的一个关键缺点是需要对后验分布进行高斯逼近，在大多数实际场景中并不是一个很好的替代方法。我们使用Langevin Monte Carlo直接从后验分布中采样Q函数，这是一种高效的马尔可夫链蒙特卡洛(MCMC)方法。我们的方法只需执行带噪梯度下降更新就可以学习Q函数的精确后验分布，这使得我们的方法在深度强化学习中易于部署。我们为所提出的方法进行了严格的理论分析，并证明在线性马尔可夫决策过程(linear MDP)设置中，它的遗憾界约为$\tilde{O}(d^{3/2}H^{3/2}\sqrt{T})$，其中 $d$ 是特征映射的维度，$H$ 是计划的长度。

    arXiv:2305.18246v2 Announce Type: replace  Abstract: We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using Langevin Monte Carlo, an efficient type of Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep RL. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear Markov decision process (linear MDP) setting, it has a regret bound of $\tilde{O}(d^{3/2}H^{3/2}\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the plannin
    
[^346]: 用双期望分位回归的分布式强化学习

    Distributional Reinforcement Learning with Dual Expectile-Quantile Regression

    [https://arxiv.org/abs/2305.16877](https://arxiv.org/abs/2305.16877)

    提出了一种用双期望分位回归的分布式强化学习方法，能够更高效地学习任意回报分布

    

    分布式强化学习（RL）已经在多个基准测试中证明其有效性，因为它可以近似整个回报分布，并更好地利用环境样本。常用的基于不对称$L_1$损失的分布式RL的分位回归方法提供了一种灵活而有效的学习任意回报分布的方式。在实践中，通过使用更高效的混合不对称$L_1$-$L_2$ Huber损失来改进往往会提高性能。然而，通过这样做，分布估计保证消失了，我们实证观察到估计的分布会迅速收敛到其均值。事实上，与期望回归相对应的不对称$L_2$损失不能直接用于分布式时序差异学习。受到$L_2$为基础学习效率的启发，我们提出了联合学习回报分布的期望值和分位数的方法。

    arXiv:2305.16877v2 Announce Type: replace-cross  Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and makes a better use of environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, hybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference learning. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution
    
[^347]: Hang-Time HAR：一种使用腕部惯性传感器进行篮球活动识别的基准数据集

    Hang-Time HAR: A Benchmark Dataset for Basketball Activity Recognition using Wrist-Worn Inertial Sensors

    [https://arxiv.org/abs/2305.13124](https://arxiv.org/abs/2305.13124)

    提出了一个用于篮球活动识别的基准数据集，记录了来自美国和德国两支篮球队的24名球员在训练和比赛中佩戴腕部惯性传感器的数据，具有不同国家文化差异和参与者篮球经验异质性等特点

    

    我们提出了一个基准数据集，用于评估腕部传感器测量的体育人体活动识别方法，具体应用于篮球训练、练习和比赛。篮球活动非常适合由腕部惯性传感器进行测量，能够检测到这种与运动相关的活动的系统可应用于比赛分析、指导训练和个人体育活动跟踪。该数据集记录了来自两个不同国家（美国和德国）的两支团队共24名运动员，在重复的篮球训练课和完整比赛期间他们佩戴了腕部惯性传感器。这个数据集的特点包括数据在两个国家记录，由于文化差异在游戏规则和风格中固有的差异，以及不同的运动技能水平，因为参与者在篮球经验方面是异质的。

    arXiv:2305.13124v2 Announce Type: replace  Abstract: We present a benchmark dataset for evaluating physical human activity recognition methods from wrist-worn sensors, for the specific setting of basketball training, drills, and games. Basketball activities lend themselves well for measurement by wrist-worn inertial sensors, and systems that are able to detect such sport-relevant activities could be used in applications toward game analysis, guided training, and personal physical activity tracking. The dataset was recorded for two teams from separate countries (USA and Germany) with a total of 24 players who wore an inertial sensor on their wrist, during both repetitive basketball training sessions and full games. Particular features of this dataset include an inherent variance through cultural differences in game rules and styles as the data was recorded in two countries, as well as different sport skill levels, since the participants were heterogeneous in terms of prior basketball ex
    
[^348]: 有限数据训练基于似然的生成模型的分阶段数据增强

    Phased Data Augmentation for Training a Likelihood-Based Generative Model with Limited Data

    [https://arxiv.org/abs/2305.12681](https://arxiv.org/abs/2305.12681)

    分阶段数据增强是一种新颖技术，能够优化有限数据训练生成模型的方法，通过限制增强强度，提高了模型学习能力，保持了模型的保真度，在整合PixelCNNs与VQ-VAE-2的模型中取得了卓越性能。

    

    生成模型在创建逼真图像方面表现出色，然而它们对于训练大量数据集的依赖在数据采集成本高昂或困难的领域中提出了重大挑战。当前的数据高效方法主要专注于GAN架构，导致其他类型的生成模型训练存在空白。我们的研究引入了“分阶段数据增强”作为一种新颖技术，通过在有限数据情景下优化训练，而不改变固有数据分布，来填补这一空白。通过在学习阶段限制增强强度，我们的方法增强了模型从有限数据中学习的能力，从而保持了保真度。将其应用于将PixelCNNs与VQ-VAE-2集成的模型时，我们的方法在各种数据集上的定量和定性评估中展示了优越性能。这代表了有效训练中的重要进展。

    arXiv:2305.12681v2 Announce Type: replace-cross  Abstract: Generative models excel in creating realistic images, yet their dependency on extensive datasets for training presents significant challenges, especially in domains where data collection is costly or challenging. Current data-efficient methods largely focus on GAN architectures, leaving a gap in training other types of generative models. Our study introduces "phased data augmentation" as a novel technique that addresses this gap by optimizing training in limited data scenarios without altering the inherent data distribution. By limiting the augmentation intensity throughout the learning phases, our method enhances the model's ability to learn from limited data, thus maintaining fidelity. Applied to a model integrating PixelCNNs with VQ-VAE-2, our approach demonstrates superior performance in both quantitative and qualitative evaluations across diverse datasets. This represents an important step forward in the efficient training
    
[^349]: BCQQ: 循环数据重新上传的批量约束量子 Q 学习

    BCQQ: Batch-Constraint Quantum Q-Learning with Cyclic Data Re-uploading

    [https://arxiv.org/abs/2305.00905](https://arxiv.org/abs/2305.00905)

    本文提出了一种 BCQ 算法，将 VQC 作为函数逼近器，并通过循环移位数据编码层中输入变量的顺序来进行全新的数据重新上传方案，以提高批量 RL 的效率。

    

    深度强化学习(DRL)通常需要大量的数据和环境交互，使得训练过程耗时。这一挑战在批量RL的情况下进一步恶化，其中代理只在一个预先收集的数据集上进行训练，而不涉及环境交互。量子计算的最新进展表明，与经典方法相比，量子模型可能需要更少的数据进行训练。本文通过提出一个利用VQC作为函数逼近器的批量RL算法，在离散批量约束深度 Q 学习(BCQ)算法中探讨了这一潜在优势。此外，我们通过循环移位数据编码层中输入变量的顺序，引入了一种新颖的数据重新上传方案。我们在OpenAI CartPole环境中评估了我们算法的效率，并将其性能与基于经典神经网络的离散BC进行了比较。

    arXiv:2305.00905v2 Announce Type: replace-cross  Abstract: Deep reinforcement learning (DRL) often requires a large number of data and environment interactions, making the training process time-consuming. This challenge is further exacerbated in the case of batch RL, where the agent is trained solely on a pre-collected dataset without environment interactions. Recent advancements in quantum computing suggest that quantum models might require less data for training compared to classical methods. In this paper, we investigate this potential advantage by proposing a batch RL algorithm that utilizes VQC as function approximators within the discrete batch-constraint deep Q-learning (BCQ) algorithm. Additionally, we introduce a novel data re-uploading scheme by cyclically shifting the order of input variables in the data encoding layers. We evaluate the efficiency of our algorithm on the OpenAI CartPole environment and compare its performance to the classical neural network-based discrete BC
    
[^350]: Tangent Bundle Convolutional Learning: 从流形到细胞层状结构再回去

    Tangent Bundle Convolutional Learning: from Manifolds to Cellular Sheaves and Back

    [https://arxiv.org/abs/2303.11323](https://arxiv.org/abs/2303.11323)

    本文介绍了一种在黎曼流形的切丛上进行卷积操作的方法，并基于此定义了切丛滤波器和切丛神经网络（TNNs），这为连续架构提供了新颖的操作方式，最后证明了离散化后的架构收敛于连续架构。

    

    在这项工作中，我们介绍了一种在黎曼流形的切丛上进行卷积操作的方法，这种方法是使用连接拉普拉斯算子的指数定义的。我们基于这种卷积操作定义了切丛滤波器和切丛神经网络（TNNs），这些是在切丛信号上操作的新型连续架构，即流形上的矢量场。切丛滤波器具有广义的谱表示，推广了标量流形滤波器、图滤波器和连续时间标准卷积滤波器的表示。然后我们介绍了一种离散化过程，涉及空间和时间域，使得TNNs可实现，并展示了它们的离散对应物是最近介绍的层神经网络的一个新的合理变种。我们正式证明这种离散化架构收敛于基础的连续TNN。最后，我们对这种方法的有效性进行了数值评估。

    arXiv:2303.11323v2 Announce Type: replace-cross  Abstract: In this work we introduce a convolution operation over the tangent bundle of Riemann manifolds in terms of exponentials of the Connection Laplacian operator. We define tangent bundle filters and tangent bundle neural networks (TNNs) based on this convolution operation, which are novel continuous architectures operating on tangent bundle signals, i.e. vector fields over the manifolds. Tangent bundle filters admit a spectral representation that generalizes the ones of scalar manifold filters, graph filters and standard convolutional filters in continuous time. We then introduce a discretization procedure, both in the space and time domains, to make TNNs implementable, showing that their discrete counterpart is a novel principled variant of the very recently introduced sheaf neural networks. We formally prove that this discretized architecture converges to the underlying continuous TNN. Finally, we numerically evaluate the effecti
    
[^351]: EHRDiff: 使用扩散模型探索真实的电子健康记录综合

    EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models

    [https://arxiv.org/abs/2303.05656](https://arxiv.org/abs/2303.05656)

    该研究探索了使用扩散模型在电子健康记录（EHR）数据合成方面的潜力，挑战传统基于生成对抗网络（GAN）的方法。

    

    电子健康记录（EHR）包含丰富的生物医学信息，是发展精准医学系统的宝贵资源。然而，隐私担忧导致研究人员获取高质量和大规模EHR数据的限制，阻碍了方法研发的进展。最近的研究深入探讨了通过生成建模技术合成真实的EHR数据，其中大多数提出的方法依赖生成对抗网络（GAN）及其变体用于EHR综合。尽管基于GAN的方法在生成EHR数据方面取得了最先进的性能，但这些方法难以训练且容易出现模式坍塌。扩散模型是生成建模中最新引入的，已经在图像生成方面取得了尖端性能，但它们在EHR数据综合方面的效力仍未得到充分探索。本研究中，我们探讨了d

    arXiv:2303.05656v2 Announce Type: replace  Abstract: Electronic health records (EHR) contain a wealth of biomedical information, serving as valuable resources for the development of precision medicine systems. However, privacy concerns have resulted in limited access to high-quality and large-scale EHR data for researchers, impeding progress in methodological development. Recent research has delved into synthesizing realistic EHR data through generative modeling techniques, where a majority of proposed methods relied on generative adversarial networks (GAN) and their variants for EHR synthesis. Despite GAN-based methods attaining state-of-the-art performance in generating EHR data, these approaches are difficult to train and prone to mode collapse. Recently introduced in generative modeling, diffusion models have established cutting-edge performance in image generation, but their efficacy in EHR data synthesis remains largely unexplored. In this study, we investigate the potential of d
    
[^352]: 用于流式自监督语音表示学习的低延迟注意力模块

    A low latency attention module for streaming self-supervised speech representation learning

    [https://arxiv.org/abs/2302.13451](https://arxiv.org/abs/2302.13451)

    本文提出了用于流式自监督语音表示学习的低延迟注意力模块，实现了在低延迟的情况下进行实时推断。

    

    transformer是深度学习中的基本构建模块，注意力机制是transformer的核心组件。自监督语音表示学习（SSRL）是transformer架构的一个流行用例。由于transformer的非因果行为，对于SSRL的transformer的使用主要集中在非因果应用上。然而，一些媒体处理问题，如语音处理，需要实时解决方案。在本文中，我们提出了一个注意力模块的实现，该模块可以通过较低的计算和内存需求训练SSRL架构，并允许在低固定延迟下进行实时推断。本文提出的注意力模块包括两个组件，分别是流式注意力（SA）和低延迟流式注意力（LLSA）。

    arXiv:2302.13451v2 Announce Type: replace-cross  Abstract: The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the late
    
[^353]: 基于Copula的可转移模型用于合成人口生成

    Copula-based transferable models for synthetic population generation

    [https://arxiv.org/abs/2302.09193](https://arxiv.org/abs/2302.09193)

    提出了一种基于Copula的新框架，利用不同人口样本以及相似边际依赖性，引入空间组件并考虑多种信息源，用于生成合成但现实的目标人口表示。

    

    人口综合涉及生成微观代理目标人口的合成但现实的表示，用于行为建模和模拟。 传统方法通常依赖于目标人口样本，如人口普查数据或旅行调查，由于高成本和较小的样本量，在较小的地理尺度上存在局限性。 我们提出了一种基于Copula的新框架，用于为仅已知经验边际分布的目标人口生成合成数据。 该方法利用来自具有相似边际依赖性的不同人口的样本，将空间组件引入到人口综合中，并考虑各种信息源用于更真实的生成器。 具体而言，该过程涉及将数据标准化并将其视为给定Copula的实现，然后在融入关于

    arXiv:2302.09193v2 Announce Type: replace-cross  Abstract: Population synthesis involves generating synthetic yet realistic representations of a target population of micro-agents for behavioral modeling and simulation. Traditional methods, often reliant on target population samples, such as census data or travel surveys, face limitations due to high costs and small sample sizes, particularly at smaller geographical scales. We propose a novel framework based on copulas to generate synthetic data for target populations where only empirical marginal distributions are known. This method utilizes samples from different populations with similar marginal dependencies, introduces a spatial component into population synthesis, and considers various information sources for more realistic generators. Concretely, the process involves normalizing the data and treat it as realizations of a given copula, and then training a generative model before incorporating the information on the marginals of the
    
[^354]: 用深度学习预测鱼对的长期集体行为

    Predicting the long-term collective behaviour of fish pairs with deep learning

    [https://arxiv.org/abs/2302.06839](https://arxiv.org/abs/2302.06839)

    该研究通过引入深度学习模型对Hemigrammus rhodostomus鱼类的社会互动进行评估，并展示了机器学习模型在再现实验可观测量方面与分析模型的竞争力，强调了跨不同时间尺度进行一致验证的必要性。

    

    现代计算增强了我们对社会相互作用如何塑造动物社会集体行为的理解。尽管分析模型在研究集体行为中占主导地位，但这项研究引入了一个深度学习模型来评估Hemigrammus rhodostomus鱼类的社会互动。我们将深度学习方法的结果与实验结果以及最先进的分析模型的结果进行比较。为此，我们提出了一种系统方法来评估集体运动模型的准确性，利用一组严格的个体和集体时空可观测量。我们证明了社会互动的机器学习模型可以直接与其分析对手竞争，以重现微妙的实验可观测量。此外，这项工作强调了在不同时间尺度上进行一致验证的必要性，并确定了使我们的深度学习模型具有竞争力的关键设计方面。

    arXiv:2302.06839v2 Announce Type: replace  Abstract: Modern computing has enhanced our understanding of how social interactions shape collective behaviour in animal societies. Although analytical models dominate in studying collective behaviour, this study introduces a deep learning model to assess social interactions in the fish species Hemigrammus rhodostomus. We compare the results of our deep learning approach to experiments and to the results of a state-of-the-art analytical model. To that end, we propose a systematic methodology to assess the faithfulness of a collective motion model, exploiting a set of stringent individual and collective spatio-temporal observables. We demonstrate that machine learning models of social interactions can directly compete with their analytical counterparts in reproducing subtle experimental observables. Moreover, this work emphasises the need for consistent validation across different timescales, and identifies key design aspects that enable our d
    
[^355]: 通过领域关系改进领域泛化

    Improving Domain Generalization with Domain Relations

    [https://arxiv.org/abs/2302.02609](https://arxiv.org/abs/2302.02609)

    通过利用领域关系对训练领域特定函数重新加权，D$^3$G方法能够改进领域泛化能力。

    

    分布转移在机器学习中构成了一个重要挑战，当模型在测试阶段面对不同于训练时的分布时往往表现不佳。本文关注领域转移，即当模型被应用于与其训练时不同的新领域时发生的情况，并提出了一种新方法称为D$^3$G。与先前旨在学习一个领域不变模型的方法不同，D$^3$G利用基于领域元数据的领域相似性来学习特定领域的模型。具体来说，D$^3$G在训练阶段学习一组特定于训练领域的函数，并在测试阶段基于领域关系对其进行重新加权。这些领域关系可以直接从领域元数据中获得和学习。在温和的假设条件下，我们在理论上证明了利用领域关系对训练领域特定函数进行重新加权可以取得强。。。（待续）

    arXiv:2302.02609v2 Announce Type: replace  Abstract: Distribution shift presents a significant challenge in machine learning, where models often underperform during the test stage when faced with a different distribution than the one they were trained on. This paper focuses on domain shifts, which occur when the model is applied to new domains that are different from the ones it was trained on, and propose a new approach called D$^3$G. Unlike previous methods that aim to learn a single model that is domain invariant, D$^3$G leverages domain similarities based on domain metadata to learn domain-specific models. Concretely, D$^3$G learns a set of training-domain-specific functions during the training stage and reweights them based on domain relations during the test stage. These domain relations can be directly obtained and learned from domain metadata. Under mild assumptions, we theoretically prove that using domain relations to reweight training-domain-specific functions achieves stron
    
[^356]: 掩码语言建模中的表示不足

    Representation Deficiency in Masked Language Modeling

    [https://arxiv.org/abs/2302.02060](https://arxiv.org/abs/2302.02060)

    掩码语言建模中的 $\texttt{[MASK]} $符号会导致模型维度过度分配，造成真实标记的表示不足，本文提出了MAE-LM来解决这一问题

    

    掩码语言建模（MLM）已经成为双向文本编码器预训练最突出的方法之一，因为它简单而有效。关于MLM的一个显著问题是特殊的 $\texttt{[MASK]}$ 符号会导致预训练数据和下游数据之间存在差异，因为它只出现在预训练中而不出现在微调中。在这项工作中，我们提供了一个新的视角，探讨了这种差异的后果：我们在理论和实践上证明了MLM预训练专门分配了一些模型维度来表示 $\texttt{[MASK]}$ 标记，导致真实标记的表示不足，并在没有 $\texttt{[MASK]}$ 标记的情况下，限制了预训练模型在适应下游数据时的表达能力。受到识别问题的启发，我们提出了MAE-LM，该方法利用MLM对掩码自动编码器进行预训练，其中排除了 $\texttt{[MASK]}$ 标记。

    arXiv:2302.02060v2 Announce Type: replace  Abstract: Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where $\texttt{[MASK]}$ tokens are excluded from the
    
[^357]: 可证明边界神经网络前像

    Provably Bounding Neural Network Preimages

    [https://arxiv.org/abs/2302.01404](https://arxiv.org/abs/2302.01404)

    提出了INVPROP算法用于验证神经网络输出集的前像上的属性，结合分支界限以增加精度，并且实现了GPU加速，避免了线性规划求解器的需求。

    

    大部分关于神经网络的形式验证工作侧重于限定给定输入集对应的输出集（例如，标准输入的有界扰动）。但是，神经网络验证的许多应用情景需要解决逆问题，或者对导致特定输出的输入集进行过度近似。我们提出了INVPROP算法，用于验证在线性约束输出集的前像上的属性，可以与分支界限结合以增加精度。与其他方法相反，我们的高效算法是GPU加速的，并且不需要线性规划求解器。我们展示了我们的算法用于通过后向可达性分析识别动态系统的安全控制区域，验证对抗鲁棒性，并检测神经网络的超出分布的输入。我们的结果表明，在某些情况下，我们找到了过渡

    arXiv:2302.01404v4 Announce Type: replace-cross  Abstract: Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-a
    
[^358]: Curriculum Learning用于从头开始的深度学习折射光学

    Curriculum Learning for ab initio Deep Learned Refractive Optics

    [https://arxiv.org/abs/2302.01089](https://arxiv.org/abs/2302.01089)

    通过Curriculum Learning，提出了一种DeepLens设计方法，能够从随机初始化表面从头开始学习复合透镜的光学设计，克服了对良好初始设计的需求，并实现了自动设计经典成像透镜和大视场延伸景深计算透镜。

    

    深度光学优化最近被提出作为使用输出图像作为目标的计算成像系统设计的新范式。然而，它目前被限制于单个元素（如衍射光学元件（DOE）或金属透镜）构成的简单光学系统，或者从良好的初始设计微调复合透镜。在这里，我们提出了一种基于Curriculum Learning的DeepLens设计方法，能够从随机初始化表面从头开始学习复合透镜的光学设计，而无需人为干预，因此克服了对良好初始设计的需求。我们通过完全自动设计经典成像透镜和一个类似手机风格的大视场延伸景深计算透镜，验证了我们方法的有效性，具有高度非球面曲面和短后焦距。

    arXiv:2302.01089v3 Announce Type: replace-cross  Abstract: Deep optical optimization has recently emerged as a new paradigm for designing computational imaging systems using only the output image as the objective. However, it has been limited to either simple optical systems consisting of a single element such as a diffractive optical element (DOE) or metalens, or the fine-tuning of compound lenses from good initial designs. Here we present a DeepLens design method based on curriculum learning, which is able to learn optical designs of compound lenses ab initio from randomly initialized surfaces without human intervention, therefore overcoming the need for a good initial design. We demonstrate the effectiveness of our approach by fully automatically designing both classical imaging lenses and a large field-of-view extended depth-of-field computational lens in a cellphone-style form factor, with highly aspheric surfaces and a short back focal length.
    
[^359]: 无偏见的视觉推荐系统：开放挑战与未来方向

    Agnostic Visual Recommendation Systems: Open Challenges and Future Directions

    [https://arxiv.org/abs/2302.00569](https://arxiv.org/abs/2302.00569)

    本文讨论了无偏见的视觉推荐系统领域的挑战和未来方向，特别是系统不依赖人类规则而自主学习任务的新方法。

    

    可视化推荐系统（VRSs）是一个新颖而具有挑战性的研究领域，旨在帮助从数据中生成洞察力图表，并支持非专业用户进行信息发现。在提出的许多贡献中，一些系统采用了模仿人类分析师的雄心勃勃目标，即识别数据中的相关关系，并做出适当的设计选择以用具有洞察力的图表来表示这些关系。我们将这些系统称为“无偏见”VRSs，因为它们不依赖于人类提供的约束和规则，而是试图自主学习任务。尽管无偏见的VRSs具有很高的应用潜力，但它们的进展受到多个障碍的阻碍，包括缺乏用于训练推荐算法的标准化数据集、学习设计规则的困难以及定义用于评估生成图表的感知有效性的定量标准。

    arXiv:2302.00569v2 Announce Type: replace  Abstract: Visualization Recommendation Systems (VRSs) are a novel and challenging field of study aiming to help generate insightful visualizations from data and support non-expert users in information discovery. Among the many contributions proposed in this area, some systems embrace the ambitious objective of imitating human analysts to identify relevant relationships in data and make appropriate design choices to represent these relationships with insightful charts. We denote these systems as "agnostic" VRSs since they do not rely on human-provided constraints and rules but try to learn the task autonomously. Despite the high application potential of agnostic VRSs, their progress is hindered by several obstacles, including the absence of standardized datasets to train recommendation algorithms, the difficulty of learning design rules, and defining quantitative criteria for evaluating the perceptual effectiveness of generated plots. This pape
    
[^360]: 使用跨域自监督深度学习的鲁棒性阿尔茨海默病进展建模

    Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning

    [https://arxiv.org/abs/2211.08559](https://arxiv.org/abs/2211.08559)

    使用跨域自监督深度学习方法，以医学图像作为输入进行疾病预后建模，提高了预测阿尔茨海默病进展的准确性。

    

    发展成功的人工智能系统实践上取决于鲁棒的深度学习模型和大规模、高质量的数据。然而，在许多实际应用中获取并标记数据可能太昂贵且耗时，例如临床疾病模型。自监督学习已经展示了在小数据情况下增加模型准确性和鲁棒性的巨大潜力。本研究中，我们开发了一种用医学图像作为输入的跨域自监督学习方法，将疾病预后建模作为回归问题。我们证明了自监督预训练可以提高从医学图像中预测阿尔茨海默病进展的准确性。

    arXiv:2211.08559v2 Announce Type: cross  Abstract: Developing successful artificial intelligence systems in practice depends on both robust deep learning models and large, high-quality data. However, acquiring and labeling data can be prohibitively expensive and time-consuming in many real-world applications, such as clinical disease models. Self-supervised learning has demonstrated great potential in increasing model accuracy and robustness in small data regimes. In addition, many clinical imaging and disease modeling applications rely heavily on regression of continuous quantities. However, the applicability of self-supervised learning for these medical-imaging regression tasks has not been extensively studied. In this study, we develop a cross-domain self-supervised learning approach for disease prognostic modeling as a regression problem using medical images as input. We demonstrate that self-supervised pretraining can improve the prediction of Alzheimer's Disease progression from 
    
[^361]: 基于任务驱动特征选择的多通道成像实验设计

    Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection

    [https://arxiv.org/abs/2210.06891](https://arxiv.org/abs/2210.06891)

    提出了一种基于任务驱动特征选择的多通道成像实验设计方法，通过优化设计和训练机器学习模型执行用户指定的图像分析任务。

    

    本文提出了一种数据驱动的、任务特定的实验设计范式，旨在缩短采集时间、降低成本、加速成像设备的部署。当前实验设计方法主要集中在模型参数估计上，并要求对特定模型进行规范，而在成像领域，其他任务可能驱动设计。此外，这种方法常常在真实世界的成像应用中导致难以求解的优化问题。本文提出了一种新的实验设计范式，同时优化设计（图像通道集）并训练一个机器学习模型来执行用户指定的图像分析任务。该方法在测量空间上密集采样数据（许多图像通道）进行了少量采集，然后识别一个预先指定尺寸的最佳支持任务的通道子集。

    arXiv:2210.06891v3 Announce Type: replace-cross  Abstract: This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experime
    
[^362]: 零阶硬阈值：梯度误差与扩展性

    Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity

    [https://arxiv.org/abs/2210.05279](https://arxiv.org/abs/2210.05279)

    本文提出了一种新的随机零阶梯度硬阈值（SZOHT）算法，通过新颖的随机支持采样，解决了$\ell_0$约束下梯度计算困难的问题。

    

    $\ell_0$约束优化在机器学习中很常见，尤其在高维问题中，因为它是实现稀疏学习的一种基本方法。硬阈值梯度下降是解决这一问题的主要技术。然而，在许多实际问题中，目标函数的一阶梯度可能无法获取或计算代价高昂，这时零阶（ZO）梯度可以成为一个很好的替代方案。遗憾的是，零阶梯度能否与硬阈值算子配合仍然是一个未解决的问题。为了解决这个谜题，本文关注$\ell_0$约束的黑盒随机优化问题，并提出了一种新的随机零阶梯度硬阈值（SZOHT）算法，其使用通用的ZO梯度估计器，通过新颖的随机支持采样得以实现。我们在标准假设下提供了SZOHT的收敛性分析。

    arXiv:2210.05279v2 Announce Type: replace  Abstract: $\ell_0$ constrained optimization is prevalent in machine learning, particularly for high-dimensional problems, because it is a fundamental approach to achieve sparse learning. Hard-thresholding gradient descent is a dominant technique to solve this problem. However, first-order gradients of the objective function may be either unavailable or expensive to calculate in a lot of real-world problems, where zeroth-order (ZO) gradients could be a good surrogate. Unfortunately, whether ZO gradients can work with the hard-thresholding operator is still an unsolved problem. To solve this puzzle, in this paper, we focus on the $\ell_0$ constrained black-box stochastic optimization problems, and propose a new stochastic zeroth-order gradient hard-thresholding (SZOHT) algorithm with a general ZO gradient estimator powered by a novel random support sampling. We provide the convergence analysis of SZOHT under standard assumptions. Importantly, we
    
[^363]: MechProNet：金属增材制造中机器学习预测机械性能

    MechProNet: Machine Learning Prediction of Mechanical Properties in Metal Additive Manufacturing

    [https://arxiv.org/abs/2209.12605](https://arxiv.org/abs/2209.12605)

    通过机器学习方法预测金属增材制造中的机械性能，建立了一个全面的框架，包括来自90多篇文章和140个数据表的大量实验数据。

    

    预测金属增材制造（MAM）中的机械性能对于确保印制零部件的性能和可靠性以及其适用于特定应用至关重要。本研究引入了一个全面的框架，用于评估预测机械性能的机器学习模型。我们从90多篇MAM文章和来自多种来源的数据表中收集了大量实验数据，包括140个不同的MAM数据表。这些数据包括MAM处理条件、机器、材料和结果。

    arXiv:2209.12605v2 Announce Type: replace-cross  Abstract: Predicting mechanical properties in metal additive manufacturing (MAM) is essential for ensuring the performance and reliability of printed parts, as well as their suitability for specific applications. However, conducting experiments to estimate mechanical properties in MAM processes can be laborious and expensive, and they are often limited to specific materials and processes. Machine learning (ML) methods offer a more flexible and cost-effective approach to predicting mechanical properties based on processing parameters and material properties. In this study, we introduce a comprehensive framework for benchmarking ML models for predicting mechanical properties. We compiled an extensive experimental dataset from over 90 MAM articles and data sheets from a diverse range of sources, encompassing 140 different MAM data sheets. This dataset includes information on MAM processing conditions, machines, materials, and resulting mech
    
[^364]: 机器学习中预测不确定性估计的综述

    A review of predictive uncertainty estimation with machine learning

    [https://arxiv.org/abs/2209.08307](https://arxiv.org/abs/2209.08307)

    该综述回顾了机器学习中利用概率分布进行预测不确定性估计的主题，为评估概率预测提供了相关度量，从经典统计方法到现代机器学习算法进行了梳理。

    

    预测和机器学习模型的预测应当以概率分布的形式呈现，旨在增加向最终用户传达的信息量。尽管学术界和工业界中利用机器学习模型进行概率预测和预测的应用越来越频繁，但相关概念和方法尚未在整个领域的整体视角下得到形式化和结构化。本文综述了利用机器学习算法进行预测不确定性估计的主题，以及用于评估概率预测的相关度量（一致评分函数和适当评分规则）。该综述涵盖了从早期统计引入（基于贝叶斯统计或分位数回归的线性回归和时间序列模型）到最近的机器学习算法（包括用于位置、规模的广义加性模型）

    arXiv:2209.08307v2 Announce Type: replace-cross  Abstract: Predictions and forecasts of machine learning models should take the form of probability distributions, aiming to increase the quantity of information communicated to end users. Although applications of probabilistic prediction and forecasting with machine learning models in academia and industry are becoming more frequent, related concepts and methods have not been formalized and structured under a holistic view of the entire field. Here, we review the topic of predictive uncertainty estimation with machine learning algorithms, as well as the related metrics (consistent scoring functions and proper scoring rules) for assessing probabilistic predictions. The review covers a time period spanning from the introduction of early statistical (linear regression and time series models, based on Bayesian statistics or quantile regression) to recent machine learning algorithms (including generalized additive models for location, scale a
    
[^365]: 机器学习中使用的log-cosh损失函数的统计特性

    Statistical Properties of the log-cosh Loss Function Used in Machine Learning

    [https://arxiv.org/abs/2208.04564](https://arxiv.org/abs/2208.04564)

    分析log-cosh损失函数的统计特性，比较它与柯西分布的性质，并研究MLE的偏差、方差和置信区间，同时提供了与其他损失函数的鲁棒估计器比较。

    

    这篇论文分析了机器学习中使用的一种常见损失函数，即log-cosh损失函数。已经发表了许多使用这种损失函数的论文，但迄今为止，文献中尚未提出过统计分析。本文介绍了log-cosh损失函数产生的分布函数。我们将其与类似的柯西分布进行比较，并进行多种表征其特性的统计程序。特别地，我们检验了与其相关的概率密度函数、累积分布函数、似然函数和费舍尔信息。我们将柯西和Cosh分布以及MLE的位置参数的渐近偏差、渐近方差和置信区间进行并列考虑。我们还提供了来自其他几种损失函数的鲁棒估计器的比较，包括Huber损失函数和秩分散函数。此外，我们还研究了该损失函数的应用。

    arXiv:2208.04564v4 Announce Type: replace-cross  Abstract: This paper analyzes a popular loss function used in machine learning called the log-cosh loss function. A number of papers have been published using this loss function but, to date, no statistical analysis has been presented in the literature. In this paper, we present the distribution function from which the log-cosh loss arises. We compare it to a similar distribution, called the Cauchy distribution, and carry out various statistical procedures that characterize its properties. In particular, we examine its associated pdf, cdf, likelihood function and Fisher information. Side-by-side we consider the Cauchy and Cosh distributions as well as the MLE of the location parameter with asymptotic bias, asymptotic variance, and confidence intervals. We also provide a comparison of robust estimators from several other loss functions, including the Huber loss function and the rank dispersion function. Further, we examine the use of the 
    
[^366]: 通过随机正交分解和深度学习进行数字孪生数据建模

    Digital Twin Data Modelling by Randomized Orthogonal Decomposition and Deep Learning

    [https://arxiv.org/abs/2206.08659](https://arxiv.org/abs/2206.08659)

    本论文引入了一种通过结合Krylov动态模态分解和适当正交分解优势的新算法，来创建高效的流体流动数字孪生模型，并证明了随机正交分解算法相对于SVD经验正交分解方法具有数个优势

    

    数字孪生是一个替代模型，其主要特点是反映原始过程行为。将动态过程与降低复杂度的数字孪生模型关联具有显著优势，可以在具有显著变化的时间尺度上以高准确性和降低CPU时间和硬件成本的高效映射动态过程。本文介绍了一种为流体流动创建高效数字孪生模型的新框架。我们提出了一种结合了基于Krylov的动态模态分解和适当正交分解优势的新算法，优于选择最具影响力模态的方法。我们证明随机正交分解算法相比SVD经验正交分解方法提供了几个优势，并缓解了投影误差，制定了一个多目标优化问题。我们包括了最先进的技术

    arXiv:2206.08659v2 Announce Type: replace-cross  Abstract: A digital twin is a surrogate model that has the main feature to mirror the original process behavior. Associating the dynamical process with a digital twin model of reduced complexity has the significant advantage to map the dynamics with high accuracy and reduced costs in CPU time and hardware to timescales over which that suffers significantly changes and so it is difficult to explore. This paper introduces a new framework for creating efficient digital twin models of fluid flows. We introduce a novel algorithm that combines the advantages of Krylov based dynamic mode decomposition with proper orthogonal decomposition and outperforms the selection of the most influential modes. We prove that randomized orthogonal decomposition algorithm provides several advantages over SVD empirical orthogonal decomposition methods and mitigates the projection error formulating a multiobjective optimization problem.We involve the state-of-th
    
[^367]: 通过机会性移动中继加速异步联邦学习收敛

    Accelerating Asynchronous Federated Learning Convergence via Opportunistic Mobile Relaying

    [https://arxiv.org/abs/2206.04742](https://arxiv.org/abs/2206.04742)

    通过机会性移动中继，提出了FedMobile算法，实现了异步联邦学习收敛速率为$O(\frac{1}{\sqrt{NT}})$。

    

    这篇论文探讨了移动网络环境下的异步联邦学习（FL）。大多数FL算法假设客户端和服务器之间的通信始终可用，然而，在许多实际系统中并非如此。为解决这一问题，本文探讨了移动性对异步FL收敛性能的影响。通过利用移动性，研究表明客户端可以通过另一客户端充当中继与服务器间接通信，创造额外的通信机会。这使客户端能够更早地上传本地模型更新或接收更新的全局模型。我们提出了一种新的FL算法，称为FedMobile，该算法融合了机会性中继，并解决了何时以及如何中继的关键问题。我们证明FedMobile实现了收敛速率为$O(\frac{1}{\sqrt{NT}})$，其中$N$是客户端数量，$T$是客户端数量。

    arXiv:2206.04742v2 Announce Type: replace-cross  Abstract: This paper presents a study on asynchronous Federated Learning (FL) in a mobile network setting. The majority of FL algorithms assume that communication between clients and the server is always available, however, this is not the case in many real-world systems. To address this issue, the paper explores the impact of mobility on the convergence performance of asynchronous FL. By exploiting mobility, the study shows that clients can indirectly communicate with the server through another client serving as a relay, creating additional communication opportunities. This enables clients to upload local model updates sooner or receive fresher global models. We propose a new FL algorithm, called FedMobile, that incorporates opportunistic relaying and addresses key questions such as when and how to relay. We prove that FedMobile achieves a convergence rate $O(\frac{1}{\sqrt{NT}})$, where $N$ is the number of clients and $T$ is the numbe
    
[^368]: 技能机器：强化学习中的时间逻辑技能组合

    Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning

    [https://arxiv.org/abs/2205.12532](https://arxiv.org/abs/2205.12532)

    代理通过学习一组足够的技能原语，可以在环境中实现所有高层目标，并能够在任何常规语言中逻辑和时间地组合这些技能，以确切实现时间逻辑规范。

    

    代理能够在相同环境中通过语言指定解决各种问题是可取的。获得这样的代理的一种流行方法是重复使用在先前任务中学到的技能，以将其组合概括到新任务中。然而，由于高层目标在语言中逻辑和时间上可以组合的方式很多且组合数目巨大导致维度灾难，这是一个具有挑战性的问题。为了解决这个问题，我们提出了一个框架，其中代理首先学习足够的技能原语来实现其环境中的所有高层目标。然后代理可以灵活地逻辑和时间地组合它们，明确地实现任何正则语言中的时间逻辑规范，如线性时态逻辑的正则片段。这使代理能够从复杂的时间逻辑任务规范映射到近乎最优的

    arXiv:2205.12532v2 Announce Type: replace  Abstract: It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a challenging problem due to the curse of dimensionality induced by the combinatorially large number of ways high-level goals can be combined both logically and temporally in language. To address this problem, we propose a framework where an agent first learns a sufficient set of skill primitives to achieve all high-level goals in its environment. The agent can then flexibly compose them both logically and temporally to provably achieve temporal logic specifications in any regular language, such as regular fragments of linear temporal logic. This provides the agent with the ability to map from complex temporal logic task specifications to near-opti
    
[^369]: PyGOD: 一个用于图异常检测的Python库

    PyGOD: A Python Library for Graph Outlier Detection

    [https://arxiv.org/abs/2204.12095](https://arxiv.org/abs/2204.12095)

    PyGOD是一个开源Python库，支持多种领先的基于图的异常检测方法，提供了易于使用的API和丰富的实用程序函数，同时采用了最佳的代码可靠性和可维护性实践。

    

    PyGOD是一个用于检测图数据中异常值的开源Python库。作为这类库中首个综合性工具，PyGOD支持多种领先的基于图的异常检测方法，提供了易于使用、有详细文档支持的API，旨在供研究人员和从业者使用。PyGOD提供了不同检测器的模块化组件，使用户可以轻松定制每个检测器以适应其用途。为简化检测工作流的构建，PyGOD提供了许多常用的实用程序函数。为了将计算扩展到大型图形，PyGOD支持深度模型的功能，如采样和小批量处理。PyGOD采用了促进代码可靠性和可维护性的最佳实践，包括单元测试、持续集成和代码覆盖。为了方便访问，PyGOD以BSD 2-Clause许可证发布在https://pygod.org 和 Python包中。

    arXiv:2204.12095v2 Announce Type: replace  Abstract: PyGOD is an open-source Python library for detecting outliers in graph data. As the first comprehensive library of its kind, PyGOD supports a wide array of leading graph-based methods for outlier detection under an easy-to-use, well-documented API designed for use by both researchers and practitioners. PyGOD provides modularized components of the different detectors implemented so that users can easily customize each detector for their purposes. To ease the construction of detection workflows, PyGOD offers numerous commonly used utility functions. To scale computation to large graphs, PyGOD supports functionalities for deep models such as sampling and mini-batch processing. PyGOD uses best practices in fostering code reliability and maintainability, including unit testing, continuous integration, and code coverage. To facilitate accessibility, PyGOD is released under a BSD 2-Clause license at https://pygod.org and at the Python Packa
    
[^370]: 自适应有理激活以提升深度强化学习

    Adaptive Rational Activations to Boost Deep Reinforcement Learning

    [https://arxiv.org/abs/2102.09407](https://arxiv.org/abs/2102.09407)

    本研究提出了利用有理数作为适应性激活函数来改进深度强化学习，并展示了这种方法在Atari游戏中取得了一致的改进，特别是将简单的DQN提升为一个稳健的方法。

    

    生物学的最新见解显示，智能不仅源自神经元之间的连接，而且单个神经元承担的计算责任比以往预期的更多。这种观点在不断变化的强化学习环境中至关重要，然而当前方法仍然主要使用静态激活函数。在本工作中，我们阐述了为什么有理数适合作为可适应的激活函数，以及为什么将其包含到神经网络中至关重要。受Residual网络中循环性的启发，我们导出了有理单位在残差连接下封闭的条件，并制定了一个自然正则化的版本：循环有理数。我们证明，为流行算法配备（循环）有理数激活会显著提高Atari游戏的性能，尤其将简单的DQN转化为一种可靠的方法。

    arXiv:2102.09407v4 Announce Type: replace  Abstract: Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. This perspective should be critical in the context of constantly changing distinct reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are suitable for adaptable activation functions and why their inclusion into neural networks is crucial. Inspired by recurrence in residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version: the recurrent-rational. We demonstrate that equipping popular algorithms with (recurrent-)rational activations leads to consistent improvements on Atari games, especially turning simple DQN into a solid approach, competiti
    
[^371]: 移动聚合提取网络

    Shift Aggregate Extract Networks

    [https://arxiv.org/abs/1703.05537](https://arxiv.org/abs/1703.05537)

    通过深层分层分解的架构，提出了一种有效学习大型图表示的方法，能够在大型社交网络数据集上胜过当前最先进的图分类方法，同时在小型化学生物基准数据集上具有竞争力。

    

    我们介绍了一种基于深层分层分解的架构，用于学习大型图的有效表示。我们的框架扩展了在核方法中使用的经典R-分解，实现了嵌套的部分-部分关系。与直接在输入图上展开模板的递归神经网络不同，我们在分解层次结构上展开神经网络模板，从而能够处理通常表征社交网络图的高度变化。深层次的分层分解也适用于领域压缩，这种技术通过利用对称性来减少空间和时间复杂度。我们在实证上证明，我们的方法在大型社交网络数据集上能够胜过当前最先进的图分类方法，同时在小型化学生物基准数据集上具有竞争力。

    arXiv:1703.05537v2 Announce Type: replace  Abstract: We introduce an architecture based on deep hierarchical decompositions to learn effective representations of large graphs. Our framework extends classic R-decompositions used in kernel methods, enabling nested part-of-part relations. Unlike recursive neural networks, which unroll a template on input graphs directly, we unroll a neural network template over the decomposition hierarchy, allowing us to deal with the high degree variability that typically characterize social network graphs. Deep hierarchical decompositions are also amenable to domain compression, a technique that reduces both space and time complexity by exploiting symmetries. We show empirically that our approach is able to outperform current state-of-the-art graph classification methods on large social network datasets, while at the same time being competitive on small chemobiological benchmark datasets.
    
[^372]: EdgeOL: 边缘设备上高效的原位在线学习

    EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])

    [http://arxiv.org/abs/2401.16694](http://arxiv.org/abs/2401.16694)

    本文提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率，在边缘设备上实现了显著的性能提升。

    

    新兴应用，如机器人辅助养老和物体识别，通常采用深度学习神经网络模型，并且自然需要：i) 处理实时推理请求和ii) 适应可能的部署场景变化。在线模型微调被广泛采用以满足这些需求。然而，微调会导致显著的能量消耗，使其难以部署在边缘设备上。在本文中，我们提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率。实验结果显示，EdgeOL平均减少了82%的微调执行时间，74%的能量消耗，并提高了平均推理准确率1.70%，相对于即时在线学习策略。

    Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
    
[^373]: 一个用于高维数据的判别贝叶斯高斯过程潜变量模型

    A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data. (arXiv:2401.16497v1 [cs.LG])

    [http://arxiv.org/abs/2401.16497](http://arxiv.org/abs/2401.16497)

    本研究提出了一个名为LDGD的判别贝叶斯高斯过程潜变量模型，能够有效地从高维数据中提取信息，并具有较高的预测准确性和鲁棒性。

    

    从高维数据中提取有意义的信息是一个具有挑战性的建模问题，特别是当数据被噪声干扰或以不同的模态表示时。在这项研究中，我们提出了一种新颖的非参数建模方法，利用高斯过程（GP）将高维数据映射到潜在的低维流形上。这个模型被命名为潜在判别生成解码器（LDGD），它在流形发现过程中利用了数据（或其特征）和相关标签（如类别或刺激）。为了推断潜在变量，我们提供了一个贝叶斯解，使得LDGD能够有效地捕捉数据中的内在不确定性，同时提高模型的预测准确性和鲁棒性。我们在合成数据集和基准数据集上演示了LDGD的应用。LDGD不仅能准确地推断流形，而且在预测标签方面的准确性超过了最先进的方法。

    Extracting meaningful information from high-dimensional data poses a formidable modeling challenge, particularly when the data is obscured by noise or represented through different modalities. In this research, we propose a novel non-parametric modeling approach, leveraging the Gaussian Process (GP), to characterize high-dimensional data by mapping it to a latent low-dimensional manifold. This model, named the Latent Discriminative Generative Decoder (LDGD), utilizes both the data (or its features) and associated labels (such as category or stimulus) in the manifold discovery process. To infer the latent variables, we derive a Bayesian solution, allowing LDGD to effectively capture inherent uncertainties in the data while enhancing the model's predictive accuracy and robustness. We demonstrate the application of LDGD on both synthetic and benchmark datasets. Not only does LDGD infer the manifold accurately, but its prediction accuracy in anticipating labels surpasses state-of-the-art a
    
[^374]: 理解域泛化：从噪声鲁棒性的角度

    Understanding Domain Generalization: A Noise Robustness Perspective. (arXiv:2401.14846v1 [cs.LG])

    [http://arxiv.org/abs/2401.14846](http://arxiv.org/abs/2401.14846)

    本文通过研究域泛化算法和经验风险最小化算法在标签噪声下的表现，发现域泛化算法在有限样本训练中显示出隐性的标签噪声鲁棒性，有助于减轻假冗余相关和提高泛化能力，但在真实世界基准数据集上的实验证明，标签噪声鲁棒性并不一定意味着相比ERM具有更好的性能。

    

    尽管快速发展了用于域泛化的机器学习算法，但没有明确的经验证据表明现有的域泛化算法在标准基准测试中优于经典的经验风险最小化（ERM）算法。为了更好地理解这一现象，我们通过标签噪声的视角研究了域泛化算法相对于ERM的优势。具体而言，我们的有限样本分析揭示了标签噪声加剧了ERM中假冗余相关性的影响，削弱了泛化能力。相反，我们证明了在有限样本训练中，域泛化算法在存在假冗余相关时具有隐性的标签噪声鲁棒性。这种有利性有助于减轻假冗余相关性并改善合成实验中的泛化能力。然而，对真实世界基准数据集进行的额外综合实验表明，标签噪声鲁棒性并不一定意味着相比ERM具有更好的性能。

    Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We co
    
[^375]: 利用Ricci流引导的自编码器学习时变动力学

    Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])

    [http://arxiv.org/abs/2401.14591](http://arxiv.org/abs/2401.14591)

    利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。

    

    我们提出了一种基于流形的自编码器方法，用于学习时间上的非线性动力学，尤其是偏微分方程（PDE），其中流形潜空间根据Ricci流发展。这可以通过在物理信息设置中模拟Ricci流来实现，并且可以匹配流形量，以便实现Ricci流。使用我们的方法，流形是作为训练过程的一部分学习的，因此可以识别出理想的几何形状，同时演变也能在静态方法上引起更宽容的潜在表示。我们在一系列数值实验中展示了我们的方法，包括具有周期性和随机性等理想特征的PDE，并在分布内和外推场景中进行误差评估。

    We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
    
[^376]: 多模态路径：通过其他模态的无关数据来改进Transformer

    Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])

    [http://arxiv.org/abs/2401.14405](http://arxiv.org/abs/2401.14405)

    本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。

    

    我们提出使用来自其他模态的无关数据来改进特定模态的Transformer，例如，使用音频或点云数据集来改进ImageNet模型。我们强调目标模态的数据样本与其他模态无关，这与利用不同模态的配对数据（如CLIP）或交错数据的其他方法不同。我们提出了一种名为多模态路径的方法-给定目标模态和设计用于该模态的Transformer，我们使用使用另一个模态的数据训练的辅助Transformer，并构建路径来连接两个模型的组件，以便目标模态的数据可以被两个模型处理。通过这种方式，我们利用了从两个模态获得的Transformer的通用序列建模能力。作为具体实现，我们通常使用特定模态的tokenizer和任务特定的head，但是利用辅助模型的Transformer block。

    We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v
    
[^377]: 在语言模型中实现对3D分子-文本的解释

    Towards 3D Molecule-Text Interpretation in Language Models. (arXiv:2401.13923v1 [cs.LG])

    [http://arxiv.org/abs/2401.13923](http://arxiv.org/abs/2401.13923)

    提出了一个名为3D-MoLM的模型，通过给语言模型配备一个3D分子编码器，实现了对3D分子-文本的解释和分析，此模型在下游任务上显著优于现有基线。

    

    语言模型（LMs）在各个领域有着很大的影响。然而，它们对于理解3D分子结构的固有限制极大地限制了它们在生物分子领域的潜力。为了弥补这一差距，我们关注于3D分子-文本解释，并提出3D-MoLM：3D分子语言模型。具体而言，3D-MoLM通过为LM配备一个3D分子编码器，使得LM能够解释和分析3D分子。这种集成是通过一个3D分子-文本投影器实现的，它连接了3D分子编码器的表示空间和LM的输入空间。此外，为了增强3D-MoLM在跨模态分子理解和指令跟随方面的能力，我们精心策划了一个以3D分子为中心的指引调整数据集--3D-MoIT。通过3D分子-文本对齐和3D分子中心的指引调整，3D-MoLM建立了3D分子编码器和LM的集成。它在下游任务上显著超过了现有的基线。

    Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks,
    
[^378]: 基于稀疏网格的不连续性检测的图信息神经网络

    Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])

    [http://arxiv.org/abs/2401.13652](http://arxiv.org/abs/2401.13652)

    本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。

    

    本文提出了一种新颖的方法来检测不连续函数的不连续界面。该方法利用了基于图的神经网络（GINNs）和稀疏网格来解决维度大于3的情况下的不连续性检测。训练过的GINNs在稀疏网格上识别有问题的点，并利用构建在网格上的图结构实现高效准确的不连续性检测性能。我们还引入了一种递归算法用于一般的基于稀疏网格的检测器，具有收敛性和易于应用性。在维度n=2和n=4的函数上进行的数值实验证明了GINNs在检测不连续界面方面的高效性和鲁棒泛化能力。值得注意的是，经过训练的GINNs具有可移植性和多功能性，可以集成到各种算法中并共享给用户。

    In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
    
[^379]: 合作多智能体图形赌博机：UCB算法和遗憾分析

    Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret Analysis. (arXiv:2401.10383v1 [cs.LG])

    [http://arxiv.org/abs/2401.10383](http://arxiv.org/abs/2401.10383)

    本文提出了一种解决多智能体图形赌博机问题的算法Multi-G-UCB，并通过数值实验验证了其有效性。

    

    本文将多智能体图形赌博机问题建模为Zhang、Johansson和Li在[CISS 57, 1-6 (2023)]中提出的图形赌博机问题的多智能体扩展。在我们的模型中，N个合作智能体在一个连通的图G上移动，图G有K个节点。抵达每个节点时，智能体观察到从一个与节点相关的概率分布中随机抽取的奖励。系统奖励被建模为智能体观测到的奖励的加权和，其中权重表达了多个智能体同时对同一节点进行采样的边际减少奖励。我们提出了一个基于上限置信区间（UCB）的学习算法，称为Multi-G-UCB，并证明了在T步内其期望遗憾被界定为$O(N\log(T)[\sqrt{KT} + DK])$，其中D是图G的直径。最后，我们通过与其他方法进行比较对算法进行了数值测试。

    In this paper, we formulate the multi-agent graph bandit problem as a multi-agent extension of the graph bandit problem introduced by Zhang, Johansson, and Li [CISS 57, 1-6 (2023)]. In our formulation, $N$ cooperative agents travel on a connected graph $G$ with $K$ nodes. Upon arrival at each node, agents observe a random reward drawn from a node-dependent probability distribution. The reward of the system is modeled as a weighted sum of the rewards the agents observe, where the weights capture the decreasing marginal reward associated with multiple agents sampling the same node at the same time. We propose an Upper Confidence Bound (UCB)-based learning algorithm, Multi-G-UCB, and prove that its expected regret over $T$ steps is bounded by $O(N\log(T)[\sqrt{KT} + DK])$, where $D$ is the diameter of graph $G$. Lastly, we numerically test our algorithm by comparing it to alternative methods.
    
[^380]: 混合任务元学习：一种用于可扩展和可转移带宽分配的图神经网络方法

    Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation. (arXiv:2401.10253v1 [cs.NI])

    [http://arxiv.org/abs/2401.10253](http://arxiv.org/abs/2401.10253)

    本文提出了一种基于图神经网络的混合任务元学习算法，用于可扩展和可转移的带宽分配。通过引入GNN和HML算法，该方法在不同的通信场景下具有较好的性能和采样效率。

    

    本文提出了一种基于深度学习的带宽分配策略，该策略具有以下特点：1）随着用户数量的增加具有可扩展性；2）能够在不同的通信场景下进行转移，例如非平稳的无线信道、不同的服务质量要求和动态可用资源。为了支持可扩展性，带宽分配策略采用了图神经网络（GNN）进行表示，训练参数的数量随用户数量的增加而不变。为了实现GNN的泛化能力，我们开发了一种混合任务元学习（HML）算法，在元训练过程中使用不同的通信场景来训练GNN的初始参数。然后，在元测试过程中，使用少量样本对GNN进行微调以适应未见过的通信场景。仿真结果表明，与现有基准相比，我们的HML方法可以将初始性能提高8.79％，并提高采样效率73％。在微调后，

    In this paper, we develop a deep learning-based bandwidth allocation policy that is: 1) scalable with the number of users and 2) transferable to different communication scenarios, such as non-stationary wireless channels, different quality-of-service (QoS) requirements, and dynamically available resources. To support scalability, the bandwidth allocation policy is represented by a graph neural network (GNN), with which the number of training parameters does not change with the number of users. To enable the generalization of the GNN, we develop a hybrid-task meta-learning (HML) algorithm that trains the initial parameters of the GNN with different communication scenarios during meta-training. Next, during meta-testing, a few samples are used to fine-tune the GNN with unseen communication scenarios. Simulation results demonstrate that our HML approach can improve the initial performance by $8.79\%$, and sampling efficiency by $73\%$, compared with existing benchmarks. After fine-tuning,
    
[^381]: 识别与早期热带气旋强化有关的三维辐射模式

    Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification. (arXiv:2401.09493v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.09493](http://arxiv.org/abs/2401.09493)

    本研究利用线性变分编码器-解码器来学习云辐射反馈对早期热带气旋强化的影响，发现内核深对流和浅云的长波辐射强迫都对强化起到贡献，其中深对流的影响最大。

    

    云辐射反馈影响了早期热带气旋的强化，但现有诊断框架的局限性使其无法用来研究不对称或瞬态的辐射加热。我们提出了一种线性变分编码器-解码器（VED）来学习辐射与实际模拟的气旋表面强化之间的隐藏关系。限制VED模型的输入可以利用其不确定性来识别辐射对强化更重要的时期。对提取的三维辐射结构的细致检查表明，内核深对流和浅云的长波辐射强迫都对强化起到贡献，其中深对流在整体上具有最大的影响。我们发现，在浅云的下风处的深对流对海燕的强化至关重要。我们的工作表明，机器学习可以发现热力-动力学关系，而不依赖于轴对称或确定性的方案。

    Cloud radiative feedback impacts early tropical cyclone (TC) intensification, but limitations in existing diagnostic frameworks make them unsuitable for studying asymmetric or transient radiative heating. We propose a linear Variational Encoder-Decoder (VED) to learn the hidden relationship between radiation and the surface intensification of realistic simulated TCs. Limiting VED model inputs enables using its uncertainty to identify periods when radiation has more importance for intensification. A close examination of the extracted 3D radiative structures suggests that longwave radiative forcing from inner core deep convection and shallow clouds both contribute to intensification, with the deep convection having the most impact overall. We find that deep convection downwind of the shallow clouds is critical to the intensification of Haiyan. Our work demonstrates that machine learning can discover thermodynamic-kinematic relationships without relying on axisymmetric or deterministic as
    
[^382]: 概率Lambert问题的解决方案：与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程的连接

    Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs. (arXiv:2401.07961v1 [math.OC])

    [http://arxiv.org/abs/2401.07961](http://arxiv.org/abs/2401.07961)

    这项研究将概率Lambert问题与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程等领域连接起来，从而解决了概率Lambert问题的解的存在和唯一性，并提供了数值求解的方法。

    

    Lambert问题涉及通过速度控制在规定的飞行时间内将航天器从给定的初始位置转移到给定的终端位置，受到重力力场的限制。我们考虑了Lambert问题的概率变种，其中位置向量的端点约束的知识被它们各自的联合概率密度函数所替代。我们证明了具有端点联合概率密度约束的Lambert问题是一个广义的最优质量传输（OMT）问题，从而将这个经典的天体动力学问题与现代随机控制和随机机器学习的新兴研究领域联系起来。这个新发现的连接使我们能够严格建立概率Lambert问题的解的存在性和唯一性。同样的连接还帮助通过扩散正规化数值求解概率Lambert问题，即通过进一步的连接来利用。

    Lambert's problem concerns with transferring a spacecraft from a given initial to a given terminal position within prescribed flight time via velocity control subject to a gravitational force field. We consider a probabilistic variant of the Lambert problem where the knowledge of the endpoint constraints in position vectors are replaced by the knowledge of their respective joint probability density functions. We show that the Lambert problem with endpoint joint probability density constraints is a generalized optimal mass transport (OMT) problem, thereby connecting this classical astrodynamics problem with a burgeoning area of research in modern stochastic control and stochastic machine learning. This newfound connection allows us to rigorously establish the existence and uniqueness of solution for the probabilistic Lambert problem. The same connection also helps to numerically solve the probabilistic Lambert problem via diffusion regularization, i.e., by leveraging further connection 
    
[^383]: 识别策略梯度子空间

    Identifying Policy Gradient Subspaces. (arXiv:2401.06604v1 [cs.LG])

    [http://arxiv.org/abs/2401.06604](http://arxiv.org/abs/2401.06604)

    本文研究了两种深度策略梯度方法在不同模拟基准任务上的评估结果，发现尽管数据分布不断变化，但存在低维且缓慢变化的梯度子空间，这有助于未来更高效的强化学习工作。

    

    策略梯度方法在解决复杂的连续控制任务方面具有巨大的潜力。然而，通过利用优化问题内部的结构，可以提高其训练效率。最近的研究表明，通过利用梯度位于低维且缓慢变化子空间中的事实，可以加速监督学习。在本文中，我们对两种流行的深度策略梯度方法在各种模拟基准任务上进行了全面的评估。我们的结果表明，尽管强化学习固有的数据分布不断变化，但存在这样的梯度子空间。这些发现为未来更高效的强化学习工作，例如改进参数空间探索或实现二阶优化，提供了有希望的方向。

    Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.
    
[^384]: 通过谈判评估语言模型的代理能力

    Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])

    [http://arxiv.org/abs/2401.04536](http://arxiv.org/abs/2401.04536)

    本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。

    

    公司、组织和政府越来越多地利用语言模型（LM）展示类似代理行为的出色能力。随着LM被采用来执行越来越具有自主性的任务，迫切需要可靠且可扩展的评估基准。当前主要是静态的LM基准无法很好地评估此类动态应用。因此，我们提议通过谈判游戏的视角来共同评估LM的性能和对齐。我们认为这个共同任务更好地反映了真实世界的部署条件，并提供了关于LM决策过程的见解。至关重要的是，谈判游戏使我们能够研究多轮次和跨模型交互，调整复杂性，并避免评估中的意外数据泄漏。我们报告了来自几个主要供应商的六个公开可访问的LM在各种谈判游戏上的结果，评估了自我对弈和交叉对弈性能。值得注意的发现包括：（i）开源模式

    Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
    
[^385]: 大型语言模型作为视觉跨领域学习器

    Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])

    [http://arxiv.org/abs/2401.03253](http://arxiv.org/abs/2401.03253)

    本研究提出了大型语言模型作为视觉跨领域学习器（LLaVO），通过将图像转换为文本描述，使用大型语言模型进行训练和微调，实现了在跨领域任务中减少领域转移的效果。

    

    深度学习模型取得的最新进展依赖于独立同分布的假设，这限制了它们在真实世界中面对领域转移时的应用。为了解决以上问题，跨领域学习旨在提取领域不变的知识，减少训练与测试数据之间的领域转移。然而，在视觉跨领域学习中，传统方法仅关注图像模态，忽视了使用文本模态来缓解领域转移的作用。在本研究中，我们提出了大型语言模型作为视觉跨领域学习器（LLaVO）。LLaVO使用视觉-语言模型将图像转换为详细的文本描述。然后，在经过设计的指导模板生成的源域/目标域的文本描述上，对大型语言模型进行微调。在领域泛化和无监督领域自适应设置下进行的各种跨领域任务的广泛实验结果表明了该方法的效果。

    Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect
    
[^386]: 训练的力量：不同的神经网络设置对能源需求的影响

    The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])

    [http://arxiv.org/abs/2401.01851](http://arxiv.org/abs/2401.01851)

    本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。

    

    本研究探讨机器学习训练方案和学习范式的变化对相应能源消耗的影响。虽然数据的可用性提高和高性能硬件的创新推动了复杂模型的训练，但也支持了能源消耗和碳排放的消隐。因此，本研究的目标是增加人们对一般训练参数和过程（从学习率到批量大小再到知识传输）的能源影响的认识。使用不同的超参数初始化在两种不同的硬件配置上评估多种设置，以获得有意义的结果。在基准结果上进行了预训练和多任务训练实验，以确定它们对可持续机器学习的潜力。

    This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
    
[^387]: 探索LLMs在心理学应用中的前沿：一份综述

    Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])

    [http://arxiv.org/abs/2401.01519](http://arxiv.org/abs/2401.01519)

    本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。

    

    本文探索了大型语言模型（LLMs）在心理学应用中的前沿。心理学经历了几次理论变革，当前人工智能（AI）和机器学习，特别是LLMs的使用有望开启新的研究方向。我们详细探讨了LLMs如ChatGPT在心理学研究中的转变。文章讨论了LLMs在认知与行为心理学、临床与咨询心理学、教育与发展心理学以及社会与文化心理学等心理学分支中的影响，强调了它们模拟人类认知和行为方面的潜力。本文深入探讨了这些模型模拟人类文本生成的能力，为心理学中的文献回顾、假设生成、实验设计、实验对象、数据分析、学术写作和同行评审等提供创新工具。虽然LLMs在推动研究方法学方面起着重要作用，

    This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
    
[^388]: 超越梯度和先验知识在隐私攻击中：利用联邦学习中语言模型的池化层输入

    Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05720](http://arxiv.org/abs/2312.05720)

    本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。

    

    联邦学习强调分散式训练，通过本地存储数据并仅发送模型更新，强调用户隐私。最近，一系列有关隐私攻击的工作通过从联邦学习上下文的语言模型中提取敏感的训练文本来损害用户隐私。然而，这些攻击技术面临着不同的障碍：一些工作主要使用有限的批处理大小（例如，批处理大小为1），而其他技术则容易被检测出来。本文介绍了一种创新的方法，具有难以检测的特点，在不同的批处理大小设置下显著提高了文本恢复率。基于基本的梯度匹配和领域先验知识，我们通过恢复语言模型的池化层输入来增强攻击能力，这使我们能够在特征级别提供额外的监督信号。与梯度数据不同，这些信号不会在句子和标记之间进行平均，从而提供更细致和有效的见解。

    Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
    
[^389]: Uni-O4: 将在线与离线深度强化学习统一起来，采用多步在线策略优化

    Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization. (arXiv:2311.03351v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03351](http://arxiv.org/abs/2311.03351)

    Uni-O4提出了统一的离线和在线深度强化学习方法，通过对齐目标实现了无缝传递，增强了学习范式的灵活性。在离线阶段，Uni-O4利用多样的集合策略解决了估计行为策略和离线数据集不匹配的问题。

    

    将离线和在线强化学习相结合对于高效和安全的学习至关重要。然而，以往的方法将离线和在线学习视为独立的过程，导致重复的设计和有限的性能。本文中，我们提出了Uni-o4，它在离线和在线学习中都使用了一个在线策略目标。由于两个阶段的目标对齐，RL代理可以在离线和在线学习之间无缝传递。这种性质增强了学习范式的灵活性，允许任意组合预训练、微调、离线和在线学习。在离线阶段，Uni-o4利用多样的集合策略来解决估计行为策略和离线数据集之间的不匹配问题。通过简单的离线策略评估（OPE）

    Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization? In this study, we propose Uni-o4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-o4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE
    
[^390]: 基于检索重建的时间序列对比学习方法

    Retrieval-Based Reconstruction For Time-series Contrastive Learning. (arXiv:2311.00519v1 [cs.LG])

    [http://arxiv.org/abs/2311.00519](http://arxiv.org/abs/2311.00519)

    本文提出了一种基于检索重建的时间序列对比学习方法（REBAR），通过检索信息和重建子序列来构建正样本对，从而解决了时间序列中使用数据增强创建正样本对的挑战。实验证明，REBAR误差可以作为正/负标记器，并且在对比学习框架中集成REBAR方法可以学习具有有用信息的嵌入表示。

    

    自监督对比学习的成功取决于鉴别出的正样本对，当它们被推到嵌入空间时，可以为后续的下游任务编码有用的信息。然而，在时间序列中，这是具有挑战性的，因为通过数据增强来创建正样本对可能会破坏原始的语义含义。我们假设如果我们能从一个子序列中检索信息，成功重建另一个子序列，那么它们应该是一个正样本对。基于这个直觉，我们引入了一种新颖的方法：基于检索重建的对比学习（REBAR）。首先，我们利用卷积交叉注意力架构计算两个不同时间序列之间的REBAR误差。然后，通过验证实验，我们展示了REBAR误差是互相类别成员的预测器，从而证明了它作为正/负标记器的使用。最后，一旦集成到对比学习框架中，我们的REBAR方法可以学习一个

    The success of self-supervised contrastive learning hinges on identifying positive data pairs that, when pushed together in embedding space, encode useful information for subsequent downstream tasks. However, in time-series, this is challenging because creating positive pairs via augmentations may break the original semantic meaning. We hypothesize that if we can retrieve information from one subsequence to successfully reconstruct another subsequence, then they should form a positive pair. Harnessing this intuition, we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR) contrastive learning. First, we utilize a convolutional cross-attention architecture to calculate the REBAR error between two different time-series. Then, through validation experiments, we show that the REBAR error is a predictor of mutual class membership, justifying its usage as a positive/negative labeler. Finally, once integrated into a contrastive learning framework, our REBAR method can learn an
    
[^391]: 用于稳定训练生成对抗网络的泛化正则化

    Flooding Regularization for Stable Training of Generative Adversarial Networks. (arXiv:2311.00318v1 [cs.LG])

    [http://arxiv.org/abs/2311.00318](http://arxiv.org/abs/2311.00318)

    本文在生成对抗网络中直接对对抗损失函数进行正则化，通过应用泛化方法防止判别器的损失过分降低，并通过实验证实了其稳定性。

    

    生成对抗网络（GANs）在图像生成方面表现出了显著的性能。然而，GAN训练存在不稳定的问题。解决这个问题的主要方法之一是修改损失函数，通常使用正则化项来改变对抗损失的类型。本文着眼于直接对对抗损失函数进行正则化。我们提出了一种方法，将泛化（过拟合抑制）方法应用于GANs中，直接防止判别器的损失过分降低。泛化需要调整泛化水平，但当应用于GANs时，我们提出适当的泛化水平设置范围由对抗损失函数确定，该论据得到了使用二元交叉熵损失对GANs进行理论分析的支持。我们通过实验证实了泛化稳定了GAN训练，并且可以与其他稳定技术相结合。我们还揭示了通过限制

    Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also reveal that by restricting
    
[^392]: Neuroformer：用于脑数据的多模态和多任务生成预训练模型

    Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])

    [http://arxiv.org/abs/2311.00136](http://arxiv.org/abs/2311.00136)

    Neuroformer是一个多模态和多任务的生成预训练模型，旨在处理系统神经科学中大规模的多模态数据。模型经过训练后能准确预测神经回路活动并推断神经回路连接性，同时能用于预测行为。

    

    最先进的系统神经科学实验产生了大规模的多模态数据，这些数据集需要新的分析工具。受到视觉和语言领域大规模预训练模型成功的启发，我们将大规模的细胞分辨率神经元尖峰数据的分析重新构建为一个自回归的时空生成问题。Neuroformer是一个多模态、多任务的生成预训练transformer（GPT）模型，专为处理系统神经科学数据的复杂性而设计。它与特征大小呈线性扩展，并且可以处理任意数量的模态，适应下游任务，比如预测行为。我们首先在模拟数据集上训练了Neuroformer，并发现它既能准确预测模拟神经回路活动，也能内在地推断出底层神经回路连接性，包括方向。当预训练用于解码神经响应时，该模型能预测小鼠行为。

    State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo
    
[^393]: 为离线增强学习和模仿学习提供指导性数据增强

    Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning. (arXiv:2310.18247v1 [cs.LG])

    [http://arxiv.org/abs/2310.18247](http://arxiv.org/abs/2310.18247)

    该论文提出了一种人工引导的数据增强框架（GuDA）用于提高演示学习模型的性能。

    

    演示学习是一种使用专家演示来学习机器人控制策略的流行技术。然而，获取专家级演示的难度限制了演示学习方法的适用性：现实世界的数据收集通常很昂贵，并且演示的质量很大程度上取决于演示者的能力和安全问题。一些工作利用数据增强来廉价生成额外的演示数据，但大多数数据增强方法以随机方式生成增强数据，最终产生高度次优的数据。在这项工作中，我们提出了一种人工引导的数据增强框架（GuDA），用于生成高质量的增强数据。GuDA的关键洞见是，虽然演示动作序列可能很难展示产生专家数据所需的动作序列，但用户经常可以轻松地辨别出增强轨迹段表示的任务进展。因此，用户可以施加一系列s

    Learning from demonstration (LfD) is a popular technique that uses expert demonstrations to learn robot control policies. However, the difficulty in acquiring expert-quality demonstrations limits the applicability of LfD methods: real-world data collection is often costly, and the quality of the demonstrations depends greatly on the demonstrator's abilities and safety concerns. A number of works have leveraged data augmentation (DA) to inexpensively generate additional demonstration data, but most DA works generate augmented data in a random fashion and ultimately produce highly suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a human-guided DA framework that generates expert-quality augmented data. The key insight of GuDA is that while it may be difficult to demonstrate the sequence of actions required to produce expert data, a user can often easily identify when an augmented trajectory segment represents task progress. Thus, the user can impose a series of s
    
[^394]: 理解何时动力学不变的数据增强对模型无关的强化学习更新有益

    Understanding when Dynamics-Invariant Data Augmentations Benefit Model-Free Reinforcement Learning Updates. (arXiv:2310.17786v1 [cs.LG])

    [http://arxiv.org/abs/2310.17786](http://arxiv.org/abs/2310.17786)

    本文研究了在稀疏奖励任务中，动力学不变的数据增强函数对模型无关的强化学习更新的影响。实验结果表明，增加状态-动作覆盖率可以提高学习效果。

    

    最近，数据增强（DA）已经成为一种利用领域知识以低成本产生额外数据的强化学习（RL）任务的方法，往往能够显著提高数据效率。虽然之前的研究已经证明将增强数据直接纳入模型无关的RL更新中的效用，但目前还不太清楚特定的DA策略何时会提高数据效率。本文旨在找出DA的一般方面，以确定导致观察到的学习改进的因素。我们的研究集中在具有动力学不变的数据增强函数的稀疏奖励任务上，这是理解DA及其与RL训练整合的更一般的理解的一个初始步骤。实验上，我们分离了三个与DA相关的方面：状态-动作覆盖率，奖励密度和每次更新生成的增强转换的数量（增强回放率）。根据我们的实验，我们得出两个结论：(1) 增加状态-动作覆盖率可改进学习效果；

    Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating augmented data directly into model-free RL updates, it is not well-understood when a particular DA strategy will improve data efficiency. In this paper, we seek to identify general aspects of DA responsible for observed learning improvements. Our study focuses on sparse-reward tasks with dynamics-invariant data augmentation functions, serving as an initial step towards a more general understanding of DA and its integration into RL training. Experimentally, we isolate three relevant aspects of DA: state-action coverage, reward density, and the number of augmented transitions generated per update (the augmented replay ratio). From our experiments, we draw two conclusions: (1) increasing state-action c
    
[^395]: 《低秩适应的表达能力》

    The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])

    [http://arxiv.org/abs/2310.17513](http://arxiv.org/abs/2310.17513)

    本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。

    

    低秩适应（LoRA）是一种参数高效的微调方法，利用矩阵的低秩适应性，在微调预训练模型（如大型语言模型和扩散模型）中得到了广泛应用。尽管在实践中取得了巨大成功，但是LoRA的理论基础在很大程度上尚未得到探索。本文通过从理论角度分析LoRA的表达能力，首次尝试弥合这一差距。我们证明了对于全连接神经网络，如果LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度），则LoRA可以使任何模型f准确表示任何较小的目标模型f。当LoRA-rank低于阈值时，我们还量化了逼近误差。对于Transformer网络，我们证明任何模型可以通过rank-（嵌入大小/ 2）的LoRA适配器适应于相同大小的目标模型。

    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
    
[^396]: 带有平稳扩散的因果建模

    Causal Modeling with Stationary Diffusions. (arXiv:2310.17405v1 [cs.LG])

    [http://arxiv.org/abs/2310.17405](http://arxiv.org/abs/2310.17405)

    本文提出了一种新颖的因果推断方法，使用随机微分方程建模系统行为，不需要因果图的形式化。在多种情况下，该方法比传统方法更好地推广到未见干预的变量。

    

    我们提出了一种新颖的因果推断方法。与使用因果图的结构方程不同，我们学习随机微分方程(SDE)，其平稳密度可以模拟系统在干预下的行为。这些平稳扩散模型不需要因果图的形式化，更不需要常见的无环性假设。我们展示了在多种情况下，它们比传统方法更好地推广到变量上的未见干预。我们的推断方法基于一个新的理论结果，该结果在再生核希尔伯特空间中表达了扩散的生成器的稳定条件。由此产生的核从稳态偏离(KDS)是一个值得独立关注的客观函数。

    We develop a novel approach towards causal inference. Rather than structural equations over a causal graph, we learn stochastic differential equations (SDEs) whose stationary densities model a system's behavior under interventions. These stationary diffusion models do not require the formalism of causal graphs, let alone the common assumption of acyclicity. We show that in several cases, they generalize to unseen interventions on their variables, often better than classical approaches. Our inference method is based on a new theoretical result that expresses a stationarity condition on the diffusion's generator in a reproducing kernel Hilbert space. The resulting kernel deviation from stationarity (KDS) is an objective function of independent interest.
    
[^397]: 超越独立同分布权重：稀疏和低秩深度神经网络也是高斯过程

    Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes. (arXiv:2310.16597v1 [stat.ML])

    [http://arxiv.org/abs/2310.16597](http://arxiv.org/abs/2310.16597)

    本文扩展了之前的研究，将证明的范围从独立同分布权重扩展到了更大的权重分布类别(PSEUDO-IID)，包括低秩和稀疏设置。作者发现使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。这些结果可以帮助我们识别更广泛的神经网络的边界混沌状态，并进行性能调优。

    

    无限宽神经网络已经被证明是一个有用且可管理的数学模型，使得我们能够理解深度学习中出现的许多现象。其中一个例子是随机深层网络收敛到高斯过程，从而能够对激活函数和网络权重选择对训练动态的影响进行严格分析。在本文中，我们将Matthews等人(2018)的开创性证明扩展到更大的初始权重分布类别(我们称之为PSEUDO-IID)，其中包括独立同分布和正交权重的已有情况，以及因其计算加速优势而受到赞誉的新兴低秩和结构稀疏设置。我们证明，使用PSEUDO-IID分布初始化的全连接和卷积网络在方差上都是等效的。利用我们的结果，可以识别更广泛的神经网络的边界混沌状态，并调整它们的临界性，以增强训练性能。

    The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that allows a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al. (2018) to a larger class of initial weight distributions (which we call PSEUDO-IID), including the established cases of IID and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialized with PSEUDO-IID distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge-of-Chaos for a broader class of neural networks and tune them at criticality in order to enhance the
    
[^398]: MARVEL: 用于大规模可变速限的多智能体强化学习

    MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits. (arXiv:2310.12359v1 [cs.MA])

    [http://arxiv.org/abs/2310.12359](http://arxiv.org/abs/2310.12359)

    MARVEL是一个多智能体强化学习框架，可以实现利用常见数据在高速公路走廊上进行大规模可变速限控制。它通过奖励结构和智能体之间的协调，提高交通安全性和流动性。与无控制情况相比，MARVEL可以提高交通安全性63.4%，提高交通流动性14.6%。

    

    可变速限（VSL）控制是一种提高安全性和流动性的有前途的交通管理策略。本工作介绍了MARVEL，这是一个使用仅有常见可用数据实现高速公路走廊大规模VSL控制的多智能体强化学习（MARL）框架。智能体通过包括对交通状况的适应性、安全性和流动性在内的奖励结构进行学习，实现了智能体之间的协调。该框架通过在所有VSL智能体之间共享参数，可以扩展到包括许多立柱的走廊。智能体在一个基于一个短的高速公路路段的微仿真环境中进行训练，该路段有8个立柱，跨越7英里，并在纳什维尔附近的I-24上有34个立柱，跨越17英里进行测试。与无控制情况相比，MARVEL将交通安全性提高了63.4%，并将交通流动性提高了14.6%，与已在I-24上部署的最新算法相比。进行了一项可解释性分析。

    Variable speed limit (VSL) control is a promising traffic management strategy for enhancing safety and mobility. This work introduces MARVEL, a multi-agent reinforcement learning (MARL) framework for implementing large-scale VSL control on freeway corridors using only commonly available data. The agents learn through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility; enabling coordination among the agents. The proposed framework scales to cover corridors with many gantries thanks to a parameter sharing among all VSL agents. The agents are trained in a microsimulation environment based on a short freeway stretch with 8 gantries spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 14.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. An explainability analysis is underta
    
[^399]: 从词语和练习到健康：用于自我依恋技术的波斯语聊天机器人

    From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique. (arXiv:2310.09362v1 [cs.HC])

    [http://arxiv.org/abs/2310.09362](http://arxiv.org/abs/2310.09362)

    这项研究开发了一个能够发出声音的波斯语聊天机器人，用于指导用户进行基于依恋理论的自我依恋技术。通过使用规则和分类模块，聊天机器人可以理解用户输入并推荐适当的自我依恋练习。该研究还开发了一种准确率超过92%的情感分析模块，以识别用户情感。这项工作有助于在后疫情时代提供数字心理疗法的替代方案。

    

    在后疫情时代，社交孤立和抑郁焦虑症的患病率攀升的背景下，基于数字心理疗法的对话代理相对于传统疗法会发挥重要的影响。本文中，我们开发了一个能够发出声音的波斯语聊天机器人，指导用户进行自我依恋(Self-Attachment, SAT)技术，这是一种基于依恋理论的新型、自我管理、全面的心理技术。我们的聊天机器人使用一系列基于规则和分类的模块来理解用户在对话中的输入，并相应地导航对话流程图，根据用户的情感和心理状态推荐适当的SAT练习。具体而言，我们收集了超过6,000次话语的数据集，并开发了一种新颖的情感分析模块，可以将用户的情感分为12个类别，准确率超过92%。为了保持对话的新颖和吸引力，聊天机器人的回答是从大量话语数据集中检索得到的。

    In the wake of the post-pandemic era, marked by social isolation and surging rates of depression and anxiety, conversational agents based on digital psychotherapy can play an influential role compared to traditional therapy sessions. In this work, we develop a voice-capable chatbot in Farsi to guide users through Self-Attachment (SAT), a novel, self-administered, holistic psychological technique based on attachment theory. Our chatbot uses a dynamic array of rule-based and classification-based modules to comprehend user input throughout the conversation and navigates a dialogue flowchart accordingly, recommending appropriate SAT exercises that depend on the user's emotional and mental state. In particular, we collect a dataset of over 6,000 utterances and develop a novel sentiment-analysis module that classifies user sentiment into 12 classes, with accuracy above 92%. To keep the conversation novel and engaging, the chatbot's responses are retrieved from a large dataset of utterances c
    
[^400]: LLM训练中的分词选择：微不足道还是至关重要？

    Tokenizer Choice For LLM Training: Negligible or Crucial?. (arXiv:2310.08754v1 [cs.LG])

    [http://arxiv.org/abs/2310.08754](http://arxiv.org/abs/2310.08754)

    在LLM训练中，分词器的选择对模型的后续性能、成本有着显著影响，常见的分词器评估指标不一定预测模型的性能。

    

    近期LLM的成功主要是由于策划训练数据集、扩展模型架构和数据集规模，以及预训练目标的进步，而分词器的影响则是一个盲点。通过对24个单语和多语言LLM进行训练，并对不同的分词器算法和参数进行大范围实验，我们对分词器选择对LLM的后续性能、训练和推理成本的影响进行了全面研究。我们的研究表明，分词器选择对模型的后续性能、训练和推理成本有着显著影响。特别是，我们发现常见的分词器评估指标（如丰富度和平等性）并不总是对模型的后续性能具有预测能力，这使得这些指标成为对分词器评估的可疑选择。此外，我们还展示了针对五种最常见的欧洲语言训练的多语言分词器需要词汇表的大小。

    The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary si
    
[^401]: 带有性能保证的机组启停预测器：支持向量机分类器

    Unit Commitment Predictor With a Performance Guarantee: A Support Vector Machine Classifier. (arXiv:2310.08601v1 [math.OC])

    [http://arxiv.org/abs/2310.08601](http://arxiv.org/abs/2310.08601)

    本文提出了一个带有性能保证的机组启停预测器，通过学习和预测常规机组的启停决策，系统运营商可以在求解器中使用预热启动并显著加速计算。对于预测，使用了适当正则化的核化支持向量机分类器，能将计算时间减少1.7倍。

    

    系统运营商通常需要在有限的时间内解决大规模的机组启停问题进行计算。本文提供了一种实用的解决方案，展示了通过学习和预测常规机组的启停决策，系统运营商有可能在求解器中使用预热启动并显著加速计算。对于预测，我们训练了线性和核化支持向量机分类器，提供了一种样本外性能保证（如果适当正则化），转化为分布鲁棒分类器。对于机组启停问题，我们求解了一个混合整数二阶锥问题。基于IEEE 6节点和118节点测试系统的结果表明，适当正则化的核化支持向量机优于其他分类器，将计算时间减少了1.7倍。此外，如果存在严格的计算限制，没有预热启动的机组启停问题与最优解的距离较远。

    The system operators usually need to solve large-scale unit commitment problems within limited time frame for computation. This paper provides a pragmatic solution, showing how by learning and predicting the on/off commitment decisions of conventional units, there is a potential for system operators to warm start their solver and speed up their computation significantly. For the prediction, we train linear and kernelized support vector machine classifiers, providing an out-of-sample performance guarantee if properly regularized, converting to distributionally robust classifiers. For the unit commitment problem, we solve a mixed-integer second-order cone problem. Our results based on the IEEE 6-bus and 118-bus test systems show that the kernelized SVM with proper regularization outperforms other classifiers, reducing the computational time by a factor of 1.7. In addition, if there is a tight computational limit, while the unit commitment problem without warm start is far away from the o
    
[^402]: 内存一致的神经网络在模仿学习中的应用

    Memory-Consistent Neural Networks for Imitation Learning. (arXiv:2310.06171v1 [cs.LG])

    [http://arxiv.org/abs/2310.06171](http://arxiv.org/abs/2310.06171)

    本文介绍了一种内存一致的神经网络模型，在模仿学习中使用专家演示训练策略。该模型通过对输出结果进行硬约束，避免了错误的累积现象，保证了策略效果的上界。

    

    模仿学习利用专家演示大大简化了策略合成的过程。然而，对于这种模仿策略来说，远离训练样本的错误尤为关键。即使在策略的行动输出中出现罕见的错误，由于这些错误会导致不熟悉的未来状态，策略在这些状态下仍更容易出错，最终导致任务失败。本文重新审视了简单的监督式“行为克隆”方法，能够方便地仅通过预先记录的演示来训练策略，并设计了一种能够抵消错误累积现象的模型类。我们的“内存一致神经网络”(MCNN)输出被强制约束在与典型的“内存”训练样本相关的明确指定的允许区域内。我们提供了MCNN策略导致的次优性差距的保证上界。通过在9个模仿学习任务上使用MCNNs，采用MLP、Transformer等方法。

    Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, 
    
[^403]: 扩散模型中的泛化性质源于几何自适应的谐波表示

    Generalization in diffusion models arises from geometry-adaptive harmonic representation. (arXiv:2310.02557v1 [cs.CV])

    [http://arxiv.org/abs/2310.02557](http://arxiv.org/abs/2310.02557)

    通过分析基于分数的反向扩散算法生成的高质量样本的研究结果，我们发现尽管存在维度灾难，但为了降噪而训练的深度神经网络可以学习到高维密度。此外，我们展示了在训练集的非重叠子集上训练的网络可以学习到相同的密度，从而证明了DNN架构和训练算法中的归纳偏差与数据分布的一致性。

    

    使用基于分数的反向扩散算法生成的高质量样本提供了证据，表明尽管存在维度灾难，为了降噪而训练的深度神经网络（DNN）可以学习高维密度。然而，关于训练集记忆化的最新报告引发了一个问题，即这些网络是否学习了数据的“真实”连续密度。我们在这里展示，训练在数据集的非重叠子集上的两个降噪DNN学习的几乎是相同的分数函数，从而学习了相同的密度，且仅需很少的训练图像。这种强大的泛化性证明了DNN架构和/或训练算法中的有力归纳偏差与数据分布的特性之间的一致性。我们分析了这些结果，展示了降噪器在适应于底层图像的基础上执行收缩操作。对这些基矢的检查揭示了沿轮廓和均匀图像区域的振荡谐波结构。

    High-quality samples generated with score-based reverse diffusion algorithms provide evidence that deep neural networks (DNN) trained for denoising can learn high-dimensional densities, despite the curse of dimensionality. However, recent reports of memorization of the training set raise the question of whether these networks are learning the "true" continuous density of the data. Here, we show that two denoising DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images. This strong generalization demonstrates an alignment of powerful inductive biases in the DNN architecture and/or training algorithm with properties of the data distribution. We analyze these, demonstrating that the denoiser performs a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous image region
    
[^404]: L2MAC：大规模语言模型自动计算机用于无限代码生成

    L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])

    [http://arxiv.org/abs/2310.02003](http://arxiv.org/abs/2310.02003)

    L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。

    

    基于Transformer的大型语言模型（LLM）受到底层Transformer架构固定上下文窗口的限制，阻碍了它们生成长且逻辑一致的代码的能力。增强记忆的LLM是一个有前途的解决方案，但目前的方法无法处理长时间的代码生成任务，因为它们要么只关注于读取内存并将其演变为新内存的连接，要么使用非常专门的内存，无法适应其他领域。本文介绍了L2MAC，这是一种基于LLM的长且一致代码生成的实用存储程序自动计算机。它的内存有两个组成部分：指令注册表，其中填充了一个解决用户给定任务的提示程序，以及文件存储，其中包含最终和中间输出。每个指令由单独的LLM实例执行，其上下文由控制单元管理，能够精确读取和写入内存，以确保有效的整合。

    Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
    
[^405]: 进向鲁棒度评估图神经网络解释性的方法

    Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks. (arXiv:2310.01820v1 [cs.LG])

    [http://arxiv.org/abs/2310.01820](http://arxiv.org/abs/2310.01820)

    本文研究了评估图神经网络解释性的鲁棒度的方法，并指出了现有度量方法的局限性。

    

    图神经网络（GNN）是一种利用图数据中的依赖结构通过节点之间的消息传递进行建模的神经网络模型。GNN已经成为分析图结构数据的关键架构，在敏感领域的广泛应用要求对其决策过程有全面的理解，这就需要一个GNN可解释性的框架。为了评估GNN解释函数的性能，需要提供可靠的保真度度量。本文研究了这一基础性挑战，重点在现有的保真度度量方法中揭示了潜在的局限性，包括Fid_+，Fid_-和Fid_Δ。具体而言，本文介绍了一个正式的信息论解释性定义，并证明了现有度量方法的限制

    Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes -- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metri
    
[^406]: PolySketchFormer:基于草图的多项式核变换器加速Transformer

    PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels. (arXiv:2310.01655v1 [cs.LG])

    [http://arxiv.org/abs/2310.01655](http://arxiv.org/abs/2310.01655)

    本文通过使用多项式函数和多项式草图，实现了一个快速注意力机制PolySketchFormer，以突破Transformer架构中注意力机制的二次复杂性难题，无需假设注意力矩阵具有稀疏结构，并提出了高效的基于块的算法。

    

    Transformer架构中注意力机制的二次复杂性一直是扩展大型基础模型进行长上下文任务的瓶颈。实际上，最近的理论结果表明，在假设强指数时间假设的情况下，近似softmax注意力机制的输出在次二次时间内是困难的。本文通过用多项式函数和多项式草图替代softmax来突破这个理论障碍。特别是，我们展示了从随机数值线性代数文献中的多项式核的草图可以用于近似多项式注意力，从而实现了显著更快的注意力机制，而不需要假设注意力矩阵具有稀疏结构，这在许多先前的工作中已经完成。此外，我们提出了一种高效的基于块的算法，该算法使我们能够将因果掩码应用于注意力矩阵，而无需显式地计算$n \times n$注意力矩阵并计算输出。

    The quadratic complexity of attention in transformer architectures remains a big bottleneck in scaling up large foundation models for long context. In fact, recent theoretical results show the hardness of approximating the output of softmax attention mechanism in sub-quadratic time assuming Strong Exponential Time Hypothesis. In this paper, we show how to break this theoretical barrier by replacing softmax with a polynomial function and polynomial sketching. In particular we show that sketches for Polynomial Kernel from the randomized numerical linear algebra literature can be used to approximate the polynomial attention which leads to a significantly faster attention mechanism without assuming any sparse structure for the attention matrix that has been done in many previous works.  In addition, we propose an efficient block-based algorithm that lets us apply the causal mask to the attention matrix without explicitly realizing the $n \times n$ attention matrix and compute the output of
    
[^407]: SmartPlay: 一种用于评估LLMs作为智能Agent能力的基准

    SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])

    [http://arxiv.org/abs/2310.01557](http://arxiv.org/abs/2310.01557)

    SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。

    

    最近的大型语言模型(LLMs)在智能Agent和下一代自动化方面展示了巨大的潜力，但目前缺乏一个系统化的基准来评估LLMs作为Agent的能力。我们介绍了SmartPlay：一个具有挑战性的基准和评估LLMs作为Agent的方法论。SmartPlay包括6个不同的游戏，包括剪刀石头布、汉诺塔、Minecraft等。每个游戏都具有独特的设置，提供最多20个评估设置和无限的环境变化。SmartPlay中的每个游戏都独特地挑战了智能LLM Agent的9个重要能力的子集，包括对对象依赖的推理、提前规划、空间推理、从历史中学习和理解随机性。每个游戏测试的能力集的区别使我们能够单独分析每个能力。SmartPlay不仅是评估LLM Agent整体性能的严格测试场地，而且也是评估Agent在不同能力方面的性能的一个重要工具。

    Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
    
[^408]: CCA家族的高效算法：无约束目标与无偏梯度

    Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])

    [http://arxiv.org/abs/2310.01012](http://arxiv.org/abs/2310.01012)

    本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。

    

    典型相关分析（CCA）方法在多视角学习中具有基础性作用。正则化线性CCA方法可以看作是偏最小二乘（PLS）的推广，并与广义特征值问题（GEP）框架统一。然而，这些线性方法的传统算法在大规模数据上计算上是不可行的。深度CCA的扩展显示出很大的潜力，但目前的训练过程缓慢且复杂。我们首先提出了一个描述GEPs的顶级子空间的新颖无约束目标。我们的核心贡献是一系列快速算法，用随机梯度下降（SGD）应用于相应的CCA目标，从而获得随机PLS、随机CCA和深度CCA。这些方法在所有标准CCA和深度CCA基准测试中显示出比先前最先进方法更快的收敛速度和更高的相关性恢复。这样的速度使我们能够首次进行大规模生物数据的PLS分析。

    The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
    
[^409]: 学习如何提供注重依从性的建议

    Learning to Make Adherence-Aware Advice. (arXiv:2310.00817v1 [stat.ML])

    [http://arxiv.org/abs/2310.00817](http://arxiv.org/abs/2310.00817)

    本文提出了一种顺序决策模型，考虑了人的依从程度和机器提供建议的时机，并提供了学习算法来学习最佳的建议策略。

    

    随着人工智能系统在人类决策中扮演越来越重要的角色，人工智能与人类之间的交互存在挑战。由于没有充分考虑到人类忽视人工智能建议和人工智能选择性提供建议的需求，一个挑战就来自于底层人工智能策略的不佳表现。本文提出了一个顺序决策模型，该模型考虑了人类的依从程度（即人类遵循/拒绝机器建议的概率），并引入了一个推迟选项，使得机器在最合适的时候可以暂时不提供建议。我们提供了学习算法，可以学习最佳的建议策略，并仅在关键时刻提供建议。与问题不可知的强化学习算法相比，我们的专门化学习算法不仅具有更好的理论收敛性能，而且在实证性能上表现出色。

    As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.
    
[^410]: 在规划中结合空间和时间抽象以实现更好的泛化

    Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00229](http://arxiv.org/abs/2310.00229)

    Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。

    

    受到人类有意识规划的启发，我们提出了Skipper，这是一个利用时空抽象来推广在新情境中学到的技能的基于模型的强化学习代理。它自动将给定任务分解为更小、更可管理的子任务，从而实现稀疏决策和对环境相关部分的专注计算。这依赖于从回溯中学习得到的表示为有向图的抽象代理问题的提取。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法在哪些方面有望提供帮助。针对泛化的实验验证了Skipper在零样本泛化方面与现有最先进的分层规划方法相比的显著优势。

    Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
    
[^411]: 增强随机平滑的Lipschitz-方差-边界权衡

    The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])

    [http://arxiv.org/abs/2309.16883](http://arxiv.org/abs/2309.16883)

    本文提出了一个增强随机平滑的方法，通过研究随机平滑引入的方差与分类器的Lipschitz常数和边界之间的关系，以及采用单纯形投影技术来增加认证鲁棒半径。

    

    面对噪声输入和对抗性攻击时，深度神经网络的实际应用受到其不稳定的预测的阻碍。在这种情况下，认证半径是模型鲁棒性的关键指标。然而，如何设计一个具有足够认证半径的高效分类器呢？随机平滑通过在输入中注入噪声来获得平滑且更鲁棒的分类器的框架提供了有希望的解决方案。本文首先展示了随机平滑引入的方差与分类器的另外两个重要属性，即其Lipschitz常数和边界之间的密切关系。更具体地说，我们的工作强调了基分类器的Lipschitz常数对平滑分类器和经验方差的双重影响。此外，为了增加认证鲁棒半径，我们引入了一种不同的单纯形投影技术，以便通过Bernst的方差-边界权衡来利用基分类器。

    Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
    
[^412]: 从复杂到清晰：通过Clifford的几何代数和凸优化的分析表达深度神经网络的权重

    From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])

    [http://arxiv.org/abs/2309.16512](http://arxiv.org/abs/2309.16512)

    本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。

    

    本文介绍了一种基于几何（Clifford）代数和凸优化的神经网络分析方法。我们展示了当使用标准正则化损失进行训练时，深度ReLU神经网络的最优权重由训练样本的楔积给出。此外，训练问题可简化为对楔积特征进行凸优化，在其中编码训练数据集的几何结构。该结构以数据向量生成的三角形和平行体的有符号体积表示。凸问题通过$\ell_1$正则化找到样本的一个小子集，以发现仅相关的楔积特征。我们的分析提供了对深度神经网络内部工作机制的新视角，并揭示了隐藏层的作用。

    In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
    
[^413]: 无监督神经网络在逆问题中的收敛和恢复性能保证

    Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems. (arXiv:2309.12128v1 [cs.LG])

    [http://arxiv.org/abs/2309.12128](http://arxiv.org/abs/2309.12128)

    本研究通过探索连接理论和实践，提供了无监督神经网络在解决逆问题中的收敛和恢复性能保证。同时，我们还得出了对于两层具有平滑激活函数的深度逆先验网络的超参数化界限，该网络将从我们的保证中受益。

    

    近年来，神经网络已成为解决逆问题的主要方法。虽然已经有很多这样的方法被提出来经验性地解决逆问题，但我们仍然缺乏对这些方法的明确理论保证。另一方面，许多研究已经证明，通过过参数化来控制神经切向核，神经网络可以在更通用的设置下收敛到最优解。在这项工作中，我们探索如何连接这两个领域，并为无监督前馈多层神经网络解决逆问题的训练过程提供确定性的收敛和恢复性能保证。我们还推导出超参数化界限，在这些界限下，具有平滑激活函数的两层深度逆先验网络将受益于我们的保证。

    Neural networks have become a prominent approach to solve inverse problems in recent years. While a plethora of such methods was developed to solve inverse problems empirically, we are still lacking clear theoretical guarantees for these methods. On the other hand, many works proved convergence to optimal solutions of neural networks in a more general setting using overparametrization as a way to control the Neural Tangent Kernel. In this work we investigate how to bridge these two worlds and we provide deterministic convergence and recovery guarantees for the class of unsupervised feedforward multilayer neural networks trained to solve inverse problems. We also derive overparametrization bounds under which a two-layers Deep Inverse Prior network with smooth activation function will benefit from our guarantees.
    
[^414]: DreamLLM：协同的多模态理解与创作

    DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])

    [http://arxiv.org/abs/2309.11499](http://arxiv.org/abs/2309.11499)

    DreamLLM是一种学习框架，实现了多模态理解与创作的协同效应。通过直接采样生成语言和图像的生成模型，避免了信息损失，并获得了更全面的多模态理解。此外，DreamLLM能够生成自由形式交织内容，展现了其在零样本多模态学习任务中的卓越性能。

    

    本文介绍了DreamLLM，一种学习框架，它首次实现了多模态大型语言模型（MLLMs），利用了多模态理解与创作之间经常被忽视的协同效应。DreamLLM遵循两个基本原则。第一个原则专注于通过在原始多模态空间中进行直接采样来生成语言和图像后验的生成建模。这种方法避免了像CLIP这样的外部特征提取器所固有的限制和信息损失，并获得了更全面的多模态理解。其次，DreamLLM促进了原始的、交织的文件生成，对文本和图像内容以及非结构化布局进行建模。这使得DreamLLM能够有效地学习所有条件、边缘和联合多模态分布。作为结果，DreamLLM是第一个能够生成自由形式交织内容的MLLM。综合实验突显了DreamLLM作为零样本多模态学习任务的卓越性能。

    This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal
    
[^415]: 浅层神经网络的几何结构和基于${\mathcal L}^2$代价最小化的构造方法

    Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])

    [http://arxiv.org/abs/2309.10370](http://arxiv.org/abs/2309.10370)

    本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。

    

    本文给出了一个几何解释：浅层神经网络的结构由一个隐藏层、一个斜坡激活函数、一个${\mathcal L}^2$谱范类（或者Hilbert-Schmidt）的代价函数、输入空间${\mathbb R}^M$、输出空间${\mathbb R}^Q$（其中$Q\leq M$），以及训练输入样本数量$N>QM$所特征。我们证明了代价函数的最小值具有$O(\delta_P)$的上界，其中$\delta_P$衡量了训练输入的信噪比。我们使用适应于属于同一输出向量$y_j$的训练输入向量$\overline{x_{0,j}}$的投影来获得近似的优化器，其中$j=1,\dots,Q$。在特殊情况$M=Q$下，我们明确确定了代价函数的一个确切退化局部最小值；这个尖锐的值与对于$Q\leq M$所获得的上界之间有一个相对误差$O(\delta_P^2)$。上界证明的方法提供了一个构造性训练的网络；我们证明它测度了$Q$维空间中的给定输出。

    In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N>QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
    
[^416]: Primal-Dual $\ell_0$-约束稀疏指数跟踪

    Primal-Dual $\ell_0$-Constrained Sparse Index Tracking. (arXiv:2309.10152v1 [q-fin.PM])

    [http://arxiv.org/abs/2309.10152](http://arxiv.org/abs/2309.10152)

    本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，可以轻松控制投资组合中资产数量的上限。

    

    稀疏指数跟踪是一种重要的被动投资组合管理策略，它通过构建稀疏投资组合来跟踪金融指数。相比于全仓投资组合，稀疏投资组合在降低交易成本和避免不流动资产方面更具优势。为了强制投资组合的稀疏性，传统研究提出了基于$\ell_p$-范数正则化的公式，作为$\ell_0$-范数正则化的连续替代。尽管这样的公式可以用来构建稀疏投资组合，但在实际投资中却不易使用，因为细致的参数调整来指定投资组合中资产数量的上限是艰难且耗时的。本文提出了一种新的稀疏指数跟踪问题的形式化，使用了$\ell_0$-范数约束，从而可以轻松控制投资组合中资产数量的上限。此外，我们的形式化允许在投资组合稀疏性和换手率之间进行选择。

    Sparse index tracking is one of the prominent passive portfolio management strategies that construct a sparse portfolio to track a financial index. A sparse portfolio is desirable over a full portfolio in terms of transaction cost reduction and avoiding illiquid assets. To enforce the sparsity of the portfolio, conventional studies have proposed formulations based on $\ell_p$-norm regularizations as a continuous surrogate of the $\ell_0$-norm regularization. Although such formulations can be used to construct sparse portfolios, they are not easy to use in actual investments because parameter tuning to specify the exact upper bound on the number of assets in the portfolio is delicate and time-consuming. In this paper, we propose a new problem formulation of sparse index tracking using an $\ell_0$-norm constraint that enables easy control of the upper bound on the number of assets in the portfolio. In addition, our formulation allows the choice between portfolio sparsity and turnover spa
    
[^417]: 两层神经网络上逻辑回归代价函数的全局收敛性

    Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets. (arXiv:2309.09258v1 [cs.LG])

    [http://arxiv.org/abs/2309.09258](http://arxiv.org/abs/2309.09258)

    本文首次证明了在深度为2的神经网络上，适当正则化的逻辑回归代价函数通过随机梯度下降（SGD）能够收敛到全局极小值，这适用于任意数据和具有充分平滑且有界激活函数。同时，我们还证明了连续时间SGD的指数级快速收敛速度，该结果也适用于光滑无界的激活函数。

    

    在本文中，我们首次证明了随机梯度下降（SGD）对于适当正则化的深度为2的神经网络的逻辑回归代价函数能够收敛到全局极小值，对于任意数据和具有充分平滑且有界激活函数（如sigmoid和tanh）。我们还证明了连续时间SGD的指数级快速收敛速度，该结果也适用于光滑无界的激活函数（如SoftPlus）。我们的关键思想是证明了在恒定大小的神经网络上存在Frobenius范数正则化的逻辑回归代价函数，这些函数是"Villani函数"，从而能够构建在最近对于此类目标函数上分析SGD的研究进展上。

    In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates with adequately smooth and bounded activations like sigmoid and tanh. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence of Frobenius norm regularized logistic loss functions on constant-sized neural nets which are "Villani functions" and thus be able to build on recent progress with analyzing SGD on such objectives.
    
[^418]: 指引的方法：利用学习到的ICP权重改进雷达-激光雷达定位

    Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights. (arXiv:2309.08731v1 [cs.RO])

    [http://arxiv.org/abs/2309.08731](http://arxiv.org/abs/2309.08731)

    本文提出了一种深度学习方法，通过学习到的ICP权重优化雷达-激光雷达的定位，从而改善了雷达测量对激光雷达地图的定位效果。这一方法在保持高质量地图定位性能的同时，提高了在降水和大雾等恶劣天气条件下的定位准确性。

    

    本文提出了一种基于深度学习的新方法，用于改进雷达测量对激光雷达地图的定位。虽然目前定位的技术水平是将激光雷达数据与激光雷达地图进行匹配，但是雷达被认为是一种有前途的替代方法，因为它对降水和大雾等恶劣天气具有更强的韧性。为了利用现有的高质量激光雷达地图，同时在恶劣天气下保持性能，将雷达数据与激光雷达地图进行匹配具有重要意义。然而，由于雷达测量中存在的独特伪影，雷达-激光雷达定位一直难以达到与激光雷达-激光雷达系统相媲美的性能，使其无法用于自动驾驶。本工作在基于ICP的雷达-激光雷达定位系统基础上，包括一个学习的预处理步骤，根据高层次的扫描信息对雷达点进行加权。将经过验证的分析方法与学习到的权重相结合，减小了雷达定位中的误差。

    This paper presents a novel deep-learning-based approach to improve localizing radar measurements against lidar maps. Although the state of the art for localization is matching lidar data to lidar maps, radar has been considered as a promising alternative, as it is potentially more resilient against adverse weather such as precipitation and heavy fog. To make use of existing high-quality lidar maps, while maintaining performance in adverse weather, matching radar data to lidar maps is of interest. However, owing in part to the unique artefacts present in radar measurements, radar-lidar localization has struggled to achieve comparable performance to lidar-lidar systems, preventing it from being viable for autonomous driving. This work builds on an ICP-based radar-lidar localization system by including a learned preprocessing step that weights radar points based on high-level scan information. Combining a proven analytical approach with a learned weight reduces localization errors in rad
    
[^419]: 带有Beta散度的深度非负矩阵分解

    Deep Nonnegative Matrix Factorization with Beta Divergences. (arXiv:2309.08249v1 [cs.LG])

    [http://arxiv.org/abs/2309.08249](http://arxiv.org/abs/2309.08249)

    本文提出了一种使用Beta散度的深度非负矩阵分解方法，应用于面部特征提取、文档主题识别和高光谱图像材料识别。

    

    深度非负矩阵分解（deep NMF）最近成为一种有价值的技术，用于在不同尺度上提取多层特征。然而，所有现有的深度NMF模型和算法主要都以最小二乘误差为评估标准，这可能不是评估多样化数据集近似质量的最合适指标。例如，当处理音频信号和文档等数据类型时，广泛认可的是$\beta$-divergences提供了更适合的替代方案。本文基于$\beta$-divergences开发了新的深度NMF模型和算法，并将这些技术应用于面部特征提取、文档集合中的主题识别以及高光谱图像中材料的识别。

    Deep Nonnegative Matrix Factorization (deep NMF) has recently emerged as a valuable technique for extracting multiple layers of features across different scales. However, all existing deep NMF models and algorithms have primarily centered their evaluation on the least squares error, which may not be the most appropriate metric for assessing the quality of approximations on diverse datasets. For instance, when dealing with data types such as audio signals and documents, it is widely acknowledged that $\beta$-divergences offer a more suitable alternative. In this paper, we develop new models and algorithms for deep NMF using $\beta$-divergences. Subsequently, we apply these techniques to the extraction of facial features, the identification of topics within document collections, and the identification of materials within hyperspectral images.
    
[^420]: 基于高级信号处理和机器学习的声音相关呼吸疾病分类用于辅助诊断支持

    Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support. (arXiv:2309.07183v1 [eess.SP])

    [http://arxiv.org/abs/2309.07183](http://arxiv.org/abs/2309.07183)

    本研究利用声音数据训练多个机器学习模型来对呼吸疾病进行分类。方法采用经验模态分解和频谱分析来提取与心血管和呼吸模式相关的生理信号，并通过特征提取和预测建模实现快速筛查和诊断支持。

    

    在全球医疗保健中，呼吸系统疾病是导致死亡的主要原因，这突显了迅速准确的诊断的需求。为了推进听诊的快速筛查技术，我们的研究侧重于利用其中一个最大的公开的呼吸音医学数据库来训练多种机器学习模型，能够对不同的健康状况进行分类。我们的方法结合了经验模态分解（EMD）和频谱分析，从声学数据中提取与心血管和呼吸模式密切相关的生理相关生物信号，使我们的方法与传统声学特征提取方法有所不同。我们使用功率谱密度分析和滤波技术来选择与潜在生理现象强相关的固有模态函数（IMFs）。这些生物信号经过全面的特征提取过程用于预测建模。首先，我们部署一个二分类模型

    In global healthcare, respiratory diseases are a leading cause of mortality, underscoring the need for rapid and accurate diagnostics. To advance rapid screening techniques via auscultation, our research focuses on employing one of the largest publicly available medical database of respiratory sounds to train multiple machine learning models able to classify different health conditions. Our method combines Empirical Mode Decomposition (EMD) and spectral analysis to extract physiologically relevant biosignals from acoustic data, closely tied to cardiovascular and respiratory patterns, making our approach apart in its departure from conventional audio feature extraction practices. We use Power Spectral Density analysis and filtering techniques to select Intrinsic Mode Functions (IMFs) strongly correlated with underlying physiological phenomena. These biosignals undergo a comprehensive feature extraction process for predictive modeling. Initially, we deploy a binary classification model t
    
[^421]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^422]: Compact：逼近复杂激活函数用于安全计算

    Compact: Approximating Complex Activation Functions for Secure Computation. (arXiv:2309.04664v1 [cs.CR])

    [http://arxiv.org/abs/2309.04664](http://arxiv.org/abs/2309.04664)

    Compact提出了一种逼近复杂激活函数的方法，可以与MPC技术高效配合使用，并且几乎不会损失模型准确性。

    

    安全多方计算（Secure multi-party computation，MPC）技术可以用于在用户查询部署在公共云上的深度神经网络（DNN）模型时提供数据隐私。最先进的MPC技术可以直接用于使用简单激活函数（AFs）的DNN模型，如ReLU。然而，为了最新的应用设计的DNN模型架构通常使用复杂且高度非线性的AFs。为这些复杂AFs设计高效的MPC技术是一个尚未解决的问题。为此，我们提出了Compact，它产生复杂AFs的分段多项式逼近，以便与最先进的MPC技术高效配合使用。Compact既不需要也不强加任何模型训练的限制，并且导致几乎相同的模型准确性。我们在使用流行的复杂AFs SiLU、GeLU和Mish的DNN架构的四个不同机器学习任务上对Compact进行了广泛评估。我们的实验结果表明，与传统方法相比，Compact几乎不会损失准确性。

    Secure multi-party computation (MPC) techniques can be used to provide data privacy when users query deep neural network (DNN) models hosted on a public cloud. State-of-the-art MPC techniques can be directly leveraged for DNN models that use simple activation functions (AFs) such as ReLU. However, DNN model architectures designed for cutting-edge applications often use complex and highly non-linear AFs. Designing efficient MPC techniques for such complex AFs is an open problem.  Towards this, we propose Compact, which produces piece-wise polynomial approximations of complex AFs to enable their efficient use with state-of-the-art MPC techniques. Compact neither requires nor imposes any restriction on model training and results in near-identical model accuracy. We extensively evaluate Compact on four different machine-learning tasks with DNN architectures that use popular complex AFs SiLU, GeLU, and Mish. Our experimental results show that Compact incurs negligible accuracy loss compared
    
[^423]: 受限制下的重新编程: 重新审视彩票式转移的高效可靠性

    Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets. (arXiv:2308.14969v1 [cs.LG])

    [http://arxiv.org/abs/2308.14969](http://arxiv.org/abs/2308.14969)

    重新审视彩票式转移的高效可靠性，研究了线性探测（LP）和视觉提示/重新编程（VP）方法在数据稀疏性和模型稀疏性方面的能力，并发现彩票式转移并非通用的重新编程器。

    

    在预训练预算巨大的基础模型时代，下游任务已经转向了高效快速适应的叙述。对于计算机视觉领域的基于分类的任务，最高效的方法是线性探测（LP）和视觉提示/重新编程（VP）; 前者旨在通过在预训练模型提取的特征上学习线性头部分类器，而后者将输入数据映射到最初在其上进行预训练的源数据领域。尽管广泛的研究已经证明了LP和VP在下游性能方面的差异，但我们通过稀疏度轴来探索这两种方法的能力: (a) 数据稀疏性: 少样本自适应的影响，以及 (b) 模型稀疏性: 彩票式转移的影响。我们证明了彩票式转移不是通用的重新编程器，即对于某些目标数据集，重新编程彩票式转移会产生显著效果。

    In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields signif
    
[^424]: OmniQuant：用于大型语言模型的全向校准量化

    OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])

    [http://arxiv.org/abs/2308.13137](http://arxiv.org/abs/2308.13137)

    OmniQuant是一种用于大型语言模型的全向校准量化技术，通过优化各种量化参数实现了良好的性能，并保持了计算效率。

    

    大型语言模型（LLM）已经在自然语言处理任务中带来了革命性的变化。然而，它们的实际部署受到了其庞大的内存和计算需求的限制。虽然最近的后训练量化（PTQ）方法在减少内存占用和提高LLM的计算效率方面非常有效，但它们手工制定量化参数，导致性能较低并且不能处理极低位量化。为了解决这个问题，我们介绍了一种全向校准量化（OmniQuant）技术，用于LLMs，它在多种量化设置下实现了良好的性能，并通过高效优化各种量化参数来保持PTQ的计算效率。OmniQuant包含两个创新组件，包括可学习的权重剪裁（LWC）和可学习的等效变换（LET）。LWC通过优化剪裁阈值来调节权重的极值。与此同时，LET处理激活函数。

    Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
    
[^425]: 量化生物混合系统中的仿生差距

    Quantifying the biomimicry gap in biohybrid systems. (arXiv:2308.08978v1 [cs.RO])

    [http://arxiv.org/abs/2308.08978](http://arxiv.org/abs/2308.08978)

    这项工作中，研究人员针对生物混合系统中的仿生差距进行了量化分析。通过使用真实鱼类和仿生诱饵进行实验，以及对鱼类对的模拟，研究人员证明了他们的系统可以生成与真实鱼类完全相似的社会互动。

    

    与动物相互作用的仿生混合系统已成为探索和识别群体动物行为机制的有力工具。一个关键挑战在于将社会互动模型从模拟转移到现实，利用机器人验证建模假设。这一挑战源于所谓的“仿生差距”的跨越，该差距使用不完美的机器人复制品、通信线索和物理约束，这些因素未被纳入模拟中，可能在动物身上引发不真实的行为反应。在这项工作中，我们使用了红嘴无须魮鱼（Hemigrammus rhodostomus）的仿生诱饵和一个神经网络（NN）模型来生成仿生社会互动。通过对一个由鱼和机器人诱饵组成的生物混合体、一对真实鱼和一对鱼的模拟的实验，我们证明了我们的生物混合系统产生了与真实鱼类完全相似的仿真社会互动。

    Biohybrid systems in which robotic lures interact with animals have become compelling tools for probing and identifying the mechanisms underlying collective animal behavior. One key challenge lies in the transfer of social interaction models from simulations to reality, using robotics to validate the modeling hypotheses. This challenge arises in bridging what we term the "biomimicry gap", which is caused by imperfect robotic replicas, communication cues and physics constrains not incorporated in the simulations that may elicit unrealistic behavioral responses in animals. In this work, we used a biomimetic lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a neural network (NN) model for generating biomimetic social interactions. Through experiments with a biohybrid pair comprising a fish and the robotic lure, a pair of real fish, and simulations of pairs of fish, we demonstrate that our biohybrid system generates high-fidelity social interactions mirroring those of genuine f
    
[^426]: 点击率预测的时间兴趣网络

    Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])

    [http://arxiv.org/abs/2308.08487](http://arxiv.org/abs/2308.08487)

    本文提出了时间兴趣网络（TIN），用于捕捉行为与目标之间的四重语义和时间相关性，以预测点击率的效果和已有方法对这种相关性的学习程度尚不清楚。

    

    用户行为的历史是预测点击率最重要的特征之一，因为它们与目标项目具有强烈的语义和时间相关性。虽然已有文献分别研究了这些相关性，但尚未分析它们的组合，即行为语义、目标语义、行为时间和目标时间的四重相关性。这种相关性对性能的影响以及现有方法学习这种相关性的程度尚不清楚。为了填补这一空白，我们在实践中测量了四重相关性，并观察到直观而强大的四重模式。我们测量了几种代表性的用户行为方法的学习相关性，但令人惊讶的是，它们都没有学习到这样的模式，特别是时间模式。在本文中，我们提出了时间兴趣网络（TIN）来捕捉行为与目标之间的四重语义和时间相关性。

    The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
    
[^427]: 加强解药：针对中毒攻击的改进点对点认证

    Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks. (arXiv:2308.07553v1 [cs.LG])

    [http://arxiv.org/abs/2308.07553](http://arxiv.org/abs/2308.07553)

    本论文提出了一种改进的点对点认证方法，通过利用差分隐私和采样高斯机制，能够确保对有限数量中毒样本的预测具有不变性，提供更大的对抗鲁棒性保证。

    

    中毒攻击可以通过对训练语料进行微小改动来对模型行为产生不成比例的影响。虽然存在对特定中毒攻击的防御方法，但它们通常不提供任何保证，可能被新型攻击所对抗。相比之下，通过考察最坏情况下的行为，认证防御可以提供针对有限数量训练样本被敌对攻击修改的样本的鲁棒性保证，称为点对点认证。我们通过利用差分隐私和采样高斯机制，确保每个测试实例对有限数量中毒样本的预测具有不变性，从而实现了这一点。通过这样做，我们的模型提供的对抗鲁棒性保证比之前的认证方法提供的保证更大两倍以上。

    Poisoning attacks can disproportionately influence model behaviour by making small changes to the training corpus. While defences against specific poisoning attacks do exist, they in general do not provide any guarantees, leaving them potentially countered by novel attacks. In contrast, by examining worst-case behaviours Certified Defences make it possible to provide guarantees of the robustness of a sample against adversarial attacks modifying a finite number of training samples, known as pointwise certification. We achieve this by exploiting both Differential Privacy and the Sampled Gaussian Mechanism to ensure the invariance of prediction for each testing instance against finite numbers of poisoned examples. In doing so, our model provides guarantees of adversarial robustness that are more than twice as large as those provided by prior certifications.
    
[^428]: 智能农业中的基础模型：基础知识、机遇和挑战

    Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v1 [cs.LG])

    [http://arxiv.org/abs/2308.06668](http://arxiv.org/abs/2308.06668)

    传统农业系统中的机器学习和深度学习模型存在局限性，而基础模型在语言和视觉任务中表现出了显著的成功。本研究旨在探索基础模型在智能农业领域的潜力和应用。

    

    过去十年间，农业系统中的机器学习和深度学习方法得到了快速发展，展示出在各种农业应用中取得了巨大成功。然而，这些传统的机器学习/深度学习模型具有一定的局限性：它们严重依赖于昂贵的、难以获取的标记数据集进行训练，需要专业知识进行开发和维护，而且大多针对特定任务，缺乏泛化能力。最近，基础模型在语言和视觉任务中展示出了显著的成功，跨越了各个领域。这些模型在来自多个领域和模态的大量数据上进行训练。一旦训练完成，它们可以通过微调和少量特定任务的标记数据完成各种多样的任务。尽管基础模型已经证明了其有效性和巨大潜力，但在农业领域中应用尚未有太多探索。因此，本研究旨在探索基础模型在智能农业领域的潜力。

    The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agricultu
    
[^429]: DCNFIS：深度卷积神经模糊推理系统

    DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])

    [http://arxiv.org/abs/2308.06378](http://arxiv.org/abs/2308.06378)

    本文介绍了一种新的深度卷积神经模糊推理系统（DCNFIS），它通过将模糊逻辑和深度学习模型相结合，实现了提高透明度而不损失准确性的目标。DCNFIS在准确性上与现有卷积神经网络相当，并且胜过了最先进的深度模糊系统。通过模糊规则提取的解释可以提高模型的可解释性。

    

    在可解释的人工智能中，透明度与准确性之间存在一个著名的权衡。本文介绍了一种新的深度网络设计，通过将模糊逻辑和深度学习模型相结合，实现了提高透明度但不损失准确性的目标。我们设计了一个深度卷积神经模糊推理系统（DCNFIS），并在四个著名数据集上展示了它与三个现有卷积神经网络的相同准确性。我们进一步发现，DCNFIS在性能上胜过了最先进的深度模糊系统。然后，我们利用模糊逻辑的透明度，从DCNFIS中编码的模糊规则中提取解释，以渐变映射的形式展示。我们还利用Fashion-MNIST数据集对这些解释的特性进行了深入研究。

    A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
    
[^430]: 在线多任务学习中基于递归最小二乘和递归核方法的应用

    Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods. (arXiv:2308.01938v1 [stat.ML])

    [http://arxiv.org/abs/2308.01938](http://arxiv.org/abs/2308.01938)

    本文提出了两种新的在线多任务学习方法，分别基于递归最小二乘和递归核方法。与基于梯度下降或不精确逼近的方法不同，我们的方法在每个实例的代价上具有二次复杂度。我们将这些方法应用于风力短期预测挑战，并与其他竞争者进行了比较。

    

    本文提出了两种新颖的在线多任务学习（MTL）回归问题的方法。我们采用高性能基于图的MTL公式，基于加权递归最小二乘（WRLS）和在线稀疏最小二乘支持向量回归（OSLSSVR）开发其递归版本。采用任务堆叠转换，我们展示了存在一个单矩阵，它融合了多任务之间的关系，并为MT-WRLS方法的初始化过程和MT-OSLSSVR的多任务核函数提供结构信息。与现有大部分基于在线梯度下降（OGD）或不精确立方逼近方法的文献相比，我们实现了精确和近似递归，其每个实例的代价在输入空间的维度（MT-WRLS）或实例字典的大小上是二次的。我们将我们的在线MTL方法与其他竞争者在实际风短期预测挑战上进行了比较。

    This paper introduces two novel approaches for Online Multi-Task Learning (MTL) Regression Problems. We employ a high performance graph-based MTL formulation and develop its recursive versions based on the Weighted Recursive Least Squares (WRLS) and the Online Sparse Least Squares Support Vector Regression (OSLSSVR). Adopting task-stacking transformations, we demonstrate the existence of a single matrix incorporating the relationship of multiple tasks and providing structural information to be embodied by the MT-WRLS method in its initialization procedure and by the MT-OSLSSVR in its multi-task kernel function. Contrasting the existing literature, which is mostly based on Online Gradient Descent (OGD) or cubic inexact approaches, we achieve exact and approximate recursions with quadratic per-instance cost on the dimension of the input space (MT-WRLS) or on the size of the dictionary of instances (MT-OSLSSVR). We compare our online MTL methods to other contenders in a real-world wind sp
    
[^431]: 更多上下文，更少干扰：通过推断和调节上下文属性进行视觉分类

    More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])

    [http://arxiv.org/abs/2308.01313](http://arxiv.org/abs/2308.01313)

    本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。

    

    CLIP作为一种基础的视觉语言模型，由于其理解各种视觉概念和自然语言描述的能力，被广泛应用于零样本图像分类。然而，如何充分利用CLIP的前所未有的人类般理解能力来实现更好的零样本分类仍然是一个开放问题。本文从人类的视觉感知过程中得到启发：现代神经科学观点认为，在对物体进行分类时，人类首先推断其与类别无关的属性（如背景和方向），这有助于将前景对象与背景区分开来，然后以此信息为基础进行决策。受此启发，我们观察到为CLIP提供上下文属性可以改善零样本分类并减轻对虚假特征的依赖。我们还观察到CLIP本身可以合理地从图像中推断出这些属性。基于这些观察，我们提出了一种零训练、两步骤的零样本分类方法。

    CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
    
[^432]: 在预测上下文中的在线学习问题

    Online learning in bandits with predicted context. (arXiv:2307.13916v1 [stat.ML])

    [http://arxiv.org/abs/2307.13916](http://arxiv.org/abs/2307.13916)

    本文研究了一种在预测上下文中的在线学习问题，通过将经典统计学中的测量误差模型推广到在线决策设置中，我们提出了第一个具有次线性后悔的在线算法。

    

    我们考虑在每个时刻，代理只能访问到上下文的一个带噪声的版本以及误差方差（或者这个方差的一个估计）。这一设置受到了许多应用的启发，在这些应用中，用于决策的真实上下文是不可观测的，而只有一个由可能复杂的机器学习算法预测出的上下文。当上下文误差是非衰减的时候，经典的bandit算法无法达到次线性的后悔。我们提出了在这一设置下，第一个具有次线性后悔的在线算法，并与适当的基准进行了比较。关键的思想是将经典统计学中的测量误差模型推广到在线决策设置中，这是非平凡的，因为策略依赖于有噪声的上下文观察。

    We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
    
[^433]: 使用切片Wasserstein损失来训练神经网络的SGD收敛性分析

    Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])

    [http://arxiv.org/abs/2307.11714](http://arxiv.org/abs/2307.11714)

    本论文研究了使用切片Wasserstein损失训练神经网络时，随机梯度下降算法的收敛性，并证明了在特定条件下，SGD轨迹逼近了梯度流方程的集合。

    

    最优输运近年来引发了广泛的兴趣，特别是由于Wasserstein距离，它提供了一种几何上合理和直观的比较概率测度的方法。出于计算原因，切片Wasserstein（SW）距离作为Wasserstein距离的一种替代方法被引入，并且已经被用于训练生成式神经网络（NNs）。虽然在这样的设置中实际观察到了随机梯度下降（SGD）的收敛性，但据我们所知，对于这一观察没有理论保证。借鉴Bianchi等人（2022）关于SGD在非光滑和非凸函数上收敛性的最新工作，我们旨在填补这一知识空白，并提供一个具有实际意义的上下文，使得SW损失对NN参数的固定步长SGD轨迹收敛到（次）梯度流方程的集合。更准确地说，我们证明了随着步长减小，这些轨迹逼近了梯度流方程的集合。

    Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much st
    
[^434]: 离散切割Wasserstein损失的性质

    Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])

    [http://arxiv.org/abs/2307.10352](http://arxiv.org/abs/2307.10352)

    本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。

    

    切割Wasserstein（SW）距离已成为比较概率测度的Wasserstein距离的一种流行替代方法。广泛应用包括图像处理、领域自适应和生成建模，常常需要优化一些参数以最小化SW，该参数充当离散概率测度之间的损失函数（因为具有密度的测度在数值上是无法实现的）。所有这些优化问题都存在相同的子问题，即最小化切割Wasserstein能量。在本文中，我们研究了$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$的属性，即两个具有与一个测度的支撑相同数量的离散均匀测度之间的SW距离作为支撑$Y \in \mathbb{R}^{n \times d}$函数的能量。我们研究了这个能量的正则性和优化性质，以及其通过蒙特卡洛近似$\mathcal{E}_p$（使用SW中的期望估计）。

    The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
    
[^435]: 在大型语言模型中的上下文压缩的上下文自编码器

    In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])

    [http://arxiv.org/abs/2307.06945](http://arxiv.org/abs/2307.06945)

    在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。

    

    我们提出了一种用于大型语言模型中上下文压缩的上下文自编码器（ICAE）。 ICAE有两个模块：一个可学习的编码器，通过从LLM中采用LoRA方式将长上下文压缩为有限数量的内存槽，以及一个固定的解码器，作为目标LLM，可以根据内存槽来进行各种目的的条件处理。我们首先使用自编码和语言建模目标在大规模文本数据上预训练ICAE，使其能够生成准确和全面表示原始上下文的内存槽。然后，我们使用少量指导数据对预训练的ICAE进行微调，以增强其与各种提示的交互，从而产生理想的响应。我们的实验结果表明，使用我们提出的预训练和微调范式学习的ICAE可以有效地产生$4\times$上下文压缩的内存槽，目标LLM可以很好地对其进行条件处理，以响应各种提示。

    We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
    
[^436]: 深度学习动态图：模型与基准

    Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])

    [http://arxiv.org/abs/2307.06104](http://arxiv.org/abs/2307.06104)

    本文对深度学习动态图领域进行了调查，总结了学习时间和空间信息的最新优势，并对最流行的方法进行了公平的性能比较，为评估新架构和方法建立了一个可靠的基准模型。

    

    近年来，在深度图网络（DGNs）的研究中取得了巨大进展，推动了图上学习的领域成熟发展。尽管这个研究领域正在快速增长，但仍然存在一些尚未解决的重要挑战。特别地，急需使DGNs适用于实时系统中随时间推移不断演化的预测任务。为促进动态图领域的研究，首先，我们调查了学习时间和空间信息的最新优势，并提供了动态图表示学习领域的当前最新概览。其次，我们对最流行的方法进行了公平的性能比较，通过严格的模型选择和评估，为评估新架构和方法建立了一个可靠的基准模型。

    Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches
    
[^437]: ELM神经元：一种高效且表达力强的皮层神经元模型可以解决长时间跨度任务

    The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])

    [http://arxiv.org/abs/2306.16922](http://arxiv.org/abs/2306.16922)

    ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。

    

    传统的大规模神经科学模型和机器学习利用简化的个体神经元模型，依靠集体活动和适当调整的连接来执行复杂的计算。然而，每个生物皮层神经元本质上都是一个复杂的计算设备，最近的一项研究证实了这一点，该研究中，需要一个具有数百万个参数的深度人工神经网络来复制详细生物物理模型的输入输出关系。我们对这些多个参数的必要性提出了质疑，并引入了表达力强的泄漏存储器（ELM）神经元，这是一种受生物启发的计算模型，具有高计算表达力，同时也非常高效。值得注意的是，我们的ELM神经元仅需要8,000个可训练参数就能准确匹配前述的输入输出关系。我们发现，准确的模型需要多个类似于存储器的隐藏状态和复杂的非线性突触整合。

    Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
    
[^438]: 基于功能团的扩散用于口袋特异性分子生成和扩展

    Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration. (arXiv:2306.13769v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.13769](http://arxiv.org/abs/2306.13769)

    提出了一种基于功能团的扩散模型D3FG，用于口袋特异性分子生成和扩展。基于刚性体的功能团和质点的连接器可以共同形成增强配体-蛋白质相互作用的复杂片段，能够生成高质量的分子。

    

    近年来，提出了AI辅助的药物设计方法，以生成给定目标蛋白质的口袋结构的分子。大部分方法都是基于原子级别的方法，将原子视为基本组件，并生成原子位置和类型。然而，这种方法很难生成具有复杂结构的现实片段。为解决这个问题，我们提出了D3FG，一种基于功能团的扩散模型，用于口袋特异性分子生成和扩展。D3FG将分子分解为两类组件：定义为刚性体的功能团和质点的连接器。两种类型的组件可以共同形成增强配体-蛋白质相互作用的复杂片段。具体而言，在扩散过程中，D3FG将组件的位置、方向和类型的数据分布扩散到一个先验分布；在生成过程中，神经网络参数化去噪器逐渐去除三个变量的噪声，以获得现实分子。我们在三个任务上评估了我们的方法：针对八个目标进行库生成，针对五种现有药物进行片段完善，以及针对两个未知参考分子的目标进行去新设计，显示出我们的方法可以生成多样化和高品质的分子，并具有出色的性能。

    In recent years, AI-assisted drug design methods have been proposed to generate molecules given the pockets' structures of target proteins. Most of them are atom-level-based methods, which consider atoms as basic components and generate atom positions and types. In this way, however, it is hard to generate realistic fragments with complicated structures. To solve this, we propose D3FG, a functional-group-based diffusion model for pocket-specific molecule generation and elaboration. D3FG decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. And the two kinds of components can together form complicated fragments that enhance ligand-protein interactions.  To be specific, in the diffusion process, D3FG diffuses the data distribution of the positions, orientations, and types of the components into a prior distribution; In the generative process, the noise is gradually removed from the three variables by denoisers parame
    
[^439]: 离散行走跳跃采样用于蛋白质发现

    Protein Discovery with Discrete Walk-Jump Sampling. (arXiv:2306.12360v1 [q-bio.BM])

    [http://arxiv.org/abs/2306.12360](http://arxiv.org/abs/2306.12360)

    本文提出了一种离散行走跳跃采样的方法，通过学习平滑能量函数、使用 Langevin Markov 链蒙特卡罗 (MCMC) 算法和一步去噪的投射技术，可以解决离散生成模型的采样难题。同时在抗体蛋白质的生成建模上进行了应用和测试，并引入了分布一致性得分作为基准测试标准。

    

    我们通过学习平滑能量函数、使用 Langevin Markov 链蒙特卡罗 (MCMC) 从平滑数据流形中采样，并使用一步去噪投射回真实数据流形，解决了离散生成模型的训练和采样困难问题。我们的离散行走跳跃采样形式化方法结合了基于能量的模型的最大似然训练和基于分数的模型的改进样本质量，同时通过仅需要一个噪声水平来简化训练和采样。我们在抗体蛋白质的生成建模上评估了我们的方法的鲁棒性，并引入了分布一致性得分以对蛋白质生成模型进行基准测试。通过优化和采样我们的模型用于提议的分布一致性得分，97-100%的生成样品可以成功表达和纯化，并且35%的功能设计在第一次尝试中显示出与已知功能抗体相同或更好的结合亲和力。

    We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling formalism combines the maximum likelihood training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the distributional conformity score to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100% of generated samples are successfully expressed and purified and 35% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a sing
    
[^440]: MUBen：评估分子性质预测预训练模型的不确定性基准

    MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction. (arXiv:2306.10060v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10060](http://arxiv.org/abs/2306.10060)

    MUBen评估不同骨干和UQ模型组合对分子不确定性估计和属性预测的性能，以解决预训练模型微调中的过拟合校准问题。

    

    预训练于大规模无标签分子数据的大型Transformer模型在预测分子性质方面取得了巨大成功。然而，在微调期间，这些模型可能容易出现过拟合，导致对测试数据的过度自信预测落在了训练分布之外。为了解决这个问题，可以使用不确定性量化（UQ）方法来改善模型的预测校准。虽然存在许多UQ方法，但并不是所有方法都会导致性能改善。虽然一些研究使用UQ来改善分子预训练模型，但选择适合的骨干和UQ方法以可靠地估计分子不确定性的过程仍然是未经探索的。为了解决这个差距，我们提出了MUBen，评估不同的骨干和UQ模型组合，以量化它们在属性预测和不确定性估计方面的性能。通过微调使用不同分子描述符的各种骨干分子表示模型。

    Large Transformer models pre-trained on massive unlabeled molecular data have shown great success in predicting molecular properties. However, these models can be prone to overfitting during fine-tuning, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have used UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different combinations of backbone and UQ models to quantify their performance for both property prediction and uncertainty estimation. By fine-tuning various backbone molecular representation models using different molecular descrip
    
[^441]: Jumanji: JAX中一套多样化可扩展的强化学习环境

    Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX. (arXiv:2306.09884v1 [cs.LG])

    [http://arxiv.org/abs/2306.09884](http://arxiv.org/abs/2306.09884)

    Jumanji是JAX中一套可扩展的强化学习系统，提供了一系列高度可定制的环境，具有快速、灵活、可扩展和模块化特点，利用硬件加速器赋能更有能力的代理人。

    

    开源强化学习环境在推动AI算法的发展方面起到了关键作用。现代强化学习研究需要模拟环境具备性能、可扩展性和模块化特点，以扩展其在更广泛的实际应用中的可用性。因此，我们提出了Jumanji，这是一套设计用于快速、灵活和可扩展的不同RL环境的强化学习系统。Jumanji提供了一系列的环境，专注于工业中经常遇到的组合问题，以及挑战性的一般决策任务。通过利用JAX和GPU、TPU等硬件加速器的效率，Jumanji能够迅速迭代研究思路和大规模实验，最终赋能更有能力的代理人。与现有的强化学习环境套件不同，Jumanji具有高度可定制性，允许用户根据其需求调整初始状态分布和问题复杂度。

    Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable. Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Further
    
[^442]: MOFI: 从含噪实体标注的图像中学习图像表示

    MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])

    [http://arxiv.org/abs/2306.07952](http://arxiv.org/abs/2306.07952)

    MOFI 提出了一种新的方法，自动从含噪图像文本对中为图像指定实体标签，创建了一个新的大规模数据集 I2E，通过研究不同的训练配方，学习到了能够有效学习图像表示的模型。

    

    本文提出了一种新的视觉基础模型 MOFI，旨在从含噪实体标注的图像中学习图像表示。MOFI 与以往的工作有两点不同：（i）预训练数据，（ii）训练配方。在数据方面，我们引入了一种新方法，自动从含噪图像文本对中为图像指定实体标签。我们使用命名实体识别模型从 alt-text 中提取实体，然后使用 CLIP 模型选择正确的实体作为图像的标签。这种方法简单易行，不需要昂贵的人工注释，并且可以轻松扩展到从 web 上挖掘的数十亿个图像文本对。通过这种方法，我们创建了 Image-to-Entities（I2E）这一新的大规模数据集，其中包含 10 亿张图像和 200 万个不同的实体，涵盖了野外丰富的视觉概念。基于 I2E 数据集，我们研究了不同的训练配方，包括有监督的预训练、对比度预训练。

    We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
    
[^443]: 深度神经网络的相似性校准

    Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])

    [http://arxiv.org/abs/2306.04590](http://arxiv.org/abs/2306.04590)

    该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。

    

    推理结果的可信度校准对于提供准确和可解释的不确定性估计至关重要，特别是在安全关键场景下。已有的校准方法通常忽略了接近度偏差的问题，即模型在低接近性数据（即分布的稀疏区域）中倾向于更自信，而在高接近性样本中表现出不一致的误校准，我们发现这一问题。我们在ImageNet预训练模型上研究了这一问题，并观察到：1）接近度偏差存在于各种模型架构和大小之间；2）基于Transformer的模型比基于CNN的模型更容易受到接近度偏差的影响；3）即使采用流行的校准算法如温度缩放，接近度偏差也会持续存在；4）模型在低接近性样本上的过拟合程度比高接近性样本更严重。在这些实证发现的基础上，我们提出了ProCal。

    Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, 
    
[^444]: 通过启发式策略混合改进离线强化学习

    Improving Offline RL by Blending Heuristics. (arXiv:2306.00321v1 [cs.LG])

    [http://arxiv.org/abs/2306.00321](http://arxiv.org/abs/2306.00321)

    HUBL是一种用于基于值函数回溯的广泛类离线强化学习算法的简单性能提升技术。我们证明了HUBL可通过调整奖励和折扣因子来简单实现，并且实验结果表明HUBL能够在提高性能的同时降低复杂度。

    

    我们提出了启发式策略混合（HUBL），一种用于基于值函数回溯的广泛类离线强化学习算法的简单性能提升技术。HUBL修改了这些算法中使用的Bellman操作符，部分用启发式的蒙特卡罗回报替换了值函数回溯。对于回报更高的轨迹，HUBL更多地依赖于启发式，较少依赖于值函数回溯。否则，它会更加倚重于值函数回溯。我们证明了这个想法可以通过调整奖励和折扣因子来简单实现，使HUBL可适用于现有的许多离线强化学习算法。我们从理论上证明HUBL降低了离线强化学习的复杂度，从而提高了其有限样本的性能。此外，我们还从实证方面表明了HUBL对四种最先进的基于回溯的离线强化学习算法的策略质量进行了改进，平均提高了9%的效果，在D4RL和Meta-Wo的27个数据集上表现稳定。

    We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies Bellman operators used in these algorithms, partially replacing the bootstrapped values with Monte-Carlo returns as heuristics. For trajectories with higher returns, HUBL relies more on heuristics and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. We show that this idea can be easily implemented by relabeling the offline datasets with adjusted rewards and discount factors, making HUBL readily usable by many existing offline RL implementations. We theoretically prove that HUBL reduces offline RL's complexity and thus improves its finite-sample performance. Furthermore, we empirically demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9% on average over 27 datasets of the D4RL and Meta-Wo
    
[^445]: 大型语言模型的自相矛盾幻觉：评估、检测和缓解

    Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])

    [http://arxiv.org/abs/2305.15852](http://arxiv.org/abs/2305.15852)

    本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。

    

    大型语言模型容易产生幻想的文本。自相矛盾是一种重要的幻觉形式，指的是语言模型在同一语境中生成两个矛盾的句子。本文针对最先进、经过指导的语言模型，对自相矛盾进行了全面的分析、评估、检测和缓解。我们设计了一个框架来有效地触发自相矛盾，评估结果表明，无论是对于著名的还是不太出名的话题，不同的语言模型中自相矛盾都经常发生。

    Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
    
[^446]: 差分隐私潜在扩散模型

    Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])

    [http://arxiv.org/abs/2305.15759](http://arxiv.org/abs/2305.15759)

    本文提出使用差分隐私训练潜在扩散模型(LDMs)，通过预训练自编码器将高维像素空间转变为低维潜在空间实现更高效快速的DMs训练，并且通过只微调注意力模块减少了可训练参数的数量。

    

    扩散模型(DMs)被广泛用于生成高质量图像数据集。然而，由于它们直接在高维像素空间中运行，DMs的优化计算成本高，需要长时间的训练。这导致由于差分隐私的可组合性属性，大量噪音注入到差分隐私学习过程中。为了解决这个挑战，我们提出使用差分隐私训练潜在扩散模型(LDMs)。LDMs使用强大的预训练自编码器将高维像素空间减少到更低维的潜在空间，使训练DMs更加高效和快速。与[Ghalebikesabi等人，2023]预先用公共数据预训练DMs，然后再用隐私数据进行微调不同，我们仅微调LDMs中不同层的注意力模块以获得隐私敏感数据，相对于整个DM微调，可减少大约96%的可训练参数数量。

    Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
    
[^447]: FEDORA：用于反应行为的飞行事件数据集

    FEDORA: Flying Event Dataset fOr Reactive behAvior. (arXiv:2305.14392v1 [cs.CV])

    [http://arxiv.org/abs/2305.14392](http://arxiv.org/abs/2305.14392)

    FEDORA是一个飞行事件数据集，解决了现有数据集缺少完整数据和时间分辨率的问题，旨在帮助在资源受限环境下实现基于视觉的自主导航和避障。

    

    生物体在飞行中使用极少数的神经元和极低的失误率执行复杂的高速机动，突显了这些资源受限制的生物系统的有效性。近年来，事件驱动硬件逐渐成为在资源受限环境中实现复杂视觉任务的一种有前途的方法。基于视觉的自主导航和避障包括几个独立但相关的任务，如光流估计、深度估计、同时定位与建图（SLAM）、物体检测和识别。为了确保这些任务之间的一致性，他们必须在单个数据集上进行训练。然而，大多数现有数据集只提供所需数据的选定子集，这使得网络间的一致性难以实现。现有数据集的另一个限制是提供的有限时间分辨率。为解决这些限制，我们提出了FEDORA，

    The ability of living organisms to perform complex high speed manoeuvers in flight with a very small number of neurons and an incredibly low failure rate highlights the efficacy of these resource-constrained biological systems. Event-driven hardware has emerged, in recent years, as a promising avenue for implementing complex vision tasks in resource-constrained environments. Vision-based autonomous navigation and obstacle avoidance consists of several independent but related tasks such as optical flow estimation, depth estimation, Simultaneous Localization and Mapping (SLAM), object detection, and recognition. To ensure coherence between these tasks, it is imperative that they be trained on a single dataset. However, most existing datasets provide only a selected subset of the required data. This makes inter-network coherence difficult to achieve. Another limitation of existing datasets is the limited temporal resolution they provide. To address these limitations, we present FEDORA, a 
    
[^448]: 基于凸组合的表达性损失可以提高网络的对抗鲁棒性

    Expressive Losses for Verified Robustness via Convex Combinations. (arXiv:2305.13991v1 [cs.LG])

    [http://arxiv.org/abs/2305.13991](http://arxiv.org/abs/2305.13991)

    通过基于凸组合的表达性损失，可以提高网络的对抗鲁棒性，最新的算法可以获得最先进的结果；这种方法通过对抗性攻击和IBP边界之间的简单凸组合进行实现。

    

    先前的工作通常通过（扰动区域的子集）的最坏情况下限，或在对抗训练之上引入可验证性来训练具有已验证鲁棒性的网络。最先进性能的关键在于所使用的损失函数的表达能力，它应该能够匹配训练后要使用的验证器的紧密度。我们形式化定义了表达力，并表明它可以通过对抗性攻击和IBP边界之间的简单凸组合来满足。然后，我们展示了所得到的算法，命名为CC-IBP和MTL-IBP，在各种设置中均可以产生最先进的结果，尽管其概念上是简单的。特别地，在TinyImageNet和缩小的ImageNet上，对于半径为$ \frac{1} {255} $的$ \ell_ \infty $扰动，MTL-IBP可以将文献中最佳标准和验证准确性从$1.98\%$提高到$3.92\%$，同时仅依赖于单步自适应优化。

    In order to train networks for verified adversarial robustness, previous work typically over-approximates the worst-case loss over (subsets of) perturbation regions or induces verifiability on top of adversarial training. The key to state-of-the-art performance lies in the expressivity of the employed loss function, which should be able to match the tightness of the verifiers to be employed post-training. We formalize a definition of expressivity, and show that it can be satisfied via simple convex combinations between adversarial attacks and IBP bounds. We then show that the resulting algorithms, named CC-IBP and MTL-IBP, yield state-of-the-art results across a variety of settings in spite of their conceptual simplicity. In particular, for $\ell_\infty$ perturbations of radius $\frac{1}{255}$ on TinyImageNet and downscaled ImageNet, MTL-IBP improves on the best standard and verified accuracies from the literature by from $1.98\%$ to $3.92\%$ points while only relying on single-step ad
    
[^449]: 具有概率保证的神经网络鲁棒的反事实解释

    Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])

    [http://arxiv.org/abs/2305.11997](http://arxiv.org/abs/2305.11997)

    本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。

    

    针对神经网络发现偏移，通过使用稳定性度量来量化反事实解释对可能的模型变化的鲁棒性。通过在反事实解释优化中引入正则化项来将生成的反事实解释靠近数据流形，从而实现了对自然发生的模型变化的高概率鲁棒性。新的算法在合成和现实世界数据集上进行实验，证明了其有效性。

    There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
    
[^450]: 计算机视觉模型哪些地方会出错？使用交互式可视化工具找出并改进CV模型

    Where does a computer vision model make mistakes? Using interactive visualizations to find where and how CV models can improve. (arXiv:2305.11927v1 [cs.HC])

    [http://arxiv.org/abs/2305.11927](http://arxiv.org/abs/2305.11927)

    研究使用交互式可视化工具在创建计算机视觉分类和检测模型时帮助用户识别和改进模型上的问题，有效减少设计师的工作量。

    

    创建计算机视觉模型仍然是一个复杂和繁琐的过程，而使终端用户可以构建、检查和改进这些模型的交互式机器学习模型视角已经在解决这些问题方面取得了一些进展。为了提高具有不同级别机器学习专业技能的终端用户的体验，我们在Sprite的上下文中设计和评估了两个交互式可视化工具，这是一个用于为从视频中抽取的图像创建CV分类和检测模型的系统。我们研究了这些可视化工具如何作为机器学习循环的一部分，帮助用户识别（评估）和选择（规划）模型存在问题的图像，并改善正在训练的模型。我们发现，使用这些可视化工具的用户在更广泛的模型错误类型和一个或多个模型的预测行为的评估和比较方面发现了更多的图像，从而减少了设计师创建和改进CV模型所需的潜在工作量。

    Creating Computer Vision (CV) models remains a complex and taxing practice for end-users to build, inspect, and improve these models. Interactive ML perspectives have helped address some of these issues by considering a teacher-in-the-loop where planning, teaching, and evaluating tasks take place. To improve the experience of end-users with various levels of ML expertise, we designed and evaluated two interactive visualizations in the context of Sprite, a system for creating CV classification and detection models for images originating from videos. We study how these visualizations, as part of the machine teaching loop, help users identify (evaluate) and select (plan) images where a model is struggling and improve the model being trained. We found that users who had used the visualizations found more images across a wider set of potential types of model errors, as well as in assessing and contrasting the prediction behavior of one or more models, thus reducing the potential effort requ
    
[^451]: LLM自身可读取和生成CXR图像

    LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])

    [http://arxiv.org/abs/2305.11490](http://arxiv.org/abs/2305.11490)

    该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。

    

    借助于近期大语言模型（LLMs）的显著发展，人们正积极尝试将LLMs的实用性扩展到多模态任务。已经有人尝试连接语言和视觉信息，并且也在不断尝试为LLMs添加视觉能力。然而，现有的尝试只使用LLMs作为图像解码器，没有尝试通过自然语言来生成图像。通过采用VQ-GAN框架，将图像的潜在表示视为一种文本标记，我们提出了一种新方法，可以微调预先训练的LLM，以像文本一样读取和生成图像，而无需进行结构更改、额外的训练目标或训练专门的网络，同时仍保留LLM的指令跟随能力。我们将此框架应用于胸部X线（CXR）图像的生成任务中，因为这是一个复杂信息在视觉和语言之间翻译的领域。

    Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
    
[^452]: 决策时间规划的更新等价框架

    The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])

    [http://arxiv.org/abs/2304.13138](http://arxiv.org/abs/2304.13138)

    该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。

    

    在棋类游戏等完全信息环境中，即时修正（或构建）策略的决策时间规划是实现超人类表现的关键。一些研究将决策时间规划扩展到更普遍的不完全信息环境，从而实现了扑克中的超人类表现。但是，这些方法需要考虑随着非公共信息量的增加而快速增长的子游戏，使得它们在非公共信息量较大时不起作用。为了解决这个问题，我们引入了一种基于更新等价而不是子游戏概念的决策时间规划框架。在这个框架中，决策时间规划算法模拟同步学习算法的更新。这个框架使我们能够引入一系列原则上的决策时间规划算法，这些算法不依赖于公共信息，并为新的一个系列的决策时间规划算法打开了大门。

    The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
    
[^453]: 基于Sheaf神经网络的基于图的推荐系统

    Sheaf Neural Networks for Graph-based Recommender Systems. (arXiv:2304.09097v1 [cs.IR])

    [http://arxiv.org/abs/2304.09097](http://arxiv.org/abs/2304.09097)

    基于Sheaf神经网络的模型提出了一种新的向量空间表示方法，使得其在基准推荐任务上获得最先进的性能表现。

    

    近年来，Graph神经网络在许多应用中得到了广泛应用，包括推荐系统。Graph神经网络对其他方法的优越性在于，推荐系统中的许多问题可以自然地建模为图，其中节点可以是用户或项目，边代表偏好关系。 在当前的Graph神经网络方法中，节点用在训练时学习到的静态向量表示。这种静态向量可能只适用于捕捉定义它们的一些用户或项目的微妙差别。为了克服这个限制，我们建议使用最近提出的启发范畴论的模型：Sheaf神经网络。Sheaf神经网络及其连接的拉普拉斯可以通过将每个节点（以及边）与向量空间而不是单个向量相关联来解决上述问题。向量空间表示更丰富，并允许在推理时选择正确的表示。这种方法使我们的模型更具表现力和灵活性，在几个基准推荐任务上实现了最先进的性能。

    Recent progress in Graph Neural Networks has resulted in wide adoption by many applications, including recommendation systems. The reason for Graph Neural Networks' superiority over other approaches is that many problems in recommendation systems can be naturally modeled as graphs, where nodes can be either users or items and edges represent preference relationships. In current Graph Neural Network approaches, nodes are represented with a static vector learned at training time. This static vector might only be suitable to capture some of the nuances of users or items they define. To overcome this limitation, we propose using a recently proposed model inspired by category theory: Sheaf Neural Networks. Sheaf Neural Networks, and its connected Laplacian, can address the previous problem by associating every node (and edge) with a vector space instead than a single vector. The vector space representation is richer and allows picking the proper representation at inference time. This approa
    
[^454]: 能量引导的熵神经最优输运

    Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])

    [http://arxiv.org/abs/2304.06094](http://arxiv.org/abs/2304.06094)

    本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。

    

    能量基础模型（EBMs）在机器学习社区已经有数十年的历史。自两千年代起，一直有很多高效的方法通过能量势（非归一化的似然函数）来解决生成建模问题。相比之下，最优输运（OT）领域，尤其是神经OT求解器，受到的探索要少得多，仅有一些近期的研究（不包括利用OT作为损失函数来解决问题的WGAN方法）。在本研究中，我们弥合了EBMs和熵正则化OT之间的差距，提出了一种新的方法，允许利用前者的最新发展和技术改进来丰富后者。我们在2D情景和标准的图像到图像翻译问题中验证了我们方法的适用性。为简单起见，我们选择了简短和长跑的EBMs。

    Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
    
[^455]: 神经群体动态和几何的可解释统计表示

    Interpretable statistical representations of neural population dynamics and geometry. (arXiv:2304.03376v1 [cs.LG])

    [http://arxiv.org/abs/2304.03376](http://arxiv.org/abs/2304.03376)

    该论文提出了一种基于统计分布的几何深度学习框架，用于表示非线性动态系统的几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。利用该方法，能够解释神经动力学的嵌入，在灵长类似任务中取得了最先进的准确性。

    

    在各种任务中，神经元群体的动态通常在低维流形上演化。然而，区分几何和动态对编码相关行为变量的贡献仍然具有挑战性。在这里，我们引入了一种基于局部相轨特征的统计分布的非线性动态系统的几何深度学习框架，用于表示。我们的方法提供了对几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。我们证明，我们的统计表示可以横跨神经网络实例进行推广，以区分计算机制，在具有几何对应的灵长类似任务中解释嵌入神经动力学，并开发具有最先进准确性的解码算法。我们的结果强调了使用内在流形结构优于时间信息的重要性。

    The dynamics of neuron populations during diverse tasks often evolve on low-dimensional manifolds. However, it remains challenging to discern the contributions of geometry and dynamics for encoding relevant behavioural variables. Here, we introduce an unsupervised geometric deep learning framework for representing non-linear dynamical systems based on statistical distributions of local phase portrait features. Our method provides robust geometry-aware or geometry-agnostic representations for the unbiased comparison of dynamics based on measured trajectories. We demonstrate that our statistical representation can generalise across neural network instances to discriminate computational mechanisms, obtain interpretable embeddings of neural dynamics in a primate reaching task with geometric correspondence to hand kinematics, and develop a decoding algorithm with state-of-the-art accuracy. Our results highlight the importance of using the intrinsic manifold structure over temporal informati
    
[^456]: DeforestVis：使用代理决策树进行机器学习模型行为分析

    DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])

    [http://arxiv.org/abs/2304.00133](http://arxiv.org/abs/2304.00133)

    DeforestVis提供了一种可视化分析工具，通过提供代理决策树，总结了复杂机器学习模型的行为，以帮助用户探索复杂性。

    

    随着机器学习（ML）模型的复杂性增加以及不同（和关键）领域中的应用增加，越来越需要更易解释和可信赖的ML。解释复杂ML模型的一种简单且与模型无关的方法是训练代理模型（例如规则集和决策树），以足够接近原始模型，但更简单和易于解释。然而，规则集可以变得非常冗长，包含许多if-else语句，而决策树的深度会随着准确模拟复杂ML模型而迅速增加。在这种情况下，两种方法都可能无法实现其核心目标，提供用户模型的可解释性。我们通过提出DeforestVis解决了这个问题，这是一种可视化分析工具，通过提供使用自适应增强（AdaBoost）技术生成的代理决策树（一级决策树），为用户提供了对复杂ML模型行为的友好总结。我们的解决方案帮助用户探索模型的复杂性。

    As the complexity of machine learning (ML) models increases and the applications in different (and critical) domains grow, there is a strong demand for more interpretable and trustworthy ML. One straightforward and model-agnostic way to interpret complex ML models is to train surrogate models, such as rule sets and decision trees, that sufficiently approximate the original ones while being simpler and easier-to-explain. Yet, rule sets can become very lengthy, with many if-else statements, and decision tree depth grows rapidly when accurately emulating complex ML models. In such cases, both approaches can fail to meet their core goal, providing users with model interpretability. We tackle this by proposing DeforestVis, a visual analytics tool that offers user-friendly summarization of the behavior of complex ML models by providing surrogate decision stumps (one-level decision trees) generated with the adaptive boosting (AdaBoost) technique. Our solution helps users to explore the comple
    
[^457]: Magnushammer: 一种基于Transformer的前提选择方法

    Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04488](http://arxiv.org/abs/2303.04488)

    Magnushammer是一种基于Transformer的前提选择方法，通过在PISA基准上的测试表明，它可以大幅度超越传统符号系统，并将先前最先进的证明率从57.0％提高到71.0％。

    

    前提选择是自动定理证明的一个基本问题。以往的方法常常使用复杂的符号方法，依赖领域知识，并需要大量的工程工作来解决这个任务。在这项工作中，我们展示了基于神经转换器的Magnushammer方法可以大幅度地超越传统的符号系统。通过在PISA基准上的测试，Magnushammer的证明率达到了59.5％，而最成熟和流行的基于符号的求解器Sledgehammer的证明率只有38.3％。此外，通过将Magnushammer与基于语言模型的神经形式证明器相结合，我们将先前最先进的证明率从57.0％大幅提高到71.0％。

    Premise selection is a fundamental problem of automated theorem proving. Previous works often use intricate symbolic methods, rely on domain knowledge, and require significant engineering effort to solve this task. In this work, we show that Magnushammer, a neural transformer-based approach, can outperform traditional symbolic systems by a large margin. Tested on the PISA benchmark, Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of Sledgehammer, the most mature and popular symbolic-based solver. Furthermore, by combining Magnushammer with a neural formal prover based on a language model, we significantly improve the previous state-of-the-art proof rate from $57.0\%$ to $71.0\%$.
    
[^458]: 基于Koopman算子的全秩权重的泛化界限：新的观点

    Koopman-based generalization bound: New aspect for full-rank weights. (arXiv:2302.05825v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05825](http://arxiv.org/abs/2302.05825)

    我们提出了一种使用Koopman算子对全秩神经网络权重进行泛化的新界限，当权重矩阵的条件数较小时，该界限比现有基于范数的界限更紧。我们的界限不与现有界限相矛盾，而是对现有界限进行的补充。此外，我们的界限可以与现有界限结合以得到更紧的界限。该研究结果为理解全秩权重神经网络的泛化提供了新的视角，同时也为算子理论分析和神经网络泛化之间提供了连接。

    

    我们提出了一种使用Koopman算子对神经网络进行泛化的新界限。大部分现有研究都集中在低秩权重矩阵上，而我们专注于全秩权重矩阵。当权重矩阵的条件数较小时，我们的界限比现有基于范数的界限更紧。特别地，如果权重矩阵是正交的，我们的界限与网络的宽度完全无关。我们的界限不与现有界限相矛盾，而是对现有界限进行的补充。由几个已有实验证明，低秩性并不是泛化的唯一原因。此外，我们的界限可以与现有界限结合以得到更紧的界限。我们的结果为理解具有全秩权重矩阵的神经网络的泛化提供了新的视角，同时还为算子理论分析和神经网络泛化之间提供了连接。

    We propose a new bound for generalization of neural networks using Koopman operators. Whereas most of existing works focus on low-rank weight matrices, we focus on full-rank weight matrices. Our bound is tighter than existing norm-based bounds when the condition numbers of weight matrices are small. Especially, it is completely independent of the width of the network if the weight matrices are orthogonal. Our bound does not contradict to the existing bounds but is a complement to the existing bounds. As supported by several existing empirical results, low-rankness is not the only reason for generalization. Furthermore, our bound can be combined with the existing bounds to obtain a tighter bound. Our result sheds new light on understanding generalization of neural networks with full-rank weight matrices, and it provides a connection between operator-theoretic analysis and generalization of neural networks.
    
[^459]: 乐观的在线镜像下降算法用于连接随机性和对抗性在线凸优化

    Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization. (arXiv:2302.04552v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04552](http://arxiv.org/abs/2302.04552)

    本论文研究了乐观的在线镜像下降算法在Stochastically Extended Adversarial (SEA)模型中的理论保证，对于凸和平滑的函数，其遗憾界限为O(sqrt(σ_{1:T}^2) + sqrt(Σ_{1:T}^2))，对于强凸和平滑的函数，其界限为O(sqrt(σ_{\max}^2) + sqrt(Σ_{\max}^2))。

    

    Sachs等人介绍了Stochastically Extended Adversarial (SEA)模型，作为随机性和对抗性在线凸优化的插值方法。在光滑条件下，他们证明了乐观的Follow-the-Regularized-Leader (FTRL)算法的期望遗憾依赖于凸函数的累积随机方差和累积对抗变化。对于强凸函数，他们也给出了基于最大随机方差和最大对抗变化的稍弱界限。受到他们的工作的启发，我们研究了乐观的在线镜像下降算法在SEA模型中的理论保证。对于凸且平滑的函数，我们得到了相同的遗憾界限，即O(sqrt(σ_{1:T}^2) + sqrt(Σ_{1:T}^2))，而不需要个别函数的凸性要求。对于强凸且平滑的函数，我们建立了一个O(sqrt(σ_{\max}^2) + sqrt(Σ_{\max}^2))的界限。

    Stochastically Extended Adversarial (SEA) model is introduced by Sachs et al. [2022] as an interpolation between stochastic and adversarial online convex optimization. Under the smoothness condition, they demonstrate that the expected regret of optimistic follow-the-regularized-leader (FTRL) depends on the cumulative stochastic variance $\sigma_{1:T}^2$ and the cumulative adversarial variation $\Sigma_{1:T}^2$ for convex functions. They also provide a slightly weaker bound based on the maximal stochastic variance $\sigma_{\max}^2$ and the maximal adversarial variation $\Sigma_{\max}^2$ for strongly convex functions. Inspired by their work, we investigate the theoretical guarantees of optimistic online mirror descent (OMD) for the SEA model. For convex and smooth functions, we obtain the same $\mathcal{O}(\sqrt{\sigma_{1:T}^2}+\sqrt{\Sigma_{1:T}^2})$ regret bound, without the convexity requirement of individual functions. For strongly convex and smooth functions, we establish an $\mathc
    
[^460]: 多源扩散模型用于同时进行音乐生成和分离。

    Multi-Source Diffusion Models for Simultaneous Music Generation and Separation. (arXiv:2302.02257v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2302.02257](http://arxiv.org/abs/2302.02257)

    本研究提出了一种基于扩散模型的生成模型，能够同时进行音乐合成和源分离，并于部分生成和分离任务上实现有竞争力的定量结果。

    

    在本研究中，我们定义了一个扩散基于的生成模型，可以通过学习共享上下文源的联合概率密度的得分来进行音乐合成和源分离。除了经典的总推理任务（即生成混合物体，分离源），我们还引入并在源填充的部分生成任务上进行实验，在这个任务中，我们生成一些源给其他人（例如，演奏一条与鼓相配的钢琴曲）。此外，我们还提出了一种基于Dirac似然函数的分离任务的新推理方法。我们在Slakh2100这个标准的音乐源分离数据集上训练我们的模型，在生成设置中提供定性结果，并展示在源分离设置中的有竞争力的定量结果。我们的方法是第一个能够处理生成和分离任务的单一模型的例子，因此代表了通用音频模型的一步。

    In this work, we define a diffusion-based generative model capable of both music synthesis and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). Additionally, we introduce a novel inference method for the separation task based on Dirac likelihood functions. We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and showcase competitive quantitative results in the source separation setting. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.
    
[^461]: 私密、公平且精确：在医学影像中训练大规模隐私保护的人工智能模型

    Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.01622](http://arxiv.org/abs/2302.01622)

    本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。

    

    人工智能模型在医学领域的应用越来越多。然而，由于医学数据的高度敏感性，需要采取特殊措施确保其保护。保护隐私的黄金标准是引入差分隐私（DP）来进行模型训练。先前的研究表明，DP对模型的准确性和公平性有负面影响，这在医学中是不可接受的，并且是隐私保护技术广泛应用的主要障碍。在这项工作中，我们评估了隐私保护训练人工智能模型对准确性和公平性的影响，与非隐私训练进行了比较。为此，我们使用了两个数据集：（1）一个大规模数据集（N=193,311）的高质量临床胸部X射线图像，和（2）一个数据集（N=1,625）的3D腹部计算机断层扫描（CT）图像，用于分类胰腺导管腺癌（PDAC）的存在。两个数据集均为回顾性采集，并由经验丰富的医学影像专家进行手动标注。

    Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
    
[^462]: 使用Tsallis KL散度的广义Munchausen强化学习

    Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11476](http://arxiv.org/abs/2301.11476)

    这篇论文通过研究广义的Tsallis KL散度，扩展了Munchausen强化学习算法，并提供了一种将KL正则化纳入实际算法的方法。对于Tsallis KL，当$q > 1$时，可以获得新的策略优化选项。

    

    许多强化学习中的策略优化方法都采用Kullback-Leibler（KL）散度到上一个策略，以防止策略变化过快。这个想法最初是在Conservative Policy Iteration的一篇重要论文中提出的，近似算法如TRPO和Munchausen Value Iteration（MVI）给出了有限的方法。我们通过研究一种广义的KL散度 - 称为Tsallis KL散度 - 来继续这一工作，它在定义中使用了$q$-对数。这种方法是一种严格的推广，因为$q = 1$对应于标准的KL散度；$q > 1$提供了一系列新的选项。我们对在Tsallis KL下学习的策略类型进行了表征，并阐述了何时$ q > 1 $可能是有益的。为了获得一个将Tsallis KL正则化纳入实际算法的方法，我们扩展了MVI，它是一种最简单的包含KL正则化的方法之一。我们展示了这种广义MVI（$q$）获得了显著的改进。

    Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q > 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q >1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
    
[^463]: 关于网络规模下的大规模多重检验：一种渐进方法

    On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach. (arXiv:2211.16059v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2211.16059](http://arxiv.org/abs/2211.16059)

    本文提出了两种针对分布式环境的多重检验方法：比例匹配和贪婪聚合。贪心聚合方法有效地近似了每个节点的最优拒绝区域，且具有计算效率性质。

    

    本文致力于开发计算和通信效率高的多重检验网络方法，对许多实际应用具有重要意义。我们采用一种渐进的方法，并提出了两种针对分布式环境的方法：比例匹配和贪婪聚合。比例匹配方法实现了全局 BH（Benjamini–Hochberg）性能，仅需要通信一次（估计）真零假设比例以及每个节点的 p 值数量。通过关注渐进最优功率，我们超越了 BH 过程，并提供了渐近最优解的显式特征。这导致了贪心聚合方法，该方法有效地近似了每个节点的最优拒绝区域，而计算效率自然来自贪心类型方法。此外，对于这两种方法，我们提供了 FDR 和功率的收敛速度。在各种情况下进行的广泛数值结果表明了方法的实用性。

    This work concerns developing communication- and computation-efficient methods for large-scale multiple testing over networks, which is of interest to many practical applications. We take an asymptotic approach and propose two methods, proportion-matching and greedy aggregation, tailored to distributed settings. The proportion-matching method achieves the global BH performance yet only requires a one-shot communication of the (estimated) proportion of true null hypotheses as well as the number of p-values at each node. By focusing on the asymptotic optimal power, we go beyond the BH procedure by providing an explicit characterization of the asymptotic optimal solution. This leads to the greedy aggregation method that effectively approximates the optimal rejection regions at each node, while computation efficiency comes from the greedy-type approach naturally. Moreover, for both methods, we provide the rate of convergence for both the FDR and power. Extensive numerical results over a va
    
[^464]: 顺序知情联合消除：联邦优化中高效且可证明的客户端消除

    Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11656](http://arxiv.org/abs/2211.11656)

    本文提出一种名为知情联合消除（IFU）的新颖联邦优化方法，可实现有效且可量化的客户端消除请求，实验结果表明其效率较基本方法和最先进的FU方法更高。

    

    机器消除（MU）旨在提供有关从训练过程中删除给定数据点的贡献的理论保证。联邦消除（FU）是将MU扩展到从联合训练过程中消除给定客户端的贡献。当前的FU方法通常不具有可扩展性，并且没有对消除效果的有效性进行合理的理论量化。在本文中，我们提出了知情联合消除(IFU)，这是一种新颖的高效且可量化的FU方法。在接收到给定客户端的消除请求后，IFU通过随机扰动机制确定了重新初始化FL所需的最佳FL迭代，可以获得消除保证。IFU的理论也可以扩展以解决顺序消除请求。不同任务和数据集上的实验结果表明，与基本重新训练和最先进的FU方法相比，IFU可以实现更高效的消除过程。

    The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.
    
[^465]: 重新思考反事实解释：作为局部和区域反事实政策的反事实解释

    Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies. (arXiv:2209.14568v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.14568](http://arxiv.org/abs/2209.14568)

    本文提出了一种概率框架，为每个观测值提供稀疏的局部反事实规则，并将这些规则聚合成区域反事实规则，以适应不稳定的实现环境，并产生稳健的救济措施。

    

    反事实解释（CE）面临着许多未解决的挑战，如确保稳定性、综合多个CE以及提供合理性和稀疏性保证。从更实际的角度来看，最近的研究表明，所规定的反事实救济措施通常不会被个体完全实施，并证明大多数最先进的CE算法在这种嘈杂的环境中很可能失败。为了解决这些问题，我们提出了一个概率框架，为每个观测值提供稀疏的局部反事实规则，提供能够以高概率改变决策的值范围的规则。这些规则作为多样的反事实解释的总结，并产生稳健的救济措施。我们进一步将这些局部规则聚合成区域反事实规则，识别数据子组的共享救济措施。我们的局部和区域规则来自于随机森林算法。

    Counterfactual Explanations (CE) face several unresolved challenges, such as ensuring stability, synthesizing multiple CEs, and providing plausibility and sparsity guarantees. From a more practical point of view, recent studies [Pawelczyk et al., 2022] show that the prescribed counterfactual recourses are often not implemented exactly by individuals and demonstrate that most state-of-the-art CE algorithms are very likely to fail in this noisy environment. To address these issues, we propose a probabilistic framework that gives a sparse local counterfactual rule for each observation, providing rules that give a range of values capable of changing decisions with high probability. These rules serve as a summary of diverse counterfactual explanations and yield robust recourses. We further aggregate these local rules into a regional counterfactual rule, identifying shared recourses for subgroups of the data. Our local and regional rules are derived from the Random Forest algorithm, which of
    
[^466]: 网络流的图神经建模

    Graph Neural Modeling of Network Flows. (arXiv:2209.05208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05208](http://arxiv.org/abs/2209.05208)

    本文提出了一种新颖的网络流问题图学习架构 PEW，相较于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。

    

    网络流问题涉及将流量分布在网络中，以使基础设施得到有效利用，这在交通运输和物流中是无处不在的。其中，多商品网络流 (MCNF) 问题是普遍感兴趣的，因为它涉及在多个源和汇之间分配不同大小的多个流，同时实现链路的有效利用。由于数据驱动优化的吸引力，这些问题越来越多地使用图学习方法来解决。本文提出了一种新颖的网络流问题图学习架构 PEW (Per-Edge Weights)。此方法基于图注意力网络，并沿着每个链接使用不同参数化的消息函数。我们通过使用 $17$ 个服务提供商拓扑和 $2$ 个路由方案进行互联网流量路由案例研究，对所提出的解决方案进行了广泛的评估。我们展示了 PEW 相对于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。

    Network flow problems, which involve distributing traffic over a network such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the Multi-Commodity Network Flow (MCNF) problem is of general interest, as it concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures wh
    

