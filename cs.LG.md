# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Matcha-TTS: A fast TTS architecture with conditional flow matching.](http://arxiv.org/abs/2309.03199) | Matcha-TTS是一种快速TTS架构，使用最优传输条件流匹配训练，具有高质量输出和快速合成步骤。它不需要外部对齐，与其他模型相比，具有最小的内存占用，速度更快，并在听觉测试中获得了最高的评分。 |
| [^2] | [Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation.](http://arxiv.org/abs/2309.03190) | 本文提出了一种使用链接本地差分隐私的方法，在图神经网络中实现与不受信任的服务器的协作训练。通过贝叶斯估计，将隐私预算分别用于链接和图的度，缓解差分隐私对训练准确性的负面影响，并限制链接概率推断与真实图拓扑之间的误差。提出的LDP机制有两个变体，在不同隐私设置下互补使用，以避免误报链接估计问题。 |
| [^3] | [SLiMe: Segment Like Me.](http://arxiv.org/abs/2309.03179) | 基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。 |
| [^4] | [3D Object Positioning Using Differentiable Multimodal Learning.](http://arxiv.org/abs/2309.03177) | 本文介绍了一种使用模拟激光雷达数据和图像像素误差的多模态方法，通过可微分渲染来优化物体在计算机图形场景中的位置。使用第二种模态（激光雷达）能够加快收敛，这为自动驾驶车辆提供了潜在的用途，并提出了多种数据模拟方法用于训练自动驾驶车辆。 |
| [^5] | [Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach.](http://arxiv.org/abs/2309.03169) | 这个论文提出了一种基于印象感知的多行为推荐系统，通过利用注意机制从行为间和行为内部获取信息，并采用多层级图注意力方法，来解决推荐系统在处理多个行为之间互动方面的挑战。 |
| [^6] | [Split-Boost Neural Networks.](http://arxiv.org/abs/2309.03167) | 这篇论文提出了一种称为分割增强的训练策略，通过自动包含正则化行为，降低了神经网络训练的复杂性和计算资源需求。 |
| [^7] | [Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning.](http://arxiv.org/abs/2309.03157) | 本论文提出了一种通过深度强化学习学习充电来解决无人机覆盖路径规划问题的方法。该方法利用基于地图的观测信息，在整个任务周期内优化覆盖轨迹，并采用动作屏蔽和折扣因子调度等技术。实验结果表明，该方法优于基准启发式方法，在不同目标区域和地图上具有一定的泛化性能。 |
| [^8] | [Data-Driven Neural Polar Codes for Unknown Channels With and Without Memory.](http://arxiv.org/abs/2309.03148) | 本文提出了一种基于数据驱动的方法，用于设计具有和不具有记忆的信道的极化码。通过替代原始的连续取消解码器的核心元素，即检验节点、比特节点和软判决，设计了神经连续取消解码器。该方法具有理论保证和较低的计算复杂度。 |
| [^9] | [The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits.](http://arxiv.org/abs/2309.03145) | 该论文通过多通道流算法给出了纯探索多臂赌博机中的近乎最优样本通道交换界限，并回答了一个悬而未决的问题。 |
| [^10] | [Using Multiple Vector Channels Improves E(n)-Equivariant Graph Neural Networks.](http://arxiv.org/abs/2309.03139) | 本文提出了一种对E(n)-等变图神经网络的自然扩展，使用每个节点的多个等变向量。多通道EGNN在多个物理系统任务上的性能优于标准的单通道EGNN，而且几乎没有额外的运行时间或参数数量的差异。 |
| [^11] | [Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features.](http://arxiv.org/abs/2309.03113) | 通过利用锡膏检查特征，基于数据的机器学习方法能够在PCB制造中的三个阶段检测缺陷，提高操作效率和减少人工干预。 |
| [^12] | [Solving multiscale elliptic problems by sparse radial basis function neural networks.](http://arxiv.org/abs/2309.03107) | 本研究提出了一种稀疏径向基函数神经网络方法，用于解决具有多尺度系数的椭圆偏微分方程（PDE）。该方法通过引入正则化项来避免过拟合问题，并通过优化特定损失函数来加速训练过程。 |
| [^13] | [ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure.](http://arxiv.org/abs/2309.03103) | ContrastWSD是一种使用了词义消岐的隐喻检测模型，通过将隐喻识别过程和词义消岐结合起来，提取并对比单词的上下文含义和基本含义，以提高隐喻检测的效果，超过其他仅依赖上下文嵌入或集成基本定义和外部知识的方法。 |
| [^14] | [PeptideBERT: A Language Model based on Transformers for Peptide Property Prediction.](http://arxiv.org/abs/2309.03099) | PeptideBERT是一种基于Transformer的语言模型，用于预测肽的溶血性、溶解性和阻垢性。通过利用预训练模型和微调，该模型在这些任务上取得了最先进的性能。 |
| [^15] | [Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties.](http://arxiv.org/abs/2309.03094) | 本文提出了一种适用于稀疏加权分位数回归的新型单循环平滑ADMM算法，名为SIAD，它在存在非凸和非光滑稀疏惩罚条件下能够加速收敛速度。 |
| [^16] | [Establishing Markov Equivalence in Cyclic Directed Graphs.](http://arxiv.org/abs/2309.03092) | 本论文提出了一种在循环有向图中建立马尔可夫等价关系的新方法，该方法不再需要对d-分离进行测试，大大减小了算法的复杂性，并且在存在潜在混淆因素的情况下具有重要的理论研究价值。 |
| [^17] | [LieDetect: Detection of representation orbits of compact Lie groups from point clouds.](http://arxiv.org/abs/2309.03086) | LieDetect是一种从紧致Lie群的有限样本轨道中估计表示的新算法。与其他技术不同，该算法可以检索精确的表示类型，并重建其轨道，有助于识别生成该作用的Lie群。该算法适用于任何紧致Lie群，并在多个领域的应用中取得了非常准确的结果。 |
| [^18] | [Pure Monte Carlo Counterfactual Regret Minimization.](http://arxiv.org/abs/2309.03084) | 纯蒙特卡洛反事实遗憾最小化算法（PCFR）是一种结合了反事实遗憾最小化（CFR）和虚拟游戏（FP）概念的新算法，能够与各种CFR变体相结合，包括蒙特卡洛CFR（MCCFR）。PCFR具有更好的性能和较快的收敛速度，同时降低了时间和空间复杂度。 |
| [^19] | [ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning.](http://arxiv.org/abs/2309.03081) | ORL-AUDITOR是一种用于审核离线深度强化学习数据集的方法，以保护知识产权和防止滥用或侵权风险。 |
| [^20] | [GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models.](http://arxiv.org/abs/2309.03079) | 本论文利用大型语言模型分析股票市场上上市公司的年度报告，生成洞察，并通过历史股价数据训练机器学习模型，取得相对标普500指数的超额回报。 |
| [^21] | [Parameterizing pressure-temperature profiles of exoplanet atmospheres with neural networks.](http://arxiv.org/abs/2309.03075) | 该论文提出了一种使用神经网络对系外行星大气压力-温度分布进行参数化的方法，该方法不依赖于对分布功能形式的明确假设，使用更少的参数，并能生成具有物理一致性的分布。 |
| [^22] | [Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation.](http://arxiv.org/abs/2309.03072) | 提出了一种基于Transformer的方法，用于在线手写字符分割。在该方法中，将分割与识别解耦，通过将其视为样点和文本中字符之间的分配问题，并利用Transformer解码器块中学习到的字符查询来形成每个聚类。对该方法进行了实验验证，并在两个常用的在线手写字迹数据集上创建了准确的标签。 |
| [^23] | [Learning Active Subspaces for Effective and Scalable Uncertainty Quantification in Deep Neural Networks.](http://arxiv.org/abs/2309.03061) | 该论文提出了一种通过识别对神经网络输出具有最显著影响的参数方向构建低维子空间的方法，从而解决了贝叶斯深度学习中由于参数空间高维度而带来的计算复杂性问题。通过在显著减少的主动子空间上进行蒙特卡罗采样或变分推理，该方法实现了有效和可扩展的贝叶斯推理，并通过多个回归任务的实证验证了可靠的预测和鲁棒的不确定性估计。 |
| [^24] | [CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra.](http://arxiv.org/abs/2309.03060) | CoLA是一个用于机器学习中大规模线性代数问题的简单但通用的框架，通过组合调度规则和线性操作符抽象，自动构建了内存和运行时高效的数值算法，提供了内存高效的自动微分、低精度计算和GPU加速，同时可以适应下游软件包中的新对象、操作和规则。 |
| [^25] | [Automated CVE Analysis for Threat Prioritization and Impact Prediction.](http://arxiv.org/abs/2309.03040) | 本论文提出了一种自动化CVE分析方法，用于威胁优先级和影响预测。它解决了CVE描述的缺陷，通过提供攻击语义信息，使得CVE的综合弱点特征和威胁影响得以全面评估。这种自动化分析方法可以大大加快CVE漏洞分析的速度和效率。 |
| [^26] | [Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis.](http://arxiv.org/abs/2309.03033) | 本研究利用深度学习方法通过基因表达分析实现对患者多囊肾病（PKD）的准确和早期检测。 |
| [^27] | [Universal Preprocessing Operators for Embedding Knowledge Graphs with Literals.](http://arxiv.org/abs/2309.03023) | 本文提出了一组通用预处理操作符，用于将具有数值、时间、文本和图像信息的知识图谱转换为能够使用任何嵌入方法的形式，并在数据集上展示了有希望的结果。 |
| [^28] | [Amortised Inference in Bayesian Neural Networks.](http://arxiv.org/abs/2309.03018) | 本文提出了一种摊销推理的贝叶斯神经网络方法，通过对推理进行摊销，能够更有效地利用数据进行概率元学习。 |
| [^29] | [SymED: Adaptive and Online Symbolic Representation of Data on the Edge.](http://arxiv.org/abs/2309.03014) | SymED 是一种在线自适应和分布式的方法，用于在边缘上进行数据的符号化表示。它解决了在资源受限的边缘设备上进行实时数据处理的挑战，并可以通过符号化表示来进行各种边缘应用的数据分析。 |
| [^30] | [Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness.](http://arxiv.org/abs/2309.03004) | 提出了一个理论解释，将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。 |
| [^31] | [Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models.](http://arxiv.org/abs/2309.02976) | 本论文通过使用强化学习来解决肌肉骨骼冗余问题，并实现了在高维度肌肉骨骼模型中自然且稳健的行走。 |
| [^32] | [On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision Making.](http://arxiv.org/abs/2309.02970) | 该论文研究了饲料成本风险对水产养殖估值和决策的影响，并成功提出了使用深度神经网络推断决策边界的方法，对于改进传统方法的回归和曲线拟合方法已取得了显著的改善。 |
| [^33] | [CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse.](http://arxiv.org/abs/2309.02968) | CR-VAE是一种对比正则化变分自动编码器的方法，通过增加对比目标来最大化类似视觉输入之间的互信息，从而避免后验坍塌现象，并在多个视觉数据集上表现出比最先进方法更好的性能。 |
| [^34] | [M3D-NCA: Robust 3D Segmentation with Built-in Quality Control.](http://arxiv.org/abs/2309.02954) | M3D-NCA是一种鲁棒的3D分割方法，具有内建质量控制，能够在资源受限的环境中运行。通过利用Neural Cellular Automata（NCA）分割和新的质量指标，M3D-NCA在海马体和前列腺分割中超越了大规模UNet模型。 |
| [^35] | [Estimating irregular water demands with physics-informed machine learning to inform leakage detection.](http://arxiv.org/abs/2309.02935) | 本研究提出了一种利用物理信息的机器学习算法，通过分析压力数据估计未知的不规则用水需求，并通过简化泄漏检测问题实现线性化。 |
| [^36] | [GroupEnc: encoder with group loss for global structure preservation.](http://arxiv.org/abs/2309.02917) | 本文提出了一个名为GroupEnc的编码器模型，该模型利用了群损失函数，以实现比传统的变分自动编码器（VAE）更少的全局结构失真，并保持模型的参数化和架构的灵活性。 |
| [^37] | [Persona-aware Generative Model for Code-mixed Language.](http://arxiv.org/abs/2309.02915) | 本论文提出了一种针对混合语言的人物感知生成模型PARADOX，它能够生成类似于真实个体代码混合文本的文本。该模型以用户的人物形象为条件来编码对话，并生成不带单语参考数据的代码混合文本。模型还进行对齐，使生成的文本更接近真实的代码混合文本。这种方法在语义上更有意义，在语言上更有效。 |
| [^38] | [Ensemble DNN for Age-of-Information Minimization in UAV-assisted Networks.](http://arxiv.org/abs/2309.02913) | 该论文提出了一种集成深度神经网络（EDNN）的方法，通过优化无人机的停留位置和设备选择概率，减少了设备之间的预期时延信息（AoI）。实验结果表明，该方法比传统的深度神经网络（DNNs）在降低预期AoI方面表现更好，可实现29.5%的降低。 |
| [^39] | [A Multimodal Learning Framework for Comprehensive 3D Mineral Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships.](http://arxiv.org/abs/2309.02911) | 本研究提出了一种新颖的多模态融合模型，通过深度网络架构有效地整合结构和流体信息，优于其他模型的结果分析表明其在区分含矿实例和预测矿产前景方面表现出优越的性能；消融研究进一步揭示了联合特征利用和CCA融合的益处。这对于增强勘探决策具有重要作用。 |
| [^40] | [DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings.](http://arxiv.org/abs/2309.02908) | 本论文介绍了一种基于历史数据、占用模式和天气条件的LSTM模型，用于准确预测建筑能耗，该模型在预测精度上表现出卓越性能。 |
| [^41] | [A Unified Framework for Discovering Discrete Symmetries.](http://arxiv.org/abs/2309.02898) | 本文提出了一个统一框架，能够发现各种类型的对称性，通过使用线性和张量值函数构成的新颖架构，在多臂赌博算法和梯度下降的帮助下，高效地优化并学习到对称性。在图像数字求和和多项式回归任务上的实验证明了该方法的有效性。 |
| [^42] | [Non-Clashing Teaching Maps for Balls in Graphs.](http://arxiv.org/abs/2309.02876) | 本文研究了在图中球的概念类中的非冲突教学图，并证明了相关决策问题{\sc B-NCTD$^+$}是NP完全的。 |
| [^43] | [Learning Hybrid Dynamics Models With Simulator-Informed Latent States.](http://arxiv.org/abs/2309.02873) | 本文提出了一种通过用模拟器更新潜在状态的方法来学习混合动力学模型的新方法，以控制预测并防止累积误差的发生。 |
| [^44] | [Rethinking Momentum Knowledge Distillation in Online Continual Learning.](http://arxiv.org/abs/2309.02870) | 该论文重新思考了在线连续学习中的动量知识蒸馏问题，通过将动量知识蒸馏应用于OCL方法，提高了现有方法的准确性，并对MKD在OCL中的训练过程进行了深入分析。 |
| [^45] | [On Reducing Undesirable Behavior in Deep Reinforcement Learning Models.](http://arxiv.org/abs/2309.02869) | 本论文提出了一个旨在降低深度强化学习模型不良行为的框架，通过从错误的状态-动作对中提取决策树分类器，并将其整合到训练循环中，来惩罚系统错误行为。这一框架在保持卓越性能的同时，为工程师提供了针对不良行为的可理解表征。 |
| [^46] | [Enhancing Event Sequence Modeling with Contrastive Relational Inference.](http://arxiv.org/abs/2309.02868) | 本文提出了一种利用对比关系推理的方法来增强事件序列建模。通过学习一个关系图并在观测数据中捕捉动态模式，我们的模型能够推断事件之间的相互作用，从而在事件序列数据的推理任务中表现出较好的效果。 |
| [^47] | [Generalised Mutual Information: a Framework for Discriminative Clustering.](http://arxiv.org/abs/2309.02858) | 本文介绍了通用互信息（GEMINI）作为一种辨别聚类的框架，相比互信息（MI），GEMINI在无监督神经网络训练过程中不需要正则化，其可以选择合适的聚类数量。 |
| [^48] | [A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques.](http://arxiv.org/abs/2309.02854) | 本文对六个公开可用的日志数据集进行了分析，重点关注异常的表现形式和其检测的简单技术。 |
| [^49] | [Random postprocessing for combinatorial Bayesian optimization.](http://arxiv.org/abs/2309.02842) | 针对组合贝叶斯优化，我们研究了一种随机后处理方法，严格禁止数据集中的重复样本，结果表明此方法显著减少了顺序步骤数，特别是在最大后验估计的情况下，为解决高维问题中贝叶斯优化的收敛速度慢提供了一种简单但通用的策略。 |
| [^50] | [BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network.](http://arxiv.org/abs/2309.02836) | 本文研究了利用切片对抗网络（SAN）来增强基于GAN的神经声码器，并通过实验证明了通过修改最小二乘GAN的损失函数，SAN可以改善声码器的性能。 |
| [^51] | [Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks.](http://arxiv.org/abs/2309.02820) | Roulette是一种用于深度学习分类任务的语义隐私保护的设备边缘协同推理框架，通过混淆和加噪声实现隐私保护，同时保持高准确性。 |
| [^52] | [Combining Thermodynamics-based Model of the Centrifugal Compressors and Active Machine Learning for Enhanced Industrial Design Optimization.](http://arxiv.org/abs/2309.02818) | 结合离心压缩机的热力学模型和主动机器学习，提出了Active-CompDesign框架用于离心压缩机的优化设计。在离线和在线环境中进行实验，显示出显著的性能提升。 |
| [^53] | [Introducing Thermodynamics-Informed Symbolic Regression -- A Tool for Thermodynamic Equations of State Development.](http://arxiv.org/abs/2309.02805) | 引入热力学信息的符号回归是一个用于加速热力学状态方程开发的工具，该工具结合了符号回归的方法和处理实验数据的特性。 |
| [^54] | [Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory.](http://arxiv.org/abs/2309.02787) | 这项研究提出了一种在移动边缘计算中实现动态编码和解码的分离学习框架，利用信息瓶颈理论实现传输资源消耗和共享潜在表示的平衡，从而提高预测性能。 |
| [^55] | [CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model.](http://arxiv.org/abs/2309.02785) | 本文介绍了一种基于语义信息提取和领域特定语言模型的CVE驱动攻击技术预测方法，通过分析CVE描述并推断出由CVE利用导致的可能的TTP攻击，有效解决了CVE和TTP之间的差距，为实时漏洞情报分析和对策制定提供了重要洞见。 |
| [^56] | [Norm Tweaking: High-performance Low-bit Quantization of Large Language Models.](http://arxiv.org/abs/2309.02784) | 本文介绍了一种称为“norm tweaking”的技术，通过调整量化的激活分布来实现高精度的低比特量化，以提高大型语言模型的压缩性能。 |
| [^57] | [Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review.](http://arxiv.org/abs/2309.02783) | 这项范围审查总结了最近基于视觉变换器的人工智能方法在肺癌成像应用方面的发展，重点探讨了如何改善肺癌的诊断和预后，以及相关数据集的贡献。 |
| [^58] | [On the Effects of Heterogeneous Errors on Multi-fidelity Bayesian Optimization.](http://arxiv.org/abs/2309.02771) | 本文研究了异质误差对多保真贝叶斯优化的影响，并提出了解决现有假设不成立时性能下降的方法。 |
| [^59] | [Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond.](http://arxiv.org/abs/2309.02769) | 本研究提出了一种基于物理信息的方法，通过反转图热方程的时间方向，提高了图神经网络的节点特征清晰度，并引入了多尺度热核滤波函数，增强了GNNs的表达能力。进一步推广为G-MHKG模型，探索了更灵活的滤波条件，以解决过度平滑和过度压缩等计算挑战。这些方法和模型在增强GNNs性能方面具有潜力。 |
| [^60] | [Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing.](http://arxiv.org/abs/2309.02762) | 提出了一个更通用的图补全学习（GCL）框架来解决节点特征和结构关系缺失的问题。 |
| [^61] | [SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series.](http://arxiv.org/abs/2309.02752) | 该论文提出了一种针对时间序列的新的对抗攻击方法SWAP，通过提高次级logits的置信度，同时最小化对其他logits的干扰来实现攻击。实验证明，该方法在ASR上取得了最先进的性能。 |
| [^62] | [Offensive Hebrew Corpus and Detection using BERT.](http://arxiv.org/abs/2309.02724) | 本研究提出了一个新的希伯来语侮辱性语料库，并使用两个希伯来语BERT模型（HeBERT和AlephBERT）进行了微调。我们观察到，我们的数据结合D_OLaH可以提高HeBERT模型的性能2%。此外，我们的数据对AlephBERT模型也具有一定的泛化性能。 |
| [^63] | [Unveiling the frontiers of deep learning: innovations shaping diverse domains.](http://arxiv.org/abs/2309.02712) | 本文广泛研究了深度学习在各个主要研究领域中的潜在应用，揭示了其准确性和计算能力的优势，以及相关的挑战。 |
| [^64] | [Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension.](http://arxiv.org/abs/2309.02711) | 本研究提出了自适应对称学习（ASL）方法，通过模型最小化的方法，在学习过程中自适应地解决不完全或不精确的对称描述。ASL包括对称拟合组件和模块化损失函数，能高效适应对称性任务。 |
| [^65] | [Improved Outlier Robust Seeding for k-means.](http://arxiv.org/abs/2309.02710) | 本论文提出了一种改进的$k$-means初始化方法，使其在存在异常值的情况下更加鲁棒。算法在复杂度低的同时提供了给定数据的有效聚类。 |
| [^66] | [Certifying LLM Safety against Adversarial Prompting.](http://arxiv.org/abs/2309.02705) | 本研究提出了首个具有可验证安全保证的框架——消除和检查，用于对抗敌对提示。通过逐个消除标记并使用安全过滤器检查生成的子序列，确保任何敌对修改的有害输入提示都能被正确标识为有害。 |
| [^67] | [Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation.](http://arxiv.org/abs/2309.02685) | 本文提出了Diffusion-EDFs，一种在视觉机器人操作中应用的基于SE(3)的等变去噪生成建模方法。通过集成SE(3)等变性，我们的方法展示了出色的数据效率和泛化能力。 |
| [^68] | [RLSynC: Offline-Online Reinforcement Learning for Synthon Completion.](http://arxiv.org/abs/2309.02671) | RLSynC是一种离线-在线强化学习方法，用于半模板化逆向合成中的合成物补全。它使用多个代理同时完成合成物的补全，并通过正向合成模型评估反应物的合成能力来指导行动搜索。 |
| [^69] | [Marketing Budget Allocation with Offline Constrained Deep Reinforcement Learning.](http://arxiv.org/abs/2309.02669) | 我们提出了一种基于混合策略的离线值强化学习方法，有效解决了在线营销活动中的预算分配问题。该方法实现了近乎最优的策略效率，并保证收敛到最优策略。 |
| [^70] | [Contrastive Learning as Kernel Approximation.](http://arxiv.org/abs/2309.02651) | 本论文总结了目前对比学习作为核近似的理论理解，通过在大型未标注数据集上将高维输入转化为低维特征表示，可以实现在有限标注数据上达到高准确性的监督学习。 |
| [^71] | [TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction.](http://arxiv.org/abs/2309.02641) | 本文提出了一种新颖的双重方面Transformer模型，用于故障预测，有效处理长序列日志和提供预测值的置信区间。 |
| [^72] | [Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation.](http://arxiv.org/abs/2309.02640) | Epi-Curriculum是一种用于神经机器翻译中低资源领域自适应的方法，通过分集训练和去噪的课程学习，提高了模型对领域变化的鲁棒性和适应性。 |
| [^73] | [Multiclass Alignment of Confidence and Certainty for Network Calibration.](http://arxiv.org/abs/2309.02636) | 提出了一种新的训练时校准方法，通过对预测均值置信度和确定性进行对齐，从而改进了深度神经网络在多分类问题上的模型校准性能。 |
| [^74] | [Deep Reinforcement Learning from Hierarchical Weak Preference Feedback.](http://arxiv.org/abs/2309.02632) | 本研究探讨了如何利用分层的弱偏好反馈进行深度强化学习。通过学习奖励函数，与人类偏好非常一致的复杂奖励可以帮助强化学习解决日益困难的问题。 |
| [^75] | [Superclustering by finding statistically significant separable groups of optimal gaussian clusters.](http://arxiv.org/abs/2309.02623) | 本文提出了一种算法，通过找到统计显著可分离的最佳高斯簇的分组，实现超聚类。算法具有三个阶段，包括表示数据集为高斯混合分布-簇、使用马氏距离估计簇之间的距离和大小以及将簇组合为超簇。算法的创新点在于引入了矩阵质量准则，并通过自动选择合适的统计显著性水平来确定最佳超簇数量。 |
| [^76] | [Compressing Vision Transformers for Low-Resource Visual Learning.](http://arxiv.org/abs/2309.02617) | 本研究旨在通过使用模型压缩技术，将视觉Transformer应用于资源受限的边缘设备，如无人机。这可以在保持准确性的同时减小模型大小和计算需求。 |
| [^77] | [Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts.](http://arxiv.org/abs/2309.02616) | 本文提出了一种通过多模态提示实现AI辅助的无训练安全语义通信系统，利用生成模型的强大学习能力实现精确内容解码。 |
| [^78] | [Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts.](http://arxiv.org/abs/2309.02615) | 本研究开发了一种生成算法，通过卫星数据与物理模型融合，利用火灾到达时间作为火灾历史的简明表示来初始化火灾预测模型。 |
| [^79] | [Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds.](http://arxiv.org/abs/2309.02614) | 本文研究了使用生成对抗网络（GANs）为愤怒的小鸟生成复杂且稳定的结构。实验结果表明GANs可以成功应用于生成多样化的愤怒的小鸟结构。 |
| [^80] | [T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data.](http://arxiv.org/abs/2309.02610) | 本文提出了一个名为T-SaS的贝叶斯框架，用于解决在流数据中出现突然分布转变的问题。这个框架能够有效地划分不同制度并捕捉到变化的模式。 |
| [^81] | [Distributed Variational Inference for Online Supervised Learning.](http://arxiv.org/abs/2309.02606) | 本文提出了一种适用于智能传感器网络的分布式变分推断算法，可以解决连续变量、大规模实时数据和难以处理的后验概率的推断问题。通过推导出可分离的较低下界，实现了在传感器网络中进行一跳通信的分布式变分推断，并提出了处理二进制问题的方法。 |
| [^82] | [Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet.](http://arxiv.org/abs/2309.02604) | TriNet是一种用于医疗指导的机器学习模型，可以在挂号处自动筛查出需要下游测试进行诊断确认的病症。这些模型优于当前的临床基准，表明机器学习医疗指导可以提供无成本、非侵入性的筛查。 |
| [^83] | [Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks.](http://arxiv.org/abs/2309.02596) | 本研究调查了自监督预训练在肺部超声分析中的应用。预训练模型在多个任务上微调时表现出改善的性能和推理效率，而且还能在使用少量标签进行训练时超过完全监督模型。 |
| [^84] | [Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning.](http://arxiv.org/abs/2309.02591) | CM3Leon是一个缩放自回归多模态语言模型，通过预训练和指令调整实现了高质量的文本和图像的生成和填充，达到了文本到图像生成方面的最先进性能，而计算资源开销较小。 |
| [^85] | [Representation Learning for Sequential Volumetric Design Tasks.](http://arxiv.org/abs/2309.02583) | 本研究提出了一种顺序体积设计任务的表示学习方法，通过利用transformer模型从专家的设计序列中提取有用的表示来提高自动生成体积设计的质量，以及支持设计偏好评估和程序化设计生成。 |
| [^86] | [Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients.](http://arxiv.org/abs/2309.02580) | 本研究提出了一种新颖而全面的框架，通过评估机器学习算法对儿童患者的脑电图信号进行分析，揭示了难治性癫痫的脑网络，并实现了可扩展的癫痫预测。 |
| [^87] | [Anatomy-Driven Pathology Detection on Chest X-rays.](http://arxiv.org/abs/2309.02578) | 本研究提出了一种基于解剖学驱动的胸部X射线病理检测方法，通过使用易于标注的解剖区域边界框作为病理的代理，取得了比弱监督方法和完全监督方法更好的性能。 |
| [^88] | [Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks.](http://arxiv.org/abs/2309.02576) | 该论文介绍了一种基于深度学习的方法，在胸部计算机断层扫描中自动化肺气肿亚型划分和严重程度分析。通过在COPDGene研究中的大规模实验中评估，该算法在预测准确性上表现出色，并与肺气肿的可视评分具有良好的一致性。 |
| [^89] | [Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach.](http://arxiv.org/abs/2309.02571) | 该论文提出了一种基于FFT的方法，用于恢复线性动态系统的因果结构。通过降低计算复杂度，可以有效地从时间序列中获取动态因果效应。 |
| [^90] | [Diffusion-based Time Series Data Imputation for Microsoft 365.](http://arxiv.org/abs/2309.02564) | 本研究提出了一种基于扩散的数据插补方法Diffusion+，通过观察到的数据高效地插补缺失数据，提高了微软365的数据质量，进而改善了下游故障预测任务的性能。 |
| [^91] | [Sparse Partitioning Around Medoids.](http://arxiv.org/abs/2309.02557) | 这篇论文介绍了一种稀疏中心对象分区的方法，通过利用稀疏性和不对称情况，该方法可以避免二次运行时间和内存要求，在处理大型问题时具有可扩展性。 |
| [^92] | [Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images.](http://arxiv.org/abs/2309.02556) | 本文提出了一种使用视觉Transformer(ViT)进行模型微调的领域自适应方法，可解决使用转换图像训练模型导致准确性下降的问题，实验证明该方法在使用加密图像时也能保持模型的准确性。 |
| [^93] | [A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images.](http://arxiv.org/abs/2309.02555) | 自监督预训练在放射学图像诊断任务中提高了下游任务性能，尤其在无标记样本远远多于有标记样本时。 |
| [^94] | [Data Aggregation for Hierarchical Clustering.](http://arxiv.org/abs/2309.02552) | 本研究介绍了如何利用数据聚合算法BETULA使得资源受限系统上的层次聚类方法HAC变得可行，从而允许对非常大的数据集进行探索性数据分析。 |
| [^95] | [Continual Improvement of Threshold-Based Novelty Detection.](http://arxiv.org/abs/2309.02551) | 论文提出了一种新的方法，利用线性搜索和离一标准交叉验证自动选择阈值，从而改进了动态环境中神经网络的新颖性检测准确率。 |
| [^96] | [Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning.](http://arxiv.org/abs/2309.02547) | 本论文提出了一种名为结构概念学习（SCL）的深度学习方法，利用图注意力网络进行多层次物体重组规划。该方法在自动生成的模拟数据集上训练，可以适应具有复杂结构依赖的未知场景，推断出独立的子结构以实现任务并行化，并且在现实世界中具有泛化能力。 |
| [^97] | [A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation.](http://arxiv.org/abs/2309.02539) | 本论文提出了一种通用带通神经网络用于电影音频源分离，通过使用心理声学的频率尺度来定义频带并且利用共同编码器结构的信息共享特性提高了分离性能。 |
| [^98] | [Experience and Prediction: A Metric of Hardness for a Novel Litmus Test.](http://arxiv.org/abs/2309.02534) | 该论文实现了一种自动化系统，通过设计和输出Winograd模式的硬度指数，可以在未来的挑战或WSC CAPTCHA服务中对模式进行区分分类。 |
| [^99] | [Diffusion on the Probability Simplex.](http://arxiv.org/abs/2309.02530) | 本文提出了一种在概率单纯形上执行扩散的方法，通过使用softmax函数应用于阿恩斯坦-乌伦贝克过程，可以在处理连续性和离散性对象之间的紧张关系时取得良好效果。这种方法也可以扩展到单位立方体上，从而在有界图像生成方面具有应用前景。 |
| [^100] | [Adaptive Adversarial Training Does Not Increase Recourse Costs.](http://arxiv.org/abs/2309.02528) | 本文研究了自适应对抗训练对算法回溯成本的影响，证明了自适应对抗训练所引起的模型鲁棒性的改进对算法回溯成本几乎没有影响。 |
| [^101] | [Comparative Analysis of CPU and GPU Profiling for Deep Learning Models.](http://arxiv.org/abs/2309.02521) | 本文通过比较GPU和CPU在深度学习模型训练中的性能，发现GPU在深度神经网络上具有更低的运行时间，对于较简单的网络，GPU并没有太多显著的改进。 |
| [^102] | [Towards User Guided Actionable Recourse.](http://arxiv.org/abs/2309.02517) | 本文致力于利用用户偏好来指导可行行动特征的生成过程，并提出了一种基于梯度的方法来识别用户优选的可行性补救。 |
| [^103] | [Enhancing Semantic Communication with Deep Generative Models -- An ICASSP Special Session Overview.](http://arxiv.org/abs/2309.02478) | 用深度生成模型增强语义通信，解决从复杂数据中提取语义信息和处理通道干扰的挑战。 |
| [^104] | [Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning.](http://arxiv.org/abs/2309.02476) | 该论文提出了一种理论最优解——COPS（基于不确定性的最优子采样），用于解决深度学习中的核心集选择和主动学习问题，在减少标记数据集成本的同时最小化模型的期望损失。 |
| [^105] | [A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges.](http://arxiv.org/abs/2309.02473) | 这篇论文综述了模仿学习的算法、最新进展和挑战，指出在复杂和非结构化的环境中，通过模仿专家行为来学习所需行为更具吸引力。 |
| [^106] | [Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D).](http://arxiv.org/abs/2309.02467) | 本研究开发了一种基于电子健康记录的机器学习分析流程，名为个性化多社交风险评分（iPsRS），用于识别与2型糖尿病患者住院风险相关的未满足社会需求。 |
| [^107] | [Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension.](http://arxiv.org/abs/2309.02465) | 本文针对三维打印的G代码文件提出了六种基础大型语言模型（LLMs），通过评估它们在G代码调试和操作方面的性能，包括错误检测和修正以及几何变换等。结果表明这些模型具有潜力应用于增材制造领域。 |
| [^108] | [Active flow control for three-dimensional cylinders through deep reinforcement learning.](http://arxiv.org/abs/2309.02462) | 本文通过深度强化学习方法，成功实现了利用多个独立控制的合成喷流进行三维圆柱的主动流控制，实现了显著的阻力减小。 |
| [^109] | [Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks.](http://arxiv.org/abs/2309.02460) | 本文介绍了一种新颖的多图神经网络模型DIAM，用于有效地检测加密货币交易网络上的非法账户。该模型通过自动学习节点表示并保留平行边的内在交易模式，在大型交易网络中取得了良好的效果。 |
| [^110] | [Towards frugal unsupervised detection of subtle abnormalities in medical imaging.](http://arxiv.org/abs/2309.02458) | 本文研究了无监督医学影像微小异常检测的节俭方法，通过使用概率分布混合模型来代替人工神经网络，实现了在准确度和计算需求之间的最优权衡。 |
| [^111] | [League of Legends: Real-Time Result Prediction.](http://arxiv.org/abs/2309.02449) | 本研究利用机器学习技术对《英雄联盟》比赛结果进行实时预测，使用未发布的数据作为预测过程的重要组成部分。在不同阶段，各种模型都取得了令人鼓舞的结果，其中基于LightGBM模型在中期阶段表现最佳，准确率达81.62\%。 |
| [^112] | [Observe Locally, Classify Globally: Using GNNs to Identify Sparse Matrix Structure.](http://arxiv.org/abs/2309.02442) | 使用图神经网络识别稀疏矩阵结构的框架在匹配数据格式和避免读取整个数据集的挑战中取得了成功。 |
| [^113] | [Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations.](http://arxiv.org/abs/2309.02332) | 神经群体在中枢神经系统中使用数学结构精确地表示和操作信息，实现了特化、泛化、新奇检测等多种功能。 |
| [^114] | [An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders.](http://arxiv.org/abs/2309.02084) | 本文提出了一种利用变分自动编码器进行无监督的离群检测的高效方法，通过引入误差减少(ER)离群得分来改进普通VAE，在各种数据集上得到了优于基准方法的实验结果。 |
| [^115] | [MvFS: Multi-view Feature Selection for Recommender System.](http://arxiv.org/abs/2309.02064) | MvFS提出了一种多视图特征选择方法，通过采用多个子网络测量具有不同特征模式的数据的特征重要性，从而更有效地选择每个实例的信息丰富的特征，并解决了当前方法对频繁出现特征的偏向问题。 |
| [^116] | [Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting.](http://arxiv.org/abs/2309.01866) | 本论文介绍了一种在无知识环境下高效的基于查询的攻击框架，针对机器学习型安卓恶意软件检测方法。对各种主流方法和杀毒软件进行了广泛评估，结果表明该框架具有强大的攻击效果。 |
| [^117] | [Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation.](http://arxiv.org/abs/2309.01860) | 本文提出了一种注意力驱动的多模态融合机制，通过将光流信息与RGB图像相结合，丰富了连续手语识别和翻译流程中的特征。该方法在手语识别任务中降低了WER 0.9，在翻译任务中提高了测试集上大多数BLEU分数约0.6。 |
| [^118] | [CONFIDERAI: a novel CONFormal Interpretable-by-Design score function forExplainable and Reliable Artificial Intelligence.](http://arxiv.org/abs/2309.01778) | 提出了一种新的可解释机器学习评分函数CONFIDERAI，它将一致性预测与规则模型相结合，利用规则的预测能力和点的几何位置，在特征空间中定义满足一致性保证的区域。 |
| [^119] | [An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports.](http://arxiv.org/abs/2309.01740) | 本研究通过对比性视觉语言学习，在COVID-19 CT扫描和非标准化报告中应用零样本多标签分类，以发现肺栓塞和细微的肺部细节，为医学图像分析领域带来了新的发展机遇。 |
| [^120] | [Memory Efficient Optimizers with 4-bit States.](http://arxiv.org/abs/2309.01507) | 本论文通过将优化器状态的位宽压缩至4位，实现了内存高效的训练神经网络。通过对一阶和二阶矩的详细经验分析，我们发现当前的块状量化方法无法准确近似复杂的异常值模式。为此，我们使用较小的块大小并同时利用行上和列上的信息进行更好的量化。此外，我们还通过排除零点的线性量化器解决了量化第二阶矩时的零点问题。我们的工作在多个基准测试上进行了评估，结果表明我们的4位优化器具有出色的性能。 |
| [^121] | [Topological Ordering in Differentiable Bayesian Structure Learning with Guaranteed Acyclicity Constraint.](http://arxiv.org/abs/2309.01392) | 本研究提出了一种在贝叶斯结构学习中严格约束图的无环性的替代方法，通过整合拓扑排序知识，能够减少推理复杂性，并确保生成的图的结构是无环的。实证实验表明，该方法胜过相关的贝叶斯基于得分的方法。 |
| [^122] | [Symbolically integrating tensor networks over various random tensors -- the second version of Python RTNI.](http://arxiv.org/abs/2309.01167) | 我们升级了Python RTNI的第二版，可以对不同随机张量进行符号性整合，支持Haar分布的酉矩阵、正交矩阵和正态分布的张量。通过导出TensorNetwork格式的张量网络，可以进行低维计算，并解释了数学原理和张量网络图之间的关系。 |
| [^123] | [Separable Hamiltonian Neural Networks.](http://arxiv.org/abs/2309.01069) | 这篇论文介绍了可分离哈密顿神经网络的应用，它通过嵌入可加性分离性来解决高维哈密顿系统中的复杂性问题。 |
| [^124] | [DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal.](http://arxiv.org/abs/2309.00855) | DoRA是一种基于领域的低资源房地产评估自监督学习框架，通过学习未标记的房地产数据集合来减少主观性，并且融入领域知识。 |
| [^125] | [Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data.](http://arxiv.org/abs/2309.00564) | 本文研究了高维线性回归在解释方面的挑战，发现空间零值和正则化对回归系数产生重要影响，并提出了一种优化公式来比较回归系数与物理工程知识得到的系数，从而实现解释性的回归结果。 |
| [^126] | [StratMed: Relevance Stratification for Low-resource Medication Recommendation.](http://arxiv.org/abs/2308.16781) | StratMed是一种面向低资源药物推荐的模型，通过相关性分层机制来解决医疗数据长尾分布不平衡的问题，平衡了药物组合的安全性和准确性。 |
| [^127] | [Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training.](http://arxiv.org/abs/2308.16453) | 本文提出了一种基于对比预训练的不平衡类别加密流量分类框架PASS，通过重新采样训练数据集并进行对比性预训练，避免了标签偏见和流量均匀性等问题，为加密流量分类提供了稳健的特征表示。 |
| [^128] | [Measurement Tampering Detection Benchmark.](http://arxiv.org/abs/2308.15605) | 本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。 |
| [^129] | [Conflict-Aware Active Automata Learning.](http://arxiv.org/abs/2308.14781) | C3AL是一种冲突感知的主动有限状态机学习框架，能够处理观测数据中的冲突，通过将观测树作为学习过程的一等公民并最小化测试次数，具有很好的效果。 |
| [^130] | [Renormalizing Diffusion Models.](http://arxiv.org/abs/2308.12355) | 该论文介绍了如何使用扩散模型学习统计学和量子场论的逆规范化群流，为构建用于研究场论的基于机器学习的模型提供了具体框架，并详细说明了这些模型如何定义一类自适应桥接取样器。 |
| [^131] | [DynED: Dynamic Ensemble Diversification in Data Stream Classification.](http://arxiv.org/abs/2308.10807) | DynED是一种动态集成多样化方法，基于MRR结合了组件的多样性和预测准确性，在数据流环境中实现了更高的准确率。 |
| [^132] | [An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network.](http://arxiv.org/abs/2308.09444) | 我们提出了一种高斯混合模型的学习算法，具有更好的鲁棒性和简单性，只需要进行1次迭代学习。我们的方法能更好地处理数据不确定性和逆问题，并且有潜力构建能够利用分布随机抽样进行随机变异和变异控制的应用。 |
| [^133] | [Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning.](http://arxiv.org/abs/2307.15778) | 本论文介绍了在图嵌入中应用信息几何来描述Ising模型的基态，通过利用球面和双曲面拓扑上的编码，建立了机器学习和纠错编码之间的联系，并通过优化纠错码和发展嵌入方法提出了一种新的编码方法。 |
| [^134] | [GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models.](http://arxiv.org/abs/2307.05735) | GOKU-UI是一种普适推理的SciML生成模型，通过关注机制和多射击训练策略，在包括随机微分方程在内的各种微分方程中展现出了优异的性能表现。它具有显著的数据效率，并在合成和实证数据集上优于所有基线模型。 |
| [^135] | [Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning.](http://arxiv.org/abs/2307.04869) | 本文提出了一种名为Fed-CPrompt的方法，用于解决无重复学习的联邦持续学习中的遗忘问题。该方法通过异步提示学习和对比持续损失处理异步任务到达和异构数据分布，并在实验证明其在该领域取得了最先进的性能。 |
| [^136] | [Manifold Filter-Combine Networks.](http://arxiv.org/abs/2307.04056) | 这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。 |
| [^137] | [Loss Functions and Metrics in Deep Learning. A Review.](http://arxiv.org/abs/2307.02694) | 本文回顾了深度学习中最常见的损失函数和性能测量方法，旨在帮助从业者选择最适合其特定任务的方法。 |
| [^138] | [Optimism and Adaptivity in Policy Optimization.](http://arxiv.org/abs/2306.10587) | 本文通过将看似无关的策略优化算法重新构造为共同的两个交错步骤，即乐观策略改进和后见适应，统一了强化学习中的策略优化方法，揭示了加速方法中的乐观性和适应性的共同理论属性。 |
| [^139] | [Datasheets for Machine Learning Sensors.](http://arxiv.org/abs/2306.08848) | 本研究提出了一种用于机器学习传感器的标准数据表模板，并讨论了其主要组成部分。这些数据表可以促进对传感器数据在机器学习应用中的理解和利用，并提供了客观的性能评估指标。 |
| [^140] | [Kernel Random Projection Depth for Outlier Detection.](http://arxiv.org/abs/2306.07056) | 本文提出了一种内核随机投影深度方法，用于处理数据云中的多模式和非凸性，实验结果表明在基准数据集上表现优异。 |
| [^141] | [High-dimensional and Permutation Invariant Anomaly Detection.](http://arxiv.org/abs/2306.03933) | 该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。 |
| [^142] | [Epilepsy Seizure Detection: Anatomy and Analysis.](http://arxiv.org/abs/2305.19347) | 该研究提出了一种通用、经济、非侵入性的癫痫检测系统，基于简单的实时kNN机器学习，可在不到四秒的训练时间内定制和适应个人用户，并具有94.5%的平均准确率。 |
| [^143] | [AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data.](http://arxiv.org/abs/2305.18798) | AnoOnly是一个新的半监督异常检测框架，通过引入一种对正常数据的弱监督形式来解决同质数据对异常的影响，以实现平衡的监督。该框架在各种模型和数据集上表现出了显著的性能提升，达到了新的最佳性能。 |
| [^144] | [On Optimal Regularization Parameters via Bilevel Learning.](http://arxiv.org/abs/2305.18394) | 本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。 |
| [^145] | [Gibbs free energies via isobaric-isothermal flows.](http://arxiv.org/abs/2305.13233) | 采用机器学习模型利用等压等温流得到吉布斯自由能，并在单原子水的结晶中进行了测试，表现出优秀的性能。 |
| [^146] | [Infinite Class Mixup.](http://arxiv.org/abs/2305.10293) | 本文提出了一种直接通过混合分类器而不是标签来增强样本的策略。新的分类器是输入对分类器向量的线性插值，使分类器之间的关系更加准确，从而提高深度网络的分类性能。 |
| [^147] | [Autoencoder-based Anomaly Detection in Streaming Data with Incremental Learning and Concept Drift Adaptation.](http://arxiv.org/abs/2305.08977) | 本文提出了一种基于自编码器的增量学习方法与漂移检测的异常检测方法（strAEm++DD），用于解决无标签的流数据中的异常检测问题，该方法在实验研究中表现出不俗的性能表现并优于现有的基线和先进方法。 |
| [^148] | [Open problems in causal structure learning: A case study of COVID-19 in the UK.](http://arxiv.org/abs/2305.03859) | 本文研究了因果机器学习在COVID-19英国疫情数据中的应用挑战，探讨了不同数据格式对学习类别不同的算法的影响，并突出了因果结构学习中的未解问题和未来研究方向。 |
| [^149] | [Verification against in-situ observations for Data-Driven Weather Prediction.](http://arxiv.org/abs/2305.00048) | 数据驱动气象预测模型（DDWP）近年来发展迅速，但需要更加严格的真实观测验证来在操作预报中更安全地使用。 |
| [^150] | [T Cell Receptor Protein Sequences and Sparse Coding: A Novel Approach to Cancer Classification.](http://arxiv.org/abs/2304.13145) | 本研究探索了利用稀疏编码的方法对具有癌症分类目标的TCR蛋白序列进行多类别分类，为基于TCR的免疫治疗提供理论支持。 |
| [^151] | [Towards transparent and robust data-driven wind turbine power curve models.](http://arxiv.org/abs/2304.09835) | 该研究利用可解释的人工智能框架研究了基于数据驱动的风力涡轮机功率曲线模型的稳健性。结果表明，学习到的策略可以更好地指示模型的稳健性。高度复杂的机器学习模型容易学习到物理上不合理的策略。 |
| [^152] | [An Arrhythmia Classification-Guided Segmentation Model for Electrocardiogram Delineation.](http://arxiv.org/abs/2304.06237) | 本文提出了一种利用深度学习模型结合心律失常分类指导的心电图分割方法，能够准确划分广泛异常节律类型的信号，减少虚报检测。 |
| [^153] | [Learning solution of nonlinear constitutive material models using physics-informed neural networks: COMM-PINN.](http://arxiv.org/abs/2304.06044) | 通过物理训练的神经网络可解决非线性材料行为的本构关系，无需初始数据，避免重复的牛顿迭代。训练好的模型可作为有限元程序的用户定义材料模型，但需要解决诸多挑战。 |
| [^154] | [A Unified Framework for Exploratory Learning-Aided Community Detection in Networks with Unknown Topology.](http://arxiv.org/abs/2304.04497) | META-CODE是一个统一的框架，通过探索学习和易于收集的节点元数据，在未知拓扑网络中检测重叠社区。实验结果证明了META-CODE的有效性和可扩展性。 |
| [^155] | [NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs.](http://arxiv.org/abs/2304.04027) | 该论文提出了一个新的框架：NeBLa，可以从全景放射线图中通过神经啤酒-兰伯特法重建精确的3D口腔结构模型。 |
| [^156] | [TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems.](http://arxiv.org/abs/2304.01951) | TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。 |
| [^157] | [Text-to-Image Diffusion Models are Zero-Shot Classifiers.](http://arxiv.org/abs/2303.15233) | 文本到图像扩散模型被提出用于零样本分类器，具有竞争性的零样本图像分类表现和先进的形状/纹理偏差测试结果，能够成功执行属性绑定。 |
| [^158] | [EdgeServe: An Execution Layer for Decentralized Prediction.](http://arxiv.org/abs/2303.08028) | EdgeServe 是一种为去中心化预测而设计的机器学习系统，通过低延迟的消息代理程序将数据路由到可以提供预测的节点。它具有一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。在多摄像机物体跟踪，网络入侵检测和人类活动识别等三个去中心化预测任务中，EdgeServe 展现了很好的性能。 |
| [^159] | [Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review.](http://arxiv.org/abs/2303.07647) | 本文综述了近年来机器学习在实验固体力学领域中的最新进展和应用，提供了物理感知和基于物理学的机器学习方法，并涵盖了断裂力学、生物力学、纳米和微观力学、构建材料和二维材料等传统和新兴领域。 |
| [^160] | [TSMixer: An all-MLP Architecture for Time Series Forecasting.](http://arxiv.org/abs/2303.06053) | TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。 |
| [^161] | [ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?.](http://arxiv.org/abs/2303.05382) | 本文探讨了ChatGPT在解决交通问题方面的应用。通过利用具有跨模态编码器的LLM，可以处理来自不同模态的交通数据并执行交通运营。作者提供了一个基于智能手机的碰撞报告自动生成和分析框架作为用例展示了这种潜力。 |
| [^162] | [Towards provably efficient quantum algorithms for large-scale machine-learning models.](http://arxiv.org/abs/2303.03428) | 本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \times \text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。 |
| [^163] | [Spacetime-Efficient Low-Depth Quantum State Preparation with Applications.](http://arxiv.org/abs/2303.02131) | 提出了一种使用占用空间和时间较小的低深度方法来准备任意量子态，能够在较少的量子资源使用下实现更快的准备速度。 |
| [^164] | [Image Labels Are All You Need for Coarse Seagrass Segmentation.](http://arxiv.org/abs/2303.00973) | 本文将海草分类重新定义为弱监督的粗糙分割问题，使用图像级标签进行训练，推理时获得补丁级输出。通过SeaFeats和SeaCLIP模型的引入和应用，证明了其在海草识别和分类中的有效性。 |
| [^165] | [Dynamic Graph Convolutional Network with Attention Fusion for Traffic Flow Prediction.](http://arxiv.org/abs/2302.12598) | 提出了一种用于交通流量预测的具有注意融合的动态图卷积网络，通过增强时间特征维度的交互作用和捕捉多尺度空间-时间依赖关系，以及有效捕捉远距离、多方面领域的空间-时间模式，实现精确且实时的交通状态预测。 |
| [^166] | [Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods.](http://arxiv.org/abs/2302.11962) | 该论文提出了一个名为辅助框架的新框架，通过统一的视角，提供了具有全局复杂性保证的随机和方差减少的二阶算法。该框架在构建和分析随机三次牛顿方法时具有高度灵活性，使用了任意大小的批量，以及有噪声和可能有偏差的梯度和Hessian的估计，结合了方差减少和惰性Hessian更新。在噪声的弱假设下，恢复了已知的随机和方差减少的三次牛顿的最佳复杂性。 |
| [^167] | [An Informative Path Planning Framework for Active Learning in UAV-based Semantic Mapping.](http://arxiv.org/abs/2302.03347) | 本文提出了一种针对语义分割的信息路径规划框架，使无人机能够自主获取信息丰富的训练影像以供模型重新训练。 |
| [^168] | [LUT-NN: Empower Efficient Neural Network Inference with Centroid Learning and Table Lookup.](http://arxiv.org/abs/2302.03213) | LUT-NN通过质心学习和表格查找提高神经网络推理效果以减少推理成本。具体而言，LUT-NN使用可微分的质心学习来最小化质心对准确性的影响，并通过表格查找直接读取近似输出结果，以实现高效的推理执行。 |
| [^169] | [Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning.](http://arxiv.org/abs/2302.02662) | 本文研究了一种名为GLAM的方法，通过功能基础设施建设，利用在线强化学习提高LLM代理程序的性能来实现LLMs与环境之间的对齐，解决决策问题。 |
| [^170] | [SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer.](http://arxiv.org/abs/2301.12811) | 本文提出了一种新的GAN训练方案，切片对抗网络（SAN），通过优化生成器和判别器的最小最大目标函数，使生成器分布接近目标分布。实验证实了SAN相对于普通GAN的有效性，并在StyleGAN-XL上取得了最先进的FID评分。 |
| [^171] | [Unifying Synergies between Self-supervised Learning and Dynamic Computation.](http://arxiv.org/abs/2301.09164) | 本文提出了自监督学习和动态计算之间相互作用的新视角。通过在自监督学习的设置中同时学习密集和门控子网络，无需额外的微调或剪枝步骤，可以获得一个通用且高效的架构，适用于资源受限的工业环境。 |
| [^172] | [A Topological Deep Learning Framework for Neural Spike Decoding.](http://arxiv.org/abs/2212.05037) | 这项工作开发了一个基于拓扑学的深度学习框架，用于解码神经突触输出，以更好地理解和表示大脑中的神经结构。 |
| [^173] | [Graph Convolutional Neural Networks with Diverse Negative Samples via Decomposed Determinant Point Processes.](http://arxiv.org/abs/2212.02055) | 本论文提出了基于DPP的方法来获取多样的负样本，在图卷积神经网络中实现图表示学习，通过提供不同的信息来更新节点的表示，从而提高了准确性。 |
| [^174] | [Fake detection in imbalance dataset by Semi-supervised learning with GAN.](http://arxiv.org/abs/2212.01071) | 本文提出了一种在不平衡数据集中使用半监督学习和生成对抗网络进行虚假检测的方法，实验证明仅使用100个标记样本的情况下，准确率达到了91\%。 |
| [^175] | [CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote Aggregation.](http://arxiv.org/abs/2211.13398) | 本文针对Sim2Real物体姿态估计问题，提出了一种新颖的CPPF++方法，通过投票聚合和概率建模来考虑投票不确定性，并通过迭代噪声过滤来提高姿态估计的准确性。 |
| [^176] | [Towards Privacy-Aware Causal Structure Learning in Federated Setting.](http://arxiv.org/abs/2211.06919) | 本文研究了在联合设置中隐私感知因果结构学习的问题，并提出了一种新的联邦PC（FedPC）算法，通过两种新策略保护数据隐私。 |
| [^177] | [Clustered Federated Learning based on Nonconvex Pairwise Fusion.](http://arxiv.org/abs/2211.04218) | 本研究提出了一种基于非凸配对融合的聚类联邦学习框架，能够自动识别聚类结构，降低通信成本，并确保隐私性。 |
| [^178] | [Explicit Second-Order Min-Max Optimization Methods with Optimal Convergence Guarantee.](http://arxiv.org/abs/2210.12860) | 本文提出了一种具有最优收敛保证的显式二阶最小最大优化方法，用于解决凸凹无约束最小最大优化问题。该方法利用二阶信息加速额外梯度方法，并且在迭代过程中保持在有界集内，达到了与理论下界相匹配的收敛速度。 |
| [^179] | [Sample-Efficient Personalization: Modeling User Parameters as Low Rank Plus Sparse Components.](http://arxiv.org/abs/2210.03505) | 该论文提出了一种高效的个性化算法，通过将网络权重建模为低秩和稀疏分量的总和，既捕捉了多个用户间的共同信息，又能够捕捉用户个性化的特点。 |
| [^180] | [Deep Metric Learning with Chance Constraints.](http://arxiv.org/abs/2209.09060) | 本文将深度度量学习与有限机会约束的可行性问题相关联，证明了基于代理的DML的最小化者满足机会约束，并提出多个代理有助于性能提升，通过迭代投影解决DML问题。 |
| [^181] | [Federated Learning of Neural ODE Models with Different Iteration Counts.](http://arxiv.org/abs/2208.09478) | 本文提出了一种利用神经常微分方程模型进行联邦学习的方法，能够在聚合具有不同迭代次数或深度的模型时减少通信量，并通过实验证明了该方法的有效性。 |
| [^182] | [Revisiting Adversarial Attacks on Graph Neural Networks for Graph Classification.](http://arxiv.org/abs/2208.06651) | 该论文提出了一种针对图分类任务的图神经网络的对抗性攻击方法。通过操作图结构和节点特征，生成对抗性示例，解决了从全局到局部的攻击挑战。这项工作为提高图神经网络的安全性和鲁棒性提供了重要的指导。 |
| [^183] | [Generative Action Description Prompts for Skeleton-based Action Recognition.](http://arxiv.org/abs/2208.05318) | 本文提出了一种基于骨骼的动作识别的生成式动作描述提示（GAP）方法，利用预训练的语言模型自动生成动作的身体部位运动的文本描述，并采用多模态训练方案。 |
| [^184] | [Learning Sparsity-Promoting Regularizers using Bilevel Optimization.](http://arxiv.org/abs/2207.08939) | 本文提出了一种使用双层优化学习稀疏促进正则化器的方法，可以用于去噪信号和图像。通过监督学习的方式，将稀疏促进正则化器的参数学习最小化重建结果误差，从而提高信号重建的效果。 |
| [^185] | [An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System.](http://arxiv.org/abs/2207.07886) | 该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。 |
| [^186] | [Understanding convolution on graphs via energies.](http://arxiv.org/abs/2206.10991) | 本论文结合能量的概念，证明了带对称滤波器的线性图卷积可以增强高频率，使图神经网络在同质和异质任务中表现更好。 |
| [^187] | [A novel physics-informed machine learning strategy to accelerate unsteady heat and mass transfer simulations.](http://arxiv.org/abs/2206.06817) | 本研究通过提出一种基于残差的物理约束迁移学习（RePIT）策略，利用ML-CFD交叉计算加速非稳态传热和传质模拟。这种方法可以减小残差的增加，并使用最新的CFD时间序列数据更新网络参数，从而实现长期CFD模拟的可行性。 |
| [^188] | [DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications.](http://arxiv.org/abs/2203.09096) | 阿尔茨海默病进展预测中，我们提出了一种新颖的多模态多任务深度学习模型，整合了多个队列中的临床和神经影像数据，通过分析高维MRI特征和其他数据模态来预测患者未来的病情轨迹。 |
| [^189] | [Error Scaling Laws for Kernel Classification under Source and Capacity Conditions.](http://arxiv.org/abs/2201.12655) | 本文研究了核分类问题中的错误缩放定律，针对满足源条件和容量条件的数据集类别，在高斯设计下导出了误差衰减率与源和容量系数的关系，并对比了最大化间隔支持向量机和岭分类两种方法。 |
| [^190] | [Optimal and Differentially Private Data Acquisition: Central and Local Mechanisms.](http://arxiv.org/abs/2201.03968) | 本文研究了一个平台从具有隐私敏感性用户那里收集数据以估计参数的问题，并提出了最优机制设计问题，以引发用户的真实报告。通过将问题形式化为贝叶斯最优机制设计问题，并使用差分隐私量化异质隐私成本，我们建立了估计误差的下界并推导出最优的估计器和支付方案。 |
| [^191] | [Transferable Time-Series Forecasting under Causal Conditional Shift.](http://arxiv.org/abs/2111.03422) | 本文提出了一个用于半监督领域适应的时序预测模型，通过分析因果结构和引入因果条件转移的假设来处理跨领域时序数据的复杂条件依赖关系。 |
| [^192] | [Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning.](http://arxiv.org/abs/2006.02482) | 本文提出了一种用于解释黑箱预测算法行为的因果学习方法，通过学习因果图表示来提供因果解释，弥补了现有方法的缺点，即解释单元更加可解释且考虑了宏观级特征和未测量的混淆。 |

# 详细

[^1]: Matcha-TTS: 一种具有条件流匹配的快速TTS架构

    Matcha-TTS: A fast TTS architecture with conditional flow matching. (arXiv:2309.03199v1 [eess.AS])

    [http://arxiv.org/abs/2309.03199](http://arxiv.org/abs/2309.03199)

    Matcha-TTS是一种快速TTS架构，使用最优传输条件流匹配训练，具有高质量输出和快速合成步骤。它不需要外部对齐，与其他模型相比，具有最小的内存占用，速度更快，并在听觉测试中获得了最高的评分。

    

    我们介绍了Matcha-TTS，一种新的编码器-解码器架构，用于高速TTS声学建模，该模型使用最优传输条件流匹配（OT-CFM）进行训练。这使得基于ODE的解码器能够在比使用得分匹配进行训练的模型更少的合成步骤中产生高质量的输出。精心设计的选择确保每个合成步骤的运行速度快。该方法是概率的、非自回归的，并且可以自主学习说话，无需外部对齐。与强大的预训练基线模型相比，Matcha-TTS系统具有最小的内存占用，与最快模型在长语音片段上的速度相当，并在一项听觉测试中获得了最高的评分。请访问https://shivammehta25.github.io/Matcha-TTS/ 查看音频示例、代码和预训练模型。

    We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest models on long utterances, and attains the highest mean opinion score in a listening test. Please see https://shivammehta25.github.io/Matcha-TTS/ for audio examples, code, and pre-trained models.
    
[^2]: Blink: 使用贝叶斯估计在图神经网络中通过链接本地差分隐私

    Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation. (arXiv:2309.03190v1 [cs.LG])

    [http://arxiv.org/abs/2309.03190](http://arxiv.org/abs/2309.03190)

    本文提出了一种使用链接本地差分隐私的方法，在图神经网络中实现与不受信任的服务器的协作训练。通过贝叶斯估计，将隐私预算分别用于链接和图的度，缓解差分隐私对训练准确性的负面影响，并限制链接概率推断与真实图拓扑之间的误差。提出的LDP机制有两个变体，在不同隐私设置下互补使用，以避免误报链接估计问题。

    

    图神经网络(GNNs)由于在各种图推理任务中学习节点嵌入的卓越能力而越来越受欢迎，但训练它们可能引起隐私问题。为了解决这个问题，我们提出使用链接本地差分隐私来进行分散节点的协作，使得GNNs可以与不受信任的服务器进行训练而不泄露任何链接的存在。我们的方法将隐私预算分别用于服务器上的链接和图的度，通过贝叶斯估计更好地去噪图拓扑结构，缓解差分隐私对训练GNNs准确性的负面影响。我们限制从推断出的链接概率与真实图拓扑之间的平均绝对误差。然后，我们提出了两种不同隐私设置下互补的LDP机制的变体之一，其中在较低的隐私预算下估计较少的链接，以避免当不确定性较高时出现误报链接估计。

    Graph neural networks (GNNs) have gained an increasing amount of popularity due to their superior capability in learning node embeddings for various graph inference tasks, but training them can raise privacy concerns. To address this, we propose using link local differential privacy over decentralized nodes, enabling collaboration with an untrusted server to train GNNs without revealing the existence of any link. Our approach spends the privacy budget separately on links and degrees of the graph for the server to better denoise the graph topology using Bayesian estimation, alleviating the negative impact of LDP on the accuracy of the trained GNNs. We bound the mean absolute error of the inferred link probabilities against the ground truth graph topology. We then propose two variants of our LDP mechanism complementing each other in different privacy settings, one of which estimates fewer links under lower privacy budgets to avoid false positive link estimates when the uncertainty is hig
    
[^3]: SLiMe: 像我一样进行分割

    SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])

    [http://arxiv.org/abs/2309.03179](http://arxiv.org/abs/2309.03179)

    基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。

    

    使用大型视觉语言模型（如稳定扩散SD），在诸多下游任务（包括图像编辑、图像对应和3D形状生成）方面取得了显著进展。受到这些进展的启发，我们探索利用这些广泛的视觉语言模型，通过提出SLiMe，以尽可能少的标注样本对图像进行任意细粒度的分割。SLiMe将这个问题作为一个优化任务来进行。具体而言，给定一张训练图像及其分割掩膜，我们首先从SD先验中提取注意力图，包括我们的新颖的“加权累积自注意力图”。然后，利用提取的注意力图，优化稳定扩散的文本嵌入，使得每个嵌入只学习训练图像中的一个分割区域。这些学习到的嵌入然后在注意力图中突出显示分割区域，从而可以生成分割图。这使得SLiMe可以进行图像分割。

    Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen
    
[^4]: 使用可微分多模态学习的三维物体定位

    3D Object Positioning Using Differentiable Multimodal Learning. (arXiv:2309.03177v1 [eess.SY])

    [http://arxiv.org/abs/2309.03177](http://arxiv.org/abs/2309.03177)

    本文介绍了一种使用模拟激光雷达数据和图像像素误差的多模态方法，通过可微分渲染来优化物体在计算机图形场景中的位置。使用第二种模态（激光雷达）能够加快收敛，这为自动驾驶车辆提供了潜在的用途，并提出了多种数据模拟方法用于训练自动驾驶车辆。

    

    本文介绍了一种使用模拟激光雷达数据和图像像素误差的多模态方法，通过可微分渲染来优化物体相对于观察者或某个参考物体在计算机图形场景中的位置。使用梯度下降完成物体位置优化，损失函数受到两种模态的影响。通常使用可微分渲染仅使用图像像素误差来进行物体放置优化，本文展示了使用第二种模态（激光雷达）能够加快收敛。融合传感器输入的这种方法为自动驾驶车辆提供了潜在的用途，因为这些方法可以用于确定场景中多个参与者的位置。本文还提出了一种用于训练自动驾驶车辆的多种数据模拟方法。

    This article describes a multi-modal method using simulated Lidar data via ray tracing and image pixel loss with differentiable rendering to optimize an object's position with respect to an observer or some referential objects in a computer graphics scene. Object position optimization is completed using gradient descent with the loss function being influenced by both modalities. Typical object placement optimization is done using image pixel loss with differentiable rendering only, this work shows the use of a second modality (Lidar) leads to faster convergence. This method of fusing sensor input presents a potential usefulness for autonomous vehicles, as these methods can be used to establish the locations of multiple actors in a scene. This article also presents a method for the simulation of multiple types of data to be used in the training of autonomous vehicles.
    
[^5]: 基于印象感知的多行为推荐系统：一种层次图注意力方法

    Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach. (arXiv:2309.03169v1 [cs.IR])

    [http://arxiv.org/abs/2309.03169](http://arxiv.org/abs/2309.03169)

    这个论文提出了一种基于印象感知的多行为推荐系统，通过利用注意机制从行为间和行为内部获取信息，并采用多层级图注意力方法，来解决推荐系统在处理多个行为之间互动方面的挑战。

    

    尽管推荐系统从隐式反馈中获益良多，但往往会忽略用户与物品之间的多行为互动的细微差别。历史上，这些系统要么将所有行为，如“印象”（以前称为“浏览”）、“添加到购物车”和“购买”，归并为一个统一的“互动”标签，要么仅优先考虑目标行为，通常是“购买”行为，并丢弃有价值的辅助信号。尽管最近的进展试图解决这种简化，但它们主要集中于优化目标行为，与数据稀缺作斗争。此外，它们往往绕过了与行为内在层次结构有关的微妙差异。为了弥合这些差距，我们引入了“H”ierarchical “M”ulti-behavior “G”raph Attention “N”etwork（HMGN）。这个开创性的框架利用注意机制从行为间和行为内部获取信息，同时采用多

    While recommender systems have significantly benefited from implicit feedback, they have often missed the nuances of multi-behavior interactions between users and items. Historically, these systems either amalgamated all behaviors, such as \textit{impression} (formerly \textit{view}), \textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label, or prioritized only the target behavior, often the \textit{buy} action, discarding valuable auxiliary signals. Although recent advancements tried addressing this simplification, they primarily gravitated towards optimizing the target behavior alone, battling with data scarcity. Additionally, they tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior \textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-
    
[^6]: 分割增强神经网络

    Split-Boost Neural Networks. (arXiv:2309.03167v1 [cs.LG])

    [http://arxiv.org/abs/2309.03167](http://arxiv.org/abs/2309.03167)

    这篇论文提出了一种称为分割增强的训练策略，通过自动包含正则化行为，降低了神经网络训练的复杂性和计算资源需求。

    

    神经网络的校准和训练是一个复杂且耗时的过程，需要大量的计算资源才能达到令人满意的结果。关键障碍是需要选择大量的超参数，并且在数据量较少的情况下容易出现过拟合。在这个框架中，我们提出了一种创新的前馈架构训练策略，称为分割增强（split-boost），它能够提高性能并自动包含一种正则化行为，而无需显式地建模。这种新颖的方法最终使我们能够避免显式建模正则化项，减少总的超参数数量并加速调优阶段。该策略在一个匿名的现实世界数据集上进行了测试，应用于基准医疗保险设计问题。

    The calibration and training of a neural network is a complex and time-consuming procedure that requires significant computational resources to achieve satisfactory results. Key obstacles are a large number of hyperparameters to select and the onset of overfitting in the face of a small amount of data. In this framework, we propose an innovative training strategy for feed-forward architectures - called split-boost - that improves performance and automatically includes a regularizing behaviour without modeling it explicitly. Such a novel approach ultimately allows us to avoid explicitly modeling the regularization term, decreasing the total number of hyperparameters and speeding up the tuning phase. The proposed strategy is tested on a real-world (anonymized) dataset within a benchmark medical insurance design problem.
    
[^7]: 通过深度强化学习学习充电：无人机覆盖路径规划

    Learning to Recharge: UAV Coverage Path Planning through Deep Reinforcement Learning. (arXiv:2309.03157v1 [cs.RO])

    [http://arxiv.org/abs/2309.03157](http://arxiv.org/abs/2309.03157)

    本论文提出了一种通过深度强化学习学习充电来解决无人机覆盖路径规划问题的方法。该方法利用基于地图的观测信息，在整个任务周期内优化覆盖轨迹，并采用动作屏蔽和折扣因子调度等技术。实验结果表明，该方法优于基准启发式方法，在不同目标区域和地图上具有一定的泛化性能。

    

    覆盖路径规划（CPP）是机器人学中一个关键问题，其目标是找到一个有效的路径，覆盖兴趣区域中的每一个点。本文解决了充电有限的无人机（UAV）的电力限制CPP问题。在这个问题中，将充电旅程整合到整体覆盖策略中带来了一个显著的挑战，突出了制定战略性、长期性决策的复杂任务。我们提出了一种基于近端策略优化（PPO）的深度强化学习（DRL）方法，利用基于地图的观测信息，运用动作屏蔽和折扣因子调度来优化整个任务周期内的覆盖轨迹。我们还提供了一个位置历史记录给智能体，以处理充电能力引起的新出现的状态循环。我们的方法优于基准启发式方法，在不同目标区域和地图上具有泛化性能，但对未知地图的泛化性能有限。

    Coverage path planning (CPP) is a critical problem in robotics, where the goal is to find an efficient path that covers every point in an area of interest. This work addresses the power-constrained CPP problem with recharge for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable challenge emerges from integrating recharge journeys into the overall coverage strategy, highlighting the intricate task of making strategic, long-term decisions. We propose a novel proximal policy optimization (PPO)-based deep reinforcement learning (DRL) approach with map-based observations, utilizing action masking and discount factor scheduling to optimize coverage trajectories over the entire mission horizon. We further provide the agent with a position history to handle emergent state loops caused by the recharge capability. Our approach outperforms a baseline heuristic, generalizes to different target zones and maps, with limited generalization to unseen maps. We offer valuable in
    
[^8]: 基于数据驱动的神经极化码用于未知带和不带记忆的信道

    Data-Driven Neural Polar Codes for Unknown Channels With and Without Memory. (arXiv:2309.03148v1 [cs.IT])

    [http://arxiv.org/abs/2309.03148](http://arxiv.org/abs/2309.03148)

    本文提出了一种基于数据驱动的方法，用于设计具有和不具有记忆的信道的极化码。通过替代原始的连续取消解码器的核心元素，即检验节点、比特节点和软判决，设计了神经连续取消解码器。该方法具有理论保证和较低的计算复杂度。

    

    本文提出了一种新颖的数据驱动方法，用于设计具有和不具有记忆的信道的极化码。该方法适用于信道以“黑盒”的形式给定，设计者可以通过生成输入和输出的观测来获取信道的信息，但无法直接访问显式的信道模型。该方法利用连续取消（SC）解码器的结构，设计了神经连续取消（NSC）解码器。NSC解码器使用神经网络（NN）来替代原始SC解码器的核心元素，即检验节点、比特节点和软判决。除了NSC，我们还设计了其他嵌入信道输出到SC解码器输入空间的NN。该方法的理论保证包括NSC的一致性。此外，NSC的计算复杂度不随信道记忆大小增长，这是其与连续取消的主要优势所在。

    In this work, a novel data-driven methodology for designing polar codes for channels with and without memory is proposed. The methodology is suitable for the case where the channel is given as a "black-box" and the designer has access to the channel for generating observations of its inputs and outputs, but does not have access to the explicit channel model. The proposed method leverages the structure of the successive cancellation (SC) decoder to devise a neural SC (NSC) decoder. The NSC decoder uses neural networks (NNs) to replace the core elements of the original SC decoder, the check-node, the bit-node and the soft decision. Along with the NSC, we devise additional NN that embeds the channel outputs into the input space of the SC decoder. The proposed method is supported by theoretical guarantees that include the consistency of the NSC. Also, the NSC has computational complexity that does not grow with the channel memory size. This sets its main advantage over successive cancellat
    
[^9]: 最佳臂躲避：纯探索多臂赌博机中的近乎最优多通道流算法界限

    The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits. (arXiv:2309.03145v1 [cs.LG])

    [http://arxiv.org/abs/2309.03145](http://arxiv.org/abs/2309.03145)

    该论文通过多通道流算法给出了纯探索多臂赌博机中的近乎最优样本通道交换界限，并回答了一个悬而未决的问题。

    

    我们通过多通道流算法给出了纯探索多臂赌博机（MABs）的近似最优样本通道交换：任何使用子线性内存的流算法，其使用 $O(\frac{n}{\Delta^2})$ 的最优样本复杂度需要 $\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ 个通道。这里，$n$ 是臂的数量，$\Delta$ 是最佳臂和次佳臂之间的奖励差距。我们的结果与Jin等人[ICML'21]的 $O(\log(\frac{1}{\Delta}))$ 通道算法相匹配（除了低阶项），该算法仅使用 $O(1)$ 内存，并回答了Assadi和Wang[STOC'20]提出的一个悬而未决的问题。

    We give a near-optimal sample-pass trade-off for pure exploration in multi-armed bandits (MABs) via multi-pass streaming algorithms: any streaming algorithm with sublinear memory that uses the optimal sample complexity of $O(\frac{n}{\Delta^2})$ requires $\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ passes. Here, $n$ is the number of arms and $\Delta$ is the reward gap between the best and the second-best arms. Our result matches the $O(\log(\frac{1}{\Delta}))$-pass algorithm of Jin et al. [ICML'21] (up to lower order terms) that only uses $O(1)$ memory and answers an open question posed by Assadi and Wang [STOC'20].
    
[^10]: 使用多个向量通道改善E(n)-等变图神经网络

    Using Multiple Vector Channels Improves E(n)-Equivariant Graph Neural Networks. (arXiv:2309.03139v1 [cs.LG])

    [http://arxiv.org/abs/2309.03139](http://arxiv.org/abs/2309.03139)

    本文提出了一种对E(n)-等变图神经网络的自然扩展，使用每个节点的多个等变向量。多通道EGNN在多个物理系统任务上的性能优于标准的单通道EGNN，而且几乎没有额外的运行时间或参数数量的差异。

    

    我们提出了一种对E(n)-等变图神经网络的自然扩展，使用每个节点的多个等变向量。我们制定了这个扩展，并显示它在不同物理系统基准任务上提高了性能，同时在运行时间或参数数量方面几乎没有差异。提出的多通道EGNN在N体带电粒子动力学、分子性质预测和预测太阳系天体轨迹方面优于标准的单通道EGNN。鉴于多通道EGNN的额外优势和最小的附加成本，我们认为这个扩展可能对从事物理科学机器学习的研究人员有实际用途。

    We present a natural extension to E(n)-equivariant graph neural networks that uses multiple equivariant vectors per node. We formulate the extension and show that it improves performance across different physical systems benchmark tasks, with minimal differences in runtime or number of parameters. The proposed multichannel EGNN outperforms the standard singlechannel EGNN on N-body charged particle dynamics, molecular property predictions, and predicting the trajectories of solar system bodies. Given the additional benefits and minimal additional cost of multi-channel EGNN, we suggest that this extension may be of practical use to researchers working in machine learning for the physical sciences
    
[^11]: 通过基于数据的机器学习在锡膏检查特征上检测PCB制造缺陷

    Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features. (arXiv:2309.03113v1 [cs.LG])

    [http://arxiv.org/abs/2309.03113](http://arxiv.org/abs/2309.03113)

    通过利用锡膏检查特征，基于数据的机器学习方法能够在PCB制造中的三个阶段检测缺陷，提高操作效率和减少人工干预。

    

    利用锡膏检查（SPI）和自动光学检查（AOI）机器自动检测印制电路板（PCB）制造中的缺陷可以提高操作效率，显著减少人工干预的需要。本文利用从SPI提取的600万个引脚的特征，展示了一种基于数据的方法来训练机器学习（ML）模型以在PCB制造的三个阶段检测缺陷。这600万个PCB引脚对应于属于15,387个PCB的200万个组件。我们使用基于极限梯度提升（XGBoost）的ML模型，迭代数据预处理步骤以提高检测性能。通过使用组件和PCB ID组合的引脚级SPI特征，我们还开发了组件和PCB级别的训练实例。这使得ML模型能够捕捉到在引脚级别可能不明显的引脚间、组件间或空间效应。模型在引脚、组件和PCB级别进行训练。

    Automated detection of defects in Printed Circuit Board (PCB) manufacturing using Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI) machines can help improve operational efficiency and significantly reduce the need for manual intervention. In this paper, using SPI-extracted features of 6 million pins, we demonstrate a data-centric approach to train Machine Learning (ML) models to detect PCB defects at three stages of PCB manufacturing. The 6 million PCB pins correspond to 2 million components that belong to 15,387 PCBs. Using a base extreme gradient boosting (XGBoost) ML model, we iterate on the data pre-processing step to improve detection performance. Combining pin-level SPI features using component and PCB IDs, we developed training instances also at the component and PCB level. This allows the ML model to capture any inter-pin, inter-component, or spatial effects that may not be apparent at the pin level. Models are trained at the pin, component, and PCB levels, 
    
[^12]: 通过稀疏径向基函数神经网络解决多尺度椭圆问题。

    Solving multiscale elliptic problems by sparse radial basis function neural networks. (arXiv:2309.03107v1 [math.NA])

    [http://arxiv.org/abs/2309.03107](http://arxiv.org/abs/2309.03107)

    本研究提出了一种稀疏径向基函数神经网络方法，用于解决具有多尺度系数的椭圆偏微分方程（PDE）。该方法通过引入正则化项来避免过拟合问题，并通过优化特定损失函数来加速训练过程。

    

    近年来，机器学习已成功应用于科学计算的各个领域。本研究提出了一种稀疏径向基函数神经网络方法，用于解决具有多尺度系数的椭圆偏微分方程（PDE）。受深度混合残差方法的启发，我们将二阶问题重写为一阶系统，并采用多个径向基函数神经网络（RBFNNs）来逼近系统中的未知函数。为了避免由于RBFNN的简单性而导致的过拟合，我们在损失函数中引入了额外的正则化项。因此，损失函数包含两部分：用于一阶系统和边界条件残差的L2损失，以及径向基函数（RBFs）的权重的L1正则化项。引入了一种优化特定损失函数的算法来加速训练过程。通过实验证明了所提方法的准确性和有效性。

    Machine learning has been successfully applied to various fields of scientific computing in recent years. In this work, we propose a sparse radial basis function neural network method to solve elliptic partial differential equations (PDEs) with multiscale coefficients. Inspired by the deep mixed residual method, we rewrite the second-order problem into a first-order system and employ multiple radial basis function neural networks (RBFNNs) to approximate unknown functions in the system. To aviod the overfitting due to the simplicity of RBFNN, an additional regularization is introduced in the loss function. Thus the loss function contains two parts: the $L_2$ loss for the residual of the first-order system and boundary conditions, and the $\ell_1$ regularization term for the weights of radial basis functions (RBFs). An algorithm for optimizing the specific loss function is introduced to accelerate the training process. The accuracy and effectiveness of the proposed method are demonstrate
    
[^13]: ContrastWSD: 使用词义消岐加强隐喻检测

    ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation Following the Metaphor Identification Procedure. (arXiv:2309.03103v1 [cs.CL])

    [http://arxiv.org/abs/2309.03103](http://arxiv.org/abs/2309.03103)

    ContrastWSD是一种使用了词义消岐的隐喻检测模型，通过将隐喻识别过程和词义消岐结合起来，提取并对比单词的上下文含义和基本含义，以提高隐喻检测的效果，超过其他仅依赖上下文嵌入或集成基本定义和外部知识的方法。

    

    本文提出了ContrastWSD，一种基于RoBERTa的隐喻检测模型，它集成了隐喻识别过程(MIP)和词义消岐(WSD)来提取并对比单词的上下文含义和基本含义，以确定它在句子中是否以隐喻的方式使用。通过利用WSD模型得出的单词词义，我们的模型增强了隐喻检测过程，并超过了仅依赖上下文嵌入或仅集成基本定义和其他外部知识的其他方法。我们在多个基准数据集上评估了我们的方法，并与强基线进行比较，结果表明它在推进隐喻检测方面的有效性。

    This paper presents ContrastWSD, a RoBERTa-based metaphor detection model that integrates the Metaphor Identification Procedure (MIP) and Word Sense Disambiguation (WSD) to extract and contrast the contextual meaning with the basic meaning of a word to determine whether it is used metaphorically in a sentence. By utilizing the word senses derived from a WSD model, our model enhances the metaphor detection process and outperforms other methods that rely solely on contextual embeddings or integrate only the basic definitions and other external knowledge. We evaluate our approach on various benchmark datasets and compare it with strong baselines, indicating the effectiveness in advancing metaphor detection.
    
[^14]: PeptideBERT: 基于Transformer的肽性质预测语言模型

    PeptideBERT: A Language Model based on Transformers for Peptide Property Prediction. (arXiv:2309.03099v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.03099](http://arxiv.org/abs/2309.03099)

    PeptideBERT是一种基于Transformer的语言模型，用于预测肽的溶血性、溶解性和阻垢性。通过利用预训练模型和微调，该模型在这些任务上取得了最先进的性能。

    

    最近语言模型的进展使得蛋白质建模领域拥有了强大的工具，因为蛋白序列可以被表示为文本。具体而言，通过利用Transformer，可以在没有显式结构数据的情况下进行序列到性质预测。在这项工作中，受到大型语言模型 (LLM) 的最新进展的启发，我们引入了PeptideBERT，这是一个用于预测肽的三个关键性质（溶血性、溶解性和阻垢性）的蛋白质语言模型。PeptideBert利用了预先训练过的具有12个注意力头和12个隐藏层的ProtBERT Transformer模型，然后对这个预训练模型进行了微调以适应三个下游任务。我们的模型在预测溶血性方面达到了最先进水平，并在预测肽的抗非特异性相互作用能力方面也取得了显著的准确性。

    Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non-fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide's potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptide's capacity to resist non-specific interactions. This m
    
[^15]: 具有非凸惩罚的稀疏加权分位数回归的平滑ADMM

    Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties. (arXiv:2309.03094v1 [stat.ML])

    [http://arxiv.org/abs/2309.03094](http://arxiv.org/abs/2309.03094)

    本文提出了一种适用于稀疏加权分位数回归的新型单循环平滑ADMM算法，名为SIAD，它在存在非凸和非光滑稀疏惩罚条件下能够加速收敛速度。

    

    本文研究了在非凸和非光滑稀疏惩罚条件下的分位数回归，如最小最大凹惩罚（MCP）和平滑剪切绝对偏差（SCAD）。这些问题的非光滑和非凸特性经常导致许多算法的收敛困难。虽然迭代技术如坐标下降和局部线性近似可以促进收敛，但过程通常很慢。这种缓慢的速度主要是因为需要在每一步运行这些近似技术直到完全收敛，这是我们称之为\emph{二次收敛迭代}的要求。为了加速收敛速度，我们采用了交替方向乘法（ADMM）并引入了一种新的具有递增惩罚参数的单循环平滑ADMM算法，命名为SIAD，专门用于稀疏加权分位数回归。我们首先深入研究了所提出的SIAD算法的收敛性质和估计。

    This paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, such as the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). The non-smooth and non-convex nature of these problems often leads to convergence difficulties for many algorithms. While iterative techniques like coordinate descent and local linear approximation can facilitate convergence, the process is often slow. This sluggish pace is primarily due to the need to run these approximation techniques until full convergence at each step, a requirement we term as a \emph{secondary convergence iteration}. To accelerate the convergence speed, we employ the alternating direction method of multipliers (ADMM) and introduce a novel single-loop smoothing ADMM algorithm with an increasing penalty parameter, named SIAD, specifically tailored for sparse-penalized quantile regression. We first delve into the convergence properties of the proposed SIAD algorithm and est
    
[^16]: 在循环有向图中建立马尔可夫等价关系

    Establishing Markov Equivalence in Cyclic Directed Graphs. (arXiv:2309.03092v1 [cs.AI])

    [http://arxiv.org/abs/2309.03092](http://arxiv.org/abs/2309.03092)

    本论文提出了一种在循环有向图中建立马尔可夫等价关系的新方法，该方法不再需要对d-分离进行测试，大大减小了算法的复杂性，并且在存在潜在混淆因素的情况下具有重要的理论研究价值。

    

    我们提出了一种新的高效的方法来在可能包含循环的有向图上建立马尔可夫等价关系，该方法基于Thomas Richardson在90年代中期关于循环模型的开创性工作中的循环等价定理(CET)，但是现在从一个祖先的角度重新表述。得到的特征导致了一种建立马尔可夫等价关系的过程，不再需要对d-分离进行测试，从而大大减小了算法的复杂性。这种概念上简化的特征可能有助于在存在潜在混淆因素的情况下重新激发对循环发现的理论研究的兴趣。该版本包括了定理1中规则(iv)的修正，以及算法2第2部分的相关调整。

    We present a new, efficient procedure to establish Markov equivalence between directed graphs that may or may not contain cycles under the \textit{d}-separation criterion. It is based on the Cyclic Equivalence Theorem (CET) in the seminal works on cyclic models by Thomas Richardson in the mid '90s, but now rephrased from an ancestral perspective. The resulting characterization leads to a procedure for establishing Markov equivalence between graphs that no longer requires tests for d-separation, leading to a significantly reduced algorithmic complexity. The conceptually simplified characterization may help to reinvigorate theoretical research towards sound and complete cyclic discovery in the presence of latent confounders. This version includes a correction to rule (iv) in Theorem 1, and the subsequent adjustment in part 2 of Algorithm 2.
    
[^17]: LieDetect: 从点云中检测紧致Lie群的表示轨道

    LieDetect: Detection of representation orbits of compact Lie groups from point clouds. (arXiv:2309.03086v1 [math.OC])

    [http://arxiv.org/abs/2309.03086](http://arxiv.org/abs/2309.03086)

    LieDetect是一种从紧致Lie群的有限样本轨道中估计表示的新算法。与其他技术不同，该算法可以检索精确的表示类型，并重建其轨道，有助于识别生成该作用的Lie群。该算法适用于任何紧致Lie群，并在多个领域的应用中取得了非常准确的结果。

    

    我们提出了一种新的算法，用于从紧致Lie群的有限样本轨道中估计表示。与其他报道的技术不同，我们的方法允许检索精确的表示类型，作为不可约表示的直和。而且，对表示类型的了解可以重建其轨道，有助于识别生成该作用的Lie群。我们的算法适用于任何紧致Lie群，但只考虑了SO(2), T^d, SU(2)和SO(3)的实例化。我们推导了在Hausdorff和Wasserstein距离方面的鲁棒性的理论保证。我们的工具来自于几何测度理论，计算几何和矩阵流形上的优化。算法在高达16维的合成数据以及图像分析，谐波分析和经典力学系统的实际应用中进行了测试，取得了非常准确的结果。

    We suggest a new algorithm to estimate representations of compact Lie groups from finite samples of their orbits. Different from other reported techniques, our method allows the retrieval of the precise representation type as a direct sum of irreducible representations. Moreover, the knowledge of the representation type permits the reconstruction of its orbit, which is useful to identify the Lie group that generates the action. Our algorithm is general for any compact Lie group, but only instantiations for SO(2), T^d, SU(2) and SO(3) are considered. Theoretical guarantees of robustness in terms of Hausdorff and Wasserstein distances are derived. Our tools are drawn from geometric measure theory, computational geometry, and optimization on matrix manifolds. The algorithm is tested for synthetic data up to dimension 16, as well as real-life applications in image analysis, harmonic analysis, and classical mechanics systems, achieving very accurate results.
    
[^18]: 纯蒙特卡洛反事实遗憾最小化

    Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])

    [http://arxiv.org/abs/2309.03084](http://arxiv.org/abs/2309.03084)

    纯蒙特卡洛反事实遗憾最小化算法（PCFR）是一种结合了反事实遗憾最小化（CFR）和虚拟游戏（FP）概念的新算法，能够与各种CFR变体相结合，包括蒙特卡洛CFR（MCCFR）。PCFR具有更好的性能和较快的收敛速度，同时降低了时间和空间复杂度。

    

    反事实遗憾最小化（CFR）及其变体是目前解决大规模不完全信息博弈的最佳算法。本文在CFR的基础上提出了一种名为纯CFR（PCFR）的新算法，以实现更好的性能。PCFR可以看作是CFR和虚拟游戏（FP）的结合，继承了CFR的反事实遗憾（值）的概念，并在下一次迭代中使用最佳响应策略而不是遗憾匹配策略。我们的理论证明了PCFR可以实现Blackwell可达性，使PCFR能够与包括蒙特卡洛CFR（MCCFR）在内的任何CFR变体相结合。由此产生的纯MCCFR（PMCCFR）可以大大降低时间和空间复杂度。特别地，PMCCFR的收敛速度至少比MCCFR快三倍。此外，由于PMCCFR不通过严格被支配策略的路径，我们开发了一种新的启动算法，受到了严格被支配策略的启示。

    Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
    
[^19]: ORL-AUDITOR：深度强化学习离线数据集审核

    ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning. (arXiv:2309.03081v1 [cs.CR])

    [http://arxiv.org/abs/2309.03081](http://arxiv.org/abs/2309.03081)

    ORL-AUDITOR是一种用于审核离线深度强化学习数据集的方法，以保护知识产权和防止滥用或侵权风险。

    

    在AI领域，数据是一项重要的资产，高质量的数据集可以显著提升机器学习模型的性能。在自动驾驶等安全关键领域中，离线深度强化学习（离线DRL）经常用于在预先收集的数据集上训练模型，而不是通过与真实环境进行交互来训练这些模型的在线DRL。为了支持这些模型的开发，许多机构以开源许可的形式公开了数据集，但这些数据集存在潜在的滥用或侵权风险。向数据集中添加水印可以保护数据的知识产权，但无法处理已经发布的数据集，且后续修改是不可行的。其他现有的解决方案，如数据集推断和成员推断，在离线DRL场景下由于模型行为特征多样和离线环境限制而不起作用。本文中，

    Data is a critical asset in AI, as high-quality datasets can significantly improve the performance of machine learning models. In safety-critical domains such as autonomous vehicles, offline deep reinforcement learning (offline DRL) is frequently used to train models on pre-collected datasets, as opposed to training these models by interacting with the real-world environment as the online DRL. To support the development of these models, many institutions make datasets publicly available with opensource licenses, but these datasets are at risk of potential misuse or infringement. Injecting watermarks to the dataset may protect the intellectual property of the data, but it cannot handle datasets that have already been published and is infeasible to be altered afterward. Other existing solutions, such as dataset inference and membership inference, do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints. In this paper, 
    
[^20]: GPT-InvestAR: 利用大型语言模型增强股票投资策略通过年度报告分析

    GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models. (arXiv:2309.03079v1 [q-fin.ST])

    [http://arxiv.org/abs/2309.03079](http://arxiv.org/abs/2309.03079)

    本论文利用大型语言模型分析股票市场上上市公司的年度报告，生成洞察，并通过历史股价数据训练机器学习模型，取得相对标普500指数的超额回报。

    

    上市公司的年度报告包含了关于其财务状况的重要信息，可以帮助评估其对股票价格的潜在影响。这些报告的内容非常全面，有时甚至超过100页。即使对于一个公司来说，分析这些报告也是一项繁琐的工作，更不用说整个公司群体了。多年来，金融专家已经能够相对快速地从这些文件中提取有价值的信息。然而，这需要多年的实践和经验。本文旨在通过利用大型语言模型（LLMs）的能力，简化对所有公司年度报告的评估过程。LLM生成的洞察力被汇编在一个量化风格的数据集中，并通过历史股价数据进行补充。然后使用LLM输出作为特征训练一种机器学习模型。前向测试结果显示相对于标普500指数获得了有希望的超额回报。本文旨在

    Annual Reports of publicly listed companies contain vital information about their financial health which can help assess the potential impact on Stock price of the firm. These reports are comprehensive in nature, going up to, and sometimes exceeding, 100 pages. Analysing these reports is cumbersome even for a single firm, let alone the whole universe of firms that exist. Over the years, financial experts have become proficient in extracting valuable information from these documents relatively quickly. However, this requires years of practice and experience. This paper aims to simplify the process of assessing Annual Reports of all the firms by leveraging the capabilities of Large Language Models (LLMs). The insights generated by the LLM are compiled in a Quant styled dataset and augmented by historical stock price data. A Machine Learning model is then trained with LLM outputs as features. The walkforward test results show promising outperformance wrt S&P500 returns. This paper intends
    
[^21]: 使用神经网络对系外行星大气压力-温度分布进行参数化

    Parameterizing pressure-temperature profiles of exoplanet atmospheres with neural networks. (arXiv:2309.03075v1 [astro-ph.IM])

    [http://arxiv.org/abs/2309.03075](http://arxiv.org/abs/2309.03075)

    该论文提出了一种使用神经网络对系外行星大气压力-温度分布进行参数化的方法，该方法不依赖于对分布功能形式的明确假设，使用更少的参数，并能生成具有物理一致性的分布。

    

    系外行星大气检测通常依赖于贝叶斯推断技术和前向模拟器的组合，以估算观测光谱来推断大气性质。模拟光谱的关键组成部分是压力-温度（PT）分布，它描述了大气的热结构。目前常用的大气检测流程通常使用了人为拟合函数，这些函数将推测的PT分布限制为简单的近似，并且使用了相对较多的参数。在本研究中，我们引入了一个概念上新颖的数据驱动参数化方案，用于生成具有物理一致性的PT分布，它不需要对PT分布的功能形式做出明确的假设，并且使用较少的参数。我们的方法包括一个基于神经网络的潜变量模型，它学习了函数（PT分布）的分布。每个分布由一个低维向量表示，可以用于条件化解码器。

    Atmospheric retrievals (AR) of exoplanets typically rely on a combination of a Bayesian inference technique and a forward simulator to estimate atmospheric properties from an observed spectrum. A key component in simulating spectra is the pressure-temperature (PT) profile, which describes the thermal structure of the atmosphere. Current AR pipelines commonly use ad hoc fitting functions here that limit the retrieved PT profiles to simple approximations, but still use a relatively large number of parameters. In this work, we introduce a conceptually new, data-driven parameterization scheme for physically consistent PT profiles that does not require explicit assumptions about the functional form of the PT profiles and uses fewer parameters than existing methods. Our approach consists of a latent variable model (based on a neural network) that learns a distribution over functions (PT profiles). Each profile is represented by a low-dimensional vector that can be used to condition a decoder
    
[^22]: 用于在线手写字符分割的基于Transformer的方法

    Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation. (arXiv:2309.03072v1 [cs.CV])

    [http://arxiv.org/abs/2309.03072](http://arxiv.org/abs/2309.03072)

    提出了一种基于Transformer的方法，用于在线手写字符分割。在该方法中，将分割与识别解耦，通过将其视为样点和文本中字符之间的分配问题，并利用Transformer解码器块中学习到的字符查询来形成每个聚类。对该方法进行了实验验证，并在两个常用的在线手写字迹数据集上创建了准确的标签。

    

    在线手写字符分割通常与手写识别相关联，即使识别模型在识别过程中包括定位相关位置的机制，通常也无法产生准确的分割。将分割与识别解耦后，可以进一步利用识别结果的潜力。我们特别关注的情景是在事先已知转录的情况下，此时字符分割变成了样点和文本中字符之间的分配问题。受到K-means聚类算法的启发，我们从聚类分配的角度来看待该问题，并提出一种基于Transformer的架构，其中每个聚类基于Transformer解码器块中学习到的字符查询形成。为了评估我们的方法的质量，我们为两种常见的在线手写字迹数据集创建了字符分割的准确标签。

    On-line handwritten character segmentation is often associated with handwriting recognition and even though recognition models include mechanisms to locate relevant positions during the recognition process, it is typically insufficient to produce a precise segmentation. Decoupling the segmentation from the recognition unlocks the potential to further utilize the result of the recognition. We specifically focus on the scenario where the transcription is known beforehand, in which case the character segmentation becomes an assignment problem between sampling points of the stylus trajectory and characters in the text. Inspired by the $k$-means clustering algorithm, we view it from the perspective of cluster assignment and present a Transformer-based architecture where each cluster is formed based on a learned character query in the Transformer decoder block. In order to assess the quality of our approach, we create character segmentation ground truths for two popular on-line handwriting d
    
[^23]: 学习主动子空间在深度神经网络的有效和可扩展的不确定性量化中

    Learning Active Subspaces for Effective and Scalable Uncertainty Quantification in Deep Neural Networks. (arXiv:2309.03061v1 [stat.ML])

    [http://arxiv.org/abs/2309.03061](http://arxiv.org/abs/2309.03061)

    该论文提出了一种通过识别对神经网络输出具有最显著影响的参数方向构建低维子空间的方法，从而解决了贝叶斯深度学习中由于参数空间高维度而带来的计算复杂性问题。通过在显著减少的主动子空间上进行蒙特卡罗采样或变分推理，该方法实现了有效和可扩展的贝叶斯推理，并通过多个回归任务的实证验证了可靠的预测和鲁棒的不确定性估计。

    

    贝叶斯推理用于神经网络或贝叶斯深度学习具有提供具有量化的不确定性和鲁棒性的良好校准预测的潜力。然而，贝叶斯深度学习的主要障碍是由于参数空间的高维度而造成的计算复杂性。在这项工作中，我们提出了一种新颖的方案，通过识别对神经网络输出具有最显著影响的参数方向，构建神经网络参数的低维子空间，即主动子空间。我们证明了显著减少的主动子空间通过蒙特卡罗（MC）采样方法（否则难以计算）或变分推理实现了有效和可扩展的贝叶斯推理。从实证上看，我们的方法为各种回归任务提供了可靠的预测和鲁棒的不确定性估计。

    Bayesian inference for neural networks, or Bayesian deep learning, has the potential to provide well-calibrated predictions with quantified uncertainty and robustness. However, the main hurdle for Bayesian deep learning is its computational complexity due to the high dimensionality of the parameter space. In this work, we propose a novel scheme that addresses this limitation by constructing a low-dimensional subspace of the neural network parameters-referred to as an active subspace-by identifying the parameter directions that have the most significant influence on the output of the neural network. We demonstrate that the significantly reduced active subspace enables effective and scalable Bayesian inference via either Monte Carlo (MC) sampling methods, otherwise computationally intractable, or variational inference. Empirically, our approach provides reliable predictions with robust uncertainty estimates for various regression tasks.
    
[^24]: CoLA: 深入利用组合结构实现自动和高效的数值线性代数

    CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra. (arXiv:2309.03060v1 [cs.LG])

    [http://arxiv.org/abs/2309.03060](http://arxiv.org/abs/2309.03060)

    CoLA是一个用于机器学习中大规模线性代数问题的简单但通用的框架，通过组合调度规则和线性操作符抽象，自动构建了内存和运行时高效的数值算法，提供了内存高效的自动微分、低精度计算和GPU加速，同时可以适应下游软件包中的新对象、操作和规则。

    

    许多机器学习和科学领域涉及到大规模的线性代数问题，如特征分解、解线性系统、计算矩阵指数和迹估计等。涉及的矩阵通常具有Krondor、卷积、块对角、求和或乘积等结构。在本文中，我们提出了一个简单但通用的机器学习大规模线性代数问题的框架，名为CoLA（组合线性代数）。通过将线性操作符抽象与组合调度规则相结合，CoLA能够自动构建内存和运行时高效的数值算法。此外，CoLA还提供内存高效的自动微分、低精度计算和JAX和PyTorch中的GPU加速，同时还能够通过多重调度适应下游软件包中的新对象、操作和规则。CoLA可以加速许多代数操作，同时也便于原型化矩阵结构和算法，提供了可行性的降低-

    Many areas of machine learning and science involve large linear algebra problems, such as eigendecompositions, solving linear systems, computing matrix exponentials, and trace estimation. The matrices involved often have Kronecker, convolutional, block diagonal, sum, or product structure. In this paper, we propose a simple but general framework for large-scale linear algebra problems in machine learning, named CoLA (Compositional Linear Algebra). By combining a linear operator abstraction with compositional dispatch rules, CoLA automatically constructs memory and runtime efficient numerical algorithms. Moreover, CoLA provides memory efficient automatic differentiation, low precision computation, and GPU acceleration in both JAX and PyTorch, while also accommodating new objects, operations, and rules in downstream packages via multiple dispatch. CoLA can accelerate many algebraic operations, while making it easy to prototype matrix structures and algorithms, providing an appealing drop-
    
[^25]: 自动化CVE分析用于威胁优先级和影响预测

    Automated CVE Analysis for Threat Prioritization and Impact Prediction. (arXiv:2309.03040v1 [cs.CR])

    [http://arxiv.org/abs/2309.03040](http://arxiv.org/abs/2309.03040)

    本论文提出了一种自动化CVE分析方法，用于威胁优先级和影响预测。它解决了CVE描述的缺陷，通过提供攻击语义信息，使得CVE的综合弱点特征和威胁影响得以全面评估。这种自动化分析方法可以大大加快CVE漏洞分析的速度和效率。

    

    通用漏洞和暴露（CVE）是主动网络安全措施的关键信息，包括服务补丁、安全加固等。然而，CVE通常提供基于产品的低级描述，公开披露的网络安全漏洞缺乏综合弱点特征和威胁影响估计所需的攻击语义信息。这种关键的洞察力对于CVE优先级和潜在对策的识别至关重要，特别是在处理大量CVE时。目前的行业实践涉及对CVE进行手动评估，使用通用漏洞评分系统（CVSS）评估攻击严重性，并将其映射到通用弱点枚举（CWE）以进行潜在缓解的识别。不幸的是，这种手动分析在漏洞分析过程中形成了一个主要瓶颈，导致主动网络安全工作的放缓。

    The Common Vulnerabilities and Exposures (CVE) are pivotal information for proactive cybersecurity measures, including service patching, security hardening, and more. However, CVEs typically offer low-level, product-oriented descriptions of publicly disclosed cybersecurity vulnerabilities, often lacking the essential attack semantic information required for comprehensive weakness characterization and threat impact estimation. This critical insight is essential for CVE prioritization and the identification of potential countermeasures, particularly when dealing with a large number of CVEs. Current industry practices involve manual evaluation of CVEs to assess their attack severities using the Common Vulnerability Scoring System (CVSS) and mapping them to Common Weakness Enumeration (CWE) for potential mitigation identification. Unfortunately, this manual analysis presents a major bottleneck in the vulnerability analysis process, leading to slowdowns in proactive cybersecurity efforts an
    
[^26]: 深度学习在多囊肾病中的应用: 通过基因表达分析实现对患者的准确和早期检测

    Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis. (arXiv:2309.03033v1 [cs.LG])

    [http://arxiv.org/abs/2309.03033](http://arxiv.org/abs/2309.03033)

    本研究利用深度学习方法通过基因表达分析实现对患者多囊肾病（PKD）的准确和早期检测。

    

    多囊肾病（PKD）可能导致患者肾脏中囊肿的形成，进而导致致命的并发症，因此早期检测PKD对于有效管理该病情至关重要。然而，诊断中涉及的各种患者特定因素使其对临床医生来说是一个复杂的难题。因此，在本研究中，我们旨在利用基于深度学习的方法来进行早期疾病检测。通过分析患者的基因表达，设计的神经网络可以对患者可能的PKD进行准确且可靠的预测。

    With Polycystic Kidney Disease (PKD) potentially leading to fatal complications in patients due to the formation of cysts in the kidneys, early detection of PKD is crucial for effective management of the condition. However, the various patient-specific factors that play a role in the diagnosis make it an intricate puzzle for clinicians to solve. Therefore, in this study, we aim to utilize a deep learning-based approach for early disease detection. The devised neural network can achieve accurate and robust predictions for possible PKD in patients by analyzing patient gene expressions.
    
[^27]: 用于将知识图谱与文字嵌入的通用预处理操作符

    Universal Preprocessing Operators for Embedding Knowledge Graphs with Literals. (arXiv:2309.03023v1 [cs.AI])

    [http://arxiv.org/abs/2309.03023](http://arxiv.org/abs/2309.03023)

    本文提出了一组通用预处理操作符，用于将具有数值、时间、文本和图像信息的知识图谱转换为能够使用任何嵌入方法的形式，并在数据集上展示了有希望的结果。

    

    知识图谱嵌入是知识图谱中实体的密集数值表示。大多数方法仅关注实体之间的关系信息，较少有方法将文字描述或数值信息等知识也考虑在内。已有的方法通常针对特定的文字类型和嵌入方法。本文提出一组通用预处理操作符，可用于将带有数值、时间、文本和图像信息的知识图谱转换为能够使用任何方法进行嵌入的形式。在kgbench数据集上使用三种不同的嵌入方法进行实验，结果显示出有希望的成果。

    Knowledge graph embeddings are dense numerical representations of entities in a knowledge graph (KG). While the majority of approaches concentrate only on relational information, i.e., relations between entities, fewer approaches exist which also take information about literal values (e.g., textual descriptions or numerical information) into account. Those which exist are typically tailored towards a particular modality of literal and a particular embedding method. In this paper, we propose a set of universal preprocessing operators which can be used to transform KGs with literals for numerical, temporal, textual, and image information, so that the transformed KGs can be embedded with any method. The results on the kgbench dataset with three different embedding methods show promising results.
    
[^28]: 贝叶斯神经网络的摊销推理

    Amortised Inference in Bayesian Neural Networks. (arXiv:2309.03018v1 [stat.ML])

    [http://arxiv.org/abs/2309.03018](http://arxiv.org/abs/2309.03018)

    本文提出了一种摊销推理的贝叶斯神经网络方法，通过对推理进行摊销，能够更有效地利用数据进行概率元学习。

    

    元学习是一种框架，机器学习模型在一组数据集上进行训练，以便在测试时对新数据集进行预测。近年来，概率元学习受到研究界的广泛关注，但许多现有的概率元模型存在一个共性问题，即需要大量数据集才能生成具有高质量预测和良好校准不确定性估计的模型。然而，在许多应用中，很难获取这么多数据。在本文中，我们通过在贝叶斯神经网络中对推理进行摊销，引入了摊销伪观测变分推理贝叶斯神经网络（APOVI-BNN），提出了一种更加高效利用数据的概率元学习方法。首先，我们展示了在我们的摊销方案下获取的近似后验与传统变分推理获取的近似后验具有相似或更好的质量。

    Meta-learning is a framework in which machine learning models train over a set of datasets in order to produce predictions on new datasets at test time. Probabilistic meta-learning has received an abundance of attention from the research community in recent years, but a problem shared by many existing probabilistic meta-models is that they require a very large number of datasets in order to produce high-quality predictions with well-calibrated uncertainty estimates. In many applications, however, such quantities of data are simply not available.  In this dissertation we present a significantly more data-efficient approach to probabilistic meta-learning through per-datapoint amortisation of inference in Bayesian neural networks, introducing the Amortised Pseudo-Observation Variational Inference Bayesian Neural Network (APOVI-BNN). First, we show that the approximate posteriors obtained under our amortised scheme are of similar or better quality to those obtained through traditional vari
    
[^29]: SymED: 在边缘上自适应和在线符号化表示数据

    SymED: Adaptive and Online Symbolic Representation of Data on the Edge. (arXiv:2309.03014v1 [cs.DC])

    [http://arxiv.org/abs/2309.03014](http://arxiv.org/abs/2309.03014)

    SymED 是一种在线自适应和分布式的方法，用于在边缘上进行数据的符号化表示。它解决了在资源受限的边缘设备上进行实时数据处理的挑战，并可以通过符号化表示来进行各种边缘应用的数据分析。

    

    边缘计算范例有助于处理物联网生成的数据，使其在源头附近进行处理。在资源受限的边缘设备上传输、存储和处理这些迅速增长的数据量存在挑战。符号化表示算法是减小数据大小的有希望的解决方案，通过将实际原始数据转换为符号。此外，它们还允许对符号进行数据分析（例如异常检测和趋势预测），从而受益于各种边缘应用。然而，现有的符号化表示算法在设计上是集中化的，并且离线处理批量数据，这对于实时应用来说是不可行的。我们提出了SymED Symbolic Edge Data表示方法，即一种在边缘上进行符号化表示的在线自适应分布式方法。SymED基于自适应布朗桥聚合（ABBA）算法，我们假设低功耗的物联网设备先对数据进行初步压缩（发送者），更强大的边缘设备对数据进行进一步处理。

    The edge computing paradigm helps handle the Internet of Things (IoT) generated data in proximity to its source. Challenges occur in transferring, storing, and processing this rapidly growing amount of data on resource-constrained edge devices. Symbolic Representation (SR) algorithms are promising solutions to reduce the data size by converting actual raw data into symbols. Also, they allow data analytics (e.g., anomaly detection and trend prediction) directly on symbols, benefiting large classes of edge applications. However, existing SR algorithms are centralized in design and work offline with batch data, which is infeasible for real-time cases. We propose SymED Symbolic Edge Data representation method, i.e., an online, adaptive, and distributed approach for symbolic representation of data on edge. SymED is based on the Adaptive Brownian Bridge-based Aggregation (ABBA), where we assume low-powered IoT devices do initial data compression (senders) and the more robust edge devices d
    
[^30]: 通过平坦极小值和对抗鲁棒性解释激活稀疏性的理论解释

    Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])

    [http://arxiv.org/abs/2309.03004](http://arxiv.org/abs/2309.03004)

    提出了一个理论解释，将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。

    

    最近对MLP层中的激活稀疏性的实证观察为大幅降低计算成本提供了机会。尽管有几项研究将其归因于训练动力学，但激活稀疏性的理论解释仅限于浅层网络、小训练步长以及修改的训练，尽管这种稀疏性已在通过vanilla协议进行大步骤训练的深层模型中被发现。为了填补这三个差距，我们提出了梯度稀疏性的概念作为激活稀疏性的源头，并基于此提出了一个理论解释，该解释将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。这个理论适用于经过LayerNorm标准训练的纯MLP，并且如果在训练过程中给权重添加噪声，还适用于Transformers或其他架构。为了消除其他来源的激活稀疏性，我们还进行了进一步的实证研究。

    A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources o
    
[^31]: 使用强化学习在高维肌肉骨骼模型中实现自然且稳健的行走，无需演示

    Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models. (arXiv:2309.02976v1 [cs.RO])

    [http://arxiv.org/abs/2309.02976](http://arxiv.org/abs/2309.02976)

    本论文通过使用强化学习来解决肌肉骨骼冗余问题，并实现了在高维度肌肉骨骼模型中自然且稳健的行走。

    

    人类在复杂的自然环境中以稳健的双足行走表现出色。在每一步中，他们充分调整生物力学肌肉动力学和神经信号的相互作用，以在地面条件的不确定性下保持稳健。然而，我们还没有完全理解神经系统如何解决肌肉骨骼冗余问题，以解决考虑稳定性、稳健性和能量效率的多目标控制问题。在计算机模拟中，能量最小化已被证明是一种成功的优化目标，在轨迹优化或基于反射的控制方法中重现了自然行走。然而，这些方法一次只关注特定的运动，并且在补偿干扰时，所产生的控制器受到限制。在机器人领域，最近的强化学习（RL）方法在四足系统上实现了高度稳定（和高效）的运动，但要使用双足生物力学模型生成类似人类行走的行走需要大量的工作。

    Humans excel at robust bipedal walking in complex natural environments. In each step, they adequately tune the interaction of biomechanical muscle dynamics and neuronal signals to be robust against uncertainties in ground conditions. However, it is still not fully understood how the nervous system resolves the musculoskeletal redundancy to solve the multi-objective control problem considering stability, robustness, and energy efficiency. In computer simulations, energy minimization has been shown to be a successful optimization target, reproducing natural walking with trajectory optimization or reflex-based control methods. However, these methods focus on particular motions at a time and the resulting controllers are limited when compensating for perturbations. In robotics, reinforcement learning~(RL) methods recently achieved highly stable (and efficient) locomotion on quadruped systems, but the generation of human-like walking with bipedal biomechanical models has required extensive 
    
[^32]: 关于饲料成本风险对水产养殖估值和决策的影响研究

    On the Impact of Feeding Cost Risk in Aquaculture Valuation and Decision Making. (arXiv:2309.02970v1 [q-fin.RM])

    [http://arxiv.org/abs/2309.02970](http://arxiv.org/abs/2309.02970)

    该论文研究了饲料成本风险对水产养殖估值和决策的影响，并成功提出了使用深度神经网络推断决策边界的方法，对于改进传统方法的回归和曲线拟合方法已取得了显著的改善。

    

    我们研究了饲料成本的随机性对以动物为基础的商品的影响，特别关注水产养殖。具体而言，我们使用大豆期货来推断鲑鱼饲料的随机行为，我们假设其遵循Schwartz-2因子模型。我们比较了以确定性或随机饲料成本（即包括饲料成本风险）做出收获鲑鱼的决策规则。我们确定了考虑随机饲料成本时会有明显改善的情况，也有确定性饲料成本足够好的情况。然而，在所有这些情况下，新推导出的规则均显示出优越的性能，而附加的计算成本几乎可以忽略不计。从方法论上来说，我们展示了如何利用深度神经网络来推断决策边界，以确定收获或延续，改进了更传统的回归和曲线拟合方法。为了实现这一目标，我们使用了

    We study the effect of stochastic feeding costs on animal-based commodities with particular focus on aquaculture. More specifically, we use soybean futures to infer on the stochastic behaviour of salmon feed, which we assume to follow a Schwartz-2-factor model. We compare the decision of harvesting salmon using a decision rule assuming either deterministic or stochastic feeding costs, i.e. including feeding cost risk. We identify cases, where accounting for stochastic feeding costs leads to significant improvements as well as cases where deterministic feeding costs are a good enough proxy. Nevertheless, in all of these cases, the newly derived rules show superior performance, while the additional computational costs are negligible. From a methodological point of view, we demonstrate how to use Deep-Neural-Networks to infer on the decision boundary that determines harvesting or continuation, improving on more classical regression-based and curve-fitting methods. To achieve this we use a
    
[^33]: CR-VAE:对变分自动编码器进行对比正则化以防止后验坍塌

    CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse. (arXiv:2309.02968v1 [cs.LG])

    [http://arxiv.org/abs/2309.02968](http://arxiv.org/abs/2309.02968)

    CR-VAE是一种对比正则化变分自动编码器的方法，通过增加对比目标来最大化类似视觉输入之间的互信息，从而避免后验坍塌现象，并在多个视觉数据集上表现出比最先进方法更好的性能。

    

    变分自动编码器（VAE）已知存在后验坍塌现象，即模型生成的潜在表示与输入之间变得独立。这导致输入的表示退化，这归因于VAE目标函数的限制。在这项工作中，我们提出了一种新的解决方案，即对比正则化变分自动编码器（CR-VAE）。我们的方法的核心是在原始VAE中增加对比目标，最大化类似视觉输入的表示之间的互信息。这种策略确保了输入与其潜在表示之间的信息流最大化，有效地避免了后验坍塌。我们在一系列视觉数据集上评估了我们的方法，并证明CR-VAE在防止后验坍塌方面优于最先进的方法。

    The Variational Autoencoder (VAE) is known to suffer from the phenomenon of \textit{posterior collapse}, where the latent representations generated by the model become independent of the inputs. This leads to degenerated representations of the input, which is attributed to the limitations of the VAE's objective function. In this work, we propose a novel solution to this issue, the Contrastive Regularization for Variational Autoencoders (CR-VAE). The core of our approach is to augment the original VAE with a contrastive objective that maximizes the mutual information between the representations of similar visual inputs. This strategy ensures that the information flow between the input and its latent representation is maximized, effectively avoiding posterior collapse. We evaluate our method on a series of visual datasets and demonstrate, that CR-VAE outperforms state-of-the-art approaches in preventing posterior collapse.
    
[^34]: M3D-NCA: 具有内建质量控制的鲁棒的3D分割

    M3D-NCA: Robust 3D Segmentation with Built-in Quality Control. (arXiv:2309.02954v1 [cs.CV])

    [http://arxiv.org/abs/2309.02954](http://arxiv.org/abs/2309.02954)

    M3D-NCA是一种鲁棒的3D分割方法，具有内建质量控制，能够在资源受限的环境中运行。通过利用Neural Cellular Automata（NCA）分割和新的质量指标，M3D-NCA在海马体和前列腺分割中超越了大规模UNet模型。

    

    医学图像分割在很大程度上依赖于大规模的深度学习模型，例如基于UNet的架构。然而，这些模型的实际应用受到其高计算要求的限制，使其在资源受限环境（如初级保健设施和冲突地区）中不切实际。此外，图像领域的变化可能使这些模型失效，甚至在未发现错误的情况下危及患者安全。为了解决这些挑战，我们提出了一种新的方法，即M3D-NCA，它利用Neural Cellular Automata（NCA）分割进行3D医学图像处理，采用n级补丁化方法。此外，我们利用M3D-NCA中的差异来开发一种新的质量指标，可以自动检测NCA分割过程中的错误。M3D-NCA在海马体和前列腺分割方面的性能优于两个规模更大的UNet模型，其Dice系数提高2％，并且可以在Raspberry Pi 4 Model B（2GB RAM）上运行。这凸显了其潜力。

    Medical image segmentation relies heavily on large-scale deep learning models, such as UNet-based architectures. However, the real-world utility of such models is limited by their high computational requirements, which makes them impractical for resource-constrained environments such as primary care facilities and conflict zones. Furthermore, shifts in the imaging domain can render these models ineffective and even compromise patient safety if such errors go undetected. To address these challenges, we propose M3D-NCA, a novel methodology that leverages Neural Cellular Automata (NCA) segmentation for 3D medical images using n-level patchification. Moreover, we exploit the variance in M3D-NCA to develop a novel quality metric which can automatically detect errors in the segmentation process of NCAs. M3D-NCA outperforms the two magnitudes larger UNet models in hippocampus and prostate segmentation by 2% Dice and can be run on a Raspberry Pi 4 Model B (2GB RAM). This highlights the potenti
    
[^35]: 利用物理信息机器学习估计不规则用水需求以支持泄漏检测

    Estimating irregular water demands with physics-informed machine learning to inform leakage detection. (arXiv:2309.02935v1 [cs.LG])

    [http://arxiv.org/abs/2309.02935](http://arxiv.org/abs/2309.02935)

    本研究提出了一种利用物理信息的机器学习算法，通过分析压力数据估计未知的不规则用水需求，并通过简化泄漏检测问题实现线性化。

    

    饮用水供应网络中的漏水问题给水务公司带来了重大挑战，导致了基础设施故障、运营中断、环境风险、财产损失和经济损失。及时识别和准确定位这些泄漏对于水务公司来说至关重要，然而，泄漏检测算法的实施在实践中受到水力模型或大量训练数据要求的限制。利用物理信息的机器学习可以利用水力信息，从而避免这两个限制。在本研究中，我们提出了一种利用物理信息的机器学习算法，通过一个全连接神经网络分析压力数据，并从中估计未知的不规则用水需求，最终利用伯努利方程有效线性化泄漏检测问题。我们的算法在L-Town基准网络的数据上进行了测试，结果表明，

    Leakages in drinking water distribution networks pose significant challenges to water utilities, leading to infrastructure failure, operational disruptions, environmental hazards, property damage, and economic losses. The timely identification and accurate localisation of such leakages is paramount for utilities to mitigate these unwanted effects. However, implementation of algorithms for leakage detection is limited in practice by requirements of either hydraulic models or large amounts of training data. Physics-informed machine learning can utilise hydraulic information thereby circumventing both limitations. In this work, we present a physics-informed machine learning algorithm that analyses pressure data and therefrom estimates unknown irregular water demands via a fully connected neural network, ultimately leveraging the Bernoulli equation and effectively linearising the leakage detection problem. Our algorithm is tested on data from the L-Town benchmark network, and results indic
    
[^36]: GroupEnc: 具有群损失的编码器以实现全局结构保持

    GroupEnc: encoder with group loss for global structure preservation. (arXiv:2309.02917v1 [cs.LG])

    [http://arxiv.org/abs/2309.02917](http://arxiv.org/abs/2309.02917)

    本文提出了一个名为GroupEnc的编码器模型，该模型利用了群损失函数，以实现比传统的变分自动编码器（VAE）更少的全局结构失真，并保持模型的参数化和架构的灵活性。

    

    最近在降维技术方面取得了较准确的高维数据的低维嵌入。除了可视化目的外，这些嵌入还可以用于下游处理，包括批次效应标准化、聚类、社区检测或轨迹推断。我们使用局部和全局层面的结构保持概念，创建了一个基于变分自动编码器（VAE）和SQuadMDS算法中的随机四分位损失的深度学习模型。我们的编码器模型名为GroupEnc，使用“群损失”函数创建嵌入，相比于VAE，能够减少全局结构畸变，同时保持模型的参数化和架构的灵活性。我们使用公开可用的生物学单细胞转录组数据集进行验证，并使用RNX曲线进行评估。

    Recent advances in dimensionality reduction have achieved more accurate lower-dimensional embeddings of high-dimensional data. In addition to visualisation purposes, these embeddings can be used for downstream processing, including batch effect normalisation, clustering, community detection or trajectory inference. We use the notion of structure preservation at both local and global levels to create a deep learning model, based on a variational autoencoder (VAE) and the stochastic quartet loss from the SQuadMDS algorithm. Our encoder model, called GroupEnc, uses a 'group loss' function to create embeddings with less global structure distortion than VAEs do, while keeping the model parametric and the architecture flexible. We validate our approach using publicly available biological single-cell transcriptomic datasets, employing RNX curves for evaluation.
    
[^37]: 《针对混合语言的人物感知生成模型》

    Persona-aware Generative Model for Code-mixed Language. (arXiv:2309.02915v1 [cs.CL])

    [http://arxiv.org/abs/2309.02915](http://arxiv.org/abs/2309.02915)

    本论文提出了一种针对混合语言的人物感知生成模型PARADOX，它能够生成类似于真实个体代码混合文本的文本。该模型以用户的人物形象为条件来编码对话，并生成不带单语参考数据的代码混合文本。模型还进行对齐，使生成的文本更接近真实的代码混合文本。这种方法在语义上更有意义，在语言上更有效。

    

    在在线社交网络和多语言社会中，代码混合和脚本混合非常普遍。然而，用户对于代码混合的偏好取决于用户的社会经济地位、人口统计信息和当地环境，而现有的生成模型在生成代码混合文本时大多忽视了这些因素。在这项工作中，我们首次尝试开发一种人物感知的生成模型，以生成类似于真实个体代码混合文本的文本。我们提出了一种针对代码混合生成的人物感知生成模型（PARADOX），这是一种基于Transformer编码器-解码器的新型模型，该模型在给定用户的人物形象的条件下对话进行编码，并生成不带单语参考数据的代码混合文本。我们提出了一个对齐模块，对生成的序列进行重新校准，使其更接近真实的代码混合文本。PARADOX生成的代码混合文本在语义上更有意义，在语言上更有效。

    Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user's preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models mostly ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose a Persona-aware Generative Model for Code-mixed Generation, PARADOX, a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user's persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARAD
    
[^38]: 集成深度神经网络用于无人机辅助网络中最小化时延信息的论文标题

    Ensemble DNN for Age-of-Information Minimization in UAV-assisted Networks. (arXiv:2309.02913v1 [math.OC])

    [http://arxiv.org/abs/2309.02913](http://arxiv.org/abs/2309.02913)

    该论文提出了一种集成深度神经网络（EDNN）的方法，通过优化无人机的停留位置和设备选择概率，减少了设备之间的预期时延信息（AoI）。实验结果表明，该方法比传统的深度神经网络（DNNs）在降低预期AoI方面表现更好，可实现29.5%的降低。

    

    本文针对无人机辅助网络中时延信息（AoI）的问题进行了研究。我们的目标是通过优化无人机的停留位置和设备选择概率来最小化设备之间的预期AoI。为了解决这个问题，我们首先推导出了一个包含设备选择概率的预期AoI的闭式表达式。然后，我们将问题形式化为一个非凸最小化问题，并添加了服务质量约束。由于这个问题很难解决，我们提出了一种基于集成深度神经网络（EDNN）的方法，该方法利用了所研究问题的对偶形式。具体而言，集成中的深度神经网络（DNNs）通过使用所研究问题的Lagrangian函数以无监督的方式进行训练。我们的实验结果表明，所提出的EDNN方法在减少预期AoI方面表现优于传统的DNNs，实现了显著的29.5%降低。

    This paper addresses the problem of Age-of-Information (AoI) in UAV-assisted networks. Our objective is to minimize the expected AoI across devices by optimizing UAVs' stopping locations and device selection probabilities. To tackle this problem, we first derive a closed-form expression of the expected AoI that involves the probabilities of selection of devices. Then, we formulate the problem as a non-convex minimization subject to quality of service constraints. Since the problem is challenging to solve, we propose an Ensemble Deep Neural Network (EDNN) based approach which takes advantage of the dual formulation of the studied problem. Specifically, the Deep Neural Networks (DNNs) in the ensemble are trained in an unsupervised manner using the Lagrangian function of the studied problem. Our experiments show that the proposed EDNN method outperforms traditional DNNs in reducing the expected AoI, achieving a remarkable reduction of $29.5\%$.
    
[^39]: 一种用于综合三维矿产前景建模的多模态学习框架以及联合学习的结构-流体关系

    A Multimodal Learning Framework for Comprehensive 3D Mineral Prospectivity Modeling with Jointly Learned Structure-Fluid Relationships. (arXiv:2309.02911v1 [cs.LG])

    [http://arxiv.org/abs/2309.02911](http://arxiv.org/abs/2309.02911)

    本研究提出了一种新颖的多模态融合模型，通过深度网络架构有效地整合结构和流体信息，优于其他模型的结果分析表明其在区分含矿实例和预测矿产前景方面表现出优越的性能；消融研究进一步揭示了联合特征利用和CCA融合的益处。这对于增强勘探决策具有重要作用。

    

    本研究提出了一种新颖的多模态融合模型，用于三维矿产前景映射（3D MPM），通过深度网络架构有效地整合结构和流体信息。利用卷积神经网络（CNN）和多层感知器（MLP），该模型采用典型相关分析（CCA）来对齐和融合多模态特征。对胶胶金矿床数据集的严格评估表明，该模型在区分含矿实例和预测矿产前景方面表现出优越的性能，优于其他模型的结果分析。消融研究进一步揭示了联合特征利用和CCA融合的益处。本研究不仅推进了矿产前景建模，还强调了数据整合和特征对齐在增强勘探决策中的关键作用。

    This study presents a novel multimodal fusion model for three-dimensional mineral prospectivity mapping (3D MPM), effectively integrating structural and fluid information through a deep network architecture. Leveraging Convolutional Neural Networks (CNN) and Multilayer Perceptrons (MLP), the model employs canonical correlation analysis (CCA) to align and fuse multimodal features. Rigorous evaluation on the Jiaojia gold deposit dataset demonstrates the model's superior performance in distinguishing ore-bearing instances and predicting mineral prospectivity, outperforming other models in result analyses. Ablation studies further reveal the benefits of joint feature utilization and CCA incorporation. This research not only advances mineral prospectivity modeling but also highlights the pivotal role of data integration and feature alignment for enhanced exploration decision-making.
    
[^40]: DECODE: 基于历史数据和环境因素的数据驱动能耗预测在建筑中的应用

    DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings. (arXiv:2309.02908v1 [cs.LG])

    [http://arxiv.org/abs/2309.02908](http://arxiv.org/abs/2309.02908)

    本论文介绍了一种基于历史数据、占用模式和天气条件的LSTM模型，用于准确预测建筑能耗，该模型在预测精度上表现出卓越性能。

    

    建筑中的能耗预测在有效的能源管理中起着至关重要的作用。精确的预测对于实现优化的能耗和电网分配是必要的。本论文引入了一种基于历史能源数据、占用模式和天气条件的长短期记忆（LSTM）模型，用于预测建筑能耗。与现有的预测模型相比，LSTM模型提供了准确的短期、中期和长期能耗预测，适用于住宅和商业建筑。我们将我们的LSTM模型与线性回归、决策树和随机森林等已有的预测方法进行了比较。令人鼓舞的是，提出的LSTM模型在所有指标上表现出优越性能。它具有出色的预测精度，R2得分为0.97，最低的平均绝对误差（MAE）为0.007。我们开发的模型的另一个优点是能够实现高效的能耗。

    Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consu
    
[^41]: 发现离散对称的统一框架

    A Unified Framework for Discovering Discrete Symmetries. (arXiv:2309.02898v1 [cs.LG])

    [http://arxiv.org/abs/2309.02898](http://arxiv.org/abs/2309.02898)

    本文提出了一个统一框架，能够发现各种类型的对称性，通过使用线性和张量值函数构成的新颖架构，在多臂赌博算法和梯度下降的帮助下，高效地优化并学习到对称性。在图像数字求和和多项式回归任务上的实验证明了该方法的有效性。

    

    我们考虑从一类对称性中学习一个符合对称性的函数的问题。我们开发了一个统一框架，能够发现包括局部对称、二面角和循环子群在内的广泛子群的对称性。该框架的核心是一种由线性和张量值函数组成的新颖架构，以原则性的方式表达这些子群不变的函数。架构的结构使我们能够利用多臂赌博算法和梯度下降分别高效优化线性和张量值函数，并推断出最终学习到的对称性。我们还讨论了架构中张量值函数的必要性。对图像数字求和和多项式回归任务的实验结果证明了我们方法的有效性。

    We consider the problem of learning a function respecting a symmetry from among a class of symmetries. We develop a unified framework that enables symmetry discovery across a broad range of subgroups including locally symmetric, dihedral and cyclic subgroups. At the core of the framework is a novel architecture composed of linear and tensor-valued functions that expresses functions invariant to these subgroups in a principled manner. The structure of the architecture enables us to leverage multi-armed bandit algorithms and gradient descent to efficiently optimize over the linear and the tensor-valued functions, respectively, and to infer the symmetry that is ultimately learnt. We also discuss the necessity of the tensor-valued functions in the architecture. Experiments on image-digit sum and polynomial regression tasks demonstrate the effectiveness of our approach.
    
[^42]: 非冲突教学图在图中球的应用研究

    Non-Clashing Teaching Maps for Balls in Graphs. (arXiv:2309.02876v1 [cs.CC])

    [http://arxiv.org/abs/2309.02876](http://arxiv.org/abs/2309.02876)

    本文研究了在图中球的概念类中的非冲突教学图，并证明了相关决策问题{\sc B-NCTD$^+$}是NP完全的。

    

    最近，Kirkpatrick等人[ALT 2019]和Fallat等人[JMLR 2023]引入了非冲突教学，并表明它是满足Goldman和Mathias提出的防止勾结基准的最高效的机器教学模型。对于一个概念类$\cal{C}$来说，教学图$T$将一个（教学）集合$T(C)$分配给每个概念$C \in \cal{C}$。如果没有一对概念与它们的教学集合的并一致，则教学图是非冲突的。非冲突教学图（NCTM）$T$的大小是$T(C)$中的最大大小，其中$C \in \cal{C}$。概念类$\mathcal{B}(G)$的非冲突教学维度NCTD$(\cal{C})$是$\cal{C}$的一个NCTM的最小大小。类似地，NCTM$^+$和NCTD$^+(\cal{C})$的定义是类似的，只是教师只能使用正例。我们研究了由图$G$的所有球组成的概念类$\mathcal{B}(G)$的NCTMs和NCTM$^+$s。我们证明了与NCTD$^+$的相关决策问题{\sc B-NCTD$^+$}是NP完全的。

    Recently, Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and showed it to be the most efficient machine teaching model satisfying the benchmark for collusion-avoidance set by Goldman and Mathias. A teaching map $T$ for a concept class $\cal{C}$ assigns a (teaching) set $T(C)$ of examples to each concept $C \in \cal{C}$. A teaching map is non-clashing if no pair of concepts are consistent with the union of their teaching sets. The size of a non-clashing teaching map (NCTM) $T$ is the maximum size of a $T(C)$, $C \in \cal{C}$. The non-clashing teaching dimension NCTD$(\cal{C})$ of $\cal{C}$ is the minimum size of an NCTM for $\cal{C}$. NCTM$^+$ and NCTD$^+(\cal{C})$ are defined analogously, except the teacher may only use positive examples.  We study NCTMs and NCTM$^+$s for the concept class $\mathcal{B}(G)$ consisting of all balls of a graph $G$. We show that the associated decision problem {\sc B-NCTD$^+$} for NCTD$^+$ is NP-complete in spl
    
[^43]: 通过具备模拟器信息的潜在状态学习混合动力学模型

    Learning Hybrid Dynamics Models With Simulator-Informed Latent States. (arXiv:2309.02873v1 [cs.LG])

    [http://arxiv.org/abs/2309.02873](http://arxiv.org/abs/2309.02873)

    本文提出了一种通过用模拟器更新潜在状态的方法来学习混合动力学模型的新方法，以控制预测并防止累积误差的发生。

    

    动力学模型学习的任务是从测量数据中推断未知的系统动力学，并预测系统未来的行为。解决这个问题的一种常见方法是训练递归模型。然而，这些模型的预测通常在物理意义上不合理，并且由于累积误差的存在，随着时间推移，它们的行为会恶化。通常情况下，基于第一原理构建的模拟器是物理意义上合理的。然而，建模简化通常会导致这些模型的不准确性。因此，混合建模是一种新兴的趋势，旨在结合两者的优点。在本文中，我们提出了一种新的混合建模方法，通过黑盒模拟器将学习模型的潜在状态进行更新。通过模拟器来控制预测，防止累积误差的发生，这是一项特别具有挑战性的任务。

    Dynamics model learning deals with the task of inferring unknown dynamics from measurement data and predicting the future behavior of the system. A typical approach to address this problem is to train recurrent models. However, predictions with these models are often not physically meaningful. Further, they suffer from deteriorated behavior over time due to accumulating errors. Often, simulators building on first principles are available being physically meaningful by design. However, modeling simplifications typically cause inaccuracies in these models. Consequently, hybrid modeling is an emerging trend that aims to combine the best of both worlds. In this paper, we propose a new approach to hybrid modeling, where we inform the latent states of a learned model via a black-box simulator. This allows to control the predictions via the simulator preventing them from accumulating errors. This is especially challenging since, in contrast to previous approaches, access to the simulator's la
    
[^44]: 在在线连续学习中重新思考动量知识蒸馏

    Rethinking Momentum Knowledge Distillation in Online Continual Learning. (arXiv:2309.02870v1 [cs.LG])

    [http://arxiv.org/abs/2309.02870](http://arxiv.org/abs/2309.02870)

    该论文重新思考了在线连续学习中的动量知识蒸馏问题，通过将动量知识蒸馏应用于OCL方法，提高了现有方法的准确性，并对MKD在OCL中的训练过程进行了深入分析。

    

    在线连续学习（OCL）解决了神经网络在连续的数据流上训练的问题，其中多个分类任务按顺序出现。与离线连续学习相比，在OCL中只能看到数据一次。在这种情况下，基于回放的策略取得了令人印象深刻的结果，大多数最先进的方法都严重依赖它们。尽管知识蒸馏（KD）在离线连续学习中已被广泛使用，但在OCL中仍然未充分利用其潜力。在本文中，我们在理论上分析了将KD应用于OCL中面临的挑战。我们介绍了一种直接而有效的方法，将动量知识蒸馏（MKD）应用于许多旗舰OCL方法，并展示了它在增强现有方法方面的能力。除了将现有的最先进的ImageNet100准确率提高了超过10个百分点之外，我们还阐明了MKD在OCL中的训练过程中的内部机制和影响。

    Online Continual Learning (OCL) addresses the problem of training neural networks on a continuous data stream where multiple classification tasks emerge in sequence. In contrast to offline Continual Learning, data can be seen only once in OCL. In this context, replay-based strategies have achieved impressive results and most state-of-the-art approaches are heavily depending on them. While Knowledge Distillation (KD) has been extensively used in offline Continual Learning, it remains under-exploited in OCL, despite its potential. In this paper, we theoretically analyze the challenges in applying KD to OCL. We introduce a direct yet effective methodology for applying Momentum Knowledge Distillation (MKD) to many flagship OCL methods and demonstrate its capabilities to enhance existing approaches. In addition to improving existing state-of-the-arts accuracy by more than $10\%$ points on ImageNet100, we shed light on MKD internal mechanics and impacts during training in OCL. We argue that 
    
[^45]: 降低深度强化学习模型中的不良行为

    On Reducing Undesirable Behavior in Deep Reinforcement Learning Models. (arXiv:2309.02869v1 [cs.LG])

    [http://arxiv.org/abs/2309.02869](http://arxiv.org/abs/2309.02869)

    本论文提出了一个旨在降低深度强化学习模型不良行为的框架，通过从错误的状态-动作对中提取决策树分类器，并将其整合到训练循环中，来惩罚系统错误行为。这一框架在保持卓越性能的同时，为工程师提供了针对不良行为的可理解表征。

    

    深度强化学习（DRL）在大量应用领域证明了其极大的效用。然而，即使是成功的基于DRL的软件也可能表现出极其不良的行为。这是因为DRL训练是基于最大化一个奖励函数，该函数通常能捕捉到一般趋势，但不能准确捕捉或排除系统的某些行为。在本文中，我们提出了一个新的框架，旨在大幅降低基于DRL的软件的不良行为，同时保持其出色的性能。此外，我们的框架可以帮助工程师对这种不良行为进行可理解的表征。在底层，我们的方法基于从错误的状态-动作对中提取决策树分类器，然后将这些决策树整合到DRL训练循环中，当系统发生错误时对其进行惩罚。我们提供了一个我们方法的概念验证实现，并用它来评估该技术。

    Deep reinforcement learning (DRL) has proven extremely useful in a large variety of application domains. However, even successful DRL-based software can exhibit highly undesirable behavior. This is due to DRL training being based on maximizing a reward function, which typically captures general trends but cannot precisely capture, or rule out, certain behaviors of the system. In this paper, we propose a novel framework aimed at drastically reducing the undesirable behavior of DRL-based software, while maintaining its excellent performance. In addition, our framework can assist in providing engineers with a comprehensible characterization of such undesirable behavior. Under the hood, our approach is based on extracting decision tree classifiers from erroneous state-action pairs, and then integrating these trees into the DRL training loop, penalizing the system whenever it performs an error. We provide a proof-of-concept implementation of our approach, and use it to evaluate the techniqu
    
[^46]: 借助对比关系推理来增强事件序列建模

    Enhancing Event Sequence Modeling with Contrastive Relational Inference. (arXiv:2309.02868v1 [cs.LG])

    [http://arxiv.org/abs/2309.02868](http://arxiv.org/abs/2309.02868)

    本文提出了一种利用对比关系推理的方法来增强事件序列建模。通过学习一个关系图并在观测数据中捕捉动态模式，我们的模型能够推断事件之间的相互作用，从而在事件序列数据的推理任务中表现出较好的效果。

    

    神经时序点过程（TPPs）在建模连续时间事件序列方面显示出了潜力。然而，捕捉事件之间的相互作用对于执行预测等事件序列数据的推理任务是具有挑战性和关键性的。现有的TPP模型侧重于参数化未来事件的条件分布，但难以建模事件之间的相互作用。本文提出了一种新颖的方法，利用神经关系推理（NRI）来学习一个关系图，从观测数据中同时学习动态模式和推断相互作用。我们的方法，对比关系推理驱动的Hawkes过程（CRIHP），在变分推理框架下推理事件之间的相互作用。它利用基于强度的学习来搜索对比关系约束的原型路径。在三个真实世界数据集上进行的大量实验证明了我们模型在捕捉事件相互作用方面的有效性。

    Neural temporal point processes(TPPs) have shown promise for modeling continuous-time event sequences. However, capturing the interactions between events is challenging yet critical for performing inference tasks like forecasting on event sequence data. Existing TPP models have focused on parameterizing the conditional distribution of future events but struggle to model event interactions. In this paper, we propose a novel approach that leverages Neural Relational Inference (NRI) to learn a relation graph that infers interactions while simultaneously learning the dynamics patterns from observational data. Our approach, the Contrastive Relational Inference-based Hawkes Process (CRIHP), reasons about event interactions under a variational inference framework. It utilizes intensity-based learning to search for prototype paths to contrast relationship constraints. Extensive experiments on three real-world datasets demonstrate the effectiveness of our model in capturing event interactions f
    
[^47]: 通用互信息：一种辨别聚类的框架

    Generalised Mutual Information: a Framework for Discriminative Clustering. (arXiv:2309.02858v1 [stat.ML])

    [http://arxiv.org/abs/2309.02858](http://arxiv.org/abs/2309.02858)

    本文介绍了通用互信息（GEMINI）作为一种辨别聚类的框架，相比互信息（MI），GEMINI在无监督神经网络训练过程中不需要正则化，其可以选择合适的聚类数量。

    

    在过去十年中，深度聚类的最新成果主要涉及作为无监督训练神经网络的客观函数的互信息（MI），并增加了正则项。尽管正则化的质量已经被广泛讨论以进行改进，但对于MI作为聚类目标的相关性却没有得到足够的关注。本文首先强调了最大化MI并不能得到令人满意的聚类结果。我们发现库尔巴克-莱布勒散度是这一行为的主要原因。因此，我们通过改变其核心差异，引入通用互信息（GEMINI）来推广互信息：一组用于无监督神经网络训练的度量。与MI不同的是，一些GEMINI在训练时不需要正则化，因为它们在数据空间中具有几何意识的距离或核函数。最后，我们强调，GEMINI可以自动选择相关数量的聚类，这是一个有意义的特性。

    In the last decade, recent successes in deep clustering majorly involved the Mutual Information (MI) as an unsupervised objective for training neural networks with increasing regularisations. While the quality of the regularisations have been largely discussed for improvements, little attention has been dedicated to the relevance of MI as a clustering objective. In this paper, we first highlight how the maximisation of MI does not lead to satisfying clusters. We identified the Kullback-Leibler divergence as the main reason of this behaviour. Hence, we generalise the mutual information by changing its core distance, introducing the Generalised Mutual Information (GEMINI): a set of metrics for unsupervised neural network training. Unlike MI, some GEMINIs do not require regularisations when training as they are geometry-aware thanks to distances or kernels in the data space. Finally, we highlight that GEMINIs can automatically select a relevant number of clusters, a property that has been
    
[^48]: 评估基于序列的异常检测技术常见日志数据集的关键回顾

    A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques. (arXiv:2309.02854v1 [cs.LG])

    [http://arxiv.org/abs/2309.02854](http://arxiv.org/abs/2309.02854)

    本文对六个公开可用的日志数据集进行了分析，重点关注异常的表现形式和其检测的简单技术。

    

    日志数据存储与系统或应用程序的底层工作流相对应的事件执行模式。虽然大多数日志具有信息性，但日志数据中也包含指示故障或事故的痕迹。因此，日志数据常用于评估旨在自动揭示意外或其他相关系统行为模式的异常检测技术。最近，利用深度学习的检测方法越来越关注以序列模式的变化为特征的异常，在一般事件追踪中。一些公开可用的数据集，如HDFS，BGL，Thunderbird，OpenStack和Hadoop，已成为评估这些异常检测技术的标准，然而，过去对这些数据集的适用性并未得到仔细研究。因此，在本文中，我们对六个公开可用的日志数据集进行分析，重点关注异常的表现形式和其检测的简单技术。我们的发现

    Log data store event execution patterns that correspond to underlying workflows of systems or applications. While most logs are informative, log data also include artifacts that indicate failures or incidents. Accordingly, log data are often used to evaluate anomaly detection techniques that aim to automatically disclose unexpected or otherwise relevant system behavior patterns. Recently, detection approaches leveraging deep learning have increasingly focused on anomalies that manifest as changes of sequential patterns within otherwise normal event traces. Several publicly available data sets, such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop, have since become standards for evaluating these anomaly detection techniques, however, the appropriateness of these data sets has not been closely investigated in the past. In this paper we therefore analyze six publicly available log data sets with focus on the manifestations of anomalies and simple techniques for their detection. Our findi
    
[^49]: 针对组合贝叶斯优化的随机后处理方法

    Random postprocessing for combinatorial Bayesian optimization. (arXiv:2309.02842v1 [cs.LG])

    [http://arxiv.org/abs/2309.02842](http://arxiv.org/abs/2309.02842)

    针对组合贝叶斯优化，我们研究了一种随机后处理方法，严格禁止数据集中的重复样本，结果表明此方法显著减少了顺序步骤数，特别是在最大后验估计的情况下，为解决高维问题中贝叶斯优化的收敛速度慢提供了一种简单但通用的策略。

    

    基于模型的顺序方法用于离散的“黑盒”优化问题，包括贝叶斯优化技术，通常会对给定的目标函数访问多次相同的点，导致需要很多步骤才能找到全局最优解。在这里，我们对贝叶斯优化中的一种后处理方法进行了数值研究，该方法严格禁止数据集中的重复样本。我们发现后处理方法显著减少了找到全局最优解所需的顺序步骤数，特别是当采样函数是最大后验估计时。我们的结果为解决高维问题中贝叶斯优化的收敛速度慢提供了一种简单但通用的策略。

    Model-based sequential approaches to discrete "black-box" optimization, including Bayesian optimization techniques, often access the same points multiple times for a given objective function in interest, resulting in many steps to find the global optimum. Here, we numerically study the effect of a postprocessing method on Bayesian optimization that strictly prohibits duplicated samples in the dataset. We find the postprocessing method significantly reduces the number of sequential steps to find the global optimum, especially when the acquisition function is of maximum a posterior estimation. Our results provide a simple but general strategy to solve the slow convergence of Bayesian optimization for high-dimensional problems.
    
[^50]: BigVSAN: 利用切片对抗网络增强基于GAN的神经声码器

    BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network. (arXiv:2309.02836v1 [cs.SD])

    [http://arxiv.org/abs/2309.02836](http://arxiv.org/abs/2309.02836)

    本文研究了利用切片对抗网络（SAN）来增强基于GAN的神经声码器，并通过实验证明了通过修改最小二乘GAN的损失函数，SAN可以改善声码器的性能。

    

    基于生成对抗网络（GAN）的声码器已经得到广泛研究，因为它们可以比实时更快地合成高保真音频波形。然而，已有研究报告发现大多数GAN在特征空间中无法获得区分真假数据的最佳投影。在文献中已经证明，切片对抗网络（SAN）作为一种改进的GAN训练框架，在图像生成任务中是有效的。本文中，我们研究了SAN在声码任务中的有效性。为此，我们提出了一种方案，修改了大多数基于GAN的声码器所采用的最小二乘GAN，使其损失函数满足SAN的要求。通过实验，我们证明了SAN可以通过小的修改改善包括BigVGAN在内的基于GAN的声码器的性能。我们的代码可在https://github.com/sony/bigvsan上获得。

    Generative adversarial network (GAN)-based vocoders have been intensively studied because they can synthesize high-fidelity audio waveforms faster than real-time. However, it has been reported that most GANs fail to obtain the optimal projection for discriminating between real and fake data in the feature space. In the literature, it has been demonstrated that slicing adversarial network (SAN), an improved GAN training framework that can find the optimal projection, is effective in the image generation task. In this paper, we investigate the effectiveness of SAN in the vocoding task. For this purpose, we propose a scheme to modify least-squares GAN, which most GAN-based vocoders adopt, so that their loss functions satisfy the requirements of SAN. Through our experiments, we demonstrate that SAN can improve the performance of GAN-based vocoders, including BigVGAN, with small modifications. Our code is available at https://github.com/sony/bigvsan.
    
[^51]: Roulette：一种用于深度学习分类任务的语义隐私保护的设备边缘协同推理框架

    Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks. (arXiv:2309.02820v1 [cs.LG])

    [http://arxiv.org/abs/2309.02820](http://arxiv.org/abs/2309.02820)

    Roulette是一种用于深度学习分类任务的语义隐私保护的设备边缘协同推理框架，通过混淆和加噪声实现隐私保护，同时保持高准确性。

    

    在人工智能时代，深度学习分类器至关重要。设备边缘协同推理已被广泛采用作为在物联网和5G/6G网络中推广其应用的高效框架。然而，在非独立同分布数据分布和隐私泄露方面都存在精度下降问题。对于精度下降，直接使用迁移学习和分割学习成本较高，隐私问题仍然存在。对于隐私泄露，基于密码学的方法会导致巨大的开销。其他轻量级方法假设真实标签是非敏感的并且可以被公开。但是对于许多应用来说，真实标签是用户的关键敏感信息。在本文中，我们提出了一种名为Roulette的框架，它是一种面向任务的语义隐私保护的深度学习分类器的协同推理框架。除了输入数据，我们将数据的真实标签视为私密信息。我们开发了一种新颖的方法，通过混淆和加噪声来实现隐私保护，并通过推理结果的概率模型来保持高准确性。

    Deep learning classifiers are crucial in the age of artificial intelligence. The device-edge-based collaborative inference has been widely adopted as an efficient framework for promoting its applications in IoT and 5G/6G networks. However, it suffers from accuracy degradation under non-i.i.d. data distribution and privacy disclosure. For accuracy degradation, direct use of transfer learning and split learning is high cost and privacy issues remain. For privacy disclosure, cryptography-based approaches lead to a huge overhead. Other lightweight methods assume that the ground truth is non-sensitive and can be exposed. But for many applications, the ground truth is the user's crucial privacy-sensitive information. In this paper, we propose a framework of Roulette, which is a task-oriented semantic privacy-preserving collaborative inference framework for deep learning classifiers. More than input data, we treat the ground truth of the data as private information. We develop a novel paradig
    
[^52]: 结合离心压缩机的热力学模型和主动机器学习的增强工业设计优化

    Combining Thermodynamics-based Model of the Centrifugal Compressors and Active Machine Learning for Enhanced Industrial Design Optimization. (arXiv:2309.02818v1 [cs.LG])

    [http://arxiv.org/abs/2309.02818](http://arxiv.org/abs/2309.02818)

    结合离心压缩机的热力学模型和主动机器学习，提出了Active-CompDesign框架用于离心压缩机的优化设计。在离线和在线环境中进行实验，显示出显著的性能提升。

    

    离心压缩机的设计过程需要应用一个优化过程，由于该过程下面的分析方程，计算开销较大。虽然回归替代模型可以大幅减少这个过程的计算开销，主要挑战是缺乏用于训练替代模型的数据。为了战略性地利用标记样本，我们提出了Active-CompDesign框架，其中在可部署的主动学习（AL）环境中将基于热力学的压缩机模型（即我们内部的压缩机设计软件）与基于高斯过程的替代模型结合起来。我们首先在离线设置中进行实验，并进一步将其扩展到在线AL框架，其中与基于热力学的压缩机模型的实时交互允许在生产中部署。ActiveCompDesign在替代建模方面显示出显著的性能提升。

    The design process of centrifugal compressors requires applying an optimization process which is computationally expensive due to complex analytical equations underlying the compressor's dynamical equations. Although the regression surrogate models could drastically reduce the computational cost of such a process, the major challenge is the scarcity of data for training the surrogate model. Aiming to strategically exploit the labeled samples, we propose the Active-CompDesign framework in which we combine a thermodynamics-based compressor model (i.e., our internal software for compressor design) and Gaussian Process-based surrogate model within a deployable Active Learning (AL) setting. We first conduct experiments in an offline setting and further, extend it to an online AL framework where a real-time interaction with the thermodynamics-based compressor's model allows the deployment in production. ActiveCompDesign shows a significant performance improvement in surrogate modeling by lev
    
[^53]: 引入热力学信息的符号回归——一种用于热力学状态方程开发的工具

    Introducing Thermodynamics-Informed Symbolic Regression -- A Tool for Thermodynamic Equations of State Development. (arXiv:2309.02805v1 [cs.LG])

    [http://arxiv.org/abs/2309.02805](http://arxiv.org/abs/2309.02805)

    引入热力学信息的符号回归是一个用于加速热力学状态方程开发的工具，该工具结合了符号回归的方法和处理实验数据的特性。

    

    热力学状态方程（EOS）对许多行业以及学术界都至关重要。即使不考虑用于数据采集的昂贵而广泛的测量活动，EOS的开发仍是一个非常耗时的过程，通常仍然严重依赖专家知识和迭代微调。为了改进和加快EOS开发过程，我们引入了热力学信息的符号回归（TiSR），这是一个针对热力学EOS建模的符号回归工具。TiSR已经是一个功能强大的符号回归工具，已经在https://doi.org/10.1007/s10765-023-03197-z的研究中使用。它旨在将基于SR的方法与处理通常强烈分散的实验数据所需的扩展、不同的残差预处理和后处理选项以及考虑热力学EOS开发所需的其他特性相结合。尽管TiSR尚未准备好供最终用户使用，本文旨在报告其当前状态。

    Thermodynamic equations of state (EOS) are essential for many industries as well as in academia. Even leaving aside the expensive and extensive measurement campaigns required for the data acquisition, the development of EOS is an intensely time-consuming process, which does often still heavily rely on expert knowledge and iterative fine-tuning. To improve upon and accelerate the EOS development process, we introduce thermodynamics-informed symbolic regression (TiSR), a symbolic regression (SR) tool aimed at thermodynamic EOS modeling. TiSR is already a capable SR tool, which was used in the research of https://doi.org/10.1007/s10765-023-03197-z. It aims to combine an SR base with the extensions required to work with often strongly scattered experimental data, different residual pre- and post-processing options, and additional features required to consider thermodynamic EOS development. Although TiSR is not ready for end users yet, this paper is intended to report on its current state, 
    
[^54]: 移动边缘计算中分离学习的动态编码和解码: 利用信息瓶颈理论

    Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory. (arXiv:2309.02787v1 [cs.LG])

    [http://arxiv.org/abs/2309.02787](http://arxiv.org/abs/2309.02787)

    这项研究提出了一种在移动边缘计算中实现动态编码和解码的分离学习框架，利用信息瓶颈理论实现传输资源消耗和共享潜在表示的平衡，从而提高预测性能。

    

    分离学习是一种保护隐私的分布式学习范式，其中机器学习模型（例如神经网络）被分为两部分（即编码器和解码器）。编码器共享所谓的潜在表示，而不是原始数据，用于模型训练。在移动边缘计算中，可以通过分离学习来训练网络功能（如流量预测），其中编码器位于用户设备（UE），解码器位于边缘网络中。基于数据处理不等式和信息瓶颈（IB）理论，我们提出了一个新的框架和训练机制，以实现传输资源消耗与共享潜在表示的信息性之间的动态平衡，这直接影响预测性能。所提出的训练机制提供了一个编码器-解码器神经网络架构，具有多种复杂性-相关性权衡的模式，实现可调节的性能。适应性可以适应不同的情况。

    Split learning is a privacy-preserving distributed learning paradigm in which an ML model (e.g., a neural network) is split into two parts (i.e., an encoder and a decoder). The encoder shares so-called latent representation, rather than raw data, for model training. In mobile-edge computing, network functions (such as traffic forecasting) can be trained via split learning where an encoder resides in a user equipment (UE) and a decoder resides in the edge network. Based on the data processing inequality and the information bottleneck (IB) theory, we present a new framework and training mechanism to enable a dynamic balancing of the transmission resource consumption with the informativeness of the shared latent representations, which directly impacts the predictive performance. The proposed training mechanism offers an encoder-decoder neural network architecture featuring multiple modes of complexity-relevance tradeoffs, enabling tunable performance. The adaptability can accommodate vary
    
[^55]: 基于语义信息提取和领域特定语言模型的CVE驱动攻击技术预测

    CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model. (arXiv:2309.02785v1 [cs.CR])

    [http://arxiv.org/abs/2309.02785](http://arxiv.org/abs/2309.02785)

    本文介绍了一种基于语义信息提取和领域特定语言模型的CVE驱动攻击技术预测方法，通过分析CVE描述并推断出由CVE利用导致的可能的TTP攻击，有效解决了CVE和TTP之间的差距，为实时漏洞情报分析和对策制定提供了重要洞见。

    

    本文解决了网络安全领域中的一个关键问题：CVE（常见漏洞和暴露）所代表的漏洞信息与导致的网络攻击行为之间的差距。CVE提供了漏洞的洞察力，但常常缺乏ATT＆CK框架中潜在威胁行为（战术、技术和程序）的详细信息。这种差距妨碍了准确的CVE分类和主动的对策措施启动。本文引入了TTPpredictor工具，它利用创新技术分析CVE描述，并推断出由CVE利用导致的可能的TTP攻击。TTPpredictor克服了标记数据有限和CVE与TTP描述之间的语义差异带来的挑战。它首先使用语义角色标注（SRL）技术从非结构化的网络威胁报告中提取威胁行为。这些行为，以及它们的上下文属性，与MITRE的攻击功能类别进行关联。这种自动化技术有效地解决了CVE和TTP之间的连接问题，并提供了在实时漏洞情报分析和对策制定中的重要洞见。

    This paper addresses a critical challenge in cybersecurity: the gap between vulnerability information represented by Common Vulnerabilities and Exposures (CVEs) and the resulting cyberattack actions. CVEs provide insights into vulnerabilities, but often lack details on potential threat actions (tactics, techniques, and procedures, or TTPs) within the ATT&CK framework. This gap hinders accurate CVE categorization and proactive countermeasure initiation. The paper introduces the TTPpredictor tool, which uses innovative techniques to analyze CVE descriptions and infer plausible TTP attacks resulting from CVE exploitation. TTPpredictor overcomes challenges posed by limited labeled data and semantic disparities between CVE and TTP descriptions. It initially extracts threat actions from unstructured cyber threat reports using Semantic Role Labeling (SRL) techniques. These actions, along with their contextual attributes, are correlated with MITRE's attack functionality classes. This automated
    
[^56]: Norm调整：大型语言模型的高性能低比特量化

    Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v1 [cs.LG])

    [http://arxiv.org/abs/2309.02784](http://arxiv.org/abs/2309.02784)

    本文介绍了一种称为“norm tweaking”的技术，通过调整量化的激活分布来实现高精度的低比特量化，以提高大型语言模型的压缩性能。

    

    随着大型语言模型（LLMs）的尺寸不断增大，在保持精度的前提下进行模型压缩已成为部署的关键挑战。虽然一些量化方法，如GPTQ，在实现可接受的4比特权重量化方面取得了进展，但尝试更低位的量化往往导致严重的性能降低。在本文中，我们引入了一种称为“norm tweaking”的技术，它可以作为当前PTQ方法的插件，实现高精度和成本高效。我们的方法受到一项观察的启示，即使调整量化的激活分布以与其浮点对应物匹配，也可以恢复LLMs的准确性。为了实现这一点，我们精心设计了一个调整策略，包括生成校准数据和通道距离约束，以更新归一化层的权重以获得更好的泛化性能。我们在各种数据集上进行了大量实验，使用了几个开源的LLMs。

    As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our me
    
[^57]: 使用视觉变换器提高肺癌的诊断和预后：一项范围审查

    Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review. (arXiv:2309.02783v1 [eess.IV])

    [http://arxiv.org/abs/2309.02783](http://arxiv.org/abs/2309.02783)

    这项范围审查总结了最近基于视觉变换器的人工智能方法在肺癌成像应用方面的发展，重点探讨了如何改善肺癌的诊断和预后，以及相关数据集的贡献。

    

    基于视觉变换器的方法在医学人工智能和癌症成像领域取得了进展，包括肺癌应用。最近，许多研究人员开发了基于视觉变换器的人工智能方法来诊断和预测肺癌。本范围审查旨在确定基于视觉变换器的人工智能方法在肺癌成像应用方面的最新发展。它提供了关于如何将视觉变换器与人工智能和深度学习方法相结合来提高肺癌性能的关键见解。此外，该审查还确定了促进该领域发展的数据集。在检索到的314项研究中，本审查包括了从2020年到2022年发表的34项研究。这些研究中最常见的任务是对肺癌类型进行分类，如肺鳞状细胞癌与肺腺癌，并鉴别良性与恶性肺结节。其他应用包括肺癌的生存预测。

    Vision transformer-based methods are advancing the field of medical artificial intelligence and cancer imaging, including lung cancer applications. Recently, many researchers have developed vision transformer-based AI methods for lung cancer diagnosis and prognosis. This scoping review aims to identify the recent developments on vision transformer-based AI methods for lung cancer imaging applications. It provides key insights into how vision transformers complemented the performance of AI and deep learning methods for lung cancer. Furthermore, the review also identifies the datasets that contributed to advancing the field. Of the 314 retrieved studies, this review included 34 studies published from 2020 to 2022. The most commonly addressed task in these studies was the classification of lung cancer types, such as lung squamous cell carcinoma versus lung adenocarcinoma, and identifying benign versus malignant pulmonary nodules. Other applications included survival prediction of lung can
    
[^58]: 关于异质误差对多保真贝叶斯优化的影响

    On the Effects of Heterogeneous Errors on Multi-fidelity Bayesian Optimization. (arXiv:2309.02771v1 [cs.LG])

    [http://arxiv.org/abs/2309.02771](http://arxiv.org/abs/2309.02771)

    本文研究了异质误差对多保真贝叶斯优化的影响，并提出了解决现有假设不成立时性能下降的方法。

    

    贝叶斯优化（BO）是一种连续优化策略，被广泛应用于包括材料设计在内的各个领域。在实际应用中，通过物理实验或高保真度模拟获取高保真度（HF）数据是BO的主要成本。为了缓解这个瓶颈，采用多保真度（MF）方法，通过查询与HF样本相关的廉价低保真度（LF）数据源，减少抽样成本。然而，现有的多保真度BO（MFBO）方法基于两个假设，这在实际应用中很少成立：（1）LF数据源在全局范围内与HF数据呈良好相关，（2）一个随机过程可以模拟融合数据的噪声。这些假设在LF数据源仅在局部与HF源相关或噪声方差在不同地方变化时，会显著降低MFBO的性能。

    Bayesian optimization (BO) is a sequential optimization strategy that is increasingly employed in a wide range of areas including materials design. In real world applications, acquiring high-fidelity (HF) data through physical experiments or HF simulations is the major cost component of BO. To alleviate this bottleneck, multi-fidelity (MF) methods are used to forgo the sole reliance on the expensive HF data and reduce the sampling costs by querying inexpensive low-fidelity (LF) sources whose data are correlated with HF samples. However, existing multi-fidelity BO (MFBO) methods operate under the following two assumptions that rarely hold in practical applications: (1) LF sources provide data that are well correlated with the HF data on a global scale, and (2) a single random process can model the noise in the fused data. These assumptions dramatically reduce the performance of MFBO when LF sources are only locally correlated with the HF source or when the noise variance varies across t
    
[^59]: 统一图神经网络中的过度平滑和过度压缩：一种物理信息驱动的方法和更多扩展

    Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond. (arXiv:2309.02769v1 [cs.LG])

    [http://arxiv.org/abs/2309.02769](http://arxiv.org/abs/2309.02769)

    本研究提出了一种基于物理信息的方法，通过反转图热方程的时间方向，提高了图神经网络的节点特征清晰度，并引入了多尺度热核滤波函数，增强了GNNs的表达能力。进一步推广为G-MHKG模型，探索了更灵活的滤波条件，以解决过度平滑和过度压缩等计算挑战。这些方法和模型在增强GNNs性能方面具有潜力。

    

    图神经网络（GNNs）已经成为处理图结构数据的领先方法之一。尽管取得了巨大的成功，但是GNNs仍然面临着过度平滑、过度压缩和有限的表达能力等关键计算挑战，这些问题会影响GNNs的性能。本研究从经典和量子物理中常用的时间反演原理得到启发，将图热方程的时间方向进行反转，得到了一类高通滤波函数，可以增强图节点特征的清晰度。基于这个概念，我们引入了基于多尺度热核的图神经网络（MHKG），通过合并多种滤波函数对节点特征的影响来增强其表达能力。为了探索更灵活的滤波条件，我们进一步将MHKG推广到一个称为G-MHKG的模型，并详细展示了每个元素在控制过度平滑、过度压缩和表达能力方面的作用。值得注意的是，我们说明了所有的观测可达一个目标。

    Graph Neural Networks (GNNs) have emerged as one of the leading approaches for machine learning on graph-structured data. Despite their great success, critical computational challenges such as over-smoothing, over-squashing, and limited expressive power continue to impact the performance of GNNs. In this study, inspired from the time-reversal principle commonly utilized in classical and quantum physics, we reverse the time direction of the graph heat equation. The resulted reversing process yields a class of high pass filtering functions that enhance the sharpness of graph node features. Leveraging this concept, we introduce the Multi-Scaled Heat Kernel based GNN (MHKG) by amalgamating diverse filtering functions' effects on node features. To explore more flexible filtering conditions, we further generalize MHKG into a model termed G-MHKG and thoroughly show the roles of each element in controlling over-smoothing, over-squashing and expressive power. Notably, we illustrate that all afo
    
[^60]: 在具有特征和结构缺失的图上进行无监督图补全学习

    Towards Unsupervised Graph Completion Learning on Graphs with Features and Structure Missing. (arXiv:2309.02762v1 [cs.LG])

    [http://arxiv.org/abs/2309.02762](http://arxiv.org/abs/2309.02762)

    提出了一个更通用的图补全学习（GCL）框架来解决节点特征和结构关系缺失的问题。

    

    最近几年，图神经网络（GNN）在各种图分析任务中取得了显著的发展。然而，由于许多不可预测的因素，当收集到的节点特征或结构关系部分缺失时，GNN的优越性能将受到严重损害。最近出现的图补全学习（GCL）已经引起了越来越多的关注，其旨在在特定的监督任务引导下重建缺失的节点特征或结构关系。尽管这些提出的GCL方法取得了很大的成功，但仍然存在以下问题：对标签的依赖，重建的节点特征和结构关系的偏差。此外，现有的GCL的泛化能力在同时部分缺失收集到的节点特征和结构关系时仍面临着巨大的挑战。为了解决上述问题，我们提出了一个更通用的GCL框架，其中包括

    In recent years, graph neural networks (GNN) have achieved significant developments in a variety of graph analytical tasks. Nevertheless, GNN's superior performance will suffer from serious damage when the collected node features or structure relationships are partially missing owning to numerous unpredictable factors. Recently emerged graph completion learning (GCL) has received increasing attention, which aims to reconstruct the missing node features or structure relationships under the guidance of a specifically supervised task. Although these proposed GCL methods have made great success, they still exist the following problems: the reliance on labels, the bias of the reconstructed node features and structure relationships. Besides, the generalization ability of the existing GCL still faces a huge challenge when both collected node features and structure relationships are partially missing at the same time. To solve the above issues, we propose a more general GCL framework with the 
    
[^61]: SWAP:利用次级logits对时间序列进行对抗攻击

    SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series. (arXiv:2309.02752v1 [cs.LG])

    [http://arxiv.org/abs/2309.02752](http://arxiv.org/abs/2309.02752)

    该论文提出了一种针对时间序列的新的对抗攻击方法SWAP，通过提高次级logits的置信度，同时最小化对其他logits的干扰来实现攻击。实验证明，该方法在ASR上取得了最先进的性能。

    

    时间序列分类（TSC）已成为各个领域中的一个重要任务，在TSC任务中，深度神经模型显示出优越的性能。然而，这些模型容易受到对抗攻击的影响，微小的扰动可以显著影响预测结果。现有的对抗方法经常面临着超参数化或随机logit扰动的问题，从而影响其效果。此外，提高攻击成功率（ASR）通常需要生成更多的噪声，使得攻击更容易被检测出来。为了解决这些问题，我们提出了一种新的针对TSC模型的攻击方法SWAP。SWAP主要关注提高次级logits的置信度，同时最小化对其他logits的干扰。这是通过最小化目标logit分布和预测logit分布之间的Kullback-Leibler散度来实现的。实验结果表明，SWAP取得了最先进的性能，ASR达到了...

    Time series classification (TSC) has emerged as a critical task in various domains, and deep neural models have shown superior performance in TSC tasks. However, these models are vulnerable to adversarial attacks, where subtle perturbations can significantly impact the prediction results. Existing adversarial methods often suffer from over-parameterization or random logit perturbation, hindering their effectiveness. Additionally, increasing the attack success rate (ASR) typically involves generating more noise, making the attack more easily detectable. To address these limitations, we propose SWAP, a novel attacking method for TSC models. SWAP focuses on enhancing the confidence of the second-ranked logits while minimizing the manipulation of other logits. This is achieved by minimizing the Kullback-Leibler divergence between the target logit distribution and the predictive logit distribution. Experimental results demonstrate that SWAP achieves state-of-the-art performance, with an ASR
    
[^62]: 希伯来语侮辱性语料库及BERT模型的检测

    Offensive Hebrew Corpus and Detection using BERT. (arXiv:2309.02724v1 [cs.CL])

    [http://arxiv.org/abs/2309.02724](http://arxiv.org/abs/2309.02724)

    本研究提出了一个新的希伯来语侮辱性语料库，并使用两个希伯来语BERT模型（HeBERT和AlephBERT）进行了微调。我们观察到，我们的数据结合D_OLaH可以提高HeBERT模型的性能2%。此外，我们的数据对AlephBERT模型也具有一定的泛化性能。

    

    侮辱性语言检测在许多语言中已经有很多研究，但在低资源语言(如希伯来语)中仍有所滞后。本文介绍了一个新的希伯来语侮辱性语料库，从Twitter上收集了15881条推文。每条推文都由阿拉伯-希伯来双语人士标记为五个类别(辱骂、仇恨、暴力、色情或非侮辱性)。标注过程具有挑战性，因为每个标注者都需要熟悉以色列的文化、政治和实践，以理解每条推文的背景。我们使用提出的数据集和另一个已发布的数据集对两个希伯来语BERT模型(HeBERT和AlephBERT)进行微调。我们观察到，我们的数据与D_OLaH结合后，提高了HeBERT模型2%的性能。在我们的数据上对AlephBERT进行微调并在D_OLaH上进行测试，准确率达到69%，而在D_OLaH上进行微调并在我们的数据上进行测试时，准确率为57%，这可能表明我们的数据具有一定的泛化性能。

    Offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as Hebrew. In this paper, we present a new offensive language corpus in Hebrew. A total of 15,881 tweets were retrieved from Twitter. Each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by Arabic-Hebrew bilingual speakers. The annotation process was challenging as each annotator is expected to be familiar with the Israeli culture, politics, and practices to understand the context of each tweet. We fine-tuned two Hebrew BERT models, HeBERT and AlephBERT, using our proposed dataset and another published dataset. We observed that our data boosts HeBERT performance by 2% when combined with D_OLaH. Fine-tuning AlephBERT on our data and testing on D_OLaH yields 69% accuracy, while fine-tuning on D_OLaH and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offer
    
[^63]: 揭示深度学习的前沿：塑造多个领域的创新

    Unveiling the frontiers of deep learning: innovations shaping diverse domains. (arXiv:2309.02712v1 [cs.LG])

    [http://arxiv.org/abs/2309.02712](http://arxiv.org/abs/2309.02712)

    本文广泛研究了深度学习在各个主要研究领域中的潜在应用，揭示了其准确性和计算能力的优势，以及相关的挑战。

    

    深度学习（DL）使得开发能够学习、可视化、优化、改进和预测数据的计算机模型成为可能。近年来，DL已经应用于多个领域，包括音频-视觉数据处理、农业、交通预测、自然语言、生物医学、灾害管理、生物信息学、药物设计、基因组学、人脸识别和生态学。为了探索深度学习的当前状态，有必要研究深度学习在这些学科中的最新发展和应用。然而，文献中缺乏对深度学习在所有潜在领域中的应用的探索。因此，本文广泛调查了深度学习在所有主要研究领域中的潜在应用，以及相关的优势和挑战。正如文献所证明的那样，DL在预测和分析方面表现出准确性，使其成为一种强大的计算工具，并具有表达能力。

    Deep learning (DL) enables the development of computer models that are capable of learning, visualizing, optimizing, refining, and predicting data. In recent years, DL has been applied in a range of fields, including audio-visual data processing, agriculture, transportation prediction, natural language, biomedicine, disaster management, bioinformatics, drug design, genomics, face recognition, and ecology. To explore the current state of deep learning, it is necessary to investigate the latest developments and applications of deep learning in these disciplines. However, the literature is lacking in exploring the applications of deep learning in all potential sectors. This paper thus extensively investigates the potential applications of deep learning across all major fields of study as well as the associated benefits and challenges. As evidenced in the literature, DL exhibits accuracy in prediction and analysis, makes it a powerful computational tool, and has the ability to articulate i
    
[^64]: 解决不完全对称性：一种新的对称学习的演员-评论者扩展

    Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension. (arXiv:2309.02711v1 [cs.LG])

    [http://arxiv.org/abs/2309.02711](http://arxiv.org/abs/2309.02711)

    本研究提出了自适应对称学习（ASL）方法，通过模型最小化的方法，在学习过程中自适应地解决不完全或不精确的对称描述。ASL包括对称拟合组件和模块化损失函数，能高效适应对称性任务。

    

    对称性是理解我们的环境的基本概念，但往往从数学的角度过于简化了现实。人类是个很好的例子，外貌和认知偏见（例如有一只占主导地位的手）都不完美地偏离了对称性。尽管如此，我们的大脑很容易克服这些不完美并高效地适应对称性任务。本研究的驱动动机在于通过强化学习捕捉这种能力。为此，我们引入了自适应对称学习（ASL）-一种模型最小化的演员-评论者扩展，通过在学习过程中自适应地解决不完全或不精确的对称描述。ASL包括一个对称拟合组件和一个模块化损失函数，它在所有状态中强制实施共同的对称关系，并适应了所学策略。将ASL的性能与现有的对称增强方法在一个涉及四足蚂蚁模型的案例研究中进行了比较。

    Symmetry, a fundamental concept to understand our environment, often oversimplifies reality from a mathematical perspective. Humans are a prime example, deviating from perfect symmetry in terms of appearance and cognitive biases (e.g. having a dominant hand). Nevertheless, our brain can easily overcome these imperfections and efficiently adapt to symmetrical tasks. The driving motivation behind this work lies in capturing this ability through reinforcement learning. To this end, we introduce Adaptive Symmetry Learning (ASL) $\unicode{x2013}$ a model-minimization actor-critic extension that addresses incomplete or inexact symmetry descriptions by adapting itself during the learning process. ASL consists of a symmetry fitting component and a modular loss function that enforces a common symmetric relation across all states while adapting to the learned policy. The performance of ASL is compared to existing symmetry-enhanced methods in a case study involving a four-legged ant model for mul
    
[^65]: 改进的鲁棒性$k$-means初始化方法

    Improved Outlier Robust Seeding for k-means. (arXiv:2309.02710v1 [cs.LG])

    [http://arxiv.org/abs/2309.02710](http://arxiv.org/abs/2309.02710)

    本论文提出了一种改进的$k$-means初始化方法，使其在存在异常值的情况下更加鲁棒。算法在复杂度低的同时提供了给定数据的有效聚类。

    

    $k$-means是一种流行的聚类目标函数，但它对异常值非常敏感。现有的$k$-means++初始化方法使用$D^2$采样并具有$O(\log k)$的近似保证。然而，在存在对抗性噪声或异常值的情况下，$D^2$采样更有可能选择远离集群的异常值作为初始聚类中心，因此其关于$k$-means在内部数据上的近似保证不再成立。我们假设异常值构成给定数据的一个常数分数，并提出了一种在$D^2$采样分布中使其鲁棒性更好的简单改进。我们的算法在$O(ndk)$时间内运行，输出$O(k)$个聚类，比最优异常值个数多丢弃一些数据点，并且具有可证明的$O(1)$近似保证。我们的算法还可以修改为输出恰好$k$个聚类而不是$O(k)$个聚类。

    The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\log k)$ approximation guarantee \cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \textit{w.r.t.} $k$-means solution on inliers, does not hold.  Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.  Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, whil
    
[^66]: 证明LLM对抗敌对提示的安全性

    Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])

    [http://arxiv.org/abs/2309.02705](http://arxiv.org/abs/2309.02705)

    本研究提出了首个具有可验证安全保证的框架——消除和检查，用于对抗敌对提示。通过逐个消除标记并使用安全过滤器检查生成的子序列，确保任何敌对修改的有害输入提示都能被正确标识为有害。

    

    为了确保语言模型的输出安全，公开使用的大型语言模型（LLM）引入了所谓的“模型对齐”防护措施。一个对齐的语言模型应该拒绝用户的请求生成有害内容。然而，这种安全措施容易受到敌对提示的攻击，敌对提示包含恶意设计的标记序列，以规避模型的安全防护并导致生成有害内容。在这项工作中，我们介绍了可验证安全保证的第一个对抗敌对提示的框架——消除和检查。我们逐个消除标记，并使用安全过滤器检查生成的子序列。如果安全过滤器检测到任何子序列或输入提示有害，我们的过程将将输入提示标记为有害。这保证了对于某个特定大小的有害输入提示的任何敌对修改也将被标记为有害。我们对抗三种攻击模式：i)敌对后缀，即附加敌对序列…

    Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
    
[^67]: Diffusion-EDFs: 基于SE(3)的等变去噪生成建模在视觉机器人操作中的应用

    Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])

    [http://arxiv.org/abs/2309.02685](http://arxiv.org/abs/2309.02685)

    本文提出了Diffusion-EDFs，一种在视觉机器人操作中应用的基于SE(3)的等变去噪生成建模方法。通过集成SE(3)等变性，我们的方法展示了出色的数据效率和泛化能力。

    

    最近的研究已经验证了等变方法可以显著提高机器人学习中的数据效率、泛化能力和鲁棒性。与此同时，去噪扩散生成建模最近作为一种具有随机行为的机器人操作学习的有前途的方法引起了极大关注。本文提出了Diffusion-EDFs，一种将空间旋转平移等变性即SE(3)等变性引入扩散生成建模的新方法。通过将SE(3)等变性集成到我们的模型架构中，我们展示了我们提出的方法具有明显的数据效率，在进行端到端训练时只需5到10个任务演示即可。此外，与之前基于扩散的操作方法相比，我们的方法展示了更好的泛化能力。

    Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
    
[^68]: RLSynC: 离线-在线强化学习用于合成方法的合成物补全

    RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])

    [http://arxiv.org/abs/2309.02671](http://arxiv.org/abs/2309.02671)

    RLSynC是一种离线-在线强化学习方法，用于半模板化逆向合成中的合成物补全。它使用多个代理同时完成合成物的补全，并通过正向合成模型评估反应物的合成能力来指导行动搜索。

    

    逆向合成是确定能够反应形成所需产物的一组反应物分子的过程。半模板化逆向合成方法首先预测产物中的反应中心，然后将生成的合成物重新补全成反应物。这些方法能够提供必要的可解释性和高实用性，以指导合成规划。我们开发了一种新的离线-在线强化学习方法RLSynC，用于半模板化方法中的合成物补全。RLSynC为每个合成物分配一个代理，所有代理都通过同步进行逐步行动，完成合成物的补全。RLSynC通过同时进行离线训练和在线交互来学习策略，从而可以探索新的反应空间。RLSynC使用正向合成模型来评估预测的反应物在合成产物时的可能性，从而指导行动搜索。

    Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
    
[^69]: 使用离线约束深度强化学习的营销预算分配

    Marketing Budget Allocation with Offline Constrained Deep Reinforcement Learning. (arXiv:2309.02669v1 [cs.LG])

    [http://arxiv.org/abs/2309.02669](http://arxiv.org/abs/2309.02669)

    我们提出了一种基于混合策略的离线值强化学习方法，有效解决了在线营销活动中的预算分配问题。该方法实现了近乎最优的策略效率，并保证收敛到最优策略。

    

    我们研究了在利用先前收集的离线数据的在线营销活动中的预算分配问题。我们首先讨论了在离线环境中优化营销预算分配决策的长期效应。为了克服这一挑战，我们提出了一种新颖的基于混合策略的博弈论离线价值强化学习方法。该方法将以前的方法中存储无数策略减少到只有恒定的策略，实现了近乎最优的策略效率，使其对工业使用实际和有利。我们进一步展示了该方法保证收敛到最优策略，这是以前的基于价值的强化学习方法对于营销预算分配无法实现的。我们在一个规模庞大的营销活动中进行了实验，涉及数千万用户和超过十亿的预算，验证了理论结果并表明所提出的方法优于各种基线方法。

    We study the budget allocation problem in online marketing campaigns that utilize previously collected offline data. We first discuss the long-term effect of optimizing marketing budget allocation decisions in the offline setting. To overcome the challenge, we propose a novel game-theoretic offline value-based reinforcement learning method using mixed policies. The proposed method reduces the need to store infinitely many policies in previous methods to only constantly many policies, which achieves nearly optimal policy efficiency, making it practical and favorable for industrial usage. We further show that this method is guaranteed to converge to the optimal policy, which cannot be achieved by previous value-based reinforcement learning methods for marketing budget allocation. Our experiments on a large-scale marketing campaign with tens-of-millions users and more than one billion budget verify the theoretical results and show that the proposed method outperforms various baseline meth
    
[^70]: 对比学习作为核近似

    Contrastive Learning as Kernel Approximation. (arXiv:2309.02651v1 [cs.LG])

    [http://arxiv.org/abs/2309.02651](http://arxiv.org/abs/2309.02651)

    本论文总结了目前对比学习作为核近似的理论理解，通过在大型未标注数据集上将高维输入转化为低维特征表示，可以实现在有限标注数据上达到高准确性的监督学习。

    

    在标准的监督机器学习中，需要为数据中的每个输入提供标签。虽然在许多应用领域，原始数据很容易获得，但手动标注这些数据的成本太高。为了解决这个问题，对比学习方法在大型未标注数据集上产生高维输入的低维向量表示（也称为特征）。这是通过使用对比损失函数进行训练来实现的，该函数强制要求在特征空间中，相似的输入具有较高的内积，而不相似的输入具有较低的内积。与逐个注释每个输入不同，只需定义一种采样相似和不相似输入对的方法即可。然后，对比特征可以作为输入提供给有限标注数据上的监督学习系统，以在感兴趣的最终任务上获得高的准确性。

    In standard supervised machine learning, it is necessary to provide a label for every input in the data. While raw data in many application domains is easily obtainable on the Internet, manual labelling of this data is prohibitively expensive. To circumvent this issue, contrastive learning methods produce low-dimensional vector representations (also called features) of high-dimensional inputs on large unlabelled datasets. This is done by training with a contrastive loss function, which enforces that similar inputs have high inner product and dissimilar inputs have low inner product in the feature space. Rather than annotating each input individually, it suffices to define a means of sampling pairs of similar and dissimilar inputs. Contrastive features can then be fed as inputs to supervised learning systems on much smaller labelled datasets to obtain high accuracy on end tasks of interest.  The goal of this thesis is to provide an overview of the current theoretical understanding of co
    
[^71]: TFBEST: 具有可学习位置编码的双重方面Transformer用于故障预测

    TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction. (arXiv:2309.02641v1 [cs.LG])

    [http://arxiv.org/abs/2309.02641](http://arxiv.org/abs/2309.02641)

    本文提出了一种新颖的双重方面Transformer模型，用于故障预测，有效处理长序列日志和提供预测值的置信区间。

    

    数据中心中的硬盘故障是昂贵的 - 从灾难性的数据丢失到商业信誉的问题，利益相关者希望像瘟疫一样避免它。积极监测硬盘故障的重要工具是及时估计剩余寿命（RUL）。为此，硬盘驱动器内部使用的自我监测、分析和报告技术（S.M.A.R.T.）为这些重要数据存储设备的长期维护提供了关键的日志。过去的数据驱动预测模型经常使用这些S.M.A.R.T.日志和基于CNN/RNN的架构。然而，它们在提供预测的RUL值的置信区间以及处理非常长的日志序列方面遇到了重大困难。此外，一些使用LSTM等方法的研究在训练速度上很慢，并且需要繁琐的特征工程开销。为了克服这些挑战，在这项工作中，我们提出了一种新颖的Transformer模型，其中包括了学习位置编码的双重方面。

    Hard Disk Drive (HDD) failures in datacenters are costly - from catastrophic data loss to a question of goodwill, stakeholders want to avoid it like the plague. An important tool in proactively monitoring against HDD failure is timely estimation of the Remaining Useful Life (RUL). To this end, the Self-Monitoring, Analysis and Reporting Technology employed within HDDs (S.M.A.R.T.) provide critical logs for long-term maintenance of the security and dependability of these essential data storage devices. Data-driven predictive models in the past have used these S.M.A.R.T. logs and CNN/RNN based architectures heavily. However, they have suffered significantly in providing a confidence interval around the predicted RUL values as well as in processing very long sequences of logs. In addition, some of these approaches, such as those based on LSTMs, are inherently slow to train and have tedious feature engineering overheads. To overcome these challenges, in this work we propose a novel transfo
    
[^72]: Epi-Curriculum: 用于神经机器翻译中低资源领域自适应的分集课程学习

    Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation. (arXiv:2309.02640v1 [cs.LG])

    [http://arxiv.org/abs/2309.02640](http://arxiv.org/abs/2309.02640)

    Epi-Curriculum是一种用于神经机器翻译中低资源领域自适应的方法，通过分集训练和去噪的课程学习，提高了模型对领域变化的鲁棒性和适应性。

    

    神经机器翻译（NMT）模型非常成功，但在限定数量的数据上进行新领域的翻译时，其性能仍然较差。本文提出了一种新颖的方法Epi-Curriculum，用于解决低资源领域自适应（DA），它包含一个新的分集训练框架和去噪的课程学习。我们的分集训练框架通过周期性地将编码器/解码器暴露给经验不足的解码器/编码器，增强了模型对领域变化的鲁棒性。去噪的课程学习通过逐步引导学习过程从简单到更复杂的任务，进一步提高了模型的适应性。在英德和英罗马尼亚翻译方向上的实验证明：（i）Epi-Curriculum在已见和未见领域中提高了模型的鲁棒性和适应性；（ii）我们的分集训练框架增强了编码器和解码器对领域变化的鲁棒性。

    Neural Machine Translation (NMT) models have become successful, but their performance remains poor when translating on new domains with a limited number of data. In this paper, we present a novel approach Epi-Curriculum to address low-resource domain adaptation (DA), which contains a new episodic training framework along with denoised curriculum learning. Our episodic training framework enhances the model's robustness to domain shift by episodically exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised curriculum learning filters the noised data and further improves the model's adaptability by gradually guiding the learning process from easy to more difficult tasks. Experiments on English-German and English-Romanian translation show that: (i) Epi-Curriculum improves both model's robustness and adaptability in seen and unseen domains; (ii) Our episodic training framework enhances the encoder and decoder's robustness to domain shift.
    
[^73]: 多类别对网络校准的置信度和确定性进行对齐

    Multiclass Alignment of Confidence and Certainty for Network Calibration. (arXiv:2309.02636v1 [cs.CV])

    [http://arxiv.org/abs/2309.02636](http://arxiv.org/abs/2309.02636)

    提出了一种新的训练时校准方法，通过对预测均值置信度和确定性进行对齐，从而改进了深度神经网络在多分类问题上的模型校准性能。

    

    深度神经网络（DNN）在几个具有挑战性的领域中取得了巨大进步。最近的研究表明，它们容易做出过于自信的预测。这极大地降低了模型预测的整体可信度，尤其是在安全关键应用中。改进模型校准的早期工作采用了后处理技术，这些技术依赖于有限的参数，并且需要一个保留数据集。一些最近的训练时校准方法，在涉及所有模型参数的情况下，可以胜过后处理方法。为此，我们提出了一种新的训练时校准方法，它具有一个简单的、即插即用的辅助损失，称为多类别对预测均值置信度和确定性进行对齐（MACC）。它基于这样一个观察结果：模型的失校准直接与其确定性相关，因此均值置信度和确定性之间的较大差距意味着在分布内和分布外都存在很差的校准。

    Deep neural networks (DNNs) have made great strides in pushing the state-of-the-art in several challenging domains. Recent studies reveal that they are prone to making overconfident predictions. This greatly reduces the overall trust in model predictions, especially in safety-critical applications. Early work in improving model calibration employs post-processing techniques which rely on limited parameters and require a hold-out set. Some recent train-time calibration methods, which involve all model parameters, can outperform the postprocessing methods. To this end, we propose a new train-time calibration method, which features a simple, plug-and-play auxiliary loss known as multi-class alignment of predictive mean confidence and predictive certainty (MACC). It is based on the observation that a model miscalibration is directly related to its predictive certainty, so a higher gap between the mean confidence and certainty amounts to a poor calibration both for in-distribution and out-o
    
[^74]: 从分层的弱偏好反馈中进行深度强化学习

    Deep Reinforcement Learning from Hierarchical Weak Preference Feedback. (arXiv:2309.02632v1 [cs.LG])

    [http://arxiv.org/abs/2309.02632](http://arxiv.org/abs/2309.02632)

    本研究探讨了如何利用分层的弱偏好反馈进行深度强化学习。通过学习奖励函数，与人类偏好非常一致的复杂奖励可以帮助强化学习解决日益困难的问题。

    

    奖励设计是实际强化学习中一个基本但具有挑战性的方面。对于简单任务，研究人员通常手工设计奖励函数，例如使用若干个奖励因子的线性组合。然而，这种奖励工程受到近似偏差的影响，需要大量的调优成本，并且通常无法提供复杂任务所需的细粒度。为了避免这些困难，研究人员开始转向从人类反馈中进行强化学习（RLHF），从轨迹序列对之间的人类偏好中学习奖励函数。通过利用基于偏好的奖励建模，RLHF学习到与人类偏好非常一致的复杂奖励，使得强化学习能够解决日益困难的问题。不幸的是，RLHF的适用性受到获得人类偏好数据的高成本和困难的限制。鉴于这个成本，我们研究了在复杂任务中使用更少人力投入的方式来学习奖励函数。

    Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; sim
    
[^75]: 通过找到显著可分离的最佳高斯簇的分组，实现超聚类

    Superclustering by finding statistically significant separable groups of optimal gaussian clusters. (arXiv:2309.02623v1 [cs.LG])

    [http://arxiv.org/abs/2309.02623](http://arxiv.org/abs/2309.02623)

    本文提出了一种算法，通过找到统计显著可分离的最佳高斯簇的分组，实现超聚类。算法具有三个阶段，包括表示数据集为高斯混合分布-簇、使用马氏距离估计簇之间的距离和大小以及将簇组合为超簇。算法的创新点在于引入了矩阵质量准则，并通过自动选择合适的统计显著性水平来确定最佳超簇数量。

    

    本文提出了一种通过将数据集的最佳高斯簇分组成统计可分离的超簇的算法。算法包括三个阶段：将数据集表示为高斯混合分布-簇，其数量基于BIC准则的最小值确定；使用马氏距离估计簇之间的距离和簇的大小；使用DBSCAN方法将得到的簇组合成超簇，通过找到其超参数（最大距离）在最大数量的超簇情况下，提供引入的矩阵质量准则的最大值。矩阵质量准则对应于所有发现的超簇中具有统计显著分离的超簇所占比例。该算法只有一个超参数-统计显著性水平，并且可以自动为其选择合适的值。

    The paper presents the algorithm for clustering a dataset by grouping the optimal, from the point of view of the BIC criterion, number of Gaussian clusters into the optimal, from the point of view of their statistical separability, superclusters.  The algorithm consists of three stages: representation of the dataset as a mixture of Gaussian distributions - clusters, which number is determined based on the minimum of the BIC criterion; using the Mahalanobis distance, to estimate the distances between the clusters and cluster sizes; combining the resulting clusters into superclusters using the DBSCAN method by finding its hyperparameter (maximum distance) providing maximum value of introduced matrix quality criterion at maximum number of superclusters. The matrix quality criterion corresponds to the proportion of statistically significant separated superclusters among all found superclusters.  The algorithm has only one hyperparameter - statistical significance level, and automatically d
    
[^76]: 压缩视觉Transformer以实现低资源的视觉学习

    Compressing Vision Transformers for Low-Resource Visual Learning. (arXiv:2309.02617v1 [cs.CV])

    [http://arxiv.org/abs/2309.02617](http://arxiv.org/abs/2309.02617)

    本研究旨在通过使用模型压缩技术，将视觉Transformer应用于资源受限的边缘设备，如无人机。这可以在保持准确性的同时减小模型大小和计算需求。

    

    视觉Transformer（ViT）及其变种已经在视觉学习领域取得了巨大的成功，通过关注视觉输入的不同部分和捕捉长距离的空间依赖性，在图像分类、目标检测和语义分割等任务中取得了最先进的准确性。然而，这些模型非常庞大且计算量大。例如，最近提出的ViT-B模型有86M个参数，使得在资源受限的设备上部署变得不切实际。因此，它们在移动和边缘场景中的应用受到限制。在我们的工作中，我们希望通过利用流行的模型压缩技术，如蒸馏、修剪和量化，为将视觉Transformer引入边缘提供一种解决方案。我们选择的应用环境是无人机（UAV），它以电池供电并且内存受限，携带了一块尺寸与NVIDIA Jetson Nano相当的单板计算机，内存为4GB。

    Vision transformer (ViT) and its variants have swept through visual learning leaderboards and offer state-of-the-art accuracy in tasks such as image classification, object detection, and semantic segmentation by attending to different parts of the visual input and capturing long-range spatial dependencies. However, these models are large and computation-heavy. For instance, the recently proposed ViT-B model has 86M parameters making it impractical for deployment on resource-constrained devices. As a result, their deployment on mobile and edge scenarios is limited. In our work, we aim to take a step toward bringing vision transformers to the edge by utilizing popular model compression techniques such as distillation, pruning, and quantization.  Our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM. On the other hand, the UAV requires hig
    
[^77]: 通过多模态提示实现AI辅助的基于生成模型的无训练安全语义通信

    Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts. (arXiv:2309.02616v1 [eess.IV])

    [http://arxiv.org/abs/2309.02616](http://arxiv.org/abs/2309.02616)

    本文提出了一种通过多模态提示实现AI辅助的无训练安全语义通信系统，利用生成模型的强大学习能力实现精确内容解码。

    

    语义通信（SemCom）在实现通信目标的同时减少网络资源消耗方面具有潜力。然而，联合训练语义编码器和解码器以及在网络设备中的部署所带来的计算开销常常被忽视。生成模型人工智能（GAI）的最新进展提供了一种潜在解决方案。GAI模型的强大学习能力表明，在仅利用有限的语义信息（如提示）的情况下，语义解码器可以重构原始消息，而不需要与语义编码器进行联合训练。然而，值得注意的挑战是GAI多样的生成能力引入的不稳定性。这种不稳定性在输出中，如文本生成的图像中表现出来，限制了GAI在需要精确消息恢复的场景（如人脸图像传输）中的直接应用。为了解决上述问题，本文提出了一种基于GAI辅助的多模态提示的SemCom系统，用于精确内容解码。

    Semantic communication (SemCom) holds promise for reducing network resource consumption while achieving the communications goal. However, the computational overheads in jointly training semantic encoders and decoders-and the subsequent deployment in network devices-are overlooked. Recent advances in Generative artificial intelligence (GAI) offer a potential solution. The robust learning abilities of GAI models indicate that semantic decoders can reconstruct source messages using a limited amount of semantic information, e.g., prompts, without joint training with the semantic encoder. A notable challenge, however, is the instability introduced by GAI's diverse generation ability. This instability, evident in outputs like text-generated images, limits the direct application of GAI in scenarios demanding accurate message recovery, such as face image transmission. To solve the above problems, this paper proposes a GAI-aided SemCom system with multi-model prompts for accurate content decodi
    
[^78]: 使用卫星数据与物理模型融合的生成算法初始化火灾预测模型

    Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts. (arXiv:2309.02615v1 [cs.LG])

    [http://arxiv.org/abs/2309.02615](http://arxiv.org/abs/2309.02615)

    本研究开发了一种生成算法，通过卫星数据与物理模型融合，利用火灾到达时间作为火灾历史的简明表示来初始化火灾预测模型。

    

    随着野火活动和其带来的影响增加，人们开始开发高分辨率的火灾行为模型以预测火灾蔓延。最近，在使用卫星检测火灾位置方面取得的进展进一步提供了使用测量数据通过数据同化来改进数值模型预测火灾蔓延的机会。本研究开发了一种从卫星测量中推断火灾历史的方法，为使用物理模型初始化耦合大气-火灾模型提供所需信息。火灾到达时间，即火灾到达给定的空间位置的时间，作为火灾历史的简明表示。本研究使用经WRF-SFIRE模拟训练的条件Wasserstein生成对抗网络（cWGAN）来从卫星活动火灾数据中推断火灾到达时间。cWGAN用于生成可能的火灾到达时间样本。

    Increases in wildfire activity and the resulting impacts have prompted the development of high-resolution wildfire behavior models for forecasting fire spread. Recent progress in using satellites to detect fire locations further provides the opportunity to use measurements to improve fire spread forecasts from numerical models through data assimilation. This work develops a method for inferring the history of a wildfire from satellite measurements, providing the necessary information to initialize coupled atmosphere-wildfire models from a measured wildfire state in a physics-informed approach. The fire arrival time, which is the time the fire reaches a given spatial location, acts as a succinct representation of the history of a wildfire. In this work, a conditional Wasserstein Generative Adversarial Network (cWGAN), trained with WRF-SFIRE simulations, is used to infer the fire arrival time from satellite active fire data. The cWGAN is used to produce samples of likely fire arrival tim
    
[^79]: 利用生成对抗网络为愤怒的小鸟生成稳定的结构

    Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds. (arXiv:2309.02614v1 [cs.LG])

    [http://arxiv.org/abs/2309.02614](http://arxiv.org/abs/2309.02614)

    本文研究了使用生成对抗网络（GANs）为愤怒的小鸟生成复杂且稳定的结构。实验结果表明GANs可以成功应用于生成多样化的愤怒的小鸟结构。

    

    本文研究了使用生成对抗网络（GANs）生成物理基础拼图游戏愤怒的小鸟中稳定结构的适用性。尽管先前对于关卡生成的GANs应用主要局限于基于瓦片的表示，本文探讨了它们在创建由多个较小块组成的稳定结构方面的适用性。这包括了详细的编码/解码过程，将愤怒的小鸟关卡描述转换为适合的基于网格的表示，以及利用最先进的GAN架构和训练方法来生成新的结构设计。结果表明，GANs成功应用于生成各种复杂且稳定的愤怒的小鸟结构。

    This paper investigates the suitability of using Generative Adversarial Networks (GANs) to generate stable structures for the physics-based puzzle game Angry Birds. While previous applications of GANs for level generation have been mostly limited to tile-based representations, this paper explores their suitability for creating stable structures made from multiple smaller blocks. This includes a detailed encoding/decoding process for converting between Angry Birds level descriptions and a suitable grid-based representation, as well as utilizing state-of-the-art GAN architectures and training methods to produce new structure designs. Our results show that GANs can be successfully applied to generate a varied range of complex and stable Angry Birds structures.
    
[^80]: T-SaS: 面向流数据的适应性动态调整

    T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data. (arXiv:2309.02610v1 [cs.LG])

    [http://arxiv.org/abs/2309.02610](http://arxiv.org/abs/2309.02610)

    本文提出了一个名为T-SaS的贝叶斯框架，用于解决在流数据中出现突然分布转变的问题。这个框架能够有效地划分不同制度并捕捉到变化的模式。

    

    在许多现实场景中，流数据在时间步骤中存在分布转变。许多复杂的序列数据可以有效地划分为展示持续动态的不同制度。发现流数据中的转变行为和演化模式对于理解动态系统非常重要。现有方法通常训练一个鲁棒的模型来处理不同分布的演化数据，或者通过明确给定的制度边界对模型进行顺序适应。然而，存在两个挑战：（1）数据流中的转变可能会突然而急剧地发生，没有前兆。分布转变的边界通常是无法得到的，以及（2）为所有领域训练一个共享模型可能无法捕捉到变化的模式。本文旨在解决在没有任何前兆的情况下出现突然分布转变的过程数据建模问题。具体而言，我们设计了一个名为的贝叶斯框架，名为T-SaS。

    In many real-world scenarios, distribution shifts exist in the streaming data across time steps. Many complex sequential data can be effectively divided into distinct regimes that exhibit persistent dynamics. Discovering the shifted behaviors and the evolving patterns underlying the streaming data are important to understand the dynamic system. Existing methods typically train one robust model to work for the evolving data of distinct distributions or sequentially adapt the model utilizing explicitly given regime boundaries. However, there are two challenges: (1) shifts in data streams could happen drastically and abruptly without precursors. Boundaries of distribution shifts are usually unavailable, and (2) training a shared model for all domains could fail to capture varying patterns. This paper aims to solve the problem of sequential data modeling in the presence of sudden distribution shifts that occur without any precursors. Specifically, we design a Bayesian framework, dubbed as 
    
[^81]: 分布式变分推断用于在线监督学习

    Distributed Variational Inference for Online Supervised Learning. (arXiv:2309.02606v1 [cs.LG])

    [http://arxiv.org/abs/2309.02606](http://arxiv.org/abs/2309.02606)

    本文提出了一种适用于智能传感器网络的分布式变分推断算法，可以解决连续变量、大规模实时数据和难以处理的后验概率的推断问题。通过推导出可分离的较低下界，实现了在传感器网络中进行一跳通信的分布式变分推断，并提出了处理二进制问题的方法。

    

    在智能传感器网络中开发高效的推断问题解决方案对于下一代定位、跟踪和地图服务至关重要。本文提出了一种适用于连续变量、难以处理的后验概率和大规模实时数据的可扩展分布式概率推断算法。在集中式设置中，变分推断是一种执行近似贝叶斯估计的基本技术，其中将难以处理的后验密度用参数化密度来近似。我们的主要贡献在于推导出一个可分离的较低下界，用于集中式估计目标，从而实现了在传感器网络中进行一跳通信的分布式变分推断。我们的分布式证据较低下界 (DELBO) 包括观测似然和距离先验密度的差值的加权和，其与测量证据的差距是由于共识和建模误差造成的。为了解决二进制问题，我们提出了一种处理方法

    Developing efficient solutions for inference problems in intelligent sensor networks is crucial for the next generation of location, tracking, and mapping services. This paper develops a scalable distributed probabilistic inference algorithm that applies to continuous variables, intractable posteriors and large-scale real-time data in sensor networks. In a centralized setting, variational inference is a fundamental technique for performing approximate Bayesian estimation, in which an intractable posterior density is approximated with a parametric density. Our key contribution lies in the derivation of a separable lower bound on the centralized estimation objective, which enables distributed variational inference with one-hop communication in a sensor network. Our distributed evidence lower bound (DELBO) consists of a weighted sum of observation likelihood and divergence to prior densities, and its gap to the measurement evidence is due to consensus and modeling errors. To solve binary 
    
[^82]: 在门诊挂号处使用TriNet进行肺炎和尿路感染筛查

    Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet. (arXiv:2309.02604v1 [cs.LG])

    [http://arxiv.org/abs/2309.02604](http://arxiv.org/abs/2309.02604)

    TriNet是一种用于医疗指导的机器学习模型，可以在挂号处自动筛查出需要下游测试进行诊断确认的病症。这些模型优于当前的临床基准，表明机器学习医疗指导可以提供无成本、非侵入性的筛查。

    

    由于人口结构和寿命的稳定增长，北美的急诊科就诊人数不断增加。随着更多患者就诊，传统的临床工作流程变得负荷过重和低效，导致等待时间延长和医疗质量降低。其中一种工作流程是挂号医疗指导，受限于有限的人力工作量、不准确的诊断和侵入性的过度测试。为了解决这个问题，我们提出了TriNet：一种用于医疗指导的机器学习模型，可以在挂号处自动筛查出需要下游测试进行诊断确认的病症。为了验证筛查潜力，TriNet在医院挂号数据上进行了训练，并在检测肺炎（0.86）和尿路感染（0.93）方面取得了高的阳性预测值。这些模型优于当前的临床基准，表明机器学习医疗指导可以提供无成本、非侵入性的筛查。

    Due to the steady rise in population demographics and longevity, emergency department visits are increasing across North America. As more patients visit the emergency department, traditional clinical workflows become overloaded and inefficient, leading to prolonged wait-times and reduced healthcare quality. One of such workflows is the triage medical directive, impeded by limited human workload, inaccurate diagnoses and invasive over-testing. To address this issue, we propose TriNet: a machine learning model for medical directives that automates first-line screening at triage for conditions requiring downstream testing for diagnosis confirmation. To verify screening potential, TriNet was trained on hospital triage data and achieved high positive predictive values in detecting pneumonia (0.86) and urinary tract infection (0.93). These models outperform current clinical benchmarks, indicating that machine-learning medical directives can offer cost-free, non-invasive screening with high s
    
[^83]: 自监督预训练提高了多项肺部超声解读任务的性能和推理效率

    Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks. (arXiv:2309.02596v1 [cs.CV])

    [http://arxiv.org/abs/2309.02596](http://arxiv.org/abs/2309.02596)

    本研究调查了自监督预训练在肺部超声分析中的应用。预训练模型在多个任务上微调时表现出改善的性能和推理效率，而且还能在使用少量标签进行训练时超过完全监督模型。

    

    在这项研究中，我们调查了自监督预训练是否能够产生一个适用于B型肺部超声分析中的多个分类任务的神经网络特征提取器。在三个肺部超声任务上微调时，经过预训练的模型在局部和外部测试集上的平均曲线下面积（AUC）分别提高了0.032和0.061。在单一预训练模型输出的特征上训练的紧凑非线性分类器并没有在所有任务上提高性能；然而，它们将推理时间与分开微调模型的串行执行相比减少了49%。当使用可用标签的1％进行训练时，预训练模型始终优于完全监督模型，在观察分类任务中测试AUC的最大增加为0.396。总体而言，结果表明自监督预训练对于产生肺部超声分类任务的初始权重是有用的。

    In this study, we investigated whether self-supervised pretraining could produce a neural network feature extractor applicable to multiple classification tasks in B-mode lung ultrasound analysis. When fine-tuning on three lung ultrasound tasks, pretrained models resulted in an improvement of the average across-task area under the receiver operating curve (AUC) by 0.032 and 0.061 on local and external test sets respectively. Compact nonlinear classifiers trained on features outputted by a single pretrained model did not improve performance across all tasks; however, they did reduce inference time by 49% compared to serial execution of separate fine-tuned models. When training using 1% of the available labels, pretrained models consistently outperformed fully supervised models, with a maximum observed test AUC increase of 0.396 for the task of view classification. Overall, the results indicate that self-supervised pretraining is useful for producing initial weights for lung ultrasound cl
    
[^84]: 缩放自回归多模态模型: 预训练和指令调整

    Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning. (arXiv:2309.02591v1 [cs.LG])

    [http://arxiv.org/abs/2309.02591](http://arxiv.org/abs/2309.02591)

    CM3Leon是一个缩放自回归多模态语言模型，通过预训练和指令调整实现了高质量的文本和图像的生成和填充，达到了文本到图像生成方面的最先进性能，而计算资源开销较小。

    

    我们提出了CM3Leon（发音为"Chameleon"），一个检索增强的基于令牌的解码器多模态语言模型，能够生成和填充文本和图像。CM3Leon使用了CM3多模态架构，同时展示了在更多样化的指令风格数据上的扩展和调整的巨大优势。它是第一个使用从纯文本语言模型中改编的配方进行训练的多模态模型，包括大规模的检索增强预训练阶段和第二个多任务监督微调阶段。它也是一个通用模型，可以进行文本到图像和图像到文本的生成，使我们能够引入自包含的对比解码方法，产生高质量的输出。广泛的实验表明，这个配方对于多模态模型非常有效。CM3Leon在文本到图像的生成方面取得了最先进的性能，训练计算量比类似方法少5倍（零样本MS-COCO FID）

    We present CM3Leon (pronounced "Chameleon"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID o
    
[^85]: 顺序体积设计任务的表示学习

    Representation Learning for Sequential Volumetric Design Tasks. (arXiv:2309.02583v1 [cs.LG])

    [http://arxiv.org/abs/2309.02583](http://arxiv.org/abs/2309.02583)

    本研究提出了一种顺序体积设计任务的表示学习方法，通过利用transformer模型从专家的设计序列中提取有用的表示来提高自动生成体积设计的质量，以及支持设计偏好评估和程序化设计生成。

    

    体积设计，也称为质量设计，是专业建筑设计中的第一步关键性任务，具有顺序性。由于体积设计过程复杂，顺序化设计过程中包含了对设计师有价值的信息。许多努力已经被投入到自动生成合理的体积设计上，但生成的设计解决方案的质量存在差异，并且评估一个设计解决方案要么需要一套过于全面的度量标准，要么需要昂贵的人力专业知识。而之前的方法主要关注学习最终设计，而不是顺序设计任务，我们提出利用专家或高性能设计序列的设计知识，并使用基于transformer的模型提取有用的表示。然后，我们提出利用所学的表示在关键的下游应用中，如设计偏好评估和程序化设计生成。我们开发了prefer

    Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the prefere
    
[^86]: 用深度学习算法揭示难治性癫痫脑网络：一种针对儿童患者单模态神经影像数据的可扩展癫痫预测新方法

    Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients. (arXiv:2309.02580v1 [cs.LG])

    [http://arxiv.org/abs/2309.02580](http://arxiv.org/abs/2309.02580)

    本研究提出了一种新颖而全面的框架，通过评估机器学习算法对儿童患者的脑电图信号进行分析，揭示了难治性癫痫的脑网络，并实现了可扩展的癫痫预测。

    

    癫痫是一种常见的神经系统疾病，全球有5000万人受到影响，美国有120万人受到影响。存在着大量儿童患者患有难治性癫痫，即癫痫发作无法得到控制。癫痫发作可能导致身体伤害、迷失方向、失去意识，以及其他可能妨碍儿童参与日常活动的症状。预测癫痫发作可以帮助家长和医护人员采取预防措施，避免危险情况，并让儿童在面对癫痫的不确定性时减少焦虑和紧张感。本研究提出了一种新颖而全面的框架，通过评估机器学习算法对以脑电图信号为基础的单模态神经影像数据进行癫痫预测。

    Epilepsy is a prevalent neurological disorder affecting 50 million individuals worldwide and 1.2 million Americans. There exist millions of pediatric patients with intractable epilepsy, a condition in which seizures fail to come under control. The occurrence of seizures can result in physical injury, disorientation, unconsciousness, and additional symptoms that could impede children's ability to participate in everyday tasks. Predicting seizures can help parents and healthcare providers take precautions, prevent risky situations, and mentally prepare children to minimize anxiety and nervousness associated with the uncertainty of a seizure. This research proposes a novel and comprehensive framework to predict seizures in pediatric patients by evaluating machine learning algorithms on unimodal neuroimaging data consisting of electroencephalogram signals. The bandpass filtering and independent component analysis proved to be effective in reducing the noise and artifacts from the dataset. 
    
[^87]: 基于解剖学驱动的胸部X射线病理检测

    Anatomy-Driven Pathology Detection on Chest X-rays. (arXiv:2309.02578v1 [cs.CV])

    [http://arxiv.org/abs/2309.02578](http://arxiv.org/abs/2309.02578)

    本研究提出了一种基于解剖学驱动的胸部X射线病理检测方法，通过使用易于标注的解剖区域边界框作为病理的代理，取得了比弱监督方法和完全监督方法更好的性能。

    

    病理检测和准确定位能够实现对医学扫描（如胸部X射线）的自动解释，并提供高度可解释性，以支持放射科医生做出明智的决策。然而，标注病理范围的工作耗时，因此用于此目的的大型公共数据集很少。目前的方法因缺乏边界框监督而在性能上受到限制，因此使用弱监督目标检测从图像级别注释中学习病理的（粗略）定位。因此，我们提出了基于解剖学驱动的病理检测（ADPD），该方法将易于注释的解剖区域边界框作为病理的代理。我们研究了两种训练方法：使用解剖级别病理标签的有监督训练和使用图像级病理标签的多示例学习（MIL）。我们的结果显示，我们的解剖级别训练方法优于弱监督方法和完全监督方法。

    Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supe
    
[^88]: 使用深度神经网络在胸部计算机断层扫描中对肺气肿进行亚型划分

    Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks. (arXiv:2309.02576v1 [eess.IV])

    [http://arxiv.org/abs/2309.02576](http://arxiv.org/abs/2309.02576)

    该论文介绍了一种基于深度学习的方法，在胸部计算机断层扫描中自动化肺气肿亚型划分和严重程度分析。通过在COPDGene研究中的大规模实验中评估，该算法在预测准确性上表现出色，并与肺气肿的可视评分具有良好的一致性。

    

    准确鉴定肺气肿的亚型和严重程度对于慢性阻塞性肺疾病的管理和疾病异质性研究至关重要。传统的肺气肿亚型和严重程度的手动分析既繁琐又主观。为了解决这个难题，我们提出了一种基于深度学习的方法，用于自动化Fleischner学会对肺气肿亚型和严重程度进行可视化评分系统。我们使用COPDGene研究中的9650个受试者进行了算法的训练和评估。我们的算法在预测准确性上达到了52％，优于以前发布的方法的45％的准确性。此外，我们的方法预测得分与可视评分之间的一致性好，而之前的方法只获得了中等一致性。我们的方法采用回归训练策略生成分类标签，同时产生高分辨率的局部激活图以可视化网络的预测结果。

    Accurate identification of emphysema subtypes and severity is crucial for effective management of COPD and the study of disease heterogeneity. Manual analysis of emphysema subtypes and severity is laborious and subjective. To address this challenge, we present a deep learning-based approach for automating the Fleischner Society's visual score system for emphysema subtyping and severity analysis. We trained and evaluated our algorithm using 9650 subjects from the COPDGene study. Our algorithm achieved the predictive accuracy at 52\%, outperforming a previously published method's accuracy of 45\%. In addition, the agreement between the predicted scores of our method and the visual scores was good, where the previous method obtained only moderate agreement. Our approach employs a regression training strategy to generate categorical labels while simultaneously producing high-resolution localized activation maps for visualizing the network predictions. By leveraging these dense activation m
    
[^89]: 线性动态系统的因果结构恢复：基于FFT的方法

    Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach. (arXiv:2309.02571v1 [cs.LG])

    [http://arxiv.org/abs/2309.02571](http://arxiv.org/abs/2309.02571)

    该论文提出了一种基于FFT的方法，用于恢复线性动态系统的因果结构。通过降低计算复杂度，可以有效地从时间序列中获取动态因果效应。

    

    从数据中学习因果效应是科学中一个基础且广泛研究的问题，尤其是当因果关系是静态的时候。然而，在存在跨时间点实体之间依赖的情况下，对动态因果效应的识别较少被探索。与静态情况相比，从时间序列观测中获取动态因果效应的计算复杂度较高。我们展示了对向量自回归 (VAR) 模型恢复因果结构的计算复杂度为 $O(Tn^3N^2)$，其中 $n$ 是节点数，$T$ 是样本数，$N$ 是实体之间的最大时间滞后。我们提出了一种复杂度为 $O(Tn^3\log N)$ 的方法，用于恢复因果结构以获得频域 (FD) 表示的时间序列。由于FFT将所有时间依赖关系积累在每个频率上，可以高效地进行因果推断。

    Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by consider
    
[^90]: 基于扩散的微软365时间序列数据插补

    Diffusion-based Time Series Data Imputation for Microsoft 365. (arXiv:2309.02564v1 [cs.DC])

    [http://arxiv.org/abs/2309.02564](http://arxiv.org/abs/2309.02564)

    本研究提出了一种基于扩散的数据插补方法Diffusion+，通过观察到的数据高效地插补缺失数据，提高了微软365的数据质量，进而改善了下游故障预测任务的性能。

    

    可靠性对于像微软365这样的大规模云系统非常重要。云故障（如磁盘故障、节点故障等）威胁到服务的可靠性，导致在线服务中断和经济损失。现有的工作侧重于预测云故障并在故障发生之前采取积极行动。然而，他们在模型训练和预测中存在数据缺失等数据质量差的问题，这限制了性能。本文通过提出的Diffusion+（一种样本效率高的扩散模型），通过观察到的数据高效地插补缺失数据，以提高数据质量。我们的实验和应用实践表明，我们的模型有助于提高下游故障预测任务的性能。

    Reliability is extremely important for large-scale cloud systems like Microsoft 365. Cloud failures such as disk failure, node failure, etc. threaten service reliability, resulting in online service interruptions and economic loss. Existing works focus on predicting cloud failures and proactively taking action before failures happen. However, they suffer from poor data quality like data missing in model training and prediction, which limits the performance. In this paper, we focus on enhancing data quality through data imputation by the proposed Diffusion+, a sample-efficient diffusion model, to impute the missing data efficiently based on the observed data. Our experiments and application practice show that our model contributes to improving the performance of the downstream failure prediction task.
    
[^91]: 稀疏中心对象分区

    Sparse Partitioning Around Medoids. (arXiv:2309.02557v1 [cs.LG])

    [http://arxiv.org/abs/2309.02557](http://arxiv.org/abs/2309.02557)

    这篇论文介绍了一种稀疏中心对象分区的方法，通过利用稀疏性和不对称情况，该方法可以避免二次运行时间和内存要求，在处理大型问题时具有可扩展性。

    

    中心对象分区（PAM，k-Medoids）是一种常用的聚类技术，适用于任意距离函数或相似度，每个聚类由其最中心的对象（称为medoid或离散中值）代表。在运营研究中，这类问题也被称为设施定位问题（FLP）。FastPAM最近引入了一种针对较大k的加速技术，使其适用于更大的问题，但该方法的运行时间仍为N的平方。在本章中，我们讨论了这个问题的稀疏和不对称变种，例如用于道路网络等图形数据。通过利用稀疏性，我们可以避免二次运行时间和内存要求，并使该方法可扩展到更大的问题，只要我们能够构建一个足够连接的小图形来进行局部优化。此外，我们还考虑了不对称情况，其中medoid的集合与要覆盖的点的集合不相同。

    Partitioning Around Medoids (PAM, k-Medoids) is a popular clustering technique to use with arbitrary distance functions or similarities, where each cluster is represented by its most central object, called the medoid or the discrete median. In operations research, this family of problems is also known as facility location problem (FLP). FastPAM recently introduced a speedup for large k to make it applicable for larger problems, but the method still has a runtime quadratic in N. In this chapter, we discuss a sparse and asymmetric variant of this problem, to be used for example on graph data such as road networks. By exploiting sparsity, we can avoid the quadratic runtime and memory requirements, and make this method scalable to even larger problems, as long as we are able to build a small enough graph of sufficient connectivity to perform local optimization. Furthermore, we consider asymmetric cases, where the set of medoids is not identical to the set of points to be covered (or in the
    
[^92]: 使用加密图像有效微调视觉Transformer的领域自适应

    Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images. (arXiv:2309.02556v1 [cs.CV])

    [http://arxiv.org/abs/2309.02556](http://arxiv.org/abs/2309.02556)

    本文提出了一种使用视觉Transformer(ViT)进行模型微调的领域自适应方法，可解决使用转换图像训练模型导致准确性下降的问题，实验证明该方法在使用加密图像时也能保持模型的准确性。

    

    近年来，使用转换数据训练的深度神经网络(DNN)已被应用于隐私保护学习、访问控制和对抗防御等各种应用。然而，使用转换数据会降低模型的性能。因此，在本文中，我们提出了一种新颖的方法，利用视觉Transformer(ViT)对使用转换图像进行模型微调。所提出的领域自适应方法不会导致模型准确性的降低，并且是在ViT的嵌入结构基础上进行的。在实验中，我们验证了所提出的方法在使用具有CIFAR-10和CIFAR-100数据集的加密图像时防止准确性降低。

    In recent years, deep neural networks (DNNs) trained with transformed data have been applied to various applications such as privacy-preserving learning, access control, and adversarial defenses. However, the use of transformed data decreases the performance of models. Accordingly, in this paper, we propose a novel method for fine-tuning models with transformed images under the use of the vision transformer (ViT). The proposed domain adaptation method does not cause the accuracy degradation of models, and it is carried out on the basis of the embedding structure of ViT. In experiments, we confirmed that the proposed method prevents accuracy degradation even when using encrypted images with the CIFAR-10 and CIFAR-100 datasets.
    
[^93]: 自监督预训练对放射学图像诊断任务的影响调研

    A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images. (arXiv:2309.02555v1 [cs.LG])

    [http://arxiv.org/abs/2309.02555](http://arxiv.org/abs/2309.02555)

    自监督预训练在放射学图像诊断任务中提高了下游任务性能，尤其在无标记样本远远多于有标记样本时。

    

    自监督预训练已被观察到在改进特征表示以进行迁移学习时非常有效，利用大量无标记数据。本综述总结了近期关于其在X光、计算机断层扫描、磁共振和超声成像中的应用的研究，重点是比较自监督预训练和完全监督学习在分类和分割等诊断任务中的表现。最重要的发现是，相对于完全监督，自监督预训练通常能够提高下游任务的性能，尤其是在无标记样本远远多于有标记样本时。根据综合证据，为考虑使用自监督学习的从业者提供了建议。鉴于当前研究中存在的限制，提出了未来研究的方向和做法，如将临床知识与理论上有充分证据支持的自监督学习相结合。

    Self-supervised pretraining has been observed to be effective at improving feature representations for transfer learning, leveraging large amounts of unlabelled data. This review summarizes recent research into its usage in X-ray, computed tomography, magnetic resonance, and ultrasound imaging, concentrating on studies that compare self-supervised pretraining to fully supervised learning for diagnostic tasks such as classification and segmentation. The most pertinent finding is that self-supervised pretraining generally improves downstream task performance compared to full supervision, most prominently when unlabelled examples greatly outnumber labelled examples. Based on the aggregate evidence, recommendations are provided for practitioners considering using self-supervised learning. Motivated by limitations identified in current research, directions and practices for future study are suggested, such as integrating clinical knowledge with theoretically justified self-supervised learni
    
[^94]: 数据聚合用于层次聚类

    Data Aggregation for Hierarchical Clustering. (arXiv:2309.02552v1 [stat.ML])

    [http://arxiv.org/abs/2309.02552](http://arxiv.org/abs/2309.02552)

    本研究介绍了如何利用数据聚合算法BETULA使得资源受限系统上的层次聚类方法HAC变得可行，从而允许对非常大的数据集进行探索性数据分析。

    

    层次凝聚聚类（HAC）可能是最早和最灵活的聚类方法，因为它可以与许多距离、相似度和不同的链接策略一起使用。当数据集形成的聚类数量未知且数据中存在一定的层次结构时，通常使用HAC。大多数HAC算法在全距离矩阵上操作，因此需要二次存储。标准算法的运行时间也是立方级别的，用于生成完整的层次结构。在嵌入式或其他资源受限的系统中，存储和运行时间尤其成问题。在本节中，我们介绍了如何利用BETULA进行数据聚合，它是著名的BIRCH数据聚合算法的数值稳定版本，可使HAC在具有受限资源的系统上可行，只造成较小的聚类质量损失，从而允许对非常大的数据集进行探索性数据分析。

    Hierarchical Agglomerative Clustering (HAC) is likely the earliest and most flexible clustering method, because it can be used with many distances, similarities, and various linkage strategies. It is often used when the number of clusters the data set forms is unknown and some sort of hierarchy in the data is plausible. Most algorithms for HAC operate on a full distance matrix, and therefore require quadratic memory. The standard algorithm also has cubic runtime to produce a full hierarchy. Both memory and runtime are especially problematic in the context of embedded or otherwise very resource-constrained systems. In this section, we present how data aggregation with BETULA, a numerically stable version of the well known BIRCH data aggregation algorithm, can be used to make HAC viable on systems with constrained resources with only small losses on clustering quality, and hence allow exploratory data analysis of very large data sets.
    
[^95]: 持续改进基于阈值的新颖性检测

    Continual Improvement of Threshold-Based Novelty Detection. (arXiv:2309.02551v1 [cs.LG])

    [http://arxiv.org/abs/2309.02551](http://arxiv.org/abs/2309.02551)

    论文提出了一种新的方法，利用线性搜索和离一标准交叉验证自动选择阈值，从而改进了动态环境中神经网络的新颖性检测准确率。

    

    在动态开放的环境中，神经网络在检测未见类别时面临困难。这个问题使得在现实环境中部署持续学习变得复杂，因为代理程序在遇到新类型时并没有明确的通知。一种常见的检测新颖性的方法是基于观察数据点与训练数据的相似度阈值。然而，这些方法通常需要手动指定这些阈值的值（提前），因此无法适应数据的性质。我们提出了一种新的方法，利用线性搜索和离一标准交叉验证自动选择这些阈值。我们证明了这种选择阈值的新方法在MNIST，时尚MNIST和CIFAR-10上提高了总体准确率。

    When evaluated in dynamic, open-world situations, neural networks struggle to detect unseen classes. This issue complicates the deployment of continual learners in realistic environments where agents are not explicitly informed when novel categories are encountered. A common family of techniques for detecting novelty relies on thresholds of similarity between observed data points and the data used for training. However, these methods often require manually specifying (ahead of time) the value of these thresholds, and are therefore incapable of adapting to the nature of the data. We propose a new method for automatically selecting these thresholds utilizing a linear search and leave-one-out cross-validation on the ID classes. We demonstrate that this novel method for selecting thresholds results in improved total accuracy on MNIST, Fashion MNIST, and CIFAR-10.
    
[^96]: 通过图注意力进行结构概念学习，实现多层次重组规划

    Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning. (arXiv:2309.02547v1 [cs.RO])

    [http://arxiv.org/abs/2309.02547](http://arxiv.org/abs/2309.02547)

    本论文提出了一种名为结构概念学习（SCL）的深度学习方法，利用图注意力网络进行多层次物体重组规划。该方法在自动生成的模拟数据集上训练，可以适应具有复杂结构依赖的未知场景，推断出独立的子结构以实现任务并行化，并且在现实世界中具有泛化能力。

    

    机器人的操作任务，如物体重组，在使机器人能够与复杂和任意环境进行交互方面起着至关重要的作用。现有的工作主要集中在单层重组规划上，即使存在多个层次，子结构之间的依赖关系几何上也较简单，如塔式堆叠。我们提出了结构概念学习（SCL），这是一种利用图注意力网络进行多层次物体重组规划的深度学习方法，用直观的结构在自动生成的模拟数据集上训练，适用于具有结构依赖层次的未知场景，具有任意数量的对象和更复杂的结构，推断出独立的子结构，通过多个操纵器实现任务并行化，并且在现实世界中具有泛化能力。我们将我们的方法与一系列经典和基于模型的基准进行比较，以显示我们的方法利用了场景的理解能力。

    Robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. Existing work focuses primarily on single-level rearrangement planning and, even if multiple levels exist, dependency relations among substructures are geometrically simpler, like tower stacking. We propose Structural Concept Learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. It is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. We compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding
    
[^97]: 一种用于电影音频源分离的通用带通神经网络

    A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation. (arXiv:2309.02539v1 [eess.AS])

    [http://arxiv.org/abs/2309.02539](http://arxiv.org/abs/2309.02539)

    本论文提出了一种通用带通神经网络用于电影音频源分离，通过使用心理声学的频率尺度来定义频带并且利用共同编码器结构的信息共享特性提高了分离性能。

    

    电影音频源分离是音频源分离的一个相对较新的子任务，其目标是从混音中提取对话音轨、音乐音轨和特效音轨。在这项工作中，我们开发了一个模型，可以对频率轴的任何完全或过完备的分区进行泛化。基于心理声学的频率尺度用于确定带通的定义，现在具备冗余性以进行更可靠的特征提取。我们提出了一个损失函数，该损失函数基于信噪比和1-范数的稀疏促进属性。我们还利用共同编码器结构的信息共享特性，在训练和推断过程中减少计算复杂性，改善难以泛化的声音类别的分离性能，并在推断时提供灵活性，可轻松分离解码器。我们的最佳模型在Divide and Remaster数据集上取得了最先进的性能。

    Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with perfor
    
[^98]: 经验与预测:一种新型硬度度量的标准化试验

    Experience and Prediction: A Metric of Hardness for a Novel Litmus Test. (arXiv:2309.02534v1 [cs.AI])

    [http://arxiv.org/abs/2309.02534](http://arxiv.org/abs/2309.02534)

    该论文实现了一种自动化系统，通过设计和输出Winograd模式的硬度指数，可以在未来的挑战或WSC CAPTCHA服务中对模式进行区分分类。

    

    在过去的十年里，Winograd Schema Challenge (WSC) 已成为研究界的一个重要方面，作为一种新型的标准化试验。因此，WSC引起了研究兴趣，因为它被视为理解人类行为的手段。在这方面，新技术的发展使得Winograd模式能够在各个领域中使用，例如设计新型的CAPTCHAs形式。早些时候的研究工作已经为人类成年人在WSC上的表现建立了基线，表明并不是所有的模式都是相同的，这意味着它们可能根据人类的感知难度进行分类。在这方面，这种"硬度度量"可以在未来的挑战或WSC CAPTCHA服务中使用，以区分Winograd模式。我们最近的工作表明，通过设计一个能够输出Winograd模式的硬度指数的自动化系统，可以实现这一目标，尽管存在一些限制。

    In the last decade, the Winograd Schema Challenge (WSC) has become a central aspect of the research community as a novel litmus test. Consequently, the WSC has spurred research interest because it can be seen as the means to understand human behavior. In this regard, the development of new techniques has made possible the usage of Winograd schemas in various fields, such as the design of novel forms of CAPTCHAs.  Work from the literature that established a baseline for human adult performance on the WSC has shown that not all schemas are the same, meaning that they could potentially be categorized according to their perceived hardness for humans. In this regard, this \textit{hardness-metric} could be used in future challenges or in the WSC CAPTCHA service to differentiate between Winograd schemas.  Recent work of ours has shown that this could be achieved via the design of an automated system that is able to output the hardness-indexes of Winograd schemas, albeit with limitations regar
    
[^99]: 概率单纯形上的扩散

    Diffusion on the Probability Simplex. (arXiv:2309.02530v1 [cs.LG])

    [http://arxiv.org/abs/2309.02530](http://arxiv.org/abs/2309.02530)

    本文提出了一种在概率单纯形上执行扩散的方法，通过使用softmax函数应用于阿恩斯坦-乌伦贝克过程，可以在处理连续性和离散性对象之间的紧张关系时取得良好效果。这种方法也可以扩展到单位立方体上，从而在有界图像生成方面具有应用前景。

    

    扩散模型通过学习逆转数据分布的逐渐噪声化来创建一个生成模型。然而，连续的噪声化过程与离散数据之间的期望不一致。为了解决连续性和离散性对象之间的紧张关系，我们提出了在概率单纯形上执行扩散的方法。使用概率单纯形自然地创建了一种解释，其中点对应于分类概率分布。我们的方法使用对阿恩斯坦-乌伦贝克过程之间进行softmax函数的应用，这是一个众所周知的随机微分方程。我们发现我们的方法也自然地扩展到包括对单位立方体的扩散，这对于有界图像生成应用具有意义。

    Diffusion models learn to reverse the progressive noising of a data distribution to create a generative model. However, the desired continuous nature of the noising process can be at odds with discrete data. To deal with this tension between continuous and discrete objects, we propose a method of performing diffusion on the probability simplex. Using the probability simplex naturally creates an interpretation where points correspond to categorical probability distributions. Our method uses the softmax function applied to an Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We find that our methodology also naturally extends to include diffusion on the unit cube which has applications for bounded image generation.
    
[^100]: 自适应对抗训练不增加追溯成本

    Adaptive Adversarial Training Does Not Increase Recourse Costs. (arXiv:2309.02528v1 [cs.LG])

    [http://arxiv.org/abs/2309.02528](http://arxiv.org/abs/2309.02528)

    本文研究了自适应对抗训练对算法回溯成本的影响，证明了自适应对抗训练所引起的模型鲁棒性的改进对算法回溯成本几乎没有影响。

    

    最近的研究将对抗攻击和算法回溯方法联系在一起：两者都寻求对输入实例进行最小更改，以改变模型的分类决策。已经证明，传统的对抗训练会增加生成回溯的成本；对抗训练半径越大，回溯成本越高。然而，从算法回溯的角度来看，适当的对抗训练半径一直是未知的。最近的研究工作提出了利用自适应训练半径进行对抗训练的方法，以解决实例间可变对抗脆弱性的问题，在未知攻击半径的领域取得了成功。本文研究了自适应对抗训练对算法回溯成本的影响。我们证明，自适应对抗训练所引起的模型鲁棒性的改进对算法回溯成本几乎没有影响。

    Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model's classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier's susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on alg
    
[^101]: 深度学习模型的CPU和GPU性能分析的比较研究

    Comparative Analysis of CPU and GPU Profiling for Deep Learning Models. (arXiv:2309.02521v1 [cs.DC])

    [http://arxiv.org/abs/2309.02521](http://arxiv.org/abs/2309.02521)

    本文通过比较GPU和CPU在深度学习模型训练中的性能，发现GPU在深度神经网络上具有更低的运行时间，对于较简单的网络，GPU并没有太多显著的改进。

    

    在最近几天，深度学习(DL)和机器学习(ML)应用正在快速增加。大量的数据通过互联网生成，可以通过使用ML和DL算法来得出有意义的结果。硬件资源和开源库使得实现这些算法变得容易。Tensorflow和PyTorch是实现ML项目的领先框架之一。通过使用这些框架，我们可以跟踪在GPU和CPU上执行的操作，以分析资源分配和消耗。本文介绍了使用PyTorch训练深度神经网络时CPU和GPU的时间和内存分配情况。该文研究表明，与CPU相比，GPU在深度神经网络上具有更低的运行时间。对于一个较简单的网络，GPU在CPU上并没有太多显著的改进。

    Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
    
[^102]: 朝着用户引导的可行性措施

    Towards User Guided Actionable Recourse. (arXiv:2309.02517v1 [cs.LG])

    [http://arxiv.org/abs/2309.02517](http://arxiv.org/abs/2309.02517)

    本文致力于利用用户偏好来指导可行行动特征的生成过程，并提出了一种基于梯度的方法来识别用户优选的可行性补救。

    

    机器学习在医疗、银行和刑事司法等关键领域的普及，促使了在ML模型中确保信任和透明度的工具的创建。其中一个工具是为受到负面影响的用户提供可行性补救（AR）。AR描述了对用户可行行动特征的经济效益变化的建议，以帮助他们获得有利结果。现有的提供补救的方法优化了接近性、稀疏性、有效性和基于距离的成本等属性。然而，对于可执行性而言，经常被忽视但却至关重要的要求是考虑用户偏好来指导补救生成过程。在这项工作中，我们尝试通过三种简单形式的软约束来捕捉用户偏好：i）对连续特征进行评分，ii）对特征值进行边界设定和iii）对分类特征进行排序。最后，我们提出了一种基于梯度的方法来确定用户优选的可行性补救（UP-AR）。

    Machine Learning's proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user's actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experi
    
[^103]: 用深度生成模型增强语义通信 - ICASSP特别会议概述

    Enhancing Semantic Communication with Deep Generative Models -- An ICASSP Special Session Overview. (arXiv:2309.02478v1 [cs.LG])

    [http://arxiv.org/abs/2309.02478](http://arxiv.org/abs/2309.02478)

    用深度生成模型增强语义通信，解决从复杂数据中提取语义信息和处理通道干扰的挑战。

    

    语义通信在塑造未来的人工智能驱动通信系统中将发挥重要作用。从机器学习的角度揭示了语义通信面临的挑战，并揭示了深度生成模型如何在处理复杂数据、提取和利用语义信息以及对通道干扰具有鲁棒性方面显著增强语义通信框架。除了建立这个新兴领域，本文还为下一代生成语义通信框架的新研究途径做了规划。

    Semantic communication is poised to play a pivotal role in shaping the landscape of future AI-driven communication systems. Its challenge of extracting semantic information from the original complex content and regenerating semantically consistent data at the receiver, possibly being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper discloses the semantic communication challenges from the machine learning perspective and unveils how deep generative models will significantly enhance semantic communication frameworks in dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions. Alongside establishing this emerging field, this paper charts novel research pathways for the next generative semantic communication frameworks.
    
[^104]: 通过不确定性估计实现优化样本选择及其在深度学习中的应用

    Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning. (arXiv:2309.02476v1 [stat.ML])

    [http://arxiv.org/abs/2309.02476](http://arxiv.org/abs/2309.02476)

    该论文提出了一种理论最优解——COPS（基于不确定性的最优子采样），用于解决深度学习中的核心集选择和主动学习问题，在减少标记数据集成本的同时最小化模型的期望损失。

    

    现代深度学习在很大程度上依赖于大型标记数据集，但在手动标注和计算资源方面往往会带来高昂的成本。为了应对这些挑战，研究人员已经探索了一些信息性子集选择技术，包括核心集选择和主动学习。具体而言，核心集选择涉及到同时采样输入（$\bx$）和输出（$\by$）的数据，而主动学习仅关注输入数据（$\bx$）。在本研究中，我们提出了针对线性softmax回归背景下同时解决核心集选择和主动学习的理论最优解。我们提出的方法，COPS（基于不确定性的最优子采样），旨在最小化基于子采样数据训练的模型的期望损失。与现有的依赖于显式计算逆协方差矩阵的方法不同，这种方法在深度学习场景中不容易应用。COPS利用模型的逻辑回归值来估计采样的...

    Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques, including coreset selection and active learning. Specifically, coreset selection involves sampling data with both input ($\bx$) and output ($\by$), active learning focuses solely on the input data ($\bx$).  In this study, we present a theoretically optimal solution for addressing both coreset selection and active learning within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampl
    
[^105]: 模仿学习综述：算法、最新进展与挑战

    A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges. (arXiv:2309.02473v1 [cs.LG])

    [http://arxiv.org/abs/2309.02473](http://arxiv.org/abs/2309.02473)

    这篇论文综述了模仿学习的算法、最新进展和挑战，指出在复杂和非结构化的环境中，通过模仿专家行为来学习所需行为更具吸引力。

    

    近年来，机器人和人工智能系统的发展令人瞩目。随着这些系统的不断演进，它们越来越被应用于复杂和非结构化的环境中，如自动驾驶、空中机器人和自然语言处理。由于这些环境需要高度的灵活性和适应性，因此手动编程行为或通过奖励函数来定义行为（如强化学习中的做法）变得非常困难。在这样的环境中，通过模仿专家行为来学习更具吸引力。这就是模仿学习的作用 - 通过模仿专家的行为来学习所期望的行为，而这些行为是通过演示提供的。

    In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through reward functions (as done in reinforcement learning (RL)) has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play - a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.  This paper a
    
[^106]: 为识别2型糖尿病患者社交风险增加而开发公平的个性化多社交风险评分（iPsRS）

    Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D). (arXiv:2309.02467v1 [cs.LG])

    [http://arxiv.org/abs/2309.02467](http://arxiv.org/abs/2309.02467)

    本研究开发了一种基于电子健康记录的机器学习分析流程，名为个性化多社交风险评分（iPsRS），用于识别与2型糖尿病患者住院风险相关的未满足社会需求。

    

    背景：种族和少数民族群体以及面临社会不利因素的个体，这些因素往往源于他们的社会健康决定因素（SDoH），承担着2型糖尿病（T2D）及其并发症的不成比例负担。因此，在护理点实施有效的社会风险管理策略至关重要。目标：开发一种基于电子健康记录（EHR）的机器学习（ML）分析流程，以识别与T2D患者住院风险相关的未满足社会需求。方法：我们从佛罗里达大学健康整合数据存储库中的EHR数据（2012年至2022年）中识别出10,192名T2D患者，包括背景SDoH（如社区贫困化）和个体级SDoH（如住房稳定性）。我们开发了一种基于电子健康记录（EHR）的机器学习（ML）分析流程，即个性化多社交风险评分（iPsRS），以识别T2D患者住院风险高的社会风险。

    Background: Racial and ethnic minority groups and individuals facing social disadvantages, which often stem from their social determinants of health (SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its complications. It is therefore crucial to implement effective social risk management strategies at the point of care. Objective: To develop an EHR-based machine learning (ML) analytical pipeline to identify the unmet social needs associated with hospitalization risk in patients with T2D. Methods: We identified 10,192 T2D patients from the EHR data (from 2012 to 2022) from the University of Florida Health Integrated Data Repository, including contextual SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing stability). We developed an electronic health records (EHR)-based machine learning (ML) analytic pipeline, namely individualized polysocial risk score (iPsRS), to identify high social risk associated with hospitalizations in T2D patients, alon
    
[^107]: 面向增材制造的基础AI模型：用于G代码调试、操作和理解的语言模型

    Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension. (arXiv:2309.02465v1 [cs.SE])

    [http://arxiv.org/abs/2309.02465](http://arxiv.org/abs/2309.02465)

    本文针对三维打印的G代码文件提出了六种基础大型语言模型（LLMs），通过评估它们在G代码调试和操作方面的性能，包括错误检测和修正以及几何变换等。结果表明这些模型具有潜力应用于增材制造领域。

    

    三维打印或增材制造是一项革命性的技术，它可以从数字模型中创建物理对象。然而，三维打印的质量和准确性取决于G代码的正确性和效率，G代码是一种低级数控编程语言，指导三维打印机如何移动和挤出材料。调试G代码是一项具有挑战性的任务，它需要对G代码格式和所打印零件的几何形状的语法和语义理解。在本文中，我们对六种最先进的基础大型语言模型（LLMs）进行了首次广泛评估，用于理解和调试三维打印的G代码文件。我们设计了有效的提示，使预训练的LLMs能够理解和操作G代码，并在G代码调试和操作的各个方面，包括检测和修正常见错误以及执行几何变换方面测试了它们的性能。我们对它们的优点进行了分析。

    3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strength
    
[^108]: 通过深度强化学习实现三维圆柱的主动流控制

    Active flow control for three-dimensional cylinders through deep reinforcement learning. (arXiv:2309.02462v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2309.02462](http://arxiv.org/abs/2309.02462)

    本文通过深度强化学习方法，成功实现了利用多个独立控制的合成喷流进行三维圆柱的主动流控制，实现了显著的阻力减小。

    

    本文首次成功展示了通过多个独立控制的零净质量流量合成喷流进行主动流控制的结果。喷流被放置在三维圆柱的跨度上，旨在减小阻力系数。该方法基于将计算流体力学求解器与采用近策略优化算法的智能体耦合的深度强化学习框架。我们实现了一个多智能体强化学习框架，具有多个优势：它利用了局部不变量，使控制适应不同的几何形状，便于迁移学习和智能体的交叉应用，并显著提高了训练速度。在本文中，我们报道了在该深度强化学习控制下，针对三种不同配置的问题实现了显著的阻力减小。

    This paper presents for the first time successful results of active flow control with multiple independently controlled zero-net-mass-flux synthetic jets. The jets are placed on a three-dimensional cylinder along its span with the aim of reducing the drag coefficient. The method is based on a deep-reinforcement-learning framework that couples a computational-fluid-dynamics solver with an agent using the proximal-policy-optimization algorithm. We implement a multi-agent reinforcement-learning framework which offers numerous advantages: it exploits local invariants, makes the control adaptable to different geometries, facilitates transfer learning and cross-application of agents and results in significant training speedup. In this contribution we report significant drag reduction after applying the DRL-based control in three different configurations of the problem.
    
[^109]: 有效的多图神经网络用于加密货币交易网络上的非法账户检测

    Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])

    [http://arxiv.org/abs/2309.02460](http://arxiv.org/abs/2309.02460)

    本文介绍了一种新颖的多图神经网络模型DIAM，用于有效地检测加密货币交易网络上的非法账户。该模型通过自动学习节点表示并保留平行边的内在交易模式，在大型交易网络中取得了良好的效果。

    

    我们研究了在线金融市场中日益重要的加密货币交易网络上的非法账户检测。在加密货币上的非法活动激增导致了普通用户数十亿的损失。现有的解决方案要么依赖于繁琐的特征工程来获得手工特征，要么不能充分利用加密货币交易数据中丰富的语义信息，从而导致亚优化的性能。在本文中，我们将非法账户检测问题定义为带有边属性的有向多图上的分类任务，并提出了DIAM，一种新颖的多图神经网络模型，用于在大型交易网络上有效地检测非法账户。首先，DIAM包含一个Edge2Seq模块，通过同时考虑边属性和有向边序列依赖关系，自动学习有效的节点表示，保留平行边的内在交易模式。然后利用t

    We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
    
[^110]: 走向节俭的无监督医学影像微小异常检测

    Towards frugal unsupervised detection of subtle abnormalities in medical imaging. (arXiv:2309.02458v1 [eess.IV])

    [http://arxiv.org/abs/2309.02458](http://arxiv.org/abs/2309.02458)

    本文研究了无监督医学影像微小异常检测的节俭方法，通过使用概率分布混合模型来代替人工神经网络，实现了在准确度和计算需求之间的最优权衡。

    

    在没有异常标注的情况下，医学影像的异常检测是一个具有挑战性的任务。这个问题可以通过无监督的异常检测方法来解决，该方法可以识别与正常模型不匹配的特征。虽然人工神经网络在无监督异常检测中被广泛使用，但它们通常不能在准确度和计算需求之间实现最优的权衡。作为一种替代方案，我们研究了概率分布混合模型，其在各种数据和任务中的多功能性得到广泛认可，并且不需要过多设计和调整。它们的表达能力使它们成为解释复杂多元参考模型的良好选择。它们的参数数量更小，更容易解释和学习。然而，标准的估计算法（如期望最大化算法）在大数据量下不易扩展。

    Anomaly detection in medical imaging is a challenging task in contexts where abnormalities are not annotated. This problem can be addressed through unsupervised anomaly detection (UAD) methods, which identify features that do not match with a reference model of normal profiles. Artificial neural networks have been extensively used for UAD but they do not generally achieve an optimal trade-o$\hookleftarrow$ between accuracy and computational demand. As an alternative, we investigate mixtures of probability distributions whose versatility has been widely recognized for a variety of data and tasks, while not requiring excessive design e$\hookleftarrow$ort or tuning. Their expressivity makes them good candidates to account for complex multivariate reference models. Their much smaller number of parameters makes them more amenable to interpretation and e cient learning. However, standard estimation procedures, such as the Expectation-Maximization algorithm, do not scale well to large data vo
    
[^111]: 英雄联盟：实时比赛结果预测

    League of Legends: Real-Time Result Prediction. (arXiv:2309.02449v1 [cs.LG])

    [http://arxiv.org/abs/2309.02449](http://arxiv.org/abs/2309.02449)

    本研究利用机器学习技术对《英雄联盟》比赛结果进行实时预测，使用未发布的数据作为预测过程的重要组成部分。在不同阶段，各种模型都取得了令人鼓舞的结果，其中基于LightGBM模型在中期阶段表现最佳，准确率达81.62\%。

    

    本文利用机器学习技术对电子游戏《英雄联盟》(LoL)的比赛结果进行预测的研究。我们旨在探索在考虑到比赛的不同变量和阶段的情况下，预测实时结果的能力，强调使用未发布的数据作为这一过程的基本部分。随着英雄联盟的日益流行和赛事的出现，与游戏相关的赌博也出现了，使得对这一领域的研究更加重要。我们评估了各种模型，结果令人鼓舞。基于LightGBM模型的表现最好，在比赛进行60\%至80\%的阶段，平均准确率达到81.62\%。而逻辑回归和梯度提升模型在比赛早期阶段表现更为有效，具有良好的结果。这项研究对机器学习领域有所贡献。

    This paper presents a study on the prediction of outcomes in matches of the electronic game League of Legends (LoL) using machine learning techniques. With the aim of exploring the ability to predict real-time results, considering different variables and stages of the match, we highlight the use of unpublished data as a fundamental part of this process. With the increasing popularity of LoL and the emergence of tournaments, betting related to the game has also emerged, making the investigation in this area even more relevant. A variety of models were evaluated and the results were encouraging. A model based on LightGBM showed the best performance, achieving an average accuracy of 81.62\% in intermediate stages of the match when the percentage of elapsed time was between 60\% and 80\%. On the other hand, the Logistic Regression and Gradient Boosting models proved to be more effective in early stages of the game, with promising results. This study contributes to the field of machine lear
    
[^112]: 观察局部，全局分类：使用图神经网络识别稀疏矩阵结构

    Observe Locally, Classify Globally: Using GNNs to Identify Sparse Matrix Structure. (arXiv:2309.02442v1 [math.NA])

    [http://arxiv.org/abs/2309.02442](http://arxiv.org/abs/2309.02442)

    使用图神经网络识别稀疏矩阵结构的框架在匹配数据格式和避免读取整个数据集的挑战中取得了成功。

    

    稀疏矩阵计算的性能高度依赖于矩阵格式与要计算的数据的底层结构的匹配。不同的稀疏矩阵格式适用于不同的数据结构。因此，首要挑战是在计算之前识别矩阵结构，以将其与适当的数据格式相匹配。第二个挑战是避免在对数据进行分类之前读取整个数据集。这可以通过识别矩阵结构通过样本及其特征来实现。然而，全局特征可能无法从一个采样集中确定，而必须从局部特征中推断出来。为了应对这些挑战，我们开发了一个使用图卷积网络生成稀疏矩阵结构分类器的框架。该框架还可以通过用户提供的生成器扩展到其他矩阵结构。该方法在一组代表性稀疏矩阵上实现了97%的分类准确率。

    The performance of sparse matrix computation highly depends on the matching of the matrix format with the underlying structure of the data being computed on. Different sparse matrix formats are suitable for different structures of data. Therefore, the first challenge is identifying the matrix structure before the computation to match it with an appropriate data format. The second challenge is to avoid reading the entire dataset before classifying it. This can be done by identifying the matrix structure through samples and their features. Yet, it is possible that global features cannot be determined from a sampling set and must instead be inferred from local features. To address these challenges, we develop a framework that generates sparse matrix structure classifiers using graph convolutional networks. The framework can also be extended to other matrix structures using user-provided generators. The approach achieves 97% classification accuracy on a set of representative sparse matrix 
    
[^113]: 神经群体在中枢神经系统中的信息处理：数据和操作的数学结构

    Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations. (arXiv:2309.02332v1 [q-bio.NC] CROSS LISTED)

    [http://arxiv.org/abs/2309.02332](http://arxiv.org/abs/2309.02332)

    神经群体在中枢神经系统中使用数学结构精确地表示和操作信息，实现了特化、泛化、新奇检测等多种功能。

    

    在哺乳动物中枢神经系统的复杂结构中，神经元形成群体。轴索束通过脉冲列作为媒介在这些群集之间进行通信。然而，这些神经群体的精确编码和操作还有待发现。在我们的分析中，出发点是一个具有可塑性的通用神经元的先进的机械模型。从这个简单的框架中出现了一个深刻的数学构造：通过有限凸锥的代数可以准确地描述信息的表示和操作。此外，这些神经群体不仅仅是被动传输者。它们在这个代数结构中扮演着运算符的角色，反映了低级编程语言的功能。当这些群体互连时，它们具有简洁而强大的代数表达式。这些网络使它们能够实现许多操作，如特化、泛化、新奇检测、维度降低等。

    In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensiona
    
[^114]: 一种利用变分自动编码器进行无监督的离群检测的高效方法

    An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders. (arXiv:2309.02084v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02084](http://arxiv.org/abs/2309.02084)

    本文提出了一种利用变分自动编码器进行无监督的离群检测的高效方法，通过引入误差减少(ER)离群得分来改进普通VAE，在各种数据集上得到了优于基准方法的实验结果。

    

    本文关注深度生成模型(DGMs)在无监督的离群检测中的应用。具体来说，我们专注于使用标准正态分布作为潜在变量的普通变分自动编码器(VAE)。这些模型具有较小的模型大小，可以更快地进行训练和推断，与更复杂的DGMs相比，使它们非常适用于资源有限的应用。我们提出了一种新的离群得分称为误差减少(ER)，专门为普通VAE设计。ER融合了从有损图像输入中重建图像的思想，并考虑了图像的科尔莫戈洛夫复杂性。对各种数据集的实验结果表明，我们的方法优于基准方法。我们的代码可在此处获取：https://github.com/ZJLAB-AMMI/VAE4OOD。

    This paper is concerned with deep generative models (DGMs) for unsupervised out-of-distribution (OOD) detection. In particular, we focus on vanilla Variational Autoencoders (VAE) that use a standard normal prior distribution for the latent variables. These models have a smaller model size, enabling faster training and inference, making them well-suited for resource-limited applications compared to more complex DGMs. We propose a novel OOD score called Error Reduction (ER) specifically designed for vanilla VAE. ER incorporate the idea of reconstructing image inputs from their lossy counterparts and takes into account the Kolmogorov complexity of the images. Experimental results on diverse datasets demonstrate the superiority of our approach over baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/VAE4OOD.
    
[^115]: MvFS: 多视图特征选择用于推荐系统

    MvFS: Multi-view Feature Selection for Recommender System. (arXiv:2309.02064v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2309.02064](http://arxiv.org/abs/2309.02064)

    MvFS提出了一种多视图特征选择方法，通过采用多个子网络测量具有不同特征模式的数据的特征重要性，从而更有效地选择每个实例的信息丰富的特征，并解决了当前方法对频繁出现特征的偏向问题。

    

    特征选择是推荐系统中选择关键特征的技术，近年来受到越来越多的研究关注。最近，自适应特征选择（AdaFS）通过针对每个数据实例自适应地选择特征，考虑到给定特征字段的重要性在数据中可以有显著差异，显示了显著的性能。然而，这种方法在选择过程中仍然存在容易偏向频繁出现的主要特征的限制。为了解决这些问题，我们提出了多视图特征选择（MvFS），它更有效地为每个实例选择信息丰富的特征。最重要的是，MvFS采用了一个多视图网络，由多个子网络组成，每个子网络学习如何测量具有不同特征模式的数据的特征重要性。通过这样做，MvFS缓解了朝向主导模式的偏见问题，并促进了更平衡的特征选择过程。此外，MvFS采用了一种有效的i

    Feature selection, which is a technique to select key features in recommender systems, has received increasing research attention. Recently, Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations in that its selection process could be easily biased to major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective i
    
[^116]: 在无知识环境下，对基于查询的高效攻击机器学习型安卓恶意软件检测方法

    Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting. (arXiv:2309.01866v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2309.01866](http://arxiv.org/abs/2309.01866)

    本论文介绍了一种在无知识环境下高效的基于查询的攻击框架，针对机器学习型安卓恶意软件检测方法。对各种主流方法和杀毒软件进行了广泛评估，结果表明该框架具有强大的攻击效果。

    

    安卓操作系统的广泛应用使得恶意安卓应用成为攻击者的吸引目标。基于机器学习（ML）的安卓恶意软件检测方法在解决这个问题上至关重要；然而，它们对对抗性样本的脆弱性引起了关注。目前对ML-based AMD方法的攻击表现出卓越的性能，但是这些攻击依赖于在实际场景中可能不现实的强假设，例如特征空间、模型参数和训练数据集的知识需求。为了解决这个限制，我们引入了AdvDroidZero，一种对ML-based AMD方法的高效查询式攻击框架，在无知识环境下工作。我们的广泛评估表明，AdvDroidZero对各种主流ML-based AMD方法具有很强的攻击效果，特别是最先进的方法和实际的杀毒软件解决方案。

    The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.
    
[^117]: 基于注意力驱动的多模态融合：增强手语识别和翻译

    Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.01860](http://arxiv.org/abs/2309.01860)

    本文提出了一种注意力驱动的多模态融合机制，通过将光流信息与RGB图像相结合，丰富了连续手语识别和翻译流程中的特征。该方法在手语识别任务中降低了WER 0.9，在翻译任务中提高了测试集上大多数BLEU分数约0.6。

    

    本文中，我们设计了一种机制，用于将多模态信息与现有的连续手语识别和翻译流程相结合。在我们的过程中，我们将光流信息与RGB图像结合，以丰富具有与运动相关信息的特征。该工作通过使用跨模态编码器研究了这种模态包含的可行性。我们使用的插件非常轻量级，并且不需要以端到端的方式为新模态包括一个单独的特征提取器。我们在手语识别和翻译中应用了这些改变，改善了每个任务的结果。我们在RWTH-PHOENIX-2014数据集上评估了性能，用于手语识别，并在RWTH-PHOENIX-2014T数据集上评估了翻译任务。在识别任务上，我们的方法将WER降低了0.9，在翻译任务上，我们的方法将大部分BLEU分数在测试集上提高了约0.6。

    In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
    
[^118]: CONFIDERAI：一种新颖的CONFIRMAL可解释设计评分函数，用于可解释和可靠的人工智能

    CONFIDERAI: a novel CONFormal Interpretable-by-Design score function forExplainable and Reliable Artificial Intelligence. (arXiv:2309.01778v1 [cs.LG])

    [http://arxiv.org/abs/2309.01778](http://arxiv.org/abs/2309.01778)

    提出了一种新的可解释机器学习评分函数CONFIDERAI，它将一致性预测与规则模型相结合，利用规则的预测能力和点的几何位置，在特征空间中定义满足一致性保证的区域。

    

    每天的生活越来越受人工智能的影响，毫无疑问，机器学习算法必须为所有人设计成可靠和值得信赖的。具体来说，如果人工智能系统满足解释性、健壮性、透明性、公平性和隐私性这五个方面，计算机科学家认为它是安全和可信赖的。除了这五个方面，我们提出了第六个基本方面：一致性，即机器学习者对系统行为的概率性保证。在本文中，我们提出了一种方法，通过定义CONFIDERAI，一种基于规则的模型的新评分函数，将一致性预测与可解释的机器学习相结合，利用规则的预测能力和点在规则边界内的几何位置。我们还通过利用控制非一致性的数量的技术来解决在特征空间中定义满足一致性保证的区域的问题。

    Everyday life is increasingly influenced by artificial intelligence, and there is no question that machine learning algorithms must be designed to be reliable and trustworthy for everyone. Specifically, computer scientists consider an artificial intelligence system safe and trustworthy if it fulfills five pillars: explainability, robustness, transparency, fairness, and privacy. In addition to these five, we propose a sixth fundamental aspect: conformity, that is, the probabilistic assurance that the system will behave as the machine learner expects. In this paper, we propose a methodology to link conformal prediction with explainable machine learning by defining CONFIDERAI, a new score function for rule-based models that leverages both rules predictive ability and points geometrical position within rules boundaries. We also address the problem of defining regions in the feature space where conformal guarantees are satisfied by exploiting techniques to control the number of non-conforma
    
[^119]: 零样本多标签分类 COVID-19 CT扫描和非标准化报告的实证分析

    An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports. (arXiv:2309.01740v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2309.01740](http://arxiv.org/abs/2309.01740)

    本研究通过对比性视觉语言学习，在COVID-19 CT扫描和非标准化报告中应用零样本多标签分类，以发现肺栓塞和细微的肺部细节，为医学图像分析领域带来了新的发展机遇。

    

    大流行导致了包括医学检查增加在内的大量非结构化数据，包括放射学报告。尽管与计算机断层扫描（CT）相比，X射线图像的精确度较低，但以往关于 COVID-19 的自动诊断研究主要集中在 X射线图像上。在本研究中，我们利用医院的非结构化数据，利用 CT 扫描提供的细节进行基于对比性视觉语言学习的零样本多标签分类。与人类专家合作，我们调查了多种零样本模型的有效性，以帮助放射科医生检测肺栓塞，并识别诸如地玻璃状浑浊和实变等细微的肺部细节。我们的实证分析提供了目前在医学多模态预训练文献中被忽视的解决这些细粒度任务的可能方案。我们的研究为医学图像分析领域的未来发展带来了希望。

    The pandemic resulted in vast repositories of unstructured data, including radiology reports, due to increased medical examinations. Previous research on automated diagnosis of COVID-19 primarily focuses on X-ray images, despite their lower precision compared to computed tomography (CT) scans. In this work, we leverage unstructured data from a hospital and harness the fine-grained details offered by CT scans to perform zero-shot multi-label classification based on contrastive visual language learning. In collaboration with human experts, we investigate the effectiveness of multiple zero-shot models that aid radiologists in detecting pulmonary embolisms and identifying intricate lung details like ground glass opacities and consolidations. Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature. Our investigation promises future advancements in the medical image analysis co
    
[^120]: 内存高效的具有4位状态的优化器

    Memory Efficient Optimizers with 4-bit States. (arXiv:2309.01507v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01507](http://arxiv.org/abs/2309.01507)

    本论文通过将优化器状态的位宽压缩至4位，实现了内存高效的训练神经网络。通过对一阶和二阶矩的详细经验分析，我们发现当前的块状量化方法无法准确近似复杂的异常值模式。为此，我们使用较小的块大小并同时利用行上和列上的信息进行更好的量化。此外，我们还通过排除零点的线性量化器解决了量化第二阶矩时的零点问题。我们的工作在多个基准测试上进行了评估，结果表明我们的4位优化器具有出色的性能。

    

    优化器状态是训练神经网络时的主要内存消耗来源，限制了在给定内存预算内可训练的最大模型。将优化器状态从32位浮点数压缩到更低的位宽有望减小训练内存占用，而当前最低可达到的位宽为8位。在这项工作中，我们通过详细的经验分析将优化器状态位宽降至4位。具体而言，我们发现矩具有复杂的异常值模式，无法通过当前的块状量化方法准确近似。我们使用较小的块大小，并提出同时利用行上和列上的信息进行更好的量化。我们还发现了量化第二阶矩时的零点问题，并通过排除零点的线性量化器来解决这个问题。我们的4位优化器在包括自然语言理解、机器翻译在内的各种基准测试上进行了评估。

    Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translat
    
[^121]: 不同iable的贝叶斯结构学习中的拓扑排序与保证无环性的约束。

    Topological Ordering in Differentiable Bayesian Structure Learning with Guaranteed Acyclicity Constraint. (arXiv:2309.01392v1 [cs.LG])

    [http://arxiv.org/abs/2309.01392](http://arxiv.org/abs/2309.01392)

    本研究提出了一种在贝叶斯结构学习中严格约束图的无环性的替代方法，通过整合拓扑排序知识，能够减少推理复杂性，并确保生成的图的结构是无环的。实证实验表明，该方法胜过相关的贝叶斯基于得分的方法。

    

    基于得分的结构学习方法因其可扩展性而蓬勃发展。连续松弛是这一进展的关键原因。尽管取得了有希望的结果，但大多数方法仍然在通过最小化定义的得分来确保从潜在空间生成的图是无环的方面遇到困难。还存在另一种基于置换的方法，关注的是有向无环图（DAG）中变量的拓扑排序的搜索，以限制图的搜索空间。在本研究中，我们提出了一种利用拓扑排序知识来严格限制图的无环性的替代方法。我们的方法可以减少推理复杂性，同时确保生成的图的结构是无环的。我们对模拟和真实数据进行的实证实验表明，我们的方法可以胜过相关的贝叶斯基于得分的方法。

    Score-based approaches in the structure learning task are thriving because of their scalability. Continuous relaxation has been the key reason for this advancement. Despite achieving promising outcomes, most of these methods are still struggling to ensure that the graphs generated from the latent space are acyclic by minimizing a defined score. There has also been another trend of permutation-based approaches, which concern the search for the topological ordering of the variables in the directed acyclic graph (DAG) in order to limit the search space of the graph. In this study, we propose an alternative approach for strictly constraining the acyclicty of the graphs with an integration of the knowledge from the topological orderings. Our approach can reduce inference complexity while ensuring the structures of the generated graphs to be acyclic. Our empirical experiments with simulated and real-world data show that our approach can outperform related Bayesian score-based approaches.
    
[^122]: 符号性地整合不同随机张量的张量网络计算 - Python RTNI的第二版本

    Symbolically integrating tensor networks over various random tensors -- the second version of Python RTNI. (arXiv:2309.01167v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2309.01167](http://arxiv.org/abs/2309.01167)

    我们升级了Python RTNI的第二版，可以对不同随机张量进行符号性整合，支持Haar分布的酉矩阵、正交矩阵和正态分布的张量。通过导出TensorNetwork格式的张量网络，可以进行低维计算，并解释了数学原理和张量网络图之间的关系。

    

    我们正在升级RTNI的Python版本，该版本能够符号性地整合Haar分布的酉矩阵上的张量网络。现在，PyRTNI2还可以处理Haar分布的正交矩阵以及实数和复数正态分布的张量。此外，它可以将张量网络以TensorNetwork的格式导出，这样可以使用具体的张量进行进一步计算，即使是低维情况下的计算，其中Weingarten函数与高维情况下的函数不同。教程笔记本可以在GitHub上找到：https://github.com/MotohisaFukuda/PyRTNI2。在本文中，我们解释了程序背后的数学原理，并展示了可以使用它进行的各种张量网络计算。关于前者，我们将上述随机矩阵和张量的逐元素矩阵微积分解释为张量网络图，认为这种观点是自然的，将微积分中的delta函数与张量网络图中的边相关联。

    We are upgrading the Python-version of RTNI, which symbolically integrates tensor networks over the Haar-distributed unitary matrices. Now, PyRTNI2 can treat the Haar-distributed orthogonal matrices and the real and complex normal Gaussian tensors as well. Moreover, it can export tensor networks in the format of TensorNetwork so that one can make further calculations with concrete tensors, even for low dimensions, where the Weingarten functions differ from the ones for high dimensions. The tutorial notebooks are found at GitHub: https://github.com/MotohisaFukuda/PyRTNI2. In this paper, we explain maths behind the program and show what kind of tensor network calculations can be made with it. For the former, we interpret the element-wise moment calculus of the above random matrices and tensors in terms of tensor network diagrams, and argue that the view is natural, relating delta functions in the calculus to edges in tensor network diagrams.
    
[^123]: 可分离哈密顿神经网络

    Separable Hamiltonian Neural Networks. (arXiv:2309.01069v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01069](http://arxiv.org/abs/2309.01069)

    这篇论文介绍了可分离哈密顿神经网络的应用，它通过嵌入可加性分离性来解决高维哈密顿系统中的复杂性问题。

    

    利用离散观测数据建模动力系统是现代科学和工程数据系统面临的挑战之一。 哈密顿系统是一类基本且广泛存在的动力系统。 哈密顿神经网络是最先进的模型，可以在汉密尔顿方程的学习偏差下，从离散观测的向量场中无监督地回归动力系统的哈密顿量。然而，哈密顿动力学通常很复杂，特别是在高维情况下，其中哈密顿系统的状态空间相对于样本数量是很大的。 最近发现的一种缓解状态变量之间复杂性的方法是利用哈密顿系统的可加性分离性，并将该可加性分离性嵌入哈密顿神经网络中。根据物理学驱动的机器学习的术语，我们提出了三种可分离的哈密顿神经网络。这些模型嵌入了可加性分离性。

    The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additi
    
[^124]: DoRA: 基于领域的低资源房地产评估自监督学习框架

    DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal. (arXiv:2309.00855v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.00855](http://arxiv.org/abs/2309.00855)

    DoRA是一种基于领域的低资源房地产评估自监督学习框架，通过学习未标记的房地产数据集合来减少主观性，并且融入领域知识。

    

    连接需求和供应的市场系统已被探索用于开发对房地产进行公正的决策。房地产评估是金融机构中一项高成本的资产估值任务，因为它需要领域专家基于相应的知识和市场的判断来评估估价。现有的自动化估值模型减少了领域专家的主观性，但需要大量的交易用于有效评估，这主要受限于交易的标记工作以及对新开发和农村地区的泛化能力。现有的自监督学习（SSL）用于表格数据学习未标记的房地产集合时，忽视了各种重要特征，并且无法融入领域知识。本文提出了DoRA，一种基于领域的低资源房地产评估自监督学习框架。

    The marketplace system connecting demands and supplies has been explored to develop unbiased decision-making in valuing properties. Real estate appraisal serves as one of the high-cost property valuation tasks for financial institutions since it requires domain experts to appraise the estimation based on the corresponding knowledge and the judgment of the market. Existing automated valuation models reducing the subjectivity of domain experts require a large number of transactions for effective evaluation, which is predominantly limited to not only the labeling efforts of transactions but also the generalizability of new developing and rural areas. To learn representations from unlabeled real estate sets, existing self-supervised learning (SSL) for tabular data neglects various important features, and fails to incorporate domain knowledge. In this paper, we propose DoRA, a Domain-based self-supervised learning framework for low-resource Real estate Appraisal. DoRA is pre-trained with an
    
[^125]: 高维线性回归的解释：空间零值和正则化在电池数据上的影响的演示

    Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data. (arXiv:2309.00564v1 [stat.ML])

    [http://arxiv.org/abs/2309.00564](http://arxiv.org/abs/2309.00564)

    本文研究了高维线性回归在解释方面的挑战，发现空间零值和正则化对回归系数产生重要影响，并提出了一种优化公式来比较回归系数与物理工程知识得到的系数，从而实现解释性的回归结果。

    

    高维线性回归在许多科学领域中非常重要。本文考虑到从化学或生物系统中经常得到的基础平滑潜在过程的离散测量数据。在高维度中解释是具有挑战性的，因为空间零值及其与正则化的相互作用会塑造回归系数。数据的空间零值包含所有满足$\mathbf{Xw}=\mathbf{0}$的系数，从而允许非常不同的系数产生相同的预测。我们开发了一种优化公式来比较回归系数和通过物理工程知识得到的系数，以了解系数差异的哪些部分接近于空间零值。这种空间零值方法在一个合成示例和锂离子电池数据上进行了测试。案例研究表明，如果根据先前的物理知识选择合适的正则化和z-score处理，可以得到可解释的回归结果。

    High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data's nullspace contains all coefficients that satisfy $\mathbf{Xw}=\mathbf{0}$, thus allowing very different coefficients to yield identical predictions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. 
    
[^126]: StratMed：面向低资源药物推荐的相关性分层方法

    StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])

    [http://arxiv.org/abs/2308.16781](http://arxiv.org/abs/2308.16781)

    StratMed是一种面向低资源药物推荐的模型，通过相关性分层机制来解决医疗数据长尾分布不平衡的问题，平衡了药物组合的安全性和准确性。

    

    随着有限医疗资源与日益增长的需求之间的失衡，基于人工智能的临床任务变得至关重要。作为一个子领域，药物推荐旨在将患者的纵向历史与医学知识相结合，帮助医生更安全、更准确地开具药物组合处方。现有方法忽视了医疗数据中固有的长尾分布，缺乏头尾数据之间的平衡表示，导致模型性能次优。为了解决这个挑战，我们引入了StratMed，这是一个结合了创新的相关性分层机制的模型。它通过协调数据长尾分布中的差异，并在药物组合的安全性和准确性之间取得平衡。具体而言，我们首先使用深度学习网络构建预训练方法来获取实体表示。然后，我们设计了一个类似金字塔的数据分层方法，以获得更通用的实体表示。

    With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
    
[^127]: 听取少数群体的声音：基于对比预训练的不平衡类别加密流量分类

    Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training. (arXiv:2308.16453v1 [cs.CR])

    [http://arxiv.org/abs/2308.16453](http://arxiv.org/abs/2308.16453)

    本文提出了一种基于对比预训练的不平衡类别加密流量分类框架PASS，通过重新采样训练数据集并进行对比性预训练，避免了标签偏见和流量均匀性等问题，为加密流量分类提供了稳健的特征表示。

    

    移动互联网在各个方面深刻地改变了现代生活方式。加密流量分类在管理移动互联网中起到了至关重要的作用，特别是在移动应用程序使用加密通信的爆炸性增长的情况下。尽管一些现有的基于学习的加密流量分类方法显示出了有希望的结果，但在真实网络环境中仍然存在三个限制：1）由流量类别不平衡引起的标签偏见，2）由组件共享引起的流量均匀性，以及3）依赖充足标记流量进行训练。没有任何现有的加密流量分类方法能够解决所有这些限制。本文提出了一种新颖的基于预训练的半监督加密流量分类框架，称为PASS。我们的关键见解是重新采样原始的训练数据集，并进行对比性预训练，而不直接使用个体应用程序标签，以避免由于类别不平衡引起的标签偏见问题，同时获得稳健的特征表示来区分重叠的流量。

    Mobile Internet has profoundly reshaped modern lifestyles in various aspects. Encrypted Traffic Classification (ETC) naturally plays a crucial role in managing mobile Internet, especially with the explosive growth of mobile apps using encrypted communication. Despite some existing learning-based ETC methods showing promising results, three-fold limitations still remain in real-world network environments, 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping 
    
[^128]: 测量篡改检测基准

    Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])

    [http://arxiv.org/abs/2308.15605](http://arxiv.org/abs/2308.15605)

    本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。

    

    在训练强大的人工智能系统来执行复杂任务时，提供对优化具有稳健性的训练信号可能是具有挑战性的。一个问题是测量篡改，即人工智能系统操纵多个测量结果，以营造良好结果的假象，而不是实现期望的结果。在这项工作中，我们构建了四个新的基于文本的数据集，用于评估大规模语言模型上的测量篡改检测技术。具体来说，给定一组文本输入和测量结果，旨在确定某个结果是否发生，以及一个能够准确预测测量结果的基础模型，目标是确定所有测量结果都表明结果发生的示例是否确实发生了结果，或者这是由于测量篡改引起的。我们展示了在大多数数据集上优于简单基准的技术，但没有达到最佳性能。我们相信在技术和数据集方面都有很大的改进空间，我们感到兴奋。

    When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
    
[^129]: 冲突感知的主动有限状态机学习

    Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])

    [http://arxiv.org/abs/2308.14781](http://arxiv.org/abs/2308.14781)

    C3AL是一种冲突感知的主动有限状态机学习框架，能够处理观测数据中的冲突，通过将观测树作为学习过程的一等公民并最小化测试次数，具有很好的效果。

    

    主动有限状态机学习算法在处理观测数据中的冲突（同一输入对应不同输出）方面存在困难。这种固有的冲突恢复能力不足，影响了它们在存在噪声或学习中的系统变化场景中的有效应用。我们提出了冲突感知的主动有限状态机学习（C3AL）框架，以在学习过程中处理冲突信息。核心思想是将所谓的观测树视为学习过程的一等公民。尽管这个想法在最近的研究中得到了探索，但我们通过将其与任何现有的学习算法结合，并在面对冲突时最小化对正在学习的系统执行的测试次数，充分发挥了它的作用。我们在大量的基准测试中评估了C3AL，涵盖了30多个不同的真实目标和18,000多个不同的场景。评估结果表明，C3AL是一个合适的替代方法。

    Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
    
[^130]: 规范化扩散模型

    Renormalizing Diffusion Models. (arXiv:2308.12355v1 [hep-th])

    [http://arxiv.org/abs/2308.12355](http://arxiv.org/abs/2308.12355)

    该论文介绍了如何使用扩散模型学习统计学和量子场论的逆规范化群流，为构建用于研究场论的基于机器学习的模型提供了具体框架，并详细说明了这些模型如何定义一类自适应桥接取样器。

    

    我们解释了如何使用扩散模型学习统计学和量子场论的逆规范化群流。扩散模型是一类机器学习模型，通过学习将数据添加噪声的逆过程，生成复杂分布的样本，例如自然图像的分布。非微扰规范化群方案可以自然地写成场空间中的扩散过程。我们将这些观察结果结合到一个具体的框架中，用于构建用于研究场论的基于机器学习的模型，其中模型学习到一个明确指定的规范化群方案的逆过程。我们详细说明了这些模型如何定义格点场论的自适应桥接（或平行淬火）取样器的一类。由于规范化群方案具有物理意义，我们提供了明确比较不同方案的指导方案。

    We explain how to use diffusion models to learn inverse renormalization group flows of statistical and quantum field theories. Diffusion models are a class of machine learning models which have been used to generate samples from complex distributions, such as the distribution of natural images, by learning the inverse process to a diffusion process which adds noise to the data until the distribution of the data is pure noise. Nonperturbative renormalization group schemes can naturally be written as diffusion processes in the space of fields. We combine these observations in a concrete framework for building ML-based models for studying field theories, in which the models learn the inverse process to an explicitly-specified renormalization group scheme. We detail how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory. Because renormalization group schemes have a physical meaning, we provide explicit prescriptions for how to compare r
    
[^131]: DynED: 数据流分类中的动态集成多样化

    DynED: Dynamic Ensemble Diversification in Data Stream Classification. (arXiv:2308.10807v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2308.10807](http://arxiv.org/abs/2308.10807)

    DynED是一种动态集成多样化方法，基于MRR结合了组件的多样性和预测准确性，在数据流环境中实现了更高的准确率。

    

    鉴于数据分布的突变性变化，也称为概念漂移，在数据流环境中实现高准确度是一项具有挑战性的任务。在这种情况下，集合方法被广泛应用于分类，因为它们具有出色的性能。 在集合内部的更大多样性已被证明可以提高预测准确性。尽管集合内组件的多样性很高，但并不是所有组件都像预期的那样对整体性能有所贡献。这需要一种方法来选择展现出高性能和多样性的组件。我们提出了一种基于MMR（最大边际相关性）的新型集合构建和维护方法，在组合集合的过程中动态地结合了组件的多样性和预测准确性。在四个真实和11个合成数据集上的实验结果表明，所提出的方法（DynED）相比于五种最先进的基准方法提供了更高的平均准确率

    Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines
    
[^132]: 一种高斯混合模型和神经网络的高效一次迭代学习算法

    An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network. (arXiv:2308.09444v1 [cs.LG])

    [http://arxiv.org/abs/2308.09444](http://arxiv.org/abs/2308.09444)

    我们提出了一种高斯混合模型的学习算法，具有更好的鲁棒性和简单性，只需要进行1次迭代学习。我们的方法能更好地处理数据不确定性和逆问题，并且有潜力构建能够利用分布随机抽样进行随机变异和变异控制的应用。

    

    我们提出了一种基于我们之前的GMM扩展思想的高斯混合模型（GMM）学习算法。新算法比传统的期望最大化（EM）算法更具鲁棒性和简单性。它还提高了准确性，并且只需要进行1次迭代学习。我们在理论上证明了这种新算法无论参数初始化如何都能保证收敛。我们将我们的GMM扩展方法与神经网络中的经典概率层进行了比较，结果表明我们的方法能更好地克服数据的不确定性和逆问题。最后，我们测试了基于GMM的生成器，显示出了进一步利用分布随机抽样进行随机变异和变异控制的潜力。

    We propose an Gaussian Mixture Model (GMM) learning algorithm, based on our previous work of GMM expansion idea. The new algorithm brings more robustness and simplicity than classic Expectation Maximization (EM) algorithm. It also improves the accuracy and only take 1 iteration for learning. We theoretically proof that this new algorithm is guarantee to converge regardless the parameters initialisation. We compare our GMM expansion method with classic probability layers in neural network leads to demonstrably better capability to overcome data uncertainty and inverse problem. Finally, we test GMM based generator which shows a potential to build further application that able to utilized distribution random sampling for stochastic variation as well as variation control.
    
[^133]: 在图嵌入中基于球面和双曲面拓扑的编码应用于Ising MRF模型：经典和量子拓扑机器学习

    Spherical and Hyperbolic Toric Topology-Based Codes On Graph Embedding for Ising MRF Models: Classical and Quantum Topology Machine Learning. (arXiv:2307.15778v1 [cs.IT])

    [http://arxiv.org/abs/2307.15778](http://arxiv.org/abs/2307.15778)

    本论文介绍了在图嵌入中应用信息几何来描述Ising模型的基态，通过利用球面和双曲面拓扑上的编码，建立了机器学习和纠错编码之间的联系，并通过优化纠错码和发展嵌入方法提出了一种新的编码方法。

    

    本文介绍了将信息几何应用于描述Ising模型的基态的方法。通过利用托里克和球面拓扑上的循环和准循环码的奇偶检验矩阵来实现。该方法建立了机器学习和纠错编码之间的联系，特别是在自同构和准循环码循环矩阵的尺寸方面。这种方法对基于捕获集的嵌入方法的发展具有影响。利用统计物理学和数字几何学来优化纠错码，从而导致这些嵌入和稀疏因子化方法的出现。本文通过演示长距离领域的最新DNN架构（ChordMixer，Mega，Mega-chunk，CDIL，...）与特定类型（Cage-graph，Repeat Accumulate）的区块和卷积LDPC码等价的方式，建立了DNN架构和纠错编码之间的直接联系。

    The paper introduces the application of information geometry to describe the ground states of Ising models. This is achieved by utilizing parity-check matrices of cyclic and quasi-cyclic codes on toric and spherical topologies. The approach establishes a connection between machine learning and error-correcting coding, specifically in terms of automorphism and the size of the circulant of the quasi-cyclic code. This proposed approach has implications for the development of new embedding methods based on trapping sets. Statistical physics and number geometry are utilized to optimize error-correcting codes, leading to these embedding and sparse factorization methods. The paper establishes a direct connection between DNN architecture and error-correcting coding by demonstrating how state-of-the-art DNN architectures (ChordMixer, Mega, Mega-chunk, CDIL, ...) from the long-range arena can be equivalent to specific types (Cage-graph, Repeat Accumulate) of block and convolutional LDPC codes. Q
    
[^134]: GOKU-UI：通过关注力和多射击实现连续时间生成模型的普适推理

    GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models. (arXiv:2307.05735v1 [cs.LG])

    [http://arxiv.org/abs/2307.05735](http://arxiv.org/abs/2307.05735)

    GOKU-UI是一种普适推理的SciML生成模型，通过关注机制和多射击训练策略，在包括随机微分方程在内的各种微分方程中展现出了优异的性能表现。它具有显著的数据效率，并在合成和实证数据集上优于所有基线模型。

    

    科学机器学习（SciML）是一个蓬勃发展的领域，它将领域感知和可解释的模型与通用的机器学习技术相结合。在这项工作中，我们介绍了GOKU-UI，这是SciML生成模型GOKU-nets的一种演进。GOKU-UI扩展了原始模型的范围，将其他类别的微分方程，如随机微分方程（SDEs），融入其中，并通过关注机制和在潜在空间中的新型多射击训练策略实现了分布式的、即无处不在的推理。这些改进使其在重建和预测任务中的性能显著提高，我们通过对模拟和实证数据的评估进行了验证。具体而言，即使训练集的大小缩小了32倍，GOKU-UI在合成数据集上表现出色，超过了所有基线模型，凸显其出色的数据效率。此外，当应用于实证的人脑数据时，同时融合了随机微分方程和注意力机制的GOKU-UI也展现出了出色的性能。

    Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. The GOKU-UI broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e. ubiquitous, inference through attention mechanisms and a novel multiple shooting training strategy in the latent space. These enhancements have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation of simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 32-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stoch
    
[^135]: Fed-CPrompt: 无重复学习的联邦持续学习的对比提示

    Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])

    [http://arxiv.org/abs/2307.04869](http://arxiv.org/abs/2307.04869)

    本文提出了一种名为Fed-CPrompt的方法，用于解决无重复学习的联邦持续学习中的遗忘问题。该方法通过异步提示学习和对比持续损失处理异步任务到达和异构数据分布，并在实验证明其在该领域取得了最先进的性能。

    

    联邦持续学习（FCL）从分布在客户端上的机密数据集中逐步学习任务。本文着重研究无重复学习的FCL，在学习新任务时存在因无法访问历史任务数据而导致严重遗忘的问题。为解决此问题，我们提出了基于提示学习技术的Fed-CPrompt，以一种高效的通信方式获得任务特定的提示。Fed-CPrompt引入了两个关键组件，异步提示学习和对比持续损失，以分别处理FCL中的异步任务到达和异构数据分布。大量实验证明了Fed-CPrompt在实现最先进的无重复学习FCL性能方面的有效性。

    Federated continual learning (FCL) learns incremental tasks over time from confidential datasets distributed across clients. This paper focuses on rehearsal-free FCL, which has severe forgetting issues when learning new tasks due to the lack of access to historical task data. To address this issue, we propose Fed-CPrompt based on prompt learning techniques to obtain task-specific prompts in a communication-efficient way. Fed-CPrompt introduces two key components, asynchronous prompt learning, and contrastive continual loss, to handle asynchronous task arrival and heterogeneous data distributions in FCL, respectively. Extensive experiments demonstrate the effectiveness of Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
    
[^136]: 流形滤波-组合网络

    Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])

    [http://arxiv.org/abs/2307.04056](http://arxiv.org/abs/2307.04056)

    这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。

    

    我们介绍了一类大型流形神经网络(MNNs)，我们称之为流形滤波-组合网络。这个类别包括了Wang、Ruiz和Ribeiro之前的研究中考虑的MNNs，流形散射变换(一种基于小波的神经网络模型)，以及其他有趣的之前在文献中未考虑的示例，如Kipf和Welling的图卷积网络的流形等效。然后，我们考虑了一种基于构建数据驱动图的方法，用于在没有对流形有全局知识的情况下实现这样的网络，而只能访问有限数量的样本点。我们提供了网络在样本点数趋于无穷大时能够保证收敛到其连续极限的充分条件。与之前的工作(主要关注特定的MNN结构和图构建)不同，我们的收敛速度并不依赖于使用的滤波器数量。而且，它表现出线性的收敛速度。

    We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
    
[^137]: 深度学习中的损失函数和度量方法：一项评论

    Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])

    [http://arxiv.org/abs/2307.02694](http://arxiv.org/abs/2307.02694)

    本文回顾了深度学习中最常见的损失函数和性能测量方法，旨在帮助从业者选择最适合其特定任务的方法。

    

    深度学习的一个重要组成部分是选择用于训练和评估模型的损失函数和性能度量。本文回顾了深度学习中最常见的损失函数和性能测量方法。我们探讨了每种技术的优势和局限性，并举例说明它们在各种深度学习问题上的应用。我们的评论旨在全面了解最常见的深度学习任务中使用的不同损失函数和性能指标，并帮助从业者选择最适合其特定任务的方法。

    One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
    
[^138]: 策略优化中的乐观性和适应性

    Optimism and Adaptivity in Policy Optimization. (arXiv:2306.10587v1 [cs.LG])

    [http://arxiv.org/abs/2306.10587](http://arxiv.org/abs/2306.10587)

    本文通过将看似无关的策略优化算法重新构造为共同的两个交错步骤，即乐观策略改进和后见适应，统一了强化学习中的策略优化方法，揭示了加速方法中的乐观性和适应性的共同理论属性。

    

    本文致力于通过“乐观性”和“适应性”在强化学习中加速策略优化方法的统一范式。通过利用策略迭代和策略梯度方法之间的深刻联系，我们将一些看似无关的策略优化算法重新构造为两个交错步骤（i）乐观策略改进操作器使用“梯度上升预测”将先前的策略$\pi_t$映射到一个假设$\pi_{t+1}$，然后（ii）对$\pi_{t+1}$的性能进行部分评估，并基于此进行“后见适应”。我们使用这个共享的视角来共同表达其他众所周知的算法，包括软件和乐观策略迭代、自然演员-评论家方法、基于前向搜索的基于模型的策略改进和元学习算法。通过这样做，我们揭示了关于通过乐观性和适应性加速的共同理论属性。

    We work towards a unifying paradigm for accelerating policy optimization methods in reinforcement learning (RL) through \emph{optimism} \& \emph{adaptivity}. Leveraging the deep connection between policy iteration and policy gradient methods, we recast seemingly unrelated policy optimization algorithms as the repeated application of two interleaving steps (i) an \emph{optimistic policy improvement operator} maps a prior policy $\pi_t$ to a hypothesis $\pi_{t+1}$ using a \emph{gradient ascent prediction}, followed by (ii) a \emph{hindsight adaptation} of the optimistic prediction based on a partial evaluation of the performance of $\pi_{t+1}$. We use this shared lens to jointly express other well-known algorithms, including soft and optimistic policy iteration, natural actor-critic methods, model-based policy improvement based on forward search, and meta-learning algorithms. By doing so, we shed light on collective theoretical properties related to acceleration via optimism \& adaptivit
    
[^139]: 机器学习传感器的数据表

    Datasheets for Machine Learning Sensors. (arXiv:2306.08848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08848](http://arxiv.org/abs/2306.08848)

    本研究提出了一种用于机器学习传感器的标准数据表模板，并讨论了其主要组成部分。这些数据表可以促进对传感器数据在机器学习应用中的理解和利用，并提供了客观的性能评估指标。

    

    机器学习（ML）传感器提供了一种新的感知范式，能够在边缘进行智能化，同时赋予终端用户更多对其数据的控制权。由于这些ML传感器在智能设备的发展中起着至关重要的作用，清晰地记录其规格、功能和限制非常关键。本文介绍了一种用于ML传感器的标准数据表模板，并讨论了其主要组成部分，包括系统的硬件、ML模型和数据集属性、端到端性能指标以及环境影响。我们提供了一个我们自己ML传感器的示例数据表，并详细讨论了每个部分。我们强调这些数据表如何促进对ML应用中传感器数据的更好理解和利用，并提供了客观的衡量系统性能的指标进行评估和比较。ML传感器及其数据表共同提供了更高的隐私、安全性、透明度、可解释性、可审计性和

    Machine learning (ML) sensors offer a new paradigm for sensing that enables intelligence at the edge while empowering end-users with greater control of their data. As these ML sensors play a crucial role in the development of intelligent devices, clear documentation of their specifications, functionalities, and limitations is pivotal. This paper introduces a standard datasheet template for ML sensors and discusses its essential components including: the system's hardware, ML model and dataset attributes, end-to-end performance metrics, and environmental impact. We provide an example datasheet for our own ML sensor and discuss each section in detail. We highlight how these datasheets can facilitate better understanding and utilization of sensor data in ML applications, and we provide objective measures upon which system performance can be evaluated and compared. Together, ML sensors and their datasheets provide greater privacy, security, transparency, explainability, auditability, and u
    
[^140]: 内核随机投影深度用于离群点检测

    Kernel Random Projection Depth for Outlier Detection. (arXiv:2306.07056v1 [stat.ML])

    [http://arxiv.org/abs/2306.07056](http://arxiv.org/abs/2306.07056)

    本文提出了一种内核随机投影深度方法，用于处理数据云中的多模式和非凸性，实验结果表明在基准数据集上表现优异。

    

    本文提出了一种扩展的随机投影深度（RPD）方法，用于处理数据云中的多模式和非凸性。在所提出的方法的框架中，RPD在再现核希尔伯特空间中计算。借助内核主成分分析，我们期望所提出的方法可以处理上述多种模式和非凸性。实验结果表明，所提出的方法优于RPD，并可与基准数据集上现有的检测模型相媲美，关于接收操作特征曲线（ROC）下的曲线下面积（AUC）。

    This paper proposes an extension of Random Projection Depth (RPD) to cope with multiple modalities and non-convexity on data clouds. In the framework of the proposed method, the RPD is computed in a reproducing kernel Hilbert space. With the help of kernel principal component analysis, we expect that the proposed method can cope with the above multiple modalities and non-convexity. The experimental results demonstrate that the proposed method outperforms RPD and is comparable to other existing detection models on benchmark datasets regarding Area Under the Curves (AUCs) of Receiver Operating Characteristic (ROC).
    
[^141]: 高维和置换不变异常检测。

    High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])

    [http://arxiv.org/abs/2306.03933](http://arxiv.org/abs/2306.03933)

    该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。

    

    由于学习高维概率密度的困难，新物理过程的异常检测方法通常局限于低维空间。特别是在成分级别上，将置换不变性和可变长度的输入等良好性质合并到流行的密度估计方法中变得更加困难。在本研究中，我们引入了一种基于扩散模型的粒子物理数据置换不变密度估计器，专门设计用于处理可变长度的输入。我们通过将学习到的密度用作置换不变的异常检测评分来展示我们方法的功效，有效地识别出在仅具备背景假设下的可能性较低的喷注。为了验证我们的密度估计方法，我们研究了学习到的密度比与被监督分类算法获得的密度之间的比较。

    Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
    
[^142]: 癫痫发作检测：解剖与分析。

    Epilepsy Seizure Detection: Anatomy and Analysis. (arXiv:2305.19347v1 [cs.LG])

    [http://arxiv.org/abs/2305.19347](http://arxiv.org/abs/2305.19347)

    该研究提出了一种通用、经济、非侵入性的癫痫检测系统，基于简单的实时kNN机器学习，可在不到四秒的训练时间内定制和适应个人用户，并具有94.5%的平均准确率。

    

    癫痫跟踪系统对于监测和评估癫痫治疗非常重要。目前癫痫护理中使用的护理记录可能会错过癫痫发作。可穿戴的监测设备可能更容易被耐受，并且更适合长期进行。许多技术和方法被提出用于癫痫发作检测；然而，在保持检测精度的同时，简单性和费用都是日常使用的关键概念。在本研究中，我们提出了一种基于简单实时k-最近邻（kNN）机器学习的通用、经济、非侵入性的癫痫检测系统，并可在不到四秒的训练时间内定制和适应个人用户；该系统经过了500个被试验者的验证和验证，抽样频率为178 Hz，其平均准确率为94.5％。

    A seizure tracking system is crucial for monitoring and evaluating epilepsy treatments. Caretaker seizure diaries are used in epilepsy care today, but clinical seizure monitoring may miss seizures. Monitoring devices that can be worn may be better tolerated and more suitable for long-term ambulatory use. Many techniques and methods are proposed for seizure detection; However, simplicity and affordability are key concepts for daily use while preserving the accuracy of the detection. In this study, we propose a versal, affordable noninvasive based on a simple real-time k-Nearest-Neighbors (kNN) machine learning that can be customized and adapted to individual users in less than four (4) seconds of training time; the system was verified and validated using 500 subjects, with seizure detection data sampled at 178 Hz, the operated with a mean accuracy of (94.5%).
    
[^143]: AnoOnly:无需损失正常数据的半监督异常检测

    AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data. (arXiv:2305.18798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18798](http://arxiv.org/abs/2305.18798)

    AnoOnly是一个新的半监督异常检测框架，通过引入一种对正常数据的弱监督形式来解决同质数据对异常的影响，以实现平衡的监督。该框架在各种模型和数据集上表现出了显著的性能提升，达到了新的最佳性能。

    

    半监督异常检测(SSAD)方法通过利用少量但有指导作用的异常实例，增强了无监督异常检测(UAD)的效果。然而，同质正常数据对异常的统治使得SSAD模型无法有效地感知异常。为了解决这个问题并在严重不平衡的正常和异常数据之间实现平衡的监督，我们开发了一个名为AnoOnly(仅异常)的新框架。与现有的SSAD方法不同，AnoOnly暂停了严格的损失监督，引入了一种对正常数据的弱监督形式。这种弱监督通过批量归一化实现，隐式地对正常数据进行聚类学习。当集成到现有的SSAD方法中时，所提出的AnoOnly在各种模型和数据集上展示了显著的性能提升，达到了新的最佳性能。此外，我们的A

    Semi-supervised anomaly detection (SSAD) methods have demonstrated their effectiveness in enhancing unsupervised anomaly detection (UAD) by leveraging few-shot but instructive abnormal instances. However, the dominance of homogeneous normal data over anomalies biases the SSAD models against effectively perceiving anomalies. To address this issue and achieve balanced supervision between heavily imbalanced normal and abnormal data, we develop a novel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methods that resort to strict loss supervision, AnoOnly suspends it and introduces a form of weak supervision for normal data. This weak supervision is instantiated through the utilization of batch normalization, which implicitly performs cluster learning on normal data. When integrated into existing SSAD methods, the proposed AnoOnly demonstrates remarkable performance enhancements across various models and datasets, achieving new state-of-the-art performance. Additionally, our A
    
[^144]: 基于双层学习的最优正则化参数研究

    On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])

    [http://arxiv.org/abs/2305.18394](http://arxiv.org/abs/2305.18394)

    本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。

    

    变分正则化常用于解线性反问题，它通过添加正则化项来提高先验信息质量，并通过正则化参数加以权衡，而合适的正则化参数的选择至关重要。现有的策略例如差异原则和L-曲线可以用于确定合适的参数值，但是近年来，一种叫做双层学习的监督机器学习方法被用于确定最优参数。虽然以前的策略有各种理论结果，但在这种情况下，双层学习的良好性质仍然是一个发展中的领域。本文提出了一个更好的条件来表征确定正则化参数的正值性。

    Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
    
[^145]: 通过等压等温流获得吉布斯自由能

    Gibbs free energies via isobaric-isothermal flows. (arXiv:2305.13233v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.13233](http://arxiv.org/abs/2305.13233)

    采用机器学习模型利用等压等温流得到吉布斯自由能，并在单原子水的结晶中进行了测试，表现出优秀的性能。

    

    我们提出了一种基于归一化流的机器学习模型，该模型经过训练可从等压等温（NPT）集合中进行采样。在我们的方法中，我们采用近似方法来得到完全灵活的三斜晶系统的联合分布和粒子坐标以达到所需的内部压力。我们对单原子水在立方和六角冰相中进行测试，并发现与已建立的基线相比，吉布斯自由能和其他可观测量的结果完全一致。

    We present a machine-learning model based on normalizing flows that is trained to sample from the isobaric-isothermal (NPT) ensemble. In our approach, we approximate the joint distribution of a fully-flexible triclinic simulation box and particle coordinates to achieve a desired internal pressure. We test our model on monatomic water in the cubic and hexagonal ice phases and find excellent agreement of Gibbs free energies and other observables compared with established baselines.
    
[^146]: 无限类别混合策略

    Infinite Class Mixup. (arXiv:2305.10293v1 [cs.CV])

    [http://arxiv.org/abs/2305.10293](http://arxiv.org/abs/2305.10293)

    本文提出了一种直接通过混合分类器而不是标签来增强样本的策略。新的分类器是输入对分类器向量的线性插值，使分类器之间的关系更加准确，从而提高深度网络的分类性能。

    

    Mixup 是一种广泛采用的深度网络训练策略，通过插值输入和标签的训练对来增加额外的样本。 Mixup 已经证明可以提高分类性能、网络校准和超出分布概括。虽然有效，但Mixup的一个基石是网络在类别之间学习线性行为模式，但它只是间接地通过概率级别进行输出插值而强制执行。这篇论文旨在通过直接混合分类器而不是混合每个混合对的标签来解决这个限制。我们建议将每个增强的样本的目标定义为一个唯一的新分类器，其参数是输入对的分类器向量的线性插值。所有可能分类器的空间是连续的，涵盖了分类器对之间的所有插值。为了使优化可行，我们提出了一个双对比无限类混合损失，其中我们对每个样本对向所有其他样本对进行对比。

    Mixup is a widely adopted strategy for training deep networks, where additional samples are augmented by interpolating inputs and labels of training pairs. Mixup has shown to improve classification performance, network calibration, and out-of-distribution generalisation. While effective, a cornerstone of Mixup, namely that networks learn linear behaviour patterns between classes, is only indirectly enforced since the output interpolation is performed at the probability level. This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair. We propose to define the target of each augmented sample as a uniquely new classifier, whose parameters are a linear interpolation of the classifier vectors of the input pair. The space of all possible classifiers is continuous and spans all interpolations between classifier pairs. To make optimisation tractable, we propose a dual-contrastive Infinite Class Mixup loss, where we contrast 
    
[^147]: 基于自编码器的流数据异常检测方法：增量学习和概念漂移适应

    Autoencoder-based Anomaly Detection in Streaming Data with Incremental Learning and Concept Drift Adaptation. (arXiv:2305.08977v1 [cs.LG])

    [http://arxiv.org/abs/2305.08977](http://arxiv.org/abs/2305.08977)

    本文提出了一种基于自编码器的增量学习方法与漂移检测的异常检测方法（strAEm++DD），用于解决无标签的流数据中的异常检测问题，该方法在实验研究中表现出不俗的性能表现并优于现有的基线和先进方法。

    

    在当今数字宇宙中，大量无标签数据以各种方式进行流式生成。在这种情况下，识别异常等罕见事件是一个巨大的挑战，而在非平稳环境中尤其困难，这可能会导致模型预测性能的恶化。为了解决以上挑战，本文提出了一种基于自编码器的增量学习方法与漂移检测的异常检测方法（strAEm++DD）。我们使用真实世界和合成数据集进行了实验研究，并对strAEm++DD进行了经验分析。我们进一步进行了比较研究，证明了所提出的方法明显优于现有的基线和先进方法。

    In our digital universe nowadays, enormous amount of data are produced in a streaming manner in a variety of application areas. These data are often unlabelled. In this case, identifying infrequent events, such as anomalies, poses a great challenge. This problem becomes even more difficult in non-stationary environments, which can cause deterioration of the predictive performance of a model. To address the above challenges, the paper proposes an autoencoder-based incremental learning method with drift detection (strAEm++DD). Our proposed method strAEm++DD leverages on the advantages of both incremental learning and drift detection. We conduct an experimental study using real-world and synthetic datasets with severe or extreme class imbalance, and provide an empirical analysis of strAEm++DD. We further conduct a comparative study, showing that the proposed method significantly outperforms existing baseline and advanced methods.
    
[^148]: 因果结构学习中的未解问题：以英国COVID-19为例研究

    Open problems in causal structure learning: A case study of COVID-19 in the UK. (arXiv:2305.03859v1 [cs.LG])

    [http://arxiv.org/abs/2305.03859](http://arxiv.org/abs/2305.03859)

    本文研究了因果机器学习在COVID-19英国疫情数据中的应用挑战，探讨了不同数据格式对学习类别不同的算法的影响，并突出了因果结构学习中的未解问题和未来研究方向。

    

    因果机器学习算法可以恢复图形结构，从而揭示因果关系。这些算法提供的因果表示使得透明度和可解释性得以实现。然而，与关联性机器学习相比，因果机器学习在实践中的影响有限。本文研究了因果机器学习在COVID-19英国疫情数据中的应用挑战。我们从各种公共来源整合数据，并研究各种结构学习算法从这些数据中学到的内容。我们探讨了不同数据格式对学习类别不同的算法的影响，并评估了每个算法及算法组产生的结果，包括图形结构、模型维度、敏感性分析、混淆变量、预测和干预推断等。我们利用这些结果来突出因果结构学习中的未解问题和未来研究方向。

    Causal machine learning (ML) algorithms recover graphical structures that tell us something about cause-and-effect relationships. The causal representation provided by these algorithms enables transparency and explainability, which is necessary in critical real-world problems. Yet, causal ML has had limited impact in practice compared to associational ML. This paper investigates the challenges of causal ML with application to COVID-19 UK pandemic data. We collate data from various public sources and investigate what the various structure learning algorithms learn from these data. We explore the impact of different data formats on algorithms spanning different classes of learning, and assess the results produced by each algorithm, and groups of algorithms, in terms of graphical structure, model dimensionality, sensitivity analysis, confounding variables, predictive and interventional inference. We use these results to highlight open problems in causal structure learning and directions f
    
[^149]: 数据驱动气象预测的真实观测验证

    Verification against in-situ observations for Data-Driven Weather Prediction. (arXiv:2305.00048v1 [cs.LG])

    [http://arxiv.org/abs/2305.00048](http://arxiv.org/abs/2305.00048)

    数据驱动气象预测模型（DDWP）近年来发展迅速，但需要更加严格的真实观测验证来在操作预报中更安全地使用。

    

    近年来，数据驱动气象预测模型（DDWP）取得了快速发展，展示了高度接近数值天气预报（NWP）模型的能力。快速、准确、低成本的DDWP预报使其在操作预报中的使用成为一个具有吸引力的选择，但是，在真正的操作环境中对DDWPs进行严格的评估仍然需要努力。DDWP通常使用ERA5重新分析数据进行训练和评估，但是DDWP仅在模拟中进行过测试，即使模拟质量很高，也无法完全准确地代表真实世界。在操作预报中安全地使用DDWP需要更加彻底的“真实世界”验证，以及对当前DDWP的训练和评估方式进行仔细的研究。值得问一下，例如，用于训练的重新分析数据集对真实世界的模拟效果如何？这对于气候公正和气象数据的不均匀可用性非常重要。

    Data-driven weather prediction models (DDWPs) have made rapid strides in recent years, demonstrating an ability to approximate Numerical Weather Prediction (NWP) models to a high degree of accuracy. The fast, accurate, and low-cost DDWP forecasts make their use in operational forecasting an attractive proposition, however, there remains work to be done in rigorously evaluating DDWPs in a true operational setting. Typically trained and evaluated using ERA5 reanalysis data, DDWPs have been tested only in a simulation, which cannot represent the real world with complete accuracy even if it is of a very high quality. The safe use of DDWPs in operational forecasting requires more thorough "real-world" verification, as well as a careful examination of how DDWPs are currently trained and evaluated. It is worth asking, for instance, how well do the reanalysis datasets, used for training, simulate the real world? With an eye towards climate justice and the uneven availability of weather data: i
    
[^150]: T细胞受体蛋白序列和稀疏编码：癌症分类的一种新方法。

    T Cell Receptor Protein Sequences and Sparse Coding: A Novel Approach to Cancer Classification. (arXiv:2304.13145v1 [cs.LG])

    [http://arxiv.org/abs/2304.13145](http://arxiv.org/abs/2304.13145)

    本研究探索了利用稀疏编码的方法对具有癌症分类目标的TCR蛋白序列进行多类别分类，为基于TCR的免疫治疗提供理论支持。

    

    癌症是一种以不受控制的细胞生长和增殖为特征的复杂疾病。T细胞受体（TCR）是适应性免疫系统中的重要蛋白质，它们对抗原的特异性识别在免疫反应中发挥着关键作用，包括对抗癌症。TCR的多样性和特异性使它们成为瞄准癌细胞的理想选择，近期测序技术的进步使得TCR库进行了全面的分析，这导致发现了具有强有力的抗癌活性的TCR，并开发了基于TCR的免疫治疗方法。在本研究中，我们探讨了利用稀疏编码对具有癌症分类作为目标标签的TCR蛋白序列进行多类别分类的应用。稀疏编码是一种机器学习中流行的技术，它能够利用一组信息丰富的特征来表示数据，可以捕捉氨基酸之间的复杂关系，识别序列中的微小模式。

    Cancer is a complex disease characterized by uncontrolled cell growth and proliferation. T cell receptors (TCRs) are essential proteins for the adaptive immune system, and their specific recognition of antigens plays a crucial role in the immune response against diseases, including cancer. The diversity and specificity of TCRs make them ideal for targeting cancer cells, and recent advancements in sequencing technologies have enabled the comprehensive profiling of TCR repertoires. This has led to the discovery of TCRs with potent anti-cancer activity and the development of TCR-based immunotherapies. In this study, we investigate the use of sparse coding for the multi-class classification of TCR protein sequences with cancer categories as target labels. Sparse coding is a popular technique in machine learning that enables the representation of data with a set of informative features and can capture complex relationships between amino acids and identify subtle patterns in the sequence tha
    
[^151]: 透明且稳健的基于数据驱动的风力涡轮机功率曲线模型

    Towards transparent and robust data-driven wind turbine power curve models. (arXiv:2304.09835v1 [cs.LG])

    [http://arxiv.org/abs/2304.09835](http://arxiv.org/abs/2304.09835)

    该研究利用可解释的人工智能框架研究了基于数据驱动的风力涡轮机功率曲线模型的稳健性。结果表明，学习到的策略可以更好地指示模型的稳健性。高度复杂的机器学习模型容易学习到物理上不合理的策略。

    

    风力涡轮机功率曲线模型将环境条件转化为涡轮机的功率输出。它们对于能量产出预测和涡轮机性能监测至关重要。近年来，基于数据驱动的机器学习方法已经优于基于参数和物理知识的方法。然而，它们常常被批评为是不透明的“黑匣子”，这引起了人们对于它们在非稳态环境下的稳健性的担忧，例如风力涡轮机所面临的情况。因此，我们引入了可解释的人工智能（XAI）框架，根据运行中的SCADA数据来研究和验证数据驱动的功率曲线模型所学习的策略。它将领域特定的考虑与Shapley值和最新的XAI回归研究结果相结合。我们的结果表明，学习到的策略可以更好地指示模型的稳健性，而不是验证或测试集的误差。此外，我们观察到，高度复杂，最先进的机器学习模型很容易学习到物理上不合理的策略，这一点应引起注意。

    Wind turbine power curve models translate ambient conditions into turbine power output. They are essential for energy yield prediction and turbine performance monitoring. In recent years, data-driven machine learning methods have outperformed parametric, physics-informed approaches. However, they are often criticised for being opaque "black boxes" which raises concerns regarding their robustness in non-stationary environments, such as faced by wind turbines. We, therefore, introduce an explainable artificial intelligence (XAI) framework to investigate and validate strategies learned by data-driven power curve models from operational SCADA data. It combines domain-specific considerations with Shapley Values and the latest findings from XAI for regression. Our results suggest, that learned strategies can be better indicators for model robustness than validation or test set errors. Moreover, we observe that highly complex, state-of-the-art ML models are prone to learn physically implausib
    
[^152]: 一种基于心律失常分类指导的心电图分割模型

    An Arrhythmia Classification-Guided Segmentation Model for Electrocardiogram Delineation. (arXiv:2304.06237v1 [cs.LG])

    [http://arxiv.org/abs/2304.06237](http://arxiv.org/abs/2304.06237)

    本文提出了一种利用深度学习模型结合心律失常分类指导的心电图分割方法，能够准确划分广泛异常节律类型的信号，减少虚报检测。

    

    准确划分ECG中的关键波形是提取相关特征以支持诊断和治疗心脏疾病的关键步骤。虽然利用分割模型的深度学习方法定位P、QRS和T波已经取得了有希望的结果，但它们处理呈现心律失常的信号的能力尚不明确。在本研究中，我们提出了一种新方法，利用深度学习模型准确划分具有广泛心律失常的信号。我们的方法是使用混合损失函数训练分割模型，将分割与心律失常分类任务相结合。此外，我们还使用包含各种心律失常类型的多样化训练集，使我们的模型能够处理广泛的具有挑战性的情况。实验结果表明，我们的模型准确划分了广泛异常节律类型的信号，同时结合分类指导的训练可以有效地减少虚报检测。

    Accurate delineation of key waveforms in an ECG is a critical initial step in extracting relevant features to support the diagnosis and treatment of heart conditions. Although deep learning based methods using a segmentation model to locate P, QRS and T waves have shown promising results, their ability to handle signals exhibiting arrhythmia remains unclear. In this study, we propose a novel approach that leverages a deep learning model to accurately delineate signals with a wide range of arrhythmia. Our approach involves training a segmentation model using a hybrid loss function that combines segmentation with the task of arrhythmia classification. In addition, we use a diverse training set containing various arrhythmia types, enabling our model to handle a wide range of challenging cases. Experimental results show that our model accurately delineates signals with a broad range of abnormal rhythm types, and the combined training with classification guidance can effectively reduce fals
    
[^153]: 使用物理训练的神经网络学习非线性本构材料模型：COMM-PINN。

    Learning solution of nonlinear constitutive material models using physics-informed neural networks: COMM-PINN. (arXiv:2304.06044v1 [cs.CE])

    [http://arxiv.org/abs/2304.06044](http://arxiv.org/abs/2304.06044)

    通过物理训练的神经网络可解决非线性材料行为的本构关系，无需初始数据，避免重复的牛顿迭代。训练好的模型可作为有限元程序的用户定义材料模型，但需要解决诸多挑战。

    

    我们使用物理训练的神经网络来解决非线性、路径相关材料行为的本构关系。训练好的网络不仅满足所有热力学约束，而且在任何给定的加载情况下，立即提供关于当前材料状态（即自由能，应力和内部变量的演变）的信息，而不需要初始数据。这项工作的一个优点是它规避了求解复材料模型中非线性方程所需的重复牛顿迭代。此外，我们提供了减少获取切向算子所需的导数次序的策略。训练好的模型可以直接用作任何有限元程序（或其他数值方法）中的用户定义材料模型。然而，在定义配点和整合同时激活或非激活的多个非相等约束方面仍存在挑战。

    We applied physics-informed neural networks to solve the constitutive relations for nonlinear, path-dependent material behavior. As a result, the trained network not only satisfies all thermodynamic constraints but also instantly provides information about the current material state (i.e., free energy, stress, and the evolution of internal variables) under any given loading scenario without requiring initial data. One advantage of this work is that it bypasses the repetitive Newton iterations needed to solve nonlinear equations in complex material models. Additionally, strategies are provided to reduce the required order of derivation for obtaining the tangent operator. The trained model can be directly used in any finite element package (or other numerical methods) as a user-defined material model. However, challenges remain in the proper definition of collocation points and in integrating several non-equality constraints that become active or non-active simultaneously. We tested this
    
[^154]: 未知拓扑网络中的探索学习辅助社区检测的统一框架

    A Unified Framework for Exploratory Learning-Aided Community Detection in Networks with Unknown Topology. (arXiv:2304.04497v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2304.04497](http://arxiv.org/abs/2304.04497)

    META-CODE是一个统一的框架，通过探索学习和易于收集的节点元数据，在未知拓扑网络中检测重叠社区。实验结果证明了META-CODE的有效性和可扩展性。

    

    在社交网络中，发现社区结构作为各种网络分析任务中的一个基本问题受到了广泛关注。然而，由于隐私问题或访问限制，网络结构通常是未知的，这使得现有的社区检测方法在没有昂贵的网络拓扑获取的情况下无效。为了解决这个挑战，我们提出了 META-CODE，这是一个统一的框架，通过探索学习辅助易于收集的节点元数据，在未知拓扑网络中检测重叠社区。具体而言，META-CODE 除了初始的网络推理步骤外，还包括三个迭代步骤：1) 基于图神经网络（GNNs）的节点级社区归属嵌入，通过我们的新重构损失进行训练，2) 基于社区归属的节点查询进行网络探索，3) 使用探索网络中的基于边连接的连体神经网络模型进行网络推理。通过实验结果证明了 META-CODE 的有效性和可扩展性。

    In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often unknown, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities in networks with unknown topology via exploratory learning aided by easy-to-collect node metadata. Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through e
    
[^155]: NeBLa: 使用神经啤酒-兰伯特法从全景放射线图中重建口腔结构的三维模型

    NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])

    [http://arxiv.org/abs/2304.04027](http://arxiv.org/abs/2304.04027)

    该论文提出了一个新的框架：NeBLa，可以从全景放射线图中通过神经啤酒-兰伯特法重建精确的3D口腔结构模型。

    

    全景X线片（全景放射线图，PX）是常用于牙科检查的成像模式。然而，与3D锥形束计算机断层扫描（CBCT）相比，PX的适用性有限，因为PX只提供口腔结构的二维扁平图像。在本文中，我们提出了一个新的框架，用于从真实的PX图像估计3D口腔结构。由于PX和CBCT数据的匹配不多，我们在训练时使用了从CBCT模拟的PX，但在推理时使用了真实的全景放射线片。我们提出了一种新的光线采样方法，受到全景放射线成像原理的启发，利用啤酒-兰伯特定律导出渲染函数生成模拟全景放射线图。我们的模型由三个部分组成：转换模块，生成模块和精炼模块。转换模块将真实的全景放射线图转换为模拟的训练图像风格。生成模块利用射线采样方法得到的模拟全景放射线图约束下的输入图像生成3D结构。精炼模块改善了3D结构的平滑性和一致性。实验结果表明，我们提出的方法可以从全景放射线片提供的有限信息中生成精确的3D牙科模型。

    Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
    
[^156]: TransPimLib：用于处理器内存系统上高效的超越函数的库

    TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])

    [http://arxiv.org/abs/2304.01951](http://arxiv.org/abs/2304.01951)

    TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。

    

    处理器内存系统（PIM）承诺减轻现代计算系统中的数据移动瓶颈。然而，现有的真实PIM系统有一个内在的劣势，即它们的硬件比传统的处理器（CPU、GPU）更加受限，因为在内存附近或内部构建处理元件的难度和成本很高。因此，通用的PIM架构支持相当有限的指令集，并且难以执行复杂的操作，例如超越函数和其他难以计算的操作（例如平方根）。这些操作对于一些现代工作负载尤其重要，例如机器学习应用中的激活函数。为了在通用的PIM系统中提供对超越（和其他难以计算）函数的支持，我们介绍了TransPimLib，这是一个库，提供基于CORDIC和LUT的三角函数、双曲函数、指数、对数、平方根等难以计算的函数的方法。

    Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
    
[^157]: 文本到图像扩散模型是零样本分类器。

    Text-to-Image Diffusion Models are Zero-Shot Classifiers. (arXiv:2303.15233v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.15233](http://arxiv.org/abs/2303.15233)

    文本到图像扩散模型被提出用于零样本分类器，具有竞争性的零样本图像分类表现和先进的形状/纹理偏差测试结果，能够成功执行属性绑定。

    

    文本到图像扩散模型具有优秀的生成能力，这表明它们学习了图像文本数据的信息表达。然而，它们所捕捉的知识尚未被充分理解，且在下游任务上尚未进行深入探索。本文提出了一种评估扩散模型作为零样本分类器的方法。关键思想是利用扩散模型根据标签的文本描述去除噪声图像的能力作为该标签概率的代理。我们将该方法应用于稳定扩散和Imagen，并与CLIP的零样本能力进行对比，探索了模型的知识的细粒度方面。在广泛的零样本图像分类数据集上，他们与CLIP的竞争性表现相当。此外，他们在形状/纹理偏差测试上取得了最先进的结果，并且能够成功执行属性绑定，而CLIP不能。尽管生成性预训练在自然语言处理中很常见，v

    The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data. However, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks. We investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers. The key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood. We apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image classification datasets. Additionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot. Although generative pre-training is prevalent in NLP, v
    
[^158]: EdgeServe:一种为去中心化预测而设计的执行层

    EdgeServe: An Execution Layer for Decentralized Prediction. (arXiv:2303.08028v1 [cs.DB])

    [http://arxiv.org/abs/2303.08028](http://arxiv.org/abs/2303.08028)

    EdgeServe 是一种为去中心化预测而设计的机器学习系统，通过低延迟的消息代理程序将数据路由到可以提供预测的节点。它具有一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。在多摄像机物体跟踪，网络入侵检测和人类活动识别等三个去中心化预测任务中，EdgeServe 展现了很好的性能。

    

    机器学习任务的相关特征可能来自于网络中不同节点收集的数据源。这种问题被称之为去中心化预测，并在数据路由、计算布局和时间同步方面带来了许多有趣的系统挑战。本文提出了一种名为EdgeServe的机器学习系统，可以为去中心化预测提供服务。 EdgeServe 依赖于一个低延迟的消息代理程序，通过网络路由数据到可以提供预测的节点。EdgeServe 依赖一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。我们在三个去中心化预测任务中评估了EdgeServe：（1）多摄像机物体跟踪，（2）网络入侵检测和（3）人类活动识别。

    The relevant features for a machine learning task may be aggregated from data sources collected on different nodes in a network. This problem, which we call decentralized prediction, creates a number of interesting systems challenges in managing data routing, placing computation, and time-synchronization. This paper presents EdgeServe, a machine learning system that can serve decentralized predictions. EdgeServe relies on a low-latency message broker to route data through a network to nodes that can serve predictions. EdgeServe relies on a series of novel optimizations that can tradeoff computation, communication, and accuracy. We evaluate EdgeServe on three decentralized prediction tasks: (1) multi-camera object tracking, (2) network intrusion detection, and (3) human activity recognition.
    
[^159]: 实验固体力学中机器学习的最新进展与应用：一篇综述

    Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review. (arXiv:2303.07647v1 [cs.LG])

    [http://arxiv.org/abs/2303.07647](http://arxiv.org/abs/2303.07647)

    本文综述了近年来机器学习在实验固体力学领域中的最新进展和应用，提供了物理感知和基于物理学的机器学习方法，并涵盖了断裂力学、生物力学、纳米和微观力学、构建材料和二维材料等传统和新兴领域。

    

    多年来，实验固体力学在表征和理解天然和新材料的力学性质方面发挥了至关重要的作用。机器学习的最新进展为该领域提供了新的机遇，包括实验设计、数据分析、不确定性量化和反问题。由于近年来该新兴领域发表的论文数量迅速增加，因此及时进行全面和更新的综述，对于最近机器学习在实验固体力学中的应用具有重要意义。在本文中，我们首先概述了与该综述相关的常见机器学习算法和术语，重点介绍了基于物理学和物理感知的机器学习方法。然后，我们全面涵盖了实验力学传统和新兴领域中机器学习的最新应用，包括断裂力学、生物力学、纳米和微观力学、构建材料和二维材料。最后，我们强调了当前活跃的研究方向和领域面临的主要挑战，最后讨论了机器学习在实验固体力学未来的潜在机会。

    For many decades, experimental solid mechanics has played a crucial role in characterizing and understanding the mechanical properties of natural and novel materials. Recent advances in machine learning (ML) provide new opportunities for the field, including experimental design, data analysis, uncertainty quantification, and inverse problems. As the number of papers published in recent years in this emerging field is exploding, it is timely to conduct a comprehensive and up-to-date review of recent ML applications in experimental solid mechanics. Here, we first provide an overview of common ML algorithms and terminologies that are pertinent to this review, with emphasis placed on physics-informed and physics-based ML methods. Then, we provide thorough coverage of recent ML applications in traditional and emerging areas of experimental mechanics, including fracture mechanics, biomechanics, nano- and micro-mechanics, architected materials, and 2D material. Finally, we highlight some curr
    
[^160]: TSMixer：一种全MLP架构用于时间序列预测

    TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)

    TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。

    

    实际时间序列数据集通常是多变量且具有复杂的动态。为了捕获这种复杂性，像循环或基于注意力的顺序深度学习模型这样的高容量结构变得受欢迎。然而，最近的研究表明，简单的单变量线性模型可以在几个常用的学术基准测试中胜过这样的深度学习模型。扩展它们，本文研究线性模型在时间序列预测中的能力，并提出了时序混合器（TSMixer），这是一种通过堆叠多层感知器（MLP）设计的新型结构。 TSMixer基于沿时间和特征维度的混合操作，以有效地提取信息。在流行的学术基准测试上，简单易行的TSMixer与利用特定基准的归纳偏差的专业先进模型相媲美。在具有挑战性和大规模的M5基准测试中，即一个实际的零售数据集上，TSMixer表现出非常出色的性能。

    Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
    
[^161]: ChatGPT已在地平线上：大语言模型是否就是我们需要的智能交通解决方案？

    ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.05382](http://arxiv.org/abs/2303.05382)

    本文探讨了ChatGPT在解决交通问题方面的应用。通过利用具有跨模态编码器的LLM，可以处理来自不同模态的交通数据并执行交通运营。作者提供了一个基于智能手机的碰撞报告自动生成和分析框架作为用例展示了这种潜力。

    

    ChatGPT是由OpenAI开发的具有60亿参数的重要大语言模型之一。ChatGPT展示了LLM的卓越的语言理解能力，特别是在生成对话响应方面。随着LLM在各种研究或工程领域越来越受到关注，现在是时候设想LLM如何革新我们处理智能交通系统的方式了。本文探讨了LLM在解决关键交通问题方面的未来应用。通过利用具有跨模态编码器的LLM，智能系统还可以处理来自不同模态的交通数据并通过LLM执行交通运营。我们提出并验证了LLM装备的这些潜在的交通应用。为了进一步证明这种潜力，我们还提供了一个具体的基于智能手机的碰撞报告自动生成和分析框架作为用例。尽管存在潜在的益处，但与数据隐私相关的挑战仍然存在。

    ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
    
[^162]: 面向大规模机器学习模型的可证明高效量子算法

    Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2303.03428](http://arxiv.org/abs/2303.03428)

    本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \times \text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。

    

    大型机器学习模型是人工智能的革命性技术，其瓶颈包括巨大的计算开销、功耗和时间，既用于预训练，也用于微调过程。本研究表明，容错量子计算可能会针对通用（随机）梯度下降算法提供可证明的高效解决方案，其缩放为 $\mathcal{O}(T^2 \times \text{polylog}(n))$，其中 $n$ 是模型的大小，$T$ 是训练中的迭代次数，只要模型足够耗散和稀疏，并具有较小的学习率。基于早期用于耗散微分方程的高效量子算法，我们发现并证明了类似的算法可用于（随机）梯度下降，这是机器学习的主要算法。在实践中，我们对拥有从700万到1.03亿个参数的大型机器学习模型进行了基准测试。我们发现，在稀疏训练的情况下，量子计算显然可以在一定程度上提高效率。

    Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\mathcal{O}(T^2 \times \text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan
    
[^163]: 使用占用空间和时间较小的低深度量子态准备方法及其应用

    Spacetime-Efficient Low-Depth Quantum State Preparation with Applications. (arXiv:2303.02131v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2303.02131](http://arxiv.org/abs/2303.02131)

    提出了一种使用占用空间和时间较小的低深度方法来准备任意量子态，能够在较少的量子资源使用下实现更快的准备速度。

    

    我们提出一种新颖的确定性方法来准备任意的量子态。当将我们的协议编译为CNOT门和任意的单比特门时，它可以在深度$O(\log(N))$和空间时间分配$O(N)$的情况下准备一个$N$维的量子态，这两个参数都是最优的。当编译为$\{\mathrm{H,S,T,CNOT}\}$门集时，我们证明它所需的量子资源比之前的方法要少。具体而言，在深度$O(\log(N/\epsilon))$和空间时间分配$O(N\log(\log(N)/\epsilon))$下，可以准备一个误差为$\epsilon$的任意的量子态，这比之前方法的$O(\log(N)\log(N/\epsilon))$和$O(N\log(N/\epsilon))$有所改进。我们说明了我们协议的减小空间时间分配可以有效地快速准备多个不相交状态，只需要常数因子的辅助量子比特开销--通过高效地重用$O(N)$个辅助比特来准备一个乘积态。

    We propose a novel deterministic method for preparing arbitrary quantum states. When our protocol is compiled into CNOT and arbitrary single-qubit gates, it prepares an $N$-dimensional state in depth $O(\log(N))$ and spacetime allocation (a metric that accounts for the fact that oftentimes some ancilla qubits need not be active for the entire circuit) $O(N)$, which are both optimal. When compiled into the $\{\mathrm{H,S,T,CNOT}\}$ gate set, we show that it requires asymptotically fewer quantum resources than previous methods. Specifically, it prepares an arbitrary state up to error $\epsilon$ in depth $O(\log(N/\epsilon))$ and spacetime allocation $O(N\log(\log(N)/\epsilon))$, improving over $O(\log(N)\log(N/\epsilon))$ and $O(N\log(N/\epsilon))$, respectively. We illustrate how the reduced spacetime allocation of our protocol enables rapid preparation of many disjoint states with only constant-factor ancilla overhead -- $O(N)$ ancilla qubits are reused efficiently to prepare a product
    
[^164]: 图像标签是粗糙海草分割的全部所需

    Image Labels Are All You Need for Coarse Seagrass Segmentation. (arXiv:2303.00973v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00973](http://arxiv.org/abs/2303.00973)

    本文将海草分类重新定义为弱监督的粗糙分割问题，使用图像级标签进行训练，推理时获得补丁级输出。通过SeaFeats和SeaCLIP模型的引入和应用，证明了其在海草识别和分类中的有效性。

    

    海草草原是重要的碳汇，但估计存储的碳量需要知道存在的海草物种。配备机器学习算法的水下和水面载具可以帮助准确估计海草草原的组成和范围。然而，以往的海草检测和分类方法需要从补丁级标签进行监督。在本文中，我们将海草分类重新定义为弱监督的粗糙分割问题，在训练过程中使用图像级标签（比补丁级标注少25倍），并在推理时获得补丁级输出。为此，我们引入了SeaFeats，一种使用无监督对比预训练和特征相似性的体系结构，以及SeaCLIP，一个展示大型语言模型作为域特定应用中监督信号效果的模型。我们证明了一个SeaFeats集合的效果。

    Seagrass meadows serve as critical carbon sinks, but estimating the amount of carbon they store requires knowledge of the seagrass species present. Underwater and surface vehicles equipped with machine learning algorithms can help to accurately estimate the composition and extent of seagrass meadows at scale. However, previous approaches for seagrass detection and classification have required supervision from patch-level labels. In this paper, we reframe seagrass classification as a weakly supervised coarse segmentation problem where image-level labels are used during training (25 times fewer labels compared to patch-level labeling) and patch-level outputs are obtained at inference time. To this end, we introduce SeaFeats, an architecture that uses unsupervised contrastive pre-training and feature similarity, and SeaCLIP, a model that showcases the effectiveness of large language models as a supervisory signal in domain-specific applications. We demonstrate that an ensemble of SeaFeats
    
[^165]: 具有注意融合的动态图卷积网络用于交通流量预测

    Dynamic Graph Convolutional Network with Attention Fusion for Traffic Flow Prediction. (arXiv:2302.12598v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12598](http://arxiv.org/abs/2302.12598)

    提出了一种用于交通流量预测的具有注意融合的动态图卷积网络，通过增强时间特征维度的交互作用和捕捉多尺度空间-时间依赖关系，以及有效捕捉远距离、多方面领域的空间-时间模式，实现精确且实时的交通状态预测。

    

    精确的实时交通状态预测对于城市交通控制和网络地图服务具有重要的实际意义。在大数据的支持下，深度学习方法在捕捉交通网络的复杂时空模式方面显示出强大的能力。然而，现有方法使用预定义的图和简单的空间-时间组件，难以建模多尺度的空间-时间依赖关系。在本文中，我们提出了一种新颖的动态图卷积网络与注意融合的方法来解决这个问题。该方法首先增强了时间特征维度的交互作用，然后将动态图学习器与GRU相结合，共同建模同步的空间-时间相关性。我们还融入了空间-时间注意模块，以有效捕捉远距离、多方面领域的空间-时间模式。我们在四个真实世界的交通数据集上进行了大量实验，证明了我们的方法的有效性。

    Accurate and real-time traffic state prediction is of great practical importance for urban traffic control and web mapping services. With the support of massive data, deep learning methods have shown their powerful capability in capturing the complex spatialtemporal patterns of traffic networks. However, existing approaches use pre-defined graphs and a simple set of spatial-temporal components, making it difficult to model multi-scale spatial-temporal dependencies. In this paper, we propose a novel dynamic graph convolution network with attention fusion to tackle this gap. The method first enhances the interaction of temporal feature dimensions, and then it combines a dynamic graph learner with GRU to jointly model synchronous spatial-temporal correlations. We also incorporate spatial-temporal attention modules to effectively capture longrange, multifaceted domain spatial-temporal patterns. We conduct extensive experiments in four real-world traffic datasets to demonstrate that our met
    
[^166]: 随机和方差减少的三次牛顿方法的统一收敛理论

    Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods. (arXiv:2302.11962v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.11962](http://arxiv.org/abs/2302.11962)

    该论文提出了一个名为辅助框架的新框架，通过统一的视角，提供了具有全局复杂性保证的随机和方差减少的二阶算法。该框架在构建和分析随机三次牛顿方法时具有高度灵活性，使用了任意大小的批量，以及有噪声和可能有偏差的梯度和Hessian的估计，结合了方差减少和惰性Hessian更新。在噪声的弱假设下，恢复了已知的随机和方差减少的三次牛顿的最佳复杂性。

    

    我们研究用于解决一般可能非凸最小化问题的随机三次牛顿方法。我们提出了一个新的框架，称之为辅助框架，它提供了具有全局复杂性保证的随机和方差减少的二阶算法的统一视角。它还可以应用于带有辅助信息的学习。我们的辅助框架为算法设计者提供了构建和分析随机三次牛顿方法的高度灵活性，允许任意大小的批量，并且使用有噪声和可能有偏差的梯度和Hessian的估计，将方差减少和惰性Hessian更新结合起来。在噪声的弱假设下，我们恢复了已知的随机和方差减少的三次牛顿的最佳复杂性。我们理论的一个直接结果是新的惰性随机二阶方法，它显著改进了大维问题的算术复杂性。

    We study stochastic Cubic Newton methods for solving general possibly non-convex minimization problems. We propose a new framework, which we call the helper framework, that provides a unified view of the stochastic and variance-reduced second-order algorithms equipped with global complexity guarantees. It can also be applied to learning with auxiliary information. Our helper framework offers the algorithm designer high flexibility for constructing and analyzing the stochastic Cubic Newton methods, allowing arbitrary size batches, and the use of noisy and possibly biased estimates of the gradients and Hessians, incorporating both the variance reduction and the lazy Hessian updates. We recover the best-known complexities for the stochastic and variance-reduced Cubic Newton, under weak assumptions on the noise. A direct consequence of our theory is the new lazy stochastic second-order method, which significantly improves the arithmetic complexity for large dimension problems. We also esta
    
[^167]: 一种用于无人机半监督分割的信息路径规划框架

    An Informative Path Planning Framework for Active Learning in UAV-based Semantic Mapping. (arXiv:2302.03347v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.03347](http://arxiv.org/abs/2302.03347)

    本文提出了一种针对语义分割的信息路径规划框架，使无人机能够自主获取信息丰富的训练影像以供模型重新训练。

    

    无人机在航空测绘和监测任务中被广泛使用。深度学习的进展使得自动语义分割能够更好地解释大规模复杂环境。常用的用于分割的有监督深度学习依赖于大量像素级标注数据，而标注费用高昂且费时。而航空环境的领域特定视觉外观通常阻碍了使用公开数据集上的预训练模型。为解决这个问题，本文提出了一个新的通用规划框架，使得无人机可以自主获取信息丰富的训练影像以供模型重新训练。本文将多个采集函数结合并融合到概率地形图中。然后将地图中的各种信息融合到无人机的规划目标中。无人机会自适应地获取信息丰富的航空影像，以进行人工标注用于模型重新训练。

    Unmanned aerial vehicles (UAVs) are frequently used for aerial mapping and general monitoring tasks. Recent progress in deep learning enabled automated semantic segmentation of imagery to facilitate the interpretation of large-scale complex environments. Commonly used supervised deep learning for segmentation relies on large amounts of pixel-wise labelled data, which is tedious and costly to annotate. The domain-specific visual appearance of aerial environments often prevents the usage of models pre-trained on publicly available datasets. To address this, we propose a novel general planning framework for UAVs to autonomously acquire informative training images for model re-training. We leverage multiple acquisition functions and fuse them into probabilistic terrain maps. Our framework combines the mapped acquisition function information into the UAV's planning objectives. In this way, the UAV adaptively acquires informative aerial images to be manually labelled for model re-training. E
    
[^168]: LUT-NN：通过质心学习和表格查找提高高效神经网络推理能力

    LUT-NN: Empower Efficient Neural Network Inference with Centroid Learning and Table Lookup. (arXiv:2302.03213v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03213](http://arxiv.org/abs/2302.03213)

    LUT-NN通过质心学习和表格查找提高神经网络推理效果以减少推理成本。具体而言，LUT-NN使用可微分的质心学习来最小化质心对准确性的影响，并通过表格查找直接读取近似输出结果，以实现高效的推理执行。

    

    在设备上进行深度神经网络（DNN）推理消耗了大量的计算资源和开发工作。为了缓解这个问题，我们提出了LUT-NN，这是第一个通过表格查找提高推理效果以减少推理成本的系统。LUT-NN学习了每个运算符的典型特征，被称为质心，并预先计算了这些质心的结果，保存在查找表中。在推理过程中，可以直接从表格中读取与输入最接近的质心的结果作为近似输出，无需进行计算。LUT-NN整合了两个主要的创新技术：（1）可微分的通过反向传播进行质心学习，通过质心适应三个级别的近似来最小化质心对准确性的影响；（2）表格查找推理执行，全面考虑了不同级别的并行性、内存访问的降低和专用硬件单元，以达到最佳性能。LUT-NN在多个实际任务上进行了评估，涵盖了图像和语音识别等领域。

    On-device Deep Neural Network (DNN) inference consumes significant computing resources and development efforts. To alleviate that, we propose LUT-NN, the first system to empower inference by table lookup, to reduce inference cost. LUT-NN learns the typical features for each operator, named centroid, and precompute the results for these centroids to save in lookup tables. During inference, the results of the closest centroids with the inputs can be read directly from the table, as the approximated outputs without computations. LUT-NN integrates two major novel techniques: (1) differentiable centroid learning through backpropagation, which adapts three levels of approximation to minimize the accuracy impact by centroids; (2) table lookup inference execution, which comprehensively considers different levels of parallelism, memory access reduction, and dedicated hardware units for optimal performance. LUT-NN is evaluated on multiple real tasks, covering image and speech recognition, and na
    
[^169]: 在交互环境中使用在线强化学习对大型语言模型进行基础设施建设

    Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. (arXiv:2302.02662v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02662](http://arxiv.org/abs/2302.02662)

    本文研究了一种名为GLAM的方法，通过功能基础设施建设，利用在线强化学习提高LLM代理程序的性能来实现LLMs与环境之间的对齐，解决决策问题。

    

    最近的研究成功地利用了大型语言模型（LLM）捕捉世界物理的抽象知识，以解决决策问题。然而，LLMs的知识与环境之间的对齐可能是错误的，并且由于缺乏基础设施建设而限制了其功能能力。在本文中，我们研究了一种通过功能基础设施建设实现这种对齐的方法（称为GLAM）：我们考虑一个使用LLM作为策略的代理程序，随着代理程序与环境进行交互而逐步更新，并利用在线强化学习来提高其解决目标的性能。使用一个交互式的文本环境设计来研究更高级形式的基础设施建设，以及一组空间和导航任务，我们研究了几个科学问题：1）LLMs能否提高各种RL任务的在线学习的样本效率？2）它如何提高不同形式的泛化？3）在线学习的影响是什么？我们通过功能方式研究这些问题。

    Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functio
    
[^170]: SAN: 利用判别式归一化线性层诱导GAN的可测性

    SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer. (arXiv:2301.12811v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12811](http://arxiv.org/abs/2301.12811)

    本文提出了一种新的GAN训练方案，切片对抗网络（SAN），通过优化生成器和判别器的最小最大目标函数，使生成器分布接近目标分布。实验证实了SAN相对于普通GAN的有效性，并在StyleGAN-XL上取得了最先进的FID评分。

    

    生成对抗网络（GAN）通过优化生成器和判别器的最小最大目标函数来学习目标概率分布。本文解决了这样一个问题：优化是否真正提供了使生成器分布接近目标分布的梯度。我们通过将GAN的形式与切片最优输运的概念结合起来，推导了可测性条件，即判别器作为分布之间的距离的充分条件。此外，通过利用这些理论结果，我们提出了一种新的GAN训练方案，称为切片对抗网络（SAN）。通过简单的修改，可以将广泛类别的现有GAN转化为SAN。在合成和图像数据集上的实验证实了我们的理论结果和SAN相对于普通GAN的有效性。此外，我们还将SAN应用于StyleGAN-XL，在分类上取得了GAN中最先进的FID（Frechet Inception Distance）评分。

    Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive metrizable conditions, sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme, called slicing adversarial network (SAN). With only simple modifications, a broad class of existing GANs can be converted to SANs. Experiments on synthetic and image datasets support our theoretical results and the SAN's effectiveness as compared to usual GANs. Furthermore, we also apply SAN to StyleGAN-XL, which leads to state-of-the-art FID score amongst GANs for class con
    
[^171]: 自监督学习和动态计算之间的统一协同作用

    Unifying Synergies between Self-supervised Learning and Dynamic Computation. (arXiv:2301.09164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09164](http://arxiv.org/abs/2301.09164)

    本文提出了自监督学习和动态计算之间相互作用的新视角。通过在自监督学习的设置中同时学习密集和门控子网络，无需额外的微调或剪枝步骤，可以获得一个通用且高效的架构，适用于资源受限的工业环境。

    

    在资源受限的工业环境中，计算昂贵的训练策略使得自监督学习（SSL）变得不切实际。为了获取轻量级模型，通常会使用知识蒸馏（KD）、动态计算（DC）和剪枝等技术，这通常涉及到对大型预训练模型进行多次微调（或蒸馏步骤），使得计算更具挑战性。在这项工作中，我们提出了自监督学习和动态计算范式之间相互作用的新视角。特别地，我们展示了在自监督学习的设置中，同时从零开始学习密集和门控子网络是可行的，而不需要额外的微调或剪枝步骤。预训练期间密集和门控编码器的共同演化提供了良好的准确性与效率之间的平衡，从而为应用特定的工业环境提供了通用且多功能的架构。我们在包括CIFAR-10/100等多个图像分类基准上进行了大量实验。

    Computationally expensive training strategies make self-supervised learning (SSL) impractical for resource constrained industrial settings. Techniques like knowledge distillation (KD), dynamic computation (DC), and pruning are often used to obtain a lightweightmodel, which usually involves multiple epochs of fine-tuning (or distilling steps) of a large pre-trained model, making it more computationally challenging. In this work we present a novel perspective on the interplay between SSL and DC paradigms. In particular, we show that it is feasible to simultaneously learn a dense and gated sub-network from scratch in a SSL setting without any additional fine-tuning or pruning steps. The co-evolution during pre-training of both dense and gated encoder offers a good accuracy-efficiency trade-off and therefore yields a generic and multi-purpose architecture for application specific industrial settings. Extensive experiments on several image classification benchmarks including CIFAR-10/100, S
    
[^172]: 基于拓扑学的神经突触解码的深度学习框架

    A Topological Deep Learning Framework for Neural Spike Decoding. (arXiv:2212.05037v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2212.05037](http://arxiv.org/abs/2212.05037)

    这项工作开发了一个基于拓扑学的深度学习框架，用于解码神经突触输出，以更好地理解和表示大脑中的神经结构。

    

    大脑的空间定位系统使用不同的神经元集合来辅助基于环境的导航。大脑通过头方向细胞和网格细胞两种方式编码空间信息。头方向细胞用于确定方向，而网格细胞由叠加的神经元层组成，提供基于环境的导航。这些神经元以集合的形式发放信号，多个神经元同时发放信号以激活单个头方向或网格。我们希望捕捉这种发放结构，并将其用于解码头方向和网格细胞数据。理解、表示和解码这些神经结构需要包含高阶连接性的模型，而不仅仅是传统基于图的模型提供的一维连接性。为此，我们在这项工作中开发了一个基于拓扑深度学习框架来解码神经突触输出的工具。我们的框架结合了无监督的简单复合体发现和深度学习的强大能力。

    The brain's spatial orientation system uses different neuron ensembles to aid in environment-based navigation. Two of the ways brains encode spatial information is through head direction cells and grid cells. Brains use head direction cells to determine orientation whereas grid cells consist of layers of decked neurons that overlay to provide environment-based navigation. These neurons fire in ensembles where several neurons fire at once to activate a single head direction or grid. We want to capture this firing structure and use it to decode head direction grid cell data. Understanding, representing, and decoding these neural structures requires models that encompass higher order connectivity, more than the 1-dimensional connectivity that traditional graph-based models provide. To that end, in this work, we develop a topological deep learning framework for neural spike train decoding. Our framework combines unsupervised simplicial complex discovery with the power of deep learning via 
    
[^173]: 通过分解行列式点过程使用多样的负样本来进行图卷积神经网络

    Graph Convolutional Neural Networks with Diverse Negative Samples via Decomposed Determinant Point Processes. (arXiv:2212.02055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02055](http://arxiv.org/abs/2212.02055)

    本论文提出了基于DPP的方法来获取多样的负样本，在图卷积神经网络中实现图表示学习，通过提供不同的信息来更新节点的表示，从而提高了准确性。

    

    图卷积网络（GCNs）通过从节点和它们的拓扑中提取高级特征来实现图表示学习。本文采用了一种基于行列式点过程（DPP）的质量-多样性分解方法来获取多样的负样本，除边缘节点之外的所有节点都被考虑在内，这些节点通过提供不同的信息有利于表示更新。

    Graph convolutional networks (GCNs) have achieved great success in graph representation learning by extracting high-level features from nodes and their topology. Since GCNs generally follow a message-passing mechanism, each node aggregates information from its first-order neighbour to update its representation. As a result, the representations of nodes with edges between them should be positively correlated and thus can be considered positive samples. However, there are more non-neighbour nodes in the whole graph, which provide diverse and useful information for the representation update. Two non-adjacent nodes usually have different representations, which can be seen as negative samples. Besides the node representations, the structural information of the graph is also crucial for learning. In this paper, we used quality-diversity decomposition in determinant point processes (DPP) to obtain diverse negative samples. When defining a distribution on diverse subsets of all non-neighbourin
    
[^174]: 通过半监督学习和生成对抗网络在不平衡数据集中进行虚假检测

    Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01071](http://arxiv.org/abs/2212.01071)

    本文提出了一种在不平衡数据集中使用半监督学习和生成对抗网络进行虚假检测的方法，实验证明仅使用100个标记样本的情况下，准确率达到了91\%。

    

    随着社交媒体的快速发展，骚扰行为变得更加普遍，这导致了虚假检测成为研究人员中引人注目的领域。数据的图形特性以及大量节点导致了许多障碍，包括矩阵中大量无关特征的高离散度和不平衡类别。为了解决这些问题，本文采用了自编码器和半监督学习与生成对抗网络算法的组合，即SGAN。本文将少量标签应用于SGAN作为分类器。实验结果表明，仅使用100个标记样本，该方法在检测虚假账户方面的准确率达到了91\%。

    As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
    
[^175]: CPPF++：基于投票聚合的考虑不确定性的Sim2Real物体姿态估计

    CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote Aggregation. (arXiv:2211.13398v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13398](http://arxiv.org/abs/2211.13398)

    本文针对Sim2Real物体姿态估计问题，提出了一种新颖的CPPF++方法，通过投票聚合和概率建模来考虑投票不确定性，并通过迭代噪声过滤来提高姿态估计的准确性。

    

    物体姿态估计是三维视觉领域中的关键技术，本文针对只利用三维CAD模型作为先验知识的情况，提出了一种新颖的CPPF++方法，用于Sim2Real物体姿态估计。该方法通过概率性视角重新构思了CPPF的基础点对投票方案，以解决投票碰撞的挑战，并通过估计规范空间中每个点对的概率分布来建模投票不确定性。此外，该方法还通过迭代噪声过滤来消除与背景或杂波有关的投票，并增强了每个投票所提供的上下文信息。

    Object pose estimation constitutes a critical area within the domain of 3D vision. While contemporary state-of-the-art methods that leverage real-world pose annotations have demonstrated commendable performance, the procurement of such real-world training data incurs substantial costs. This paper focuses on a specific setting wherein only 3D CAD models are utilized as a priori knowledge, devoid of any background or clutter information. We introduce a novel method, CPPF++, designed for sim-to-real pose estimation. This method builds upon the foundational point-pair voting scheme of CPPF, reconceptualizing it through a probabilistic lens. To address the challenge of voting collision, we model voting uncertainty by estimating the probabilistic distribution of each point pair within the canonical space. This approach is further augmented by iterative noise filtering, employed to eradicate votes associated with backgrounds or clutters. Additionally, we enhance the context provided by each v
    
[^176]: 在联合设置中面向隐私感知的因果结构学习的探索

    Towards Privacy-Aware Causal Structure Learning in Federated Setting. (arXiv:2211.06919v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.06919](http://arxiv.org/abs/2211.06919)

    本文研究了在联合设置中隐私感知因果结构学习的问题，并提出了一种新的联邦PC（FedPC）算法，通过两种新策略保护数据隐私。

    

    因果结构学习在机器学习和各种应用中得到了广泛研究和应用。为了达到理想的性能，现有的因果结构学习算法通常需要将来自多个数据源的大量数据集中在一起。然而，在隐私保护的设置中，不可能将所有数据集中并放在一个数据集中。为了保护数据隐私，联邦学习作为一种新的学习范式在近年来在机器学习中引起了广泛关注。在本文中，我们研究了联合设置中的隐私感知因果结构学习问题，并提出了一种新的联邦PC（FedPC）算法，采用两种新策略来保护数据隐私而不集中数据。具体而言，我们首先提出了一种新的逐层聚合策略，将PC算法平稳地适应到联邦学习范式中，以实现联合骨架学习，然后我们设计了一种有效的策略。

    Causal structure learning has been extensively studied and widely used in machine learning and various applications. To achieve an ideal performance, existing causal structure learning algorithms often need to centralize a large amount of data from multiple data sources. However, in the privacy-preserving setting, it is impossible to centralize data from all sources and put them together as a single dataset. To preserve data privacy, federated learning as a new learning paradigm has attracted much attention in machine learning in recent years. In this paper, we study a privacy-aware causal structure learning problem in the federated setting and propose a novel Federated PC (FedPC) algorithm with two new strategies for preserving data privacy without centralizing data. Specifically, we first propose a novel layer-wise aggregation strategy for a seamless adaptation of the PC algorithm into the federated learning paradigm for federated skeleton learning, then we design an effective strate
    
[^177]: 基于非凸配对融合的聚类联邦学习

    Clustered Federated Learning based on Nonconvex Pairwise Fusion. (arXiv:2211.04218v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04218](http://arxiv.org/abs/2211.04218)

    本研究提出了一种基于非凸配对融合的聚类联邦学习框架，能够自动识别聚类结构，降低通信成本，并确保隐私性。

    

    本研究探讨了聚类联邦学习（FL），即具有非i.i.d.数据的FL的一种形式。在聚类联邦学习中，设备被分成聚类，并且每个聚类通过使用本地模型来最优地匹配其数据。我们提出了一种结合了非凸惩罚以配对参数差异的聚类联邦学习框架。该框架可以自动识别聚类结构，无需预先知道聚类的数量和每个聚类中的设备集合。为了实现所提出的框架，我们引入了一种称为融合惩罚联邦聚类（FPFC）的新型聚类联邦学习方法。基于标准的交替方向乘子方法（ADMM），FPFC在并行中实施，仅在每轮通信中更新设备的子集，并允许每个设备的可变工作负载。这些策略显著降低了通信成本，同时确保隐私性，使其在FL中实际可行。我们还提出了一种新的预热策略以提高性能。

    This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. This framework can automatically identify cluster structures without a priori knowledge of the number of clusters and the set of devices in each cluster. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC is implemented in parallel, updates only a subset of devices at each communication round, and allows for variable workload per device. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hype
    
[^178]: 具有最优收敛保证的显式二阶最小最大优化方法

    Explicit Second-Order Min-Max Optimization Methods with Optimal Convergence Guarantee. (arXiv:2210.12860v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.12860](http://arxiv.org/abs/2210.12860)

    本文提出了一种具有最优收敛保证的显式二阶最小最大优化方法，用于解决凸凹无约束最小最大优化问题。该方法利用二阶信息加速额外梯度方法，并且在迭代过程中保持在有界集内，达到了与理论下界相匹配的收敛速度。

    

    本文提出并分析了一种精确和不精确正则化牛顿型方法，用于求解凸凹无约束最小最大优化问题的全局鞍点。与一阶方法相比，我们对于二阶最小最大优化方法的理解相对较少，因为利用二阶信息获得全局收敛速度更加复杂。在本文中，我们研究了如何利用二阶信息加速额外梯度方法，即使在不精确的情况下也能实现。具体而言，我们证明了所提出的算法生成的迭代保持在有界集内，并且平均迭代收敛到一个 $\epsilon$-鞍点，所需迭代次数为 $O(\epsilon^{-2/3})$，其中使用了受限间隙函数。我们的算法与该领域已经建立的理论下界相匹配，而且我们的分析提供了一种简单直观的二阶方法收敛分析，不需要任何有界性要求。最后，我们提出了一个

    We propose and analyze exact and inexact regularized Newton-type methods for finding a global saddle point of \emph{convex-concave} unconstrained min-max optimization problems. Compared to first-order methods, our understanding of second-order methods for min-max optimization is relatively limited, as obtaining global rates of convergence with second-order information is much more involved. In this paper, we examine how second-order information can be used to speed up extra-gradient methods, even under inexactness. Specifically, we show that the proposed algorithms generate iterates that remain within a bounded set and the averaged iterates converge to an $\epsilon$-saddle point within $O(\epsilon^{-2/3})$ iterations in terms of a restricted gap function. Our algorithms match the theoretically established lower bound in this context and our analysis provides a simple and intuitive convergence analysis for second-order methods without any boundedness requirements. Finally, we present a 
    
[^179]: 高效个性化：将用户参数建模为低秩加稀疏分量

    Sample-Efficient Personalization: Modeling User Parameters as Low Rank Plus Sparse Components. (arXiv:2210.03505v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03505](http://arxiv.org/abs/2210.03505)

    该论文提出了一种高效的个性化算法，通过将网络权重建模为低秩和稀疏分量的总和，既捕捉了多个用户间的共同信息，又能够捕捉用户个性化的特点。

    

    个性化机器学习（ML）对个体用户/域/企业的预测至关重要，标准的个性化方法涉及学习一个用户/域特定的嵌入，然后将其馈入一个固定的全局模型，这种方法存在限制。另一方面，为每个用户/域自身个性化/微调模型本身，即元学习，具有高存储/基础架构成本。此外，对可扩展个性化方法的严格理论研究非常有限。为了解决上述问题，我们提出了一种新颖的元学习风格的方法，将网络权重建模为低秩和稀疏分量的总和。这在低秩部分捕捉了多个个体/用户的共同信息，而稀疏部分则捕捉了用户特定的特性。然后我们在线性设置中研究了该框架，其中问题简化为使用简单的方法估计秩为$r$和$k$列的稀疏矩阵的总和

    Personalization of machine learning (ML) predictions for individual users/domains/enterprises is critical for practical recommendation systems. Standard personalization approaches involve learning a user/domain specific embedding that is fed into a fixed global model which can be limiting. On the other hand, personalizing/fine-tuning model itself for each user/domain -a.k.a meta-learning -- has high storage/infrastructure cost. Moreover, rigorous theoretical studies of scalable personalization approaches have been very limited. To address the above issues, we propose a novel meta-learning style approach that models network weights as a sum of low-rank and sparse components. This captures common information from multiple individuals/users together in the low-rank part while sparse part captures user-specific idiosyncrasies. We then study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-$r$ and a $k$-column sparse matrix using a sma
    
[^180]: 带有机会约束的深度度量学习

    Deep Metric Learning with Chance Constraints. (arXiv:2209.09060v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.09060](http://arxiv.org/abs/2209.09060)

    本文将深度度量学习与有限机会约束的可行性问题相关联，证明了基于代理的DML的最小化者满足机会约束，并提出多个代理有助于性能提升，通过迭代投影解决DML问题。

    

    深度度量学习（DML）旨在最小化嵌入空间中成对内/间类近似违规的经验预期损失。我们将DML与有限机会约束的可行性问题相关联。我们证明了基于代理的DML的最小化者满足某些机会约束，并且基于代理的方法的最坏情况泛化性能可以用覆盖对应类样本整个域的最小球的半径来描述，这表明每个类别使用多个代理有助于性能提升。为了提供可扩展的算法并利用更多的代理，我们考虑了基于代理的DML实例最小化者所蕴含的机会约束，并将DML重新制定为在这些约束交集中找到可行点的问题，从而得到一个通过迭代投影来近似解决的问题。简而言之，我们反复训练正则化的基于代理的损失，并重新初始化嵌入的代理。

    Deep metric learning (DML) aims to minimize empirical expected loss of the pairwise intra-/inter- class proximity violations in the embedding space. We relate DML to feasibility problem of finite chance constraints. We show that minimizer of proxy-based DML satisfies certain chance constraints, and that the worst case generalization performance of the proxy-based methods can be characterized by the radius of the smallest ball around a class proxy to cover the entire domain of the corresponding class samples, suggesting multiple proxies per class helps performance. To provide a scalable algorithm as well as exploiting more proxies, we consider the chance constraints implied by the minimizers of proxy-based DML instances and reformulate DML as finding a feasible point in intersection of such constraints, resulting in a problem to be approximately solved by iterative projections. Simply put, we repeatedly train a regularized proxy-based loss and re-initialize the proxies with the embeddin
    
[^181]: 具有不同迭代次数的神经常微分方程模型的联邦学习

    Federated Learning of Neural ODE Models with Different Iteration Counts. (arXiv:2208.09478v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09478](http://arxiv.org/abs/2208.09478)

    本文提出了一种利用神经常微分方程模型进行联邦学习的方法，能够在聚合具有不同迭代次数或深度的模型时减少通信量，并通过实验证明了该方法的有效性。

    

    联邦学习是一种分布式机器学习方法，其中客户端使用自己的数据在本地训练模型，并将其上传到服务器，以便在它们之间共享训练结果，而无需将原始数据上传到服务器。联邦学习面临一些挑战，如通信量的减少和客户端的异质性。前者可以减轻通信开销，后者可以让客户端根据其可用计算资源选择适当的模型。为了解决这些挑战，本文利用基于神经常微分方程的模型进行联邦学习。所提出的灵活联邦学习方法可以在聚合具有不同迭代次数或深度的模型时减少通信量。我们的贡献是通过实验证明了所提出的联邦学习方法可以聚合具有不同迭代次数或深度的模型。它与另一种联邦学习方法在通信开销、模型质量等方面进行了比较。

    Federated learning is a distributed machine learning approach in which clients train models locally with their own data and upload them to a server so that their trained results are shared between them without uploading raw data to the server. There are some challenges in federated learning, such as communication size reduction and client heterogeneity. The former can mitigate the communication overheads, and the latter can allow the clients to choose proper models depending on their available compute resources. To address these challenges, in this paper, we utilize Neural ODE based models for federated learning. The proposed flexible federated learning approach can reduce the communication size while aggregating models with different iteration counts or depths. Our contribution is that we experimentally demonstrate that the proposed federated learning can aggregate models with different iteration counts or depths. It is compared with a different federated learning approach in terms of
    
[^182]: 重新审视对图分类的图神经网络的对抗性攻击

    Revisiting Adversarial Attacks on Graph Neural Networks for Graph Classification. (arXiv:2208.06651v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2208.06651](http://arxiv.org/abs/2208.06651)

    该论文提出了一种针对图分类任务的图神经网络的对抗性攻击方法。通过操作图结构和节点特征，生成对抗性示例，解决了从全局到局部的攻击挑战。这项工作为提高图神经网络的安全性和鲁棒性提供了重要的指导。

    

    图神经网络（GNN）在图分类任务及其各种实际应用中取得了巨大成功。尽管在学习图表示方面取得了巨大成功，当前的GNN模型显示出对可能存在的图结构数据中的对抗性示例的脆弱性。现有方法要么局限于结构攻击，要么局限于局部信息，迫切需要设计一种更通用的图分类攻击框架，面临着使用全局图级信息生成局部节点级对抗性示例的复杂性挑战。为了解决这一“从全局到局部”的攻击挑战，我们提出了一种通过操作图结构和节点特征生成对抗性示例的新型通用框架。具体来说，我们利用图类别激活映射及其变种来生成与图分类任务对应的节点级重要性。

    Graph neural networks (GNNs) have achieved tremendous success in the task of graph classification and its diverse downstream real-world applications. Despite the huge success in learning graph representations, current GNN models have demonstrated their vulnerability to potentially existent adversarial examples on graph-structured data. Existing approaches are either limited to structure attacks or restricted to local information, urging for the design of a more general attack framework on graph classification, which faces significant challenges due to the complexity of generating local-node-level adversarial examples using the global-graph-level information. To address this "global-to-local" attack challenge, we present a novel and general framework to generate adversarial examples via manipulating graph structure and node features. Specifically, we make use of Graph Class Activation Mapping and its variant to produce node-level importance corresponding to the graph classification task
    
[^183]: 基于骨骼的动作识别的生成式动作描述提示

    Generative Action Description Prompts for Skeleton-based Action Recognition. (arXiv:2208.05318v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.05318](http://arxiv.org/abs/2208.05318)

    本文提出了一种基于骨骼的动作识别的生成式动作描述提示（GAP）方法，利用预训练的语言模型自动生成动作的身体部位运动的文本描述，并采用多模态训练方案。

    

    基于骨骼的动作识别近年来受到了相当大的关注。目前的基于骨骼的动作识别方法通常被定义为单热分类任务，并没有充分利用动作之间的语义关系。例如，“做胜利手势”和“竖起大拇指”是手势的两种动作，其主要区别在于手部的运动。这些信息与动作类别的单热编码无关，但可以从动作描述中揭示出来。因此，在训练中利用动作描述可能有助于表示学习。在这项工作中，我们提出了一种基于骨骼的动作识别的生成式动作描述提示（GAP）方法。具体而言，我们使用预训练的大规模语言模型作为知识引擎，自动生成动作的身体部位运动的文本描述，并提出了一种多模态训练方案，利用文本编码器。

    Skeleton-based action recognition has recently received considerable attention. Current approaches to skeleton-based action recognition are typically formulated as one-hot classification tasks and do not fully exploit the semantic relations between actions. For example, "make victory sign" and "thumb up" are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encoding of action classes but could be unveiled from the action description. Therefore, utilizing action description in training could potentially benefit representation learning. In this work, we propose a Generative Action-description Prompts (GAP) approach for skeleton-based action recognition. More specifically, we employ a pre-trained large-scale language model as the knowledge engine to automatically generate text descriptions for body parts movements of actions, and propose a multi-modal training scheme by utilizing the text encoder t
    
[^184]: 使用双层优化学习稀疏促进正则化器

    Learning Sparsity-Promoting Regularizers using Bilevel Optimization. (arXiv:2207.08939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08939](http://arxiv.org/abs/2207.08939)

    本文提出了一种使用双层优化学习稀疏促进正则化器的方法，可以用于去噪信号和图像。通过监督学习的方式，将稀疏促进正则化器的参数学习最小化重建结果误差，从而提高信号重建的效果。

    

    我们提出了一种方法，用于监督学习稀疏促进正则化器来去噪信号和图像。稀疏促进正则化器是解决现代信号重建问题的关键因素；然而，这些正则化器的操作符通常要么是手工设计的，要么是通过无监督方式从数据中学习得到的。监督学习（主要是卷积神经网络）在解决图像重建问题方面取得了最近的成功，这表明它可能是设计正则化器的一种富有成效的方法。为了实现这一目标，我们提出使用变分表达式和参数化的、稀疏促进的正则化器来去噪信号，其中正则化器的参数通过最小化训练集上重建结果与真实图像及测量对的均方误差来学习。训练涉及解决具有挑战性的双层优化问题；我们推导出了训练损失的梯度的闭式表达式

    We present a method for supervised learning of sparsity-promoting regularizers for denoising signals and images. Sparsity-promoting regularization is a key ingredient in solving modern signal reconstruction problems; however, the operators underlying these regularizers are usually either designed by hand or learned from data in an unsupervised way. The recent success of supervised learning (mainly convolutional neural networks) in solving image reconstruction problems suggests that it could be a fruitful approach to designing regularizers. Towards this end, we propose to denoise signals using a variational formulation with a parametric, sparsity-promoting regularizer, where the parameters of the regularizer are learned to minimize the mean squared error of reconstructions on a training set of ground truth image and measurement pairs. Training involves solving a challenging bilievel optimization problem; we derive an expression for the gradient of the training loss using the closed-form
    
[^185]: 基于处理内存系统的机器学习训练的实验评估

    An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2207.07886](http://arxiv.org/abs/2207.07886)

    该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。

    

    训练机器学习算法是一种计算密集型的过程，由于不断访问大型训练数据集，这种过程通常会受到内存限制。因此，以处理器为中心的系统（例如CPU，GPU）在内存单元和处理单元之间的数据传输方面存在昂贵的瓶颈，这会消耗大量的能量和执行周期。具有处理内存（PIM）功能的内存中心计算系统可以缓解这种数据移动瓶颈。我们的目标是了解现代通用PIM架构加速ML训练的潜力。为此，我们（1）在实际通用PIM架构上实现了几种代表性的传统ML算法（即线性回归，逻辑回归，决策树，K-Means聚类），（2）严格评估和表征这些算法的准确性，性能和扩展性，并且（3）与它们在CPU和GPU上的相应实现进行比较。我们在实际内存中心计算平台上的评估表明，与相应的CPU和GPU方法相比，基于PIM的ML训练实现了显着的加速和能量效率。

    Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
    
[^186]: 通过图上的能量理解卷积

    Understanding convolution on graphs via energies. (arXiv:2206.10991v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10991](http://arxiv.org/abs/2206.10991)

    本论文结合能量的概念，证明了带对称滤波器的线性图卷积可以增强高频率，使图神经网络在同质和异质任务中表现更好。

    

    图神经网络（GNN）通常通过消息传递操作，其中节点的状态是基于其邻居收到的信息进行更新的。大多数消息传递模型都是作为图卷积进行操作的，其中特征在被传播到边缘之前通过共享的线性变换混合。在节点分类任务中，图卷积已经表现出两个限制：在heterophilic图上表现欠佳，并且过度平滑。常见的看法是，这两种现象的发生是因为这种模型表现为低通滤波器，意味着在图层间特征的Dirichlet能量会减少，导致平滑效应，最终特征不再可区分。在这项工作中，我们严谨地证明了简单的图卷积模型实际上可以增强高频率甚至引导一种我们所称的过度锐化的渐近行为，与过度平滑相反。我们通过表明对称滤波器的线性图卷积可以被解释为在图形上的能量最小化问题来做到这一点。具体而言，能量函数惩罚高能信号，有效地抑制低频，同时促进相关的高频。我们的结果表明，精心设计的图卷积模型可以在同质和异质任务上提供更好的性能。

    Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with sy
    
[^187]: 一种新颖的物理约束机器学习策略加速非稳态传热和传质模拟

    A novel physics-informed machine learning strategy to accelerate unsteady heat and mass transfer simulations. (arXiv:2206.06817v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2206.06817](http://arxiv.org/abs/2206.06817)

    本研究通过提出一种基于残差的物理约束迁移学习（RePIT）策略，利用ML-CFD交叉计算加速非稳态传热和传质模拟。这种方法可以减小残差的增加，并使用最新的CFD时间序列数据更新网络参数，从而实现长期CFD模拟的可行性。

    

    尽管中央处理器（CPU）的性能迅速提升，但非稳态传热和传质模拟在大型领域中的计算成本非常高。尽管机器学习在加速计算流体力学（CFD）研究方面取得了重大进展，但最近的研究表明，在单次训练方法中，随着训练和预测时间之间的差距增加，完全消除错误增长是不现实的。在本研究中，我们提出了一种基于残差的物理约束迁移学习（RePIT）策略，利用ML-CFD交叉计算加速非稳态传热和传质模拟。我们的假设是，如果定期进行连续的ML-CFD交叉计算，不仅可以减小残差的增加，还可以使用最新的CFD时间序列数据更新网络参数（迁移学习方法），从而实现长期CFD模拟的可行性。

    Despite the rapid advancements in the performance of central processing units (CPUs), the simulation of unsteady heat and mass transfer is computationally very costly, particularly in large domains. While a big wave of machine learning (ML) has propagated in accelerating computational fluid dynamics (CFD) studies, recent research has revealed that it is unrealistic to completely suppress the error increase as the gap between the training and prediction times increases in single training approach. In this study, we propose a residual-based physics-informed transfer learning (RePIT) strategy to accelerate unsteady heat and mass transfer simulations using ML-CFD cross computation. Our hypothesis is that long-term CFD simulations become feasible if continuous ML-CFD cross computation is periodically carried out to not only reduce increased residuals but also update network parameters with the latest CFD time series data (transfer learning approach). The cross point of ML-CFD is determined 
    
[^188]: DeepAD:一种用于实际临床应用的阿尔茨海默病进展鲁棒深度学习模型

    DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications. (arXiv:2203.09096v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09096](http://arxiv.org/abs/2203.09096)

    阿尔茨海默病进展预测中，我们提出了一种新颖的多模态多任务深度学习模型，整合了多个队列中的临床和神经影像数据，通过分析高维MRI特征和其他数据模态来预测患者未来的病情轨迹。

    

    预测患者未来的病情轨迹对于阿尔茨海默病等复杂疾病的药物研发至关重要。然而，大多数用于预测疾病进展的机器学习方法要么是单任务要么是单模型，不能直接应用于包含多任务学习与高维影像的我们的场景。此外，这些方法大多训练于单个数据集（即队列），很难推广到其他队列。我们提出了一种新颖的多模态多任务深度学习模型，通过分析多个队列中的纵向临床和神经影像数据，预测阿尔茨海默病的进展。我们的模型将三维卷积神经网络的高维MRI特征与临床和人口统计学信息等其他数据模态整合，来预测患者未来的病情轨迹。我们的模型引入了对抗损失来缓解队列特异性问题。

    The ability to predict the future trajectory of a patient is a key step toward the development of therapeutics for complex diseases such as Alzheimer's disease (AD). However, most machine learning approaches developed for prediction of disease progression are either single-task or single-modality models, which can not be directly adopted to our setting involving multi-task learning with high dimensional images. Moreover, most of those approaches are trained on a single dataset (i.e. cohort), which can not be generalized to other cohorts. We propose a novel multimodal multi-task deep learning model to predict AD progression by analyzing longitudinal clinical and neuroimaging data from multiple cohorts. Our proposed model integrates high dimensional MRI features from a 3D convolutional neural network with other data modalities, including clinical and demographic information, to predict the future trajectory of patients. Our model employs an adversarial loss to alleviate the study-specifi
    
[^189]: 核分类问题下的错误缩放定律：源条件和容量条件下的研究

    Error Scaling Laws for Kernel Classification under Source and Capacity Conditions. (arXiv:2201.12655v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.12655](http://arxiv.org/abs/2201.12655)

    本文研究了核分类问题中的错误缩放定律，针对满足源条件和容量条件的数据集类别，在高斯设计下导出了误差衰减率与源和容量系数的关系，并对比了最大化间隔支持向量机和岭分类两种方法。

    

    我们考虑核分类问题。尽管某些分类器的最坏情况下样本数量与预测错误的衰减率的边界已知，但它们经常不能准确描述真实数据集的学习曲线。在这项工作中，我们考虑满足标准源条件和容量条件的重要数据集类别，其中包括一些实际数据集，我们通过数值分析证明了这一点。在高斯设计下，我们导出了错误分类（预测）误差的衰减率作为源和容量系数的函数。我们针对两种标准的核分类设置（即最大化间隔支持向量机和岭分类）进行了这样的推导，并对比了两种方法。我们发现我们的衰减率紧密地描述了这类数据集的学习曲线，并且也在实际数据上观察到。我们的结果也可以看作是对核分类的缩放定律指数的显式预测。

    We consider the problem of kernel classification. While worst-case bounds on the decay rate of the prediction error with the number of samples are known for some classifiers, they often fail to accurately describe the learning curves of real data sets. In this work, we consider the important class of data sets satisfying the standard source and capacity conditions, comprising a number of real data sets as we show numerically. Under the Gaussian design, we derive the decay rates for the misclassification (prediction) error as a function of the source and capacity coefficients. We do so for two standard kernel classification settings, namely margin-maximizing Support Vector Machines (SVM) and ridge classification, and contrast the two methods. We find that our rates tightly describe the learning curves for this class of data sets, and are also observed on real data. Our results can also be seen as an explicit prediction of the exponents of a scaling law for kernel classification that is 
    
[^190]: 最优和差分隐私数据获取: 中央和本地机制

    Optimal and Differentially Private Data Acquisition: Central and Local Mechanisms. (arXiv:2201.03968v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2201.03968](http://arxiv.org/abs/2201.03968)

    本文研究了一个平台从具有隐私敏感性用户那里收集数据以估计参数的问题，并提出了最优机制设计问题，以引发用户的真实报告。通过将问题形式化为贝叶斯最优机制设计问题，并使用差分隐私量化异质隐私成本，我们建立了估计误差的下界并推导出最优的估计器和支付方案。

    

    我们考虑一个平台从具有隐私敏感性用户那里收集数据以估计感兴趣的基本参数的问题。我们将这个问题形式化为一个贝叶斯最优机制设计问题，在这个问题中，个体可以分享她的（可验证的）数据以换取货币奖励或服务，但同时具有（私密的）异质隐私成本，我们用差分隐私来量化。我们考虑两种常见的差分隐私设置来为用户提供隐私保证：中央和本地。在两种设置中，我们建立了估计误差的最小二乘下界，并根据给定的不同隐私损失水平为用户推导出（近）最优的估计器。基于这个特征，我们将机制设计问题作为选择最优估计器和支付的问题，在用户报告隐私敏感度时引发真实报告。在隐私敏感度分布的正则条件下，我们开发了高效算法

    We consider a platform's problem of collecting data from privacy sensitive users to estimate an underlying parameter of interest. We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her (verifiable) data in exchange for a monetary reward or services, but at the same time has a (private) heterogeneous privacy cost which we quantify using differential privacy. We consider two popular differential privacy settings for providing privacy guarantees for the users: central and local. In both settings, we establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Building on this characterization, we pose the mechanism design problem as the optimal selection of an estimator and payments that will elicit truthful reporting of users' privacy sensitivities. Under a regularity condition on the distribution of privacy sensitivities we develop efficient alg
    
[^191]: 可迁移的因果条件变化下的时序预测

    Transferable Time-Series Forecasting under Causal Conditional Shift. (arXiv:2111.03422v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.03422](http://arxiv.org/abs/2111.03422)

    本文提出了一个用于半监督领域适应的时序预测模型，通过分析因果结构和引入因果条件转移的假设来处理跨领域时序数据的复杂条件依赖关系。

    

    本文关注于半监督领域适应中的时序预测问题，尽管在实践中经常遇到，但在文献中仍然不够探索。现有的时序领域适应方法主要遵循为静态数据设计的范例，无法处理由数据偏移、时间滞后和变化的数据分布引发的领域特定复杂条件依赖关系。为了解决这些挑战，我们分析了时序数据中的变量条件依赖关系，并发现因果结构通常在领域之间保持稳定，进一步提出了因果条件转移的假设。在这一假设的启示下，我们考虑了时序数据的因果生成过程，并提出了一个端到端模型用于半监督领域适应中的时序预测问题。我们的方法不仅可以发现跨领域数据之间的Granger-因果结构，还可以处理跨领域时序数据的预测问题。

    This paper focuses on the problem of semi-supervised domain adaptation for time-series forecasting, which is underexplored in literatures, despite being often encountered in practice. Existing methods on time-series domain adaptation mainly follow the paradigm designed for the static data, which cannot handle domain-specific complex conditional dependencies raised by data offset, time lags, and variant data distributions. In order to address these challenges, we analyze variational conditional dependencies in time-series data and find that the causal structures are usually stable among domains, and further raise the causal conditional shift assumption. Enlightened by this assumption, we consider the causal generation process for time-series data and propose an end-to-end model for the semi-supervised domain adaptation problem on time-series forecasting. Our method can not only discover the Granger-Causal structures among cross-domain data but also address the cross-domain time-series f
    
[^192]: 用因果学习解释黑箱预测算法的行为

    Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning. (arXiv:2006.02482v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.02482](http://arxiv.org/abs/2006.02482)

    本文提出了一种用于解释黑箱预测算法行为的因果学习方法，通过学习因果图表示来提供因果解释，弥补了现有方法的缺点，即解释单元更加可解释且考虑了宏观级特征和未测量的混淆。

    

    因果学方法在解释黑箱预测模型（例如基于图像像素数据训练的深度神经网络）方面越来越受欢迎。然而，现有方法存在两个重要缺点：（i）“解释单元”是相关预测模型的微观级输入，例如图像像素，而不是更有用于理解如何可能改变算法行为的可解释的宏观级特征；（ii）现有方法假设特征与目标模型预测之间不存在未测量的混淆，这在解释单元是宏观级变量时不成立。我们关注的是在分析人员无法访问目标预测算法内部工作原理的重要情况，而只能根据特定输入查询模型输出的能力。为了在这种情况下提供因果解释，我们提出学习因果图表示，允许更好地理解算法的行为。

    Causal approaches to post-hoc explainability for black-box prediction models (e.g., deep neural networks trained on image pixel data) have become increasingly popular. However, existing approaches have two important shortcomings: (i) the "explanatory units" are micro-level inputs into the relevant prediction model, e.g., image pixels, rather than interpretable macro-level features that are more useful for understanding how to possibly change the algorithm's behavior, and (ii) existing approaches assume there exists no unmeasured confounding between features and target model predictions, which fails to hold when the explanatory units are macro-level variables. Our focus is on the important setting where the analyst has no access to the inner workings of the target prediction algorithm, rather only the ability to query the output of the model in response to a particular input. To provide causal explanations in such a setting, we propose to learn causal graphical representations that allo
    

