# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation.](http://arxiv.org/abs/2307.06333) | 提出了一种交互式框架，通过从用户那里直接获取反馈来识别个性化的无关紧要的概念，从而进行数据增强并获得适应个性化用户目标的政策。 |
| [^2] | [Budgeting Counterfactual for Offline RL.](http://arxiv.org/abs/2307.06328) | 离线强化学习中，通过动态规划的方法限制超出分布动作的数量。 |
| [^3] | [Provably Faster Gradient Descent via Long Steps.](http://arxiv.org/abs/2307.06324) | 本研究通过计算机辅助分析技术，证明了非常数步长策略下的梯度下降方法经过长距离步骤可以实现更快的收敛速度。 |
| [^4] | [Facial Reenactment Through a Personalized Generator.](http://arxiv.org/abs/2307.06307) | 本文提出了一种使用个性化生成器进行面部复原的新方法，通过训练生成器并根据个体的眼睛、鼻子和嘴巴位置来生成保留身份信息的图像。 |
| [^5] | [Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes.](http://arxiv.org/abs/2307.06306) | 本文将随机Polyak步长方法扩展到联邦学习，提出了新的局部自适应和几乎无需调参的FedSPS和FedDecSPS变体。我们证明了当插值条件满足时，FedSPS以线性速度收敛，一般情况下收敛到解的邻域。 |
| [^6] | [Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution.](http://arxiv.org/abs/2307.06304) | NaViT是一个视觉Transformer模型，通过序列打包的方式处理任意分辨率和纵横比的输入图像，提高了训练效率和模型在标准任务上的性能，并在鲁棒性和公平性测试中取得了显著的改进。 |
| [^7] | [Towards a Certified Proof Checker for Deep Neural Network Verification.](http://arxiv.org/abs/2307.06299) | 面向DNN验证的一种新型证明检查器实现，通过提供数值稳定性和更大的可验证性改进现有搜索工具存在的问题。这一实现利用了Imandra的两个关键能力：无限精度实数算术和形式化验证基础设施。 |
| [^8] | [Instruction Mining: High-Quality Instruction Data Selection for Large Language Models.](http://arxiv.org/abs/2307.06290) | 本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。 |
| [^9] | [Rational Neural Network Controllers.](http://arxiv.org/abs/2307.06287) | 神经网络在控制系统中的应用面临鲁棒性和不确定性问题，现有的神经网络架构不适合作为控制器。通过着重分析和设计神经网络控制器，可以解决这一问题。 |
| [^10] | [Tackling Computational Heterogeneity in FL: A Few Theoretical Insights.](http://arxiv.org/abs/2307.06283) | 本论文介绍并分析了一种新的聚合框架，用于处理联邦学习中的计算异质性。该框架涉及到处理异构数据和本地更新，通过理论和实验分析进行了广泛验证。 |
| [^11] | [SpreadNUTS -- Moderate Dynamic Extension of Paths for No-U-Turn Sampling & Partitioning Visited Regions.](http://arxiv.org/abs/2307.06279) | SpreadNUTS是一个逆转采样和划分访问区域的方法，旨在改善MCMC算法的效率和收敛速度。 |
| [^12] | [Exposing the Fake: Effective Diffusion-Generated Images Detection.](http://arxiv.org/abs/2307.06272) | 本文提出了一种名为SeDID的新的扩散生成图像检测方法，该方法结合了统计和神经网络技术，并利用了扩散模型的独特属性，实现了对扩散生成图像的有效检测。实验证明，SeDID在扩散模型上具有优越的性能，对于区分扩散模型生成的图像具有重要贡献，标志着人工智能领域迈出了重要的一步。 |
| [^13] | [Physics-informed Machine Learning for Calibrating Macroscopic Traffic Flow Models.](http://arxiv.org/abs/2307.06267) | 本文提出了一种基于物理信息化的机器学习方法来校准宏观交通流模型，通过将深度自编码器与交通流模型相结合，提高了校准效果。在案例研究中验证了该方法的可行性。 |
| [^14] | [On the hierarchical Bayesian modelling of frequency response functions.](http://arxiv.org/abs/2307.06263) | 结构健康监测(PBSHM)旨在在人群成员之间共享信息以改善健康状态的推断。由于差异和数据丢失所带来的挑战需要解决。 |
| [^15] | [Machine learning and Topological data analysis identify unique features of human papillae in 3D scans.](http://arxiv.org/abs/2307.06255) | 这项研究使用机器学习和拓扑数据分析揭示了人类乳突的几何和拓扑特征的独特性，并且展示了乳突形状的持续同调特征在预测生物变量中的最高效应用。 |
| [^16] | [Identifiability Guarantees for Causal Disentanglement from Soft Interventions.](http://arxiv.org/abs/2307.06250) | 本文研究了从软干预中确保因果分解的可识别性。通过开发一种自编码变分贝叶斯算法，我们展示了在给定一般化的忠诚性概念的情况下，即使存在未观测到的因果变量，仍然可以恢复潜在的因果模型，并在无限数据的极限情况下预测未见组合的干预效果。 |
| [^17] | [Diffusion Based Multi-Agent Adversarial Tracking.](http://arxiv.org/abs/2307.06244) | 本文介绍了CADENCE，一种基于扩散的多智能体对抗追踪方法，通过利用过去的稀疏状态信息生成全面的对手位置预测，并通过蒙特卡洛采样评估了其有效性。 |
| [^18] | [Reconstructing Spatiotemporal Data with C-VAEs.](http://arxiv.org/abs/2307.06243) | 本文通过使用C-VAE模型来生成平滑且逼真的时空演变表示，探索了深度学习技术在重建时空数据方面的潜力。 |
| [^19] | [DSSE: a drone swarm search environment.](http://arxiv.org/abs/2307.06240) | DSSE是一个无人机群集搜索环境，用于研究需要动态概率作为输入的强化学习算法。 |
| [^20] | [Unified Molecular Modeling via Modality Blending.](http://arxiv.org/abs/2307.06235) | MoleBLEND是一种通过对2D和3D分子结构进行统一编码和融合的自监督学习方法，实现了分子表示学习的最新性能表现。 |
| [^21] | [Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior.](http://arxiv.org/abs/2307.06175) | 本文提出了一种分散式部分可观察的场均控制模型（Dec-POMFC），用于解决多智能体强化学习中的分散化、部分可观察和可扩展性等挑战。该模型可将问题简化为可解决的单智能体马尔可夫决策过程，为实现人工集体行为提供了解决方案。 |
| [^22] | [Auxiliary-Tasks Learning for Physics-Informed Neural Network-Based Partial Differential Equations Solving.](http://arxiv.org/abs/2307.06167) | 我们提出了基于辅助任务学习的物理信息神经网络（ATL-PINNs），旨在解决原始PINNs在复杂物理情境中的低准确性和不收敛等问题。我们通过四种不同的辅助任务学习模式和梯度余弦相似度算法，提升了ATL-PINNs的效果。这是首次将辅助任务学习模式引入物理信息学习的研究中。 |
| [^23] | [Deep Generative Models for Physiological Signals: A Systematic Literature Review.](http://arxiv.org/abs/2307.06162) | 本文是对深度生成模型在生理信号研究领域的系统综述，总结了最新最先进的研究进展，有助于了解这些模型在生理信号中的应用和挑战，同时提供了评估和基准测试的指导。 |
| [^24] | [Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions.](http://arxiv.org/abs/2307.06152) | 本文提出了一种无需手工设计奖励函数的自动课程强化学习方法，使代理能够从零开始学习空战中的有效决策。通过逐渐学习一系列从简单到困难的子任务，代理能够在各种状态下做出有效的机动决策。 |
| [^25] | [NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services.](http://arxiv.org/abs/2307.06148) | NetGPT是一个能够在边缘和云端部署适当的大型语言模型的本地AI网络架构，实现了个性化生成服务，并通过协作云边方法论来优化资源协调和互动效果。 |
| [^26] | [Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation.](http://arxiv.org/abs/2307.06125) | 这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。 |
| [^27] | [SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark.](http://arxiv.org/abs/2307.06123) | 本文旨在开发一个综合基准，即MIBench，用于比较不同成员推断攻击。这个基准不仅包括评估指标，还包括评估场景，从不同角度考虑了数据样本之间的距离分布和差异。 |
| [^28] | [Deep learning for dynamic graphs: models and benchmarks.](http://arxiv.org/abs/2307.06104) | 本文对深度学习动态图领域进行了调查，总结了学习时间和空间信息的最新优势，并对最流行的方法进行了公平的性能比较，为评估新架构和方法建立了一个可靠的基准模型。 |
| [^29] | [Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks.](http://arxiv.org/abs/2307.06097) | 本论文提出了一种使用图神经网络学习随机动力系统的隐式正则化方法，通过学习随机微分方程的漂移项和扩散项来捕捉时间序列中的观测到的随机性和空间相关性。实验结果表明，该方法在收敛性、鲁棒性和泛化能力方面表现出优越性。 |
| [^30] | [Online Laplace Model Selection Revisited.](http://arxiv.org/abs/2307.06093) | 本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。 |
| [^31] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^32] | [Interpreting deep embeddings for disease progression clustering.](http://arxiv.org/abs/2307.06060) | 本文提出了一种在疾病进展聚类中解读深度嵌入的新方法，并通过评估2型糖尿病参与者数据集展示了对疾病进展模式的临床意义性见解。 |
| [^33] | [Function-Space Regularization for Deep Bayesian Classification.](http://arxiv.org/abs/2307.06055) | 本研究提出了一种函数空间正则化方法来增加深度贝叶斯分类模型的不确定性量化和对抗性鲁棒性。该方法使用Dirichlet先验在预测空间中进行变分推断，并能与不同模型相结合而不影响模型的架构大小。 |
| [^34] | [Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex Optimization.](http://arxiv.org/abs/2307.06048) | 该论文研究了具有非独立同分布设置的在线库存问题，提出了一个在线算法MaxCOSD，并证明了其在考虑了非独立同分布需求和有状态动态的情况下的有效性 |
| [^35] | [An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes.](http://arxiv.org/abs/2307.06046) | 本论文提出了一种在OOD测试多图中进行链接预测的方法，通过使用双可交换性概念，该方法能够处理具有不同预测模式且可能冲突的关系类型集合的属性多图。 |
| [^36] | [Rhythm Modeling for Voice Conversion.](http://arxiv.org/abs/2307.06040) | 本论文提出了一种名为Urhythmic的无监督节奏转换方法，通过对源语音进行分割和时间拉伸，实现了声音转换中的节奏匹配，实验结果表明该方法在质量和语调方面优于现有方法。 |
| [^37] | [Learning from Exemplary Explanations.](http://arxiv.org/abs/2307.06026) | 本论文介绍了一种基于优秀解释的学习方法，通过使用两个输入实例和其相应的梯度加权类激活映射（GradCAM）模型解释，可以在减少用户交互的情况下改善解释性学习的效果，并在医学图像分类任务中实现了更好的解释和较小的分类性能损失。 |
| [^38] | [An Effective and Efficient Time-aware Entity Alignment Framework via Two-aspect Three-view Label Propagation.](http://arxiv.org/abs/2307.06013) | 本文提出了一个高效且有效的非神经网络时间感知实体对齐框架，通过两方面三视图标签传播、稀疏相似度、Sinkhorn算子和时态迭代学习来提高对齐性能，并减少模型的时间开销。 |
| [^39] | [What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation.](http://arxiv.org/abs/2307.06006) | 本研究通过调查预训练的Vision Transformers和Finetuned版本之间的关系，发现预训练引入了可转移的不变性，在Finetuning过程中，深层的不变性向浅层压缩。 |
| [^40] | [DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification.](http://arxiv.org/abs/2307.06005) | 这篇论文提出了一种名为DDNAS的离散化可微分神经架构搜索方法，用于文本分类。通过使用连续松弛的架构表示和互信息最大化的离散化层，DDNAS在文本表示学习和分类任务中表现优于其他NAS方法。 |
| [^41] | [A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition.](http://arxiv.org/abs/2307.05988) | 该论文对人类活动识别中的自动数据标注技术进行了全面综述，提出了不同方法和解决方案，以解决人工标注的耗时和繁琐问题。 |
| [^42] | [Transformers in Reinforcement Learning: A Survey.](http://arxiv.org/abs/2307.05979) | 本综述介绍了基于Transformer的强化学习的应用。Transformer在解决强化学习中的挑战上具有潜在优势，包括不稳定训练、信用分配、缺乏可解释性和部分可观测性等方面。研究还探讨了其在表示学习、状态转移和奖励函数建模以及策略优化等方面的应用。此外，还有一些最新的研究旨在提高Transformer的可解释性和效率。 |
| [^43] | [Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models.](http://arxiv.org/abs/2307.05977) | 本文介绍了一种名为SDD的方法，用于实现互联网规模的文本到图像扩散模型的安全自蒸馏。该方法通过引导噪声估计与无条件模型匹配，在生成的图像中消除有害内容的比例更大，同时保持整体图像质量，并且允许一次移除多个概念。 |
| [^44] | [Outlier detection in regression: conic quadratic formulations.](http://arxiv.org/abs/2307.05975) | 本论文提出了一种在回归中解决异常值检测问题的方法，通过引入二次锥松弛形式而不是使用big-M约束，这种方法比现有的big-M公式快数个数量级。 |
| [^45] | [Contrastive Learning for Conversion Rate Prediction.](http://arxiv.org/abs/2307.05974) | 对比学习用于转化率预测的框架(CL4CVR)可以利用丰富的无标签数据学习更好的数据表示，并提高转化率预测性能。 |
| [^46] | [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models.](http://arxiv.org/abs/2307.05973) | VoxPoser提出了一种新方法，通过组合3D价值映射和语言模型，实现了机器人在多种操作任务下根据自由形式的指令和对象合成机器人轨迹的能力。 |
| [^47] | [Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models.](http://arxiv.org/abs/2307.05972) | 本研究探讨了事后训练量化和量化感知训练对Transformer语言模型泛化效果的影响，并提出了自学习量化（SDQ）方法，该方法最小化累积量化误差并在多语言模型上优于基准模型。通过在XGLUE基准上的实验证明，SDQ方法可以将模型从32位浮点权重减少到8位整数权重，同时保持高水平的性能。 |
| [^48] | [Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations.](http://arxiv.org/abs/2307.05959) | 使用无标签的人类视频演示增强了眼手协同的视觉运动策略的泛化能力，而无需昂贵的专家演示数据或领域适应方法。 |
| [^49] | [Newell's theory based feature transformations for spatio-temporal traffic prediction.](http://arxiv.org/abs/2307.05949) | 本文提出了一种基于Newell理论的特征转换方法用于时空交通预测，用于改善模型在不同位置的迁移性问题。 |
| [^50] | [Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation.](http://arxiv.org/abs/2307.05948) | 这篇论文提出了一种用于少样本假设适应问题的增强多样性生成网络，通过最小化生成数据的语义特征之间的HSIC值来生成多样化的无标签数据，从而提高学习效果。 |
| [^51] | [A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models.](http://arxiv.org/abs/2307.05946) | 本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。 |
| [^52] | [YOGA: Deep Object Detection in the Wild with Lightweight Feature Learning and Multiscale Attention.](http://arxiv.org/abs/2307.05945) | YOGA是一种基于深度学习的轻量级目标检测模型，采用廉价线性变换进行特征学习，并使用多尺度注意力机制进行特征融合。它在模型大小和准确性之间取得了最佳平衡，适用于低端边缘设备。 |
| [^53] | [Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation.](http://arxiv.org/abs/2307.05926) | 本论文提出了一种使用多维环境自编码器的方法来填补能源数据中的缺失间隙。这个方法可以解决能源系统准确预测和管理的问题，并提高数据在决策和研究中的可用性。 |
| [^54] | [Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt.](http://arxiv.org/abs/2307.05920) | 本论文提出了一种统一的医学图像-文本-标签对比学习方法，通过持续提示策略来解决医学图像-文本预训练中的隐私、样本差异和提示差异等挑战。 |
| [^55] | [Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering.](http://arxiv.org/abs/2307.05915) | 提出了一个名为Prompt Generate Train (PGT)的框架，用于少样本领域适应、对齐和不确定性校准的检索增强生成模型的开发。该框架通过有监督的微调和强化学习，将模型适应到目标领域，并在生成相关答案方面具有与基于GPT-4的模型相竞争的性能。 |
| [^56] | [FIS-ONE: Floor Identification System with One Label for Crowdsourced RF Signals.](http://arxiv.org/abs/2307.05914) | 这项研究提出了FIS-ONE，一种使用一个标签的众包RF信号楼层识别系统。通过信号聚类和聚类索引的步骤，我们证明了仅使用一个底层带有标签的信号样本，以及其余的样本不带标签，也能实现有效的楼层识别。 |
| [^57] | [Grain and Grain Boundary Segmentation using Machine Learning with Real and Generated Datasets.](http://arxiv.org/abs/2307.05911) | 本研究采用机器学习方法，在真实和生成的数据集上训练CNN模型，显著提高了谷物边界分割的准确度，并同时具备了手动分割的准确度和计算方法的高效性。 |
| [^58] | [Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding.](http://arxiv.org/abs/2307.05908) | 本论文提出了一种预测性流水线解码（PPD）方法，通过并行启动后续令牌解码来加速大型语言模型（LLMs）中的贪婪解码过程，同时保持完全相同的输出。该方法在减少解码延迟方面具有潜力，提供了新的LLM解码策略权衡理解。 |
| [^59] | [Mini-Batch Optimization of Contrastive Loss.](http://arxiv.org/abs/2307.05906) | 本文研究了对比学习中小批量优化的理论方面，证明了全部选择小批量与全批量优化等效，提出了一种利用高损失小批量加速随机梯度下降收敛的方法，并通过谱聚类识别高损失小批量。 |
| [^60] | [Stability Guarantees for Feature Attributions with Multiplicative Smoothing.](http://arxiv.org/abs/2307.05902) | 本文提出了一种基于乘法平滑的特征归因方法，通过证明模型的Lipschitz性质，保证了其稳定性，并在视觉和语言模型上进行了评估，显示了非平凡的稳定性保证。 |
| [^61] | [Deep Unrolling for Nonconvex Robust Principal Component Analysis.](http://arxiv.org/abs/2307.05893) | 这个论文提出了一种基于深度展开算法的非凸鲁棒主成分分析算法，能够自动学习超参数并在合成数据集和人脸建模问题上取得更好的数值和视觉性能。 (arXiv:2307.05893v1 [eess.SP]) |
| [^62] | [PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks.](http://arxiv.org/abs/2307.05891) | 该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。 |
| [^63] | [Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment.](http://arxiv.org/abs/2307.05888) | 本论文提出了一种新的数字孪生系统模型，考虑了异构MEC/MCC环境，并基于分布式深度学习提出了一种新的任务卸载方案，实现了高效的实时反馈。 |
| [^64] | [Dynamic Prediction using Time-Dependent Cox Survival Neural Network.](http://arxiv.org/abs/2307.05881) | 该论文通过使用time-dependent Cox模型和神经网络，提出了一种动态预测模型来预测进行性眼部疾病年龄相关性黄斑变性（AMD）的进展。通过使用纵向眼底图像作为输入，该模型可以建立一个个体化的风险预测模型，并且在实证研究中表现出良好的效果。 |
| [^65] | [Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes.](http://arxiv.org/abs/2307.05862) | 部署机器学习存在系统性故障, 即使单个模型在总体上的改善也不能解决这个问题 |
| [^66] | [FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems.](http://arxiv.org/abs/2307.05857) | 这篇论文提出了一种名为FAIRO的算法，用于在人机交互环境中实现公平的顺序决策。该算法将人的行为可变性和偏好变化考虑在内，通过利用选项强化学习框架将复杂的公平任务分解为自适应子任务。 |
| [^67] | [PIGEON: Predicting Image Geolocations.](http://arxiv.org/abs/2307.05845) | PIGEON是一个用于全球规模图像地理定位的多任务端到端系统，通过语义地理单元的创建和精化，以及无监督聚类和ProtoNets的应用，实现了最先进的性能，并提供了预训练的CLIP转换器模型StreetCLIP。 |
| [^68] | [Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing.](http://arxiv.org/abs/2307.05834) | 本文研究了分布式多任务强化学习中的经验共享问题，提出了一种算法DistMT-LSVI，并通过理论和实证研究表明，使用DistMT-LSVI的单个代理可以实现所有任务的ε-最优策略。 |
| [^69] | [Memorization Through the Lens of Curvature of Loss Function Around Samples.](http://arxiv.org/abs/2307.05831) | 本研究通过对损失函数曲率进行分析，研究了神经网络在不同样本上的泛化与记忆化特性。我们发现高曲率的样本通常是具有标签错误或冲突的长尾样本，并在CIFAR100数据集上发现了一种新的失败模型。通过对部分样本进行随机标签错误，我们展示了曲率排序可以有效识别出这些样本。 |
| [^70] | [Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks.](http://arxiv.org/abs/2307.05827) | 使用卷积和记忆网络，在维基百科的表格数据中进行关系抽取。该模型在关系抽取任务中表现出色，并且经过全面的分析和研究，展示了各个模型组件的贡献。 |
| [^71] | [Bayesian taut splines for estimating the number of modes.](http://arxiv.org/abs/2307.05825) | 本研究提出了一种贝叶斯紧系数样条方法，用于估计概率密度函数中模式的数量。该方法结合了核估计器和组合样条，实现了特征探索、模型选择和模式检验，并允许引入专家判断。通过在体育分析中的案例研究中的验证，证明了该方法的实用性。 |
| [^72] | [Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets.](http://arxiv.org/abs/2307.05812) | 本文提出了一种新颖的安全增强学习算法，用于虚拟电力厂在日前电力市场中的战略竞标。该算法不需要精确的市场模型，通过深度确定性策略梯度方法学习具有竞争力的竞标策略，并通过引入安全屏蔽和奖励函数的惩罚机制来考虑虚拟电力厂的物理约束，使代理能够学习到更安全的策略。 |
| [^73] | [Differentiable Forward Projector for X-ray Computed Tomography.](http://arxiv.org/abs/2307.05801) | 本文提出了一个准确的可微分前向和后向投影软件库，以确保预测的图像与原始测量之间的一致性，并支持各种投影几何类型，并且最小化了GPU内存占用。 |
| [^74] | [Machine Learning Study of the Extended Drug-target Interaction Network informed by Pain Related Voltage-Gated Sodium Channels.](http://arxiv.org/abs/2307.05794) | 本研究基于与疼痛相关的钠通道构建了蛋白质相互作用网络，并开发了药物靶标相互作用网络，以发现疼痛管理的潜在首选化合物。 |
| [^75] | [Implicit regularisation in stochastic gradient descent: from single-objective to two-player games.](http://arxiv.org/abs/2307.05789) | 本文研究了隐式正则化在随机梯度下降中的应用，通过向后误差分析构建连续时间流量来量化离散优化器的离散化误差，并提供了一种新的使用BEA的方法。 |
| [^76] | [Making the Nystr\"om method highly accurate for low-rank approximations.](http://arxiv.org/abs/2307.05785) | 本论文提出了一种将Nyström方法用于低秩逼近的高精度方法，通过多种启发式策略使得该方法能够逼近非对称和长宽比不同的矩阵，并提出了快速子集更新策略加速细化过程。 |
| [^77] | [Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test.](http://arxiv.org/abs/2307.05775) | 本文通过系统分析，揭示了$k$-WL的可靠性和有效性问题，并提出了基于基准的表达能力的外延定义和测量。 |
| [^78] | [Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning.](http://arxiv.org/abs/2307.05772) | 这篇论文提出了一种新的随机集合卷积神经网络（RS-CNN）用于分类，通过预测信念函数而不是概率矢量集合，以表示模型的置信度和认识不确定性。基于认识论深度学习方法，该模型能够估计由有限训练集引起的认识不确定性。 |
| [^79] | [Realtime Spectrum Monitoring via Reinforcement Learning -- A Comparison Between Q-Learning and Heuristic Methods.](http://arxiv.org/abs/2307.05763) | 本文比较了线性频率调谐作为启发式方法和强化学习中的Q学习算法在实时频谱监测中的表现，研究表明在非均匀信号活动的场景下，使用Q学习算法的检测率显著高于启发式方法。 |
| [^80] | [Fermat Distances: Metric Approximation, Spectral Convergence, and Clustering Algorithms.](http://arxiv.org/abs/2307.05750) | 本研究分析了Fermat距离的收敛性质和其在聚类算法中的应用。我们证明了离散采样的Fermat距离在小邻域中收敛于它们的连续模拟，同时也证明了基于离散采样的Fermat距离的离散图拉普拉斯算子收敛于连续算子。 |
| [^81] | [Integrating Curricula with Replays: Its Effects on Continual Learning.](http://arxiv.org/abs/2307.05747) | 将课程与回放方法相结合可以提高持续学习的效果，通过调整回放实例与训练数据的交替频率、回放实例的顺序以及选择进入回放缓冲区的策略实现。 |
| [^82] | [GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models.](http://arxiv.org/abs/2307.05735) | GOKU-UI是一种普适推理的SciML生成模型，通过关注机制和多射击训练策略，在包括随机微分方程在内的各种微分方程中展现出了优异的性能表现。它具有显著的数据效率，并在合成和实证数据集上优于所有基线模型。 |
| [^83] | [Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification.](http://arxiv.org/abs/2307.05728) | 本论文提出面向组合分类中多组公平性改善的可扩展解决方案。通过引入任务过条件化和组间交替的技术，在多组多标签设定下实现了恒定比例缩放，并在学术和实际环境中的实验中证明了其有效性。 |
| [^84] | [MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning.](http://arxiv.org/abs/2307.05707) | MoP-CLIP是一种用于领域增量学习的混合Prompt-Tuned CLIP模型，该模型通过模拟每个领域中每个类的特征分布并学习个体化的提示，实现了对分布内外数据的处理和推断。 |
| [^85] | [A Causal Ordering Prior for Unsupervised Representation Learning.](http://arxiv.org/abs/2307.05704) | 该论文提出了一种无监督表示学习的方法，通过考虑具有潜在加性噪声模型的数据生成过程，以及基于潜在分布的海森矩阵的损失函数，鼓励潜在空间遵循因果排序。 |
| [^86] | [Online Ad Procurement in Non-stationary Autobidding Worlds.](http://arxiv.org/abs/2307.05698) | 提出了一个在线学习框架，帮助广告商在非稳态采购环境下动态优化广告平台参数决策。 |
| [^87] | [A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car Pooling Services.](http://arxiv.org/abs/2307.05697) | 本研究提出了GoTogether，一个利用学习排序技术为拼车服务提供个性化推荐的系统。通过分析用户的历史选择，GoTogether能够预测个人共乘的愿望，并提供高成功率的拼车匹配。 |
| [^88] | [Stack More Layers Differently: High-Rank Training Through Low-Rank Updates.](http://arxiv.org/abs/2307.05695) | 本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。 |
| [^89] | [A Survey on Figure Classification Techniques in Scientific Documents.](http://arxiv.org/abs/2307.05694) | 《科技文档中的图形分类技术综述》对图形分类问题进行了系统梳理，包括表格、照片、图表、地图和绘图五类，并批判性地评述了现有方法和数据集，并提出了进一步研究的方向。 |
| [^90] | [Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning.](http://arxiv.org/abs/2307.05643) | 提出了一种使用基于Transformer的深度强化学习方法来优化多目标水电站水库调度问题。实验结果表明，该方法相较于最先进的方法具有更好的运行效果。 |
| [^91] | [ConFL: Constraint-guided Fuzzing for Machine Learning Framework.](http://arxiv.org/abs/2307.05642) | 本文介绍了一种约束引导模糊测试器ConFL，可以从机器学习框架中自动提取约束，并生成能够通过验证和探索更深路径的有效输入。 |
| [^92] | [Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks.](http://arxiv.org/abs/2307.05639) | 本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。 |
| [^93] | [A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions.](http://arxiv.org/abs/2307.05638) | 本综述调查了深度迁移学习在工业时间序列异常检测中的使用。深度迁移学习通过利用相关任务的知识和考虑数据分布的变化，解决了仅有少量或没有附加标记数据情况下的新任务。 |
| [^94] | [Speech Diarization and ASR with GMM.](http://arxiv.org/abs/2307.05637) | 该论文研究了声音日历和自动语音识别（ASR）的主题。通过使用高斯混合模型（GMM）表示语音分段，我们提出了一种新的声音日历方法。我们的研究目标是提高声音日历和ASR的准确性和效率。 |
| [^95] | [Fundamental limits of overparametrized shallow neural networks for supervised learning.](http://arxiv.org/abs/2307.05635) | 本文通过信息论分析，研究了超参数化情况下两层神经网络在监督学习中的基本限制。研究结果通过界限将训练数据的互信息或贝叶斯最优泛化误差与简单线性模型联系起来，并提供了神经网络训练的基本性能限制。证明方法利用严格的自旋玻璃工具和“高斯等效原理”。 |
| [^96] | [Transaction Fraud Detection via an Adaptive Graph Neural Network.](http://arxiv.org/abs/2307.05633) | 本文提出了一种自适应采样和聚合的图神经网络（ASA-GNN）来提高交易欺诈检测的性能，通过学习判别表示，并利用邻居采样策略来过滤噪声节点和补充信息。具体使用余弦相似度和边权重来自适应选择相似行为模式的邻居节点，并找到多跳邻居来检测欺诈交易。 |
| [^97] | [DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks.](http://arxiv.org/abs/2307.05628) | DNAGPT是一个通用的基础模型，通过预训练模型和独特的标记设计，可以适用于任何DNA序列分析任务。它在多个任务上进行了评估，并展示出了良好的性能。 |
| [^98] | [CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction.](http://arxiv.org/abs/2307.05624) | 提出了一种因果启发学习框架（CILF），通过明确定义数据的潜在因果结构，并利用因果特征进行预测来解决轨迹预测中超出分布数据的问题。 |
| [^99] | [A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence.](http://arxiv.org/abs/2307.05623) | 本文提出了一个综合方法，使用深度学习方法推断OD序列的结构，并使用结构约束指导传统的数值优化，解决了交通领域中静态和动态OD矩阵估计中的欠定和滞后挑战。 |
| [^100] | [Latent Space Perspicacity and Interpretation Enhancement (LS-PIE) Framework.](http://arxiv.org/abs/2307.05620) | 本文提出了一个名为LS-PIE的框架，用于提高线性潜在空间的解释能力。该框架通过自动化潜在向量的聚类和排序，从而改善了主成分分析、独立成分分析等线性潜变量模型的可解释性。 |
| [^101] | [Impact of Feature Encoding on Malware Classification Explainability.](http://arxiv.org/abs/2307.05614) | 本文研究了特征编码技术对可解释的人工智能算法的影响。使用恶意软件分类数据集，比较了标签编码和独热编码的性能。发现虽然独热编码会带来轻微性能损失，但可以提供更详细的解释，帮助更全面地理解。使用独热编码还可以减小解释文件大小，缩短人工分析时间。这些结果强调了在XAI研究中重视特征编码技术的重要性，并提出了进一步探索的可能性。 |
| [^102] | [Substance or Style: What Does Your Image Embedding Know?.](http://arxiv.org/abs/2307.05610) | 本文研究了图像嵌入中的非语义信息，设计了一个系统的转换预测任务，并发现六个嵌入可以识别出多个转换。这对于训练算法和基础模型的应用具有重要意义。 |
| [^103] | [Can You Improve My Code? Optimizing Programs with Local Search.](http://arxiv.org/abs/2307.05603) | 这项研究提出了一种利用局部搜索方法改进现有程序的方法，通过固定其余行而改进程序的单个行，以显着改进程序性能，可应用于具有可衡量目标的编程问题。 |
| [^104] | [Unsupervised Domain Adaptation with Deep Neural-Network.](http://arxiv.org/abs/2307.05601) | 本篇论文分析了现有的无监督领域自适应方法，提出了一种新方法，并展示了改进不同领域视觉识别任务的潜力。 |
| [^105] | [Compositional Generalization from First Principles.](http://arxiv.org/abs/2307.05596) | 本论文将组合性泛化视为数据生成过程的属性，通过导出对训练分布支持和模型架构的条件要求，实现了组合性泛化。对于机器学习中的组合性泛化问题提供了理论性的研究基础。 |
| [^106] | [Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification with Transient Experimental Data.](http://arxiv.org/abs/2307.05592) | 本研究提出了一种基于功能主成分分析和深度神经网络的逆UQ过程，用于时间相关响应的模型输入不确定性量化，并通过功能对齐方法解决了PCT时间序列数据中的温度下降问题。 |
| [^107] | [SITTA: A Semantic Image-Text Alignment for Image Captioning.](http://arxiv.org/abs/2307.05591) | SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。 |
| [^108] | [Active Learning for Video Classification with Frame Level Queries.](http://arxiv.org/abs/2307.05587) | 本论文提出了一个新颖的主动学习框架，用于视频分类中的标注减负。这个框架通过自动识别最具信息量的样本来减少人工标注工作量和训练时间。 |
| [^109] | [DBFed: Debiasing Federated Learning Framework based on Domain-Independent.](http://arxiv.org/abs/2307.05582) | DBFed是一个基于领域无关性的去偏差联邦学习框架，解决了在联邦学习中由数据质量差异引起的公平性问题。 |
| [^110] | [RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset.](http://arxiv.org/abs/2307.05563) | RidgeBase是一个跨传感器多指非接触指纹数据集，旨在促进不同匹配场景下非接触指纹匹配的研究。 |
| [^111] | [Review of feedback in Automated Essay Scoring.](http://arxiv.org/abs/2307.05553) | 这篇论文综述了自动化论文评分中的反馈研究，包括不同类型的反馈和论文特征，并回顾了提供反馈的最新案例研究。 |
| [^112] | [Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization.](http://arxiv.org/abs/2307.05551) | 本研究提出了一种基于图神经网络的太赫兹流导向纳米尺度定位方法，可以提高定位精度和覆盖范围，解决了现有方法的定位精度低和无法全局定位的问题。 |
| [^113] | [NLP Meets RNA: Unsupervised Embedding Learning for Ribozymes with Word2Vec.](http://arxiv.org/abs/2307.05537) | 本文将自然语言处理中的无监督学习技术Word2Vec应用于核酶的嵌入学习，实现了对核酶类别的准确分类，并提供了有关核酶的有意义的信息。 |
| [^114] | [Keystroke Dynamics for User Identification.](http://arxiv.org/abs/2307.05529) | 该论文研究了按键动力学在自由文本数据上的多类用户身份识别问题。通过使用复杂的图像特征和多类卷积神经网络，可以获得较高的识别准确率。然而，通过稍微修改特征的随机森林分类器可以获得更高的准确率。 |
| [^115] | [The Ethical Implications of Generative Audio Models: A Systematic Literature Review.](http://arxiv.org/abs/2307.05527) | 这项研究对生成音频模型的伦理影响进行了系统文献综述，发现当前有较少论文讨论负面影响，但其中引发了重大的伦理问题，如欺诈、假脸和侵权潜能。 |
| [^116] | [Toward High-Performance Energy and Power Battery Cells with Machine Learning-based Optimization of Electrode Manufacturing.](http://arxiv.org/abs/2307.05521) | 本研究通过基于机器学习的优化电极制造，提出了一种解决高性能电池电极问题的数据驱动方法，可以在能量和功率应用中反向设计电化学性能的制造过程参数。 |
| [^117] | [Do DL models and training environments have an impact on energy consumption?.](http://arxiv.org/abs/2307.05520) | 本研究分析了模型架构和训练环境对训练更环保的计算机视觉模型的影响，并找出了能源效率和模型正确性之间的权衡关系。 |
| [^118] | [Adaptive Graph Convolution Networks for Traffic Flow Forecasting.](http://arxiv.org/abs/2307.05517) | 本论文提出了一种自适应图卷积网络（AGC-net）用于解决交通流量预测中GNN的时间变化问题，通过自适应图卷积（AGC）和上下文注意机制来考虑时间上下文，以提高预测性能。 |
| [^119] | [Data-Driven Design for Metamaterials and Multiscale Systems: A Review.](http://arxiv.org/abs/2307.05506) | 基于数据驱动的超材料设计为实现下一代具有特殊功能的设备提供了巨大潜力，但其庞大的设计空间和复杂的结构-性能关系是重要的挑战。该综述提供了当前快速发展的领域的综合概述，强调方法论而非特定领域。数据驱动模块包括数据获取、机器学习单元设计和数据驱动多尺度优化，研究方法被基于共享原则进行分类，并讨论了未来研究方向。 |
| [^120] | [HIVA: Holographic Intellectual Voice Assistant.](http://arxiv.org/abs/2307.05501) | HIVA 是一个全息智能语音助手，通过视听效果和3D动画促进人机交互。它提供大学的各种信息，支持"面对面"交流，并提供多个子模块和连接其他应用程序的功能。 |
| [^121] | [Importance of equivariant and invariant symmetries for fluid flow modeling.](http://arxiv.org/abs/2307.05486) | 通过构建一个等变GNN来研究在流体流动建模中等变和不变的对称性的影响，结果显示模拟不变数量能够产生更准确的长期预测，并可从速度场中学习到。 |
| [^122] | [Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models.](http://arxiv.org/abs/2307.05350) | 本文模糊了后解释黑盒模型与构建可解释模型之间的界限，通过从灵活的黑盒模型开始，逐渐引入可解释模型和残差网络，实现了对样本的路由和解释。 |
| [^123] | [Test-Time Training on Video Streams.](http://arxiv.org/abs/2307.05014) | 该论文扩展了测试时培训（TTT）到视频流的设置中，提出了在线TTT方法，相对于固定模型基线和离线TTT，在多个任务上都有显著的性能优势，包括实例和全景分割。 |
| [^124] | [Probabilistic Counterexample Guidance for Safer Reinforcement Learning.](http://arxiv.org/abs/2307.04927) | 本文提出了一种安全强化学习方法，通过引导训练中的反例来解决安全探索的问题，该方法将连续和离散状态空间系统抽象为紧凑的模型，并利用概率性反例生成构建最小化仿真子模型，以揭示安全需求的违反情况。 |
| [^125] | [Weakly-supervised positional contrastive learning: application to cirrhosis classification.](http://arxiv.org/abs/2307.04617) | 本研究提出了一种弱监督定位对比学习策略，在医学应用中使用大量弱标签图像进行肝硬化预测。这种策略将每个2D切片的空间上下文和弱标签整合起来，通过使用通用基于核的损失函数实现高效学习。 |
| [^126] | [Solvent: A Framework for Protein Folding.](http://arxiv.org/abs/2307.04603) | Solvent是一个用于蛋白质折叠的统一研究框架，支持最新模型重要组件的实现和基准测试，并提供对蛋白质结构建模领域的有用见解。 |
| [^127] | [Latent Graph Attention for Enhanced Spatial Context.](http://arxiv.org/abs/2307.04149) | 本文提出了一种称为潜在图注意力（LGA）的模块化框架，用于增强图像中的全局上下文。LGA在计算上简洁且稳定，能够在小规模体系结构中实现接近大规模体系结构的性能，使得轻量级体系结构在边缘设备上更加实用。 |
| [^128] | [Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features.](http://arxiv.org/abs/2307.03544) | 本文提出了一种基于图神经网络的新方法，用于自动罗马数字分析。该方法可以直接处理乐谱中的每个音符，利用音符特征和音符之间的相互依赖关系，并通过新型边缩减算法产生按音的表示。在参考数据集上，ChordGNN模型表现优于现有的最先进模型，具有更高的罗马数字分析准确率。 |
| [^129] | [Distilled Pruning: Using Synthetic Data to Win the Lottery.](http://arxiv.org/abs/2307.03364) | 该论文介绍了一种使用蒸馏数据来修剪深度学习模型的新方法，能够比传统方法更快地找到稀疏的可训练子网络，具有资源高效的神经网络修剪潜力。 |
| [^130] | [Synthesizing Artistic Cinemagraphs from Text.](http://arxiv.org/abs/2307.03190) | 本论文介绍了一种通过文本描述来创建艺术性影动图的自动化方法。通过合成图像双胞胎，即一对艺术图像和与之对齐的真实图像，可以同时满足艺术风格和外观的要求并简化动作分析。同时，利用现有数据集可以准确地分割真实图像并预测合理的运动。 |
| [^131] | [Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain.](http://arxiv.org/abs/2307.03042) | 本研究提出了一种参数高效微调（PEFT）方法，在临床领域使用临床记录训练了一个专门适配临床领域的LLaMA-LoRA模型，同时提出了一个两步PEFT框架，用于将其与Downstream LLaMA-LoRA适配器进行融合，以实现领域适应。 |
| [^132] | [MOPO-LSI: A User Guide.](http://arxiv.org/abs/2307.01719) | MOPO-LSI是一款开源的多目标投资组合优化库，为可持续投资提供用户指南，并介绍了版本1.0的问题设置、工作流程和超参数。 |
| [^133] | [ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting.](http://arxiv.org/abs/2307.01227) | ESGCN是一种用于交通流量预测的边缘压缩注意图卷积网络，通过建模时空动态和引入边缘特征和边缘注意机制来提高预测的准确性。 |
| [^134] | [Neural Polytopes.](http://arxiv.org/abs/2307.00721) | 使用ReLU激活的简单神经网络可以在不同维度中逼近单位球形，生成神经多面体，这一发现开启了利用机器学习来逼近曲面的生成离散几何研究。 |
| [^135] | [Reduce Computational Complexity for Convolutional Layers by Skipping Zeros.](http://arxiv.org/abs/2306.15951) | 本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。 |
| [^136] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^137] | [A Bayesian Take on Gaussian Process Networks.](http://arxiv.org/abs/2306.11380) | 该论文提出了一种基于高斯过程和贝叶斯方法的网络模型，通过蒙特卡罗和马尔可夫链蒙特卡罗方法采样网络结构的后验分布。该方法在恢复网络的图形结构方面优于最先进的算法，并提供了后验概率的准确近似。 |
| [^138] | [Evaluating the Zero-shot Robustness of Instruction-tuned Language Models.](http://arxiv.org/abs/2306.11270) | 本文评估了指导微调语言模型的零样本鲁棒性，并发现使用新颖但合适的指导措辞会降低模型性能。 |
| [^139] | [Sparse Modular Activation for Efficient Sequence Modeling.](http://arxiv.org/abs/2306.11197) | 本论文引入了稀疏模块激活 (SMA) 机制，用于高效的序列建模。这种机制可以动态地稀疏激活序列元素的子模块，减少计算和内存消耗。 |
| [^140] | [MARBLE: Music Audio Representation Benchmark for Universal Evaluation.](http://arxiv.org/abs/2306.10548) | 本论文介绍了MARBLE，一个音乐音频表征通用评估基准，它为音乐理解领域的研究和发展提供了一个全面和可持续性的基础，并提供各种音乐信息检索（MIR）任务的基准。 |
| [^141] | [Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking.](http://arxiv.org/abs/2306.10453) | 评估了图神经网络在链接预测任务中的应用，并提出了一种基于启发式相关采样技术的实用评估设置，克服了当前存在的问题。 |
| [^142] | [Deep Generative Models for Decision-Making and Control.](http://arxiv.org/abs/2306.08810) | 本论文研究了基于深度模型的强化学习方法在决策和控制问题中的缺点，并提出了解决方法。同时，将当代生成建模工具中的推理技术重新解释为强化学习问题的规划策略。 |
| [^143] | [Collaborative Robotic Biopsy with Trajectory Guidance and Needle Tip Force Feedback.](http://arxiv.org/abs/2306.07129) | 这项研究介绍了一个具有轨迹引导和针尖力反馈的协作机器人活检系统，通过结合机器人的导航和医生的实时控制，提供了针头放置的辅助。研究中提出了一种能够实时感知针尖力的针头设计，通过此系统可以改善针头的定位，提高活检的准确性。 |
| [^144] | [Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory.](http://arxiv.org/abs/2306.04026) | 本研究将控制理论中的验证方法应用于强化学习中的价值函数，提出了新的度量方法以验证安全控制任务中的价值函数，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。 |
| [^145] | [Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret.](http://arxiv.org/abs/2306.03372) | 本文提出了在线黎曼梯度下降算法，用于在在线情况下估计潜在的低秩张量。其中，我们在处理连续或分类变量时提供了灵活的方法，并在在线情况下尝试了两个具体的应用，即在线张量补全和在线二元张量学习。我们还建立了逐个条目的精确错误界限，这是在在线张量补全中首次纳入噪声。我们观察到，在存在噪声的情况下，计算和统计方面存在着令人惊讶的权衡。 |
| [^146] | [A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition.](http://arxiv.org/abs/2306.02422) | 本研究提出了一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法，即GALET，可以用于解决非凸下层目标的双层问题。该方法可以在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现问题的$\epsilon$-静态度量。 |
| [^147] | [Identifying Subgroups of ICU Patients Using End-to-End Multivariate Time-Series Clustering Algorithm Based on Real-World Vital Signs Data.](http://arxiv.org/abs/2306.02121) | 本研究使用真实世界生命体征数据开发了一种端到端多元时间序列聚类算法，并发现不同子群之间存在着不同的ICU和医院死亡率风险。 |
| [^148] | [Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya.](http://arxiv.org/abs/2305.19779) | 本研究提出了一种利用aggVAE进行深度学习和MCMC处理行政边界变化的解决方案，可以更准确地映射以县为层级的聚合级别数据，并处理行政边界的变化，相比最先进的模型表现更好。 |
| [^149] | [DreamWaltz: Make a Scene with Complex 3D Animatable Avatars.](http://arxiv.org/abs/2305.12529) | DreamWaltz是一个新颖的框架，可以根据文本指导和参数化的人体先验生成和动画化复杂的3D角色。它提出了三维一致的遮挡感知得分蒸馏采样（SDS）来优化隐式神经表示和规范姿势，并学习了一种可动画且具有泛化能力的角色表示。这种方法在创建3D角色方面表现出了高效性和稳健性。 |
| [^150] | [ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification.](http://arxiv.org/abs/2305.04003) | 本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。 |
| [^151] | [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages.](http://arxiv.org/abs/2305.02309) | 本研究旨在提高LLMs编程合成训练的效率，通过统一模型架构、学习方法、填充采样和数据分布，来提高训练模型的泛化能力。 |
| [^152] | [DataComp: In search of the next generation of multimodal datasets.](http://arxiv.org/abs/2304.14108) | DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。 |
| [^153] | [Matching-based Data Valuation for Generative Model.](http://arxiv.org/abs/2304.10701) | 本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。 |
| [^154] | [Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization.](http://arxiv.org/abs/2304.08309) | 本论文研究了在线性化Laplace逼近(LLA)在Bayesian optimization中的应用。虽然LLA在构建贝叶斯神经网络时已被证明具有效性和高效性，但是在序列决策问题中，需要考虑其可能的局限性。 |
| [^155] | [Pointwise convergence theorem of gradient descent in sparse deep neural network.](http://arxiv.org/abs/2304.08172) | 本文研究了稀疏深度神经网络中梯度下降的点对点收敛定理，针对非光滑指示函数构造了一种特殊形状的DNN，实现了梯度下降过程的点对点收敛。 |
| [^156] | [CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments.](http://arxiv.org/abs/2304.06848) | 本文提出了一种新的因果关系在线POMDP规划方法CAR-DESPOT，使用因果建模和推理来消除未测量混淆变量引起的错误，并在混杂环境中表现优异。 |
| [^157] | [GPT detectors are biased against non-native English writers.](http://arxiv.org/abs/2304.02819) | 该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。 |
| [^158] | [DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks.](http://arxiv.org/abs/2303.04878) | DeepGD是一种用于深度神经网络的多目标黑盒测试选择方法，通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。 |
| [^159] | [Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement.](http://arxiv.org/abs/2303.01388) | 本论文介绍了一种利用多智能体深度强化学习的标签放置方法，该方法在数据可视化中解决了标签重叠和可读性问题，与现有的手工设计算法相比具有显著的优势。 |
| [^160] | [Supervised topological data analysis for MALDI mass spectrometry imaging applications.](http://arxiv.org/abs/2302.13948) | 这项研究提出了一个新的代数拓扑框架，通过从MALDI数据中获得内在信息并转化为反映拓扑持久性的形式，实现了在肺癌亚型分类中的噪音信号区分和数据压缩的功能。 |
| [^161] | [On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process.](http://arxiv.org/abs/2302.13152) | 本文研究了安全限制马尔可夫决策过程，强调了贝尔曼最优性原理在具有多链结构的受限马尔可夫决策问题中可能不成立。通过将多目标优化问题表示为新的优化框架，解决了这个问题。 |
| [^162] | [Linearization Algorithms for Fully Composite Optimization.](http://arxiv.org/abs/2302.12808) | 本文提出了针对完全复合优化问题的线性化算法。通过将目标函数的可微和不可微部分分开处理，仅线性化平滑部分，从而推广了Frank-Wolfe方法和Conditional Gradient Sliding算法，适用于一类不可微的问题。算法基于一个更强的线性最小化预言子版本，可在多个实际应用中高效实现，并提供了凸和非凸目标的全局收敛速度分析。另外，在凸情况下，还提出了一个加速方法以改善复杂度。通过说明性实验，验证了理论结果的有效性。 |
| [^163] | [From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning.](http://arxiv.org/abs/2302.12559) | 本论文研究了将差分隐私机器学习算法视为有噪声的不动点迭代，从而从这一框架中推导出隐私和效用结果。通过利用这个新的视角，我们恢复了流行的私有梯度下降方法并提供了一种有原则的方法来设计和分析新的私有优化算法。我们通过通用框架推导出了用于集中式、联合和完全去中心化学习的新颖私有ADMM算法，并通过迭代和子采样的隐私放大建立了强隐私保证。最后，我们利用最近的有噪声固定点迭代的线性收敛结果提供效用保证。 |
| [^164] | [Improved uncertainty quantification for neural networks with Bayesian last layer.](http://arxiv.org/abs/2302.10975) | 本文提出了一种改进神经网络不确定性量化的方法，使用贝叶斯最后一层近似不可处理的贝叶斯神经网络，通过最大化边际来获得权重的点估计。 |
| [^165] | [One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data.](http://arxiv.org/abs/2302.06375) | 本研究提出了一种Transformer架构，用于表示具有时间相关的异构表格数据，通过使用一组频率函数来表示数值特征，并采用唯一的损失函数进行统一训练。 |
| [^166] | [Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds.](http://arxiv.org/abs/2302.02988) | 本文研究了最小化期望简单遗憾的固定预算最优臂识别问题。通过推导最坏情况期望简单遗憾的渐进下界，提出了基于HIR估计的TS-HIR策略，该策略在推荐最优臂时表现出近似最优性。 |
| [^167] | [Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses.](http://arxiv.org/abs/2302.01241) | 本文提出了一种图解化的方法，以支持可解释的人工智能，通过图解型和假设性推理，缩小可解释性差距。通过临床应用研究和建模研究，我们发现DiagramNet不仅能提供忠实的杂音形状解释，还具有较好的预测性能，而且图解型解释在临床相关的情况下更受推崇。 |
| [^168] | [MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows.](http://arxiv.org/abs/2302.01075) | 本文提出了一个新的GAN生成建模框架MonoFlow，通过Wasserstein梯度流获得理论洞见和算法启示。该框架使用密度比例的单调递增映射重新缩放粒子演化，并通过训练鉴别器获得MonoFlow的向量场，利用相应的向量场进行粒子流的生成。 |
| [^169] | [Privacy Risk for anisotropic Langevin dynamics using relative entropy bounds.](http://arxiv.org/abs/2302.00766) | 本文通过建立相对熵边界，利用Fokker-Planck方程的稳定性估计结果，研究了各向异性噪声情况下的隐私风险。 |
| [^170] | [Plugin estimators for selective classification with out-of-distribution detection.](http://arxiv.org/abs/2301.12386) | 这篇论文提出了一种新的插件估计器方法，用于有选择性分类和外部分布检测（SCOD）问题，并且在理论上有基础，有效性高，可以推广现有方法。 |
| [^171] | [A Deep Learning Method for Comparing Bayesian Hierarchical Models.](http://arxiv.org/abs/2301.11873) | 这个论文提出了一种深度学习方法，用于比较贝叶斯层次模型。该方法通过支持分摊推断，能够高效地进行模型比较和性能验证。同时，作者还对四个层次证据积累模型进行了比较。 |
| [^172] | [A data science and machine learning approach to continuous analysis of Shakespeare's plays.](http://arxiv.org/abs/2301.06024) | 通过数据科学和机器学习的方法，对莎士比亚的作品进行了连续分析，发现他的写作风格随着时间的推移发生了变化，其中包括句子长度、形容词和副词的频率以及文本中表达的情感。此外，研究还发现，部分戏剧的风格特征更类似于其写作时间之前或之后的作品。 |
| [^173] | [In and Out-of-Domain Text Adversarial Robustness via Label Smoothing.](http://arxiv.org/abs/2212.10258) | 本文研究了标签平滑策略在不同领域的自然语言处理任务中对对抗鲁棒性的影响。实验证明，标签平滑显著提高了预训练模型的鲁棒性，并减少了在对抗样本上出现的过度自信错误。 |
| [^174] | [Towards Fleet-wide Sharing of Wind Turbine Condition Information through Privacy-preserving Federated Learning.](http://arxiv.org/abs/2212.03529) | 本文提出了一种分布式联邦机器学习方法，通过数据隐私保护，启用风力涡轮机队本地数据的船队范围学习，解决风力涡轮机制造商数据隐私的问题，提供改进数据驱动的涡轮机运维策略并减少停机时间的机会。 |
| [^175] | [Closing the gap between SVRG and TD-SVRG with Gradient Splitting.](http://arxiv.org/abs/2211.16237) | 本论文通过将TD学习视为适当选择函数的梯度分割，将TD和SVRG相结合，实现了具有几何收敛速度的策略评估方法，并在理论和实验上得到了支持。 |
| [^176] | [Bidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model.](http://arxiv.org/abs/2211.10590) | 本论文提出了一种新颖的多模态分子预训练模型，通过将结构和生化性质的模态结合，使得模型能够将分子的结构和性质之间的双向信息联系起来，并能够处理多模态和单模态的下游任务。 |
| [^177] | [What Images are More Memorable to Machines?.](http://arxiv.org/abs/2211.07625) | 本论文研究了如何测量和预测图像对机器的记忆性，并发现“复杂”图像通常对机器记忆更加深刻。通过详细分析和实验验证，我们提出了机器记忆性的概念，并为机器记忆和视觉数据的研究方向开辟了新的可能性。 |
| [^178] | [Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets.](http://arxiv.org/abs/2211.04125) | 本文描述了一项多中心研究，旨在评估MRI数据协调在机器学习中的效用;作者提出了一种“调和器变压器”方法，在不泄露信息的前提下，在机器学习的预处理步骤中实现了数据协调。 |
| [^179] | [Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI.](http://arxiv.org/abs/2211.01753) | 本论文介绍了LADDER，这是一个知识提取框架，可以从网络威胁情报中提取攻击模式。该框架可以帮助安全分析师及时确定现有和新兴威胁的攻击向量，从而使他们能够主动准备防御措施。 |
| [^180] | [Harnessing the Power of Explanations for Incremental Training: A LIME-Based Approach.](http://arxiv.org/abs/2211.01413) | 本研究提出了一种基于LIME的方法，利用解释来改善神经网络模型性能。通过引入自定义加权损失，将真实LIME解释与模型预测的LIME解释之间的距离考虑在内，帮助模型更好地泛化。此外，该研究还提出了一种适用于实际训练场景的解决方案，以帮助模型顺序学习而不丢失先前数据分布信息。 |
| [^181] | [Contrastive Decoding: Open-ended Text Generation as Optimization.](http://arxiv.org/abs/2210.15097) | 对比解码是一种可靠的解码方法，它通过优化对比目标来生成合理且高质量的文本，与仅使用较大的语言模型进行解码相比，对比解码能够避免短而重复的文本和不连贯的文本，并适用于不同的模型规模。 |
| [^182] | [Active Learning for Single Neuron Models with Lipschitz Non-Linearities.](http://arxiv.org/abs/2210.13601) | 该论文提出一种针对具有利普希茨非线性的单个神经元模型的主动学习策略，该策略在敌对标签噪声下拟合线性函数，并在逼近保证方面具有强有力的可证明性能。 |
| [^183] | [Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning.](http://arxiv.org/abs/2210.09452) | 本论文提出了一个新的框架，通过迭代的自适应有监督对比学习，解决了多实例学习中的类别不平衡问题，并在医学数据集上取得了良好的结果。 |
| [^184] | [Distributionally Adaptive Meta Reinforcement Learning.](http://arxiv.org/abs/2210.03104) | 本论文介绍了一种基于自适应分布鲁棒性的元强化学习算法框架，该框架能够在测试任务分布发生变化时行为适应，提供了对分布转变的改善遗憾的能力。 |
| [^185] | [Polysemanticity and Capacity in Neural Networks.](http://arxiv.org/abs/2210.01892) | 该论文通过分析特征容量来理解神经网络中的多义性现象，发现在最优的容量分配下，神经网络倾向于单义地表示重要特征，多义地表示次重要特征，并忽略最不重要的特征。多义性现象在输入具有更高的峰度或稀疏性时更为普遍，并且在某些体系结构中比其他体系结构更为普遍。此外，作者还发现了嵌入空间中的分块半正交结构，不同模型中的分块大小不同，突出了模型体系结构的影响。 |
| [^186] | [Sparsity by Redundancy: Solving $L_1$ with SGD.](http://arxiv.org/abs/2210.01212) | 该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。 |
| [^187] | [A large sample theory for infinitesimal gradient boosting.](http://arxiv.org/abs/2210.00736) | 本研究研究了无穷小梯度提升在大样本极限下的渐近性质，证明了其收敛到一个确定性过程，并探讨了其使得测试误差减小的动力学以及其长时间行为。 |
| [^188] | [Green, Quantized Federated Learning over Wireless Networks: An Energy-Efficient Design.](http://arxiv.org/abs/2207.09387) | 本文提出了一种绿色量化的无线网络联合学习框架，通过使用量化神经网络来实现有限精度级别的数据表示。针对能量消耗和通信轮次数量的最小化，提出了一个多目标优化问题，并分析推导了系统的收敛速度。 |
| [^189] | [Towards a More Rigorous Science of Blindspot Discovery in Image Models.](http://arxiv.org/abs/2207.04104) | 本文介绍了一种新的BDM（盲点发现方法）评估框架SpotCheck和一个使用2D图像表示的BDMPlaneSpot。实验结果给出影响BDM性能的因素，证明PlaneSpot与现有的BDM相竞争，在许多情况下表现更好。 |
| [^190] | [Dynamic mean field programming.](http://arxiv.org/abs/2206.05200) | 本文发展了一种动态均场规划方法，用于有限状态和行为的贝叶斯强化学习。通过模拟统计物理中的概念，研究了贝尔曼方程作为一种无序动力学系统，并通过均场方程计算状态行为值的统计信息。 |
| [^191] | [Self-Supervised Anomaly Detection: A Survey and Outlook.](http://arxiv.org/abs/2205.05173) | 自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文全面综述了当前自监督异常检测方法的技术细节，并讨论了它们的优势和缺点，同时比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。 |
| [^192] | [VAE-Loco: Versatile Quadruped Locomotion by Learning a Disentangled Gait Representation.](http://arxiv.org/abs/2205.01179) | 本论文通过学习一个解缠的步态表示来解决四足机器人步态参数无法在空中变化的问题，实现了多样化的连续合成步态，增加了控制器的稳健性。 |
| [^193] | [High Dimensional Quantum Machine Learning With Small Quantum Computers.](http://arxiv.org/abs/2203.13739) | 本研究探索了使用小型量子计算机进行高维量子机器学习的可能性，并且通过构建一个机器学习模型成功地在数字识别任务中实现了利用更少的电路评估来近似较大电路的输出。 |
| [^194] | [Fundamental Limits for Sensor-Based Robot Control.](http://arxiv.org/abs/2202.00129) | 该论文提出了一个新的方法，利用信息论和动态规划技术，为基于传感器的机器人控制建立了性能的基本限制。通过计算任务相关信息的数量，可以获得一步决策任务的最高可实现预期报酬的上界，并通过动态规划将这个上界扩展到多步问题。实验证明了该方法在三个示例上的有效性，包括部分可观察性决策过程、自由落体物体的捕捉和使用深度传感器的避障问题。 |
| [^195] | [On Complexity of 1-Center in Various Metrics.](http://arxiv.org/abs/2112.03222) | 在本文中，研究了不同度量下1-中心问题的复杂性。小d时，在假设命中集合猜想（HSC）成立的情况下，无法通过任何lp度量或编辑或Ulam度量实现1-中心问题的次二次算法。大d时，在假设Quantified SETH的情况下，排除了基于编辑度量中的1-中心问题的次四次算法，同时给出了Ulam度量中1-中心问题的（1+ε）逼近算法。 |
| [^196] | [Empowering General-purpose User Representation with Full-life Cycle Behavior Modeling.](http://arxiv.org/abs/2110.11337) | 本研究提出了一种名为全生命周期用户表示模型（LURM）的新框架，能够对超长行为序列进行全生命周期建模。该模型由兴趣包编码和自监督多锚点编码网络组成，能够生成适用于各种下游用户认知任务的通用表示。 |
| [^197] | [The Deep Generative Decoder: MAP estimation of representations improves modeling of single-cell RNA data.](http://arxiv.org/abs/2110.06672) | Deep Generative Decoder (DGD) is a simple generative model that uses MAP estimation to compute model parameters and representations. It outperforms variational autoencoders (VAEs) by handling complex parameterized latent distributions and learning meaningful and well-structured latent representations on single-cell RNA data, including sub-clustering beyond provided labels. |
| [^198] | [Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation.](http://arxiv.org/abs/2106.10865) | 多类分类中的良性过拟合问题进行了研究，提出了一个简单的确定性条件，当前三种算法在满足条件时会得到插值数据并具有相等准确率的分类器。 |
| [^199] | [Regularization of Mixture Models for Robust Principal Graph Learning.](http://arxiv.org/abs/2106.09035) | 本文提出了一种正则化的混合模型方法，用于从数据分布中学习主图，能够处理脊检测中的流形学习问题以及异常值和异方差等问题。 |
| [^200] | [Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness.](http://arxiv.org/abs/2104.05942) | 本文介绍了一种新型非线性动力学模型——递归均衡网络（RENs），它具有稳定性和鲁棒性的“内置”保证。RENs具有很强的灵活性，能表示多种系统，并用参数化方法简化了学习过程。 |
| [^201] | [Fast Rates for the Regret of Offline Reinforcement Learning.](http://arxiv.org/abs/2102.00479) | 本文研究了离线数据对强化学习的遗憾，提出了精细的收敛速率分析，揭示了离线强化学习收敛速度较快的现象，并通过指数形式的加速机制加快了收敛速度。 |
| [^202] | [B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows.](http://arxiv.org/abs/2101.10870) | B-HAR是一个开源的基准框架，用于深入研究人体活动识别的数据集和工作流程，解决了HAR方法缺乏标准工作流程的问题，并提供了可配置的框架来评估模式识别模型的质量。 |
| [^203] | [Deep Learning based Uncertainty Decomposition for Real-time Control.](http://arxiv.org/abs/2010.02613) | 通过使用深度学习技术，在实时控制中提出了一种新颖的方法，用于检测认知不确定性，该方法可以在未知环境中的数据驱动控制中实现安全和高效的探索。 |
| [^204] | [Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning.](http://arxiv.org/abs/1910.06151) | 我们提出了一个基于采样的亚线性低秩矩阵算法框架，用于去量化量子机器学习，推广了Tang的量子启发算法的一系列结果。基于量子线性代数算法和量子奇异值变换框架，我们开发了经典的SVT算法，运行时间与输入维度无关，证明了量子SVT无法实现指数级的量子加速。我们的结果足以推广去量化量子机器学习算法的所有最近研究成果。 |

# 详细

[^1]: 诊断、反馈、适应性: 用于测试时政策调整的人-机环路框架

    Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])

    [http://arxiv.org/abs/2307.06333](http://arxiv.org/abs/2307.06333)

    提出了一种交互式框架，通过从用户那里直接获取反馈来识别个性化的无关紧要的概念，从而进行数据增强并获得适应个性化用户目标的政策。

    

    政策常常由于分布偏移而失效——即当政策在新环境中部署时，状态和奖励发生变化。数据增强可以通过使模型对与任务无关的变化具有不变性来增加鲁棒性。然而，设计者在事先往往不知道哪些概念是无关紧要的，尤其是当不同的最终用户对任务执行方式有不同的偏好时。我们提出了一个互动框架，通过直接从用户那里获得反馈来识别个性化的无关紧要的概念。我们的核心思想是生成反事实演示，使用户能够快速确定可能与任务相关和无关的概念。然后利用无关紧要的概念的知识进行数据增强，从而获得适应于个性化用户目标的政策。我们在离散和连续控制任务上进行实验证实了我们的框架。我们的方法(1)使用户能够……

    Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments. Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation. However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed. We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts. Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts. The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives. We present experiments validating our framework on discrete and continuous control tasks with real human users. Our method (1) enables users to 
    
[^2]: Offline RL的预算反事实推理

    Budgeting Counterfactual for Offline RL. (arXiv:2307.06328v1 [cs.LG])

    [http://arxiv.org/abs/2307.06328](http://arxiv.org/abs/2307.06328)

    离线强化学习中，通过动态规划的方法限制超出分布动作的数量。

    

    离线强化学习的主要挑战在于数据有限的情况下，由于潜在动作领域内的反事实推理困境所引起：如果我们选择了不同的行动会怎么样？这些情况通常会导致指数级累积的外推误差。因此，认识到并不是所有的决策步骤对最终结果都同样重要，并在政策制定中预算反事实决策的数量以控制外推是至关重要的。与现有方法在政策或值函数上使用规则化不同，我们提出了一种方法来明确限制训练期间的超出分布动作的数量。具体而言，我们的方法利用动态规划来决定在哪里进行外推和在哪里不进行外推，并且对决策的上限不同于行为策略。它在潜在改进的潜力和外推控制之间进行平衡。

    The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvemen
    
[^3]: 经过长距离步骤的梯度下降的可证明更快收敛速度

    Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])

    [http://arxiv.org/abs/2307.06324](http://arxiv.org/abs/2307.06324)

    本研究通过计算机辅助分析技术，证明了非常数步长策略下的梯度下降方法经过长距离步骤可以实现更快的收敛速度。

    

    本研究通过计算机辅助分析技术，建立了经过长距离步骤的梯度下降的可证明更快收敛速度。我们的理论允许非常数步长策略，通过分析多次迭代的整体效果而不是典型的一次迭代归纳使用的，从而有可能破坏下降。我们表明，长距离步骤，可能在短期内增加目标值，但在长期内带来更快的收敛速度。此外，我们还提出了一个关于梯度下降更快收敛速度的猜想，并进行了简单的数值验证。

    This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
    
[^4]: 个性化生成器在面部复原中的应用

    Facial Reenactment Through a Personalized Generator. (arXiv:2307.06307v1 [cs.CV])

    [http://arxiv.org/abs/2307.06307](http://arxiv.org/abs/2307.06307)

    本文提出了一种使用个性化生成器进行面部复原的新方法，通过训练生成器并根据个体的眼睛、鼻子和嘴巴位置来生成保留身份信息的图像。

    

    近年来，图像生成模型在面部复原中的作用逐渐增加。这些模型通常不依赖于特定的个体，并且是在广泛的数据集上训练的。然而，这种方法往往无法完全捕捉到个体整体外貌，导致生成的图像不够准确。最近的进展使得训练个性化生成模型成为可能。本文提出了一种使用个性化生成器进行面部复原的新方法。我们使用一台普通相机拍摄的一段短而多样化的自拍视频中的帧来训练生成器。由个性化生成器合成的图像将保留个体的身份信息。我们的工作基于这样一个前提，即复原任务可以准确地模仿头部姿势和表情。为此，我们定位到的是个体的眼睛、鼻子和嘴巴的位置来进行面部操作。

    In recent years, the role of image generative models in facial reenactment has been steadily increasing. Such models are usually subject-agnostic and trained on domain-wide datasets. The appearance of the reenacted individual is learned from a single image, and hence, the entire breadth of the individual's appearance is not entirely captured, leading these methods to resort to unfaithful hallucination. Thanks to recent advancements, it is now possible to train a personalized generative model tailored specifically to a given individual. In this paper, we propose a novel method for facial reenactment using a personalized generator. We train the generator using frames from a short, yet varied, self-scan video captured using a simple commodity camera. Images synthesized by the personalized generator are guaranteed to preserve identity. The premise of our work is that the task of reenactment is thus reduced to accurately mimicking head poses and expressions. To this end, we locate the desir
    
[^5]: 通过随机Polyak步长的局部自适应联邦学习

    Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes. (arXiv:2307.06306v1 [cs.LG])

    [http://arxiv.org/abs/2307.06306](http://arxiv.org/abs/2307.06306)

    本文将随机Polyak步长方法扩展到联邦学习，提出了新的局部自适应和几乎无需调参的FedSPS和FedDecSPS变体。我们证明了当插值条件满足时，FedSPS以线性速度收敛，一般情况下收敛到解的邻域。

    

    最先进的联邦学习算法，如FedAvg，需要精心调整的步长才能达到最佳性能。现有自适应联邦方法提出的改进仅涉及额外的超参数调整，如动量参数，并且仅考虑在服务器聚合轮次中的适应性，而不是局部的。这些方法在许多实际场景下效率低下，因为它们需要过多的超参数调整，并且不能捕捉局部几何信息。本文将最近提出的随机Polyak步长方法扩展到联邦学习环境，并提出了新的局部自适应和几乎无需调参的分布式SPS变体（FedSPS和FedDecSPS）。我们证明当插值条件（过参数化）满足时，FedSPS在强凸和凸设置中以线性速度收敛，一般情况下收敛到解的邻域。

    State-of-the-art federated learning algorithms such as FedAvg require carefully tuned stepsizes to achieve their best performance. The improvements proposed by existing adaptive federated methods involve tuning of additional hyperparameters such as momentum parameters, and consider adaptivity only in the server aggregation round, but not locally. These methods can be inefficient in many practical scenarios because they require excessive tuning of hyperparameters and do not capture local geometric information. In this work, we extend the recently proposed stochastic Polyak stepsize (SPS) to the federated learning setting, and propose new locally adaptive and nearly parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that FedSPS converges linearly in strongly convex and sublinearly in convex settings when the interpolation condition (overparametrization) is satisfied, and converges to a neighborhood of the solution in the general case. We extend our proposed method t
    
[^6]: Patch n' Pack: NaViT,一个适用于任意纵横比和分辨率的视觉Transformer

    Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. (arXiv:2307.06304v1 [cs.CV])

    [http://arxiv.org/abs/2307.06304](http://arxiv.org/abs/2307.06304)

    NaViT是一个视觉Transformer模型，通过序列打包的方式处理任意分辨率和纵横比的输入图像，提高了训练效率和模型在标准任务上的性能，并在鲁棒性和公平性测试中取得了显著的改进。

    

    在计算机视觉模型中，将图像调整为固定分辨率后进行处理是普遍且明显次优的选择。然而，像Vision Transformer（ViT）这样的模型提供了灵活的基于序列的建模，因此可以处理不同长度的输入序列。我们利用这一点，使用称为NaViT（Native Resolution ViT）的模型，在训练过程中进行序列打包，以处理任意分辨率和纵横比的输入。除了灵活的模型使用方式外，我们还展示了在大规模监督和对比度图像-文本预训练中的训练效率提升。NaViT可以高效地应用于图像和视频分类、目标检测、语义分割等标准任务，并在鲁棒性和公平性基准测试中取得了提升的结果。在推理阶段，输入分辨率的灵活性可以用于平稳地在测试时间的成本和性能之间进行权衡。我们相信NaViT标志着一个离开了以往思维的新篇章。

    The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a depart
    
[^7]: 面向深度神经网络验证的可证明审查器

    Towards a Certified Proof Checker for Deep Neural Network Verification. (arXiv:2307.06299v1 [cs.LO])

    [http://arxiv.org/abs/2307.06299](http://arxiv.org/abs/2307.06299)

    面向DNN验证的一种新型证明检查器实现，通过提供数值稳定性和更大的可验证性改进现有搜索工具存在的问题。这一实现利用了Imandra的两个关键能力：无限精度实数算术和形式化验证基础设施。

    

    最近深度神经网络（DNNs）的发展导致了它们在安全关键系统中的应用，从而增加了对其安全性的保证的需求。可以使用由验证社区开发的工具来证明DNN的这些安全性质。然而，这些工具本身容易出现实现错误和数值稳定性问题，这使得它们的可靠性值得怀疑。为了克服这个问题，一些验证器会产生可以由可信检查器检查的结果证明。在这项工作中，我们提出了一种用于DNN验证的新型证明检查器的实现。它通过提供数值稳定性和更大的可验证性来改进现有的实现。为了实现这一点，我们利用了Imandra（一种工业级的定理证明器）的两个关键能力：对无限精度实数算术的支持和其形式化验证基础设施。迄今为止，我们在Imandra中实现了一个证明检查器，规定了其正确性属性和开始

    Recent developments in deep neural networks (DNNs) have led to their adoption in safety-critical systems, which in turn has heightened the need for guaranteeing their safety. These safety properties of DNNs can be proven using tools developed by the verification community. However, these tools are themselves prone to implementation bugs and numerical stability problems, which make their reliability questionable. To overcome this, some verifiers produce proofs of their results which can be checked by a trusted checker. In this work, we present a novel implementation of a proof checker for DNN verification. It improves on existing implementations by offering numerical stability and greater verifiability. To achieve this, we leverage two key capabilities of Imandra, an industrial theorem prover: its support of infinite precision real arithmetic and its formal verification infrastructure. So far, we have implemented a proof checker in Imandra, specified its correctness properties and start
    
[^8]: 指令挖掘：大语言模型的高质量指令数据选择

    Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])

    [http://arxiv.org/abs/2307.06290](http://arxiv.org/abs/2307.06290)

    本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。

    

    大型语言模型通常经历预训练和微调两个训练阶段。尽管大规模预训练赋予模型强大的生成自然语言回应的能力，但这些预训练模型有时仍然无法理解人类指令。为了增强语言模型解释和响应指令的能力，指令微调已成为该领域的关键方法。最近的研究发现，即使只有少量高质量的指令跟随数据，大型语言模型也可以进行良好的微调。然而，选择用于微调语言模型的高质量数据集仍缺乏明确的指导方针。在本文中，我们提出了InstructMining，一个用于评估指令跟随数据质量的线性规则。我们使用具体的自然语言指标来进行InstructMining的建模。为了研究数据质量与这些指标之间的关系，我们还进行了广泛的细致研究。

    Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
    
[^9]: 理性神经网络控制器

    Rational Neural Network Controllers. (arXiv:2307.06287v1 [eess.SY])

    [http://arxiv.org/abs/2307.06287](http://arxiv.org/abs/2307.06287)

    神经网络在控制系统中的应用面临鲁棒性和不确定性问题，现有的神经网络架构不适合作为控制器。通过着重分析和设计神经网络控制器，可以解决这一问题。

    

    由于神经网络作为通用函数逼近器的能力，已在许多与机器学习相关的任务中取得了巨大成功。最近的研究证明了神经网络在控制系统中（称为神经反馈环）的有效性，特别是通过使用神经网络作为控制器。然而，这种方法的一个重要挑战是神经网络对敌对攻击非常敏感。这意味着，除非设计得当，否则它们不是控制器的理想选择，因为控制系统的鲁棒性和不确定性是至关重要的方面。已经初步研究了对神经网络控制器进行鲁棒性分析和设计的工作。然而，这些方法的一个突出问题是它们使用现有针对传统机器学习任务定制的神经网络架构。这些结构可能不适合神经网络控制器。

    Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems. There has been initial work on robustness to both analyse and design dynamical systems with neural network controllers. However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks. These structures may not be appropriate for neural network controllers and it is imp
    
[^10]: 处理FL中的计算异质性：一些理论见解

    Tackling Computational Heterogeneity in FL: A Few Theoretical Insights. (arXiv:2307.06283v1 [cs.LG])

    [http://arxiv.org/abs/2307.06283](http://arxiv.org/abs/2307.06283)

    本论文介绍并分析了一种新的聚合框架，用于处理联邦学习中的计算异质性。该框架涉及到处理异构数据和本地更新，通过理论和实验分析进行了广泛验证。

    

    机器学习的未来在于将数据收集与训练移至边缘。最近提出了联邦学习（FL）以实现这一目标。该方法的原则是聚合在大量分布式客户端上学习的模型，即从其环境中收集数据的资源受限移动设备，以获得新的更通用的模型。后者随后重新分发给客户端进行进一步的训练。联邦学习与数据中心的分布式训练之间的关键特征是内在的异质性。在这项工作中，我们引入并分析了一种新的聚合框架，以便形式化和解决联邦优化中的计算异质性，包括异构数据和本地更新。所提出的聚合算法从理论和实验角度进行了广泛分析。

    The future of machine learning lies in moving data collection along with training to the edge. Federated Learning, for short FL, has been recently proposed to achieve this goal. The principle of this approach is to aggregate models learned over a large number of distributed clients, i.e., resource-constrained mobile devices that collect data from their environment, to obtain a new more general model. The latter is subsequently redistributed to clients for further training. A key feature that distinguishes federated learning from data-center-based distributed training is the inherent heterogeneity. In this work, we introduce and analyse a novel aggregation framework that allows for formalizing and tackling computational heterogeneity in federated optimization, in terms of both heterogeneous data and local updates. Proposed aggregation algorithms are extensively analyzed from a theoretical, and an experimental prospective.
    
[^11]: SpreadNUTS -- 适度动态路径扩展的逆转采样和划分访问区域

    SpreadNUTS -- Moderate Dynamic Extension of Paths for No-U-Turn Sampling & Partitioning Visited Regions. (arXiv:2307.06279v1 [stat.CO])

    [http://arxiv.org/abs/2307.06279](http://arxiv.org/abs/2307.06279)

    SpreadNUTS是一个逆转采样和划分访问区域的方法，旨在改善MCMC算法的效率和收敛速度。

    

    马尔可夫链蒙特卡罗（MCMC）方法存在已久，该领域已经深入研究。MCMC方法的目的是通过重复采样来近似一个分布；大多数MCMC算法在渐进时都表现出最优行为，即在极限下收敛于真实分布。然而，这些算法的区别在于它们的实际收敛保证和效率。虽然采样器可能最终很好地近似了一个分布，但因为它在现实世界中使用，在合理的时间内达到采样器产生良好分布估计的点是必要的。同样，如果从一个分布中产生良好的样本用于估计在计算上很困难或无法操作，则采样器没有提供现实世界的实用性。因此，现在的大多数MCMC方法都集中于提高效率和加速收敛。然而，许多MCMC算法都受到随机游走行为的影响。

    Markov chain Monte Carlo (MCMC) methods have existed for a long time and the field is well-explored. The purpose of MCMC methods is to approximate a distribution through repeated sampling; most MCMC algorithms exhibit asymptotically optimal behavior in that they converge to the true distribution at the limit. However, what differentiates these algorithms are their practical convergence guarantees and efficiency. While a sampler may eventually approximate a distribution well, because it is used in the real world it is necessary that the point at which the sampler yields a good estimate of the distribution is reachable in a reasonable amount of time. Similarly, if it is computationally difficult or intractable to produce good samples from a distribution for use in estimation, then there is no real-world utility afforded by the sampler. Thus, most MCMC methods these days focus on improving efficiency and speeding up convergence. However, many MCMC algorithms suffer from random walk behavi
    
[^12]: 揭露假像：有效的扩散生成图像检测

    Exposing the Fake: Effective Diffusion-Generated Images Detection. (arXiv:2307.06272v1 [cs.CV])

    [http://arxiv.org/abs/2307.06272](http://arxiv.org/abs/2307.06272)

    本文提出了一种名为SeDID的新的扩散生成图像检测方法，该方法结合了统计和神经网络技术，并利用了扩散模型的独特属性，实现了对扩散生成图像的有效检测。实验证明，SeDID在扩散模型上具有优越的性能，对于区分扩散模型生成的图像具有重要贡献，标志着人工智能领域迈出了重要的一步。

    

    随着扩散式生成模型（例如去噪扩散概率模型，文本到图像扩散模型）的出现，图像合成得到了显著的进展。尽管这些模型非常高效，但在检测扩散生成图像方面的研究还不够，而这可能带来潜在的安全和隐私风险。本文通过提出一种名为Stepwise Error for Diffusion-generated Image Detection（SeDID）的新的检测方法来填补这一研究空白。SeDID由基于统计的$\text{SeDID}_{\text{Stat}}$和基于神经网络的$\text{SeDID}_{\text{NNs}}$组成，利用了扩散模型的独特属性，即确定性反转和确定性去噪计算错误。我们的评估结果表明，在应用于扩散模型时，SeDID的性能优于现有方法。因此，我们的工作在区分扩散模型生成的图像方面做出了重要贡献，标志着人工智能领域迈出了重要的一步。

    Image synthesis has seen significant advancements with the advent of diffusion-based generative models like Denoising Diffusion Probabilistic Models (DDPM) and text-to-image diffusion models. Despite their efficacy, there is a dearth of research dedicated to detecting diffusion-generated images, which could pose potential security and privacy risks. This paper addresses this gap by proposing a novel detection method called Stepwise Error for Diffusion-generated Image Detection (SeDID). Comprising statistical-based $\text{SeDID}_{\text{Stat}}$ and neural network-based $\text{SeDID}_{\text{NNs}}$, SeDID exploits the unique attributes of diffusion models, namely deterministic reverse and deterministic denoising computation errors. Our evaluations demonstrate SeDID's superior performance over existing methods when applied to diffusion models. Thus, our work makes a pivotal contribution to distinguishing diffusion model-generated images, marking a significant step in the domain of artificia
    
[^13]: 物理信息化的机器学习方法用于校准宏观交通流模型

    Physics-informed Machine Learning for Calibrating Macroscopic Traffic Flow Models. (arXiv:2307.06267v1 [cs.LG])

    [http://arxiv.org/abs/2307.06267](http://arxiv.org/abs/2307.06267)

    本文提出了一种基于物理信息化的机器学习方法来校准宏观交通流模型，通过将深度自编码器与交通流模型相结合，提高了校准效果。在案例研究中验证了该方法的可行性。

    

    良好校准的交通流模型对于理解交通现象和设计控制策略至关重要。传统的校准方法是基于优化算法开发的。本文提出了一种新颖的基于物理信息化的学习方法，该方法在性能上与优化算法相当甚至更好。为了实现这一目标，我们将经典的深度自编码器与交通流模型相结合。我们的方法将物理交通流模型信息提供给解码器，从而使得编码器能够根据流量和速度测量结果得出合理的交通参数。我们还引入了去噪自编码器，使其不仅能处理正常数据，还能处理有缺失值的错误数据。我们通过对加利福尼亚州I-210 E的案例研究验证了我们的方法。

    Well-calibrated traffic flow models are fundamental to understanding traffic phenomena and designing control strategies. Traditional calibration has been developed base on optimization methods. In this paper, we propose a novel physics-informed, learning-based calibration approach that achieves performances comparable to and even better than those of optimization-based methods. To this end, we combine the classical deep autoencoder, an unsupervised machine learning model consisting of one encoder and one decoder, with traffic flow models. Our approach informs the decoder of the physical traffic flow models and thus induces the encoder to yield reasonable traffic parameters given flow and speed measurements. We also introduce the denoising autoencoder into our method so that it can handles not only with normal data but also with corrupted data with missing values. We verified our approach with a case study of I-210 E in California.
    
[^14]: 关于频率响应函数的分层贝叶斯建模

    On the hierarchical Bayesian modelling of frequency response functions. (arXiv:2307.06263v1 [cs.LG])

    [http://arxiv.org/abs/2307.06263](http://arxiv.org/abs/2307.06263)

    结构健康监测(PBSHM)旨在在人群成员之间共享信息以改善健康状态的推断。由于差异和数据丢失所带来的挑战需要解决。

    

    基于人群的结构健康监测(PBSHM)旨在在人群成员之间共享有价值的信息，例如正常和损伤状态的数据，以改进对成员健康状态的推断。即使人群由名义上相同的结构组成，由于材料性质、几何形状、边界条件或环境影响(例如温度变化)的细微差异产生了 benign variations。这些差异可以影响模态性质，并表现为频率响应函数(FRF)的共振峰特征的变化。许多SHM策略依赖于对结构的动态特性进行监测，因此 benign variations 对这些系统的实际实施是具有挑战性的。振动式SHM的另一个常见挑战是数据丢失，可能是由于传输问题、传感器故障、传感器之间的采样率不匹配等原因引起的。

    Population-based structural health monitoring (PBSHM) aims to share valuable information among members of a population, such as normal- and damage-condition data, to improve inferences regarding the health states of the members. Even when the population is comprised of nominally-identical structures, benign variations among the members will exist as a result of slight differences in material properties, geometry, boundary conditions, or environmental effects (e.g., temperature changes). These discrepancies can affect modal properties and present as changes in the characteristics of the resonance peaks of the frequency response function (FRF). Many SHM strategies depend on monitoring the dynamic properties of structures, so benign variations can be challenging for the practical implementation of these systems. Another common challenge with vibration-based SHM is data loss, which may result from transmission issues, sensor failure, a sample-rate mismatch between sensors, and other causes
    
[^15]: 机器学习和拓扑数据分析在3D扫描中识别出人类乳突的特征。</br>

    Machine learning and Topological data analysis identify unique features of human papillae in 3D scans. (arXiv:2307.06255v1 [cs.LG])

    [http://arxiv.org/abs/2307.06255](http://arxiv.org/abs/2307.06255)

    这项研究使用机器学习和拓扑数据分析揭示了人类乳突的几何和拓扑特征的独特性，并且展示了乳突形状的持续同调特征在预测生物变量中的最高效应用。

    

    舌表面有许多乳突，对于味觉和口感的机械和化学功能至关重要。虽然乳突的味觉功能已经得到了很好的研究，但乳突在个体内外的独特性仍然不清楚。在这里，我们提出了第一个基于3D显微扫描的人类乳突的机器学习框架（n = 2092），揭示了乳突的几何和拓扑特征的独特性。基于离散微分几何和计算拓扑学的特征，计算机模拟了乳突形状中微小的差异。可解释的机器学习技术表明，乳突形状的持续同调特征是预测生物变量最有效的。使用少量数据样本训练的模型可以以85%的准确率预测乳突类型。乳突类型分类模型可以映射乳丝的空间排列。

    The tongue surface houses a range of papillae that are integral to the mechanics and chemistry of taste and textural sensation. Although gustatory function of papillae is well investigated, the uniqueness of papillae within and across individuals remains elusive. Here, we present the first machine learning framework on 3D microscopic scans of human papillae (n = 2092), uncovering the uniqueness of geometric and topological features of papillae. The finer differences in shapes of papillae are investigated computationally based on a number of features derived from discrete differential geometry and computational topology. Interpretable machine learning techniques show that persistent homology features of the papillae shape are the most effective in predicting the biological variables. Models trained on these features with small volumes of data samples predict the type of papillae with an accuracy of 85%. The papillae type classification models can map the spatial arrangement of filiform 
    
[^16]: 从软干预中确保因果分解的可识别性

    Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])

    [http://arxiv.org/abs/2307.06250](http://arxiv.org/abs/2307.06250)

    本文研究了从软干预中确保因果分解的可识别性。通过开发一种自编码变分贝叶斯算法，我们展示了在给定一般化的忠诚性概念的情况下，即使存在未观测到的因果变量，仍然可以恢复潜在的因果模型，并在无限数据的极限情况下预测未见组合的干预效果。

    

    因果分解旨在通过潜在变量的相关性揭示数据的表征，其通过因果模型相互关联。如果解释数据的潜在模型是唯一的，那么这种表示是可识别的。本文关注的是当存在不配对的观测和干预数据时的情况，每个干预都会改变一个潜在变量的机制。当因果变量完全观测到时，在诚实性假设下，已经开发出了统计一致的算法来识别因果模型。我们在这里展示，即使存在未观测到的因果变量，在给定一般化的忠诚性概念的情况下仍然可以实现可识别性。我们的结果保证了我们可以恢复潜在的因果模型，预测未见组合的干预效果，在无限数据的极限情况下。我们通过开发一种自编码变分贝叶斯算法和ap来实现我们的因果分解框架。

    Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and ap
    
[^17]: 基于扩散的多智能体对抗追踪

    Diffusion Based Multi-Agent Adversarial Tracking. (arXiv:2307.06244v1 [cs.RO])

    [http://arxiv.org/abs/2307.06244](http://arxiv.org/abs/2307.06244)

    本文介绍了CADENCE，一种基于扩散的多智能体对抗追踪方法，通过利用过去的稀疏状态信息生成全面的对手位置预测，并通过蒙特卡洛采样评估了其有效性。

    

    目标追踪在现实场景中起着关键作用，特别是在打击毒品走私行动中，对抗性目标的位置信息往往是有限的。改进自主追踪系统将使无人机、水面舰艇和水下器械能够更好地协助打击使用人工水面船只、半潜艇和航空器的走私犯。随着无人机的普及，准确的自主目标估计对安全和保障更为重要。本文提出了一种名为CADENCE的约束基于智能体的扩散增强多智能体追踪方法，旨在通过利用过去的稀疏状态信息生成对手位置的全面预测。为了评估这种方法的有效性，我们对单目标和多目标追踪环境进行了预测，利用扩散模型的蒙特卡洛采样来估计每个生成轨迹的概率。

    Target tracking plays a crucial role in real-world scenarios, particularly in drug-trafficking interdiction, where the knowledge of an adversarial target's location is often limited. Improving autonomous tracking systems will enable unmanned aerial, surface, and underwater vehicles to better assist in interdicting smugglers that use manned surface, semi-submersible, and aerial vessels. As unmanned drones proliferate, accurate autonomous target estimation is even more crucial for security and safety. This paper presents Constrained Agent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach aimed at generating comprehensive predictions of adversary locations by leveraging past sparse state information. To assess the effectiveness of this approach, we evaluate predictions on single-target and multi-target pursuit environments, employing Monte-Carlo sampling of the diffusion model to estimate the probability associated with each generated trajectory. We propose a novel 
    
[^18]: 用C-VAEs重建时空数据

    Reconstructing Spatiotemporal Data with C-VAEs. (arXiv:2307.06243v1 [cs.DB])

    [http://arxiv.org/abs/2307.06243](http://arxiv.org/abs/2307.06243)

    本文通过使用C-VAE模型来生成平滑且逼真的时空演变表示，探索了深度学习技术在重建时空数据方面的潜力。

    

    时空数据的连续表示通常依赖于使用抽象数据类型，例如移动区域，来表示形状和位置在时间上连续变化的实体。从离散的现实世界实体的快照创建这种表示需要使用插值方法来计算中间数据表示，并估计感兴趣对象在任意时间点的位置和形状。现有的区域插值方法常常无法生成平滑和逼真的区域演变表示。然而，深度学习技术的最新进展揭示了基于离散观测训练的深度模型通过隐式特征学习可以捕捉时空依赖关系的潜力。在这项工作中，我们探索了条件变分自编码器（C-VAE）模型生成移动区域的时空演变平滑和逼真表示的能力。

    The continuous representation of spatiotemporal data commonly relies on using abstract data types, such as \textit{moving regions}, to represent entities whose shape and position continuously change over time. Creating this representation from discrete snapshots of real-world entities requires using interpolation methods to compute in-between data representations and estimate the position and shape of the object of interest at arbitrary temporal points. Existing region interpolation methods often fail to generate smooth and realistic representations of a region's evolution. However, recent advancements in deep learning techniques have revealed the potential of deep models trained on discrete observations to capture spatiotemporal dependencies through implicit feature learning.  In this work, we explore the capabilities of Conditional Variational Autoencoder (C-VAE) models to generate smooth and realistic representations of the spatiotemporal evolution of moving regions. We evaluate our
    
[^19]: DSSE: 无人机群集搜索环境

    DSSE: a drone swarm search environment. (arXiv:2307.06240v1 [cs.LG])

    [http://arxiv.org/abs/2307.06240](http://arxiv.org/abs/2307.06240)

    DSSE是一个无人机群集搜索环境，用于研究需要动态概率作为输入的强化学习算法。

    

    无人机群集搜索项目是一个基于PettingZoo的环境，与多智能体（或单智能体）强化学习算法配合使用。该环境中的智能体（无人机）必须找到目标（遇险人员），但不知道目标的位置，并且不会根据自身与目标的距离得到奖励。但是，智能体会接收到目标出现在地图某个单元格的概率。该项目的目标是帮助研究需要动态概率作为输入的强化学习算法。

    The Drone Swarm Search project is an environment, based on PettingZoo, that is to be used in conjunction with multi-agent (or single-agent) reinforcement learning algorithms. It is an environment in which the agents (drones), have to find the targets (shipwrecked people). The agents do not know the position of the target and do not receive rewards related to their own distance to the target(s). However, the agents receive the probabilities of the target(s) being in a certain cell of the map. The aim of this project is to aid in the study of reinforcement learning algorithms that require dynamic probabilities as inputs.
    
[^20]: 通过模态融合实现统一的分子建模

    Unified Molecular Modeling via Modality Blending. (arXiv:2307.06235v1 [cs.LG])

    [http://arxiv.org/abs/2307.06235](http://arxiv.org/abs/2307.06235)

    MoleBLEND是一种通过对2D和3D分子结构进行统一编码和融合的自监督学习方法，实现了分子表示学习的最新性能表现。

    

    自监督的分子表示学习对于基于分子的任务如人工智能辅助药物发现至关重要。最近的研究考虑利用2D和3D信息进行表示学习，采用将每种模态分开处理的直接对齐策略。在这项工作中，我们引入了一种新的"混合-预测"自监督学习方法（MoleBLEND），将来自不同模态的原子间关系融合成一个统一的关系矩阵进行编码，然后恢复2D和3D结构的模态特定信息。通过将原子关系视为锚点，看似不相似的2D和3D流形在细粒度的关系级别上有机地对齐和整合。大量实验证明，MoleBLEND在主要的2D/3D基准测试中达到了最先进的性能。我们从相互信息最大化的角度提供了理论洞察，证明了我们的方法统一了对比、生成

    Self-supervised molecular representation learning is critical for molecule-based tasks such as AI-assisted drug discovery. Recent studies consider leveraging both 2D and 3D information for representation learning, with straightforward alignment strategies that treat each modality separately. In this work, we introduce a novel "blend-then-predict" self-supervised learning method (MoleBLEND), which blends atom relations from different modalities into one unified relation matrix for encoding, then recovers modality-specific information for both 2D and 3D structures. By treating atom relationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned and integrated at fine-grained relation-level organically. Extensive experiments show that MoleBLEND achieves state-of-the-art performance across major 2D/3D benchmarks. We further provide theoretical insights from the perspective of mutual-information maximization, demonstrating that our method unifies contrastive, generative (inte
    
[^21]: 学习分散式部分可观察场均控制来实现人工集体行为

    Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. (arXiv:2307.06175v1 [cs.LG])

    [http://arxiv.org/abs/2307.06175](http://arxiv.org/abs/2307.06175)

    本文提出了一种分散式部分可观察的场均控制模型（Dec-POMFC），用于解决多智能体强化学习中的分散化、部分可观察和可扩展性等挑战。该模型可将问题简化为可解决的单智能体马尔可夫决策过程，为实现人工集体行为提供了解决方案。

    

    近期，强化学习方法在各个领域取得了成功。然而，多智能体强化学习在分散化、部分可观察以及面对众多智能体时仍然存在挑战。与此同时，集体行为要解决上述问题，并且对于许多最前沿的应用，如活动物质物理、自组织系统、舆论动态以及生物或机器人群体来说非常重要。在这篇论文中，我们通过提出新的分散式部分可观察场均控制（Dec-POMFC）模型，实现了代理在部分信息下的分散行为，这是一类允许将问题简化为可解决的单智能体马尔可夫决策过程（MDP）的排列不变代理的广泛问题，以及单智能体强化学习解决方案。

    Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical
    
[^22]: 物理信息神经网络用于解决偏微分方程的辅助任务学习

    Auxiliary-Tasks Learning for Physics-Informed Neural Network-Based Partial Differential Equations Solving. (arXiv:2307.06167v1 [cs.LG])

    [http://arxiv.org/abs/2307.06167](http://arxiv.org/abs/2307.06167)

    我们提出了基于辅助任务学习的物理信息神经网络（ATL-PINNs），旨在解决原始PINNs在复杂物理情境中的低准确性和不收敛等问题。我们通过四种不同的辅助任务学习模式和梯度余弦相似度算法，提升了ATL-PINNs的效果。这是首次将辅助任务学习模式引入物理信息学习的研究中。

    

    物理信息神经网络（PINNs）已成为解决偏微分方程（PDE）的有希望的替代模式。它们的有效性在于通过神经网络捕捉与解决方案相关的特征。然而，原始的PINNs经常面临低准确性和不收敛等瓶颈，限制了它们在复杂物理情境中的适用性。为了缓解这些问题，我们提出了基于辅助任务学习的物理信息神经网络（ATL-PINNs），提供了四种不同的辅助任务学习模式，并与原始PINNs进行了性能比较。我们还采用梯度余弦相似度算法将辅助问题损失与主要问题损失相结合在ATL-PINNs中，旨在增强辅助任务学习模式的有效性。据我们所知，这是首次在物理信息学习的背景下引入辅助任务学习模式的研究。我们进行了实验验证...

    Physics-informed neural networks (PINNs) have emerged as promising surrogate modes for solving partial differential equations (PDEs). Their effectiveness lies in the ability to capture solution-related features through neural networks. However, original PINNs often suffer from bottlenecks, such as low accuracy and non-convergence, limiting their applicability in complex physical contexts. To alleviate these issues, we proposed auxiliary-task learning-based physics-informed neural networks (ATL-PINNs), which provide four different auxiliary-task learning modes and investigate their performance compared with original PINNs. We also employ the gradient cosine similarity algorithm to integrate auxiliary problem loss with the primary problem loss in ATL-PINNs, which aims to enhance the effectiveness of the auxiliary-task learning modes. To the best of our knowledge, this is the first study to introduce auxiliary-task learning modes in the context of physics-informed learning. We conduct exp
    
[^23]: 深度生成模型对生理信号的系统文献综述

    Deep Generative Models for Physiological Signals: A Systematic Literature Review. (arXiv:2307.06162v1 [cs.LG])

    [http://arxiv.org/abs/2307.06162](http://arxiv.org/abs/2307.06162)

    本文是对深度生成模型在生理信号研究领域的系统综述，总结了最新最先进的研究进展，有助于了解这些模型在生理信号中的应用和挑战，同时提供了评估和基准测试的指导。

    

    本文对深度生成模型在生理信号，特别是心电图、脑电图、光电容抗图和肌电图领域的文献进行了系统综述。与已有的综述文章相比，本文是第一篇总结最新最先进的深度生成模型的综述。通过分析与深度生成模型相关的最新研究，以及这些模型的主要应用和挑战，本综述为对这些模型应用于生理信号的整体理解做出了贡献。此外，通过强调采用的评估协议和最常用的生理数据库，本综述有助于对深度生成模型进行评估和基准测试。

    In this paper, we present a systematic literature review on deep generative models for physiological signals, particularly electrocardiogram, electroencephalogram, photoplethysmogram and electromyogram. Compared to the existing review papers, we present the first review that summarizes the recent state-of-the-art deep generative models. By analysing the state-of-the-art research related to deep generative models along with their main applications and challenges, this review contributes to the overall understanding of these models applied to physiological signals. Additionally, by highlighting the employed evaluation protocol and the most used physiological databases, this review facilitates the assessment and benchmarking of deep generative models.
    
[^24]: 通过无需手工设计奖励函数的自动课程强化学习实现机动决策

    Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions. (arXiv:2307.06152v1 [cs.AI])

    [http://arxiv.org/abs/2307.06152](http://arxiv.org/abs/2307.06152)

    本文提出了一种无需手工设计奖励函数的自动课程强化学习方法，使代理能够从零开始学习空战中的有效决策。通过逐渐学习一系列从简单到困难的子任务，代理能够在各种状态下做出有效的机动决策。

    

    机动决策是无人作战飞行器自主空战的核心。为了解决这个问题，我们提出了一种自动课程强化学习方法，使代理能够从零开始学习空战中的有效决策。通过初始状态范围来区分不同难度水平的课程，将机动决策分为一系列从简单到困难的子任务，并使用测试结果来改变子任务。随着子任务的变化，代理逐渐学会完成一系列从简单到困难的子任务，使其能够在无需进行奖励函数设计的情况下做出有效的机动决策以应对各种状态。消融研究表明，本文提出的自动课程学习是通过强化学习进行训练的一个重要组成部分，即代理在没有课程学习的情况下无法完成有效决策。

    Maneuver decision-making is the core of unmanned combat aerial vehicle for autonomous air combat. To solve this problem, we propose an automatic curriculum reinforcement learning method, which enables agents to learn effective decisions in air combat from scratch. The range of initial states are used for distinguishing curricula of different difficulty levels, thereby maneuver decision is divided into a series of sub-tasks from easy to difficult, and test results are used to change sub-tasks. As sub-tasks change, agents gradually learn to complete a series of sub-tasks from easy to difficult, enabling them to make effective maneuvering decisions to cope with various states without the need to spend effort designing reward functions. The ablation studied show that the automatic curriculum learning proposed in this article is an essential component for training through reinforcement learning, namely, agents cannot complete effective decisions without curriculum learning. Simulation exper
    
[^25]: NetGPT: 超越提供个性化生成服务的本地AI网络架构

    NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])

    [http://arxiv.org/abs/2307.06148](http://arxiv.org/abs/2307.06148)

    NetGPT是一个能够在边缘和云端部署适当的大型语言模型的本地AI网络架构，实现了个性化生成服务，并通过协作云边方法论来优化资源协调和互动效果。

    

    大型语言模型（LLMs）通过生成信息在日常生活中取得了巨大成功，LLMs的个性化可能进一步促进它们在应用中的作用，因为它们能更好地与人类意图对齐。针对个性化生成服务，协作云边方法论听起来很有前景，因为它有助于有效协调异构分布式通信和计算资源。在本文中，我们讨论了几种候选的云边协作技术的利弊，提出了NetGPT，根据其计算能力在边缘和云端部署适当的LLMs。此外，边缘LLMs可以高效利用基于位置的信息进行个性化提示完成，从而有益于与云端LLMs的互动。在边缘和云端部署代表性的开源LLMs（例如GPT-2-base和LLaMA模型）之后，我们展示了NetGPT的可行性。

    Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on th
    
[^26]: 学习层次交互式多目标搜索以进行移动操作

    Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])

    [http://arxiv.org/abs/2307.06125](http://arxiv.org/abs/2307.06125)

    这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。

    

    现有的目标搜索方法使得机器人可以在自由路径上进行搜索，然而，在非结构化的以人为中心的环境中操作的机器人经常需要操控环境以满足他们的需求。在这项工作中，我们引入了一种新的交互式多目标搜索任务，机器人需要打开门以浏览房间，并在橱柜和抽屉内搜索目标物品。这些新挑战需要在未知环境中结合操控和导航技能。我们提出了HIMOS，一种层次强化学习方法，学习组合探索、导航和操控技能。为了实现这一点，我们设计了一个围绕语义地图记忆的抽象高级动作空间，并利用探索过的环境作为实例导航点。我们在仿真和真实世界中进行了大量实验，证明HIMOS可以零样本方式有效地迁移到新的环境中，并且对于未见过的子任务具有鲁棒性。

    Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
    
[^27]: SoK: 使用综合基准比较不同成员推断攻击

    SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark. (arXiv:2307.06123v1 [cs.CR])

    [http://arxiv.org/abs/2307.06123](http://arxiv.org/abs/2307.06123)

    本文旨在开发一个综合基准，即MIBench，用于比较不同成员推断攻击。这个基准不仅包括评估指标，还包括评估场景，从不同角度考虑了数据样本之间的距离分布和差异。

    

    成员推断攻击威胁用户隐私，通过确定给定的数据示例是否被用于训练目标模型。然而，越来越多人认识到现有作品中使用的"比较不同成员推断攻击"方法有严重的局限性。通过本研究中的实验，我们发现一些文献中报告的比较结果是相当误导人的。在本文中，我们旨在开发一个用于比较不同成员推断攻击的综合基准，称为MIBench，它不仅包括评估指标，还包括评估场景。我们从四个角度设计评估场景：目标数据集中数据样本的距离分布，目标数据集中数据样本之间的距离，两个数据集之间的差异距离（即目标数据集和一个仅包含非成员的生成数据集），以及没有推断的样本比例。

    Membership inference (MI) attacks threaten user privacy through determining if a given data example has been used to train a target model. However, it has been increasingly recognized that the "comparing different MI attacks" methodology used in the existing works has serious limitations. Due to these limitations, we found (through the experiments in this work) that some comparison results reported in the literature are quite misleading. In this paper, we seek to develop a comprehensive benchmark for comparing different MI attacks, called MIBench, which consists not only the evaluation metrics, but also the evaluation scenarios. And we design the evaluation scenarios from four perspectives: the distance distribution of data samples in the target dataset, the distance between data samples of the target dataset, the differential distance between two datasets (i.e., the target dataset and a generated dataset with only nonmembers), and the ratio of the samples that are made no inferences b
    
[^28]: 深度学习动态图：模型与基准

    Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])

    [http://arxiv.org/abs/2307.06104](http://arxiv.org/abs/2307.06104)

    本文对深度学习动态图领域进行了调查，总结了学习时间和空间信息的最新优势，并对最流行的方法进行了公平的性能比较，为评估新架构和方法建立了一个可靠的基准模型。

    

    近年来，在深度图网络（DGNs）的研究中取得了巨大进展，推动了图上学习的领域成熟发展。尽管这个研究领域正在快速增长，但仍然存在一些尚未解决的重要挑战。特别地，急需使DGNs适用于实时系统中随时间推移不断演化的预测任务。为促进动态图领域的研究，首先，我们调查了学习时间和空间信息的最新优势，并提供了动态图表示学习领域的当前最新概览。其次，我们对最流行的方法进行了公平的性能比较，通过严格的模型选择和评估，为评估新架构和方法建立了一个可靠的基准模型。

    Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches
    
[^29]: 使用图神经网络学习随机动力系统作为一种隐式正则化方法

    Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks. (arXiv:2307.06097v1 [cs.LG])

    [http://arxiv.org/abs/2307.06097](http://arxiv.org/abs/2307.06097)

    本论文提出了一种使用图神经网络学习随机动力系统的隐式正则化方法，通过学习随机微分方程的漂移项和扩散项来捕捉时间序列中的观测到的随机性和空间相关性。实验结果表明，该方法在收敛性、鲁棒性和泛化能力方面表现出优越性。

    

    提出了随机Gumbel图网络来学习高维时间序列，其中观测到的维度通常具有空间相关性。为此，通过学习随机微分方程的漂移项和扩散项，分别捕捉观测到的随机性和空间相关性，并采用Gumbel矩阵嵌入来表示。特别地，这种新颖的框架使我们能够研究S-GGNs中噪声项的隐式正则化效果。通过推导权重的小邻域中两个相应损失函数的差异，我们为提出的S-GGNs提供了理论保证。然后，我们使用Kuramoto模型生成数据，比较两个损失函数的Hessian矩阵的谱密度。实验结果表明，与现有技术相比，S-GGNs具有较好的收敛性、鲁棒性和泛化能力。

    Stochastic Gumbel graph networks are proposed to learn high-dimensional time series, where the observed dimensions are often spatially correlated. To that end, the observed randomness and spatial-correlations are captured by learning the drift and diffusion terms of the stochastic differential equation with a Gumble matrix embedding, respectively. In particular, this novel framework enables us to investigate the implicit regularization effect of the noise terms in S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by deriving the difference between the two corresponding loss functions in a small neighborhood of weight. Then, we employ Kuramoto's model to generate data for comparing the spectral density from the Hessian Matrix of the two loss functions. Experimental results on real-world data, demonstrate that S-GGNs exhibit superior convergence, robustness, and generalization, compared with state-of-the-arts.
    
[^30]: 在线 Laplace 模型选择的再探讨

    Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])

    [http://arxiv.org/abs/2307.06093](http://arxiv.org/abs/2307.06093)

    本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。

    

    Laplace 近似为神经网络提供了一个封闭形式的模型选择目标。在贝叶斯深度学习领域，将神经网络参数与超参数（如权重衰减强度）一起进行优化的在线变体方法再次引起了人们的关注。然而，这些方法违反了 Laplace 方法的一个关键假设，即近似是围绕损失的模态进行的，这就对它们的合理性提出了质疑。本研究重新推导了在线 Laplace 方法，展示了它们针对 Laplace 证据的一个修正模态的变分上界，从而避免了对平稳性的假设。在线 Laplace 方法及其修正模态的对应点满足两个条件：1. 神经网络参数是一个最大后验概率，满足 Laplace 方法的假设；2. 超参数最大化 Laplace 证据，从而促使在线方法的应用。我们通过使用全批量梯度的在线算法演示了这些最优点在实践中的近似程度。

    The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradien
    
[^31]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^32]: 解读疾病进展聚类中的深度嵌入

    Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])

    [http://arxiv.org/abs/2307.06060](http://arxiv.org/abs/2307.06060)

    本文提出了一种在疾病进展聚类中解读深度嵌入的新方法，并通过评估2型糖尿病参与者数据集展示了对疾病进展模式的临床意义性见解。

    

    我们提出了一种在患者聚类的背景下解读深度嵌入的新方法。我们在来自英国生物库的2型糖尿病参与者数据集上评估我们的方法，并展示出对疾病进展模式的临床意义性见解。

    We propose a novel approach for interpreting deep embeddings in the context of patient clustering. We evaluate our approach on a dataset of participants with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful insights into disease progression patterns.
    
[^33]: 深度贝叶斯分类的函数空间正则化

    Function-Space Regularization for Deep Bayesian Classification. (arXiv:2307.06055v1 [cs.LG])

    [http://arxiv.org/abs/2307.06055](http://arxiv.org/abs/2307.06055)

    本研究提出了一种函数空间正则化方法来增加深度贝叶斯分类模型的不确定性量化和对抗性鲁棒性。该方法使用Dirichlet先验在预测空间中进行变分推断，并能与不同模型相结合而不影响模型的架构大小。

    

    贝叶斯深度学习方法假设模型参数为潜在随机变量，并推断后验分布以量化不确定性，增加安全性和可信度，并防止过于自信和不可预测的行为。然而，权重空间先验是特定于模型的，可能难以解释和难以指定。相反，我们在预测空间中应用Dirichlet先验，并执行近似函数空间变分推断。为此，我们将随机神经网络分类器的传统分类预测解释为来自隐式Dirichlet分布的样本。通过调整推断，可以将相同的函数空间先验与不同的模型结合在一起，而不影响模型的架构或大小。我们通过玩具实验说明了这种先验的灵活性和功效，并通过大规模图像分类实验展示了可扩展性、改进的不确定性量化和对抗性鲁棒性。

    Bayesian deep learning approaches assume model parameters to be latent random variables and infer posterior distributions to quantify uncertainty, increase safety and trust, and prevent overconfident and unpredictable behavior. However, weight-space priors are model-specific, can be difficult to interpret and are hard to specify. Instead, we apply a Dirichlet prior in predictive space and perform approximate function-space variational inference. To this end, we interpret conventional categorical predictions from stochastic neural network classifiers as samples from an implicit Dirichlet distribution. By adapting the inference, the same function-space prior can be combined with different models without affecting model architecture or size. We illustrate the flexibility and efficacy of such a prior with toy experiments and demonstrate scalability, improved uncertainty quantification and adversarial robustness with large-scale image classification experiments.
    
[^34]: 在线库存问题：在线凸优化中的非独立同分布设置

    Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex Optimization. (arXiv:2307.06048v1 [math.OC])

    [http://arxiv.org/abs/2307.06048](http://arxiv.org/abs/2307.06048)

    该论文研究了具有非独立同分布设置的在线库存问题，提出了一个在线算法MaxCOSD，并证明了其在考虑了非独立同分布需求和有状态动态的情况下的有效性

    

    我们研究多产品库存控制问题，其中经理根据部分历史信息作出顺序补充决策，以最小化其累积损失。我们的动机是考虑到一般需求、损失和动态，超越通常依赖新闻供应商类型损失、固定动态和不现实的独立同分布需求假设的标准模型。我们提出了MaxCOSD，一个在线算法，即使对于非独立同分布需求和有状态动态（包括易腐烂物品）的问题，也具有可证明的保证性能。我们考虑了我们所称的需求过程的非退化性假设，并认为它们是允许学习的必要条件。

    We study multi-product inventory control problems where a manager makes sequential replenishment decisions based on partial historical information in order to minimize its cumulative losses. Our motivation is to consider general demands, losses and dynamics to go beyond standard models which usually rely on newsvendor-type losses, fixed dynamics, and unrealistic i.i.d. demand assumptions. We propose MaxCOSD, an online algorithm that has provable guarantees even for problems with non-i.i.d. demands and stateful dynamics, including for instance perishability. We consider what we call non-degeneracy assumptions on the demand process, and argue that they are necessary to allow learning.
    
[^35]: 借助新的关系类型和节点，以OOD多任务视角进行链接预测

    An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes. (arXiv:2307.06046v1 [cs.LG])

    [http://arxiv.org/abs/2307.06046](http://arxiv.org/abs/2307.06046)

    本论文提出了一种在OOD测试多图中进行链接预测的方法，通过使用双可交换性概念，该方法能够处理具有不同预测模式且可能冲突的关系类型集合的属性多图。

    

    在归纳链接预测任务中，我们推断具有属性的多图中新测试多图中节点之间的缺失属性链接（关系）。传统的关系学习方法面临着对OOD测试多图的有限泛化能力的挑战，这些多图包含了训练中未见过的新节点和新关系类型。最近，高等人（2023）在所有关系类型共享相同结构预测模式（单个任务）的唯一假设下，提出了一种使用双可交换性理论概念（用于节点和关系类型）来进行OOD链接预测的方法，与使用图神经网络（GNNs）设计的（单个）可交换性（仅用于节点）相反。在这项工作中，我们进一步将双可交换性概念扩展到多任务双可交换性，其中我们定义了属性多图中的链接预测，这些图可能对不同的关系类型集合具有不同且可能冲突的预测模式（多个任务）。

    The task of inductive link prediction in (discrete) attributed multigraphs infers missing attributed links (relations) between nodes in new test multigraphs. Traditional relational learning methods face the challenge of limited generalization to OOD test multigraphs containing both novel nodes and novel relation types not seen in training. Recently, under the only assumption that all relation types share the same structural predictive patterns (single task), Gao et al. (2023) proposed an OOD link prediction method using the theoretical concept of double exchangeability (for nodes & relation types), in contrast to the (single) exchangeability (only for nodes) used to design Graph Neural Networks (GNNs). In this work we further extend the double exchangeability concept to multi-task double exchangeability, where we define link prediction in attributed multigraphs that can have distinct and potentially conflicting predictive patterns for different sets of relation types (multiple tasks). 
    
[^36]: 声音转换的节奏建模

    Rhythm Modeling for Voice Conversion. (arXiv:2307.06040v1 [eess.AS])

    [http://arxiv.org/abs/2307.06040](http://arxiv.org/abs/2307.06040)

    本论文提出了一种名为Urhythmic的无监督节奏转换方法，通过对源语音进行分割和时间拉伸，实现了声音转换中的节奏匹配，实验结果表明该方法在质量和语调方面优于现有方法。

    

    声音转换旨在将源语音转换为不同的目标声音。然而，典型的声音转换系统没有考虑节奏，而节奏是对说话人身份感知的重要因素。为了弥补这个差距，我们引入了一种无监督的节奏转换方法Urhythmic，它不需要平行数据或文本转录。使用自监督表示，我们首先将源音频分割成近似鼻音、障音和静音的片段。然后，我们通过估计每个片段类型的说话速率或持续时间分布来建模节奏。最后，我们通过对语音片段进行时间拉伸来匹配目标说话速率或节奏。实验证明，在质量和语调方面，Urhythmic优于现有的无监督方法。

    Voice conversion aims to transform source speech into a different target voice. However, typical voice conversion systems do not account for rhythm, which is an important factor in the perception of speaker identity. To bridge this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion that does not require parallel data or text transcriptions. Using self-supervised representations, we first divide source audio into segments approximating sonorants, obstruents, and silences. Then we model rhythm by estimating speaking rate or the duration distribution of each segment type. Finally, we match the target speaking rate or rhythm by time-stretching the speech segments. Experiments show that Urhythmic outperforms existing unsupervised methods in terms of quality and prosody. Code and checkpoints: https://github.com/bshall/urhythmic. Audio demo page: https://ubisoft-laforge.github.io/speech/urhythmic.
    
[^37]: 从优秀解释中学习

    Learning from Exemplary Explanations. (arXiv:2307.06026v1 [cs.LG])

    [http://arxiv.org/abs/2307.06026](http://arxiv.org/abs/2307.06026)

    本论文介绍了一种基于优秀解释的学习方法，通过使用两个输入实例和其相应的梯度加权类激活映射（GradCAM）模型解释，可以在减少用户交互的情况下改善解释性学习的效果，并在医学图像分类任务中实现了更好的解释和较小的分类性能损失。

    

    解释式学习（XBL）是一种交互式机器学习（IML）形式，通过用户反馈收集的模型解释，提供了一种模型细化方法。尽管XBL的交互性促进了模型的透明性，但XBL需要大量的用户交互，并且在高成本领域，如医学图像分类中，由于反馈以详细注释形式而非简单的类别标注，这种成本会加剧。为了减少XBL的工作量和成本，我们引入了一种新方法，该方法利用两个输入实例及其相应的梯度加权类激活映射（GradCAM）模型解释作为优秀解释来实现XBL。通过医学图像分类任务，我们证明，使用最少的人工输入，我们的方法产生了改进的解释（+0.02，+3%），并在与仅使用模型训练的模型相比时，实现了分类性能的降低（-0.04，-4%）。

    eXplanation Based Learning (XBL) is a form of Interactive Machine Learning (IML) that provides a model refining approach via user feedback collected on model explanations. Although the interactivity of XBL promotes model transparency, XBL requires a huge amount of user interaction and can become expensive as feedback is in the form of detailed annotation rather than simple category labelling which is more common in IML. This expense is exacerbated in high stakes domains such as medical image classification. To reduce the effort and expense of XBL we introduce a new approach that uses two input instances and their corresponding Gradient Weighted Class Activation Mapping (GradCAM) model explanations as exemplary explanations to implement XBL. Using a medical image classification task, we demonstrate that, using minimal human input, our approach produces improved explanations (+0.02, +3%) and achieves reduced classification performance (-0.04, -4%) when compared against a model trained wi
    
[^38]: 通过两方面三视图标签传播的高效时间感知实体对齐框架

    An Effective and Efficient Time-aware Entity Alignment Framework via Two-aspect Three-view Label Propagation. (arXiv:2307.06013v1 [cs.AI])

    [http://arxiv.org/abs/2307.06013](http://arxiv.org/abs/2307.06013)

    本文提出了一个高效且有效的非神经网络时间感知实体对齐框架，通过两方面三视图标签传播、稀疏相似度、Sinkhorn算子和时态迭代学习来提高对齐性能，并减少模型的时间开销。

    

    实体对齐旨在寻找不同知识图谱之间的等价实体对，这对于推动知识融合至关重要。随着时态知识图谱的广泛使用，时间感知实体对齐方法出现以增强对齐效果。现有的时间感知实体对齐模型基于图神经网络（GNN），取得了最先进的性能，但由于GNN的可扩展性问题，很难将其应用到大规模时态知识图谱中。因此，本文提出了一个有效且高效的非神经网络实体对齐框架LightTEA，它由四个关键组件组成：(1)两方面三视图标签传播，(2)带有时态约束的稀疏相似度，(3)Sinkhorn算子，以及(4)时态迭代学习。所有这些模块共同作用，提高了实体对齐的性能，同时减少了模型的时间开销。在公共数据集上的广泛实验证明，我们提出的模型明显优于最先进的方法。

    Entity alignment (EA) aims to find the equivalent entity pairs between different knowledge graphs (KGs), which is crucial to promote knowledge fusion. With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA) methods appear to enhance EA. Existing TEA models are based on Graph Neural Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is difficult to transfer them to large-scale TKGs due to the scalability issue of GNN. In this paper, we propose an effective and efficient non-neural EA framework between TKGs, namely LightTEA, which consists of four essential components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative Learning. All of these modules work together to improve the performance of EA while reducing the time consumption of the model. Extensive experiments on public datasets indicate that our proposed model significantly outperforms the SOTA method
    
[^39]: Vision Transformers的Finetuning过程中发生了什么：基于不变性的研究

    What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation. (arXiv:2307.06006v1 [cs.CV])

    [http://arxiv.org/abs/2307.06006](http://arxiv.org/abs/2307.06006)

    本研究通过调查预训练的Vision Transformers和Finetuned版本之间的关系，发现预训练引入了可转移的不变性，在Finetuning过程中，深层的不变性向浅层压缩。

    

    通过在几个基准数据集和任务上研究预训练的Vision Transformers及其对应的Finetuned版本之间的关系，我们发现了一些新的指标，并特别调查了预训练模型学习的不变性在Finetuning过程中的保留程度。使用这些指标，我们呈现了一系列实证发现，包括预训练在浅层中引入了可转移的不变性，并且在Finetuning过程中，深层的不变性向浅层压缩。这些发现有助于理解成功的原因之一。

    The pretrain-finetune paradigm usually improves downstream performance over training a model from scratch on the same task, becoming commonplace across many areas of machine learning. While pretraining is empirically observed to be beneficial for a range of tasks, there is not a clear understanding yet of the reasons for this effect. In this work, we examine the relationship between pretrained vision transformers and the corresponding finetuned versions on several benchmark datasets and tasks. We present new metrics that specifically investigate the degree to which invariances learned by a pretrained model are retained or forgotten during finetuning. Using these metrics, we present a suite of empirical findings, including that pretraining induces transferable invariances in shallow layers and that invariances from deeper pretrained layers are compressed towards shallower layers during finetuning. Together, these findings contribute to understanding some of the reasons for the successes
    
[^40]: DDNAS: 离散化可微分神经架构搜索用于文本分类

    DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])

    [http://arxiv.org/abs/2307.06005](http://arxiv.org/abs/2307.06005)

    这篇论文提出了一种名为DDNAS的离散化可微分神经架构搜索方法，用于文本分类。通过使用连续松弛的架构表示和互信息最大化的离散化层，DDNAS在文本表示学习和分类任务中表现优于其他NAS方法。

    

    神经架构搜索（NAS）在学习文本表示方面展现出了很好的能力。然而，现有的基于文本的NAS既未对架构进行可学习的融合以优化，也未对文本输入背后的潜在层级分类进行编码。本文提出了一种新颖的NAS方法，即Discretized Differentiable Neural Architecture Search (DDNAS)，用于文本表示学习和分类。通过架构表示的连续松弛，DDNAS可以使用梯度下降来进行搜索优化。我们还提出了一种新颖的离散化层，通过最大化互信息将其施加于每个搜索节点上，以对文本表示中的潜在层级分类进行建模。在八个不同的真实数据集上进行的大量实验表明，DDNAS始终能够优于最先进的NAS方法。尽管DDNAS仅依赖于卷积，池化和无操作这三个基本操作，作为候选操作。

    Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input. This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the cand
    
[^41]: 《人类活动识别中自动数据标注技术的综合综述》

    A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition. (arXiv:2307.05988v1 [cs.LG])

    [http://arxiv.org/abs/2307.05988](http://arxiv.org/abs/2307.05988)

    该论文对人类活动识别中的自动数据标注技术进行了全面综述，提出了不同方法和解决方案，以解决人工标注的耗时和繁琐问题。

    

    《人类活动识别（HAR）已成为过去十年中领先的研究课题之一。随着传感技术的成熟和经济成本的降低，一系列新颖的应用在医疗、工业、体育和日常生活活动等领域变得流行。HAR系统的设计需要不同耗时的处理步骤，如数据采集、标注、模型训练和优化。特别是数据标注是HAR中最费时和繁琐的步骤，因为它需要人工标注者进行大量和详细的手工工作。因此，已提出了关于HAR中注解过程自动化的不同方法。注解问题在不同的概念和场景中都存在，都需要个别的解决方案。本文提供了关于HAR数据标注技术的首个系统综述。通过将现有方法分组并提供税onomr， »

    Human Activity Recognition (HAR) has become one of the leading research topics of the last decade. As sensing technologies have matured and their economic costs have declined, a host of novel applications, e.g., in healthcare, industry, sports, and daily life activities have become popular. The design of HAR systems requires different time-consuming processing steps, such as data collection, annotation, and model training and optimization. In particular, data annotation represents the most labor-intensive and cumbersome step in HAR, since it requires extensive and detailed manual work from human annotators. Therefore, different methodologies concerning the automation of the annotation procedure in HAR have been proposed. The annotation problem occurs in different notions and scenarios, which all require individual solutions. In this paper, we provide the first systematic review on data annotation techniques for HAR. By grouping existing approaches into classes and providing a taxonomy,
    
[^42]: 基于Transformer的强化学习综述

    Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])

    [http://arxiv.org/abs/2307.05979](http://arxiv.org/abs/2307.05979)

    本综述介绍了基于Transformer的强化学习的应用。Transformer在解决强化学习中的挑战上具有潜在优势，包括不稳定训练、信用分配、缺乏可解释性和部分可观测性等方面。研究还探讨了其在表示学习、状态转移和奖励函数建模以及策略优化等方面的应用。此外，还有一些最新的研究旨在提高Transformer的可解释性和效率。

    

    Transformer在自然语言处理、计算机视觉和机器人等领域具有显著的影响，在性能上优于其他神经网络。本综述探讨了Transformer在强化学习（RL）中的应用，它被视为解决不稳定训练、信用分配、缺乏可解释性和部分可观测性等挑战的有希望解决方案。我们首先简要介绍了RL领域的概述，然后讨论了传统RL算法的挑战。接下来，我们深入探讨了Transformer及其变体的特性，并讨论了使其适合应对RL中固有挑战的特点。我们还研究了Transformer在RL中各个方面的应用，包括表示学习、状态转移和奖励函数建模以及策略优化。此外，我们还讨论了最近的研究，旨在提高Transformer的可解释性和效率。

    Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers i
    
[^43]: 实现互联网规模的文本到图像扩散模型的安全自蒸馏

    Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models. (arXiv:2307.05977v1 [cs.CV])

    [http://arxiv.org/abs/2307.05977](http://arxiv.org/abs/2307.05977)

    本文介绍了一种名为SDD的方法，用于实现互联网规模的文本到图像扩散模型的安全自蒸馏。该方法通过引导噪声估计与无条件模型匹配，在生成的图像中消除有害内容的比例更大，同时保持整体图像质量，并且允许一次移除多个概念。

    

    大规模图像生成模型借助互联网上丰富的数据具有卓越的质量，但也引发了社会关切，担心这些模型可能生成有害或受版权保护的内容。这些偏见和有害性在整个训练过程中产生，并且很难完全消除，这已成为安全部署这些模型的重要障碍。本文提出了一种名为SDD的方法，用于在文本到图像扩散模型中防止问题内容的生成。我们通过自蒸馏扩散模型来引导基于目标移除概念的噪声估计与无条件模型匹配。与之前的方法相比，我们的方法可以消除更大比例的有害内容，同时不降低整体图像质量。此外，我们的方法还允许一次移除多个概念，而之前的工作只能一次移除一个概念。

    Large-scale image generation models, with impressive quality made possible by the vast amount of data available on the Internet, raise social concerns that these models may generate harmful or copyrighted content. The biases and harmfulness arise throughout the entire training process and are hard to completely remove, which have become significant hurdles to the safe deployment of these models. In this paper, we propose a method called SDD to prevent problematic content generation in text-to-image diffusion models. We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one. Compared to the previous methods, our method eliminates a much greater proportion of harmful content from the generated images without degrading the overall image quality. Furthermore, our method allows the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time.
    
[^44]: 回归中的异常值检测：锥二次形式

    Outlier detection in regression: conic quadratic formulations. (arXiv:2307.05975v1 [math.OC])

    [http://arxiv.org/abs/2307.05975](http://arxiv.org/abs/2307.05975)

    本论文提出了一种在回归中解决异常值检测问题的方法，通过引入二次锥松弛形式而不是使用big-M约束，这种方法比现有的big-M公式快数个数量级。

    

    在许多应用中，当构建线性回归模型时，考虑到异常值的存在——即受损的输入数据点，是很重要的。这类问题可以通过混合整数优化问题来表达，其中每个问题由二进制变量和连续变量的二次项的乘积给出，形成三次项。现有的文献方法通常依靠使用big-M约束线性化三次项，但在实践中表现出弱放松和性能差的缺点。本工作中，我们推导了不涉及big-M约束的二次锥松弛形式。我们的计算实验表明，对于这个问题，提议的公式比现有文献中的big-M公式快数个数量级。

    In many applications, when building linear regression models, it is important to account for the presence of outliers, i.e., corrupted input data points. Such problems can be formulated as mixed-integer optimization problems involving cubic terms, each given by the product of a binary variable and a quadratic term of the continuous variables. Existing approaches in the literature, typically relying on the linearization of the cubic terms using big-M constraints, suffer from weak relaxation and poor performance in practice. In this work we derive stronger second-order conic relaxations that do not involve big-M constraints. Our computational experiments indicate that the proposed formulations are several orders-of-magnitude faster than existing big-M formulations in the literature for this problem.
    
[^45]: 对比学习用于转化率预测

    Contrastive Learning for Conversion Rate Prediction. (arXiv:2307.05974v1 [cs.IR])

    [http://arxiv.org/abs/2307.05974](http://arxiv.org/abs/2307.05974)

    对比学习用于转化率预测的框架(CL4CVR)可以利用丰富的无标签数据学习更好的数据表示，并提高转化率预测性能。

    

    转化率（CVR）预测在广告系统中扮演重要角色。近年来，基于监督深度神经网络的模型在CVR预测方面表现出了良好的性能。然而，它们需要大量的训练数据，对数据的需求较高。在线广告系统中，虽然存在数以百万计的广告，但用户往往只点击其中的一小部分，并在其中的更小部分进行转化。数据的稀疏性限制了这些深度模型的能力。本文提出了对比学习用于CVR预测（CL4CVR）框架。该框架将监督CVR预测任务与对比学习任务关联起来，可以利用丰富的无标签数据学习更好的数据表示，并提高CVR预测性能。为了将对比学习任务应用于CVR预测问题，我们提出了嵌入式掩码（EM），而不是特征掩码，来创建两个增强样本视图。我们还提出了一个假阴性的...

    Conversion rate (CVR) prediction plays an important role in advertising systems. Recently, supervised deep neural network-based models have shown promising performance in CVR prediction. However, they are data hungry and require an enormous amount of training data. In online advertising systems, although there are millions to billions of ads, users tend to click only a small set of them and to convert on an even smaller set. This data sparsity issue restricts the power of these deep models. In this paper, we propose the Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the supervised CVR prediction task with a contrastive learning task, which can learn better data representations exploiting abundant unlabeled data and improve the CVR prediction performance. To tailor the contrastive learning task to the CVR prediction problem, we propose embedding masking (EM), rather than feature masking, to create two views of augmented samples. We also propose a false negativ
    
[^46]: VoxPoser: 用于带有语言模型的机器人操作的可组合的3D价值映射

    VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])

    [http://arxiv.org/abs/2307.05973](http://arxiv.org/abs/2307.05973)

    VoxPoser提出了一种新方法，通过组合3D价值映射和语言模型，实现了机器人在多种操作任务下根据自由形式的指令和对象合成机器人轨迹的能力。

    

    研究表明，大型语言模型（LLMs）具有丰富的可行动知识，可以以推理和规划的形式提取出用于机器人操作的信息。尽管取得了进展，大多数模型仍然依赖于预定义的运动原语来执行与环境的物理交互，这仍然是一个重大瓶颈。在这项工作中，我们的目标是在给定开集指令和开集对象的情况下，为各种操作任务合成机器人轨迹，即一系列密集的6-DoF末端执行器路径点。我们首先观察到LLMs在给定自由形式的语言指令时擅长推断可行性和约束。更重要的是，通过利用它们的代码编写能力，它们可以与视觉-语言模型（VLM）交互，以组合3D价值映射将知识接地到Agent的观测空间中。然后在基于模型的规划框架中使用组合的价值映射来零试合成闭环轨迹。

    Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
    
[^47]: 自学习量化：在基于Transformer的语言模型中实现高压缩率

    Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models. (arXiv:2307.05972v1 [cs.CL])

    [http://arxiv.org/abs/2307.05972](http://arxiv.org/abs/2307.05972)

    本研究探讨了事后训练量化和量化感知训练对Transformer语言模型泛化效果的影响，并提出了自学习量化（SDQ）方法，该方法最小化累积量化误差并在多语言模型上优于基准模型。通过在XGLUE基准上的实验证明，SDQ方法可以将模型从32位浮点权重减少到8位整数权重，同时保持高水平的性能。

    

    我们研究了事后训练量化和量化感知训练对Transformer语言模型的泛化效果。我们提出了一种称为自学习量化（SDQ）的新方法，该方法最小化累积量化误差并优于基准模型。我们将SDQ应用于多语言模型XLM-R-Base和InfoXLM-Base，并证明这两个模型可以从32位浮点权重减少到8位整数权重，同时在XGLUE基准上保持高水平的性能。我们的结果还凸显了量化多语言模型面临的挑战，这些模型必须能够推广到未经微调的语言。

    We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark. Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on.
    
[^48]: 给机器人以帮助：通过眼手协同的人类视频演示学习通用操作

    Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations. (arXiv:2307.05959v1 [cs.RO])

    [http://arxiv.org/abs/2307.05959](http://arxiv.org/abs/2307.05959)

    使用无标签的人类视频演示增强了眼手协同的视觉运动策略的泛化能力，而无需昂贵的专家演示数据或领域适应方法。

    

    眼手协同摄像头在视觉导向的机器人操作中显示出了更高的样本效率和泛化能力。然而，在机器人模仿中，让人类远程操作员收集大量专家演示对于真实机器人来说仍然很昂贵。另一方面，人类进行任务的视频要便宜得多，因为它们消除了对机器人遥操作专业知识的需求，并且可以在各种场景中快速捕捉。因此，人类视频演示是学习大规模通用机器人操作策略的有希望的数据来源。在这项工作中，我们使用广泛的无标签人类视频演示来增强狭窄的机器人模仿数据集，从而大大提高了眼手协同视觉运动策略的泛化能力。尽管人类和机器人数据之间存在明显的视觉领域差距，但我们的框架不需要使用任何明确的领域适应方法，因为我们利用了部分可观察性。

    Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of e
    
[^49]: 基于Newell理论的特征转换用于时空交通预测

    Newell's theory based feature transformations for spatio-temporal traffic prediction. (arXiv:2307.05949v1 [cs.LG])

    [http://arxiv.org/abs/2307.05949](http://arxiv.org/abs/2307.05949)

    本文提出了一种基于Newell理论的特征转换方法用于时空交通预测，用于改善模型在不同位置的迁移性问题。

    

    深度学习模型在时空交通流预测中使用卷积或图卷积过滤器以及循环神经网络来捕捉交通数据的空间和时间依赖关系。这些模型, 如CNN-LSTM, 利用邻近检测站的交通流来预测特定位置的流量。然而, 这些模型在捕捉交通系统的更广泛动态方面具有局限性, 因为它们主要学习特定于检测配置和目标位置交通特征的特征。因此, 当在新的位置缺少用于模型训练的数据时, 这些模型的可迁移性变得具有挑战性。为了解决这个问题, 我们提出了一个基于交通流物理学的特征转换方法用于时空深度学习模型。

    Deep learning (DL) models for spatio-temporal traffic flow forecasting employ convolutional or graph-convolutional filters along with recurrent neural networks to capture spatial and temporal dependencies in traffic data. These models, such as CNN-LSTM, utilize traffic flows from neighboring detector stations to predict flows at a specific location of interest. However, these models are limited in their ability to capture the broader dynamics of the traffic system, as they primarily learn features specific to the detector configuration and traffic characteristics at the target location. Hence, the transferability of these models to different locations becomes challenging, particularly when data is unavailable at the new location for model training. To address this limitation, we propose a traffic flow physics-based feature transformation for spatio-temporal DL models. This transformation incorporates Newell's uncongested and congested-state estimators of traffic flows at the target loc
    
[^50]: 为少样本假设适应问题增强多样性的生成网络

    Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation. (arXiv:2307.05948v1 [cs.LG])

    [http://arxiv.org/abs/2307.05948](http://arxiv.org/abs/2307.05948)

    这篇论文提出了一种用于少样本假设适应问题的增强多样性生成网络，通过最小化生成数据的语义特征之间的HSIC值来生成多样化的无标签数据，从而提高学习效果。

    

    最近的研究表明，生成无标签数据有助于解决少样本假设适应（FHA）问题，其中我们旨在使用少量标记目标域数据和经过训练的源域分类器（即源假设）为目标域训练分类器，以获取高度兼容的无标签数据的附加信息。然而，现有方法生成的数据非常相似甚至相同。生成数据之间的强依赖性将导致学习失败。在本文中，我们提出了一种增强多样性的生成网络（DEG-Net）来解决FHA问题，该网络可以通过利用核独立性度量——希尔伯特-施密特独立性准则（HSIC）来生成多样化的无标签数据。具体而言，DEG-Net将通过最小化生成数据的语义特征之间的HSIC值（即最大化独立性）来生成数据。通过DEG-Net，生成的无标签数据更加多样化和有效。

    Generating unlabeled data has been recently shown to help address the few-shot hypothesis adaptation (FHA) problem, where we aim to train a classifier for the target domain with a few labeled target-domain data and a well-trained source-domain classifier (i.e., a source hypothesis), for the additional information of the highly-compatible unlabeled data. However, the generated data of the existing methods are extremely similar or even the same. The strong dependency among the generated data will lead the learning to fail. In this paper, we propose a diversity-enhancing generative network (DEG-Net) for the FHA problem, which can generate diverse unlabeled data with the help of a kernel independence measure: the Hilbert-Schmidt independence criterion (HSIC). Specifically, DEG-Net will generate data via minimizing the HSIC value (i.e., maximizing the independence) among the semantic features of the generated data. By DEG-Net, the generated unlabeled data are more diverse and more effective
    
[^51]: 一种贝叶斯方法用于量化交通预测模型中的不确定性和改善泛化能力

    A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])

    [http://arxiv.org/abs/2307.05946](http://arxiv.org/abs/2307.05946)

    本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。

    

    交通数据预测的深度学习模型可以通过多层架构对复杂函数进行优化建模，但这些方法的一个主要缺点是大多数方法不提供带有不确定性估计的预测结果，而这对于交通运营和控制是必需的。本研究提出了一种贝叶斯循环神经网络框架，通过引入谱归一化到其隐藏层，实现交通预测中的不确定性量化和更高的泛化能力。我们的论文表明，归一化通过控制模型的复杂性并减少对训练数据的过度拟合风险，改善了深度神经网络的泛化性能。

    Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization perfor
    
[^52]: YOGA: 用轻量级特征学习和多尺度注意力进行野外深度目标检测

    YOGA: Deep Object Detection in the Wild with Lightweight Feature Learning and Multiscale Attention. (arXiv:2307.05945v1 [cs.CV])

    [http://arxiv.org/abs/2307.05945](http://arxiv.org/abs/2307.05945)

    YOGA是一种基于深度学习的轻量级目标检测模型，采用廉价线性变换进行特征学习，并使用多尺度注意力机制进行特征融合。它在模型大小和准确性之间取得了最佳平衡，适用于低端边缘设备。

    

    我们引入了YOGA，一种基于深度学习的轻量级目标检测模型，可以在低端边缘设备上运行，同时仍然实现竞争性的准确性。YOGA架构由一个两阶段特征学习流水线和一个廉价线性变换组成，只使用常规卷积神经网络所需卷积滤波器数量的一半来学习特征图。此外，它使用注意机制在其颈部进行多尺度特征融合，而不是常规检测器使用的朴素连接。YOGA是一种灵活的模型，可以轻松地按数量级缩放，以适应各种硬件约束。我们在COCO-val和COCO-testdev数据集上与其他10多个最先进的目标检测器评估了YOGA。结果表明，YOGA在模型大小和准确性之间取得了最佳平衡（AP增加了22％，参数和FLOP减少了23-34％），这使它成为部署的理想选择。

    We introduce YOGA, a deep learning based yet lightweight object detection model that can operate on low-end edge devices while still achieving competitive accuracy. The YOGA architecture consists of a two-phase feature learning pipeline with a cheap linear transformation, which learns feature maps using only half of the convolution filters required by conventional convolutional neural networks. In addition, it performs multi-scale feature fusion in its neck using an attention mechanism instead of the naive concatenation used by conventional detectors. YOGA is a flexible model that can be easily scaled up or down by several orders of magnitude to fit a broad range of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets with other over 10 state-of-the-art object detectors. The results show that YOGA strikes the best trade-off between model size and accuracy (up to 22% increase of AP and 23-34% reduction of parameters and FLOPs), making it an ideal choice for deplo
    
[^53]: 使用图像技术填补时间序列间隙的方法：基于多维环境自编码器的建筑能源数据填补

    Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation. (arXiv:2307.05926v1 [cs.LG])

    [http://arxiv.org/abs/2307.05926](http://arxiv.org/abs/2307.05926)

    本论文提出了一种使用多维环境自编码器的方法来填补能源数据中的缺失间隙。这个方法可以解决能源系统准确预测和管理的问题，并提高数据在决策和研究中的可用性。

    

    建筑能源预测和管理在最近几十年中变得越来越重要，受到物联网设备的增长和更多能源数据的可用性的推动。然而，能源数据经常来自多个来源，可能不完整或不一致，这可能阻碍能源系统的准确预测和管理，并限制数据在决策和研究中的用途。为了解决这个问题，过去的研究集中于填补能源数据中的缺失间隙，包括随机间隙和连续间隙。这一领域的主要挑战之一是缺乏在各种建筑和仪表类型的基准数据集上进行验证，这使得难以准确评估不同填补方法的性能。另一个挑战是缺乏将最先进的填补方法应用于能源数据中的缺失间隙。现代图像修复方法，如部分卷积(PConv)，已经广泛应用

    Building energy prediction and management has become increasingly important in recent decades, driven by the growth of Internet of Things (IoT) devices and the availability of more energy data. However, energy data is often collected from multiple sources and can be incomplete or inconsistent, which can hinder accurate predictions and management of energy systems and limit the usefulness of the data for decision-making and research. To address this issue, past studies have focused on imputing missing gaps in energy data, including random and continuous gaps. One of the main challenges in this area is the lack of validation on a benchmark dataset with various building and meter types, making it difficult to accurately evaluate the performance of different imputation methods. Another challenge is the lack of application of state-of-the-art imputation methods for missing gaps in energy data. Contemporary image-inpainting methods, such as Partial Convolution (PConv), have been widely used 
    
[^54]: 带有持续提示的统一医学图像-文本-标签对比学习

    Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt. (arXiv:2307.05920v1 [eess.IV])

    [http://arxiv.org/abs/2307.05920](http://arxiv.org/abs/2307.05920)

    本论文提出了一种统一的医学图像-文本-标签对比学习方法，通过持续提示策略来解决医学图像-文本预训练中的隐私、样本差异和提示差异等挑战。

    

    对比性语言-图像预训练（CLIP）可以利用大规模未标记的图像-文本对数据集，在各种下游任务中展现出了令人印象深刻的性能。鉴于医学数据的注释是耗时且费力的，图像-文本预训练在利用大规模医学影像和放射学报告数据集方面具有很大的应用前景。然而，医学图像-文本预训练面临以下几个挑战：（1）由于隐私问题，可用的医学数据相对较少，与自然数据相比，模型的泛化能力较弱。（2）医学图像之间非常相似，细微差别很多，导致比较学习中有大量的假阴性样本对。（3）手工制作的提示通常与自然的医学图像报告不同，措辞上的细微变化可能导致性能的显著差异。本文提出了一种统一的图像-文本-标签对比学习方法，通过持续提示策略，解决了上述挑战。

    Contrastive language-image Pre-training (CLIP) [13] can leverage large datasets of unlabeled Image-Text pairs, which have demonstrated impressive performance in various downstream tasks. Given that annotating medical data is time-consuming and laborious, Image-Text Pre-training has promising applications in exploiting large-scale medical image and radiology report datasets. However, medical Image-Text Pre-training faces several challenges, as follows: (1) Due to privacy concerns, the amount of available medical data is relatively small compared to natural data, leading to weaker generalization ability of the model. (2) Medical images are highly similar with only fine-grained differences in subtleties, resulting in a large number of false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt usually differs from the natural medical image report, Subtle changes in wording can lead to significant differences in performance. In this paper, we propose a unified Image-Tex
    
[^55]: Prompt Generate Train (PGT): 一种用于领域特定开放式问题回答的检索增强生成模型的少样本领域适应、对齐和不确定性校准框架

    Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering. (arXiv:2307.05915v1 [cs.LG])

    [http://arxiv.org/abs/2307.05915](http://arxiv.org/abs/2307.05915)

    提出了一个名为Prompt Generate Train (PGT)的框架，用于少样本领域适应、对齐和不确定性校准的检索增强生成模型的开发。该框架通过有监督的微调和强化学习，将模型适应到目标领域，并在生成相关答案方面具有与基于GPT-4的模型相竞争的性能。

    

    我们提出了一种名为 Prompt, Generate, Train (PGT) 的框架，用于高效地开发一个针对专有的文本文档集合进行开放式问题回答的生成模型。该框架通过有监督的微调和强化学习，在少样本的情况下将检索增强的生成模型适应到目标领域。这产生了一个对齐、不确定性校准的模型，在生成相关答案时具有与基于 GPT-4 的上下文检索增强生成模型相竞争的性能，并且服务成本更低。通过使用中等规模的LLM (Flan-T5 XXL) 和一种新颖的一致性过滤方案，合成生成管道生成高质量合成训练数据。该管道旨在生成涵盖整个语料库的抽象和提取式问题。使用该数据集的样本，该框架对一个由稠密检索器和较小规模的LLM组成的较小的RAG模型进行微调。

    We present a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents. The framework adapts a retriever augmented generation model to the target domain using supervised finetuning and reinforcement learning with synthetic feedback in a few-shot setting. This yields an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs. The synthetic generation pipeline generates high quality synthetic training data musing a medium sized LLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is designed to generate both abstractive and extractive questions that span the entire corpus. Using samples from this dataset, the framework fine-tunes a smaller RAG model comprising a dense retriever and a smaller sized LLM on samples from the dataset.
    
[^56]: FIS-ONE: 使用一个标签的众包RF信号楼层识别系统

    FIS-ONE: Floor Identification System with One Label for Crowdsourced RF Signals. (arXiv:2307.05914v1 [cs.NI])

    [http://arxiv.org/abs/2307.05914](http://arxiv.org/abs/2307.05914)

    这项研究提出了FIS-ONE，一种使用一个标签的众包RF信号楼层识别系统。通过信号聚类和聚类索引的步骤，我们证明了仅使用一个底层带有标签的信号样本，以及其余的样本不带标签，也能实现有效的楼层识别。

    

    众包RF信号的楼层标签对于许多智慧城市应用非常重要，例如多楼层室内定位、地理围栏和机器人监控。传统方法使用众包RF信号进行楼层号码的识别模型建设时，假设每个楼层至少有几个带有标签的信号样本。在这项工作中，我们进一步突破界限，并且证明只需要一个带有标签的底层信号样本以及其余的信号样本不带标签也可以实现楼层识别。我们提出了一种新颖的FIS-ONE楼层识别系统，只使用一个带有标签的样本。FIS-ONE包括两个步骤，即信号聚类和聚类索引。我们首先构建一个二分图来建模RF信号样本，并使用基于注意力的图神经网络模型获得每个节点（每个信号样本）的潜在表示，以便实现楼层识别。

    Floor labels of crowdsourced RF signals are crucial for many smart-city applications, such as multi-floor indoor localization, geofencing, and robot surveillance. To build a prediction model to identify the floor number of a new RF signal upon its measurement, conventional approaches using the crowdsourced RF signals assume that at least few labeled signal samples are available on each floor. In this work, we push the envelope further and demonstrate that it is technically feasible to enable such floor identification with only one floor-labeled signal sample on the bottom floor while having the rest of signal samples unlabeled.  We propose FIS-ONE, a novel floor identification system with only one labeled sample. FIS-ONE consists of two steps, namely signal clustering and cluster indexing. We first build a bipartite graph to model the RF signal samples and obtain a latent representation of each node (each signal sample) using our attention-based graph neural network model so that the R
    
[^57]: 使用真实和生成的数据集的机器学习方法进行谷物和晶界分割

    Grain and Grain Boundary Segmentation using Machine Learning with Real and Generated Datasets. (arXiv:2307.05911v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.05911](http://arxiv.org/abs/2307.05911)

    本研究采用机器学习方法，在真实和生成的数据集上训练CNN模型，显著提高了谷物边界分割的准确度，并同时具备了手动分割的准确度和计算方法的高效性。

    

    我们使用卷积神经网络（CNN）在真实和生成的数据组合上训练，报告了谷物边界分割的显着提高的准确度。手动分割准确但耗时，现有的计算方法速度较快但常常不准确。为了解决这一困境，可以使用机器学习模型既能达到手动分割的准确度，又具有计算方法的效率。我们通过制备、抛光、蚀刻等过程，对316L不锈钢样品进行了大量数据收集，并进行了现有计算方法和手动分割创建“真实”训练数据。我们还开发了一种基于Voronoi图案和随机合成噪声、模拟缺陷的人工谷物图像制作方法，为数据密集型的机器学习方法提供了训练数据补充。

    We report significantly improved accuracy of grain boundary segmentation using Convolutional Neural Networks (CNN) trained on a combination of real and generated data. Manual segmentation is accurate but time-consuming, and existing computational methods are faster but often inaccurate. To combat this dilemma, machine learning models can be used to achieve the accuracy of manual segmentation and have the efficiency of a computational method. An extensive dataset of from 316L stainless steel samples is additively manufactured, prepared, polished, etched, and then microstructure grain images were systematically collected. Grain segmentation via existing computational methods and manual (by-hand) were conducted, to create "real" training data. A Voronoi tessellation pattern combined with random synthetic noise and simulated defects, is developed to create a novel artificial grain image fabrication method. This provided training data supplementation for data-intensive machine learning meth
    
[^58]: 预测性流水线解码：准确LLM解码中的计算延迟权衡

    Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])

    [http://arxiv.org/abs/2307.05908](http://arxiv.org/abs/2307.05908)

    本论文提出了一种预测性流水线解码（PPD）方法，通过并行启动后续令牌解码来加速大型语言模型（LLMs）中的贪婪解码过程，同时保持完全相同的输出。该方法在减少解码延迟方面具有潜力，提供了新的LLM解码策略权衡理解。

    

    本论文提出了一种名为"预测性流水线解码（PPD）"的方法，该方法可以加速大型语言模型（LLMs）中的贪婪解码，同时保持与原始解码完全相同的输出。与传统策略不同，PPD利用额外的计算资源在当前令牌解码期间并行启动后续令牌解码。这种创新方法减少了解码延迟，并重新塑造了LLM解码策略中的权衡理解。我们开发了一个理论框架，可以分析计算和延迟之间的权衡关系。使用这个框架，我们可以通过评估匹配率（表示为p_correct）来对我们提出的方法可能的延迟减少进行分析估计。结果表明，使用额外的计算资源有潜力加速LLM的贪婪解码过程。

    This paper presents "Predictive Pipelined Decoding (PPD)," an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding.
    
[^59]: 对比损失的小批量优化

    Mini-Batch Optimization of Contrastive Loss. (arXiv:2307.05906v1 [cs.LG])

    [http://arxiv.org/abs/2307.05906](http://arxiv.org/abs/2307.05906)

    本文研究了对比学习中小批量优化的理论方面，证明了全部选择小批量与全批量优化等效，提出了一种利用高损失小批量加速随机梯度下降收敛的方法，并通过谱聚类识别高损失小批量。

    

    对比学习作为一种自监督学习方法，引起了广泛关注。对比损失函数确保正样本对的嵌入（例如，同一类别的不同样本或同一对象的不同视图）相似，而负样本对的嵌入不相似。实际约束条件，如大内存需求，使得考虑所有可能的正负样本对变得具有挑战性，因此使用小批量优化。在本文中，我们研究了对比学习中小批量优化的理论方面。我们证明了如果且仅当选择了所有 $\binom{N}{B}$ 小批量，小批量优化与全批量优化等效，而仅考虑子集可能导致次优。我们随后证明了利用高损失小批量可以加速随机梯度下降的收敛，并提出了一种基于谱聚类的方法来识别这些高损失小批量。我们的实验表明，这种方法在多个数据集和模型上都取得了优越的性能。

    Contrastive learning has gained significant attention as a method for self-supervised learning. The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar. Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization. In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning. We show that mini-batch optimization is equivalent to full-batch optimization if and only if all $\binom{N}{B}$ mini-batches are selected, while sub-optimality may arise when examining only a subset. We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches. Our experime
    
[^60]: 带有乘法平滑的特征归因稳定性保证

    Stability Guarantees for Feature Attributions with Multiplicative Smoothing. (arXiv:2307.05902v1 [cs.LG])

    [http://arxiv.org/abs/2307.05902](http://arxiv.org/abs/2307.05902)

    本文提出了一种基于乘法平滑的特征归因方法，通过证明模型的Lipschitz性质，保证了其稳定性，并在视觉和语言模型上进行了评估，显示了非平凡的稳定性保证。

    

    机器学习模型的解释方法往往不能提供任何形式的保证，也可能不反映底层的决策过程。在这项工作中，我们将稳定性作为可靠的特征归因方法的一个属性进行分析。我们证明了如果模型在特征屏蔽方面具有足够的Lipschitz性质，则可以保证放松变体的稳定性。为了实现这样的模型，我们开发了一种称为乘法平滑（MuS）的平滑方法。我们展示了MuS克服了标准平滑技术的理论限制，并且可以与任何分类器和特征归因方法结合使用。我们使用各种特征归因方法（如LIME和SHAP）对视觉和语言模型进行了MuS的评估，并展示了MuS赋予了特征归因以非平凡的稳定性保证。

    Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.
    
[^61]: 非凸鲁棒主成分分析的深度展开

    Deep Unrolling for Nonconvex Robust Principal Component Analysis. (arXiv:2307.05893v1 [eess.SP])

    [http://arxiv.org/abs/2307.05893](http://arxiv.org/abs/2307.05893)

    这个论文提出了一种基于深度展开算法的非凸鲁棒主成分分析算法，能够自动学习超参数并在合成数据集和人脸建模问题上取得更好的数值和视觉性能。 (arXiv:2307.05893v1 [eess.SP])

    

    我们设计了用于鲁棒主成分分析（RPCA）的算法，该算法将一个矩阵分解为一个低秩矩阵和一个稀疏矩阵的和。我们提出了一种基于加速交替投影算法的深度展开算法，旨在解决非凸形式的RPCA问题。所提出的过程结合了深度神经网络的优势和原始算法的可解释性，并自动学习超参数。我们在合成数据集上展示了展开算法的有效性，并在人脸建模问题上展示了其在数值和视觉性能上的改进。

    We design algorithms for Robust Principal Component Analysis (RPCA) which consists in decomposing a matrix into the sum of a low rank matrix and a sparse matrix. We propose a deep unrolled algorithm based on an accelerated alternating projection algorithm which aims to solve RPCA in its nonconvex form. The proposed procedure combines benefits of deep neural networks and the interpretability of the original algorithm and it automatically learns hyperparameters. We demonstrate the unrolled algorithm's effectiveness on synthetic datasets and also on a face modeling problem, where it leads to both better numerical and visual performances.
    
[^62]: 受PID控制器启发的偏差归纳法在部分可观测控制任务中的深度强化学习

    PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])

    [http://arxiv.org/abs/2307.05891](http://arxiv.org/abs/2307.05891)

    该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。

    

    深度强化学习（RL）已经展现出通过数据自己学习控制系统的巨大潜力。然而，深度RL面临的一个挑战是系统的完整状态通常不可观测。当出现这种情况时，策略需要利用观察历史来推断当前状态。同时，训练和测试环境之间的差异使得策略不会过度拟合训练时观察到的序列。因此，在历史记录编码器灵活提取相关信息的同时，要对环境变化具有鲁棒性，这是一个重要的平衡。为了达到这个平衡，我们寻求PID控制器的启发。我们断定PID控制器的成功表明，许多控制任务只需要求和和求差来累积信息。基于这个原则，我们提出了两种用于编码历史记录的架构：一种直接使用...

    Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses
    
[^63]: 针对边缘/云计算环境中数字孪生的高效任务卸载算法

    Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment. (arXiv:2307.05888v1 [cs.LG])

    [http://arxiv.org/abs/2307.05888](http://arxiv.org/abs/2307.05888)

    本论文提出了一种新的数字孪生系统模型，考虑了异构MEC/MCC环境，并基于分布式深度学习提出了一种新的任务卸载方案，实现了高效的实时反馈。

    

    在物联网时代，数字孪生被视为连接实物对象和数字世界之间的桥梁，通过虚拟化和模拟技术，可以利用计算资源实现多种功能。在这个过程中，移动云计算和移动边缘计算已成为实现实时反馈的关键因素。然而，当前的研究只考虑了数字孪生系统模型中的边缘服务器或云服务器，同时忽略了具有多个数据资源的数字孪生。本文提出了一种考虑异构MEC/MCC环境的新的数字孪生系统模型，模型中的每个数字孪生都通过多个数据采集设备在服务器中维护。还考虑了卸载决策问题，并基于分布式深度学习提出了一种新的卸载方案。仿真结果表明，我们提出的算法可以有效且高效地解决这个问题。

    In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to empower various areas as a bridge between physical objects and the digital world. Through virtualization and simulation techniques, multiple functions can be achieved by leveraging computing resources. In this process, Mobile Cloud Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key factors to achieve real-time feedback. However, current works only considered edge servers or cloud servers in the DT system models. Besides, The models ignore the DT with not only one data resource. In this paper, we propose a new DT system model considering a heterogeneous MEC/MCC environment. Each DT in the model is maintained in one of the servers via multiple data collection devices. The offloading decision-making problem is also considered and a new offloading scheme is proposed based on Distributed Deep Learning (DDL). Simulation results demonstrate that our proposed algorithm can effectively and efficie
    
[^64]: 动态预测使用时变Cox生存神经网络

    Dynamic Prediction using Time-Dependent Cox Survival Neural Network. (arXiv:2307.05881v1 [stat.ML])

    [http://arxiv.org/abs/2307.05881](http://arxiv.org/abs/2307.05881)

    该论文通过使用time-dependent Cox模型和神经网络，提出了一种动态预测模型来预测进行性眼部疾病年龄相关性黄斑变性（AMD）的进展。通过使用纵向眼底图像作为输入，该模型可以建立一个个体化的风险预测模型，并且在实证研究中表现出良好的效果。

    

    动态预测的目标是在不断更新的新数据可用时提供个体化的风险预测。受到建立一个针对进行性眼部疾病，年龄相关性黄斑变性（AMD），我们提出了一种基于时变Cox模型的生存神经网络（tdCoxSNN）来预测其在持续时间尺度上的进展，使用纵向眼底图像。tdCoxSNN通过利用神经网络来模拟时变协变量对生存结果的非线性影响扩展了时变Cox模型。此外，通过结合卷积神经网络（CNN），tdCoxSNN可以以纵向原始图像作为输入。我们通过全面的模拟，使用两个时变精度度量标准，Brier分数和动态AUC比较和评估我们提出的方法与联合建模和里程碑方法。我们将所提出的方法应用于两个真实数据集。一个是一个大型AMD数据集。

    The target of dynamic prediction is to provide individualized risk predictions over time which can be updated as new data become available. Motivated by establishing a dynamic prediction model for the progressive eye disease, age-related macular degeneration (AMD), we proposed a time-dependent Cox model-based survival neural network (tdCoxSNN) to predict its progression on a continuous time scale using longitudinal fundus images. tdCoxSNN extends the time-dependent Cox model by utilizing a neural network to model the non-linear effect of the time-dependent covariates on the survival outcome. Additionally, by incorporating the convolutional neural network (CNN), tdCoxSNN can take the longitudinal raw images as input. We evaluate and compare our proposed method with joint modeling and landmarking approaches through comprehensive simulations using two time-dependent accuracy metrics, the Brier Score and dynamic AUC. We applied the proposed approach to two real datasets. One is a large AMD
    
[^65]: 部署机器学习的生态系统级分析揭示了同质化的结果

    Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])

    [http://arxiv.org/abs/2307.05862](http://arxiv.org/abs/2307.05862)

    部署机器学习存在系统性故障, 即使单个模型在总体上的改善也不能解决这个问题

    

    传统上，机器学习通常在模型层面进行研究：研究人员衡量和改进特定模型的准确性、鲁棒性、偏见、效率和其他维度。实际上，机器学习的社会影响取决于机器学习部署的周围环境。为了捕捉这一点，我们引入了生态系统级分析：不是分析单个模型，而是考虑在给定环境中部署的所有模型的集合。例如，在招聘中进行生态系统级分析意味着认识到一个求职者的结果不仅仅取决于单个招聘算法或公司，而是取决于他们申请的所有公司的集体决策。在三种模式（文本、图像、语音）和11个数据集上，我们建立了一个明显的趋势：部署的机器学习容易出现系统性故障，这意味着一些用户被所有可用的模型错误分类。即使在个体模型随时间在总体水平上改善，我们也发现

    Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we fin
    
[^66]: FAIRO: 面向人机交互系统中顺序决策的公平适应性

    FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])

    [http://arxiv.org/abs/2307.05857](http://arxiv.org/abs/2307.05857)

    这篇论文提出了一种名为FAIRO的算法，用于在人机交互环境中实现公平的顺序决策。该算法将人的行为可变性和偏好变化考虑在内，通过利用选项强化学习框架将复杂的公平任务分解为自适应子任务。

    

    在人机交互环境中实现顺序决策系统的公平性是一个重要问题，特别是当多个具有不同行为和期望的人受到系统中相同适应决策的影响时。人的可变性因素增加了复杂性，因为在某一时间点被认为是公平的政策可能会随着时间的推移由于人的偏好变化而成为歧视性政策。本文从公平性视角考虑人的行为可变性和人的偏好变化，解决了公平性问题。我们提出了一种新颖的算法FAIRO，用于面向人机交互适应的公平顺序决策，它将这些概念纳入决策过程中。具体而言，FAIRO将这个复杂的公平任务基于个体人的偏好通过利用选项强化学习框架分解为自适应子任务。

    Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design 
    
[^67]: PIGEON: 预测图像地理位置

    PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])

    [http://arxiv.org/abs/2307.05845](http://arxiv.org/abs/2307.05845)

    PIGEON是一个用于全球规模图像地理定位的多任务端到端系统，通过语义地理单元的创建和精化，以及无监督聚类和ProtoNets的应用，实现了最先进的性能，并提供了预训练的CLIP转换器模型StreetCLIP。

    

    我们引入PIGEON，一个用于全球规模图像地理定位的多任务端到端系统，在外部基准测试和人类评估中均实现了最先进的性能。我们的工作结合语义地理单元的创建和标签平滑，对具有地理信息的图像进行视觉转换器的预训练，并通过ProtoNets在候选地理单元集合中改进位置预测。PIGEON的贡献有三个方面：首先，我们设计了一种基于开源数据的语义地理单元创建和分割算法，可以适用于任何地理空间数据集。第二，我们展示了地理单元内部精化的有效性，并展示了无监督聚类和ProtoNets在该任务中的适用性。最后，我们将我们预训练的CLIP转换器模型，StreetCLIP，公开提供，可用于与应对气候变化和城市乡村场景理解相关的领域。

    We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
    
[^68]: 用经验共享来扩展分布式多任务强化学习

    Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing. (arXiv:2307.05834v1 [cs.LG])

    [http://arxiv.org/abs/2307.05834](http://arxiv.org/abs/2307.05834)

    本文研究了分布式多任务强化学习中的经验共享问题，提出了一种算法DistMT-LSVI，并通过理论和实证研究表明，使用DistMT-LSVI的单个代理可以实现所有任务的ε-最优策略。

    

    最近，DARPA推出了ShELL计划，旨在探索经验共享如何使分布式终身学习代理在适应新挑战方面受益。本文通过对分布式多任务强化学习（RL）进行理论和实证研究，解决了这个问题，其中一组N个代理协作解决了M个任务，而不需要先验知识。我们将问题表述为线性参数化的上下文马尔可夫决策过程（MDP），其中每个任务由指定转移动力学和奖励的上下文表示。为了解决这个问题，我们提出了一种名为DistMT-LSVI的算法。首先，代理识别任务，然后通过中央服务器交换信息，为任务导出ε-最优策略。我们的研究证明，为了实现所有M个任务的ε-最优策略，使用DistMT-LSVI的单个代理需要运行一定数量的操作。

    Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $\epsilon$-optimal policies for the tasks. Our research demonstrates that to achieve $\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number o
    
[^69]: 通过损失函数曲率视角揭示记忆化过程

    Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])

    [http://arxiv.org/abs/2307.05831](http://arxiv.org/abs/2307.05831)

    本研究通过对损失函数曲率进行分析，研究了神经网络在不同样本上的泛化与记忆化特性。我们发现高曲率的样本通常是具有标签错误或冲突的长尾样本，并在CIFAR100数据集上发现了一种新的失败模型。通过对部分样本进行随机标签错误，我们展示了曲率排序可以有效识别出这些样本。

    

    神经网络参数过多，很容易过拟合训练数据。极端情况下，它们可以完全记忆训练集，即使标签是随机的。我们提议使用训练样本周围的损失函数曲率作为记忆化程度的度量，对所有训练轮次进行平均。我们利用这个度量来研究常见图像数据集中不同样本的泛化与记忆化特性。我们可视化具有最高损失曲率的样本，发现它们通常是长尾样本、标签错误或冲突样本。这种分析帮助我们在CIFAR100数据集上发现了一种新的失败模型，即具有不同标签的重复图像。我们还通过随机错误化少量样本的标签来人为地给数据集引入标签错误，并展示了按曲率排序可以高效地识别出标签错误样本的高AUROC值。

    Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.
    
[^70]: 使用卷积和记忆网络在维基百科表格上进行关系抽取

    Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])

    [http://arxiv.org/abs/2307.05827](http://arxiv.org/abs/2307.05827)

    使用卷积和记忆网络，在维基百科的表格数据中进行关系抽取。该模型在关系抽取任务中表现出色，并且经过全面的分析和研究，展示了各个模型组件的贡献。

    

    关系抽取是从文本中提取实体之间关系的任务。大部分关系抽取方法从自由格式的连续文本中提取关系，而忽略了其他丰富的数据来源，比如表格。我们从应用神经网络方法处理表格化数据的角度探索关系抽取。我们引入了一个新模型，由卷积神经网络（CNN）和双向长短期记忆（BiLSTM）网络组成，分别用于编码实体和学习它们之间的依赖关系。我们在一个大规模且最新的数据集上评估了我们的模型，并与之前的神经网络方法进行了比较。实验结果显示，我们的模型在表格数据上的关系抽取任务中始终优于之前的模型。我们进行了全面的错误分析和剥离研究，以展示我们的模型的各个组成部分的贡献。最后，我们讨论了我们方法的实用性和权衡，并提供了进一步研究的建议。

    Relation extraction (RE) is the task of extracting relations between entities in text. Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables. We explore RE from the perspective of applying neural methods on tabularly organized data. We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively. We evaluate our model on a large and recent dataset and compare results with previous neural methods. Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data. We perform comprehensive error analyses and ablation study to show the contribution of various components of our model. Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research.
    
[^71]: 贝叶斯紧系数样条估计模式的数量

    Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v1 [stat.ME])

    [http://arxiv.org/abs/2307.05825](http://arxiv.org/abs/2307.05825)

    本研究提出了一种贝叶斯紧系数样条方法，用于估计概率密度函数中模式的数量。该方法结合了核估计器和组合样条，实现了特征探索、模型选择和模式检验，并允许引入专家判断。通过在体育分析中的案例研究中的验证，证明了该方法的实用性。

    

    概率密度函数中模式的数量代表模型的复杂性，也可以看作现有亚群体的数量。尽管其相关性，对其估计的研究非常有限。我们针对单变量情况提出一个新颖的方法，致力于预测准确性，受到了问题的一些被忽视的方面的启发。我们认为解决方案需要结构，模式的主观且不确定性，以及融合全局和局部密度特性的整体视图的便利性。我们的方法结合了灵活的核估计器和简洁的组合样条。特征探索、模型选择和模式检验都在贝叶斯推理范式中实现，为软解决方案提供了便利，并允许在过程中引入专家判断。我们的提议的实用性通过在体育分析中的案例研究中进行了验证，并展示了多个陪伴的可视化。

    The number of modes in a probability density function is representative of the model's complexity and can also be viewed as the number of existing subpopulations. Despite its relevance, little research has been devoted to its estimation. Focusing on the univariate setting, we propose a novel approach targeting prediction accuracy inspired by some overlooked aspects of the problem. We argue for the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view blending global and local density properties. Our method builds upon a combination of flexible kernel estimators and parsimonious compositional splines. Feature exploration, model selection and mode testing are implemented in the Bayesian inference paradigm, providing soft solutions and allowing to incorporate expert judgement in the process. The usefulness of our proposal is illustrated through a case study in sports analytics, showcasing multiple companion visualisation 
    
[^72]: 虚拟电力厂在日前电力市场中的战略竞标的安全增强学习算法

    Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets. (arXiv:2307.05812v1 [eess.SY])

    [http://arxiv.org/abs/2307.05812](http://arxiv.org/abs/2307.05812)

    本文提出了一种新颖的安全增强学习算法，用于虚拟电力厂在日前电力市场中的战略竞标。该算法不需要精确的市场模型，通过深度确定性策略梯度方法学习具有竞争力的竞标策略，并通过引入安全屏蔽和奖励函数的惩罚机制来考虑虚拟电力厂的物理约束，使代理能够学习到更安全的策略。

    

    本文提出了一种新颖的安全增强学习算法，用于虚拟电力厂在日前电力市场中的战略竞标。提出的算法采用深度确定性策略梯度（DDPG）方法，学习具有竞争力的竞标策略，而不需要精确的市场模型。此外，为了考虑虚拟电力厂的复杂内部物理约束，我们对DDPG方法进行了两个增强。首先，推导了一种基于投影的安全屏蔽，将代理的行为限制在分布式能源资源的非线性功率流方程和运行约束所定义的可行空间内。其次，引入了奖励函数中的屏蔽激活惩罚，鼓励代理学习更安全的策略。基于IEEE 13-bus网络的案例研究证明了所提方法在使代理学习到高竞争力，安全的战略策略方面的有效性。

    This paper presents a novel safe reinforcement learning algorithm for strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient (DDPG) method to learn competitive bidding policies without requiring an accurate market model. Furthermore, to account for the complex internal physical constraints of VPPs we introduce two enhancements to the DDPG method. Firstly, a projection-based safety shield that restricts the agent's actions to the feasible space defined by the non-linear power flow equations and operating constraints of distributed energy resources is derived. Secondly, a penalty for the shield activation in the reward function that incentivizes the agent to learn a safer policy is introduced. A case study based on the IEEE 13-bus network demonstrates the effectiveness of the proposed approach in enabling the agent to learn a highly competitive, safe strategic policy.
    
[^73]: X射线计算机断层扫描的可微分前向投影器

    Differentiable Forward Projector for X-ray Computed Tomography. (arXiv:2307.05801v1 [cs.LG])

    [http://arxiv.org/abs/2307.05801](http://arxiv.org/abs/2307.05801)

    本文提出了一个准确的可微分前向和后向投影软件库，以确保预测的图像与原始测量之间的一致性，并支持各种投影几何类型，并且最小化了GPU内存占用。

    

    数据驱动的深度学习已成功应用于各种计算机断层扫描重建问题。深度推断模型可能会在病态CT重建中胜过现有的解析和迭代算法。然而，这些方法通常预测的图像与测量的投影数据不一致。本文提出了一个准确的可微分前向和后向投影软件库，以确保预测的图像与原始测量值之间的一致性。该软件库有效地支持各种投影几何类型，同时最小化了GPU内存占用，有利于与现有的深度学习训练和推断流程的无缝集成。所提出的软件库可以作为开源工具来使用：https://github.com/LLNL/LEAP。

    Data-driven deep learning has been successfully applied to various computed tomographic reconstruction problems. The deep inference models may outperform existing analytical and iterative algorithms, especially in ill-posed CT reconstruction. However, those methods often predict images that do not agree with the measured projection data. This paper presents an accurate differentiable forward and back projection software library to ensure the consistency between the predicted images and the original measurements. The software library efficiently supports various projection geometry types while minimizing the GPU memory footprint requirement, which facilitates seamless integration with existing deep learning training and inference pipelines. The proposed software is available as open source: https://github.com/LLNL/LEAP.
    
[^74]: 基于与疼痛相关的电压门控钠通道的扩展药物靶标相互作用网络的机器学习研究

    Machine Learning Study of the Extended Drug-target Interaction Network informed by Pain Related Voltage-Gated Sodium Channels. (arXiv:2307.05794v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.05794](http://arxiv.org/abs/2307.05794)

    本研究基于与疼痛相关的钠通道构建了蛋白质相互作用网络，并开发了药物靶标相互作用网络，以发现疼痛管理的潜在首选化合物。

    

    疼痛是一个重要的全球健康问题，目前的疼痛管理治疗选择在效果、副作用和潜在成瘾性方面存在局限性。改进疼痛治疗和开发新药物的需求迫切。电压门控钠通道，特别是Nav1.3、Nav1.7、Nav1.8和Nav1.9，在神经兴奋性中起着关键作用，并主要表达于外周神经系统中。靶向这些通道可能提供治疗疼痛的手段，同时减少中央和心脏不良反应。在本研究中，我们基于与疼痛相关的钠通道构建蛋白质相互作用（PPI）网络，并开发相应的药物靶标相互作用（DTI）网络，以寻找疼痛管理的潜在首选化合物。为确保可靠的机器学习预测，我们从超过1,000个目标的PPI网络中精选了111个抑制剂数据集。我们采用三种不同的机器学习算法进行预测。

    Pain is a significant global health issue, and the current treatment options for pain management have limitations in terms of effectiveness, side effects, and potential for addiction. There is a pressing need for improved pain treatments and the development of new drugs. Voltage-gated sodium channels, particularly Nav1.3, Nav1.7, Nav1.8, and Nav1.9, play a crucial role in neuronal excitability and are predominantly expressed in the peripheral nervous system. Targeting these channels may provide a means to treat pain while minimizing central and cardiac adverse effects. In this study, we construct protein-protein interaction (PPI) networks based on pain-related sodium channels and develop a corresponding drug-target interaction (DTI) network to identify potential lead compounds for pain management. To ensure reliable machine learning predictions, we carefully select 111 inhibitor datasets from a pool of over 1,000 targets in the PPI network. We employ three distinct machine learning alg
    
[^75]: 隐式正则化在随机梯度下降中的应用：从单目标到双人游戏

    Implicit regularisation in stochastic gradient descent: from single-objective to two-player games. (arXiv:2307.05789v1 [stat.ML])

    [http://arxiv.org/abs/2307.05789](http://arxiv.org/abs/2307.05789)

    本文研究了隐式正则化在随机梯度下降中的应用，通过向后误差分析构建连续时间流量来量化离散优化器的离散化误差，并提供了一种新的使用BEA的方法。

    

    近年来，通过发现常用的基于梯度的优化器的隐式正则化效应，为深度学习优化带来了许多新的见解。理解隐式正则化不仅可以揭示优化动态，还可以用于改善性能和稳定性，涉及到从有监督学习到生成对抗网络等问题领域的两人游戏。通过向后误差分析（BEA）构建的连续时间流量来量化离散优化器的离散化误差是找到隐式正则化效应的一种方法。然而，目前BEA的使用存在限制，因为并不是通过BEA获得的所有连续时间流的向量场都可以写成梯度，这阻碍了构建揭示隐式正则化器的修正损失。在这项工作中，我们提供了一种新的使用BEA的方法，并展示了我们的方法如何用于构建连续时间流量。

    Recent years have seen many insights on deep learning optimisation being brought forward by finding implicit regularisation effects of commonly used gradient-based optimisers. Understanding implicit regularisation can not only shed light on optimisation dynamics, but it can also be used to improve performance and stability across problem domains, from supervised learning to two-player games such as Generative Adversarial Networks. An avenue for finding such implicit regularisation effects has been quantifying the discretisation errors of discrete optimisers via continuous-time flows constructed by backward error analysis (BEA). The current usage of BEA is not without limitations, since not all the vector fields of continuous-time flows obtained using BEA can be written as a gradient, hindering the construction of modified losses revealing implicit regularisers. In this work, we provide a novel approach to use BEA, and show how our approach can be used to construct continuous-time flows
    
[^76]: 将Nyström方法用于低秩逼近的高精度方法

    Making the Nystr\"om method highly accurate for low-rank approximations. (arXiv:2307.05785v1 [math.NA])

    [http://arxiv.org/abs/2307.05785](http://arxiv.org/abs/2307.05785)

    本论文提出了一种将Nyström方法用于低秩逼近的高精度方法，通过多种启发式策略使得该方法能够逼近非对称和长宽比不同的矩阵，并提出了快速子集更新策略加速细化过程。

    

    Nyström方法是一种方便的启发式方法，以近线性的复杂度获得低秩逼近的核矩阵。现有的研究通常使用该方法对低或中等精度的正半定矩阵进行逼近。在这项工作中，我们提出了一系列启发式策略，使得Nyström方法能够对非对称和/或长宽比不同的矩阵实现高精度。所得到的方法（称为高精度Nyström方法）将Nyström方法和瘦秩显露分解进行了快速的枢轴策略，在渐进交替方向细化过程中实现了两种细化机制：从随机选择的少数列开始交替行和列枢轴，并自适应增加样本数量，直到达到所需的秩或精度。进一步提出了一种基于逐步采样Schur补的快速子集更新策略，以加速细化过程。

    The Nystr\"om method is a convenient heuristic method to obtain low-rank approximations to kernel matrices in nearly linear complexity. Existing studies typically use the method to approximate positive semidefinite matrices with low or modest accuracies. In this work, we propose a series of heuristic strategies to make the Nystr\"om method reach high accuracies for nonsymmetric and/or rectangular matrices. The resulting methods (called high-accuracy Nystr\"om methods) treat the Nystr\"om method and a skinny rank-revealing factorization as a fast pivoting strategy in a progressive alternating direction refinement process. Two refinement mechanisms are used: alternating the row and column pivoting starting from a small set of randomly chosen columns, and adaptively increasing the number of samples until a desired rank or accuracy is reached. A fast subset update strategy based on the progressive sampling of Schur complements is further proposed to accelerate the refinement process. Effic
    
[^77]: Weisfeiler和Lehman进行度量建模：探索WL测试的有效性

    Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test. (arXiv:2307.05775v1 [cs.LG])

    [http://arxiv.org/abs/2307.05775](http://arxiv.org/abs/2307.05775)

    本文通过系统分析，揭示了$k$-WL的可靠性和有效性问题，并提出了基于基准的表达能力的外延定义和测量。

    

    图神经网络的表达能力通常通过比较一个架构能够区分的非同构图或节点对的数量与$k$-维Weisfeiler-Lehman ($k$-WL)测试能够区分的数量来衡量。本文通过对$k$-WL的可靠性和有效性进行系统分析，揭示了从业者对表达能力和$k$-WL的概念化之间的不一致性。我们进一步进行了对从业者的调查（n=18），以了解他们对表达能力的概念以及对$k$-WL的假设。与从业者的观点相反，我们的分析（借鉴了图论和基准审核）揭示了$k$-WL不能保证等距、可能与现实世界的图任务无关，并且可能不促进泛化或可信度。我们主张基于基准的表达能力的外延定义和测量，进一步贡献了构建此类基准的指导问题。

    The expressive power of graph neural networks is usually measured by comparing how many pairs of graphs or nodes an architecture can possibly distinguish as non-isomorphic to those distinguishable by the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments between practitioners' conceptualizations of expressive power and $k$-WL through a systematic analysis of the reliability and validity of $k$-WL. We further conduct a survey ($n = 18$) of practitioners to surface their conceptualizations of expressive power and their assumptions about $k$-WL. In contrast to practitioners' opinions, our analysis (which draws from graph theory and benchmark auditing) reveals that $k$-WL does not guarantee isometry, can be irrelevant to real-world graph tasks, and may not promote generalization or trustworthiness. We argue for extensional definitions and measurement of expressive power based on benchmarks; we further contribute guiding questions for constructing such 
    
[^78]: 随机集合卷积神经网络（RS-CNN）用于认识论深度学习

    Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])

    [http://arxiv.org/abs/2307.05772](http://arxiv.org/abs/2307.05772)

    这篇论文提出了一种新的随机集合卷积神经网络（RS-CNN）用于分类，通过预测信念函数而不是概率矢量集合，以表示模型的置信度和认识不确定性。基于认识论深度学习方法，该模型能够估计由有限训练集引起的认识不确定性。

    

    机器学习越来越多地应用于安全关键领域，对抗攻击的鲁棒性至关重要，错误的预测可能导致潜在的灾难性后果。这突出了学习系统需要能够确定模型对其预测的置信度以及与之相关联的认识不确定性的手段，“知道一个模型不知道”。在本文中，我们提出了一种新颖的用于分类的随机集合卷积神经网络（RS-CNN），其预测信念函数而不是概率矢量集合，使用随机集合的数学，即对样本空间的幂集的分布。基于认识论深度学习方法，随机集模型能够表示机器学习中由有限训练集引起的“认识性”不确定性。我们通过近似预测信念函数相关联的置信集的大小来估计认识不确定性。

    Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
    
[^79]: 通过强化学习实现的实时频谱监测--Q学习和启发式方法的比较

    Realtime Spectrum Monitoring via Reinforcement Learning -- A Comparison Between Q-Learning and Heuristic Methods. (arXiv:2307.05763v1 [eess.SY])

    [http://arxiv.org/abs/2307.05763](http://arxiv.org/abs/2307.05763)

    本文比较了线性频率调谐作为启发式方法和强化学习中的Q学习算法在实时频谱监测中的表现，研究表明在非均匀信号活动的场景下，使用Q学习算法的检测率显著高于启发式方法。

    

    由于无线电技术的技术进步和可用性，无线电频谱中干扰信号的数量不断增加。为了保持标准并保持紧急频率的开放，必须及时检测干扰信号。为此，使用专门的（多通道）接收器进行频谱监测。本文比较了两种不同的方法来控制可用接收器资源的性能。资源管理（ReMa）所使用的方法是线性频率调谐作为启发式方法和来自强化学习领域的Q学习算法。为了测试要研究的方法，设计了一个简化的场景，其中两个接收器通道监测十个非重叠的频带，具有非均匀的信号活动。对于这种设置，结果表明，在相同的环境下，Q学习算法比启发式方法有更高的检测率。

    Due to technological advances in the field of radio technology and its availability, the number of interference signals in the radio spectrum is continuously increasing. Interference signals must be detected in a timely fashion, in order to maintain standards and keep emergency frequencies open. To this end, specialized (multi-channel) receivers are used for spectrum monitoring. In this paper, the performances of two different approaches for controlling the available receiver resources are compared. The methods used for resource management (ReMa) are linear frequency tuning as a heuristic approach and a Q-learning algorithm from the field of reinforcement learning. To test the methods to be investigated, a simplified scenario was designed with two receiver channels monitoring ten non-overlapping frequency bands with non-uniform signal activity. For this setting, it is shown that the Q-learning algorithm used has a significantly higher detection rate than the heuristic approach at the e
    
[^80]: Fermat距离：度量逼近、谱收敛和聚类算法

    Fermat Distances: Metric Approximation, Spectral Convergence, and Clustering Algorithms. (arXiv:2307.05750v1 [stat.ML])

    [http://arxiv.org/abs/2307.05750](http://arxiv.org/abs/2307.05750)

    本研究分析了Fermat距离的收敛性质和其在聚类算法中的应用。我们证明了离散采样的Fermat距离在小邻域中收敛于它们的连续模拟，同时也证明了基于离散采样的Fermat距离的离散图拉普拉斯算子收敛于连续算子。

    

    我们分析了Fermat距离的收敛性质，这是一类在具有关联概率测度的Riemann流形上定义的密度驱动度量。Fermat距离可以在离散采样上定义，此时它们是随机的；也可以在连续设置中定义，此时它们由密度扭曲的Riemann度量下的测地线导出。我们证明了基于离散采样的Fermat距离在小邻域中收敛于它们的连续模拟，收敛速率取决于数据的内在维度和Fermat距离中密度加权的参数。这是通过利用新颖的几何和统计论证在渗流理论中允许非均匀密度和曲面域的方法完成的。然后，我们利用这些结果证明了基于离散，采样驱动的Fermat距离的离散图拉普拉斯算子收敛于相应的连续算子。

    We analyze the convergence properties of Fermat distances, a family of density-driven metrics defined on Riemannian manifolds with an associated probability measure. Fermat distances may be defined either on discrete samples from the underlying measure, in which case they are random, or in the continuum setting, in which they are induced by geodesics under a density-distorted Riemannian metric. We prove that discrete, sample-based Fermat distances converge to their continuum analogues in small neighborhoods with a precise rate that depends on the intrinsic dimensionality of the data and the parameter governing the extent of density weighting in Fermat distances. This is done by leveraging novel geometric and statistical arguments in percolation theory that allow for non-uniform densities and curved domains. Our results are then used to prove that discrete graph Laplacians based on discrete, sample-driven Fermat distances converge to corresponding continuum operators. In particular, we 
    
[^81]: 将课程与回放相结合：对持续学习的影响

    Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])

    [http://arxiv.org/abs/2307.05747](http://arxiv.org/abs/2307.05747)

    将课程与回放方法相结合可以提高持续学习的效果，通过调整回放实例与训练数据的交替频率、回放实例的顺序以及选择进入回放缓冲区的策略实现。

    

    人类在获取新技能或知识时，通过课程进行学习和复习。这种人类学习行为启发了在持续学习代理中将课程与回放方法相结合。目标是模拟人类学习过程，从而提高知识保留和促进学习转移。现有的持续学习代理中的回放方法涉及从先前任务中随机选择和排序数据，已经证明是有效的。然而，有限的研究探讨了将不同课程与回放方法相结合以增强持续学习的问题。我们的研究首次考察了将课程与回放方法相结合对持续学习的影响的三个具体方面：回放实例与训练数据的交替频率，回放实例的顺序，以及选择实例进入回放缓冲区的策略。这些课程设计的方面

    Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design al
    
[^82]: GOKU-UI：通过关注力和多射击实现连续时间生成模型的普适推理

    GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models. (arXiv:2307.05735v1 [cs.LG])

    [http://arxiv.org/abs/2307.05735](http://arxiv.org/abs/2307.05735)

    GOKU-UI是一种普适推理的SciML生成模型，通过关注机制和多射击训练策略，在包括随机微分方程在内的各种微分方程中展现出了优异的性能表现。它具有显著的数据效率，并在合成和实证数据集上优于所有基线模型。

    

    科学机器学习（SciML）是一个蓬勃发展的领域，它将领域感知和可解释的模型与通用的机器学习技术相结合。在这项工作中，我们介绍了GOKU-UI，这是SciML生成模型GOKU-nets的一种演进。GOKU-UI扩展了原始模型的范围，将其他类别的微分方程，如随机微分方程（SDEs），融入其中，并通过关注机制和在潜在空间中的新型多射击训练策略实现了分布式的、即无处不在的推理。这些改进使其在重建和预测任务中的性能显著提高，我们通过对模拟和实证数据的评估进行了验证。具体而言，即使训练集的大小缩小了32倍，GOKU-UI在合成数据集上表现出色，超过了所有基线模型，凸显其出色的数据效率。此外，当应用于实证的人脑数据时，同时融合了随机微分方程和注意力机制的GOKU-UI也展现出了出色的性能。

    Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. The GOKU-UI broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e. ubiquitous, inference through attention mechanisms and a novel multiple shooting training strategy in the latent space. These enhancements have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation of simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 32-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stoch
    
[^83]: 面向组合分类中多组公平性改善的可扩展解决方案

    Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification. (arXiv:2307.05728v1 [cs.LG])

    [http://arxiv.org/abs/2307.05728](http://arxiv.org/abs/2307.05728)

    本论文提出面向组合分类中多组公平性改善的可扩展解决方案。通过引入任务过条件化和组间交替的技术，在多组多标签设定下实现了恒定比例缩放，并在学术和实际环境中的实验中证明了其有效性。

    

    尽管机器学习公平性方面的研究文献丰富，但对于修复复杂系统（其中最终预测是多个分类器的组合，存在多个组）的关注相对较少。本文首先展示了用于改善机会平等公平性的自然基线方法与被修复组数和被修复预测标签数的乘积呈线性关系，使其不实用。然后，我们引入了两种简单的技术，称为“任务过条件化”和“组间交替”，以在多组多标签设定中实现恒定比例缩放。我们在学术和实际环境中的实验结果证明了我们的方案在这个环境中的有效性。

    Despite the rich literature on machine learning fairness, relatively little attention has been paid to remediating complex systems, where the final prediction is the combination of multiple classifiers and where multiple groups are present. In this paper, we first show that natural baseline approaches for improving equal opportunity fairness scale linearly with the product of the number of remediated groups and the number of remediated prediction labels, rendering them impractical. We then introduce two simple techniques, called {\em task-overconditioning} and {\em group-interleaving}, to achieve a constant scaling in this multi-group multi-label setup. Our experimental results in academic and real-world environments demonstrate the effectiveness of our proposal at mitigation within this environment.
    
[^84]: MoP-CLIP：一种用于领域增量学习的混合Prompt-Tuned CLIP模型

    MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning. (arXiv:2307.05707v1 [cs.CV])

    [http://arxiv.org/abs/2307.05707](http://arxiv.org/abs/2307.05707)

    MoP-CLIP是一种用于领域增量学习的混合Prompt-Tuned CLIP模型，该模型通过模拟每个领域中每个类的特征分布并学习个体化的提示，实现了对分布内外数据的处理和推断。

    

    尽管增量学习取得了近期的进展，但解决在分布漂移下的灾难性遗忘问题仍然是一个开放且重要的问题。尽管当前领域增量学习（DIL）方法在已知领域内表现令人满意，但在新领域中性能大幅下降。这种限制使其难以泛化，并限制了在训练和测试数据来源于不同分布的更现实的情景下的可扩展性。为了解决这些限制，我们提出了一种基于混合Prompt-Tuned CLIP模型（MoP-CLIP）的新型DIL方法，该方法将S-Prompting的范式推广到处理推断时的分布内和分布外数据。具体而言，在训练阶段，我们对每个领域中每个类的特征分布进行建模，并学习个体化的文本和视觉提示来适应给定的领域。在推断阶段，学到的分布可以帮助我们识别是否

    Despite the recent progress in incremental learning, addressing catastrophic forgetting under distributional drift is still an open and important problem. Indeed, while state-of-the-art domain incremental learning (DIL) methods perform satisfactorily within known domains, their performance largely degrades in the presence of novel domains. This limitation hampers their generalizability, and restricts their scalability to more realistic settings where train and test data are drawn from different distributions. To address these limitations, we present a novel DIL approach based on a mixture of prompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of S-Prompting to handle both in-distribution and out-of-distribution data at inference. In particular, at the training stage we model the features distribution of every class in each domain, learning individual text and visual prompts to adapt to a given domain. At inference, the learned distributions allow us to identify whether 
    
[^85]: 无监督表示学习的因果排序先验方法

    A Causal Ordering Prior for Unsupervised Representation Learning. (arXiv:2307.05704v1 [cs.LG])

    [http://arxiv.org/abs/2307.05704](http://arxiv.org/abs/2307.05704)

    该论文提出了一种无监督表示学习的方法，通过考虑具有潜在加性噪声模型的数据生成过程，以及基于潜在分布的海森矩阵的损失函数，鼓励潜在空间遵循因果排序。

    

    无监督表示学习依赖于对潜变量的独立性假设。然而，因果表示学习（CRL）认为数据集中的变异因素实际上是因果相关的。允许潜变量由于因果关系而相关性更加真实和可泛化。到目前为止，可证明可识别的方法依赖于：辅助信息、弱标签，以及干预或甚至对照数据。受到功能因果模型的因果发现的启发，我们提出了一种完全无监督的表示学习方法，考虑了具有潜在加性噪声模型（ANM）的数据生成过程。通过基于潜在分布的海森矩阵的损失函数，我们鼓励潜在空间遵循因果排序。

    Unsupervised representation learning with variational inference relies heavily on independence assumptions over latent variables. Causal representation learning (CRL), however, argues that factors of variation in a dataset are, in fact, causally related. Allowing latent variables to be correlated, as a consequence of causal relationships, is more realistic and generalisable. So far, provably identifiable methods rely on: auxiliary information, weak labels, and interventional or even counterfactual data. Inspired by causal discovery with functional causal models, we propose a fully unsupervised representation learning method that considers a data generation process with a latent additive noise model (ANM). We encourage the latent space to follow a causal ordering via loss function based on the Hessian of the latent distribution.
    
[^86]: 非稳态自动投标世界中的在线广告采购

    Online Ad Procurement in Non-stationary Autobidding Worlds. (arXiv:2307.05698v1 [cs.IR])

    [http://arxiv.org/abs/2307.05698](http://arxiv.org/abs/2307.05698)

    提出了一个在线学习框架，帮助广告商在非稳态采购环境下动态优化广告平台参数决策。

    

    当今的在线广告商通过与自动投标平台进行交互来采购数字广告展示：广告商通过设置预算、目标投资回报率、每次点击的最大成本等参数来传达高级采购目标。然后广告平台代表广告商采购展示，并向广告商报告最终采购转化结果（例如点击量）。在实践中，广告商可能只会接收到平台采购细节的最少信息，并且采购结果受到季节性模式、偶发性系统故障和市场趋势等非稳态因素的影响，这使得广告商难以有效优化参数决策。鉴于此，我们提出了一个在线学习框架，帮助广告商在具有非稳态采购结果的现实多臂赌博环境下，在受通用长期约束条件限制的情况下动态优化广告平台的参数决策。具体而言，我们引入了一个原始的-d

    Today's online advertisers procure digital ad impressions through interacting with autobidding platforms: advertisers convey high level procurement goals via setting levers such as budget, target return-on-investment, max cost per click, etc.. Then ads platforms subsequently procure impressions on advertisers' behalf, and report final procurement conversions (e.g. click) to advertisers. In practice, advertisers may receive minimal information on platforms' procurement details, and procurement outcomes are subject to non-stationary factors like seasonal patterns, occasional system corruptions, and market trends which make it difficult for advertisers to optimize lever decisions effectively. Motivated by this, we present an online learning framework that helps advertisers dynamically optimize ad platform lever decisions while subject to general long-term constraints in a realistic bandit feedback environment with non-stationary procurement outcomes. In particular, we introduce a primal-d
    
[^87]: 一个动态个性化拼车服务的机器学习排序算法

    A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car Pooling Services. (arXiv:2307.05697v1 [cs.IR])

    [http://arxiv.org/abs/2307.05697](http://arxiv.org/abs/2307.05697)

    本研究提出了GoTogether，一个利用学习排序技术为拼车服务提供个性化推荐的系统。通过分析用户的历史选择，GoTogether能够预测个人共乘的愿望，并提供高成功率的拼车匹配。

    

    通过使司机与具有相似行程和时间安排的旅客共享汽车，拼车被期望在减少交通拥堵和污染方面发挥重要作用。为了在一组司机和潜在乘客中高效地找到成功的拼车匹配，设计了许多拼车匹配服务。然而，现在已经认识到除了简单的出行需求外，许多非货币方面和社会因素可能影响个人愿意共乘的意愿，这些因素很难预测。为了解决这个问题，在这项研究中，我们提出了GoTogether，这是一个拼车服务的推荐系统，它利用学习排序技术从用户的选择历史（即接受或拒绝共享乘车的类型）中自动推导出每个用户的个性化排序模型。然后，GoTogether构建推荐乘车列表以最大化匹配成功率。

    Car pooling is expected to significantly help in reducing traffic congestion and pollution in cities by enabling drivers to share their cars with travellers with similar itineraries and time schedules. A number of car pooling matching services have been designed in order to efficiently find successful ride matches in a given pool of drivers and potential passengers. However, it is now recognised that many non-monetary aspects and social considerations, besides simple mobility needs, may influence the individual willingness of sharing a ride, which are difficult to predict. To address this problem, in this study we propose GoTogether, a recommender system for car pooling services that leverages on learning-to-rank techniques to automatically derive the personalised ranking model of each user from the history of her choices (i.e., the type of accepted or rejected shared rides). Then, GoTogether builds the list of recommended rides in order to maximise the success rate of the offered matc
    
[^88]: 以不同方式堆叠更多层：通过低秩更新进行高秩训练

    Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])

    [http://arxiv.org/abs/2307.05695](http://arxiv.org/abs/2307.05695)

    本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。

    

    尽管大规模网络拥有数百亿个参数的规模已经占主导地位并且效果显著，但对于过度参数化模型的训练必要性仍然缺乏清晰的理解，而替代方法不一定能够降低训练高性能模型的成本。本文探索了低秩训练技术作为训练大型神经网络的替代方法。我们引入了一种称为ReLoRA的新方法，它利用低秩更新来训练高秩网络。我们将ReLoRA应用于预训练的Transformer语言模型，参数量高达350M，并且证明了与常规神经网络训练相当的性能。此外，我们观察到ReLoRA的效率随着模型大小的增加而提高，这使得它成为高效训练千亿级参数网络的有希望的方法。我们的研究结果揭示了低秩训练技术的潜力及其对于缩放定律的影响。

    Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
    
[^89]: 《科技文档中的图形分类技术综述》

    A Survey on Figure Classification Techniques in Scientific Documents. (arXiv:2307.05694v1 [cs.IR])

    [http://arxiv.org/abs/2307.05694](http://arxiv.org/abs/2307.05694)

    《科技文档中的图形分类技术综述》对图形分类问题进行了系统梳理，包括表格、照片、图表、地图和绘图五类，并批判性地评述了现有方法和数据集，并提出了进一步研究的方向。

    

    图形对于传达科学事实和信息起着重要作用，近年来，通过人工智能和机器学习技术从图形中提取数据成为研究热点。本综述系统地将图形分为表格、照片、图表、地图和绘图五类，并对解决图形分类问题的现有方法和数据集进行批判性综述。最后，我们指出当前研究的空白并提出了进一步研究图形分类的可能方向。

    Figures visually represent an essential piece of information and provide an effective means to communicate scientific facts. Recently there have been many efforts toward extracting data directly from figures, specifically from tables, diagrams, and plots, using different Artificial Intelligence and Machine Learning techniques. This is because removing information from figures could lead to deeper insights into the concepts highlighted in the scientific documents. In this survey paper, we systematically categorize figures into five classes - tables, photos, diagrams, maps, and plots, and subsequently present a critical review of the existing methodologies and data sets that address the problem of figure classification. Finally, we identify the current research gaps and provide possible directions for further research on figure classification.
    
[^90]: 使用基于Transformer的深度强化学习进行多目标水电站水库调度优化

    Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning. (arXiv:2307.05643v1 [cs.LG])

    [http://arxiv.org/abs/2307.05643](http://arxiv.org/abs/2307.05643)

    提出了一种使用基于Transformer的深度强化学习方法来优化多目标水电站水库调度问题。实验结果表明，该方法相较于最先进的方法具有更好的运行效果。

    

    由于水资源短缺和水需求增加，多水库系统的联合运行以平衡发电、生态保护和居民用水供应已成为水电管理中的关键问题。然而，多个水库的众多约束和非线性性使得解决这个问题非常耗时。为了应对这一挑战，提出了一种融合Transformer框架的深度强化学习方法。编码器的多头注意力机制有效地从水库和居民区提取信息，解码器的多水库注意网络生成适当的运行决策。将该方法应用于科罗拉多河流域的梅德湖和鲍威尔湖。实验结果表明，基于Transformer的深度强化学习方法能够产生合适的运营结果。与最先进的方法相比，该方法的运行效果更好。

    Due to shortage of water resources and increasing water demands, the joint operation of multireservoir systems for balancing power generation, ecological protection, and the residential water supply has become a critical issue in hydropower management. However, the numerous constraints and nonlinearity of multiple reservoirs make solving this problem time-consuming. To address this challenge, a deep reinforcement learning approach that incorporates a transformer framework is proposed. The multihead attention mechanism of the encoder effectively extracts information from reservoirs and residential areas, and the multireservoir attention network of the decoder generates suitable operational decisions. The proposed method is applied to Lake Mead and Lake Powell in the Colorado River Basin. The experimental results demonstrate that the transformer-based deep reinforcement learning approach can produce appropriate operational outcomes. Compared to a state-of-the-art method, the operation st
    
[^91]: ConFL：用于机器学习框架的约束引导模糊测试

    ConFL: Constraint-guided Fuzzing for Machine Learning Framework. (arXiv:2307.05642v1 [cs.SE])

    [http://arxiv.org/abs/2307.05642](http://arxiv.org/abs/2307.05642)

    本文介绍了一种约束引导模糊测试器ConFL，可以从机器学习框架中自动提取约束，并生成能够通过验证和探索更深路径的有效输入。

    

    随着机器学习在各个社会领域中自动决策的重要性不断增加，人们对机器学习（ML）框架的潜在漏洞产生了担忧。然而，由于这些框架的复杂实现，对它们进行测试是一项艰巨的任务。以往关于对ML框架进行模糊测试的研究一直面临着有效提取输入约束和生成有效输入的困难，导致深度执行的模糊测试持续时间过长或无法揭示目标崩溃。本文提出了一种称为ConFL的约束引导模糊器，用于ML框架。ConFL能够自动从内核代码中提取约束，而无需任何先前知识。在约束的指导下，ConFL能够生成能够通过验证并探索更深内核代码路径的有效输入。此外，我们设计了一种分组技术来提高模糊测试的效率。为了证明ConFL的有效性，我们主要在Tensorflow上评估了其性能。

    As machine learning gains prominence in various sectors of society for automated decision-making, concerns have risen regarding potential vulnerabilities in machine learning (ML) frameworks. Nevertheless, testing these frameworks is a daunting task due to their intricate implementation. Previous research on fuzzing ML frameworks has struggled to effectively extract input constraints and generate valid inputs, leading to extended fuzzing durations for deep execution or revealing the target crash.  In this paper, we propose ConFL, a constraint-guided fuzzer for ML frameworks. ConFL automatically extracting constraints from kernel codes without the need for any prior knowledge. Guided by the constraints, ConFL is able to generate valid inputs that can pass the verification and explore deeper paths of kernel codes. In addition, we design a grouping technique to boost the fuzzing efficiency.  To demonstrate the effectiveness of ConFL, we evaluated its performance mainly on Tensorflow. We fi
    
[^92]: 使用高斯径向基函数神经网络学习活跃子空间并发现重要特征

    Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])

    [http://arxiv.org/abs/2307.05639](http://arxiv.org/abs/2307.05639)

    本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。

    

    提供一个既能达到强大预测性能，又能被人类解释的模型是机器学习研究中最困难的挑战之一，由于这两个目标的冲突性。为解决这个挑战，我们提出了一种修改的径向基函数神经网络模型，通过为其高斯核添加可学习的精度矩阵。我们展示了训练完成后可以从精度矩阵的谱中提取宝贵的信息。特别是，特征向量解释了模型最敏感的方向，揭示了活跃子空间，并提出了用于监督降维的潜在应用。同时，特征向量凸显了输入和潜在变量之间的绝对变化关系，从而使我们能够基于其对预测的重要性提取输入变量的排序。

    Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi
    
[^93]: 深度迁移学习在工业时间序列异常检测中的综合调查：方法、应用和方向

    A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])

    [http://arxiv.org/abs/2307.05638](http://arxiv.org/abs/2307.05638)

    本综述调查了深度迁移学习在工业时间序列异常检测中的使用。深度迁移学习通过利用相关任务的知识和考虑数据分布的变化，解决了仅有少量或没有附加标记数据情况下的新任务。

    

    自动化监测工业过程有潜力通过及时检测异常事件并促进及时干预来提高效率和优化质量。深度学习通过识别大数据集中的非平凡模式，在这一过程中发挥着关键作用。标准的深度学习方法适用于解决特定类型的数据给定特定任务的问题。在训练过程中，这些算法需要大量的标记训练数据。然而，由于工艺和环境的动态性，为每个稍有不同的情况重新获得所需数据进行标准深度学习训练是不现实的。深度迁移学习提供了解决这个问题的方法。通过利用相关任务的知识和考虑数据分布的变化，这个学习框架可以解决新任务，即使没有或只有很少的附加标记数据。这种方法避免了从头开始重新训练模型的需要。

    Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for ev
    
[^94]: 声音日历和GMM的ASR研究

    Speech Diarization and ASR with GMM. (arXiv:2307.05637v1 [eess.AS])

    [http://arxiv.org/abs/2307.05637](http://arxiv.org/abs/2307.05637)

    该论文研究了声音日历和自动语音识别（ASR）的主题。通过使用高斯混合模型（GMM）表示语音分段，我们提出了一种新的声音日历方法。我们的研究目标是提高声音日历和ASR的准确性和效率。

    

    在这篇研究论文中，我们深入探讨了声音日历和自动语音识别（ASR）的主题。声音日历涉及将音频流中的个体发言者分离出来。通过使用ASR转录，日历过程旨在根据其独特的音频特征将每个发言者的发言分组。另一方面，自动语音识别是指机器或程序识别和转换口语词语和短语成为机器可读格式的能力。在我们的声音日历方法中，我们利用高斯混合模型（GMM）表示语音分段。基于GMM参数计算簇间距离，距离阈值作为停止准则。ASR包括将未知语音波形转换为相应的书面转录。语音信号使用同步算法进行分析，考虑到声音的音高。我们的主要目标是提高声音日历和ASR的准确性和效率。

    In this research paper, we delve into the topics of Speech Diarization and Automatic Speech Recognition (ASR). Speech diarization involves the separation of individual speakers within an audio stream. By employing the ASR transcript, the diarization process aims to segregate each speaker's utterances, grouping them based on their unique audio characteristics. On the other hand, Automatic Speech Recognition refers to the capability of a machine or program to identify and convert spoken words and phrases into a machine-readable format. In our speech diarization approach, we utilize the Gaussian Mixer Model (GMM) to represent speech segments. The inter-cluster distance is computed based on the GMM parameters, and the distance threshold serves as the stopping criterion. ASR entails the conversion of an unknown speech waveform into a corresponding written transcription. The speech signal is analyzed using synchronized algorithms, taking into account the pitch frequency. Our primary objectiv
    
[^95]: 表面层神经网络在监督学习中的超参数化基本限制

    Fundamental limits of overparametrized shallow neural networks for supervised learning. (arXiv:2307.05635v1 [cs.LG])

    [http://arxiv.org/abs/2307.05635](http://arxiv.org/abs/2307.05635)

    本文通过信息论分析，研究了超参数化情况下两层神经网络在监督学习中的基本限制。研究结果通过界限将训练数据的互信息或贝叶斯最优泛化误差与简单线性模型联系起来，并提供了神经网络训练的基本性能限制。证明方法利用严格的自旋玻璃工具和“高斯等效原理”。

    

    我们对一个两层神经网络在超参数化情况下，从与其结构类似的教师网络生成的输入-输出对进行信息论分析。我们的结果以界限的形式给出，将i)训练数据和网络权重之间的互信息，或ii)贝叶斯最优泛化误差与对应的简单（广义）线性模型之间的相同量联系起来，而这个线性模型的显式表达式已经得到严格确定。我们的界限以训练样本数量、输入维度和隐藏单元数量为参数，从而为任何神经网络（实际上是任何学习过程）在有限数据下通过我们的两层教师神经网络模型进行训练提供了基本性能限制。证明依赖于自旋玻璃的严格工具，并由处于众多最新神经网络分析核心的“高斯等效原理”指导。

    We carry out an information-theoretical analysis of a two-layer neural network trained from input-output pairs generated by a teacher network with matching architecture, in overparametrized regimes. Our results come in the form of bounds relating i) the mutual information between training data and network weights, or ii) the Bayes-optimal generalization error, to the same quantities but for a simpler (generalized) linear model for which explicit expressions are rigorously known. Our bounds, which are expressed in terms of the number of training samples, input dimension and number of hidden units, thus yield fundamental performance limits for any neural network (and actually any learning procedure) trained from limited data generated according to our two-layer teacher neural network model. The proof relies on rigorous tools from spin glasses and is guided by ``Gaussian equivalence principles'' lying at the core of numerous recent analyses of neural networks. With respect to the existing
    
[^96]: 通过自适应图神经网络进行交易欺诈检测

    Transaction Fraud Detection via an Adaptive Graph Neural Network. (arXiv:2307.05633v1 [cs.LG])

    [http://arxiv.org/abs/2307.05633](http://arxiv.org/abs/2307.05633)

    本文提出了一种自适应采样和聚合的图神经网络（ASA-GNN）来提高交易欺诈检测的性能，通过学习判别表示，并利用邻居采样策略来过滤噪声节点和补充信息。具体使用余弦相似度和边权重来自适应选择相似行为模式的邻居节点，并找到多跳邻居来检测欺诈交易。

    

    许多机器学习方法已经被提出来实现准确的交易欺诈检测，这对个人和银行的金融安全至关重要。然而，大多数现有方法仅利用原始特征或需要手动进行特征工程。它们缺乏从交易数据中学习判别表示的能力。而且，犯罪分子通常通过模仿持卡人的行为来犯罪，这导致现有检测模型性能较差。在本文中，我们提出了一种基于自适应采样和聚合的图神经网络（ASA-GNN），其学习判别表示以提高交易欺诈检测的性能。我们使用邻居采样策略来过滤噪声节点，并为欺诈节点补充信息。具体地，我们利用余弦相似度和边权重来自适应选择具有相似行为模式的邻居节点，并找到多跳邻居来检测欺诈交易。

    Many machine learning methods have been proposed to achieve accurate transaction fraud detection, which is essential to the financial security of individuals and banks. However, most existing methods leverage original features only or require manual feature engineering. They lack the ability to learn discriminative representations from transaction data. Moreover, criminals often commit fraud by imitating cardholders' behaviors, which causes the poor performance of existing detection models. In this paper, we propose an Adaptive Sampling and Aggregation-based Graph Neural Network (ASA-GNN) that learns discriminative representations to improve the performance of transaction fraud detection. A neighbor sampling strategy is performed to filter noisy nodes and supplement information for fraudulent nodes. Specifically, we leverage cosine similarity and edge weights to adaptively select neighbors with similar behavior patterns for target nodes and then find multi-hop neighbors for fraudulent 
    
[^97]: DNAGPT：用于多个DNA序列分析任务的通用预训练工具

    DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks. (arXiv:2307.05628v1 [q-bio.GN])

    [http://arxiv.org/abs/2307.05628](http://arxiv.org/abs/2307.05628)

    DNAGPT是一个通用的基础模型，通过预训练模型和独特的标记设计，可以适用于任何DNA序列分析任务。它在多个任务上进行了评估，并展示出了良好的性能。

    

    GPT系列的成功证明了GPT可以从序列中提取一般性信息，从而使得所有下游任务受益。这激发了我们使用预训练模型来探索DNA序列中的隐藏信息。然而，DNA序列分析中的数据和任务需求非常复杂和多样化，因为DNA相关数据包含不同类型的信息，如序列、表达水平等，目前还没有专门针对这些特点设计的模型。因此，我们提出了DNAGPT，这是一个通用的基础模型，它在9个物种的超过100亿个碱基对上进行了预训练，并可以针对任何DNA序列分析任务进行微调。我们的模型可以同时处理或输出DNA序列和数值。此外，我们独特的标记设计允许用户根据自己的任务需求来设计提示，使其适用于任何类型的任务。我们在分类、回归和生成任务上对模型进行了评估。

    The success of the GPT series proves that GPT can extract general information from sequences, thereby benefiting all downstream tasks. This motivates us to use pre-trained models to explore the hidden information in DNA sequences. However, data and task requirements in DNA sequence analysis are complexity and diversity as DNA relevant data includes different types of information, such as sequences, expression levels, etc, while there is currently no model specifically designed for these characteristics. Hereby, we present DNAGPT, a generalized foundation model pre-trained on over 10 billion base pairs from 9 species which can be fine-tuned for any DNA sequence analysis task. Our model can simultaneously process or output DNA sequences and numbers. In addition, our unique token design allows users to design prompts according to their own task requirements, making it applicable to any type of task. We have evaluated our model on classification, regression, and generation tasks. We demons
    
[^98]: CILF:因果启发学习框架用于超出分布的车辆轨迹预测

    CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction. (arXiv:2307.05624v1 [cs.LG])

    [http://arxiv.org/abs/2307.05624](http://arxiv.org/abs/2307.05624)

    提出了一种因果启发学习框架（CILF），通过明确定义数据的潜在因果结构，并利用因果特征进行预测来解决轨迹预测中超出分布数据的问题。

    

    轨迹预测对于自动驾驶车辆至关重要。现有的大多数方法倾向于建模历史轨迹（输入）与未来轨迹（输出）之间的相关性。由于相关性只是对现实的一种表面描述，这些方法在很大程度上依赖于独立同分布的假设，并对超出分布的数据表现出高度的敏感性。为了解决这个问题，我们提出了一个超出分布因果图（OOD-CG），它明确地定义了数据的潜在因果结构，包括三个纠缠的潜在特征：1）领域不变的因果特征（IC），2）领域变量的因果特征（VC），3）领域变量的非因果特征（VN）。这些特征受到混淆因子（C）和领域选择器（D）的影响。为了利用因果特征进行预测，我们提出了一个因果启发学习框架（CILF），它包括三个步骤：1）通过不变性损失提取领域不变的因果特征，2）通过因果性损失提取领域变量特征

    Trajectory prediction is critical for autonomous driving vehicles. Most existing methods tend to model the correlation between history trajectory (input) and future trajectory (output). Since correlation is just a superficial description of reality, these methods rely heavily on the i.i.d. assumption and evince a heightened susceptibility to out-of-distribution data. To address this problem, we propose an Out-of- Distribution Causal Graph (OOD-CG), which explicitly defines the underlying causal structure of the data with three entangled latent features: 1) domain-invariant causal feature (IC), 2) domain-variant causal feature (VC), and 3) domain-variant non-causal feature (VN ). While these features are confounded by confounder (C) and domain selector (D). To leverage causal features for prediction, we propose a Causal Inspired Learning Framework (CILF), which includes three steps: 1) extracting domain-invariant causal feature by means of an invariance loss, 2) extracting domain varian
    
[^99]: 用于动态估计出发地-目的地序列的深度学习框架

    A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence. (arXiv:2307.05623v1 [cs.LG])

    [http://arxiv.org/abs/2307.05623](http://arxiv.org/abs/2307.05623)

    本文提出了一个综合方法，使用深度学习方法推断OD序列的结构，并使用结构约束指导传统的数值优化，解决了交通领域中静态和动态OD矩阵估计中的欠定和滞后挑战。

    

    OD矩阵估计是交通领域的一个关键问题。主要方法使用交通传感器测量信息（如交通流量）来估计由OD矩阵表示的交通需求。该问题分为静态OD矩阵估计和动态OD矩阵序列（简称OD序列）估计两类。上述两种方法面临由于大量估计参数和不足的约束信息造成的欠定问题。此外，OD序列估计还面临滞后挑战：由于拥堵等不同交通条件，相同的车辆在同一观测时段内会出现在不同的路段上，导致相同的OD需求对应不同的行程。为此，本文提出了一种综合方法，它使用深度学习方法推断OD序列的结构，并使用结构约束指导传统的数值优化。我们的实验显示...

    OD matrix estimation is a critical problem in the transportation domain. The principle method uses the traffic sensor measured information such as traffic counts to estimate the traffic demand represented by the OD matrix. The problem is divided into two categories: static OD matrix estimation and dynamic OD matrices sequence(OD sequence for short) estimation. The above two face the underdetermination problem caused by abundant estimated parameters and insufficient constraint information. In addition, OD sequence estimation also faces the lag challenge: due to different traffic conditions such as congestion, identical vehicle will appear on different road sections during the same observation period, resulting in identical OD demands correspond to different trips. To this end, this paper proposes an integrated method, which uses deep learning methods to infer the structure of OD sequence and uses structural constraints to guide traditional numerical optimization. Our experiments show th
    
[^100]: 潜在空间洞察力和解释增强（LS-PIE）框架

    Latent Space Perspicacity and Interpretation Enhancement (LS-PIE) Framework. (arXiv:2307.05620v1 [stat.ML])

    [http://arxiv.org/abs/2307.05620](http://arxiv.org/abs/2307.05620)

    本文提出了一个名为LS-PIE的框架，用于提高线性潜在空间的解释能力。该框架通过自动化潜在向量的聚类和排序，从而改善了主成分分析、独立成分分析等线性潜变量模型的可解释性。

    

    线性潜变量模型如主成分分析（PCA）、独立成分分析（ICA）、典型相关分析（CCA）和因子分析（FA）通常会识别有序或无序的潜在方向（或载荷）。然后将数据投影到潜在方向上以获得它们的投影表示（或得分）。然而，ICA求解器通常无序地返回独立方向，并且往往将单一源分布在多个方向上作为多个子源，这对于其可用性和可解释性很不利。本文提出了一个通用框架，用于增强线性潜在空间的解释能力。尽管本文介绍的概念与语言无关，但该框架是用Python编写的。该框架自动化了潜在向量的聚类和排序，以增强解释能力。

    Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example, PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability.  This paper proposes a general framework to enhance latent space representations for improving the interpretability of linear latent spaces. Although the concepts in this paper are language agnostic, the framework is written in Python. This framework automates the clustering and ranking of latent vectors to enhance
    
[^101]: 特征编码对恶意软件分类解释性的影响

    Impact of Feature Encoding on Malware Classification Explainability. (arXiv:2307.05614v1 [cs.LG])

    [http://arxiv.org/abs/2307.05614](http://arxiv.org/abs/2307.05614)

    本文研究了特征编码技术对可解释的人工智能算法的影响。使用恶意软件分类数据集，比较了标签编码和独热编码的性能。发现虽然独热编码会带来轻微性能损失，但可以提供更详细的解释，帮助更全面地理解。使用独热编码还可以减小解释文件大小，缩短人工分析时间。这些结果强调了在XAI研究中重视特征编码技术的重要性，并提出了进一步探索的可能性。

    

    本文研究特征编码技术对可解释的人工智能算法（XAI）的解释性的影响。使用一个恶意软件分类数据集，我们训练了一个XGBoost模型，并比较了两种特征编码方法：标签编码（LE）和独热编码（OHE）的性能。我们的发现表明，使用OHE相比LE会带来轻微的性能损失。然而，OHE所提供的更详细的解释弥补了这种损失。我们观察到OHE能够更详细地探索全局和局部上的细节，提供更全面的答案。此外，我们还观察到使用OHE会导致较小的解释文件和减少人工分析的时间。这些发现强调了在XAI研究中考虑特征编码技术的重要性，并建议通过结合其他编码方法和创新的可视化方法进行进一步探索。

    This paper investigates the impact of feature encoding techniques on the explainability of XAI (Explainable Artificial Intelligence) algorithms. Using a malware classification dataset, we trained an XGBoost model and compared the performance of two feature encoding methods: Label Encoding (LE) and One Hot Encoding (OHE). Our findings reveal a marginal performance loss when using OHE instead of LE. However, the more detailed explanations provided by OHE compensated for this loss. We observed that OHE enables deeper exploration of details in both global and local contexts, facilitating more comprehensive answers. Additionally, we observed that using OHE resulted in smaller explanation files and reduced analysis time for human analysts. These findings emphasize the significance of considering feature encoding techniques in XAI research and suggest potential for further exploration by incorporating additional encoding methods and innovative visualization approaches.
    
[^102]: 物质还是风格：你的图像嵌入知道什么？

    Substance or Style: What Does Your Image Embedding Know?. (arXiv:2307.05610v1 [cs.LG])

    [http://arxiv.org/abs/2307.05610](http://arxiv.org/abs/2307.05610)

    本文研究了图像嵌入中的非语义信息，设计了一个系统的转换预测任务，并发现六个嵌入可以识别出多个转换。这对于训练算法和基础模型的应用具有重要意义。

    

    探针是一种从嵌入中预测底层数据属性的小型网络，它们提供了一种有针对性和有效的方法来揭示嵌入中包含的信息。虽然通过使用探针进行分析在自然语言处理中已经很常见了，但在计算机视觉领域中的探索却比较少。图像基础模型主要用于评估语义内容。更好地理解流行嵌入（如MAE，SimCLR或CLIP）中的非语义信息，将为训练算法和这些基础模型的用途提供新的视角。我们设计了一个系统的转换预测任务，并在多个维度上测量嵌入的视觉内容，包括图像风格、质量以及各种自然和人工转换。令人惊讶的是，有六个嵌入（包括SimCLR）编码了足够的非语义信息，可以识别出数十个转换。我们还考虑了一个泛化任务，将相似的转换分组，并留出一部分数据进行评估。

    Probes are small networks that predict properties of underlying data from embeddings, and they provide a targeted, effective way to illuminate the information contained in embeddings. While analysis through the use of probes has become standard in NLP, there has been much less exploration in vision. Image foundation models have primarily been evaluated for semantic content. Better understanding the non-semantic information in popular embeddings (e.g., MAE, SimCLR, or CLIP) will shed new light both on the training algorithms and on the uses for these foundation models. We design a systematic transformation prediction task and measure the visual content of embeddings along many axes, including image style, quality, and a range of natural and artificial transformations. Surprisingly, six embeddings (including SimCLR) encode enough non-semantic information to identify dozens of transformations. We also consider a generalization task, where we group similar transformations and hold out seve
    
[^103]: 通过局部搜索方法优化程序：你能改进我的代码吗？

    Can You Improve My Code? Optimizing Programs with Local Search. (arXiv:2307.05603v1 [cs.SE])

    [http://arxiv.org/abs/2307.05603](http://arxiv.org/abs/2307.05603)

    这项研究提出了一种利用局部搜索方法改进现有程序的方法，通过固定其余行而改进程序的单个行，以显着改进程序性能，可应用于具有可衡量目标的编程问题。

    

    本文介绍了一种利用局部搜索方法改进现有程序在可衡量目标方面的方法。POLIS（Program Optimization with Locally Improving Search）利用程序的结构，通过固定其余行而改进程序的单个行，使用现有的暴力合成算法，并在无法改进程序性能时继续迭代。POLIS在一个由27名参与者组成的用户研究中进行了评估，参与者编写程序以尝试最大化两个单一代理游戏（Lunar Lander和Highway）的得分。POLIS能够在游戏得分方面显着改进参与者的程序。对现有Stack Overflow代码的概念验证演示测量了其在实际问题中的适用性。这些结果表明，POLIS可用作可衡量目标的编程问题的有益编程助手。

    This paper introduces a local search method for improving an existing program with respect to a measurable objective. Program Optimization with Locally Improving Search (POLIS) exploits the structure of a program, defined by its lines. POLIS improves a single line of the program while keeping the remaining lines fixed, using existing brute-force synthesis algorithms, and continues iterating until it is unable to improve the program's performance. POLIS was evaluated with a 27-person user study, where participants wrote programs attempting to maximize the score of two single-agent games: Lunar Lander and Highway. POLIS was able to substantially improve the participants' programs with respect to the game scores. A proof-of-concept demonstration on existing Stack Overflow code measures applicability in real-world problems. These results suggest that POLIS could be used as a helpful programming assistant for programming problems with measurable objectives.
    
[^104]: 无监督领域自适应与深度神经网络

    Unsupervised Domain Adaptation with Deep Neural-Network. (arXiv:2307.05601v1 [cs.CV])

    [http://arxiv.org/abs/2307.05601](http://arxiv.org/abs/2307.05601)

    本篇论文分析了现有的无监督领域自适应方法，提出了一种新方法，并展示了改进不同领域视觉识别任务的潜力。

    

    本篇报告通过对现有方法进行分析，引入了一种新方法，并展示了通过不同领域间的视觉识别任务的改进潜力，为无监督领域自适应的研究做出了贡献。该研究的结果为领域自适应领域的进一步研究和方法的演进提供了机会。

    This report contributes to the field of unsupervised domain adaptation by providing an analysis of existing methods, introducing a new approach, and demonstrating the potential for improving visual recognition tasks across different domains. The results of this study open up opportunities for further study and development of advanced methods in the field of domain adaptation.
    
[^105]: 从第一原理中实现组合性泛化

    Compositional Generalization from First Principles. (arXiv:2307.05596v1 [cs.LG])

    [http://arxiv.org/abs/2307.05596](http://arxiv.org/abs/2307.05596)

    本论文将组合性泛化视为数据生成过程的属性，通过导出对训练分布支持和模型架构的条件要求，实现了组合性泛化。对于机器学习中的组合性泛化问题提供了理论性的研究基础。

    

    利用我们世界的组合性质加快学习和促进泛化是人类感知的一个特点。然而，在机器学习中，即使对于具有明确组合性先验的模型，实现组合性泛化也是一个难以实现的目标。为了更好地理解组合性泛化，我们从底层开始进行探索：受可识别表示学习的启发，我们研究组合性作为数据生成过程的属性，而不是数据本身。这种改进使我们能够导出仅对训练分布的支持和模型架构有轻微条件的要求，这些条件足以实现组合性泛化。我们进一步展示了我们的理论框架如何应用于现实场景，并通过实验证实了我们的发现。我们的结果为组合性泛化的原则性理论研究奠定了基础。

    Leveraging the compositional nature of our world to expedite learning and facilitate generalization is a hallmark of human perception. In machine learning, on the other hand, achieving compositional generalization has proven to be an elusive goal, even for models with explicit compositional priors. To get a better handle on compositional generalization, we here approach it from the bottom up: Inspired by identifiable representation learning, we investigate compositionality as a property of the data-generating process rather than the data itself. This reformulation enables us to derive mild conditions on only the support of the training distribution and the model architecture, which are sufficient for compositional generalization. We further demonstrate how our theoretical framework applies to real-world scenarios and validate our findings empirically. Our results set the stage for a principled theoretical study of compositional generalization.
    
[^106]: 基于功能主成分分析和深度神经网络的基于贝叶斯的逆不确定性量化与瞬态实验数据

    Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification with Transient Experimental Data. (arXiv:2307.05592v1 [stat.ML])

    [http://arxiv.org/abs/2307.05592](http://arxiv.org/abs/2307.05592)

    本研究提出了一种基于功能主成分分析和深度神经网络的逆UQ过程，用于时间相关响应的模型输入不确定性量化，并通过功能对齐方法解决了PCT时间序列数据中的温度下降问题。

    

    逆UQ是基于实验数据对模型输入不确定性进行逆向量化的过程。本研究重点发展了一种针对时间相关响应的逆UQ过程，利用功能主成分分析（PCA）和基于深度神经网络（DNN）的代理模型进行降维。该演示基于使用FEBA瞬态实验数据来逆向量化TRACE物理模型参数，测量数据是时间相关的最高包覆温度（PCT）。由于感兴趣的数量（QoI）是时间相关的，对应于无限维响应，因此使用PCA对QoI维度进行降低，同时保留PCT的瞬态特征，以使逆UQ过程更加高效。然而，直接应用传统PCA到PCT时间序列数据中无法准确表示数据，因为在淬灭时刻会出现突然的温度下降。因此，采用了一种功能对齐方法。

    Inverse UQ is the process to inversely quantify the model input uncertainties based on experimental data. This work focuses on developing an inverse UQ process for time-dependent responses, using dimensionality reduction by functional principal component analysis (PCA) and deep neural network (DNN)-based surrogate models. The demonstration is based on the inverse UQ of TRACE physical model parameters using the FEBA transient experimental data. The measurement data is time-dependent peak cladding temperature (PCT). Since the quantity-of-interest (QoI) is time-dependent that corresponds to infinite-dimensional responses, PCA is used to reduce the QoI dimension while preserving the transient profile of the PCT, in order to make the inverse UQ process more efficient. However, conventional PCA applied directly to the PCT time series profiles can hardly represent the data precisely due to the sudden temperature drop at the time of quenching. As a result, a functional alignment method is used
    
[^107]: SITTA: 一种用于图像描述的语义图像文本对齐方法

    SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])

    [http://arxiv.org/abs/2307.05591](http://arxiv.org/abs/2307.05591)

    SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。

    

    对图像的文本和语义理解对于生成适当的描述非常重要。这需要检测图像中的对象，建模它们之间的关系，评估场景的语义，并将提取的知识表示在语言空间中。为了在保证良好的图像-语言映射的同时实现丰富的语言能力，预训练的语言模型（LMs）被条件化为预训练的多模态（图像-文本）模型，允许使用图像输入。这要求将多模态模型的视觉编码器中检测到的语义与生成性LM的语言表示进行对齐。然而，如何最好地将视觉编码器检测到的语义传递给LM还不清楚。我们介绍了两种构建线性映射的新方法，成功地将两个预训练模型的嵌入空间之间的语义转移。第一种方法是将多模态语言编码器的嵌入空间与生成性LM的嵌入空间进行对齐。

    Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
    
[^108]: 使用帧级别查询的主动学习视频分类

    Active Learning for Video Classification with Frame Level Queries. (arXiv:2307.05587v1 [cs.CV])

    [http://arxiv.org/abs/2307.05587](http://arxiv.org/abs/2307.05587)

    本论文提出了一个新颖的主动学习框架，用于视频分类中的标注减负。这个框架通过自动识别最具信息量的样本来减少人工标注工作量和训练时间。

    

    深度学习算法推动了计算机视觉研究的边界，并在各种应用中展现出了令人瞩目的性能。然而，训练一个强大的深度神经网络需要大量标记的训练数据，获取这些数据需要大量时间和人力。这个问题在视频分类等应用中更为严重，因为人工标注者必须完整地观看整个视频以提供标签。主动学习算法可以自动识别大量未标记数据中最具信息量的样本，这极大地减少了人工注释的工作量，只需要手动标记算法识别出的少数样本。本文提出了一个新颖的主动学习框架用于视频分类，旨在进一步减轻人工标注者的工作负担。我们的框架识别一批图像样本视频，

    Deep learning algorithms have pushed the boundaries of computer vision research and have depicted commendable performance in a variety of applications. However, training a robust deep neural network necessitates a large amount of labeled training data, acquiring which involves significant time and human effort. This problem is even more serious for an application like video classification, where a human annotator has to watch an entire video end-to-end to furnish a label. Active learning algorithms automatically identify the most informative samples from large amounts of unlabeled data; this tremendously reduces the human annotation effort in inducing a machine learning model, as only the few samples that are identified by the algorithm, need to be labeled manually. In this paper, we propose a novel active learning framework for video classification, with the goal of further reducing the labeling onus on the human annotators. Our framework identifies a batch of exemplar videos, togethe
    
[^109]: DBFed: 基于领域无关性的去偏差联邦学习框架

    DBFed: Debiasing Federated Learning Framework based on Domain-Independent. (arXiv:2307.05582v1 [cs.LG])

    [http://arxiv.org/abs/2307.05582](http://arxiv.org/abs/2307.05582)

    DBFed是一个基于领域无关性的去偏差联邦学习框架，解决了在联邦学习中由数据质量差异引起的公平性问题。

    

    随着数字化转型的持续进行，企业正在产生、管理和存储大量的数据，而人工智能技术也在迅速发展。然而，这也给信息安全和数据安全带来了挑战。数据安全是指在其整个生命周期内，保护数字信息免受未经授权的访问、损坏、盗窃等的损害。随着数据安全法的颁布和执行以及组织和用户对数据安全和数据隐私的重视，以联邦学习为代表的隐私保护技术在各种应用场景中有广泛的应用。联邦学习是一种分布式机器学习计算框架，允许多个主体在不共享数据的情况下进行共同模型训练，以保护数据隐私并解决数据孤岛问题。然而，多个主体之间的数据彼此独立，而质量上的差异可能导致联邦学习中的公平性问题。

    As digital transformation continues, enterprises are generating, managing, and storing vast amounts of data, while artificial intelligence technology is rapidly advancing. However, it brings challenges in information security and data security. Data security refers to the protection of digital information from unauthorized access, damage, theft, etc. throughout its entire life cycle. With the promulgation and implementation of data security laws and the emphasis on data security and data privacy by organizations and users, Privacy-preserving technology represented by federated learning has a wide range of application scenarios. Federated learning is a distributed machine learning computing framework that allows multiple subjects to train joint models without sharing data to protect data privacy and solve the problem of data islands. However, the data among multiple subjects are independent of each other, and the data differences in quality may cause fairness issues in federated learnin
    
[^110]: RidgeBase：一种跨传感器多指非接触指纹数据集

    RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset. (arXiv:2307.05563v1 [cs.CV])

    [http://arxiv.org/abs/2307.05563](http://arxiv.org/abs/2307.05563)

    RidgeBase是一个跨传感器多指非接触指纹数据集，旨在促进不同匹配场景下非接触指纹匹配的研究。

    

    使用智能手机相机进行非接触指纹匹配可以解决传统指纹系统的一些主要挑战，包括卫生采集、便携性和攻击防范。然而，实际和鲁棒的非接触指纹匹配技术的发展受到了大规模真实世界数据集的限制。为了激发在不同传感器之间进一步发展非接触指纹匹配的进步，我们介绍了RidgeBase基准数据集。RidgeBase包含了来自88个个体，在不同背景和照明条件下使用两个智能手机相机和一个平板触摸传感器获得的超过15,000个非接触和接触式指纹图像对。与现有数据集不同，RidgeBase旨在促进在不同匹配场景下的研究，包括单指匹配和多指匹配，既包括非接触到非接触（CL2CL）的验证和识别，也包括接触到非接触（C2CL）的验证和识别。

    Contactless fingerprint matching using smartphone cameras can alleviate major challenges of traditional fingerprint systems including hygienic acquisition, portability and presentation attacks. However, development of practical and robust contactless fingerprint matching techniques is constrained by the limited availability of large scale real-world datasets. To motivate further advances in contactless fingerprint matching across sensors, we introduce the RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless and contact-based fingerprint image pairs acquired from 88 individuals under different background and lighting conditions using two smartphone cameras and one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to promote research under different matching scenarios that include Single Finger Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL) and contact-to-contactless (C2CL) verification and identification. 
    
[^111]: 自动化论文评分中的反馈综述

    Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])

    [http://arxiv.org/abs/2307.05553](http://arxiv.org/abs/2307.05553)

    这篇论文综述了自动化论文评分中的反馈研究，包括不同类型的反馈和论文特征，并回顾了提供反馈的最新案例研究。

    

    第一个自动化论文评分系统诞生于50年前。自动化论文评分系统正在发展成为比以前简单评分系统更加功能丰富的系统。它的目的不仅仅是评分，还作为一个学习工具来提高用户的写作能力。反馈是使自动化论文评分系统在实际生活中有用的最重要方面。在第一个自动化论文评分系统中已经强调了反馈的重要性。本文综述了关于自动化论文评分的反馈研究，包括不同类型的反馈和论文特征。我们还回顾了提供反馈的最新案例研究。

    The first automated essay scoring system was developed 50 years ago. Automated essay scoring systems are developing into systems with richer functions than the previous simple scoring systems. Its purpose is not only to score essays but also as a learning tool to improve the writing skill of users. Feedback is the most important aspect of making an automated essay scoring system useful in real life. The importance of feedback was already emphasized in the first AES system. This paper reviews research on feedback including different feedback types and essay traits on automated essay scoring. We also reviewed the latest case studies of the automated essay scoring system that provides feedback.
    
[^112]: 基于图神经网络的太赫兹流导向纳米尺度定位

    Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])

    [http://arxiv.org/abs/2307.05551](http://arxiv.org/abs/2307.05551)

    本研究提出了一种基于图神经网络的太赫兹流导向纳米尺度定位方法，可以提高定位精度和覆盖范围，解决了现有方法的定位精度低和无法全局定位的问题。

    

    纳米技术和先进材料的科学进展为体内精准医学的纳米尺度装置铺平了道路，其中包括集成感应、计算、通信、数据和能量存储能力。在人体心血管系统中，这些装置被设想为被动流动并持续感知以便检测诊断感兴趣的事件。通过将这些事件的物理位置（如身体区域）分配给它们，可以提高检测到这些事件的诊断价值，这是流导向定位的主要命题。当前的流导向定位方法存在定位精度低和无法在整个心血管系统内本地化事件的问题。为了解决这个问题，我们提出利用图神经网络（GNNs）来进行定位，并证明我们的方法在定位精度和覆盖范围上优于现有的最先进方法。

    Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based
    
[^113]: NLP遇上RNA：基于Word2Vec的自监督嵌入学习用于核酶

    NLP Meets RNA: Unsupervised Embedding Learning for Ribozymes with Word2Vec. (arXiv:2307.05537v1 [cs.LG])

    [http://arxiv.org/abs/2307.05537](http://arxiv.org/abs/2307.05537)

    本文将自然语言处理中的无监督学习技术Word2Vec应用于核酶的嵌入学习，实现了对核酶类别的准确分类，并提供了有关核酶的有意义的信息。

    

    核酶是具有独特的三维结构和催化活性的RNA分子，在合成生物学和治疗学中有广泛应用。然而，相对较少的研究关注利用深度学习来增强我们对核酶的理解。本研究实施了Word2Vec，一种用于自然语言处理的无监督学习技术，用于学习核酶的嵌入。Ribo2Vec在超过9000个多样的核酶上进行训练，学习将序列映射到128和256维的向量空间。利用Ribo2Vec，计算了五类核酶（hatchet，pistol，hairpin，hovlinc和twister sister）的序列嵌入。主成分分析表明这些嵌入可以区分核酶的类别。此外，用核酶嵌入训练的简单SVM分类器在准确分类核酶类型方面显示出有希望的结果。我们的结果表明嵌入向量包含有关核酶的有意义的信息。

    Ribozymes, RNA molecules with distinct 3D structures and catalytic activity, have widespread applications in synthetic biology and therapeutics. However, relatively little research has focused on leveraging deep learning to enhance our understanding of ribozymes. This study implements Word2Vec, an unsupervised learning technique for natural language processing, to learn ribozyme embeddings. Ribo2Vec was trained on over 9,000 diverse ribozymes, learning to map sequences to 128 and 256-dimensional vector spaces. Using Ribo2Vec, sequence embeddings for five classes of ribozymes (hatchet, pistol, hairpin, hovlinc, and twister sister) were calculated. Principal component analysis demonstrated the ability of these embeddings to distinguish between ribozyme classes. Furthermore, a simple SVM classifier trained on ribozyme embeddings showed promising results in accurately classifying ribozyme types. Our results suggest that the embedding vectors contained meaningful information about ribozymes
    
[^114]: 用户身份识别的按键动力学

    Keystroke Dynamics for User Identification. (arXiv:2307.05529v1 [cs.LG])

    [http://arxiv.org/abs/2307.05529](http://arxiv.org/abs/2307.05529)

    该论文研究了按键动力学在自由文本数据上的多类用户身份识别问题。通过使用复杂的图像特征和多类卷积神经网络，可以获得较高的识别准确率。然而，通过稍微修改特征的随机森林分类器可以获得更高的准确率。

    

    在先前的研究中，按键动力学已经显示出在基于固定文本和自由文本数据的用户认证方面具有潜力。在这项研究中，我们考虑了更具挑战性的基于自由文本数据的多类用户身份识别问题。我们尝试了一种复杂的类似图像的特征，该特征在先前的研究中已经被用于实现自由文本数据的最先进的认证结果。通过使用这个类似图像的特征和多类卷积神经网络，我们能够在148个用户的数据集上获得0.78的分类（即识别）准确率。然而，我们发现，对于经过稍微修改的相同特征的随机森林分类器，其准确率可以达到0.93。

    In previous research, keystroke dynamics has shown promise for user authentication, based on both fixed-text and free-text data. In this research, we consider the more challenging multiclass user identification problem, based on free-text data. We experiment with a complex image-like feature that has previously been used to achieve state-of-the-art authentication results over free-text data. Using this image-like feature and multiclass Convolutional Neural Networks, we are able to obtain a classification (i.e., identification) accuracy of 0.78 over a set of 148 users. However, we find that a Random Forest classifier trained on a slightly modified version of this same feature yields an accuracy of 0.93.
    
[^115]: 生成音频模型的伦理影响：一项系统文献综述

    The Ethical Implications of Generative Audio Models: A Systematic Literature Review. (arXiv:2307.05527v1 [cs.CY])

    [http://arxiv.org/abs/2307.05527](http://arxiv.org/abs/2307.05527)

    这项研究对生成音频模型的伦理影响进行了系统文献综述，发现当前有较少论文讨论负面影响，但其中引发了重大的伦理问题，如欺诈、假脸和侵权潜能。

    

    生成音频模型通常将其应用集中在音乐和语音生成上，近期的模型在音频输出方面具有类似人类的质量。本文对884篇生成音频模型领域的论文进行了系统性文献综述，旨在量化领域内研究者考虑潜在负面影响的程度，并确定研究者在这个领域需要考虑的伦理影响类型。虽然65%的生成音频研究论文提到了他们工作的积极潜力，但不到10%的论文讨论了任何负面影响。这个惊人的小比例使人担忧，因为少数提到负面影响的论文揭示了在更大范围领域内相关的严重伦理问题和关切，如欺诈、假脸和侵权潜能。通过量化生成音频研究中缺乏伦理考虑的现状，这篇论文呼吁研究者在格外关注这些伦理问题上做出努力。

    Generative audio models typically focus their applications in music and speech generation, with recent models having human-like quality in their audio output. This paper conducts a systematic literature review of 884 papers in the area of generative audio models in order to both quantify the degree to which researchers in the field are considering potential negative impacts and identify the types of ethical implications researchers in this area need to consider. Though 65% of generative audio research papers note positive potential impacts of their work, less than 10% discuss any negative impacts. This jarringly small percentage of papers considering negative impact is particularly worrying because the issues brought to light by the few papers doing so are raising serious ethical implications and concerns relevant to the broader field such as the potential for fraud, deep-fakes, and copyright infringement. By quantifying this lack of ethical consideration in generative audio research a
    
[^116]: 朝着使用基于机器学习的优化电极制造的高性能能量和功率电池的方向发展

    Toward High-Performance Energy and Power Battery Cells with Machine Learning-based Optimization of Electrode Manufacturing. (arXiv:2307.05521v1 [cs.LG])

    [http://arxiv.org/abs/2307.05521](http://arxiv.org/abs/2307.05521)

    本研究通过基于机器学习的优化电极制造，提出了一种解决高性能电池电极问题的数据驱动方法，可以在能量和功率应用中反向设计电化学性能的制造过程参数。

    

    电极制造过程的优化对于提升锂离子电池（LIBs）的应用规模以满足不断增长的能源需求非常重要。特别是，LIB制造的优化对于确定电池在诸如电动车等应用中的实际性能非常关键。本研究通过提出一种强大的基于数据驱动的方法，支持一种确定性机器学习（ML）辅助的管道来优化电化学性能的双目标优化，以解决期望电池应用条件下的高性能电极问题。这种ML管道允许反向设计过程参数以制造适用于能量或功率应用的电极。该研究类似于我们之前的研究，支持优化电极微结构以改善动力学、离子和电子传输性能。

    The optimization of the electrode manufacturing process is important for upscaling the application of Lithium Ion Batteries (LIBs) to cater for growing energy demand. In particular, LIB manufacturing is very important to be optimized because it determines the practical performance of the cells when the latter are being used in applications such as electric vehicles. In this study, we tackled the issue of high-performance electrodes for desired battery application conditions by proposing a powerful data-driven approach supported by a deterministic machine learning (ML)-assisted pipeline for bi-objective optimization of the electrochemical performance. This ML pipeline allows the inverse design of the process parameters to adopt in order to manufacture electrodes for energy or power applications. The latter work is an analogy to our previous work that supported the optimization of the electrode microstructures for kinetic, ionic, and electronic transport properties improvement. An electr
    
[^117]: DL模型和训练环境对能源消耗有影响吗？

    Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v1 [cs.LG])

    [http://arxiv.org/abs/2307.05520](http://arxiv.org/abs/2307.05520)

    本研究分析了模型架构和训练环境对训练更环保的计算机视觉模型的影响，并找出了能源效率和模型正确性之间的权衡关系。

    

    当前计算机视觉领域的研究主要集中在提高深度学习（DL）的正确性和推理时间性能上。然而，目前很少有关于训练DL模型带来巨大碳足迹的研究。本研究旨在分析模型架构和训练环境对训练更环保的计算机视觉模型的影响。我们将这个目标分为两个研究问题。首先，我们分析模型架构对实现更环保模型同时保持正确性在最佳水平的影响。其次，我们研究训练环境对生成更环保模型的影响。为了调查这些关系，我们在模型训练过程中收集了与能源效率和模型正确性相关的多个指标。然后，我们描述了模型架构在测量能源效率和模型正确性方面的权衡，以及它们与训练环境的关系。我们在一个实验平台上进行了这项研究。

    Current research in the computer vision field mainly focuses on improving Deep Learning (DL) correctness and inference time performance. However, there is still little work on the huge carbon footprint that has training DL models. This study aims to analyze the impact of the model architecture and training environment when training greener computer vision models. We divide this goal into two research questions. First, we analyze the effects of model architecture on achieving greener models while keeping correctness at optimal levels. Second, we study the influence of the training environment on producing greener models. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy efficiency and the models' correctness regarding model architecture, and their relationship with the training environment. We conduct this research in the context of a 
    
[^118]: 自适应图卷积网络用于交通流量预测

    Adaptive Graph Convolution Networks for Traffic Flow Forecasting. (arXiv:2307.05517v1 [cs.LG])

    [http://arxiv.org/abs/2307.05517](http://arxiv.org/abs/2307.05517)

    本论文提出了一种自适应图卷积网络（AGC-net）用于解决交通流量预测中GNN的时间变化问题，通过自适应图卷积（AGC）和上下文注意机制来考虑时间上下文，以提高预测性能。

    

    交通流量预测是一项非常具有挑战性的任务，由于动态的时空道路条件。图神经网络（GNN）已广泛应用于该任务。然而，大多数这些GNN忽略了时间变化的道路条件的影响，因为卷积感受野的范围是固定的。在本文中，我们提出了一种新颖的自适应图卷积网络（AGC-net）来解决GNN中的这个问题。AGC-net是通过基于新颖的上下文注意机制的自适应图卷积（AGC）构建的，该机制由一组具有各种可学习尺度的图小波组成。AGC将空间图表示转化为考虑时间上下文的时间敏感特征。此外，设计了一种偏移图卷积核以增强AGC，它试图纠正由不准确的拓扑引起的偏差。两个公共交通数据集上的实验结果证明了AGC-net的有效性。

    Traffic flow forecasting is a highly challenging task due to the dynamic spatial-temporal road conditions. Graph neural networks (GNN) has been widely applied in this task. However, most of these GNNs ignore the effects of time-varying road conditions due to the fixed range of the convolution receptive field. In this paper, we propose a novel Adaptive Graph Convolution Networks (AGC-net) to address this issue in GNN. The AGC-net is constructed by the Adaptive Graph Convolution (AGC) based on a novel context attention mechanism, which consists of a set of graph wavelets with various learnable scales. The AGC transforms the spatial graph representations into time-sensitive features considering the temporal context. Moreover, a shifted graph convolution kernel is designed to enhance the AGC, which attempts to correct the deviations caused by inaccurate topology. Experimental results on two public traffic datasets demonstrate the effectiveness of the AGC-net\footnote{Code is available at: 
    
[^119]: 基于数据驱动的超材料和多尺度系统设计综述

    Data-Driven Design for Metamaterials and Multiscale Systems: A Review. (arXiv:2307.05506v1 [cs.CE])

    [http://arxiv.org/abs/2307.05506](http://arxiv.org/abs/2307.05506)

    基于数据驱动的超材料设计为实现下一代具有特殊功能的设备提供了巨大潜力，但其庞大的设计空间和复杂的结构-性能关系是重要的挑战。该综述提供了当前快速发展的领域的综合概述，强调方法论而非特定领域。数据驱动模块包括数据获取、机器学习单元设计和数据驱动多尺度优化，研究方法被基于共享原则进行分类，并讨论了未来研究方向。

    

    超材料是一种人工设计的材料，其有效材料参数超越自然材料。由丰富设计性的单元单元构成的多尺度系统，它们有着实现下一代具有特殊功能性的设备的巨大潜力。然而，庞大的设计空间和复杂的结构-性能关系在其设计中带来了重要挑战。数据驱动设计是一种有潜力实现超材料的全部潜能的新兴范例。在这篇综述中，我们提供了这个迅速发展领域的整体概述，重点是总体方法论而不是特定领域和部署环境。我们将现有研究区分为数据驱动模块，包括数据获取、基于机器学习的单元单元设计和数据驱动的多尺度优化。我们进一步基于共享原则对每个模块中的方法进行了分类，分析了各种方法的优势和不足，并讨论了未来研究的方向。

    Metamaterials are artificial materials designed to exhibit effective material parameters that go beyond those found in nature. Composed of unit cells with rich designability that are assembled into multiscale systems, they hold great promise for realizing next-generation devices with exceptional, often exotic, functionalities. However, the vast design space and intricate structure-property relationships pose significant challenges in their design. A compelling paradigm that could bring the full potential of metamaterials to fruition is emerging: data-driven design. In this review, we provide a holistic overview of this rapidly evolving field, emphasizing the general methodology instead of specific domains and deployment contexts. We organize existing research into data-driven modules, encompassing data acquisition, machine learning-based unit cell design, and data-driven multiscale optimization. We further categorize the approaches within each module based on shared principles, analyze
    
[^120]: HIVA: 全息智能语音助手

    HIVA: Holographic Intellectual Voice Assistant. (arXiv:2307.05501v1 [cs.HC])

    [http://arxiv.org/abs/2307.05501](http://arxiv.org/abs/2307.05501)

    HIVA 是一个全息智能语音助手，通过视听效果和3D动画促进人机交互。它提供大学的各种信息，支持"面对面"交流，并提供多个子模块和连接其他应用程序的功能。

    

    全息智能语音助手（HIVA）旨在利用视听效果和3D动画来促进人机交互。HIVA提供有关大学的完整信息，包括各种自然请求：入学，学习问题，学费，院系，大学结构和历史，食堂，人力资源，图书馆，学生生活和活动，以及有关该国家和城市的信息等。获取上述数据的其他方式包括大学官方网站和其他支持应用程序，高等教育机构（HEI）官方社交媒体，直接向高等教育机构工作人员提问等。然而，HIVA提供了与动画3D吉祥物的“面对面”交互的独特体验，帮助人们获得“现实生活”交流的感觉。该系统包括许多子模块，并连接了一系列应用程序，如移动应用程序、Telegram聊天机器人、建议分类和娱乐服务。

    Holographic Intellectual Voice Assistant (HIVA) aims to facilitate human computer interaction using audiovisual effects and 3D avatar. HIVA provides complete information about the university, including requests of various nature: admission, study issues, fees, departments, university structure and history, canteen, human resources, library, student life and events, information about the country and the city, etc. There are other ways for receiving the data listed above: the university's official website and other supporting apps, HEI (Higher Education Institution) official social media, directly asking the HEI staff, and other channels. However, HIVA provides the unique experience of "face-to-face" interaction with an animated 3D mascot, helping to get a sense of 'real-life' communication. The system includes many sub-modules and connects a family of applications such as mobile applications, Telegram chatbot, suggestion categorization, and entertainment services. The Voice assistant us
    
[^121]: 等变和不变的对称性对流体流动建模的重要性

    Importance of equivariant and invariant symmetries for fluid flow modeling. (arXiv:2307.05486v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.05486](http://arxiv.org/abs/2307.05486)

    通过构建一个等变GNN来研究在流体流动建模中等变和不变的对称性的影响，结果显示模拟不变数量能够产生更准确的长期预测，并可从速度场中学习到。

    

    图神经网络（GNN）在学习基于非结构化网格的物理系统模拟中显示出了潜力，包括流体动力学。同时，几何深度学习原理指导了建立尊重底层物理对称性的等变结构。然而，在模拟流体中，旋转等变性的影响仍不清楚。我们构建了一个多尺度等变GNN来预测流体流动，并研究了建模流动状态的不变和非不变表示的效果。我们评估了几种等变和非等变结构在预测两种流体流动（圆柱周围的流动和浮力驱动的剪切流动）演化过程中的模型性能，以了解等变性和不变性对数据驱动建模方法的影响。我们的结果表明，模拟不变数量能够产生更准确的长期预测，并且这些不变数量可以从速度场中学习到。

    Graph neural networks (GNNs) have shown promise in learning unstructured mesh-based simulations of physical systems, including fluid dynamics. In tandem, geometric deep learning principles have informed the development of equivariant architectures respecting underlying physical symmetries. However, the effect of rotational equivariance in modeling fluids remains unclear. We build a multi-scale equivariant GNN to forecast fluid flow and study the effect of modeling invariant and non-invariant representations of the flow state. We evaluate the model performance of several equivariant and non-equivariant architectures on predicting the evolution of two fluid flows, flow around a cylinder and buoyancy-driven shear flow, to understand the effect of equivariance and invariance on data-driven modeling approaches. Our results show that modeling invariant quantities produces more accurate long-term predictions and that these invariant quantities may be learned from the velocity field using a da
    
[^122]: 路由、解释、重复：模糊后解释性与可解释模型之间的界限。

    Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models. (arXiv:2307.05350v1 [cs.LG])

    [http://arxiv.org/abs/2307.05350](http://arxiv.org/abs/2307.05350)

    本文模糊了后解释黑盒模型与构建可解释模型之间的界限，通过从灵活的黑盒模型开始，逐渐引入可解释模型和残差网络，实现了对样本的路由和解释。

    

    目前的机器学习模型设计方法要么选择一个灵活的黑盒模型并在后期解释它，要么从一个可解释的模型开始。黑盒模型灵活但难以解释，而可解释模型设计为可解释。然而，开发可解释模型需要深厚的机器学习知识，而得到的模型往往不够灵活，可能性能不及其黑盒模型的等价物。本文旨在模糊后解释黑盒模型与构建可解释模型之间的区别。我们提议从一个灵活的黑盒模型开始，并逐渐「雕刻」出一种混合了可解释模型和一个「残差网络」的架构。我们的设计通过可解释模型「路由」一部分样本，剩余的样本则通过灵活的残差网络进行路由。我们采用一阶逻辑（FOL）作为可解释模型的基础。

    The current approach to ML model design is either to choose a flexible Blackbox model and explain it post hoc or to start with an interpretable model. Blackbox models are flexible but difficult to explain, whereas interpretable models are designed to be explainable. However, developing interpretable models necessitates extensive ML knowledge, and the resulting models tend to be less flexible, offering potentially subpar performance compared to their Blackbox equivalents. This paper aims to blur the distinction between a post hoc explanation of a BlackBox and constructing interpretable models. We propose beginning with a flexible BlackBox model and gradually \emph{carving out} a mixture of interpretable models and a \emph{residual network}. Our design identifies a subset of samples and \emph{routes} them through the interpretable models. The remaining samples are routed through a flexible residual network. We adopt First Order Logic (FOL) as the interpretable model's backbone, which pro
    
[^123]: 视频流上的测试时培训

    Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])

    [http://arxiv.org/abs/2307.05014](http://arxiv.org/abs/2307.05014)

    该论文扩展了测试时培训（TTT）到视频流的设置中，提出了在线TTT方法，相对于固定模型基线和离线TTT，在多个任务上都有显著的性能优势，包括实例和全景分割。

    

    先前的研究已经将测试时培训（TTT）确定为一种在测试时进一步改进训练模型的通用框架。在对每个测试实例进行预测之前，模型会使用自监督任务（例如使用掩蔽自动编码器进行图像重建）在同一实例上进行训练。我们将TTT扩展到流式设置中，其中多个测试实例（在我们的情况下为视频帧）按时间顺序到达。我们的扩展是在线TTT：当前模型从上个模型初始化，然后在当前帧和前几个帧的小窗口上进行训练。在线TTT在四个任务上明显优于固定模型基线，在三个实际数据集上的相对改进分别为45%和66%。令人惊讶的是，在线TTT也优于其离线版本，后者访问更多信息，可以训练所有帧而不考虑时间顺序。这与先前的研究结果不同。

    Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using 
    
[^124]: 安全强化学习的概率性反例引导

    Probabilistic Counterexample Guidance for Safer Reinforcement Learning. (arXiv:2307.04927v1 [cs.LG])

    [http://arxiv.org/abs/2307.04927](http://arxiv.org/abs/2307.04927)

    本文提出了一种安全强化学习方法，通过引导训练中的反例来解决安全探索的问题，该方法将连续和离散状态空间系统抽象为紧凑的模型，并利用概率性反例生成构建最小化仿真子模型，以揭示安全需求的违反情况。

    

    安全探索旨在解决强化学习在安全关键场景中的局限性，其中在试错学习过程中的失败可能会导致高成本。存在多种方法来整合外部知识或使用近距离传感器数据来限制对不安全状态的探索。然而，在未知环境中减少探索风险仍然具有挑战性，因为代理必须在探索过程中发现安全威胁。本文通过采用安全需求的反例引导训练来解决安全探索问题。我们的方法将连续和离散状态空间系统抽象为紧凑的抽象模型，代表代理在探索过程中获得的与安全相关的知识。然后，我们利用概率性反例生成构建最小化仿真子模型，以揭示安全需求的违反情况，代理可以使用这些模型在离线环境中进行高效的训练，以优化其策略，减小安全风险。

    Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the 
    
[^125]: 弱监督定位对比学习：肝硬化分类的应用

    Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04617](http://arxiv.org/abs/2307.04617)

    本研究提出了一种弱监督定位对比学习策略，在医学应用中使用大量弱标签图像进行肝硬化预测。这种策略将每个2D切片的空间上下文和弱标签整合起来，通过使用通用基于核的损失函数实现高效学习。

    

    大型医学影像数据集可以通过低置信度的弱标签（例如放射学评分）进行廉价快速的注释。而高置信度的标签（如基于组织学的诊断）很少且昂贵。预训练策略，如对比学习方法，可以利用未标记或弱标注的数据集。然而，这些方法通常需要较大的批处理大小，这在大型3D图像的全分辨率情况下存在困难，因为GPU内存有限。尽管如此，关于每个2D切片的空间上下文的体积信息对于某些医学应用非常重要。在这项研究中，我们提出了一种高效的弱监督定位对比学习策略，通过使用通用基于核的损失函数将每个2D切片的空间上下文和弱标签进行整合。我们通过使用大量弱标签图像（即放射学低置信度标注）来说明我们的方法在肝硬化预测中的应用。

    Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations,
    
[^126]: Solvent: 一个用于蛋白质折叠的框架

    Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2307.04603](http://arxiv.org/abs/2307.04603)

    Solvent是一个用于蛋白质折叠的统一研究框架，支持最新模型重要组件的实现和基准测试，并提供对蛋白质结构建模领域的有用见解。

    

    一致性和可靠性对于进行人工智能研究至关重要。许多著名的研究领域，如目标检测，已经通过稳定的基准框架进行了比较和验证。在AlphaFold2之后，蛋白质折叠任务已经进入了一个新阶段，许多方法都是基于AlphaFold2的组件提出的。在蛋白质折叠中，一个统一的研究框架的重要性包括实现和基准，以一致且公平地比较各种方法。为了实现这一点，我们提出了Solvent，一个支持最新模型重要组件的蛋白质折叠框架。Solvent包含在一个统一的代码库中实现的不同模型，并支持在相同数据集上对定义的模型进行训练和评估。我们对著名算法及其组件进行了基准测试，并进行了实验以对蛋白质结构建模领域提供有用的见解。我们希望Solvent能提高蛋白质折叠研究的可靠性。

    Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, an protein folding framework that supports significant components of state-of-th-arts models in the manner of off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliabili
    
[^127]: 增强空间上下文的潜在图注意力

    Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])

    [http://arxiv.org/abs/2307.04149](http://arxiv.org/abs/2307.04149)

    本文提出了一种称为潜在图注意力（LGA）的模块化框架，用于增强图像中的全局上下文。LGA在计算上简洁且稳定，能够在小规模体系结构中实现接近大规模体系结构的性能，使得轻量级体系结构在边缘设备上更加实用。

    

    图像中的全局上下文在图像到图像转换问题中非常有价值。传统的基于注意力和图模型在很大程度上捕捉到了全局上下文，但是这些方法计算上比较昂贵。此外，现有的方法只能学习图像中任意两个点之间的配对语义关系。在本文中，我们提出了一种称为潜在图注意力（LGA）的计算简洁（与节点数量呈线性关系）和稳定的模块化框架，用于将全局上下文纳入现有体系结构中，特别是增强小规模体系结构的性能，使得轻量级体系结构对于计算能力较低和能量需求较低的边缘设备更加有用。LGA使用局部连接图网络来在空间上传播信息，从而方便地构建远距离的两个空间点之间的语义一致关系。

    Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also 
    
[^128]: 使用图神经网络的罗马数字分析：从音符特征到按音预测

    Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features. (arXiv:2307.03544v1 [cs.SD])

    [http://arxiv.org/abs/2307.03544](http://arxiv.org/abs/2307.03544)

    本文提出了一种基于图神经网络的新方法，用于自动罗马数字分析。该方法可以直接处理乐谱中的每个音符，利用音符特征和音符之间的相互依赖关系，并通过新型边缩减算法产生按音的表示。在参考数据集上，ChordGNN模型表现优于现有的最先进模型，具有更高的罗马数字分析准确率。

    

    罗马数字分析是在调性音乐作品中识别和确定和弦以及其功能背景的重要任务。本文提出了一种基于符号音乐的自动罗马数字分析新方法。现有技术依赖于对乐谱的中间丢失压缩表示，我们提出了一种基于图神经网络(GNNs)的新方法，可以直接描述和处理乐谱中的每一个音符。所提出的架构可以利用音符特征和音符之间的相互依赖关系，并通过我们的新型边缩减算法产生按音的表示。我们的结果表明ChordGNN优于现有的最先进模型，在参考数据集上实现了更高的罗马数字分析准确率。此外，我们还探索了使用NADE和后处理和弦预测等技术的模型变体。本文的完整源代码可在https://github.com/mano上获取。

    Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. The proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. Our results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. In addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/mano
    
[^129]: 蒸馏修剪：使用合成数据赢得彩票的方法

    Distilled Pruning: Using Synthetic Data to Win the Lottery. (arXiv:2307.03364v1 [cs.LG])

    [http://arxiv.org/abs/2307.03364](http://arxiv.org/abs/2307.03364)

    该论文介绍了一种使用蒸馏数据来修剪深度学习模型的新方法，能够比传统方法更快地找到稀疏的可训练子网络，具有资源高效的神经网络修剪潜力。

    

    该论文介绍了一种通过使用蒸馏数据来修剪深度学习模型的新方法。与传统策略主要关注体系结构或算法优化不同，我们的方法重新考虑了数据在这些场景中的作用。蒸馏数据集捕捉了更大数据集中的重要模式，并且我们展示了如何利用这种能力来实现计算效率高的修剪过程。我们的方法可以在CIFAR-10上比迭代幅值修剪更快地找到稀疏的可训练子网络（也称为彩票票）。实验结果突显了使用蒸馏数据进行资源高效的神经网络修剪、模型压缩和神经网络架构搜索的潜力。

    This work introduces a novel approach to pruning deep learning models by using distilled data. Unlike conventional strategies which primarily focus on architectural or algorithmic optimization, our method reconsiders the role of data in these scenarios. Distilled datasets capture essential patterns from larger datasets, and we demonstrate how to leverage this capability to enable a computationally efficient pruning process. Our approach can find sparse, trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results highlight the potential of using distilled data for resource-efficient neural network pruning, model compression, and neural architecture search.
    
[^130]: 从文本生成艺术性的影动图

    Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v1 [cs.CV])

    [http://arxiv.org/abs/2307.03190](http://arxiv.org/abs/2307.03190)

    本论文介绍了一种通过文本描述来创建艺术性影动图的自动化方法。通过合成图像双胞胎，即一对艺术图像和与之对齐的真实图像，可以同时满足艺术风格和外观的要求并简化动作分析。同时，利用现有数据集可以准确地分割真实图像并预测合理的运动。

    

    我们介绍了一种全自动的方法，通过文本描述来创建艺术性的影动图。在处理虚构元素和艺术风格的提示时，这是一项特别具有挑战性的任务，因为需要解释这些图像的语义和动作的复杂性。现有的单图动画方法在艺术性输入方面存在不足，而最近的基于文本的视频方法常常引入时间不一致性，难以使某些区域保持静态。为了应对这些挑战，我们提出了一种通过单个文本提示合成图像双胞胎的思想，即艺术图像和其与像素对齐的自然外观配对。虽然艺术图像描绘了我们在文本提示中详细描述的风格和外观，但真实的对应图像大大简化了布局和动作分析。利用现有的自然图像和视频数据集，我们可以准确地分割出真实图像并根据语义信息预测出合理的运动。

    We introduce Artistic Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions - an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image twins from a single text prompt - a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predi
    
[^131]: LLaMA在临床领域的参数高效微调

    Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])

    [http://arxiv.org/abs/2307.03042](http://arxiv.org/abs/2307.03042)

    本研究提出了一种参数高效微调（PEFT）方法，在临床领域使用临床记录训练了一个专门适配临床领域的LLaMA-LoRA模型，同时提出了一个两步PEFT框架，用于将其与Downstream LLaMA-LoRA适配器进行融合，以实现领域适应。

    

    传统上，将预训练的语言模型适应到新领域，如临床应用，需要重新训练所有参数。然而，由于训练这些大型语言模型所需的计算资源巨大，这种方法的实践性越来越被证明是不切实际的。为了解决这个问题，参数高效微调（PEFT）技术提供了一种可行的解决方案，通过选择性地微调一个小的附加参数集，显著减少了领域适应所需的计算资源。在本研究中，我们提出了临床LLaMA-LoRA，这是一个构建在开源LLaMA模型上的PEFT适配器层。临床LLaMA-LoRA使用从MIMIC-IV数据库中获取的临床记录进行训练，从而创建了一个专为临床领域设计的专用适配器。此外，我们提出了一个两步PEFT框架，将临床LLaMA-LoRA与Downstream LLaMA-LoRA进行融合，后者是另一个专为下游任务设计的PEFT适配器。

    Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
    
[^132]: MOPO-LSI：用户指南

    MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.01719](http://arxiv.org/abs/2307.01719)

    MOPO-LSI是一款开源的多目标投资组合优化库，为可持续投资提供用户指南，并介绍了版本1.0的问题设置、工作流程和超参数。

    

    MOPO-LSI是一种开源的可持续投资多目标投资组合优化库。本文提供了MOPO-LSI版本1.0的用户指南，包括问题设置、工作流程和配置中的超参数。

    MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
    
[^133]: ESGCN: 边缘压缩注意图卷积网络用于交通流量预测

    ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])

    [http://arxiv.org/abs/2307.01227](http://arxiv.org/abs/2307.01227)

    ESGCN是一种用于交通流量预测的边缘压缩注意图卷积网络，通过建模时空动态和引入边缘特征和边缘注意机制来提高预测的准确性。

    

    交通流量预测是一个极具挑战性的任务，由于交通流的动态时空依赖关系。为了应对这个问题，我们着重于建模时空动态并提出了一种名为Edge Squeeze Graph Convolutional Network (ESGCN)的网络来预测多个地区的交通流量。ESGCN由两个模块组成：W模块和ES模块。W模块是一个完全以节点为基础的卷积网络。它分别对每个交通区域的时间序列进行编码，并在不同尺度上分解时间序列以捕捉细粒度和粗粒度的特征。ES模块使用图卷积网络(GCN)建模时空动态，并利用时序特征生成自适应邻接矩阵(AAM)。为了提高AAM的准确性，我们引入了三个关键概念。1）使用边缘特征直接捕捉区域之间的时空流动表示。2）将边缘注意机制应用于GCN，从边缘特征中提取AAM。

    Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention m
    
[^134]: 神经多面体

    Neural Polytopes. (arXiv:2307.00721v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00721](http://arxiv.org/abs/2307.00721)

    使用ReLU激活的简单神经网络可以在不同维度中逼近单位球形，生成神经多面体，这一发现开启了利用机器学习来逼近曲面的生成离散几何研究。

    

    我们发现，使用ReLU激活的简单神经网络可以在不同维度中逼近单位球形，生成多面体。多面体的种类受到网络架构的调节，如单元数和层数。对于多种激活函数，我们得到了多面体的泛化，我们称之为神经多面体。它们是多面体的平滑类比，展现了几何的对偶性。这一发现开启了利用机器学习来逼近曲面的生成离散几何研究。

    We find that simple neural networks with ReLU activation generate polytopes as an approximation of a unit sphere in various dimensions. The species of polytopes are regulated by the network architecture, such as the number of units and layers. For a variety of activation functions, generalization of polytopes is obtained, which we call neural polytopes. They are a smooth analogue of polytopes, exhibiting geometric duality. This finding initiates research of generative discrete geometry to approximate surfaces by machine learning.
    
[^135]: 通过跳过零元素降低卷积层的计算复杂度

    Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])

    [http://arxiv.org/abs/2306.15951](http://arxiv.org/abs/2306.15951)

    本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。

    

    深度神经网络依赖并行处理器进行加速。为了为其设计运算符，需要不仅有优化算法以降低复杂度，还需要充分利用硬件资源。卷积层主要包含三种运算符：前向传播的卷积，反向传播的反卷积和膨胀卷积。当执行这些运算时，始终会向张量中添加0元素，导致冗余计算。本文提出了C-K-S算法（ConvV2, KS-deconv, Sk-dilated），以两种方式跳过这些0元素：修剪滤波器以排除填充的0元素；将稀疏张量转换为稠密张量，避免在反卷积和膨胀卷积中插入0元素。与普通卷积相比，反卷积由于其复杂性而难以加速。本文提供了C-K-S的高性能GPU实现，并通过与PyTorch的比较验证了其有效性。根据实验结果，在某些情况下，C-K-S相对于PyTorch具有优势。

    Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
    
[^136]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^137]: 高斯过程网络的贝叶斯方法

    A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v1 [stat.ML])

    [http://arxiv.org/abs/2306.11380](http://arxiv.org/abs/2306.11380)

    该论文提出了一种基于高斯过程和贝叶斯方法的网络模型，通过蒙特卡罗和马尔可夫链蒙特卡罗方法采样网络结构的后验分布。该方法在恢复网络的图形结构方面优于最先进的算法，并提供了后验概率的准确近似。

    

    高斯过程网络（GPNs）是一类有向图模型，其使用高斯过程作为网络中每个变量给定其父变量的条件期望的先验分布。该模型允许以紧凑但灵活的方式描述连续联合分布，对变量之间的依赖关系仅做最少的参数假设。GPNs的贝叶斯结构学习需要计算网络结构的后验分布，即使在低维情况下，这也是计算上不可行的。本文实现了蒙特卡罗和马尔可夫链蒙特卡罗方法来从网络结构的后验分布中采样。因此，该方法遵循贝叶斯范式，通过边缘似然比较模型，并计算GPN特征的后验概率。模拟研究表明，我们的方法在恢复网络的图形结构方面优于最先进的算法，并提供其后验的准确近似。

    Gaussian Process Networks (GPNs) are a class of directed graphical models which employ Gaussian processes as priors for the conditional expectation of each variable given its parents in the network. The model allows describing continuous joint distributions in a compact but flexible manner with minimal parametric assumptions on the dependencies between variables. Bayesian structure learning of GPNs requires computing the posterior over graphs of the network and is computationally infeasible even in low dimensions. This work implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the posterior distribution of network structures. As such, the approach follows the Bayesian paradigm, comparing models via their marginal likelihood and computing the posterior probability of the GPN features. Simulation studies show that our method outperforms state-of-the-art algorithms in recovering the graphical structure of the network and provides an accurate approximation of its poste
    
[^138]: 评估指导微调语言模型的零样本鲁棒性

    Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11270](http://arxiv.org/abs/2306.11270)

    本文评估了指导微调语言模型的零样本鲁棒性，并发现使用新颖但合适的指导措辞会降低模型性能。

    

    指导微调最近被提出作为提高大型语言模型在新任务上的零样本能力的一种有希望的方法。这种技术在改善中等大小的语言模型（LLMs）的性能方面表现出了特别的优势，有时甚至与更大的模型变种相竞争。在本文中，我们提出两个问题：（1）指导微调的模型对指导的特定措辞有多敏感，（2）如何使它们更能抵抗自然语言的变化。为了回答前一个问题，我们收集了由NLP从业者手工编写的319个指导，涵盖了广泛使用的基准测试中的80多个独特任务，并评估了这些指导与指导微调期间观察到的指导措辞之间的方差和平均性能。我们发现，使用新颖（未观察到的）但合适的指导措辞会一致地降低模型的性能，有时甚至会大幅降低。

    Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further,
    
[^139]: 稀疏模块激活用于高效的序列建模

    Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11197](http://arxiv.org/abs/2306.11197)

    本论文引入了稀疏模块激活 (SMA) 机制，用于高效的序列建模。这种机制可以动态地稀疏激活序列元素的子模块，减少计算和内存消耗。

    

    线性状态空间模型 (SSM) 在各种序列建模任务中表现出了很强的性能，因为它们有效地编码了循环结构。然而，在更综合的任务中，如语言建模和机器翻译中，基于自注意力的模型仍然优于SSM。同时使用SSM和自注意力的混合模型通常显示出有希望的性能，但当前方法将注意力模块静态且均匀地应用于输入序列中的所有元素，导致了质量和效率之间的次优权衡。在这项工作中，我们引入了稀疏模块激活 (SMA)，这是一种通用机制，使神经网络能够以可微分的方式稀疏地动态激活序列元素的子模块。通过允许每个元素跳过非激活的子模块，SMA可以在序列建模的训练和推理阶段降低计算和内存消耗。作为SMA的一个特定实例，我们设计了一种新颖的神经网络模型。

    Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
    
[^140]: MARBLE：音乐音频表征通用评估基准

    MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.10548](http://arxiv.org/abs/2306.10548)

    本论文介绍了MARBLE，一个音乐音频表征通用评估基准，它为音乐理解领域的研究和发展提供了一个全面和可持续性的基础，并提供各种音乐信息检索（MIR）任务的基准。

    

    在艺术与人工智能（AI）之间交叉的广泛时代中，例如图像生成和虚构共创，音乐的AI仍然相对初步，特别是在音乐理解方面。针对这个问题，本论文介绍了一个通用的音乐音频表征评估基准MARBLE，旨在提供各种音乐信息检索（MIR）任务的基准，通过定义包括声学，演奏，乐谱和高级描述在内的四个层次的综合分类法。然后，我们基于8个公共可用数据集上的14项任务建立了一个统一的协议，提供了所有基于音乐录音开发的开放源代码的预训练模型的表征的公平和标准的评估。此外，MARBLE提供了一个易于使用、可扩展和可重用的工具库，以支持社区驱动的客观基准评估。

    In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
    
[^141]: 评估用于链接预测的图神经网络：当前的问题与新的基准

    Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. (arXiv:2306.10453v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10453](http://arxiv.org/abs/2306.10453)

    评估了图神经网络在链接预测任务中的应用，并提出了一种基于启发式相关采样技术的实用评估设置，克服了当前存在的问题。

    

    链接预测试图根据图的一部分边来预测是否存在未见的边。近年来引入了一系列试图利用图神经网络（GNN）进行这一任务的方法。此外，还创建了新的多样化数据集以更好地评估这些新模型的有效性。然而，目前存在多个问题阻碍我们能够正确评估这些新方法。这些问题主要包括：（1）多个基准测试的表现低于实际表现，（2）某些数据集上缺乏统一的数据划分和评估指标，以及（3）使用简单负样本的不现实的评估设置。为了克服这些挑战，我们首先在知名方法和数据集上进行公平比较，使用相同的数据集和超参数搜索设置。然后，我们基于启发式相关采样技术（HeaRT）创建了一个更实际的评估设置。

    Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard ne
    
[^142]: Decision-Making and Control的深度生成模型

    Deep Generative Models for Decision-Making and Control. (arXiv:2306.08810v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08810](http://arxiv.org/abs/2306.08810)

    本论文研究了基于深度模型的强化学习方法在决策和控制问题中的缺点，并提出了解决方法。同时，将当代生成建模工具中的推理技术重新解释为强化学习问题的规划策略。

    

    基于深度模型的强化学习方法提供了一种概念上简单的决策和控制问题的方法：使用学习来估计近似动力学模型，并将剩余工作委托给经典轨迹优化。然而，这种组合存在一些经验上的缺点，限制了实际中基于模型的方法的实用性。本论文的双重目的是研究这些缺点的原因并提出解决方法。在此过程中，我们强调了当代生成建模工具箱中的推理技术，包括束搜索、分类器引导采样和图像修复，如何重新解释为强化学习问题的可行规划策略。

    Deep model-based reinforcement learning methods offer a conceptually simple approach to the decision-making and control problem: use learning for the purpose of estimating an approximate dynamics model, and offload the rest of the work to classical trajectory optimization. However, this combination has a number of empirical shortcomings, limiting the usefulness of model-based methods in practice. The dual purpose of this thesis is to study the reasons for these shortcomings and to propose solutions for the uncovered problems. Along the way, we highlight how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems.
    
[^143]: 具有轨迹引导和针尖力反馈的协作机器人活检

    Collaborative Robotic Biopsy with Trajectory Guidance and Needle Tip Force Feedback. (arXiv:2306.07129v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.07129](http://arxiv.org/abs/2306.07129)

    这项研究介绍了一个具有轨迹引导和针尖力反馈的协作机器人活检系统，通过结合机器人的导航和医生的实时控制，提供了针头放置的辅助。研究中提出了一种能够实时感知针尖力的针头设计，通过此系统可以改善针头的定位，提高活检的准确性。

    

    活检的诊断价值高度依赖于针头的放置。机器人轨迹引导已被证明能改善针头定位，但实时导航的反馈有限。针尖力的触觉显示可以通过使组织结构在插入路径上定位，为针头导航提供丰富的反馈。我们提出了一个协作的机器人活检系统，将轨迹引导与动觉反馈相结合，辅助医生进行针头放置。机器人在插入过程中调整针头的位置，与现场控制针头位置的医疗专家协作进行。我们提出了一种能够实时感知针尖力的针头设计，基于光学相干断层扫描和机器学习进行实时数据处理。我们的机器人设置允许操作人员独立于摩擦力感知深层组织界面，以改善相对于期望目标结构的针头放置。我们首先评估了...

    The diagnostic value of biopsies is highly dependent on the placement of needles. Robotic trajectory guidance has been shown to improve needle positioning, but feedback for real-time navigation is limited. Haptic display of needle tip forces can provide rich feedback for needle navigation by enabling localization of tissue structures along the insertion path. We present a collaborative robotic biopsy system that combines trajectory guidance with kinesthetic feedback to assist the physician in needle placement. The robot aligns the needle while the insertion is performed in collaboration with a medical expert who controls the needle position on site. We present a needle design that senses forces at the needle tip based on optical coherence tomography and machine learning for real-time data processing. Our robotic setup allows operators to sense deep tissue interfaces independent of frictional forces to improve needle placement relative to a desired target structure. We first evaluate ne
    
[^144]: 价值函数即控制障碍函数：使用控制理论验证学习策略

    Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])

    [http://arxiv.org/abs/2306.04026](http://arxiv.org/abs/2306.04026)

    本研究将控制理论中的验证方法应用于强化学习中的价值函数，提出了新的度量方法以验证安全控制任务中的价值函数，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。

    

    尽管强化学习具有高度的通用性和可伸缩性，但验证策略行为的难度对于安全关键应用程序构成了挑战。为了解决这个问题，我们建议将控制理论中使用的验证方法应用于学习的价值函数。通过分析安全维护的简单任务结构，我们推导出将值函数与控制障碍函数相联系的原始定理。受此启发，我们提出了新的度量方法，以验证安全控制任务中的价值函数，并提出了改善学习的实际实施细节。除了提出证书学习的新方法外，我们的工作为RL策略解锁了丰富的控制理论验证方法，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。

    Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
    
[^145]: 在线张量学习：计算和统计权衡，适应性和最优遗憾

    Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v1 [stat.ML])

    [http://arxiv.org/abs/2306.03372](http://arxiv.org/abs/2306.03372)

    本文提出了在线黎曼梯度下降算法，用于在在线情况下估计潜在的低秩张量。其中，我们在处理连续或分类变量时提供了灵活的方法，并在在线情况下尝试了两个具体的应用，即在线张量补全和在线二元张量学习。我们还建立了逐个条目的精确错误界限，这是在在线张量补全中首次纳入噪声。我们观察到，在存在噪声的情况下，计算和统计方面存在着令人惊讶的权衡。

    

    我们研究了一个广义框架，用于在线情况下估计潜在的低秩张量，包括线性和广义线性模型。该框架提供了一种处理连续或分类变量的灵活方法。此外，我们研究了两个具体的应用：在线张量补全和在线二元张量学习。为了应对这些挑战，我们提出了在线黎曼梯度下降算法，在所有应用程序中都可以根据适当的条件线性收敛并恢复低秩组件。此外，我们为在线张量补全建立了精确的逐个条目错误界限。值得注意的是，我们的工作代表了首次尝试在在线低秩张量恢复任务中纳入噪声的努力。有趣的是，我们观察到在存在噪声的情况下，在计算和统计方面存在着令人惊讶的权衡。增加步长可以加快收敛，但会导致更高的统计误差。

    We investigate a generalized framework for estimating latent low-rank tensors in an online setting, encompassing both linear and generalized linear models. This framework offers a flexible approach for handling continuous or categorical variables. Additionally, we investigate two specific applications: online tensor completion and online binary tensor learning. To address these challenges, we propose the online Riemannian gradient descent algorithm, which demonstrates linear convergence and the ability to recover the low-rank component under appropriate conditions in all applications. Furthermore, we establish a precise entry-wise error bound for online tensor completion. Notably, our work represents the first attempt to incorporate noise in the online low-rank tensor recovery task. Intriguingly, we observe a surprising trade-off between computational and statistical aspects in the presence of noise. Increasing the step size accelerates convergence but leads to higher statistical error
    
[^146]: 一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法

    A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition. (arXiv:2306.02422v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.02422](http://arxiv.org/abs/2306.02422)

    本研究提出了一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法，即GALET，可以用于解决非凸下层目标的双层问题。该方法可以在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现问题的$\epsilon$-静态度量。

    

    双层优化近年来因其在超参数优化、元学习和强化学习等新兴机器学习领域中的应用而重新引起人们的关注。最近的结果显示，对于具有强凸下层目标的双层问题，简单的交替（隐式）基于梯度的算法可以实现与单层梯度下降（GD）相同的收敛速率。然而，对于超出此基本设置的双层问题，尚不清楚是否可以推广该结果。在本文中，我们提出了一种基于满足Polyak-{\L}ojasiewicz (PL)条件的非凸下层目标的双层优化的广义交替方法（GALET）。我们首先介绍了所考虑的双层问题的一个静态度量，它推广了现有的度量。然后我们证明GALET在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现了所考虑问题的$\epsilon$-静态度量。

    Bilevel optimization has recently regained interest owing to its applications in emerging machine learning fields such as hyperparameter optimization, meta-learning, and reinforcement learning. Recent results have shown that simple alternating (implicit) gradient-based algorithms can achieve the same convergence rate of single-level gradient descent (GD) for bilevel problems with a strongly convex lower-level objective. However, it remains unclear whether this result can be generalized to bilevel problems beyond this basic setting. In this paper, we propose a Generalized ALternating mEthod for bilevel opTimization (GALET) with a nonconvex lower-level objective that satisfies the Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric for the considered bilevel problems, which generalizes the existing metric. We then establish that GALET achieves an $\epsilon$-stationary metric for the considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which match
    
[^147]: 使用基于真实世界生命体征数据的端到端多元时间序列聚类算法识别ICU患者子群

    Identifying Subgroups of ICU Patients Using End-to-End Multivariate Time-Series Clustering Algorithm Based on Real-World Vital Signs Data. (arXiv:2306.02121v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02121](http://arxiv.org/abs/2306.02121)

    本研究使用真实世界生命体征数据开发了一种端到端多元时间序列聚类算法，并发现不同子群之间存在着不同的ICU和医院死亡率风险。

    

    本研究使用MIMIC-IV数据库作为数据源，研究了动态、高频率、多元时间序列生命体征数据在ICU逗留期间的应用，包括体温、心率、平均血压、呼吸频率和SpO2。比较了各种聚类算法，并选择了一种名为Time2Feat的端到端多元时间序列聚类系统结合K-Means作为最有效的ICU患者聚类方法。在聚类分析中，使用了2008年至2016年期间收治的8,080名患者的数据进行模型开发，使用了2017年至2019年期间收治的2,038名患者的数据进行模型验证。通过分析不同类别之间的临床死亡预后差异，发现了不同亚组之间ICU死亡率和医院死亡率的风险差异。此外，本研究还可视化了生命体征变化的轨迹。这项研究的发现为ICU患者提供了有价值的洞察。

    This study employed the MIMIC-IV database as data source to investigate the use of dynamic, high-frequency, multivariate time-series vital signs data, including temperature, heart rate, mean blood pressure, respiratory rate, and SpO2, monitored first 8 hours data in the ICU stay. Various clustering algorithms were compared, and an end-to-end multivariate time series clustering system called Time2Feat, combined with K-Means, was chosen as the most effective method to cluster patients in the ICU. In clustering analysis, data of 8,080 patients admitted between 2008 and 2016 was used for model development and 2,038 patients admitted between 2017 and 2019 for model validation. By analyzing the differences in clinical mortality prognosis among different categories, varying risks of ICU mortality and hospital mortality were found between different subgroups. Furthermore, the study visualized the trajectory of vital signs changes. The findings of this study provide valuable insights into the p
    
[^148]: 利用aggVAE进行深度学习和MCMC以处理行政边界变化：以肯尼亚的疟疾患病率为例

    Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v1 [cs.LG])

    [http://arxiv.org/abs/2305.19779](http://arxiv.org/abs/2305.19779)

    本研究提出了一种利用aggVAE进行深度学习和MCMC处理行政边界变化的解决方案，可以更准确地映射以县为层级的聚合级别数据，并处理行政边界的变化，相比最先进的模型表现更好。

    

    基于模型的疾病映射是公共卫生和疾病监测中基本的政策信息工具，分层贝叶斯模型是当前最先进的方法。当处理区域数据，如行政区划单位（例如县或省）的聚合数据时，常用的模型依赖于区域单元的相邻结构以考虑空间相关性。疾病监测系统的目标是随时间跟踪疾病结果，但在危机情况下（例如政治变化导致行政边界更改），这将带来挑战。我们提出了一种新颖、实用和易于实施的解决方案，该方案依赖于组合深层生成模型和全贝叶斯推断。我们建立在现有的变分自编码器(VAE) 工作上，并展示我们提出的聚合VAE(aggVAE)体系结构可用于在以县为层级的聚合级别处理数据，以映射肯尼亚的疟疾患病率。我们的模型可以以连续的方式考虑空间相关性，而不依赖于相邻性假设，并且能够处理行政边界的变化。结果表明，相比最先进的模型，我们的模型表现出更好的性能和更准确的疟疾患病率映射。

    Model-based disease mapping remains a fundamental policy-informing tool in public health and disease surveillance with hierarchical Bayesian models being the current state-of-the-art approach. When working with areal data, e.g. aggregates at the administrative unit level such as district or province, routinely used models rely on the adjacency structure of areal units to account for spatial correlations. The goal of disease surveillance systems is to track disease outcomes over time, but this provides challenging in situations of crises, such as political changes, leading to changes of administrative boundaries. Kenya is an example of such country. Moreover, adjacency-based approach ignores the continuous nature of spatial processes and cannot solve the change-of-support problem, i.e. when administrative boundaries change. We present a novel, practical, and easy to implement solution relying on a methodology combining deep generative modelling and fully Bayesian inference. We build on 
    
[^149]: DreamWaltz：使用复杂3D动画角色制作场景

    DreamWaltz: Make a Scene with Complex 3D Animatable Avatars. (arXiv:2305.12529v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.12529](http://arxiv.org/abs/2305.12529)

    DreamWaltz是一个新颖的框架，可以根据文本指导和参数化的人体先验生成和动画化复杂的3D角色。它提出了三维一致的遮挡感知得分蒸馏采样（SDS）来优化隐式神经表示和规范姿势，并学习了一种可动画且具有泛化能力的角色表示。这种方法在创建3D角色方面表现出了高效性和稳健性。

    

    我们提出了DreamWaltz，这是一个新颖的框架，可以根据文本指导和参数化的人体先验生成和动画化复杂的3D角色。尽管最近的方法在将文本转化为普通物体的3D生成方面取得了令人鼓舞的结果，但创建高质量且可动画的3D角色仍然具有挑战性。为了创建高质量的3D角色，DreamWaltz提出了三维一致的遮挡感知得分蒸馏采样（SDS）来优化隐式神经表示和规范姿势。它通过三维感知的骨架调节提供视角对齐的监督，使得复杂的角色生成不会产生伪影和多个面。对于动画，我们的方法学习了一种可动画且具有泛化能力的角色表示，可以将任意姿势映射到规范姿势表示中。广泛的评估表明，DreamWaltz是一种有效且稳健的方法，可以创建具有复杂形状和外观以及动画的3D角色。

    We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable and generalizable avatar representation which could map arbitrary poses to the canonical pose representation. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The 
    
[^150]: ANTONIO:面向NLP验证的系统化基准生成方法

    ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])

    [http://arxiv.org/abs/2305.04003](http://arxiv.org/abs/2305.04003)

    本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    

    自然语言处理（NLP）中使用的机器学习模型的验证被认为是一个难题。现有的神经网络验证方法常用于计算机视觉和其他数字数据集，但并不适用于NLP。本研究探讨了造成这一问题的技术原因，并在此基础上提出了实用的方法和启发式规则，以便将NLP数据集和模型准备为适合基于抽象解释的已知验证方法。我们将这些方法实现为一个名为ANTONIO的Python库，该库连接到神经网络验证器ERAN和Marabou。我们使用一个名为R-U-A-Robot的NLP数据集对工具进行了评估，该数据集被提议作为验证具有法律重要性的NLP应用的基准。我们希望，由于其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
    
[^151]: CodeGen2：编程和自然语言LLM训练的经验

    CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. (arXiv:2305.02309v1 [cs.LG])

    [http://arxiv.org/abs/2305.02309](http://arxiv.org/abs/2305.02309)

    本研究旨在提高LLMs编程合成训练的效率，通过统一模型架构、学习方法、填充采样和数据分布，来提高训练模型的泛化能力。

    

    大型语言模型在编程合成和理解任务的表示学习中展示了卓越的能力。学习到的表示质量似乎由神经比例定律作为模型参数和观察值的函数决定，同时通过可用数据和计算量的数量限制模型的性能。本研究尝试通过统一四个关键组件使LLMs的程序合成训练更加高效：（1）模型架构，（2）学习方法，（3）填充采样和（4）数据分布。

    Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.  In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a "free lunch" hypothesis. For data distributions, the eff
    
[^152]: DataComp：寻找下一代多模态数据集

    DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])

    [http://arxiv.org/abs/2304.14108](http://arxiv.org/abs/2304.14108)

    DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。

    

    大型的多模态数据集在近期的突破中起到了关键作用，比如CLIP、Stable Diffusion和GPT-4等。与此同时，数据集很少得到与模型架构或训练算法同等的研究关注。为了解决这个在机器学习生态系统中的缺陷，我们介绍了DataComp，一个基准测试，其中训练代码是固定的，研究人员通过提出新的训练集来进行创新。我们提供了一个基于Common Crawl的新候选池，其中包含12.8B个图像-文本对的数据集实验测试平台。参加我们基准测试的研究人员可以设计新的过滤技术或策划新的数据源，并通过运行我们标准化的CLIP训练代码并在38个下游测试集上进行测试来评估他们的新数据集。我们的基准测试包含多个规模，四个候选池大小和相应的计算预算，在训练期间涵盖了从12.8M到12.8B个样本。这种多规模设计有助于研究规模趋势，并为研究人员提供了更多的选择余地。

    Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
    
[^153]: 基于匹配的生成模型数据估值方法

    Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])

    [http://arxiv.org/abs/2304.10701](http://arxiv.org/abs/2304.10701)

    本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。

    

    数据估值对于机器学习非常重要，因为它有助于增强模型的透明度并保护数据特性。现有的数据估值方法主要集中在判别模型上，忽略了最近吸引了大量关注的深度生成模型。与判别模型类似，需要评估深度生成模型中数据贡献的紧迫需求也存在。然而，以往的数据估值方法主要依赖于判别模型性能指标，并需要对模型进行重新训练。因此，它们不能在实际中直接高效地应用于近期的深度生成模型，例如生成对抗网络和扩散模型。为了弥补这一差距，我们从相似性匹配的角度对生成模型中的数据估值问题进行了构建。具体地，我们引入了“Generative Model Valuator”（GMValuator）——第一个针对任何生成模型的模型无关方法，旨在为生成模型提供数据估值而无需重新训练模型。我们的方法利用数据实例及由生成模型生成的相应合成实例之间的相似度来估计原始数据的价值。大量实验证明了我们的方法在为不同的生成模型（包括GAN和扩散模型）评估数据实例方面的优越性。

    Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
    
[^154]: Bayesian Optimization中线性化Laplace的优势和局限性

    Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization. (arXiv:2304.08309v1 [cs.LG])

    [http://arxiv.org/abs/2304.08309](http://arxiv.org/abs/2304.08309)

    本论文研究了在线性化Laplace逼近(LLA)在Bayesian optimization中的应用。虽然LLA在构建贝叶斯神经网络时已被证明具有效性和高效性，但是在序列决策问题中，需要考虑其可能的局限性。

    

    线性化Laplace逼近(LLA)已被证明在构建贝叶斯神经网络时有效且高效。它在理论上具有吸引力，因为它可以被看作是具有高斯过程后验的最大后验预测函数最大化的神经网络的平均函数，并且由经验神经曲面核诱导的协方差函数。然而，尽管已经研究过其在图像分类等大规模任务中的效果，但在诸如Bayesian optimization这样的序列决策问题中尚未对其进行研究，其中高斯过程是默认的代理模型，具有简单的平均函数和核函数，例如径向基函数。在本文中，我们研究了LLA在Bayesian optimization中的有用性和灵活性，并强调其强大的性能。但是，我们还提出了可能出现的一些问题和一个LLA可能存在的问题，即当搜索空间是无界的时候。

    The linearized-Laplace approximation (LLA) has been shown to be effective and efficient in constructing Bayesian neural networks. It is theoretically compelling since it can be seen as a Gaussian process posterior with the mean function given by the neural network's maximum-a-posteriori predictive function and the covariance function induced by the empirical neural tangent kernel. However, while its efficacy has been studied in large-scale tasks like image classification, it has not been studied in sequential decision-making problems like Bayesian optimization where Gaussian processes -- with simple mean functions and kernels such as the radial basis function -- are the de-facto surrogate models. In this work, we study the usefulness of the LLA in Bayesian optimization and highlight its strong performance and flexibility. However, we also present some pitfalls that might arise and a potential problem with the LLA when the search space is unbounded.
    
[^155]: 稀疏深度神经网络中梯度下降的点对点收敛定理

    Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.08172](http://arxiv.org/abs/2304.08172)

    本文研究了稀疏深度神经网络中梯度下降的点对点收敛定理，针对非光滑指示函数构造了一种特殊形状的DNN，实现了梯度下降过程的点对点收敛。

    

    深度神经网络（DNN）的理论结构逐渐得到了阐明。Imaizumi-Fukumizu（2019）和Suzuki（2019）指出，当目标函数为非光滑函数时，DNN的学习能力优于先前的理论。然而，据作者所知，迄今为止的众多研究尝试在没有任何统计论证的情况下进行数学研究，探究真正能够引发梯度下降的DNN架构的点对点收敛性，这一尝试似乎更贴近实际DNN。本文将目标函数限制为非光滑指示函数，并在ReLU-DNN中构造了一个稀疏且具有特殊形状的DNN，从而实现了梯度下降过程中的点对点收敛。

    The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.
    
[^156]: CAR-DESPOT: 针对混杂环境下的机器人的因果关系在线POMDP规划

    CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])

    [http://arxiv.org/abs/2304.06848](http://arxiv.org/abs/2304.06848)

    本文提出了一种新的因果关系在线POMDP规划方法CAR-DESPOT，使用因果建模和推理来消除未测量混淆变量引起的错误，并在混杂环境中表现优异。

    

    在现实环境中工作的机器人必须考虑随机行为的可能结果，并根据真实的世界状态的部分观察进行决策。因果混淆的问题是进行准确和强健的行为预测的主要挑战。部分可观察的马尔可夫决策过程(POMDP)是一种广泛使用的框架，用于模拟这些随机和部分可观测的决策问题。然而，由于缺乏明确的因果语义，POMDP规划方法容易受到混淆偏差的影响，在未观察到混杂变量的情况下，可能会产生表现不佳的策略。本文提出了一种新的因果关系在线POMDP规划方法，使用因果建模和推理来消除未测量混淆变量引起的错误。我们进一步提出了一种从观测数据中学习因果模型的方法，以在我们的方法中使用。实验结果表明，我们的方法CAR-DESPOT在混杂环境中比现有的最先进的POMDP规划程序表现显著更好。

    Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
    
[^157]: GPT检测器对非英语母语的作者存在偏见。

    GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])

    [http://arxiv.org/abs/2304.02819](http://arxiv.org/abs/2304.02819)

    该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。

    

    生成语言模型的快速推广带来了数字通信方面的实质性进展，同时也引发了AI生成内容潜在误用的担忧。虽然已经提出了许多检测方法来区分AI和人类生成的内容，但这些检测器的公平性和鲁棒性仍未得到充分探讨。在这项研究中，我们使用来自英语母语和非英语母语作者的写作样本评估了几种广泛使用的GPT检测器的性能表现。我们的研究发现，这些检测器持续将非英语母语的写作样本错误地分类为AI生成的内容，而原生写作样本则能够被准确识别。此外，我们证明了简单的提示策略不仅可以缓解这种偏见，而且还可以有效地规避GPT检测器，这表明GPT检测器可能无意中惩罚具有受限语言表达能力的作者。我们的研究结果呼吁进行更广泛的讨论。

    The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
    
[^158]: DeepGD: 一种用于深度神经网络的多目标黑盒测试选择方法

    DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04878](http://arxiv.org/abs/2303.04878)

    DeepGD是一种用于深度神经网络的多目标黑盒测试选择方法，通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。

    

    深度神经网络（DNN）被广泛应用于图像处理、语音识别和自然语言处理等各个应用领域。然而，由于其输入域的复杂性和规模，测试DNN模型可能具有挑战性。特别地，测试DNN模型通常需要生成或探索大规模的未标记数据集。在实践中，DNN测试神谕通常需要昂贵的人工工作来标记测试数据，可能涉及多个专家以确保标记的正确性。在本文中，我们提出了DeepGD，一种用于DNN模型的黑盒多目标测试选择方法。它通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。

    Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by s
    
[^159]: 强化标签: 多智能体深度强化学习用于点特征标签放置

    Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement. (arXiv:2303.01388v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01388](http://arxiv.org/abs/2303.01388)

    本论文介绍了一种利用多智能体深度强化学习的标签放置方法，该方法在数据可视化中解决了标签重叠和可读性问题，与现有的手工设计算法相比具有显著的优势。

    

    最近几年，结合深度学习技术的强化学习已成功应用于解决各个领域的复杂问题，包括机器人、自动驾驶汽车和金融等。本文将强化学习引入标签放置，这是数据可视化中一个复杂的任务，旨在找到最佳位置以避免重叠并确保可读性。我们提出了一种新颖的点特征标签放置方法，利用多智能体深度强化学习(MADRL)来学习标签放置策略，这是与现有手工设计的算法相对应的第一个机器学习驱动的标签放置方法。为了方便强化学习的学习过程，我们开发了一个环境，其中代理作为标签的代理，这些标签是一种增强可视化的短文本注释。我们的结果表明，通过我们的方法训练得到的策略明显优于未经训练的代理的随机策略以及由人类设计的比较方法。

    Over recent years, Reinforcement Learning combined with Deep Learning techniques has successfully proven to solve complex problems in various domains, including robotics, self-driving cars, and finance. In this paper, we are introducing Reinforcement Learning (RL) to label placement, a complex task in data visualization that seeks optimal positioning for labels to avoid overlap and ensure legibility. Our novel point-feature label placement method utilizes Multi-Agent Deep Reinforcement Learning (MADRL) to learn the label placement strategy, which is the first machine-learning-driven labeling method in contrast to existing hand-crafted algorithms designed by human experts. To facilitate RL learning, we developed an environment where an agent acts as a proxy for a label, a short textual annotation that augments visualization. Our results show that the strategy trained by our method significantly outperforms the random strategy of an untrained agent and compared methods designed by human 
    
[^160]: 监督拓扑数据分析在MALDI质谱成像应用中的应用

    Supervised topological data analysis for MALDI mass spectrometry imaging applications. (arXiv:2302.13948v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.13948](http://arxiv.org/abs/2302.13948)

    这项研究提出了一个新的代数拓扑框架，通过从MALDI数据中获得内在信息并转化为反映拓扑持久性的形式，实现了在肺癌亚型分类中的噪音信号区分和数据压缩的功能。

    

    背景：基质辅助激光解吸/电离质谱成像（MALDI MSI）在癌症研究中显示出重要的潜力，特别是在肿瘤分型和亚型分析中。肺癌是肿瘤相关死亡的主要原因，其中最致命的实体是腺癌（ADC）和鳞状细胞癌（SqCC）。区分这两种常见亚型对于治疗决策和患者管理至关重要。结果：我们提出了一个新的代数拓扑框架，通过从MALDI数据中获取内在信息并将其转化为反映拓扑持久性的形式。我们的框架具有两个主要优点。首先，拓扑持久性有助于区分信号和噪音。其次，它压缩了MALDI数据，节省了存储空间，并优化了后续分类任务的计算时间。我们提供了一种高效实现我们拓扑框架的算法，该算法依赖于单一调优参数。

    Background: Matrix-assisted laser desorption/ionization mass spectrometry imaging (MALDI MSI) displays significant potential for applications in cancer research, especially in tumor typing and subtyping. Lung cancer is the primary cause of tumor-related deaths, where the most lethal entities are adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing between these two common subtypes is crucial for therapy decisions and successful patient management.  Results: We propose a new algebraic topological framework, which obtains intrinsic information from MALDI data and transforms it to reflect topological persistence. Our framework offers two main advantages. Firstly, topological persistence aids in distinguishing the signal from noise. Secondly, it compresses the MALDI data, saving storage space and optimizes computational time for subsequent classification tasks. We present an algorithm that efficiently implements our topological framework, relying on a single tuning param
    
[^161]: 关于贝尔曼最优性原理和安全限制马尔可夫决策过程的强化学习研究

    On Bellman's principle of optimality and Reinforcement learning for safety-constrained Markov decision process. (arXiv:2302.13152v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2302.13152](http://arxiv.org/abs/2302.13152)

    本文研究了安全限制马尔可夫决策过程，强调了贝尔曼最优性原理在具有多链结构的受限马尔可夫决策问题中可能不成立。通过将多目标优化问题表示为新的优化框架，解决了这个问题。

    

    本文研究了安全限制马尔可夫决策过程，这是安全强化学习的基本框架。具体而言，我们考虑了一个有限状态和有限动作的受限马尔可夫决策过程，在这个过程中，决策者的目标是在一定的概率保证下到达目标集合，同时避免进入一个不安全的集合。因此，任何控制策略下的马尔可夫链都会表现为多链结构，因为根据定义，存在一个目标集合和一个不安全集合。决策者在导航到目标集合时还必须是最优的（基于一个成本函数）。这导致了一个多目标优化问题。我们强调贝尔曼最优性原理在具有多链结构的受限马尔可夫决策问题中可能不成立（正如Haviv的反例所示）。我们通过将上述多目标优化问题表示为一个新的优化框架，解决了这个反例。

    We study optimality for the safety-constrained Markov decision process which is the underlying framework for safe reinforcement learning. Specifically, we consider a constrained Markov decision process (with finite states and finite actions) where the goal of the decision maker is to reach a target set while avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the underlying Markov chain for any control policy will be multichain since by definition there exists a target set and an unsafe set. The decision maker also has to be optimal (with respect to a cost function) while navigating to the target set. This gives rise to a multi-objective optimization problem. We highlight the fact that Bellman's principle of optimality may not hold for constrained Markov decision problems with an underlying multichain structure (as shown by the counterexample due to Haviv. We resolve the counterexample by formulating the aforementioned multi-objective optimization problem as a ze
    
[^162]: 线性化算法用于完全复合优化

    Linearization Algorithms for Fully Composite Optimization. (arXiv:2302.12808v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.12808](http://arxiv.org/abs/2302.12808)

    本文提出了针对完全复合优化问题的线性化算法。通过将目标函数的可微和不可微部分分开处理，仅线性化平滑部分，从而推广了Frank-Wolfe方法和Conditional Gradient Sliding算法，适用于一类不可微的问题。算法基于一个更强的线性最小化预言子版本，可在多个实际应用中高效实现，并提供了凸和非凸目标的全局收敛速度分析。另外，在凸情况下，还提出了一个加速方法以改善复杂度。通过说明性实验，验证了理论结果的有效性。

    

    本文研究了解决凸和紧集合上的完全复合优化问题的一阶算法。我们通过分别处理可微和不可微组成部分的结构，仅线性化平滑部分，提供了经典Frank-Wolfe方法和Conditional Gradient Sliding算法的新的推广，适用于一类不可微的问题。我们的算法依赖于更强的线性最小化预言子版本，可以在多个实际应用中高效实现。我们为我们的方法提供了基本版本的仿射不变分析，并证明了凸和非凸目标的全局收敛速度。此外，在凸情况下，我们提出了一个加速方法，复杂度相应提高。最后，我们提供了说明性实验来支持我们的理论结果。

    This paper studies first-order algorithms for solving fully composite optimization problems over convex and compact sets. We leverage the structure of the objective by handling its differentiable and non-differentiable components separately, linearizing only the smooth parts. This provides us with new generalizations of the classical Frank-Wolfe method and the Conditional Gradient Sliding algorithm, that cater to a subclass of non-differentiable problems. Our algorithms rely on a stronger version of the linear minimization oracle, which can be efficiently implemented in several practical applications. We provide the basic version of our method with an affine-invariant analysis and prove global convergence rates for both convex and non-convex objectives. Furthermore, in the convex case, we propose an accelerated method with correspondingly improved complexity. Finally, we provide illustrative experiments to support our theoretical results.
    
[^163]: 从有噪声的不动点迭代到私有的ADMM算法，用于集中式和联合学习

    From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning. (arXiv:2302.12559v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12559](http://arxiv.org/abs/2302.12559)

    本论文研究了将差分隐私机器学习算法视为有噪声的不动点迭代，从而从这一框架中推导出隐私和效用结果。通过利用这个新的视角，我们恢复了流行的私有梯度下降方法并提供了一种有原则的方法来设计和分析新的私有优化算法。我们通过通用框架推导出了用于集中式、联合和完全去中心化学习的新颖私有ADMM算法，并通过迭代和子采样的隐私放大建立了强隐私保证。最后，我们利用最近的有噪声固定点迭代的线性收敛结果提供效用保证。

    

    我们研究差分隐私（DP）机器学习算法作为噪声固定点迭代的实例，以便从这个经过充分研究的框架中得出隐私和效用结果。我们展示了这一新的视角恢复了流行的私有梯度下降方法，如DP-SGD，并为设计和分析新的私有优化算法提供了一种有原则的方式。专注于广泛使用的交替方向乘子法（ADMM）方法，我们利用我们的通用框架推导出用于集中式、联合和完全去中心化学习的新颖私有ADMM算法。对于这三个算法，我们利用迭代和子采样的隐私放大建立了强隐私保证。最后，我们利用最近的有噪声固定点迭代的线性收敛结果进行统一分析，提供效用保证。

    We study differentially private (DP) machine learning algorithms as instances of noisy fixed-point iterations, in order to derive privacy and utility results from this well-studied framework. We show that this new perspective recovers popular private gradient-based methods like DP-SGD and provides a principled way to design and analyze new private optimization algorithms in a flexible manner. Focusing on the widely-used Alternating Directions Method of Multipliers (ADMM) method, we use our general framework to derive novel private ADMM algorithms for centralized, federated and fully decentralized learning. For these three algorithms, we establish strong privacy guarantees leveraging privacy amplification by iteration and by subsampling. Finally, we provide utility guarantees using a unified analysis that exploits a recent linear convergence result for noisy fixed-point iterations.
    
[^164]: 使用贝叶斯最后一层改进神经网络的不确定性量化

    Improved uncertainty quantification for neural networks with Bayesian last layer. (arXiv:2302.10975v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10975](http://arxiv.org/abs/2302.10975)

    本文提出了一种改进神经网络不确定性量化的方法，使用贝叶斯最后一层近似不可处理的贝叶斯神经网络，通过最大化边际来获得权重的点估计。

    

    不确定性量化是机器学习中的重要任务，神经网络在这方面通常表现不佳。这对于安全关键应用来说是一个限制，因此常常倾向于使用能够感知不确定性的方法，如高斯过程或贝叶斯线性回归。贝叶斯神经网络是一种解决这个问题的方法，它假设所有参数都服从概率分布，并产生分布预测。然而，训练和推断通常是不可处理的，需要使用近似方法。一种有前景的近似方法是具有贝叶斯最后一层的神经网络。他们仅在最后一个线性层中假设分布权重，并产生一个正态分布的预测。具有贝叶斯最后一层的神经网络可以看作是具有学习非线性特征的贝叶斯线性回归模型。为了近似不可处理的贝叶斯神经网络，应通过最大化边际来获得除最后一层以外的所有分布权重的点估计。

    Uncertainty quantification is an essential task in machine learning - a task in which neural networks (NNs) have traditionally not excelled. This can be a limitation for safety-critical applications, where uncertainty-aware methods like Gaussian processes or Bayesian linear regression are often preferred. Bayesian neural networks are an approach to address this limitation. They assume probability distributions for all parameters and yield distributed predictions. However, training and inference are typically intractable and approximations must be employed. A promising approximation is NNs with Bayesian last layer (BLL). They assume distributed weights only in the last linear layer and yield a normally distributed prediction. NNs with BLL can be seen as a Bayesian linear regression model with learned nonlinear features. To approximate the intractable Bayesian neural network, point estimates of the distributed weights in all but the last layer should be obtained by maximizing the margina
    
[^165]: 一种适用于所有时间序列的Transformer：表示和训练具有时间相关的异构表格数据

    One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06375](http://arxiv.org/abs/2302.06375)

    本研究提出了一种Transformer架构，用于表示具有时间相关的异构表格数据，通过使用一组频率函数来表示数值特征，并采用唯一的损失函数进行统一训练。

    

    近年来，将深度学习技术应用于表格数据的兴趣日益增长，以复制其他人工智能领域在这一结构化领域的成功。特别有趣的是，表格数据具有时间依赖性，例如金融交易。然而，表格值的异质性，其中类别元素与数值项混合，使得这种适应变得困难。在本文中，我们提出了一种Transformer架构来表示异构的时间相关的表格数据，数值特征使用一组频率函数表示，并且整个网络使用唯一的损失函数进行统一训练。

    There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
    
[^166]: 渐进最优的固定预算最优臂识别方法与方差相关界限

    Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds. (arXiv:2302.02988v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02988](http://arxiv.org/abs/2302.02988)

    本文研究了最小化期望简单遗憾的固定预算最优臂识别问题。通过推导最坏情况期望简单遗憾的渐进下界，提出了基于HIR估计的TS-HIR策略，该策略在推荐最优臂时表现出近似最优性。

    

    本文研究了最小化期望简单遗憾的固定预算最优臂识别问题。在自适应实验中，决策者根据过去的观测结果选择多个处理臂之一，并观察所选择的臂的结果。实验结束后，决策者推荐期望结果最高的处理臂。我们基于期望简单遗憾评估决策的好坏，其定义为最优臂的期望结果与推荐臂的期望结果之差。由于固有的不确定性，我们使用极小极大准则评估遗憾。首先，我们推导了最坏情况期望简单遗憾的渐进下界，该下界由潜在结果的方差（主导因素）所确定。基于这些下界，我们提出了Two-Stage (TS)-Hirano-Imbens-Ridder (HIR)策略，在推荐最优臂时利用HIR估计（Hirano et al., 2003）。我们的理论分析表明，TS-HIR策略近似是最优的，并且在一定条件下能达到极小极大准则下的渐进最优性。

    We investigate the problem of fixed-budget best arm identification (BAI) for minimizing expected simple regret. In an adaptive experiment, a decision maker draws one of multiple treatment arms based on past observations and observes the outcome of the drawn arm. After the experiment, the decision maker recommends the treatment arm with the highest expected outcome. We evaluate the decision based on the expected simple regret, which is the difference between the expected outcomes of the best arm and the recommended arm. Due to inherent uncertainty, we evaluate the regret using the minimax criterion. First, we derive asymptotic lower bounds for the worst-case expected simple regret, which are characterized by the variances of potential outcomes (leading factor). Based on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder (HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) in recommending the best arm. Our theoretical analysis shows that the TS-HIR str
    
[^167]: 图解化：利用图解型AI解释对假设性演绎推理的理性化

    Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses. (arXiv:2302.01241v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.01241](http://arxiv.org/abs/2302.01241)

    本文提出了一种图解化的方法，以支持可解释的人工智能，通过图解型和假设性推理，缩小可解释性差距。通过临床应用研究和建模研究，我们发现DiagramNet不仅能提供忠实的杂音形状解释，还具有较好的预测性能，而且图解型解释在临床相关的情况下更受推崇。

    

    许多可解释的人工智能（XAI）可视化工具已经被开发出来，但它们通常需要用户进一步推理来解释。我们认为，XAI应该支持图解型和假设性推理，以便AI能够进行假设生成和评估，从而减少可解释性差距。我们提出了图解化方法，以i)进行Peircean推导-演绎推理，ii)遵循领域惯例，和iii)用图示或语言进行解释。我们在临床应用领域实现了DiagramNet，以预测心脏听诊中的心脏诊断，并用基于形状的杂音图解进行解释。在建模研究中，我们发现DiagramNet不仅提供了忠实的杂音形状解释，而且比基线模型具有更好的预测性能。我们进一步通过医学生的定性用户研究展示了图解型解释的可理解性和可信度，并表明在临床相关的情况下，图解式解释比其他方式更受推崇。

    Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. We argue that XAI should support diagrammatic and abductive reasoning for the AI to perform hypothesis generation and evaluation to reduce the interpretability gap. We propose Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii) follow domain conventions, and iii) explain with diagrams visually or verbally. We implemented DiagramNet for a clinical application to predict cardiac diagnoses from heart auscultation, and explain with shape-based murmur diagrams. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better prediction performance than baseline models. We further demonstrate the interpretability and trustworthiness of diagrammatic explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred ov
    
[^168]: MonoFlow: 从Wasserstein梯度流的角度重新思考Divergence GANs

    MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows. (arXiv:2302.01075v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01075](http://arxiv.org/abs/2302.01075)

    本文提出了一个新的GAN生成建模框架MonoFlow，通过Wasserstein梯度流获得理论洞见和算法启示。该框架使用密度比例的单调递增映射重新缩放粒子演化，并通过训练鉴别器获得MonoFlow的向量场，利用相应的向量场进行粒子流的生成。

    

    传统上，生成对抗网络（GANs）的对抗训练是通过判别器来估计离散度，生成器学习最小化这个离散度。我们认为，尽管许多GANs变体都是按照这个范例开发的，但当前GANs的理论理解和实际算法是不一致的。在本文中，通过利用展示了样本空间内粒子演化的Wasserstein梯度流来获得GANs的理论洞见和算法启示，我们介绍了一个统一的生成建模框架MonoFlow：粒子演化通过密度比例的单调递增映射进行重新缩放。在我们的框架下，对抗性训练可以被视为一个过程，首先通过训练鉴别器获得MonoFlow的向量场，然后生成器学习由相应向量场所定义的粒子流。

    The conventional understanding of adversarial training in generative adversarial networks (GANs) is that the discriminator is trained to estimate a divergence, and the generator learns to minimize this divergence. We argue that despite the fact that many variants of GANs were developed following this paradigm, the current theoretical understanding of GANs and their practical algorithms are inconsistent. In this paper, we leverage Wasserstein gradient flows which characterize the evolution of particles in the sample space, to gain theoretical insights and algorithmic inspiration of GANs. We introduce a unified generative modeling framework - MonoFlow: the particle evolution is rescaled via a monotonically increasing mapping of the log density ratio. Under our framework, adversarial training can be viewed as a procedure first obtaining MonoFlow's vector field via training the discriminator and the generator learns to draw the particle flow defined by the corresponding vector field. We al
    
[^169]: 使用相对熵边界研究各向异性Langevin动力学的隐私风险

    Privacy Risk for anisotropic Langevin dynamics using relative entropy bounds. (arXiv:2302.00766v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00766](http://arxiv.org/abs/2302.00766)

    本文通过建立相对熵边界，利用Fokker-Planck方程的稳定性估计结果，研究了各向异性噪声情况下的隐私风险。

    

    针对具有加性各向同性噪声的Langevin动力学的隐私保护特性已经得到了广泛的研究。然而，各向同性噪声假设非常严格：（a）在为了保护隐私并保持最佳准确性而向现有学习算法添加噪声时，应考虑输出的相对幅度和它们之间的相关性；（b）流行的算法，如随机梯度下降（及其连续时间极限），似乎具有各向异性的协方差特性。要研究各向异性噪声情况下的隐私风险，需要关于具有不同漂移和扩散系数的两个随机微分方程之间的相对熵的一般结果。我们的主要贡献是通过功能不等式利用对Fokker-Planck方程的解的稳定性估计来建立这样的边界。在附加假设的情况下，相对熵边界意味着一个$(\epsilon,\delta)$-differential的风险。

    The privacy preserving properties of Langevin dynamics with additive isotropic noise have been extensively studied. However, the isotropic noise assumption is very restrictive: (a) when adding noise to existing learning algorithms to preserve privacy and maintain the best possible accuracy one should take into account the relative magnitude of the outputs and their correlations; (b) popular algorithms such as stochastic gradient descent (and their continuous time limits) appear to possess anisotropic covariance properties. To study the privacy risks for the anisotropic noise case, one requires general results on the relative entropy between the laws of two Stochastic Differential Equations with different drifts and diffusion coefficients. Our main contribution is to establish such a bound using stability estimates for solutions to the Fokker-Planck equations via functional inequalities. With additional assumptions, the relative entropy bound implies an $(\epsilon,\delta)$-differential 
    
[^170]: 使用外部分布检测的插件估计器进行有选择性分类

    Plugin estimators for selective classification with out-of-distribution detection. (arXiv:2301.12386v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12386](http://arxiv.org/abs/2301.12386)

    这篇论文提出了一种新的插件估计器方法，用于有选择性分类和外部分布检测（SCOD）问题，并且在理论上有基础，有效性高，可以推广现有方法。

    

    真实世界的分类器可以受益于在对置信度较低的样本进行预测时选择性地放弃。这种弃权对于接近学习决策边界的样本或者相对于训练样本来说是异常值的样本特别有用。这些设置已经在选择性分类(SC)和外部分布(OOD)检测的领域中被广泛但不关联的研究所研究。最近有关选择性分类与外部分布检测(SCOD)的研究认为这两个问题的统一研究是有必要的；然而，对于这个问题的正式基础仍然不成熟，现有的技术也都是启发式的。在本文中，我们提出了一种新的SCOD的插件估计器，它们在理论上是有根据的、有效的，并且泛化了SC和OOD检测领域的现有方法。在我们的分析过程中，我们正式说明了对现有的SC和OOD检测基线的天真使用可能不足以适应SCOD。

    Real-world classifiers can benefit from the option of abstaining from predicting on samples where they have low confidence. Such abstention is particularly useful on samples which are close to the learned decision boundary, or which are outliers with respect to the training sample. These settings have been the subject of extensive but disjoint study in the selective classification (SC) and out-of-distribution (OOD) detection literature. Recent work on selective classification with OOD detection (SCOD) has argued for the unified study of these problems; however, the formal underpinnings of this problem are still nascent, and existing techniques are heuristic in nature. In this paper, we propose new plugin estimators for SCOD that are theoretically grounded, effective, and generalise existing approaches from the SC and OOD detection literature. In the course of our analysis, we formally explicate how na\"{i}ve use of existing SC and OOD detection baselines may be inadequate for SCOD. We 
    
[^171]: 比较贝叶斯层次模型的深度学习方法

    A Deep Learning Method for Comparing Bayesian Hierarchical Models. (arXiv:2301.11873v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11873](http://arxiv.org/abs/2301.11873)

    这个论文提出了一种深度学习方法，用于比较贝叶斯层次模型。该方法通过支持分摊推断，能够高效地进行模型比较和性能验证。同时，作者还对四个层次证据积累模型进行了比较。

    

    贝叶斯模型比较（BMC）提供了一种基于原则的方法来评估竞争计算模型的相对优势，并将不确定性传播到模型选择决策中。然而，由于高维嵌套参数结构，BMC在常见的层次模型中常常难以计算。为了解决这个难题，我们提出了一种深度学习方法，用于对任何可实例化为概率程序的层次模型集进行BMC。由于我们的方法支持分摊推断，它可以在任何实际数据应用之前，对后验模型概率进行高效的重新估计和快速性能验证。在一系列广泛的验证研究中，我们对比了我们的方法与最先进的桥式抽样方法的性能，并展示了在所有BMC设置中出色的分摊推断能力。然后，我们展示了我们的方法，通过比较先前被认为是四个层次证据积累模型。

    Bayesian model comparison (BMC) offers a principled approach for assessing the relative merits of competing computational models and propagating uncertainty into model selection decisions. However, BMC is often intractable for the popular class of hierarchical models due to their high-dimensional nested parameter structure. To address this intractability, we propose a deep learning method for performing BMC on any set of hierarchical models which can be instantiated as probabilistic programs. Since our method enables amortized inference, it allows efficient re-estimation of posterior model probabilities and fast performance validation prior to any real-data application. In a series of extensive validation studies, we benchmark the performance of our method against the state-of-the-art bridge sampling method and demonstrate excellent amortized inference across all BMC settings. We then showcase our method by comparing four hierarchical evidence accumulation models that have previously b
    
[^172]: 对莎士比亚戏剧的连续分析的数据科学和机器学习方法

    A data science and machine learning approach to continuous analysis of Shakespeare's plays. (arXiv:2301.06024v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.06024](http://arxiv.org/abs/2301.06024)

    通过数据科学和机器学习的方法，对莎士比亚的作品进行了连续分析，发现他的写作风格随着时间的推移发生了变化，其中包括句子长度、形容词和副词的频率以及文本中表达的情感。此外，研究还发现，部分戏剧的风格特征更类似于其写作时间之前或之后的作品。

    

    数量化的文本分析方法的可行性为文学分析提供了新的途径。本文将综合机器学习分析应用于威廉·莎士比亚的作品。分析结果显示，莎士比亚的写作风格随着时间的推移发生了明显的变化，其中最显著的变化包括句子长度、形容词和副词的频率以及文本中表达的情感。将机器学习应用于对戏剧年份的风格预测表明，实际年份和预测年份之间的皮尔逊相关系数为0.71，表明莎士比亚的写作风格在数量化测量方面随时间变化。此外，研究还发现，某些戏剧的风格特征更类似于它们的写作年份之前或之后的作品。例如，"罗密欧与朱丽叶"的日期为1596年，但在风格特征上更类似于1600年之后莎士比亚的其他作品。

    The availability of quantitative text analysis methods has provided new ways of analyzing literature in a manner that was not available in the pre-information era. Here we apply comprehensive machine learning analysis to the work of William Shakespeare. The analysis shows clear changes in the style of writing over time, with the most significant changes in the sentence length, frequency of adjectives and adverbs, and the sentiments expressed in the text. Applying machine learning to make a stylometric prediction of the year of the play shows a Pearson correlation of 0.71 between the actual and predicted year, indicating that Shakespeare's writing style as reflected by the quantitative measurements changed over time. Additionally, it shows that the stylometrics of some of the plays is more similar to plays written either before or after the year they were written. For instance, Romeo and Juliet is dated 1596, but is more similar in stylometrics to plays written by Shakespeare after 1600
    
[^173]: 利用标签平滑实现领域内外文本对抗鲁棒性

    In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10258](http://arxiv.org/abs/2212.10258)

    本文研究了标签平滑策略在不同领域的自然语言处理任务中对对抗鲁棒性的影响。实验证明，标签平滑显著提高了预训练模型的鲁棒性，并减少了在对抗样本上出现的过度自信错误。

    

    最近研究表明，最新的自然语言处理模型容易受到对抗性攻击，即对输入进行细微修改（如同义词替换）会极大地改变模型的预测结果。虽然已经提出了几种针对文本对抗性攻击的防御技术，并将其调整至离散性质的文本数据上，但是综合性的规则化方法，如语言模型的标签平滑对于文本模型的鲁棒性提供的效果还没有被研究过。在本文中，我们研究了各种标签平滑策略在领域内和领域外的基础模型中对多样化自然语言处理任务的对抗鲁棒性。我们的实验证明，标签平滑显著提高了预训练模型（如BERT）在各种流行攻击中的对抗鲁棒性。我们还分析了预测可信度与鲁棒性之间的关系，并显示标签平滑减少了在对抗样本上出现的过度自信错误。

    Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by various label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.
    
[^174]: 开发风力涡轮机条件信息的船队共享，通过隐私保护联邦学习

    Towards Fleet-wide Sharing of Wind Turbine Condition Information through Privacy-preserving Federated Learning. (arXiv:2212.03529v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03529](http://arxiv.org/abs/2212.03529)

    本文提出了一种分布式联邦机器学习方法，通过数据隐私保护，启用风力涡轮机队本地数据的船队范围学习，解决风力涡轮机制造商数据隐私的问题，提供改进数据驱动的涡轮机运维策略并减少停机时间的机会。

    

    风力涡轮机制造商每天从自己的船队中收集了数千兆字节的数据。这些数据包含有价值的实时信息，用于涡轮机健康诊断、性能监测、预测罕见故障和关键部件的剩余使用寿命。尽管如此，风力涡轮机队的这些数据财富因制造商出于商业战略原因而无法被操作者、公用事业公司和研究人员访问。数据访问的缺乏妨碍了利用机会，如改进数据驱动的涡轮机运维策略并减少停机时间。我们提出了一种分布式联邦机器学习方法，将数据留在风力涡轮机上以保持数据隐私，并仍然在本地数据上实现船队范围的学习。我们在两个案例研究中证明了风力涡轮机的稀缺代表性培训数据。

    Terabytes of data are collected every day by wind turbine manufacturers from their fleets. The data contain valuable real-time information for turbine health diagnostics and performance monitoring, for predicting rare failures and the remaining service life of critical parts. And yet, this wealth of data from wind turbine fleets remains inaccessible to operators, utility companies, and researchers as manufacturing companies prefer the privacy of their fleets' turbine data for business strategic reasons. The lack of data access impedes the exploitation of opportunities, such as improving data-driven turbine operation and maintenance strategies and reducing downtimes. We present a distributed federated machine learning approach that leaves the data on the wind turbines to preserve the data privacy, as desired by manufacturers, while still enabling fleet-wide learning on those local data. We demonstrate in two case studies that wind turbines which are scarce in representative training dat
    
[^175]: 用梯度分割方法缩小SVRG与TD-SVRG之间的差距

    Closing the gap between SVRG and TD-SVRG with Gradient Splitting. (arXiv:2211.16237v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16237](http://arxiv.org/abs/2211.16237)

    本论文通过将TD学习视为适当选择函数的梯度分割，将TD和SVRG相结合，实现了具有几何收敛速度的策略评估方法，并在理论和实验上得到了支持。

    

    TD（时序差分）学习是一种增强学习中的策略评估方法，其性能可以通过方差缩减技术进行增强。最近，多个工作尝试将TD学习与SVRG相结合，以获得一种具有几何收敛速度的策略评估方法。然而，在凸优化设置下，所得到的收敛速度明显不及SVRG。本研究利用最近对TD学习的解释，将其视为一个适当选择函数的梯度的分割，从而简化了算法，并将TD与SVRG相结合。我们的主要结果是一个具有预定学习速率为1/8的几何收敛界限，与凸设置下SVRG的收敛界限相同。我们的理论发现得到了一系列实验证明。

    Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose performance can be enhanced by variance reduction techniques. Recently, multiple works have sought to fuse TD learning with SVRG to obtain a policy evaluation method with a geometric rate of convergence. However, the resulting convergence rate is significantly weaker than what is achieved by SVRG in the setting of convex optimization. In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient of an appropriately chosen function, thus simplifying the algorithm and fusing TD with SVRG. Our main result is a geometric convergence bound with predetermined learning rate of $1/8$, which is identical to the convergence bound available for SVRG in the convex setting. Our theoretical findings are supported by a set of experiments.
    
[^176]: 单个分子基础模型的双向结构和性质的生成

    Bidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model. (arXiv:2211.10590v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10590](http://arxiv.org/abs/2211.10590)

    本论文提出了一种新颖的多模态分子预训练模型，通过将结构和生化性质的模态结合，使得模型能够将分子的结构和性质之间的双向信息联系起来，并能够处理多模态和单模态的下游任务。

    

    人工智能中大型基础模型的成功促使了化学预训练模型的出现。尽管近年来对提供下游任务的信息表示的大分子预训练模型的兴趣日益增长，但分子领域的多模态预训练方法尝试有限。为了解决这个问题，我们提出了一种新颖的多模态分子预训练模型，从多模态学习技术的最新进展中汲取灵感，将结构和生化性质的模态结合起来。我们提出的模型管道处理数据并根据共同的嵌入空间对齐结构/性质特征，使得模型能够将分子的结构和性质之间的双向信息联系起来。这些贡献产生了协同的知识，使我们能够通过单一模型处理多模态和单模态的下游任务。通过大量实验证明，我们的模型足以生成具有双向结构和性质的分子。

    The recent success of large foundation models in artificial intelligence has prompted the emergence of chemical pre-trained models. Despite the growing interest in large molecular pre-trained models that provide informative representations for downstream tasks, attempts for multimodal pre-training approaches on the molecule domain were limited. To address this, we present a novel multimodal molecular pre-trained model that incorporates the modalities of structure and biochemical properties, drawing inspiration from recent advances in multimodal learning techniques. Our proposed model pipeline of data handling and training objectives aligns the structure/property features in a common embedding space, which enables the model to regard bidirectional information between the molecules' structure and properties. These contributions emerge synergistic knowledge, allowing us to tackle both multimodal and unimodal downstream tasks through a single model. Through extensive experiments, we demons
    
[^177]: 图像对机器的记忆性有何影响？

    What Images are More Memorable to Machines?. (arXiv:2211.07625v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.07625](http://arxiv.org/abs/2211.07625)

    本论文研究了如何测量和预测图像对机器的记忆性，并发现“复杂”图像通常对机器记忆更加深刻。通过详细分析和实验验证，我们提出了机器记忆性的概念，并为机器记忆和视觉数据的研究方向开辟了新的可能性。

    

    本论文研究了如何测量和预测图像对模式识别机器的记忆性，以探索机器智能。首先，我们提出了一个自监督的机器记忆量化流程，称为“MachineMem measurer”，用于收集图像的机器记忆性分数。与人类类似，机器也倾向于记忆某些类型的图像，但机器和人类记忆的图像类型是不同的。通过深入分析和全面的可视化，我们逐渐揭示了“复杂”图像通常对机器记忆更加深刻。我们进一步在11台不同的机器（从线性分类器到现代ViTs）和9种预训练方法上进行了广泛实验，以分析和理解机器记忆。本研究提出了机器记忆性的概念，并在机器记忆和视觉数据的交叉界面上开辟了一条新的研究方向。

    This paper studies the problem of measuring and predicting how memorable an image is to pattern recognition machines, as a path to explore machine intelligence. Firstly, we propose a self-supervised machine memory quantification pipeline, dubbed ``MachineMem measurer'', to collect machine memorability scores of images. Similar to humans, machines also tend to memorize certain kinds of images, whereas the types of images that machines and humans memorize are different. Through in-depth analysis and comprehensive visualizations, we gradually unveil that``complex" images are usually more memorable to machines. We further conduct extensive experiments across 11 different machines (from linear classifiers to modern ViTs) and 9 pre-training methods to analyze and understand machine memory. This work proposes the concept of machine memorability and opens a new research direction at the interface between machine memory and visual data.
    
[^178]: 机器学习时代MRI数据协调的有效性：一项跨36个数据集的多中心研究

    Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets. (arXiv:2211.04125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04125](http://arxiv.org/abs/2211.04125)

    本文描述了一项多中心研究，旨在评估MRI数据协调在机器学习中的效用;作者提出了一种“调和器变压器”方法，在不泄露信息的前提下，在机器学习的预处理步骤中实现了数据协调。

    

    从多个网站汇集公开可用的MRI数据可以组装大量受试对象，增加统计功率，并通过机器学习技术促进数据重用。多中心数据的协调是减少数据中与非生物来源的变异度量的混杂效应的必要条件。然而，将协调应用于机器学习之前的整个数据集会导致数据泄漏，因为训练集之外的信息可能会影响模型构建并可能导致性能过高。我们提出了1）数据协调的有效性测量方法；2）“调和器变压器”，即ComBat协调方法的一个实现，它允许将其封装在机器学习的预处理步骤中，避免数据泄漏。我们使用了来自36个网站的1740名健康受试者的大脑T1加权MRI数据来测试这些工具。经过协调后，网站效应被删除或减少了，

    Pooling publicly-available MRI data from multiple sites allows to assemble extensive groups of subjects, increase statistical power, and promote data reuse with machine learning techniques. The harmonization of multicenter data is necessary to reduce the confounding effect associated with non-biological sources of variability in the data. However, when applied to the entire dataset before machine learning, the harmonization leads to data leakage, because information outside the training set may affect model building, and potentially falsely overestimate performance. We propose a 1) measurement of the efficacy of data harmonization; 2) harmonizer transformer, i.e., an implementation of the ComBat harmonization allowing its encapsulation among the preprocessing steps of a machine learning pipeline, avoiding data leakage. We tested these tools using brain T1-weighted MRI data from 1740 healthy subjects acquired at 36 sites. After harmonization, the site effect was removed or reduced, and 
    
[^179]: 超越IoCs：自动从外部CTI中提取攻击模式

    Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI. (arXiv:2211.01753v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.01753](http://arxiv.org/abs/2211.01753)

    本论文介绍了LADDER，这是一个知识提取框架，可以从网络威胁情报中提取攻击模式。该框架可以帮助安全分析师及时确定现有和新兴威胁的攻击向量，从而使他们能够主动准备防御措施。

    

    公共和商业组织广泛共享网络威胁情报(CTI)以防范现有和新兴的网络攻击。然而，传统的CTI主要集中在追踪已知的威胁指标（如IP地址和域名），可能在防御不断演变的攻击方面提供长期价值。为了应对这一挑战，我们提出使用更强大的威胁情报信号，称为攻击模式。LADDER是一个知识提取框架，可以从CTI报告中大规模提取基于文本的攻击模式。该框架通过捕捉Android和企业网络中攻击的各个阶段，并将其系统地映射到MITRE ATT\&CK模式框架，来对攻击模式进行表征。LADDER可以被安全分析师用来确定与现有和新兴威胁相关的攻击向量的存在，使他们能够主动准备防御措施。我们还提供了几个使用案例来演示其应用。

    Public and commercial organizations extensively share cyberthreat intelligence (CTI) to prepare systems to defend against existing and emerging cyberattacks. However, traditional CTI has primarily focused on tracking known threat indicators such as IP addresses and domain names, which may not provide long-term value in defending against evolving attacks. To address this challenge, we propose to use more robust threat intelligence signals called attack patterns. LADDER is a knowledge extraction framework that can extract text-based attack patterns from CTI reports at scale. The framework characterizes attack patterns by capturing the phases of an attack in Android and enterprise networks and systematically maps them to the MITRE ATT\&CK pattern framework. LADDER can be used by security analysts to determine the presence of attack vectors related to existing and emerging threats, enabling them to prepare defenses proactively. We also present several use cases to demonstrate the applicati
    
[^180]: 利用解释的力量进行增量训练：基于LIME的方法

    Harnessing the Power of Explanations for Incremental Training: A LIME-Based Approach. (arXiv:2211.01413v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01413](http://arxiv.org/abs/2211.01413)

    本研究提出了一种基于LIME的方法，利用解释来改善神经网络模型性能。通过引入自定义加权损失，将真实LIME解释与模型预测的LIME解释之间的距离考虑在内，帮助模型更好地泛化。此外，该研究还提出了一种适用于实际训练场景的解决方案，以帮助模型顺序学习而不丢失先前数据分布信息。

    

    神经网络预测的可解释性对于理解特征重要性并获得可解释的洞察力至关重要。然而，对神经网络结果的解释大多局限于可视化，并且很少有研究将这些解释用作改进模型性能的反馈。本研究将模型的解释反馈到前馈训练中，以帮助模型更好地泛化。为此，提出了一种自定义加权损失，其中权重是通过考虑真实LIME（局部可解释的模型无关解释）解释和模型预测的LIME解释之间的欧式距离来生成的。此外，在实际的训练场景中，开发一个能够帮助模型顺序学习而不丢失先前数据分布信息的解决方案是必要的，因为不会一次性提供所有的训练数据。因此，该框架将自定义加权损失与弹性训练相结合。

    Explainability of neural network prediction is essential to understand feature importance and gain interpretable insight into neural network performance. However, explanations of neural network outcomes are mostly limited to visualization, and there is scarce work that looks to use these explanations as feedback to improve model performance. In this work, model explanations are fed back to the feed-forward training to help the model generalize better. To this extent, a custom weighted loss where the weights are generated by considering the Euclidean distances between true LIME (Local Interpretable Model-Agnostic Explanations) explanations and model-predicted LIME explanations is proposed. Also, in practical training scenarios, developing a solution that can help the model learn sequentially without losing information on previous data distribution is imperative due to the unavailability of all the training data at once. Thus, the framework incorporates the custom weighted loss with Elas
    
[^181]: 对比解码：将开放式文本生成视为优化问题

    Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15097](http://arxiv.org/abs/2210.15097)

    对比解码是一种可靠的解码方法，它通过优化对比目标来生成合理且高质量的文本，与仅使用较大的语言模型进行解码相比，对比解码能够避免短而重复的文本和不连贯的文本，并适用于不同的模型规模。

    

    鉴于语言模型（LM），最大概率是开放式生成的较差解码目标，因为它会产生短而重复的文本。另一方面，采样往往会产生与原始主题偏离的不连贯文本。我们提出了对比解码（CD），这是一种可靠的解码方法，它在满足合理性约束条件的前提下优化对比目标。对比目标返回一个大型LM（被称为专家，例如OPT-13B）和一个小型LM（被称为业余者，例如OPT-125M）之间的似然差异，并且约束条件确保输出是合理的。CD的灵感来自于这样一个事实，即较大的LM（例如重复、不连贯）在较小的LM中更为普遍，并且这种差异表明哪些文本应优先考虑。CD不需要额外的培训，并且比仅从较大的LM进行解码的情况下产生更高质量的文本。它还适用于不同的模型规模（OPT-13B和GPT2-1.5B）。

    Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and 
    
[^182]: 具有利普希茨非线性的单个神经元模型的主动学习

    Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13601](http://arxiv.org/abs/2210.13601)

    该论文提出一种针对具有利普希茨非线性的单个神经元模型的主动学习策略，该策略在敌对标签噪声下拟合线性函数，并在逼近保证方面具有强有力的可证明性能。

    

    我们考虑在敌对标签噪声下的对单个神经元模型进行主动学习的问题，有时也被称为“岭函数”。这些模型已被证明广泛有效地模拟物理现象，并构建用于偏微分方程的代理数据驱动模型。令人惊讶的是，我们证明对于具有任何利普希茨非线性（如ReLU函数、sigmoid函数、绝对值函数、低次多项式函数等）的单个神经元模型，可以使用已知的在敌对标签噪声下拟合“线性函数”的主动学习策略获得强有力的可证明的逼近保证。

    We consider the problem of active learning for single neuron models, also sometimes called ``ridge functions'', in the agnostic setting (under adversarial label noise). Such models have been shown to be broadly effective in modeling physical phenomena, and for constructing surrogate data-driven models for partial differential equations.  Surprisingly, we show that for a single neuron model with any Lipschitz non-linearity (such as the ReLU, sigmoid, absolute value, low-degree polynomial, among others), strong provable approximation guarantees can be obtained using a well-known active learning strategy for fitting \emph{linear functions} in the agnostic setting. % -- i.e. for the case when there is no non-linearity. Namely, we can collect samples via statistical \emph{leverage score sampling}, which has been shown to be near-optimal in other active learning scenarios. We support our theoretical results with empirical simulations showing that our proposed active learning strategy based o
    
[^183]: 通过迭代的自适应有监督对比学习进行多实例学习

    Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning. (arXiv:2210.09452v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09452](http://arxiv.org/abs/2210.09452)

    本论文提出了一个新的框架，通过迭代的自适应有监督对比学习，解决了多实例学习中的类别不平衡问题，并在医学数据集上取得了良好的结果。

    

    在只有袋级标签可用的情况下，学习单个实例的表示是多实例学习中的一个基本挑战。最近的研究表明，使用对比自我监督学习（CSSL）可以取得有希望的结果，CSSL学习将对应于两个不同随机选择的实例的表示推开。然而，在真实世界的应用中，例如医学图像分类，通常存在类别不平衡，因此随机选择的实例大部分属于同一个多数类别，这使得CSSL无法学习类间差异。为了解决这个问题，我们提出了一个新的框架，称为迭代自适应有监督对比学习多实例表示（ItS2CLR），通过利用从包级标签派生的实例级伪标签来改善学习到的表示。该框架采用了一种新的自适应有监督对比学习多实例表示的策略来确保伪标签的准确性。我们在三个医学数据集上评估了ItS2CLR。

    Learning representations for individual instances when only bag-level labels are available is a fundamental challenge in multiple instance learning (MIL). Recent works have shown promising results using contrastive self-supervised learning (CSSL), which learns to push apart representations corresponding to two different randomly-selected instances. Unfortunately, in real-world applications such as medical image classification, there is often class imbalance, so randomly-selected instances mostly belong to the same majority class, which precludes CSSL from learning inter-class differences. To address this issue, we propose a novel framework, Iterative Self-paced Supervised Contrastive Learning for MIL Representations (ItS2CLR), which improves the learned representation by exploiting instance-level pseudo labels derived from the bag-level labels. The framework employs a novel self-paced sampling strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three medical datase
    
[^184]: 分布自适应元强化学习

    Distributionally Adaptive Meta Reinforcement Learning. (arXiv:2210.03104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03104](http://arxiv.org/abs/2210.03104)

    本论文介绍了一种基于自适应分布鲁棒性的元强化学习算法框架，该框架能够在测试任务分布发生变化时行为适应，提供了对分布转变的改善遗憾的能力。

    

    元强化学习算法提供了一种数据驱动的方式来获取能够快速适应多个具有不同奖励或动力学函数的任务的策略。然而，学习得到的元策略通常只在其训练时的精确任务分布上有效，在测试时的奖励或过渡动力学的分布发生变化时会变得困难。在这项工作中，我们开发了一个框架，用于元强化学习算法在任务空间中适应性地应对测试时的分布转变。我们的框架以自适应的分布鲁棒性方法为核心，训练一群具有不同程度分布转变鲁棒性的元策略。当在可能发生分布转变的测试任务分布上评估时，我们可以选择具有最合适鲁棒性水平的元策略，并使用它进行快速适应。我们在理论上证明了我们的框架在分布转变下能够改善遗憾，并通过实验证明了我们方法的有效性。

    Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirica
    
[^185]: 神经网络中的多义性和容量

    Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2210.01892](http://arxiv.org/abs/2210.01892)

    该论文通过分析特征容量来理解神经网络中的多义性现象，发现在最优的容量分配下，神经网络倾向于单义地表示重要特征，多义地表示次重要特征，并忽略最不重要的特征。多义性现象在输入具有更高的峰度或稀疏性时更为普遍，并且在某些体系结构中比其他体系结构更为普遍。此外，作者还发现了嵌入空间中的分块半正交结构，不同模型中的分块大小不同，突出了模型体系结构的影响。

    

    神经网络中的单个神经元通常代表无关特征的混合。这种现象称为多义性，使解释神经网络变得更加困难，因此我们的目标是理解其原因。我们提出通过特征容量的视角来理解这一现象，特征容量是每个特征在嵌入空间中占用的分形维度。我们展示了在一个玩具模型中，最优的容量分配倾向于单义地表示最重要的特征，多义地表示次重要特征（与其对损失的影响成比例），并完全忽略最不重要的特征。当输入具有更高的峰度或稀疏性时，多义性更为普遍，并且在某些体系结构中比其他体系结构更为普遍。在得到最优容量分配后，我们进一步研究了嵌入空间的几何结构。我们发现了一个分块半正交的结构，不同模型中的分块大小不同，突出了模型体系结构的影响。

    Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature \emph{capacity}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model archit
    
[^186]: 通过冗余性实现稀疏性：用SGD求解$L_1$

    Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01212](http://arxiv.org/abs/2210.01212)

    该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.

    我们提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法。我们的提议是$L_1$惩罚等价于带有权重衰减的可微重参数化的直接推广。我们证明了所提出的方法，即\textit{spred}，是$L_1$的精确求解器，并且对于通用的非凸函数，重参数化技巧是完全“良性”的。在实践中，我们展示了该方法的实用性，包括(1)训练稀疏神经网络以执行基因选择任务，其中涉及在非常高维空间中找到相关特征，以及(2)神经网络压缩任务，先前尝试应用$L_1$惩罚的方法均未成功。从概念上讲，我们的结果弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
    
[^187]: 无穷小梯度提升的大样本理论

    A large sample theory for infinitesimal gradient boosting. (arXiv:2210.00736v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.00736](http://arxiv.org/abs/2210.00736)

    本研究研究了无穷小梯度提升在大样本极限下的渐近性质，证明了其收敛到一个确定性过程，并探讨了其使得测试误差减小的动力学以及其长时间行为。

    

    无穷小梯度提升是机器学习中流行的基于树的梯度提升算法的消失学习率极限。它被定义为在无穷维函数空间中的非线性常微分方程的解，其中驱动动力学的无穷小提升算子依赖于训练样本。我们研究了模型在大样本极限下的渐近性质，并证明了其收敛到一个确定性过程。这个种群极限再次被一个依赖于种群分布的微分方程所描述。我们探讨了这个种群极限的一些性质：我们证明了动力学使得测试误差减小，并考虑了它在长时间行为上的表现。

    Infinitesimal gradient boosting (Dombry and Duchamps, 2021) is defined as the vanishing-learning-rate limit of the popular tree-based gradient boosting algorithm from machine learning. It is characterized as the solution of a nonlinear ordinary differential equation in a infinite-dimensional function space where the infinitesimal boosting operator driving the dynamics depends on the training sample. We consider the asymptotic behavior of the model in the large sample limit and prove its convergence to a deterministic process. This population limit is again characterized by a differential equation that depends on the population distribution. We explore some properties of this population limit: we prove that the dynamics makes the test error decrease and we consider its long time behavior.
    
[^188]: 绿色，量化的无线网络联合学习：一种节能设计

    Green, Quantized Federated Learning over Wireless Networks: An Energy-Efficient Design. (arXiv:2207.09387v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09387](http://arxiv.org/abs/2207.09387)

    本文提出了一种绿色量化的无线网络联合学习框架，通过使用量化神经网络来实现有限精度级别的数据表示。针对能量消耗和通信轮次数量的最小化，提出了一个多目标优化问题，并分析推导了系统的收敛速度。

    

    本文提出了一种绿色量化的无线网络联合学习框架，在本地训练和上行传输中使用有限精度级别来表示数据。这里，有限精度级别通过使用量化神经网络（QNNs）来实现，该网络以固定精度格式量化权重和激活函数。在考虑的联合学习模型中，每个设备训练其QNN并将量化的训练结果传输给基站。本文严格推导了本地训练和量化传输的能量模型。为了同时最小化能量消耗和通信轮次数量，本文制定了一个多目标优化问题，该问题涉及本地迭代次数、选定设备数量以及本地训练和传输的精度级别，并确保在目标准确度约束下的收敛性。为了解决这个问题，本文还分析推导了所提出的联合学习系统的收敛速度。

    In this paper, a green-quantized FL framework, which represents data with a finite precision level in both local training and uplink transmission, is proposed. Here, the finite precision level is captured through the use of quantized neural networks (QNNs) that quantize weights and activations in fixed-precision format. In the considered FL model, each device trains its QNN and transmits a quantized training result to the base station. Energy models for the local training and the transmission with quantization are rigorously derived. To minimize the energy consumption and the number of communication rounds simultaneously, a multi-objective optimization problem is formulated with respect to the number of local iterations, the number of selected devices, and the precision levels for both local training and transmission while ensuring convergence under a target accuracy constraint. To solve this problem, the convergence rate of the proposed FL system is analytically derived with respect t
    
[^189]: 在图像模型的盲点发现中朝着更加严谨的科学方法

    Towards a More Rigorous Science of Blindspot Discovery in Image Models. (arXiv:2207.04104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04104](http://arxiv.org/abs/2207.04104)

    本文介绍了一种新的BDM（盲点发现方法）评估框架SpotCheck和一个使用2D图像表示的BDMPlaneSpot。实验结果给出影响BDM性能的因素，证明PlaneSpot与现有的BDM相竞争，在许多情况下表现更好。

    

    越来越多的工作研究了盲点发现方法（“BDM”），这些方法使用图像嵌入来查找数据的语义有意义的子集，在这些子集中图像分类器表现显著更差（即存在盲点）。受到之前工作中观察到的差距的启发，我们介绍了一种新的BDM评估框架（SpotCheck），该框架使用合成图像数据集来训练具有已知盲点的模型，并且使用新的BDM（PlaneSpot）来使用2D图像表示。我们使用SpotCheck进行受控实验，以确定影响BDM性能的因素（例如模型中的盲点数量或定义盲点的特征），并表明PlaneSpot与现有的BDM相竞争，在许多情况下表现更好。重要的是，我们通过设计额外的实验来验证这些发现，使用MS-COCO的真实图像数据集进行实验。我们的发现为未来在BDM设计方面提出了几个有前途的方向。

    A growing body of work studies Blindspot Discovery Methods ("BDM"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM des
    
[^190]: 动态均场规划

    Dynamic mean field programming. (arXiv:2206.05200v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.05200](http://arxiv.org/abs/2206.05200)

    本文发展了一种动态均场规划方法，用于有限状态和行为的贝叶斯强化学习。通过模拟统计物理中的概念，研究了贝尔曼方程作为一种无序动力学系统，并通过均场方程计算状态行为值的统计信息。

    

    在大状态空间限制下，发展了一种动态均场理论，用于有限状态和行为的贝叶斯强化学习。类比于统计物理学，对贝尔曼方程进行了研究，将马尔科夫决策过程的转移概率解释为耦合，将值函数解释为动态演化的确定性自旋。因此，平均回报和转移概率被认为是淬灭随机变量。该理论揭示了在某些假设下，在渐近状态空间极限下，状态行为值在状态行为对之间具有统计独立性，并提供了确切的分布形式。这些结果适用于有限和无限折现时间视野，在价值迭代和策略评估中均成立。状态行为值的统计信息可以从一组均场方程中计算，我们称之为动态均场规划（DMFP）。对于策略评估，可以使用期望值迭代算法。

    A dynamic mean field theory is developed for finite state and action Bayesian reinforcement learning in the large state space limit. In an analogy with statistical physics, the Bellman equation is studied as a disordered dynamical system; the Markov decision process transition probabilities are interpreted as couplings and the value functions as deterministic spins that evolve dynamically. Thus, the mean-rewards and transition probabilities are considered to be quenched random variables. The theory reveals that, under certain assumptions, the state-action values are statistically independent across state-action pairs in the asymptotic state space limit, and provides the form of the distribution exactly. The results hold in the finite and discounted infinite horizon settings, for both value iteration and policy evaluation. The state-action value statistics can be computed from a set of mean field equations, which we call dynamic mean field programming (DMFP). For policy evaluation the e
    
[^191]: 自监督异常检测：综述与展望

    Self-Supervised Anomaly Detection: A Survey and Outlook. (arXiv:2205.05173v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.05173](http://arxiv.org/abs/2205.05173)

    自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文全面综述了当前自监督异常检测方法的技术细节，并讨论了它们的优势和缺点，同时比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。

    

    异常检测在网络安全、金融和医疗等各个领域中起到了至关重要的作用，通过识别偏离正常行为的模式或事件。近年来，深度学习模型的显著增长使得在异常检测领域取得了重大进展。值得注意的是，自监督学习的出现引发了新型异常检测算法的发展，其表现优于现有的最先进方法。本文旨在全面综述当前自监督异常检测方法的技术细节，并讨论它们的优势和缺点。我们还比较了这些模型与其他自监督异常检测模型以及最先进的异常检测模型的性能。最后，本文对自监督异常检测的未来方向进行了讨论，包括开发更加有效和高效的算法等等。

    Anomaly detection (AD) plays a crucial role in various domains, including cybersecurity, finance, and healthcare, by identifying patterns or events that deviate from normal behaviour. In recent years, significant progress has been made in this field due to the remarkable growth of deep learning models. Notably, the advent of self-supervised learning has sparked the development of novel AD algorithms that outperform the existing state-of-the-art approaches by a considerable margin. This paper aims to provide a comprehensive review of the current methodologies in self-supervised anomaly detection. We present technical details of the standard methods and discuss their strengths and drawbacks. We also compare the performance of these models against each other and other state-of-the-art anomaly detection models. Finally, the paper concludes with a discussion of future directions for self-supervised anomaly detection, including the development of more effective and efficient algorithms and t
    
[^192]: VAE-Loco：通过学习解缠踏步表示实现多功能四足动态步态

    VAE-Loco: Versatile Quadruped Locomotion by Learning a Disentangled Gait Representation. (arXiv:2205.01179v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.01179](http://arxiv.org/abs/2205.01179)

    本论文通过学习一个解缠的步态表示来解决四足机器人步态参数无法在空中变化的问题，实现了多样化的连续合成步态，增加了控制器的稳健性。

    

    四足动态步态正在快速发展，使得机器人能够实现高度动态的机动动作。然而，当前的规划器无法在空中改变摆动脚的关键步态参数。在这项工作中，我们解决了这个限制，并展示了通过学习捕捉构成特定步态的关键支撑阶段的潜在空间，可以增加控制器的稳健性。通过在单一驰步风格上训练的生成模型实现此目标，该模型鼓励解缠，使得对潜在状态的单一维度施加驱动信号能够综合连续多样化的驰步风格合成全面性计划。我们证明了驱动信号的特定属性直接映射到步态参数，如节奏、脚步高度和全阶段持续时间。由于我们方法的特点，这些合成的步态在机器人运行过程中可以在线持续变化。生成模型的使用有助于检测和缓解问题。

    Quadruped locomotion is rapidly maturing to a degree where robots are able to realise highly dynamic manoeuvres. However, current planners are unable to vary key gait parameters of the in-swing feet midair. In this work we address this limitation and show that it is pivotal in increasing controller robustness by learning a latent space capturing the key stance phases constituting a particular gait. This is achieved via a generative model trained on a single trot style, which encourages disentanglement such that application of a drive signal to a single dimension of the latent state induces holistic plans synthesising a continuous variety of trot styles. We demonstrate that specific properties of the drive signal map directly to gait parameters such as cadence, footstep height and full stance duration. Due to the nature of our approach these synthesised gaits are continuously variable online during robot operation. The use of a generative model facilitates the detection and mitigation o
    
[^193]: 使用小型量子计算机进行高维量子机器学习

    High Dimensional Quantum Machine Learning With Small Quantum Computers. (arXiv:2203.13739v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.13739](http://arxiv.org/abs/2203.13739)

    本研究探索了使用小型量子计算机进行高维量子机器学习的可能性，并且通过构建一个机器学习模型成功地在数字识别任务中实现了利用更少的电路评估来近似较大电路的输出。

    

    量子计算机对于增强机器学习具有巨大潜力，但目前的量子比特数量限制了这一潜力的实现。为了缓解这个限制，可以采用一种技术，在比电路所需的比特少的机器上评估量子电路。这些技术通过在较小的机器上评估许多较小的电路，然后将它们组合成一个多项式来复制较大机器的输出。这种方案对于一般电路而言需要更多的电路评估，这是不切实际的。然而，我们探讨了一种可能性，即对于某些应用来说，许多这些子电路是多余的，并且一个更小的求和足以估计完整的电路。我们构建了一个机器学习模型，可以利用更少的电路评估来逼近较大电路的输出。我们成功地将该模型应用于数字识别任务，使用模拟的量子计算机。

    Quantum computers hold great promise to enhance machine learning, but their current qubit counts restrict the realisation of this promise. In an attempt to placate this limitation techniques can be applied for evaluating a quantum circuit using a machine with fewer qubits than the circuit naively requires. These techniques work by evaluating many smaller circuits on the smaller machine, that are then combined in a polynomial to replicate the output of the larger machine. This scheme requires more circuit evaluations than are practical for general circuits. However, we investigate the possibility that for certain applications many of these subcircuits are superfluous, and that a much smaller sum is sufficient to estimate the full circuit. We construct a machine learning model that may be capable of approximating the outputs of the larger circuit with much fewer circuit evaluations. We successfully apply our model to the task of digit recognition, using simulated quantum computers much s
    
[^194]: 基于传感器的机器人控制的基本限制

    Fundamental Limits for Sensor-Based Robot Control. (arXiv:2202.00129v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2202.00129](http://arxiv.org/abs/2202.00129)

    该论文提出了一个新的方法，利用信息论和动态规划技术，为基于传感器的机器人控制建立了性能的基本限制。通过计算任务相关信息的数量，可以获得一步决策任务的最高可实现预期报酬的上界，并通过动态规划将这个上界扩展到多步问题。实验证明了该方法在三个示例上的有效性，包括部分可观察性决策过程、自由落体物体的捕捉和使用深度传感器的避障问题。

    

    我们的目标是为给定任务建立机器人传感器性能的基本限制的理论和算法。为了实现这一目标，我们定义了一个捕捉传感器提供的与任务相关信息数量的量。利用信息论中广义Fano不等式的新颖版本，我们证明了这个量提供了一步决策任务的最高可实现预期报酬的上界。然后，我们通过动态规划方法将这个上界扩展到多步问题。我们提出了用于数值计算得出的上界的算法，并在三个示例上演示了我们的方法：（i）来自部分可观察性马尔可夫决策过程文献的熔岩问题，（ii）具有连续状态和观测空间的示例，对应于机器人接住自由落体物体的问题，以及（iii）使用非高斯噪声深度传感器的避障问题。我们展示了算法的能力

    Our goal is to develop theory and algorithms for establishing fundamental limits on performance imposed by a robot's sensors for a given task. In order to achieve this, we define a quantity that captures the amount of task-relevant information provided by a sensor. Using a novel version of the generalized Fano inequality from information theory, we demonstrate that this quantity provides an upper bound on the highest achievable expected reward for one-step decision making tasks. We then extend this bound to multi-step problems via a dynamic programming approach. We present algorithms for numerically computing the resulting bounds, and demonstrate our approach on three examples: (i) the lava problem from the literature on partially observable Markov decision processes, (ii) an example with continuous state and observation spaces corresponding to a robot catching a freely-falling object, and (iii) obstacle avoidance using a depth sensor with non-Gaussian noise. We demonstrate the ability
    
[^195]: 不同度量下1-中心问题的复杂性研究

    On Complexity of 1-Center in Various Metrics. (arXiv:2112.03222v2 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2112.03222](http://arxiv.org/abs/2112.03222)

    在本文中，研究了不同度量下1-中心问题的复杂性。小d时，在假设命中集合猜想（HSC）成立的情况下，无法通过任何lp度量或编辑或Ulam度量实现1-中心问题的次二次算法。大d时，在假设Quantified SETH的情况下，排除了基于编辑度量中的1-中心问题的次四次算法，同时给出了Ulam度量中1-中心问题的（1+ε）逼近算法。

    

    本文研究了经典的1-中心问题：给定度量空间中的一组n个点P，找到距离其他P中点距离最大的点。我们研究了在d维lp度量中以及基于字符串长度为d的编辑和Ulam度量中该问题的复杂性。对于1-中心问题，我们的研究结果可以按d进行分类。小d时，在假设命中集合猜想（HSC）成立的情况下，我们证明了当d=ω（logn）时，无法通过任何lp度量或编辑或Ulam度量实现1-中心问题的次二次算法。大d时，我们将有条件的下界推广到基于编辑度量中的1-中心问题，排除了次四次算法（假设Quantified SETH）。另一方面，我们给出了Ulam度量中1-中心问题的（1+ε）逼近算法，其运行时间为$\tilde{O_{\varepsilon}}(nd+n^2\sqrt{d})$。我们还通过允许近似来加强了一些之前的下界。

    We consider the classic 1-center problem: Given a set $P$ of $n$ points in a metric space find the point in $P$ that minimizes the maximum distance to the other points of $P$. We study the complexity of this problem in $d$-dimensional $\ell_p$-metrics and in edit and Ulam metrics over strings of length $d$. Our results for the 1-center problem may be classified based on $d$ as follows.  $\bullet$ Small $d$: Assuming the hitting set conjecture (HSC), we show that when $d=\omega(\log n)$, no subquadratic algorithm can solve 1-center problem in any of the $\ell_p$-metrics, or in edit or Ulam metrics.  $\bullet$ Large $d$: When $d=\Omega(n)$, we extend our conditional lower bound to rule out subquartic algorithms for 1-center problem in edit metric (assuming Quantified SETH). On the other hand, we give a $(1+\epsilon)$-approximation for 1-center in Ulam metric with running time $\tilde{O_{\varepsilon}}(nd+n^2\sqrt{d})$.  We also strengthen some of the above lower bounds by allowing approxi
    
[^196]: 以全生命周期行为建模赋能通用用户表示

    Empowering General-purpose User Representation with Full-life Cycle Behavior Modeling. (arXiv:2110.11337v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.11337](http://arxiv.org/abs/2110.11337)

    本研究提出了一种名为全生命周期用户表示模型（LURM）的新框架，能够对超长行为序列进行全生命周期建模。该模型由兴趣包编码和自监督多锚点编码网络组成，能够生成适用于各种下游用户认知任务的通用表示。

    

    用户建模在行业中起着重要作用。在这一领域中，相对于特定任务的表示学习，能够生成适用于各种下游用户认知任务的通用表示的任务无关方法，具有更高的价值和经济性。随着互联网服务平台的快速发展，用户行为不断积累。然而，现有的通用用户表示研究对自用户注册以来的极长行为序列的全生命周期建模能力较弱。在本研究中，我们提出了一种称为全生命周期用户表示模型（LURM）的新框架来解决这一挑战。具体而言，LURM由两个级联的子模型组成：（I）兴趣包（BoI）将任何时间段的用户行为编码为超高维度（例如10^5）的稀疏向量；（II）自监督多锚点编码网络（SMEN）将BoI特征序列映射到多个低维表示。

    User Modeling plays an essential role in industry. In this field, task-agnostic approaches, which generate general-purpose representation applicable to diverse downstream user cognition tasks, is a promising direction being more valuable and economical than task-specific representation learning. With the rapid development of Internet service platforms, user behaviors have been accumulated continuously. However, existing general-purpose user representation researches have little ability for full-life cycle modeling on extremely long behavior sequences since user registration. In this study, we propose a novel framework called full- Life cycle User Representation Model (LURM) to tackle this challenge. Specifically, LURM consists of two cascaded sub-models: (I) Bag-of-Interests (BoI) encodes user behaviors in any time period into a sparse vector with super-high dimension (e.g., 10^5); (II) Self-supervised Multi-anchor Encoder Network (SMEN) maps sequences of BoI features to multiple low-d
    
[^197]: 深度生成解码器：MAP估计的表示改进了单细胞RNA数据建模

    The Deep Generative Decoder: MAP estimation of representations improves modeling of single-cell RNA data. (arXiv:2110.06672v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06672](http://arxiv.org/abs/2110.06672)

    Deep Generative Decoder (DGD) is a simple generative model that uses MAP estimation to compute model parameters and representations. It outperforms variational autoencoders (VAEs) by handling complex parameterized latent distributions and learning meaningful and well-structured latent representations on single-cell RNA data, including sub-clustering beyond provided labels.

    

    学习单细胞转录组学的低维表示对其下游分析至关重要。目前的最新技术是神经网络模型，如变分自编码器(VAEs)，它使用变分近似来进行推断。我们在这里介绍了深度生成解码器(DGD)，这是一个简单的生成模型，通过最大后验(MAP)估计直接计算模型参数和表示。DGD可以自然地处理复杂参数化的潜在分布，而VAEs通常使用固定的高斯分布，因为添加其他类型的分布会增加复杂性。我们首先在一个常用的基准数据集Fashion-MNIST上展示了其通用功能。其次，我们将该模型应用于多个单细胞数据集。在这里，DGD能够学习到低维、有意义且结构良好的潜在表示，并能在提供的标签之外进行子聚类。这种方法的优势在于它的简洁性和潜在表示的高质量。

    Learning low-dimensional representations of single-cell transcriptomics has become instrumental to its downstream analysis. The state of the art is currently represented by neural network models such as variational autoencoders (VAEs) which use a variational approximation of the likelihood for inference. We here present the Deep Generative Decoder (DGD), a simple generative model that computes model parameters and representations directly via maximum a posteriori (MAP) estimation. The DGD handles complex parameterized latent distributions naturally unlike VAEs which typically use a fixed Gaussian distribution, because of the complexity of adding other types. We first show its general functionality on a commonly used benchmark set, Fashion-MNIST. Secondly, we apply the model to multiple single-cell data sets. Here the DGD learns low-dimensional, meaningful and well-structured latent representations with sub-clustering beyond the provided labels. The advantages of this approach are its s
    
[^198]: 多类分类中的良性过拟合：所有路径都通往插值

    Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation. (arXiv:2106.10865v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.10865](http://arxiv.org/abs/2106.10865)

    多类分类中的良性过拟合问题进行了研究，提出了一个简单的确定性条件，当前三种算法在满足条件时会得到插值数据并具有相等准确率的分类器。

    

    在超参数化模型中，“良性过拟合”的文献大多局限于回归或二分类问题；然而，现代机器学习在多类别设置中运行。受此差异的启发，我们研究了多类线性分类中的良性过拟合。具体而言，我们考虑在可分数据上的以下训练算法：（i）交叉熵损失的经验风险最小化（ERM），收敛到多类支持向量机（SVM）解；（ii）最小二乘损失的ERM，收敛到最小范数插值（MNI）解；及（iii）一对多SVM分类器。首先，我们提供了一个简单的充分确定性条件，在该条件下，所有三种算法都会导致插值训练数据并具有相等准确率的分类器。当数据来自高斯混合模型或多项式逻辑模型时，在足够高的有效超参数化下，这个条件成立。我们还展示了...

    The literature on "benign overfitting" in overparameterized models has been mostly restricted to regression or binary classification; however, modern machine learning operates in the multiclass setting. Motivated by this discrepancy, we study benign overfitting in multiclass linear classification. Specifically, we consider the following training algorithms on separable data: (i) empirical risk minimization (ERM) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) ERM with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple sufficient deterministic condition under which all three algorithms lead to classifiers that interpolate the training data and have equal accuracy. When the data is generated from Gaussian mixtures or a multinomial logistic model, this condition holds under high enough effective overparameterization. We also show that t
    
[^199]: 鲁棒主图学习的混合模型正则化

    Regularization of Mixture Models for Robust Principal Graph Learning. (arXiv:2106.09035v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.09035](http://arxiv.org/abs/2106.09035)

    本文提出了一种正则化的混合模型方法，用于从数据分布中学习主图，能够处理脊检测中的流形学习问题以及异常值和异方差等问题。

    

    提出了混合模型的正则化版本，用于从$D$维数据点的分布中学习主图。在用于脊检测的流形学习的特殊情况下，我们假设潜在流形可以建模为拓扑先验作用下的高斯聚类图结构，从而将问题转化为最大后验估计。模型的参数通过期望最大化算法进行迭代估计，使得结构的学习在任何图先验下都具有多项式时间的收敛保证，同时结合图结构的一致性，以自然的方式使算法对异常值和流形采样的异方差具有鲁棒性。该方法使用由最小生成树给出的图先验，通过对数据集进行随机子采样来扩展图的范围，以考虑在空间分布中观察到的循环。

    A regularized version of Mixture Models is proposed to learn a principal graph from a distribution of $D$-dimensional data points. In the particular case of manifold learning for ridge detection, we assume that the underlying manifold can be modeled as a graph structure acting like a topological prior for the Gaussian clusters turning the problem into a maximum a posteriori estimation. Parameters of the model are iteratively estimated through an Expectation-Maximization procedure making the learning of the structure computationally efficient with guaranteed convergence for any graph prior in a polynomial time. We also embed in the formalism a natural way to make the algorithm robust to outliers of the pattern and heteroscedasticity of the manifold sampling coherently with the graph structure. The method uses a graph prior given by the minimum spanning tree that we extend using random sub-samplings of the dataset to take into account cycles that can be observed in the spatial distributi
    
[^200]: 递归均衡网络：具有稳定性和鲁棒性保证的灵活动力学模型

    Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness. (arXiv:2104.05942v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.05942](http://arxiv.org/abs/2104.05942)

    本文介绍了一种新型非线性动力学模型——递归均衡网络（RENs），它具有稳定性和鲁棒性的“内置”保证。RENs具有很强的灵活性，能表示多种系统，并用参数化方法简化了学习过程。

    

    本文介绍了递归均衡网络（RENs），一种用于机器学习、系统识别和控制的新型非线性动力学模型。这种新模型具有稳定性和鲁棒性的“内置”行为保证。所提出的模型类中的所有模型都是收缩的——一种强非线性稳定性形式——并且可以满足规定的增量积分二次约束（IQC），包括Lipschitz界限和增量被动性。RENs具有很强的灵活性：它们可以表示所有稳定的线性系统，所有先前已知的收缩递归神经网络和回声状态网络，所有深层前馈神经网络，以及所有稳定的Wiener/Hammerstein模型，并且可以近似所有褪色记忆和收缩非线性系统。RENs直接由R^N中的向量参数化，即在不受参数约束的情况下保证稳定性和鲁棒性，这简化了学习过程，因为通用方法可以用来直接学习RENs。

    This paper introduces recurrent equilibrium networks (RENs), a new class of nonlinear dynamical models} for applications in machine learning, system identification and control. The new model class admits ``built in'' behavioural guarantees of stability and robustness. All models in the proposed class are contracting -- a strong form of nonlinear stability -- and models can satisfy prescribed incremental integral quadratic constraints (IQC), including Lipschitz bounds and incremental passivity. RENs are otherwise very flexible: they can represent all stable linear systems, all previously-known sets of contracting recurrent neural networks and echo state networks, all deep feedforward neural networks, and all stable Wiener/Hammerstein models, and can approximate all fading-memory and contracting nonlinear systems. RENs are parameterized directly by a vector in R^N, i.e. stability and robustness are ensured without parameter constraints, which simplifies learning since \HL{generic methods
    
[^201]: 离线强化学习的遗憾快速收敛速率研究

    Fast Rates for the Regret of Offline Reinforcement Learning. (arXiv:2102.00479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.00479](http://arxiv.org/abs/2102.00479)

    本文研究了离线数据对强化学习的遗憾，提出了精细的收敛速率分析，揭示了离线强化学习收敛速度较快的现象，并通过指数形式的加速机制加快了收敛速度。

    

    本文研究了固定行为策略在无限时间折现马尔可夫决策过程（MDP）中生成的离线数据对强化学习的遗憾。现有方法（如拟合Q-迭代）的分析表明，对于遗憾的收敛速率是O(1/√n)，但实证行为表现出非常快的收敛速度。本文通过提供遗憾收敛速率的快速收敛进行更精细的遗憾分析，准确地表征了这一现象。首先，我们证明在给定最优质量函数Q*的估计的情况下，其对应的策略遗憾按照Q*估计的点对点收敛速率的指数进行收敛，从而加速了收敛速度。指数的级别取决于“决策问题”中的噪声水平，而不是估计问题。我们以线性和表格型MDP作为示例，建立了这样的噪声水平。其次，我们对拟合Q-迭代和Bellman残差进行了新的分析。

    We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret, empirical behavior exhibits \emph{much} faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate's pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the \emph{decision-making} problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman resid
    
[^202]: B-HAR:一个用于深入研究人体活动识别数据集和工作流程的开源基准框架

    B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows. (arXiv:2101.10870v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2101.10870](http://arxiv.org/abs/2101.10870)

    B-HAR是一个开源的基准框架，用于深入研究人体活动识别的数据集和工作流程，解决了HAR方法缺乏标准工作流程的问题，并提供了可配置的框架来评估模式识别模型的质量。

    

    基于机器学习和深度学习算法的人体活动识别（HAR）被认为是最具潜力的技术之一，用于监测不同人群（如运动员、老年人、儿童、雇主）的专业和日常活动，以提供各种与福祉、技术性能增强、风险预防和教育目的相关的服务。然而，HAR方法的有效性和效率分析受到缺乏标准工作流程的影响，这可能成为评估开发的模式识别模型质量的基准。这使得不同方法之间的比较成为一项具有挑战性的任务。此外，研究人员可能会犯错误，当这些错误没有被检测到时，会对达到的结果产生重大影响。为了缓解这些问题，本文提出了一个名为B-HAR的开源自动化和高度可配置的框架，用于定义和标准化HAR实验的工作流程。

    Human Activity Recognition (HAR), based on machine and deep learning algorithms is considered one of the most promising technologies to monitor professional and daily life activities for different categories of people (e.g., athletes, elderly, kids, employers) in order to provide a variety of services related, for example to well-being, empowering of technical performances, prevention of risky situation, and educational purposes. However, the analysis of the effectiveness and the efficiency of HAR methodologies suffers from the lack of a standard workflow, which might represent the baseline for the estimation of the quality of the developed pattern recognition models. This makes the comparison among different approaches a challenging task. In addition, researchers can make mistakes that, when not detected, definitely affect the achieved results. To mitigate such issues, this paper proposes an open-source automatic and highly configurable framework, named B-HAR, for the definition, stan
    
[^203]: 基于深度学习的实时控制中的不确定性分解

    Deep Learning based Uncertainty Decomposition for Real-time Control. (arXiv:2010.02613v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.02613](http://arxiv.org/abs/2010.02613)

    通过使用深度学习技术，在实时控制中提出了一种新颖的方法，用于检测认知不确定性，该方法可以在未知环境中的数据驱动控制中实现安全和高效的探索。

    

    在未知环境中的数据驱动控制需要对涉及的不确定性有清晰的理解，以确保安全和高效的探索。虽然由于测量误差而产生的随机不确定性可以在给定参数化描述的情况下明确地进行建模，但对于描述训练数据的存在或不存在的认知不确定性，建模可能更加困难。当系统动态未知时，后者在实施探索性控制策略时特别有用。我们提出了一种新颖的方法，利用深度学习来检测训练数据的缺失，其输出一个介于0（表示低不确定性）和1（表示高不确定性）之间的连续值标量。我们将这个检测器作为认知不确定性的代理，并展示了其在合成和真实数据集上相对现有方法的优势。我们的方法可以直接与随机不确定性估计相结合，并允许实时进行不确定性估计。

    Data-driven control in unknown environments requires a clear understanding of the involved uncertainties for ensuring safety and efficient exploration. While aleatoric uncertainty that arises from measurement noise can often be explicitly modeled given a parametric description, it can be harder to model epistemic uncertainty, which describes the presence or absence of training data. The latter can be particularly useful for implementing exploratory control strategies when system dynamics are unknown. We propose a novel method for detecting the absence of training data using deep learning, which gives a continuous valued scalar output between $0$ (indicating low uncertainty) and $1$ (indicating high uncertainty). We utilize this detector as a proxy for epistemic uncertainty and show its advantages over existing approaches on synthetic and real-world datasets. Our approach can be directly combined with aleatoric uncertainty estimates and allows for uncertainty estimation in real-time as 
    
[^204]: 基于采样的亚线性低秩矩阵算法框架用于去量化量子机器学习

    Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. (arXiv:1910.06151v4 [cs.DS] UPDATED)

    [http://arxiv.org/abs/1910.06151](http://arxiv.org/abs/1910.06151)

    我们提出了一个基于采样的亚线性低秩矩阵算法框架，用于去量化量子机器学习，推广了Tang的量子启发算法的一系列结果。基于量子线性代数算法和量子奇异值变换框架，我们开发了经典的SVT算法，运行时间与输入维度无关，证明了量子SVT无法实现指数级的量子加速。我们的结果足以推广去量化量子机器学习算法的所有最近研究成果。

    

    我们提出了一个基于算法的框架，用于对接近低秩矩阵进行量子启发的经典算法，并且推广了Tang在推荐系统上的突破性量子启发算法的一系列结果[STOC'19]。受到量子线性代数算法和Gilyén、Su、Low和Wiebe [STOC'19]的量子奇异值变换（SVT）框架启发，我们开发了经典的SVT算法，在适当的量子启发抽样假设下，运行时间与输入维度无关。我们的结果提供了有力的证据，说明在相应的QRAM数据结构输入模型下，量子SVT并不产生指数级的量子加速。由于量子SVT框架基本上包含了所有已知的量子线性代数技术，我们的结果与前期工作中的采样引理结合起来，足以推广所有最近关于去量化量子机器学习算法的结果。

    We present an algorithmic framework for quantum-inspired classical algorithms on close-to-low-rank matrices, generalizing the series of results started by Tang's breakthrough quantum-inspired algorithm for recommendation systems [STOC'19]. Motivated by quantum linear algebra algorithms and the quantum singular value transformation (SVT) framework of Gily\'en, Su, Low, and Wiebe [STOC'19], we develop classical algorithms for SVT that run in time independent of input dimension, under suitable quantum-inspired sampling assumptions. Our results give compelling evidence that in the corresponding QRAM data structure input model, quantum SVT does not yield exponential quantum speedups. Since the quantum SVT framework generalizes essentially all known techniques for quantum linear algebra, our results, combined with sampling lemmas from previous work, suffice to generalize all recent results about dequantizing quantum machine learning algorithms. In particular, our classical SVT framework reco
    

