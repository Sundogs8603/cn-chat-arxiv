# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables.](http://arxiv.org/abs/2308.06718) | 这篇论文提出了具有潜变量的因果结构估计的广义独立噪声（GIN）条件，并给出了线性非高斯无环因果模型中满足GIN条件的图形判据。 |
| [^2] | [Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards.](http://arxiv.org/abs/2308.06717) | 本文研究了一个自私学习代理和学习委托人之间的重复对自选游戏，探索如何估计和激励具有隐藏奖励的不完全知识代理。 |
| [^3] | [Learning on Graphs with Out-of-Distribution Nodes.](http://arxiv.org/abs/2308.06714) | 这篇论文提出了一种在图中学习的方法，能够处理存在分布外节点（OOD nodes）的场景。作者定义了分布外节点，并设定了两个任务：检测不属于已知分布的节点，并对剩余节点进行分类。他们通过提出的Out-of-Distribution Graph Attention Network (OODGAT)方法来解决这个问题。 |
| [^4] | [The Hard-Constraint PINNs for Interface Optimal Control Problems.](http://arxiv.org/abs/2308.06709) | 本研究将物理信息神经网络（PINNs）与不连续性捕获神经网络相结合，应用于具有界面和控制约束的优化控制问题，并通过将边界和界面条件作为硬约束以确保数值精度。 |
| [^5] | [Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model.](http://arxiv.org/abs/2308.06708) | 本文提出了一种利用去噪扩散概率模型生成伪集合的集合数据同化方法，该方法在模拟模型不完美时展示出更好的性能。 |
| [^6] | [Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods.](http://arxiv.org/abs/2308.06703) | 我们通过实验证明，相比于自适应梯度方法，使用随机梯度下降（SGD）训练的模型在输入扰动下展现出更大的鲁棒性。这种差异可以归因于自适应方法使用了不相关的频率，导致对扰动敏感的解决方案。 |
| [^7] | [Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection.](http://arxiv.org/abs/2308.06701) | 该研究提出了一个用于合成伪装数据以改善对自然场景中伪装物体检测的框架，该方法利用生成模型生成逼真的伪装图像，并在三个数据集上取得了优于目前最先进方法的结果。 |
| [^8] | [SimMatchV2: Semi-Supervised Learning with Graph Consistency.](http://arxiv.org/abs/2308.06692) | SimMatchV2是一种半监督学习算法，通过建立标记和未标记数据之间的一致性约束，从图的角度解决了半监督图像分类问题。通过节点之间的一致性和特征归一化，SimMatchV2显著提高了性能。 |
| [^9] | [MDB: Interactively Querying Datasets and Models.](http://arxiv.org/abs/2308.06686) | MDB是一个调试框架，用于互动查询数据集和模型。它通过集成函数式编程与关系代数，能够快速迭代和优化查询，发现和描述错误和模型行为。实验证明，MDB比其他工具能够实现更快的查询速度加快和查询长度缩短。 |
| [^10] | [Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations.](http://arxiv.org/abs/2308.06679) | 可分离高斯神经网络（SGNN）通过利用高斯函数的可分离性，将输入数据分成多列并依次馈送到并行层中，从而实现了指数级的计算速度提升和线性的扩展性。同时，SGNN能够在训练过程中保持类似于GRBFNN的准确性水平。 |
| [^11] | [A deep learning framework for multi-scale models based on physics-informed neural networks.](http://arxiv.org/abs/2308.06672) | 本文提出了一个基于物理约束神经网络的新框架，用于解决具有不同数量级损失项的多尺度问题。通过重新构建损失函数，并应用不同数量的幂运算，使各个损失项在数量级上大致相等。同时提供了一种分组正则化策略来应对在不同子领域中显着变化的问题。 |
| [^12] | [Law of Balance and Stationary Distribution of Stochastic Gradient Descent.](http://arxiv.org/abs/2308.06671) | 本文证明了随机梯度下降算法中的小批量噪音会使解决方案向平衡解靠近，只要损失函数包含重新缩放对称性。利用这个结果，我们导出了对角线性网络的随机梯度流稳态分布，该分布展示了复杂的非线性现象。这些发现揭示了动态梯度下降法在训练神经网络中的工作原理。 |
| [^13] | [Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges.](http://arxiv.org/abs/2308.06668) | 传统农业系统中的机器学习和深度学习模型存在局限性，而基础模型在语言和视觉任务中表现出了显著的成功。本研究旨在探索基础模型在智能农业领域的潜力和应用。 |
| [^14] | [ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN.](http://arxiv.org/abs/2308.06663) | 该论文提出了一种名为ALGAN的新型GAN模型，通过调整LSTM网络的输出，实现了在无监督设置下对单变量和多变量时间序列数据进行异常检测，并在实验中优于传统方法和其他GAN方法。 |
| [^15] | [Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks.](http://arxiv.org/abs/2308.06654) | 在共享空间中，通过使用碰撞风险计算来选择相互作用体，可以有效建模行人轨迹预测，提高准确性。 |
| [^16] | [Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation.](http://arxiv.org/abs/2308.06644) | 本文提出使用逐步蒸馏来加速基于扩散的组合优化求解器，并在TSP-50数据集上展示了16倍的推理速度提升，仅有0.019%的性能降级。 |
| [^17] | [ADRMX: Additive Disentanglement of Domain Features with Remix Loss.](http://arxiv.org/abs/2308.06624) | ADRMX是一种新的架构，通过使用添加模综特征和域不变特征来解决域泛化中的限制性问题。 |
| [^18] | [Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?.](http://arxiv.org/abs/2308.06619) | 本研究介绍了一种名为EGP的创新的熵引导剪枝算法，该算法能够通过优先剪除熵较低的层中的连接来有效压缩深度神经网络，同时保持其竞争性能水平。 |
| [^19] | [On the Interplay of Convolutional Padding and Adversarial Robustness.](http://arxiv.org/abs/2308.06612) | 本研究分析了卷积填充和对抗攻击之间的相互作用，探讨了不同填充模式对对抗鲁棒性的影响。 |
| [^20] | [LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net.](http://arxiv.org/abs/2308.06603) | LadleNet是一种使用可扩展的两阶段U-Net将热红外图像转换为可见光图像的算法，通过引入跳跃连接和精细特征聚合技术，显著提高了模型性能。Handle模块构建抽象语义空间，Bowl模块解码该空间生成映射的VI图像。 |
| [^21] | [CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning.](http://arxiv.org/abs/2308.06594) | 本文提出了一种名为CoverNav的基于深度强化学习的算法，用于在隐蔽和可导航的路径上进行非结构化室外环境的自主导航规划。该算法通过生成海拔地图来计算本地代价，帮助机器人智能体在保持低成本轨迹的同时选择最大的隐蔽性。这对于无人地面车辆在寻找避难所和掩护的同时安全导航至目的地非常有帮助。 |
| [^22] | [Value-Distributional Model-Based Reinforcement Learning.](http://arxiv.org/abs/2308.06590) | 该论文介绍了一种基于价值分布模型的强化学习方法，该方法通过学习后验分布来解决决策任务中的政策不确定性问题。所提出的算法能够有效地优化策略，在连续控制任务中表现出性能优势。 |
| [^23] | [Approximate Answering of Graph Queries.](http://arxiv.org/abs/2308.06585) | 本章概述了几种方法，用于在知识图谱不完整的情况下回答查询。这些方法涵盖了不同的查询类型和图类型，并具有不同的推理能力。 |
| [^24] | [Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System.](http://arxiv.org/abs/2308.06549) | 提出了一种基于人类行为的个性化膳食推荐与菜单规划社交系统，该系统通过脑电图信号识别人们对餐食的情感，并考虑营养需求和社交情感来规划菜单。 |
| [^25] | [Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters.](http://arxiv.org/abs/2308.06545) | 本研究采用了极限梯度提升算法来提高城市区域中数字高程模型的精度，并结合土地覆盖和地形参数校正，以解决DEM在城市环境建模中的质量和适用性问题。 |
| [^26] | [Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models.](http://arxiv.org/abs/2308.06534) | 本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。 |
| [^27] | [Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface.](http://arxiv.org/abs/2308.06533) | 我们提出了一种基于知识蒸馏的轻量级深度学习集成模型，用于sEMG-based SSIs，通过对26个北约音标数据集进行分类，实现了英文单词的拼写生成。实验表明该模型的高准确率，为便携、实用设备的端到端系统开辟了新的可能性。 |
| [^28] | [Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices.](http://arxiv.org/abs/2308.06528) | 通过任务分解学习抽象视觉推理，提出了一种基于变形器蓝图的深度学习架构，该架构预测单个对象及其排列的视觉特性，通过多维预测来选择答案。 |
| [^29] | [SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models.](http://arxiv.org/abs/2308.06522) | SLoRA是一种联邦参数高效微调语言模型的方法，用于在联邦学习中利用分布式和私有数据进行微调，以克服高异构数据下的性能差距。 |
| [^30] | [Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks.](http://arxiv.org/abs/2308.06496) | 本研究对资源受限的分散式联邦学习在无线网络中的性能进行了分析，研究了不同的通信方案并优化了通信效率。研究结果包括数字和模拟传输方法的收敛界限、资源分配和收敛速度，以及模拟传输中信道衰落和噪声对模型性能的影响。 |
| [^31] | [Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding.](http://arxiv.org/abs/2308.06472) | 本文提出了一种基于同质音频-文本嵌入的灵活关键词检测方法，通过使用小型的音频-compliant文本编码器，将文本转化为音素表示并与音频进行联合分析。实验结果表明，该方法在关键词检测上表现出色，优于当前最先进的方法。 |
| [^32] | [Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest.](http://arxiv.org/abs/2308.06471) | 本文研究了利用Volterra强调非线性动力学可通量性 (VANYA) 模型来模拟森林砍伐，该模型结合了捕食者-被捕食者动力学，并在对亚马逊雨林数据进行了预测，并与其他预测方法进行了比较。 |
| [^33] | [Tiny and Efficient Model for the Edge Detection Generalization.](http://arxiv.org/abs/2308.06468) | 该论文提出了一种小而高效的边缘检测模型TEED，它具有极低的复杂度和参数数量，能够在短时间内训练并且能够产生高质量的边缘图像。同时，还提出了一个新的数据集用于测试边缘检测的泛化能力。 |
| [^34] | [Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks.](http://arxiv.org/abs/2308.06467) | 这项研究旨在评估深度神经网络对未知对抗攻击的鲁棒性，并挑战针对这些攻击的现有防御机制的有效性。实验结果表明，在仅使用鲁棒特征的数据集上训练的DNN模型并不一定能抵抗对抗性攻击。 |
| [^35] | [A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors.](http://arxiv.org/abs/2308.06464) | 本文提出一种基于一维预测运动矢量最优性的HEVC视频隐写分析方法，通过分析和定义最优比特率作为隐写分析特征，提高了检测性能。 |
| [^36] | [Multi-Label Knowledge Distillation.](http://arxiv.org/abs/2308.06453) | 这篇论文提出了一种专门针对多标签学习的新颖知识蒸馏方法，通过将多标签问题分解为一组二分类问题以利用逻辑回归的信息性语义知识，并通过利用标签嵌入的结构信息增强学习到的特征表示的独特性。 |
| [^37] | [Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More.](http://arxiv.org/abs/2308.06448) | 该论文提出了一种基于非负矩阵分解的概率模型，将聚类和简化结合起来，并提供了建模任意图结构的框架。通过将硬聚类放松为软聚类，该算法将可能困难的聚类问题放松为可解的问题。 |
| [^38] | [A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing.](http://arxiv.org/abs/2308.06447) | 本论文提出了一种顺序元转移（SMT）学习框架，用于解决物理信息神经网络（PINNs）在高度非线性系统中的训练和适应问题，提供了一种统一的、快速的解决方案。 |
| [^39] | [Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data.](http://arxiv.org/abs/2308.06443) | 本研究提出了一种名为神经潜在对齐器的框架，用于学习复杂行为的神经表示。通过对齐重复试验的表示，并使用时间弯曲模型解决时间不对齐问题，该模型能够在降维空间中学习到更好的行为解码表示。 |
| [^40] | [A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell's Equations in Heterogeneous Media.](http://arxiv.org/abs/2308.06436) | 本文提出了一个面向域自适应的基于物理知识的神经网络（da-PINN），用于解决异质介质中麦克斯韦方程的反问题。通过将介质界面位置参数与损失函数结合，并采用面向域自适应的训练策略，da-PINN在求解麦克斯韦方程中取得了显著的效果。 |
| [^41] | [Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration.](http://arxiv.org/abs/2308.06432) | 提出了一种用于预测处理后新生血管性年龄相关黄斑变性的单时间段疾病演变网络(SHENet)，可以通过输入术前SD-OCT图像预测生成术后SD-OCT图像。 |
| [^42] | [Genetic heterogeneity analysis using genetic algorithm and network science.](http://arxiv.org/abs/2308.06429) | 本论文介绍了一种使用遗传算法和网络科学进行遗传异质性分析的新方法，通过构建基于多个独立特征选择运行的网络来提取遗传变量的异质子集，并引入了综合特征风险评分来解释基因变量之间的关联。 |
| [^43] | [Multiclass Learnability Does Not Imply Sample Compression.](http://arxiv.org/abs/2308.06424) | 学习二元假设类具有样本压缩方案，而多类别假设类则不具备这个性质。 |
| [^44] | [Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation.](http://arxiv.org/abs/2308.06422) | 本研究引入了一种创新的深度神经网络优化方法，通过自动选择最佳的位宽和层宽来提高网络效率。同时，通过剪枝和聚类技术，优化了搜索过程，并在多个数据集上进行了严格测试，结果显示该方法明显优于现有方法。 |
| [^45] | [Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review.](http://arxiv.org/abs/2308.06419) | 本文系统综述了行人-车辆混合环境下的行人轨迹预测方法，探讨了车辆和行人的相互作用对行人未来行为的影响，并回顾了先前提出的预测模型中如何考虑不确定性和行为差异。 |
| [^46] | [Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering.](http://arxiv.org/abs/2308.06399) | 本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。 |
| [^47] | [Detecting and Preventing Hallucinations in Large Vision Language Models.](http://arxiv.org/abs/2308.06394) | 本论文提出了一个用于训练和评估模型的多模态幻觉检测数据集，以解决大型视觉语言模型中存在的幻觉文本问题。这是第一个用于详细图像描述的全面多模态幻觉检测数据集。 |
| [^48] | [Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion.](http://arxiv.org/abs/2308.06382) | 本论文提出了一种名为“音素幻觉器”的一次性语音转换模型，通过集合扩展的方法，只需短时间目标说话人语音即可生成多样和高保真度的目标说话人音素，并用于基于邻居的语音转换。 |
| [^49] | [DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System.](http://arxiv.org/abs/2308.06378) | 本文介绍了一种新的深度卷积神经模糊推理系统（DCNFIS），它通过将模糊逻辑和深度学习模型相结合，实现了提高透明度而不损失准确性的目标。DCNFIS在准确性上与现有卷积神经网络相当，并且胜过了最先进的深度模糊系统。通过模糊规则提取的解释可以提高模型的可解释性。 |
| [^50] | [UAMM: UBET Automated Market Maker.](http://arxiv.org/abs/2308.06375) | UAMM是一种新的自动市场做市商方法，通过考虑外部市场价格和流动性池的暂时损失来定价，并且有效消除了套利机会。 |
| [^51] | [Topic-Level Bayesian Surprise and Serendipity for Recommender Systems.](http://arxiv.org/abs/2308.06368) | 本文通过引入基于主题的贝叶斯惊喜概念，提出了一种用于推荐系统的意外性模型，以解决过滤泡问题，通过识别相似用户和测量用户对物品的意外性来推荐具有高潜力的意外性物品。 |
| [^52] | [Learning Distributions via Monte-Carlo Marginalization.](http://arxiv.org/abs/2308.06352) | 通过蒙特卡洛边缘化学习难以计算的分布，使用参数化分布模型近似，解决了KL散度计算复杂度和优化过程可微性的问题。 |
| [^53] | [Mirror Diffusion Models.](http://arxiv.org/abs/2308.06342) | 本文提出了镜像扩散模型(MDMs)，用于在离散分类数据和连续领域中进行生成任务。MDMs受限制抽样问题的镜像Langevin算法启发，并提供了适应简单扩散、图像生成和文本生成等领域的自然扩展。 |
| [^54] | [Size Lowerbounds for Deep Operator Networks.](http://arxiv.org/abs/2308.06338) | 本文建立了深度算子网络的数据依赖性大小下界，并证明了在解决偏微分方程时，支路网络和主干网络的共同输出维度需要与数据点数量按照Ω(√n)的比例扩展，并且为了获得更低的训练误差，训练数据的大小可能需要与共同输出维度按照二次比例关系扩展。 |
| [^55] | [Predicting Resilience with Neural Networks.](http://arxiv.org/abs/2308.06309) | 本文提出了三种神经网络方法来预测系统的弹性，包括负面和正面因素的影响，对破坏性事件和恢复活动进行建模和预测。实验结果显示神经网络模型可以有效预测系统的弹性。 |
| [^56] | [A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning.](http://arxiv.org/abs/2308.06303) | 本研究提出了一种克服重力模型中零贸易问题的两步技术，通过局部线性回归和虚拟值替代来确定重力参数，并使用机器学习进行参数验证。通过该方法，解决了线性对数方程中的不确定值问题，进一步完善了重力模型在解释国际贸易方面的应用。 |
| [^57] | [Classification of Blood Cells Using Deep Learning Models.](http://arxiv.org/abs/2308.06300) | 这项研究使用深度学习模型通过图像分类对人类血细胞进行了分类和识别，为诊断疾病提供了重要的帮助。 |
| [^58] | [Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment.](http://arxiv.org/abs/2308.06299) | 本文提出了一种方法，用于解决自动驾驶中神经网络的部署中不被察觉的灾难性问题和领域转移问题。我们通过不确定性估计封装了部署中的神经网络，以提高感知系统的性能和安全性。 |
| [^59] | [A Review on Classification of White Blood Cells Using Machine Learning Models.](http://arxiv.org/abs/2308.06296) | 本综述系统分析了在医学图像分析领域中应用的机器学习技术，为白细胞分类提供了有价值的洞察力和最佳方法。 |
| [^60] | [Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks.](http://arxiv.org/abs/2308.06293) | 本研究应用和比较了MCMC和VI训练的贝叶斯神经网络在高光谱图像上进行目标检测，解决了神经网络预测的不确定性量化问题。 |
| [^61] | [The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences.](http://arxiv.org/abs/2308.06292) | 通过蛋白质结构的保守性模式，建立了一个时间参数化的统计模型，用于量化蛋白质结构的分化演化，并与序列的分化关系进行对比。这个模型能更准确估计蛋白质结构的分化时间。 |
| [^62] | [Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds.](http://arxiv.org/abs/2308.06271) | 本研究提出了一种简单且通用的方法，使用随机特征学习三维点云数据的旋转不变函数，为机器学习在3D点云上提供了一个强大的基准。 |
| [^63] | [DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning.](http://arxiv.org/abs/2308.06267) | DynamicFL是一个解决在联邦学习中通信和时效性问题的新框架，通过考虑边缘设备的通信动态和数据质量，以及采用特殊设计的客户端操作策略，实现了高效的模型更新。 |
| [^64] | [Long-term Effects of Temperature Variations on Economic Growth: A Machine Learning Approach.](http://arxiv.org/abs/2308.06265) | 这项研究利用机器学习方法调查了温度变化对经济增长的长期影响，发现平均温度和GDP增长之间存在显著关系，表明气候变化可对经济表现产生重大影响。 |
| [^65] | [Cost-effective On-device Continual Learning over Memory Hierarchy with Miro.](http://arxiv.org/abs/2308.06053) | 这项工作是首次探索基于层次内存回放的持续学习的设计空间，旨在在边缘设备上实现成本效益。提出了Miro，一个通过动态配置持续学习系统的新颖系统运行时，以实现最佳的成本效益。广泛的评估显示Miro明显优于其他方案。 |
| [^66] | [Shadow Datasets, New challenging datasets for Causal Representation Learning.](http://arxiv.org/abs/2308.05707) | 该论文提出了两个新的挑战性数据集，用于因果表示学习，并对现有数据集进行了修改，以更好地评估CRL性能。 |
| [^67] | [Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance.](http://arxiv.org/abs/2308.05034) | Kairos是第一个同时满足范围、攻击不可知性、时效性和攻击重建维度要求的溯源为基础的入侵检测和调查系统。 |
| [^68] | [NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?.](http://arxiv.org/abs/2308.04889) | 该报告关注当前最具影响力的AI论文，并以标准化引用计数为依据编制了40篇最受欢迎的论文列表。观察到在2023年上半年，大型语言模型（LLMs）和具体而言的ChatGPT相关的论文占主导地位，ChatGPT表现出下降的趋势。 |
| [^69] | [A General Implicit Framework for Fast NeRF Composition and Rendering.](http://arxiv.org/abs/2308.04669) | 本研究提出了一个通用的隐式框架，可以快速合成和渲染NeRF对象。通过引入神经深度场的新的表面表示方法，可以实现动态阴影并允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。 |
| [^70] | [Diffusion Model in Causal Inference with Unmeasured Confounders.](http://arxiv.org/abs/2308.03669) | 本研究扩展了扩散模型的使用，提出了一种新的模型BDCM，可以在存在无法测量的混淆因素的情况下更准确地回答因果问题。 |
| [^71] | [Expediting Neural Network Verification via Network Reduction.](http://arxiv.org/abs/2308.03330) | 本研究提出了一种网络简化技术，在验证之前对神经网络进行预处理，通过消除稳定的ReLU神经元，并将其转化为由ReLU和仿射层组成的顺序神经网络，从而显著减小了神经网络的规模并加速了现有的验证工具。 |
| [^72] | [Source-free Domain Adaptive Human Pose Estimation.](http://arxiv.org/abs/2308.03202) | 提出了无源域自适应的人体姿势估计任务，旨在解决在适应过程中无法访问源数据的跨域学习挑战。通过提出的新框架，源保护模块更有效地保留源信息并抵抗噪声。 |
| [^73] | [Spectral Ranking Inferences based on General Multiway Comparisons.](http://arxiv.org/abs/2308.02918) | 本文研究了使用光谱方法在广义多维比较中估计和量化未观察到的比较实体的偏好分数的性能，并揭示了光谱估计量与最大似然估计量之间的关系。 |
| [^74] | [A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation.](http://arxiv.org/abs/2308.02293) | 通过引入高阶总变差正则化的随机优化算法，可以高效地训练非线性神经网络，避免过拟合问题。 |
| [^75] | [Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases.](http://arxiv.org/abs/2308.01138) | 这项研究提出了一个噪声模式转移模型，可以将噪声模式从不同环境的标准样本应用到未知样本，通过生成案例库来解决样本级噪声对数据集级噪声学习的干扰，提高了系统的学习性能。 |
| [^76] | [A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness.](http://arxiv.org/abs/2308.01050) | 本文基于反事实模拟提出了一个数据驱动的框架，用于比较不同自动驾驶车辆在不同操作设计领域中行为风险。通过引入反事实安全边界的概念，该框架可以找到最关键的情景，并评估自动驾驶车辆的风险频率和严重程度。该方法即使在自动驾驶车辆的行为策略未知的情况下也适用，对外部第三方风险评估机构有用。 |
| [^77] | [Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN.](http://arxiv.org/abs/2308.00715) | 本文提出了一种使用多头通道注意力机制的深度学习方法，能够自动分类COVID-19 CT图像，并在广泛使用的数据集上展示了96.99%的准确性。 |
| [^78] | [On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey.](http://arxiv.org/abs/2307.16680) | 本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。 |
| [^79] | [Continual Learning in Predictive Autoscaling.](http://arxiv.org/abs/2307.15941) | 本论文提出了一种基于重放的持续学习方法，使用少量历史数据，解决了预测自动缩放中的性能下降问题。 |
| [^80] | [AlignDet: Aligning Pre-training and Fine-tuning in Object Detection.](http://arxiv.org/abs/2307.11077) | 本文提出了一个统一的预训练框架AlignDet，通过将预训练过程分解为图像域和框域预训练，可以减轻目标检测中现有实践中的数据、模型和任务差异，提高检测器性能和泛化能力。 |
| [^81] | [Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions.](http://arxiv.org/abs/2307.10644) | 本研究提出了一种快速和鲁棒的方法来近似计算多元正态分布之间的Fisher-Rao距离，并引入了一类基于正态流形嵌入到高维对称正定锥子流形的距离。 |
| [^82] | [Efficient Guided Generation for LLMs.](http://arxiv.org/abs/2307.09702) | 本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。 |
| [^83] | [CaRT: Certified Safety and Robust Tracking in Learning-based Motion Planning for Multi-Agent Systems.](http://arxiv.org/abs/2307.08602) | CaRT是一种验证安全和鲁棒跟踪的分层分布式架构，针对多智能体系统基于学习的运动规划。它通过分析形式的安全过滤器确保了安全机动，并通过鲁棒过滤器优化跟踪认证安全轨迹，即使在干扰存在的情况下也能保证安全和轨迹跟踪误差的指数有界性。 |
| [^84] | [Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB.](http://arxiv.org/abs/2307.07539) | 本文提出了对GP-UCB算法进行改进，使其具有几乎最优的次线性遗憾，并解决了关于遗憾分析的开放问题。 |
| [^85] | [The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence.](http://arxiv.org/abs/2307.07522) | 生成型人工智能和大型语言模型可能为基础科学的发现提供机会，通过其自主生成假设和探索假设空间的闭环方法，加速科学发现的进程。 |
| [^86] | [Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series.](http://arxiv.org/abs/2307.01482) | 提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。 |
| [^87] | [Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges.](http://arxiv.org/abs/2307.01050) | 本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。 |
| [^88] | [Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective.](http://arxiv.org/abs/2306.13926) | 本论文研究了GNN在神经网络特征学习理论中的作用。 发现图卷积网络显著增强了良性过拟合区域，在这个区域内信号学习超越了噪声记忆。 |
| [^89] | [What Constitutes Good Contrastive Learning in Time-Series Forecasting?.](http://arxiv.org/abs/2306.12086) | 本文通过对比分析各种训练变量(包括不同的SSCL算法、学习策略，模型体系结构以及它们之间的相互作用)的有效性，研究了SSCL在时间序列预测中的影响及具体好处。 |
| [^90] | [Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method.](http://arxiv.org/abs/2306.11307) | 本文提出了一种称为GTAGC的图形自编码器图形变换自编码器方法，通过融合图自编码器和图形变换器，GTAGC能够捕获全局依赖关系，从而有助于提高图聚类的性能。 |
| [^91] | [Decentralized SGD and Average-direction SAM are Asymptotically Equivalent.](http://arxiv.org/abs/2306.02913) | 分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力 |
| [^92] | [Permutation Decision Trees.](http://arxiv.org/abs/2306.02617) | 该论文提出了一种称为排列决策树的模型，通过引入一种新的复杂度度量方法，能够捕捉到数据实例的顺序依赖性，在不同排列的数据实例上得到不同的决策树模型，从而克服了传统决策树模型在处理顺序相关数据时的限制。 |
| [^93] | [Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance.](http://arxiv.org/abs/2305.20057) | 本文研究了多目标学习中的三重权衡问题，通过研究动态加权算法在MoDo算法中的泛化性能和与优化的相互作用，发现了动态加权方法的局限性。 |
| [^94] | [MADiff: Offline Multi-agent Learning with Diffusion Models.](http://arxiv.org/abs/2305.17330) | 本论文提出了基于注意力的扩散模型MADiff，解决了多智能体问题，是第一个扩散模型应用于多智能体离线RL的框架。 |
| [^95] | [How many samples are needed to leverage smoothness?.](http://arxiv.org/abs/2305.16014) | 本文通过研究泛化误差的新下界，探讨了学习平滑函数时需要的样本数量及其机器学习问题中的挑战。 |
| [^96] | [Evaluating the Impact of Social Determinants on Health Prediction in the Intensive Care Unit.](http://arxiv.org/abs/2305.12622) | 该论文研究了社会决定因素对重症监护室中健康预测的影响。研究发现，对于一般患者群体来说，SDOH特征并不能提高预测模型的性能，但对于特定亚群体的数据受限模型的公平性具有改善作用。 |
| [^97] | [Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization.](http://arxiv.org/abs/2305.11095) | 本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。 |
| [^98] | [Seq-HGNN: Learning Sequential Node Representation on Heterogeneous Graph.](http://arxiv.org/abs/2305.10771) | Seq-HGNN提出了一种新颖的异构图神经网络，具有序列节点表示，通过学习一种序列节点表示机制，避免了由单个向量节点表示引起的信息丢失。 |
| [^99] | [Towards Object Re-Identification from Point Clouds for 3D MOT.](http://arxiv.org/abs/2305.10210) | 该论文研究面向三维MOT中的点云再识别问题，提出了一种轻量级匹配头用于点云ReID的网络，通过实验结果表明，随着传感器分辨率的提高和观测点密度的增加，点云ReID的表现逐渐接近于图像ReID。 |
| [^100] | [Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples.](http://arxiv.org/abs/2305.09241) | “无法学习的样本”提出一种对数据进行保护的方法，但它无法阻止未经授权的用户对保护后的数据进行利用。通过提出“可学习的未经授权示例”和一种新的纯化过程，我们可以实现对数据的更好保护。 |
| [^101] | [Fairness in Machine Learning meets with Equity in Healthcare.](http://arxiv.org/abs/2305.07041) | 本文提出了一个基于软件工程原理的人工智能框架，用于在确保医疗保健公正的同时，识别和减轻数据和模型中的偏见。未来的研究旨在在真实临床环境中测试和验证框架，以评估其在促进健康公平方面的影响。 |
| [^102] | [Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices.](http://arxiv.org/abs/2305.03308) | Tiny-PPG是一个用于边缘设备上实时检测PPG信号中运动伪影的轻量级深度神经网络，通过使用深度可分离卷积和空洞空间金字塔池化模块，以及通道注意机制，能够在平衡检测精度和速度的情况下实现最先进的检测效果，并可以在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。 |
| [^103] | [G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer.](http://arxiv.org/abs/2305.03153) | G-MATT是一个结合数据驱动模型与化学知识的化学感知回溯合成预测框架，在分层SMILES语法树输入的基础上采用树到序列变换器架构，能够显著提高单步回溯合成的准确率。 |
| [^104] | [Augmented balancing weights as linear regression.](http://arxiv.org/abs/2304.14545) | 本文探索了自动去偏重重配的新颖特征描述，并将其等同于基于内核岭回归的单个欠平滑岭回归，进一步将这种方法推广到特定的结果和重配模型选择上。 |
| [^105] | [Learning Human-Human Interactions in Images from Weak Textual Supervision.](http://arxiv.org/abs/2304.14104) | 本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。 |
| [^106] | [Sample-Specific Debiasing for Better Image-Text Models.](http://arxiv.org/abs/2304.13181) | 发现从训练数据集中均匀地抽取负样本会引入错误的负面样本。我们提出了一种纠正错误负面样本的新方法，即针对图文模型的样本特异性去偏方法。 |
| [^107] | [UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite.](http://arxiv.org/abs/2304.08842) | 该论文介绍了一个开源的道路坑洞检测基准套件UDTIRI，包含了标记齐全的1000张道路坑洞图像，可以用于深度学习方法在城市道路检查中的目标检测、语义分割和实例分割任务。 |
| [^108] | [Logistic-Normal Likelihoods for Heteroscedastic Label Noise in Classification.](http://arxiv.org/abs/2304.02849) | 该论文介绍了一种新的分类方法，用于提高鲁棒性，减少标签噪声的影响，其基于正态分布，并可通过最小化负对数似然来学习参数。 |
| [^109] | [Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning.](http://arxiv.org/abs/2304.01203) | 本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数；在离线和在线的目标达成基准测试中，QRL展示了更好的采样效率和性能，包括基于状态和基于图像的观测。 |
| [^110] | [Multi-Task Learning for Post-transplant Cause of Death Analysis: A Case Study on Liver Transplant.](http://arxiv.org/abs/2304.00012) | CoD-MTL是一个新的死因分析框架，采用多任务学习来共同模拟不同CoD预测任务之间的语义关系，并且利用树蒸馏策略进行多任务学习，在具备树模型和多任务学习的优点中发掘最大化的利益。该框架可以提供精确可靠的CoD预测，具有重要的临床应用价值。 |
| [^111] | [ACAT: Adversarial Counterfactual Attention for Classification and Detection in Medical Imaging.](http://arxiv.org/abs/2303.15421) | ACAT使用拟对抗反事实注意力的方法可以提高医学图像分类和检测的准确性，在脑部CT扫描中将病变的分类准确率提高到72.55%，在肺部CT扫描中将与COVID-19相关结果的分类准确率提高到70.84%。 |
| [^112] | [Multiscale Attention via Wavelet Neural Operators for Vision Transformers.](http://arxiv.org/abs/2303.12398) | 本文介绍了一种基于小波神经算子的多尺度注意力机制，它通过使用小波神经算子将注意力机制从局部扩展到全域和多尺度范围内，取得了比ViT和AFNO更显著的性能提高。 |
| [^113] | [Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum Estimators.](http://arxiv.org/abs/2303.11908) | 本文给出了非渐进误差边界，为广泛类别的谱估计器提供了误差边界，包括在特定频率处的点值误差边界和所有频率下的最坏情况误差边界，并利用该方法导出了经典谱估计器（Blackman-Tukey、Bartlett 和 Welch 估计器）的误差边界。 |
| [^114] | [Localizing Object-level Shape Variations with Text-to-Image Diffusion Models.](http://arxiv.org/abs/2303.11306) | 本文介绍了一种利用文本到图像扩散模型，能够生成变化形状的图像集合，从而支持对象级形状探索过程的技术。通过引入提示混合技术和使用自注意力层的方法，实现了可控制的形状变化并准确定位图像空间操作。 |
| [^115] | [TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation.](http://arxiv.org/abs/2303.06937) | 本文研究了一个重要但鲜为人知的问题：联邦类式持续学习，在联邦学习中动态添加新的类别。我们提出了一种称为TARGET的新颖方法，通过无样本蒸馏来减轻FCCL中的灾难性遗忘问题，并保护客户数据的隐私。该方法利用先前训练的全局模型在模型层面上传递旧任务的知识，并通过生成器生成合成数据来模拟数据的全局分布。与先前的FCCL方法相比，TARGET无需额外的数据集或存储先前任务的私有数据。 |
| [^116] | [Miipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations.](http://arxiv.org/abs/2303.01664) | Miipher是一个稳健的语音修复模型，通过整合自监督语音和文本表示，可以将受损的语音信号转换为高质量语音，并且可以应用于增加高质量训练数据的新型语音生成应用。 |
| [^117] | [Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation.](http://arxiv.org/abs/2303.01464) | 这篇论文提出了OMG-CMDP!算法，用于在对抗性上下文MDPs中最小化遗憾。该算法在具备可实现函数类和在线回归预言机的最小假设下操作，具有高效、简单和对近似错误的鲁棒性。它具有高效率最优的遗憾保证，是对抗CMDPs领域的首个满足最小假设条件的算法。 |
| [^118] | [Joint Task and Data Oriented Semantic Communications: A Deep Separate Source-channel Coding Scheme.](http://arxiv.org/abs/2302.13580) | 本文提出了一种深度分离的源信道编码框架，用于联合任务和数据导向的语义通信，充分利用语义特征和较少的频谱资源，通过变分自编码器方法解决率失真问题。 |
| [^119] | [Why Target Networks Stabilise Temporal Difference Methods.](http://arxiv.org/abs/2302.12537) | 本文解释了深度强化学习中一种流行的时序差分方法中关键的稳定性问题：为什么目标网络能够有效降低不满足条件时的影响。 |
| [^120] | [On the Training Instability of Shuffling SGD with Batch Normalization.](http://arxiv.org/abs/2302.12444) | 本文研究了随机梯度下降算法与批量归一化的相互作用，在特定网络中表现为单次重排与随机重排收敛到不同的扭曲全局最优点，建议使用随机重排。 |
| [^121] | [Using Automated Algorithm Configuration for Parameter Control.](http://arxiv.org/abs/2302.12334) | 本研究通过自动算法配置来解决参数控制的问题，并提出了能够显著提高方法性能的技术。 |
| [^122] | [Time-varying Signals Recovery via Graph Neural Networks.](http://arxiv.org/abs/2302.11313) | 本文提出了一种基于图神经网络的时变信号恢复方法，通过编码器-解码器架构与Sobolev平滑算子组成的专门损失函数，可以有效地恢复时变图信号。 |
| [^123] | [LabelPrompt: Effective Prompt-based Learning for Relation Classification.](http://arxiv.org/abs/2302.08068) | LabelPrompt是一种面向关系分类任务的提示式学习方法，通过定义额外的令牌来表示关系标签，并使用提示模板方法明确构建它们，从而解决了将填充掩码标记的自然语言词汇与语义关系标签相关联的挑战。同时，该方法还实现了一个实体感知模块来减轻预测关系和给定实体之间的不一致性。 |
| [^124] | [Zero-shot causal learning.](http://arxiv.org/abs/2301.12292) | 无先验因果学习是一个解决预测新型干预措施个性化影响的框架，并通过元学习对任务的处理达成了目的，能够将干预措施的知识传输到未见过的干预措施中，并在合成和真实数据集上表现出了优越性能。 |
| [^125] | [Analyzing Robustness of the Deep Reinforcement Learning Algorithm in Ramp Metering Applications Considering False Data Injection Attack and Defense.](http://arxiv.org/abs/2301.12036) | 本研究提出了一种只使用环路检测器信息作为输入的深度Q学习算法，并通过设计虚假数据注入攻击和随机噪声攻击来研究模型的鲁棒性。 |
| [^126] | [Communication-Efficient Collaborative Regret Minimization in Multi-Armed Bandits.](http://arxiv.org/abs/2301.11442) | 本文研究了多智能体多臂赌博机中并行性和沟通开销之间的折中问题，并提出了一组代理之间通信轮次和合作学习过程后悔之间的权衡关系。 |
| [^127] | [Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps.](http://arxiv.org/abs/2301.08957) | 这篇论文介绍了一种新的自监督学习方法，首次使用Transformer进行室外定位的任务。作者提出了切片Transformer模型，并利用轴向特性重新组织了激光雷达扫描的切片。此外，作者还引入了Perth-WA数据集，并在多个数据集上进行了实验评估，证明了方法的有效性。 |
| [^128] | [Harmonic (Quantum) Neural Networks.](http://arxiv.org/abs/2212.07462) | 本文介绍了一种在神经网络中表示谐波函数的有效方法，并将这一方法扩展到了量子神经网络中，通过对比基准测试发现其性能优越。 |
| [^129] | [Multi-view Graph Convolutional Networks with Differentiable Node Selection.](http://arxiv.org/abs/2212.05124) | 本文提出了一种具有可微化节点选择的多视角图卷积网络，可有效地利用多视图数据的互补和一致信息，提取多个对象之间的潜在信息。 |
| [^130] | [A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization.](http://arxiv.org/abs/2212.02387) | 本文提出了一种称为去中心化递归梯度上升法（DREAM）的简单高效算法，用于解决去中心化非凸极小化问题，并实现了寻找原函数的 $\epsilon$-稳定点的最佳理论保证。 |
| [^131] | [Review of medical data analysis based on spiking neural networks.](http://arxiv.org/abs/2212.02234) | 本文综述了基于脉冲神经网络的医疗数据分析，使用医疗数据进行信号分类和疾病诊断。与传统网络相比，脉冲神经网络具有一些优势和劣势。 |
| [^132] | [Euler Characteristic Curves and Profiles: a stable shape invariant for big data problems.](http://arxiv.org/abs/2212.01666) | Euler特征曲线和轮廓是一种稳定的形状不变量，用于大数据问题。与持久同调相比，它们具有分布式计算、多滤波推广和适用于大数据的优势，同时具有稳定性。 |
| [^133] | [Online Estimation of the Koopman Operator Using Fourier Features.](http://arxiv.org/abs/2212.01503) | 本文提出了一种在线数据估计Koopman算符的优化方案，通过联合学习可观测量和Koopman算符，能够重构演变并表示复杂动力系统的全局特征。 |
| [^134] | [Model-Free Reinforcement Learning with the Decision-Estimation Coefficient.](http://arxiv.org/abs/2211.14250) | 本文介绍了决策估计系数和Estimation-to-Decisions元算法，在无模型强化学习中结合乐观估计，提供了更宽松的估计误差概念的保证。 |
| [^135] | [Overparameterized random feature regression with nearly orthogonal data.](http://arxiv.org/abs/2211.06077) | 本文研究了过度参数化的随机特征回归(RFRR)在几乎正交数据上的行为，证明了在第一层宽度大于样本大小的情况下，RFRR的训练误差、交叉验证和泛化误差可以高概率集中在核岭回归(KRR)的相应值周围，同时给出了用多项式核近似KRR性能的方法。 |
| [^136] | [StyleNAT: Giving Each Head a New Perspective.](http://arxiv.org/abs/2211.05770) | StyleNAT是一个新的基于transformer的图像生成框架，通过使用邻域注意力（NA）来捕捉局部和全局信息，能够高效灵活地适应不同的数据集，并在FFHQ-256上取得了新的最佳结果。 |
| [^137] | [Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids Representations.](http://arxiv.org/abs/2211.03226) | 本文提出了一种旋转等变图神经网络（GNN），通过约束保持旋转平移等变性的方式，学习玻璃液体静态结构的稳健表示。这种约束显著提高了预测能力和泛化能力，同时减少了参数数量，并且提高了解释性。通过迁移学习实验证明了网络的有效性。 |
| [^138] | [Agent-Controller Representations: Principled Offline RL with Rich Exogenous Information.](http://arxiv.org/abs/2211.00164) | 本文针对具有丰富外部信息的原则性离线强化学习进行研究，并提出了新的离线强化学习基准。研究发现，当噪声是复杂且与时间相关的过程时，现有的表示学习技术可能无法成功应用于这类数据集。 |
| [^139] | [A jet tagging algorithm of graph network with HaarPooling message passing.](http://arxiv.org/abs/2210.13869) | 本论文介绍了一种将图神经网络与HaarPooling操作相结合的方法，称为HMPNet，用于高能物理中的夸克胶子标记。实验结果表明，适当选择HaarPooling的信息可以提高夸克胶子标记的准确性。 |
| [^140] | [A Primal-Dual Algorithm for Hybrid Federated Learning.](http://arxiv.org/abs/2210.08106) | 该论文提出了一种基于Fenchel对偶性的快速、稳健的混合联邦学习算法。实验证明了该算法相对于传统的FedAvg方法的性能改进，并提供了隐私保护措施。 |
| [^141] | [An Embarrassingly Simple Backdoor Attack on Self-supervised Learning.](http://arxiv.org/abs/2210.07346) | 这项研究探索了自监督学习后门攻击的问题，提出了一种尴尬简单但高效的攻击方法，通过污染训练数据并插入触发器，使得在推理时任何带有触发器的输入都能以高概率被错误分类到对手指定的类别。这一发现表明，自监督学习和监督学习在对抗性攻击方面存在差异。 |
| [^142] | [Large Language Models can Implement Policy Iteration.](http://arxiv.org/abs/2210.03821) | 本论文提出一种名为ICPI的算法，利用基础模型在上下文中执行强化学习任务，无需专家示范或梯度。算法采用策略迭代方法，不仅避免了示范收集的繁琐工作，还解决了梯度方法的运行速度问题。 |
| [^143] | [Label-Noise Learning with Intrinsically Long-Tailed Data.](http://arxiv.org/abs/2208.09833) | 本文针对自带长尾数据的标签噪声学习问题，提出了一个学习框架并设计了双维样本选择算法 TABASCO，有效地将干净样本与噪声样本分开，特别适用于长尾类别。 |
| [^144] | [Physics-Constrained Deep Learning for Climate Downscaling.](http://arxiv.org/abs/2208.05424) | 本文提出了一种物理约束深度学习降尺度模型的方法，以保证模型在预测物理变量时满足守恒定律，并提高其性能。 |
| [^145] | [What Can Transformers Learn In-Context? A Case Study of Simple Function Classes.](http://arxiv.org/abs/2208.01066) | 本研究通过考虑在上下文中学习线性函数的问题，证明了标准的Transformers模型可以从头训练，在推断时实现线性函数的上下文学习能力。 |
| [^146] | [Machine Learning and Computer Vision Techniques in Bee Monitoring Applications.](http://arxiv.org/abs/2208.00085) | 本文介绍了机器学习和计算机视觉在蜜蜂监测中的最新应用，展示了自动化蜜蜂计数算法的潜力，并希望能够激发其他科学家的灵感和兴趣。 |
| [^147] | [SmartGD: A GAN-Based Graph Drawing Framework for Diverse Aesthetic Goals.](http://arxiv.org/abs/2206.06434) | SmartGD提出了一种基于GAN的图形绘制框架，可以优化不同的定量美学目标，解决了现有方法只关注单一美学方面的问题。 |
| [^148] | [Analysis of functional neural codes of deep learning models.](http://arxiv.org/abs/2205.10952) | 本研究使用自组织映射(SOM)分析了深度学习模型中与决策相关的内部编码，发现浅层将特征压缩到紧凑空间中，而深层将特征空间扩展，并指出压缩特征可能导致对敌对扰动的脆弱性。 |
| [^149] | [GUARD: Graph Universal Adversarial Defense.](http://arxiv.org/abs/2204.09803) | GUARD是一种新颖的图形通用对抗防御方法，旨在提高GCNs对针对性攻击的局部节点的鲁棒性，并在不降低整体性能的情况下进行防御。 |
| [^150] | [Maximum entropy optimal density control of discrete-time linear systems and Schr\"odinger bridges.](http://arxiv.org/abs/2204.05263) | 本文研究了离散时间线性系统和Schr\"odinger桥的最大熵最优密度控制问题，通过引入特定时间点的高斯密度约束来直接控制系统状态的不确定性。 |
| [^151] | [Predicting the generalization gap in neural networks using topological data analysis.](http://arxiv.org/abs/2203.12330) | 本文利用拓扑数据分析的方法研究神经网络的泛化差距，通过计算加权图的同调持久图，并比较不同数值汇总的有用性，可以准确预测和部分解释泛化差距，而无需测试集。实验结果表明，在计算机视觉识别任务上具有竞争力。 |
| [^152] | [Automated Learning for Deformable Medical Image Registration by Jointly Optimizing Network Architectures and Objective Functions.](http://arxiv.org/abs/2203.06810) | 本文提出了一种自动学习配准算法（AutoReg），它可以协同优化网络架构和训练目标，使非计算机专家能够方便地找到适用于不同场景的配准算法。 |
| [^153] | [Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation.](http://arxiv.org/abs/2203.05400) | 该论文研究了高斯过程插值中光滑参数估计的渐近界限。结果表明，光滑参数的最大似然估计不能在渐近意义下欠平滑真值，并且最大似然估计能恢复一类分段支持自相似函数的真实光滑度。 |
| [^154] | [Scalable Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Health.](http://arxiv.org/abs/2202.00916) | 本文提出了一种在不安定多臂赌博（RMAB）问题中进行决策集中学习的新方法，通过直接训练预测模型来最大化Whittle指数解决方案质量。 |
| [^155] | [On the Power of Gradual Network Alignment Using Dual-Perception Similarities.](http://arxiv.org/abs/2201.10945) | 本研究提出了Grad-Align，一种渐进网络对齐方法，通过利用强一致性节点对逐步发现节点对。该方法首先生成节点嵌入，然后计算双感知相似性度量逐步对齐节点。 |
| [^156] | [Fine-grained Graph Learning for Multi-view Subspace Clustering.](http://arxiv.org/abs/2201.04604) | 本文提出了一种细粒度图学习框架用于多视角子空间聚类，解决了传统方法未建立图学习和聚类之间关联以及忽略局部结构重要性的问题。 |
| [^157] | [Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles.](http://arxiv.org/abs/2112.11217) | 本文提出了一种安全RL算法，结合了基于屏障力的控制策略结构与多步策略评估机制，在保证控制安全的同时，能够应对时间变化的安全约束，并证明了其稳定性、鲁棒性和收敛性，优于几种最先进的RL算法。 |
| [^158] | [Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity.](http://arxiv.org/abs/2111.06781) | 本文研究了在具有连续状态和动作空间的MDPs中使用Q-Learning的收敛和近似最优性问题。通过量化状态和动作，我们证明Quantized Q-Learning会收敛到一个极限，并且这个极限满足一个最优性方程，从而实现近似最优性。 |
| [^159] | [Deconfounded Causal Collaborative Filtering.](http://arxiv.org/abs/2110.07122) | 该论文提出了解决推荐系统中混杂因素影响的问题的方法，通过提出混杂因果协同过滤方法，解决了现有方法需要为每个特定混杂因素设计模型的不现实性以及潜在混杂因素无法观察的难题。 |
| [^160] | [Training Spiking Neural Networks Using Lessons From Deep Learning.](http://arxiv.org/abs/2109.12894) | 本论文介绍如何将几十年的深度学习、梯度下降、反向传播和神经科学研究的经验教训应用于生物可行的脉冲神经网络，并探索了将数据编码为脉冲和学习过程之间的微妙相互作用以及生物可行的在线学习的发展方向。 |
| [^161] | [Hybrid quantum-classical machine learning for generative chemistry and drug design.](http://arxiv.org/abs/2108.11644) | 本研究构建了一个混合的量子经典机器学习模型，利用深度生成化学模型加速药物发现。通过在D-Wave量子退火器上训练，成功生成了具有药物化学和合成可及性特性的2331个新化学结构。 |
| [^162] | [Locally differentially private estimation of nonlinear functionals of discrete distributions.](http://arxiv.org/abs/2107.03940) | 本文研究了在局部差分隐私背景下估计离散分布的非线性函数的问题，并展示了互动和非互动的隐私机制。通过对幂和函数的二次风险行为的研究，在非互动情况下提出了两种插补类型的估计方法。 |
| [^163] | [Adaptive Filters in Graph Convolutional Neural Networks.](http://arxiv.org/abs/2105.10377) | 该论文研究了自适应滤波器在图卷积神经网络中的应用，通过探索和利用图结构的灵活性，提高了网络处理图数据的性能。 |
| [^164] | [Contrastive Attraction and Contrastive Repulsion for Representation Learning.](http://arxiv.org/abs/2105.03746) | 本论文提出了一种双重对比学习策略，通过分别比较正样本和负样本在各自群组内的关系，并对正负群组之间进行对比，以进一步提高对比学习方法的性能和鲁棒性。 |
| [^165] | [Causal Collaborative Filtering.](http://arxiv.org/abs/2102.01868) | 本论文提出了一种名为因果协同过滤（CCF）的通用框架，用于对协同过滤和推荐中的因果关系进行建模。这种方法可以解决纯粹相关学习导致的预测中的辛普森悖论问题，提高推荐性能。 |
| [^166] | [A Deep Learning Framework for Generation and Analysis of Driving Scenario Trajectories.](http://arxiv.org/abs/2007.14524) | 我们提出了一个深度学习框架，用于生成和分析驾驶场景轨迹。我们开发了两种方法来处理不同长度的轨迹，并通过实验证明了其有效性。 |
| [^167] | [Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning.](http://arxiv.org/abs/1905.12204) | 本文探索了使用学习算法近乎最优地解决具有时间相关奖励的多智能体、多任务的NP-hard规划问题的可能性。研究结果展示了提出方法在解决机器人/机器调度问题上的近乎最优性。 |
| [^168] | [Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks.](http://arxiv.org/abs/1903.10047) | 在更可信的情况下，我们展示了ResNet类型的CNN可以在一些重要的函数类中实现极小值最优误差率，并且可以通过复制全连接神经网络的学习能力来实现。 |

# 详细

[^1]: 具有潜变量的因果结构估计的广义独立噪声条件

    Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables. (arXiv:2308.06718v1 [cs.LG])

    [http://arxiv.org/abs/2308.06718](http://arxiv.org/abs/2308.06718)

    这篇论文提出了具有潜变量的因果结构估计的广义独立噪声（GIN）条件，并给出了线性非高斯无环因果模型中满足GIN条件的图形判据。

    

    我们研究了在存在潜变量的情况下学习因果结构的挑战性任务，包括定位潜变量并确定它们的数量，以及识别潜变量和观测变量之间的因果关系。为了解决这个问题，我们提出了一种适用于包含潜变量的线性非高斯无环因果模型的广义独立噪声（GIN）条件，该条件建立了某些测量变量的线性组合与其他测量变量之间的独立性。具体而言，对于两个观测随机向量 $\bf{Y}$ 和 $\bf{Z}$，当且仅当 $\omega^{\intercal}\mathbf{Y}$ 和 $\mathbf{Z}$ 是独立的时，GIN 成立，其中 $\omega$ 是由 $\mathbf{Y}$ 和 $\mathbf{Z}$ 之间的交叉协方差确定的非零参数向量。然后，我们给出了线性非高斯无环因果模型中 GIN 条件的必要和充分图形判据。简言之，GIN 意味着存在一个外源的...

    We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous se
    
[^2]: 估计和激励具有隐藏奖励的不完全知识代理

    Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards. (arXiv:2308.06717v1 [cs.LG])

    [http://arxiv.org/abs/2308.06717](http://arxiv.org/abs/2308.06717)

    本文研究了一个自私学习代理和学习委托人之间的重复对自选游戏，探索如何估计和激励具有隐藏奖励的不完全知识代理。

    

    在实践中，激励提供者（即委托人）通常无法观察受到激励的代理的奖励实现情况，这与许多先前研究过的委托人-代理模型形成了对比。这种信息不对称性使委托人仅通过观察代理的决策就要始终估计代理的未知奖励变得更加困难，当代理需要学习自己的奖励时，这个挑战变得更加困难。这种复杂情况在各种现实场景中被观察到，从可再生能源储存合同到个性化医疗保健激励。因此，它不仅提供了有趣的理论问题，而且具有广泛的实际相关性。本文探讨了一个自私学习代理和学习委托人之间的重复对自选游戏。代理解决多臂老虎机（MAB）问题，以最大化他们预期的奖励和激励。除代理的学习外，委托人还训练了一个并行算法，并面临一个折中情况

    In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-of
    
[^3]: 存在分布外节点的图上学习

    Learning on Graphs with Out-of-Distribution Nodes. (arXiv:2308.06714v1 [cs.LG])

    [http://arxiv.org/abs/2308.06714](http://arxiv.org/abs/2308.06714)

    这篇论文提出了一种在图中学习的方法，能够处理存在分布外节点（OOD nodes）的场景。作者定义了分布外节点，并设定了两个任务：检测不属于已知分布的节点，并对剩余节点进行分类。他们通过提出的Out-of-Distribution Graph Attention Network (OODGAT)方法来解决这个问题。

    

    图神经网络（GNNs）是在图上执行预测任务的最先进模型。现有的GNNs在与图相关的各种任务上表现出色，但在训练和推理过程中存在分布外（OOD）节点的情况下，却很少引起注意。借鉴CV和NLP的概念，我们将OOD节点定义为训练集中未见的节点标签。由于许多网络是由程序自动生成的，现实世界的图往往存在噪音，并且可能包含来自未知分布的节点。本文定义了存在分布外节点的图上学习问题。具体而言，我们的目标是完成两个任务：1）检测不属于已知分布的节点；2）将剩余的节点分类为已知类别之一。我们证明图中的连接模式对于异常值检测具有信息性，并提出了Out-of-Distribution Graph Attention Network (OODGAT)。

    Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT)
    
[^4]: 硬约束PINNs用于界面最优控制问题

    The Hard-Constraint PINNs for Interface Optimal Control Problems. (arXiv:2308.06709v1 [math.OC])

    [http://arxiv.org/abs/2308.06709](http://arxiv.org/abs/2308.06709)

    本研究将物理信息神经网络（PINNs）与不连续性捕获神经网络相结合，应用于具有界面和控制约束的优化控制问题，并通过将边界和界面条件作为硬约束以确保数值精度。

    

    我们展示了物理信息神经网络（PINNs）与最近开发的不连续性捕获神经网络相结合，可以应用于具有界面和一些控制约束的偏微分方程（PDE）优化控制问题的求解。该算法无网格且可扩展到不同的PDE，并确保严格满足控制约束。由于边界和界面条件以及PDE都被视为软约束，通过将它们汇总到加权损失函数中进行处理，因此需要同时学习它们，并且不能保证边界和界面条件能够完全满足。这立即引起了在相应的损失函数中调整权重和训练神经网络的困难。为了解决这些困难并确保数值精度，我们提出在PINNs中将边界和界面条件作为硬约束。

    We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a nove
    
[^5]: 利用去噪扩散概率模型生成观测引导的集合数据同化方法

    Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model. (arXiv:2308.06708v1 [cs.LG])

    [http://arxiv.org/abs/2308.06708](http://arxiv.org/abs/2308.06708)

    本文提出了一种利用去噪扩散概率模型生成伪集合的集合数据同化方法，该方法在模拟模型不完美时展示出更好的性能。

    

    本文提出了一种使用去噪扩散概率模型生成伪集合的集合数据同化方法。由于该模型是针对嘈杂和稀疏观测数据进行训练的，因此该模型可以产生接近观测值的发散集合。得益于生成的集合的方差，我们提出的方法在模拟模型不完美时展示出比已建立的集合数据同化方法更好的性能。

    This paper presents an ensemble data assimilation method using the pseudo ensembles generated by denoising diffusion probabilistic model. Since the model is trained against noisy and sparse observation data, this model can produce divergent ensembles close to observations. Thanks to the variance in generated ensembles, our proposed method displays better performance than the well-established ensemble data assimilation method when the simulation model is imperfect.
    
[^6]: 理解随机梯度下降和自适应梯度方法之间的鲁棒性差异

    Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods. (arXiv:2308.06703v1 [cs.LG])

    [http://arxiv.org/abs/2308.06703](http://arxiv.org/abs/2308.06703)

    我们通过实验证明，相比于自适应梯度方法，使用随机梯度下降（SGD）训练的模型在输入扰动下展现出更大的鲁棒性。这种差异可以归因于自适应方法使用了不相关的频率，导致对扰动敏感的解决方案。

    

    随机梯度下降（SGD）和自适应梯度方法，如Adam和RMSProp，在训练深度神经网络中被广泛使用。我们通过实验证明，虽然使用这些方法训练的模型的标准泛化性能之间的差异很小，但使用SGD训练的模型在输入扰动下表现出更大的鲁棒性。值得注意的是，我们的研究表明，在自然数据集中存在不相关的频率，对这些频率进行变动不会影响模型的泛化性能。然而，使用自适应方法训练的模型对这些变化显示出敏感性，这表明它们使用的不相关频率会导致对扰动敏感的解决方案。为了更好地理解这种差异，我们研究了梯度下降（GD）和符号梯度下降（signGD）在模拟自然信号的合成数据集上的学习动态。在三维输入空间中，使用GD和signGD优化的模型具有标准风险

    Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risk
    
[^7]: 伪装图像合成是提高伪装物体检测的关键

    Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection. (arXiv:2308.06701v1 [cs.CV])

    [http://arxiv.org/abs/2308.06701](http://arxiv.org/abs/2308.06701)

    该研究提出了一个用于合成伪装数据以改善对自然场景中伪装物体检测的框架，该方法利用生成模型生成逼真的伪装图像，并在三个数据集上取得了优于目前最先进方法的结果。

    

    融入自然场景的伪装物体给深度学习模型检测和合成带来了重大挑战。伪装物体检测是计算机视觉中一个关键任务，具有广泛的实际应用，然而由于数据有限，该研究课题一直受到限制。我们提出了一个用于合成伪装数据以增强对自然场景中伪装物体检测的框架。我们的方法利用生成模型生成逼真的伪装图像，这些图像可以用来训练现有的物体检测模型。具体而言，我们使用伪装环境生成器，由伪装分布分类器进行监督，合成伪装图像，然后将其输入我们的生成器以扩展数据集。我们的框架在三个数据集（COD10k、CAMO和CHAMELEON）上的效果超过了目前最先进的方法，证明了它在改善伪装物体检测方面的有效性。

    Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-
    
[^8]: SimMatchV2: 基于图一致性的半监督学习

    SimMatchV2: Semi-Supervised Learning with Graph Consistency. (arXiv:2308.06692v1 [cs.CV])

    [http://arxiv.org/abs/2308.06692](http://arxiv.org/abs/2308.06692)

    SimMatchV2是一种半监督学习算法，通过建立标记和未标记数据之间的一致性约束，从图的角度解决了半监督图像分类问题。通过节点之间的一致性和特征归一化，SimMatchV2显著提高了性能。

    

    半监督图像分类是计算机视觉中最基本的问题之一，它显著减少了对人力的需求。本文引入了一种新的半监督学习算法 - SimMatchV2，它从图的角度建立了标记和未标记数据之间的一致性约束。在SimMatchV2中，我们将样本的增强视图视为一个节点，其中包含一个标签和其相应的表示。不同的节点通过节点表示的相似性连接起来。受到图论中的消息传递和节点分类的启发，我们提出了四种类型的一致性约束，分别为1) 节点-节点一致性，2) 节点-边一致性，3) 边-边一致性和4) 边-节点一致性。我们还发现简单的特征归一化可以减小不同增强视图之间特征范数的差距，从而显著提高SimMatchV2的性能。

    Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2
    
[^9]: MDB：互动查询数据集和模型

    MDB: Interactively Querying Datasets and Models. (arXiv:2308.06686v1 [cs.DB])

    [http://arxiv.org/abs/2308.06686](http://arxiv.org/abs/2308.06686)

    MDB是一个调试框架，用于互动查询数据集和模型。它通过集成函数式编程与关系代数，能够快速迭代和优化查询，发现和描述错误和模型行为。实验证明，MDB比其他工具能够实现更快的查询速度加快和查询长度缩短。

    

    随着模型的训练和部署，开发者需要能够系统地调试在机器学习流程中出现的错误。我们提出了MDB，一个用于互动查询数据集和模型的调试框架。MDB通过将函数式编程与关系代数结合起来，构建了一个对数据集和模型预测的数据库进行表达性查询的工具。查询可重用且易于修改，使得调试人员能够快速迭代和优化查询，以发现和描述错误和模型行为。我们在目标检测、偏差发现、图像分类和数据填充任务中评估了MDB在自动驾驶视频、大型语言模型和医疗记录上的性能。我们的实验证明，MDB比其他基准测试工具能够实现最高10倍的查询速度加快和40%的查询长度缩短。在用户研究中，我们发现开发者能够成功构建复杂查询来描述机器学习模型的错误。

    As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
    
[^10]: 可分离高斯神经网络：结构、分析和函数逼近

    Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations. (arXiv:2308.06679v1 [cs.LG])

    [http://arxiv.org/abs/2308.06679](http://arxiv.org/abs/2308.06679)

    可分离高斯神经网络（SGNN）通过利用高斯函数的可分离性，将输入数据分成多列并依次馈送到并行层中，从而实现了指数级的计算速度提升和线性的扩展性。同时，SGNN能够在训练过程中保持类似于GRBFNN的准确性水平。

    

    高斯径向基函数神经网络（GRBFNN）一直是插值和分类的常见选择。然而，在输入向量的维度较高时，计算量很大。为解决这个问题，我们提出了一种新的前馈网络-可分离高斯神经网络（SGNN），利用高斯函数的可分离性，将输入数据分成多列，并依次将它们馈送到由单变量高斯函数形成的并行层中。这种结构将GRBFNN的神经元数量从O(N^d)降低到O(dN)，从而指数级提高了SGNN的计算速度，并使其能够线性地随着输入维度的增加而扩展。此外，SGNN可以在梯度下降训练中保留GRBFNN的Hessian矩阵的主要子空间，从而实现与GRBFNN类似水平的准确性。实验证明，SGNN可以以与GRBFNN相似的准确性实现100倍的加速。

    The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GR
    
[^11]: 基于物理约束神经网络的多尺度模型的深度学习框架

    A deep learning framework for multi-scale models based on physics-informed neural networks. (arXiv:2308.06672v1 [cs.LG])

    [http://arxiv.org/abs/2308.06672](http://arxiv.org/abs/2308.06672)

    本文提出了一个基于物理约束神经网络的新框架，用于解决具有不同数量级损失项的多尺度问题。通过重新构建损失函数，并应用不同数量的幂运算，使各个损失项在数量级上大致相等。同时提供了一种分组正则化策略来应对在不同子领域中显着变化的问题。

    

    物理约束神经网络（PINN）将深度神经网络与偏微分方程（PDE）的解相结合，创建了一个新的有希望的研究领域，用于数值求解PDE。面对一类包含不同数量级损失项的多尺度问题，对于标准PINN方法来说，获得可用的预测是具有挑战性的。在本文中，我们提出了一个通过重构损失函数来解决多尺度问题的新框架。该框架基于标准PINN方法，并通过对不同量级的损失项应用不同数量的幂运算来修改标准PINN方法的损失函数，从而使构成损失函数的各个损失项在数量级上大致相等。此外，我们提出了一种分组正则化策略，该策略可以很好地处理在不同子领域中显着变化的问题。

    Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdo
    
[^12]: 动态梯度下降法的平衡法则与稳态分布

    Law of Balance and Stationary Distribution of Stochastic Gradient Descent. (arXiv:2308.06671v1 [cs.LG])

    [http://arxiv.org/abs/2308.06671](http://arxiv.org/abs/2308.06671)

    本文证明了随机梯度下降算法中的小批量噪音会使解决方案向平衡解靠近，只要损失函数包含重新缩放对称性。利用这个结果，我们导出了对角线性网络的随机梯度流稳态分布，该分布展示了复杂的非线性现象。这些发现揭示了动态梯度下降法在训练神经网络中的工作原理。

    

    随机梯度下降（SGD）算法是我们用于训练神经网络的算法。然而，我们很难理解SGD如何在神经网络的非线性和退化的损失曲面中进行导航。在这项工作中，我们证明了SGD的小批量噪音可以使解决方案向平衡解靠近，只要损失函数包含一个重新缩放对称性。由于简单扩散过程和SGD动力学的差异在对称性存在时最重要，我们的理论表明，损失函数的对称性是了解SGD工作方式的重要线索。然后，我们将这个结果应用于导出具有任意深度和宽度的对角线性网络的随机梯度流的稳态分布。稳态分布展现了复杂的非线性现象，如相变、破坏的遍历性和波动反转。这些现象仅在深层网络中存在，表明了一种基本的新的加深训练理论。

    The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundam
    
[^13]: 智能农业中的基础模型：基础知识、机遇和挑战

    Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v1 [cs.LG])

    [http://arxiv.org/abs/2308.06668](http://arxiv.org/abs/2308.06668)

    传统农业系统中的机器学习和深度学习模型存在局限性，而基础模型在语言和视觉任务中表现出了显著的成功。本研究旨在探索基础模型在智能农业领域的潜力和应用。

    

    过去十年间，农业系统中的机器学习和深度学习方法得到了快速发展，展示出在各种农业应用中取得了巨大成功。然而，这些传统的机器学习/深度学习模型具有一定的局限性：它们严重依赖于昂贵的、难以获取的标记数据集进行训练，需要专业知识进行开发和维护，而且大多针对特定任务，缺乏泛化能力。最近，基础模型在语言和视觉任务中展示出了显著的成功，跨越了各个领域。这些模型在来自多个领域和模态的大量数据上进行训练。一旦训练完成，它们可以通过微调和少量特定任务的标记数据完成各种多样的任务。尽管基础模型已经证明了其有效性和巨大潜力，但在农业领域中应用尚未有太多探索。因此，本研究旨在探索基础模型在智能农业领域的潜力。

    The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agricultu
    
[^14]: ALGAN：具有调整的LSTM GAN的时间序列异常检测

    ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN. (arXiv:2308.06663v1 [cs.LG])

    [http://arxiv.org/abs/2308.06663](http://arxiv.org/abs/2308.06663)

    该论文提出了一种名为ALGAN的新型GAN模型，通过调整LSTM网络的输出，实现了在无监督设置下对单变量和多变量时间序列数据进行异常检测，并在实验中优于传统方法和其他GAN方法。

    

    时间序列数据中的异常检测是各个领域（如制造业，医学成像和网络安全）中常见的问题，旨在识别偏离正常行为的点。最近，生成对抗网络（GANs）在检测时间序列数据中的异常方面显示出了有效性。GANs的神经网络结构（即生成器和鉴别器）可以显着提高异常检测准确性。本文提出了一种新的GAN模型，名为Adjusted-LSTM GAN（ALGAN），它调整LSTM网络的输出，以提高单变量和多变量时间序列数据的异常检测能力，而且是在无监督设置下进行的。我们在46个真实世界的单变量时间序列数据集和涵盖多个领域的大型多变量数据集上评估了ALGAN的性能。我们的实验表明，ALGAN在时间序列数据的异常检测中优于传统的、基于神经网络的和其他基于GAN的方法。

    Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
    
[^15]: 极化碰撞网络: 在共享空间中使用碰撞检测有效建模行人轨迹预测

    Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks. (arXiv:2308.06654v1 [cs.RO])

    [http://arxiv.org/abs/2308.06654](http://arxiv.org/abs/2308.06654)

    在共享空间中，通过使用碰撞风险计算来选择相互作用体，可以有效建模行人轨迹预测，提高准确性。

    

    预测行人轨迹是自动驾驶车辆安全导航的关键能力，尤其是在与行人共享空间中。在共享空间中，行人的运动受到车辆和其他行人的影响。因此，有效地建模行人与行人以及行人与车辆之间的相互作用可以提高行人轨迹预测模型的准确性。尽管有关使用深度学习模型对相互作用体现在行人的预测轨迹中的大量文献，但在选择相互作用体时，很少有人对此进行了有效努力。在大多数情况下，所使用的相互作用特征主要基于相对距离，较少关注速度和接近方向对相互作用的影响。在本文中，我们提出了一种基于碰撞风险计算的启发式选择相互作用体的过程。

    Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on in
    
[^16]: 通过逐步蒸馏加速基于扩散的组合优化求解器

    Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation. (arXiv:2308.06644v1 [cs.LG])

    [http://arxiv.org/abs/2308.06644](http://arxiv.org/abs/2308.06644)

    本文提出使用逐步蒸馏来加速基于扩散的组合优化求解器，并在TSP-50数据集上展示了16倍的推理速度提升，仅有0.019%的性能降级。

    

    基于图扩散模型在生成高质量解决方案的NP完全组合优化问题方面表现出了良好的结果。然而，由于去噪扩散过程的迭代评估特性，这些模型在推理上常常效率低下。本文提出使用逐步蒸馏来加速推理过程，仅在单步内预测两个步骤之前的情况。我们的实验结果表明，经过逐步蒸馏的模型可以在TSP-50数据集上进行推理，速度快了16倍，性能仅有0.019%的降级。

    Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
    
[^17]: ADRMX: 添加混合损失的域特征加法解缠

    ADRMX: Additive Disentanglement of Domain Features with Remix Loss. (arXiv:2308.06624v1 [cs.LG])

    [http://arxiv.org/abs/2308.06624](http://arxiv.org/abs/2308.06624)

    ADRMX是一种新的架构，通过使用添加模综特征和域不变特征来解决域泛化中的限制性问题。

    

    在部署环境中，经常违反训练集和测试集遵循相似分布的常见假设。鉴于多个源域，域泛化旨在创建具有泛化到新的未见域的能力的鲁棒模型。为此，大多数现有研究侧重于提取在所有可用源域上域不变特征，以减轻域间分布变化的影响。然而，这种方法可能通过仅仅依靠在源域中找到共同特征来限制模型的泛化能力。它忽视了可能在某些域的子集中普遍存在的特定于域的特征，这些特征可能包含有价值的信息。本文提出了一种名为ADR MX的新型架构，它通过使用原始的添加模综特征和域不变特征来解决这个局限性。

    The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original ad
    
[^18]: 能否通过非结构化剪枝来减少深度神经网络的层数？

    Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?. (arXiv:2308.06619v1 [cs.LG])

    [http://arxiv.org/abs/2308.06619](http://arxiv.org/abs/2308.06619)

    本研究介绍了一种名为EGP的创新的熵引导剪枝算法，该算法能够通过优先剪除熵较低的层中的连接来有效压缩深度神经网络，同时保持其竞争性能水平。

    

    剪枝是一种广泛使用的技术，可以减小深度神经网络的大小，同时保持其性能。然而，即使是在有结构的情况下，这种技术也很难从模型中完全去除整个层：这是一个可以解决的任务吗？在这项研究中，我们引入了一种名为EGP的创新的熵引导剪枝算法，旨在减小深度神经网络的大小，同时保持其性能。EGP的关键重点是优先剪除熵较低的层中的连接，最终完全去除这些层。通过在ResNet-18和Swin-T等流行模型上进行大量实验，我们的研究结果表明，EGP能够有效压缩深度神经网络，同时保持竞争性能水平。我们的结果不仅揭示了非结构化剪枝优势背后的机制，还为进一步研究复杂的关系铺平了道路。

    Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relations
    
[^19]: 关于卷积填充与对抗鲁棒性之间的相互作用

    On the Interplay of Convolutional Padding and Adversarial Robustness. (arXiv:2308.06612v1 [cs.CV])

    [http://arxiv.org/abs/2308.06612](http://arxiv.org/abs/2308.06612)

    本研究分析了卷积填充和对抗攻击之间的相互作用，探讨了不同填充模式对对抗鲁棒性的影响。

    

    在卷积神经网络（CNN）中，常常在卷积操作之前进行填充操作，以保留特征图的分辨率。虽然有很多替代方法，但通常是通过在输入周围添加一圈零进行填充。在这项工作中，我们展示了对抗攻击通常会在图像边界出现扰动异常，而这些边界正是填充所用的区域。因此，我们旨在对填充和对抗攻击之间的相互作用进行分析，并寻找不同填充模式（或缺乏填充）对不同场景下的对抗鲁棒性产生的影响。

    It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
    
[^20]: LadleNet: 使用可扩展的两阶段U-Net将热红外图像转换为可见光图像

    LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net. (arXiv:2308.06603v1 [cs.CV])

    [http://arxiv.org/abs/2308.06603](http://arxiv.org/abs/2308.06603)

    LadleNet是一种使用可扩展的两阶段U-Net将热红外图像转换为可见光图像的算法，通过引入跳跃连接和精细特征聚合技术，显著提高了模型性能。Handle模块构建抽象语义空间，Bowl模块解码该空间生成映射的VI图像。

    

    将热红外（TIR）图像转换为可见光（VI）图像是一项具有挑战性的任务，具有潜在的应用领域，如TIR-VI图像配准和融合。利用从TIR图像转换中得到的补充信息可以显着提高模型性能和应用程序的泛化能力。然而，该领域存在的主要问题包括图像保真度不高和模型可扩展性有限。本文介绍了一种基于U-Net架构的算法LadleNet。 LadleNet采用了两阶段U-Net串联结构，增加了跳跃连接和精细特征聚合技术，从而显著提高了模型性能。LadleNet由“Handle”和“Bowl”模块组成，Handle模块用于构建抽象语义空间，而Bowl模块则解码这个抽象语义空间，生成映射的VI图像。

    The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibilit
    
[^21]: CoverNav：利用深度强化学习在非结构化室外环境中进行隐蔽导航规划

    CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning. (arXiv:2308.06594v1 [cs.RO])

    [http://arxiv.org/abs/2308.06594](http://arxiv.org/abs/2308.06594)

    本文提出了一种名为CoverNav的基于深度强化学习的算法，用于在隐蔽和可导航的路径上进行非结构化室外环境的自主导航规划。该算法通过生成海拔地图来计算本地代价，帮助机器人智能体在保持低成本轨迹的同时选择最大的隐蔽性。这对于无人地面车辆在寻找避难所和掩护的同时安全导航至目的地非常有帮助。

    

    在机器人领域中，已经广泛研究了在越野环境中的自主导航。然而，在需要将自动驾驶车辆从外部观察者隐藏起来的隐蔽情况下的导航仍然是一个不完全开发的领域。在本文中，我们提出了一种新颖的基于深度强化学习（DRL）的算法，称为CoverNav，用于在越野地形和丛林环境中识别具有最小代价的隐蔽和可导航轨迹。CoverNav专注于无人地面车辆在安全导航到预定目的地的同时寻找避难所并找到掩护。我们的DRL方法计算一个本地代价地图，通过从3D点云数据、机器人的姿态和目标导向信息生成的海拔地图来区分哪条路径将提供最大的隐蔽性，同时保持低成本轨迹。CoverNav帮助机器人智能体使用奖励函数学习低海拔地形，同时对它进行惩罚。

    Autonomous navigation in offroad environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an underexplored area. In this paper, we propose a novel Deep Reinforcement Learning (DRL) based algorithm, called CoverNav, for identifying covert and navigable trajectories with minimal cost in offroad terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. CoverNav helps robot agents to learn the low elevation terrain using a reward function while penalizing it propor
    
[^22]: 基于价值分布模型的强化学习

    Value-Distributional Model-Based Reinforcement Learning. (arXiv:2308.06590v1 [cs.LG])

    [http://arxiv.org/abs/2308.06590](http://arxiv.org/abs/2308.06590)

    该论文介绍了一种基于价值分布模型的强化学习方法，该方法通过学习后验分布来解决决策任务中的政策不确定性问题。所提出的算法能够有效地优化策略，在连续控制任务中表现出性能优势。

    

    在解决顺序决策任务中，量化政策长期绩效的不确定性是很重要的。我们从基于模型的贝叶斯强化学习的角度研究这个问题，目标是学习由马尔科夫决策过程的参数（认知）不确定性引发的值函数的后验分布。以往的研究将分析限制在少数分布值上，或者约束分布形状，例如，高斯分布。受到分布式强化学习的启发，我们引入一个Bellman算子，其固定点是值分布函数。基于我们的理论，我们提出了Epistemic Quantile-Regression（EQR），这是一种基于模型的算法，可以学习一个值分布函数用于策略优化。在几个连续控制任务上的评估结果显示相对于已有的基于模型和基于模型的算法，EQR具有性能优势。

    Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
    
[^23]: 图查询的近似答案

    Approximate Answering of Graph Queries. (arXiv:2308.06585v1 [cs.LG])

    [http://arxiv.org/abs/2308.06585](http://arxiv.org/abs/2308.06585)

    本章概述了几种方法，用于在知识图谱不完整的情况下回答查询。这些方法涵盖了不同的查询类型和图类型，并具有不同的推理能力。

    

    知识图谱（KG）由于世界知识的不完整和输入KG的偏见而本质上是不完整的。此外，世界知识不断扩展和发展，使现有事实过时或引入新的事实。然而，我们仍然希望能够像图谱完整一样回答查询。在本章中，我们将概述几种已经提出的方法，以在这种情况下回答查询。首先，我们将概述这些方法支持的不同查询类型和通常用于评估的数据集，以及对它们的限制的了解。然后，我们将概述不同的方法，并以表达能力、支持的图类型和推理能力来描述它们。

    Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.
    
[^24]: 基于人类行为的个性化膳食推荐与菜单规划社交系统

    Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System. (arXiv:2308.06549v1 [cs.HC])

    [http://arxiv.org/abs/2308.06549](http://arxiv.org/abs/2308.06549)

    提出了一种基于人类行为的个性化膳食推荐与菜单规划社交系统，该系统通过脑电图信号识别人们对餐食的情感，并考虑营养需求和社交情感来规划菜单。

    

    传统的饮食推荐系统基本上只考虑营养或健康，忽略了人们对食物的感受。人们在食欲上有所差异，不同的心情下并不是所有食物都令人愉悦。基于问卷和偏好的餐食推荐系统可以是一个解决方案。然而，自动识别不同食物对社交情感的影响，并考虑营养需求和社交情感来规划菜单，具有问卷和偏好餐食推荐的一些显著优势。严重疾病的患者、昏迷中的人或患有锁定综合征和肌萎缩性侧索硬化症（ALS）的患者无法表达他们的餐食偏好。因此，提出的框架包括一个社交情感计算模块，可以通过脑电图信号来识别不同餐食的情感。

    The traditional dietary recommendation systems are basically nutrition or health-aware where the human feelings on food are ignored. Human affects vary when it comes to food cravings, and not all foods are appealing in all moods. A questionnaire-based and preference-aware meal recommendation system can be a solution. However, automated recognition of social affects on different foods and planning the menu considering nutritional demand and social-affect has some significant benefits of the questionnaire-based and preference-aware meal recommendations. A patient with severe illness, a person in a coma, or patients with locked-in syndrome and amyotrophic lateral sclerosis (ALS) cannot express their meal preferences. Therefore, the proposed framework includes a social-affective computing module to recognize the affects of different meals where the person's affect is detected using electroencephalography signals. EEG allows to capture the brain signals and analyze them to anticipate affect
    
[^25]: 在城市区域中使用极限梯度提升、土地覆盖和地形参数进行数字高程模型校正

    Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters. (arXiv:2308.06545v1 [cs.LG])

    [http://arxiv.org/abs/2308.06545](http://arxiv.org/abs/2308.06545)

    本研究采用了极限梯度提升算法来提高城市区域中数字高程模型的精度，并结合土地覆盖和地形参数校正，以解决DEM在城市环境建模中的质量和适用性问题。

    

    在城市区域中，数字高程模型（DEMs）的精度受到土地覆盖和地形不规则性等多种因素的影响。此外，全局DEM中的建筑物人工阻塞了地表水流路径，降低了DEM的质量和适用性，而在精确和准确的地形信息在城市景观中需要进行水文和环境建模。本研究采用了极限梯度提升（XGBoost）集成算法来提高两个中分辨率30米DEM（Copernicus GLO-30和ALOS World 3D）在南非开普敦的精度。XGBoost是一个可扩展、可移植和多功能的梯度提升库，可以解决许多环境建模问题。训练数据集包括十一个预测变量，包括高程、城市轮廓、坡度、坡向、表面粗糙度、地形位置指数、地形崎岖指数、地表纹理、矢量粗糙度测量等。

    The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, f
    
[^26]: 解决医学影像深度学习中的小型注释数据集问题：对比共同对比学习和掩码自编码器方法在CT扫描卷积模型中的自监督预训练的评估

    Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])

    [http://arxiv.org/abs/2308.06534](http://arxiv.org/abs/2308.06534)

    本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。

    

    医学影像中的深度学习有潜力减少诊断错误的风险、减轻放射科医生的工作负担并加速确诊。训练这样的深度学习模型需要大型且准确的数据集，并且需要为所有训练样本提供注释。然而，在医学影像领域，由于注释的高复杂性、受限的获取方式或疾病的罕见性，特定任务的注释数据集通常很小。为了应对这一挑战，深度学习模型可以使用自监督学习领域的方法，在没有注释的大型图像数据集上进行预训练。在预训练之后，小型的已注释数据集就足以对模型进行特定任务的微调，即所谓的“下游任务”。医学影像中最流行的自监督预训练方法基于共同对比学习。然而，最近的自然图像处理研究表明掩码自编码器方法具有很大的潜力。本研究比较了二者在CT扫描卷积模型中的性能。

    Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
    
[^27]: 基于知识蒸馏的静默语音界面的sEMG集成模型研究

    Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface. (arXiv:2308.06533v1 [eess.AS])

    [http://arxiv.org/abs/2308.06533](http://arxiv.org/abs/2308.06533)

    我们提出了一种基于知识蒸馏的轻量级深度学习集成模型，用于sEMG-based SSIs，通过对26个北约音标数据集进行分类，实现了英文单词的拼写生成。实验表明该模型的高准确率，为便携、实用设备的端到端系统开辟了新的可能性。

    

    语音障碍影响着全球数百万人。基于表面肌电图的静默语音界面（sEMG-based SSIs）已被研究了几十年作为潜在的解决方案。然而，以往的研究受到了词汇量有限和从原始数据中手动提取特征的限制。为了解决这些局限性，我们提出了一种轻量级的基于知识蒸馏的sEMG集成模型（KDE-SSI）。我们的模型可以对包含3900个数据样本的26个北约音标数据集进行分类，从而通过拼写方式生成任何英文单词。广泛的实验证实了KDE-SSI的有效性，实现了85.9%的测试准确率。我们的研究结果还为便携、实用设备的端到端系统提供了启示。

    Voice disorders affect millions of people worldwide. Surface electromyography-based Silent Speech Interfaces (sEMG-based SSIs) have been explored as a potential solution for decades. However, previous works were limited by small vocabularies and manually extracted features from raw data. To address these limitations, we propose a lightweight deep learning knowledge-distilled ensemble model for sEMG-based SSI (KDE-SSI). Our model can classify a 26 NATO phonetic alphabets dataset with 3900 data samples, enabling the unambiguous generation of any English word through spelling. Extensive experiments validate the effectiveness of KDE-SSI, achieving a test accuracy of 85.9\%. Our findings also shed light on an end-to-end system for portable, practical equipment.
    
[^28]: 通过任务分解学习抽象视觉推理：基于Raven渐进矩阵的案例研究

    Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices. (arXiv:2308.06528v1 [cs.AI])

    [http://arxiv.org/abs/2308.06528](http://arxiv.org/abs/2308.06528)

    通过任务分解学习抽象视觉推理，提出了一种基于变形器蓝图的深度学习架构，该架构预测单个对象及其排列的视觉特性，通过多维预测来选择答案。

    

    学习进行抽象推理的挑战之一是问题通常被提出为整体任务，没有中间目标。在Raven渐进矩阵（RPM）中，任务是在给定上下文的情况下选择一个可用答案，其中上下文和答案都是复合图像，具有多个对象以及各种空间安排。由于只有这个高级目标作为指导，学习变得具有挑战性，大多数现代解决方案往往不透明。在本研究中，我们提出了一种基于变形器蓝图的深度学习架构，该架构不直接进行上述选择，而是预测单个对象及其排列的视觉特性。通过这种方式获得的多维预测直接并置以选择答案。我们考虑了模型将视觉输入解析为令牌的几种方式，并采用了几种自监督训练中输入的屏蔽方法。

    One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assess
    
[^29]: SLoRA: 联邦参数高效微调语言模型

    SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models. (arXiv:2308.06522v1 [cs.LG])

    [http://arxiv.org/abs/2308.06522](http://arxiv.org/abs/2308.06522)

    SLoRA是一种联邦参数高效微调语言模型的方法，用于在联邦学习中利用分布式和私有数据进行微调，以克服高异构数据下的性能差距。

    

    通过微调预训练的转换器模型进行迁移学习，在各种自然语言处理任务中取得了显著的成功。在没有集中式数据的情况下，联邦学习可以从联邦学习边缘客户端的分布式和私有数据中受益。然而，由于边缘设备的有限通信、计算和存储能力以及流行的转换器模型的巨大大小，高效的微调对于使联邦训练成为可行的是至关重要的。本文探讨了在不同的联邦学习设置下应用参数高效微调方法（PEFT）在语言任务中的机遇和挑战。具体而言，我们的研究发现，随着用户之间的数据越来越多样化，全面微调模型和使用PEFT方法之间的差距变大。为了弥补这个性能差距，我们提出了一种称为SLoRA的方法，它克服了高异构数据下LoRA的关键限制。

    Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data
    
[^30]: 资源受限的分散式联邦学习在无线网络中的性能分析

    Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks. (arXiv:2308.06496v1 [cs.LG])

    [http://arxiv.org/abs/2308.06496](http://arxiv.org/abs/2308.06496)

    本研究对资源受限的分散式联邦学习在无线网络中的性能进行了分析，研究了不同的通信方案并优化了通信效率。研究结果包括数字和模拟传输方法的收敛界限、资源分配和收敛速度，以及模拟传输中信道衰落和噪声对模型性能的影响。

    

    联邦学习可以导致显著的通信开销并依赖于中央服务器。为了解决这些挑战，提出了分散式联邦学习作为一种更具弹性的框架。分散式联邦学习涉及通过无线网络在设备之间进行参数交换。本研究分析了在无线网络上使用不同通信方案（数字和模拟）进行资源受限的分散式联邦学习的性能，以优化通信效率。具体而言，我们提供了数字和模拟传输方法的收敛界限，使得可以分析在分散式联邦学习上训练的模型性能。此外，对于数字传输，我们研究和分析了计算和通信之间的资源分配以及收敛速度，得到了其通信复杂度和收敛保证所需的最小纠错通信概率。对于模拟传输，我们讨论了信道衰落和噪声对模型性能的影响。

    Federated learning (FL) can lead to significant communication overhead and reliance on a central server. To address these challenges, decentralized federated learning (DFL) has been proposed as a more resilient framework. DFL involves parameter exchange between devices through a wireless network. This study analyzes the performance of resource-constrained DFL using different communication schemes (digital and analog) over wireless networks to optimize communication efficiency. Specifically, we provide convergence bounds for both digital and analog transmission approaches, enabling analysis of the model performance trained on DFL. Furthermore, for digital transmission, we investigate and analyze resource allocation between computation and communication and convergence rates, obtaining its communication complexity and the minimum probability of correction communication required for convergence guarantee. For analog transmission, we discuss the impact of channel fading and noise on the mo
    
[^31]: 基于同质音频-文本嵌入的灵活关键词检测

    Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding. (arXiv:2308.06472v1 [cs.SD])

    [http://arxiv.org/abs/2308.06472](http://arxiv.org/abs/2308.06472)

    本文提出了一种基于同质音频-文本嵌入的灵活关键词检测方法，通过使用小型的音频-compliant文本编码器，将文本转化为音素表示并与音频进行联合分析。实验结果表明，该方法在关键词检测上表现出色，优于当前最先进的方法。

    

    在频繁使用文本来表示用户自定义/灵活关键词的场景中，通常使用昂贵的文本编码器与音频编码器在嵌入空间进行联合分析，但这可能导致模态表示不一致（即较大的不匹配）和增加复杂性。本文提出了一种新颖的架构，通过基于符合音频的文本编码器来高效检测任意关键词，这个编码器在音频嵌入方面具有同质的表示，且比兼容的文本编码器小得多。我们的文本编码器使用字素到音素（G2P）模型将文本转化为音素，然后使用从已配对的音频编码器提取的代表性音素向量将其转化为嵌入。此外，我们还通过产生混淆关键词来进一步增强我们的方法，从而开发出具有强大辨别能力的音频-文本嵌入验证器。实验结果表明，我们的方案在Libriphrase难度数据集上优于最先进的结果。

    Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (i.e., large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a grapheme-to-phoneme (G2P) model, and then to an embedding using representative phoneme vectors, extracted from the paired audio encoder on rich speech datasets. We further augment our method with confusable keyword generation to develop an audio-text embedding verifier with strong discriminative power. Experimental results show that our scheme outperforms the state-of-the-art results on Libriphrase hard datas
    
[^32]: Volterra强调非线性动力学可通量性 (VANYA) 在森林砍伐模拟中的应用：以亚马逊雨林为例

    Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest. (arXiv:2308.06471v1 [cs.LG])

    [http://arxiv.org/abs/2308.06471](http://arxiv.org/abs/2308.06471)

    本文研究了利用Volterra强调非线性动力学可通量性 (VANYA) 模型来模拟森林砍伐，该模型结合了捕食者-被捕食者动力学，并在对亚马逊雨林数据进行了预测，并与其他预测方法进行了比较。

    

    智能自动化技术通过其最新的技术进展，在抵御飓风、干旱和地震等方面给予了我们支持。算法学习已经推动了神经科学、遗传学和人机交互等领域的发展。时间序列数据对进展起到了促进作用。在传统领域中采用这些方法仍存在挑战。神经网络面临理解和偏见问题。人工智能在科学领域的扩展是由于其可适应的简单描述符和组合论证。本文侧重于利用VANYA模型预测林地损失，并结合捕食者-被捕食者动力学。VANYA模型对亚马逊雨林的数据进行了预测，并与其他预测方法（如长短期记忆、N-BEATS和RCN）进行了对比。

    Intelligent automation supports us against cyclones, droughts, and seismic events with recent technology advancements. Algorithmic learning has advanced fields like neuroscience, genetics, and human-computer interaction. Time-series data boosts progress. Challenges persist in adopting these approaches in traditional fields. Neural networks face comprehension and bias issues. AI's expansion across scientific areas is due to adaptable descriptors and combinatorial argumentation. This article focuses on modeling Forest loss using the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest cover, demonstrated on Amazon Rainforest data against other forecasters like Long Short-Term Memory, N-BEATS, RCN.
    
[^33]: 边缘检测泛化的小而高效的模型

    Tiny and Efficient Model for the Edge Detection Generalization. (arXiv:2308.06468v1 [cs.CV])

    [http://arxiv.org/abs/2308.06468](http://arxiv.org/abs/2308.06468)

    该论文提出了一种小而高效的边缘检测模型TEED，它具有极低的复杂度和参数数量，能够在短时间内训练并且能够产生高质量的边缘图像。同时，还提出了一个新的数据集用于测试边缘检测的泛化能力。

    

    大多数高级计算机视觉任务依赖于低级图像操作作为其初始过程。诸如边缘检测、图像增强和超分辨率等操作为更高级的图像分析提供了基础。在这项工作中，我们考虑了三个主要目标，即简单性、效率性和泛化性，以解决边缘检测问题，因为当前最先进的边缘检测模型为了提高准确性而变得更加复杂。为了实现这一目标，我们提出了一种名为Tiny and Efficient Edge Detector (TEED)的轻量级卷积神经网络，仅有58K个参数，是当前最先进模型的0.2%。在BIPED数据集上训练仅需不到30分钟，每个epoch耗时不到5分钟。我们提出的模型易于训练，并在最早几个epoch内快速收敛，同时预测的边缘图像清晰且质量较高。此外，我们提出了一个新的数据集来测试边缘检测的泛化能力。

    Most high-level computer vision tasks rely on low-level image operations as their initial processes. Operations such as edge detection, image enhancement, and super-resolution, provide the foundations for higher level image analysis. In this work we address the edge detection considering three main objectives: simplicity, efficiency, and generalization since current state-of-the-art (SOTA) edge detection models are increased in complexity for better accuracy. To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light convolutional neural network with only $58K$ parameters, less than $0.2$% of the state-of-the-art models. Training on the BIPED dataset takes $less than 30 minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model is easy to train and it quickly converges within very first few epochs, while the predicted edge-maps are crisp and of high quality. Additionally, we propose a new dataset to test the generalization of edge detection, which c
    
[^34]: 并不那么强大：评估深度神经网络对未知对抗攻击的鲁棒性

    Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks. (arXiv:2308.06467v1 [cs.LG])

    [http://arxiv.org/abs/2308.06467](http://arxiv.org/abs/2308.06467)

    这项研究旨在评估深度神经网络对未知对抗攻击的鲁棒性，并挑战针对这些攻击的现有防御机制的有效性。实验结果表明，在仅使用鲁棒特征的数据集上训练的DNN模型并不一定能抵抗对抗性攻击。

    

    深度神经网络(DNNs)在各种应用中取得了显著的成就，如分类、识别和预测，这引起了对其属性的增加关注。传统DNN的一个基本属性是它们对输入数据的修改的脆弱性，这导致了对对抗攻击的调查。这些攻击通过操纵数据来误导DNN。本研究旨在挑战针对对抗攻击的当代防御机制的有效性和泛化性。具体而言，我们探讨了Ilyas等人提出的假设，即DNN的图像特征可以是鲁棒的或非鲁棒的，而对抗性攻击针对的是后者。该假设认为，仅在由鲁棒特征组成的数据集上训练DNN应该产生对抗攻击具有抵抗力的模型。然而，我们的实验表明这并不普遍成立。为了进一步了解我们的发现，我们对我们的实验结果进行了分析。

    Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the imp
    
[^35]: 一种使用预测运动矢量最优性的一维HEVC视频隐写分析方法

    A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors. (arXiv:2308.06464v1 [cs.CR])

    [http://arxiv.org/abs/2308.06464](http://arxiv.org/abs/2308.06464)

    本文提出一种基于一维预测运动矢量最优性的HEVC视频隐写分析方法，通过分析和定义最优比特率作为隐写分析特征，提高了检测性能。

    

    在隐写分析技术中，对基于运动矢量（MV）域的HEVC视频隐写术的检测仍然是一个热门和具有挑战性的问题。为了提高检测性能，本文提出了一种基于一维预测MV最优性的隐写分析特征。首先，我们指出使用高级运动矢量预测（AMVP）技术编码的预测单元（PU）的运动矢量预测（MVP）满足封面视频中的局部最优性。其次，我们分析HEVC视频中使用MVP索引或运动矢量差（MVD）进行消息嵌入可能破坏上述MVP的最优性。然后，我们定义了HEVC视频中MVP的最优比特率作为一种隐写分析特征。最后，我们在两个常见数据集上对三种流行隐写方法进行了隐写分析检测实验，并与四种最先进的隐写分析方法进行了性能比较。

    Among steganalysis techniques, detection against motion vector (MV) domain-based video steganography in High Efficiency Video Coding (HEVC) standard remains a hot and challenging issue. For the purpose of improving the detection performance, this paper proposes a steganalysis feature based on the optimality of predicted MVs with a dimension of one. Firstly, we point out that the motion vector prediction (MVP) of the prediction unit (PU) encoded using the Advanced Motion Vector Prediction (AMVP) technique satisfies the local optimality in the cover video. Secondly, we analyze that in HEVC video, message embedding either using MVP index or motion vector differences (MVD) may destroy the above optimality of MVP. And then, we define the optimal rate of MVP in HEVC video as a steganalysis feature. Finally, we conduct steganalysis detection experiments on two general datasets for three popular steganography methods and compare the performance with four state-of-the-art steganalysis methods. 
    
[^36]: 多标签知识蒸馏

    Multi-Label Knowledge Distillation. (arXiv:2308.06453v1 [cs.LG])

    [http://arxiv.org/abs/2308.06453](http://arxiv.org/abs/2308.06453)

    这篇论文提出了一种专门针对多标签学习的新颖知识蒸馏方法，通过将多标签问题分解为一组二分类问题以利用逻辑回归的信息性语义知识，并通过利用标签嵌入的结构信息增强学习到的特征表示的独特性。

    

    现有的知识蒸馏方法通常通过将教师网络的输出逻辑回归或中间特征映射传授给学生网络来工作，在多类单标签学习中非常成功。然而，在多标签学习场景下，这些方法很难推广，每个实例都与多个语义标签相关，因为预测概率不等于一，并且在这种情况下，整个示例的特征映射可能会忽略小类别。在本文中，我们提出了一种新颖的多标签知识蒸馏方法。一方面，它通过将多标签学习问题分解为一组二分类问题来利用逻辑回归中的信息性语义知识；另一方面，它通过利用标签嵌入的结构信息来增强学习到的特征表示的独特性。多个基准数据集上的实验结果验证了该方法的有效性。

    Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the pr
    
[^37]: 潜在随机步骤作为最大割、最小割等问题的放松方法

    Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More. (arXiv:2308.06448v1 [cs.LG])

    [http://arxiv.org/abs/2308.06448](http://arxiv.org/abs/2308.06448)

    该论文提出了一种基于非负矩阵分解的概率模型，将聚类和简化结合起来，并提供了建模任意图结构的框架。通过将硬聚类放松为软聚类，该算法将可能困难的聚类问题放松为可解的问题。

    

    节点聚类算法通常关注于在图中寻找同质结构。也就是说，它们寻找具有许多边缘的相似节点集合，而不是跨集群。然而，图通常还展现出不同种结构，例如近似二分和三分图，其中大部分边缘发生在集群之间。解决这种结构通常留给图简化的任务。我们提出了一个基于非负矩阵分解的概率模型，它统一了聚类和简化，并提供了建模任意图结构的框架。我们的模型基于对图上进行随机游走的过程进行分解。它允许无限制的参数化，通过简单的梯度下降进行优化。通过将硬聚类放松为软聚类，我们的算法将可能困难的聚类问题放松为可解的问题。我们在一个合成数据集上展示了我们算法的能力。

    Algorithms for node clustering typically focus on finding homophilous structure in graphs. That is, they find sets of similar nodes with many edges within, rather than across, the clusters. However, graphs often also exhibit heterophilous structure, as exemplified by (nearly) bipartite and tripartite graphs, where most edges occur across the clusters. Grappling with such structure is typically left to the task of graph simplification. We present a probabilistic model based on non-negative matrix factorization which unifies clustering and simplification, and provides a framework for modeling arbitrary graph structure. Our model is based on factorizing the process of taking a random walk on the graph. It permits an unconstrained parametrization, allowing for optimization via simple gradient descent. By relaxing the hard clustering to a soft clustering, our algorithm relaxes potentially hard clustering problems to a tractable ones. We illustrate our algorithm's capabilities on a synthetic
    
[^38]: 顺序元转移（SMT）学习用于应对物理信息神经网络的复杂性：在复合材料热压过程中的应用

    A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing. (arXiv:2308.06447v1 [cs.LG])

    [http://arxiv.org/abs/2308.06447](http://arxiv.org/abs/2308.06447)

    本论文提出了一种顺序元转移（SMT）学习框架，用于解决物理信息神经网络（PINNs）在高度非线性系统中的训练和适应问题，提供了一种统一的、快速的解决方案。

    

    物理信息神经网络（PINNs）通过将物理定律融入神经网络的训练中，已经在解决非线性偏微分方程（PDEs）方面变得越来越流行，并在许多科学和工程应用中表现出优越性。然而，传统的PINNs在准确逼近具有强非线性的复杂系统的解方面仍然存在一定的局限性，尤其是在长时间域下。此外，由于PINNs被设计为逼近给定PDE系统的特定实现，它们缺乏必要的通用性来有效适应新系统配置。这就需要在系统发生任何变化时进行从头重新训练，计算成本较高。为了解决这些问题，在这项工作中提出了一种新颖的顺序元转移（SMT）学习框架，为高度非线性系统的PINNs提供了快速训练和高效适应的统一解决方案。

    Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework deco
    
[^39]: 神经潜在对齐器：用于学习复杂自然神经数据表示的跨试验对齐

    Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data. (arXiv:2308.06443v1 [cs.LG])

    [http://arxiv.org/abs/2308.06443](http://arxiv.org/abs/2308.06443)

    本研究提出了一种名为神经潜在对齐器的框架，用于学习复杂行为的神经表示。通过对齐重复试验的表示，并使用时间弯曲模型解决时间不对齐问题，该模型能够在降维空间中学习到更好的行为解码表示。

    

    理解复杂人类行为的神经实现是神经科学的主要目标之一。为此，寻找神经数据的真实表示是至关重要的，这由于行为的高复杂性和信号到噪声比（SNR）低而具有挑战性。本文提出了一种新的无监督学习框架，神经潜在对齐器（NLA），用于找到具有行为相关性的受限神经表示。其关键思想是通过对齐重复试验的表示来学习跨试验一致的信息。此外，我们提出了一种新颖的全可微分时间弯曲模型（TWM），以解决试验之间的时间不对齐问题。当应用于自然言语的颅内脑电图（ECoG）时，我们的模型在降维空间中比基准模型更好地学习解码行为的表示。通过测量对齐和行为之间的关联性，TWM在实验证实了。

    Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space. The TWM is empirically validated by measuring behavioral coherence between align
    
[^40]: 一个面向域自适应的基于物理知识的神经网络用于求解异质介质下麦克斯韦方程的反问题

    A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell's Equations in Heterogeneous Media. (arXiv:2308.06436v1 [cs.LG])

    [http://arxiv.org/abs/2308.06436](http://arxiv.org/abs/2308.06436)

    本文提出了一个面向域自适应的基于物理知识的神经网络（da-PINN），用于解决异质介质中麦克斯韦方程的反问题。通过将介质界面位置参数与损失函数结合，并采用面向域自适应的训练策略，da-PINN在求解麦克斯韦方程中取得了显著的效果。

    

    麦克斯韦方程是一组耦合的偏微分方程（PDEs），与洛伦兹力法则一起构成了经典电磁学和电路的基础。在诸如电磁散射和天线设计优化等领域中，有效求解麦克斯韦方程至关重要。物理知识导向的神经网络（PINNs）在求解PDEs方面表现出强大的能力。然而，PINNs在异质介质中求解麦克斯韦方程仍存在困难。为此，我们提出了一种面向域自适应的PINN（da-PINN），用于求解异质介质下麦克斯韦方程的反问题。首先，我们提出介质界面的位置参数，将整个域分解为多个子域。此外，将电磁界面条件结合到损失函数中，以提高接近界面处的预测性能。然后，我们提出了一种面向域自适应的训练策略用于da-PINN。最后，我们验证了da-PINN的有效性。

    Maxwell's equations are a collection of coupled partial differential equations (PDEs) that, together with the Lorentz force law, constitute the basis of classical electromagnetism and electric circuits. Effectively solving Maxwell's equations is crucial in various fields, like electromagnetic scattering and antenna design optimization. Physics-informed neural networks (PINNs) have shown powerful ability in solving PDEs. However, PINNs still struggle to solve Maxwell's equations in heterogeneous media. To this end, we propose a domain-adaptive PINN (da-PINN) to solve inverse problems of Maxwell's equations in heterogeneous media. First, we propose a location parameter of media interface to decompose the whole domain into several sub-domains. Furthermore, the electromagnetic interface conditions are incorporated into a loss function to improve the prediction performance near the interface. Then, we propose a domain-adaptive training strategy for da-PINN. Finally, the effectiveness of da-
    
[^41]: 学习单时间段疾病演变，用于预测处理后新生血管性年龄相关黄斑变性的生成（arXiv:2308.06432v1 [eess.IV]）

    Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration. (arXiv:2308.06432v1 [eess.IV])

    [http://arxiv.org/abs/2308.06432](http://arxiv.org/abs/2308.06432)

    提出了一种用于预测处理后新生血管性年龄相关黄斑变性的单时间段疾病演变网络(SHENet)，可以通过输入术前SD-OCT图像预测生成术后SD-OCT图像。

    

    在医学图像处理领域，大部分现有的疾病预测方法可以归为两类，一类是图像到类别的预测，一类是图像到参数的预测。鲜有研究专注于图像到图像的预测。与其他领域的多时间段预测不同，眼科医生更倾向于对单时间段预测表示更大的信心，因为他们对预测风险的容忍度较低。本文提出了一种单时间段疾病演变网络（SHENet），通过输入带有新生血管性年龄相关黄斑变性（nAMD）的术前SD-OCT图像，预测生成术后SD-OCT图像。在SHENet中，特征编码器将输入的SD-OCT图像转换为深层特征，然后图形演变模块在高维潜在空间中预测疾病演变过程并输出预测的深层特征，最后特征解码器将预测的深层特征恢复为SD-OCT图像。我们进一步提出了一种演变强化模块

    Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk. We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pre-therapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement modul
    
[^42]: 使用遗传算法和网络科学进行遗传异质性分析

    Genetic heterogeneity analysis using genetic algorithm and network science. (arXiv:2308.06429v1 [cs.LG])

    [http://arxiv.org/abs/2308.06429](http://arxiv.org/abs/2308.06429)

    本论文介绍了一种使用遗传算法和网络科学进行遗传异质性分析的新方法，通过构建基于多个独立特征选择运行的网络来提取遗传变量的异质子集，并引入了综合特征风险评分来解释基因变量之间的关联。

    

    通过基因组关联研究(GWAS)，我们可以通过比较患有特定疾病和未患病个体的基因数据来确定疾病易感的基因变量。然而，这些关联的发现由于遗传异质性和特征交互而面临着重大挑战。这些受到这些影响的基因变量通常表现出较低的效应大小，因此使用机器学习特征选择方法很难检测到它们。为了解决这些挑战，本文介绍了一种新颖的GWAS特征选择机制，名为Feature Co-selection Network (FCSNet)。FCS-Net旨在从基于遗传算法(GA)和进化学习算法的多个独立特征选择运行构建的网络中提取遗传变量的异质子集。我们采用非线性机器学习算法来检测特征交互。我们引入了一个综合特征风险评分(CRS)来解释基因变量之间的关联。

    Through genome-wide association studies (GWAS), disease susceptible genetic variables can be identified by comparing the genetic data of individuals with and without a specific disease. However, the discovery of these associations poses a significant challenge due to genetic heterogeneity and feature interactions. Genetic variables intertwined with these effects often exhibit lower effect-size, and thus can be difficult to be detected using machine learning feature selection methods. To address these challenges, this paper introduces a novel feature selection mechanism for GWAS, named Feature Co-selection Network (FCSNet). FCS-Net is designed to extract heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA), an evolutionary learning algorithm. We employ a non-linear machine learning algorithm to detect feature interaction. We introduce the Community Risk Score (CRS), a synthetic feature de
    
[^43]: 学习能力与样本压缩并不相同的多类别问题

    Multiclass Learnability Does Not Imply Sample Compression. (arXiv:2308.06424v1 [cs.LG])

    [http://arxiv.org/abs/2308.06424](http://arxiv.org/abs/2308.06424)

    学习二元假设类具有样本压缩方案，而多类别假设类则不具备这个性质。

    

    如果一个假设类能够通过只保留一个小的子样本推断出整个样本的标签，那么它就具有样本压缩方案。学习二元假设类（必须具有有限的VC维度）都可以通过VC维度的一个有限函数大小的样本压缩方案实现。然而，对于多类别假设类来说，DS维度是相对应的，我们发现学习多类别假设类（必须具有有限的DS维度）并不能通过一个DS维度的有限函数大小的样本压缩方案实现。

    A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.
    
[^44]: 使用基于聚类的树状Parzen估计的敏感性感知混合精度量化和宽度优化的深度神经网络

    Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])

    [http://arxiv.org/abs/2308.06422](http://arxiv.org/abs/2308.06422)

    本研究引入了一种创新的深度神经网络优化方法，通过自动选择最佳的位宽和层宽来提高网络效率。同时，通过剪枝和聚类技术，优化了搜索过程，并在多个数据集上进行了严格测试，结果显示该方法明显优于现有方法。

    

    随着深度学习模型的复杂性和计算需求的提高，对神经网络设计的有效优化方法的需求变得至关重要。本文引入了一种创新的搜索机制，用于自动选择单个神经网络层的最佳位宽和层宽。这导致深度神经网络效率的明显提高。通过利用基于Hessian的剪枝策略，有选择地减少搜索域，确保移除非关键参数。随后，我们通过使用基于聚类的树状Parzen估计器开发有利和不利结果的替代模型。这种策略允许对架构可能性进行简化的探索，并迅速确定表现最好的设计。通过对知名数据集进行严格测试，我们的方法证明了与现有方法相比的明显优势。与领先的压缩策略相比，我们的方法取得了令人瞩目的成果。

    As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
    
[^45]: 行人-车辆混合环境下的行人轨迹预测:一项系统综述

    Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review. (arXiv:2308.06419v1 [cs.RO])

    [http://arxiv.org/abs/2308.06419](http://arxiv.org/abs/2308.06419)

    本文系统综述了行人-车辆混合环境下的行人轨迹预测方法，探讨了车辆和行人的相互作用对行人未来行为的影响，并回顾了先前提出的预测模型中如何考虑不确定性和行为差异。

    

    在与行人共享空间的自动驾驶车辆（AV）路径规划中，需要推理出行人的未来轨迹。为了应用于AV的实际行人轨迹预测算法，需要考虑车辆与行人的相互作用对行人未来运动行为的影响。在这方面，本文系统综述了文献中针对在存在车辆的非结构化环境中对行人轨迹预测进行建模的不同方法。本文还探讨了与行人-行人交互相比，行人-车辆交互的特定考虑，并回顾了先前提出的预测模型中如何考虑预测不确定性和行为差异等不同变量。遵循PRISMA指南。本文还考察了如何在先前提出的预测模型中考虑预测不确定性和行为差异等不同变量。

    Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focu
    
[^46]: 通过混合效应模型和层次聚类学习具有异构农业数据集的贝叶斯网络

    Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])

    [http://arxiv.org/abs/2308.06399](http://arxiv.org/abs/2308.06399)

    本研究介绍了一种将混合效应模型和层次聚类应用于贝叶斯网络学习的新方法，在农学研究中广泛应用。通过整合随机效应，该方法可以提高贝叶斯网络的结构学习能力，实现因果关系网络的发现。

    

    在涉及多样但相关数据集的研究中，其中协变量与结果之间的关联可能会有所不同，在包括农学研究在内的各个领域都很普遍。在这种情况下，常常使用层次模型，也被称为多层模型，来融合来自不同数据集的信息，并适应它们的不同特点。然而，它们的结构超出了简单的异质性，因为变量通常形成复杂的因果关系网络。贝叶斯网络（BNs）使用有向无环图来模拟这种关系的强大框架。本研究介绍了一种将随机效应整合到BN学习中的新方法。这种方法基于线性混合效应模型，特别适用于处理层次数据。来自真实农学试验的结果表明，采用这种方法可以增强结构学习，从而实现发现

    Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
    
[^47]: 在大型视觉语言模型中检测和预防幻觉

    Detecting and Preventing Hallucinations in Large Vision Language Models. (arXiv:2308.06394v1 [cs.CV])

    [http://arxiv.org/abs/2308.06394](http://arxiv.org/abs/2308.06394)

    本论文提出了一个用于训练和评估模型的多模态幻觉检测数据集，以解决大型视觉语言模型中存在的幻觉文本问题。这是第一个用于详细图像描述的全面多模态幻觉检测数据集。

    

    经过调整的大型视觉语言模型（LVLMs）在泛化跨多种多模态任务方面取得了显著进展，特别是在视觉问答（VQA）方面。然而，为这些模型生成与视觉相关的详细回答仍然是一个具有挑战性的任务。我们发现，即使是当前最先进的LVLM（InstructBLIP）仍然存在着惊人的30%的幻觉文本，包括不存在的对象、不忠实的描述和不准确的关系。为了解决这个问题，我们引入了M-HalDetect，这是一个用于训练和评估幻觉检测和预防模型的多模态幻觉检测数据集。M-HalDetect包含了16k个细粒度的VQA示例标签，是第一个用于详细图像描述的全面多模态幻觉检测数据集。与之前只考虑对象幻觉的工作不同，我们还注释了实体描述。

    Instruction tuned Large Vision Language Models (LVLMs) have made significant advancements in generalizing across a diverse set of multimodal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a {M}ultimodal {Hal}lucination {Detect}ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained labels on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions
    
[^48]: 通过集合扩展实现一次性语音转换的音素幻觉器

    Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion. (arXiv:2308.06382v1 [cs.SD])

    [http://arxiv.org/abs/2308.06382](http://arxiv.org/abs/2308.06382)

    本论文提出了一种名为“音素幻觉器”的一次性语音转换模型，通过集合扩展的方法，只需短时间目标说话人语音即可生成多样和高保真度的目标说话人音素，并用于基于邻居的语音转换。

    

    语音转换旨在改变一个人的声音，使其听起来与另一个人的声音相似，同时保留语言内容。现有的方法在内容可理解性和说话人相似性之间存在困境；即具有更高可理解性的方法通常具有较低的说话人相似性，而具有更高说话人相似性的方法通常需要大量目标说话人的语音数据来实现高可理解性。在这项工作中，我们提出了一种新方法“音素幻觉器”，它兼具两者的优势。音素幻觉器是一种一次性语音转换模型；它采用一种新颖的模型，仅基于较短的目标说话人语音（例如3秒）生成多样化和高保真度的目标说话人音素。然后利用生成的音素进行基于邻居的语音转换。我们的模型是一种无需文本注释的任意到任意的语音转换模型，支持转换到任何未知说话人。

    Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subje
    
[^49]: DCNFIS：深度卷积神经模糊推理系统

    DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])

    [http://arxiv.org/abs/2308.06378](http://arxiv.org/abs/2308.06378)

    本文介绍了一种新的深度卷积神经模糊推理系统（DCNFIS），它通过将模糊逻辑和深度学习模型相结合，实现了提高透明度而不损失准确性的目标。DCNFIS在准确性上与现有卷积神经网络相当，并且胜过了最先进的深度模糊系统。通过模糊规则提取的解释可以提高模型的可解释性。

    

    在可解释的人工智能中，透明度与准确性之间存在一个著名的权衡。本文介绍了一种新的深度网络设计，通过将模糊逻辑和深度学习模型相结合，实现了提高透明度但不损失准确性的目标。我们设计了一个深度卷积神经模糊推理系统（DCNFIS），并在四个著名数据集上展示了它与三个现有卷积神经网络的相同准确性。我们进一步发现，DCNFIS在性能上胜过了最先进的深度模糊系统。然后，我们利用模糊逻辑的透明度，从DCNFIS中编码的模糊规则中提取解释，以渐变映射的形式展示。我们还利用Fashion-MNIST数据集对这些解释的特性进行了深入研究。

    A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
    
[^50]: UAMM: UBET自动市场做市商

    UAMM: UBET Automated Market Maker. (arXiv:2308.06375v1 [cs.LG])

    [http://arxiv.org/abs/2308.06375](http://arxiv.org/abs/2308.06375)

    UAMM是一种新的自动市场做市商方法，通过考虑外部市场价格和流动性池的暂时损失来定价，并且有效消除了套利机会。

    

    自动市场做市商（AMM）是去中心化交易所（DEX）使用的定价机制。传统的AMM方法仅基于其自身的流动性池进行定价，而不考虑外部市场或流动性提供者的风险管理。在本文中，我们提出了一种称为UBET AMM（UAMM）的新方法，通过考虑外部市场价格和流动性池的暂时损失来计算价格。尽管依赖于外部市场价格，我们的方法在计算滑点时仍然保持了恒定产品曲线的期望属性。UAMM的关键要素是根据期望的目标余额确定合适的滑点金额，以鼓励流动性池最小化暂时损失。我们证明了当外部市场价格有效时，我们的方法消除了套利机会。

    Automated market makers (AMMs) are pricing mechanisms utilized by decentralized exchanges (DEX). Traditional AMM approaches are constrained by pricing solely based on their own liquidity pool, without consideration of external markets or risk management for liquidity providers. In this paper, we propose a new approach known as UBET AMM (UAMM), which calculates prices by considering external market prices and the impermanent loss of the liquidity pool. Despite relying on external market prices, our method maintains the desired properties of a constant product curve when computing slippages. The key element of UAMM is determining the appropriate slippage amount based on the desired target balance, which encourages the liquidity pool to minimize impermanent loss. We demonstrate that our approach eliminates arbitrage opportunities when external market prices are efficient.
    
[^51]: 基于主题的贝叶斯惊喜和意外性用于推荐系统

    Topic-Level Bayesian Surprise and Serendipity for Recommender Systems. (arXiv:2308.06368v1 [cs.IR])

    [http://arxiv.org/abs/2308.06368](http://arxiv.org/abs/2308.06368)

    本文通过引入基于主题的贝叶斯惊喜概念，提出了一种用于推荐系统的意外性模型，以解决过滤泡问题，通过识别相似用户和测量用户对物品的意外性来推荐具有高潜力的意外性物品。

    

    推荐系统优化其推荐仅适合用户对已消费物品的评级历史，这可能导致过滤泡，用户无法从新颖、未见过的类别中体验物品。我们提出了一种基于内容的意外性形式，以贝叶斯惊喜为基础，用于测量用户消费并评级后物品的意外性。结合识别相似用户的协同过滤组件，可以推荐具有高潜力意外性的物品。为了便于评估主题级别的惊喜和意外性模型，我们介绍了一个从Goodreads中提取的图书阅读历史数据集，包含超过26千个用户和近130万本书，并对其中的449篇书进行了手动注释。

    A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 
    
[^52]: 通过蒙特卡洛边缘化学习分布

    Learning Distributions via Monte-Carlo Marginalization. (arXiv:2308.06352v1 [cs.LG])

    [http://arxiv.org/abs/2308.06352](http://arxiv.org/abs/2308.06352)

    通过蒙特卡洛边缘化学习难以计算的分布，使用参数化分布模型近似，解决了KL散度计算复杂度和优化过程可微性的问题。

    

    我们提出了一种新的方法来从样本中学习难以计算的分布。主要思想是使用参数化分布模型，如高斯混合模型（GMM），通过最小化KL散度来近似难以计算的分布。基于这个思想，有两个需要解决的挑战。首先，当分布的维度增加时，KL散度的计算复杂度是不可接受的。我们提出了蒙特卡洛边缘化（MCMarg）来解决这个问题。第二个挑战是优化过程的可微性，因为目标分布是难以计算的。我们通过使用核密度估计（KDE）来处理这个问题。提出的方法是学习复杂分布的有力工具，整个过程是可微的。因此，它可以更好地替代变分自动编码器（VAE）中的变分推断。我们的方法受益的一个强有力的证据是学习的分布

    We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned
    
[^53]: 镜像扩散模型

    Mirror Diffusion Models. (arXiv:2308.06342v1 [cs.LG])

    [http://arxiv.org/abs/2308.06342](http://arxiv.org/abs/2308.06342)

    本文提出了镜像扩散模型(MDMs)，用于在离散分类数据和连续领域中进行生成任务。MDMs受限制抽样问题的镜像Langevin算法启发，并提供了适应简单扩散、图像生成和文本生成等领域的自然扩展。

    

    扩散模型在各种连续领域的生成任务中取得了成功。然而，在离散的分类数据中应用扩散仍然是一个非平凡的任务。此外，在连续领域的生成中常常需要进行剪切，这就需要一个将扩散适应约束域的理论框架。受限制抽样问题的镜像Langevin算法的启发，本理论报告中我们提出了镜像扩散模型(MDMs)。我们演示了MDMs在simplex扩散的背景下，并提出了对图像和文本生成等热门领域的自然扩展。

    Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
    
[^54]: 深度算子网络的大小下界

    Size Lowerbounds for Deep Operator Networks. (arXiv:2308.06338v1 [cs.LG])

    [http://arxiv.org/abs/2308.06338](http://arxiv.org/abs/2308.06338)

    本文建立了深度算子网络的数据依赖性大小下界，并证明了在解决偏微分方程时，支路网络和主干网络的共同输出维度需要与数据点数量按照Ω(√n)的比例扩展，并且为了获得更低的训练误差，训练数据的大小可能需要与共同输出维度按照二次比例关系扩展。

    

    深度算子网络是一种在无限维度中解决回归问题和一次解决一类偏微分方程组的流行范式。本文旨在建立一种首次依赖于数据的深度算子网络大小下界，以便能够在噪声数据上减小经验误差。具体而言，我们证明了为了获得低训练误差，需要将支路网络和主干网络的共同输出维度与数据点数量n按照Ω(√n)的比例扩展。这启发了我们在解决对流-扩散-反应偏微分方程时对深度算子网络进行的实验，我们证明了在固定模型大小的情况下，利用这种共同输出维度的增加，可以单调降低训练误差，而训练数据的大小可能需要与之呈二次比例关系。

    Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
    
[^55]: 用神经网络预测弹性

    Predicting Resilience with Neural Networks. (arXiv:2308.06309v1 [eess.SY])

    [http://arxiv.org/abs/2308.06309](http://arxiv.org/abs/2308.06309)

    本文提出了三种神经网络方法来预测系统的弹性，包括负面和正面因素的影响，对破坏性事件和恢复活动进行建模和预测。实验结果显示神经网络模型可以有效预测系统的弹性。

    

    弹性工程研究系统在遭受破坏性事件后的存活和恢复能力，在多个领域有应用。大部分研究强调弹性度量来量化系统性能，而最近的研究提出了统计建模方法来预测系统在退化后的恢复时间。此外，过去的研究要么在恢复后的数据上进行，要么局限于理想化的趋势。因此，本文提出了三种替代的神经网络（NN）方法，包括（i）人工神经网络，（ii）循环神经网络，（iii）长短期记忆（LSTM），用于建模和预测系统性能，包括驱动弹性的负面和正面因素，以量化破坏性事件和恢复活动的影响。使用均方误差和调整R平方等拟合度量指标来评估这些模型并与经典的统计模型进行比较。我们的结果表明NN模型可以有效预测系统的弹性。

    Resilience engineering studies the ability of a system to survive and recover from disruptive events, which finds applications in several domains. Most studies emphasize resilience metrics to quantify system performance, whereas recent studies propose statistical modeling approaches to project system recovery time after degradation. Moreover, past studies are either performed on data after recovering or limited to idealized trends. Therefore, this paper proposes three alternative neural network (NN) approaches including (i) Artificial Neural Networks, (ii) Recurrent Neural Networks, and (iii) Long-Short Term Memory (LSTM) to model and predict system performance, including negative and positive factors driving resilience to quantify the impact of disruptive events and restorative activities. Goodness-of-fit measures are computed to evaluate the models and compared with a classical statistical model, including mean squared error and adjusted R squared. Our results indicate that NN models
    
[^56]: 克服重力模型中零贸易的新方法，避免线性对数方程中的不确定值并使用机器学习进行参数验证

    A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning. (arXiv:2308.06303v1 [econ.GN])

    [http://arxiv.org/abs/2308.06303](http://arxiv.org/abs/2308.06303)

    本研究提出了一种克服重力模型中零贸易问题的两步技术，通过局部线性回归和虚拟值替代来确定重力参数，并使用机器学习进行参数验证。通过该方法，解决了线性对数方程中的不确定值问题，进一步完善了重力模型在解释国际贸易方面的应用。

    

    在使用重力模型解释国际贸易时，存在大量零贸易流的情况，这给确定重力参数提出了挑战。线性回归及对数线性方程在对数贸易上会遇到不确定值的问题。尽管已经提出了几种解决该问题的方法，但其中大部分不再基于线性回归，使得寻找解决方案的过程更加复杂。在本研究中，我们提出了一种两步技术来确定重力参数：首先，在局部进行线性回归以建立一个替代零贸易流量的虚拟值，然后估计重力参数。采用迭代技术确定最优参数。使用机器学习通过分析参数在聚类中的位置来测试估计得到的参数。我们计算了2004年、2009年、2014年和2019年的国际贸易数据。我们只研究了经典重力方程并发现...

    The presence of a high number of zero flow trades continues to provide a challenge in identifying gravity parameters to explain international trade using the gravity model. Linear regression with a logarithmic linear equation encounters an indefinite value on the logarithmic trade. Although several approaches to solving this problem have been proposed, the majority of them are no longer based on linear regression, making the process of finding solutions more complex. In this work, we suggest a two-step technique for determining the gravity parameters: first, perform linear regression locally to establish a dummy value to substitute trade flow zero, and then estimating the gravity parameters. Iterative techniques are used to determine the optimum parameters. Machine learning is used to test the estimated parameters by analyzing their position in the cluster. We calculated international trade figures for 2004, 2009, 2014, and 2019. We just examine the classic gravity equation and discove
    
[^57]: 使用深度学习模型进行血细胞分类

    Classification of Blood Cells Using Deep Learning Models. (arXiv:2308.06300v1 [eess.IV])

    [http://arxiv.org/abs/2308.06300](http://arxiv.org/abs/2308.06300)

    这项研究使用深度学习模型通过图像分类对人类血细胞进行了分类和识别，为诊断疾病提供了重要的帮助。

    

    人类血液主要包括血浆、红细胞、白细胞和血小板。血细胞为身体细胞提供氧气，滋养它们，保护它们免受感染，增强免疫力并促进凝血。人的健康状况可以从血细胞中反映出来。一个人被诊断出某种疾病的机会很大程度上受其血细胞类型和计数的影响。因此，血细胞分类非常重要，它可以帮助识别疾病，包括癌症、骨髓损伤、良性肿瘤和它们的生长。这种分类可以帮助血液学家区分不同的血细胞片段，以便确定疾病的原因。卷积神经网络是一种深度学习技术，它将人类血细胞（红细胞、白细胞和血小板）的图像分类为它们的亚型。在这项研究中，使用迁移学习将不同的CNN预训练模型应用于血细胞分类。

    Human blood mainly comprises plasma, red blood cells, white blood cells, and platelets. The blood cells provide the body's cells oxygen to nourish them, shield them from infections, boost immunity, and aid in clotting. Human health is reflected in blood cells. The chances that a human being can be diagnosed with a disease are significantly influenced by their blood cell type and count. Therefore, blood cell classification is crucial because it helps identify diseases, including cancer, damaged bone marrow, benign tumors, and their growth. This classification allows hematologists to distinguish between different blood cell fragments so that the cause of diseases can be identified. Convolution neural networks are a deep learning technique that classifies images of human blood cells (RBCs, WBCs, and platelets) into their subtypes. For this study, transfer learning is used to apply different CNN pre-trained models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3 Mobi
    
[^58]: 防御感知：神经网络在部署中的性能估计和监测

    Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment. (arXiv:2308.06299v1 [cs.CV])

    [http://arxiv.org/abs/2308.06299](http://arxiv.org/abs/2308.06299)

    本文提出了一种方法，用于解决自动驾驶中神经网络的部署中不被察觉的灾难性问题和领域转移问题。我们通过不确定性估计封装了部署中的神经网络，以提高感知系统的性能和安全性。

    

    本文提出了一种方法，用于解决自动驾驶中语义分割神经网络在部署过程中不被察觉的灾难性问题和领域转移问题。我们的方法基于深度学习感知对自动驾驶的不确定性并将其最佳表示为概率分布。由于自动车辆的安全至关重要，感知系统必须能够识别车辆是否离开了操作设计领域，预测危险的不确定性并降低感知系统性能。为了解决这个问题，我们提出了在部署中对神经网络进行不确定性估计封装，该封装基于蒙特卡罗 Dropout 方法通过对认知不确定性进行估计。该方法不需要修改部署的神经网络，并保证预期的模型性能。我们的防御性感知封装能够估计神经网络的

    In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's 
    
[^59]: 通过机器学习模型对白细胞进行分类的综述

    A Review on Classification of White Blood Cells Using Machine Learning Models. (arXiv:2308.06296v1 [eess.IV])

    [http://arxiv.org/abs/2308.06296](http://arxiv.org/abs/2308.06296)

    本综述系统分析了在医学图像分析领域中应用的机器学习技术，为白细胞分类提供了有价值的洞察力和最佳方法。

    

    机器学习（ML）和深度学习（DL）模型在医学图像分析方面做出了重要贡献。这些模型通过预测和分类提高了预测的准确性，帮助血液学家根据计算和事实来诊断血液癌症和脑肿瘤。本综述主要关注于对白细胞分类的医学图像分析领域中应用的现代技术进行深入分析。针对本综述，讨论了使用血涂片图像、磁共振成像（MRI）、X射线和类似医学影像领域的方法。本综述的主要影响在于对应用于白细胞分类的机器学习技术进行详细分析。这种分析提供了有价值的洞察力，例如最常使用的技术和最佳表现的白细胞分类方法。最近几十年的研究表明，研究人员一直在使用ML和DL方法。

    The machine learning (ML) and deep learning (DL) models contribute to exceptional medical image analysis improvement. The models enhance the prediction and improve the accuracy by prediction and classification. It helps the hematologist to diagnose the blood cancer and brain tumor based on calculations and facts. This review focuses on an in-depth analysis of modern techniques applied in the domain of medical image analysis of white blood cell classification. For this review, the methodologies are discussed that have used blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains. The main impact of this review is to present a detailed analysis of machine learning techniques applied for the classification of white blood cells (WBCs). This analysis provides valuable insight, such as the most widely used techniques and best-performing white blood cell classification methods. It was found that in recent decades researchers have been using ML and DL f
    
[^60]: 使用MCMC和VI训练的贝叶斯神经网络在高光谱图像上进行目标检测

    Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks. (arXiv:2308.06293v1 [eess.IV])

    [http://arxiv.org/abs/2308.06293](http://arxiv.org/abs/2308.06293)

    本研究应用和比较了MCMC和VI训练的贝叶斯神经网络在高光谱图像上进行目标检测，解决了神经网络预测的不确定性量化问题。

    

    神经网络已经在图像分类中变得无处不在，但它们的标准形式只能产生点估计，没有置信度的度量。贝叶斯神经网络通过后验分布提供神经网络预测和估计的不确定性量化。随着神经网络在更高风险的应用中的应用，不确定性量化变得越来越重要。贝叶斯神经网络通过提供准确的预测和估计，以及一个包含在期望概率内合理值的区间，解决了这个问题。尽管具有积极的特点，但贝叶斯神经网络训练起来非常困难而且耗时。传统的贝叶斯方法使用马尔可夫链蒙特卡洛(MCMC)，但这通常被视为太慢。最常见的方法是变分推断(VI)，因为它可以进行快速计算，但它的有效性存在多个问题。我们在高光谱目标检测的背景下应用和比较了MCMC和VI训练的贝叶斯神经网络。

    Neural networks (NN) have become almost ubiquitous with image classification, but in their standard form produce point estimates, with no measure of confidence. Bayesian neural networks (BNN) provide uncertainty quantification (UQ) for NN predictions and estimates through the posterior distribution. As NN are applied in more high-consequence applications, UQ is becoming a requirement. BNN provide a solution to this problem by not only giving accurate predictions and estimates, but also an interval that includes reasonable values within a desired probability. Despite their positive attributes, BNN are notoriously difficult and time consuming to train. Traditional Bayesian methods use Markov Chain Monte Carlo (MCMC), but this is often brushed aside as being too slow. The most common method is variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy. We apply and compare MCMC- and VI-trained BNN in the context of target detection in hypersp
    
[^61]: 蛋白质结构的分化时间模型与序列分化的关系

    The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences. (arXiv:2308.06292v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.06292](http://arxiv.org/abs/2308.06292)

    通过蛋白质结构的保守性模式，建立了一个时间参数化的统计模型，用于量化蛋白质结构的分化演化，并与序列的分化关系进行对比。这个模型能更准确估计蛋白质结构的分化时间。

    

    通过大量蛋白质三维结构对齐的方式，推导出一个完整的、时间参数化的统计模型，用于量化蛋白质结构的分化演化，该模型以二级结构的保守性模式为指标。相对于以序列为基础的时间参数化模型，该模型能更好地处理序列关系的黄昏区和午夜区的限制。由于蛋白质结构受功能直接施加的选择压力更大，从结构推断分化时间的估计更准确。我们使用最小信息长度的贝叶斯和信息论框架来推断一个时间参数化的随机矩阵（考虑相关残基的扰动结构状态）和相关的Dirichlet模型（考虑蛋白质域演化过程中的插入和删除）。这些模型共同用于估计马尔可夫链的分化时间。

    A complete time-parameterized statistical model quantifying the divergent evolution of protein structures in terms of the patterns of conservation of their secondary structures is inferred from a large collection of protein 3D structure alignments. This provides a better alternative to time-parameterized sequence-based models of protein relatedness, that have clear limitations dealing with twilight and midnight zones of sequence relationships. Since protein structures are far more conserved due to the selection pressure directly placed on their function, divergence time estimates can be more accurate when inferred from structures. We use the Bayesian and information-theoretic framework of Minimum Message Length to infer a time-parameterized stochastic matrix (accounting for perturbed structural states of related residues) and associated Dirichlet models (accounting for insertions and deletions during the evolution of protein domains). These are used in concert to estimate the Markov ti
    
[^62]: 旋转不变的随机特征为3D点云上的机器学习提供了一个强大的基准

    Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds. (arXiv:2308.06271v1 [cs.CV])

    [http://arxiv.org/abs/2308.06271](http://arxiv.org/abs/2308.06271)

    本研究提出了一种简单且通用的方法，使用随机特征学习三维点云数据的旋转不变函数，为机器学习在3D点云上提供了一个强大的基准。

    

    旋转不变性是机器学习中许多领域使用的一种普遍的归纳偏好，例如计算机视觉和量子化学的机器学习。旋转不变的机器学习方法在许多任务中达到了最先进的水平，包括分子性质预测和3D形状分类。这些方法通常要么依赖于任务特定的旋转不变特征，要么使用复杂设计和训练的通用深度神经网络。然而，目前尚不清楚这些方法的成功主要是由于旋转不变性还是深度神经网络。为了解决这个问题，我们提出了一种简单且通用的方法来学习三维点云数据的旋转不变函数，采用了随机特征方法。具体来说，我们通过推导出一种对三维旋转不变的版本，并证明它在点云数据上的快速评估。

    Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point
    
[^63]: DynamicFL：平衡通信动态和客户端操作用于联邦学习

    DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning. (arXiv:2308.06267v1 [cs.DC])

    [http://arxiv.org/abs/2308.06267](http://arxiv.org/abs/2308.06267)

    DynamicFL是一个解决在联邦学习中通信和时效性问题的新框架，通过考虑边缘设备的通信动态和数据质量，以及采用特殊设计的客户端操作策略，实现了高效的模型更新。

    

    联邦学习（FL）是一种分布式机器学习（ML）范例，旨在利用数百万边缘设备上的分散数据来训练全局模型。与集中式学习相比，FL通过避免明确下载客户端数据来保护客户隐私。然而，在现实中，由于地理分布的边缘设备（如移动设备、汽车、火车或地铁）的网络高度动态，从参与设备聚合所有模型更新将导致不可避免的长尾延迟。这将严重降低训练过程的效率。为了解决FL场景中高系统异质性的时间敏感性问题，我们提出了一种新的FL框架DynamicFL，通过考虑边缘设备上的通信动态和数据质量，并采用特殊设计的客户端操作策略，\ours基于动态网络条件和先前的网络预测选择客户端进行模型更新。

    Federated Learning (FL) is a distributed machine learning (ML) paradigm, aiming to train a global model by exploiting the decentralized data across millions of edge devices. Compared with centralized learning, FL preserves the clients' privacy by refraining from explicitly downloading their data. However, given the geo-distributed edge devices (e.g., mobile, car, train, or subway) with highly dynamic networks in the wild, aggregating all the model updates from those participating devices will result in inevitable long-tail delays in FL. This will significantly degrade the efficiency of the training process. To resolve the high system heterogeneity in time-sensitive FL scenarios, we propose a novel FL framework, DynamicFL, by considering the communication dynamics and data quality across massive edge devices with a specially designed client manipulation strategy. \ours actively selects clients for model updating based on the network prediction from its dynamic network conditions and the
    
[^64]: 温度变化对经济增长的长期影响：一种机器学习方法的研究

    Long-term Effects of Temperature Variations on Economic Growth: A Machine Learning Approach. (arXiv:2308.06265v1 [econ.GN])

    [http://arxiv.org/abs/2308.06265](http://arxiv.org/abs/2308.06265)

    这项研究利用机器学习方法调查了温度变化对经济增长的长期影响，发现平均温度和GDP增长之间存在显著关系，表明气候变化可对经济表现产生重大影响。

    

    本研究采用数据驱动方法，调查了温度变化对经济增长的长期影响。利用机器学习技术，分析了来自伯克利地球和世界银行的全球陆地表面温度数据和经济指标，包括GDP和人口数据。我们的分析揭示了平均温度和GDP增长之间的显著关系，表明气候变化可以大幅影响经济表现。该研究强调了将气候因素纳入经济规划和政策制定中的重要性，并证明了机器学习在揭示气候经济研究中复杂关系方面的实用性。

    This study investigates the long-term effects of temperature variations on economic growth using a data-driven approach. Leveraging machine learning techniques, we analyze global land surface temperature data from Berkeley Earth and economic indicators, including GDP and population data, from the World Bank. Our analysis reveals a significant relationship between average temperature and GDP growth, suggesting that climate variations can substantially impact economic performance. This research underscores the importance of incorporating climate factors into economic planning and policymaking, and it demonstrates the utility of machine learning in uncovering complex relationships in climate-economy studies.
    
[^65]: 在内存层次结构上具有MiRo的成本效益的设备上的持续学习

    Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])

    [http://arxiv.org/abs/2308.06053](http://arxiv.org/abs/2308.06053)

    这项工作是首次探索基于层次内存回放的持续学习的设计空间，旨在在边缘设备上实现成本效益。提出了Miro，一个通过动态配置持续学习系统的新颖系统运行时，以实现最佳的成本效益。广泛的评估显示Miro明显优于其他方案。

    

    持续学习是从持续的任务流中逐步训练神经网络模型。为了记住先前学到的知识，之前的研究将旧样本存储在一个内存层次结构中，并在新任务到来时进行回放。采用持续学习以保护数据隐私的边缘设备通常对能源敏感，因此需要在不损害能源效率的情况下保持高模型准确度，即成本效益。我们的工作是首次探索基于层次内存回放的持续学习的设计空间，以获得在边缘设备上的成本效益。我们提出了Miro，一个新颖的系统运行时，通过使其能够根据资源状态动态配置持续学习系统，从而将我们的见解精确地整合到持续学习框架中，以实现最佳成本效益。为了实现这个目标，Miro还对带有明确准确度-能量平衡的参数进行在线分析，并以低开销地适应最佳值。广泛的评估显示Miro明显优于其他方案。

    Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
    
[^66]: 隐形数据集，用于因果表示学习的新挑战性数据集

    Shadow Datasets, New challenging datasets for Causal Representation Learning. (arXiv:2308.05707v1 [cs.LG])

    [http://arxiv.org/abs/2308.05707](http://arxiv.org/abs/2308.05707)

    该论文提出了两个新的挑战性数据集，用于因果表示学习，并对现有数据集进行了修改，以更好地评估CRL性能。

    

    在表示学习中，发现语义因素之间的因果关系是一个新兴的话题。大多数因果表示学习（CRL）方法都是完全监督的，由于标记成本高昂而不切实际。为了解决这个限制，引入了弱监督的CRL方法。为了评估CRL性能，使用了四个现有数据集：Pendulum、Flow、CelebA（BEARD）和CelebA（SMILE）。然而，现有的CRL数据集仅限于具有少量生成因素的简单图形。因此，我们提出了两个新的数据集，其中包含更多种类的生成因素和更复杂的因果图。此外，在当前的真实数据集CelebA（BEARD）和CelebA（SMILE）中，最初提出的因果图与数据集分布不一致。因此，我们提出了对它们的修改。

    Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
    
[^67]: Kairos: 使用整体系统溯源进行实用的入侵检测和调查

    Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance. (arXiv:2308.05034v1 [cs.CR])

    [http://arxiv.org/abs/2308.05034](http://arxiv.org/abs/2308.05034)

    Kairos是第一个同时满足范围、攻击不可知性、时效性和攻击重建维度要求的溯源为基础的入侵检测和调查系统。

    

    溯源图是描述系统执行历史的结构化审计日志。最近的研究探索了各种技术来分析溯源图，以实现自动化主机入侵检测，特别关注高级持久性威胁。通过研究其设计文档，我们确定了四个常见维度，推动溯源为基础的入侵检测系统（PIDS）的发展：范围（PIDS能否检测跨应用边界渗透的现代攻击？）、攻击不可知性（PIDS能否在没有攻击特征先验知识的情况下检测新型攻击？）、时效性（PIDS能否高效监视主机系统运行？）和攻击重建（PIDS能否从大型溯源图中提炼攻击活动，以便系统管理员能够轻松理解并迅速应对系统入侵？）。我们提出了KAIROS，这是第一个同时满足所有四个维度要求的PIDS，而现有的方法不能做到。

    Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approac
    
[^68]: NLLG季度arXiv报告 06/23：当前最具影响力的AI论文是什么？（arXiv:2308.04889v1 [cs.CY]）

    NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY])

    [http://arxiv.org/abs/2308.04889](http://arxiv.org/abs/2308.04889)

    该报告关注当前最具影响力的AI论文，并以标准化引用计数为依据编制了40篇最受欢迎的论文列表。观察到在2023年上半年，大型语言模型（LLMs）和具体而言的ChatGPT相关的论文占主导地位，ChatGPT表现出下降的趋势。

    

    人工智能（AI）领域中的生成式人工智能（Generative Artificial Intelligence，特别是自然语言处理（Natural Language Processing，NLP）和机器学习（Machine Learning，ML））信息的快速增长给研究人员和从业者带来了巨大的挑战，使得他们难以跟上最新的发展。为了解决信息过载的问题，Bielefeld大学的自然语言学习组在本报告中专注于识别arXiv上最受欢迎的论文，特别关注NLP和ML。其目标是为最相关且被广泛讨论的研究提供快速指南，以帮助新来者和已有研究人员跟上当前趋势。具体而言，我们根据2023年上半年的标准化引用计数编制了一个由40篇最受欢迎的论文组成的列表。我们观察到在2023年上半年，与大型语言模型（Large Language Models，LLMs）和具体而言的ChatGPT相关的论文占主导地位，而ChatGPT显示出下降的趋势。

    The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popul
    
[^69]: 快速NeRF合成和渲染的通用隐式框架

    A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v1 [cs.CV])

    [http://arxiv.org/abs/2308.04669](http://arxiv.org/abs/2308.04669)

    本研究提出了一个通用的隐式框架，可以快速合成和渲染NeRF对象。通过引入神经深度场的新的表面表示方法，可以实现动态阴影并允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。

    

    最近，各种神经辐射场方法在高渲染速度方面取得了显著成功。然而，当前的加速方法专门化并且不适用于各种隐式方法，这阻碍了对不同类型的NeRF作品进行实时合成。由于NeRF依赖于沿光线采样，因此可以提供一般性的指导。我们提出了一个通用的隐式流水线来快速合成NeRF对象。这种新方法使得动态阴影可以使用解析光源在对象内部或对象之间进行投射，同时允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。主要地，我们的工作引入了一种称为神经深度场（NeDF）的新的表面表示方法，它通过允许光线和隐式表面之间的直接相交计算来快速确定对象之间的空间关系。它利用一个交点神经网络来加速查询NeRF。

    Recently, a variety of Neural radiance fields methods have garnered remarkable success in high render speed. However, current accelerating methods is specialized and not compatible for various implicit method, which prevent a real-time composition over different kinds of NeRF works. Since NeRF relies on sampling along rays, it's possible to provide a guidance generally. We propose a general implicit pipeline to rapidly compose NeRF objects. This new method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration inste
    
[^70]: 无法测量混淆因素下因果推断中的扩散模型

    Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])

    [http://arxiv.org/abs/2308.03669](http://arxiv.org/abs/2308.03669)

    本研究扩展了扩散模型的使用，提出了一种新的模型BDCM，可以在存在无法测量的混淆因素的情况下更准确地回答因果问题。

    

    我们研究了如何在无法测量的混淆因素存在的情况下，扩展扩散模型的使用，以从观测数据中回答因果问题。在Pearl的使用有向无环图（DAG）捕捉因果干预的框架中，提出了一种基于扩散模型的因果模型（DCM），可以更准确地回答因果问题，假设所有混淆因素都是可以观察到的。然而，实际中存在无法测量的混淆因素，这使得DCM无法应用。为了缓解DCM的这一局限性，我们提出了一个扩展模型，称为基于反门准则的DCM（BDCM），其思想根植于在DAG中找到要包括在扩散模型解码过程中的变量的反门准则，这样我们可以将DCM扩展到存在无法测量的混淆因素的情况。合成数据实验表明，我们提出的模型在无法测量混淆因素的情况下更精确地捕捉到了反事实分布。

    We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
    
[^71]: 通过网络简化加速神经网络验证

    Expediting Neural Network Verification via Network Reduction. (arXiv:2308.03330v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2308.03330](http://arxiv.org/abs/2308.03330)

    本研究提出了一种网络简化技术，在验证之前对神经网络进行预处理，通过消除稳定的ReLU神经元，并将其转化为由ReLU和仿射层组成的顺序神经网络，从而显著减小了神经网络的规模并加速了现有的验证工具。

    

    已经提出了各种验证方法来验证深度神经网络的安全性，以确保网络在关键应用中正确运行。然而，许多众所周知的验证工具在复杂的网络架构和大型网络大小上仍然存在困难。在这项工作中，我们提出了一种网络简化技术作为验证之前的预处理方法。所提出的方法通过消除稳定的ReLU神经元，并将其转化为由ReLU和仿射层组成的顺序神经网络，从而可以通过大多数验证工具处理。我们在最先进的完整和不完整验证工具上实现了简化技术，包括alpha-beta-crown，VeriNet和PRIMA。我们在大量基准测试中的实验表明，所提出的技术可以显著减小神经网络并加速现有的验证工具。此外，实验结果还表明...

    A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also sh
    
[^72]: 无源域自适应的人体姿势估计

    Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.03202](http://arxiv.org/abs/2308.03202)

    提出了无源域自适应的人体姿势估计任务，旨在解决在适应过程中无法访问源数据的跨域学习挑战。通过提出的新框架，源保护模块更有效地保留源信息并抵抗噪声。

    

    人体姿势估计广泛应用于运动分析、医疗保健和虚拟现实等领域。然而，标注实际场景的数据集的巨大开销对姿势估计构成了重要挑战。为了解决这个问题，一种方法是在合成数据集上训练姿势估计模型，然后在真实世界数据上进行域自适应(DA)。然而，现有的HPE的DA方法在适应过程中忽略了数据隐私和安全问题，因为使用了源数据和目标数据。为此，我们提出了一种新的任务，名为无源域自适应的HPE，旨在解决在适应过程中无法访问源数据的HPE的跨域学习挑战。我们进一步提出了一个由三个模型组成的新框架：源模型、中间模型和目标模型，从源数据和目标数据的角度探索该任务。源保护模块更有效地保留源信息并抵抗噪声。

    Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
    
[^73]: 基于广义多维比较的光谱排名推断

    Spectral Ranking Inferences based on General Multiway Comparisons. (arXiv:2308.02918v1 [stat.ME])

    [http://arxiv.org/abs/2308.02918](http://arxiv.org/abs/2308.02918)

    本文研究了使用光谱方法在广义多维比较中估计和量化未观察到的比较实体的偏好分数的性能，并揭示了光谱估计量与最大似然估计量之间的关系。

    

    本文研究了在一个非常普遍和更加真实的情景中，使用光谱方法对未观察到的比较实体的偏好分数进行估计和不确定性量化的性能。在这种情况下，比较图由可能具有异构大小的超边组成，对于给定的超边，比较数量可能仅为1。这种设置在实际应用中普遍存在，避免了需要指定图的随机性以及在常用的Bradley-Terry-Luce (BTL)或Plackett-Luce (PL)模型中施加的限制性均匀采样假设。此外，在适用BTL或PL模型的情况下，我们揭示了光谱估计量与最大似然估计量（MLE）之间的关系。我们发现，通过应用从等权重传统光谱方法估计得到的最佳加权，可以实现与MLE相同的渐近效率的双步光谱方法。考虑到渐近情况，

    This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptot
    
[^74]: 用正则化高阶总变差的随机优化方法训练非线性神经网络

    A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])

    [http://arxiv.org/abs/2308.02293](http://arxiv.org/abs/2308.02293)

    通过引入高阶总变差正则化的随机优化算法，可以高效地训练非线性神经网络，避免过拟合问题。

    

    尽管包括深度神经网络在内的高度表达的参数模型可以更好地建模复杂概念，但训练这种高度非线性模型已知会导致严重的过拟合风险。针对这个问题，本研究考虑了一种k阶总变差（k-TV）正则化，它被定义为要训练的参数模型的k阶导数的平方积分，通过惩罚k-TV来产生一个更平滑的函数，从而避免过拟合。尽管将k-TV项应用于一般的参数模型由于积分而导致计算复杂，本研究提供了一种随机优化算法，可以高效地训练带有k-TV正则化的一般模型，而无需进行显式的数值积分。这种方法可以应用于结构任意的深度神经网络的训练，因为它只需要进行简单的随机梯度优化即可实现。

    While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
    
[^75]: 能否转移噪声模式？使用生成案例的多环境频谱分析模型

    Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases. (arXiv:2308.01138v1 [cs.LG])

    [http://arxiv.org/abs/2308.01138](http://arxiv.org/abs/2308.01138)

    这项研究提出了一个噪声模式转移模型，可以将噪声模式从不同环境的标准样本应用到未知样本，通过生成案例库来解决样本级噪声对数据集级噪声学习的干扰，提高了系统的学习性能。

    

    在在线水质检测中，频谱分析系统旨在检测污染物的类型和浓度，并使监管机构能够及时回应污染事件。然而，基于频谱数据的测试设备在非实验室环境中部署时会受到复杂的噪声模式的影响。为了使分析模型适用于更多的环境，我们提出了一个噪声模式转移模型，该模型将不同环境中标准水样品的频谱作为案例，并学习它们噪声模式的差异，从而使噪声模式能够应用于未知样品。不幸的是，必然存在的样本级基线噪声使得模型无法获取只在数据集级环境噪声上有差异的配对数据。为了解决这个问题，我们生成了一个样本对样本的案例库，排除了样本级噪声对数据集级噪声学习的干扰，提高了系统的学习性能。

    Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on sp
    
[^76]: 对自动驾驶车辆风险评估的反事实安全边界视角

    A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])

    [http://arxiv.org/abs/2308.01050](http://arxiv.org/abs/2308.01050)

    本文基于反事实模拟提出了一个数据驱动的框架，用于比较不同自动驾驶车辆在不同操作设计领域中行为风险。通过引入反事实安全边界的概念，该框架可以找到最关键的情景，并评估自动驾驶车辆的风险频率和严重程度。该方法即使在自动驾驶车辆的行为策略未知的情况下也适用，对外部第三方风险评估机构有用。

    

    自动驾驶车辆（AVs）有潜力提供诸多社会效益，如减少道路事故和提高交通效率。然而，由于缺乏历史数据和技术的快速发展，量化AVs的风险是具有挑战性的。本文提出了一个基于数据驱动的框架，用于比较不同AVs在各种操作设计领域（ODDs）中行为的风险，该框架基于对“不良”道路用户进行反事实模拟。我们引入了反事实安全边界的概念，表示可能导致碰撞的最小偏离正常行为的量。该概念有助于找到最关键的情景，同时也有助于评估AVs的风险频率和严重程度。我们证明，即使AV的行为策略是未知的，提出的方法仍然适用于最坏和最佳情况分析，使该方法对外部第三方风险评估机构也有用。

    Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
    
[^77]: 使用深度卷积神经网络中的多头通道注意力自动COVID-19 CT图像分类

    Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN. (arXiv:2308.00715v1 [eess.IV])

    [http://arxiv.org/abs/2308.00715](http://arxiv.org/abs/2308.00715)

    本文提出了一种使用多头通道注意力机制的深度学习方法，能够自动分类COVID-19 CT图像，并在广泛使用的数据集上展示了96.99%的准确性。

    

    COVID-19的迅速传播需要有效和准确的诊断方法。计算机断层扫描（CT）图像已成为检测该疾病的有价值工具。本文提出了一种新颖的深度学习方法，用于自动化COVID-19 CT扫描分类，其中提出了一个修改版Xception模型，该模型结合了新设计的通道注意力机制和加权全局平均池化，以增强特征提取，从而提高分类准确性。通道注意力模块选择性地关注每个通道中的信息区域，使模型能够学习用于COVID-19检测的判别特征。对广泛使用的COVID-19 CT扫描数据集进行的实验显示出96.99%的非常好的准确性，并展示了其对其他最先进技术的优越性。这项研究可以为使用人工智能抗击当前和未来的流行病的不懈努力做出贡献，提供有希望和及时的解决方案。

    The rapid spread of COVID-19 has necessitated efficient and accurate diagnostic methods. Computed Tomography (CT) scan images have emerged as a valuable tool for detecting the disease. In this article, we present a novel deep learning approach for automated COVID-19 CT scan classification where a modified Xception model is proposed which incorporates a newly designed channel attention mechanism and weighted global average pooling to enhance feature extraction thereby improving classification accuracy. The channel attention module selectively focuses on informative regions within each channel, enabling the model to learn discriminative features for COVID-19 detection. Experiments on a widely used COVID-19 CT scan dataset demonstrate a very good accuracy of 96.99% and show its superiority to other state-of-the-art techniques. This research can contribute to the ongoing efforts in using artificial intelligence to combat current and future pandemics and can offer promising and timely solut
    
[^78]: 关于最先进生成模型的可信度景观：一项综合调查

    On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16680](http://arxiv.org/abs/2307.16680)

    本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。

    

    扩散模型和大规模语言模型已经成为领先的生成模型，并对人类生活的各个方面产生了革命性的影响。然而，这些模型的实际应用也暴露出固有的风险，突显了它们的双重性质，并引发了对它们可信度的担忧。尽管有大量关于这个主题的文献，但针对大规模生成模型及其可信度的综合调查仍然很少见。为了弥补这一空白，本文调查了涉及这些模型的长期和新兴威胁，涵盖了隐私、安全、公平和责任这四个基本维度。通过这种方式，我们构建了一张详尽的地图，概述了这些模型的可信度，并提供了实际建议和未来的发展方向。这些努力对于促进这些模型的可信度部署至关重要。

    Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
    
[^79]: 在预测自动缩放中的持续学习

    Continual Learning in Predictive Autoscaling. (arXiv:2307.15941v1 [cs.LG])

    [http://arxiv.org/abs/2307.15941](http://arxiv.org/abs/2307.15941)

    本论文提出了一种基于重放的持续学习方法，使用少量历史数据，解决了预测自动缩放中的性能下降问题。

    

    预测自动缩放被用于预测服务器的工作负载，并提前准备资源，以确保在动态云环境中的服务水平目标（SLOs）。然而，在实践中，其预测任务常常在外部事件（如促销活动和应用程序重新配置）引起的异常流量下性能下降，常见的解决方案是使用长时间历史数据重新训练模型，但代价是高计算和存储成本。为了更好地解决这个问题，我们提出了一种基于重放的持续学习方法，即基于密度的记忆选择和基于提示的网络学习模型（DMSHM），只使用一小部分历史日志数据来实现准确的预测。首先，我们发现了在预测任务中应用重放式持续学习时的样本重叠现象。为了克服这一挑战并有效地整合新的样本分布，我们提出了一种基于密度的记忆选择和基于提示的网络学习模型（DMSHM）。

    Predictive Autoscaling is used to forecast the workloads of servers and prepare the resources in advance to ensure service level objectives (SLOs) in dynamic cloud environments.However, in practice, its prediction task often suffers from performance degradation under abnormal traffics caused by external events (such as sales promotional activities and applications' re-configurations), for which a common solution is to re-train the model with data of a long historical period, but at the expense of high computational and storage costs.To better address this problem, we propose a replay-based continual learning method, i.e., Density-based Memory Selection and Hint-based Network Learning Model (DMSHM), using only a small part of the historical log to achieve accurate predictions.First, we discover the phenomenon of sample overlap when applying replay-based continual learning in prediction tasks. In order to surmount this challenge and effectively integrate new sample distribution, we propo
    
[^80]: AlignDet: 对齐目标检测的预训练与微调

    AlignDet: Aligning Pre-training and Fine-tuning in Object Detection. (arXiv:2307.11077v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.11077](http://arxiv.org/abs/2307.11077)

    本文提出了一个统一的预训练框架AlignDet，通过将预训练过程分解为图像域和框域预训练，可以减轻目标检测中现有实践中的数据、模型和任务差异，提高检测器性能和泛化能力。

    

    大规模预训练后进行微调的范式在各种目标检测算法中被广泛应用。本文揭示了现有实践中预训练和微调过程之间的数据、模型和任务差异，这些差异隐含地限制了检测器的性能、泛化能力和收敛速度。为此，我们提出了AlignDet，一个统一的预训练框架，可适用于各种现有检测器，以减轻这些差异。AlignDet将预训练过程分解为两个阶段，即图像域和框域预训练。图像域预训练优化检测主干以捕捉整体视觉抽象，而框域预训练学习实例级语义和任务感知概念以初始化主干之外的部分。通过融入自监督预训练的主干，我们可以对各种检测器的所有模块进行无监督预训练。

    The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised par
    
[^81]: Fisher-Rao距离和逆推到SPD锥距离在多元正态分布之间的应用

    Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])

    [http://arxiv.org/abs/2307.10644](http://arxiv.org/abs/2307.10644)

    本研究提出了一种快速和鲁棒的方法来近似计算多元正态分布之间的Fisher-Rao距离，并引入了一类基于正态流形嵌入到高维对称正定锥子流形的距离。

    

    许多科学领域，如扩散张量成像、结构张量计算机视觉、雷达信号处理和机器学习等，都存在着多元正态分布的数据集。为了处理这些正态数据集以进行过滤、分类或聚类等下游任务，需要定义合适的正态和它们之间的路径之间的差异度量。Fisher-Rao距离，作为Fisher信息度量引起的Riemann几何距离，是一种合理的度量距离，但除了一些特殊情况外，并没有闭式求解。本文首先报告了一种快速且鲁棒的方法，可以精确地近似计算多元正态分布之间的Fisher-Rao距离。其次，我们介绍了一类基于正态流形到高维对称正定锥的子流形的微分同胚嵌入的距离。

    Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of
    
[^82]: 高效的LLM引导生成

    Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])

    [http://arxiv.org/abs/2307.09702](http://arxiv.org/abs/2307.09702)

    本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。

    

    在本文中，我们描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。我们的方法在标记序列生成过程中几乎不增加任何开销，并使得引导生成在实际中可行。在开源Python库Outlines中提供了一个实现。

    In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
    
[^83]: CaRT: 针对多智能体系统基于学习的运动规划的验证安全和鲁棒跟踪

    CaRT: Certified Safety and Robust Tracking in Learning-based Motion Planning for Multi-Agent Systems. (arXiv:2307.08602v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2307.08602](http://arxiv.org/abs/2307.08602)

    CaRT是一种验证安全和鲁棒跟踪的分层分布式架构，针对多智能体系统基于学习的运动规划。它通过分析形式的安全过滤器确保了安全机动，并通过鲁棒过滤器优化跟踪认证安全轨迹，即使在干扰存在的情况下也能保证安全和轨迹跟踪误差的指数有界性。

    

    我们分析方法CaRT的关键创新在于建立了一种新的分层分布式架构，以保证给定学习-based运动规划策略的安全性和鲁棒性。首先，在名义设置中，我们的CaRT安全过滤器的分析形式正式确保非线性多智能体系统的安全机动，且与基于学习的策略最小偏差。其次，在非名义设置中，我们的CaRT鲁棒过滤器的分析形式最优跟踪由层次结构中前一层生成的经过认证的安全轨迹，即CaRT安全过滤器。我们使用收缩理论证明CaRT在存在确定性和随机干扰的情况下保证安全和轨迹跟踪误差的指数有界性。此外，CaRT的分层性质使其能够通过对经过认证的安全轨迹的卓越跟踪来增强其对安全性的鲁棒性，从而使其适用于非名义设置。

    The key innovation of our analytical method, CaRT, lies in establishing a new hierarchical, distributed architecture to guarantee the safety and robustness of a given learning-based motion planning policy. First, in a nominal setting, the analytical form of our CaRT safety filter formally ensures safe maneuvers of nonlinear multi-agent systems, optimally with minimal deviation from the learning-based policy. Second, in off-nominal settings, the analytical form of our CaRT robust filter optimally tracks the certified safe trajectory, generated by the previous layer in the hierarchy, the CaRT safety filter. We show using contraction theory that CaRT guarantees safety and the exponential boundedness of the trajectory tracking error, even under the presence of deterministic and stochastic disturbance. Also, the hierarchical nature of CaRT enables enhancing its robustness for safety just by its superior tracking to the certified safe trajectory, thereby making it suitable for off-nominal sc
    
[^84]: 在希尔伯特空间中改进自标准化浓度：对GP-UCB算法的次线性遗憾

    Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB. (arXiv:2307.07539v1 [cs.LG])

    [http://arxiv.org/abs/2307.07539](http://arxiv.org/abs/2307.07539)

    本文提出了对GP-UCB算法进行改进，使其具有几乎最优的次线性遗憾，并解决了关于遗憾分析的开放问题。

    

    在核化赌博机问题中，学习器旨在通过仅在顺序选择的点处进行噪声评估，顺序计算位于再生核希尔伯特空间中的函数的最优解。特别地，学习器旨在最小化遗憾，遗憾是所做选择的次优性度量。可以说最受欢迎的算法是高斯过程上界置信区间（GP-UCB）算法，它涉及根据未知函数的简单线性估计器进行行动。尽管它很受欢迎，但现有的GP-UCB遗憾分析给出了次优遗憾率，对于许多常用的内核（如Matérn内核）而言，遗憾率并不次线性。这引发了一个长期存在的问题：现有的GP-UCB遗憾分析是否紧密，或者是否可以通过使用更复杂的分析技术改进界限？在这项工作中，我们解决了这个开放问题，并证明了GP-UCB具有几乎最优的遗憾。特别地，我们的结果直接暗示了次线性遗憾率。

    In the kernelized bandit problem, a learner aims to sequentially compute the optimum of a function lying in a reproducing kernel Hilbert space given only noisy evaluations at sequentially chosen points. In particular, the learner aims to minimize regret, which is a measure of the suboptimality of the choices made. Arguably the most popular algorithm is the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, which involves acting based on a simple linear estimator of the unknown function. Despite its popularity, existing analyses of GP-UCB give a suboptimal regret rate, which fails to be sublinear for many commonly used kernels such as the Mat\'ern kernel. This has led to a longstanding open question: are existing regret analyses for GP-UCB tight, or can bounds be improved by using more sophisticated analytical techniques? In this work, we resolve this open question and show that GP-UCB enjoys nearly optimal regret. In particular, our results directly imply sublinear regret rate
    
[^85]: 由生成闭环人工智能引领的基础科学的未来

    The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])

    [http://arxiv.org/abs/2307.07522](http://arxiv.org/abs/2307.07522)

    生成型人工智能和大型语言模型可能为基础科学的发现提供机会，通过其自主生成假设和探索假设空间的闭环方法，加速科学发现的进程。

    

    机器学习和人工智能的最新进展，包括生成型人工智能和大型语言模型，正在颠覆技术创新、产品开发和整个社会。人工智能对技术的贡献可以通过多种途径实现，需要大量训练数据集和明确的性能评估标准，范围从模式识别和分类到生成模型。然而，由于科学实践和模型发现需要访问高质量的大型数据集，人工智能对基础科学的贡献较少。生成型人工智能，特别是大型语言模型，可能代表了通过定量模型增强和加速基础深度科学的科学发现的机会。在这里，我们探索和研究了一种由人工智能驱动、自动化的闭环科学发现方法的各个方面，包括自主生成假设和开放式自主探索假设空间。

    Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
    
[^86]: Nexus sine qua non：基于节点识别的神经网络连接的时空预测多变量时间序列

    Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])

    [http://arxiv.org/abs/2307.01482](http://arxiv.org/abs/2307.01482)

    提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。

    

    建模和预测多变量时间序列不仅有助于从业者的决策，还加深我们对底层动态系统的科学理解。时空图神经网络（STGNNs）已经成为强大的预测器，并成为学习时空表示的事实标准模型。然而，现有的STGNNs的架构往往通过堆叠一系列复杂的层次而变得复杂。设计的模型可能多余或难以理解，这给复杂性和可扩展性带来了巨大挑战。这些问题促使我们重新审视现代STGNNs的设计，并确定对强大和高效的神经预测器有所贡献的核心原则。在这里，我们提出了一个紧凑的预测模型，完全由密集编码器-解码器和消息传递层来定义，基于节点识别，没有任何复杂的顺序模块，例如TCNs，RNNs和Transformers。通过实证重新评估该模型的性能，我们证明了该模型的有效性和高效性。

    Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
    
[^87]: 运输、变分推断和扩散：应用于回火流和薛定谔桥的论文研究

    Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])

    [http://arxiv.org/abs/2307.01050](http://arxiv.org/abs/2307.01050)

    本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。

    

    本文探讨了最优运输与变分推断之间的联系，重点研究了正向和反向随机微分方程以及Girsanov变换。我们提出了一个基于路径空间散度的采样和生成建模的原则性和系统性框架。我们的工作最终发展出一个新颖的基于得分的回火流技术（与统计物理中的Jarzynski和Crooks恒等式有关）和一个正则化的迭代比例拟合（IPF）型目标，不同于标准IPF的顺序性。通过一系列的生成建模示例和基于双井的稀有事件任务，我们展示了所提方法的潜力。

    This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
    
[^88]: 图神经网络从结构信息中获益的证明：一个特征学习的视角

    Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective. (arXiv:2306.13926v1 [cs.LG])

    [http://arxiv.org/abs/2306.13926](http://arxiv.org/abs/2306.13926)

    本论文研究了GNN在神经网络特征学习理论中的作用。 发现图卷积网络显著增强了良性过拟合区域，在这个区域内信号学习超越了噪声记忆。

    

    图神经网络(GNNs)在图表示学习方面取得了先驱性进展，在处理图输入时表现出比多层感知器(MLPs)更优越的特征学习和性能。然而，理解GNN的特征学习方面仍处于初始阶段。本研究旨在通过使用梯度下降训练研究图卷积在神经网络特征学习理论中的作用来弥补这一差距。我们提供了对两层图卷积网络(GCNs)中信号学习和噪声记忆的不同刻画，并将它们与两层卷积神经网络(CNNs)进行对比。我们的研究结果表明，与对应的CNNs相比，图卷积网络显著增强了良性过拟合区域，在这个区域内信号学习超越了噪声记忆，并且近似于因子$\sqrt{D}^{q-2}$，其中$D$表示节点的期望度数，$q$表示ReLU激活功能的幂次。

    Graph neural networks (GNNs) have pioneered advancements in graph representation learning, exhibiting superior feature learning and performance over multilayer perceptrons (MLPs) when handling graph inputs. However, understanding the feature learning aspect of GNNs is still in its initial stage. This study aims to bridge this gap by investigating the role of graph convolution within the context of feature learning theory in neural networks using gradient descent training. We provide a distinct characterization of signal learning and noise memorization in two-layer graph convolutional networks (GCNs), contrasting them with two-layer convolutional neural networks (CNNs). Our findings reveal that graph convolution significantly augments the benign overfitting regime over the counterpart CNNs, where signal learning surpasses noise memorization, by approximately factor $\sqrt{D}^{q-2}$, with $D$ denoting a node's expected degree and $q$ being the power of the ReLU activation function where 
    
[^89]: 时间序列预测中的对比学习的重要性是什么？

    What Constitutes Good Contrastive Learning in Time-Series Forecasting?. (arXiv:2306.12086v1 [cs.LG])

    [http://arxiv.org/abs/2306.12086](http://arxiv.org/abs/2306.12086)

    本文通过对比分析各种训练变量(包括不同的SSCL算法、学习策略，模型体系结构以及它们之间的相互作用)的有效性，研究了SSCL在时间序列预测中的影响及具体好处。

    

    近年来，自监督对比学习(Self-Supervised Contrastive Learning, SSCL)在各个领域中(包括自然语言处理和计算机视觉等)的引入已经展示了在表示学习方面的显著提升。通过利用自监督的潜在优势，SSCL使用大量无标签数据进行了表示模型的预训练。尽管这些进展，但仍然存在一个显著的差距——即我们对于不同的SSCL策略对时间序列预测性能的影响以及SSCL所带来的具体好处理解不足。本文旨在通过对各种训练变量的有效性进行全面分析来解决这些差距，其中包括不同的SSCL算法、学习策略、模型体系结构以及它们之间的相互作用。此外，为了深入了解SSCL在时间序列预测背景下带来的改进，我们还进行了经验感受野的定性分析。

    In recent years, the introduction of self-supervised contrastive learning (SSCL) has demonstrated remarkable improvements in representation learning across various domains, including natural language processing and computer vision. By leveraging the inherent benefits of self-supervision, SSCL enables the pre-training of representation models using vast amounts of unlabeled data. Despite these advances, there remains a significant gap in understanding the impact of different SSCL strategies on time series forecasting performance, as well as the specific benefits that SSCL can bring. This paper aims to address these gaps by conducting a comprehensive analysis of the effectiveness of various training variables, including different SSCL algorithms, learning strategies, model architectures, and their interplay. Additionally, to gain deeper insights into the improvements brought about by SSCL in the context of time-series forecasting, a qualitative analysis of the empirical receptive field i
    
[^90]: 增强属性聚类的图形变换方法：一种创新的图形转换器方法

    Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11307](http://arxiv.org/abs/2306.11307)

    本文提出了一种称为GTAGC的图形自编码器图形变换自编码器方法，通过融合图自编码器和图形变换器，GTAGC能够捕获全局依赖关系，从而有助于提高图聚类的性能。

    

    图表示学习是一种有影响力的方法，它使得我们更深入地理解图结构化数据，并有助于图聚类，这是各个领域的一个关键任务。注意力机制最近已经进入了图学习的领域，这从根本上改变了研究趋势。因此，图注意力网络和图注意力自编码器已成为图聚类任务优选的工具。然而，这些方法主要采用局部注意机制，从而限制了它们理解图中节点之间复杂全局依赖性的能力。为了解决这些障碍，本研究介绍了一种称为图自编码器的图形变换自编码器的图形聚类（GTAGC）的创新方法。通过将图自编码器与图形变换器融合，GTAGC能够捕获节点之间的全局依赖关系。这种融合提高了性能，使得图形聚类任务有了显着的提升。

    Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi
    
[^91]: 分散化SGD和平均方向SAM在渐近意义下是等价的

    Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])

    [http://arxiv.org/abs/2306.02913](http://arxiv.org/abs/2306.02913)

    分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力

    

    分散随机梯度下降（D-SGD）允许在没有中央服务器的控制下，大量设备同时进行协作学习。然而，现有理论认为，分散化不可避免地削弱了泛化能力。本文挑战传统信念，提出了完全新的角度来理解分散学习。我们证明了在一般非凸非-$\beta$-平滑设置下，D-SGD隐式地最小化了平均方向锐度感知最小化（SAM）算法的损失函数。这种惊人的渐近等价揭示了内在的正则化-优化权衡以及分散化的三个优点：（1）D-SGD中存在一个自由的不确定性评估机制，可以提高后验估计；（2）D-SGD表现出梯度平滑效应；（3）D-SGD的锐度正则化效应不会随着总批处理大小的增加而减少，这证明了潜在的泛化能力

    Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
    
[^92]: 排列决策树

    Permutation Decision Trees. (arXiv:2306.02617v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02617](http://arxiv.org/abs/2306.02617)

    该论文提出了一种称为排列决策树的模型，通过引入一种新的复杂度度量方法，能够捕捉到数据实例的顺序依赖性，在不同排列的数据实例上得到不同的决策树模型，从而克服了传统决策树模型在处理顺序相关数据时的限制。

    

    决策树是一种基于最小化内部节点中的不纯度的机器学习模型。最常见的不纯度度量是香农熵和基尼不纯度。这些不纯度度量对训练数据的顺序不敏感，因此得到的最终树对数据的任何排列都是不变的。这导致了在建模存在顺序依赖性的数据实例时的严重限制。在这项工作中，我们首次提出使用“压缩努力”(ETC) - 一种复杂度度量，作为不纯度度量。与香农熵和基尼不纯度不同，基于ETC的结构性不纯度能够捕捉到数据的顺序依赖性，从而为相同数据实例的不同排列获得潜在不同的决策树（排列决策树）。然后，我们引入了使用排列决策树实现排列Bagging的概念，而无需随机特征选择和子采样。我们比较了翻译包…(信息不全)

    Decision Tree is a well understood Machine Learning model that is based on minimizing impurities in the internal nodes. The most common impurity measures are Shannon entropy and Gini impurity. These impurity measures are insensitive to the order of training data and hence the final tree obtained is invariant to any permutation of the data. This leads to a serious limitation in modeling data instances that have order dependencies. In this work, we propose the use of Effort-To-Compress (ETC) - a complexity measure, for the first time, as an impurity measure. Unlike Shannon entropy and Gini impurity, structural impurity based on ETC is able to capture order dependencies in the data, thus obtaining potentially different decision trees for different permutations of the same data instances (Permutation Decision Trees). We then introduce the notion of Permutation Bagging achieved using permutation decision trees without the need for random feature selection and sub-sampling. We compare the pe
    
[^93]: 多目标学习中的三重权衡：优化、泛化和冲突避免

    Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance. (arXiv:2305.20057v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.20057](http://arxiv.org/abs/2305.20057)

    本文研究了多目标学习中的三重权衡问题，通过研究动态加权算法在MoDo算法中的泛化性能和与优化的相互作用，发现了动态加权方法的局限性。

    

    在新兴的机器学习问题中，当存在多个学习准则或多个学习任务时，多目标学习（MOL）问题经常出现。最近的研究提出了各种动态加权算法用于MOL，如MGDA及其变种，其核心思想是找到一个能够避免目标冲突的更新方向。尽管其直观吸引人，实证研究表明动态加权方法并不总是优于静态方法。为了理解这一理论与实践的差距，我们重点研究了MGDA的新随机变体-多目标梯度双采样（MoDo）算法，并通过算法稳定性的视角研究了基于动态加权的MoDo算法的泛化性能以及其与优化的相互作用。出乎意料的是，我们发现MGDA背后的关键原理-沿着避免冲突的方向进行更新-可能会阻碍动态加权算法实现${\cal O}(1/\sqrt{n})$的最优泛化性能。

    Multi-objective learning (MOL) problems often arise in emerging machine learning problems when there are multiple learning criteria or multiple learning tasks. Recent works have developed various dynamic weighting algorithms for MOL such as MGDA and its variants, where the central idea is to find an update direction that avoids conflicts among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not always outperform static ones. To understand this theory-practical gap, we focus on a new stochastic variant of MGDA - the Multi-objective gradient with Double sampling (MoDo) algorithm, and study the generalization performance of the dynamic weighting-based MoDo and its interplay with optimization through the lens of algorithm stability. Perhaps surprisingly, we find that the key rationale behind MGDA -- updating along conflict-avoidant direction - may hinder dynamic weighting algorithms from achieving the optimal ${\cal O}(1/\sqrt{n})$ popu
    
[^94]: MADiff：离线多智能体学习与扩散模型

    MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])

    [http://arxiv.org/abs/2305.17330](http://arxiv.org/abs/2305.17330)

    本论文提出了基于注意力的扩散模型MADiff，解决了多智能体问题，是第一个扩散模型应用于多智能体离线RL的框架。

    

    扩散模型（DM）是一种强大的生成模型，最近在包括离线强化学习在内的各种场景中取得了巨大成功，其中策略通过在在线评估中产生轨迹来进行规划学习。然而，尽管单智能体学习显示了其有效性，但仍不清楚DM如何在多智能体问题中操作，其中代理商很难在独立建模每个代理商轨迹的情况下完成团队合作。在本文中，我们提出MADiff，一种新的生成式多智能体学习框架，以解决这个问题。MADiff是通过基于注意力的扩散模型来实现对多个扩散智能体行为的复杂协调建模。据我们所知，MADiff是第一个基于扩散的多智能体离线RL框架，它既可以行为为分散的政策，又可以为集中控制器，其中包括对手建模，并可用于多智能体轨迹预测。

    Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
    
[^95]: 当前机器学习需要多少样本才能利用平滑性？

    How many samples are needed to leverage smoothness?. (arXiv:2305.16014v1 [stat.ML])

    [http://arxiv.org/abs/2305.16014](http://arxiv.org/abs/2305.16014)

    本文通过研究泛化误差的新下界，探讨了学习平滑函数时需要的样本数量及其机器学习问题中的挑战。

    

    统计学习的核心原则之一是，目标函数的平滑性可以打破维度灾难。然而，通过泰勒展开学习平滑函数需要足够接近一起的样本来获得高阶导数的有意义估计，这在数据量相对较小的机器学习问题中似乎很困难。本文通过推导广义泛化误差的新的下界，研究了常数和瞬态区域在实践中通常被忽略却发挥了主导作用的问题。

    A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function through Taylor expansions requires enough samples close to one another to get meaningful estimate of high-order derivatives, which seems hard in machine learning problems where the ratio between number of data and input dimension is relatively small. Should we really hope to break the curse of dimensionality based on Taylor expansion estimation? What happens if Taylor expansions are replaced by Fourier or wavelet expansions? By deriving a new lower bound on the generalization error, this paper investigates the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while that play a dominant role in practice.
    
[^96]: 在重症监护室中评估社会决定因素对健康预测的影响。

    Evaluating the Impact of Social Determinants on Health Prediction in the Intensive Care Unit. (arXiv:2305.12622v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12622](http://arxiv.org/abs/2305.12622)

    该论文研究了社会决定因素对重症监护室中健康预测的影响。研究发现，对于一般患者群体来说，SDOH特征并不能提高预测模型的性能，但对于特定亚群体的数据受限模型的公平性具有改善作用。

    

    健康的社会决定因素（SDOH）──人们居住、成长和老去的条件──对一个人的健康和幸福起着至关重要的作用。人口健康研究中的大量有力证据显示，各种各样的SDOH与健康结果密切相关。然而，大多数基于电子健康记录（EHR）的风险预测模型并没有涵盖全面的SDOH特征，因为它们通常噪声很大或者根本无法获取。我们的研究将一个公开可用的EHR数据库MIMIC-IV与被充分记录的SDOH特征联系起来。我们研究了这些特征对不同患者群体的常见EHR预测任务的影响。我们发现，对于一般的患者群体来说，社区层面的SDOH特征并不能提高模型的性能，但可以提高特定亚群体的数据受限模型的公平性。我们还证明，SDOH特征对于进行详尽的算法偏见审计非常重要。

    Social determinants of health (SDOH) -- the conditions in which people live, grow, and age -- play a crucial role in a person's health and well-being. There is a large, compelling body of evidence in population health studies showing that a wide range of SDOH is strongly correlated with health outcomes. Yet, a majority of the risk prediction models based on electronic health records (EHR) do not incorporate a comprehensive set of SDOH features as they are often noisy or simply unavailable. Our work links a publicly available EHR database, MIMIC-IV, to well-documented SDOH features. We investigate the impact of such features on common EHR prediction tasks across different patient populations. We find that community-level SDOH features do not improve model performance for a general patient population, but can improve data-limited model fairness for specific subpopulations. We also demonstrate that SDOH features are vital for conducting thorough audits of algorithmic biases beyond protect
    
[^97]: 激发Web规模语音模型的潜在能力以实现零-shot任务泛化

    Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])

    [http://arxiv.org/abs/2305.11095](http://arxiv.org/abs/2305.11095)

    本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。

    

    本文研究了最近提出的Web规模语音模型Whisper的新兴功能，在使用提示工程技术调整模型后，适应了未见过的AVSR，CS-ASR和ST三个任务。我们设计了特定于任务的提示，要么利用另一个大规模模型，要么简单地操作默认提示中的特殊标记。实验证明，与默认提示相比，我们提出的提示使这三个零-shot任务的性能提高了10%到45％，甚至在一些数据集上超过了SotA监督模型。此外，我们的实验揭示了Whisper的许多有趣属性，包括其提示的鲁棒性，对口音的偏好以及潜在空间中的多语言理解。代码可在https://github.com/jasonppy/PromptingWhisper上找到。

    We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
    
[^98]: Seq-HGNN: 学习异构图上的序列节点表示

    Seq-HGNN: Learning Sequential Node Representation on Heterogeneous Graph. (arXiv:2305.10771v1 [cs.LG])

    [http://arxiv.org/abs/2305.10771](http://arxiv.org/abs/2305.10771)

    Seq-HGNN提出了一种新颖的异构图神经网络，具有序列节点表示，通过学习一种序列节点表示机制，避免了由单个向量节点表示引起的信息丢失。

    

    在信息检索应用中，异构图神经网络（HGNN）得到了迅速发展。许多现有的HGNN设计了多种量身定制的图卷积来捕获异构图中的结构和语义信息。然而，现有的HGNN通常将每个节点表示为多层图卷积计算中的单个向量，这使得高层图卷积层无法区分来自不同关系和不同顺序的信息，导致信息传递中的信息丢失。为了解决这个问题，我们提出了一种新颖的异构图神经网络，即Seq-HGNN，它具有序列节点表示。为了避免由单个向量节点表示引起的信息丢失，我们首先设计了一种序列节点表示学习机制，在节点信息传递期间将每个节点表示为一系列元路径表示。

    Recent years have witnessed the rapid development of heterogeneous graph neural networks (HGNNs) in information retrieval (IR) applications. Many existing HGNNs design a variety of tailor-made graph convolutions to capture structural and semantic information in heterogeneous graphs. However, existing HGNNs usually represent each node as a single vector in the multi-layer graph convolution calculation, which makes the high-level graph convolution layer fail to distinguish information from different relations and different orders, resulting in the information loss in the message passing. %insufficient mining of information. To this end, we propose a novel heterogeneous graph neural network with sequential node representation, namely Seq-HGNN. To avoid the information loss caused by the single vector node representation, we first design a sequential node representation learning mechanism to represent each node as a sequence of meta-path representations during the node message passing. The
    
[^99]: 面向三维MOT中的点云目标再识别

    Towards Object Re-Identification from Point Clouds for 3D MOT. (arXiv:2305.10210v1 [cs.CV])

    [http://arxiv.org/abs/2305.10210](http://arxiv.org/abs/2305.10210)

    该论文研究面向三维MOT中的点云再识别问题，提出了一种轻量级匹配头用于点云ReID的网络，通过实验结果表明，随着传感器分辨率的提高和观测点密度的增加，点云ReID的表现逐渐接近于图像ReID。

    

    本研究旨在通过学习从剪裁的点云观测中匹配对象对（例如使用其预测的三维边界框）来解决三维多目标跟踪（MOT）上的对象再识别（ReID）问题。 我们不关心三维MOT的SOTA性能，而是追求回答以下问题：在实际的跟踪检测环境中，与图片中的ReID相比，来自点云的对象ReID的表现如何？ 为了实现这样的研究，我们提出了一个可以连接到任何集合或序列处理骨干（例如PointNet或ViT）的轻量级匹配头，为两种模态创造可比较的对象ReID网络家族。在孪生样式下运行，我们提出的点云ReID网络可以在实时（10 hz）中进行数千个成对比较。我们的研究结果表明，其表现随着更高的传感器分辨率而提高，并在观测足够密集时接近图像ReID的表现。

    In this work, we study the problem of object re-identification (ReID) in a 3D multi-object tracking (MOT) context, by learning to match pairs of objects from cropped (e.g., using their predicted 3D bounding boxes) point cloud observations. We are not concerned with SOTA performance for 3D MOT, however. Instead, we seek to answer the following question: In a realistic tracking by-detection context, how does object ReID from point clouds perform relative to ReID from images? To enable such a study, we propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in siamese style, our proposed point-cloud ReID networks can make thousands of pairwise comparisons in real-time (10 hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Ad
    
[^100]: 无法学习的样本给出了一种虚假的安全感：通过可学习的例子穿透那些无法利用的数据

    Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])

    [http://arxiv.org/abs/2305.09241](http://arxiv.org/abs/2305.09241)

    “无法学习的样本”提出一种对数据进行保护的方法，但它无法阻止未经授权的用户对保护后的数据进行利用。通过提出“可学习的未经授权示例”和一种新的纯化过程，我们可以实现对数据的更好保护。

    

    在当下随处可见的安全漏洞中，保护数据免于未经授权的利用是至关重要的。最近，一种叫做“无法学习的样本”（UEs）的方法被提出，通过对数据进行微小的扰动，使得模型无法在原始的干净分布上准确地对其进行分类，从而提供了一种强大的保护措施。然而，我们发现 UEs 带来的安全威胁是虚假的，因为它们无法阻止未经授权的用户利用其他未受保护的数据来去除保护，将无法学习的数据重转为可学习。基于这一观察，我们正式定义了一种威胁，引入了“可学习的未经授权示例”（LEs），这些是已经去除保护的UEs。我们的方法的核心是通过一种新的纯化过程，将UEs投射到LEs的流形上。这是通过一种新的联合条件扩散模型来实现的，该模型对UEs进行去噪。

    Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
    
[^101]: 机器学习中的公正性与医疗保健中的平等相遇

    Fairness in Machine Learning meets with Equity in Healthcare. (arXiv:2305.07041v1 [cs.LG])

    [http://arxiv.org/abs/2305.07041](http://arxiv.org/abs/2305.07041)

    本文提出了一个基于软件工程原理的人工智能框架，用于在确保医疗保健公正的同时，识别和减轻数据和模型中的偏见。未来的研究旨在在真实临床环境中测试和验证框架，以评估其在促进健康公平方面的影响。

    

    随着机器学习在医疗保健中的日益应用，提高医疗保健效果和效率的潜力不断增加。然而，这也带来了潜在的风险，即在数据和模型设计中延续偏见，从而伤害某些受保护群体，如年龄、性别和种族。本研究提出了一个基于软件工程原理的人工智能框架，用于在确保医疗保健公正的同时，识别和减轻数据和模型中的偏见。通过案例研究展示了数据中系统性偏见如何导致模型预测中的放大偏见，并提出机器学习方法以预防此类偏见。未来的研究旨在在真实临床环境中测试和验证所提出的ML框架，以评估其在促进健康公平方面的影响。

    With the growing utilization of machine learning in healthcare, there is increasing potential to enhance healthcare outcomes and efficiency. However, this also brings the risk of perpetuating biases in data and model design that can harm certain protected groups based on factors such as age, gender, and race. This study proposes an artificial intelligence framework, grounded in software engineering principles, for identifying and mitigating biases in data and models while ensuring fairness in healthcare settings. A case study is presented to demonstrate how systematic biases in data can lead to amplified biases in model predictions, and machine learning methods are suggested to prevent such biases. Future research aims to test and validate the proposed ML framework in real-world clinical settings to evaluate its impact on promoting health equity.
    
[^102]: Tiny-PPG: 用于边缘设备上实时检测光电容积脉搏图信号中运动伪影的轻量级深度神经网络

    Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices. (arXiv:2305.03308v1 [eess.SP])

    [http://arxiv.org/abs/2305.03308](http://arxiv.org/abs/2305.03308)

    Tiny-PPG是一个用于边缘设备上实时检测PPG信号中运动伪影的轻量级深度神经网络，通过使用深度可分离卷积和空洞空间金字塔池化模块，以及通道注意机制，能够在平衡检测精度和速度的情况下实现最先进的检测效果，并可以在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。

    

    尽管光电容积脉搏图（PPG）信号已经被广泛应用于基于物联网的可穿戴设备和智能健康设备中进行心血管健康监护，但在现实世界中，PPG信号很容易受到运动伪影的污染。本研究提出了一种名为“Tiny-PPG”的轻量级深度神经网络，用于在物联网边缘设备上准确实时地分割PPG伪影。该模型在公开数据集PPG DaLiA上进行了训练和测试，该数据集使用手表式设备（Empatica E4）对15名受试者在各种日常活动中的PPG信号，具有不同长度和形态的复杂伪影。该模型结构、训练方法和损失函数特别设计，以平衡检测精度和速度，适用于资源受限的嵌入式设备上进行实时PPG伪影检测。为了优化多尺度特征表示的模型大小和能力，该模型采用深度可分离卷积和空洞空间金字塔池化模块。此外，还提出了一个通道注意机制，以跨通道学习区分性和信息丰富的PPG信号特征。实验结果表明，Tiny-PPG在检测精度和计算效率方面均实现了最先进的性能，并可在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。

    Photoplethysmogram (PPG) signals are easily contaminated by motion artifacts in real-world settings, despite their widespread use in Internet-of-Things (IoT) based wearable and smart health devices for cardiovascular health monitoring. This study proposed a lightweight deep neural network, called Tiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edge devices. The model was trained and tested on a public dataset, PPG DaLiA, which featured complex artifacts with diverse lengths and morphologies during various daily activities of 15 subjects using a watch-type device (Empatica E4). The model structure, training method and loss function were specifically designed to balance detection accuracy and speed for real-time PPG artifact detection in resource-constrained embedded devices. To optimize the model size and capability in multi-scale feature representation, the model employed deep separable convolution and atrous spatial pyramid pooling modules, respectively. Addition
    
[^103]: G-MATT: 分子语法树变换器的单步回溯合成预测

    G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer. (arXiv:2305.03153v1 [cs.LG])

    [http://arxiv.org/abs/2305.03153](http://arxiv.org/abs/2305.03153)

    G-MATT是一个结合数据驱动模型与化学知识的化学感知回溯合成预测框架，在分层SMILES语法树输入的基础上采用树到序列变换器架构，能够显著提高单步回溯合成的准确率。

    

    近年来，已经报道了几种基于反应模板和基于自由模板的单步回溯合成预测方法。尽管这些方法中的许多在传统数据驱动指标方面表现良好，但使用的模型架构与支配反向合成的底层化学原则之间存在脱节。在这里，我们提出了一种新颖的化学感知回溯合成预测框架，将强大的数据驱动模型与化学知识相结合。我们报告了一种基于分层SMILES语法树的树到序列变换器架构，其中包含被纯SMILES表示法的模型忽略的底层化学信息。所提出的框架，基于语法的分子注意力树变换器（G-MATT），与基线回溯合成模型相比，实现了显着的性能提高。 G-MATT的准确率排名前1为51％（前10为79.1％），无效率为1.5％，

    In recent years, several reaction templates-based and template-free approaches have been reported for single-step retrosynthesis prediction. Even though many of these approaches perform well from traditional data-driven metrics standpoint, there is a disconnect between model architectures used and underlying chemistry principles governing retrosynthesis. Here, we propose a novel chemistry-aware retrosynthesis prediction framework that combines powerful data-driven models with chemistry knowledge. We report a tree-to-sequence transformer architecture based on hierarchical SMILES grammar trees as input containing underlying chemistry information that is otherwise ignored by models based on purely SMILES-based representations. The proposed framework, grammar-based molecular attention tree transformer (G-MATT), achieves significant performance improvements compared to baseline retrosynthesis models. G-MATT achieves a top-1 accuracy of 51% (top-10 accuracy of 79.1%), invalid rate of 1.5%, a
    
[^104]: 自动去偏重重配作为线性回归

    Augmented balancing weights as linear regression. (arXiv:2304.14545v1 [stat.ME])

    [http://arxiv.org/abs/2304.14545](http://arxiv.org/abs/2304.14545)

    本文探索了自动去偏重重配的新颖特征描述，并将其等同于基于内核岭回归的单个欠平滑岭回归，进一步将这种方法推广到特定的结果和重配模型选择上。

    

    我们提供了对于自动去偏重重配(AutoDML)的新颖特征描述。这些估算器将结果建模与重配相结合，直接估计反向倾向积分权重。当结果与权重模型都是某些（可能是无限的）基础中的线性时，我们表明增强的估算器等同于具有将原始结果模型系数和OLS相结合的系数的单个线性模型；在许多设置中，增强估算器合并为仅使用OLS. 然后，我们将这些结果扩展到特定的结果和重配模型选择上。我们首先表明，使用(内核)岭回归作为结果和重配模型的联合估算器等同于单个、欠平滑(内核)岭回归；当考虑到渐近速率时，这一结果也成立。当代替权重模型为套索回归时，我们给出了特殊情况的解析表达式并且演示了…

    We provide a novel characterization of augmented balancing weights, also known as Automatic Debiased Machine Learning (AutoDML). These estimators combine outcome modeling with balancing weights, which estimate inverse propensity score weights directly. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the original outcome model coefficients and OLS; in many settings, the augmented estimator collapses to OLS alone. We then extend these results to specific choices of outcome and weighting models. We first show that the combined estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression; this also holds when considering asymptotic rates. When the weighting model is instead lasso regression, we give closed-form expressions for special cases and demonstrate
    
[^105]: 从弱文本监督中学习图像中的人际互动

    Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])

    [http://arxiv.org/abs/2304.14104](http://arxiv.org/abs/2304.14104)

    本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。

    

    人际互动是多样且依赖于上下文的，但先前的工作将它们视为分类，忽略了可能的互动的重尾。本文提出了一种新的学习人际互动的范式，将其作为自由文本从单一的静态图像中学习，从而允许对情况和人际关系的无限空间进行灵活建模。为了克服缺乏特定于此任务的标记数据的问题，我们使用知识蒸馏应用于由大型语言模型产生的合成字幕数据，以此生成伪标签。我们展示了通过这个过程产生的伪标签可以用于训练一种字幕模型，能有效理解图像中的人际互动，通过衡量我们预测的文本和语义质量与事实的基础性的各种指标来衡量。我们进一步展示了我们的方法在这个任务上的性能优于SOTA的图像字幕和情境识别模型。我们将公开我们的代码。

    Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
    
[^106]: 针对图文模型的样本特异性去偏方法

    Sample-Specific Debiasing for Better Image-Text Models. (arXiv:2304.13181v1 [cs.LG])

    [http://arxiv.org/abs/2304.13181](http://arxiv.org/abs/2304.13181)

    发现从训练数据集中均匀地抽取负样本会引入错误的负面样本。我们提出了一种纠正错误负面样本的新方法，即针对图文模型的样本特异性去偏方法。

    

    对于图文数据的自监督表征学习在医学应用中具有重要意义，例如图像分类、视觉定位和跨模态检索。一种常见的方法涉及对语义上相似（正）和不相似（负）的数据点进行对比。从训练数据集中均匀地抽取负样本会引入错误的负面样本，即将同属一类的样本视为不相似。在医疗保健数据中，潜在的类别分布是不均匀的，意味着错误的负面样本出现的比例高度不同。为了提高学得的表示质量，我们提出了一种纠正错误负面样本的新方法。我们的方法可以被看作是使用估计的样本特异性类别概率的去偏对比学习的变体。我们提供了目标函数的理论分析，并在图像和配对的图文数据集上展示了所提出的方法。我们的实验证明了我们的方法取得的更好的效果。

    Self-supervised representation learning on image-text data facilitates crucial medical applications, such as image classification, visual grounding, and cross-modal retrieval. One common approach involves contrasting semantically similar (positive) and dissimilar (negative) pairs of data points. Drawing negative samples uniformly from the training data set introduces false negatives, i.e., samples that are treated as dissimilar but belong to the same class. In healthcare data, the underlying class distribution is nonuniform, implying that false negatives occur at a highly variable rate. To improve the quality of learned representations, we develop a novel approach that corrects for false negatives. Our method can be viewed as a variant of debiased constrastive learning that uses estimated sample-specific class probabilities. We provide theoretical analysis of the objective function and demonstrate the proposed approach on both image and paired image-text data sets. Our experiments demo
    
[^107]: UDTIRI:一个开源的道路坑洞检测基准套件

    UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])

    [http://arxiv.org/abs/2304.08842](http://arxiv.org/abs/2304.08842)

    该论文介绍了一个开源的道路坑洞检测基准套件UDTIRI，包含了标记齐全的1000张道路坑洞图像，可以用于深度学习方法在城市道路检查中的目标检测、语义分割和实例分割任务。

    

    看到在城市数字孪生领域中利用强大的深度学习方法的巨大潜力。特别是在智能道路检查领域，目前研究和数据有限。为了促进这一领域的进展，我们开发了一个名为Urban Digital Twins Intelligent Road Inspection (UDTIRI)数据集的标记齐全的道路坑洞数据集。我们希望这个数据集能够让强大的深度学习方法在城市道路检查中发挥作用，让算法更全面地理解场景并最大化其潜力。我们的数据集包括1000张道路坑洞图像，拍摄于不同的情境中，具有不同的光照和湿度条件。我们的意图是将这个数据集应用于目标检测、语义分割和实例分割任务。我们的团队花费了大量精力进行了详细的统计分析，并对UDTIRI数据集的一些代表性深度学习模型进行了基准测试。

    It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
    
[^108]: 分类中异方差标签噪声的逻辑正态似然

    Logistic-Normal Likelihoods for Heteroscedastic Label Noise in Classification. (arXiv:2304.02849v1 [cs.LG])

    [http://arxiv.org/abs/2304.02849](http://arxiv.org/abs/2304.02849)

    该论文介绍了一种新的分类方法，用于提高鲁棒性，减少标签噪声的影响，其基于正态分布，并可通过最小化负对数似然来学习参数。

    

    在回归中估计异方差标签噪声的一种自然方法是将观测到的（可能带有噪声的）目标建模为一个正态分布的样本，其参数可以通过最小化负对数似然来学习。该损失具有期望的损失衰减特性，因为它可以降低高误差示例的贡献。直观地说，这种行为可以通过减少过拟合来提高对标签噪声的鲁棒性。我们提出了这种简单且概率化方法在分类中的扩展，具有相同的期望损失衰减特性。我们通过测量其对分类中标签噪声的鲁棒性来评估该方法的有效性。我们进行了启发性的实验，探索了该方法的内部工作原理，包括对超参数的敏感性，消融研究等。

    A natural way of estimating heteroscedastic label noise in regression is to model the observed (potentially noisy) target as a sample from a normal distribution, whose parameters can be learned by minimizing the negative log-likelihood. This loss has desirable loss attenuation properties, as it can reduce the contribution of high-error examples. Intuitively, this behavior can improve robustness against label noise by reducing overfitting. We propose an extension of this simple and probabilistic approach to classification that has the same desirable loss attenuation properties. We evaluate the effectiveness of the method by measuring its robustness against label noise in classification. We perform enlightening experiments exploring the inner workings of the method, including sensitivity to hyperparameters, ablation studies, and more.
    
[^109]: 基于准度量学习的最优目标达成强化学习方法

    Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.01203](http://arxiv.org/abs/2304.01203)

    本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数；在离线和在线的目标达成基准测试中，QRL展示了更好的采样效率和性能，包括基于状态和基于图像的观测。

    

    在目标达成强化学习中，最优价值函数具有特定的几何结构，称为准度量结构。本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数。与以往的方法不同，QRL的目标是专门为准度量设计的，并提供了强大的理论恢复保证。在离散化的MountainCar环境上进行了全面分析，确定了QRL的性质以及其优于其他方法的优势。在离线和在线的目标达成基准测试中，QRL还展示了更好的采样效率和性能，包括基于状态和基于图像的观测。

    In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
    
[^110]: 多任务学习用于移植后死因分析：以肝移植为例的案例研究

    Multi-Task Learning for Post-transplant Cause of Death Analysis: A Case Study on Liver Transplant. (arXiv:2304.00012v1 [cs.LG])

    [http://arxiv.org/abs/2304.00012](http://arxiv.org/abs/2304.00012)

    CoD-MTL是一个新的死因分析框架，采用多任务学习来共同模拟不同CoD预测任务之间的语义关系，并且利用树蒸馏策略进行多任务学习，在具备树模型和多任务学习的优点中发掘最大化的利益。该框架可以提供精确可靠的CoD预测，具有重要的临床应用价值。

    

    器官移植是某些末期疾病（如肝衰竭）的基本治疗方法。分析器官移植后的死因可以为临床决策提供有力的工具，包括个性化治疗和器官分配。然而，传统方法如MELD评分和传统机器学习方法在死因分析中存在两个主要的数据和模型相关挑战，这限制了它们在此领域中的应用。为了克服这一难题，我们提出了一个新的框架CoD-MTL，利用多任务学习来共同模拟不同CoD预测任务之间的语义关系。具体来讲，我们开发了一种新的树蒸馏策略用于多任务学习，结合了树模型和多任务学习的优点。实验结果表明，我们的框架可以提供精确可靠的CoD预测。我们进行了一个案例研究，以展示该框架的临床重要性。

    Organ transplant is the essential treatment method for some end-stage diseases, such as liver failure. Analyzing the post-transplant cause of death (CoD) after organ transplant provides a powerful tool for clinical decision making, including personalized treatment and organ allocation. However, traditional methods like Model for End-stage Liver Disease (MELD) score and conventional machine learning (ML) methods are limited in CoD analysis due to two major data and model-related challenges. To address this, we propose a novel framework called CoD-MTL leveraging multi-task learning to model the semantic relationships between various CoD prediction tasks jointly. Specifically, we develop a novel tree distillation strategy for multi-task learning, which combines the strength of both the tree model and multi-task learning. Experimental results are presented to show the precise and reliable CoD predictions of our framework. A case study is conducted to demonstrate the clinical importance of 
    
[^111]: ACAT: 拟对抗反事实注意力用于医学图像分类和检测

    ACAT: Adversarial Counterfactual Attention for Classification and Detection in Medical Imaging. (arXiv:2303.15421v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.15421](http://arxiv.org/abs/2303.15421)

    ACAT使用拟对抗反事实注意力的方法可以提高医学图像分类和检测的准确性，在脑部CT扫描中将病变的分类准确率提高到72.55%，在肺部CT扫描中将与COVID-19相关结果的分类准确率提高到70.84%。

    

    在某些医学图像任务和其他仅有图像的小部分对分类任务有信息贡献的情况下，传统的卷积神经网络有时很难泛化。手动注释的感兴趣区域(ROI)有时被用来隔离图像中最具信息价值的部分。然而，这些注释很费时费力，并且在不同的注释者之间可能存在显著差异。为了克服这些问题，我们提出了一种框架，利用显著性图来获取软性空间注意力掩膜，调节不同尺度下的图像特征。我们称之为拟对抗反事实注意力(ACAT)方法。ACAT将脑部CT扫描中病变的基准分类准确率从71.39％提高到72.55％，将肺部CT扫描中与COVID-19相关结果的基准分类准确率从67.71％提高到70.84％，并超过了竞争方法的性能。我们研究了生成我们架构中使用的显著性图的最佳方法，并提出了一种从拟对抗生成中获取它们的方法。

    In some medical imaging tasks and other settings where only small parts of the image are informative for the classification task, traditional CNNs can sometimes struggle to generalise. Manually annotated Regions of Interest (ROI) are sometimes used to isolate the most informative parts of the image. However, these are expensive to collect and may vary significantly across annotators. To overcome these issues, we propose a framework that employs saliency maps to obtain soft spatial attention masks that modulate the image features at different scales. We refer to our method as Adversarial Counterfactual Attention (ACAT). ACAT increases the baseline classification accuracy of lesions in brain CT scans from 71.39% to 72.55% and of COVID-19 related findings in lung CT scans from 67.71% to 70.84% and exceeds the performance of competing methods. We investigate the best way to generate the saliency maps employed in our architecture and propose a way to obtain them from adversarially generated
    
[^112]: 通过小波神经算子实现Transformers的多尺度注意力机制

    Multiscale Attention via Wavelet Neural Operators for Vision Transformers. (arXiv:2303.12398v1 [cs.CV])

    [http://arxiv.org/abs/2303.12398](http://arxiv.org/abs/2303.12398)

    本文介绍了一种基于小波神经算子的多尺度注意力机制，它通过使用小波神经算子将注意力机制从局部扩展到全域和多尺度范围内，取得了比ViT和AFNO更显著的性能提高。

    

    Transformer在计算机视觉中取得了广泛的成功。其核心是自注意机制（SA），它是一种归纳偏见，通过加权基础将输入中的每个token与每个其他token相关联。标准的SA机制具有二次复杂度，难以处理高分辨率图像中出现的长序列。为此，我们引入了基于小波神经算子的Multiscale Wavelet Attention（MWA），使用小波神经算子将注意力机制从局部扩展到全域和多尺度范围内。我们用CIFAR和ImageNet进行了实验，结果表明，MWA比ViT和AFNO都表现出显著的性能提高。

    Transformers have achieved widespread success in computer vision. At their heart, there is a Self-Attention (SA) mechanism, an inductive bias that associates each token in the input with every other token through a weighted basis. The standard SA mechanism has quadratic complexity with the sequence length, which impedes its utility to long sequences appearing in high resolution vision. Recently, inspired by operator learning for PDEs, Adaptive Fourier Neural Operators (AFNO) were introduced for high resolution attention based on global convolution that is efficiently implemented via FFT. However, the AFNO global filtering cannot well represent small and moderate scale structures that commonly appear in natural images. To leverage the coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention (MWA) by leveraging wavelet neural operators which incurs linear complexity in the sequence size. We replace the attention in ViT with MWA and our experiments with CIFAR and ImageN
    
[^113]: 经典谱估计的非渐进式点值和最坏情况边界

    Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum Estimators. (arXiv:2303.11908v1 [math.ST])

    [http://arxiv.org/abs/2303.11908](http://arxiv.org/abs/2303.11908)

    本文给出了非渐进误差边界，为广泛类别的谱估计器提供了误差边界，包括在特定频率处的点值误差边界和所有频率下的最坏情况误差边界，并利用该方法导出了经典谱估计器（Blackman-Tukey、Bartlett 和 Welch 估计器）的误差边界。

    

    谱估计是时间序列数据分析的基本方法，应用包括医学、语音分析和控制设计。虽然谱估计的渐进理论很好理解，但在样本数量固定且有限的情况下，该理论存在一定局限性。本文为广泛类别的谱估计器提供了非渐进误差边界，包括在特定频率处的点值误差边界和所有频率下的最坏情况误差边界。本文所提的一般方法也用于导出了经典的 Blackman-Tukey、Bartlett 和 Welch 估计器的误差边界。

    Spectrum estimation is a fundamental methodology in the analysis of time-series data, with applications including medicine, speech analysis, and control design. The asymptotic theory of spectrum estimation is well-understood, but the theory is limited when the number of samples is fixed and finite. This paper gives non-asymptotic error bounds for a broad class of spectral estimators, both pointwise (at specific frequencies) and in the worst case over all frequencies. The general method is used to derive error bounds for the classical Blackman-Tukey, Bartlett, and Welch estimators.
    
[^114]: 使用文本到图像扩散模型定位对象水平的形状变化

    Localizing Object-level Shape Variations with Text-to-Image Diffusion Models. (arXiv:2303.11306v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.11306](http://arxiv.org/abs/2303.11306)

    本文介绍了一种利用文本到图像扩散模型，能够生成变化形状的图像集合，从而支持对象级形状探索过程的技术。通过引入提示混合技术和使用自注意力层的方法，实现了可控制的形状变化并准确定位图像空间操作。

    

    文本到图像模型引出的工作流通常从一个探索步骤开始，用户在其中浏览大量生成的图像。文本到图像生成过程的全局性质使得用户无法将探索范围限定在图像中的特定对象上。本文提出了一种技术，用于生成一系列展示特定对象形状变化的图像，从而支持对象级形状探索过程。生成可信的变化是具有挑战性的，因为它要求在保持对象语义的同时对生成对象的形状进行控制。在生成对象变化时，一个特别的挑战是准确定位应用于对象形状的操作。我们引入了一种提示混合技术，在去噪过程中切换提示以获得多样的形状选择。为了定位图像空间操作，我们提出了两种使用自注意力层的技术，与注入异常的图像进行编码以捕捉形状变化的细节。

    Text-to-image models give rise to workflows which often begin with an exploration step, where users sift through a large collection of generated images. The global nature of the text-to-image generation process prevents users from narrowing their exploration to a particular object in the image. In this paper, we present a technique to generate a collection of images that depicts variations in the shape of a specific object, enabling an object-level shape exploration process. Creating plausible variations is challenging as it requires control over the shape of the generated object while respecting its semantics. A particular challenge when generating object variations is accurately localizing the manipulation applied over the object's shape. We introduce a prompt-mixing technique that switches between prompts along the denoising process to attain a variety of shape choices. To localize the image-space operation, we present two techniques that use the self-attention layers in conjunction
    
[^115]: TARGET: 通过无样本蒸馏实现联邦类式持续学习

    TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation. (arXiv:2303.06937v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06937](http://arxiv.org/abs/2303.06937)

    本文研究了一个重要但鲜为人知的问题：联邦类式持续学习，在联邦学习中动态添加新的类别。我们提出了一种称为TARGET的新颖方法，通过无样本蒸馏来减轻FCCL中的灾难性遗忘问题，并保护客户数据的隐私。该方法利用先前训练的全局模型在模型层面上传递旧任务的知识，并通过生成器生成合成数据来模拟数据的全局分布。与先前的FCCL方法相比，TARGET无需额外的数据集或存储先前任务的私有数据。

    

    本文针对一个鲜为人知但重要的问题进行研究：联邦类式持续学习（FCCL），在联邦学习中动态添加新的类别。已有的FCCL方法存在各种限制，如需要额外的数据集或存储先前任务的私有数据。为此，我们首先证明非独立同分布的数据加剧了联邦学习中的灾难性遗忘问题。然后，我们提出了一种新颖的方法——TARGET（通过无样本蒸馏实现联邦类式持续学习），该方法在减轻FCCL的灾难性遗忘问题的同时保护客户数据的隐私。我们的方法利用先前训练的全局模型，在模型层面上将旧任务的知识传递给当前任务。此外，我们训练一个生成器来生成合成数据，以模拟每个客户端上数据的全局分布。与先前的FCCL方法相比，TARGET不需要额外的数据集或存储先前任务的私有数据。

    This paper focuses on an under-explored yet important problem: Federated Class-Continual Learning (FCCL), where new classes are dynamically added in federated learning. Existing FCCL works suffer from various limitations, such as requiring additional datasets or storing the private data from previous tasks. In response, we first demonstrate that non-IID data exacerbates catastrophic forgetting issue in FL. Then we propose a novel method called TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G} via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates catastrophic forgetting in FCCL while preserving client data privacy. Our proposed method leverages the previously trained global model to transfer knowledge of old tasks to the current task at the model level. Moreover, a generator is trained to produce synthetic data to simulate the global distribution of data on each client at the data level. Compared to previous FCCL methods, TARGET does not requi
    
[^116]: Miipher: 整合自监督语音和文本表示的稳健语音修复模型

    Miipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations. (arXiv:2303.01664v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.01664](http://arxiv.org/abs/2303.01664)

    Miipher是一个稳健的语音修复模型，通过整合自监督语音和文本表示，可以将受损的语音信号转换为高质量语音，并且可以应用于增加高质量训练数据的新型语音生成应用。

    

    语音修复（SR）是将受损的语音信号转换为高质量语音的任务。在本研究中，我们提出了一种称为Miipher的稳健SR模型，并将Miipher应用于一个新的SR应用：通过将从网络上收集的语音样本转换为工作室质量，增加高质量训练数据用于语音生成。为了使我们的SR模型对各种降解具有鲁棒性，我们使用了（i）从w2v-BERT提取的语音表示作为输入特征，以及（ii）通过PnG-BERT从转录中提取的文本表示作为语言条件特征。实验结果表明，Miipher（i）对各种音频降解具有鲁棒性，并且（ii）使我们能够从从网络上收集的修复后的语音样本中训练出高质量的文本到语音（TTS）模型。音频样本可在我们的演示页面上找到：google.github.io/df-conformer/miipher/

    Speech restoration (SR) is a task of converting degraded speech signals into high-quality ones. In this study, we propose a robust SR model called Miipher, and apply Miipher to a new SR application: increasing the amount of high-quality training data for speech generation by converting speech samples collected from the Web to studio-quality. To make our SR model robust against various degradation, we use (i) a speech representation extracted from w2v-BERT for the input feature, and (ii) a text representation extracted from transcripts via PnG-BERT as a linguistic conditioning feature. Experiments show that Miipher (i) is robust against various audio degradation and (ii) enable us to train a high-quality text-to-speech (TTS) model from restored speech samples collected from the Web. Audio samples are available at our demo page: google.github.io/df-conformer/miipher/
    
[^117]: 使用在线函数逼近的方法实现对抗性上下文MDPs的高效率最优遗憾

    Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation. (arXiv:2303.01464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01464](http://arxiv.org/abs/2303.01464)

    这篇论文提出了OMG-CMDP!算法，用于在对抗性上下文MDPs中最小化遗憾。该算法在具备可实现函数类和在线回归预言机的最小假设下操作，具有高效、简单和对近似错误的鲁棒性。它具有高效率最优的遗憾保证，是对抗CMDPs领域的首个满足最小假设条件的算法。

    

    我们提出了用于对抗性上下文MDPs遗憾最小化的OMG-CMDP！算法。该算法在可实现的函数类和在线最小二乘和对数损失回归预言机的最小假设下运行。我们的算法高效（假设在线回归预言机高效），简单且对近似错误具有鲁棒性。它享有$\widetilde{O}(H^{2.5} \sqrt{ T|S||A| ( \mathcal{R}(\mathcal{O}) + H \log(\delta^{-1}) )})$的遗憾保证，其中$T$是回合数，$S$是状态空间，$A$是动作空间，$H$是视界，$\mathcal{R}(\mathcal{O}) = \mathcal{R}(\mathcal{O}_{\mathrm{sq}}^\mathcal{F}) + \mathcal{R}(\mathcal{O}_{\mathrm{log}}^\mathcal{P})$是用于近似上下文相关奖励和动态的回归预言机遗憾之和。据我们所知，我们的算法是对抗CMDPs的第一个高效率最优遗憾最小化算法，并满足最小的假设条件。

    We present the OMG-CMDP! algorithm for regret minimization in adversarial Contextual MDPs. The algorithm operates under the minimal assumptions of realizable function class and access to online least squares and log loss regression oracles. Our algorithm is efficient (assuming efficient online regression oracles), simple and robust to approximation errors. It enjoys an $\widetilde{O}(H^{2.5} \sqrt{ T|S||A| ( \mathcal{R}(\mathcal{O}) + H \log(\delta^{-1}) )})$ regret guarantee, with $T$ being the number of episodes, $S$ the state space, $A$ the action space, $H$ the horizon and $\mathcal{R}(\mathcal{O}) = \mathcal{R}(\mathcal{O}_{\mathrm{sq}}^\mathcal{F}) + \mathcal{R}(\mathcal{O}_{\mathrm{log}}^\mathcal{P})$ is the sum of the regression oracles' regret, used to approximate the context-dependent rewards and dynamics, respectively. To the best of our knowledge, our algorithm is the first efficient rate optimal regret minimization algorithm for adversarial CMDPs that operates under the mi
    
[^118]: 联合任务和数据导向的语义通信：深度分离的源信道编码方案

    Joint Task and Data Oriented Semantic Communications: A Deep Separate Source-channel Coding Scheme. (arXiv:2302.13580v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2302.13580](http://arxiv.org/abs/2302.13580)

    本文提出了一种深度分离的源信道编码框架，用于联合任务和数据导向的语义通信，充分利用语义特征和较少的频谱资源，通过变分自编码器方法解决率失真问题。

    

    语义通信利用源数据的语义特征，通过较少的频谱资源实现各种语义任务。为了同时满足数据传输和语义任务，联合数据压缩和语义分析成为语义通信中的关键问题。本文提出了一种深度分离的源信道编码框架，用于联合任务和数据导向的语义通信，并利用变分自编码器方法解决具有语义失真的率失真问题。首先，通过分析源信道编码框架的贝叶斯模型，我们利用贝叶斯推理方法为一般数据分布和语义任务推导出一种新的率失真优化问题。接下来，针对联合图像传输和分类的典型应用，我们将变分自编码器方法与前向适应方案结合起来，有效提取图像特征和适应语义失真。

    Semantic communications are expected to accomplish various semantic tasks with relatively less spectrum resource by exploiting the semantic feature of source data. To simultaneously serve both the data transmission and semantic tasks, joint data compression and semantic analysis has become pivotal issue in semantic communications. This paper proposes a deep separate source-channel coding (DSSCC) framework for the joint task and data oriented semantic communications (JTD-SC) and utilizes the variational autoencoder approach to solve the rate-distortion problem with semantic distortion. First, by analyzing the Bayesian model of the DSSCC framework, we derive a novel rate-distortion optimization problem via the Bayesian inference approach for general data distributions and semantic tasks. Next, for a typical application of joint image transmission and classification, we combine the variational autoencoder approach with a forward adaption scheme to effectively extract image features and ad
    
[^119]: 目标网络如何稳定时间差分方法

    Why Target Networks Stabilise Temporal Difference Methods. (arXiv:2302.12537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12537](http://arxiv.org/abs/2302.12537)

    本文解释了深度强化学习中一种流行的时序差分方法中关键的稳定性问题：为什么目标网络能够有效降低不满足条件时的影响。

    

    深度强化学习中近期成功的关键在于一类使用不频繁更新目标值进行策略评估的时序差分方法。然而，有关目标网络有效性的完整理论解释仍然难以捉摸。本文针对这种流行算法进行了分析，最终回答了“为什么目标网络可以稳定时间差分学习”的问题。我们规范化了部分拟合的策略评估方法的概念，其中包括目标网络的使用，并且填补了拟合方法和半梯度时序差分算法之间的差距。利用这个框架，我们能够独特地描述所谓的致命三元组，即使用时序差分更新，结合（非线性）函数逼近和处于离线状态的数据，这经常会导致不收敛的算法。这一认识使我们得出结论：目标网络的使用可以减轻条件差时的影响。

    Integral to recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. Yet a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: `why do target networks stabilise TD learning'? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditionin
    
[^120]: 关于带批量归一化的随机梯度下降训练不稳定性的研究

    On the Training Instability of Shuffling SGD with Batch Normalization. (arXiv:2302.12444v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12444](http://arxiv.org/abs/2302.12444)

    本文研究了随机梯度下降算法与批量归一化的相互作用，在特定网络中表现为单次重排与随机重排收敛到不同的扭曲全局最优点，建议使用随机重排。

    

    本论文研究了随机梯度下降算法与批量归一化的相互作用，发现常用的单次重排和随机重排这两种 SGD 方式的训练稳定性差异非常大。实验证明，在使用批量归一化的线性网络回归问题中，单次重排和随机重排会分别收敛到扭曲的全局最优点，而在分类问题中，我们进一步探究了它们的训练是否会发散，并提出了实证验证结果，建议在使用 SGD 与批量归一化时，优先考虑随机重排。

    We uncover how SGD interacts with batch normalization and can exhibit undesirable training dynamics such as divergence. More precisely, we study how Single Shuffle (SS) and Random Reshuffle (RR) -- two widely used variants of SGD -- interact surprisingly differently in the presence of batch normalization: RR leads to much more stable evolution of training loss than SS. As a concrete example, for regression using a linear network with batch normalization, we prove that SS and RR converge to distinct global optima that are "distorted" away from gradient descent. Thereafter, for classification we characterize conditions under which training divergence for SS and RR can, and cannot occur. We present explicit constructions to show how SS leads to distorted optima in regression and divergence for classification, whereas RR avoids both distortion and divergence. We validate our results by confirming them empirically in realistic settings, and conclude that the separation between SS and RR use
    
[^121]: 使用自动算法配置实现参数控制

    Using Automated Algorithm Configuration for Parameter Control. (arXiv:2302.12334v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12334](http://arxiv.org/abs/2302.12334)

    本研究通过自动算法配置来解决参数控制的问题，并提出了能够显著提高方法性能的技术。

    

    动态算法配置（DAC）通过数据驱动的方式自动学习控制算法参数的策略。最近几年，这个问题在进化社区中受到了相当多的关注。为了对不同DAC解决方法的有效性和限制进行结构性理解，拥有一个好的基准集合是非常有必要的。在最近关于提供具有可理解的理论属性和基准真值信息的DAC基准的工作中，本文建议将关键参数λ的控制作为新的DAC基准，用于解决OneMax问题的(1+(λ,λ))遗传算法。我们对如何通过使用（静态）自动算法配置在基准测试中解决DAC问题进行了研究，并提出了可以显著提高方法性能的技术。我们的方法能够在基准测试中始终优于默认方案。

    Dynamic Algorithm Configuration (DAC) tackles the question of how to automatically learn policies to control parameters of algorithms in a data-driven fashion. This question has received considerable attention from the evolutionary community in recent years. Having a good benchmark collection to gain structural understanding on the effectiveness and limitations of different solution methods for DAC is therefore strongly desirable. Following recent work on proposing DAC benchmarks with well-understood theoretical properties and ground truth information, in this work, we suggest as a new DAC benchmark the controlling of the key parameter $\lambda$ in the $(1+(\lambda,\lambda))$~Genetic Algorithm for solving OneMax problems. We conduct a study on how to solve the DAC problem via the use of (static) automated algorithm configuration on the benchmark, and propose techniques to significantly improve the performance of the approach. Our approach is able to consistently outperform the default 
    
[^122]: 基于图神经网络的时变信号恢复

    Time-varying Signals Recovery via Graph Neural Networks. (arXiv:2302.11313v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.11313](http://arxiv.org/abs/2302.11313)

    本文提出了一种基于图神经网络的时变信号恢复方法，通过编码器-解码器架构与Sobolev平滑算子组成的专门损失函数，可以有效地恢复时变图信号。

    

    时变图信号的恢复是传感器网络和时间序列预测中具有广泛应用的一个基本问题。有效地捕捉这些信号的时空信息对下游任务至关重要。以前的研究使用这些图信号时间差分的平滑性作为一个初始假设。然而，当该先验不成立时，这种平滑性假设可能导致性能下降。在本工作中，我们通过包含一个学习模块来放松这个假设的需求。我们提出了一种用于恢复时变图信号的时间图神经网络(TimeGNN)。我们的算法使用编码器-解码器架构，并采用由均方误差函数和Sobolev平滑算子组成的专门损失。TimeGNN在真实数据集上显示出与以前方法竞争的性能。

    The recovery of time-varying graph signals is a fundamental problem with numerous applications in sensor networks and forecasting in time series. Effectively capturing the spatio-temporal information in these signals is essential for the downstream tasks. Previous studies have used the smoothness of the temporal differences of such graph signals as an initial assumption. Nevertheless, this smoothness assumption could result in a degradation of performance in the corresponding application when the prior does not hold. In this work, we relax the requirement of this hypothesis by including a learning module. We propose a Time Graph Neural Network (TimeGNN) for the recovery of time-varying graph signals. Our algorithm uses an encoder-decoder architecture with a specialized loss composed of a mean squared error function and a Sobolev smoothness operator.TimeGNN shows competitive performance against previous methods in real datasets.
    
[^123]: LabelPrompt: 关于关系分类的有效的提示式学习

    LabelPrompt: Effective Prompt-based Learning for Relation Classification. (arXiv:2302.08068v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08068](http://arxiv.org/abs/2302.08068)

    LabelPrompt是一种面向关系分类任务的提示式学习方法，通过定义额外的令牌来表示关系标签，并使用提示模板方法明确构建它们，从而解决了将填充掩码标记的自然语言词汇与语义关系标签相关联的挑战。同时，该方法还实现了一个实体感知模块来减轻预测关系和给定实体之间的不一致性。

    

    最近，通过将自然语言处理（NLP）任务转换为填空式格式，以更好地与预训练语言模型（PLMs）对齐的方式，提示式学习在许多NLP任务中变得流行起来。然而，将这种方法应用于关系分类任务面临着独特的挑战。具体而言，将填充掩码标记的自然语言词汇与语义关系标签（如"org:founded_by"）相关联是困难的。为了解决这个挑战，本文提出了一种新颖的提示式学习方法，称为LabelPrompt，用于关系分类任务。受到“给予模型选择”的直觉的启发，我们首先定义额外的令牌来表示关系标签，将这些令牌视为具有语义初始化的口述者，并使用提示模板方法明确构建它们。然后，为了减轻预测关系和给定实体之间的不一致性，我们实现了一个实体感知模块，并采用对抗性的方式进行引导。

    Recently, prompt-based learning has gained popularity across many natural language processing (NLP) tasks by reformulating them into a cloze-style format to better align pre-trained language models (PLMs) with downstream tasks. However, applying this approach to relation classification poses unique challenges. Specifically, associating natural language words that fill the masked token with semantic relation labels (\textit{e.g.} \textit{``org:founded\_by}'') is difficult. To address this challenge, this paper presents a novel prompt-based learning method, namely LabelPrompt, for the relation classification task. Motivated by the intuition to ``GIVE MODEL CHOICES!'', we first define additional tokens to represent relation labels, which regard these tokens as the verbaliser with semantic initialisation and explicitly construct them with a prompt template method. Then, to mitigate inconsistency between predicted relations and given entities, we implement an entity-aware module with contra
    
[^124]: 无先验因果学习

    Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12292](http://arxiv.org/abs/2301.12292)

    无先验因果学习是一个解决预测新型干预措施个性化影响的框架，并通过元学习对任务的处理达成了目的，能够将干预措施的知识传输到未见过的干预措施中，并在合成和真实数据集上表现出了优越性能。

    

    在个性化医疗、公共政策和在线营销等领域，预测不同干预措施对特定个体的因果影响非常重要。预测现有干预措施的影响有许多方法，这些方法基于接受过干预措施的个体的历史数据。然而，在许多场景中，预测新型干预措施的影响也很重要，这些方法无法解决。在这里，我们考虑了无先验因果学习：预测新型干预措施的个性化影响。我们提出了CaML，这是一个因果元学习框架，它将每个干预措施的个性化预测效果作为一个任务来进行处理。CaML在数千个任务中训练单一的元模型，每个任务都是通过抽样生成一个干预措施及其接收者和非接收者来构建的。通过利用干预信息（例如，药物的属性）和个体特征（例如，特定个体的医疗记录），CaML学习如何将已观察到的干预措施的知识有效地传输给未见过的干预措施。我们在合成和真实数据集上展示了我们方法的有效性，展示了该方法具有推广到未见过干预措施并胜过现有方法的能力。

    Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
    
[^125]: 在考虑虚假数据注入攻击和防御的坡道计量应用中分析深度强化学习算法的鲁棒性

    Analyzing Robustness of the Deep Reinforcement Learning Algorithm in Ramp Metering Applications Considering False Data Injection Attack and Defense. (arXiv:2301.12036v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12036](http://arxiv.org/abs/2301.12036)

    本研究提出了一种只使用环路检测器信息作为输入的深度Q学习算法，并通过设计虚假数据注入攻击和随机噪声攻击来研究模型的鲁棒性。

    

    坡道计量是控制进入高速公路主干道的车辆行驶的行为。几十年的坡道计量实践证明，坡道计量可以减少总旅行时间，减轻车流波动，通过平滑交通交织过程减少追尾碰撞等。除了传统的控制算法如ALINEA，深度强化学习(DRL)算法已经被引入来构建更精细的控制。然而，仍然存在两个挑战阻碍了DRL在现实世界中的应用：(1)一些算法的假设在现实世界中很难匹配；(2)丰富的输入状态可能使模型容易受到攻击和数据噪声的影响。为了研究这些问题，本研究提出了一种只使用环路检测器信息作为输入的深度Q学习算法。然后，设计了一组虚假数据注入攻击和随机噪声攻击来研究模型的鲁棒性。该模型的主要优点是可以应用于...

    Ramp metering is the act of controlling on-going vehicles to the highway mainlines. Decades of practices of ramp metering have proved that ramp metering can decrease total travel time, mitigate shockwaves, decrease rear-end collisions by smoothing the traffic interweaving process, etc. Besides traditional control algorithm like ALINEA, Deep Reinforcement Learning (DRL) algorithms have been introduced to build a finer control. However, two remaining challenges still hinder DRL from being implemented in the real world: (1) some assumptions of algorithms are hard to be matched in the real world; (2) the rich input states may make the model vulnerable to attacks and data noises. To investigate these issues, we propose a Deep Q-Learning algorithm using only loop detectors information as inputs in this study. Then, a set of False Data Injection attacks and random noise attack are designed to investigate the robustness of the model. The major benefit of the model is that it can be applied to 
    
[^126]: 多臂赌博机中的高效沟通合作后悔最小化研究

    Communication-Efficient Collaborative Regret Minimization in Multi-Armed Bandits. (arXiv:2301.11442v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11442](http://arxiv.org/abs/2301.11442)

    本文研究了多智能体多臂赌博机中并行性和沟通开销之间的折中问题，并提出了一组代理之间通信轮次和合作学习过程后悔之间的权衡关系。

    

    本文研究了多智能体多臂赌博机中并行性和沟通开销之间的折中问题。为了在多臂赌博机中实现后悔最小化，我们提出了一组代理之间通信轮次和合作学习过程后悔之间的权衡关系。

    In this paper, we study the collaborative learning model, which concerns the tradeoff between parallelism and communication overhead in multi-agent multi-armed bandits. For regret minimization in multi-armed bandits, we present the first set of tradeoffs between the number of rounds of communication among the agents and the regret of the collaborative learning process.
    
[^127]: 切片Transformer和自监督学习用于3D点云地图中的六自由度定位

    Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps. (arXiv:2301.08957v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.08957](http://arxiv.org/abs/2301.08957)

    这篇论文介绍了一种新的自监督学习方法，首次使用Transformer进行室外定位的任务。作者提出了切片Transformer模型，并利用轴向特性重新组织了激光雷达扫描的切片。此外，作者还引入了Perth-WA数据集，并在多个数据集上进行了实验评估，证明了方法的有效性。

    

    精确定位对于自动驾驶汽车至关重要。我们提出了一种自监督学习方法，首次采用了Transformer进行使用激光雷达数据进行室外定位的任务。我们提出了一个预训练任务，重新组织了$360^\circ$激光雷达扫描的切片，利用其轴向特性。我们的模型称为切片Transformer，在处理切片时使用多头注意力。据我们所知，这是首次利用多头注意力进行室外点云处理。我们还引入了Perth-WA数据集，该数据集提供了西澳大利亚珀斯市的大规模激光雷达地图，涵盖了约4平方公里的区域。Perth-WA提供了定位标注。我们在Perth-WA和Appollo-SouthBay数据集上对所提出的定位方法进行了全面评估。我们还证明了我们的自监督学习方法在常见的下游任务，如ModelNet40和...方面的有效性。

    Precise localization is critical for autonomous vehicles. We present a self-supervised learning method that employs Transformers for the first time for the task of outdoor localization using LiDAR data. We propose a pre-text task that reorganizes the slices of a $360^\circ$ LiDAR scan to leverage its axial properties. Our model, called Slice Transformer, employs multi-head attention while systematically processing the slices. To the best of our knowledge, this is the first instance of leveraging multi-head attention for outdoor point clouds. We additionally introduce the Perth-WA dataset, which provides a large-scale LiDAR map of Perth city in Western Australia, covering $\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The proposed localization method is thoroughly evaluated on Perth-WA and Appollo-SouthBay datasets. We also establish the efficacy of our self-supervised learning approach for the common downstream task of object classification using ModelNet40 and
    
[^128]: 谐波（量子）神经网络

    Harmonic (Quantum) Neural Networks. (arXiv:2212.07462v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07462](http://arxiv.org/abs/2212.07462)

    本文介绍了一种在神经网络中表示谐波函数的有效方法，并将这一方法扩展到了量子神经网络中，通过对比基准测试发现其性能优越。

    

    谐波函数在自然界中非常常见，在麦克斯韦方程、纳维-斯托克斯方程、热传导方程和波动方程的极限情况中都出现。因此，谐波函数有许多应用，从工业过程优化到机器人路径规划和随机行走的首次退出时间的计算。尽管谐波函数非常普遍和相关，但在机器学习环境中很少尝试将其归纳偏好引入其中。在这项工作中，我们展示了在神经网络中有效表示谐波函数的方法，并将这些结果扩展到量子神经网络中以展示我们方法的普适性。我们将我们的方法与（量子）物理信息神经网络进行了基准测试，结果显示我们的方法表现较好。

    Harmonic functions are abundant in nature, appearing in limiting cases of Maxwell's, Navier-Stokes equations, the heat and the wave equation. Consequently, there are many applications of harmonic functions from industrial process optimisation to robotic path planning and the calculation of first exit times of random walks. Despite their ubiquity and relevance, there have been few attempts to incorporate inductive biases towards harmonic functions in machine learning contexts. In this work, we demonstrate effective means of representing harmonic functions in neural networks and extend such results also to quantum neural networks to demonstrate the generality of our approach. We benchmark our approaches against (quantum) physics-informed neural networks, where we show favourable performance.
    
[^129]: 具有可微化节点选择的多视角图卷积网络

    Multi-view Graph Convolutional Networks with Differentiable Node Selection. (arXiv:2212.05124v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05124](http://arxiv.org/abs/2212.05124)

    本文提出了一种具有可微化节点选择的多视角图卷积网络，可有效地利用多视图数据的互补和一致信息，提取多个对象之间的潜在信息。

    

    包含互补和一致信息的多视角数据通过利用多视图特征的完整整合来促进表示学习。由于现实世界中大多数对象通常具有潜在的连接，将多视点数据组织为异构图对于提取不同对象之间的潜在信息非常有益。由于具有收集附近节点信息的强大能力，本文将图卷积网络（GCN）应用于处理源自多视图数据的异构图数据，这在GCN领域仍未得到充分探索。为了改善网络拓扑的质量并减轻图融合产生的噪声干扰，一些方法在图卷积过程之前进行了排序操作。这些基于GCN的方法通常对每个顶点排序和选择最可信任的邻近节点，例如根据预定义的置信值选择前k个节点。

    Multi-view data containing complementary and consensus information can facilitate representation learning by exploiting the intact integration of multi-view features. Because most objects in real world often have underlying connections, organizing multi-view data as heterogeneous graphs is beneficial to extracting latent information among different objects. Due to the powerful capability to gather information of neighborhood nodes, in this paper, we apply Graph Convolutional Network (GCN) to cope with heterogeneous-graph data originating from multi-view data, which is still under-explored in the field of GCN. In order to improve the quality of network topology and alleviate the interference of noises yielded by graph fusion, some methods undertake sorting operations before the graph convolution procedure. These GCN-based methods generally sort and select the most confident neighborhood nodes for each vertex, such as picking the top-k nodes according to pre-defined confidence values. No
    
[^130]: 一种简单高效的去中心化非凸极小化问题随机算法

    A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. (arXiv:2212.02387v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02387](http://arxiv.org/abs/2212.02387)

    本文提出了一种称为去中心化递归梯度上升法（DREAM）的简单高效算法，用于解决去中心化非凸极小化问题，并实现了寻找原函数的 $\epsilon$-稳定点的最佳理论保证。

    

    本文研究了去中心化非凸极小化问题的随机优化。我们提出了一种简单高效的算法，称为去中心化递归梯度上升法（\texttt{DREAM}），它实现了寻找原函数的$\epsilon$-稳定点的最佳已知理论保证。在在线设置下，所提出的方法需要$\mathcal{O}(\kappa^3\epsilon^{-3})$随机一阶预言机（SFO）调用以及$\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$通信轮次来找到$\epsilon$-稳定点，其中$\kappa$是条件数，$\lambda_2(W)$是八卦矩阵$W$的次大特征值。对于完全由$N$个分量函数组成的离线设置，所提出的方法需要$\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO 调用和与在线设置相同的通信复杂度。

    This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\mathcal{O}(\kappa^3\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$ communication rounds to find an $\epsilon$-stationary point, where $\kappa$ is the condition number and $\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO calls and the same communication complexity as the online setting.
    
[^131]: 基于脉冲神经网络的医疗数据分析综述

    Review of medical data analysis based on spiking neural networks. (arXiv:2212.02234v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2212.02234](http://arxiv.org/abs/2212.02234)

    本文综述了基于脉冲神经网络的医疗数据分析，使用医疗数据进行信号分类和疾病诊断。与传统网络相比，脉冲神经网络具有一些优势和劣势。

    

    医疗数据主要包括各种类型的生物医学信号和医学影像，可以被专业医生用来判断患者的健康状况。然而，医疗数据的解读需要大量的人力成本，可能存在误判，因此许多学者使用神经网络和深度学习来对医疗数据进行分类和研究，可以提高医生的效率和准确性，实现早期诊断等。因此，它具有广阔的应用前景。然而，传统神经网络存在能耗高和计算速度慢等缺点。本文介绍了基于第三代神经网络脉冲神经元网络的信号分类和疾病诊断的最新研究，使用医疗数据包括脑电图信号、心电图信号、肌电图信号和核磁共振图像。脉冲神经网络相对于传统网络的优缺点进行了比较。

    Medical data mainly includes various types of biomedical signals and medical images, which can be used by professional doctors to make judgments on patients' health conditions. However, the interpretation of medical data requires a lot of human cost and there may be misjudgments, so many scholars use neural networks and deep learning to classify and study medical data, which can improve the efficiency and accuracy of doctors and detect diseases early for early diagnosis, etc. Therefore, it has a wide range of application prospects. However, traditional neural networks have disadvantages such as high energy consumption and high latency (slow computation speed). This paper presents recent research on signal classification and disease diagnosis based on a third-generation neural network, the spiking neuron network, using medical data including EEG signals, ECG signals, EMG signals and MRI images. The advantages and disadvantages of pulsed neural networks compared with traditional networks
    
[^132]: Euler特征曲线和轮廓：一种适用于大数据问题的稳定形状不变量

    Euler Characteristic Curves and Profiles: a stable shape invariant for big data problems. (arXiv:2212.01666v2 [math.AT] UPDATED)

    [http://arxiv.org/abs/2212.01666](http://arxiv.org/abs/2212.01666)

    Euler特征曲线和轮廓是一种稳定的形状不变量，用于大数据问题。与持久同调相比，它们具有分布式计算、多滤波推广和适用于大数据的优势，同时具有稳定性。

    

    拓扑数据分析工具提供了稳定的摘要信息，概括了所考虑数据的形状。持久同调，作为最常见和研究最透彻的数据摘要方法，存在一些限制：计算难以分布、难以推广到多滤波和对大数据集来说计算量过大。本文研究了Euler特征曲线和多参数滤波下的Euler特征轮廓的概念。尽管在一维上是一个较弱的不变量，我们展示了Euler特征的方法不具备持久同调的一些劣势；我们提出了有效的分布式计算算法，以及它们在多滤波下的泛化和在大数据问题中的实际适用性。此外，我们展示了Euler曲线和轮廓具有某种稳定性，使它们成为数据分析中强大的工具。最后，为了展示它们的实际应用...

    Tools of Topological Data Analysis provide stable summaries encapsulating the shape of the considered data. Persistent homology, the most standard and well studied data summary, suffers a number of limitations; its computations are hard to distribute, it is hard to generalize to multifiltrations and is computationally prohibitive for big data-sets. In this paper we study the concept of Euler Characteristics Curves, for one parameter filtrations and Euler Characteristic Profiles, for multi-parameter filtrations. While being a weaker invariant in one dimension, we show that Euler Characteristic based approaches do not possess some handicaps of persistent homology; we show efficient algorithms to compute them in a distributed way, their generalization to multifiltrations and practical applicability for big data problems. In addition we show that the Euler Curves and Profiles enjoys certain type of stability which makes them robust tool in data analysis. Lastly, to show their practical app
    
[^133]: 在线估计使用傅里叶特征的Koopman算符

    Online Estimation of the Koopman Operator Using Fourier Features. (arXiv:2212.01503v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.01503](http://arxiv.org/abs/2212.01503)

    本文提出了一种在线数据估计Koopman算符的优化方案，通过联合学习可观测量和Koopman算符，能够重构演变并表示复杂动力系统的全局特征。

    

    转移算子提供非线性动力系统的线性表示和全局且物理意义明确的特征。发现转移算子，例如Koopman算符，需要精心构建的可观测量词典，作用于动力系统的状态上。这种方法是临时性的，需要使用整个数据集进行评估。在本文中，我们提供了一种优化方案，可以使用在线数据联合学习可观测量和Koopman算符。我们的结果表明，我们能够重构演变并表示复杂动力系统的全局特征。

    Transfer operators offer linear representations and global, physically meaningful features of nonlinear dynamical systems. Discovering transfer operators, such as the Koopman operator, require careful crafted dictionaries of observables, acting on states of the dynamical system. This is ad hoc and requires the full dataset for evaluation. In this paper, we offer an optimization scheme to allow joint learning of the observables and Koopman operator with online data. Our results show we are able to reconstruct the evolution and represent the global features of complex dynamical systems.
    
[^134]: 无模型强化学习与决策估计系数

    Model-Free Reinforcement Learning with the Decision-Estimation Coefficient. (arXiv:2211.14250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14250](http://arxiv.org/abs/2211.14250)

    本文介绍了决策估计系数和Estimation-to-Decisions元算法，在无模型强化学习中结合乐观估计，提供了更宽松的估计误差概念的保证。

    

    本文考虑交互式决策问题，涵盖结构化赌博机和具有通用函数逼近的强化学习。最近，Foster等人（2021）引入了决策估计系数，这是一种统计复杂度的度量，其为交互式决策制定了一个最优遗憾的下界，并引入了Estimation-to-Decisions元算法，该算法以相同数量为上界。Estimation-to-Decisions是一种约简，将（监督）在线估计的算法提升为决策制定的算法。在本文中，我们展示了通过将Estimation-to-Decisions与Zhang（2022）引入的一种专门的乐观估计形式相结合，可以得到比Foster等人（2021）更为宽松的估计误差概念的保证。我们利用这种方法导出了与值函数逼近相结合的无模型强化学习的遗憾界，并给出了策略。

    We consider the problem of interactive decision making, encompassing structured bandits and reinforcement learning with general function approximation. Recently, Foster et al. (2021) introduced the Decision-Estimation Coefficient, a measure of statistical complexity that lower bounds the optimal regret for interactive decision making, as well as a meta-algorithm, Estimation-to-Decisions, which achieves upper bounds in terms of the same quantity. Estimation-to-Decisions is a reduction, which lifts algorithms for (supervised) online estimation into algorithms for decision making. In this paper, we show that by combining Estimation-to-Decisions with a specialized form of optimistic estimation introduced by Zhang (2022), it is possible to obtain guarantees that improve upon those of Foster et al. (2021) by accommodating more lenient notions of estimation error. We use this approach to derive regret bounds for model-free reinforcement learning with value function approximation, and give str
    
[^135]: 过度参数化的随机特征回归与几乎正交数据

    Overparameterized random feature regression with nearly orthogonal data. (arXiv:2211.06077v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.06077](http://arxiv.org/abs/2211.06077)

    本文研究了过度参数化的随机特征回归(RFRR)在几乎正交数据上的行为，证明了在第一层宽度大于样本大小的情况下，RFRR的训练误差、交叉验证和泛化误差可以高概率集中在核岭回归(KRR)的相应值周围，同时给出了用多项式核近似KRR性能的方法。

    

    本文研究了随机特征岭回归 (RFRR)，其中包括一个具有随机高斯初始化的两层神经网络。我们研究了在过度参数化情况下，第一层的宽度远大于样本大小的条件下，采用几乎正交确定性单位长度输入数据向量进行 RFRR 的非渐近行为。我们的分析显示了 RFRR 训练误差、交叉验证和泛化误差的非渐近集中结果，在核岭回归 (KRR) 的相应值周围有高概率出现。该 KRR 是由非线性随机特征映射生成的期望核导出的。然后，我们通过激活函数的 Hermite 多项式展开获得的多项式核矩阵来近似 KRR 的性能，其次数仅取决于不同数据点之间的正交性。这个多项式核确定了 RFRR 和 KRR 的渐近行为。

    We investigate the properties of random feature ridge regression (RFRR) given by a two-layer neural network with random Gaussian initialization. We study the non-asymptotic behaviors of the RFRR with nearly orthogonal deterministic unit-length input data vectors in the overparameterized regime, where the width of the first layer is much larger than the sample size. Our analysis shows high-probability non-asymptotic concentration results for the training errors, cross-validations, and generalization errors of RFRR centered around their respective values for a kernel ridge regression (KRR). This KRR is derived from an expected kernel generated by a nonlinear random feature map. We then approximate the performance of the KRR by a polynomial kernel matrix obtained from the Hermite polynomial expansion of the activation function, whose degree only depends on the orthogonality among different data points. This polynomial kernel determines the asymptotic behavior of the RFRR and the KRR. Our 
    
[^136]: StyleNAT：给每个头部一个新的视角

    StyleNAT: Giving Each Head a New Perspective. (arXiv:2211.05770v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05770](http://arxiv.org/abs/2211.05770)

    StyleNAT是一个新的基于transformer的图像生成框架，通过使用邻域注意力（NA）来捕捉局部和全局信息，能够高效灵活地适应不同的数据集，并在FFHQ-256上取得了新的最佳结果。

    

    图像生成一直是一个既期望又具有挑战性的任务，以高效的方式执行生成任务同样困难。通常，研究人员试图创建一个“一刀切”的生成器，在参数空间中，即使是截然不同的数据集，也有很少的差异。在这里，我们提出了一种新的基于transformer的框架，称为StyleNAT，旨在实现高质量的图像生成，并具有卓越的效率和灵活性。在我们的模型核心是一个精心设计的框架，它将注意力头部划分为捕捉局部和全局信息的方式，这是通过使用邻域注意力（NA）实现的。由于不同的头部能够关注不同的感受野，模型能够更好地结合这些信息，并以高度灵活的方式适应手头的数据。StyleNAT在FFHQ-256上获得了新的SOTA FID得分2.046 ，击败了以卷积模型（如StyleGAN-XL）和transformer模型（如HIT）为基础的先前方法。

    Image generation has been a long sought-after but challenging task, and performing the generation task in an efficient manner is similarly difficult. Often researchers attempt to create a "one size fits all" generator, where there are few differences in the parameter space for drastically different datasets. Herein, we present a new transformer-based framework, dubbed StyleNAT, targeting high-quality image generation with superior efficiency and flexibility. At the core of our model, is a carefully designed framework that partitions attention heads to capture local and global information, which is achieved through using Neighborhood Attention (NA). With different heads able to pay attention to varying receptive fields, the model is able to better combine this information, and adapt, in a highly flexible manner, to the data at hand. StyleNAT attains a new SOTA FID score on FFHQ-256 with 2.046, beating prior arts with convolutional models such as StyleGAN-XL and transformers such as HIT 
    
[^137]: 学习玻璃液体表示的旋转等变图神经网络

    Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids Representations. (arXiv:2211.03226v2 [cond-mat.soft] UPDATED)

    [http://arxiv.org/abs/2211.03226](http://arxiv.org/abs/2211.03226)

    本文提出了一种旋转等变图神经网络（GNN），通过约束保持旋转平移等变性的方式，学习玻璃液体静态结构的稳健表示。这种约束显著提高了预测能力和泛化能力，同时减少了参数数量，并且提高了解释性。通过迁移学习实验证明了网络的有效性。

    

    在玻璃液体研究领域，使用机器学习（ML）对粒子的静态结构进行建模是一个热门话题。最先进的方法是使用图神经网络（GNN），它具有强大的表达能力，但是模型参数多且缺乏可解释性。受机器学习群等变表示领域的最新进展的启发，我们构建了一个GNN，通过约束其保持旋转平移（SE（3））等变性，学习玻璃静态结构的稳健表示。我们发现，这种约束不仅显著提高了预测能力，还提高了对未见过温度的泛化能力，同时可以减少参数数量。此外，解释性也得到了改善，因为我们可以将基本卷积层的作用与众所周知的旋转不变专家特征联系起来。通过迁移学习实验，我们证明了我们的网络学习了一个稳健的表示。

    Within the glassy liquids community, the use of Machine Learning (ML) to model particles' static structure is currently a hot topic. The state of the art consists in Graph Neural Networks (GNNs), which have a great expressive power but are heavy models with numerous parameters and lack interpretability. Inspired by recent advances in the field of Machine Learning group-equivariant representations, we build a GNN that learns a robust representation of the glass' static structure by constraining it to preserve the roto-translation (SE(3)) equivariance. We show that this constraint not only significantly improves the predictive power but also improves the ability to generalize to unseen temperatures while allowing to reduce the number of parameters. Furthermore, interpretability is improved, as we can relate the action of our basic convolution layer to well-known rotation-invariant expert features. Through transfer-learning experiments we demonstrate that our network learns a robust repre
    
[^138]: 代理-控制器表示：具有丰富外部信息的原则性离线强化学习

    Agent-Controller Representations: Principled Offline RL with Rich Exogenous Information. (arXiv:2211.00164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00164](http://arxiv.org/abs/2211.00164)

    本文针对具有丰富外部信息的原则性离线强化学习进行研究，并提出了新的离线强化学习基准。研究发现，当噪声是复杂且与时间相关的过程时，现有的表示学习技术可能无法成功应用于这类数据集。

    

    在丰富的像素视觉观测空间中，从离线数据中学习控制代理对于强化学习在实际应用中至关重要。这种设置中的一个主要挑战是输入信息中存在难以建模和控制代理相关的信息。理论强化学习领域已经通过外部信息的观点来解决这个问题，即观测中包含的与控制无关的信息。例如，一个在繁忙街道上导航的机器人需要忽略与控制无关的信息，如背景中的其他人行走、物体的纹理或天空中的鸟类。本文针对具有视觉细节的外部信息的设置，并引入了新的离线强化学习基准，以研究这个问题。我们发现当噪声是复杂且与时间相关的过程时，当代的表示学习技术可能在数据集上失败，而这种噪声在实际中普遍存在。

    Learning to control an agent from data collected offline in a rich pixel-based visual observation space is vital for real-world applications of reinforcement learning (RL). A major challenge in this setting is the presence of input information that is hard to model and irrelevant to controlling the agent. This problem has been approached by the theoretical RL community through the lens of exogenous information, i.e, any control-irrelevant information contained in observations. For example, a robot navigating in busy streets needs to ignore irrelevant information, such as other people walking in the background, textures of objects, or birds in the sky. In this paper, we focus on the setting with visually detailed exogenous information, and introduce new offline RL benchmarks offering the ability to study this problem. We find that contemporary representation learning techniques can fail on datasets where the noise is a complex and time dependent process, which is prevalent in practical 
    
[^139]: 带有HaarPooling消息传递的图网络喷注标记算法

    A jet tagging algorithm of graph network with HaarPooling message passing. (arXiv:2210.13869v4 [hep-ex] UPDATED)

    [http://arxiv.org/abs/2210.13869](http://arxiv.org/abs/2210.13869)

    本论文介绍了一种将图神经网络与HaarPooling操作相结合的方法，称为HMPNet，用于高能物理中的夸克胶子标记。实验结果表明，适当选择HaarPooling的信息可以提高夸克胶子标记的准确性。

    

    最近，图神经网络（GNNs）的方法已经被应用于解决高能物理（HEP）中的问题，并且在使用图表示的喷注事件的夸克胶子标记中展现出了巨大潜力。在本文中，我们引入了一种GNNs与HaarPooling操作相结合的方法来分析事件，称之为HaarPooling消息传递神经网络（HMPNet）。在HMPNet中，HaarPooling不仅提取了图的特征，还嵌入了通过k-means对不同粒子特征进行聚类得到的附加信息。我们从五个不同的特征构建了HaarPooling：绝对能量$\log E$，横向动量$\log p_T$，相对坐标$(\Delta\eta,\Delta\phi)$，混合特征$(\log E, \log p_T)$和$(\log E, \log p_T, \Delta\eta,\Delta\phi)$。结果表明，适当选择HaarPooling的信息可以提高夸克胶子标记的准确性，将$\log P_T$的额外信息添加到HMPNet中优于其他所有方法。

    Recently methods of graph neural networks (GNNs) have been applied to solving the problems in high energy physics (HEP) and have shown its great potential for quark-gluon tagging with graph representation of jet events. In this paper, we introduce an approach of GNNs combined with a HaarPooling operation to analyze the events, called HaarPooling Message Passing neural network (HMPNet). In HMPNet, HaarPooling not only extracts the features of graph, but embeds additional information obtained by clustering of k-means of different particle features. We construct Haarpooling from five different features: absolute energy $\log E$, transverse momentum $\log p_T$, relative coordinates $(\Delta\eta,\Delta\phi)$, the mixed ones $(\log E, \log p_T)$ and $(\log E, \log p_T, \Delta\eta,\Delta\phi)$. The results show that an appropriate selection of information for HaarPooling enhances the accuracy of quark-gluon tagging, as adding extra information of $\log P_T$ to the HMPNet outperforms all the o
    
[^140]: 一种混合联邦学习的原始-对偶算法

    A Primal-Dual Algorithm for Hybrid Federated Learning. (arXiv:2210.08106v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08106](http://arxiv.org/abs/2210.08106)

    该论文提出了一种基于Fenchel对偶性的快速、稳健的混合联邦学习算法。实验证明了该算法相对于传统的FedAvg方法的性能改进，并提供了隐私保护措施。

    

    在实际情况中，混合联邦学习很重要，但对于仅持有部分特征和样本的客户端，很少有方法存在。我们提供了一种基于Fenchel对偶性的快速、稳健的混合联邦学习算法。我们证明了算法在各种实际情况下收敛于与在中心训练模型相同的解决方案。此外，我们还提供了实验结果，证明了该算法相对于联邦学习中常用的方法FedAvg的性能改进。我们还提供了隐私考虑和保护客户数据的必要步骤。

    Very few methods for hybrid federated learning, where clients only hold subsets of both features and samples, exist. Yet, this scenario is very important in practical settings. We provide a fast, robust algorithm for hybrid federated learning that hinges on Fenchel Duality. We prove the convergence of the algorithm to the same solution as if the model was trained centrally in a variety of practical regimes. Furthermore, we provide experimental results that demonstrate the performance improvements of the algorithm over a commonly used method in federated learning, FedAvg. We also provide privacy considerations and necessary steps to protect client data.
    
[^141]: 一种尴尬简单的自监督学习后门攻击

    An Embarrassingly Simple Backdoor Attack on Self-supervised Learning. (arXiv:2210.07346v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.07346](http://arxiv.org/abs/2210.07346)

    这项研究探索了自监督学习后门攻击的问题，提出了一种尴尬简单但高效的攻击方法，通过污染训练数据并插入触发器，使得在推理时任何带有触发器的输入都能以高概率被错误分类到对手指定的类别。这一发现表明，自监督学习和监督学习在对抗性攻击方面存在差异。

    

    自监督学习（SSL）作为机器学习中的新范式，能够在不依赖标签的情况下学习复杂数据的高质量表示。研究发现，除了消除对有标签数据的依赖外，SSL还提高了对抗性鲁棒性，因为缺乏标签使得对手更难以操纵模型预测。然而，这种鲁棒性优势在其他类型的攻击中是否具有普适性仍然是一个未解之谜。我们在后门攻击的背景下探索了这个问题。具体地，我们设计并评估了CTRL，一种尴尬简单但非常有效的自监督后门攻击。通过仅污染微小比例（<= 1%）的训练数据，CTRL在推理时以高概率（>= 99%）将任何携带触发器的输入错误分类为对手指定的类别。我们的研究结果表明，SSL和监督学习在对抗性攻击中存在一定的差异。

    As a new paradigm in machine learning, self-supervised learning (SSL) is capable of learning high-quality representations of complex data without relying on labels. In addition to eliminating the need for labeled data, research has found that SSL improves the adversarial robustness over supervised learning since lacking labels makes it more challenging for adversaries to manipulate model predictions. However, the extent to which this robustness superiority generalizes to other types of attacks remains an open question.  We explore this question in the context of backdoor attacks. Specifically, we design and evaluate CTRL, an embarrassingly simple yet highly effective self-supervised backdoor attack. By only polluting a tiny fraction of training data (<= 1%) with indistinguishable poisoning samples, CTRL causes any trigger-embedded input to be misclassified to the adversary's designated class with a high probability (>= 99%) at inference time. Our findings suggest that SSL and supervise
    
[^142]: 大型语言模型可以实现策略迭代

    Large Language Models can Implement Policy Iteration. (arXiv:2210.03821v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03821](http://arxiv.org/abs/2210.03821)

    本论文提出一种名为ICPI的算法，利用基础模型在上下文中执行强化学习任务，无需专家示范或梯度。算法采用策略迭代方法，不仅避免了示范收集的繁琐工作，还解决了梯度方法的运行速度问题。

    

    本文介绍了一种称为“上下文策略迭代”的算法，它利用基础模型在上下文中执行强化学习任务。虽然将基础模型应用于强化学习已经受到了重视，但大多数方法要么依赖于专家示范的策划（通过手动设计或任务特定的预训练），要么通过梯度方法（微调或适配层训练）来适应感兴趣的任务。这些技术都有一些缺点。收集示范是费时费力的，依赖示范的算法并不能超越示范中的专家。而所有的梯度技术天生都很慢，丧失了一开始使上下文学习具有吸引力的“少样本”质量。本文提出了一种名为ICPI的算法，它能够在没有专家示范或梯度的情况下学习执行强化学习任务。我们提出了一种策略迭代方法，其中包括了提示文本。

    This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the "few-shot" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt conten
    
[^143]: 自带长尾数据的标签噪声学习

    Label-Noise Learning with Intrinsically Long-Tailed Data. (arXiv:2208.09833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09833](http://arxiv.org/abs/2208.09833)

    本文针对自带长尾数据的标签噪声学习问题，提出了一个学习框架并设计了双维样本选择算法 TABASCO，有效地将干净样本与噪声样本分开，特别适用于长尾类别。

    

    标签噪声是导致深度学习模型泛化能力差的关键因素之一。现有的标签噪声学习方法通常假设训练数据的真实类别是平衡的。然而，现实世界的数据往往是不平衡的，导致观察到的和内在类别分布之间存在不一致的标签噪声。在这种情况下，很难在具有未知内在类别分布的长尾类上区分干净样本和噪声样本。本文提出了一种用于自带长尾数据的标签噪声学习的学习框架。具体而言，我们提出了两阶段的双维样本选择（TABASCO）来更好地将干净样本与噪声样本分开，尤其是对于尾部类别。TABASCO包括两个新的分离度量，相互补充，弥补了在样本分离中使用单个度量的局限性。对我们提出的具有真实数据的基准进行了大量实验证明。

    Label noise is one of the key factors that lead to the poor generalization of deep learning models. Existing label-noise learning methods usually assume that the ground-truth classes of the training data are balanced. However, the real-world data is often imbalanced, leading to the inconsistency between observed and intrinsic class distribution with label noises. In this case, it is hard to distinguish clean samples from noisy samples on the intrinsic tail classes with the unknown intrinsic class distribution. In this paper, we propose a learning framework for label-noise learning with intrinsically long-tailed data. Specifically, we propose two-stage bi-dimensional sample selection (TABASCO) to better separate clean samples from noisy samples, especially for the tail classes. TABASCO consists of two new separation metrics that complement each other to compensate for the limitation of using a single metric in sample separation. Extensive experiments on benchmarks we proposed with real-
    
[^144]: 物理约束深度学习用于气候降尺度

    Physics-Constrained Deep Learning for Climate Downscaling. (arXiv:2208.05424v6 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2208.05424](http://arxiv.org/abs/2208.05424)

    本文提出了一种物理约束深度学习降尺度模型的方法，以保证模型在预测物理变量时满足守恒定律，并提高其性能。

    This paper proposes a method for physics-constrained deep learning downscaling models to ensure that the models satisfy conservation laws when predicting physical variables, while improving their performance according to traditional metrics.

    可靠的高分辨率气候和天气数据的可用性对于指导气候适应和减缓的长期决策以及指导对极端事件的快速响应至关重要。预测模型受计算成本限制，因此通常生成粗分辨率预测。统计降尺度，包括深度学习的超分辨率方法，可以提供一种有效的方法来上采样低分辨率数据。然而，尽管在某些情况下取得了视觉上令人信服的结果，但这些模型在预测物理变量时经常违反守恒定律。为了保持物理量的守恒，我们开发了一种方法，保证深度学习降尺度模型满足物理约束条件，同时根据传统指标提高其性能。我们比较了不同的约束方法，并展示了它们在不同的神经架构以及各种气候和天气数据上的适用性。

    The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and, therefore, often generate coarse-resolution predictions. Statistical downscaling, including super-resolution methods from deep learning, can provide an efficient method of upsampling low-resolution data. However, despite achieving visually compelling results in some cases, such models frequently violate conservation laws when predicting physical variables. In order to conserve physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep learning downscaling model while also improving their performance according to traditional metrics. We compare different constraining approaches and demonstrate their applicability across different neural architectures as well as a variety of climate and weather
    
[^145]: Transformers可以在上下文中学习什么？一个简单函数类的案例研究。

    What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. (arXiv:2208.01066v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.01066](http://arxiv.org/abs/2208.01066)

    本研究通过考虑在上下文中学习线性函数的问题，证明了标准的Transformers模型可以从头训练，在推断时实现线性函数的上下文学习能力。

    

    上下文学习是指模型能够依赖于包含上下文示例（与某个任务对应的输入-输出对）和新的查询输入的提示序列，并生成相应的输出。关键是，在推断时，上下文学习仅发生在模型参数未更新的情况下。虽然像GPT-3这样的大型语言模型表现出了一定的上下文学习能力，但目前尚不清楚成功的任务与训练数据中的什么内容之间的关系。为了进一步理解上下文学习，我们考虑了一个明确定义的问题，即训练一个模型以在上下文中学习函数类（例如线性函数）：也就是说，给定从该类中导出的数据，我们能否训练一个模型来在上下文中学习“大多数”函数？我们通过实验证明，标准的Transformers可以从头开始训练，以在上下文中学习线性函数。

    In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions 
    
[^146]: 机器学习和计算机视觉技术在蜜蜂监测应用中的应用

    Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2208.00085](http://arxiv.org/abs/2208.00085)

    本文介绍了机器学习和计算机视觉在蜜蜂监测中的最新应用，展示了自动化蜜蜂计数算法的潜力，并希望能够激发其他科学家的灵感和兴趣。

    

    机器学习和计算机视觉是快速发展的领域，已经证明能够解决非常复杂的任务。它们可以用于监测蜜蜂群体并检查其健康状况，从而在情况变得严重之前，识别出潜在危险状态，或者更好地计划定期蜜蜂群体检查，从而节省重要的成本。本文概述了用于蜜蜂监测的最先进的计算机视觉和机器学习应用，并以自动化蜜蜂计数算法为例展示了这些方法的潜力。本文面向兽医学和蜜蜂学专业人员和专家，旨在向他们介绍机器学习的可能性，因此每个应用类别都以简要的理论介绍和与其基本方法相关的动机开篇。我们希望这篇论文能激发其他科学家的灵感...

    Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
    
[^147]: SmartGD: 一种基于GAN的图形绘制框架，用于实现多样化的美学目标。

    SmartGD: A GAN-Based Graph Drawing Framework for Diverse Aesthetic Goals. (arXiv:2206.06434v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06434](http://arxiv.org/abs/2206.06434)

    SmartGD提出了一种基于GAN的图形绘制框架，可以优化不同的定量美学目标，解决了现有方法只关注单一美学方面的问题。

    

    尽管已经有大量的研究在图形绘制方面进行了，但是许多现有的方法只关注优化图形布局的某一个美学方面，这可能导致次优的结果。已有的一些方法试图开发一种灵活的解决方案，以优化由不同美学标准测量的不同美学方面。此外，由于深度学习技术的重大进展，最近提出了几种基于深度学习的布局方法。这些方法展示了深度学习方法在图形绘制中的优势。然而，这些现有方法都无法直接应用于优化非可微标准而不需要特殊处理。在这项工作中，我们提出了一种新颖的基于生成对抗网络（GAN）的深度学习框架SmartGD，该框架可以优化不同的定量美学目标，无论它们是否可微。

    While a multitude of studies have been conducted on graph drawing, many existing methods only focus on optimizing a single aesthetic aspect of graph layouts, which can lead to sub-optimal results. There are a few existing methods that have attempted to develop a flexible solution for optimizing different aesthetic aspects measured by different aesthetic criteria. Furthermore, thanks to the significant advance in deep learning techniques, several deep learning-based layout methods were proposed recently. These methods have demonstrated the advantages of deep learning approaches for graph drawing. However, none of these existing methods can be directly applied to optimizing non-differentiable criteria without special accommodation. In this work, we propose a novel Generative Adversarial Network (GAN) based deep learning framework for graph drawing, called SmartGD, which can optimize different quantitative aesthetic goals, regardless of their differentiability. To demonstrate the effectiv
    
[^148]: 深度学习模型的功能性神经编码分析

    Analysis of functional neural codes of deep learning models. (arXiv:2205.10952v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10952](http://arxiv.org/abs/2205.10952)

    本研究使用自组织映射(SOM)分析了深度学习模型中与决策相关的内部编码，发现浅层将特征压缩到紧凑空间中，而深层将特征空间扩展，并指出压缩特征可能导致对敌对扰动的脆弱性。

    

    深度神经网络(DNNs)作为深度学习(DL)的代理，需要大量的并行/顺序操作。这使得理解DNNs的操作变得困难，阻碍了适当的诊断。在没有对其内部过程有更好的了解之前，在高风险领域部署DNNs可能导致灾难性故障。因此，为了构建更可靠的DNNs/DL来解决高风险现实世界问题，我们必须深入了解DNNs决策背后的内部操作。在这里，我们使用自组织映射(SOM)分析与DNNs决策相关的DL模型的内部编码。我们的分析表明，靠近输入层的浅层将特征压缩到紧凑空间中，而靠近输出层的深层将特征空间扩展。我们还发现有证据表明，压缩特征可能导致DNNs对敌对扰动的脆弱性。

    Deep neural networks (DNNs), the agents of deep learning (DL), require a massive number of parallel/sequential operations. This makes it difficult to comprehend DNNs' operations and impedes proper diagnosis. Without better knowledge of their internal process, deploying DNNs in high-stakes domains can lead to catastrophic failures. Therefore, to build more reliable DNNs/DL to be deployed in high-stakes real-world problems, it is imperative that we gain insights into DNNs' internal operations underlying their decision-making. Here, we use the self-organizing map (SOM) to analyze DL models' internal codes associated with DNNs' decision-making. Our analyses suggest that shallow layers close to the input layer compress features into condensed space and that deep layers close to the output layer expand feature space. We also found evidence indicating that compressed features may underlie DNNs' vulnerabilities to adversarial perturbations.
    
[^149]: GUARD: 图形通用对抗防御

    GUARD: Graph Universal Adversarial Defense. (arXiv:2204.09803v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.09803](http://arxiv.org/abs/2204.09803)

    GUARD是一种新颖的图形通用对抗防御方法，旨在提高GCNs对针对性攻击的局部节点的鲁棒性，并在不降低整体性能的情况下进行防御。

    

    图形卷积网络（GCNs）已被证明对小的对抗性扰动具有脆弱性，这成为安全关键场景下应用的严重威胁并且极大限制了其应用。为了减轻这种威胁，已经投入大量研究工作来增加GCNs对抗性攻击的鲁棒性。然而，当前的防御方法通常被设计用于防止GCNs遭受非针对性的对抗性攻击并且着重于整体性能，这使得保护重要局部节点免受更强有力的针对性攻击变得具有挑战性。此外，现有研究往往在鲁棒性和性能之间进行权衡。这些限制凸显了开发一种能够防御针对性攻击的局部节点而不损害GCNs整体性能的有效且高效方法的必要性。在这项工作中，我们提出了一种简单而有效的方法，名为Graph Universal Adversarial Defense（GUARD）。

    Graph convolutional networks (GCNs) have been shown to be vulnerable to small adversarial perturbations, which becomes a severe threat and largely limits their applications in security-critical scenarios. To mitigate such a threat, considerable research efforts have been devoted to increasing the robustness of GCNs against adversarial attacks. However, current defense approaches are typically designed to prevent GCNs from untargeted adversarial attacks and focus on overall performance, making it challenging to protect important local nodes from more powerful targeted adversarial attacks. Additionally, a trade-off between robustness and performance is often made in existing research. Such limitations highlight the need for developing an effective and efficient approach that can defend local nodes against targeted attacks, without compromising the overall performance of GCNs. In this work, we present a simple yet effective method, named Graph Universal Adversarial Defense (GUARD). Unlike
    
[^150]: 离散时间线性系统和Schr\"odinger桥的最大熵最优密度控制

    Maximum entropy optimal density control of discrete-time linear systems and Schr\"odinger bridges. (arXiv:2204.05263v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2204.05263](http://arxiv.org/abs/2204.05263)

    本文研究了离散时间线性系统和Schr\"odinger桥的最大熵最优密度控制问题，通过引入特定时间点的高斯密度约束来直接控制系统状态的不确定性。

    

    我们考虑了确定性离散时间线性系统的熵正则化版本的最优密度控制。熵正则化或最大熵（MaxEnt）方法在最优控制中引起了广泛关注，特别是在强化学习中，由于其自然的探索策略等许多优点。尽管有这些优点，由正则化引起的高熵控制策略将概率不确定性引入系统，这严重限制了MaxEnt最优控制在安全关键系统中的适用性。为了解决这个问题，我们在指定的时间点对MaxEnt最优控制施加了一个Gaussian密度约束，以直接控制状态的不确定性。具体地，我们推导出了MaxEnt最优密度控制的显式形式。此外，我们还考虑了将密度约束替换为定点约束的情况。然后，我们将相关的状态过程描述为一个固定过程，这是对经典Schr\"odinger桥模型的一般化。

    We consider an entropy-regularized version of optimal density control of deterministic discrete-time linear systems. Entropy regularization, or a maximum entropy (MaxEnt) method for optimal control has attracted much attention especially in reinforcement learning due to its many advantages such as a natural exploration strategy. Despite the merits, high-entropy control policies induced by the regularization introduce probabilistic uncertainty into systems, which severely limits the applicability of MaxEnt optimal control to safety-critical systems. To remedy this situation, we impose a Gaussian density constraint at a specified time on the MaxEnt optimal control to directly control state uncertainty. Specifically, we derive the explicit form of the MaxEnt optimal density control. In addition, we also consider the case where density constraints are replaced by fixed point constraints. Then, we characterize the associated state process as a pinned process, which is a generalization of th
    
[^151]: 使用拓扑数据分析预测神经网络的泛化差距

    Predicting the generalization gap in neural networks using topological data analysis. (arXiv:2203.12330v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.12330](http://arxiv.org/abs/2203.12330)

    本文利用拓扑数据分析的方法研究神经网络的泛化差距，通过计算加权图的同调持久图，并比较不同数值汇总的有用性，可以准确预测和部分解释泛化差距，而无需测试集。实验结果表明，在计算机视觉识别任务上具有竞争力。

    

    理解神经网络在未见数据上的泛化能力对于设计更健壮可靠的模型至关重要。本文利用拓扑数据分析的方法研究神经网络的泛化差距。为此，我们在训练阶段后计算神经元激活相关性构建的加权图的同调持久图，旨在捕捉与网络的泛化能力相关的模式。我们比较了持久图的不同数值汇总的有用性，并显示其中一些的组合可以准确预测和部分解释泛化差距，而无需测试集。在两个计算机视觉识别任务（CIFAR10和SVHN）上的评估结果表明，与最先进的方法相比，我们的泛化差距预测具有竞争力。

    Understanding how neural networks generalize on unseen data is crucial for designing more robust and reliable models. In this paper, we study the generalization gap of neural networks using methods from topological data analysis. For this purpose, we compute homological persistence diagrams of weighted graphs constructed from neuron activation correlations after a training phase, aiming to capture patterns that are linked to the generalization capacity of the network. We compare the usefulness of different numerical summaries from persistence diagrams and show that a combination of some of them can accurately predict and partially explain the generalization gap without the need of a test set. Evaluation on two computer vision recognition tasks (CIFAR10 and SVHN) shows competitive generalization gap prediction when compared against state-of-the-art methods.
    
[^152]: 通过联合优化网络架构和目标函数的自动化学习在可变形医学图像配准中应用

    Automated Learning for Deformable Medical Image Registration by Jointly Optimizing Network Architectures and Objective Functions. (arXiv:2203.06810v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.06810](http://arxiv.org/abs/2203.06810)

    本文提出了一种自动学习配准算法（AutoReg），它可以协同优化网络架构和训练目标，使非计算机专家能够方便地找到适用于不同场景的配准算法。

    

    可变形图像配准在医学图像分析的各种任务中起着至关重要的作用。一个成功的配准算法，无论是从传统能量优化还是深度网络中导出，都需要计算机专家花费大量的精力来设计良好的配准能量或者为特定类型的医学数据精心调整网络架构。为了解决上述问题，本文提出了一种自动学习配准算法（AutoReg），它可以协同优化网络架构和相应的训练目标，使非计算机专家（例如医学/临床用户）能够方便地找到适用于不同场景的现成配准算法。具体而言，我们建立了一个三层框架，通过自动搜索机制和协同优化推导出配准网络架构和目标。我们在多站点体积数据集和各种配准任务上进行了图像配准实验。

    Deformable image registration plays a critical role in various tasks of medical image analysis. A successful registration algorithm, either derived from conventional energy optimization or deep networks requires tremendous efforts from computer experts to well design registration energy or to carefully tune network architectures for the specific type of medical data. To tackle the aforementioned problems, this paper proposes an automated learning registration algorithm (AutoReg) that cooperatively optimizes both architectures and their corresponding training objectives, enabling non-computer experts, e.g., medical/clinical users, to conveniently find off-the-shelf registration algorithms for diverse scenarios. Specifically, we establish a triple-level framework to deduce registration network architectures and objectives with an auto-searching mechanism and cooperating optimization. We conduct image registration experiments on multi-site volume datasets and various registration tasks. E
    
[^153]: 高斯过程插值中光滑参数估计的渐近界限

    Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation. (arXiv:2203.05400v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/2203.05400](http://arxiv.org/abs/2203.05400)

    该论文研究了高斯过程插值中光滑参数估计的渐近界限。结果表明，光滑参数的最大似然估计不能在渐近意义下欠平滑真值，并且最大似然估计能恢复一类分段支持自相似函数的真实光滑度。

    

    常见的方法是用Matern协方差核将确定性响应函数（如计算机实验的输出）建模为高斯过程。Matern核的光滑参数决定了模型在大数据极限下的许多重要属性，包括条件均值收敛到响应函数的速率。我们证明，当数据在固定有界子集$\mathbb{R}^d$上获得时，光滑参数的最大似然估计不能在渐近意义下欠平滑真值。换句话说，如果数据生成的响应函数具有Sobolev光滑度$\nu_0 > d/2$，那么光滑参数估计不能在渐近意义下小于$\nu_0$。这一下界是精准的。此外，我们还展示了最大似然估计在一类分段支持自相似函数中能恢复真实的光滑度。对于交叉验证，我们证明了一个渐近下界$\nu_0-d/2$，但这很不可能成立。

    It is common to model a deterministic response function, such as the output of a computer experiment, as a Gaussian process with a Mat\'ern covariance kernel. The smoothness parameter of a Mat\'ern kernel determines many important properties of the model in the large data limit, including the rate of convergence of the conditional mean to the response function. We prove that the maximum likelihood estimate of the smoothness parameter cannot asymptotically undersmooth the truth when the data are obtained on a fixed bounded subset of $\mathbb{R}^d$. That is, if the data-generating response function has Sobolev smoothness $\nu_0 > d/2$, then the smoothness parameter estimate cannot be asymptotically less than $\nu_0$. The lower bound is sharp. Additionally, we show that maximum likelihood estimation recovers the true smoothness for a class of compactly supported self-similar functions. For cross-validation we prove an asymptotic lower bound $\nu_0 - d/2$, which however is unlikely to be s
    
[^154]: 可扩展的决策集中学习在具有应用于母婴健康的不安定多臂赌博环境下

    Scalable Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Health. (arXiv:2202.00916v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.00916](http://arxiv.org/abs/2202.00916)

    本文提出了一种在不安定多臂赌博（RMAB）问题中进行决策集中学习的新方法，通过直接训练预测模型来最大化Whittle指数解决方案质量。

    

    本文研究了具有未知臂之间转移动力学但具有已知相关臂特征的不安定多臂赌博（RMAB）问题。目标是学习一个模型，根据特征预测转移动力学，其中Whittle指数策略使用预测的转移来解决RMAB问题。然而，先前的研究通常通过最大化预测准确性而不是最终RMAB解决方案质量来学习模型，导致训练目标与评估目标不匹配。为了解决这个问题，我们提出了一种在RMAB中直接训练预测模型以最大化Whittle指数解决方案质量的新方法。

    This paper studies restless multi-armed bandit (RMAB) problems with unknown arm transition dynamics but with known correlated arm features. The goal is to learn a model to predict transition dynamics given features, where the Whittle index policy solves the RMAB problems using predicted transitions. However, prior works often learn the model by maximizing the predictive accuracy instead of final RMAB solution quality, causing a mismatch between training and evaluation objectives. To address this shortcoming, we propose a novel approach for decision-focused learning in RMAB that directly trains the predictive model to maximize the Whittle index solution quality. We present three key contributions: (i) we establish differentiability of the Whittle index policy to support decision-focused learning; (ii) we significantly improve the scalability of decision-focused learning approaches in sequential problems, specifically RMAB problems; (iii) we apply our algorithm to a previously collected 
    
[^155]: 关于使用双感知相似性进行渐进网络对齐的能力研究

    On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2201.10945](http://arxiv.org/abs/2201.10945)

    本研究提出了Grad-Align，一种渐进网络对齐方法，通过利用强一致性节点对逐步发现节点对。该方法首先生成节点嵌入，然后计算双感知相似性度量逐步对齐节点。

    

    网络对齐（NA）是基于网络结构和节点属性查找两个网络之间节点对应关系的任务。我们的研究动机在于，由于大多数现有的NA方法都试图一次性发现所有节点对，因此它们没有利用通过节点对应关系的中间发现来更准确地找到节点匹配过程中的下一个对应关系的信息。为了解决这个挑战，我们提出了Grad-Align，一种新的渐进网络对齐方法，通过充分利用在渐进匹配的早期阶段容易发现的节点对来逐步发现节点对。具体而言，Grad-Align首先基于图神经网络和我们的逐层重构损失生成两个网络的节点嵌入。然后，通过计算双感知相似性度量逐步对齐节点。

    Network alignment (NA) is the task of finding the correspondence of nodes between two networks based on the network structure and node attributes. Our study is motivated by the fact that, since most of existing NA methods have attempted to discover all node pairs at once, they do not harness information enriched through interim discovery of node correspondences to more accurately find the next correspondences during the node matching. To tackle this challenge, we propose Grad-Align, a new NA method that gradually discovers node pairs by making full use of node pairs exhibiting strong consistency, which are easy to be discovered in the early stage of gradual matching. Specifically, Grad-Align first generates node embeddings of the two networks based on graph neural networks along with our layer-wise reconstruction loss, a loss built upon capturing the first-order and higher-order neighborhood structures. Then, nodes are gradually aligned by computing dual-perception similarity measures 
    
[^156]: 多视角子空间聚类的细粒度图学习

    Fine-grained Graph Learning for Multi-view Subspace Clustering. (arXiv:2201.04604v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.04604](http://arxiv.org/abs/2201.04604)

    本文提出了一种细粒度图学习框架用于多视角子空间聚类，解决了传统方法未建立图学习和聚类之间关联以及忽略局部结构重要性的问题。

    

    多视角子空间聚类（MSC）是一种通过整合异构信息来揭示不同视角下隐藏的内在聚类结构的一种流行无监督方法。通常，MSC方法使用图（或相似性矩阵）融合来学习一个共同的结构，并进一步应用基于图的方法进行聚类。尽管取得了进展，但大多数方法没有建立图学习和聚类之间的关联。同时，传统的图融合策略将粗粒度权重分配给多图的组合，忽视了局部结构的重要性。在本文中，我们提出了一种针对多视角子空间聚类的细粒度图学习框架（FGL-MSC）来解决这些问题。为了充分利用多视角信息，我们设计了一种特定的图学习方法，引入图正则化和局部结构融合模式。主要挑战是如何在生成适应于聚类的学习图的同时优化细粒度融合权重。

    Multi-view subspace clustering (MSC) is a popular unsupervised method by integrating heterogeneous information to reveal the intrinsic clustering structure hidden across views. Usually, MSC methods use graphs (or affinity matrices) fusion to learn a common structure, and further apply graph-based approaches to clustering. Despite progress, most of the methods do not establish the connection between graph learning and clustering. Meanwhile, conventional graph fusion strategies assign coarse-grained weights to combine multi-graph, ignoring the importance of local structure. In this paper, we propose a fine-grained graph learning framework for multi-view subspace clustering (FGL-MSC) to address these issues. To utilize the multi-view information sufficiently, we design a specific graph learning method by introducing graph regularization and a local structure fusion pattern. The main challenge is how to optimize the fine-grained fusion weights while generating the learned graph that fits t
    
[^157]: 模型为基础的安全强化学习在时间变化状态和控制约束下的应用：智能车辆中的应用

    Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11217](http://arxiv.org/abs/2112.11217)

    本文提出了一种安全RL算法，结合了基于屏障力的控制策略结构与多步策略评估机制，在保证控制安全的同时，能够应对时间变化的安全约束，并证明了其稳定性、鲁棒性和收敛性，优于几种最先进的RL算法。

    

    近年来，基于演员-评论家结构的安全强化学习（RL）在连续控制任务中受到越来越多的关注。学习一个具有安全性和收敛性保证的近似最优控制策略仍然具有挑战性。同时，很少有作品讨论了在时间变化的安全性约束下设计安全RL算法。本文提出了一种安全RL算法，用于具有时间变化状态和控制约束的非线性系统的最优控制。在所提出的方法中，我们构建了一种新颖的基于屏障力的控制策略结构，以保证控制安全。提出了一种多步策略评估机制，用于预测策略在时间变化的安全约束下的安全风险，并指导策略安全更新。已证明稳定性和鲁棒性的理论结果。同时，分析了演员评论家实现的收敛性。所提出的算法的性能在模拟的Sa中优于几种最先进的RL算法

    Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa
    
[^158]: Q-Learning用于具有一般状态空间的MDPs: 通过弱连续性下的量化来实现收敛和近似最优性

    Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity. (arXiv:2111.06781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.06781](http://arxiv.org/abs/2111.06781)

    本文研究了在具有连续状态和动作空间的MDPs中使用Q-Learning的收敛和近似最优性问题。通过量化状态和动作，我们证明Quantized Q-Learning会收敛到一个极限，并且这个极限满足一个最优性方程，从而实现近似最优性。

    

    强化学习算法通常要求马尔可夫决策过程(MDPs)中的状态和动作空间是有限的（也称为可控马尔可夫链），文献中已经做出了各种努力，以便将这些算法应用于连续的状态和动作空间。在本文中，我们证明在非常温和的正则性条件下（特别是只涉及MDP的过渡核的弱连续性），通过对状态和动作进行量化的标准 Borel MDPs 的 Q-Learning（称为Quantized Q-Learning）会收敛到一个极限，并且这个极限满足一个最优性方程，从而实现近似最优性，要么具有显式的性能界限，要么具有渐近最优保证。我们的方法基于：(i) 将量化视为一个测量核，并将量化的MDP视为一个部分观测的马尔可夫决策过程（POMDP），(ii) 利用 Q-Learning 对 POMDP 的近似最优性和收敛性结果。

    Reinforcement learning algorithms often require finiteness of state and action spaces in Markov decision processes (MDPs) (also called controlled Markov chains) and various efforts have been made in the literature towards the applicability of such algorithms for continuous state and action spaces. In this paper, we show that under very mild regularity conditions (in particular, involving only weak continuity of the transition kernel of an MDP), Q-learning for standard Borel MDPs via quantization of states and actions (called Quantized Q-Learning) converges to a limit, and furthermore this limit satisfies an optimality equation which leads to near optimality with either explicit performance bounds or which are guaranteed to be asymptotically optimal. Our approach builds on (i) viewing quantization as a measurement kernel and thus a quantized MDP as a partially observed Markov decision process (POMDP), (ii) utilizing near optimality and convergence results of Q-learning for POMDPs, and (
    
[^159]: 解决混杂因果协同过滤问题

    Deconfounded Causal Collaborative Filtering. (arXiv:2110.07122v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2110.07122](http://arxiv.org/abs/2110.07122)

    该论文提出了解决推荐系统中混杂因素影响的问题的方法，通过提出混杂因果协同过滤方法，解决了现有方法需要为每个特定混杂因素设计模型的不现实性以及潜在混杂因素无法观察的难题。

    

    推荐系统可能会受到各种混杂因素的干扰，导致推荐结果不准确，性能下降。目前解决这个问题的方法通常是为每个特定的混杂因素设计特定的模型。然而，现实世界中可能包含大量混杂因素，因此为每个特定的混杂因素设计特定的模型可能并不现实。更重要的是，除了那些专家可以手动识别和处理的“明确混杂因素”之外，还有许多“潜在混杂因素”超出了专家的想象力。例如，用户对一首歌的评分可能取决于他们当时的心情或当前的天气情况，用户对冰淇淋的偏好可能取决于空气温度。这些潜在混杂因素在记录的训练数据中可能是不可观察的。为了解决这个问题，我们提出了解混杂因果协同过滤方法。

    Recommender systems may be confounded by various types of confounding factors (also called confounders) that may lead to inaccurate recommendations and sacrificed recommendation performance. Current approaches to solving the problem usually design each specific model for each specific confounder. However, real-world systems may include a huge number of confounders and thus designing each specific model for each specific confounder could be unrealistic. More importantly, except for those ``explicit confounders'' that experts can manually identify and process such as item's position in the ranking list, there are also many ``latent confounders'' that are beyond the imagination of experts. For example, users' rating on a song may depend on their current mood or the current weather, and users' preference on ice creams may depend on the air temperature. Such latent confounders may be unobservable in the recorded training data. To solve the problem, we propose Deconfounded Causal Collaborati
    
[^160]: 从深度学习中汲取经验教训，训练脉冲神经网络

    Training Spiking Neural Networks Using Lessons From Deep Learning. (arXiv:2109.12894v5 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2109.12894](http://arxiv.org/abs/2109.12894)

    本论文介绍如何将几十年的深度学习、梯度下降、反向传播和神经科学研究的经验教训应用于生物可行的脉冲神经网络，并探索了将数据编码为脉冲和学习过程之间的微妙相互作用以及生物可行的在线学习的发展方向。

    

    大脑是寻找灵感以开发更有效的神经网络的完美之地。我们的突触和神经元的内部运作提供了对未来深度学习可能呈现的样子的一瞥。本文旨在介绍如何将几十年的深度学习、梯度下降、反向传播和神经科学研究的经验教训应用于生物可行的脉冲神经网络。我们还探讨了将数据编码为脉冲和学习过程之间微妙的相互作用；将基于梯度的学习应用到脉冲神经网络 (SNNs) 的挑战和解决方案；时间反向传播和脉冲时序依赖性可塑性之间微妙的联系以及深度学习如何向着生物可行的在线学习发展。一些想法在神经形态工程界得到了广泛的认可和使用，而其他一些在本文中首次呈现或得到了证明。SNN领域仍处于开发初期，需要解决许多挑战和限制。尽管如此，我们认为脉冲神经网络是理解大脑和开发更有效和智能的人工智能系统的有前途的途径。

    The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This paper serves as a tutorial and perspective showing how to apply the lessons learnt from several decades of research in deep learning, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks.  We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to spiking neural networks (SNNs); the subtle link between temporal backpropagation and spike timing dependent plasticity, and how deep learning might move towards biologically plausible online learning. Some ideas are well accepted and commonly used amongst the neuromorphic engineering community, while others are presented or justified for the first time here.  The fiel
    
[^161]: 混合量子经典机器学习用于生成化学和药物设计

    Hybrid quantum-classical machine learning for generative chemistry and drug design. (arXiv:2108.11644v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2108.11644](http://arxiv.org/abs/2108.11644)

    本研究构建了一个混合的量子经典机器学习模型，利用深度生成化学模型加速药物发现。通过在D-Wave量子退火器上训练，成功生成了具有药物化学和合成可及性特性的2331个新化学结构。

    

    深度生成化学模型成为加速药物发现的强大工具。然而，所有可能的药物分子的结构空间的巨大大小和复杂性构成了重大的障碍，可以通过将量子计算机与深度经典网络相结合来克服。作为实现这一目标的第一步，我们构建了一个紧凑的离散变分自动编码器（DVAE），其中潜在层中的受限玻尔兹曼机（RBM）的大小被减小。拟议模型的大小足够小，可以适应最先进的D-Wave量子退火器，并允许在ChEMBL生物活性化合物数据集子集上进行训练。最后，我们生成了2331个具有药物化学和合成可及性特性的新化学结构，其范围与来自ChEMBL的分子类似。所呈现的结果证明了使用已经存在或即将推出的量子计算设备作为未来测试平台的可行性。

    Deep generative chemistry models emerge as powerful tools to expedite drug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome with hybrid architectures combining quantum computers with deep classical networks. As the first step toward this goal, we built a compact discrete variational autoencoder (DVAE) with a Restricted Boltzmann Machine (RBM) of reduced size in its latent layer. The size of the proposed model was small enough to fit on a state-of-the-art D-Wave quantum annealer and allowed training on a subset of the ChEMBL dataset of biologically active compounds. Finally, we generated 2331 novel chemical structures with medicinal chemistry and synthetic accessibility properties in the ranges typical for molecules from ChEMBL. The presented results demonstrate the feasibility of using already existing or soon-to-be-available quantum computing devices as testbeds for futur
    
[^162]: 非线性离散分布的局部差分隐私估计

    Locally differentially private estimation of nonlinear functionals of discrete distributions. (arXiv:2107.03940v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2107.03940](http://arxiv.org/abs/2107.03940)

    本文研究了在局部差分隐私背景下估计离散分布的非线性函数的问题，并展示了互动和非互动的隐私机制。通过对幂和函数的二次风险行为的研究，在非互动情况下提出了两种插补类型的估计方法。

    

    我们研究了在局部差分隐私的背景下估计离散分布的非线性函数的问题。初始数据$x_1,\ldots,x_n \in [K]$被假设为i.i.d.并根据未知的离散分布$p = (p_1,\ldots,p_K)$进行分布。只有$\alpha$-局部差分隐私(LDP)样本$z_1,...,z_n$是公开可用的，其中术语“局部”表示每个$z_i$都是使用一个个人属性$x_i$生成的。我们展示了可以使用互动的隐私机制（PM）或非互动的隐私机制来实现这一目标。我们描述了作为$K, \, n$和$\alpha$的函数的幂和函数$F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$的二次风险的行为。在非互动情况下，我们研究了两种插补类型的估计$F_{\gamma}$的方法，适用于所有$\gamma >0$，其类似于Jiao等(2017)在多项式模型中分析的MLE。

    We study the problem of estimating non-linear functionals of discrete distributions in the context of local differential privacy. The initial data $x_1,\ldots,x_n \in [K]$ are supposed i.i.d. and distributed according to an unknown discrete distribution $p = (p_1,\ldots,p_K)$. Only $\alpha$-locally differentially private (LDP) samples $z_1,...,z_n$ are publicly available, where the term 'local' means that each $z_i$ is produced using one individual attribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e. they are allowed to use already published confidential data) or non-interactive. We describe the behavior of the quadratic risk for estimating the power sum functional $F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$, $\gamma >0$ as a function of $K, \, n$ and $\alpha$. In the non-interactive case, we study two plug-in type estimators of $F_{\gamma}$, for all $\gamma >0$, that are similar to the MLE analyzed by Jiao et al. (2017) in the multinomial model. However, due to 
    
[^163]: 图卷积神经网络中的自适应滤波器

    Adaptive Filters in Graph Convolutional Neural Networks. (arXiv:2105.10377v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.10377](http://arxiv.org/abs/2105.10377)

    该论文研究了自适应滤波器在图卷积神经网络中的应用，通过探索和利用图结构的灵活性，提高了网络处理图数据的性能。

    

    在过去的几年中，我们目睹了非欧几里得领域产生的越来越多的数据可用性，这些领域通常以复杂关系表示为图形，图神经网络 (GNN) 由于在处理图结构数据时的潜力而引起了高度关注。特别地，人们对使用 GNN 架构的扩展来在图上进行卷积的可能性非常感兴趣，这个架构通常被称为图卷积神经网络 (ConvGNN)。在图上的卷积主要分为频谱卷积和空间卷积两种形式。由于在探索和利用数据的图结构方面更加灵活，最近对于研究空间方法所能提供的可能性越来越感兴趣。找到一种适应处理输入的网络行为的方法，以最大化总体性能的想法，在神经网络中引起了广泛的兴趣。

    Over the last few years, we have witnessed the availability of an increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in exploring the possibilities in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (ConvGNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, there is recently an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks
    
[^164]: 对比吸引和对比排斥在表示学习中的应用

    Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.03746](http://arxiv.org/abs/2105.03746)

    本论文提出了一种双重对比学习策略，通过分别比较正样本和负样本在各自群组内的关系，并对正负群组之间进行对比，以进一步提高对比学习方法的性能和鲁棒性。

    

    对比学习方法以自我监督方式有效地学习数据表示，其中编码器通过一对多的softmax交叉熵损失将每个正样本与多个负样本进行对比。最近的对比学习方法通过利用大量未标记的图像数据，在预训练模型上取得了有希望的结果，如ImageNet。然而，大多数方法认为来自同一实例的增强视图是正样本对，而来自其他实例的视图则是负样本对。这种二分法不充分地考虑样本之间的关系，并且在应用到野外图像时往往会产生较差的性能。为了进一步提高对比学习的性能，并增强其在各种数据集上的鲁棒性，我们提出了一种双重对比学习策略，分别在各自的群组内比较正样本和负样本，然后进行正负群组之间的对比。

    Contrastive learning (CL) methods effectively learn data representations in a self-supervision manner, where the encoder contrasts each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. By leveraging large amounts of unlabeled image data, recent CL methods have achieved promising results when pretrained on large-scale datasets, such as ImageNet. However, most of them consider the augmented views from the same instance are positive pairs, while views from other instances are negative ones. Such binary partition insufficiently considers the relation between samples and tends to yield worse performance when generalized on images in the wild. In this paper, to further improve the performance of CL and enhance its robustness on various datasets, {we propose a doubly CL strategy that separately compares positive and negative samples within their own groups, and then proceeds with a contrast between positive and negative groups}. We realize this stra
    
[^165]: 因果协同过滤

    Causal Collaborative Filtering. (arXiv:2102.01868v5 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2102.01868](http://arxiv.org/abs/2102.01868)

    本论文提出了一种名为因果协同过滤（CCF）的通用框架，用于对协同过滤和推荐中的因果关系进行建模。这种方法可以解决纯粹相关学习导致的预测中的辛普森悖论问题，提高推荐性能。

    

    许多传统的推荐算法基于从数据中挖掘或学习相关模式来估计用户-项目的相关偏好的基本思想设计。然而，纯粹的相关学习可能导致预测中的辛普森悖论，从而导致推荐性能的损失。辛普森悖论是一个众所周知的统计现象，它会导致统计结论的混淆，忽视这个悖论可能导致不准确的决策。幸运的是，因果和反事实建模可以帮助我们超越观察数据进行用户建模和个性化，以解决这些问题。在本文中，我们提出了一种名为因果协同过滤（CCF）的通用框架，用于对协同过滤和推荐中的因果关系进行建模。我们提供了一个统一的因果视图，并从数学上证明了许多传统的协同过滤算法实际上是简化因果图下CCF的特殊情况。

    Many of the traditional recommendation algorithms are designed based on the fundamental idea of mining or learning correlative patterns from data to estimate the user-item correlative preference. However, pure correlative learning may lead to Simpson's paradox in predictions, and thus results in sacrificed recommendation performance. Simpson's paradox is a well-known statistical phenomenon, which causes confusions in statistical conclusions and ignoring the paradox may result in inaccurate decisions. Fortunately, causal and counterfactual modeling can help us to think outside of the observational data for user modeling and personalization so as to tackle such issues. In this paper, we propose Causal Collaborative Filtering (CCF) -- a general framework for modeling causality in collaborative filtering and recommendation. We provide a unified causal view of CF and mathematically show that many of the traditional CF algorithms are actually special cases of CCF under simplified causal grap
    
[^166]: 深度学习框架用于生成和分析驾驶场景轨迹

    A Deep Learning Framework for Generation and Analysis of Driving Scenario Trajectories. (arXiv:2007.14524v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2007.14524](http://arxiv.org/abs/2007.14524)

    我们提出了一个深度学习框架，用于生成和分析驾驶场景轨迹。我们开发了两种方法来处理不同长度的轨迹，并通过实验证明了其有效性。

    

    我们提出了一个统一的深度学习框架，用于生成和分析驾驶场景轨迹，并以一种原则性的方式验证其有效性。为了对不同长度的轨迹进行建模和生成，我们开发了两种方法。首先，我们通过在轨迹长度的条件下来调整循环条件生成对抗网络（RC-GAN），从而能够生成可变长度的驾驶轨迹，这是自主驾驶验证中生成测试案例的理想特性。其次，我们基于循环自编码器与生成对抗网络的架构来解决可变长度的问题，在这个方法中，我们训练一个生成对抗网络来学习/生成原始轨迹的潜在表示。在这种方法中，我们训练一个集成的前馈神经网络来估计轨迹的长度，从而能够将其从潜在空间表示中还原出来。除了轨迹生成外

    We propose a unified deep learning framework for the generation and analysis of driving scenario trajectories, and validate its effectiveness in a principled way. To model and generate scenarios of trajectories with different lengths, we develop two approaches. First, we adapt the Recurrent Conditional Generative Adversarial Networks (RC-GAN) by conditioning on the length of the trajectories. This provides us the flexibility to generate variable-length driving trajectories, a desirable feature for scenario test case generation in the verification of autonomous driving. Second, we develop an architecture based on Recurrent Autoencoder with GANs to obviate the variable length issue, wherein we train a GAN to learn/generate the latent representations of original trajectories. In this approach, we train an integrated feed-forward neural network to estimate the length of the trajectories to be able to bring them back from the latent space representation. In addition to trajectory generation
    
[^167]: 使用GNN学习NP-Hard多智能体分配规划：在随机图上的推理和可证明的拍卖适配Q学习

    Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning. (arXiv:1905.12204v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1905.12204](http://arxiv.org/abs/1905.12204)

    本文探索了使用学习算法近乎最优地解决具有时间相关奖励的多智能体、多任务的NP-hard规划问题的可能性。研究结果展示了提出方法在解决机器人/机器调度问题上的近乎最优性。

    

    本文探讨了使用基于学习的算法来近乎最优地解决具有时间相关奖励的多智能体、多任务的NP-hard规划问题的可能性。特别地，我们考虑了一类称为多机器人奖励收集问题（MRRC）的机器人/机器调度问题。这些MRRC问题很好地模拟了共乘、取送和其他相关问题。在将MRRC问题表示为顺序决策问题时，我们观察到每个状态可以被表示为概率图模型（PGM）的扩展，我们将其称为随机PGMs。然后，我们为随机PGMs开发了一种均场推理方法。接下来，我们提出了（1）一个可进行顺序转移的Q函数估计器和（2）一个支持顺序转移的拍卖方法，以在多项式时间内选择联合分配。这些方法导致了一个具有至少$1-1/e$最优性的强化学习框架。在解决MRRC问题的实验结果中突出了近乎最优性。

    This paper explores the possibility of near-optimally solving multi-agent, multi-task NP-hard planning problems with time-dependent rewards using a learning-based algorithm. In particular, we consider a class of robot/machine scheduling problems called the multi-robot reward collection problem (MRRC). Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety of related problems. In representing the MRRC problem as a sequential decision-making problem, we observe that each state can be represented as an extension of probabilistic graphical models (PGMs), which we refer to as random PGMs. We then develop a mean-field inference method for random PGMs. We then propose (1) an order-transferable Q-function estimator and (2) an order-transferability-enabled auction to select a joint assignment in polynomial time. These result in a reinforcement learning framework with at least $1-1/e$ optimality. Experimental results on solving MRRC problems highlight the near-optimality 
    
[^168]: 近似和非参数估计的ResNet类型卷积神经网络

    Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks. (arXiv:1903.10047v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1903.10047](http://arxiv.org/abs/1903.10047)

    在更可信的情况下，我们展示了ResNet类型的CNN可以在一些重要的函数类中实现极小值最优误差率，并且可以通过复制全连接神经网络的学习能力来实现。

    

    卷积神经网络(CNNs)已经在多种函数类中显示出了在近似和估计误差率方面的最优性(在极小值最大化意义上)。然而，以前分析的最优CNN是不现实的宽度，并且很难通过优化获得，因为在重要的函数类中具有稀疏约束，包括Holder类。我们展示了ResNet类型的CNN可以在更可信的情况下实现这些类的极小值最优误差率--它可以是密集的，并且其宽度、通道大小和滤波器大小与样本大小无关。关键思想是，我们可以通过定制的CNN复制全连接神经网络(FNNs)的学习能力，只要FNNs具有块稀疏结构。我们的理论在某种意义上是通用的，我们可以自动将块稀疏FNNs实现的任何近似率转化为CNNs实现的近似率。作为一个应用，我们推导了前述类型CNN的近似和估计误差率。

    Convolutional neural networks (CNNs) have been shown to achieve optimal approximation and estimation error rates (in minimax sense) in several function classes. However, previous analyzed optimal CNNs are unrealistically wide and difficult to obtain via optimization due to sparse constraints in important function classes, including the H\"older class. We show a ResNet-type CNN can attain the minimax optimal error rates in these classes in more plausible situations -- it can be dense, and its width, channel size, and filter size are constant with respect to sample size. The key idea is that we can replicate the learning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as long as the FNNs have \textit{block-sparse} structures. Our theory is general in a sense that we can automatically translate any approximation rate achieved by block-sparse FNNs into that by CNNs. As an application, we derive approximation and estimation error rates of the aformentioned type of CNNs f
    

