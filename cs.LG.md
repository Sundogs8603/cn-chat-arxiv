# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Recurrent Transformers with Dynamic Halt](https://rss.arxiv.org/abs/2402.00976) | 本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。 |
| [^2] | [NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields](https://arxiv.org/abs/2404.01300) | 通过使用Masked AutoEncoders，本文提出了NeRF-MAE用于自监督三维表示学习，利用标准的三维Vision Transformers适应NeRF的独特公式，将NeRF的体积网格作为密集输入，以产生有效的三维表示。 |
| [^3] | [CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes](https://arxiv.org/abs/2404.01299) | 利用卡通图像构建的CausalChaos!数据集，包含更长因果链的因果问答，通过动态互动和视觉展示挑战性因果关系，为模型提供了更多具挑战性且明确定义的因果关系。 |
| [^4] | [Measuring Style Similarity in Diffusion Models](https://arxiv.org/abs/2404.01292) | 提出了一个新的框架，用于理解和提取图像的风格描述符，通过新数据集和研究呈现了一种了解图像风格相似性的方法。 |
| [^5] | [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291) | 引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果 |
| [^6] | [TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model](https://arxiv.org/abs/2404.01273) | 提出了基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。 |
| [^7] | [Decentralized Collaborative Learning Framework with External Privacy Leakage Analysis](https://arxiv.org/abs/2404.01270) | 本文提出了分散多任务学习中的两种方法论进展，一是在CollabDict框架中整合深度变分自动编码器用于异常检测，二是提供了使用CollabDict训练的模型在外部共享时数据隐私泄漏的数学分析。 |
| [^8] | [Mapping the Increasing Use of LLMs in Scientific Papers](https://arxiv.org/abs/2404.01268) | 该论文通过对科学论文中LLM使用增加的映射进行了大规模分析，填补了学术写作中真实LLM修改内容比例缺失的空白。 |
| [^9] | [Bridging Remote Sensors with Multisensor Geospatial Foundation Models](https://arxiv.org/abs/2404.01260) | msGFM是一个多传感器地理空间基础模型，能够有效统一四种关键传感器模态的数据，具有处理成对和非成对传感器数据的能力，通过创新的跨传感器预训练方法在蒙版图像建模中合成联合表示，展现出在各种传感器类型下都具有强大性能的综合模型。 |
| [^10] | [New logarithmic step size for stochastic gradient descent](https://arxiv.org/abs/2404.01257) | 提出了一种新的对数步长，通过该步长可改善随机梯度下降方法在非凸函数上的收敛速率，并在CNN模型下将CIFAR100数据集的测试准确度提高了0.9%。 |
| [^11] | [A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules](https://arxiv.org/abs/2404.01245) | 该论文提出了一个通用框架，用于设计大型语言模型水印的统计效率和检测规则，通过关键统计量和秘密密钥控制误报率，同时评估水印检测规则的能力。 |
| [^12] | [Optimal Ridge Regularization for Out-of-Distribution Prediction](https://arxiv.org/abs/2404.01233) | 研究了针对分布外预测的最优岭回归正则化下的行为，并建立了确定最优正则化水平的一般条件，揭示了与分布内设置的鲜明差异。 |
| [^13] | [Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models](https://arxiv.org/abs/2404.01231) | 本文揭示了一种新的隐私后门攻击漏洞，通过污染预训练模型来增强成员推理，在多个数据集和模型上进行了广泛实验，并展示了攻击的广泛适用性和有效性。 |
| [^14] | [Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems](https://arxiv.org/abs/2404.01224) | 提出了一种协同帕累托集学习(CoPSL)框架，可以同时学习多个多目标优化问题的帕累托集，通过共享和特定层的结构，实现了不同MOP之间的协同学习。 |
| [^15] | [Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing](https://arxiv.org/abs/2404.01223) | 该研究提出了一种名为Feature Splatting的方法，将基于物理的动态场景合成与源自视觉语言的丰富语义统一起来，实现了高质量的对象分解和基于文本查询的物理特性自动合成。 |
| [^16] | [Entity-Centric Reinforcement Learning for Object Manipulation from Pixels](https://arxiv.org/abs/2404.01220) | 提出了一种适用于表示多个对象及其相互作用的视觉强化学习的结构化方法，能够处理具有对象间依赖关系的目标，并展示了训练代理的泛化能力。 |
| [^17] | [Towards System Modelling to Support Diseases Data Extraction from the Electronic Health Records for Physicians Research Activities](https://arxiv.org/abs/2404.01218) | 本文旨在通过系统建模支持从电子健康记录中提取疾病数据，以便将其用于医师研究活动，解决EHRs数据格式多样化的标准化问题。 |
| [^18] | [Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy](https://arxiv.org/abs/2404.01217) | 将领域微分方程纳入图卷积网络可提高其对不匹配训练和测试数据的稳健性。 |
| [^19] | [Novel Node Category Detection Under Subpopulation Shift](https://arxiv.org/abs/2404.01216) | 提出了一种新方法 RECO-SLIP，用于在属性图中检测属于新类别的节点，能够有效解决子群体转移下的节点检测问题，实验证明其性能优越。 |
| [^20] | [Machine Unlearning for Traditional Models and Large Language Models: A Short Survey](https://arxiv.org/abs/2404.01206) | 这项调查研究了机器遗忘在传统模型和大型语言模型中的应用，包括定义、分类、评估标准以及挑战和解决方案。 |
| [^21] | [Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization](https://arxiv.org/abs/2404.01200) | 本文开发了一种用于非凸约束分布鲁棒优化的随机算法，其计算复杂度与整体数据集大小无关，适用于大规模应用。 |
| [^22] | [Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2404.01198) | 对改进的多臂老虎机问题，我们给出了一个随机在线算法，可以在不了解最优臂最大奖励的情况下，以$O(\sqrt{k} \log k)$的逼近相对于最优。 |
| [^23] | [Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller](https://arxiv.org/abs/2404.01184) | 提出了一种利用控制屏障函数诱导的神经控制器来减少基于采样的运动规划中所需样本数量的方法，实现了实时碰撞避免控制和长视程运动规划的结合。 |
| [^24] | [BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2404.01179) | 本文提出了一种名为BEM的平衡熵混合方法，用于长尾半监督学习，旨在重新平衡数据数量和不确定性的类分布。 |
| [^25] | [TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression](https://arxiv.org/abs/2404.01153) | 提出了一种新型融合正则化器的两步法方法，有效处理高维回归中的模型偏移和协变量转移，提高了目标任务的学习性能，具有稳健性并满足最小-最大最优条件。 |
| [^26] | [Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit](https://arxiv.org/abs/2404.01147) | LLMs在模拟基于事实问题的社交媒体问题中人类答案时表现较好，为未来研究提供了方向。 |
| [^27] | [Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations](https://arxiv.org/abs/2404.01145) | 该研究显示逐时间训练方法可以被广泛理解为优化-离散化或离散化-优化方案，在稳定性和误差分析方面取得新颖结果，同时有助于建立方法之间的连接。 |
| [^28] | [SoK: A Review of Differentially Private Linear Models For High-Dimensional Data](https://arxiv.org/abs/2404.01141) | 本文对高维数据中差分私有线性模型的优化方法进行全面审查，发现鲁棒和优化的坐标算法效果最好，可为未来研究提供参考。 |
| [^29] | [Enhanced Precision in Rainfall Forecasting for Mumbai: Utilizing Physics Informed ConvLSTM2D Models for Finer Spatial and Temporal Resolution](https://arxiv.org/abs/2404.01122) | 该研究引入了物理信息的ConvLSTM2D模型，旨在提高较细尺度的降雨预测准确性，尤其针对孟买城市的降水预测。 |
| [^30] | [Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation](https://arxiv.org/abs/2404.01102) | 使用扩散引导的新型无监督图像翻译方法，解决了零样本跨模态图像分割任务中的挑战 |
| [^31] | [UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models](https://arxiv.org/abs/2404.01101) | 扩散模型容易受到后门攻击，本文提出了一个统一的框架用于输入级后门检测，弥补了该领域的空白，并不需要访问模型的白盒信息。 |
| [^32] | [Finite Sample Frequency Domain Identification](https://arxiv.org/abs/2404.01100) | 本研究提出了一种在有限样本情况下进行非参数频域系统识别的方法，通过Empirical Transfer Function Estimate（ETFE）在特定频率处准确估计频率响应，并证明在次高斯彩色噪声和稳定性假设下，ETFE估计值准确可靠。 |
| [^33] | [What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety](https://arxiv.org/abs/2404.01099) | 通过双向锚定方法，识别那些在微调后更可能降低模型安全性的良性数据子集，提高模型对有害请求的响应率。 |
| [^34] | [Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling](https://arxiv.org/abs/2404.01078) | EmSHAP提出了基于能量模型的Shapley值估计方法，通过引入GRU来消除输入特征顺序的影响，从而可以有效近似任意特征子集下深度学习模型的Shapley值贡献函数。 |
| [^35] | [Prompt Learning for Oriented Power Transmission Tower Detection in High-Resolution SAR Images](https://arxiv.org/abs/2404.01074) | 本文将提示学习引入到面向对象检测器（P2Det）中，通过引入局部化和提示位置来提升高分辨率SAR图像中输电塔的检测性能 |
| [^36] | [A comparison of Single- and Double-generator formalisms for Thermodynamics-Informed Neural Networks](https://arxiv.org/abs/2404.01060) | 引入热力学原理到神经网络架构中是一种有效的方式来增加预测准确性和鲁棒性 |
| [^37] | [A Novel Audio Representation for Music Genre Identification in MIR](https://arxiv.org/abs/2404.01058) | 本研究探索了一种新颖的音频表示形式，但发现其在音乐流派识别任务中并不比传统的Mel频谱图更优越。 |
| [^38] | [Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation](https://arxiv.org/abs/2404.01050) | DragNoise通过利用U-Net预测的噪声输出作为语义编辑器，在不重新追踪潜在地图的情况下，实现了稳健且加速的编辑。 |
| [^39] | [A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification](https://arxiv.org/abs/2404.01049) | 提出了一种基于区块的新型星系分类算法，利用SDSS数据和卷积神经网络，在星系分类领域取得了最新的性能表现。 |
| [^40] | [Can LLMs get help from other LLMs without revealing private information?](https://arxiv.org/abs/2404.01041) | 本研究展示了在级联系统中运用隐私保护技术的可行性，以减少在查询远程模型时泄漏私人信息的风险，并引入了两个隐私度量。 |
| [^41] | [A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide](https://arxiv.org/abs/2404.01039) | 该研究是第一份致力于超图神经网络的调查报告，深入介绍了HNN架构、训练策略和应用，为从事数据挖掘和机器学习的研究人员提供了重要的指导。 |
| [^42] | [Higher education assessment practice in the era of generative AI tools](https://arxiv.org/abs/2404.01036) | 该研究评估了生成人工智能工具对高等教育评估实践的影响，发现这些工具展示了学科知识、问题解决、分析能力等技能，但在不道德使用时可能限制学习，同时也揭示了部分学科评估中这些工具的局限性。 |
| [^43] | [Query Performance Prediction using Relevance Judgments Generated by Large Language Models](https://arxiv.org/abs/2404.01012) | 提出了一种使用自动生成的相关性判断的查询性能预测框架，能够解决先前方法中对不同IR评估指标准确性和解释性的限制。 |
| [^44] | [Make Continual Learning Stronger via C-Flat](https://arxiv.org/abs/2404.00986) | 通过C-Flat方法，我们提出了一种更平坦的损失景观，可用于持续学习，简化了模型训练过程并提高了模型泛化能力。 |
| [^45] | [Continual Learning for Smart City: A Survey](https://arxiv.org/abs/2404.00983) | 该调研综合审查了智慧城市发展中广泛使用的持续学习方法，内容涵盖了方法论分类、多种应用领域和相关数据集。 |
| [^46] | [Diffusion-Driven Domain Adaptation for Generating 3D Molecules](https://arxiv.org/abs/2404.00962) | 该研究提出了一种基于扩散的领域自适应分子生成方法 GADM，可以在不需要收集数据的情况下将生成模型迁移到新领域，通过利用等变蒙板自编码器和各种掩蔽策略来捕获结构变化所带来的领域差异，并能够泛化到目标领域中看不见的结构变化。 |
| [^47] | [Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs](https://arxiv.org/abs/2404.00942) | 使用大型知识图谱创建评估模型以检查大型语言模型在事实性上的表现，有效降低评估成本。 |
| [^48] | [Instance-Aware Group Quantization for Vision Transformers](https://arxiv.org/abs/2404.00928) | IGQ-ViT是一种面向视觉Transformer的实例感知组量化方法，通过动态地将激活映射的通道分成多个组，使得每个输入实例内的激活具有相似统计特性。 |
| [^49] | [Token-Efficient Leverage Learning in Large Language Models](https://arxiv.org/abs/2404.00914) | 介绍了一种名为Token-Efficient Leverage Learning（TELL）的方法，在大型语言模型中展示了其降低任务数据需求、提高任务性能的潜力，为低资源任务带来了竞争性能。 |
| [^50] | [CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series](https://arxiv.org/abs/2404.00898) | 该研究提出了一种名为类别依赖性自动自适应策略（CAAP）的新颖基于深度学习的方法，旨在解决时间序列中存在的类别依赖性偏差问题，特别关注心电图等重要信号在医疗领域中的应用潜力。 |
| [^51] | [Machine Learning Robustness: A Primer](https://arxiv.org/abs/2404.00897) | 该章节探讨了机器学习中稳健性的重要概念及关键的技术和因素，以确立人工智能系统的可信度。 |
| [^52] | [Modeling Output-Level Task Relatedness in Multi-Task Learning with Feedback Mechanism](https://arxiv.org/abs/2404.00885) | 在多任务学习中引入反馈机制，将一个任务的输出作为另一个任务的隐藏特征，使静态的多任务学习模型转变为动态模型。 |
| [^53] | [Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization](https://arxiv.org/abs/2404.00883) | 使用非负张量因子分解解决了基于锚图的多视图聚类方法缺乏聚类可解释性和忽视视图间信息的问题 |
| [^54] | [Metric Learning to Accelerate Convergence of Operator Splitting Methods for Differentiable Parametric Programming](https://arxiv.org/abs/2404.00882) | 该研究提出了一种新方法，通过学习优化算子分裂算法的度量空间，从而最大化其收敛速度，特别适用于二次规划问题。 |
| [^55] | [Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity](https://arxiv.org/abs/2404.00880) | 循环神经网络（RNN）与其他类型的神经网络之间存在更紧密的关系，这种关系比我们通常认为的更实际，并且具有更深层次的联系。 |
| [^56] | [Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance](https://arxiv.org/abs/2404.00860) | 本研究提出了一种名为 Lipsum-FT 的算法，通过有效利用视觉-语言预训练模型的语言建模，实现了在分布转移场景下对零样本模型进行稳健微调，提高了下游任务的性能。 |
| [^57] | [Do language models plan ahead for future tokens?](https://arxiv.org/abs/2404.00859) | 语言模型在推理过程中会提前准备未来标记所需的信息，可能是通过预缓存或面包屑的方式实现。 |
| [^58] | [Ensemble Learning for Vietnamese Scene Text Spotting in Urban Environments](https://arxiv.org/abs/2404.00852) | 该方法提出了一个简单而高效的集成学习框架，可以显著提升在具有挑战性的城市环境中的越南场景文字识别性能。 |
| [^59] | [Predictive Performance Comparison of Decision Policies Under Confounding](https://arxiv.org/abs/2404.00848) | 提出了一种方法，通过现代识别方法比较决策政策的预测性能，关键在于可以安全忽略不确定性区域。 |
| [^60] | [Transfer Learning with Point Transformers](https://arxiv.org/abs/2404.00846) | 基于点变换器的迁移学习模型在处理点云数据分类等任务时表现良好，但在不同数据集上迁移学习效果并不总是优于从头开始训练模型，因为数据集之间的分布差异较大。 |
| [^61] | [Automated HER2 Scoring in Breast Cancer Images Using Deep Learning and Pyramid Sampling](https://arxiv.org/abs/2404.00837) | 使用深度学习和金字塔采样的方法实现了在乳腺癌组织图像中对HER2状态的自动分类，有效管理计算负荷并有效地分析细胞和更大范围的组织细节。 |
| [^62] | [Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm](https://arxiv.org/abs/2404.00836) | 该论文提出了一种边缘学习中联合预训练和微调设计范式，通过分析系统参数对收敛速率的影响，并提出了联合通信和计算资源管理方案。 |
| [^63] | [HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs](https://arxiv.org/abs/2404.00816) | HeteroMILE提出了一种多级嵌入框架，可以将当代图嵌入方法扩展到大型异构图，并通过反复粗化和细化的方式有效降低计算成本。 |
| [^64] | [On Difficulties of Attention Factorization through Shared Memory](https://arxiv.org/abs/2404.00798) | 通过过滤输入信号来优化与内存通信，可以显着提高模型性能，挑战了使用注意力机制的传统思维。 |
| [^65] | [Metarobotics for Industry and Society: Vision, Technologies, and Opportunities](https://arxiv.org/abs/2404.00797) | Metarobotics旨在通过结合无线通信、多感官沉浸和集体智能，为远程机器人应用提供普遍、流动和非侵入式的访问和互动，有望为工业和社会带来诸多益处。 |
| [^66] | [Rehearsal-Free Modular and Compositional Continual Learning for Language Models](https://arxiv.org/abs/2404.00790) | 提出了一种无需排练的模块化和组合式持续学习框架，可以持续向语言模型添加新模块并将其与现有模块组合，实验证明该框架优于现有技术并有效推动知识转移。 |
| [^67] | [Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning](https://arxiv.org/abs/2404.00785) | 本研究利用图变分自动编码器和对比学习解开神经系统疾病中海马形状变异的关键潜变量，超越了其他先进方法在解开能力上的表现。 |
| [^68] | [Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2404.00781) | 本文提出了一种新方法，即基于效用的扰动梯度下降（UPGD），通过在梯度更新中应用不同大小的扰动，保护有用单元以防遗忘，同时恢复不太有用单元的可塑性。 |
| [^69] | [Privacy-preserving Optics for Enhancing Protection in Face De-identification](https://arxiv.org/abs/2404.00777) | 本文提出了一种硬件级人脸去标识化方法，通过学习光学编码器和回归模型生成人脸热图，同时隐藏人脸身份，解决了人脸去标识化软件存在的漏洞。 |
| [^70] | [PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning](https://arxiv.org/abs/2404.00776) | PyTorch Frame是一个用于处理多模态表格数据的PyTorch框架，通过提供数据结构、模型抽象和外部基础模型整合等功能，实现了模块化的表格模型实现，并成功将这些模型应用于复杂的数据集。 |
| [^71] | [SOAR: Improved Indexing for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2404.00774) | SOAR提出了一种针对近似最近邻搜索的新数据索引技术，通过使用增强正交残差损失来优化每个表示，从而改善了索引质量并实现了最先进的性能表现。 |
| [^72] | [Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery](https://arxiv.org/abs/2404.00756) | 该论文提出了一种神经符号框架，名为Recover，用于在线故障识别和恢复，通过集成符号信息来提升大型语言模型生成恢复计划的能力，降低相关成本。 |
| [^73] | [C-XGBoost: A tree boosting model for causal effect estimation](https://arxiv.org/abs/2404.00751) | 该论文提出了一种名为C-XGBoost的树提升模型，可用于预测潜在结果，结合了基于树的模型和基于神经网络的因果推断模型的优势，同时继承了XGBoost模型的高效处理缺失值特征的优势。 |
| [^74] | [Nonparametric End-to-End Probabilistic Forecasting of Distributed Generation Outputs Considering Missing Data Imputation](https://arxiv.org/abs/2404.00729) | 本文提出了一种非参数化的端到端方法，用于考虑缺失数据插补的分布式可再生发电输出的概率预测，通过长短期记忆网络建模概率分布，并结合端到端训练过程，成功展示了在处理缺失值情况下的卓越性能。 |
| [^75] | [MugenNet: A Novel Combined Convolution Neural Network and Transformer Network with its Application for Colonic Polyp Image Segmentation](https://arxiv.org/abs/2404.00726) | 提出了MugenNet，一种结合了卷积神经网络和Transformer网络的方法，用于解决结肠息肉图像分割中CNN训练时间长和Transformer信息丢失的问题 |
| [^76] | [The Larger the Better? Improved LLM Code-Generation via Budget Reallocation](https://arxiv.org/abs/2404.00725) | 较小的语言模型可以在相同预算下产生可靠的改进，但在无法进行单元测试的情况下，较小的模型选择排名次于较大模型的单个输出。 |
| [^77] | [Survey of Computerized Adaptive Testing: A Machine Learning Perspective](https://arxiv.org/abs/2404.00712) | 本文以机器学习视角综述了计算机自适应测试（CAT），重点解析其测试问题选择算法和如何优化认知诊断模型、题库构建和测试控制。 |
| [^78] | [Privacy Re-identification Attacks on Tabular GANs](https://arxiv.org/abs/2404.00696) | 本研究调查了使用生成对抗网络（GANs）生成表格合成数据时可能产生的隐私风险，分析了对合成数据进行再识别攻击的影响以及信息对发动更成功的再识别攻击的潜在用处。 |
| [^79] | [Meta Learning in Bandits within Shared Affine Subspaces](https://arxiv.org/abs/2404.00688) | 通过利用低维仿射子空间集中性，我们提出两种策略解决了在多个环境随机臂上任务中减少预期遗憾的问题。 |
| [^80] | [Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning](https://arxiv.org/abs/2404.00686) | 这项工作引入了最大均值差异Q学习（MMD-QL）来改进强化学习中价值函数不确定性的传播，通过使用MMD重心，实现了比Wasserstein距离更紧的概率度量，在实验中表现优于其他算法，并结合深度网络创造了MMD Q网络（MMD-QN）。 |
| [^81] | [A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures](https://arxiv.org/abs/2404.00673) | 本研究是第一个全面调查模型解释中隐私攻击及其对抗措施的论文，通过分类隐私攻击和对抗措施，初步探讨了隐私泄漏原因，提出未解决问题和未来研究方向。 |
| [^82] | [A General and Efficient Training for Transformer via Token Expansion](https://arxiv.org/abs/2404.00672) | 本文提出了一种名为ToE的令牌生长方案，旨在通过加速一致性训练来改善ViT的训练效果。 |
| [^83] | [Accelerated Parameter-Free Stochastic Optimization](https://arxiv.org/abs/2404.00666) | 提出了一种加速的无参数随机优化方法，不需要先验了解问题参数，在平滑随机凸优化的情况下实现了近乎最佳收敛速率并表现强大。 |
| [^84] | [Observations on Building RAG Systems for Technical Documents](https://arxiv.org/abs/2404.00657) | 研究回顾了对技术文档RAG系统构建的重要因素，并通过实验证明了最佳实践和潜在挑战 |
| [^85] | [Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration](https://arxiv.org/abs/2404.00651) | 本文提出了一种结合预测模型和离线学习元素的强化学习算法，通过内在奖励与模型不确定性的关联，在连续控制任务中实现了样本有效的探索。 |
| [^86] | [Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs](https://arxiv.org/abs/2404.00640) | 提出了一种基于LLM的两阶段策略，帮助终端用户通过日志定位配置错误的根本原因，并开发了相应工具LogConfigLocalizer，以帮助终端用户通过日志分析解决配置错误。 |
| [^87] | [RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning](https://arxiv.org/abs/2404.00639) | 提出了基于强化学习的乘法器设计优化框架RL-MUL，利用矩阵和张量表示乘法器的压缩树，通过定制化的奖励实现区域和延迟之间的权衡，同时扩展到优化融合乘-累加（MAC）设计。 |
| [^88] | [HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs](https://arxiv.org/abs/2404.00638) | 设计了一种新颖的生成式自监督学习策略，通过超边填充的任务有效捕获复杂的超图拓扑结构 |
| [^89] | [Variational Autoencoders for exteroceptive perception in reinforcement learning-based collision avoidance](https://arxiv.org/abs/2404.00623) | 该论文研究了在强化学习驱动碰撞回避中使用外部感知变分自编码器，以获取传感器数据的低维表示。 |
| [^90] | [A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours](https://arxiv.org/abs/2404.00618) | 提出了一种多分支径向基网络方法来成功预测复杂混沌行为，通过独特的神经网络架构并引入了注意机制，展示了先进机器学习算法在阐明中的潜力 |
| [^91] | [Extensive Self-Contrast Enables Feedback-Free Language Model Alignment](https://arxiv.org/abs/2404.00604) | 本论文介绍了一种利用广泛自对比生成负例的无需反馈的大型语言模型对齐方法，该方法在实验中表现优于直接偏好优化方法。 |
| [^92] | [LAESI: Leaf Area Estimation with Synthetic Imagery](https://arxiv.org/abs/2404.00593) | LAESI数据集包含10万张合成叶片图像，用于叶形态分析，机器学习模型训练后可预测叶片表面积，并提供了基于3D程序模型和生成式AI的高效框架。 |
| [^93] | [Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing](https://arxiv.org/abs/2404.00589) | 介绍了一种利用大型语言模型处理图数据中不确定性的方法，通过不确定性感知模块增强，提供置信度评分，实验结果表明该方法在知识图完成和图分类任务上超越了最先进算法。 |
| [^94] | [Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification](https://arxiv.org/abs/2404.00576) | 提出了两种新型的自动双向加权集成算法，用于提高脑肿瘤检测和分类的效果 |
| [^95] | [ADs: Active Data-sharing for Data Quality Assurance in Advanced Manufacturing Systems](https://arxiv.org/abs/2404.00572) | 提出了一种ADs框架，旨在通过同时选择对下游任务有益的最具信息量的数据点并减轻所有选定数据点之间的分布不匹配，确保多台机器之间共享数据的质量。 |
| [^96] | [Convergence of Continuous Normalizing Flows for Learning Probability Distributions](https://arxiv.org/abs/2404.00551) | 本文研究了具有线性插值的CNFs在从有限随机样本中学习概率分布时的理论性质，建立了非渐近误差界，并提供了收敛分析框架。 |
| [^97] | [Unified, Verifiable Neural Simulators for Electromagnetic Wave Inverse Problems](https://arxiv.org/abs/2404.00545) | 提出了一个统一的神经模拟器模型，可以快速处理数千个自由度的散射模拟，并通过中间预测能力提供了准确性保证。 |
| [^98] | [Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement Learning](https://arxiv.org/abs/2404.00539) | 通过引入两阶段图指针网络和强化学习，解决QAP问题的方法在实验中展示出半最优解。 |
| [^99] | [Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization](https://arxiv.org/abs/2404.00530) | 本研究提出了一种新的偏好获取方法，通过DOVE协议对指令-响应对的联合概率进行优化，以对齐大型语言模型。 |
| [^100] | [Super Non-singular Decompositions of Polynomials and their Application to Robustly Learning Low-degree PTFs](https://arxiv.org/abs/2404.00529) | 提出了在强污染模型下鲁棒学习低次多项式阈函数的多项式时间PAC学习算法，在高斯分布下具有误差保证$O_{d, c}(\text{opt}^{1-c})$。 |
| [^101] | [Generative weather for improved crop model simulations](https://arxiv.org/abs/2404.00528) | 提出了一种新方法，构建生成式模型用于长期天气预报，从而改进作物产量预测，结果表明在小麦、大麦和油菜的生产中，以及这些作物轮作三年生产中，我们的方法在每个指标上均优于传统方法。 |
| [^102] | [Creating synthetic energy meter data using conditional diffusion and building metadata](https://arxiv.org/abs/2404.00525) | 该研究提出了一种使用条件扩散模型生成高质量合成能源数据的方法，能够处理长期年度消费配置文件并利用元数据产生连贯的数据 |
| [^103] | [Minimum-Norm Interpolation Under Covariate Shift](https://arxiv.org/abs/2404.00522) | 本研究首次证明了在转移学习设置下，良性过拟合线性插值器的非渐近超额风险界，并提出了一种新的分类方法。 |
| [^104] | [CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization](https://arxiv.org/abs/2404.00521) | 通过引入CHAIN，该方法在数据有限的情况下，解决了GANs中鉴别器过拟合和训练不稳定的问题，提高了泛化能力和训练稳定性。 |
| [^105] | [DailyMAE: Towards Pretraining Masked Autoencoders in One Day](https://arxiv.org/abs/2404.00509) | 该研究提出了一种针对遮蔽自动编码器（MAE）的高效训练方法，使得能够在短短18小时内使用单台机器训练模型，取得了高达5倍的速度提升。 |
| [^106] | [Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models](https://arxiv.org/abs/2404.00506) | 该论文提出了一种无需标签的无监督去学习方法，通过引入变分方法并利用表示分布的近似，实现了在深度模型中消除已遗忘数据信息的目标。 |
| [^107] | [Transfer Learning with Reconstruction Loss](https://arxiv.org/abs/2404.00505) | 本文通过引入额外的重建阶段和重建损失，提出了一种具有共享模型参数和特征表示的模型训练方法，建立了共同信息的概念，用于解决相关任务。 |
| [^108] | [Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation](https://arxiv.org/abs/2404.00502) | 该论文引入了一种条件伪可逆标准化流模型，可以有效构建代理模型，无需事先了解噪声和函数，直接学习并高效生成样本，用于量化不确定性传播中的正向和反向传播。 |
| [^109] | [94% on CIFAR-10 in 3.29 Seconds on a Single GPU](https://arxiv.org/abs/2404.00498) | 在单个GPU上以3.29秒实现CIFAR-10数据集94%准确率，并提出了一种改进的水平翻转增强方法。 |
| [^110] | [Multi-hop Question Answering under Temporal Knowledge Editing](https://arxiv.org/abs/2404.00492) | 提出了一个新颖的框架TEMPLE-MQA，通过构建时间感知图和推理路径、结构检索和联合推理阶段，有效地识别多跳问题回答中的时间背景，在基准数据集上显著优于基线模型，并贡献了一个新数据集TKEMQA。 |
| [^111] | [PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression](https://arxiv.org/abs/2404.00489) | 提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。 |
| [^112] | [Noise-Aware Training of Layout-Aware Language Models](https://arxiv.org/abs/2404.00488) | 本论文提出了一种噪声感知训练方法，可以以可扩展的方式使用弱标记文档训练提取器，从而避免了在企业场景中训练大量不同文档类型的自定义提取器所需的昂贵人工标记成本。 |
| [^113] | [Cross-lingual Named Entity Corpus for Slavic Languages](https://arxiv.org/abs/2404.00482) | 介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。 |
| [^114] | [Convolutional Bayesian Filtering](https://arxiv.org/abs/2404.00481) | 引入不等条件的额外事件，将条件概率转化为特殊积分形式，推广为卷积形式的新滤波框架，称为卷积贝叶斯滤波。 |
| [^115] | [DE-HNN: An effective neural model for Circuit Netlist representation](https://arxiv.org/abs/2404.00477) | 设计师们开发了一种名为DE-HNN的神经模型，用于电路网表表示，以解决优化工具运行时间长的问题。 |
| [^116] | [Linguistic Calibration of Language Models](https://arxiv.org/abs/2404.00474) | 该论文提出了一种通过语言模型的文本生成来实现语言校准，可以使用户做出校准概率预测的方法。 |
| [^117] | [Privacy Backdoors: Stealing Data with Corrupted Pretrained Models](https://arxiv.org/abs/2404.00473) | 预训练模型的权重遭到篡改后，可以构建隐私后门，完全损害微调数据的隐私性，进而对使用差分隐私训练的模型进行严格的隐私攻击。 |
| [^118] | [Score-Based Diffusion Models for Photoacoustic Tomography Image Reconstruction](https://arxiv.org/abs/2404.00471) | 本研究提出了使用基于评分的扩散模型来解决光声 tomography 图像重建中由于有限传感器覆盖或换能器密度不足而导致的逆问题，该方法能够在不同的换能器稀疏条件下保持鲁棒性。 |
| [^119] | [Classification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network](https://arxiv.org/abs/2404.00470) | 本研究使用变压器卷积神经网络对儿童心音进行短段分类，调查了自动分类所需的最小信号持续时间，并确定了适合RMSSD和ZCR指标的理想阈值。 |
| [^120] | [Computation and Communication Efficient Lightweighting Vertical Federated Learning](https://arxiv.org/abs/2404.00466) | 提出轻量级纵向联邦学习（LVFL）的概念，针对计算和通信效率采用分离的轻量化策略，建立了收敛界限，并在图像分类数据集上得到了验证 |
| [^121] | [Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to Characterize Heterogeneity Across Alzheimer's Disease and Related Dementias](https://arxiv.org/abs/2404.00464) | 利用电子病历中的预训练嵌入和基于Transformer的嵌入，对阿尔茨海默病及相关痴呆患者进行特征分析，发现了疾病人群的异质性，并探讨了亚型的临床意义。 |
| [^122] | [Addressing Both Statistical and Causal Gender Fairness in NLP Models](https://arxiv.org/abs/2404.00463) | 本研究评估了在NLP模型中同时处理统计和因果性别公平性偏见的方法，发现结合统计和因果性去偏置技术能够有效减少偏见。 |
| [^123] | [Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models](https://arxiv.org/abs/2404.00462) | 基于基础世界模型，提出了一种能够直接预测因果未来状态的方法，在安全预测任务中表现优于标准世界模型，并且性能与监督学习相当，尽管没有使用任何数据。 |
| [^124] | [Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning](https://arxiv.org/abs/2404.00461) | 基于提示的学习中的干净标签攻击通过特定提示作为触发器，实现成功的同时确保正确标记有毒样本，但仍面临误激活问题和挑战，需要更高比例的毒害。 |
| [^125] | [QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs](https://arxiv.org/abs/2404.00456) | QuaRot是一种基于旋转的新量化方案，能够在LLMs中进行无异常值的4位推断，实现了端到端的量化，并保持了99%的零-shot表现。 |
| [^126] | [Communication Efficient Distributed Training with Distributed Lion](https://arxiv.org/abs/2404.00438) | 分布式狮子是对 Lion 进行了创新性改进，利用符号操作符降低了通信成本，在分布式训练中取得了与标准 Lion 或 AdamW 优化器相当的性能，并显著减少了通信带宽。 |
| [^127] | [Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators](https://arxiv.org/abs/2404.00437) | 本研究结合自然语言处理和机器学习，提出了一种可解释的法律文本分类系统，使模型的决策变得可理解给最终用户 |
| [^128] | [Visualizing Routes with AI-Discovered Street-View Patterns](https://arxiv.org/abs/2404.00431) | 本文通过实验AI技术提出了利用语义潜在向量量化视觉外观特征，计算街景图像相似性并发现空间图像模式，并将其整合到驾驶路线规划器中，最终提出了交互式可视化原型VivaRoutes，帮助用户有效规划驾驶路线。 |
| [^129] | [Learning Service Selection Decision Making Behaviors During Scientific Workflow Development](https://arxiv.org/abs/2404.00420) | 提出了一种基于工作流来源学习服务表示和服务选择决策行为的上下文感知方法，用于推荐科学工作流开发过程中的下一步服务。 |
| [^130] | [Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation](https://arxiv.org/abs/2404.00417) | 引入了Multi-level Online Sequential Experts (MOSE)方法，通过多层监督和反向自蒸馏，解决了在线持续学习中新旧训练样本学习不足和重复学习的问题。 |
| [^131] | [Language Models are Spacecraft Operators](https://arxiv.org/abs/2404.00413) | 该论文旨在将大型语言模型(LLMs)应用于空间导航和控制领域，通过开发纯LLM解决方案，并在 Kerbal 太空计划差分游戏挑战中取得第二名，首次将LLM代理集成到空间资源中。 |
| [^132] | [SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout](https://arxiv.org/abs/2404.00412) | SVGCraft引入了一种端到端框架，可以从文本描述中生成描绘整个场景的矢量图，其中包括利用预训练的LLM进行布局生成、产生遮罩潜变量以进行准确对象放置、融合注意力图以及使用扩散U-Net进行合成，同时通过预训练的编码器和LPIPS损失进行优化。 |
| [^133] | [Aardvark Weather: end-to-end data-driven weather forecasting](https://arxiv.org/abs/2404.00411) | Aardvark Weather是第一个端到端数据驱动的预报系统，能够取代传统数值天气预报系统，提供全球和本地精准预报。 |
| [^134] | [Deep Learning with Parametric Lenses](https://arxiv.org/abs/2404.00408) | 提出了一种基于镜头的深度学习方法，能够统一各种梯度下降算法和损失函数，同时在连续和离散领域都具有实际意义。 |
| [^135] | [Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order](https://arxiv.org/abs/2404.00399) | Aurora-M 是第一个根据美国行政命令进行红队测试的开源多语言模型，通过在英语、芬兰语、印地语、日语、越南语和代码上训练，不断预训练，包括了人工审核的安全说明，总训练 token 数超过 2 万亿个 |
| [^136] | [Learning truly monotone operators with applications to nonlinear inverse problems](https://arxiv.org/abs/2404.00390) | 通过新定义的惩罚损失学习单调神经网络，解决图像处理中的问题，并利用FBF算法提供收敛保证，以解决非线性逆问题。 |
| [^137] | [Constrained Layout Generation with Factor Graphs](https://arxiv.org/abs/2404.00385) | 本文提出了一种基于因子图的方法，用于面向对象的受约束布局生成，可以准确捕捉复杂交互关系，填补了现有方法的不足。 |
| [^138] | [From Learning to Analytics: Improving Model Efficacy with Goal-Directed Client Selection](https://arxiv.org/abs/2404.00371) | 提出闭环模型分析框架，通过目标导向客户选择，解决联邦学习中的系统和数据异构性挑战。 |
| [^139] | [Revisiting Random Weight Perturbation for Efficiently Improving Generalization](https://arxiv.org/abs/2404.00357) | 重新审视随机权重扰动方法用于改善深度神经网络的泛化能力，通过两方面的改进实现更高效的效果。 |
| [^140] | [YNetr: Dual-Encoder architecture on Plain Scan Liver Tumors (PSLT)](https://arxiv.org/abs/2404.00327) | YNetr模型在Plain Scan Liver Tumors数据集上实现了62.63%的Dice系数，优于其他模型，填补了肝肿瘤普通扫描分割数据集和算法的空白。 |
| [^141] | [CLIP-driven Outliers Synthesis for few-shot OOD detection](https://arxiv.org/abs/2404.00323) | 提出了一种基于CLIP的离群值合成方法（CLIP-OS），能够在少样本场景下的OOD检测中解决缺乏可靠OOD监督信息的问题，实现了ID相关信息和ID不相关信息的分离，通过混合ID相关特征合成可靠的OOD数据。 |
| [^142] | [TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa](https://arxiv.org/abs/2404.00297) | TRABSA是一个集成了transformer架构、注意力机制和BiLSTM网络的混合框架，利用RoBERTa在大量推特上训练，填补了情感分析领域的差距，实现了94%的准确性和显著的性能提升。 |
| [^143] | [Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods](https://arxiv.org/abs/2404.00282) | 大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。 |
| [^144] | [TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search](https://arxiv.org/abs/2404.00271) | TG-NAS提出了一种新型模型通用代理，利用Transformer的运算符嵌入生成器和图卷积网络来预测架构性能，指导神经结构搜索。 |
| [^145] | [DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation](https://arxiv.org/abs/2404.00264) | 提出了一种名为DiLM的文本数据集蒸馏方法，通过训练语言模型生成文本数据作为合成训练样本，解决了嵌入级别蒸馏数据集无法用于训练其他模型的问题。 |
| [^146] | [YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery](https://arxiv.org/abs/2404.00257) | 该论文提出了基于YOLO架构的YOLOOC检测器，针对开放类别设置引入了标签平滑，有效应对新类别检测和增量学习的挑战。 |
| [^147] | [Clustering for Protein Representation Learning](https://arxiv.org/abs/2404.00254) | 该论文提出了一个神经聚类框架，通过考虑蛋白质的一级和三级结构信息，自动发现蛋白质的关键组分，并利用迭代聚类策略创建层次化和信息丰富的蛋白质表示。 |
| [^148] | [Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives](https://arxiv.org/abs/2404.00247) | 本文从迁移学习的角度探讨了如何将其与强化学习相结合，为过程控制带来新的可能性。 |
| [^149] | [Information Security and Privacy in the Digital World: Some Selected Topics](https://arxiv.org/abs/2404.00235) | 数字世界中的信息安全与隐私面临新挑战，需要更强大和更具韧性的安全方案，这本书呈现了密码学和计算与通信安全领域的最前沿研究。 |
| [^150] | [Efficient Automatic Tuning for Data-driven Model Predictive Control via Meta-Learning](https://arxiv.org/abs/2404.00232) | 本文提出了一种使用元学习方法Portfolio来改进AutoMPC的效率和稳定性，在多个任务上优化BO的初始设计，并通过固定初始配置来稳定调节过程，在有限计算资源内超越了纯BO方法。 |
| [^151] | [Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images](https://arxiv.org/abs/2404.00231) | 这项工作提出了TransDeformer，使用注意力机制实现了对腰椎轮廓的高空间准确性重建，并跨患者实现了网格对应，为医学参数测量提供了可靠性，还设计了变体用于错误估计。 |
| [^152] | [InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning](https://arxiv.org/abs/2404.00228) | InfLoRA提出了一种新的PEFT方法，名为无干扰低秩自适应（InfLoRA），用于持续学习，旨在消除新任务对旧任务的干扰，帮助模型在稳定性和可塑性之间取得良好平衡。 |
| [^153] | [Heterogeneous Contrastive Learning for Foundation Models and Beyond](https://arxiv.org/abs/2404.00225) | 对比自监督学习为基础模型提供了泛化能力，本调查对基于对比学习的异构学习方法进行了评估，突出了挑战和未来趋势。 |
| [^154] | [Partially-Observable Sequential Change-Point Detection for Autocorrelated Data via Upper Confidence Region](https://arxiv.org/abs/2404.00220) | 提出了一种适用于可部分观测多传感器序贯变点检测的自适应上置信区间状态空间模型（AUCRSS），通过自适应采样策略实现高效的变点检测和定位。 |
| [^155] | [Functional-Edged Network Modeling](https://arxiv.org/abs/2404.00218) | 本研究提出了一种功能边缘网络模型，通过将边视为功能数据，并引入额外维度来表示函数，使用Tucker功能分解处理功能邻接张量，进行模型推断以解决不规则观测问题，并通过正则化使基础矩阵对称化，最终展示了模型的理想属性。 |
| [^156] | [Multi-Conditional Ranking with Large Language Models](https://arxiv.org/abs/2404.00211) | 该论文提出了一种新颖的分解推理方法(MCRank)，用于解决大型语言模型在多条件排序任务中性能下降的问题。 |
| [^157] | [Causal Inference for Human-Language Model Collaboration](https://arxiv.org/abs/2404.00207) | 本文研究了人类和语言模型之间的协作动态，并提出了一个新的因果估计Incremental Stylistic Effect来解释如何改变合作结果。 |
| [^158] | [A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights](https://arxiv.org/abs/2404.00204) | 该项目将非线性深度强化学习（DRL）代理引入无人机控制中，取代传统线性PID控制器，实现了无缝过渡、提高响应速度和稳定性，同时结合PPO策略训练DRL代理，并利用高精度跟踪系统提高自主飞行精度。 |
| [^159] | [Multiple-policy Evaluation via Density Estimation](https://arxiv.org/abs/2404.00195) | 该研究提出一种名为 $\mathrm{CAESAR}$ 的算法，通过计算一个近似的最优离线采样分布，同时估计多个策略的价值，以解决多策略评估问题。 |
| [^160] | [Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries in Satellite Images with Limited Labels](https://arxiv.org/abs/2404.00179) | 提出了一种多区域迁移学习的方法，用于在卫星图像中分割农田边界，解决了标记数据不足的问题。 |
| [^161] | [Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues](https://arxiv.org/abs/2404.00178) | 该研究提出了一种利用预测性和处方性分析的数据驱动模型，为结束现有体育赛季提供了一种新的两阶段方法，以产生类似完整赛季结果的队伍排名。 |
| [^162] | [Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells](https://arxiv.org/abs/2404.00173) | 该研究通过超优化的机器学习模型，成功预测有机太阳能电池效率退化，准确度高且具有实用价值。 |
| [^163] | [Universal Bovine Identification via Depth Data and Deep Metric Learning](https://arxiv.org/abs/2404.00172) | 该论文提出了一种利用深度数据和深度度量学习进行通用牛标识的方法，可以在不需要物种特定的外套图案或特写口吻印记的情况下，通过CNN和MLP基础实现学习到良好泛化嵌入空间，从体形区分个体。 |
| [^164] | [Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education](https://arxiv.org/abs/2404.00165) | 本研究探讨了个人对体验的开放性这一人格维度是否可以通过个体的谷歌搜索历史预测，并通过相似性特征基于个人文本语料库来解释35%的开放性方差。 |
| [^165] | [Modeling Large-Scale Walking and Cycling Networks: A Machine Learning Approach Using Mobile Phone and Crowdsourced Data](https://arxiv.org/abs/2404.00162) | 该研究使用机器学习方法结合手机和众包数据，建立了一个模型来估算大规模区域网络中的步行和骑行量，讨论了在模型训练、测试和推断中面临的挑战和限制。 |
| [^166] | [Fully Zeroth-Order Bilevel Programming via Gaussian Smoothing](https://arxiv.org/abs/2404.00158) | 通过高斯平滑估计函数的一阶和二阶偏导数，用于解决双层优化问题的全零阶随机逼近算法，并建立了其非渐近收敛分析。 |
| [^167] | [Verifying the Selected Completely at Random Assumption in Positive-Unlabeled Learning](https://arxiv.org/abs/2404.00145) | 在正-无监督学习中，研究了验证完全随机选择假设（SCAR）和更为现实的随机选择假设（SAR）对算法复杂性和速度的影响。 |
| [^168] | [Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks](https://arxiv.org/abs/2404.00140) | 传统的扰动方法Shapley值和LIME可实现更高的信实性和可信度，建议优化可解释性算法以实现高效的双重目标 |
| [^169] | [Budget-aware Query Tuning: An AutoML Perspective](https://arxiv.org/abs/2404.00137) | 将传统成本单位视为变量，通过调整值可以获得比默认查询计划更优的查询计划 |
| [^170] | [FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures](https://arxiv.org/abs/2404.00130) | FISBe提供了一个真实世界基准数据集，用于长程细丝状结构的对象分割，有助于解决神经科学中对神经元进行实例分割时面临的挑战。 |
| [^171] | [PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks](https://arxiv.org/abs/2404.00103) | 本文提出了一个ACEv2的扩展版本，与量化模型的推理成本和在ML硬件上的能耗更匹配，同时引入了PikeLPN模型，通过将量化应用于逐元素操作和乘累积操作，解决了低精度神经网络中被忽视的低效问题。 |
| [^172] | [Bayesian Nonparametrics: An Alternative to Deep Learning](https://arxiv.org/abs/2404.00085) | 贝叶斯非参数模型为统计模型选择提供了灵活而强大的框架，揭示了贝叶斯非参数方法的多才多艺和高效性，为在各个学科领域中应对复杂挑战提供了创新解决方案。 |
| [^173] | [Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines](https://arxiv.org/abs/2404.00082) | 通过可学习延迟线实现可微分反馈延迟网络的参数优化，实现了对室内声学特性的数据驱动建模。 |
| [^174] | [Molecular Generative Adversarial Network with Multi-Property Optimization](https://arxiv.org/abs/2404.00081) | 该研究引入了一种新型的基于演员-评论家强化学习的GAN，即InstGAN，以在令牌级别上生成具有多属性优化的分子，并利用最大化信息熵来缓解模式崩溃。 |
| [^175] | [A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks](https://arxiv.org/abs/2404.00076) | 提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。 |
| [^176] | [BEACON: Bayesian Experimental design Acceleration with Conditional Normalizing flows $-$ a case study in optimal monitor well placement for CO$_2$ sequestration](https://arxiv.org/abs/2404.00075) | 该论文提出了一种在CO$_2$封存项目中采用贝叶斯框架下的优化实验设计方法，通过整合流体流动求解器和生成式神经网络，以确保在有限预算内布置最佳的监测井位。 |
| [^177] | [A finite operator learning technique for mapping the elastic properties of microstructures to their mechanical deformations](https://arxiv.org/abs/2404.00074) | 引入一种有限算子学习技术，通过学习参数化解决了微结构弹性属性映射到机械变形的问题，在计算成本和准确性方面优于传统方法，并能处理具有明显不连续性的解。 |
| [^178] | [A Two-Phase Recall-and-Select Framework for Fast Model Selection](https://arxiv.org/abs/2404.00069) | 通过两阶段（粗略召回和精细选择）模型选择框架，本文旨在提高选择稳健模型的效率，通过利用模型在基准数据集上的训练性能，聚类展示相似训练性能的模型 |
| [^179] | [A Data-Driven Predictive Analysis on Cyber Security Threats with Key Risk Factors](https://arxiv.org/abs/2404.00068) | 该研究提出了一种基于机器学习的模型，使用社会经济因素来预测可能成为网络攻击受害者的个人，并展示了对该模型的数据收集和处理方法。 |
| [^180] | [Temporal Graph Networks for Graph Anomaly Detection in Financial Networks](https://arxiv.org/abs/2404.00060) | 时间图网络（TGN）在金融网络中的异常检测中表现出显著优势，适应了现代金融系统动态和复杂的特性。 |
| [^181] | [Fingerprinting web servers through Transformer-encoded HTTP response headers](https://arxiv.org/abs/2404.00056) | 运用深度学习和自然语言处理技术，通过对HTTP响应头的编码识别Web服务器，可以在提高准确性和特异性的基础上实现对易受攻击的Web服务器版本的检测。 |
| [^182] | [Choreographing the Digital Canvas: A Machine Learning Approach to Artistic Performance](https://arxiv.org/abs/2404.00054) | 这项研究提出了一种基于属性描述的设计工具，结合机器学习模型，用于生成和可视化艺术运动，其中关键创新在于循环属性条件变分自动编码器模型能够捕捉和生成逼真的3D人体动作，分别学习倒下动作的不同阶段。 |
| [^183] | [Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2404.00051) | 提出了ChapTER，一个对比历史建模框架，通过前缀调整实现文本与时间的平衡，用于时间知识图推理。 |
| [^184] | [Grappa -- A Machine Learned Molecular Mechanics Force Field](https://arxiv.org/abs/2404.00050) | 提出了一个用于预测分子力学参数的机器学习架构Grappa，通过图注意力神经网络和transformer提高了准确性，减少了计算开销，能在现有MD引擎中使用。 |
| [^185] | [SLIMBRAIN: Augmented Reality Real-Time Acquisition and Processing System For Hyperspectral Classification Mapping with Depth Information for In-Vivo Surgical Procedures](https://arxiv.org/abs/2404.00048) | SLIMBRAIN是一种实时获取和处理AR系统，用于在体内手术过程中从高光谱信息中分类和显示脑肿瘤组织，并结合RGB点云进行AR可视化。 |
| [^186] | [Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games](https://arxiv.org/abs/2404.00045) | 引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡产生影响，证明了NE符合线性高斯策略，并提出了政策优化算法以及增强技术来找到游戏内的NE。 |
| [^187] | [UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment](https://arxiv.org/abs/2404.00044) | 本文提出了UAlign，一种无模板化的图到序列的逆合成预测方法，通过结合图神经网络和Transformer，利用分子的固有图结构，并引入一种简单有效的SMILES对齐技术来促进未改变结构的复用。 |
| [^188] | [Improve accessibility for Low Vision and Blind people using Machine Learning and Computer Vision](https://arxiv.org/abs/2404.00043) | 该项目利用机器学习和计算机视觉开发移动应用，帮助盲人通过音频和触觉反馈实时定位周围环境，具有扫描文本和朗读、检测物体等功能。 |
| [^189] | [Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis](https://arxiv.org/abs/2404.00042) | 该论文研究了具有凸约束的随机凸优化问题，提出了一种非渐近保证的VRPG算法，并展示了其性能受到解以及带凸约束解决的问题的缩放距离控制。 |
| [^190] | [MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing Algorithms for TinyML systems](https://arxiv.org/abs/2404.00039) | 提出了MicroHD，一个新颖的面向TinyML系统的精度驱动超高维计算优化方法 |
| [^191] | [Investigating Similarities Across Decentralized Financial (DeFi) Services](https://arxiv.org/abs/2404.00034) | 该研究通过采用图表示学习算法，提出了一种方法来研究去中心化金融协议提供的服务之间的相似性，并成功将这些服务分组为具有相似功能的集群。 |
| [^192] | [Towards gaze-independent c-VEP BCI: A pilot study](https://arxiv.org/abs/2404.00031) | 这项试验性研究首次尝试朝向无需凝视的脑机接口拼写器，通过使用空间注意力而非眼球移动来解码视觉刺激，取得了很高的分类准确率。 |
| [^193] | [Visualization of Unstructured Sports Data -- An Example of Cricket Short Text Commentary](https://arxiv.org/abs/2404.00030) | 使用板球短文本评论数据进行可视化，包括构建球员的实力规则和弱点规则，并展示具有类似规则的球员。 |
| [^194] | [LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning](https://arxiv.org/abs/2404.00027) | 探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。 |
| [^195] | [Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs](https://arxiv.org/abs/2404.00026) | 研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。 |
| [^196] | [Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review and Research Roadmap](https://arxiv.org/abs/2404.00019) | 本研究综述了现有自动驾驶车辆解释方法的不确定性，提出了一个全面的未来研究路线图，重点放在了了解交流对象和生成及时解释上。 |
| [^197] | [SOMson -- Sonification of Multidimensional Data in Kohonen Maps](https://arxiv.org/abs/2404.00016) | SOMson提出了一种交互式音频化技术，用于增强Kohonen地图下数据的信息量，解决SOM在提供整体图片时的缺陷。 |
| [^198] | [Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning](https://arxiv.org/abs/2404.00015) | 提出了一种名为Systemic Quantum Score (SQS)的新方法，展示在金融领域生产级应用案例中相比纯经典模型更有优势，能够从较少数据点中提取模式并表现出更好性能。 |
| [^199] | [Missing Data Imputation With Granular Semantics and AI-driven Pipeline for Bankruptcy Prediction](https://arxiv.org/abs/2404.00013) | 本文介绍了一种具有精确语义的缺失数据插补方法，通过在粒空间中利用特征语义和可靠观测来预测缺失值，从而解决了破产预测中的重要挑战。 |
| [^200] | [FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation](https://arxiv.org/abs/2403.20261) | FABind+通过改进口袋预测和姿态生成，提升分子对接表现 |
| [^201] | [DiJiang: Efficient Large Language Models through Compact Kernelization](https://arxiv.org/abs/2403.19928) | DiJiang提出了一种新颖的频域核方法，可以将预训练的基本Transformer模型转化为具有线性复杂度的模型，大大减少训练成本，并在理论上提供更好的逼近效率。 |
| [^202] | [NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification](https://arxiv.org/abs/2403.19713) | 该研究提出了在TRAC-2024离线危害潜在性识别任务中的方法，利用专家标注的社交媒体评论数据集，成功设计了能够准确评估危害可能性并识别目标的算法，在两个赛道中取得第二名的成绩。 |
| [^203] | [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](https://arxiv.org/abs/2403.19647) | 该论文介绍了一种新方法，即稀疏特征电路，可以在语言模型中发现和编辑可解释的因果图，为我们提供了对未预料机制的详细理解和包含了用于提高分类器泛化能力的SHIFT方法。 |
| [^204] | [Self-Improved Learning for Scalable Neural Combinatorial Optimization](https://arxiv.org/abs/2403.19561) | 提出一种新颖的自我改进学习(SIL)方法，实现神经组合优化的更好可扩展性，通过自身生成解决方案作为伪标签，设计线性复杂度的注意机制来处理大规模组合优化问题实例。 |
| [^205] | [Evaluating Fair Feature Selection in Machine Learning for Healthcare](https://arxiv.org/abs/2403.19165) | 通过考虑对所有人口统计群体均等重要性的公平特征选择方法，在医疗保健领域评估算法公平性，确保在减少偏见和全局分类错误之间实现平衡。 |
| [^206] | [Tiny Machine Learning: Progress and Futures](https://arxiv.org/abs/2403.19076) | TinyML是一种将深度学习模型压缩到物联网设备和微控制器中实现无处不在智能的新方法，需要共同设计算法和系统堆栈以克服硬件限制。 |
| [^207] | [Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems](https://arxiv.org/abs/2403.18998) | 提出了针对微服务系统的少样本异常跟踪分类的新框架，利用多头注意力自编码器构建系统特定的跟踪表示，并应用基于Transformer编码器的模型无关元学习进行高效分类。 |
| [^208] | [Deep Learning for Traffic Flow Prediction using Cellular Automata-based Model and CNN-LSTM architecture](https://arxiv.org/abs/2403.18710) | 该论文提出使用基于元胞自动机模型和CNN-LSTM架构的深度学习方法成功预测交通流，充分利用交通流动态领域知识。 |
| [^209] | [Safe and Robust Reinforcement-Learning: Principles and Practice](https://arxiv.org/abs/2403.18539) | 本文通过对安全和稳健RL领域的探索，识别并进一步理解了在实际场景中部署RL系统面临的重大挑战，提出了不同算法方法的综合评述以增强RL代理的安全性和稳健性。 |
| [^210] | [Bidirectional Consistency Models](https://arxiv.org/abs/2403.18035) | 提出了双向一致性模型（BCM），学习一个神经网络，能够实现沿着概率流常微分方程前向和后向遍历，从而有效地统一了生成和编辑图像等任务。 |
| [^211] | [Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation](https://arxiv.org/abs/2403.17701) | 本文提出了Triplet Mamba-UNet，利用残余VSS块提取密集上下文特征，并利用Triplet SSM融合空间和通道维度上的特征。 |
| [^212] | [A Survey on Deep Learning and State-of-the-arts Applications](https://arxiv.org/abs/2403.17561) | 深度学习是解决复杂问题的强大工具，本研究旨在全面审视深度学习模型及其应用的最新发展 |
| [^213] | [Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens](https://arxiv.org/abs/2403.17407) | 通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。 |
| [^214] | [A Transformer approach for Electricity Price Forecasting](https://arxiv.org/abs/2403.16108) | 这种独特的Transformer模型在电力价格预测中取得了更好的表现，为可靠和可持续的电力系统运行提供了有前景的解决方案。 |
| [^215] | [Understanding Emergent Abilities of Language Models from the Loss Perspective](https://arxiv.org/abs/2403.15796) | 本文从损失角度重新定义了语言模型的突现能力，发现具有相同预训练损失的模型在不同任务上表现相似，而当预训练损失低于特定阈值时，模型将展现出突现能力。 |
| [^216] | [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638) | 提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。 |
| [^217] | [Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images](https://arxiv.org/abs/2403.15443) | 通过分析PET扫描图像，引入了一种集成方法早期检测阿尔茨海默病，并且在分类阿尔茨海默病时使用了多种深度学习和传统机器学习模型。 |
| [^218] | [Understanding the Transit Gap: A Comparative Study of On-Demand Bus Services and Urban Climate Resilience in South End, Charlotte, NC and Avondale, Chattanooga, TN](https://arxiv.org/abs/2403.14671) | 本研究揭示了城市设计在公共交通效率和减少碳排放方面的关键作用，指出城市布局对公共交通结果有重要影响，提出了针对不同城市设计元素的定制策略对于气候适应至关重要。 |
| [^219] | [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/abs/2403.14608) | 大型模型参数高效微调（PEFT）是通过调整预训练模型的参数，以适应特定任务，并减少引入的附加参数或计算资源数量的实用解决方案。 |
| [^220] | [ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training](https://arxiv.org/abs/2403.14589) | 提出了A$^3$T框架，通过ActRe提示代理实现了ReAct风格代理对代理轨迹的自主标注，同时增强了新的轨迹合成能力。 |
| [^221] | [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://arxiv.org/abs/2403.14119) | 本文研究了在测试时提示调整过程中通过利用CLIP的固有属性来探讨校准的方法，发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。 |
| [^222] | [ZigMa: Zigzag Mamba Diffusion Model](https://arxiv.org/abs/2403.13802) | 本研究提出了一种名为Zigzag Mamba的零参数方法，通过纠正当前Mamba-based视觉方法中对空间连续性的忽视，实现了更好的速度和内存利用，同时在大分辨率视觉数据集上展示了出色的性能。 |
| [^223] | [Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression](https://arxiv.org/abs/2403.13300) | 通过核包技术证明反向拟合的收敛速度，并提出了核多重网格算法，通过稀疏高斯过程回归增强反向拟合，适用于结构化和分散数据的加性GPs。 |
| [^224] | [STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model](https://arxiv.org/abs/2403.12418) | STG-Mamba 是首个利用选择性状态空间模型进行时空图学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。 |
| [^225] | [Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders](https://arxiv.org/abs/2403.10800) | 模型重新编程方法 Reprogrammer 在文本图像编码器中的应用优于传统微调方法，能够提高下游模型在分布内和分布外数据中的性能表现 |
| [^226] | [A Conceptual Framework For White Box Neural Networks](https://arxiv.org/abs/2403.09863) | 引入语义特征作为通用概念框架，实现了完全可解释的神经网络层，为白盒神经网络的范式转变开辟了新的可能性。 |
| [^227] | [Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition](https://arxiv.org/abs/2403.07953) | 本文提出了通过结构化分解张量进一步抽象稀疏DNN加速的方法，实现了将稀疏张量转换成一系列结构化稀疏张量，从而弥合了稀疏DNN模型和硬件之间的差距。 |
| [^228] | [SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression](https://arxiv.org/abs/2403.07378) | SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。 |
| [^229] | [The Hidden Attention of Mamba Models](https://arxiv.org/abs/2403.01590) | Mamba模型可以被视为关注驱动的模型，这与变压器中的自注意力层有所不同，并且通过可解释性方法可以深入了解其内部工作。 |
| [^230] | [Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks](https://arxiv.org/abs/2403.00344) | 提出了一个框架，通过训练适应多样化护理接收者响应的鲁棒护理人员政策，以提高多智能体强化学习中政策的鲁棒性。 |
| [^231] | [Q-FOX Learning: Breaking Tradition in Reinforcement Learning](https://arxiv.org/abs/2402.16562) | Q-FOX学习是一种新颖的自动超参数调整方法，结合了FOX优化器和Q-learning算法，提出了使用新的目标函数来解决强化学习中超参数调整的问题。 |
| [^232] | [A Provably Accurate Randomized Sampling Algorithm for Logistic Regression](https://arxiv.org/abs/2402.16326) | 提出了一种逻辑回归问题的简单随机抽样算法，通过随机矩阵乘法实现高质量逼近估计概率和模型整体差异性。 |
| [^233] | [ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters](https://arxiv.org/abs/2402.15733) | 该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。 |
| [^234] | [Learning the Topology and Behavior of Discrete Dynamical Systems](https://arxiv.org/abs/2402.11686) | 本文研究了学习黑盒离散动力系统的行为和底层拓扑结构的问题，证明了这是一个计算上难以解决的问题，并提出了在某些条件下的高效学习方法。 |
| [^235] | [RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models](https://arxiv.org/abs/2402.10038) | 本研究提出了一种名为RS-DPO的方法，它将拒绝采样和直接优化偏好结合起来，用于对齐大型语言模型。通过开发一个经过监督微调的策略模型，并从该模型中直接采样响应，RS-DPO能够有效解决基于近端策略优化的不稳定性和高计算成本的问题。通过识别对比样本对，RS-DPO能够更好地进行RLHF。 |
| [^236] | [Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards](https://arxiv.org/abs/2402.09075) | 该论文研究提出了一种使用积分项补偿二次奖励函数稳态误差的方法，通过增强长期奖励的考虑，有效降低了系统性能（如自适应巡航控制和变道模型）中的稳态误差问题。 |
| [^237] | [Nesting Particle Filters for Experimental Design in Dynamical Systems](https://arxiv.org/abs/2402.07868) | 本文提出了一种新颖的方法来解决动态系统中的贝叶斯实验设计问题，利用嵌套粒子滤波器和立体蒙特卡洛方法来进行基于梯度的策略优化，相比于其他方法具有更好的性能。 |
| [^238] | [Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA](https://arxiv.org/abs/2402.07710) | 本论文研究了在3D点云上进行稀疏卷积的GPU优化方法，以解决点云的稀疏性和计算问题。 |
| [^239] | [A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents](https://arxiv.org/abs/2402.07365) | 本文提出了一种基于深度学习的方法，用于相对绩效标准下的最优投资。该方法利用前向-后向随机微分方程的纳什均衡特征和随机微分游戏的机器学习算法的最新进展。数值实验在两个不同的金融模型上进行，比较了不同交互结构的图形图的影响。 |
| [^240] | [X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design](https://arxiv.org/abs/2402.07148) | X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。 |
| [^241] | [TinyLLM: Learning a Small Student from Multiple Large Language Models](https://arxiv.org/abs/2402.04616) | TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。 |
| [^242] | [Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction](https://arxiv.org/abs/2402.04154) | 本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示) |
| [^243] | [Causal Bayesian Optimization via Exogenous Distribution Learning](https://arxiv.org/abs/2402.02277) | 本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。 |
| [^244] | [Explaining latent representations of generative models with large multimodal models](https://arxiv.org/abs/2402.01858) | 该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。 |
| [^245] | [Simple Policy Optimization](https://arxiv.org/abs/2401.16025) | SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。 |
| [^246] | [Faster ISNet for Background Bias Mitigation on Deep Neural Networks](https://arxiv.org/abs/2401.08409) | 提出了一种用于缓解深度神经网络中背景偏见的更快ISNet，通过重新构建架构以减少训练时间并引入一种模型无关的LRP实现，成功阻止了背景注意力和快捷学习，优于多个最新模型。 |
| [^247] | [Causal State Distillation for Explainable Reinforcement Learning](https://arxiv.org/abs/2401.00104) | 本文提出了一种因果状态精炼的方法，通过揭示在训练过程中影响智能体目标的奖励的各个方面，解决了强化学习模型不透明性的问题。 |
| [^248] | [Metalearning with Very Few Samples Per Task](https://arxiv.org/abs/2312.13978) | 在针对每个任务样本数量很少的情况下，研究了如何通过共享表示进行元学习。 |
| [^249] | [Near-Optimal Resilient Aggregation Rules for Distributed Learning Using 1-Center and 1-Mean Clustering with Outliers](https://arxiv.org/abs/2312.12835) | 本文研究了在分布式学习中使用聚类方法处理异常值的近似最优鲁棒聚合规则，通过恒定近似1中心和1均值聚类问题解决方案，提供了重要的度量标准优化方法 |
| [^250] | [A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors](https://arxiv.org/abs/2312.11517) | 本研究利用自然语言处理和基于模式的排名方法对肌肉骨骼疾病的风险因素进行了分类和排名，以提高对其理解、分类和优先考虑预防和治疗的能力。 |
| [^251] | [ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity](https://arxiv.org/abs/2312.11511) | ComplexityNet通过学习任务复杂性，提高了LLM推理效率，通过预测任务的准确输出概率，成功降低了90%的计算资源使用，并在任务复杂性确定方面取得了79%准确率。 |
| [^252] | [Collaborating Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2312.09788) | 提出了一种协作基础模型框架（CLOUDS）用于领域泛化语义分割，集成了CLIP骨干、生成模型和“Segment Anything Model”，以实现对各种目标分布模式的覆盖和预测改进。 |
| [^253] | [Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft](https://arxiv.org/abs/2312.09238) | 本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型自动设计稠密奖励函数来提高学习效率 |
| [^254] | [OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods](https://arxiv.org/abs/2312.08255) | 该研究介绍了一个名为OCTDL的开放获取光学相干断层扫描数据集，包括超过2000张标记有疾病组和视网膜病理的OCT图像，有助于诊断眼部状况。 |
| [^255] | [Honeybee: Locality-enhanced Projector for Multimodal LLM](https://arxiv.org/abs/2312.06742) | 该研究提出了一种既灵活又增强局部性的新型投影仪设计，有助于连接视觉编码器和语言模型，提高模型的效率和空间理解。 |
| [^256] | [All Rivers Run to the Sea: Private Learning with Asymmetric Flows](https://arxiv.org/abs/2312.05264) | 提出了一种新的私有训练和推理框架Delta，通过两个不对称数据流实现了具有可比较模型性能的隐私保护。 |
| [^257] | [Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil](https://arxiv.org/abs/2312.03041) | 基于Transformer架构的深度学习模型用于曼谷地基灌注桩的载荷变形预测，在考虑土壤剖面和桩特征的基础上，结合之前顺序数据以提高预测准确性，可满意地预测载荷-变形曲线，并可用于不同桩条件下的参数分析和设计优化。 |
| [^258] | [Improving Plasticity in Online Continual Learning via Collaborative Learning](https://arxiv.org/abs/2312.00600) | 在线持续学习中，为了提高模型的可塑性，我们提出了协作持续学习（CCL）和蒸馏链（DC）策略。 |
| [^259] | [Automating Continual Learning](https://arxiv.org/abs/2312.00276) | 提出了自动化持续学习（ACL）来训练自引用神经网络，使其元学习自身的上下文持续学习算法，并在解决“上下文灾难性遗忘”方面取得成功。 |
| [^260] | [Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing](https://arxiv.org/abs/2311.18608) | 提出了一种名为Contrastive Denoising Score (CDS) 的简单但强大的修改版本，用于潜在扩散模型 (LDM)，在DDS框架内使用了CUT损失来更好地保留原始图像的特定结构元素。 |
| [^261] | [Compositional Chain-of-Thought Prompting for Large Multimodal Models](https://arxiv.org/abs/2311.17076) | 提出了一种新颖的零样式思维提示（CCoT），以克服大型多模态模型难以捕捉到组合视觉推理方面的细节的问题。 |
| [^262] | [MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training](https://arxiv.org/abs/2311.17049) | MobileCLIP通过多模态强化训练实现了高效率图像-文本模型，在零-shot任务中取得了新的性能平衡。 |
| [^263] | [Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework](https://arxiv.org/abs/2311.13958) | 提出一种具有可学习张量核范数的新型张量恢复模型，引入交替近端乘子方法（APMM）优化算法，解决处理非光滑变化的张量数据挑战 |
| [^264] | [Discovering Effective Policies for Land-Use Planning](https://arxiv.org/abs/2311.12304) | 通过学习代理模型并使用进化搜索过程，发现了可定制到不同位置的有效土地利用政策，为土地利用规划提供了一个潜在有用的工具。 |
| [^265] | [Multimodal Representation Learning by Alternating Unimodal Adaptation](https://arxiv.org/abs/2311.10707) | 提出了一种通过交替单模态适应来进行多模态表示学习的方法，旨在最小化干扰并捕获跨模态交互 |
| [^266] | [Low-Rank Adaptation for Multilingual Summarization: An Empirical Study](https://arxiv.org/abs/2311.08572) | LoRA 在多语言摘要方面竞争力强，特别在低数据情况和跨语言转移方面表现出色。 |
| [^267] | [Evaluating Neighbor Explainability for Graph Neural Networks](https://arxiv.org/abs/2311.08118) | 评价图神经网络中邻居的可解释性，提出新的度量标准并发现基于梯度的方法在GNN领域的解释没有太大差异，同时发现很多技术在没有自环的GNNs下无法准确识别重要邻居。 |
| [^268] | [In-context Learning and Gradient Descent Revisited](https://arxiv.org/abs/2311.07772) | 本文重新审视了在现实NLP任务和模型中的上下文学习（ICL）与梯度下降（GD）之间的联系证据，找到了评估上的不足，提出了遵循层次因果关系的简单GD优化程序来改善相似性分数。 |
| [^269] | [EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road Autonomy](https://arxiv.org/abs/2311.06234) | 提出了一种统一框架，通过证据深度学习有效量化不确定性，学习不确定性感知牵引模型并规划风险感知轨迹。 |
| [^270] | [A Lightweight Architecture for Real-Time Neuronal-Spike Classification](https://arxiv.org/abs/2311.04808) | 该研究提出了一种轻量级的神经突触检测和分类架构，利用普京细胞的特征实时处理数据，实现了将数据压缩并存储在可移动装置上的目标。 |
| [^271] | [Improved weight initialization for deep and narrow feedforward neural network](https://arxiv.org/abs/2311.03733) | 本文提出了一种新颖的权重初始化方法，能有效解决在使用ReLU激活函数训练极深窄前馈网络时遇到的“死亡ReLU”问题。 |
| [^272] | [Riemannian Laplace Approximation with the Fisher Metric](https://arxiv.org/abs/2311.02766) | 黎曼拉普拉斯逼近的新方法利用Fisher度量提供更丰富的逼近族，解决了在无限数据极限下先前方法度量选择不当导致逼近过于狭窄和有偏的问题。 |
| [^273] | [Optimization of utility-based shortfall risk: A non-asymptotic viewpoint](https://arxiv.org/abs/2310.18743) | 该论文研究了基于效用的缺口风险的估计和优化问题，提出了非渐近界限来解决UBSR的估计误差和优化过程中的梯度问题。 |
| [^274] | [DistillSpec: Improving Speculative Decoding via Knowledge Distillation](https://arxiv.org/abs/2310.08461) | DistillSpec通过知识蒸馏技术改进了投机性解码，在标准基准测试中取得了10-45%的速度提升。 |
| [^275] | [Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676) | 该研究通过复合后门攻击(CBA)展示了在大型语言模型中植入多个触发关键词的方法，相较于现有方法更为隐蔽，并确保只有当所有触发关键词同时出现时后门才会被激活。 |
| [^276] | [FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics](https://arxiv.org/abs/2310.06588) | 训练动态可在不同模型大小和预训练方法之间进行转移，通过选定的训练实例微调主模型实现比经验风险最小化更高的训练效率 |
| [^277] | [Attesting Distributional Properties of Training Data for Machine Learning](https://arxiv.org/abs/2308.09552) | 提出了一种新型的财产证明概念，可以在不泄露数据的情况下向验证者展示训练数据的分布特性。 |
| [^278] | [Latent Code Augmentation Based on Stable Diffusion for Data-free Substitute Attacks](https://arxiv.org/abs/2307.12872) | 提出了一种基于稳定扩散的新型无数据替代攻击方案，引入潜在编码增强（LCA）来提高替代训练的效率和准确性 |
| [^279] | [DANSE: Data-driven Non-linear State Estimation of Model-free Process in Unsupervised Learning Setup](https://arxiv.org/abs/2306.03897) | 在无监督学习设置中，提出了一种名为DANSE的基于数据驱动的非线性状态估计方法，利用数据驱动的循环神经网络捕捉模型无关过程中的潜在非线性动态。 |
| [^280] | [Cross-Modal Entity Matching for Visually Rich Documents](https://arxiv.org/abs/2303.00720) | 提出了一个跨模态实体匹配框架 Juno，通过深度神经网络和注意力机制，实现了文档中文本跨度与外部数据库中语义相似元组的匹配，从而解决了视觉丰富文档信息检索中缺乏上下文和见解的问题。 |
| [^281] | [Gauss-Newton Temporal Difference Learning with Nonlinear Function Approximation](https://arxiv.org/abs/2302.13087) | 提出了一种在非线性函数逼近下的高斯-牛顿时差学习方法，能够显著改进对全局最优Q函数的样本复杂度。 |
| [^282] | [Semantic-Fused Multi-Granularity Cross-City Traffic Prediction](https://arxiv.org/abs/2302.11774) | 提出了一种语义融合的多粒度迁移学习模型，能够在具有不同粒度的城市之间实现交通预测知识转移 |
| [^283] | [Towards Universal Fake Image Detectors that Generalize Across Generative Models](https://arxiv.org/abs/2302.10174) | 通过在不进行训练的情况下执行真伪图像分类，使用非明确区分特征空间，可以更好地识别不同生成模型来源的图像。 |
| [^284] | [Similarity, Compression and Local Steps: Three Pillars of Efficient Communications for Distributed Variational Inequalities](https://arxiv.org/abs/2302.07615) | 相似性、压缩和局部更新是本文提出的三大技术，用于减少分布式变分不等式问题中通信轮次和成本，实现了前所未有的三重协同作用。 |
| [^285] | [BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations](https://arxiv.org/abs/2301.13418) | 本文提出了一种中间解决方案，即将训练构建为弱监督和半监督学习问题，以解决通过不完整注释数据检测恶性乳腺病变的困境 |
| [^286] | [An Analysis of Attention via the Lens of Exchangeability and Latent Variable Models](https://arxiv.org/abs/2212.14852) | 通过研究可交换性和潜变量模型，我们建立了对注意力机制工作原理的理论分析，从而揭示了transformers如何实现对长序列进行关系推断以产生理想表示的方法。 |
| [^287] | [Time-aware Metapath Feature Augmentation for Ponzi Detection in Ethereum](https://arxiv.org/abs/2210.16863) | 该研究介绍了一种用于以太坊Ponzi检测的时序元路径特征增强方法，旨在捕获交易模式信息中的时间依赖性。 |
| [^288] | [G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System](https://arxiv.org/abs/2210.09846) | 通过结合周期激活函数的启发和使用隐马尔可夫模型（HMMs）和强化学习（RL）进行合成轨迹数据增强，我们的方法G-PECNet在行人轨迹预测任务中取得了9.5%的最终位移误差（FDE）改进。 |
| [^289] | [Toward the application of XAI methods in EEG-based systems](https://arxiv.org/abs/2210.06554) | 本文旨在将合适的可解释人工智能（XAI）方法应用于解决脑-计算机接口（BCI）中的数据集转移问题，以提高在情绪识别中的EEG信号分类系统的泛化性能。 |
| [^290] | [Exploring Adaptive MCTS with TD Learning in miniXCOM](https://arxiv.org/abs/2210.05014) | 在miniXCOM中，作者提出了一种改进的自适应MCTS算法MCTS-TD，不需要预训练即可实现在线适应性，展示了其在游戏中表现优异的结果。 |
| [^291] | [Identifiable Latent Causal Content for Domain Adaptation under Latent Covariate Shift](https://arxiv.org/abs/2208.14161) | 提出了一种新的隐含协变量转移（LCS）范式，增加了领域间的可变性和适应性，并提供了恢复标签变量潜在原因的理论保证。 |
| [^292] | [Visually Evaluating Generative Adversarial Networks Using Itself under Multivariate Time Series](https://arxiv.org/abs/2208.02649) | 提出了一种名为高斯GANs的通用框架，用于通过在多变量时间序列生成任务中使用GANs自身进行可视化评估，并提出了使用卡方分布的有效可视化方法。 |
| [^293] | [Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation](https://arxiv.org/abs/2207.14000) | 提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。 |
| [^294] | [Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification](https://arxiv.org/abs/2207.09237) | 提出了一种基于半监督学习的预测聚类树的（分层）多标签分类方法。 |
| [^295] | [Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency](https://arxiv.org/abs/2205.13476) | 论文提出了一种名为Embed to Control（ETC）的强化学习算法，通过在两个级别学习表示的方法来解决部分观察马尔可夫决策问题，以实现样本高效利用。 |
| [^296] | [ENS-t-SNE: Embedding Neighborhoods Simultaneously t-SNE](https://arxiv.org/abs/2205.11720) | ENS-t-SNE算法推广了t-SNE方法，通过在3D嵌入中使用不同视角，可以同时可视化高维数据集中的不同类型聚类。 |
| [^297] | [Aligning Logits Generatively for Principled Black-Box Knowledge Distillation](https://arxiv.org/abs/2205.10490) | 本文提出了一个新的黑盒知识蒸馏方法MEKD，通过将教师和学生模型的低维度对数对齐，实现将一个繁琐模型压缩成轻量级模型。 |
| [^298] | [Reinforcement Learning from Partial Observation: Linear Function Approximation with Provable Sample Efficiency](https://arxiv.org/abs/2204.09787) | 该研究提出了一种基于线性函数逼近的部分观测强化学习算法（OP-TENET），在有限的情节数内实现了$\epsilon$-最优策略，样本复杂度与线性结构的本征维度多项式缩放，与观测和状态空间的大小无关。 |
| [^299] | [Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions](https://arxiv.org/abs/2202.13046) | 通过利用图结构，本文提出了一种通用计算高效的分布式框架，基于局部值函数的分布式RL方法在协作多智能体强化学习中取得了显著的样本复杂性降低。 |
| [^300] | [Communication-Efficient Federated Learning with Accelerated Client Gradient](https://arxiv.org/abs/2201.03172) | 通过使用具有前瞻梯度的全局模型，并通过与超调全局模型对齐来规范本地更新，提出了一种简单但有效的联邦学习框架，以改善参与者之间的一致性，促进服务器模型的收敛。 |
| [^301] | [Wasserstein Flow Meets Replicator Dynamics: A Mean-Field Analysis of Representation Learning in Actor-Critic](https://arxiv.org/abs/2112.13530) | 本文通过均场分析研究了基于特征的神经AC的演化和收敛，提出了一个使用两个学习率更新的AC版本，其中评论家通过大步长进行TD学习更新，演员通过小步长进行PPO更新。 |
| [^302] | [CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations](https://arxiv.org/abs/2111.03262) | 提出了一种新颖的Collaborative Graph Contrastive Learning框架（CGCL），利用多个图编码器观察图形，并避免引入不稳定扰动，保证图的不变性。 |
| [^303] | [Valid prediction intervals for regression problems](https://arxiv.org/abs/2107.00363) | 本文回顾了回归问题中四类预测区间估计方法，并指出了一些方法在不同数据集上性能波动大的原因。 |
| [^304] | [GraphFM: Graph Factorization Machines for Feature Interaction Modeling](https://arxiv.org/abs/2105.11866) | 提出了一种名为GraphFM的图因子分解机方法，通过图结构自然表示特征，并将FM的交互功能集成到GNN的特征聚合策略中，能够模拟任意阶特征交互。 |
| [^305] | [MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control](https://arxiv.org/abs/2101.00746) | 提出了一种新颖的MetaVIM强化学习方法，用于分散式交通信号控制，通过元学习考虑邻居信息，以解决现实世界中交通信号控制面临的挑战。 |
| [^306] | [Variational Transport: A Convergent Particle-BasedAlgorithm for Distributional Optimization](https://arxiv.org/abs/2012.11554) | 提出了一种新的基于粒子的算法，名为变分传输，通过迭代地推动一组粒子，在概率分布流形上近似执行沃瑟斯坦梯度下降。 |
| [^307] | [Can Temporal-Difference and Q-Learning Learn Representation? A Mean-Field Theory](https://arxiv.org/abs/2006.04761) | 研究探讨离散时间差分学习和Q学习在深度强化学习中的特征表示演变，证明利用过度参数化的方法可以实现这种演变，并关注特征表示对于算法收敛的重要性。 |
| [^308] | [Provably Efficient Exploration in Policy Optimization](https://arxiv.org/abs/1912.05830) | OPPO是第一个在探索中高效的策略优化算法，在处理具有线性函数近似、未知转移和对抗性奖励的问题中取得了 $\tilde{O}(\sqrt{d^2 H^3 T} )$ 的遗憾。 |
| [^309] | [Deep Semantic Segmentation of Natural and Medical Images: A Review](https://arxiv.org/abs/1910.07655) | 对于自然和医学图像的深度语义分割任务，本综述将深度学习模型分为六大类别，全面审查了各组方法的贡献并讨论了当前方法的局限性和未来研究方向。 |
| [^310] | [Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning.](http://arxiv.org/abs/2401.15043) | 该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。 |
| [^311] | [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts.](http://arxiv.org/abs/2401.14295) | 这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。 |
| [^312] | [TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation.](http://arxiv.org/abs/2401.12987) | TelME是一种教师导向的多模融合网络，通过跨模态知识蒸馏实现对话中情绪识别的优化，取得了在多说话人数据集MELD上的最先进性能。 |
| [^313] | [The twin peaks of learning neural networks.](http://arxiv.org/abs/2401.12610) | 该论文研究了神经网络的双峰现象，发现高度过参数化的模型可以避免过拟合并实现良好的测试性能，与传统的偏差-方差折衷法则不同。研究分析了布尔均值维度（BMD）与网络复杂性和敏感性之间的关系，得到了在高维度范围内BMD的可解释表达式，发现BMD在网络过参数化程度增加时达到极值点。 |
| [^314] | [GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?.](http://arxiv.org/abs/2401.11748) | 本文提出了一种新颖的梯度反转攻击方法GI-PIP，不需要依赖不切实际的辅助数据集，通过利用异常检测模型从较少的数据中捕获底层分布，并能在图像恢复方面表现出优异的能力，同时在分布泛化方面也更强大。 |
| [^315] | [Quantum Machine Learning: from NISQ to Fault Tolerance.](http://arxiv.org/abs/2401.11351) | 本文提供了对量子机器学习领域的全面回顾，涵盖了在NISQ技术和容错量子计算硬件上使用的技术和算法，并深入讨论了与量子机器学习相关的基本概念和统计学习理论。 |
| [^316] | [SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI.](http://arxiv.org/abs/2401.09627) | SymTC是一种新颖的腰椎MR图像分割模型，通过将Transformer和CNN相结合，并利用位置嵌入和自注意力模块，实现了更准确的实例分割。 |
| [^317] | [Fun with Flags: Robust Principal Directions via Flag Manifolds.](http://arxiv.org/abs/2401.04071) | 本研究提出了一种统一的PCA和其变种框架，该框架基于线性子空间旗帜，并引入了对异常值和数据流形的考虑。通过在旗帜流形上进行优化问题的求解，结合主测地线近似，提出了一系列新的降维算法。 |
| [^318] | [Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition.](http://arxiv.org/abs/2401.01482) | 本文研究了在目标识别中将地理多样知识融入提示以提高地理鲁棒性的方法，探索了通过大型语言模型获取地理特定对象知识并结合CLIP视觉语言模型的零样本和可学习软提示。通过提出一种地理知识正则化方法，实现了从源地理位置推广到未见目标地理位置的鲁棒性提升。 |
| [^319] | [Taming Mode Collapse in Score Distillation for Text-to-3D Generation.](http://arxiv.org/abs/2401.00909) | 本文揭示了现有的基于Score Distillation的文本到3D生成框架存在模式崩溃问题，通过在变分目标中加入熵项来改进生成的3D模型的多样性，解决了Janus伪像问题。 |
| [^320] | [Long-Range Neural Atom Learning for Molecular Graphs.](http://arxiv.org/abs/2311.01276) | 这项研究提出了一种针对分子图的长程神经原子学习方法，通过将原子投射为神经原子并在其之间交换信息，实现了远距离节点之间的通信，缩小了任意节点对的相互作用范围。 |
| [^321] | [Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion.](http://arxiv.org/abs/2311.01017) | 本论文提出了一种通过离散扩散学习无监督的自动驾驶世界模型的新方法，通过使用VQVAE对传感器观察进行标记化并通过离散扩散预测未来，我们的模型在点云观察中实现了显著改进，将1秒预测的SOTA Chamfer距离降低了65%以上。 |
| [^322] | [Intrinsic Gaussian Vector Fields on Manifolds.](http://arxiv.org/abs/2310.18824) | 本文提出了一种新型的在流形上处理矢量值信号的高斯过程模型，具有内在定义和考虑空间几何的特点，并为部署在二维球面和超曲面上的Hodge-Mat\'ern高斯向量场提供了计算基元。 |
| [^323] | [Robust Offline Policy Evaluation and Optimization with Heavy-Tailed Rewards.](http://arxiv.org/abs/2310.18715) | 本文提出的ROAM和ROOM算法框架通过将中位数法与离线强化学习策略相结合，提供了对重尾奖励的直接不确定性估计，从而增强了离线强化学习在现实应用中的鲁棒性。 |
| [^324] | [A minimax optimal control approach for robust neural ODEs.](http://arxiv.org/abs/2310.17584) | 本文提出了一种鲁棒神经ODE的极小极大优化控制方法，将对抗训练问题转化为一个最优控制问题，并提供了一种新的加权技术来实现鲁棒训练。 |
| [^325] | [Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity.](http://arxiv.org/abs/2310.17247) | 本文发现神经网络中的grokking现象不仅局限于神经网络，还出现在其他算法和模型中。通过在数据集中添加虚假信息的维度，可以诱发grokking现象。研究表明，grokking现象在解决方案搜索受复杂性和错误指导的任何情况下可能发生。这对理解grokking现象提供了更广泛的理论支持。 |
| [^326] | [Estimating Trustworthy and Safe Optimal Treatment Regimes.](http://arxiv.org/abs/2310.15333) | 这篇论文提出了一种安全和可解释的框架，通过匹配患者的医学和药物特性来识别最佳治疗方案。研究结果表明，个性化的治疗策略可以根据患者的病史和药物特征来制定，并发现减少药物剂量可以减轻病情而不会对治疗效果产生负面影响。 |
| [^327] | [Principled Approaches for Learning to Defer with Multiple Experts.](http://arxiv.org/abs/2310.14774) | 我们研究了多个专家学习推迟问题的代理损失和算法，并证明了这些代理损失函数具有强H一致性界限。我们展示了几个实际应用的代理损失函数，并设计了基于最小化这些损失函数的新的学习推迟算法。我们还进行了在SVHN和CIFAR-10数据集上的实验。 |
| [^328] | [Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms.](http://arxiv.org/abs/2310.14772) | 我们研究了多类别分类设置中的学习与放弃框架，并提出了一系列新的理论和算法结果，解决了两个现存的开放问题。这些保证为基于最小化放弃损失的新的多类别放弃算法提供了启示。 |
| [^329] | [Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention.](http://arxiv.org/abs/2310.14770) | 本文提出了基于分数的多类放弃的理论基础损失函数和算法，包括引入了新的代理损失函数族群以及证明了这些代理损失的一致性保证。我们通过实验证明了这些算法的实际意义。 |
| [^330] | [Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?.](http://arxiv.org/abs/2310.10908) | 该论文研究了密集预训练Transformer是否以及如何从自发的模块化结构中获益。 |
| [^331] | [Instruction Tuning with Human Curriculum.](http://arxiv.org/abs/2310.09518) | 本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。 |
| [^332] | [Mirage: Model-Agnostic Graph Distillation for Graph Classification.](http://arxiv.org/abs/2310.09486) | Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。 |
| [^333] | [Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting.](http://arxiv.org/abs/2310.06081) | 本文研究了一类广泛的马尔可夫链，即Ito链的Ito扩散逼近。与大多数相关论文不同，我们的链具有各向同性和状态相关的噪声，并可以适用于多种应用场景。我们证明了Ito链与对应的随机微分方程之间的W2-距离的上界。这些结果改进了已有的估计方法，并在某些特殊情况下提供了首次的分析。 |
| [^334] | [Observation-Guided Diffusion Probabilistic Models.](http://arxiv.org/abs/2310.04041) | 提出了观测引导的扩散概率模型（OGDM），通过引入基于条件鉴别器的观测所产生的额外损失项，实现了更准确的负对数似然优化，在函数评估次数有限的推理阶段表现出色。 |
| [^335] | [LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers.](http://arxiv.org/abs/2310.03294) | LightSeq是用于长上下文转换器分布式训练的一种新方法，它通过序列维度进行分区，与不同注意力头数量的模型架构兼容，并且减少了与Megatron-LM相比的通信量，同时还实现了通信和计算的重叠。 |
| [^336] | [SE(3)-Stochastic Flow Matching for Protein Backbone Generation.](http://arxiv.org/abs/2310.02391) | 通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。 |
| [^337] | [Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion.](http://arxiv.org/abs/2310.02279) | 提出了一种一致性轨迹模型（CTM），它可以加速扩散模型的采样，同时通过对抗训练和去噪得分匹配损失的组合来提高性能，并实现了最先进的采样质量。 |
| [^338] | [Efficient Post-training Quantization with FP8 Formats.](http://arxiv.org/abs/2309.14592) | 本研究研究了FP8格式在后训练量化中的优势，并开发了一个通用的量化工作流程。实验结果表明，FP8相对于INT8具有更好的工作负载覆盖率和模型准确性。 |
| [^339] | [Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming.](http://arxiv.org/abs/2309.12701) | 本文提出了一种使用动态规划找到最优决策树的方法，可以得到多个可解释性-性能权衡的最优决策树，使用户可以根据自己的需求选择最适合的树。 |
| [^340] | [A Comprehensive Review of Community Detection in Graphs.](http://arxiv.org/abs/2309.11798) | 本综述对图中的社区检测进行了全面回顾。社区结构是真实世界图的重要特征，社区检测方法的研究具有社会学、生物学和计算机科学方面的应用。尽管科学家们做出了努力，但尚未找到一个令人满意的解决方案。本综述介绍了社区结构的概念，各种社区检测方法，以及在各种网络中的实际应用。 |
| [^341] | [Learning Orbitally Stable Systems for Diagrammatically Teaching.](http://arxiv.org/abs/2309.10298) | 本文提出了一种学习轨道稳定系统用于图示教学的框架，通过将已知的轨道渐近稳定系统进行形变，实现机器人跟随用户指定草图进行周期运动的目标。 |
| [^342] | [Learning from Demonstration via Probabilistic Diagrammatic Teaching.](http://arxiv.org/abs/2309.03835) | 本文介绍了一种名为图示教学的示教学习的替代范式，通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，并将其合成为三维任务空间中的运动轨迹的生成模型。 |
| [^343] | [Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs.](http://arxiv.org/abs/2308.16859) | 本文研究了学习动态有向无环图（DDAG）的信息理论最优样本复杂度，提出了一种基于观测时间序列的功率谱密度矩阵的度量和算法来重建DDAG。 |
| [^344] | [Ensuring User-side Fairness in Dynamic Recommender Systems.](http://arxiv.org/abs/2308.15651) | 本文提出了一种名为FADE的端到端框架，通过微调策略动态减轻推荐系统中用户群体之间的性能差异。 |
| [^345] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^346] | [Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19.](http://arxiv.org/abs/2308.09730) | 本研究通过使用多样性的临床和虚拟生成的医学图像开发和评估了COVID-19诊断的AI模型，发现数据集特征对于AI性能具有重要影响，容易导致泛化能力较差，最高下降20％。 |
| [^347] | [RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models.](http://arxiv.org/abs/2308.07922) | RAVEN是一种结合了检索增强的蒙特卡洛语言建模和前缀语言建模的模型，通过引入上下文融合学习，它能够在上下文学习方面取得比ATLAS更好的性能。 |
| [^348] | [The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions.](http://arxiv.org/abs/2308.05864) | 本研究提出了一个多模态细胞分割基准，采用Transformer-based深度学习算法，不仅超越了现有方法，而且可以应用于各种显微成像平台和组织类型的图像，无需手动参数调整，为显微成像中更准确和多功能的细胞分析提供了有希望的途径。 |
| [^349] | [Iterative Sketching for Secure Coded Regression.](http://arxiv.org/abs/2308.04185) | 这篇论文提出了一种迭代草图方法，用于加速分布式线性回归计算并确保安全性。通过利用随机草图技术和改进异步系统中的块效应韧性，将信息保护与回归问题维度的减小相结合。特别是，通过应用随机正交矩阵和子采样"块"，实现了在每次迭代中考虑新草图的分布式迭代草图方法。同时，对子采样随机哈达玛变换进行了推广并修改以保证数据的安全性。 |
| [^350] | [Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias.](http://arxiv.org/abs/2308.00225) | 这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。 |
| [^351] | [Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems.](http://arxiv.org/abs/2307.16120) | 本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。 |
| [^352] | [Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search.](http://arxiv.org/abs/2307.10438) | 用于分子属性预测的图神经网络方法通常无法量化预测的不确定性，本研究提出了一种自动化的不确定性量化方法AutoGNNUQ，通过架构搜索生成高性能的图神经网络集合，并利用方差分解将数据和模型的不确定性分开，从而提供了减少不确定性的有价值见解。 |
| [^353] | [Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images.](http://arxiv.org/abs/2307.08919) | 半监督和自监督学习在医学图像上的准确性与时间前沿进行比较，通过一个精心设计的基准研究来回答从业者的问题。 |
| [^354] | [Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test.](http://arxiv.org/abs/2307.05775) | 本文通过系统分析，揭示了$k$-WL的可靠性和有效性问题，并提出了基于基准的表达能力的外延定义和测量。 |
| [^355] | [An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application.](http://arxiv.org/abs/2307.00185) | 本文提出了一种可解释的增量随机权重神经网络的构造算法，通过几何信息约束和节点池策略解决了难以解释隐藏参数与残差误差之间关系的问题。这种算法在大规模数据建模任务中表现出了良好的性能。 |
| [^356] | [A Survey on Multimodal Large Language Models.](http://arxiv.org/abs/2306.13549) | 本文追踪和总结了多模态大语言模型（MLLM）的最新进展，包括多模态指令调整、多模态上下文学习、多模态思维链和LLM辅助视觉推理等应用，指出了现有挑战和有前途的研究方向。 |
| [^357] | [Subgraph Networks Based Contrastive Learning.](http://arxiv.org/abs/2306.03506) | 本文提出了一种新的对比学习框架，名为基于子图网络的对比学习(SGNCL)，通过应用子图网络生成策略以产生增强视图，并探究了子结构相互作用对图形表示的影响。 |
| [^358] | [Improved Probabilistic Image-Text Representations.](http://arxiv.org/abs/2305.18171) | 本论文提出了一种改进的概率图像-文本表示方法，通过引入新的概率距离和两种优化技术，解决了现有方法中的计算负担过重和损失饱和问题，取得了显著的性能提升。 |
| [^359] | [First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities.](http://arxiv.org/abs/2305.15938) | 本论文研究了涉及马尔科夫噪声的随机优化问题，提出了一种适用于非凸和强凸最小化问题的一阶梯度方法，使用基于多层蒙特卡罗方法的随机批处理方案以获得最优线性关系，并消除了以前研究中的限制条件。在马尔可夫噪声下对变分不等式的扩展是原创性的。 |
| [^360] | [Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution.](http://arxiv.org/abs/2305.15357) | 本文提出了一种求解最优边界条件解决扩散ODE问题的有效采样方法，以稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像。 |
| [^361] | [Multi-State RNA Design with Geometric Multi-Graph Neural Networks.](http://arxiv.org/abs/2305.14749) | 本论文提出了一种基于几何多图神经网络的多状态RNA设计方法，可以明确考虑和反映RNA构象多样性在其设计中。其能够显著提高原生序列的恢复，尤其适用于多状态和结构多样化的RNA。 |
| [^362] | [Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape.](http://arxiv.org/abs/2305.11584) | 本文提出了一种动态正则化锐度感知联邦学习方法，通过同时考虑优化和泛化目标来高效地提高联邦学习的性能。 |
| [^363] | [SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.](http://arxiv.org/abs/2305.09781) | SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。 |
| [^364] | [iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer.](http://arxiv.org/abs/2304.13061) | 本文推广了 Hopkins field 分层网络，并介绍了 iMixer，MLP-Mixer 模型的新概括，不同于普通的前馈网络，iMixer 涉及到从输出到输入的传播的 MLP 层，被特征化为一个可逆、隐式、迭代的 mixing block。 |
| [^365] | [3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI.](http://arxiv.org/abs/2303.09373) | 本文提出了一种名为 MAPSeg 的新框架，采用 3D 蒙版自编码和伪标签的方式，实现了跨年龄、跨模态和跨场景下对婴儿脑 MRI 中亚皮质区域的分割，充分考虑不同 MRI 扫描仪、供应商或采集序列以及不同的神经发育阶段所造成的内在异质性，提高了分割结果的鲁棒性。 |
| [^366] | [Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics.](http://arxiv.org/abs/2303.05101) | 本文提出了两种非对角度量，可以在随机梯度采样中使用，以改善神经网络模型的贝叶斯推断性能，尤其对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络效果显著。 |
| [^367] | [CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification.](http://arxiv.org/abs/2302.02314) | 本研究提出了一种新的分类网络CECT，利用可控的卷积神经网络和Transformer进行组合，能同时捕捉多个局部和全局尺度上的特征。在两个公共COVID-19数据集上评估后表现优于现有的最先进方法。这一新方法在医学图像分类领域中有广泛的应用前景。 |
| [^368] | [Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram.](http://arxiv.org/abs/2301.10856) | 本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。 |
| [^369] | [Training Data Influence Analysis and Estimation: A Survey.](http://arxiv.org/abs/2212.04612) | 本文对训练数据影响分析与估计进行了全面调查，通过量化每个训练实例对最终模型的改变程度揭示了训练的基本相互作用。调查了当前的影响分析方法，并提出了未来的研究方向。 |
| [^370] | [Shape-Guided Diffusion with Inside-Outside Attention.](http://arxiv.org/abs/2212.00210) | 该论文提出了一种无需训练的形状引导扩散方法，使用一种新颖的内外部注意机制将形状限制应用于跨注意力图和自注意力图上，从而在文本到图像扩散模型中考虑到对象形状，进而可以实现对象形状忠实度更高的图像生成。 |
| [^371] | [Using Persuasive Writing Strategies to Explain and Detect Health Misinformation.](http://arxiv.org/abs/2211.05985) | 本研究旨在通过使用说服性写作技巧的文本段落进行分类来增加自动化虚假信息检测的新层次，以产生可解释的理由。我们提出了一个包含常见说服性写作策略的注释方案和数据集，并使用 RoBERTa 文本分类模型进行实验。 |
| [^372] | [Online Convex Optimization with Unbounded Memory.](http://arxiv.org/abs/2210.09903) | 本论文提出了一种新的在线凸优化框架，可以处理决策历史的长期依赖关系，并介绍了用于量化依赖程度的$p$-有效内存容量的概念。 |
| [^373] | [A Machine Learning Approach to Solving Large Bilevel and Stochastic Programs: Application to Cycling Network Design.](http://arxiv.org/abs/2209.09404) | 我们提出了一种基于机器学习的新方法，用于求解涉及大量独立从属者的双层规划问题。该方法通过从一个采样的子集中估计未采样从属者的目标值来优化模型，实现了对一般从属者特征的表征学习。 |
| [^374] | [Estimating large causal polytrees from small samples.](http://arxiv.org/abs/2209.07028) | 本文介绍了一种算法，可以在变量数量远大于样本大小的情况下，准确地估计大规模因果多树结构，而几乎不需要任何分布或建模的假设。 |
| [^375] | [WaveMix: A Resource-efficient Neural Network for Image Analysis.](http://arxiv.org/abs/2205.14375) | WaveMix是一种资源高效的神经网络结构，可以在多项任务上达到可比或更好的准确率，并且需要更少的参数和GPU RAM，实现时间、成本和能量的节省。 |
| [^376] | [Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes.](http://arxiv.org/abs/2205.13589) | 本文提出了代理变量悲观策略优化 (P3O) 算法，它通过近端因果推断构建悲观置信区间耦合序列解决了部分可观察马尔可夫决策过程中的混淆偏差和最优策略与行为策略之间的分布偏移问题。 |
| [^377] | [Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs.](http://arxiv.org/abs/2202.03583) | 本研究提出了一种基于密集卷积神经网络和GRADCAM的胸部X光疾病多标签诊断模型，获得了在Cardiomegaly条件下最高的AUC得分0.896，并使用热图提高了模型的可解释性。 |
| [^378] | [Multitask Learning and Bandits via Robust Statistics.](http://arxiv.org/abs/2112.14233) | 本研究探讨了多任务学习以及Bandits方法的健壮统计学实现，提出了一种新颖的两阶段多任务学习估计器，该估计器以一种样本高效的方式利用共享全局参数和稀疏实例特定术语的结构。 |

# 详细

[^1]: 具有动态停止的循环Transformer

    Recurrent Transformers with Dynamic Halt

    [https://rss.arxiv.org/abs/2402.00976](https://rss.arxiv.org/abs/2402.00976)

    本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。

    

    本文研究了两种主要方法在增强Transformer与循环机制方面的归纳偏好——（1）类似于Universal Transformers的深度逐层循环方法；和（2）类似于Temporal Latent Bottleneck的分块时态循环方法。此外，我们提出并研究了扩展和组合上述方法的新方式，例如，我们提出了一种基于全局均值的Universal Transformer动态停止机制，并将Universal Transformer的元素融入到Temporal Latent Bottleneck中。我们通过多个诊断任务（如Long Range Arena（LRA），翻转-翻转语言建模，ListOps和逻辑推理）比较了模型并探索了它们的归纳偏好。

    In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
    
[^2]: NeRF-MAE: 自监督三维表示学习中的Masked AutoEncoders

    NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields

    [https://arxiv.org/abs/2404.01300](https://arxiv.org/abs/2404.01300)

    通过使用Masked AutoEncoders，本文提出了NeRF-MAE用于自监督三维表示学习，利用标准的三维Vision Transformers适应NeRF的独特公式，将NeRF的体积网格作为密集输入，以产生有效的三维表示。

    

    由于神经场在计算机视觉和机器人领域的卓越能力，能够理解三维视觉世界，如推断语义、几何和动态等，本文探讨了神经场在从二维图像中密集表示三维场景的自监督预训练，具体使用Masked AutoEncoders的可能性。我们借鉴了将transformers扩展到新数据模态的令人惊讶的成功，利用标准的三维Vision Transformers来适应NeRF的独特公式。我们将NeRF的体积网格作为transformer的密集输入，与其他三维表示（如点云）进行对比，其信息密度可能不均匀，而表示是不规则的。由于将masked autoencoders应用于类似NeRF这样的隐式表示的困难，我们选择提取一个显式的表示。

    arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
    
[^3]: CausalChaos!数据集：基于动态视觉场景中更长因果链的全面因果行动问答

    CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes

    [https://arxiv.org/abs/2404.01299](https://arxiv.org/abs/2404.01299)

    利用卡通图像构建的CausalChaos!数据集，包含更长因果链的因果问答，通过动态互动和视觉展示挑战性因果关系，为模型提供了更多具挑战性且明确定义的因果关系。

    

    因果视频问答（QA）越来越受到关注，然而现有数据集在因果推理分析方面往往缺乏深度。为了填补这一空白，我们利用卡通的独特属性构建了CausalChaos!，这是一个新颖且具有挑战性的因果问答（Why-QA）数据集，基于标志性的“猫和老鼠”卡通系列。我们的数据集通过周到的问题和多层次答案，包含着嵌入动态互动和视觉中的更长因果链，同时动画原理允许动画师创造定义明确、明了的因果关系。这些因素使模型能够解决更具挑战性但明确定义的因果关系。我们还引入了硬负采样，包括CausalConfusion版本。虽然模型表现良好，但仍有很大改进空间，特别是在开放式答案方面。我们确定了更为先进/明确的因果关系建模和联合建模等改进方向。

    arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
    
[^4]: 在扩散模型中测量风格相似性

    Measuring Style Similarity in Diffusion Models

    [https://arxiv.org/abs/2404.01292](https://arxiv.org/abs/2404.01292)

    提出了一个新的框架，用于理解和提取图像的风格描述符，通过新数据集和研究呈现了一种了解图像风格相似性的方法。

    

    arXiv:2404.01292v1 公告类型：交叉 抽象：生成模型现在被广泛应用于图形设计师和艺术家。先前的作品表明，这些模型在生成过程中会记住并经常复制其训练数据中的内容。因此，随着它们的普及增加，每次在将生成的图像用于专业用途之前，执行数据库搜索以确定图像的属性是否归因于特定的训练数据变得越来越重要。现有工具旨在检索具有相似语义内容的图像。与此同时，许多艺术家关注文本到图像模型中的风格复制。我们提出了一个框架来理解和提取图像的风格描述符。我们的框架包括使用这样一种洞察力筛选的新数据集，即风格是图像的主观属性，捕捉到包括但不限于颜色、纹理、形状等因素的复杂而有意义的相互作用。

    arXiv:2404.01292v1 Announce Type: cross  Abstract: Generative models are now widely used by graphic designers and artists. Prior works have shown that these models remember and often replicate content from their training data during generation. Hence as their proliferation increases, it has become important to perform a database search to determine whether the properties of the image are attributable to specific training data, every time before a generated image is used for professional purposes. Existing tools for this purpose focus on retrieving images of similar semantic content. Meanwhile, many artists are concerned with style replication in text-to-image models. We present a framework for understanding and extracting style descriptors from images. Our framework comprises a new dataset curated using the insight that style is a subjective property of an image that captures complex yet meaningful interactions of factors including but not limited to colors, textures, shapes, etc. We a
    
[^5]: 评估文本到图像生成与图像到文本生成

    Evaluating Text-to-Visual Generation with Image-to-Text Generation

    [https://arxiv.org/abs/2404.01291](https://arxiv.org/abs/2404.01291)

    引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果

    

    尽管生成式人工智能取得了显著进展，但由于缺乏有效的度量标准和标准化基准，综合评估仍然具有挑战性。为了解决这个问题，我们引入了VQAScore，使用视觉问答（VQA）模型通过计算对简单的“这幅图表现出了'{文本}'吗？”问题的“是”答案的概率来生成对齐分数。尽管比先前的方法更简单，但使用现成模型计算的VQAScore在许多（8个）图像文本对齐基准上产生了最先进的结果。

    arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
    
[^6]: TWIN-GPT: 基于大语言模型的临床试验数字孪生体

    TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model

    [https://arxiv.org/abs/2404.01273](https://arxiv.org/abs/2404.01273)

    提出了基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。

    

    最近，对虚拟临床试验产生了日益增长的兴趣，这些试验模拟了现实世界情境，有望显著增强患者安全性，加快开发速度，降低成本，并为医疗领域的更广泛科学知识贡献力量。本文提出了一种基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。

    arXiv:2404.01273v1 Announce Type: cross  Abstract: Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin crea
    
[^7]: 具有外部隐私泄露分析的分散协作学习框架

    Decentralized Collaborative Learning Framework with External Privacy Leakage Analysis

    [https://arxiv.org/abs/2404.01270](https://arxiv.org/abs/2404.01270)

    本文提出了分散多任务学习中的两种方法论进展，一是在CollabDict框架中整合深度变分自动编码器用于异常检测，二是提供了使用CollabDict训练的模型在外部共享时数据隐私泄漏的数学分析。

    

    本文提出了在隐私约束条件下分散多任务学习中的两种方法论进展，旨在为未来下一代区块链平台的发展铺平道路。首先，我们将用于协作词典学习（CollabDict）的现有框架进行扩展，该框架先前仅限于高斯混合模型，通过将深度变分自动编码器（VAEs）纳入框架，特别关注异常检测。我们展示基于VAE的异常评分函数与非深度模型具有相同的数学结构，并提供全面的定性比较。其次，考虑到“预训练模型”广泛使用，我们对使用CollabDict训练的模型在外部共享时的数据隐私泄漏进行了数学分析。我们展示了当将CollabDict方法应用于高斯混合模型时，符合Renyi微分隐私标准。

    arXiv:2404.01270v1 Announce Type: new  Abstract: This paper presents two methodological advancements in decentralized multi-task learning under privacy constraints, aiming to pave the way for future developments in next-generation Blockchain platforms. First, we expand the existing framework for collaborative dictionary learning (CollabDict), which has previously been limited to Gaussian mixture models, by incorporating deep variational autoencoders (VAEs) into the framework, with a particular focus on anomaly detection. We demonstrate that the VAE-based anomaly score function shares the same mathematical structure as the non-deep model, and provide comprehensive qualitative comparison. Second, considering the widespread use of "pre-trained models," we provide a mathematical analysis on data privacy leakage when models trained with CollabDict are shared externally. We show that the CollabDict approach, when applied to Gaussian mixtures, adheres to a Renyi differential privacy criterion
    
[^8]: 科学论文中LLM使用增加的映射

    Mapping the Increasing Use of LLMs in Scientific Papers

    [https://arxiv.org/abs/2404.01268](https://arxiv.org/abs/2404.01268)

    该论文通过对科学论文中LLM使用增加的映射进行了大规模分析，填补了学术写作中真实LLM修改内容比例缺失的空白。

    

    科学出版通过传播研究成果、促进合作、鼓励可重复性，并确保科学知识随时间可访问、可验证并不断建立，为科学奠定了基础。近年来，人们对多少人在学术写作中使用大型语言模型（LLMs）如ChatGPT及这种工具对全球科学实践可能产生何种影响进行了大量猜测。然而，我们缺乏对学术写作中实质性修改或生成内容的准确度量。为填补这一空白，我们对在arXiv、bioRxiv和自然学报系列期刊上的950,965篇论文（时间跨度从2020年1月至2024年2月）进行了首次系统性、大规模的分析，利用人群级别的统计框架来测量LLM修改内容随时间的盛行程度。我们的统计估计是在语料库级别及i

    arXiv:2404.01268v1 Announce Type: cross  Abstract: Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and i
    
[^9]: 将远程传感器与多传感器地理空间基础模型联系起来

    Bridging Remote Sensors with Multisensor Geospatial Foundation Models

    [https://arxiv.org/abs/2404.01260](https://arxiv.org/abs/2404.01260)

    msGFM是一个多传感器地理空间基础模型，能够有效统一四种关键传感器模态的数据，具有处理成对和非成对传感器数据的能力，通过创新的跨传感器预训练方法在蒙版图像建模中合成联合表示，展现出在各种传感器类型下都具有强大性能的综合模型。

    

    在地理空间分析领域，包括光学和微波技术在内的远程传感器的多样性提供了丰富的独特观测能力。鉴于此，我们提出了msGFM，一个多传感器地理空间基础模型，有效地统一了来自四种关键传感器模态的数据。这种集成涵盖了两百万多传感器图像的庞大数据集。msGFM在处理成对和非成对传感器数据方面独具才能。对于来自相同地理位置的数据，我们的模型采用了一种创新的跨传感器预训练方法，在蒙版图像建模中实现联合表示的合成。msGFM结合了四种远程传感器，具有强大的性能，构建了一个适应各种传感器类型的综合模型。msGFM在一系列单传感器和多传感器下游任务中展现出了卓越的性能。

    arXiv:2404.01260v1 Announce Type: cross  Abstract: In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include sce
    
[^10]: 随机梯度下降的新对数步长

    New logarithmic step size for stochastic gradient descent

    [https://arxiv.org/abs/2404.01257](https://arxiv.org/abs/2404.01257)

    提出了一种新的对数步长，通过该步长可改善随机梯度下降方法在非凸函数上的收敛速率，并在CNN模型下将CIFAR100数据集的测试准确度提高了0.9%。

    

    在本文中，我们提出了一种使用新的对数步长的温暖重启技术，用于随机梯度下降（SGD）方法。对于光滑和非凸函数，我们为SGD建立了一个$O(\frac{1}{\sqrt{T}})$的收敛速率。我们进行了全面的实现，以展示新提出的步长在FashionMinst、CIFAR10和CIFAR100数据集上的效率。此外，我们与其他九种现有方法进行了比较，并证明了在使用卷积神经网络（CNN）模型时，新的对数步长可以将CIFAR100数据集的测试准确度提高$0.9\%$。

    arXiv:2404.01257v1 Announce Type: new  Abstract: In this paper, we propose a novel warm restart technique using a new logarithmic step size for the stochastic gradient descent (SGD) approach. For smooth and non-convex functions, we establish an $O(\frac{1}{\sqrt{T}})$ convergence rate for the SGD. We conduct a comprehensive implementation to demonstrate the efficiency of the newly proposed step size on the ~FashionMinst,~ CIFAR10, and CIFAR100 datasets. Moreover, we compare our results with nine other existing approaches and demonstrate that the new logarithmic step size improves test accuracy by $0.9\%$ for the CIFAR100 dataset when we utilize a convolutional neural network (CNN) model.
    
[^11]: 大型语言模型水印的统计框架: 枢轴、检测效率和最优规则

    A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules

    [https://arxiv.org/abs/2404.01245](https://arxiv.org/abs/2404.01245)

    该论文提出了一个通用框架，用于设计大型语言模型水印的统计效率和检测规则，通过关键统计量和秘密密钥控制误报率，同时评估水印检测规则的能力。

    

    自ChatGPT于2022年11月推出以来，将几乎不可察觉的统计信号嵌入到大型语言模型（LLMs）生成的文本中，也被称为水印，已被用作从其人类撰写对应物上可证检测LLM生成文本的原则性方法。 本文介绍了一个通用灵活的框架，用于推理水印的统计效率并设计强大的检测规则。受水印检测的假设检验公式启发，我们的框架首先选择文本的枢轴统计量和由LLM提供给验证器的秘密密钥，以实现控制误报率（将人类撰写的文本错误地检测为LLM生成的错误）。 接下来，该框架允许通过获取渐近错误负率（将LLM生成文本错误地检测为人类撰写的错误）的封闭形式表达式来评估水印检测规则的能力。

    arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
    
[^12]: 针对分布外预测的最优岭回归正则化

    Optimal Ridge Regularization for Out-of-Distribution Prediction

    [https://arxiv.org/abs/2404.01233](https://arxiv.org/abs/2404.01233)

    研究了针对分布外预测的最优岭回归正则化下的行为，并建立了确定最优正则化水平的一般条件，揭示了与分布内设置的鲜明差异。

    

    我们研究了针对分布外预测的最优岭回归正则化和最优岭风险的行为，其中测试分布与训练分布任意偏离。我们建立了确定在协变量和回归偏移下最优正则化水平符号的一般条件。这些条件捕捉了训练数据和测试数据之间协方差和信号结构之间的对齐，并揭示了与在分布内设置相比的鲜明差异。例如，在协变量偏移或回归偏移下，即使训练特征是各向同性的或设计是欠参数化的，负正则化水平也可能是最优的。此外，我们证明了在数据纵横比中，甚至在最优化负正则化水平时，最优调整的风险是单调的，即在分布外设置中也是如此。总的来说，我们的结果对训练数据没有做出任何建模假设。

    arXiv:2404.01233v1 Announce Type: cross  Abstract: We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the tra
    
[^13]: 隐私后门: 通过污染预训练模型增强成员推理

    Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models

    [https://arxiv.org/abs/2404.01231](https://arxiv.org/abs/2404.01231)

    本文揭示了一种新的隐私后门攻击漏洞，通过污染预训练模型来增强成员推理，在多个数据集和模型上进行了广泛实验，并展示了攻击的广泛适用性和有效性。

    

    通过使用小型定制数据集微调大型预训练模型来生成特定应用程序模型已经司空见惯。网络上基础模型检查点的广泛可用性存在重大风险，包括易受后门攻击的脆弱性。本文揭示了一种新的漏洞：隐私后门攻击。这种黑盒隐私攻击旨在增强模型微调时产生的隐私泄露：当受害者微调一个带有后门的模型时，他们的训练数据泄露速率会比微调典型模型时显著提高。我们在各种数据集和模型上进行了大量实验，包括视觉-语言模型（CLIP）和大型语言模型，展示了这种攻击的广泛适用性和有效性。此外，我们进行了多个消融研究，使用不同的微调方法和推理策略进行彻底分析。

    arXiv:2404.01231v1 Announce Type: cross  Abstract: It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly a
    
[^14]: 多目标优化问题中的协同帕累托集学习

    Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems

    [https://arxiv.org/abs/2404.01224](https://arxiv.org/abs/2404.01224)

    提出了一种协同帕累托集学习(CoPSL)框架，可以同时学习多个多目标优化问题的帕累托集，通过共享和特定层的结构，实现了不同MOP之间的协同学习。

    

    帕累托集学习(PSL)是多目标优化中一个新兴研究领域，专注于训练神经网络学习从偏好向量到帕累托最优解的映射。然而，现有的PSL方法仅限于一次解决单个多目标优化问题(MOP)。面对多个MOP时，这种限制不仅导致显著的低效，而且未能利用横跨不同MOP的潜在协同效应。本文提出了一种协同帕累托集学习(CoPSL)框架，它以协同方式同时学习多个MOP的帕累托集。CoPSL采用了一个架构，包括共享和MOP特定层，其中共享层旨在协同捕捉MOP之间的公共关系，而MOP特定层处理这些关系以生成每个MOP的解集。这种协同方法使得CoPSL能够高效地...

    arXiv:2404.01224v1 Announce Type: new  Abstract: Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions. However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time. When faced with multiple MOPs, this limitation not only leads to significant inefficiencies but also fails to exploit the potential synergies across varying MOPs. In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which simultaneously learns the Pareto sets of multiple MOPs in a collaborative manner. CoPSL employs an architecture consisting of shared and MOP-specific layers, where shared layers aim to capture common relationships among MOPs collaboratively, and MOP-specific layers process these relationships to generate solution sets for each MOP. This collaborative approach enables CoPSL to effi
    
[^15]: 特征点切片：基于语言驱动的基于物理的场景合成和编辑

    Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing

    [https://arxiv.org/abs/2404.01223](https://arxiv.org/abs/2404.01223)

    该研究提出了一种名为Feature Splatting的方法，将基于物理的动态场景合成与源自视觉语言的丰富语义统一起来，实现了高质量的对象分解和基于文本查询的物理特性自动合成。

    

    使用3D高斯原语表示场景在建模静态和动态3D场景的外观方面取得了出色的结果。 然而，许多图形应用程序需要能够操纵对象的外观和物理特性。 我们介绍了一种称为Feature Splatting的方法，它将基于物理的动态场景合成与源于自然语言的丰富语义统一起来。 我们的第一个贡献是一种方法，可以将高质量的，以对象为中心的视觉语言特征提炼成3D高斯，从而利用文本查询实现半自动场景分解。 我们的第二个贡献是一种方法，通过基于粒子的模拟器从静态场景中合成基于物理的动态场景，在此过程中，材料属性通过文本查询自动分配。 我们剔除了在这个流程中使用的关键技术，以阐明挑战和机会。

    arXiv:2404.01223v1 Announce Type: cross  Abstract: Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunit
    
[^16]: 基于实体中心的像素级目标操纵强化学习

    Entity-Centric Reinforcement Learning for Object Manipulation from Pixels

    [https://arxiv.org/abs/2404.01220](https://arxiv.org/abs/2404.01220)

    提出了一种适用于表示多个对象及其相互作用的视觉强化学习的结构化方法，能够处理具有对象间依赖关系的目标，并展示了训练代理的泛化能力。

    

    操纵物体是人类智能的标志，也是机器人领域中的重要任务。本文提出了一种适用于表示多个对象及其相互作用的视觉强化学习的结构化方法，并用它来学习几个对象的目标操纵。我们方法的关键在于能够处理具有对象间依赖关系的目标（例如按特定顺序移动对象）。此外，我们将我们的架构与训练代理的泛化能力联系起来，基于复合泛化的理论结果，展示了学习3个对象但能泛化到类似任务的代理。

    arXiv:2404.01220v1 Announce Type: cross  Abstract: Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar ta
    
[^17]: 支持医师研究活动的系统建模以从电子健康记录中提取疾病数据

    Towards System Modelling to Support Diseases Data Extraction from the Electronic Health Records for Physicians Research Activities

    [https://arxiv.org/abs/2404.01218](https://arxiv.org/abs/2404.01218)

    本文旨在通过系统建模支持从电子健康记录中提取疾病数据，以便将其用于医师研究活动，解决EHRs数据格式多样化的标准化问题。

    

    过去15年中，电子健康记录（EHRs）的使用量急剧增加，因为它被认为是管理病人数据的重要来源。EHRs是世界范围内疾病诊断和人口统计数据的主要来源。因此，这些数据可以用于诸如研究之类的次要任务。本文旨在使这些数据可用于监测特定人群的疾病统计数据等研究活动。因此，研究人员可以检测目标群体的病因与行为和生活方式之间的关系。EHRs系统的局限之一是数据不以标准格式而是以各种形式可用。因此，需要首先将疾病名称和人口统计数据转换为一个标准格式，以使其可用于研究活动。有大量的EHRs可用，解决标准化问题需要一些优化。

    arXiv:2404.01218v1 Announce Type: new  Abstract: The use of Electronic Health Records (EHRs) has increased dramatically in the past 15 years, as, it is considered an important source of managing data od patients. The EHRs are primary sources of disease diagnosis and demographic data of patients worldwide. Therefore, the data can be utilized for secondary tasks such as research. This paper aims to make such data usable for research activities such as monitoring disease statistics for a specific population. As a result, the researchers can detect the disease causes for the behavior and lifestyle of the target group. One of the limitations of EHRs systems is that the data is not available in the standard format but in various forms. Therefore, it is required to first convert the names of the diseases and demographics data into one standardized form to make it usable for research activities. There is a large amount of EHRs available, and solving the standardizing issues requires some optim
    
[^18]: 将领域微分方程纳入图卷积网络以降低泛化差异

    Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy

    [https://arxiv.org/abs/2404.01217](https://arxiv.org/abs/2404.01217)

    将领域微分方程纳入图卷积网络可提高其对不匹配训练和测试数据的稳健性。

    

    确保时间序列预测的准确性和稳健性对许多应用至关重要，从城市规划到疫情管理。本工作表明，在涵盖所有时空模式的充分训练数据的情况下，现有的深度学习模型可以做出相当准确的预测。然而，当训练数据来自与测试数据不同环境（例如，正常天交通模式与自然灾害后交通模式）时，现有方法会失败。在本工作中，我们展示了解决这一挑战的一种方法，即将领域微分方程纳入图卷积网络（GCNs）中。

    arXiv:2404.01217v1 Announce Type: cross  Abstract: Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline do
    
[^19]: 在子群体转移下的新颖节点类别检测

    Novel Node Category Detection Under Subpopulation Shift

    [https://arxiv.org/abs/2404.01216](https://arxiv.org/abs/2404.01216)

    提出了一种新方法 RECO-SLIP，用于在属性图中检测属于新类别的节点，能够有效解决子群体转移下的节点检测问题，实验证明其性能优越。

    

    在现实世界的图数据中，分布转移可以通过各种方式表现，例如新类别的出现和现有类别相对比例的变化。在这种分布转移下，检测属于新类别的节点对于安全或洞察发现至关重要。我们引入了一种新方法，称为具有选择性链路预测的召回约束优化（RECO-SLIP），用于在子群体转移下检测属性图中属于新类别的节点。通过将召回约束学习框架与高效样本预测机制相结合，RECO-SLIP解决了抵抗子群体转移和有效利用图结构的双重挑战。我们在多个图数据集上进行了大量实证评估，结果表明RECO-SLIP相对于现有方法具有更优异的性能。

    arXiv:2404.01216v1 Announce Type: new  Abstract: In real-world graph data, distribution shifts can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories. It is often important to detect nodes of novel categories under such distribution shifts for safety or insight discovery purposes. We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed graphs under subpopulation shifts. By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of graph structure. Our extensive empirical evaluation across multiple graph datasets demonstrates the superior performance of RECO-SLIP over existing methods.
    
[^20]: 传统模型和大型语言模型的机器遗忘：简短调查

    Machine Unlearning for Traditional Models and Large Language Models: A Short Survey

    [https://arxiv.org/abs/2404.01206](https://arxiv.org/abs/2404.01206)

    这项调查研究了机器遗忘在传统模型和大型语言模型中的应用，包括定义、分类、评估标准以及挑战和解决方案。

    

    随着个人数据隐私规定的实施，机器学习（ML）领域面临“被遗忘的权利”的挑战。机器遗忘应运而生，旨在根据用户请求删除数据并减少其对模型的影响。尽管机器遗忘受到广泛关注，但对其最新进展，尤其是在大型语言模型（LLMs）领域的全面调查仍然缺乏。本调查旨在填补这一空白，深入探讨机器遗忘，包括定义、分类和评估标准，以及不同环境下的挑战及其解决方案。具体而言，本文对传统模型和LLMs上的遗忘进行了分类和研究，并提出了评估遗忘的有效性和效率的方法，以及绩效衡量标准。该论文揭示了

    arXiv:2404.01206v1 Announce Type: new  Abstract: With the implementation of personal data privacy regulations, the field of machine learning (ML) faces the challenge of the "right to be forgotten". Machine unlearning has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in machine unlearning, comprehensive surveys on its latest advancements, especially in the field of Large Language Models (LLMs) is lacking. This survey aims to fill this gap by providing an in-depth exploration of machine unlearning, including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and LLMs, and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations 
    
[^21]: 大规模非凸随机约束分布鲁棒优化

    Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization

    [https://arxiv.org/abs/2404.01200](https://arxiv.org/abs/2404.01200)

    本文开发了一种用于非凸约束分布鲁棒优化的随机算法，其计算复杂度与整体数据集大小无关，适用于大规模应用。

    

    分布鲁棒优化（DRO）是针对数据分布变化训练健壮模型的强大框架。本文关注具有鲁棒性水平明确特征的约束DRO。现有研究主要集中在具有凸损失函数的约束DRO上，并排除了具有非凸损失函数（如神经网络）的实践和具有挑战性的情况。本文为非凸约束DRO开发了一种随机算法及其性能分析。我们的随机算法在每次迭代的计算复杂度与整体数据集大小独立无关，因此适用于大规模应用。我们侧重于将Cressie-Read家族散度定义的不确定性集成中包含$\chi^2$-散度作为特例。我们证明了我们的算法在计算复杂度为$\mathcal O(\epsilon^{-3k_*-5})$的情况下找到了一个$\epsilon$-稳定点。

    arXiv:2404.01200v1 Announce Type: cross  Abstract: Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts. This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\chi^2$-divergences as a special case. We prove that our algorithm finds an $\epsilon$-stationary point with a computational complexity of $\mathcal O(\epsilon^{-3k_*-5})$, wh
    
[^22]: 对改进的多臂老虎机问题的近乎最紧密的逼近保证

    Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem

    [https://arxiv.org/abs/2404.01198](https://arxiv.org/abs/2404.01198)

    对改进的多臂老虎机问题，我们给出了一个随机在线算法，可以在不了解最优臂最大奖励的情况下，以$O(\sqrt{k} \log k)$的逼近相对于最优。

    

    我们为改进的多臂老虎机问题提供了近乎最紧密的上界和下界。这个问题的一个实例有$k$个臂，每个臂的奖励函数都是一个凹函数，并且是一个与到目前为止拉动该臂的次数成增函数。我们证明了对于任何随机在线算法，都存在一个实例，使其相对于最优奖励必须至少承受一个$\Omega(\sqrt{k})$的近似因子。然后我们提供了一个随机在线算法，如果事先告知最优臂可实现的最大奖励，就可以保证一个$O(\sqrt{k})$的近似因子。我们还展示了如何在额外付出$O(\log k)$的近似因子的代价下，消除这个假设，实现相对于最优的总体$O(\sqrt{k} \log k)$的逼近。

    arXiv:2404.01198v1 Announce Type: new  Abstract: We give nearly-tight upper and lower bounds for the improving multi-armed bandits problem. An instance of this problem has $k$ arms, each of whose reward function is a concave and increasing function of the number of times that arm has been pulled so far. We show that for any randomized online algorithm, there exists an instance on which it must suffer at least an $\Omega(\sqrt{k})$ approximation factor relative to the optimal reward. We then provide a randomized online algorithm that guarantees an $O(\sqrt{k})$ approximation factor, if it is told the maximum reward achievable by the optimal arm in advance. We then show how to remove this assumption at the cost of an extra $O(\log k)$ approximation factor, achieving an overall $O(\sqrt{k} \log k)$ approximation relative to optimal.
    
[^23]: 具有控制屏障函数诱导神经控制器的机器人运动规划的高效实现

    Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller

    [https://arxiv.org/abs/2404.01184](https://arxiv.org/abs/2404.01184)

    提出了一种利用控制屏障函数诱导的神经控制器来减少基于采样的运动规划中所需样本数量的方法，实现了实时碰撞避免控制和长视程运动规划的结合。

    

    在拥挤环境中的机器人的基于采样的运动规划方法通常会受到昂贵的碰撞检查和高采样复杂性的困扰，这使得它们难以实时使用。为了解决这个问题，我们提出了一种新的通用控制屏障函数(CBF)为基础的转向控制器，以减少采样式运动规划器RRT中所需的样本数量。我们的方法结合了CBF用于实时避免碰撞控制的优势和RRT用于长视程运动规划的优势，通过使用CBF诱导的神经控制器(CBF-INC)生成操纵信号，将系统引导向RRT采样的配置。CBF-INC被学习为神经网络，有两个处理不同输入的变体，分别是：状态（符号距离）输入和来自LiDAR的点云输入。在后一种情况下，我们还研究了两种不同的设置：完全和部分观察的环境信息。

    arXiv:2404.01184v1 Announce Type: cross  Abstract: Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time. To address this issue, we propose a new generalizable control barrier function (CBF)-based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT. Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT. CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR. In the latter case, we also study two different settings: fully and partially observed environmental information. Compared to man
    
[^24]: BEM：用于长尾半监督学习的平衡熵混合方法

    BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning

    [https://arxiv.org/abs/2404.01179](https://arxiv.org/abs/2404.01179)

    本文提出了一种名为BEM的平衡熵混合方法，用于长尾半监督学习，旨在重新平衡数据数量和不确定性的类分布。

    

    数据混合方法在半监督学习（SSL）中起着至关重要的作用，但它们在长尾半监督学习（LTSSL）中的应用尚未被探索。本文介绍了Balanced and Entropy-based Mix（BEM），这是一种开创性的混合方法，用于重新平衡数据数量和不确定性的类分布。

    arXiv:2404.01179v1 Announce Type: cross  Abstract: Data mixing methods play a crucial role in semi-supervised learning (SSL), but their application is unexplored in long-tailed semi-supervised learning (LTSSL). The primary reason is that the in-batch mixing manner fails to address class imbalance. Furthermore, existing LTSSL methods mainly focus on re-balancing data quantity but ignore class-wise uncertainty, which is also vital for class balance. For instance, some classes with sufficient samples might still exhibit high uncertainty due to indistinguishable features. To this end, this paper introduces the Balanced and Entropy-based Mix (BEM), a pioneering mixing approach to re-balance the class distribution of both data quantity and uncertainty. Specifically, we first propose a class balanced mix bank to store data of each class for mixing. This bank samples data based on the estimated quantity distribution, thus re-balancing data quantity. Then, we present an entropy-based learning a
    
[^25]: TransFusion：用于高维回归的抗协变量转移学习

    TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression

    [https://arxiv.org/abs/2404.01153](https://arxiv.org/abs/2404.01153)

    提出了一种新型融合正则化器的两步法方法，有效处理高维回归中的模型偏移和协变量转移，提高了目标任务的学习性能，具有稳健性并满足最小-最大最优条件。

    

    传统监督学习与转移学习的主要挑战在于分布偏移，体现为源模型和目标模型之间的偏移以及边际协变量分布之间的偏移。在这项工作中，我们在高维回归设置中处理存在协变量变化的模型变化。具体来说，我们提出了一个两步法方法，使用一种新颖的融合正则化器，有效利用来自源任务的样本来提高在具有有限样本的目标任务上的学习性能。提供了目标模型估计误差的非渐近界限，显示了所提方法对协变量转移的稳健性。我们进一步确定了估计器是最小-最大最优的条件。此外，我们将该方法扩展到分布式设置，允许进行预训练和微调策略，仅需一轮通信即可保留估计

    arXiv:2404.01153v1 Announce Type: cross  Abstract: The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the esti
    
[^26]: LLM是否会对基于事实的问题的人类答案感到困惑？以Reddit为个案研究

    Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit

    [https://arxiv.org/abs/2404.01147](https://arxiv.org/abs/2404.01147)

    LLMs在模拟基于事实问题的社交媒体问题中人类答案时表现较好，为未来研究提供了方向。

    

    大型语言模型（LLMs）已被证明可以熟练地正确回答在线话语环境下的问题。然而，使用LLMs来模拟人类对基于事实的社交媒体问题的回答仍未被充分探讨。在本研究中，我们调查了LLMs如何模拟在几个专题性Reddit社区（或子社区）中提出的基于事实问题的各种人类答案。我们收集并公开了409个基于事实问题和来自15个r/Ask{Topic}社区的7,534个多样化的、经人类评分的答案的数据集，这些社区覆盖了3个类别：职业、社会身份和地理位置。我们发现LLMs在模拟高评分的人类答案方面要比模拟低评分的人类答案效果显著。我们基于我们的初步发现提出了几个未来研究的方向。

    arXiv:2404.01147v1 Announce Type: new  Abstract: Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.
    
[^27]: 在解决时变偏微分方程时进行非线性参数化的逐时间训练

    Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations

    [https://arxiv.org/abs/2404.01145](https://arxiv.org/abs/2404.01145)

    该研究显示逐时间训练方法可以被广泛理解为优化-离散化或离散化-优化方案，在稳定性和误差分析方面取得新颖结果，同时有助于建立方法之间的连接。

    

    逐时间顺序方法解决一系列训练问题，以拟合非线性参数化（如神经网络）以近似解的轨迹随时间变化的偏微分方程。这项工作表明，逐时间训练方法可以广泛理解为优化-离散化（OtD）或离散化-优化（DtO）方案，这在数值分析中是众所周知的概念。统一的观点导致了提出新的稳定性和后验误差分析结果，提供了对OtD或DtO方案本质的理论和数值方面洞察力，例如正切空间坍缩现象，这是一种过拟合形式。此外，统一的观点有助于建立逐时间训练方法的变体之间的连接，通过确定将自然梯度下降方法应用为OtD方案的能量泛函。

    arXiv:2404.01145v1 Announce Type: cross  Abstract: Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied
    
[^28]: SoK: 高维数据中差分私有线性模型的综述

    SoK: A Review of Differentially Private Linear Models For High-Dimensional Data

    [https://arxiv.org/abs/2404.01141](https://arxiv.org/abs/2404.01141)

    本文对高维数据中差分私有线性模型的优化方法进行全面审查，发现鲁棒和优化的坐标算法效果最好，可为未来研究提供参考。

    

    线性模型在数据科学中随处可见，但在高维度中特别容易出现过拟合和数据记忆。为了保证训练数据的隐私性，可以使用差分隐私。许多论文提出了针对高维度差分私有线性模型的优化技术，但这些方法之间缺乏系统比较。我们通过对私有高维线性模型的优化方法进行全面审查来填补这一空白。对所有方法进行的实证测试表明，鲁棒和优化的坐标算法表现最佳，这可为未来研究提供参考。在线发布了所有方法的实现代码。

    arXiv:2404.01141v1 Announce Type: new  Abstract: Linear models are ubiquitous in data science, but are particularly prone to overfitting and data memorization in high dimensions. To guarantee the privacy of training data, differential privacy can be used. Many papers have proposed optimization techniques for high-dimensional differentially private linear models, but a systematic comparison between these methods does not exist. We close this gap by providing a comprehensive review of optimization methods for private high-dimensional linear models. Empirical tests on all methods demonstrate robust and coordinate-optimized algorithms perform best, which can inform future research. Code for implementing all methods is released online.
    
[^29]: 孟买降雨预测的精确度增强：利用物理信息的ConvLSTM2D模型实现更精细的时空分辨率

    Enhanced Precision in Rainfall Forecasting for Mumbai: Utilizing Physics Informed ConvLSTM2D Models for Finer Spatial and Temporal Resolution

    [https://arxiv.org/abs/2404.01122](https://arxiv.org/abs/2404.01122)

    该研究引入了物理信息的ConvLSTM2D模型，旨在提高较细尺度的降雨预测准确性，尤其针对孟买城市的降水预测。

    

    在热带地区预测降雨具有挑战性，由于大气行为复杂、湿度高，且对流性降雨事件普遍存在。在印度的背景下，降雨预测的困难进一步加剧，因为季风季节性振荡会在短期内引入显著的降雨模式变化。本研究引入了深度学习空间模型，旨在提高较细尺度的降雨预测准确性。研究假设融合物理理解可以提高具有较高精度的深度学习模型对较细尺度（如城市）的降水预测能力。为了测试这一假设，我们介绍了一种物理信息的ConvLSTM2D模型，用于预测孟买城市未来6小时和12小时的降水。

    arXiv:2404.01122v1 Announce Type: new  Abstract: Forecasting rainfall in tropical areas is challenging due to complex atmospheric behaviour, elevated humidity levels, and the common presence of convective rain events. In the Indian context, the difficulty is further exacerbated because of the monsoon intra seasonal oscillations, which introduce significant variability in rainfall patterns over short periods. Earlier investigations into rainfall prediction leveraged numerical weather prediction methods, along with statistical and deep learning approaches. This study introduces deep learning spatial model aimed at enhancing rainfall prediction accuracy on a finer scale. In this study, we hypothesize that integrating physical understanding improves the precipitation prediction skill of deep learning models with high precision for finer spatial scales, such as cities. To test this hypothesis, we introduce a physics informed ConvLSTM2D model to predict precipitation 6hr and 12hr ahead for M
    
[^30]: 基于扩散的零样本医学图像到图像翻译用于跨模态分割

    Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation

    [https://arxiv.org/abs/2404.01102](https://arxiv.org/abs/2404.01102)

    使用扩散引导的新型无监督图像翻译方法，解决了零样本跨模态图像分割任务中的挑战

    

    交叉模态图像分割旨在使用在源模态中设计的方法对目标模态进行分割。深度生成模型可以将目标模态图像转换为源模态，从而实现跨模态分割。然而，现有大量交叉模态图像翻译方法依赖于监督学习。本工作旨在解决基于零样本学习的图像翻译任务的挑战（极端情况下目标模态在训练阶段未知）。为了利用生成学习进行零样本跨模态图像分割，我们提出了一种新颖的无监督图像翻译方法。该框架通过利用不同模态之间固有的统计一致性进行扩散引导，学习将未知源图像转换为目标模态以进行图像分割。我们的框架捕捉了相同的跨模态特征...

    arXiv:2404.01102v1 Announce Type: cross  Abstract: Cross-modality image segmentation aims to segment the target modalities using a method designed in the source modality. Deep generative models can translate the target modality images into the source modality, thus enabling cross-modality segmentation. However, a vast body of existing cross-modality image translation methods relies on supervised learning. In this work, we aim to address the challenge of zero-shot learning-based image translation tasks (extreme scenarios in the target modality is unseen in the training phase). To leverage generative learning for zero-shot cross-modality image segmentation, we propose a novel unsupervised image translation method. The framework learns to translate the unseen source image to the target modality for image segmentation by leveraging the inherent statistical consistency between different modalities for diffusion guidance. Our framework captures identical cross-modality features in the statis
    
[^31]: UFID: 一个统一的框架用于扩散模型上的输入级后门检测

    UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models

    [https://arxiv.org/abs/2404.01101](https://arxiv.org/abs/2404.01101)

    扩散模型容易受到后门攻击，本文提出了一个统一的框架用于输入级后门检测，弥补了该领域的空白，并不需要访问模型的白盒信息。

    

    扩散模型容易受到后门攻击，即恶意攻击者在训练阶段通过对部分训练样本进行毒化来注入后门。为了减轻后门攻击的威胁，对后门检测进行了大量研究。然而，没有人为扩散模型设计了专门的后门检测方法，使得这一领域较少被探索。此外，大多数先前的方法主要集中在传统神经网络的分类任务上，很难轻松地将其适应生成任务上的后门检测。此外，大多数先前的方法需要访问模型权重和架构的白盒访问，或概率logits作为额外信息，这并不总是切实可行的。在本文中

    arXiv:2404.01101v1 Announce Type: cross  Abstract: Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper
    
[^32]: 有限样本频域识别

    Finite Sample Frequency Domain Identification

    [https://arxiv.org/abs/2404.01100](https://arxiv.org/abs/2404.01100)

    本研究提出了一种在有限样本情况下进行非参数频域系统识别的方法，通过Empirical Transfer Function Estimate（ETFE）在特定频率处准确估计频率响应，并证明在次高斯彩色噪声和稳定性假设下，ETFE估计值准确可靠。

    

    我们从有限样本的角度研究了非参数频域系统识别。我们假设在开环情况下，激励输入是周期性的，并考虑经验传递函数估计（ETFE），其中目标是在给定输入-输出样本的情况下在某些所需（均匀间隔的）频率处估计频率响应。我们表明在次高斯彩色噪声（在时域）和稳定性假设下，ETFE估计值集中在真实值周围。误差率为$\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$，其中$N_{\mathrm{tot}}$是样本的总数，$M$是所需频率的数量，$d_{\mathrm{u}},\,d_{\mathrm{y}}$分别为输入和输出信号的维数。这个速率对于一般的非理性传递函数仍然有效，并且不需要有限阶的状态空间。

    arXiv:2404.01100v1 Announce Type: cross  Abstract: We study non-parametric frequency-domain system identification from a finite-sample perspective. We assume an open loop scenario where the excitation input is periodic and consider the Empirical Transfer Function Estimate (ETFE), where the goal is to estimate the frequency response at certain desired (evenly-spaced) frequencies, given input-output samples. We show that under sub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE estimates are concentrated around the true values. The error rate is of the order of $\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$, where $N_{\mathrm{tot}}$ is the total number of samples, $M$ is the number of desired frequencies, and $d_{\mathrm{u}},\,d_{\mathrm{y}}$ are the dimensions of the input and output signals respectively. This rate remains valid for general irrational transfer functions and does not require a finite order state-sp
    
[^33]: 你的“安全”数据中有什么？：识别破坏安全性的良性数据

    What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety

    [https://arxiv.org/abs/2404.01099](https://arxiv.org/abs/2404.01099)

    通过双向锚定方法，识别那些在微调后更可能降低模型安全性的良性数据子集，提高模型对有害请求的响应率。

    

    当前的大型语言模型（LLMs），即使经过调整以确保安全性和对齐性，也容易被越狱。一些研究表明，只是进一步使用良性数据（即没有有害内容的数据）对一个对齐模型进行微调，会导致安全性大幅下降。我们深入探讨良性微调不经意间导致越狱的数据中心方面。首先，我们通过两种视角表征微调数据：表示和梯度空间。此外，我们提出了一种双向锚定方法，该方法优先考虑靠近有害示例并远离良性示例的数据点。通过这样做，我们的方法有效地识别出更有可能在微调后降低模型安全性的良性数据子集。仅仅训练100个这些看似良性的数据点，就可以使微调模型肯定地回应超过70％的被测试的有害请求，相比之下，...

    arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to <
    
[^34]: 基于能量模型的准确Shapley值估计用于可解释深度学习预测建模

    Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling

    [https://arxiv.org/abs/2404.01078](https://arxiv.org/abs/2404.01078)

    EmSHAP提出了基于能量模型的Shapley值估计方法，通过引入GRU来消除输入特征顺序的影响，从而可以有效近似任意特征子集下深度学习模型的Shapley值贡献函数。

    

    作为可解释人工智能（XAI）的有利工具，Shapley值已被广泛用于解释基于深度学习的预测模型。然而，由于计算负载随着输入特征的增加呈指数级增长，准确且高效地估计Shapley值是一项困难任务。大多数现有的加速Shapley值估计方法必须在估计准确性和效率之间做出妥协。在本文中，我们提出了EmSHAP（基于能量模型的Shapley值估计），它可以有效地近似预期Shapley贡献函数/深度学习模型在任意特征子集下给出其余特征的情况。为了确定能量模型中的提议条件分布，引入了门控循环单元（GRU），通过将输入特征映射到隐藏空间，从而消除了输入特征顺序的影响。此外，还采用了动态掩蔽方案.

    arXiv:2404.01078v1 Announce Type: new  Abstract: As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is 
    
[^35]: 高分辨率SAR图像中方向性输电塔检测的提示学习

    Prompt Learning for Oriented Power Transmission Tower Detection in High-Resolution SAR Images

    [https://arxiv.org/abs/2404.01074](https://arxiv.org/abs/2404.01074)

    本文将提示学习引入到面向对象检测器（P2Det）中，通过引入局部化和提示位置来提升高分辨率SAR图像中输电塔的检测性能

    

    从合成孔径雷达（SAR）图像中检测输电塔仍然是一个具有挑战性的任务，这是由于输电塔的相对较小尺寸和侧视几何形状，背景杂波干扰经常阻碍了塔的识别。大量的干扰信号叠加在塔的回波信号上。我们发现，定位或提示输电塔的位置有利于解决这一障碍。基于这一发现，本文将提示学习引入到面向对象检测器（P2Det）进行多模态信息学习。P2Det包含了稀疏提示编码和多模态数据之间的交叉注意力。具体来说，提出了稀疏提示编码器（SPE）来表示点位置，将提示转换为稀疏嵌入。图像嵌入是通过Transformer层生成的。然后，提出了一个双向融合模块（TWFM）来计算交叉注意力。

    arXiv:2404.01074v1 Announce Type: cross  Abstract: Detecting transmission towers from synthetic aperture radar (SAR) images remains a challenging task due to the comparatively small size and side-looking geometry, with background clutter interference frequently hindering tower identification. A large number of interfering signals superimposes the return signal from the tower. We found that localizing or prompting positions of power transmission towers is beneficial to address this obstacle. Based on this revelation, this paper introduces prompt learning into the oriented object detector (P2Det) for multimodal information learning. P2Det contains the sparse prompt coding and cross-attention between the multimodal data. Specifically, the sparse prompt encoder (SPE) is proposed to represent point locations, converting prompts into sparse embeddings. The image embeddings are generated through the Transformer layers. Then a two-way fusion module (TWFM) is proposed to calculate the cross-att
    
[^36]: 热力学指导神经网络的单发电机和双发电机形式主义的比较

    A comparison of Single- and Double-generator formalisms for Thermodynamics-Informed Neural Networks

    [https://arxiv.org/abs/2404.01060](https://arxiv.org/abs/2404.01060)

    引入热力学原理到神经网络架构中是一种有效的方式来增加预测准确性和鲁棒性

    

    研究表明，引入归纳偏见是提高神经网络准确性和鲁棒性的一种非常有效的方法，特别是在预测物理现象时。这些偏见显著增加了预测的确定性，降低了误差，并允许使用较小的数据集。文献中有许多方法可以开发这些偏见。在处理物理现象时，其中一种最有效的方法是将被认可的物理原则引入网络架构。

    arXiv:2404.01060v1 Announce Type: new  Abstract: The development of inductive biases has been shown to be a very effective way to increase the accuracy and robustness of neural networks, particularly when they are used to predict physical phenomena. These biases significantly increase the certainty of predictions, decrease the error made and allow considerably smaller datasets to be used.   There are a multitude of methods in the literature to develop these biases. One of the most effective ways, when dealing with physical phenomena, is to introduce physical principles of recognised validity into the network architecture.   The problem becomes more complex without knowledge of the physical principles governing the phenomena under study. A very interesting possibility then is to turn to the principles of thermodynamics, which are universally valid, regardless of the level of abstraction of the description sought for the phenomenon under study.   To ensure compliance with the principles 
    
[^37]: 音乐流派识别中一种新颖的音频表示

    A Novel Audio Representation for Music Genre Identification in MIR

    [https://arxiv.org/abs/2404.01058](https://arxiv.org/abs/2404.01058)

    本研究探索了一种新颖的音频表示形式，但发现其在音乐流派识别任务中并不比传统的Mel频谱图更优越。

    

    对于音乐信息检索的下游任务，最常见的音频表示形式是基于时频的表示，例如Mel频谱图。为了识别音乐流派，本研究探索了一种新形式的音频表示，用于其中最常见的MIR下游任务之一。因此，为了使用深度向量量化离散地编码音乐；为创新的生成式音乐模型Jukebox创建了一种新颖的音频表示。通过使用几乎等同于业界领先水平(SOTA)和几乎相同的变压器设计的数据集来比较Jukebox的音频表示与Mel频谱图的有效性。本研究结果暗示，至少当变压器使用非常适度的数据集（20k首音轨）进行预训练时，Jukebox的音频表示并不优于Mel频谱图。这可能是因为Jukebox的音频表示并未充分考虑到

    arXiv:2404.01058v1 Announce Type: cross  Abstract: For Music Information Retrieval downstream tasks, the most common audio representation is time-frequency-based, such as Mel spectrograms. In order to identify musical genres, this study explores the possibilities of a new form of audio representation one of the most usual MIR downstream tasks. Therefore, to discretely encoding music using deep vector quantization; a novel audio representation was created for the innovative generative music model i.e. Jukebox. The effectiveness of Jukebox's audio representation is compared to Mel spectrograms using a dataset that is almost equivalent to State-of-the-Art (SOTA) and an almost same transformer design. The results of this study imply that, at least when the transformers are pretrained using a very modest dataset of 20k tracks, Jukebox's audio representation is not superior to Mel spectrograms. This could be explained by the fact that Jukebox's audio representation does not sufficiently take
    
[^38]: 通过扩散语义传播实现交互式基于点的编辑

    Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation

    [https://arxiv.org/abs/2404.01050](https://arxiv.org/abs/2404.01050)

    DragNoise通过利用U-Net预测的噪声输出作为语义编辑器，在不重新追踪潜在地图的情况下，实现了稳健且加速的编辑。

    

    点形式的交互式编辑作为一种重要工具，用于补充现有生成模型的可控性。我们提出了DragNoise，通过利用每个U-Net的预测噪声输出作为语义编辑器，实现了稳健且加速的编辑，而无需重追踪潜在地图。

    arXiv:2404.01050v1 Announce Type: cross  Abstract: Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion sema
    
[^39]: 一种优化的星系分类的新型基于区块的算法

    A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification

    [https://arxiv.org/abs/2404.01049](https://arxiv.org/abs/2404.01049)

    提出了一种基于区块的新型星系分类算法，利用SDSS数据和卷积神经网络，在星系分类领域取得了最新的性能表现。

    

    本文介绍了一种新颖的基于区块的星系分类方法，利用最新的Sloan数字天文数据库数据（SDSS-DR18）。通过将天空战略性地分割成与SDSS观测模式对齐的区块，并利用专用卷积神经网络（CNN），我们实现了星系分类的最新性能。我们的初步结果展示了一条在实时观测设置中进行高效和精确天文分析的有前途的道路。

    arXiv:2404.01049v1 Announce Type: cross  Abstract: This paper introduces a novel sector-based methodology for star-galaxy classification, leveraging the latest Sloan Digital Sky Survey data (SDSS-DR18). By strategically segmenting the sky into sectors aligned with SDSS observational patterns and employing a dedicated convolutional neural network (CNN), we achieve state-of-the-art performance for star galaxy classification. Our preliminary results demonstrate a promising pathway for efficient and precise astronomical analysis, especially in real-time observational settings.
    
[^40]: LLM可以在不透露私人信息的情况下获得其他LLM的帮助吗？

    Can LLMs get help from other LLMs without revealing private information?

    [https://arxiv.org/abs/2404.01041](https://arxiv.org/abs/2404.01041)

    本研究展示了在级联系统中运用隐私保护技术的可行性，以减少在查询远程模型时泄漏私人信息的风险，并引入了两个隐私度量。

    

    级联是一种常见类型的机器学习系统，其中如果本地模型无法单独准确标记用户数据，则可以查询一个大型的远程模型。对于大型语言模型（LLMs），由于其在显著降低推断成本的同时保持任务性能的能力，服务堆栈越来越多地使用级联。然而，在本地模型可以访问敏感数据的情况下应用级联系统构成用户的重大隐私风险，因为这些数据可能被转发到远程模型。在这项工作中，我们展示了在此类设置中应用级联系统的可行性，方法是为本地模型配备隐私保护技术，从而减少访问远程模型时泄漏私人信息的风险。为了量化此类设置中的信息泄漏，我们引入了两个隐私度量。然后，我们提出了一个利用最近引入的社交学习范式的系统

    arXiv:2404.01041v1 Announce Type: cross  Abstract: Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm 
    
[^41]: 基于超图神经网络的调查：深入和分步指南

    A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide

    [https://arxiv.org/abs/2404.01039](https://arxiv.org/abs/2404.01039)

    该研究是第一份致力于超图神经网络的调查报告，深入介绍了HNN架构、训练策略和应用，为从事数据挖掘和机器学习的研究人员提供了重要的指导。

    

    高阶相互作用在现实世界中的复杂系统和应用中无处不在，因此，对于高阶相互作用的深度学习已成为数据挖掘和机器学习社区的一项重要议程。由于高阶相互作用网络在数学上被表达为超图，超图神经网络（HNNs）已成为在超图上进行表示学习的强大工具。鉴于这一新趋势，我们提出了首份致力于HNNs的调查报告，其中包含深入和分步指南。总体而言，本调查概述了HNN架构、训练策略和应用。首先，我们将现有的HNNs分解为四个设计组成部分：（i）输入特征，（ii）输入结构，（iii）消息传递方案，和（iv）训练策略。其次，我们考察HNNs如何通过各自的组成部分处理和学习高阶相互作用。第三，我们总结了HNNs在推荐、生物和医学中的最新应用。

    arXiv:2404.01039v1 Announce Type: new  Abstract: Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications, and thus investigation of deep learning for HOIs has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, biological and med
    
[^42]: 在生成人工智能工具时代的高等教育评估实践

    Higher education assessment practice in the era of generative AI tools

    [https://arxiv.org/abs/2404.01036](https://arxiv.org/abs/2404.01036)

    该研究评估了生成人工智能工具对高等教育评估实践的影响，发现这些工具展示了学科知识、问题解决、分析能力等技能，但在不道德使用时可能限制学习，同时也揭示了部分学科评估中这些工具的局限性。

    

    高等教育（HE）部门对每个国家的经济和社会都有益处，但它们的贡献受到生成人工智能（GenAI）等先进技术的挑战。本文全面评估了GenAI工具对评估和教学实践的影响，并随后讨论了潜在影响。研究使用了数据科学、数据分析和建筑管理学科的三种评估工具进行实验。研究发现有两个方面：首先，结果显示GenAI工具展示了学科知识、问题解决能力、分析能力、批判性思维和表达能力，因此在不道德使用时可能会限制学习。其次，某些学科评估的设计揭示了GenAI工具的局限性。根据研究结果，我们提出了关于如何在高等教育中利用人工智能工具进行教学与学习的建议。

    arXiv:2404.01036v1 Announce Type: cross  Abstract: The higher education (HE) sector benefits every nation's economy and society at large. However, their contributions are challenged by advanced technologies like generative artificial intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made recommendations on how AI tools can be utilised for teaching and learning in HE.
    
[^43]: 使用大型语言模型生成的相关性判断来预测查询性能

    Query Performance Prediction using Relevance Judgments Generated by Large Language Models

    [https://arxiv.org/abs/2404.01012](https://arxiv.org/abs/2404.01012)

    提出了一种使用自动生成的相关性判断的查询性能预测框架，能够解决先前方法中对不同IR评估指标准确性和解释性的限制。

    

    查询性能预测（QPP）旨在估计搜索系统对查询的检索质量，而无需人工相关性判断。先前的QPP方法通常返回单个标量值，并不要求预测值接近特定的信息检索（IR）评估指标，从而导致以下某些缺点：（i）单个标量无法准确表示不同的IR评估指标，特别是当度量不高度相关时，（ii）单个标量限制了QPP方法的可解释性，因为仅使用标量无法解释QPP结果。为解决这些问题，我们提出了一个使用自动生成的相关性判断的QPP框架（QPP-GenRE），将QPP分解为独立的子任务，即对排名列表中每个项目对给定查询的相关性进行判断。这样我们可以使用生成的相关性判断来预测任何IR评估指标。

    arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
    
[^44]: 通过C-Flat使持续学习更强大

    Make Continual Learning Stronger via C-Flat

    [https://arxiv.org/abs/2404.00986](https://arxiv.org/abs/2404.00986)

    通过C-Flat方法，我们提出了一种更平坦的损失景观，可用于持续学习，简化了模型训练过程并提高了模型泛化能力。

    

    持续学习中模型的泛化能力对于处理连续到达任务的动态更新知识是至关重要的，为了解决持续学习中的敏感性-稳定性困境。研究证明，通过最小化权重损失景观的陡峭度，寻找位于具有统一低损失或平稳梯度的邻域中的平坦最小值，是一种强大的训练方式，相较于基于损失最小化的优化器如SGD来提高模型的泛化性。然而，只有少数作品讨论了这种训练方式在持续学习中的应用，证明特定设计的零阶陡峭度优化器可以提升持续学习性能。在这项工作中，我们提出了一种名为Continual Flatness（C-Flat）的方法，具有为持续学习定制的更平坦的损失景观。C-Flat只需一行代码即可轻松调用，并可与任何持续学习方法插播。C-Flat应用于所有持续学习类别的一般框架，并与损失最小化优化器进行了彻底比较。

    arXiv:2404.00986v1 Announce Type: new  Abstract: Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD. Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer an
    
[^45]: 智慧城市的持续学习：一项调研

    Continual Learning for Smart City: A Survey

    [https://arxiv.org/abs/2404.00983](https://arxiv.org/abs/2404.00983)

    该调研综合审查了智慧城市发展中广泛使用的持续学习方法，内容涵盖了方法论分类、多种应用领域和相关数据集。

    

    随着现代城市数字化，庞大的数据量和强大的计算资源促使智慧城市中部署的智能模型迅速更新。持续学习（CL）是一种新颖的机器学习范式，不断更新模型以适应变化的环境，其中学习任务、数据和分布可以随时间变化。我们的调查全面审查了在智慧城市发展中广泛使用的持续学习方法。内容分为三个部分：1）方法论。我们将大量基本的CL方法和结合其他学习范例的高级CL框架进行分类，包括图学习、时空学习、多模态学习和联邦学习。2）应用方面。我们介绍了涵盖交通、环境、公共卫生、安全、网络以及与城市计算相关的相关数据集的众多CL应用。3）

    arXiv:2404.00983v1 Announce Type: cross  Abstract: With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3
    
[^46]: 基于扩散驱动的领域自适应生成3D分子

    Diffusion-Driven Domain Adaptation for Generating 3D Molecules

    [https://arxiv.org/abs/2404.00962](https://arxiv.org/abs/2404.00962)

    该研究提出了一种基于扩散的领域自适应分子生成方法 GADM，可以在不需要收集数据的情况下将生成模型迁移到新领域，通过利用等变蒙板自编码器和各种掩蔽策略来捕获结构变化所带来的领域差异，并能够泛化到目标领域中看不见的结构变化。

    

    我们能否训练一个分子生成器，可以生成来自新领域的3D分子，从而避免收集数据的需求？这个问题可以被视为领域自适应分子生成的问题。本文提出了一种新颖且基于原则的基于扩散的方法，称为GADM，它允许将生成模型移至所需的新领域，而无需收集任何一个分子。由于领域转移通常由分子的结构变化引起，例如骨架变化，我们利用指定的等变蒙板自编码器（MAE）以及各种掩蔽策略来捕获领域内变体的结构精细表示。特别是，通过一个不对称的编码器-解码器模块，MAE可以泛化到目标领域中看不见的结构变化。这些结构变化被编码为等变编码器，并被视为领域监督员来控制去噪。

    arXiv:2404.00962v1 Announce Type: new  Abstract: Can we train a molecule generator that can generate 3D molecules from a new domain, circumventing the need to collect data? This problem can be cast as the problem of domain adaptive molecule generation. This work presents a novel and principled diffusion-based approach, called GADM, that allows shifting a generative model to desired new domains without the need to collect even a single molecule. As the domain shift is typically caused by the structure variations of molecules, e.g., scaffold variations, we leverage a designated equivariant masked autoencoder (MAE) along with various masking strategies to capture the structural-grained representations of the in-domain varieties. In particular, with an asymmetric encoder-decoder module, the MAE can generalize to unseen structure variations from the target domains. These structure variations are encoded with an equivariant encoder and treated as domain supervisors to control denoising. We s
    
[^47]: 使用大规模知识图谱评估大型语言模型的事实性

    Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs

    [https://arxiv.org/abs/2404.00942](https://arxiv.org/abs/2404.00942)

    使用大型知识图谱创建评估模型以检查大型语言模型在事实性上的表现，有效降低评估成本。

    

    大型语言模型（LLMs）的出现显著改变了人工智能领域，增强了机器学习和人工智能的能力。事实性问题对LLMs来说是一个关键问题，因为它们可能生成事实不准确的响应。本文提出了GraphEval，通过大规模测试数据集对LLM的性能进行评估。具体而言，测试数据集是从拥有超过1000万个事实的大型知识图谱中检索而来，无需昂贵的人力成本。与基于生成响应评估LLMs的传统方法不同，GraphEval通过创建一个评估模型简化了评估过程，用于估计LLM给出的答案的正确性。我们的实验表明，评估模型的事实性评估与LLM生成的输出的正确性密切相关，同时显著降低了评估成本。此外，我们的发现为LLM的性能提供了宝贵的洞见。

    arXiv:2404.00942v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM per
    
[^48]: 视觉Transformer的实例感知组量化

    Instance-Aware Group Quantization for Vision Transformers

    [https://arxiv.org/abs/2404.00928](https://arxiv.org/abs/2404.00928)

    IGQ-ViT是一种面向视觉Transformer的实例感知组量化方法，通过动态地将激活映射的通道分成多个组，使得每个输入实例内的激活具有相似统计特性。

    

    后训练量化（PTQ）是一种高效的模型压缩技术，它使用仅有少量未标记样本的校准集对预训练的全精度模型进行量化，而无需重新训练。对卷积神经网络（CNNs）的PTQ方法提供了与全精度对应物可比的量化结果。然而，直接将它们应用于视觉Transformer（ViTs），会导致严重的性能下降，主要是由于CNNs和ViTs之间的架构差异。特别是，每个通道的激活分布根据输入实例大大变化，使得CNNs的PTQ方法不适用于ViTs。为了解决这个问题，我们引入了适用于ViTs的实例感知组量化（IGQ-ViT）。为此，我们提出将激活映射的通道动态地分成多个组，以便为每个输入实例，使得每组内的激活具有相似的统计特性。

    arXiv:2404.00928v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) is an efficient model compression technique that quantizes a pretrained full-precision model using only a small calibration set of unlabeled samples without retraining. PTQ methods for convolutional neural networks (CNNs) provide quantization results comparable to full-precision counterparts. Directly applying them to vision transformers (ViTs), however, incurs severe performance degradation, mainly due to the differences in architectures between CNNs and ViTs. In particular, the distribution of activations for each channel vary drastically according to input instances, making PTQ methods for CNNs inappropriate for ViTs. To address this, we introduce instance-aware group quantization for ViTs (IGQ-ViT). To this end, we propose to split the channels of activation maps into multiple groups dynamically for each input instance, such that activations within each group share similar statistical properties. We
    
[^49]: 大型语言模型中的高效令牌利用学习

    Token-Efficient Leverage Learning in Large Language Models

    [https://arxiv.org/abs/2404.00914](https://arxiv.org/abs/2404.00914)

    介绍了一种名为Token-Efficient Leverage Learning（TELL）的方法，在大型语言模型中展示了其降低任务数据需求、提高任务性能的潜力，为低资源任务带来了竞争性能。

    

    大型语言模型（LLMs）在各种任务中表现出色，但在高资源场景中表现更佳，这在低资源场景中存在挑战。数据稀缺和LLMs适应特定任务固有的困难加剧了这一挑战。为了解决这两大难题，我们引入了\textbf{Leverage Learning}。我们提出了这一方法的简化实现，称为Token-Efficient Leverage Learning (TELL)。TELL展示了Leverage Learning的潜力，证明它在各种LLMs和低资源任务中的有效性，从$10^4$到$10^6$个令牌不等。与传统的监督微调（SFT）相比，它将任务数据需求降低了近一个数量级，同时提供竞争力的性能。在相同量的任务数据情况下，TELL在改善任务性能方面领先于SFT。我们讨论了Leverage Learning的机制，暗示其符合量化假设。

    arXiv:2404.00914v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypoth
    
[^50]: CAAP：基于自适应策略的类别依赖性自动数据增强方法用于时间序列

    CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series

    [https://arxiv.org/abs/2404.00898](https://arxiv.org/abs/2404.00898)

    该研究提出了一种名为类别依赖性自动自适应策略（CAAP）的新颖基于深度学习的方法，旨在解决时间序列中存在的类别依赖性偏差问题，特别关注心电图等重要信号在医疗领域中的应用潜力。

    

    数据增强是一种常见技术，通过扩展训练数据集来提升深度学习模型的性能。自动数据增强（ADA）方法因能够生成适用于各种数据集的策略而变得流行。然而，现有ADA方法主要关注整体性能改进，忽视了导致特定类别性能下降的类别依赖性偏差问题。这种偏差在将模型部署到实际应用中时带来重大挑战。此外，针对时间序列的ADA仍是一个未深入探讨的领域，突显了这一领域的发展需求。特别是将ADA技术应用于心电图等重要信号是一个引人注目的例子，由于其在心脏病诊断等医疗领域的潜力。

    arXiv:2404.00898v1 Announce Type: new  Abstract: Data Augmentation is a common technique used to enhance the performance of deep learning models by expanding the training dataset. Automatic Data Augmentation (ADA) methods are getting popular because of their capacity to generate policies for various datasets. However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes. This bias poses significant challenges when deploying models in real-world applications. Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field. In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics.   We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAA
    
[^51]: 机器学习鲁棒性：入门指南

    Machine Learning Robustness: A Primer

    [https://arxiv.org/abs/2404.00897](https://arxiv.org/abs/2404.00897)

    该章节探讨了机器学习中稳健性的重要概念及关键的技术和因素，以确立人工智能系统的可信度。

    

    这一章节探讨了机器学习（ML）中稳健性的基本概念及其在确立人工智能（AI）系统的可信度中的重要作用。讨论始于稳健性的详细定义，将其描述为ML模型在各种不同和意外的环境条件下保持稳定性能的能力。ML鲁棒性通过多个视角进行了剖析：其与泛化能力的互补性；其作为可信AI的要求；其对抗性与非对抗性方面；其数量化指标；以及其可复现性和可解释性等指标。章节深入探讨了影响鲁棒性的因素，如数据偏差、模型复杂性以及ML流程不明确的风险。它从广泛的视角调查了鲁棒性评估的关键技术，包括对抗性攻击，包括数字和物理领域。

    arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
    
[^52]: 用反馈机制建模多任务学习中的输出级任务相关性

    Modeling Output-Level Task Relatedness in Multi-Task Learning with Feedback Mechanism

    [https://arxiv.org/abs/2404.00885](https://arxiv.org/abs/2404.00885)

    在多任务学习中引入反馈机制，将一个任务的输出作为另一个任务的隐藏特征，使静态的多任务学习模型转变为动态模型。

    

    多任务学习（MTL）是一种通过在不同层次共享信息来同时学习多个任务的范式，增强每个单独任务的性能。我们旨在探索输出级任务相关性，通过将后验信息引入模型，考虑到不同任务可能产生相关的相互影响的输出。

    arXiv:2404.00885v1 Announce Type: new  Abstract: Multi-task learning (MTL) is a paradigm that simultaneously learns multiple tasks by sharing information at different levels, enhancing the performance of each individual task. While previous research has primarily focused on feature-level or parameter-level task relatedness, and proposed various model architectures and learning algorithms to improve learning performance, we aim to explore output-level task relatedness. This approach introduces a posteriori information into the model, considering that different tasks may produce correlated outputs with mutual influences. We achieve this by incorporating a feedback mechanism into MTL models, where the output of one task serves as a hidden feature for another task, thereby transforming a static MTL model into a dynamic one. To ensure the training process converges, we introduce a convergence loss that measures the trend of a task's outputs during each iteration. Additionally, we propose a 
    
[^53]: 基于锚图张量分解的可解释多视图聚类

    Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization

    [https://arxiv.org/abs/2404.00883](https://arxiv.org/abs/2404.00883)

    使用非负张量因子分解解决了基于锚图的多视图聚类方法缺乏聚类可解释性和忽视视图间信息的问题

    

    基于锚图的聚类方法因其出色的聚类性能和处理大规模数据的能力而备受关注。一种常见的方法是学习具有K个连接组件的二部图，有助于避免后处理的需要。然而，现有基于锚图因子化的多视图聚类方法缺乏对分解矩阵的充分聚类可解释性，并经常忽视视图间信息。我们通过使用非负张量因子分解来解决这一限制，以分解结合了多视图锚图的锚图张量。这一方法使我们能够同时考虑视图间的相关性，并获得更具解释性的聚类结果。

    arXiv:2404.00883v1 Announce Type: new  Abstract: The clustering method based on the anchor graph has gained significant attention due to its exceptional clustering performance and ability to process large-scale data. One common approach is to learn bipartite graphs with K-connected components, helping avoid the need for post-processing. However, this method has strict parameter requirements and may not always get K-connected components. To address this issue, an alternative approach is to directly obtain the cluster label matrix by performing non-negative matrix factorization (NMF) on the anchor graph. Nevertheless, existing multi-view clustering methods based on anchor graph factorization lack adequate cluster interpretability for the decomposed matrix and often overlook the inter-view information. We address this limitation by using non-negative tensor factorization to decompose an anchor graph tensor that combines anchor graphs from multiple views. This approach allows us to conside
    
[^54]: 度量学习以加速可微参数规划的算子分裂方法收敛

    Metric Learning to Accelerate Convergence of Operator Splitting Methods for Differentiable Parametric Programming

    [https://arxiv.org/abs/2404.00882](https://arxiv.org/abs/2404.00882)

    该研究提出了一种新方法，通过学习优化算子分裂算法的度量空间，从而最大化其收敛速度，特别适用于二次规划问题。

    

    最近的研究表明机器学习可以用于加速解决约束优化问题的多种方法。在诸如人工智能和最优控制等应用中对实时决策能力的不断增强，已导致基于不同策略的各种方法的出现。本研究提出了一种学习优化的新方法，其中学习了一个基于近端算子分裂算法的度量空间，以最大化其收敛速度。虽然之前的优化理论研究得出了一些特定问题类的最优度量，但这些结果并不能推广到许多实际问题形式，包括一般的二次规划（QP）问题。本文展示了可微优化如何实现端到端学习近端度量，提升近端算法在QP问题上的收敛性，超越了基于已知理论的可能性。

    arXiv:2404.00882v1 Announce Type: new  Abstract: Recent work has shown a variety of ways in which machine learning can be used to accelerate the solution of constrained optimization problems. Increasing demand for real-time decision-making capabilities in applications such as artificial intelligence and optimal control has led to a variety of approaches, based on distinct strategies. This work proposes a novel approach to learning optimization, in which the underlying metric space of a proximal operator splitting algorithm is learned so as to maximize its convergence rate. While prior works in optimization theory have derived optimal metrics for limited classes of problems, the results do not extend to many practical problem forms including general Quadratic Programming (QP). This paper shows how differentiable optimization can enable the end-to-end learning of proximal metrics, enhancing the convergence of proximal algorithms for QP problems beyond what is possible based on known theo
    
[^55]: 重新思考循环神经网络和非循环神经网络之间的关系：稀疏性研究

    Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity

    [https://arxiv.org/abs/2404.00880](https://arxiv.org/abs/2404.00880)

    循环神经网络（RNN）与其他类型的神经网络之间存在更紧密的关系，这种关系比我们通常认为的更实际，并且具有更深层次的联系。

    

    神经网络（NN）可以分为两大类，即循环和非循环。这两种神经网络类型都很受欢迎并广泛研究，但通常将它们视为机器学习算法的不同家族。在这篇立场论文中，我们认为循环神经网络和其他类型的神经网络之间存在比通常认识更紧密的关系。我们展示了许多常见的神经网络模型，如循环神经网络（RNN）、多层感知器（MLP）甚至深度多层变压器都可以表示为迭代映射。

    arXiv:2404.00880v1 Announce Type: new  Abstract: Neural networks (NN) can be divided into two broad categories, recurrent and non-recurrent. Both types of neural networks are popular and extensively studied, but they are often treated as distinct families of machine learning algorithms. In this position paper, we argue that there is a closer relationship between these two types of neural networks than is normally appreciated. We show that many common neural network models, such as Recurrent Neural Networks (RNN), Multi-Layer Perceptrons (MLP), and even deep multi-layer transformers, can all be represented as iterative maps.   The close relationship between RNNs and other types of NNs should not be surprising. In particular, RNNs are known to be Turing complete, and therefore capable of representing any computable function (such as any other types of NNs), but herein we argue that the relationship runs deeper and is more practical than this. For example, RNNs are often thought to be mor
    
[^56]: Lipsum-FT: 使用随机文本引导进行零样本模型的稳健微调

    Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance

    [https://arxiv.org/abs/2404.00860](https://arxiv.org/abs/2404.00860)

    本研究提出了一种名为 Lipsum-FT 的算法，通过有效利用视觉-语言预训练模型的语言建模，实现了在分布转移场景下对零样本模型进行稳健微调，提高了下游任务的性能。

    

    大规模对比视觉-语言预训练模型为零样本模型提供了在一系列图像分类任务上取得竞争性表现的能力，而无需在下游数据上进行训练。我们的调查从研究需要达到稳健微调目标的条件开始，采用了基于特征失真理论和联合能量模型的描述。随后，我们提出了一种新颖的稳健微调算法 Lipsum-FT，有效利用视觉-语言预训练模型的语言建模方面。在 DomainNet 和 ImageNet 上进行的分布转移场景的大量实验验证了我们提出的 Lipsum-FT 方法的优越性。

    arXiv:2404.00860v1 Announce Type: new  Abstract: Large-scale contrastive vision-language pre-trained models provide the zero-shot model achieving competitive performance across a range of image classification tasks without requiring training on downstream data. Recent works have confirmed that while additional fine-tuning of the zero-shot model on the reference data results in enhanced downstream performance, it compromises the model's robustness against distribution shifts. Our investigation begins by examining the conditions required to achieve the goals of robust fine-tuning, employing descriptions based on feature distortion theory and joint energy-based models. Subsequently, we propose a novel robust fine-tuning algorithm, Lipsum-FT, that effectively utilizes the language modeling aspect of the vision-language pre-trained models. Extensive experiments conducted on distribution shift scenarios in DomainNet and ImageNet confirm the superiority of our proposed Lipsum-FT approach over
    
[^57]: 语言模型是否提前为未来标记进行规划？

    Do language models plan ahead for future tokens?

    [https://arxiv.org/abs/2404.00859](https://arxiv.org/abs/2404.00859)

    语言模型在推理过程中会提前准备未来标记所需的信息，可能是通过预缓存或面包屑的方式实现。

    

    arXiv:2404.00859v1 公告类型：跨领域 摘要：在给定位置的推理过程中，变压器是否会“提前思考”？已知变压器在$t$的前向传递的隐藏状态中准备信息，然后在未来的前向传递$t+\tau$中使用。我们提出了两种解释这种现象的可能性：预缓存，即训练中存在的非对角梯度项导致模型在$t$计算与当前推理任务无关但对未来有用的特征，以及面包屑，即与时间步长$t$最相关的特征已经与那些将最有利于时间步长$t+\tau$的特征相同。我们通过训练不将梯度传播到过去时间步的语言模型来测试这些假设，这种方案我们正式称为短视训练。在合成数据设置中，我们发现了预缓存的明确证据。在自回归语言建模设置中，我们的实验更多地支持了面包屑假设。

    arXiv:2404.00859v1 Announce Type: cross  Abstract: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.
    
[^58]: 针对越南城市环境的集成学习用于场景文字识别

    Ensemble Learning for Vietnamese Scene Text Spotting in Urban Environments

    [https://arxiv.org/abs/2404.00852](https://arxiv.org/abs/2404.00852)

    该方法提出了一个简单而高效的集成学习框架，可以显著提升在具有挑战性的城市环境中的越南场景文字识别性能。

    

    本文提出了一个简单而高效的集成学习框架，用于越南场景文字识别。利用集成学习的强大优势，结合多个模型以提高预测准确性，我们的方法旨在显著提升在具有挑战性的城市环境中的场景文字识别性能。通过对VinText数据集进行实验评估，我们提出的方法在准确性上相较现有方法实现了显著改进，达到了惊人的5%准确性。这些结果无疑展示了集成学习在越南城市环境下场景文字识别中的功效，突显其在城市标识、广告以及各种富含文字的城市场景等实际应用中的潜力。

    arXiv:2404.00852v1 Announce Type: cross  Abstract: This paper presents a simple yet efficient ensemble learning framework for Vietnamese scene text spotting. Leveraging the power of ensemble learning, which combines multiple models to yield more accurate predictions, our approach aims to significantly enhance the performance of scene text spotting in challenging urban settings. Through experimental evaluations on the VinText dataset, our proposed method achieves a significant improvement in accuracy compared to existing methods with an impressive accuracy of 5%. These results unequivocally demonstrate the efficacy of ensemble learning in the context of Vietnamese scene text spotting in urban environments, highlighting its potential for real world applications, such as text detection and recognition in urban signage, advertisements, and various text-rich urban scenes.
    
[^59]: 决策政策在混杂情况下的预测性能比较

    Predictive Performance Comparison of Decision Policies Under Confounding

    [https://arxiv.org/abs/2404.00848](https://arxiv.org/abs/2404.00848)

    提出了一种方法，通过现代识别方法比较决策政策的预测性能，关键在于可以安全忽略不确定性区域。

    

    预测模型通常被引入决策任务中，其基本理念是它们可以提升决策政策的性能。然而，与通常存在于未明确规定和依赖不可观测因素的现有决策政策相比较预测性能是具有挑战性的。这些不确定性来源通常在实践中被通过对数据生成机制进行强假设来处理。在这项研究中，我们提出了一种方法，来比较决策政策的预测性能，根据因果推断和离线评估文献中的各种现代识别方法进行评估（例如，工具变量，边际敏感性模型，近端变量）。我们的方法的关键是我们可以安全地忽略政策比较中的不确定性区域。我们开发了一种有限样本估计遗憾区间的实用方法。

    arXiv:2404.00848v1 Announce Type: new  Abstract: Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals u
    
[^60]: 基于点变换器的迁移学习

    Transfer Learning with Point Transformers

    [https://arxiv.org/abs/2404.00846](https://arxiv.org/abs/2404.00846)

    基于点变换器的迁移学习模型在处理点云数据分类等任务时表现良好，但在不同数据集上迁移学习效果并不总是优于从头开始训练模型，因为数据集之间的分布差异较大。

    

    点变换器是近乎处于最先进水平的模型，用于处理点云数据上的分类、分割和检测任务。它们利用基于自注意力机制来模拟多个点集之间的大范围空间依赖关系。在这个项目中，我们探讨了两件事：这些基于注意力的网络在ModelNet10数据集上的分类性能，然后，我们使用训练好的模型来对3D MNIST数据集进行分类微调。我们还从头开始训练模型在3D MNIST数据集上，以比较微调和从头开始模型在MNIST数据集上的性能。我们观察到，由于这两个数据集在分布程度上存在很大的差异，迁移学习模型在这种情况下并没有超越从头开始模型。尽管我们预期迁移学习模型会收敛更快，因为它们已经从ModelNet10数据集中知道了边缘、角等底层特征。

    arXiv:2404.00846v1 Announce Type: cross  Abstract: Point Transformers are near state-of-the-art models for classification, segmentation, and detection tasks on Point Cloud data. They utilize a self attention based mechanism to model large range spatial dependencies between multiple point sets. In this project we explore two things: classification performance of these attention based networks on ModelNet10 dataset and then, we use the trained model to classify 3D MNIST dataset after finetuning. We also train the model from scratch on 3D MNIST dataset to compare the performance of finetuned and from-scratch model on the MNIST dataset. We observe that since the two datasets have a large difference in the degree of the distributions, transfer learned models do not outperform the from-scratch models in this case. Although we do expect transfer learned models to converge faster since they already know the lower level edges, corners, etc features from the ModelNet10 dataset.
    
[^61]: 使用深度学习和金字塔采样在乳腺癌图像中自动进行HER2评分

    Automated HER2 Scoring in Breast Cancer Images Using Deep Learning and Pyramid Sampling

    [https://arxiv.org/abs/2404.00837](https://arxiv.org/abs/2404.00837)

    使用深度学习和金字塔采样的方法实现了在乳腺癌组织图像中对HER2状态的自动分类，有效管理计算负荷并有效地分析细胞和更大范围的组织细节。

    

    人表皮生长因子受体2（HER2）是癌细胞增长中至关重要的蛋白质，标志着乳腺癌（BC）的侵略性并有助于预测其预后。对于HER2在免疫组织化学（IHC）染色组织切片中表达水平的准确评估对于治疗指导和了解癌症机制至关重要。然而，传统的由董事会认证的病理学家手动检查的工作流程面临挑战，包括观察者间和观察者内的不一致性以及延长的周转时间。在这里，我们介绍了一种利用金字塔采样的基于深度学习的方法，用于自动对IHC染色的BC组织图像中的HER2状态进行分类。我们的方法分析不同空间尺度下的形态特征，有效管理计算负荷，并促进对细胞和更大范围组织水平细节的详细检查。该方法解决了

    arXiv:2404.00837v1 Announce Type: cross  Abstract: Human epidermal growth factor receptor 2 (HER2) is a critical protein in cancer cell growth that signifies the aggressiveness of breast cancer (BC) and helps predict its prognosis. Accurate assessment of immunohistochemically (IHC) stained tissue slides for HER2 expression levels is essential for both treatment guidance and understanding of cancer mechanisms. Nevertheless, the traditional workflow of manual examination by board-certified pathologists encounters challenges, including inter- and intra-observer inconsistency and extended turnaround times. Here, we introduce a deep learning-based approach utilizing pyramid sampling for the automated classification of HER2 status in IHC-stained BC tissue images. Our approach analyzes morphological features at various spatial scales, efficiently managing the computational load and facilitating a detailed examination of cellular and larger-scale tissue-level details. This method addresses the
    
[^62]: 在边缘学习中重新思考资源管理：一种联合预训练和微调设计范式

    Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm

    [https://arxiv.org/abs/2404.00836](https://arxiv.org/abs/2404.00836)

    该论文提出了一种边缘学习中联合预训练和微调设计范式，通过分析系统参数对收敛速率的影响，并提出了联合通信和计算资源管理方案。

    

    在一些应用中，边缘学习正经历着从传统的从头开始学习转向统一预训练和特定任务微调的新两阶段学习。本文考虑了在两阶段边缘学习系统中进行联合通信和计算资源管理的问题。在这个系统中，模型预训练首先通过边缘服务器在本地预存的通用数据上进行集中式学习，然后基于预训练模型通过联邦边缘学习在边缘设备上执行特定任务微调。对于两阶段学习模型，我们首先分析了收敛行为（以平均平方梯度范数界形式），这表征了各种系统参数（如两阶段中学习轮数和批次大小）对收敛速率的影响。基于我们的分析结果，我们提出了一种联合通信和计算

    arXiv:2404.00836v1 Announce Type: cross  Abstract: In some applications, edge learning is experiencing a shift in focusing from conventional learning from scratch to new two-stage learning unifying pre-training and task-specific fine-tuning. This paper considers the problem of joint communication and computation resource management in a two-stage edge learning system. In this system, model pre-training is first conducted at an edge server via centralized learning on local pre-stored general data, and then task-specific fine-tuning is performed at edge devices based on the pre-trained model via federated edge learning. For the two-stage learning model, we first analyze the convergence behavior (in terms of the average squared gradient norm bound), which characterizes the impacts of various system parameters such as the number of learning rounds and batch sizes in the two stages on the convergence rate. Based on our analytical results, we then propose a joint communication and computatio
    
[^63]: HeteroMILE: 用于异构图的多层图表示学习框架

    HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs

    [https://arxiv.org/abs/2404.00816](https://arxiv.org/abs/2404.00816)

    HeteroMILE提出了一种多级嵌入框架，可以将当代图嵌入方法扩展到大型异构图，并通过反复粗化和细化的方式有效降低计算成本。

    

    异构图在现实世界的应用中是普遍存在的，因为它们可以表示不同类型实体之间的各种关系。因此，学习这种图中的嵌入是图机器学习中的一个关键问题。然而，现有的解决方案由于计算复杂度高而无法扩展到大型异构图。为了解决这个问题，我们提出了一个节点在异构图上的多级嵌入框架（HeteroMILE）-一种通用方法，可以使当代图嵌入方法扩展到大图。HeteroMILE将大图反复粗化为较小的大小，同时保留图的主干结构，然后将其嵌入，通过避免耗时的处理操作有效地降低计算成本。然后，它使用异构图卷积神经网络将粗化的嵌入优化到原始图中。

    arXiv:2404.00816v1 Announce Type: cross  Abstract: Heterogeneous graphs are ubiquitous in real-world applications because they can represent various relationships between different types of entities. Therefore, learning embeddings in such graphs is a critical problem in graph machine learning. However, existing solutions for this problem fail to scale to large heterogeneous graphs due to their high computational complexity. To address this issue, we propose a Multi-Level Embedding framework of nodes on a heterogeneous graph (HeteroMILE) - a generic methodology that allows contemporary graph embedding methods to scale to large graphs. HeteroMILE repeatedly coarsens the large sized graph into a smaller size while preserving the backbone structure of the graph before embedding it, effectively reducing the computational cost by avoiding time-consuming processing operations. It then refines the coarsened embedding to the original graph using a heterogeneous graph convolution neural network.
    
[^64]: 在通过共享内存进行注意力因子分解的困难

    On Difficulties of Attention Factorization through Shared Memory

    [https://arxiv.org/abs/2404.00798](https://arxiv.org/abs/2404.00798)

    通过过滤输入信号来优化与内存通信，可以显着提高模型性能，挑战了使用注意力机制的传统思维。

    

    Transformers在包括自然语言处理、计算机视觉和音频处理在内的许多领域中革命了深度学习。它们的优势在于其注意力机制，允许发现复杂的输入关系。然而，这种机制的二次时间和内存复杂性为更大的输入提出了挑战。研究人员现在正在研究诸如线性统一嵌套注意力（Luna）或Memory Augmented Transformer等模型，这些模型利用外部可学习内存，将注意力计算复杂性降低到线性，或者在以块为单位的处理中在块之间传播信息。我们的研究挑战了这些模型的传统思维，揭示了通过各种操作直接与内存接口的方法并不是最佳选择，并且通过在与内存通信之前过滤输入信号，性能可能会得到显着改善。

    arXiv:2404.00798v1 Announce Type: new  Abstract: Transformers have revolutionized deep learning in numerous fields, including natural language processing, computer vision, and audio processing. Their strength lies in their attention mechanism, which allows for the discovering of complex input relationships. However, this mechanism's quadratic time and memory complexity pose challenges for larger inputs. Researchers are now investigating models like Linear Unified Nested Attention (Luna) or Memory Augmented Transformer, which leverage external learnable memory to either reduce the attention computation complexity down to linear, or to propagate information between chunks in chunk-wise processing. Our findings challenge the conventional thinking on these models, revealing that interfacing with the memory directly through an attention operation is suboptimal, and that the performance may be considerably improved by filtering the input signal before communicating with memory.
    
[^65]: 面向工业和社会的元机器人：愿景、技术和机遇

    Metarobotics for Industry and Society: Vision, Technologies, and Opportunities

    [https://arxiv.org/abs/2404.00797](https://arxiv.org/abs/2404.00797)

    Metarobotics旨在通过结合无线通信、多感官沉浸和集体智能，为远程机器人应用提供普遍、流动和非侵入式的访问和互动，有望为工业和社会带来诸多益处。

    

    Metarobotics旨在将下一代无线通信，多感官沉浸和集体智能相结合，提供对远程机器人应用的普遍、流动和非侵入式访问和互动。工业和社会有望从这些功能中受益。本文描述了Metarobotics在社会、工业和两者之间的目标。它确定并调查了可能实现这些目标的技术，并提供了一个架构来推进Metarobotics关键组件的相互作用。

    arXiv:2404.00797v1 Announce Type: cross  Abstract: Metarobotics aims to combine next generation wireless communication, multi-sense immersion, and collective intelligence to provide a pervasive, itinerant, and non-invasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This paper describes objectives of Metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of Metarobotics. Potentials for self-determ
    
[^66]: 无需排练的模块化和组合式持续学习对语言模型的应用

    Rehearsal-Free Modular and Compositional Continual Learning for Language Models

    [https://arxiv.org/abs/2404.00790](https://arxiv.org/abs/2404.00790)

    提出了一种无需排练的模块化和组合式持续学习框架，可以持续向语言模型添加新模块并将其与现有模块组合，实验证明该框架优于现有技术并有效推动知识转移。

    

    持续学习旨在在不遗忘现有知识的情况下，逐步获取新的知识。为了克服灾难性遗忘，方法要么基于排练，即存储来自先前任务的数据示例以进行数据重播，要么将参数隔离分配给每个任务。然而，基于排练的方法会引发隐私和内存问题，参数隔离的持续学习方法不考虑任务之间的相互作用，从而阻碍知识转移。在这项工作中，我们提出了MoCL，一个无需排练的模块化和组合式持续学习框架，该框架不断向语言模型添加新的模块，并将其与现有模块组合在一起。在各种基准测试中的实验表明，MoCL优于现有技术，并有效促进了知识转移。

    arXiv:2404.00790v1 Announce Type: cross  Abstract: Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free Modular and Compositional Continual Learning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.
    
[^67]: 解开海马形状变异之谜：利用对比学习的图变分自动编码器研究神经系统疾病

    Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning

    [https://arxiv.org/abs/2404.00785](https://arxiv.org/abs/2404.00785)

    本研究利用图变分自动编码器和对比学习解开神经系统疾病中海马形状变异的关键潜变量，超越了其他先进方法在解开能力上的表现。

    

    本文提出了一项综合研究，专注于在神经系统疾病背景下从扩散张量成像（DTI）数据集中解开海马形状变异。借助增强的监督对比学习图变分自动编码器（VAE），我们的方法旨在通过区分代表年龄和是否患病的两个不同潜变量来提高解释性。在我们的消融研究中，我们调查了一系列VAE架构和对比损失函数，展示了我们方法增强的解开能力。这个评估使用了来自DTI海马数据集的合成3D环形网格数据和真实的3D海马网格数据集。我们的监督解开模型在解开分数方面优于几种最先进的方法，如属性和引导VAE。我们的模型可以区分不同年龄组和疾病状况。

    arXiv:2404.00785v1 Announce Type: cross  Abstract: This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in pa
    
[^68]: 处理连续学习中的可塑性丧失和灾难性遗忘

    Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning

    [https://arxiv.org/abs/2404.00781](https://arxiv.org/abs/2404.00781)

    本文提出了一种新方法，即基于效用的扰动梯度下降（UPGD），通过在梯度更新中应用不同大小的扰动，保护有用单元以防遗忘，同时恢复不太有用单元的可塑性。

    

    深度表示学习方法在连续学习中存在困难，既遭受有用单元的灾难性遗忘，又因僵化和无用单元导致可塑性丢失。虽然许多方法分别解决这两个问题，但目前只有少数方法能同时处理这两个问题。本文引入了基于效用的扰动梯度下降（UPGD）作为一种用于表示持续学习的新方法。UPGD结合了梯度更新和扰动，它对更有用的单元应用较小的修改，保护它们免受遗忘，对不太有用的单元应用较大的修改，恢复它们的可塑性。

    arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta
    
[^69]: 隐私保护光学技术增强人脸去标识化的保护

    Privacy-preserving Optics for Enhancing Protection in Face De-identification

    [https://arxiv.org/abs/2404.00777](https://arxiv.org/abs/2404.00777)

    本文提出了一种硬件级人脸去标识化方法，通过学习光学编码器和回归模型生成人脸热图，同时隐藏人脸身份，解决了人脸去标识化软件存在的漏洞。

    

    相机的现代激增，以及广泛应用的计算机视觉技术，引发了重大的隐私和安全担忧。当前人工智能（AI）技术帮助识别相关事件，并在家庭、办公室、医院等领域的日常任务中提供支持。为了这些目的而访问或处理个人信息引发了隐私担忧。虽然软件级解决方案如人脸去标识化提供了很好的隐私/效用权衡，但它们存在嗅探攻击的漏洞。本文提出了一种硬件级人脸去标识化方法来解决这一漏洞。具体地，我们的方法首先学习光学编码器以及回归模型，获取人脸热图，同时隐藏源图像中的人脸身份。我们还提出了一个匿名化框架，通过隐私保护图像、人脸热图和参考人脸生成一个新的人脸。

    arXiv:2404.00777v1 Announce Type: cross  Abstract: The modern surge in camera usage alongside widespread computer vision technology applications poses significant privacy and security concerns. Current artificial intelligence (AI) technologies aid in recognizing relevant events and assisting in daily tasks in homes, offices, hospitals, etc. The need to access or process personal information for these purposes raises privacy concerns. While software-level solutions like face de-identification provide a good privacy/utility trade-off, they present vulnerabilities to sniffing attacks. In this paper, we propose a hardware-level face de-identification method to solve this vulnerability. Specifically, our approach first learns an optical encoder along with a regression model to obtain a face heatmap while hiding the face identity from the source image. We also propose an anonymization framework that generates a new face using the privacy-preserving image, face heatmap, and a reference face i
    
[^70]: PyTorch Frame: 一个用于多模态表格学习的模块化框架

    PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning

    [https://arxiv.org/abs/2404.00776](https://arxiv.org/abs/2404.00776)

    PyTorch Frame是一个用于处理多模态表格数据的PyTorch框架，通过提供数据结构、模型抽象和外部基础模型整合等功能，实现了模块化的表格模型实现，并成功将这些模型应用于复杂的数据集。

    

    我们提出了PyTorch Frame，这是一个基于PyTorch的框架，用于处理多模态表格数据的深度学习。PyTorch Frame通过提供基于PyTorch的数据结构来处理复杂的表格数据，引入模型抽象以实现表格模型的模块化实现，并允许整合外部基础模型来处理复杂列（例如，用于文本列的LLMs）。我们通过以模块化方式实现多样的表格模型，成功将这些模型应用于复杂的多模态表格数据，并将我们的框架与PyTorch Geometric集成，PyTorch Geometric是一个用于图神经网络（GNNs）的PyTorch库，以实现对关系数据库的端到端学习。

    arXiv:2404.00776v1 Announce Type: new  Abstract: We present PyTorch Frame, a PyTorch-based framework for deep learning over multi-modal tabular data. PyTorch Frame makes tabular deep learning easy by providing a PyTorch-based data structure to handle complex tabular data, introducing a model abstraction to enable modular implementation of tabular models, and allowing external foundation models to be incorporated to handle complex columns (e.g., LLMs for text columns). We demonstrate the usefulness of PyTorch Frame by implementing diverse tabular models in a modular way, successfully applying these models to complex multi-modal tabular data, and integrating our framework with PyTorch Geometric, a PyTorch library for Graph Neural Networks (GNNs), to perform end-to-end learning over relational databases.
    
[^71]: SOAR: 改进的近似最近邻搜索索引方法

    SOAR: Improved Indexing for Approximate Nearest Neighbor Search

    [https://arxiv.org/abs/2404.00774](https://arxiv.org/abs/2404.00774)

    SOAR提出了一种针对近似最近邻搜索的新数据索引技术，通过使用增强正交残差损失来优化每个表示，从而改善了索引质量并实现了最先进的性能表现。

    

    本文介绍了SOAR：增强正交残差的泄漏索引技术，这是一种针对近似最近邻（ANN）搜索的新型数据索引技术。SOAR在分区数据时扩展了先前的ANN搜索方法，如泄漏树，这些方法利用多个冗余表示来降低在搜索过程中错过最近邻的概率。然而，与独立训练和计算这些冗余表示不同，SOAR使用了一种增强正交残差损失，优化每个表示以补偿其他表示性能不佳的情况。这大大提高了整体索引质量，实现了最先进的ANN基准性能，同时保持快速索引时间和低内存消耗。

    arXiv:2404.00774v1 Announce Type: new  Abstract: This paper introduces SOAR: Spilling with Orthogonality-Amplified Residuals, a novel data indexing technique for approximate nearest neighbor (ANN) search. SOAR extends upon previous approaches to ANN search, such as spill trees, that utilize multiple redundant representations while partitioning the data to reduce the probability of missing a nearest neighbor during search. Rather than training and computing these redundant representations independently, however, SOAR uses an orthogonality-amplified residual loss, which optimizes each representation to compensate for cases where other representations perform poorly. This drastically improves the overall index quality, resulting in state-of-the-art ANN benchmark performance while maintaining fast indexing times and low memory consumption.
    
[^72]: Recover：用于故障检测和恢复的神经符号框架

    Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery

    [https://arxiv.org/abs/2404.00756](https://arxiv.org/abs/2404.00756)

    该论文提出了一种神经符号框架，名为Recover，用于在线故障识别和恢复，通过集成符号信息来提升大型语言模型生成恢复计划的能力，降低相关成本。

    

    论文介绍了Recover，这是一个用于在线故障识别和恢复的神经符号框架。Recover通过集成本体论、逻辑规则和基于LLM的规划器，利用符号信息增强LLM生成恢复计划的能力，并降低相关成本。

    arXiv:2404.00756v1 Announce Type: new  Abstract: Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical ru
    
[^73]: C-XGBoost：一种用于因果效应估计的树提升模型

    C-XGBoost: A tree boosting model for causal effect estimation

    [https://arxiv.org/abs/2404.00751](https://arxiv.org/abs/2404.00751)

    该论文提出了一种名为C-XGBoost的树提升模型，可用于预测潜在结果，结合了基于树的模型和基于神经网络的因果推断模型的优势，同时继承了XGBoost模型的高效处理缺失值特征的优势。

    

    因果效应估计旨在估计处理对结果的平均处理效应以及条件平均处理效应，这些知识在许多安全关键领域中至关重要，通常需要从观测数据中提取。在这项工作中，我们提出了一种新的因果推断模型，名为C-XGBoost，用于预测潜在结果。我们的方法动机在于利用基于树的模型处理表格数据的优越性，以及基于神经网络的因果推断模型学习有用于估计处理和非处理案例的结果的表示的显著特性。所提出的模型还继承了XGBoost模型的显著优势，如高效处理具有缺失值特征的特性，需要最少的预处理工作，同时具备正则化技术。

    arXiv:2404.00751v1 Announce Type: cross  Abstract: Causal effect estimation aims at estimating the Average Treatment Effect as well as the Conditional Average Treatment Effect of a treatment to an outcome from the available data. This knowledge is important in many safety-critical domains, where it often needs to be extracted from observational data. In this work, we propose a new causal inference model, named C-XGBoost, for the prediction of potential outcomes. The motivation of our approach is to exploit the superiority of tree-based models for handling tabular data together with the notable property of causal inference neural network-based models to learn representations that are useful for estimating the outcome for both the treatment and non-treatment cases. The proposed model also inherits the considerable advantages of XGBoost model such as efficiently handling features with missing values requiring minimum preprocessing effort, as well as it is equipped with regularization tech
    
[^74]: 考虑缺失数据插补的分布式发电输出非参数端到端概率预测

    Nonparametric End-to-End Probabilistic Forecasting of Distributed Generation Outputs Considering Missing Data Imputation

    [https://arxiv.org/abs/2404.00729](https://arxiv.org/abs/2404.00729)

    本文提出了一种非参数化的端到端方法，用于考虑缺失数据插补的分布式可再生发电输出的概率预测，通过长短期记忆网络建模概率分布，并结合端到端训练过程，成功展示了在处理缺失值情况下的卓越性能。

    

    在本文中，我们介绍了一种非参数化的端到端方法，用于考虑缺失数据插补的分布式可再生发电输出的概率预测。首先，我们利用长短期记忆（LSTM）网络构建非参数化概率预测模型，对分布式可再生发电输出的概率分布进行建模。其次，我们设计了一个端到端训练过程，通过迭代插补和迭代基于损失的训练过程来包括缺失数据的插补。这种两步建模方法有效地结合了非参数方法和端到端方法的优势。因此，我们的方法在处理分布式可再生发电输出的概率预测中表现出卓越的能力，同时有效处理缺失值。仿真结果证实了我们的方法相比现有算法的卓越性能。

    arXiv:2404.00729v1 Announce Type: cross  Abstract: In this paper, we introduce a nonparametric end-to-end method for probabilistic forecasting of distributed renewable generation outputs while including missing data imputation. Firstly, we employ a nonparametric probabilistic forecast model utilizing the long short-term memory (LSTM) network to model the probability distributions of distributed renewable generations' outputs. Secondly, we design an end-to-end training process that includes missing data imputation through iterative imputation and iterative loss-based training procedures. This two-step modeling approach effectively combines the strengths of the nonparametric method with the end-to-end approach. Consequently, our approach demonstrates exceptional capabilities in probabilistic forecasting for the outputs of distributed renewable generations while effectively handling missing values. Simulation results confirm the superior performance of our approach compared to existing al
    
[^75]: MugenNet：一种新型结合了卷积神经网络和Transformer网络的方法及其在结肠息肉图像分割中的应用

    MugenNet: A Novel Combined Convolution Neural Network and Transformer Network with its Application for Colonic Polyp Image Segmentation

    [https://arxiv.org/abs/2404.00726](https://arxiv.org/abs/2404.00726)

    提出了MugenNet，一种结合了卷积神经网络和Transformer网络的方法，用于解决结肠息肉图像分割中CNN训练时间长和Transformer信息丢失的问题

    

    生物医学图像分割是疾病诊断中非常重要的一部分。在临床实践中，早期检测息肉是通过结肠镜检查和生物医学图像处理来进行的。因此，在结肠镜检查中准确的息肉图像分割具有重要意义。本文报告的研究基于著名的混合原则，提出了一种结合了卷积神经网络和Transformer网络的方法，以解决CNN训练时间长和Transformer信息丢失的问题。

    arXiv:2404.00726v1 Announce Type: cross  Abstract: Biomedical image segmentation is a very important part in disease diagnosis. The term "colonic polyps" refers to polypoid lesions that occur on the surface of the colonic mucosa within the intestinal lumen. In clinical practice, early detection of polyps is conducted through colonoscopy examinations and biomedical image processing. Therefore, the accurate polyp image segmentation is of great significance in colonoscopy examinations. Convolutional Neural Network (CNN) is a common automatic segmentation method, but its main disadvantage is the long training time. Transformer utilizes a self-attention mechanism, which essentially assigns different importance weights to each piece of information, thus achieving high computational efficiency during segmentation. However, a potential drawback is the risk of information loss. In the study reported in this paper, based on the well-known hybridization principle, we proposed a method to combine 
    
[^76]: 越大越好吗？通过预算重新分配改进LLM代码生成

    The Larger the Better? Improved LLM Code-Generation via Budget Reallocation

    [https://arxiv.org/abs/2404.00725](https://arxiv.org/abs/2404.00725)

    较小的语言模型可以在相同预算下产生可靠的改进，但在无法进行单元测试的情况下，较小的模型选择排名次于较大模型的单个输出。

    

    人们普遍认为，大型语言模型(LLMs)比较小的模型更好。然而，更大的模型在推断过程中也需要更多的时间和计算资源。这就引出了一个问题：当两个模型在相同的预算下运行时会发生什么？（例如，计算资源，运行时间）。为了解决这个问题，我们分析了各种大小的代码生成LLMs，并进行比较，例如运行一个70B模型一次与从13B模型生成五个输出并选择一个的情况。我们的研究结果表明，在标准单元测试设置中，反复使用较小的模型可以产生一致的改进，在五个任务中最高可达15%的增益。另一方面，在无法进行单元测试的情况下，从较小模型中基于排名的候选选择表现不及来自较大模型的单个输出。我们的结果突显了使用较小模型而非较大模型的潜力。

    arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
    
[^77]: 计算机自适应测试综述：机器学习视角

    Survey of Computerized Adaptive Testing: A Machine Learning Perspective

    [https://arxiv.org/abs/2404.00712](https://arxiv.org/abs/2404.00712)

    本文以机器学习视角综述了计算机自适应测试（CAT），重点解析其测试问题选择算法和如何优化认知诊断模型、题库构建和测试控制。

    

    计算机自适应测试（CAT）提供了一种高效、量身定制的评估考生熟练程度的方法，通过根据他们的表现动态调整测试问题。CAT广泛应用于教育、医疗、体育和社会学等多个领域，彻底改变了测试实践。然而，随着大规模测试的增加复杂性，CAT已经融合了机器学习技术。本文旨在提供一个以机器学习为重点的CAT综述，从新的角度解读这种自适应测试方法。通过研究CAT适应性核心的测试问题选择算法，我们揭示了其功能。此外，我们探讨了认知诊断模型、题库构建和CAT中的测试控制，探索了机器学习如何优化这些组成部分。通过对当前情况的分析，

    arXiv:2404.00712v1 Announce Type: cross  Abstract: Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of curre
    
[^78]: 针对表格生成对抗网络的隐私再识别攻击

    Privacy Re-identification Attacks on Tabular GANs

    [https://arxiv.org/abs/2404.00696](https://arxiv.org/abs/2404.00696)

    本研究调查了使用生成对抗网络（GANs）生成表格合成数据时可能产生的隐私风险，分析了对合成数据进行再识别攻击的影响以及信息对发动更成功的再识别攻击的潜在用处。

    

    生成模型容易出现过拟合，因此可能潜在地泄露训练数据的敏感信息。本研究调查了使用生成对抗网络（GANs）生成表格合成数据时可能产生的隐私风险。我们分析了对合成数据进行再识别攻击的影响，即攻击旨在选择被预测与基于最接近合成记录的记忆训练样本对应的样本。我们考虑了多种不同攻击者可能具有不同访问级别或对生成模型和预测的知识，并评估哪些信息可能最有助于发动更成功的再识别攻击。在此过程中，我们还考虑了将再识别攻击制定为重建攻击的情形。

    arXiv:2404.00696v1 Announce Type: cross  Abstract: Generative models are subject to overfitting and thus may potentially leak sensitive information from the training data. In this work. we investigate the privacy risks that can potentially arise from the use of generative adversarial networks (GANs) for creating tabular synthetic datasets. For the purpose, we analyse the effects of re-identification attacks on synthetic data, i.e., attacks which aim at selecting samples that are predicted to correspond to memorised training samples based on their proximity to the nearest synthetic records. We thus consider multiple settings where different attackers might have different access levels or knowledge of the generative model and predictive, and assess which information is potentially most useful for launching more successful re-identification attacks. In doing so we also consider the situation for which re-identification attacks are formulated as reconstruction attacks, i.e., the situation 
    
[^79]: 共享仿射子空间中的臂上元学习

    Meta Learning in Bandits within Shared Affine Subspaces

    [https://arxiv.org/abs/2404.00688](https://arxiv.org/abs/2404.00688)

    通过利用低维仿射子空间集中性，我们提出两种策略解决了在多个环境随机臂上任务中减少预期遗憾的问题。

    

    我们研究了通过利用多个环境随机臂上任务在低维仿射子空间周围的集中性，通过在线主成分分析来减少在遇到的臂上任务中的预期遗憾的问题。我们提出并理论分析了两种解决该问题的策略：一种基于面对不确定性时的乐观原则，另一种通过汤普森取样。我们的框架是通用的，并包括先前提出的方法作为特例。此外，实证结果表明我们的方法显著减少了几个臂上任务的遗憾。

    arXiv:2404.00688v1 Announce Type: new  Abstract: We study the problem of meta-learning several contextual stochastic bandits tasks by leveraging their concentration around a low-dimensional affine subspace, which we learn via online principal component analysis to reduce the expected regret over the encountered bandits. We propose and theoretically analyze two strategies that solve the problem: One based on the principle of optimism in the face of uncertainty and the other via Thompson sampling. Our framework is generic and includes previously proposed approaches as special cases. Besides, the empirical results show that our methods significantly reduce the regret on several bandit tasks.
    
[^80]: 利用最大均值差异重心在强化学习中传播价值函数的不确定性

    Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning

    [https://arxiv.org/abs/2404.00686](https://arxiv.org/abs/2404.00686)

    这项工作引入了最大均值差异Q学习（MMD-QL）来改进强化学习中价值函数不确定性的传播，通过使用MMD重心，实现了比Wasserstein距离更紧的概率度量，在实验中表现优于其他算法，并结合深度网络创造了MMD Q网络（MMD-QN）。

    

    考虑到价值函数的不确定性可以促进强化学习中的探索。我们的工作引入了最大均值差异Q学习（MMD-QL），以改进Wasserstein Q学习（WQL），用于在时间差分（TD）更新期间传播不确定性。MMD-QL使用MMD重心来实现这一目的，因为MMD提供了比Wasserstein距离更紧的概率度量之间的接近度估计。首先，我们证明了在平均损失度量下，MMD-QL在马尔可夫决策过程（MDP）中是“可能近似正确”的。在考虑到累积奖励的情况下，对表格环境进行的实验表明，MMD-QL优于WQL和其他算法。其次，我们将深度网络纳入MMD-QL中，创建MMD Q网络（MMD-QN）。通过合理假设，我们分析了MMD-QN在函数逼近中的收敛速度。在具有挑战性的Atari游戏上的实证结果表明，MMD-QN表现良好。

    arXiv:2404.00686v1 Announce Type: new  Abstract: Accounting for the uncertainty of value functions boosts exploration in Reinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD barycenter for this purpose, as MMD provides a tighter estimate of closeness between probability measures than the Wasserstein distance. Firstly, we establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under the average loss metric. Concerning the accumulated rewards, experiments on tabular environments show that MMD-QL outperforms WQL and other algorithms. Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network (MMD-QN). Making reasonable assumptions, we analyze the convergence rates of MMD-QN using function approximation. Empirical results on challenging Atari games demonstrate that MMD-QN performs well comp
    
[^81]: 隐私保护型模型解释研究综述：隐私风险、攻击和对抗措施

    A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures

    [https://arxiv.org/abs/2404.00673](https://arxiv.org/abs/2404.00673)

    本研究是第一个全面调查模型解释中隐私攻击及其对抗措施的论文，通过分类隐私攻击和对抗措施，初步探讨了隐私泄漏原因，提出未解决问题和未来研究方向。

    

    随着可解释人工智能（XAI）的采用不断扩大，解决其隐私影响的紧迫性变得更加迫切。尽管在人工智能隐私和可解释性方面有越来越多的研究，但对于隐私保护型模型解释却鲜有关注。本文首次全面调查了模型解释的隐私攻击及其对抗措施。我们在这一领域的贡献包括对研究论文进行彻底分析，并提供了一个相互连接的分类法，便于根据目标解释对隐私攻击和对抗措施进行分类。本研究还对隐私泄漏原因进行了初步调查。最后，我们讨论了我们分析中发现的未解决问题和未来研究方向。该调查旨在成为研究界的宝贵资源，并为这一领域的新手提供明确的见解。

    arXiv:2404.00673v1 Announce Type: cross  Abstract: As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have estab
    
[^82]: 通过令牌扩展实现Transformer的一般高效训练

    A General and Efficient Training for Transformer via Token Expansion

    [https://arxiv.org/abs/2404.00672](https://arxiv.org/abs/2404.00672)

    本文提出了一种名为ToE的令牌生长方案，旨在通过加速一致性训练来改善ViT的训练效果。

    

    Vision Transformer（ViT）通常需要极大的训练成本才能取得显著性能。本文提出一种新的令牌生长方案Token Expansion (ToE)，以实现ViT的一致性训练加速。我们引入了一个“初始化-扩展-合并”管道，以保持原始Transformer的中间特征分布的完整性，防止在训练过程中丢失关键的可学习信息。

    arXiv:2404.00672v1 Announce Type: cross  Abstract: The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e
    
[^83]: 加速的无参数随机优化

    Accelerated Parameter-Free Stochastic Optimization

    [https://arxiv.org/abs/2404.00666](https://arxiv.org/abs/2404.00666)

    提出了一种加速的无参数随机优化方法，不需要先验了解问题参数，在平滑随机凸优化的情况下实现了近乎最佳收敛速率并表现强大。

    

    我们提出了一种方法，该方法实现了对平滑随机凸优化的近乎最佳收敛速率，并且基本上不需要先验了解问题参数。这改进了之前的工作，之前的工作需要至少知道到最优解的初始距离 d0。我们的方法 U-DoG 将 UniXGrad (Kavis 等人，2019) 和 DoG (Ivgi 等人，2023) 与新颖的迭代稳定技术相结合。它仅需要对 d0 和噪声幅度有松散的界限，在次高斯噪声下提供高概率保证，并且在非平滑情况下也接近最佳。我们的实验表明，在凸问题上能够稳定地表现强大，在神经网络训练上有着不同的成效。

    arXiv:2404.00666v1 Announce Type: new  Abstract: We propose a method that achieves near-optimal rates for smooth stochastic convex optimization and requires essentially no prior knowledge of problem parameters. This improves on prior work which requires knowing at least the initial distance to optimality d0. Our method, U-DoG, combines UniXGrad (Kavis et al., 2019) and DoG (Ivgi et al., 2023) with novel iterate stabilization techniques. It requires only loose bounds on d0 and the noise magnitude, provides high probability guarantees under sub-Gaussian noise, and is also near-optimal in the non-smooth case. Our experiments show consistent, strong performance on convex problems and mixed results on neural network training.
    
[^84]: 关于为技术文档构建RAG系统的观察

    Observations on Building RAG Systems for Technical Documents

    [https://arxiv.org/abs/2404.00657](https://arxiv.org/abs/2404.00657)

    研究回顾了对技术文档RAG系统构建的重要因素，并通过实验证明了最佳实践和潜在挑战

    

    RAG（检索增强生成）用于技术文档时存在挑战，因为嵌入通常无法捕捉领域信息。我们回顾了影响RAG的重要因素的先前研究，并进行实验以突出构建技术文档RAG系统的最佳实践和潜在挑战。

    arXiv:2404.00657v1 Announce Type: cross  Abstract: Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.
    
[^85]: 通过基于模型的内在动机学习离线策略以实现主动在线探索

    Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration

    [https://arxiv.org/abs/2404.00651](https://arxiv.org/abs/2404.00651)

    本文提出了一种结合预测模型和离线学习元素的强化学习算法，通过内在奖励与模型不确定性的关联，在连续控制任务中实现了样本有效的探索。

    

    在深度强化学习领域，最近的进展在样本效率方面取得了显着进展，涵盖了基于模型的方法和无模型的方法。本文研究了如何在连续控制任务中实现样本有效的探索。我们引入了一种强化学习算法，结合了预测模型和离线学习元素，其中在线规划器通过一种新颖感知的终端价值函数用于样本收集。利用潜在状态空间中的前向预测错误，我们推导出了一种内在奖励，而不会产生参数开销。该奖励建立了与模型不确定性的牢固联系，使代理能够有效地克服渐近性能差距。

    arXiv:2404.00651v1 Announce Type: cross  Abstract: Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive expe
    
[^86]: 《面对它们自己：基于LLM的两阶段策略通过日志定位配置错误》

    Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs

    [https://arxiv.org/abs/2404.00640](https://arxiv.org/abs/2404.00640)

    提出了一种基于LLM的两阶段策略，帮助终端用户通过日志定位配置错误的根本原因，并开发了相应工具LogConfigLocalizer，以帮助终端用户通过日志分析解决配置错误。

    

    可配置软件系统容易出现配置错误，给公司带来重大损失。然而，由于庞大而复杂的配置空间，诊断这些错误具有挑战性。这些错误对有经验的维护者和没有软件系统源代码访问权限的新终端用户都构成重大挑战。鉴于日志对大多数终端用户来说易于访问，我们进行了初步研究，概述了利用日志定位配置错误的挑战和机遇。根据初步研究所得的见解，我们提出了基于LLM的两阶段策略，帮助终端用户通过日志定位配置错误的根本原因。我们进一步实现了一个工具LogConfigLocalizer，符合上述策略的设计，希望通过日志分析帮助终端用户应对配置错误。

    arXiv:2404.00640v1 Announce Type: cross  Abstract: Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis.   To the best
    
[^87]: RL-MUL：使用深度强化学习进行乘法器设计优化

    RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning

    [https://arxiv.org/abs/2404.00639](https://arxiv.org/abs/2404.00639)

    提出了基于强化学习的乘法器设计优化框架RL-MUL，利用矩阵和张量表示乘法器的压缩树，通过定制化的奖励实现区域和延迟之间的权衡，同时扩展到优化融合乘-累加（MAC）设计。

    

    乘法是许多应用中的基本操作，乘法器被广泛应用于各种电路中。然而，由于设计空间巨大，优化乘法器是具有挑战性和非平凡的。在本文中，我们提出了RL-MUL，一个基于强化学习的乘法器设计优化框架。具体来说，我们利用矩阵和张量表示乘法器的压缩树，基于这一表示，卷积神经网络可以无缝地集成为代理网络。代理可以学习根据定制化的可容忍区域与延迟之间的权衡关系来优化乘法器结构。此外，RL-MUL的能力被扩展到优化融合乘-累加（MAC）设计。实验在不同位宽的乘法器上进行。结果表明，RL-MUL生成的乘法器能够超越所有基线。

    arXiv:2404.00639v1 Announce Type: cross  Abstract: Multiplication is a fundamental operation in many applications, and multipliers are widely adopted in various circuits. However, optimizing multipliers is challenging and non-trivial due to the huge design space. In this paper, we propose RL-MUL, a multiplier design optimization framework based on reinforcement learning. Specifically, we utilize matrix and tensor representations for the compressor tree of a multiplier, based on which the convolutional neural networks can be seamlessly incorporated as the agent network. The agent can learn to optimize the multiplier structure based on a Pareto-driven reward which is customized to accommodate the trade-off between area and delay. Additionally, the capability of RL-MUL is extended to optimize the fused multiply-accumulator (MAC) designs. Experiments are conducted on different bit widths of multipliers. The results demonstrate that the multipliers produced by RL-MUL can dominate all baseli
    
[^88]: HypeBoy: 超图上的生成自监督表示学习

    HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs

    [https://arxiv.org/abs/2404.00638](https://arxiv.org/abs/2404.00638)

    设计了一种新颖的生成式自监督学习策略，通过超边填充的任务有效捕获复杂的超图拓扑结构

    

    超图以复杂的拓扑结构为特征，表达了多个节点之间的高阶交互，并且更好地捕捉拓扑结构对于有效的表示学习至关重要。最近在生成自监督学习（SSL）方面的进展表明，从生成自监督中学习的超图神经网络有潜力有效地编码复杂的超图拓扑结构。然而，为超图设计生成式SSL策略并不是一件简单的事情。关于其生成式SSL任务、与下游任务的连接以及所学表示的实验性质仍然存在问题。鉴于这些期望和挑战，我们提出了一种新颖的超图生成式SSL策略。我们首先在超图上制定了一个生成式SSL任务：超边填充，并强调了它与节点分类的理论联系。基于这个生成式SSL任务，我们提出了一个超图的生成式SSL

    arXiv:2404.00638v1 Announce Type: new  Abstract: Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple nodes with hyperedges, and better capturing the topology is essential for effective representation learning. Recent advances in generative self-supervised learning (SSL) suggest that hypergraph neural networks learned from generative self supervision have the potential to effectively encode the complex hypergraph topology. Designing a generative SSL strategy for hypergraphs, however, is not straightforward. Questions remain with regard to its generative SSL task, connection to downstream tasks, and empirical properties of learned representations. In light of the promises and challenges, we propose a novel generative SSL strategy for hypergraphs. We first formulate a generative SSL task on hypergraphs, hyperedge filling, and highlight its theoretical connection to node classification. Based on the generative SSL task, we propose a hypergraph SS
    
[^89]: 强化学习驱动碰撞回避中的外部感知变分自编码器

    Variational Autoencoders for exteroceptive perception in reinforcement learning-based collision avoidance

    [https://arxiv.org/abs/2404.00623](https://arxiv.org/abs/2404.00623)

    该论文研究了在强化学习驱动碰撞回避中使用外部感知变分自编码器，以获取传感器数据的低维表示。

    

    现代控制系统越来越倾向于使用机器学习算法来增强其性能和适应性。在这个背景下，深度强化学习（DRL）已经成为一种有前途的控制框架，特别在海洋运输领域。我们的工作探讨了变分自编码器（VAE）在获取高保真度测距传感器的泛化低维潜在编码方面的应用，这将作为DRL代理的外部输入。

    arXiv:2404.00623v1 Announce Type: new  Abstract: Modern control systems are increasingly turning to machine learning algorithms to augment their performance and adaptability. Within this context, Deep Reinforcement Learning (DRL) has emerged as a promising control framework, particularly in the domain of marine transportation. Its potential for autonomous marine applications lies in its ability to seamlessly combine path-following and collision avoidance with an arbitrary number of obstacles. However, current DRL algorithms require disproportionally large computational resources to find near-optimal policies compared to the posed control problem when the searchable parameter space becomes large. To combat this, our work delves into the application of Variational AutoEncoders (VAEs) to acquire a generalized, low-dimensional latent encoding of a high-fidelity range-finding sensor, which serves as the exteroceptive input to a DRL agent. The agent's performance, encompassing path-following
    
[^90]: 一种多分支径向基网络方法用于预测复杂混沌行为

    A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours

    [https://arxiv.org/abs/2404.00618](https://arxiv.org/abs/2404.00618)

    提出了一种多分支径向基网络方法来成功预测复杂混沌行为，通过独特的神经网络架构并引入了注意机制，展示了先进机器学习算法在阐明中的潜力

    

    在这项研究中，我们提出了一种多分支网络方法来预测由错综复杂和混沌行为特征的物理吸引子的动力学。我们引入了一种独特的神经网络架构，由径向基函数（RBF）层和旨在有效捕捉吸引子时间演变中非线性相互依赖关系的注意机制组成。我们的结果表明，通过使用包含大约28分钟活动的现实数据集的36,700个时间序列观测，我们成功预测了吸引子的轨迹的100次预测。为了进一步说明我们提出的技术的性能，我们提供了全面的可视化，展示了吸引子的原始和预测行为，并将观察到的与估计结果进行了定量比较。总体而言，这项工作展示了先进机器学习算法在阐明中的潜力

    arXiv:2404.00618v1 Announce Type: new  Abstract: In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucid
    
[^91]: 广泛的自对比使得无需反馈的语言模型对齐成为可能

    Extensive Self-Contrast Enables Feedback-Free Language Model Alignment

    [https://arxiv.org/abs/2404.00604](https://arxiv.org/abs/2404.00604)

    本论文介绍了一种利用广泛自对比生成负例的无需反馈的大型语言模型对齐方法，该方法在实验中表现优于直接偏好优化方法。

    

    人类反馈的强化学习（RLHF）一直是最近大型语言模型（LLM）对齐的核心技术。然而，其严重依赖昂贵的人类或LLM作为评判者的偏好反馈可能会阻碍其更广泛的应用。在这项工作中，我们引入了Self-Contrast，一种通过利用广泛自动生成的负例来进行无需反馈的大型语言模型对齐方法。仅通过监督的微调（SFT）目标，Self-Contrast利用LLM本身生成大量多样的候选项，并利用预训练的嵌入模型根据文本相似性过滤多个负例。理论上，我们证明了在这种设置中，仅仅扩大负面回应仍然可以有效地近似具有更平衡的正面和负面偏好注释的情况。我们对三个数据集进行了直接偏好优化（DPO）的实验表明，Self-Contrast能够始终优于

    arXiv:2404.00604v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform
    
[^92]: LAESI: 使用合成图像进行叶片面积估计

    LAESI: Leaf Area Estimation with Synthetic Imagery

    [https://arxiv.org/abs/2404.00593](https://arxiv.org/abs/2404.00593)

    LAESI数据集包含10万张合成叶片图像，用于叶形态分析，机器学习模型训练后可预测叶片表面积，并提供了基于3D程序模型和生成式AI的高效框架。

    

    我们引入了LAESI，这是一个包含了10万张合成叶片图像的合成叶片数据集，每张图像都有语义分割掩模和表面积标注，图像背景是毫米纸。该数据集主要用于对山毛榉和橡树叶进行形态分析。我们通过训练机器学习模型进行叶片表面积预测和语义分割来评估数据集的适用性，使用真实图像进行验证。验证结果表明，这些模型可以训练出预测叶面积的能力，相对误差不会超过平均人类标注员的误差。LAESI还提供了一个基于3D程序模型和生成式人工智能的高效框架，可用于大规模、可控的数据生成，在农业和生物学等领域具有潜在应用。我们评估了将生成式人工智能纳入我们的程序化数据生成流程中，并展示了基于注释一致性的数据过滤如何导致数据集的生成。

    arXiv:2404.00593v1 Announce Type: cross  Abstract: We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and generative AI for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of generative AI in our procedural data generation pipeline and show how data filtering based on annotation consistency results in dataset
    
[^93]: 利用大型语言模型处理图数据中的不确定性

    Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing

    [https://arxiv.org/abs/2404.00589](https://arxiv.org/abs/2404.00589)

    介绍了一种利用大型语言模型处理图数据中不确定性的方法，通过不确定性感知模块增强，提供置信度评分，实验结果表明该方法在知识图完成和图分类任务上超越了最先进算法。

    

    处理图数据是一项非常困难的任务。传统技术，例如基于几何和矩阵分解的技术，依赖于对数据关系的假设，在处理大型和复杂的图数据时变得不足够。另一方面，深度学习方法展示了处理大型图数据的良好结果，但它们通常无法提供可解释的解释。为了使图处理具有高准确性和可解释性，我们引入了一种新颖的方法，利用了增强不确定性感知模块的大型语言模型(LLM)的力量，以提供生成答案的置信度分数。我们在两个图处理任务上对我们的方法进行了实验：少样本知识图完成和图分类。我们的结果表明，通过参数高效微调，LLM在各个方面超越了最先进的算法

    arXiv:2404.00589v1 Announce Type: cross  Abstract: Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across
    
[^94]: 自动双向加权集成算法及其在脑肿瘤检测和分类中的应用

    Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification

    [https://arxiv.org/abs/2404.00576](https://arxiv.org/abs/2404.00576)

    提出了两种新型的自动双向加权集成算法，用于提高脑肿瘤检测和分类的效果

    

    脑细胞的不受控制和无结构生长被称为脑肿瘤，它在各种癌症中拥有最高的死亡率之一。由于有限的诊断和治疗能力，尤其是在第三世界国家，脑肿瘤带来了重大挑战。早期诊断在有效管理脑肿瘤和减少死亡率方面起着至关重要的作用。然而，由于高成本和长时间的结果获取时间等各种限制，诊断方法的可用性受到阻碍，从而阻碍了疾病的早期检测。在本研究中，我们提出了两种前沿的双向加权投票集成模型，旨在提高加权集成方法的效力。这两种提出的方法结合了来自多个分类器的分类结果，并通过选择第一种方法中具有最高概率的结果，以及最高加权 p

    arXiv:2404.00576v1 Announce Type: cross  Abstract: The uncontrolled and unstructured growth of brain cells is known as brain tumor, which has one of the highest mortality rates among diseases from all types of cancers. Due to limited diagnostic and treatment capabilities, they pose significant challenges, especially in third-world countries. Early diagnosis plays a vital role in effectively managing brain tumors and reducing mortality rates. However, the availability of diagnostic methods is hindered by various limitations, including high costs and lengthy result acquisition times, impeding early detection of the disease. In this study, we present two cutting-edge bi-fold weighted voting ensemble models that aim to boost the effectiveness of weighted ensemble methods. These two proposed methods combine the classification outcomes from multiple classifiers and determine the optimal result by selecting the one with the highest probability in the first approach, and the highest weighted p
    
[^95]: ADs: 活跃数据共享用于先进制造系统中的数据质量保障

    ADs: Active Data-sharing for Data Quality Assurance in Advanced Manufacturing Systems

    [https://arxiv.org/abs/2404.00572](https://arxiv.org/abs/2404.00572)

    提出了一种ADs框架，旨在通过同时选择对下游任务有益的最具信息量的数据点并减轻所有选定数据点之间的分布不匹配，确保多台机器之间共享数据的质量。

    

    机器学习（ML）方法在工业应用中被广泛使用，通常需要大量的训练数据。然而，数据收集需要大量时间成本和对制造系统的投资，常常存在数据稀缺性。因此，在具有类似功能的多台机器之间广泛开启数据共享，以增加用于构建ML方法的数据集。然而，由于不同的工作条件，它们的数据中不可避免地存在分布不匹配，而ML方法被假设是在遵循相同分布的数据集上构建和测试的。因此，提出了一种主动数据共享（ADs）框架，以确保多台机器之间共享数据的质量。它旨在同时选择对下游任务有益的最具信息量的数据点，并减轻所有选定数据点之间的分布不匹配。所提出的方法在异常检测上得到验证。

    arXiv:2404.00572v1 Announce Type: new  Abstract: Machine learning (ML) methods are widely used in industrial applications, which usually require a large amount of training data. However, data collection needs extensive time costs and investments in the manufacturing system, and data scarcity commonly exists. Therefore, data-sharing is widely enabled among multiple machines with similar functionality to augment the dataset for building ML methods. However, distribution mismatch inevitably exists in their data due to different working conditions, while the ML methods are assumed to be built and tested on the dataset following the same distribution. Thus, an Active Data-sharing (ADs) framework is proposed to ensure the quality of the shared data among multiple machines. It is designed to simultaneously select the most informative data points benefiting the downstream tasks and mitigate the distribution mismatch among all selected data points. The proposed method is validated on anomaly de
    
[^96]: 连续正规化流在学习概率分布中的收敛性

    Convergence of Continuous Normalizing Flows for Learning Probability Distributions

    [https://arxiv.org/abs/2404.00551](https://arxiv.org/abs/2404.00551)

    本文研究了具有线性插值的CNFs在从有限随机样本中学习概率分布时的理论性质，建立了非渐近误差界，并提供了收敛分析框架。

    

    连续正规化流（CNFs）是一种基于常微分方程的学习概率分布的生成方法。这种方法在各种应用中表现出显著的经验成功，包括大规模图像合成、蛋白质结构预测和分子生成。本文研究了具有线性插值的CNFs在从有限随机样本中学习概率分布时的理论性质，使用了流匹配目标函数。我们建立了基于CNFs的分布估计器的非渐近误差界，以Wasserstein-2距离表示。我们分析的关键假设是目标分布满足以下三个条件之一：要么具有有界支持，要么是强对数凹的，要么是有限或无限混合的高斯分布。我们提出了一个包含了误差收敛分析框架

    arXiv:2404.00551v1 Announce Type: cross  Abstract: Continuous normalizing flows (CNFs) are a generative method for learning probability distributions, which is based on ordinary differential equations. This method has shown remarkable empirical success across various applications, including large-scale image synthesis, protein structure prediction, and molecule generation. In this work, we study the theoretical properties of CNFs with linear interpolation in learning probability distributions from a finite random sample, using a flow matching objective function. We establish non-asymptotic error bounds for the distribution estimator based on CNFs, in terms of the Wasserstein-2 distance. The key assumption in our analysis is that the target distribution satisfies one of the following three conditions: it either has a bounded support, is strongly log-concave, or is a finite or infinite mixture of Gaussian distributions. We present a convergence analysis framework that encompasses the err
    
[^97]: 统一、可验证的神经网络模拟器用于电磁波反问题

    Unified, Verifiable Neural Simulators for Electromagnetic Wave Inverse Problems

    [https://arxiv.org/abs/2404.00545](https://arxiv.org/abs/2404.00545)

    提出了一个统一的神经模拟器模型，可以快速处理数千个自由度的散射模拟，并通过中间预测能力提供了准确性保证。

    

    基于神经网络的模拟器为电磁波模拟提供了一条速度快数个数量级的路径。然而，现有模型仅针对狭窄的问题类别，并且仅能扩展到几十个自由度（DoFs）的系统。在这里，我们展示了一个单一、统一的模型，能够处理数千个DoFs的散射模拟，任意波长、任意照明波前和自由形式的材料，且在广泛的可配置范围内。基于注意力多条件策略，我们的方法还允许非递归监督和预测中间物理状态，从而提供了改进的泛化能力，而无需额外的数据生成成本。利用这种O(1)时间的中间预测能力，我们提出并证明了一种严格且高效计算的预测误差上界，允许推断时对所有预测进行准确性保证。

    arXiv:2404.00545v1 Announce Type: cross  Abstract: Simulators based on neural networks offer a path to orders-of-magnitude faster electromagnetic wave simulations. Existing models, however, only address narrowly tailored classes of problems and only scale to systems of a few dozen degrees of freedom (DoFs). Here, we demonstrate a single, unified model capable of addressing scattering simulations with thousands of DoFs, of any wavelength, any illumination wavefront, and freeform materials, within broad configurable bounds. Based on an attentional multi-conditioning strategy, our method also allows non-recurrent supervision on and prediction of intermediate physical states, which provides improved generalization with no additional data-generation cost. Using this O(1)-time intermediate prediction capability, we propose and prove a rigorous, efficiently computable upper bound on prediction error, allowing accuracy guarantees at inference time for all predictions. After training solely on 
    
[^98]: 通过两阶段图指针网络和强化学习解决QAP问题

    Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement Learning

    [https://arxiv.org/abs/2404.00539](https://arxiv.org/abs/2404.00539)

    通过引入两阶段图指针网络和强化学习，解决QAP问题的方法在实验中展示出半最优解。

    

    二次分配问题（QAP）是一个实际的组合优化问题，在多年来一直受到研究。由于它是NP困难的，解决QAP的大问题实例是具有挑战性的。尽管启发式方法可以找到半最优解，但随着问题规模的增加，执行时间显着增加。最近，通过深度学习解决组合优化问题比启发式算法更快的解决器引起了关注。但即使使用深度学习，解决大型QAP问题仍然具有挑战性。在本文中，我们提出了称为两阶段图指针网络（GPN）的深度强化学习模型，用于解决QAP。两阶段GPN依赖于GPN，该模型已被提出用于欧几里得旅行商问题（TSP）。首先，我们扩展GPN以用于一般TSP，然后我们向该模型添加新算法以解决QAP。我们的实验结果表明，我们的两阶段GPN提供了半最优解。

    arXiv:2404.00539v1 Announce Type: new  Abstract: Quadratic Assignment Problem (QAP) is a practical combinatorial optimization problems that has been studied for several years. Since it is NP-hard, solving large problem instances of QAP is challenging. Although heuristics can find semi-optimal solutions, the execution time significantly increases as the problem size increases. Recently, solving combinatorial optimization problems by deep learning has been attracting attention as a faster solver than heuristics. Even with deep learning, however, solving large QAP is still challenging. In this paper, we propose the deep reinforcement learning model called the two-stage graph pointer network (GPN) for solving QAP. Two-stage GPN relies on GPN, which has been proposed for Euclidean Traveling Salesman Problem (TSP). First, we extend GPN for general TSP, and then we add new algorithms to that model for solving QAP. Our experimental results show that our two-stage GPN provides semi-optimal solu
    
[^99]: 将坏苹果与好橘子进行比较：通过联合优化偏好对齐大型语言模型

    Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization

    [https://arxiv.org/abs/2404.00530](https://arxiv.org/abs/2404.00530)

    本研究提出了一种新的偏好获取方法，通过DOVE协议对指令-响应对的联合概率进行优化，以对齐大型语言模型。

    

    一种常见的对齐大型语言模型（LLMs）的技术依赖于通过比较在固定上下文中条件生成的多个生成的人类偏好。然而，当这些生成放置在相同的上下文中时，这仅利用了成对比较。然而，这种条件排名通常无法捕获人类偏好的复杂和多维方面。在这项工作中，我们重新审视偏好获取的传统范式，并提出了一个基于在指令-响应对上联合引发偏好的新轴。虽然先前的偏好优化是针对条件排名协议（例如，DPO）设计的，但我们提出的偏好获取协议引入了DOVE，这是一个新的偏好优化目标，通过提升所选指令-响应对的联合概率来降低所拒绝指令-响应对的概率。

    arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
    
[^100]: 多项式的超非奇异分解及其在鲁棒学习低次多项式阈函数中的应用

    Super Non-singular Decompositions of Polynomials and their Application to Robustly Learning Low-degree PTFs

    [https://arxiv.org/abs/2404.00529](https://arxiv.org/abs/2404.00529)

    提出了在强污染模型下鲁棒学习低次多项式阈函数的多项式时间PAC学习算法，在高斯分布下具有误差保证$O_{d, c}(\text{opt}^{1-c})$。

    

    我们研究了在存在恶意破坏的情况下，低次多项式阈值函数（PTFs）的有效可学习性。我们的主要算法结果是在强污染模型下对这个概念类别提出了一个多项式时间的PAC学习算法，针对高斯分布，保证误差为$O_{d, c}(\text{opt}^{1-c})$，其中$c>0$是任意常数，$\text{opt}$是破坏的比例。在强污染模型中，一个无所不知的对手可以任意破坏$\text{opt}$比例的数据点及其标签。该模型泛化了恶意噪声模型和对抗性标签噪声模型。在我们的工作之前，已知在这种破坏模型（甚至在较弱的对抗性标签噪声模型）中的多项式时间算法的错误率为$\tilde{O}_d(\text{opt}^{1/(d+1)})$，这随着度数$d$的增加而显著恶化。

    arXiv:2404.00529v1 Announce Type: cross  Abstract: We study the efficient learnability of low-degree polynomial threshold functions (PTFs) in the presence of a constant fraction of adversarial corruptions. Our main algorithmic result is a polynomial-time PAC learning algorithm for this concept class in the strong contamination model under the Gaussian distribution with error guarantee $O_{d, c}(\text{opt}^{1-c})$, for any desired constant $c>0$, where $\text{opt}$ is the fraction of corruptions. In the strong contamination model, an omniscient adversary can arbitrarily corrupt an $\text{opt}$-fraction of the data points and their labels. This model generalizes the malicious noise model and the adversarial label noise model. Prior to our work, known polynomial-time algorithms in this corruption model (or even in the weaker adversarial label noise model) achieved error $\tilde{O}_d(\text{opt}^{1/(d+1)})$, which deteriorates significantly as a function of the degree $d$.   Our algorithm e
    
[^101]: 用于改进作物模拟的生成式天气模型

    Generative weather for improved crop model simulations

    [https://arxiv.org/abs/2404.00528](https://arxiv.org/abs/2404.00528)

    提出了一种新方法，构建生成式模型用于长期天气预报，从而改进作物产量预测，结果表明在小麦、大麦和油菜的生产中，以及这些作物轮作三年生产中，我们的方法在每个指标上均优于传统方法。

    

    准确和精准的作物产量预测对于农场水平和区域水平的决策制定至关重要。为了进行产量预测，作物模型广泛应用于其模拟假设情景的能力。尽管产量预测的准确性和精度在很大程度上取决于模拟中的天气输入，但令人惊讶的是对准备天气输入的关注甚少。我们提出了一种新方法，用于构建长期天气预报的生成模型，最终改进作物产量预测。我们演示了该方法在两个代表性场景中的应用--小麦、大麦和油菜的单年生产以及利用这些作物轮作的三年生产。结果展示了较传统方法显著的改进，通过预测误差的平均值和标准差来衡量。我们的方法在第一场景的18个指标中每一个都优于传统方法。

    arXiv:2404.00528v1 Announce Type: new  Abstract: Accurate and precise crop yield prediction is invaluable for decision making at both farm levels and regional levels. To make yield prediction, crop models are widely used for their capability to simulate hypothetical scenarios. While accuracy and precision of yield prediction critically depend on weather inputs to simulations, surprisingly little attention has been paid to preparing weather inputs. We propose a new method to construct generative models for long-term weather forecasts and ultimately improve crop yield prediction. We demonstrate use of the method in two representative scenarios -- single-year production of wheat, barley and canola and three-year production using rotations of these crops. Results show significant improvement from the conventional method, measured in terms of mean and standard deviation of prediction errors. Our method outperformed the conventional method in every one of 18 metrics for the first scenario an
    
[^102]: 使用条件扩散和建筑元数据创建合成能源表数据

    Creating synthetic energy meter data using conditional diffusion and building metadata

    [https://arxiv.org/abs/2404.00525](https://arxiv.org/abs/2404.00525)

    该研究提出了一种使用条件扩散模型生成高质量合成能源数据的方法，能够处理长期年度消费配置文件并利用元数据产生连贯的数据

    

    机器学习的进步和计算能力的增强推动了能源相关研究的进展。然而，对来自建筑物的私人能源数据的有限访问限制了依赖于历史数据的传统回归模型。尽管生成模型提供了一种解决方案，但先前的研究主要集中在短期生成周期（例如，日常配置文件）和有限数量的表计上。因此，该研究提出了一种使用相关元数据生成高质量合成能源数据的条件扩散模型。使用包含来自不同建筑物和国家的1,828个功率表的数据集，将该模型与传统方法（如条件生成对抗网络（CGAN）和条件变分自动编码器（CVAE））进行了比较。它明确处理长期年度消费配置文件，利用位置、天气、建筑物和表类型等元数据产生连贯的合成数据。

    arXiv:2404.00525v1 Announce Type: new  Abstract: Advances in machine learning and increased computational power have driven progress in energy-related research. However, limited access to private energy data from buildings hinders traditional regression models relying on historical data. While generative models offer a solution, previous studies have primarily focused on short-term generation periods (e.g., daily profiles) and a limited number of meters. Thus, the study proposes a conditional diffusion model for generating high-quality synthetic energy data using relevant metadata. Using a dataset comprising 1,828 power meters from various buildings and countries, this model is compared with traditional methods like Conditional Generative Adversarial Networks (CGAN) and Conditional Variational Auto-Encoders (CVAE). It explicitly handles long-term annual consumption profiles, harnessing metadata such as location, weather, building, and meter type to produce coherent synthetic data that 
    
[^103]: 最小范数插值在协变量转移下的应用

    Minimum-Norm Interpolation Under Covariate Shift

    [https://arxiv.org/abs/2404.00522](https://arxiv.org/abs/2404.00522)

    本研究首次证明了在转移学习设置下，良性过拟合线性插值器的非渐近超额风险界，并提出了一种新的分类方法。

    

    转移学习是现实世界机器学习部署的关键组成部分，并在过参数化神经网络的实验研究中得到广泛研究。然而，即使在线性回归的最简单设置中，在对转移学习的理论理解仍存在显著差距。在高维线性回归的分布研究中，已经发现了一种被称为“良性过拟合”现象的现象，即线性插值器会对噪声训练标签过拟合，但仍然能很好地泛化。这种行为发生在源协方差矩阵和输入数据维度上的特定条件下。因此，自然而然地想知道这样的高维线性模型在转移学习下如何行为。我们证明了在转移学习设置中良性过拟合线性插值器的第一个非渐近超额风险界。通过我们的分析，我们提出了一个对转移学习中的\textit {b进行分类}}的方法

    arXiv:2404.00522v1 Announce Type: new  Abstract: Transfer learning is a critical part of real-world machine learning deployments and has been extensively studied in experimental works with overparameterized neural networks. However, even in the simplest setting of linear regression a notable gap still exists in the theoretical understanding of transfer learning. In-distribution research on high-dimensional linear regression has led to the identification of a phenomenon known as \textit{benign overfitting}, in which linear interpolators overfit to noisy training labels and yet still generalize well. This behavior occurs under specific conditions on the source covariance matrix and input data dimension. Therefore, it is natural to wonder how such high-dimensional linear models behave under transfer learning. We prove the first non-asymptotic excess risk bounds for benignly-overfit linear interpolators in the transfer learning setting. From our analysis, we propose a taxonomy of \textit{b
    
[^104]: CHAIN：通过受限唯一性连续性规范化增强数据高效GANs的泛化能力

    CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization

    [https://arxiv.org/abs/2404.00521](https://arxiv.org/abs/2404.00521)

    通过引入CHAIN，该方法在数据有限的情况下，解决了GANs中鉴别器过拟合和训练不稳定的问题，提高了泛化能力和训练稳定性。

    

    生成对抗网络（GANs）显着推动了图像生成，但它们的性能严重依赖大量的训练数据。在数据有限的情况下，GANs经常面临鉴别器过拟合和训练不稳定的问题。我们的工作通过识别Batch Normalization（BN）中的关键缺陷来解决这一问题：在中心化和缩放步骤中梯度爆炸的倾向。为了解决这个问题，我们提出了CHAIN（受限唯一性连续性规范化），它将传统的中心化步骤替换为零均值正则化，并在缩放步骤中集成了Lipschitz连续性约束。CHAIN通过自适应插值归一化和非归一化特征进一步增强了GANs的训练，有效避免了鉴别器过拟合。

    arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
    
[^105]: DailyMAE：朝着一天预训练的遮蔽自动编码器迈进

    DailyMAE: Towards Pretraining Masked Autoencoders in One Day

    [https://arxiv.org/abs/2404.00509](https://arxiv.org/abs/2404.00509)

    该研究提出了一种针对遮蔽自动编码器（MAE）的高效训练方法，使得能够在短短18小时内使用单台机器训练模型，取得了高达5倍的速度提升。

    

    最近，遮蔽图像建模（MIM）作为一种重要的自监督学习（SSL）方法，因其在从未标记数据中学习数据表示方面的有效性而引起关注。许多研究强调了MIM的优势，突显出在广泛数据集上预训练的模型如何提高下游任务的性能。然而，预训练的高计算需求带来了重大挑战，特别是在学术环境中，从而阻碍了SSL研究的进展。在本研究中，我们提出了用于MIM的高效训练配方的建议，重点是减轻数据加载瓶颈，并采用渐进训练技术和其他技巧以紧密维持预训练性能。我们的库使得在仅18小时内通过单台配备8颗A100 GPU的机器训练MAE-Base/16模型在ImageNet 1K数据集上进行800个epochs成为可能。通过获得高达5倍的速度提升。

    arXiv:2404.00509v1 Announce Type: new  Abstract: Recently, masked image modeling (MIM), an important self-supervised learning (SSL) method, has drawn attention for its effectiveness in learning data representation from unlabeled data. Numerous studies underscore the advantages of MIM, highlighting how models pretrained on extensive datasets can enhance the performance of downstream tasks. However, the high computational demands of pretraining pose significant challenges, particularly within academic environments, thereby impeding the SSL research progress. In this study, we propose efficient training recipes for MIM based SSL that focuses on mitigating data loading bottlenecks and employing progressive training techniques and other tricks to closely maintain pretraining performance. Our library enables the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs within just 18 hours, using a single machine equipped with 8 A100 GPUs. By achieving speed gains of up to 5.
    
[^106]: 与标签无关的遗忘:深度模型中无监督的去学习

    Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models

    [https://arxiv.org/abs/2404.00506](https://arxiv.org/abs/2404.00506)

    该论文提出了一种无需标签的无监督去学习方法，通过引入变分方法并利用表示分布的近似，实现了在深度模型中消除已遗忘数据信息的目标。

    

    机器去学习旨在从已遗忘数据中删除信息，同时保留训练良好的模型中剩余数据的信息。然而，现有的机器去学习方法通常依赖于整个去学习过程中的完全监督。不幸的是，由于标注真实世界数据集所需的巨大成本，获得这种监督可能实际上是不切实际的。这个挑战促使我们提出一种在去学习过程中无需标签的无监督去学习方法。具体地，我们引入一种变分方法来近似剩余数据的表示分布。利用这种近似，我们调整原始模型以在表示级别消除已遗忘数据中的信息。

    arXiv:2404.00506v1 Announce Type: new  Abstract: Machine unlearning aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to machine unlearning have emerged. However, these methods typically rely on complete supervision throughout the unlearning process. Unfortunately, obtaining such supervision, whether for the forgetting or remaining data, can be impractical due to the substantial cost associated with annotating real-world datasets. This challenge prompts us to propose a supervision-free unlearning approach that operates without the need for labels during the unlearning process. Specifically, we introduce a variational approach to approximate the distribution of representations for the remaining data. Leveraging this approximation, we adapt the original model to eliminate information from the forgotten data at the representation level. To further a
    
[^107]: 具有重建损失的迁移学习

    Transfer Learning with Reconstruction Loss

    [https://arxiv.org/abs/2404.00505](https://arxiv.org/abs/2404.00505)

    本文通过引入额外的重建阶段和重建损失，提出了一种具有共享模型参数和特征表示的模型训练方法，建立了共同信息的概念，用于解决相关任务。

    

    在大多数利用神经网络进行数学优化的应用中，通常为每个特定优化目标训练一个专用模型。然而，在许多场景中，同一组问题输入上经常需要优化几个不同但相关的目标或任务。与为每个问题单独训练不同的神经网络相比，更有效的方法是利用这些目标之间的相关性，使用共享模型参数和特征表示训练多个神经网络模型。为实现这一目标，本文首先建立了共同信息的概念：解决相关任务所需的共享知识，然后提出了一种新颖的模型训练方法，通过在模型中添加一个额外的重建阶段以及相关的新重建损失。该损失用于从选择的隐藏状态开始重新构建共同信息。

    arXiv:2404.00505v1 Announce Type: cross  Abstract: In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidde
    
[^108]: 条件伪可逆标准化流在量化不确定传播中的代理建模中的应用

    Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation

    [https://arxiv.org/abs/2404.00502](https://arxiv.org/abs/2404.00502)

    该论文引入了一种条件伪可逆标准化流模型，可以有效构建代理模型，无需事先了解噪声和函数，直接学习并高效生成样本，用于量化不确定性传播中的正向和反向传播。

    

    我们引入了一种条件伪可逆标准化流用于构建物理模型受附加噪声污染的代理模型，以有效量化正向和反向不确定传播。现有的代理建模方法通常专注于近似物理模型的确定性部分。然而，这种策略需要对噪声有所了解，并且借助辅助采样方法来量化反向不确定传播。在这项工作中，我们开发了条件伪可逆标准化流模型，以直接学习和高效地生成样本从条件概率密度函数。训练过程利用由输入-输出对组成的数据集，无需事先了解噪声和函数。一旦训练完成，我们的模型可以从任何高概率区域被训练覆盖的条件概率密度函数中生成样本。

    arXiv:2404.00502v1 Announce Type: new  Abstract: We introduce a conditional pseudo-reversible normalizing flow for constructing surrogate models of a physical model polluted by additive noise to efficiently quantify forward and inverse uncertainty propagation. Existing surrogate modeling approaches usually focus on approximating the deterministic component of physical model. However, this strategy necessitates knowledge of noise and resorts to auxiliary sampling methods for quantifying inverse uncertainty propagation. In this work, we develop the conditional pseudo-reversible normalizing flow model to directly learn and efficiently generate samples from the conditional probability density functions. The training process utilizes dataset consisting of input-output pairs without requiring prior knowledge about the noise and the function. Our model, once trained, can generate samples from any conditional probability density functions whose high probability regions are covered by the train
    
[^109]: 在单个GPU上以3.29秒实现CIFAR-10数据集94%准确率

    94% on CIFAR-10 in 3.29 Seconds on a Single GPU

    [https://arxiv.org/abs/2404.00498](https://arxiv.org/abs/2404.00498)

    在单个GPU上以3.29秒实现CIFAR-10数据集94%准确率，并提出了一种改进的水平翻转增强方法。

    

    CIFAR-10是机器学习中最广泛使用的数据集之一，每年有数千个研究项目以此为基础。为加速研究并降低实验成本，我们提出了针对CIFAR-10的训练方法，使用单个NVIDIA A100 GPU时，可以在3.29秒内达到94%准确率，10.4秒内达到95%，46.3秒内达到96%。作为加速训练速度的一个因素，我们提出了一种去随机化的水平翻转增强方法，我们展示了该方法在每种情况下均优于标准方法，即在适合水平翻转的所有情况下均有所改善。我们的代码已在https://github.com/KellerJordan/cifar10-airbench上发布。

    arXiv:2404.00498v1 Announce Type: new  Abstract: CIFAR-10 is among the most widely used datasets in machine learning, facilitating thousands of research projects per year. To accelerate research and reduce the cost of experiments, we introduce training methods for CIFAR-10 which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when run on a single NVIDIA A100 GPU. As one factor contributing to these training speeds, we propose a derandomized variant of horizontal flipping augmentation, which we show improves over the standard method in every case where flipping is beneficial over no flipping at all. Our code is released at https://github.com/KellerJordan/cifar10-airbench.
    
[^110]: 时态知识编辑下的多跳问题回答

    Multi-hop Question Answering under Temporal Knowledge Editing

    [https://arxiv.org/abs/2404.00492](https://arxiv.org/abs/2404.00492)

    提出了一个新颖的框架TEMPLE-MQA，通过构建时间感知图和推理路径、结构检索和联合推理阶段，有效地识别多跳问题回答中的时间背景，在基准数据集上显著优于基线模型，并贡献了一个新数据集TKEMQA。

    

    在大语言模型时代，多跳问题回答 (MQA) 在知识编辑 (KE) 下引起了广泛关注。然而，现有的MQA在处理包含显式时间背景的问题时表现不佳。为了解决这一限制，我们提出了一个新颖的框架，即时态知识增强的多跳问题回答 (TEMPLE-MQA)。不同于以往的方法，TEMPLE-MQA首先构建一个时间感知图 (TAG)，以结构化方式存储编辑知识。然后，通过我们提出的推理路径、结构检索和联合推理阶段，TEMPLE-MQA有效地识别问题查询中的时间背景。对基准数据集的实验表明，TEMPLE-MQA显著优于基线模型。此外，我们贡献了一个新数据集，名为TKEMQA，专门为带有时间约束的MQA量身定制，作为首个基准。

    arXiv:2404.00492v1 Announce Type: cross  Abstract: Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal
    
[^111]: PROMPT-SAW：利用关系感知图进行文本提示压缩

    PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression

    [https://arxiv.org/abs/2404.00489](https://arxiv.org/abs/2404.00489)

    提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。

    

    大型语言模型(LLMs)在多种不同的自然语言处理任务中展现出卓越的能力。提示是LLM推理中的基本工具，但我们观察到超长提示会带来显著的成本。现有的压缩长提示的尝试导致压缩提示在可读性和可解释性方面表现不佳，对提示效用产生有害影响。为了解决这一问题，我们提出了PROMPT-SAW：通过关系感知图进行提示压缩，这是一种针对任务不可知和任务感知提示的有效策略。PROMPT-SAW使用提示的文本信息构建图形，在图形中提取关键信息元素，从而得出压缩提示。我们还提出了GSM8K-AUG，即现有GSM8k基准的扩展版本，用于任务不可知提示，以提供全面的评估平台。

    arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
    
[^112]: 噪声感知的布局感知语言模型训练

    Noise-Aware Training of Layout-Aware Language Models

    [https://arxiv.org/abs/2404.00488](https://arxiv.org/abs/2404.00488)

    本论文提出了一种噪声感知训练方法，可以以可扩展的方式使用弱标记文档训练提取器，从而避免了在企业场景中训练大量不同文档类型的自定义提取器所需的昂贵人工标记成本。

    

    一篇视觉丰富的文档（VRD）利用视觉特征和语言线索传播信息。训练一个能够从文档中识别命名实体的自定义提取器需要大量标记为文本和视觉模态的目标文档实例。在企业场景中，我们希望以可扩展的方式为成千上万种不同文档类型训练自定义提取器，这是一个昂贵的瓶颈。在未标记目标文档实例上预训练提取器模型，然后在人工标记实例上进行微调，在这些情况下是行不通的，因为它超出了为提取器分配的最大允许训练时间。本文提出了一种噪声感知训练方法（NAT）来解决这个场景。NAT利用弱标记文档以可扩展的方式训练提取器，而不是获取昂贵的人工标记文档。

    arXiv:2404.00488v1 Announce Type: cross  Abstract: A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degr
    
[^113]: 用于斯拉夫语的跨语言命名实体语料库

    Cross-lingual Named Entity Corpus for Slavic Languages

    [https://arxiv.org/abs/2404.00482](https://arxiv.org/abs/2404.00482)

    介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。

    

    本文介绍了一个手动注释的包含六种斯拉夫语言（保加利亚语、捷克语、波兰语、斯洛文尼亚语、俄语和乌克兰语）命名实体的语料库。这项工作是2017-2023年间斯拉夫自然语言处理研讨会的一系列共享任务的结果。该语料库包含了5017份涵盖七个主题的文档，文档标有五类命名实体，每个实体由类别、引用词和唯一跨语言标识符描述。我们提供了两个训练调整的数据集划分 - 单个主题划分和跨主题划分。对于每个划分，我们使用基于transformer的神经网络架构设置了基准，使用预训练的多语言模型XLM-RoBERTa-large进行命名实体提及识别和分类，以及mT5-large进行命名实体引用词化和链接。

    arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
    
[^114]: 卷积贝叶斯滤波

    Convolutional Bayesian Filtering

    [https://arxiv.org/abs/2404.00481](https://arxiv.org/abs/2404.00481)

    引入不等条件的额外事件，将条件概率转化为特殊积分形式，推广为卷积形式的新滤波框架，称为卷积贝叶斯滤波。

    

    贝叶斯滤波是动态系统状态估计的主要框架。标准版本利用全概率规则和贝叶斯定理交替使用，而定义和计算条件概率的方式对状态分布推断至关重要。以前，条件概率被假定为精确已知，代表一个事件发生概率给定第二个事件的度量。本文发现通过添加一个规定不等条件的额外事件，可以将条件概率转化为类似于卷积的特殊积分形式。基于这一转化，我们展示了过渡概率和输出概率均可以推广为卷积形式，从而得到一个更一般的滤波框架，我们称之为卷积贝叶斯滤波。

    arXiv:2404.00481v1 Announce Type: cross  Abstract: Bayesian filtering serves as the mainstream framework of state estimation in dynamic systems. Its standard version utilizes total probability rule and Bayes' law alternatively, where how to define and compute conditional probability is critical to state distribution inference. Previously, the conditional probability is assumed to be exactly known, which represents a measure of the occurrence probability of one event, given the second event. In this paper, we find that by adding an additional event that stipulates an inequality condition, we can transform the conditional probability into a special integration that is analogous to convolution. Based on this transformation, we show that both transition probability and output probability can be generalized to convolutional forms, resulting in a more general filtering framework that we call convolutional Bayesian filtering. This new framework encompasses standard Bayesian filtering as a spe
    
[^115]: DE-HNN: 一种用于电路网表表示的有效神经模型

    DE-HNN: An effective neural model for Circuit Netlist representation

    [https://arxiv.org/abs/2404.00477](https://arxiv.org/abs/2404.00477)

    设计师们开发了一种名为DE-HNN的神经模型，用于电路网表表示，以解决优化工具运行时间长的问题。

    

    优化工具的运行时间随着设计复杂性的增加而增长，到了可以花费数天来完成一个设计周期的地步，这已经成为一个瓶颈。设计师们希望能够快速获得设计反馈的工具。通过使用过去设计的工具的输入和输出数据，可以尝试构建一个机器学习模型，以显著较短的时间预测设计结果，这比运行工具要快得多。这样的模型的准确性受到设计数据表示的影响，这些数据通常是描述数字电路元素及其连接方式的网表。网表的图表示与图神经网络已经被研究用于这种模型。然而，由于节点数量众多和远程连接的重要性，网表的特性给现有图学习框架带来了几个挑战。

    arXiv:2404.00477v1 Announce Type: new  Abstract: The run-time for optimization tools used in chip design has grown with the complexity of designs to the point where it can take several days to go through one design cycle which has become a bottleneck. Designers want fast tools that can quickly give feedback on a design. Using the input and output data of the tools from past designs, one can attempt to build a machine learning model that predicts the outcome of a design in significantly shorter time than running the tool. The accuracy of such models is affected by the representation of the design data, which is usually a netlist that describes the elements of the digital circuit and how they are connected. Graph representations for the netlist together with graph neural networks have been investigated for such models. However, the characteristics of netlists pose several challenges for existing graph learning frameworks, due to the large number of nodes and the importance of long-range 
    
[^116]: 语言模型的语言校准

    Linguistic Calibration of Language Models

    [https://arxiv.org/abs/2404.00474](https://arxiv.org/abs/2404.00474)

    该论文提出了一种通过语言模型的文本生成来实现语言校准，可以使用户做出校准概率预测的方法。

    

    语言模型可能会在自信幻觉时导致用户做出次优化的下游决策。通过语言模型口头传达其主张正确概率可以缓解这个问题，但现有模型无法生成具有校准置信度声明的文本。我们通过决策角度，为长篇生成形式的语言校准形式化定义：如果语言模型的生成使其用户能够做出校准概率预测，则该模型是语言上校准的。这个定义使得一个训练框架成为可能，其中一个监督微调步骤引导一个语言模型发出带有置信度声明的长篇生成，诸如“我估计有30%的机会…”或“我确信…”，然后是一个强化学习步骤，奖励使用户能够对相关问题提供校准答案的生成。我们对Llama 2 7B 进行语言校准，并发现在自动化和人类测试中...

    arXiv:2404.00474v1 Announce Type: cross  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and huma
    
[^117]: 隐私后门：使用损坏的预训练模型窃取数据

    Privacy Backdoors: Stealing Data with Corrupted Pretrained Models

    [https://arxiv.org/abs/2404.00473](https://arxiv.org/abs/2404.00473)

    预训练模型的权重遭到篡改后，可以构建隐私后门，完全损害微调数据的隐私性，进而对使用差分隐私训练的模型进行严格的隐私攻击。

    

    实践者通常从开放仓库下载预训练的机器学习模型，并进行微调以适应特定应用。我们展示了这种实践引入了新的隐私后门风险。通过篡改预训练模型的权重，攻击者可以完全损害微调数据的隐私性。我们展示了如何为各种模型构建隐私后门，包括transformers，使攻击者能够重构单个微调样本，且成功担保！我们进一步展示，带有后门的模型允许对使用差分隐私（DP）训练的模型进行严格的隐私攻击。如果模型不受信任，则典型的用宽松隐私保证训练DP模型的乐观做法是不安全的。总体而言，我们的工作突出了对机器学习隐私的一种关键且被忽视的供应链攻击。

    arXiv:2404.00473v1 Announce Type: cross  Abstract: Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.
    
[^118]: 基于评分的扩散模型用于光声 tomography 图像重建

    Score-Based Diffusion Models for Photoacoustic Tomography Image Reconstruction

    [https://arxiv.org/abs/2404.00471](https://arxiv.org/abs/2404.00471)

    本研究提出了使用基于评分的扩散模型来解决光声 tomography 图像重建中由于有限传感器覆盖或换能器密度不足而导致的逆问题，该方法能够在不同的换能器稀疏条件下保持鲁棒性。

    

    光声断层扫描（PAT）是一种迅速发展的医学成像技术，它将光学吸收对比与超声成像深度相结合。PAT 中的一个挑战是由于传感器覆盖范围有限或换能器阵列的密度不足而导致声学信号不足，这种情况需要解决一个逆问题。本研究使用基于评分的扩散模型来解决从有限的PAT测量中重建图像的逆问题。所提出的方法允许我们在对模拟血管结构进行扩散模型学习的基础上，同时在不同的换能器稀疏条件下仍然具有鲁棒性。

    arXiv:2404.00471v1 Announce Type: cross  Abstract: Photoacoustic tomography (PAT) is a rapidly-evolving medical imaging modality that combines optical absorption contrast with ultrasound imaging depth. One challenge in PAT is image reconstruction with inadequate acoustic signals due to limited sensor coverage or due to the density of the transducer array. Such cases call for solving an ill-posed inverse reconstruction problem. In this work, we use score-based diffusion models to solve the inverse problem of reconstructing an image from limited PAT measurements. The proposed approach allows us to incorporate an expressive prior learned by a diffusion model on simulated vessel structures while still being robust to varying transducer sparsity conditions.
    
[^119]: 基于变压器卷积神经网络的儿童心音短段分类

    Classification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network

    [https://arxiv.org/abs/2404.00470](https://arxiv.org/abs/2404.00470)

    本研究使用变压器卷积神经网络对儿童心音进行短段分类，调查了自动分类所需的最小信号持续时间，并确定了适合RMSSD和ZCR指标的理想阈值。

    

    先天性心脏病或CHD是由心脏和大血管结构缺陷导致的先天性异常。心脏音频信号可以提供关于心脏机械传导系统的重要细节，并指出与不同类型CHD相关联的特定模式。本研究旨在探讨自动分类心音所需的最小信号持续时间。该研究还调查了最佳信号质量评估指标（连续差的均方根值）RMSSD和（过零率）ZCR值。基于梅尔频率倒谱系数（MFCCs）的特征被用作输入，构建了一个基于变压器的残余一维卷积神经网络，用于分类心音。研究表明0.4是获取适合RMSSD和ZCR指标的信号的理想阈值。此外，为了在基于变压器的卷积神经网络中获得更准确的心音分类结果，研究确定了最佳信号长度。

    arXiv:2404.00470v1 Announce Type: cross  Abstract: Congenital anomalies arising as a result of a defect in the structure of the heart and great vessels are known as congenital heart diseases or CHDs. A PCG can provide essential details about the mechanical conduction system of the heart and point out specific patterns linked to different kinds of CHD. This study aims to investigate the minimum signal duration required for the automatic classification of heart sounds. This study also investigated the optimum signal quality assessment indicator (Root Mean Square of Successive Differences) RMSSD and (Zero Crossings Rate) ZCR value. Mel-frequency cepstral coefficients (MFCCs) based feature is used as an input to build a Transformer-Based residual one-dimensional convolutional neural network, which is then used for classifying the heart sound. The study showed that 0.4 is the ideal threshold for getting suitable signals for the RMSSD and ZCR indicators. Moreover, a minimum signal length of 
    
[^120]: 计算和通信高效的轻量级纵向联邦学习

    Computation and Communication Efficient Lightweighting Vertical Federated Learning

    [https://arxiv.org/abs/2404.00466](https://arxiv.org/abs/2404.00466)

    提出轻量级纵向联邦学习（LVFL）的概念，针对计算和通信效率采用分离的轻量化策略，建立了收敛界限，并在图像分类数据集上得到了验证

    

    在联邦学习（FL）中探索计算和通信效率已成为一个突出和关键的研究领域。尽管大多数现有的努力都集中在提高这些效率，但由于垂直FL的不同过程和模型结构，无法直接应用基于水平FL的技术。因此，我们引入了轻量级纵向联邦学习（LVFL）的概念，旨在提高计算和通信效率。这种方法涉及针对特征模型的单独轻量化策略，以提高计算效率，并针对特征嵌入进行轻量化，以增强通信效率。此外，我们为LVFL算法建立了收敛界限，考虑了通信和计算轻量化比率。我们在图像分类数据集上对该算法进行评估的结果表明，LVFL显著减轻了c

    arXiv:2404.00466v1 Announce Type: new  Abstract: The exploration of computational and communication efficiency within Federated Learning (FL) has emerged as a prominent and crucial field of study. While most existing efforts to enhance these efficiencies have focused on Horizontal FL, the distinct processes and model structures of Vertical FL preclude the direct application of Horizontal FL-based techniques. In response, we introduce the concept of Lightweight Vertical Federated Learning (LVFL), targeting both computational and communication efficiencies. This approach involves separate lightweighting strategies for the feature model, to improve computational efficiency, and for feature embedding, to enhance communication efficiency. Moreover, we establish a convergence bound for our LVFL algorithm, which accounts for both communication and computational lightweighting ratios. Our evaluation of the algorithm on a image classification dataset reveals that LVFL significantly alleviates c
    
[^121]: 利用来自电子病历的预训练和基于Transformer的嵌入来表征阿尔茨海默病及相关痴呆的异质性

    Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to Characterize Heterogeneity Across Alzheimer's Disease and Related Dementias

    [https://arxiv.org/abs/2404.00464](https://arxiv.org/abs/2404.00464)

    利用电子病历中的预训练嵌入和基于Transformer的嵌入，对阿尔茨海默病及相关痴呆患者进行特征分析，发现了疾病人群的异质性，并探讨了亚型的临床意义。

    

    阿尔茨海默病是一种进行性的、使人衰弱的神经退行性疾病，全球有5000万人受影响。尽管这是一个巨大的健康负担，但目前治疗手段有限，其根本原因仍然知之甚少。先前的研究表明存在临床意义的亚型，可能对应于不同的病因、疾病进程，以及最终的适当治疗。本文使用无监督学习技术对一组记忆障碍患者的电子病历进行分析，以表征该疾病人群的异质性。利用医学代码的预训练嵌入以及基于Transformer的临床BERT嵌入来对患者的电子病历进行编码。我们基于合并症和共享文本特征的存在，确定了亚群体的存在，并讨论了它们的临床意义。

    arXiv:2404.00464v1 Announce Type: new  Abstract: Alzheimer's disease is a progressive, debilitating neurodegenerative disease that affects 50 million people globally. Despite this substantial health burden, available treatments for the disease are limited and its fundamental causes remain poorly understood. Previous work has suggested the existence of clinically-meaningful sub-types, which it is suggested may correspond to distinct etiologies, disease courses, and ultimately appropriate treatments. Here, we use unsupervised learning techniques on electronic health records (EHRs) from a cohort of memory disorder patients to characterise heterogeneity in this disease population. Pre-trained embeddings for medical codes as well as transformer-derived Clinical BERT embeddings of free text are used to encode patient EHRs. We identify the existence of sub-populations on the basis of comorbidities and shared textual features, and discuss their clinical significance.
    
[^122]: 在NLP模型中解决统计和因果性别公平性问题

    Addressing Both Statistical and Causal Gender Fairness in NLP Models

    [https://arxiv.org/abs/2404.00463](https://arxiv.org/abs/2404.00463)

    本研究评估了在NLP模型中同时处理统计和因果性别公平性偏见的方法，发现结合统计和因果性去偏置技术能够有效减少偏见。

    

    统计公平性规定对每个受保护群体有相同的结果，而因果公平性要求模型对个体的预测不受其受保护特征的影响。反事实数据增强（CDA）对于减少NLP模型中的偏见是有效的，然而使用CDA训练的模型通常只基于与因果公平性概念密切相关的指标进行评估；同样，为促进统计公平性而设计的基于抽样的方法很少受到因果公平性的评估。在这项工作中，我们评估了在NLP模型中处理性别偏见的统计性和因果性去偏置方法，发现虽然这些方法能够降低偏见，但并不一定会改善其他偏见指标。我们展示了统计性和因果性去偏置技术的组合能够减少通过这两种类型指标衡量的偏见。

    arXiv:2404.00463v1 Announce Type: new  Abstract: Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.
    
[^123]: 具有基础世界模型的无人驾驶汽车零射击安全预测

    Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models

    [https://arxiv.org/abs/2404.00462](https://arxiv.org/abs/2404.00462)

    基于基础世界模型，提出了一种能够直接预测因果未来状态的方法，在安全预测任务中表现优于标准世界模型，并且性能与监督学习相当，尽管没有使用任何数据。

    

    世界模型创造一个代理世界来训练控制器，并通过学习系统的内部动态模型来预测安全违规行为。然而，现有的世界模型仅依赖于统计学习如何观察随着行动而变化，缺乏对代理动态准确性的精确量化，这在安全关键系统中构成重大挑战。为了解决这一挑战，我们提出了将观察嵌入到有意义且因果潜在的表示中的基础世界模型。这使得代理动态能够通过利用无需训练的大型语言模型直接预测因果未来状态。在两个常见的基准测试中，这种新颖模型在安全预测任务中优于标准世界模型，并且性能与监督学习相当，尽管没有使用任何数据。我们通过比较更专业和系统相关的指标评估其性能。

    arXiv:2404.00462v1 Announce Type: new  Abstract: A world model creates a surrogate world to train a controller and predict safety violations by learning the internal dynamic model of systems. However, the existing world models rely solely on statistical learning of how observations change in response to actions, lacking precise quantification of how accurate the surrogate dynamics are, which poses a significant challenge in safety-critical systems. To address this challenge, we propose foundation world models that embed observations into meaningful and causally latent representations. This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free large language model. In two common benchmarks, this novel model outperforms standard world models in the safety prediction task and has a performance comparable to supervised learning despite not using any data. We evaluate its performance with a more specialized and system-relevant metric by compar
    
[^124]: 从对比中出现的快速方法：基于提示的学习中的有效和隐蔽干净标签攻击

    Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning

    [https://arxiv.org/abs/2404.00461](https://arxiv.org/abs/2404.00461)

    基于提示的学习中的干净标签攻击通过特定提示作为触发器，实现成功的同时确保正确标记有毒样本，但仍面临误激活问题和挑战，需要更高比例的毒害。

    

    Prompt-based learning范式表现出卓越的效力，可以提升预训练语言模型（PLMs）在少样本情况下的适应能力。然而，这种学习范式已被证明容易受到后门攻击的影响。当前的干净标签攻击，利用特定提示作为触发器，可以在不需要外部触发器的情况下成功，并确保对有毒样本的正确标记，相比有毒标签攻击更具隐蔽性，但另一方面，它面临着严重的误激活问题，并提出了更大的挑战，需要更高比例的毒害。通过传统的负数据增强方法，我们发现在干净标签设置中在效力和隐蔽性之间取得平衡是具有挑战性的。在解决这一问题时，我们受到后门充当快捷方式的观念的启发，并假设这一快捷方式源于t。

    arXiv:2404.00461v1 Announce Type: cross  Abstract: Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from t
    
[^125]: QuaRot：旋转LLMs中无异常值的4位推断

    QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs

    [https://arxiv.org/abs/2404.00456](https://arxiv.org/abs/2404.00456)

    QuaRot是一种基于旋转的新量化方案，能够在LLMs中进行无异常值的4位推断，实现了端到端的量化，并保持了99%的零-shot表现。

    

    我们介绍了QuaRot，一种基于旋转的新量化方案，能够端对端地将LLMs中的所有权重、激活和KV缓存量化为4位。QuaRot以一种能够去除隐藏状态中异常值但不改变输出的方式对LLMs进行旋转，使得量化变得更简单。这种计算不变性被应用于LLM的隐藏状态（残差），以及前馈组件的激活、注意机制的部分内容和KV缓存。结果是一个量化模型，其中所有矩阵乘法都以4位进行，无需识别需要以更高精度保留的通道。我们的量化LLaMa2-70B模型在最坏情况下仅损失0.29的WikiText-2困惑度，并保留了99%的零-shot表现。代码可在此处获得：https://github.com/spcl/QuaRot。

    arXiv:2404.00456v1 Announce Type: new  Abstract: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.
    
[^126]: 使用分布式狮子进行高效通信的分布式训练

    Communication Efficient Distributed Training with Distributed Lion

    [https://arxiv.org/abs/2404.00438](https://arxiv.org/abs/2404.00438)

    分布式狮子是对 Lion 进行了创新性改进，利用符号操作符降低了通信成本，在分布式训练中取得了与标准 Lion 或 AdamW 优化器相当的性能，并显著减少了通信带宽。

    

    Lion优化器在训练大型AI模型方面与AdamW有一定竞争力，具有在内存、计算和样本效率上的优势。本文介绍了分布式狮子，这是狮子在分布式训练环境中的创新性改进。利用狮子中的符号操作符，我们的分布式狮子只需要在工作节点和中心服务器之间传递二进制或低精度向量，显著降低了通信成本。我们的理论分析证实了分布式狮子的收敛性质。实证结果表明，它在多种任务、工作者数量和批量大小上表现稳健，在视觉和语言问题上表现出色。值得注意的是，分布式狮子在聚合梯度上达到了与标准狮子或AdamW优化器相当的性能，但通信带宽显著减少。这个特性对于训练大型模型尤为有利。

    arXiv:2404.00438v1 Announce Type: cross  Abstract: The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large model
    
[^127]: 利用树估计器自动解释西班牙法律判决在依赖司法管辖的法律类别中的分类

    Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators

    [https://arxiv.org/abs/2404.00437](https://arxiv.org/abs/2404.00437)

    本研究结合自然语言处理和机器学习，提出了一种可解释的法律文本分类系统，使模型的决策变得可理解给最终用户

    

    自动法律文本分类系统已经被提出在文献中，以解决知识从判决中提取并检测其方面。然而，即使它们的模型是可解释的，大多数这些系统都是黑盒的。这可能引发对它们可信度的担忧。因此，本研究提出了一个系统，结合自然语言处理（NLP）和机器学习（ML），以可解释的方式对法律文本进行分类。我们分析了决策中涉及的特征和树结构的决策路径的阈值分叉值，并以自然语言的方式向用户呈现这些信息。这是第一项关于自动分析法律文本的工作，结合NLP和ML以及可解释的人工智能技术，以使模型的决策自动变得可理解给最终用户。此外，法律专家已经验证了我们的解决方案，这些知识也已

    arXiv:2404.00437v1 Announce Type: cross  Abstract: Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has al
    
[^128]: 利用AI发现的街景模式可视化路径

    Visualizing Routes with AI-Discovered Street-View Patterns

    [https://arxiv.org/abs/2404.00431](https://arxiv.org/abs/2404.00431)

    本文通过实验AI技术提出了利用语义潜在向量量化视觉外观特征，计算街景图像相似性并发现空间图像模式，并将其整合到驾驶路线规划器中，最终提出了交互式可视化原型VivaRoutes，帮助用户有效规划驾驶路线。

    

    街道级别的视觉外观在研究社会系统中起着重要作用，例如理解建筑环境、驾驶路径以及相关社会和经济因素。本文研究了这一新的可视化任务，并提出了几项新的贡献。首先，我们尝试了一系列AI技术，并提出了使用语义潜在向量来量化视觉外观特征的解决方案。其次，我们在大量街景图像中计算图像相似性，然后发现空间图像模式。第三，我们将这些发现的模式与新的可视化技术整合到驾驶路线规划器中。最后，我们提出了一个交互式可视化原型VivaRoutes，展示了如何利用这些发现的模式来帮助用户有效。

    arXiv:2404.00431v1 Announce Type: cross  Abstract: Street-level visual appearances play an important role in studying social systems, such as understanding the built environment, driving routes, and associated social and economic factors. It has not been integrated into a typical geographical visualization interface (e.g., map services) for planning driving routes. In this paper, we study this new visualization task with several new contributions. First, we experiment with a set of AI techniques and propose a solution of using semantic latent vectors for quantifying visual appearance features. Second, we calculate image similarities among a large set of street-view images and then discover spatial imagery patterns. Third, we integrate these discovered patterns into driving route planners with new visualization techniques. Finally, we present VivaRoutes, an interactive visualization prototype, to show how visualizations leveraged with these discovered patterns can help users effectively
    
[^129]: 学习科学工作流开发过程中的服务选择决策行为

    Learning Service Selection Decision Making Behaviors During Scientific Workflow Development

    [https://arxiv.org/abs/2404.00420](https://arxiv.org/abs/2404.00420)

    提出了一种基于工作流来源学习服务表示和服务选择决策行为的上下文感知方法，用于推荐科学工作流开发过程中的下一步服务。

    

    越来越多的软件服务被发布到互联网上，这给科学工作流构建过程中的服务推荐带来了巨大挑战。本文提出了一种新颖的上下文感知方法，通过从工作流来源中学习服务表示和服务选择决策行为，来推荐下一个服务。受自然语言句子生成的启发，将科学工作流的组合过程形式化为在工作流目标背景下的逐步过程，下一个服务推荐的问题被映射为下一个单词预测。首先从科学工作流来源中提取历史服务依赖关系以构建知识图谱，然后基于不同的组合路径生成策略生成服务序列，最后根据生成的组合路径语料库进行处理。

    arXiv:2404.00420v1 Announce Type: cross  Abstract: Increasingly, more software services have been published onto the Internet, making it a big challenge to recommend services in the process of a scientific workflow composition. In this paper, a novel context-aware approach is proposed to recommending next services in a workflow development process, through learning service representation and service selection decision making behaviors from workflow provenance. Inspired by natural language sentence generation, the composition process of a scientific workflow is formalized as a step-wise procedure within the context of the goal of workflow, and the problem of next service recommendation is mapped to next word prediction. Historical service dependencies are first extracted from scientific workflow provenance to build a knowledge graph. Service sequences are then generated based on diverse composition path generation strategies. Afterwards, the generated corpus of composition paths are lev
    
[^130]: 在线持续学习中推进潜在专业知识的编排：多层监督和反向自蒸馏

    Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation

    [https://arxiv.org/abs/2404.00417](https://arxiv.org/abs/2404.00417)

    引入了Multi-level Online Sequential Experts (MOSE)方法，通过多层监督和反向自蒸馏，解决了在线持续学习中新旧训练样本学习不足和重复学习的问题。

    

    为了适应现实世界的动态变化，人工智能系统需要以在线方式处理连续到达的内容。在正常持续学习（CL）试图通过离线训练每个任务来解决灾难性遗忘的情况之外，在一次数据流中执行CL的在线持续学习（OCL）是一种更具挑战性但更现实的设置。当前的OCL方法主要依赖于旧训练样本的内存重放。然而，从CL到OCL的一个显着差距源于与重演缓冲区的使用相关的过度拟合-欠拟合困境：对新训练样本的学习不足（欠拟合）以及对少量旧训练样本的重复学习（过拟合）。为此，我们引入了一种新颖的方法，即多级在线顺序专家（MOSE），它将模型作为堆叠的子专家进行培育，并整合了多级监督和反向自蒸馏。

    arXiv:2404.00417v1 Announce Type: cross  Abstract: To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervisi
    
[^131]: 语言模型是航天器操作员

    Language Models are Spacecraft Operators

    [https://arxiv.org/abs/2404.00413](https://arxiv.org/abs/2404.00413)

    该论文旨在将大型语言模型(LLMs)应用于空间导航和控制领域，通过开发纯LLM解决方案，并在 Kerbal 太空计划差分游戏挑战中取得第二名，首次将LLM代理集成到空间资源中。

    

    最近出现了一个趋势，即广泛采用大型语言模型(LLMs)作为根据用户文本提示内容采取行动的自主代理。我们打算将这些概念应用于空间导航和控制领域，使LLMs在自主卫星操作的决策过程中扮演重要角色。作为实现这一目标的第一步，我们为 Kerbal 太空计划差分游戏(KSPDG)挑战开发了一种纯LLM方案，这是一个公开的软件设计竞赛，参与者为卫星操纵创建自主代理，在KSP游戏引擎上运行。我们的方法利用提示工程、少样本提示和微调技术创建了一个效果良好的LLM代理，在竞赛中排名第二。据我们所知，这项工作开创了将LLM代理整合到空间资源中的先河。

    arXiv:2404.00413v1 Announce Type: cross  Abstract: Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling LLMs to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space res
    
[^132]: SVGCraft:超越单个目标文字到SVG综合画布布局

    SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout

    [https://arxiv.org/abs/2404.00412](https://arxiv.org/abs/2404.00412)

    SVGCraft引入了一种端到端框架，可以从文本描述中生成描绘整个场景的矢量图，其中包括利用预训练的LLM进行布局生成、产生遮罩潜变量以进行准确对象放置、融合注意力图以及使用扩散U-Net进行合成，同时通过预训练的编码器和LPIPS损失进行优化。

    

    生成从文本提示到矢量图的VectorArt是一项具有挑战性的视觉任务，需要对已知和未知实体进行多样化而真实的描述。然而，现有研究主要局限于生成单个对象，而不是由多个元素组成的场景。为此，本文介绍了SVGCraft，这是一个新颖的端到端框架，用于从文本描述中生成描绘整个场景的矢量图。该框架利用预训练的LLM从文本提示生成布局，并引入了一种技术，通过生产特定边界框中的掩膜潜变量实现准确的对象放置。它引入了一个融合机制，用于集成注意力图，并使用扩散U-Net进行连贯的合成，加快绘图过程。生成的SVG使用预训练的编码器和LPIPS损失进行优化，通过透明度调制来最大程度地增加相似性。

    arXiv:2404.00412v1 Announce Type: cross  Abstract: Generating VectorArt from text prompts is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained LLM for layout generation from text prompts, this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, th
    
[^133]: Aardvark Weather:端对端数据驱动的天气预报

    Aardvark Weather: end-to-end data-driven weather forecasting

    [https://arxiv.org/abs/2404.00411](https://arxiv.org/abs/2404.00411)

    Aardvark Weather是第一个端到端数据驱动的预报系统，能够取代传统数值天气预报系统，提供全球和本地精准预报。

    

    机器学习正在彻底改变中程天气预测。然而，它仅被应用于天气预测管道的特定和单个组件。因此，这些数据驱动方法无法在没有来自传统操作数值天气预报系统的输入的情况下部署，这是计算成本高昂且不支持端到端优化。在这项工作中，我们采用了一种根本不同的方法，用机器学习模型取代整个数值天气预报管道。我们提出了Aardvark Weather，这是第一个端到端数据驱动的预报系统，它接受原始观测数据作为输入，并提供全球和本地预报。这些全球预报以一度空间分辨率，24小时时间分辨率为多个压力水平24个变量产生，并在五到七天的前期领先时段对每小时气候的预测能力。本地预报是...

    arXiv:2404.00411v1 Announce Type: cross  Abstract: Machine learning is revolutionising medium-range weather prediction. However it has only been applied to specific and individual components of the weather prediction pipeline. Consequently these data-driven approaches are unable to be deployed without input from conventional operational numerical weather prediction (NWP) systems, which is computationally costly and does not support end-to-end optimisation. In this work, we take a radically different approach and replace the entire NWP pipeline with a machine learning model. We present Aardvark Weather, the first end-to-end data-driven forecasting system which takes raw observations as input and provides both global and local forecasts. These global forecasts are produced for 24 variables at multiple pressure levels at one-degree spatial resolution and 24 hour temporal resolution, and are skillful with respect to hourly climatology at five to seven day lead times. Local forecasts are pr
    
[^134]: 使用参数化镜头的深度学习

    Deep Learning with Parametric Lenses

    [https://arxiv.org/abs/2404.00408](https://arxiv.org/abs/2404.00408)

    提出了一种基于镜头的深度学习方法，能够统一各种梯度下降算法和损失函数，同时在连续和离散领域都具有实际意义。

    

    我们提出了一种基于镜头、参数映射和反向导数范畴的机器学习算法的范畴语义。这一基础提供了一个强大的解释性和统一性框架：它包括各种梯度下降算法，如ADAM、AdaGrad和Nesterov动量，以及各种损失函数，如均方误差和Softmax 交叉熵，并涵盖不同的架构，揭示它们之间的相似性和差异。此外，我们的学习方法具有超越熟悉连续域（在光滑映射范畴中建模）的泛化示例，并且可以在布尔和多项式电路的离散设置中实现。我们通过在Python中实现来演示我们框架的实际重要性。

    arXiv:2404.00408v1 Announce Type: new  Abstract: We propose a categorical semantics for machine learning algorithms in terms of lenses, parametric maps, and reverse derivative categories. This foundation provides a powerful explanatory and unifying framework: it encompasses a variety of gradient descent algorithms such as ADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions such as MSE and Softmax cross-entropy, and different architectures, shedding new light on their similarities and differences. Furthermore, our approach to learning has examples generalising beyond the familiar continuous domains (modelled in categories of smooth maps) and can be realised in the discrete setting of Boolean and polynomial circuits. We demonstrate the practical significance of our framework with an implementation in Python.
    
[^135]: Aurora-M: 根据美国行政命令，第一个开源的多语言语言模型进行了红队测试

    Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order

    [https://arxiv.org/abs/2404.00399](https://arxiv.org/abs/2404.00399)

    Aurora-M 是第一个根据美国行政命令进行红队测试的开源多语言模型，通过在英语、芬兰语、印地语、日语、越南语和代码上训练，不断预训练，包括了人工审核的安全说明，总训练 token 数超过 2 万亿个

    

    预训练语言模型支持多种人工智能应用，但是它们在训练时高昂的计算成本限制了可访问性。BLOOM 和 StarCoder 等倡议旨在使预训练模型对于协作社区开发更具民主性。然而，目前存在的模型面临一些挑战：多语言能力有限，持续的预训练会导致灾难性遗忘，而从头开始预训练又具有高昂的计算成本，并且需要遵守人工智能安全和发展法律。本文介绍了 Aurora-M，一个包含 15B 参数的多语言开源模型，训练语言包括英语、芬兰语、印地语、日语、越南语和代码。Aurora-M 不断从 StarCoderPlus 上预训练，额外训练了 4350 亿个 token，总训练 token 数超过了 2 万亿个。它是第一个在人工审核的安全说明上进行微调的开源多语言模型，使其开发与传统

    arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
    
[^136]: 学习真正单调算子及其在非线性逆问题中的应用

    Learning truly monotone operators with applications to nonlinear inverse problems

    [https://arxiv.org/abs/2404.00390](https://arxiv.org/abs/2404.00390)

    通过新定义的惩罚损失学习单调神经网络，解决图像处理中的问题，并利用FBF算法提供收敛保证，以解决非线性逆问题。

    

    本文介绍了一种通过新定义的惩罚损失来学习单调神经网络的新方法。该方法在解决一类变分问题中特别有效，特别是图像处理任务中常遇到的单调包含问题。采用前-后-前（FBF）算法来解决这些问题，在神经网络的Lipschitz常数未知的情况下也能提供解决方案。值得注意的是，FBF算法在学习算子单调的条件下提供收敛保证。借鉴即插即用的方法，我们的目标是将这些新学习的算子应用于解决非线性逆问题。为实现这一目标，我们首先将问题制定为一个变分包含问题，随后训练一个单调神经网络来逼近一个本质上可能不是单调的算子。利用FBF算法

    arXiv:2404.00390v1 Announce Type: cross  Abstract: This article introduces a novel approach to learning monotone neural networks through a newly defined penalization loss. The proposed method is particularly effective in solving classes of variational problems, specifically monotone inclusion problems, commonly encountered in image processing tasks. The Forward-Backward-Forward (FBF) algorithm is employed to address these problems, offering a solution even when the Lipschitz constant of the neural network is unknown. Notably, the FBF algorithm provides convergence guarantees under the condition that the learned operator is monotone. Building on plug-and-play methodologies, our objective is to apply these newly learned operators to solving non-linear inverse problems. To achieve this, we initially formulate the problem as a variational inclusion problem. Subsequently, we train a monotone neural network to approximate an operator that may not inherently be monotone. Leveraging the FBF al
    
[^137]: 基于因子图的受约束布局生成

    Constrained Layout Generation with Factor Graphs

    [https://arxiv.org/abs/2404.00385](https://arxiv.org/abs/2404.00385)

    本文提出了一种基于因子图的方法，用于面向对象的受约束布局生成，可以准确捕捉复杂交互关系，填补了现有方法的不足。

    

    本文解决了在包括平面设计过程在内的多个领域中出现的面向对象的受约束布局生成挑战。现有的作品通常将对象表示为单个节点，缺乏细粒度来准确建模对象之间的复杂交互。为了填补这一空白，我们引入了一种基于因子图的方法，对每个房间引入四个潜变量节点和每个约束引入一个因子节点。因子节点表示与其连接的变量之间的依赖关系，有效捕捉可能是高阶的约束。

    arXiv:2404.00385v1 Announce Type: cross  Abstract: This paper addresses the challenge of object-centric layout generation under spatial constraints, seen in multiple domains including floorplan design process. The design process typically involves specifying a set of spatial constraints that include object attributes like size and inter-object relations such as relative positioning. Existing works, which typically represent objects as single nodes, lack the granularity to accurately model complex interactions between objects. For instance, often only certain parts of an object, like a room's right wall, interact with adjacent objects. To address this gap, we introduce a factor graph based approach with four latent variable nodes for each room, and a factor node for each constraint. The factor nodes represent dependencies among the variables to which they are connected, effectively capturing constraints that are potentially of a higher order. We then develop message-passing on the bipar
    
[^138]: 从学习到分析：通过目标导向的客户选择提高模型效能

    From Learning to Analytics: Improving Model Efficacy with Goal-Directed Client Selection

    [https://arxiv.org/abs/2404.00371](https://arxiv.org/abs/2404.00371)

    提出闭环模型分析框架，通过目标导向客户选择，解决联邦学习中的系统和数据异构性挑战。

    

    联邦学习（FL）是一种吸引人的学习范式，可以在分布式客户端之间学习全局模型同时保护数据隐私。本文提出了一个闭环模型分析框架，允许利用客户端的本地数据有效评估训练好的全局模型。为了解决FL过程中系统和数据异构性带来的挑战，我们研究了一个基于模型分析框架的目标导向客户选择问题，通过选择一部分客户端进行模型训练。这个问题被制定为一个随机多臂老虎机（SMAB）问题。我们首先提出了一个快速初始上置信界（Quick-Init UCB）算法来解决这个SMAB问题，即在联邦分析（FA）框架下。然后，我们进一步提出了一个基于信念传播的UCB（BP-U

    arXiv:2404.00371v1 Announce Type: new  Abstract: Federated learning (FL) is an appealing paradigm for learning a global model among distributed clients while preserving data privacy. Driven by the demand for high-quality user experiences, evaluating the well-trained global model after the FL process is crucial. In this paper, we propose a closed-loop model analytics framework that allows for effective evaluation of the trained global model using clients' local data. To address the challenges posed by system and data heterogeneities in the FL process, we study a goal-directed client selection problem based on the model analytics framework by selecting a subset of clients for the model training. This problem is formulated as a stochastic multi-armed bandit (SMAB) problem. We first put forth a quick initial upper confidence bound (Quick-Init UCB) algorithm to solve this SMAB problem under the federated analytics (FA) framework. Then, we further propose a belief propagation-based UCB (BP-U
    
[^139]: 重新审视随机权重扰动以有效改善泛化能力

    Revisiting Random Weight Perturbation for Efficiently Improving Generalization

    [https://arxiv.org/abs/2404.00357](https://arxiv.org/abs/2404.00357)

    重新审视随机权重扰动方法用于改善深度神经网络的泛化能力，通过两方面的改进实现更高效的效果。

    

    现代深度神经网络（DNNs）的泛化能力是机器学习中的一个基本挑战。两种方法的分支已被提出以寻求平坦最小值并改善泛化：一种是由锐度感知最小化（SAM）领导的方法，通过对抗权重扰动（AWP）最小化最坏情况邻域损失；另一种是通过随机权重扰动（RWP）最小化期望贝叶斯目标。尽管RWP在计算方面具有优势，并且在数学基础上与AWP有密切联系，但其实证性能一直落后于AWP。在本文中，我们重新审视了使用RWP改善泛化的方法，并从两个角度提出了改进: i）泛化和收敛之间的权衡以及ii）随机扰动生成。通过大量实验评估，我们证明了我们增强的RWP方法在提高效率方面表现更优秀。

    arXiv:2404.00357v1 Announce Type: new  Abstract: Improving the generalization ability of modern deep neural networks (DNNs) is a fundamental challenge in machine learning. Two branches of methods have been proposed to seek flat minima and improve generalization: one led by sharpness-aware minimization (SAM) minimizes the worst-case neighborhood loss through adversarial weight perturbation (AWP), and the other minimizes the expected Bayes objective with random weight perturbation (RWP). While RWP offers advantages in computation and is closely linked to AWP on a mathematical basis, its empirical performance has consistently lagged behind that of AWP. In this paper, we revisit the use of RWP for improving generalization and propose improvements from two perspectives: i) the trade-off between generalization and convergence and ii) the random perturbation generation. Through extensive experimental evaluations, we demonstrate that our enhanced RWP methods achieve greater efficiency in enhan
    
[^140]: YNetr：在Plain Scan Liver Tumors (PSLT)上的双编码器架构

    YNetr: Dual-Encoder architecture on Plain Scan Liver Tumors (PSLT)

    [https://arxiv.org/abs/2404.00327](https://arxiv.org/abs/2404.00327)

    YNetr模型在Plain Scan Liver Tumors数据集上实现了62.63%的Dice系数，优于其他模型，填补了肝肿瘤普通扫描分割数据集和算法的空白。

    

    肝肿瘤是肝脏中不正常的生长，可能是良性或恶性，肝癌是全球重要的健康问题。然而，目前没有用于肝肿瘤普通扫描分割的数据集，也没有相关算法。为了填补这一空白，我们提出了Plain Scan Liver Tumors(PSLT)和YNetr。使用40个肝肿瘤普通扫描分割数据集进行了组装和注释。同时，我们利用Dice系数作为评估YNetr产生的分割结果的指标，有利于捕获不同频率信息。YNetr模型在PSLT数据集上实现了62.63%的Dice系数，超过其他公开模型的准确度范围1.22%。进行了与包括 UNet 3+、XNet、UNetr、Swin UNetr、Trans-BTS、COTr、nnUNetv2 (2D)、nnUNetv2 (3D fullres)、MedNext 在内的一系列模型的比较评估。

    arXiv:2404.00327v1 Announce Type: cross  Abstract: Background: Liver tumors are abnormal growths in the liver that can be either benign or malignant, with liver cancer being a significant health concern worldwide. However, there is no dataset for plain scan segmentation of liver tumors, nor any related algorithms. To fill this gap, we propose Plain Scan Liver Tumors(PSLT) and YNetr. Methods: A collection of 40 liver tumor plain scan segmentation datasets was assembled and annotated. Concurrently, we utilized Dice coefficient as the metric for assessing the segmentation outcomes produced by YNetr, having advantage of capturing different frequency information. Results: The YNetr model achieved a Dice coefficient of 62.63% on the PSLT dataset, surpassing the other publicly available model by an accuracy margin of 1.22%. Comparative evaluations were conducted against a range of models including UNet 3+, XNet, UNetr, Swin UNetr, Trans-BTS, COTr, nnUNetv2 (2D), nnUNetv2 (3D fullres), MedNext
    
[^141]: 基于CLIP的离群值合成用于少样本场景下的OOD检测

    CLIP-driven Outliers Synthesis for few-shot OOD detection

    [https://arxiv.org/abs/2404.00323](https://arxiv.org/abs/2404.00323)

    提出了一种基于CLIP的离群值合成方法（CLIP-OS），能够在少样本场景下的OOD检测中解决缺乏可靠OOD监督信息的问题，实现了ID相关信息和ID不相关信息的分离，通过混合ID相关特征合成可靠的OOD数据。

    

    少样本场景下的OOD检测着重于识别那些在训练过程中未见过的类别的、仅使用少量标记的ID图像的out-of-distribution (OOD) 图像。到目前为止，主流的策略是基于大规模视觉-语言模型，如CLIP。然而，这些方法忽视了一个关键问题：缺乏可靠的OOD监督信息，这可能导致在ID和OOD之间产生偏见的边界。为了解决这个问题，我们提出了基于CLIP的离群值合成（CLIP-OS）。首先，CLIP-OS通过新提出的patch uniform convolution增强了patch级特征的感知能力，并通过采用CLIP-surgery-discrepancy自适应地获取ID相关信息的比例，从而实现了ID相关信息和ID不相关信息之间的分离。接下来，CLIP-OS通过混合来自不同类别的ID相关特征来合成可靠的OOD数据，以提供OOD监督信息。

    arXiv:2404.00323v1 Announce Type: cross  Abstract: Few-shot OOD detection focuses on recognizing out-of-distribution (OOD) images that belong to classes unseen during training, with the use of only a small number of labeled in-distribution (ID) images. Up to now, a mainstream strategy is based on large-scale vision-language models, such as CLIP. However, these methods overlook a crucial issue: the lack of reliable OOD supervision information, which can lead to biased boundaries between in-distribution (ID) and OOD. To tackle this problem, we propose CLIP-driven Outliers Synthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception by newly proposed patch uniform convolution, and adaptively obtains the proportion of ID-relevant information by employing CLIP-surgery-discrepancy, thus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS synthesizes reliable OOD data by mixing up ID-relevant features from different classes to provide OOD supervision i
    
[^142]: TRABSA：使用基于注意力的BiLSTM和Twitter-RoBERTa进行可解释的推文情感分析

    TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa

    [https://arxiv.org/abs/2404.00297](https://arxiv.org/abs/2404.00297)

    TRABSA是一个集成了transformer架构、注意力机制和BiLSTM网络的混合框架，利用RoBERTa在大量推特上训练，填补了情感分析领域的差距，实现了94%的准确性和显著的性能提升。

    

    情感分析对于理解公众舆论和消费者行为至关重要。现有模型面临着语言多样性、泛化能力和可解释性方面的挑战。我们提出了TRABSA，这是一个集成了基于transformer的架构、注意力机制和BiLSTM网络的混合框架，旨在解决这些挑战。利用在124M条推文上训练的RoBERTa，我们填补了情感分析基准测试中的差距，确保了最先进的准确性。通过将来自32个国家和美国各州的推文与数据集相结合，我们比较了六种词嵌入技术和三种基于词典的标注技术，并选择了最佳技术以实现最佳情感分析效果。TRABSA以94%的准确性和显著的精确度、召回率和F1得分增益，胜过了传统的机器学习和深度学习模型。在不同数据集上的评估显示了一致的优越性和泛化能力。SHAP和LIME分析提高了可解释性，增强了信心。

    arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
    
[^143]: 基于大型语言模型增强强化学习的调查:概念、分类和方法

    Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

    [https://arxiv.org/abs/2404.00282](https://arxiv.org/abs/2404.00282)

    大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。

    

    随着大规模语言模型(LLMs)拥有广泛的预训练知识和高级通用能力，它们在增强学习方面如多任务学习、样本效率和任务规划等方面展现出潜力。本调查综述了现有$\textit{LLM增强RL}$文献，总结了其与传统RL方法的特征，旨在澄清研究范围和未来研究方向。利用经典的Agent-环境交互范例，我们提出了一个结构化的分类法，系统地将LLMs在RL中的功能分类，包括四种角色：信息处理器、奖励设计者、决策者和生成器。此外，针对每个角色，我们总结了方法论，分析了缓解的特定RL挑战，并提供了未来方向的见解。最后，潜在应用、前景

    arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
    
[^144]: TG-NAS：利用Transformer和图卷积网络与零成本代理进行高效神经结构搜索

    TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search

    [https://arxiv.org/abs/2404.00271](https://arxiv.org/abs/2404.00271)

    TG-NAS提出了一种新型模型通用代理，利用Transformer的运算符嵌入生成器和图卷积网络来预测架构性能，指导神经结构搜索。

    

    神经结构搜索(NAS)是一种发现新的卷积神经网络(CNN)架构的有效方法。然而，现有方法通常需要耗时的训练或密集的采样和评估。零成本NAS旨在为架构性能预测创建免训练代理。然而，现有代理性能亚优，并且常常被模型参数数量或浮点运算次数等简单指标所超越。此外，现有基于模型的代理无法将泛化到新的搜索空间，其中具有未见新类型运算符且不带有黄金准确度。一个普遍最优的代理仍然难以找到。我们引入了TG-NAS，一种利用基于Transformer的运算符嵌入生成器和图卷积网络(GCN)来预测架构性能的新型模型通用代理。这种方法指导着在任何给定搜索空间内进行神经结构搜索。

    arXiv:2404.00271v1 Announce Type: cross  Abstract: Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any giv
    
[^145]: 将数据集提炼为语言模型，用于文本级数据集蒸馏

    DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation

    [https://arxiv.org/abs/2404.00264](https://arxiv.org/abs/2404.00264)

    提出了一种名为DiLM的文本数据集蒸馏方法，通过训练语言模型生成文本数据作为合成训练样本，解决了嵌入级别蒸馏数据集无法用于训练其他模型的问题。

    

    数据集蒸馏旨在通过创建少量信息丰富的合成样本来压缩训练数据集，从而使得在其上训练的神经网络的性能能够与在原始训练数据集上训练的网络一样好。当前的文本数据集蒸馏方法将每个合成样本创建为词嵌入序列而不是文本，以应用基于梯度的优化；然而，这种嵌入级别的蒸馏数据集无法用于训练其他模型，其词嵌入权重不同于用于蒸馏的模型。为解决这一问题，本文提出了一种新颖的文本数据集蒸馏方法，称为Distilling dataset into Language Model（DiLM），该方法训练语言模型以生成信息丰富的文本数据作为合成训练样本，而不是直接优化合成样本。我们在各种文本分类数据集上评估了DiLM，并展示了从DiLM 中蒸馏得到的合成数据集的优秀表现。

    arXiv:2404.00264v1 Announce Type: new  Abstract: Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outp
    
[^146]: YOLOOC: 基于YOLO的开放类别增量目标检测与新类别发现

    YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery

    [https://arxiv.org/abs/2404.00257](https://arxiv.org/abs/2404.00257)

    该论文提出了基于YOLO架构的YOLOOC检测器，针对开放类别设置引入了标签平滑，有效应对新类别检测和增量学习的挑战。

    

    最近，由于其在实践中的运用，开放世界目标检测（OWOD）受到了很多关注。挑战在于模型如何检测新类别，然后增量学习它们而不会忘记先前已知的类别。先前的方法依赖于强监督或弱监督的新类别数据用于新类别检测，这可能不适用于实际应用。我们构建了一个新的基准，其中新类别只在推断阶段遇到。我们提出了一种基于YOLO架构的新OWOD检测器YOLOOC，专门针对开放类别的设置。我们引入了标签平滑以防止检测器过于自信地将新类别映射到已知类别并发现新类别。在我们更加现实的设置上进行的大量实验表明，我们的方法在我们的新基准下发现新类别的有效性。

    arXiv:2404.00257v1 Announce Type: cross  Abstract: Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.
    
[^147]: 蛋白质表示学习的聚类

    Clustering for Protein Representation Learning

    [https://arxiv.org/abs/2404.00254](https://arxiv.org/abs/2404.00254)

    该论文提出了一个神经聚类框架，通过考虑蛋白质的一级和三级结构信息，自动发现蛋白质的关键组分，并利用迭代聚类策略创建层次化和信息丰富的蛋白质表示。

    

    蛋白质表示学习是一个具有挑战性的任务，旨在从氨基酸序列中捕获蛋白质的结构和功能。以往的方法很大程度上忽视了并非所有氨基酸对于蛋白质的折叠和活性都同样重要的事实。本文提出了一个神经聚类框架，通过考虑蛋白质的一级和三级结构信息，可以自动发现蛋白质的关键组分。我们的框架将蛋白质视为一个图，其中每个节点代表一个氨基酸，每条边代表氨基酸之间的空间或序列连接。然后，我们应用迭代聚类策略将节点根据它们的一维和三维位置分组，并为每个群分配分数。我们选择得分最高的群，并使用它们的中心节点进行下一轮的聚类，直到获得层次化和信息丰富的表示。

    arXiv:2404.00254v1 Announce Type: new  Abstract: Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of th
    
[^148]: 利用迁移学习促进过程控制的强化学习：观点

    Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives

    [https://arxiv.org/abs/2404.00247](https://arxiv.org/abs/2404.00247)

    本文从迁移学习的角度探讨了如何将其与强化学习相结合，为过程控制带来新的可能性。

    

    本文从迁移学习的角度，为过程控制中的深度强化学习（DRL）提供了深入见解。我们分析了在过程工业领域应用DRL所面临的挑战，以及引入迁移学习的必要性。此外，我们为未来研究方向提供了建议和展望，探讨了如何将迁移学习与DRL结合起来加强过程控制。

    arXiv:2404.00247v1 Announce Type: cross  Abstract: This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.
    
[^149]: 数字世界中的信息安全与隐私：一些精选主题

    Information Security and Privacy in the Digital World: Some Selected Topics

    [https://arxiv.org/abs/2404.00235](https://arxiv.org/abs/2404.00235)

    数字世界中的信息安全与隐私面临新挑战，需要更强大和更具韧性的安全方案，这本书呈现了密码学和计算与通信安全领域的最前沿研究。

    

    在生成式人工智能和物联网时代，随着数据量的爆炸性增长以及与之相关的处理、分析和存储需求，我们面临着识别虚假信息、保护敏感数据隐私等多项新挑战。这导致对于更强大和更具韧性的身份验证、完整性保护、加密、不可否认以及数据隐私保护方案需求不断增加。本书的章节呈现了密码学和计算与通信安全领域一些最前沿的研究工作。

    arXiv:2404.00235v1 Announce Type: cross  Abstract: In the era of generative artificial intelligence and the Internet of Things, while there is explosive growth in the volume of data and the associated need for processing, analysis, and storage, several new challenges are faced in identifying spurious and fake information and protecting the privacy of sensitive data. This has led to an increasing demand for more robust and resilient schemes for authentication, integrity protection, encryption, non-repudiation, and privacy-preservation of data. The chapters in this book present some of the state-of-the-art research works in the field of cryptography and security in computing and communications.
    
[^150]: 通过元学习实现数据驱动模型预测控制的高效自动调节

    Efficient Automatic Tuning for Data-driven Model Predictive Control via Meta-Learning

    [https://arxiv.org/abs/2404.00232](https://arxiv.org/abs/2404.00232)

    本文提出了一种使用元学习方法Portfolio来改进AutoMPC的效率和稳定性，在多个任务上优化BO的初始设计，并通过固定初始配置来稳定调节过程，在有限计算资源内超越了纯BO方法。

    

    AutoMPC是一个自动优化数据驱动模型预测控制的Python软件包。然而，当使用纯贝叶斯优化探索大搜索空间时，可能会导致计算负担过重和不稳定。为了解决这些问题，本文提出了一种称为Portfolio的元学习方法，通过热启动贝叶斯优化来提高AutoMPC的效率和稳定性。Portfolio利用前一任务的多样配置集优化BO的初始设计，并通过固定初始配置而不是随机选择来稳定调节过程。实验结果表明，在11个非线性控制仿真基准和1个物理水下软体机器人数据集上，Portfolio在有限计算资源内优于纯BO寻找AutoMPC的理想解决方案。

    arXiv:2404.00232v1 Announce Type: cross  Abstract: AutoMPC is a Python package that automates and optimizes data-driven model predictive control. However, it can be computationally expensive and unstable when exploring large search spaces using pure Bayesian Optimization (BO). To address these issues, this paper proposes to employ a meta-learning approach called Portfolio that improves AutoMPC's efficiency and stability by warmstarting BO. Portfolio optimizes initial designs for BO using a diverse set of configurations from previous tasks and stabilizes the tuning process by fixing initial configurations instead of selecting them randomly. Experimental results demonstrate that Portfolio outperforms the pure BO in finding desirable solutions for AutoMPC within limited computational resources on 11 nonlinear control simulation benchmarks and 1 physical underwater soft robot dataset.
    
[^151]: 基于注意力机制的形状变形网络用于无伪影几何重构骨盆腰椎MR图像

    Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images

    [https://arxiv.org/abs/2404.00231](https://arxiv.org/abs/2404.00231)

    这项工作提出了TransDeformer，使用注意力机制实现了对腰椎轮廓的高空间准确性重建，并跨患者实现了网格对应，为医学参数测量提供了可靠性，还设计了变体用于错误估计。

    

    腰椎椎间盘退变，是腰椎间盘渐进性结构性磨损，被认为在腰部疼痛中发挥重要作用，这是一个重要的全球健康关注焦点。从MR图像中自动重建腰椎几何形状，将使医学参数的快速测量成为可能，以评估腰椎状态，从而确定合适的治疗方案。现有的基于图像分割的技术通常会生成错误的分割或不适合医学参数测量的无结构点云。在这项工作中，我们提出了TransDeformer：一种新颖的基于注意力机制的深度学习方法，以高空间准确度和患者间网格对应的方式重建腰椎轮廓，并且我们还提出了一种TransDeformer的变种用于错误估计。特别是，我们设计了新的注意力模块和新的注意力公式，将图像特征和标记化的轮廓特征集成起来，用于预测...

    arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
    
[^152]: InfLoRA：无干扰的低秩自适应持续学习方法

    InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning

    [https://arxiv.org/abs/2404.00228](https://arxiv.org/abs/2404.00228)

    InfLoRA提出了一种新的PEFT方法，名为无干扰低秩自适应（InfLoRA），用于持续学习，旨在消除新任务对旧任务的干扰，帮助模型在稳定性和可塑性之间取得良好平衡。

    

    持续学习要求模型依次学习多个任务。在持续学习中，模型应具备在旧任务上维持性能（稳定性）和不断适应新任务的能力（可塑性）。最近，基于参数高效微调（PEFT）的持续学习方法变得越来越受欢迎。尽管现有基于PEFT的持续学习方法表现出比非PEFT方法更优秀的性能，但大多数方法并未考虑如何消除新任务对旧任务的干扰，从而阻碍模型在稳定性和可塑性之间取得良好平衡。本文提出了一种新的PEFT方法，称为无干扰低秩自适应（InfLoRA）方法，用于持续学习。

    arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
    
[^153]: 基于异构对比学习的基础模型及其发展

    Heterogeneous Contrastive Learning for Foundation Models and Beyond

    [https://arxiv.org/abs/2404.00225](https://arxiv.org/abs/2404.00225)

    对比自监督学习为基础模型提供了泛化能力，本调查对基于对比学习的异构学习方法进行了评估，突出了挑战和未来趋势。

    

    在大数据和人工智能时代，利用对比自监督学习来建模大规模异构数据的新兴范式受到关注。许多现有的基础模型通过学习紧凑、高质量的表示形式而受益于对比自监督学习的泛化能力，而无需依赖任何标签信息。本调查对基础模型的异构对比学习的当前情况进行了批判性评估，突出了对比学习的挑战和未来趋势。特别是，我们首先介绍了最近提出的基于对比学习的高级方法如何处理视图的异构性，以及对比学习在基础模型中的作用。

    arXiv:2404.00225v1 Announce Type: new  Abstract: In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how con
    
[^154]: 可部分观测序贯自相关数据的变点检测：上置信区间方法

    Partially-Observable Sequential Change-Point Detection for Autocorrelated Data via Upper Confidence Region

    [https://arxiv.org/abs/2404.00220](https://arxiv.org/abs/2404.00220)

    提出了一种适用于可部分观测多传感器序贯变点检测的自适应上置信区间状态空间模型（AUCRSS），通过自适应采样策略实现高效的变点检测和定位。

    

    arXiv:2404.00220v1 公告类型: 交叉摘要: 多变量自相关数据的序贯变点检测是实践中一个非常常见的问题。然而，当感知资源有限时，每次感知时间点只能观测多变量系统的一个子集。这就提出了可部分观测多传感器序贯变点检测的问题。为此，我们提出了一种称为自适应上置信区间状态空间模型(AUCRSS)的检测方案。它通过状态空间模型(SSM)对多变量时间序列进行建模，并利用自适应采样策略进行高效的变点检测和定位。对在线推断SSM的部分可观测卡尔曼滤波算法进行开发，并相应地，基于广义似然比检验的变点检测方案被开发。分析了其检测能力与自适应采样策略的关系。同时，通过将检测能力视为一种再

    arXiv:2404.00220v1 Announce Type: cross  Abstract: Sequential change point detection for multivariate autocorrelated data is a very common problem in practice. However, when the sensing resources are limited, only a subset of variables from the multivariate system can be observed at each sensing time point. This raises the problem of partially observable multi-sensor sequential change point detection. For it, we propose a detection scheme called adaptive upper confidence region with state space model (AUCRSS). It models multivariate time series via a state space model (SSM), and uses an adaptive sampling policy for efficient change point detection and localization. A partially-observable Kalman filter algorithm is developed for online inference of SSM, and accordingly, a change point detection scheme based on a generalized likelihood ratio test is developed. How its detection power relates to the adaptive sampling strategy is analyzed. Meanwhile, by treating the detection power as a re
    
[^155]: 功能边缘网络建模

    Functional-Edged Network Modeling

    [https://arxiv.org/abs/2404.00218](https://arxiv.org/abs/2404.00218)

    本研究提出了一种功能边缘网络模型，通过将边视为功能数据，并引入额外维度来表示函数，使用Tucker功能分解处理功能邻接张量，进行模型推断以解决不规则观测问题，并通过正则化使基础矩阵对称化，最终展示了模型的理想属性。

    

    与现有作品形成对比，现有作品都将节点视为函数，并使用边来表示不同函数之间的关系。我们的目标是网络建模，其中边是功能数据，并将邻接矩阵转换为功能邻接张量，引入一个额外的维度专门用于函数表示。我们使用Tucker功能分解来处理功能邻接张量，为进一步考虑节点之间的社区，对基础矩阵进行正则化使其对称化。此外，为了处理功能边的不规则观测，我们进行模型推断以解决张量完成问题，通过Riemann共轭梯度下降方法进行优化。除此之外，我们还推导出几个定理来展示功能边缘网络模型的理想属性。最后，我们使用模拟数据和真实地铁系统数据评估了我们提出的模型的有效性。

    arXiv:2404.00218v1 Announce Type: cross  Abstract: Contrasts with existing works which all consider nodes as functions and use edges to represent the relationships between different functions. We target at network modeling whose edges are functional data and transform the adjacency matrix into a functional adjacency tensor, introducing an additional dimension dedicated to function representation. Tucker functional decomposition is used for the functional adjacency tensor, and to further consider the community between nodes, we regularize the basis matrices to be symmetrical. Furthermore, to deal with irregular observations of the functional edges, we conduct model inference to solve a tensor completion problem. It is optimized by a Riemann conjugate gradient descent method. Besides these, we also derive several theorems to show the desirable properties of the functional edged network model. Finally, we evaluate the efficacy of our proposed model using simulation data and real metro sys
    
[^156]: 大型语言模型下的多条件排序

    Multi-Conditional Ranking with Large Language Models

    [https://arxiv.org/abs/2404.00211](https://arxiv.org/abs/2404.00211)

    该论文提出了一种新颖的分解推理方法(MCRank)，用于解决大型语言模型在多条件排序任务中性能下降的问题。

    

    利用大型语言模型(LLMs)对一组项目进行排序已成为推荐和检索系统中的常见方法。在这篇论文中，我们定义并探讨了多条件排序的任务，引入了一个名为MCRank的基准，旨在评估跨不同项目类型和条件进行多条件排序。我们使用MCRank对LLMs进行分析表明，随着项目和条件数量以及复杂性的增长，性能显著下降。为了克服这一限制，我们提出了一种新颖的分解推理方法，包括提取和排序条件，然后迭代地对条件进行排序。

    arXiv:2404.00211v1 Announce Type: new  Abstract: Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the i
    
[^157]: 人类-语言模型协作的因果推断

    Causal Inference for Human-Language Model Collaboration

    [https://arxiv.org/abs/2404.00207](https://arxiv.org/abs/2404.00207)

    本文研究了人类和语言模型之间的协作动态，并提出了一个新的因果估计Incremental Stylistic Effect来解释如何改变合作结果。

    

    在本文中，我们研究了人类和语言模型（LMs）之间的协作动态，这些互动通常涉及LMs提出文本段落，而人类编辑或回应这些建议。在这种情况下与LMs进行有效的互动要求人类辨别出有效的基于文本的互动策略，例如编辑和回应样式，从历史人类-LM互动中。这个目标本质上是因果关系，受到反事实“如果”问题的驱动:如果人类采用不同的文本编辑/精炼策略，协作的结果会如何改变？回答这个因果推断问题的一个关键挑战是制定一个适当的因果估计:传统的平均处理效应（ATE）估计由于文本的高维度而不适用于基于文本的处理。为了解决这一问题，我们引入了一个新的因果估计 - 增量风格效应

    arXiv:2404.00207v1 Announce Type: cross  Abstract: In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (
    
[^158]: 基于PPO的DRL自调PID非线性无人机控制器用于稳健自主飞行

    A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights

    [https://arxiv.org/abs/2404.00204](https://arxiv.org/abs/2404.00204)

    该项目将非线性深度强化学习（DRL）代理引入无人机控制中，取代传统线性PID控制器，实现了无缝过渡、提高响应速度和稳定性，同时结合PPO策略训练DRL代理，并利用高精度跟踪系统提高自主飞行精度。

    

    该项目旨在通过将非线性深度强化学习（DRL）代理作为传统线性比例积分微分（PID）控制器的替代品，从而彻底改变无人机飞行控制。主要目标是在手动和自主模式之间实现无缝过渡，提高响应速度和稳定性。我们在Gazebo模拟器中利用近端策略优化（PPO）强化学习策略来训练DRL代理。添加20000美元的室内Vicon跟踪系统提供<1mm的定位精度，显着提高了自主飞行精度。为了在最短的无碰撞轨迹中导航无人机，我们还建立了一个三维A*路径规划器并成功地将其实施到实际飞行中。

    arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers <1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
    
[^159]: 通过密度估计进行多策略评估

    Multiple-policy Evaluation via Density Estimation

    [https://arxiv.org/abs/2404.00195](https://arxiv.org/abs/2404.00195)

    该研究提出一种名为 $\mathrm{CAESAR}$ 的算法，通过计算一个近似的最优离线采样分布，同时估计多个策略的价值，以解决多策略评估问题。

    

    在这项工作中，我们专注于多策略评估问题，给定一组 $K$ 个目标策略，目标是以至少 $1-\delta$ 的概率评估它们的性能（期望总奖励）达到精度 $\epsilon$。我们提出了一种名为 $\mathrm{CAESAR}$ 的算法来解决这个问题。我们的方法基于计算一个近似的最优离线采样分布，并利用从中采样的数据来同时估计策略价值。$\mathrm{CAESAR}$ 包括两个阶段。在第一个阶段，我们以随着 $\tilde{O}(\frac{1}{\epsilon})$ 缩放的低订单采样复杂性率产生目标策略的访问分布的粗略估计。在第二阶段，我们近似最优离线采样分布，并通过最小化一个逐步二次损失函数来计算所有目标策略的重要性权重比例。

    arXiv:2404.00195v1 Announce Type: cross  Abstract: In this work, we focus on the multiple-policy evaluation problem where we are given a set of $K$ target policies and the goal is to evaluate their performance (the expected total rewards) to an accuracy $\epsilon$ with probability at least $1-\delta$. We propose an algorithm named $\mathrm{CAESAR}$ to address this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. $\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse estimates of the vistation distributions of the target policies at a low order sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the
    
[^160]: 多区域迁移学习用于在卫星图像中分割农田边界

    Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries in Satellite Images with Limited Labels

    [https://arxiv.org/abs/2404.00179](https://arxiv.org/abs/2404.00179)

    提出了一种多区域迁移学习的方法，用于在卫星图像中分割农田边界，解决了标记数据不足的问题。

    

    田界勾画的目标是在高空感应图像（例如来自卫星或无人机的图像）中预测单个农田的多边形边界和内部。自动勾画田界是许多农业实际用例中必不可少的任务，例如估计一个地区的耕地面积或预测一个田地的季末产量。田界勾画可以被归类为一个实例分割问题，但与传统用于实例分割的计算机视觉数据集相比，它提出了独特的研究挑战。以前作品的实际适用性也受限于一个假设，即在田界勾画模型将被应用的地方有足够大的标记数据集可用，这对大多数地区来说并非现实（尤其是在资源匮乏的地区，如撒哈拉以南的非洲）。我们提出了一种用于农田边界分割的方法。

    arXiv:2404.00179v1 Announce Type: cross  Abstract: The goal of field boundary delineation is to predict the polygonal boundaries and interiors of individual crop fields in overhead remotely sensed images (e.g., from satellites or drones). Automatic delineation of field boundaries is a necessary task for many real-world use cases in agriculture, such as estimating cultivated area in a region or predicting end-of-season yield in a field. Field boundary delineation can be framed as an instance segmentation problem, but presents unique research challenges compared to traditional computer vision datasets used for instance segmentation. The practical applicability of previous work is also limited by the assumption that a sufficiently-large labeled dataset is available where field boundary delineation models will be applied, which is not the reality for most regions (especially under-resourced regions such as Sub-Saharan Africa). We present an approach for segmentation of crop field boundarie
    
[^161]: 超越停赛：结束体育联盟的两阶段方法论

    Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues

    [https://arxiv.org/abs/2404.00178](https://arxiv.org/abs/2404.00178)

    该研究提出了一种利用预测性和处方性分析的数据驱动模型，为结束现有体育赛季提供了一种新的两阶段方法，以产生类似完整赛季结果的队伍排名。

    

    问题定义：专业体育联盟可能因各种原因暂停，比如最近的COVID-19大流行。重要问题是联盟在重新开赛时如何适当地选择其余比赛的子集，以在缩短的时间内结束赛季。学术/实践意义：尽管有丰富的文献关于从零开始安排整个赛季，结束现有赛季却截然不同。我们的方法旨在实现团队排名，使其与完整赛季比赛的结果类似。方法：我们提出了一个数据驱动模型，利用预测性和处方性分析，为剩余赛季制定一个由原定安排比赛组成的赛程子集。我们的模型在随机优化模型中引入了基于新颖排名目标，其参数首先使用预测性确定。

    arXiv:2404.00178v1 Announce Type: cross  Abstract: Problem definition: Professional sports leagues may be suspended due to various reasons such as the recent COVID-19 pandemic. A critical question the league must address when re-opening is how to appropriately select a subset of the remaining games to conclude the season in a shortened time frame. Academic/practical relevance: Despite the rich literature on scheduling an entire season starting from a blank slate, concluding an existing season is quite different. Our approach attempts to achieve team rankings similar to that which would have resulted had the season been played out in full. Methodology: We propose a data-driven model which exploits predictive and prescriptive analytics to produce a schedule for the remainder of the season comprised of a subset of originally-scheduled games. Our model introduces novel rankings-based objectives within a stochastic optimization model, whose parameters are first estimated using a predictive 
    
[^162]: 比较超优化的机器学习模型以预测有机太阳能电池效率退化

    Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells

    [https://arxiv.org/abs/2404.00173](https://arxiv.org/abs/2404.00173)

    该研究通过超优化的机器学习模型，成功预测有机太阳能电池效率退化，准确度高且具有实用价值。

    

    本文提出了一组最优化的机器学习（ML）模型，来表示多层结构ITO/PEDOT:PSS/P3HT:PCBM/Al聚合物有机太阳能电池（OSCs）的功率转换效率（PCE）所遭受的时间退化。为此，我们生成了一个包含996条数据的数据库，其中包括关于制造过程和环境条件的7个变量，超过180天。然后，我们依靠一个软件框架，汇集了一系列自动化ML协议，通过简单的命令行界面顺序地针对我们的数据库执行，从而轻松地通过详尽的基准测试来超优化和随机化ML模型的种子，以获得最佳模型。所达到的准确度达到了广泛超过0.90的系数确定值（R2），而均方根误差（RMSE）、平方误差（SSE）和平均绝对误

    arXiv:2404.00173v1 Announce Type: new  Abstract: This work presents a set of optimal machine learning (ML) models to represent the temporal degradation suffered by the power conversion efficiency (PCE) of polymeric organic solar cells (OSCs) with a multilayer structure ITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996 entries, which includes up to 7 variables regarding both the manufacturing process and environmental conditions for more than 180 days. Then, we relied on a software framework that brings together a conglomeration of automated ML protocols that execute sequentially against our database by simply command-line interface. This easily permits hyper-optimizing and randomizing seeds of the ML models through exhaustive benchmarking so that optimal models are obtained. The accuracy achieved reaches values of the coefficient determination (R2) widely exceeding 0.90, whereas the root mean squared error (RMSE), sum of squared error (SSE), and mean absolute er
    
[^163]: 通过深度数据和深度度量学习实现的通用牛标识

    Universal Bovine Identification via Depth Data and Deep Metric Learning

    [https://arxiv.org/abs/2404.00172](https://arxiv.org/abs/2404.00172)

    该论文提出了一种利用深度数据和深度度量学习进行通用牛标识的方法，可以在不需要物种特定的外套图案或特写口吻印记的情况下，通过CNN和MLP基础实现学习到良好泛化嵌入空间，从体形区分个体。

    

    本文首次提出并评估了一种顶部（背部视图）深度数据深度学习系统，用于准确识别个体牛，并提供了相关代码、数据集和训练权重，以便立即复现。畜群规模增加导致牛与人的比例失衡，使得对个体进行手动监测变得更具挑战性。因此，实时牛标识对农场至关重要，是精准畜牧业的关键一步。

    arXiv:2404.00172v1 Announce Type: cross  Abstract: This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data from an off-the-shelf 3D camera. The method relies on CNN and MLP backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals -- requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simp
    
[^164]: 个人文本语料库预测开放性、兴趣、知识和教育水平

    Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education

    [https://arxiv.org/abs/2404.00165](https://arxiv.org/abs/2404.00165)

    本研究探讨了个人对体验的开放性这一人格维度是否可以通过个体的谷歌搜索历史预测，并通过相似性特征基于个人文本语料库来解释35%的开放性方差。

    

    在这项研究中，我们调查了个体对体验的开放性这一人格维度是否可以通过个体的谷歌搜索历史进行预测。通过网络抓取，我们生成了来自214名参与者的个人文本语料库，平均词汇量为500万个词元。我们训练了word2vec模型，并使用每个个人文本语料库与标记单词的相似性，这些标记单词来自于人格的词汇方法。这些个人文本语料库-标记单词的相似性被用作神经模型中的预测特征。为了训练和验证，我们依赖179名参与者，并保留了35名参与者的测试样本。我们进行了一个带有不同数量预测特征、隐藏单元和增量因子的网格搜索。作为模型选择标准，我们使用了在验证样本中由绝对R2差异惩罚的R2。选择的神经模型在测试样本中解释了35%的开放性方差，而集成模型…

    arXiv:2404.00165v1 Announce Type: new  Abstract: Here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history. By web scraping, individual text corpora (ICs) were generated from 214 participants with a mean number of 5 million word tokens. We trained word2vec models and used the similarities of each IC to label words, which were derived from a lexical approach of personality. These IC-label-word similarities were utilized as predictive features in neural models. For training and validation, we relied on 179 participants and held out a test sample of 35 participants. A grid search with varying number of predictive features, hidden units and boost factor was performed. As model selection criterion, we used R2 in the validation samples penalized by the absolute R2 difference between training and validation. The selected neural model explained 35% of the openness variance in the test sample, while an ensemble model w
    
[^165]: 建模大规模步行和骑行网络：使用手机和众包数据的机器学习方法

    Modeling Large-Scale Walking and Cycling Networks: A Machine Learning Approach Using Mobile Phone and Crowdsourced Data

    [https://arxiv.org/abs/2404.00162](https://arxiv.org/abs/2404.00162)

    该研究使用机器学习方法结合手机和众包数据，建立了一个模型来估算大规模区域网络中的步行和骑行量，讨论了在模型训练、测试和推断中面临的挑战和限制。

    

    步行和骑行被认为可以带来显著的健康、环境和经济优势。然而，基于证据的积极交通规划和政策的发展受到数据限制的阻碍，例如众包数据的偏见和手机数据的代表性问题。在这项研究中，我们开发并应用了基于机器学习的建模方法，用于估算澳大利亚新南威尔士州一个包含188,999个步行链接和114,885个骑行链接的大规模地区网络的日常步行和骑行量。建模方法利用了众包和手机数据以及人口、土地利用、地形、气候等一系列其他数据集。该研究讨论了与模型训练、测试和推断的三个方面相关的独特挑战和限制，考虑到建模网络的大地理范围和相对稀缺的情况。

    arXiv:2404.00162v1 Announce Type: new  Abstract: Walking and cycling are known to bring substantial health, environmental, and economic advantages. However, the development of evidence-based active transportation planning and policies has been impeded by significant data limitations, such as biases in crowdsourced data and representativeness issues of mobile phone data. In this study, we develop and apply a machine learning based modeling approach for estimating daily walking and cycling volumes across a large-scale regional network in New South Wales, Australia that includes 188,999 walking links and 114,885 cycling links. The modeling methodology leverages crowdsourced and mobile phone data as well as a range of other datasets on population, land use, topography, climate, etc. The study discusses the unique challenges and limitations related to all three aspects of model training, testing, and inference given the large geographical extent of the modeled networks and relative scarcity
    
[^166]: 完全零阶双层规划通过高斯平滑

    Fully Zeroth-Order Bilevel Programming via Gaussian Smoothing

    [https://arxiv.org/abs/2404.00158](https://arxiv.org/abs/2404.00158)

    通过高斯平滑估计函数的一阶和二阶偏导数，用于解决双层优化问题的全零阶随机逼近算法，并建立了其非渐近收敛分析。

    

    在本文中，我们研究并分析了零阶随机逼近算法，用于解决双层问题，即在上/下目标值或其无偏梯度估计均不可用的情况下。特别地，利用Stein's identity，我们首先使用高斯平滑来估计具有两个独立变量块的函数的一阶和二阶偏导数。然后将这些估计值应用于随机逼近算法框架，以解决双层优化问题，并建立其非渐近收敛分析。据我们所知，这是首次为完全随机零阶双层优化算法建立了样本复杂度界限。

    arXiv:2404.00158v1 Announce Type: cross  Abstract: In this paper, we study and analyze zeroth-order stochastic approximation algorithms for solving bilvel problems, when neither the upper/lower objective values, nor their unbiased gradient estimates are available. In particular, exploiting Stein's identity, we first use Gaussian smoothing to estimate first- and second-order partial derivatives of functions with two independent block of variables. We then used these estimates in the framework of a stochastic approximation algorithm for solving bilevel optimization problems and establish its non-asymptotic convergence analysis. To the best of our knowledge, this is the first time that sample complexity bounds are established for a fully stochastic zeroth-order bilevel optimization algorithm.
    
[^167]: 在正-无监督学习中验证完全随机选择假设

    Verifying the Selected Completely at Random Assumption in Positive-Unlabeled Learning

    [https://arxiv.org/abs/2404.00145](https://arxiv.org/abs/2404.00145)

    在正-无监督学习中，研究了验证完全随机选择假设（SCAR）和更为现实的随机选择假设（SAR）对算法复杂性和速度的影响。

    

    正-无监督学习的目标是在包含正例和未标记实例的训练数据基础上训练二元分类器，其中未标记观测可以属于正类或负类。建模正-无监督数据需要关于标签机制的一些假设，描述哪些正例被分配标签。早期研究中考虑的最简单假设是SCAR（完全随机选择假设），其概率分数函数定义为给正例分配标签的概率是常数。另一方面，一个更为现实的假设是SAR（随机选择），它表明概率函数仅依赖于观察到的特征向量。基于SCAR的算法比基于SAR的算法简单得多，并且在计算上更快，后者通常需要挑战性的估计。

    arXiv:2404.00145v1 Announce Type: cross  Abstract: The goal of positive-unlabeled (PU) learning is to train a binary classifier on the basis of training data containing positive and unlabeled instances, where unlabeled observations can belong either to the positive class or to the negative class. Modeling PU data requires certain assumptions on the labeling mechanism that describes which positive observations are assigned a label. The simplest assumption, considered in early works, is SCAR (Selected Completely at Random Assumption), according to which the propensity score function, defined as the probability of assigning a label to a positive observation, is constant. On the other hand, a much more realistic assumption is SAR (Selected at Random), which states that the propensity function solely depends on the observed feature vector. SCAR-based algorithms are much simpler and computationally much faster compared to SAR-based algorithms, which usually require challenging estimation of 
    
[^168]: 信实性与可信度是否存在冲突？一项关于自然语言处理任务中可解释人工智能的实证研究

    Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks

    [https://arxiv.org/abs/2404.00140](https://arxiv.org/abs/2404.00140)

    传统的扰动方法Shapley值和LIME可实现更高的信实性和可信度，建议优化可解释性算法以实现高效的双重目标

    

    旨在解释决策型人工智能系统的可解释性算法通常要平衡两个关键方面：1）\textit{信实性}，即解释必须准确反映模型的推理过程；2）\textit{可信度}，即解释必须与领域专家保持一致。然而，一个问题出现了：信实性和可信度是否从根本上存在冲突？通过在情感分析、意图检测和主题标注三个NLP任务中选定的解释方法与专家级解释之间的全面量化比较，我们证明传统的扰动方法Shapley值和LIME可以实现更高的信实性和可信度。我们的发现表明，我们可以寻求通过双重目标优化可解释性算法来实现高

    arXiv:2404.00140v1 Announce Type: new  Abstract: Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \textit{faithfulness}, where explanations accurately reflect the model's inference process. 2) \textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high l
    
[^169]: 预算感知查询优化：自动机器学习视角

    Budget-aware Query Tuning: An AutoML Perspective

    [https://arxiv.org/abs/2404.00137](https://arxiv.org/abs/2404.00137)

    将传统成本单位视为变量，通过调整值可以获得比默认查询计划更优的查询计划

    

    现代数据库系统依赖基于成本的查询优化器为输入查询提供良好的执行计划。这种查询优化器依赖成本模型来估算候选查询执行计划的成本。本文挑战了传统观念，将这些成本单位视为变量，表明通过调整成本单位值，可以获得明显优于将成本单位视为常数时查询优化器返回的默认查询计划的查询计划。

    arXiv:2404.00137v1 Announce Type: cross  Abstract: Modern database systems rely on cost-based query optimizers to come up with good execution plans for input queries. Such query optimizers rely on cost models to estimate the costs of candidate query execution plans. A cost model represents a function from a set of cost units to query execution cost, where each cost unit specifies the unit cost of executing a certain type of query processing operation (such as table scan or join). These cost units are traditionally viewed as constants, whose values only depend on the platform configuration where the database system runs on top of but are invariant for queries processed by the database system. In this paper, we challenge this classic view by thinking of these cost units as variables instead. We show that, by varying the cost-unit values one can obtain query plans that significantly outperform the default query plans returned by the query optimizer when viewing the cost units as constants
    
[^170]: FISBe: 一种用于长程细丝狀结构对象分割的真实世界基准数据集

    FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures

    [https://arxiv.org/abs/2404.00130](https://arxiv.org/abs/2404.00130)

    FISBe提供了一个真实世界基准数据集，用于长程细丝状结构的对象分割，有助于解决神经科学中对神经元进行实例分割时面临的挑战。

    

    在神经系统体积光学显微成像中对神经元进行实例分割，可以通过促进神经回路的联合功能和形态分析，为神经科学的开创性研究提供支持。然而，这种多神经元光学显微数据具有极具挑战性的特点：单个神经元具有长程、细丝狀和广泛分支的形态，多个神经元紧密交织，部分体积效应、不均匀照明和光学显微固有的噪音严重阻碍了个体神经元的局部分离和长程追踪。这些属性反映了机器学习研究中的一个当前关键挑战，即如何有效地捕捉数据中的长程依赖关系。尽管相关方法研究十分活跃，但迄今为止，方法主要在合成数据集上进行基准测试。为了解决这一挑战

    arXiv:2404.00130v1 Announce Type: cross  Abstract: Instance segmentation of neurons in volumetric light microscopy images of nervous systems enables groundbreaking research in neuroscience by facilitating joint functional and morphological analyses of neural circuits at cellular resolution. Yet said multi-neuron light microscopy data exhibits extremely challenging properties for the task of instance segmentation: Individual neurons have long-ranging, thin filamentous and widely branching morphologies, multiple neurons are tightly inter-weaved, and partial volume effects, uneven illumination and noise inherent to light microscopy severely impede local disentangling as well as long-range tracing of individual neurons. These properties reflect a current key challenge in machine learning research, namely to effectively capture long-range dependencies in the data. While respective methodological research is buzzing, to date methods are typically benchmarked on synthetic datasets. To address
    
[^171]: PikeLPN: 缓解低精度神经网络的被忽视的低效问题

    PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks

    [https://arxiv.org/abs/2404.00103](https://arxiv.org/abs/2404.00103)

    本文提出了一个ACEv2的扩展版本，与量化模型的推理成本和在ML硬件上的能耗更匹配，同时引入了PikeLPN模型，通过将量化应用于逐元素操作和乘累积操作，解决了低精度神经网络中被忽视的低效问题。

    

    低精度量化以其优化神经网络的功效而闻名。我们的分析揭示了非量化的逐元素操作，在诸如参数化激活函数、批量归一化和量化缩放等层中普遍存在，并且主导了低精度模型的推理成本。这些非量化的逐元素操作通常被忽视于基于算术计算工作量（ACE）等最先进的效率度量中。在本文中，我们提出了ACEv2 - 一个ACE的扩展版本，能更好地与量化模型的推理成本以及它们在ML硬件上的能耗相匹配。此外，我们介绍了PikeLPN，一个通过将量化应用于逐元素操作和乘累积操作来解决这些效率问题的模型。特别地，我们提出了一种新颖的针对批量归一化层的量化技术，名为QuantNorm，可以实现对批量归一化操作的量化。

    arXiv:2404.00103v1 Announce Type: new  Abstract: Low-precision quantization is recognized for its efficacy in neural network optimization. Our analysis reveals that non-quantized elementwise operations which are prevalent in layers such as parameterized activation functions, batch normalization, and quantization scaling dominate the inference cost of low-precision models. These non-quantized elementwise operations are commonly overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort (ACE). In this paper, we propose ACEv2 - an extended version of ACE which offers a better alignment with the inference cost of quantized models and their energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that addresses these efficiency issues by applying quantization to both elementwise operations and multiply-accumulate operations. In particular, we present a novel quantization technique for batch normalization layers named QuantNorm which allows for quantizing t
    
[^172]: 贝叶斯非参数方法：深度学习的替代选择

    Bayesian Nonparametrics: An Alternative to Deep Learning

    [https://arxiv.org/abs/2404.00085](https://arxiv.org/abs/2404.00085)

    贝叶斯非参数模型为统计模型选择提供了灵活而强大的框架，揭示了贝叶斯非参数方法的多才多艺和高效性，为在各个学科领域中应对复杂挑战提供了创新解决方案。

    

    贝叶斯非参数模型为统计模型选择提供了灵活而强大的框架，能够使模型复杂度适应各种数据集的复杂性。本调查旨在深入探讨贝叶斯非参数方法的重要性，特别是在解决统计学、计算机科学和电气工程等各个领域的复杂挑战方面。通过阐明这些非参数模型的基本特性和理论基础，本调查旨在提供对贝叶斯非参数方法及其在解决复杂问题方面的相关性的全面理解，特别是在多目标跟踪领域。通过这种探索，我们揭示了贝叶斯非参数方法的多才多艺和高效性，为在各个学科领域中应对复杂挑战提供了创新解决方案。

    arXiv:2404.00085v1 Announce Type: new  Abstract: Bayesian nonparametric models offer a flexible and powerful framework for statistical model selection, enabling the adaptation of model complexity to the intricacies of diverse datasets. This survey intends to delve into the significance of Bayesian nonparametrics, particularly in addressing complex challenges across various domains such as statistics, computer science, and electrical engineering. By elucidating the basic properties and theoretical foundations of these nonparametric models, this survey aims to provide a comprehensive understanding of Bayesian nonparametrics and their relevance in addressing complex problems, particularly in the domain of multi-object tracking. Through this exploration, we uncover the versatility and efficacy of Bayesian nonparametric methodologies, paving the way for innovative solutions to intricate challenges across diverse disciplines.
    
[^173]: 基于可微分反馈延迟网络和可学习延迟线的数据驱动室内声学建模

    Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines

    [https://arxiv.org/abs/2404.00082](https://arxiv.org/abs/2404.00082)

    通过可学习延迟线实现可微分反馈延迟网络的参数优化，实现了对室内声学特性的数据驱动建模。

    

    在过去的几十年中，人们致力于设计人工混响算法，旨在模拟物理环境的室内声学。尽管取得了显著进展，但延迟网络模型的自动参数调整仍然是一个开放性挑战。我们提出了一种新方法，通过学习可微分反馈延迟网络（FDN）的参数，使其输出呈现出所测得的室内脉冲响应的感知特性。

    arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
    
[^174]: 具有多属性优化的分子生成对抗网络

    Molecular Generative Adversarial Network with Multi-Property Optimization

    [https://arxiv.org/abs/2404.00081](https://arxiv.org/abs/2404.00081)

    该研究引入了一种新型的基于演员-评论家强化学习的GAN，即InstGAN，以在令牌级别上生成具有多属性优化的分子，并利用最大化信息熵来缓解模式崩溃。

    

    深度生成模型，如生成对抗网络（GANs），已被应用于药物发现中$de~novo$分子生成。大多数先前的研究使用强化学习（RL）算法，特别是蒙特卡罗树搜索（MCTS），来处理GANs中分子表示的离散特性。然而，由于GANs和RL模型的固有训练不稳定性，以及与MCTS采样相关的高计算成本，MCTS RL-based GANs难以扩展到大型化学数据库。为了解决这些挑战，本研究提出了一种基于带即时和全局奖励的演员-评论家RL的新型GAN，称为InstGAN，以在令牌级别上生成具有多属性优化的分子。此外，最大化信息熵被利用来缓解模式崩溃。实验结果表明，InstGAN优于其他基线，达到了可比较的性能。

    arXiv:2404.00081v1 Announce Type: cross  Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance
    
[^175]: 使用倒置标签的后门方法：脏标签翻转攻击

    A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks

    [https://arxiv.org/abs/2404.00076](https://arxiv.org/abs/2404.00076)

    提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。

    

    基于声音的机器学习系统经常使用公共或第三方数据，这可能是不准确的。这使得训练在这些数据上的深度神经网络（DNN）模型容易受到潜在的数据毒化攻击。在这种攻击类型中，攻击者可以使用毒化数据来训练DNN模型，可能会降低其性能。另一种对我们的研究非常相关的数据毒化攻击类型是标签翻转，攻击者在其中操纵数据子集的标签。已经证明，即使是能力有限的攻击者，这些攻击也可能极大地降低系统性能。在本研究中，我们提出了一种名为“DirtyFlipping”的后门攻击，使用脏标签技术，“标签对标签”，在与目标类别相关的选定数据模式中输入触发器（拍手），从而实现了隐蔽的后门。

    arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
    
[^176]: BEACON: 具有条件正态流的贝叶斯实验设计加速 - 二氧化碳封存中最佳监测井布置的案例研究

    BEACON: Bayesian Experimental design Acceleration with Conditional Normalizing flows $-$ a case study in optimal monitor well placement for CO$_2$ sequestration

    [https://arxiv.org/abs/2404.00075](https://arxiv.org/abs/2404.00075)

    该论文提出了一种在CO$_2$封存项目中采用贝叶斯框架下的优化实验设计方法，通过整合流体流动求解器和生成式神经网络，以确保在有限预算内布置最佳的监测井位。

    

    CO$_2$封存是缓解气候变化的关键工程解决方案。然而，储层属性的不确定性，需要对CO$_2$地下流的严格监测，以防止风险，如泄漏、诱发地震或突破许可边界。为解决这个问题，项目经理使用钻孔井在特定位置直接监测CO$_2$和压力。鉴于钻井的高成本，关键是战略地布置有限数量的井，以确保在预算限制内进行最有效的监测。我们选择井位的方法集成了用于预测地下流轨迹的流体流动求解器和用于推断地下流不确定性的生成式神经网络。我们的方法在三维领域中是可扩展的，并在贝叶斯框架内开发，以实现最优实验设计，保证可扩展性和数学最优性。

    arXiv:2404.00075v1 Announce Type: new  Abstract: CO$_2$ sequestration is a crucial engineering solution for mitigating climate change. However, the uncertain nature of reservoir properties, necessitates rigorous monitoring of CO$_2$ plumes to prevent risks such as leakage, induced seismicity, or breaching licensed boundaries. To address this, project managers use borehole wells for direct CO$_2$ and pressure monitoring at specific locations. Given the high costs associated with drilling, it is crucial to strategically place a limited number of wells to ensure maximally effective monitoring within budgetary constraints. Our approach for selecting well locations integrates fluid-flow solvers for forecasting plume trajectories with generative neural networks for plume inference uncertainty. Our methodology is extensible to three-dimensional domains and is developed within a Bayesian framework for optimal experimental design, ensuring scalability and mathematical optimality. We use a reali
    
[^177]: 一种用于将微结构的弹性属性映射到其机械变形的有限算子学习技术

    A finite operator learning technique for mapping the elastic properties of microstructures to their mechanical deformations

    [https://arxiv.org/abs/2404.00074](https://arxiv.org/abs/2404.00074)

    引入一种有限算子学习技术，通过学习参数化解决了微结构弹性属性映射到机械变形的问题，在计算成本和准确性方面优于传统方法，并能处理具有明显不连续性的解。

    

    为了开发固体力学中控制物理方程的更快求解器，我们引入了一种参数化学习机械平衡解的方法。该方法在计算成本方面优于传统方法，同时保持准确性。此外，它将标准的物理信息神经网络进行了泛化和增强，以学习一个带有相当明显不连续性的参数解。我们以微观力学为例，其中对于给定的异质微结构来说，微观力学解（即变形和应力场）的知识至关重要。我们针对的参数是异质固体系统内的杨氏模量分布。受算子学习和有限元方法的启发，我们的方法展示了无需依赖其他数值求解器数据就能进行训练的能力。相反，我们利用了有限元方法的思想。

    arXiv:2404.00074v1 Announce Type: new  Abstract: To develop faster solvers for governing physical equations in solid mechanics, we introduce a method that parametrically learns the solution to mechanical equilibrium. The introduced method outperforms traditional ones in terms of computational cost while acceptably maintaining accuracy. Moreover, it generalizes and enhances the standard physics-informed neural networks to learn a parametric solution with rather sharp discontinuities. We focus on micromechanics as an example, where the knowledge of the micro-mechanical solution, i.e., deformation and stress fields for a given heterogeneous microstructure, is crucial. The parameter under investigation is the Young modulus distribution within the heterogeneous solid system. Our method, inspired by operator learning and the finite element method, demonstrates the ability to train without relying on data from other numerical solvers. Instead, we leverage ideas from the finite element approac
    
[^178]: 一个用于快速模型选择的两阶段召回和选择框架

    A Two-Phase Recall-and-Select Framework for Fast Model Selection

    [https://arxiv.org/abs/2404.00069](https://arxiv.org/abs/2404.00069)

    通过两阶段（粗略召回和精细选择）模型选择框架，本文旨在提高选择稳健模型的效率，通过利用模型在基准数据集上的训练性能，聚类展示相似训练性能的模型

    

    随着深度学习在各种机器学习应用中的普及，神经网络模型的大量训练和在公共模型存储库中共享已经是司空见惯。在针对性机器学习任务中，利用适当的源模型作为起点通常优于从头开始训练的策略，特别是在训练数据有限的情况下。尽管先前工作中对许多模型选择策略进行了调查和开发，但该过程仍然耗时，特别是考虑到模型存储库规模不断增加的情况。本文提出了一个两阶段（粗略召回和精细选择）模型选择框架，旨在通过利用模型在基准数据集上的训练性能来提高选择稳健模型的效率。具体来说，粗略召回阶段将展示在基准数据集上训练性能相似的模型进行聚类

    arXiv:2404.00069v1 Announce Type: new  Abstract: As the ubiquity of deep learning in various machine learning applications has amplified, a proliferation of neural network models has been trained and shared on public model repositories. In the context of a targeted machine learning assignment, utilizing an apt source model as a starting point typically outperforms the strategy of training from scratch, particularly with limited training data. Despite the investigation and development of numerous model selection strategies in prior work, the process remains time-consuming, especially given the ever-increasing scale of model repositories. In this paper, we propose a two-phase (coarse-recall and fine-selection) model selection framework, aiming to enhance the efficiency of selecting a robust model by leveraging the models' training performances on benchmark datasets. Specifically, the coarse-recall phase clusters models showcasing similar training performances on benchmark datasets in an 
    
[^179]: 基于数据驱动的关键风险因素对网络安全威胁进行预测分析

    A Data-Driven Predictive Analysis on Cyber Security Threats with Key Risk Factors

    [https://arxiv.org/abs/2404.00068](https://arxiv.org/abs/2404.00068)

    该研究提出了一种基于机器学习的模型，使用社会经济因素来预测可能成为网络攻击受害者的个人，并展示了对该模型的数据收集和处理方法。

    

    网络风险指的是破坏声誉、货币损失或组织或个人的干扰风险，这种情况通常是由于对网络系统的无意识使用而导致的。网络风险每天都在逐渐增加，目前已成为全球性威胁。像孟加拉国这样的发展中国家面临着重大的网络风险挑战。全球范围内不断增长的网络威胁强调了对有效建模以预测和管理相关风险的需求。本文展示了一种基于机器学习（ML）的模型，通过分析社会经济因素来预测可能成为网络攻击受害者的个人。我们从网络攻击受害者和非受害者中收集了基于社会人口特征的数据集。该研究涉及开发问卷以收集数据，然后用于衡量特征的重要性。通过数据增强，数据集扩展到3286条记录。

    arXiv:2404.00068v1 Announce Type: cross  Abstract: Cyber risk refers to the risk of defacing reputation, monetary losses, or disruption of an organization or individuals, and this situation usually occurs by the unconscious use of cyber systems. The cyber risk is unhurriedly increasing day by day and it is right now a global threat. Developing countries like Bangladesh face major cyber risk challenges. The growing cyber threat worldwide focuses on the need for effective modeling to predict and manage the associated risk. This paper exhibits a Machine Learning(ML) based model for predicting individuals who may be victims of cyber attacks by analyzing socioeconomic factors. We collected the dataset from victims and non-victims of cyberattacks based on socio-demographic features. The study involved the development of a questionnaire to gather data, which was then used to measure the significance of features. Through data augmentation, the dataset was expanded to encompass 3286 entries, se
    
[^180]: 金融网络中的时间图网络用于异常检测

    Temporal Graph Networks for Graph Anomaly Detection in Financial Networks

    [https://arxiv.org/abs/2404.00060](https://arxiv.org/abs/2404.00060)

    时间图网络（TGN）在金融网络中的异常检测中表现出显著优势，适应了现代金融系统动态和复杂的特性。

    

    本文探讨了利用时间图网络（TGN）进行金融异常检测，在金融科技和数字化金融交易时代这是一个迫切需要。我们提出了一个全面的框架，利用TGN捕捉金融网络中边的动态变化，用于欺诈检测。我们比较了TGN与静态图神经网络（GNN）基线以及使用DGraph数据集进行现实金融场景下的前沿超图神经网络基线的性能。结果表明，TGN在AUC指标方面显著优于其他模型。这种优越性能突显了TGN作为检测金融欺诈的有效工具的潜力，展示了其适应现代金融系统动态和复杂性的能力。我们还在TGN框架内尝试了各种图嵌入模块，并比较了它们的有效性。

    arXiv:2404.00060v1 Announce Type: cross  Abstract: This paper explores the utilization of Temporal Graph Networks (TGN) for financial anomaly detection, a pressing need in the era of fintech and digitized financial transactions. We present a comprehensive framework that leverages TGN, capable of capturing dynamic changes in edges within financial networks, for fraud detection. Our study compares TGN's performance against static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph neural network baselines using DGraph dataset for a realistic financial context. Our results demonstrate that TGN significantly outperforms other models in terms of AUC metrics. This superior performance underlines TGN's potential as an effective tool for detecting financial fraud, showcasing its ability to adapt to the dynamic and complex nature of modern financial systems. We also experimented with various graph embedding modules within the TGN framework and compared the effectiveness of 
    
[^181]: 通过Transformer编码的HTTP响应头指纹识别Web服务器

    Fingerprinting web servers through Transformer-encoded HTTP response headers

    [https://arxiv.org/abs/2404.00056](https://arxiv.org/abs/2404.00056)

    运用深度学习和自然语言处理技术，通过对HTTP响应头的编码识别Web服务器，可以在提高准确性和特异性的基础上实现对易受攻击的Web服务器版本的检测。

    

    我们探讨了利用最先进的深度学习、大数据和自然语言处理技术来增强对易受攻击的Web服务器版本的检测能力。通过向477万个域发送各种模糊和非标准的HTTP请求，并捕获HTTP响应状态行来进行实验。我们通过训练BPE分词器和RoBERTa编码器进行无监督遮蔽语言建模来表示这些状态行。然后对编码的响应行进行降维并连接以表示每个域的Web服务器。一个随机森林和多层感知器（MLP）对这些Web服务器进行分类，在检测到五种最流行的初始Web服务器时分别达到0.94和0.96的宏F1分数。MLP在分类347个主要类型和次要版本对时实现了0.55的加权F1分数。分析表明我们的测试用例...

    arXiv:2404.00056v1 Announce Type: cross  Abstract: We explored leveraging state-of-the-art deep learning, big data, and natural language processing to enhance the detection of vulnerable web server versions. Focusing on improving accuracy and specificity over rule-based systems, we conducted experiments by sending various ambiguous and non-standard HTTP requests to 4.77 million domains and capturing HTTP response status lines. We represented these status lines through training a BPE tokenizer and RoBERTa encoder for unsupervised masked language modeling. We then dimensionality reduced and concatenated encoded response lines to represent each domain's web server. A Random Forest and multilayer perceptron (MLP) classified these web servers, and achieved 0.94 and 0.96 macro F1-score, respectively, on detecting the five most popular origin web servers. The MLP achieved a weighted F1-score of 0.55 on classifying 347 major type and minor version pairs. Analysis indicates that our test cases 
    
[^182]: 在数字画布上编舞：一种艺术表现的机器学习方法

    Choreographing the Digital Canvas: A Machine Learning Approach to Artistic Performance

    [https://arxiv.org/abs/2404.00054](https://arxiv.org/abs/2404.00054)

    这项研究提出了一种基于属性描述的设计工具，结合机器学习模型，用于生成和可视化艺术运动，其中关键创新在于循环属性条件变分自动编码器模型能够捕捉和生成逼真的3D人体动作，分别学习倒下动作的不同阶段。

    

    本文介绍了一种基于属性描述的艺术表演设计工具的概念。为实现这一目的，我们使用了一个特定的倒下动作表演。该平台集成了一个新颖的机器学习（ML）模型和交互式界面，以生成和可视化艺术运动。我们方法的核心是一个循环属性条件变分自动编码器（AC-VAE）模型，旨在解决从动作捕捉（MoCap）数据中捕捉和生成逼真的3D人体动作的挑战。我们创建了一个专注于倒下动作动力学的独特数据集，其特点是将运动分为三个不同阶段：冲击、故障和下降。该ML模型的创新在于其能够分别学习这些阶段。通过应用全面的数据增强技术和初始姿势损失函数，来生成自然和合理的运动。

    arXiv:2404.00054v1 Announce Type: cross  Abstract: This paper introduces the concept of a design tool for artistic performances based on attribute descriptions. To do so, we used a specific performance of falling actions. The platform integrates a novel machine-learning (ML) model with an interactive interface to generate and visualize artistic movements. Our approach's core is a cyclic Attribute-Conditioned Variational Autoencoder (AC-VAE) model developed to address the challenge of capturing and generating realistic 3D human body motions from motion capture (MoCap) data. We created a unique dataset focused on the dynamics of falling movements, characterized by a new ontology that divides motion into three distinct phases: Impact, Glitch, and Fall. The ML model's innovation lies in its ability to learn these phases separately. It is achieved by applying comprehensive data augmentation techniques and an initial pose loss function to generate natural and plausible motion. Our web-based 
    
[^183]: Deja vu: 使用前缀调整进行对比历史建模，用于时间知识图推理

    Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning

    [https://arxiv.org/abs/2404.00051](https://arxiv.org/abs/2404.00051)

    提出了ChapTER，一个对比历史建模框架，通过前缀调整实现文本与时间的平衡，用于时间知识图推理。

    

    时间知识图推理（TKGR）是在复杂场景中为不完整的TKG推断缺失事实的任务（例如，传导和归纳设置），越来越受到关注。最近，为了减少TKG中结构连接的依赖性，已开发了基于文本的方法，利用实体描述中丰富的语言信息。然而，由于预训练语言模型的巨大参数和不灵活性，现有的基于文本的方法在计算昂贵且目的建立的训练策略上很难平衡文本知识和时间信息。为了发掘文本模型在各种复杂场景中用于TKGR的潜力，我们提出了ChapTER，一个具有前缀调整对比历史建模框架，用于时间推理。

    arXiv:2404.00051v1 Announce Type: new  Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via cont
    
[^184]: Grappa--一个机器学习的分子力学势场

    Grappa -- A Machine Learned Molecular Mechanics Force Field

    [https://arxiv.org/abs/2404.00050](https://arxiv.org/abs/2404.00050)

    提出了一个用于预测分子力学参数的机器学习架构Grappa，通过图注意力神经网络和transformer提高了准确性，减少了计算开销，能在现有MD引擎中使用。

    

    模拟长时间尺度上的大分子系统需要既准确又高效的力场。近年来，E(3)等变神经网络解决了计算效率和力场准确性之间的张力，但其代价仍比传统的分子力学（MM）力场昂贵数个数量级。在这项研究中，我们提出了一种新颖的机器学习架构，利用图注意力神经网络和具有保持对称性的位置编码的transformer来预测分子图中的MM参数。由此产生的力场Grappa在准确性方面优于已建立的其他机器学习的MM力场，并且能以相同的计算效率在现有的分子动力学（MD）引擎如GROMACS和OpenMM中使用。它可以预测小分子、肽、RNA的能量和力--展示了其可扩展性到未知区域。

    arXiv:2404.00050v1 Announce Type: cross  Abstract: Simulating large molecular systems over long timescales requires force fields that are both accurate and efficient. In recent years, E(3) equivariant neural networks have lifted the tension between computational efficiency and accuracy of force fields, but they are still several orders of magnitude more expensive than classical molecular mechanics (MM) force fields.   Here, we propose a novel machine learning architecture to predict MM parameters from the molecular graph, employing a graph attentional neural network and a transformer with symmetry-preserving positional encoding. The resulting force field, Grappa, outperforms established and other machine-learned MM force fields in terms of accuracy at the same computational efficiency and can be used in existing Molecular Dynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces of small molecules, peptides, RNA and - showcasing its extensibility to uncharted regio
    
[^185]: SLIMBRAIN：增强现实实时获取与处理系统，用于带有深度信息的高光谱分类映射在体内手术程序中

    SLIMBRAIN: Augmented Reality Real-Time Acquisition and Processing System For Hyperspectral Classification Mapping with Depth Information for In-Vivo Surgical Procedures

    [https://arxiv.org/abs/2404.00048](https://arxiv.org/abs/2404.00048)

    SLIMBRAIN是一种实时获取和处理AR系统，用于在体内手术过程中从高光谱信息中分类和显示脑肿瘤组织，并结合RGB点云进行AR可视化。

    

    在过去的二十年中，增强现实（AR）导致了各种社会和技术应用领域的新界面的快速发展。其中之一领域是医学，尤其是在手术方面，这些可视化技术有助于提高术前和术中程序的效果。本文介绍了SLIMBRAIN，这是一个实时获取和处理AR系统，适用于从高光谱（HS）信息中分类和显示脑肿瘤组织。该系统在肿瘤切除手术过程中以每秒14帧的速度捕获并处理HS图像，以便在神经外科医生进行手术时同时检测和界定癌组织。结果呈现在AR可视化中，分类结果与LiDAR相机捕获的RGB点云重叠。这种表示允许对场景进行自然导航。

    arXiv:2404.00048v1 Announce Type: cross  Abstract: Over the last two decades, augmented reality (AR) has led to the rapid development of new interfaces in various fields of social and technological application domains. One such domain is medicine, and to a higher extent surgery, where these visualization techniques help to improve the effectiveness of preoperative and intraoperative procedures. Following this trend, this paper presents SLIMBRAIN, a real-time acquisition and processing AR system suitable to classify and display brain tumor tissue from hyperspectral (HS) information. This system captures and processes HS images at 14 frames per second (FPS) during the course of a tumor resection operation to detect and delimit cancer tissue at the same time the neurosurgeon operates. The result is represented in an AR visualization where the classification results are overlapped with the RGB point cloud captured by a LiDAR camera. This representation allows natural navigation of the scen
    
[^186]: 政策优化在正则化广义和总 LQ 游戏中找到纳什均衡

    Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games

    [https://arxiv.org/abs/2404.00045](https://arxiv.org/abs/2404.00045)

    引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡产生影响，证明了NE符合线性高斯策略，并提出了政策优化算法以及增强技术来找到游戏内的NE。

    

    在本文中，我们研究了引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡 (NE) 的影响，揭示了这类游戏的NE符合线性高斯策略的事实。此外，它描绘了在熵正则化的适当性方面，对游戏内NE独特性的充分条件。由于政策优化是强化学习 (RL) 技术的基础方法，旨在找到 NE，在这项工作中，我们证明了一个政策优化算法的线性收敛性，该算法 (在熵正则化的适当性下) 能够明显地实现 NE。此外，在熵正则化证明不足的情况下，我们提出了一个 $\delta$-增强技术，有助于实现游戏内的 $\epsilon$-NE。

    arXiv:2404.00045v1 Announce Type: cross  Abstract: In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.
    
[^187]: UAlign: 无模板化的非监督式SMILES对齐推动无模板化逆合成预测的极限

    UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment

    [https://arxiv.org/abs/2404.00044](https://arxiv.org/abs/2404.00044)

    本文提出了UAlign，一种无模板化的图到序列的逆合成预测方法，通过结合图神经网络和Transformer，利用分子的固有图结构，并引入一种简单有效的SMILES对齐技术来促进未改变结构的复用。

    

    逆合成规划在有机化工行业中，特别是在制药领域，面临着巨大挑战。单步逆合成预测是规划过程中至关重要的一步，近年来由于科学人工智能的进步，这一步骤引起了人们的浓厚兴趣。近年来已经提出了各种基于深度学习的方法来解决这一问题，其中包括不同程度的额外化学知识依赖。本文介绍了UAlign，这是一种基于图到序列的无模板化逆合成预测管线。通过结合图神经网络和Transformer，我们的方法能够更有效地利用分子的固有图结构。基于分子结构在化学反应过程中保持不变的事实，我们提出了一种简单而有效的SMILES对齐技术，以促进未改变结构的复用以生成反应物。大量实验...

    arXiv:2404.00044v1 Announce Type: cross  Abstract: Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments 
    
[^188]: 利用机器学习和计算机视觉提高低视力和盲人的可访问性

    Improve accessibility for Low Vision and Blind people using Machine Learning and Computer Vision

    [https://arxiv.org/abs/2404.00043](https://arxiv.org/abs/2404.00043)

    该项目利用机器学习和计算机视觉开发移动应用，帮助盲人通过音频和触觉反馈实时定位周围环境，具有扫描文本和朗读、检测物体等功能。

    

    随着全球移动技术的不断扩展，残障人士急需得到相应的帮助。该项目探讨了如何利用机器学习和计算机视觉来提高视觉障碍人士的可访问性。过去的尝试开发了许多软件，旨在改善盲人日常生活中的可访问性。然而，市场上的应用准确度较低，并且仅提供音频反馈。该项目将专注于建立一个移动应用，通过实时接收用户周围的音频和触觉反馈（如振动），来帮助盲人在空间中定位。移动应用将具有三个主要功能。首要功能是从相机扫描文本并朗读给用户听。这一功能可用于纸张上的文本、环境中以及道路标识上。

    arXiv:2404.00043v1 Announce Type: cross  Abstract: With the ever-growing expansion of mobile technology worldwide, there is an increasing need for accommodation for those who are disabled. This project explores how machine learning and computer vision could be utilized to improve accessibility for people with visual impairments. There have been many attempts to develop various software that would improve accessibility in the day-to-day lives of blind people. However, applications on the market have low accuracy and only provide audio feedback. This project will concentrate on building a mobile application that helps blind people to orient in space by receiving audio and haptic feedback, e.g. vibrations, about their surroundings in real-time. The mobile application will have 3 main features. The initial feature is scanning text from the camera and reading it to a user. This feature can be used on paper with text, in the environment, and on road signs. The second feature is detecting obj
    
[^189]: 具有约束的随机优化：非渐近实例相关分析

    Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis

    [https://arxiv.org/abs/2404.00042](https://arxiv.org/abs/2404.00042)

    该论文研究了具有凸约束的随机凸优化问题，提出了一种非渐近保证的VRPG算法，并展示了其性能受到解以及带凸约束解决的问题的缩放距离控制。

    

    我们考虑了随机凸优化在凸约束下的问题。我们分析了一种适用于这个问题的自然方差减少的近端梯度（VRPG）算法的行为。我们的主要结果是VRPG算法的非渐近保证。与极小值最坏情况保证相反，我们的结果是基于实例的。这意味着我们的保证捕捉了损失函数的复杂性，噪声的变异性和约束集的几何性。我们表明，VRPG算法的非渐近性能受给定问题的解和给定凸约束下解决的特定小扰动问题的解之间的缩放距离（由$\sqrt{N}$缩放）的控制，这里，$N$表示样本数。利用局部极小值下界和扰动问题解之间的一种成熟联系，我们表明当$N \rightarrow +\infty$时，极小值存在并且受指定凸约束的约束。

    arXiv:2404.00042v1 Announce Type: cross  Abstract: We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \right
    
[^190]: MicroHD：面向TinyML系统的高维超计算算法的精度驱动优化

    MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing Algorithms for TinyML systems

    [https://arxiv.org/abs/2404.00039](https://arxiv.org/abs/2404.00039)

    提出了MicroHD，一个新颖的面向TinyML系统的精度驱动超高维计算优化方法

    

    超维计算（HDC）作为一种能够有效定位到TinyML应用的有前途的人工智能方法正在兴起，这要归功于其轻量级的计算和内存需求。在HDC的先前研究中，证明将标准的10k维超维空间限制在更低的值是可行的，进一步降低HDC的资源需求。类似地，其他研究表明二进制值可用作生成的超矢量的元素，可以显著提高效率，代价是某种程度的精度降低。然而，当前的优化尝试没有同时协同优化HDC的超参数，并且精度降低不能直接控制，导致HDC模型不够优化，提供了几个质量不可接受的输出。在这项工作中，我们提出MicroHD，一种新颖的精度驱动HDC优化方法，通过迭代调整HDC的超参数，

    arXiv:2404.00039v1 Announce Type: cross  Abstract: Hyperdimensional computing (HDC) is emerging as a promising AI approach that can effectively target TinyML applications thanks to its lightweight computing and memory requirements. Previous works on HDC showed that limiting the standard 10k dimensions of the hyperdimensional space to much lower values is possible, reducing even more HDC resource requirements. Similarly, other studies demonstrated that binary values can be used as elements of the generated hypervectors, leading to significant efficiency gains at the cost of some degree of accuracy degradation. Nevertheless, current optimization attempts do not concurrently co-optimize HDC hyper-parameters, and accuracy degradation is not directly controlled, resulting in sub-optimal HDC models providing several applications with unacceptable output qualities. In this work, we propose MicroHD, a novel accuracy-driven HDC optimization approach that iteratively tunes HDC hyper-parameters, 
    
[^191]: 探究去中心化金融（DeFi）服务之间的相似性

    Investigating Similarities Across Decentralized Financial (DeFi) Services

    [https://arxiv.org/abs/2404.00034](https://arxiv.org/abs/2404.00034)

    该研究通过采用图表示学习算法，提出了一种方法来研究去中心化金融协议提供的服务之间的相似性，并成功将这些服务分组为具有相似功能的集群。

    

    我们探讨了采用图表示学习（GRL）算法来研究去中心化金融（DeFi）协议提供的服务之间的相似性。我们使用以太坊交易数据来识别DeFi构建模块，这些是协议特定的智能合约集，它们在单个交易中以组合方式使用，并封装了执行特定金融服务（如加密资产交换或借贷）的逻辑。我们提出了一种基于智能合约属性和智能合约调用的图结构将这些模块分类进集群的方法。我们利用GRL从构建模块创建嵌入向量，并利用凝聚模型对其进行聚类。为了评估它们是否有效地分组为具有相似功能的集群，我们将它们与八个金融功能类别关联，并将此信息用作目标l

    arXiv:2404.00034v1 Announce Type: cross  Abstract: We explore the adoption of graph representation learning (GRL) algorithms to investigate similarities across services offered by Decentralized Finance (DeFi) protocols. Following existing literature, we use Ethereum transaction data to identify the DeFi building blocks. These are sets of protocol-specific smart contracts that are utilized in combination within single transactions and encapsulate the logic to conduct specific financial services such as swapping or lending cryptoassets. We propose a method to categorize these blocks into clusters based on their smart contract attributes and the graph structure of their smart contract calls. We employ GRL to create embedding vectors from building blocks and agglomerative models for clustering them. To evaluate whether they are effectively grouped in clusters of similar functionalities, we associate them with eight financial functionality categories and use this information as the target l
    
[^192]: 朝向无需凝视的c-VEP脑机接口：一项试验性研究

    Towards gaze-independent c-VEP BCI: A pilot study

    [https://arxiv.org/abs/2404.00031](https://arxiv.org/abs/2404.00031)

    这项试验性研究首次尝试朝向无需凝视的脑机接口拼写器，通过使用空间注意力而非眼球移动来解码视觉刺激，取得了很高的分类准确率。

    

    脑机接口（BCI）拼写器的一个局限性在于它们要求用户能够移动眼睛注视目标。对于无法自愿控制眼睛移动的用户（例如患有晚期肌萎缩侧索硬化症（ALS）的人群），这会造成问题。这项试验性研究首次迈出了朝向基于代码调制视觉诱发电位（c-VEP）的无需凝视的拼写器的第一步。参与者被呈现两个双侧位置的刺激，其中一个在闪烁，并被要求专注于这些刺激中的一个，可以通过直接看着刺激（明显条件）或使用空间注意力来完成，消除了眼球移动的需要（隐蔽条件）。被专注的刺激从脑电图（EEG）中解码，隐蔽和明显条件的分类准确率分别为88%和100%。这些基础性见解展示了这一技术的前景。

    arXiv:2404.00031v1 Announce Type: cross  Abstract: A limitation of brain-computer interface (BCI) spellers is that they require the user to be able to move the eyes to fixate on targets. This poses an issue for users who cannot voluntarily control their eye movements, for instance, people living with late-stage amyotrophic lateral sclerosis (ALS). This pilot study makes the first step towards a gaze-independent speller based on the code-modulated visual evoked potential (c-VEP). Participants were presented with two bi-laterally located stimuli, one of which was flashing, and were tasked to attend to one of these stimuli either by directly looking at the stimuli (overt condition) or by using spatial attention, eliminating the need for eye movement (covert condition). The attended stimuli were decoded from electroencephalography (EEG) and classification accuracies of 88% and 100% were obtained for the covert and overt conditions, respectively. These fundamental insights show the promisin
    
[^193]: 非结构化体育数据的可视化 - 以板球短文本评论为例

    Visualization of Unstructured Sports Data -- An Example of Cricket Short Text Commentary

    [https://arxiv.org/abs/2404.00030](https://arxiv.org/abs/2404.00030)

    使用板球短文本评论数据进行可视化，包括构建球员的实力规则和弱点规则，并展示具有类似规则的球员。

    

    arXiv:2404.00030v1 公告类型：跨领域 体育可视化关注于使用结构化数据，如比赛数据和跟踪数据。与体育相关的非结构化数据来源包括博客、社交媒体帖子和在线新闻文章等。体育可视化方法要么没有充分利用这些来源中存在的信息，要么通过利用这些来源提出的可视化方法没有增强体育可视化方法的范围。我们提出利用非结构化数据，即板球短文本评论进行可视化。短文本评论数据用于构建个人球员的实力规则和弱点规则。我们提出了一个计算上可行的球员实力规则和弱点规则的定义。我们提出了构建规则的可视化方法。此外，计算并可视化具有类似实力规则或弱点规则的球员。我们展示了用于可视化这些规则和球员的方法。

    arXiv:2404.00030v1 Announce Type: cross  Abstract: Sports visualization focuses on the use of structured data, such as box-score data and tracking data. Unstructured data sources pertaining to sports are available in various places such as blogs, social media posts, and online news articles. Sports visualization methods either not fully exploited the information present in these sources or the proposed visualizations through the use of these sources did not augment to the body of sports visualization methods. We propose the use of unstructured data, namely cricket short text commentary for visualization. The short text commentary data is used for constructing individual player's strength rules and weakness rules. A computationally feasible definition for player's strength rule and weakness rule is proposed. A visualization method for the constructed rules is presented. In addition, players having similar strength rules or weakness rules is computed and visualized. We demonstrate the us
    
[^194]: LLM作为写作助手：探讨所有权感和推理的视角

    LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning

    [https://arxiv.org/abs/2404.00027](https://arxiv.org/abs/2404.00027)

    探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。

    

    写作中的所有权感限制了我们对思想、时间和贡献的投入，导致对产出物的依恋。然而，使用写作助手引入了一种心理困境，因为一些内容并非直接我们的创作。我们往往更倾向于在创造性任务中更多地归功于大型语言模型（LLMs），尽管它们对所有任务都是平等的。此外，虽然我们可能不会完全声称对由LLM生成的内容拥有所有权，但却自由地声称作者身份。我们进行了一项简短调查来研究这些问题，并了解潜在的认知过程，以更好地了解人机交互在写作中的应用并改进写作辅助系统。

    arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
    
[^195]: 墨水与个性：在LLMs时代塑造个性化叙事

    Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs

    [https://arxiv.org/abs/2404.00026](https://arxiv.org/abs/2404.00026)

    研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。

    

    个性和个性化构成了使每个作家独特并影响其文字以有效吸引读者同时传达真实性的独特特征。然而，我们日益依赖基于LLM的写作助手可能会危及我们的创造力和个性。我们经常忽视这一趋势对我们的创造力和独特性的负面影响，尽管可能会造成后果。本研究通过进行简要调查探索不同的观点和概念，以及尝试理解人们的观点，结合以往在该领域的研究，来研究这些问题。解决这些问题对于改进人机交互系统和增强个性化和个性化写作助手至关重要。

    arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
    
[^196]: 推进可解释自动驾驶车辆系统：综述与研究路线图

    Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review and Research Roadmap

    [https://arxiv.org/abs/2404.00019](https://arxiv.org/abs/2404.00019)

    本研究综述了现有自动驾驶车辆解释方法的不确定性，提出了一个全面的未来研究路线图，重点放在了了解交流对象和生成及时解释上。

    

    鉴于现有自动驾驶车辆（AVs）解释方法如何满足利益相关者的多样需求存在不确定性，必须进行深入调查以确定需要解释的情境和适当的互动策略。一项全面的综述至关重要，以评估当前方法与AV生态系统内不同利益和期望的一致性。本研究提供了一项综述，讨论了生成和呈现解释所涉及的复杂性，以促进开发更加有效和包容的可解释AV系统。我们的调查将现有文献分类为三个主要主题：解释任务、解释信息和解释信息传达。根据我们的见解，我们提出了一个未来研究的全面路线图，集中在（i）了解交流对象，（ii）生成及时的解释。

    arXiv:2404.00019v1 Announce Type: cross  Abstract: Given the uncertainty surrounding how existing explainability methods for autonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough investigation is imperative to determine the contexts requiring explanations and suitable interaction strategies. A comprehensive review becomes crucial to assess the alignment of current approaches with the varied interests and expectations within the AV ecosystem. This study presents a review to discuss the complexities associated with explanation generation and presentation to facilitate the development of more effective and inclusive explainable AV systems. Our investigation led to categorising existing literature into three primary topics: explanatory tasks, explanatory information, and explanatory information communication. Drawing upon our insights, we have proposed a comprehensive roadmap for future research centred on (i) knowing the interlocutor, (ii) generating timely explanat
    
[^197]: SOMson -- 在Kohonen地图中对多维数据进行音频化

    SOMson -- Sonification of Multidimensional Data in Kohonen Maps

    [https://arxiv.org/abs/2404.00016](https://arxiv.org/abs/2404.00016)

    SOMson提出了一种交互式音频化技术，用于增强Kohonen地图下数据的信息量，解决SOM在提供整体图片时的缺陷。

    

    Kohonen Maps，又称自组织映射（SOMs），是一种可以将高维特征空间可视化到低维地图上的神经网络。虽然SOMs是数据审查和探索的绝佳工具，但它们固有地会导致信息丢失。地图下的数据可视化并不完全整合，因此无法提供全局图片。因此，我们建议使用SOMson，一种对数据进行交互音频化的数据增强技术。音频化增加了SOM同时提供的信息量。我们没有进行用户研究，而是提供了一个交互式在线示例，让读者可以自行探索SOMson。我们讨论了其优势、劣势和前景。

    arXiv:2404.00016v1 Announce Type: cross  Abstract: Kohonen Maps, aka. Self-organizing maps (SOMs) are neural networks that visualize a high-dimensional feature space on a low-dimensional map. While SOMs are an excellent tool for data examination and exploration, they inherently cause a loss of detail. Visualizations of the underlying data do not integrate well and, therefore, fail to provide an overall picture. Consequently, we suggest SOMson, an interactive sonification of the underlying data, as a data augmentation technique. The sonification increases the amount of information provided simultaneously by the SOM. Instead of a user study, we present an interactive online example, so readers can explore SOMson themselves. Its strengths, weaknesses, and prospects are discussed.
    
[^198]: 利用量子增强机器学习赋能信用评分系统

    Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning

    [https://arxiv.org/abs/2404.00015](https://arxiv.org/abs/2404.00015)

    提出了一种名为Systemic Quantum Score (SQS)的新方法，展示在金融领域生产级应用案例中相比纯经典模型更有优势，能够从较少数据点中提取模式并表现出更好性能。

    

    Quantum Kernels被认为在量子机器学习的早期阶段提供了有用性。然而，在利用庞大数据集时，高度复杂的经典模型很难超越，特别是在理解力方面。尽管如此，一旦数据稀缺且倾斜，经典模型就会遇到困难。量子特征空间被预计在这样具有挑战性的情景中能够找到更好的数据特征和目标类别之间的联系，最重要的是增强了泛化能力。在这项工作中，我们提出了一种名为Systemic Quantum Score (SQS)的新方法，并提供了初步结果，表明在金融行业生产级应用案例中，SQS可能比纯经典模型具有优势。我们的具体研究表明，SQS能够从较少的数据点中提取出模式，并且在数据需求量大的算法（如XGBoost）上表现出更好的性能，带来优势。

    arXiv:2404.00015v1 Announce Type: cross  Abstract: Quantum Kernels are projected to provide early-stage usefulness for quantum machine learning. However, highly sophisticated classical models are hard to surpass without losing interpretability, particularly when vast datasets can be exploited. Nonetheless, classical models struggle once data is scarce and skewed. Quantum feature spaces are projected to find better links between data features and the target class to be predicted even in such challenging scenarios and most importantly, enhanced generalization capabilities. In this work, we propose a novel approach called Systemic Quantum Score (SQS) and provide preliminary results indicating potential advantage over purely classical models in a production grade use case for the Finance sector. SQS shows in our specific study an increased capacity to extract patterns out of fewer data points as well as improved performance over data-hungry algorithms such as XGBoost, providing advantage i
    
[^199]: 具有精确语义和AI驱动流程的缺失数据插补及破产预测

    Missing Data Imputation With Granular Semantics and AI-driven Pipeline for Bankruptcy Prediction

    [https://arxiv.org/abs/2404.00013](https://arxiv.org/abs/2404.00013)

    本文介绍了一种具有精确语义的缺失数据插补方法，通过在粒空间中利用特征语义和可靠观测来预测缺失值，从而解决了破产预测中的重要挑战。

    

    本文着重设计了一个用于预测破产的流程。缺失值、高维数据以及高度类别不平衡的数据库是该任务中的主要挑战。本文介绍了一种具有精确语义的缺失数据插补新方法。探讨了粒计算的优点以定义此方法。利用特征语义和可靠观测在低维空间、粒空间中预测缺失值。围绕每个缺失条目形成粒子，考虑到一些高度相关的特征和最可靠的最近观测以保持数据库对缺失条目的相关性和可靠性。然后在这些上下文粒子中进行跨粒子预测进行插补。也就是说，上下文粒子使得在巨大数据库中进行一小部分相关的插补成为可能。

    arXiv:2404.00013v1 Announce Type: cross  Abstract: This work focuses on designing a pipeline for the prediction of bankruptcy. The presence of missing values, high dimensional data, and highly class-imbalance databases are the major challenges in the said task. A new method for missing data imputation with granular semantics has been introduced here. The merits of granular computing have been explored here to define this method. The missing values have been predicted using the feature semantics and reliable observations in a low-dimensional space, in the granular space. The granules are formed around every missing entry, considering a few of the highly correlated features and most reliable closest observations to preserve the relevance and reliability, the context, of the database against the missing entries. An intergranular prediction is then carried out for the imputation within those contextual granules. That is, the contextual granules enable a small relevant fraction of the huge 
    
[^200]: FABind+: 通过改进口袋预测和姿态生成增强分子对接

    FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation

    [https://arxiv.org/abs/2403.20261](https://arxiv.org/abs/2403.20261)

    FABind+通过改进口袋预测和姿态生成，提升分子对接表现

    

    分子对接是药物发现中至关重要的过程。传统技术依赖于受物理原理支配的广泛采样和模拟，但这些方法往往速度慢且昂贵。基于深度学习的方法的出现显示出显著的前景，提供了精确性和效率的增长。建立在FABind的基础工作之上，这是一个专注于速度和准确性的模型，我们提出了FABind+，这是一个大大提升其前身性能的增强版。我们确定口袋预测是分子对接中的一个关键瓶颈，并提出了一种显著改进口袋预测的新方法，从而简化了对接过程。此外，我们对对接模块进行了修改，以增强其姿态生成能力。为了缩小与传统采样/生成方法之间的差距，我们结合了一个简单而有效的s

    arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
    
[^201]: DiJiang：通过紧凑的核方法实现高效的大型语言模型

    DiJiang: Efficient Large Language Models through Compact Kernelization

    [https://arxiv.org/abs/2403.19928](https://arxiv.org/abs/2403.19928)

    DiJiang提出了一种新颖的频域核方法，可以将预训练的基本Transformer模型转化为具有线性复杂度的模型，大大减少训练成本，并在理论上提供更好的逼近效率。

    

    为了减少Transformers的计算负荷，线性注意力的研究已经取得了显著的进展。然而，注意机制的改进策略通常需要经过大量的重新训练，在具有大量参数的大型语言模型上是不切实际的。本文介绍了DiJiang，一种新颖的频域核方法，可将预训练的基本Transformer转化为具有较小训练成本的线性复杂度模型。通过采用加权拟随机采样法，所提出的方法在理论上提供了更好的逼近效率。为了进一步降低训练的计算复杂度，我们的核方法基于离散余弦变换（DCT）操作。大量实验证明，所提出的方法达到了与原始Transformer相当的性能，但训练时间大大减少。

    arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin
    
[^202]: NJUST-KMG参加TRAC-2024任务1和任务2：离线危害潜在性识别

    NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification

    [https://arxiv.org/abs/2403.19713](https://arxiv.org/abs/2403.19713)

    该研究提出了在TRAC-2024离线危害潜在性识别任务中的方法，利用专家标注的社交媒体评论数据集，成功设计了能够准确评估危害可能性并识别目标的算法，在两个赛道中取得第二名的成绩。

    

    这份报告详细描述了我们在TRAC-2024离线危害潜在性识别中提出的方法，该比赛包含两个子任务。研究利用了一个包含多种印度语言社交媒体评论的丰富数据集，由专家评分标注，以捕捉离线环境危害的微妙含义。参与者的任务是设计能够准确评估特定情况下危害可能性并识别离线危害最可能的目标的算法。我们的方法在两个不同的赛道中排名第二，F1值分别为0.73和0.96。我们的方法主要涉及选择预训练模型进行微调，整合对比学习技术，并通过集成方法应用于测试集。

    arXiv:2403.19713v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.
    
[^203]: 稀疏特征电路：在语言模型中发现和编辑可解释的因果图

    Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models

    [https://arxiv.org/abs/2403.19647](https://arxiv.org/abs/2403.19647)

    该论文介绍了一种新方法，即稀疏特征电路，可以在语言模型中发现和编辑可解释的因果图，为我们提供了对未预料机制的详细理解和包含了用于提高分类器泛化能力的SHIFT方法。

    

    我们介绍了用于发现和应用稀疏特征电路的方法。这些电路是人类可解释特征的因果相关子网络，用于解释语言模型行为。 在先前的工作中确定的电路由多义且难以解释的单元组成，例如注意力头或神经元，使它们不适用于许多下游应用。 相比之下，稀疏特征电路实现了对未预料机制的详细理解。 由于它们基于细粒度单元，稀疏特征电路对下游任务非常有用：我们 introduc了SHIFT，通过切除人类判断为任务不相关的特征，从而提高分类器的泛化能力。 最后，我们通过发现成千上万个稀疏特征电路来展示一个完全无监督且可扩展的可解释性管线，用于自动发现的模型行为。

    arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
    
[^204]: 自我改进学习用于可扩展神经组合优化

    Self-Improved Learning for Scalable Neural Combinatorial Optimization

    [https://arxiv.org/abs/2403.19561](https://arxiv.org/abs/2403.19561)

    提出一种新颖的自我改进学习(SIL)方法，实现神经组合优化的更好可扩展性，通过自身生成解决方案作为伪标签，设计线性复杂度的注意机制来处理大规模组合优化问题实例。

    

    end-to-end神经组合优化(NCO)方法在解决复杂组合优化问题方面表现出有希望的性能，而不需要专家设计。然而，现有方法在处理大规模问题时存在困难，限制了它们的实际适用性。为了克服这一限制，本研究提出了一种新颖的自我改进学习(SIL)方法，以实现神经组合优化的更好可扩展性。具体来说，我们开发了一种高效的自我改进机制，使模型能够在没有标记数据的情况下直接在大规模问题实例上进行训练。通过一种创新的局部重构方法，该方法可以通过自身迭代生成更好的解决方案作为伪标签，以指导有效的模型训练。此外，我们设计了一种线性复杂度的注意机制，使模型能够有效处理低计算开销的大规模组合优化问题实例。

    arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
    
[^205]: 在医疗保健领域评估机器学习中的公平特征选择

    Evaluating Fair Feature Selection in Machine Learning for Healthcare

    [https://arxiv.org/abs/2403.19165](https://arxiv.org/abs/2403.19165)

    通过考虑对所有人口统计群体均等重要性的公平特征选择方法，在医疗保健领域评估算法公平性，确保在减少偏见和全局分类错误之间实现平衡。

    

    随着机器学习在医疗保健领域的普及，自动化社会偏见进一步加剧健康差距的潜力构成了重大风险。我们从特征选择的角度探讨算法公平性。传统的特征选择方法通过去除资源密集、相关或不相关的特征来识别用于更好决策的特征，但忽略了这些因素在不同子群体中可能存在的差异。为了应对这些问题，我们评估了一种考虑对所有人口统计群体均等重要性的公平特征选择方法。我们在特征选择过程中同时考虑了公平性度量和错误度量，以确保在最大程度减少偏见和全局分类错误之间实现平衡。我们在三个公开可用的医疗保健数据集上测试了我们的方法。在所有三个数据集上，我们观察到公平性指标得到改善，同时分类错误仅有轻微下降。

    arXiv:2403.19165v1 Announce Type: new  Abstract: With the universal adoption of machine learning in healthcare, the potential for the automation of societal biases to further exacerbate health disparities poses a significant risk. We explore algorithmic fairness from the perspective of feature selection. Traditional feature selection methods identify features for better decision making by removing resource-intensive, correlated, or non-relevant features but overlook how these factors may differ across subgroups. To counter these issues, we evaluate a fair feature selection method that considers equal importance to all demographic groups. We jointly considered a fairness metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error. We tested our approach on three publicly available healthcare datasets. On all three datasets, we observed improvements in fairness metrics coupled with a minimal degradation 
    
[^206]: 小型机器学习：进展与未来展望

    Tiny Machine Learning: Progress and Futures

    [https://arxiv.org/abs/2403.19076](https://arxiv.org/abs/2403.19076)

    TinyML是一种将深度学习模型压缩到物联网设备和微控制器中实现无处不在智能的新方法，需要共同设计算法和系统堆栈以克服硬件限制。

    

    Tiny Machine Learning（TinyML）是机器学习的一个新领域。通过将深度学习模型压缩到数十亿个物联网设备和微控制器（MCUs）中，我们扩展了人工智能应用的范围，实现了无处不在的智能。然而，由于硬件限制，TinyML具有挑战性：有限的内存资源使得难以容纳为云和移动平台设计的深度学习模型。对于裸机设备，编译器和推断引擎支持也有限。因此，我们需要共同设计算法和系统堆栈以实现TinyML。

    arXiv:2403.19076v1 Announce Type: cross  Abstract: Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and in
    
[^207]: 微服务系统的少样本跨系统异常跟踪分类

    Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems

    [https://arxiv.org/abs/2403.18998](https://arxiv.org/abs/2403.18998)

    提出了针对微服务系统的少样本异常跟踪分类的新框架，利用多头注意力自编码器构建系统特定的跟踪表示，并应用基于Transformer编码器的模型无关元学习进行高效分类。

    

    微服务系统（MSS）由于其复杂和动态的特性可能在各种故障类别中出现故障。为了有效处理故障，AIOps工具利用基于跟踪的异常检测和根本原因分析。本文提出了一个新颖的框架，用于微服务系统的少样本异常跟踪分类。我们的框架包括两个主要组成部分：（1）多头注意力自编码器用于构建系统特定的跟踪表示，从而实现（2）基于Transformer编码器的模型无关元学习，以进行有效和高效的少样本异常跟踪分类。该框架在两个代表性的MSS，Trainticket和OnlineBoutique上进行了评估，使用开放数据集。结果表明，我们的框架能够调整学到的知识，以对新的、未见的新颖故障类别的异常跟踪进行分类，无论是在最初训练的同一系统内，还是在其他系统中。

    arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
    
[^208]: 使用基于元胞自动机模型和CNN-LSTM架构进行交通流预测的深度学习

    Deep Learning for Traffic Flow Prediction using Cellular Automata-based Model and CNN-LSTM architecture

    [https://arxiv.org/abs/2403.18710](https://arxiv.org/abs/2403.18710)

    该论文提出使用基于元胞自动机模型和CNN-LSTM架构的深度学习方法成功预测交通流，充分利用交通流动态领域知识。

    

    近期的研究尝试使用深度学习来预测交通流的未来状态，但取得了不同的结果。这些方法面临两个关键挑战。首先，训练深度学习神经网络需要大量的训练数据，而对于交通流系统来说这样的数据目前并不容易获得。其次，即使有数据，神经网络也需要访问涵盖大多数可能的交通流动态的历史数据才能成功预测未来的交通状态。具体来说，这些深度学习方法没有充分利用关于交通流动态的领域知识，尽管已经有相当多的现有知识库。在这项工作中，我们提出使用卷积神经网络（CNNs）与长短期记忆（LSTM）深度学习架构成功预测交通流，并利用基于元胞自动机的交通流统计力学模型生成交通流数据。

    arXiv:2403.18710v1 Announce Type: new  Abstract: Recent works have attempted to use deep learning to predict future states of traffic flow, but have met with mixed results. These approaches face two key challenges. First, training deep learning neural networks requires large amounts of training data which are not yet easily available for traffic flow systems. Second, even when data is available, the neural networks require access to historical data that covers most possible traffic flow dynamics to successfully predict future traffic states. Specifically, these deep learning approaches do not fully leverage domain-knowledge about traffic flow dynamics, despite a significant existing knowledge-base. In this work, we propose to solve both issues using a Convolutional Neural Network (CNNs) with Long Short Term Memory (LSTM) deep learning architecture to successfully predict traffic flow, while leveraging a cellular automata-based statistical mechanics model of traffic flow to generate tra
    
[^209]: 安全稳健的强化学习: 原则与实践

    Safe and Robust Reinforcement-Learning: Principles and Practice

    [https://arxiv.org/abs/2403.18539](https://arxiv.org/abs/2403.18539)

    本文通过对安全和稳健RL领域的探索，识别并进一步理解了在实际场景中部署RL系统面临的重大挑战，提出了不同算法方法的综合评述以增强RL代理的安全性和稳健性。

    

    强化学习（RL）在解决相对复杂的任务方面取得了显著成功，然而在实际场景中部署RL系统面临与安全和稳健性有关的重大挑战。本文旨在通过探索安全和稳健RL领域的主要维度，涵盖算法、伦理和实际考虑因素，识别并进一步理解这些挑战。我们对近年来致力于解决与RL应用相关固有风险的方法和开放问题进行了全面审查。在讨论并提出安全和稳健RL的定义后，本文将现有研究工作归类为不同的算法方法，以增强RL代理的安全性和稳健性。我们考察了诸如不确定性估计、优化方法学、探索和利用的权衡以及对抗性等技术。

    arXiv:2403.18539v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness. This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations. We conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with RL applications.   After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents. We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and adversari
    
[^210]: 双向一致性模型

    Bidirectional Consistency Models

    [https://arxiv.org/abs/2403.18035](https://arxiv.org/abs/2403.18035)

    提出了双向一致性模型（BCM），学习一个神经网络，能够实现沿着概率流常微分方程前向和后向遍历，从而有效地统一了生成和编辑图像等任务。

    

    扩散模型（DMs）通过迭代去噪一个随机向量能够生成非常高质量的样本，这个过程对应于沿着概率流常微分方程（PF ODE）移动。有趣的是，DMs还可以通过沿着PF ODE向后移动将输入图像转换为噪声，这是下游任务（如插值和图像编辑）的关键操作。然而，这一过程的迭代性质限制了其速度，阻碍了其更广泛的应用。最近，一致性模型（CMs）已经出现，以解决这一挑战，通过近似PF ODE的积分，从而避免了需要迭代。然而，缺乏显式ODE求解器使得反演过程复杂化。为了解决这个问题，我们引入了双向一致性模型（BCM），学习单个神经网络，能够同时实现沿着PF ODE的前向和后向遍历，有效地统一生成和

    arXiv:2403.18035v1 Announce Type: new  Abstract: Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation an
    
[^211]: 旋转扫描：带有三元SSM模块的UNet-like Mamba用于医学图像分割

    Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation

    [https://arxiv.org/abs/2403.17701](https://arxiv.org/abs/2403.17701)

    本文提出了Triplet Mamba-UNet，利用残余VSS块提取密集上下文特征，并利用Triplet SSM融合空间和通道维度上的特征。

    

    图像分割在医疗领域的诊断和治疗中占据重要位置。传统的卷积神经网络（CNN）和Transformer模型在这一领域取得了重大进展，但仍然面临由于有限感受野或高计算复杂性而带来的挑战。最近，状态空间模型（SSM），特别是Mamba及其变体，在视觉领域表现出显著性能。然而，它们的特征提取方法可能不够有效，保留了一些冗余结构，留下了参数减少的空间。受先前的空间和通道注意方法的启发，我们提出了Triplet Mamba-UNet。该方法利用残余VSS块来提取密集的上下文特征，同时利用Triplet SSM来融合空间和通道维度上的特征。我们在ISIC17、ISIC18、CVC-300、CVC-ClinicDB上进行了实验。

    arXiv:2403.17701v1 Announce Type: cross  Abstract: Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, 
    
[^212]: 深度学习及其最新应用综述

    A Survey on Deep Learning and State-of-the-arts Applications

    [https://arxiv.org/abs/2403.17561](https://arxiv.org/abs/2403.17561)

    深度学习是解决复杂问题的强大工具，本研究旨在全面审视深度学习模型及其应用的最新发展

    

    深度学习, 是人工智能的一个分支，是一种利用多层互连单元（神经元）从原始输入数据中直接学习复杂模式和表示的计算模型。受到这种学习能力的赋能，深度学习已成为解决复杂问题的强大工具，是许多突破性技术和创新的核心驱动力。构建深度学习模型是一项具有挑战性的任务，因为算法的复杂性和现实问题的动态性。有几项研究回顾了深度学习的概念和应用。然而，这些研究大多集中于深度学习模型类型和卷积神经网络架构，对深度学习模型及其在不同领域解决复杂问题的最新发展的覆盖面有限。因此，受到这些限制的启发，本研究旨在全面审视th

    arXiv:2403.17561v1 Announce Type: new  Abstract: Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review th
    
[^213]: 使用区域指导标记将孟加拉文本与地方方言转录为国际音标

    Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens

    [https://arxiv.org/abs/2403.17407](https://arxiv.org/abs/2403.17407)

    通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。

    

    孟加拉文本到国际音标（IPA）的准确转录是一项具有挑战性的任务，主要是由于语言的复杂音韵学和语境相关的音变。对于区域孟加拉方言来说，由于缺乏针对这些方言的标准拼写约定、当地和外语在这些地区中流行的词汇以及不同地区之间的音韵多样性，这一挑战甚至更为严峻。本文提出了一种方法来解决这个序列到序列的问题，即在覆盖孟加拉国六个地区的新数据集上引入“区域指导标记”（DGT）技术。其关键思想是在生成IPA转录之前向模型提供有关输入文本的区域方言或“地区”的明确信息。这通过在输入序列前添加一个地区标记来实现，有效地引导模型理解与每个地区相关的独特音韵模式。

    arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
    
[^214]: 一种用于电力价格预测的Transformer方法

    A Transformer approach for Electricity Price Forecasting

    [https://arxiv.org/abs/2403.16108](https://arxiv.org/abs/2403.16108)

    这种独特的Transformer模型在电力价格预测中取得了更好的表现，为可靠和可持续的电力系统运行提供了有前景的解决方案。

    

    本文提出了一种使用纯Transformer模型进行电力价格预测（EPF）的新方法。与其他方法不同，没有使用其他递归网络结合注意力机制。因此，表明注意力层足以捕捉时间模式。该论文还通过使用开源EPF工具进行了对模型的公平比较，并提供了代码以增强EPF研究的可再现性和透明度。结果表明，Transformer模型优于传统方法，为可靠和可持续的电力系统运行提供了一种有希望的解决方案。

    arXiv:2403.16108v1 Announce Type: cross  Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
    
[^215]: 从损失角度理解语言模型的突现能力

    Understanding Emergent Abilities of Language Models from the Loss Perspective

    [https://arxiv.org/abs/2403.15796](https://arxiv.org/abs/2403.15796)

    本文从损失角度重新定义了语言模型的突现能力，发现具有相同预训练损失的模型在不同任务上表现相似，而当预训练损失低于特定阈值时，模型将展现出突现能力。

    

    近期研究质疑了传统认为语言模型的突现能力仅存在于大模型中的观点。这种怀疑源自两点观察：1）较小的模型也能展现出对突现能力的高性能；2）质疑用于测量这些能力的不连续性指标。本文提议从预训练损失的角度研究突现能力，而非模型大小或训练计算。我们展示了具有相同预训练损失但不同模型和数据大小的模型，在各种下游任务上表现相同。我们还发现，当某一模型的预训练损失低于特定阈值时，在某些任务上表现出突现能力，而不论指标的连续性如何；而在达到该阈值之前，其性能仍保持在随机猜测水平。这启发我们重新定义突现能力为那些......

    arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
    
[^216]: 大型语言模型的差分私有下一个标记预测

    Differentially Private Next-Token Prediction of Large Language Models

    [https://arxiv.org/abs/2403.15638](https://arxiv.org/abs/2403.15638)

    提出了Private Mixing of Ensemble Distributions (PMixED)：通过将模型的输出分布投影到公共LLM的输出分布周围的集合上，并采样平均来实现实际的下一个标记预测，以更轻量化的方式实现对隐私敏感的大型语言模型的预测。

    

    确保大型语言模型（LLMs）的隐私日益重要。DP-SGD是实现这一目标的最广泛采用的技术，它以一种保证差分隐私的方式训练模型。然而，DP-SGD需要比SGD更长的训练时间和更大的内存需求，同时过高估计对手具有白盒访问模型的能力。更现实的场景假设只有对隐私敏感的LLM进行黑盒访问。在这些观察的基础上，我们提出了私有混合集合分布（PMixED）：一种通过将模型的每个输出分布从一个经过精细调整的LLM集合投影到公共LLM输出分布周围的集合上，然后对投影分布进行平均并从中抽样来实现实际的下一个标记预测的私有预测协议。我们的方法比DP-SGD更轻量化，因为它与模型无关。

    arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
    
[^217]: 引入一种集成方法，通过分析PET扫描图像早期检测阿尔茨海默病

    Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images

    [https://arxiv.org/abs/2403.15443](https://arxiv.org/abs/2403.15443)

    通过分析PET扫描图像，引入了一种集成方法早期检测阿尔茨海默病，并且在分类阿尔茨海默病时使用了多种深度学习和传统机器学习模型。

    

    阿尔茨海默病是一种逐渐恶化的神经退行性疾病，主要影响记忆、思维和行为等认知功能。本病存在一个关键阶段，即轻度认知障碍，非常重要尽早诊断，因为一些逐渐发展为病症的MCI患者会发展为这种疾病。本研究探讨了将阿尔茨海默病分类为四个不同组：控制正常（CN）、逐渐发展的轻度认知障碍（pMCI）、稳定的轻度认知障碍（sMCI）和阿尔茨海默病（AD）的具有挑战性的任务。这种分类是基于对从ADNI数据集获得的PET扫描图像的彻底检查，这提供了对疾病进展的彻底理解。已经使用了几种深度学习和传统机器学习模型来检测阿尔茨海默病。在本文中，使用了三种深度学习模型，即VGG16、AlexNet和自定义的卷积神经网络。

    arXiv:2403.15443v1 Announce Type: cross  Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that primarily affects cognitive functions such as memory, thinking, and behavior. In this disease, there is a critical phase, mild cognitive impairment, that is really important to be diagnosed early since some patients with progressive MCI will develop the disease. This study delves into the challenging task of classifying Alzheimer's disease into four distinct groups: control normal (CN), progressive mild cognitive impairment (pMCI), stable mild cognitive impairment (sMCI), and Alzheimer's disease (AD). This classification is based on a thorough examination of PET scan images obtained from the ADNI dataset, which provides a thorough understanding of the disease's progression. Several deep-learning and traditional machine-learning models have been used to detect Alzheimer's disease. In this paper, three deep-learning models, namely VGG16 and AlexNet, and a custom Convolu
    
[^218]: 了解过境缺口：南卡罗来纳州夏洛特市南端和田纳西州查塔努加市阿冯代尔的即需公共汽车服务及城市气候适应能力的比较研究

    Understanding the Transit Gap: A Comparative Study of On-Demand Bus Services and Urban Climate Resilience in South End, Charlotte, NC and Avondale, Chattanooga, TN

    [https://arxiv.org/abs/2403.14671](https://arxiv.org/abs/2403.14671)

    本研究揭示了城市设计在公共交通效率和减少碳排放方面的关键作用，指出城市布局对公共交通结果有重要影响，提出了针对不同城市设计元素的定制策略对于气候适应至关重要。

    

    城市设计在可持续性方面具有重要影响，特别是在公共交通效率和减少碳排放方面。本研究探讨了具有不同城市设计的两个社区：北卡罗来纳州夏洛特市南端，具有动态混合用途的城市设计模式；和田纳西州查塔努加市阿冯代尔，采用住宅郊区网格布局。通过使用TRANSIT-GYM工具，我们评估了在这些不同城市环境中增加公共汽车利用率对交通和CO2排放的影响。我们的结果突显了城市设计和规划在交通系统效率中的关键作用。在夏洛特市南端，混合用途设计导致更多的排放减少，表明城市布局可以显著影响公共交通结果。考虑到独特的城市设计元素的量身定制策略对于气候适应至关重要。值得注意的是，在南端，公共汽车利用率翻倍使日常排放减少了10.18％。

    arXiv:2403.14671v1 Announce Type: cross  Abstract: Urban design significantly impacts sustainability, particularly in the context of public transit efficiency and carbon emissions reduction. This study explores two neighborhoods with distinct urban designs: South End, Charlotte, NC, featuring a dynamic mixed-use urban design pattern, and Avondale, Chattanooga, TN, with a residential suburban grid layout. Using the TRANSIT-GYM tool, we assess the impact of increased bus utilization in these different urban settings on traffic and CO2 emissions. Our results highlight the critical role of urban design and planning in transit system efficiency. In South End, the mixed-use design led to more substantial emission reductions, indicating that urban layout can significantly influence public transit outcomes. Tailored strategies that consider the unique urban design elements are essential for climate resilience. Notably, doubling bus utilization decreased daily emissions by 10.18% in South End a
    
[^219]: 大型模型的参数高效微调：一项全面调研

    Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey

    [https://arxiv.org/abs/2403.14608](https://arxiv.org/abs/2403.14608)

    大型模型参数高效微调（PEFT）是通过调整预训练模型的参数，以适应特定任务，并减少引入的附加参数或计算资源数量的实用解决方案。

    

    大型模型在多个应用领域代表了一项突破性的进展，使得在各种任务中取得了显著成就。然而，它们空前的规模带来了巨大的计算成本。这些模型通常由数十亿个参数组成，需要大量的计算资源来执行。特别是，在为特定下游任务定制大型模型时，尤其是在受到计算能力限制的硬件平台上，规模庞大和计算要求巨大构成了重大挑战。参数高效微调（PEFT）提供了一个实用解决方案，可以有效地调整大型模型以适应各种下游任务。具体而言，PEFT是指调整预训练大型模型的参数，使其适应特定任务的过程，同时尽量减少引入的附加参数或所需的计算资源数量。

    arXiv:2403.14608v1 Announce Type: new  Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approac
    
[^220]: ReAct遇上ActRe：对比性自训练中的代理轨迹自动标注

    ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training

    [https://arxiv.org/abs/2403.14589](https://arxiv.org/abs/2403.14589)

    提出了A$^3$T框架，通过ActRe提示代理实现了ReAct风格代理对代理轨迹的自主标注，同时增强了新的轨迹合成能力。

    

    arXiv:2403.14589v1 公告类型：新 文摘：语言代理通过与基础模型推理展示了自主决策能力。最近，人们致力于通过多步推理和行动轨迹作为训练数据来训练语言代理以提高性能。然而，收集这样的轨迹仍需要相当大的人力，无论是通过人工标注还是实施多样化提示框架。在这项工作中，我们提出了A$^3$T，一个允许以ReAct风格自主注释代理轨迹的框架。其中心是一个ActRe提示代理，它解释任意动作的原因。当随机抽取外部动作时，ReAct风格代理可以查询ActRe代理以获取其文本理由。新颖的轨迹然后通过将ActRe的后验推理前置到抽样动作中进行综合合成。通过这种方式，ReAct风格代理可执行

    arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
    
[^221]: C-TPT：通过文本特征离散性的校准测试时提示调整视觉-语言模型

    C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion

    [https://arxiv.org/abs/2403.14119](https://arxiv.org/abs/2403.14119)

    本文研究了在测试时提示调整过程中通过利用CLIP的固有属性来探讨校准的方法，发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。

    

    在深度学习中，测试时适应已经引起了人们的关注，作为一种在不需要标记数据的情况下对模型进行微调的方法。一个主要的例证是最近提出的用于大规模视觉-语言模型（如CLIP）的测试时提示调整。然而，这些提示主要是为了提高准确性而开发的，忽视了校准的重要性——量化预测不确定性的关键方面。然而，传统的校准方法依赖大量标记数据，这使得它们在测试时场景下不切实际。为此，本文通过利用CLIP的固有属性，在测试时提示调整过程中探讨校准。通过一系列观察，我们发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。

    arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
    
[^222]: ZigMa：蜿蜒曼巴扩散模型

    ZigMa: Zigzag Mamba Diffusion Model

    [https://arxiv.org/abs/2403.13802](https://arxiv.org/abs/2403.13802)

    本研究提出了一种名为Zigzag Mamba的零参数方法，通过纠正当前Mamba-based视觉方法中对空间连续性的忽视，实现了更好的速度和内存利用，同时在大分辨率视觉数据集上展示了出色的性能。

    

    扩散模型长期以来一直受到可伸缩性和二次复杂性问题的困扰，特别是在基于变压器的结构内部。在这项研究中，我们旨在利用一种称为曼巴的状态空间模型的长序列建模能力，以扩展其在视觉数据生成中的适用性。首先，我们确定了大多数当前基于曼巴的视觉方法中的一个关键疏忽，即曼巴的扫描方案中缺乏对空间连续性的考虑。其次，基于这一洞察力，我们介绍了一种名为Zigzag Mamba的简单、即插即用、零参数方法，它优于基于曼巴的基线，并表现出比基于变压器的基线更快速和更好的内存利用。最后，我们将Zigzag Mamba集成到随机插值框架中，以研究模型在大分辨率视觉数据集（例如FacesHQ $1024\times 1024$和UCF101，MultiModal-CelebA-HQ）上的可伸缩性。

    arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
    
[^223]: 核多重网格：通过稀疏高斯过程回归加速反向拟合

    Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression

    [https://arxiv.org/abs/2403.13300](https://arxiv.org/abs/2403.13300)

    通过核包技术证明反向拟合的收敛速度，并提出了核多重网格算法，通过稀疏高斯过程回归增强反向拟合，适用于结构化和分散数据的加性GPs。

    

    添加高斯过程(GPs)是非参数特征选择的流行方法。对于这些模型的常见训练方法是贝叶斯反向拟合。然而，在训练加性GPs时，反向拟合的收敛速度仍然是一个悬而未决的问题。通过利用一种称为核包(KP)的技术，我们证明了反向拟合的收敛速度不会比$(1-\mathcal{O}(\frac{1}{n}))^t$更快，其中$n$和$t$分别表示数据大小和迭代次数。因此，反向拟合需要最少$\mathcal{O}(n\log n)$次迭代才能实现收敛。基于KP，我们进一步提出了一种称为核多重网格(KMG)的算法。该算法通过将稀疏高斯过程回归(GPR)纳入每个反向拟合迭代之后处理残差来增强反向拟合。它适用于具有结构化和分散数据的加性GPs。从理论上讲，我们证明K

    arXiv:2403.13300v1 Announce Type: cross  Abstract: Additive Gaussian Processes (GPs) are popular approaches for nonparametric feature selection. The common training method for these models is Bayesian Back-fitting. However, the convergence rate of Back-fitting in training additive GPs is still an open problem. By utilizing a technique called Kernel Packets (KP), we prove that the convergence rate of Back-fitting is no faster than $(1-\mathcal{O}(\frac{1}{n}))^t$, where $n$ and $t$ denote the data size and the iteration number, respectively. Consequently, Back-fitting requires a minimum of $\mathcal{O}(n\log n)$ iterations to achieve convergence. Based on KPs, we further propose an algorithm called Kernel Multigrid (KMG). This algorithm enhances Back-fitting by incorporating a sparse Gaussian Process Regression (GPR) to process the residuals subsequent to each Back-fitting iteration. It is applicable to additive GPs with both structured and scattered data. Theoretically, we prove that K
    
[^224]: STG-Mamba: 通过选择性状态空间模型进行时空图学习

    STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model

    [https://arxiv.org/abs/2403.12418](https://arxiv.org/abs/2403.12418)

    STG-Mamba 是首个利用选择性状态空间模型进行时空图学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。

    

    Spatial-Temporal Graph（STG）数据具有动态性、异质性和非平稳性特点，导致时空图学习持续面临挑战。近年来，提出了各种基于GNN的方法，主要集中于模拟STG网络中节点个体之间的关系，忽略了随时间存在的STG系统本质特征的建模重要性。相反，现代选择性状态空间模型（SSSMs）提出了一种将STG网络视为系统的新方法，并精心探索了STG系统在时间维度上的动态状态演变。在本工作中，我们引入了Spatial-Temporal Graph Mamba（STG-Mamba），作为首个利用强大的选择性状态空间模型进行STG学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。

    arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
    
[^225]: 模型重新编程优于在文本图像编码器中针对分布外数据进行微调

    Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders

    [https://arxiv.org/abs/2403.10800](https://arxiv.org/abs/2403.10800)

    模型重新编程方法 Reprogrammer 在文本图像编码器中的应用优于传统微调方法，能够提高下游模型在分布内和分布外数据中的性能表现

    

    在评估将预训练模型转移到下游任务的性能时，不仅需要评估下游模型的分布内（ID）准确性，还需要评估其泛化能力并识别分布外（OOD）样本。本文揭示了侵入性微调技术所带来的隐藏成本。具体来说，我们证明了常用的微调方法不仅扭曲了用于泛化到协变量转移的OOD样本（OOD泛化）所需的表示，还扭曲了用于检测在语义上转移的OOD样本（OOD检测）所需的表示。为了解决这些挑战，我们引入了一种新的模型重新编程方法用于微调，我们将其命名为重新编程器。重新编程器旨在提高下游模型在ID、OOD泛化和OOD检测任务中的整体性能。我们的实证证据显示重新编程器优于对抗性微调与原型微调方法，是一种通用、有效且强大的工具来应对OOD挑战。

    arXiv:2403.10800v1 Announce Type: new  Abstract: When evaluating the performance of a pre-trained model transferred to a downstream task, it is imperative to assess not only the in-distribution (ID) accuracy of the downstream model but also its capacity to generalize and identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden costs associated with intrusive fine-tuning techniques. Specifically, we demonstrate that commonly used fine-tuning methods not only distort the representations necessary for generalizing to covariate-shifted OOD samples (OOD generalization) but also distort the representations necessary for detecting semantically-shifted OOD samples (OOD detection). To address these challenges, we introduce a new model reprogramming approach for fine-tuning, which we name Reprogrammer. Reprogrammer aims to improve the holistic performance of the downstream model across ID, OOD generalization, and OOD detection tasks. Our empirical evidence reveals that Rep
    
[^226]: 一个白盒神经网络的概念框架

    A Conceptual Framework For White Box Neural Networks

    [https://arxiv.org/abs/2403.09863](https://arxiv.org/abs/2403.09863)

    引入语义特征作为通用概念框架，实现了完全可解释的神经网络层，为白盒神经网络的范式转变开辟了新的可能性。

    

    本文引入语义特征作为完全可解释神经网络层的通用概念框架。一个充分动机的MNIST相关子问题的概念验证模型包括4个这样的层，总共4800个可学习参数。该模型易于解释，无需任何形式的对抗训练即可实现人类水平的对抗测试准确率，需要较少的超参数调节，并且可以在单个CPU上快速训练。该技术的通用性承诺为彻底民主化和真正通用的白盒神经网络带来了希望。代码可在https://github.com/314-Foundation/white-box-nn找到。

    arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
    
[^227]: 通过结构化稀疏张量分解对稀疏DNN加速进行抽象化

    Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition

    [https://arxiv.org/abs/2403.07953](https://arxiv.org/abs/2403.07953)

    本文提出了通过结构化分解张量进一步抽象稀疏DNN加速的方法，实现了将稀疏张量转换成一系列结构化稀疏张量，从而弥合了稀疏DNN模型和硬件之间的差距。

    

    在深度神经网络（DNNs）中利用稀疏性已成为满足现代DNN日益增长的计算需求的一种具有前景的领域。然而，在实践中，稀疏DNN加速仍然面临一个关键挑战。为了最小化稀疏加速的开销，硬件设计师最近提出了结构化稀疏硬件支持，这提供了有限的灵活性并需要额外的模型微调。此外，为某些结构化稀疏硬件微调的任何稀疏模型无法被其他结构化硬件加速。为了弥合稀疏DNN模型和硬件之间的差距，本文提出了通过结构分解的张量近似（TASD），利用了线性代数中的分配性质将任何稀疏张量转化为一系列结构化稀疏张量。接下来，我们开发了一个软件框架TASDER，通过搜索逐层高质量的结构化分解来加速DNNs的权重和...

    arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
    
[^228]: SVD-LLM: 针对大型语言模型压缩的截断感知奇异值分解

    SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression

    [https://arxiv.org/abs/2403.07378](https://arxiv.org/abs/2403.07378)

    SVD-LLM是一种新的基于SVD的LLM压缩方法，通过截断感知数据白化策略和逐层闭式模型参数更新策略，解决了现有方法的限制，实现了直接映射奇异值和压缩损失之间的关系。

    

    大型语言模型（LLMs）的进展受到其庞大尺寸的限制，这需要LLM压缩方法以实现实际部署。奇异值分解（SVD）为LLM压缩提供了一个有希望的解决方案。然而，现有的基于SVD的LLM压缩方法存在两个关键限制：截断较小的奇异值可能导致更高的压缩损失，并且在SVD截断后剩余模型参数的更新缺失。在这项工作中，我们提出了SVD-LLM，一种新的基于SVD的LLM压缩方法，解决了现有方法的限制。SVD-LLM采用了一种截断感知的数据白化策略，以确保奇异值和压缩损失之间的直接映射。此外，SVD-LLM采用一种逐层闭式模型参数更新策略，以弥补SVD截断引起的准确性降低。我们在总共11个数据集和七个m上评估了SVD-LLM。

    arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
    
[^229]: Mamba模型的隐藏关注

    The Hidden Attention of Mamba Models

    [https://arxiv.org/abs/2403.01590](https://arxiv.org/abs/2403.01590)

    Mamba模型可以被视为关注驱动的模型，这与变压器中的自注意力层有所不同，并且通过可解释性方法可以深入了解其内部工作。

    

    Mamba层提供了一种高效的选择性状态空间模型(SSM)，在建模多个领域包括NLP、长距离序列处理和计算机视觉方面非常有效。选择性SSMs被视为双重模型，其中一个通过IO-aware并行扫描在整个序列上进行并行训练，并以自回归方式部署。我们添加了第三个视角，并展示这样的模型可以被视为关注驱动的模型。这一新视角使我们能够将底层机制与变压器中的自注意力层进行比较，并让我们通过可解释性方法窥探Mamba模型的内部工作。我们的代码可公开获取。

    arXiv:2403.01590v1 Announce Type: new  Abstract: The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains including NLP, long-range sequences processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.
    
[^230]: 采用多样化的合作行为和对抗样式采样提高多智能体强化学习中政策的鲁棒性用于辅助任务

    Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks

    [https://arxiv.org/abs/2403.00344](https://arxiv.org/abs/2403.00344)

    提出了一个框架，通过训练适应多样化护理接收者响应的鲁棒护理人员政策，以提高多智能体强化学习中政策的鲁棒性。

    

    个人携带动作障碍者的自主协助是自主机器人系统最有前景的应用之一。最近的研究报道了在医疗领域使用深度强化学习（RL）取得了令人鼓舞的结果。先前的研究表明，辅助任务可以被制定为多智能体强化学习，其中有两个智能体：护理人员和护理接收者。然而，多智能体强化学习训练的政策往往对其他智能体的政策敏感。在这种情况下，训练有素的护理人员政策可能不适用于不同的护理接收者。为了缓解这个问题，我们提出了一个通过训练适应多样化护理接收者响应的鲁棒护理人员政策的框架。在我们的框架中，多样化的护理接收者响应是通过试验和错误自主学习的。此外，为了增强护理者政策的鲁棒性，我们提出了一种对护理接收者响应进行对抗采样的策略。

    arXiv:2403.00344v1 Announce Type: cross  Abstract: Autonomous assistance of people with motor impairments is one of the most promising applications of autonomous robotic systems. Recent studies have reported encouraging results using deep reinforcement learning (RL) in the healthcare domain. Previous studies showed that assistive tasks can be formulated as multi-agent RL, wherein there are two agents: a caregiver and a care-receiver. However, policies trained in multi-agent RL are often sensitive to the policies of other agents. In such a case, a trained caregiver's policy may not work for different care-receivers. To alleviate this issue, we propose a framework that learns a robust caregiver's policy by training it for diverse care-receiver responses. In our framework, diverse care-receiver responses are autonomously learned through trials and errors. In addition, to robustify the care-giver's policy, we propose a strategy for sampling a care-receiver's response in an adversarial mann
    
[^231]: Q-FOX学习：颠覆传统的强化学习

    Q-FOX Learning: Breaking Tradition in Reinforcement Learning

    [https://arxiv.org/abs/2402.16562](https://arxiv.org/abs/2402.16562)

    Q-FOX学习是一种新颖的自动超参数调整方法，结合了FOX优化器和Q-learning算法，提出了使用新的目标函数来解决强化学习中超参数调整的问题。

    

    强化学习（RL）是人工智能（AI）的一个子集，代理通过与环境的交互来学习最佳动作，因此适用于不需要标记数据或直接监督的任务。 本文提出了一种名为Q-FOX的新颖自动调参方法，该方法使用了FOX优化器和常用的易于实现的RL Q-learning算法解决了调参的问题。此外，还提出了一个新的目标函数，该函数将奖励放在均方误差（MSE）和学习时间之上。

    arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
    
[^232]: 逻辑回归的可证实准确性随机抽样算法

    A Provably Accurate Randomized Sampling Algorithm for Logistic Regression

    [https://arxiv.org/abs/2402.16326](https://arxiv.org/abs/2402.16326)

    提出了一种逻辑回归问题的简单随机抽样算法，通过随机矩阵乘法实现高质量逼近估计概率和模型整体差异性。

    

    在统计学和机器学习中，逻辑回归是一种广泛应用于二分类任务的监督学习技术。当观测数量远远超过预测变量数量时，我们提出了一种简单的基于随机抽样的逻辑回归问题算法，保证高质量逼近估计概率和模型整体差异性。我们的分析建立在两个简单的结构条件基础上，这两个条件可归结为随机矩阵乘法，是随机化数值线性代数的基本且深入理解的基元。当利用杠杆分数对观测进行抽样时，我们分析了逻辑回归的估计概率属性，并证明准确逼近可以通过远小于总观测数的样本实现。为了进一步验证我们的理论发现，

    arXiv:2402.16326v1 Announce Type: cross  Abstract: In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findi
    
[^233]: ArEEG_Chars: 用于基于脑电图的设想语音识别的阿拉伯字符数据集

    ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters

    [https://arxiv.org/abs/2402.15733](https://arxiv.org/abs/2402.15733)

    该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。

    

    脑机接口（BCI）是近年来热门的研究课题，可以帮助瘫痪患者改善生活。有几项研究自动将脑电图（EEG）信号分类为英文字符和单词。阿拉伯语是世界上使用最广泛的语言之一。然而据我们所知，目前没有针对阿拉伯字符的脑电图信号数据集。在本文中，我们创建了一个用于阿拉伯字符的EEG数据集，并命名为ArEEG_Chars。此外，我们使用深度学习对ArEEG_Chars进行了多项实验。在使用LSTM时获得了最佳结果，准确率达到97%。ArEEG_Chars数据集将对研究人员公开。

    arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
    
[^234]: 学习离散动力系统的拓扑结构和行为

    Learning the Topology and Behavior of Discrete Dynamical Systems

    [https://arxiv.org/abs/2402.11686](https://arxiv.org/abs/2402.11686)

    本文研究了学习黑盒离散动力系统的行为和底层拓扑结构的问题，证明了这是一个计算上难以解决的问题，并提出了在某些条件下的高效学习方法。

    

    离散动力系统通常用于模拟真实世界网络上传染的传播。现有研究已经在PAC框架下研究了在已知底层网络的条件下学习系统行为的问题。本文着眼于一个更具挑战性的情境：学习一个黑盒系统的行为和底层拓扑结构。我们证明，一般情况下，这个学习问题在计算上是难以解决的。在积极的一面，我们在PAC模型下针对某些类属于的动力系统图提出了高效的学习方法。此外，我们考虑了一个放松的情境，其中未知系统的拓扑结构部分可观测。针对这种情况，我们开发了一个高效的PAC学习器来推断系统并确定样本复杂性。最后，我们对动力系统的假设类的表达能力进行了形式化分析，其中包含了行为和拓扑结构。

    arXiv:2402.11686v1 Announce Type: new  Abstract: Discrete dynamical systems are commonly used to model the spread of contagions on real-world networks. Under the PAC framework, existing research has studied the problem of learning the behavior of a system, assuming that the underlying network is known. In this work, we focus on a more challenging setting: to learn both the behavior and the underlying topology of a black-box system. We show that, in general, this learning problem is computationally intractable. On the positive side, we present efficient learning methods under the PAC model when the underlying graph of the dynamical system belongs to some classes. Further, we examine a relaxed setting where the topology of an unknown system is partially observed. For this case, we develop an efficient PAC learner to infer the system and establish the sample complexity. Lastly, we present a formal analysis of the expressive power of the hypothesis class of dynamical systems where both the
    
[^235]: RS-DPO：一种用于对齐大型语言模型的混合拒绝采样和直接优化偏好的方法

    RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models

    [https://arxiv.org/abs/2402.10038](https://arxiv.org/abs/2402.10038)

    本研究提出了一种名为RS-DPO的方法，它将拒绝采样和直接优化偏好结合起来，用于对齐大型语言模型。通过开发一个经过监督微调的策略模型，并从该模型中直接采样响应，RS-DPO能够有效解决基于近端策略优化的不稳定性和高计算成本的问题。通过识别对比样本对，RS-DPO能够更好地进行RLHF。

    

    强化学习从人类反馈中学习（RLHF）已被广泛应用于将大型语言模型与用户意图对齐。然而，基于近端策略优化（PPO）的RLHF有时不稳定，需要显著的超参数微调，并且在对齐过程中计算成本高昂。最近，提出了直接优化偏好（DPO）来解决这些挑战。然而，DPO依赖于从人类标注者和替代LLM生成的对比回复，而不是策略模型，限制了RLHF的效果。本文通过系统地结合拒绝采样（RS）和DPO来解决这两个挑战。我们提出的方法RS-DPO，首先开发出一个经过监督微调的策略模型（SFT）。然后直接从SFT模型中采样每个提示的k个响应。RS-DPO基于其相似度识别对比样本对。

    arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
    
[^236]: 用于具有二次奖励的强化学习的稳态误差补偿

    Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards

    [https://arxiv.org/abs/2402.09075](https://arxiv.org/abs/2402.09075)

    该论文研究提出了一种使用积分项补偿二次奖励函数稳态误差的方法，通过增强长期奖励的考虑，有效降低了系统性能（如自适应巡航控制和变道模型）中的稳态误差问题。

    

    强化学习中的奖励函数选择对系统性能的影响引起了广泛关注。当使用二次奖励函数时，经常会出现稳态误差的问题。尽管已有的使用绝对值类型奖励函数的解决方案在一定程度上解决了这个问题，但往往会在特定系统状态下引起较大波动，导致突然的变化。为了应对这一挑战，本研究提出了一种引入积分项的方法。通过将这个项积分进二次奖励函数中，对RL算法进行精确调整，增强系统对长期奖励的考虑，从而缓解稳态误差的问题。通过在自适应巡航控制（ACC）模型和变道模型上的实验和性能评估，我们验证了提出的方法不仅有效地减小了稳态误差，还提高了系统的性能表现。

    arXiv:2402.09075v1 Announce Type: cross Abstract: The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes st
    
[^237]: 动态系统中的实验设计的嵌套粒子滤波器

    Nesting Particle Filters for Experimental Design in Dynamical Systems

    [https://arxiv.org/abs/2402.07868](https://arxiv.org/abs/2402.07868)

    本文提出了一种新颖的方法来解决动态系统中的贝叶斯实验设计问题，利用嵌套粒子滤波器和立体蒙特卡洛方法来进行基于梯度的策略优化，相比于其他方法具有更好的性能。

    

    本文提出了一种新颖的贝叶斯实验设计方法，用于非交换数据，并将其形式化为风险敏感的策略优化。我们开发了内外SMC^2算法，使用嵌套顺序蒙特卡洛（SMC）估计器来预测期望的信息增益，并将其嵌入到粒子马尔可夫链蒙特卡洛（pMCMC）框架中进行基于梯度的策略优化。与最近依赖于偏估计器来摊销先前学习设计策略的成本的方法相比，我们的方法具有更好的性能。在一组动态系统的数值验证中展示了我们方法的有效性。

    In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.
    
[^238]: 基于CUDA的GPU优化稀疏卷积在3D点云上的应用

    Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA

    [https://arxiv.org/abs/2402.07710](https://arxiv.org/abs/2402.07710)

    本论文研究了在3D点云上进行稀疏卷积的GPU优化方法，以解决点云的稀疏性和计算问题。

    

    最近几年，深度学习方法的应用显著增加，特别是卷积神经网络（CNN），它们已经成为涉及结构化格网数据的各个领域中的主要方法，如图像分析和处理。然而，随着LiDAR和3D传感器在许多领域的使用呈指数增长，对3D点云的分析需求也增加了。利用3D点云在包括物体识别和分割在内的各种应用中至关重要，因为它们提供了三维环境中事物的空间描述。与照片不同，点云具有稀疏性和缺乏规则的格网，因此存在着独特的处理和计算问题。

    In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.
    
[^239]: 相对绩效标准下的异质代理人最优投资的深度学习方法

    A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents

    [https://arxiv.org/abs/2402.07365](https://arxiv.org/abs/2402.07365)

    本文提出了一种基于深度学习的方法，用于相对绩效标准下的最优投资。该方法利用前向-后向随机微分方程的纳什均衡特征和随机微分游戏的机器学习算法的最新进展。数值实验在两个不同的金融模型上进行，比较了不同交互结构的图形图的影响。

    

    引入了Graphon游戏来研究通过一张交互的加权图的许多玩家的游戏。通过取极限，得到了一个具有连续玩家的游戏，其中的交互是通过一个图形图。本文关注相对绩效标准下的图形游戏的最优投资，并提出了一种深度学习方法。该方法建立在两个关键要素的基础上：首先，通过前向-后向随机微分方程对纳什均衡进行了表征，其次，利用随机微分游戏的机器学习算法的最新进展。我们对两个不同的金融模型进行了数值实验，在每个模型中比较了几种不同的图形图对交互结构的影响。

    Graphon games have been introduced to study games with many players who interact through a weighted graph of interaction. By passing to the limit, a game with a continuum of players is obtained, in which the interactions are through a graphon. In this paper, we focus on a graphon game for optimal investment under relative performance criteria, and we propose a deep learning method. The method builds upon two key ingredients: first, a characterization of Nash equilibria by forward-backward stochastic differential equations and, second, recent advances of machine learning algorithms for stochastic differential games. We provide numerical experiments on two different financial models. In each model, we compare the effect of several graphons, which correspond to different structures of interactions.
    
[^240]: X-LoRA: 一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略在蛋白质力学和设计中的应用

    X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design

    [https://arxiv.org/abs/2402.07148](https://arxiv.org/abs/2402.07148)

    X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。

    

    我们报道了一种使用深层逐层基于低秩适应（LoRA）的新颖预训练适配器的混合专家策略，用于创建精细调整的大型语言模型。我们提出了一种利用隐藏状态动态混合经过适应的层的门控策略，允许得到的X-LoRA模型利用不同的能力并创建以前未使用的深层逐层适应的组合来解决特定任务。该设计受到了生物普遍性和多样性的生物学原理的启发，其中神经网络建模块在不同的分层表示中被重复使用。因此，X-LoRA模型可以轻松用于任何现有的大型语言模型（LLM），无需修改底层结构。我们还开发了一个定制的X-LoRA模型，提供了包括前向/逆向分析任务和增强推理能力在内的科学能力，重点是生物材料分析。

    We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
    
[^241]: TinyLLM: 从多个大型语言模型学习一个小型学生模型

    TinyLLM: Learning a Small Student from Multiple Large Language Models

    [https://arxiv.org/abs/2402.04616](https://arxiv.org/abs/2402.04616)

    TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。

    

    将更强大的大型语言模型（LLMs）的推理能力转移到较小的模型上具有吸引力，因为较小的LLMs更灵活，成本更低。在现有的解决方案中，知识蒸馏因其出色的效率和泛化能力而脱颖而出。然而，现有方法存在一些缺点，包括知识多样性有限和缺乏丰富的上下文信息。为了解决这些问题并促进紧凑语言模型的学习，我们提出了TinyLLM，一种从多个大型教师LLMs中学习小型学生LLM的新型知识蒸馏范式。特别地，我们鼓励学生LLM不仅生成正确答案，而且理解这些答案背后的原理。鉴于不同的LLMs具有不同的推理能力，我们引导学生模型吸收来自多个教师LLMs的知识。我们进一步引入了一个上下文示例生成器和一个老师强制模块...

    Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
    
[^242]: 《读玩游戏（R2-Play）: 多模态游戏指导下的决策 Transformer》

    Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction

    [https://arxiv.org/abs/2402.04154](https://arxiv.org/abs/2402.04154)

    本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示)

    

    在人工智能领域，开发一款通用智能体一直是一个长期的目标。先前的研究利用来自各种任务的大量离线数据集，在强化学习的多任务场景中表现出了出色的性能。然而，这些工作在扩展到新任务方面面临挑战。最近的方法将文本指导或视觉轨迹整合到决策网络中，提供任务特定的上下文提示，代表了一个有前途的方向。然而，观察到仅依赖于文本指导或视觉轨迹对于准确传达任务的上下文信息是不足够的。本文探索了增强智能体任务指导的形式，使其能够理解游戏指导，从而实现"读玩游戏"的能力。受到多模态指导调优在视觉任务中的成功启发，我们将基于视觉的强化学习任务视为一个长期视觉任务，并构建了一组... (内容太长，无法继续显示)

    Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
    
[^243]: 因果贝叶斯优化通过外源分布学习

    Causal Bayesian Optimization via Exogenous Distribution Learning

    [https://arxiv.org/abs/2402.02277](https://arxiv.org/abs/2402.02277)

    本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。

    

    在结构化因果模型中，将目标变量最大化作为操作目标是一个重要的问题。现有的因果贝叶斯优化（CBO）方法要么依赖于改变因果结构以最大化奖励的硬干预，要么引入动作节点到内生变量中，以调整数据生成机制以实现目标。本文引入了一种新的方法来学习外源变量的分布，这在现有方法中通常被忽略或通过期望进行边缘化。外源分布学习提高了通常通过有限观测数据训练的代理模型中的结构化因果模型的近似精度。此外，学习到的外源分布将现有的CBO扩展到超出加性噪声模型（ANM）的一般因果方案。恢复外源变量使我们能够为噪声或未观测到的隐藏变量使用更灵活的先验。引入了一种新的CBO方法。

    Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
    
[^244]: 使用大型多模态模型解释生成模型的潜在表示

    Explaining latent representations of generative models with large multimodal models

    [https://arxiv.org/abs/2402.01858](https://arxiv.org/abs/2402.01858)

    该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    

    学习可解释的数据生成潜在因素表示是人工智能发展的重要课题。随着大型多模态模型的兴起，它可以将图像与文本对齐以生成答案。在本研究中，我们提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。我们进一步量化评估了我们生成的解释的不确定性，在多个大型多模态模型之间评估了解释生成的性能，并定性地可视化了每个潜在因素的变化，以学习不同生成模型对解释的解缠效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
    
[^245]: 简单策略优化

    Simple Policy Optimization

    [https://arxiv.org/abs/2401.16025](https://arxiv.org/abs/2401.16025)

    SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。

    

    PPO（Proximal Policy Optimization）算法在许多领域表现出色，被认为是TRPO（Trust Region Policy Optimization）算法的简化版本。然而，PPO中的比率剪切操作并不总是有效地强制执行信任区域约束，这可能会影响算法的稳定性。本文提出了一种新颖的剪切方法，即Simple Policy Optimization（SPO）算法，用于旧策略和当前策略之间的KL散度。在Atari 2600环境中进行的大量实验结果表明，与PPO的主流变体相比，SPO实现了更好的样本效率，极低的KL散度和更高的策略熵，并且对网络深度或复杂度的增加具有鲁棒性。更重要的是，SPO保持了无约束一阶算法的简单性。

    arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co
    
[^246]: 用于深度神经网络背景偏差缓解的更快ISNet

    Faster ISNet for Background Bias Mitigation on Deep Neural Networks

    [https://arxiv.org/abs/2401.08409](https://arxiv.org/abs/2401.08409)

    提出了一种用于缓解深度神经网络中背景偏见的更快ISNet，通过重新构建架构以减少训练时间并引入一种模型无关的LRP实现，成功阻止了背景注意力和快捷学习，优于多个最新模型。

    

    图像背景中的偏见或伪相关性可能会影响神经网络，导致快捷学习（聪明的汉斯效应）并阻碍对真实世界数据的泛化。最近引入的ISNet架构提出了优化逐层相关传播（LRP，一种解释技术）热图，以减轻背景对深度分类器的影响。然而，ISNet的训练时间与应用程序中的类数量成线性关系。在这里，我们提出了经重新构建的架构，其训练时间不再依赖于这个数量。此外，我们引入了一种简明且与模型无关的LRP实现。我们利用合成背景偏见和胸部X光中的COVID-19检测挑战所提出的架构，该应用程序通常存在背景偏见。这些网络阻碍了背景注意力和快捷学习，超越了多个最新模型。

    arXiv:2401.08409v2 Announce Type: replace-cross  Abstract: Bias or spurious correlations in image backgrounds can impact neural networks, causing shortcut learning (Clever Hans Effect) and hampering generalization to real-world data. ISNet, a recently introduced architecture, proposed the optimization of Layer-Wise Relevance Propagation (LRP, an explanation technique) heatmaps, to mitigate the influence of backgrounds on deep classifiers. However, ISNet's training time scales linearly with the number of classes in an application. Here, we propose reformulated architectures whose training time becomes independent from this number. Additionally, we introduce a concise and model-agnostic LRP implementation. We challenge the proposed architectures using synthetic background bias, and COVID-19 detection in chest X-rays, an application that commonly presents background bias. The networks hindered background attention and shortcut learning, surpassing multiple state-of-the-art models on out-o
    
[^247]: 因果状态精炼用于可解释强化学习

    Causal State Distillation for Explainable Reinforcement Learning

    [https://arxiv.org/abs/2401.00104](https://arxiv.org/abs/2401.00104)

    本文提出了一种因果状态精炼的方法，通过揭示在训练过程中影响智能体目标的奖励的各个方面，解决了强化学习模型不透明性的问题。

    

    强化学习（RL）是训练智能体的强大技术，但理解这些智能体为何做出特定决策可能非常具有挑战性。RL模型的不透明性一直是一个长期存在的问题，使用户难以理解智能体行为背后的原因。本文提出了一种基于因果状态精炼（Causal State Distillation）的方法，旨在克服奖励分解等其他方法所带来的问题，该方法在训练过程中揭示出对智能体目标产生影响的奖励的各个方面。

    arXiv:2401.00104v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an e
    
[^248]: 针对每个任务样本数量很少的元学习

    Metalearning with Very Few Samples Per Task

    [https://arxiv.org/abs/2312.13978](https://arxiv.org/abs/2312.13978)

    在针对每个任务样本数量很少的情况下，研究了如何通过共享表示进行元学习。

    

    Metalearning和多任务学习是两种解决相关学习任务组的框架，能够比我们单独解决每个任务更高效。 在多任务学习中，我们被给定一组相关学习任务，并需要为每个任务输出一个准确的模型；而在元学习中，我们被给定从元分布中独立同分布绘制的任务，并需要输出一些可以很容易地专门用于元分布中新任务的公共信息。 我们考虑一个二分类设置，任务是由共享表示相关联的，也就是说，每个任务$P$可以通过形式为$f_{P}\circ h$的分类器解决，其中$h \in H$是从特征到共享表示空间的映射，而$f_{P} \in F$是从表示空间到标签的任务特定分类器。 我们主要探讨的问题是我们需要多少数据来元学习

    arXiv:2312.13978v2 Announce Type: replace  Abstract: Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own. In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new tasks from the metadistribution.   We consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ can be solved by a classifier of the form $f_{P} \circ h$ where $h \in H$ is a map from features to a representation space that is shared across tasks, and $f_{P} \in F$ is a task-specific classifier from the representation space to labels. The main question we ask is how much data do we need to metalearn a go
    
[^249]: 使用1中心和1均值聚类处理异常值的分布式学习中近似最优的鲁棒聚合规则

    Near-Optimal Resilient Aggregation Rules for Distributed Learning Using 1-Center and 1-Mean Clustering with Outliers

    [https://arxiv.org/abs/2312.12835](https://arxiv.org/abs/2312.12835)

    本文研究了在分布式学习中使用聚类方法处理异常值的近似最优鲁棒聚合规则，通过恒定近似1中心和1均值聚类问题解决方案，提供了重要的度量标准优化方法

    

    近年来，拜占庭机器学习在大规模分布式学习系统中引起了人们的极大关注，因为不可预知的故障可能会导致系统崩溃。在分布式学习中，实现对抗拜占庭机器的安全鲁棒性的关键在于鲁棒聚合机制。本文研究了在存在异常值的情况下使用聚类的近似最优聚合规则。我们提出的抗异常值的聚类方法利用了工作者提供的更新向量的几何属性。我们的分析表明，具有异常值的1中心和1均值聚类问题的恒定近似解提供了在度量标准上近似最优的鲁棒聚合器，这在均匀和非均匀情况下都被证明是至关重要的。

    arXiv:2312.12835v2 Announce Type: replace  Abstract: Byzantine machine learning has garnered considerable attention in light of the unpredictable faults that can occur in large-scale distributed learning systems. The key to secure resilience against Byzantine machines in distributed learning is resilient aggregation mechanisms. Although abundant resilient aggregation rules have been proposed, they are designed in ad-hoc manners, imposing extra barriers on comparing, analyzing, and improving the rules across performance criteria. This paper studies near-optimal aggregation rules using clustering in the presence of outliers. Our outlier-robust clustering approach utilizes geometric properties of the update vectors provided by workers. Our analysis show that constant approximations to the 1-center and 1-mean clustering problems with outliers provide near-optimal resilient aggregators for metric-based criteria, which have been proven to be crucial in the homogeneous and heterogeneous cases
    
[^250]: 基于自然语言处理的肌肉骨骼疾病风险因素分类与基于模式的排名

    A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors

    [https://arxiv.org/abs/2312.11517](https://arxiv.org/abs/2312.11517)

    本研究利用自然语言处理和基于模式的排名方法对肌肉骨骼疾病的风险因素进行了分类和排名，以提高对其理解、分类和优先考虑预防和治疗的能力。

    

    本研究探讨了肌肉骨骼疾病（MSD）风险因素，使用自然语言处理（NLP）和基于模式的排名相结合。旨在精细化理解、分类和优先考虑针对性预防和治疗。评估了八个NLP模型，结合预训练的转换器、余弦相似度和距离度量将因素分类为个人、生物力学、工作场所、心理和组织等类别。BERT与余弦相似度达到28%的准确率；句子转换器与欧氏、布雷曲蒂斯和闵可夫斯基距离得分为100%。通过10倍交叉验证，统计检验确保鲁棒结果。调查数据和基于模式的排名确定了严重性等级，与文献相一致。"工作姿势"是最严重的，凸显了姿势的作用。调查结果强调了"工作不稳定性"、"工作努力和回报不平衡"和"员工设施差"等因素的显著性。

    arXiv:2312.11517v3 Announce Type: replace  Abstract: This research delves into Musculoskeletal Disorder (MSD) risk factors, using a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is to refine understanding, classification, and prioritization for focused prevention and treatment. Eight NLP models are evaluated, combining pre-trained transformers, cosine similarity, and distance metrics to categorize factors into personal, biomechanical, workplace, psychological, and organizational classes. BERT with cosine similarity achieves 28% accuracy; sentence transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%. With 10-fold cross-validation, statistical tests ensure robust results. Survey data and mode-based ranking determine severity hierarchy, aligning with the literature. "Working posture" is the most severe, highlighting posture's role. Survey insights emphasize "Job insecurity," "Effort reward imbalance," and "Poor employee facility" as sig
    
[^251]: ComplexityNet: 通过学习任务复杂性提高LLM推理效率

    ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity

    [https://arxiv.org/abs/2312.11511](https://arxiv.org/abs/2312.11511)

    ComplexityNet通过学习任务复杂性，提高了LLM推理效率，通过预测任务的准确输出概率，成功降低了90%的计算资源使用，并在任务复杂性确定方面取得了79%准确率。

    

    我们提出了ComplexityNet，这是一个专为评估任务复杂性而设计的简化语言模型。该模型通过不同能力的各种语言模型来预测准确输出的可能性。我们首次在Mostly Basic Python Problems（MBPP）数据集上应用了ComplexityNet。我们开创性地创建了第一组标签来定义任务复杂性。ComplexityNet在确定任务复杂性方面取得了显著的79%准确率，较原始、非微调模型的34%准确率有了显著改进。此外，与使用最高复杂性模型相比，ComplexityNet有效地减少了90%的计算资源使用，同时保持了86.7%的高代码生成准确率。这项研究表明，通过微调较小的模型来对任务进行分类，可以在准确性和效率之间取得更平衡的权衡。

    arXiv:2312.11511v2 Announce Type: replace-cross  Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Lan
    
[^252]: 协作基础模型用于领域泛化的语义分割

    Collaborating Foundation Models for Domain Generalized Semantic Segmentation

    [https://arxiv.org/abs/2312.09788](https://arxiv.org/abs/2312.09788)

    提出了一种协作基础模型框架（CLOUDS）用于领域泛化语义分割，集成了CLIP骨干、生成模型和“Segment Anything Model”，以实现对各种目标分布模式的覆盖和预测改进。

    

    领域泛化语义分割（DGSS）涉及在标记的源域上训练模型，旨在在推断过程中推广到未见过的领域。现有的DGSS方法通常通过域随机化（DR）实现强健特征。这种方法通常存在局限性，因为它只能考虑样式多样性而不是内容。在这项工作中，我们采用了一种正交的DGSS方法，即利用一系列协作基础模型用于领域泛化的语义分割（CLOUDS）。具体而言，CLOUDS是一个集成了各种基础模型的框架：（i）CLIP骨干用于其强健特征表示，（ii）生成模型用于使内容多样化，从而涵盖可能目标分布的各种模式，以及（iii）使用“Segment Anything Model”（SAM）迭代地改进分割模型预测。大量实验表明，我们的CLOUDS表现突出。

    arXiv:2312.09788v2 Announce Type: replace-cross  Abstract: Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during inference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP backbone for its robust feature representation, (ii) generative models to diversify the content, thereby covering various modes of the possible target distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS excels
    
[^253]: 使用大语言模型为Minecraft自动设计稠密奖励的Auto MC-Reward

    Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft

    [https://arxiv.org/abs/2312.09238](https://arxiv.org/abs/2312.09238)

    本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型自动设计稠密奖励函数来提高学习效率

    

    许多强化学习环境（例如Minecraft）仅提供指示任务完成或失败的稀疏奖励，这些奖励以二进制值表示。在这种环境中探索效率的挑战使得基于强化学习的代理程序难以学习复杂任务。为了解决这个问题，本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型（LLMs）自动设计稠密奖励函数，从而提高学习效率。Auto MC-Reward包括三个重要组件：奖励设计者、奖励评论家和轨迹分析器。给定环境信息和任务描述，奖励设计者首先通过编写可执行的Python函数和预定义的观测输入来设计奖励函数。然后，我们的奖励评论家将负责验证代码，检查代码是否自洽且无语法错误。

    arXiv:2312.09238v2 Announce Type: replace  Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax 
    
[^254]: OCTDL：基于图像的深度学习方法的光学相干断层扫描数据集

    OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods

    [https://arxiv.org/abs/2312.08255](https://arxiv.org/abs/2312.08255)

    该研究介绍了一个名为OCTDL的开放获取光学相干断层扫描数据集，包括超过2000张标记有疾病组和视网膜病理的OCT图像，有助于诊断眼部状况。

    

    光学相干断层扫描（OCT）是一种非侵入性成像技术，在眼科学中具有广泛的临床应用。OCT可以可视化视网膜层，对早期检测和监测视网膜疾病起着重要作用。本文介绍了一个开放获取的OCT数据集（OCTDL），包括超过2000张根据疾病组和视网膜病理标记的OCT图像。该数据集包括患有老年性黄斑变性（AMD）、糖尿病黄斑水肿（DME）、玻璃体视网膜膜（ERM）、视网膜动脉闭塞（RAO）、视网膜静脉闭塞（RVO）和玻璃体黄斑界面疾病（VID）的患者的OCT记录。这些图像是使用Optovue Avanti RTVue XR采集的，采用了动态扫描长度的光栅扫描协议。

    arXiv:2312.08255v2 Announce Type: replace-cross  Abstract: Optical coherence tomography (OCT) is a non-invasive imaging technique with extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. This work presents an open-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology. The dataset consists of OCT records of patients with Age-related Macular Degeneration (AMD), Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The images were acquired with an Optovue Avanti RTVue XR using raster scanning protocols with dynamic scan length a
    
[^255]: 蜜蜂：多模态LLM的增强局部投影仪

    Honeybee: Locality-enhanced Projector for Multimodal LLM

    [https://arxiv.org/abs/2312.06742](https://arxiv.org/abs/2312.06742)

    该研究提出了一种既灵活又增强局部性的新型投影仪设计，有助于连接视觉编码器和语言模型，提高模型的效率和空间理解。

    

    在多模态大型语言模型（MLLMs）中，视觉投影仪在连接预先训练的视觉编码器与LLMs之间发挥着至关重要的作用，实现深入的视觉理解并利用LLMs的强大能力。尽管视觉投影仪的重要性不言而喻，但研究相对较少。在这项研究中，我们首先确定了两个关键的投影仪属性：（i）灵活性以管理视觉代币的数量，对于MLLMs的整体效率至关重要；（ii）保留来自视觉特征的局部上下文，对于空间理解至关重要。基于这些发现，我们提出了一种既灵活又增强局部性的新型投影仪设计，有效地满足了这两种理想属性。此外，我们提出了全面的策略，以有效利用多个和多方面的指导数据集。通过广泛的实验，我们检验了各种设计选择的影响。

    arXiv:2312.06742v2 Announce Type: replace-cross  Abstract: In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, o
    
[^256]: 所有的河流都汇聚到大海：具有不对称流量的私有学习

    All Rivers Run to the Sea: Private Learning with Asymmetric Flows

    [https://arxiv.org/abs/2312.05264](https://arxiv.org/abs/2312.05264)

    提出了一种新的私有训练和推理框架Delta，通过两个不对称数据流实现了具有可比较模型性能的隐私保护。

    

    数据隐私在云机器学习服务平台中备受关注，当敏感数据暴露给服务提供商时。为了在保护隐私的同时实现高性能计算，我们提出了一种新的私有训练和推理框架Delta，具有与非私有集中训练相当的模型性能。Delta具有两个不对称的数据流：主要的信息敏感流和残差流。主要部分流入一个小模型，而残余部分则被转移到一个大模型。具体来说，Delta将信息敏感表示嵌入到低维空间中，同时将信息不敏感部分推入高维残差中。

    arXiv:2312.05264v2 Announce Type: replace-cross  Abstract: Data privacy is of great concern in cloud machine-learning service platforms, when sensitive data are exposed to service providers. While private computing environments (e.g., secure enclaves), and cryptographic approaches (e.g., homomorphic encryption) provide strong privacy protection, their computing performance still falls short compared to cloud GPUs. To achieve privacy protection with high computing performance, we propose Delta, a new private training and inference framework, with comparable model performance as non-private centralized training. Delta features two asymmetric data flows: the main information-sensitive flow and the residual flow. The main part flows into a small model while the residuals are offloaded to a large model. Specifically, Delta embeds the information-sensitive representations into a low-dimensional space while pushing the information-insensitive part into high-dimension residuals. To ensure priv
    
[^257]: 基于Transformer的深度学习模型在曼谷地基中用于桩载荷变形预测

    Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil

    [https://arxiv.org/abs/2312.03041](https://arxiv.org/abs/2312.03041)

    基于Transformer架构的深度学习模型用于曼谷地基灌注桩的载荷变形预测，在考虑土壤剖面和桩特征的基础上，结合之前顺序数据以提高预测准确性，可满意地预测载荷-变形曲线，并可用于不同桩条件下的参数分析和设计优化。

    

    本文提出了一种基于Transformer架构的新型深度学习模型，用于预测曼谷地基中大型灌注桩的载荷变形行为。该模型将土壤剖面和桩的特征编码为tokenization输入，并生成载荷变形曲线作为输出。模型还将之前的载荷-变形曲线顺序数据编码进解码器中，以提高预测准确性。该模型展现了对载荷-变形曲线预测的满意准确性和泛化能力，在测试数据中的平均绝对误差为5.72%。该模型还可用于不同土壤和桩条件下，桩横截面、桩长度和桩类型的参数分析和设计优化。

    arXiv:2312.03041v2 Announce Type: replace  Abstract: This paper presents a novel deep learning model based on the transformer architecture to predict the load-deformation behavior of large bored piles in Bangkok subsoil. The model encodes the soil profile and pile features as tokenization input, and generates the load-deformation curve as output. The model also incorporates the previous sequential data of load-deformation curve into the decoder to improve the prediction accuracy. The model also incorporates the previous sequential data of load-deformation curve into the decoder. The model shows a satisfactory accuracy and generalization ability for the load-deformation curve prediction, with a mean absolute error of 5.72% for the test data. The model could also be used for parametric analysis and design optimization of piles under different soil and pile conditions, pile cross section, pile length and type of pile.
    
[^258]: 通过协作学习提高在线持续学习中的可塑性

    Improving Plasticity in Online Continual Learning via Collaborative Learning

    [https://arxiv.org/abs/2312.00600](https://arxiv.org/abs/2312.00600)

    在线持续学习中，为了提高模型的可塑性，我们提出了协作持续学习（CCL）和蒸馏链（DC）策略。

    

    在线持续学习解决了从连续数据流中学习不断出现的新分类任务的问题。本文认为，在线持续学习中，模型获取新知识的能力（即模型的可塑性）是另一个挑战。我们提出了协作持续学习（CCL），一个基于协作学习的策略，以提高模型在获取新概念方面的能力。此外，我们引入了蒸馏链（DC），一种协作学习方案，以提升模型的训练。

    arXiv:2312.00600v2 Announce Type: replace  Abstract: Online Continual Learning (CL) solves the problem of learning the ever-emerging new classification tasks from a continuous data stream. Unlike its offline counterpart, in online CL, the training data can only be seen once. Most existing online CL research regards catastrophic forgetting (i.e., model stability) as almost the only challenge. In this paper, we argue that the model's capability to acquire new knowledge (i.e., model plasticity) is another challenge in online CL. While replay-based strategies have been shown to be effective in alleviating catastrophic forgetting, there is a notable gap in research attention toward improving model plasticity. To this end, we propose Collaborative Continual Learning (CCL), a collaborative learning based strategy to improve the model's capability in acquiring new concepts. Additionally, we introduce Distillation Chain (DC), a collaborative learning scheme to boost the training of the models. 
    
[^259]: 自动化持续学习

    Automating Continual Learning

    [https://arxiv.org/abs/2312.00276](https://arxiv.org/abs/2312.00276)

    提出了自动化持续学习（ACL）来训练自引用神经网络，使其元学习自身的上下文持续学习算法，并在解决“上下文灾难性遗忘”方面取得成功。

    

    一般用途的学习系统应在不断变化的环境中以开放式方式不断改进自身。然而，神经网络的传统学习算法常常遭受灾难性遗忘（CF）-当学习新任务时，先前获得的技能被遗忘。我们提出了自动化持续学习（ACL）来训练自引用神经网络，使其元学习自身的上下文持续(meta-)学习算法。ACL将所有期望表现良好于新旧任务的要求编码到其元学习目标中。我们的实验表明ACL有效地解决了“上下文灾难性遗忘”; 我们通过ACL学到的算法在无重放设置下优于手工制定的算法，例如在Split-MNIST基准测试中，并且可以持续学习由多个少量示例和标准图像分类数据组成的各种任务。

    arXiv:2312.00276v2 Announce Type: replace  Abstract: General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF) -- previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives. Our experiments demonstrate that ACL effectively solves "in-context catastrophic forgetting"; our ACL-learned algorithms outperform hand-crafted ones, e.g., on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple few-shot and standard image classification da
    
[^260]: 基于对比去噪分数的文本引导潜在扩散图像编辑

    Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing

    [https://arxiv.org/abs/2311.18608](https://arxiv.org/abs/2311.18608)

    提出了一种名为Contrastive Denoising Score (CDS) 的简单但强大的修改版本，用于潜在扩散模型 (LDM)，在DDS框架内使用了CUT损失来更好地保留原始图像的特定结构元素。

    

    随着文本到图像扩散模型的显著出现，图像编辑方法变得更加多样化并不断发展。在这个领域中一种很有前景的最近方法是Delta Denoising Score (DDS) - 一种基于Score Distillation Sampling (SDS)框架的图像编辑技术，它利用了文本到图像扩散模型的丰富生成先验。然而，仅依赖评分函数之间的差异不足以保留原始图像的特定结构元素，这是图像编辑的关键方面之一。为了解决这个问题，我们提出了 DDS 的一个尴尬简单但非常强大的修改版本，称为Contrastive Denoising Score (CDS)，用于潜在扩散模型 (LDM)。受到 DDS 和无配对图像到图像翻译的对比学习 (CUT) 之间的相似性和差异的启发，我们介绍了一种简单直接的方法，在DDS框架内使用CUT损失。

    arXiv:2311.18608v2 Announce Type: replace-cross  Abstract: With the remarkable advent of text-to-image diffusion models, image editing methods have become more diverse and continue to evolve. A promising recent approach in this realm is Delta Denoising Score (DDS) - an image editing technique based on Score Distillation Sampling (SDS) framework that leverages the rich generative prior of text-to-image diffusion models. However, relying solely on the difference between scoring functions is insufficient for preserving specific structural elements from the original image, a crucial aspect of image editing. To address this, here we present an embarrassingly simple yet very powerful modification of DDS, called Contrastive Denoising Score (CDS), for latent diffusion models (LDM). Inspired by the similarities and differences between DDS and the contrastive learning for unpaired image-to-image translation(CUT), we introduce a straightforward approach using CUT loss within the DDS framework. Ra
    
[^261]: 大型多模态模型的组合式思维提示

    Compositional Chain-of-Thought Prompting for Large Multimodal Models

    [https://arxiv.org/abs/2311.17076](https://arxiv.org/abs/2311.17076)

    提出了一种新颖的零样式思维提示（CCoT），以克服大型多模态模型难以捕捉到组合视觉推理方面的细节的问题。

    

    强大的视觉骨干和大规模语言模型(LLM)推理的结合已经导致大型多模态模型(LMM)成为当前广泛视觉和语言(VL)任务的标准。然而，最近的研究表明，即使是最先进的LMM仍然难以捕捉到组合视觉推理方面的细节，比如对象之间的属性和关系。一种解决方案是利用场景图(SGs)——对象及其关系和属性的形式化表达，它已被广泛用作视觉和文本领域之间的桥梁。然而，场景图数据需要场景图注释，这种数据收集成本高昂，因此难以扩展。此外，基于场景图数据微调LMM可能导致预训练目标的灾难性遗忘。为了克服这一问题，受到思维链方法的启发，我们提出了一种新颖的零样式思维提示（CCoT）。

    arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
    
[^262]: MobileCLIP: 通过多模态强化训练实现快速图像-文本模型

    MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training

    [https://arxiv.org/abs/2311.17049](https://arxiv.org/abs/2311.17049)

    MobileCLIP通过多模态强化训练实现了高效率图像-文本模型，在零-shot任务中取得了新的性能平衡。

    

    对图像-文本基础模型（如CLIP）进行对比预训练表明，在广泛的下游任务中表现出色的零-shot性能和改善的鲁棒性。然而，这些模型利用大型基于transformer的编码器，具有显著的内存和延迟开销，这会给在移动设备上部署带来挑战。在这项工作中，我们介绍了MobileCLIP--一种优化了运行时性能的高效图像-文本模型家族，以及一种新颖而高效的训练方法，即多模态强化训练。所提出的训练方法利用来自图像字幕模型和强CLIP编码器集成的知识转移来提高高效模型的准确性。我们的方法通过将额外的知识存储在强化数据集中避免了训练时的计算开销。MobileCLIP为零-shot分类和检索设置了一个新的延迟-准确性权衡的最新水平。

    arXiv:2311.17049v2 Announce Type: replace-cross  Abstract: Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrie
    
[^263]: 处理张量奇异值分解中的非光滑挑战：多目标张量恢复框架

    Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework

    [https://arxiv.org/abs/2311.13958](https://arxiv.org/abs/2311.13958)

    提出一种具有可学习张量核范数的新型张量恢复模型，引入交替近端乘子方法（APMM）优化算法，解决处理非光滑变化的张量数据挑战

    

    最近，许多基于张量奇异值分解（t-SVD）的张量恢复方法在处理视觉数据（如彩色图像和视频）方面表现出潜力。然而，当面对显示出非光滑变化的张量数据时，这些方法通常会遭受严重的性能退化。虽然在现实世界中经常观察到这种情况，但传统的基于t-SVD的方法却忽视了这一点。在这项工作中，我们引入了一种新颖的张量恢复模型，其中包括可学习的张量核范数，以解决这一挑战。我们开发了一种名为交替近端乘子方法（APMM）的新优化算法，以迭代地解决提出的张量补全模型。理论分析证明了所提出的APMM收敛到优化问题的Karush-Kuhn-Tucker（KKT）点。此外，我们基于APMM提出了一个多目标张量恢复框架，以有效探索协

    arXiv:2311.13958v2 Announce Type: replace-cross  Abstract: Recently, numerous tensor singular value decomposition (t-SVD)-based tensor recovery methods have shown promise in processing visual data, such as color images and videos. However, these methods often suffer from severe performance degradation when confronted with tensor data exhibiting non-smooth changes. It has been commonly observed in real-world scenarios but ignored by the traditional t-SVD-based methods. In this work, we introduce a novel tensor recovery model with a learnable tensor nuclear norm to address such a challenge. We develop a new optimization algorithm named the Alternating Proximal Multiplier Method (APMM) to iteratively solve the proposed tensor completion model. Theoretical analysis demonstrates the convergence of the proposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization problem. In addition, we propose a multi-objective tensor recovery framework based on APMM to efficiently explore the co
    
[^264]: 发现有效的土地利用规划政策

    Discovering Effective Policies for Land-Use Planning

    [https://arxiv.org/abs/2311.12304](https://arxiv.org/abs/2311.12304)

    通过学习代理模型并使用进化搜索过程，发现了可定制到不同位置的有效土地利用政策，为土地利用规划提供了一个潜在有用的工具。

    

    土地被分配给不同的用途，如森林、城市区域和农业，对陆地碳平衡和气候变化有重大影响。基于可用的土地利用变化的历史数据和相关的碳排放和吸收的模拟，可以学习到一个代理模型，从而能够高效评估决策者可选择的不同选项。然后可以使用进化搜索过程来发现特定位置的有效土地利用政策。该系统构建在Project Resilience平台上，并使用Land-Use Harmonization数据集LUH2和簿记模型BLUE进行评估。它生成可定制到不同位置的碳影响和土地利用变化量的帕累托前沿，从而为土地利用规划提供了一个潜在有用的工具。

    How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
    
[^265]: 通过交替单模态适应进行多模态表示学习

    Multimodal Representation Learning by Alternating Unimodal Adaptation

    [https://arxiv.org/abs/2311.10707](https://arxiv.org/abs/2311.10707)

    提出了一种通过交替单模态适应来进行多模态表示学习的方法，旨在最小化干扰并捕获跨模态交互

    

    多模态学习在人工智能领域中起着关键作用，然而，现有的多模态学习方法往往在某些感知模态在学习过程中表现更显著时面临挑战，导致性能不佳。为了解决这一挑战，我们提出了MLA（具有交替单模态适应的多模态学习）。MLA通过将传统的联合多模态学习过程转化为交替单模态学习过程来重新定义，从而最小化模态之间的干扰。同时，它通过一个共享头部捕获跨模态交互，该头部在不同模态间经历持续优化。这种优化过程由梯度修改机制控制，以防止共享头部丢失先前获取的信息。在推理阶段，MLA利用一个测试时间

    arXiv:2311.10707v2 Announce Type: replace  Abstract: Multimodal learning, which integrates data from diverse sensory modes, plays a pivotal role in artificial intelligence. However, existing multimodal learning methods often struggle with challenges where some modalities appear more dominant than others during multimodal learning, resulting in suboptimal performance. To address this challenge, we propose MLA (Multimodal Learning with Alternating Unimodal Adaptation). MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities. Simultaneously, it captures cross-modal interactions through a shared head, which undergoes continuous optimization across different modalities. This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information. During the inference phase, MLA utilizes a test-time
    
[^266]: 低秩适应用于多语言摘要：一项实证研究

    Low-Rank Adaptation for Multilingual Summarization: An Empirical Study

    [https://arxiv.org/abs/2311.08572](https://arxiv.org/abs/2311.08572)

    LoRA 在多语言摘要方面竞争力强，特别在低数据情况和跨语言转移方面表现出色。

    

    尽管预训练的大型语言模型的进展显著加速了近年来自然语言处理领域的进步，但它们不断增长的体积对传统的微调提出了重要挑战，特别是在内存密集型任务中。我们调查了参数高效微调的潜力，重点是低秩适应（LoRA），涉及多语言摘要领域，这是一个具有挑战性的任务（因为输入通常很长），且相对未被充分探索。我们进行了一项广泛的研究，涵盖不同数据可用性场景，包括高数据和低数据设置，以及跨语言转移，利用不同规模的模型。我们的发现表明，当使用大量数据训练时，LoRA与完全微调竞争激烈，并且在低数据情况和跨语言转移方面表现出色。我们还研究了不同的少数据点跨语言转移策略，发现持续的LoRA调优...

    arXiv:2311.08572v2 Announce Type: replace-cross  Abstract: Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning
    
[^267]: 评价图神经网络的邻居可解释性

    Evaluating Neighbor Explainability for Graph Neural Networks

    [https://arxiv.org/abs/2311.08118](https://arxiv.org/abs/2311.08118)

    评价图神经网络中邻居的可解释性，提出新的度量标准并发现基于梯度的方法在GNN领域的解释没有太大差异，同时发现很多技术在没有自环的GNNs下无法准确识别重要邻居。

    

    图神经网络（GNNs）中的可解释性是近年来新兴领域。在这篇文章中，我们解决了一个问题，即在对节点进行分类时，确定每个邻居对于 GNN 的重要性以及如何衡量这一特定任务的表现。为此，各种已知的可解释性方法被重新构造以获取邻居重要性，并提出了四种新的度量标准。我们的结果表明，在 GNN 领域，基于梯度的技术提供的解释几乎没有差异。此外，许多可解释性技术在使用没有自环的 GNNs 时未能识别重要的邻居。

    arXiv:2311.08118v2 Announce Type: replace-cross  Abstract: Explainability in Graph Neural Networks (GNNs) is a new field growing in the last few years. In this publication we address the problem of determining how important is each neighbor for the GNN when classifying a node and how to measure the performance for this specific task. To do this, various known explainability methods are reformulated to get the neighbor importance and four new metrics are presented. Our results show that there is almost no difference between the explanations provided by gradient-based techniques in the GNN domain. In addition, many explainability techniques failed to identify important neighbors when GNNs without self-loops are used.
    
[^268]: 在上下文学习和梯度下降的再审视

    In-context Learning and Gradient Descent Revisited

    [https://arxiv.org/abs/2311.07772](https://arxiv.org/abs/2311.07772)

    本文重新审视了在现实NLP任务和模型中的上下文学习（ICL）与梯度下降（GD）之间的联系证据，找到了评估上的不足，提出了遵循层次因果关系的简单GD优化程序来改善相似性分数。

    

    在上下文学习（ICL）已在少样本学习任务中取得了令人印象深刻的结果，然而其基本机制仍未被充分理解。最近的一系列研究表明，ICL隐式地执行梯度下降（GD）优化。尽管具有吸引力，但很多研究集中在简化设置，其中优化浅层模型的参数。在这项工作中，我们重新审视了针对现实NLP任务和模型的ICL-GD对应的证据。我们发现在评估方面存在差距，无论是在有问题的指标还是不足的基线方面。我们发现令人惊讶的是，即使是未经训练的模型也能实现可比的ICL-GD相似性分数，尽管未表现出ICL。接下来，我们探讨了模型中信息流动在ICL和GD之间的主要差异，我们将其称为“层因果关系”。我们提出了一个尊重层因果关系的简单GD优化过程，并表明它显著改善了相似性分数。

    arXiv:2311.07772v4 Announce Type: replace  Abstract: In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significan
    
[^269]: EVORA：面向风险感知越野自主行驶的深度证据可穿越性学习

    EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road Autonomy

    [https://arxiv.org/abs/2311.06234](https://arxiv.org/abs/2311.06234)

    提出了一种统一框架，通过证据深度学习有效量化不确定性，学习不确定性感知牵引模型并规划风险感知轨迹。

    

    高质量牵引的地形穿越对于实现快速越野导航至关重要。现有方法通过自监督学习直接从数据中学习地形特性，自动惩罚通过不良地形的轨迹，但在适当量化和减轻学习模型中的不确定性风险方面仍存在挑战。为此，本工作提出了一个统一的框架，用于学习不确定性感知牵引模型并规划风险感知轨迹。为了量化不确定性，我们通过学习离散牵引分布和牵引预测器潜在特征的概率密度，高效地建模了混合不确定性。利用证据深度学习，我们通过网络输出参数化狄利克雷分布，并提出了一种新颖的不确定性感知平方地球移动距离损失，其具有一个闭合的损失公式。

    arXiv:2311.06234v2 Announce Type: replace-cross  Abstract: Traversing terrain with good traction is crucial for achieving fast off-road navigation. Instead of manually designing costs based on terrain features, existing methods learn terrain properties directly from data via self-supervision to automatically penalize trajectories moving through undesirable terrain, but challenges remain to properly quantify and mitigate the risk due to uncertainty in learned models. To this end, this work proposes a unified framework to learn uncertainty-aware traction model and plan risk-aware trajectories. For uncertainty quantification, we efficiently model both aleatoric and epistemic uncertainty by learning discrete traction distributions and probability densities of the traction predictor's latent features. Leveraging evidential deep learning, we parameterize Dirichlet distributions with the network outputs and propose a novel uncertainty-aware squared Earth Mover's distance loss with a closed-fo
    
[^270]: 一种用于实时神经突触分类的轻量级架构

    A Lightweight Architecture for Real-Time Neuronal-Spike Classification

    [https://arxiv.org/abs/2311.04808](https://arxiv.org/abs/2311.04808)

    该研究提出了一种轻量级的神经突触检测和分类架构，利用普京细胞的特征实时处理数据，实现了将数据压缩并存储在可移动装置上的目标。

    

    神经科学家们非常喜欢使用小鼠大脑的神经活动电生理记录来了解大脑功能。本文专注于从小脑中的普京细胞获取记录，以便了解大脑损伤和运动功能丧失。我们提出了一种轻量级的神经突触检测和分类架构，利用普京细胞的独特特征实时丢弃稀疏神经数据中不需要的信息。这使得（压缩）数据可以轻松存储在头部设备上的可移动存储设备上，减轻了需要使用连线的需求。

    arXiv:2311.04808v2 Announce Type: replace-cross  Abstract: Electrophysiological recordings of neural activity in a mouse's brain are very popular among neuroscientists for understanding brain function. One particular area of interest is acquiring recordings from the Purkinje cells in the cerebellum in order to understand brain injuries and the loss of motor functions. However, current setups for such experiments do not allow the mouse to move freely and, thus, do not capture its natural behaviour since they have a wired connection between the animal's head stage and an acquisition device. In this work, we propose a lightweight neuronal-spike detection and classification architecture that leverages on the unique characteristics of the Purkinje cells to discard unneeded information from the sparse neural data in real time. This allows the (condensed) data to be easily stored on a removable storage device on the head stage, alleviating the need for wires. Synthesis results reveal a >95% o
    
[^271]: 改进的深窄前馈神经网络权重初始化方法

    Improved weight initialization for deep and narrow feedforward neural network

    [https://arxiv.org/abs/2311.03733](https://arxiv.org/abs/2311.03733)

    本文提出了一种新颖的权重初始化方法，能有效解决在使用ReLU激活函数训练极深窄前馈网络时遇到的“死亡ReLU”问题。

    

    适当的权重初始化设置，以及ReLU激活函数，已成为现代深度学习的基石，使得高效和有效的神经网络模型能够在人工智能的各个领域得以训练和推广。在使用ReLU激活函数训练深度神经网络时，“死亡ReLU”问题，即ReLU神经元变得不活跃并输出为零，是一个重要挑战。理论研究和各种方法已被引入以解决这一问题。然而，即使有了这些方法和研究，对于使用ReLU激活函数的极深窄前馈网络的训练仍然具有挑战性。本文提出了一种新颖的权重初始化方法来解决这个问题。我们确定了我们初始权重矩阵的几个特性，并演示了这些特性如何使有效传播更加可能。

    arXiv:2311.03733v2 Announce Type: replace  Abstract: Appropriate weight initialization settings, along with the ReLU activation function, have become cornerstones of modern deep learning, enabling the training and deployment of highly effective and efficient neural network models across diverse areas of artificial intelligence. The problem of \textquotedblleft dying ReLU," where ReLU neurons become inactive and yield zero output, presents a significant challenge in the training of deep neural networks with ReLU activation function. Theoretical research and various methods have been introduced to address the problem. However, even with these methods and research, training remains challenging for extremely deep and narrow feedforward networks with ReLU activation function. In this paper, we propose a novel weight initialization method to address this issue. We establish several properties of our initial weight matrix and demonstrate how these properties enable the effective propagation o
    
[^272]: 具有Fisher度量的黎曼拉普拉斯逼近

    Riemannian Laplace Approximation with the Fisher Metric

    [https://arxiv.org/abs/2311.02766](https://arxiv.org/abs/2311.02766)

    黎曼拉普拉斯逼近的新方法利用Fisher度量提供更丰富的逼近族，解决了在无限数据极限下先前方法度量选择不当导致逼近过于狭窄和有偏的问题。

    

    Laplace方法用高斯分布在其模式处对目标密度进行近似。基于Bernstein-von Mises定理，它在贝叶斯推断中是计算效率高且渐近准确的，但对于复杂的目标和有限数据后验，它往往是一种过于粗糙的近似。最近对Laplace逼近的一般化是根据选择的黎曼几何对高斯近似进行转换，提供了更丰富的近似族，同时保持计算效率。然而，正如本文所示，其性质严重依赖于所选择的度量，实际上，在先前研究中采用的度量导致的逼近即使在无限数据量的极限下也过于狭窄且存在偏差。我们通过进一步发展逼近族，推导出两种在无限数据极限下精确的替代变种，扩展了理论分析。

    arXiv:2311.02766v3 Announce Type: replace  Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the
    
[^273]: 优化基于效用的缺口风险：一种非渐近视角

    Optimization of utility-based shortfall risk: A non-asymptotic viewpoint

    [https://arxiv.org/abs/2310.18743](https://arxiv.org/abs/2310.18743)

    该论文研究了基于效用的缺口风险的估计和优化问题，提出了非渐近界限来解决UBSR的估计误差和优化过程中的梯度问题。

    

    我们考虑效用基缺口风险（UBSR）的估计和优化问题，这是金融领域中一种流行的风险度量。在UBSR估计的情境中，我们推导出了对经典样本平均逼近（SAA）的UBSR均方误差的非渐近界限。接下来，在UBSR优化的情境中，我们推导出了在平滑参数化下UBSR梯度的表达式。这个表达式是期望的比值，两者都涉及UBSR。我们在UBSR梯度表达式中使用SAA作为分子和分母，得出了一个有偏梯度估计器。我们推导出估计误差的非渐近界限，表明我们的梯度估计器是渐近无偏的。我们将上述梯度估计器结合到一个用于UBSR优化的随机梯度（SG）算法中。最后，我们推导出量化我们方法收敛速度的非渐近界限。

    arXiv:2310.18743v2 Announce Type: replace  Abstract: We consider the problems of estimation and optimization of utility-based shortfall risk (UBSR), which is a popular risk measure in finance. In the context of UBSR estimation, we derive a non-asymptotic bound on the mean-squared error of the classical sample average approximation (SAA) of UBSR. Next, in the context of UBSR optimization, we derive an expression for the UBSR gradient under a smooth parameterization. This expression is a ratio of expectations, both of which involve the UBSR. We use SAA for the numerator as well as denominator in the UBSR gradient expression to arrive at a biased gradient estimator. We derive non-asymptotic bounds on the estimation error, which show that our gradient estimator is asymptotically unbiased. We incorporate the aforementioned gradient estimator into a stochastic gradient (SG) algorithm for UBSR optimization. Finally, we derive non-asymptotic bounds that quantify the rate of convergence of our 
    
[^274]: DistillSpec：通过知识蒸馏改进投机性解码

    DistillSpec: Improving Speculative Decoding via Knowledge Distillation

    [https://arxiv.org/abs/2310.08461](https://arxiv.org/abs/2310.08461)

    DistillSpec通过知识蒸馏技术改进了投机性解码，在标准基准测试中取得了10-45%的速度提升。

    

    投机性解码（SD）通过使用更快的草稿模型生成多个标记，然后由更大的目标模型并行验证这些标记，从而生成符合目标模型分布的文本，加速大型语言模型推理。然而，找到与目标模型良好对齐的紧凑草稿模型具有挑战性。为了解决这个问题，我们提出了DistillSpec，它使用知识蒸馏来更好地将草稿模型与目标模型对齐，然后应用SD。DistillSpec做出了两个关键设计选择，我们通过系统研究证明这对改进草稿和目标对齐至关重要：利用来自草稿模型的on-policy数据生成，以及将发散函数定制到任务和解码策略。值得注意的是，DistillSpec在一系列标准基准测试上比标准SD获得了令人印象深刻的10-45%的加速，使用贪婪和非贪婪方法。

    arXiv:2310.08461v2 Announce Type: replace-cross  Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-gr
    
[^275]: 大型语言模型的复合后门攻击

    Composite Backdoor Attacks Against Large Language Models

    [https://arxiv.org/abs/2310.07676](https://arxiv.org/abs/2310.07676)

    该研究通过复合后门攻击(CBA)展示了在大型语言模型中植入多个触发关键词的方法，相较于现有方法更为隐蔽，并确保只有当所有触发关键词同时出现时后门才会被激活。

    

    大型语言模型（LLMs）在各种任务上表现出优越性能，通常作为许多研究和服务的基础模型。然而，不可信任的第三方LLMs可能会暗中为下游任务引入漏洞。本文通过后门攻击的视角探讨了LLMs的脆弱性。与现有的对LLMs的后门攻击不同，我们在不同的提示组件中分散多个触发关键词。这种复合后门攻击（CBA）被证明比仅在单个组件中植入相同的多个触发关键词更隐蔽。CBA确保只有当所有触发关键词出现时后门才会被激活。我们的实验表明，CBA在自然语言处理（NLP）和多模式任务中都是有效的。

    arXiv:2310.07676v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our att
    
[^276]: FTFT:通过转移训练动态实现高效且稳健的微调

    FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics

    [https://arxiv.org/abs/2310.06588](https://arxiv.org/abs/2310.06588)

    训练动态可在不同模型大小和预训练方法之间进行转移，通过选定的训练实例微调主模型实现比经验风险最小化更高的训练效率

    

    尽管微调预训练语言模型（PLMs）取得了巨大成功，但它们仍然容易受到分布外输入的影响。 数据集制图是一种简单而有效的双模型方法，可以提高微调PLMs的鲁棒性。 它涉及在原始训练集上微调模型（即参考模型），根据训练动态选择一些重要的训练实例，并仅对这些选定的示例再次进行微调（即主模型）。 然而，这种方法需要对同一模型进行两次微调，这对于大型PLMs而言在计算上是昂贵的。 在本文中，我们展示了（1）训练动态在模型大小和预训练方法之间具有高度可传递性，以及（2）使用这些选定的训练实例对主模型进行微调可以比经验风险最小化（ERM）实现更高的训练效率。 基于这些观察结果，我们提出了一种新颖的微调方法...

    arXiv:2310.06588v2 Announce Type: replace  Abstract: Despite the massive success of fine-tuning Pre-trained Language Models (PLMs), they remain susceptible to out-of-distribution input. Dataset cartography is a simple yet effective dual-model approach that improves the robustness of fine-tuned PLMs. It involves fine-tuning a model on the original training set (i.e. reference model), selecting a subset of important training instances based on the training dynamics, and fine-tuning again only on these selected examples (i.e. main model). However, this approach requires fine-tuning the same model twice, which is computationally expensive for large PLMs. In this paper, we show that (1) training dynamics are highly transferable across model sizes and pre-training methods, and that (2) fine-tuning main models using these selected training instances achieves higher training efficiency than empirical risk minimization (ERM). Building on these observations, we propose a novel fine-tuning approa
    
[^277]: 机器学习训练数据分布特性的验证

    Attesting Distributional Properties of Training Data for Machine Learning

    [https://arxiv.org/abs/2308.09552](https://arxiv.org/abs/2308.09552)

    提出了一种新型的财产证明概念，可以在不泄露数据的情况下向验证者展示训练数据的分布特性。

    

    机器学习的成功伴随着对其可信度的增加关注。一些司法管辖区正在准备机器学习监管框架。其中一个关注点是确保模型训练数据具有某些特定敏感属性的理想分布特性。我们提出了财产证明的概念，允许证明者（例如，模型训练者）向验证者（例如，客户）展示训练数据的相关分布特性，而不泄露数据。我们提出了一种有效的混合财产证明，结合了财产推理与加密机制。

    arXiv:2308.09552v3 Announce Type: replace-cross  Abstract: The success of machine learning (ML) has been accompanied by increased concerns about its trustworthiness. Several jurisdictions are preparing ML regulatory frameworks. One such concern is ensuring that model training data has desirable distributional properties for certain sensitive attributes. For example, draft regulations indicate that model trainers are required to show that training datasets have specific distributional properties, such as reflecting diversity of the population. We propose the notion of property attestation allowing a prover (e.g., model trainer) to demonstrate relevant distributional properties of training data to a verifier (e.g., a customer) without revealing the data. We present an effective hybrid property attestation combining property inference with cryptographic mechanisms.
    
[^278]: 基于稳定扩散的潜在编码增强的无数据替代攻击

    Latent Code Augmentation Based on Stable Diffusion for Data-free Substitute Attacks

    [https://arxiv.org/abs/2307.12872](https://arxiv.org/abs/2307.12872)

    提出了一种基于稳定扩散的新型无数据替代攻击方案，引入潜在编码增强（LCA）来提高替代训练的效率和准确性

    

    在黑盒替代攻击中，由于目标模型的训练数据不可用，大多数最近的方案利用GAN生成数据来训练替代模型。然而，这些基于GAN的方案在训练效率上存在问题，因为生成器在替代训练过程中每次需要重新训练为目标模型，同时生成质量不高。为了克服这些限制，我们考虑利用扩散模型生成数据，并提出了一种基于稳定扩散（SD）的新型无数据替代攻击方案，以提高替代训练的效率和准确性。尽管SD生成的数据质量很高，但它呈现出不同的领域分布以及目标模型正负样本的大变化。针对这个问题，我们提出了潜在编码增强（LCA）来促进SD生成与目标数据对齐的数据。

    arXiv:2307.12872v2 Announce Type: replace-cross  Abstract: Since the training data of the target model is not available in the black-box substitute attack, most recent schemes utilize GANs to generate data for training the substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality. To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a novel data-free substitute attack scheme based on the Stable Diffusion (SD) to improve the efficiency and accuracy of substitute training. Despite the data generated by the SD exhibiting high quality, it presents a different distribution of domains and a large variation of positive and negative samples for the target model. For this problem, we propose Latent Code Augmentation (LCA) to facilitate SD in generating data that aligns with the data 
    
[^279]: DANSE: 无监督学习设置中模型无关过程的基于数据驱动的非线性状态估计

    DANSE: Data-driven Non-linear State Estimation of Model-free Process in Unsupervised Learning Setup

    [https://arxiv.org/abs/2306.03897](https://arxiv.org/abs/2306.03897)

    在无监督学习设置中，提出了一种名为DANSE的基于数据驱动的非线性状态估计方法，利用数据驱动的循环神经网络捕捉模型无关过程中的潜在非线性动态。

    

    我们解决了在无监督学习设置中针对模型无关过程的贝叶斯状态估计和预测任务。对于模型无关过程，我们没有任何关于过程动态的先验知识。在文章中，我们提出了DANSE——一种基于数据驱动的非线性状态估计方法。DANSE提供了给定状态的线性测量的封闭形式后验概率。此外，它还提供了预测的封闭形式后验概率。DANSE中使用数据驱动的循环神经网络（RNN）来提供状态先验的参数。先验依赖于过去的测量作为输入，然后使用当前测量作为输入找到状态的封闭形式后验概率。数据驱动的RNN捕捉模型无关过程的潜在非线性动态。DANSE的训练，主要是学习RNN的参数，是使用无监督的方法进行的。

    arXiv:2306.03897v2 Announce Type: replace-cross  Abstract: We address the tasks of Bayesian state estimation and forecasting for a model-free process in an unsupervised learning setup. For a model-free process, we do not have any a-priori knowledge of the process dynamics. In the article, we propose DANSE -- a Data-driven Nonlinear State Estimation method. DANSE provides a closed-form posterior of the state of the model-free process, given linear measurements of the state. In addition, it provides a closed-form posterior for forecasting. A data-driven recurrent neural network (RNN) is used in DANSE to provide the parameters of a prior of the state. The prior depends on the past measurements as input, and then we find the closed-form posterior of the state using the current measurement as input. The data-driven RNN captures the underlying non-linear dynamics of the model-free process. The training of DANSE, mainly learning the parameters of the RNN, is executed using an unsupervised lea
    
[^280]: 视觉丰富文档的跨模态实体匹配

    Cross-Modal Entity Matching for Visually Rich Documents

    [https://arxiv.org/abs/2303.00720](https://arxiv.org/abs/2303.00720)

    提出了一个跨模态实体匹配框架 Juno，通过深度神经网络和注意力机制，实现了文档中文本跨度与外部数据库中语义相似元组的匹配，从而解决了视觉丰富文档信息检索中缺乏上下文和见解的问题。

    

    视觉丰富文档（如传单、横幅、杂志文章）是利用视觉线索来增强语义的实体或数字化文档。这些文档中包含的信息往往是临时的，经常是不完整的。现有的允许对这些文档进行结构化查询的方法没有考虑到这一点。这使得在从这些文档中进行查询并从中获取可操作见解时，很难将检索到的信息置于上下文中。我们提出了Juno - 一个跨模态实体匹配框架，以解决这一局限。它通过将文档中的文本跨度与外部数据库中语义类似的元组进行匹配，从而为异构文档提供补充信息。我们的主要贡献是一个带注意力机制的深度神经网络，它超越了传统的基于关键字的匹配，通过在多模态编码空间上对齐文本跨度和关系元组来找到匹配的元组。

    arXiv:2303.00720v2 Announce Type: replace  Abstract: Visually rich documents (e.g. leaflets, banners, magazine articles) are physical or digital documents that utilize visual cues to augment their semantics. Information contained in these documents are ad-hoc and often incomplete. Existing works that enable structured querying on these documents do not take this into account. This makes it difficult to contextualize the information retrieved from querying these documents and gather actionable insights from them. We propose Juno -- a cross-modal entity matching framework to address this limitation. It augments heterogeneous documents with supplementary information by matching a text span in the document with semantically similar tuples from an external database. Our main contribution in this is a deep neural network with attention that goes beyond traditional keyword-based matching and finds matching tuples by aligning text spans and relational tuples on a multimodal encoding space with
    
[^281]: 非线性函数逼近下的高斯-牛顿时差学习

    Gauss-Newton Temporal Difference Learning with Nonlinear Function Approximation

    [https://arxiv.org/abs/2302.13087](https://arxiv.org/abs/2302.13087)

    提出了一种在非线性函数逼近下的高斯-牛顿时差学习方法，能够显著改进对全局最优Q函数的样本复杂度。

    

    在这篇论文中，提出了一种使用高斯-牛顿时差（GNTD）学习方法来解决具有非线性函数逼近的Q-learning问题。在每次迭代中，我们的方法采用高斯-牛顿（GN）步骤来优化一种变体的均方贝尔曼误差（MSBE），其中采用目标网络来避免双重采样。分析了不精确的GN步骤，因此可以通过廉价的矩阵迭代安全且高效地计算GN更新。在温和条件下，针对各种非线性函数逼近推导了有限样本非渐近收敛到全局最优Q函数。特别是对于具有relu激活的神经网络参数化，GNTD达到了$\tilde{\mathcal{O}}(\varepsilon^{-1})$的改进样本复杂度，而现有神经网络TD方法的样本复杂度为$\mathcal{\mathcal{O}}(\varepsilon^{-2})$。最近阶$\tilde{\mathcal{O}}(\varepsilon^{-1.5})$的样本

    arXiv:2302.13087v2 Announce Type: replace-cross  Abstract: In this paper, a Gauss-Newton Temporal Difference (GNTD) learning method is proposed to solve the Q-learning problem with nonlinear function approximation. In each iteration, our method takes one Gauss-Newton (GN) step to optimize a variant of Mean-Squared Bellman Error (MSBE), where target networks are adopted to avoid double sampling. Inexact GN steps are analyzed so that one can safely and efficiently compute the GN updates by cheap matrix iterations. Under mild conditions, non-asymptotic finite-sample convergence to the globally optimal Q function is derived for various nonlinear function approximations. In particular, for neural network parameterization with relu activation, GNTD achieves an improved sample complexity of $\tilde{\mathcal{O}}(\varepsilon^{-1})$, as opposed to the $\mathcal{\mathcal{O}}(\varepsilon^{-2})$ sample complexity of the existing neural TD methods. An $\tilde{\mathcal{O}}(\varepsilon^{-1.5})$ sample
    
[^282]: 语义融合的多粒度跨城市交通预测

    Semantic-Fused Multi-Granularity Cross-City Traffic Prediction

    [https://arxiv.org/abs/2302.11774](https://arxiv.org/abs/2302.11774)

    提出了一种语义融合的多粒度迁移学习模型，能够在具有不同粒度的城市之间实现交通预测知识转移

    

    精确的交通预测对有效城市管理和提高交通效率至关重要。最近，基于数据驱动的交通预测方法得到了广泛采用，表现优于传统方法。然而，它们通常需要大量数据进行有效训练，鉴于传感基础设施不足的地区普遍存在数据稀缺问题，这变得具有挑战性。为解决这一问题，我们提出了一种语义融合的多粒度迁移学习（SFMGTL）模型，实现在具有不同粒度的融合语义的城市之间进行知识转移。具体而言，我们设计了一个语义融合模块，通过重构损失融合各种语义，同时保留静态空间依赖性。然后，基于节点特征构建了一个融合图，并通过图结构学习实现层次节点聚类，生成图。

    arXiv:2302.11774v2 Announce Type: replace  Abstract: Accurate traffic prediction is essential for effective urban management and the improvement of transportation efficiency. Recently, data-driven traffic prediction methods have been widely adopted, with better performance than traditional approaches. However, they often require large amounts of data for effective training, which becomes challenging given the prevalence of data scarcity in regions with inadequate sensing infrastructures. To address this issue, we propose a Semantic-Fused Multi-Granularity Transfer Learning (SFMGTL) model to achieve knowledge transfer across cities with fused semantics at different granularities. In detail, we design a semantic fusion module to fuse various semantics while conserving static spatial dependencies via reconstruction losses. Then, a fused graph is constructed based on node features through graph structure learning. Afterwards, we implement hierarchical node clustering to generate graphs wit
    
[^283]: 通向跨生成模型泛化的通用假图像检测器

    Towards Universal Fake Image Detectors that Generalize Across Generative Models

    [https://arxiv.org/abs/2302.10174](https://arxiv.org/abs/2302.10174)

    通过在不进行训练的情况下执行真伪图像分类，使用非明确区分特征空间，可以更好地识别不同生成模型来源的图像。

    

    随着生成模型的快速增多，对于通用目的的假图像检测器的需求正在增长。本文首先展示了现有范式的失败，该范式包括训练深度网络进行真假分类，当训练以检测GAN伪图像时，无法检测到来自新型生成模型的假图像。通过分析，我们发现得到的分类器对检测使图像伪造的模式进行了不对称调整。真实类成为一个盛放任何非假的东西的汇类，包括在训练期间无法访问的模型生成的图像。基于这一发现，我们提出执行真伪分类而不进行学习；即使用非明确训练以区分真假图像的特征空间。我们使用最近邻和线性探查作为这一想法的实例化。当提供对一个大型预训练特征空间的访问时，我们能够比现有的[image-based]学习方法更好地区分不同的生成模型来源的图像。

    arXiv:2302.10174v2 Announce Type: replace-cross  Abstract: With generative models proliferating at a rapid rate, there is a growing need for general purpose fake image detectors. In this work, we first show that the existing paradigm, which consists of training a deep network for real-vs-fake classification, fails to detect fake images from newer breeds of generative models when trained to detect GAN fake images. Upon analysis, we find that the resulting classifier is asymmetrically tuned to detect patterns that make an image fake. The real class becomes a sink class holding anything that is not fake, including generated images from models not accessible during training. Building upon this discovery, we propose to perform real-vs-fake classification without learning; i.e., using a feature space not explicitly trained to distinguish real from fake images. We use nearest neighbor and linear probing as instantiations of this idea. When given access to the feature space of a large pretrain
    
[^284]: 相似性、压缩和局部步骤：分布式变分不等式高效通信的三大支柱

    Similarity, Compression and Local Steps: Three Pillars of Efficient Communications for Distributed Variational Inequalities

    [https://arxiv.org/abs/2302.07615](https://arxiv.org/abs/2302.07615)

    相似性、压缩和局部更新是本文提出的三大技术，用于减少分布式变分不等式问题中通信轮次和成本，实现了前所未有的三重协同作用。

    

    变分不等式是一个广泛而灵活的问题类，包括最小化、鞍点和不动点问题作为特例。因此，变分不等式在各种应用中被使用，从均衡搜索到对抗学习都有涉及。随着数据和模型规模的增加，当今的实例需要并行和分布式计算来解决现实世界中的机器学习问题，其中大部分可以表示为变分不等式。同时，大多数分布式方法存在一个重大瓶颈 - 通信成本。减少通信轮次的总数和每轮成本的三种主要技术是本地函数的相似性、传输信息的压缩和局部更新。本文结合了所有这些方法。对于变分不等式和鞍点问题来说，这样的三重协同作用以前并不存在。

    arXiv:2302.07615v2 Announce Type: replace-cross  Abstract: Variational inequalities are a broad and flexible class of problems that includes minimization, saddle point, and fixed point problems as special cases. Therefore, variational inequalities are used in various applications ranging from equilibrium search to adversarial learning. With the increasing size of data and models, today's instances demand parallel and distributed computing for real-world machine learning problems, most of which can be represented as variational inequalities. Meanwhile, most distributed approaches have a significant bottleneck - the cost of communications. The three main techniques to reduce the total number of communication rounds and the cost of one such round are the similarity of local functions, compression of transmitted information, and local updates. In this paper, we combine all these approaches. Such a triple synergy did not exist before for variational inequalities and saddle problems, nor eve
    
[^285]: BRAIxDet：学习使用不完整注释检测恶性乳腺病变

    BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations

    [https://arxiv.org/abs/2301.13418](https://arxiv.org/abs/2301.13418)

    本文提出了一种中间解决方案，即将训练构建为弱监督和半监督学习问题，以解决通过不完整注释数据检测恶性乳腺病变的困境

    

    从筛查乳房X线照片中检测恶性病变的方法通常是使用具有完全注释的数据集进行训练，其中图像被标记为癌症病变的定位和分类。然而，真实世界的筛查乳房X线照片数据集通常有一个部分是完全注释的，另一个部分只有全局分类的弱注释（即没有病变定位）。鉴于这类数据集的庞大规模，研究人员通常在弱注释子集面临两难选择：要么不使用它，要么完全注释它。第一种选择会降低检测准确性，因为它没有使用整个数据集，而第二种选择则过于昂贵，因为注释需要专业放射科医师完成。在本文中，我们提出了这一困境的一个中间解决方案，即将训练构建为一个我们称之为恶性

    arXiv:2301.13418v3 Announce Type: replace-cross  Abstract: Methods to detect malignant lesions from screening mammograms are usually trained with fully annotated datasets, where images are labelled with the localisation and classification of cancerous lesions. However, real-world screening mammogram datasets commonly have a subset that is fully annotated and another subset that is weakly annotated with just the global classification (i.e., without lesion localisation). Given the large size of such datasets, researchers usually face a dilemma with the weakly annotated subset: to not use it or to fully annotate it. The first option will reduce detection accuracy because it does not use the whole dataset, and the second option is too expensive given that the annotation needs to be done by expert radiologists. In this paper, we propose a middle-ground solution for the dilemma, which is to formulate the training as a weakly- and semi-supervised learning problem that we refer to as malignant
    
[^286]: 通过可交换性和潜变量模型分析注意力机制

    An Analysis of Attention via the Lens of Exchangeability and Latent Variable Models

    [https://arxiv.org/abs/2212.14852](https://arxiv.org/abs/2212.14852)

    通过研究可交换性和潜变量模型，我们建立了对注意力机制工作原理的理论分析，从而揭示了transformers如何实现对长序列进行关系推断以产生理想表示的方法。

    

    利用注意力机制，transformers取得了显著的实证成功。尽管我们直觉地认为transformers通过对长序列进行关系推断来产生理想表示，但我们缺乏对注意力机制如何实现此目标的严格理论。本文观察到，正如在BERT和ViT中一样，输入标记通常是可交换的，因为它们已经包含位置编码。可交换性的概念引入了一个对输入大小不变的潜变量模型，从而使得我们的理论分析成为可能。

    arXiv:2212.14852v3 Announce Type: replace  Abstract: With the attention mechanism, transformers achieve significant empirical successes. Despite the intuitive understanding that transformers perform relational inference over long sequences to produce desirable representations, we lack a rigorous theory on how the attention mechanism achieves it. In particular, several intriguing questions remain open: (a) What makes a desirable representation? (b) How does the attention mechanism infer the desirable representation within the forward pass? (c) How does a pretraining procedure learn to infer the desirable representation through the backward pass?   We observe that, as is the case in BERT and ViT, input tokens are often exchangeable since they already include positional encodings. The notion of exchangeability induces a latent variable model that is invariant to input sizes, which enables our theoretical analysis.   - To answer (a) on representation, we establish the existence of a suffic
    
[^287]: 以太坊Ponzi检测的时序元路径特征增强

    Time-aware Metapath Feature Augmentation for Ponzi Detection in Ethereum

    [https://arxiv.org/abs/2210.16863](https://arxiv.org/abs/2210.16863)

    该研究介绍了一种用于以太坊Ponzi检测的时序元路径特征增强方法，旨在捕获交易模式信息中的时间依赖性。

    

    随着强调去中心化的Web 3.0的发展，区块链技术迎来了自己的革命，并带来了许多挑战，特别是在加密货币领域。最近，大量的犯罪行为不断在区块链上出现，如庞氏骗局和钓鱼欺诈，这严重危害了去中心化金融。现有基于图的区块链异常行为检测方法通常侧重于构建同质交易图，而没有区分节点和边的异质性，导致交易模式信息的部分丢失。尽管现有的异质建模方法可以通过元路径描述更丰富的信息，但提取的元路径通常忽视实体之间的时间依赖性，并且不反映真实行为。本文引入了时间感知元路径特征增强（TMFAug）作为一种即插即用模块，用于捕获

    arXiv:2210.16863v2 Announce Type: replace  Abstract: With the development of Web 3.0 which emphasizes decentralization, blockchain technology ushers in its revolution and also brings numerous challenges, particularly in the field of cryptocurrency. Recently, a large number of criminal behaviors continuously emerge on blockchain, such as Ponzi schemes and phishing scams, which severely endanger decentralized finance. Existing graph-based abnormal behavior detection methods on blockchain usually focus on constructing homogeneous transaction graphs without distinguishing the heterogeneity of nodes and edges, resulting in partial loss of transaction pattern information. Although existing heterogeneous modeling methods can depict richer information through metapaths, the extracted metapaths generally neglect temporal dependencies between entities and do not reflect real behavior. In this paper, we introduce Time-aware Metapath Feature Augmentation (TMFAug) as a plug-and-play module to captu
    
[^288]: 朝向通用化的行人轨迹预测系统——G-PECNet

    G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System

    [https://arxiv.org/abs/2210.09846](https://arxiv.org/abs/2210.09846)

    通过结合周期激活函数的启发和使用隐马尔可夫模型（HMMs）和强化学习（RL）进行合成轨迹数据增强，我们的方法G-PECNet在行人轨迹预测任务中取得了9.5%的最终位移误差（FDE）改进。

    

    在本研究中，我们通过使用深度生成模型解决了自主无人机导航中的子问题，即预测域外人类和代理人轨迹。我们的方法——General-PECNet或G-PECNet，通过受周期激活函数启发的架构改进和使用隐马尔可夫模型（HMMs）和强化学习（RL）进行合成轨迹数据增强，在2020年基准PECNet上将最终位移误差（FDE）提高了9.5％。此外，我们提出了一个基于简单几何形状的度量方法，用于轨迹非线性和异常值检测，有助于该任务的完成。

    arXiv:2210.09846v3 Announce Type: replace  Abstract: Navigating dynamic physical environments without obstructing or damaging human assets is of quintessential importance for social robots. In this work, we solve autonomous drone navigation's sub-problem of predicting out-of-domain human and agent trajectories using a deep generative model. Our method: General-PECNet or G-PECNet observes an improvement of 9.5\% on the Final Displacement Error (FDE) on 2020's benchmark: PECNet through a combination of architectural improvements inspired by periodic activation functions and synthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and Reinforcement Learning (RL). Additionally, we propose a simple geometry-inspired metric for trajectory non-linearity and outlier detection, helpful for the task. Code available at https://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git
    
[^289]: 运用XAI方法于基于EEG的系统应用研究

    Toward the application of XAI methods in EEG-based systems

    [https://arxiv.org/abs/2210.06554](https://arxiv.org/abs/2210.06554)

    本文旨在将合适的可解释人工智能（XAI）方法应用于解决脑-计算机接口（BCI）中的数据集转移问题，以提高在情绪识别中的EEG信号分类系统的泛化性能。

    

    众所周知，数据集转移问题的一个有趣案例是在脑-计算机接口（BCI）背景下对脑电图（EEG）信号进行分类。 EEG信号的非静止性可能导致BCI分类系统在不同会话中使用时性能泛化差，甚至是同一被试验。 本文的出发点是，通过利用合适的可解释人工智能（XAI）方法来定位和转换输入的相关特征，从而缓解数据集转移问题。 具体来说，我们专注于对几种XAI方法在在典型的用于情绪识别的EEG数据集上训练的ML系统上生成的解释的实验分析。结果表明，XAI方法找到的许多相关组件在会话之间是共享的，可以用来构建一个具有更好泛化能力的系统。

    arXiv:2210.06554v3 Announce Type: replace-cross  Abstract: An interesting case of the well-known Dataset Shift Problem is the classification of Electroencephalogram (EEG) signals in the context of Brain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to poor generalisation performance in BCI classification systems used in different sessions, also from the same subject. In this paper, we start from the hypothesis that the Dataset Shift problem can be alleviated by exploiting suitable eXplainable Artificial Intelligence (XAI) methods to locate and transform the relevant characteristics of the input for the goal of classification. In particular, we focus on an experimental analysis of explanations produced by several XAI methods on an ML system trained on a typical EEG dataset for emotion recognition. Results show that many relevant components found by XAI methods are shared across the sessions and can be used to build a system able to generalise better. However, re
    
[^290]: 在miniXCOM中探索自适应MCTS与TD学习

    Exploring Adaptive MCTS with TD Learning in miniXCOM

    [https://arxiv.org/abs/2210.05014](https://arxiv.org/abs/2210.05014)

    在miniXCOM中，作者提出了一种改进的自适应MCTS算法MCTS-TD，不需要预训练即可实现在线适应性，展示了其在游戏中表现优异的结果。

    

    在近年来，蒙特卡罗树搜索（MCTS）在游戏社区中得到了广泛应用。与深度强化学习结合使用在许多应用中取得了成功。在各种游戏中实施了这些方法，从简单的棋盘游戏到更复杂的视频游戏，如星际争霸，而深度神经网络的使用则需要大量的训练时间。在这项工作中，我们探讨了MCTS中的在线适应性，而不需要预训练。我们提出了MCTS-TD，这是一种使用时间差异学习改进的自适应MCTS算法。我们在游戏miniXCOM上展示了我们的新方法，这是XCOM的一个简化版本，XCOM是一个包含多款回合制战术游戏的流行商业系列，展示了MCTS-TD中的适应性如何提高了与对手的表现。

    arXiv:2210.05014v3 Announce Type: cross  Abstract: In recent years, Monte Carlo tree search (MCTS) has achieved widespread adoption within the game community. Its use in conjunction with deep reinforcement learning has produced success stories in many applications. While these approaches have been implemented in various games, from simple board games to more complicated video games such as StarCraft, the use of deep neural networks requires a substantial training period. In this work, we explore on-line adaptivity in MCTS without requiring pre-training. We present MCTS-TD, an adaptive MCTS algorithm improved with temporal difference learning. We demonstrate our new approach on the game miniXCOM, a simplified version of XCOM, a popular commercial franchise consisting of several turn-based tactical games, and show how adaptivity in MCTS-TD allows for improved performances against opponents.
    
[^291]: 可识别的潜在因果内容用于隐含协变量转移下的领域自适应

    Identifiable Latent Causal Content for Domain Adaptation under Latent Covariate Shift

    [https://arxiv.org/abs/2208.14161](https://arxiv.org/abs/2208.14161)

    提出了一种新的隐含协变量转移（LCS）范式，增加了领域间的可变性和适应性，并提供了恢复标签变量潜在原因的理论保证。

    

    多源领域自适应（MSDA）解决了利用来自多个源域的标记数据和来自目标域的未标记数据来学习针对未标记目标领域的标签预测函数的挑战。我们提出了一种称为潜在协变量转移（LCS）的新范式，它引入了更大的领域间可变性和适应性。值得注意的是，它为恢复标签变量的潜在原因提供了理论保证。

    arXiv:2208.14161v3 Announce Type: replace  Abstract: Multi-source domain adaptation (MSDA) addresses the challenge of learning a label prediction function for an unlabeled target domain by leveraging both the labeled data from multiple source domains and the unlabeled data from the target domain. Conventional MSDA approaches often rely on covariate shift or conditional shift paradigms, which assume a consistent label distribution across domains. However, this assumption proves limiting in practical scenarios where label distributions do vary across domains, diminishing its applicability in real-world settings. For example, animals from different regions exhibit diverse characteristics due to varying diets and genetics.   Motivated by this, we propose a novel paradigm called latent covariate shift (LCS), which introduces significantly greater variability and adaptability across domains. Notably, it provides a theoretical assurance for recovering the latent cause of the label variable, w
    
[^292]: 使用多变量时间序列下的生成对抗网络自身进行可视化评估

    Visually Evaluating Generative Adversarial Networks Using Itself under Multivariate Time Series

    [https://arxiv.org/abs/2208.02649](https://arxiv.org/abs/2208.02649)

    提出了一种名为高斯GANs的通用框架，用于通过在多变量时间序列生成任务中使用GANs自身进行可视化评估，并提出了使用卡方分布的有效可视化方法。

    

    可视化评估生成的多变量时间序列的好坏在实现上很困难，特别是在生成模型为生成对抗网络（GANs）的情况下。我们提出了一个名为高斯GANs的通用框架，用于在多变量时间序列生成任务中使用GANs自身进行可视化评估。首先，我们尝试通过显式重建GANs的架构来在多变量 Kolmogorov Smirnov（MKS）测试中找到转换函数。其次，在转换后的MST上进行正态性测试，其中高斯GANs作为MKS测试中的转换函数。为了简化正态性测试，提出了使用卡方分布的有效可视化方法。在实验中，我们使用UniMiB数据集，并提供了实证证据表明使用高斯GANs和卡方可视化的正态性测试是有效和可信的。

    arXiv:2208.02649v2 Announce Type: replace  Abstract: Visually evaluating the goodness of generated Multivariate Time Series (MTS) are difficult to implement, especially in the case that the generative model is Generative Adversarial Networks (GANs). We present a general framework named Gaussian GANs to visually evaluate GANs using itself under the MTS generation task. Firstly, we attempt to find the transformation function in the multivariate Kolmogorov Smirnov (MKS) test by explicitly reconstructing the architecture of GANs. Secondly, we conduct the normality test of transformed MST where the Gaussian GANs serves as the transformation function in the MKS test. In order to simplify the normality test, an efficient visualization is proposed using the chi square distribution. In the experiment, we use the UniMiB dataset and provide empirical evidence showing that the normality test using Gaussian GANs and chi sqaure visualization is effective and credible.
    
[^293]: 自然语言上的多步演绎推理：基于超领域泛化的实证研究

    Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation

    [https://arxiv.org/abs/2207.14000](https://arxiv.org/abs/2207.14000)

    提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。

    

    将深度学习与符号逻辑推理结合起来，旨在充分利用这两个领域的成功，并引起了越来越多的关注。受DeepLogic启发，该模型经过端到端训练，用于执行逻辑程序推理，我们介绍了IMA-GloVe-GA，这是一个用自然语言表达的多步推理的迭代神经推理网络。在我们的模型中，推理是使用基于RNN的迭代内存神经网络进行的，其中包含一个门关注机制。我们在PARARULES、CONCEPTRULES V1和CONCEPTRULES V2三个数据集上评估了IMA-GloVe-GA。实验结果表明，带有门关注机制的DeepLogic比DeepLogic和其他RNN基线模型能够实现更高的测试准确性。我们的模型在规则被打乱时比RoBERTa-Large实现了更好的超领域泛化性能。此外，为了解决当前多步推理数据集中推理深度不平衡的问题

    arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
    
[^294]: 半监督预测聚类树用于（分层）多标签分类

    Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification

    [https://arxiv.org/abs/2207.09237](https://arxiv.org/abs/2207.09237)

    提出了一种基于半监督学习的预测聚类树的（分层）多标签分类方法。

    

    半监督学习（SSL）是一种常见的学习预测模型的方法，不仅使用标记的示例，还使用未标记的示例。尽管SSL在简单的分类和回归任务上得到了研究界的关注，但对于具有结构相关变量的复杂预测任务尚未得到充分调查。这种情况出现在多标签分类和分层多标签分类任务中，这可能需要额外的信息，可能来自描述空间中由未标记示例提供的基础分布，以更好地面对挑战性的同时预测多个类标签的任务。在本文中，我们研究了这一方面，并提出了一种基于半监督学习的预测聚类树的（分层）多标签分类方法。我们还将该方法扩展到集成学习，并提出

    arXiv:2207.09237v2 Announce Type: replace-cross  Abstract: Semi-supervised learning (SSL) is a common approach to learning predictive models using not only labeled examples, but also unlabeled examples. While SSL for the simple tasks of classification and regression has received a lot of attention from the research community, this is not properly investigated for complex prediction tasks with structurally dependent variables. This is the case of multi-label classification and hierarchical multi-label classification tasks, which may require additional information, possibly coming from the underlying distribution in the descriptive space provided by unlabeled examples, to better face the challenging task of predicting simultaneously multiple class labels.   In this paper, we investigate this aspect and propose a (hierarchical) multi-label classification method based on semi-supervised learning of predictive clustering trees. We also extend the method towards ensemble learning and propose
    
[^295]: 嵌入控制部分观察系统：具有可证明样本效率的表示学习

    Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency

    [https://arxiv.org/abs/2205.13476](https://arxiv.org/abs/2205.13476)

    论文提出了一种名为Embed to Control（ETC）的强化学习算法，通过在两个级别学习表示的方法来解决部分观察马尔可夫决策问题，以实现样本高效利用。

    

    强化学习在部分观察马尔可夫决策过程（POMDPs）中面临两个挑战。一是通常需要全部历史记录来预测未来，这导致样本复杂度随着时间跨度呈指数级增长。二是观测和状态空间通常是连续的，这导致样本复杂度随外在维数呈指数级增长。为了解决这些挑战，需要通过利用POMDP的结构学习观测和状态历史的最小但足够的表示。为此，我们提出了一种名为Embed to Control (ETC)的强化学习算法，该算法在优化策略的同时学习两个级别的表示。(i)在每一步，ETC学习用低维特征表示状态，这对转移核进行因子分解。(ii)在多个步骤中，ETC学习用低维表示完整历史记录。

    arXiv:2205.13476v2 Announce Type: replace-cross  Abstract: Reinforcement learning in partially observed Markov decision processes (POMDPs) faces two challenges. (i) It often takes the full history to predict the future, which induces a sample complexity that scales exponentially with the horizon. (ii) The observation and state spaces are often continuous, which induces a sample complexity that scales exponentially with the extrinsic dimension. Addressing such challenges requires learning a minimal but sufficient representation of the observation and state histories by exploiting the structure of the POMDP.   To this end, we propose a reinforcement learning algorithm named Embed to Control (ETC), which learns the representation at two levels while optimizing the policy.~(i) For each step, ETC learns to represent the state with a low-dimensional feature, which factorizes the transition kernel. (ii) Across multiple steps, ETC learns to represent the full history with a low-dimensional emb
    
[^296]: ENS-t-SNE: 同时嵌入邻域的t-SNE

    ENS-t-SNE: Embedding Neighborhoods Simultaneously t-SNE

    [https://arxiv.org/abs/2205.11720](https://arxiv.org/abs/2205.11720)

    ENS-t-SNE算法推广了t-SNE方法，通过在3D嵌入中使用不同视角，可以同时可视化高维数据集中的不同类型聚类。

    

    当可视化高维数据集时，通常会采用降维技术，提供数据的单一二维视图。我们描述了ENS-t-SNE：一种用于同时嵌入邻域的算法，它推广了t-随机邻域嵌入方法。通过在ENS-t-SNE的3D嵌入中使用不同的视角，可以在相同的高维数据集中可视化不同类型的聚类。这使得观察者能够看到并跟踪不同类型的聚类，而当提供多个2D嵌入时，难以识别对应点。我们通过真实应用程序展示了ENS-t-SNE的效用，并使用不同类型和大小的数据集进行了广泛的定量评估。

    arXiv:2205.11720v3 Announce Type: replace  Abstract: When visualizing a high-dimensional dataset, dimension reduction techniques are commonly employed which provide a single 2-dimensional view of the data. We describe ENS-t-SNE: an algorithm for Embedding Neighborhoods Simultaneously that generalizes the t-Stochastic Neighborhood Embedding approach. By using different viewpoints in ENS-t-SNE's 3D embedding, one can visualize different types of clusters within the same high-dimensional dataset. This enables the viewer to see and keep track of the different types of clusters, which is harder to do when providing multiple 2D embeddings, where corresponding points cannot be easily identified. We illustrate the utility of ENS-t-SNE with real-world applications and provide an extensive quantitative evaluation with datasets of different types and sizes.
    
[^297]: 将生成的对数进行准则对齐的黑盒知识蒸馏

    Aligning Logits Generatively for Principled Black-Box Knowledge Distillation

    [https://arxiv.org/abs/2205.10490](https://arxiv.org/abs/2205.10490)

    本文提出了一个新的黑盒知识蒸馏方法MEKD，通过将教师和学生模型的低维度对数对齐，实现将一个繁琐模型压缩成轻量级模型。

    

    黑盒知识蒸馏（B2KD）是一个处理云端到边缘模型压缩的问题，其中数据和模型托管在服务器上且无法看见。B2KD面临的挑战包括互联网交换受限和数据分布在边缘和云端之间的不一致。本文提出了一个包括去隐去和蒸馏两步工作流程，并在理论上提供了一个从对数到单元边界的新优化方向，不同于直接对数对齐。在其指导下，我们提出了一种新方法Mapping-Emulation KD（MEKD），将一个黑盒繁琐模型蒸馏成一个轻量级模型。我们的方法不区分软或硬响应处理，并包括：1）去隐去：通过生成器模拟教师函数的逆映射，和2）蒸馏：通过减小高维图像点之间的距离来对齐教师模型和学生模型的低维度对数。

    arXiv:2205.10490v2 Announce Type: replace-cross  Abstract: Black-Box Knowledge Distillation (B2KD) is a formulated problem for cloud-to-edge model compression with invisible data and models hosted on the server. B2KD faces challenges such as limited Internet exchange and edge-cloud disparity of data distributions. In this paper, we formalize a two-step workflow consisting of deprivatization and distillation, and theoretically provide a new optimization direction from logits to cell boundary different from direct logits alignment. With its guidance, we propose a new method Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a lightweight one. Our method does not differentiate between treating soft or hard responses, and consists of: 1) deprivatization: emulating the inverse mapping of the teacher function with a generator, and 2) distillation: aligning low-dimensional logits of the teacher and student models by reducing the distance of high-dimensional image poin
    
[^298]: 基于线性函数逼近的部分观测强化学习及其可证明的样本效率

    Reinforcement Learning from Partial Observation: Linear Function Approximation with Provable Sample Efficiency

    [https://arxiv.org/abs/2204.09787](https://arxiv.org/abs/2204.09787)

    该研究提出了一种基于线性函数逼近的部分观测强化学习算法（OP-TENET），在有限的情节数内实现了$\epsilon$-最优策略，样本复杂度与线性结构的本征维度多项式缩放，与观测和状态空间的大小无关。

    

    我们研究具有无限观测和状态空间的部分观测马尔可夫决策过程（POMDP）的强化学习，在理论上仍然受到较少的研究。为此，我们首次尝试将部分可观测性与具有线性结构的一类POMDP的函数逼近联系起来。具体来说，我们提出了一种强化学习算法（乐观探索通过对抗积分方程或OP-TENET），在$ O（1 / \ epsilon ^ 2）$个情节内实现了$\ epsilon $-最优策略。特别地，样本复杂度在线性结构的本征维度多项式地缩放，并且与观测和状态空间的大小无关。OP-TENET的样本效率由一系列因素实现：（i）具有有限记忆的Bellman算子，以递归方式表示值函数，（ii）识别和估计这样一个算子

    arXiv:2204.09787v3 Announce Type: replace  Abstract: We study reinforcement learning for partially observed Markov decision processes (POMDPs) with infinite observation and state spaces, which remains less investigated theoretically. To this end, we make the first attempt at bridging partial observability and function approximation for a class of POMDPs with a linear structure. In detail, we propose a reinforcement learning algorithm (Optimistic Exploration via Adversarial Integral Equation or OP-TENET) that attains an $\epsilon$-optimal policy within $O(1/\epsilon^2)$ episodes. In particular, the sample complexity scales polynomially in the intrinsic dimension of the linear structure and is independent of the size of the observation and state spaces.   The sample efficiency of OP-TENET is enabled by a sequence of ingredients: (i) a Bellman operator with finite memory, which represents the value function in a recursive manner, (ii) the identification and estimation of such an operator 
    
[^299]: 基于图诱导的局部值函数的分布式多智能体强化学习

    Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions

    [https://arxiv.org/abs/2202.13046](https://arxiv.org/abs/2202.13046)

    通过利用图结构，本文提出了一种通用计算高效的分布式框架，基于局部值函数的分布式RL方法在协作多智能体强化学习中取得了显著的样本复杂性降低。

    

    实现大规模协作多智能体系统的分布式强化学习(RL)具有挑战性，因为：(i)每个智能体只能访问有限的信息；(ii)由于维度诅咒，会出现收敛或计算复杂性问题。本文提出了一种通用的计算高效的协作多智能体强化学习(MARL)分布式框架，通过利用该问题中涉及的图结构。我们引入了描述MARL中三种类型智能体耦合的三个耦合图，分别是状态图、观测图和奖励图。通过进一步考虑通信图，我们提出了两种基于耦合图中派生的局部值函数的分布式RL方法。第一种方法在前述四个图上的特定条件下可以显著降低样本复杂性。第二种

    arXiv:2202.13046v4 Announce Type: replace-cross  Abstract: Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second
    
[^300]: 具有加速客户梯度的通信高效联邦学习

    Communication-Efficient Federated Learning with Accelerated Client Gradient

    [https://arxiv.org/abs/2201.03172](https://arxiv.org/abs/2201.03172)

    通过使用具有前瞻梯度的全局模型，并通过与超调全局模型对齐来规范本地更新，提出了一种简单但有效的联邦学习框架，以改善参与者之间的一致性，促进服务器模型的收敛。

    

    联邦学习常常因参与客户数据集的异质性特征而导致收敛缓慢不稳定。当客户参与率较低时，此趋势会加剧，因为从客户收集的信息具有较大的变化。为了解决这一挑战，我们提出了一个简单但有效的联邦学习框架，改善了客户间的一致性，促进了服务器模型的收敛。通过使服务器广播具有前瞻梯度的全局模型，从而实现了该策略，使所提出的方法能够有效地向参与者传达投影的全局更新信息，而无需额外的客户内存和通信成本。我们还通过将每个客户端与超调全局模型对齐来规范本地更新，以减少偏差并改善算法的稳定性。我们提供了理论收敛率。

    arXiv:2201.03172v2 Announce Type: replace-cross  Abstract: Federated learning often suffers from slow and unstable convergence due to the heterogeneous characteristics of participating client datasets. Such a tendency is aggravated when the client participation ratio is low since the information collected from the clients has large variations. To address this challenge, we propose a simple but effective federated learning framework, which improves the consistency across clients and facilitates the convergence of the server model. This is achieved by making the server broadcast a global model with a lookahead gradient. This strategy enables the proposed approach to convey the projected global update information to participants effectively without additional client memory and extra communication costs. We also regularize local updates by aligning each client with the overshot global model to reduce bias and improve the stability of our algorithm. We provide the theoretical convergence ra
    
[^301]: Wasserstein Flow遇见复制动力学：Actor-Critic中代表学习的均场分析

    Wasserstein Flow Meets Replicator Dynamics: A Mean-Field Analysis of Representation Learning in Actor-Critic

    [https://arxiv.org/abs/2112.13530](https://arxiv.org/abs/2112.13530)

    本文通过均场分析研究了基于特征的神经AC的演化和收敛，提出了一个使用两个学习率更新的AC版本，其中评论家通过大步长进行TD学习更新，演员通过小步长进行PPO更新。

    

    Actor-critic (AC)算法借助神经网络取得了显著的经验成功。然而，目前大部分关于AC算法的理论支持集中在具有线性函数逼近或线性化神经网络的情况下，其中特征表示在整个训练过程中保持不变。这种限制未能捕捉神经AC中代表学习的关键方面，在实际问题中至关重要。本文从均场的角度对基于特征的神经AC的演化和收敛进行了研究。具体地，我们考虑了一个AC的版本，其中演员和评论家由超参数化的两层神经网络表示，并使用两个时间尺度的学习率进行更新。评论家通过较大的步长进行时差（TD）学习更新，而演员通过较小步长进行邻域策略优化（PPO）更新。

    arXiv:2112.13530v2 Announce Type: replace  Abstract: Actor-critic (AC) algorithms, empowered by neural networks, have had significant empirical success in recent years. However, most of the existing theoretical support for AC algorithms focuses on the case of linear function approximations, or linearized neural networks, where the feature representation is fixed throughout training. Such a limitation fails to capture the key aspect of representation learning in neural AC, which is pivotal in practical problems. In this work, we take a mean-field perspective on the evolution and convergence of feature-based neural AC. Specifically, we consider a version of AC where the actor and critic are represented by overparameterized two-layer neural networks and are updated with two-timescale learning rates. The critic is updated by temporal-difference (TD) learning with a larger stepsize while the actor is updated via proximal policy optimization (PPO) with a smaller stepsize. In the continuous-t
    
[^302]: CGCL：无需手工图数据增强的协作式图对比学习

    CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations

    [https://arxiv.org/abs/2111.03262](https://arxiv.org/abs/2111.03262)

    提出了一种新颖的Collaborative Graph Contrastive Learning框架（CGCL），利用多个图编码器观察图形，并避免引入不稳定扰动，保证图的不变性。

    

    未监督的图表示学习是一个非常棘手的问题。在结构化数据的无监督表示学习中对比方法的成功启发了对图类似尝试。现有的图对比学习（GCL）旨在学习跨多个增强视图的不变性，这使其严重依赖于手工制作的图增强。然而，不当的图数据增强可能会危害这种不变性。在本文中，我们展示了不当增强的潜在危险，然后提出了一种新颖的协作式图对比学习框架（CGCL）。该框架利用多个图编码器观察图形。来自不同编码器观察到的特征作为对比学习中的对比视图，避免诱发不稳定的扰动并保证不变性。为了确保不同图编码器之间的协作，

    arXiv:2111.03262v2 Announce Type: replace-cross  Abstract: Unsupervised graph representation learning is a non-trivial topic. The success of contrastive methods in the unsupervised representation learning on structured data inspires similar attempts on the graph. Existing graph contrastive learning (GCL) aims to learn the invariance across multiple augmentation views, which renders it heavily reliant on the handcrafted graph augmentations. However, inappropriate graph data augmentations can potentially jeopardize such invariance. In this paper, we show the potential hazards of inappropriate augmentations and then propose a novel Collaborative Graph Contrastive Learning framework (CGCL). This framework harnesses multiple graph encoders to observe the graph. Features observed from different encoders serve as the contrastive views in contrastive learning, which avoids inducing unstable perturbation and guarantees the invariance. To ensure the collaboration among diverse graph encoders, we
    
[^303]: 回归问题的有效预测区间

    Valid prediction intervals for regression problems

    [https://arxiv.org/abs/2107.00363](https://arxiv.org/abs/2107.00363)

    本文回顾了回归问题中四类预测区间估计方法，并指出了一些方法在不同数据集上性能波动大的原因。

    

    在过去几十年中，针对回归设置提出了各种方法来估计预测区间，包括贝叶斯方法、集成方法、直接区间估计方法和符合预测方法。一个重要问题是这些方法的校准：生成的预测区间应该具有预定义的覆盖水平，而不应该过于保守。在这项工作中，我们从概念和实验角度回顾了上述四类方法。来自各个领域的基准数据集的结果突显出在数据集之间的性能有很大波动。这些观察结果可归因于某些类方法固有假设的违背。我们阐述了如何将符合预测用作没有校准步骤会产生差结果的方法的通用校准过程。

    arXiv:2107.00363v4 Announce Type: replace-cross  Abstract: Over the last few decades, various methods have been proposed for estimating prediction intervals in regression settings, including Bayesian methods, ensemble methods, direct interval estimation methods and conformal prediction methods. An important issue is the calibration of these methods: the generated prediction intervals should have a predefined coverage level, without being overly conservative. In this work, we review the above four classes of methods from a conceptual and experimental point of view. Results on benchmark data sets from various domains highlight large fluctuations in performance from one data set to another. These observations can be attributed to the violation of certain assumptions that are inherent to some classes of methods. We illustrate how conformal prediction can be used as a general calibration procedure for methods that deliver poor results without a calibration step.
    
[^304]: GraphFM：图因子分解机用于特征交互建模

    GraphFM: Graph Factorization Machines for Feature Interaction Modeling

    [https://arxiv.org/abs/2105.11866](https://arxiv.org/abs/2105.11866)

    提出了一种名为GraphFM的图因子分解机方法，通过图结构自然表示特征，并将FM的交互功能集成到GNN的特征聚合策略中，能够模拟任意阶特征交互。

    

    因子分解机（FM）是处理高维稀疏数据时建模成对（二阶）特征交互的一种常见方法。然而，一方面，FM未能捕捉到高阶特征交互，受到组合扩展的影响。另一方面，考虑每对特征之间的交互可能会引入噪声并降低预测准确性。为了解决这些问题，我们提出了一种新方法，称为Graph Factorization Machine（GraphFM），通过将特征自然表示成图结构。具体而言，我们设计了一种机制来选择有益的特征交互，并将其形式化为特征之间的边。然后，所提出的模型将FM的交互功能整合到图神经网络（GNN）的特征聚合策略中，通过堆叠层来模拟图结构特征上的任意阶特征交互。

    arXiv:2105.11866v4 Announce Type: replace-cross  Abstract: Factorization machine (FM) is a prevalent approach to modeling pairwise (second-order) feature interactions when dealing with high-dimensional sparse data. However, on the one hand, FM fails to capture higher-order feature interactions suffering from combinatorial expansion. On the other hand, taking into account interactions between every pair of features may introduce noise and degrade prediction accuracy. To solve the problems, we propose a novel approach, Graph Factorization Machine (GraphFM), by naturally representing features in the graph structure. In particular, we design a mechanism to select the beneficial feature interactions and formulate them as edges between features. Then the proposed model, which integrates the interaction function of FM into the feature aggregation strategy of Graph Neural Network (GNN), can model arbitrary-order feature interactions on the graph-structured features by stacking layers. Experime
    
[^305]: MetaVIM：元变分内在激励强化学习用于分散式交通信号控制

    MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control

    [https://arxiv.org/abs/2101.00746](https://arxiv.org/abs/2101.00746)

    提出了一种新颖的MetaVIM强化学习方法，用于分散式交通信号控制，通过元学习考虑邻居信息，以解决现实世界中交通信号控制面临的挑战。

    

    交通信号控制旨在协调交叉口的交通信号，以改善区域或城市的交通效率。最近，深度强化学习（RL）已被应用于交通信号控制，并表现出有希望的性能，其中每个交通信号被视为一个代理。然而，在现实世界中，仍存在一些可能限制其大规模应用的挑战。为了使从训练场景中学到的策略能够推广到新的未见场景，提出了一种新颖的Meta Variationally Intrinsic Motivated（MetaVIM）RL方法，用于学习考虑邻居信息的每个交叉口的分散式策略。具体来说，我们将策略学习形式化为一个关于一组相关任务的元学习问题，其中每个任务对应于一个交叉口的交通信号控制，其邻居被视为状态的未观察部分。

    arXiv:2101.00746v5 Announce Type: replace-cross  Abstract: Traffic signal control aims to coordinate traffic signals across intersections to improve the traffic efficiency of a district or a city. Deep reinforcement learning (RL) has been applied to traffic signal control recently and demonstrated promising performance where each traffic signal is regarded as an agent. However, there are still several challenges that may limit its large-scale application in the real world. To make the policy learned from a training scenario generalizable to new unseen scenarios, a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method is proposed to learn the decentralized policy for each intersection that considers neighbor information in a latent way. Specifically, we formulate the policy learning as a meta-learning problem over a set of related tasks, where each task corresponds to traffic signal control at an intersection whose neighbors are regarded as the unobserved part of the state. T
    
[^306]: 变分传输：一种用于分布优化的收敛粒子算法

    Variational Transport: A Convergent Particle-BasedAlgorithm for Distributional Optimization

    [https://arxiv.org/abs/2012.11554](https://arxiv.org/abs/2012.11554)

    提出了一种新的基于粒子的算法，名为变分传输，通过迭代地推动一组粒子，在概率分布流形上近似执行沃瑟斯坦梯度下降。

    

    我们考虑最小化一个在概率分布族上定义的函数的优化问题，其中假定目标函数具有变分形式。这种分布优化问题在机器学习和统计学中广泛存在，蒙特卡洛抽样、变分推断、策略优化和生成对抗网络是其中的例子。针对这个问题，我们提出了一种新的基于粒子的算法，名为变分传输，通过迭代地推动一组粒子，在概率分布流形上近似执行沃瑟斯坦梯度下降。具体而言，我们证明沿着函数梯度的测地线方向移动，与对概率分布施加一个推前映射等价于通过推动一组粒子来准确近似实施。

    arXiv:2012.11554v2 Announce Type: replace  Abstract: We consider the optimization problem of minimizing a functional defined over a family of probability distributions, where the objective functional is assumed to possess a variational form. Such a distributional optimization problem arises widely in machine learning and statistics, with Monte-Carlo sampling, variational inference, policy optimization, and generative adversarial network as examples. For this problem, we propose a novel particle-based algorithm, dubbed as variational transport, which approximately performs Wasserstein gradient descent over the manifold of probability distributions via iteratively pushing a set of particles. Specifically, we prove that moving along the geodesic in the direction of functional gradient with respect to the second-order Wasserstein distance is equivalent to applying a pushforward mapping to a probability distribution, which can be approximated accurately by pushing a set of particles. Specif
    
[^307]: 离散时间差分和Q学习能学得特征表示吗？一种平均场理论

    Can Temporal-Difference and Q-Learning Learn Representation? A Mean-Field Theory

    [https://arxiv.org/abs/2006.04761](https://arxiv.org/abs/2006.04761)

    研究探讨离散时间差分学习和Q学习在深度强化学习中的特征表示演变，证明利用过度参数化的方法可以实现这种演变，并关注特征表示对于算法收敛的重要性。

    

    离散时间差分和Q学习在深度强化学习中发挥关键作用，它们利用神经网络等表达力非线性函数逼近器。它们的实证成功的核心是学得的特征表示，将丰富的观测，如图像和文本，嵌入到编码语义结构的潜在空间中。同时，这种特征表示的演变对离散时间差分学习和Q学习的收敛至关重要。特别地，当函数逼近器在特征表示中是线性的且在整个学习过程中保持不变时，离散时间差分学习会收敛，否则可能发散。我们的目标是回答以下问题：当函数逼近器是神经网络时，相关的特征表示如何演进？如果它收敛，它是否收敛至最优的特征表示？

    arXiv:2006.04761v2 Announce Type: replace  Abstract: Temporal-difference and Q-learning play a key role in deep reinforcement learning, where they are empowered by expressive nonlinear function approximators such as neural networks. At the core of their empirical successes is the learned feature representation, which embeds rich observations, e.g., images and texts, into the latent space that encodes semantic structures. Meanwhile, the evolution of such a feature representation is crucial to the convergence of temporal-difference and Q-learning.   In particular, temporal-difference learning converges when the function approximator is linear in a feature representation, which is fixed throughout learning, and possibly diverges otherwise. We aim to answer the following questions: When the function approximator is a neural network, how does the associated feature representation evolve? If it converges, does it converge to the optimal one?   We prove that, utilizing an overparameterized tw
    
[^308]: 在策略优化中实现可证明高效的探索

    Provably Efficient Exploration in Policy Optimization

    [https://arxiv.org/abs/1912.05830](https://arxiv.org/abs/1912.05830)

    OPPO是第一个在探索中高效的策略优化算法，在处理具有线性函数近似、未知转移和对抗性奖励的问题中取得了 $\tilde{O}(\sqrt{d^2 H^3 T} )$ 的遗憾。

    

    虽然基于策略的强化学习（RL）在实践中取得了巨大成功，但在理论上却远不如基于值函数的RL被理解的充分。具体来说，如何设计一个在探索中综合高效的策略优化算法仍然是模糊的。为弥合这一差距，本文提出了一种Proximal Policy Optimization算法的"乐观变体"（OPPO），其遵循“策略梯度方向”的“乐观版本”。本文证明，对于具有线性函数近似、未知转移和具有完全信息反馈的对抗性奖励的基于情节马尔可夫决策过程问题，OPPO实现了 $\tilde{O}(\sqrt{d^2 H^3 T} )$ 的遗憾。其中，$d$ 是特征维度，$H$ 是情节长度，$T$ 是总步数。就我们所知，OPPO是第一个可证明高效的策略优化算法。

    arXiv:1912.05830v4 Announce Type: replace  Abstract: While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper proposes an Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows an ``optimistic version'' of the policy gradient direction. This paper proves that, in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback, OPPO achieves $\tilde{O}(\sqrt{d^2 H^3 T} )$ regret. Here $d$ is the feature dimension, $H$ is the episode horizon, and $T$ is the total number of steps. To the best of our knowledge, OPPO is the first provably efficient policy optimization algorithm that explo
    
[^309]: 自然和医学图像的深度语义分割：一项综述

    Deep Semantic Segmentation of Natural and Medical Images: A Review

    [https://arxiv.org/abs/1910.07655](https://arxiv.org/abs/1910.07655)

    对于自然和医学图像的深度语义分割任务，本综述将深度学习模型分为六大类别，全面审查了各组方法的贡献并讨论了当前方法的局限性和未来研究方向。

    

    语义图像分割任务包括将图像的每个像素分类为一个实例，其中每个实例对应一个类别。这项任务是场景理解概念的一部分，或者更好地解释图像的全局上下文。在医学图像分析领域，图像分割可用于图像引导介入、放射治疗或改善放射诊断。本综述将领先的基于深度学习的医学和非医学图像分割解决方案分类为六大主要组：深度结构、数据合成、损失函数、序列模型、弱监督和多任务方法，并全面审查了每个组中的贡献。此外，对于每个组，我们分析了每个变体，并讨论了当前方法的局限性，提出了语义图像分割的潜在未来研究方向。

    arXiv:1910.07655v4 Announce Type: replace-cross  Abstract: The semantic image segmentation task consists of classifying each pixel of an image into an instance, where each instance corresponds to a class. This task is a part of the concept of scene understanding or better explaining the global context of an image. In the medical image analysis domain, image segmentation can be used for image-guided interventions, radiotherapy, or improved radiological diagnostics. In this review, we categorize the leading deep learning-based medical and non-medical image segmentation solutions into six main groups of deep architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, and multi-task methods and provide a comprehensive review of the contributions in each of these groups. Further, for each group, we analyze each variant of these groups and discuss the limitations of the current approaches and present potential future research directions for semantic image s
    
[^310]: 健康文本简化：消化癌症教育的注释语料库和增强学习的新策略

    Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])

    [http://arxiv.org/abs/2401.15043](http://arxiv.org/abs/2401.15043)

    该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。

    

    目标：健康教育材料的阅读水平显著影响信息的可理解性和可接触性，特别是对于少数族裔人群。许多患者教育资源超过了广泛接受的标准的阅读水平和复杂性。在健康信息中，急需高性能的文本简化模型以增强传播和识字能力。这种需要在癌症教育中尤为迫切，有效的预防和筛查教育可以大大减少发病率和死亡率。方法：我们引入了简化的消化癌症（SimpleDC）并行语料库，用于健康文本简化研究。利用SimpleDC和现有的Med-EASi语料库，我们探索了基于大型语言模型（LLM）的简化方法，包括微调、增强学习（RL）、增强学习与人类反馈（RLHF）、领域自适应和基于提示的应用。

    Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
    
[^311]: 推理的拓扑学：揭秘思维链、树和图

    Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])

    [http://arxiv.org/abs/2401.14295](http://arxiv.org/abs/2401.14295)

    这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。

    

    自然语言处理（NLP）领域近年来取得了显著进展，特别是在通过创新的提示技术提高大型语言模型（LLM）性能方面。其中，与结构相结合的提示工程被视为一种有前途的范式，其设计如思维链、思维树或思维图等，通过结构指导整体LLM推理过程。通过大量实例的说明，这种范式显著增强了LLM在逻辑或数学推理、规划或创造性写作等各种任务中的能力。为了方便理解这个不断发展的领域并为未来的发展铺平道路，我们设计了一个有效和高效的LLM推理方案的通用蓝图。为此，我们对提示执行流程进行了深入分析，澄清并明确定义了不同的概念。然后我们建立第一个分类系统

    The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
    
[^312]: TelME：教师导向的多模融合网络用于对话中的情绪识别

    TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation. (arXiv:2401.12987v1 [cs.CL])

    [http://arxiv.org/abs/2401.12987](http://arxiv.org/abs/2401.12987)

    TelME是一种教师导向的多模融合网络，通过跨模态知识蒸馏实现对话中情绪识别的优化，取得了在多说话人数据集MELD上的最先进性能。

    

    对话中的情绪识别在使对话系统能够有效回应用户请求方面起着至关重要的作用。对话中的情绪可以通过音频、视觉和文本等多种模态的表示进行识别。然而，由于非语言模态对识别情绪的贡献较弱，多模态情绪识别一直被认为是一项具有挑战性的任务。本文提出了一种用于对话中情绪识别的教师导向多模融合网络（TelME）。TelME通过跨模态知识蒸馏将信息从作为教师的语言模型传递给非语言的学生，从而优化了弱模态的效能。然后，我们采用一种移动融合方法将多模态特征组合起来，其中学生网络支持教师。TelME在MELD（一种用于对话情绪识别的多说话人数据集）上实现了最先进的性能。最后，我们通过额外的实验论证了我们组件的有效性。

    Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the non-verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional expe
    
[^313]: 学习神经网络的双峰现象

    The twin peaks of learning neural networks. (arXiv:2401.12610v1 [cs.LG])

    [http://arxiv.org/abs/2401.12610](http://arxiv.org/abs/2401.12610)

    该论文研究了神经网络的双峰现象，发现高度过参数化的模型可以避免过拟合并实现良好的测试性能，与传统的偏差-方差折衷法则不同。研究分析了布尔均值维度（BMD）与网络复杂性和敏感性之间的关系，得到了在高维度范围内BMD的可解释表达式，发现BMD在网络过参数化程度增加时达到极值点。

    

    最近的研究表明，在神经网络的泛化误差方面存在双峰现象，即高度过参数化的模型可以避免过拟合并实现良好的测试性能，与统计学习理论描述的标准偏差-方差折衷法则不符。在本研究中，我们探讨了这一现象与神经网络所表示的函数的复杂性和敏感性增加之间的联系。具体而言，我们研究了布尔均值维度（BMD），这是在布尔函数分析背景下发展起来的一种度量。针对随机特征模型的简单教师-学生设置，我们基于副本方法进行理论分析，得到了一个可解释的BMD表达式，其中数据点的数量、特征的数量和输入大小在高维度范围内不断增长。我们发现，随着网络过参数化程度的增加，BMD达到一个极值点。

    Recent works demonstrated the existence of a double-descent phenomenon for the generalization error of neural networks, where highly overparameterized models escape overfitting and achieve good test performance, at odds with the standard bias-variance trade-off described by statistical learning theory. In the present work, we explore a link between this phenomenon and the increase of complexity and sensitivity of the function represented by neural networks. In particular, we study the Boolean mean dimension (BMD), a metric developed in the context of Boolean function analysis. Focusing on a simple teacher-student setting for the random feature model, we derive a theoretical analysis based on the replica method that yields an interpretable expression for the BMD, in the high dimensional regime where the number of data points, the number of features, and the input size grow to infinity. We find that, as the degree of overparameterization of the network is increased, the BMD reaches an ev
    
[^314]: GI-PIP：梯度反转攻击是否需要不切实际的辅助数据集？

    GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?. (arXiv:2401.11748v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.11748](http://arxiv.org/abs/2401.11748)

    本文提出了一种新颖的梯度反转攻击方法GI-PIP，不需要依赖不切实际的辅助数据集，通过利用异常检测模型从较少的数据中捕获底层分布，并能在图像恢复方面表现出优异的能力，同时在分布泛化方面也更强大。

    

    深度梯度反转攻击通过准确地恢复共享梯度中的隐私数据，对联邦学习构成了严重威胁。然而，现有技术在访问过多的辅助数据方面依赖于不切实际的假设，这违反了联邦学习的基本数据分区原则。本文提出了一种新颖的方法，即使用实用图像先验的梯度反转攻击（GI-PIP），在经过修订的威胁模型下。GI-PIP利用异常检测模型从更少的数据中捕获底层分布，而基于GAN的方法需要消耗更多的数据来合成图像。然后，利用提取出的分布来调节攻击过程作为异常得分损失。实验结果表明，GI-PIP只使用了ImageNet数据的3.8%即可实现16.12 dB的PSNR恢复，而基于GAN的方法则需要超过70%的数据。此外，与基于GAN的方法相比，GI-PIP在分布泛化方面表现出更强大的能力。

    Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients. However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL. In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images. The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss. Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8% data of ImageNet, while GAN-based methods necessitate over 70%. Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods. Our approach signif
    
[^315]: 量子机器学习：从NISQ到容错。

    Quantum Machine Learning: from NISQ to Fault Tolerance. (arXiv:2401.11351v1 [quant-ph])

    [http://arxiv.org/abs/2401.11351](http://arxiv.org/abs/2401.11351)

    本文提供了对量子机器学习领域的全面回顾，涵盖了在NISQ技术和容错量子计算硬件上使用的技术和算法，并深入讨论了与量子机器学习相关的基本概念和统计学习理论。

    

    量子机器学习是在量子设备上运行机器学习算法的过程，在学术界和商业界引起了广泛关注。本文对量子机器学习领域涌现的各种概念进行了全面而公正的回顾。这包括在噪声中间尺度量子（NISQ）技术中使用的技术，以及与容错量子计算硬件兼容的算法方法。我们的回顾涵盖了与量子机器学习相关的基本概念、算法和统计学习理论。

    Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.
    
[^316]: SymTC:一种用于腰椎MRI实例分割的共生Transformer-CNN网络

    SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])

    [http://arxiv.org/abs/2401.09627](http://arxiv.org/abs/2401.09627)

    SymTC是一种新颖的腰椎MR图像分割模型，通过将Transformer和CNN相结合，并利用位置嵌入和自注意力模块，实现了更准确的实例分割。

    

    椎间盘疾病是一种常见病症，经常导致间歇性或持续性的腰背疼痛，对该疾病的诊断和评估依赖于腰椎MR图像中椎骨和椎间盘几何形状的准确测量。深度神经网络（DNN）模型可以帮助临床医生以更高效的方式自动化地对腰椎的个体实例（椎骨和椎间盘）进行图像分割，这被称为实例图像分割。在这项工作中，我们提出了SymTC，一种创新的腰椎MR图像分割模型，它结合了Transformer和卷积神经网络（CNN）的优势。具体而言，我们设计了一个并行的双路径架构来融合CNN层和Transformer层，并在Transformer的自注意力模块中集成了一种新颖的位置嵌入，增强了位置信息的利用以实现更准确的分割。为了进一步提高模型的性能，我们引入了一个新的定位系统进行模型优化。

    Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new
    
[^317]: 旗帜游戏：通过旗帜流形来获得鲁棒的主方向

    Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])

    [http://arxiv.org/abs/2401.04071](http://arxiv.org/abs/2401.04071)

    本研究提出了一种统一的PCA和其变种框架，该框架基于线性子空间旗帜，并引入了对异常值和数据流形的考虑。通过在旗帜流形上进行优化问题的求解，结合主测地线近似，提出了一系列新的降维算法。

    

    主成分分析（PCA）及其对流形和异常数据的扩展，在计算机视觉和机器学习中是不可或缺的。本研究提出了PCA及其变种的统一形式，引入了基于线性子空间旗帜的框架，即逐渐增加维度的嵌套线性子空间的层次结构，不仅允许共同实现，还产生了新的未曾探索的变种。我们从广义化传统的PCA方法开始，这些方法要么最大化方差，要么最小化重构误差。我们扩展这些解释，通过考虑异常值和数据流形，开发出了大量新的降维算法。为了设计一种通用的计算方法，我们将鲁棒和对偶形式的PCA重新构建为在旗帜流形上的优化问题。然后，我们将主测地线近似（切线PCA）整合到这个基于旗帜的框架中，创造出一种新的方法。

    Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
    
[^318]: 在目标识别中将地理多样知识融入提示以提高地理鲁棒性

    Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])

    [http://arxiv.org/abs/2401.01482](http://arxiv.org/abs/2401.01482)

    本文研究了在目标识别中将地理多样知识融入提示以提高地理鲁棒性的方法，探索了通过大型语言模型获取地理特定对象知识并结合CLIP视觉语言模型的零样本和可学习软提示。通过提出一种地理知识正则化方法，实现了从源地理位置推广到未见目标地理位置的鲁棒性提升。

    

    现有的目标识别模型在不同地理情景下缺乏鲁棒性，这是由于设计和环境中存在重要的领域转移。为了更准确地反映这些转移下的对象概念，需要调整类别表示。在缺乏目标地理位置训练数据的情况下，我们假设可以利用地理特定的对象类别描述性知识来增强鲁棒性。为此，我们探索了通过探测大型语言模型的地理特定对象知识的可行性，并研究了在CLIP视觉语言模型中集成知识的零样本和可学习软提示。特别地，我们提出了一种地理知识正则化方法，以确保在一组源地理位置上训练的软提示能够推广到未见过的目标地理位置集合。当仅依赖来自欧洲的数据进行训练时，我们在DollarStreet上的增益达到了+2.8个国家。

    Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries fro
    
[^319]: Score Distillation在文本到3D生成中解决模式崩溃的方法

    Taming Mode Collapse in Score Distillation for Text-to-3D Generation. (arXiv:2401.00909v1 [cs.CV])

    [http://arxiv.org/abs/2401.00909](http://arxiv.org/abs/2401.00909)

    本文揭示了现有的基于Score Distillation的文本到3D生成框架存在模式崩溃问题，通过在变分目标中加入熵项来改进生成的3D模型的多样性，解决了Janus伪像问题。

    

    尽管Score Distillation在文本到3D生成中表现出色，但这些技术普遍存在视图一致性问题，也被称为“Janus”伪像，即生成的对象伪造了多个前视图。虽然已经有一些经验有效的方法通过去偏置或者引导工程来解决这个问题，但对于这个问题的更严格的解释和解决方法仍然很难找到。在本文中，我们揭示了现有的基于Score Distillation的文本到3D生成框架陷入了在每个视图上独立最大似然求解的最大似然模式崩溃问题，实践中表现为Janus伪像。为了避免模式崩溃，我们通过在相应变分目标中重新引入熵项，对渲染图像的分布进行改进。最大化熵鼓励生成的3D模型在不同视图之间具有多样性，从而缓解了模式崩溃问题。

    Despite the remarkable performance of score distillation in text-to-3D generation, such techniques notoriously suffer from view inconsistency issues, also known as "Janus" artifact, where the generated objects fake each view with multiple front faces. Although empirically effective methods have approached this problem via score debiasing or prompt engineering, a more rigorous perspective to explain and tackle this problem remains elusive. In this paper, we reveal that the existing score distillation-based text-to-3D generation frameworks degenerate to maximal likelihood seeking on each view independently and thus suffer from the mode collapse problem, manifesting as the Janus artifact in practice. To tame mode collapse, we improve score distillation by re-establishing in entropy term in the corresponding variational objective, which is applied to the distribution of rendered images. Maximizing the entropy encourages diversity among different views in generated 3D assets, thereby mitiga
    
[^320]: 针对分子图的长距离神经原子学习

    Long-Range Neural Atom Learning for Molecular Graphs. (arXiv:2311.01276v1 [cs.LG])

    [http://arxiv.org/abs/2311.01276](http://arxiv.org/abs/2311.01276)

    这项研究提出了一种针对分子图的长程神经原子学习方法，通过将原子投射为神经原子并在其之间交换信息，实现了远距离节点之间的通信，缩小了任意节点对的相互作用范围。

    

    图神经网络（GNN）已广泛应用于药物发现中的分子图。然而，当前的GNN主要擅长利用短程相互作用（SRI），但难以捕捉长程相互作用（LRI），这两者对于确定分子性质都至关重要。为解决这个问题，我们提出了一种将所有原子隐式投射为少数神经原子的方法，这些神经原子抽象出分子内原子组的集体信息。具体而言，我们明确地在神经原子之间交换信息，并将其作为一种增强将其重新投射到原子的表示上。通过这种机制，神经原子在远距离节点之间建立通信通道，有效地将任意节点对的相互作用范围减少到单次跳跃。为了从物理角度对我们的方法进行审查，我们揭示了它与传统的LRI计算方法Ewald求和的联系。我们进行了大量实验验证

    Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few Neural Atoms, which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms' representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiment
    
[^321]: 通过离散扩散学习无监督的自动驾驶世界模型

    Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])

    [http://arxiv.org/abs/2311.01017](http://arxiv.org/abs/2311.01017)

    本论文提出了一种通过离散扩散学习无监督的自动驾驶世界模型的新方法，通过使用VQVAE对传感器观察进行标记化并通过离散扩散预测未来，我们的模型在点云观察中实现了显著改进，将1秒预测的SOTA Chamfer距离降低了65%以上。

    

    学习世界模型可以以无监督的方式教会智能体世界的运作方式。尽管它可以看作是序列建模的特殊情况，但在自动驾驶等机器人应用中，与使用生成预训练转换器（GPT）扩展语言模型相比，扩展世界模型的进展相对较慢。我们指出了两个主要瓶颈：处理复杂和无结构的观察空间以及具有可扩展性的生成模型。因此，我们提出了一种新颖的世界建模方法，首先使用VQVAE对传感器观察进行标记化，然后通过离散扩散预测未来。为了有效地并行解码和去噪标记，我们将遮蔽生成图像转换器转换为离散扩散框架，并进行了一些简单的改进，取得了显着的改进效果。当应用于点云观察的世界模型学习时，我们的模型将1秒预测的SOTA Chamfer距离降低了65%以上。

    Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
    
[^322]: 球面上的内在高斯向量场

    Intrinsic Gaussian Vector Fields on Manifolds. (arXiv:2310.18824v1 [stat.ML])

    [http://arxiv.org/abs/2310.18824](http://arxiv.org/abs/2310.18824)

    本文提出了一种新型的在流形上处理矢量值信号的高斯过程模型，具有内在定义和考虑空间几何的特点，并为部署在二维球面和超曲面上的Hodge-Mat\'ern高斯向量场提供了计算基元。

    

    从机器人技术到气候科学等各种应用都需要对非欧几里得域（如球面）上的信号进行建模。最近，在流行度量空间上提出了高斯过程模型，尤其是在需要进行不确定性量化的任务中。在流形设置中，与标量值信号相比，矢量值信号可能表现出截然不同的行为，迄今为止的大部分进展都集中在对前者进行建模。然而，对于许多应用，如对未知动力系统的风速或力场进行建模，后者至关重要。本文提出了一种在流形上为矢量值信号提供内在定义并考虑空间几何的新型高斯过程模型。我们提供了部署所得到的Hodge-Mat\'ern高斯向量场在二维球面和超曲面上所需的计算基元。此外，我们还强调了两个推广方向：离散的二维网格和”ide“（暂且译为：想法）。

    Various applications ranging from robotics to climate science require modeling signals on non-Euclidean domains, such as the sphere. Gaussian process models on manifolds have recently been proposed for such tasks, in particular when uncertainty quantification is needed. In the manifold setting, vector-valued signals can behave very differently from scalar-valued ones, with much of the progress so far focused on modeling the latter. The former, however, are crucial for many applications, such as modeling wind speeds or force fields of unknown dynamical systems. In this paper, we propose novel Gaussian process models for vector-valued signals on manifolds that are intrinsically defined and account for the geometry of the space in consideration. We provide computational primitives needed to deploy the resulting Hodge-Mat\'ern Gaussian vector fields on the two-dimensional sphere and the hypertori. Further, we highlight two generalization directions: discrete two-dimensional meshes and "ide
    
[^323]: 具有重尾奖励的强化学习离线策略评估和优化的鲁棒性提升

    Robust Offline Policy Evaluation and Optimization with Heavy-Tailed Rewards. (arXiv:2310.18715v1 [cs.LG])

    [http://arxiv.org/abs/2310.18715](http://arxiv.org/abs/2310.18715)

    本文提出的ROAM和ROOM算法框架通过将中位数法与离线强化学习策略相结合，提供了对重尾奖励的直接不确定性估计，从而增强了离线强化学习在现实应用中的鲁棒性。

    

    本文旨在增强离线强化学习在现实世界应用中普遍存在的重尾奖励情况下的鲁棒性。我们提出了两个算法框架，ROAM和ROOM，用于鲁棒的离线策略评估和离线策略优化。我们的框架的核心是将中位数法与离线强化学习策略相结合，能够对值函数估计器进行直接的不确定性估计。这不仅符合离线策略优化中的保守主义原则，而且灵活处理重尾奖励。理论结果和广泛的实验证明，我们的两个框架在记录的数据集中展示了具有重尾奖励分布时超越现有方法的性能。

    This paper endeavors to augment the robustness of offline reinforcement learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent circumstance in real-world applications. We propose two algorithmic frameworks, ROAM and ROOM, for robust off-policy evaluation (OPE) and offline policy optimization (OPO), respectively. Central to our frameworks is the strategic incorporation of the median-of-means method with offline RL, enabling straightforward uncertainty estimation for the value function estimator. This not only adheres to the principle of pessimism in OPO but also adeptly manages heavy-tailed rewards. Theoretical results and extensive experiments demonstrate that our two frameworks outperform existing methods on the logged dataset exhibits heavy-tailed reward distributions.
    
[^324]: 一种鲁棒神经ODE的极小极大优化控制方法

    A minimax optimal control approach for robust neural ODEs. (arXiv:2310.17584v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2310.17584](http://arxiv.org/abs/2310.17584)

    本文提出了一种鲁棒神经ODE的极小极大优化控制方法，将对抗训练问题转化为一个最优控制问题，并提供了一种新的加权技术来实现鲁棒训练。

    

    本文从鲁棒控制的角度解决了神经ODE的对抗训练问题。这是一种替代经验风险最小化的训练方法，被广泛用于确保对输入扰动的可靠结果。神经ODE允许将深度神经网络解释为控制系统的离散化，从控制理论中提供了强大的工具，用于机器学习的开发和理解。在这种特定情况下，我们将扰动数据的对抗训练公式化为极小极大优化控制问题，并推导出了Pontryagin最大值原理的一阶最优性条件。我们提供了鲁棒训练的新解释，导致了一种替代的加权技术，并在低维分类任务上进行了测试。

    In this paper, we address the adversarial training of neural ODEs from a robust control perspective. This is an alternative to the classical training via empirical risk minimization, and it is widely used to enforce reliable outcomes for input perturbations. Neural ODEs allow the interpretation of deep neural networks as discretizations of control systems, unlocking powerful tools from control theory for the development and the understanding of machine learning. In this specific case, we formulate the adversarial training with perturbed data as a minimax optimal control problem, for which we derive first order optimality conditions in the form of Pontryagin's Maximum Principle. We provide a novel interpretation of robust training leading to an alternative weighted technique, which we test on a low-dimensional classification task.
    
[^325]: 超越神经网络：模型复杂性的经验探索

    Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity. (arXiv:2310.17247v1 [cs.LG])

    [http://arxiv.org/abs/2310.17247](http://arxiv.org/abs/2310.17247)

    本文发现神经网络中的grokking现象不仅局限于神经网络，还出现在其他算法和模型中。通过在数据集中添加虚假信息的维度，可以诱发grokking现象。研究表明，grokking现象在解决方案搜索受复杂性和错误指导的任何情况下可能发生。这对理解grokking现象提供了更广泛的理论支持。

    

    在某些情况下，神经网络展现出一种称为“grokking”的现象，即它们在验证集上实现完美或接近完美的准确度，而在训练集上则早已达到相同的性能。本文发现，grokking不仅限于神经网络，还出现在其他设置中，例如高斯过程（GP）分类、GP回归和线性回归。我们还发现了一种通过添加包含虚假信息的维度来诱发基于算法的数据集中的grokking现象的机制。非神经结构中的这种现象的存在证明了grokking不局限于SGD或权重范数正则化。相反，grokking可能发生在任何由复杂性和错误指导解决方案搜索的情况中。基于这一洞察和我们在贝叶斯神经网络（BNN）和GP回归模型的训练轨迹中观察到的进一步趋势，我们在grokking的更一般的理论方面取得了进展。

    In some settings neural networks exhibit a phenomenon known as grokking, where they achieve perfect or near-perfect accuracy on the validation set long after the same performance has been achieved on the training set. In this paper, we discover that grokking is not limited to neural networks but occurs in other settings such as Gaussian process (GP) classification, GP regression and linear regression. We also uncover a mechanism by which to induce grokking on algorithmic datasets via the addition of dimensions containing spurious information. The presence of the phenomenon in non-neural architectures provides evidence that grokking is not specific to SGD or weight norm regularisation. Instead, grokking may be possible in any setting where solution search is guided by complexity and error. Based on this insight and further trends we see in the training trajectories of a Bayesian neural network (BNN) and GP regression model, we make progress towards a more general theory of grokking. Spe
    
[^326]: 估计可信赖和安全的最佳治疗方案

    Estimating Trustworthy and Safe Optimal Treatment Regimes. (arXiv:2310.15333v1 [cs.LG])

    [http://arxiv.org/abs/2310.15333](http://arxiv.org/abs/2310.15333)

    这篇论文提出了一种安全和可解释的框架，通过匹配患者的医学和药物特性来识别最佳治疗方案。研究结果表明，个性化的治疗策略可以根据患者的病史和药物特征来制定，并发现减少药物剂量可以减轻病情而不会对治疗效果产生负面影响。

    

    最近的统计学和强化学习方法显著推动了患者护理策略的发展。然而，在高风险环境中，这些方法面临着很大的挑战，包括缺失数据、固有的随机性以及对解释性和患者安全性的重要要求。我们的工作运用了一种安全且可解释的框架来识别最佳治疗方案。这种方法涉及将具有相似医学和药物特征的患者进行匹配，从而通过插值构建最佳政策。我们进行了全面的模拟研究，以展示该框架即使在复杂环境中也能够识别最佳政策的能力。最终，我们将我们的方法应用于研究对重症患者进行癫痫治疗的方案。我们的发现强烈支持基于患者的医疗历史和药物特征的个性化治疗策略。值得注意的是，我们发现减少药物剂量可以减轻病情而不会对治疗的效果产生负面影响。

    Recent statistical and reinforcement learning methods have significantly advanced patient care strategies. However, these approaches face substantial challenges in high-stakes contexts, including missing data, inherent stochasticity, and the critical requirements for interpretability and patient safety. Our work operationalizes a safe and interpretable framework to identify optimal treatment regimes. This approach involves matching patients with similar medical and pharmacological characteristics, allowing us to construct an optimal policy via interpolation. We perform a comprehensive simulation study to demonstrate the framework's ability to identify optimal policies even in complex settings. Ultimately, we operationalize our approach to study regimes for treating seizures in critically ill patients. Our findings strongly support personalized treatment strategies based on a patient's medical history and pharmacological features. Notably, we identify that reducing medication doses for 
    
[^327]: 多个专家学习推迟的原则方法

    Principled Approaches for Learning to Defer with Multiple Experts. (arXiv:2310.14774v1 [cs.LG])

    [http://arxiv.org/abs/2310.14774](http://arxiv.org/abs/2310.14774)

    我们研究了多个专家学习推迟问题的代理损失和算法，并证明了这些代理损失函数具有强H一致性界限。我们展示了几个实际应用的代理损失函数，并设计了基于最小化这些损失函数的新的学习推迟算法。我们还进行了在SVHN和CIFAR-10数据集上的实验。

    

    我们提出了一项关于使用多个专家学习推迟问题的代理损失和算法的研究。我们首先引入了一类专门针对多专家设置的代理损失函数，其中预测和推迟函数同时学习。然后，我们证明了这些代理损失函数受益于强H一致性界限。我们通过几个实际代理损失函数的示例展示了我们分析的应用，并给出了明确的保证。这些损失函数直接导致了基于它们最小化的新的学习推迟算法的设计。虽然本工作的主要重点是理论分析，但我们还报告了在SVHN和CIFAR-10数据集上的多个实验结果。

    We present a study of surrogate losses and algorithms for the general problem of learning to defer with multiple experts. We first introduce a new family of surrogate losses specifically tailored for the multiple-expert setting, where the prediction and deferral functions are learned simultaneously. We then prove that these surrogate losses benefit from strong $H$-consistency bounds. We illustrate the application of our analysis through several examples of practical surrogate losses, for which we give explicit guarantees. These loss functions readily lead to the design of new learning to defer algorithms based on their minimization. While the main focus of this work is a theoretical analysis, we also report the results of several experiments on SVHN and CIFAR-10 datasets.
    
[^328]: 预测-拒绝多类放弃：理论分析和算法

    Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms. (arXiv:2310.14772v1 [cs.LG])

    [http://arxiv.org/abs/2310.14772](http://arxiv.org/abs/2310.14772)

    我们研究了多类别分类设置中的学习与放弃框架，并提出了一系列新的理论和算法结果，解决了两个现存的开放问题。这些保证为基于最小化放弃损失的新的多类别放弃算法提供了启示。

    

    我们研究了多类别分类设置中的学习与放弃框架。在这种设置中，学习者可以选择以一定的预定义成本放弃进行预测。我们提出了一系列新的理论和算法结果，解决了预测-拒绝框架下的学习问题。我们引入了几个新的替代损失函数家族，证明了强非渐进和假设集特定的一致性保证，从而积极地解决了两个现存的开放问题。这些保证提供了放弃损失函数的估计误差的上界，与替代损失的误差相关。我们分析了同时学习预测器和拒绝器的单阶段设置，以及在应用中至关重要的两阶段设置，在第一阶段使用标准替代损失函数如交叉熵来学习预测器。这些保证为基于最小化放弃损失的新的多类别放弃算法提供了启示。

    We study the key framework of learning with abstention in the multi-class classification setting. In this setting, the learner can choose to abstain from making a prediction with some pre-defined cost. We present a series of new theoretical and algorithmic results for this learning problem in the predictor-rejector framework. We introduce several new families of surrogate losses for which we prove strong non-asymptotic and hypothesis set-specific consistency guarantees, thereby resolving positively two existing open questions. These guarantees provide upper bounds on the estimation error of the abstention loss function in terms of that of the surrogate loss. We analyze both a single-stage setting where the predictor and rejector are learned simultaneously and a two-stage setting crucial in applications, where the predictor is learned in a first stage using a standard surrogate loss such as cross-entropy. These guarantees suggest new multi-class abstention algorithms based on minimizing
    
[^329]: 基于分数的多类放弃的理论基础损失函数和算法

    Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention. (arXiv:2310.14770v1 [cs.LG])

    [http://arxiv.org/abs/2310.14770](http://arxiv.org/abs/2310.14770)

    本文提出了基于分数的多类放弃的理论基础损失函数和算法，包括引入了新的代理损失函数族群以及证明了这些代理损失的一致性保证。我们通过实验证明了这些算法的实际意义。

    

    学习中的放弃是一种重要的场景，学习者可以选择在某个代价下放弃进行预测。本文在多类别分类的设置下分析了基于分数的学习中的放弃。我们引入了放弃损失函数的新代理损失族群，其中包括单阶段设置中最先进的代理损失以及二阶段设置中的新型损失函数。我们证明了这些代理损失的强非渐近和假设集特定的一致性保证，这些保证上界了放弃损失函数的估计误差，与代理损失的估计误差相关。我们的上界可以帮助比较不同的基于分数的代理损失，指导通过最小化提出的代理损失函数来设计新的放弃算法。我们在CIFAR-10、CIFAR-100和SVHN数据集上对我们的新算法进行了实验评估，展示了我们的新代理损失函数的实际意义。

    Learning with abstention is a key scenario where the learner can abstain from making a prediction at some cost. In this paper, we analyze the score-based formulation of learning with abstention in the multi-class classification setting. We introduce new families of surrogate losses for the abstention loss function, which include the state-of-the-art surrogate losses in the single-stage setting and a novel family of loss functions in the two-stage setting. We prove strong non-asymptotic and hypothesis set-specific consistency guarantees for these surrogate losses, which upper-bound the estimation error of the abstention loss function in terms of the estimation error of the surrogate loss. Our bounds can help compare different score-based surrogates and guide the design of novel abstention algorithms by minimizing the proposed surrogate losses. We experimentally evaluate our new algorithms on CIFAR-10, CIFAR-100, and SVHN datasets and the practical significance of our new surrogate losse
    
[^330]: 自发性模块化结构：密集预训练Transformer能否从自发模块化结构中获益？

    Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])

    [http://arxiv.org/abs/2310.10908](http://arxiv.org/abs/2310.10908)

    该论文研究了密集预训练Transformer是否以及如何从自发的模块化结构中获益。

    

    将模块化设计引入神经网络能够展示出较好的泛化能力和学习效率等优点。现有的模块化神经网络通常是“显式”的，因为它们的模块化架构是预先定义的，每个模块都被期望实现不同的功能。相反，最近的研究表明在标准的预训练Transformer中存在“隐式”的模块化结构，即“自发模块化”。他们表明这样的模块化结构在早期预训练阶段就会出现，并且完全是自发的。然而，大多数Transformer模型仍然被视为单体模型，没有充分利用其模块化的特性。因此，鉴于显式模块化架构的优良特性，我们探索了密集预训练Transformer是否以及如何从自发模块化结构中获益的问题。

    Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (E
    
[^331]: 人类课程指导下的指令调整

    Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])

    [http://arxiv.org/abs/2310.09518](http://arxiv.org/abs/2310.09518)

    本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。

    

    指令调整的主流范式是随机洗牌训练最大多样化指令-响应对。本文探讨了在当代大型语言模型如ChatGPT和GPT-4中应用结构化认知学习方法进行指令调整的潜在好处。与以往传统的随机化指令数据集不同，我们提出了一个高度结构化的合成数据集，模拟了人类教育的渐进性和有组织性。我们通过将数据集与教育框架对齐来策划我们的数据集，为每个样本包括主题和认知严谨程度等元信息。我们的数据集涵盖了从中学到研究生阶段的全面细粒度主题，每个主题都有各种问题，以利用布鲁姆的认知分级法提高概念深度，该分级法用于区分每个概念的不同人类认知水平。结果表明，这种认知学习方法优于传统的随机化方法，提高了指令调整的性能。

    The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
    
[^332]: Mirage: 图分类的模型无关图蒸馏

    Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])

    [http://arxiv.org/abs/2310.09486](http://arxiv.org/abs/2310.09486)

    Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。

    

    GNNs和其他深度学习模型一样，对数据和计算需求量很大。急需在大型数据集上扩展GNN的训练，以便在资源有限的环境中使用它们。图蒸馏是为此目的而努力，旨在从原始训练数据构建一个更小的合成训练集，而不会显著影响模型性能。虽然初步工作取得了一些进展，但这项工作基于两个关键观察：(1)现有的图蒸馏算法本身依赖于使用完整数据集进行训练，这就破坏了图蒸馏的前提。(2)蒸馏过程对目标GNN架构和超参数具有特异性，因此对建模流程的变化不具备鲁棒性。我们通过设计一种名为Mirage的图分类蒸馏算法来避免这些限制。Mirage建立在一个洞察的基础上，即一个消息传递的GNN将输入图分解为计算的多重集合。

    GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
    
[^333]: 用于采样、优化和提升的通用Ito链的Ito扩散逼近

    Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting. (arXiv:2310.06081v1 [math.OC])

    [http://arxiv.org/abs/2310.06081](http://arxiv.org/abs/2310.06081)

    本文研究了一类广泛的马尔可夫链，即Ito链的Ito扩散逼近。与大多数相关论文不同，我们的链具有各向同性和状态相关的噪声，并可以适用于多种应用场景。我们证明了Ito链与对应的随机微分方程之间的W2-距离的上界。这些结果改进了已有的估计方法，并在某些特殊情况下提供了首次的分析。

    

    本文考虑了一类相当一般和广泛的马尔可夫链，即Ito链，其类似于某些随机微分方程的Euler-Maruyama离散化。我们研究的链是一个统一的理论分析框架。与大多数相关论文中的正态和状态独立噪声不同，我们的链具有几乎任意各向同性和状态相关噪声。此外，我们链的漂移和扩散系数可以是精确的，以涵盖诸如随机梯度Langevin动力学、采样、随机梯度下降或随机梯度提升等广泛的应用。我们证明了Ito链与对应的随机微分方程之间的W2-距离的一个上界。这些结果改进或覆盖了大部分已知的估计。此外，对于某些特殊情况，我们的分析是第一个。

    This work considers a rather general and broad class of Markov chains, Ito chains that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one, as in most related papers. Moreover, our chain's drift and diffusion coefficient can be inexact to cover a wide range of applications such as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic Gradient Boosting. We prove an upper bound for $W_{2}$-distance between laws of the Ito chain and the corresponding Stochastic Differential Equation. These results improve or cover most of the known estimates. Moreover, for some particular cases, our analysis is the first.
    
[^334]: 观测引导的扩散概率模型

    Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])

    [http://arxiv.org/abs/2310.04041](http://arxiv.org/abs/2310.04041)

    提出了观测引导的扩散概率模型（OGDM），通过引入基于条件鉴别器的观测所产生的额外损失项，实现了更准确的负对数似然优化，在函数评估次数有限的推理阶段表现出色。

    

    我们提出了一种新的扩散模型，称为观测引导的扩散概率模型（OGDM），它有效地解决了质量控制和快速采样之间的权衡问题。我们的方法以原则性的方式将观测过程的引导与马尔可夫链相结合，重新建立了训练目标。通过引入基于条件鉴别器的观测所产生的额外损失项，我们使得优化更准确的负对数似然成为可能，尤其是在函数评估次数有限的推理阶段。这种策略使得我们的训练方法即使只用于微调过程也具有优势，并且与各种快速推理策略兼容，因为我们的方法使用完全相同的推理过程产生更好的去噪网络。

    We propose a novel diffusion model called observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling. Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on the conditional discriminator on noise level, which employs Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training method is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly same inferen
    
[^335]: LightSeq：用于长上下文转换器分布式训练的序列级并行ism

    LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])

    [http://arxiv.org/abs/2310.03294](http://arxiv.org/abs/2310.03294)

    LightSeq是用于长上下文转换器分布式训练的一种新方法，它通过序列维度进行分区，与不同注意力头数量的模型架构兼容，并且减少了与Megatron-LM相比的通信量，同时还实现了通信和计算的重叠。

    

    增加大型语言模型（LLM）的上下文长度可以解开基本上新的能力，但也显著增加了训练的内存占用。以往的模型并行系统（例如Megatron-LM）对不同的注意力头进行分区和计算，并行处理，导致大量通信量，因此不能在注意力头数量之外扩展，从而阻碍了其采用。本文提出了一种新方法LightSeq，用于长上下文LLM的训练。LightSeq具有许多显著优势。首先，LightSeq通过序列维度进行分区，因此对于具有不同注意力头数量的模型架构是不可知的，适用于Multi-Head，Multi-Query和Grouped-Query attention等模型。其次，LightSeq与Megatron-LM相比，在流行的LLM上不仅需求少至4.7倍的通信，而且还可以将通信与计算重叠。为了进一步减少训练时间，LightSeq还具有一种新的梯度che

    Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient che
    
[^336]: SE(3)-蛋白质主链生成中的随机流匹配

    SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])

    [http://arxiv.org/abs/2310.02391](http://arxiv.org/abs/2310.02391)

    通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。

    

    通过基于三维刚体运动（即SE(3)群）的流匹配范式，我们引入了一系列具有不断增强建模能力的新型生成模型：FoldFlow，从而实现了对蛋白质主链的准确建模。首先，我们介绍了FoldFlow-Base，一种无需模拟的学习确定性连续时间动力学和匹配不变目标分布的方法。接下来，我们通过引入Riemannian最优传输来加速训练，创建了FoldFlow-OT，从而构建了更简单和稳定的流。最后，我们设计了FoldFlow-SFM，将Riemannian最优传输和无需模拟训练相结合，可以学习SE(3)上的随机连续时间动力学。我们的FoldFlow生成模型家族相比之前的方法具有几个关键优势。

    The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
    
[^337]: 一致性轨迹模型：学习扩散的概率流ODE轨迹

    Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])

    [http://arxiv.org/abs/2310.02279](http://arxiv.org/abs/2310.02279)

    提出了一种一致性轨迹模型（CTM），它可以加速扩散模型的采样，同时通过对抗训练和去噪得分匹配损失的组合来提高性能，并实现了最先进的采样质量。

    

    一致性模型（CM）加速基于得分的扩散模型采样，但以牺牲样本质量为代价，缺乏一种自然的方法来权衡速度和质量。为了解决这个限制，我们提出了一致性轨迹模型（CTM），它是包括CM和基于得分模型在内的泛化模型。CTM训练一个单一的神经网络，可以在单次前向传递中输出得分（即对数密度的梯度），并允许在扩散过程中任意初始和最终时间之间进行不受限制的遍历概率流普通微分方程（ODE）。CTM利用对抗训练和去噪得分匹配损失的有效组合来提高性能，并在CIFAR-10（FID 1.73）和64X64分辨率的ImageNet上实现新的最先进FID。CTM还实现了一系列新的采样方案，包括确定性和随机的ODE解中的长跳跃。

    Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
    
[^338]: 使用FP8格式的高效后训练量化方法

    Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])

    [http://arxiv.org/abs/2309.14592](http://arxiv.org/abs/2309.14592)

    本研究研究了FP8格式在后训练量化中的优势，并开发了一个通用的量化工作流程。实验结果表明，FP8相对于INT8具有更好的工作负载覆盖率和模型准确性。

    

    最近深度学习方法的进展，如LLMs和扩散模型，提出了对改进的量化方法的需求，以满足这些现代架构的计算需求，同时保持准确性。为了实现这一目标，我们研究了FP8数据格式在75个不同网络架构上进行后训练量化的优势，这些网络架构涵盖了多种任务，包括机器翻译、语言建模、文本生成、图像分类、生成和分割。我们研究了三种不同的FP8表示（E5M2、E4M3和E3M4），以研究在动态范围和精度之间不同权衡程度对模型准确性的影响。基于我们的广泛研究，我们开发了一个量化工作流程，可以概括适用于不同的网络架构。我们的实证结果表明，FP8格式在多个方面优于INT8，包括工作负载覆盖率（92.64％对65.87％）、模型准确性和适用于更广泛的操作范围。

    Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
    
[^339]: 使用动态规划发现决策树的可解释性-性能帕累托前沿

    Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming. (arXiv:2309.12701v1 [cs.LG])

    [http://arxiv.org/abs/2309.12701](http://arxiv.org/abs/2309.12701)

    本文提出了一种使用动态规划找到最优决策树的方法，可以得到多个可解释性-性能权衡的最优决策树，使用户可以根据自己的需求选择最适合的树。

    

    众所周知，决策树由于可以被人类检查和解释而具有固有的可解释性。此外，最近硬件的进步重新引起了对最优决策树算法的关注，这些算法比通常的贪婪方法产生更准确的树。然而，这些最优算法返回的是一个优化手动定义的可解释性-性能权衡的单个树，通过指定最大决策节点数量来获得，对于这个权衡的质量没有进一步的洞察。在本文中，我们提出了一种新的马尔可夫决策问题（MDP）形式来找到最优决策树。这种形式的主要优点是，我们可以通过解决一个单一的动态规划问题计算出多个可解释性-性能权衡的最优决策树，让用户事后选择最适合他们需求的树。在实证方面，我们证明我们的方法在准确性和运行时间方面与最先进的算法竞争力相当。

    Decision trees are known to be intrinsically interpretable as they can be inspected and interpreted by humans. Furthermore, recent hardware advances have rekindled an interest for optimal decision tree algorithms, that produce more accurate trees than the usual greedy approaches. However, these optimal algorithms return a single tree optimizing a hand defined interpretability-performance trade-off, obtained by specifying a maximum number of decision nodes, giving no further insights about the quality of this trade-off. In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. The main interest of this formulation is that we can compute the optimal decision trees for several interpretability-performance trade-offs by solving a single dynamic program, letting the user choose a posteriori the tree that best suits their needs. Empirically, we show that our method is competitive with state-of-the-art algorithms in terms of accuracy and run
    
[^340]: 图中社区检测的综合评述

    A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])

    [http://arxiv.org/abs/2309.11798](http://arxiv.org/abs/2309.11798)

    本综述对图中的社区检测进行了全面回顾。社区结构是真实世界图的重要特征，社区检测方法的研究具有社会学、生物学和计算机科学方面的应用。尽管科学家们做出了努力，但尚未找到一个令人满意的解决方案。本综述介绍了社区结构的概念，各种社区检测方法，以及在各种网络中的实际应用。

    

    复杂网络研究显著促进了我们对真实世界图的社区结构的理解，这是一个具有挑战性的问题，在社会学、生物学和计算机科学中具有应用价值。尽管跨学科科学家社区的努力，但尚未找到一个令人满意的解决方案。本综述详细介绍了图中社区检测的主题，这对于理解复杂系统的组织和功能起着关键的作用。首先，我们介绍社区结构的概念，它指的是将顶点划分为具有强内部连接和较弱连接的集群。然后，我们对各种社区检测方法进行了彻底的阐述，包括我们设计的一种新方法。此外，我们还探讨了社区检测在各种网络中的真实应用。

    The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
    
[^341]: 学习轨道稳定系统以图示教学

    Learning Orbitally Stable Systems for Diagrammatically Teaching. (arXiv:2309.10298v1 [cs.RO])

    [http://arxiv.org/abs/2309.10298](http://arxiv.org/abs/2309.10298)

    本文提出了一种学习轨道稳定系统用于图示教学的框架，通过将已知的轨道渐近稳定系统进行形变，实现机器人跟随用户指定草图进行周期运动的目标。

    

    图示教学是一种机器人获取新技能的范式，用户在场景图像上提供2D草图来指导机器人的运动。本文解决了教导机器人接近表面并在其上进行周期运动的问题，运动的周期可以由用户提供的单个草图在机器人摄像头的图像上任意指定。因此，我们引入了“稳定迪歪形图示教学”（SDDT）框架，将机器人的运动建模为“轨道渐近稳定”（O.A.S.）的动力学系统，学习跟随用户指定的草图。通过应用可微分且可逆的函数来对已知的O.A.S.系统进行形变，从而实现这一目标。参数化的迪正形变在我们建模系统的极限周期和草图之间进行Hausdorff距离优化，产生所需的机器人运动。

    Diagrammatic Teaching is a paradigm for robots to acquire novel skills, whereby the user provides 2D sketches over images of the scene to shape the robot's motion. In this work, we tackle the problem of teaching a robot to approach a surface and then follow cyclic motion on it, where the cycle of the motion can be arbitrarily specified by a single user-provided sketch over an image from the robot's camera. Accordingly, we introduce the \emph{Stable Diffeomorphic Diagrammatic Teaching} (SDDT) framework. SDDT models the robot's motion as an \emph{Orbitally Asymptotically Stable} (O.A.S.) dynamical system that learns to follow the user-specified sketch. This is achieved by applying a \emph{diffeomorphism}, i.e. a differentiable and invertible function, to morph a known O.A.S. system. The parameterised diffeomorphism is then optimised with respect to the Hausdorff distance between the limit cycle of our modelled system and the sketch, to produce the desired robot motion. We provide theoret
    
[^342]: 通过概率性图示教学进行示教学习

    Learning from Demonstration via Probabilistic Diagrammatic Teaching. (arXiv:2309.03835v1 [cs.RO])

    [http://arxiv.org/abs/2309.03835](http://arxiv.org/abs/2309.03835)

    本文介绍了一种名为图示教学的示教学习的替代范式，通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，并将其合成为三维任务空间中的运动轨迹的生成模型。

    

    示教学习（Learning for Demonstration，LfD）使得机器人可以通过模仿专家示范来获得新技能，允许用户以直观的方式传达他们的指示。最近在LfD领域的进展往往依赖于动作示范教学或远程操作作为用户指定示范的手段。动作示范教学需要对机器人进行物理操纵，而远程操作则需要熟练掌握额外的硬件。本文介绍了一种名为图示教学的LfD的替代范式。图示教学旨在通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，然后这些轨迹将被合成为三维任务空间中的运动轨迹的生成模型。此外，我们还提出了用于图示教学的射线追踪概率轨迹学习（RPTL）框架。RPTL从二维图示中提取时间变化的概率密度，并应用射线追踪来寻找相应的区域。

    Learning for Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions i
    
[^343]: 学习动态有向无环图的信息理论最优样本复杂度

    Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs. (arXiv:2308.16859v1 [stat.ML])

    [http://arxiv.org/abs/2308.16859](http://arxiv.org/abs/2308.16859)

    本文研究了学习动态有向无环图（DDAG）的信息理论最优样本复杂度，提出了一种基于观测时间序列的功率谱密度矩阵的度量和算法来重建DDAG。

    

    本文研究了学习线性动态系统（LDS）在有向无环图（DAG）上的底层相互作用/依赖关系的最优样本复杂度。学习DAG结构的样本复杂度在静态系统中已经得到了很好的研究，其中节点状态的样本是独立同分布的（i.i.d.）。然而，在具有动态系统的DAG中，这样的研究较少。我们将这样的DAG称为\emph{动态}DAG（DDAG）。具体来说，我们考虑了一个DDAG，其中节点动力学由未观测的外生噪声源驱动，这些噪声源在时间上是宽幅平稳的（WSS），但彼此之间是不相关的，并且具有相同的功率谱密度（PSD）。受静态设置的启发，我们提出了一种基于观测时间序列的PSD矩阵的度量和算法来重建DDAG。噪声PSD相等的假设可以放宽，以使其可识别。

    In this article, the optimal sample complexity of learning the underlying interaction/dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG's structure is well-studied for static systems, where the samples of nodal states are independent and identically distributed (i.i.d.). However, such a study is less explored for DAGs with dynamical systems, where the nodal states are temporally correlated. We call such a DAG underlying an LDS as \emph{dynamical} DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are driven by unobserved exogenous noise sources that are wide-sense stationary (WSS) in time but are mutually uncorrelated, and have the same {power spectral density (PSD)}. Inspired by the static settings, a metric and an algorithm based on the PSD matrix of the observed time series are proposed to reconstruct the DDAG. The equal noise PSD assumption can be relaxed such that identifiabil
    
[^344]: 在动态推荐系统中确保用户侧公平性

    Ensuring User-side Fairness in Dynamic Recommender Systems. (arXiv:2308.15651v1 [cs.IR])

    [http://arxiv.org/abs/2308.15651](http://arxiv.org/abs/2308.15651)

    本文提出了一种名为FADE的端到端框架，通过微调策略动态减轻推荐系统中用户群体之间的性能差异。

    

    用户侧群体公平性对现代推荐系统至关重要，它旨在减轻由敏感属性（如性别、种族或年龄）定义的用户群体之间的性能差异。我们发现这种差异往往会随着时间的推移而持续存在甚至增加。这需要在动态环境中有效解决用户侧公平性的方法，然而这在文献中很少被探讨。然而，用于确保用户侧公平性（即减少性能差异）的典型方法——公平约束重新排名，在动态设定中面临两个基本挑战：（1）基于排名的公平约束的非可微性，阻碍了端到端训练范式；（2）时间效率低下，阻碍了对用户偏好变化的快速适应。在本文中，我们提出了一种名为FADE的端到端框架，通过微调策略动态减轻性能差异。为了解决上述挑战，FADE提出了一种 fine-tuning 策略。

    User-side group fairness is crucial for modern recommender systems, as it aims to alleviate performance disparity between groups of users defined by sensitive attributes such as gender, race, or age. We find that the disparity tends to persist or even increase over time. This calls for effective ways to address user-side fairness in a dynamic environment, which has been infrequently explored in the literature. However, fairness-constrained re-ranking, a typical method to ensure user-side fairness (i.e., reducing performance disparity), faces two fundamental challenges in the dynamic setting: (1) non-differentiability of the ranking-based fairness constraint, which hinders the end-to-end training paradigm, and (2) time-inefficiency, which impedes quick adaptation to changes in user preferences. In this paper, we propose FAir Dynamic rEcommender (FADE), an end-to-end framework with fine-tuning strategy to dynamically alleviate performance disparity. To tackle the above challenges, FADE u
    
[^345]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^346]: 基于COVID-19的数据多样性和虚拟成像的AI诊断：以病例研究为基础

    Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19. (arXiv:2308.09730v1 [eess.IV])

    [http://arxiv.org/abs/2308.09730](http://arxiv.org/abs/2308.09730)

    本研究通过使用多样性的临床和虚拟生成的医学图像开发和评估了COVID-19诊断的AI模型，发现数据集特征对于AI性能具有重要影响，容易导致泛化能力较差，最高下降20％。

    

    许多研究已经调查了基于深度学习的人工智能（AI）模型在新型冠状病毒（COVID-19）的医学影像诊断中的应用，许多报道称其性能几乎完美。然而，性能的变异性和潜在的数据偏差引发了对临床适用性的担忧。本回顾性研究涉及使用临床多样性和虚拟生成的医学图像开发和评估COVID-19诊断的人工智能（AI）模型。此外，我们进行了一次虚拟成像试验，以评估AI性能受疾病范围、辐射剂量和计算机断层扫描（CT）和胸部放射摄影（CXR）成像模态等几个患者和物理性因素的影响。数据集特征（包括数量、多样性和患病率）强烈影响了AI的性能，导致接收者操作特征曲线下面积下降了高达20％，且泛化能力差。

    Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model
    
[^347]: RAVEN：上下文学习与检索增强的编码器-解码器语言模型

    RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])

    [http://arxiv.org/abs/2308.07922](http://arxiv.org/abs/2308.07922)

    RAVEN是一种结合了检索增强的蒙特卡洛语言建模和前缀语言建模的模型，通过引入上下文融合学习，它能够在上下文学习方面取得比ATLAS更好的性能。

    

    本文研究了检索增强的编码器-解码器语言模型在上下文学习方面的能力。我们首先对现有的ATLAS模型进行全面分析，发现其在上下文学习方面存在限制，主要原因是预训练和测试之间存在不匹配，以及上下文长度受限。为了解决这些问题，我们提出了RAVEN模型，该模型结合了检索增强的蒙特卡洛语言建模和前缀语言建模。我们还引入了上下文融合学习，通过使模型能够利用更多上下文示例而无需额外训练或模型修改来提高少样本性能。通过大量实验，我们证明了RAVEN在某些场景下明显优于ATLAS，并达到了与最先进的语言模型相当的结果，尽管参数数量显著较少。我们的工作强调了检索增强的编码器-解码器语言模型的潜力。

    In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
    
[^348]: 多模态细胞分割挑战：迈向通用解决方案

    The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions. (arXiv:2308.05864v1 [eess.IV])

    [http://arxiv.org/abs/2308.05864](http://arxiv.org/abs/2308.05864)

    本研究提出了一个多模态细胞分割基准，采用Transformer-based深度学习算法，不仅超越了现有方法，而且可以应用于各种显微成像平台和组织类型的图像，无需手动参数调整，为显微成像中更准确和多功能的细胞分析提供了有希望的途径。

    

    细胞分割是显微镜图像中进行定量单细胞分析的关键步骤。现有的细胞分割方法通常针对特定模态或需要手动干预来指定不同实验设置中的超参数。在这里，我们提出了一个多模态细胞分割基准，包括来自50多个不同生物实验的1500多个标记图像。前几名参与者开发了一种基于Transformer的深度学习算法，不仅超过了现有的方法，而且还可以应用于不同显微成像平台和组织类型的多样显微镜图像，无需手动参数调整。这个基准和改进的算法为显微成像中更准确和多功能的细胞分析提供了有希望的途径。

    Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.
    
[^349]: 迭代式草图用于安全编码回归

    Iterative Sketching for Secure Coded Regression. (arXiv:2308.04185v1 [cs.IT])

    [http://arxiv.org/abs/2308.04185](http://arxiv.org/abs/2308.04185)

    这篇论文提出了一种迭代草图方法，用于加速分布式线性回归计算并确保安全性。通过利用随机草图技术和改进异步系统中的块效应韧性，将信息保护与回归问题维度的减小相结合。特别是，通过应用随机正交矩阵和子采样"块"，实现了在每次迭代中考虑新草图的分布式迭代草图方法。同时，对子采样随机哈达玛变换进行了推广并修改以保证数据的安全性。

    

    在这项工作中，我们提出了一种加速线性回归分布式计算并确保安全性的方法。我们利用随机草图技术，并改善了异步系统中的块效应韧性。具体而言，我们应用了一个随机正交矩阵，然后对"块"进行子采样，以同时保护信息并减小回归问题的维数。在我们的设置中，这个转换对应于"近似梯度编码方案"中的编码加密，而子采样对应于非散乱工作节点的响应；在一个集中式编码计算网络中。这导致了一种分布式的"迭代草图"方法，用于$\ell_2$-子空间嵌入，即在每次迭代中考虑一个新的草图。我们还专注于"子采样随机哈达玛变换"的特殊情况，将其推广为块采样，并讨论了如何修改该方法以确保数据的安全。

    In this work, we propose methods for speeding up linear regression distributively, while ensuring security. We leverage randomized sketching techniques, and improve straggler resilience in asynchronous systems. Specifically, we apply a random orthonormal matrix and then subsample \textit{blocks}, to simultaneously secure the information and reduce the dimension of the regression problem. In our setup, the transformation corresponds to an encoded encryption in an \textit{approximate gradient coding scheme}, and the subsampling corresponds to the responses of the non-straggling workers; in a centralized coded computing network. This results in a distributive \textit{iterative sketching} approach for an $\ell_2$-subspace embedding, \textit{i.e.} a new sketch is considered at each iteration. We also focus on the special case of the \textit{Subsampled Randomized Hadamard Transform}, which we generalize to block sampling; and discuss how it can be modified in order to secure the data.
    
[^350]: 被指导的偏见：经过指导调优的语言模型呈现出新兴的认知偏见

    Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])

    [http://arxiv.org/abs/2308.00225](http://arxiv.org/abs/2308.00225)

    这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。

    

    最近的研究表明，指导调优和从人类反馈中学习可以显著提高大语言模型（LMs）的能力。虽然这些调优方法可以使模型生成高质量的文本，但我们推测这些经过调优的模型可能会产生更多隐含的认知偏见。我们的研究提供了证据，表明这些经过调优的模型呈现出先前预训练模型中不存在或较不明显的偏见。我们对三种认知偏见进行了研究，包括矛盾效应、确定性效应和信念偏见，这些偏见已被证实对人类的决策和推理有影响。我们的研究结果突显了这些偏见在各种模型中的存在，特别是那些经过指导调优的模型，如Flan-T5、GPT3.5和GPT4。这项研究对于理解指导调优的LMs中的认知偏见是至关重要的，这有助于开发更可靠和无偏的语言模型。

    Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
    
[^351]: 深度展开网络与循环动量加速用于非线性反问题

    Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])

    [http://arxiv.org/abs/2307.16120](http://arxiv.org/abs/2307.16120)

    本文提出了一种使用循环动量加速的深度展开网络，该网络能够有效应用于非线性逆向成像问题。通过利用长短期记忆循环神经网络学习和保留先前梯度的知识，该方法在两个非线性逆向问题上获得了良好的结果。

    

    结合基于模型的迭代算法和数据驱动的深度学习解决方案，深度展开网络(DuNets)已成为解决逆向成像问题的流行工具。虽然DuNets已成功应用于许多线性逆向问题，但非线性问题往往会影响方法的性能。受优化算法中常用的动量加速技术启发，我们提出了一种循环动量加速(RMA)框架，该框架使用长短期记忆循环神经网络(LSTM-RNN)来模拟动量加速过程。RMA模块利用LSTM-RNN学习和保留先前梯度的知识能力。我们将RMA应用于两种流行的DuNets——学习的近端梯度下降(LPGD)和学习的原始-对偶(LPD)方法，分别得到LPGD-RMA和LPD-RMA。我们在两个非线性逆向问题上提供了实验结果：非线性去卷积问题、

    Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an
    
[^352]: 用图神经结构搜索进行分子属性预测的不确定性量化

    Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search. (arXiv:2307.10438v1 [cs.LG])

    [http://arxiv.org/abs/2307.10438](http://arxiv.org/abs/2307.10438)

    用于分子属性预测的图神经网络方法通常无法量化预测的不确定性，本研究提出了一种自动化的不确定性量化方法AutoGNNUQ，通过架构搜索生成高性能的图神经网络集合，并利用方差分解将数据和模型的不确定性分开，从而提供了减少不确定性的有价值见解。

    

    图神经网络（GNN）已成为分子属性预测中突出的数据驱动方法。然而，典型GNN模型的一个关键限制是无法量化预测中的不确定性。这种能力对于确保在下游任务中可信地使用和部署模型至关重要。为此，我们引入了AutoGNNUQ，一种自动化的分子属性预测不确定性量化方法。AutoGNNUQ利用架构搜索生成一组高性能的GNN集合，能够估计预测的不确定性。我们的方法使用方差分解来分离数据（aleatoric）和模型（epistemic）不确定性，为减少它们提供了有价值的见解。在我们的计算实验中，我们展示了AutoGNNUQ在多个基准数据集上在预测准确性和不确定性测量性能方面超过了现有的不确定性量化方法。此外，我们利用t-SNE可视化来解释不确定性的来源和结构。

    Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to exp
    
[^353]: 半监督和自监督学习在医学图像上的准确性与时间前沿比较

    Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])

    [http://arxiv.org/abs/2307.08919](http://arxiv.org/abs/2307.08919)

    半监督和自监督学习在医学图像上的准确性与时间前沿进行比较，通过一个精心设计的基准研究来回答从业者的问题。

    

    对于许多医学图像分类器的应用来说，每个图像都很难或昂贵地获得一个可信的标签。相比之下，没有标签的图像更容易获取。两个主要研究方向都承诺额外的无标签数据可以提高分类器的性能：自监督学习仅在无标签数据上预训练有用的表示，然后通过标记集对这些表示进行微调以获得分类器；半监督学习同时在标记和无标签数据上直接训练分类器。最近的方法从两个方向上都声称在非医学任务上取得了显著的收益，但没有系统评估医学图像，并且大多只与同一方向的方法进行比较。本研究提供了一个经过精心设计的基准来回答从业者的一个关键问题：在小型标记数据集和有限的培训时间预算下，额外的无标签图像能够产生多大的收益。

    For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and 
    
[^354]: Weisfeiler和Lehman进行度量建模：探索WL测试的有效性

    Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test. (arXiv:2307.05775v1 [cs.LG])

    [http://arxiv.org/abs/2307.05775](http://arxiv.org/abs/2307.05775)

    本文通过系统分析，揭示了$k$-WL的可靠性和有效性问题，并提出了基于基准的表达能力的外延定义和测量。

    

    图神经网络的表达能力通常通过比较一个架构能够区分的非同构图或节点对的数量与$k$-维Weisfeiler-Lehman ($k$-WL)测试能够区分的数量来衡量。本文通过对$k$-WL的可靠性和有效性进行系统分析，揭示了从业者对表达能力和$k$-WL的概念化之间的不一致性。我们进一步进行了对从业者的调查（n=18），以了解他们对表达能力的概念以及对$k$-WL的假设。与从业者的观点相反，我们的分析（借鉴了图论和基准审核）揭示了$k$-WL不能保证等距、可能与现实世界的图任务无关，并且可能不促进泛化或可信度。我们主张基于基准的表达能力的外延定义和测量，进一步贡献了构建此类基准的指导问题。

    The expressive power of graph neural networks is usually measured by comparing how many pairs of graphs or nodes an architecture can possibly distinguish as non-isomorphic to those distinguishable by the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments between practitioners' conceptualizations of expressive power and $k$-WL through a systematic analysis of the reliability and validity of $k$-WL. We further conduct a survey ($n = 18$) of practitioners to surface their conceptualizations of expressive power and their assumptions about $k$-WL. In contrast to practitioners' opinions, our analysis (which draws from graph theory and benchmark auditing) reveals that $k$-WL does not guarantee isometry, can be irrelevant to real-world graph tasks, and may not promote generalization or trustworthiness. We argue for extensional definitions and measurement of expressive power based on benchmarks; we further contribute guiding questions for constructing such 
    
[^355]: 一种可解释的增量随机权重神经网络及其应用的构造算法

    An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])

    [http://arxiv.org/abs/2307.00185](http://arxiv.org/abs/2307.00185)

    本文提出了一种可解释的增量随机权重神经网络的构造算法，通过几何信息约束和节点池策略解决了难以解释隐藏参数与残差误差之间关系的问题。这种算法在大规模数据建模任务中表现出了良好的性能。

    

    增量随机权重神经网络(IRWNNs)由于易于实现和快速学习而受到关注。然而，IRWNNs的一个显著缺点是难以解释隐藏参数（节点）与残差误差（模型性能）之间的关系。为了解决这个问题，本文提出了一个具有几何信息约束的可解释的构造算法(ICA)。首先，基于隐藏参数与残差误差之间的几何关系，提出了一个可解释的几何信息约束来随机分配隐藏参数。同时，采用节点池策略获取更有利于收敛的隐藏参数。此外，证明了ICA的通用逼近性质。最后，提出了ICA的轻量级版本用于大规模数据建模任务。在六个基准数据集上的实验结果表明了该算法的有效性。

    Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
    
[^356]: 多模态大语言模型综述

    A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])

    [http://arxiv.org/abs/2306.13549](http://arxiv.org/abs/2306.13549)

    本文追踪和总结了多模态大语言模型（MLLM）的最新进展，包括多模态指令调整、多模态上下文学习、多模态思维链和LLM辅助视觉推理等应用，指出了现有挑战和有前途的研究方向。

    

    多模态大语言模型（MLLM）是一种新兴的研究热点，使用强大的大语言模型作为大脑执行多模态任务。MLLM 的惊人能力，如基于图像编写故事和无OCR数学推理等，在传统方法中很少见，表明了通向人工智能的潜在路径。本文旨在追踪和总结 MLLM 的最新进展。首先，我们介绍了 MLLM 的构成，概述了相关概念。然后，讨论了关键技术和应用，包括多模态指令调整（M-IT）、多模态上下文学习（M-ICL）、多模态思维链（M-CoT）和LLM辅助视觉推理（LAVR）。最后，我们讨论了现有的挑战，并指出了有前途的研究方向。鉴于 MLLM 时代才刚刚开始，我们会不断更新这个综述，并希望能激发更多的研究。

    Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
    
[^357]: 基于子图网络的对比学习

    Subgraph Networks Based Contrastive Learning. (arXiv:2306.03506v1 [cs.LG])

    [http://arxiv.org/abs/2306.03506](http://arxiv.org/abs/2306.03506)

    本文提出了一种新的对比学习框架，名为基于子图网络的对比学习(SGNCL)，通过应用子图网络生成策略以产生增强视图，并探究了子结构相互作用对图形表示的影响。

    

    图对比学习(GCL)是一种自监督学习方法，可解决注释数据稀缺的问题。 它在未注释的图形中挖掘显式特征以生成下游任务的有利图形表示。大多数现有的GCL方法侧重于图形增强策略和相互信息估计操作的设计。 然而，这些方法没有考虑子图中存在的相互作用。为了探索子结构相互作用对图形表示的影响，我们提出了一种名为subgraph network-based contrastive learning (SGNCL)的新框架。SGNCL应用子图网络生成策略以产生增强视图。该策略将原始图转换为具有拓扑和属性特征的边到节点映射网络。单次增强视图是

    Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a 
    
[^358]: 改进的概率图像-文本表示方法

    Improved Probabilistic Image-Text Representations. (arXiv:2305.18171v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.18171](http://arxiv.org/abs/2305.18171)

    本论文提出了一种改进的概率图像-文本表示方法，通过引入新的概率距离和两种优化技术，解决了现有方法中的计算负担过重和损失饱和问题，取得了显著的性能提升。

    

    图像-文本匹配是一种基本的视觉-语言任务，由于多样性和不完美注释导致的固有歧义使其受到困扰。确定性函数无法足够强大地捕捉到这种歧义，因此需要探索概率嵌入来解决这个挑战。然而，现有的概率图像-文本匹配方法存在两个关键缺点：蒙特卡洛逼近导致计算负担较重，且在大量误检情况下容易出现损失饱和问题。为了克服这些问题，本文提出了一种改进的概率跨模态嵌入方法（PCME++），通过引入具有闭合形式解的新的概率距离。此外，还提出了两种优化技术进一步增强PCME++：首先，引入伪正样本以防止大量误检情况下的损失饱和问题；其次，采用混合样本数据增强进行概率匹配。实验结果表明，PCME++在ITM任务中取得了显著的性能提升。

    Image-Text Matching (ITM) task, a fundamental vision-language (VL) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to tackle the challenge. However, the existing probabilistic ITM approach encounters two key shortcomings; the burden of heavy computations due to the Monte Carlo approximation, and the loss saturation issue in the face of abundant false negatives. To overcome the issues, this paper presents an improved Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new probabilistic distance with a closed-form solution. In addition, two optimization techniques are proposed to enhance PCME++ further; first, the incorporation of pseudo-positives to prevent the loss saturation problem under massive false negatives; second, mixed sample data augmentation for probabilistic matching. Experimental re
    
[^359]: 具有马尔可夫噪声的一阶方法：从加速到变分不等式

    First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities. (arXiv:2305.15938v1 [math.OC])

    [http://arxiv.org/abs/2305.15938](http://arxiv.org/abs/2305.15938)

    本论文研究了涉及马尔科夫噪声的随机优化问题，提出了一种适用于非凸和强凸最小化问题的一阶梯度方法，使用基于多层蒙特卡罗方法的随机批处理方案以获得最优线性关系，并消除了以前研究中的限制条件。在马尔可夫噪声下对变分不等式的扩展是原创性的。

    

    本文研究涉及马尔可夫噪声的随机优化问题。我们提出了一个统一的方法来理论分析一阶梯度方法用于解决随机优化和变分不等式的问题。我们的方法涵盖了非凸和强凸最小化问题的情况。为了实现一个依赖于底层噪声序列混合时间的最优(线性)关系，我们使用基于多层蒙特卡罗方法的随机批处理方案。此外，我们的技术允许我们消除以前关于马尔可夫噪声的研究中的限制条件，例如需要有界域和均匀有界随机梯度。我们在马尔可夫噪声下对变分不等式的扩展是原创性的。此外，我们提供了匹配强凸优化问题的理论最优解的下限。

    This paper delves into stochastic optimization problems that involve Markovian noise. We present a unified approach for the theoretical analysis of first-order gradient methods for stochastic optimization and variational inequalities. Our approach covers scenarios for both non-convex and strongly convex minimization problems. To achieve an optimal (linear) dependence on the mixing time of the underlying noise sequence, we use the randomized batching scheme, which is based on the multilevel Monte Carlo method. Moreover, our technique allows us to eliminate the limiting assumptions of previous research on Markov noise, such as the need for a bounded domain and uniformly bounded stochastic gradients. Our extension to variational inequalities under Markovian noise is original. Additionally, we provide lower bounds that match the oracle complexity of our method in the case of strongly convex optimization problems.
    
[^360]: 通过求解最优边界条件解决扩散ODE问题以实现更好的图像超分辨率

    Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.15357](http://arxiv.org/abs/2305.15357)

    本文提出了一种求解最优边界条件解决扩散ODE问题的有效采样方法，以稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像。

    

    扩散模型作为一种强大的生成模型，已经在图像超分辨率任务中取得了令人印象深刻的结果。然而，由于扩散模型反向过程中引入的随机性，基于扩散的超分辨率模型在每次采样时性能波动很大，特别是对于具有少量重新采样步骤的采样器。扩散模型的这种固有随机性导致其无效和不稳定，使用户难以保证超分辨结果的质量。然而，我们的工作将这种随机性视为一种机遇：全面分析和利用它导致了构建一种有效的即插即用采样方法，具有潜力使一系列基于扩散的超分辨率方法受益。更详细地说，我们建议通过求解扩散普通微分方程（扩散ODE）和最优边界条件（BC），稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像，并分析其特性。

    Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of sampling, especially for samplers with few resampled steps. This inherent randomness of diffusion models results in ineffectiveness and instability, making it challenging for users to guarantee the quality of SR results. However, our work takes this randomness as an opportunity: fully analyzing and leveraging it leads to the construction of an effective plug-and-play sampling method that owns the potential to benefit a series of diffusion-based SR methods. More in detail, we propose to steadily sample high-quality SR images from pretrained diffusion-based SR models by solving diffusion ordinary differential equations (diffusion ODEs) with optimal boundary conditions (BCs) and analyze the characterist
    
[^361]: 基于几何多图神经网络的多状态RNA设计

    Multi-State RNA Design with Geometric Multi-Graph Neural Networks. (arXiv:2305.14749v1 [cs.LG])

    [http://arxiv.org/abs/2305.14749](http://arxiv.org/abs/2305.14749)

    本论文提出了一种基于几何多图神经网络的多状态RNA设计方法，可以明确考虑和反映RNA构象多样性在其设计中。其能够显著提高原生序列的恢复，尤其适用于多状态和结构多样化的RNA。

    

    计算RNA设计在合成生物学和治疗开发方面具有广泛的应用。RNA多样的生物学功能的基础是它的构象灵活性，使单一序列能够采用多种不同的三维结构状态。目前，计算生物分子设计任务经常被提出为逆问题，即基于采用单一预期结构构象来设计序列。在这项工作中，我们提出了gRNAde，这是一个基于一组三维RNA骨架结构操作的几何RNA设计流程，以明确考虑和反映RNA构象多样性在其设计中。我们在一个新的大规模三维RNA设计数据集上演示了gRNAde的效用，特别适用于多状态和结构多样化的RNA，能够显著提高原生序列的恢复。我们的代码可在https://github.com/chaitjo/geometric-rna-design上获得。

    Computational RNA design has broad applications across synthetic biology and therapeutic development. Fundamental to the diverse biological functions of RNA is its conformational flexibility, enabling single sequences to adopt a variety of distinct 3D states. Currently, computational biomolecule design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired structural conformation. In this work, we propose gRNAde, a geometric RNA design pipeline that operates on sets of 3D RNA backbone structures to explicitly account for and reflect RNA conformational diversity in its designs. We demonstrate the utility of gRNAde for improving native sequence recovery over single-state approaches on a new large-scale 3D RNA design dataset, especially for multi-state and structurally diverse RNAs. Our code is available at https://github.com/chaitjo/geometric-rna-design
    
[^362]: 动态正则化锐度感知联邦学习中的全局一致性和平滑场景方法

    Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape. (arXiv:2305.11584v1 [cs.LG])

    [http://arxiv.org/abs/2305.11584](http://arxiv.org/abs/2305.11584)

    本文提出了一种动态正则化锐度感知联邦学习方法，通过同时考虑优化和泛化目标来高效地提高联邦学习的性能。

    

    在联邦学习（FL）中，一组本地客户端在全局服务器的协调下协作训练带有隐私保护的模型。由于多个本地更新和隔离的非 iid 数据集，客户端容易过度拟合到自己的自身最优解，这极大地偏离了全局目标并严重削弱了性能。本文提出了一种新颖的通用算法 FedSMOO，通过同时考虑优化和泛化目标来高效地提高 FL 中的性能。

    In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance. Most previous works only focus on enhancing the consistency between the local and global objectives to alleviate this prejudicial client drifts from the perspective of the optimization view, whose performance would be prominently deteriorated on the high heterogeneity. In this work, we propose a novel and general algorithm {\ttfamily FedSMOO} by jointly considering the optimization and generalization targets to efficiently improve the performance in FL. Concretely, {\ttfamily FedSMOO} adopts a dynamic regularizer to guarantee the local optima towards the global objective, which is meanwhile revised by the 
    
[^363]: SpecInfer：利用推测推断和令牌树验证加速生成式大语言模型的服务

    SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])

    [http://arxiv.org/abs/2305.09781](http://arxiv.org/abs/2305.09781)

    SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。

    

    由于生成式大语言模型（LLMs）需要高计算和内存需求，因此快速和廉价地为它们提供服务是具有挑战性的。本文介绍SpecInfer，一个LLM服务系统，它利用推测推断和令牌树验证加速生成式LLM推断。SpecInfer背后的关键是将各种小型语言模型进行集体提升调整，共同预测LLM的输出； 预测结果组织成一个令牌树，其中每个节点都表示候选令牌序列。通过一种新颖的基于树的并行解码机制，以LMM作为令牌树验证器来验证令牌树所代表的所有候选令牌序列的正确性。SpecInfer使用LLM作为令牌树验证器，而不是增量解码器，从而显著减少了为生成式LLM提供服务所需的端到端延迟和计算要求，同时可确保模型质量。

    The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
    
[^364]: iMixer: 分层Hopfield网络暗示了可逆、隐式和迭代的MLP-Mixer

    iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer. (arXiv:2304.13061v1 [cs.LG])

    [http://arxiv.org/abs/2304.13061](http://arxiv.org/abs/2304.13061)

    本文推广了 Hopkins field 分层网络，并介绍了 iMixer，MLP-Mixer 模型的新概括，不同于普通的前馈网络，iMixer 涉及到从输出到输入的传播的 MLP 层，被特征化为一个可逆、隐式、迭代的 mixing block。

    

    在过去的几年中，Transformer在计算机视觉领域的成功促使寻找可以与之竞争的许多替代模型，如MLP-Mixer。尽管这些模型的引入偏差较弱，但它们的表现可与研究较多的卷积神经网络相媲美。最近对现代Hopfield网络的研究表明了某些基于能量的关联记忆模型与Transformer或MLP-Mixer之间的对应关系，并揭示了Transformer类型架构设计的理论背景。在本文中，我们将该对应关系推广到最近引入的分层Hopfield网络，并找到了iMixer，这是MLP-Mixer模型的新的概括。与普通的前馈神经网络不同，iMixer涉及从输出侧向输入侧传播的MLP层。我们将该模块特征化为可逆、隐式和迭代混合模块的一个例子。我们通过各种任务评估了模型的性能。

    In the last few years, the success of Transformers in computer vision has stimulated the discovery of many alternative models that compete with Transformers, such as the MLP-Mixer. Despite their weak induced bias, these models have achieved performance comparable to well-studied convolutional neural networks. Recent studies on modern Hopfield networks suggest the correspondence between certain energy-based associative memory models and Transformers or MLP-Mixer, and shed some light on the theoretical background of the Transformer-type architectures design. In this paper we generalize the correspondence to the recently introduced hierarchical Hopfield network, and find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary feedforward neural networks, iMixer involves MLP layers that propagate forward from the output side to the input side. We characterize the module as an example of invertible, implicit, and iterative mixing module. We evaluate the model performance with var
    
[^365]: 3D蒙版自编码和伪标签用于异构婴儿脑 MRI 领域间适应性标记

    3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])

    [http://arxiv.org/abs/2303.09373](http://arxiv.org/abs/2303.09373)

    本文提出了一种名为 MAPSeg 的新框架，采用 3D 蒙版自编码和伪标签的方式，实现了跨年龄、跨模态和跨场景下对婴儿脑 MRI 中亚皮质区域的分割，充分考虑不同 MRI 扫描仪、供应商或采集序列以及不同的神经发育阶段所造成的内在异质性，提高了分割结果的鲁棒性。

    

    婴儿脑 MRI 在跨年龄、跨模态、跨场景下实现鲁棒的分割仍然是具有挑战性的。本文介绍了一种名为 MAPSeg 的新框架，它使用 3D 蒙版自编码和蒙版伪标签的方式来对婴儿脑MRI的不同亚皮质区域进行分割，并联合学习标记源域数据和未标记目标域数据，以提高分割结果的鲁棒性。

    Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consist
    
[^366]: 非对角度量中的可扩展随机梯度里曼动力学

    Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics. (arXiv:2303.05101v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05101](http://arxiv.org/abs/2303.05101)

    本文提出了两种非对角度量，可以在随机梯度采样中使用，以改善神经网络模型的贝叶斯推断性能，尤其对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络效果显著。

    

    随机梯度采样方法通常用于对神经网络进行贝叶斯推断。观察到，包含微分几何概念的方法往往具有更好的性能，里曼度量通过考虑局部曲率来改善后验探索。然而，现有方法往往采用简单的对角度量以保持计算效率，这会损失一些性能。我们提出了两种非对角度量，可以在随机梯度采样中使用，以改善收敛性和探索性，在对比对角度量只有轻微的计算开销。我们展示了对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络，使用这些度量可以提供改进。对于其他一些选择，后验分布在简单度量下也足够容易。

    Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics.
    
[^367]: CECT：可控的卷积神经网络和Transformer用于COVID-19图像分类

    CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification. (arXiv:2302.02314v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.02314](http://arxiv.org/abs/2302.02314)

    本研究提出了一种新的分类网络CECT，利用可控的卷积神经网络和Transformer进行组合，能同时捕捉多个局部和全局尺度上的特征。在两个公共COVID-19数据集上评估后表现优于现有的最先进方法。这一新方法在医学图像分类领域中有广泛的应用前景。

    

    大多数计算机视觉模型基于卷积神经网络（CNN）或Transformer开发，前者（后者）可以捕捉局部（全局）特征。为了减轻模型性能受局部（全局）特征缺乏的限制，我们开发了一种新的分类网络CECT，通过可控的卷积神经网络和Transformer进行组合。CECT由卷积编码块、转置卷积解码块和Transformer分类块组成。不同于传统的基于CNN或Transformer的方法，我们的CECT可以同时捕捉多个局部和全局尺度上的特征。此外，本文提出的集合系数可以控制不同尺度局部特征的贡献。我们在两个公共COVID-19数据集上评估了CECT，并且在所有评估指标上均优于现有的最先进方法。由于其卓越的特征捕捉能力，我们相信CECT可以作为一种通用而有效的工具扩展到其他医学图像分类场景中。

    Most computer vision models are developed based on either convolutional neural network (CNN) or transformer, while the former (latter) method captures local (global) features. To relieve model performance limitations due to the lack of global (local) features, we develop a novel classification network CECT by controllable ensemble CNN and transformer. CECT is composed of a convolutional encoder block, a transposed-convolutional decoder block, and a transformer classification block. Different from conventional CNN- or transformer-based methods, our CECT can capture features at both multi-local and global scales. Besides, the contribution of local features at different scales can be controlled with the proposed ensemble coefficients. We evaluate CECT on two public COVID-19 datasets and it outperforms existing state-of-the-art methods on all evaluation metrics. With remarkable feature capture ability, we believe CECT can be extended to other medical image classification scenarios as a dia
    
[^368]: 部分动员：跟踪俄罗斯媒体和电报之间的多语言信息流

    Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2301.10856](http://arxiv.org/abs/2301.10856)

    本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。

    

    在俄罗斯入侵乌克兰后，针对俄罗斯在线媒体的虚假信息和宣传，包括俄罗斯之声和卫星新闻在内的俄罗斯媒体在欧洲遭到禁止。为了保持观众数量，许多俄罗斯媒体开始在电报等消息服务上大力宣传其内容。在这项工作中，我们研究了2022年期间16家俄罗斯媒体机构如何与732个电报频道互动和利用。利用基础模型MPNet、DP-means聚类和Hawkes过程，我们跟踪新闻网站和电报频道之间的叙事传播情况。我们表明，新闻媒体不仅通过电报传播现有的叙事，而且他们会从电报平台源材料。在我们研究的网站中，2.3％（ura.news）至26.7％（ukraina.ru）的文章讨论了源于/导致电报活动的内容。最后，通过跟踪个别主题的扩散，我们测量新闻网站发表文章的速率。

    In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
    
[^369]: 训练数据影响分析与估计：一项调查

    Training Data Influence Analysis and Estimation: A Survey. (arXiv:2212.04612v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04612](http://arxiv.org/abs/2212.04612)

    本文对训练数据影响分析与估计进行了全面调查，通过量化每个训练实例对最终模型的改变程度揭示了训练的基本相互作用。调查了当前的影响分析方法，并提出了未来的研究方向。

    

    好的模型需要好的训练数据。对于过度参数化的深度模型，训练数据与模型预测之间的因果关系变得越来越不透明和难以理解。通过量化每个训练实例对最终模型的改变程度，影响分析部分揭示了训练的基本相互作用。在最坏情况下，准确测量训练数据的影响力是有可能证明困难的；因此，发展和使用影响力估计器，仅对真实影响进行近似估计。本文首次全面调查了训练数据影响分析与估计。首先，我们系统化各种关于训练数据影响的定义，并在某些方面进行解析。然后，我们将最先进的影响分析方法分类整理；详细描述每种方法，比较其基本假设、渐近复杂度以及整体优缺点。最后，我们提出未来的研究方向。

    Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is increasingly opaque and poorly understood. Influence analysis partially demystifies training's underlying interactions by quantifying the amount each training instance alters the final model. Measuring the training data's influence exactly can be provably hard in the worst case; this has led to the development and use of influence estimators, which only approximate the true influence. This paper provides the first comprehensive survey of training data influence analysis and estimation. We begin by formalizing the various, and in places orthogonal, definitions of training data influence. We then organize state-of-the-art influence analysis methods into a taxonomy; we describe each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses. Finally, we propose future resear
    
[^370]: 带有内外部注意力的形状引导扩散

    Shape-Guided Diffusion with Inside-Outside Attention. (arXiv:2212.00210v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.00210](http://arxiv.org/abs/2212.00210)

    该论文提出了一种无需训练的形状引导扩散方法，使用一种新颖的内外部注意机制将形状限制应用于跨注意力图和自注意力图上，从而在文本到图像扩散模型中考虑到对象形状，进而可以实现对象形状忠实度更高的图像生成。

    

    在操作对象时，现有的文本到图像扩散模型通常忽略对象的形状并生成错误比例、被截断或被背景内容替换的图像。我们提出了一种无需训练的方法 Shape-Guided Diffusion，该方法修改了预训练扩散模型，使之对用户指定的形状输入或从文本中自动推断的形状敏感。我们使用一种新颖的内外部注意机制，在反演和生成过程中将此形状限制应用于跨注意力图和自注意力图上。我们的机制指定空间区域是对象（内部）还是背景（外部），然后将文本提示指定的编辑与正确的区域相关联。我们在形状引导编辑任务上展示了我们方法的有效性，其中模型必须根据文本提示和对象掩码替换对象。我们创建了一个新的从 MS-COCO 衍生的 ShapePrompts 基准，并在形状忠实度方面实现了 SOTA 的结果，而不需要降级。

    When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degra
    
[^371]: 使用说服性写作策略来解释和检测健康错误信息

    Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.05985](http://arxiv.org/abs/2211.05985)

    本研究旨在通过使用说服性写作技巧的文本段落进行分类来增加自动化虚假信息检测的新层次，以产生可解释的理由。我们提出了一个包含常见说服性写作策略的注释方案和数据集，并使用 RoBERTa 文本分类模型进行实验。

    

    虚假信息的传播是当今社会的一大问题，许多学术界和工业界的研究人员正在努力解决这个问题。由于每天创造的虚假信息数量巨大，将此任务留给人工事实检查员是不切实际的。数据科学家和研究人员多年来一直致力于自动化虚假信息检测，但今天仍然是一个具有挑战性的问题。我们的研究目标是为自动化虚假信息检测添加一个新层次；使用具有说服性写作技巧的文本段落进行分类，以产生可解释的理由，说明为什么这篇文章可以标记为虚假信息。为此，我们提出了一个包含许多常见说服性写作策略的新注释方案，以及相应的人工注释数据集。我们使用 RoBERTa 文本分类模型来完成此任务，因为它在自然语言处理方面具有高性能。我们开发了几种基于语言模型的基线模型，并提供了结果分析。

    The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
    
[^372]: 具有无限制内存的在线凸优化

    Online Convex Optimization with Unbounded Memory. (arXiv:2210.09903v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09903](http://arxiv.org/abs/2210.09903)

    本论文提出了一种新的在线凸优化框架，可以处理决策历史的长期依赖关系，并介绍了用于量化依赖程度的$p$-有效内存容量的概念。

    

    在线凸优化（OCO）是在线学习中广泛使用的框架。然而，在很多应用中，学习者的损失不仅取决于当前的决策，还取决于直到那个时间点的所有决策历史。本文引入了一种OCO的扩展框架，“具有无限制内存的在线凸优化”，来捕捉对过去决策的长期依赖关系，并介绍了$p$-有效内存容量的概念，$H_p$，它量化了$p$阶影响的最大值。

    Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework, ``Online Convex Optimization with Unbounded Memory'', that captures long-term dependence on past decisions. We introduce the notion of $p$-effective memory capacity, $H_p$, that quantifies the maximum influence of p
    
[^373]: 用机器学习方法求解大规模双层和随机规划问题的新方法: 以自行车网络设计为例

    A Machine Learning Approach to Solving Large Bilevel and Stochastic Programs: Application to Cycling Network Design. (arXiv:2209.09404v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.09404](http://arxiv.org/abs/2209.09404)

    我们提出了一种基于机器学习的新方法，用于求解涉及大量独立从属者的双层规划问题。该方法通过从一个采样的子集中估计未采样从属者的目标值来优化模型，实现了对一般从属者特征的表征学习。

    

    我们提出了一种基于机器学习的新方法，用于求解涉及大量独立从属者的双层规划问题，其中特殊情况包括两阶段随机规划。我们提出了一个优化模型，明确考虑到从一个采样的子集的从属者，并利用机器学习模型估计未采样的从属者的目标值。与现有方法不同的是，我们将机器学习模型训练嵌入到优化问题中，这允许我们使用无法通过领导者决策表示的一般从属者特征。我们证明了生成的领导者决策在原始目标函数的最优间隙上的界限，该目标函数考虑到完整的从属者集合。然后，我们开发了从属者采样算法来缩小界限，并提出了一种表示学习方法来学习从属者特征，这些特征可以用作嵌入机器学习模型的输入。通过使用一个自行车网络设计问题的合成实例进行实验，

    We present a novel machine learning-based approach to solving bilevel programs that involve a large number of independent followers, which as a special case include two-stage stochastic programming. We propose an optimization model that explicitly considers a sampled subset of followers and exploits a machine learning model to estimate the objective values of unsampled followers. Unlike existing approaches, we embed machine learning model training into the optimization problem, which allows us to employ general follower features that can not be represented using leader decisions. We prove bounds on the optimality gap of the generated leader decision as measured by the original objective function that considers the full follower set. We then develop follower sampling algorithms to tighten the bounds and a representation learning approach to learn follower features, which can be used as inputs to the embedded machine learning model. Using synthetic instances of a cycling network design p
    
[^374]: 从小样本中估计大的因果多树

    Estimating large causal polytrees from small samples. (arXiv:2209.07028v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2209.07028](http://arxiv.org/abs/2209.07028)

    本文介绍了一种算法，可以在变量数量远大于样本大小的情况下，准确地估计大规模因果多树结构，而几乎不需要任何分布或建模的假设。

    

    我们考虑从相对较小的独立同分布样本中估计大的因果多树的问题。这是在变量数量与样本大小相比非常大的情况下确定因果结构的问题，例如基因调控网络。我们提出了一种算法，在这种情况下以高准确度恢复树形结构。该算法除了一些温和的非退化条件外，基本不需要分布或建模的假设。

    We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.
    
[^375]: WaveMix: 一种用于图像分析的资源高效神经网络

    WaveMix: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.14375](http://arxiv.org/abs/2205.14375)

    WaveMix是一种资源高效的神经网络结构，可以在多项任务上达到可比或更好的准确率，并且需要更少的参数和GPU RAM，实现时间、成本和能量的节省。

    

    我们提出了WaveMix——一种新颖的神经网络结构，既具有资源效率性，又具有通用性和可扩展性。WaveMix网络在多项任务上达到了可比或更好的准确率，包括Cityscapes中的分割和Places-365、五个EMNIST数据集和iNAT-mini中的分类，并建立了新的基准。令人惊奇的是，与先前的最先进技术相比，WaveMix结构所需的参数更少。此外，当控制参数数量时，WaveMix所需的GPU RAM更少，这意味着节省时间、成本和能量。为了实现这些收益，我们在WaveMix块中使用了多级二维离散小波变换（2D-DWT），它具有以下优点:(1)它基于三种强图像先验条件重新组织空间信息——尺度不变性，位移不变性和边缘的稀疏性,(2) i

    We propose WaveMix -- a novel neural architecture for computer vision that is resource-efficient yet generalizable and scalable. WaveMix networks achieve comparable or better accuracy than the state-of-the-art convolutional neural networks, vision transformers, and token mixers for several tasks, establishing new benchmarks for segmentation on Cityscapes; and for classification on Places-365, five EMNIST datasets, and iNAT-mini. Remarkably, WaveMix architectures require fewer parameters to achieve these benchmarks compared to the previous state-of-the-art. Moreover, when controlled for the number of parameters, WaveMix requires lesser GPU RAM, which translates to savings in time, cost, and energy. To achieve these gains we used multi-level two-dimensional discrete wavelet transform (2D-DWT) in WaveMix blocks, which has the following advantages: (1) It reorganizes spatial information based on three strong image priors -- scale-invariance, shift-invariance, and sparseness of edges, (2) i
    
[^376]: 面对混淆因素的悲观情绪：部分可观察马尔可夫决策过程的证明有效离线强化学习

    Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes. (arXiv:2205.13589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13589](http://arxiv.org/abs/2205.13589)

    本文提出了代理变量悲观策略优化 (P3O) 算法，它通过近端因果推断构建悲观置信区间耦合序列解决了部分可观察马尔可夫决策过程中的混淆偏差和最优策略与行为策略之间的分布偏移问题。

    

    本文研究了部分可观测马尔可夫决策过程中的离线强化学习。特别地，我们旨在从由行为策略收集的数据集中学习最优策略，该策略可能取决于潜在状态。这样的数据集在混淆意义上同时影响行动和观测值，这对于现有的离线强化学习算法来说是禁止的。为此，我们提出了通过近端因果推断构建的悲观置信区间耦合序列的代理变量悲观策略优化（P3O）算法，该算法在广义函数逼近的上下文中解决了混淆偏差和最优策略与行为策略之间的分布偏移问题。我们证明，在混淆数据集的部分覆盖假设下，P3O可以实现n^{-1/2}的收敛率。

    We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions constructed via proximal causal inference, which is formulated as minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-
    
[^377]: 基于密集卷积神经网络的胸部疾病多标签分类方法

    Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.03583](http://arxiv.org/abs/2202.03583)

    本研究提出了一种基于密集卷积神经网络和GRADCAM的胸部X光疾病多标签诊断模型，获得了在Cardiomegaly条件下最高的AUC得分0.896，并使用热图提高了模型的可解释性。

    

    传统的X光图像病理识别方法依赖于熟练的人类解释，并且往往耗时。深度学习技术的出现使自动诊断系统的开发成为可能，但这类系统的表现取决于模型的质量和它提供的可解释性水平。本文提出了一种使用密集卷积神经网络（DenseNet）和GRADCAM进行模型可解释性的胸部X光疾病多标签诊断模型。我们使用前置X光训练了我们的模型，并使用各种定量指标（包括受试者操作特征曲线下面积（AUC））评估了模型的性能。我们的模型在Cardiomegaly条件下达到了最高的AUC得分0.896，并获得了0.826的准确度。而在Nodule条件下获得了最低的AUC得分0.655，准确度为0.66。为了提高模型可解释性，并在决策方面建立信任，我们使用GRADCAM生成了热图，突出显示了对诊断最重要的X光区域。

    Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
    
[^378]: 多任务学习和Bandits通过健壮统计学

    Multitask Learning and Bandits via Robust Statistics. (arXiv:2112.14233v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.14233](http://arxiv.org/abs/2112.14233)

    本研究探讨了多任务学习以及Bandits方法的健壮统计学实现，提出了一种新颖的两阶段多任务学习估计器，该估计器以一种样本高效的方式利用共享全局参数和稀疏实例特定术语的结构。

    

    决策者经常同时面对许多相关但异质的学习问题。在此工作中，我们研究了一种自然的设置，其中每个学习实例中的未知参数可以分解为共享全局参数加上稀疏的实例特定术语。我们提出了一种新颖的两阶段多任务学习估计器，以一种样本高效的方式利用这种结构，使用健壮统计学（在相似实例上学习）和LASSO回归（去偏差结果）的独特组合。我们的估计器提供了改进的样本复杂度界限。

    Decision-makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real datasets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bound
    

