# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Bridging Dimensions: Confident Reachability for High-Dimensional Controllers](https://rss.arxiv.org/abs/2311.04843) | 本文介绍了一种连接高维控制器的详尽闭环验证方法，通过将高维控制器的行为近似为不同状态空间区域内的低维控制器，平衡了逼近精度和可验证性。 |
| [^2] | [QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection](https://arxiv.org/abs/2404.02595) | 介绍了将量子机器学习和量子计算技术与联邦学习相结合的Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)框架，提出了一种安全、高效的欺诈交易识别方法，显著改进了欺诈检测并确保了数据机密性。 |
| [^3] | [Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2404.00781) | 本文提出了一种新方法，即基于效用的扰动梯度下降（UPGD），通过在梯度更新中应用不同大小的扰动，保护有用单元以防遗忘，同时恢复不太有用单元的可塑性。 |
| [^4] | [Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives](https://arxiv.org/abs/2404.00247) | 本文从迁移学习的角度探讨了如何将其与强化学习相结合，为过程控制带来新的可能性。 |
| [^5] | [Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images](https://arxiv.org/abs/2404.00231) | 这项工作提出了TransDeformer，使用注意力机制实现了对腰椎轮廓的高空间准确性重建，并跨患者实现了网格对应，为医学参数测量提供了可靠性，还设计了变体用于错误估计。 |
| [^6] | [Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models](https://arxiv.org/abs/2403.13890) | 提出了一个多条件潜在扩散模型来学习对比动力学，以减少对静脉内对比剂的依赖性。 |
| [^7] | [Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects](https://arxiv.org/abs/2403.12830) | 本文介绍了针对机器近似遗忘审计任务的明确定义和有效指标，通过改变审计挑战的方式为非成员推断问题，开发了高效的审计指标，简化了评估过程 |
| [^8] | [The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy](https://arxiv.org/abs/2403.05452) | 提出一种新颖的深度学习方法R2D2，用于解决射电天文学中高分辨率高动态范围成像的可扩展性挑战。 |
| [^9] | [An Efficient Difference-of-Convex Solver for Privacy Funnel](https://arxiv.org/abs/2403.04778) | 提出了一种针对隐私漏斗方法的高效求解器，能在已知和未知分布条件下均有效地进行求解，并在实验中展示出优于现有方法的性能。 |
| [^10] | [SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix](https://arxiv.org/abs/2403.02833) | SOFIM利用正则化Fisher信息矩阵和Sherman-Morrison矩阵求逆改善了大规模随机优化中梯度更新的收敛速率，同时解决了数据异质性带来的非平稳目标问题。 |
| [^11] | [Measuring and Controlling Persona Drift in Language Model Dialogs](https://arxiv.org/abs/2402.10962) | 提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移 |
| [^12] | [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146) | 本文介绍了一种增强量子卷积神经网络性能的新框架ResQuNNs，在quanvolutional层中引入可训练性，通过残差学习的概念解决了跨层梯度访问的问题。 |
| [^13] | [Model Collapse Demystified: The Case of Regression](https://arxiv.org/abs/2402.07712) | 本研究在核回归的简化环境中解析了模型崩溃现象，并发现了模型能够处理虚假数据与性能完全崩溃之间的交叉点。通过提出基于自适应正则化的策略，成功缓解了模型崩溃问题。这些发现通过实验证实。 |
| [^14] | [Automatic Segmentation of the Spinal Cord Nerve Rootlets](https://arxiv.org/abs/2402.00724) | 本研究开发了一种自动化的方法，可以从MRI扫描中准确地分割脊髓神经根分支，并且具有良好的性能和稳定性。 |
| [^15] | [Hidden yet quantifiable: A lower bound for confounding strength using randomized trials](https://arxiv.org/abs/2312.03871) | 利用随机试验设计了一种统计检验，能够量化未观察到的混淆强度，并估计其下界，有效应用于现实世界中识别混淆。 |
| [^16] | [Federated Learning with Convex Global and Local Constraints](https://arxiv.org/abs/2310.10117) | 该论文提出了一种针对具有约束的机器学习问题的新联邦学习算法，建立了基于凸目标和凸约束的最坏情况复杂度，并通过数值实验证明了算法的有效性 |
| [^17] | [Cross-Validation Conformal Risk Control.](http://arxiv.org/abs/2401.11974) | 本文提出了一种基于交叉验证的新型合规风险控制方法(CV-CRC)，它扩展了一致性预测的概念，能够控制更广泛的风险函数，并在预测器集合的平均风险上提供了理论保证。 |
| [^18] | [Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation.](http://arxiv.org/abs/2401.11648) | 通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。 |
| [^19] | [Thousands of AI Authors on the Future of AI.](http://arxiv.org/abs/2401.02843) | 数千位AI作者对未来AI的预测显示，到2028年，AI系统有50%的几率实现多个里程碑，包括自主构建全新的付款处理网站、创作一首与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。同时，无需辅助的机器在各种任务上胜过人类的几率估计为10%到2047年为50%。 |
| [^20] | [Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint.](http://arxiv.org/abs/2312.11456) | 该论文研究了在KL约束下的反馈强化学习的理论框架，并提出了有效的算法和实践。实证评估表明，该框架在大型语言模型的对齐实验中表现出良好的效果。 |
| [^21] | [Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy.](http://arxiv.org/abs/2310.14661) | 本文介绍了一种基于纯差分隐私和高斯差分隐私的可计算MCMC私有学习方法，通过引入近似采样扰动算法，结合Metropolis-Hastings算法和局部化步骤，实现了对隐私的保护并获得了较好的收敛性能。 |
| [^22] | [Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing.](http://arxiv.org/abs/2310.12153) | 该论文研究了使用绝热量子计算的平衡K-Means聚类的概率采样方法，通过利用非最优解来计算校准后验概率，实现在D-Wave AQC上识别模糊解决方案和数据点的目标。 |
| [^23] | [IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training.](http://arxiv.org/abs/2310.07355) | IMITATE是一种临床先验指导的分层视觉语言预训练模型。它利用医学报告的层级结构，从胸部X射线图像中提取多级视觉特征，并与分层医学报告中的描述性和结论性文本进行对齐。 |
| [^24] | [Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients.](http://arxiv.org/abs/2310.01012) | 本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。 |
| [^25] | [PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling.](http://arxiv.org/abs/2309.15375) | 通过基于注意力的深度状态空间建模，我们提出了一种不受个体限制的方法，将PPG信号转换为ECG，用于连续性心房颤动检测。 |
| [^26] | [Reconstruction of Unstable Heavy Particles Using Deep Symmetry-Preserving Attention Networks.](http://arxiv.org/abs/2309.01886) | 使用对称保持注意力网络 (Spa-Net) 对不稳定的重粒子进行重建，并扩展其能力以处理多种输入物体类型和全局事件特征。在顶夸克对的半轻子衰变和与希格斯玻色子共同产生的顶夸克对背景下，我们发现了显著的性能改进。 |
| [^27] | [GRASP: A Rehearsal Policy for Efficient Online Continual Learning.](http://arxiv.org/abs/2308.13646) | GRASP是一种新的样本选择策略，根据样本的代表性选择最适合学习的样本，从而提高了在线渐进式学习的效率。 |
| [^28] | [Prediction without Preclusion: Recourse Verification with Reachable Sets.](http://arxiv.org/abs/2308.12820) | 这项研究引入了一种称为后续验证的正式测试程序，用于检测模型分配固定预测的情况。通过开发可靠的机制，可以确定给定模型是否能为决策对象提供后续措施，从而解决了模型分配固定预测可能带来的问题。该研究还展示了如何在真实世界的数据集中确保后续措施和对抗鲁棒性，并探讨了在贷款数据集中实现后续措施的不可行性。 |
| [^29] | [Aligning Agent Policy with Externalities: Reward Design via Bilevel RL.](http://arxiv.org/abs/2308.02585) | 本文提出了一种将强化学习的策略优化与外部性对齐的方法，通过双层优化框架和委托-代理框架，上层学习适当的奖励参数化，下层学习代理人的策略。 |
| [^30] | [Gaussian random field approximation via Stein's method with applications to wide random neural networks.](http://arxiv.org/abs/2306.16308) | 本研究利用Stein方法推导出Wasserstein距离的上界，通过高斯平滑技术将平滑度量转化为Wasserstein距离。通过特殊化结果，我们获得了广义随机神经网络中对高斯随机场逼近的首个上界。 |
| [^31] | [Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data.](http://arxiv.org/abs/2306.14063) | 本论文提出了一种自适应采集数据的离线强化学习策略评估方法，为表格MDPs推导出高概率、实例相关的误差边界，并实现了自适应设置下的极小值最优离线学习。 |
| [^32] | [AnyPredict: Foundation Model for Tabular Prediction.](http://arxiv.org/abs/2305.12081) | 本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。 |
| [^33] | [Multi-Objective Optimization Using the R2 Utility.](http://arxiv.org/abs/2305.11774) | 本文提出将多目标优化问题转化为一组单目标问题进行解决，并介绍了R2效用函数作为适当的目标函数。该效用函数单调且次模，可以使用贪心优化算法计算全局最优解。 |
| [^34] | [Closed-Loop Koopman Operator Approximation.](http://arxiv.org/abs/2303.15318) | 本文提出了一种闭环Koopman算子逼近法，通过利用Koopman算子的线性性质和对控制器和闭环系统结构的了解，可以同时识别闭环和装置系统。 |

# 详细

[^1]: 跨越维度：高维控制器的可信达性

    Bridging Dimensions: Confident Reachability for High-Dimensional Controllers

    [https://rss.arxiv.org/abs/2311.04843](https://rss.arxiv.org/abs/2311.04843)

    本文介绍了一种连接高维控制器的详尽闭环验证方法，通过将高维控制器的行为近似为不同状态空间区域内的低维控制器，平衡了逼近精度和可验证性。

    

    自主系统越来越多地使用端到端学习的控制器进行实现。这样的控制器做出的决策通过图像作为主要感知模式在真实系统上执行。深度神经网络是这种控制器的基本构建模块。然而，现有的神经网络验证工具在处理具有数千个维度的输入时无法扩展，特别是当各个输入（如像素）缺乏明确的物理意义时。本文在连接详尽的闭环验证与高维控制器方面迈出了一步。我们的关键洞见是，高维控制器的行为可以用不同状态空间区域内的几个低维控制器来近似。为了平衡低维控制器的逼近精度和可验证性，我们利用了最新的验证感知知识蒸馏。然后，如果低维可达性结果已经得到了收敛。

    Autonomous systems are increasingly implemented using end-to-end learning-based controllers. Such controllers make decisions that are executed on the real system with images as one of the primary sensing modalities. Deep neural networks form a fundamental building block of such controllers. Unfortunately, the existing neural-network verification tools do not scale to inputs with thousands of dimensions -- especially when the individual inputs (such as pixels) are devoid of clear physical meaning. This paper takes a step towards connecting exhaustive closed-loop verification with high-dimensional controllers. Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space. To balance the approximation accuracy and verifiability of our low-dimensional controllers, we leverage the latest verification-aware knowledge distillation. Then, if low-dimensional reachability results are infl
    
[^2]: QFNN-FFD：用于金融欺诈检测的量子联邦神经网络

    QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection

    [https://arxiv.org/abs/2404.02595](https://arxiv.org/abs/2404.02595)

    介绍了将量子机器学习和量子计算技术与联邦学习相结合的Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)框架，提出了一种安全、高效的欺诈交易识别方法，显著改进了欺诈检测并确保了数据机密性。

    

    这项研究介绍了Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)，这是一个融合了量子机器学习（QML）和量子计算技术与联邦学习（FL）的前沿框架，用于创新金融欺诈检测。利用量子技术的计算能力和FL的数据隐私，QFNN-FFD提出了一种安全、高效的识别欺诈交易的方法。在分布式客户端实施双阶段训练模型超越了现有的性能方法。QFNN-FFD显著改进了欺诈检测并确保了数据机密性，标志着金融科技解决方案的重大进步，并为以隐私为重点的欺诈检测建立了新标准。

    arXiv:2404.02595v1 Announce Type: cross  Abstract: This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.
    
[^3]: 处理连续学习中的可塑性丧失和灾难性遗忘

    Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning

    [https://arxiv.org/abs/2404.00781](https://arxiv.org/abs/2404.00781)

    本文提出了一种新方法，即基于效用的扰动梯度下降（UPGD），通过在梯度更新中应用不同大小的扰动，保护有用单元以防遗忘，同时恢复不太有用单元的可塑性。

    

    深度表示学习方法在连续学习中存在困难，既遭受有用单元的灾难性遗忘，又因僵化和无用单元导致可塑性丢失。虽然许多方法分别解决这两个问题，但目前只有少数方法能同时处理这两个问题。本文引入了基于效用的扰动梯度下降（UPGD）作为一种用于表示持续学习的新方法。UPGD结合了梯度更新和扰动，它对更有用的单元应用较小的修改，保护它们免受遗忘，对不太有用的单元应用较大的修改，恢复它们的可塑性。

    arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta
    
[^4]: 利用迁移学习促进过程控制的强化学习：观点

    Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives

    [https://arxiv.org/abs/2404.00247](https://arxiv.org/abs/2404.00247)

    本文从迁移学习的角度探讨了如何将其与强化学习相结合，为过程控制带来新的可能性。

    

    本文从迁移学习的角度，为过程控制中的深度强化学习（DRL）提供了深入见解。我们分析了在过程工业领域应用DRL所面临的挑战，以及引入迁移学习的必要性。此外，我们为未来研究方向提供了建议和展望，探讨了如何将迁移学习与DRL结合起来加强过程控制。

    arXiv:2404.00247v1 Announce Type: cross  Abstract: This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.
    
[^5]: 基于注意力机制的形状变形网络用于无伪影几何重构骨盆腰椎MR图像

    Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images

    [https://arxiv.org/abs/2404.00231](https://arxiv.org/abs/2404.00231)

    这项工作提出了TransDeformer，使用注意力机制实现了对腰椎轮廓的高空间准确性重建，并跨患者实现了网格对应，为医学参数测量提供了可靠性，还设计了变体用于错误估计。

    

    腰椎椎间盘退变，是腰椎间盘渐进性结构性磨损，被认为在腰部疼痛中发挥重要作用，这是一个重要的全球健康关注焦点。从MR图像中自动重建腰椎几何形状，将使医学参数的快速测量成为可能，以评估腰椎状态，从而确定合适的治疗方案。现有的基于图像分割的技术通常会生成错误的分割或不适合医学参数测量的无结构点云。在这项工作中，我们提出了TransDeformer：一种新颖的基于注意力机制的深度学习方法，以高空间准确度和患者间网格对应的方式重建腰椎轮廓，并且我们还提出了一种TransDeformer的变种用于错误估计。特别是，我们设计了新的注意力模块和新的注意力公式，将图像特征和标记化的轮廓特征集成起来，用于预测...

    arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
    
[^6]: 以多条件潜在扩散模型学习对比动力学

    Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models

    [https://arxiv.org/abs/2403.13890](https://arxiv.org/abs/2403.13890)

    提出了一个多条件潜在扩散模型来学习对比动力学，以减少对静脉内对比剂的依赖性。

    

    动态对比增强磁共振成像中的对比剂可以定位肿瘤并观察其对比动力学，这对于癌症表征和治疗决策至关重要。然而，对比剂的使用不仅与不良健康风险相关，而且对于怀孕患者、肾功能障碍患者或其他不良反应患者存在限制。由于对比剂摄取是病灶恶性、癌症复发风险和治疗反应的关键生物标志物，因此减少静脉内对比剂的依赖性变得至关重要。为此，我们提出了一个能够进行DCE-MRI时间序列的获取时间条件图像合成的多条件潜在扩散模型。为了评估医学图像合成，我们还提出并验证了基于生物标志物变异性的Fr\'echet放射组学距离作为图像质量度量。

    arXiv:2403.13890v1 Announce Type: cross  Abstract: Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\'echet radiomics distance as an image quality measure based on biomarker variability 
    
[^7]: 机器近似遗忘已经得到适当评估了吗？从审计到副作用

    Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects

    [https://arxiv.org/abs/2403.12830](https://arxiv.org/abs/2403.12830)

    本文介绍了针对机器近似遗忘审计任务的明确定义和有效指标，通过改变审计挑战的方式为非成员推断问题，开发了高效的审计指标，简化了评估过程

    

    随着对数据隐私和安全日益关注，彻底从机器学习模型中删除数据血统的机器近似遗忘变得至关重要。MLaaS提供者希望将其视为符合监管合规性的最终保障。然而，尽管其至关重要，隐私社区验证机器近似遗忘效果的方法发展和实施的速度令人失望，这一关键领域经常未能得到足够关注。本文旨在通过引入明确定义且有效的指标，为黑盒遗忘审计任务提供解决方法。我们将审计挑战转化为非成员推断问题，并开发出高效的审计指标。通过仅依赖原始和已遗忘模型--消除了训练额外阴影模型的需要--我们的方法简化了评估过程。

    arXiv:2403.12830v1 Announce Type: new  Abstract: The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models. MLaaS providers expect this to be their ultimate safeguard for regulatory compliance. Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus. This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks. We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing. By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation
    
[^8]: 用于射电天文学中快速精密成像的R2D2深度神经网络系列范式

    The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy

    [https://arxiv.org/abs/2403.05452](https://arxiv.org/abs/2403.05452)

    提出一种新颖的深度学习方法R2D2，用于解决射电天文学中高分辨率高动态范围成像的可扩展性挑战。

    

    射电干涉成像需要解决来自大数据量的高分辨率高动态范围逆问题。最近基于优化理论的图像重建技术展示了惊人的成像精度能力，远远超出了CLEAN的能力。这些方法包括由手工设计的正则化算子推动的先进近端算法，如SARA系列，以及由学习正则化去噪器推动的混合插拔（PnP）算法，如AIRI。然而，优化和PnP结构高度迭代，这阻碍了它们处理未来仪器预期的极端数据大小的能力。为解决这一可扩展性挑战，我们引入一种新颖的深度学习方法，命名为“用于高动态范围成像的残差对残差DNN系列”。R2D2的重建被形成为一系列残差图像，这些图像作为深度神经网络的输出，通过迭代估计而得到。

    arXiv:2403.05452v1 Announce Type: cross  Abstract: Radio-interferometric (RI) imaging entails solving high-resolution high-dynamic range inverse problems from large data volumes. Recent image reconstruction techniques grounded in optimization theory have demonstrated remarkable capability for imaging precision, well beyond CLEAN's capability. These range from advanced proximal algorithms propelled by handcrafted regularization operators, such as the SARA family, to hybrid plug-and-play (PnP) algorithms propelled by learned regularization denoisers, such as AIRI. Optimization and PnP structures are however highly iterative, which hinders their ability to handle the extreme data sizes expected from future instruments. To address this scalability challenge, we introduce a novel deep learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic range imaging'. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Netw
    
[^9]: 一种用于隐私漏斗的高效凸差分求解器

    An Efficient Difference-of-Convex Solver for Privacy Funnel

    [https://arxiv.org/abs/2403.04778](https://arxiv.org/abs/2403.04778)

    提出了一种针对隐私漏斗方法的高效求解器，能在已知和未知分布条件下均有效地进行求解，并在实验中展示出优于现有方法的性能。

    

    我们提出了一种针对隐私漏斗（PF）方法的高效求解器，利用其凸差分（DC）结构。所提出的DC分离导致了闭式更新方程，可以直接应用于已知和未知分布设置。对于已知分布情况，我们证明了所提出的非贪婪求解器的收敛性（局部稳定点），并在经验上展示它在表征隐私-效用权衡方面优于现有技术方法。我们的DC方法洞察力适用于具有标记经验样本的未知分布设置。利用这些洞察力，我们的交替最小化求解器满足了PF的基本Markov关系，与以往基于变分推理的求解器相比。在经验上，我们使用MNIST和Fashion-MNIST数据集评估了所提出的求解器。

    arXiv:2403.04778v1 Announce Type: new  Abstract: We propose an efficient solver for the privacy funnel (PF) method, leveraging its difference-of-convex (DC) structure. The proposed DC separation results in a closed-form update equation, which allows straightforward application to both known and unknown distribution settings. For known distribution case, we prove the convergence (local stationary points) of the proposed non-greedy solver, and empirically show that it outperforms the state-of-the-art approaches in characterizing the privacy-utility trade-off. The insights of our DC approach apply to unknown distribution settings where labeled empirical samples are available instead. Leveraging the insights, our alternating minimization solver satisfies the fundamental Markov relation of PF in contrast to previous variational inference-based solvers. Empirically, we evaluate the proposed solver with MNIST and Fashion-MNIST datasets. Our results show that under a comparable reconstruction 
    
[^10]: SOFIM: 使用正则化Fisher信息矩阵的随机优化

    SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix

    [https://arxiv.org/abs/2403.02833](https://arxiv.org/abs/2403.02833)

    SOFIM利用正则化Fisher信息矩阵和Sherman-Morrison矩阵求逆改善了大规模随机优化中梯度更新的收敛速率，同时解决了数据异质性带来的非平稳目标问题。

    

    这篇论文介绍了一种新的基于正则化Fisher信息矩阵（FIM）的随机优化方法，称为SOFIM，可以有效利用FIM来逼近Hessian矩阵，以找到大规模随机优化机器学习模型中的牛顿梯度更新。可以视为自然梯度下降（NGD）的一种变体，通过使用正则化FIM和直接通过Sherman-Morrison矩阵求逆找到梯度更新方向来解决存储和计算完整FIM的挑战。此外，像广受欢迎的Adam方法一样，SOFIM利用梯度的第一时刻来处理由异构数据引起的跨小批次非平稳目标的问题。正则化FIM和Sherman-Morrison矩阵求逆的利用导致了收敛速率的改善，同时space和time复杂度与随机梯度下降相同。

    arXiv:2403.02833v1 Announce Type: new  Abstract: This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (
    
[^11]: 在语言模型对话中测量和控制“人设”漂移

    Measuring and Controlling Persona Drift in Language Model Dialogs

    [https://arxiv.org/abs/2402.10962](https://arxiv.org/abs/2402.10962)

    提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移

    

    提示是定制语言模型聊天机器人的标准工具，使其能够承担特定的“人设”。在使用提示时的一个隐含假设是，它们将是稳定的，因此聊天机器人将在整个对话过程中继续根据规定的“人设”生成文本。我们提出了一个量化基准来测试这一假设，通过两个个性化聊天机器人之间的自我对话来评估“人设”的稳定性。我们对流行模型如LLaMA2-chat-70B进行测试，发现在八轮对话中存在显著的“人设”漂移。对这一现象的实证和理论分析表明，由于长对话中的注意力衰减，变压器注意力机制起到了一定作用。为了对抗注意力衰减和“人设”漂移，我们提出了一种称为split-softmax的轻量级方法，与两个强基线方法相比表现优异。

    arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
    
[^12]: ResQuNNs: 实现量子卷积神经网络中深度学习的新框架

    ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks

    [https://arxiv.org/abs/2402.09146](https://arxiv.org/abs/2402.09146)

    本文介绍了一种增强量子卷积神经网络性能的新框架ResQuNNs，在quanvolutional层中引入可训练性，通过残差学习的概念解决了跨层梯度访问的问题。

    

    本文提出了一种增强量子卷积神经网络（QuNNs）性能的新框架，通过引入可训练的quanvolutional层并解决与其相关的关键挑战。传统的quanvolutional层虽然有助于特征提取，但往往是静态的，适应性有限。与最先进的研究不同，我们的研究通过在这些层内部进行训练，显著提高了QuNNs的灵活性和潜力。然而，多个可训练的quanvolutional层的引入给基于梯度的优化带来了复杂性，主要是由于难以在这些层之间访问梯度。为了解决这个问题，我们提出了一种新的架构，Residual Quanvolutional Neural Networks (ResQuNNs)，利用残差学习的概念，在这些层之间添加跳过连接以促进梯度的流动。

    arXiv:2402.09146v1 Announce Type: new Abstract: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between 
    
[^13]: 模型崩溃解密：回归案例研究

    Model Collapse Demystified: The Case of Regression

    [https://arxiv.org/abs/2402.07712](https://arxiv.org/abs/2402.07712)

    本研究在核回归的简化环境中解析了模型崩溃现象，并发现了模型能够处理虚假数据与性能完全崩溃之间的交叉点。通过提出基于自适应正则化的策略，成功缓解了模型崩溃问题。这些发现通过实验证实。

    

    在像ChatGPT这样的大型语言模型的时代，"模型崩溃"现象指的是模型在递归地训练自身上一代又一代生成的数据时，其性能逐渐降低，最终变得完全无用，即模型崩溃。在这项工作中，我们在核回归的简化环境中研究了这一现象，并获得了结果，显示模型能够处理虚假数据与模型性能完全崩溃之间存在明显的交叉点。在多项式衰减的光谱和源条件下，我们获得了修改后的缩放定律，展示了从快速到缓慢速率的新交叉现象。我们还提出了基于自适应正则化的简单策略来缓解模型崩溃。我们的理论结果通过实验证实。

    In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.
    
[^14]: 脊髓神经根分支的自动分割

    Automatic Segmentation of the Spinal Cord Nerve Rootlets

    [https://arxiv.org/abs/2402.00724](https://arxiv.org/abs/2402.00724)

    本研究开发了一种自动化的方法，可以从MRI扫描中准确地分割脊髓神经根分支，并且具有良好的性能和稳定性。

    

    准确识别脊髓神经根分支对于描绘脊髓的功能活动具有重要意义。本研究旨在开发一种自动方法，用于从T2加权磁共振成像（MRI）扫描中语义分割脊髓神经根分支。通过使用两个开放存取的MRI数据集中的图像，采用主动学习方法训练了一个三维多类卷积神经网络，以分割C2-C8背面神经根分支。每个输出类别对应一个脊髓水平。该方法在训练阶段未见的数据集中进行了测试，以评估不同场地、不同会话和不同分辨率之间的变异性。测试结果的Dice分数为0.67+-0.16（均值+-标准差，针对不同神经根分支水平），表明方法具有良好的性能。该方法还展示了低厂商间和场地间的变异性（变异系数<=1.41%），以及低会话间变异性（变异系数

    Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 
    
[^15]: 隐蔽而可量化：使用随机试验的混淆强度下界

    Hidden yet quantifiable: A lower bound for confounding strength using randomized trials

    [https://arxiv.org/abs/2312.03871](https://arxiv.org/abs/2312.03871)

    利用随机试验设计了一种统计检验，能够量化未观察到的混淆强度，并估计其下界，有效应用于现实世界中识别混淆。

    

    在快节奏精准医学时代，观察性研究在正确评估临床实践中新疗法方面发挥着重要作用。然而，未观察到的混淆可能严重损害从非随机数据中得出的因果结论。我们提出了一种利用随机试验来量化未观察到的混淆的新策略。首先，我们设计了一种统计检验来检测强度超过给定阈值的未观察到的混淆。然后，我们使用该检验来估计未观察到的混淆强度的渐近有效下界。我们在几个合成和半合成数据集上评估了我们的统计检验的功效和有效性。此外，我们展示了我们的下界如何能够在真实环境中正确识别未观察到的混淆的存在和不存在。

    arXiv:2312.03871v2 Announce Type: replace-cross  Abstract: In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. We propose a novel strategy that leverages randomized trials to quantify unobserved confounding. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.
    
[^16]: 具有凸全局和局部约束的联邦学习

    Federated Learning with Convex Global and Local Constraints

    [https://arxiv.org/abs/2310.10117](https://arxiv.org/abs/2310.10117)

    该论文提出了一种针对具有约束的机器学习问题的新联邦学习算法，建立了基于凸目标和凸约束的最坏情况复杂度，并通过数值实验证明了算法的有效性

    

    在实践中，许多机器学习（ML）问题都带有约束，其应用领域涉及无法与他人共享的分布式敏感数据，例如在医疗保健中。在这种实际场景中的协作学习涉及联邦学习（FL）用于带约束的ML问题，或简称带约束的FL。尽管近年来FL技术得到了广泛发展，但这些技术仅处理无约束的FL问题。为了填补这一空白，我们迈出了解决带约束FL问题的通用算法框架的第一步。具体而言，我们基于近端增广拉格朗日（AL）方法，提出了一种针对受限制ML问题的新FL算法。假设凸目标和凸约束以及其他一些温和条件，我们确定了所提算法的最坏情况复杂度。我们的数值实验证明了我们的算法的有效性。

    arXiv:2310.10117v2 Announce Type: replace  Abstract: In practice, many machine learning (ML) problems come with constraints, and their applied domains involve distributed sensitive data that cannot be shared with others, e.g., in healthcare. Collaborative learning in such practical scenarios entails federated learning (FL) for ML problems with constraints, or FL with constraints for short. Despite the extensive developments of FL techniques in recent years, these techniques only deal with unconstrained FL problems. To fill this gap, we take the first step toward building a general algorithmic framework for solving FL problems with constraints. In particular, we propose a new FL algorithm for constrained ML problems based on the proximal augmented Lagrangian (AL) method. Assuming convex objective and convex constraints plus other mild conditions, we establish the worst-case complexity of the proposed algorithm. Our numerical experiments show the effectiveness of our algorithm in perform
    
[^17]: 交叉验证合规风险控制

    Cross-Validation Conformal Risk Control. (arXiv:2401.11974v1 [cs.LG])

    [http://arxiv.org/abs/2401.11974](http://arxiv.org/abs/2401.11974)

    本文提出了一种基于交叉验证的新型合规风险控制方法(CV-CRC)，它扩展了一致性预测的概念，能够控制更广泛的风险函数，并在预测器集合的平均风险上提供了理论保证。

    

    合规风险控制（CRC）是一种最近提出的技术，它应用于传统的点预测器上，以提供校准保证。在CRC中推广一致性预测（CP），通过从点预测器中提取一个预测器集合来控制风险函数（如误覆盖概率或错误负例率），从而确保校准性。原始的CRC需要将可用数据集分为训练和验证数据集。当数据可用性有限时，这可能导致预测器集合效率低下。本文介绍了一种基于交叉验证而不是原始CRC的新型CRC方法。所提出的交叉验证CRC（CV-CRC）将CP的一种版本扩展到CRC，可以控制更广泛的风险函数。CV-CRC被证明在预测器集合的平均风险上具有理论保证。此外，通过数值实验证明CV-CRC在实践中的有效性。

    Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. The original CRC requires the available data set to be split between training and validation data sets. This can be problematic when data availability is limited, resulting in inefficient set predictors. In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC. The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor. Furthermore, numerical exper
    
[^18]: 通过具有分层正则化的医学代码中心的多模态对比EHR建模预测下次就诊诊断

    Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11648](http://arxiv.org/abs/2401.11648)

    通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。

    

    在医疗保健中，利用电子健康记录（EHR）预测下次就诊的诊断是一项必要的任务，对于制定医疗保健提供者和患者的主动未来计划至关重要。然而，之前的许多研究并没有充分解决EHR数据固有的异构和分层特征，必然导致次优的性能。为此，我们提出了NECHO，一种新颖的医学代码中心的多模态对比EHR学习框架，其中包括分层正则化。首先，我们使用定制的网络设计和一对双模态对比损失融合涵盖医学代码、人口统计数据和临床笔记的多方面信息，所有这些都围绕着医学代码表现。我们还使用医学本体中的父级信息来规范特定模态的编码器，以学习EHR数据的层次结构。对MIMIC-III数据进行的一系列实验证明了我们方法的有效性。

    Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
    
[^19]: 数千位AI作者对未来AI的预测

    Thousands of AI Authors on the Future of AI. (arXiv:2401.02843v1 [cs.CY])

    [http://arxiv.org/abs/2401.02843](http://arxiv.org/abs/2401.02843)

    数千位AI作者对未来AI的预测显示，到2028年，AI系统有50%的几率实现多个里程碑，包括自主构建全新的付款处理网站、创作一首与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。同时，无需辅助的机器在各种任务上胜过人类的几率估计为10%到2047年为50%。

    

    在迄今为止最大规模的调查中，2778名在顶级人工智能（AI）会议上发表过论文的研究人员对AI进展的速度、高级AI系统的性质和影响进行了预测。总体预测显示，到2028年，AI系统至少有50%的几率实现多个里程碑，包括自主构建一个全新的付款处理网站、创作一首可以与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。如果科学持续不受干扰，2027年无需辅助的机器在各种任务上胜过人类的几率估计为10%，到2047年为50%。后者的估计比我们一年前进行的类似调查[Grace et al., 2022]提前了13年。然而，所有人类职业完全可自动化的几率预计要到2037年达到10%，到2116年才达到50%（与2022年调查中的2164年相比）。

    In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).  Most
    
[^20]: 人类反馈的迭代偏好学习：在KL约束下将理论与实践联系起来的RLHF

    Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11456](http://arxiv.org/abs/2312.11456)

    该论文研究了在KL约束下的反馈强化学习的理论框架，并提出了有效的算法和实践。实证评估表明，该框架在大型语言模型的对齐实验中表现出良好的效果。

    

    本文研究了生成模型与强化学习从人类反馈中的对齐过程的理论框架。我们考虑了一个标准的数学表达式，即反向KL正则化的上下文多臂赌博机用于RLHF。尽管它被广泛应用于实际应用，但对这个公式的严格理论分析仍然很开放。我们研究了它在离线、在线和混合三种不同场景下的行为，并提出了具有有限样本理论保证的高效算法。朝着实际应用的方向，我们的框架通过对信息理论策略改进预言的稳健近似，自然地产生了几种新颖的RLHF算法。这包括在线场景中的迭代版本的直接偏好优化(DPO)算法，以及离线情景下的多步拒绝抽样策略。我们对大型语言模型的真实对齐实验进行了实证评估。

    This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
    
[^21]: 基于纯差分隐私和高斯差分隐私的可计算MCMC私有学习方法。

    Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy. (arXiv:2310.14661v1 [cs.LG])

    [http://arxiv.org/abs/2310.14661](http://arxiv.org/abs/2310.14661)

    本文介绍了一种基于纯差分隐私和高斯差分隐私的可计算MCMC私有学习方法，通过引入近似采样扰动算法，结合Metropolis-Hastings算法和局部化步骤，实现了对隐私的保护并获得了较好的收敛性能。

    

    后验采样即从后验分布中采样的指数机制，提供ε-纯差分隐私（DP）保证，并不受（ε，δ）-近似DP引入的潜在无界隐私泄漏的影响。然而，在实践中，需要应用近似采样方法，如马尔科夫链蒙特卡洛（MCMC），从而重新引入了对隐私保证的δ-近似误差。为了弥合这一差距，我们提出了近似采样扰动（即ASAP）算法，该算法通过与满足纯DP或纯高斯DP（即δ=0）的参考分布有界Wasserstein无穷距离的MCMC样本加噪声。然后利用Metropolis-Hastings算法生成样本并证明算法在W$_\infty$距离上收敛。我们展示了通过将我们的新技术与细致的局部化步骤相结合，我们获得了第一个可计算MCMC私有学习方法。

    Posterior sampling, i.e., exponential mechanism to sample from the posterior distribution, provides $\varepsilon$-pure differential privacy (DP) guarantees and does not suffer from potentially unbounded privacy breach introduced by $(\varepsilon,\delta)$-approximate DP. In practice, however, one needs to apply approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus re-introducing the unappealing $\delta$-approximation error into the privacy guarantees. To bridge this gap, we propose the Approximate SAample Perturbation (abbr. ASAP) algorithm which perturbs an MCMC sample with noise proportional to its Wasserstein-infinity ($W_\infty$) distance from a reference distribution that satisfies pure DP or pure Gaussian DP (i.e., $\delta=0$). We then leverage a Metropolis-Hastings algorithm to generate the sample and prove that the algorithm converges in W$_\infty$ distance. We show that by combining our new techniques with a careful localization step, we obtain the first ne
    
[^22]: 使用绝热量子计算的平衡K-Means的概率采样

    Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing. (arXiv:2310.12153v1 [cs.LG])

    [http://arxiv.org/abs/2310.12153](http://arxiv.org/abs/2310.12153)

    该论文研究了使用绝热量子计算的平衡K-Means聚类的概率采样方法，通过利用非最优解来计算校准后验概率，实现在D-Wave AQC上识别模糊解决方案和数据点的目标。

    

    绝热量子计算（AQC）是一种有望用于离散且通常为NP困难优化问题的量子计算方法。目前的AQC允许实现感兴趣的问题，这促使了为许多机器学习和计算机视觉任务开发量子表示的发展。尽管需要从噪声AQC进行多次测量，但当前方法仅利用最佳测量，丢弃了其他测量中包含的信息。在这项工作中，我们探讨了利用这些信息进行概率平衡k-means聚类的潜力。我们提出了使用非最优解来计算校准后验概率的方法，计算成本很低。这使我们能够识别模糊的解决方案和数据点，我们在D-Wave AQC上使用合成和真实数据进行了验证。

    Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.
    
[^23]: IMITATE: 临床先验指导的分层视觉语言预训练模型

    IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])

    [http://arxiv.org/abs/2310.07355](http://arxiv.org/abs/2310.07355)

    IMITATE是一种临床先验指导的分层视觉语言预训练模型。它利用医学报告的层级结构，从胸部X射线图像中提取多级视觉特征，并与分层医学报告中的描述性和结论性文本进行对齐。

    

    在医学视觉语言预训练（VLP）领域，人们致力于从临床报告和相关医学图像中提取文本和图像特征。然而，大多数现有的方法可能忽视了利用临床报告固有的层级结构的机会，这些报告通常被分为描述性内容的“发现”和结论性观察的“印象”。当前的医学VLP方法往往将报告简化为一个统一的实体或分散的标记，而没有利用这种丰富的、结构化的格式。在这项工作中，我们提出了一种新的临床先验指导的VLP框架，名为IMITATE，用于从医学报告中学习结构信息，并使用分层视觉语言对齐。该框架从胸部X射线（CXR）图像中提取多级视觉特征，并将这些特征与分层医学报告中的描述性和结论性文本分别对齐。

    In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
    
[^24]: CCA家族的高效算法：无约束目标与无偏梯度

    Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])

    [http://arxiv.org/abs/2310.01012](http://arxiv.org/abs/2310.01012)

    本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。

    

    典型相关分析（CCA）方法在多视角学习中具有基础性作用。正则化线性CCA方法可以看作是偏最小二乘（PLS）的推广，并与广义特征值问题（GEP）框架统一。然而，这些线性方法的传统算法在大规模数据上计算上是不可行的。深度CCA的扩展显示出很大的潜力，但目前的训练过程缓慢且复杂。我们首先提出了一个描述GEPs的顶级子空间的新颖无约束目标。我们的核心贡献是一系列快速算法，用随机梯度下降（SGD）应用于相应的CCA目标，从而获得随机PLS、随机CCA和深度CCA。这些方法在所有标准CCA和深度CCA基准测试中显示出比先前最先进方法更快的收敛速度和更高的相关性恢复。这样的速度使我们能够首次进行大规模生物数据的PLS分析。

    The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
    
[^25]: 通过基于注意力的深度状态空间建模，将PPG信号转换为ECG，用于连续性心房颤动检测

    PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling. (arXiv:2309.15375v1 [cs.LG])

    [http://arxiv.org/abs/2309.15375](http://arxiv.org/abs/2309.15375)

    通过基于注意力的深度状态空间建模，我们提出了一种不受个体限制的方法，将PPG信号转换为ECG，用于连续性心房颤动检测。

    

    电信号图（ECG或EKG）是一种测量心脏电活动的医学测试。ECG常用于诊断和监测各种心脏疾病，包括心律失常、心肌梗塞和心力衰竭。然而，传统的ECG需要临床测量，限制了其在医疗机构的应用。相比之下，单导联ECG已经在佩戴式设备上应用广泛。另一种ECG的替代方法是光浊度脉搏检测（PPG），它采用非侵入性、低成本的光学方法来测量心脏生理学，使其成为捕捉日常生活中重要心脏信号的合适选择。虽然ECG和PPG之间具有强烈的相关性，但后者并没有提供明显的临床诊断价值。在这里，我们提出了一种不受个体限制的基于注意力的深度状态空间模型，用于将PPG信号转换为ECG，从而实现连续性心房颤动检测。

    An electrocardiogram (ECG or EKG) is a medical test that measures the heart's electrical activity. ECGs are often used to diagnose and monitor a wide range of heart conditions, including arrhythmias, heart attacks, and heart failure. On the one hand, the conventional ECG requires clinical measurement, which restricts its deployment to medical facilities. On the other hand, single-lead ECG has become popular on wearable devices using administered procedures. An alternative to ECG is Photoplethysmography (PPG), which uses non-invasive, low-cost optical methods to measure cardiac physiology, making it a suitable option for capturing vital heart signs in daily life. As a result, it has become increasingly popular in health monitoring and is used in various clinical and commercial wearable devices. While ECG and PPG correlate strongly, the latter does not offer significant clinical diagnostic value. Here, we propose a subject-independent attention-based deep state-space model to translate P
    
[^26]: 使用深度对称保持注意力网络重建不稳定的重粒子

    Reconstruction of Unstable Heavy Particles Using Deep Symmetry-Preserving Attention Networks. (arXiv:2309.01886v2 [hep-ex] UPDATED)

    [http://arxiv.org/abs/2309.01886](http://arxiv.org/abs/2309.01886)

    使用对称保持注意力网络 (Spa-Net) 对不稳定的重粒子进行重建，并扩展其能力以处理多种输入物体类型和全局事件特征。在顶夸克对的半轻子衰变和与希格斯玻色子共同产生的顶夸克对背景下，我们发现了显著的性能改进。

    

    重建不稳定的重粒子需要精密的技术来筛选出将探测器物体分配给底层粒子的大量可能排列。一种基于广义注意机制的方法，称为对称保持注意力网络 (Spa-Net)，先前已应用于只产生强子喷注的大型强子对撞机中的顶夸克对衰变。在这里，我们扩展了 Spa-Net 架构来考虑多种输入物体类型，如轻子，以及全局事件特征，如失去的横向动量。此外，我们提供回归和分类输出以补充粒子分配。我们在顶夸克对的半轻子衰变以及与希格斯玻色子共同产生的顶夸克对的背景下探讨了 Spa-Net 扩展能力的性能。我们发现在三个代表性研究的能力方面有显著的改进：ttH 搜索、顶夸克对的测量等等。

    Reconstructing unstable heavy particles requires sophisticated techniques to sift through the large number of possible permutations for assignment of detector objects to the underlying partons. An approach based on a generalized attention mechanism, symmetry preserving attention networks (Spa-Net), has been previously applied to top quark pair decays at the Large Hadron Collider which produce only hadronic jets. Here we extend the Spa-Net architecture to consider multiple input object types, such as leptons, as well as global event features, such as the missing transverse momentum. In addition, we provide regression and classification outputs to supplement the parton assignment. We explore the performance of the extended capability of Spa-Net in the context of semi-leptonic decays of top quark pairs as well as top quark pairs produced in association with a Higgs boson. We find significant improvements in the power of three representative studies: a search for ttH, a measurement of the 
    
[^27]: GRASP: 一种高效的在线渐进式学习的重演策略

    GRASP: A Rehearsal Policy for Efficient Online Continual Learning. (arXiv:2308.13646v1 [cs.LG])

    [http://arxiv.org/abs/2308.13646](http://arxiv.org/abs/2308.13646)

    GRASP是一种新的样本选择策略，根据样本的代表性选择最适合学习的样本，从而提高了在线渐进式学习的效率。

    

    深度神经网络中的渐进学习涉及从不断增长的数据流中逐步累积知识。渐进学习的一个主要挑战是非平稳的数据流会导致之前学到的能力遭受灾难性遗忘。重演是一种常用且有效的缓解这个问题的方法，即将过去的观测结果存储在缓冲区中，并在学习过程中将它们与新的观测结果混合。这带来了一个问题：应该选择哪些存储样本进行重演？选择最适合学习的样本而不是随机选择样本，可能会导致学习速度显著加快。对于类增量学习，先前的研究表明简单的类均衡随机选择策略优于更复杂的方法。在这里，我们通过探索一种新的样本选择策略GRASP重新思考这个问题。GRASP首先选择最具代表性的样本，然后逐渐选择较不具代表性的样本。

    Continual learning (CL) in deep neural networks (DNNs) involves incrementally accumulating knowledge in a DNN from a growing data stream. A major challenge in CL is that non-stationary data streams cause catastrophic forgetting of previously learned abilities. Rehearsal is a popular and effective way to mitigate this problem, which is storing past observations in a buffer and mixing them with new observations during learning. This leads to a question: Which stored samples should be selected for rehearsal? Choosing samples that are best for learning, rather than simply selecting them at random, could lead to significantly faster learning. For class incremental learning, prior work has shown that a simple class balanced random selection policy outperforms more sophisticated methods. Here, we revisit this question by exploring a new sample selection policy called GRASP. GRASP selects the most prototypical (class representative) samples first and then gradually selects less prototypical (h
    
[^28]: 不排除预测：基于可达集的后续验证方法

    Prediction without Preclusion: Recourse Verification with Reachable Sets. (arXiv:2308.12820v1 [cs.LG])

    [http://arxiv.org/abs/2308.12820](http://arxiv.org/abs/2308.12820)

    这项研究引入了一种称为后续验证的正式测试程序，用于检测模型分配固定预测的情况。通过开发可靠的机制，可以确定给定模型是否能为决策对象提供后续措施，从而解决了模型分配固定预测可能带来的问题。该研究还展示了如何在真实世界的数据集中确保后续措施和对抗鲁棒性，并探讨了在贷款数据集中实现后续措施的不可行性。

    

    机器学习模型常被用于决定谁有资格得到贷款、面试或公共福利。标准技术用于构建这些模型时，会使用关于人的特征，但忽视他们的可操作性。因此，模型可能会分配固定的预测，这意味着被拒绝贷款、面试或福利的消费者可能永久被排除在获得信贷、就业或援助的机会之外。在这项工作中，我们引入了一种正式的测试程序来检测分配固定预测的模型，我们称之为后续验证。我们开发了一套机制可靠地确定给定模型是否能提供对决策对象的后续手段，这些手段由用户指定的可操作性约束确定。我们演示了我们的工具如何在真实世界的数据集中确保后续措施和对抗鲁棒性，并利用它们研究了在真实世界的贷款数据集中实现后续措施的不可行性。我们的结果凸显了模型如何无意中分配固定预测，从而永久禁止使用者获得相关权益。

    Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar acces
    
[^29]: 将代理策略与外部性对齐：通过双层强化学习设计奖励

    Aligning Agent Policy with Externalities: Reward Design via Bilevel RL. (arXiv:2308.02585v1 [cs.LG])

    [http://arxiv.org/abs/2308.02585](http://arxiv.org/abs/2308.02585)

    本文提出了一种将强化学习的策略优化与外部性对齐的方法，通过双层优化框架和委托-代理框架，上层学习适当的奖励参数化，下层学习代理人的策略。

    

    在强化学习中，通常在策略优化过程的开始处假设一个奖励函数。在这种固定奖励范式下的学习中，可能会忽略重要的策略优化考虑因素，比如状态空间覆盖和安全性。此外，它可能无法涵盖社会福利、可持续性或市场稳定性方面的更广泛影响，可能导致不可取的 emergent 行为和可能不对齐的策略。为了数学化地概括将强化学习的策略优化与这种外在性对齐问题，我们考虑了一个双层优化问题，并将其与委托-代理框架相联系，在这个框架中，委托人在上层确定系统的更广泛目标和约束，代理人在下层解决一个马尔可夫决策过程。上层任务是学习一个与更广泛目标相对应的适当奖励参数化，下层任务是学习代理人的策略。

    In reinforcement learning (RL), a reward function is often assumed at the outset of a policy optimization procedure. Learning in such a fixed reward paradigm in RL can neglect important policy optimization considerations, such as state space coverage and safety. Moreover, it can fail to encompass broader impacts in terms of social welfare, sustainability, or market stability, potentially leading to undesirable emergent behavior and potentially misaligned policy. To mathematically encapsulate the problem of aligning RL policy optimization with such externalities, we consider a bilevel optimization problem and connect it to a principal-agent framework, where the principal specifies the broader goals and constraints of the system at the upper level and the agent solves a Markov Decision Process (MDP) at the lower level. The upper-level deals with learning a suitable reward parametrization corresponding to the broader goals and the lower-level deals with learning the policy for the agent. 
    
[^30]: 通过Stein方法对高斯随机场进行逼近及其在广义随机神经网络中的应用

    Gaussian random field approximation via Stein's method with applications to wide random neural networks. (arXiv:2306.16308v1 [math.PR])

    [http://arxiv.org/abs/2306.16308](http://arxiv.org/abs/2306.16308)

    本研究利用Stein方法推导出Wasserstein距离的上界，通过高斯平滑技术将平滑度量转化为Wasserstein距离。通过特殊化结果，我们获得了广义随机神经网络中对高斯随机场逼近的首个上界。

    

    我们利用Stein方法推导出了基于Wasserstein距离（$W_1$）的上界，该距离是连续随机场与高斯分布之间的距离。我们开发了一种新颖的高斯平滑技术，使我们能够将平滑度量中的上界转化为$W_1$距离。平滑性是基于使用Laplacian算子的幂构建的协方差函数，设计成与Cameron-Martin或Reproducing Kernel Hilbert Space相关联的高斯过程具有易操作的特征。这个特征使我们能够超越之前文献中考虑的一维区间型指标集。通过特化我们的一般结果，我们获得了在任意深度和Lipschitz激活函数的广义随机神经网络中对高斯随机场逼近的首个上界。我们的上界明确地用网络宽度和随机权重的矩来表示。

    We derive upper bounds on the Wasserstein distance ($W_1$), with respect to $\sup$-norm, between any continuous $\mathbb{R}^d$ valued random field indexed by the $n$-sphere and the Gaussian, based on Stein's method. We develop a novel Gaussian smoothing technique that allows us to transfer a bound in a smoother metric to the $W_1$ distance. The smoothing is based on covariance functions constructed using powers of Laplacian operators, designed so that the associated Gaussian process has a tractable Cameron-Martin or Reproducing Kernel Hilbert Space. This feature enables us to move beyond one dimensional interval-based index sets that were previously considered in the literature. Specializing our general result, we obtain the first bounds on the Gaussian random field approximation of wide random neural networks of any depth and Lipschitz activation functions at the random field level. Our bounds are explicitly expressed in terms of the widths of the network and moments of the random wei
    
[^31]: 自适应采集数据的离线强化学习策略评估

    Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data. (arXiv:2306.14063v1 [cs.LG])

    [http://arxiv.org/abs/2306.14063](http://arxiv.org/abs/2306.14063)

    本论文提出了一种自适应采集数据的离线强化学习策略评估方法，为表格MDPs推导出高概率、实例相关的误差边界，并实现了自适应设置下的极小值最优离线学习。

    

    发展离线RL方法样本复杂度的理论保证是实现数据需求量较大的RL算法实际可行的重要步骤。目前，大多数结果依赖于关于数据分布的不现实的假设，即包括一个由单一记录策略收集的i.i.d.轨迹集。我们考虑一个更一般的设置，即数据集可以是自适应收集的。我们为表格MDPs中的TMIS离线策略评估（OPE）估计器在这个广义设置中开发理论，推导其估计误差的高概率、实例相关边界。我们还回收了自适应设置下的极小值最优离线学习。最后，我们进行模拟，以经验分析这些估计器在自适应和非自适应模式下的行为。

    Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Currently, most results hinge on unrealistic assumptions about the data distribution -- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We consider a more general setting where the dataset may have been gathered adaptively. We develop theory for the TMIS Offline Policy Evaluation (OPE) estimator in this generalized setting for tabular MDPs, deriving high-probability, instance-dependent bounds on its estimation error. We also recover minimax-optimal offline learning in the adaptive setting. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive regimes.
    
[^32]: AnyPredict: 表格预测的基础模型

    AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])

    [http://arxiv.org/abs/2305.12081](http://arxiv.org/abs/2305.12081)

    本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。

    

    基础模型是在大规模数据上预先训练的模型，可以在许多下游任务中表现良好。它们在自然语言处理和计算机视觉方面取得了显著的成功。然而，这种模型在表格预测任务中的使用受到限制，主要问题包括 (1) 缺乏大规模和多样化的带有标准标签的表格数据集，以及 (2) 不同领域之间的模式不匹配和预测目标的异质性。本文提出了一种方法，用于构建基于 AnyPredict 的表格预测基础模型的大规模训练数据，包括领域内和广泛的领域外数据集。该方法使用数据引擎，利用大型语言模型 (LLM) 来整合表格样本，克服了不同模式表格之间的障碍，并使用“学习，注释和审计”流程将领域外数据与目标任务对齐。扩展的训练数据使预训练的 AnyPredict 能够支持每个表格领域。

    Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
    
[^33]: 使用R2效用的多目标优化

    Multi-Objective Optimization Using the R2 Utility. (arXiv:2305.11774v1 [math.OC])

    [http://arxiv.org/abs/2305.11774](http://arxiv.org/abs/2305.11774)

    本文提出将多目标优化问题转化为一组单目标问题进行解决，并介绍了R2效用函数作为适当的目标函数。该效用函数单调且次模，可以使用贪心优化算法计算全局最优解。

    

    多目标优化的目标是确定描述多目标之间最佳权衡的点集合。为了解决这个矢量值优化问题，从业者常常使用标量化函数将多目标问题转化为一组单目标问题。这组标量化问题可以使用传统的单目标优化技术来解决。在这项工作中，我们将这个约定形式化为一个通用的数学框架。我们展示了这种策略如何有效地将原始的多目标优化问题重新转化为定义在集合上的单目标优化问题。针对这个新问题的适当类别的目标函数是R2效用函数，它被定义为标量化优化问题的加权积分。我们证明了这个效用函数是单调的和次模的集合函数，可以通过贪心优化算法有效地计算出全局最优解。

    The goal of multi-objective optimization is to identify a collection of points which describe the best possible trade-offs between the multiple objectives. In order to solve this vector-valued optimization problem, practitioners often appeal to the use of scalarization functions in order to transform the multi-objective problem into a collection of single-objective problems. This set of scalarized problems can then be solved using traditional single-objective optimization techniques. In this work, we formalise this convention into a general mathematical framework. We show how this strategy effectively recasts the original multi-objective optimization problem into a single-objective optimization problem defined over sets. An appropriate class of objective functions for this new problem is the R2 utility function, which is defined as a weighted integral over the scalarized optimization problems. We show that this utility function is a monotone and submodular set function, which can be op
    
[^34]: 闭环Koopman算子逼近法

    Closed-Loop Koopman Operator Approximation. (arXiv:2303.15318v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2303.15318](http://arxiv.org/abs/2303.15318)

    本文提出了一种闭环Koopman算子逼近法，通过利用Koopman算子的线性性质和对控制器和闭环系统结构的了解，可以同时识别闭环和装置系统。

    

    本文提出了一种方法，用于在已知控制器的情况下，识别反馈控制系统的Koopman模型。Koopman算子通过将非线性系统视为无限维线性系统，可以通过无限数量的映射函数进行重写。通过选择有限数量的映射函数并在映射空间中解决回归问题，可以识别出Koopman算子的有限维近似。现有方法旨在识别开环系统。然而，对于某些系统（如不稳定系统），以开环方式运行实验是不切实际或不可能的。所提出的方法利用Koopman算子的线性性质，结合控制器和闭环系统的结构，同时识别闭环和装置系统。通过实验证明了所提出的闭环Koopman算子逼近法的优势。

    This paper proposes a method to identify a Koopman model of a feedback-controlled system given a known controller. The Koopman operator allows a nonlinear system to be rewritten as an infinite-dimensional linear system by viewing it in terms of an infinite set of lifting functions. A finite-dimensional approximation of the Koopman operator can be identified from data by choosing a finite subset of lifting functions and solving a regression problem in the lifted space. Existing methods are designed to identify open-loop systems. However, it is impractical or impossible to run experiments on some systems, such as unstable systems, in an open-loop fashion. The proposed method leverages the linearity of the Koopman operator, along with knowledge of the controller and the structure of the closed-loop system, to simultaneously identify the closed-loop and plant systems. The advantages of the proposed closed-loop Koopman operator approximation method are demonstrated experimentally using a ro
    

