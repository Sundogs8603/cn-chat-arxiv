# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Decoding Speculative Decoding](https://rss.arxiv.org/abs/2402.01528) | 推测解码是一种用于加速大型语言模型推断的技术，但我们的实验表明，选择的草稿模型生成的令牌被目标模型接受的概率越高，吞吐量越低。我们通过大量实验，分析了各种因素对推测解码效果的影响，并提出了一个分析模型来提高效率。 |
| [^2] | [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/abs/2403.14608) | 大型模型参数高效微调（PEFT）是通过调整预训练模型的参数，以适应特定任务，并减少引入的附加参数或计算资源数量的实用解决方案。 |
| [^3] | [Collision Avoidance Verification of Multiagent Systems with Learned Policies](https://arxiv.org/abs/2403.03314) | 该论文提出了一种基于向后可达性的方法，用于验证多智能体神经反馈环路的碰撞避免属性，通过解决一系列混合整数线性规划计算相对反投影集，并且该逐对方法可并行化，随着智能体数量的增加能够很好地扩展 |
| [^4] | [Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning](https://arxiv.org/abs/2403.00843) | 利用大型语言模型的规划能力来增强长期推荐，使模型在个性化推荐中更有效地理解和应用任务解决原则 |
| [^5] | [Rethinking The Uniformity Metric in Self-Supervised Learning](https://arxiv.org/abs/2403.00642) | 通过识别并满足现有均匀性度量未能达标的五个基本性质，本文引入了一个对维度崩溃敏感的新均匀性度量。 |
| [^6] | [Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?](https://arxiv.org/abs/2402.12819) | 专门模型通常只需少量标记样本（100-1000个）就能与通用模型持平甚至更好，取决于任务的复杂性和结果的变化。 |
| [^7] | [Learning to Defer in Content Moderation: The Human-AI Interplay](https://arxiv.org/abs/2402.12237) | 本文提出了一个模型，捕捉内容审核中人工智能的相互作用。 |
| [^8] | [Fairness Auditing with Multi-Agent Collaboration](https://arxiv.org/abs/2402.08522) | 本文研究了多代理协作下的公平性审计，证明了有时协调对审计准确性可能有害，而非协调的合作通常会产生良好的结果。 |
| [^9] | [Forecasting Events in Soccer Matches Through Language](https://arxiv.org/abs/2402.06820) | 本文提出了一种使用语言模型预测足球比赛中下一个事件的方法，该方法受到大型语言模型方法的启发。通过深度学习和WyScout数据集，该方法在预测准确性方面明显超过了以往的方法。该方法的应用包括博彩和比赛分析，并提供了一个模拟骨架用于构建分析流水线。 |
| [^10] | [Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models](https://arxiv.org/abs/2402.06815) | 本文介绍了将大型事件模型（LEMs）应用于足球分析领域的创新方法。通过学习“足球语言”，LEMs可以预测后续事件的变量，从而模拟比赛并预测球员在不同团队背景下的表现。研究通过对2017-2018英超赛季使用WyScout数据集进行LEMs的精细调整，发现了LEMs在足球分析中的有效性和局限性，同时突出了该模型在预测球队排名和探索高级场景方面的潜力。 |
| [^11] | [Batched Low-Rank Adaptation of Foundation Models](https://arxiv.org/abs/2312.05677) | 提出了Fast LoRA（FLoRA）框架，使得基础模型的低秩调整可以高效批处理异构请求，并在绩效上保持竞争性。 |
| [^12] | [Raising the ClaSS of Streaming Time Series Segmentation](https://arxiv.org/abs/2310.20431) | ClaSS是一种新颖、高效且高精度的流式时间序列分割算法，通过自监督时间序列分类评估同质性，并应用统计测试检测显著的变化点。 |
| [^13] | [Subtractive Mixture Models via Squaring: Representation and Learning](https://arxiv.org/abs/2310.00724) | 通过平方操作实现的消减混合模型在表达能力上优于传统加法混合模型，并在真实世界分布估计任务中得到了实验证明。 |
| [^14] | [Deep Classifier Mimicry without Data Access](https://arxiv.org/abs/2306.02090) | 提出了一种无需访问原始数据的模型-无关知识蒸馏过程CAKE，可以模拟深度分类器，通过生成噪声合成样本对比地扩散到模型的决策边界。 |
| [^15] | [Inverse analysis of granular flows using differentiable graph neural network simulator.](http://arxiv.org/abs/2401.13695) | 通过使用可微分图神经网络模拟器进行反演分析，解决了传统模拟器计算开销大、不可微等问题，提高了计算效率和准确性。 |
| [^16] | [Benchmarking the Fairness of Image Upsampling Methods.](http://arxiv.org/abs/2401.13555) | 这项工作提出了一个评估有条件生成模型性能和公平性的框架，并针对图像上采样应用创建了一个涵盖现代方法的基准测试。实证研究发现使用无偏训练集对结果至关重要，并揭示了不同算法对该问题的响应变化。 |
| [^17] | [Learning to Visually Connect Actions and their Effects.](http://arxiv.org/abs/2401.10805) | 该论文提出了视觉连接动作和其效果的概念（CATE），用于视频理解。研究表明，不同的任务形式产生了捕捉直观动作特性的表示，但模型表现不佳，人类的表现明显优于它们。该研究为未来的努力奠定了基础，并希望能激发出高级形式和模型的灵感。 |
| [^18] | [ODIN: A Single Model for 2D and 3D Perception.](http://arxiv.org/abs/2401.02416) | ODIN是一个模型，可以同时对2D RGB图像和3D点云进行分割和标记，使用变压器架构进行2D和3D视图间的信息融合。 |
| [^19] | [A First Look at Information Highlighting in Stack Overflow Answers.](http://arxiv.org/abs/2401.01472) | 本论文进行了首次大规模的探索性研究，研究了Stack Overflow回答中的信息高亮。通过使用神经网络架构，开发了自动推荐突出内容的方法。 |
| [^20] | [Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks.](http://arxiv.org/abs/2401.01373) | 我们引入了一种张量卷积神经网络（T-CNN）来提高制造业中的缺陷检测任务，通过减少模型参数空间，我们实现了比等效CNN模型更快的训练速度和性能。与传统的人类视觉检查相比，在质量指标方面，T-CNN在参数数量上只有15倍少，训练时间快4%至19%。这项研究在实际制造应用中取得了显著的成果。 |
| [^21] | [Conditional Variational Diffusion Models.](http://arxiv.org/abs/2312.02246) | 该论文提出了一种新的条件变分扩散模型，通过学习调度作为训练过程的一部分，解决了扩散模型的敏感性问题，并且能够适应不同的应用场景，提供高质量的解决方案。 |
| [^22] | [Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data.](http://arxiv.org/abs/2311.07550) | 这项研究全面分析了使用DNNs对表格数据进行后门攻击，揭示了基于转换器的DNNs对表格数据非常容易受到后门攻击，甚至只需最小的特征值修改。该攻击还可以推广到其他模型。 |
| [^23] | [BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling.](http://arxiv.org/abs/2310.10879) | 本论文提出了一种名为BLoad的训练方案，通过最小化填充量并实现高效的分布式数据并行训练，来提高训练效率和召回率。 |
| [^24] | [ZeroSwap: Data-driven Optimal Market Making in DeFi.](http://arxiv.org/abs/2310.09413) | ZeroSwap 是第一个基于数据驱动算法的 DeFi 市场做市方案，在保持市场做市商零利润的情况下，通过适应交易者行为来解决了流动性提供者遭受套利损失的问题。 |
| [^25] | [Ensemble Distillation for Unsupervised Constituency Parsing.](http://arxiv.org/abs/2310.01717) | 本论文提出了一种集成蒸馏的方法来提高无监督句法解析的性能，并且通过蒸馏将集成知识转移到一个学生模型中，解决了常见的多教师蒸馏方法中的过度平滑问题。 |
| [^26] | [Energy Preservation and Stability of Random Filterbanks.](http://arxiv.org/abs/2309.05855) | 本文从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性，发现具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，并推导了其期望帧边界的理论界限。 |
| [^27] | [Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package.](http://arxiv.org/abs/2308.09375) | 本文概述了高光谱解混的先进和常规方法，并比较了这些方法在不同解混场景下的性能。另外，还介绍了一个开源的解混软件包HySUPP。 |
| [^28] | [A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting.](http://arxiv.org/abs/2307.15830) | 本文通过距离相关性的方法来研究循环神经网络对于时间序列预测的有效性，发现激活层能够学习时间序列的滞后结构，但是在连续的几层中逐渐丧失这些信息，导致预测质量变差，同时激活层也不能很好地建模移动平均和异方差时间。 |
| [^29] | [Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach.](http://arxiv.org/abs/2307.09691) | 该论文提出了一个基于DRL的双时间尺度方案，旨在通过联合优化服务缓存、协作卸载和计算通信资源分配来提高MEC系统中的长期服务质量并降低缓存切换成本。 |
| [^30] | [Outlier-Robust Tensor Low-Rank Representation for Data Clustering.](http://arxiv.org/abs/2307.09055) | 本文提出了一种异常鲁棒张量低秩表示方法，用于同时检测异常值和进行数据聚类。该方法基于张量奇异值分解（t-SVD）代数框架，并在较弱条件下具有恢复干净数据的行空间和检测异常值的可证明性能保证。此外，还提出了扩展方法以处理数据部分缺失的情况。 |
| [^31] | [Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models.](http://arxiv.org/abs/2306.05272) | 本论文提出了一种新的图像聚类流程，利用大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类，并通过优化速率降低目标和 CLIP 的图像-文本绑定，成功地提高了聚类的准确性和自标记算法的效果。 |
| [^32] | [Retrieving Texts based on Abstract Descriptions.](http://arxiv.org/abs/2305.12517) | 本研究针对语义检索问题，提出了一种基于摘要描述的文本检索模型，通过改进当前的文本嵌入方法，在标准最近邻搜索中取得了显著性能提升。 |
| [^33] | [IMAP: Intrinsically Motivated Adversarial Policy.](http://arxiv.org/abs/2305.02605) | IMAP是一个内在驱动的对抗策略，无需受害者策略的任何知识，能够高效地进行黑盒规避攻击，并且在单一和多智能体环境中优于目前最先进的方法。 |
| [^34] | [Bake off redux: a review and experimental evaluation of recent time series classification algorithms.](http://arxiv.org/abs/2304.13029) | 本文重访烘焙大赛，评估了最近时间序列分类算法在112个数据集上的表现。该论文通过分类法将这些算法分为五类，为TSC领域的发展提供了贡献。 |
| [^35] | [A machine-learning approach to thunderstorm forecasting through post-processing of simulation data.](http://arxiv.org/abs/2303.08736) | 介绍了一种机器学习方法SALAMA用于预测雷暴发生情况，可以在长达11小时的时间内进行预测，预测技能优于传统方案，且预测时间尺度随着预报的空间尺度呈线性增加。 |
| [^36] | [Adversarial Estimation of Riesz Representers.](http://arxiv.org/abs/2101.00009) | 我们提出了一个敌对框架，使用通用函数空间来估计Riesz Representer，并且证明了非渐近均方速率以及渐近正态性的条件。这个条件使得在机器学习中进行推断时无需样本分割，并且能够提高有限样本性能。 |

# 详细

[^1]: 解码推测解码

    Decoding Speculative Decoding

    [https://rss.arxiv.org/abs/2402.01528](https://rss.arxiv.org/abs/2402.01528)

    推测解码是一种用于加速大型语言模型推断的技术，但我们的实验表明，选择的草稿模型生成的令牌被目标模型接受的概率越高，吞吐量越低。我们通过大量实验，分析了各种因素对推测解码效果的影响，并提出了一个分析模型来提高效率。

    

    推测解码是一种常用的技术，用于加速大型语言模型（LLM）的推断，而不修改其结果。在对LLM进行推断时，推测解码使用较小的草稿模型生成推测令牌，然后使用目标LLM验证这些草稿令牌。推测解码提供的加速取决于草稿模型的选择。普遍建议选择一个草稿模型，该模型生成的令牌被LLM接受的概率很高，以实现最高吞吐量。然而，我们的实验结果与之相反，随着生成的令牌被目标模型接受的概率增加，吞吐量减少。为了理解这一现象，我们进行了大量实验，对影响推测解码的不同因素进行了表征，并研究了这些因素如何相互作用和影响加速效果。基于我们的实验结果，我们描述了一个分析模型，可以使用该模型来进行决策，提高推测解码的效率。

    Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
    
[^2]: 大型模型的参数高效微调：一项全面调研

    Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey

    [https://arxiv.org/abs/2403.14608](https://arxiv.org/abs/2403.14608)

    大型模型参数高效微调（PEFT）是通过调整预训练模型的参数，以适应特定任务，并减少引入的附加参数或计算资源数量的实用解决方案。

    

    大型模型在多个应用领域代表了一项突破性的进展，使得在各种任务中取得了显著成就。然而，它们空前的规模带来了巨大的计算成本。这些模型通常由数十亿个参数组成，需要大量的计算资源来执行。特别是，在为特定下游任务定制大型模型时，尤其是在受到计算能力限制的硬件平台上，规模庞大和计算要求巨大构成了重大挑战。参数高效微调（PEFT）提供了一个实用解决方案，可以有效地调整大型模型以适应各种下游任务。具体而言，PEFT是指调整预训练大型模型的参数，使其适应特定任务的过程，同时尽量减少引入的附加参数或所需的计算资源数量。

    arXiv:2403.14608v1 Announce Type: new  Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approac
    
[^3]: 基于学习策略的多智能体系统碰撞避免验证

    Collision Avoidance Verification of Multiagent Systems with Learned Policies

    [https://arxiv.org/abs/2403.03314](https://arxiv.org/abs/2403.03314)

    该论文提出了一种基于向后可达性的方法，用于验证多智能体神经反馈环路的碰撞避免属性，通过解决一系列混合整数线性规划计算相对反投影集，并且该逐对方法可并行化，随着智能体数量的增加能够很好地扩展

    

    对于许多多智能体控制问题，神经网络（NN）已经实现了有前途的新能力。然而，许多这些系统缺乏正式的保证（例如，碰撞避免、鲁棒性），这阻止了在安全关键环境中利用这些进展。鉴于现有技术不能处理多于一个智能体的情况，为了填补这一研究空白，本论文提出了一种用于验证多智能体神经反馈环路（MA-NFLs）的碰撞避免属性的基于向后可达性的方法。通过给定每个智能体的动力学模型和训练控制策略，所提出的算法通过离线为每对智能体解决一系列混合整数线性规划（MILPs）来计算相对反投影集。我们的逐对方法可并行化，从而可以很好地随着智能体数量的增加而扩展，并且我们考虑了状态。

    arXiv:2403.03314v1 Announce Type: cross  Abstract: For many multiagent control problems, neural networks (NNs) have enabled promising new capabilities. However, many of these systems lack formal guarantees (e.g., collision avoidance, robustness), which prevents leveraging these advances in safety-critical settings. While there is recent work on formal verification of NN-controlled systems, most existing techniques cannot handle scenarios with more than one agent. To address this research gap, this paper presents a backward reachability-based approach for verifying the collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs). Given the dynamics models and trained control policies of each agent, the proposed algorithm computes relative backprojection sets by solving a series of Mixed Integer Linear Programs (MILPs) offline for each pair of agents. Our pair-wise approach is parallelizable and thus scales well with increasing number of agents, and we account for state 
    
[^4]: 利用双层可学习大型语言模型规划增强长期推荐

    Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning

    [https://arxiv.org/abs/2403.00843](https://arxiv.org/abs/2403.00843)

    利用大型语言模型的规划能力来增强长期推荐，使模型在个性化推荐中更有效地理解和应用任务解决原则

    

    传统推荐系统倾向于过分迎合用户的即时兴趣而忽视他们的长期参与。 为了解决这个问题，在推荐决策过程中合并规划能力是至关重要的，以开发能够同时考虑即时兴趣和长期参与的策略。本文提出利用大型语言模型（LLMs）对稀疏数据的显著规划能力用于长期推荐。关键在于使语言模型能够在个性化推荐场景中有效理解和应用任务解决原则，因为模型的预训练可能并未自然包含这些内容。

    arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
    
[^5]: 重新审视自监督学习中的均匀性度量

    Rethinking The Uniformity Metric in Self-Supervised Learning

    [https://arxiv.org/abs/2403.00642](https://arxiv.org/abs/2403.00642)

    通过识别并满足现有均匀性度量未能达标的五个基本性质，本文引入了一个对维度崩溃敏感的新均匀性度量。

    

    均匀性在评估学习表示方面起着至关重要的作用，有助于更深入理解自监督学习。之前的一项开创性工作引入了一个均匀性度量，定量衡量学习表示的崩溃程度。直接优化这一度量与对齐一起，被证明能够有效地防止不断崩溃。然而，我们提出理论和实证证据表明这一度量缺乏对维度崩溃的敏感性，凸显了其局限性。为了解决这一局限性并设计一个更有效的均匀性度量，本文确定了五个基本性质，其中现有的均匀性度量未能满足其中的一些。我们随后引入了一个新的均匀性度量，满足所有这些期望，并且对维度崩溃具有敏感性。

    arXiv:2403.00642v1 Announce Type: cross  Abstract: Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various 
    
[^6]: 微调、提示、上下文学习和指导微调：我们需要多少标记样本？

    Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?

    [https://arxiv.org/abs/2402.12819](https://arxiv.org/abs/2402.12819)

    专门模型通常只需少量标记样本（100-1000个）就能与通用模型持平甚至更好，取决于任务的复杂性和结果的变化。

    

    当解决具有有限标记数据的任务时，研究人员可以选择使用通用的大型语言模型而不进行进一步更新，或者使用少量示例来调整专门的较小模型。 当有足够的标记可用时，专门的模型在许多自然语言处理任务上表现优于通用模型。 在这项工作中，我们旨在调查专门模型需要多少标记样本才能实现这种出色的性能，同时考虑结果的变化。观察提示、上下文学习、微调和指导微调的行为，识别它们在增加不同复杂性任务的标记训练样本数量时的收支平衡点，我们发现专门模型通常只需少量样本（100-1000个）就能与通用模型持平甚至更好。 同时，所需的标记数据量强烈依赖于任务的复杂性和结果的变化。

    arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
    
[^7]: 学习在内容审核中推迟：人工智能与人类协同作用

    Learning to Defer in Content Moderation: The Human-AI Interplay

    [https://arxiv.org/abs/2402.12237](https://arxiv.org/abs/2402.12237)

    本文提出了一个模型，捕捉内容审核中人工智能的相互作用。

    

    成功的在线平台内容审核依赖于人工智能协同方法。本文介绍了一个模型，捕捉内容审核中人工智能的相互作用。算法观察到即将发布的帖子的背景信息，做出分类和准入决策，并安排帖子进行人工审核。

    arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
    
[^8]: 多代理协作下的公平性审计

    Fairness Auditing with Multi-Agent Collaboration

    [https://arxiv.org/abs/2402.08522](https://arxiv.org/abs/2402.08522)

    本文研究了多代理协作下的公平性审计，证明了有时协调对审计准确性可能有害，而非协调的合作通常会产生良好的结果。

    

    现有的公平性审计工作假设代理人独立操作。本文考虑多个代理人对同一平台进行不同任务的审计情况。代理人有两个杠杆：协作策略（是否进行协调）和抽样方法。我们在代理人独立操作或协作时对它们的相互作用进行了理论研究。我们证明了有时协调对审计准确性可能有害，而非协调的合作通常会产生良好的结果。对实际数据集的实验验证了这一观察结果，非协调的合作的审计准确性与协作的最优抽样相匹配。

    Existing work in fairness audits assumes that agents operate independently. In this paper, we consider the case of multiple agents auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method. We theoretically study their interplay when agents operate independently or collaborate. We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results. Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling.
    
[^9]: 通过语言预测足球比赛事件

    Forecasting Events in Soccer Matches Through Language

    [https://arxiv.org/abs/2402.06820](https://arxiv.org/abs/2402.06820)

    本文提出了一种使用语言模型预测足球比赛中下一个事件的方法，该方法受到大型语言模型方法的启发。通过深度学习和WyScout数据集，该方法在预测准确性方面明显超过了以往的方法。该方法的应用包括博彩和比赛分析，并提供了一个模拟骨架用于构建分析流水线。

    

    本文介绍了一种预测足球比赛中下一个事件的方法，这是一个与大型语言模型（LLMs）面临的问题非常相似的挑战。与其他严重限制足球事件动态的方法不同，这些方法往往从很多变量中抽象出来或依赖于混合顺序模型，我们的研究提出了一种受到LLMs方法学启发的新技术。这些模型预测了组成一个事件的完整变量链，大大简化了构建足球大事件模型（LEMs）的过程。利用公开可用的WyScout数据集进行深度学习，所提出的方法在关键领域（如下一个事件类型的预测准确性）显著超越了以往LEM提案的性能。本文突显了LEM在多种应用中的实用性，包括博彩和比赛分析。此外，我们还展示了LEM提供了一个模拟骨架，可以构建许多分析流水线。

    This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including betting and match analytics. Moreover, we show that LEMs provide a simulation backbone on which many analytics pipelines can be bu
    
[^10]: 使用精细调整的大型事件模型估计不同背景下的球员表现

    Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models

    [https://arxiv.org/abs/2402.06815](https://arxiv.org/abs/2402.06815)

    本文介绍了将大型事件模型（LEMs）应用于足球分析领域的创新方法。通过学习“足球语言”，LEMs可以预测后续事件的变量，从而模拟比赛并预测球员在不同团队背景下的表现。研究通过对2017-2018英超赛季使用WyScout数据集进行LEMs的精细调整，发现了LEMs在足球分析中的有效性和局限性，同时突出了该模型在预测球队排名和探索高级场景方面的潜力。

    

    本文引入了大型事件模型（LEMs）在足球分析领域的创新应用，类似于大型语言模型。通过学习足球的“语言” - 预测后续事件的变量而不是单词，LEMs可以模拟比赛并提供各种应用，包括预测不同团队背景下的球员表现。我们专注于使用WyScout数据集对2017-2018英超赛季进行LEMs的精细调整，以获取关于球员贡献和团队战略的具体见解。我们的方法包括调整这些模型以反映足球的微妙动态，从而评估假设的转会。我们的研究结果验证了LEMs在足球分析中的效果和局限性，突显了该模型预测球队预期排名并探索高级场景（例如将Cristiano Ronaldo或Lionel Messi转会至不同球队的潜在影响）的能力。

    This paper introduces an innovative application of Large Event Models (LEMs), akin to Large Language Models, to the domain of soccer analytics. By learning the "language" of soccer - predicting variables for subsequent events rather than words LEMs facilitate the simulation of matches and offer various applications, including player performance prediction across different team contexts. We focus on fine-tuning LEMs with the WyScout dataset for the 2017-2018 Premier League season to derive specific insights into player contributions and team strategies. Our methodology involves adapting these models to reflect the nuanced dynamics of soccer, enabling the evaluation of hypothetical transfers. Our findings confirm the effectiveness and limitations of LEMs in soccer analytics, highlighting the model's capability to forecast teams' expected standings and explore high-profile scenarios, such as the potential effects of transferring Cristiano Ronaldo or Lionel Messi to different teams in the 
    
[^11]: 基于批处理的基础模型低秩调整

    Batched Low-Rank Adaptation of Foundation Models

    [https://arxiv.org/abs/2312.05677](https://arxiv.org/abs/2312.05677)

    提出了Fast LoRA（FLoRA）框架，使得基础模型的低秩调整可以高效批处理异构请求，并在绩效上保持竞争性。

    

    最近，低秩适应（LoRA）因通过引入可训练的低秩矩阵微调基础模型并减少可训练参数的数量而引起关注。虽然LoRA提供了许多优点，但其在实时为各种全球用户提供服务时无法高效处理多个特定任务适配器的能力受到限制。这为需要为每个传入请求个性化、特定任务适应的场景中造成了性能瓶颈。为了减轻这一约束，我们提出了快速LoRA（FLoRA）框架，其中批处理中的每个输入示例都可以与其独特的低秩适应权重相关联，从而实现对异构请求的高效批处理。我们通过实证表明，FLoRA保留了LoRA的绩效优点，在跨越8种语言的MultiPL-E代码生成基准测试上展示出竞争结果。

    arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
    
[^12]: 提升流式时间序列分割的等级

    Raising the ClaSS of Streaming Time Series Segmentation

    [https://arxiv.org/abs/2310.20431](https://arxiv.org/abs/2310.20431)

    ClaSS是一种新颖、高效且高精度的流式时间序列分割算法，通过自监督时间序列分类评估同质性，并应用统计测试检测显著的变化点。

    

    今天，普遍存在的传感器发射高频数值测量流，反映了人类、动物、工业、商业和自然过程的特性。这些过程的变化，例如由外部事件或内部状态变化引起的，会表现为记录信号中的变化。流式时间序列分割（STSS）的任务是将流分割为对应于所观察的过程或实体状态的连续可变大小的分段。分割操作本身必须能够应对输入信号的频率。我们引入了ClaSS，一种新颖、高效且高精度的STSS算法。ClaSS使用自监督时间序列分类评估潜在分割的同质性，并应用统计测试来检测显著的变化点（CPs）。在我们的实验证评中使用了两个大型基准和六个真实世界的数据档案。

    arXiv:2310.20431v2 Announce Type: replace-cross  Abstract: Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, 
    
[^13]: 通过平方的消减混合模型:表示和学习

    Subtractive Mixture Models via Squaring: Representation and Learning

    [https://arxiv.org/abs/2310.00724](https://arxiv.org/abs/2310.00724)

    通过平方操作实现的消减混合模型在表达能力上优于传统加法混合模型，并在真实世界分布估计任务中得到了实验证明。

    

    混合模型传统上是通过将几个分布作为组件相加来表示和学习的。允许混合减去概率质量或密度可以大大减少建模复杂分布所需的组件数量。然而，学习这种减法混合模型并确保它们仍然编码非负函数是具有挑战性的。我们探讨了如何通过平方来学习和执行深度减法混合模型。我们在概率电路框架中进行这些研究，这使我们能够表示张量化的混合模型并泛化其他减法模型。我们在理论上证明了允许减法的平方电路类可以比传统的加法混合模型具有指数级更具表达力；我们在一系列真实世界分布估计任务上实证展示了这种增加的表达力。

    arXiv:2310.00724v2 Announce Type: replace-cross  Abstract: Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
    
[^14]: 没有数据访问的深度分类器模拟

    Deep Classifier Mimicry without Data Access

    [https://arxiv.org/abs/2306.02090](https://arxiv.org/abs/2306.02090)

    提出了一种无需访问原始数据的模型-无关知识蒸馏过程CAKE，可以模拟深度分类器，通过生成噪声合成样本对比地扩散到模型的决策边界。

    

    最近，对预先训练模型的访问已经成为许多机器学习领域的标准。不幸的是，可能无法等同地获得模型训练所需的原始数据。这使得微调、压缩模型、持续调整或进行任何其他类型的数据驱动更新变得极具挑战性。我们认为可能无需原始数据访问。具体而言，我们提出了对比推理知识提取（CAKE），这是一种模型无关的知识蒸馏过程，可以模拟深度分类器而无需访问原始数据。为此，CAKE生成一对噪声合成样本，并将它们对比地扩散到模型的决策边界。我们通过几个基准数据集和各种架构选择在实证上证实了CAKE的有效性，为广泛应用铺平了道路。

    arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
    
[^15]: 通过可微分图神经网络模拟器进行颗粒流的反演分析

    Inverse analysis of granular flows using differentiable graph neural network simulator. (arXiv:2401.13695v1 [physics.geo-ph])

    [http://arxiv.org/abs/2401.13695](http://arxiv.org/abs/2401.13695)

    通过使用可微分图神经网络模拟器进行反演分析，解决了传统模拟器计算开销大、不可微等问题，提高了计算效率和准确性。

    

    颗粒流中的反演问题，如山体滑坡和碎屑流，涉及基于目标波动剖面估计材料参数或边界条件。传统的高保真度模拟器对这些反演问题是计算密集型的，限制了可能的模拟次数。此外，它们的不可微性使得梯度优化方法无法应用，而这些方法在高维问题中以其效率而闻名。虽然基于机器学习的代理模型提供了计算效率和可微性，但由于其依赖于低维输入-输出映射，无法捕捉到颗粒流的完整物理过程，因此往往难以推广到训练数据之外。我们提出了一种新颖的可微分图神经网络模拟器(GNS)，通过将图神经网络的反向模式自动微分与基于梯度的优化相结合，用于解决反演问题。GNS学习了颗粒流的动力学特性。

    Inverse problems in granular flows, such as landslides and debris flows, involve estimating material parameters or boundary conditions based on target runout profile. Traditional high-fidelity simulators for these inverse problems are computationally demanding, restricting the number of simulations possible. Additionally, their non-differentiable nature makes gradient-based optimization methods, known for their efficiency in high-dimensional problems, inapplicable. While machine learning-based surrogate models offer computational efficiency and differentiability, they often struggle to generalize beyond their training data due to their reliance on low-dimensional input-output mappings that fail to capture the complete physics of granular flows. We propose a novel differentiable graph neural network simulator (GNS) by combining reverse mode automatic differentiation of graph neural networks with gradient-based optimization for solving inverse problems. GNS learns the dynamics of granula
    
[^16]: 图像上采样方法的公平性基准测试

    Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])

    [http://arxiv.org/abs/2401.13555](http://arxiv.org/abs/2401.13555)

    这项工作提出了一个评估有条件生成模型性能和公平性的框架，并针对图像上采样应用创建了一个涵盖现代方法的基准测试。实证研究发现使用无偏训练集对结果至关重要，并揭示了不同算法对该问题的响应变化。

    

    近年来，深度生成模型在创建合成媒体（如图像和视频）方面取得了快速发展。虽然这些模型在日常任务中的实际应用非常诱人，但评估其公平性相关的潜在风险至关重要。在这项工作中，我们引入了一个全面的框架，用于评估有条件生成模型的性能和公平性。我们开发了一套度量标准——受监督公平性的灵感来源——来评估模型的公平性和多样性。我们针对图像上采样这个特定应用，创建了一个涵盖各种现代上采样方法的基准测试。作为基准测试的一部分，我们引入了UnfairFace，这是FairFace的一个子集，复制了常见大规模人脸数据集的种族分布。我们的实证研究凸显了使用无偏训练集的重要性，并揭示了算法对该问题的响应变化。

    Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to 
    
[^17]: 学习视觉连接动作和其效果

    Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])

    [http://arxiv.org/abs/2401.10805](http://arxiv.org/abs/2401.10805)

    该论文提出了视觉连接动作和其效果的概念（CATE），用于视频理解。研究表明，不同的任务形式产生了捕捉直观动作特性的表示，但模型表现不佳，人类的表现明显优于它们。该研究为未来的努力奠定了基础，并希望能激发出高级形式和模型的灵感。

    

    在这项工作中，我们引入了视觉连接动作和其效果（CATE）的新概念，用于视频理解。CATE可以在任务规划和从示范中学习等领域中应用。我们提出了不同基于CATE的任务形式，如动作选择和动作指定，其中视频理解模型以语义和细粒度的方式连接动作和效果。我们观察到不同的形式产生了捕捉直观动作特性的表示。我们还设计了各种基线模型用于动作选择和动作指定。尽管任务具有直观性，但我们观察到模型困难重重，人类表现明显优于它们。本研究旨在为未来的努力奠定基础，展示了连接视频理解中动作和效果的灵活性和多功能性，希望能激发出高级形式和模型的灵感。

    In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
    
[^18]: ODIN: 一个用于2D和3D感知的单一模型

    ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])

    [http://arxiv.org/abs/2401.02416](http://arxiv.org/abs/2401.02416)

    ODIN是一个模型，可以同时对2D RGB图像和3D点云进行分割和标记，使用变压器架构进行2D和3D视图间的信息融合。

    

    目前的先进模型在像ScanNet这样的当代3D感知基准上使用并标记依赖于数据集提供的3D点云，该点云是通过对感知到的多视角RGB-D图像进行后处理获得的。它们通常在领域内进行训练，放弃了大规模的2D预训练，并且胜过将姿态RGB-D多视角图像进行特征化的替代方案。消耗姿态图像和后处理的3D点云之间的性能差距，加剧了2D和3D感知需要不同模型架构的观点。在本文中，我们挑战这个观点，并提出ODIN（Omni-Dimensional INstance segmentation），一种能够使用变压器架构对2D RGB图像和3D点云进行分割和标记的模型，该模型通过交替的2D视图内和3D视图间信息融合来区分2D和3D特征操作，利用涉及的令牌的位置编码来捕捉2D补丁令牌和3D坐标的像素坐标。

    State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
    
[^19]: Stack Overflow回答中信息高亮的初探

    A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])

    [http://arxiv.org/abs/2401.01472](http://arxiv.org/abs/2401.01472)

    本论文进行了首次大规模的探索性研究，研究了Stack Overflow回答中的信息高亮。通过使用神经网络架构，开发了自动推荐突出内容的方法。

    

    背景：浏览Stack Overflow（SO）的知识仍然具有挑战性。为了使帖子对用户更生动，SO允许用户使用Markdown或HTML编写和编辑帖子，以便用户可以利用各种格式化样式（例如粗体、斜体和代码）来突出重要信息。然而，关于突出信息的研究仍然有限。目标：我们在最近的研究中进行了首次大规模的探索性研究，研究了SO回答中的信息高亮。为了扩展我们之前的研究，我们利用最初设计用于命名实体识别任务的神经网络架构，开发了自动推荐带有格式化样式的突出内容的方法。方法：本文研究了Stack Overflow的31,169,429个回答。为了训练推荐模型，我们选择了CNN和BERT模型，针对每种格式化类型（即粗体、斜体、代码和标题）使用我们从SO回答收集的突出信息数据集。

    Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
    
[^20]: 使用张量卷积神经网络提高制造业中的缺陷检测

    Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])

    [http://arxiv.org/abs/2401.01373](http://arxiv.org/abs/2401.01373)

    我们引入了一种张量卷积神经网络（T-CNN）来提高制造业中的缺陷检测任务，通过减少模型参数空间，我们实现了比等效CNN模型更快的训练速度和性能。与传统的人类视觉检查相比，在质量指标方面，T-CNN在参数数量上只有15倍少，训练时间快4%至19%。这项研究在实际制造应用中取得了显著的成果。

    

    缺陷检测是制造业质量控制阶段中最重要但也最具挑战性的任务之一。在本研究中，我们引入了一种张量卷积神经网络（T-CNN），并在罗伯特·博世制造厂生产的超声波传感器组件的真实缺陷检测应用中考察其性能。我们的量子启发式T-CNN在减少的模型参数空间上运行，极大地提高了等效CNN模型的训练速度和性能，而不会牺牲准确性。具体来说，我们演示了T-CNN可以通过质量指标来衡量与传统CNN相同的性能，但参数数量只有其15倍少，训练时间快4%至19%。我们的结果表明，T-CNN大大超越了传统人类视觉检查的结果，在当前制造应用中具有价值。

    Defect detection is one of the most important yet challenging tasks in the quality control stage in the manufacturing sector. In this work, we introduce a Tensor Convolutional Neural Network (T-CNN) and examine its performance on a real defect detection application in one of the components of the ultrasonic sensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired T-CNN operates on a reduced model parameter space to substantially improve the training speed and performance of an equivalent CNN model without sacrificing accuracy. More specifically, we demonstrate how T-CNNs are able to reach the same performance as classical CNNs as measured by quality metrics, with up to fifteen times fewer parameters and 4% to 19% faster training times. Our results demonstrate that the T-CNN greatly outperforms the results of traditional human visual inspection, providing value in a current real application in manufacturing.
    
[^21]: 条件变分扩散模型

    Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.02246](http://arxiv.org/abs/2312.02246)

    该论文提出了一种新的条件变分扩散模型，通过学习调度作为训练过程的一部分，解决了扩散模型的敏感性问题，并且能够适应不同的应用场景，提供高质量的解决方案。

    

    逆问题旨在从观测中确定参数，这是工程和科学中的一个关键任务。最近，生成模型，特别是扩散模型，因其能够产生逼真的解决方案和良好的数学特性而在这一领域中越来越受欢迎。尽管取得了成功，但扩散模型的一个重要缺点是对方差调度的选择敏感，该调度控制着扩散过程的动态。为特定应用程序微调这个调度是至关重要的，但时间成本高昂，并且不能保证最优结果。我们提出了一种新颖的方法，将学习调度作为训练过程的一部分。我们的方法支持对数据的概率条件，提供高质量的解决方案，并且具有灵活性，能够在最小的开销下适应不同的应用。这种方法在两个不相关的逆问题中进行了测试：超分辨率显微镜和定量相位成像，结果表明比较或更好。

    Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior 
    
[^22]: Tabdoor：基于转换器的表格数据神经网络存在后门漏洞

    Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data. (arXiv:2311.07550v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2311.07550](http://arxiv.org/abs/2311.07550)

    这项研究全面分析了使用DNNs对表格数据进行后门攻击，揭示了基于转换器的DNNs对表格数据非常容易受到后门攻击，甚至只需最小的特征值修改。该攻击还可以推广到其他模型。

    

    深度神经网络(DNNs)在各个领域都显示出巨大的潜力。与这些发展同时，与DNN训练相关的漏洞，如后门攻击，是一个重大关切。这些攻击涉及在模型训练过程中微妙地插入触发器，从而允许操纵预测。最近，由于转换器模型的崛起，DNNs用于表格数据越来越受关注。我们的研究对使用DNNs对表格数据进行后门攻击进行了全面分析，特别关注转换器。鉴于表格数据的固有复杂性，我们探究了嵌入后门的挑战。通过对基准数据集进行系统实验，我们发现基于转换器的DNNs对表格数据非常容易受到后门攻击，即使只有最小的特征值修改。我们还验证了我们的攻击可以推广到其他模型，如XGBoost和DeepFM。我们的研究结果几乎表明后门攻击可以完美实现。

    Deep Neural Networks (DNNs) have shown great promise in various domains. Alongside these developments, vulnerabilities associated with DNN training, such as backdoor attacks, are a significant concern. These attacks involve the subtle insertion of triggers during model training, allowing for manipulated predictions.More recently, DNNs for tabular data have gained increasing attention due to the rise of transformer models.  Our research presents a comprehensive analysis of backdoor attacks on tabular data using DNNs, particularly focusing on transformers. Given the inherent complexities of tabular data, we explore the challenges of embedding backdoors. Through systematic experimentation across benchmark datasets, we uncover that transformer-based DNNs for tabular data are highly susceptible to backdoor attacks, even with minimal feature value alterations. We also verify that our attack can be generalized to other models, like XGBoost and DeepFM. Our results indicate nearly perfect attac
    
[^23]: BLoad：增强神经网络训练的高效顺序数据处理方法

    BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling. (arXiv:2310.10879v1 [cs.LG])

    [http://arxiv.org/abs/2310.10879](http://arxiv.org/abs/2310.10879)

    本论文提出了一种名为BLoad的训练方案，通过最小化填充量并实现高效的分布式数据并行训练，来提高训练效率和召回率。

    

    随着现代深度神经网络模型的复杂性不断增加和数据集的扩大，需要开发优化且可扩展的训练方法。本白皮书中，我们解决了使用不同大小的序列进行神经网络模型训练的高效性挑战。为了解决这个问题，我们提出了一种新的训练方案，能够在序列的分布式数据并行训练中实现高效处理，同时还能最小化额外开销。通过使用这个方案，我们能够将填充量减少超过100倍，同时不删除任何帧，从而在实验证明了总体上增加了训练时间和召回率。

    The increasing complexity of modern deep neural network models and the expanding sizes of datasets necessitate the development of optimized and scalable training methods. In this white paper, we addressed the challenge of efficiently training neural network models using sequences of varying sizes. To address this challenge, we propose a novel training scheme that enables efficient distributed data-parallel training on sequences of different sizes with minimal overhead. By using this scheme we were able to reduce the padding amount by more than 100$x$ while not deleting a single frame, resulting in an overall increased performance on both training time and Recall in our experiments.
    
[^24]: ZeroSwap: 基于数据驱动的 DeFi 中的最优市场做市

    ZeroSwap: Data-driven Optimal Market Making in DeFi. (arXiv:2310.09413v1 [cs.LG])

    [http://arxiv.org/abs/2310.09413](http://arxiv.org/abs/2310.09413)

    ZeroSwap 是第一个基于数据驱动算法的 DeFi 市场做市方案，在保持市场做市商零利润的情况下，通过适应交易者行为来解决了流动性提供者遭受套利损失的问题。

    

    自动做市商 (AMMs) 是去中心化金融中匹配流动性供给和需求的主要中心。它们的功能主要依赖于流动性提供者 (LPs) 将其资产投资于流动性池。然而，池中资产交易的价格通常比集中化和更流动的交易所价格延迟更多。这导致流动性提供者遭受套利损失。我们通过采用 Glosten 和 Milgrom 的经典市场微观结构模型，将市场价格适应于交易者行为，从而解决了这个问题。在本文中，我们提出了第一个最优贝叶斯和第一个无模型数据驱动算法来最优地跟踪资产的外部价格。我们使用的最优性概念在市场做市商的价格上强制执行了零利润条件，因此取名为 ZeroSwap。这确保了市场做市商在损失知情交易者的同时从噪声交易者那里获得利润。

    Automated Market Makers (AMMs) are major centers of matching liquidity supply and demand in Decentralized Finance. Their functioning relies primarily on the presence of liquidity providers (LPs) incentivized to invest their assets into a liquidity pool. However, the prices at which a pooled asset is traded is often more stale than the prices on centralized and more liquid exchanges. This leads to the LPs suffering losses to arbitrage. This problem is addressed by adapting market prices to trader behavior, captured via the classical market microstructure model of Glosten and Milgrom. In this paper, we propose the first optimal Bayesian and the first model-free data-driven algorithm to optimally track the external price of the asset. The notion of optimality that we use enforces a zero-profit condition on the prices of the market maker, hence the name ZeroSwap. This ensures that the market maker balances losses to informed traders with profits from noise traders. The key property of our 
    
[^25]: 无监督句法分析的集成蒸馏

    Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])

    [http://arxiv.org/abs/2310.01717](http://arxiv.org/abs/2310.01717)

    本论文提出了一种集成蒸馏的方法来提高无监督句法解析的性能，并且通过蒸馏将集成知识转移到一个学生模型中，解决了常见的多教师蒸馏方法中的过度平滑问题。

    

    我们研究了无监督句法分析任务，该任务将句子的词和短语组织成一个层次结构，而不使用语言学注释的数据。我们观察到现有的无监督解析器捕捉到了解析结构的不同方面，可以利用这些来提高无监督分析的性能。为此，我们提出了“树平均”的概念，基于此我们进一步提出了一种新的无监督解析的集成方法。为了提高推理效率，我们进一步将集成知识蒸馏到一个学生模型中；这种集成-蒸馏的过程是缓解常见的多教师蒸馏方法中存在的过度平滑问题的有效方法。实验证明我们的方法超过了所有先前的方法，始终表现出其在不同集成组件和领域转移条件下的有效性和稳健性。

    We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
    
[^26]: 随机滤波器组的能量保持和稳定性

    Energy Preservation and Stability of Random Filterbanks. (arXiv:2309.05855v1 [cs.LG])

    [http://arxiv.org/abs/2309.05855](http://arxiv.org/abs/2309.05855)

    本文从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性，发现具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，并推导了其期望帧边界的理论界限。

    

    波形为基础的深度学习为什么如此困难？尽管有多次尝试训练卷积神经网络(convnets)进行滤波器设计，但它们往往无法超越手工创建的基线。这更令人惊讶，因为这些基线是线性时不变系统：因此，它们的传递函数可以通过具有大感受野的卷积神经网络准确表示。在本文中，我们从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性。我们发现，具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，这在音频信号处理应用中是典型的。此外，我们观察到随机滤波器组的期望能量保持对于数值稳定性是不足够的，并推导了其期望帧边界的理论界限。

    What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
    
[^27]: 图像处理和机器学习在高光谱解混方面的应用：概述和HySUPP Python包

    Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package. (arXiv:2308.09375v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2308.09375](http://arxiv.org/abs/2308.09375)

    本文概述了高光谱解混的先进和常规方法，并比较了这些方法在不同解混场景下的性能。另外，还介绍了一个开源的解混软件包HySUPP。

    

    由于高光谱传感器的低空间分辨率、双重散射和场景中材料的混合现象，光谱像素往往是材料的纯光谱成分的混合物，被称为端元。解混即估计像素点中各端元的比例。根据端元的先验知识，线性解混可分为监督、半监督和无监督（盲解混）三大类。图像处理和机器学习的进展对解混产生了重大影响。本文概述了先进和常规的解混方法。此外，我们对这三个类别中先进和常规技术进行了重要对比。我们比较了解混技术在三个模拟和两个真实数据集上的性能。实验结果揭示了对不同解混场景来说，不同解混类别的优势。此外，我们还提供了一个开源的软件包——HySUPP。

    Spectral pixels are often a mixture of the pure spectra of the materials, called endmembers, due to the low spatial resolution of hyperspectral sensors, double scattering, and intimate mixtures of materials in the scenes. Unmixing estimates the fractional abundances of the endmembers within the pixel. Depending on the prior knowledge of endmembers, linear unmixing can be divided into three main groups: supervised, semi-supervised, and unsupervised (blind) linear unmixing. Advances in Image processing and machine learning substantially affected unmixing. This paper provides an overview of advanced and conventional unmixing approaches. Additionally, we draw a critical comparison between advanced and conventional techniques from the three categories. We compare the performance of the unmixing techniques on three simulated and two real datasets. The experimental results reveal the advantages of different unmixing categories for different unmixing scenarios. Moreover, we provide an open-sou
    
[^28]: 基于距离相关性的方法来刻画循环神经网络在时间序列预测中的有效性

    A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting. (arXiv:2307.15830v1 [cs.LG])

    [http://arxiv.org/abs/2307.15830](http://arxiv.org/abs/2307.15830)

    本文通过距离相关性的方法来研究循环神经网络对于时间序列预测的有效性，发现激活层能够学习时间序列的滞后结构，但是在连续的几层中逐渐丧失这些信息，导致预测质量变差，同时激活层也不能很好地建模移动平均和异方差时间。

    

    时间序列预测受到了广泛关注，循环神经网络(RNNs)作为处理序列数据的一种常用模型之一，具有很强的能力。然而，之前关于RNNs在时间序列预测中的研究结果不一致，并且对于不同数据集的性能差异缺乏深入洞察。本文提出了一种通过距离相关性这一多功能指标来将时间序列的特征与RNNs的组成部分联系起来的方法。该指标允许我们通过RNN激活层的信息流来解释和说明其性能。我们实证表明，RNN的激活层能够很好地学习时间序列的滞后结构。然而，在连续的几层中，它们逐渐丧失了这些信息，从而使具有大滞后结构的序列的预测质量变差。我们还显示，激活层不能充分建模移动平均和异方差时间。

    Time series forecasting has received a lot of attention with recurrent neural networks (RNNs) being one of the widely used models due to their ability to handle sequential data. Prior studies of RNNs for time series forecasting yield inconsistent results with limited insights as to why the performance varies for different datasets. In this paper, we provide an approach to link the characteristics of time series with the components of RNNs via the versatile metric of distance correlation. This metric allows us to examine the information flow through the RNN activation layers to be able to interpret and explain their performance. We empirically show that the RNN activation layers learn the lag structures of time series well. However, they gradually lose this information over a span of a few consecutive layers, thereby worsening the forecast quality for series with large lag structures. We also show that the activation layers cannot adequately model moving average and heteroskedastic time
    
[^29]: 在协作式MEC系统中的联合服务缓存、通信和计算资源分配：基于DRL的双时间尺度方法

    Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach. (arXiv:2307.09691v1 [cs.NI])

    [http://arxiv.org/abs/2307.09691](http://arxiv.org/abs/2307.09691)

    该论文提出了一个基于DRL的双时间尺度方案，旨在通过联合优化服务缓存、协作卸载和计算通信资源分配来提高MEC系统中的长期服务质量并降低缓存切换成本。

    

    由于多维资源的限制，满足终端的严格服务质量要求对多接入边缘计算（MEC）系统构成了重要挑战。为了应对这个挑战，我们提出了一个协作式MEC框架，促进边缘服务器之间的资源共享，并通过联合优化服务缓存、协作卸载、计算和通信资源分配来最大化长期的服务质量和降低缓存切换成本。服务缓存和其他资源分配之间的双时间尺度特性和时间回归关系使解决问题变得更加困难。为了解决这个问题，我们提出了一种基于深度强化学习（DRL）的双时间尺度方案，称为DGL-DDPG，它由短期遗传算法（GA）和基于长短期记忆网络的深度确定性策略梯度（LSTM-DDPG）组成。

    Meeting the strict Quality of Service (QoS) requirements of terminals has imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems, due to the limited multidimensional resources. To address this challenge, we propose a collaborative MEC framework that facilitates resource sharing between the edge servers, and with the aim to maximize the long-term QoS and reduce the cache switching cost through joint optimization of service caching, collaborative offfoading, and computation and communication resource allocation. The dual timescale feature and temporal recurrence relationship between service caching and other resource allocation make solving the problem even more challenging. To solve it, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate the optimizatio
    
[^30]: 异常鲁棒张量低秩表示用于数据聚类

    Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])

    [http://arxiv.org/abs/2307.09055](http://arxiv.org/abs/2307.09055)

    本文提出了一种异常鲁棒张量低秩表示方法，用于同时检测异常值和进行数据聚类。该方法基于张量奇异值分解（t-SVD）代数框架，并在较弱条件下具有恢复干净数据的行空间和检测异常值的可证明性能保证。此外，还提出了扩展方法以处理数据部分缺失的情况。

    

    低秩张量分析在许多实际应用中受到广泛关注。然而，张量数据经常受到异常值或样本特定的污染。如何恢复被异常值损坏的张量数据并进行数据聚类仍然是一个具有挑战性的问题。本文基于张量奇异值分解（t-SVD）代数框架，提出了一种用于同时检测异常值和张量数据聚类的异常鲁棒张量低秩表示（OR-TLRR）方法。该方法受到最近提出的满足一定条件的可逆线性变换引起的张量张量积的启发。对于带有任意异常值污染的张量观测，OR-TLRR在较弱条件下能够确切恢复干净数据的行空间并检测异常值。此外，还提出了OR-TLRR的扩展来处理数据部分缺失的情况。

    Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experim
    
[^31]: 利用预训练模型的速率降低原则进行图像聚类

    Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models. (arXiv:2306.05272v1 [cs.CV])

    [http://arxiv.org/abs/2306.05272](http://arxiv.org/abs/2306.05272)

    本论文提出了一种新的图像聚类流程，利用大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类，并通过优化速率降低目标和 CLIP 的图像-文本绑定，成功地提高了聚类的准确性和自标记算法的效果。

    

    大型预训练模型的出现已经在视觉表示学习和自然语言处理方面带来了范式转变，但是聚类未标记的图像作为一种基本和经典的机器学习问题，仍然缺乏有效的解决方案，特别是对于大规模数据集。在本文中，我们提出了一种新的图像聚类流程，利用 CLIP 等大型预训练模型的强大特征表示，在规模上有效地对图像进行聚类。我们展示了预训练特征通过进一步优化速率降低目标，更具有结构性。由此产生的特征可以显著提高聚类的准确性，例如从 ImageNet-1k 的 57％提高到 66％。此外，通过利用 CLIP 的图像-文本绑定，我们展示了新的聚类方法如何导致简单而有效的自标记算法，成功地应用于未标记的大型数据集，例如 MS-COCO 和 LAION-Aesthetics。

    The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. We show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore, by leveraging CLIP's image-text binding, we show how the new clustering method leads to a simple yet effective self-labeling algorithm that successfully works on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We wi
    
[^32]: 基于摘要描述的文本检索

    Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12517](http://arxiv.org/abs/2305.12517)

    本研究针对语义检索问题，提出了一种基于摘要描述的文本检索模型，通过改进当前的文本嵌入方法，在标准最近邻搜索中取得了显著性能提升。

    

    虽然针对文本的信息提取，指令优化的大型语言模型表现优异，但对于在大规模文档集合中定位符合给定描述的文本（语义检索）并不适用。基于嵌入向量的相似度搜索可以通过查询执行检索，但嵌入中的相似度定义不明确且不一致，并且对于许多用例来说都是次优的。那么，什么是有效检索的好的查询表示？我们确定了根据内容的摘要描述检索句子的明确定义且一致的任务。我们展示了当前文本嵌入的不足，并提出了一种替代模型，在标准最近邻搜索中的表现显著提升。该模型使用通过提示LLM获得的正负样本对进行训练。虽然很容易从LLM中获得训练材料，但LLM无法直接执行检索任务。

    While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
    
[^33]: IMAP: 内在驱动的对抗策略

    IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])

    [http://arxiv.org/abs/2305.02605](http://arxiv.org/abs/2305.02605)

    IMAP是一个内在驱动的对抗策略，无需受害者策略的任何知识，能够高效地进行黑盒规避攻击，并且在单一和多智能体环境中优于目前最先进的方法。

    

    强化学习（RL）代理在部署过程中容易受到规避攻击的影响。在单智能体环境中，攻击者可以对策略或值网络的输入或输出注入无法察觉的扰动；在多智能体环境中，攻击者可以通过控制对手间接影响受害者的观察。 对抗性策略为解决此类攻击提供了一种有前途的解决方案。然而，目前的方法要么需要受害者政策的完美或部分知识，要么由于任务相关奖励的稀疏性而导致样本效率低下。为克服这些局限性，我们提出了内在驱动的对抗政策（IMAP），用于单智能体和多智能体环境中高效的黑盒规避攻击，而不需任何关于受害者策略的知识。 IMAP利用基于状态覆盖率，策略覆盖率，风险和政策分歧的四个内在目标，以鼓励探索并发现更强的攻击技能。我们还描述了一种处理多个具有不同实力的对手的可推广算法。我们的实验表明，IMAP在单智能体和多智能体环境中均优于最先进的方法，包括两个Atari游戏，一个机器人运动任务和一个多智能体游戏。

    Reinforcement learning (RL) agents are known to be vulnerable to evasion attacks during deployment. In single-agent environments, attackers can inject imperceptible perturbations on the policy or value network's inputs or outputs; in multi-agent environments, attackers can control an adversarial opponent to indirectly influence the victim's observation. Adversarial policies offer a promising solution to craft such attacks. Still, current approaches either require perfect or partial knowledge of the victim policy or suffer from sample inefficiency due to the sparsity of task-related rewards. To overcome these limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy. IMAP uses four intrinsic objectives based on state coverage, policy coverage, risk, and policy divergence to encourage exploration and discover stronger attacking skills. We also des
    
[^34]: Bake Off重访：对最近时间序列分类算法的评述和实验评估

    Bake off redux: a review and experimental evaluation of recent time series classification algorithms. (arXiv:2304.13029v1 [cs.LG])

    [http://arxiv.org/abs/2304.13029](http://arxiv.org/abs/2304.13029)

    本文重访烘焙大赛，评估了最近时间序列分类算法在112个数据集上的表现。该论文通过分类法将这些算法分为五类，为TSC领域的发展提供了贡献。

    

    一篇研究论文在2017年比较了18个时间序列分类（TSC）算法在来自加州大学河滨分校（UCR）存档的85个数据集上的表现。这项研究通常被称为“烘焙比赛”，发现只有9个算法的表现显著优于使用的动态时间规整（DTW）和旋转森林基准。该研究通过从时间序列数据中提取特征的算法类型对每个算法进行分类，形成了五种主要算法类型的分类法。与可以重现结果的代码和结果的提供相结合，这些算法的分类和可访问的结果推动了TSC领域的普及。六年过去了，UCR存档已扩展到112个数据集，并且已经提出了大量新的算法。我们重访烘焙大赛，看看每个提出的类别自原始出版以来的进展，并评估新算法的性能。

    In 2017, a research paper compared 18 Time Series Classification (TSC) algorithms on 85 datasets from the University of California, Riverside (UCR) archive. This study, commonly referred to as a `bake off', identified that only nine algorithms performed significantly better than the Dynamic Time Warping (DTW) and Rotation Forest benchmarks that were used. The study categorised each algorithm by the type of feature they extract from time series data, forming a taxonomy of five main algorithm types. This categorisation of algorithms alongside the provision of code and accessible results for reproducibility has helped fuel an increase in popularity of the TSC field. Over six years have passed since this bake off, the UCR archive has expanded to 112 datasets and there have been a large number of new algorithms proposed. We revisit the bake off, seeing how each of the proposed categories have advanced since the original publication, and evaluate the performance of newer algorithms against t
    
[^35]: 一种机器学习方法用于通过后处理模拟数据预测雷暴天气(arXiv:2303.08736v1 [physics.ao-ph])

    A machine-learning approach to thunderstorm forecasting through post-processing of simulation data. (arXiv:2303.08736v1 [physics.ao-ph])

    [http://arxiv.org/abs/2303.08736](http://arxiv.org/abs/2303.08736)

    介绍了一种机器学习方法SALAMA用于预测雷暴发生情况，可以在长达11小时的时间内进行预测，预测技能优于传统方案，且预测时间尺度随着预报的空间尺度呈线性增加。

    

    雷暴对社会和经济构成重大威胁，需要可靠的雷暴预测。本文介绍了SALAMA，一种前馈神经网络模型，用于识别数值天气预报（NWP）数据中的雷暴发生情况。该模型在中欧对流层解析集合预报和雷电观测数据上进行训练。仅给出从NWP数据中提取的与雷暴发展相关的像素输入参数集，SALAMA以可靠的校准方式推断雷暴发生的概率。对于长达11小时的前置时间，我们发现其预测技能优于仅基于对流有效位能的分类。通过改变将闪电观测与NWP数据相关联的时空标准，我们展示了熟练的雷暴预测时间尺度随着预报的空间尺度的线性增加。

    Thunderstorms pose a major hazard to society and economy, which calls for reliable thunderstorm forecasts. In this work, we introduce SALAMA, a feedforward neural network model for identifying thunderstorm occurrence in numerical weather prediction (NWP) data. The model is trained on convection-resolving ensemble forecasts over Central Europe and lightning observations. Given only a set of pixel-wise input parameters that are extracted from NWP data and related to thunderstorm development, SALAMA infers the probability of thunderstorm occurrence in a reliably calibrated manner. For lead times up to eleven hours, we find a forecast skill superior to classification based only on convective available potential energy. Varying the spatiotemporal criteria by which we associate lightning observations with NWP data, we show that the time scale for skillful thunderstorm predictions increases linearly with the spatial scale of the forecast.
    
[^36]: 对Riesz Representer的敌对估计

    Adversarial Estimation of Riesz Representers. (arXiv:2101.00009v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2101.00009](http://arxiv.org/abs/2101.00009)

    我们提出了一个敌对框架，使用通用函数空间来估计Riesz Representer，并且证明了非渐近均方速率以及渐近正态性的条件。这个条件使得在机器学习中进行推断时无需样本分割，并且能够提高有限样本性能。

    

    许多因果和结构参数是基于底层回归的线性泛函。Riesz Representer是半参数线性泛函渐近方差的关键组成部分。我们提出了一个敌对框架，使用通用函数空间来估计Riesz Representer。我们证明了一个非渐近均方速率，其中涉及一个称为临界半径的抽象量，然后将其专门应用于神经网络、随机森林和再生核希尔伯特空间作为主要案例。此外，我们使用临界半径理论来证明了渐近正态性，而不需要样本分割，揭示了一种“复杂度-速率鲁棒性”条件。这个条件具有实际后果：在几个机器学习设置中，可以实现无需样本分割的推断，这可能会提高有限样本性能。我们的估计器在高度非线性的模拟中实现了名义覆盖率。

    Many causal and structural parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Furthermore, we use critical radius theory -- in place of Donsker theory -- to prove asymptotic normality without sample splitting, uncovering a ``complexity-rate robustness'' condition. This condition has practical consequences: inference without sample splitting is possible in several machine learning settings, which may improve finite sample performance compared to sample splitting. Our estimators achieve nominal coverage in highly nonlinear simulations where previo
    

