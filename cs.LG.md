# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Coercing LLMs to do and reveal (almost) anything](https://arxiv.org/abs/2402.14020) | 本研究发现对大型语言模型的对抗性攻击不仅仅局限于“越狱”，而包括迫使模型展示各种意外行为，攻击表面和目标广泛。这些攻击源于LLMs的预训练和常见词汇中存在的“故障”标记。 |
| [^2] | [D-Flow: Differentiating through Flows for Controlled Generation](https://arxiv.org/abs/2402.14017) | D-Flow框架通过控制生成过程中的流形，优化源点，解决了扩散和流匹配模型中生成结果的控制问题，同时在多个领域取得了最先进的性能表现。 |
| [^3] | [Corrective Machine Unlearning](https://arxiv.org/abs/2402.14015) | 该论文通过形式化“修正机器消除”来解决受未知操纵影响的数据对训练模型的影响问题，可能仅知道一部分受影响样本。发现纠正消除问题与传统以隐私为导向的消除方法有显著不同的要求。 |
| [^4] | [Misalignment, Learning, and Ranking: Harnessing Users Limited Attention](https://arxiv.org/abs/2402.14013) | 通过利用用户有限的注意力，我们提出了一种模型来解决推荐系统面临的用户选择冲动与长期回报不一致的挑战，设计在线赌博算法以逐渐减少后悔。 |
| [^5] | [Chasing Convex Functions with Long-term Constraints](https://arxiv.org/abs/2402.14012) | 引入并研究了一类带有长期约束的在线度量问题，提出了在可持续能源和计算系统中在线资源分配应用中的最优竞争算法和学习增强算法，并通过数值实验表现良好。 |
| [^6] | [Geometry-Informed Neural Networks](https://arxiv.org/abs/2402.14009) | GINNs提出了一种新颖的几何信息神经网络范式，可以在几何任务中生成多样的解决方案，无需训练数据，采用显式多样性损失以及可微损失来减轻模态坍缩，并在实验中展示了其在各种复杂性场景中的高效性。 |
| [^7] | [Asymptotics of Learning with Deep Structured (Random) Features](https://arxiv.org/abs/2402.13999) | 在高维情况下，我们提供了学习输出层测试误差的严格渐近特性，并对使用高斯彩虹神经网络进行学习的问题做出了重要贡献 |
| [^8] | [FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning](https://arxiv.org/abs/2402.13989) | 提出了一种不精确和自适应的FedADMM算法，通过为客户端的本地更新设计一个不精确性标准，消除了调整本地训练准确度的需要，降低了计算成本并减轻了滞后效应。 |
| [^9] | [A Simple and Yet Fairly Effective Defense for Graph Neural Networks](https://arxiv.org/abs/2402.13987) | 本文介绍了一种名为NoisyGNNs的新颖防御方法，通过将噪声引入基础模型的架构，建立了噪声注入与增强GNN鲁棒性之间的理论连接，实验证明其在节点分类任务上取得了优越或可比的防御性能。 |
| [^10] | [Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators](https://arxiv.org/abs/2402.13984) | 提出了稳定性感知Boltzmann估计器（StABlE）训练方法，结合传统监督训练和参考系统可观察量，用于生成稳定且准确的神经网络原子间势。 |
| [^11] | [The Importance of Architecture Choice in Deep Learning for Climate Applications](https://arxiv.org/abs/2402.13979) | 本文研究了深度学习在气候科学应用中架构选择的重要性，并展示了神经网络可以在多样化的气候情景下可预测大西洋经向翻转环流（AMOC），进一步揭示MLP和深度集成可以学习AMOC的物理过程而非模拟其进展。 |
| [^12] | [Linear-Time Graph Neural Networks for Scalable Recommendations](https://arxiv.org/abs/2402.13973) | 本文提出了一种线性时间图神经网络（LTGNN），用于扩展图神经网络在推荐系统中的可扩展性。 |
| [^13] | [Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges](https://arxiv.org/abs/2402.13957) | 本研究提出了一种结合人工智能和机器学习的音频指纹识别算法，通过在具有多样背景噪音和失真的真实环境场景模拟中的工作，以提高准确性，实现100%准确性的5秒音频输入，系统匹配速度可预测，强调了实际实施中的关键空间-速度权衡。 |
| [^14] | [AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning](https://arxiv.org/abs/2402.13946) | 这项工作提出了AttackGNN，针对硬件安全中使用的基于GNN的技术进行了首次红队攻击，通过设计新颖的强化学习代理生成对抗性示例电路。 |
| [^15] | [Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning](https://arxiv.org/abs/2402.13945) | 本文探讨了使用概率神经网络（PNNs）来建模Aleatoric不确定性，通过开发概率距离度量来优化PNN架构，证实了PNNs在模拟Aleatoric不确定性中的有效性。 |
| [^16] | [Verifying message-passing neural networks via topology-based bounds tightening](https://arxiv.org/abs/2402.13937) | 通过基于拓扑的界限紧缩，我们提出了一种强大的证书生成方法，用于验证消息传递神经网络，支持添加和删除边、全局和局部预算的设置，以及拓扑扰动和特征修改，展示了其在优化约束动态调整方面的有效性。 |
| [^17] | [Do Efficient Transformers Really Save Computation?](https://arxiv.org/abs/2402.13934) | 本研究旨在理解高效Transformer（例如稀疏Transformer和线性Transformer）的能力和限制，发现它们适合解决一般DP任务，但不同于标准Transformer。 |
| [^18] | [Enhancing Reinforcement Learning Agents with Local Guides](https://arxiv.org/abs/2402.13930) | 本文提出了一种添加本地指南策略到强化学习代理中的新算法，通过噪声策略切换程序和近似策略评估来引导本地指南，提高了在强化学习算法的性能，尤其在学习初期。 |
| [^19] | [SDXL-Lightning: Progressive Adversarial Diffusion Distillation](https://arxiv.org/abs/2402.13929) | 提出了一种结合渐进和对抗性蒸馏的扩散蒸馏方法，在文本到图像生成任务中取得了新的最先进结果，并开源了相应模型。 |
| [^20] | [BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2402.13918) | 本文通过对遥感图像中的云进行分割，旨在提高卫星图像分析的精度和效率，应用于环境监测、资源管理和灾害响应。 |
| [^21] | [Bias correction of wind power forecasts with SCADA data and continuous learning](https://arxiv.org/abs/2402.13916) | 本研究利用机器学习方法对风力发电进行预测，通过校正数值天气预报模型提取的48小时预测数据，其中卷积神经网络取得了最佳的预测效果，将平均NRMSE降低至22%。 |
| [^22] | [Explain to Question not to Justify](https://arxiv.org/abs/2402.13914) | XAI领域被划分为蓝色XAI和红色XAI两种解释文化，指出了红色XAI领域的重要性和研究潜力，并提出了未来的研究挑战。 |
| [^23] | [Replication Study: Enhancing Hydrological Modeling with Physics-Guided Machine Learning](https://arxiv.org/abs/2402.13911) | 本研究引入了一种名为物理信息引导的机器学习（PIML）模型，将概念性水文模型的过程理解与机器学习算法的预测效率结合，在水文建模中取得了优异的性能表现。 |
| [^24] | [Dealing with unbounded gradients in stochastic saddle-point optimization](https://arxiv.org/abs/2402.13903) | 提出一种简单而有效的正则化技术，稳定了随机鞍点优化过程中的梯度不断增长的问题，能够在无界梯度和噪声的情况下提供有意义的性能保证 |
| [^25] | [Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate](https://arxiv.org/abs/2402.13901) | 本文提出了离散时间扩散模型的新方法，改进了对更大类的分布的收敛保证，并提高了具有有界支撑的分布的收敛速率。 |
| [^26] | [Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning](https://arxiv.org/abs/2402.13897) | 提出了一个两块式的方法来解决长文档中信息检索领域的挑战，并实现了双向交互 |
| [^27] | [Overcoming Saturation in Density Ratio Estimation by Iterated Regularization](https://arxiv.org/abs/2402.13891) | 引入迭代正则化方法解决了密度比估计中的饱和问题，实现了快速收敛，在密度比估计基准测试和大规模深度无监督领域自适应模型的重要性加权集成中表现优异。 |
| [^28] | [An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach](https://arxiv.org/abs/2402.13871) | 优化的Transformer模型DistilBERT用于检测钓鱼邮件，通过预处理技术解决了类别不平衡问题 |
| [^29] | [Generative Probabilistic Time Series Forecasting and Applications in Grid Operations](https://arxiv.org/abs/2402.13870) | 提出了一种弱创新自动编码器结构和学习算法，用于从非参数稳态时间序列中提取独立同分布的创新序列，适用于生成式概率时间序列预测。 |
| [^30] | [RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency interference -- Application to pulsar observations](https://arxiv.org/abs/2402.13867) | 提出一种以联合检测和恢复形式处理射电频率干扰（RFI）的方法，利用深度卷积网络来识别和恢复受到RFI影响的动态谱部分 |
| [^31] | [Improving Efficiency of Iso-Surface Extraction on Implicit Neural Representations Using Uncertainty Propagation](https://arxiv.org/abs/2402.13861) | 通过重新审视算术规则并分析网络输出的概率分布，本文提出了一种改进的范围分析技术，以提高隐式神经表示中等值面提取的效率。 |
| [^32] | [Replicable Learning of Large-Margin Halfspaces](https://arxiv.org/abs/2402.13857) | 该论文提出了解决学习大间距半空间问题的可复制算法，相比之前的算法，在维度无关、时间复杂度优化、样本复杂度方面等多个关键参数上均有显著改进。 |
| [^33] | [Neural Control System for Continuous Glucose Monitoring and Maintenance](https://arxiv.org/abs/2402.13852) | 引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，实时动态调整胰岛素输送，增强葡萄糖优化，最大化效率并确保个性化护理。 |
| [^34] | [Large Language Models are Advanced Anonymizers](https://arxiv.org/abs/2402.13846) | 大型语言模型在保护个人数据方面取得了重要进展，提出了一种基于对抗性LLM推断的匿名化框架。 |
| [^35] | [MLXP: A framework for conducting replicable Machine Learning eXperiments in Python](https://arxiv.org/abs/2402.13831) | MLXP是一个简单轻量的基于Python的实验管理工具，旨在解决机器学习研究中复制性的挑战 |
| [^36] | [Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes](https://arxiv.org/abs/2402.13821) | 本文提出了对于满足Lipschitz连续性条件的Conf-MDP的性能改进界限，并推导了一个新颖的性能改进下界 |
| [^37] | [FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning](https://arxiv.org/abs/2402.13820) | 介绍了一种自监督的、结构化的表示和生成方法，通过傅立叶潜动力学提高了运动学习算法的插值和泛化能力。 |
| [^38] | [Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers](https://arxiv.org/abs/2402.13812) | 该研究展示了一种强大有效的机器学习模型，利用声音生物标志物来预测住院心力衰竭患者的死亡率。 |
| [^39] | [The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank](https://arxiv.org/abs/2402.13810) | 预条件Langevin动力学的预期损失与目标函数的Hessian秩成正比，并且在神经网络中具有应用前景。 |
| [^40] | [Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing](https://arxiv.org/abs/2402.13791) | 本研究通过系统性评估揭示了遥感领域中可解释人工智能的使用趋势，探讨了新型方法和面临的挑战，为解决特定遥感难题提供了新思路。 |
| [^41] | [Preserving Near-Optimal Gradient Sparsification Cost for Scalable Distributed Deep Learning](https://arxiv.org/abs/2402.13781) | 提出了一种名为ExDyna的新型梯度稀疏化方案，通过细粒度块和分区组合来减少通信开销。 |
| [^42] | [Contextual Molecule Representation Learning from Chemical Reaction Knowledge](https://arxiv.org/abs/2402.13779) | REMO是一个自监督学习框架，通过利用常见化学中明确定义的原子组合规则，在1.7百万个已知化学反应上预训练图形/Transformer编码器，并提出了Masked Reaction Centre Reconstruction (MRCR)和Reaction Centre Identification (RCI)两个预训练目标，为分子表示学习提供了新颖解决方案。 |
| [^43] | [Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions](https://arxiv.org/abs/2402.13777) | 深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。 |
| [^44] | [Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion](https://arxiv.org/abs/2402.13776) | 本文提出了一个两阶段级联扩散模型Cas-DiffCom，用于解决婴儿纵向3D医学图像补全中存在的个体时间维度一致性不足的问题。 |
| [^45] | [Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex](https://arxiv.org/abs/2402.13765) | 提出一种使用Concrete分布作为概率单纯形上的概率模型的保持精度的校准方法，并证明其在交叉熵损失上训练的DNN模型具有最优性，同时提出了一种有效的样本生成方法。 |
| [^46] | [Reinforcement learning-assisted quantum architecture search for variational quantum algorithms](https://arxiv.org/abs/2402.13754) | 通过强化学习自动搜索变分电路的最佳结构，改善了VQAs的性能。 |
| [^47] | [AI-Powered Predictions for Electricity Load in Prosumer Communities](https://arxiv.org/abs/2402.13752) | 本文审查了如何利用人工智能技术对生产者兼消费者社区的电力负载进行预测的方法和可行性 |
| [^48] | [Reasoning Algorithmically in Graph Neural Networks](https://arxiv.org/abs/2402.13744) | 神经算法推理(NAR)是一个有希望的研究领域，旨在将算法的结构化和基于规则的推理与神经网络的自适应学习能力相结合。 |
| [^49] | [Average gradient outer product as a mechanism for deep neural collapse](https://arxiv.org/abs/2402.13728) | 本文通过提供证据表明，深度神经网络中的神经坍塌主要是通过平均梯度外积进行深度特征学习的，权重的奇异结构与AGOP高度相关，导致类内变异坍塌。 |
| [^50] | [Sparse and Structured Hopfield Networks](https://arxiv.org/abs/2402.13725) | 通过与Fenchel-Young损失建立链接，我们提出了一种新的稀疏霍普菲尔德网络框架，能够实现端到端可微分的稀疏转换，揭示了损失边界、稀疏性和精确存储之间的关联，同时通过SparseMAP转换将框架扩展到结构化霍普菲尔德网络。 |
| [^51] | [The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning](https://arxiv.org/abs/2402.13723) | 通过研究批处理大小对预训练的影响，本研究表明较大的批次大小有助于更好的预训练模型，但存在稳定性下限和有效性上限，模型质量取决于训练过程中看到的语音数据量。 |
| [^52] | [An Evaluation of Large Language Models in Bioinformatics Research](https://arxiv.org/abs/2402.13714) | 大型语言模型在生物信息学研究中展现出了潜力，可成功处理多项关键任务，但在复杂任务中仍存在局限性。 |
| [^53] | [DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning](https://arxiv.org/abs/2402.13711) | DSLR提出了一种基于覆盖范围的多样性方法，以解决基于重播的图持续学习中回放节点过于集中导致过拟合和灾难性遗忘的问题。 |
| [^54] | [On the Conflict of Robustness and Learning in Collaborative Machine Learning](https://arxiv.org/abs/2402.13700) | 在协作机器学习中，研究人员正式规范了稳健聚合器的领域，并发现现有的稳健聚合器无法实现其目标，要么无法准确识别有针对性的恶意更新，要么方法成功率不够。 |
| [^55] | [Explainable Classification Techniques for Quantum Dot Device Measurements](https://arxiv.org/abs/2402.13699) | 提出了一种基于合成数据的的可解释特征技术，利用可解释性提升机（EBMs）实现了在量子点调谐中较高的可解释性和准确性。 |
| [^56] | [Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks](https://arxiv.org/abs/2402.13673) | 该研究使用一维卷积神经网络模型对过境系外行星进行参数表征，能够有效降低分析过境系外行星所需的时间和计算成本。 |
| [^57] | [Measurement Uncertainty: Relating the uncertainties of physical and virtual measurements](https://arxiv.org/abs/2402.13666) | 概率（机器学习）模型提供了预测不确定性，而在质量管理中物理检查的测量不确定性与概率模型的预测不确定性之间的关系仍未完全阐明 |
| [^58] | [Stable Update of Regression Trees](https://arxiv.org/abs/2402.13655) | 提出了一种回归树的稳定更新方法，通过正则化和调整超参数来平衡预测性能和经验稳定性。 |
| [^59] | [Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark](https://arxiv.org/abs/2402.13654) | 通过引入强化学习与引导，结合比例积分（PI）控制器，本文提出了一种学习基础的控制策略，用于非线性节流阀的控制，实现了一个几乎最优的控制器。 |
| [^60] | [PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models](https://arxiv.org/abs/2402.13653) | 零样本蛋白质问题回答任务，提供专门数据集并建立了PQA框架，包含生物相关的科学PQA基准，通过多模态架构取得了最新技术水平。 |
| [^61] | [Robustness of Deep Neural Networks for Micro-Doppler Radar Classification](https://arxiv.org/abs/2402.13651) | 评估了两种深度卷积架构的鲁棒性，发现在训练中加入对抗样本和时间增强样本可以改善模型的泛化能力 |
| [^62] | [A Large Dimensional Analysis of Multi-task Semi-Supervised Learning](https://arxiv.org/abs/2402.13646) | 本文进行了针对一个简单而非常通用的分类模型的大维分析研究，该模型同时涵盖了多任务和半监督学习，并考虑了不确定的标签，通过随机矩阵理论的工具表征了关键功能的渐近性质，从而揭示了关于有效使用该模型的反直觉指导。 |
| [^63] | [FlexHB: a More Efficient and Flexible Framework for Hyperparameter Optimization](https://arxiv.org/abs/2402.13641) | FlexHB通过细粒度保真度方法提高了搜索最佳配置的效率，重新设计了早停框架，并结合了连续减半的方法。 |
| [^64] | [Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures](https://arxiv.org/abs/2402.13640) | 本研究通过监测三种知名DL框架以及ONNX的运行时基础设施中的能耗和推理时间，使用三种不同的DL模型，初步探究了它们的能源效率。 |
| [^65] | [The METRIC-framework for assessing data quality for trustworthy AI in medicine: a systematic review](https://arxiv.org/abs/2402.13635) | 本文提出了用于评估医疗人工智能数据质量的METRIC框架，着重探讨数据质量在深度学习应用中的重要性，强调数据质量对于医学人工智能产品监管批准的关键作用。 |
| [^66] | [Learning Dual-arm Object Rearrangement for Cartesian Robots](https://arxiv.org/abs/2402.13634) | 提出了一种基于强化学习的在线任务分配决策方法，计算时间随着物体数量的增加仅呈线性增长，以解决笛卡尔机器人双臂物体重新排列问题。 |
| [^67] | [UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language](https://arxiv.org/abs/2402.13630) | UniGraph框架旨在训练一个能够泛化到不同领域的未见图和任务的图基础模型 |
| [^68] | [Improving Building Temperature Forecasting: A Data-driven Approach with System Scenario Clustering](https://arxiv.org/abs/2402.13628) | 本文提出了一种基于系统场景聚类的数据驱动室温预测模型，通过历史数据分析提取系统运行特征，进一步简化系统模型以提高泛化和计算效率，并在真实世界中取得了显著降低建模时间的效果。 |
| [^69] | [Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression](https://arxiv.org/abs/2402.13622) | 重要发现包括高维情况下重抽样方法的问题，仅当$\alpha$足够大时提供一致可靠的误差估计，以及在超参数化区域$\alpha\!<\!1$的情况下它们的预测表现 |
| [^70] | [Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews](https://arxiv.org/abs/2402.13613) | 该论文总结了VLSP 2023中ComOM任务的一个数据挑战，旨在推动自然语言处理领域通过开发从越南产品评论中提取比较意见的技术，参与者需提出能够提取比较"五元组"的模型并根据F1分数进行评估排名。 |
| [^71] | [Data-driven Discovery with Large Generative Models](https://arxiv.org/abs/2402.13610) | 大型生成模型在数据驱动发现中的应用开创了端到端发现系统的新模式，利用提供的数据集搜寻和验证假设，突显了自动化系统的重要性和局限性。 |
| [^72] | [Convergence Acceleration of Markov Chain Monte Carlo-based Gradient Descent by Deep Unfolding](https://arxiv.org/abs/2402.13608) | 提出了一种结合了MCMC和梯度下降的Ohzeki方法的可训练采样求解器，通过最小化损失函数训练步长，采用基于采样的梯度估计替代自动微分，并在数值实验中显示相对于原始方法显著加快了收敛速度 |
| [^73] | [User-LLM: Efficient LLM Contextualization with User Embeddings](https://arxiv.org/abs/2402.13598) | User-LLM框架利用用户嵌入对LLMs进行语境化，使其能够动态适应用户上下文，在各种任务中实现显著性能提升。 |
| [^74] | [A cutting plane algorithm for globally solving low dimensional k-means clustering problems](https://arxiv.org/abs/2402.13595) | 本文提出一种切平面算法，针对低维数据的k-means聚类问题进行全局最优解，利用结构化凹形分配问题和全局优化理论方法，在合理时间内解决大数据集的聚类问题，并展示收敛于零最优性差值的结果。 |
| [^75] | [Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating](https://arxiv.org/abs/2402.13582) | 该论文主要贡献是通过深度神经网络和行为调控方案，提出了一个框架GuanoZero，使AI代理能够掌握《关旦》游戏。 |
| [^76] | [ToDo: Token Downsampling for Efficient Generation of High-Resolution Images](https://arxiv.org/abs/2402.13573) | 提出了一种新的训练-free 方法 ToDo，通过令牌下采样加速 Stable Diffusion 推理，以实现高分辨率图像的高效生成。 |
| [^77] | [On the Expressive Power of a Variant of the Looped Transformer](https://arxiv.org/abs/2402.13572) | 设计了一种新型Transformer块AlgoFormer，相比标准Transformer和Looped Transformer，AlgoFormer在相同参数数量下能够实现更高的算法表达能力 |
| [^78] | [Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms](https://arxiv.org/abs/2402.13567) | 提出了"点检查等价性"，为同行预测机制的效力提供了一个可解释的度量 |
| [^79] | [Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective](https://arxiv.org/abs/2402.13556) | IGAP提出了一种新型的基于图提示的方法，用于将图预训练泛化到归纳场景，弥合了预训练图和微调图之间的差距。 |
| [^80] | [Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions](https://arxiv.org/abs/2402.13551) | 提出了一种新颖且实用的叙事理解范式，通过在叙事中形成图NARCO来描述整个背景的任务无关的连贯依赖，其中的边反映了高层次的连贯关系，无需依赖人类注释。 |
| [^81] | [DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load](https://arxiv.org/abs/2402.13548) | 提出了DiffPLF模型，用于电动汽车充电的概率负载预测，结合了去噪扩散模型和交叉注意力机制，执行条件生成充电需求配置。 |
| [^82] | [ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling](https://arxiv.org/abs/2402.13542) | ARL2提出了一种检索器学习技术，利用LLMs作为标注者，并采用自适应自训练策略，能够有效减少注释成本，并在NQ和MMLU上取得了5.4%和4.6%的准确度提升。 |
| [^83] | [FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing](https://arxiv.org/abs/2402.13533) | 该论文提出了一种基于高性能GPU的方法，利用低秩结构来高效地预训练和微调大型语言模型，解决了线性层冗余性、GPU内存占用和分布式训练中GPU利用率不足的挑战 |
| [^84] | [Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation](https://arxiv.org/abs/2402.13531) | 对于线性回归的标准差分隐私梯度下降，通过改进分析得出了更紧的误差界限和实例特定的不确定性估计。 |
| [^85] | [Best of Many in Both Worlds: Online Resource Allocation with Predictions under Unknown Arrival Model](https://arxiv.org/abs/2402.13530) | 本文提出了一种算法，能够在未知预测质量的情况下，以预测作为输入准确执行在线资源分配，解决了在线决策中预测质量未知的问题。 |
| [^86] | [Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response](https://arxiv.org/abs/2402.13528) | 本文开发了一种基础设施调解员系统，用于自动检测特定基础设施问题，通过挖掘社交网络中关于预期失败的担忧，有助于预防和减轻潜在的基础设施失败。 |
| [^87] | [MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment](https://arxiv.org/abs/2402.13525) | MatchNAS提出了一种新颖的方案，通过同时使用有标签和无标签数据来优化大型网络家族，并自动搜索针对不同硬件平台的定制网络，从而解决了在稀疏标签数据环境中优化边缘人工智能模型的问题。 |
| [^88] | [Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer's Disease Classification](https://arxiv.org/abs/2402.13523) | 通过变化每个维度的比例来调查空间信息对于脑电图阿尔茨海默病分类的重要性，结果表明空间信息比时间信息更相关，与频谱信息的相关性相同 |
| [^89] | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | 本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能 |
| [^90] | [From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers](https://arxiv.org/abs/2402.13512) | 本文研究了从自注意力模型到马尔可夫模型的转变，揭示了生成Transformer动态的机理和相关条件，为一致估计提供了保证，并在IID样本下建立了样本复杂性保证。 |
| [^91] | [SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2402.13505) | SimPro提出了一种高度适应的框架，不依赖于任何关于未标记数据分布的预定义假设，通过创新地改进期望最大化（EM）算法，明确分离条件和边缘类别分布的建模。 |
| [^92] | [HetTree: Heterogeneous Tree Graph Neural Network](https://arxiv.org/abs/2402.13496) | HetTree提出了一种新颖的异构树图神经网络，通过构建语义树数据结构捕捉元路径之间的层次关系，解决了现有方法忽略的异构图中的树形层次结构问题。 |
| [^93] | [Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits](https://arxiv.org/abs/2402.13487) | 该论文研究了针对随机多臂老虎机的隐身对手攻击，指出大部分现有攻击容易被提出的检测方法发现，因此提出了隐身攻击概念并进行了探讨，分析发现在特定条件下几乎总是可以成功发动隐身攻击，为MAB的安全风险带来新见解。 |
| [^94] | [ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding](https://arxiv.org/abs/2402.13485) | 提出ProPD，一种基于动态令牌树修剪和生成的高效LLM并行解码框架，通过先进的提前修剪机制和动态令牌树生成算法来提高验证效率。 |
| [^95] | [Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks](https://arxiv.org/abs/2402.13482) | 提出了一种用于低资源领域任务的新方法，通过结合来自其他数据集的相关示例来增强训练数据，以解决在低资源环境中生成样本不够理想和缺乏多样性的挑战 |
| [^96] | [STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning](https://arxiv.org/abs/2402.13468) | STENCIL利用次模互信息选择弱标记的稀有类实例，并通过标注者强标记，提高了文本分类数据集上的准确率和稀有类F-1分数。 |
| [^97] | [Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459) | 通过设计新的数据注入攻击攻击LLMs，并提出一种梯度引导后门触发器学习方法，通过实验验证表明成功地破坏模型输出，仅改变1%的指导调优样本即可导致性能下降率达到约80％。 |
| [^98] | [Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection](https://arxiv.org/abs/2402.13454) | 通过推导与相关性和覆盖性相关的基于相似度的界限，为子模互信息在目标数据子集选择中的表现提供了理论保证 |
| [^99] | [LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data](https://arxiv.org/abs/2402.13452) | 本研究提出了一个新的基于Twitter数据的框架LocalHealth，用于预测当地精神健康结果。通过与GPT3.5结合使用，该框架在MH监测中取得了显著的改进。 |
| [^100] | [ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance](https://arxiv.org/abs/2402.13448) | 本研究提出了一种在急诊科中减少等待时间的诊断辅助方法，利用人工智能系统帮助医生进行快速准确的诊断，并开发了ED-Copilot系统来推荐实验室检测并进行诊断预测。 |
| [^101] | [PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models](https://arxiv.org/abs/2402.13441) | 提出了一种名为PaCKD的模式聚类知识蒸馏方法，用于压缩内存访问预测模型，通过将内存访问序列聚类并训练模式特定的教师模型，最终训练出一个轻量级的学生模型。 |
| [^102] | [Learning to Retrieve for Job Matching](https://arxiv.org/abs/2402.13435) | 将学习检索技术应用于提升领英的工作搜索和推荐系统，通过构建评估求职者资格的图表并利用学习到的链接进行检索，以提高申请者质量和优化求职者参与度。 |
| [^103] | [DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain](https://arxiv.org/abs/2402.13432) | DrBenchmark提出了一个针对法语生物医学领域的大型语言理解评估基准，旨在弥补对最新法语生物医学模型评估的不足，并考虑到法语的独特敏感性。 |
| [^104] | [LinkSAGE: Optimizing Job Matching Using Graph Neural Networks](https://arxiv.org/abs/2402.13430) | LinkSAGE是一个采用图神经网络的优化工作匹配框架，通过独特的训练和服务方法，实现了在庞大而复杂的领英专业网络中进行个性化工作匹配。 |
| [^105] | [Investigating the Histogram Loss in Regression](https://arxiv.org/abs/2402.13425) | 学习整个分布在回归中的性能提升主要来自于优化的改进，而不是学习更好的表示。 |
| [^106] | [Context-Aware Quantitative Risk Assessment Machine Learning Model for Drivers Distraction](https://arxiv.org/abs/2402.13421) | 提供了一种考虑车辆、驾驶员和环境数据的全新多类驾驶员分心风险评估（MDDRA）模型，可以灵活调整参数和权重以考虑不同严重级别的事件，从而减少由驾驶员分心引起的道路事故。 |
| [^107] | [EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding](https://arxiv.org/abs/2402.13418) | EvolMPNN通过进化感知的方式捕捉蛋白质突变对于锚定蛋白质的影响，并最终生成综合蛋白质嵌入。 |
| [^108] | [Harnessing Large Language Models as Post-hoc Correctors](https://arxiv.org/abs/2402.13414) | 通过提出的无需训练的框架 LlmCorr，本文展示了一个LLM可以作为事后校正器，为任意ML模型的预测提出修正。 |
| [^109] | [Scaling physics-informed hard constraints with mixture-of-experts](https://arxiv.org/abs/2402.13412) | 新方法能够有效扩展物理信息的硬约束，提高了对复杂动力系统的建模效率。 |
| [^110] | [Bayesian Neural Networks with Domain Knowledge Priors](https://arxiv.org/abs/2402.13410) | 提出了一个框架，通过变分推断将各种形式的领域知识整合到贝叶斯神经网络（BNNs）先验中，以实现更好符合领域知识的模型，从而获得更具表现力的后验样本。 |
| [^111] | [Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities](https://arxiv.org/abs/2402.13402) | 该论文提出一种通过非交互式和交互式多保真度贝叶斯优化加速物理发现的方法，解决了探索多维非可微参数空间的挑战，如相图和组成空间等，运用主动学习方法来进行自适应探索。 |
| [^112] | [The Dimension of Self-Directed Learning](https://arxiv.org/abs/2402.13400) | 本论文研究了二元和多类别设置下的自主学习复杂性，提出了一个新的维度$SDdim$来精确刻画任何概念类别的自主学习错误上界，并利用“标记游戏”进行解释，展示了在各种例子中的计算结果和对自主学习的学习差距。 |
| [^113] | [Fairness Risks for Group-conditionally Missing Demographics](https://arxiv.org/abs/2402.13393) | 通过概率填充敏感特征，联合学习群体条件性缺失概率，增强一般公平风险，实现准确性和公平性之间的改进平衡 |
| [^114] | [Transformer tricks: Precomputing the first layer](https://arxiv.org/abs/2402.13388) | 该论文描述了一种加速具有RoPE的transformer推断的技巧，通过预计算第一层来降低延迟和成本，最大节省取决于总层数。 |
| [^115] | [Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers](https://arxiv.org/abs/2402.13380) | 这项研究利用变压器模型解决混合整数规划问题，首次采用变压器预测二进制变量，提出的算法在解决时间上超越了传统CPLEX和LSTM。 |
| [^116] | [Referee-Meta-Learning for Fast Adaptation of Locational Fairness](https://arxiv.org/abs/2402.13379) | 提出了一个定位元裁判（Meta-Ref）的方法，通过动态调整训练样本的学习率，以在不同地点之间实现公平性，并在深度神经网络上进行指导，从而消除地点偏见。 |
| [^117] | [FIDLAR: Forecast-Informed Deep Learning Architecture for Flood Mitigation](https://arxiv.org/abs/2402.13371) | FIDLAR提出了一种预测导向深度学习架构，实现了快速和最优洪水管理，准确进行水前释放。 |
| [^118] | [The Uncanny Valley: A Comprehensive Analysis of Diffusion Models](https://arxiv.org/abs/2402.13369) | 通过对扩散模型的综合分析，揭示了决定模型性能的隐藏关键因素，为DMs的推进提供了见解。 |
| [^119] | [Statistical curriculum learning: An elimination algorithm achieving an oracle risk](https://arxiv.org/abs/2402.13366) | 提出了一种淘汰学习方法，其风险与强Oracle学习者相匹配，并将弱Oracle学习者的风险作为自适应学习者风险的一个实际基准。 |
| [^120] | [Combining unsupervised and supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer](https://arxiv.org/abs/2402.13353) | 通过结合各种图像分析和数据挖掘技术，实现了对一个KOHH蚀刻的4H-SiC晶片显微镜图像中所有缺陷类型和位置的自动化提取 |
| [^121] | [KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers](https://arxiv.org/abs/2402.13352) | 该研究利用Transformer机器学习架构生成“看起来真实”的量子电路，以增强现有的量子电路数据集。 |
| [^122] | [Incentivized Exploration via Filtered Posterior Sampling](https://arxiv.org/abs/2402.13338) | 后验采样算法在激励性探索问题中是一种通用解决方案，可以适用于多种扩展设置，为扩展了的IE问题提供了一般性分析。 |
| [^123] | [Double machine learning for causal hybrid modeling -- applications in the Earth sciences](https://arxiv.org/abs/2402.13332) | 本文引入了一种通过因果推断框架估计混合模型的新方法，利用双机器学习(DML)来估计因果效应，并在地球科学中展示了其在估计因果参数方面优于端到端深度神经网络(DNN)方法的效率、鲁棒性和避免等效性的能力。 |
| [^124] | [Rigor with Machine Learning from Field Theory to the Poincar\'e Conjecture](https://arxiv.org/abs/2402.13321) | 机器学习技术如何在自然科学领域取得严格的结果，通过猜想生成或强化学习验证，解决了在理论物理和数学领域中严谨性与理解之间的矛盾。 |
| [^125] | [Harmful algal bloom forecasting. A comparison between stream and batch learning](https://arxiv.org/abs/2402.13304) | 流式学习是有望解决时间序列问题中概念漂移的最有希望方法之一，本研究对其在预测有害藻华细胞损害方面的效力进行了测试并与批处理学习进行了比较。 |
| [^126] | [DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models](https://arxiv.org/abs/2402.13291) | 使用大型语言模型修复复杂语义bug，通过新的查询和微调方法来解决长距离代码关系学习的挑战。 |
| [^127] | [Manipulating hidden-Markov-model inferences by corrupting batch data](https://arxiv.org/abs/2402.13287) | 通过污染数据，本研究提供了一种新颖的概率视角，用于操纵隐马尔可夫模型的推断，并开发了多种解决方法进行应用和实证测试。 |
| [^128] | [Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization Bounds with Complexity Measures](https://arxiv.org/abs/2402.13285) | 本文利用 PAC-Bayes 理论和 Gibbs 分布提出了一个新的泛化界限框架，可适用于任意复杂度度量，允许对泛化差距进行定制化调整。 |
| [^129] | [MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek in WSNs](https://arxiv.org/abs/2402.13277) | 提出了一种将机器学习技术与Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)算法相结合的创新入侵检测方法 |
| [^130] | [Global Tropical Cyclone Intensity Forecasting with Multi-modal Multi-scale Causal Autoregressive Model](https://arxiv.org/abs/2402.13270) | 提出了一种全球热带气旋强度预测模型MSCAR，首次将因果关系与大规模多模态数据相结合，有效优于现有最先进方法。 |
| [^131] | [Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming](https://arxiv.org/abs/2402.13224) | 本文介绍了一个新的电动汽车充电站模型，通过用户行为建模和随机规划，解决了充电会话不确定性问题，并提出了两种方法来优化成本并提高用户满意度。 |
| [^132] | [CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning](https://arxiv.org/abs/2402.13221) | 图机器学习领域目前主要集中在预测分子和材料的目标特性，而尚未达到生成能力与其他领域的水平。 |
| [^133] | [A unifying primary framework for quantum graph neural networks from quantum graph states](https://arxiv.org/abs/2402.13001) | 量子图神经网络模型可基于图态理解和实现，可用作参数化的量子电路来表示神经网络，或作为构建量子计算机上的图神经网络的基础结构。 |
| [^134] | [PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling](https://arxiv.org/abs/2402.12503) | PARCv2通过引入微分算子扩展了PARC模型，用于模拟不稳定、瞬态和传输主导系统的时空动力学。 |
| [^135] | [Turn Waste into Worth: Rectifying Top-$k$ Router of MoE](https://arxiv.org/abs/2402.12399) | 提出了Rectify-Router解决了MoE模型中常用的Top-k路由机制所带来的令牌丢失和填充问题，通过Intra-GPU矫正和Fill-in矫正来实现。 |
| [^136] | [Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data](https://arxiv.org/abs/2402.12391) | 引入了一个名为AI科学家团队（TAIS）的框架，旨在简化科学发现流程，由模拟角色协作，特别关注于识别具有疾病预测价值的基因 |
| [^137] | [Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!](https://arxiv.org/abs/2402.12343) | 安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。 |
| [^138] | [Causal Equal Protection as Algorithmic Fairness](https://arxiv.org/abs/2402.12062) | 本文提出了一种新的算法公平性原则——平等保护，其关键在于将错误分类的风险均等化，避免了许多对传统分类平等原则的反例。 |
| [^139] | [The effect of Leaky ReLUs on the training and generalization of overparameterized networks](https://arxiv.org/abs/2402.11942) | Leaky ReLU参数$\alpha=-1$在训练误差和泛化误差界方面是最优的选择。 |
| [^140] | [Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge](https://arxiv.org/abs/2402.11459) | 提出了一种新颖的扩散桥生成模型 Re-Dock，用于灵活和现实的分子对接，通过能量到几何映射来共同建模结合能和构象，填补了对接中的实用性和构象预测方面的差距 |
| [^141] | [Trust Regions for Explanations via Black-Box Probabilistic Certification](https://arxiv.org/abs/2402.11168) | 通过黑盒概率认证解释的信任区域能够有效地洞察模型行为、保证解释的稳定性，并实现解释的重用 |
| [^142] | [Accelerating Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2402.10991) | 提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。 |
| [^143] | [CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback](https://arxiv.org/abs/2402.10980) | 通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索 |
| [^144] | [In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss](https://arxiv.org/abs/2402.10790) | 通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理长达 1000 万个元素的任务，这是迄今为止处理最长输入的开放神经网络模型，并展示了对长序列处理能力的显著改进。 |
| [^145] | [Simple, unified analysis of Johnson-Lindenstrauss with applications](https://arxiv.org/abs/2402.10232) | 这项工作提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，简化和统一了各种构造，包括球形、高斯、二进制硬币和次高斯模型，通过创新性地将Hanson-Wright不等式拓展到高维度，标志着对数据固有几何的保持取得重大进展。 |
| [^146] | [Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/abs/2402.10097) | 这项研究提出了一种适用于异构无线网络的自适应联邦学习方法，其中包括了独立客户端采样和带宽分配方案，以提高训练效率和适应数据和系统的异构特性。 |
| [^147] | [User Modeling and User Profiling: A Comprehensive Survey](https://arxiv.org/abs/2402.09660) | 这篇综述论文介绍了用户建模与用户画像研究的现状、发展和未来方向。该研究主要关注在人工智能应用中构建准确的用户表示，包括利用大量数据进行建模以及采用深度学习和图数据技术等先进方法。 |
| [^148] | [Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?](https://arxiv.org/abs/2402.09056) | 本论文提出了关于证据深度学习的新理论洞见, 高亮了在优化二阶损失函数和解释得出的认识不确定性度量上的困难性 |
| [^149] | [SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds](https://arxiv.org/abs/2402.08653) | SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。 |
| [^150] | [Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning](https://arxiv.org/abs/2402.07818) | 本文研究了差分隐私零阶方法在大型语言模型微调中的应用，该方法通过使用零阶梯度来避免传统优化方法的可扩展性瓶颈，实现了在隐私、效用和可扩展性之间的良好平衡。 |
| [^151] | [Rethinking Scaling Laws for Learning in Strategic Environments](https://arxiv.org/abs/2402.07588) | 本文重新思考了在战略环境中学习的比例定律，发现战略互动可以打破传统的观点，即模型越大或表达能力越强并不一定会随之提高性能。通过几个战略环境的例子，我们展示了这种现象的影响。 |
| [^152] | [Thresholded Oja does Sparse PCA?](https://arxiv.org/abs/2402.07240) | 阈值和重新归一化Oja算法的输出可获得一个接近最优的错误率，与未经阈值处理的Oja向量相比，这大大减小了误差。 |
| [^153] | [TREET: TRansfer Entropy Estimation via Transformer](https://arxiv.org/abs/2402.06919) | 本研究提出了TREET，一种基于Transformer的传输熵估计方法，通过引入Donsker-Vardhan表示法和注意力机制，实现了对稳定过程的传输熵估计。我们设计了估计TE的优化方案，并展示了通过联合优化方案优化通信通道容量和估计器的记忆能力。 |
| [^154] | [Tactile-based Object Retrieval From Granular Media](https://arxiv.org/abs/2402.04536) | 这项研究介绍了一种基于触觉反馈的机器人操作方法，用于在颗粒介质中检索埋藏的物体。通过模拟传感器噪声进行端到端训练，实现了自然出现的学习推动行为，并成功将其迁移到实际硬件上。 |
| [^155] | [Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources](https://arxiv.org/abs/2402.04273) | 本文提出了一种名为FDA的框架，通过跨领域学习来打破多智能体感知中的数据孤岛问题。该框架包括可学习特征补偿模块和分布感知统计一致性模块，用于增强中间特征交流和数据分布一致性。 |
| [^156] | [SEABO: A Simple Search-Based Method for Offline Imitation Learning](https://arxiv.org/abs/2402.03807) | SEABO是一种简单而有效的基于搜索的离线模仿学习方法，它以无监督学习的方式，根据专家数据和无标签数据得到奖励函数，实验结果表明其性能与离线强化学习相当。 |
| [^157] | [MolTC: Towards Molecular Relational Modeling In Language Models](https://arxiv.org/abs/2402.03781) | 本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。 |
| [^158] | [Denoising Diffusion via Image-Based Rendering](https://arxiv.org/abs/2402.03445) | 该论文介绍了一个新的基于图像渲染的去噪扩散模型，可以快速重构和生成真实世界3D场景，通过引入新的神经场景表示方法和扩散建模方法来实现。 |
| [^159] | [Exploiting Class Probabilities for Black-box Sentence-level Attacks](https://arxiv.org/abs/2402.02695) | 该论文研究了在黑盒子句级攻击中利用类别概率的有效性，并开发了一种新的算法进行攻击。通过与基线方法进行对比，进行了广泛的评估。 |
| [^160] | [Graph Contrastive Learning with Cohesive Subgraph Awareness](https://arxiv.org/abs/2401.17580) | 本研究提出了一种名为CTAug的新框架，将内聚子图意识无缝整合到图对比学习中。通过改进图拓扑增强和图学习过程，提高了对各种图的表征学习性能。 |
| [^161] | [A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning](https://arxiv.org/abs/2312.15474) | 提出了一种受最近模仿学习和保守RL算法进展启发的创新方法，在离线动力学强化学习中的少样本转移过程中引入惩罚来调节源训练策略生成的轨迹。 |
| [^162] | [Towards Message Brokers for Generative AI: Survey, Challenges, and Opportunities](https://arxiv.org/abs/2312.14647) | 该研究调查了传统和现代消息代理，比较分析了流行平台，为数据中心GenAI模型的需求增加提供了健壮的数据通信基础设施 |
| [^163] | [Comparing Machine Learning Algorithms by Union-Free Generic Depth](https://arxiv.org/abs/2312.12839) | 本研究提出了一种描述性分析偏序集合的框架，通过改进的无交并泛深度 (ufg) 比较机器学习算法，并在标准基准数据集上提供了示例。研究结果展示了基于ufg方法的多样性分析方法，并与现有的基准测试方法有很大区别。 |
| [^164] | [Efficient and Scalable Graph Generation through Iterative Local Expansion](https://arxiv.org/abs/2312.11529) | 通过逐步扩展单个节点到目标图的方法，避免了对所有节点对的整个联合分布进行建模，实现了高效可扩展的图生成，同时通过多尺度生成保持了高表达性。 |
| [^165] | [Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning](https://arxiv.org/abs/2312.10107) | 分析了上下文感知领域泛化的条件，提出了理论分析和实证分析所需的标准，并展示了该方法可以检测非常数域的场景。 |
| [^166] | [Reconstruction of Sound Field through Diffusion Models](https://arxiv.org/abs/2312.08821) | 本文提出了一种用于在房间内重建声场的数据驱动生成模型，通过条件化去噪扩散概率模型训练和生成声场，实现了准确的重建。 |
| [^167] | [On the Impact of Multi-dimensional Local Differential Privacy on Fairness](https://arxiv.org/abs/2312.04404) | 多维局部差分隐私在减少不公平性方面表现出高效，而多维方法及结果分布均对公平性产生影响。 |
| [^168] | [Hidden yet quantifiable: A lower bound for confounding strength using randomized trials](https://arxiv.org/abs/2312.03871) | 利用随机试验设计了一种统计检验，能够量化未观察到的混淆强度，并估计其下界，有效应用于现实世界中识别混淆。 |
| [^169] | [LSTSVR-PI: Least square twin support vector regression with privileged information](https://arxiv.org/abs/2312.02596) | 提出的 LSTSVR-PI模型结合了最小二乘双支持向量回归和特权信息，提高了模型效率，并建立了泛化误差界限。 |
| [^170] | [Tree of Attacks: Jailbreaking Black-Box LLMs Automatically](https://arxiv.org/abs/2312.02119) | 提出了一种名为Tree of Attacks with Pruning (TAP)的自动化方法，用于生成只需要对目标大型语言模型进行黑盒访问的越狱方法，并通过思维树推理和修剪生成准确的越狱提示。 |
| [^171] | [Creating and Leveraging a Synthetic Dataset of Cloud Optical Thickness Measures for Cloud Detection in MSI](https://arxiv.org/abs/2311.14024) | 本论文提出了一种利用合成数据集进行云光学厚度测量的方法，以解决在地球观测背景下标记数据稀缺的问题。 |
| [^172] | [A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables](https://arxiv.org/abs/2311.11343) | 提出了一种用于加速反向建模的生成模型，利用了新型的连续变量嵌入策略，消除了归一化的需要，保留信息并创造了一个多功能的嵌入空间。 |
| [^173] | [Adaptive Interventions with User-Defined Goals for Health Behavior Change](https://arxiv.org/abs/2311.09483) | 该论文介绍了一种修改过的Thompson抽样算法，强调通过优化个性化奖励函数实现个性化目标设定，为支持目标设定提供了一个平衡方法，并证明此修改仅对累积遗憾产生恒定的惩罚。 |
| [^174] | [Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation](https://arxiv.org/abs/2311.06973) | 本论文研究了使用深度神经网络进行配电系统同步时间状态估计的性能和鲁棒性。通过将输入扰动视为混合整数线性规划问题进行分析验证，并强调了批归一化在提高问题可扩展性方面的作用。该框架在修改后的IEEE 34节点系统和真实的大型分布系统上进行验证。 |
| [^175] | [Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting](https://arxiv.org/abs/2310.18948) | 通过利用AIS数据预测船舶轨迹，本研究旨在通过减少船舶与鲸鱼碰撞来建立更安全的海洋环境。 |
| [^176] | [Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems](https://arxiv.org/abs/2310.10462) | 提出了一种新颖的透视角度，强调优化级联排名系统的适应性，解决了现有方法无法适应不同级联排名场景的问题，可以提升性能并避免次优情况的发生。 |
| [^177] | [The Emergence of Reproducibility and Consistency in Diffusion Models](https://arxiv.org/abs/2310.05264) | 该论文研究了扩散模型中的一致模型可重复性现象，实验证实了无论模型框架、模型架构或训练过程如何，不同的扩散模型都能够一致地达到相同的数据分布和评分函数。此外，研究发现扩散模型在学习过程中受训练数据规模的影响，表现出两种不同的训练模式：记忆化模式和泛化模式。 |
| [^178] | [Fair Ranking under Disparate Uncertainty](https://arxiv.org/abs/2309.01610) | 提出了一种新的公平排名标准Equal-Opportunity Ranking（EOR），将底层相关性模型的不确定性差异考虑在内，通过组内公平抽奖实现公平排名。 |
| [^179] | [Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads](https://arxiv.org/abs/2308.14456) | 本研究探讨了如何改变探测头架构会对基准测试结果产生影响，在语音自监督学习中评估了更大容量的探测头，展示了其对性能和推断成本的影响。 |
| [^180] | [Fixing confirmation bias in feature attribution methods via semantic match](https://arxiv.org/abs/2307.00897) | 提出了通过语义匹配修复特征归因方法中的确认偏见问题，引入了人类概念与（亚符号）解释之间的概念框架，并提出了一种结构化方法来评估语义匹配。 |
| [^181] | [Globally Interpretable Graph Learning via Distribution Matching](https://arxiv.org/abs/2306.10447) | 该论文提出了通过分布匹配实现全局可解释的图学习的方法，旨在提取主导学习过程的高级模式，以实现全局解释。 |
| [^182] | [TESS: Text-to-Text Self-Conditioned Simplex Diffusion](https://arxiv.org/abs/2305.08379) | TESS是一个全非自回归的文本扩散模型，通过在逻辑空间而不是学习嵌入空间应用扩散过程，进行了自条件单纯形扩散，实验证明在自然语言理解和生成任务中表现优于最先进的非自回归模型，并且所需的扩散步骤更少。 |
| [^183] | [PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics](https://arxiv.org/abs/2303.05376) | PC-JeDi是一种在高能物理中生成粒子云的新方法，利用分数扩散模型和变换器，实现了竞争性能，并能以条件生成方式产生具有所需质量和横动量的喷注。 |
| [^184] | [Learning-based Online Optimization for Autonomous Mobility-on-Demand Fleet Control](https://arxiv.org/abs/2302.03963) | 开发了一种新颖的混合组合优化增强机器学习流水线，从最优的完全信息解中学习在线派遣和再平衡策略，并在大规模实际场景中展示了其性能优于现有方法，实现利润提高了6.3%。 |
| [^185] | [Approximation of optimization problems with constraints through kernel Sum-Of-Squares](https://arxiv.org/abs/2301.06339) | 通过核和平方和逼近解决带约束的优化问题，提出了统一定理证明这些方案的收敛性，引入散射不等式以缓解维度灾难问题，并在学习向量场的应用中进行了验证。 |
| [^186] | [The non-overlapping statistical approximation to overlapping group lasso](https://arxiv.org/abs/2211.09221) | 该论文提出了一种针对重叠分组 lasso 的非重叠统计逼近方法，在大规模问题中计算速度更快，为现代问题的应用提供了可能性。 |
| [^187] | [Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition](https://arxiv.org/abs/2211.07245) | 评估相似度评分函数性能和公平性质的关键工具是ROC曲线，文章提出了一种准确评估与ROC曲线相关不确定性水平的方法，特别适用于面部识别等具有社会影响的应用。 |
| [^188] | [CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction](https://arxiv.org/abs/2210.05988) | CLEEGN是一种卷积神经网络，用于即插即用自动脑电图重建，基于独立于受试者的预训练模型，无需进一步校准即可在新用户上操作。 |
| [^189] | [Self-supervised Representation Learning on Electronic Health Records with Graph Kernel Infomax](https://arxiv.org/abs/2209.00655) | 提出了一种名为Graph Kernel Infomax的自监督图核学习方法，用于处理电子健康记录的表征，克服了复杂时间性对性能的影响。 |
| [^190] | [Mildly Conservative Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2206.04745) | 本文提出了一种离线强化学习中的温和保守Q学习（MCQ）方法，通过为OOD动作分配适当的伪Q值来训练，从而在不损害泛化能力的情况下实现价值函数的保守性，避免过度高估超出分布的动作。 |
| [^191] | [Trustworthy Graph Neural Networks: Aspects, Methods and Trends](https://arxiv.org/abs/2205.07424) | 论文提出了构建可信赖图神经网络的综合路线图，关注解决性能导向的图神经网络可能存在的对抗性攻击、歧视问题和资源消耗过多等挑战。 |
| [^192] | [Conflict-Averse Gradient Descent for Multi-task Learning](https://arxiv.org/abs/2110.14048) | 冲突回避梯度下降（CAGrad）是针对多任务学习中梯度冲突问题提出的方法，旨在解决不同任务梯度不一致导致的性能下降挑战。 |
| [^193] | [Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible Cluster Shapes.](http://arxiv.org/abs/2401.16708) | 本文提出了一种名为多元贝塔混合模型（MBMM）的新的概率模型，用于软聚类。MBMM通过其灵活的多元贝塔分布的概率密度函数适应不同的聚类形状，并在合成和真实数据集上展示了其适应性。 |
| [^194] | [High-Quality Image Restoration Following Human Instructions.](http://arxiv.org/abs/2401.16468) | 本论文提出了一种使用人类编写的指令来指导图像恢复模型的方法，并在多个恢复任务上取得了最先进的结果，为基于文本指导的图像恢复和增强研究提供了一个新的基准。 |
| [^195] | [The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images.](http://arxiv.org/abs/2401.08865) | 本文研究了神经网络在自然图像和医学图像领域学习时的差异，提出了一个与训练集维度有关的泛化缩放定律，并认为医学图像数据集更高的固有“标签锐度”可能是两个领域之间显著差异的部分原因。 |
| [^196] | [Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models.](http://arxiv.org/abs/2401.06432) | 本文提出了一种用于异构设备的设备本地基础模型联邦微调的参数高效方法，使用了异构低秩近似（LoRA），解决了资源受限和异构设备的挑战。 |
| [^197] | [GNNShap: Fast and Accurate GNN Explanations using Shapley Values.](http://arxiv.org/abs/2401.04829) | GNNShap是一种使用Shapley值的解释方法，能够快速而准确地解释图神经网络的预测结果。相较于其他方法，GNNShap通过抽样、并行化计算等技术提高了解释速度和精细度。 |
| [^198] | [Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors.](http://arxiv.org/abs/2401.02686) | 这项研究评估了基于图和序列表示的十种漏洞检测器解释方法的性能，发现单纯的忠诚度评估不足够。 |
| [^199] | [A Study of Continual Learning Under Language Shift.](http://arxiv.org/abs/2311.01200) | 本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。 |
| [^200] | [Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution.](http://arxiv.org/abs/2310.16834) | 本研究通过引入得分熵这一新颖的离散得分匹配损失，弥补了离散数据领域中现有方法的不足，提出了得分熵离散扩散模型(SEDD)并在GPT-2实验中取得了有竞争力的效果。 |
| [^201] | [A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs.](http://arxiv.org/abs/2310.12248) | 这个论文介绍了一种基于模型的PAC学习算法，用于在MDP中学习ω-regular目标，不需要系统拓扑的先前知识。 |
| [^202] | [EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms.](http://arxiv.org/abs/2310.11198) | 本研究探索了在运动意向解码领域应用不同通道关注机制的可行性，通过构建一个轻量级架构框架，并在同一环境中比较它们的影响，结果表明这些机制的易集成性和低计算复杂度使其成为BCI中运动意向解码的有效方法。 |
| [^203] | [Comparing Comparators in Generalization Bounds.](http://arxiv.org/abs/2310.10534) | 本文推导了涉及任意凸比较函数的通用信息理论和PAC-Bayesian泛化界限，证明了最紧界限是由凸共轭的累积生成函数(CGF)构成的，使得这些界限广泛适用于不同结构的泛化界限。 |
| [^204] | [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models.](http://arxiv.org/abs/2310.08041) | QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。 |
| [^205] | [Interpretable Diffusion via Information Decomposition.](http://arxiv.org/abs/2310.07972) | 本研究通过观察扩散和信息分解之间的关系，揭示了扩散模型学习到的细粒度关系，进一步解决了高维空间中信息携带变量的问题。 |
| [^206] | [Relational Convolutional Networks: A framework for learning representations of hierarchical relations.](http://arxiv.org/abs/2310.03240) | 关系卷积网络是一个学习显式层次关系表示的框架，通过使用多维内积关系模块和关系卷积层，以及基于图元滤波器的群组比较，能够表达更高阶、层次的关系。 |
| [^207] | [Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel.](http://arxiv.org/abs/2310.03054) | 本文提出了一种基于负距离核的最大平均距离(MMD)的条件流方法，用于后验抽样和条件生成建模。通过离散的Wasserstein梯度流近似联合分布，证明了粒子流是适当功能的Wasserstein梯度流。在条件图像生成和超分辨率等逆问题中展示了方法的有效性。 |
| [^208] | [Scaling Laws for Associative Memories.](http://arxiv.org/abs/2310.02984) | 本文研究了应用于联想记忆中的缩放定律，通过高维矩阵和嵌入的外积来模拟内层Transformer语言模型。作者推导出了与样本数量和参数大小相关的精确缩放定律，并验证了理论结果的有效性。同时，作者还通过大量实验展示了存储记忆关联的细粒度可视化。 |
| [^209] | [The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs.](http://arxiv.org/abs/2310.01468) | 本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。 |
| [^210] | [TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework.](http://arxiv.org/abs/2309.16935) | TranDRL是一种基于Transformer驱动的深度强化学习支持的预防性维护框架，结合了复杂时间模式捕捉和经济高效维护建议，显著提高了剩余寿命（RUL）预测准确性和维护行动优化。 |
| [^211] | [Are Human-generated Demonstrations Necessary for In-context Learning?.](http://arxiv.org/abs/2309.14681) | 本文研究了上下文学习中人工生成的演示是否有必要，并提出了一种新的自反思提示策略（SEC），通过这种策略，大型语言模型（LLMs）可以自行生成演示和最终输出，避免了手动生成过程的复杂性。 |
| [^212] | [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training.](http://arxiv.org/abs/2309.10400) | 本文介绍了一种名为PoSE的训练方法，通过在训练过程中使用固定的上下文窗口和操纵位置索引来适应极长的上下文窗口，实验证明这种方法大大减小了内存和时间开销，对性能影响较小，成功将LLaMA模型扩展到了128k个标记。 |
| [^213] | [A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification.](http://arxiv.org/abs/2309.08415) | 本研究提出了一种使用机器学习和不确定性量化建模的多阶段决策过程方法，用于预测心力衰竭患者对心脏再同步治疗的反应。该模型能够推荐收集额外的SPECT MPI变量，以提高预测准确性。 |
| [^214] | [What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving.](http://arxiv.org/abs/2309.07808) | 本文提出了一种基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能，并解决了交通规则遵守和传感器感知问题。 |
| [^215] | [Cross-domain Sound Recognition for Efficient Underwater Data Analysis.](http://arxiv.org/abs/2309.03451) | 本文提出了一种跨域声音识别方法，通过利用非水下声音模型训练数据来分析水下声学数据。通过聚类和可视化方法，简化了候选标签选择的过程，并通过训练神经网络模型实现了对空气枪声的高效识别。 |
| [^216] | [Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks.](http://arxiv.org/abs/2308.16800) | 本文研究了图神经网络中的平滑过度和特征关联过高现象，发现固定不变的子空间导致了节点表示的等级崩塌。在该子空间中平滑向量的存在导致过度平滑，即使避免过度平滑也会导致过高的关联。为了解决这个问题，我们提出了一种克罗内克积之和作为一种有效方法。 |
| [^217] | [Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.16198) | 本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。 |
| [^218] | [An Adaptive Tangent Feature Perspective of Neural Networks.](http://arxiv.org/abs/2308.15478) | 本研究提出了一个切向特征视角的框架，通过线性变换和结构正则化优化神经网络的特征学习，从而更好地理解神经网络的特征学习过程。在实验证明了该方法在回归问题中的有效性，并提供了对核对齐现象的细致分析。 |
| [^219] | [Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies.](http://arxiv.org/abs/2308.14120) | chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。 |
| [^220] | [CausalLM is not optimal for in-context learning.](http://arxiv.org/abs/2308.06912) | 最近的研究显示，上下文学习中使用前缀语言模型（PrefixLM）比因果语言模型（CausalLM）效果更好。本文通过理论分析证明，虽然两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，因果语言模型的收敛动态遵循在线梯度下降算法，不保证收敛到最优解。 |
| [^221] | [Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning.](http://arxiv.org/abs/2307.05209) | 我们提出了一种使用奖励机器抽象来表示当前任务，并在迁移学习中提升DRL代理的性能的方法，实验表明该方法能够提高样本效率并在多个领域中进行少样本迁移。 |
| [^222] | [A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding.](http://arxiv.org/abs/2307.01470) | 本文综述了驾驶员凝视估计的基本知识、估计方法以及在实际驾驶场景中的应用。通过讨论不同的数据收集方法和算法技术，对驾驶员的凝视行为进行了理解。 |
| [^223] | [NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes.](http://arxiv.org/abs/2306.16869) | NeuralFuse是一个新颖的附加模块，通过学习输入转换来生成抗误差的数据表示，解决了低电压环境下有限访问神经网络推断的准确性与能量之间的权衡问题。 |
| [^224] | [Latent SDEs on Homogeneous Spaces.](http://arxiv.org/abs/2306.16248) | 这篇论文研究了在齐次空间上的潜在SDE，通过使用单位球上的SDE进行变分推断，提出了一种简单且直观的表达式来计算近似后验和先验过程之间的KL散度。 |
| [^225] | [Hierarchical Neural Simulation-Based Inference Over Event Ensembles.](http://arxiv.org/abs/2306.12584) | 本文介绍了一种基于层级神经模拟的方法，可以在似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断，着重考虑了模型的层级结构，可以导致更紧凑的参数约束。 |
| [^226] | [AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks.](http://arxiv.org/abs/2306.08107) | 论文探讨了AutoML和LLMs之间的共生关系，并指出这两个领域的融合有望颠覆NLP和AutoML两个领域，同时也存在风险。 |
| [^227] | [A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning.](http://arxiv.org/abs/2306.07541) | SUNG是一种基于不确定性引导的离线到在线强化学习框架，在通过量化不确定性进行探索和应用保守Q值估计的指导下，实现了高效的老化强化学习。 |
| [^228] | [SENS: Sketch-based Implicit Neural Shape Modeling.](http://arxiv.org/abs/2306.06088) | SENS是一种基于草图的生成和编辑3D模型的新方法，该方法通过ViT补丁编码将草图映射到神经隐式形状架构的潜空间中，可以捕捉用户草图的意图进行生成，具有强大的性能和直观的基于草图的形状编辑功能。 |
| [^229] | [How Sparse Can We Prune A Deep Network: A Geometric Viewpoint.](http://arxiv.org/abs/2306.05857) | 本文从高维几何的角度，通过在原始损失函数中强制施加稀疏性约束，描述了深度网络剪枝比率的相变点，该点等于某些凸体的平方高斯宽度除以参数的原始维度。 |
| [^230] | [dotears: Scalable, consistent DAG estimation using observational and interventional data.](http://arxiv.org/abs/2305.19215) | dotears是一个可扩展的DAG结构学习框架，使用观测和干预数据来推断单个因果结构。它直接估计外生误差结构，避免了循环估计问题。 |
| [^231] | [Rotational Optimizers: Simple & Robust DNN Training.](http://arxiv.org/abs/2305.17212) | 该论文提出了旋转优化器，这些优化器可以简化深度神经网络训练过程，甚至在几乎不需调整基线超参数的情况下与原始优化器的性能相匹配。 |
| [^232] | [InstructIE: A Chinese Instruction-based Information Extraction Dataset.](http://arxiv.org/abs/2305.11527) | 介绍了一份中文的基于指令的信息提取数据集InstructIE，其中包括了270,000个弱监督的数据和1,000个高质量注释实例。实验结果表明当前的模型表现有待改进，该任务仍存在挑战。 |
| [^233] | [Data Redaction from Conditional Generative Models.](http://arxiv.org/abs/2305.11351) | 本文研究如何对已训练好的条件生成模型进行后期编辑，以便编辑掉某些条件分支，这些条件分支很可能会生成不良内容。通过精简模型中的条件网络实现，提出的解决方案有效、高效、具有可控性和普适性，在文本到图像和文本到语音生成模型中取得了良好效果。 |
| [^234] | [Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation.](http://arxiv.org/abs/2303.11602) | 本论文在高维度球体上，通过神经网络参数化，使用SGD算法在监督学习和无监督学习中，提供了一种新算法，并且证明了其收敛性。 |
| [^235] | [A Survey on Automated Design of Metaheuristic Algorithms.](http://arxiv.org/abs/2303.06532) | 本文综述了自动设计元启发式算法的形式化、方法论、挑战和研究趋势，讨论了自动设计的潜在未来方向和开放问题。 |
| [^236] | [The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes.](http://arxiv.org/abs/2212.04631) | 本文提出了一种用于量化随机过程统计依赖关系的框架，通过最大化交替协方差估计和规范化交叉密度来衡量多变量统计依赖性，并应用于机器学习架构中。 |
| [^237] | [RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation.](http://arxiv.org/abs/2211.09869) | 本文提出了第一个支持3D理解任务的扩散模型RenderDiffusion，只需使用单眼2D监督进行训练。它利用一种新颖的图像去噪架构，生成和渲染中间的三维表示，在扩散过程中提供强有力的3D一致性。 |
| [^238] | [Boosting Object Representation Learning via Motion and Object Continuity.](http://arxiv.org/abs/2211.09771) | 通过集成运动和连续性信息，我们提出了一种提升物体表示学习的方法，可以在无监督多对象检测模型中产生更优的物体编码，并在物体发现、收敛速度和总体潜在物体表示等方面取得明显的改进。 |
| [^239] | [The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning.](http://arxiv.org/abs/2110.14427) | 本文提出了一种称为ODE方法的渐近统计方法解决$d$维随机逼近递归的问题，证明了其收敛性和中心极限定理，为强化学习等领域的应用提供了有力的理论支持。 |
| [^240] | [Machine Learning with a Reject Option: A survey.](http://arxiv.org/abs/2107.11277) | 这项调查综述了机器学习中的拒绝选项。通过机器学习模型避免在可能犯错误时做出预测，可以在决策支持应用中避免严重后果。调查介绍了拒绝选项的条件、评估策略以及相关应用领域，并探讨了它与其他机器学习方法的关系。 |

# 详细

[^1]: 迫使LLMs执行并揭示（几乎）任何事情

    Coercing LLMs to do and reveal (almost) anything

    [https://arxiv.org/abs/2402.14020](https://arxiv.org/abs/2402.14020)

    本研究发现对大型语言模型的对抗性攻击不仅仅局限于“越狱”，而包括迫使模型展示各种意外行为，攻击表面和目标广泛。这些攻击源于LLMs的预训练和常见词汇中存在的“故障”标记。

    

    最近有研究表明，对大型语言模型（LLMs）的对抗性攻击可以“越狱”该模型以发表有害言论。在这项工作中，我们认为LLMs的对抗性攻击范围远不止于越狱。我们提供了对可能的攻击面和攻击目标的广泛概述。根据一系列具体示例，我们讨论、分类和系统化了一些攻击，这些攻击迫使LLMs展示各种意外行为，如误导、模型控制、拒绝服务或数据提取。我们通过控制实验分析这些攻击，并发现其中许多是由于预训练LLMs具有编码能力的实践，以及常见LLMs词汇中应删除的奇怪“故障”标记的持续存在所导致的。

    arXiv:2402.14020v1 Announce Type: cross  Abstract: It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.
    
[^2]: D-Flow: 通过流形进行区分生成

    D-Flow: Differentiating through Flows for Controlled Generation

    [https://arxiv.org/abs/2402.14017](https://arxiv.org/abs/2402.14017)

    D-Flow框架通过控制生成过程中的流形，优化源点，解决了扩散和流匹配模型中生成结果的控制问题，同时在多个领域取得了最先进的性能表现。

    

    通过控制当今最先进的扩散和流匹配（FM）模型的生成结果，而无需重新训练特定任务模型，可以解决反问题、条件生成和一般控制生成的强大工具。在这项工作中，我们介绍了D-Flow，一种简单的通过流形控制生成过程的框架，通过优化源（噪声）点进行区分。我们通过我们的关键观察来激发这一框架，该观察指出，对于使用高斯概率路径训练的扩散/FM模型，通过生成过程进行区分会在数据流形上投影梯度，将先验隐式注入到优化过程中。我们在包括图像和音频反问题以及条件分子生成在内的线性和非线性控制生成问题上验证了我们的框架，在所有问题上均达到了最先进的性能。

    arXiv:2402.14017v1 Announce Type: new  Abstract: Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.
    
[^3]: 修正机器消除

    Corrective Machine Unlearning

    [https://arxiv.org/abs/2402.14015](https://arxiv.org/abs/2402.14015)

    该论文通过形式化“修正机器消除”来解决受未知操纵影响的数据对训练模型的影响问题，可能仅知道一部分受影响样本。发现纠正消除问题与传统以隐私为导向的消除方法有显著不同的要求。

    

    机器学习模型越来越面临数据完整性挑战，因为它们使用了大规模的从互联网中获取的训练数据集。本文研究了如果模型开发者发现某些数据被篡改或错误，他们可以采取什么措施。这些被篡改的数据会导致不利影响，如容易受到后门样本的攻击、系统性偏见，以及在某些输入领域的准确度降低。通常，并非所有被篡改的训练样本都是已知的，而只有一小部分代表性的受影响数据被标记。

    arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
    
[^4]: 不对齐、学习和排名：利用用户有限的注意力

    Misalignment, Learning, and Ranking: Harnessing Users Limited Attention

    [https://arxiv.org/abs/2402.14013](https://arxiv.org/abs/2402.14013)

    通过利用用户有限的注意力，我们提出了一种模型来解决推荐系统面临的用户选择冲动与长期回报不一致的挑战，设计在线赌博算法以逐渐减少后悔。

    

    在数字健康和教育科技领域，推荐系统面临着一个重大挑战：用户通常会冲动地选择，这与平台的长期回报相冲突。这种不对齐使得有效地学习排名项目变得困难，因为它可能阻碍了对具有更大长期回报的项目的探索。我们的论文通过利用用户有限的注意力跨越这个问题。我们提出了一个模型，在这个模型中，平台会随着时间向$T$个用户呈现具有未知回报的项目的排名列表。每个用户通过首先考虑这些排名项目的一个前缀窗口，然后选择该窗口中最受欢迎的项目来选择一个项目（平台观察到了该项目的回报）。我们研究了在线赌博算法的设计，使得它们在面对事后最优基准时能够实现逐渐减少的后悔。我们首先考虑对抗性窗口大小和随机iid回报。我们设计了一种基于主动淘汰的算法

    arXiv:2402.14013v1 Announce Type: new  Abstract: In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users' limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algor
    
[^5]: 在满足长期约束条件下追逐凸函数

    Chasing Convex Functions with Long-term Constraints

    [https://arxiv.org/abs/2402.14012](https://arxiv.org/abs/2402.14012)

    引入并研究了一类带有长期约束的在线度量问题，提出了在可持续能源和计算系统中在线资源分配应用中的最优竞争算法和学习增强算法，并通过数值实验表现良好。

    

    我们引入并研究了一类带有长期约束的在线度量问题。在这些问题中，一个在线玩家在度量空间$(X,d)$中做出决策$\mathbf{x}_t$，同时最小化他们的命中成本$f_t(\mathbf{x}_t)$和由度量确定的切换成本。在时间跨度$T$内，玩家必须满足长期需求约束$\sum_{t} c(\mathbf{x}_t) \geq 1$，其中$c(\mathbf{x}_t)$表示时间$t$时满足的需求比例。这类问题在可持续能源和计算系统中的在线资源分配中有着广泛的应用。我们为这些问题的具体实例设计了最优的竞争算法和学习增强算法，并进一步展示了我们提出的算法在数值实验中表现良好。

    arXiv:2402.14012v1 Announce Type: cross  Abstract: We introduce and study a family of online metric problems with long-term constraints. In these problems, an online player makes decisions $\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\mathbf{x}_t)$ and switching cost as determined by the metric. Over the time horizon $T$, the player must satisfy a long-term demand constraint $\sum_{t} c(\mathbf{x}_t) \geq 1$, where $c(\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems. We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments.
    
[^6]: 几何信息神经网络

    Geometry-Informed Neural Networks

    [https://arxiv.org/abs/2402.14009](https://arxiv.org/abs/2402.14009)

    GINNs提出了一种新颖的几何信息神经网络范式，可以在几何任务中生成多样的解决方案，无需训练数据，采用显式多样性损失以及可微损失来减轻模态坍缩，并在实验中展示了其在各种复杂性场景中的高效性。

    

    我们引入了几何信息神经网络（GINNs）的概念，涵盖了（i）在几何约束下学习，（ii）神经场作为合适的表示，（iii）生成在几何任务中经常遇到的欠定系统的多样解决方案。值得注意的是，GINN的构建不需要训练数据，因此可以被纯约束驱动地视为生成建模。我们增加了显式的多样性损失来减轻模态坍缩。我们考虑了几种约束，特别是组件的连通性，我们通过莫尔斯理论将其转化为可微损失。在实验中，我们展示了在不断增加复杂性的二维和三维场景中，GINN学习范式的高效性。

    arXiv:2402.14009v1 Announce Type: new  Abstract: We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.
    
[^7]: 深度结构化（随机）特征学习的渐近分析

    Asymptotics of Learning with Deep Structured (Random) Features

    [https://arxiv.org/abs/2402.13999](https://arxiv.org/abs/2402.13999)

    在高维情况下，我们提供了学习输出层测试误差的严格渐近特性，并对使用高斯彩虹神经网络进行学习的问题做出了重要贡献

    

    针对一大类特征映射，我们在输入维度、隐藏层宽度和训练样本数量成比例增长的高维极限下，提供了与学习输出层相关的测试误差的严格渐近特性刻画。这一特征以特征的总体协方差为基础。我们的工作部分受到使用高斯彩虹神经网络进行学习的问题的启发，即具有随机但结构化权重的深层非线性全连接网络，其按行的协方差进一步允许依赖于之前层的权重。对于这样的网络，我们还推导出了一个以权重矩阵为基础的特征协方差的闭合形式公式。我们进一步发现，在某些情况下，我们的结果能够捕捉通过梯度下降训练的具有有限宽度的深度神经网络学习到的特征映射。

    arXiv:2402.13999v1 Announce Type: cross  Abstract: For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large. This characterization is formulated in terms of the population covariance of the features. Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers. For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices. We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent.
    
[^8]: FedADMM-InSa: 一种不精确和自适应的联邦学习ADMM

    FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning

    [https://arxiv.org/abs/2402.13989](https://arxiv.org/abs/2402.13989)

    提出了一种不精确和自适应的FedADMM算法，通过为客户端的本地更新设计一个不精确性标准，消除了调整本地训练准确度的需要，降低了计算成本并减轻了滞后效应。

    

    联邦学习(FL)是一个有希望的框架，可以从分布式数据中学习同时保持隐私。有效的FL算法的发展面临各种挑战，包括异构数据和系统、通信能力有限以及受限的本地计算资源。最近开发的FedADMM方法对数据和系统的异构性表现出很强的韧性。然而，如果超参数没有经过精心调整，它们仍然会遭受性能下降的问题。为了解决这个问题，我们提出了一种不精确和自适应的FedADMM算法，名为FedADMM-InSa。首先，我们为客户端的本地更新设计了一个不精确性标准，以消除必须根据经验设置本地训练准确性的需求。这种不精确性标准可以由每个客户端独立地根据其独特条件进行评估，从而降低本地计算成本并减轻不良的滞后效应。

    arXiv:2402.13989v1 Announce Type: new  Abstract: Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle e
    
[^9]: 一种简单且相当有效的图神经网络防御方法

    A Simple and Yet Fairly Effective Defense for Graph Neural Networks

    [https://arxiv.org/abs/2402.13987](https://arxiv.org/abs/2402.13987)

    本文介绍了一种名为NoisyGNNs的新颖防御方法，通过将噪声引入基础模型的架构，建立了噪声注入与增强GNN鲁棒性之间的理论连接，实验证明其在节点分类任务上取得了优越或可比的防御性能。

    

    图神经网络（GNNs）已经成为处理图结构数据的机器学习中占主导地位的方法。然而，人们对GNN对小干扰的脆弱性提出了担忧。现有的防御方法针对这种干扰 suffer from high time complexity and can negatively impact the model's performance on clean graphs。为了应对这些挑战，本文引入了 NoisyGNNs，这是一种将噪声融入基础模型架构的新颖防御方法。我们建立了噪声注入与增强GNN鲁棒性之间的理论连接，突显了我们方法的有效性。我们进一步在节点分类任务上进行了广泛的实证评估，验证了我们的理论发现，重点关注了两种流行的GNNs：GCN和GIN。结果表明，NoisyGNN实现了优越或可比的防御性能

    arXiv:2402.13987v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data. However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations. Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs. To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture. We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach. We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN. The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while
    
[^10]: 具有可微Boltzmann估计器的神经网络原子间势的稳定性训练

    Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators

    [https://arxiv.org/abs/2402.13984](https://arxiv.org/abs/2402.13984)

    提出了稳定性感知Boltzmann估计器（StABlE）训练方法，结合传统监督训练和参考系统可观察量，用于生成稳定且准确的神经网络原子间势。

    

    神经网络原子间势（NNIPs）是分子动力学（MD）模拟中的一种吸引人的替代方法。然而，它们可能产生不稳定的模拟，采样非物理状态，从而限制了其在对模拟长时间尺度现象建模中的实用性。为解决这些挑战，我们提出了稳定性感知Boltzmann估计器（StABlE）训练，这是一种多模式训练过程，结合了传统监督训练和参考系统可观察量，以产生稳定且准确的NNIPs。StABlE训练通过迭代运行MD模拟以寻找不稳定区域，并通过与参考可观察量的监督来纠正这些不稳定性。该训练过程由Boltzmann估计器支持，该估计器允许对系统可观察量训练神经网络所需的梯度进行高效计算，并能检测全局和局部

    arXiv:2402.13984v1 Announce Type: new  Abstract: Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local 
    
[^11]: 深度学习在气候应用中的架构选择的重要性

    The Importance of Architecture Choice in Deep Learning for Climate Applications

    [https://arxiv.org/abs/2402.13979](https://arxiv.org/abs/2402.13979)

    本文研究了深度学习在气候科学应用中架构选择的重要性，并展示了神经网络可以在多样化的气候情景下可预测大西洋经向翻转环流（AMOC），进一步揭示MLP和深度集成可以学习AMOC的物理过程而非模拟其进展。

    

    机器学习已经成为气候科学应用中普遍使用的工具。然而，目前的模型未能解决由人为改变温室气体排放引起的非平稳性，并且不会定期量化所提出的预测的不确定性。在本文中，我们对大西洋经向翻转环流（AMOC）进行建模，通过将温暖水输送到欧洲和美国东海岸，对这些地区的气候至关重要，并且有潜在的突然崩溃风险。我们可以生成通过任意时间尺度的任意极端气候场景，然后利用神经网络进行预测。我们的分析显示，在多样化的气候情景下，AMOC可以通过神经网络进行可预测。进一步的实验显示，MLP和深度集成可以学习AMOC的物理过程，而不是通过自相关模拟其进展。通过量化不确定性，发现了一个有趣的模式。

    arXiv:2402.13979v1 Announce Type: cross  Abstract: Machine Learning has become a pervasive tool in climate science applications. However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections. In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse. We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks. Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios. Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation. With quantified uncertainty, an intriguing pattern 
    
[^12]: 可扩展推荐系统的线性时间图神经网络

    Linear-Time Graph Neural Networks for Scalable Recommendations

    [https://arxiv.org/abs/2402.13973](https://arxiv.org/abs/2402.13973)

    本文提出了一种线性时间图神经网络（LTGNN），用于扩展图神经网络在推荐系统中的可扩展性。

    

    在信息爆炸的时代，推荐系统是为用户提供个性化推荐的重要工具。推荐系统的关键在于根据先前的用户-物品互动来预测用户的未来行为。近年来，由于其强大的高阶连接性捕捉能力，人们对利用图神经网络（GNNs）来提升推荐系统预测性能的兴趣日益增加。然而，经典的矩阵分解（MF）和深度神经网络（DNN）方法由于其可扩展性优势，仍在实际的大规模推荐系统中扮演着重要角色。尽管存在GNN加速解决方案，但GNN-based推荐系统能否像经典的MF和DNN方法一样高效扩展仍是一个开放问题。本文提出了一种线性时间图神经网络（LTGNN）来扩展GN

    arXiv:2402.13973v1 Announce Type: cross  Abstract: In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GN
    
[^13]: 提高音频指纹识别准确性，解决背景噪音和失真挑战

    Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges

    [https://arxiv.org/abs/2402.13957](https://arxiv.org/abs/2402.13957)

    本研究提出了一种结合人工智能和机器学习的音频指纹识别算法，通过在具有多样背景噪音和失真的真实环境场景模拟中的工作，以提高准确性，实现100%准确性的5秒音频输入，系统匹配速度可预测，强调了实际实施中的关键空间-速度权衡。

    

    音频指纹识别，如 Shazam 等先驱所示，已经改变了数字音频识别。然而，现有系统在复杂环境下的准确性存在问题，限制了广泛应用。本研究提出了一种人工智能和机器学习集成的音频指纹识别算法以增强准确性。建立在 Dejavu 项目的基础上，本研究强调了在具有多样背景噪音和失真的真实环境场景模拟中的工作。信号处理，是 Dejavu 模型的核心，包括快速傅里叶变换、频谱图和峰值提取。"星座"概念和指纹哈希使得歌曲独特标识成为可能。性能评估证实，在5秒音频输入时可达到100%的准确性，系统展示出可预测的匹配速度以提高效率。存储分析凸显了实际实施中的关键空间-速度权衡。本研究推动了音频指纹识别的适应性。

    arXiv:2402.13957v1 Announce Type: cross  Abstract: Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The "constellation" concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting's adaptab
    
[^14]: AttackGNN: 使用强化学习在硬件安全中对GNN进行红队测试

    AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning

    [https://arxiv.org/abs/2402.13946](https://arxiv.org/abs/2402.13946)

    这项工作提出了AttackGNN，针对硬件安全中使用的基于GNN的技术进行了首次红队攻击，通过设计新颖的强化学习代理生成对抗性示例电路。

    

    机器学习在解决一些关键的硬件安全问题上表现出了极大的潜力。研究人员开发了基于图神经网络（GNN）的新颖技术，用于检测知识产权（IP）盗版、检测硬件特洛伊木马（HTs）和反向工程电路等问题。这些技术表现出色，受到了广泛关注。本文提出了AttackGNN，这是针对硬件安全中基于GNN的技术的第一个红队攻击。为此，我们设计了一种新颖的强化学习（RL）代理，用于生成针对GNN技术的对抗示例，即电路。

    arXiv:2402.13946v1 Announce Type: new  Abstract: Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scal
    
[^15]: 用于建模科学机器学习中Aleatoric不确定性的概率神经网络（PNNs）

    Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning

    [https://arxiv.org/abs/2402.13945](https://arxiv.org/abs/2402.13945)

    本文探讨了使用概率神经网络（PNNs）来建模Aleatoric不确定性，通过开发概率距离度量来优化PNN架构，证实了PNNs在模拟Aleatoric不确定性中的有效性。

    

    本文探讨了使用概率神经网络（PNNs）来建模Aleatoric不确定性，该不确定性是指系统输入输出关系中固有的变异性，通常表现为不均等的方差或异方差性。不同于产生确定性输出的传统神经网络，PNNs为目标变量生成概率分布，允许在回归场景中确定预测均值和区间。本文的贡献包括开发概率距离度量来优化PNN架构，以及在受控数据集和涉及纤维增强复合材料的实际材料科学案例中部署PNNs。研究结果证实，PNNs有效地模拟了Aleatoric不确定性，证明在这一目的上，它比通常采用的高斯过程回归更为合适。具体来说，在一个真实的科学环境中

    arXiv:2402.13945v1 Announce Type: cross  Abstract: This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity. Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios. Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites. The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose. Specifically, in a real-world scientific
    
[^16]: 通过基于拓扑的界限紧缩验证消息传递神经网络

    Verifying message-passing neural networks via topology-based bounds tightening

    [https://arxiv.org/abs/2402.13937](https://arxiv.org/abs/2402.13937)

    通过基于拓扑的界限紧缩，我们提出了一种强大的证书生成方法，用于验证消息传递神经网络，支持添加和删除边、全局和局部预算的设置，以及拓扑扰动和特征修改，展示了其在优化约束动态调整方面的有效性。

    

    由于图神经网络（GNNs）经常容易遭受攻击，我们需要知道何时可以信任它们。我们发展了一种计算有效的方法，通过使用修正线性单元（ReLU）激活函数为消息传递神经网络（MPNNs）提供强大的证书。我们的工作建立在混合整数优化之上，编码了多种子问题，例如允许添加和删除边，全局和局部预算，以及拓扑扰动和特征修改。我们的关键技术，基于拓扑的界限紧缩，利用图结构来收紧界限。我们还尝试使用积极的界限紧缩来动态改变优化约束，即通过收紧变量界限。为了证明这些策略的效果，我们扩展了开源的分支定界求解器SCIP。我们在节点和图分类上进行了测试。

    arXiv:2402.13937v1 Announce Type: cross  Abstract: Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and graph classifi
    
[^17]: 确实高效的Transformer能够节约计算吗？

    Do Efficient Transformers Really Save Computation?

    [https://arxiv.org/abs/2402.13934](https://arxiv.org/abs/2402.13934)

    本研究旨在理解高效Transformer（例如稀疏Transformer和线性Transformer）的能力和限制，发现它们适合解决一般DP任务，但不同于标准Transformer。

    

    随着基于Transformer的语言模型在越来越大的数据集上训练，并拥有大量参数，找到更高效的替代标准Transformer变得非常有价值。虽然已经提出了许多高效的Transformer和Transformer的替代方案，但没有一个能够提供它们适合替代标准Transformer的理论保证。这使得很难确定何时使用特定模型以及进一步研究的重点。在本文中，我们旨在理解高效Transformer的能力和局限性，特别是稀疏Transformer和线性Transformer。我们专注于它们在Chain-of-Thought (CoT)提示中展示的推理能力，并遵循先前的研究将它们建模为动态规划（DP）问题。我们的结果表明，虽然这些模型足够表达解决一般DP任务的能力，但与标准Transformer不同

    arXiv:2402.13934v1 Announce Type: cross  Abstract: As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to ex
    
[^18]: 使用本地指南增强强化学习代理

    Enhancing Reinforcement Learning Agents with Local Guides

    [https://arxiv.org/abs/2402.13930](https://arxiv.org/abs/2402.13930)

    本文提出了一种添加本地指南策略到强化学习代理中的新算法，通过噪声策略切换程序和近似策略评估来引导本地指南，提高了在强化学习算法的性能，尤其在学习初期。

    

    本文解决了将本地指南策略整合进强化学习代理的问题。我们展示了如何调整现有算法以适应这种设置，然后介绍了一种基于噪声策略切换程序的新算法。该方法建立在适当的近似策略评估（APE）方案之上，通过精心引导本地指南朝着更好的行为方向进行扰动。我们在一组经典的强化学习问题上评估了我们的方法，包括对安全关键系统的评估，其中代理不能进入某些区域，以免触发灾难性后果。在所有提出的环境中，我们的代理都被证明在改进任何基于APE的强化学习算法的性能上是高效的，尤其是在其学习的最初阶段。

    arXiv:2402.13930v1 Announce Type: new  Abstract: This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.
    
[^19]: SDXL-Lightning: 渐进式对抗性扩散蒸馏

    SDXL-Lightning: Progressive Adversarial Diffusion Distillation

    [https://arxiv.org/abs/2402.13929](https://arxiv.org/abs/2402.13929)

    提出了一种结合渐进和对抗性蒸馏的扩散蒸馏方法，在文本到图像生成任务中取得了新的最先进结果，并开源了相应模型。

    

    我们提出了一种扩散蒸馏方法，在基于SDXL的一步/几步1024像素文本到图像生成任务中实现了全新的最先进水平。我们的方法结合了渐进和对抗性蒸馏，实现了质量和模式覆盖之间的平衡。本文讨论了理论分析、判别器设计、模型公式和训练技巧。我们以LoRA和完整UNet权重的形式开源了我们的蒸馏SDXL-Lightning模型。

    arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
    
[^20]: BenchCloudVision: 云检测和分割中深度学习方法的基准分析

    BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery

    [https://arxiv.org/abs/2402.13918](https://arxiv.org/abs/2402.13918)

    本文通过对遥感图像中的云进行分割，旨在提高卫星图像分析的精度和效率，应用于环境监测、资源管理和灾害响应。

    

    配备光学传感器的卫星捕获高分辨率图像，为各种环境现象提供了宝贵的见解。近年来，针对遥感中一些挑战的研究激增，从不同景观中的水检测到山地和地形的分割。正在进行的研究旨在提高卫星图像分析的精度和效率。尤其是，越来越多的重点放在开发准确的水体检测、雪和云的方法上，这些对环境监测、资源管理和灾害响应至关重要。在这个背景下，本文专注于从遥感图像中分割云。由于光学传感器应用中云的存在，准确的遥感数据分析可能具有挑战性。由此产生的产品（如应用和研究）的质量直接受到影响。

    arXiv:2402.13918v1 Announce Type: cross  Abstract: Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directl
    
[^21]: 利用SCADA数据和连续学习进行风力发电预测的偏差校正

    Bias correction of wind power forecasts with SCADA data and continuous learning

    [https://arxiv.org/abs/2402.13916](https://arxiv.org/abs/2402.13916)

    本研究利用机器学习方法对风力发电进行预测，通过校正数值天气预报模型提取的48小时预测数据，其中卷积神经网络取得了最佳的预测效果，将平均NRMSE降低至22%。

    

    风能在向可再生能源过渡中扮演着关键角色，然而，风力的不确定性和变化性可能会限制其全面潜力和风电容量的必要增长。为了减轻这些挑战，风力预测方法被应用于电力管理、能源交易或维护调度等应用中。本文提出、评估并比较了四种基于机器学习的风力预测模型。我们的模型对从数值天气预报（NWP）模型提取的48小时预测进行了校正和改进。模型在包含65台风力涡轮机的风电场数据集上进行了评估。卷积神经网络取得了最佳的预测误差和平均偏差改进，将平均NRMSE降低到22%，与未校正NWP使用的强偏差基线模型的35%的NRMSE相比，偏差显著降低。

    arXiv:2402.13916v1 Announce Type: new  Abstract: Wind energy plays a critical role in the transition towards renewable energy sources. However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity. To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling. In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models. Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model. The models are evaluated on datasets from a wind park comprising 65 wind turbines. The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP 
    
[^22]: 不是为了辩解而是为了解释

    Explain to Question not to Justify

    [https://arxiv.org/abs/2402.13914](https://arxiv.org/abs/2402.13914)

    XAI领域被划分为蓝色XAI和红色XAI两种解释文化，指出了红色XAI领域的重要性和研究潜力，并提出了未来的研究挑战。

    

    可解释人工智能（XAI）是一个年轻但非常有前途的研究领域。不幸的是，该领域目前的进展受到了不同和不兼容目标的限制。在本文中，我们将XAI领域内纠缠在一起的各种线索分为两种互补的文化，即人类/价值取向解释（蓝色XAI）和模型/验证取向解释（红色XAI）。我们还认为，红色XAI领域目前未被充分探索，隐藏着巨大的机遇和重要研究的潜力，以确保AI系统的安全。我们通过提出这一领域的有前途的挑战来总结本文。

    arXiv:2402.13914v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.
    
[^23]: 通过物理引导的机器学习增强水文建模

    Replication Study: Enhancing Hydrological Modeling with Physics-Guided Machine Learning

    [https://arxiv.org/abs/2402.13911](https://arxiv.org/abs/2402.13911)

    本研究引入了一种名为物理信息引导的机器学习（PIML）模型，将概念性水文模型的过程理解与机器学习算法的预测效率结合，在水文建模中取得了优异的性能表现。

    

    当前的水文建模方法将数据驱动的机器学习算法和传统的基于物理的模型相结合，以解决各自的局限性，即刚性基于物理的模型产生的参数估计不准确，以及机器学习算法忽略物理过程约束的问题。尽管机器学习在结果预测方面很准确，但融合科学知识对于可靠的预测至关重要。本研究引入了一种物理信息引导的机器学习（PIML）模型，将概念性水文模型的过程理解与机器学习算法的预测效率相结合。该PIML模型应用于Anandapur分水岭，在预测月均流量和实际蒸腾蒸发时展示出卓越的性能，优于独立的概念模型和机器学习算法，确保输出的物理一致性。本研究复制了Bhasme，P.，Vagadiya，J.和Bhatia的方法。

    arXiv:2402.13911v1 Announce Type: new  Abstract: Current hydrological modeling methods combine data-driven Machine Learning (ML) algorithms and traditional physics-based models to address their respective limitations incorrect parameter estimates from rigid physics-based models and the neglect of physical process constraints by ML algorithms. Despite the accuracy of ML in outcome prediction, the integration of scientific knowledge is crucial for reliable predictions. This study introduces a Physics Informed Machine Learning (PIML) model, which merges the process understanding of conceptual hydrological models with the predictive efficiency of ML algorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates superior performance in forecasting monthly streamflow and actual evapotranspiration over both standalone conceptual models and ML algorithms, ensuring physical consistency of the outputs. This study replicates the methodologies of Bhasme, P., Vagadiya, J., & Bhatia
    
[^24]: 处理随机鞍点优化中的无界梯度

    Dealing with unbounded gradients in stochastic saddle-point optimization

    [https://arxiv.org/abs/2402.13903](https://arxiv.org/abs/2402.13903)

    提出一种简单而有效的正则化技术，稳定了随机鞍点优化过程中的梯度不断增长的问题，能够在无界梯度和噪声的情况下提供有意义的性能保证

    

    我们研究了用于寻找凸凹函数鞍点的随机一阶方法的性能。这类方法面临的一个举世闻名的挑战是，在优化过程中梯度可能会任意增长，这可能导致不稳定性和发散。在本文中，我们提出了一种简单而有效的正则化技术，稳定了迭代并产生了有意义的性能保证，即使定义域和梯度噪声随迭代的规模线性变化（因此可能是无界的）。除了提供一系列一般性结果外，我们还将我们的算法应用到强化学习中的一个具体问题，该问题导致在不需要有关偏置跨度先验知识的情况下，找到平均奖励MDP中接近最优策略的性能保证。

    arXiv:2402.13903v1 Announce Type: new  Abstract: We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.
    
[^25]: 离散时间扩散模型的非渐近收敛：新方法和改进速率

    Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate

    [https://arxiv.org/abs/2402.13901](https://arxiv.org/abs/2402.13901)

    本文提出了离散时间扩散模型的新方法，改进了对更大类的分布的收敛保证，并提高了具有有界支撑的分布的收敛速率。

    

    最近，去噪扩散模型作为一种强大的生成技术出现，将噪声转化为数据。理论上主要研究了连续时间扩散模型的收敛性保证，并且仅在文献中对具有有界支撑的分布的离散时间扩散模型进行了获得。本文为更大类的分布建立了离散时间扩散模型的收敛性保证，并进一步改进了对具有有界支撑的分布的收敛速率。特别地，首先为具有有限二阶矩的平滑和一般（可能非光滑）分布建立了收敛速率。然后将结果专门应用于一些有明确参数依赖关系的有趣分布类别，包括具有Lipschitz分数、高斯混合分布和具有有界支撑的分布。

    arXiv:2402.13901v1 Announce Type: new  Abstract: The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further 
    
[^26]: 科学检查者再度升级：透明度和逻辑推理的双向范式

    Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning

    [https://arxiv.org/abs/2402.13897](https://arxiv.org/abs/2402.13897)

    提出了一个两块式的方法来解决长文档中信息检索领域的挑战，并实现了双向交互

    

    信息检索是一个快速发展的领域。然而，它仍然面临着在科学和工业的海量信息中的诸多限制，比如语义分歧和检索中的词汇差距、语义搜索中的低精度和缺乏可解释性，或者生成模型中的幻觉和过时信息。在本文中，我们提出了一个两块式的方法来解决长文档的这些障碍。第一个模块通过查询扩展增强了在稀疏检索中的语言理解，以检索相关文档。第二个模块通过只使用长文档中传播的信息，为复杂问题提供全面和信息丰富的答案来加深结果，实现双向交互。在管道的各个阶段，向用户呈现中间结果以促进对系统推理的理解。我们相信这种双向方法带来了

    arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
    
[^27]: 克服迭代正则化中密度比估计的饱和问题

    Overcoming Saturation in Density Ratio Estimation by Iterated Regularization

    [https://arxiv.org/abs/2402.13891](https://arxiv.org/abs/2402.13891)

    引入迭代正则化方法解决了密度比估计中的饱和问题，实现了快速收敛，在密度比估计基准测试和大规模深度无监督领域自适应模型的重要性加权集成中表现优异。

    

    从有限样本中估计两个概率密度的比率，是机器学习和统计学中的一个核心任务。在这项工作中，我们发现一大类密度比估计的核方法存在错误饱和问题，这阻碍了算法在高度规则学习问题上实现快速错误收敛率。为了解决饱和问题，我们引入了迭代正则化方法在密度比估计中以实现快速错误率。我们的方法在密度比估计基准测试以及大规模评估深度无监督领域自适应模型的重要性加权集成方面表现优异。

    arXiv:2402.13891v1 Announce Type: new  Abstract: Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.
    
[^28]: 一种可解释的基于Transformer的钓鱼邮件检测模型：基于大型语言模型的方法

    An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach

    [https://arxiv.org/abs/2402.13871](https://arxiv.org/abs/2402.13871)

    优化的Transformer模型DistilBERT用于检测钓鱼邮件，通过预处理技术解决了类别不平衡问题

    

    钓鱼邮件是一种严重的网络威胁，试图通过发送虚假邮件来欺骗用户，意图是窃取机密信息或造成财务损失。攻击者常常冒充可信实体，利用技术进步和复杂性使得钓鱼的检测和预防更具挑战性。尽管进行了大量学术研究，但钓鱼邮件检测在网络安全领域仍然是一个持续且严峻的挑战。大型语言模型（LLMs）和掩盖语言模型（MLMs）拥有巨大潜力，能够提供创新解决方案来解决长期存在的挑战。在本研究论文中，我们提出了一个经过优化的、经过微调的基于Transformer的DistilBERT模型，用于检测钓鱼邮件。在检测过程中，我们使用了一组钓鱼邮件数据集，并利用预处理技术来清理和解决类别不平衡问题。通过我们的实验，

    arXiv:2402.13871v1 Announce Type: cross  Abstract: Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, 
    
[^29]: 生成式概率时间序列预测及在电网运营中的应用

    Generative Probabilistic Time Series Forecasting and Applications in Grid Operations

    [https://arxiv.org/abs/2402.13870](https://arxiv.org/abs/2402.13870)

    提出了一种弱创新自动编码器结构和学习算法，用于从非参数稳态时间序列中提取独立同分布的创新序列，适用于生成式概率时间序列预测。

    

    生成式概率预测根据过去时间序列观测值给出条件概率分布，并产生未来时间序列样本。这种技术在基于风险的决策和不确定性规划中至关重要，在电网运营中有广泛应用，包括电价预测、基于风险的经济调度和随机优化。受Wiener和Kallianpur的创新表达启发，我们提出了一个弱创新自动编码器结构和一个学习算法，从非参数稳态时间序列中提取独立同分布的创新序列。我们证明了弱创新序列是贝叶斯充分的，这使得所提出的弱创新自动编码器成为生成式概率预测的标准体系结构。该技术被应用于预测高度波动的实时电价，取得了良好效果。

    arXiv:2402.13870v1 Announce Type: new  Abstract: Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating
    
[^30]: RFI-DRUnet: 恢复被射电干扰污染的动态谱 -- 应用于脉冲星观测

    RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency interference -- Application to pulsar observations

    [https://arxiv.org/abs/2402.13867](https://arxiv.org/abs/2402.13867)

    提出一种以联合检测和恢复形式处理射电频率干扰（RFI）的方法，利用深度卷积网络来识别和恢复受到RFI影响的动态谱部分

    

    射电频率干扰（RFI）一直是射电天文学中的一个长期关注问题，尤其是对需要高时序精度和数据灵敏度的脉冲星观测而言。在大多数文献中，RFI抑制被构建为一个检测任务，其中包括对动态谱中可能出现的RFI进行定位。该策略不可避免地导致信息的潜在丢失，因为在随后的数据处理流程中通常不考虑被识别为可能受RFI污染的信号部分。相反，这项工作提出将RFI抑制作为一个联合检测和恢复过程，允许对RFI影响的动态谱部分进行识别和恢复。所提出的监督方法依赖于一个深度卷积网络，其架构继承了一个最近流行的图像去噪网络达到的性能。为了训练这个网络

    arXiv:2402.13867v1 Announce Type: cross  Abstract: Radio frequency interference (RFI) have been an enduring concern in radio astronomy, particularly for the observations of pulsars which require high timing precision and data sensitivity. In most works of the literature, RFI mitigation has been formulated as a detection task that consists of localizing possible RFI in dynamic spectra. This strategy inevitably leads to a potential loss of information since parts of the signal identified as possibly RFI-corrupted are generally not considered in the subsequent data processing pipeline. Conversely, this work proposes to tackle RFI mitigation as a joint detection and restoration that allows parts of the dynamic spectrum affected by RFI to be not only identified but also recovered. The proposed supervised method relies on a deep convolutional network whose architecture inherits the performance reached by a recent yet popular image-denoising network. To train this network, a whole simulation 
    
[^31]: 利用不确定性传播改进隐式神经表示的等值面提取效率

    Improving Efficiency of Iso-Surface Extraction on Implicit Neural Representations Using Uncertainty Propagation

    [https://arxiv.org/abs/2402.13861](https://arxiv.org/abs/2402.13861)

    通过重新审视算术规则并分析网络输出的概率分布，本文提出了一种改进的范围分析技术，以提高隐式神经表示中等值面提取的效率。

    

    隐式神经表示（INRs）广泛用于科学数据降维和可视化，通过建模将空间位置映射到数据值的函数。在没有关于值的空间分布的任何先验知识的情况下，我们被迫从INRs中密集采样来执行诸如等值面提取之类的可视化任务，这可能非常耗费计算资源。最近的范围分析已经显示，通过使用算术规则将网络输出范围限制在空间区域内，可以明显提高在INRs上进行几何查询（如射线投射和层次网格提取）的效率，用于3D几何图形。然而，对于复杂的科学数据，分析边界往往过于保守。本文通过重新审视算术规则并分析网络输出在空间区域内的概率分布，提出了对范围分析的改进技术。

    arXiv:2402.13861v1 Announce Type: cross  Abstract: Implicit Neural representations (INRs) are widely used for scientific data reduction and visualization by modeling the function that maps a spatial location to a data value. Without any prior knowledge about the spatial distribution of values, we are forced to sample densely from INRs to perform visualization tasks like iso-surface extraction which can be very computationally expensive. Recently, range analysis has shown promising results in improving the efficiency of geometric queries, such as ray casting and hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic rules to bound the output range of the network within a spatial region. However, the analysis bounds are often too conservative for complex scientific data. In this paper, we present an improved technique for range analysis by revisiting the arithmetic rules and analyzing the probability distribution of the network output within a spatial region. We mode
    
[^32]: 可复制学习大间距半空间

    Replicable Learning of Large-Margin Halfspaces

    [https://arxiv.org/abs/2402.13857](https://arxiv.org/abs/2402.13857)

    该论文提出了解决学习大间距半空间问题的可复制算法，相比之前的算法，在维度无关、时间复杂度优化、样本复杂度方面等多个关键参数上均有显著改进。

    

    我们提供了有效的可复制算法来解决学习大间距半空间的问题。我们的结果改进了Impagliazzo, Lei, Pitassi和Sorrell在STOC, 2022中提供的算法。我们设计了这个任务的首个与维度无关的可复制算法，其运行时间为多项式，是正确的，并且在所有相关参数方面的样本复杂度都严格比Impagliazzo等人在2022年实现的算法要好。此外，我们的第一个算法在精度参数$\epsilon$方面具有样本复杂度。我们还设计了一个基于SGD的可复制算法，在某些参数范围内，其样本复杂度和时间复杂度优于我们的第一个算法。

    arXiv:2402.13857v1 Announce Type: new  Abstract: We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to o
    
[^33]: 连续葡萄糖监测和维护的神经控制系统

    Neural Control System for Continuous Glucose Monitoring and Maintenance

    [https://arxiv.org/abs/2402.13852](https://arxiv.org/abs/2402.13852)

    引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，实时动态调整胰岛素输送，增强葡萄糖优化，最大化效率并确保个性化护理。

    

    精确的葡萄糖水平管理对于糖尿病患者至关重要，可以避免严重并发症。本研究引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，利用微分预测控制。我们的系统受到复杂神经策略和可区分建模的指导，实时动态调整胰岛素输送，增强葡萄糖优化。这种端到端方法最大化效率，确保个性化护理和改善健康结果，如经验发现所证实。

    arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
    
[^34]: 大型语言模型是先进的匿名化工具

    Large Language Models are Advanced Anonymizers

    [https://arxiv.org/abs/2402.13846](https://arxiv.org/abs/2402.13846)

    大型语言模型在保护个人数据方面取得了重要进展，提出了一种基于对抗性LLM推断的匿名化框架。

    

    最近在隐私研究领域对大型语言模型的研究表明，它们在推断真实世界在线文本中的个人数据方面表现出接近人类水平的性能。随着模型能力的不断增强，现有的文本匿名化方法当前已经落后于监管要求和对抗威胁。这引出了一个问题：个人如何有效地保护他们在分享在线文本时的个人数据。在这项工作中，我们采取了两步来回答这个问题：首先，我们提出了一个新的设置，用于评估面对对抗性LLM的推断时的匿名化效果，从而允许自然地测量匿名化性能，同时纠正了以前指标的一些缺陷。然后，我们提出了基于LLM的对抗性匿名化框架，利用LLM的强大推断能力来指导我们的匿名化过程。在我们的实验评估中，我们展示了在真实世界中的匿名化实践。

    arXiv:2402.13846v1 Announce Type: cross  Abstract: Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world 
    
[^35]: MLXP：一个用于在Python中进行可复制的机器学习实验的框架

    MLXP: A framework for conducting replicable Machine Learning eXperiments in Python

    [https://arxiv.org/abs/2402.13831](https://arxiv.org/abs/2402.13831)

    MLXP是一个简单轻量的基于Python的实验管理工具，旨在解决机器学习研究中复制性的挑战

    

    机器学习（ML）研究中的可复制性越来越受关注，因为使用了复杂的非确定性算法，并依赖于众多超参数选择，如模型架构和训练数据集。确保可重现和可复制的结果对于推进该领域至关重要，但往往需要进行系统化和组织良好的实验，从而得出稳健的结论，却需要投入大量技术工作。为了解决低采纳率的挑战，我们提出MLXP，一个基于Python的开源、简单、轻量级实验管理工具。

    arXiv:2402.13831v1 Announce Type: new  Abstract: Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets. Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions. Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings. To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the experimental process with minimal p
    
[^36]: Lipschitz可配置马尔可夫决策过程的性能改进界限

    Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes

    [https://arxiv.org/abs/2402.13821](https://arxiv.org/abs/2402.13821)

    本文提出了对于满足Lipschitz连续性条件的Conf-MDP的性能改进界限，并推导了一个新颖的性能改进下界

    

    可配置马尔可夫决策过程（Conf-MDPs）最近被引入作为传统马尔可夫决策过程（MDPs）的一个扩展，用于模拟在真实世界场景中有可能干预环境以配置一些参数的情况。本文侧重于满足正则性条件的Conf-MDP的一个特定子类，即Lipschitz连续性。我们首先提供了由改变策略和配置引发的$\gamma$-折扣稳态分布之间的Wasserstein距离的界限。该结果推广了已经存在的Conf-MDP和传统MDP的界限。然后，我们推导了一个新颖的性能改进下界。

    arXiv:2402.13821v1 Announce Type: new  Abstract: Configurable Markov Decision Processes (Conf-MDPs) have recently been introduced as an extension of the traditional Markov Decision Processes (MDPs) to model the real-world scenarios in which there is the possibility to intervene in the environment in order to configure some of its parameters. In this paper, we focus on a particular subclass of Conf-MDP that satisfies regularity conditions, namely Lipschitz continuity. We start by providing a bound on the Wasserstein distance between $\gamma$-discounted stationary distributions induced by changing policy and configuration. This result generalizes the already existing bounds both for Conf-MDPs and traditional MDPs. Then, we derive a novel performance improvement lower bound.
    
[^37]: FLD：傅立叶潜动力学用于结构化运动表示和学习

    FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning

    [https://arxiv.org/abs/2402.13820](https://arxiv.org/abs/2402.13820)

    介绍了一种自监督的、结构化的表示和生成方法，通过傅立叶潜动力学提高了运动学习算法的插值和泛化能力。

    

    运动轨迹为基于物理的运动学习提供可靠参考，但在缺乏足够数据覆盖的区域，存在稀疏性问题。为了解决这一挑战，我们引入了一种自监督的结构化表示和生成方法，提取周期性或准周期性运动的时空关系。在连续参数化的潜空间中的运动动力学使我们的方法能够增强运动学习算法的插值和泛化能力。受运动参数化启发的运动学习控制器可以在线跟踪各种运动，包括训练时未见过的目标。通过一个回退机制，控制器可以动态调整其跟踪策略，并在提出潜在危险目标时自动采取安全行动执行。通过利用识别的时空结构，我们的工作开启

    arXiv:2402.13820v1 Announce Type: cross  Abstract: Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work open
    
[^38]: 基于声音的机器学习方法增强的住院心力衰竭患者死亡率预测

    Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers

    [https://arxiv.org/abs/2402.13812](https://arxiv.org/abs/2402.13812)

    该研究展示了一种强大有效的机器学习模型，利用声音生物标志物来预测住院心力衰竭患者的死亡率。

    

    应对心力衰竭作为一种普遍存在的全球健康问题，实施创新方法以增强患者护理存在困难。特别是在心力衰竭患者中预测死亡率既困难又关键，需要个性化护理、积极管理，并支持知情决策以增强结果。最近，声音生物标志物与机器学习（ML）相结合的重要性日益突出，特别在预测心力衰竭方面表现出显著有效性。声音分析与ML算法的协同作用提供了一种非侵入式且易于获取的评估患者健康状况的方法。然而，针对符合标准化语音协议的心力衰竭患者预测死亡率缺乏声音生物标志物。在这里，我们展示了一个强大有效的ML模型，通过利用声音生物标志物来预测住院心力衰竭患者的死亡率。

    arXiv:2402.13812v1 Announce Type: new  Abstract: Addressing heart failure (HF) as a prevalent global health concern poses difficulties in implementing innovative approaches for enhanced patient care. Predicting mortality rates in HF patients, in particular, is difficult yet critical, necessitating individualized care, proactive management, and enabling educated decision-making to enhance outcomes. Recently, the significance of voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating remarkable efficacy, particularly in predicting heart failure. The synergy of voice analysis and ML algorithms provides a non-invasive and easily accessible means to evaluate patients' health. However, there is a lack of voice biomarkers for predicting mortality rates among heart failure patients with standardized speech protocols. Here, we demonstrate a powerful and effective ML model for predicting mortality rates in hospitalized HF patients through the utilization of voice biomarkers
    
[^39]: 通过预条件Langevin 动力学的预期损失揭示Hessian秩

    The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank

    [https://arxiv.org/abs/2402.13810](https://arxiv.org/abs/2402.13810)

    预条件Langevin动力学的预期损失与目标函数的Hessian秩成正比，并且在神经网络中具有应用前景。

    

    Langevin 动力学（LD）被广泛用于从分布中抽样和优化。在这项工作中，我们推导出了预条件LD在目标函数的稳定点附近的预期损失的闭式表达式。我们利用了在这些点附近，LD会退化为一种适合便利数学处理的Ornstein-Uhlenbeck过程的事实。我们的分析表明，当预条件矩阵满足与噪声协方差的特定关系时，LD的预期损失将成正比于目标Hessian的秩。我们展示了这个结果在神经网络的背景下的适用性，其中Hessian秩已被证明能够捕捉预测函数的复杂性，但通常难以计算。最后，我们利用我们的分析比较了类似于SGD和类似于Adam的预处理器，并确定了它们各自导致的情况下的区域。

    arXiv:2402.13810v1 Announce Type: new  Abstract: Langevin dynamics (LD) is widely used for sampling from distributions and for optimization. In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function. We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment. Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian. We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe. Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to
    
[^40]: 打开黑匣子：遥感中可解释人工智能的系统性评估

    Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing

    [https://arxiv.org/abs/2402.13791](https://arxiv.org/abs/2402.13791)

    本研究通过系统性评估揭示了遥感领域中可解释人工智能的使用趋势，探讨了新型方法和面临的挑战，为解决特定遥感难题提供了新思路。

    

    近年来，黑匣子机器学习方法已成为遥感领域知识提取的主导建模范式。尽管利用可解释人工智能揭示这些模型内部运行机制的潜在好处，但迄今仍缺乏一份全面概述，总结在遥感应用中使用的可解释人工智能方法及其目标、发现和挑战。本文通过进行系统性评估来解决这一问题，以识别可解释AI在遥感中的使用主要趋势，揭示解决特定遥感挑战的新型可解释AI方法和新兴方向。我们还揭示了解释解释的常见模式，讨论了遥感中提取的科学见解，并反思了用于可解释AI方法评估的方法。我们的评估提供了该领域现有技术的完整总结。

    arXiv:2402.13791v1 Announce Type: new  Abstract: In recent years, black-box machine learning approaches have become a dominant modeling paradigm for knowledge extraction in Remote Sensing. Despite the potential benefits of uncovering the inner workings of these models with explainable AI, a comprehensive overview summarizing the used explainable AI methods and their objectives, findings, and challenges in Remote Sensing applications is still missing. In this paper, we address this issue by performing a systematic review to identify the key trends of how explainable AI is used in Remote Sensing and shed light on novel explainable AI approaches and emerging directions that tackle specific Remote Sensing challenges. We also reveal the common patterns of explanation interpretation, discuss the extracted scientific insights in Remote Sensing, and reflect on the approaches used for explainable AI methods evaluation. Our review provides a complete summary of the state-of-the-art in the field.
    
[^41]: 保持接近最优梯度稀疏化成本的可扩展分布式深度学习

    Preserving Near-Optimal Gradient Sparsification Cost for Scalable Distributed Deep Learning

    [https://arxiv.org/abs/2402.13781](https://arxiv.org/abs/2402.13781)

    提出了一种名为ExDyna的新型梯度稀疏化方案，通过细粒度块和分区组合来减少通信开销。

    

    通信开销是扩展分布式训练系统的一大障碍。梯度稀疏化是一种潜在的优化方法，可以减少通信量而不显著损失模型的保真度。然而，现有的梯度稀疏化方法由于其算法设计低效，从而导致可扩展性差，显著提高了通信开销。为了解决这些挑战，我们提出了一种名为ExDyna的新型梯度稀疏化方案，具有细粒度块和组成非重叠分区的连续块。

    arXiv:2402.13781v1 Announce Type: new  Abstract: Communication overhead is a major obstacle to scaling distributed training systems. Gradient sparsification is a potential optimization approach to reduce the communication volume without significant loss of model fidelity. However, existing gradient sparsification methods have low scalability owing to inefficient design of their algorithms, which raises the communication overhead significantly. In particular, gradient build-up and inadequate sparsity control methods degrade the sparsification performance considerably. Moreover, communication traffic increases drastically owing to workload imbalance of gradient selection between workers.   To address these challenges, we propose a novel gradient sparsification scheme called ExDyna. In ExDyna, the gradient tensor of the model comprises fined-grained blocks, and contiguous blocks are grouped into non-overlapping partitions. Each worker selects gradients in its exclusively allocated partiti
    
[^42]: 从化学反应知识中学习上下文分子表示

    Contextual Molecule Representation Learning from Chemical Reaction Knowledge

    [https://arxiv.org/abs/2402.13779](https://arxiv.org/abs/2402.13779)

    REMO是一个自监督学习框架，通过利用常见化学中明确定义的原子组合规则，在1.7百万个已知化学反应上预训练图形/Transformer编码器，并提出了Masked Reaction Centre Reconstruction (MRCR)和Reaction Centre Identification (RCI)两个预训练目标，为分子表示学习提供了新颖解决方案。

    

    近年来，自监督学习已经成为一种强大的工具，用于利用丰富的未标记数据进行表示学习，并已广泛应用于各个领域。然而，当应用于分子表示学习（MRL）时，流行的技术(如掩码亚单位重建)往往表现不佳，这是因为分子中可能的原子组合方式自由度较高，给掩码重建范式带来了难以逾越的复杂性。为了应对这一挑战，我们引入了REMO，这是一个自监督学习框架，利用常见化学中的明确定义的原子组合规则。具体来说，REMO在文献中已知的170万个化学反应上进行了图/变换器编码器的预训练。我们提出了两个预训练目标: 掩码反应中心重建（MRCR）和反应中心识别（RCI）。REMO通过利用新颖的解决方案在MRL方面做出了贡献。

    arXiv:2402.13779v1 Announce Type: cross  Abstract: In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploi
    
[^43]: 离线策略学习的深度生成模型：教程、调查和未来方向展望

    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions

    [https://arxiv.org/abs/2402.13777](https://arxiv.org/abs/2402.13777)

    深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。

    

    深度生成模型(DGMs)在各个领域展示了巨大成功，特别是在使用从离线数据训练的模型生成文本、图像和视频方面。类似地，基于数据驱动的决策和机器人控制也需要从离线数据中学习一个生成函数作为策略或政策。在这种情况下，将深度生成模型应用于离线策略学习展现出巨大潜力，许多研究在这个方向上进行了探索。然而，这一领域仍然缺乏全面的评估，因此不同分支的发展相对独立。因此，我们提供了深度生成模型在离线策略学习应用方面的第一次系统性综述。具体而言，我们涵盖了五种主流深度生成模型，包括变分自动编码器、生成对抗网络、归一化流、变压器和扩散模型，以及它们的应用。

    arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
    
[^44]: Cas-DiffCom: 婴儿纵向超分辨率3D医学图像补全的级联扩散模型

    Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion

    [https://arxiv.org/abs/2402.13776](https://arxiv.org/abs/2402.13776)

    本文提出了一个两阶段级联扩散模型Cas-DiffCom，用于解决婴儿纵向3D医学图像补全中存在的个体时间维度一致性不足的问题。

    

    早期婴儿期是行为和神经认知快速且动态发展的重要阶段。纵向磁共振成像（MRI）是一种有效的工具，可通过捕捉脑结构的发育轨迹来探究这个关键阶段。然而，由于受试者退出和扫描失败，纵向MRI采集总会遇到严重的数据缺失问题，这使得纵向婴儿脑图构建和发育轨迹描绘变得非常具有挑战性。感谢基于人工智能的生成模型的发展，神经影像补全已成为一种保留尽可能多可用数据的强大技术。然而，当前的图像补全方法通常在时间维度内的每个个体主体内存在不一致性，从而影响整体质量。为解决这一问题，我们的论文提出了一个两阶段级联扩散模型Cas-DiffCom，用于密集和纵向

    arXiv:2402.13776v1 Announce Type: cross  Abstract: Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures. However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging. Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible. However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality. To solve this problem, our paper proposed a two-stage cascaded diffusion model, Cas-DiffCom, for dense and longitudinal
    
[^45]: 通过概率单纯形上的统计建模实现保持精度的校准

    Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex

    [https://arxiv.org/abs/2402.13765](https://arxiv.org/abs/2402.13765)

    提出一种使用Concrete分布作为概率单纯形上的概率模型的保持精度的校准方法，并证明其在交叉熵损失上训练的DNN模型具有最优性，同时提出了一种有效的样本生成方法。

    

    基于深度神经网络（DNNs）的分类模型必须进行校准，以评估预测结果的可靠性。一些最近的校准方法采用了概率单纯形上的概率模型。然而，这些校准方法无法保持预训练模型的准确性，即使这些模型具有很高的分类准确性。我们提出了一种使用Concrete分布作为概率单纯形上的概率模型的保持精度的校准方法。我们在理论上证明，在交叉熵损失上训练的DNN模型具有Concrete分布参数的最优性。我们还提出了一种有效的方法，可以合成生成样本，用于在概率单纯形上训练概率模型。我们证明了所提出的方法在精度保持校准任务上可以优于以往的方法，使用基准测试。

    arXiv:2402.13765v1 Announce Type: new  Abstract: Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions. Some recent calibration methods have employed a probabilistic model on the probability simplex. However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex. We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution. We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex. We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks.
    
[^46]: 强化学习辅助的变分量子算法量子架构搜索

    Reinforcement learning-assisted quantum architecture search for variational quantum algorithms

    [https://arxiv.org/abs/2402.13754](https://arxiv.org/abs/2402.13754)

    通过强化学习自动搜索变分电路的最佳结构，改善了VQAs的性能。

    

    在嘈杂中等规模量子（NISQ）时代，一个重要障碍是确定功能性量子电路。这些电路必须同时符合当前量子硬件限制所施加的约束。变分量子算法（VQA）是一类量子-经典优化算法，旨在解决当前可用量子设备中的这些挑战。本论文侧重于电路结构，通过使用强化学习（RL）自动搜索变分电路的最优结构，改善了VQAs的性能。论文内通过评估电路的深度、门和参数的总数以及准确性来确定电路的优越性。

    arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
    
[^47]: AI-Powered Predictions for Electricity Load in Prosumer Communities

    AI-Powered Predictions for Electricity Load in Prosumer Communities

    [https://arxiv.org/abs/2402.13752](https://arxiv.org/abs/2402.13752)

    本文审查了如何利用人工智能技术对生产者兼消费者社区的电力负载进行预测的方法和可行性

    

    住宅楼宇社区的电力消耗和产出灵活性，包括具有可再生能源和能源储存设施（也称为生产者兼消费者），可以通过先进的短期需求响应机制得到有效利用。众所周知，如果在生产者兼消费者社区层面进行需求响应，灵活性就可以进一步提高，因为聚合的群体可以更好地协调电力消费。然而，这种短期优化的效果在很大程度上取决于对每栋建筑物以及整个社区的电力负载预测的准确性。电力负载曲线中的结构变化可能与不同的外部因素相关联，例如天气条件、日历信息、星期几以及用户行为。本文审查了一系列广泛的电力负载预测技术，

    arXiv:2402.13752v1 Announce Type: cross  Abstract: The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms. It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption. However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community. Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior. In this paper, we review a wide range of electricity load forecasting techniques, tha
    
[^48]: 在图神经网络中进行算法推理

    Reasoning Algorithmically in Graph Neural Networks

    [https://arxiv.org/abs/2402.13744](https://arxiv.org/abs/2402.13744)

    神经算法推理(NAR)是一个有希望的研究领域，旨在将算法的结构化和基于规则的推理与神经网络的自适应学习能力相结合。

    

    人工智能系统中具有先进推理能力的发展代表着一个长期而持久的研究问题。传统上，解决这一挑战的主要策略涉及采用符号化方法，即通过符号明确地表示知识并通过明确编程规则进行表示。然而，随着机器学习的出现，系统出现了从数据中自主学习的范式转变，需要最少的人类指导。考虑到这种转变，近年来，越来越多的人开始对赋予神经网络推理能力的研究感兴趣，并将数据驱动学习与逻辑推理紧密联系在一起。在这个背景下，神经算法推理（NAR）作为一个有希望的研究领域脱颖而出，旨在将算法的结构化和基于规则的推理与神经网络的自适应学习能力相结合。

    arXiv:2402.13744v1 Announce Type: new  Abstract: The development of artificial intelligence systems with advanced reasoning capabilities represents a persistent and long-standing research question. Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules. However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance. In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical reasoning. Within this context, Neural Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based reasoning of algorithms with the adaptive learning capabilities of neu
    
[^49]: 平均梯度外积作为深度神经坍塌机制的研究

    Average gradient outer product as a mechanism for deep neural collapse

    [https://arxiv.org/abs/2402.13728](https://arxiv.org/abs/2402.13728)

    本文通过提供证据表明，深度神经网络中的神经坍塌主要是通过平均梯度外积进行深度特征学习的，权重的奇异结构与AGOP高度相关，导致类内变异坍塌。

    

    Deep Neural Collapse (DNC)指的是深度神经网络(DNNs)最后几层数据表示的惊人刚性结构。尽管这种现象在各种情境中都得到了测量，但其出现只有部分被理解。本文提供了充分证据，表明DNC主要是通过平均梯度外积(AGOP)进行深度特征学习而发生的。相比于解释神经坍塌的特征不可知方法，如无约束特征模型，这一进展更进一步。我们继续提供证据表明，权重的右奇异向量和奇异值是DNN中类内变异坍塌的主要因素。正如最近的研究所示，这种奇异结构与AGOP的高度相关。然后我们在实验和理论上证明了AGOP在随机初始化的神经网络中引发神经坍塌。

    arXiv:2402.13728v1 Announce Type: new  Abstract: Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized ne
    
[^50]: 稀疏结构化霍普菲尔德网络

    Sparse and Structured Hopfield Networks

    [https://arxiv.org/abs/2402.13725](https://arxiv.org/abs/2402.13725)

    通过与Fenchel-Young损失建立链接，我们提出了一种新的稀疏霍普菲尔德网络框架，能够实现端到端可微分的稀疏转换，揭示了损失边界、稀疏性和精确存储之间的关联，同时通过SparseMAP转换将框架扩展到结构化霍普菲尔德网络。

    

    现代霍普菲尔德网络由于其与变压器中的注意力的联系，近年来备受关注。我们的论文通过与Fenchel-Young损失建立联系，为稀疏霍普菲尔德网络提供了一个统一的框架。结果是一类新的霍普菲尔德-Fenchel-Young能量，其更新规则是端到端可微分的稀疏转换。我们揭示了损失边界、稀疏性和精确存储的关联。我们进一步通过SparseMAP转换将这一框架扩展到结构化霍普菲尔德网络，该网络可以检索模式关联而不是单个模式。在多实例学习和文本理性化实验中展示了我们方法的实用性。

    arXiv:2402.13725v1 Announce Type: new  Abstract: Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers. Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach.
    
[^51]: 批大小对比自监督语音表示学习的影响

    The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning

    [https://arxiv.org/abs/2402.13723](https://arxiv.org/abs/2402.13723)

    通过研究批处理大小对预训练的影响，本研究表明较大的批次大小有助于更好的预训练模型，但存在稳定性下限和有效性上限，模型质量取决于训练过程中看到的语音数据量。

    

    在语音中，基础模型通常使用多个GPU进行训练，这隐含地导致了较大的有效批处理大小。本文研究了批处理大小对预训练的影响，无论是在训练过程中可以监视的统计信息方面，还是对下游微调任务性能的影响。通过使用从87.5秒到80分钟的语音不同批次大小，我们发现，对于相同迭代次数，较大的批次大小会导致更好的预训练模型。然而，稳定性存在下限，有效性存在上限。然后我们指出，预训练模型的质量主要取决于训练过程中看到的语音数据量，即批处理大小与迭代次数的乘积。所有结果均通过独立实现的wav2vec 2.0架构生成，该架构在很大程度上复现了原始作品的结果(arXiv:2006.11477)。

    arXiv:2402.13723v1 Announce Type: cross  Abstract: Foundation models in speech are often trained using many GPUs, which implicitly leads to large effective batch sizes. In this paper we study the effect of batch size on pre-training, both in terms of statistics that can be monitored during training, and in the effect on the performance of a downstream fine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes of speech we show that, for a fixed amount of iterations, larger batch sizes result in better pre-trained models. However, there is lower limit for stability, and an upper limit for effectiveness. We then show that the quality of the pre-trained model depends mainly on the amount of speech data seen during training, i.e., on the product of batch size and number of iterations. All results are produced with an independent implementation of the wav2vec 2.0 architecture, which to a large extent reproduces the results of the original work (arXiv:2006.11477). Our ext
    
[^52]: 生物信息学研究中大型语言模型的评估

    An Evaluation of Large Language Models in Bioinformatics Research

    [https://arxiv.org/abs/2402.13714](https://arxiv.org/abs/2402.13714)

    大型语言模型在生物信息学研究中展现出了潜力，可成功处理多项关键任务，但在复杂任务中仍存在局限性。

    

    大型语言模型（LLMs）如ChatGPT在各个研究领域引起了极大关注。它们在文本完成和生成方面的显著能力开创了一种新的用于解决问题的语言界面范式。然而，这些模型在生物信息学中的潜力和功效尚未完全探索。在这项工作中，我们研究了LLMs在各种关键生物信息学任务中的表现。这些任务包括潜在编码区域的识别，基因和蛋白质的命名实体提取，抗微生物和抗癌肽的检测，分子优化以及解决教育性生物信息学问题。我们的发现表明，在给定适当提示的情况下，像GPT变种这样的LLMs可以成功处理大多数这些任务。此外，我们对它们在复杂生物信息学任务背景下的限制进行了彻底的分析。在结论部分，

    arXiv:2402.13714v1 Announce Type: cross  Abstract: Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion
    
[^53]: DSLR：多样性增强和结构学习用于基于重播的图持续学习

    DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning

    [https://arxiv.org/abs/2402.13711](https://arxiv.org/abs/2402.13711)

    DSLR提出了一种基于覆盖范围的多样性方法，以解决基于重播的图持续学习中回放节点过于集中导致过拟合和灾难性遗忘的问题。

    

    我们研究了基于重播方法中回放缓冲区对图持续学习（GCL）方法的影响。现有的基于重播的GCL方法为每个类别选择最具代表性的节点并将它们存储在重播缓冲区中，以供在训练后续任务时使用。然而，我们发现，仅考虑每个回放节点的类别代表性会使回放节点集中在每个类别的中心周围，可能存在过拟合于位于那些区域的节点的风险，从而加剧灾难性遗忘。此外，由于基于重播方法严重依赖于少数回放节点来保留从先前任务中获得的知识，涉及在模型训练中具有不相关邻居的回放节点可能对模型性能产生显着的负面影响。在本文中，我们提出了一种名为DSLR的GCL模型，具体来说，我们设计了一种基于覆盖范围的多样性（CD）

    arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
    
[^54]: 在协作机器学习中稳健性和学习的冲突

    On the Conflict of Robustness and Learning in Collaborative Machine Learning

    [https://arxiv.org/abs/2402.13700](https://arxiv.org/abs/2402.13700)

    在协作机器学习中，研究人员正式规范了稳健聚合器的领域，并发现现有的稳健聚合器无法实现其目标，要么无法准确识别有针对性的恶意更新，要么方法成功率不够。

    

    协作机器学习（CML）允许参与者共同训练机器学习模型，同时保持他们的训练数据私密。在隐私是一个强烈要求的情况下，比如健康相关应用中，安全也是首要关注的问题。这意味着保护隐私的CML流程必须产生能够输出正确可靠决策的模型，甚至在可能不受信任参与者的情况下也是如此。为了解决这个问题，研究人员提出使用依赖于帮助过滤可能危及训练过程的恶意贡献的度量的“稳健聚合器”。在这项工作中，我们在文献中规范化了稳健聚合器的进展。我们的规范化能够表明现有的稳健聚合器无法实现其目标：无论是它们使用无法准确识别有针对性的恶意更新的基于距离的度量；还是提出的方法成功率不够。

    arXiv:2402.13700v1 Announce Type: new  Abstract: Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern. This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \emph{even in the presence of potentially untrusted participants}. In response to this issue, researchers propose to use \textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process. In this work, we formalize the landscape of robust aggregators in the literature. Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is i
    
[^55]: 可解释的量子点器件测量分类技术

    Explainable Classification Techniques for Quantum Dot Device Measurements

    [https://arxiv.org/abs/2402.13699](https://arxiv.org/abs/2402.13699)

    提出了一种基于合成数据的的可解释特征技术，利用可解释性提升机（EBMs）实现了在量子点调谐中较高的可解释性和准确性。

    

    在物理科学中，对图像数据的稳健特征表示需求增加：图像采集，在广义上指二维数据，现在在许多领域广泛应用，包括我们在此考虑的量子信息科学。虽然在这些情况下广泛使用传统图像特征，但它们的使用正在迅速被神经网络技术所取代，后者往往以牺牲可解释性为代价换取高准确性。为了弥合这种权衡，我们提出了一种基于合成数据的技术，可以产生可解释的特征。我们利用可解释性提升机（EBMs）展示，这种方法提供了卓越的可解释性，并且不会降低准确性。具体而言，我们展示了在量子点调谐的背景下，这种技术带来了实质性的益处，当前发展阶段需要人类干预。

    arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
    
[^56]: 使用一维卷积神经网络计算过境系外行星参数

    Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks

    [https://arxiv.org/abs/2402.13673](https://arxiv.org/abs/2402.13673)

    该研究使用一维卷积神经网络模型对过境系外行星进行参数表征，能够有效降低分析过境系外行星所需的时间和计算成本。

    

    凌日方法允许通过分析恒星光变曲线来检测和表征行星系统。卷积神经网络似乎提供了自动化这些分析的可行解决方案。在这项研究中，提出了两个使用模拟的光变曲线（其中注入了类似于过境的信号）的一维卷积神经网络模型。一个模型在完整的光变曲线上操作并估计轨道周期，另一个模型在折叠相位的光变曲线上操作并估计轨道的半长轴和行星与恒星半径比的平方。这两个模型都经过了来自具有确认行星的TESS光变曲线的真实数据测试，以确保它们能够处理真实数据。得到的结果表明，一维卷积神经网络能够从宿主恒星的去趋势化光变曲线中表征过境系外行星，并且还减少了所需时间和计算成本。

    arXiv:2402.13673v1 Announce Type: cross  Abstract: The transit method allows the detection and characterization of planetary systems by analyzing stellar light curves. Convolutional neural networks appear to offer a viable solution for automating these analyses. In this research, two 1D convolutional neural network models, which work with simulated light curves in which transit-like signals were injected, are presented. One model operates on complete light curves and estimates the orbital period, and the other one operates on phase-folded light curves and estimates the semimajor axis of the orbit and the square of the planet-to-star radius ratio. Both models were tested on real data from TESS light curves with confirmed planets to ensure that they are able to work with real data. The results obtained show that 1D CNNs are able to characterize transiting exoplanets from their host star's detrended light curve and, furthermore, reducing both the required time and computational costs comp
    
[^57]: 测量不确定性：关于物理和虚拟测量的不确定性关系

    Measurement Uncertainty: Relating the uncertainties of physical and virtual measurements

    [https://arxiv.org/abs/2402.13666](https://arxiv.org/abs/2402.13666)

    概率（机器学习）模型提供了预测不确定性，而在质量管理中物理检查的测量不确定性与概率模型的预测不确定性之间的关系仍未完全阐明

    

    在工业大规模生产的产品背景下，质量管理是基于从大批次中实物检查一小部分样本并推理出批次质量符合性。当将物理检查与机器学习模型的预测相结合时，预测的不确定性是已知的至关重要。否则，应用建立的质量管理概念是不合法的。确定性（机器学习）模型缺乏对其预测不确定性的量化，因此不合适。概率（机器学习）模型提供了预测不确定性以及预测。然而，在质量管理中应用时，物理检查的测量不确定性与概率模型的预测不确定性之间缺乏简明关系。在这里，我们展示了概率（机器学习）模型的预测不确定性

    arXiv:2402.13666v1 Announce Type: cross  Abstract: In the context of industrially mass-manufactured products, quality management is based on physically inspecting a small sample from a large batch and reasoning about the batch's quality conformance. When complementing physical inspections with predictions from machine learning models, it is crucial that the uncertainty of the prediction is known. Otherwise, the application of established quality management concepts is not legitimate. Deterministic (machine learning) models lack quantification of their predictive uncertainty and are therefore unsuitable. Probabilistic (machine learning) models provide a predictive uncertainty along with the prediction. However, a concise relationship is missing between the measurement uncertainty of physical inspections and the predictive uncertainty of probabilistic models in their application in quality management. Here, we show how the predictive uncertainty of probabilistic (machine learning) models
    
[^58]: 回归树的稳定更新

    Stable Update of Regression Trees

    [https://arxiv.org/abs/2402.13655](https://arxiv.org/abs/2402.13655)

    提出了一种回归树的稳定更新方法，通过正则化和调整超参数来平衡预测性能和经验稳定性。

    

    更新机器学习模型以获取新信息通常会改善它们的预测性能，然而，在许多应用中，也希望避免过多改变模型的预测。这种属性被称为稳定性。在大多数情况下，稳定性很重要，解释性也很重要。因此，我们专注于固有可解释的机器学习方法，即回归树的稳定性。我们旨在利用经验稳定性概念，并设计用于更新回归树的算法，以提供平衡预测性能和经验稳定性的方法。为实现这一目标，我们提出一种正则化方法，其中数据点根据初始模型的不确定性进行加权。通过超参数可以调整预测性能和经验稳定性之间的平衡。这种正则化方法在损失和稳定性方面进行评估，并在广泛的数据特征上进行评估。

    arXiv:2402.13655v1 Announce Type: new  Abstract: Updating machine learning models with new information usually improves their predictive performance, yet, in many applications, it is also desirable to avoid changing the model predictions too much. This property is called stability. In most cases when stability matters, so does explainability. We therefore focus on the stability of an inherently explainable machine learning method, namely regression trees. We aim to use the notion of empirical stability and design algorithms for updating regression trees that provide a way to balance between predictability and empirical stability. To achieve this, we propose a regularization method, where data points are weighted based on the uncertainty in the initial model. The balance between predictability and empirical stability can be adjusted through hyperparameters. This regularization method is evaluated in terms of loss and stability and assessed on a broad range of data characteristics. The r
    
[^59]: 使用强化学习改进比例积分控制器在节流阀基准上的应用

    Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark

    [https://arxiv.org/abs/2402.13654](https://arxiv.org/abs/2402.13654)

    通过引入强化学习与引导，结合比例积分（PI）控制器，本文提出了一种学习基础的控制策略，用于非线性节流阀的控制，实现了一个几乎最优的控制器。

    

    本文提出了一种基于学习的控制策略，用于非线性节流阀，该节流阀具有不对称的磁滞，实现了一个几乎最优的控制器，而不需要任何关于环境的先验知识。我们首先通过精心调整的比例积分（PI）控制器开始，并利用强化学习（RL）与引导的最新进展，通过从与阀门的额外交互中学习来改进闭环行为。我们在三个不同的阀门上的各种场景中测试了所提出的控制方法，所有这些都突显了将PI和RL框架结合以提高非线性随机系统控制性能的好处。在所有实验测试案例中，结果代理的样本效率都优于传统RL代理，并且优于PI控制器。

    arXiv:2402.13654v1 Announce Type: cross  Abstract: This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment. We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve. We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems. In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller.
    
[^60]: PQA：零样本蛋白质问题回答与大型语言模型进行自由科学探究

    PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models

    [https://arxiv.org/abs/2402.13653](https://arxiv.org/abs/2402.13653)

    零样本蛋白质问题回答任务，提供专门数据集并建立了PQA框架，包含生物相关的科学PQA基准，通过多模态架构取得了最新技术水平。

    

    我们引入了零样本蛋白质问题回答（PQA）这一新颖任务，用于自由形式科学探究。给定一个以前未见过的蛋白质序列和一个自然语言问题，任务是提供一个科学上准确的答案。这一任务不仅支持未来的生物研究，还可以为评估大型语言模型（LLMs）的科学精度提供一个测试基准。我们贡献了第一个专门用于PQA模型训练的数据集，其中包含257K个蛋白质序列，注释有1.97M个科学问题-答案对。此外，我们提出并研究了几个新颖的与生物相关的科学PQA基准。通过两种强大的多模态架构，我们建立了PQA的最新技术水平，并通过消融研究揭示了关键性能因素。我们的全面PQA框架，名为Pika，包括数据集、代码、模型检查点和一个用户友好的演示。

    arXiv:2402.13653v1 Announce Type: new  Abstract: We introduce the novel task of zero-shot Protein Question Answering (PQA) for free-form scientific enquiry. Given a previously unseen protein sequence and a natural language question, the task is to deliver a scientifically accurate answer. This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of large language models (LLMs). We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific question-answer pairs. Additionally, we propose and study several novel biologically relevant benchmarks for scientific PQA. Employing two robust multi-modal architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies. Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is o
    
[^61]: 深度神经网络在微多普勒雷达分类中的鲁棒性

    Robustness of Deep Neural Networks for Micro-Doppler Radar Classification

    [https://arxiv.org/abs/2402.13651](https://arxiv.org/abs/2402.13651)

    评估了两种深度卷积架构的鲁棒性，发现在训练中加入对抗样本和时间增强样本可以改善模型的泛化能力

    

    随着深度分类器在雷达数据处理方面的巨大能力，学习特定于数据集的特征而无法很好泛化的风险也随之而来。在这项工作中，评估了两种深度卷积架构在相同数据上训练和测试时的鲁棒性。当遵循标准训练实践时，两个分类器都展现出对输入表示的微小时间偏移的敏感性，这种增强带有最小的语义内容。此外，模型极易受到对抗样本的影响。小的时间偏移和对抗样本都是模型过拟合于无法很好泛化的特征的结果。为了解决这个问题，表明在对抗样本和时间增强样本上进行训练可以减少这种影响，进而导致更好泛化的模型。最后，演示了操作在节奏-速度图表示而不是多普勒-时间的模型。

    arXiv:2402.13651v1 Announce Type: cross  Abstract: With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to 
    
[^62]: 对多任务半监督学习的大维分析

    A Large Dimensional Analysis of Multi-task Semi-Supervised Learning

    [https://arxiv.org/abs/2402.13646](https://arxiv.org/abs/2402.13646)

    本文进行了针对一个简单而非常通用的分类模型的大维分析研究，该模型同时涵盖了多任务和半监督学习，并考虑了不确定的标签，通过随机矩阵理论的工具表征了关键功能的渐近性质，从而揭示了关于有效使用该模型的反直觉指导。

    

    本文对一个简单但非常通用的分类模型进行了大维研究，同时涵盖了多任务和半监督学习，并考虑了不确定的标签。利用随机矩阵理论的工具，我们表征了一些关键功能的渐近性质，从而一方面可以预测算法的性能，另一方面可以揭示一些关于如何高效使用它的反直觉指导。该模型强大到足以提供良好的性能保证，并且简单直观到足以深入了解其行为。

    arXiv:2402.13646v1 Announce Type: cross  Abstract: This article conducts a large dimensional study of a simple yet quite versatile classification model, encompassing at once multi-task and semi-supervised learning, and taking into account uncertain labeling. Using tools from random matrix theory, we characterize the asymptotics of some key functionals, which allows us on the one hand to predict the performances of the algorithm, and on the other hand to reveal some counter-intuitive guidance on how to use it efficiently. The model, powerful enough to provide good performance guarantees, is also straightforward enough to provide strong insights into its behavior.
    
[^63]: FlexHB: 用于超参数优化的更高效灵活框架

    FlexHB: a More Efficient and Flexible Framework for Hyperparameter Optimization

    [https://arxiv.org/abs/2402.13641](https://arxiv.org/abs/2402.13641)

    FlexHB通过细粒度保真度方法提高了搜索最佳配置的效率，重新设计了早停框架，并结合了连续减半的方法。

    

    给定一个超参数优化（HPO）问题，如何设计一种算法来高效地找到最佳配置？贝叶斯优化（BO）和多保真度BO方法利用替代模型根据历史评估来采样配置。更近期的研究通过将BO与HyperBand（HB）相结合获得了更好的性能，后者通过提前停止机制加快评估。然而，这些方法忽略了适当评估方案相对于默认HyperBand的优势，而且BO的能力仍受到倾斜评估结果的限制。本文提出了FlexHB，一种将多保真度BO推至极限并重新设计早停框架与连续减半（SH）的新方法。对FlexHB的全面研究表明（1）我们的细粒度保真度方法显著提高了搜索最佳配置的效率，（2）我们的FlexBand框架（自适应配置）

    arXiv:2402.13641v1 Announce Type: new  Abstract: Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm to find optimal configurations efficiently? Bayesian Optimization(BO) and the multi-fidelity BO methods employ surrogate models to sample configurations based on history evaluations. More recent studies obtain better performance by integrating BO with HyperBand(HB), which accelerates evaluation by early stopping mechanism. However, these methods ignore the advantage of a suitable evaluation scheme over the default HyperBand, and the capability of BO is still constrained by skewed evaluation results. In this paper, we propose FlexHB, a new method pushing multi-fidelity BO to the limit as well as re-designing a framework for early stopping with Successive Halving(SH). Comprehensive study on FlexHB shows that (1) our fine-grained fidelity method considerably enhances the efficiency of searching optimal configurations, (2) our FlexBand framework (self-adaptive alloc
    
[^64]: 绿色人工智能: 跨不同运行时基础设施的深度学习模型能耗初步实证研究

    Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures

    [https://arxiv.org/abs/2402.13640](https://arxiv.org/abs/2402.13640)

    本研究通过监测三种知名DL框架以及ONNX的运行时基础设施中的能耗和推理时间，使用三种不同的DL模型，初步探究了它们的能源效率。

    

    arXiv:2402.13640v1 公告类型: 跨学科 摘要: 深度学习（DL）框架如PyTorch和TensorFlow包括运行时基础设施，负责在目标硬件上执行训练好的模型，并管理内存、数据传输以及多加速器执行（如果适用）。此外，将预训练模型部署到与其原生开发环境不同的环境是一种常见做法。这导致引入了诸如ONNX之类的交换格式，其中包括其运行时基础设施，以及ONNX Runtime，可作为可在不同DL框架和语言之间使用的标准格式。尽管这些运行时基础设施对推理性能有很大影响，但以前没有论文调查过它们的能源效率。在这项研究中，我们监测了三种知名DL框架以及ONNX的运行时基础设施中的能耗和推理时间，使用了三种不同的DL模型。为了使我们的调查更加细致

    arXiv:2402.13640v1 Announce Type: cross  Abstract: Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime infrastructures responsible for executing trained models on target hardware, managing memory, data transfers, and multi-accelerator execution, if applicable. Additionally, it is a common practice to deploy pre-trained models on environments distinct from their native development settings. This led to the introduction of interchange formats such as ONNX, which includes its runtime infrastructure, and ONNX Runtime, which work as standard formats that can be used across diverse DL frameworks and languages. Even though these runtime infrastructures have a great impact on inference performance, no previous paper has investigated their energy efficiency. In this study, we monitor the energy consumption and inference time in the runtime infrastructures of three well-known DL frameworks as well as ONNX, using three various DL models. To have nuance in our investigatio
    
[^65]: 用于评估可信医疗人工智能数据质量的METRIC框架: 一项系统性综述

    The METRIC-framework for assessing data quality for trustworthy AI in medicine: a systematic review

    [https://arxiv.org/abs/2402.13635](https://arxiv.org/abs/2402.13635)

    本文提出了用于评估医疗人工智能数据质量的METRIC框架，着重探讨数据质量在深度学习应用中的重要性，强调数据质量对于医学人工智能产品监管批准的关键作用。

    

    arXiv:2402.13635v1 公告类型:跨领域 摘要: 机器学习(ML)的采用，更具体地说是深度学习(DL)应用正在蔓延到我们生活的各个主要领域中。在医学领域，发展可信人工智能尤为重要，因为这对患者的生活有着重大影响。虽然可信性涉及多个方面，包括道德、技术和隐私要求，但我们着重于DL中数据质量(训练/测试)的重要性。由于数据质量决定了ML产品的行为，评估数据质量将在医疗人工智能产品的监管批准中起关键作用。我们按照PRISMA指南进行系统性综述，使用PubMed和ACM数字图书馆数据库。我们发现了2362项研究，其中62项符合我们的资格标准。在这一文献中，我们综合现有的关于数据质量框架的知识，并结合ML在医学中的应用视角。因此，我们的工作总结了医疗AI领域数据质量框架的现有知识，并将其与ML应用的视角结合在一起。

    arXiv:2402.13635v1 Announce Type: cross  Abstract: The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we p
    
[^66]: 学习笛卡尔机器人的双臂物体重新排列

    Learning Dual-arm Object Rearrangement for Cartesian Robots

    [https://arxiv.org/abs/2402.13634](https://arxiv.org/abs/2402.13634)

    提出了一种基于强化学习的在线任务分配决策方法，计算时间随着物体数量的增加仅呈线性增长，以解决笛卡尔机器人双臂物体重新排列问题。

    

    这项工作专注于从笛卡尔机器人现实工业场景中抽象出的双臂物体重新排列问题。该问题的目标是以最短的总完成时间将所有物体从来源地转移到目标地。为了实现这一目标，核心思想是开发一种有效的物体-臂任务分配策略，以最小化累积任务执行时间并最大化双臂协作效率。在任务分配中的一个困难是可伸缩性问题，随着物体数量的增加，传统的基于离线搜索的方法的计算时间会由于计算复杂性而大幅增长。受强化学习（RL）在长序列任务决策中的适应性启发，我们提出了一种基于RL的在线任务分配决策方法，我们的方法的计算时间随物体数量的增加仅呈线性增长。

    arXiv:2402.13634v1 Announce Type: cross  Abstract: This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots. The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time. To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency. One of the difficulties in the task assignment is the scalability problem. As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity. Encouraged by the adaptability of reinforcement learning (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects. Further, we design an
    
[^67]: UniGraph: 从自然语言中学习跨领域图基础模型

    UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language

    [https://arxiv.org/abs/2402.13630](https://arxiv.org/abs/2402.13630)

    UniGraph框架旨在训练一个能够泛化到不同领域的未见图和任务的图基础模型

    

    arXiv:2402.13630v1 公告类型: 新摘要: ChatGPT 和 GPT-4 等基础模型已经彻底改变了人工智能，展示出在各种任务和应用中泛化的显著能力，超越了它们最初的训练目标。然而，当这个概念应用于图学习时，出现了鲜明的对比。图学习主要集中在针对特定任务或数据集定制的单个图模型上，缺乏将学到的知识转移到不同领域的能力。这种限制源于图结构的内在复杂性和多样性，以及特定于图数据的不同特征和标签空间。在本文中，我们提出了我们的UniGraph框架，旨在训练一个能够泛化到不同领域的未见图和任务的图基础模型。

    arXiv:2402.13630v1 Announce Type: new  Abstract: Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (T
    
[^68]: 改进建筑温度预测：一种基于系统场景聚类的数据驱动方法

    Improving Building Temperature Forecasting: A Data-driven Approach with System Scenario Clustering

    [https://arxiv.org/abs/2402.13628](https://arxiv.org/abs/2402.13628)

    本文提出了一种基于系统场景聚类的数据驱动室温预测模型，通过历史数据分析提取系统运行特征，进一步简化系统模型以提高泛化和计算效率，并在真实世界中取得了显著降低建模时间的效果。

    

    arXiv:2402.13628v1 公告类型：新 抽象：供热、通风和空调系统在维持舒适的热环境中起着至关重要的作用，占建筑部门约40%的主要能源使用量。为了实现建筑智能能源管理，使用模式及其产生的配置允许改进预测能力的控制系统。然而，对于大规模供热通风空调系统管理来说，为每个子系统构建详细模型是困难的。本文提出了一种基于k-means聚类方法的新的数据驱动室温预测模型。所提出的数据驱动温度预测方法通过历史数据分析提取系统运行特征，并进一步简化系统级模型以提高泛化和计算效率。我们在真实世界中评估了所提出的方法。结果表明，我们的方法可以显著减少建模时间。

    arXiv:2402.13628v1 Announce Type: new  Abstract: Heat, Ventilation and Air Conditioning (HVAC) systems play a critical role in maintaining a comfortable thermal environment and cost approximately 40% of primary energy usage in the building sector. For smart energy management in buildings, usage patterns and their resulting profiles allow the improvement of control systems with prediction capabilities. However, for large-scale HVAC system management, it is difficult to construct a detailed model for each subsystem. In this paper, a new data-driven room temperature prediction model is proposed based on the k-means clustering method. The proposed data-driven temperature prediction approach extracts the system operation feature through historical data analysis and further simplifies the system-level model to improve generalization and computational efficiency. We evaluate the proposed approach in the real world. The results demonstrated that our approach can significantly reduce modeling t
    
[^69]: 在高维正则化回归中对自举和子抽样的分析

    Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression

    [https://arxiv.org/abs/2402.13622](https://arxiv.org/abs/2402.13622)

    重要发现包括高维情况下重抽样方法的问题，仅当$\alpha$足够大时提供一致可靠的误差估计，以及在超参数化区域$\alpha\!<\!1$的情况下它们的预测表现

    

    我们研究了用于估计统计模型不确定性的流行重抽样方法，如子抽样、自举和jackknife，以及它们在高维监督回归任务中的性能。在广义线性模型的情境下，例如岭回归和逻辑回归，我们对这些方法估计的偏差和方差提供了紧致的渐近描述，考虑到样本数量$n$和协变量维度$d$以可比固定速率$\alpha\!=\! n/d$增长的极限情况。我们的发现有三个方面：i）在高维情况下，重抽样方法存在问题，并表现出这些情况典型的双峰行为；ii）只有在$\alpha$足够大时，它们才提供一致可靠的误差估计（我们给出收敛率）；iii）在现代机器学习实践中相关的超参数化区域$\alpha\!<\!1$，它们的预测是

    arXiv:2402.13622v1 Announce Type: cross  Abstract: We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\alpha\!<\!1$ relevant to modern machine learning practice, their predictions are
    
[^70]: VLSP 2023综述--ComOM任务：越南产品评论的比较意见挖掘数据挑战

    Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews

    [https://arxiv.org/abs/2402.13613](https://arxiv.org/abs/2402.13613)

    该论文总结了VLSP 2023中ComOM任务的一个数据挑战，旨在推动自然语言处理领域通过开发从越南产品评论中提取比较意见的技术，参与者需提出能够提取比较"五元组"的模型并根据F1分数进行评估排名。

    

    本文提供了越南语产品评论比较意见挖掘共享任务（ComOM）的综合概述，该任务作为第十届越南语言和语音处理国际研讨会（VLSP 2023）的一部分举行。此共享任务的主要目标是通过开发能够有效从越南产品评论中提取比较意见的技术来推动自然语言处理领域的发展。参与者被挑战提出能够从比较句中熟练提取比较“五元组”的模型，包括主题、客体、方面、谓词和比较类型标签。我们构建了一个包含120个文档的人工标记数据集，其中包括7427个非比较句和1798个句子中的2468个比较。参与的模型将根据准确匹配宏平均的五元组F1分数进行评估和排名。

    arXiv:2402.13613v1 Announce Type: new  Abstract: This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023). The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews. Participants are challenged to propose models that adeptly extract a comparative "quintuple" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label. We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences. Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score.
    
[^71]: 数据驱动的大型生成模型在科学发现中的应用

    Data-driven Discovery with Large Generative Models

    [https://arxiv.org/abs/2402.13610](https://arxiv.org/abs/2402.13610)

    大型生成模型在数据驱动发现中的应用开创了端到端发现系统的新模式，利用提供的数据集搜寻和验证假设，突显了自动化系统的重要性和局限性。

    

    随着数据以前所未有的速度累积，它作为促进科学发现的潜力呈指数增长。这篇立场论文敦促机器学习（ML）社区利用大型生成模型（LGMs）的能力，开发自动化系统用于端到端的数据驱动发现 -- 一种范式，从所提供的数据集中纯粹搜索和验证假设，而无需额外的数据收集或物理实验。我们首先概述了理想数据驱动发现系统的几个期望条件。然后，通过使用GPT-4的DATAVOYAGER作为概念验证，我们展示了LGMs如何实现几项这些期望条件 -- 这是以前无法做到的成就 -- 同时也突显了当前系统中的重要局限性，从而为开展新型机器学习研究提供了机遇。

    arXiv:2402.13610v1 Announce Type: cross  Abstract: With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely th
    
[^72]: Markov Chain Monte Carlo梯度下降的收敛加速通过深度展开

    Convergence Acceleration of Markov Chain Monte Carlo-based Gradient Descent by Deep Unfolding

    [https://arxiv.org/abs/2402.13608](https://arxiv.org/abs/2402.13608)

    提出了一种结合了MCMC和梯度下降的Ohzeki方法的可训练采样求解器，通过最小化损失函数训练步长，采用基于采样的梯度估计替代自动微分，并在数值实验中显示相对于原始方法显著加快了收敛速度

    

    本研究提出了一种可训练的基于深度展开的采样求解器，用于组合优化问题（COPs），该求解器基于结合了马尔可夫链—蒙特卡洛（MCMC）和梯度下降的Ohzeki方法，并通过最小化损失函数来训练其步长。在训练过程中，我们提出了一种基于采样的梯度估计，用方差估计代替自动微分，从而规避了由于MCMC的不可微分性而导致反向传播失败的问题。少数COPs的数值结果表明，与原始的Ohzeki方法相比，提出的求解器显著加快了收敛速度。

    arXiv:2402.13608v1 Announce Type: cross  Abstract: This study proposes a trainable sampling-based solver for combinatorial optimization problems (COPs) using a deep-learning technique called deep unfolding. The proposed solver is based on the Ohzeki method that combines Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are trained by minimizing a loss function. In the training process, we propose a sampling-based gradient estimation that substitutes auto-differentiation with a variance estimation, thereby circumventing the failure of back propagation due to the non-differentiability of MCMC. The numerical results for a few COPs demonstrated that the proposed solver significantly accelerated the convergence speed compared with the original Ohzeki method.
    
[^73]: User-LLM: 利用用户嵌入实现有效的LLM语境化

    User-LLM: Efficient LLM Contextualization with User Embeddings

    [https://arxiv.org/abs/2402.13598](https://arxiv.org/abs/2402.13598)

    User-LLM框架利用用户嵌入对LLMs进行语境化，使其能够动态适应用户上下文，在各种任务中实现显著性能提升。

    

    大语言模型(LLMs)已经彻底改变了自然语言处理。然而，有效地整合复杂且潜在嘈杂的用户交互数据仍然是一个挑战。为了解决这个问题，我们提出了User-LLM，这是一个新颖的框架，利用用户嵌入来对LLMs进行语境化。这些嵌入是通过自监督预训练从各种用户交互中精炼出来的，能够捕捉潜在用户偏好及其随时间的演变。我们通过交叉注意力和软提示将这些用户嵌入与LLMs集成起来，使LLMs能够动态适应用户上下文。我们在MovieLens、亚马逊评论和谷歌本地评论等数据集上进行了全面实验，展示了在各种任务中的显著性能提升。值得注意的是，我们的方法在长序列任务和需要深入理解用户的任务上超过了基于文本提示的语境化，同时在计算上也更加高效。

    arXiv:2402.13598v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorpora
    
[^74]: 一种用于全局解决低维k-means聚类问题的切平面算法

    A cutting plane algorithm for globally solving low dimensional k-means clustering problems

    [https://arxiv.org/abs/2402.13595](https://arxiv.org/abs/2402.13595)

    本文提出一种切平面算法，针对低维数据的k-means聚类问题进行全局最优解，利用结构化凹形分配问题和全局优化理论方法，在合理时间内解决大数据集的聚类问题，并展示收敛于零最优性差值的结果。

    

    聚类是数据科学和机器学习中最基本的工具之一，k-means聚类是最常见的方法之一。针对低维数据的k-means问题，本文将其制定为结构化凹形分配问题。通过利用低维结构，我们能够在合理的时间内为具有多个簇的大数据集找到全局最优解。该方法基于迭代求解一个小凹问题和一个大线性规划问题。我们展示了一系列可行解以及收敛于零最优性差值的边界。本文结合了全局优化理论方法来加速程序，并提供了它们在性能方面的数值结果。

    arXiv:2402.13595v1 Announce Type: cross  Abstract: Clustering is one of the most fundamental tools in data science and machine learning, and k-means clustering is one of the most common such methods. There is a variety of approximate algorithms for the k-means problem, but computing the globally optimal solution is in general NP-hard. In this paper we consider the k-means problem for instances with low dimensional data and formulate it as a structured concave assignment problem. This allows us to exploit the low dimensional structure and solve the problem to global optimality within reasonable time for large data sets with several clusters. The method builds on iteratively solving a small concave problem and a large linear programming problem. This gives a sequence of feasible solutions along with bounds which we show converges to zero optimality gap. The paper combines methods from global optimization theory to accelerate the procedure, and we provide numerical results on their perfor
    
[^75]: 用深度强化学习和行为调控掌握关旦游戏

    Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating

    [https://arxiv.org/abs/2402.13582](https://arxiv.org/abs/2402.13582)

    该论文主要贡献是通过深度神经网络和行为调控方案，提出了一个框架GuanoZero，使AI代理能够掌握《关旦》游戏。

    

    游戏是现实的简化模型，通常被视为人工智能研究的首选平台。 很多研究关注于游戏代理和它们的决策过程。《关旦》是一种具有挑战性的游戏，即使是专业的人类玩家有时也难以做出正确的决策。 在本文中，我们提出了一个名为GuanZero的框架，让AI代理通过蒙特卡洛方法和深度神经网络来掌握这个游戏。 本文的主要贡献在于通过精心设计的神经网络编码方案来调控代理的行为。 然后，我们通过与最先进的方法进行比较，展示了所提出框架的有效性。

    arXiv:2402.13582v1 Announce Type: new  Abstract: Games are a simplified model of reality and often serve as a favored platform for Artificial Intelligence (AI) research. Much of the research is concerned with game-playing agents and their decision making processes. The game of Guandan (literally, "throwing eggs") is a challenging game where even professional human players struggle to make the right decision at times. In this paper we propose a framework named GuanZero for AI agents to master this game using Monte-Carlo methods and deep neural networks. The main contribution of this paper is about regulating agents' behavior through a carefully designed neural network encoding scheme. We then demonstrate the effectiveness of the proposed framework by comparing it with state-of-the-art approaches.
    
[^76]: 任务待办：用于高分辨率图像高效生成的令牌下采样

    ToDo: Token Downsampling for Efficient Generation of High-Resolution Images

    [https://arxiv.org/abs/2402.13573](https://arxiv.org/abs/2402.13573)

    提出了一种新的训练-free 方法 ToDo，通过令牌下采样加速 Stable Diffusion 推理，以实现高分辨率图像的高效生成。

    

    注意力机制对于图像扩散模型至关重要，然而，它们的二次计算复杂性限制了我们可以在合理时间和内存限制内处理的图像大小。本文研究了在生成图像模型中密集注意力的重要性，这些模型通常包含冗余特征，使它们适合稀疏注意力机制。我们提出了一种新颖的无需训练的方法 ToDo，该方法依赖于关键和值令牌的令牌下采样，可将常见大小的 Stable Diffusion 推理加速至多达2倍，对于2048x2048等高分辨率，加速比可达4.5倍或更高。我们证明了我们的方法在平衡高效吞吐量和保真度方面优于先前的方法。

    arXiv:2402.13573v1 Announce Type: cross  Abstract: Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.
    
[^77]: 论一种变种Looped Transformer的表达能力

    On the Expressive Power of a Variant of the Looped Transformer

    [https://arxiv.org/abs/2402.13572](https://arxiv.org/abs/2402.13572)

    设计了一种新型Transformer块AlgoFormer，相比标准Transformer和Looped Transformer，AlgoFormer在相同参数数量下能够实现更高的算法表达能力

    

    除了自然语言处理，在解决更广泛的应用程序（包括科学计算和计算机视觉）方面，Transformer展现出卓越的性能。先前的工作试图从表达能力和功能性角度解释，标准的Transformer能够执行一些算法。为了赋予Transformer算法能力，并受到最近提出的Looped Transformer的启发，我们设计了一种新颖的Transformer块，名为Algorithm Transformer（简称AlgoFormer）。与标准Transformer和纯粹的Looped Transformer相比，所提出的AlgoFormer在使用相同数量的参数时可以实现更高的算法表示表达能力。特别是，受人类设计的学习算法结构的启发，我们的Transformer块包括一个负责进行ta

    arXiv:2402.13572v1 Announce Type: cross  Abstract: Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for ta
    
[^78]: Spot Check Equivalence：一个可解释的信息引出机制度量

    Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms

    [https://arxiv.org/abs/2402.13567](https://arxiv.org/abs/2402.13567)

    提出了"点检查等价性"，为同行预测机制的效力提供了一个可解释的度量

    

    由于高质量数据对于AI系统如同氧气一般重要，有效地从众包工作者中引出信息已成为开发高性能机器学习算法的首要问题。两种普遍的范式，即点检查和同行预测，使得设计机制来评估和激励来自人类标注者的高质量数据成为可能。到目前为止，至少提出了三种指标来比较这些技术的性能。然而，不同的指标在不同背景下导致了分歧甚至矛盾的结果。本文将调和这些不同的故事，展示其中两个指标在某些背景下实际上是相同的，并解释第三个的分歧。此外，我们通过引入"点检查等价性"来统一这些不同的背景，为同行预测机制的有效性提供了一个可解释的度量。最后，我们提出...（未完）

    arXiv:2402.13567v1 Announce Type: cross  Abstract: Because high-quality data is like oxygen for AI systems, effectively eliciting information from crowdsourcing workers has become a first-order problem for developing high-performance machine learning algorithms. Two prevalent paradigms, spot-checking and peer prediction, enable the design of mechanisms to evaluate and incentivize high-quality data from human labelers. So far, at least three metrics have been proposed to compare the performances of these techniques [33, 8, 3]. However, different metrics lead to divergent and even contradictory results in various contexts. In this paper, we harmonize these divergent stories, showing that two of these metrics are actually the same within certain contexts and explain the divergence of the third. Moreover, we unify these different contexts by introducing \textit{Spot Check Equivalence}, which offers an interpretable metric for the effectiveness of a peer prediction mechanism. Finally, we pr
    
[^79]: 归纳图对齐提示：从谱角度弥合图预训练和归纳微调之间的差距

    Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective

    [https://arxiv.org/abs/2402.13556](https://arxiv.org/abs/2402.13556)

    IGAP提出了一种新型的基于图提示的方法，用于将图预训练泛化到归纳场景，弥合了预训练图和微调图之间的差距。

    

    “图预训练和微调”范式通过捕捉下游任务无需手动标注的通用知识，显著改进了图神经网络(GNNs)。然而，由于在预训练和微调阶段之间的数据和任务巨大差距，模型性能仍然受限。受自然语言处理(NLP)中提示微调的启发，许多努力已经为在图领域中弥合差距做出了努力。但现有方法仅仅将微调任务的形式重新表述为预训练任务。在预训练图与微调图兼容的前提下，这些方法通常在转导设置中运行。为了将图预训练泛化到归纳场景，其中微调图可能与预训练图显著不同，我们提出了一种名为归纳图对齐提示(IGAP)的新型基于图提示的方法。首先，我们统

    arXiv:2402.13556v1 Announce Type: cross  Abstract: The "Graph pre-training and fine-tuning" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we uni
    
[^80]: 叙事背景的图表示：通过回顾性问题的连贯依赖

    Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions

    [https://arxiv.org/abs/2402.13551](https://arxiv.org/abs/2402.13551)

    提出了一种新颖且实用的叙事理解范式，通过在叙事中形成图NARCO来描述整个背景的任务无关的连贯依赖，其中的边反映了高层次的连贯关系，无需依赖人类注释。

    

    这项工作介绍了一种新颖且实用的叙事理解范式，这是基于一个观察：叙述中的个别段落通常是相互关联的，而不是孤立的。因此，我们提出在叙事中形成一个名为NARCO的图，描述整个背景的任务无关的连贯依赖。特别是，NARCO中的边涵盖了两个上下文片段之间的自由形式回顾性问题，反映了高层次的连贯关系，受到人类认知感知的启发，人类不断从先前背景中重申相关事件。重要的是，我们的图是通过我们设计的两阶段LLM提示实例化的，因此无需依赖人类注释。我们展示了三个关于其实际效用的独特研究，通过总结识别检验边的有效性，通过情节检索进行本地上下文增强，以及通过长文档问答示例化的更广泛应用。

    arXiv:2402.13551v1 Announce Type: new  Abstract: This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Expe
    
[^81]: DiffPLF：一种用于电动汽车充电负荷概率预测的条件扩散模型

    DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load

    [https://arxiv.org/abs/2402.13548](https://arxiv.org/abs/2402.13548)

    提出了DiffPLF模型，用于电动汽车充电的概率负载预测，结合了去噪扩散模型和交叉注意力机制，执行条件生成充电需求配置。

    

    由于电动汽车（EV）对配电网的深度渗透，充电负荷预测对于推动充电站运营和需求侧管理至关重要。然而，随机充电行为和相关外生因素使得未来的充电负载模式变化剧烈且难以预测。因此，我们设计了一种称为DiffPLF的新颖扩散模型，用于电动汽车充电的概率负载预测，可以明确地近似于基于历史数据和相关协变量的预测负载分布。具体来说，我们利用去噪扩散模型，通过学习扩散过程的逆转，逐渐将高斯先验转换为实时数据。此外，我们将这种扩散模型与基于交叉注意力的调节机制相结合，执行可能的充电需求配置的条件生成。我们还提出了一种任务感知的微调技术。

    arXiv:2402.13548v1 Announce Type: new  Abstract: Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict. Accordingly, we devise a novel Diffusion model termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates. Specifically, we leverage a denoising diffusion model, which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the diffusion process. Besides, we couple such diffusion model with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles. We also propose a task-informed fine-tuning tec
    
[^82]: ARL2: 通过自导自适应相关性标记将检索器与黑盒大型语言模型对齐

    ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling

    [https://arxiv.org/abs/2402.13542](https://arxiv.org/abs/2402.13542)

    ARL2提出了一种检索器学习技术，利用LLMs作为标注者，并采用自适应自训练策略，能够有效减少注释成本，并在NQ和MMLU上取得了5.4%和4.6%的准确度提升。

    

    arXiv:2402.13542v1 公告类型: 交叉 摘要: 检索增强生成通过整合外部知识源的相关信息改进大型语言模型（LLMs），使LLMs能够适应特定领域，并减轻知识密集任务中的幻觉。然而，由于其分开的训练过程和LLMs的黑盒特性，现有的检索器通常与LLMs不匹配。为解决这一挑战，我们提出了ARL2，一种利用LLMs作为标注者的检索器学习技术。ARL2利用LLMs注释和评分相关证据，从而能够从强大的LLM监督中学习检索器。此外，ARL2使用自适应自训练策略来策划高质量和多样性相关性数据，可以有效降低标注成本。大量实验表明ARL2的有效性，与最先进方法相比，在NQ上提高了5.4%的准确率，在MMLU上提高了4.6%。

    arXiv:2402.13542v1 Announce Type: cross  Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionall
    
[^83]: FinGPT-HPC: 高性能计算下用于金融应用的高效预训练和微调大型语言模型

    FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing

    [https://arxiv.org/abs/2402.13533](https://arxiv.org/abs/2402.13533)

    该论文提出了一种基于高性能GPU的方法，利用低秩结构来高效地预训练和微调大型语言模型，解决了线性层冗余性、GPU内存占用和分布式训练中GPU利用率不足的挑战

    

    大型语言模型(LLMs)的计算密集性很高。计算工作量和内存占用量随维度(层宽度)的增加呈二次增长。大多数LLM参数来自变压器结构的线性层，具有高度冗余性。这些线性层贡献了超过80%的计算工作量和99%的模型大小。为了高效地预训练和微调LLMs，需要解决三个主要挑战：1) 减少线性层的冗余性；2) 减少GPU内存占用；3) 在使用分布式训练时提高GPU利用率。之前的方法，如LoRA和QLoRA，利用低秩矩阵和量化来分别减少可训练参数的数量和模型大小。然而， resulting model 仍然消耗大量GPU内存。在本文中，我们提出了基于高性能GPU的方法，利用低秩结构来预训练和微调。

    arXiv:2402.13533v1 Announce Type: cross  Abstract: Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetun
    
[^84]: 私有梯度下降用于线性回归：更紧的误差界限和实例特定的不确定性估计

    Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation

    [https://arxiv.org/abs/2402.13531](https://arxiv.org/abs/2402.13531)

    对于线性回归的标准差分隐私梯度下降，通过改进分析得出了更紧的误差界限和实例特定的不确定性估计。

    

    我们对标准的具有差分隐私的线性回归梯度下降进行了改进分析，针对平方误差损失。 在对输入进行适度假设的情况下，我们表征了每个时间步骤中迭代的分布。 我们的分析导致了算法准确性方面的新结果：对于适当固定的超参数选择，样本复杂度仅与数据维度线性相关。 这与（非私有）普通最小二乘估计器的维度相关性以及依赖于复杂自适应梯度裁剪方案的最近私有算法（Varshney等人，2022年; Liu等人，2023年）的维度相关性相匹配。 我们对迭代分布的分析还允许我们构建置信区间，用于自动适应于特定数据集上算法的方差的经验优化器。 我们通过对合成数据的实验验证了我们的定理。

    arXiv:2402.13531v1 Announce Type: new  Abstract: We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step.   Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023).   Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data.
    
[^85]: 两个世界中的最佳之选：在未知到达模型下带预测的在线资源分配

    Best of Many in Both Worlds: Online Resource Allocation with Predictions under Unknown Arrival Model

    [https://arxiv.org/abs/2402.13530](https://arxiv.org/abs/2402.13530)

    本文提出了一种算法，能够在未知预测质量的情况下，以预测作为输入准确执行在线资源分配，解决了在线决策中预测质量未知的问题。

    

    arXiv:2402.13530v1 公告类型: 跨领域 摘要: 当今的在线决策者通常可以获得关于未来变量的预测，如到达、需求、库存等。这些预测可以由简单的单变量时间序列预测算法生成，一直到利用多个时间序列和附加特征信息的最先进的机器学习模型。然而，预测质量通常对决策者来说是未知的，因此盲目地遵循预测可能是有害的。本文通过给出将预测作为输入并针对未知预测质量进行稳健执行的算法来解决这个问题。我们考虑在线资源分配问题，这是收益管理和在线决策制定中最通用的模型之一。在这个问题中，决策者拥有有限数量的资源，并且请求是顺序到来的。对于每个请求，决策者需要决定采取何种行动。

    arXiv:2402.13530v1 Announce Type: cross  Abstract: Online decision-makers today can often obtain predictions on future variables, such as arrivals, demands, inventories, and so on. These predictions can be generated from simple forecasting algorithms for univariate time-series, all the way to state-of-the-art machine learning models that leverage multiple time-series and additional feature information. However, the prediction quality is often unknown to decisions-makers a priori, hence blindly following the predictions can be harmful. In this paper, we address this problem by giving algorithms that take predictions as inputs and perform robustly against the unknown prediction quality.   We consider the online resource allocation problem, one of the most generic models in revenue management and online decision-making. In this problem, a decision maker has a limited amount of resources, and requests arrive sequentially. For each request, the decision-maker needs to decide on an action, w
    
[^86]: 基础设施调解员：从结构灾难响应中挖掘未来失效担忧

    Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response

    [https://arxiv.org/abs/2402.13528](https://arxiv.org/abs/2402.13528)

    本文开发了一种基础设施调解员系统，用于自动检测特定基础设施问题，通过挖掘社交网络中关于预期失败的担忧，有助于预防和减轻潜在的基础设施失败。

    

    当前研究集中于研究社交媒体上与结构失败相关的讨论，以改进灾难响应策略。然而，检测社交网络帖子中讨论关于预期失败的担忧是未被充分探索的。如果这些担忧被传达给适当的机构，可以帮助预防和减轻潜在的基础设施失败。本文中，我们开发了一种基础设施调解员——用于自动检测特定基础设施问题。我们的工作考虑了美国几起最近的结构失效事件。我们呈现了一份首创性数据集，包括从Reddit和YouTube中挖掘的2,662个社交网络实例，用于这一新颖任务。

    arXiv:2402.13528v1 Announce Type: cross  Abstract: Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.
    
[^87]: MatchNAS：通过自动化深度神经网络移植优化稀疏标签数据环境中的边缘人工智能

    MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment

    [https://arxiv.org/abs/2402.13525](https://arxiv.org/abs/2402.13525)

    MatchNAS提出了一种新颖的方案，通过同时使用有标签和无标签数据来优化大型网络家族，并自动搜索针对不同硬件平台的定制网络，从而解决了在稀疏标签数据环境中优化边缘人工智能模型的问题。

    

    近年来，边缘智能在强大的深度神经网络 (DNNs) 的支持下迅猛发展。一种流行的方案是在强大的云服务器上训练 DNNs，然后在经过轻量化后将其移植到移动设备上。本文提出了MatchNAS，一种用于将 DNN 移植到移动设备的新颖方案。具体地，我们同时使用有标签和无标签数据优化一个大型网络家族，然后自动搜索针对不同硬件平台的定制网络。

    arXiv:2402.13525v1 Announce Type: new  Abstract: Recent years have seen the explosion of edge intelligence with powerful Deep Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud servers and subsequently porting them to mobile devices after being lightweight. Conventional approaches manually specialized DNNs for various edge platforms and retrain them with real-world data. However, as the number of platforms increases, these approaches become labour-intensive and computationally prohibitive. Additionally, real-world data tends to be sparse-label, further increasing the difficulty of lightweight models. In this paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices. Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms. MatchNAS acts as an intermediary that bridges the gap between cloud-based DNNs and edge-
    
[^88]: 平衡脑电图中的频谱、时间和空间信息用于基于脑电图的阿尔茨海默病分类

    Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer's Disease Classification

    [https://arxiv.org/abs/2402.13523](https://arxiv.org/abs/2402.13523)

    通过变化每个维度的比例来调查空间信息对于脑电图阿尔茨海默病分类的重要性，结果表明空间信息比时间信息更相关，与频谱信息的相关性相同

    

    未来治疗前景需要开发经济有效的阿尔茨海默病筛查方法。在这方面，脑电图（EEG）是一个有前途的候选方法，因为它是最经济的成像模式之一。最近的脑电图分析工作已经转向利用空间信息，采用新颖的框架，如图信号处理或图神经网络。在本研究中，我们系统地调查了相对于频谱或时间信息的空间信息的重要性，通过改变每个维度所占比例来进行阿尔茨海默病分类。为此，我们在两个常规脑电图数据集上测试了各种维度分辨率配置。我们发现，空间信息始终比时间信息更相关，与频谱信息的相关性相同。这些结果强调了考虑空间信息进行基于脑电图的阿尔茨海默病分类的必要性。在我们的第二个数据集上，

    arXiv:2402.13523v1 Announce Type: cross  Abstract: The prospect of future treatment warrants the development of cost-effective screening for Alzheimer's disease (AD). A promising candidate in this regard is electroencephalography (EEG), as it is one of the most economic imaging modalities. Recent efforts in EEG analysis have shifted towards leveraging spatial information, employing novel frameworks such as graph signal processing or graph neural networks. Here, we systematically investigate the importance of spatial information relative to spectral or temporal information by varying the proportion of each dimension for AD classification. To do so, we test various dimension resolution configurations on two routine EEG datasets. We find that spatial information is consistently more relevant than temporal information and equally relevant as spectral information. These results emphasise the necessity to consider spatial information for EEG-based AD classification. On our second dataset, we
    
[^89]: ProSparse: 引入和增强大型语言模型内部激活稀疏性

    ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models

    [https://arxiv.org/abs/2402.13516](https://arxiv.org/abs/2402.13516)

    本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能

    

    Activation sparsity指的是激活输出中存在许多弱贡献元素。作为使用ReLU激活函数的模型的普遍属性，已被证明是提高模型推理效率的一种有前途的范例。然而，大多数大型语言模型（LLMs）采用了没有内在激活稀疏性的激活函数（例如GELU和Swish）。一些最近的努力尝试引入ReLU或其变体作为替代激活函数，以帮助LLMs实现激活稀疏性和推理加速，但很少能同时获得高稀疏度和可比较的模型性能。本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动LLMs实现更高的激活稀疏性而不降低模型性能。具体来说，将LLMs的激活函数替换为ReLU后，ProSparse采用渐进稀疏正则化

    arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
    
[^90]: 从自注意力到马尔可夫模型：揭示生成Transformer的动态

    From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers

    [https://arxiv.org/abs/2402.13512](https://arxiv.org/abs/2402.13512)

    本文研究了从自注意力模型到马尔可夫模型的转变，揭示了生成Transformer动态的机理和相关条件，为一致估计提供了保证，并在IID样本下建立了样本复杂性保证。

    

    现代语言模型依赖Transformer架构和注意力机制来进行语言理解和文本生成。本文研究了从一组提示和与模型采样的关联输出数据中学习一个单层自注意模型。我们首先建立了自注意机制和马尔可夫模型之间的精确映射：将提示输入模型会根据上下文条件的马尔可夫链（CCMC）对输出标记进行采样，该链加权了基本马尔可夫链的转移矩阵。此外，引入位置编码导致了转移概率的位置相关缩放。基于这种形式主义，我们为提示分布开发了可辨识性/覆盖条件，确保一致估计，并在IID样本下建立了样本复杂性保证。最后，我们研究了从单个输出轨迹生成中学习的问题。

    arXiv:2402.13512v1 Announce Type: cross  Abstract: Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from
    
[^91]: SimPro：一个简单的概率框架实现逼真的长尾半监督学习

    SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning

    [https://arxiv.org/abs/2402.13505](https://arxiv.org/abs/2402.13505)

    SimPro提出了一种高度适应的框架，不依赖于任何关于未标记数据分布的预定义假设，通过创新地改进期望最大化（EM）算法，明确分离条件和边缘类别分布的建模。

    

    近年来半监督学习的最新进展集中在解决一个更为逼真但具有挑战性的任务：解决标记数据的不平衡问题，同时未标记数据的类别分布既未知又可能不匹配。当前这一领域的方法往往预设了关于未标记数据类别分布的严格假设，从而限制了模型仅适应于某些分布范围。在本研究中，我们提出了一种新颖的方法，引入了一个高度适应性的框架，命名为SimPro，它不依赖于任何关于未标记数据分布的预定义假设。我们的框架建立在一个概率模型上，通过明确分离条件和边缘类别分布的建模，创新地改进了期望最大化（EM）算法。这种分离促进了在最大化过程中对类别分布进行估计的闭合形式解决方案。

    arXiv:2402.13505v1 Announce Type: new  Abstract: Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization p
    
[^92]: HetTree: 异构树图神经网络

    HetTree: Heterogeneous Tree Graph Neural Network

    [https://arxiv.org/abs/2402.13496](https://arxiv.org/abs/2402.13496)

    HetTree提出了一种新颖的异构树图神经网络，通过构建语义树数据结构捕捉元路径之间的层次关系，解决了现有方法忽略的异构图中的树形层次结构问题。

    

    最近的过去看到了对异构图神经网络（HGNNs）的兴趣日益增长，因为许多现实世界中的图是异构的，从引用图到电子邮件图。然而，现有方法忽略了元路径之间的树形层次结构，该结构是由不同的节点类型和关系类型自然构成的。在本文中，我们提出了HetTree，一种新颖的异构树图神经网络，以可扩展且有效的方式建模图结构和异构方面。具体来说，HetTree构建了一个语义树数据结构，用于捕捉元路径之间的层次关系。现有的树形编码技术通过根据子节点与父节点的相似性来加权子节点的贡献来聚合子节点。然而，我们发现这种树形编码未能捕捉整个父子层次结构，因为只考虑了父节点。因此，HetTree使用了一种新颖的子树注意机制。

    arXiv:2402.13496v1 Announce Type: new  Abstract: The recent past has seen an increasing interest in Heterogeneous Graph Neural Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from citation graphs to email graphs. However, existing methods ignore a tree hierarchy among metapaths, which is naturally constituted by different node types and relation types. In this paper, we present HetTree, a novel heterogeneous tree graph neural network that models both the graph structure and heterogeneous aspects in a scalable and effective manner. Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths. Existing tree encoding techniques aggregate children nodes by weighting the contribution of children nodes based on similarity to the parent node. However, we find that this tree encoding fails to capture the entire parent-children hierarchy by only considering the parent node. Hence, HetTree uses a novel subtree attention mechanism
    
[^93]: 针对隐蔽性对策的隐身对手攻击 随机多臂老虎机

    Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits

    [https://arxiv.org/abs/2402.13487](https://arxiv.org/abs/2402.13487)

    该论文研究了针对随机多臂老虎机的隐身对手攻击，指出大部分现有攻击容易被提出的检测方法发现，因此提出了隐身攻击概念并进行了探讨，分析发现在特定条件下几乎总是可以成功发动隐身攻击，为MAB的安全风险带来新见解。

    

    针对随机多臂老虎机（MAB）算法的对手性攻击在文献中得到了广泛研究。本文聚焦在奖励投毒攻击，并发现大多数现有攻击可以被我们提出的基于均匀性检测的检测方法轻易发现，因为它们在奖励操纵方面的攻击性较强。这激励我们研究针对随机MAB的隐身攻击概念，并调查由此产生的攻击性。我们的分析表明，针对两种常用的MAB算法UCB1和$\epsilon$-greedy，隐身攻击的成功取决于环境条件和第一轮拉动臂数的实现奖励。我们还分析了一般MAB算法配备我们的攻击检测方法的情况，发现几乎总是能够进行一次成功的隐身攻击。这为MAB的安全风险带来新的见解。

    arXiv:2402.13487v1 Announce Type: new  Abstract: Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms have been extensively studied in the literature. In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations. This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability. Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\epsilon$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round. We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds. This brings new insights into the security risks of MAB 
    
[^94]: ProPD：LLM并行解码的动态令牌树修剪和生成

    ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding

    [https://arxiv.org/abs/2402.13485](https://arxiv.org/abs/2402.13485)

    提出ProPD，一种基于动态令牌树修剪和生成的高效LLM并行解码框架，通过先进的提前修剪机制和动态令牌树生成算法来提高验证效率。

    

    近期生成式大型语言模型（LLMs）的进展显著提升了自然语言处理任务的性能。然而，它们的效率受到自回归令牌生成中固有限制的影响。尽管已经提出了带有令牌树验证的并行解码方法，例如Medusa，以改善解码并行性和效率，但由于其独立令牌预测方法以及大树大小和批处理时产生的显著验证开销，它经常难以保持上下文关系。在本文中，我们提出了ProPD，一种基于动态令牌树修剪和生成的高效LLM并行解码框架。ProPD具有一种先进的提前修剪机制，可以有效地消除不太可能的令牌序列以提高验证效率。此外，它引入了一种动态令牌树生成算法来平衡计算成本。

    arXiv:2402.13485v1 Announce Type: cross  Abstract: Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation. ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the com
    
[^95]: 用于低资源领域任务的检索增强数据增强

    Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks

    [https://arxiv.org/abs/2402.13482](https://arxiv.org/abs/2402.13482)

    提出了一种用于低资源领域任务的新方法，通过结合来自其他数据集的相关示例来增强训练数据，以解决在低资源环境中生成样本不够理想和缺乏多样性的挑战

    

    尽管最近语言模型在多样任务上取得了巨大成功，但在训练数据有限的低资源环境中，它们的性能会严重下降。许多现有作品通过从训练数据生成合成数据，然后在其上训练模型来解决这个问题，最近使用大型语言模型（LLM）进行。然而，在低资源环境中，用于数据增强的种子数据样本数量非常少，这使得生成的样本不够理想且缺乏多样性。为了解决这一挑战，我们提出了一种新颖的方法，通过将其他数据集中丰富的示例与给定的训练数据结合起来，来增强训练数据。具体来说，我们首先通过与给定种子数据相似性基于其他数据集检索相关实例，例如它们的输入-输出对或上下文，然后提示LLM使用上下文信息生成新样本。

    arXiv:2402.13482v1 Announce Type: cross  Abstract: Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information 
    
[^96]: STENCIL：基于次模互信息的冷启动主动学习弱监督

    STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning

    [https://arxiv.org/abs/2402.13468](https://arxiv.org/abs/2402.13468)

    STENCIL利用次模互信息选择弱标记的稀有类实例，并通过标注者强标记，提高了文本分类数据集上的准确率和稀有类F-1分数。

    

    随着在NLP应用中对预训练模型进行监督微调越来越受欢迎，需要更大量的标注数据，特别是在大型语言模型的参数计数增加时。主动学习试图挖掘和注释未标记的实例以最大限度地快速改善模型性能，是减少注释成本的常见选择；然而，大多数方法通常忽视类别不平衡，并且要么假设可以访问初始标注数据，要么要求改进稀有类之前需要多轮主动学习选择。我们提出了STENCIL，它利用一组文本示例和最近提出的次模互信息来选择一组弱标记的稀有类实例，然后由标注者对其进行强标记。我们展示了STENCIL在多个文本分类数据集上将整体准确率提高了10%-24%，将稀有类F-1分数提高了17%-40%。

    arXiv:2402.13468v1 Announce Type: cross  Abstract: As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on multiple text classification datasets over commo
    
[^97]: 学习在指导调优期间操纵大型语言模型

    Learning to Poison Large Language Models During Instruction Tuning

    [https://arxiv.org/abs/2402.13459](https://arxiv.org/abs/2402.13459)

    通过设计新的数据注入攻击攻击LLMs，并提出一种梯度引导后门触发器学习方法，通过实验验证表明成功地破坏模型输出，仅改变1%的指导调优样本即可导致性能下降率达到约80％。

    

    大型语言模型（LLMs）的出现标志着语言处理和推理能力方面的重大突破。虽然它们取得了显著进展，但LLMs面临着数据注入攻击的漏洞，其中对手将后门触发器插入训练数据，以操纵输出以进行恶意行为。本研究通过设计一种新的数据注入攻击，旨在利用指导调优过程，进一步识别LLMs中的额外安全风险。我们提出了一种新颖的梯度引导后门触发器学习方法，以有效识别敌对触发器，确保对传统防御手段的规避，同时保持内容的完整性。通过对各种LLMs和任务的实验验证，我们的策略表明在破坏模型输出方面取得了很高的成功率；仅对4,000个指导调优样本中的1％进行注入就导致性能降低率（PDR）约为80％。我们的工作高

    arXiv:2402.13459v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\%. Our work high
    
[^98]: 面向目标数据子集选择的子模信息量的理论分析

    Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection

    [https://arxiv.org/abs/2402.13454](https://arxiv.org/abs/2402.13454)

    通过推导与相关性和覆盖性相关的基于相似度的界限，为子模互信息在目标数据子集选择中的表现提供了理论保证

    

    随着在机器学习任务中使用的数据量增加，定位特定数据子集的能力变得越来越重要。为了帮助实现这一能力，最近提出的子模互信息（SMI）已经在文献中有效应用于执行使用示例查询集进行定位子集选择的多个任务。然而，所有这些工作都没有在理论上保证SMI对于子集相关性和目标数据的覆盖性的敏感性。我们首次通过推导与相关性和目标数据覆盖相关的基于相似度的界限，提供了此类保证。通过这些界限，我们展示了在多个应用中已经表现成功的SMI函数在理论上确保实现良好的查询相关性和查询覆盖。

    arXiv:2402.13454v1 Announce Type: new  Abstract: With increasing volume of data being used across machine learning tasks, the capability to target specific subsets of data becomes more important. To aid in this capability, the recently proposed Submodular Mutual Information (SMI) has been effectively applied across numerous tasks in literature to perform targeted subset selection with the aid of a exemplar query set. However, all such works are deficient in providing theoretical guarantees for SMI in terms of its sensitivity to a subset's relevance and coverage of the targeted data. For the first time, we provide such guarantees by deriving similarity-based bounds on quantities related to relevance and coverage of the targeted data. With these bounds, we show that the SMI functions, which have empirically shown success in multiple applications, are theoretically sound in achieving good query relevance and query coverage.
    
[^99]: 基于Twitter数据的精神健康监测框架：从当地推文到当地健康

    LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data

    [https://arxiv.org/abs/2402.13452](https://arxiv.org/abs/2402.13452)

    本研究提出了一个新的基于Twitter数据的框架LocalHealth，用于预测当地精神健康结果。通过与GPT3.5结合使用，该框架在MH监测中取得了显著的改进。

    

    先前关于Twitter数据的研究已经提供了它在开发补充健康监测系统方面的实用性证据。在这项研究中，我们提出了一个新的框架来监测公共健康，重点关注精神健康（MH）结果。我们假设当地发布的推文可以表明当地的精神健康结果，并收集了来自美国765个地区（人口普查分组）的推文。我们将每个地区的这些推文与疾病控制中心（CDC）报告的相应MH结果配对，创建了一个基准数据集LocalTweets。借助LocalTweets，我们提出了基于Twitter的MH监测系统的首个人口级评估任务。随后，我们开发了一个高效有效的方法LocalHealth，用于根据LocalTweets预测MH结果。当与GPT3.5一起使用时，LocalHealth实现了最高的F1值和准确率，分别达到0.7429和79.78\%，F1值提高了59\%。

    arXiv:2402.13452v1 Announce Type: cross  Abstract: Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\% improvement in F
    
[^100]: ED-Copilot: 使用语言模型诊断辅助减少急诊科等待时间

    ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance

    [https://arxiv.org/abs/2402.13448](https://arxiv.org/abs/2402.13448)

    本研究提出了一种在急诊科中减少等待时间的诊断辅助方法，利用人工智能系统帮助医生进行快速准确的诊断，并开发了ED-Copilot系统来推荐实验室检测并进行诊断预测。

    

    在急诊科（ED）中，患者在诊断前需要进行分诊和多种实验室检测。这个过程耗时，导致急诊科拥挤，显著影响患者死亡率、医疗错误、人员枯竭等。本研究提出了一种（时间）成本有效的诊断辅助方法，探索人工智能系统在协助急诊科临床医生进行高效准确诊断方面的潜力。使用公开可获得的患者数据，我们与急诊科临床医生合作策划了MIMIC-ED-Assist，这是一个衡量人工智能系统在建议最大程度减少急诊等待时间的实验室检测，并在正确预测诸如死亡之类关键结果方面的能力的基准。我们开发了ED-Copilot，它依次建议患者特定的实验室检测并进行诊断预测。ED-Copilot使用预训练的生物医学语言模型对患者信息进行编码并进行增强学习。

    arXiv:2402.13448v1 Announce Type: cross  Abstract: In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learn
    
[^101]: PaCKD: 模式聚类知识蒸馏用于压缩内存访问预测模型

    PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models

    [https://arxiv.org/abs/2402.13441](https://arxiv.org/abs/2402.13441)

    提出了一种名为PaCKD的模式聚类知识蒸馏方法，用于压缩内存访问预测模型，通过将内存访问序列聚类并训练模式特定的教师模型，最终训练出一个轻量级的学生模型。

    

    深度神经网络（DNNs）已被证明是用于准确的内存访问预测（MAP）的有效模型，这是通过数据预取来缓解内存延迟的关键任务。然而，现有基于DNN的MAP模型存在诸如显著的物理存储空间和推理延迟不佳等挑战，主要是由于它们庞大的参数数量。这些限制使它们在实际部署中变得不切实际。在本文中，我们提出了一种名为PaCKD的模式聚类知识蒸馏方法，用于压缩MAP模型同时保持预测性能。PaCKD方法包括三个步骤：将内存访问序列聚类到涉及相似模式的不同分区中，为每个分区训练大型模式特定的用于内存访问预测的教师模型，以及通过从经过训练的模式特定教师那里提炼知识来训练单个轻量级学生模型。

    arXiv:2402.13441v1 Announce Type: new  Abstract: Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers. 
    
[^102]: 学习检索用于工作匹配

    Learning to Retrieve for Job Matching

    [https://arxiv.org/abs/2402.13435](https://arxiv.org/abs/2402.13435)

    将学习检索技术应用于提升领英的工作搜索和推荐系统，通过构建评估求职者资格的图表并利用学习到的链接进行检索，以提高申请者质量和优化求职者参与度。

    

    Web规模搜索系统通常通过两步骤范式来解决可伸缩性挑战：检索和排名。检索步骤，也称为候选选择，通常涉及提取标准化实体，创建反向索引，并执行检索的术语匹配。这种传统方法需要手动和耗时的查询模型开发。本文讨论了将学习检索技术应用于提升领英的工作搜索和推荐系统。在推广工作领域，关键目标是提高申请者的质量，从而为招聘客户提供价值。为了实现这一目标，我们利用确认的雇佣数据构建一个评估求职者对工作资格的图表，并利用学习到的链接进行检索。我们的学习模型易于解释，调试和调整。另一方面，有机工作的重点是优化求职者参与度。

    arXiv:2402.13435v1 Announce Type: cross  Abstract: Web-scale search systems typically tackle the scalability challenge with a two-step paradigm: retrieval and ranking. The retrieval step, also known as candidate selection, often involves extracting standardized entities, creating an inverted index, and performing term matching for retrieval. Such traditional methods require manual and time-consuming development of query models. In this paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns job search and recommendation systems. In the realm of promoted jobs, the key objective is to improve the quality of applicants, thereby delivering value to recruiter customers. To achieve this, we leverage confirmed hire data to construct a graph that evaluates a seeker's qualification for a job, and utilize learned links for retrieval. Our learned model is easy to explain, debug, and adjust. On the other hand, the focus for organic jobs is to optimize seeker engagement. We 
    
[^103]: DrBenchmark: 一个针对法语生物医学领域的大型语言理解评估基准

    DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain

    [https://arxiv.org/abs/2402.13432](https://arxiv.org/abs/2402.13432)

    DrBenchmark提出了一个针对法语生物医学领域的大型语言理解评估基准，旨在弥补对最新法语生物医学模型评估的不足，并考虑到法语的独特敏感性。

    

    生物医学领域在自然语言处理（NLP）领域引起了极大的兴趣，随着预训练语言模型（PLMs）的实质性进展。然而，由于不同模型之间评估协议的变化，比较这些模型已经变得具有挑战性。一个公平的解决方案是将不同的下游任务聚合到一个基准中，允许从各种角度评估PLMs的内在品质。尽管这一倡议仍然局限于少数语言，特别是英语和中文，但已经在生物医学领域展开。这一限制阻碍了对最新的法语生物医学模型的评价，因为它们要么在少量任务上进行评估，而且使用的协议不够标准化，要么使用一般的下游任务进行评估。为弥补这一研究差距，并考虑到法语的独特敏感性，我们提出了首个公开可用的法语生物医学基准

    arXiv:2402.13432v1 Announce Type: cross  Abstract: The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biom
    
[^104]: LinkSAGE: 使用图神经网络优化工作匹配

    LinkSAGE: Optimizing Job Matching Using Graph Neural Networks

    [https://arxiv.org/abs/2402.13430](https://arxiv.org/abs/2402.13430)

    LinkSAGE是一个采用图神经网络的优化工作匹配框架，通过独特的训练和服务方法，实现了在庞大而复杂的领英专业网络中进行个性化工作匹配。

    

    我们提出了LinkSAGE，一个创新性框架，将图神经网络（GNNs）集成到大规模个性化工作匹配系统中，旨在应对领英庞大专业网络的复杂动态。我们的方法利用了一个全新的工作市场图，这是工业界规模最大、最复杂的图之一，拥有数十亿个节点和边。这个图不仅广泛，而且详细丰富，包含会员和工作节点以及关键属性，从而创建了一个广阔而交织的网络。LinkSAGE的一个关键创新在于其训练和服务方法，它有效地将感知图学习与编码器-解码器GNN模型相结合，训练GNN模型与现有深度神经网络（DNN）模型的训练分离，消除了频繁重新训练GNN的需要，同时保持图信号最新。

    arXiv:2402.13430v1 Announce Type: cross  Abstract: We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIns extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Nets (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in ne
    
[^105]: 在回归中探讨直方图损失

    Investigating the Histogram Loss in Regression

    [https://arxiv.org/abs/2402.13425](https://arxiv.org/abs/2402.13425)

    学习整个分布在回归中的性能提升主要来自于优化的改进，而不是学习更好的表示。

    

    越来越常见的是，在回归中训练神经网络来建模整个分布，即使只需要均值来进行预测。 这种额外的建模通常会带来性能增益，但背后的原因尚不完全清楚。 本文研究了回归中的一种最新方法，即直方图损失，该方法通过最小化目标分布和灵活直方图预测之间的交叉熵来学习目标变量的条件分布。 我们设计了理论和实证分析，以确定为什么以及何时会出现性能增益，以及损失的不同组件如何为此做出贡献。 我们的结果表明，在这种设置中学习分布的好处来自于优化的改进，而不是学习更好的表示。 然后，我们展示了直方图损失在常见的深度学习应用中的可行性。

    arXiv:2402.13425v1 Announce Type: cross  Abstract: It is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction. This additional modeling often comes with performance gain and the reasons behind the improvement are not fully known. This paper investigates a recent approach to regression, the Histogram Loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction. We design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it. Our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation. We then demonstrate the viability of the Histogram Loss in common deep learning applications wi
    
[^106]: 驾驶员分心的上下文感知定量风险评估机器学习模型

    Context-Aware Quantitative Risk Assessment Machine Learning Model for Drivers Distraction

    [https://arxiv.org/abs/2402.13421](https://arxiv.org/abs/2402.13421)

    提供了一种考虑车辆、驾驶员和环境数据的全新多类驾驶员分心风险评估（MDDRA）模型，可以灵活调整参数和权重以考虑不同严重级别的事件，从而减少由驾驶员分心引起的道路事故。

    

    风险缓解技术对避免与驾驶行为相关的事故至关重要。我们提供了一种独创的多类驾驶员分心风险评估（MDDRA）模型，在旅程中考虑了车辆、驾驶员和环境数据。MDDRA将驾驶员在风险矩阵中分类为安全、粗心或危险，可灵活调整参数和权重以考虑每个事件在特定严重级别上的情况。我们使用Field Operation Test（TeleFOT）收集了在英国东米德兰地区使用相同路线的驾驶员的真实世界数据。结果显示减少由驾驶员分心引起的道路事故是可能的。我们还研究了分心（驾驶员、车辆和环境）与基于持续分心严重得分的分类严重程度之间的相关性。此外，我们应用机器学习技术对驾驶员分心进行分类和预测。

    arXiv:2402.13421v1 Announce Type: new  Abstract: Risk mitigation techniques are critical to avoiding accidents associated with driving behaviour. We provide a novel Multi-Class Driver Distraction Risk Assessment (MDDRA) model that considers the vehicle, driver, and environmental data during a journey. MDDRA categorises the driver on a risk matrix as safe, careless, or dangerous. It offers flexibility in adjusting the parameters and weights to consider each event on a specific severity level. We collect real-world data using the Field Operation Test (TeleFOT), covering drivers using the same routes in the East Midlands, United Kingdom (UK). The results show that reducing road accidents caused by driver distraction is possible. We also study the correlation between distraction (driver, vehicle, and environment) and the classification severity based on a continuous distraction severity score. Furthermore, we apply machine learning techniques to classify and predict driver distraction acco
    
[^107]: EvolMPNN：通过进化编码预测同源蛋白质的突变效应

    EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding

    [https://arxiv.org/abs/2402.13418](https://arxiv.org/abs/2402.13418)

    EvolMPNN通过进化感知的方式捕捉蛋白质突变对于锚定蛋白质的影响，并最终生成综合蛋白质嵌入。

    

    预测蛋白质属性对生物和医学进步至关重要。当前的蛋白工程通过对典型蛋白质（称为野生型）进行突变，构建同源蛋白质家族并研究其属性。然而，现有方法很容易忽略细微的突变，无法捕捉蛋白质属性的影响。为此，我们提出了EvolMPNN，一种具有进化感知的消息传递神经网络，用于学习进化感知的蛋白质嵌入。

    arXiv:2402.13418v1 Announce Type: new  Abstract: Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way EvolMPNNcan capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 6.4% better than sta
    
[^108]: 将大型语言模型用作事后校正器

    Harnessing Large Language Models as Post-hoc Correctors

    [https://arxiv.org/abs/2402.13414](https://arxiv.org/abs/2402.13414)

    通过提出的无需训练的框架 LlmCorr，本文展示了一个LLM可以作为事后校正器，为任意ML模型的预测提出修正。

    

    随着机器学习（ML）模型的规模增长并需求更高质量的训练数据，与对这些模型进行重新训练和微调相关的费用正在迅速增加。受最近大型语言模型（LLMs）在不同领域取得的令人瞩目成就启发，本文探讨了一个问题：LLMs能否以极低成本有效地改善ML的性能？我们展示了，通过我们提出的无需训练的框架 LlmCorr，一个LLM可以作为事后校正器，为任意ML模型的预测提出修正。特别是，我们通过整合数据集的标签信息和ML模型对验证集的预测来形成一个上下文知识数据库。利用LLMs的上下文学习能力，我们要求LLM总结ML模型犯错误的实例以及主要预测与真实标签之间的相关性。随后，LLM可以

    arXiv:2402.13414v1 Announce Type: cross  Abstract: As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can tr
    
[^109]: 使用专家混合模型扩展物理信息的硬约束

    Scaling physics-informed hard constraints with mixture-of-experts

    [https://arxiv.org/abs/2402.13412](https://arxiv.org/abs/2402.13412)

    新方法能够有效扩展物理信息的硬约束，提高了对复杂动力系统的建模效率。

    

    在神经网络训练中强加已知的物理约束，比如守恒定律，引入了一种归纳偏差，可以提高模拟物理动态的准确性、可靠性、收敛性和数据效率。虽然这些约束可以通过损失函数惩罚软性地强加，但最近不同iable物理和优化的进展通过将PDE约束优化作为神经网络中的单独层来改善性能。这使得对物理约束的遵守更加严格。然而，强加硬约束显著增加了计算和内存成本，尤其是对于复杂的动力系统。这是因为它需要在网格中的大量点上求解优化问题，表示空间和时间的离散化，从而极大地增加了约束的复杂性。为了解决这一挑战，我们开发了一种可扩展的方法来实施硬

    arXiv:2402.13412v1 Announce Type: cross  Abstract: Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard 
    
[^110]: 具有领域知识先验的贝叶斯神经网络

    Bayesian Neural Networks with Domain Knowledge Priors

    [https://arxiv.org/abs/2402.13410](https://arxiv.org/abs/2402.13410)

    提出了一个框架，通过变分推断将各种形式的领域知识整合到贝叶斯神经网络（BNNs）先验中，以实现更好符合领域知识的模型，从而获得更具表现力的后验样本。

    

    最近，由于其能够量化模型不确定性的能力，贝叶斯神经网络（BNNs）变得越来越受欢迎。然而，为BNNs指定能够捕捉相关领域知识的先验往往极具挑战性。在这项工作中，我们提出了一个框架，通过变分推断将各种形式的领域知识（即可以用损失函数表示的任何知识）整合到BNN先验中，同时实现高效的后验推断和抽样。具体来说，我们的方法导致对神经网络权重的先验分配高概率质量给更符合我们领域知识的模型，从而导致后验样本也表现出这种行为。我们展示了，使用我们提出的领域知识先验的BNNs优于具有标准先验（例如各向同性高斯、高斯过程）的模型，在成功整合多种类型的先验信息（例如公平性）方面表现出色。

    arXiv:2402.13410v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty. However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging. In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling. Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic Gaussian, Gaussian process), successfully incorporating diverse types of prior information such as fairness, 
    
[^111]: 通过非交互式和交互式多保真度贝叶斯优化加速物理发现：当前挑战与未来机遇

    Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities

    [https://arxiv.org/abs/2402.13402](https://arxiv.org/abs/2402.13402)

    该论文提出一种通过非交互式和交互式多保真度贝叶斯优化加速物理发现的方法，解决了探索多维非可微参数空间的挑战，如相图和组成空间等，运用主动学习方法来进行自适应探索。

    

    计算和实验材料发现都面临着探索多维且通常非可微参数空间的挑战，例如具有多个相互作用的哈密顿量的相图、组合图书馆的组成空间、加工空间和分子嵌入空间等。通常这些系统评估单个实例的成本昂贵或耗时，因此基于穷举网格或随机搜索的经典方法过于数据密集。这导致了对主动学习方法（如贝叶斯优化）的浓厚兴趣，其中自适应探索基于人类学习（发现）目标。然而，经典贝叶斯优化基于预定义的优化目标，平衡探索和开发的策略纯粹由数据驱动。在实际设置中，领域专家可以以部分已知物理定律的形式提出对系统的先验知识。

    arXiv:2402.13402v1 Announce Type: new  Abstract: Both computational and experimental material discovery bring forth the challenge of exploring multidimensional and often non-differentiable parameter spaces, such as phase diagrams of Hamiltonians with multiple interactions, composition spaces of combinatorial libraries, processing spaces, and molecular embedding spaces. Often these systems are expensive or time-consuming to evaluate a single instance, and hence classical approaches based on exhaustive grid or random search are too data intensive. This resulted in strong interest towards active learning methods such as Bayesian optimization (BO) where the adaptive exploration occurs based on human learning (discovery) objective. However, classical BO is based on a predefined optimization target, and policies balancing exploration and exploitation are purely data driven. In practical settings, the domain expert can pose prior knowledge on the system in form of partially known physics laws
    
[^112]: 自主学习维度

    The Dimension of Self-Directed Learning

    [https://arxiv.org/abs/2402.13400](https://arxiv.org/abs/2402.13400)

    本论文研究了二元和多类别设置下的自主学习复杂性，提出了一个新的维度$SDdim$来精确刻画任何概念类别的自主学习错误上界，并利用“标记游戏”进行解释，展示了在各种例子中的计算结果和对自主学习的学习差距。

    

    理解自主学习的复杂性是自1990年代初以来吸引在线学习理论社区关注的重要问题。在这个框架内，学习者被允许自适应地选择下一个数据点来进行预测，与对抗性在线学习设置不同。本文研究了二元和多类别设置下的自主学习复杂性，并开发了一个维度，即$SDdim$，精确地刻画了任何概念类别的自主学习错误上界。$SDdim$背后的直觉可以理解为一个称为“标记游戏”的双人游戏。利用这个双人游戏，我们对许多例子进行了$SDdim$的计算，特别是在轴对齐矩形、VC维数为$1$的类别和线性分隔器等方面取得了显着结果。我们展示了几个关于自主学习的学习差距，重点关注自主学习。

    arXiv:2402.13400v1 Announce Type: cross  Abstract: Understanding the self-directed learning complexity has been an important problem that has captured the attention of the online learning theory community since the early 1990s. Within this framework, the learner is allowed to adaptively choose its next data point in making predictions unlike the setting in adversarial online learning.   In this paper, we study the self-directed learning complexity in both the binary and multi-class settings, and we develop a dimension, namely $SDdim$, that exactly characterizes the self-directed learning mistake-bound for any concept class. The intuition behind $SDdim$ can be understood as a two-player game called the "labelling game". Armed with this two-player game, we calculate $SDdim$ on a whole host of examples with notable results on axis-aligned rectangles, VC dimension $1$ classes, and linear separators. We demonstrate several learnability gaps with a central focus on self-directed learning and
    
[^113]: 针对群体条件性缺失人口统计数据的公平风险

    Fairness Risks for Group-conditionally Missing Demographics

    [https://arxiv.org/abs/2402.13393](https://arxiv.org/abs/2402.13393)

    通过概率填充敏感特征，联合学习群体条件性缺失概率，增强一般公平风险，实现准确性和公平性之间的改进平衡

    

    具有公平意识的分类模型近年来越来越受到关注，因为对某些人口统计群体的歧视问题日益引起担忧。大多数现有模型要求完全了解敏感特征，这可能由于隐私、法律问题和个人对歧视的恐惧而不切实际。我们将解决的关键挑战是不可用性的群体依赖性，例如，某些年龄范围的人可能更不愿透露他们的年龄。我们的解决方案通过对敏感特征进行概率填充，同时在变分自动编码器中联合学习群体条件性缺失的概率，将一般公平风险与之增强。我们的模型在图像和表格数据集上表现出了有效性，实现了准确性和公平性之间的改进平衡。

    arXiv:2402.13393v1 Announce Type: new  Abstract: Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.
    
[^114]: Transformer 技巧：预计算第一层

    Transformer tricks: Precomputing the first layer

    [https://arxiv.org/abs/2402.13388](https://arxiv.org/abs/2402.13388)

    该论文描述了一种加速具有RoPE的transformer推断的技巧，通过预计算第一层来降低延迟和成本，最大节省取决于总层数。

    

    这篇简短的论文描述了一种加速具有 RoPE（如 LLaMA、Mistral 和 PaLM）的 transformer 推断的技巧。对于这些模型，第一个 transformer 层的大部分内容可以预先计算，从而导致稍低的延迟和更低的每令牌成本。因为这种技巧仅优化了一层，相对节省取决于总层数。例如，对于只有 4 层的模型（如 Whisper tiny），最大节省仅限于 25%，而对于 32 层模型（如 Mistral-7B），节省则是 3%。

    arXiv:2402.13388v1 Announce Type: new  Abstract: This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings.
    
[^115]: 迈向变压器：用变压器彻底改变混合整数规划的解决方案

    Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers

    [https://arxiv.org/abs/2402.13380](https://arxiv.org/abs/2402.13380)

    这项研究利用变压器模型解决混合整数规划问题，首次采用变压器预测二进制变量，提出的算法在解决时间上超越了传统CPLEX和LSTM。

    

    在这项研究中，我们引入了一种创新的深度学习框架，利用变压器模型来解决混合整数规划的挑战，特别是专注于容量限制批量生产问题（CLSP）。据我们所知，我们的方法是首个利用变压器来预测混合整数规划问题中的二进制变量。具体而言，我们的方法利用编码器-解码器变压器处理顺序数据的能力，非常适合预测每个CLSP周期中表示生产设置决策的二进制变量。这个问题本质上是动态的，我们需要在约束条件下处理顺序决策。我们提出了一种有效的算法，通过变压器神经网络学习CLSP解决方案。所提出的后处理变压器算法在解决时间上超越了最先进的求解器CPLEX和长短期记忆（LSTM）。

    arXiv:2402.13380v1 Announce Type: new  Abstract: In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time
    
[^116]: 快速调整定位公平性的裁判元学习

    Referee-Meta-Learning for Fast Adaptation of Locational Fairness

    [https://arxiv.org/abs/2402.13379](https://arxiv.org/abs/2402.13379)

    提出了一个定位元裁判（Meta-Ref）的方法，通过动态调整训练样本的学习率，以在不同地点之间实现公平性，并在深度神经网络上进行指导，从而消除地点偏见。

    

    处理来自不同地点的数据时，机器学习算法往往显示出对某些地点的隐式偏好，这构成了破坏算法空间公平性的偏见，这种不公平很容易在实践中广泛采用基于学习的解决方案时引入偏见进而影响后续的决策制定。然而，人工智能中关于地点偏见的研究大多被低估了。为了减轻对于地点的偏见，我们提出了一个定位元裁判（Meta-Ref）来监督深度神经网络的少样本元训练和元测试。Meta-Ref动态调整了给定地点的训练样本的学习率，通过明确考虑地点偏见和输入数据的特征来提倡跨地点的公平性表现。我们提出了一个三阶段的训练框架，来学习一个基于元学习的预测器和一个管理的整合Meta-Ref。

    arXiv:2402.13379v1 Announce Type: new  Abstract: When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network. Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that govern
    
[^117]: FIDLAR: 水灾减灾的预测导向深度学习架构

    FIDLAR: Forecast-Informed Deep Learning Architecture for Flood Mitigation

    [https://arxiv.org/abs/2402.13371](https://arxiv.org/abs/2402.13371)

    FIDLAR提出了一种预测导向深度学习架构，实现了快速和最优洪水管理，准确进行水前释放。

    

    在沿海河流系统中，频繁发生的洪水往往在大风暴或满潮时发生，对生命和财产构成严重威胁。然而，通过在枢纽结构(如水坝、闸门、泵站和水库)在极端天气事件前进行战略性地释放水，这些洪水可以得到减轻甚至预防。当地水利管理机构通常使用的标准方法是“基于规则”的方法，该方法基于历史和经过时间验证的人类经验指定预先释放水，但往往导致过量或不足的水量释放。模型预测控制(MPC)是一个替代方法，是基于物理的预测模型，尽管需要进行计算密集型计算。本文提出了一种名为FIDLAR的预测导向深度学习架构，以实现精确的水前释放，实现快速和最优的洪水管理。

    arXiv:2402.13371v1 Announce Type: new  Abstract: In coastal river systems, frequent floods, often occurring during major storms or king tides, pose a severe threat to lives and property. However, these floods can be mitigated or even prevented by strategically releasing water before extreme weather events with hydraulic structures such as dams, gates, pumps, and reservoirs. A standard approach used by local water management agencies is the "rule-based" method, which specifies predetermined pre-releases of water based on historical and time-tested human experience, but which tends to result in excess or inadequate water release. The model predictive control (MPC), a physics-based model for prediction, is an alternative approach, albeit involving computationally intensive calculations. In this paper, we propose a Forecast Informed Deep Learning Architecture, FIDLAR, to achieve rapid and optimal flood management with precise water pre-releases. FIDLAR seamlessly integrates two neural netw
    
[^118]: 异议山谷：扩散模型的综合分析

    The Uncanny Valley: A Comprehensive Analysis of Diffusion Models

    [https://arxiv.org/abs/2402.13369](https://arxiv.org/abs/2402.13369)

    通过对扩散模型的综合分析，揭示了决定模型性能的隐藏关键因素，为DMs的推进提供了见解。

    

    通过扩散模型（DMs），我们在生成高质量图像方面取得了重要进展。我们深入探讨了这些模型的核心操作原则，系统地调查了不同DM架构中的关键方面：i）噪声时间表，ii）采样器和iii）引导。我们对这些模型的全面审查揭示了它们隐藏的基本机制，揭示了对其有效性至关重要的隐藏基础要素。我们的分析强调了决定模型性能的隐藏关键因素，提供了有助于推动DMs发展的见解。过去的研究发现，噪声时间表、采样器和引导的配置对生成图像的质量至关重要；然而，模型在不同配置下在一个非常相似的稳定质量水平上达到，揭示了决定最佳性能的关键因素。

    arXiv:2402.13369v1 Announce Type: cross  Abstract: Through Diffusion Models (DMs), we have made significant advances in generating high-quality images. Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance. Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness. Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs. Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance pre
    
[^119]: 统计课程学习：实现Oracle风险的淘汰算法

    Statistical curriculum learning: An elimination algorithm achieving an oracle risk

    [https://arxiv.org/abs/2402.13366](https://arxiv.org/abs/2402.13366)

    提出了一种淘汰学习方法，其风险与强Oracle学习者相匹配，并将弱Oracle学习者的风险作为自适应学习者风险的一个实际基准。

    

    我们考虑一个参数预测设置下的统计版本课程学习（CL）。学习者需要估计目标参数向量，并可以自适应地从目标模型或其他类似于目标模型但噪声较小的源模型中收集样本。根据他们接收的辅助信息水平，我们考虑三种类型的学习者。在单一来源情况下，我们提出了一个淘汰学习方法，其风险与强-Oracle学习者的风险相匹配。在多源情况下，我们主张弱-Oracle学习者的风险是自适应学习者风险的一个现实基准。我们发展了一种自适应多重淘汰方法。

    arXiv:2402.13366v1 Announce Type: new  Abstract: We consider a statistical version of curriculum learning (CL) in a parametric prediction setting. The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy. We consider three types of learners, depending on the level of side-information they receive. The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn. The third, a fully adaptive learner, estimates the target parameter vector without any prior information. In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner. In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners. We develop an adaptive multiple elimination
    
[^120]: 结合无监督和监督学习在显微镜学中实现对全片4H-SiC晶片的缺陷分析

    Combining unsupervised and supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer

    [https://arxiv.org/abs/2402.13353](https://arxiv.org/abs/2402.13353)

    通过结合各种图像分析和数据挖掘技术，实现了对一个KOHH蚀刻的4H-SiC晶片显微镜图像中所有缺陷类型和位置的自动化提取

    

    发现和分析半导体材料中的各种缺陷类型是了解潜在机制以及调整生产过程的重要前提。显微镜图像的分析通常需要像分割和目标检测这样的图像分析任务。本文中，我们结合各种图像分析和数据挖掘技术，创建了一个稳健且准确的自动化图像分析流程，能够从大约40,000张单独图像拼接在一起的KOH蚀刻的4H-SiC晶片显微镜图像中提取所有缺陷的类型和位置。

    arXiv:2402.13353v1 Announce Type: cross  Abstract: Detecting and analyzing various defect types in semiconductor materials is an important prerequisite for understanding the underlying mechanisms as well as tailoring the production processes. Analysis of microscopy images that reveal defects typically requires image analysis tasks such as segmentation and object detection. With the permanently increasing amount of data that is produced by experiments, handling these tasks manually becomes more and more impossible. In this work, we combine various image analysis and data mining techniques for creating a robust and accurate, automated image analysis pipeline. This allows for extracting the type and position of all defects in a microscopy image of a KOH-etched 4H-SiC wafer that was stitched together from approximately 40,000 individual images.
    
[^121]: KetGPT -- 使用Transformer对量子电路进行数据增强

    KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers

    [https://arxiv.org/abs/2402.13352](https://arxiv.org/abs/2402.13352)

    该研究利用Transformer机器学习架构生成“看起来真实”的量子电路，以增强现有的量子电路数据集。

    

    量子算法，表示为量子电路，可用作评估量子系统性能的基准。现有数据集在规模和多样性方面存在限制，在该领域广泛使用，导致研究人员使用随机生成的电路。然而，随机电路并不是代表性基准，因为它们缺乏量子系统制造的真实量子算法的固有属性。这种缺乏“有用”的量子基准构成了推动量子编译器和硬件开发与比较的挑战。本研究旨在通过使用Transformer机器学习架构生成我们称之为“看起来真实”的电路，以增强现有的量子电路数据集。为此，我们引入了KetGPT，一种以OpenQASM语言生成合成电路的工具，其结构是基于推导自量子电路的

    arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
    
[^122]: 通过过滤后验采样激励性探索

    Incentivized Exploration via Filtered Posterior Sampling

    [https://arxiv.org/abs/2402.13338](https://arxiv.org/abs/2402.13338)

    后验采样算法在激励性探索问题中是一种通用解决方案，可以适用于多种扩展设置，为扩展了的IE问题提供了一般性分析。

    

    我们研究了在社交学习问题中的“激励性探索”（IE），其中委托人（推荐算法）可以利用信息不对称性来激励顺序到达的代理采取探索性行动。我们确定了后验采样，这是多臂赌博机文献中众所周知的算法方法，作为IE的通用解决方案。特别是，我们从私人代理类型到信息性推荐再到相关的贝叶斯先验，扩展了IE的现有范围。我们对IE中的后验采样进行了一般性分析，这使我们可以将这些扩展设置纳入为推论，并在恢复现有结果的同时将其视为特殊情况。

    arXiv:2402.13338v1 Announce Type: new  Abstract: We study "incentivized exploration" (IE) in social learning problems where the principal (a recommendation algorithm) can leverage information asymmetry to incentivize sequentially-arriving agents to take exploratory actions. We identify posterior sampling, an algorithmic approach that is well known in the multi-armed bandits literature, as a general-purpose solution for IE. In particular, we expand the existing scope of IE in several practically-relevant dimensions, from private agent types to informative recommendations to correlated Bayesian priors. We obtain a general analysis of posterior sampling in IE which allows us to subsume these extended settings as corollaries, while also recovering existing results as special cases.
    
[^123]: 用于因果混合建模的双机器学习——地球科学应用

    Double machine learning for causal hybrid modeling -- applications in the Earth sciences

    [https://arxiv.org/abs/2402.13332](https://arxiv.org/abs/2402.13332)

    本文引入了一种通过因果推断框架估计混合模型的新方法，利用双机器学习(DML)来估计因果效应，并在地球科学中展示了其在估计因果参数方面优于端到端深度神经网络(DNN)方法的效率、鲁棒性和避免等效性的能力。

    

    arXiv:2402.13332v1 公告类型: 新摘要: 混合建模将机器学习与科学知识相结合，旨在提高解释性、泛化性和遵守自然规律。然而，在混合建模中，等效性和正则化偏差对于实现这些目的构成挑战。本文介绍了一种通过因果推断框架估计混合模型的新方法，特别使用双机器学习(DML)来估计因果效应。我们展示了其在两个涉及二氧化碳通量的地球科学问题上的应用。在$Q_{10}$模型中，我们证明了基于DML的混合建模比端到端深度神经网络(DNN)方法更优，证明了效率高、鲁棒性强，并且规避了正则化方法带来的偏差和等效性。我们的方法应用于碳通量分配，展现了适应不同因果效应的灵活性。这项研究展示了因果混合建模概念在地球科学中的潜力。

    arXiv:2402.13332v1 Announce Type: new  Abstract: Hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emp
    
[^124]: 从场论到庞加莱猜想：机器学习在严谨性方面的应用

    Rigor with Machine Learning from Field Theory to the Poincar\'e Conjecture

    [https://arxiv.org/abs/2402.13321](https://arxiv.org/abs/2402.13321)

    机器学习技术如何在自然科学领域取得严格的结果，通过猜想生成或强化学习验证，解决了在理论物理和数学领域中严谨性与理解之间的矛盾。

    

    机器学习技术变得越来越强大，在自然科学领域取得了许多突破，但它们往往是随机的、容易出错的、且黑匣子一般。那么，它们应该如何在理论物理和纯数学等强调严谨和理解的领域中被利用呢？在这个视角中，我们讨论了如何在自然科学中利用机器学习技术取得严谨性。非严谨的方法可能通过猜想生成或通过强化学习进行验证，从而产生严谨的结果。我们调查了这些技术对严谨性的应用，从弦理论到低维拓扑中的平滑$4$d庞加莱猜想。人们还可以想象在机器学习理论与数学或理论物理之间建立直接桥梁。举例来说，我们描述了一个由神经网络理论激发的场论的新方法，以及一种关于黎曼流形流的理论。

    arXiv:2402.13321v1 Announce Type: cross  Abstract: Machine learning techniques are increasingly powerful, leading to many breakthroughs in the natural sciences, but they are often stochastic, error-prone, and blackbox. How, then, should they be utilized in fields such as theoretical physics and pure mathematics that place a premium on rigor and understanding? In this Perspective we discuss techniques for obtaining rigor in the natural sciences with machine learning. Non-rigorous methods may lead to rigorous results via conjecture generation or verification by reinforcement learning. We survey applications of these techniques-for-rigor ranging from string theory to the smooth $4$d Poincar\'e conjecture in low-dimensional topology. One can also imagine building direct bridges between machine learning theory and either mathematics or theoretical physics. As examples, we describe a new approach to field theory motivated by neural network theory, and a theory of Riemannian metric flows indu
    
[^125]: 有害藻华细胞损害(DSP)的预测：流式学习和批处理学习的比较

    Harmful algal bloom forecasting. A comparison between stream and batch learning

    [https://arxiv.org/abs/2402.13304](https://arxiv.org/abs/2402.13304)

    流式学习是有望解决时间序列问题中概念漂移的最有希望方法之一，本研究对其在预测有害藻华细胞损害方面的效力进行了测试并与批处理学习进行了比较。

    

    有害藻华细胞损害(DSP)是一种全球健康威胁，源于贝类受到甲藻产生的毒素污染。这种疾病由于普遍性发生、高致病率和贝类持续的毒性，对公共卫生和贝类产业构成危险。毒素产生藻类生物量高的情况，如DSP，被称为有害藻华细胞损害(HABs)。监测和预测系统对于减轻HABs影响至关重要。预测有害藻华细胞损害涉及一个以时间序列为基础的问题，其中有一个强烈的历史季节性组成部分，然而，近期由于气象和海洋事件变化而引起的异常现象已被观察到。流式学习是解决具有概念漂移的时间序列问题的最有希望的方法之一。然而，其在预测HABs方面的有效性尚未得到证明，并且需要与批处理学习进行比较测试。

    arXiv:2402.13304v1 Announce Type: cross  Abstract: Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from shellfish contaminated with toxins produced by dinoflagellates. The condition, with its widespread incidence, high morbidity rate, and persistent shellfish toxicity, poses risks to public health and the shellfish industry. High biomass of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs). Monitoring and forecasting systems are crucial for mitigating HABs impact. Predicting harmful algal blooms involves a time-series-based problem with a strong historical seasonal component, however, recent anomalies due to changes in meteorological and oceanographic events have been observed. Stream Learning stands out as one of the most promising approaches for addressing time-series-based problems with concept drifts. However, its efficacy in predicting HABs remains unproven and needs to be tested in comparison with Batch Learning. Historical data ava
    
[^126]: DeepCode AI Fix: 使用大型语言模型修复安全漏洞

    DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models

    [https://arxiv.org/abs/2402.13291](https://arxiv.org/abs/2402.13291)

    使用大型语言模型修复复杂语义bug，通过新的查询和微调方法来解决长距离代码关系学习的挑战。

    

    自动程序修复领域能引起广泛关注，但尽管有大量研究工作，创建一个对于复杂语义错误（如安全漏洞）效果良好的系统仍然很困难。解决这一挑战的一个有前途的方向是利用越来越多地用于解决各种编程任务的大型语言模型（LLMs）。本文研究了LLMs在解决代码修复任务中的有效性。我们表明这个任务很困难，因为它要求模型学习长距离的代码关系，这是一个天然依赖大量训练数据的任务。同时，为复杂程序错误和其对应修复创建一个大型且干净的数据集并不是一个简单的事情。我们提出了一种方法来应对这些挑战，通过一种新的查询和微调LLMs的方法。这个想法是利用程序分析来限制LLMs的关注度。

    arXiv:2402.13291v1 Announce Type: cross  Abstract: The automated program repair field has attracted substantial interest over the years, but despite significant research efforts, creating a system that works well for complex semantic bugs such as security vulnerabilities has proven difficult. A promising direction to solve this challenge is by leveraging large language models (LLMs), which are increasingly used to solve various programming tasks. In this paper, we investigate the effectiveness of LLMs for solving code-repair task. We show that the task is difficult as it requires the model to learn long-range code relationships, a task that inherently relies on extensive amounts of training data. At the same time, creating a large, clean dataset for complex program bugs and their corresponding fixes is non-trivial. We propose a technique to address these challenges with a new approach for querying and fine-tuning LLMs. The idea is to use program analysis to limit the LLM's attention me
    
[^127]: 通过污染批量数据操纵隐马尔可夫模型的推断

    Manipulating hidden-Markov-model inferences by corrupting batch data

    [https://arxiv.org/abs/2402.13287](https://arxiv.org/abs/2402.13287)

    通过污染数据，本研究提供了一种新颖的概率视角，用于操纵隐马尔可夫模型的推断，并开发了多种解决方法进行应用和实证测试。

    

    时间序列模型通常假设数据流是未被污染和合法的。然而，一个自私的对手可能有动机来破坏这些数据，从而改变决策者的推断。在更广泛的对抗机器学习领域，这项研究提供了一种新颖的概率视角，用于通过污染数据来操纵隐马尔可夫模型的推断。具体而言，我们提供了一套污染问题，用于过滤、平滑和解码推断，利用对抗风险分析方法。提出了多个随机规划模型，这些模型结合了现实中的不确定性和不同的攻击者目标。通过从频率主义和贝叶斯角度交替地观察问题，开发了三种一般的解决方法。通过广泛的实证测试，说明了每种方法的有效性。开发的方法以其解决质量为特点

    arXiv:2402.13287v1 Announce Type: cross  Abstract: Time-series models typically assume untainted and legitimate streams of data. However, a self-interested adversary may have incentive to corrupt this data, thereby altering a decision maker's inference. Within the broader field of adversarial machine learning, this research provides a novel, probabilistic perspective toward the manipulation of hidden Markov model inferences via corrupted data. In particular, we provision a suite of corruption problems for filtering, smoothing, and decoding inferences leveraging an adversarial risk analysis approach. Multiple stochastic programming models are set forth that incorporate realistic uncertainties and varied attacker objectives. Three general solution methods are developed by alternatively viewing the problem from frequentist and Bayesian perspectives. The efficacy of each method is illustrated via extensive, empirical testing. The developed methods are characterized by their solution qualit
    
[^128]: 利用 PAC-Bayes 理论和 Gibbs 分布推导带有复杂度度量的泛化界限

    Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization Bounds with Complexity Measures

    [https://arxiv.org/abs/2402.13285](https://arxiv.org/abs/2402.13285)

    本文利用 PAC-Bayes 理论和 Gibbs 分布提出了一个新的泛化界限框架，可适用于任意复杂度度量，允许对泛化差距进行定制化调整。

    

    在统计学习理论中，泛化界限通常涉及由考虑的理论框架施加的复杂度度量。本文利用了分解的 PAC-Bayes 界限框架，推导出一个可实例化为任意复杂度度量的泛化界限。我们的界限以概率同时涵盖假设和学习样本，可以根据泛化差距调整复杂度，因为它可定制以适应假设类和任务。

    arXiv:2402.13285v1 Announce Type: cross  Abstract: In statistical learning theory, a generalization bound usually involves a complexity measure imposed by the considered theoretical framework. This limits the scope of such bounds, as other forms of capacity measures or regularizations are used in algorithms. In this paper, we leverage the framework of disintegrated PAC-Bayes bounds to derive a general generalization bound instantiable with arbitrary complexity measures. One trick to prove such a result involves considering a commonly used family of distributions: the Gibbs distributions. Our bound stands in probability jointly over the hypothesis and the learning sample, which allows the complexity to be adapted to the generalization gap as it can be customized to fit both the hypothesis class and the task.
    
[^129]: 使用SMOTETomek在WSNs中基于机器学习的入侵检测

    MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek in WSNs

    [https://arxiv.org/abs/2402.13277](https://arxiv.org/abs/2402.13277)

    提出了一种将机器学习技术与Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)算法相结合的创新入侵检测方法

    

    无线传感器网络(WSNs)在基础设施中发挥着关键作用，包括固定和移动传感器。这些传感器自组织并建立多跳连接进行通信，共同感知、收集、处理和传输有关周围环境的数据。尽管它们的重要性，WSNs面临着可能破坏功能的快速和有害的攻击。现有的WSN入侵检测方法遇到了低检测率、计算开销和误报警的挑战。这些问题源于传感器节点资源约束、数据冗余以及网络内高相关性。为了解决这些挑战，我们提出了一种创新的入侵检测方法，该方法将机器学习技术与Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)算法相结合。这种融合合成了少数实例并消除了Tomek链接，结果

    arXiv:2402.13277v1 Announce Type: cross  Abstract: Wireless Sensor Networks (WSNs) play a pivotal role as infrastructures, encompassing both stationary and mobile sensors. These sensors self-organize and establish multi-hop connections for communication, collectively sensing, gathering, processing, and transmitting data about their surroundings. Despite their significance, WSNs face rapid and detrimental attacks that can disrupt functionality. Existing intrusion detection methods for WSNs encounter challenges such as low detection rates, computational overhead, and false alarms. These issues stem from sensor node resource constraints, data redundancy, and high correlation within the network. To address these challenges, we propose an innovative intrusion detection approach that integrates Machine Learning (ML) techniques with the Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink) algorithm. This blend synthesizes minority instances and eliminates Tomek links, result
    
[^130]: 全球热带气旋强度预测的多模态多尺度因果自回归模型

    Global Tropical Cyclone Intensity Forecasting with Multi-modal Multi-scale Causal Autoregressive Model

    [https://arxiv.org/abs/2402.13270](https://arxiv.org/abs/2402.13270)

    提出了一种全球热带气旋强度预测模型MSCAR，首次将因果关系与大规模多模态数据相结合，有效优于现有最先进方法。

    

    精确预测热带气旋（TC）强度对于制定灾害风险减少策略至关重要。目前的方法主要依赖于来自ERA5数据的有限时空信息，并忽视这些物理变量之间的因果关系，未能充分捕捉强度预测所需的空间和时间模式。为解决这一问题，作者提出了一种多模态多尺度因果自回归模型（MSCAR），这是第一个将因果关系与大规模多模态数据结合起来进行全球TC强度自回归预测的模型。此外，鉴于目前缺乏一个提供广泛空间变量的TC数据集，我们推出了基于卫星和ERA5的热带气旋数据集（SETCD），它是与TC有关的最长和最全面的全球数据集。对该数据集的实验表明MSCAR胜过了现有的最先进方法。

    arXiv:2402.13270v1 Announce Type: cross  Abstract: Accurate forecasting of Tropical cyclone (TC) intensity is crucial for formulating disaster risk reduction strategies. Current methods predominantly rely on limited spatiotemporal information from ERA5 data and neglect the causal relationships between these physical variables, failing to fully capture the spatial and temporal patterns required for intensity forecasting. To address this issue, we propose a Multi-modal multi-Scale Causal AutoRegressive model (MSCAR), which is the first model that combines causal relationships with large-scale multi-modal data for global TC intensity autoregressive forecasting. Furthermore, given the current absence of a TC dataset that offers a wide range of spatial variables, we present the Satellite and ERA5-based Tropical Cyclone Dataset (SETCD), which stands as the longest and most comprehensive global dataset related to TCs. Experiments on the dataset show that MSCAR outperforms the state-of-the-art
    
[^131]: 通过用户行为建模和随机规划控制大型电动汽车充电站

    Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming

    [https://arxiv.org/abs/2402.13224](https://arxiv.org/abs/2402.13224)

    本文介绍了一个新的电动汽车充电站模型，通过用户行为建模和随机规划，解决了充电会话不确定性问题，并提出了两种方法来优化成本并提高用户满意度。

    

    本文介绍了一个电动汽车充电站（EVCS）模型，该模型融合了真实世界的约束条件，如插槽功率限制、合同阈值超限惩罚以及电动汽车（EVs）的早期断开。我们提出了一个在不确定性下控制EVCS的问题形式，并实施了两种多阶段随机规划方法，利用用户提供的信息，即模型预测控制和二阶段随机规划。该模型解决了充电会话开始和结束时间以及能量需求的不确定性。基于驻留时间依赖随机过程的用户行为模型增强了成本降低的同时保持客户满意度。通过使用真实世界数据集进行的22天模拟展示了两种提出方法相对于两个基线的优势。两阶段方法证明了针对早期断开的鲁棒性，考虑了更多

    arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
    
[^132]: CHILI: 用于推进图机器学习的化学信息的大型无机纳米材料数据集

    CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning

    [https://arxiv.org/abs/2402.13221](https://arxiv.org/abs/2402.13221)

    图机器学习领域目前主要集中在预测分子和材料的目标特性，而尚未达到生成能力与其他领域的水平。

    

    图机器学习的进展主要受化学应用的驱动，因为图一直是分子最具表现力的表示形式。虽然早期的图机器学习方法主要集中在小有机分子上，但最近，图机器学习的范围已经扩展到包括无机材料。建模无机晶体材料的周期性和对称性带来独特挑战，现有的图机器学习方法无法解决。转向无机纳米材料会增加复杂性，因为每个图中节点数量的范围可能很广（$10$到$10^5$）。现有图机器学习的主要重点是通过图作为输入来预测目标特性，来表征分子和材料。但是，图机器学习最激动人心的应用将在其生成能力方面，目前与图像或文本等其他领域还不在同一水平。

    arXiv:2402.13221v1 Announce Type: new  Abstract: Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images or text.   We invite the graph ML community to address th
    
[^133]: 一个统一的量子图神经网络主要框架来自量子图态

    A unifying primary framework for quantum graph neural networks from quantum graph states

    [https://arxiv.org/abs/2402.13001](https://arxiv.org/abs/2402.13001)

    量子图神经网络模型可基于图态理解和实现，可用作参数化的量子电路来表示神经网络，或作为构建量子计算机上的图神经网络的基础结构。

    

    图态被用来将数学图表示为量子计算机上的量子状态。它们可以通过稳定子码或直接的量子门和量子状态来构建。本文展示了量子图神经网络模型可以基于图态加以理解和实现。我们展示了它们可以被用作参数化的量子电路来表示神经网络，或作为构建量子计算机上的图神经网络的基础结构。

    arXiv:2402.13001v1 Announce Type: cross  Abstract: Graph states are used to represent mathematical graphs as quantum states on quantum computers. They can be formulated through stabilizer codes or directly quantum gates and quantum states. In this paper we show that a quantum graph neural network model can be understood and realized based on graph states. We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers.
    
[^134]: PARCv2：物理感知循环卷积神经网络用于时空动力学建模

    PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling

    [https://arxiv.org/abs/2402.12503](https://arxiv.org/abs/2402.12503)

    PARCv2通过引入微分算子扩展了PARC模型，用于模拟不稳定、瞬态和传输主导系统的时空动力学。

    

    arXiv:2402.12503v1 公告类型：新摘要：对不稳定的、快速瞬态和优势传输主导的物理问题进行建模是物理感知深度学习（PADL）面临的迫切挑战。复杂系统的物理由大型偏微分方程（PDEs）系统和带有非线性结构的辅助本构模型所控制，同时还包括表现出急剧梯度和快速变形材料界面的演化状态场。在这里，我们研究了一种多功能且通用的归纳偏见方法，用于模拟通用的非线性场演变问题。我们的研究聚焦于最近的物理感知循环卷积（PARC），它结合了一种区分-积分器结构，归纳地模拟了通用物理系统的时空动力学。我们扩展了PARC的功能，以模拟不稳定、瞬态和传输主导系统。这个扩展模型被称为PARCv2，配备了微分算子来建模

    arXiv:2402.12503v1 Announce Type: new  Abstract: Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model
    
[^135]: 将废料变废为宝：矫正MoE的Top-k路由器

    Turn Waste into Worth: Rectifying Top-$k$ Router of MoE

    [https://arxiv.org/abs/2402.12399](https://arxiv.org/abs/2402.12399)

    提出了Rectify-Router解决了MoE模型中常用的Top-k路由机制所带来的令牌丢失和填充问题，通过Intra-GPU矫正和Fill-in矫正来实现。

    

    稀疏混合专家（MoE）模型因其计算效率而受到欢迎，用于训练大型语言模型。然而，常用的Top-k路由机制由于不平衡的路由导致冗余计算和内存成本过高。一些专家会溢出，其中超出的令牌会被丢弃。而一些专家是空闲的，这些专家会填充为零，负面影响了模型性能。为了解决丢弃令牌和填充问题，我们提出了Rectify-Router，包括Intra-GPU矫正和Fill-in矫正。Intra-GPU矫正处理丢弃的令牌，将它们有效地路由到GPU内的专家，避免跨GPU通信。Fill-in矫正通过用具有高路由分数的令牌替换填充令牌来解决填充问题。我们的实验结果表明，Intra-GPU矫正和Fill-in矫正

    arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati
    
[^136]: 实现基因表达数据科学发现的AI科学家团队

    Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data

    [https://arxiv.org/abs/2402.12391](https://arxiv.org/abs/2402.12391)

    引入了一个名为AI科学家团队（TAIS）的框架，旨在简化科学发现流程，由模拟角色协作，特别关注于识别具有疾病预测价值的基因

    

    机器学习已成为科学发现的强大工具，使研究人员能够从复杂数据集中提取有意义的见解。我们引入了一个新颖的框架，名为AI科学家团队（TAIS），旨在简化科学发现流程。TAIS包括模拟角色，包括项目经理、数据工程师和领域专家，每个角色由大型语言模型（LLM）代表。这些角色协作以复制数据科学家通常执行的任务，特别关注于识别具有疾病预测价值的基因。

    arXiv:2402.12391v1 Announce Type: cross  Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset t
    
[^137]: 模拟失调: 大型语言模型的安全对齐可能会适得其反！

    Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!

    [https://arxiv.org/abs/2402.12343](https://arxiv.org/abs/2402.12343)

    安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。

    

    大型语言模型（LLMs）需要进行安全对齐，以确保与人类进行安全的对话。然而，在这项工作中，我们引入了一种推理时攻击框架，表明安全对齐也可能在对抗性操纵下无意中促成有害结果。这个框架被命名为模拟失调（ED），在输出空间中不良地组合了一对开源预训练和安全对齐的语言模型，产生了一个有害的语言模型而无需任何训练。我们对ED在三个数据集和四个模型系列（Llama-1、Llama-2、Mistral和Alpaca）上的实验表明，ED使预训练模型的有害性增加了一倍，并胜过强基线，以较大优势在48个评估子集中的43个中实现了最高的有害率。至关重要的是，我们的研究结果凸显了即使在安全对齐后，重新评估开源语言模型实践的重要性。

    arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
    
[^138]: 因果平等保护与算法公平性

    Causal Equal Protection as Algorithmic Fairness

    [https://arxiv.org/abs/2402.12062](https://arxiv.org/abs/2402.12062)

    本文提出了一种新的算法公平性原则——平等保护，其关键在于将错误分类的风险均等化，避免了许多对传统分类平等原则的反例。

    

    过去十年，计算机科学和哲学的文献形成了不同的算法公平性标准。其中最受争议的分类平等要求，预测算法的错误分类在被保护特征所指示的群体中以相等频率发生。尽管分类平等具有直观吸引力，但已受到攻击。我们转向一个相关原则，即平等保护，该原则最初是在刑事司法领域发展起来的。平等保护的关键在于将错误分类的风险（将在规定的意义上具体说明）进行均等化，而不是将错误分类的比率均等化。我们展示了平等保护避免了许多对分类平等的反例。

    arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
    
[^139]: Leaky ReLU对超参数网络的训练和泛化的影响

    The effect of Leaky ReLUs on the training and generalization of overparameterized networks

    [https://arxiv.org/abs/2402.11942](https://arxiv.org/abs/2402.11942)

    Leaky ReLU参数$\alpha=-1$在训练误差和泛化误差界方面是最优的选择。

    

    我们研究了具有各种泄漏修正线性单元（ReLU）函数的超参数神经网络（NNs）的训练和泛化误差。更具体地，我们仔细地对这些NNs的训练误差的收敛速率和泛化误差进行了上界估计，并研究了这些界限对Leaky ReLU参数$\alpha$的依赖性。我们表明$\alpha=-1$，对应于绝对值激活函数，对于训练误差界是最优的。此外，在特定设置中，这也是泛化误差界的最优选择。数值实验在实践中支持了理论引导的实际选择。

    arXiv:2402.11942v1 Announce Type: new  Abstract: We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions. More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\alpha$. We show that $\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound. Furthermore, in special settings, it is also optimal for the generalization error bound. Numerical experiments empirically support the practical choices guided by the theory.
    
[^140]: Re-Dock: 朝向具有扩散桥的灵活和现实分子对接

    Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge

    [https://arxiv.org/abs/2402.11459](https://arxiv.org/abs/2402.11459)

    提出了一种新颖的扩散桥生成模型 Re-Dock，用于灵活和现实的分子对接，通过能量到几何映射来共同建模结合能和构象，填补了对接中的实用性和构象预测方面的差距

    

    准确预测蛋白质-配体结合结构，即分子对接任务对于药物设计至关重要，但仍然具有挑战性。尽管深度学习显示出了潜力，但现有方法通常依赖于完整蛋白质结构（对接，且在现实任务中不可达）或忽略口袋侧链构象，导致有限的实用性和不切实际的构象预测。为填补这些差距，我们引入了一个未经探索的任务，命名为柔性对接，以同时预测配体和口袋侧链的姿势，并引入了一种扩展到几何流形的新型扩散桥生成模型 Re-Dock。具体而言，我们提出了受牛顿-欧拉方程启发的能量到几何映射，以共同建模结合能和构象，以反映能量约束对接生成过程。我们在设计的基准数据集上进行了全面的实验，包括apo-dock和cross-dock d

    arXiv:2402.11459v1 Announce Type: cross  Abstract: Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock d
    
[^141]: 基于信任区域的黑盒概率认证解释

    Trust Regions for Explanations via Black-Box Probabilistic Certification

    [https://arxiv.org/abs/2402.11168](https://arxiv.org/abs/2402.11168)

    通过黑盒概率认证解释的信任区域能够有效地洞察模型行为、保证解释的稳定性，并实现解释的重用

    

    由于机器学习模型的黑盒性质，人们开发了大量的可解释性方法来解析个别决策背后的因素。本文提出了一个新颖的黑盒（概率性）解释认证问题。我们提出了一个问题：给定一个黑盒模型，只有查询访问权，一个示例的解释以及一个质量度量（如逼真度、稳定性），我们是否能找到最大的超立方体（即 $\ell_{\infty}$ 球），以示例为中心，使得当解释被应用于超立方体内的所有示例时（高概率下）质量标准得到满足（比如逼真度高于某个值）？能够高效地找到这样一个信任区域有多重好处：i）洞察模型在一个区域内的行为，具有保证；ii）解释的稳定性得到保证；iii）解释的重用，可以节省时间、精力和金钱。

    arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
    
[^142]: 加速半异步联邦学习

    Accelerating Semi-Asynchronous Federated Learning

    [https://arxiv.org/abs/2402.10991](https://arxiv.org/abs/2402.10991)

    提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。

    

    联邦学习（FL）是一种分布式机器学习范例，允许客户端在保护隐私的同时在其数据上训练模型。现有的FL算法，如Federated Averaging（FedAvg）及其变种，在许多情况下已经被证明收敛良好。然而，这些方法需要客户端以同步方式将其本地更新上传至服务器，这在现实情况下可能会变得缓慢和不可靠。为了解决这个问题，研究人员开发了异步FL方法，允许客户端继续使用陈旧的全局模型对其本地数据进行训练。然而，大多数这些方法仅仅聚合了所有接收到的更新，而没有考虑其相对贡献，这可能导致收敛速度变慢。在本文中，我们提出了一种考虑贡献的异步FL方法，考虑了接收到的更新的陈旧程度和统计异质性。我们的方法动态调整

    arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
    
[^143]: CHEMREASONER：使用量子化学反馈在大型语言模型的知识空间中进行启发式搜索

    CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback

    [https://arxiv.org/abs/2402.10980](https://arxiv.org/abs/2402.10980)

    通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索

    

    arXiv:2402.10980v1 类型公告：跨领域 摘要：发现新的催化剂对于设计新的更高效的化学过程至关重要，以实现向可持续未来的过渡。我们引入了一种人工智能引导的计算筛选框架，将语言推理与基于量子化学的三维原子表示的反馈统一起来。我们的方法将催化剂发现构建为一个不确定环境，其中一个代理通过大型语言模型（LLM）推导的假设与基于原子图神经网络（GNN）的反馈的迭代组合，积极搜索高效催化剂。在中间搜索步骤确定的催化剂经过基于空间定向、反应途径和稳定性的结构评估。基于吸附能和势垒的评分函数引导在LLM的知识空间中向能量有利、高效的催化剂探索。我们引入了可以自动规划的方法

    arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
    
[^144]: 在一个 1000 万根草垛中寻找针：循环记忆找到了语言模型不擅长的内容

    In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss

    [https://arxiv.org/abs/2402.10790](https://arxiv.org/abs/2402.10790)

    通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理长达 1000 万个元素的任务，这是迄今为止处理最长输入的开放神经网络模型，并展示了对长序列处理能力的显著改进。

    

    本文解决了使用生成式 Transformer 模型处理长文档的挑战。为了评估不同方法，我们引入了 BABILong，这是一个新的基准，旨在评估模型在提取和处理广泛文本中分布式事实方面的能力。我们的评估包括 GPT-4 和 RAG 的基准，结果显示常见方法仅适用于最多 $10^4$ 个元素的序列。相反，通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理涉及最多 $10^7$ 个元素的任务。这一成就标志着迄今为止任何开源神经网络模型处理的最长输入，显示了对长序列处理能力的显著改进。

    arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
    
[^145]: Johnson-Lindenstrauss的简单统一分析及其应用

    Simple, unified analysis of Johnson-Lindenstrauss with applications

    [https://arxiv.org/abs/2402.10232](https://arxiv.org/abs/2402.10232)

    这项工作提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，简化和统一了各种构造，包括球形、高斯、二进制硬币和次高斯模型，通过创新性地将Hanson-Wright不等式拓展到高维度，标志着对数据固有几何的保持取得重大进展。

    

    在这项工作中，我们提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，这是处理高维数据至关重要的降维领域中的基石。我们的方法不仅简化了理解，还将各种构造统一到JL框架下，包括球形、高斯、二进制硬币和次高斯模型。这种简化和统一在保持数据固有几何的重要性方面取得了重大进展，对从流算法到强化学习等各种应用至关重要。值得注意的是，我们在这个简化框架内提出了球形构造有效性的第一个严格证明。我们贡献的核心是将Hanson-Wright不等式拓展到高维度，具有明确的常数，这标志着文献中质的飞跃。通过运用简单而强大的概率工具

    arXiv:2402.10232v1 Announce Type: new  Abstract: In this work, we present a simple and unified analysis of the Johnson-Lindenstrauss (JL) lemma, a cornerstone in the field of dimensionality reduction critical for managing high-dimensional data. Our approach not only simplifies the understanding but also unifies various constructions under the JL framework, including spherical, Gaussian, binary coin, and sub-Gaussian models. This simplification and unification make significant strides in preserving the intrinsic geometry of data, essential across diverse applications from streaming algorithms to reinforcement learning. Notably, we deliver the first rigorous proof of the spherical construction's effectiveness within this simplified framework. At the heart of our contribution is an innovative extension of the Hanson-Wright inequality to high dimensions, complete with explicit constants, marking a substantial leap in the literature. By employing simple yet powerful probabilistic tools and 
    
[^146]: 异构无线网络中具有独立采样的自适应联邦学习

    Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling

    [https://arxiv.org/abs/2402.10097](https://arxiv.org/abs/2402.10097)

    这项研究提出了一种适用于异构无线网络的自适应联邦学习方法，其中包括了独立客户端采样和带宽分配方案，以提高训练效率和适应数据和系统的异构特性。

    

    联邦学习算法通常通过对客户端进行随机子集采样来解决迟到者问题并提高通信效率。然而，最近的研究在联合系统和数据异构设计方面存在一些限制，可能与实际的异构无线网络不一致。在本文中，我们提倡一种新的独立客户端采样策略，以最小化联邦学习的实际训练时间，同时考虑通信和计算中的数据异构性和系统异构性。我们首先推导了带有独立客户端采样的非凸损失函数的新收敛界限，然后提出了一种自适应带宽分配方案。此外，我们还提出了一种基于收敛轮数上界和每轮预期训练时间的高效独立客户端采样算法，以最小化联邦学习的实际训练时间，同时考虑数据异构性和系统异构性。

    arXiv:2402.10097v1 Announce Type: new  Abstract: Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while consider
    
[^147]: 用户建模与用户画像：综述

    User Modeling and User Profiling: A Comprehensive Survey

    [https://arxiv.org/abs/2402.09660](https://arxiv.org/abs/2402.09660)

    这篇综述论文介绍了用户建模与用户画像研究的现状、发展和未来方向。该研究主要关注在人工智能应用中构建准确的用户表示，包括利用大量数据进行建模以及采用深度学习和图数据技术等先进方法。

    

    人工智能（AI）融入日常生活，特别是通过信息检索和推荐系统，已经促使先进的用户建模和用户画像技术，以提供个性化体验。这些技术旨在基于与这些系统的互动中生成的大量数据构建准确的用户表示。本文对用户建模和用户画像研究的现状、发展和未来方向进行了全面综述。我们提供了一个历史概述，追溯了从早期的刻板模型到最新的深度学习技术，并提出了一个新的分类体系，涵盖了这一研究领域中的所有活动主题，包括最近的趋势。我们的综述突出了向更复杂的用户画像方法的范式转变，强调了隐式数据收集、多行为建模以及图数据的整合。

    arXiv:2402.09660v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data
    
[^148]: 证据深度学习方法是否准确地表示认识不确定性？

    Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?

    [https://arxiv.org/abs/2402.09056](https://arxiv.org/abs/2402.09056)

    本论文提出了关于证据深度学习的新理论洞见, 高亮了在优化二阶损失函数和解释得出的认识不确定性度量上的困难性

    

    可信的机器学习系统不仅应返回准确的预测结果，还应提供可靠的不确定性表示。贝叶斯方法常用于量化不确定性，但近年来，证据深度学习方法等替代方法也变得流行起来。后者本质上扩展了经验风险最小化（ERM），用于预测结果的二阶概率分布，从中可以提取认识（和随机）不确定性的度量。本文提供了证据深度学习的新理论洞见，强调了优化二阶损失函数以及解释结果认识不确定性度量的困难性。通过系统化的设置，涵盖了分类、回归和计数的广泛方法，这篇论文为可辨识性和收敛性问题提供了新的洞察。

    arXiv:2402.09056v1 Announce Type: new Abstract: Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-o
    
[^149]: SAGMAN: 用于图神经网络在流形上的稳定性分析的方法

    SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds

    [https://arxiv.org/abs/2402.08653](https://arxiv.org/abs/2402.08653)

    SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。

    

    现代图神经网络（GNN）对输入图结构和节点特征的变化敏感，可能导致不可预测的行为和性能下降。本文引入了一种称为SAGMAN的谱框架，用于检验GNN的稳定性。该框架评估非线性映射中GNN在输入和输出流形之间引起的距离失真: 当输入流行中两个附近的节点（通过GNN模型）被映射到输出流行上的两个远离的节点时，意味着存在较大的距离失真，从而导致GNN的稳定性较差。我们提出了一种距离保持的图降维（GDR）方法，利用谱图嵌入和概率图模型（PGMs）来创建低维的输入/输出基于图的流形，以进行有意义的稳定性分析。我们的实证评估表明，SAGMAN能够有效评估每个节点在面对不同边缘或特征扰动时的稳定性。

    Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
    
[^150]: 可扩展大型语言模型微调的差分隐私零阶方法

    Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning

    [https://arxiv.org/abs/2402.07818](https://arxiv.org/abs/2402.07818)

    本文研究了差分隐私零阶方法在大型语言模型微调中的应用，该方法通过使用零阶梯度来避免传统优化方法的可扩展性瓶颈，实现了在隐私、效用和可扩展性之间的良好平衡。

    

    在特定任务的数据集上进行微调是利用预训练语言模型的强大能力进行各种下游任务的广泛接受的范例。由于预训练语言模型微调的普及以及与之相关的隐私问题，差分隐私预训练语言模型微调引起了越来越多的关注，以保护特定任务数据集的隐私。差分隐私预训练语言模型微调方法的设计核心是在隐私、效用和可扩展性之间达到满意的权衡。大多数现有方法都是基于DP-SGD的创新性工作。尽管将DP-SGD的可扩展性推到了极限，但基于DP-SGD的微调方法不幸地受到了SGD固有低效率的限制。在本文中，我们研究了DP零阶方法在LLM预训练中的潜力，该方法通过用更高效的零阶梯度来近似梯度，避免了SGD的可扩展性瓶颈。与将零阶方法作为一种替代方法进行处理不同，我们引入了一种新的割接框架，该框架能够以非常接近的方式模拟DP-SGD的基本操作，然后利用零阶优化方法来近似梯度。

    Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
    
[^151]: 重新思考战略环境中学习的比例定律

    Rethinking Scaling Laws for Learning in Strategic Environments

    [https://arxiv.org/abs/2402.07588](https://arxiv.org/abs/2402.07588)

    本文重新思考了在战略环境中学习的比例定律，发现战略互动可以打破传统的观点，即模型越大或表达能力越强并不一定会随之提高性能。通过几个战略环境的例子，我们展示了这种现象的影响。

    

    越来越大的机器学习模型的部署反映出一个共识：模型越有表达能力，越拥有大量数据，就能改善性能。随着模型在各种真实场景中的部署，它们不可避免地面临着战略环境。本文考虑了模型与战略互动对比例定律的相互作用对性能的影响这个自然问题。我们发现战略互动可以打破传统的比例定律观点，即性能并不一定随着模型的扩大和/或表达能力的增强（即使有无限数据）而单调提高。我们通过战略回归、战略分类和多智能体强化学习的例子展示了这一现象的影响，这些例子展示了战略环境中的限制模型或策略类的表达能力即可。

    The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
    
[^152]: 阈值Oja是否适用于稀疏PCA？

    Thresholded Oja does Sparse PCA?

    [https://arxiv.org/abs/2402.07240](https://arxiv.org/abs/2402.07240)

    阈值和重新归一化Oja算法的输出可获得一个接近最优的错误率，与未经阈值处理的Oja向量相比，这大大减小了误差。

    

    我们考虑了当比值$d/n \rightarrow c > 0$时稀疏主成分分析（PCA）的问题。在离线设置下，关于稀疏PCA的最优率已经有很多研究，其中所有数据都可以用于多次传递。相比之下，当人口特征向量是$s$-稀疏时，具有$O(d)$存储和$O(nd)$时间复杂度的流算法通常要求强初始化条件，否则会有次优错误。我们展示了一种简单的算法，对Oja算法的输出（Oja向量）进行阈值和重新归一化，从而获得接近最优的错误率。这非常令人惊讶，因为没有阈值，Oja向量的误差很大。我们的分析集中在限制未归一化的Oja向量的项上，这涉及将一组独立随机矩阵的乘积在随机初始向量上的投影。 这是非平凡且新颖的，因为以前的Oja算法分析没有考虑这一点。

    arXiv:2402.07240v2 Announce Type: cross  Abstract: We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \rightarrow c > 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's al
    
[^153]: TREET: 基于Transformer的传输熵估计

    TREET: TRansfer Entropy Estimation via Transformer

    [https://arxiv.org/abs/2402.06919](https://arxiv.org/abs/2402.06919)

    本研究提出了TREET，一种基于Transformer的传输熵估计方法，通过引入Donsker-Vardhan表示法和注意力机制，实现了对稳定过程的传输熵估计。我们设计了估计TE的优化方案，并展示了通过联合优化方案优化通信通道容量和估计器的记忆能力。

    

    传输熵（TE）是信息论中揭示过程之间信息流动方向的度量，对各种实际应用提供了宝贵的见解。本研究提出了一种名为TREET的基于Transformer的传输熵估计方法，用于估计稳定过程的TE。所提出的方法利用Donsker-Vardhan（DV）表示法对TE进行估计，并利用注意力机制进行神经估计任务。我们对TREET进行了详细的理论和实证研究，并将其与现有方法进行了比较。为了增加其适用性，我们设计了一种基于功能表示引理的估计TE优化方案。之后，我们利用联合优化方案来优化具有记忆性的通信通道容量，这是信息论中的一个典型优化问题，并展示了我们估计器的记忆能力。

    Transfer entropy (TE) is a measurement in information theory that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel transformer-based approach for estimating the TE for stationary processes. The proposed approach employs Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma. Afterwards, we take advantage of the joint optimization scheme to optimize the capacity of communication channels with memory, which is a canonical optimization problem in information theory, and show the memory capabilities of our estimator. Finally, we apply
    
[^154]: 基于触觉的从颗粒介质中检索物体的研究

    Tactile-based Object Retrieval From Granular Media

    [https://arxiv.org/abs/2402.04536](https://arxiv.org/abs/2402.04536)

    这项研究介绍了一种基于触觉反馈的机器人操作方法，用于在颗粒介质中检索埋藏的物体。通过模拟传感器噪声进行端到端训练，实现了自然出现的学习推动行为，并成功将其迁移到实际硬件上。

    

    我们介绍了一种名为GEOTACT的机器人操作方法，能够在颗粒介质中检索埋藏的物体。这是一项具有挑战性的任务，因为需要与颗粒介质进行交互，并且仅依靠触觉反馈来完成，因为一个埋藏的物体可能完全被视觉隐藏。在这种环境中，触觉反馈本身具有挑战性，因为需要与周围介质进行普遍接触，并且由触觉读数引起的固有噪声水平。为了解决这些挑战，我们使用了一种通过模拟传感器噪声进行端到端训练的学习方法。我们展示了我们的问题表述导致了学习推动行为的自然出现，操作器使用这些行为来减少不确定性并将物体引导到稳定的抓取位置，尽管存在假的和噪声的触觉读数。我们还引入了一种培训方案，可以在仿真中学习这些行为，并在实际硬件上进行零样本迁移。据我们所知，GEOTACT是第一个这样的方法。

    We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first meth
    
[^155]: 打破数据孤岛：跨领域学习实现独立私有源的多智能体感知

    Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources

    [https://arxiv.org/abs/2402.04273](https://arxiv.org/abs/2402.04273)

    本文提出了一种名为FDA的框架，通过跨领域学习来打破多智能体感知中的数据孤岛问题。该框架包括可学习特征补偿模块和分布感知统计一致性模块，用于增强中间特征交流和数据分布一致性。

    

    多智能体感知系统中的不同智能体可能来自不同公司。每个公司可能都使用相同的经典神经网络结构的编码器进行特征提取。然而，训练各个智能体的数据源在每个公司中是相互独立和私有的，导致多智能体感知系统中训练不同智能体的不同私有数据的分布差异。上述分布差异造成的数据孤岛可能导致多智能体感知性能明显下降。本文深入研究了分布差异对现有多智能体感知系统的影响。为了打破数据孤岛，我们引入了面向跨领域学习的特征分布感知聚合（FDA）框架，以减轻多智能体感知中的分布差异。FDA包括两个关键组成部分：可学习特征补偿模块和分布感知统计一致性模块，旨在加强中间特征交流和数据分布一致性。

    The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing interme
    
[^156]: SEABO: 一种简单的基于搜索的离线模仿学习方法

    SEABO: A Simple Search-Based Method for Offline Imitation Learning

    [https://arxiv.org/abs/2402.03807](https://arxiv.org/abs/2402.03807)

    SEABO是一种简单而有效的基于搜索的离线模仿学习方法，它以无监督学习的方式，根据专家数据和无标签数据得到奖励函数，实验结果表明其性能与离线强化学习相当。

    

    离线强化学习（RL）由于能够从静态离线数据集中学习并消除与环境交互的需求，受到了广泛关注。然而，离线RL的成功在很大程度上取决于标有奖励标签的离线转换。在实践中，我们经常需要手工设计奖励函数，这有时是困难的、劳动密集的或低效的。为了解决这个挑战，我们把重点放在离线模仿学习（IL）设置上，旨在基于专家数据和无标签数据得到一个奖励函数。为此，我们提出了一种简单但有效的基于搜索的离线IL方法，称为SEABO。SEABO以无监督学习的方式，将较大的奖励分配给与专家演示中最接近的转换，否则分配较小的奖励。在多个D4RL数据集上的实验结果表明，SEABO能够达到与离线RL相当的性能水平。

    Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL 
    
[^157]: MolTC: 在语言模型中进行分子关系建模

    MolTC: Towards Molecular Relational Modeling In Language Models

    [https://arxiv.org/abs/2402.03781](https://arxiv.org/abs/2402.03781)

    本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。

    

    分子关系学习（MRL）旨在理解分子之间的相互作用，在推进生物化学研究方面起到了关键作用。最近，大型语言模型（LLMs）的采用已成为一种有效和高效的MRL方法，这些模型以其庞大的知识存储库和先进的逻辑推理能力而闻名。尽管具有潜力，但这些方法主要依赖于文本数据，因此没有充分利用分子图中固有的丰富结构信息。此外，缺乏统一的框架加剧了信息的浪费，因为它阻碍了在不同数据集之间共享学习到的相互作用理由。为了解决这些挑战，本研究提出了一种基于LLM的多模态框架，用于根据思维链（CoT）理论对分子相互作用进行预测，称为MolTC，它可以高效地整合分子对的丰富图形信息。

    Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
    
[^158]: 基于图像渲染的去噪扩散

    Denoising Diffusion via Image-Based Rendering

    [https://arxiv.org/abs/2402.03445](https://arxiv.org/abs/2402.03445)

    该论文介绍了一个新的基于图像渲染的去噪扩散模型，可以快速重构和生成真实世界3D场景，通过引入新的神经场景表示方法和扩散建模方法来实现。

    

    生成3D场景是一个具有挑战性的问题，需要合成在三维空间中完全一致的可信内容。最近的方法如神经辐射场在视图合成和3D重构方面表现出色，但由于缺乏生成能力，它们无法合成未观察区域中的可信细节。相反，现有的生成方法通常不能在野外重构具有详细的大规模场景，因为它们使用容量有限的3D场景表示，需要对齐的相机姿态或依赖额外的正则化器。在这项工作中，我们引入了首个能够快速进行详细重构和生成真实世界3D场景的扩散模型。为了实现这一点，我们提出了三个贡献。首先，我们引入了一种新的神经场景表示——IB-planes，可以高效准确地表示大规模3D场景，并根据每张图像中可见的细节动态分配更多容量。其次，我们提供了一种新的扩散建模方法。

    Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability. Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers. In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes. To achieve this, we make three contributions. First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image. Second, we 
    
[^159]: 利用类别概率进行黑盒子句级攻击

    Exploiting Class Probabilities for Black-box Sentence-level Attacks

    [https://arxiv.org/abs/2402.02695](https://arxiv.org/abs/2402.02695)

    该论文研究了在黑盒子句级攻击中利用类别概率的有效性，并开发了一种新的算法进行攻击。通过与基线方法进行对比，进行了广泛的评估。

    

    句级攻击是针对文本分类器的对抗性句子生成方法，这些句子与正确分类的句子同义，但被分类器错误地分类。在黑盒设置下，分类器只能通过对查询输入的反馈进行访问，这主要以类别概率的形式提供。尽管利用类别概率可以获得更强大的攻击效果，但由于在句级攻击中使用类别概率存在挑战，现有的攻击方法要么不使用反馈，要么仅使用类别标签。为了克服这些挑战，我们开发了一种新的算法，使用类别概率进行黑盒句级攻击，并研究了在攻击成功率上使用类别概率的有效性，并探讨了在黑盒句级攻击中使用类别概率是否值得或可行。我们在各种分类器和基准数据集上对提出的攻击方法进行了广泛评估，并与基线进行了对比。

    Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
    
[^160]: 具有内聚子图意识的图对比学习

    Graph Contrastive Learning with Cohesive Subgraph Awareness

    [https://arxiv.org/abs/2401.17580](https://arxiv.org/abs/2401.17580)

    本研究提出了一种名为CTAug的新框架，将内聚子图意识无缝整合到图对比学习中。通过改进图拓扑增强和图学习过程，提高了对各种图的表征学习性能。

    

    图对比学习（GCL）已成为学习各种图表征的先进策略，包括社交和生物医学网络。GCL广泛使用随机图拓扑增强，如均匀节点丢失，生成增强图。然而，这种随机增强可能严重损害图的内在属性并恶化后续的表征学习过程。我们认为，在图增强和学习过程中引入内聚子图意识有可能提高GCL的性能。为此，我们提出了一种称为CTAug的新颖统一框架，以无缝地将内聚意识整合到各种现有的GCL机制中。具体来说，CTAug包括两个专门的模块：拓扑增强增强和图学习增强。前者生成谨慎保留内聚性质的增强图，而后者增强了图的学习能力。

    Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the grap
    
[^161]: 在离线动力学强化学习中的少样本转移的保守方法

    A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning

    [https://arxiv.org/abs/2312.15474](https://arxiv.org/abs/2312.15474)

    提出了一种受最近模仿学习和保守RL算法进展启发的创新方法，在离线动力学强化学习中的少样本转移过程中引入惩罚来调节源训练策略生成的轨迹。

    

    离线动力学强化学习（ODRL）旨在将策略从源环境转移到具有不同但相似动力学特征的目标环境。在这种情况下，传统RL代理过度依赖源环境的动力学，导致发现在该环境中表现卓越的策略，但在目标环境中表现不佳。在少样本框架中，引入了来自目标环境的有限数量转换以促进更有效的转移。为了解决这一挑战，我们提出了一种受最近模仿学习和保守RL算法进展启发的创新方法。所提出的方法引入了一个惩罚来调节源训练策略生成的轨迹。我们在代表不同离线动力学条件的各种环境中评估了我们的方法，在这些环境中访问目标环境是极端困难的。

    arXiv:2312.15474v2 Announce Type: replace  Abstract: Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from a source environment to a target environment characterized by distinct yet similar dynamics. In this context, traditional RL agents depend excessively on the dynamics of the source environment, resulting in the discovery of policies that excel in this environment but fail to provide reasonable performance in the target one. In the few-shot framework, a limited number of transitions from the target environment are introduced to facilitate a more effective transfer. Addressing this challenge, we propose an innovative approach inspired by recent advancements in Imitation Learning and conservative RL algorithms. The proposed method introduces a penalty to regulate the trajectories generated by the source-trained policy. We evaluate our method across various environments representing diverse off-dynamics conditions, where access to the target environment is extreme
    
[^162]: 面向生成人工智能的消息代理：调研、挑战和机遇

    Towards Message Brokers for Generative AI: Survey, Challenges, and Opportunities

    [https://arxiv.org/abs/2312.14647](https://arxiv.org/abs/2312.14647)

    该研究调查了传统和现代消息代理，比较分析了流行平台，为数据中心GenAI模型的需求增加提供了健壮的数据通信基础设施

    

    在当今数字化世界中，生成人工智能（GenAI）如大型语言模型（LLMs）正变得越来越普遍，扩展其影响范围至各种应用。这种采用激增引发了对数据中心GenAI模型的需求显著增加，突显出健壮的数据通信基础设施的必要性。消息代理在这一需求中至关重要，它们作为各个系统组件之间数据传输的重要通道。本调查旨在深入分析传统和现代消息代理，提供流行平台的比较研究。我们的研究考虑了许多标准，包括但不限于开源可用性、集成监控工具、消息优先级机制、并行处理能力、可靠性、分发和集群功能、认证流程、数据持久化。

    arXiv:2312.14647v2 Announce Type: replace-cross  Abstract: In today's digital world, Generative Artificial Intelligence (GenAI) such as Large Language Models (LLMs) is becoming increasingly prevalent, extending its reach across diverse applications. This surge in adoption has sparked a significant increase in demand for data-centric GenAI models, highlighting the necessity for robust data communication infrastructures. Central to this need are message brokers, which serve as essential channels for data transfer within various system components. This survey aims to delve into a comprehensive analysis of traditional and modern message brokers, offering a comparative study of prevalent platforms. Our study considers numerous criteria including, but not limited to, open-source availability, integrated monitoring tools, message prioritization mechanisms, capabilities for parallel processing, reliability, distribution and clustering functionalities, authentication processes, data persistence
    
[^163]: 通过无交并的泛深度比较机器学习算法

    Comparing Machine Learning Algorithms by Union-Free Generic Depth

    [https://arxiv.org/abs/2312.12839](https://arxiv.org/abs/2312.12839)

    本研究提出了一种描述性分析偏序集合的框架，通过改进的无交并泛深度 (ufg) 比较机器学习算法，并在标准基准数据集上提供了示例。研究结果展示了基于ufg方法的多样性分析方法，并与现有的基准测试方法有很大区别。

    

    我们提出了一个基于深度函数概念的描述性分析偏序集合的框架。尽管线性空间和度量空间的研究非常深入，但关于偏序集合等非标准数据类型的深度函数的讨论几乎没有。我们介绍了一种适用于所有偏序集合的著名简单深度的改进版本，无交并泛深度 (ufg)。此外，我们利用我们的ufg深度来比较基于多维性能指标的机器学习算法。具体而言，我们提供了两个示例，对标准基准数据集的分类器比较。我们的结果有希望地展示了基于ufg方法的不同分析方法的广泛多样性。此外，这些示例说明了我们的方法与现有的基准测试方法有很大区别，因此为分类器比较的热烈讨论增添了新的视角。

    We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we provide two examples of classifier comparisons on samples of standard benchmark data sets. Our results demonstrate promisingly the wide variety of different analysis approaches based on ufg methods. Furthermore, the examples outline that our approach differs substantially from existing benchmarking approaches, and thus adds a new perspective to the vivid debate on classifier comparison.
    
[^164]: 通过迭代本地扩展实现高效可扩展的图生成

    Efficient and Scalable Graph Generation through Iterative Local Expansion

    [https://arxiv.org/abs/2312.11529](https://arxiv.org/abs/2312.11529)

    通过逐步扩展单个节点到目标图的方法，避免了对所有节点对的整个联合分布进行建模，实现了高效可扩展的图生成，同时通过多尺度生成保持了高表达性。

    

    在图的生成模型领域，进行了大量研究。然而，由于代表所有节点对的整个联合分布的复杂性以及同时捕捉全局和局部图结构，大多数现有方法在处理大型图时存在困难。为了克服这些问题，我们引入了一种方法，通过逐步将单个节点扩展到目标图来生成图。在每一步中，通过去噪扩散以本地化方式添加节点和边，首先构建全局结构，然后细化局部细节。局部生成避免了对所有节点对上的整个联合分布进行建模，相对于节点数而言实现了大幅的计算节约，并通过多尺度生成保持了高表达性。我们的实验表明，我们的模型在公认的基准数据集上实现了最先进的性能。

    arXiv:2312.11529v2 Announce Type: replace-cross  Abstract: In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established b
    
[^165]: 迈向面向上下文感知领域泛化：理解边缘传递学习的好处和限制

    Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning

    [https://arxiv.org/abs/2312.10107](https://arxiv.org/abs/2312.10107)

    分析了上下文感知领域泛化的条件，提出了理论分析和实证分析所需的标准，并展示了该方法可以检测非常数域的场景。

    

    在这项工作中，我们分析了关于输入$X$的上下文信息如何改善深度学习模型在新领域中的预测的条件。在领域泛化中边缘传递学习的研究基础上，我们将上下文的概念形式化为一组数据点的排列不变表示，这些数据点来自于与输入本身相同的域。我们对这种方法在原则上可以产生好处的条件进行了理论分析，并制定了两个在实践中可以轻松验证的必要标准。此外，我们提供了关于边缘传递学习方法有望具有稳健性的分布变化类型的见解。实证分析表明我们的标准有效地区分了有利和不利的场景。最后，我们证明可以可靠地检测模型面临非常数域的场景。

    arXiv:2312.10107v2 Announce Type: replace-cross  Abstract: In this work, we analyze the conditions under which information about the context of an input $X$ can improve the predictions of deep learning models in new domains. Following work in marginal transfer learning in Domain Generalization (DG), we formalize the notion of context as a permutation-invariant representation of a set of data points that originate from the same domain as the input itself. We offer a theoretical analysis of the conditions under which this approach can, in principle, yield benefits, and formulate two necessary criteria that can be easily verified in practice. Additionally, we contribute insights into the kind of distribution shifts for which the marginal transfer learning approach promises robustness. Empirical analysis shows that our criteria are effective in discerning both favorable and unfavorable scenarios. Finally, we demonstrate that we can reliably detect scenarios where a model is tasked with unw
    
[^166]: 通过扩散模型重建声场

    Reconstruction of Sound Field through Diffusion Models

    [https://arxiv.org/abs/2312.08821](https://arxiv.org/abs/2312.08821)

    本文提出了一种用于在房间内重建声场的数据驱动生成模型，通过条件化去噪扩散概率模型训练和生成声场，实现了准确的重建。

    

    在房间内重建声场是几种应用中的一项重要任务，比如声控、增强现实（AR）或虚拟现实（VR）。本文提出了一种数据驱动的生成模型，用于重建房间内声场的声学场振幅，重点关注模态频率范围。我们首次引入条件化去噪扩散概率模型（DDPM）的使用，经过训练用于在扩展领域内重建声场（SF-Diff）。该架构被设计为在一组不同频率的有限可用测量条件下，生成目标未知位置的声场。结果表明，SF-Diff能够提供准确的重建，优于基于核插值的现有基线模型。

    arXiv:2312.08821v2 Announce Type: replace-cross  Abstract: Reconstructing the sound field in a room is an important task for several applications, such as sound control and augmented (AR) or virtual reality (VR). In this paper, we propose a data-driven generative model for reconstructing the magnitude of acoustic fields in rooms with a focus on the modal frequency range. We introduce, for the first time, the use of a conditional Denoising Diffusion Probabilistic Model (DDPM) trained in order to reconstruct the sound field (SF-Diff) over an extended domain. The architecture is devised in order to be conditioned on a set of limited available measurements at different frequencies and generate the sound field in target, unknown, locations. The results show that SF-Diff is able to provide accurate reconstructions, outperforming a state-of-the-art baseline based on kernel interpolation.
    
[^167]: 论多维局部差分隐私对公平性的影响

    On the Impact of Multi-dimensional Local Differential Privacy on Fairness

    [https://arxiv.org/abs/2312.04404](https://arxiv.org/abs/2312.04404)

    多维局部差分隐私在减少不公平性方面表现出高效，而多维方法及结果分布均对公平性产生影响。

    

    自动决策系统越来越多地用于在人们的生活中做出重要决策。由于操纵数据的敏感性以及由此产生的决策，需要解决一些伦理关切以便适当地使用这些技术，特别是公平性和隐私性。与之前关注单一敏感属性的集中式差分隐私（DP）或局部差分隐私（LDP）的工作不同，本文研究了多个敏感属性（即多维数据）存在时 LDP 对公平性的影响。对合成和基准数据集进行了详细的实证分析，揭示了非常相关的观察结果。特别是，（1）多维 LDP 是减少差距的有效方法，（2）在低隐私保证下，LDP 的多维方法（独立 vs. 组合）很重要，（3）结果 Y 分布对哪个群体有重要影响。

    arXiv:2312.04404v3 Announce Type: replace  Abstract: Automated decision systems are increasingly used to make consequential decisions in people's lives. Due to the sensitivity of the manipulated data as well as the resulting decisions, several ethical concerns need to be addressed for the appropriate use of such technologies, in particular, fairness and privacy. Unlike previous work, which focused on centralized differential privacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper, we examine the impact of LDP in the presence of several sensitive attributes (i.e., multi-dimensional data) on fairness. Detailed empirical analysis on synthetic and benchmark datasets revealed very relevant observations. In particular, (1) multi-dimensional LDP is an efficient approach to reduce disparity, (2) the multi-dimensional approach of LDP (independent vs. combined) matters only at low privacy guarantees, and (3) the outcome Y distribution has an important effect on which group
    
[^168]: 隐蔽而可量化：使用随机试验的混淆强度下界

    Hidden yet quantifiable: A lower bound for confounding strength using randomized trials

    [https://arxiv.org/abs/2312.03871](https://arxiv.org/abs/2312.03871)

    利用随机试验设计了一种统计检验，能够量化未观察到的混淆强度，并估计其下界，有效应用于现实世界中识别混淆。

    

    在快节奏精准医学时代，观察性研究在正确评估临床实践中新疗法方面发挥着重要作用。然而，未观察到的混淆可能严重损害从非随机数据中得出的因果结论。我们提出了一种利用随机试验来量化未观察到的混淆的新策略。首先，我们设计了一种统计检验来检测强度超过给定阈值的未观察到的混淆。然后，我们使用该检验来估计未观察到的混淆强度的渐近有效下界。我们在几个合成和半合成数据集上评估了我们的统计检验的功效和有效性。此外，我们展示了我们的下界如何能够在真实环境中正确识别未观察到的混淆的存在和不存在。

    arXiv:2312.03871v2 Announce Type: replace-cross  Abstract: In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. We propose a novel strategy that leverages randomized trials to quantify unobserved confounding. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.
    
[^169]: LSTSVR-PI: 具有特权信息的最小二乘双支持向量回归

    LSTSVR-PI: Least square twin support vector regression with privileged information

    [https://arxiv.org/abs/2312.02596](https://arxiv.org/abs/2312.02596)

    提出的 LSTSVR-PI模型结合了最小二乘双支持向量回归和特权信息，提高了模型效率，并建立了泛化误差界限。

    

    在教育环境中，教师在各种课堂教学模式中扮演了至关重要的角色。类似地，借鉴人类学习的这一方面，使用特权信息进行学习（LUPI）范式在训练阶段引入额外信息来指导学习模型。提出了一种新的利用特权信息的最小二乘双支持向量回归(LSTSVR-PI)的不同训练方法，该方法将LUPI范式集成到最小二乘双支持向量回归中以利用额外信息源。提出的LSTSVR-PI解决了线性方程组，提高了模型的效率。此外，我们还基于所提出模型的Rademacher复杂性建立了泛化误差界限，并融入了结构风险最小化原则。提出的LSTSVR-PI填补了当代模型之间的差距。

    arXiv:2312.02596v2 Announce Type: replace  Abstract: In an educational setting, a teacher plays a crucial role in various classroom teaching patterns. Similarly, mirroring this aspect of human learning, the learning using privileged information (LUPI) paradigm introduces additional information to instruct learning models during the training stage. A different approach to train the twin variant of the regression model is provided by the new least square twin support vector regression using privileged information (LSTSVR-PI), which integrates the LUPI paradigm to utilize additional sources of information into the least square twin support vector regression. The proposed LSTSVR-PI solves system of linear equations which adds up to the efficiency of the model. Further, we also establish a generalization error bound based on the Rademacher complexity of the proposed model and incorporate the structural risk minimization principle. The proposed LSTSVR-PI fills the gap between the contemporar
    
[^170]: 攻击树：自动破解黑盒大型语言模型

    Tree of Attacks: Jailbreaking Black-Box LLMs Automatically

    [https://arxiv.org/abs/2312.02119](https://arxiv.org/abs/2312.02119)

    提出了一种名为Tree of Attacks with Pruning (TAP)的自动化方法，用于生成只需要对目标大型语言模型进行黑盒访问的越狱方法，并通过思维树推理和修剪生成准确的越狱提示。

    

    大型语言模型(LLMs)展示了多功能性，但仍在生成有害、带偏见和有毒内容，这一点由人为设计的越狱行为的普遍存在得以证明。在这项工作中，我们提出了一种名为Tree of Attacks with Pruning (TAP)的自动化方法，用于生成越狱，仅需要对目标LLM进行黑盒访问。TAP利用LLM来通过思维树推理迭代地优化候选（攻击）提示，直到生成的提示之一越狱目标。关键在于，在将提示发送给目标之前，TAP对其进行评估并移除可能不会导致越狱的提示。使用思维树推理使TAP能够在大量提示的搜索空间中导航，而修剪则减少了发送给目标的总查询数量。在实证评估中，我们观察到TAP生成的提示越狱了超过80%的最先进LLMs（包括GPT4和GPT4-Turbo）。

    arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
    
[^171]: 利用合成数据集进行云光学厚度测量的多光谱成像仪云检测

    Creating and Leveraging a Synthetic Dataset of Cloud Optical Thickness Measures for Cloud Detection in MSI

    [https://arxiv.org/abs/2311.14024](https://arxiv.org/abs/2311.14024)

    本论文提出了一种利用合成数据集进行云光学厚度测量的方法，以解决在地球观测背景下标记数据稀缺的问题。

    

    云团通常会遮蔽地球表面的光学卫星监测，从而限制了土地覆盖映射、海洋色彩分析和农田监测等地球观测活动。 在遥感领域内整合机器学习方法显著提高了各种地球观测任务的性能，包括云检测和过滤，但仍有很大改进空间。 ML方法通常依赖大量标记数据进行训练，这在地球观测背景下通常很难获得，这在云光学厚度（COT）估算方面尤为明显。 可靠的COT估计相比使用常规云类别能更精细和应用相关地控制。 为了缓解COT数据稀缺问题，本研究提出了一种新颖的合成数据集方法。

    arXiv:2311.14024v2 Announce Type: replace-cross  Abstract: Cloud formations often obscure optical satellite-based monitoring of the Earth's surface, thus limiting Earth observation (EO) activities such as land cover mapping, ocean color analysis, and cropland monitoring. The integration of machine learning (ML) methods within the remote sensing domain has significantly improved performance on a wide range of EO tasks, including cloud detection and filtering, but there is still much room for improvement. A key bottleneck is that ML methods typically depend on large amounts of annotated data for training, which is often difficult to come by in EO contexts. This is especially true when it comes to cloud optical thickness (COT) estimation. A reliable estimation of COT enables more fine-grained and application-dependent control compared to using pre-specified cloud categories, as is commonly done in practice. To alleviate the COT data scarcity problem, in this work we propose a novel synthe
    
[^172]: 一种用于加速反向建模的生成模型，使用了一种连续变量的新型嵌入

    A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables

    [https://arxiv.org/abs/2311.11343](https://arxiv.org/abs/2311.11343)

    提出了一种用于加速反向建模的生成模型，利用了新型的连续变量嵌入策略，消除了归一化的需要，保留信息并创造了一个多功能的嵌入空间。

    

    在材料科学中，快速原型制作具有所需性能的材料的挑战通常涉及大量的实验，以找到合适的微结构。此外，对于给定性能寻找微结构通常是一个不适定问题，可能存在多个解决方案。使用生成式机器学习模型可以是一个可行的解决方案，同时减少计算成本。然而，这也带来了新的挑战，例如，需要将连续属性变量作为模型的条件输入。我们研究了现有方法的缺点，并将其与一种基于浮点数的二进制表示的生成模型的新型嵌入策略进行了比较。这种方法消除了归一化的需要，保留了信息，并为生成模型的条件提供了一个多功能的嵌入空间。

    arXiv:2311.11343v2 Announce Type: replace  Abstract: In materials science, the challenge of rapid prototyping materials with desired properties often involves extensive experimentation to find suitable microstructures. Additionally, finding microstructures for given properties is typically an ill-posed problem where multiple solutions may exist. Using generative machine learning models can be a viable solution which also reduces the computational cost. This comes with new challenges because, e.g., a continuous property variable as conditioning input to the model is required. We investigate the shortcomings of an existing method and compare this to a novel embedding strategy for generative models that is based on the binary representation of floating point numbers. This eliminates the need for normalization, preserves information, and creates a versatile embedding space for conditioning the generative model. This technique can be applied to condition a network on any number, to provide 
    
[^173]: 具有用户定义目标的自适应干预用于健康行为改变

    Adaptive Interventions with User-Defined Goals for Health Behavior Change

    [https://arxiv.org/abs/2311.09483](https://arxiv.org/abs/2311.09483)

    该论文介绍了一种修改过的Thompson抽样算法，强调通过优化个性化奖励函数实现个性化目标设定，为支持目标设定提供了一个平衡方法，并证明此修改仅对累积遗憾产生恒定的惩罚。

    

    身体活动不足仍然是一个主要的公共健康问题，与心血管疾病和2型糖尿病等不良健康结果相关。移动健康应用程序为低成本、可扩展的身体活动促进提供了一个有希望的途径，然而通常效果较小，粘附率低，特别是与人类辅导相比。目标设定是健康辅导的一个关键组成部分，在移动健康干预的自适应算法中一直未充分利用。本文介绍了对Thompson抽样算法的修改，重点放在通过优化个性化奖励函数实现个性化目标设定。作为支持目标设定的一步，本文提供了一个可以利用共享结构同时优化个人偏好和目标的平衡方法。我们证明，我们的修改只对累积遗憾造成一个常数惩罚。

    arXiv:2311.09483v2 Announce Type: replace-cross  Abstract: Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret 
    
[^174]: 分析验证同步时间的深度神经网络在配电系统状态估计中的性能

    Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation

    [https://arxiv.org/abs/2311.06973](https://arxiv.org/abs/2311.06973)

    本论文研究了使用深度神经网络进行配电系统同步时间状态估计的性能和鲁棒性。通过将输入扰动视为混合整数线性规划问题进行分析验证，并强调了批归一化在提高问题可扩展性方面的作用。该框架在修改后的IEEE 34节点系统和真实的大型分布系统上进行验证。

    

    最近，我们展示了使用深度神经网络（DNN）进行实时不可观测分布系统的同步时间状态估计的成功。在这个论文中，我们提供了该状态估计器在输入测量扰动的情况下的性能的分析界限。已经有人表明，仅基于测试数据集来评估性能可能不能有效地说明训练好的DNN处理输入扰动的能力。因此，我们将输入扰动作为混合整数线性规划（MILP）问题从分析上验证了DNN对输入扰动的鲁棒性和可靠性。同时，我们还强调了批归一化在解决MILP公式的可扩展性限制方面的能力。该框架通过在修改后的IEEE 34节点系统和一个真实的大型分布系统上进行同步时间的配电系统状态估计来进行验证，这两个系统都是通过微相位测量不完全观测到的。

    Recently, we demonstrated success of a time-synchronized state estimator using deep neural networks (DNNs) for real-time unobservable distribution systems. In this letter, we provide analytical bounds on the performance of that state estimator as a function of perturbations in the input measurements. It has already been shown that evaluating performance based on only the test dataset might not effectively indicate a trained DNN's ability to handle input perturbations. As such, we analytically verify robustness and trustworthiness of DNNs to input perturbations by treating them as mixed-integer linear programming (MILP) problems. The ability of batch normalization in addressing the scalability limitations of the MILP formulation is also highlighted. The framework is validated by performing time-synchronized distribution system state estimation for a modified IEEE 34-node system and a real-world large distribution system, both of which are incompletely observed by micro-phasor measuremen
    
[^175]: 通过多路径长期船舶轨迹预测建立更安全的海洋环境

    Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting

    [https://arxiv.org/abs/2310.18948](https://arxiv.org/abs/2310.18948)

    通过利用AIS数据预测船舶轨迹，本研究旨在通过减少船舶与鲸鱼碰撞来建立更安全的海洋环境。

    

    海上交通对于实现全球经济增长至关重要，同时也需要在可持续性和保护濒危海洋物种方面履行生态义务，尤其是保护大型鲸类种群。在这方面，自动识别系统(AIS)数据通过提供船舶运动的实时流数据，可以实现强化的交通监控，从而避免船舶与鲸鱼碰撞。本研究探讨利用AIS数据预测长期船舶轨迹，从而预防船舶与鲸鱼的碰撞。为此，我们采用双向长短期记忆网络(Bi-LSTM)构建了一种编码器-解码器模型架构，通过将1到3小时的AIS数据作为输入，预测接下来12小时的船舶轨迹。我们从历史AIS数据中提取潜在路线和目的地的概率特征，并将其作为模型的输入。模型随后预测船舶的轨迹，考虑到潜在路线和目的地的影响。

    Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
    
[^176]: 自适应神经排名框架：面向级联排名系统的最大化业务目标

    Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems

    [https://arxiv.org/abs/2310.10462](https://arxiv.org/abs/2310.10462)

    提出了一种新颖的透视角度，强调优化级联排名系统的适应性，解决了现有方法无法适应不同级联排名场景的问题，可以提升性能并避免次优情况的发生。

    

    级联排名在在线广告和推荐系统中广泛应用于大规模top-k选择问题，而学习排序是优化级联排名模型的重要方式。以前关于学习排序的工作通常侧重于让模型学习完整顺序或top-k顺序，并采用相应的排名指标（如OPA和NDCG@k）作为优化目标。然而，这些目标无法适应具有不同数据复杂性和模型能力的各种级联排名场景；而现有的基于度量的方法如Lambda框架只能优化有限指标的粗略上界，可能导致次优和性能不对齐。为了解决这些问题，我们提出了一种新颖的优化级联排名系统的透视角度，强调优化目标对数据复杂性和模型能力的适应性。

    arXiv:2310.10462v2 Announce Type: replace  Abstract: Cascade ranking is widely used for large-scale top-k selection problems in online advertising and recommendation systems, and learning-to-rank is an important way to optimize the models in cascade ranking. Previous works on learning-to-rank usually focus on letting the model learn the complete order or top-k order, and adopt the corresponding rank metrics (e.g. OPA and NDCG@k) as optimization targets. However, these targets can not adapt to various cascade ranking scenarios with varying data complexities and model capabilities; and the existing metric-driven methods such as the Lambda framework can only optimize a rough upper bound of limited metrics, potentially resulting in sub-optimal and performance misalignment. To address these issues, we propose a novel perspective on optimizing cascade ranking systems by highlighting the adaptability of optimization targets to data complexities and model capabilities. Concretely, we employ mu
    
[^177]: 扩散模型中的可重复性和一致性的出现

    The Emergence of Reproducibility and Consistency in Diffusion Models

    [https://arxiv.org/abs/2310.05264](https://arxiv.org/abs/2310.05264)

    该论文研究了扩散模型中的一致模型可重复性现象，实验证实了无论模型框架、模型架构或训练过程如何，不同的扩散模型都能够一致地达到相同的数据分布和评分函数。此外，研究发现扩散模型在学习过程中受训练数据规模的影响，表现出两种不同的训练模式：记忆化模式和泛化模式。

    

    在这项工作中，我们研究了扩散模型中的一个有趣且普遍存在的现象，我们称之为“一致的模型可重复性”：在给定相同的起始噪声输入和确定性采样器的情况下，不同的扩散模型通常产生非常相似的输出。我们通过全面的实验证实了这一现象，表明不同的扩散模型无论扩散模型框架、模型架构或训练过程如何，在数据分布和评分函数上都能够一致地达到相同的结果。更令人惊讶的是，我们进一步的调查表明，扩散模型在学习受训数据规模影响下的不同分布。这一点得到了两种不同训练模式下模型可重复性的体现：（i）“记忆化模式”，其中扩散模型过度拟合于训练数据分布，和（ii）“泛化模式”，其中模型学习到了基础数据分布。

    arXiv:2310.05264v2 Announce Type: replace  Abstract: In this work, we investigate an intriguing and prevalent phenomenon of diffusion models which we term as "consistent model reproducibility": given the same starting noise input and a deterministic sampler, different diffusion models often yield remarkably similar outputs. We confirm this phenomenon through comprehensive experiments, implying that different diffusion models consistently reach the same data distribution and scoring function regardless of diffusion model frameworks, model architectures, or training procedures. More strikingly, our further investigation implies that diffusion models are learning distinct distributions affected by the training data size. This is supported by the fact that the model reproducibility manifests in two distinct training regimes: (i) "memorization regime", where the diffusion model overfits to the training data distribution, and (ii) "generalization regime", where the model learns the underlyin
    
[^178]: 不同不确定性下的公平排名

    Fair Ranking under Disparate Uncertainty

    [https://arxiv.org/abs/2309.01610](https://arxiv.org/abs/2309.01610)

    提出了一种新的公平排名标准Equal-Opportunity Ranking（EOR），将底层相关性模型的不确定性差异考虑在内，通过组内公平抽奖实现公平排名。

    

    排名是一种广泛使用的方法，用于将人类评估者的注意力集中在可管理的选项子集上。它作为人类决策过程的一部分的使用范围从在电子商务网站上展示潜在相关产品到为人工审查优先处理大学申请。虽然排名可以通过将关注集中在最有前途的选项上使人类评估更加高效，但我们认为，如果底层相关性模型的不确定性在不同组别的选项之间存在差异，排名可能会引入不公平。不幸的是，这种不确定性差异似乎普遍存在，常常对少数群体造成损害，因为这些群体的相关性估计可能由于缺乏数据或合适的特征而具有更高的不确定性。为了解决这个公平问题，我们提出了Equal-Opportunity Ranking（EOR）作为排名的新公平标准，并展示它对应于在相关选项之间进行组内公平抽奖

    arXiv:2309.01610v2 Announce Type: replace  Abstract: Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use as part of human decision-making processes ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, often to the detriment of minority groups for which relevance estimates can have higher uncertainty due to a lack of data or appropriate features. To address this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking and show that it corresponds to a group-wise fair lottery among the relevant options even
    
[^179]: 语音自监督表示基准测试：更大的探测头的案例

    Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads

    [https://arxiv.org/abs/2308.14456](https://arxiv.org/abs/2308.14456)

    本研究探讨了如何改变探测头架构会对基准测试结果产生影响，在语音自监督学习中评估了更大容量的探测头，展示了其对性能和推断成本的影响。

    

    自监督学习（SSL）利用大量未标记的语音数据集，在减少注释数据量的情况下达到了出色的性能。大量提出的方法促使出现了全面的基准测试，评估它们在一组探索语音信号各个方面的下游任务上的性能。然而，尽管考虑到的任务数量不断增加，大多数提议仍依赖于一个将冻结的SSL表示映射到任务标签的下游架构。本研究研究了如何改变探测头架构会影响基准测试结果。有趣的是，我们发现改变下游架构结构会导致评估模型的性能排名出现显著波动。与语音SSL基准测试中的常见做法相对立，我们评估了更大容量的探测头，展示了它们对性能和推断成本的影响。

    arXiv:2308.14456v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference cos
    
[^180]: 通过语义匹配修复特征归因方法中的确认偏见

    Fixing confirmation bias in feature attribution methods via semantic match

    [https://arxiv.org/abs/2307.00897](https://arxiv.org/abs/2307.00897)

    提出了通过语义匹配修复特征归因方法中的确认偏见问题，引入了人类概念与（亚符号）解释之间的概念框架，并提出了一种结构化方法来评估语义匹配。

    

    特征归因方法已经成为解析黑盒模型复杂行为的重要方法。尽管取得了成功，一些学者指出这类方法存在严重缺陷：它们不能可靠地用人类概念进行解释。简而言之，仅仅可视化一系列特征贡献对于人类来说无法得出关于模型内部表示的结论，而确认偏见可能会让用户产生关于模型行为的错误信念。我们认为需要一种结构化方法来验证我们对模型的假设是否得到了特征归因的确认。这就是我们所说的人类概念与（亚符号）解释之间的“语义匹配”。在 Cin\`a等人[2023]提出的概念框架基础上，我们提出了一种结构化方法来在实践中评估语义匹配。我们在一系列实验中展示了这一过程。

    arXiv:2307.00897v2 Announce Type: replace-cross  Abstract: Feature attribution methods have become a staple method to disentangle the complex behavior of black box models. Despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. Simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. We argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. This is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. Building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. We showcase the procedure in a suite of experiments spa
    
[^181]: 通过分布匹配实现全局可解释的图学习

    Globally Interpretable Graph Learning via Distribution Matching

    [https://arxiv.org/abs/2306.10447](https://arxiv.org/abs/2306.10447)

    该论文提出了通过分布匹配实现全局可解释的图学习的方法，旨在提取主导学习过程的高级模式，以实现全局解释。

    

    图神经网络（GNNs）已经成为捕捉关键图模式的强大模型。现在，人们正在试图解释模型行为而不是将其视为黑盒子。现有的工作主要集中在本地解释，揭示每个个体实例的区分模式，但这不能直接反映实例之间的高层模型行为。为了获得全局见解，我们旨在回答一个尚未得到很好研究的重要问题：如何为图学习过程提供全局解释？我们将这个问题制定为全局可解释的图学习，旨在提取主导学习过程的高级和人类可解读的模式，这样在这种模式上训练可以恢复类似的模型。作为一个开始，我们提出了一个新颖的模型保真度度量标准，用于评估所得到的模型的保真度。

    arXiv:2306.10447v2 Announce Type: replace  Abstract: Graph neural networks (GNNs) have emerged as a powerful model to capture critical graph patterns. Instead of treating them as black boxes in an end-to-end fashion, attempts are arising to explain the model behavior. Existing works mainly focus on local interpretation to reveal the discriminative pattern for each individual instance, which however cannot directly reflect the high-level model behavior across instances. To gain global insights, we aim to answer an important question that is not yet well studied: how to provide a global interpretation for the graph learning procedure? We formulate this problem as globally interpretable graph learning, which targets on distilling high-level and human-intelligible patterns that dominate the learning procedure, such that training on this pattern can recover a similar model. As a start, we propose a novel model fidelity metric, tailored for evaluating the fidelity of the resulting model trai
    
[^182]: TESS：文本到文本自条件单纯形扩散

    TESS: Text-to-Text Self-Conditioned Simplex Diffusion

    [https://arxiv.org/abs/2305.08379](https://arxiv.org/abs/2305.08379)

    TESS是一个全非自回归的文本扩散模型，通过在逻辑空间而不是学习嵌入空间应用扩散过程，进行了自条件单纯形扩散，实验证明在自然语言理解和生成任务中表现优于最先进的非自回归模型，并且所需的扩散步骤更少。

    

    扩散模型已经成为一种在各种连续领域中表现出色的生成方法范式。然而，将连续扩散模型应用于自然语言仍然具有挑战性，因为自然语言是离散的，并且需要大量的扩散步骤来生成文本，这使得基于扩散的生成变得昂贵。在这项工作中，我们提出了文本到文本自条件单纯形扩散（TESS），这是一个全非自回归的文本扩散模型，采用一种新形式的自条件，将扩散过程应用于逻辑空间而不是学习嵌入空间。通过对包括总结、文本简化、释义生成和问题生成在内的自然语言理解和生成任务的广泛实验，我们证明了TESS优于最先进的非自回归模型，在需要更少的扩散步骤的情况下表现出最小的性能下降。

    arXiv:2305.08379v2 Announce Type: replace  Abstract: Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to generate text, making diffusion-based generation expensive. In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the learned embedding space. Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models, requires fewer diffusion steps with minimal drop i
    
[^183]: PC-JeDi: 在高能物理中用于生成粒子云的扩散方法

    PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics

    [https://arxiv.org/abs/2303.05376](https://arxiv.org/abs/2303.05376)

    PC-JeDi是一种在高能物理中生成粒子云的新方法，利用分数扩散模型和变换器，实现了竞争性能，并能以条件生成方式产生具有所需质量和横动量的喷注。

    

    在这篇论文中，我们提出了一种名为PC-JeDi的新方法，用于在高能物理中高效生成喷注。该方法利用基于分数的扩散模型，结合适用于生成粒子云的变换器，因为它们具有置换等变性。PC-JeDi在评估生成喷注质量的几个指标上取得了与当前最先进方法相竞争的性能。尽管由于扩散模型需要大量前向传递而比其他模型慢，但仍然比传统详细模拟快得多。此外，PC-JeDi使用条件生成来为两种不同的粒子，夸克和胶子，生成具有所需质量和横动量的喷注。

    arXiv:2303.05376v2 Announce Type: replace-cross  Abstract: In this paper, we present a new method to efficiently generate jets in High Energy Physics called PC-JeDi. This method utilises score-based diffusion models in conjunction with transformers which are well suited to the task of generating jets as particle clouds due to their permutation equivariance. PC-JeDi achieves competitive performance with current state-of-the-art methods across several metrics that evaluate the quality of the generated jets. Although slower than other models, due to the large number of forward passes required by diffusion models, it is still substantially faster than traditional detailed simulation. Furthermore, PC-JeDi uses conditional generation to produce jets with a desired mass and transverse momentum for two different particles, top quarks and gluons.
    
[^184]: 学习型在线优化用于自主移动出行需求车队控制

    Learning-based Online Optimization for Autonomous Mobility-on-Demand Fleet Control

    [https://arxiv.org/abs/2302.03963](https://arxiv.org/abs/2302.03963)

    开发了一种新颖的混合组合优化增强机器学习流水线，从最优的完全信息解中学习在线派遣和再平衡策略，并在大规模实际场景中展示了其性能优于现有方法，实现利润提高了6.3%。

    

    自主出行需求系统是减缓城市交通相关外部性的可行替代方案，如城市区域内交通工具数量增加和交通相关污染。然而，这些系统的成功在很大程度上取决于高效和有效的车队控制策略。在这个背景下，我们研究了自主出行需求系统的在线控制算法，并开发了一种新颖的混合组合优化增强机器学习流水线，从最优的完全信息解中学习在线派遣和再平衡政策。我们在不同车队规模和各种请求密度的大规模实际场景中测试了我们的混合流水线。我们展示了我们的方法在实现利润方面优于最先进的贪婪和模型预测控制方法，例如在各种关键绩效指标方面比最高表现提高了高达17.1%，平均提高了6.3%。

    arXiv:2302.03963v2 Announce Type: replace-cross  Abstract: Autonomous mobility-on-demand systems are a viable alternative to mitigate many transportation-related externalities in cities, such as rising vehicle volumes in urban areas and transportation-related pollution. However, the success of these systems heavily depends on efficient and effective fleet control strategies. In this context, we study online control algorithms for autonomous mobility-on-demand systems and develop a novel hybrid combinatorial optimization enriched machine learning pipeline which learns online dispatching and rebalancing policies from optimal full-information solutions. We test our hybrid pipeline on large-scale real-world scenarios with different vehicle fleet sizes and various request densities. We show that our approach outperforms state-of-the-art greedy, and model-predictive control approaches with respect to various KPIs, e.g., by up to 17.1% and on average by 6.3% in terms of realized profit.
    
[^185]: 通过核和平方和逼近带约束的优化问题

    Approximation of optimization problems with constraints through kernel Sum-Of-Squares

    [https://arxiv.org/abs/2301.06339](https://arxiv.org/abs/2301.06339)

    通过核和平方和逼近解决带约束的优化问题，提出了统一定理证明这些方案的收敛性，引入散射不等式以缓解维度灾难问题，并在学习向量场的应用中进行了验证。

    

    处理无限维空间中无限数量的不等式约束在许多领域中都存在，从全局优化到最优输运。这些问题在以前的几篇文章中分别通过核和平方和（kSoS）逼近进行了处理。我们在这里提出了一个统一的定理，证明了这些方案的收敛性保证。逐点不等式在一类非负kSoS函数中转化为等式。进一步假设问题中出现的函数是光滑的，专注于逐点相等约束使得可以使用散射不等式来缓解在采样约束方面的维度灾难。我们的方法在学习带有边缘信息的向量场中得到了说明，这里的不变性是一个集合。

    arXiv:2301.06339v2 Announce Type: replace-cross  Abstract: Handling an infinite number of inequality constraints in infinite-dimensional spaces occurs in many fields, from global optimization to optimal transport. These problems have been tackled individually in several previous articles through kernel Sum-Of-Squares (kSoS) approximations. We propose here a unified theorem to prove convergence guarantees for these schemes. Pointwise inequalities are turned into equalities within a class of nonnegative kSoS functions. Assuming further that the functions appearing in the problem are smooth, focusing on pointwise equality constraints enables the use of scattering inequalities to mitigate the curse of dimensionality in sampling the constraints. Our approach is illustrated in learning vector fields with side information, here the invariance of a set.
    
[^186]: 针对重叠分组 lasso 的非重叠统计逼近

    The non-overlapping statistical approximation to overlapping group lasso

    [https://arxiv.org/abs/2211.09221](https://arxiv.org/abs/2211.09221)

    该论文提出了一种针对重叠分组 lasso 的非重叠统计逼近方法，在大规模问题中计算速度更快，为现代问题的应用提供了可能性。

    

    组 lasso 是统计学习中常用的正则化方法，根据预定义的组从模型中消除参数。然而，当这些组重叠时，由于重叠组引起的不可分性，优化组 lasso 惩罚目标在大规模问题上可能会变得耗时，这一瓶颈严重限制了重叠分组 lasso 正则化在许多现代问题中的应用，比如基因通路选择和图模型估计。 在本文中，我们提出了一个可分的惩罚作为重叠分组 lasso 惩罚的逼近。由于可分性，基于我们的惩罚的正则化计算相对于重叠分组 lasso 要快得多，尤其对于大规模和高维问题。我们展示了该惩罚是重叠组 lasso 的最严格的可分松弛。

    arXiv:2211.09221v3 Announce Type: replace-cross  Abstract: Group lasso is a commonly used regularization method in statistical learning in which parameters are eliminated from the model according to predefined groups. However, when the groups overlap, optimizing the group lasso penalized objective can be time-consuming on large-scale problems because of the non-separability induced by the overlapping groups. This bottleneck has seriously limited the application of overlapping group lasso regularization in many modern problems, such as gene pathway selection and graphical model estimation. In this paper, we propose a separable penalty as an approximation of the overlapping group lasso penalty. Thanks to the separability, the computation of regularization based on our penalty is substantially faster than that of the overlapping group lasso, especially for large-scale and high-dimensional problems. We show that the penalty is the tightest separable relaxation of the overlapping group lass
    
[^187]: 评估相似度评分的不确定性：面部识别中的性能与公平性

    Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition

    [https://arxiv.org/abs/2211.07245](https://arxiv.org/abs/2211.07245)

    评估相似度评分函数性能和公平性质的关键工具是ROC曲线，文章提出了一种准确评估与ROC曲线相关不确定性水平的方法，特别适用于面部识别等具有社会影响的应用。

    

    ROC曲线是评估相似度评分函数性能和公平性质的主要工具。为了基于经验ROC分析得出可靠结论，准确评估与感兴趣的ROC曲线的统计版本相关的不确定性水平是绝对必要的，特别是对于具有重要社会影响的应用，如面部识别。在本文中，我们证明了相似性函数的经验ROC曲线以及用于评估公平性的副产品指标的渐近保证。我们还解释，由于在相似度评分情况下，误接受/拒绝率的形式为U-统计量，所以天真的自助法可能会危及评估过程。必须使用专门的重新居中技术。除进行的理论分析外，还使用真实人脸图像数据集进行了各种实验。

    arXiv:2211.07245v2 Announce Type: replace-cross  Abstract: The ROC curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function. In order to draw reliable conclusions based on empirical ROC analysis, accurately evaluating the uncertainty level related to statistical versions of the ROC curves of interest is absolutely necessary, especially for applications with considerable societal impact such as Face Recognition. In this article, we prove asymptotic guarantees for empirical ROC curves of similarity functions as well as for by-product metrics useful to assess fairness. We also explain that, because the false acceptance/rejection rates are of the form of U-statistics in the case of similarity scoring, the naive bootstrap approach may jeopardize the assessment procedure. A dedicated recentering technique must be used instead. Beyond the theoretical analysis carried out, various experiments using real face image datasets
    
[^188]: CLEEGN：用于即插即用自动脑电图重建的卷积神经网络

    CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction

    [https://arxiv.org/abs/2210.05988](https://arxiv.org/abs/2210.05988)

    CLEEGN是一种卷积神经网络，用于即插即用自动脑电图重建，基于独立于受试者的预训练模型，无需进一步校准即可在新用户上操作。

    

    人脑电图（EEG）是一种感知高时空分辨率下皮层神经电生理活动的大脑监测模态。应用EEG面临的最大挑战之一是信号质量不稳定，容易在记录过程中受到不可避免的伪迹影响。迄今为止，大多数现有的EEG伪迹去除和重建技术仅适用于离线分析，或需要个性化训练数据以进行在线重建。我们提出了CLEEGN，一种用于即插即用自动EEG重建的新型卷积神经网络。CLEEGN基于一个独立于受试者的预训练模型使用现有数据，可以在新用户上操作而无需进一步校准。CLEEGN的性能经过多个评估验证，包括波形观察、重建误差评估以及在众所周知的标记数据集上的解码准确性。

    arXiv:2210.05988v2 Announce Type: replace-cross  Abstract: Human electroencephalography (EEG) is a brain monitoring modality that senses cortical neuroelectrophysiological activity in high-temporal resolution. One of the greatest challenges posed in applications of EEG is the unstable signal quality susceptible to inevitable artifacts during recordings. To date, most existing techniques for EEG artifact removal and reconstruction are applicable to offline analysis solely, or require individualized training data to facilitate online reconstruction. We have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. CLEEGN is based on a subject-independent pre-trained model using existing data and can operate on a new user without any further calibration. The performance of CLEEGN was validated using multiple evaluations including waveform observation, reconstruction error assessment, and decoding accuracy on well-studied labeled datasets. The re
    
[^189]: 基于图核Infomax的电子健康记录自监督表征学习

    Self-supervised Representation Learning on Electronic Health Records with Graph Kernel Infomax

    [https://arxiv.org/abs/2209.00655](https://arxiv.org/abs/2209.00655)

    提出了一种名为Graph Kernel Infomax的自监督图核学习方法，用于处理电子健康记录的表征，克服了复杂时间性对性能的影响。

    

    学习电子健康记录（EHRs）表征是一个重要但尚未被充分发掘的研究课题。这有利于各种临床决策支持应用，例如药物结果预测或患者相似性搜索。我们提出了一种自监督图核学习方法，称为Graph Kernel Infomax，用于处理EHR的图形表征，以克服先前的问题。

    arXiv:2209.00655v2 Announce Type: replace  Abstract: Learning Electronic Health Records (EHRs) representation is a preeminent yet under-discovered research topic. It benefits various clinical decision support applications, e.g., medication outcome prediction or patient similarity search. Current approaches focus on task-specific label supervision on vectorized sequential EHR, which is not applicable to large-scale unsupervised scenarios. Recently, contrastive learning shows great success on self-supervised representation learning problems. However, complex temporality often degrades the performance. We propose Graph Kernel Infomax, a self-supervised graph kernel learning approach on the graphical representation of EHR, to overcome the previous problems. Unlike the state-of-the-art, we do not change the graph structure to construct augmented views. Instead, we use Kernel Subspace Augmentation to embed nodes into two geometrically different manifold views. The entire framework is trained
    
[^190]: 离线强化学习中的温和保守Q学习

    Mildly Conservative Q-Learning for Offline Reinforcement Learning

    [https://arxiv.org/abs/2206.04745](https://arxiv.org/abs/2206.04745)

    本文提出了一种离线强化学习中的温和保守Q学习（MCQ）方法，通过为OOD动作分配适当的伪Q值来训练，从而在不损害泛化能力的情况下实现价值函数的保守性，避免过度高估超出分布的动作。

    

    离线强化学习定义了从静态记录的数据集中学习而无需持续与环境进行交互的任务。学习策略与行为策略之间的分布转移使得价值函数保持保守成为必要，以确保超出分布（OOD）的动作不会被严重高估。然而，现有的方法，如对未见动作进行惩罚或与行为策略进行正则化，都过于悲观，抑制了价值函数的泛化能力，并阻碍了性能的提高。本文探讨了离线学习中的温和但足够保守，同时不损害泛化能力。我们提出了温和保守Q学习（MCQ），通过为OOD动作分配适当的伪Q值来积极训练它们。我们理论上证明了MCQ会产生一个至少与行为策略一样好的策略，并且不会发生错误的过估计。

    arXiv:2206.04745v3 Announce Type: replace-cross  Abstract: Offline reinforcement learning (RL) defines the task of learning from a static logged dataset without continually interacting with the environment. The distribution shift between the learned policy and the behavior policy makes it necessary for the value function to stay conservative such that out-of-distribution (OOD) actions will not be severely overestimated. However, existing approaches, penalizing the unseen actions or regularizing with the behavior policy, are too pessimistic, which suppresses the generalization of the value function and hinders the performance improvement. This paper explores mild but enough conservatism for offline learning while not harming generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD actions are actively trained by assigning them proper pseudo Q values. We theoretically show that MCQ induces a policy that behaves at least as well as the behavior policy and no erroneous ov
    
[^191]: 值得信赖的图神经网络：方面、方法和趋势

    Trustworthy Graph Neural Networks: Aspects, Methods and Trends

    [https://arxiv.org/abs/2205.07424](https://arxiv.org/abs/2205.07424)

    论文提出了构建可信赖图神经网络的综合路线图，关注解决性能导向的图神经网络可能存在的对抗性攻击、歧视问题和资源消耗过多等挑战。

    

    论文探讨了图神经网络在各种现实场景中的应用，不仅仅关注任务表现，还着重指出性能导向的图神经网络可能存在对抗性攻击、歧视弱势群体、在边缘计算环境中消耗资源过多等问题。为了避免这些意外伤害，有必要构建具有信赖性的高效图神经网络，提出了从涉及的各种计算技术视角构建可信赖图神经网络的全面路线图。

    arXiv:2205.07424v2 Announce Type: replace-cross  Abstract: Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarise existin
    
[^192]: 冲突回避梯度下降用于多任务学习

    Conflict-Averse Gradient Descent for Multi-task Learning

    [https://arxiv.org/abs/2110.14048](https://arxiv.org/abs/2110.14048)

    冲突回避梯度下降（CAGrad）是针对多任务学习中梯度冲突问题提出的方法，旨在解决不同任务梯度不一致导致的性能下降挑战。

    

    多任务学习的目标是通过共享模型结构来实现比单任务学习更高效的学习，以适应各种任务。标准的多任务学习目标是最小化所有任务的平均损失。然而，使用这个目标通常会导致每个任务的最终表现比独立学习它们时更差。在优化多任务模型中的一个主要挑战是冲突梯度，即不同任务目标的梯度不太一致，因此遵循平均梯度方向可能对特定任务的性能有害。先前的研究提出了几种启发式方法来操纵任务梯度以缓解这个问题。但是大多数方法缺乏收敛保证和/或可能收敛到任何帕累托稳定点。在本文中，我们介绍了一种叫做冲突回避梯度下降（CAGrad）的方法，通过最小化平均损失函数

    arXiv:2110.14048v2 Announce Type: replace-cross  Abstract: The goal of multi-task learning is to enable more efficient learning than single task learning by sharing model structures for a diverse set of tasks. A standard multi-task learning objective is to minimize the average loss across all tasks. While straightforward, using this objective often results in much worse final performance for each task than learning them independently. A major challenge in optimizing a multi-task model is the conflicting gradients, where gradients of different task objectives are not well aligned so that following the average gradient direction can be detrimental to specific tasks' performance. Previous work has proposed several heuristics to manipulate the task gradients for mitigating this problem. But most of them lack convergence guarantee and/or could converge to any Pareto-stationary point. In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss funct
    
[^193]: 多元贝塔混合模型：具有灵活聚类形状的概率聚类方法

    Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible Cluster Shapes. (arXiv:2401.16708v1 [cs.LG])

    [http://arxiv.org/abs/2401.16708](http://arxiv.org/abs/2401.16708)

    本文提出了一种名为多元贝塔混合模型（MBMM）的新的概率模型，用于软聚类。MBMM通过其灵活的多元贝塔分布的概率密度函数适应不同的聚类形状，并在合成和真实数据集上展示了其适应性。

    

    本文介绍了多元贝塔混合模型（MBMM），这是一种新的概率模型用于软聚类。MBMM通过多元贝塔分布的灵活概率密度函数适应不同的聚类形状。我们介绍了MBMM的属性，描述了参数学习过程，并展示了在合成和真实数据集上适合各种聚类形状的实验结果。代码匿名发布在\url{https://github.com/hhchen1105/mbmm/}上。

    This paper introduces the multivariate beta mixture model (MBMM), a new probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes because of the flexible probability density function of the multivariate beta distribution. We introduce the properties of MBMM, describe the parameter learning procedure, and present the experimental results, showing that MBMM fits diverse cluster shapes on synthetic and real datasets. The code is released anonymously at \url{https://github.com/hhchen1105/mbmm/}.
    
[^194]: 遵循人类指令的高质量图像恢复

    High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])

    [http://arxiv.org/abs/2401.16468](http://arxiv.org/abs/2401.16468)

    本论文提出了一种使用人类编写的指令来指导图像恢复模型的方法，并在多个恢复任务上取得了最先进的结果，为基于文本指导的图像恢复和增强研究提供了一个新的基准。

    

    图像恢复是一个基本问题，涉及从退化观测中恢复出高质量的干净图像。全能图像恢复模型可以通过使用特定于退化类型的信息作为提示来有效地恢复各种类型和级别的退化图像，并引导恢复模型。我们提出了一种使用人类编写的指令来指导图像恢复模型的方法。在给定自然语言提示的情况下，我们的模型可以从退化图像中恢复出高质量的图像，并考虑多种退化类型。我们的方法InstructIR在图像去噪、雨水去除、去模糊、去雾和(低光)图像增强等多个恢复任务上取得了最先进的结果。InstructIR在之前的全能恢复方法上提高了1dB。此外，我们的数据集和结果为基于文本指导的图像恢复和增强的新研究提供了一个新的基准。我们提供了代码、数据集和模型。

    Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
    
[^195]: Intrinsic Dataset Properties对泛化能力的影响：揭示自然图像和医学图像之间的学习差异

    The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])

    [http://arxiv.org/abs/2401.08865](http://arxiv.org/abs/2401.08865)

    本文研究了神经网络在自然图像和医学图像领域学习时的差异，提出了一个与训练集维度有关的泛化缩放定律，并认为医学图像数据集更高的固有“标签锐度”可能是两个领域之间显著差异的部分原因。

    

    本文研究了神经网络在不同图像领域学习时的差异，这在从自然图像到其他专门领域（如医学图像）采用计算机视觉技术时通常被忽视。最近的研究发现，训练集的固有维度($d_{data}$)与网络的泛化错误一般会增加。然而，医学（放射学）和自然图像领域之间的这种关系的陡峭程度存在显著差异，且无现有的理论解释。我们通过建立并经验证一个与$d_{data}$相关的泛化缩放定律来解决这个知识空白，并提出考虑到医学图像数据集更高的固有“标签锐度”($K_F$)这一度量指标可以部分解释这两个领域之间的显著缩放差异。接下来，我们展示了利用测量这一指标可以提供的额外好处。

    This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring th
    
[^196]: 异构低秩近似用于设备本地基础模型联邦微调

    Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models. (arXiv:2401.06432v1 [cs.LG])

    [http://arxiv.org/abs/2401.06432](http://arxiv.org/abs/2401.06432)

    本文提出了一种用于异构设备的设备本地基础模型联邦微调的参数高效方法，使用了异构低秩近似（LoRA），解决了资源受限和异构设备的挑战。

    

    大型基础模型（FMs）通过微调适应特定领域或任务。联邦学习（FL）进一步利用设备上的本地数据实现了私有化的FM微调。然而，标准FMs的大尺寸对于资源受限和异构设备带来了挑战。为了解决这个问题，我们考虑了参数尺寸较小的FM，称为设备本地FM（ODFMs）。虽然ODFMs允许设备上的推断，但计算限制仍然阻碍了高效的联邦微调。我们提出了一种参数高效的ODFM联邦微调方法，使用了异构低秩近似（LoRA），解决了系统和数据异质性的问题。我们发现，同质LoRA秩面临着过拟合和收敛缓慢之间的折衷，提出了HetLoRA，它在客户端之间使用异质的秩并消除了同质HetLoRA的缺点。通过在本地应用秩自剪枝，并在服务器上应用稀疏加权聚合，我们完成了摘要中的内容。

    Large foundation models (FMs) adapt surprisingly well to specific domains or tasks with fine-tuning. Federated learning (FL) further enables private FM fine-tuning using the local data on devices. However, the standard FMs' large size poses challenges for resource-constrained and heterogeneous devices. To address this, we consider FMs with reduced parameter sizes, referred to as on-device FMs (ODFMs). While ODFMs allow on-device inference, computational constraints still hinder efficient federated fine-tuning. We propose a parameter-efficient federated fine-tuning method for ODFMs using heterogeneous low-rank approximations (LoRAs) that addresses system and data heterogeneity. We show that homogeneous LoRA ranks face a trade-off between overfitting and slow convergence, and propose HetLoRA, which employs heterogeneous ranks across clients and eliminates the shortcomings of homogeneous HetLoRA. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, we com
    
[^197]: GNNShap: 使用Shapley值快速而准确解释GNN的论文

    GNNShap: Fast and Accurate GNN Explanations using Shapley Values. (arXiv:2401.04829v1 [cs.LG])

    [http://arxiv.org/abs/2401.04829](http://arxiv.org/abs/2401.04829)

    GNNShap是一种使用Shapley值的解释方法，能够快速而准确地解释图神经网络的预测结果。相较于其他方法，GNNShap通过抽样、并行化计算等技术提高了解释速度和精细度。

    

    图神经网络(GNN)是一种在科学领域中具有广泛应用的图机器学习模型。然而，GNN被认为是黑盒模型，很难理解模型如何进行预测。基于博弈论的Shapley值方法在其他领域中被广泛应用于解释模型，但在图领域中研究较少。一些研究已经提出了基于Shapley值的GNN解释方法，然而它们存在一些限制：它们只考虑了有限的样本来近似Shapley值；有些方法主要关注小和大的联盟大小，并且它们比其他解释方法慢了一个数量级，使得它们在中等规模的图中无法应用。在这项工作中，我们提出了GNNShap，它提供边的解释，因为它们对图提供了更自然和精细的解释。我们通过对所有联盟大小进行抽样、在GPU上并行抽样和加速模型等方面克服了这些限制。

    Graph neural networks (GNNs) are popular machine learning models for graphs with many applications across scientific domains. However, GNNs are considered black box models, and it is challenging to understand how the model makes predictions. Game theory-based Shapley value approaches are popular explanation methods in other domains but are not well-studied for graphs. Some studies have proposed Shapley value-based GNN explanations, yet they have several limitations: they consider limited samples to approximate Shapley values; some mainly focus on small and large coalition sizes, and they are an order of magnitude slower than other explanation methods, making them inapplicable to even moderate-size graphs. In this work, we propose GNNShap, which provides explanations for edges since they provide more natural explanations for graphs and more fine-grained explanations. We overcome the limitations by sampling from all coalition sizes, parallelizing the sampling on GPUs, and speeding up mod
    
[^198]: 超越忠诚度：解释基于学习的漏洞检测器的漏洞定位

    Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors. (arXiv:2401.02686v1 [cs.CR])

    [http://arxiv.org/abs/2401.02686](http://arxiv.org/abs/2401.02686)

    这项研究评估了基于图和序列表示的十种漏洞检测器解释方法的性能，发现单纯的忠诚度评估不足够。

    

    近年来，基于深度学习模型的漏洞检测器证明了其有效性。然而，这些检测器决策过程的不透明性使安全分析师难以理解。为了解决这个问题，已经提出了各种解释方法来解释预测结果，通过突出重要特征，在计算机视觉和自然语言处理等其他领域已经证明是有效的。不幸的是，这些解释方法对于漏洞检测器学习和理解的细粒度的漏洞相关代码行等关键特征的深入评估仍然缺乏。在本研究中，我们首先通过两个定量指标——忠诚度和漏洞行覆盖率——评估了基于图和序列表示的十种漏洞检测器解释方法的性能。我们的结果显示，仅仅依靠忠诚度是不足够的。

    Vulnerability detectors based on deep learning (DL) models have proven their effectiveness in recent years. However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend. To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in other domains such as computer vision and natural language processing. Unfortunately, an in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches remains lacking. In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate. Our results show that fidelity alone is not sufficient f
    
[^199]: 持续学习在语言转换中的研究

    A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])

    [http://arxiv.org/abs/2311.01200](http://arxiv.org/abs/2311.01200)

    本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。

    

    最近语言模型预训练的数据和模型规模的增加导致了巨大的训练成本。在随时间推移而出现新数据的情况下，更新模型而不是完全重新训练可以带来显著的收益。在本文中，我们研究了在新语言出现时更新语言模型时的好处和弊端，即在语言转换中持续学习的情况。从单语英语语言模型出发，我们逐步添加了来自挪威语和冰岛语的数据，以研究前向和后向转移效果如何取决于预训练顺序和语言特征，对于不同的模型大小和学习率调度器。我们的结果表明，尽管前向转移主要是正向的，不受语言顺序的影响，但后向转移则可能是正向的或负向的，具体取决于新语言的顺序和特征。为了解释这些模式，我们探索了几种语言相似度度量方法。

    The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
    
[^200]: 通过估计数据分布比例的离散扩散语言建模

    Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution. (arXiv:2310.16834v1 [stat.ML])

    [http://arxiv.org/abs/2310.16834](http://arxiv.org/abs/2310.16834)

    本研究通过引入得分熵这一新颖的离散得分匹配损失，弥补了离散数据领域中现有方法的不足，提出了得分熵离散扩散模型(SEDD)并在GPT-2实验中取得了有竞争力的效果。

    

    尽管扩散模型在许多生成建模任务中具有突破性的性能，但在自然语言等离散数据领域中却表现不佳。关键是，标准的扩散模型依赖于成熟的得分匹配理论，但是将其推广到离散结构并没有取得相同的经验收益。在本文中，我们通过提出得分熵，一种新颖的离散得分匹配损失，来弥补这个差距，它比现有方法更稳定，可以形成最大似然训练的ELBO，并且可以通过去噪变体高效优化。我们将我们的得分熵离散扩散模型（SEDD）扩展到GPT-2的实验设置中，实现了极具竞争力的似然度，同时引入了独特的算法优势。特别是，在比较大小相似的SEDD和GPT-2模型时，SEDD达到了可比较的困惑度（通常在基线的+$10\%$内，并且有时超过基线）。此外，SEDD模型学到了...

    Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of and sometimes outperforming the baseline). Furthermore, SEDD models lear
    
[^201]: MDP中LTL和ω-regular目标的PAC学习算法

    A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v1 [cs.LG])

    [http://arxiv.org/abs/2310.12248](http://arxiv.org/abs/2310.12248)

    这个论文介绍了一种基于模型的PAC学习算法，用于在MDP中学习ω-regular目标，不需要系统拓扑的先前知识。

    

    线性时序逻辑（LTL）和ω-regular目标是近期用于在强化学习中表达非马尔可夫目标的一种方式。我们提出了一种基于模型的可能近似正确（PAC）学习算法，用于MDP中的ω-regular目标。与之前的方法不同，我们的算法从系统的采样轨迹中进行学习，并且不需要系统拓扑的先前知识。

    Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
    
[^202]: EEG运动意向解码：一种与通道关注机制相比较的分析框架

    EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms. (arXiv:2310.11198v1 [cs.HC])

    [http://arxiv.org/abs/2310.11198](http://arxiv.org/abs/2310.11198)

    本研究探索了在运动意向解码领域应用不同通道关注机制的可行性，通过构建一个轻量级架构框架，并在同一环境中比较它们的影响，结果表明这些机制的易集成性和低计算复杂度使其成为BCI中运动意向解码的有效方法。

    

    本研究的目标是探讨在大脑-计算机接口（BCI）领域中应用各种通道关注机制于运动意向解码。通道关注机制可以视为传统用于运动意向解码的空间滤波器的强大演进。本研究通过将这些机制整合到一个轻量级架构框架中，系统地比较它们的影响。我们精心构建了一个简单而轻量级的基准架构，旨在无缝集成不同的通道关注机制。这种方法与之前的研究相反，之前的研究只研究一个关注机制，并且通常构建一个非常复杂、有时嵌套的架构。我们的框架使我们能够在相同情况下评估和比较不同的关注机制的影响。易于集成不同的通道关注机制以及低计算复杂度使我们能够高效地研究和推进BCI中的运动意向解码。

    The objective of this study is to investigate the application of various channel attention mechanisms within the domain of brain-computer interface (BCI) for motor imagery decoding. Channel attention mechanisms can be seen as a powerful evolution of spatial filters traditionally used for motor imagery decoding. This study systematically compares such mechanisms by integrating them into a lightweight architecture framework to evaluate their impact. We carefully construct a straightforward and lightweight baseline architecture designed to seamlessly integrate different channel attention mechanisms. This approach is contrary to previous works which only investigate one attention mechanism and usually build a very complex, sometimes nested architecture. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances. The easy integration of different channel attention mechanisms as well as the low computational complexity enables us
    
[^203]: 对比分类器在泛化界限中的比较

    Comparing Comparators in Generalization Bounds. (arXiv:2310.10534v1 [cs.LG])

    [http://arxiv.org/abs/2310.10534](http://arxiv.org/abs/2310.10534)

    本文推导了涉及任意凸比较函数的通用信息理论和PAC-Bayesian泛化界限，证明了最紧界限是由凸共轭的累积生成函数(CGF)构成的，使得这些界限广泛适用于不同结构的泛化界限。

    

    我们推导了涉及任意凸比较函数的通用信息理论和PAC-Bayesian泛化界限，该函数测量训练误差和样本误差之间的差异。该界限在比较函数的累积生成函数(CG), 被界定在一族限制分布函数的CGF上限的假设下成立。我们证明了当比较函数是CGF的凸共轭，也被称为Cram\'er函数时，得到的界限是最紧的。这个结论更广泛地适用于具有类似结构的泛化界限。这证实了已知界限在有界和次高斯损失情况下的近最优性，并且在其他限制分布下得到了新的界限。

    We derive generic information-theoretic and PAC-Bayesian generalization bounds involving an arbitrary convex comparator function, which measures the discrepancy between the training and population loss. The bounds hold under the assumption that the cumulant-generating function (CGF) of the comparator is upper-bounded by the corresponding CGF within a family of bounding distributions. We show that the tightest possible bound is obtained with the comparator being the convex conjugate of the CGF of the bounding distribution, also known as the Cram\'er function. This conclusion applies more broadly to generalization bounds with a similar structure. This confirms the near-optimality of known bounds for bounded and sub-Gaussian losses and leads to novel bounds under other bounding distributions.
    
[^204]: QLLM: 大规模语言模型的准确高效低位宽量化

    QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])

    [http://arxiv.org/abs/2310.08041](http://arxiv.org/abs/2310.08041)

    QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。

    

    大规模语言模型在自然语言处理领域表现出色，但由于其所需资源过大，限制了其广泛应用。虽然量化感知训练（Quantization-Aware Training，QAT）提供了一种解决方案，但它的训练成本过高，因此后训练量化（Post-Training Quantization，PTQ）成为大规模语言模型更实际的方法。在现有研究中，特定通道中的激活离群值被认为是导致后训练量化准确性下降的瓶颈。本文提出了QLLM，一种为大规模语言模型设计的准确高效的低位宽后训练量化方法。QLLM引入了一种自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。具体来说，通过通道拆分和通道组装，在保证低位宽的情况下将离群通道分解成多个子通道。

    Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
    
[^205]: 可解释的扩散模型通过信息分解

    Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])

    [http://arxiv.org/abs/2310.07972](http://arxiv.org/abs/2310.07972)

    本研究通过观察扩散和信息分解之间的关系，揭示了扩散模型学习到的细粒度关系，进一步解决了高维空间中信息携带变量的问题。

    

    去噪扩散模型能够用于复杂关系的条件生成和密度建模，如图像和文本。然而，学习到的关系的本质是不透明的，因此很难准确理解单词和图像部分之间的关系，或者预测干预的效果。我们通过观察扩散和信息分解之间的精确关系，揭示了扩散模型学习到的细粒度关系。互信息和条件互信息的精确表达可以通过去噪模型来计算。此外，也可以轻松估计在特定图像和标题之间的关系。进一步对信息进行分解，以理解高维空间中哪些变量携带信息，是一个长期存在的问题。对于扩散模型，我们展示了一种自然的非负信息分解方法。

    Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
    
[^206]: 关系卷积网络：学习层次关系表示的框架

    Relational Convolutional Networks: A framework for learning representations of hierarchical relations. (arXiv:2310.03240v1 [cs.LG])

    [http://arxiv.org/abs/2310.03240](http://arxiv.org/abs/2310.03240)

    关系卷积网络是一个学习显式层次关系表示的框架，通过使用多维内积关系模块和关系卷积层，以及基于图元滤波器的群组比较，能够表达更高阶、层次的关系。

    

    深度学习中一个成熟的研究领域是开发能够学习显式关系特征表示的架构。本文着重于学习层次关系表示的问题，提出了一个名为“关系卷积网络”的架构框架。给定一系列对象，一个“多维内积关系”模块生成一个描述所有成对关系的关系张量。然后，一个“关系卷积”层将关系张量转化为一个新对象序列，每个对象描述前一层某群对象内的关系。类似于卷积神经网络中的滤波器，图元滤波器代表要与关系张量在每个分组中进行比较的关系模板。通过重复这个过程，得到更高阶、层次的关系表示。我们介绍了架构的动机和细节，以及一系列实验来证明…

    A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of hierarchical relations, proposing an architectural framework we call "relational convolutional networks". Given a sequence of objects, a "multi-dimensional inner product relation" module produces a relation tensor describing all pairwise relations. A "relational convolution" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demons
    
[^207]: 基于负距离核的最大平均距离(MMD)梯度流的后验抽样

    Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel. (arXiv:2310.03054v1 [stat.ML])

    [http://arxiv.org/abs/2310.03054](http://arxiv.org/abs/2310.03054)

    本文提出了一种基于负距离核的最大平均距离(MMD)的条件流方法，用于后验抽样和条件生成建模。通过离散的Wasserstein梯度流近似联合分布，证明了粒子流是适当功能的Wasserstein梯度流。在条件图像生成和超分辨率等逆问题中展示了方法的有效性。

    

    我们提出了基于负距离核的最大平均距离(MMD)的条件流用于后验抽样和条件生成建模。这个MMD，也被称为能量距离，具有像通过切片和排序进行高效计算的几个有益属性。我们使用离散的Wasserstein梯度流来近似真实情况和观察值的联合分布，并为后验分布建立了误差界限。此外，我们证明了我们的粒子流确实是适当功能的Wasserstein梯度流。我们方法的能力通过数字示例进行了演示，包括条件图像生成和诸如超分辨率、修复和低剂量和有限角度设置下的计算机断层扫描等逆问题。

    We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modeling. This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting. We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions. Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional. The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.
    
[^208]: 缩放定律在联想记忆中的应用

    Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])

    [http://arxiv.org/abs/2310.02984](http://arxiv.org/abs/2310.02984)

    本文研究了应用于联想记忆中的缩放定律，通过高维矩阵和嵌入的外积来模拟内层Transformer语言模型。作者推导出了与样本数量和参数大小相关的精确缩放定律，并验证了理论结果的有效性。同时，作者还通过大量实验展示了存储记忆关联的细粒度可视化。

    

    学习很可能涉及到抽象规则的发现和记忆。本文旨在研究联想记忆机制。我们的模型基于高维矩阵，由嵌入的外积组成，与Transformer语言模型的内层相关。我们推导出关于样本数量和参数规模的精确缩放定律，并讨论了不同估计器的统计效率，包括基于优化的算法。我们进行了大量的数值实验，以验证和解释理论结果，包括对存储记忆关联的细粒度可视化。

    Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
    
[^209]: 实体推断竞技场：探究LLMs的对话推理和规划能力的平台

    The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])

    [http://arxiv.org/abs/2310.01468](http://arxiv.org/abs/2310.01468)

    本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。

    

    目前，大型语言模型（LLMs）在回答明确提问时非常有效。然而，当面临含糊不清的查询时，它们可能行为难以预测并产生错误的输出。这凸显了需要开发能够提出澄清问题以有效解决歧义的智能代理的需求。这种能力需要对多个对话轮次进行复杂的理解、状态跟踪、推理和规划。然而，直接测量这种能力可能具有挑战性。在本文中，我们提供了一个替代性问题，通过向法官提出一系列查询，评估了LLMs推断自己不知道但被法官揭示的实体的能力。这个“实体推断游戏”可以作为一个评估框架，用于探究语言模型的对话推理和规划能力。我们系统地评估了各种LLMs，并发现在这个任务上它们的性能存在显著差异。我们发现强大的LLMs...

    Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
    
[^210]: TranDRL：一种基于Transformer驱动的深度强化学习支持的预防性维护框架

    TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])

    [http://arxiv.org/abs/2309.16935](http://arxiv.org/abs/2309.16935)

    TranDRL是一种基于Transformer驱动的深度强化学习支持的预防性维护框架，结合了复杂时间模式捕捉和经济高效维护建议，显著提高了剩余寿命（RUL）预测准确性和维护行动优化。

    

    工业系统需要可靠的预测性维护策略来提高运营效率并减少停机时间。本文介绍了一种新颖的综合框架，利用Transformer神经网络和深度强化学习（DRL）算法来优化维护行动。我们的方法采用Transformer模型来有效捕捉传感器数据中的复杂时间模式，从而准确预测设备的剩余寿命（RUL）。同时，我们框架中的DRL组件提供了经济高效和及时的维护建议。我们在NASA C-MPASS数据集上验证了我们框架的有效性，结果显示在RUL预测准确性和维护行动优化方面取得了显著进展。因此，我们的创新方法为预防性维护提供了一种创新的数据驱动方法，解决了工业运营中的关键挑战，并带来了更多发展机遇。

    Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
    
[^211]: 人工生成的演示是否对于上下文学习有必要？

    Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])

    [http://arxiv.org/abs/2309.14681](http://arxiv.org/abs/2309.14681)

    本文研究了上下文学习中人工生成的演示是否有必要，并提出了一种新的自反思提示策略（SEC），通过这种策略，大型语言模型（LLMs）可以自行生成演示和最终输出，避免了手动生成过程的复杂性。

    

    尽管大型语言模型（LLMs）具备良好的少样本能力，但在上下文学习（ICL）的标准范式中存在以下弊端：易受选定演示的影响，生成这些演示的复杂性。本文提出了对于ICL，人工生成的演示是否有必要的基本问题，并提出了自反思提示策略（SEC），这是一种不依赖人工演示的范例。SEC的关键点在于，不使用手工制作的示例作为ICL中的演示，而是要求LLMs首先自行创建演示，然后生成最终输出。SEC是一种灵活的框架，可适应原始ICL和“思维链”（CoT），并且更加便捷：因为可以节省示例和理由的手动生成过程。在算术推理、常识推理和多任务语言理解方面进行了大量实验。

    Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
    
[^212]: PoSE: 通过位置跳跃式训练提高LLMs对于上下文窗口的有效拓展

    PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])

    [http://arxiv.org/abs/2309.10400](http://arxiv.org/abs/2309.10400)

    本文介绍了一种名为PoSE的训练方法，通过在训练过程中使用固定的上下文窗口和操纵位置索引来适应极长的上下文窗口，实验证明这种方法大大减小了内存和时间开销，对性能影响较小，成功将LLaMA模型扩展到了128k个标记。

    

    本文介绍了一种名为Positional Skip-wise (PoSE)训练的方法，用于将大型语言模型（LLMs）适应于极长的上下文窗口。PoSE通过在训练过程中使用固定的上下文窗口和操纵位置索引来模拟长输入，将训练长度与目标上下文窗口大小分离。具体而言，我们从长输入序列中选择若干短块，并引入不同的跳跃偏置项来修改每个块的位置索引。这些跳跃偏置项以及每个块的长度在每个训练样本中都会变化，使得模型能够适应目标上下文窗口中的所有位置，而无需对完整长度的输入进行训练。实验证明，与对完整长度进行微调相比，PoSE大大减小了内存和时间开销，对性能影响较小。利用这一优势，我们成功将LLaMA模型扩展到了128k个标记。此外，我们经验证实，PoSE与

    In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
    
[^213]: 使用机器学习和不确定性量化对CRT的多阶段决策过程进行建模的新方法

    A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification. (arXiv:2309.08415v1 [cs.LG])

    [http://arxiv.org/abs/2309.08415](http://arxiv.org/abs/2309.08415)

    本研究提出了一种使用机器学习和不确定性量化建模的多阶段决策过程方法，用于预测心力衰竭患者对心脏再同步治疗的反应。该模型能够推荐收集额外的SPECT MPI变量，以提高预测准确性。

    

    目的。本研究旨在创建一个多阶段的机器学习模型，用于预测心力衰竭（HF）患者心脏再同步治疗（CRT）的反应。该模型利用不确定性量化来推荐在基线临床变量和心电图（ECG）的特征不足时收集额外的单光子发射计算机体层摄影心肌灌注显像（SPECT MPI）变量。方法。本研究纳入了218名接受静息门控SPECT MPI的患者。CRT反应被定义为6个月随访时左室射血分数（LVEF）增加> 5%。通过组合两个集成模型创建了一个多阶段的机器学习模型。结果。CRT的反应率为55.5%（n = 121），整体男性占61.0%（n = 133），平均年龄62.0岁，LVEF为27.7。该多阶段模型的性能与集成模型2（利用了额外的SPECT数据）相似，AUC分别为0.75和0.77，准确性分别为0.71和...

    Aims. The purpose of this study is to create a multi-stage machine learning model to predict cardiac resynchronization therapy (CRT) response for heart failure (HF) patients. This model exploits uncertainty quantification to recommend additional collection of single-photon emission computed tomography myocardial perfusion imaging (SPECT MPI) variables if baseline clinical variables and features from electrocardiogram (ECG) are not sufficient. Methods. 218 patients who underwent rest-gated SPECT MPI were enrolled in this study. CRT response was defined as an increase in left ventricular ejection fraction (LVEF) > 5% at a 6 month follow-up. A multi-stage ML model was created by combining two ensemble models. Results. The response rate for CRT was 55.5% (n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, and LVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (which utilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71 vs
    
[^214]: 提升模仿学习用于自动驾驶的交通规则遵守的关键因素

    What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])

    [http://arxiv.org/abs/2309.07808](http://arxiv.org/abs/2309.07808)

    本文提出了一种基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能，并解决了交通规则遵守和传感器感知问题。

    

    最近越来越多的研究关注于全端到端的自动驾驶技术，在这种技术中，整个驾驶流程被替换为一个简单的神经网络，由于其结构简单和推理时间快，因此变得非常吸引人。尽管这种方法大大减少了驾驶流程中的组件，但其简单性也导致解释性问题和安全问题。训练得到的策略并不总是符合交通规则，同时也很难发现其错误的原因，因为缺乏中间输出。同时，传感器对于自动驾驶的安全性和可行性也至关重要，可以帮助感知复杂驾驶场景下的周围环境。本文提出了一种全新的基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能。我们对模型的性能进行了评估。

    More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
    
[^215]: 跨域声音识别用于高效的水下数据分析

    Cross-domain Sound Recognition for Efficient Underwater Data Analysis. (arXiv:2309.03451v1 [cs.SD])

    [http://arxiv.org/abs/2309.03451](http://arxiv.org/abs/2309.03451)

    本文提出了一种跨域声音识别方法，通过利用非水下声音模型训练数据来分析水下声学数据。通过聚类和可视化方法，简化了候选标签选择的过程，并通过训练神经网络模型实现了对空气枪声的高效识别。

    

    本文介绍了一种新颖的深度学习方法，通过利用在广谱非水下（空中）声音上训练的模型，分析海量水下声学数据。鉴于标记大量水下数据的挑战，我们提出了一个双重方法来加速这一劳动密集型过程。我们的方法的第一部分涉及使用空中声音识别模型的特征向量对水下数据进行PCA和UMAP可视化。这使我们能够在二维空间中对数据进行聚类，并听取这些聚类中的点以了解其定义特性。这种创新方法简化了选择候选标签进行进一步训练的过程。在第二部分中，我们使用选定的水下数据和非水下数据集训练了一个神经网络模型。我们进行了定量分析，衡量了我们模型识别空气枪声的精确度、召回率和F1得分。

    This paper presents a novel deep learning approach for analyzing massive underwater acoustic data by leveraging a model trained on a broad spectrum of non-underwater (aerial) sounds. Recognizing the challenge in labeling vast amounts of underwater data, we propose a two-fold methodology to accelerate this labor-intensive procedure.  The first part of our approach involves PCA and UMAP visualization of the underwater data using the feature vectors of an aerial sound recognition model. This enables us to cluster the data in a two dimensional space and listen to points within these clusters to understand their defining characteristics. This innovative method simplifies the process of selecting candidate labels for further training.  In the second part, we train a neural network model using both the selected underwater data and the non-underwater dataset. We conducted a quantitative analysis to measure the precision, recall, and F1 score of our model for recognizing airgun sounds, a common
    
[^216]: 图神经网络中的等级崩塌导致平滑过度和关联过高

    Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])

    [http://arxiv.org/abs/2308.16800](http://arxiv.org/abs/2308.16800)

    本文研究了图神经网络中的平滑过度和特征关联过高现象，发现固定不变的子空间导致了节点表示的等级崩塌。在该子空间中平滑向量的存在导致过度平滑，即使避免过度平滑也会导致过高的关联。为了解决这个问题，我们提出了一种克罗内克积之和作为一种有效方法。

    

    我们的研究揭示了深度图神经网络中平滑过度和特征关联过高的新理论见解。我们展示了固定不变子空间的普遍存在，它表现出一种相对的行为，不受特征转换的影响。我们的工作阐明了与收敛到常数状态和节点状态的过分分离相关的最新观察结果，因为子空间的放大只取决于聚合函数的频谱。在线性场景中，这导致节点表示由低维子空间主导，并且具有与特征转换无关的渐近收敛速率。当平滑向量跨越这个子空间时，这会导致节点表示的等级崩塌，从而导致过度平滑，即使避免过度平滑也会导致过高的关联。在我们的理论指导下，我们提出了一种克罗内克积之和作为一种有益特性，可以可靠地防止过度平滑、过高关联和等级崩塌。

    Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
    
[^217]: 使用基于图的多智能体强化学习学习协作信息传播

    Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])

    [http://arxiv.org/abs/2308.16198](http://arxiv.org/abs/2308.16198)

    本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。

    

    在现代通信系统中，高效可靠的信息传播对支持关键操作至关重要，如灾难响应、自动驾驶车辆和传感器网络。本文介绍了一种多智能体强化学习（MARL）方法，作为实现更为分散、高效和协作解决方案的重要进展。我们提出了一种用于信息传播的分布式POMDP（Decentralized-POMDP）形式，使得每个智能体可以独立决定消息的转发。这构成了一种从传统基于多点中继（MPR）选择的启发式方法的重大范式转移。我们的方法利用图卷积强化学习，采用具有动态注意力的图注意力网络（GAT）来捕捉关键网络特征。我们提出了两种方法，L-DGN和HL-DGN，它们在智能体之间交换的信息上有所不同。通过将我们的分散方法与基于MPR的方法进行比较，我们评估了其性能。

    In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
    
[^218]: 神经网络的自适应切向特征视角

    An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v1 [cs.LG])

    [http://arxiv.org/abs/2308.15478](http://arxiv.org/abs/2308.15478)

    本研究提出了一个切向特征视角的框架，通过线性变换和结构正则化优化神经网络的特征学习，从而更好地理解神经网络的特征学习过程。在实验证明了该方法在回归问题中的有效性，并提供了对核对齐现象的细致分析。

    

    为了更好地理解神经网络中的特征学习，我们提出了一个框架来理解在切向特征空间中的线性模型，其中特征在训练过程中可以进行转换。我们考虑特征的线性变换，从而通过双线性插值约束在参数和变换上进行联合优化。我们证明了这个优化问题与具有结构化正则化的等价线性约束优化问题具有近似低秩解的关系。通过将其应用于神经网络结构，我们对特征以及核函数的变化获得了更深入的理解，为当目标函数在切向特征上表征不好时的核对齐现象提供了额外的细微差别。除了在简单回归问题上验证我们的理论观察结果外，我们还通过实验证明了切向特征分类的自适应特征实现方法。

    In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classificat
    
[^219]: 授权临床医生并民主化数据科学：大型语言模型自动化临床研究的机器学习。 (arXiv:2308.14120v2 [cs.LG] 更新版)

    Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14120](http://arxiv.org/abs/2308.14120)

    chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。

    

    机器学习（ML）开发者（如数据科学家）和从业者（如临床医生）之间存在知识差距，阻碍了ML在临床数据分析中的充分利用。我们研究了chatGPT Advanced Data Analysis（ADA），即GPT-4的扩展，来弥合这一差距并高效执行ML分析的潜力。我们向chatGPT ADA提供了各种医学专业的大型试验的真实临床数据和研究详细信息，没有给出具体指导。ChatGPT ADA基于原始研究的训练数据自主开发了最先进的ML模型，用于预测临床结果，如癌症发展、癌症进展、疾病并发症或致病基因序列等生物标志物。令人惊讶的是，这些ML模型与其已发表的对应物相匹配甚至表现更好。我们得出结论，chatGPT ADA为民主化医学中的ML提供了一个有前景的途径，使非ML专家能够获得先进的分析工具并推动广泛应用。

    A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
    
[^220]: CausalLM不适用于上下文学习

    CausalLM is not optimal for in-context learning. (arXiv:2308.06912v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06912](http://arxiv.org/abs/2308.06912)

    最近的研究显示，上下文学习中使用前缀语言模型（PrefixLM）比因果语言模型（CausalLM）效果更好。本文通过理论分析证明，虽然两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，因果语言模型的收敛动态遵循在线梯度下降算法，不保证收敛到最优解。

    

    最近的实证证据表明，在上下文学习中，使用前缀语言模型（PrefixLM）表现更好，其允许上下文样本相互关注；相比之下，因果语言模型（CausalLM）使用自回归注意力机制，禁止上下文样本关注未来的样本。虽然这个结果是直观的，但从理论角度并不清楚。本文采用理论方法，分析了在特定参数构建下，前缀语言模型和因果语言模型的收敛行为。分析结果显示，两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，而因果语言模型的收敛动态遵循在线梯度下降算法，即使样本数量趋于无穷，也不能保证收敛到最优解。我们通过对合成数据的经验实验来支持我们的理论观点。

    Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic a
    
[^221]: 强化学习中基于奖励机器抽象的上下文预规划以增强迁移学习

    Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])

    [http://arxiv.org/abs/2307.05209](http://arxiv.org/abs/2307.05209)

    我们提出了一种使用奖励机器抽象来表示当前任务，并在迁移学习中提升DRL代理的性能的方法，实验表明该方法能够提高样本效率并在多个领域中进行少样本迁移。

    

    最近的研究表明，深度强化学习（DRL）代理倾向于过拟合训练任务，并且无法适应轻微的环境变化。为了在转移到未见任务时加快学习，我们提出了一种使用奖励机器（RM）来表示当前任务的新方法，奖励机器是基于当前任务的奖励和动态生成子任务的状态机抽象。我们的方法为代理提供了当前抽象状态的符号表示，并奖励它们达成这些转换。这些表示在任务之间共享，使代理能够利用先前遇到的符号和转换的知识，从而增强迁移能力。我们的实证评估表明，我们的表示在各种领域中提高了样本效率和少样本迁移。

    Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
    
[^222]: 对驾驶员凝视估计和在凝视行为理解中的应用的综述

    A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding. (arXiv:2307.01470v1 [cs.CV])

    [http://arxiv.org/abs/2307.01470](http://arxiv.org/abs/2307.01470)

    本文综述了驾驶员凝视估计的基本知识、估计方法以及在实际驾驶场景中的应用。通过讨论不同的数据收集方法和算法技术，对驾驶员的凝视行为进行了理解。

    

    驾驶员的凝视在驾驶员注意力检测、视觉分心检测、凝视行为理解和构建驾驶员辅助系统等凝视基于应用中起着重要的作用。本研究的主要目标是对驾驶员的凝视基础知识、驾驶员凝视估计方法及其在现实驾驶场景中的应用进行总结。首先我们讨论与驾驶员凝视相关的基础知识，包括头戴式和远程设置的凝视估计以及每种数据收集方法所使用的术语。接下来，我们列举了现有的基准驾驶员凝视数据集，重点介绍了数据收集方法和所使用的设备。然后，我们讨论了用于驾驶员凝视估计的算法，主要涉及传统的机器学习和深度学习技术。接着，估计的驾驶员凝视被用于理解在驾驶过程中的凝视行为。

    Driver gaze plays an important role in different gaze-based applications such as driver attentiveness detection, visual distraction detection, gaze behavior understanding, and building driver assistance system. The main objective of this study is to perform a comprehensive summary of driver gaze fundamentals, methods to estimate driver gaze, and it's applications in real world driving scenarios. We first discuss the fundamentals related to driver gaze, involving head-mounted and remote setup based gaze estimation and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and the equipment used for such data collection. This is followed by a discussion of the algorithms used for driver gaze estimation, which primarily involves traditional machine learning and deep learning based techniques. The estimated driver gaze is then used for understanding gaze behavior while maneuver
    
[^223]: NeuralFuse: 学习改善低电压环境下有限访问神经网络推断的准确性

    NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes. (arXiv:2306.16869v1 [cs.LG])

    [http://arxiv.org/abs/2306.16869](http://arxiv.org/abs/2306.16869)

    NeuralFuse是一个新颖的附加模块，通过学习输入转换来生成抗误差的数据表示，解决了低电压环境下有限访问神经网络推断的准确性与能量之间的权衡问题。

    

    深度神经网络在机器学习中已经无处不在，但其能量消耗仍然是一个值得关注的问题。降低供电电压是降低能量消耗的有效策略。然而，过度降低供电电压可能会导致准确性降低，因为模型参数存储在静态随机存储器(SRAM)中，而SRAM中会发生随机位翻转。为了解决这个挑战，我们引入了NeuralFuse，这是一个新颖的附加模块，通过学习输入转换来生成抗误差的数据表示，以在低电压环境中解决准确性与能量之间的权衡。NeuralFuse在标称电压和低电压情况下都能保护DNN的准确性。此外，NeuralFuse易于实现，并可以轻松应用于有限访问的DNN，例如不可配置的硬件或云端API的远程访问。实验结果表明，在1%的位错误率下，NeuralFuse可以将SRAM内存访问能量降低高达24%，同时保持准确性。

    Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains a notable issue. Lowering the supply voltage is an effective strategy for reducing energy consumption. However, aggressively scaling down the supply voltage can lead to accuracy degradation due to random bit flips in static random access memory (SRAM) where model parameters are stored. To address this challenge, we introduce NeuralFuse, a novel add-on module that addresses the accuracy-energy tradeoff in low-voltage regimes by learning input transformations to generate error-resistant data representations. NeuralFuse protects DNN accuracy in both nominal and low-voltage scenarios. Moreover, NeuralFuse is easy to implement and can be readily applied to DNNs with limited access, such as non-configurable hardware or remote access to cloud-based APIs. Experimental results demonstrate that, at a 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to 24% while imp
    
[^224]: 齐次空间上的潜在SDE

    Latent SDEs on Homogeneous Spaces. (arXiv:2306.16248v1 [cs.LG])

    [http://arxiv.org/abs/2306.16248](http://arxiv.org/abs/2306.16248)

    这篇论文研究了在齐次空间上的潜在SDE，通过使用单位球上的SDE进行变分推断，提出了一种简单且直观的表达式来计算近似后验和先验过程之间的KL散度。

    

    我们考虑在潜在变量模型中的变分贝叶斯推断问题，其中一个（可能是复杂的）观测随机过程由潜在随机微分方程（SDE）的解决方案所驱动。受到学习大规模数据中（几乎任意）潜在神经SDE时所面临的挑战的启发，例如效率梯度计算，我们退一步并研究了一个特定的子类。在我们的情况下，SDE在一个齐次潜在空间上演变，并由相应（矩阵）Lie群的随机动力学所诱导。在学习问题中，单位$n$-球上的SDE可以说是这一设置中最相关的。值得注意的是，在变分推断中，单位球不仅有助于使用真正无信息的先验SDE，而且我们还获得了关于近似后验和先验过程之间的Kullback-Leibler散度的特别简单和直观的表达式，这在证据下界中至关重要。实验证明了我们的方法的性能优势。

    We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn an (almost arbitrary) latent neural SDE from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves on a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In learning problems, SDEs on the unit $n$-sphere are arguably the most relevant incarnation of this setup. Notably, for variational inference, the sphere not only facilitates using a truly uninformative prior SDE, but we also obtain a particularly simple and intuitive expression for the Kullback-Leibler divergence between the approximate posterior and prior process in the evidence lower bound. Experiments demonstrat
    
[^225]: 基于层级神经模拟的事件集推断

    Hierarchical Neural Simulation-Based Inference Over Event Ensembles. (arXiv:2306.12584v1 [stat.ML])

    [http://arxiv.org/abs/2306.12584](http://arxiv.org/abs/2306.12584)

    本文介绍了一种基于层级神经模拟的方法，可以在似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断，着重考虑了模型的层级结构，可以导致更紧凑的参数约束。

    

    在实际数据分析中，事件集是常见的观测值集合，它们共同约束了感兴趣的模型参数。这些模型通常具有层级结构，其中“局部”参数影响单个事件，“全局”参数影响整个数据集。我们引入了实用的方法，用于处理似然函数不可计算但可以通过前向模拟实现的情况下，对整个数据集进行最优概率推断。我们构建了似然函数（比）或后验概率的神经估计器，并展示了明确考虑模型层级结构可以导致更紧凑的参数约束。我们以物理科学为例研究了本文讨论的内容，着重于粒子物理学（粒子对撞机数据）和天体物理学（强引力透镜观测）的案例。

    When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where "local" parameters impact individual events and "global" parameters influence the entire dataset. We introduce practical approaches for optimal dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via forward modeling. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics (particle collider data) and astrophysics (strong gravitational lensing observations).
    
[^226]: 巨型语言模型时代的AutoML：当前挑战，未来机遇和风险。

    AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])

    [http://arxiv.org/abs/2306.08107](http://arxiv.org/abs/2306.08107)

    论文探讨了AutoML和LLMs之间的共生关系，并指出这两个领域的融合有望颠覆NLP和AutoML两个领域，同时也存在风险。

    

    在过去的几年中，自然语言处理（NLP）和自动化机器学习（AutoML）领域取得了显著的成果。特别是在NLP领域，巨型语言模型（LLMs）最近经历了一系列突破。我们设想，两个领域通过紧密的融合可以彼此推动极限。为了展示这一愿景，我们探索了AutoML和LLMs之间的共生关系潜力，着重探讨了它们如何互相受益。我们特别研究了从不同角度增强LLMs的AutoML方法的机会以及利用AutoML进一步改进LLMs的挑战。为此，我们调查了现有工作，并对其中的风险进行了批判性评估。我们坚信，两个领域的融合有可能颠覆NLP和AutoML两个领域。通过强调可想象的协同作用和风险，我们旨在促进在交叉点的进一步探索。

    The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
    
[^227]: 一种简单统一的基于不确定性引导的离线到在线强化学习框架

    A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning. (arXiv:2306.07541v1 [cs.LG])

    [http://arxiv.org/abs/2306.07541](http://arxiv.org/abs/2306.07541)

    SUNG是一种基于不确定性引导的离线到在线强化学习框架，在通过量化不确定性进行探索和应用保守Q值估计的指导下，实现了高效的老化强化学习。

    

    离线强化学习为依靠数据驱动范例学习智能体提供了一种有前途的解决方案。 然而，受限于离线数据集的有限质量，其性能常常不够优秀。因此，在部署之前通过额外的在线交互进一步微调智能体是有必要的。不幸的是，由于受到两个主要挑战的制约，即受限的探索行为和状态-动作分布偏移，离线到在线强化学习可能具有挑战性。为此，我们提出了一个简单统一的基于不确定性引导的（SUNG）框架，其通过不确定性工具自然地统一了这两个挑战的解决方案。具体而言，SUNG通过基于VAE的状态-动作访问密度估计器量化不确定性。为了促进高效探索，SUNG提出了一种实用的乐观探索策略，以选择具有高价值和高不确定性的信息动作。此外，SUNG通过在不确定性指导下应用保守Q值估计来开发一种自适应利用方法。我们在Atari和MuJoCo基准测试上进行了全面的实验，结果表明SUNG始终优于最先进的离线到在线强化学习方法，并在许多任务中实现了接近在线学习的性能。

    Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conserva
    
[^228]: SENS：基于草图的隐式神经形状建模

    SENS: Sketch-based Implicit Neural Shape Modeling. (arXiv:2306.06088v1 [cs.GR])

    [http://arxiv.org/abs/2306.06088](http://arxiv.org/abs/2306.06088)

    SENS是一种基于草图的生成和编辑3D模型的新方法，该方法通过ViT补丁编码将草图映射到神经隐式形状架构的潜空间中，可以捕捉用户草图的意图进行生成，具有强大的性能和直观的基于草图的形状编辑功能。

    

    本文提出了SENS，一种从手绘草图中生成和编辑3D模型的新方法，包括那些抽象的草图。我们的方法允许用户快速轻松地草绘形状，然后将草图映射到一个面向部件的神经隐式形状架构的潜空间中。SENS分析草图并将其部件编码成ViT补丁编码，然后将其馈送到transformer解码器中，将其转换为适用于编辑3D隐式神经形状的形状嵌入。SENS不仅提供了直观的基于草图的生成和编辑，而且在捕捉用户草图意图方面表现出色，可以生成各种新颖和富有表现力的3D形状，甚至可以从抽象的草图中生成。我们使用客观指标评估标准和决定性用户研究来展示该模型的有效性，两者均表明它在具有中等抽象程度的草图中表现出强大的性能。此外，我们展示了它直观的基于草图的形状编辑功能。

    We present SENS, a novel method for generating and editing 3D models from hand-drawn sketches, including those of an abstract nature. Our method allows users to quickly and easily sketch a shape, and then maps the sketch into the latent space of a part-aware neural implicit shape architecture. SENS analyzes the sketch and encodes its parts into ViT patch encoding, then feeds them into a transformer decoder that converts them to shape embeddings, suitable for editing 3D neural implicit shapes. SENS not only provides intuitive sketch-based generation and editing, but also excels in capturing the intent of the user's sketch to generate a variety of novel and expressive 3D shapes, even from abstract sketches. We demonstrate the effectiveness of our model compared to the state-of-the-art using objective metric evaluation criteria and a decisive user study, both indicating strong performance on sketches with a medium level of abstraction. Furthermore, we showcase its intuitive sketch-based s
    
[^229]: 深度网络可以被剪枝到多么稀疏：几何视角下的研究

    How Sparse Can We Prune A Deep Network: A Geometric Viewpoint. (arXiv:2306.05857v1 [stat.ML])

    [http://arxiv.org/abs/2306.05857](http://arxiv.org/abs/2306.05857)

    本文从高维几何的角度，通过在原始损失函数中强制施加稀疏性约束，描述了深度网络剪枝比率的相变点，该点等于某些凸体的平方高斯宽度除以参数的原始维度。

    

    过度参数化是深度神经网络最重要的特征之一。虽然它可以提供出色的泛化性能，但同时也强加了重大的存储负担，因此有必要研究网络剪枝。一个自然而基本的问题是：我们能剪枝一个深度网络到多么稀疏（几乎不影响性能）？为了解决这个问题，本文采用了第一原理方法，具体地，只通过在原始损失函数中强制施加稀疏性约束，我们能够从高维几何的角度描述剪枝比率的尖锐相变点，该点对应于可行和不可行之间的边界。结果表明，剪枝比率的相变点等于某些凸体的平方高斯宽度，这些凸体是由$l_1$-规则化损失函数得出的，除以参数的原始维度。作为副产品，我们证明了剪枝过程中参数的分布性质。

    Overparameterization constitutes one of the most significant hallmarks of deep neural networks. Though it can offer the advantage of outstanding generalization performance, it meanwhile imposes substantial storage burden, thus necessitating the study of network pruning. A natural and fundamental question is: How sparse can we prune a deep network (with almost no hurt on the performance)? To address this problem, in this work we take a first principles approach, specifically, by merely enforcing the sparsity constraint on the original loss function, we're able to characterize the sharp phase transition point of pruning ratio, which corresponds to the boundary between the feasible and the infeasible, from the perspective of high-dimensional geometry. It turns out that the phase transition point of pruning ratio equals the squared Gaussian width of some convex body resulting from the $l_1$-regularized loss function, normalized by the original dimension of parameters. As a byproduct, we pr
    
[^230]: dotears: 使用观测和干预数据进行可扩展和一致的DAG估计

    dotears: Scalable, consistent DAG estimation using observational and interventional data. (arXiv:2305.19215v1 [stat.ML])

    [http://arxiv.org/abs/2305.19215](http://arxiv.org/abs/2305.19215)

    dotears是一个可扩展的DAG结构学习框架，使用观测和干预数据来推断单个因果结构。它直接估计外生误差结构，避免了循环估计问题。

    

    从数据中学习因果有向无环图 (DAG)面临着可辨识性缺失和解决方案组合空间的复杂性。最近的研究提高了观测数据中基于得分的DAG结构学习的可操作性，但对外生误差方差的结构敏感。同时，从观测数据学习外生方差结构需要结构的先验知识。针对新的生物技术，将高度并行的基因干预与高维观测数据联系起来，我们提出了一个可扩展的结构学习框架dotears，通过连续优化利用观测和干预数据来推断单个因果结构。dotears利用干预的可预测的结构后果直接估计外生误差结构，从而避免了循环估计问题。我们扩展了先前的工作，从经验和分析方面进行了展示。

    Learning causal directed acyclic graphs (DAGs) from data is complicated by a lack of identifiability and the combinatorial space of solutions. Recent work has improved tractability of score-based structure learning of DAGs in observational data, but is sensitive to the structure of the exogenous error variances. On the other hand, learning exogenous variance structure from observational data requires prior knowledge of structure. Motivated by new biological technologies that link highly parallel gene interventions to a high-dimensional observation, we present $\texttt{dotears}$ [doo-tairs], a scalable structure learning framework which leverages observational and interventional data to infer a single causal structure through continuous optimization. $\texttt{dotears}$ exploits predictable structural consequences of interventions to directly estimate the exogenous error structure, bypassing the circular estimation problem. We extend previous work to show, both empirically and analytical
    
[^231]: 旋转优化器：简单而强健的深度神经网络训练。

    Rotational Optimizers: Simple & Robust DNN Training. (arXiv:2305.17212v1 [cs.LG])

    [http://arxiv.org/abs/2305.17212](http://arxiv.org/abs/2305.17212)

    该论文提出了旋转优化器，这些优化器可以简化深度神经网络训练过程，甚至在几乎不需调整基线超参数的情况下与原始优化器的性能相匹配。

    

    现代深度神经网络的训练动态取决于学习率、权重衰减、初始化等超参数之间的复杂交互作用。这些交互作用可以在尺度不变层（如归一化层）中产生球面运动动态，这些动态收敛到平衡状态，其中权重范数和预期旋转更新大小是固定的。我们对AdamW、带动量的SGD和Lion中的这个平衡进行了分析，提供了关于不同超参数及其相互作用对训练过程的影响的新见解。我们提出了这些优化器的旋转变体（RVs），强制预期角度更新大小与整个训练期间的平衡值相匹配。这简化了训练动态，通过消除收敛到平衡状态的瞬态相应。我们的旋转优化器可以匹配原始变体的性能，通常需要对基线超参数进行最少或不调整。

    The training dynamics of modern deep neural networks depend on complex interactions between the learning rate, weight decay, initialization, and other hyperparameters. These interactions can give rise to Spherical Motion Dynamics in scale-invariant layers (e.g., normalized layers), which converge to an equilibrium state, where the weight norm and the expected rotational update size are fixed. Our analysis of this equilibrium in AdamW, SGD with momentum, and Lion provides new insights into the effects of different hyperparameters and their interactions on the training process. We propose rotational variants (RVs) of these optimizers that force the expected angular update size to match the equilibrium value throughout training. This simplifies the training dynamics by removing the transient phase corresponding to the convergence to an equilibrium. Our rotational optimizers can match the performance of the original variants, often with minimal or no tuning of the baseline hyperparameters,
    
[^232]: InstructIE: 一份基于指令的中文信息提取数据集

    InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])

    [http://arxiv.org/abs/2305.11527](http://arxiv.org/abs/2305.11527)

    介绍了一份中文的基于指令的信息提取数据集InstructIE，其中包括了270,000个弱监督的数据和1,000个高质量注释实例。实验结果表明当前的模型表现有待改进，该任务仍存在挑战。

    

    我们引入了一项新的信息提取任务，称为基于指令的信息提取 (Instruction-based IE)，它旨在要求系统遵循特定的指令或指南来提取信息。为了促进该领域的研究，我们构建了一个数据集，称为InstructIE，其中包括来自中文维基百科的 270,000 个弱监督数据和 1,000 个高质量众包注释实例。我们进一步评估了各种基线模型在InstructIE数据集上的表现。结果表明，尽管当前的模型表现很有希望，但仍有改进的空间。此外，我们进行了全面的案例研究分析，强调了基于指令的信息提取任务中固有的挑战。代码和数据集可在 https://github.com/zjunlp/DeepKE/tree/main/example/llm 找到。

    We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^233]: 有条件生成模型中的数据编辑

    Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])

    [http://arxiv.org/abs/2305.11351](http://arxiv.org/abs/2305.11351)

    本文研究如何对已训练好的条件生成模型进行后期编辑，以便编辑掉某些条件分支，这些条件分支很可能会生成不良内容。通过精简模型中的条件网络实现，提出的解决方案有效、高效、具有可控性和普适性，在文本到图像和文本到语音生成模型中取得了良好效果。

    

    深度生成模型因生成不良内容而受到批评。传统的缓解方法包括重新训练、过滤或编辑；然而这些方法要么计算成本高，要么会被第三方回避。本文提出一种不同的方法，研究如何后期编辑已经训练好的条件生成模型，使其编辑掉某些条件分支，这些条件分支很可能会生成不良内容。这是通过精简模型中的条件网络来实现的，提出的解决方案既有效又高效、具有可控性和普适性，能用于一类深度生成模型。我们在文本到图像生成模型和文本到语音生成模型中进行了数据编辑实验，并表明我们的方法计算成本较低，相比基线方法具有更好的编辑质量和鲁棒性，同时仍保持高生成质量。

    Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.
    
[^234]: 带应用于变分蒙特卡罗模拟的参数化球的随机梯度下降的收敛性分析。

    Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation. (arXiv:2303.11602v1 [cs.LG])

    [http://arxiv.org/abs/2303.11602](http://arxiv.org/abs/2303.11602)

    本论文在高维度球体上，通过神经网络参数化，使用SGD算法在监督学习和无监督学习中，提供了一种新算法，并且证明了其收敛性。

    

    本文分析了在由神经网络参数化为常数倍的高维球上使用随机梯度下降（SGD）类型算法。我们为有监督学习提供了一种新算法，并在理论和数值上证明了其收敛性。我们还首次提供了无监督设置的收敛证明，该设置对应于量子物理学中广泛使用的变分蒙特卡罗（VMC）方法。

    We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics.
    
[^235]: 自动设计元启发式算法的综述

    A Survey on Automated Design of Metaheuristic Algorithms. (arXiv:2303.06532v1 [cs.NE])

    [http://arxiv.org/abs/2303.06532](http://arxiv.org/abs/2303.06532)

    本文综述了自动设计元启发式算法的形式化、方法论、挑战和研究趋势，讨论了自动设计的潜在未来方向和开放问题。

    This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, and discusses the potential future directions and open issues in this field.

    元启发式算法由于其能够独立于问题结构和问题领域进行搜索的能力，已经引起了学术界和工业界的广泛关注。通常，需要人类专家手动调整算法以适应解决目标问题。手动调整过程可能是费力的、容易出错的，并且需要大量的专业知识。这引起了对自动设计元启发式算法的越来越多的兴趣和需求，以减少人类干预。自动设计可以使高性能算法对更广泛的研究人员和实践者可用；通过利用计算能力来充分探索潜在的设计选择，自动设计可以达到甚至超过人类水平的设计。本文通过对现有工作的共同点和差异进行调查，提出了自动设计元启发式算法的形式化、方法论、挑战和研究趋势的广泛概述。我们还讨论了这一领域的潜在未来方向和开放问题。

    Metaheuristic algorithms have attracted wide attention from academia and industry due to their capability of conducting search independent of problem structures and problem domains. Often, human experts are requested to manually tailor algorithms to fit for solving a targeted problem. The manual tailoring process may be laborious, error-prone, and require intensive specialized knowledge. This gives rise to increasing interests and demands for automated design of metaheuristic algorithms with less human intervention. The automated design could make high-performance algorithms accessible to a much broader range of researchers and practitioners; and by leveraging computing power to fully explore the potential design choices, automated design could reach or even surpass human-level design. This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and 
    
[^236]: 规范化交叉密度函数：一种用于量化随机过程统计依赖关系的框架

    The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes. (arXiv:2212.04631v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04631](http://arxiv.org/abs/2212.04631)

    本文提出了一种用于量化随机过程统计依赖关系的框架，通过最大化交替协方差估计和规范化交叉密度来衡量多变量统计依赖性，并应用于机器学习架构中。

    

    本文提出了一种基于阿尔弗雷德·雷尼（Alfr\'ed R\'enyi）的功能方法论，通过对两个连续随机过程（r.p.）之间的统计依赖关系进行新颖的多变量定义。将随机过程样本对的互信息的对数论证命名为规范化交叉密度（NCD），定义了一种对称和自伴的正定函数。我们证明，最大化交替协方差估计（ACE）递归应用于输入样本对的联合概率密度，符合雷尼的最大相关性的所有性质。我们提出了NCD的特征谱作为一种新颖的多变量度量，用于衡量输入和输出r.p.之间的统计依赖关系。利用r.p.的实现，也可以直接估计多变量统计依赖性。提出的功能最大相关算法（FMCA）应用于由两个神经网络构建的机器学习架构上，通过逼近联合训练来同时学习。

    This paper proposes a novel multivariate definition of statistical dependence between two continuous random processes (r.p.) using a functional methodology inspired by Alfr\'ed R\'enyi. The argument of the logarithm of mutual information between pairs of samples of a r.p., named here the normalized cross density (NCD), defines a symmetric and self-adjoint positive definite function. We show that maximizing the alternating covariance estimation (ACE) recursion, applied to each of the joint probability density of input sample pairs, obeys all the properties of Renyi's maximal correlation. We propose the NCD's eigenspectrum as a novel multivariate measure of the statistical dependence between the input and output r.p.  The multivariate statistical dependence can also be estimated directly from r.p. realizations. The proposed functional maximum correlation algorithm (FMCA) is applied to a machine learning architecture built from two neural networks that learn concurrently by approximating 
    
[^237]: RenderDiffusion: 用于3D重建、修复和生成的图像扩散方法

    RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation. (arXiv:2211.09869v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.09869](http://arxiv.org/abs/2211.09869)

    本文提出了第一个支持3D理解任务的扩散模型RenderDiffusion，只需使用单眼2D监督进行训练。它利用一种新颖的图像去噪架构，生成和渲染中间的三维表示，在扩散过程中提供强有力的3D一致性。

    

    目前，扩散模型在有条件和无条件的图像生成方面均达到了最先进的水平。但是，目前的图像扩散模型不支持用于3D理解所需的任务，例如视角一致的3D生成或单视角物体重建。本文提出了RenderDiffusion，这是第一个用于3D生成和推断的扩散模型，只需使用单眼2D监督进行训练。方法的核心是一种新颖的图像去噪架构，在每个去噪步骤中生成和渲染场景的中间三维表示。这在扩散过程中强制实现了一个强的归纳结构，提供了3D一致的表示，同时只需要2D监督。生成的3D表示可以从任何视角渲染。我们评估了RenderDiffusion在FFHQ、AFHQ、ShapeNet和CLEVR数据集上的性能，显示出了在生成3D场景和从2D图像推断3D场景方面具有竞争力的表现。

    Diffusion models currently achieve state-of-the-art performance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruction. In this paper, we present RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong inductive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervision. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive performance for generation of 3D scenes and inference of 3D scenes from 2D images. Ad
    
[^238]: 通过运动和物体连续性提升物体表示学习

    Boosting Object Representation Learning via Motion and Object Continuity. (arXiv:2211.09771v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.09771](http://arxiv.org/abs/2211.09771)

    通过集成运动和连续性信息，我们提出了一种提升物体表示学习的方法，可以在无监督多对象检测模型中产生更优的物体编码，并在物体发现、收敛速度和总体潜在物体表示等方面取得明显的改进。

    

    最近的无监督多对象检测模型在性能方面取得了令人印象深刻的改进，这主要归因于新颖的架构偏见。不幸的是，它们可能为下游任务产生次优的物体编码。为了克服这一问题，我们提出利用物体运动和连续性，即物体不能突然出现和消失。这通过两个机制实现：（i）通过集成光流提供物体位置的先验知识，（ii）对连续图像帧之间的对比物体连续性损失进行建模。与开发显式深度架构不同，得到的运动和物体连续性（MOC）方案可以使用任何基线对象检测模型来实现。我们的结果表明，相对于玩Atari游戏而言，SOTA模型在物体发现、收敛速度和总体潜在物体表示方面都有很大的改进。总的来说，我们展示了整合运动和连续性的明显好处。

    Recent unsupervised multi-object detection models have shown impressive performance improvements, largely attributed to novel architectural inductive biases. Unfortunately, they may produce suboptimal object encodings for downstream tasks. To overcome this, we propose to exploit object motion and continuity, i.e., objects do not pop in and out of existence. This is accomplished through two mechanisms: (i) providing priors on the location of objects through integration of optical flow, and (ii) a contrastive object continuity loss across consecutive image frames. Rather than developing an explicit deep architecture, the resulting Motion and Object Continuity (MOC) scheme can be instantiated using any baseline object detection model. Our results show large improvements in the performances of a SOTA model in terms of object discovery, convergence speed and overall latent object representations, particularly for playing Atari games. Overall, we show clear benefits of integrating motion and
    
[^239]: 随机逼近和强化学习中渐近统计的ODE方法

    The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning. (arXiv:2110.14427v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2110.14427](http://arxiv.org/abs/2110.14427)

    本文提出了一种称为ODE方法的渐近统计方法解决$d$维随机逼近递归的问题，证明了其收敛性和中心极限定理，为强化学习等领域的应用提供了有力的理论支持。

    

    本文研究了$d$维随机逼近递归$$\theta_{n+1}=\theta_n+\alpha_{n+1}f(\theta_n, \Phi_{n+1})$$其中$\Phi$是一个在一般状态空间$\textsf{X}$上具有平稳分布$\pi$的几何遍历马尔可夫链，$f：\Re^d\times\textsf{X}\to\Re^d$。在称为（DV3）的Donsker-Varadhan Lyapunov漂移条件的一种版本和对具有向量场$\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$以及$\Phi\sim\pi$的均值流的稳定性条件下，建立了主要结果。(i) $\{\theta_n\}$以概率1和$L_4$收敛于$\bar{f}(\theta)$的唯一根$\theta^*$。(ii) 建立了泛函中心极限定理，以及归一化误差一维中心极限定理。(iii) 对于归一化版本$z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$的平均参数$\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$ ，在步长的标准假设下，建立了中心极限定理。

    The paper concerns the $d$-dimensional stochastic approximation recursion, $$ \theta_{n+1}= \theta_n + \alpha_{n + 1} f(\theta_n, \Phi_{n+1}) $$ in which $\Phi$ is a geometrically ergodic Markov chain on a general state space $\textsf{X}$ with stationary distribution $\pi$, and $f:\Re^d\times\textsf{X}\to\Re^d$.  The main results are established under a version of the Donsker-Varadhan Lyapunov drift condition known as (DV3), and a stability condition for the mean flow with vector field $\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$, with $\Phi\sim\pi$.  (i) $\{ \theta_n\}$ is convergent a.s. and in $L_4$ to the unique root $\theta^*$ of $\bar{f}(\theta)$.  (ii) A functional CLT is established, as well as the usual one-dimensional CLT for the normalized error.  (iii) The CLT holds for the normalized version, $z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$, of the averaged parameters, $\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$, subject to standard assumptions on the step-s
    
[^240]: 机器学习中的拒绝选项：一项调查

    Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.11277](http://arxiv.org/abs/2107.11277)

    这项调查综述了机器学习中的拒绝选项。通过机器学习模型避免在可能犯错误时做出预测，可以在决策支持应用中避免严重后果。调查介绍了拒绝选项的条件、评估策略以及相关应用领域，并探讨了它与其他机器学习方法的关系。

    

    机器学习模型总是做出预测，即使可能是不准确的。在许多决策支持应用中，应避免这种行为，因为错误可能带来严重后果。尽管在1970年已经研究过，但近年来机器学习中的拒绝选项引起了人们的关注。这个机器学习子领域使得机器学习模型能够在可能犯错误时避免做出预测。本调查旨在提供机器学习中拒绝选项的概述。我们介绍了导致两种拒绝情况（模糊和新奇拒绝）的条件，并对其进行了仔细的形式化。此外，我们还回顾和分类了评估模型预测和拒绝质量的策略。此外，我们定义了现有的带有拒绝选项的模型架构，并描述了学习这些模型的标准技术。最后，我们提供了相关应用领域的示例，并展示了机器学习中的拒绝选项与其他机器学习方法之间的关系。

    Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.  This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machi
    

