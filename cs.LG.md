# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories.](http://arxiv.org/abs/2304.05371) | 本文研究了聊天机器人的一个新发展：长期记忆机制。然而，我们发现这种机制可能会导致机器人记住了错误和虚假的信息，并将其作为事实陈述重复。 |
| [^2] | [A surprisingly simple technique to control the pretraining bias for better transfer: Expand or Narrow your representation.](http://arxiv.org/abs/2304.05369) | 本研究提出了一种简单的方法来控制预训练偏差，即通过扩展或缩小骨干网络的表示维度，从而显著提高了下游迁移性能。 |
| [^3] | [The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning.](http://arxiv.org/abs/2304.05366) | 本论文阐述了无免费午餐定理的监督学习中的限制，证明了归纳偏差可以提高学习算法的效果，并且展示了神经网络模型的偏好与现实世界的数据分布相关。 |
| [^4] | [Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling.](http://arxiv.org/abs/2304.05365) | 本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。 |
| [^5] | [Diffusion Models for Constrained Domains.](http://arxiv.org/abs/2304.05364) | 本研究提出了两种方法来创建约束域的降噪扩散模型。第一种方法基于不等式约束诱导的对数障碍度量，第二种方法基于反射布朗运动。这些方法将扩散模型的应用范围扩展到了机器人和蛋白设计等领域。 |
| [^6] | [Asymmetric Polynomial Loss For Multi-Label Classification.](http://arxiv.org/abs/2304.05361) | 本文提出了一种有效的非对称多项式损失（APL），可以根据不同任务优化模型，并在关系提取、文本分类和图像分类上表现良好。 |
| [^7] | [iDML: Incentivized Decentralized Machine Learning.](http://arxiv.org/abs/2304.05354) | 通过区块链的激励机制，让用户资源贡献变得可行，实现去中心化的机器学习。 |
| [^8] | [The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges.](http://arxiv.org/abs/2304.05351) | 本研究对ChatGPT在股票预测方面进行了零样本分析，结果表明其预测股票移动的表现不如最先进和传统方法，需要进一步改进。 |
| [^9] | [Deep-learning assisted detection and quantification of (oo)cysts of Giardia and Cryptosporidium on smartphone microscopy images.](http://arxiv.org/abs/2304.05339) | 本研究采用基于深度学习的RetinaNet模型针对采用智能手机显微系统检测贾第虫和隐孢子进行检测和计数，并在速度和准确度方面表现出最佳效果，为在资源有限的环境下解决这一问题提供了潜在解决方案。 |
| [^10] | [Toxicity in ChatGPT: Analyzing Persona-assigned Language Models.](http://arxiv.org/abs/2304.05335) | 论文分析了基于对话的大型语言模型ChatGPT中的毒性，通过指定人物角色，输出会涉及刻板印象、有害对话和伤人的言论。因此，我们需要充分理解LLMs的能力和局限性，以确保这些系统的安全性。 |
| [^11] | [Generative Modeling via Hierarchical Tensor Sketching.](http://arxiv.org/abs/2304.05305) | 本文提出了一种利用分层张量草图来近似高维概率密度的方法，通过随机奇异值分解技术解决线性方程达到此目的，其算法复杂度在高维密度维度上呈线性规模。 |
| [^12] | [TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training.](http://arxiv.org/abs/2304.05301) | TACOS 是一个能够自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。与基准算法相比，TACOS 合成的 All-Reduce 算法速度提高了 3.73 倍，为 512-NPU 系统合成集体算法只需 6.1 分钟。 |
| [^13] | [A Comprehensive Study on Object Detection Techniques in Unconstrained Environments.](http://arxiv.org/abs/2304.05295) | 本文全面研究了面向非约束环境的目标检测技术，包括挑战、数据集和最先进的方法，并进行了方法的比较分析，提供了未来的研究方向。 |
| [^14] | [Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery.](http://arxiv.org/abs/2304.05294) | 本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。 |
| [^15] | [Equivariant Graph Neural Networks for Charged Particle Tracking.](http://arxiv.org/abs/2304.05293) | 本文提出了对称等变GNN模型EuclidNet，适用于带电粒子跟踪，通过旋转对称性处理碰撞事件的图表示，实现更高效的模型。实验结果表明EuclidNet在小模型规模下比非等变基准更有效。 |
| [^16] | [MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos.](http://arxiv.org/abs/2304.05292) | 本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。 |
| [^17] | [Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning.](http://arxiv.org/abs/2304.05288) | 本文提出了一种任务难度感知的增量学习参数分配与正则化方法，该方法可以根据任务的学习难度自适应地选择适当的分配或正则化策略，以克服在学习不同任务时的挑战。 |
| [^18] | [Controllable Textual Inversion for Personalized Text-to-Image Generation.](http://arxiv.org/abs/2304.05265) | 本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。 |
| [^19] | [Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning.](http://arxiv.org/abs/2304.05260) | 本文提出一种重新加权softmax的交叉熵方法来解决联邦学习中客户端的灾难性遗忘问题，并证明这种方法可以缓解客户端遗忘并对标准联邦学习算法提供一致的改进。 |
| [^20] | [Multi-granulariy Time-based Transformer for Knowledge Tracing.](http://arxiv.org/abs/2304.05257) | 本文提出了一种基于Transformer的架构用于准确地预测学生在标准化测试中的表现。该模型考虑了学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，并在解码器输入中使用了多个时间特征粒度以显著提高模型性能。与LightGBM相比，该方法更加准确，为教育领域的AI发展提供了一个可伸缩和准确的预测学生成果的工具。 |
| [^21] | [OpenAL: Evaluation and Interpretation of Active Learning Strategies.](http://arxiv.org/abs/2304.05246) | OpenAL是一个灵活且开源的框架，可以在一系列现实任务上轻松运行和比较采样AL策略，具有可解释性指标和统计分析方法，针对主动学习的一次性特性，从业人员也可以轻松扩展基准测试。 |
| [^22] | [r-softmax: Generalized Softmax with Controllable Sparsity Rate.](http://arxiv.org/abs/2304.05243) | 本文提出了一种新的广义Softmax函数r-softmax，可以输出具有可控稀疏度的概率分布，相较于现有的替代方案效果更好，在多标签数据集上表现突出，在预训练转换语言模型的自我注意模块中具有重要应用。 |
| [^23] | [Mask-conditioned latent diffusion for generating gastrointestinal polyp images.](http://arxiv.org/abs/2304.05233) | 本文提出了一种基于掩蔽条件潜在扩散的方法，可生成给定肿瘤掩模条件下的合成 GI 息肉图像，并证明该方法可以生成无限量高保真合成息肉图像，为消除内镜诊断中的有限标注问题提供了可行性。 |
| [^24] | [Lady and the Tramp Nextdoor: Online Manifestations of Real-World Inequalities in the Nextdoor Social Network.](http://arxiv.org/abs/2304.05232) | 本文第一个大规模研究了Nextdoor社交网络，发现不同收入水平的社区在社交网络上的在线行为不同，更富裕的社区情感更积极，更多地讨论犯罪，尽管实际犯罪率要低得多，同时用户生成内容能够预测收入和不平等。 |
| [^25] | [Inhomogeneous graph trend filtering via a l2,0 cardinality penalty.](http://arxiv.org/abs/2304.05223) | 本文提出了一种基于L2，0基数惩罚的图趋势过滤（GTF）模型，可同时进行k-means聚类和基于图的最小割，以估计在节点之间具有不均匀平滑水平的分段平滑图信号，并在降噪、支持恢复和半监督分类任务上表现更好，比现有方法更高效地处理大型数据集。 |
| [^26] | [BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments.](http://arxiv.org/abs/2304.05219) | 提出了一种名为BanditQ的新的在线预测策略，在敌对设置中以公平的方式达到目标速率约束，并实现了$O(T^{3/4})$的遗憾。 |
| [^27] | [A Billion-scale Foundation Model for Remote Sensing Images.](http://arxiv.org/abs/2304.05215) | 本文介绍了一个用于遥感图像的十亿级基础模型，并研究了增加模型参数数量对该模型在下游任务中的性能影响，实验显示增加模型参数数量可以显著提高性能。 |
| [^28] | [CGXplain: Rule-Based Deep Neural Network Explanations Using Dual Linear Programs.](http://arxiv.org/abs/2304.05207) | 该论文介绍了一种名为 CGX 的分解方法，使用双线性规划从深度神经网络的隐藏表示中提取规则，优化对齐，复杂度和稳定性。 |
| [^29] | [The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2304.05206) | 本文研究了多元时间序列预测中的通道相关（CD）与通道独立（CI）策略。结果表明，CD策略容量更高但鲁棒性较差，CI策略鲁棒性更强且能更好地处理缺失值和不规则时间间隔。 |
| [^30] | [TinyReptile: TinyML with Federated Meta-Learning.](http://arxiv.org/abs/2304.05201) | TinyReptile是一个联邦元学习实现的迷你机器学习算法，可以在迷你设备上协作学习神经网络，并通过使用全局数据实现快速收敛和保护本地数据隐私。 |
| [^31] | [HPN: Personalized Federated Hyperparameter Optimization.](http://arxiv.org/abs/2304.05195) | 该论文提出了一种新的学习超参数网络方案(HPN)，该网络可以使用客户端编码来决定个性化的超参数选择，同时保护客户端的隐私，以解决联邦学习中客户端异质性的问题，并提出了一种去除函数评估偏差的机制。 |
| [^32] | [Automatic Gradient Descent: Deep Learning without Hyperparameters.](http://arxiv.org/abs/2304.05187) | 本文提出了一种使用神经网络结构信息定义优化算法的方法，实现了一种无需手动调整超参数的一阶优化器 - 自动梯度下降。该算法在深度全连接网络和卷积网络中表现良好，并在标准基准测试数据集上表现出与手动调整优化器相当的效果。 |
| [^33] | [Decoupling anomaly discrimination and representation learning: self-supervised learning for anomaly detection on attributed graph.](http://arxiv.org/abs/2304.05176) | 本文提出了一种独特的自我监督算法DSLAD，通过解耦异常判别和表示学习来进行属性图上的异常检测，构建了一个平衡的特征空间解决了语义混合和不平衡问题，证明了其有效性。 |
| [^34] | [Electricity Demand Forecasting with Hybrid Statistical and Machine Learning Algorithms: Case Study of Ukraine.](http://arxiv.org/abs/2304.05174) | 本文提出了一种新型的混合方法，使用统计和机器学习的算法结合预测电力需求，并成功地应用于乌克兰的电力消耗数据。方法中包括了宏观经济回归分析、温度和日历回归变量相结合、ARIMA和LSTM“黑匣子”模型等，使得预测能够覆盖长、中、短期不同阶段以及小时季节性。 |
| [^35] | [Improving Image Recognition by Retrieving from Web-Scale Image-Text Data.](http://arxiv.org/abs/2304.05173) | 本论文介绍了一种基于注意力的记忆模块，通过从存储器集合中检索相似实例，消除了不相关实例的影响，保留了有益于输入查询的实例。使用大规模记忆数据集的实验表明该方法在三个不同的分类任务中实现了一流的成果。 |
| [^36] | [Curriculum-Based Imitation of Versatile Skills.](http://arxiv.org/abs/2304.05171) | 论文提出了基于课程的多技能模仿学习算法，以专业化处理局部内容，同时通过奖励机制激励尽可能多地涵盖数据。 |
| [^37] | [Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification.](http://arxiv.org/abs/2304.05165) | 提出了一种基于不确定性的不完整多视图数据分类模型，通过构建分布并多次抽样，实现更加可感知的填充和可控的融合。 |
| [^38] | [NeAT: Neural Artistic Tracing for Beautiful Style Transfer.](http://arxiv.org/abs/2304.05139) | NeAT是一种神经艺术追踪技术，它重新将前馈式风格转移表述为图像编辑，能在保持源内容和匹配目标风格方面都达到最先进的效果，而且在识别和修复"样式光环"方面也更加出色，使用BBST-4M数据集提高了其在各种风格下的泛化能力。 |
| [^39] | [Modeling and design of heterogeneous hierarchical bioinspired spider web structures using generative deep learning and additive manufacturing.](http://arxiv.org/abs/2304.05137) | 本文采用生成式深度学习和增材制造来建模和设计基于蜘蛛网的生物启发式三维网状结构，以解决其复杂的设计特征带来的挑战性。 |
| [^40] | [RecUP-FL: Reconciling Utility and Privacy in Federated Learning via User-configurable Privacy Defense.](http://arxiv.org/abs/2304.05135) | 该论文提出了一种名为RecUP-FL的用户可配置隐私防护机制，该机制可以更好地平衡隐私和效用，提高了联邦学习中模型的效果。 |
| [^41] | [Neural Network Architectures.](http://arxiv.org/abs/2304.05133) | 这份讲义概述了神经网络的数学视角，并介绍了前馈神经网络、卷积神经网络、残差网络和循环神经网络等架构，这些架构给机器学习提供了一种优化问题的思路。 |
| [^42] | [Improving Performance of Private Federated Models in Medical Image Analysis.](http://arxiv.org/abs/2304.05127) | 本论文通过本地步骤和调整功能进一步提高医学影像分析中的联邦学习模型性能。 |
| [^43] | [Online Spatio-Temporal Learning with Target Projection.](http://arxiv.org/abs/2304.05124) | 提出了一种名为OSTTP的新型学习算法，解决了通过时间反向传播算法所引入的限制，使网络能够同时处理和学习新的传入数据，具有竞争性能。 |
| [^44] | [Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction.](http://arxiv.org/abs/2304.05116) | 使用深度学习模型进行运动预测在自动驾驶中表现出色，但缺乏解释性和可能违反物理约束。因此，结合差分约束运动模型能提供物理上可行的轨迹，研究表明低阶积分器模型表现更好，并且数值求解器对模型性能产生影响。 |
| [^45] | [Towards systematic intraday news screening: a liquidity-focused approach.](http://arxiv.org/abs/2304.05115) | 该论文提出了一种系统化的新闻筛选方法，以识别有影响力的新闻，基于流动性驱动的变量，并利用朴素贝叶斯方法分析相关新闻情绪。 |
| [^46] | [Actually Sparse Variational Gaussian Processes.](http://arxiv.org/abs/2304.05091) | 提出了一种新的跨领域变量高斯过程类，利用紧支撑B样条基函数，可以使用稀疏线性代数来显著加快矩阵运算， 实现在大规模数据集下快速高效地建模快速变化的空间现象。 |
| [^47] | [A Self-attention Knowledge Domain Adaptation Network for Commercial Lithium-ion Batteries State-of-health Estimation under Shallow Cycles.](http://arxiv.org/abs/2304.05084) | 提出了一种无监督深度迁移学习方法，使用自我关注蒸馏模块和多核最大均值差异技术来跨越不同领域的鸿沟，自动从充电曲线中提取领域特定功能，并成功地将知识从大规模标记的完整循环转移到未标记的浅循环，实现了对商用锂离子电池健康状态的准确估计。 |
| [^48] | [TodyNet: Temporal Dynamic Graph Neural Network for Multivariate Time Series Classification.](http://arxiv.org/abs/2304.05078) | TodyNet 提出了一种新的时间动态图神经网络可以提取隐藏的时空依赖关系，通过动态图机制捕获不同时间槽之间的关联，有效地解决了多变量时间序列分类中出现的问题。 |
| [^49] | [A Tale of Sampling and Estimation in Discounted Reinforcement Learning.](http://arxiv.org/abs/2304.05073) | 本文通过最小值上界提出了折扣均值估计问题的估计误差与马尔可夫过程混合特性和折扣因子之间的明确联系，并对一组显著估计器及其对应的采样程序进行了统计分析。 |
| [^50] | [Hyperbolic Geometric Graph Representation Learning for Hierarchy-imbalance Node Classification.](http://arxiv.org/abs/2304.05059) | 本研究利用双曲几何有效表达了图的分层属性，并探究了不同层次属性训练节点对节点分类任务的影响。 |
| [^51] | [A Comprehensive Survey on Deep Graph Representation Learning.](http://arxiv.org/abs/2304.05055) | 本文综述了深度图表示学习的研究现状和存在的问题，并指出利用深度学习已经显示出巨大的优势和潜力。 |
| [^52] | [A Hybrid Approach combining ANN-based and Conventional Demapping in Communication for Efficient FPGA-Implementation.](http://arxiv.org/abs/2304.05042) | 本文提出了一种新的方法，在FPGA上通过结合传统信号解析算法和自适应的自编码器实现高效的信号映射，克服了ANN网络在资源受限设备中的性能瓶颈。 |
| [^53] | [Soft Dynamic Time Warping for Multi-Pitch Estimation and Beyond.](http://arxiv.org/abs/2304.05032) | 本文介绍了软动态时间规整（SoftDTW）作为连续时间分类（CTC）的一种优秀替代方案，可以更有效地解决弱对齐数据的特征表示，特别适用于多标签问题和实值目标序列。 |
| [^54] | [Learning Optimal Fair Scoring Systems for Multi-Class Classification.](http://arxiv.org/abs/2304.05023) | 本文使用 MIP 技术生成公平且可解释的评分系统解决一般多类分类问题，扩展了 SLIM 框架，实验证明该方法可产生与现有方法竞争性相当的预测性能。 |
| [^55] | [A Deep Analysis of Transfer Learning Based Breast Cancer Detection Using Histopathology Images.](http://arxiv.org/abs/2304.05022) | 本文研究了基于组织病理学图像的乳腺癌检测，采用 ResNet50 模型进行深度迁移学习，实现了在准确率、AUC、召回率和损失率等指标方面的最佳表现。 |
| [^56] | [Habits and goals in synergy: a variational Bayesian framework for behavior.](http://arxiv.org/abs/2304.05008) | 本文提出一种基于变分贝叶斯理论的行为模型，通过引入一种贝叶斯潜变量“意图”，将习惯性行为和目标导向行为联系起来，从而实现更灵活、高效的行为模拟。 |
| [^57] | [Bayes correlated equilibria and no-regret dynamics.](http://arxiv.org/abs/2304.05005) | 本文提出了贝叶斯相关均衡的一个概念，可以以分布式方式有效地计算和实现，并在广泛的博弈类别中达到近似最优的社会福利，在实验中验证了其有效性。 |
| [^58] | [Neural Multi-network Diffusion towards Social Recommendation.](http://arxiv.org/abs/2304.04994) | NeMo是一个基于神经网络多网络扩散的社交推荐模型，采用生成式的负采样策略，并利用正负用户-商品交互来进行用户兴趣传播。NeMo在各种真实世界基准数据集上均优于现有基线。 |
| [^59] | [Detecting Anomalous Microflows in IoT Volumetric Attacks via Dynamic Monitoring of MUD Activity.](http://arxiv.org/abs/2304.04987) | 本文使用SDN技术来监测和强制每个IoT设备的预期行为，并利用MUD标准减少攻击形势，通过训练单类分类模型，动态检测网络活动中的异常模式以检测大流量攻击，同时保持误报率低。 |
| [^60] | [Biological Factor Regulatory Neural Network.](http://arxiv.org/abs/2304.04982) | 本文提出了一种生物因子调控神经网络（BFReg-NN），它使用基因表达数据，并将大多数现有的生物学知识（如基因调节网络和GO）集成到模型中，以模拟生物系统中生物因子之间的关系，并在模拟数据和真实数据上取得了优越的性能和可解释性。 |
| [^61] | [Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR.](http://arxiv.org/abs/2304.04974) | Wav2code是一种基于自监督学习的ASR模型，可以实现用于噪声鲁棒的无失真增强，从而提供更好的语音表征。 |
| [^62] | [Federated Learning with Classifier Shift for Class Imbalance.](http://arxiv.org/abs/2304.04972) | 本文提出了一种名为FedShift的简单有效方法，通过在本地训练阶段在分类器输出上添加偏移以解决类别不平衡问题，并在各种数据集上表现优异。 |
| [^63] | [GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning.](http://arxiv.org/abs/2304.04970) | 本文提出一种名为GRIL的方法，用于将拓扑特征表示散度到机器学习模型中，该方法可以稳定地用于不同的过滤函数。 |
| [^64] | [Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond.](http://arxiv.org/abs/2304.04968) | 本研究提出了Perp-Neg算法，通过利用得分空间的几何特性来解决目前文本到图像扩散模型中负面提示算法存在的问题，使得用户能够编辑掉初始生成图像中不想要的概念，从而提供了更大的灵活性。同时，我们还通过提出基于Perp-Neg的3D负面提示算法，将算法扩展到3D应用中。 |
| [^65] | [A priori compression of convolutional neural networks for wave simulators.](http://arxiv.org/abs/2304.04964) | 该论文提出了一种在训练神经网络之前压缩卷积层的张量格式方法，通过替换多维核为一维滤波器来减少CNN模型的大小和减少过度拟合，从而提高实时准确预测的速度。 |
| [^66] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^67] | [Robust Dequantization of the Quantum Singular value Transformation and Quantum Machine Learning Algorithms.](http://arxiv.org/abs/2304.04932) | 本文研究了量子机器学习算法的鲁棒去量子化方法。我们提出了近似长度平方采样的概念，并展示了如何将随机线性代数技术适应到这种更弱的假设下。我们使用这些技术证明了最近的低秩去量子化框架和spa去量子化框架。 |
| [^68] | [A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models.](http://arxiv.org/abs/2304.04916) | 本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。 |
| [^69] | [Financial Time Series Forecasting using CNN and Transformer.](http://arxiv.org/abs/2304.04912) | 本文提出了结合使用CNN和Transformer来预测金融时间序列的涨跌幅。 |
| [^70] | [Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator.](http://arxiv.org/abs/2304.04911) | 本研究论文探讨了一种实时无模型深度强化学习技术用于系列弹性执行器的力量控制，证明了这种技术在连续控制任务中的有效性和在硬件学习中的应用价值。 |
| [^71] | [Improving Vision-and-Language Navigation by Generating Future-View Image Semantics.](http://arxiv.org/abs/2304.04907) | 本文提出了三个代理任务用于在代理的领域内预训练，以帮助模型生成未来视图图像语义，从而在视觉与语言导航任务中提高性能。 |
| [^72] | [Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing.](http://arxiv.org/abs/2304.04906) | 本文综合论述了文献中提出的带有拒绝选项的预测方法在各种神经网络中的应用，讨论了不同的损失函数以及网络输出的训练后处理，最终探讨了拒绝选项在减少实时预测时间方面的应用。 |
| [^73] | [Neural Network Predicts Ion Concentration Profiles under Nanoconfinement.](http://arxiv.org/abs/2304.04896) | 本论文提出了用神经网络预测纳米通道中离子浓度分布的方法，可以作为MD模拟的快速替代模型，比XGBoost有更高的准确性并且具有灵活性。 |
| [^74] | [ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment.](http://arxiv.org/abs/2304.04874) | 本文提出了一种新的图像字幕生成器 ImageCaptioner$^2$ ，用于针对图像字幕偏差放大进行评估。 |
| [^75] | [DASS Good: Explainable Data Mining of Spatial Cohort Data.](http://arxiv.org/abs/2304.04870) | 该论文描述了一个名为DASS的协同设计建模系统，结合人-机循环视觉引导、空间数据和可解释的AI，自动数据挖掘增加领域知识，在头颈癌患者的长期毒副作用预测模型开发中具有实际应用及领域专家反馈，为解决临床机器学习模型在数据包括空间信息时的困难提供了一种新兴方法。 |
| [^76] | [A few-shot graph Laplacian-based approach for improving the accuracy of low-fidelity data.](http://arxiv.org/abs/2304.04862) | 本文提出的方法利用少量高保真数据来提高大量低保真数据的准确性，方法包括构建图拉普拉斯和计算其低能量谱来将数据聚类，并获取关键点的高保真数据进行转换。 |
| [^77] | [ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping.](http://arxiv.org/abs/2304.04861) | ShapeShift是一个机器人抓取物体姿态估计的框架，它使用超椭球的参考形状预测物体的姿态，具有广泛的泛化能力和准确性。 |
| [^78] | [Simulated Annealing in Early Layers Leads to Better Generalization.](http://arxiv.org/abs/2304.04858) | 使用模拟退火在早期层次的网络中可以提高泛化性能，并且在Tiny-ImageNet数据集基准测试和一系列传递学习和少量样本学习任务上表现得比LLF更好。 |
| [^79] | [iPINNs: Incremental learning for Physics-informed neural networks.](http://arxiv.org/abs/2304.04854) | 提出了一种增量学习方法，可以顺序学习多个物理方程，而无需为新任务添加额外参数，在多个测试问题上展现了较高准确性和更快的收敛速度。 |
| [^80] | [Deploying Machine Learning Models to Ahead-of-Time Runtime on Edge Using MicroTVM.](http://arxiv.org/abs/2304.04842) | 本文介绍了使用MicroTVM在边缘设备上部署机器学习模型的方法，可以将预训练模型解析为后端的C源代码库，并使用自动生成的Ahead-of-Time C运行时在ARM Cortex M4F核心上进行手势识别实验。 |
| [^81] | [MHfit: Mobile Health Data for Predicting Athletics Fitness Using Machine Learning.](http://arxiv.org/abs/2304.04839) | 本文提出了利用移动健康数据来比较多种机器学习算法，预测人体健康行为和健身情况的方法；结果表明，XGBoost算法表现优于其他算法。 |
| [^82] | [Ordinal Motifs in Lattices.](http://arxiv.org/abs/2304.04827) | 本研究提出了“序模式”作为分析意义的单位，并研究了在格上通过正式背景的全尺度测量来识别这些序子结构的方法，以实现从中等尺寸序数数据集中检索基本含义。 |
| [^83] | [Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning.](http://arxiv.org/abs/2304.04824) | 本论文提出了一个基于梯度的不确定性归因方法来确定导致预测的不确定性的最具问题性的输入区域，从而实现可解释和可操作的贝叶斯深度学习。 |
| [^84] | [Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques.](http://arxiv.org/abs/2304.04819) | 本文从突破安全系统和窃取敏感数据的角度出发，介绍了机器学习、深度学习和迁移学习技术在预测和防止网络犯罪方面的最新进展，提到了最近研究中效果最好的递归和卷积神经网络。 |
| [^85] | [LCDctCNN: Lung Cancer Diagnosis of CT scan Images Using CNN Based Model.](http://arxiv.org/abs/2304.04814) | 本研究提出了一种使用基于CNN的模型进行CT扫描图像的肺癌诊断的方法，经过与其他模型的比较，CNN得到了最优的性能表现。 |
| [^86] | [Scallop: A Language for Neurosymbolic Programming.](http://arxiv.org/abs/2304.04812) | Scallop是一种能够同时利用深度学习和逻辑推理优点的神经符号编程语言，它能够以数据和计算有效的方式训练神经符号应用程序，通过它可在AI任务中表达算法推理并融合逻辑领域知识，其解决方案可与最先进的模型相媲美或更高。 |
| [^87] | [Deep-learning based measurement of planetary radial velocities in the presence of stellar variability.](http://arxiv.org/abs/2304.04807) | 本文提出了一种基于深度学习的方法，使用神经网络来减少三年HARPS-N太阳-星形光谱中的恒星径向速度抖动。该方法能够以前无法想象的小行星径向速度检测精度，为缓解恒星径向速度变异提供了希望。 |
| [^88] | [RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments.](http://arxiv.org/abs/2304.04797) | 提出了在动态公共云环境中实现快速在线策略学习的框架RAPID，通过领域知识启发的技术实现样本效率和偏差减少，学习并实时调整资源分配策略，能有效解决资源共享中的问题。 |
| [^89] | [Revisiting Test Time Adaptation under Online Evaluation.](http://arxiv.org/abs/2304.04795) | 本文提出了一种新颖的在线评估协议，该协议通过为较慢的方法提供更少的样本来惩罚它们，以更加现实的方式评估了测试时间适应（TTA）方法。广泛实验表明，当考虑推断速度时，简单快速的方法可以优于更复杂但较慢的方法。 |
| [^90] | [Criticality versus uniformity in deep neural networks.](http://arxiv.org/abs/2304.04784) | 本文研究了深度神经网络在混沌边缘初始化时的训练能力，发现饱和的激活函数会妨碍训练效率。结果表明，沿混沌边缘初始化只是获得最佳可训练性所必需但不充分的条件。 |
| [^91] | [Reinforcement Learning from Passive Data via Latent Intentions.](http://arxiv.org/abs/2304.04782) | 本文提出了一种基于意图建模的强化学习方法，可以从被动数据中学习特征，并用于下游任务的价值预测。 |
| [^92] | [An autoencoder compression approach for accelerating large-scale inverse problems.](http://arxiv.org/abs/2304.04781) | 通过自编码器压缩策略来加速计算PDE约束的逆问题，降低了内存瓶颈的影响。 |
| [^93] | [A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?.](http://arxiv.org/abs/2304.04780) | 本篇综述系统分析了可解释人工智能（XAI）在医疗保健领域的应用及其流行趋势，阐述了XAI的使用原因、如何使用以及何时使用及其影响，并给出了如何获得可信赖的AI的方法，对研究人员、临床医生和政策制定者有重要的指导作用。 |
| [^94] | [GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner.](http://arxiv.org/abs/2304.04779) | GraphMAE2是一种掩码自监督学习框架，采用多视角随机重新屏蔽解码和潜在表示预测策略对图SSL中的特征重构进行正则化。 |
| [^95] | [First-order methods for Stochastic Variational Inequality problems with Function Constraints.](http://arxiv.org/abs/2304.04778) | 本文提出了一种新的一阶方法，适用于具有随机算子和/或随机约束的带函数约束的变分不等式问题，当FCVI问题是确定性非光滑的或随机的时，这些方法可以实现最优算子或样本复杂性。 |
| [^96] | [Connecting Fairness in Machine Learning with Public Health Equity.](http://arxiv.org/abs/2304.04761) | 这篇论文总结了机器学习公平方面的划时代文献，并提出了一个框架，用于识别和缓解数据和模型中的偏差，强调在公共卫生领域需要公平和公正的机器学习模型。 |
| [^97] | [Similarity search in the blink of an eye with compressed indices.](http://arxiv.org/abs/2304.04759) | 本文提出一种新的向量压缩方法局部自适应量化(LVQ)，并在基于图的索引的关键优化下实现减少有效带宽同时启用随机访问友好的快速相似性计算，从而在性能和内存占用方面创造了新的最佳表现。 |
| [^98] | [A new perspective on building efficient and expressive 3D equivariant graph neural networks.](http://arxiv.org/abs/2304.04757) | 本文提出了一个本地等同性的三维等变图神经网络层次结构来评估等变GNN的表达能力，并提出局部亚结构编码（LSE）和帧转换编码（FTE）两个关键模块。LEFTNet有效地应用了这些模块并在分子属性预测任务中实现了最先进的性能。 |
| [^99] | [A Novel Two-level Causal Inference Framework for On-road Vehicle Quality Issues Diagnosis.](http://arxiv.org/abs/2304.04755) | 该论文提出了一种新的两级因果推断框架，利用因果机器学习可以加速汽车行业中处理车辆质量问题的全周期，从而更快地隔离根本原因、确定治疗措施并评估其有效性。 |
| [^100] | [Por\'ownanie metod detekcji zaj\k{e}to\'sci widma radiowego z wykorzystaniem uczenia federacyjnego z oraz bez w\k{e}z{\l}a centralnego.](http://arxiv.org/abs/2304.04754) | 本文比较了两种使用联邦机器学习的系统设计方法：带有中央节点和不带中央节点。 |
| [^101] | [$\textit{e-Uber}$: A Crowdsourcing Platform for Electric Vehicle-based Ride- and Energy-sharing.](http://arxiv.org/abs/2304.04753) | 本论文提出了一个名为e-Uber的众包平台，该平台利用电动汽车的日益普及，通过V2G和BST共同实现拼车和能量共享。平台利用强化学习，基于CMAB算法实现个性化任务推荐系统(CARS)，并利用反向拍卖机制选择最优出价。 |
| [^102] | [A Practitioner's Guide to Bayesian Inference in Pharmacometrics using Pumas.](http://arxiv.org/abs/2304.04752) | 本文为使用Pumas工作流程的药代动力学的贝叶斯实践者提供了全面的教程，介绍了标准贝叶斯工作流程的所有步骤，以及许多重要的想法和预防措施。 |
| [^103] | [Probably Approximately Correct Federated Learning.](http://arxiv.org/abs/2304.04641) | 本文提出了FedPAC框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。 |
| [^104] | [Neural Diffeomorphic Non-uniform B-spline Flows.](http://arxiv.org/abs/2304.04555) | 提出了一种具有高效参数化和解析逆变换的至少二次连续可微、双Lipschitz连续的非均匀B样条流形，能够在密度估计和图像生成任务中表现出具有竞争力的结果。 |
| [^105] | [Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis.](http://arxiv.org/abs/2304.04468) | 提出了一种通用的COhort Representation lEarning（CORE）框架，用于增强EHR表示学习，支持针对不同队列的特征进行可解释性分析。 |
| [^106] | [On Robustness in Multimodal Learning.](http://arxiv.org/abs/2304.04385) | 本文提出了一个多模态鲁棒性框架，分析了多种多模态学习方法的鲁棒性不足，并提出了两种干预技术，在多个数据集上实现了1.5倍至4倍的鲁棒性提高，同时在AudioSet 20K上实现了44.2 mAP的有竞争力结果。 |
| [^107] | [NeRF applied to satellite imagery for surface reconstruction.](http://arxiv.org/abs/2304.04133) | 本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。 |
| [^108] | [TC-VAE: Uncovering Out-of-Distribution Data Generative Factors.](http://arxiv.org/abs/2304.04103) | 本文提出了一种基于总相关性的生成模型TC-VAE，可以揭示数据生成因素中的未知分布数据，在处理具有不平衡生成因素的数据集上表现优秀。 |
| [^109] | [StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables.](http://arxiv.org/abs/2304.03853) | StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。 |
| [^110] | [Graph Collaborative Signals Denoising and Augmentation for Recommendation.](http://arxiv.org/abs/2304.03344) | 本文提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，并通过预训练和top-K采样增强了用户-项目交互矩阵，以更好地适应所有用户的需求。 |
| [^111] | [Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification.](http://arxiv.org/abs/2304.02836) | 本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。 |
| [^112] | [Approach Intelligent Writing Assistants Usability with Seven Stages of Action.](http://arxiv.org/abs/2304.02822) | 本文提出采用诺曼的七个动作阶段作为智能写作助手交互设计的框架，并提供支持这些动作阶段的工具的示例。该框架有潜力成为人-LLM交互研究的重要工具。 |
| [^113] | [AutoRL Hyperparameter Landscapes.](http://arxiv.org/abs/2304.02396) | 本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。 |
| [^114] | [EGC: Image Generation and Classification via a Single Energy-Based Model.](http://arxiv.org/abs/2304.02012) | EGC是一种使用单个神经网络在图像分类和图像生成任务中实现卓越性能的方法，可以较好地生成出高质量图像，并在多项数据集上实现了领先的分类结果。 |
| [^115] | [Dynamic Accommodation Measurement using Purkinje Images and ML Algorithms.](http://arxiv.org/abs/2304.01296) | 本研究开发出一种基于普尔金氏图像和机器学习算法的原型设备，可用于动态注视和调节测量，预测调节可以精确到0.25D，正在使用机器学习生成大量合成数据集。 |
| [^116] | [HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices.](http://arxiv.org/abs/2303.17218) | 本研究提出了一种面向FPGA设备的基于延迟的3D-CNN加速器工具链HARFLOW3D，它以机器学习模型和FPGA的特性描述为输入，生成最小化计算延迟的设计。实验证明HARFLOW3D相比其他方案能够实现更低的延迟。 |
| [^117] | [Learning Flow Functions from Data with Applications to Nonlinear Oscillators.](http://arxiv.org/abs/2303.16656) | 本文提出一种基于循环神经网络的架构，通过数据学习了一个因果、时不变且连续时间控制系统的流函数，并证明了该架构能够近似表示流函数，在实验中应用于Van der Pol和FitzHugh Nagumo振荡器的轨迹重现和最优控制输入计算。 |
| [^118] | [Non-Asymptotic Lower Bounds For Training Data Reconstruction.](http://arxiv.org/abs/2303.16372) | 本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析 |
| [^119] | [Asynchronous Online Federated Learning with Reduced Communication Requirements.](http://arxiv.org/abs/2303.15226) | 提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，能够处理异构和延迟设备，实现参与学习任务的可访问性和效率。 |
| [^120] | [How many dimensions are required to find an adversarial example?.](http://arxiv.org/abs/2303.14173) | 本文研究了对抗性漏洞如何取决于受限于高维输入空间中的子空间维数，同时针对标准PGD攻击的对抗性成功率提出了单调递增函数表达式。 |
| [^121] | [ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions.](http://arxiv.org/abs/2303.12364) | ExBEHRT是一种扩展Transformer模型，应用于电子病历数据，将多种类型的记录包括在特征空间中，可以预测不同疾病下游任务的性能更好，并使用预期梯度对结果进行更细粒度的解释。 |
| [^122] | [Does Synthetic Data Generation of LLMs Help Clinical Text Mining?.](http://arxiv.org/abs/2303.04360) | 研究探讨了利用 ChatGPT 进行临床文本挖掘的潜力，但初步结果表明效果欠佳，同时上传患者信息也引发隐私问题。因此，提出了一种新的训练方法，即使用 ChatGPT 生成带标签的大量高质量合成数据进行训练。 |
| [^123] | [Convolutional Neural Networks as 2-D systems.](http://arxiv.org/abs/2303.03042) | 本文介绍了一种将卷积神经网络表示为二维动力系统的方法，将其视为Lur'e系统的二维版本，从而有效地使用鲁棒控制理论进行鲁兹常数估计。 |
| [^124] | [Neural Laplace Control for Continuous-time Delayed Systems.](http://arxiv.org/abs/2302.12604) | 本文介绍了一种神经拉普拉斯控制方法，用于解决具有不规则状态观测和未知延迟的连续时间环境下的离线强化学习问题。 |
| [^125] | [Revised Conditional t-SNE: Looking Beyond the Nearest Neighbors.](http://arxiv.org/abs/2302.03493) | 修正的条件t-SNE算法通过对高维相似度进行条件限制，解决了在数据经过标签良好聚类时，条件t-SNE算法的不足。并通过对相似度矩阵存储的改变提高了可扩展性。 |
| [^126] | [On a continuous time model of gradient descent dynamics and instability in deep learning.](http://arxiv.org/abs/2302.01952) | 本文提出了一个连续时间流模型——主要流（PF），通过对Hessian矩阵特征分解的依赖，捕捉到了梯度下降中的发散和振荡行为、逃逸局部极小值和鞍点的连续性流，并解释了深度学习中的稳定边缘现象。通过对不稳定性的新理解，提出了一种学习率适应方法，可以控制训练稳定性和测试集评估性能之间的权衡。 |
| [^127] | [Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics.](http://arxiv.org/abs/2301.13306) | 本文提出了一个基于梯度的学习算法，可以在多种拍卖方式下满足预算和ROI约束，并达到个体后悔逐渐减小；结果表明，当各自竞争时，期望资金流动至少达到最优分配的期望流动的一半。 |
| [^128] | [Language-Driven Anchors for Zero-Shot Adversarial Robustness.](http://arxiv.org/abs/2301.13096) | 本文提出了一种基于语言驱动、基于锚点的对抗训练策略LAAT，通过利用文本编码器的语义一致性，在零样本图像分类场景下增强图像模型的对抗鲁棒性。实验结果表明，该方法在零样本对抗性能上优于先前的最佳状态对抗性一次性方法，同时能为流行的图像分类模型带来实质性的零样本对抗性能提升。 |
| [^129] | [Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging.](http://arxiv.org/abs/2301.10520) | 该论文提出了一种物理增强的隐式神经表示（INR）用于超声成像，该表示可以从重叠的超声扫描中学习组织属性。利用基于光线追踪的神经渲染进行新视图的超声合成，实现几何精确的B模式图像的生成。 |
| [^130] | [Async-HFL: Efficient and Robust Asynchronous Federated Learning in Hierarchical IoT Networks.](http://arxiv.org/abs/2301.06646) | 提出了异步HFL框架，以解决复杂、层次化的IoT网络中的联合学习挑战。该算法采用了异步聚合来避免长时间等待，并在网关和云级别上采用设备选择和设备网关调度来提高收敛速度、鲁棒性和可扩展性。 |
| [^131] | [Who Should Predict? Exact Algorithms For Learning to Defer to Humans.](http://arxiv.org/abs/2301.06197) | 本论文提出了一种人工智能分类器推迟预测并交给人类决策者作出决策的算法，在可实现的设置下，获得低误差的线性组合是NP难问题，作者提出了一种混合整数线性规划公式来解决问题，同时也提出了一种在实践中表现良好的替代损失函数。 |
| [^132] | [Model Parameter Identification via a Hyperparameter Optimization Scheme for Autonomous Racing Systems.](http://arxiv.org/abs/2301.01470) | 本文提出了基于超参优化方案的模型参数识别方法（MIHO），并实现了AV-21全尺寸自主赛车的模型参数识别。MIHO收敛速度比传统方法快13倍以上，参数模型具有良好适应性和泛化能力，车辆在场地测试中达到了217公里/小时的高速行驶和稳定避障性能。 |
| [^133] | [Impossibility Theorems for Feature Attribution.](http://arxiv.org/abs/2212.11870) | 本文阐述了对于中等规模的模型类，任何完整的线性特征归因方法都无法胜任推断模型行为的改进，启示我们在具体定义最终任务的重要性方面要汲取经验，采用重复模型评估的简单直接方法可以胜过许多其他复杂的特征归因方法。 |
| [^134] | [Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning.](http://arxiv.org/abs/2211.16078) | 本文提出了一种用于离线强化学习的多源数据行为估计方法，通过潜变量模型推断策略来克服数据异构性导致的行为错误。 |
| [^135] | [Ensemble Multi-Quantiles: Adaptively Flexible Distribution Prediction for Uncertainty Quantification.](http://arxiv.org/abs/2211.14545) | 本文提出了一种自适应灵活的分布预测方法EMQ，用于量化机器学习中的不确定性。该方法逐步偏离高斯分布并在提升中发现最优条件分布，因此具有较好的实用性。 |
| [^136] | [Are we certain it's anomalous?.](http://arxiv.org/abs/2211.09224) | 本论文提出了一种新的异常检测方法HypAD，它采用超几何不确定性来评估异常，利用无监督学习来重新建立输入信号，这种方法可以有效解决时间序列中的异常检测问题。 |
| [^137] | [Toward Adaptive Semantic Communications: Efficient Data Transmission via Online Learned Nonlinear Transform Source-Channel Coding.](http://arxiv.org/abs/2211.04339) | 本论文的主要贡献是提出了一种基于深度学习模型的在线学习联合源和信道编码方法，通过利用模型的过度拟合能力，提高模型的适应性，从而在实际应用中实现更高效的数据传输。 |
| [^138] | [Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects.](http://arxiv.org/abs/2211.02247) | 本论文提出了一种对比学习方法，用于实现音乐混音风格转换。该方法采用已处理的数据集进行自监督训练，并使用编码器从参考歌曲中提取仅与音频效果相关的信息。系统实现了多轨音频的混音风格转换，并且在使用音乐源分离模型时具有鲁棒性。 |
| [^139] | [Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks.](http://arxiv.org/abs/2210.15629) | 本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。 |
| [^140] | [Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise.](http://arxiv.org/abs/2210.11075) | 本文提出了一种称为冻结再变换(FTT)的算法，用于在存在虚假相关和特征噪声下实现可证明的表示学习。该算法首先冻结特征学习器，然后在其上训练分类器，利用学习到的核心特征，经过实验证明其有效性。 |
| [^141] | [The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima.](http://arxiv.org/abs/2210.01513) | 本文研究了一种梯度优化方法SAM，发现在凸二次目标中它会在最小值两侧来回振荡，但在非二次情况中从光谱范数的角度执行梯度下降，更新方式被认为是Hessian矩阵在领先特征向量方向上的导数，鼓励漂移向更宽的极小值。 |
| [^142] | [Federated Training of Dual Encoding Models on Small Non-IID Client Datasets.](http://arxiv.org/abs/2210.00092) | 本论文研究了在小型非独立同分布客户数据上的双重编码模型的联邦训练问题，提出了一种叫做DCCO的方法，通过联邦平均和交叉相关损失来训练双重编码模型，在两个机器学习任务中证明了该方法有效性的提高。 |
| [^143] | [Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability.](http://arxiv.org/abs/2209.15594) | 传统梯度下降分析不适用于现代神经网络用全批次或大批次梯度下降训练，Cohen等人(2021)发现的梯度下降边缘稳定性现象表明，当锐度达到不稳定性截止值$2/\eta$时，迭代具有自稳定性，并表现出隐式偏向稳定边缘解的偏差，这种现象通过捕获二阶和三阶导数的比例系数得到了解释。 |
| [^144] | [TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method.](http://arxiv.org/abs/2209.13791) | TRBoost 是一种新型通用梯度提升机，使用约束二次模型来近似目标并应用信赖域算法来获得新的学习器，具有适用于任意损失函数的通用性和竞争性能。 |
| [^145] | [Reduction from Complementary-Label Learning to Probability Estimates.](http://arxiv.org/abs/2209.09500) | 本论文提出了一种新的对互补标签学习的简化方法，它减少到互补类的概率估计，通过一个简单的解码步骤，准确的互补标签概率估计可以产生良好的分类器。 |
| [^146] | [Bi-level Physics-Informed Neural Networks for PDE Constrained Optimization using Broyden's Hypergradients.](http://arxiv.org/abs/2209.07075) | 本文提出了一种解决具有复杂或非线性依赖关系的PDE约束优化问题的新颖双层优化框架，并用Broyden's方法逼近超梯度，取得了最先进的结果。 |
| [^147] | [Oracle-free Reinforcement Learning in Mean-Field Games along a Single Sample Path.](http://arxiv.org/abs/2208.11639) | 本文中提出了一种基于单一智能体样本路径的“沙箱学习”算法，可以在多智能体非合作的环境下作为预热开始。算法不需要均场oracle，样本复杂度为$\tilde{\mathcal{O}}(\epsilon^{-4})$。 |
| [^148] | [SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge.](http://arxiv.org/abs/2208.11266) | 本文提出了一个更加实用的在线自监督终身学习的问题设置，该设置具有挑战性，因为它涉及到数据的非独立同分布和单次遍历、缺乏外部监督和先验知识等。为了解决这些问题，我们提出了SCALE方法，它可以纯粹地从数据连续体中提取和记忆表示。 |
| [^149] | [LogLG: Weakly Supervised Log Anomaly Detection via Log-Event Graph Construction.](http://arxiv.org/abs/2208.10833) | 本文提出了一种LogLG弱监督日志异常检测框架，采用事件图构建和伪标签生成，旨在探索序列中关键字之间的语义联系，有效识别应用程序日志的异常行为。 |
| [^150] | [DIET: Conditional independence testing with marginal dependence measures of residual information.](http://arxiv.org/abs/2208.08579) | 本文提出了一种名为解耦的独立性检验（DIET）算法，通过利用边际独立统计量测试条件独立关系，避免了需要大量预测模型的拟合和相互作用的启发式方法所导致的损失功率问题。 |
| [^151] | [MENLI: Robust Evaluation Metrics from Natural Language Inference.](http://arxiv.org/abs/2208.07316) | 本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。 |
| [^152] | [BeCAPTCHA-Type: Biometric Keystroke Data Generation for Improved Bot Detection.](http://arxiv.org/abs/2207.13394) | 本研究提出了一种基于生物识别击键数据生成的方法，通过合成击键生物识别数据来改进基于击键的机器人检测系统的训练过程，实验证明了合成样本的真实性并表明在大量标记数据场景中，这种方法表现出色。 |
| [^153] | [Delayed Feedback in Generalised Linear Bandits Revisited.](http://arxiv.org/abs/2207.10786) | 研究了广义线性赌博机中的延迟奖励现象，提出了一种自然的乐观算法，可实现一个独立于时间的惩罚函数，降低了现有工作中随着时间增长而增加的惩罚函数的界限。 |
| [^154] | [Competence-based Multimodal Curriculum Learning for Medical Report Generation.](http://arxiv.org/abs/2206.14579) | 本论文提出了一个基于能力的多模态课程学习框架（CMCL），通过模拟放射学家的学习过程来逐步优化医学报告生成模型，在公共数据集上实验表明CMCL可以有效地提高模型性能。 |
| [^155] | [A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games.](http://arxiv.org/abs/2206.05825) | 本文展示了磁镜面下降算法作为均衡求解器和强化学习方法的优点，包括实现了线性收敛的相应均衡求解器和在表格式设置中实现了与CFR相竞争的标准强化学习算法，以及在“黑暗六角”和“幻象井字”中的自我玩耍深度强化学习算法的良好表现。 |
| [^156] | [Neural Collapse: A Review on Modelling Principles and Generalization.](http://arxiv.org/abs/2206.04041) | 神经崩溃是当深度分类神经网络训练误差降至零时出现的一种状态，它简化了最后一层的行为，具有最近类中心决策规则。本文综述了神经崩溃的建模原则，以及它对神经网络的泛化和迁移学习能力的影响。 |
| [^157] | [Accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient.](http://arxiv.org/abs/2206.01209) | 本文开发了针对具有局部Lipschitz连续梯度的凸优化问题的加速一阶方法，分别提出了无约束凸优化的加速近端梯度算法(APG)以及约束凸优化的一阶近端增广拉格朗日方法。通过这些方法，可以快速地寻找出解决方案。 |
| [^158] | [DPSNN: A Differentially Private Spiking Neural Network with Temporal Enhanced Pooling.](http://arxiv.org/abs/2205.12718) | 本文提出了一种差分隐私脉冲神经网络（DPSNN），将差分隐私算法与脉冲神经网络相结合，保持了SNN的强隐私保护；同时提出了时序增强池化（TEP）方法，使SNN能够获得更好的信息传输。在MNIST和CIFAR-10数据集上进行实验验证了该方法的有效性。 |
| [^159] | [Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation.](http://arxiv.org/abs/2205.10868) | 本论文提出了一种基于价值知识整合的高效内存强化学习算法，通过从目标 Q 网络向当前 Q 网络整合知识，减少遗忘并保持高样本效率。相较于传统方法，本算法在特征和图像任务中表现相当或更好，同时减轻了大经验重放缓存器带来的内存负担。 |
| [^160] | [Fairness in Graph Mining: A Survey.](http://arxiv.org/abs/2204.09888) | 本文综述了近年来图挖掘算法在公平性方面的研究进展。在人类中心应用中，大多数算法缺乏公平性考虑，可能对某些人群造成歧视。与独立同分布的数据不同，图挖掘中的公平性具有独特的分类和实现技术。本文提出了一个新的图公平概念分类法，并总结了现有技术以促进图挖掘的公平性。 |
| [^161] | [Neural Message Passing for Objective-Based Uncertainty Quantification and Optimal Experimental Design.](http://arxiv.org/abs/2203.07120) | 本论文提出了一种降低计算成本实现客观目标不确定性量化和最优实验设计的新方案。 |
| [^162] | [Human-Level Control through Directly-Trained Deep Spiking Q-Networks.](http://arxiv.org/abs/2201.07211) | 本文提出了一种直接训练的深度脉冲Q网络架构，解决了基于SNN的深度脉冲强化学习中输出二进制和不可微性问题，并在Atari游戏上实现了人类水平的控制能力。 |
| [^163] | [Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures.](http://arxiv.org/abs/2110.13179) | 该论文提出了一种新的深度泊松混合网络方法（DPMN），能够在存在可靠的层级信息时进行准确和一致的概率时间序列预测。该模型可以保证层级一致性，并在大规模的层级预测问题中获得了最先进的结果。 |
| [^164] | [On Geometric Connections of Embedded and Quotient Geometries in Riemannian Fixed-rank Matrix Optimization.](http://arxiv.org/abs/2110.12121) | 本文提出了一种建立黎曼优化问题的嵌入和商几何的几何景观联系的通用程序，应用于固定秩矩阵优化可以在黎曼FOSP集、SOSP集和严格鞍点集上建立等同。 |
| [^165] | [Entropy Regularized Reinforcement Learning Using Large Deviation Theory.](http://arxiv.org/abs/2106.03931) | 本文利用大偏差理论建立了熵正则化强化学习与非平衡统计力学的联系，在长时间极限下推导出了马尔可夫决策过程模型中最优策略和最优动态的精确解析结果，并提出了新的分析和计算框架。 |
| [^166] | [An Offline Risk-aware Policy Selection Method for Bayesian Markov Decision Processes.](http://arxiv.org/abs/2105.13431) | 本文提出了一种基于风险意识的离线策略选择方法，该方法采用“利用”与“谨慎”(EvC)的范式，选取权重值最大化且同时考虑长期奖励和风险的策略。 |
| [^167] | [Monotonic Alpha-divergence Minimisation for Variational Inference.](http://arxiv.org/abs/2103.05684) | 本文提出了一种用于变分推断的单调α散度最小化算法，可优化混合模型的权重和分量参数，获得了在多峰目标分布和实际数据应用上的改进效果。 |
| [^168] | [On Feature Relevance Uncertainty: A Monte Carlo Dropout Sampling Approach.](http://arxiv.org/abs/2008.01468) | 本文提出了一种名为MCRP的方法，采用蒙特卡罗估计计算特征相关性分布，以评估特征相关性的不确定性，以更好地理解神经网络感知和推理。 |
| [^169] | [FLUID: A Unified Evaluation Framework for Flexible Sequential Data.](http://arxiv.org/abs/2007.02519) | FLUID是一个统一评估框架，可以处理灵活序列数据，并结合了少样本、持续、转移和表示学习的特点，支持在线评估和超出分布评估。它有助于发展通用的机器学习方法。 |
| [^170] | [Convolutional Neural Networks for Epileptic Seizure Prediction.](http://arxiv.org/abs/1811.00915) | 本文提出了一种使用卷积神经网络拓扑结构进行iEEG的癫痫发作预测的新方法，避免提取手工特征，并在多个数据集上进行了评估。研究结果表明该方法具有普遍适用性。 |

# 详细

[^1]: 那不是你的记忆，它是别人的：在聊天机器人记忆中播撒错误信息

    Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories. (arXiv:2304.05371v1 [cs.CL])

    [http://arxiv.org/abs/2304.05371](http://arxiv.org/abs/2304.05371)

    本文研究了聊天机器人的一个新发展：长期记忆机制。然而，我们发现这种机制可能会导致机器人记住了错误和虚假的信息，并将其作为事实陈述重复。

    

    聊天机器人的一个新发展是长期记忆机制，可以记住过去对话中的信息，以增加响应的连贯性和一致性。机器人被设计为从其对话伙伴中提取个人性质的知识，例如表明对特定颜色的偏好。在本文中，我们展示了这种记忆机制可能会导致意外行为。具体而言，我们发现一个人可以将个人陈述与信息陈述结合起来，导致机器人将信息陈述与个人知识一起记录在其长期记忆中。这意味着机器人可能被欺骗记住错误信息，并在回忆与对话主题相关的信息时将其作为事实陈述重复。我们在基于ParlAI平台实现的BlenderBot 2框架上展示了这种漏洞，并在更近期、规模更大的BlenderBot 3模型上提供了例子。

    One of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing engagement and consistency of responses. The bot is designed to extract knowledge of personal nature from their conversation partner, e.g., stating preference for a particular color. In this paper, we show that this memory mechanism can result in unintended behavior. In particular, we found that one can combine a personal statement with an informative statement that would lead the bot to remember the informative statement alongside personal knowledge in its long term memory. This means that the bot can be tricked into remembering misinformation which it would regurgitate as statements of fact when recalling information relevant to the topic of conversation. We demonstrate this vulnerability on the BlenderBot 2 framework implemented on the ParlAI platform and provide examples on the more recent and significantly larger BlenderBot 3 model. We gen
    
[^2]: 一种意外简单的技术来控制预训练偏差以实现更好的迁移：扩展或缩小你的表示

    A surprisingly simple technique to control the pretraining bias for better transfer: Expand or Narrow your representation. (arXiv:2304.05369v1 [cs.LG])

    [http://arxiv.org/abs/2304.05369](http://arxiv.org/abs/2304.05369)

    本研究提出了一种简单的方法来控制预训练偏差，即通过扩展或缩小骨干网络的表示维度，从而显著提高了下游迁移性能。

    

    自监督学习（SSL）模型依赖于预文本任务来学习表示。由于这个任务与用于评估这些模型性能的下游任务不同，因此存在固有的不对齐或预训练偏差。在SSL中常用的一个技巧是在训练期间在骨干网络之上添加一个小的投影机（通常是一个2或3层的多层感知器），以使深度网络更具抗偏差性。与先前研究投影机架构的影响不同，我们在这里专注于一个更简单但被忽视的方法来控制骨干表示中的信息。我们展示了仅通过改变骨干网络的最后一个块的大小来改变其维数是一种极其有效的技术，可以减轻预训练偏差。它显着改善了自监督和监督预训练模型的下游传输性能。

    Self-Supervised Learning (SSL) models rely on a pretext task to learn representations. Because this pretext task differs from the downstream tasks used to evaluate the performance of these models, there is an inherent misalignment or pretraining bias. A commonly used trick in SSL, shown to make deep networks more robust to such bias, is the addition of a small projector (usually a 2 or 3 layer multi-layer perceptron) on top of a backbone network during training. In contrast to previous work that studied the impact of the projector architecture, we here focus on a simpler, yet overlooked lever to control the information in the backbone representation. We show that merely changing its dimensionality -- by changing only the size of the backbone's very last block -- is a remarkably effective technique to mitigate the pretraining bias. It significantly improves downstream transfer performance for both Self-Supervised and Supervised pretrained models.
    
[^3]: 《无免费午餐定理、科尔莫戈洛夫复杂性及归纳偏差在机器学习中的作用》

    The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning. (arXiv:2304.05366v1 [cs.LG])

    [http://arxiv.org/abs/2304.05366](http://arxiv.org/abs/2304.05366)

    本论文阐述了无免费午餐定理的监督学习中的限制，证明了归纳偏差可以提高学习算法的效果，并且展示了神经网络模型的偏好与现实世界的数据分布相关。

    

    监督学习的无免费午餐定理指出，没有一个学习算法可以解决所有问题，或者所有学习算法在均匀分布的学习问题上平均精度达到完全相同。因此，这些定理经常被引用来支持个别问题需要特别定制的归纳偏差的概念。我们认为，尽管几乎所有均匀采样的数据集具有高复杂性，但现实世界中的问题不成比例地产生低复杂度的数据，并且我们认为神经网络模型也具有同样的偏好，这种偏好使用科尔莫戈洛夫复杂度进行了形式化。值得注意的是，我们展示了为特定领域设计的体系结构，例如计算机视觉，可以压缩各种看似不相关的领域的数据集。我们的实验表明，预先训练和即使是随机初始化的语言模型都更喜欢生成低复杂度的序列。尽管无免费午餐定理似乎表明各个问题需要专门的学习算法，但我们解释说，学习算法通常可以通过编码关于真实世界数据分布的先前知识的归纳偏差来改进。

    No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we exp
    
[^4]: 我们实现了个性化治疗吗？使用重复采样的在线强化学习算法进行个性化评估

    Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])

    [http://arxiv.org/abs/2304.05365](http://arxiv.org/abs/2304.05365)

    本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。

    

    在数字健康中，使用强化学习（RL）个性化治疗序列以支持用户采取更健康的行为越来越受到关注。这种连续决策问题涉及到基于用户的上下文（例如，先前的活动水平、位置等）在何时治疗以及如何治疗的决定。在线RL算法是这个问题的一个有前途的数据驱动方法，因为它基于每个用户的历史反馈进行学习，并利用这些知识个性化这些决策。然而，要决定是否应在实际部署的“优化”干预中包含RL算法，我们必须评估数据证据，表明RL算法实际上正在将治疗个性化适应其用户。由于RL算法中的随机性，人们可能会对其在某些状态下的学习并使用此学习来提供特定治疗的能力产生误解。我们使用工作定义的个性化，并介绍了一种重复采样政策评估方法来评估在线RL算法实现的个性化水平。我们使用模拟评估了我们提出的方法，并展示了我们的方法可以准确地识别个性化的策略。我们提出的方法在优化数字健康的个性化干预方面具有潜在应用。

    There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
    
[^5]: 约束域的扩散模型

    Diffusion Models for Constrained Domains. (arXiv:2304.05364v1 [cs.LG])

    [http://arxiv.org/abs/2304.05364](http://arxiv.org/abs/2304.05364)

    本研究提出了两种方法来创建约束域的降噪扩散模型。第一种方法基于不等式约束诱导的对数障碍度量，第二种方法基于反射布朗运动。这些方法将扩散模型的应用范围扩展到了机器人和蛋白设计等领域。

    

    降噪扩散模型是新近涌现的一种生成模型，它在无条件图像生成和语音生成等众多领域实现了最先进的成果。它们由破坏数据的加噪过程和定义为加噪扩散的时间反演的后向阶段组成。以这些成功为基础，扩散模型最近扩展到了黎曼流形设置。然而，这些黎曼扩散模型要求在所有时间上定义测地线。虽然该设置包括许多重要应用，但不包括由不等式约束集定义的流形，这在许多科学领域，如机器人和蛋白设计中是普遍存在的。在本文中，我们介绍了两种方法来弥合这个差距。首先，我们设计了一个基于不等式约束诱导的对数障碍度量的加噪过程。其次，我们介绍了一种基于反射布朗运动的加噪过程。由于现有的扩散模型不能直接应用于约束域，因此本文提出了两种方法来创建约束域的降噪扩散模型。

    Denoising diffusion models are a recent class of generative models which achieve state-of-the-art results in many domains such as unconditional image generation and text-to-speech tasks. They consist of a noising process destroying the data and a backward stage defined as the time-reversal of the noising diffusion. Building on their success, diffusion models have recently been extended to the Riemannian manifold setting. Yet, these Riemannian diffusion models require geodesics to be defined for all times. While this setting encompasses many important applications, it does not include manifolds defined via a set of inequality constraints, which are ubiquitous in many scientific domains such as robotics and protein design. In this work, we introduce two methods to bridge this gap. First, we design a noising process based on the logarithmic barrier metric induced by the inequality constraints. Second, we introduce a noising process based on the reflected Brownian motion. As existing diffu
    
[^6]: 针对多标签分类的非对称多项式损失

    Asymmetric Polynomial Loss For Multi-Label Classification. (arXiv:2304.05361v1 [cs.LG])

    [http://arxiv.org/abs/2304.05361](http://arxiv.org/abs/2304.05361)

    本文提出了一种有效的非对称多项式损失（APL），可以根据不同任务优化模型，并在关系提取、文本分类和图像分类上表现良好。

    

    多种任务被重新构造为多标签分类问题，其中二元交叉熵（BCE）损失经常用于优化设计良好的模型。然而，纯BCE损失无法根据不同任务进行优化，从而导致不同模型的表现亚优。此外，冗余负样品和罕见正样品之间的不平衡可能会降低模型性能。在本文中，我们提出了一种有效的非对称多项式损失（APL）来缓解上述问题。具体而言，我们首先对BCE损失进行泰勒展开，然后改善多项式函数的系数。我们进一步采用非对称聚焦机制来解耦来自负样本和正样本的梯度贡献。此外，我们验证，多项式系数可以重新校准非对称聚焦超参数。在关系提取、文本分类和图像分类上的实验表明，我们的APL损失可以提供更好的性能。

    Various tasks are reformulated as multi-label classification problems, in which the binary cross-entropy (BCE) loss is frequently utilized for optimizing well-designed models. However, the vanilla BCE loss cannot be tailored for diverse tasks, resulting in a suboptimal performance for different models. Besides, the imbalance between redundant negative samples and rare positive samples could degrade the model performance. In this paper, we propose an effective Asymmetric Polynomial Loss (APL) to mitigate the above issues. Specifically, we first perform Taylor expansion on BCE loss. Then we ameliorate the coefficients of polynomial functions. We further employ the asymmetric focusing mechanism to decouple the gradient contribution from the negative and positive samples. Moreover, we validate that the polynomial coefficients can recalibrate the asymmetric focusing hyperparameters. Experiments on relation extraction, text classification, and image classification show that our APL loss can 
    
[^7]: iDML: 激励去中心化机器学习

    iDML: Incentivized Decentralized Machine Learning. (arXiv:2304.05354v1 [cs.LG])

    [http://arxiv.org/abs/2304.05354](http://arxiv.org/abs/2304.05354)

    通过区块链的激励机制，让用户资源贡献变得可行，实现去中心化的机器学习。

    

    随着去中心化和机会主义的机器学习方法的兴起，终端设备越来越需要使用其自己收集的众包数据训练深度学习模型。这些方法从资源消耗和隐私保护的角度都是可取的。当设备直接从训练模型中受益时，资源贡献是被激励的，因为协作会产生更高准确度的模型。然而，当要求终端用户设备为仅为他人受益的任务（例如为一个无足轻重的邻居训练模型）贡献自己的资源（例如计算、通信和数据）时，需要提供明确的激励机制。在这个项目中，我们提出了一种新的基于区块链的激励机制，用于完全去中心化和鼓励用户资源贡献的机器学习。

    With the rising emergence of decentralized and opportunistic approaches to machine learning, end devices are increasingly tasked with training deep learning models on-devices using crowd-sourced data that they collect themselves. These approaches are desirable from a resource consumption perspective and also from a privacy preservation perspective. When the devices benefit directly from the trained models, the incentives are implicit contributing devices' resources are incentivized by the availability of the higher-accuracy model that results from collaboration. However, explicit incentive mechanisms must be provided when end-user devices are asked to contribute their resources (e.g., computation, communication, and data) to a task performed primarily for the benefit of others, e.g., training a model for a task that a neighbor device needs but the device owner is uninterested in. In this project, we propose a novel blockchain-based incentive mechanism for completely decentralized and
    
[^8]: ChatGPT在多模态股票预测挑战中的零样本分析：华尔街新手？

    The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v1 [cs.CL])

    [http://arxiv.org/abs/2304.05351](http://arxiv.org/abs/2304.05351)

    本研究对ChatGPT在股票预测方面进行了零样本分析，结果表明其预测股票移动的表现不如最先进和传统方法，需要进一步改进。

    

    最近，如ChatGPT这样的大型语言模型在各种自然语言处理任务中展示了惊人的性能。然而，在预测股市走势方面，它们的有效性仍然有待探索。本文通过三个推文和历史股票价格数据集的广泛零样本分析，探讨了ChatGPT在多模态股票移动预测方面的能力。我们的研究表明，ChatGPT是一个“华尔街新手”，在预测股票移动方面的成功有限，不仅不如最先进的方法，而且不如使用价格特征的线性回归这样的传统方法。尽管思维链提示策略和推文的包含具有潜在的优势，ChatGPT的表现仍然不佳。此外，我们观察到它的可解释性和稳定性存在局限性，需要更专业的训练或微调。这项研究提供了有关ChatGPT在股票预测方面的见解。

    Recently, large language models (LLMs) like ChatGPT have demonstrated remarkable performance across a variety of natural language processing tasks. However, their effectiveness in the financial domain, specifically in predicting stock market movements, remains to be explored. In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets. Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features. Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar. Furthermore, we observe limitations in its explainability and stability, suggesting the need for more specialized training or fine-tuning. This research provides insights i
    
[^9]: 基于深度学习的智能手机显微镜图像中贾第虫和隐孢子的检测与计数

    Deep-learning assisted detection and quantification of (oo)cysts of Giardia and Cryptosporidium on smartphone microscopy images. (arXiv:2304.05339v1 [eess.IV])

    [http://arxiv.org/abs/2304.05339](http://arxiv.org/abs/2304.05339)

    本研究采用基于深度学习的RetinaNet模型针对采用智能手机显微系统检测贾第虫和隐孢子进行检测和计数，并在速度和准确度方面表现出最佳效果，为在资源有限的环境下解决这一问题提供了潜在解决方案。

    

    食用受微生物污染的食物和水每年造成数百万人死亡。基于智能手机的显微系统是一种便携、低成本和比传统的亮场显微镜更易接近的方法用于检测贾第虫和隐孢子。然而，智能手机显微镜的图像有很多噪声，需要培训有素的技术人员进行手动囊泡识别，而这通常在资源有限的环境中是不可用的。采用基于深度学习的对象检测自动检测卵/梭状体可能为此限制提供解决方案。我们评估了三种最先进的物体检测器在自定义数据集上检测贾第虫和隐孢子的效果，数据集包括从蔬菜样品中获取的智能手机和亮场显微镜图像。Faster RCNN、RetinaNet和You Only Look Once（YOLOv8s）深度学习模型被用来探索它们的有效性和限制。我们的结果表明，虽然深度学习模型能够准确地检测卵/梭状体，但RetinaNet在速度和准确度方面优于其他两种模型。本研究提供了一种潜在的解决方案，利用基于智能手机的显微系统在资源有限的环境中自动检测和计数贾第虫和隐孢子。

    The consumption of microbial-contaminated food and water is responsible for the deaths of millions of people annually. Smartphone-based microscopy systems are portable, low-cost, and more accessible alternatives for the detection of Giardia and Cryptosporidium than traditional brightfield microscopes. However, the images from smartphone microscopes are noisier and require manual cyst identification by trained technicians, usually unavailable in resource-limited settings. Automatic detection of (oo)cysts using deep-learning-based object detection could offer a solution for this limitation. We evaluate the performance of three state-of-the-art object detectors to detect (oo)cysts of Giardia and Cryptosporidium on a custom dataset that includes both smartphone and brightfield microscopic images from vegetable samples. Faster RCNN, RetinaNet, and you only look once (YOLOv8s) deep-learning models were employed to explore their efficacy and limitations. Our results show that while the deep-l
    
[^10]: 聊天GPT中的毒性：分析个性化语言模型

    Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. (arXiv:2304.05335v1 [cs.CL])

    [http://arxiv.org/abs/2304.05335](http://arxiv.org/abs/2304.05335)

    论文分析了基于对话的大型语言模型ChatGPT中的毒性，通过指定人物角色，输出会涉及刻板印象、有害对话和伤人的言论。因此，我们需要充分理解LLMs的能力和局限性，以确保这些系统的安全性。

    

    大型语言模型（LLMs）展现了令人难以置信的能力，超越了自然语言处理（NLP）社区，并被广泛应用于医疗保健、治疗、教育和客户服务等多种服务中。由于用户包括有重要信息需求的人，如与聊天机器人交互的学生或患者，因此这些系统的安全性至关重要。因此，必须明确了解LLMs的能力和局限性。为此，我们系统地评估了ChatGPT中的毒性，这是一种流行的基于对话的LLM，超过半百万次Generation被测试。我们发现，通过为ChatGPT指定一个人物角色，比如拳击手穆罕默德·阿里，可以显著增加产生的毒性。根据指定给ChatGPT的角色，其毒性可能会增加到6倍，其输出会涉及不正确的刻板印象、有害的对话和伤人的言论。这可能会潜在地损害人物角色的名誉。

    Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and ha
    
[^11]: 基于分层张量草图的生成建模

    Generative Modeling via Hierarchical Tensor Sketching. (arXiv:2304.05305v1 [math.NA])

    [http://arxiv.org/abs/2304.05305](http://arxiv.org/abs/2304.05305)

    本文提出了一种利用分层张量草图来近似高维概率密度的方法，通过随机奇异值分解技术解决线性方程达到此目的，其算法复杂度在高维密度维度上呈线性规模。

    

    我们提出了一种使用经验分布来近似高维概率密度的分层张量网络方法。该方法利用随机奇异值分解（SVD）技术，并涉及在该张量网络中解线性方程以获得张量核心。该算法的复杂性在高维密度的维度上呈线性规模。通过几个数值实验，对估计误差进行了分析，证明了此方法的有效性。

    We propose a hierarchical tensor-network approach for approximating high-dimensional probability density via empirical distribution. This leverages randomized singular value decomposition (SVD) techniques and involves solving linear equations for tensor cores in this tensor network. The complexity of the resulting algorithm scales linearly in the dimension of the high-dimensional density. An analysis of estimation error demonstrates the effectiveness of this method through several numerical experiments.
    
[^12]: TACOS: 面向拓扑结构的分布式训练集合算法合成器

    TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training. (arXiv:2304.05301v1 [cs.DC])

    [http://arxiv.org/abs/2304.05301](http://arxiv.org/abs/2304.05301)

    TACOS 是一个能够自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。与基准算法相比，TACOS 合成的 All-Reduce 算法速度提高了 3.73 倍，为 512-NPU 系统合成集体算法只需 6.1 分钟。

    

    集群之间的集合通讯是分布式训练不可或缺的一部分。运行面向拓扑结构的集合算法对于优化通讯性能以最小化拥塞是至关重要的。目前，此类算法仅适用于一小部分简单拓扑结构，限制了训练集群中采用的拓扑结构并处理由于网络故障而产生的不规则拓扑结构。 本文提出了 TACOS，这是一个可自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。TACOS 合成的 All-Reduce 算法比基线算法快了 3.73 倍，并为 512-NPU 系统合成集体算法仅需 6.1 分钟。

    Collective communications are an indispensable part of distributed training. Running a topology-aware collective algorithm is crucial for optimizing communication performance by minimizing congestion. Today such algorithms only exist for a small set of simple topologies, limiting the topologies employed in training clusters and handling irregular topologies due to network failures. In this paper, we propose TACOS, an automated topology-aware collective synthesizer for arbitrary input network topologies. TACOS synthesized 3.73x faster All-Reduce algorithm over baselines, and synthesized collective algorithms for 512-NPU system in just 6.1 minutes.
    
[^13]: 面向非约束环境的目标检测技术综合研究

    A Comprehensive Study on Object Detection Techniques in Unconstrained Environments. (arXiv:2304.05295v1 [cs.CV])

    [http://arxiv.org/abs/2304.05295](http://arxiv.org/abs/2304.05295)

    本文全面研究了面向非约束环境的目标检测技术，包括挑战、数据集和最先进的方法，并进行了方法的比较分析，提供了未来的研究方向。

    

    目标检测是计算机视觉中的重要任务，旨在识别和定位图像或视频中的对象。深度学习和卷积神经网络（CNN）的最新进展显著提高了对象检测技术的性能。本文对非约束环境下的目标检测技术进行了全面研究，包括各种挑战、数据集和最先进的方法。此外，我们进行了方法的比较分析，并突出了它们的优缺点。最后，我们提供了一些未来的研究方向，以进一步提高非约束环境下的目标检测能力。

    Object detection is a crucial task in computer vision that aims to identify and localize objects in images or videos. The recent advancements in deep learning and Convolutional Neural Networks (CNNs) have significantly improved the performance of object detection techniques. This paper presents a comprehensive study of object detection techniques in unconstrained environments, including various challenges, datasets, and state-of-the-art approaches. Additionally, we present a comparative analysis of the methods and highlight their strengths and weaknesses. Finally, we provide some future research directions to further improve object detection in unconstrained environments.
    
[^14]: 使用多数据因果推断选择机器学习应用的强健特征

    Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])

    [http://arxiv.org/abs/2304.05294](http://arxiv.org/abs/2304.05294)

    本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。

    

    强健的特征选择对于创建可靠和可解释的机器学习（ML）模型至关重要。在领域知识有限、潜在交互未知的情况下设计统计预测模型时，选择最优特征集通常很困难。为了解决这个问题，我们引入了一种多数据（M）因果特征选择方法，它同时处理一组时间序列数据集，并生成一个单一的因果驱动集。该方法使用Tigramite Python包中实现的因果发现算法PC1或PCMCI。这些算法利用条件独立性测试推断因果图的部分。我们的因果特征选择方法在将剩余因果特征作为输入传递给ML模型（多元线性回归，随机森林）预测目标之前，过滤掉因果虚假链接。我们将该框架应用于预测西太平洋热带地区的地震强度。

    Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
    
[^15]: 带电粒子跟踪的等变图神经网络

    Equivariant Graph Neural Networks for Charged Particle Tracking. (arXiv:2304.05293v1 [physics.ins-det])

    [http://arxiv.org/abs/2304.05293](http://arxiv.org/abs/2304.05293)

    本文提出了对称等变GNN模型EuclidNet，适用于带电粒子跟踪，通过旋转对称性处理碰撞事件的图表示，实现更高效的模型。实验结果表明EuclidNet在小模型规模下比非等变基准更有效。

    

    图神经网络（GNN）由于其提高精度和可扩展性的潜力而在高能物理（HEP）中受到关注。然而，它们的资源密集型特性和复杂的操作促使对对称等变的结构进行开发。在本文中，我们介绍了EuclidNet，这是一种新颖的对称等变GNN，适用于带电粒子跟踪。EuclidNet利用碰撞事件的图表示，并强制执行以探测器的束线轴为中心的旋转对称性，从而实现了更高效的模型。我们在TrackML数据集上将EuclidNet与最先进的Interaction Network进行基准测试，该数据集模拟预期在高亮度大型强子对撞机（HL-LHC）中出现的高堆叠条件。我们的结果表明，在小模型规模（<1000个参数）下，EuclidNet实现了接近最先进的性能，优于非等变基准。该研究为将来探索更具资源效率的带电粒子跟踪GNN模型铺平了道路。

    Graph neural networks (GNNs) have gained traction in high-energy physics (HEP) for their potential to improve accuracy and scalability. However, their resource-intensive nature and complex operations have motivated the development of symmetry-equivariant architectures. In this work, we introduce EuclidNet, a novel symmetry-equivariant GNN for charged particle tracking. EuclidNet leverages the graph representation of collision events and enforces rotational symmetry with respect to the detector's beamline axis, leading to a more efficient model. We benchmark EuclidNet against the state-of-the-art Interaction Network on the TrackML dataset, which simulates high-pileup conditions expected at the High-Luminosity Large Hadron Collider (HL-LHC). Our results show that EuclidNet achieves near-state-of-the-art performance at small model scales (<1000 parameters), outperforming the non-equivariant benchmarks. This study paves the way for future investigations into more resource-efficient GNN mod
    
[^16]: MC-ViViT: 多分支分类器-ViViT用于使用面部视频检测老年人轻度认知障碍

    MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])

    [http://arxiv.org/abs/2304.05292](http://arxiv.org/abs/2304.05292)

    本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。

    

    深度机器学习模型包括卷积神经网络(CNN)已成功地应用于使用医学图像、问卷和视频检测轻度认知障碍(MCI)。本文提出了一种新的多分支分类器-视频视觉变换器(MC-ViViT)模型，通过分析面部特征区分MCI和正常认知。数据来自I-CONECT，一个旨在通过提供频繁视频聊天来改善认知功能的行为干预试验。MC-ViViT在一个分支中提取视频的时空特征，并通过MC模块增强表示。由于I-CONECT数据集中的样本不平衡问题（包含难易和正负样本），这使MC-ViViT的性能受到影响。我们提出了一种Hard-Easy和Positive-Negative样本的损失函数（HP Loss）来结合对比度调节损失Focal loss和AD-CORRE loss来解决不平衡问题。我们在I-CONECT数据集上的实验结果显示出该算法的有效性。

    Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
    
[^17]: 任务难度感知的增量学习参数分配与正则化

    Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning. (arXiv:2304.05288v1 [cs.LG])

    [http://arxiv.org/abs/2304.05288](http://arxiv.org/abs/2304.05288)

    本文提出了一种任务难度感知的增量学习参数分配与正则化方法，该方法可以根据任务的学习难度自适应地选择适当的分配或正则化策略，以克服在学习不同任务时的挑战。

    

    参数正则化或分配方法对于克服增量学习中的灾难性遗忘非常有效。然而，它们在序列中均匀解决所有任务，忽略了不同任务的学习难度差异。因此，当学习与已学任务非常不同的新任务时，参数正则化方法会面临显着的遗忘，而参数分配方法在学习简单任务时则面临不必要的参数开销。本文提出了参数分配和正则化策略（PAR），其可以根据学习难度从参数分配和正则化中自适应地选择适当的策略。对于一个已经学习相关任务的模型来说，一个任务会变得容易，反之亦然。本文提出了基于最近原型距离的离散度估计方法，仅使用新任务的特征来度量任务相关性。此外，我们提出了一种时效性相关性感知采样的架构。

    Parameter regularization or allocation methods are effective in overcoming catastrophic forgetting in lifelong learning. However, they solve all tasks in a sequence uniformly and ignore the differences in the learning difficulty of different tasks. So parameter regularization methods face significant forgetting when learning a new task very different from learned tasks, and parameter allocation methods face unnecessary parameter overhead when learning simple tasks. In this paper, we propose the Parameter Allocation & Regularization (PAR), which adaptively select an appropriate strategy for each task from parameter allocation and regularization based on its learning difficulty. A task is easy for a model that has learned tasks related to it and vice versa. We propose a divergence estimation method based on the Nearest-Prototype distance to measure the task relatedness using only features of the new task. Moreover, we propose a time-efficient relatedness-aware sampling-based architecture
    
[^18]: 个性化文本到图像生成的可控文本反转

    Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])

    [http://arxiv.org/abs/2304.05265](http://arxiv.org/abs/2304.05265)

    本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。

    

    最近，大规模生成模型在以文本为引导的高保真图像的生成方面取得了前所未有的性能。当引导信息包含用户定义的、未见过的或长尾概念标记时，文本反转成为一种有效的个性化生成技术。尽管如此，我们发现并展示了文本反转的部署仍充满了“黑魔法”，例如额外数据集的严苛要求，在循环中需要艰苦的人力成本和缺乏鲁棒性等。在这项工作中，我们提出了一种名为可控文本反转的大大增强版反转，解决了所有上述问题，并反过来提供了一个强大，数据效率高，易于使用的框架。COTI的核心是基于理论的损失目标，具有全面和新颖的加权评分机制，并由主动学习范式所提取。广泛的结果表明，COTI的性能比之前技术有了显著的提升，尤其是在数据少的情况下。

    The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
    
[^19]: 控制联邦学习中遗忘问题的重新加权Softmax交叉熵方法

    Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning. (arXiv:2304.05260v1 [cs.LG])

    [http://arxiv.org/abs/2304.05260](http://arxiv.org/abs/2304.05260)

    本文提出一种重新加权softmax的交叉熵方法来解决联邦学习中客户端的灾难性遗忘问题，并证明这种方法可以缓解客户端遗忘并对标准联邦学习算法提供一致的改进。

    

    在联邦学习中，通过聚合在一组独立客户节点计算的模型更新来学习全局模型，在聚合之前在每个节点上执行多个梯度步骤，以减少通信成本。在这种情况下，数据异构性会导致不同的客户端具有不同的本地目标，这可能会导致客户端过度减少其自己的本地目标，使其与全局解分歧。本文提出了一种有效的方法，对每个客户端的交叉熵目标进行修改，通过重新加权softmax的logits以计算损失，从而解决了每个客户端模型对来自其他客户端的数据的灾难性遗忘问题。这种方法可以保护不在客户端标签集中的类别免受突然的表示变化，并且通过实验证明，可以缓解客户端遗忘并对标准联邦学习算法提供一致的改进。 我们的方法特别有益。

    In Federated Learning, a global model is learned by aggregating model updates computed at a set of independent client nodes, to reduce communication costs multiple gradient steps are performed at each node prior to aggregation. A key challenge in this setting is data heterogeneity across clients resulting in differing local objectives which can lead clients to overly minimize their own local objective, diverging from the global solution. We demonstrate that individual client models experience a catastrophic forgetting with respect to data from other clients and propose an efficient approach that modifies the cross-entropy objective on a per-client basis by re-weighting the softmax logits prior to computing the loss. This approach shields classes outside a client's label set from abrupt representation change and we empirically demonstrate it can alleviate client forgetting and provide consistent improvements to standard federated learning algorithms. Our method is particularly beneficia
    
[^20]: 多粒度时间变换器用于知识追踪

    Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])

    [http://arxiv.org/abs/2304.05257](http://arxiv.org/abs/2304.05257)

    本文提出了一种基于Transformer的架构用于准确地预测学生在标准化测试中的表现。该模型考虑了学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，并在解码器输入中使用了多个时间特征粒度以显著提高模型性能。与LightGBM相比，该方法更加准确，为教育领域的AI发展提供了一个可伸缩和准确的预测学生成果的工具。

    

    本文提出了一种基于Transformer的架构，用于预测标准化测试中学生的表现。具体来说，我们利用学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，为每个学生创建一个个性化的模型。然后，我们使用这些模型来预测学生在给定测试中的未来表现。将该模型应用于RIIID数据集，我们证明使用多个时间特征粒度作为解码器输入可以显着提高模型性能。我们的结果还表明了我们方法的有效性，相对于LightGBM方法有很大的改进。我们的工作为教育领域的AI发展做出了贡献，提供了一个可伸缩和准确的预测学生成果的工具。

    In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
    
[^21]: OpenAL: 主动学习策略的评估与解释

    OpenAL: Evaluation and Interpretation of Active Learning Strategies. (arXiv:2304.05246v1 [cs.LG])

    [http://arxiv.org/abs/2304.05246](http://arxiv.org/abs/2304.05246)

    OpenAL是一个灵活且开源的框架，可以在一系列现实任务上轻松运行和比较采样AL策略，具有可解释性指标和统计分析方法，针对主动学习的一次性特性，从业人员也可以轻松扩展基准测试。

    

    尽管在主动学习（AL）方面已经有大量的文献，但是还没有一种全面且开放的基准可以有效地比较提出的采样器。此外，文献中实验设置的变异性使得选择采样策略变得困难，这是由于主动学习实验的一次性特性非常重要。为了解决这些限制，我们引入了OpenAL，这是一个灵活且开源的框架，可以在一系列现实任务上轻松运行和比较采样AL策略。该基准测试还加入了可解释性指标和统计分析方法，以了解何时以及为什么一些采样器优于其他采样器。最后，从业人员可以通过提交自己的AL采样器轻松扩展基准测试。

    Despite the vast body of literature on Active Learning (AL), there is no comprehensive and open benchmark allowing for efficient and simple comparison of proposed samplers. Additionally, the variability in experimental settings across the literature makes it difficult to choose a sampling strategy, which is critical due to the one-off nature of AL experiments. To address those limitations, we introduce OpenAL, a flexible and open-source framework to easily run and compare sampling AL strategies on a collection of realistic tasks. The proposed benchmark is augmented with interpretability metrics and statistical analysis methods to understand when and why some samplers outperform others. Last but not least, practitioners can easily extend the benchmark by submitting their own AL samplers.
    
[^22]: r-softmax: 具有可控稀疏率的广义Softmax

    r-softmax: Generalized Softmax with Controllable Sparsity Rate. (arXiv:2304.05243v1 [cs.LG])

    [http://arxiv.org/abs/2304.05243](http://arxiv.org/abs/2304.05243)

    本文提出了一种新的广义Softmax函数r-softmax，可以输出具有可控稀疏度的概率分布，相较于现有的替代方案效果更好，在多标签数据集上表现突出，在预训练转换语言模型的自我注意模块中具有重要应用。

    

    如今，人工神经网络模型在许多领域取得了显著的成果。将模型提供的表示映射到概率分布的函数是深度学习解决方案的不可分割的方面。虽然softmax是机器学习社区中通常接受的概率映射函数，但它不能返回稀疏的输出，并且总是将正概率分散到所有位置。在本文中，我们提出了r-softmax，这是softmax的一种修改，它输出具有可控稀疏度的稀疏概率分布。与现有的稀疏概率映射函数相比，我们提供了一种直观的机制来控制输出稀疏度。我们在几个多标签数据集上展示了r-softmax优于其他稀疏的softmax替代方案，并且与原始的softmax相比具有高竞争力。我们还将r-softmax应用于预训练转换语言模型的自我注意模块中，并展示了它在自然语言处理方面的应用。

    Nowadays artificial neural network models achieve remarkable results in many disciplines. Functions mapping the representation provided by the model to the probability distribution are the inseparable aspect of deep learning solutions. Although softmax is a commonly accepted probability mapping function in the machine learning community, it cannot return sparse outputs and always spreads the positive probability to all positions. In this paper, we propose r-softmax, a modification of the softmax, outputting sparse probability distribution with controllable sparsity rate. In contrast to the existing sparse probability mapping functions, we provide an intuitive mechanism for controlling the output sparsity level. We show on several multi-label datasets that r-softmax outperforms other sparse alternatives to softmax and is highly competitive with the original softmax. We also apply r-softmax to the self-attention module of a pre-trained transformer language model and demonstrate that it l
    
[^23]: 基于掩蔽条件潜在扩散的生成胃肠息肉图像方法

    Mask-conditioned latent diffusion for generating gastrointestinal polyp images. (arXiv:2304.05233v1 [eess.IV])

    [http://arxiv.org/abs/2304.05233](http://arxiv.org/abs/2304.05233)

    本文提出了一种基于掩蔽条件潜在扩散的方法，可生成给定肿瘤掩模条件下的合成 GI 息肉图像，并证明该方法可以生成无限量高保真合成息肉图像，为消除内镜诊断中的有限标注问题提供了可行性。

    

    为了在内窥镜诊断中应用人工智能技术，我们需要克服有限注释问题。这些限制是由医学领域的高保密性和需求导致的，需要从专家获取密集的，昂贵的医学数据标注过程。在计算机视觉中，由于生成对抗网络(GAN)和扩散概率模型(DPM)的进展，图像合成近年来有了显著的贡献。新型 DPM 在文本、图像和视频生成任务中的表现优于 GAN。因此，本研究提出了一种条件 DPM 框架，以生成给定的生成分割掩模所限制的合成 GI 息肉图像。我们的实验结果表明，我们的系统可以生成无限数量的具有相应肉瘤真实标注掩模的高保真合成肉瘤图像。为了测试所生成的数据的实用性，我们训练了二值图像分割模型。

    In order to take advantage of AI solutions in endoscopy diagnostics, we must overcome the issue of limited annotations. These limitations are caused by the high privacy concerns in the medical field and the requirement of getting aid from experts for the time-consuming and costly medical data annotation process. In computer vision, image synthesis has made a significant contribution in recent years as a result of the progress of generative adversarial networks (GANs) and diffusion probabilistic models (DPM). Novel DPMs have outperformed GANs in text, image, and video generation tasks. Therefore, this study proposes a conditional DPM framework to generate synthetic GI polyp images conditioned on given generated segmentation masks. Our experimental results show that our system can generate an unlimited number of high-fidelity synthetic polyp images with the corresponding ground truth masks of polyps. To test the usefulness of the generated data, we trained binary image segmentation model
    
[^24]: 邻里犬与街头流浪汉：Nextdoor社交网络中真实世界不平等的在线体现

    Lady and the Tramp Nextdoor: Online Manifestations of Real-World Inequalities in the Nextdoor Social Network. (arXiv:2304.05232v1 [cs.SI])

    [http://arxiv.org/abs/2304.05232](http://arxiv.org/abs/2304.05232)

    本文第一个大规模研究了Nextdoor社交网络，发现不同收入水平的社区在社交网络上的在线行为不同，更富裕的社区情感更积极，更多地讨论犯罪，尽管实际犯罪率要低得多，同时用户生成内容能够预测收入和不平等。

    

    收入影响着生活中很多选择，从健康到教育。许多研究都利用在线社交网络的数据研究此问题。但在本文中，我们提出了一个相反的问题：不同收入水平是否导致不同的在线行为？我们证明了确实是如此。我们展示了第一个Nextdoor社交网络的大规模研究，在美国的64,283个社区和英国的3,325个社区收集了2.6万篇帖子，以研究在线话语是否反映了社区的收入和收入不平等。我们发现不同收入的社区的帖子确实不同，比如更富裕的社区情感更积极，更多地讨论犯罪，尽管实际犯罪率要低得多。然后，我们展示了用户生成内容可预测收入和不平等。我们训练了多个机器学习模型，预测了收入（R-Square=0.841）和不平等（R-Sq…

    From health to education, income impacts a huge range of life choices. Many papers have leveraged data from online social networks to study precisely this. In this paper, we ask the opposite question: do different levels of income result in different online behaviors? We demonstrate it does. We present the first large-scale study of Nextdoor, a popular location-based social network. We collect 2.6 Million posts from 64,283 neighborhoods in the United States and 3,325 neighborhoods in the United Kingdom, to examine whether online discourse reflects the income and income inequality of a neighborhood. We show that posts from neighborhoods with different income indeed differ, e.g. richer neighborhoods have a more positive sentiment and discuss crimes more, even though their actual crime rates are much lower. We then show that user-generated content can predict both income and inequality. We train multiple machine learning models and predict both income (R-Square=0.841) and inequality (R-Sq
    
[^25]: 基于L2，0基数惩罚的不均匀图趋势过滤。

    Inhomogeneous graph trend filtering via a l2,0 cardinality penalty. (arXiv:2304.05223v1 [cs.LG])

    [http://arxiv.org/abs/2304.05223](http://arxiv.org/abs/2304.05223)

    本文提出了一种基于L2，0基数惩罚的图趋势过滤（GTF）模型，可同时进行k-means聚类和基于图的最小割，以估计在节点之间具有不均匀平滑水平的分段平滑图信号，并在降噪、支持恢复和半监督分类任务上表现更好，比现有方法更高效地处理大型数据集。

    

    我们研究了在图上估计分段平滑信号的方法，并提出了一种$\ell_{2,0}$-范数惩罚图趋势过滤（GTF）模型，以估计在节点之间具有不均匀平滑水平的分段平滑图信号。我们证明了所提出的GTF模型同时是基于节点上的信号的k-means聚类和基于图的最小割，其中聚类和割共享相同的分配矩阵。我们提出了两种方法来解决所提出的GTF模型：一种是基于谱分解的方法，另一种是基于模拟退火的方法。在合成和现实数据集的实验中，我们展示了所提出的GTF模型在降噪、支持恢复和半监督分类任务上表现更好，且比现有方法更高效地解决了大型数据集的问题。

    We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibits inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.
    
[^26]: BanditQ - 在敌对环境中保证用户每次奖励的无悔学习

    BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments. (arXiv:2304.05219v1 [cs.LG])

    [http://arxiv.org/abs/2304.05219](http://arxiv.org/abs/2304.05219)

    提出了一种名为BanditQ的新的在线预测策略，在敌对设置中以公平的方式达到目标速率约束，并实现了$O(T^{3/4})$的遗憾。

    

    经典的在线预测算法如Hedge在设计上具有不公平性，因为它们尝试尽可能多地玩最具回报的臂而忽略次优臂，以实现亚线性遗憾。本文考虑在具有对所有臂累积奖励速率下界的敌对设置中，以公平的在线预测问题。通过将基本排队论与在线学习相结合，我们提出了一种名为BanditQ的新的在线预测策略，它在全信息设置下实现了目标速率约束，并实现了$O(T^{3/4})$的遗憾。BanditQ的设计和分析涉及潜在函数方法的新颖应用，并具有独立的兴趣。

    Classic online prediction algorithms, such as Hedge, are inherently unfair by design, as they try to play the most rewarding arm as many times as possible while ignoring the sub-optimal arms to achieve sublinear regret. In this paper, we consider a fair online prediction problem in the adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms. By combining elementary queueing theory with online learning, we propose a new online prediction policy, called BanditQ, that achieves the target rate constraints while achieving a regret of $O(T^{3/4})$ in the full-information setting. The design and analysis of BanditQ involve a novel use of the potential function method and are of independent interest.
    
[^27]: 一种用于遥感图像的十亿级基础模型

    A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])

    [http://arxiv.org/abs/2304.05215](http://arxiv.org/abs/2304.05215)

    本文介绍了一个用于遥感图像的十亿级基础模型，并研究了增加模型参数数量对该模型在下游任务中的性能影响，实验显示增加模型参数数量可以显著提高性能。

    

    随着基础模型在视觉任务中的潜力引起了广泛关注，先对这些模型进行预训练已成为一个关键步骤。预训练基础模型的三个关键因素是预训练方法、预训练数据集的大小以及模型参数的数量。最近，遥感领域的研究主要关注预训练方法和数据集的大小，对模型参数的数量关注较少。本文通过研究增加模型参数数量对基础模型在旋转目标检测和语义分割等下游任务中性能的影响来弥补这一空白。我们使用不同数量参数（包括86M、605.26M、1.3B和2.4B）的基础模型进行预训练，以确定参数增加是否会提高下游任务的性能。据我们所知，这是第一个用于遥感图像的十亿级基础模型。我们的实验表明，增加模型参数数量可以显著提高下游任务的性能。此外，我们还介绍了一个包含10亿个遥感图像的新的预训练数据集，并向研究社区公开。

    As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
    
[^28]: CGXplain: 基于规则的深度神经网络解释，使用双线性规划。

    CGXplain: Rule-Based Deep Neural Network Explanations Using Dual Linear Programs. (arXiv:2304.05207v1 [cs.LG])

    [http://arxiv.org/abs/2304.05207](http://arxiv.org/abs/2304.05207)

    该论文介绍了一种名为 CGX 的分解方法，使用双线性规划从深度神经网络的隐藏表示中提取规则，优化对齐，复杂度和稳定性。

    

    基于规则的代理模型是近似深度神经网络决策边界的一种有效且可解释的方式，使人类易于理解深度学习模型。现有的最先进的分解方法，即那些考虑到DNN的潜在空间以提取更精确规则集的方法，成功地推导出了高精度的规则集。但是，它们 a) 不能保证代理模型已从与 DNN 相同的变量中学习（对齐），b) 只允许优化单个目标，例如准确性，这可能导致过度大的规则集（复杂性），并且 c) 使用决策树算法作为中间模型，可能会导致相同DNN的不同解释（稳定性）。本文介绍了使用双线性规划从DNN的隐藏表示中提取规则的分解方法 CGX（列生成解释器） 来解决这些限制。这种方法允许优化多个目标，例如对齐，复杂度和稳定性。

    Rule-based surrogate models are an effective and interpretable way to approximate a Deep Neural Network's (DNN) decision boundaries, allowing humans to easily understand deep learning models. Current state-of-the-art decompositional methods, which are those that consider the DNN's latent space to extract more exact rule sets, manage to derive rule sets at high accuracy. However, they a) do not guarantee that the surrogate model has learned from the same variables as the DNN (alignment), b) only allow to optimise for a single objective, such as accuracy, which can result in excessively large rule sets (complexity), and c) use decision tree algorithms as intermediate models, which can result in different explanations for the same DNN (stability). This paper introduces the CGX (Column Generation eXplainer) to address these limitations a decompositional method using dual linear programming to extract rules from the hidden representations of the DNN. This approach allows to optimise for a
    
[^29]: 容量与鲁棒性之间的折衷：重访多元时间序列预测的通道独立策略

    The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting. (arXiv:2304.05206v1 [cs.LG])

    [http://arxiv.org/abs/2304.05206](http://arxiv.org/abs/2304.05206)

    本文研究了多元时间序列预测中的通道相关（CD）与通道独立（CI）策略。结果表明，CD策略容量更高但鲁棒性较差，CI策略鲁棒性更强且能更好地处理缺失值和不规则时间间隔。

    

    多元时间序列数据包含各种变量通道。多元预测模型需要捕捉通道之间的关系，以准确预测未来的值。然而，最近出现了一些采用通道独立（CI）策略的方法。这些方法将多元时间序列数据视为分离的单元时间序列，忽略通道之间的相关性。令人惊讶的是，我们的实证结果表明，采用CI策略训练的模型通常比采用通道相关（CD）策略的模型表现更好，差距显著。然而，这种现象背后的原因尚未在文献中得到深入探讨。本文全面分析了多元时间序列数据集和CI / CD策略的特征。我们的结果表明，CD方法具有更高的容量，但通常缺乏鲁棒性以准确预测中断的时间序列。另一方面，CI方法牺牲了一些容量，但具有更高的鲁棒性，并且可以更好地处理缺失值和不规则的时间间隔。

    Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict dis
    
[^30]: TinyReptile：联邦元学习实现的迷你机器学习

    TinyReptile: TinyML with Federated Meta-Learning. (arXiv:2304.05201v1 [cs.LG])

    [http://arxiv.org/abs/2304.05201](http://arxiv.org/abs/2304.05201)

    TinyReptile是一个联邦元学习实现的迷你机器学习算法，可以在迷你设备上协作学习神经网络，并通过使用全局数据实现快速收敛和保护本地数据隐私。

    

    迷你机器学习（TinyML）是一个迅速发展的领域，旨在为资源受限的微控制器（MCU）民主化机器学习。鉴于这些微型设备的普及性，有必要问是否TinyML应用程序可以从聚合他们的知识中受益。联邦学习（FL）使得分散的代理可以共同学习一个全局模型，而不共享敏感的本地数据。然而，由于实际部署环境的复杂性和每个设备可用数据的异构性，一个常见的全局模型可能不能适用于所有设备。此外， TinyML 硬件的部署具有重要的计算和通信约束，传统机器学习无法解决这些问题。针对这些挑战，我们提出了 TinyReptile，这是一个简单而有效的算法，灵感来源于元学习和在线学习，可以在迷你设备上协作学习神经网络（NN）的坚实初始化，可以快速适应这些设备的本地数据。我们使用 FL 来评估 TinyReptile，结果表明 TinyReptile 可以在保护本地数据隐私的同时，利用全局数据实现快速收敛和精确性能。

    Tiny machine learning (TinyML) is a rapidly growing field aiming to democratize machine learning (ML) for resource-constrained microcontrollers (MCUs). Given the pervasiveness of these tiny devices, it is inherent to ask whether TinyML applications can benefit from aggregating their knowledge. Federated learning (FL) enables decentralized agents to jointly learn a global model without sharing sensitive local data. However, a common global model may not work for all devices due to the complexity of the actual deployment environment and the heterogeneity of the data available on each device. In addition, the deployment of TinyML hardware has significant computational and communication constraints, which traditional ML fails to address. Considering these challenges, we propose TinyReptile, a simple but efficient algorithm inspired by meta-learning and online learning, to collaboratively learn a solid initialization for a neural network (NN) across tiny devices that can be quickly adapted 
    
[^31]: HPN: 个性化联邦超参数优化

    HPN: Personalized Federated Hyperparameter Optimization. (arXiv:2304.05195v1 [cs.LG])

    [http://arxiv.org/abs/2304.05195](http://arxiv.org/abs/2304.05195)

    该论文提出了一种新的学习超参数网络方案(HPN)，该网络可以使用客户端编码来决定个性化的超参数选择，同时保护客户端的隐私，以解决联邦学习中客户端异质性的问题，并提出了一种去除函数评估偏差的机制。

    

    在联邦学习领域的众多研究中，个性化是解决客户端异质性的一个至关重要和具有挑战性的问题，但已有的作品主要关注的是调整模型。然而，由于客户端的异质性，他们每个人可能需要不同的超参数选择，这方面尚未进行研究。我们指出了个性化联邦超参数优化 (pFedHPO) 的两个挑战：如何处理指数级增长的搜索空间，如何表征每个客户端而不损害其数据隐私。为了克服这些挑战，我们提出了一种学习超参数网络 (HPN) 的方案，该网络使用客户端编码来决定个性化的超参数选择。客户端编码使用基于随机投影的过程进行计算，以保护每个客户端的隐私。此外，我们设计了一种新的机制来去除低保真度函数评估样本的偏差，以便学习 HPN。

    Numerous research studies in the field of federated learning (FL) have attempted to use personalization to address the heterogeneity among clients, one of FL's most crucial and challenging problems. However, existing works predominantly focus on tailoring models. Yet, due to the heterogeneity of clients, they may each require different choices of hyperparameters, which have not been studied so far. We pinpoint two challenges of personalized federated hyperparameter optimization (pFedHPO): handling the exponentially increased search space and characterizing each client without compromising its data privacy. To overcome them, we propose learning a \textsc{H}yper\textsc{P}arameter \textsc{N}etwork (HPN) fed with client encoding to decide personalized hyperparameters. The client encoding is calculated with a random projection-based procedure to protect each client's privacy. Besides, we design a novel mechanism to debias the low-fidelity function evaluation samples for learning HPN. We con
    
[^32]: 自动梯度下降：无超参数的深度学习

    Automatic Gradient Descent: Deep Learning without Hyperparameters. (arXiv:2304.05187v1 [cs.LG])

    [http://arxiv.org/abs/2304.05187](http://arxiv.org/abs/2304.05187)

    本文提出了一种使用神经网络结构信息定义优化算法的方法，实现了一种无需手动调整超参数的一阶优化器 - 自动梯度下降。该算法在深度全连接网络和卷积网络中表现良好，并在标准基准测试数据集上表现出与手动调整优化器相当的效果。

    

    本文提出了一种新的方法来派生特定于神经网络结构的优化算法，实现了无超参数的一阶优化器，称为“自动梯度下降”。该方法利用神经体系结构显式地定义网络结构参数来优化深度全连接网络和卷积网络，证明了在标准基准测试数据集上与手动调整优化器效果相当。该算法扩展了镜像下降方法以处理非凸性复合目标函数。

    The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at Im
    
[^33]: 自我监督学习用于属性图异常检测中的异常判别与表示学习解耦：DSLAD算法

    Decoupling anomaly discrimination and representation learning: self-supervised learning for anomaly detection on attributed graph. (arXiv:2304.05176v1 [cs.LG])

    [http://arxiv.org/abs/2304.05176](http://arxiv.org/abs/2304.05176)

    本文提出了一种独特的自我监督算法DSLAD，通过解耦异常判别和表示学习来进行属性图上的异常检测，构建了一个平衡的特征空间解决了语义混合和不平衡问题，证明了其有效性。

    

    属性图上的异常检测是一个实际应用中很重要的课题。现有方法因主要关注异常判别而忽略表示学习，存在语义混合和不平衡问题。这与异质性假设相冲突，即异常节点通常直接与正常节点相连。此外，异常节点比正常节点少得多，表明数据分布呈长尾形态。为了应对这些挑战，本文提出了一种独特的算法DSLAD，它是一种自我监督的方法，通过将异常判别和表示学习进行解耦来进行异常检测。DSLAD采用双线性池和掩码自编码器作为异常判别器。通过解耦异常判别和表示学习，构建了一个平衡的特征空间，其中节点更具语义鉴别能力，同时解决了不平衡问题。在各种数据集上的广泛实验表明DSLAD的有效性。

    Anomaly detection on attributed graphs is a crucial topic for its practical application. Existing methods suffer from semantic mixture and imbalance issue because they mainly focus on anomaly discrimination, ignoring representation learning. It conflicts with the assortativity assumption that anomalous nodes commonly connect with normal nodes directly. Additionally, there are far fewer anomalous nodes than normal nodes, indicating a long-tailed data distribution. To address these challenges, a unique algorithm,Decoupled Self-supervised Learning forAnomalyDetection (DSLAD), is proposed in this paper. DSLAD is a self-supervised method with anomaly discrimination and representation learning decoupled for anomaly detection. DSLAD employs bilinear pooling and masked autoencoder as the anomaly discriminators. By decoupling anomaly discrimination and representation learning, a balanced feature space is constructed, in which nodes are more semantically discriminative, as well as imbalance issu
    
[^34]: 基于统计和机器学习算法的电力需求预测：以乌克兰为例

    Electricity Demand Forecasting with Hybrid Statistical and Machine Learning Algorithms: Case Study of Ukraine. (arXiv:2304.05174v1 [cs.LG])

    [http://arxiv.org/abs/2304.05174](http://arxiv.org/abs/2304.05174)

    本文提出了一种新型的混合方法，使用统计和机器学习的算法结合预测电力需求，并成功地应用于乌克兰的电力消耗数据。方法中包括了宏观经济回归分析、温度和日历回归变量相结合、ARIMA和LSTM“黑匣子”模型等，使得预测能够覆盖长、中、短期不同阶段以及小时季节性。

    

    本文提出了一种新颖的混合方法，利用统计和机器学习来预测国家电力需求。由于未来能源系统的投资和运营需要具有小时分辨率的长期电力需求预测，因此我们的数学模型填补了能源预测中的空白。所提出的方法使用乌克兰从2013年至2020年的电力消耗小时数据进行构建。为此，我们分析了电力消费的小时、日和年的时间序列的基本结构。通过宏观经济回归分析评估了长期年趋势。中期模型结合温度和日历回归变量来描述基本结构，并结合ARIMA和LSTM“黑匣子”模型来描述误差项。短期模型通过日历回归变量和多个ARMA模型来捕捉小时季节性。结果表明，最佳预测可实现

    This article presents a novel hybrid approach using statistics and machine learning to forecast the national demand of electricity. As investment and operation of future energy systems require long-term electricity demand forecasts with hourly resolution, our mathematical model fills a gap in energy forecasting. The proposed methodology was constructed using hourly data from Ukraine's electricity consumption ranging from 2013 to 2020. To this end, we analysed the underlying structure of the hourly, daily and yearly time series of electricity consumption. The long-term yearly trend is evaluated using macroeconomic regression analysis. The mid-term model integrates temperature and calendar regressors to describe the underlying structure, and combines ARIMA and LSTM ``black-box'' pattern-based approaches to describe the error term. The short-term model captures the hourly seasonality through calendar regressors and multiple ARMA models for the residual. Results show that the best forecast
    
[^35]: 从网络规模的图像-文本数据检索提高图像识别的精度

    Improving Image Recognition by Retrieving from Web-Scale Image-Text Data. (arXiv:2304.05173v1 [cs.CV])

    [http://arxiv.org/abs/2304.05173](http://arxiv.org/abs/2304.05173)

    本论文介绍了一种基于注意力的记忆模块，通过从存储器集合中检索相似实例，消除了不相关实例的影响，保留了有益于输入查询的实例。使用大规模记忆数据集的实验表明该方法在三个不同的分类任务中实现了一流的成果。

    

    基于检索的增强模型在计算机视觉任务中变得越来越流行，尤其是在自然语言处理问题中的成功后。目标是通过从外部存储器集合中检索相似实例来增强模型的识别能力。在本文中，我们提出了一种基于注意力的记忆模块，该模块学习从存储器检索到的每个实例的重要性。与现有方法相比，我们的方法消除了不相关检索实例的影响，并保留了有益于输入查询的实例。我们还彻底研究了构建记忆数据集的各种方法。我们的实验表明，使用10亿个图像-文本对的大规模记忆数据集具有很大的好处，并展示了不同记忆表示的性能。我们在三个不同的分类任务中评价了我们的方法，即长尾识别、学习嘈杂标签和细粒度分类，并展示了它实现了一流的成果。

    Retrieval augmented models are becoming increasingly popular for computer vision tasks after their recent success in NLP problems. The goal is to enhance the recognition capabilities of the model by retrieving similar examples for the visual input from an external memory set. In this work, we introduce an attention-based memory module, which learns the importance of each retrieved example from the memory. Compared to existing approaches, our method removes the influence of the irrelevant retrieved examples, and retains those that are beneficial to the input query. We also thoroughly study various ways of constructing the memory dataset. Our experiments show the benefit of using a massive-scale memory dataset of 1B image-text pairs, and demonstrate the performance of different memory representations. We evaluate our method in three different classification tasks, namely long-tailed recognition, learning with noisy labels, and fine-grained classification, and show that it achieves state-
    
[^36]: 基于课程的多技能模仿学习

    Curriculum-Based Imitation of Versatile Skills. (arXiv:2304.05171v1 [cs.LG])

    [http://arxiv.org/abs/2304.05171](http://arxiv.org/abs/2304.05171)

    论文提出了基于课程的多技能模仿学习算法，以专业化处理局部内容，同时通过奖励机制激励尽可能多地涵盖数据。

    

    通过模仿学习技能是为机器人提供直观教学的有前途的概念。学习此类技能的一种常见方法是通过最大化给定演示下的似然来学习参数模型。然而，人类演示往往是多模态的，即相同的任务以多种方式解决，这对大多数基于最大似然（ML）目标的模仿学习方法构成了重大挑战。ML目标强制模型涵盖所有数据，防止它在上下文空间中专业化，并可能导致行为空间中的模式平均化，从而导致次优或潜在的灾难性行为。在这里，我们通过引入一个课程，为每个数据点引入一个权重来缓解这些问题，允许模型专门处理它可以代表的数据，同时通过熵奖励来激励模型尽可能涵盖更多的数据。我们将算法扩展到线性专家的混合物，使单个组件可以专门处理局部内容。

    Learning skills by imitation is a promising concept for the intuitive teaching of robots. A common way to learn such skills is to learn a parametric model by maximizing the likelihood given the demonstrations. Yet, human demonstrations are often multi-modal, i.e., the same task is solved in multiple ways which is a major challenge for most imitation learning methods that are based on such a maximum likelihood (ML) objective. The ML objective forces the model to cover all data, it prevents specialization in the context space and can cause mode-averaging in the behavior space, leading to suboptimal or potentially catastrophic behavior. Here, we alleviate those issues by introducing a curriculum using a weight for each data point, allowing the model to specialize on data it can represent while incentivizing it to cover as much data as possible by an entropy bonus. We extend our algorithm to a Mixture of (linear) Experts (MoE) such that the single components can specialize on local context
    
[^37]: 探索和利用不确定性进行不完整多视图分类

    Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification. (arXiv:2304.05165v1 [cs.LG])

    [http://arxiv.org/abs/2304.05165](http://arxiv.org/abs/2304.05165)

    提出了一种基于不确定性的不完整多视图数据分类模型，通过构建分布并多次抽样，实现更加可感知的填充和可控的融合。

    

    由于在实际应用中任意视图缺失广泛存在，因此分类不完整的多视图数据是不可避免的。尽管取得了巨大进展，但由于缺失视图的相对高的不确定性特性，现有的不完整多视图方法仍然很难获得可信的预测。为了探索和利用不确定性，我们提出了一种基于不确定性的不完整多视图数据分类模型，用于在稳定可靠的框架下对不完整的多视图数据进行分类。我们构建了一个分布并进行多次抽样以表征缺失视图的不确定性，并根据抽样质量自适应地利用它们。因此，所提出的方法实现了更加可感知的填充和可控的融合。

    Classifying incomplete multi-view data is inevitable since arbitrary view missing widely exists in real-world applications. Although great progress has been achieved, existing incomplete multi-view methods are still difficult to obtain a trustworthy prediction due to the relatively high uncertainty nature of missing views. First, the missing view is of high uncertainty, and thus it is not reasonable to provide a single deterministic imputation. Second, the quality of the imputed data itself is of high uncertainty. To explore and exploit the uncertainty, we propose an Uncertainty-induced Incomplete Multi-View Data Classification (UIMC) model to classify the incomplete multi-view data under a stable and reliable framework. We construct a distribution and sample multiple times to characterize the uncertainty of missing views, and adaptively utilize them according to the sampling quality. Accordingly, the proposed method realizes more perceivable imputation and controllable fusion. Specifi
    
[^38]: NeAT：神经艺术追踪技术实现美学风格转换

    NeAT: Neural Artistic Tracing for Beautiful Style Transfer. (arXiv:2304.05139v1 [cs.CV])

    [http://arxiv.org/abs/2304.05139](http://arxiv.org/abs/2304.05139)

    NeAT是一种神经艺术追踪技术，它重新将前馈式风格转移表述为图像编辑，能在保持源内容和匹配目标风格方面都达到最先进的效果，而且在识别和修复"样式光环"方面也更加出色，使用BBST-4M数据集提高了其在各种风格下的泛化能力。

    

    风格转移任务是在第二个目标图像的艺术风格下重新生成源图像的语义内容。本文提出了NeAT，一种新的最先进的前馈式风格转移方法。我们重新将前馈式风格转移表述为图像编辑，而不是图像生成，从而得到一个模型，在保持源内容和匹配目标风格方面都优于现有技术。我们模型成功的重要组成部分是识别和修复"样式光环"，这是许多风格转移技术中经常出现的产物。除了在标准数据集上进行训练和测试，我们还介绍了BBST-4M数据集，这是一个新的大规模、高分辨率的4M图像数据集。作为筛选这些数据的一部分，我们提出了一种新颖的模型，能够对一张图像进行风格分类。我们使用BBST-4M数据集来提高并测量NeAT在各种风格下的泛化能力。NeAT不仅提供最先进的质量，还具有更好的内容保存和风格匹配性能，是一种非常优秀的风格转移技术。

    Style transfer is the task of reproducing the semantic contents of a source image in the artistic style of a second target image. In this paper, we present NeAT, a new state-of-the art feed-forward style transfer method. We re-formulate feed-forward style transfer as image editing, rather than image generation, resulting in a model which improves over the state-of-the-art in both preserving the source content and matching the target style. An important component of our model's success is identifying and fixing "style halos", a commonly occurring artefact across many style transfer techniques. In addition to training and testing on standard datasets, we introduce the BBST-4M dataset, a new, large scale, high resolution dataset of 4M images. As a component of curating this data, we present a novel model able to classify if an image is stylistic. We use BBST-4M to improve and measure the generalization of NeAT across a huge variety of styles. Not only does NeAT offer state-of-the-art qual
    
[^39]: 基于生成式深度学习和增材制造的异质性分层生物启发式蜘蛛网结构的建模及设计

    Modeling and design of heterogeneous hierarchical bioinspired spider web structures using generative deep learning and additive manufacturing. (arXiv:2304.05137v1 [cs.LG])

    [http://arxiv.org/abs/2304.05137](http://arxiv.org/abs/2304.05137)

    本文采用生成式深度学习和增材制造来建模和设计基于蜘蛛网的生物启发式三维网状结构，以解决其复杂的设计特征带来的挑战性。

    

    蜘蛛网是非常不可思议的生物结构，由薄但强韧的蚕丝组成，并排列成复杂的分层结构，具有惊人的力学性能（例如轻量但高强度，实现各种力学响应）。简单的2D网形状很容易被模仿，但建模和合成基于三维网状结构的网是具有挑战性的，部分原因是由于设计特征的丰富性。在本文中，我们提供了关于蜘蛛网异质性图结构的详细分析，并使用深度学习对人工合成的生物启发式三维网状结构进行建模。生成式AI模型是基于主要几何参数进行约束（包括平均边长度，节点数，平均节点度等等）。为了识别图构建原则，我们使用归纳表示采样的大型实验确定的蜘蛛网图形数据集，从而训练出三个有条件的生成模型：1) A

    Spider webs are incredible biological structures, comprising thin but strong silk filament and arranged into complex hierarchical architectures with striking mechanical properties (e.g., lightweight but high strength, achieving diverse mechanical responses). While simple 2D orb webs can easily be mimicked, the modeling and synthesis of 3D-based web structures remain challenging, partly due to the rich set of design features. Here we provide a detailed analysis of the heterogenous graph structures of spider webs, and use deep learning as a way to model and then synthesize artificial, bio-inspired 3D web structures. The generative AI models are conditioned based on key geometric parameters (including average edge length, number of nodes, average node degree, and others). To identify graph construction principles, we use inductive representation sampling of large experimentally determined spider web graphs, to yield a dataset that is used to train three conditional generative models: 1) A
    
[^40]: RecUP-FL: 通过用户可配置的隐私防护实现联邦学习中的效用和隐私的协调

    RecUP-FL: Reconciling Utility and Privacy in Federated Learning via User-configurable Privacy Defense. (arXiv:2304.05135v1 [cs.LG])

    [http://arxiv.org/abs/2304.05135](http://arxiv.org/abs/2304.05135)

    该论文提出了一种名为RecUP-FL的用户可配置隐私防护机制，该机制可以更好地平衡隐私和效用，提高了联邦学习中模型的效果。

    

    联邦学习（FL）通过允许客户端协同训练模型而不共享私人数据，提供了各种隐私优势。然而，最近的研究表明，通过共享的梯度仍然可能泄露私人信息。为了进一步降低隐私泄露的风险，现有的防御通常要求客户端在共享给服务器之前本地修改其梯度（例如，差分隐私）。虽然这些方法在某些情况下是有效的，但它们将整个数据视为单个实体来保护，这通常会付出非常高的模型效用代价。在本文中，我们通过提出一个用户可配置的隐私防护机制RecUP-FL来协调FL中的效用和隐私，可以更好地关注用户指定的敏感属性，并在传统防御方法之上获得显着的模型效用改进。此外，我们观察到现有的推断攻击通常依赖于机器学习模型从共享的梯度中提取私人信息，并提出了一种可有效防御此类攻击的防御机制。我们在两个真实数据集上的实验结果表明，RecUP-FL实现了隐私和效用之间更好的平衡，超过了现有最先进方法。

    Federated learning (FL) provides a variety of privacy advantages by allowing clients to collaboratively train a model without sharing their private data. However, recent studies have shown that private information can still be leaked through shared gradients. To further minimize the risk of privacy leakage, existing defenses usually require clients to locally modify their gradients (e.g., differential privacy) prior to sharing with the server. While these approaches are effective in certain cases, they regard the entire data as a single entity to protect, which usually comes at a large cost in model utility. In this paper, we seek to reconcile utility and privacy in FL by proposing a user-configurable privacy defense, RecUP-FL, that can better focus on the user-specified sensitive attributes while obtaining significant improvements in utility over traditional defenses. Moreover, we observe that existing inference attacks often rely on a machine learning model to extract the private inf
    
[^41]: 神经网络架构

    Neural Network Architectures. (arXiv:2304.05133v1 [cs.LG])

    [http://arxiv.org/abs/2304.05133](http://arxiv.org/abs/2304.05133)

    这份讲义概述了神经网络的数学视角，并介绍了前馈神经网络、卷积神经网络、残差网络和循环神经网络等架构，这些架构给机器学习提供了一种优化问题的思路。

    

    这份讲义从数学角度概述了神经网络架构。特别地，使用神经网络进行机器学习被视为一个优化问题。介绍了神经网络的基础知识以及下列几种架构：前馈神经网络，卷积神经网络，残差网络和循环神经网络。

    These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.
    
[^42]: 提高医学图像分析中私有联邦模型的性能

    Improving Performance of Private Federated Models in Medical Image Analysis. (arXiv:2304.05127v1 [cs.CR])

    [http://arxiv.org/abs/2304.05127](http://arxiv.org/abs/2304.05127)

    本论文通过本地步骤和调整功能进一步提高医学影像分析中的联邦学习模型性能。

    

    联邦学习（FL）是一种分布式机器学习（ML）方法，允许在不集中数据的情况下进行训练。这种方法对医学应用特别有益，因为它解决了与医疗数据相关的一些关键挑战，如隐私、安全和数据所有权。此外，FL可以提高用于医学应用中的ML模型的质量。医疗数据通常是多样化的，并且可以根据病人群体而有很大变化，这使得开发准确且具有一般性的ML模型具有挑战性。FL允许使用来自多个源的医疗数据，这有助于提高ML模型的质量和一般化能力。差分隐私（DP）是保护该过程安全和私密性的工具。在这项工作中，我们展示了通过采用本地步骤（一种提高FL通信效率的常用方法）和调整通信次数的数量，可以进一步提高模型性能。

    Federated learning (FL) is a distributed machine learning (ML) approach that allows data to be trained without being centralized. This approach is particularly beneficial for medical applications because it addresses some key challenges associated with medical data, such as privacy, security, and data ownership. On top of that, FL can improve the quality of ML models used in medical applications. Medical data is often diverse and can vary significantly depending on the patient population, making it challenging to develop ML models that are accurate and generalizable. FL allows medical data to be used from multiple sources, which can help to improve the quality and generalizability of ML models. Differential privacy (DP) is a go-to algorithmic tool to make this process secure and private. In this work, we show that the model performance can be further improved by employing local steps, a popular approach to improving the communication efficiency of FL, and tuning the number of communica
    
[^43]: 带目标投影的在线时空学习

    Online Spatio-Temporal Learning with Target Projection. (arXiv:2304.05124v1 [cs.NE])

    [http://arxiv.org/abs/2304.05124](http://arxiv.org/abs/2304.05124)

    提出了一种名为OSTTP的新型学习算法，解决了通过时间反向传播算法所引入的限制，使网络能够同时处理和学习新的传入数据，具有竞争性能。

    

    通过时间反向传播（BPTT）算法训练的循环神经网络已经在各种时间任务中取得了惊人的成功。但是，BPTT引入了严重的限制，例如需要向后通过时间传播信息，权重对称要求以及空间和时间上的更新锁定。这些问题成为在线训练能力至关重要的AI系统的障碍。最近，研究人员开发了受生物启发的训练算法，解决了其中的一部分问题。在这项工作中，我们提出了一种名为带目标投影的在线时空学习（OSTTP）的新型学习算法，解决了BPTT的所有前述问题。具体而言，OSTTP使网络同时处理和学习新的传入数据的能力，缓解了权重对称和更新锁定问题。我们在两个时间任务上评估了OSTTP，展示了与BPTT相比的竞争性能。

    Recurrent neural networks trained with the backpropagation through time (BPTT) algorithm have led to astounding successes in various temporal tasks. However, BPTT introduces severe limitations, such as the requirement to propagate information backwards through time, the weight symmetry requirement, as well as update-locking in space and time. These problems become roadblocks for AI systems where online training capabilities are vital. Recently, researchers have developed biologically-inspired training algorithms, addressing a subset of those problems. In this work, we propose a novel learning algorithm called online spatio-temporal learning with target projection (OSTTP) that resolves all aforementioned issues of BPTT. In particular, OSTTP equips a network with the capability to simultaneously process and learn from new incoming data, alleviating the weight symmetry and update-locking problems. We evaluate OSTTP on two temporal tasks, showcasing competitive performance compared to BPTT
    
[^44]: 不同约束运动模型在基于图的轨迹预测中的评估

    Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction. (arXiv:2304.05116v1 [cs.RO])

    [http://arxiv.org/abs/2304.05116](http://arxiv.org/abs/2304.05116)

    使用深度学习模型进行运动预测在自动驾驶中表现出色，但缺乏解释性和可能违反物理约束。因此，结合差分约束运动模型能提供物理上可行的轨迹，研究表明低阶积分器模型表现更好，并且数值求解器对模型性能产生影响。

    

    随着深度学习模型在自动驾驶中表现出色且可调性高，成为运动预测的标准。然而，高度灵活性伴随的是解释性缺失和可能违反的物理约束。使用差分约束运动模型来提供物理上可行的轨迹，可以作为与这些数据驱动方法相配合的一个有前途的方向。本研究基于先前提出的基于图神经网络的模型 MTP-GO，研究了各种运动模型结合数值求解器进行预测任务的表现。研究表明，为了获得精确的预测结果，简单的模型，如低阶积分器模型，优于更复杂的运动学模型。此外，数值求解器可以对运动预测模型的性能产生重大影响。

    Given their adaptability and encouraging performance, deep-learning models are becoming standard for motion prediction in autonomous driving. However, with great flexibility comes a lack of interpretability and possible violations of physical constraints. Accompanying these data-driven methods with differentially-constrained motion models to provide physically feasible trajectories is a promising future direction. The foundation for this work is a previously introduced graph-neural-network-based model, MTP-GO. The neural network learns to compute the inputs to an underlying motion model to provide physically feasible trajectories. This research investigates the performance of various motion models in combination with numerical solvers for the prediction task. The study shows that simpler models, such as low-order integrator models, are preferred over more complex ones, e.g., kinematic models, to achieve accurate predictions. Further, the numerical solver can have a substantial impact o
    
[^45]: 目标是系统化的日内新闻筛选: 一个关注流动性的方法

    Towards systematic intraday news screening: a liquidity-focused approach. (arXiv:2304.05115v1 [q-fin.TR])

    [http://arxiv.org/abs/2304.05115](http://arxiv.org/abs/2304.05115)

    该论文提出了一种系统化的新闻筛选方法，以识别有影响力的新闻，基于流动性驱动的变量，并利用朴素贝叶斯方法分析相关新闻情绪。

    

    新闻可以传达对金融资产看跌或看涨的观点。机构投资者需要基于文本数据自动评估隐含的新闻情绪。鉴于每天发布的大量新闻文章中，大多数都是中性的，我们提出了一种系统化的新闻筛选方法，以识别“真正”有影响力的新闻，旨在更有效地开发新闻情绪学习方法。基于几个由流动性驱动的变量，包括波动性、成交量、买卖价差和成交额，我们将每个5分钟时间段与两种特定的流动性模式之一相关联。其中一个代表市场在大部分时间停留在“平静”状态，而另一个则以相对较高的波动率和交易量为特征，描述受某些外生事件驱动的情况。然后，我们关注流动性模式从前者切换到后者的时刻，并考虑附近发布的新闻文章具有影响力。我们在这些文章上应用朴素贝叶斯方法。

    News can convey bearish or bullish views on financial assets. Institutional investors need to evaluate automatically the implied news sentiment based on textual data. Given the huge amount of news articles published each day, most of which are neutral, we present a systematic news screening method to identify the ``true'' impactful ones, aiming for more effective development of news sentiment learning methods. Based on several liquidity-driven variables, including volatility, turnover, bid-ask spread, and book size, we associate each 5-min time bin to one of two specific liquidity modes. One represents the ``calm'' state at which the market stays for most of the time and the other, featured with relatively higher levels of volatility and trading volume, describes the regime driven by some exogenous events. Then we focus on the moments where the liquidity mode switches from the former to the latter and consider the news articles published nearby impactful. We apply naive Bayes on these 
    
[^46]: 实际稀疏变分高斯过程

    Actually Sparse Variational Gaussian Processes. (arXiv:2304.05091v1 [stat.ML])

    [http://arxiv.org/abs/2304.05091](http://arxiv.org/abs/2304.05091)

    提出了一种新的跨领域变量高斯过程类，利用紧支撑B样条基函数，可以使用稀疏线性代数来显著加快矩阵运算， 实现在大规模数据集下快速高效地建模快速变化的空间现象。

    

    高斯过程（GP）通常因计算和内存需求的不利扩展而受到批评。对于大量数据集，稀疏 GP 通过在少量感应变量的条件下对数据进行总结来减少这些需求。然而，在需要许多感应变量的大型数据集（例如低长度尺度空间数据）中，即使是稀疏 GP 也可能变得计算昂贵，受使用感应变量的数量的限制。在这项工作中，我们提出了一种新的跨领域变量高斯过程类，通过将 GP 投影到一组紧支撑 B 样条基函数上来构建。我们方法的关键优势在于，B 样条基函数的紧支撑允许使用稀疏线性代数来显著加快矩阵运算，并大大减少存储占用。这使我们能够使用数以万计的感应变量非常有效地建模快速变化的空间现象，而先前的方法由于感应变量的使用限制而无法实现。

    Gaussian processes (GPs) are typically criticised for their unfavourable scaling in both computational and memory requirements. For large datasets, sparse GPs reduce these demands by conditioning on a small set of inducing variables designed to summarise the data. In practice however, for large datasets requiring many inducing variables, such as low-lengthscale spatial data, even sparse GPs can become computationally expensive, limited by the number of inducing variables one can use. In this work, we propose a new class of inter-domain variational GP, constructed by projecting a GP onto a set of compactly supported B-spline basis functions. The key benefit of our approach is that the compact support of the B-spline basis functions admits the use of sparse linear algebra to significantly speed up matrix operations and drastically reduce the memory footprint. This allows us to very efficiently model fast-varying spatial phenomena with tens of thousands of inducing variables, where previo
    
[^47]: 自我关注知识领域适应网络：浅循环下商用锂离子电池健康状态估计 (arXiv:2304.05084v1 [cs.LG])

    A Self-attention Knowledge Domain Adaptation Network for Commercial Lithium-ion Batteries State-of-health Estimation under Shallow Cycles. (arXiv:2304.05084v1 [cs.LG])

    [http://arxiv.org/abs/2304.05084](http://arxiv.org/abs/2304.05084)

    提出了一种无监督深度迁移学习方法，使用自我关注蒸馏模块和多核最大均值差异技术来跨越不同领域的鸿沟，自动从充电曲线中提取领域特定功能，并成功地将知识从大规模标记的完整循环转移到未标记的浅循环，实现了对商用锂离子电池健康状态的准确估计。

    

    准确的健康状态（SOH）估计对于确保电池供电应用的安全性，效率和可靠性至关重要。大多数SOH估计方法关注具有相似分布的0-100％满充状态（SOC）范围。然而，在现实世界的应用中，电池通常在部分SOC范围内在浅循环条件下工作，并遵循不同的退化曲线，没有可用的标记数据，因此使SOH估计变得具有挑战性。为了估计浅循环电池的SOH，提出了一种新颖的无监督深度迁移学习方法，使用自我关注蒸馏模块和多核最大均值差异技术来跨越不同领域的鸿沟。本方法自动从充电曲线中提取领域特定功能，以将知识从大规模标记的完整循环转移到未标记的浅循环。使用CALCE和SNL电池数据集验证了所提出的方法估计浅循环电池SOH的有效性。

    Accurate state-of-health (SOH) estimation is critical to guarantee the safety, efficiency and reliability of battery-powered applications. Most SOH estimation methods focus on the 0-100\% full state-of-charge (SOC) range that has similar distributions. However, the batteries in real-world applications usually work in the partial SOC range under shallow-cycle conditions and follow different degradation profiles with no labeled data available, thus making SOH estimation challenging. To estimate shallow-cycle battery SOH, a novel unsupervised deep transfer learning method is proposed to bridge different domains using self-attention distillation module and multi-kernel maximum mean discrepancy technique. The proposed method automatically extracts domain-variant features from charge curves to transfer knowledge from the large-scale labeled full cycles to the unlabeled shallow cycles. The CALCE and SNL battery datasets are employed to verify the effectiveness of the proposed method to estima
    
[^48]: TodyNet：用于多变量时间序列分类的时间动态图神经网络

    TodyNet: Temporal Dynamic Graph Neural Network for Multivariate Time Series Classification. (arXiv:2304.05078v1 [cs.LG])

    [http://arxiv.org/abs/2304.05078](http://arxiv.org/abs/2304.05078)

    TodyNet 提出了一种新的时间动态图神经网络可以提取隐藏的时空依赖关系，通过动态图机制捕获不同时间槽之间的关联，有效地解决了多变量时间序列分类中出现的问题。

    

    多变量时间序列分类(MTSC)是一项重要的数据挖掘任务，可以通过流行的深度学习技术有效地解决。然而，现有的基于深度学习的方法忽视了不同维度之间的隐藏依赖关系，并且很少考虑时间序列的独特动态特征，这缺乏足够的特征提取能力来获得令人满意的分类准确性。为了解决这个问题，我们提出了一种新的时间动态图神经网络(TodyNet)，它可以提取隐藏的时空依赖关系，而无需定义图结构。它使得隔离但相互依赖的变量之间能够进行信息流动，并通过动态图机制捕获不同时间槽之间的关联，从而进一步提高模型的分类性能。同时，由于GNN的限制，无法学习到图的层次表示。因此，我们还设计了一个时间图池层，以沿时间维度学习有区分性的多级表示。实验结果表明，TodyNet有效地捕捉了多变量时间序列固有的动态性，并在几个基准测试上显着优于现有方法。

    Multivariate time series classification (MTSC) is an important data mining task, which can be effectively solved by popular deep learning technology. Unfortunately, the existing deep learning-based methods neglect the hidden dependencies in different dimensions and also rarely consider the unique dynamic features of time series, which lack sufficient feature extraction capability to obtain satisfactory classification accuracy. To address this problem, we propose a novel temporal dynamic graph neural network (TodyNet) that can extract hidden spatio-temporal dependencies without undefined graph structure. It enables information flow among isolated but implicit interdependent variables and captures the associations between different time slots by dynamic graph mechanism, which further improves the classification performance of the model. Meanwhile, the hierarchical representations of graphs cannot be learned due to the limitation of GNNs. Thus, we also design a temporal graph pooling laye
    
[^49]: 折扣强化学习中的采样和估计故事

    A Tale of Sampling and Estimation in Discounted Reinforcement Learning. (arXiv:2304.05073v1 [cs.LG])

    [http://arxiv.org/abs/2304.05073](http://arxiv.org/abs/2304.05073)

    本文通过最小值上界提出了折扣均值估计问题的估计误差与马尔可夫过程混合特性和折扣因子之间的明确联系，并对一组显著估计器及其对应的采样程序进行了统计分析。

    

    折扣强化学习中最相关的问题包括在马尔可夫奖励过程的稳态分布下对函数均值进行估计，例如策略评估中的预期回报或策略优化中的策略梯度。在实践中，这些估计通过有限地进行周期采样来产生，这种采样方式忽略了马尔可夫过程的混合特性。目前还不清楚这种实际和理论设置之间的不匹配如何影响估计，文献中也缺乏对周期采样的缺陷以及如何最优地进行周期采样的正式研究。在本文中，我们提出了折扣均值估计问题的最小值上界，明确地将估计误差与马尔可夫过程的混合特性和折扣因子联系起来。然后，我们对一组显著估计器及其对应的采样程序进行了统计分析，包括通常使用的有限时间估计器。

    The most relevant problems in discounted reinforcement learning involve estimating the mean of a function under the stationary distribution of a Markov reward process, such as the expected return in policy evaluation, or the policy gradient in policy optimization. In practice, these estimates are produced through a finite-horizon episodic sampling, which neglects the mixing properties of the Markov process. It is mostly unclear how this mismatch between the practical and the ideal setting affects the estimation, and the literature lacks a formal study on the pitfalls of episodic sampling, and how to do it optimally. In this paper, we present a minimax lower bound on the discounted mean estimation problem that explicitly connects the estimation error with the mixing properties of the Markov process and the discount factor. Then, we provide a statistical analysis on a set of notable estimators and the corresponding sampling procedures, which includes the finite-horizon estimators often u
    
[^50]: 基于双曲几何图表示学习的不平衡节点分类研究

    Hyperbolic Geometric Graph Representation Learning for Hierarchy-imbalance Node Classification. (arXiv:2304.05059v1 [cs.LG])

    [http://arxiv.org/abs/2304.05059](http://arxiv.org/abs/2304.05059)

    本研究利用双曲几何有效表达了图的分层属性，并探究了不同层次属性训练节点对节点分类任务的影响。

    

    对于图中不平衡节点的学习已成为一个更为显著和重要的问题。图中一个重要的挑战是节点的拓扑特性（例如位置，角色）的不平衡（拓扑不平衡），而不仅仅是训练标记节点的数量（数量不平衡）。现有的关于拓扑不平衡的研究集中在节点的位置或者局部邻域结构上，忽略了图的全局底层分层属性，即层级。在现实世界中，图数据的分层结构揭示了图的重要拓扑属性并与广泛的应用相关。我们发现以不同层次属性训练标记节点对节点分类任务具有重要的影响，并在我们的实验中加以证实。众所周知，双曲几何在表示图的层次结构方面具有独特优势。

    Learning unbiased node representations for imbalanced samples in the graph has become a more remarkable and important topic. For the graph, a significant challenge is that the topological properties of the nodes (e.g., locations, roles) are unbalanced (topology-imbalance), other than the number of training labeled nodes (quantity-imbalance). Existing studies on topology-imbalance focus on the location or the local neighborhood structure of nodes, ignoring the global underlying hierarchical properties of the graph, i.e., hierarchy. In the real-world scenario, the hierarchical structure of graph data reveals important topological properties of graphs and is relevant to a wide range of applications. We find that training labeled nodes with different hierarchical properties have a significant impact on the node classification tasks and confirm it in our experiments. It is well known that hyperbolic geometry has a unique advantage in representing the hierarchical structure of graphs. Theref
    
[^51]: 深度图表示学习综述

    A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])

    [http://arxiv.org/abs/2304.05055](http://arxiv.org/abs/2304.05055)

    本文综述了深度图表示学习的研究现状和存在的问题，并指出利用深度学习已经显示出巨大的优势和潜力。

    

    图表示学习旨在将高维稀疏的图结构数据有效地编码成低维密集向量，这是一个基本任务，在包括机器学习和数据挖掘在内的一系列领域都得到了广泛的研究。传统图嵌入方法遵循这样一种基本思想，即图中相互连接的节点的嵌入矢量仍然能够保持相对接近的距离，从而保留了图中节点之间的结构信息。然而，这种方法存在以下问题：（i）传统方法的模型容量受限，限制了学习性能; （ii）现有技术通常依赖于无监督学习策略，无法与最新的学习范式相结合；（iii）表示学习和下游任务相互依存，应共同加强。随着深度学习的显着成功，深度图表示学习已经显示出巨大的潜力和优势。

    Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
    
[^52]: 一种结合基于ANN的和传统信号解析方法的混合方法，用于通信系统中FPGA的高效实现

    A Hybrid Approach combining ANN-based and Conventional Demapping in Communication for Efficient FPGA-Implementation. (arXiv:2304.05042v1 [eess.SP])

    [http://arxiv.org/abs/2304.05042](http://arxiv.org/abs/2304.05042)

    本文提出了一种新的方法，在FPGA上通过结合传统信号解析算法和自适应的自编码器实现高效的信号映射，克服了ANN网络在资源受限设备中的性能瓶颈。

    

    在通信系统中，自编码器（AE）是指利用人工神经网络（ANN）替换收发器中的部分组件，通过信道模型对整个系统进行端对端训练。该方法旨在提高通信性能，特别是针对不同的信道条件，但训练和推理过程的计算复杂度较高。可编程门阵列（FPGAs）已被证明是适用于节能的ANN实现平台。然而，由于ANN中的操作数量和模型规模巨大，因此限制了在资源受限设备上的性能，这对于低延迟和高吞吐量的通信系统至关重要。为了解决这一挑战，我们提出了一种新的方法，在FPGA上使用高效率的传统信号解析算法与具有适应性的AE相结合。在适应信道条件并调整后，ANN会隐式学习到信道特征。

    In communication systems, Autoencoder (AE) refers to the concept of replacing parts of the transmitter and receiver by artificial neural networks (ANNs) to train the system end-to-end over a channel model. This approach aims to improve communication performance, especially for varying channel conditions, with the cost of high computational complexity for training and inference. Field-programmable gate arrays (FPGAs) have been shown to be a suitable platform for energy-efficient ANN implementation. However, the high number of operations and the large model size of ANNs limit the performance on resource-constrained devices, which is critical for low latency and high-throughput communication systems. To tackle his challenge, we propose a novel approach for efficient ANN-based remapping on FPGAs, which combines the adaptability of the AE with the efficiency of conventional demapping algorithms. After adaption to channel conditions, the channel characteristics, implicitly learned by the ANN
    
[^53]: 多音高估计及其他任务的软动态时间规整方法

    Soft Dynamic Time Warping for Multi-Pitch Estimation and Beyond. (arXiv:2304.05032v1 [cs.SD])

    [http://arxiv.org/abs/2304.05032](http://arxiv.org/abs/2304.05032)

    本文介绍了软动态时间规整（SoftDTW）作为连续时间分类（CTC）的一种优秀替代方案，可以更有效地解决弱对齐数据的特征表示，特别适用于多标签问题和实值目标序列。

    

    音乐信息检索(MIR)中许多任务涉及到弱对齐数据，其中确切的时间对应关系未知。连续时间分类（CTC）损失是一种基于弱对齐训练数据学习特征表示的标准技术。然而，CTC仅限于离散值目标序列，并且在扩展到多标签问题时可能很难。在本文中，我们展示了如何使用软动态时间规整（SoftDTW），一种经典DTW的可微变体，作为CTC的替代方案。通过多音高估计作为示例场景，我们展示了SoftDTW与最先进的CTC多标签扩展获得了相当的结果。除了在算法上更加优雅，SoftDTW自然地扩展到实值目标序列。

    Many tasks in music information retrieval (MIR) involve weakly aligned data, where exact temporal correspondences are unknown. The connectionist temporal classification (CTC) loss is a standard technique to learn feature representations based on weakly aligned training data. However, CTC is limited to discrete-valued target sequences and can be difficult to extend to multi-label problems. In this article, we show how soft dynamic time warping (SoftDTW), a differentiable variant of classical DTW, can be used as an alternative to CTC. Using multi-pitch estimation as an example scenario, we show that SoftDTW yields results on par with a state-of-the-art multi-label extension of CTC. In addition to being more elegant in terms of its algorithmic formulation, SoftDTW naturally extends to real-valued target sequences.
    
[^54]: 学习用于多类分类的最优公平评分系统

    Learning Optimal Fair Scoring Systems for Multi-Class Classification. (arXiv:2304.05023v1 [cs.LG])

    [http://arxiv.org/abs/2304.05023](http://arxiv.org/abs/2304.05023)

    本文使用 MIP 技术生成公平且可解释的评分系统解决一般多类分类问题，扩展了 SLIM 框架，实验证明该方法可产生与现有方法竞争性相当的预测性能。

    

    机器学习模型在决策制定中越来越被广泛应用，特别是在信用评分、医学或累犯预测等高风险应用中。然而，人们越来越关注这些模型在解释性和它们可能产生或再现的不良偏差方面的问题。虽然解释性和公平性的概念近年来得到了科学界的广泛研究，但很少有研究探讨在公平性约束下的一般多类分类问题，并且没有提出为多类分类生成公平且可解释的模型。在本文中，我们利用混合整数线性规划（MILP）技术，在稀疏性和公平性约束下，为一般的多类分类设置生成本质上可解释的评分系统。我们的工作推广了 Rudin 和 Ustun 提出的 SLIM（Supersparse Linear Integer Models）框架，以学习二元分类的最优公平评分系统。我们在合成和真实数据集上进行了广泛的实验，比较了我们的方法与现有最先进和基准方法。结果表明，我们提出的方法可以生成具有竞争性预测性能的公平且可解释的模型。

    Machine Learning models are increasingly used for decision making, in particular in high-stakes applications such as credit scoring, medicine or recidivism prediction. However, there are growing concerns about these models with respect to their lack of interpretability and the undesirable biases they can generate or reproduce. While the concepts of interpretability and fairness have been extensively studied by the scientific community in recent years, few works have tackled the general multi-class classification problem under fairness constraints, and none of them proposes to generate fair and interpretable models for multi-class classification. In this paper, we use Mixed-Integer Linear Programming (MILP) techniques to produce inherently interpretable scoring systems under sparsity and fairness constraints, for the general multi-class classification setup. Our work generalizes the SLIM (Supersparse Linear Integer Models) framework that was proposed by Rudin and Ustun to learn optimal 
    
[^55]: 基于组织病理学图像的深度迁移学习乳腺癌检测的深入分析

    A Deep Analysis of Transfer Learning Based Breast Cancer Detection Using Histopathology Images. (arXiv:2304.05022v1 [eess.IV])

    [http://arxiv.org/abs/2304.05022](http://arxiv.org/abs/2304.05022)

    本文研究了基于组织病理学图像的乳腺癌检测，采用 ResNet50 模型进行深度迁移学习，实现了在准确率、AUC、召回率和损失率等指标方面的最佳表现。

    

    乳腺癌是妇女中最常见和最危险的癌症之一，也可能影响男性。使用组织病理学图像可以极大地提高乳腺癌治疗和检测的准确率，因为它们包含足够的表型数据。深度神经网络（DNN）通常用于提高乳腺癌检测的准确性。在我们的研究中，我们分析了 ResNet50、ResNet101、VGG16 和 VGG19 等预训练的深度迁移学习模型，用于检测包含 2453 个组织病理学图像数据集的乳腺癌。数据集中的图像被分为含有浸润性导管癌（IDC）和不含 IDC 两类。在对迁移学习模型进行分析后，我们发现 ResNet50 优于其他模型，在准确率为 90.2%、曲线下面积（AUC）为 90.0%、召回率为 94.7% 和小 margin 损失率为 3.5%。

    Breast cancer is one of the most common and dangerous cancers in women, while it can also afflict men. Breast cancer treatment and detection are greatly aided by the use of histopathological images since they contain sufficient phenotypic data. A Deep Neural Network (DNN) is commonly employed to improve accuracy and breast cancer detection. In our research, we have analyzed pre-trained deep transfer learning models such as ResNet50, ResNet101, VGG16, and VGG19 for detecting breast cancer using the 2453 histopathology images dataset. Images in the dataset were separated into two categories: those with invasive ductal carcinoma (IDC) and those without IDC. After analyzing the transfer learning model, we found that ResNet50 outperformed other models, achieving accuracy rates of 90.2%, Area under Curve (AUC) rates of 90.0%, recall rates of 94.7%, and a marginal loss of 3.5%.
    
[^56]: 习惯与目标的协同作用：一种基于变分贝叶斯框架的行为模型

    Habits and goals in synergy: a variational Bayesian framework for behavior. (arXiv:2304.05008v1 [cs.LG])

    [http://arxiv.org/abs/2304.05008](http://arxiv.org/abs/2304.05008)

    本文提出一种基于变分贝叶斯理论的行为模型，通过引入一种贝叶斯潜变量“意图”，将习惯性行为和目标导向行为联系起来，从而实现更灵活、高效的行为模拟。

    

    如何高效、灵活地行为是理解生物机体和创建智能化身人工智能的核心问题。行为被归类为两种类型：快速而不灵活的奖励最大化的习惯行为，和灵活而慢的目标导向行为。传统上，习惯和目标导向行为被认为是由大脑中的两个不同系统处理的。本文提出基于变分贝叶斯理论，将这两种行为联系起来，引入一种贝叶斯潜变量称为“意图”。习惯性行为是通过使用意图的先验分布生成的，该先验分布是没有目标的；而目标导向行为是由意图的后验分布生成的，该后验分布取决于目标。基于这个想法，我们提出了一种新颖的行为建模的贝叶斯框架。

    How to behave efficiently and flexibly is a central problem for understanding biological agents and creating intelligent embodied AI. It has been well known that behavior can be classified as two types: reward-maximizing habitual behavior, which is fast while inflexible; and goal-directed behavior, which is flexible while slow. Conventionally, habitual and goal-directed behaviors are considered handled by two distinct systems in the brain. Here, we propose to bridge the gap between the two behaviors, drawing on the principles of variational Bayesian theory. We incorporate both behaviors in one framework by introducing a Bayesian latent variable called "intention". The habitual behavior is generated by using prior distribution of intention, which is goal-less; and the goal-directed behavior is generated by the posterior distribution of intention, which is conditioned on the goal. Building on this idea, we present a novel Bayesian framework for modeling behaviors. Our proposed framework 
    
[^57]: 贝叶斯相关均衡和无悔动态

    Bayes correlated equilibria and no-regret dynamics. (arXiv:2304.05005v1 [cs.GT])

    [http://arxiv.org/abs/2304.05005](http://arxiv.org/abs/2304.05005)

    本文提出了贝叶斯相关均衡的一个概念，可以以分布式方式有效地计算和实现，并在广泛的博弈类别中达到近似最优的社会福利，在实验中验证了其有效性。

    

    本文研究了贝叶斯博弈的均衡概念，这是一种具有不完全信息的基本博弈模型。我们旨在实现三种理想的平衡性质。首先，通过在博弈中引入调解者来自然地实现均衡。其次，可以以分布式方式有效地计算均衡。第三，对于广泛的博弈类别，该类均衡近似地最大化社会福利，即通过灾变代价来度量。这三种属性允许玩家计算均衡，并通过调解者使其实现，从而在近乎最优的社会福利中达成稳定状态。我们的主要结果是存在一个均衡概念，满足这三种性质。为此，我们描述了各种（不等价的）相关均衡扩展，统称为贝叶斯相关均衡。特别是，我们关注促进玩家之间交流的沟通均衡（也称为协调机制）。然后，我们提出了一种无悔动态，以分布式方式收敛于贝叶斯相关均衡。最后，我们展示了所提出的平衡概念满足三种理想的平衡性质，并呈现了证明其有效性的实验结果。

    This paper explores equilibrium concepts for Bayesian games, which are fundamental models of games with incomplete information. We aim at three desirable properties of equilibria. First, equilibria can be naturally realized by introducing a mediator into games. Second, an equilibrium can be computed efficiently in a distributed fashion. Third, any equilibrium in that class approximately maximizes social welfare, as measured by the price of anarchy, for a broad class of games. These three properties allow players to compute an equilibrium and realize it via a mediator, thereby settling into a stable state with approximately optimal social welfare. Our main result is the existence of an equilibrium concept that satisfies these three properties.  Toward this goal, we characterize various (non-equivalent) extensions of correlated equilibria, collectively known as Bayes correlated equilibria. In particular, we focus on communication equilibria (also known as coordination mechanisms), which 
    
[^58]: 基于神经网络多网络扩散的社交推荐

    Neural Multi-network Diffusion towards Social Recommendation. (arXiv:2304.04994v1 [cs.LG])

    [http://arxiv.org/abs/2304.04994](http://arxiv.org/abs/2304.04994)

    NeMo是一个基于神经网络多网络扩散的社交推荐模型，采用生成式的负采样策略，并利用正负用户-商品交互来进行用户兴趣传播。NeMo在各种真实世界基准数据集上均优于现有基线。

    

    图神经网络在社交推荐等各类实际应用中被广泛采用。但是，现有基于图神经网络的社交推荐模型普遍存在泛化和过分平滑化的严重问题，这是由于未被充分探索的负采样方法和直接嵌入现成的图神经网络模型引起的。本文提出了一种简洁的多网络图神经网络模型（NeMo），用于社交推荐。与现有方法相比，该模型采用一种生成式的负采样策略，并利用正负用户-商品交互来进行用户兴趣传播。实验结果表明，NeMo在各种真实世界基准数据集上均优于现有基线（例如，在NDCG@15方面提高了高达38.8%）。

    Graph Neural Networks (GNNs) have been widely applied on a variety of real-world applications, such as social recommendation. However, existing GNN-based models on social recommendation suffer from serious problems of generalization and oversmoothness, because of the underexplored negative sampling method and the direct implanting of the off-the-shelf GNN models. In this paper, we propose a succinct multi-network GNN-based neural model (NeMo) for social recommendation. Compared with the existing methods, the proposed model explores a generative negative sampling strategy, and leverages both the positive and negative user-item interactions for users' interest propagation. The experiments show that NeMo outperforms the state-of-the-art baselines on various real-world benchmark datasets (e.g., by up to 38.8% in terms of NDCG@15).
    
[^59]: 通过对MUD活动的动态监测，检测IoT大流量攻击中的异常微流

    Detecting Anomalous Microflows in IoT Volumetric Attacks via Dynamic Monitoring of MUD Activity. (arXiv:2304.04987v1 [cs.CR])

    [http://arxiv.org/abs/2304.04987](http://arxiv.org/abs/2304.04987)

    本文使用SDN技术来监测和强制每个IoT设备的预期行为，并利用MUD标准减少攻击形势，通过训练单类分类模型，动态检测网络活动中的异常模式以检测大流量攻击，同时保持误报率低。

    

    IoT网络越来越成为新型网络攻击的目标。基于异常检测的方法能够发现新型攻击，但在实践中存在诸多挑战，比如误报警、解释困难和难以实现成本效益。IETF最近发布的厂商使用描述（MUD）标准有望通过明确指定IoT设备预期的网络行为来限制它们的攻击表面。在本文中，我们使用SDN来强制执行和监测每个IoT设备的预期行为，并训练单类分类模型来检测大流量攻击。我们的贡献有四个方面。一是我们开发了一个多级推理模型，通过SDN遥测动态检测MUD合规流量的网络活动中的异常模式，然后对异常流进行数据包检查。这提供了对分布式和直接攻击的强化细粒度可见性，使我们能够精确地隔离大流量来源。二是我们利用MUD减少攻击表面，使我们能够在已知的正常流量模式上训练异常检测器。三是我们介绍了一种使用MUD增强分类器以提高其准确性的新技术。四是我们在一个真实的IoT测试平台上进行了广泛的评估，结果显示我们的方法能够有效地检测大流量攻击，同时保持误报率低。

    IoT networks are increasingly becoming target of sophisticated new cyber-attacks. Anomaly-based detection methods are promising in finding new attacks, but there are certain practical challenges like false-positive alarms, hard to explain, and difficult to scale cost-effectively. The IETF recent standard called Manufacturer Usage Description (MUD) seems promising to limit the attack surface on IoT devices by formally specifying their intended network behavior. In this paper, we use SDN to enforce and monitor the expected behaviors of each IoT device, and train one-class classifier models to detect volumetric attacks.  Our specific contributions are fourfold. (1) We develop a multi-level inferencing model to dynamically detect anomalous patterns in network activity of MUD-compliant traffic flows via SDN telemetry, followed by packet inspection of anomalous flows. This provides enhanced fine-grained visibility into distributed and direct attacks, allowing us to precisely isolate volumetr
    
[^60]: 生物因子调控神经网络

    Biological Factor Regulatory Neural Network. (arXiv:2304.04982v1 [cs.LG])

    [http://arxiv.org/abs/2304.04982](http://arxiv.org/abs/2304.04982)

    本文提出了一种生物因子调控神经网络（BFReg-NN），它使用基因表达数据，并将大多数现有的生物学知识（如基因调节网络和GO）集成到模型中，以模拟生物系统中生物因子之间的关系，并在模拟数据和真实数据上取得了优越的性能和可解释性。

    

    基因是分析生物系统的基础，许多最近的工作提出通过深度学习模型利用基因表达进行各种生物任务。尽管这些模型有很好的性能，但由于它们的黑盒本质，深度神经网络很难为人类提供生物学洞见。最近，一些工作将生物知识与神经网络结合起来，以提高模型的透明性和性能。然而，这些方法只能纳入部分生物学知识，导致性能亚优。在本文中，我们提出了生物因子调控神经网络（BFReg-NN），这是一种可以模拟细胞系统中生物因子之间关系的通用框架。BFReg-NN从基因表达数据开始，能够将大多数现有的生物学知识集成到模型中，包括基因或蛋白质之间的调节关系（例如基因调节网络（GRN），蛋白质相互作用网络（PPI））和生物因子之间的层次关系（例如Gene Ontology（GO），pathways）。BFReg-NN还引入了一种称为因子调节惩罚（FRP）的新的正则化项到损失函数中，以确保学习模型中生物因子之间的调节关系得到表示。在模拟数据和真实数据上的实验结果表明，BFReg-NN相对于几种最先进的深度神经网络模型具有卓越的性能和解释性。

    Genes are fundamental for analyzing biological systems and many recent works proposed to utilize gene expression for various biological tasks by deep learning models. Despite their promising performance, it is hard for deep neural networks to provide biological insights for humans due to their black-box nature. Recently, some works integrated biological knowledge with neural networks to improve the transparency and performance of their models. However, these methods can only incorporate partial biological knowledge, leading to suboptimal performance. In this paper, we propose the Biological Factor Regulatory Neural Network (BFReg-NN), a generic framework to model relations among biological factors in cell systems. BFReg-NN starts from gene expression data and is capable of merging most existing biological knowledge into the model, including the regulatory relations among genes or proteins (e.g., gene regulatory networks (GRN), protein-protein interaction networks (PPI)) and the hierarc
    
[^61]: Wav2code：通过码本查找恢复干净的语音表征，用于噪声鲁棒的ASR

    Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR. (arXiv:2304.04974v1 [eess.AS])

    [http://arxiv.org/abs/2304.04974](http://arxiv.org/abs/2304.04974)

    Wav2code是一种基于自监督学习的ASR模型，可以实现用于噪声鲁棒的无失真增强，从而提供更好的语音表征。

    

    自动语音识别(ASR)由于深度学习的最新进展已经取得了显著的成功，但在实际嘈杂环境下，其性能通常会显著降低。最近的研究将语音增强(SE)引入作为前端来提高语音质量，证明了其有效性，但由于语音失真问题，可能对下游ASR不是最优的。基于这一点，最新的工作将SE和当前流行的自监督学习(SSL)结合起来来缓解失真问题并提高噪声鲁棒性。尽管有效性，但传统SE引起的语音失真仍无法完全消除。本文提出了一种名为Wav2code的自监督框架，用于实现用于噪声鲁棒ASR的无失真广义SE。首先，在预训练阶段，从SSL模型获得干净的语音表征，通过最近邻特征匹配查找离散码本，然后利用得到的代码序列来重构原始音频信号以获得干净的语音表征；接着，该代码序列被用于无失真地增强带噪语音以便于提高语音识别的鲁棒性。

    Automatic speech recognition (ASR) has gained a remarkable success thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be completely eliminated. In this paper, we propose a self-supervised framework named Wav2code to implement a generalized SE without distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the origin
    
[^62]: 带有分类器偏移的联邦学习解决类别不平衡问题

    Federated Learning with Classifier Shift for Class Imbalance. (arXiv:2304.04972v1 [cs.LG])

    [http://arxiv.org/abs/2304.04972](http://arxiv.org/abs/2304.04972)

    本文提出了一种名为FedShift的简单有效方法，通过在本地训练阶段在分类器输出上添加偏移以解决类别不平衡问题，并在各种数据集上表现优异。

    

    联邦学习旨在协作学习全局模型，而训练数据属于不同的客户端，并且不允许交换。然而，在非IID数据上的统计异质性挑战，如分类中的类别不平衡，会导致客户端漂移并显著降低全局模型的性能。本文提出了一种称为FedShift的简单有效方法，通过在本地训练阶段在分类器输出上添加偏移以减轻类别不平衡的消极影响。我们从理论上证明了FedShift中的分类器偏移可以使本地最优解与全局最优解一致，并确保算法的收敛。此外，我们的实验表明，在各种数据集上，FedShift在准确性和通信效率方面显着优于其他最先进的联邦学习方法。

    Federated learning aims to learn a global model collaboratively while the training data belongs to different clients and is not allowed to be exchanged. However, the statistical heterogeneity challenge on non-IID data, such as class imbalance in classification, will cause client drift and significantly reduce the performance of the global model. This paper proposes a simple and effective approach named FedShift which adds the shift on the classifier output during the local training phase to alleviate the negative impact of class imbalance. We theoretically prove that the classifier shift in FedShift can make the local optimum consistent with the global optimum and ensure the convergence of the algorithm. Moreover, our experiments indicate that FedShift significantly outperforms the other state-of-the-art federated learning approaches on various datasets regarding accuracy and communication efficiency.
    
[^63]: GRIL：一种二参数持久性基于向量化的机器学习方法

    GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning. (arXiv:2304.04970v1 [cs.LG])

    [http://arxiv.org/abs/2304.04970](http://arxiv.org/abs/2304.04970)

    本文提出一种名为GRIL的方法，用于将拓扑特征表示散度到机器学习模型中，该方法可以稳定地用于不同的过滤函数。

    

    一参数持久性同Topology Data Analysis (TDA)相关，可研究数据中隐藏着的连通分量和循环等拓扑特征。已应用于提高图神经网络（GNNs）等深度学习模型的表示能力。为了丰富拓扑特征的表示，本研究提出了研究双过滤函数诱导的二参数持久性模块的方法。为了将这些表示信息加入到机器学习模型中，我们引入了一个新的向量表示称为Generalized Rank Invariant Landscape \textsc{Gril}，并将其证明为在Lipschitz稳定条件下可微分，并且通过对基础过滤函数的编码可以容易地融入到机器学习模型中。我们提出了一个高效计算向量表示的算法。本研究还对我们的方法进行了测试。

    $1$-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA), studies the evolution of topological features such as connected components and cycles hidden in data. It has been applied to enhance the representation power of deep learning models, such as Graph Neural Networks (GNNs). To enrich the representations of topological features, here we propose to study $2$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vector representation called Generalized Rank Invariant Landscape \textsc{Gril} for $2$-parameter persistence modules. We show that this vector representation is $1$-Lipschitz stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vector representation efficiently. We also test our method
    
[^64]: 重新构想负提示算法：将2D扩散转化为3D，缓解“扬尼斯问题”等等

    Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond. (arXiv:2304.04968v1 [cs.CV])

    [http://arxiv.org/abs/2304.04968](http://arxiv.org/abs/2304.04968)

    本研究提出了Perp-Neg算法，通过利用得分空间的几何特性来解决目前文本到图像扩散模型中负面提示算法存在的问题，使得用户能够编辑掉初始生成图像中不想要的概念，从而提供了更大的灵活性。同时，我们还通过提出基于Perp-Neg的3D负面提示算法，将算法扩展到3D应用中。

    

    尽管文本到图像扩散模型在从文本生成图像方面取得了显著进展，但它们有时更倾向于生成类似于模型训练数据的图像，而不是提供的文本。这限制了它们在2D和3D应用中的使用。为了解决这个问题，我们探索了使用负面提示，但发现当前的实现无法产生期望的结果，特别是当主提示和负面提示之间存在重叠时。为了克服这个问题，我们提出Perp-Neg，一种利用得分空间的几何特性来解决当前负性提示算法缺点的新算法。Perp-Neg不需要对模型进行任何训练或微调。此外，我们通过实验表明，Perp-Neg通过在2D情况下使用户能够编辑掉初始生成的图像中不想要的概念，提供了生成图像更大的灵活性。此外，为了扩展我们的算法到3D应用，我们还提出了一种基于Perp-Neg的3D负面提示算法。

    Although text-to-image diffusion models have made significant strides in generating images from text, they are sometimes more inclined to generate images like the data on which the model was trained rather than the provided text. This limitation has hindered their usage in both 2D and 3D applications. To address this problem, we explored the use of negative prompts but found that the current implementation fails to produce desired results, particularly when there is an overlap between the main and negative prompts. To overcome this issue, we propose Perp-Neg, a new algorithm that leverages the geometrical properties of the score space to address the shortcomings of the current negative prompts algorithm. Perp-Neg does not require any training or fine-tuning of the model. Moreover, we experimentally demonstrate that Perp-Neg provides greater flexibility in generating images by enabling users to edit out unwanted concepts from the initially generated images in 2D cases. Furthermore, to e
    
[^65]: 卷积神经网络在波形模拟器中的先验压缩

    A priori compression of convolutional neural networks for wave simulators. (arXiv:2304.04964v1 [cs.LG])

    [http://arxiv.org/abs/2304.04964](http://arxiv.org/abs/2304.04964)

    该论文提出了一种在训练神经网络之前压缩卷积层的张量格式方法，通过替换多维核为一维滤波器来减少CNN模型的大小和减少过度拟合，从而提高实时准确预测的速度。

    

    卷积神经网络现在被广泛应用于各个领域，包括图像分类、面部和物体识别、医学成像分析等。此外，像物理信息模拟器这样的应用需要实时准确预测，但现有的神经网络模型包含数以百万计的参数，这使得在具有有限内存的设备上安装此类模型变得困难。压缩技术可能通过减少贡献到模型复杂性的参数数量来减小CNN模型的大小来解决这些问题。我们提出了一种压缩的卷积层张量格式，先于神经网络的训练进行。卷积层中的三维或二维核被一维过滤器替换。过度拟合现象也将减少。预测所需的时间或计算机仿真和预测所需的时间也将减少。

    Convolutional neural networks are now seeing widespread use in a variety of fields, including image classification, facial and object recognition, medical imaging analysis, and many more. In addition, there are applications such as physics-informed simulators in which accurate forecasts in real time with a minimal lag are required. The present neural network designs include millions of parameters, which makes it difficult to install such complex models on devices that have limited memory. Compression techniques might be able to resolve these issues by decreasing the size of CNN models that are created by reducing the number of parameters that contribute to the complexity of the models. We propose a compressed tensor format of convolutional layer, a priori, before the training of the neural network. 3-way kernels or 2-way kernels in convolutional layers are replaced by one-way fiters. The overfitting phenomena will be reduced also. The time needed to make predictions or time required fo
    
[^66]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^67]: 量子奇异值变换及量子机器学习算法的鲁棒去量子化方法研究

    Robust Dequantization of the Quantum Singular value Transformation and Quantum Machine Learning Algorithms. (arXiv:2304.04932v1 [quant-ph])

    [http://arxiv.org/abs/2304.04932](http://arxiv.org/abs/2304.04932)

    本文研究了量子机器学习算法的鲁棒去量子化方法。我们提出了近似长度平方采样的概念，并展示了如何将随机线性代数技术适应到这种更弱的假设下。我们使用这些技术证明了最近的低秩去量子化框架和spa去量子化框架。

    

    在过去的几年中，已经有几种用于解决线性代数问题和特别是量子机器学习问题的量子算法被“去量子化”。这些去量子化结果通常在经典算法通过长度平方采样方法访问数据时成立。本文研究了这些去量子化结果的稳健性。我们引入了近似长度平方采样的概念，其中经典算法只能从接近理想分布的分布中进行采样。虽然量子算法在面对小扰动时本质上是鲁棒的，但当前的去量子化技术并不是。我们的主要技术贡献在于展示了如何将许多随机线性代数技术适应到这种更弱的假设下。然后，我们使用这些技术证明了最近由Chia、Gily\'en、Li、Lin、Tang和Wang（JACM 2022）提出的低秩去量子化框架和用于spa的去量子化框架。

    Several quantum algorithms for linear algebra problems, and in particular quantum machine learning problems, have been "dequantized" in the past few years. These dequantization results typically hold when classical algorithms can access the data via length-squared sampling. In this work we investigate how robust these dequantization results are. We introduce the notion of approximate length-squared sampling, where classical algorithms are only able to sample from a distribution close to the ideal distribution in total variation distance. While quantum algorithms are natively robust against small perturbations, current techniques in dequantization are not. Our main technical contribution is showing how many techniques from randomized linear algebra can be adapted to work under this weaker assumption as well. We then use these techniques to show that the recent low-rank dequantization framework by Chia, Gily\'en, Li, Lin, Tang and Wang (JACM 2022) and the dequantization framework for spa
    
[^68]: 一种基于数据驱动的状态聚合方法用于动态离散选择模型

    A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models. (arXiv:2304.04916v1 [cs.LG])

    [http://arxiv.org/abs/2304.04916](http://arxiv.org/abs/2304.04916)

    本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。

    

    我们研究了动态离散选择模型，其中一个常见的问题是使用代理行为数据估计代理奖励函数（也称为“结构参数”）的参数。这种模型的最大似然估计需要动态规划，这受到维度灾难的限制。在本文中，我们提出了一种新颖的算法，提供了一种数据驱动的方法来选择和聚合状态，降低了估计的计算和样本复杂度。我们的方法分两个阶段。在第一阶段中，我们使用灵活的反向强化学习方法来估计代理Q函数。我们使用这些估计的Q函数，以及一个聚类算法，选择了一些最为重要的状态，这些状态对于驱动Q函数的变化最为关键。在第二阶段，利用这些被选择的“聚合”状态，我们使用常用的嵌套固定点算法进行最大似然估计。所提出的二阶段方法实现了...

    We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as "structural" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected "aggregated" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach 
    
[^69]: 使用CNN和Transformer进行金融时间序列预测

    Financial Time Series Forecasting using CNN and Transformer. (arXiv:2304.04912v1 [cs.LG])

    [http://arxiv.org/abs/2304.04912](http://arxiv.org/abs/2304.04912)

    本文提出了结合使用CNN和Transformer来预测金融时间序列的涨跌幅。

    

    时间序列预测在各个领域的决策中都是重要的。其中，金融时间序列（如股票价格）的预测往往比较困难，因为很难对数据点之间的短期和长期时间依赖性建模。卷积神经网络（CNN）对于捕捉短期依赖关系的局部模式很擅长。然而，由于有限的感受野，CNN不能学习长期依赖性。另一方面，Transformer可以学习全局上下文和长期依赖关系。在本文中，我们提出了利用CNN和Transformer的优势来建模时间序列中的短期和长期依赖关系，并预测未来价格的涨跌幅。在实验中，我们比较了所提出的方法与常用的统计和深度学习方法在预测标普500成分股盘中盘股价格变化方面的成功应用。

    Time series forecasting is important across various domains for decision-making. In particular, financial time series such as stock prices can be hard to predict as it is difficult to model short-term and long-term temporal dependencies between data points. Convolutional Neural Networks (CNN) are good at capturing local patterns for modeling short-term dependencies. However, CNNs cannot learn long-term dependencies due to the limited receptive field. Transformers on the other hand are capable of learning global context and long-term dependencies. In this paper, we propose to harness the power of CNNs and Transformers to model both short-term and long-term dependencies within a time series, and forecast if the price would go up, down or remain the same (flat) in the future. In our experiments, we demonstrated the success of the proposed method in comparison to commonly adopted statistical and deep learning methods on forecasting intraday stock price change of S&P 500 constituents.
    
[^70]: 实时无模型深度强化学习用于系列弹性执行器力控制

    Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator. (arXiv:2304.04911v1 [cs.LG])

    [http://arxiv.org/abs/2304.04911](http://arxiv.org/abs/2304.04911)

    本研究论文探讨了一种实时无模型深度强化学习技术用于系列弹性执行器的力量控制，证明了这种技术在连续控制任务中的有效性和在硬件学习中的应用价值。

    

    系列弹性执行器(SEAs)是许多最先进的机器人应用中使用的一种闭环力控制技术，可实现诸如行走、举起和操作等复杂任务。模型自由PID控制方法更容易受到SEAs非线性的不稳定性影响，而级联模型基础鲁棒控制器可以消除这些影响以实现稳定的力控制。然而，这些模型基础方法需要详细调查以准确表征系统。深度强化学习(DRL)已被证明是一种有效的连续控制任务无模型方法，其中仅有少量工作涉及硬件学习。本文描述了在SEAs摆系统硬件上使用Proximal Policy Optimization（PPO）算法对DRL策略进行训练过程，以跟踪力控制轨迹从0.05 ~ 0.35 Hz，振幅为50 N。开发和使用安全机制来训练策略12小时(过夜)而无需操作员的存在。

    Many state-of-the art robotic applications utilize series elastic actuators (SEAs) with closed-loop force control to achieve complex tasks such as walking, lifting, and manipulation. Model-free PID control methods are more prone to instability due to nonlinearities in the SEA where cascaded model-based robust controllers can remove these effects to achieve stable force control. However, these model-based methods require detailed investigations to characterize the system accurately. Deep reinforcement learning (DRL) has proved to be an effective model-free method for continuous control tasks, where few works deal with hardware learning. This paper describes the training process of a DRL policy on hardware of an SEA pendulum system for tracking force control trajectories from 0.05 - 0.35 Hz at 50 N amplitude using the Proximal Policy Optimization (PPO) algorithm. Safety mechanisms are developed and utilized for training the policy for 12 hours (overnight) without an operator present with
    
[^71]: 通过生成未来视图图像语义以改善视觉与语言导航

    Improving Vision-and-Language Navigation by Generating Future-View Image Semantics. (arXiv:2304.04907v1 [cs.CV])

    [http://arxiv.org/abs/2304.04907](http://arxiv.org/abs/2304.04907)

    本文提出了三个代理任务用于在代理的领域内预训练，以帮助模型生成未来视图图像语义，从而在视觉与语言导航任务中提高性能。

    

    视觉与语言导航是一项任务，要求代理根据自然语言指令在环境中进行导航。在每个步骤中，代理通过从可导航位置集合中进行选择来选择下一步动作。在本文中，我们旨在进一步探索代理是否可以受益于在导航期间生成潜在未来视图。直观地说，人类根据自然语言指令和周围的视图会对未来的环境有一个预期，并帮助正确地导航。因此，为了给代理装备这种生成未来导航视图语义的能力，我们首先在代理的领域内预训练过程中提出了三种代理任务: 掩蔽全景建模 (MPM)，掩蔽轨迹建模 (MTM) 和带有图像生成的动作预测 (APIG)。这三个目标教会了模型预测全景中的缺少视图 (MPM)、预测完整轨迹中的缺少步骤 (MTM) 和进行动作预测和图像生成 (APIG)。

    Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full tra
    
[^72]: 关于利用不确定性估计实现可信深度神经网络的调查：拒绝选项和训练后处理的案例研究

    Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing. (arXiv:2304.04906v1 [cs.LG])

    [http://arxiv.org/abs/2304.04906](http://arxiv.org/abs/2304.04906)

    本文综合论述了文献中提出的带有拒绝选项的预测方法在各种神经网络中的应用，讨论了不同的损失函数以及网络输出的训练后处理，最终探讨了拒绝选项在减少实时预测时间方面的应用。

    

    尽管神经网络（尤其是深度神经网络）在许多领域已经取得了优于人类的表现，由于对其知识限制的缺乏认识，它们的实际部署仍然受到质疑。为在机器学习模型中纳入这种认识，文献中提出了带有拒绝选项的预测方法（也称为选择性分类或带有放弃机制的分类）。本文系统地综述了拒绝选项在各种神经网络中的应用。据我们所知，这是第一项专注于这一方面的研究。此外，我们讨论了与拒绝选项相关的不同新型损失函数和网络输出的训练后处理（如果有的话）以生成适当的模型知识认知度量。最后，我们探讨了拒绝选项在减少实时预测时间方面的应用。

    Although neural networks (especially deep neural networks) have achieved \textit{better-than-human} performance in many fields, their real-world deployment is still questionable due to the lack of awareness about the limitation in their knowledge. To incorporate such awareness in the machine learning model, prediction with reject option (also known as selective classification or classification with abstention) has been proposed in literature. In this paper, we present a systematic review of the prediction with the reject option in the context of various neural networks. To the best of our knowledge, this is the first study focusing on this aspect of neural networks. Moreover, we discuss different novel loss functions related to the reject option and post-training processing (if any) of network output for generating suitable measurements for knowledge awareness of the model. Finally, we address the application of the rejection option in reducing the prediction time for the real-time pro
    
[^73]: 神经网络预测纳米限制下离子浓度分布

    Neural Network Predicts Ion Concentration Profiles under Nanoconfinement. (arXiv:2304.04896v1 [cs.LG])

    [http://arxiv.org/abs/2304.04896](http://arxiv.org/abs/2304.04896)

    本论文提出了用神经网络预测纳米通道中离子浓度分布的方法，可以作为MD模拟的快速替代模型，比XGBoost有更高的准确性并且具有灵活性。

    

    纳米通道中的离子浓度分布模拟在理解电双层和电渗流中扮演重要角色。分子动力学模拟由于表面相互作用和离散溶剂分子效应，经常被用作研究在纳米限制下离子的行为的重要工具，然而计算开销很大。本文提出了用神经网络预测纳米通道中不同构型（包括通道宽度，离子浓度和离子种类等）中离子浓度分布。通过将离子浓度分布建模为概率分布，我们的方法可以作为模拟逼真的MD模拟的快速替代模型。我们进一步展示了神经网络相对于XGBoost的优越预测准确性。最后，我们还展示了神经网络在不同精度下预测离子浓度分布的灵活性。

    Modeling the ion concentration profile in nanochannel plays an important role in understanding the electrical double layer and electroosmotic flow. Due to the non-negligible surface interaction and the effect of discrete solvent molecules, molecular dynamics (MD) simulation is often used as an essential tool to study the behavior of ions under nanoconfinement. Despite the accuracy of MD simulation in modeling nanoconfinement systems, it is computationally expensive. In this work, we propose neural network to predict ion concentration profiles in nanochannels with different configurations, including channel widths, ion molarity, and ion types. By modeling the ion concentration profile as a probability distribution, our neural network can serve as a much faster surrogate model for MD simulation with high accuracy. We further demonstrate the superior prediction accuracy of neural network over XGBoost. Lastly, we demonstrated that neural network is flexible in predicting ion concentration 
    
[^74]: ImageCaptioner$^2$: 针对图像字幕偏差放大评估的图像字幕生成器

    ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment. (arXiv:2304.04874v1 [cs.CV])

    [http://arxiv.org/abs/2304.04874](http://arxiv.org/abs/2304.04874)

    本文提出了一种新的图像字幕生成器 ImageCaptioner$^2$ ，用于针对图像字幕偏差放大进行评估。

    

    大多数预训练学习系统都会受到偏差的影响，这通常来自数据、模型或两者。衡量和量化偏差及其来源是一项具有挑战性的任务，并在图像字幕生成方面得到了广泛的研究。然而，我们观察到现有评估指标在包括视觉信号方面存在一定不一致性。本文提出了一种新的针对图像字幕生成的偏差评估指标，称为 ImageCaptioner$^2$。与现有方法仅基于生成的字幕评估图像字幕算法不同，ImageCaptioner$^2$在测量偏差时考虑图像。我们还设计了一种公式来作为基于提示的图像字幕生成来测量生成字幕的偏差，而不是使用传统方法。

    Most pre-trained learning systems are known to suffer from bias, which typically emerges from the data, the model, or both. Measuring and quantifying bias and its sources is a challenging task and has been extensively studied in image captioning. Despite the significant effort in this direction, we observed that existing metrics lack consistency in the inclusion of the visual signal. In this paper, we introduce a new bias assessment metric, dubbed $ImageCaptioner^2$, for image captioning. Instead of measuring the absolute bias in the model or the data, $ImageCaptioner^2$ pay more attention to the bias introduced by the model w.r.t the data bias, termed bias amplification. Unlike the existing methods, which only evaluate the image captioning algorithms based on the generated captions only, $ImageCaptioner^2$ incorporates the image while measuring the bias. In addition, we design a formulation for measuring the bias of generated captions as prompt-based image captioning instead of using 
    
[^75]: DASS Good: 解释性的空间队列数据挖掘方法

    DASS Good: Explainable Data Mining of Spatial Cohort Data. (arXiv:2304.04870v1 [cs.HC])

    [http://arxiv.org/abs/2304.04870](http://arxiv.org/abs/2304.04870)

    该论文描述了一个名为DASS的协同设计建模系统，结合人-机循环视觉引导、空间数据和可解释的AI，自动数据挖掘增加领域知识，在头颈癌患者的长期毒副作用预测模型开发中具有实际应用及领域专家反馈，为解决临床机器学习模型在数据包括空间信息时的困难提供了一种新兴方法。

    

    当数据包括空间信息时，例如邻近的有风险器官周围的辐射剂量分布，开发适用的临床机器学习模型是一项艰巨的任务。我们描述了一个建模系统DASS的协同设计，以支持人机混合开发和验证预测模型，用于估计头颈癌患者放疗剂量相关的长期毒副作用。与肿瘤学和数据挖掘领域专家合作开发的DASS，结合人-机循环视觉引导、空间数据和可解释的AI，以自动数据挖掘增加领域知识。我们使用两个实用的临床分层模型展示了DASS，并报告了领域专家的反馈。最后，我们描述了从这次协作经验中学到的设计经验。

    Developing applicable clinical machine learning models is a difficult task when the data includes spatial information, for example, radiation dose distributions across adjacent organs at risk. We describe the co-design of a modeling system, DASS, to support the hybrid human-machine development and validation of predictive models for estimating long-term toxicities related to radiotherapy doses in head and neck cancer patients. Developed in collaboration with domain experts in oncology and data mining, DASS incorporates human-in-the-loop visual steering, spatial data, and explainable AI to augment domain knowledge with automatic data mining. We demonstrate DASS with the development of two practical clinical stratification models and report feedback from domain experts. Finally, we describe the design lessons learned from this collaborative experience.
    
[^76]: 一种用于提高低保真数据准确性的少样本图拉普拉斯方法

    A few-shot graph Laplacian-based approach for improving the accuracy of low-fidelity data. (arXiv:2304.04862v1 [cs.LG])

    [http://arxiv.org/abs/2304.04862](http://arxiv.org/abs/2304.04862)

    本文提出的方法利用少量高保真数据来提高大量低保真数据的准确性，方法包括构建图拉普拉斯和计算其低能量谱来将数据聚类，并获取关键点的高保真数据进行转换。

    

    低保真数据通常产生成本低廉，但不准确。相反，高保真数据准确但获取成本昂贵。多保真度方法使用一小组高保真数据来增强大量低保真数据的准确性。本文所描述的方法是通过使用低保真数据构建图拉普拉斯并计算其低能量谱来实现的。然后，利用此谱来对数据进行聚类并识别与聚类重心最近的数据点。随后，为这些关键点获取高保真数据。然后通过最小化关键点处的双保真数据与高保真数据之间的差异，并保持低保真数据分布的基础结构来确定将每个低保真数据点映射到其双保真数据对应物的转换。后一个目标再次是通过依赖图拉普拉斯的谱特性来实现的。

    Low-fidelity data is typically inexpensive to generate but inaccurate. On the other hand, high-fidelity data is accurate but expensive to obtain. Multi-fidelity methods use a small set of high-fidelity data to enhance the accuracy of a large set of low-fidelity data. In the approach described in this paper, this is accomplished by constructing a graph Laplacian using the low-fidelity data and computing its low-lying spectrum. This spectrum is then used to cluster the data and identify points that are closest to the centroids of the clusters. High-fidelity data is then acquired for these key points. Thereafter, a transformation that maps every low-fidelity data point to its bi-fidelity counterpart is determined by minimizing the discrepancy between the bi- and high-fidelity data at the key points, and to preserve the underlying structure of the low-fidelity data distribution. The latter objective is achieved by relying, once again, on the spectral properties of the graph Laplacian. This
    
[^77]: ShapeShift：基于超椭球形状的机器人抓取物体姿态估计

    ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping. (arXiv:2304.04861v1 [cs.CV])

    [http://arxiv.org/abs/2304.04861](http://arxiv.org/abs/2304.04861)

    ShapeShift是一个机器人抓取物体姿态估计的框架，它使用超椭球的参考形状预测物体的姿态，具有广泛的泛化能力和准确性。

    

    物体姿态估计是精确物体操纵中至关重要的任务。然而，当前技术严重依赖于参考3D物体，限制了它们的泛化能力，并使扩展到新的物体类别变得昂贵。直接姿态预测也不能提供关于机器人抓取的充分信息而不参考3D模型。基于关键点的方法提供了内在的描述性能力而不依赖于准确的3D模型，但它们可能缺乏一致性和准确性。针对这些挑战，本文提出了一个基于超椭球形状的ShapeShift框架，用于物体姿态估计，该框架预测与适合于物体的原始形状相对的物体姿态。该框架提供了内在的描述能力，并具有超出训练集的任意几何形状的概括能力。

    Object pose estimation is a critical task in robotics for precise object manipulation. However, current techniques heavily rely on a reference 3D object, limiting their generalizability and making it expensive to expand to new object categories. Direct pose predictions also provide limited information for robotic grasping without referencing the 3D model. Keypoint-based methods offer intrinsic descriptiveness without relying on an exact 3D model, but they may lack consistency and accuracy. To address these challenges, this paper proposes ShapeShift, a superquadric-based framework for object pose estimation that predicts the object's pose relative to a primitive shape which is fitted to the object. The proposed framework offers intrinsic descriptiveness and the ability to generalize to arbitrary geometric shapes beyond the training set.
    
[^78]: 在早期层中使用模拟退火可提高泛化性能

    Simulated Annealing in Early Layers Leads to Better Generalization. (arXiv:2304.04858v1 [cs.LG])

    [http://arxiv.org/abs/2304.04858](http://arxiv.org/abs/2304.04858)

    使用模拟退火在早期层次的网络中可以提高泛化性能，并且在Tiny-ImageNet数据集基准测试和一系列传递学习和少量样本学习任务上表现得比LLF更好。

    

    最近，引入了一些迭代学习方法来改进泛化性能。这些方法通常依靠更长时间的训练来换取更好的泛化性能。LLF是这一类方法中最先进的方法。它通过定期重新初始化网络的最后几层来增强早期层次的学习。我们在这项工作中的主要创新是在网络的早期层中使用模拟退火(SEAL)代替后面层次的重新初始化。实质上，后面的层通过正常的梯度下降过程，而早期层通过短暂的梯度上升后跟着梯度下降。在流行的Tiny-ImageNet数据集基准测试以及一系列传递学习和少量样本学习任务上进行了广泛的实验，结果显示SEAL比LLF的表现要好得多。我们进一步表明，与正常训练相比，虽然LLF特性能提高目标任务的性能，但会降低原始任务的性能。另一方面，SEAL不仅可提高目标任务的性能，还可改善原始任务的性能。

    Recently, a number of iterative learning methods have been introduced to improve generalization. These typically rely on training for longer periods of time in exchange for improved generalization. LLF (later-layer-forgetting) is a state-of-the-art method in this category. It strengthens learning in early layers by periodically re-initializing the last few layers of the network. Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers. Essentially, later layers go through the normal gradient descent process, while the early layers go through short stints of gradient ascent followed by gradient descent. Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin. We further show that, compared to normal training, LLF features, although improving on the target task, degrade
    
[^79]: iPINNs：物理信息神经网络的增量学习

    iPINNs: Incremental learning for Physics-informed neural networks. (arXiv:2304.04854v1 [cs.LG])

    [http://arxiv.org/abs/2304.04854](http://arxiv.org/abs/2304.04854)

    提出了一种增量学习方法，可以顺序学习多个物理方程，而无需为新任务添加额外参数，在多个测试问题上展现了较高准确性和更快的收敛速度。

    

    物理信息神经网络（PINNs）最近成为解决偏微分方程（PDE）的强大工具。但由于需要遍历复杂的损失函数空间，找到一组满足PDE的神经网络参数可能具有挑战性和不唯一性。虽然已提出了各种多任务学习和转移学习方法来克服这些问题，但目前还没有针对PINNs的增量培训程序，它可以有效地减轻这些培训挑战。我们提出了增量PINNs（iPINNs），它可以顺序学习多个任务（方程），无需为新任务添加额外的参数，并改进序列中每个方程的性能。我们的方法从最简单的PDE开始学习多个PDE，为每个PDE创建自己的子网络，并允许每个子网络与之前学习的子网络重叠。我们证明了之前的子网络是新任务网络的良好初始化，从而提高了训练速度和精度。此外，我们的方法还提供了一种自适应学习的方法，以控制每个子网络的复杂性。在多个测试问题上进行的实验表明，iPINNs比其他PINN方法具有更高的准确性和更快的收敛速度。

    Physics-informed neural networks (PINNs) have recently become a powerful tool for solving partial differential equations (PDEs). However, finding a set of neural network parameters that lead to fulfilling a PDE can be challenging and non-unique due to the complexity of the loss landscape that needs to be traversed. Although a variety of multi-task learning and transfer learning approaches have been proposed to overcome these issues, there is no incremental training procedure for PINNs that can effectively mitigate such training challenges. We propose incremental PINNs (iPINNs) that can learn multiple tasks (equations) sequentially without additional parameters for new tasks and improve performance for every equation in the sequence. Our approach learns multiple PDEs starting from the simplest one by creating its own subnetwork for each PDE and allowing each subnetwork to overlap with previously learned subnetworks. We demonstrate that previous subnetworks are a good initialization for 
    
[^80]: 使用MicroTVM将机器学习模型部署到边缘Ahead-of-Time运行

    Deploying Machine Learning Models to Ahead-of-Time Runtime on Edge Using MicroTVM. (arXiv:2304.04842v1 [cs.LG])

    [http://arxiv.org/abs/2304.04842](http://arxiv.org/abs/2304.04842)

    本文介绍了使用MicroTVM在边缘设备上部署机器学习模型的方法，可以将预训练模型解析为后端的C源代码库，并使用自动生成的Ahead-of-Time C运行时在ARM Cortex M4F核心上进行手势识别实验。

    

    近年来，越来越多的AI应用程序已经应用到边缘设备上。然而，由数据科学家使用机器学习框架（如PyTorch或TensorFlow）训练的模型无法无缝地在边缘上执行。在本文中，我们开发了一个端到端的代码生成器，使用MicroTVM将预训练模型解析为后端的C源代码库，MicroTVM是一种机器学习编译器框架扩展，用于处理裸机设备上的推断。分析表明，特定的计算密集型运算符可以轻松地通过通用模块加速器（UMA）接口卸载到专用加速器上，而其他运算符则在CPU核心中处理。通过使用自动生成的Ahead-of-Time C运行时，在ARM Cortex M4F核心上进行手势识别实验。

    In the past few years, more and more AI applications have been applied to edge devices. However, models trained by data scientists with machine learning frameworks, such as PyTorch or TensorFlow, can not be seamlessly executed on edge. In this paper, we develop an end-to-end code generator parsing a pre-trained model to C source libraries for the backend using MicroTVM, a machine learning compiler framework extension addressing inference on bare metal devices. An analysis shows that specific compute-intensive operators can be easily offloaded to the dedicated accelerator with a Universal Modular Accelerator (UMA) interface, while others are processed in the CPU cores. By using the automatically generated ahead-of-time C runtime, we conduct a hand gesture recognition experiment on an ARM Cortex M4F core.
    
[^81]: MHfit：使用机器学习预测运动员健康状况的移动健康数据

    MHfit: Mobile Health Data for Predicting Athletics Fitness Using Machine Learning. (arXiv:2304.04839v1 [cs.LG])

    [http://arxiv.org/abs/2304.04839](http://arxiv.org/abs/2304.04839)

    本文提出了利用移动健康数据来比较多种机器学习算法，预测人体健康行为和健身情况的方法；结果表明，XGBoost算法表现优于其他算法。

    

    移动电话和其他电子设备已经帮助人们在不需要进行数据输入的情况下收集数据。本文将特别关注移动健康数据。移动健康数据使用移动设备实时收集临床健康数据并跟踪患者生命体征。我们的研究旨在使用从移动设备和患者身上的传感器收集的数据来比较多种机器学习算法，以预测人类行为和健康，并为小型或大型运动队提供关于某个运动员是否适合参与特定比赛的决策。本研究从一项类似的移动健康研究中获得了包含来自不同背景的10名志愿者的生命体征记录的数据集。他们必须在身体上放置传感器并进行多项体力活动。我们使用了5种机器学习算法（XGBoost，朴素贝叶斯，决策树，随机森林和逻辑回归）来分析和预测人类健康行为和健身情况。我们的研究结果表明，XGBoost算法表现优于其他算法。

    Mobile phones and other electronic gadgets or devices have aided in collecting data without the need for data entry. This paper will specifically focus on Mobile health data. Mobile health data use mobile devices to gather clinical health data and track patient vitals in real-time. Our study is aimed to give decisions for small or big sports teams on whether one athlete good fit or not for a particular game with the compare several machine learning algorithms to predict human behavior and health using the data collected from mobile devices and sensors placed on patients. In this study, we have obtained the dataset from a similar study done on mhealth. The dataset contains vital signs recordings of ten volunteers from different backgrounds. They had to perform several physical activities with a sensor placed on their bodies. Our study used 5 machine learning algorithms (XGBoost, Naive Bayes, Decision Tree, Random Forest, and Logistic Regression) to analyze and predict human health behav
    
[^82]: 格上的序模式

    Ordinal Motifs in Lattices. (arXiv:2304.04827v1 [cs.AI])

    [http://arxiv.org/abs/2304.04827](http://arxiv.org/abs/2304.04827)

    本研究提出了“序模式”作为分析意义的单位，并研究了在格上通过正式背景的全尺度测量来识别这些序子结构的方法，以实现从中等尺寸序数数据集中检索基本含义。

    

    格是表示和分析关系和本体知识的常用结构。本研究提出了“序模式”作为分析意义的单位，研究这些序子结构（或标准尺度）通过来自正式概念分析领域的正式背景的全尺度测量。我们展示了底层决策问题是NP完全的，并提供了如何逐步识别序模式以节省计算量的结果。伴随我们的理论结果，我们展示了如何利用序模式从中等尺寸的序数数据集中检索基本含义。

    Lattices are a commonly used structure for the representation and analysis of relational and ontological knowledge. In particular, the analysis of these requires a decomposition of a large and high-dimensional lattice into a set of understandably large parts. With the present work we propose /ordinal motifs/ as analytical units of meaning. We study these ordinal substructures (or standard scales) through (full) scale-measures of formal contexts from the field of formal concept analysis. We show that the underlying decision problems are NP-complete and provide results on how one can incrementally identify ordinal motifs to save computational effort. Accompanying our theoretical results, we demonstrate how ordinal motifs can be leveraged to retrieve basic meaning from a medium sized ordinal data set.
    
[^83]: 基于梯度的不确定性归因于可解释的贝叶斯深度学习

    Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning. (arXiv:2304.04824v1 [cs.LG])

    [http://arxiv.org/abs/2304.04824](http://arxiv.org/abs/2304.04824)

    本论文提出了一个基于梯度的不确定性归因方法来确定导致预测的不确定性的最具问题性的输入区域，从而实现可解释和可操作的贝叶斯深度学习。

    

    深度学习模型所作出的预测容易受到数据扰动、对抗攻击和超出分布范围的输入的影响。为了构建一个可信赖的AI系统，准确量化预测的不确定性至关重要。虽然当前的工作重点是提高不确定性量化的准确性和效率，但有必要确定不确定性源并采取措施减轻对预测的影响。因此，我们提出了开发可解释和可操作的贝叶斯深度学习方法来不仅进行准确的不确定性量化，而且解释不确定性、确定其来源并提出减轻不确定性影响的策略。具体而言，我们引入了一种基于梯度的不确定性归因方法来确定最具问题性的输入区域，从而导致预测的不确定性。与现有方法相比，所提出的UA-Backprop方法具有竞争性的准确性、松弛的假设和高效性。

    Predictions made by deep learning models are prone to data perturbations, adversarial attacks, and out-of-distribution inputs. To build a trusted AI system, it is therefore critical to accurately quantify the prediction uncertainties. While current efforts focus on improving uncertainty quantification accuracy and efficiency, there is a need to identify uncertainty sources and take actions to mitigate their effects on predictions. Therefore, we propose to develop explainable and actionable Bayesian deep learning methods to not only perform accurate uncertainty quantification but also explain the uncertainties, identify their sources, and propose strategies to mitigate the uncertainty impacts. Specifically, we introduce a gradient-based uncertainty attribution method to identify the most problematic regions of the input that contribute to the prediction uncertainty. Compared to existing methods, the proposed UA-Backprop has competitive accuracy, relaxed assumptions, and high efficiency.
    
[^84]: 网络犯罪预测的进展：机器学习、深度学习、迁移学习和自适应学习技术综述

    Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques. (arXiv:2304.04819v1 [cs.LG])

    [http://arxiv.org/abs/2304.04819](http://arxiv.org/abs/2304.04819)

    本文从突破安全系统和窃取敏感数据的角度出发，介绍了机器学习、深度学习和迁移学习技术在预测和防止网络犯罪方面的最新进展，提到了最近研究中效果最好的递归和卷积神经网络。

    

    网络犯罪是一种不断增长的威胁，罪犯使用越来越复杂的技术来突破安全系统并窃取敏感数据。近年来，机器学习、深度学习和迁移学习技术已经成为预测网络犯罪和在其发生之前防止的有前途的工具。本文旨在提供使用上述技术预测网络犯罪的最新进展的全面调查，重点介绍每种方法相关的最新研究。为此，我们回顾了150多篇研究文章，并讨论了大约50篇最近和最相关的研究文章。我们首先讨论了一些网络犯罪常用的方法，然后重点介绍了最新的机器学习技术和深度学习技术，例如递归和卷积神经网络，这些技术在检测异常行为和识别潜在威胁方面非常有效。

    Cybercrime is a growing threat to organizations and individuals worldwide, with criminals using increasingly sophisticated techniques to breach security systems and steal sensitive data. In recent years, machine learning, deep learning, and transfer learning techniques have emerged as promising tools for predicting cybercrime and preventing it before it occurs. This paper aims to provide a comprehensive survey of the latest advancements in cybercrime prediction using above mentioned techniques, highlighting the latest research related to each approach. For this purpose, we reviewed more than 150 research articles and discussed around 50 most recent and relevant research articles. We start the review by discussing some common methods used by cyber criminals and then focus on the latest machine learning techniques and deep learning techniques, such as recurrent and convolutional neural networks, which were effective in detecting anomalous behavior and identifying potential threats. We al
    
[^85]: 使用基于CNN的模型进行CT扫描图像的肺癌诊断

    LCDctCNN: Lung Cancer Diagnosis of CT scan Images Using CNN Based Model. (arXiv:2304.04814v1 [eess.IV])

    [http://arxiv.org/abs/2304.04814](http://arxiv.org/abs/2304.04814)

    本研究提出了一种使用基于CNN的模型进行CT扫描图像的肺癌诊断的方法，经过与其他模型的比较，CNN得到了最优的性能表现。

    

    肺癌是世界上最致命和危机的疾病之一，早期诊断和精确治疗是降低肺癌死亡率的必要条件。基于计算机断层扫描(CT)的图像是利用深度学习模型进行肺癌检测的最有效方法之一。本文提出了一种基于深度学习模型的卷积神经网络(CNN)框架，用于使用CT扫描图像早期检测肺癌。我们还分析了其他模型，例如Inception V3、Xception和ResNet-50模型，以与我们的模型进行比较。我们根据准确率、曲线下面积(AUC)、召回率和损失等指标对模型进行了比较。在评估模型性能后，我们发现CNN的性能优于其他模型，相比传统方法表现出了极大的潜力。它的准确率达到了92%、AUC为98.21%、召回率为91.72%、损失为0.328。

    The most deadly and life-threatening disease in the world is lung cancer. Though early diagnosis and accurate treatment are necessary for lowering the lung cancer mortality rate. A computerized tomography (CT) scan-based image is one of the most effective imaging techniques for lung cancer detection using deep learning models. In this article, we proposed a deep learning model-based Convolutional Neural Network (CNN) framework for the early detection of lung cancer using CT scan images. We also have analyzed other models for instance Inception V3, Xception, and ResNet-50 models to compare with our proposed model. We compared our models with each other considering the metrics of accuracy, Area Under Curve (AUC), recall, and loss. After evaluating the model's performance, we observed that CNN outperformed other models and has been shown to be promising compared to traditional methods. It achieved an accuracy of 92%, AUC of 98.21%, recall of 91.72%, and loss of 0.328.
    
[^86]: Scallop: 一种神经符号编程语言

    Scallop: A Language for Neurosymbolic Programming. (arXiv:2304.04812v1 [cs.PL])

    [http://arxiv.org/abs/2304.04812](http://arxiv.org/abs/2304.04812)

    Scallop是一种能够同时利用深度学习和逻辑推理优点的神经符号编程语言，它能够以数据和计算有效的方式训练神经符号应用程序，通过它可在AI任务中表达算法推理并融合逻辑领域知识，其解决方案可与最先进的模型相媲美或更高。

    

    我们提出了 Scallop，这是一种结合了深度学习和逻辑推理优点的语言。通过三个关键特性，Scallop 启用用户编写广泛的神经符号应用程序并以数据和计算有效的方式训练它们。这三个关键特性包括：1）基于关系数据模型的灵活符号表示；2）基于 Datalog 的声明性逻辑编程语言，支持递归、聚合和否定；3）基于证明半环理论的自动高效可微推理框架。我们在文献中的八种神经符号应用程序套件上评估 Scallop。我们的评估表明，Scallop 能够在多样化和具有挑战性的 AI 任务中表达算法推理，为机器学习程序员提供简洁的接口以融合逻辑领域知识，并提供可与最先进的模型相媲美或更高的解决方案。

    We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art mode
    
[^87]: 在恒星变异存在下，基于深度学习的行星径向速度测量方法

    Deep-learning based measurement of planetary radial velocities in the presence of stellar variability. (arXiv:2304.04807v1 [astro-ph.EP])

    [http://arxiv.org/abs/2304.04807](http://arxiv.org/abs/2304.04807)

    本文提出了一种基于深度学习的方法，使用神经网络来减少三年HARPS-N太阳-星形光谱中的恒星径向速度抖动。该方法能够以前无法想象的小行星径向速度检测精度，为缓解恒星径向速度变异提供了希望。

    

    本文提出了一种基于深度学习的方法，用于在恒星变异存在下测量小行星的径向速度。我们使用神经网络来减少三年HARPS-N太阳-星形光谱中的恒星径向速度抖动。我们开发并比较了不同的降维和数据分割方法，以及各种神经网络体系结构，包括单线CNN、单线CNN集合和多线CNN。我们将类似于行星的径向速度注入光谱中并使用网络恢复它们。我们发现，多线CNN能够恢复0.2m/s半振幅、50天周期的行星，其振幅误差为8.8％，周期误差为0.7％。这种方法展示了在缓解恒星径向速度变异的同时，实现了以前无法想象的小行星径向速度检测精度的承诺。

    We present a deep-learning based approach for measuring small planetary radial velocities in the presence of stellar variability. We use neural networks to reduce stellar RV jitter in three years of HARPS-N sun-as-a-star spectra. We develop and compare dimensionality-reduction and data splitting methods, as well as various neural network architectures including single line CNNs, an ensemble of single line CNNs, and a multi-line CNN. We inject planet-like RVs into the spectra and use the network to recover them. We find that the multi-line CNN is able to recover planets with 0.2 m/s semi-amplitude, 50 day period, with 8.8% error in the amplitude and 0.7% in the period. This approach shows promise for mitigating stellar RV variability and enabling the detection of small planetary RVs with unprecedented precision.
    
[^88]: RAPID: 在动态公共云环境中实现快速在线策略学习

    RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments. (arXiv:2304.04797v1 [cs.LG])

    [http://arxiv.org/abs/2304.04797](http://arxiv.org/abs/2304.04797)

    提出了在动态公共云环境中实现快速在线策略学习的框架RAPID，通过领域知识启发的技术实现样本效率和偏差减少，学习并实时调整资源分配策略，能有效解决资源共享中的问题。

    

    多个工作负载之间的资源共享已成为云服务提供商之间的一种突出实践，这是由需求改进资源利用率和降低拥有成本所驱动的。然而，由于资源争用可能会对具有严格服务质量 (QoS) 要求的优先级高、面向用户的负载产生不利影响，因此有效的资源共享仍然是一个开放的挑战。虽然最近的方法已经展示了有希望的结果，但这些工作在公共云环境中仍然很难实践，因为负载事先是未知的，可能仅运行短暂的时间，从而禁止脱机学习，并且显著阻碍在线学习。在本文中，我们提出 RAPID，这是一种新颖的框架，用于在高度动态的操作环境中实现快速、完全在线的资源分配策略学习。RAPID 利用轻量级 QoS 预测，通过领域知识启发的技术实现样本效率和偏差减少，以分离争用检测和分配策略。

    Resource sharing between multiple workloads has become a prominent practice among cloud service providers, motivated by demand for improved resource utilization and reduced cost of ownership. Effective resource sharing, however, remains an open challenge due to the adverse effects that resource contention can have on high-priority, user-facing workloads with strict Quality of Service (QoS) requirements. Although recent approaches have demonstrated promising results, those works remain largely impractical in public cloud environments since workloads are not known in advance and may only run for a brief period, thus prohibiting offline learning and significantly hindering online learning. In this paper, we propose RAPID, a novel framework for fast, fully-online resource allocation policy learning in highly dynamic operating environments. RAPID leverages lightweight QoS predictions, enabled by domain-knowledge-inspired techniques for sample efficiency and bias reduction, to decouple contr
    
[^89]: 在在线评估下重新审视测试时间适应

    Revisiting Test Time Adaptation under Online Evaluation. (arXiv:2304.04795v1 [cs.LG])

    [http://arxiv.org/abs/2304.04795](http://arxiv.org/abs/2304.04795)

    本文提出了一种新颖的在线评估协议，该协议通过为较慢的方法提供更少的样本来惩罚它们，以更加现实的方式评估了测试时间适应（TTA）方法。广泛实验表明，当考虑推断速度时，简单快速的方法可以优于更复杂但较慢的方法。

    

    本文提出了一种新颖的在线评估协议，用于测试时间适应（TTA）方法，通过为较慢的方法提供更少的样本来惩罚它们。TTA方法利用测试时间的未标记数据来适应分布移位。虽然已经提出了许多有效的方法，但它们惊人的性能通常要以显着增加的计算预算为代价。当前的评估协议忽略了这种额外计算成本的影响，影响它们在实际中的适用性。为了解决这个问题，我们提出了一种更加现实的TTA方法评估协议，在这个协议中数据以恒定速率从数据流中在线接收，从而考虑到方法的适应速度。我们将我们提出的协议应用于多个数据集和场景中对多种TTA方法进行基准测试。广泛的实验表明，在考虑推断速度时，简单快速的方法可以优于更复杂但较慢的方法。

    This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods, which penalizes slower methods by providing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Though many effective methods have been proposed, their impressive performance usually comes at the cost of significantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA methods, where data is received in an online fashion from a constant-speed data stream, thereby accounting for the method's adaptation speed. We apply our proposed protocol to benchmark several TTA methods on multiple datasets and scenarios. Extensive experiments shows that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower
    
[^90]: 深度神经网络中的临界性与均匀性比较

    Criticality versus uniformity in deep neural networks. (arXiv:2304.04784v1 [cs.LG])

    [http://arxiv.org/abs/2304.04784](http://arxiv.org/abs/2304.04784)

    本文研究了深度神经网络在混沌边缘初始化时的训练能力，发现饱和的激活函数会妨碍训练效率。结果表明，沿混沌边缘初始化只是获得最佳可训练性所必需但不充分的条件。

    

    沿着混沌边缘初始化的深层前馈网络表现出指数级优越的训练能力，其最大可训练深度可以量化。本文探讨了沿混沌边缘饱和tanh激活函数的影响。具体而言，我们确定了相空间中最大熵的后激活分布的均匀性线。该线交叉于混沌边缘，并指示了超过激活函数饱和开始妨碍训练效率的区域。我们的结果表明，沿混沌边缘初始化是获得最佳可训练性所必需但不充分的条件。

    Deep feedforward networks initialized along the edge of chaos exhibit exponentially superior training ability as quantified by maximum trainable depth. In this work, we explore the effect of saturation of the tanh activation function along the edge of chaos. In particular, we determine the line of uniformity in phase space along which the post-activation distribution has maximum entropy. This line intersects the edge of chaos, and indicates the regime beyond which saturation of the activation function begins to impede training efficiency. Our results suggest that initialization along the edge of chaos is a necessary but not sufficient condition for optimal trainability.
    
[^91]: 通过潜在意图从被动数据中进行强化学习

    Reinforcement Learning from Passive Data via Latent Intentions. (arXiv:2304.04782v1 [cs.LG])

    [http://arxiv.org/abs/2304.04782](http://arxiv.org/abs/2304.04782)

    本文提出了一种基于意图建模的强化学习方法，可以从被动数据中学习特征，并用于下游任务的价值预测。

    

    被动观察数据丰富而富有信息，然而当前强化学习方法很少能够利用该数据。本文提出了一种通过建模意图从被动数据中进行学习的方法，该方法通过衡量当智能体为实现特定任务而采取行动时未来结果的可能性如何变化来学习意图。我们提出了一个时差学习目标来学习意图，得到了一个类似于传统强化学习的算法，但是完全是从被动数据中学习得到的。通过优化该目标，我们的智能体可以同时从原始的观察数据中学习出状态、策略和环境下的可能结果。从理论和实验上看，该方法学习出的特征可用于下游任务的价值预测。

    Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from m
    
[^92]: 一种自编码器压缩方法加速大规模逆问题求解

    An autoencoder compression approach for accelerating large-scale inverse problems. (arXiv:2304.04781v1 [math.NA])

    [http://arxiv.org/abs/2304.04781](http://arxiv.org/abs/2304.04781)

    通过自编码器压缩策略来加速计算PDE约束的逆问题，降低了内存瓶颈的影响。

    

    PDE约束的逆问题是当今计算科学中最具挑战性和计算密集型的问题之一。为了精确计算PDE解，需要细微的网格，这引入了大量参数，并需要大规模的计算资源，如更多的处理器和更多的内存来在合理的时间内解决这些系统。为了高效地计算梯度和高阶导数，通常采用伴随方法来限制由时间演化的PDE约束的逆问题，该方法要求在每个时间步骤上求解一个时间反演的伴随PDE，该伴随PDE依赖于前向PDE解。这需要在每个时间步长上存储一个高维度的前向解向量。这种过程快速耗尽了可用的内存资源。已经提出了几种手段，如检查点和压缩策略，以降低内存瓶颈的影响并且换取了额外的计算。

    PDE-constrained inverse problems are some of the most challenging and computationally demanding problems in computational science today. Fine meshes that are required to accurately compute the PDE solution introduce an enormous number of parameters and require large scale computing resources such as more processors and more memory to solve such systems in a reasonable time. For inverse problems constrained by time dependent PDEs, the adjoint method that is often employed to efficiently compute gradients and higher order derivatives requires solving a time-reversed, so-called adjoint PDE that depends on the forward PDE solution at each timestep. This necessitates the storage of a high dimensional forward solution vector at every timestep. Such a procedure quickly exhausts the available memory resources. Several approaches that trade additional computation for reduced memory footprint have been proposed to mitigate the memory bottleneck, including checkpointing and compression strategies
    
[^93]: 关于可解释人工智能在医疗保健领域的综述：为什么、 如何和何时？

    A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?. (arXiv:2304.04780v1 [cs.LG])

    [http://arxiv.org/abs/2304.04780](http://arxiv.org/abs/2304.04780)

    本篇综述系统分析了可解释人工智能（XAI）在医疗保健领域的应用及其流行趋势，阐述了XAI的使用原因、如何使用以及何时使用及其影响，并给出了如何获得可信赖的AI的方法，对研究人员、临床医生和政策制定者有重要的指导作用。

    

    人工智能模型在医学领域中的应用越来越广泛。同时也出现了关于这些人工智能模型决策可解释性的担忧。本文系统分析了可解释人工智能（XAI），重点关注目前在医疗保健领域中使用的模型。按照系统性综述和元分析的首选报告项目（PRISMA）标准，检索了2012年1月1日至2022年2月2日期间发表的相关文章。本综述分析了XAI的流行趋势，并阐述了研究的主要方向。我们调查这些XAI模型的使用原因、如何使用以及何时使用以及其影响。我们对XAI方法进行了全面的研究，以及阐述了如何通过描述医疗领域的AI模型来获得可信赖的AI。对本文的讨论将有助于研究人员、临床医生和政策制定者深入了解XAI在医疗保健应用中的重要性。

    Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) standards for relevant work published from 1 January 2012 to 02 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will
    
[^94]: GraphMAE2：一种解码增强的自监督图学习方法

    GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner. (arXiv:2304.04779v1 [cs.LG])

    [http://arxiv.org/abs/2304.04779](http://arxiv.org/abs/2304.04779)

    GraphMAE2是一种掩码自监督学习框架，采用多视角随机重新屏蔽解码和潜在表示预测策略对图SSL中的特征重构进行正则化。

    

    图自监督学习（SSL）包括对比和生成方法，为解决现实世界中标签稀缺的图数据的根本难题提供了很大的潜力。在这两种图SSL技术中，掩码图自动编码器（例如GraphMAE）是最近取得有希望的结果的一种生成方法。其思想是使用自动编码器架构对从输入中随机屏蔽的节点特征（或结构）进行重构。然而，掩码特征重构的性能自然取决于输入特征的可辨别性，并且通常会对特征中的扰动产生影响。本文提出了一种掩码自监督学习框架GraphMAE2，旨在克服这个问题。想法是在图SSL中对特征重构施加正则化。具体而言，我们设计了多视角随机重新掩码解码和潜在表示预测的策略以实现正则化。

    Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)--one type of generative method--have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)--that are randomly masked from the input--with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to reg
    
[^95]: 带函数约束的随机变分不等式问题的一阶方法

    First-order methods for Stochastic Variational Inequality problems with Function Constraints. (arXiv:2304.04778v1 [math.OC])

    [http://arxiv.org/abs/2304.04778](http://arxiv.org/abs/2304.04778)

    本文提出了一种新的一阶方法，适用于具有随机算子和/或随机约束的带函数约束的变分不等式问题，当FCVI问题是确定性非光滑的或随机的时，这些方法可以实现最优算子或样本复杂性。

    

    在机器学习中，单调变分不等式是一个重要的问题。许多情况下，变分不等式问题伴随着可能是数据驱动的函数约束，这使得投影算子的计算变得具有挑战性。本文针对各种情况下的带函数约束的变分不等式问题，包括具有随机算子和/或随机约束的光滑或非光滑问题，提出了新的一阶方法。首先，我们介绍了{\texttt{OpConEx}}方法及其随机变体，它们采用算子和限制评估的外推来更新变量和Lagrangian乘数。当FCVI问题是确定性非光滑的或随机的（包括光滑或非光滑的随机约束）时，这些方法可以实现最优算子或样本复杂性。值得注意的是，我们的算法是简单的单循环程序，不需要知道拉格朗日乘数就可以达到最优性。

    The monotone Variational Inequality (VI) is an important problem in machine learning. In numerous instances, the VI problems are accompanied by function constraints which can possibly be data-driven, making the projection operator challenging to compute. In this paper, we present novel first-order methods for function constrained VI (FCVI) problem under various settings, including smooth or nonsmooth problems with a stochastic operator and/or stochastic constraints. First, we introduce the~{\texttt{OpConEx}} method and its stochastic variants, which employ extrapolation of the operator and constraint evaluations to update the variables and the Lagrangian multipliers. These methods achieve optimal operator or sample complexities when the FCVI problem is either (i) deterministic nonsmooth, or (ii) stochastic, including smooth or nonsmooth stochastic constraints. Notably, our algorithms are simple single-loop procedures and do not require the knowledge of Lagrange multipliers to attain th
    
[^96]: 将机器学习中的公平性与公共卫生平等联系起来

    Connecting Fairness in Machine Learning with Public Health Equity. (arXiv:2304.04761v1 [cs.LG])

    [http://arxiv.org/abs/2304.04761](http://arxiv.org/abs/2304.04761)

    这篇论文总结了机器学习公平方面的划时代文献，并提出了一个框架，用于识别和缓解数据和模型中的偏差，强调在公共卫生领域需要公平和公正的机器学习模型。

    

    机器学习已成为公共卫生中至关重要的工具，有望提高人口健康、诊断、治疗选择和卫生系统效率。然而，数据和模型设计中的偏见可能导致某些受保护群体的不平等，并放大现有的医疗保健不平等。为了解决这一挑战，本研究总结了机器学习公平方面的划时代文献，并提出了一个框架，用于识别和缓解数据和模型中的偏差。该框架提供了指导，可以将公平性纳入典型的机器学习流程的不同阶段，如数据处理、模型设计、部署和评估。为了说明数据中偏见对机器学习模型的影响，我们提供了例子，展示了系统性偏见如何通过模型预测被放大。这些案例展示了如何使用该框架来预防这些偏见，并强调了在公共卫生领域需要公平和公正的机器学习模型。

    Machine learning (ML) has become a critical tool in public health, offering the potential to improve population health, diagnosis, treatment selection, and health system efficiency. However, biases in data and model design can result in disparities for certain protected groups and amplify existing inequalities in healthcare. To address this challenge, this study summarizes seminal literature on ML fairness and presents a framework for identifying and mitigating biases in the data and model. The framework provides guidance on incorporating fairness into different stages of the typical ML pipeline, such as data processing, model design, deployment, and evaluation. To illustrate the impact of biases in data on ML models, we present examples that demonstrate how systematic biases can be amplified through model predictions. These case studies suggest how the framework can be used to prevent these biases and highlight the need for fair and equitable ML models in public health. This work aims
    
[^97]: 压缩索引实现瞬间相似性搜索

    Similarity search in the blink of an eye with compressed indices. (arXiv:2304.04759v1 [cs.LG])

    [http://arxiv.org/abs/2304.04759](http://arxiv.org/abs/2304.04759)

    本文提出一种新的向量压缩方法局部自适应量化(LVQ)，并在基于图的索引的关键优化下实现减少有效带宽同时启用随机访问友好的快速相似性计算，从而在性能和内存占用方面创造了新的最佳表现。

    

    如今，数据以向量表示。在海量数据中寻找与给定查询相似的向量是一项广泛应用的问题。本文提出了创建更快、更小的索引以运行这些搜索的新技术。为此，我们介绍了一种新的向量压缩方法，局部自适应量化(LVQ)，它同时减少内存占用和改善搜索性能，对搜索准确性的影响最小。LVQ被设计为与基于图的索引一起工作以实现减少有效带宽同时启用随机访问友好的快速相似性计算。我们的实验结果表明，在现代数据中心系统中针对基于图的索引进行关键优化后，LVQ的性能和内存占用方面创造了新的最佳表现。在处理数十亿个向量时，LVQ超过第二佳方案：

    Nowadays, data is represented by vectors. Retrieving those vectors, among millions and billions, that are similar to a given query is a ubiquitous problem of relevance for a wide range of applications. In this work, we present new techniques for creating faster and smaller indices to run these searches. To this end, we introduce a novel vector compression method, Locally-adaptive Vector Quantization (LVQ), that simultaneously reduces memory footprint and improves search performance, with minimal impact on search accuracy. LVQ is designed to work optimally in conjunction with graph-based indices, reducing their effective bandwidth while enabling random-access-friendly fast similarity computations. Our experimental results show that LVQ, combined with key optimizations for graph-based indices in modern datacenter systems, establishes the new state of the art in terms of performance and memory footprint. For billions of vectors, LVQ outcompetes the second-best alternatives: (1) in the low
    
[^98]: 建立高效和有表现力的三维等变图神经网络的新视角

    A new perspective on building efficient and expressive 3D equivariant graph neural networks. (arXiv:2304.04757v1 [cs.LG])

    [http://arxiv.org/abs/2304.04757](http://arxiv.org/abs/2304.04757)

    本文提出了一个本地等同性的三维等变图神经网络层次结构来评估等变GNN的表达能力，并提出局部亚结构编码（LSE）和帧转换编码（FTE）两个关键模块。LEFTNet有效地应用了这些模块并在分子属性预测任务中实现了最先进的性能。

    

    几何深度学习使得在建模三维物体时可以编码物理对称性。本文提出了一个本地等同性的三维等变图神经网络层次结构来评估等变GNN的表达能力，并调查从局部块代表全局几何信息的过程。研究结果表明，两个关键模块——局部亚结构编码（LSE）和帧转换编码（FTE）是设计高效和有表现力几何GNN的基础。为了证明我们理论的适用性，我们提出了LEFTNet，它有效地实现了这些模块，并在标量值和矢量值分子属性预测任务中实现了最先进的性能。此外，我们还指出等变图神经网络的未来发展空间。

    Geometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these networks through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (LSE) and frame transition encoding (FTE). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out the design space for future developments of equivariant graph neural networks. O
    
[^99]: 一种新的道路车辆质量问题诊断的两层因果推断框架

    A Novel Two-level Causal Inference Framework for On-road Vehicle Quality Issues Diagnosis. (arXiv:2304.04755v1 [cs.AI])

    [http://arxiv.org/abs/2304.04755](http://arxiv.org/abs/2304.04755)

    该论文提出了一种新的两级因果推断框架，利用因果机器学习可以加速汽车行业中处理车辆质量问题的全周期，从而更快地隔离根本原因、确定治疗措施并评估其有效性。

    

    在汽车行业中，处理使用中车辆质量问题的全周期可能需要数周进行调查。这个过程涉及到隔离根本原因、定义和实施适当的治疗措施，以及在必要时改进治疗措施。主要问题是缺乏一种系统性的方法来确定因果关系、评估治疗效果，并在当前治疗被认为无效时指导下一个可行的治疗措施。本文将展示如何利用因果机器学习（ML）加速这些过程。使用从路上车辆收集的真实数据集来演示所提出的框架。还将讨论车辆质量应用的开放挑战。

    In the automotive industry, the full cycle of managing in-use vehicle quality issues can take weeks to investigate. The process involves isolating root causes, defining and implementing appropriate treatments, and refining treatments if needed. The main pain-point is the lack of a systematic method to identify causal relationships, evaluate treatment effectiveness, and direct the next actionable treatment if the current treatment was deemed ineffective. This paper will show how we leverage causal Machine Learning (ML) to speed up such processes. A real-word data set collected from on-road vehicles will be used to demonstrate the proposed framework. Open challenges for vehicle quality applications will also be discussed.
    
[^100]: 基于联邦学习的无线电频谱占用检测方法的比较：节点和无节点设计

    Por\'ownanie metod detekcji zaj\k{e}to\'sci widma radiowego z wykorzystaniem uczenia federacyjnego z oraz bez w\k{e}z{\l}a centralnego. (arXiv:2304.04754v1 [cs.NI])

    [http://arxiv.org/abs/2304.04754](http://arxiv.org/abs/2304.04754)

    本文比较了两种使用联邦机器学习的系统设计方法：带有中央节点和不带中央节点。

    

    动态频谱接入系统通常需要关于频谱占用情况和其他用户存在的信息，以便为新设备做出频谱分配决策。简单的频谱占用检测方法经常不够可靠，因此通常成功地采用支持机器学习或人工智能的频谱占用检测算法。为了保护用户数据的隐私和减少控制数据量，一个有趣的方法是使用联邦机器学习。本文比较了两种使用联邦机器学习的系统设计方法：带有中央节点和不带中央节点。

    Dynamic spectrum access systems typically require information about the spectrum occupancy and thus the presence of other users in order to make a spectrum al-location decision for a new device. Simple methods of spectrum occupancy detection are often far from reliable, hence spectrum occupancy detection algorithms supported by machine learning or artificial intelligence are often and successfully used. To protect the privacy of user data and to reduce the amount of control data, an interesting approach is to use federated machine learning. This paper compares two approaches to system design using federated machine learning: with and without a central node.
    
[^101]: $\textit{e-Uber}$：一个基于共享经济的平台，实现基于电动汽车的拼车和能量共享

    $\textit{e-Uber}$: A Crowdsourcing Platform for Electric Vehicle-based Ride- and Energy-sharing. (arXiv:2304.04753v1 [cs.AI])

    [http://arxiv.org/abs/2304.04753](http://arxiv.org/abs/2304.04753)

    本论文提出了一个名为e-Uber的众包平台，该平台利用电动汽车的日益普及，通过V2G和BST共同实现拼车和能量共享。平台利用强化学习，基于CMAB算法实现个性化任务推荐系统(CARS)，并利用反向拍卖机制选择最优出价。

    

    最近，基于共享经济的商业模式在交通和住宿领域取得了成功，如Uber和Airbnb。人们越来越有兴趣将此模式应用于能源系统，包括点对点（P2P）能源交易和基于电动汽车（EV）的车对网（V2G）、车对家（V2H）、车对车（V2V）以及电池更换技术(BST)等方式。本文利用电动汽车的日益普及，实现了一个名为e-Uber的众包平台，通过V2G和BST共同实现拼车和能量共享。e-Uber利用空间众包、强化学习和反向拍卖理论实现。具体而言，本平台利用强化学习了解司机对不同拼车和能量共享任务的偏好。基于这些偏好，通过基于CMAB的算法进行任务推荐系统(CARS)的个性化推荐。司机在愿意执行的任务上进行竞标，而反向拍卖机制则选择最优出价。计算实验表明，所提出的系统在匹配效率、时间效率和用户满意度方面具有优越性。

    The sharing-economy-based business model has recently seen success in the transportation and accommodation sectors with companies like Uber and Airbnb. There is growing interest in applying this model to energy systems, with modalities like peer-to-peer (P2P) Energy Trading, Electric Vehicles (EV)-based Vehicle-to-Grid (V2G), Vehicle-to-Home (V2H), Vehicle-to-Vehicle (V2V), and Battery Swapping Technology (BST). In this work, we exploit the increasing diffusion of EVs to realize a crowdsourcing platform called e-Uber that jointly enables ride-sharing and energy-sharing through V2G and BST. e-Uber exploits spatial crowdsourcing, reinforcement learning, and reverse auction theory. Specifically, the platform uses reinforcement learning to understand the drivers' preferences towards different ride-sharing and energy-sharing tasks. Based on these preferences, a personalized list is recommended to each driver through CMAB-based Algorithm for task Recommendation System (CARS). Drivers bid on 
    
[^102]: Pumas在药代动力学中贝叶斯推断的实践指南

    A Practitioner's Guide to Bayesian Inference in Pharmacometrics using Pumas. (arXiv:2304.04752v1 [stat.AP])

    [http://arxiv.org/abs/2304.04752](http://arxiv.org/abs/2304.04752)

    本文为使用Pumas工作流程的药代动力学的贝叶斯实践者提供了全面的教程，介绍了标准贝叶斯工作流程的所有步骤，以及许多重要的想法和预防措施。

    

    本文提供了一个全面的教程，介绍了使用Pumas工作流程的药代动力学贝叶斯实践者的实践指南。我们从简要介绍药代动力学中的贝叶斯推断的动机开始，强调现有软件的局限性，并介绍了药代动力学标准贝叶斯工作流程的所有步骤，包括模型定义、先验选择、从后验采样、先验和后验模拟和预测、反事实模拟和预测、收敛诊断、视觉预测检查以及模型比较和交叉验证。最后，我们用简单的语言解释了贝叶斯统计学中许多先进概念的背景和直觉，包括用户在执行贝叶斯分析时需要记住的许多重要的想法和预防措施。本文介绍的许多算法、代码和思想都有很高的实用性。

    This paper provides a comprehensive tutorial for Bayesian practitioners in pharmacometrics using Pumas workflows. We start by giving a brief motivation of Bayesian inference for pharmacometrics highlighting limitations in existing software that Pumas addresses. We then follow by a description of all the steps of a standard Bayesian workflow for pharmacometrics using code snippets and examples. This includes: model definition, prior selection, sampling from the posterior, prior and posterior simulations and predictions, counter-factual simulations and predictions, convergence diagnostics, visual predictive checks, and finally model comparison with cross-validation. Finally, the background and intuition behind many advanced concepts in Bayesian statistics are explained in simple language. This includes many important ideas and precautions that users need to keep in mind when performing Bayesian analysis. Many of the algorithms, codes, and ideas presented in this paper are highly applicab
    
[^103]: 可能大致正确联邦学习

    Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04641](http://arxiv.org/abs/2304.04641)

    本文提出了FedPAC框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。

    

    联邦学习是一种新的分布式学习范例，其主要支柱为隐私、效用和效率。现有研究表明，同时实现无穷小隐私泄露、效用损失和效率是不可能的。因此，在设计联邦学习算法时，如何找到最佳权衡解决方案是关键考虑因素。一种常见的方法是将权衡问题视为多目标优化问题，即目标是在约束隐私泄露不超过预定值的情况下最小化效用损失和效率降低。然而，现有的多目标优化框架非常耗时，并且不能保证帕累托前沿的存在性，这激励我们寻求一种方法，将多目标问题转化为单目标问题，因为它更高效、更容易被解决。为此，本文提出了FedPAC，这是一个统一的框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。具体而言，我们首先将FL问题公式化为一个二分类任务，然后设计一个自适应FL算法，动态调整每个客户端的采样比率，以平衡全局和本地的隐私-效用权衡，最后证明FedPAC可以在温和的假设下高概率地实现最优的隐私-效用权衡。基准数据集上的大量实验证明了我们提出的FedPAC框架的功效和效率。

    Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
    
[^104]: 神经流形非均匀B样条流

    Neural Diffeomorphic Non-uniform B-spline Flows. (arXiv:2304.04555v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04555](http://arxiv.org/abs/2304.04555)

    提出了一种具有高效参数化和解析逆变换的至少二次连续可微、双Lipschitz连续的非均匀B样条流形，能够在密度估计和图像生成任务中表现出具有竞争力的结果。

    

    正则化流成功地将复杂的概率分布建模为简单基本分布的可逆变换。然而，有时候需要更多。在物理中计算能量和力要求变换的二阶导数是良好定义和连续的，平滑正则化流采用无限可微变换，但以缓慢的非解析逆变换的代价。本文提出了至少二次连续可微且双Lipschitz连续的非均匀B样条流形，实现了高效的参数化和基于微分同胚的解析逆变换。首先，我们研究了Ck-2微分同胚的非均匀k阶B样条变换的充分条件。然后，我们推导了符合充分条件的非均匀立方B样条变换的解析逆变换。实验表明，与现有最先进的方法相比，所提出的方法在密度估计和图像生成任务上获得了具有竞争力的结果。

    Normalizing flows have been successfully modeling a complex probability distribution as an invertible transformation of a simple base distribution. However, there are often applications that require more than invertibility. For instance, the computation of energies and forces in physics requires the second derivatives of the transformation to be well-defined and continuous. Smooth normalizing flows employ infinitely differentiable transformation, but with the price of slow non-analytic inverse transforms. In this work, we propose diffeomorphic non-uniform B-spline flows that are at least twice continuously differentiable while bi-Lipschitz continuous, enabling efficient parametrization while retaining analytic inverse transforms based on a sufficient condition for diffeomorphism. Firstly, we investigate the sufficient condition for Ck-2-diffeomorphic non-uniform kth-order B-spline transformations. Then, we derive an analytic inverse transformation of the non-uniform cubic B-spline tran
    
[^105]: 实现队列智能化：一种针对电子病历分析的通用群体表示学习框架

    Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis. (arXiv:2304.04468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04468](http://arxiv.org/abs/2304.04468)

    提出了一种通用的COhort Representation lEarning（CORE）框架，用于增强EHR表示学习，支持针对不同队列的特征进行可解释性分析。

    

    电子病历（EHR）是从临床常规护理中生成的，记录了广泛的病人人群有价值的信息，为改善临床实践中的病人管理和干预策略提供了丰富的机会。为了利用EHR数据的巨大潜力，机器学习中流行的EHR数据分析范式是EHR表示学习，它首先利用单个病人的EHR数据通过一个主干学习信息丰富的表示，并支持建立在这些表示的多样化的医疗下游任务。然而，这种范式无法深入分析病人的相关性，通常在临床实践中被称为队列研究。具体来说，同一队列中的病人倾向于具有相似的特征，表明他们在医疗条件（如症状或疾病）方面具有相似之处。在本文中，我们提出了一种通用COhort Representation lEarning (CORE)框架来增强EHR表示学习，通过使用队列表示学习算法，对群体信息进行建模并支持针对不同队列的特征进行可解释性分析。

    Electronic Health Records (EHR) are generated from clinical routine care recording valuable information of broad patient populations, which provide plentiful opportunities for improving patient management and intervention strategies in clinical practice. To exploit the enormous potential of EHR data, a popular EHR data analysis paradigm in machine learning is EHR representation learning, which first leverages the individual patient's EHR data to learn informative representations by a backbone, and supports diverse health-care downstream tasks grounded on the representations. Unfortunately, such a paradigm fails to access the in-depth analysis of patients' relevance, which is generally known as cohort studies in clinical practice. Specifically, patients in the same cohort tend to share similar characteristics, implying their resemblance in medical conditions such as symptoms or diseases. In this paper, we propose a universal COhort Representation lEarning (CORE) framework to augment EHR
    
[^106]: 关于多模态学习的鲁棒性

    On Robustness in Multimodal Learning. (arXiv:2304.04385v1 [cs.LG])

    [http://arxiv.org/abs/2304.04385](http://arxiv.org/abs/2304.04385)

    本文提出了一个多模态鲁棒性框架，分析了多种多模态学习方法的鲁棒性不足，并提出了两种干预技术，在多个数据集上实现了1.5倍至4倍的鲁棒性提高，同时在AudioSet 20K上实现了44.2 mAP的有竞争力结果。

    

    多模态学习被定义为对多种异构输入模态（如视频、音频和文本等）进行学习。本文关注了解当训练和部署之间的模态类型不同时，模型如何表现，这在许多应用多模态学习应用于硬件平台时会自然发生。我们提出了一个多模态鲁棒性框架来系统分析常见的多模态表示学习方法。此外，我们发现了这些方法的鲁棒性不足，并提出了两种干预技术，在AudioSet、Kinetics-400和ImageNet-Captions三个数据集上实现了1.5倍至4倍的鲁棒性提高。最后，我们展示了这些干预技术可以更好地利用额外的模态，在AudioSet 20K上实现了44.2 mAP的有竞争力结果。

    Multimodal learning is defined as learning over multiple heterogeneous input modalities such as video, audio, and text. In this work, we are concerned with understanding how models behave as the type of modalities differ between training and deployment, a situation that naturally arises in many applications of multimodal learning to hardware platforms. We present a multimodal robustness framework to provide a systematic analysis of common multimodal representation learning methods. Further, we identify robustness short-comings of these approaches and propose two intervention techniques leading to $1.5\times$-$4\times$ robustness improvements on three datasets, AudioSet, Kinetics-400 and ImageNet-Captions. Finally, we demonstrate that these interventions better utilize additional modalities, if present, to achieve competitive results of $44.2$ mAP on AudioSet 20K.
    
[^107]: 基于NeRF技术的卫星图像表面重建

    NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])

    [http://arxiv.org/abs/2304.04133](http://arxiv.org/abs/2304.04133)

    本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。

    

    本文提出了Sat-NeRF模型，是对最近引入的S-NeRF模型的修改实现。该模型能够从稀疏的卫星图像集合中合成新的视角，同时考虑到图片中的光照变化。训练好的模型还能够精确地估计场景表面的高程，这对卫星观测应用非常有帮助。S-NeRF方法改进了标准的NeRF方法，将辐射强度考虑为高反射率和入射辐照度的函数。这两个量都是模型的全连接神经网络枝条的输出，而后者则被视为来自太阳的直接光线和来自天空的漫反射颜色函数。该实现基于用缩放-裁剪技术增强的卫星图像数据集。对NeRF进行了超参数研究，得出了一些有趣的观察结果。

    We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
    
[^108]: TC-VAE：揭示数据生成因素中的未知分布数据

    TC-VAE: Uncovering Out-of-Distribution Data Generative Factors. (arXiv:2304.04103v1 [cs.LG])

    [http://arxiv.org/abs/2304.04103](http://arxiv.org/abs/2304.04103)

    本文提出了一种基于总相关性的生成模型TC-VAE，可以揭示数据生成因素中的未知分布数据，在处理具有不平衡生成因素的数据集上表现优秀。

    

    揭示数据生成因素是解决解缠结学习的最终目标。本文提出了一种生成模型-TC-VAE，它可以基于所学的潜在表征和输入数据之间的总相关性下界进行优化，从而发现不在数据集中显式出现的变化因素。我们分析了在使用具有不平衡的生成因素数据集时，所提出的模型的效果，并在定量和定性实验中表明了TC-VAE的优越性。

    Uncovering data generative factors is the ultimate goal of disentanglement learning. Although many works proposed disentangling generative models able to uncover the underlying generative factors of a dataset, so far no one was able to uncover OOD generative factors (i.e., factors of variations that are not explicitly shown on the dataset). Moreover, the datasets used to validate these models are synthetically generated using a balanced mixture of some predefined generative factors, implicitly assuming that generative factors are uniformly distributed across the datasets. However, real datasets do not present this property. In this work we analyse the effect of using datasets with unbalanced generative factors, providing qualitative and quantitative results for widely used generative models. Moreover, we propose TC-VAE, a generative model optimized using a lower bound of the joint total correlation between the learned latent representations and the input data. We show that the proposed
    
[^109]: StepMix: 一个用于外部变量广义混合模型的伪似然估计的Python包

    StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])

    [http://arxiv.org/abs/2304.03853](http://arxiv.org/abs/2304.03853)

    StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。

    

    StepMix是一个用于广义有限混合模型(潜在剖面和潜在类分析)与外部变量(协变量和远程结果)的伪似然估计(单步、两步和三步方法)的开源软件包。在许多社会科学的应用中，主要目标不仅是将个体聚类成潜在类别，还包括使用这些类别来开发更复杂的统计模型。这些模型通常分为一个将潜在类别与观察指标相关联的测量模型和一个将协变量和结果变量与潜在类别相关联的结构模型。测量和结构模型可以使用所谓的一步法共同估计，也可以使用逐步方法逐步估计，对于从业人员来说，这些方法在估计潜在类别的可解释性方面具有显著优势。除了一步法，StepMix还实现了文献中提出的最重要的逐步估计方法，提供了用户友好的界面，方便模型的估计、选择和解释。

    StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
    
[^110]: 推荐系统的图协作信号去噪与增强

    Graph Collaborative Signals Denoising and Augmentation for Recommendation. (arXiv:2304.03344v1 [cs.IR])

    [http://arxiv.org/abs/2304.03344](http://arxiv.org/abs/2304.03344)

    本文提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，并通过预训练和top-K采样增强了用户-项目交互矩阵，以更好地适应所有用户的需求。

    

    图协作过滤（GCF）是捕捉推荐系统中高阶协同信号的流行技术。然而，GCF的双向邻接矩阵，其定义了基于用户-项目交互进行聚合的邻居，对于有大量交互但不足的用户/项目来说可能是嘈杂的。此外，邻接矩阵忽略了用户-用户和项目-项目之间的相关性，这可能限制了聚合的有益邻居的范围。在这项工作中，我们提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，以平衡所有用户之间的交互数量。为了实现这一点，我们预先训练了一个基于图的推荐方法来获得用户/项目嵌入，然后通过top-K采样增强了用户-项目交互矩阵。我们还增强了对称的用户-用户和项目-项目相关组件，以更好地适应所有用户的需求。

    Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated.  In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to th
    
[^111]: 长期的多模式变压器整合EHR中成像和潜在临床特征，用于肺部结节分类。

    Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])

    [http://arxiv.org/abs/2304.02836](http://arxiv.org/abs/2304.02836)

    本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。

    

    将重复成像和医疗背景（如电子健康记录）纳入预测性孤立性肺部结节（SPN）诊断模型可以极大增加准确性。然而，像成像和诊断代码这样的临床常规模式可能是异步的，并且在不同时间尺度上进行不规则采样，这是长期多模态学习的障碍。我们提出了一种基于变压器的多模态策略，将重复成像与日常收集的EHR中的长期临床特征相整合，以进行SPN分类。我们对潜在临床特征进行无监督的解缠缚，并利用时间距离缩放自注意力来联合学习临床特征表达和胸部计算机断层扫描（CT）。我们的分类器是在一个公共数据集的2,668个扫描和1,149名志愿者的长期胸部CT、账单代码、药物和实验室检查记录中进行预训练的。

    The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
    
[^112]: 以七个动作阶段的方式设计智能写作助手的可用性

    Approach Intelligent Writing Assistants Usability with Seven Stages of Action. (arXiv:2304.02822v1 [cs.HC] CROSS LISTED)

    [http://arxiv.org/abs/2304.02822](http://arxiv.org/abs/2304.02822)

    本文提出采用诺曼的七个动作阶段作为智能写作助手交互设计的框架，并提供支持这些动作阶段的工具的示例。该框架有潜力成为人-LLM交互研究的重要工具。

    

    尽管大型语言模型( LLMS )具备成为写作助手的潜力，但它们仍然存在一些问题，例如模型输出的连贯性和流畅性、可信度、生成内容的所有权以及模型性能的可预测性，从而限制了它们的可用性。在本文中，我们提出采用诺曼的七个动作阶段作为智能写作助手交互设计的框架。我们通过提供软件教程创作的示例，说明该框架适用于写作任务。本文还讨论了该框架作为综合基于LLMS工具的交互设计研究的工具，并提供了支持这些动作阶段的工具的示例。最后，我们简要概述了人-LLMS交互研究框架的潜力。

    Despite the potential of Large Language Models (LLMs) as writing assistants, they are plagued by issues like coherence and fluency of the model output, trustworthiness, ownership of the generated content, and predictability of model performance, thereby limiting their usability. In this position paper, we propose to adopt Norman's seven stages of action as a framework to approach the interaction design of intelligent writing assistants. We illustrate the framework's applicability to writing tasks by providing an example of software tutorial authoring. The paper also discusses the framework as a tool to synthesize research on the interaction design of LLM-based tools and presents examples of tools that support the stages of action. Finally, we briefly outline the potential of a framework for human-LLM interaction research.
    
[^113]: AutoRL超参数景观

    AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])

    [http://arxiv.org/abs/2304.02396](http://arxiv.org/abs/2304.02396)

    本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。

    

    强化学习（RL）在取得令人瞩目成果的同时，其超参数对性能的影响限制了其应用范围。这经常使得在实践中难以获得良好的结果。自动化RL（AutoRL）解决了这个难题，但有关超参数优化（HPO）方法在搜索最佳配置时所遍历的超参数景观动态变化的信息很少。鉴于现有AutoRL方法动态调整超参数配置的情况，我们提出了一种方法，在训练期间不仅在一个时间点，而且在多个时间点上建立和分析这些超参数景观。针对关于这种动态AutoRL方法合法性的一个重要开放问题，我们提供了充分的证据，表明在不同种类的环境（Cartpole和Pendulum）中，来自RL文献的代表算法（DQN和SAC）的超参数景观会随时间而强烈变化。

    Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
    
[^114]: EGC: 一种通过单一能量模型生成与分类图像的方法

    EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])

    [http://arxiv.org/abs/2304.02012](http://arxiv.org/abs/2304.02012)

    EGC是一种使用单个神经网络在图像分类和图像生成任务中实现卓越性能的方法，可以较好地生成出高质量图像，并在多项数据集上实现了领先的分类结果。

    

    使用相同的网络参数学习图像分类和生成图像是一个具有挑战性的问题。最近的先进方法在一项任务上表现良好，但在另一项任务上却表现不佳。本文引入了一种名为EGC的基于能量的分类器和生成器，它可以使用单个神经网络在两个任务中实现卓越性能。与传统的分类器输出给定图像的标签（即条件分布$p(y|\mathbf{x})$）不同，EGC的前向传递器是一个分类器，它输出一个联合分布$p(\mathbf{x},y)$，在后向传递器中通过边缘化标签$y$实现生成器。在前向传递中，估计给定噪声图像的能量和分类概率，而在后向传递中，通过估计得分函数对其进行去噪。EGC在ImageNet-1k、CelebA-HQ和LSUN Church上实现了与最先进方法相当的生成结果，同时在CIFAR-10、CIFAR-100和ImageNet-1k上实现了最先进的分类结果。

    Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
    
[^115]: 利用普尔金氏图像和机器学习算法进行动态调节测量

    Dynamic Accommodation Measurement using Purkinje Images and ML Algorithms. (arXiv:2304.01296v1 [physics.med-ph])

    [http://arxiv.org/abs/2304.01296](http://arxiv.org/abs/2304.01296)

    本研究开发出一种基于普尔金氏图像和机器学习算法的原型设备，可用于动态注视和调节测量，预测调节可以精确到0.25D，正在使用机器学习生成大量合成数据集。

    

    我们开发了一种基于4个普尔金氏反射（PR）的原型设备，用于适用于AR和眼科应用的动态注视和调节测量。 PR1和2以及PR3和4分别用于准确测量凝视和调节。我们的眼睛模型在ZEMAX中开发，并与实验结果相匹配。 我们的模型能够以超过0.25D的精度从4度到1度预测调节。我们进行了重复性测试，并从受试者身上获得了准确的凝视和调节估计。我们正在使用物理精确的模型和机器学习生成大量合成数据集。

    We developed a prototype device for dynamic gaze and accommodation measurements based on 4 Purkinje reflections (PR) suitable for use in AR and ophthalmology applications. PR1&2 and PR3&4 are used for accurate gaze and accommodation measurements, respectively. Our eye model was developed in ZEMAX and matches the experiments well. Our model predicts the accommodation from 4 diopters to 1 diopter with better than 0.25D accuracy. We performed repeatability tests and obtained accurate gaze and accommodation estimations from subjects. We are generating a large synthetic data set using physically accurate models and machine learning.
    
[^116]: HARFLOW3D：一种面向FPGA设备的基于延迟的3D-CNN加速器工具链

    HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])

    [http://arxiv.org/abs/2303.17218](http://arxiv.org/abs/2303.17218)

    本研究提出了一种面向FPGA设备的基于延迟的3D-CNN加速器工具链HARFLOW3D，它以机器学习模型和FPGA的特性描述为输入，生成最小化计算延迟的设计。实验证明HARFLOW3D相比其他方案能够实现更低的延迟。

    

    3D卷积神经网络已被证明在人体动作识别任务中具有高效性和最先进的结果。本研究引入一种新的基于流式架构的工具链，将此类模型映射到FPGA上，考虑模型固有特性和目标FPGA设备的特征。HARFLOW3D工具链以ONNX格式的3D卷积神经网络和FPGA特性描述为输入，生成最小化计算延迟的设计。该工具链由多个部分组成，包括i) 3D CNN解析器，ii) 性能和资源模型，iii) 用于在生成的硬件上执行3D模型的调度算法，iv) 针对3D模型量身定制的资源感知优化引擎，v) 自动映射到可合成的FPGA代码。通过对各种3D CNN和FPGA系统配对进行多个实验，展示了工具链支持广泛模型和设备的能力。此外，与其他最先进的3D CNN加速器设计方法相比，该工具链实现了更低的延迟。

    For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
    
[^117]: 从数据中学习流函数及其在非线性振荡器中的应用

    Learning Flow Functions from Data with Applications to Nonlinear Oscillators. (arXiv:2303.16656v1 [eess.SY])

    [http://arxiv.org/abs/2303.16656](http://arxiv.org/abs/2303.16656)

    本文提出一种基于循环神经网络的架构，通过数据学习了一个因果、时不变且连续时间控制系统的流函数，并证明了该架构能够近似表示流函数，在实验中应用于Van der Pol和FitzHugh Nagumo振荡器的轨迹重现和最优控制输入计算。

    

    本文提出了一种基于循环神经网络（RNN）的架构来从轨迹数据中学习一个因果、时不变且连续时间控制系统的流函数。通过将控制输入的类别限制为分段常数函数，我们展示了学习流函数等价于学习离散时间动态系统的状态映射的输入输出映射关系。因此，提出一个由编码器和解码器网络构成的RNN，以将系统状态映射到RNN的隐藏状态以及将隐藏状态映射回系统状态。通过利用系统的因果性和时不变性，我们证明了提出的架构能够近似地表示流函数，且可以在任意时间查询学习到的流函数。我们通过Van der Pol和FitzHugh Nagumo振荡器的模型验证了所提出方法的有效性，结果表明该架构能够准确地重现这两个系统的轨迹。对于Van der Pol振荡器，我们展示了该架构如何用于计算将系统引导到所需轨迹的最优控制输入。

    We describe a recurrent neural network (RNN) based architecture to learn the flow function of a causal, time-invariant and continuous-time control system from trajectory data. By restricting the class of control inputs to piecewise constant functions, we show that learning the flow function is equivalent to learning the input-to-state map of a discrete-time dynamical system. This motivates the use of an RNN together with encoder and decoder networks which map the state of the system to the hidden state of the RNN and back. We show that the proposed architecture is able to approximate the flow function by exploiting the system's causality and time-invariance. The output of the learned flow function model can be queried at any time instant. We experimentally validate the proposed method using models of the Van der Pol and FitzHugh Nagumo oscillators. In both cases, the results demonstrate that the architecture is able to closely reproduce the trajectories of these two systems. For the Va
    
[^118]: 训练数据重构的非渐进性下界

    Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])

    [http://arxiv.org/abs/2303.16372](http://arxiv.org/abs/2303.16372)

    本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析

    

    本文研究了专业对手进行训练数据重构攻击时私有学习算法的语义保证强度。我们通过导出非渐进量级下界来研究了满足差分隐私（DP）和度量隐私（mDP）的学习器对抗者重构错误的鲁棒性。此外，我们还证明了我们对mDP的分析覆盖了高维情况。本文进一步对流行的深度学习算法，如DP-SGD和Projected Noisy SGD进行了度量差分隐私的扩展隐私分析。

    We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
    
[^119]: 带有降低通信要求的异步在线联邦学习

    Asynchronous Online Federated Learning with Reduced Communication Requirements. (arXiv:2303.15226v1 [cs.LG])

    [http://arxiv.org/abs/2303.15226](http://arxiv.org/abs/2303.15226)

    提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，能够处理异构和延迟设备，实现参与学习任务的可访问性和效率。

    

    在线联邦学习（FL）使得地理分布的设备可以从本地的流数据中学习到全局共享模型。大多数关于在线FL的文献都考虑了最佳情况下的参与客户端和通信渠道。然而，这些假设在实际应用中通常无法满足。异步设置可以反映出更现实的环境，例如由于可用的计算能力和电池限制而发生的异构客户端参与，以及由通信渠道或落后设备引起的延迟。此外，在大多数应用中，必须考虑能源效率。我们提出了一种基于部分共享通信原理的通信高效异步在线联邦学习（PAO-Fed）策略，通过减少参与者的通信开销，提高了参与学习任务的可访问性和效率。此外，该方法能够处理异构和延迟设备，使其更适用于实际应用。

    Online federated learning (FL) enables geographically distributed devices to learn a global shared model from locally available streaming data. Most online FL literature considers a best-case scenario regarding the participating clients and the communication channels. However, these assumptions are often not met in real-world applications. Asynchronous settings can reflect a more realistic environment, such as heterogeneous client participation due to available computational power and battery constraints, as well as delays caused by communication channels or straggler devices. Further, in most applications, energy efficiency must be taken into consideration. Using the principles of partial-sharing-based communications, we propose a communication-efficient asynchronous online federated learning (PAO-Fed) strategy. By reducing the communication overhead of the participants, the proposed method renders participation in the learning task more accessible and efficient. In addition, the prop
    
[^120]: 找到对抗样本需要多少维度？

    How many dimensions are required to find an adversarial example?. (arXiv:2303.14173v1 [cs.LG])

    [http://arxiv.org/abs/2303.14173](http://arxiv.org/abs/2303.14173)

    本文研究了对抗性漏洞如何取决于受限于高维输入空间中的子空间维数，同时针对标准PGD攻击的对抗性成功率提出了单调递增函数表达式。

    

    过去探索对抗性漏洞的研究都着眼于对手可以扰动模型输入的所有维度的情况。另一方面，许多最近的研究考虑以下情况：（i）对手可以扰动有限数量的输入参数或（ii）多模态问题中的模态子集。在这两种情况下，对抗性样本有效地受限于高维输入空间中的子空间$V$。出于这个动机，我们在本文中研究了对抗性漏洞如何取决于$V$的维数。特别地，我们展示了标准PGD攻击的对抗性成功率如何表现为$\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$的单调递增函数，其中$\epsilon$是扰动预算，$\frac{1}{p}+\frac{q}{q}=1$，只要$p>1$（当$p=1$时会出现额外的细微差别，我们对此进行了详细的分析）。这个函数形式可以很容易地推导。

    Past work exploring adversarial vulnerability have focused on situations where an adversary can perturb all dimensions of model input. On the other hand, a range of recent works consider the case where either (i) an adversary can perturb a limited number of input parameters or (ii) a subset of modalities in a multimodal problem. In both of these cases, adversarial examples are effectively constrained to a subspace $V$ in the ambient input space $\mathcal{X}$. Motivated by this, in this work we investigate how adversarial vulnerability depends on $\dim(V)$. In particular, we show that the adversarial success of standard PGD attacks with $\ell^p$ norm constraints behaves like a monotonically increasing function of $\epsilon (\frac{\dim(V)}{\dim \mathcal{X}})^{\frac{1}{q}}$ where $\epsilon$ is the perturbation budget and $\frac{1}{p} + \frac{1}{q} =1$, provided $p > 1$ (the case $p=1$ presents additional subtleties which we analyze in some detail). This functional form can be easily deriv
    
[^121]: ExBEHRT：基于电子病历的扩展Transformer预测疾病亚型和进展

    ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions. (arXiv:2303.12364v1 [cs.LG])

    [http://arxiv.org/abs/2303.12364](http://arxiv.org/abs/2303.12364)

    ExBEHRT是一种扩展Transformer模型，应用于电子病历数据，将多种类型的记录包括在特征空间中，可以预测不同疾病下游任务的性能更好，并使用预期梯度对结果进行更细粒度的解释。

    

    本研究引入了ExBEHRT，它是BEHRT（应用于电子病历的BERT）的扩展版本，并应用不同的算法来解释其结果。我们将特征空间从仅考虑诊断和患者年龄扩展到包括多种类型的记录，包括人口统计学、临床特征、生命体征、吸烟状态、诊断、手术、药物和实验室检查，并采用一种新方法来统一不同特征的频率和时间维度。我们展示了附加特征可以显著改善不同疾病下游任务的模型性能。为了保证模型的稳健性，我们使用了预期梯度的改进方法对模型预测结果进行解释，该方法以前未应用于将EHR数据与Transformer相结合，提供了比以前方法更细粒度的解释，如特征和令牌重要性。此外，通过对肿瘤学患者的模型表示进行聚类，我们展示了ExBEHRT可以用于预测疾病亚型和进展。

    In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
    
[^122]: LLM的合成数据生成对临床文本挖掘有帮助吗？

    Does Synthetic Data Generation of LLMs Help Clinical Text Mining?. (arXiv:2303.04360v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04360](http://arxiv.org/abs/2303.04360)

    研究探讨了利用 ChatGPT 进行临床文本挖掘的潜力，但初步结果表明效果欠佳，同时上传患者信息也引发隐私问题。因此，提出了一种新的训练方法，即使用 ChatGPT 生成带标签的大量高质量合成数据进行训练。

    

    大型语言模型（LLM）的最近进展推动了诸如OpenAI的ChatGPT之类的高性能模型的发展。这些模型在问答、论文写作和代码生成等各种任务中表现出色。然而，它们在医疗保健领域的有效性仍不确定。本研究旨在通过检查ChatGPT从非结构化健康文本中提取结构化信息的能力，重点关注生物命名实体识别和关系提取，探讨ChatGPT在临床文本挖掘中的潜力。然而，我们的初步结果表明，直接应用ChatGPT进行这些任务导致表现不佳，并引发与将患者信息上传到ChatGPT API有关的隐私问题。为了克服这些限制，我们提出了一种新的训练范式，利用ChatGPT生成大量高质量的带标签的合成数据进行训练。

    Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine
    
[^123]: 作为2D系统的卷积神经网络

    Convolutional Neural Networks as 2-D systems. (arXiv:2303.03042v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.03042](http://arxiv.org/abs/2303.03042)

    本文介绍了一种将卷积神经网络表示为二维动力系统的方法，将其视为Lur'e系统的二维版本，从而有效地使用鲁棒控制理论进行鲁兹常数估计。

    

    本文介绍了一种新颖的卷积神经网络（CNN）的表示方法，即在二维动力系统方面。为此，使用卷积核的卷积层的常规描述，即线性滤波器的脉冲响应，在状态空间中表示为线性时不变的二维系统。然后将由卷积层和非线性激活函数组成的卷积神经网络视为Lur'e系统的二维版本，即与静态非线性组件相互连接的线性动力系统。这种对CNN的二维Lur'e系统视角的好处是我们可以比以前更有效地使用鲁兹常数估计的鲁棒控制理论。

    This paper introduces a novel representation of convolutional Neural Networks (CNNs) in terms of 2-D dynamical systems. To this end, the usual description of convolutional layers with convolution kernels, i.e., the impulse responses of linear filters, is realized in state space as a linear time-invariant 2-D system. The overall convolutional Neural Network composed of convolutional layers and nonlinear activation functions is then viewed as a 2-D version of a Lur'e system, i.e., a linear dynamical system interconnected with static nonlinear components. One benefit of this 2-D Lur'e system perspective on CNNs is that we can use robust control theory much more efficiently for Lipschitz constant estimation than previously possible.
    
[^124]: 连续时间延迟系统的神经拉普拉斯控制

    Neural Laplace Control for Continuous-time Delayed Systems. (arXiv:2302.12604v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12604](http://arxiv.org/abs/2302.12604)

    本文介绍了一种神经拉普拉斯控制方法，用于解决具有不规则状态观测和未知延迟的连续时间环境下的离线强化学习问题。

    

    许多实际的离线强化学习问题包括具有延迟的连续时间环境。这些环境具有两个显著特点：首先，观察到的状态x(t)在不规则的时间间隔内进行观察；其次，当前行动a(t)仅在未知延迟g > 0 的情况下影响未来状态x(t+g)。这样的环境的一个典型例子是卫星控制，其中地球和卫星之间的通信链路会造成观测不规则和延迟。现有的离线强化学习算法在具有时间不规则观测或已知延迟的环境中取得了成功。然而，涉及时间不规则观测和未知延迟的环境仍然是一个开放且具有挑战性的问题。因此，我们提出了神经拉普拉斯控制，一种连续时间基于模型的离线强化学习方法，将神经拉普拉斯动力学模型与模型预测控制（MPC）规划器相结合，并能够从离线数据集中进行学习。

    Many real-world offline reinforcement learning (RL) problems involve continuous-time environments with delays. Such environments are characterized by two distinctive features: firstly, the state x(t) is observed at irregular time intervals, and secondly, the current action a(t) only affects the future state x(t + g) with an unknown delay g > 0. A prime example of such an environment is satellite control where the communication link between earth and a satellite causes irregular observations and delays. Existing offline RL algorithms have achieved success in environments with irregularly observed states in time or known delays. However, environments involving both irregular observations in time and unknown delays remains an open and challenging problem. To this end, we propose Neural Laplace Control, a continuous-time model-based offline RL method that combines a Neural Laplace dynamics model with a model predictive control (MPC) planner--and is able to learn from an offline dataset sam
    
[^125]: 修正的条件t-SNE算法：超越最近邻近的限制

    Revised Conditional t-SNE: Looking Beyond the Nearest Neighbors. (arXiv:2302.03493v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03493](http://arxiv.org/abs/2302.03493)

    修正的条件t-SNE算法通过对高维相似度进行条件限制，解决了在数据经过标签良好聚类时，条件t-SNE算法的不足。并通过对相似度矩阵存储的改变提高了可扩展性。

    

    条件t-SNE（ct-SNE）是t-SNE的最新扩展，它允许从嵌入中删除已知的聚类信息，以获得揭示标签信息之外的结构的可视化结果。这在需要消除一组类之间不想要的差异时非常有用。我们发现在许多现实情况下，ct-SNE的表现并不理想，尤其是当数据在原始的高维空间中按标签进行良好的聚类时。我们提出了一种改进的方法，通过对高维相似度进行条件限制，并将基于标签的最近邻和跨标签的最近邻存储在不同的矩阵中。这还使得我们能够使用最近提出的t-SNE加速技术，提高算法的可扩展性。从合成数据的实验中，我们发现我们提出的方法解决了考虑的问题，并改善了嵌入质量。但是在包含批次效应的实际数据中，预期的改进并不总是存在。我们认为经过修订的ct-SNE较原始的算法更优。

    Conditional t-SNE (ct-SNE) is a recent extension to t-SNE that allows removal of known cluster information from the embedding, to obtain a visualization revealing structure beyond label information. This is useful, for example, when one wants to factor out unwanted differences between a set of classes. We show that ct-SNE fails in many realistic settings, namely if the data is well clustered over the labels in the original high-dimensional space. We introduce a revised method by conditioning the high-dimensional similarities instead of the low-dimensional similarities and storing within- and across-label nearest neighbors separately. This also enables the use of recently proposed speedups for t-SNE, improving the scalability. From experiments on synthetic data, we find that our proposed method resolves the considered problems and improves the embedding quality. On real data containing batch effects, the expected improvement is not always there. We argue revised ct-SNE is preferable ove
    
[^126]: 关于梯度下降动态和深度学习不稳定性的连续时间模型研究

    On a continuous time model of gradient descent dynamics and instability in deep learning. (arXiv:2302.01952v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01952](http://arxiv.org/abs/2302.01952)

    本文提出了一个连续时间流模型——主要流（PF），通过对Hessian矩阵特征分解的依赖，捕捉到了梯度下降中的发散和振荡行为、逃逸局部极小值和鞍点的连续性流，并解释了深度学习中的稳定边缘现象。通过对不稳定性的新理解，提出了一种学习率适应方法，可以控制训练稳定性和测试集评估性能之间的权衡。

    

    深度学习成功的秘诀在于神经网络和基于梯度的优化的结合。然而，理解梯度下降的行为，特别是其不稳定性，落后于其经验成功。为了增加研究梯度下降的理论工具，我们提出了主要流（PF），一种近似梯度下降动态的连续时间流。据我们所知，PF是唯一捕捉到梯度下降的发散和振荡行为，包括逃逸局部极小值和鞍点的连续性流。通过其对于Hessian特征分解的依赖，PF解释了深度学习中最近观察到的稳定边缘现象。通过对不稳定性的新理解，我们提出了一种学习率适应方法，使我们能够控制训练稳定性和测试集评估性能之间的权衡。

    The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.
    
[^127]: 带有预算和ROI约束的自动出价算法：效率、后悔和节奏动态

    Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics. (arXiv:2301.13306v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2301.13306](http://arxiv.org/abs/2301.13306)

    本文提出了一个基于梯度的学习算法，可以在多种拍卖方式下满足预算和ROI约束，并达到个体后悔逐渐减小；结果表明，当各自竞争时，期望资金流动至少达到最优分配的期望流动的一半。

    

    我们研究了自动出价算法在在线广告平台上进行博弈的情况。每个自动出价算法被赋予任务，在多轮重复拍卖中，最大化其广告主的总价值，同时受到预算和/或投资回报率约束。我们提出了一种基于梯度的学习算法，它可以保证满足所有约束条件，并达到逐渐减小的个体后悔。我们的算法仅使用自助反馈，并可与第一或第二价格拍卖以及任何“中间”拍卖格式一起使用。我们的主要结果是，当这些自动出价算法相互竞争时，所有轮次的期望资金流动 welfare 都至少达到了任何分配所实现的期望最优流动 welfare 的一半。这在出价动态是否收敛到均衡以及广告主估值之间的相关结构如何不同的情况下均成立。

    We study a game between autobidding algorithms that compete in an online advertising platform. Each autobidder is tasked with maximizing its advertiser's total value over multiple rounds of a repeated auction, subject to budget and/or return-on-investment constraints. We propose a gradient-based learning algorithm that is guaranteed to satisfy all constraints and achieves vanishing individual regret. Our algorithm uses only bandit feedback and can be used with the first- or second-price auction, as well as with any "intermediate" auction format. Our main result is that when these autobidders play against each other, the resulting expected liquid welfare over all rounds is at least half of the expected optimal liquid welfare achieved by any allocation. This holds whether or not the bidding dynamics converges to an equilibrium and regardless of the correlation structure between advertiser valuations.
    
[^128]: 基于语言驱动的锚点的零样本对抗鲁棒性

    Language-Driven Anchors for Zero-Shot Adversarial Robustness. (arXiv:2301.13096v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13096](http://arxiv.org/abs/2301.13096)

    本文提出了一种基于语言驱动、基于锚点的对抗训练策略LAAT，通过利用文本编码器的语义一致性，在零样本图像分类场景下增强图像模型的对抗鲁棒性。实验结果表明，该方法在零样本对抗性能上优于先前的最佳状态对抗性一次性方法，同时能为流行的图像分类模型带来实质性的零样本对抗性能提升。

    

    深度神经网络容易受到对抗性攻击。本文旨在改善具有挑战性的零样本图像分类场景下的对抗鲁棒性。为解决这一问题，我们提出了一种新的基于语言驱动、基于锚点的对抗训练策略LAAT。LAAT利用文本编码器为每个类别生成固定的锚点（归一化特征嵌入），并在对抗训练中使用这些锚点。通过利用文本编码器的语义一致性，LAAT可以增强图像模型在新类别上的对抗鲁棒性，而无需额外的样例。我们发现了最近文本编码器的余弦相似度问题，并设计了几种有效的技术来解决它。实验结果表明，LAAT显著提高了零样本对抗性能，优于先前的最佳状态对抗性一次性方法。此外，我们的方法在几个基准数据集上为流行的图像分类模型（如ResNet-50和DenseNet-121）产生了实质性的零样本对抗性能提升。

    Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adver
    
[^129]: 超级NeRF：用于超声成像的神经辐射场

    Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging. (arXiv:2301.10520v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.10520](http://arxiv.org/abs/2301.10520)

    该论文提出了一种物理增强的隐式神经表示（INR）用于超声成像，该表示可以从重叠的超声扫描中学习组织属性。利用基于光线追踪的神经渲染进行新视图的超声合成，实现几何精确的B模式图像的生成。

    

    我们提出了一种物理增强的隐式神经表示（INR），用于从重叠的超声扫描中学习组织属性的超声成像。我们提出的方法利用基于光线追踪的神经渲染进行新视图的超声合成。最近的研究表明，INR模型可以从一组二维超声框架中编码三维场景的表示。然而，这些模型未能考虑到超声成像固有的外观和几何视角相关变化。在我们的工作中，我们讨论了场景中方向相关的变化，并展示了物理启发式渲染如何提高超声图像合成的保真度。特别是，我们实验性地证明了我们提出的方法能够为具有模糊表示的区域生成几何精确的B模式图像，这是由于超声图像的视角相关差异所致。我们使用模拟肝脏B模式超声扫描和脊柱幻影超声扫描进行实验验证。

    We present a physics-enhanced implicit neural representation (INR) for ultrasound (US) imaging that learns tissue properties from overlapping US sweeps. Our proposed method leverages a ray-tracing-based neural rendering for novel view US synthesis. Recent publications demonstrated that INR models could encode a representation of a three-dimensional scene from a set of two-dimensional US frames. However, these models fail to consider the view-dependent changes in appearance and geometry intrinsic to US imaging. In our work, we discuss direction-dependent changes in the scene and show that a physics-inspired rendering improves the fidelity of US image synthesis. In particular, we demonstrate experimentally that our proposed method generates geometrically accurate B-mode images for regions with ambiguous representation owing to view-dependent differences of the US images. We conduct our experiments using simulated B-mode US sweeps of the liver and acquired US sweeps of a spine phantom tra
    
[^130]: 异步HFL：在分层IoT网络中进行高效、鲁棒的异步联邦学习 (arXiv:2301.06646v3 [cs.LG] UPDATED)

    Async-HFL: Efficient and Robust Asynchronous Federated Learning in Hierarchical IoT Networks. (arXiv:2301.06646v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06646](http://arxiv.org/abs/2301.06646)

    提出了异步HFL框架，以解决复杂、层次化的IoT网络中的联合学习挑战。该算法采用了异步聚合来避免长时间等待，并在网关和云级别上采用设备选择和设备网关调度来提高收敛速度、鲁棒性和可扩展性。

    

    近年来，联邦学习 (FL) 作为一种分布式设备学习范式受到了越来越多的关注。然而，在具有层次结构的真实物联网 (IoT) 网络中部署FL仍然面临着多重挑战。虽然现有的研究已经提出了各种方法来解决数据异质性、系统异质性、意外延迟者和可扩展性等问题，但没有一种方法提供一个系统性的解决方案来应对分层和不可靠的IoT网络中的所有挑战。在本文中，我们提出了一个异步和分层框架 (Async-HFL)，用于在常见的三层IoT网络结构中执行FL。针对大量不同的延迟，Async-HFL在网关和云级别均采用异步聚合，从而避免长时间等待。为了充分发挥在系统异质性和延迟者下异步HFL的收敛速度潜力，我们分别设计了网关级别的设备选择和云级别的设备网关调度。结果表明，Async-HFL在收敛速度、鲁棒性和可扩展性方面优于现有的FL算法。

    Federated Learning (FL) has gained increasing interest in recent years as a distributed on-device learning paradigm. However, multiple challenges remain to be addressed for deploying FL in real-world Internet-of-Things (IoT) networks with hierarchies. Although existing works have proposed various approaches to account data heterogeneity, system heterogeneity, unexpected stragglers and scalibility, none of them provides a systematic solution to address all of the challenges in a hierarchical and unreliable IoT network. In this paper, we propose an asynchronous and hierarchical framework (Async-HFL) for performing FL in a common three-tier IoT network architecture. In response to the largely varied delays, Async-HFL employs asynchronous aggregations at both the gateway and the cloud levels thus avoids long waiting time. To fully unleash the potential of Async-HFL in converging speed under system heterogeneities and stragglers, we design device selection at the gateway level and device-ga
    
[^131]: 谁应该进行预测？学习推迟让人类决策的精确算法

    Who Should Predict? Exact Algorithms For Learning to Defer to Humans. (arXiv:2301.06197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06197](http://arxiv.org/abs/2301.06197)

    本论文提出了一种人工智能分类器推迟预测并交给人类决策者作出决策的算法，在可实现的设置下，获得低误差的线性组合是NP难问题，作者提出了一种混合整数线性规划公式来解决问题，同时也提出了一种在实践中表现良好的替代损失函数。

    

    自动化的AI分类器应该能够推迟预测，让人类决策者做出决策来确保更准确的预测。在这项工作中，我们联合训练一个分类器和一个拒绝器，拒绝器会在每个数据点上决定分类器或人类应该进行预测。我们展示了先前的方法可能无法找到一个具有低误差的人工智能系统和人类，即使存在具有零误差的线性分类器和拒绝器（可实现的设置）。我们证明，即使问题可实现，获得低误差的线性组合也是NP难问题。为了补充这一负面结果，我们提供了一个混合整数线性规划（MILP）公式，可以在线性设置下最优地解决该问题。然而，MILP只适用于中等规模的问题。因此，我们提供了一种新的替代损失函数，它是可实现一致的，并在实践中表现良好。我们在一组全面的数据集上测试了我们的方法，并与广泛的基准进行了比较。

    Automated AI classifiers should be able to defer the prediction to a human decision maker to ensure more accurate predictions. In this work, we jointly train a classifier with a rejector, which decides on each data point whether the classifier or the human should predict. We show that prior approaches can fail to find a human-AI system with low misclassification error even when there exists a linear classifier and rejector that have zero error (the realizable setting). We prove that obtaining a linear pair with low error is NP-hard even when the problem is realizable. To complement this negative result, we give a mixed-integer-linear-programming (MILP) formulation that can optimally solve the problem in the linear setting. However, the MILP only scales to moderately-sized problems. Therefore, we provide a novel surrogate loss function that is realizable-consistent and performs well empirically. We test our approaches on a comprehensive set of datasets and compare to a wide range of bas
    
[^132]: 通过超参优化方案实现自主赛车系统的模型参数识别

    Model Parameter Identification via a Hyperparameter Optimization Scheme for Autonomous Racing Systems. (arXiv:2301.01470v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.01470](http://arxiv.org/abs/2301.01470)

    本文提出了基于超参优化方案的模型参数识别方法（MIHO），并实现了AV-21全尺寸自主赛车的模型参数识别。MIHO收敛速度比传统方法快13倍以上，参数模型具有良好适应性和泛化能力，车辆在场地测试中达到了217公里/小时的高速行驶和稳定避障性能。

    

    本文提出了一种通过超参数优化方案（MIHO）实现模型参数识别的方法。我们采用高效的探索-利用策略来以数据驱动的方式识别动态模型的参数。我们利用MIHO进行AV-21全尺寸自主赛车的模型参数识别。我们将优化后的参数融入我们平台的基于模型的规划和控制系统的设计中。实验结果显示，MIHO的收敛速度比传统的参数识别方法快13倍以上。此外，通过MIHO学习到的参数模型表现出良好的适应性和泛化能力。我们还进行了广泛的场地测试，证明了我们基于模型的系统具有稳定的避障性能和高速行驶性能，在印第安纳波利斯车速公园和拉斯维加斯车速公园的测试中达到了217公里/小时的高速行驶。

    In this letter, we propose a model parameter identification method via a hyperparameter optimization scheme (MIHO). Our method adopts an efficient explore-exploit strategy to identify the parameters of dynamic models in a data-driven optimization manner. We utilize MIHO for model parameter identification of the AV-21, a full-scaled autonomous race vehicle. We then incorporate the optimized parameters for the design of model-based planning and control systems of our platform. In experiments, MIHO exhibits more than 13 times faster convergence than traditional parameter identification methods. Furthermore, the parametric models learned via MIHO demonstrate good fitness to the given datasets and show generalization ability in unseen dynamic scenarios. We further conduct extensive field tests to validate our model-based system, demonstrating stable obstacle avoidance and high-speed driving up to 217 km/h at the Indianapolis Motor Speedway and Las Vegas Motor Speedway. The source code for M
    
[^133]: 特征归因的不可能定理

    Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11870](http://arxiv.org/abs/2212.11870)

    本文阐述了对于中等规模的模型类，任何完整的线性特征归因方法都无法胜任推断模型行为的改进，启示我们在具体定义最终任务的重要性方面要汲取经验，采用重复模型评估的简单直接方法可以胜过许多其他复杂的特征归因方法。

    

    尽管有许多可产生合理解释的可解释性方法，但该领域也经验性地看到了许多失败案例。鉴于这些结果，对于实践者如何以原则性方式使用这些方法并在它们之间进行选择仍不清楚。在本文中，我们展示了对于中等规模的模型类（神经网络容易满足），任何完整的线性特征归因方法（例如Integrated Gradients和SHAP）可以被证明对于推断模型行为的改进都无法胜任。我们的结果适用于常见的最终任务，如描述局部模型行为、识别虚假特征和算法回溯。我们工作的一个重要启示是具体定义最终任务的重要性：一旦这样的最终任务被定义，一个简单和直接的方法——重复模型评估——可以胜过许多其他复杂的特征归因方法。

    Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.
    
[^134]: 多源数据用于离线强化学习的行为估计

    Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning. (arXiv:2211.16078v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16078](http://arxiv.org/abs/2211.16078)

    本文提出了一种用于离线强化学习的多源数据行为估计方法，通过潜变量模型推断策略来克服数据异构性导致的行为错误。

    

    离线强化学习由于数据效率高而备受关注。本文关注于行为估计，这是许多离线强化学习算法的基础任务。行为估计的目标是估计生成训练数据的策略。本文考虑了从多个来源收集数据的场景。在这种情况下，忽略数据异构性，现有的行为估计方法会出现行为错误。为了克服这一缺陷，本文提出了一个潜变量模型来从数据中推断一组策略，使代理能够使用最能描述特定轨迹的策略作为行为策略。该模型为多源数据提供了代理的精细化描述，并帮助它克服行为错误。本文还为该模型提出了一个学习算法，并通过扩展现有的离线强化学习算法Soft Actor-Critic（SAC）来处理多源数据，说明了它的实际用途。实验结果表明，与现有最先进的替代方法相比，所提出的方法是有效的。

    Offline reinforcement learning (RL) have received rising interest due to its appealing data efficiency. The present study addresses behavior estimation, a task that lays the foundation of many offline RL algorithms. Behavior estimation aims at estimating the policy with which training data are generated. In particular, this work considers a scenario where the data are collected from multiple sources. In this case, neglecting data heterogeneity, existing approaches for behavior estimation suffers from behavior misspecification. To overcome this drawback, the present study proposes a latent variable model to infer a set of policies from data, which allows an agent to use as behavior policy the policy that best describes a particular trajectory. This model provides with a agent fine-grained characterization for multi-source data and helps it overcome behavior misspecification. This work also proposes a learning algorithm for this model and illustrates its practical usage via extending an 
    
[^135]: 集成多分位数算法：自适应灵活的分布预测用于不确定性量化

    Ensemble Multi-Quantiles: Adaptively Flexible Distribution Prediction for Uncertainty Quantification. (arXiv:2211.14545v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14545](http://arxiv.org/abs/2211.14545)

    本文提出了一种自适应灵活的分布预测方法EMQ，用于量化机器学习中的不确定性。该方法逐步偏离高斯分布并在提升中发现最优条件分布，因此具有较好的实用性。

    

    本文提出了一种新颖、简洁而有效的方法来量化机器学习中的不确定性。它结合了自适应灵活的分布预测，用于回归任务中的条件分布$\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$预测。通过将概率水平的分位数（覆盖区间$(0,1)$）用由我们设计的加法模型提升，来预测这个条件分布。我们寻求$\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$的结构完整性和灵活性之间的自适应平衡，而高斯假设对于真实数据的灵活性不足，高度灵活的方法（例如在没有分布结构的情况下分别估计分位数）不可避免地具有缺陷，并且可能导致无法很好地概括。我们提出的集成多分位数方法EMQ完全是数据驱动的，可以逐步偏离高斯分布并在提升中发现最优条件分布。

    We propose a novel, succinct, and effective approach to quantify uncertainty in machine learning. It incorporates adaptively flexible distribution prediction for $\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$ in regression tasks. For predicting this conditional distribution, its quantiles of probability levels spreading the interval $(0,1)$ are boosted by additive models which are designed by us with intuitions and interpretability. We seek an adaptive balance between the structural integrity and the flexibility for $\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$, while Gaussian assumption results in a lack of flexibility for real data and highly flexible approaches (e.g., estimating the quantiles separately without a distribution structure) inevitably have drawbacks and may not lead to good generalization. This ensemble multi-quantiles approach called EMQ proposed by us is totally data-driven, and can gradually depart from Gaussian and discover the optimal conditional distribution in the boosting. On ex
    
[^136]: “我们是否确信这是异常？（arXiv:2211.09224v3 [cs.LG] UPDATED）”

    Are we certain it's anomalous?. (arXiv:2211.09224v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09224](http://arxiv.org/abs/2211.09224)

    本论文提出了一种新的异常检测方法HypAD，它采用超几何不确定性来评估异常，利用无监督学习来重新建立输入信号，这种方法可以有效解决时间序列中的异常检测问题。

    

    最近，对于时间序列和结构化数据序列的建模取得了进展，这大大推进了异常检测领域的研究。该任务用于识别金融序列、IT系统、航天测量以及医疗领域中的异常行为，其中异常检测可以帮助隔离出抑郁症和老年人的特殊病例。由于非常不确定性的时间相关性，以及异常的定义有时是主观的，所以时间序列中的异常检测是一项复杂的任务。我们在这里提出了一个新的方法，即使用“超几何不确定性”进行异常检测（HypAD）。HypAD通过无监督学习来重建输入信号。我们采用现有技术的最佳实践，通过LSTM建立序列，同时学习解码器来重建信号，并借助GAN评论家的帮助。利用超几何神经网络来进行端到端的不确定性估计。通过使用不确定性，HypAD可以评估是否存在异常。

    The progress in modelling time series and, more generally, sequences of structured data has recently revamped research in anomaly detection. The task stands for identifying abnormal behaviors in financial series, IT systems, aerospace measurements, and the medical domain, where anomaly detection may aid in isolating cases of depression and attend the elderly. Anomaly detection in time series is a complex task since anomalies are rare due to highly non-linear temporal correlations and since the definition of anomalous is sometimes subjective. Here we propose the novel use of Hyperbolic uncertainty for Anomaly Detection (HypAD). HypAD learns self-supervisedly to reconstruct the input signal. We adopt best practices from the state-of-the-art to encode the sequence by an LSTM, jointly learned with a decoder to reconstruct the signal, with the aid of GAN critics. Uncertainty is estimated end-to-end by means of a hyperbolic neural network. By using uncertainty, HypAD may assess whether it is
    
[^137]: 迈向自适应语义通信：基于在线学习非线性变换源通道编码的高效数据传输

    Toward Adaptive Semantic Communications: Efficient Data Transmission via Online Learned Nonlinear Transform Source-Channel Coding. (arXiv:2211.04339v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2211.04339](http://arxiv.org/abs/2211.04339)

    本论文的主要贡献是提出了一种基于深度学习模型的在线学习联合源和信道编码方法，通过利用模型的过度拟合能力，提高模型的适应性，从而在实际应用中实现更高效的数据传输。

    

    新兴的语义通信领域推动了端到端数据传输的研究。通过利用深度学习模型的强大表示能力，学习的数据传输方案表现出比已有的源码和信道编码方法更优越的性能。然而，迄今为止，研究重点主要集中在架构和模型改进方面，朝向静态目标域。尽管这些学习模型取得了成功，但由于模型容量的限制和不完美的优化和推广，特别是在测试数据分布或信道响应与模型训练不同的情况下，它们仍然是次优的，而这在实际情况下很可能发生。为了解决这个问题，我们提出了一种新颖的在线学习联合源和信道编码方法，利用了深度学习模型的过度拟合属性。具体而言，我们以轻量级在线方式更新部署后的现成预训练模型。

    The emerging field semantic communication is driving the research of end-to-end data transmission. By utilizing the powerful representation ability of deep learning models, learned data transmission schemes have exhibited superior performance than the established source and channel coding methods. While, so far, research efforts mainly concentrated on architecture and model improvements toward a static target domain. Despite their successes, such learned models are still suboptimal due to the limitations in model capacity and imperfect optimization and generalization, particularly when the testing data distribution or channel response is different from that adopted for model training, as is likely to be the case in real-world. To tackle this, we propose a novel online learned joint source and channel coding approach that leverages the deep learning model's overfitting property. Specifically, we update the off-the-shelf pre-trained models after deployment in a lightweight online fashion
    
[^138]: 音乐混音风格转换：采用对比学习方法解开音频效果的紧密联系

    Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects. (arXiv:2211.02247v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.02247](http://arxiv.org/abs/2211.02247)

    本论文提出了一种对比学习方法，用于实现音乐混音风格转换。该方法采用已处理的数据集进行自监督训练，并使用编码器从参考歌曲中提取仅与音频效果相关的信息。系统实现了多轨音频的混音风格转换，并且在使用音乐源分离模型时具有鲁棒性。

    

    我们提出了一种端到端的音乐混音风格转换系统，将输入多轨混音的混音风格转换为参考歌曲的风格。这是通过使用一个预先经过对比目标训练的编码器来实现的，该编码器从参考音乐录音中提取仅与音频效果相关的信息。我们所有的模型都是自监督方式训练的，使用一种有效的数据预处理方法从已处理的湿度多轨数据集中缓解了获取未处理干燥数据的数据稀缺性。我们对所提出的编码器进行分析，以确定其分离音频效果的能力，并通过客观和主观评估验证其混音风格转换的性能。从结果中，我们展示了所提出的系统不仅可以将多轨音频的混音风格转换为参考风格，而且在使用音乐源分离模型时也具有混合风格转换的鲁棒性。

    We propose an end-to-end music mixing style transfer system that converts the mixing style of an input multitrack to that of a reference song. This is achieved with an encoder pre-trained with a contrastive objective to extract only audio effects related information from a reference music recording. All our models are trained in a self-supervised manner from an already-processed wet multitrack dataset with an effective data preprocessing method that alleviates the data scarcity of obtaining unprocessed dry data. We analyze the proposed encoder for the disentanglement capability of audio effects and also validate its performance for mixing style transfer through both objective and subjective evaluations. From the results, we show the proposed system not only converts the mixing style of multitrack audio close to a reference but is also robust with mixture-wise style transfer upon using a music source separation model.
    
[^139]: 语言控制扩散：通过空间、时间和任务高效扩展

    Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15629](http://arxiv.org/abs/2210.15629)

    本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。

    

    训练通用型智能体在各个方面都很困难，需要处理高维输入（空间）、长时间跨度（时间）和多个新任务。最近的结构方面的进展使得我们可以沿着其中一个或两个维度提高扩展性能力，但计算成本仍然很高。本文提出使用语言控制扩散模型作为一种基于自然语言条件的分层规划器（LCD）来应对这三个方面。我们有效而高效地扩展扩散模型，以应对时间、状态和任务空间维度的长时间跨度控制问题。我们在CALVIN语言机器人基准测试中将LCD与其他最先进的模型进行比较，发现LCD在多任务成功率方面优于其他最先进的方法，而单任务成功率（SR）为88.7%，远高于以前的最佳成绩82.6%，大大提高了计算效率。

    Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
    
[^140]: 冻结再训练：在虚假相关和特征噪声下实现可证明的表示学习

    Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise. (arXiv:2210.11075v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11075](http://arxiv.org/abs/2210.11075)

    本文提出了一种称为冻结再变换(FTT)的算法，用于在存在虚假相关和特征噪声下实现可证明的表示学习。该算法首先冻结特征学习器，然后在其上训练分类器，利用学习到的核心特征，经过实验证明其有效性。

    

    在训练环境中存在虚假相关，如图像背景，可能使经验风险最小化方法(ERM)在测试环境中表现不佳。为了解决这个问题，Kirichenko等人(2022) 实证发现，即使存在虚假相关，与结果相关的核心特征仍然可以很好地学习。这开启了一种有前途的策略，即首先训练特征学习器而不是分类器，然后在测试环境中进行线性探测(重训练最后一层)。然而，缺乏一个理论上的理解何时以及为什么这种方法有效。在本文中，我们发现只有当与结果相关的核心特征关联的不可实现噪声小于虚假特征的噪声时，才能很好地学习这些特征，这在实践中并不一定成立。我们提供理论和实验证据支持这个发现，并阐述不可实现噪声的重要性。此外，我们提出了一种称为冻结再变换(FTT)的算法，首先冻结特征学习器，然后在其上训练分类器，利用学习到的核心特征。我们证明了FTT在特征学习器上的一个温和条件下保证有界的泛化误差。实验证明了FTT在各种数据集和虚假相关以及特征噪声设置下的有效性。

    The existence of spurious correlations such as image backgrounds in the training environment can make empirical risk minimization (ERM) perform badly in the test environment. To address this problem, Kirichenko et al. (2022) empirically found that the core features that are related to the outcome can still be learned well even with the presence of spurious correlations. This opens a promising strategy to first train a feature learner rather than a classifier, and then perform linear probing (last layer retraining) in the test environment. However, a theoretical understanding of when and why this approach works is lacking. In this paper, we find that core features are only learned well when their associated non-realizable noise is smaller than that of spurious features, which is not necessarily true in practice. We provide both theories and experiments to support this finding and to illustrate the importance of non-realizable noise. Moreover, we propose an algorithm called Freeze then T
    
[^141]: 锐度感知优化的动态：从峡谷反弹到漂向宽极小值

    The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima. (arXiv:2210.01513v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01513](http://arxiv.org/abs/2210.01513)

    本文研究了一种梯度优化方法SAM，发现在凸二次目标中它会在最小值两侧来回振荡，但在非二次情况中从光谱范数的角度执行梯度下降，更新方式被认为是Hessian矩阵在领先特征向量方向上的导数，鼓励漂移向更宽的极小值。

    

    本文研究了一种名为锐度感知优化（SAM）的梯度优化方法，该方法在图像和语言预测问题上表现出较好的性能。我们表明，当使用SAM应用于凸二次目标时，针对大多数随机初始化，它会收敛于在沿着主曲率最大方向的最小值两侧来回振荡的循环，并给出了收敛率的界限。在非二次情况下，我们表明这种振荡实质上是在Hessian矩阵的光谱范数上以更小的步长执行梯度下降，SAM的更新可被认为是一个三阶导数 - 即Hessian矩阵在领先的特征向量方向上的导数，它鼓励朝着更宽的极小值漂移。

    We consider Sharpness-Aware Minimization (SAM), a gradient-based optimization method for deep networks that has exhibited performance improvements on image and language prediction problems. We show that when SAM is applied with a convex quadratic objective, for most random initializations it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature, and we provide bounds on the rate of convergence.  In the non-quadratic case, we show that such oscillations effectively perform gradient descent, with a smaller step-size, on the spectral norm of the Hessian. In such cases, SAM's update may be regarded as a third derivative -the derivative of the Hessian in the leading eigenvector direction -- that encourages drift toward wider minima.
    
[^142]: 小型非独立同分布客户数据上的双重编码模型联邦训练

    Federated Training of Dual Encoding Models on Small Non-IID Client Datasets. (arXiv:2210.00092v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00092](http://arxiv.org/abs/2210.00092)

    本论文研究了在小型非独立同分布客户数据上的双重编码模型的联邦训练问题，提出了一种叫做DCCO的方法，通过联邦平均和交叉相关损失来训练双重编码模型，在两个机器学习任务中证明了该方法有效性的提高。

    

    双重编码模型广泛用于表示学习，通过最大化中心化训练数据中编码对的一致性来训练。然而，在许多情况下，由于隐私问题，数据集在许多客户端（用户设备或组织）上本质上是分散的，这促进了联邦学习。本文关注在许多小型非独立同分布的客户数据上实现双重编码模型的联邦训练。我们发现现有的在中心化环境中表现良好的方法在使用联合平均方法进行简单改进时表现糟糕。我们观察到，对于基于编码统计的损失函数，可以在各个客户端上模拟大批量损失计算。基于这一洞见，我们提出了一种新的联邦训练方法，分布式交叉相关优化（DCCO），它使用交叉相关损失和客户端更新的联邦平均来训练双重编码模型。我们在两个机器学习任务上评估了我们的方法，并证明它相对于现有的联邦学习方法在提高性能方面更有效。

    Dual encoding models that encode a pair of inputs are widely used for representation learning. Many approaches train dual encoding models by maximizing agreement between pairs of encodings on centralized training data. However, in many scenarios, datasets are inherently decentralized across many clients (user devices or organizations) due to privacy concerns, motivating federated learning. In this work, we focus on federated training of dual encoding models on decentralized data composed of many small, non-IID (independent and identically distributed) client datasets. We show that existing approaches that work well in centralized settings perform poorly when naively adapted to this setting using federated averaging. We observe that, we can simulate large-batch loss computation on individual clients for loss functions that are based on encoding statistics. Based on this insight, we propose a novel federated training approach, Distributed Cross Correlation Optimization (DCCO), which trai
    
[^143]: 自稳定性：梯度下降在稳定边缘的隐式偏差

    Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability. (arXiv:2209.15594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15594](http://arxiv.org/abs/2209.15594)

    传统梯度下降分析不适用于现代神经网络用全批次或大批次梯度下降训练，Cohen等人(2021)发现的梯度下降边缘稳定性现象表明，当锐度达到不稳定性截止值$2/\eta$时，迭代具有自稳定性，并表现出隐式偏向稳定边缘解的偏差，这种现象通过捕获二阶和三阶导数的比例系数得到了解释。

    

    传统的梯度下降分析表明，当Hessian矩阵的最大特征值，也称为锐度$S(\theta)$，被$2/\eta$限制时，训练是“稳定的”，训练损失单调下降。然而，最近的研究发现，当用全批量或大批量梯度下降训练现代神经网络时，这种假设不成立。最近，Cohen等人(2021)观察到了两个重要现象。第一个现象被称为渐进锐化，在训练期间锐度稳步增加，直到达到不稳定性截止值$2/\eta$。第二个现象被称为稳定边缘，在剩余的训练过程中，锐度停留在$2/\eta$，而损失则持续下降，尽管不是单调的。我们证明了，在稳定边缘处，梯度下降的动态可以由一个三次泰勒展开式捕获:当迭代在Hessian矩阵的最大特征向量方向上发散时，锐度呈二次增长，比例系数由损失的二阶和三阶导数决定。当锐度精确为$2/\eta$时，我们表明，迭代的自稳定性可以永久地保持在稳定边缘。此外，我们通过实验证明，这种自稳定现象使梯度下降具有隐式偏向稳定边缘解的偏差，而稳定边缘是损失明显低于稳定区域的区域。

    Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\theta)$, is bounded by $2/\eta$, training is "stable" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\eta$. The second, dubbed edge of stability, is that the sharpness hovers at $2/\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessi
    
[^144]: TRBoost: 基于信赖域方法的通用梯度提升机

    TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method. (arXiv:2209.13791v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.13791](http://arxiv.org/abs/2209.13791)

    TRBoost 是一种新型通用梯度提升机，使用约束二次模型来近似目标并应用信赖域算法来获得新的学习器，具有适用于任意损失函数的通用性和竞争性能。

    

    梯度提升机 (GBMs) 利用函数空间的泰勒展开显著成功地解决了各种问题。然而，性能和通用性之间的平衡对 GBMs 提出了挑战。尤其是，基于梯度下降的 GBMs 使用一阶泰勒展开以确保适用于所有损失函数，而基于牛顿方法的 GBMs 利用正定的黑塞矩阵获得卓越的性能，但以牺牲通用性为代价。为解决这个问题，本研究提出了一种新型的通用梯度提升机，称为 TRBoost。在每次迭代中，TRBoost 使用一个约束二次模型来近似目标并应用信赖域算法来解决它并获得一个新的学习器。与基于牛顿方法的 GBMs 不同，TRBoost 不要求黑塞矩阵是正定的，因此允许它用于任意损失函数，同时仍保持竞争性能。

    Gradient Boosting Machines (GBMs) have demonstrated remarkable success in solving diverse problems by utilizing Taylor expansions in functional space. However, achieving a balance between performance and generality has posed a challenge for GBMs. In particular, gradient descent-based GBMs employ the first-order Taylor expansion to ensure applicability to all loss functions, while Newton's method-based GBMs use positive Hessian information to achieve superior performance at the expense of generality. To address this issue, this study proposes a new generic Gradient Boosting Machine called Trust-region Boosting (TRBoost). In each iteration, TRBoost uses a constrained quadratic model to approximate the objective and applies the Trust-region algorithm to solve it and obtain a new learner. Unlike Newton's method-based GBMs, TRBoost does not require the Hessian to be positive definite, thereby allowing it to be applied to arbitrary loss functions while still maintaining competitive performan
    
[^145]: 从互补标签学习到概率估计的简化

    Reduction from Complementary-Label Learning to Probability Estimates. (arXiv:2209.09500v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.09500](http://arxiv.org/abs/2209.09500)

    本论文提出了一种新的对互补标签学习的简化方法，它减少到互补类的概率估计，通过一个简单的解码步骤，准确的互补标签概率估计可以产生良好的分类器。

    

    互补标签学习是一个弱监督学习问题，旨在仅从互补标签中学习多类分类器，其指示实例不属于的类。现有方法主要采用减少到普通分类的范例，对CLL进行特定转换和替代损失。然而，这些方法面临着几个限制，例如过拟合的倾向或深度模型的偏见。在本文中，我们通过一种新颖的视角-减少到互补类的概率估计，规避了这些限制。我们证明了准确的互补标签概率估计通过简单的解码步骤产生良好的分类器。该证明建立了一个从CLL到概率估计的简化框架。该框架提供了解释几个关键CLL方法的特殊情况的方法，并允许我们设计一种改进的算法，这种算法在某些情况下比现有的CLL方法更具优越性。

    Complementary-Label Learning (CLL) is a weakly-supervised learning problem that aims to learn a multi-class classifier from only complementary labels, which indicate a class to which an instance does not belong. Existing approaches mainly adopt the paradigm of reduction to ordinary classification, which applies specific transformations and surrogate losses to connect CLL back to ordinary classification. Those approaches, however, face several limitations, such as the tendency to overfit or be hooked on deep models. In this paper, we sidestep those limitations with a novel perspective--reduction to probability estimates of complementary classes. We prove that accurate probability estimates of complementary labels lead to good classifiers through a simple decoding step. The proof establishes a reduction framework from CLL to probability estimates. The framework offers explanations of several key CLL approaches as its special cases and allows us to design an improved algorithm that is mor
    
[^146]: 使用Broyden超梯度的PDE约束优化的双层物理信息神经网络 (arXiv:2209.07075v4 [cs.LG] UPDATED)

    Bi-level Physics-Informed Neural Networks for PDE Constrained Optimization using Broyden's Hypergradients. (arXiv:2209.07075v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07075](http://arxiv.org/abs/2209.07075)

    本文提出了一种解决具有复杂或非线性依赖关系的PDE约束优化问题的新颖双层优化框架，并用Broyden's方法逼近超梯度，取得了最先进的结果。

    

    基于深度学习的方法，如物理信息神经网络（PINNs）和DeepONets已经显示出解决PDE约束优化（PDECO）问题的潜力。然而，现有方法不足以处理那些PDE约束在优化目标上具有复杂或非线性依赖关系的情况。为了解决这个挑战，我们提出了一个新颖的双层优化框架，通过分离目标和约束的优化来解决。对于内部循环优化，我们采用PINNs仅解决PDE约束。对于外部循环，我们设计了一种新的方法，该方法基于隐式函数定理（IFT）使用Broyden方法，该方法在逼近超梯度方面高效准确。此外，我们进一步提供了超梯度计算的理论解释和误差分析。在多个大规模和非线性PDE约束优化问题的广泛实验中，我们的方法取得了最先进的结果。

    Deep learning based approaches like Physics-informed neural networks (PINNs) and DeepONets have shown promise on solving PDE constrained optimization (PDECO) problems. However, existing methods are insufficient to handle those PDE constraints that have a complicated or nonlinear dependency on optimization targets. In this paper, we present a novel bi-level optimization framework to resolve the challenge by decoupling the optimization of the targets and constraints. For the inner loop optimization, we adopt PINNs to solve the PDE constraints only. For the outer loop, we design a novel method by using Broyden's method based on the Implicit Function Theorem (IFT), which is efficient and accurate for approximating hypergradients. We further present theoretical explanations and error analysis of the hypergradients computation. Extensive experiments on multiple large-scale and nonlinear PDE constrained optimization problems demonstrate that our method achieves state-of-the-art results compar
    
[^147]: 基于单一样本路径的均场博弈无需oracle的强化学习

    Oracle-free Reinforcement Learning in Mean-Field Games along a Single Sample Path. (arXiv:2208.11639v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11639](http://arxiv.org/abs/2208.11639)

    本文中提出了一种基于单一智能体样本路径的“沙箱学习”算法，可以在多智能体非合作的环境下作为预热开始。算法不需要均场oracle，样本复杂度为$\tilde{\mathcal{O}}(\epsilon^{-4})$。

    

    本文考虑均场博弈中的在线强化学习。与传统方法不同的是，我们通过开发一种算法来利用单个智能体的样本路径近似均场平衡(MFE)，从而消除了均场oracle的需求。我们将其称为“沙箱学习”，因为任何在多智能体非合作设置中学习的代理都可以使用它作为预热开始。我们采用两个时间尺度的方法，在较慢的时间尺度上进行均场的在线固定点递归，与普通代理的更快时间尺度上的控制策略更新一起操作。鉴于代理的基本马尔可夫决策过程(MDP)是连通的，我们提供了保证均场和控制策略收敛于均场平衡的有限样本收敛保证。沙箱学习算法的样本复杂度是$\tilde{\mathcal{O}}(\epsilon^{-4})$，其中$\epsilon$是MFE近似误差。

    We consider online reinforcement learning in Mean-Field Games (MFGs). Unlike traditional approaches, we alleviate the need for a mean-field oracle by developing an algorithm that approximates the Mean-Field Equilibrium (MFE) using the single sample path of the generic agent. We call this {\it Sandbox Learning}, as it can be used as a warm-start for any agent learning in a multi-agent non-cooperative setting. We adopt a two time-scale approach in which an online fixed-point recursion for the mean-field operates on a slower time-scale, in tandem with a control policy update on a faster time-scale for the generic agent. Given that the underlying Markov Decision Process (MDP) of the agent is communicating, we provide finite sample convergence guarantees in terms of convergence of the mean-field and control policy to the mean-field equilibrium. The sample complexity of the Sandbox learning algorithm is $\tilde{\mathcal{O}}(\epsilon^{-4})$ where $\epsilon$ is the MFE approximation error. Thi
    
[^148]: SCALE：无先验知识的在线自监督终身学习

    SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge. (arXiv:2208.11266v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11266](http://arxiv.org/abs/2208.11266)

    本文提出了一个更加实用的在线自监督终身学习的问题设置，该设置具有挑战性，因为它涉及到数据的非独立同分布和单次遍历、缺乏外部监督和先验知识等。为了解决这些问题，我们提出了SCALE方法，它可以纯粹地从数据连续体中提取和记忆表示。

    

    无监督的终身学习是指能够在没有监督的情况下在时间上学习新的模式并记忆以前的模式。尽管在这个方向上取得了很大的进展，但现有的工作通常假设有关输入数据的强先验知识（例如，已知类别边界），而这在复杂和不可预测的环境中可能是不可能获得的。为了解决这个问题，本文提出了在线自监督终身学习无先验知识的更实践的问题设置。为了应对挑战，我们提出了一种名为SCALE的自监督对比终身学习方法，它可以纯粹地从数据连续体中提取和记忆表示。

    Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries), which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on the fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgettin
    
[^149]: LogLG: 通过事件图构建实现弱监督日志异常检测

    LogLG: Weakly Supervised Log Anomaly Detection via Log-Event Graph Construction. (arXiv:2208.10833v5 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2208.10833](http://arxiv.org/abs/2208.10833)

    本文提出了一种LogLG弱监督日志异常检测框架，采用事件图构建和伪标签生成，旨在探索序列中关键字之间的语义联系，有效识别应用程序日志的异常行为。

    

    完全监督的日志异常检测方法需要为大量未标记的日志数据进行标注。最近，许多半监督方法已经被提出，以降低标注成本，并借助解析模板。然而，这些方法独立考虑每个关键字，忽略关键字之间的相关性和日志序列之间的上下文关系。本文提出了一种新型的弱监督日志异常检测框架，名为LogLG，以探索序列中关键字之间的语义联系。具体来说，我们设计了一种端到端的迭代过程，首先提取未标记日志的关键字以构建日志事件图。然后，我们构建了一个子图注释器生成未标记日志序列的伪标签。为了改善注释质量，我们采用了自监督任务来预训练子图注释器。之后，我们使用生成的伪标签训练检测模型。在条件 下，模型能够识别来自应用程序日志的异常行为。

    Fully supervised log anomaly detection methods suffer the heavy burden of annotating massive unlabeled log data. Recently, many semi-supervised methods have been proposed to reduce annotation costs with the help of parsed templates. However, these methods consider each keyword independently, which disregards the correlation between keywords and the contextual relationships among log sequences. In this paper, we propose a novel weakly supervised log anomaly detection framework, named LogLG, to explore the semantic connections among keywords from sequences. Specifically, we design an end-to-end iterative process, where the keywords of unlabeled logs are first extracted to construct a log-event graph. Then, we build a subgraph annotator to generate pseudo labels for unlabeled log sequences. To ameliorate the annotation quality, we adopt a self-supervised task to pre-train a subgraph annotator. After that, a detection model is trained with the generated pseudo labels. Conditioned on the cl
    
[^150]: DIET: 利用剩余信息的边际相关度量进行条件独立性检验

    DIET: Conditional independence testing with marginal dependence measures of residual information. (arXiv:2208.08579v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.08579](http://arxiv.org/abs/2208.08579)

    本文提出了一种名为解耦的独立性检验（DIET）算法，通过利用边际独立统计量测试条件独立关系，避免了需要大量预测模型的拟合和相互作用的启发式方法所导致的损失功率问题。

    

    条件随机化检验（CRT）用于评估变量$x$在已知协变量$z$的情况下对另一个变量$y$的预测能力。CRT通常需要大量预测模型的拟合，这通常是计算上不可行的。现有的解决方案通常将数据集分成训练和测试部分，或依靠相互作用的启发式方法，这两种方法都会导致功率损失。本文提出了解耦的独立性检验（DIET）算法，通过利用边际独立统计量测试条件独立关系，避免了这两种问题。DIET测试两个随机变量的边际独立性：$F(x \mid z)$和$F(y \mid z)$其中$F(\cdot \mid z)$是条件累积分布函数（CDF），这些变量被称为“信息残差”。我们给出了DIET实现有限样本的类型1错误控制和功率大于类型1错误率的充分条件。然后，我们证明当DIET应用时，数据的分布不需要满足任何特定要求。

    Conditional randomization tests (CRTs) assess whether a variable $x$ is predictive of another variable $y$, having observed covariates $z$. CRTs require fitting a large number of predictive models, which is often computationally intractable. Existing solutions to reduce the cost of CRTs typically split the dataset into a train and test portion, or rely on heuristics for interactions, both of which lead to a loss in power. We propose the decoupled independence test (DIET), an algorithm that avoids both of these issues by leveraging marginal independence statistics to test conditional independence relationships. DIET tests the marginal independence of two random variables: $F(x \mid z)$ and $F(y \mid z)$ where $F(\cdot \mid z)$ is a conditional cumulative distribution function (CDF). These variables are termed "information residuals." We give sufficient conditions for DIET to achieve finite sample type-1 error control and power greater than the type-1 error rate. We then prove that when 
    
[^151]: MENLI: 自然语言推理的鲁棒性评估指标

    MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07316](http://arxiv.org/abs/2208.07316)

    本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。

    

    最近被提出的基于BERT的文本生成评估指标在标准基准测试中表现良好，但易受到对信息正确性的攻击。我们认为这部分原因是此类模型是基于语义相似性建模的。相反，我们提出一种基于自然语言推理（NLI）的鲁棒性评估指标，这种指标更适合建模。我们设计了一种基于偏好的对抗性攻击框架，并表明我们的NLI基础指标比最近的BERT基础指标更具鲁棒性。在标准基准测试中，我们的NLI基础指标优于现有的摘要评估指标，但低于SOTA MT指标。然而，在现有指标与我们的NLI指标相结合时，我们既获得了更高的对抗鲁棒性（15％-30％），又获得了标准基准测试中更高的质量指标（+5％至30％）。

    Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
    
[^152]: 基于生物识别击键数据生成的 BeCAPTCHA 类型，用于改进机器人检测

    BeCAPTCHA-Type: Biometric Keystroke Data Generation for Improved Bot Detection. (arXiv:2207.13394v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13394](http://arxiv.org/abs/2207.13394)

    本研究提出了一种基于生物识别击键数据生成的方法，通过合成击键生物识别数据来改进基于击键的机器人检测系统的训练过程，实验证明了合成样本的真实性并表明在大量标记数据场景中，这种方法表现出色。

    

    本研究提出了一种数据驱动的学习模型，用于合成击键生物识别数据。该方法与基于通用模型和用户相关模型的两种统计方法进行比较。这些方法通过使用合成击键数据来改进基于击键的机器人检测系统的训练过程。实验使用 136 百万个来自 168 千个受试者的击键事件数据集来进行。我们通过定性和定量实验分析了三种合成方法的性能。考虑了基于几种受监督分类器（支持向量机，随机森林，高斯朴素贝叶斯和长短期记忆网络）和包括人类和合成样本的学习框架的不同机器人检测器。实验证明了合成样本的真实性。分类结果表明，在具有大量标记数据的场景中，这些合成样本的表现相当出色。

    This work proposes a data driven learning model for the synthesis of keystroke biometric data. The proposed method is compared with two statistical approaches based on Universal and User-dependent models. These approaches are validated on the bot detection task, using the keystroke synthetic data to improve the training process of keystroke-based bot detection systems. Our experimental framework considers a dataset with 136 million keystroke events from 168 thousand subjects. We have analyzed the performance of the three synthesis approaches through qualitative and quantitative experiments. Different bot detectors are considered based on several supervised classifiers (Support Vector Machine, Random Forest, Gaussian Naive Bayes and a Long Short-Term Memory network) and a learning framework including human and synthetic samples. The experiments demonstrate the realism of the synthetic samples. The classification results suggest that in scenarios with large labeled data, these synthetic 
    
[^153]: 延迟反馈在广义线性赌博机中的研究再访

    Delayed Feedback in Generalised Linear Bandits Revisited. (arXiv:2207.10786v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10786](http://arxiv.org/abs/2207.10786)

    研究了广义线性赌博机中的延迟奖励现象，提出了一种自然的乐观算法，可实现一个独立于时间的惩罚函数，降低了现有工作中随着时间增长而增加的惩罚函数的界限。

    

    随着许多真实世界的应用中奖励几乎总是被延迟，导致要求即时奖励的模型难以应用。本文将研究在广义线性赌博机中延迟奖励的现象。我们证明了一种自然的乐观算法适应延迟反馈领域能够有一个与时间无关的惩罚函数。这比现有的工作显著的提高了，因为最佳的已知的惩罚函数的界限随着时间的推移而增加。

    The stochastic generalised linear bandit is a well-understood model for sequential decision-making problems, with many algorithms achieving near-optimal regret guarantees under immediate feedback. However, the stringent requirement for immediate rewards is unmet in many real-world applications where the reward is almost always delayed. We study the phenomenon of delayed rewards in generalised linear bandits in a theoretical manner. We show that a natural adaptation of an optimistic algorithm to the delayed feedback achieves a regret bound where the penalty for the delays is independent of the horizon. This result significantly improves upon existing work, where the best known regret bound has the delay penalty increasing with the horizon. We verify our theoretical results through experiments on simulated data.
    
[^154]: 基于能力的多模态课程学习用于医学报告生成

    Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.14579](http://arxiv.org/abs/2206.14579)

    本论文提出了一个基于能力的多模态课程学习框架（CMCL），通过模拟放射学家的学习过程来逐步优化医学报告生成模型，在公共数据集上实验表明CMCL可以有效地提高模型性能。

    

    近年来，医学报告生成任务吸引了越来越多的研究关注，其目标是生成医学图像的连贯描述。与一般的图像字幕任务不同，医学报告生成对数据驱动的神经模型更具挑战性。主要原因是 1) 严重的数据偏差和 2) 有限的医学数据。为了减轻数据偏差并充分利用可用数据，我们提出了基于能力的多模态课程学习框架（CMCL）。具体来说，CMCL 模拟了放射学家的学习过程，并逐步优化模型。首先，CMCL 估计每个训练实例的难度并评估当前模型的能力; 其次，CMCL 选择最适合的训练实例组合以考虑当前模型的能力。通过迭代上述两个步骤，CMCL 可以逐渐提高模型的性能。在公共的IU-Xray和MIMIC-CXR数据集上的实验表明，CMCL 可以有效地提高医学报告生成的性能，并胜过几种最先进的方法。

    Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model's performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMC
    
[^155]: 强化学习、量化相应均衡和二人零和博弈的统一方法

    A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games. (arXiv:2206.05825v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05825](http://arxiv.org/abs/2206.05825)

    本文展示了磁镜面下降算法作为均衡求解器和强化学习方法的优点，包括实现了线性收敛的相应均衡求解器和在表格式设置中实现了与CFR相竞争的标准强化学习算法，以及在“黑暗六角”和“幻象井字”中的自我玩耍深度强化学习算法的良好表现。

    

    本文研究了一种算法，我们称之为磁镜面下降，它受到镜面下降和非欧几里德近端梯度算法的启发。我们的贡献在于展示了磁镜面下降作为均衡求解器以及在两人零和博弈中作为强化学习方法的优点。这些优点包括：1) 成为首个对于具有一阶反馈的扩展形式游戏实现线性收敛的量化相应均衡求解器；2) 成为首个在表格式设置中与CFR实现实验性竞争结果的标准强化学习算法；3) 实现了在3x3黑暗六角和幻象井字游戏中成为自我游戏深度强化学习算法的有利性能。

    This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm. Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games. These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.
    
[^156]: 神经崩溃：建模原则和泛化的综述

    Neural Collapse: A Review on Modelling Principles and Generalization. (arXiv:2206.04041v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04041](http://arxiv.org/abs/2206.04041)

    神经崩溃是当深度分类神经网络训练误差降至零时出现的一种状态，它简化了最后一层的行为，具有最近类中心决策规则。本文综述了神经崩溃的建模原则，以及它对神经网络的泛化和迁移学习能力的影响。

    

    当训练误差降至零并且最终隐藏层输出的类内变异性趋近于零时，深度分类神经网络进入训练终端阶段，往往呈现出引人入胜的神经崩溃特性。神经崩溃实际上代表了一个状态，其中最终隐藏层输出的类均值形成一个简单的等角紧框架。尽管这种状态很简单，但达到它的动态和影响还有待完全理解。在这项工作中，我们回顾了有助于建模神经崩溃的原则，接着研究了这种状态对神经网络的泛化和迁移学习能力的影响。最后，我们讨论了未来研究的潜在途径和方向。

    Deep classifier neural networks enter the terminal phase of training (TPT) when training error reaches zero and tend to exhibit intriguing Neural Collapse (NC) properties. Neural collapse essentially represents a state at which the within-class variability of final hidden layer outputs is infinitesimally small and their class means form a simplex equiangular tight frame. This simplifies the last layer behaviour to that of a nearest-class center decision rule. Despite the simplicity of this state, the dynamics and implications of reaching it are yet to be fully understood. In this work, we review the principles which aid in modelling neural collapse, followed by the implications of this state on generalization and transfer learning capabilities of neural networks. Finally, we conclude by discussing potential avenues and directions for future research.
    
[^157]: 具有局部Lipschitz连续梯度的凸优化的加速一阶方法。

    Accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient. (arXiv:2206.01209v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.01209](http://arxiv.org/abs/2206.01209)

    本文开发了针对具有局部Lipschitz连续梯度的凸优化问题的加速一阶方法，分别提出了无约束凸优化的加速近端梯度算法(APG)以及约束凸优化的一阶近端增广拉格朗日方法。通过这些方法，可以快速地寻找出解决方案。

    

    本文针对具有局部Lipschitz连续梯度的凸优化问题，开发了加速的一阶方法。特别地，我们首先考虑了具有LLCG的无约束凸优化，并提出了加速近端梯度算法(APG)来解决它。所提出的APG方法具有可验证的终止准则，并且在寻找无约束凸优化和强凸优化问题的ε-残差解时，其操作复杂度分别为${\cal O}(\varepsilon^{-1/2}\log \varepsilon^{-1})$和${\cal O}(\log \varepsilon^{-1})$。然后，我们考虑了具有LLCG的约束凸优化，并提出了一种一阶近端增广拉格朗日方法来解决它，通过将我们提出的APG方法应用于近似求解一系列近端增广拉格朗日子问题。由此得出的方法具有可验证的终止准则，并且具有快速的收敛速度。

    In this paper we develop accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient (LLCG), which is beyond the well-studied class of convex optimization with Lipschitz continuous gradient. In particular, we first consider unconstrained convex optimization with LLCG and propose accelerated proximal gradient (APG) methods for solving it. The proposed APG methods are equipped with a verifiable termination criterion and enjoy an operation complexity of ${\cal O}(\varepsilon^{-1/2}\log \varepsilon^{-1})$ and ${\cal O}(\log \varepsilon^{-1})$ for finding an $\varepsilon$-residual solution of an unconstrained convex and strongly convex optimization problem, respectively. We then consider constrained convex optimization with LLCG and propose an first-order proximal augmented Lagrangian method for solving it by applying one of our proposed APG methods to approximately solve a sequence of proximal augmented Lagrangian subproblems. The resulting method is 
    
[^158]: DPSNN: 差分隐私脉冲神经网络与时序增强池化

    DPSNN: A Differentially Private Spiking Neural Network with Temporal Enhanced Pooling. (arXiv:2205.12718v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2205.12718](http://arxiv.org/abs/2205.12718)

    本文提出了一种差分隐私脉冲神经网络（DPSNN），将差分隐私算法与脉冲神经网络相结合，保持了SNN的强隐私保护；同时提出了时序增强池化（TEP）方法，使SNN能够获得更好的信息传输。在MNIST和CIFAR-10数据集上进行实验验证了该方法的有效性。

    

    隐私保护是机器学习算法中至关重要的问题，目前的隐私保护方法常常结合传统基于实数的人工神经网络。作为新一代人工神经网络，脉冲神经网络（SNN）在许多领域中发挥着关键作用，因此，有必要对SNN的隐私保护进行研究。本文将差分隐私（DP）算法与SNN相结合，提出了一种差分隐私脉冲神经网络（DPSNN）。SNN使用离散的脉冲序列来传输信息，并结合DP引入的梯度噪声，从而使SNN保持强大的隐私保护。同时，为了使SNN在获得高隐私保护的同时保持高性能，我们提出了时序增强池化（TEP）方法。该方法充分将SNN的时序信息与空间信息传输结合起来，使SNN能够更好地进行信息传输。我们对DPSNN在MNIST和CIFAR-10数据集上进行了实验证明了其有效性。

    Privacy protection is a crucial issue in machine learning algorithms, and the current privacy protection is combined with traditional artificial neural networks based on real values. Spiking neural network (SNN), the new generation of artificial neural networks, plays a crucial role in many fields. Therefore, research on the privacy protection of SNN is urgently needed. This paper combines the differential privacy(DP) algorithm with SNN and proposes a differentially private spiking neural network (DPSNN). The SNN uses discrete spike sequences to transmit information, combined with the gradient noise introduced by DP so that SNN maintains strong privacy protection. At the same time, to make SNN maintain high performance while obtaining high privacy protection, we propose the temporal enhanced pooling (TEP) method. It fully integrates the temporal information of SNN into the spatial information transfer, which enables SNN to perform better information transfer. We conduct experiments on 
    
[^159]: 基于价值知识整合的高效内存强化学习

    Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation. (arXiv:2205.10868v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10868](http://arxiv.org/abs/2205.10868)

    本论文提出了一种基于价值知识整合的高效内存强化学习算法，通过从目标 Q 网络向当前 Q 网络整合知识，减少遗忘并保持高样本效率。相较于传统方法，本算法在特征和图像任务中表现相当或更好，同时减轻了大经验重放缓存器带来的内存负担。

    

    人工神经网络在通用函数逼近上非常有前途，但面临灾难性遗忘的训练难题，特别是在非独立或非同分布的数据上训练困难。经验重放缓存器是深度强化学习中的标准组件，通常通过将经验存储在大缓存器中并延迟使用进行训练，以减少遗忘并提高样本效率。然而，大型重放缓存器会导致重负载内存，特别是对于内存容量有限的边缘设备和嵌入式设备。我们提出了基于深度 Q 网络算法的内存高效强化学习算法，以缓解这个问题。我们的算法通过从目标 Q 网络向当前 Q 网络整合知识，减少遗忘并保持高样本效率。与基准方法相比，我们的算法在基于特征和基于图像的任务中实现了可比较或更好的性能，同时减轻了大经验重放缓存器的负担。

    Artificial neural networks are promising for general function approximation but challenging to train on non-independent or non-identically distributed data due to catastrophic forgetting. The experience replay buffer, a standard component in deep reinforcement learning, is often used to reduce forgetting and improve sample efficiency by storing experiences in a large buffer and using them for training later. However, a large replay buffer results in a heavy memory burden, especially for onboard and edge devices with limited memory capacities. We propose memory-efficient reinforcement learning algorithms based on the deep Q-network algorithm to alleviate this problem. Our algorithms reduce forgetting and maintain high sample efficiency by consolidating knowledge from the target Q-network to the current Q-network. Compared to baseline methods, our algorithms achieve comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience re
    
[^160]: 图挖掘中的公平性：综述

    Fairness in Graph Mining: A Survey. (arXiv:2204.09888v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.09888](http://arxiv.org/abs/2204.09888)

    本文综述了近年来图挖掘算法在公平性方面的研究进展。在人类中心应用中，大多数算法缺乏公平性考虑，可能对某些人群造成歧视。与独立同分布的数据不同，图挖掘中的公平性具有独特的分类和实现技术。本文提出了一个新的图公平概念分类法，并总结了现有技术以促进图挖掘的公平性。

    

    过去几年中，图挖掘算法在各个领域扮演着重要角色。然而，尽管这些算法在各种图分析任务上表现出色，它们中的大多数缺乏公平性考虑。因此，在人类中心应用中使用时，它们可能会对某些人群造成歧视。最近，算法公平性在基于图的应用中得到了广泛研究。与独立同分布（i.i.d.）数据的算法公平性相比，在图挖掘中的公平性具有独特的背景、分类和实现技术。在本综述中，我们在公平图挖掘的背景下，全面而最新地介绍现有文献。具体而言，我们提出了一个新的图公平概念分类法，来阐明其联系和区别。我们进一步对促进图挖掘公平性的现有技术进行了有组织的总结。

    Graph mining algorithms have been playing a significant role in myriad fields over the years. However, despite their promising performance on various graph analytical tasks, most of these algorithms lack fairness considerations. As a consequence, they could lead to discrimination towards certain populations when exploited in human-centered applications. Recently, algorithmic fairness has been extensively studied in graph-based applications. In contrast to algorithmic fairness on independent and identically distributed (i.i.d.) data, fairness in graph mining has exclusive backgrounds, taxonomies, and fulfilling techniques. In this survey, we provide a comprehensive and up-to-date introduction of existing literature under the context of fair graph mining. Specifically, we propose a novel taxonomy of fairness notions on graphs, which sheds light on their connections and differences. We further present an organized summary of existing techniques that promote fairness in graph mining. Final
    
[^161]: 神经消息传递用于客观目标的不确定性量化和最优实验设计

    Neural Message Passing for Objective-Based Uncertainty Quantification and Optimal Experimental Design. (arXiv:2203.07120v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07120](http://arxiv.org/abs/2203.07120)

    本论文提出了一种降低计算成本实现客观目标不确定性量化和最优实验设计的新方案。

    

    许多实际科学应用涉及对具有许多未知参数的复杂不确定系统进行数学建模。在这些系统中，准确的参数估计通常是不切实际的，因为可用的训练数据可能是不充分的，获取更多数据的成本可能很高。在这种情况下，我们可以基于贝叶斯模型设计强大的操作符，并设计最优实验来有效地减少不确定性，从而最大程度地提高这些操作符的性能表现。

    Various real-world scientific applications involve the mathematical modeling of complex uncertain systems with numerous unknown parameters. Accurate parameter estimation is often practically infeasible in such systems, as the available training data may be insufficient and the cost of acquiring additional data may be high. In such cases, based on a Bayesian paradigm, we can design robust operators retaining the best overall performance across all possible models and design optimal experiments that can effectively reduce uncertainty to enhance the performance of such operators maximally. While objective-based uncertainty quantification (objective-UQ) based on MOCU (mean objective cost of uncertainty) provides an effective means for quantifying uncertainty in complex systems, the high computational cost of estimating MOCU has been a challenge in applying it to real-world scientific/engineering problems. In this work, we propose a novel scheme to reduce the computational cost for objectiv
    
[^162]: 直接训练的深度脉冲Q网络实现人类水平的控制能力

    Human-Level Control through Directly-Trained Deep Spiking Q-Networks. (arXiv:2201.07211v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2201.07211](http://arxiv.org/abs/2201.07211)

    本文提出了一种直接训练的深度脉冲Q网络架构，解决了基于SNN的深度脉冲强化学习中输出二进制和不可微性问题，并在Atari游戏上实现了人类水平的控制能力。

    

    作为第三代神经网络，脉冲神经网络（SNN）由于其高能效而在神经形态硬件中具有巨大潜力。然而，基于SNN的深度脉冲强化学习（DSRL）由于脉冲函数输出的二进制以及不可微性等问题，仍处于初步阶段。为了解决这些问题，本文提出了一种深度脉冲Q网络（DSQN）。具体来说，我们基于漏电整流与火（LIF）神经元和深度Q网络（DQN）提出了一个直接训练的深度脉冲强化学习架构。然后，我们为深度脉冲Q网络适应了一种直接脉冲学习算法。我们进一步从理论上论证了在DSQN中使用LIF神经元的优势。我们对17个在Atari游戏中表现最佳的游戏进行了全面实验，将我们的方法与最先进的转换方法进行了比较。实验结果证明了我们方法的优越性。

    As the third-generation neural networks, Spiking Neural Networks (SNNs) have great potential on neuromorphic hardware because of their high energy-efficiency. However, Deep Spiking Reinforcement Learning (DSRL), i.e., the Reinforcement Learning (RL) based on SNNs, is still in its preliminary stage due to the binary output and the non-differentiable property of the spiking function. To address these issues, we propose a Deep Spiking Q-Network (DSQN) in this paper. Specifically, we propose a directly-trained deep spiking reinforcement learning architecture based on the Leaky Integrate-and-Fire (LIF) neurons and Deep Q-Network (DQN). Then, we adapt a direct spiking learning algorithm for the Deep Spiking Q-Network. We further demonstrate the advantages of using LIF neurons in DSQN theoretically. Comprehensive experiments have been conducted on 17 top-performing Atari games to compare our method with the state-of-the-art conversion method. The experimental results demonstrate the superiori
    
[^163]: 深度泊松混合模型的概率层级预测

    Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. (arXiv:2110.13179v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.13179](http://arxiv.org/abs/2110.13179)

    该论文提出了一种新的深度泊松混合网络方法（DPMN），能够在存在可靠的层级信息时进行准确和一致的概率时间序列预测。该模型可以保证层级一致性，并在大规模的层级预测问题中获得了最先进的结果。

    

    层级预测问题在时间序列具有自然的分组结构时出现，并需要在不同层次的聚合和分解中进行预测。在这种问题中，通常希望在给定的层次结构中满足聚合约束，称为层级一致性。在概率预测的情况下，保持一致性同时产生准确的预测可能是一个具有挑战性的问题。我们提出了一种新颖的方法，当存在可靠的层级信息时，可以进行准确和一致的概率时间序列预测。我们称之为深度泊松混合网络（DPMN）。它依赖于神经网络和一种统计模型的组合，用于层次多变量时间序列结构的联合分布。通过构建，该模型保证了层级一致性，并提供了用于预测分布聚合和分解的简单规则。我们在一个大规模的层级预测问题—M4竞赛上进行了实验，在非集成方法中，DPMN获得了最先进的结果。

    Hierarchical forecasting problems arise when time series have a natural group structure, and predictions at multiple levels of aggregation and disaggregation across the groups are needed. In such problems, it is often desired to satisfy the aggregation constraints in a given hierarchy, referred to as hierarchical coherence in the literature. Maintaining coherence while producing accurate forecasts can be a challenging problem, especially in the case of probabilistic forecasting. We present a novel method capable of accurate and coherent probabilistic forecasts for time series when reliable hierarchical information is present. We call it Deep Poisson Mixture Network (DPMN). It relies on the combination of neural networks and a statistical model for the joint distribution of the hierarchical multivariate time series structure. By construction, the model guarantees hierarchical coherence and provides simple rules for aggregation and disaggregation of the predictive distributions. We perfo
    
[^164]: 关于黎曼固定秩矩阵优化中嵌入几何和商几何的几何联系

    On Geometric Connections of Embedded and Quotient Geometries in Riemannian Fixed-rank Matrix Optimization. (arXiv:2110.12121v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2110.12121](http://arxiv.org/abs/2110.12121)

    本文提出了一种建立黎曼优化问题的嵌入和商几何的几何景观联系的通用程序，应用于固定秩矩阵优化可以在黎曼FOSP集、SOSP集和严格鞍点集上建立等同。

    

    本文提出了一种建立黎曼优化问题的嵌入和商几何的几何景观联系的通用程序。通过将通用程序应用于固定秩正定半定（PSD）矩阵和一般矩阵优化，我们在流形上每一点都建立了两种几何下的精确黎曼梯度连接，并在黎曼一阶稳定点（FOSP）处的黎曼黑塞矩阵的谱之间建立了夹杂不等式。这些结果立即暗示固定秩矩阵优化在嵌入几何和商几何下黎曼FOSP集，黎曼二阶稳定点（SOSP）集和严格鞍点集的等同。据我们所知，这是固定秩矩阵优化中嵌入和商几何的首个几何景观联系，并提供了这两种几何在黎曼优化中如何联系的具体示例。

    In this paper, we propose a general procedure for establishing the geometric landscape connections of a Riemannian optimization problem under the embedded and quotient geometries. By applying the general procedure to the fixed-rank positive semidefinite (PSD) and general matrix optimization, we establish an exact Riemannian gradient connection under two geometries at every point on the manifold and sandwich inequalities between the spectra of Riemannian Hessians at Riemannian first-order stationary points (FOSPs). These results immediately imply an equivalence on the sets of Riemannian FOSPs, Riemannian second-order stationary points (SOSPs), and strict saddles of fixed-rank matrix optimization under the embedded and the quotient geometries. To the best of our knowledge, this is the first geometric landscape connection between the embedded and the quotient geometries for fixed-rank matrix optimization and it provides a concrete example of how these two geometries are connected in Riema
    
[^165]: 利用大偏差理论的熵正则化强化学习

    Entropy Regularized Reinforcement Learning Using Large Deviation Theory. (arXiv:2106.03931v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03931](http://arxiv.org/abs/2106.03931)

    本文利用大偏差理论建立了熵正则化强化学习与非平衡统计力学的联系，在长时间极限下推导出了马尔可夫决策过程模型中最优策略和最优动态的精确解析结果，并提出了新的分析和计算框架。

    

    强化学习是机器学习中一个重要的研究领域，越来越多地被应用于物理学中的复杂优化问题。同时，物理学中的概念也为强化学习带来了重大进展，如熵正则化强化学习。然而，针对熵正则化强化学习中优化的解析解目前是一个未解之谜。在本文中，我们建立了熵正则化强化学习与非平衡统计力学的联系，重点关注在罕见事件条件下的马尔可夫过程。在长时间极限下，我们应用大偏差理论的方法，推导出马尔可夫决策过程（MDP）模型中最优策略和最优动态的精确解析结果，从而得到了一个新的熵正则化强化学习的分析和计算框架，经过模拟验证。

    Reinforcement learning (RL) is an important field of research in machine learning that is increasingly being applied to complex optimization problems in physics. In parallel, concepts from physics have contributed to important advances in RL with developments such as entropy-regularized RL. While these developments have led to advances in both fields, obtaining analytical solutions for optimization in entropy-regularized RL is currently an open problem. In this paper, we establish a mapping between entropy-regularized RL and research in non-equilibrium statistical mechanics focusing on Markovian processes conditioned on rare events. In the long-time limit, we apply approaches from large deviation theory to derive exact analytical results for the optimal policy and optimal dynamics in Markov Decision Process (MDP) models of reinforcement learning. The results obtained lead to a novel analytical and computational framework for entropy-regularized RL which is validated by simulations. The
    
[^166]: 一种基于风险意识的贝叶斯马尔可夫决策过程离线策略选择方法

    An Offline Risk-aware Policy Selection Method for Bayesian Markov Decision Processes. (arXiv:2105.13431v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.13431](http://arxiv.org/abs/2105.13431)

    本文提出了一种基于风险意识的离线策略选择方法，该方法采用“利用”与“谨慎”(EvC)的范式，选取权重值最大化且同时考虑长期奖励和风险的策略。

    

    在规划的离线模型学习和离线强化学习中，有限的数据集限制了相对马尔可夫决策过程(MDP)的价值函数的估计。因此，在真实世界中获得的策略的性能受到限制，并可能存在风险，特别是当部署错误的策略可能导致灾难性后果时。为此，为了减少模型误差（或学习模型与实际模型之间的分布偏移）并广泛地获得风险感知的解决方案，正在采取多种途径。但在最终应用中，从哪个角度出发选择基线呢？在计算时间不是问题而鲁棒性是首要考虑因素的离线情形下，本文提出了一种“利用”与“谨慎”(EvC)的范式，该范式(1)利用贝叶斯形式主张模型不确定性，同时(2)选择最大化风险权重值和长期奖励的策略。

    In Offline Model Learning for Planning and in Offline Reinforcement Learning, the limited data set hinders the estimate of the Value function of the relative Markov Decision Process (MDP). Consequently, the performance of the obtained policy in the real world is bounded and possibly risky, especially when the deployment of a wrong policy can lead to catastrophic consequences. For this reason, several pathways are being followed with the scope of reducing the model error (or the distributional shift between the learned model and the true one) and, more broadly, obtaining risk-aware solutions with respect to model uncertainty. But when it comes to the final application which baseline should a practitioner choose? In an offline context where computational time is not an issue and robustness is the priority we propose Exploitation vs Caution (EvC), a paradigm that (1) elegantly incorporates model uncertainty abiding by the Bayesian formalism, and (2) selects the policy that maximizes a ris
    
[^167]: 变分推断中的单调α散度最小化

    Monotonic Alpha-divergence Minimisation for Variational Inference. (arXiv:2103.05684v4 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2103.05684](http://arxiv.org/abs/2103.05684)

    本文提出了一种用于变分推断的单调α散度最小化算法，可优化混合模型的权重和分量参数，获得了在多峰目标分布和实际数据应用上的改进效果。

    

    本文引入了一种新的迭代算法系列，用于在变分推断上进行α散度最小化。该算法通过确保在每一步中变分分布与后验分布之间的α散度系统性降低，从而实现上述目标。在其最一般的形式下，变分分布是一个混合模型，我们的框架允许我们同时优化此混合模型的权重和分量参数。本文在已有的基于α散度最小化方法，如梯度或幂下降方案的基础上提出了新的见解，并阐述了一种集成期望最大化算法。最后，通过实证，证明了我们的方法在多峰目标分布和实际数据应用上都获得了改进的效果。

    In this paper, we introduce a novel family of iterative algorithms which carry out $\alpha$-divergence minimisation in a Variational Inference context. They do so by ensuring a systematic decrease at each step in the $\alpha$-divergence between the variational and the posterior distributions. In its most general form, the variational distribution is a mixture model and our framework allows us to simultaneously optimise the weights and components parameters of this mixture model. Our approach permits us to build on various methods previously proposed for $\alpha$-divergence minimisation such as Gradient or Power Descent schemes and we also shed a new light on an integrated Expectation Maximization algorithm. Lastly, we provide empirical evidence that our methodology yields improved results on several multimodal target distributions and on a real data example.
    
[^168]: 特征相关性不确定性的蒙特卡罗Dropout采样方法研究

    On Feature Relevance Uncertainty: A Monte Carlo Dropout Sampling Approach. (arXiv:2008.01468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.01468](http://arxiv.org/abs/2008.01468)

    本文提出了一种名为MCRP的方法，采用蒙特卡罗估计计算特征相关性分布，以评估特征相关性的不确定性，以更好地理解神经网络感知和推理。

    

    理解神经网络决策是实现智能系统在现实应用中的关键，然而这些系统不透明的决策过程在需要解释性的情况下是不利的。为了更好地理解神经网络的决策，机器学习领域近年来引入了许多特征解释技术，并成为验证其推理能力的重要组成部分。然而，现有方法并不允许关于特征相关性的不确定性进行陈述。本文提出了一种基于蒙特卡罗估计的特征相关性分布的计算方法，即Monte Carlo Relevance Propagation (MCRP)用于特征相关性的不确定性估计，以计算特征相关性不确定性分数，从而深入理解神经网络的感知和推理。

    Understanding decisions made by neural networks is key for the deployment of intelligent systems in real world applications. However, the opaque decision making process of these systems is a disadvantage where interpretability is essential. Many feature-based explanation techniques have been introduced over the last few years in the field of machine learning to better understand decisions made by neural networks and have become an important component to verify their reasoning capabilities. However, existing methods do not allow statements to be made about the uncertainty regarding a feature's relevance for the prediction. In this paper, we introduce Monte Carlo Relevance Propagation (MCRP) for feature relevance uncertainty estimation. A simple but powerful method based on Monte Carlo estimation of the feature relevance distribution to compute feature relevance uncertainty scores that allow a deeper understanding of a neural network's perception and reasoning.
    
[^169]: FLUID：一种用于灵活序列数据的统一评估框架

    FLUID: A Unified Evaluation Framework for Flexible Sequential Data. (arXiv:2007.02519v6 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2007.02519](http://arxiv.org/abs/2007.02519)

    FLUID是一个统一评估框架，可以处理灵活序列数据，并结合了少样本、持续、转移和表示学习的特点，支持在线评估和超出分布评估。它有助于发展通用的机器学习方法。

    

    现代机器学习方法在训练数据是IID、大规模和标记充分时表现优异，而在不理想的条件下学习仍然是一个开放的挑战。少样本、持续、转移和表示学习的子领域在在不良条件下的学习方面取得了长足的进步，每个子领域通过不同的方法和洞见具有独特的优势。这些方法解决了不同的挑战，如数据按顺序到达或训练示例有限，然而，在部署之前往往无法预见ML系统在其生命周期内将面临的困难条件。因此，需要能够处理学习实际设置中众多挑战的通用ML系统。为了促进通用ML方法的研究，我们引入了一种新的统一评估框架——FLUID（灵活序列数据）。FLUID整合了少样本、持续、转移和表示学习的目标，同时实现了这些子领域技术的比较和整合。它支持顺序数据、在线评估和超出分布评估，同时允许评估不同的语态、任务和领域。我们使用几个案例研究展示了FLUID的多功能性，并讨论了其使用的未来方向。

    Modern ML methods excel when training data is IID, large-scale, and well labeled. Learning in less ideal conditions remains an open challenge. The sub-fields of few-shot, continual, transfer, and representation learning have made substantial strides in learning under adverse conditions; each affording distinct advantages through methods and insights. These methods address different challenges such as data arriving sequentially or scarce training examples, however often the difficult conditions an ML system will face over its lifetime cannot be anticipated prior to deployment. Therefore, general ML systems which can handle the many challenges of learning in practical settings are needed. To foster research towards the goal of general ML methods, we introduce a new unified evaluation framework - FLUID (Flexible Sequential Data). FLUID integrates the objectives of few-shot, continual, transfer, and representation learning while enabling comparison and integration of techniques across thes
    
[^170]: 用于癫痫发作预测的卷积神经网络

    Convolutional Neural Networks for Epileptic Seizure Prediction. (arXiv:1811.00915v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1811.00915](http://arxiv.org/abs/1811.00915)

    本文提出了一种使用卷积神经网络拓扑结构进行iEEG的癫痫发作预测的新方法，避免提取手工特征，并在多个数据集上进行了评估。研究结果表明该方法具有普遍适用性。

    

    癫痫是最常见的神经系统疾病，准确预测癫痫发作将有助于克服患者的不确定和无助感。本研究提出了一种针对颅内脑电图（iEEG）进行癫痫发作预测的新方法，并使用卷积神经网络（CNN）拓扑结构进行信号特征的确定和预ictal和interictal段的二元分类。我们对来自四只狗和三名患者的长期记录进行了三个不同模型的评估。总的来说，我们的研究结果表明了方法的普遍适用性。本研究还讨论了我们方法的优点和局限性。

    Epilepsy is the most common neurological disorder and an accurate forecast of seizures would help to overcome the patient's uncertainty and helplessness. In this contribution, we present and discuss a novel methodology for the classification of intracranial electroencephalography (iEEG) for seizure prediction. Contrary to previous approaches, we categorically refrain from an extraction of hand-crafted features and use a convolutional neural network (CNN) topology instead for both the determination of suitable signal characteristics and the binary classification of preictal and interictal segments. Three different models have been evaluated on public datasets with long-term recordings from four dogs and three patients. Overall, our findings demonstrate the general applicability. In this work we discuss the strengths and limitations of our methodology.
    

