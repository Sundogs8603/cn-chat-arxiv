# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Parametric-Task MAP-Elites](https://rss.arxiv.org/abs/2402.01275) | 本文引入了参数任务MAP-Elites（PT-ME）算法，解决连续多任务优化问题。该算法在每次迭代中解决新任务，利用局部线性回归进行变异操作，通过得到的解集数据集创建了映射任务参数到最优解的函数，实验证明PT-ME算法优于所有基准算法。 |
| [^2] | [Identifying Climate Targets in National Laws and Policies using Machine Learning](https://arxiv.org/abs/2404.02822) | 本文提出了一种从国家法律和政策中提取气候目标的方法，可以可靠地识别出三类目标（“净零”，“减少”和“其他”），并调查了与模型相关的偏见和公平影响。 |
| [^3] | [AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs](https://arxiv.org/abs/2404.02761) | 提出了AQuA，一种综合的磋商质量得分计算方法，可以从多个指标中提取各个讨论帖子的统一得分，保留了评论中磋商方面的信息，提高了模型的透明度。 |
| [^4] | [EGTR: Extracting Graph from Transformer for Scene Graph Generation](https://arxiv.org/abs/2404.02072) | 提出了一种从Transformer中提取图形以用于场景图生成的轻量级单阶段模型，有效地提取了关系图。 |
| [^5] | [Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks](https://arxiv.org/abs/2404.01965) | 该研究旨在利用AutoML技术最大化Deep Shift神经网络性能并最小化资源消耗，提出了结合多保真度HPO和多目标优化的方法，实验证明该方法在提高准确率的同时降低了计算复杂性。 |
| [^6] | [A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling](https://arxiv.org/abs/2404.01685) | 通过核大小缩放提高嵌入式脉冲神经网络准确性的方法学在实验中表现出更高的准确性。 |
| [^7] | [Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation](https://arxiv.org/abs/2404.01518) | 提出了一种基于解决最优传输问题的动作分割方法，通过在Gromov-Wasserstein问题中编码时间一致性先验来实现从视频帧和动作类别之间的噪声成本中解码时间一致的分割。 |
| [^8] | [Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance](https://arxiv.org/abs/2404.01436) | 本文提出了对于RMSProp和Adam在非凸优化中的紧致收敛性分析，首次展示了在最宽松的假设下的收敛性结果，并展示了RMSProp和Adam的迭代复杂度分别为$\mathcal O(\epsilon^{-4})$。 |
| [^9] | [Planning and Editing What You Retrieve for Enhanced Tool Learning](https://arxiv.org/abs/2404.00450) | 该论文提出了一种新颖的模型，结合了“规划与检索”和“编辑与确认”范式，通过神经检索模块和LLM-based查询规划器提高了工具利用的效果。 |
| [^10] | [Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science](https://arxiv.org/abs/2403.20208) | 本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。 |
| [^11] | [ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D](https://arxiv.org/abs/2403.19612) | ILPO-Net是一种处理任意形状模式的新方法，通过卷积运算对局部空间模式方向具有不变性，在各种体积数据集上展现出优越性能并显著减少参数数量。 |
| [^12] | [Long-form factuality in large language models](https://arxiv.org/abs/2403.18802) | 该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。 |
| [^13] | [EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention](https://arxiv.org/abs/2403.17729) | EulerFormer提出了一种具有复杂向量注意力的新型转换器变体，统一了语义差异和位置差异的理论框架。 |
| [^14] | [Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting](https://arxiv.org/abs/2403.16612) | 通过校准贝叶斯UNet++模型，我们可以获得更可靠和更清晰的次季节预测，这对于安全关键的机器学习应用如天气预测员来说尤为重要。 |
| [^15] | [Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes](https://arxiv.org/abs/2403.13032) | 本文提出了一种混合无监督学习策略（HULS），用于监测复杂工业流程，以解决传统方法在数据不平衡和高度相关变量情况下的局限性。 |
| [^16] | [Characterization of Large Language Model Development in the Datacenter](https://arxiv.org/abs/2403.07648) | 本研究对大型语言模型的开发工作负载进行了深入特征化研究，发现了与先前任务特定深度学习工作负载的差异，探索了资源利用模式，并提出了优化系统以适应LLMs的潜在机会。 |
| [^17] | [LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits](https://arxiv.org/abs/2403.03219) | 该研究提出了一种广义最佳双赢线性背景强化型赌博机算法，能够在次优性差距受到下界限制时遗憾为$O(\log(T))$。同时引入了边缘条件来描述次优性差距对问题难度的影响。 |
| [^18] | [Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks](https://arxiv.org/abs/2403.03149) | 本研究提出了一种新颖的拜占庭-鲁棒聚合规则InferGuard，用于防御客户端训练数据分布推断攻击。 |
| [^19] | [An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation](https://arxiv.org/abs/2403.02821) | 提出了一种使用自适应生态排放来保护生态系统的水电管理方法，并结合神经网络和优化算法，旨在推动水电厂兼顾环境保护和能源生产。 |
| [^20] | [API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access](https://arxiv.org/abs/2403.01216) | 本研究提出了一种针对无需访问对数的API-only LLMs的整体预测方法，旨在最小化预测集大小并确保用户定义的覆盖范围的统计保证。 |
| [^21] | [Cameras as Rays: Pose Estimation via Ray Diffusion](https://arxiv.org/abs/2402.14817) | 提出了一种将相机姿势视为射线束的分布表示方法，结合空间图像特征，开发了基于回归和扩散的姿势估计方法，在CO3D数据集上取得了最先进的性能。 |
| [^22] | [Big data analytics to classify earthwork-related locations: A Chengdu study](https://arxiv.org/abs/2402.14698) | 使用大数据分析方法，研究者利用自卸车轨迹、城市兴趣点和土地覆盖数据，成功对城市灰尘污染源进行了分类，证明仅需有限数量特征即可实现高准确度分类。 |
| [^23] | [BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives](https://arxiv.org/abs/2402.14151) | BIRCO基准评估基于大型语言模型的信息检索系统对多方面用户目标的检索能力，发现新的检索协议和更强大的模型是解决复杂用户需求的必要条件。 |
| [^24] | [Double machine learning for causal hybrid modeling -- applications in the Earth sciences](https://arxiv.org/abs/2402.13332) | 本文引入了一种通过因果推断框架估计混合模型的新方法，利用双机器学习(DML)来估计因果效应，并在地球科学中展示了其在估计因果参数方面优于端到端深度神经网络(DNN)方法的效率、鲁棒性和避免等效性的能力。 |
| [^25] | [Generative Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2402.11887) | 提出了一种用于半监督图异常检测的生成式方法，通过生成模拟异常节点来训练判别性单类分类器，以更好地利用图中的已知正常节点。 |
| [^26] | [SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems](https://arxiv.org/abs/2402.11322) | SpikeNAS提出了一种快速内存感知神经架构搜索框架，旨在帮助脉冲神经网络系统快速找到在给定内存预算下高准确性的适当架构。 |
| [^27] | [Better-than-KL PAC-Bayes Bounds](https://arxiv.org/abs/2402.09201) | 本文提出了一种更好的比KL PAC-Bayes界限方法来估计序列均值，应用于预测器泛化误差的估计。 |
| [^28] | [Tensor Completion via Integer Optimization](https://arxiv.org/abs/2402.05141) | 本文开发了一种通过整数优化实现张量补全的新算法，该算法通过在线性步骤中实现了收敛并达到了信息理论速率。 |
| [^29] | [A Learning-Based Caching Mechanism for Edge Content Delivery](https://arxiv.org/abs/2402.02795) | 基于学习的边缘缓存框架HR-Cache能够优化边缘缓存，提高字节命中率，降低网络负载，并加速内容传递给用户。 |
| [^30] | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079) | KVQuant是一种解决LLM推理中大量内存消耗的KV缓存量化方法，通过引入新颖的量化方法，包括分通道键量化、RoPE前量化键和非均匀KV缓存量化，准确表示超低精度的KV激活。 |
| [^31] | [Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization](https://arxiv.org/abs/2401.15496) | 本文提出了Baichuan2-Sum模型，通过指导微调Baichuan2-7B模型进行对话摘要，并应用NEFTune技术改进训练过程。实验证明该模型在CSDS和SAMSUM数据集上取得了新的最先进结果。 |
| [^32] | [Eliciting Latent Knowledge from Quirky Language Models](https://arxiv.org/abs/2312.01037) | 本研究通过引入一套“古怪”的语言模型，调取了这些模型在特定上下文中的潜在知识，展示了从可信度低的模型中调取可靠知识的前景。 |
| [^33] | [From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning](https://arxiv.org/abs/2310.00492) | 指令调整对LLMs产生了三个重要影响：1）使其能够识别用户提示中的指令部分；2）促进响应生成的不断调整 |
| [^34] | [TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records.](http://arxiv.org/abs/2401.14694) | TA-RNN和TA-RNN-AE是两种基于RNN的可解释深度学习架构，用于分析电子健康记录并预测患者的临床结果。这些架构考虑了EHR数据的不规则性和时间间隔，并采用时间嵌入的方法解决了这些问题。 |
| [^35] | [RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models.](http://arxiv.org/abs/2401.09432) | RoleCraft-GLM是一个创新框架，通过大型语言模型实现个性化角色扮演，解决了缺乏个性化互动的问题。通过独特的对话数据集和细致入微的角色发展，它能够生成准确反映角色个性特征和情感的对话，提升用户参与度。 |
| [^36] | [Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection.](http://arxiv.org/abs/2401.03737) | 本文介绍了MarketSenseAI，一个利用GPT-4进行股票选择的人工智能框架，融合了多种数据源和推理能力，提供具有可行解释的投资信号。 |
| [^37] | [MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation.](http://arxiv.org/abs/2310.19454) | 该论文提出了MMM和MMMSynth算法，用于聚类异构表格数据和生成合成数据。MMM算法利用EM算法，在同类算法中表现更优，对于确定合成数据的聚类以及恢复真实数据的结构有较好的效果。 MMMSynth算法则用于从真实数据生成合成表格数据。 |
| [^38] | [SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation.](http://arxiv.org/abs/2310.12508) | 这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。 |
| [^39] | [L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation.](http://arxiv.org/abs/2310.02003) | L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。 |
| [^40] | [Data-Efficient Power Flow Learning for Network Contingencies.](http://arxiv.org/abs/2310.00763) | 本论文提出了一种数据高效的方法，用于学习具有网络事故的电网中的电力流，并估计电力流的概率性。通过使用网络感知高斯过程和多任务顶点度核，该方法实现了对未见网络的电力流推断，并在低训练数据情况下比现有方法具有更好的性能。 |
| [^41] | [Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation.](http://arxiv.org/abs/2309.12545) | 本文提出了一种名为PROPLACE的方法，通过鲁棒优化技术为神经网络提供可证明的鲁棒和可信的反事实解释，解决了现有方法在保持鲁棒性的同时生成不合理解释的问题。 |
| [^42] | [Towards Robust Continual Learning with Bayesian Adaptive Moment Regularization.](http://arxiv.org/abs/2309.08546) | 基于贝叶斯自适应时刻正则化的鲁棒性持续学习方法能够在机器人应用中有效地解决灾难性遗忘问题，并具有轻量级和任务实验室等优势。 |
| [^43] | [Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof.](http://arxiv.org/abs/2309.08339) | 本文分析了ADAM在非凸设置中具有恒定步长的收敛性，给出了步长达到几乎肯定渐近收敛的充分条件，并提供了确定性ADAM在处理平滑非凸函数时达到近似临界性所需的运行时间界限。 |
| [^44] | [Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy.](http://arxiv.org/abs/2309.07402) | 提出了一种名为SemiGCL的方法，使用图对比学习和最小最大熵训练来解决图上的半监督领域适应问题，该方法通过对比学得表示来生成信息丰富的节点表示，并使用对抗优化减小域差异。在实验中取得了良好的性能。 |
| [^45] | [FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data.](http://arxiv.org/abs/2308.12388) | FOSA是一种全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法，通过融合FIML估计和自注意力机制，实现了在准确性、计算效率和适应不同数据结构方面的显著优势。 |
| [^46] | [Incorporating Recklessness to Collaborative Filtering based Recommender Systems.](http://arxiv.org/abs/2308.02058) | 本文提出了一种将鲁莽行为引入基于矩阵分解的推荐系统学习过程的方法，通过控制风险水平来提高预测的数量和质量。 |
| [^47] | [An Empirical Study on Fairness Improvement with Multiple Protected Attributes.](http://arxiv.org/abs/2308.01923) | 本文通过广泛研究，发现对于单个保护属性的公平性改善会大大降低对未考虑保护属性的公平性，但在多属性模式下可以保持准确性。 |
| [^48] | [Hybrid Ground-State Quantum Algorithms based on Neural Schr\"odinger Forging.](http://arxiv.org/abs/2307.02633) | 提出了一种基于神经网络的纠缠锻造方法来解决基态问题，通过识别最相关的基态位串，消除了指数级求和的需求，并展示了该方法在不同系统上可以达到相当或更优的性能。 |
| [^49] | [Scalable tensor methods for nonuniform hypergraphs.](http://arxiv.org/abs/2306.17825) | 本论文提出了一种可扩展的张量方法，用于处理非均匀超图。通过开发新的TTSV算法，我们能够在低于指数复杂度的情况下处理邻接张量，并应用于超图中心性和聚类等问题。这些方法不仅能提供与图缩减方法互补的信息，还能够探测到高阶结构。 |
| [^50] | [Improved Financial Forecasting via Quantum Machine Learning.](http://arxiv.org/abs/2306.12965) | 本研究利用量子机器学习提升了金融预测的表现，包括使用行列式点过程来增强随机森林模型进行流失预测并设计了量子神经网络架构用于信用风险评估，比传统方法使用更少的参数达到相似的性能。 |
| [^51] | [Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning.](http://arxiv.org/abs/2306.05494) | 本文对于基于机器学习的网络入侵检测系统(NIDS)的对抗性攻击进行了分类，同时探究了持续再训练对NIDS对抗性攻击的影响。实验表明，即使没有对抗性训练，持续再训练也可以减少对抗性攻击的影响。 |
| [^52] | [Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection.](http://arxiv.org/abs/2306.00006) | 本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。 |
| [^53] | [Stable Anisotropic Regularization.](http://arxiv.org/abs/2305.19358) | 本文提出了一种新颖的正则化方法I-STAR，可以增加模型的稳定性，提高性能，并改善自然语言处理中的组合表示问题。 |
| [^54] | [CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants.](http://arxiv.org/abs/2304.14364) | 本文提出了一种名为CONSCENDI的蒸馏方法，用于构建防护栏模型，以监控任务型虚拟助手的输出。关键方法包括场景增强生成和对比训练样例。这种方法产生了一组多样化的违反规则的对话训练集，并且可以更好地检测代理的输出是否符合设计者指定的规则。 |
| [^55] | [OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer Thin Film Structures.](http://arxiv.org/abs/2304.10294) | OptoGPT是一种基于大型数据集训练的仅包含解码器的Transformer模型，可以自主全局设计探索，同时选择材料和厚度，用于光学多层薄膜结构反向设计。 |
| [^56] | [Long-term Forecasting with TiDE: Time-series Dense Encoder.](http://arxiv.org/abs/2304.08424) | TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。 |
| [^57] | [Risk-Adaptive Approaches to Learning and Decision Making: A Survey.](http://arxiv.org/abs/2212.00856) | 本文调查了过去25年中风险测度的快速发展，介绍了其在各个领域的应用，以及与效用理论和分布鲁棒优化的关系，并指出了公平机器学习等新兴应用领域。 |
| [^58] | [Generalization in the Face of Adaptivity: A Bayesian Perspective.](http://arxiv.org/abs/2106.10761) | 本文提出面对自适应选择数据样本引起的过度拟合问题，使用噪声加算法可以提供不依赖于查询规模的误差保证，这一结果表明适应数据分析的问题在于新查询与过去查询的协方差。 |

# 详细

[^1]: 参数任务MAP-Elites

    Parametric-Task MAP-Elites

    [https://rss.arxiv.org/abs/2402.01275](https://rss.arxiv.org/abs/2402.01275)

    本文引入了参数任务MAP-Elites（PT-ME）算法，解决连续多任务优化问题。该算法在每次迭代中解决新任务，利用局部线性回归进行变异操作，通过得到的解集数据集创建了映射任务参数到最优解的函数，实验证明PT-ME算法优于所有基准算法。

    

    将一组函数的优化同时应用于它们的相似性被称为多任务优化。当前的黑箱多任务算法仅解决有限的一组任务，即使这些任务来自连续空间。在本文中，我们介绍了参数任务MAP-Elites（PT-ME），一种解决连续多任务优化问题的新型黑箱算法。该算法（1）在每次迭代中解决一个新任务，有效地涵盖了连续空间，（2）利用基于局部线性回归的新变异操作符。所得到的解集数据集使得能够创建一个将任何任务参数映射到其最优解的函数。通过在两个参数任务玩具问题和一个更现实和具有挑战性的仿真机器人问题上的表现，我们展示了PT-ME优于所有基准算法，包括深度强化学习算法PPO。

    Optimizing a set of functions simultaneously by leveraging their similarity is called multi-task optimization. Current black-box multi-task algorithms only solve a finite set of tasks, even when the tasks originate from a continuous space. In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel black-box algorithm to solve continuous multi-task optimization problems. This algorithm (1) solves a new task at each iteration, effectively covering the continuous space, and (2) exploits a new variation operator based on local linear regression. The resulting dataset of solutions makes it possible to create a function that maps any task parameter to its optimal solution. We show on two parametric-task toy problems and a more realistic and challenging robotic problem in simulation that PT-ME outperforms all baselines, including the deep reinforcement learning algorithm PPO.
    
[^2]: 使用机器学习识别国家法律和政策中的气候目标

    Identifying Climate Targets in National Laws and Policies using Machine Learning

    [https://arxiv.org/abs/2404.02822](https://arxiv.org/abs/2404.02822)

    本文提出了一种从国家法律和政策中提取气候目标的方法，可以可靠地识别出三类目标（“净零”，“减少”和“其他”），并调查了与模型相关的偏见和公平影响。

    

    定量政策目标是气候政策的基本要素，通常以领域特定和技术性语言为特征。目前，筛选全球气候政策目标的方法涉及大量手动工作。目前很少有可扩展的方法从国家法律或政策中提取气候目标，这限制了政策制定者和研究人员评估私营和公共部门与全球目标的一致性以及为政策决策提供信息的能力。在本文中，我们提出了一种从国家法律和政策中提取气候目标提及的方法。我们创建了一个专家注释的数据集，识别了三类目标（“净零”，“减少”和“其他”（例如可再生能源目标）），并训练了一个可靠地在文本中识别它们的分类器。我们调查了与我们模型相关的偏差和公平影响，并确定了特定年份和国家名称作为问题。

    arXiv:2404.02822v1 Announce Type: cross  Abstract: Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problemati
    
[^3]: AQuA --结合专家和非专家观点，利用LLMs评估在线讨论中的磋商质量

    AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs

    [https://arxiv.org/abs/2404.02761](https://arxiv.org/abs/2404.02761)

    提出了AQuA，一种综合的磋商质量得分计算方法，可以从多个指标中提取各个讨论帖子的统一得分，保留了评论中磋商方面的信息，提高了模型的透明度。

    

    在政治在线讨论中衡量贡献质量对于研究磋商和计算机科学至关重要。随着深度学习的进步，自动衡量这些指标变得可行。本文介绍了AQuA，它是一个添加分数，从多个指标中计算每个讨论帖子的统一磋商质量得分。与其他特定分数不同，AQuA保留了评论中存在的磋商方面的信息，增强了模型的透明度。

    arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
    
[^4]: 从Transformer中提取图形用于场景图生成

    EGTR: Extracting Graph from Transformer for Scene Graph Generation

    [https://arxiv.org/abs/2404.02072](https://arxiv.org/abs/2404.02072)

    提出了一种从Transformer中提取图形以用于场景图生成的轻量级单阶段模型，有效地提取了关系图。

    

    场景图生成（SGG）是一项具有挑战性的任务，涉及检测对象并预测对象之间的关系。提出了一个轻量级的单阶段SGG模型，它从DETR解码器的多头自注意力层中学习的各种关系中提取关系图。

    arXiv:2404.02072v1 Announce Type: cross  Abstract: Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively accor
    
[^5]: 旨在利用AutoML实现可持续深度学习：基于Deep Shift神经网络的多目标HPO方法

    Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks

    [https://arxiv.org/abs/2404.01965](https://arxiv.org/abs/2404.01965)

    该研究旨在利用AutoML技术最大化Deep Shift神经网络性能并最小化资源消耗，提出了结合多保真度HPO和多目标优化的方法，实验证明该方法在提高准确率的同时降低了计算复杂性。

    

    深度学习（DL）通过从大型数据集中提取复杂模式推动了各个领域的发展。然而，DL模型的计算需求带来了环境和资源挑战。Deep Shift神经网络（DSNN）利用shift操作减少推理时的计算复杂性，为此提供了解决方案。通过借鉴标准DNN的见解，我们有兴趣通过AutoML技术充分发挥DSNN的潜力。我们研究了超参数优化（HPO）对于最大化DSNN性能同时最小化资源消耗的影响。由于将准确性和能耗作为可能互补目标结合的多目标（MO）优化，我们建议将最先进的多保真度（MF）HPO与多目标优化相结合。实验结果证明了我们方法的有效性，得到了准确率超过80％且计算低耗的模型。

    arXiv:2404.01965v1 Announce Type: cross  Abstract: Deep Learning (DL) has advanced various fields by extracting complex patterns from large datasets. However, the computational demands of DL models pose environmental and resource challenges. Deep shift neural networks (DSNNs) offer a solution by leveraging shift operations to reduce computational complexity at inference. Following the insights from standard DNNs, we are interested in leveraging the full potential of DSNNs by means of AutoML techniques. We study the impact of hyperparameter optimization (HPO) to maximize DSNN performance while minimizing resource consumption. Since this combines multi-objective (MO) optimization with accuracy and energy consumption as potentially complementary objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with multi-objective optimization. Experimental results demonstrate the effectiveness of our approach, resulting in models with over 80\% in accuracy and low computational 
    
[^6]: 通过核大小缩放提高嵌入式脉冲神经网络准确性的方法学

    A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling

    [https://arxiv.org/abs/2404.01685](https://arxiv.org/abs/2404.01685)

    通过核大小缩放提高嵌入式脉冲神经网络准确性的方法学在实验中表现出更高的准确性。

    

    脉冲神经网络（SNNs）由于其稀疏的基于脉冲的操作而能为基于机器学习的应用提供超低功耗/能耗。目前，大多数SNN架构需要更大的模型大小才能实现更高的准确性，这对资源受限的嵌入式应用不太适合。因此，迫切需要开发能够以可接受的内存占用实现高准确性的SNNs。为此，我们提出了一种通过核大小缩放提高SNNs准确性的新方法学。其关键步骤包括调查不同核大小对准确性的影响，设计新的核大小集合，基于选定的核大小生成SNN架构，并分析SNN模型选择的准确性-内存折衷。实验结果表明，我们的方法学在准确性方面优于最先进的方法（对于CIFAR10有93.24%的准确度）

    arXiv:2404.01685v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70
    
[^7]: 一种用于无监督动作分割的临时一致不平衡最优传输方法

    Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation

    [https://arxiv.org/abs/2404.01518](https://arxiv.org/abs/2404.01518)

    提出了一种基于解决最优传输问题的动作分割方法，通过在Gromov-Wasserstein问题中编码时间一致性先验来实现从视频帧和动作类别之间的噪声成本中解码时间一致的分割。

    

    我们提出了一种针对长时间未修剪视频的动作分割任务的新方法，基于解决最优传输问题。通过将时间一致性先验编码到Gromov-Wasserstein问题中，我们能够从视频帧和动作类别之间的噪声关联/匹配成本矩阵中解码出一个时间一致的分割。与先前方法不同，我们的方法不需要知道视频的动作顺序来实现时间一致性。此外，我们的结果（融合）Gromov-Wasserstein问题可以在GPU上使用几次投影镜下降迭代高效求解。我们在无监督学习环境中展示了我们方法的有效性，其中我们的方法用于生成自训练的伪标签。我们在Breakfast、50-Salads、YouTube Instructions和Desktop Assembly数据集上评估了我们的分割方法和无监督学习流程，取得了最先进的结果。

    arXiv:2404.01518v1 Announce Type: cross  Abstract: We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state
    
[^8]: RMSProp和Adam在具有仿射噪声方差的广义光滑非凸优化中的收敛性保证

    Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance

    [https://arxiv.org/abs/2404.01436](https://arxiv.org/abs/2404.01436)

    本文提出了对于RMSProp和Adam在非凸优化中的紧致收敛性分析，首次展示了在最宽松的假设下的收敛性结果，并展示了RMSProp和Adam的迭代复杂度分别为$\mathcal O(\epsilon^{-4})$。

    

    本文在坐标级别广义光滑性和仿射噪声方差的最宽松假设下，为非凸优化中的RMSProp和Adam提供了首个收敛性分析。首先分析了RMSProp，它是一种具有自适应学习率但没有一阶动量的Adam的特例。具体地，为了解决自适应更新、无界梯度估计和Lipschitz常数之间的依赖挑战，我们证明了下降引理中的一阶项收敛，并且其分母由梯度范数的函数上界限制。基于这一结果，我们展示了使用适当的超参数的RMSProp收敛到一个$\epsilon$-稳定点，其迭代复杂度为$\mathcal O(\epsilon^{-4})$。然后，将我们的分析推广到Adam，额外的挑战是由于梯度与一阶动量之间的不匹配。我们提出了一个新的上界限制

    arXiv:2404.01436v1 Announce Type: cross  Abstract: This paper provides the first tight convergence analyses for RMSProp and Adam in non-convex optimization under the most relaxed assumptions of coordinate-wise generalized smoothness and affine noise variance. We first analyze RMSProp, which is a special case of Adam with adaptive learning rates but without first-order momentum. Specifically, to solve the challenges due to dependence among adaptive update, unbounded gradient estimate and Lipschitz constant, we demonstrate that the first-order term in the descent lemma converges and its denominator is upper bounded by a function of gradient norm. Based on this result, we show that RMSProp with proper hyperparameters converges to an $\epsilon$-stationary point with an iteration complexity of $\mathcal O(\epsilon^{-4})$. We then generalize our analysis to Adam, where the additional challenge is due to a mismatch between the gradient and first-order momentum. We develop a new upper bound on
    
[^9]: 规划和编辑检索以增强工具学习

    Planning and Editing What You Retrieve for Enhanced Tool Learning

    [https://arxiv.org/abs/2404.00450](https://arxiv.org/abs/2404.00450)

    该论文提出了一种新颖的模型，结合了“规划与检索”和“编辑与确认”范式，通过神经检索模块和LLM-based查询规划器提高了工具利用的效果。

    

    最近在将外部工具与大型语言模型（LLMs）集成方面取得的进展打开了新的领域，应用范围涵盖数学推理、代码生成器和智能助手。然而，现有方法依赖简单的一次性检索策略，无法有效准确地筛选相关工具。本文介绍了一种新颖的“规划与检索（P&R）”和“编辑与确认（E&G）”范式的模型，包括了神经检索模块和基于LLM的查询规划器，以增强工具利用的效果。

    arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&R)'' and ``Edit-and-Ground (E\&G)'' paradigms. The P\&R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
    
[^10]: 发挥大型语言模型在数据科学中预测表格任务的潜力

    Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science

    [https://arxiv.org/abs/2403.20208](https://arxiv.org/abs/2403.20208)

    本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。

    

    在数据科学领域，分类、回归和缺失值填充等预测任务是与表格数据相关的常见挑战。这项研究旨在应用大型语言模型(LLMs)来解决这些预测任务。尽管LLMs擅长理解自然语言，但在处理结构化表格数据方面表现不佳。我们的研究旨在通过收集带有指令注释的表格语料库，并在这一丰富的数据集上对Llama-2进行大规模训练，以弥合这一差距。此外，我们研究了将训练模型应用于零-shot预测、少-shot预测和上下文学习场景的实际应用。通过广泛实验，我们的方法论显示了显著的改进。

    arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
    
[^11]: ILPO-NET：用于三维中任意体积模式不变识别的网络

    ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D

    [https://arxiv.org/abs/2403.19612](https://arxiv.org/abs/2403.19612)

    ILPO-Net是一种处理任意形状模式的新方法，通过卷积运算对局部空间模式方向具有不变性，在各种体积数据集上展现出优越性能并显著减少参数数量。

    

    现代空间数据分析中，有效识别空间模式并学习其层次结构至关重要。体积数据应用寻求确保对位移和模式旋转均具有不变性的技术。ILPO-Net（Invariant to Local Patterns Orientation Network）是一种新颖方法，通过Wigner矩阵展开，在卷积操作中处理任意形状的模式，从而本质上对局部空间模式方向具有不变性。我们的架构无缝集成了新的卷积运算符，在各种体积数据集（如MedMNIST和CATH）上进行基准测试，表现出比基准线更卓越的性能，并且参数数量显著减少 - 在MedMNIST的情况下减少了高达1000倍。

    arXiv:2403.19612v1 Announce Type: cross  Abstract: Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis. Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations. While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research. Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the convolutional operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions. Our architecture seamlessly integrates the new convolution operator and, when benchmarked on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIS
    
[^12]: 大型语言模型中的长篇事实性

    Long-form factuality in large language models

    [https://arxiv.org/abs/2403.18802](https://arxiv.org/abs/2403.18802)

    该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。

    

    大型语言模型（LLMs）在回答开放性主题的事实性提示时，经常生成包含事实错误的内容。为了在开放领域中对模型的长篇事实性进行基准测试，我们首先使用GPT-4生成了一个名为LongFact的提示集，其中包含数千个囊括38个主题的问题。然后，我们提出LLM代理可以通过一种名为Search-Augmented Factuality Evaluator（SAFE）的方法作为长篇事实性的自动评估器。SAFE利用LLM将长篇回应分解为一组单独的事实，并通过发送搜索查询到Google搜索以及确定一个事实是否得到搜索结果支持的多步推理过程来评估每个事实的准确性。此外，我们还提议将F1分数扩展为长篇事实性的聚合度量。为此，我们平衡了回应中支持事实的百分比（精度）与

    arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
    
[^13]: EulerFormer：具有复杂向量注意力的顺序用户行为建模

    EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention

    [https://arxiv.org/abs/2403.17729](https://arxiv.org/abs/2403.17729)

    EulerFormer提出了一种具有复杂向量注意力的新型转换器变体，统一了语义差异和位置差异的理论框架。

    

    为了捕捉用户偏好，转换器模型被广泛应用于建模顺序用户行为数据。转换器架构的核心在于自注意力机制，它计算序列中的成对注意力分数。由于排列等变性的特性，位置编码用于增强令牌表示之间的注意力。在这种设定下，成对注意力分数可以通过语义差异和位置差异两者衍生出来。然而，先前的研究经常以不同方式建模两种不同类型的差异测量，这可能限制了序列建模的表达能力。为了解决这个问题，本文提出了一种名为EulerFormer的具有复杂向量注意力的新型转换器变体，提供了一个统一的理论框架来表述语义差异和位置差异。 EulerFormer包含两个关键技术改进。

    arXiv:2403.17729v1 Announce Type: cross  Abstract: To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. Fi
    
[^14]: 为次季节预测校准贝叶斯UNet++

    Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting

    [https://arxiv.org/abs/2403.16612](https://arxiv.org/abs/2403.16612)

    通过校准贝叶斯UNet++模型，我们可以获得更可靠和更清晰的次季节预测，这对于安全关键的机器学习应用如天气预测员来说尤为重要。

    

    季节性预测在检测由气候变化引起的极端热和寒冷时是一项关键任务。对预测的信心应当是可靠的，因为一年中温度的微小增加对世界有着巨大影响。对神经网络进行校准提供了一种确保我们对预测的信心的方式。然而，校准回归模型是一个研究不足的话题，特别是在预测者中。我们校准了基于UNet++结构的模型，该模型被证明在温度异常方面优于基于物理的模型。我们展示出，在预测误差和校准误差之间略微权衡的情况下，可以获得更可靠和更清晰的预测。我们认为，校准应当成为诸如天气预报员等安全关键的机器学习应用的重要组成部分。

    arXiv:2403.16612v1 Announce Type: new  Abstract: Seasonal forecasting is a crucial task when it comes to detecting the extreme heat and colds that occur due to climate change. Confidence in the predictions should be reliable since a small increase in the temperatures in a year has a big impact on the world. Calibration of the neural networks provides a way to ensure our confidence in the predictions. However, calibrating regression models is an under-researched topic, especially in forecasters. We calibrate a UNet++ based architecture, which was shown to outperform physics-based models in temperature anomalies. We show that with a slight trade-off between prediction error and calibration error, it is possible to get more reliable and sharper forecasts. We believe that calibration should be an important part of safety-critical machine learning applications such as weather forecasters.
    
[^15]: 用于监测工业批处理过程的混合无监督学习策略

    Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes

    [https://arxiv.org/abs/2403.13032](https://arxiv.org/abs/2403.13032)

    本文提出了一种混合无监督学习策略（HULS），用于监测复杂工业流程，以解决传统方法在数据不平衡和高度相关变量情况下的局限性。

    

    工业生产过程，尤其是制药行业，是复杂的系统，需要持续监测以确保效率、产品质量和安全性。本文提出了一种用于监测复杂工业流程的混合无监督学习策略（HULS）。HULS结合了现有的无监督学习技术，以解决传统自组织映射（SOM）在数据不平衡和高度相关的过程变量情况下的局限性。为了评估HULS概念的性能，进行了基于实验室批处理的比较实验。

    arXiv:2403.13032v1 Announce Type: new  Abstract: Industrial production processes, especially in the pharmaceutical industry, are complex systems that require continuous monitoring to ensure efficiency, product quality, and safety. This paper presents a hybrid unsupervised learning strategy (HULS) for monitoring complex industrial processes. Addressing the limitations of traditional Self-Organizing Maps (SOMs), especially in scenarios with unbalanced data sets and highly correlated process variables, HULS combines existing unsupervised learning techniques to address these challenges. To evaluate the performance of the HULS concept, comparative experiments are performed based on a laboratory batch
    
[^16]: 大型语言模型在数据中心开发的特征化研究

    Characterization of Large Language Model Development in the Datacenter

    [https://arxiv.org/abs/2403.07648](https://arxiv.org/abs/2403.07648)

    本研究对大型语言模型的开发工作负载进行了深入特征化研究，发现了与先前任务特定深度学习工作负载的差异，探索了资源利用模式，并提出了优化系统以适应LLMs的潜在机会。

    

    大型语言模型（LLMs）在多个革命性任务上展现出了令人印象深刻的性能。然而，要有效利用大规模集群资源来开发LLMs并非易事，经常面临诸多挑战，如频繁的硬件故障、复杂的并行化策略和资源利用不平衡。本文对我们的GPU数据中心Acme中收集的为期六个月的LLM开发工作负载跟踪进行了深入的特征化研究。具体地，我们调查了LLMs与先前任务特定的深度学习（DL）工作负载之间的差异，探索了资源利用模式，并确定了各种作业故障的影响。我们的分析总结了我们遇到的障碍，并发现了优化专为LLMs定制的系统的潜在机会。此外，我们介绍了我们的系统努力：（1）容错预训练，通过LLM参与来增强容错能力。

    arXiv:2403.07648v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs. Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved 
    
[^17]: LC-Tsalis-INF: 广义最佳双赢线性背景强化型赌博机

    LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits

    [https://arxiv.org/abs/2403.03219](https://arxiv.org/abs/2403.03219)

    该研究提出了一种广义最佳双赢线性背景强化型赌博机算法，能够在次优性差距受到下界限制时遗憾为$O(\log(T))$。同时引入了边缘条件来描述次优性差距对问题难度的影响。

    

    本研究考虑具有独立同分布（i.i.d.）背景的线性背景强化型赌博机问题。在这个问题中，现有研究提出了最佳双赢（BoBW）算法，其遗憾在随机区域中满足$O(\log^2(T))$，其中$T$为回合数，其次优性差距由正常数下界，同时在对抗性区域中满足$O(\sqrt{T})$。然而，对$T$的依赖仍有改进空间，并且次优性差距的假设可以放宽。针对这个问题，本研究提出了一个算法，当次优性差距受到下界限制时，其遗憾满足$O(\log(T))$。此外，我们引入了一个边缘条件，即对次优性差距的一个更温和的假设。该条件使用参数$\beta \in (0, \infty]$表征与次优性差距相关的问题难度。然后我们证明该算法的遗憾满足$O\left(\

    arXiv:2403.03219v1 Announce Type: new  Abstract: This study considers the linear contextual bandit problem with independent and identically distributed (i.i.d.) contexts. In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed. For this issue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ in the setting when the suboptimality gap is lower-bounded. Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap. That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\beta \in (0, \infty]$. We then show that the algorithm's regret satisfies $O\left(\
    
[^18]: 强大的联邦学习方法缓解客户端训练数据分布推断攻击

    Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks

    [https://arxiv.org/abs/2403.03149](https://arxiv.org/abs/2403.03149)

    本研究提出了一种新颖的拜占庭-鲁棒聚合规则InferGuard，用于防御客户端训练数据分布推断攻击。

    

    近期研究揭示了联邦学习（FL）曾被认为安全的漏洞，因为客户端不向服务器共享其私有数据，然而这种方法容易遭受诸如客户端训练数据分布推断攻击等攻击，此攻击可以让恶意客户端重现受害者的数据。本文提出了一种新颖的拜占庭-鲁棒聚合规则InferGuard，旨在防御客户端训练数据分布推断攻击。在我们提出的InferGuard中，服务器首先计算其收到的所有模型更新的坐标中位数。如果客户端的模型更新与计算出的中位数更新显著偏离，则视为恶意。我们对我们的InferGuard在五个基准数据集上进行了彻底评估。

    arXiv:2403.03149v1 Announce Type: cross  Abstract: Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark dat
    
[^19]: 一种适应性水电管理方法用于下游生态系统保护

    An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation

    [https://arxiv.org/abs/2403.02821](https://arxiv.org/abs/2403.02821)

    提出了一种使用自适应生态排放来保护生态系统的水电管理方法，并结合神经网络和优化算法，旨在推动水电厂兼顾环境保护和能源生产。

    

    水电厂在推动清洁和可持续能源生产方面起着关键作用，对全球向可再生能源来源的过渡发挥着重要作用。然而，水电厂目前被视为既是可再生能源的来源，又是生态系统的破坏者。在这项工作中，我们强调了使用自适应生态排放作为保护生态系统的一种潜力被忽视了。为了提倡这一观点，我们提出使用神经网络在每个所需时间预测最小生态排放值。此外，我们提出了一种新颖框架，将其无缝集成到水电管理软件中，利用传统受限优化算法的成熟方法。这种新颖方法不仅可以保护生态系统免受气候变化的影响，还有可能增加电力产量。

    arXiv:2403.02821v1 Announce Type: new  Abstract: Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources. However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems. In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges. To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time. Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms. This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the elec
    
[^20]: API就够了：无需对数访问的大型语言模型的整体预测

    API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access

    [https://arxiv.org/abs/2403.01216](https://arxiv.org/abs/2403.01216)

    本研究提出了一种针对无需访问对数的API-only LLMs的整体预测方法，旨在最小化预测集大小并确保用户定义的覆盖范围的统计保证。

    

    本研究旨在解决无法访问对数时如何量化大型语言模型（LLMs）中的不确定性这一普遍挑战。整体预测（CP）以其与模型无关和无需分布的特点而闻名，是各种LLMs和数据分布的理想方法。然而，现有的LLMs整体预测方法通常假定可以访问对数，这对于一些仅支持API的LLMs来说是不可用的。此外，已知对数可能存在校准不准确的问题，可能导致整体预测性能下降。为了应对这些挑战，我们提出一种新颖的CP方法，（1）专为无需对数访问的API-only LLMs量身定制; (2) 最小化预测集的大小; 以及(3)确保用户定义的覆盖范围具有统计保证。该方法的核心思想是利用粗粒度（例如，样本频率）和细粒度不确定性概念（例如，语义相似性）来制定不一致性度量。实验结果表明，

    arXiv:2403.01216v1 Announce Type: cross  Abstract: This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on 
    
[^21]: 摄像头作为射线: 通过射线扩散进行姿势估计

    Cameras as Rays: Pose Estimation via Ray Diffusion

    [https://arxiv.org/abs/2402.14817](https://arxiv.org/abs/2402.14817)

    提出了一种将相机姿势视为射线束的分布表示方法，结合空间图像特征，开发了基于回归和扩散的姿势估计方法，在CO3D数据集上取得了最先进的性能。

    

    估计相机姿势是3D重建的基本任务，鉴于视图稀疏（<10），该任务仍具有挑战性。与现有方法不同，后者追求相机外参的全局参数化的自上而下预测，我们提出了一种将相机姿势视为射线束的分布表示。这种表示允许与空间图像特征紧密耦合，提高了姿势精度。我们观察到，这种表示自然适用于集合级别的Transformer，并开发了一种基于回归的方法，将图像块映射到相应的射线上。为了捕捉稀疏视角姿势推断中固有的不确定性，我们调整了这种方法，学习了一个去噪扩散模型，使我们能够采样合理的模式，同时提高性能。我们提出的方法，既是基于回归，也是基于扩散的，在CO3D相机姿势估计方面展现出了最先进的性能。

    arXiv:2402.14817v1 Announce Type: cross  Abstract: Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while
    
[^22]: 大数据分析用于分类与土方相关的地点：成都研究

    Big data analytics to classify earthwork-related locations: A Chengdu study

    [https://arxiv.org/abs/2402.14698](https://arxiv.org/abs/2402.14698)

    使用大数据分析方法，研究者利用自卸车轨迹、城市兴趣点和土地覆盖数据，成功对城市灰尘污染源进行了分类，证明仅需有限数量特征即可实现高准确度分类。

    

    空气污染显著加剧，导致全球范围内的严重健康后果。土方相关的地点（ERLs）是城市灰尘污染的重要来源。长期以来，ERLs的有效管理一直是政府和环境机构面临的挑战之一，主要原因包括其分类分属不同的监管部门、信息障碍、数据更新延迟，以及对不同源头灰尘污染的抑制措施的缺乏。为解决这些挑战，我们利用自卸车轨迹、城市兴趣点（POI）和土地覆盖数据对城市灰尘污染源进行分类。我们比较了几种预测模型，并利用实际数据研究了特征与灰尘污染源之间的关系。结果表明，通过有限数量的特征可以实现高准确度的分类。这种方法已成功实施在一个名为的系统中。

    arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
    
[^23]: BIRCO：具有复杂目标的信息检索任务基准

    BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives

    [https://arxiv.org/abs/2402.14151](https://arxiv.org/abs/2402.14151)

    BIRCO基准评估基于大型语言模型的信息检索系统对多方面用户目标的检索能力，发现新的检索协议和更强大的模型是解决复杂用户需求的必要条件。

    

    我们提出了具有复杂目标的信息检索(IR)任务基准(BIRCO)。 BIRCO评估IR系统根据多方面用户目标检索文档的能力。 该基准的复杂性和紧凑大小使其适用于评估基于大型语言模型(LLM)的信息检索系统。 我们提出了一个模块化框架，用于研究可能影响LLM在检索任务上的性能的因素，并确定了一个简单的基线模型，该模型与或优于现有方法和更复杂的替代方案。 没有一种方法在所有基准任务上均达到令人满意的性能，这表明需要更强大的模型和新的检索协议来解决复杂的用户需求。

    arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
    
[^24]: 用于因果混合建模的双机器学习——地球科学应用

    Double machine learning for causal hybrid modeling -- applications in the Earth sciences

    [https://arxiv.org/abs/2402.13332](https://arxiv.org/abs/2402.13332)

    本文引入了一种通过因果推断框架估计混合模型的新方法，利用双机器学习(DML)来估计因果效应，并在地球科学中展示了其在估计因果参数方面优于端到端深度神经网络(DNN)方法的效率、鲁棒性和避免等效性的能力。

    

    arXiv:2402.13332v1 公告类型: 新摘要: 混合建模将机器学习与科学知识相结合，旨在提高解释性、泛化性和遵守自然规律。然而，在混合建模中，等效性和正则化偏差对于实现这些目的构成挑战。本文介绍了一种通过因果推断框架估计混合模型的新方法，特别使用双机器学习(DML)来估计因果效应。我们展示了其在两个涉及二氧化碳通量的地球科学问题上的应用。在$Q_{10}$模型中，我们证明了基于DML的混合建模比端到端深度神经网络(DNN)方法更优，证明了效率高、鲁棒性强，并且规避了正则化方法带来的偏差和等效性。我们的方法应用于碳通量分配，展现了适应不同因果效应的灵活性。这项研究展示了因果混合建模概念在地球科学中的潜力。

    arXiv:2402.13332v1 Announce Type: new  Abstract: Hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emp
    
[^25]: 生成式半监督图异常检测

    Generative Semi-supervised Graph Anomaly Detection

    [https://arxiv.org/abs/2402.11887](https://arxiv.org/abs/2402.11887)

    提出了一种用于半监督图异常检测的生成式方法，通过生成模拟异常节点来训练判别性单类分类器，以更好地利用图中的已知正常节点。

    

    这项工作考虑了一个实际情境下的半监督图异常检测（GAD），在这个情境中，图中的部分节点被知晓是正常的，与大多数GAD研究中使用完全未标记图的无监督情况形成对比。我们发现，可以利用这些正常节点有助于提升现有无监督GAD方法在半监督情境下的检测性能。然而，它们对这些正常节点的利用是有限的。在本文中，我们提出了一种新颖的用于半监督情境的生成式GAD方法（GGAD），以更好地利用这些正常节点。其关键思想是生成模拟异常节点的异常节点，它们融合了本地结构和节点表示，为训练判别型单类分类器提供有效的负面节点样本。

    arXiv:2402.11887v1 Announce Type: new  Abstract: This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and 
    
[^26]: SpikeNAS: 一种面向脉冲神经网络系统的快速内存感知神经架构搜索框架

    SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems

    [https://arxiv.org/abs/2402.11322](https://arxiv.org/abs/2402.11322)

    SpikeNAS提出了一种快速内存感知神经架构搜索框架，旨在帮助脉冲神经网络系统快速找到在给定内存预算下高准确性的适当架构。

    

    脉冲神经网络（SNN）为解决机器学习任务提供了实现超低功耗计算的有前途的解决方案。目前，大多数SNN架构都源自人工神经网络，其神经元的架构和操作与SNN不同，或者在不考虑来自底层处理硬件的内存预算的情况下开发。这些限制阻碍了SNN在准确性和效率方面充分发挥潜力。为此，我们提出了SpikeNAS，一种新颖的内存感知神经架构搜索（NAS）框架，可在给定内存预算下快速找到一个具有高准确性的适当SNN架构。为实现这一目标，我们的SpikeNAS采用了几个关键步骤：分析网络操作对准确性的影响，增强网络架构以提高学习质量，并开发快速内存感知搜索算法。

    arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
    
[^27]: 更好的比KL PAC-Bayes界限

    Better-than-KL PAC-Bayes Bounds

    [https://arxiv.org/abs/2402.09201](https://arxiv.org/abs/2402.09201)

    本文提出了一种更好的比KL PAC-Bayes界限方法来估计序列均值，应用于预测器泛化误差的估计。

    

    让$f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$成为一个随机元素序列，其中$f$是一个固定的标量函数，$X_1, \dots, X_n$是独立的随机变量（数据），而$\theta$是根据一些数据相关的后验分布$P_n$分布的随机参数。本文考虑了证明浓度不等式来估计序列均值的问题。这样一个问题的一个例子是对某些通过随机算法训练的预测器的泛化误差的估计，比如神经网络，其中$f$是一个损失函数。传统上，这个问题是通过PAC-Bayes分析来解决的，在这个分析中，除了后验分布，我们还选择一个能够捕捉到学习问题归纳偏差的先验分布。然后，PAC-Bayes浓度界限中的关键数量是一个能够捕捉到学习问题复杂性的分歧。

    arXiv:2402.09201v1 Announce Type: new Abstract: Let $f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \dots, X_n$ are independent random variables (data), and $\theta$ is a random parameter distributed according to some data-dependent posterior distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function. Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem. Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto stand
    
[^28]: 通过整数优化实现张量补全

    Tensor Completion via Integer Optimization

    [https://arxiv.org/abs/2402.05141](https://arxiv.org/abs/2402.05141)

    本文开发了一种通过整数优化实现张量补全的新算法，该算法通过在线性步骤中实现了收敛并达到了信息理论速率。

    

    张量补全问题的主要挑战在于计算能力和信息理论样本复杂度之间的基本张力。过去的方法要么达到了信息理论的速率但缺乏计算相应解的实际算法，要么具有多项式时间算法，但需要指数级更多的样本以达到低估计误差。本文开发了一种新颖的张量补全算法，通过在线性数目的oracle步骤中同时实现可证的收敛（在数字容差方面）和信息理论速率，从而解决了这个张力。我们的方法通过使用基于规范的张量范数构造了张量补全问题的凸优化问题，这种方法定义了一种允许使用整数线性优化在这种新范数下解决线性分离问题的规范。基于这个洞察力的调整被纳入到一种基于Frank-Wolfe变体的算法中来构建我们的算法。

    The main challenge with the tensor completion problem is a fundamental tension between computation power and the information-theoretic sample complexity rate. Past approaches either achieve the information-theoretic rate but lack practical algorithms to compute the corresponding solution, or have polynomial-time algorithms that require an exponentially-larger number of samples for low estimation error. This paper develops a novel tensor completion algorithm that resolves this tension by achieving both provable convergence (in numerical tolerance) in a linear number of oracle steps and the information-theoretic rate. Our approach formulates tensor completion as a convex optimization problem constrained using a gauge-based tensor norm, which is defined in a way that allows the use of integer linear optimization to solve linear separation problems over the unit-ball in this new norm. Adaptations based on this insight are incorporated into a Frank-Wolfe variant to build our algorithm. We s
    
[^29]: 基于学习的边缘内容传递缓存机制

    A Learning-Based Caching Mechanism for Edge Content Delivery

    [https://arxiv.org/abs/2402.02795](https://arxiv.org/abs/2402.02795)

    基于学习的边缘缓存框架HR-Cache能够优化边缘缓存，提高字节命中率，降低网络负载，并加速内容传递给用户。

    

    随着5G网络的兴起和物联网(IoT)的发展，内容传递网络(CDNs)越来越多地扩展到网络边缘。这种转变带来了独特的挑战，特别是由于边缘的有限缓存存储和多样化的请求模式。边缘环境可以托管具有不同对象大小分布和对象访问模式的流量类别。这种复杂性使得传统的缓存策略很难发挥作用，传统策略通常依赖于请求频率或时间间隔等指标。尽管存在这些复杂性，优化边缘缓存至关重要。在边缘实现更高的字节命中率不仅可以减轻网络骨干的负载，还可以最小化运营成本并加快内容传递给最终用户。在本文中，我们介绍了一种名为HR-Cache的综合学习缓存框架，它基于危险率(Hazard Rate)排序原则，这是一种最初用来计算待命时间的规则。

    With the advent of 5G networks and the rise of the Internet of Things (IoT), Content Delivery Networks (CDNs) are increasingly extending into the network edge. This shift introduces unique challenges, particularly due to the limited cache storage and the diverse request patterns at the edge. These edge environments can host traffic classes characterized by varied object-size distributions and object-access patterns. Such complexity makes it difficult for traditional caching strategies, which often rely on metrics like request frequency or time intervals, to be effective. Despite these complexities, the optimization of edge caching is crucial. Improved byte hit rates at the edge not only alleviate the load on the network backbone but also minimize operational costs and expedite content delivery to end-users.   In this paper, we introduce HR-Cache, a comprehensive learning-based caching framework grounded in the principles of Hazard Rate (HR) ordering, a rule originally formulated to com
    
[^30]: KVQuant: 以KV缓存量化实现1000万上下文长度LLM推理

    KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization

    [https://arxiv.org/abs/2401.18079](https://arxiv.org/abs/2401.18079)

    KVQuant是一种解决LLM推理中大量内存消耗的KV缓存量化方法，通过引入新颖的量化方法，包括分通道键量化、RoPE前量化键和非均匀KV缓存量化，准确表示超低精度的KV激活。

    

    LLM在文档分析和摘要等需要大窗口上下文的应用中越来越受到关注，在推理过程中，KV缓存激活成为记忆消耗的主要贡献者。量化是一种压缩KV缓存激活的有效方法，然而现有的解决方案无法准确表示超低精度（如低于4位）的激活。本文提出了KVQuant，通过引入新颖的方法量化缓存的KV激活来解决这个问题，包括：(i)分通道键量化，在量化键激活时调整维度以更好地匹配分布；(ii)RoPE前量化键，在旋转位置嵌入之前量化键激活以减轻其对量化的影响；(iii)非均匀KV缓存量化，在每层推导出权重感知的非均匀数据类型，以更好地表示不同层的敏感性。

    LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
    
[^31]: Baichuan2-Sum: 使用指导微调Baichuan2-7B模型进行对话摘要

    Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization

    [https://arxiv.org/abs/2401.15496](https://arxiv.org/abs/2401.15496)

    本文提出了Baichuan2-Sum模型，通过指导微调Baichuan2-7B模型进行对话摘要，并应用NEFTune技术改进训练过程。实验证明该模型在CSDS和SAMSUM数据集上取得了新的最先进结果。

    

    巨大的语言模型（LLM）如Llama、Baichuan和Bloom模型在许多自然语言任务中展现出了令人瞩目的能力。然而，对于对话摘要任务，该任务旨在为对话中的不同角色生成摘要，大多数最先进的方法都是基于小模型（例如Bart和Bert）进行的。现有方法尝试在小模型上添加任务指定的优化，如向模型添加全局-局部中心度得分。在本文中，我们提出了一种指导微调模型：Baichuan2-Sum，用于面向角色的对话摘要。通过为不同角色设置不同的指令，模型可以从对话交互中学习并输出期望的摘要。此外，我们还应用了NEFTune技术，在训练过程中添加合适的噪声以提高结果。实验证明，所提出的模型在两个公开的对话摘要数据集CSDS和SAMSUM上取得了新的最先进结果。

    Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
    
[^32]: 从古怪的语言模型中调取潜在知识

    Eliciting Latent Knowledge from Quirky Language Models

    [https://arxiv.org/abs/2312.01037](https://arxiv.org/abs/2312.01037)

    本研究通过引入一套“古怪”的语言模型，调取了这些模型在特定上下文中的潜在知识，展示了从可信度低的模型中调取可靠知识的前景。

    

    调取潜在知识（ELK）旨在在一个能力强大的神经网络的激活中找到模式，即使网络的明显输出是错误或误导性的，也能稳定跟踪世界的真实状态。为了进一步研究ELK，我们引入了12个数据集和一套相应的“古怪”的语言模型，这些模型在回答问题时，只有在提示中包含关键词“Bob”时才会进行系统性错误的微调。我们证明了简单的探测方法可以调取模型在这些上下文中对正确答案的潜在知识，即使问题比探测器训练的问题更困难。这是由于中间层激活中的上下文无关的知识表示的存在。我们还发现，一种机械的异常检测方法可以以94%的AUROC标识不真实行为。我们的结果显示，从能力强但不受信任的模型中调取可靠的知识，并促进未来研究ELK方法的实证研究是有希望的。

    Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
    
[^33]: 从语言建模到指令跟随：理解指令调整后LLMs中行为的转变

    From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning

    [https://arxiv.org/abs/2310.00492](https://arxiv.org/abs/2310.00492)

    指令调整对LLMs产生了三个重要影响：1）使其能够识别用户提示中的指令部分；2）促进响应生成的不断调整

    

    大型语言模型（LLMs）已经取得了显著的成功，其中指令调整是将LLMs与用户意图对齐的关键步骤。在这项工作中，我们研究了指令调整如何调整经过预训练的模型，重点关注内在变化。具体来说，我们首先开发了几种本地和全局解释方法，包括一种基于梯度的输入输出归因方法，以及用于解释自注意力和前馈层中的模式和概念的技术。然后通过比较从预训练和指令调整模型中得出的解释来研究指令调整的影响。这种方法在人可理解的水平上提供了模型转变的内部视角。我们的研究发现了指令调整的三个重要影响：1）它使LLMs能够识别用户提示中的指令部分，并不断促进响应生成

    arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
    
[^34]: TA-RNN：一种基于注意力机制的面向电子健康记录的时间感知递归神经网络架构

    TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])

    [http://arxiv.org/abs/2401.14694](http://arxiv.org/abs/2401.14694)

    TA-RNN和TA-RNN-AE是两种基于RNN的可解释深度学习架构，用于分析电子健康记录并预测患者的临床结果。这些架构考虑了EHR数据的不规则性和时间间隔，并采用时间嵌入的方法解决了这些问题。

    

    动机：电子健康记录（EHR）是患者医疗历史的全面资源。EHR对于利用深度学习（DL）等先进技术至关重要，使医疗提供者能够分析大量数据，提取有价值的见解，并做出精确、数据驱动的临床决策。DL方法如递归神经网络（RNN）已被用于分析EHR以建模疾病进展并预测诊断。然而，这些方法并没有解决EHR数据中一些固有的不规则性，如临床访问之间的不规则时间间隔。此外，大多数DL模型都不可解释。在这项研究中，我们提出了两种基于RNN的可解释DL架构，分别是时间感知RNN（TA-RNN）和TA-RNN-Autoencoder（TA-RNN-AE），用于预测下一次访问和多次未来访问中患者的临床结果。为了减轻不规则时间间隔的影响，我们提出了时间嵌入的方法将时间信息纳入模型中。

    Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
    
[^35]: RoleCraft-GLM：推动大型语言模型中的个性化角色扮演

    RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])

    [http://arxiv.org/abs/2401.09432](http://arxiv.org/abs/2401.09432)

    RoleCraft-GLM是一个创新框架，通过大型语言模型实现个性化角色扮演，解决了缺乏个性化互动的问题。通过独特的对话数据集和细致入微的角色发展，它能够生成准确反映角色个性特征和情感的对话，提升用户参与度。

    

    本研究介绍了RoleCraft-GLM，这是一个创新的框架，旨在通过大型语言模型（LLMs）增强个性化角色扮演。RoleCraft-GLM解决了对话式人工智能中缺乏个性化互动的关键问题，并提供了一种能够详细描绘情感细腻的角色刻画的解决方案。我们贡献了一组独特的对话数据集，这些数据从传统的以名人为中心的角色转变为多样化的非名人角色，从而增强了语言建模互动的真实性和复杂性。此外，我们的方法还包括细致入微的角色发展，确保对话既真实又情感共鸣。通过多个案例研究验证了RoleCraft-GLM的有效性，突显了它在不同场景中的多功能性和技能。我们的框架在生成对话方面表现出色，能够准确反映角色的个性特征和情感，从而增强用户参与度。总之，RoleCraft-GLM标志着一个创新的里程碑，推动了大型语言模型中的个性化角色扮演。

    This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
    
[^36]: 能否打败华尔街？揭示人工智能在股票选择中的潜力

    Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection. (arXiv:2401.03737v1 [q-fin.CP])

    [http://arxiv.org/abs/2401.03737](http://arxiv.org/abs/2401.03737)

    本文介绍了MarketSenseAI，一个利用GPT-4进行股票选择的人工智能框架，融合了多种数据源和推理能力，提供具有可行解释的投资信号。

    

    在金融市场动态和数据驱动的环境中，本文介绍了MarketSenseAI，一个利用GPT-4先进推理能力进行可扩展股票选择的新型人工智能框架。MarketSenseAI整合了“思维链”和“上下文学习”方法，分析包括市场价格动态、财经新闻、公司基本面和宏观经济报告等多种数据源，模仿知名金融投资团队的决策过程。文章详细介绍了MarketSenseAI的开发、实施和实证验证，重点关注其提供具有充分解释支撑的可行投资信号（买入、持有、卖出）的能力。本研究的一个显著特点是使用GPT-4不仅作为预测工具，还作为评估器，揭示了人工智能生成的解释对所建议的投资信号的可靠性和接受度的重要影响。通过广泛的实证评估

    In the dynamic and data-driven landscape of financial markets, this paper introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI incorporates Chain of Thought and In-Context Learning methodologies to analyze a wide array of data sources, including market price dynamics, financial news, company fundamentals, and macroeconomic reports emulating the decision making process of prominent financial investment teams. The development, implementation, and empirical validation of MarketSenseAI are detailed, with a focus on its ability to provide actionable investment signals (buy, hold, sell) backed by cogent explanations. A notable aspect of this study is the use of GPT-4 not only as a predictive tool but also as an evaluator, revealing the significant impact of the AI-generated explanations on the reliability and acceptance of the suggested investment signals. In an extensive empirical evaluation
    
[^37]: MMM和MMMSynth：异构表格数据的聚类和合成数据生成

    MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation. (arXiv:2310.19454v1 [cs.LG])

    [http://arxiv.org/abs/2310.19454](http://arxiv.org/abs/2310.19454)

    该论文提出了MMM和MMMSynth算法，用于聚类异构表格数据和生成合成数据。MMM算法利用EM算法，在同类算法中表现更优，对于确定合成数据的聚类以及恢复真实数据的结构有较好的效果。 MMMSynth算法则用于从真实数据生成合成表格数据。

    

    我们提出了两个与异构表格数据相关的任务的新算法：聚类和合成数据生成。表格数据集通常由列中的异构数据类型（数值、有序、分类）组成，但行中可能还存在隐藏的聚类结构：例如，它们可能来自异构的（地理、社会经济、方法论）来源，因此所描述的结果变量（如疾病的存在）可能不仅依赖其他变量，还依赖于聚类上下文。此外，医学数据的共享通常受到患者隐私法律的限制，因此目前对于通过深度学习等方法从真实数据生成合成表格数据的算法非常感兴趣。我们展示了一种新颖的基于EM的聚类算法MMM（“Madras混合模型”），它在确定合成异构数据的聚类和恢复真实数据结构方面优于标准算法。基于此，我们可将MMM应用于数据合成任务的MMMSynth算法。

    We provide new algorithms for two tasks relating to heterogeneous tabular datasets: clustering, and synthetic data generation. Tabular datasets typically consist of heterogeneous data types (numerical, ordinal, categorical) in columns, but may also have hidden cluster structure in their rows: for example, they may be drawn from heterogeneous (geographical, socioeconomic, methodological) sources, such that the outcome variable they describe (such as the presence of a disease) may depend not only on the other variables but on the cluster context. Moreover, sharing of biomedical data is often hindered by patient confidentiality laws, and there is current interest in algorithms to generate synthetic tabular data from real data, for example via deep learning.  We demonstrate a novel EM-based clustering algorithm, MMM (``Madras Mixture Model''), that outperforms standard algorithms in determining clusters in synthetic heterogeneous data, and recovers structure in real data. Based on this, we
    
[^38]: SalUn：通过基于梯度的权重显著性增强机器遗忘在图像分类和生成中的效果

    SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])

    [http://arxiv.org/abs/2310.12508](http://arxiv.org/abs/2310.12508)

    这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。

    

    随着数据法规的不断发展，机器遗忘（MU）已成为增强当前AI模型的信任和安全性的重要工具。然而，现有的MU方法通常在遗忘精度、稳定性和跨领域适用性方面存在局限。为了解决这些挑战，我们引入了MU中的“权重显著性”概念，借鉴了模型解释中的输入显著性。这一创新将MU的关注点从整个模型引导到了具体的模型权重上，提高了其效果和效率。我们称之为显著性遗忘（SalUn）的方法将其与“精确”遗忘（在删除遗忘数据集后从头开始重新训练模型）的性能差距缩小。据我们所知，SalUn是第一个能够在图像分类和生成中有效消除遗忘数据、类别或概念影响的有原则的MU方法。例如，SalUn可在图片分类和生成任务中擦除遗忘数据、类别或概念。

    With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
    
[^39]: L2MAC：大规模语言模型自动计算机用于无限代码生成

    L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])

    [http://arxiv.org/abs/2310.02003](http://arxiv.org/abs/2310.02003)

    L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。

    

    基于Transformer的大型语言模型（LLM）受到底层Transformer架构固定上下文窗口的限制，阻碍了它们生成长且逻辑一致的代码的能力。增强记忆的LLM是一个有前途的解决方案，但目前的方法无法处理长时间的代码生成任务，因为它们要么只关注于读取内存并将其演变为新内存的连接，要么使用非常专门的内存，无法适应其他领域。本文介绍了L2MAC，这是一种基于LLM的长且一致代码生成的实用存储程序自动计算机。它的内存有两个组成部分：指令注册表，其中填充了一个解决用户给定任务的提示程序，以及文件存储，其中包含最终和中间输出。每个指令由单独的LLM实例执行，其上下文由控制单元管理，能够精确读取和写入内存，以确保有效的整合。

    Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
    
[^40]: 数据高效的网络事故电力流学习方法

    Data-Efficient Power Flow Learning for Network Contingencies. (arXiv:2310.00763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00763](http://arxiv.org/abs/2310.00763)

    本论文提出了一种数据高效的方法，用于学习具有网络事故的电网中的电力流，并估计电力流的概率性。通过使用网络感知高斯过程和多任务顶点度核，该方法实现了对未见网络的电力流推断，并在低训练数据情况下比现有方法具有更好的性能。

    

    本研究提出了一种高效的数据驱动方法，用于学习具有网络事故的电网中的电力流，并估计相应的概率性电压包络（PVE）。首先，使用前期研究中开发的网络感知高斯过程（GP），称为顶点度核（VDK-GP），来估计少数网络配置的电压-功率函数。文章引入了一种新颖的多任务顶点度核（MT-VDK），将学习到的VDK-GP汇集在一起，以确定未见网络的电力流，与其他方法相比，计算复杂度和超参数要求显著降低。在IEEE 30-Bus网络上的模拟实验中，在N-1和N-2事故情况下表明了电力流知识的保留和传递。在低训练数据情况下（50-250个样本），MT-VDK-GP方法在新颖的N-1事故网络配置上的平均预测误差较VDK-GP减少了50%以上。此外，MT-VDK-GP在N-1事故网络配置的低训练数据情况下（50-250个样本）达到了平均预测误差降低50%以上的效果。

    This work presents an efficient data-driven method to learn power flows in grids with network contingencies and to estimate corresponding probabilistic voltage envelopes (PVE). First, a network-aware Gaussian process (GP) termed Vertex-Degree Kernel (VDK-GP), developed in prior work, is used to estimate voltage-power functions for a few network configurations. The paper introduces a novel multi-task vertex degree kernel (MT-VDK) that amalgamates the learned VDK-GPs to determine power flows for unseen networks, with a significant reduction in the computational complexity and hyperparameter requirements compared to alternate approaches. Simulations on the IEEE 30-Bus network demonstrate the retention and transfer of power flow knowledge in both N-1 and N-2 contingency scenarios. The MT-VDK-GP approach achieves over 50% reduction in mean prediction error for novel N-1 contingency network configurations in low training data regimes (50-250 samples) over VDK-GP. Additionally, MT-VDK-GP outp
    
[^41]: 通过鲁棒优化方法为神经网络提供可证明的鲁棒和可信的反事实解释

    Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation. (arXiv:2309.12545v1 [cs.LG])

    [http://arxiv.org/abs/2309.12545](http://arxiv.org/abs/2309.12545)

    本文提出了一种名为PROPLACE的方法，通过鲁棒优化技术为神经网络提供可证明的鲁棒和可信的反事实解释，解决了现有方法在保持鲁棒性的同时生成不合理解释的问题。

    

    反事实解释(CEs)作为解释神经网络分类器的主要方法已经引起了越来越多的关注。通常，CEs对于输入-输出对被定义为到输入的最小距离的数据点，其与输出具有不同的标签。为了解决CEs在模型参数更新(比如重新训练)时很容易被无效的问题，研究提出了一种通过模型参数变化的范数球界限来证明CEs的鲁棒性的方法。然而，现有的针对这种鲁棒性的方法不是完全正确的，或者可能生成不合理的CEs，即与训练数据集存在离群值。事实上，目前没有一种方法能够同时优化距离和可信度，并保持鲁棒性保证。在这项工作中，我们提出了一种名为PROPLACE的方法，利用鲁棒优化技术来解决上述问题。

    Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limi
    
[^42]: 基于贝叶斯自适应时刻正则化的鲁棒性持续学习

    Towards Robust Continual Learning with Bayesian Adaptive Moment Regularization. (arXiv:2309.08546v1 [cs.LG])

    [http://arxiv.org/abs/2309.08546](http://arxiv.org/abs/2309.08546)

    基于贝叶斯自适应时刻正则化的鲁棒性持续学习方法能够在机器人应用中有效地解决灾难性遗忘问题，并具有轻量级和任务实验室等优势。

    

    为了追求长期自主性，机器人代理必须不断适应不断变化的环境并学习解决新任务。持续学习试图克服灾难性遗忘的挑战，即学习解决新任务导致模型忘记先前学到的信息。基于先验的持续学习方法对于机器人应用具有吸引力，因为它们在空间效率上很高，并且通常不会随着任务数量的增加而增加计算复杂性。尽管具有这些理想的特性，但基于先验的方法通常在重要的基准测试中失败，因此与基于记忆的方法相比，在潜在应用方面有限。我们引入了贝叶斯自适应时刻正则化（BAdam），一种新的基于先验的方法，它更好地约束参数增长，降低灾难性遗忘。我们的方法在机器人应用中具有一系列理想的特性，例如轻量级和任务实验室。

    The pursuit of long-term autonomy mandates that robotic agents must continuously adapt to their changing environments and learn to solve new tasks. Continual learning seeks to overcome the challenge of catastrophic forgetting, where learning to solve new tasks causes a model to forget previously learnt information. Prior-based continual learning methods are appealing for robotic applications as they are space efficient and typically do not increase in computational complexity as the number of tasks grows. Despite these desirable properties, prior-based approaches typically fail on important benchmarks and consequently are limited in their potential applications compared to their memory-based counterparts. We introduce Bayesian adaptive moment regularization (BAdam), a novel prior-based method that better constrains parameter growth, leading to lower catastrophic forgetting. Our method boasts a range of desirable properties for robotic applications such as being lightweight and task lab
    
[^43]: ADAM在非凸设置中具有恒定步长的收敛性：一个简单的证明

    Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof. (arXiv:2309.08339v1 [cs.LG])

    [http://arxiv.org/abs/2309.08339](http://arxiv.org/abs/2309.08339)

    本文分析了ADAM在非凸设置中具有恒定步长的收敛性，给出了步长达到几乎肯定渐近收敛的充分条件，并提供了确定性ADAM在处理平滑非凸函数时达到近似临界性所需的运行时间界限。

    

    在神经网络训练中，RMSProp和ADAM仍然是广泛使用的优化算法。它们的性能关键之一在于选择适当的步长，这会显著影响它们的有效性。值得注意的是，这些算法的性能可以因选择的步长而变化很大。此外，关于它们的理论收敛性问题仍然是一个感兴趣的话题。在本文中，我们在非凸设置中对ADAM的恒定步长版本进行了理论分析。我们证明了步长达到几乎肯定渐近收敛到零的充分条件，而只需最小的假设。我们还给出了确定性ADAM在处理平滑非凸函数时达到近似临界性所需的运行时间界限。

    In neural network training, RMSProp and ADAM remain widely favoured optimization algorithms. One of the keys to their performance lies in selecting the correct step size, which can significantly influence their effectiveness. It is worth noting that these algorithms performance can vary considerably, depending on the chosen step sizes. Additionally, questions about their theoretical convergence properties continue to be a subject of interest. In this paper, we theoretically analyze a constant stepsize version of ADAM in the non-convex setting. We show sufficient conditions for the stepsize to achieve almost sure asymptotic convergence of the gradients to zero with minimal assumptions. We also provide runtime bounds for deterministic ADAM to reach approximate criticality when working with smooth, non-convex functions.
    
[^44]: 使用对比学习和最小最大熵的图上半监督领域适应

    Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy. (arXiv:2309.07402v1 [cs.LG])

    [http://arxiv.org/abs/2309.07402](http://arxiv.org/abs/2309.07402)

    提出了一种名为SemiGCL的方法，使用图对比学习和最小最大熵训练来解决图上的半监督领域适应问题，该方法通过对比学得表示来生成信息丰富的节点表示，并使用对抗优化减小域差异。在实验中取得了良好的性能。

    

    由于数据标记成本高昂，图中的标签稀缺在现实世界中经常遇到。为此，图上的半监督领域适应(SSDA)旨在利用标记源图的知识来帮助有限标签的目标图中的节点分类。SSDA任务需要克服源图和目标图之间的域差异。然而，到目前为止，现有的用于跨图节点分类的方法尚未正式考虑这个具有挑战性的研究问题。为了解决图上的SSDA问题，提出了一种名为SemiGCL的新方法，它获益于图对比学习和最小最大熵训练。SemiGCL通过对比从图的局部和全局视图中学到的表示来生成信息丰富的节点表示。此外，SemiGCL通过目标图中未标记节点的熵损失进行对抗优化，以减小域差异。在基准数据集上的实验结果表明，SemiGCL在图上的SSDA任务中取得了良好的性能。

    Label scarcity in a graph is frequently encountered in real-world applications due to the high cost of data labeling. To this end, semi-supervised domain adaptation (SSDA) on graphs aims to leverage the knowledge of a labeled source graph to aid in node classification on a target graph with limited labels. SSDA tasks need to overcome the domain gap between the source and target graphs. However, to date, this challenging research problem has yet to be formally considered by the existing approaches designed for cross-graph node classification. To tackle the SSDA problem on graphs, a novel method called SemiGCL is proposed, which benefits from graph contrastive learning and minimax entropy training. SemiGCL generates informative node representations by contrasting the representations learned from a graph's local and global views. Additionally, SemiGCL is adversarially optimized with the entropy loss of unlabeled target nodes to reduce domain divergence. Experimental results on benchmark d
    
[^45]: FOSA: 全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法

    FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])

    [http://arxiv.org/abs/2308.12388](http://arxiv.org/abs/2308.12388)

    FOSA是一种全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法，通过融合FIML估计和自注意力机制，实现了在准确性、计算效率和适应不同数据结构方面的显著优势。

    

    数据补全中，有效地处理缺失值尤为重要，特别是在复杂的数据集中。本论文深入研究了FIML优化自注意力（FOSA）框架，这是一种融合了全信息最大似然（FIML）估计和自注意力神经网络能力的创新方法。我们的方法首先通过FIML对缺失值进行初始估计，然后通过利用自注意力机制来进一步提炼这些估计值。我们在模拟数据集和真实数据集上的全面实验证明了FOSA相对于传统的FIML技术在准确性、计算效率和适应不同数据结构方面的显著优势。有趣的是，即使在结构方程模型（SEM）可能错误规定导致子优的FIML估计的情况下，FOSA自注意力组件的稳健架构能够灵活地纠正和优化补全结果。

    In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputati
    
[^46]: 整合鲁莽行为到基于协同过滤的推荐系统中

    Incorporating Recklessness to Collaborative Filtering based Recommender Systems. (arXiv:2308.02058v1 [cs.IR])

    [http://arxiv.org/abs/2308.02058](http://arxiv.org/abs/2308.02058)

    本文提出了一种将鲁莽行为引入基于矩阵分解的推荐系统学习过程的方法，通过控制风险水平来提高预测的数量和质量。

    

    包含可靠性测量的推荐系统往往在预测中更加保守，因为它们需要保持可靠性。这导致了这些系统可以提供的覆盖范围和新颖性的显著下降。在本文中，我们提出了在矩阵分解型推荐系统的学习过程中加入一项新的项，称为鲁莽行为，它可以控制在做出关于预测可靠性的决策时所希望的风险水平。实验结果表明，鲁莽行为不仅允许进行风险调控，还提高了推荐系统提供的预测的数量和质量。

    Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
    
[^47]: 多重保护属性的公平性改善的实证研究

    An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])

    [http://arxiv.org/abs/2308.01923](http://arxiv.org/abs/2308.01923)

    本文通过广泛研究，发现对于单个保护属性的公平性改善会大大降低对未考虑保护属性的公平性，但在多属性模式下可以保持准确性。

    

    现有研究主要关注单个保护属性的机器学习（ML）软件的公平性改善，但考虑到许多用户具有多个保护属性，这是不现实的。本文对多个保护属性的公平性改善进行了广泛研究，涵盖了11种最先进的公平性改善方法。我们分析了在考虑多个保护属性时，这些方法在不同数据集、评估指标和ML模型上的有效性。结果显示，改善单个保护属性的公平性大大降低了未考虑的保护属性的公平性。在88.3％的情况下观察到这种降低（平均为57.5％）。更令人惊讶的是，在考虑单个和多个保护属性时，准确率损失方面几乎没有差异，这表明在多属性模式下可以保持准确性。然而，在处理多个保护属性时，精确度和召回率的影响较大。

    Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
    
[^48]: 基于神经薛定谔锻造的混合基态量子算法

    Hybrid Ground-State Quantum Algorithms based on Neural Schr\"odinger Forging. (arXiv:2307.02633v1 [quant-ph])

    [http://arxiv.org/abs/2307.02633](http://arxiv.org/abs/2307.02633)

    提出了一种基于神经网络的纠缠锻造方法来解决基态问题，通过识别最相关的基态位串，消除了指数级求和的需求，并展示了该方法在不同系统上可以达到相当或更优的性能。

    

    基于纠缠锻造的变分算法利用量子系统的双分割来解决基态问题。这些方法的主要限制在于对整个系统的Schmidt分解时需要对无数潜在基态进行指数级求和。为了克服这个挑战，我们提出了一种新的纠缠锻造方法，利用生成性神经网络来识别最相关的基态位串，消除了指数级求和的需求。通过在复杂度递增的系统上进行实证演示，我们展示了所提出的算法与现有的纠缠锻造标准实现相比可达到相当或更优的性能。此外，通过控制所需资源的数量，该方案可以应用于更大的非置换不变系统，后者限制与海森伯锻造过程相关。

    Entanglement forging based variational algorithms leverage the bi-partition of quantum systems for addressing ground state problems. The primary limitation of these approaches lies in the exponential summation required over the numerous potential basis states, or bitstrings, when performing the Schmidt decomposition of the whole system. To overcome this challenge, we propose a new method for entanglement forging employing generative neural networks to identify the most pertinent bitstrings, eliminating the need for the exponential sum. Through empirical demonstrations on systems of increasing complexity, we show that the proposed algorithm achieves comparable or superior performance compared to the existing standard implementation of entanglement forging. Moreover, by controlling the amount of required resources, this scheme can be applied to larger, as well as non permutation invariant systems, where the latter constraint is associated with the Heisenberg forging procedure. We substan
    
[^49]: 可扩展的非均匀超图的张量方法

    Scalable tensor methods for nonuniform hypergraphs. (arXiv:2306.17825v1 [math.NA])

    [http://arxiv.org/abs/2306.17825](http://arxiv.org/abs/2306.17825)

    本论文提出了一种可扩展的张量方法，用于处理非均匀超图。通过开发新的TTSV算法，我们能够在低于指数复杂度的情况下处理邻接张量，并应用于超图中心性和聚类等问题。这些方法不仅能提供与图缩减方法互补的信息，还能够探测到高阶结构。

    

    尽管多线性代数在研究由超图模拟的多方交互方面似乎很自然，但通用超图的张量方法受到理论和实际限制的阻碍。最近提出的邻接张量适用于非均匀超图，但在实践中形成和分析它是代价高昂的。我们开发了这个张量的张量乘相同向量（TTSV）算法，将复杂度从$O(n^r)$降低到$r$的低次多项式，其中$n$是顶点的数量，$r$是最大超边大小。我们的算法是隐式的，避免了形成$r$阶邻接张量。通过开发基于张量的超图中心性和聚类算法，我们展示了我们方法的灵活性和实用性。我们还展示了这些张量度量在数据上与类似的图缩减方法提供互补信息，并且还能够检测到许多现有基于矩阵的方法无法检测到的高阶结构。

    While multilinear algebra appears natural for studying the multiway interactions modeled by hypergraphs, tensor methods for general hypergraphs have been stymied by theoretical and practical barriers. A recently proposed adjacency tensor is applicable to nonuniform hypergraphs, but is prohibitively costly to form and analyze in practice. We develop tensor times same vector (TTSV) algorithms for this tensor which improve complexity from $O(n^r)$ to a low-degree polynomial in $r$, where $n$ is the number of vertices and $r$ is the maximum hyperedge size. Our algorithms are implicit, avoiding formation of the order $r$ adjacency tensor. We demonstrate the flexibility and utility of our approach in practice by developing tensor-based hypergraph centrality and clustering algorithms. We also show these tensor measures offer complementary information to analogous graph-reduction approaches on data, and are also able to detect higher-order structure that many existing matrix-based approaches p
    
[^50]: 基于量子机器学习的金融预测的改进

    Improved Financial Forecasting via Quantum Machine Learning. (arXiv:2306.12965v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.12965](http://arxiv.org/abs/2306.12965)

    本研究利用量子机器学习提升了金融预测的表现，包括使用行列式点过程来增强随机森林模型进行流失预测并设计了量子神经网络架构用于信用风险评估，比传统方法使用更少的参数达到相似的性能。

    

    量子算法有潜力提高机器学习在各领域和应用中的表现。在本文中，我们展示了如何使用量子机器学习来改进金融预测。首先，我们使用经典和量子行列式点过程来增强随机森林模型以进行流失预测，提高了近6％的精度。其次，我们设计了具有正交和复合层的量子神经网络架构，用较少的参数达到了与经典性能相当的信用风险评估效果。我们的结果表明，利用量子思想可以有效提升机器学习的表现，无论是现在作为量子启发式的经典ML解决方案，还是在未来更好的量子硬件的到来时。

    Quantum algorithms have the potential to enhance machine learning across a variety of domains and applications. In this work, we show how quantum machine learning can be used to improve financial forecasting. First, we use classical and quantum Determinantal Point Processes to enhance Random Forest models for churn prediction, improving precision by almost 6%. Second, we design quantum neural network architectures with orthogonal and compound layers for credit risk assessment, which match classical performance with significantly fewer parameters. Our results demonstrate that leveraging quantum ideas can effectively enhance the performance of machine learning, both today as quantum-inspired classical ML solutions, and even more in the future, with the advent of better quantum hardware.
    
[^51]: 神经网络中对抗性漏洞攻击的实用性测试：动态学习的影响

    Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning. (arXiv:2306.05494v1 [cs.CR])

    [http://arxiv.org/abs/2306.05494](http://arxiv.org/abs/2306.05494)

    本文对于基于机器学习的网络入侵检测系统(NIDS)的对抗性攻击进行了分类，同时探究了持续再训练对NIDS对抗性攻击的影响。实验表明，即使没有对抗性训练，持续再训练也可以减少对抗性攻击的影响。

    

    机器学习被广泛应用于网络入侵检测系统(NIDS)中，由于其自动化的特性和在处理和分类大量数据上的高精度。但机器学习存在缺陷，其中最大的问题之一是对抗性攻击，其目的是使机器学习模型产生错误的预测。本文提出了两个独特的贡献：对抗性攻击对基于机器学习的NIDS实用性问题的分类和对持续训练对NIDS对抗性攻击的影响进行了研究。我们的实验表明，即使没有对抗性训练，持续再训练也可以减少对抗性攻击的影响。虽然对抗性攻击可能会危及基于机器学习的NIDS，但持续再训练可带来一定的缓解效果。

    Machine Learning (ML) has become ubiquitous, and its deployment in Network Intrusion Detection Systems (NIDS) is inevitable due to its automated nature and high accuracy in processing and classifying large volumes of data. However, ML has been found to have several flaws, on top of them are adversarial attacks, which aim to trick ML models into producing faulty predictions. While most adversarial attack research focuses on computer vision datasets, recent studies have explored the practicality of such attacks against ML-based network security entities, especially NIDS.  This paper presents two distinct contributions: a taxonomy of practicality issues associated with adversarial attacks against ML-based NIDS and an investigation of the impact of continuous training on adversarial attacks against NIDS. Our experiments indicate that continuous re-training, even without adversarial training, can reduce the effect of adversarial attacks. While adversarial attacks can harm ML-based NIDSs, ou
    
[^52]: 截断亲和力最大化：用于图形异常监测的单类同型建模

    Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])

    [http://arxiv.org/abs/2306.00006](http://arxiv.org/abs/2306.00006)

    本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。

    

    我们在现实世界的图形异常监测（GAD）数据集中经常发现一种普遍的属性......本文提出了一种新的无监督异常评分度量 - 当前节点亲和力......我们进一步提出了截断亲和力最大化 (TAM)，该方法通过最大化与_neighbors的本地亲和力来学习量身定制的节点表示。本文所提方法在原始图形结构上进行优化，可以进行双重One-Class的GAD。

    One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
    
[^53]: 稳健的各向异性正则化

    Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])

    [http://arxiv.org/abs/2305.19358](http://arxiv.org/abs/2305.19358)

    本文提出了一种新颖的正则化方法I-STAR，可以增加模型的稳定性，提高性能，并改善自然语言处理中的组合表示问题。

    

    鉴于大型语言模型（LLMs）的成功，研究模型激活的属性已引起了相当大的兴趣。文献普遍认为LLMs表示由少数具有极高方差和幅度的“异常维度”主导。自然语言处理（NLP）中的几项研究试图减轻这些异常维度的影响，并迫使LLMs成为各向同性（即在嵌入空间中所有维度具有均匀方差）的。各向同性被认为是LLMs的一种理想属性，可以提高模型性能并更加贴近人类直觉的文本表示。然而，关于NLP中各向同性的许多观点都是基于嵌入的平均余弦相似度，最近已经表明这是一种有缺陷的各向同性度量。在本文中，我们提出了I-STAR：基于IsoScore$^{\star}$的稳定各向异性正则化，这是一种新颖的正则化方法，可以用于增加模型的稳定性并提高性能。

    Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
    
[^54]: CONSCENDI: 一种反对比且场景引导的蒸馏方法来为虚拟助手构建防护栏模型

    CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])

    [http://arxiv.org/abs/2304.14364](http://arxiv.org/abs/2304.14364)

    本文提出了一种名为CONSCENDI的蒸馏方法，用于构建防护栏模型，以监控任务型虚拟助手的输出。关键方法包括场景增强生成和对比训练样例。这种方法产生了一组多样化的违反规则的对话训练集，并且可以更好地检测代理的输出是否符合设计者指定的规则。

    

    随着GPT-4等越来越强大的语言模型的出现，新一代的基于任务的虚拟助手应运而生。这些对话系统可以根据客户的具体用例进行定制，但确保代理生成的文本仅符合提示指令中设计者指定的规则是具有挑战性的。因此，聊天机器人设计师通常使用另一个称为防护栏模型的模型来验证代理输出是否与其规则和约束对齐。我们探索了使用蒸馏方法来构建防护栏模型，以监控使用GPT-4中的训练数据的第一个模型的输出。我们发现，我们的CONSCENDI过程包括两个关键步骤：场景增强生成和对比训练样例。在生成对话数据时，我们会生成一组违反规则的场景，这些场景列举了违反规则的多样化高级方式。这种场景引导方法产生了一组多样化的违反规则的对话训练集，并且它使得模型更容易检测到代理生成的文本是否符合设计者指定的规则。

    A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
    
[^55]: OptoGPT：一种用于光学多层薄膜结构反向设计的基础模型

    OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer Thin Film Structures. (arXiv:2304.10294v1 [physics.optics])

    [http://arxiv.org/abs/2304.10294](http://arxiv.org/abs/2304.10294)

    OptoGPT是一种基于大型数据集训练的仅包含解码器的Transformer模型，可以自主全局设计探索，同时选择材料和厚度，用于光学多层薄膜结构反向设计。

    

    基础模型是大型机器学习模型，一旦训练完成即可解决各种下游任务，在自然语言处理、计算机视觉和强化学习等领域引领研究趋势。然而，目前还没有适用于光学多层薄膜结构反向设计的基础模型。当前的反向设计算法要么不能探索全局设计空间，要么计算效率低下。为了填补这一空白，我们提出了Opto Generative Pretrained Transformer（OptoGPT）。OptoGPT是一个仅包含解码器的Transformer，可以根据特定的频谱目标自回归地生成设计。通过训练一组大型数据集（1000万个设计），我们的模型展现了卓越的能力: 1）自主全局设计探索，通过确定层数（高达20层），同时选择每个层的材料（高达18种不同类型）和厚度；2）高效的结构颜色设计，吸收器，滤波器，分布反射镜。

    Foundation models are large machine learning models that can tackle various downstream tasks once trained on diverse and large-scale data, leading research trends in natural language processing, computer vision, and reinforcement learning. However, no foundation model exists for optical multilayer thin film structure inverse design. Current inverse design algorithms either fail to explore the global design space or suffer from low computational efficiency. To bridge this gap, we propose the Opto Generative Pretrained Transformer (OptoGPT). OptoGPT is a decoder-only transformer that auto-regressively generates designs based on specific spectrum targets. Trained on a large dataset of 10 million designs, our model demonstrates remarkable capabilities: 1) autonomous global design exploration by determining the number of layers (up to 20) while selecting the material (up to 18 distinct types) and thickness at each layer, 2) efficient designs for structural color, absorbers, filters, distrib
    
[^56]: 用TiDE进行长期预测：时间序列稠密编码器

    Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])

    [http://arxiv.org/abs/2304.08424](http://arxiv.org/abs/2304.08424)

    TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。

    

    最近的研究表明，相比于基于Transformer的方法，简单的线性模型在长期时间序列预测中表现更好。鉴于此，我们提出了一种基于多层感知机(MLP)的编码器-解码器模型，即时间序列稠密编码器(TiDE)，用于长期时间序列预测。它既享有线性模型的简单性和速度，又能处理协变量和非线性依赖。从理论上讲，我们证明了我们模型的最简线性类比在一些假设下可以达到线性动态系统(LDS)的近乎最优误差率。实证上，我们表明，我们的方法可以在流行的长期时间序列预测基准测试中匹配或胜过以前的方法，同时比最佳的基于Transformer的模型快5-10倍。

    Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
    
[^57]: 学习和决策的风险自适应方法：一项调查

    Risk-Adaptive Approaches to Learning and Decision Making: A Survey. (arXiv:2212.00856v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2212.00856](http://arxiv.org/abs/2212.00856)

    本文调查了过去25年中风险测度的快速发展，介绍了其在各个领域的应用，以及与效用理论和分布鲁棒优化的关系，并指出了公平机器学习等新兴应用领域。

    

    不确定性在工程设计、统计学习和决策制定中普遍存在。由于固有的风险规避和对假设的模糊性，通常通过制定和解决使用风险和相关概念的保守优化模型来解决不确定性问题。我们对过去25年来风险测度的快速发展进行了调查。从它们在金融工程领域的起步，我们回顾了它们在几乎所有领域的工程和应用数学中的应用。风险测度扎根于凸分析，为处理不确定性提供了一个具有重要计算和理论优势的通用框架。我们描述了关键事实，列举了几种具体算法，并提供了大量参考文献供进一步阅读。该调查还回顾了与效用理论和分布鲁棒优化的联系，指出了新兴应用领域，如公平机器学习，并定义了相对测度。

    Uncertainty is prevalent in engineering design, statistical learning, and decision making broadly. Due to inherent risk-averseness and ambiguity about assumptions, it is common to address uncertainty by formulating and solving conservative optimization models expressed using measures of risk and related concepts. We survey the rapid development of risk measures over the last quarter century. From their beginning in financial engineering, we recount the spread to nearly all areas of engineering and applied mathematics. Solidly rooted in convex analysis, risk measures furnish a general framework for handling uncertainty with significant computational and theoretical advantages. We describe the key facts, list several concrete algorithms, and provide an extensive list of references for further reading. The survey recalls connections with utility theory and distributionally robust optimization, points to emerging applications areas such as fair machine learning, and defines measures of rel
    
[^58]: 面对适应性泛化：贝叶斯视角下的研究

    Generalization in the Face of Adaptivity: A Bayesian Perspective. (arXiv:2106.10761v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.10761](http://arxiv.org/abs/2106.10761)

    本文提出面对自适应选择数据样本引起的过度拟合问题，使用噪声加算法可以提供不依赖于查询规模的误差保证，这一结果表明适应数据分析的问题在于新查询与过去查询的协方差。

    

    自适应选择样本可能导致过度拟合，简单的噪声加算法可以避免这一问题。本文证明了噪声加算法可以提供不依赖于查询规模的误差保证，这一结果来源于更好地理解自适应数据分析的核心问题。我们表明适应数据分析的问题在于新查询与过去查询的协方差。

    Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the empirical evaluation of queries on the sample significantly deviates from their mean with respect to the underlying data distribution. It turns out that simple noise addition algorithms suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows that they can handle an asymptotically optimal number of queries. However, differential privacy's worst-case nature entails scaling such noise to the range of the queries even for highly-concentrated queries, or introducing more complex algorithms.  In this paper, we prove that straightforward noise-addition algorithms already provide variance-dependent guarantees that also extend to unbounded queries. This improvement stems from a novel characterization that illuminates the core problem of adaptive data analysis. We show that the harm of adaptivity results from the covariance between the new query and a 
    

