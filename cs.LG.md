# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction.](http://arxiv.org/abs/2308.05103) | 本研究提出了一种零射波自监督学习的多次扫描图像重建方法，用于改进扩散MRI，通过深度学习和虚拟线圈的应用，能够克服多次扫描中的相位变化问题，并提高图像重建质量。 |
| [^2] | [DOST -- Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels.](http://arxiv.org/abs/2308.05101) | 本文提出了一种名为"DOST"的方法，通过领域规则的纳入，利用领域遵从自监督训练的范式，解决了多标签分类任务中标签噪声的问题，并改善了学习性能和关键指标，减小了注释噪声的影响。 |
| [^3] | [A degree of image identification at sub-human scales could be possible with more advanced clusters.](http://arxiv.org/abs/2308.05092) | 该研究旨在通过扩展数据量和图像质量，利用自我监督学习方法在更先进的集群条件下实现了亚人类尺度下的人类级别物体检测性能。 |
| [^4] | [Bayesian Inverse Transition Learning for Offline Settings.](http://arxiv.org/abs/2308.05075) | 该论文提出了一种用于离线设置的贝叶斯逆转换学习方法，通过结合专家数据，可可靠地学习过渡动态T的后验分布，并且在不同数据集上显著减少政策的方差。 |
| [^5] | [Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language.](http://arxiv.org/abs/2308.05061) | 本文提出了一种使用传感器数据、方程和自然语言提示上下文中运算符学习的方法。通过整合人类知识和语言描述，该方法不仅扩展了物理信息学习的灵活性和普适性，而且显著提高了学习性能和减少了数据需求。 |
| [^6] | [A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique.](http://arxiv.org/abs/2308.05059) | 本文提出了一种新的即时参数更新方法来解决深度神经网络训练中的梯度消失问题，加速学习过程并在基准数据集上取得了优于最先进方法的成果。 |
| [^7] | [RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction.](http://arxiv.org/abs/2308.05046) | RadGraph2是一个新的数据集，用于从放射学报告中提取信息，并以分层信息提取模型HGIE为基础，能够更好地捕捉疾病进展和关系提取任务中的发现。 |
| [^8] | [Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems.](http://arxiv.org/abs/2308.05036) | 本文提出了一种协作式网络化无人机在UTM系统中进行宽带频谱感知和调度的数据驱动框架，通过融合多个无人机的分类输出来增强频谱感知模块的准确性，并利用强化学习方法将检测到的频谱空洞分配给次要用户进行动态调度。 |
| [^9] | [Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance.](http://arxiv.org/abs/2308.05034) | Kairos是第一个同时满足范围、攻击不可知性、时效性和攻击重建维度要求的溯源为基础的入侵检测和调查系统。 |
| [^10] | [Density Crop-guided Semi-supervised Object Detection in Aerial Images.](http://arxiv.org/abs/2308.05032) | 密度作物引导的半监督检测器通过识别小物体的聚类来提高在航拍图像中的性能。 |
| [^11] | [AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies.](http://arxiv.org/abs/2308.05027) | AbDiffuser是一个物理性扩散模型，用于联合生成抗体的三维结构和序列。该方法利用领域知识和基于物理的约束改善蛋白质扩散，处理序列长度变化，并能够生成与参考集合的序列和结构特性密切匹配的抗体。实验结果表明，AbDiffuser能够生成高水平表达的抗体，其中57.1%的设计选择是紧密结合剂。 |
| [^12] | [Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization.](http://arxiv.org/abs/2308.05021) | 扩散模型可能受到错误传播的影响，本文通过理论分析和数据实证验证了这个问题，并提出了一种一致性正则化方法来解决。我们的分析揭示了模型的去噪模块是否具有容错性以及如何减少分布差异。 |
| [^13] | [When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis.](http://arxiv.org/abs/2308.05017) | 本文提出了一个分析框架来研究何时和如何利用已知类别帮助发现新颖类别，并通过引入NCD谱对比损失（NSCL）提供了可证明的误差界限和NCD的充分必要条件。实验证明NSCL可以在常见的基准数据集上与或优于几个强基准方法，具有实际使用价值且理论保证。 |
| [^14] | [An Empirical Study of Bugs in Open-Source Federated Learning Framework.](http://arxiv.org/abs/2308.05014) | 本文通过实证研究发现了开源联邦学习框架中存在的错误，并对这些错误的特征进行了详细分析，为提高框架的安全性和稳定性提供了指导。 |
| [^15] | [Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories.](http://arxiv.org/abs/2308.05011) | 提出了一种专门用于处理具有不同内部类别的天文学异常检测方法，采用多类深度支持向量数据描述(MCDSVDD)方法，通过使用神经网络将数据映射到超球体中，每个超球体代表一个特定的内部类别，并计算样本与超球体中心的距离以确定异常分数。 |
| [^16] | [Transferable Models for Bioacoustics with Human Language Supervision.](http://arxiv.org/abs/2308.04978) | 本论文提出了一种基于对比语音-语言预训练的生物声学模型BioLingual，通过训练模型连接语言和音频表示，实现了跨物种识别和零样本学习，并在动物声音基准测试中取得了九项任务的最新最先进结果。 |
| [^17] | [Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning.](http://arxiv.org/abs/2308.04964) | 这篇论文介绍了Adversarial ModSecurity，它是一个使用强大的机器学习来对抗SQL注入攻击的防火墙。通过将核心规则集作为输入特征，该模型可以识别并防御对抗性SQL注入攻击。实验结果表明，AdvModSec在训练后能够有效地应对这类攻击。 |
| [^18] | [CasCIFF: A Cross-Domain Information Fusion Framework Tailored for Cascade Prediction in Social Networks.](http://arxiv.org/abs/2308.04961) | CasCIFF是一种专为社交网络中级联预测量身定制的跨领域信息融合框架，旨在解决深度学习方法中存在的用户属性表示不准确、忽视激活时间和无法整合时间和结构因素等问题。 |
| [^19] | [Representation Learning for Audio Privacy Preservation using Source Separation and Robust Adversarial Learning.](http://arxiv.org/abs/2308.04960) | 本文提出了一种使用源分离和对抗学习的音频隐私保护的表征学习方法，通过学习音频记录的潜在表征，提高语音隐私保护的效果。 |
| [^20] | [Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks.](http://arxiv.org/abs/2308.04958) | 本文提出了一个分布式强化学习框架，利用速度和垂直机动手段，在高密度环境中实现了自主的自我分离能力，从而改进了自主分离保障技术。 |
| [^21] | [Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection.](http://arxiv.org/abs/2308.04950) | 该论文分析了基于Transformer模型的BERT、ALBERT和RoBERTa在假新闻检测中的性能。研究发现，Transformer模型（特别是BERT）在处理文本上表现优异，但不同研究对性能评估和结论的实现方法存在差异。 |
| [^22] | [Methods for Acquiring and Incorporating Knowledge into Stock Price Prediction: A Survey.](http://arxiv.org/abs/2308.04947) | 这项调查论文系统而全面地描述了从不同来源获得外部知识的方法，以应用于股票价格预测，帮助理解股票市场的复杂性。 |
| [^23] | [Differentially Private Graph Neural Network with Importance-Grained Noise Adaption.](http://arxiv.org/abs/2308.04943) | 本文提出了一种差分隐私图神经网络方法，考虑了节点的重要性，并通过自适应差分隐私的方式保护节点信息的隐私。其中，根据拓扑结构和中心性信息推断节点重要性，通过节点重要性级别对邻域聚合进行隐私保护。 |
| [^24] | [An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.04938) | 本文深入分析了使用基于多智能体强化学习的反向传播方法进行通信学习的离散化方法。研究比较了几种先进的离散化方法，并提出了一种新的方法。 |
| [^25] | [JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition.](http://arxiv.org/abs/2308.04934) | JEDI是一种利用多个数据集的专家知识进行训练的半监督学习方法，能够解决跨数据集泛化和标签数据稀缺等问题。 |
| [^26] | [Deep Learning-Based Prediction of Fractional Flow Reserve along the Coronary Artery.](http://arxiv.org/abs/2308.04923) | 提出了一种基于深度学习的方法，从冠状CT血管造影（CCTA）扫描中预测冠状动脉的分数流预测（FFR），避免了侵入性测量，并且不仅预测了每个冠状动脉的FFR值，还提供了关于狭窄位置和治疗策略的信息。 |
| [^27] | [GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters.](http://arxiv.org/abs/2308.04905) | 本论文提出了一种名为GraphCC的实用图学习方法，用于优化数据中心的拥塞控制。通过结合多智能体强化学习（MARL）和图神经网络，该方法能够适应快速和突然的网络状态变化，并提高网络的性能。 |
| [^28] | [Towards true discovery of the differential equations.](http://arxiv.org/abs/2308.04901) | 本文研究了独立方程发现的先决条件和工具，并解决了在正确方程未知的情况下评估发现方程的挑战，旨在为在没有先验方程知识的情况下可靠地发现方程提供洞察力。 |
| [^29] | [An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures.](http://arxiv.org/abs/2308.04898) | 本研究通过使用大语言模型（LLMs）对历史软件供应链安全失败进行分析，评估了其能力。通过自动化分析，可以降低成本并实现对更多失败案例的研究。 |
| [^30] | [Why Data Science Projects Fail.](http://arxiv.org/abs/2308.04896) | 数据科学是一种现代数据智能实践，对于企业来说非常重要，但是数据科学项目失败的原因主要在于数据的可用性、算法和计算能力或基础设施等方面。 |
| [^31] | [NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?.](http://arxiv.org/abs/2308.04889) | 该报告关注当前最具影响力的AI论文，并以标准化引用计数为依据编制了40篇最受欢迎的论文列表。观察到在2023年上半年，大型语言模型（LLMs）和具体而言的ChatGPT相关的论文占主导地位，ChatGPT表现出下降的趋势。 |
| [^32] | [Targeted and Troublesome: Tracking and Advertising on Children's Websites.](http://arxiv.org/abs/2308.04887) | 这项研究提供了针对面向儿童的网站进行追踪和广告的详细测量。通过构建一个多语言分类器，并对过两百万个网页进行分类，研究者们编制了一个儿童网站列表。通过对这些网站进行爬取和分析，研究发现追踪器、指纹脚本和广告都存在于这些网站上。同时，研究还检测到这些广告是否启用了定向。 |
| [^33] | [Decorrelating neurons using persistence.](http://arxiv.org/abs/2308.04870) | 本论文提出了一种使用持久性方法去除神经元之间高相关性的新方法，通过计算最小生成树的权重来构建正则化项，并通过大量实验证明了这些正则化项的有效性。结果表明，与常见的正则化项相比，这些正则化项能更好地提高深度学习模型的泛化能力，还发现冗余在人工神经网络中发挥着重要的作用。 |
| [^34] | [Scalability of Message Encoding Techniques for Continuous Communication Learned with Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.04844) | 本论文使用多智能体强化学习研究了在增加消息中包含的信息量和智能体数量增加的情况下，采用不同的消息编码方法对系统性能的影响。结果表明，均值消息编码器比注意力消息编码器更优，在智能体之间的通信协议中起到关键作用。 |
| [^35] | [Intrinsic Motivation via Surprise Memory.](http://arxiv.org/abs/2308.04836) | 该论文提出了一种新的计算模型，通过意外记忆作为内在奖励的基础，在强化学习中解决了现有方法的局限性。实验结果表明，通过结合意外预测器的意外记忆在稀疏奖励环境中表现出高效的探索行为，并显著提升了最终性能。 |
| [^36] | [TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks.](http://arxiv.org/abs/2308.04832) | 本文介绍了一种新的激活函数 TSSR，具有奇数、非线性、单调和可微分的特性。实验证实了TSSR相比其他激活函数具有更好的性能，对神经网络模型的发展具有重要意义，并适用于多个领域的广泛应用。 |
| [^37] | [Bayes Risk Consistency of Nonparametric Classification Rules for Spike Trains Data.](http://arxiv.org/abs/2308.04796) | 本文研究了非参数分类规则在脉冲序列数据中的贝叶斯风险一致性，并导出了最优的贝叶斯规则和插值的非参数核分类器。通过渐近性质证明了核分类器收敛到贝叶斯规则，并通过有限样本模拟研究进行了验证。 |
| [^38] | [PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer.](http://arxiv.org/abs/2308.04791) | PETformer是一个创新的模型，通过引入占位符增强技术，长子序列划分和多通道分离与交互的方法解决了将Transformer应用于长期时间序列预测时的关键问题。实验证明PETformer实现了最先进的性能。 |
| [^39] | [SUnAA: Sparse Unmixing using Archetypal Analysis.](http://arxiv.org/abs/2308.04771) | 本文介绍了一种使用典型分析的新型稀疏解混技术（SUnAA），通过设计新模型并使用主动集算法迭代地最小化优化目标，实现了对混合成分的准确解混。实验证明SUnAA在信号重建误差方面具有比其他方法更好的性能。 |
| [^40] | [Tram-FL: Routing-based Model Training for Decentralized Federated Learning.](http://arxiv.org/abs/2308.04762) | Tram-FL是一种基于路由的去中心化联邦学习（DFL）模型训练方法，通过逐步传输全局模型而不是本地模型的交换和汇聚，以及动态模型路由算法，实现在非IID条件下高模型准确性和降低通信成本。 |
| [^41] | [Feature Matching Data Synthesis for Non-IID Federated Learning.](http://arxiv.org/abs/2308.04761) | 本文提出了一种用于非独立同分布联邦学习的特征匹配数据合成方法，通过生成合成数据来解决联邦学习中的非独立同分布问题，并提出了一种隐私保护的特征增强方法。这种方法能够改善模型的泛化能力并抹去真实特征的信息。 |
| [^42] | [Collaborative Learning From Distributed Data With Differentially Private Synthetic Twin Data.](http://arxiv.org/abs/2308.04755) | 本研究提出了一种差分隐私合成对数据进行分布式协作学习的框架，在真实世界健康数据的实验中验证了其可行性。合作学习方通过共享合成数据可以获得更准确的统计估计，尤其对于包含少数群体的数据方有帮助。 |
| [^43] | [Universal Fuzzing via Large Language Models.](http://arxiv.org/abs/2308.04748) | 本文介绍了Fuzz4All，这是第一个能够针对许多不同的输入语言和这些语言的许多不同功能进行模糊测试的通用工具。 |
| [^44] | [Optimizing a Transformer-based network for a deep learning seismic processing workflow.](http://arxiv.org/abs/2308.04739) | 这项研究对基于Transformer的地震处理网络StorSeismic进行优化，通过改进位置编码和注意力机制，提高了其效率和表现力，并在实际数据上取得了良好的结果。 |
| [^45] | [Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations.](http://arxiv.org/abs/2308.04735) | 该论文提出了使用五点卷积神经网络进行反应-扩散方程深入研究的方法，并提出了具有较大感受野的深层FCNNs，可以预测具有大于CFL条件阈值的时间演化。 |
| [^46] | [JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models.](http://arxiv.org/abs/2308.04729) | JEN-1是一个高保真度通用音乐生成模型，通过结合自回归和非自回归训练，实现了文本引导的音乐生成、音乐修补和延续等生成任务，在文本音乐对齐和音乐质量方面表现出优越性，同时保持计算效率。 |
| [^47] | [Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning.](http://arxiv.org/abs/2308.04712) | 本研究通过利用预训练语言模型的探测和对比学习机制，在槽位归纳任务中，成功地诱导了槽位边界，并在两个NLU基准数据集上表现出与令牌级监督模型相当的性能，同时也能提供增强的槽位标签表示。 |
| [^48] | [Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution.](http://arxiv.org/abs/2308.04708) | 本文提出了一种新颖的概率异常归因框架，通过生成扰动来计算每个输入变量的归因得分的概率分布，并量化这些得分的不确定性。 |
| [^49] | [Pareto Invariant Representation Learning for Multimedia Recommendation.](http://arxiv.org/abs/2308.04706) | 本文介绍了一种名为Pareto Invariant Representation Learning（PaInvRL）的框架，应用于多媒体推荐。该框架通过学习不变表示和变体表示的同时来缓解通用表示引入的错误相关性问题。从IID-OOD多目标优化的角度，PaInvRL减少了错误相关性对用户偏好的影响。 |
| [^50] | [A Feature Set of Small Size for the PDF Malware Detection.](http://arxiv.org/abs/2308.04704) | 该研究提出了一种小型特征集，用于检测PDF恶意软件，无需太多领域知识。在六种机器学习模型中，使用Random Forest模型时可以达到99.75%的准确性。这个仅包含12个特征的特征集是最简洁的之一，尽管规模较小，但结果可与采用更大特征集的最先进方法相媲美。 |
| [^51] | [An Analytical Study of Covid-19 Dataset using Graph-Based Clustering Algorithms.](http://arxiv.org/abs/2308.04697) | 该研究使用基于图的聚类算法对Covid-19数据集进行了分析。研究结果对疾病药物研发和人们免受大流行病伤害具有重要意义。 |
| [^52] | [Explainable AI in Orthopedics: Challenges, Opportunities, and Prospects.](http://arxiv.org/abs/2308.04696) | 解释性人工智能在骨科中的应用面临挑战，但也带来机遇。为了实现透明度和可解释性，需要开发注重人工智能模型的透明性和可解释性的算法，并促进跨学科合作。 |
| [^53] | [Finite Element Operator Network for Solving Parametric PDEs.](http://arxiv.org/abs/2308.04690) | 本文提出了一种新方法，通过有限元算子网络（FEONet）解决参数PDE。它结合了深度学习和传统数值方法，展示了在没有输入-输出训练数据的情况下解决参数PDE的有效性，并在准确度、泛化性和计算灵活性方面优于现有方法。 |
| [^54] | [A General Implicit Framework for Fast NeRF Composition and Rendering.](http://arxiv.org/abs/2308.04669) | 本研究提出了一个通用的隐式框架，可以快速合成和渲染NeRF对象。通过引入神经深度场的新的表面表示方法，可以实现动态阴影并允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。 |
| [^55] | [Classification of lung cancer subtypes on CT images with synthetic pathological priors.](http://arxiv.org/abs/2308.04663) | 本文提出了一种使用合成病理学先验知识的分类方法，通过深度神经网络从CT图像中提取信息并将其与病理学知识结合，实现了对肺癌亚型的准确分类。 |
| [^56] | [Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained on Multiple Heterogeneous Datasets.](http://arxiv.org/abs/2308.04660) | 本文提出了一种高效的贝叶斯优化方法，使用变换器预训练的深度核学习和多元异构数据集。通过从先前任务中学习来共同预训练替代模型，并通过简单有效的混合初始化策略加速新任务的收敛，实验结果证明了方法的有效性。 |
| [^57] | [Assessing the performance of deep learning-based models for prostate cancer segmentation using uncertainty scores.](http://arxiv.org/abs/2308.04653) | 本研究比较了多种基于深度学习的模型来进行前列腺癌分割，并评估了不确定性。最佳模型Attention R2U-Net在分割所有区域时具有最高的IoU和DSC，并且在过渡区域和肿瘤边界具有最低的不确定性值。 |
| [^58] | [Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals.](http://arxiv.org/abs/2308.04650) | 本研究提出了一种利用心电图信号进行血流动力学推理的深度度量学习方法。通过使用非侵入性信号进行心脏压力的评估，既可以在住院环境中用于诊断和治疗预后，也可以应用于门诊设置中。 |
| [^59] | [Enhancing Optimization Performance: A Novel Hybridization of Gaussian Crunching Search and Powell's Method for Derivative-Free Optimization.](http://arxiv.org/abs/2308.04649) | 通过将高斯崩溃搜索(GCS)与传统无导数优化方法混合，可以提高优化性能并打开新的可能性。 |
| [^60] | [Sparse Binary Transformers for Multivariate Time Series Modeling.](http://arxiv.org/abs/2308.04637) | 本研究将稀疏二值变压器应用于多元时间序列问题，并展示了该轻量级模型在分类、异常检测和单步预测任务中取得了与密集浮点变压器相当的准确度。同时，通过两个修改降低了注意机制的计算复杂性。 |
| [^61] | [Multiclass Online Learnability under Bandit Feedback.](http://arxiv.org/abs/2308.04620) | Bandit反馈下的在线多类学习的关键在于Bandit Littlestone维度的有限性，无论标签空间是否无界。 |
| [^62] | [Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection.](http://arxiv.org/abs/2308.04617) | 该论文提出了一种改进的激活剪裁方法用于普适性后门缓解和测试时间检测，该方法通过限制激活边界来提高对后门攻击的鲁棒性并在图像分类任务中表现出优越性能。 |
| [^63] | [Machine Learning, Deep Learning and Data Preprocessing Techniques for Detection, Prediction, and Monitoring of Stress and Stress-related Mental Disorders: A Scoping Review.](http://arxiv.org/abs/2308.04616) | 这项综述研究系统评估了机器学习在压力和压力相关精神障碍检测、预测和分析方面的应用。支持向量机、神经网络和随机森林模型展现出了更高的准确性和可靠性，并且生理参数如心率测量和皮肤反应在压力预测中被广泛使用。 |
| [^64] | [Sparse Array Design for Direction Finding using Deep Learning.](http://arxiv.org/abs/2308.04615) | 该论文介绍了使用深度学习设计稀疏数组的几个方向找寻应用，包括认知雷达、无线通信和综合感知和通信（ISAC）应用等。这些方法具有特征工程和低预测阶段复杂性的优势。 |
| [^65] | [Deep Learning Driven Detection of Tsunami Related Internal GravityWaves: a path towards open-ocean natural hazards detection.](http://arxiv.org/abs/2308.04611) | 该论文提出了一个利用深度学习驱动的方法，通过检测电离层中的内部重力波来实现海啸的早期预警系统，提供了开放海洋覆盖。这个方法利用了大量的GNSS数据，通过处理成千上万个数据流之间的复杂非线性关系来增强海啸检测能力。 |
| [^66] | [PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data.](http://arxiv.org/abs/2308.04605) | PSRFlow是一种基于流式模型的科学数据超分辨率算法，能够量化超分辨结果的误差和不确定性，以及生成多个可行的超分辨率输出。 |
| [^67] | [A Survey on Decentralized Federated Learning.](http://arxiv.org/abs/2308.04604) | 最近几年，联邦学习成为训练分布式、大规模、保护隐私的机器学习系统的流行范式。然而，其中一个关键挑战是克服集中式编排的单点故障问题。 |
| [^68] | [Deep Learning based Image Watermarking: A Brief Survey.](http://arxiv.org/abs/2308.04603) | 该论文调查了基于深度学习的图像水印技术的最新研究进展，将其分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。对每个类别的研究方向进行了分析和总结，并讨论了未来的研究方向。 |
| [^69] | [Quantization Aware Factorization for Deep Neural Network Compression.](http://arxiv.org/abs/2308.04595) | 本论文提出了一种量化感知分解方法，通过在量化因子上找到张量逼近，既实现了神经网络压缩，又保持了模型预测的准确性。该方法使用交替方向乘子法进行规范多线性分解，并与最新的后训练量化方法进行比较分析。 |
| [^70] | [ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems.](http://arxiv.org/abs/2308.04588) | ScatterUQ是一个交互式系统，通过可视化展示模型在上下文驱动的不确定性设置下的性能，为用户提供了更好的理解模型预测的能力。 |
| [^71] | [Kernel Single Proxy Control for Deterministic Confounding.](http://arxiv.org/abs/2308.04585) | 本研究考虑了具有未观测混淆因素的因果效应估计问题，在结果是确定性生成的情况下，提出了一种使用单一代理变量的内核方法，通过两阶段回归和最大矩约束的方法可以一致估计因果效应，并在合成数据集上成功恢复了因果效应。 |
| [^72] | [RECipe: Does a Multi-Modal Recipe Knowledge Graph Fit a Multi-Purpose Recommendation System?.](http://arxiv.org/abs/2308.04579) | RECipe是一个多用途的菜谱推荐框架，使用多模态知识图谱作为支撑，通过在用户以自然语言查询或提供图像时向用户推荐菜谱，超越传统推荐系统的方法。 |
| [^73] | [From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data.](http://arxiv.org/abs/2308.04553) | 本文提出了一个两阶段训练流程，通过在一个平衡的合成数据集上进行预训练，然后在真实数据上进行微调，减少了视觉识别模型学习到与数据集偏差相关的错误的问题。 |
| [^74] | [Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining.](http://arxiv.org/abs/2308.04551) | 本研究旨在探索在噪声标签下仅使用自监督预训练方法，以改善医学图像分类。通过对比和预训练任务为基础的自监督方法，我们发现这些方法在医学图像上也具有改进学习效果的潜力。 |
| [^75] | [Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures.](http://arxiv.org/abs/2308.04539) | 本研究提出了一种受生物启发的轻量级神经网络架构，通过本地误差信号实现在线连续学习，克服了传统方法的局限性，在多个数据集上展现出优秀的表现。 |
| [^76] | [Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP.](http://arxiv.org/abs/2308.04524) | 本研究调查了学术界和工业界在自然语言处理（NLP）领域的合作对研究的影响。结果显示，学术界和工业界的合作出版物数量有增长趋势，并且这些出版物往往比仅由学术界产生的出版物具有更高的影响力。 |
| [^77] | [Deep Learning for Diverse Data Types Steganalysis: A Review.](http://arxiv.org/abs/2308.04522) | 本综述论文详细综述了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的最新研究进展。 |
| [^78] | [MT-IceNet -- A Spatial and Multi-Temporal Deep Learning Model for Arctic Sea Ice Forecasting.](http://arxiv.org/abs/2308.04511) | 提出了一种名为MT-IceNet的深度学习模型，用于预测北极海冰浓度。该模型采用了空间和多时序结构，利用编码器-解码器架构和跳跃连接处理多时序输入流，并可生成未来时间步骤的空间地图。 |
| [^79] | [Investigation of compressor cascade flow based on physics-informed neural networks.](http://arxiv.org/abs/2308.04501) | 本研究首次使用物理信息神经网络（PINNs）方法来预测压缩机叶栅的流场，结果表明PINNs在预测精度和处理反向问题方面比传统CFD方法具有明显优势。 |
| [^80] | [Efficient option pricing with unary-based photonic computing chip and generative adversarial learning.](http://arxiv.org/abs/2308.04493) | 本研究提出了一种基于一元光子计算芯片和生成对抗学习的高效期权定价方法，通过引入量子幅度估计算法，相比传统方法实现了二次加速。其中，生成对抗网络用于高效学习和加载资产分布，准确捕捉市场趋势。这项工作为金融应用中的专用光子处理器发展提供了新的思路。 |
| [^81] | [D-Score: A Synapse-Inspired Approach for Filter Pruning.](http://arxiv.org/abs/2308.04470) | 本文通过受突触启发的方法，提出了一种用于过滤修剪的方法，通过分析滤波器中正负权重的独立重要性，并排名进行修剪，从而显著减少FLOPs和Params数量并保持准确率稳定。 |
| [^82] | [Correlating Medi- Claim Service by Deep Learning Neural Networks.](http://arxiv.org/abs/2308.04469) | 本论文通过使用深度学习神经网络，结合回归模型的相关性研究，利用卷积神经网络架构来检测医疗保险索赔中的欺诈行为，并使用有监督和无监督分类器来区分欺诈和非欺诈索赔。 |
| [^83] | [Backdoor Federated Learning by Poisoning Backdoor-Critical Layers.](http://arxiv.org/abs/2308.04466) | 该论文研究了后门联邦学习中后门关键层的存在，并提出了一种针对这些层的新型后门攻击方法，旨在在各种防御策略下实现攻击效果和隐蔽性之间的平衡。 |
| [^84] | [Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller.](http://arxiv.org/abs/2308.04462) | 本研究通过基于强化学习的肌肉控制器，探索了利用质心状态空间监测人类平衡能力的可行性。 |
| [^85] | [The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data.](http://arxiv.org/abs/2308.04460) | Pangu天气预报模型与不同的数值模拟预报系统的模型初始条件兼容，并且具有相对稳定的预测能力。 |
| [^86] | [MCTS guided Genetic Algorithm for optimization of neural network weights.](http://arxiv.org/abs/2308.04459) | 本文研究了将MCTS引导的遗传算法应用于神经网络权重优化的可能性，并提出了一种有效的树搜索策略，用于探索整个基因树结构，以最优方式搜索使用遗传算法产生的最佳结果。 |
| [^87] | [A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems.](http://arxiv.org/abs/2308.04457) | 本文批判性地审查了物理驱动的机器学习在地下能源系统中的应用，强调了PIML在地震应用、油藏模拟和油气生产等任务中的成功利用。 |
| [^88] | [High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning.](http://arxiv.org/abs/2308.04450) | 本研究提出使用ResNets-10模型对金属-绝缘体-金属介质布面进行高精度预测。通过两阶段训练和小学习率进行训练，实现了超低误差值的预测结果。该网络可以取代传统的电磁计算方法，并可大幅降低设计过程时间。 |
| [^89] | [Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI.](http://arxiv.org/abs/2308.04448) | 本文讨论了生成AI领域的治理问题，提出了将集中监管与众包安全机制相结合的双重治理方式。集中监管存在缺乏明确性和统一性等问题，而众包安全机制则存在缺乏统一性和合规性的不足。该研究旨在促进生成AI领域的负责任和道德发展。 |
| [^90] | [Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc.](http://arxiv.org/abs/2308.04445) | 对于未来的AI，需要从生成式AI转向可信赖的AI。通过培养基于明确知识和经验规则的AI，可以解决当前方法的限制并实现推理过程的可信赖和可解释性。 |
| [^91] | [Changes in Policy Preferences in German Tweets during the COVID Pandemic.](http://arxiv.org/abs/2308.04444) | 这项研究提供了一种量化德国推特上COVID疫情期间政策偏好的方法，通过建立细粒度政治偏好的数据集，并使用文本分类模型进行分析，结果显示政治观点在疫情期间有所增加，研究还突出了不同政治类别的变化。 |
| [^92] | [Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining.](http://arxiv.org/abs/2308.04396) | 本论文提出了一种定制的企业协作系统的事件抽象方法，通过比较实际用户活动和系统生成的低级别跟踪来训练模型，并将低级别跟踪转换为抽象的高级别日志，以支持社会流程挖掘。 |
| [^93] | [SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling.](http://arxiv.org/abs/2308.04365) | SLEM是一种路径建模技术，通过集成机器学习超级学习者，实现了一致且无偏的因果效应估计，并在处理非线性关系时超过了传统的结构方程模型。 |
| [^94] | [OpinionConv: Conversational Product Search with Grounded Opinions.](http://arxiv.org/abs/2308.04226) | OpinionConv是第一个用于模拟销售对话的对话式AI，通过利用产品评论作为观点的丰富来源，实现了对话和决策中的真实性和信息基础。 |
| [^95] | [Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning.](http://arxiv.org/abs/2308.03999) | 本文提供了一种使用结构化背景知识和演绎推理的方法，用于解释CNN隐藏神经元的激活。该方法能够提供有意义的解释，解决了深度学习系统黑盒特性的问题。 |
| [^96] | [Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces.](http://arxiv.org/abs/2308.03443) | 本文提出了一种用于具有大动作空间的离策略评估的双重稳健估计器（MDR）。与现有的基准估计器相比，MDR能够在减小方差的同时保持无偏性，从而提高了估计的准确性。实验结果证实了MDR相对于现有估计器的优越性。 |
| [^97] | [Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning.](http://arxiv.org/abs/2308.03217) | 本文提出了一种本地一致增强的Siamese网络用于双视图对应学习，通过增强特征的一致性强化内点相关性、抑制异常点的干扰，并使用正向和反向映射的信息来监督网络训练。 |
| [^98] | [Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series.](http://arxiv.org/abs/2308.03210) | 本研究提出了时间参数化卷积神经网络（TPCNN），通过时间显式初始化的核函数来参数化卷积层，以处理不规则采样的多变量时间序列。这一方法增强了对连续时间隐藏动态的学习。 |
| [^99] | [Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting.](http://arxiv.org/abs/2308.02582) | 该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。 |
| [^100] | [AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification.](http://arxiv.org/abs/2308.02182) | AutoML4ETC是一个自动设计高效且高性能神经架构的工具，用于加密流量分类。其通过定义新颖的搜索空间和使用不同的搜索策略，在多个数据集上优于当前最先进的加密流量分类器。 |
| [^101] | [A Novel Convolutional Neural Network Architecture with a Continuous Symmetry.](http://arxiv.org/abs/2308.01621) | 本文介绍了一种新的卷积神经网络架构，通过连续的对称性修改权重，具有与传统模型不同的特点，希望将对称性作为神经网络的新期望特性，并推广将偏微分方程的视角应用于ConvNet的分析和解释。 |
| [^102] | [Spatio-Temporal Branching for Motion Prediction using Motion Increments.](http://arxiv.org/abs/2308.01097) | 本论文提出了一种利用运动增量进行时空分支的运动预测网络，通过解耦时域和空域特征的学习，提取更多的运动信息。 |
| [^103] | [Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach.](http://arxiv.org/abs/2308.01011) | 本文提出了一种叫做Floss的无监督方法，通过在频域上对学到的表示进行正则化来增强周期性时间序列的表示学习。Floss方法可以自动检测时间序列中的周期性并学习具有周期一致性的有意义的表示。 |
| [^104] | [An Exact Kernel Equivalence for Finite Classification Models.](http://arxiv.org/abs/2308.00824) | 本研究推导出梯度下降训练的有限分类模型的精确核表示，揭示了神经网络和核方法之间的等价性，并通过实验证明了精确核对神经网络预测的指导作用。 |
| [^105] | [ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2307.16186) | 本文提出了一种利用对称性先验知识的框架来解决多Agent强化学习中的数据效率问题，通过将数据增强和一致性损失集成到现有方法中，能够提高模型训练效率，并且泛化性能良好。 |
| [^106] | [Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples.](http://arxiv.org/abs/2307.14565) | Auto-Tables系统能自动合成多步转换的流水线，将非关系式表格转换为关系式表格，解决了非技术用户使用SQL分析工具的痛点。 |
| [^107] | [Efficiently Sampling the PSD Cone with the Metric Dikin Walk.](http://arxiv.org/abs/2307.12943) | 本文通过对Dikin步行方法进行分析并适应一般度量，为带约束的PSD锥体抽样问题提供了一种有效的解决方案，并提出了优化的自共轭矩阵函数概念。 |
| [^108] | [WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms.](http://arxiv.org/abs/2307.12449) | 本研究提出了一种名为WEPRO的新方法，通过利用参数权重中的规律趋势加速了混合量子-经典算法的收敛速度，相比标准训练方法，速度提高约2.25倍，准确性提高了2.3%，损失降低了6.1%。 |
| [^109] | [Tackling the Curse of Dimensionality with Physics-Informed Neural Networks.](http://arxiv.org/abs/2307.12306) | 本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。 |
| [^110] | [MSQNet: Actor-agnostic Action Recognition with Multi-modal Query.](http://arxiv.org/abs/2307.10763) | MSQNet是一种无关演员的多模态多标签动作识别方法，通过使用视觉和文本模态来更好地表示动作类别，克服了现有方法中针对特定演员的限制。 |
| [^111] | [An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient.](http://arxiv.org/abs/2307.08873) | 本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。 |
| [^112] | [Retentive Network: A Successor to Transformer for Large Language Models.](http://arxiv.org/abs/2307.08621) | Retentive Network（RetNet）作为大型语言模型的基础架构，实现了训练并行、低成本推理和良好的性能。通过并行、循环和分块循环三种计算范式，RetNet具有训练并行化、低成本推理和高效的长序列建模的特点。 |
| [^113] | [INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks.](http://arxiv.org/abs/2307.08131) | INFLECT-DGNN是一个结合了图神经网络和递归神经网络的框架，使用加权损失函数、针对图数据适应的合成少数过采样技术和滚动窗口策略，用于影响者预测。实验结果显示，使用RNN来编码时间属性和GNN显著提高了预测性能。 |
| [^114] | [A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media.](http://arxiv.org/abs/2307.06775) | 本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。 |
| [^115] | [Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models.](http://arxiv.org/abs/2307.06713) | 本文提出了一种使用大型语言模型进行文本分类的无监督校准方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行任务。 |
| [^116] | [A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection.](http://arxiv.org/abs/2307.03759) | 这项综述介绍了图神经网络在时间序列分析中的应用，包括预测、分类、异常检测和插值。图神经网络能够显式地建模时间序列和变量之间的关系，为时间序列数据分析带来了新的方法和技术。 |
| [^117] | [Distilled Pruning: Using Synthetic Data to Win the Lottery.](http://arxiv.org/abs/2307.03364) | 该论文介绍了一种使用蒸馏数据来修剪深度学习模型的新方法，能够比传统方法更快地找到稀疏的可训练子网络，具有资源高效的神经网络修剪潜力。 |
| [^118] | [Understanding recent deep-learning techniques for identifying collective variables of molecular dynamics.](http://arxiv.org/abs/2307.00365) | 这篇论文阐述了最近发展的深度学习方法在分子动力学中识别集体变量方面的应用。作者提供了两种不同的深度学习方法，一种基于微扰生成器和转移算符的主导特征函数，另一种通过学习自编码器来识别集体变量，并进行了比较性的数值研究。 |
| [^119] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^120] | [Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis.](http://arxiv.org/abs/2306.09417) | Diff-TTSG是第一个基于扩散的概率模型，用于联合学习合成语音和手势，相比于先前最新技术的非概率方法，它可以更好地捕捉人类讲话和运动的变化，产生更逼真和多样化的集成语音和手势合成。 |
| [^121] | [GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction.](http://arxiv.org/abs/2306.01951) | 本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。 |
| [^122] | [The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting.](http://arxiv.org/abs/2305.08992) | 该论文介绍了BraTS 2023修复挑战，要求参与者使用修补技术从有病变的脑部扫描中合成健康脑扫描，以解决许多算法无法分析病变图像的问题。 |
| [^123] | [DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors.](http://arxiv.org/abs/2305.05738) | DOCTOR是一种基于可穿戴医疗传感器的多疾病检测持续学习框架，采用了多头深度神经网络和Exemplar-replay风格的CL算法。它可以不断地学习新任务，并在内存使用、电池消耗和检测复杂度方面优于传统的ML驱动疾病检测方法。 |
| [^124] | [AttentionViz: A Global View of Transformer Attention.](http://arxiv.org/abs/2305.03210) | 这篇论文介绍了AttentionViz，一种以联合嵌入为基础的交互式可视化工具，用于帮助研究人员理解Transformer中的自我注意机制。该方法使得可以全局分析多个输入序列的注意力模式，提高对模型的理解并通过多个应用场景和专家反馈提供新的交互见解。 |
| [^125] | [Can Feature Engineering Help Quantum Machine Learning for Malware Detection?.](http://arxiv.org/abs/2305.02396) | 本文通过量子机器学习与特征选择策略相结合的混合框架，以降低恶意软件分类器培训时间，初步结果表明在模拟器上可以达到78.91％的测试准确性。 |
| [^126] | [Low-complexity subspace-descent over symmetric positive definite manifold.](http://arxiv.org/abs/2305.02041) | 本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。 |
| [^127] | [Quantum Natural Policy Gradients: Towards Sample-Efficient Reinforcement Learning.](http://arxiv.org/abs/2304.13571) | 本文提出了量子自然策略梯度(QNPG)算法，利用了量子费舍尔信息矩阵的高效近似方法，提高了强化学习的效率，实验结果表明，相比基于一阶梯度的训练，QNPG具有更快的收敛速度和稳定性，可以减少样本复杂度。 |
| [^128] | [Long-term Forecasting with TiDE: Time-series Dense Encoder.](http://arxiv.org/abs/2304.08424) | TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。 |
| [^129] | [DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection.](http://arxiv.org/abs/2304.00409) | 这个论文提供了一个包含150个CWE的新的漏洞源代码数据集，覆盖比以前所有数据集加起来多305个项目。作者通过增加训练数据的多样性和数量改进了深度学习模型在漏洞检测方面的表现。通过结合已有数据集，作者还研究了使用深度学习检测软件漏洞的挑战和前途的分析，结果表明目前深度学习检测漏洞仍存在高误报率，低F1分数和难以检测严重CWE的问题。 |
| [^130] | [On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks.](http://arxiv.org/abs/2303.17805) | 本文研究了具有不同尺度的非零权重的无限宽2层ReLU神经网络的正则化路径，展示该问题具有一个无限维度的凸对应，随着初始化的尺度在0到+∞范围内变化，关联路径在所谓的内核和丰富的区域之间连续插值。 |
| [^131] | [MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks.](http://arxiv.org/abs/2303.16839) | 提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。 |
| [^132] | [Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference.](http://arxiv.org/abs/2303.07122) | 该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。 |
| [^133] | [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference.](http://arxiv.org/abs/2303.04673) | 本文研究了优化大规模语言模型生成推理的成本效益超参数，通过经济的超参数优化和基于成本的修剪，提出了EcoOptiGen框架，该框架在使用GPT-3.5/GPT-4模型的任务中表现出有效性。 |
| [^134] | [Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics at Single-Precision.](http://arxiv.org/abs/2303.02898) | 本文的创新在于将最大熵时刻法稳定应用于单精度下的稀疏气体动力学，使其能够在现代GPU上模拟非常强的正常激波。 |
| [^135] | [Connectivity Optimized Nested Graph Networks for Crystal Structures.](http://arxiv.org/abs/2302.14102) | 连通性优化和更深的消息函数在晶体结构研究中的嵌套图网络中能够显著提高计算效率和性能。 |
| [^136] | [A Survey of Deep Learning: From Activations to Transformers.](http://arxiv.org/abs/2302.00722) | 这篇综述调查了深度学习领域的重要进展，包括各种架构、层、目标和优化技术的发展，以及关注机制、归一化、跳跃连接、Transformer和自监督学习等方法的变体。总结了成功创新的关键策略，并讨论了最近的商业闭源模型。 |
| [^137] | [Emergence of the SVD as an interpretable factorization in deep learning for inverse problems.](http://arxiv.org/abs/2301.07820) | 深度学习中的奇异值分解（SVD）在逆问题中成为可解释的因子化工具，通过与解密变换结合，可以用来解释神经网络（NN）在噪声参数估计问题中编码信号模型的结构 |
| [^138] | [Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization.](http://arxiv.org/abs/2212.10445) | 本论文提出了一种名为模型美食的新策略，通过回收基于同一基础模型在多样辅助任务上的多次微调，实现了在目标任务上的超出分布的泛化能力。通过利用辅助任务的多样性，这种策略旨在最大限度地提高模型权重的多样性。 |
| [^139] | [Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models.](http://arxiv.org/abs/2211.14946) | 本论文提出了一种名为自毁模型的方法，通过任务阻断范式实现对基础模型下游使用的更精确控制，从而降低基础模型的有害双重用途风险。 |
| [^140] | [RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System.](http://arxiv.org/abs/2211.06108) | 本论文提出了一种基于鸟瞰视角的引导框自由物体检测系统，通过雷达和激光雷达的特征融合学习，解决了在恶劣天气下物体检测的问题。 |
| [^141] | [Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis.](http://arxiv.org/abs/2211.02408) | 本文证明了文本生成图像模型中使用的文本编码器存在重大的篡改风险，并提出了一种基于反向门攻击的方法，可以插入一个单一字符触发器进提示中，从而触发模型生成具有预定义属性的图像或遵循隐藏的、潜在的恶意描述的图像。 |
| [^142] | [One-Shot Neural Fields for 3D Object Understanding.](http://arxiv.org/abs/2210.12126) | 本文提出了一种一次性神经场方法，用于机器人学中的3D对象理解。这种方法利用单个RGB图像构建统一而紧凑的场景表示，可以用于多个任务，如新视角渲染、3D重建、碰撞检查和稳定抓取预测。研究结果表明，这种方法能够从新视角进行渲染并预测成功的抓取。 |
| [^143] | [An out-of-distribution discriminator based on Bayesian neural network epistemic uncertainty.](http://arxiv.org/abs/2210.10780) | 本文研究了基于贝叶斯神经网络的越域鉴别器，并探讨了BNN中的不确定性类型以及如何计算。实验证明在训练数据集中较好表示的图像的先验不确定性较低，而在训练数据集中较差表示的图像中先验不确定性较高。文章还提出了一种基于BNN先验不确定性的越域检测算法，并通过实验证明了其有效性。 |
| [^144] | [Causal Fourier Analysis on Directed Acyclic Graphs and Posets.](http://arxiv.org/abs/2209.07970) | 本文提出了一种用于有向无环图的新型傅里叶分析方法，通过特征分解转换信号，关联数据与其因果关系，扩展了组合数学中的经典模比乌斯反演理论。 |
| [^145] | [3D-Aware Video Generation.](http://arxiv.org/abs/2206.14797) | 通过结合神经隐式表示和时间感知的判别器，我们开发了一个4D GAN框架，能够在没有条件的情况下生成3D感知视频，产生具有多视图和时态一致性的高质量图像，并学习了丰富的可分解的3D结构和运动。 |
| [^146] | [A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph Representations.](http://arxiv.org/abs/2204.05885) | 本文提出了一种层次分块距离模型（HBDM）用于图表示学习，可以处理大规模网络分析，考虑多尺度结构并且明确考虑到网络的同质性和传递性。 |
| [^147] | [BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification.](http://arxiv.org/abs/2203.01937) | 本文提出了一种适用于多标签、嘈杂CXR学习的方法，使用基于袋的多标签描述符平滑地重新标记数据集中的样本，并进行训练以提高模型性能。 |
| [^148] | [Inverse problem for parameters identification in a modified SIRD epidemic model using ensemble neural networks.](http://arxiv.org/abs/2203.00407) | 本文提出了一个基于集成神经网络的方法，用于识别修改的SIRD流行病模型中的参数。该方法考虑了死亡人数作为一个独立类别，并引入了一个衡量实际感染总数与官方统计感染人数差异的参数。通过依赖前7天的数据进行估计，我们可以预测未来的流行病情况。 |
| [^149] | [Imaginary Hindsight Experience Replay: Curious Model-based Learning for Sparse Reward Tasks.](http://arxiv.org/abs/2110.02414) | 这篇论文提出了一种虚拟回顾经验重放的简单基于模型的方法，适用于稀疏奖励的多目标任务，无需复杂的奖励工程。通过引入虚拟数据和好奇心驱动的内在奖励，此方法在OpenAI Gym Fetch Robotics任务上平均提高了一个数量级的数据效率。 |
| [^150] | [SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models.](http://arxiv.org/abs/2108.13672) | 使用无注意力的序列模型SANSformer在电子健康记录中进行自我监督预测，充分挖掘了Transformer在EHR应用中的优势。主要应用于预测未来的医疗资源利用，特别适用于处理不同患者子组，如罕见疾病患者。 |
| [^151] | [Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity.](http://arxiv.org/abs/2007.07461) | 本文研究了基于模型的多智能体强化学习在零和马尔可夫博弈中的样本复杂度问题，并证明了其在找到纳什均衡值及具有平滑规划预言机的ε-NE策略方面具有接近最优的样本复杂度。 |
| [^152] | [Sparse and Low-Rank High-Order Tensor Regression via Parallel Proximal Method.](http://arxiv.org/abs/1911.12965) | 本论文提出了一种通过并行近端方法实现稀疏和低秩高阶张量回归的模型，该模型通过直接应用$\ell_1$范数和张量核范数来保留张量的结构信息，并且在处理大规模数据和高阶结构时具有可扩展性和高效性。 |

# 详细

[^1]: 利用零射波自监督学习重建改进的多次扫描扩散加权MRI

    Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction. (arXiv:2308.05103v1 [eess.IV])

    [http://arxiv.org/abs/2308.05103](http://arxiv.org/abs/2308.05103)

    本研究提出了一种零射波自监督学习的多次扫描图像重建方法，用于改进扩散MRI，通过深度学习和虚拟线圈的应用，能够克服多次扫描中的相位变化问题，并提高图像重建质量。

    

    扩散MRI通常使用回波平面成像（EPI）进行，因为其采集时间快。然而，扩散加权图像的分辨率常常受到磁场不均匀性相关伪影以及T2和T2*弛豫效应引起的模糊影响。为了解决这些局限，常常采用多次扫描EPI（msEPI）与并行成像技术相结合。然而，由于多个扫描之间的相位变化，重建msEPI可能具有挑战性。在本研究中，我们引入了一种新颖的msEPI重建方法，称为zero-MIRID（零射波自监督学习多次扫描图像重建改进扩散MRI）。该方法通过结合基于深度学习的图像正则化技术来联合重建msEPI数据。该网络在k空间和图像空间中都使用CNN去噪器，并利用虚拟线圈来增强图像重建条件。通过采用自监督学习技术和分割

    Diffusion MRI is commonly performed using echo-planar imaging (EPI) due to its rapid acquisition time. However, the resolution of diffusion-weighted images is often limited by magnetic field inhomogeneity-related artifacts and blurring induced by T2- and T2*-relaxation effects. To address these limitations, multi-shot EPI (msEPI) combined with parallel imaging techniques is frequently employed. Nevertheless, reconstructing msEPI can be challenging due to phase variation between multiple shots. In this study, we introduce a novel msEPI reconstruction approach called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). This method jointly reconstructs msEPI data by incorporating deep learning-based image regularization techniques. The network incorporates CNN denoisers in both k- and image-spaces, while leveraging virtual coils to enhance image reconstruction conditioning. By employing a self-supervised learning technique and divi
    
[^2]: DOST - 无噪声标签的多标签分类的领域遵从自监督训练

    DOST -- Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels. (arXiv:2308.05101v1 [cs.LG])

    [http://arxiv.org/abs/2308.05101](http://arxiv.org/abs/2308.05101)

    本文提出了一种名为"DOST"的方法，通过领域规则的纳入，利用领域遵从自监督训练的范式，解决了多标签分类任务中标签噪声的问题，并改善了学习性能和关键指标，减小了注释噪声的影响。

    

    深度学习技术对标注数据的巨大需求引发了注释噪声的问题。尽管这个问题在机器学习文献中广泛讨论，但在"多标签分类"（MLC）任务的复杂噪声情况下，却相对未经探索。此外，当所涉及的领域具有某些逻辑约束时，噪声标注常常加剧违规情况，使得该系统被专家认为是不可接受的。本文研究了标签噪声对MLC任务中领域规则违规事件的影响，并将领域规则纳入我们的学习算法，以减轻噪声的影响。我们提出了"Domain Obedient Self-supervised Training"（DOST）范式，不仅可以使深度学习模型更符合领域规则，还可以在关键指标上提高学习性能，并最小化注释噪声的影响。这种新颖的方法使用领域指导训练技术。

    The enormous demand for annotated data brought forth by deep learning techniques has been accompanied by the problem of annotation noise. Although this issue has been widely discussed in machine learning literature, it has been relatively unexplored in the context of "multi-label classification" (MLC) tasks which feature more complicated kinds of noise. Additionally, when the domain in question has certain logical constraints, noisy annotations often exacerbate their violations, making such a system unacceptable to an expert. This paper studies the effect of label noise on domain rule violation incidents in the MLC task, and incorporates domain rules into our learning algorithm to mitigate the effect of noise. We propose the Domain Obedient Self-supervised Training (DOST) paradigm which not only makes deep learning models more aligned to domain rules, but also improves learning performance in key metrics and minimizes the effect of annotation noise. This novel approach uses domain guid
    
[^3]: 在更先进的集群条件下，可能实现亚人类尺度下的图像识别程度

    A degree of image identification at sub-human scales could be possible with more advanced clusters. (arXiv:2308.05092v1 [cs.CV])

    [http://arxiv.org/abs/2308.05092](http://arxiv.org/abs/2308.05092)

    该研究旨在通过扩展数据量和图像质量，利用自我监督学习方法在更先进的集群条件下实现了亚人类尺度下的人类级别物体检测性能。

    

    研究的目的是确定当前可用的自我监督学习技术是否能够使用与人类获取感官输入相同程度和数量的数据来实现人类水平的对视觉图像的理解。一开始，该领域的研究仅考虑了数据量的扩大。在这里，我们同时扩展了数据量和图像质量。这个扩展实验是一种无需任何外部融资的自我监督学习方法。我们发现，在同时扩展数据量和图片分辨率的情况下，可以实现在亚人类尺度下的人类级别物体检测性能。我们对以最高256像素每英寸分辨率训练的Vision Transformers进行了可扩展实验，使用了多达200,000张图像。

    The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi.
    
[^4]: 具有贝叶斯逆转换学习的离线设置

    Bayesian Inverse Transition Learning for Offline Settings. (arXiv:2308.05075v1 [cs.LG])

    [http://arxiv.org/abs/2308.05075](http://arxiv.org/abs/2308.05075)

    该论文提出了一种用于离线设置的贝叶斯逆转换学习方法，通过结合专家数据，可可靠地学习过渡动态T的后验分布，并且在不同数据集上显著减少政策的方差。

    

    离线强化学习通常用于在诸如医疗保健和教育等领域进行顺序决策，其中奖励是已知的，并且必须基于批量数据估计过渡动态T。对于所有任务而言，一个关键挑战是如何学习一个可靠的过渡动态T估计，这些估计能够产生接近最优策略，并且足够安全，以至于它们从未采取远离最佳动作的行动，并且足够信息丰富，以传达其所具有的不确定性。使用来自专家的数据，我们提出了一种新的基于约束的方法，用于捕捉我们可靠地学习过渡动态T后验分布的需要，而这些分布又不涉及梯度。我们的结果表明，通过使用我们的约束条件，我们可以学习一个高性能政策，同时在不同数据集上显著减少政策的方差。我们还解释了如何将不确定性估计与…

    Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data. A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have. Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients. Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets. We also explain how combining uncertainty estimation with th
    
[^5]: 使用传感器数据、方程和自然语言提示上下文中的运算符学习

    Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])

    [http://arxiv.org/abs/2308.05061](http://arxiv.org/abs/2308.05061)

    本文提出了一种使用传感器数据、方程和自然语言提示上下文中运算符学习的方法。通过整合人类知识和语言描述，该方法不仅扩展了物理信息学习的灵活性和普适性，而且显著提高了学习性能和减少了数据需求。

    

    在科学机器学习领域中，上下文中的运算符学习已经展示出了在推理阶段从提示数据中学习运算符的显著潜力，而无需进行权重更新。然而，当前模型对传感器数据的过度依赖可能会无意中忽视运算符的宝贵的人类洞察力。为了解决这个问题，我们将上下文中的运算符学习转化为一种多模式范式。我们提出使用“标题”来整合通过自然语言描述和方程式表达的运算符的人类知识。我们演示了这种方法不仅扩展了物理信息学习的灵活性和普遍性，而且还显著提高了学习性能并减少了数据需求。此外，我们引入了一种更高效的多模式上下文运算符学习的神经网络架构，称为“ICON-LM”，基于类似于语言模型的架构。

    In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
    
[^6]: 通过恢复传统的反向传播技术来提高神经网络的准确性的一种新方法

    A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique. (arXiv:2308.05059v1 [cs.LG])

    [http://arxiv.org/abs/2308.05059](http://arxiv.org/abs/2308.05059)

    本文提出了一种新的即时参数更新方法来解决深度神经网络训练中的梯度消失问题，加速学习过程并在基准数据集上取得了优于最先进方法的成果。

    

    深度学习已经在计算机视觉、自然语言处理和语音识别等领域彻底改变了工业。然而，用于训练深度神经网络的主要方法——反向传播，面临着计算开销大和梯度消失等挑战。在本文中，我们提出了一种新的即时参数更新方法，它消除了在每个层计算梯度的需要。我们的方法加速了学习过程，避免了梯度消失问题，并在基准数据集上优于最先进的方法。这项研究为高效有效的深度神经网络训练提供了有希望的方向。

    Deep learning has revolutionized industries like computer vision, natural language processing, and speech recognition. However, back propagation, the main method for training deep neural networks, faces challenges like computational overhead and vanishing gradients. In this paper, we propose a novel instant parameter update methodology that eliminates the need for computing gradients at each layer. Our approach accelerates learning, avoids the vanishing gradient problem, and outperforms state-of-the-art methods on benchmark data sets. This research presents a promising direction for efficient and effective deep neural network training.
    
[^7]: RadGraph2：通过分层信息提取建模放射学报告中的疾病进展

    RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction. (arXiv:2308.05046v1 [cs.CL])

    [http://arxiv.org/abs/2308.05046](http://arxiv.org/abs/2308.05046)

    RadGraph2是一个新的数据集，用于从放射学报告中提取信息，并以分层信息提取模型HGIE为基础，能够更好地捕捉疾病进展和关系提取任务中的发现。

    

    我们提出了RadGraph2，这是一个新的数据集，用于从放射学报告中提取信息，重点是捕捉疾病状态和设备放置随时间的变化。我们引入了一个基于关系组织实体的分层模式，并展示了在训练过程中使用这种层次结构可以改善信息提取模型的性能。具体地，我们对DyGIE++框架进行了修改，得到了我们的模型HGIE，该模型在实体和关系提取任务中优于先前的模型。我们证明了RadGraph2使模型能够捕捉更广泛的发现，并在关系提取方面表现优于那些在原始RadGraph数据集上训练的模型。我们的工作奠定了在医学领域开发可以跟踪疾病进展并利用标签的自然分层的信息提取模型的自动化系统的基础。

    We present RadGraph2, a novel dataset for extracting information from radiology reports that focuses on capturing changes in disease state and device placement over time. We introduce a hierarchical schema that organizes entities based on their relationships and show that using this hierarchy during training improves the performance of an information extraction model. Specifically, we propose a modification to the DyGIE++ framework, resulting in our model HGIE, which outperforms previous models in entity and relation extraction tasks. We demonstrate that RadGraph2 enables models to capture a wider variety of findings and perform better at relation extraction compared to those trained on the original RadGraph dataset. Our work provides the foundation for developing automated systems that can track disease progression over time and develop information extraction models that leverage the natural hierarchy of labels in the medical domain.
    
[^8]: 协作式网络化无人机在UTM系统中的宽带频谱感知和调度

    Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems. (arXiv:2308.05036v1 [eess.SP])

    [http://arxiv.org/abs/2308.05036](http://arxiv.org/abs/2308.05036)

    本文提出了一种协作式网络化无人机在UTM系统中进行宽带频谱感知和调度的数据驱动框架，通过融合多个无人机的分类输出来增强频谱感知模块的准确性，并利用强化学习方法将检测到的频谱空洞分配给次要用户进行动态调度。

    

    本文提出了一种基于数据驱动的框架，用于协作式网络化无人机在UTM系统中的宽带频谱感知和调度，这些无人机作为次要用户可以机会性地利用检测到的频谱空洞。为此，我们提出了一个多类别分类问题，用于基于采集到的I/Q样本检测空闲频谱位置。为了提高频谱感知模块的准确性，在无人机系统交通管理（UTM）生态系统中，通过将每个单个无人机的多类别分类的输出在服务器上进行融合。在频谱调度阶段，我们利用强化学习（RL）解决方案，动态地将检测到的频谱空洞分配给次要用户（即无人机）。为了评估所提出的方法，我们建立了一个综合的仿真框架，使用MATLAB LTE工具箱生成近真实的合成数据集，并将基站（BS）位置纳入到仿真中。

    In this paper, we propose a data-driven framework for collaborative wideband spectrum sensing and scheduling for networked unmanned aerial vehicles (UAVs), which act as the secondary users to opportunistically utilize detected spectrum holes. To this end, we propose a multi-class classification problem for wideband spectrum sensing to detect vacant spectrum spots based on collected I/Q samples. To enhance the accuracy of the spectrum sensing module, the outputs from the multi-class classification by each individual UAV are fused at a server in the unmanned aircraft system traffic management (UTM) ecosystem. In the spectrum scheduling phase, we leverage reinforcement learning (RL) solutions to dynamically allocate the detected spectrum holes to the secondary users (i.e., UAVs). To evaluate the proposed methods, we establish a comprehensive simulation framework that generates a near-realistic synthetic dataset using MATLAB LTE toolbox by incorporating base-station~(BS) locations in a cho
    
[^9]: Kairos: 使用整体系统溯源进行实用的入侵检测和调查

    Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance. (arXiv:2308.05034v1 [cs.CR])

    [http://arxiv.org/abs/2308.05034](http://arxiv.org/abs/2308.05034)

    Kairos是第一个同时满足范围、攻击不可知性、时效性和攻击重建维度要求的溯源为基础的入侵检测和调查系统。

    

    溯源图是描述系统执行历史的结构化审计日志。最近的研究探索了各种技术来分析溯源图，以实现自动化主机入侵检测，特别关注高级持久性威胁。通过研究其设计文档，我们确定了四个常见维度，推动溯源为基础的入侵检测系统（PIDS）的发展：范围（PIDS能否检测跨应用边界渗透的现代攻击？）、攻击不可知性（PIDS能否在没有攻击特征先验知识的情况下检测新型攻击？）、时效性（PIDS能否高效监视主机系统运行？）和攻击重建（PIDS能否从大型溯源图中提炼攻击活动，以便系统管理员能够轻松理解并迅速应对系统入侵？）。我们提出了KAIROS，这是第一个同时满足所有四个维度要求的PIDS，而现有的方法不能做到。

    Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approac
    
[^10]: 密度作物引导的半监督航空图像目标检测

    Density Crop-guided Semi-supervised Object Detection in Aerial Images. (arXiv:2308.05032v1 [cs.CV])

    [http://arxiv.org/abs/2308.05032](http://arxiv.org/abs/2308.05032)

    密度作物引导的半监督检测器通过识别小物体的聚类来提高在航拍图像中的性能。

    

    训练现代目标检测器的一个重要瓶颈是需要标记的图像，其中需要为图像中的每个对象产生边界框注释。在航拍图像中，这个瓶颈更加严重，标注员必须对高分辨率图像上分布的小物体进行标注。最近，使用伪标签和弱强增强一致性训练的平均教师方法在半监督目标检测中越来越受欢迎。然而，直接将这种半监督检测器应用于航空图像，其中经常存在小的聚类对象，可能不会得到最佳结果。在本文中，我们提出了一种密度作物引导的半监督检测器，它在训练过程中识别出小物体的聚类，并利用它们来提高推理性能。

    One of the important bottlenecks in training modern object detectors is the need for labeled images where bounding box annotations have to be produced for each object present in the image. This bottleneck is further exacerbated in aerial images where the annotators have to label small objects often distributed in clusters on high-resolution images. In recent days, the mean-teacher approach trained with pseudo-labels and weak-strong augmentation consistency is gaining popularity for semi-supervised object detection. However, a direct adaptation of such semi-supervised detectors for aerial images where small clustered objects are often present, might not lead to optimal results. In this paper, we propose a density crop-guided semi-supervised detector that identifies the cluster of small objects during training and also exploits them to improve performance at inference. During training, image crops of clusters identified from labeled and unlabeled images are used to augment the training s
    
[^11]: AbDiffuser：体外功能抗体的全原子生成

    AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies. (arXiv:2308.05027v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.05027](http://arxiv.org/abs/2308.05027)

    AbDiffuser是一个物理性扩散模型，用于联合生成抗体的三维结构和序列。该方法利用领域知识和基于物理的约束改善蛋白质扩散，处理序列长度变化，并能够生成与参考集合的序列和结构特性密切匹配的抗体。实验结果表明，AbDiffuser能够生成高水平表达的抗体，其中57.1%的设计选择是紧密结合剂。

    

    我们介绍了一个名为AbDiffuser的等变物理性扩散模型，用于联合生成抗体的三维结构和序列。AbDiffuser建立在一种新的蛋白质结构表示上，依赖于一种针对齐位蛋白的新型架构，并利用强扩散先验改善去噪过程。我们的方法通过利用领域知识和基于物理的约束改善了蛋白质扩散；处理序列长度变化；并将内存复杂性降低一个数量级，实现了骨架和侧链的生成。我们在体内和体外验证了AbDiffuser。数值实验展示了AbDiffuser生成与参考集合的序列和结构特性密切匹配的抗体的能力。实验室实验证实，发现的16种HER2抗体均以高水平表达，并且57.1%的设计选择是紧密结合剂。

    We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of selected designs were tight binders.
    
[^12]: 扩散模型是否会受到错误传播的影响？理论分析和一致性正则化。

    Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization. (arXiv:2308.05021v1 [cs.LG])

    [http://arxiv.org/abs/2308.05021](http://arxiv.org/abs/2308.05021)

    扩散模型可能受到错误传播的影响，本文通过理论分析和数据实证验证了这个问题，并提出了一种一致性正则化方法来解决。我们的分析揭示了模型的去噪模块是否具有容错性以及如何减少分布差异。

    

    虽然扩散模型在数据合成方面取得了令人期待的成果，但由于其级联结构，即去噪模块链式传播和放大了分布不匹配的错误，因此可能会受到错误传播的影响。然而，我们期望进行严格的分析，因为许多顺序模型，如条件随机场（CRF），是不会出现错误传播的。在本文中，我们通过实证和理论验证了扩散模型确实受到错误传播的影响，并提出一种正则化方法来解决这个问题。我们的理论分析揭示了这个问题是否可以归结为扩散模型的每个去噪模块是否具有容错性。我们推导出了有深刻见解的转移方程，表明模块无法从输入错误中恢复，甚至会将额外的错误传播到下一个模块。我们的分析直接导致了扩散模型的一致性正则化方案，可以明确减少分布的差异。

    While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem. Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution 
    
[^13]: 何时和如何通过谱分析利用已知类别帮助发现未知类别？通过可证明的谱分析理解。

    When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis. (arXiv:2308.05017v1 [cs.LG])

    [http://arxiv.org/abs/2308.05017](http://arxiv.org/abs/2308.05017)

    本文提出了一个分析框架来研究何时和如何利用已知类别帮助发现新颖类别，并通过引入NCD谱对比损失（NSCL）提供了可证明的误差界限和NCD的充分必要条件。实验证明NSCL可以在常见的基准数据集上与或优于几个强基准方法，具有实际使用价值且理论保证。

    

    新颖类别发现（NCD）旨在通过利用具有已知类别的标记集合的先验知识，在未标记的集合中推断出新颖类别。尽管其重要性，但NCD缺乏理论基础。本文通过提供一个分析框架来弥补这个差距，以形式化和研究何时和如何已知类别能够帮助发现新颖类别。针对NCD问题，我们引入了一种图论表示，可以通过一种新颖的NCD谱对比损失（NSCL）进行学习。最小化这个目标等同于分解图的邻接矩阵，这使我们能够推导出可证明的误差界限，并提供NCD的充分且必要条件。在实证上，NSCL可以在常见的基准数据集上与或优于几个强基准方法，这对实际使用是有吸引力的，同时享有理论保证。

    Novel Class Discovery (NCD) aims at inferring novel classes in an unlabeled set by leveraging prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for NCD. This paper bridges the gap by providing an analytical framework to formalize and investigate when and how known classes can help discover novel classes. Tailored to the NCD problem, we introduce a graph-theoretic representation that can be learned by a novel NCD Spectral Contrastive Loss (NSCL). Minimizing this objective is equivalent to factorizing the graph's adjacency matrix, which allows us to derive a provable error bound and provide the sufficient and necessary condition for NCD. Empirically, NSCL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees.
    
[^14]: 开源联邦学习框架中错误的实证研究

    An Empirical Study of Bugs in Open-Source Federated Learning Framework. (arXiv:2308.05014v1 [cs.SE])

    [http://arxiv.org/abs/2308.05014](http://arxiv.org/abs/2308.05014)

    本文通过实证研究发现了开源联邦学习框架中存在的错误，并对这些错误的特征进行了详细分析，为提高框架的安全性和稳定性提供了指导。

    

    联邦学习作为一种分散式的机器学习解决方案，旨在保护用户的私密数据，在近年来已成为重要的学习模式，尤其是在大多数国家实施更严格的法律法规之后。因此，发布了各种各样的联邦学习框架，以促进联邦学习的开发和应用。尽管在FL模型和系统的安全性和隐私性方面进行了大量研究，但FL框架的安全问题尚未得到系统地研究。本文首次对1,112个FL框架错误进行实证研究，以了解其特征。这些错误是通过手动从GitHub上收集、分类和标记的12个开源FL框架得来的。具体来说，我们构建了这些错误的15个症状、12个根本原因和20个修复模式的分类，并研究了它们在23个逻辑组件和两个主要应用场景上的相关性和分布。

    Federated learning (FL), as a decentralized machine learning solution to the protection of users' private data, has become an important learning paradigm in recent years, especially since the enforcement of stricter laws and regulations in most countries. Therefore, a variety of FL frameworks are released to facilitate the development and application of federated learning. Despite the considerable amount of research on the security and privacy of FL models and systems, the security issues in FL frameworks have not been systematically studied yet. In this paper, we conduct the first empirical study on 1,112 FL framework bugs to investigate their characteristics. These bugs are manually collected, classified, and labeled from 12 open-source FL frameworks on GitHub. In detail, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fix patterns of these bugs and investigate their correlations and distributions on 23 logical components and two main application scenarios. From the re
    
[^15]: 多类深度支持向量数据描述：一种用于天文学中具有不同内部类别的异常检测方法

    Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories. (arXiv:2308.05011v1 [cs.LG])

    [http://arxiv.org/abs/2308.05011](http://arxiv.org/abs/2308.05011)

    提出了一种专门用于处理具有不同内部类别的天文学异常检测方法，采用多类深度支持向量数据描述(MCDSVDD)方法，通过使用神经网络将数据映射到超球体中，每个超球体代表一个特定的内部类别，并计算样本与超球体中心的距离以确定异常分数。

    

    随着现代勘测望远镜产生的天文数据量不断增加，自动化流程和机器学习技术已成为对这些数据进行分析和提取知识的关键。异常检测，在数据中识别不规则或意外模式的任务，是天文学中的一个复杂挑战。本文中，我们提出了多类深度支持向量数据描述(MCDSVDD)方法，它是一种对现有异常检测算法One-Class Deep SVDD进行扩展的方法，专门用于处理具有不同数据分布的不同内部类别。MCDSVDD使用神经网络将数据映射到超球体中，其中每个超球体代表一个特定的内部类别。样本距离这些超球体中心的距离决定了异常分数。我们通过将MCDSVDD的性能与多个异常检测算法在大规模天文光变曲线数据集上进行比较，评估了MCDSVDD的有效性。

    With the increasing volume of astronomical data generated by modern survey telescopes, automated pipelines and machine learning techniques have become crucial for analyzing and extracting knowledge from these datasets. Anomaly detection, i.e. the task of identifying irregular or unexpected patterns in the data, is a complex challenge in astronomy. In this paper, we propose Multi-Class Deep Support Vector Data Description (MCDSVDD), an extension of the state-of-the-art anomaly detection algorithm One-Class Deep SVDD, specifically designed to handle different inlier categories with distinct data distributions. MCDSVDD uses a neural network to map the data into hyperspheres, where each hypersphere represents a specific inlier category. The distance of each sample from the centers of these hyperspheres determines the anomaly score. We evaluate the effectiveness of MCDSVDD by comparing its performance with several anomaly detection algorithms on a large dataset of astronomical light-curves 
    
[^16]: 具有人类语言监督的可转移生物声学模型

    Transferable Models for Bioacoustics with Human Language Supervision. (arXiv:2308.04978v1 [cs.LG])

    [http://arxiv.org/abs/2308.04978](http://arxiv.org/abs/2308.04978)

    本论文提出了一种基于对比语音-语言预训练的生物声学模型BioLingual，通过训练模型连接语言和音频表示，实现了跨物种识别和零样本学习，并在动物声音基准测试中取得了九项任务的最新最先进结果。

    

    无源声学监测提供了一种可扩展的、非侵入性的方法来追踪全球生物多样性和人为活动对物种的影响。虽然深度学习已成为处理这些数据的重要工具，但当前的模型缺乏灵活性，通常只涵盖少数物种，并受到数据稀缺性的限制。在这项工作中，我们提出了一种基于对比语音-语言预训练的生物声学模型BioLingual。我们首先将生物声学档案聚合成一个名为AnimalSpeak的语音-语言数据集，其中含有超过一百万个音频-标题对，包含物种、发声上下文和动物行为的信息。在对该数据集进行语言和音频表示的连接训练后，我们的模型可以跨物种识别超过一千种物种的叫声，实现零样本学习完成生物声学任务，并能从自然文本查询中检索动物的发声录音。在经过微调后，BioLingual在动物声音基准测试中取得了九项任务的新的最先进结果。

    Passive acoustic monitoring offers a scalable, non-invasive method for tracking global biodiversity and anthropogenic impacts on species. Although deep learning has become a vital tool for processing this data, current models are inflexible, typically cover only a handful of species, and are limited by data scarcity. In this work, we propose BioLingual, a new model for bioacoustics based on contrastive language-audio pretraining. We first aggregate bioacoustic archives into a language-audio dataset, called AnimalSpeak, with over a million audio-caption pairs holding information on species, vocalization context, and animal behavior. After training on this dataset to connect language and audio representations, our model can identify over a thousand species' calls across taxa, complete bioacoustic tasks zero-shot, and retrieve animal vocalization recordings from natural text queries. When fine-tuned, BioLingual sets a new state-of-the-art on nine tasks in the Benchmark of Animal Sounds. G
    
[^17]: Adversarial ModSecurity: 使用强大的机器学习对抗SQL注入攻击

    Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning. (arXiv:2308.04964v1 [cs.LG])

    [http://arxiv.org/abs/2308.04964](http://arxiv.org/abs/2308.04964)

    这篇论文介绍了Adversarial ModSecurity，它是一个使用强大的机器学习来对抗SQL注入攻击的防火墙。通过将核心规则集作为输入特征，该模型可以识别并防御对抗性SQL注入攻击。实验结果表明，AdvModSec在训练后能够有效地应对这类攻击。

    

    ModSecurity被广泛认可为标准的开源Web应用防火墙(WAF)，由OWASP基金会维护。它通过与核心规则集进行匹配来检测恶意请求，识别出常见的攻击模式。每个规则在CRS中都被手动分配一个权重，基于相应攻击的严重程度，如果触发规则的权重之和超过给定的阈值，就会被检测为恶意请求。然而，我们的研究表明，这种简单的策略在检测SQL注入攻击方面很不有效，因为它往往会阻止许多合法请求，同时还容易受到对抗性SQL注入攻击的影响，即故意操纵以逃避检测的攻击。为了克服这些问题，我们设计了一个名为AdvModSec的强大机器学习模型，它将CRS规则作为输入特征，并经过训练以检测对抗性SQL注入攻击。我们的实验表明，AdvModSec在针对该攻击的流量上进行训练后表现出色。

    ModSecurity is widely recognized as the standard open-source Web Application Firewall (WAF), maintained by the OWASP Foundation. It detects malicious requests by matching them against the Core Rule Set, identifying well-known attack patterns. Each rule in the CRS is manually assigned a weight, based on the severity of the corresponding attack, and a request is detected as malicious if the sum of the weights of the firing rules exceeds a given threshold. In this work, we show that this simple strategy is largely ineffective for detecting SQL injection (SQLi) attacks, as it tends to block many legitimate requests, while also being vulnerable to adversarial SQLi attacks, i.e., attacks intentionally manipulated to evade detection. To overcome these issues, we design a robust machine learning model, named AdvModSec, which uses the CRS rules as input features, and it is trained to detect adversarial SQLi attacks. Our experiments show that AdvModSec, being trained on the traffic directed towa
    
[^18]: CasCIFF: 一种专为社交网络中级联预测量身定制的跨领域信息融合框架。

    CasCIFF: A Cross-Domain Information Fusion Framework Tailored for Cascade Prediction in Social Networks. (arXiv:2308.04961v1 [cs.SI])

    [http://arxiv.org/abs/2308.04961](http://arxiv.org/abs/2308.04961)

    CasCIFF是一种专为社交网络中级联预测量身定制的跨领域信息融合框架，旨在解决深度学习方法中存在的用户属性表示不准确、忽视激活时间和无法整合时间和结构因素等问题。

    

    现有的信息级联预测方法可以分为三类：特征驱动方法、点过程方法和深度学习方法。其中，深度学习方法以其卓越的学习和表示能力来缓解其他方法的固有缺点。然而，当前的深度学习方法仍面临一些持久性的挑战。特别是，由于存在伪粉丝和复杂的网络配置等因素，准确表示用户属性仍然有问题。之前专注于用户激活顺序的算法往往忽视了激活时间提供的丰富见解。此外，这些技术通常无法全面地整合时间和结构方面的因素，从而错过了信息级联中固有的微妙传播趋势。为了解决这些问题，我们提出了跨领域信息融合框架（CasCIFF）。

    Existing approaches for information cascade prediction fall into three main categories: feature-driven methods, point process-based methods, and deep learning-based methods. Among them, deep learning-based methods, characterized by its superior learning and representation capabilities, mitigates the shortcomings inherent of the other methods. However, current deep learning methods still face several persistent challenges. In particular, accurate representation of user attributes remains problematic due to factors such as fake followers and complex network configurations. Previous algorithms that focus on the sequential order of user activations often neglect the rich insights offered by activation timing. Furthermore, these techniques often fail to holistically integrate temporal and structural aspects, thus missing the nuanced propagation trends inherent in information cascades.To address these issues, we propose the Cross-Domain Information Fusion Framework (CasCIFF), which is tailor
    
[^19]: 使用源分离和鲁棒对抗学习的音频隐私保护的表征学习

    Representation Learning for Audio Privacy Preservation using Source Separation and Robust Adversarial Learning. (arXiv:2308.04960v1 [cs.SD])

    [http://arxiv.org/abs/2308.04960](http://arxiv.org/abs/2308.04960)

    本文提出了一种使用源分离和对抗学习的音频隐私保护的表征学习方法，通过学习音频记录的潜在表征，提高语音隐私保护的效果。

    

    长期以来，隐私保护一直是智能声学监测系统中的一个关注点，其中语音可以被被动地记录下来，同时也记录下了系统操作环境中的目标信号。在本研究中，我们提出了在隐私保护中常用的两种方法的整合：源分离和对抗性表征学习。所提出的系统学习音频记录的潜在表征，以防止区分语音和非语音记录。最初，源分离网络过滤掉了一些隐私敏感数据，而在对抗学习过程中，系统会在过滤后的信号上学习隐私保护表征。我们通过将我们的方法与没有源分离、没有对抗学习以及两者都没有的系统进行比较，证明了我们提出方法的有效性。总体而言，我们的结果表明，与其他系统相比，所提出的系统可以显著提高语音隐私保护。

    Privacy preservation has long been a concern in smart acoustic monitoring systems, where speech can be passively recorded along with a target signal in the system's operating environment. In this study, we propose the integration of two commonly used approaches in privacy preservation: source separation and adversarial representation learning. The proposed system learns the latent representation of audio recordings such that it prevents differentiating between speech and non-speech recordings. Initially, the source separation network filters out some of the privacy-sensitive data, and during the adversarial learning process, the system will learn privacy-preserving representation on the filtered signal. We demonstrate the effectiveness of our proposed method by comparing our method against systems without source separation, without adversarial learning, and without both. Overall, our results suggest that the proposed system can significantly improve speech privacy preservation compared
    
[^20]: 通过具有注意力网络的分布式强化学习改进自主分离保障

    Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks. (arXiv:2308.04958v1 [cs.AI])

    [http://arxiv.org/abs/2308.04958](http://arxiv.org/abs/2308.04958)

    本文提出了一个分布式强化学习框架，利用速度和垂直机动手段，在高密度环境中实现了自主的自我分离能力，从而改进了自主分离保障技术。

    

    先进的空中移动（AAM）介绍了一种新的、高效的交通方式，利用车辆自主性和电动飞机，在之前未得到充分服务的市场之间提供越来越自主的交通。通过高密度环境中低空飞行器的安全和高效导航，需要整合大量复杂观测数据，如监视，车辆动力学知识和天气。在处理和推理这些观测数据时，面临着信息不确定性的多个来源的挑战，同时要确保与空域中可变数量的飞机合作。这些挑战加上需要实时做出安全关键决策的要求，使得传统的分离保障技术无法使用。我们提出了一个分布式强化学习框架，在AAM走廊内使用速度和垂直机动手段提供自主的自我分离能力。

    Advanced Air Mobility (AAM) introduces a new, efficient mode of transportation with the use of vehicle autonomy and electrified aircraft to provide increasingly autonomous transportation between previously underserved markets. Safe and efficient navigation of low altitude aircraft through highly dense environments requires the integration of a multitude of complex observations, such as surveillance, knowledge of vehicle dynamics, and weather. The processing and reasoning on these observations pose challenges due to the various sources of uncertainty in the information while ensuring cooperation with a variable number of aircraft in the airspace. These challenges coupled with the requirement to make safety-critical decisions in real-time rule out the use of conventional separation assurance techniques. We present a decentralized reinforcement learning framework to provide autonomous self-separation capabilities within AAM corridors with the use of speed and vertical maneuvers. The probl
    
[^21]: 基于Transformer模型（BERT，ALBERT和RoBERTa）在假新闻检测中的性能分析

    Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection. (arXiv:2308.04950v1 [cs.CL])

    [http://arxiv.org/abs/2308.04950](http://arxiv.org/abs/2308.04950)

    该论文分析了基于Transformer模型的BERT、ALBERT和RoBERTa在假新闻检测中的性能。研究发现，Transformer模型（特别是BERT）在处理文本上表现优异，但不同研究对性能评估和结论的实现方法存在差异。

    

    假新闻是一种媒体格式的虚假材料，但没有经过新闻机构的适当处理。这些虚假材料可能会激起或诽谤重要实体或个人，甚至可能是创作者的个人利益，给社会带来问题。由于领域知识有限和时间限制，区分假新闻和真新闻是具有挑战性的。根据调查，受到谣言和信息误导的三个地区最多的是万丹特区、雅加达特区和西爪哇。Transformer模型是指在自然语言处理中利用深度学习架构的一种人工智能（AI）方法。Transformer模型通过强大的注意机制并行处理文本，并生成丰富和上下文相关的词表示。先前的研究表明，一种名为BERT的Transformer模型在性能上优于非Transformer方法。然而，一些研究表明性能评估和结bonclusion的实现方法之间可能存在一定的差异。

    Fake news is fake material in a news media format but is not processed properly by news agencies. The fake material can provoke or defame significant entities or individuals or potentially even for the personal interests of the creators, causing problems for society. Distinguishing fake news and real news is challenging due to limited of domain knowledge and time constraints. According to the survey, the top three areas most exposed to hoaxes and misinformation by residents are in Banten, DKI Jakarta and West Java. The model of transformers is referring to an approach in the field of artificial intelligence (AI) in natural language processing utilizing the deep learning architectures. Transformers exercise a powerful attention mechanism to process text in parallel and produce rich and contextual word representations. A previous study indicates a superior performance of a transformer model known as BERT over and above non transformer approach. However, some studies suggest the performan
    
[^22]: 获得和整合知识用于股票价格预测的方法：一项调查

    Methods for Acquiring and Incorporating Knowledge into Stock Price Prediction: A Survey. (arXiv:2308.04947v1 [q-fin.ST])

    [http://arxiv.org/abs/2308.04947](http://arxiv.org/abs/2308.04947)

    这项调查论文系统而全面地描述了从不同来源获得外部知识的方法，以应用于股票价格预测，帮助理解股票市场的复杂性。

    

    预测股票价格是一个具有挑战性的研究问题，因为股票市场的固有波动性和非线性性质。近年来，利用外部知识来理解股票市场的知识增强型股票价格预测方法表现出突破性的成果。尽管这些方法的重要性，但在从外部知识类型的角度系统地综合以往研究方面，学术作品的稀缺性存在一定问题。具体而言，外部知识可以以不同的数据结构建模，我们将其分为非图形化格式和图形化格式两类：1) 非图形化知识捕获与个别股票密切相关的上下文信息和多媒体描述；2) 图形化知识捕获股票市场中相互关联和相互依赖的信息。本调查论文旨在系统而全面地描述从不同来源获取外部知识的方法。

    Predicting stock prices presents a challenging research problem due to the inherent volatility and non-linear nature of the stock market. In recent years, knowledge-enhanced stock price prediction methods have shown groundbreaking results by utilizing external knowledge to understand the stock market. Despite the importance of these methods, there is a scarcity of scholarly works that systematically synthesize previous studies from the perspective of external knowledge types. Specifically, the external knowledge can be modeled in different data structures, which we group into non-graph-based formats and graph-based formats: 1) non-graph-based knowledge captures contextual information and multimedia descriptions specifically associated with an individual stock; 2) graph-based knowledge captures interconnected and interdependent information in the stock market. This survey paper aims to provide a systematic and comprehensive description of methods for acquiring external knowledge from va
    
[^23]: 具有重要性分级噪声自适应的差分隐私图神经网络

    Differentially Private Graph Neural Network with Importance-Grained Noise Adaption. (arXiv:2308.04943v1 [cs.LG])

    [http://arxiv.org/abs/2308.04943](http://arxiv.org/abs/2308.04943)

    本文提出了一种差分隐私图神经网络方法，考虑了节点的重要性，并通过自适应差分隐私的方式保护节点信息的隐私。其中，根据拓扑结构和中心性信息推断节点重要性，通过节点重要性级别对邻域聚合进行隐私保护。

    

    提出了一种差分隐私图神经网络方法，该方法考虑了节点的重要性对隐私需求的影响，并提供了保护节点信息的隐私保证。首先，提出了一种基于拓扑的节点重要性估计方法，利用邻域和中心性信息来推断未知节点的重要性。其次，提出了一种自适应的隐私聚合方法，通过节点重要性级别来对邻域聚合进行扰动。最后，提出了一种私有训练图神经网络的方法，以保护节点隐私。

    Graph Neural Networks (GNNs) with differential privacy have been proposed to preserve graph privacy when nodes represent personal and sensitive information. However, the existing methods ignore that nodes with different importance may yield diverse privacy demands, which may lead to over-protect some nodes and decrease model utility. In this paper, we study the problem of importance-grained privacy, where nodes contain personal data that need to be kept private but are critical for training a GNN. We propose NAP-GNN, a node-importance-grained privacy-preserving GNN algorithm with privacy guarantees based on adaptive differential privacy to safeguard node information. First, we propose a Topology-based Node Importance Estimation (TNIE) method to infer unknown node importance with neighborhood and centrality awareness. Second, an adaptive private aggregation method is proposed to perturb neighborhood aggregation from node-importance-grain. Third, we propose to privately train a graph lea
    
[^24]: 使用基于多智能体强化学习的反向传播方法进行通信学习的离散化方法的深入分析

    An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning. (arXiv:2308.04938v1 [cs.LG])

    [http://arxiv.org/abs/2308.04938](http://arxiv.org/abs/2308.04938)

    本文深入分析了使用基于多智能体强化学习的反向传播方法进行通信学习的离散化方法。研究比较了几种先进的离散化方法，并提出了一种新的方法。

    

    当智能体无法观察到完整的环境状态时，通信在多智能体强化学习中至关重要。允许智能体之间学习通信的常见方法是使用可微分的通信通道，以便梯度能够作为反馈流动。然而，当我们想要使用离散消息来减小消息的大小时，这会带来挑战，因为梯度不能通过离散的通信通道传播。以前的研究提出了处理这个问题的方法。然而，这些方法在不同的通信学习架构和环境中进行了测试，使得很难进行比较。在本文中，我们比较了几种最先进的离散化方法以及一种新的方法。我们在使用来自其他智能体的梯度进行通信学习的背景下进行了比较，并在多个环境中进行了测试。另外，我们介绍了COMA-DIAL，这是一种基于通信学习的方法。

    Communication is crucial in multi-agent reinforcement learning when agents are not able to observe the full state of the environment. The most common approach to allow learned communication between agents is the use of a differentiable communication channel that allows gradients to flow between agents as a form of feedback. However, this is challenging when we want to use discrete messages to reduce the message size, since gradients cannot flow through a discrete communication channel. Previous work proposed methods to deal with this problem. However, these methods are tested in different communication learning architectures and environments, making it hard to compare them. In this paper, we compare several state-of-the-art discretization methods as well as a novel approach. We do this comparison in the context of communication learning using gradients from other agents and perform tests on several environments. In addition, we present COMA-DIAL, a communication learning approach based
    
[^25]: JEDI：一种用于视频动作识别的半监督多数据集学生-教师联合专家蒸馏方法

    JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition. (arXiv:2308.04934v1 [cs.CV])

    [http://arxiv.org/abs/2308.04934](http://arxiv.org/abs/2308.04934)

    JEDI是一种利用多个数据集的专家知识进行训练的半监督学习方法，能够解决跨数据集泛化和标签数据稀缺等问题。

    

    我们提出了JEDI，一种多数据集半监督学习方法，能够有效地结合来自不同数据集上训练的多个专家的知识，以训练和提高单个数据集上的学生模型的性能。我们的方法解决了当前机器学习研究中的两个重要问题：跨数据集的泛化和有限标签数据导致的监督训练限制。我们从任意数量的专家开始，他们在各自特定数据集上进行预训练，形成初始的学生模型集合。教师立即通过连接学生倒数第二层的特征表示派生出来。然后，我们在学生-教师半监督学习场景中同时训练所有模型，直至收敛。我们的高效方法表明，学生和教师在训练过程中都提高了泛化能力。我们对该方法进行了验证。

    We propose JEDI, a multi-dataset semi-supervised learning method, which efficiently combines knowledge from multiple experts, learned on different datasets, to train and improve the performance of individual, per dataset, student models. Our approach achieves this by addressing two important problems in current machine learning research: generalization across datasets and limitations of supervised training due to scarcity of labeled data. We start with an arbitrary number of experts, pretrained on their own specific dataset, which form the initial set of student models. The teachers are immediately derived by concatenating the feature representations from the penultimate layers of the students. We then train all models in a student-teacher semi-supervised learning scenario until convergence. In our efficient approach, student-teacher training is carried out jointly and end-to-end, showing that both students and teachers improve their generalization capacity during training. We validate
    
[^26]: 基于深度学习的冠状动脉分数流预测

    Deep Learning-Based Prediction of Fractional Flow Reserve along the Coronary Artery. (arXiv:2308.04923v1 [eess.IV])

    [http://arxiv.org/abs/2308.04923](http://arxiv.org/abs/2308.04923)

    提出了一种基于深度学习的方法，从冠状CT血管造影（CCTA）扫描中预测冠状动脉的分数流预测（FFR），避免了侵入性测量，并且不仅预测了每个冠状动脉的FFR值，还提供了关于狭窄位置和治疗策略的信息。

    

    由冠状动脉内斑块的堆积引起的功能性冠状动脉疾病（CAD）可能导致冠状动脉管腔狭窄，即冠状动脉狭窄，从而显著阻碍血液流向心肌。目前用于确定功能性狭窄存在的参考是侵入性的分数流预测（FFR）测量。为了避免侵入性测量，非侵入性的从冠状CT血管造影（CCTA）预测FFR的方法逐渐出现。为此，特征为快速推理的机器学习方法日趋发展。然而，这些方法仅预测每个冠状动脉的单个FFR值，即它们不提供关于狭窄位置或治疗策略的信息。我们提出了一种基于深度学习的方法，从CCTA扫描中预测沿冠状动脉的FFR。该研究包括110名患者进行的112条冠状动脉侵入性FFR回拉测量的CCTA图像。

    Functionally significant coronary artery disease (CAD) is caused by plaque buildup in the coronary arteries, potentially leading to narrowing of the arterial lumen, i.e. coronary stenosis, that significantly obstructs blood flow to the myocardium. The current reference for establishing the presence of a functionally significant stenosis is invasive fractional flow reserve (FFR) measurement. To avoid invasive measurements, non-invasive prediction of FFR from coronary CT angiography (CCTA) has emerged. For this, machine learning approaches, characterized by fast inference, are increasingly developed. However, these methods predict a single FFR value per artery i.e. they don't provide information about the stenosis location or treatment strategy. We propose a deep learning-based method to predict the FFR along the artery from CCTA scans. This study includes CCTA images of 110 patients who underwent invasive FFR pullback measurement in 112 arteries. First, a multi planar reconstruction (MP
    
[^27]: GraphCC: 数据中心拥塞控制的实用图学习方法

    GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters. (arXiv:2308.04905v1 [cs.NI])

    [http://arxiv.org/abs/2308.04905](http://arxiv.org/abs/2308.04905)

    本论文提出了一种名为GraphCC的实用图学习方法，用于优化数据中心的拥塞控制。通过结合多智能体强化学习（MARL）和图神经网络，该方法能够适应快速和突然的网络状态变化，并提高网络的性能。

    

    拥塞控制（CC）在优化数据中心网络（DCN）中的流量方面起着基础作用。目前，DCN主要实现了两种主要的CC协议：DCTCP和DCQCN。这两个协议及其主要变体都基于显式拥塞通知（ECN），其中中间交换机在检测到拥塞时标记数据包。ECN配置因此成为CC协议性能的关键因素。现在，网络专家设定静态ECN参数，精心选择以优化网络的平均性能。然而，现今的高速DCN经历快速和突然的变化，严重改变了网络状态（例如，动态流量工作负载，incast事件，故障）。这导致了低效利用和次优性能。本文提出了一种名为GraphCC的新颖的基于机器学习的网络内CC优化框架。我们的分布式解决方案依赖于多智能体强化学习（MARL）和图神经网络。

    Congestion Control (CC) plays a fundamental role in optimizing traffic in Data Center Networks (DCN). Currently, DCNs mainly implement two main CC protocols: DCTCP and DCQCN. Both protocols -- and their main variants -- are based on Explicit Congestion Notification (ECN), where intermediate switches mark packets when they detect congestion. The ECN configuration is thus a crucial aspect on the performance of CC protocols. Nowadays, network experts set static ECN parameters carefully selected to optimize the average network performance. However, today's high-speed DCNs experience quick and abrupt changes that severely change the network state (e.g., dynamic traffic workloads, incast events, failures). This leads to under-utilization and sub-optimal performance. This paper presents GraphCC, a novel Machine Learning-based framework for in-network CC optimization. Our distributed solution relies on a novel combination of Multi-agent Reinforcement Learning (MARL) and Graph Neural Networks (
    
[^28]: 迈向真正的微分方程发现

    Towards true discovery of the differential equations. (arXiv:2308.04901v1 [cs.LG])

    [http://arxiv.org/abs/2308.04901](http://arxiv.org/abs/2308.04901)

    本文研究了独立方程发现的先决条件和工具，并解决了在正确方程未知的情况下评估发现方程的挑战，旨在为在没有先验方程知识的情况下可靠地发现方程提供洞察力。

    

    微分方程发现是机器学习的一个子领域，用于开发可解释的模型，特别是在与自然相关的应用中。通过巧妙地结合运动方程的一般参数形式和合适的微分项，算法可以自动从数据中发现方程。本文探讨了独立方程发现的先决条件和工具，消除了对方程形式假设的需求。我们重点解决了在正确方程未知的情况下评估发现方程的适当性的挑战，旨在为在没有先验方程知识的情况下可靠地发现方程提供洞察力。

    Differential equation discovery, a machine learning subfield, is used to develop interpretable models, particularly in nature-related applications. By expertly incorporating the general parametric form of the equation of motion and appropriate differential terms, algorithms can autonomously uncover equations from data. This paper explores the prerequisites and tools for independent equation discovery without expert input, eliminating the need for equation form assumptions. We focus on addressing the challenge of assessing the adequacy of discovered equations when the correct equation is unknown, with the aim of providing insights for reliable equation discovery without prior knowledge of the equation form.
    
[^29]: 使用大语言模型分析软件供应链安全失败的实证研究

    An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures. (arXiv:2308.04898v1 [cs.CR])

    [http://arxiv.org/abs/2308.04898](http://arxiv.org/abs/2308.04898)

    本研究通过使用大语言模型（LLMs）对历史软件供应链安全失败进行分析，评估了其能力。通过自动化分析，可以降低成本并实现对更多失败案例的研究。

    

    随着我们越来越依赖软件系统，软件供应链被攻破的后果变得更加严重。像SolarWinds和ShadowHammer这样的高调网络攻击导致了重大的财务和数据损失，凸显了加强网络安全的需求。防止未来的破坏的一种方法是研究过去的失败案例。然而，传统的分析方法需要手动阅读和总结报告。自动化的支持可以降低成本并允许分析更多的失败案例。自然语言处理（NLP）技术如大语言模型（LLM）可以用来辅助分析失败。在这项研究中，我们评估了大语言模型（LLM）分析历史软件供应链违规的能力。我们使用LLM复制了Cloud Native Computing Foundation（CNCF）成员对69个软件供应链安全失败的手动分析。

    As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts
    
[^30]: 为何数据科学项目失败。

    Why Data Science Projects Fail. (arXiv:2308.04896v1 [cs.LG])

    [http://arxiv.org/abs/2308.04896](http://arxiv.org/abs/2308.04896)

    数据科学是一种现代数据智能实践，对于企业来说非常重要，但是数据科学项目失败的原因主要在于数据的可用性、算法和计算能力或基础设施等方面。

    

    数据科学是一种现代数据智能实践，是许多企业的核心，帮助企业更高效地解决业务挑战并建立智能战略。数据科学实践还可以使用算法自动化业务流程，并具有许多其他非盈利框架中的优势。就数据科学而言，影响数据科学项目有效结果的三个主要因素是：1. 数据的可用性，2. 算法，3. 计算能力或基础设施。

    Data Science is a modern Data Intelligence practice, which is the core of many businesses and helps businesses build smart strategies around to deal with businesses challenges more efficiently. Data Science practice also helps in automating business processes using the algorithm, and it has several other benefits, which also deliver in a non-profitable framework. In regards to data science, three key components primarily influence the effective outcome of a data science project. Those are 1.Availability of Data 2.Algorithm 3.Processing power or infrastructure
    
[^31]: NLLG季度arXiv报告 06/23：当前最具影响力的AI论文是什么？（arXiv:2308.04889v1 [cs.CY]）

    NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY])

    [http://arxiv.org/abs/2308.04889](http://arxiv.org/abs/2308.04889)

    该报告关注当前最具影响力的AI论文，并以标准化引用计数为依据编制了40篇最受欢迎的论文列表。观察到在2023年上半年，大型语言模型（LLMs）和具体而言的ChatGPT相关的论文占主导地位，ChatGPT表现出下降的趋势。

    

    人工智能（AI）领域中的生成式人工智能（Generative Artificial Intelligence，特别是自然语言处理（Natural Language Processing，NLP）和机器学习（Machine Learning，ML））信息的快速增长给研究人员和从业者带来了巨大的挑战，使得他们难以跟上最新的发展。为了解决信息过载的问题，Bielefeld大学的自然语言学习组在本报告中专注于识别arXiv上最受欢迎的论文，特别关注NLP和ML。其目标是为最相关且被广泛讨论的研究提供快速指南，以帮助新来者和已有研究人员跟上当前趋势。具体而言，我们根据2023年上半年的标准化引用计数编制了一个由40篇最受欢迎的论文组成的列表。我们观察到在2023年上半年，与大型语言模型（Large Language Models，LLMs）和具体而言的ChatGPT相关的论文占主导地位，而ChatGPT显示出下降的趋势。

    The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popul
    
[^32]: 瞄准且麻烦：追踪和广告在儿童网站上的问题

    Targeted and Troublesome: Tracking and Advertising on Children's Websites. (arXiv:2308.04887v1 [cs.CY])

    [http://arxiv.org/abs/2308.04887](http://arxiv.org/abs/2308.04887)

    这项研究提供了针对面向儿童的网站进行追踪和广告的详细测量。通过构建一个多语言分类器，并对过两百万个网页进行分类，研究者们编制了一个儿童网站列表。通过对这些网站进行爬取和分析，研究发现追踪器、指纹脚本和广告都存在于这些网站上。同时，研究还检测到这些广告是否启用了定向。

    

    在现代网络上，追踪器和广告商经常在未经同意的情况下构建和利用用户的详细行为配置文件。尽管对网络追踪机制和广告进行了各种研究，但还没有对面向儿童的网站进行严格的研究。为了填补这一空白，我们提出了一种针对面向儿童的网站进行追踪和（定向）广告的测量方法。在缺乏全面的针对儿童的网站（即面向儿童的网站）列表的基础上，我们首先构建了一个基于网页标题和描述的多语言分类器。将该分类器应用于超过两百万个网页中，我们编制了一个包含两千个儿童网站的列表。通过从五个观测点爬取这些网站，我们测量了追踪器、指纹脚本和广告的普遍存在。我们的爬虫检测到在面向儿童的网站上显示的广告，并通过抓取广告披露页面来确定是否启用了广告定向。我们的结果显示，ar

    On the modern web, trackers and advertisers frequently construct and monetize users' detailed behavioral profiles without consent. Despite various studies on web tracking mechanisms and advertisements, there has been no rigorous study focusing on websites targeted at children. To address this gap, we present a measurement of tracking and (targeted) advertising on websites directed at children. Motivated by lacking a comprehensive list of child-directed (i.e., targeted at children) websites, we first build a multilingual classifier based on web page titles and descriptions. Applying this classifier to over two million pages, we compile a list of two thousand child-directed websites. Crawling these sites from five vantage points, we measure the prevalence of trackers, fingerprinting scripts, and advertisements. Our crawler detects ads displayed on child-directed websites and determines if ad targeting is enabled by scraping ad disclosure pages whenever available. Our results show that ar
    
[^33]: 使用持久性方法去相关神经元

    Decorrelating neurons using persistence. (arXiv:2308.04870v1 [cs.LG])

    [http://arxiv.org/abs/2308.04870](http://arxiv.org/abs/2308.04870)

    本论文提出了一种使用持久性方法去除神经元之间高相关性的新方法，通过计算最小生成树的权重来构建正则化项，并通过大量实验证明了这些正则化项的有效性。结果表明，与常见的正则化项相比，这些正则化项能更好地提高深度学习模型的泛化能力，还发现冗余在人工神经网络中发挥着重要的作用。

    

    我们提出了一种改善深度学习模型泛化能力的新方法，通过减少神经元之间的高相关性。为此，我们从一个给定网络的神经元（或其中一部分样本）构成的团中，计算最小生成树的权重来计算两个正则化项，而边上的权重是相关性的差异。我们进行了大量的实验证明了我们的正则化项的有效性，并表明它们优于常见的正则化项。此外，我们还证明了仅仅最小化神经元之间的所有相关性得到的准确率比我们的正则化项要低，这表明冗余在人工神经网络中起到了重要作用，这一点在神经科学的一些研究中也有所证明。我们还证明了我们正则化项的可微性，从而开发了第一个考虑整个神经元集的基于拓扑持久性的有效的正则化方法，并且可以应用于实际网络。

    We propose a novel way to improve the generalisation capacity of deep learning models by reducing high correlations between neurons. For this, we present two regularisation terms computed from the weights of a minimum spanning tree of the clique whose vertices are the neurons of a given network (or a sample of those), where weights on edges are correlation dissimilarities. We provide an extensive set of experiments to validate the effectiveness of our terms, showing that they outperform popular ones. Also, we demonstrate that naive minimisation of all correlations between neurons obtains lower accuracies than our regularisation terms, suggesting that redundancies play a significant role in artificial neural networks, as evidenced by some studies in neuroscience for real networks. We include a proof of differentiability of our regularisers, thus developing the first effective topological persistence-based regularisation terms that consider the whole set of neurons and that can be applie
    
[^34]: 使用多智能体强化学习学习连续通信的消息编码技术的可扩展性

    Scalability of Message Encoding Techniques for Continuous Communication Learned with Multi-Agent Reinforcement Learning. (arXiv:2308.04844v1 [cs.LG])

    [http://arxiv.org/abs/2308.04844](http://arxiv.org/abs/2308.04844)

    本论文使用多智能体强化学习研究了在增加消息中包含的信息量和智能体数量增加的情况下，采用不同的消息编码方法对系统性能的影响。结果表明，均值消息编码器比注意力消息编码器更优，在智能体之间的通信协议中起到关键作用。

    

    许多多智能体系统需要智能体之间的通信以确保正确实现其目标。通过使用多智能体强化学习技术学习通信协议和动作协议，智能体可以灵活决定应该共享哪些信息。然而，当智能体数量增加时，我们需要创建包含在这些消息中的信息的编码方式。在本文中，我们研究了增加消息中应包含的信息量和增加智能体数量对两种不同消息编码方法（均值消息编码器和注意力消息编码器）的影响，并在一个矩阵环境中对这些影响进行了评估。令人惊讶的是，我们的结果表明，均值消息编码器始终优于注意力消息编码器。因此，我们分析了使用均值消息编码器的智能体使用的通信协议，并得出以下结论：

    Many multi-agent systems require inter-agent communication to properly achieve their goal. By learning the communication protocol alongside the action protocol using multi-agent reinforcement learning techniques, the agents gain the flexibility to determine which information should be shared. However, when the number of agents increases we need to create an encoding of the information contained in these messages. In this paper, we investigate the effect of increasing the amount of information that should be contained in a message and increasing the number of agents. We evaluate these effects on two different message encoding methods, the mean message encoder and the attention message encoder. We perform our experiments on a matrix environment. Surprisingly, our results show that the mean message encoder consistently outperforms the attention message encoder. Therefore, we analyse the communication protocol used by the agents that use the mean message encoder and can conclude that the a
    
[^35]: 通过意外记忆实现内在动机

    Intrinsic Motivation via Surprise Memory. (arXiv:2308.04836v1 [cs.LG])

    [http://arxiv.org/abs/2308.04836](http://arxiv.org/abs/2308.04836)

    该论文提出了一种新的计算模型，通过意外记忆作为内在奖励的基础，在强化学习中解决了现有方法的局限性。实验结果表明，通过结合意外预测器的意外记忆在稀疏奖励环境中表现出高效的探索行为，并显著提升了最终性能。

    

    我们提出了一种新的计算模型，用于强化学习中的内在奖励，解决了现有基于意外的探索的局限性。奖励是意外的新颖性，而不是意外的规范。我们通过一个记忆网络中的检索错误来估计意外的新颖性，记忆存储和重建意外。我们的意外记忆（SM）增加了基于意外的内在动力学的能力，保持了对激动人心的探索的兴趣，同时减少了对不可预测或噪声观察的不必要的吸引力。我们的实验表明，结合各种意外预测器的SM展示了高效的探索行为，并显著提升了稀疏奖励环境中的最终性能，包括噪声电视、导航和具有挑战性的Atari游戏。

    We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games.
    
[^36]: TSSR：一种截断和带符号的平方根激活函数用于神经网络

    TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks. (arXiv:2308.04832v1 [cs.CV])

    [http://arxiv.org/abs/2308.04832](http://arxiv.org/abs/2308.04832)

    本文介绍了一种新的激活函数 TSSR，具有奇数、非线性、单调和可微分的特性。实验证实了TSSR相比其他激活函数具有更好的性能，对神经网络模型的发展具有重要意义，并适用于多个领域的广泛应用。

    

    激活函数是神经网络的重要组成部分。本文介绍了一种新的激活函数，称为截断和带符号平方根（TSSR）函数。该函数具有奇数、非线性、单调和可微分的特性。其梯度是连续且始终为正。由于这些特性，它有潜力改善神经网络的数值稳定性。多个实验证实了所提出的TSSR比其他最先进的激活函数具有更好的性能。该函数对神经网络模型的发展具有重要意义，并可应用于计算机视觉、自然语言处理和语音识别等领域的广泛应用。

    Activation functions are essential components of neural networks. In this paper, we introduce a new activation function called the Truncated and Signed Square Root (TSSR) function. This function is distinctive because it is odd, nonlinear, monotone and differentiable. Its gradient is continuous and always positive. Thanks to these properties, it has the potential to improve the numerical stability of neural networks. Several experiments confirm that the proposed TSSR has better performance than other stat-of-the-art activation functions. The proposed function has significant implications for the development of neural network models and can be applied to a wide range of applications in fields such as computer vision, natural language processing, and speech recognition.
    
[^37]: 非参数分类规则在脉冲序列数据中的贝叶斯风险一致性

    Bayes Risk Consistency of Nonparametric Classification Rules for Spike Trains Data. (arXiv:2308.04796v1 [cs.IT])

    [http://arxiv.org/abs/2308.04796](http://arxiv.org/abs/2308.04796)

    本文研究了非参数分类规则在脉冲序列数据中的贝叶斯风险一致性，并导出了最优的贝叶斯规则和插值的非参数核分类器。通过渐近性质证明了核分类器收敛到贝叶斯规则，并通过有限样本模拟研究进行了验证。

    

    脉冲序列数据在计算神经科学、成像、流数据和金融等领域应用日益增多。脉冲序列的机器学习策略基于各种神经网络和概率模型。概率方法依赖于底层脉冲生成模型的参数化或非参数化规范。本文考虑了由非参数强度函数表征的一类脉冲序列数据的二类统计分类问题。我们推导出最优的贝叶斯规则，并构建了插值的非参数核分类器。我们建立了规则的渐近性质，包括随着记录时间间隔和训练样本量的增加而收敛。特别是，我们证明了核分类器收敛到贝叶斯规则。通过有限样本的模拟研究验证了所得到的结果。

    Spike trains data find a growing list of applications in computational neuroscience, imaging, streaming data and finance. Machine learning strategies for spike trains are based on various neural network and probabilistic models. The probabilistic approach is relying on parametric or nonparametric specifications of the underlying spike generation model. In this paper we consider the two-class statistical classification problem for a class of spike train data characterized by nonparametrically specified intensity functions. We derive the optimal Bayes rule and next form the plug-in nonparametric kernel classifier. Asymptotical properties of the rules are established including the limit with respect to the increasing recording time interval and the size of a training set. In particular the convergence of the kernel classifier to the Bayes rule is proved. The obtained results are supported by a finite sample simulation studies.
    
[^38]: PETformer: 通过增强占位符的Transformer实现长期时间序列预测

    PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer. (arXiv:2308.04791v1 [cs.LG])

    [http://arxiv.org/abs/2308.04791](http://arxiv.org/abs/2308.04791)

    PETformer是一个创新的模型，通过引入占位符增强技术，长子序列划分和多通道分离与交互的方法解决了将Transformer应用于长期时间序列预测时的关键问题。实验证明PETformer实现了最先进的性能。

    

    最近，基于Transformer的模型在长期时间序列预测（LTSF）任务中显示出卓越的性能，这归因于它们能够建模长期依赖关系。然而，将Transformer应用于LTSF任务的有效性仍然存在争议，尤其是最近的研究表明简单的线性模型可以胜过许多基于Transformer的方法。这表明，在LTSF中应用Transformer存在局限性。因此，本文研究了将Transformer应用于LTSF时的三个关键问题：时间连续性，信息密度和多通道关系。因此，我们提出了三种创新性解决方案，包括占位符增强技术（PET），长子序列划分（LSD）和多通道分离与交互（MSI），共同构成了一个名为PETformer的新模型。这三个关键设计引入了适合LTSF任务的先验偏差。大量实验证明PETformer实现了最先进的性能。

    Recently, Transformer-based models have shown remarkable performance in long-term time series forecasting (LTSF) tasks due to their ability to model long-term dependencies. However, the validity of Transformers for LTSF tasks remains debatable, particularly since recent work has shown that simple linear models can outperform numerous Transformer-based approaches. This suggests that there are limitations to the application of Transformer in LTSF. Therefore, this paper investigates three key issues when applying Transformer to LTSF: temporal continuity, information density, and multi-channel relationships. Accordingly, we propose three innovative solutions, including Placeholder Enhancement Technique (PET), Long Sub-sequence Division (LSD), and Multi-channel Separation and Interaction (MSI), which together form a novel model called PETformer. These three key designs introduce prior biases suitable for LTSF tasks. Extensive experiments have demonstrated that PETformer achieves state-of-th
    
[^39]: SUnAA: 使用典型分析的稀疏解混技术

    SUnAA: Sparse Unmixing using Archetypal Analysis. (arXiv:2308.04771v1 [cs.CV])

    [http://arxiv.org/abs/2308.04771](http://arxiv.org/abs/2308.04771)

    本文介绍了一种使用典型分析的新型稀疏解混技术（SUnAA），通过设计新模型并使用主动集算法迭代地最小化优化目标，实现了对混合成分的准确解混。实验证明SUnAA在信号重建误差方面具有比其他方法更好的性能。

    

    本文介绍了一种使用典型分析（SUnAA）的新型稀疏解混技术。首先，我们基于典型分析设计了一个新模型。我们假设感兴趣的成分是来自光谱库提供的成分的凸组合，并且感兴趣的成分数量是已知的。然后，我们提出了一个最小化问题。与大多数传统稀疏解混方法不同，这里的最小化问题是非凸的。我们使用主动集算法迭代地最小化优化目标。我们的方法对初始化鲁棒，并且只需要感兴趣的成分数量。使用两个模拟数据集评估了SUnAA，结果证实了其在信号重建误差方面相比其他常规和先进技术的更好性能。SUnAA还应用于Cuprite数据集，并将结果与该数据集提供的地质图进行了视觉比较。定性评估表明了它的有效性。

    This paper introduces a new sparse unmixing technique using archetypal analysis (SUnAA). First, we design a new model based on archetypal analysis. We assume that the endmembers of interest are a convex combination of endmembers provided by a spectral library and that the number of endmembers of interest is known. Then, we propose a minimization problem. Unlike most conventional sparse unmixing methods, here the minimization problem is non-convex. We minimize the optimization objective iteratively using an active set algorithm. Our method is robust to the initialization and only requires the number of endmembers of interest. SUnAA is evaluated using two simulated datasets for which results confirm its better performance over other conventional and advanced techniques in terms of signal-to-reconstruction error. SUnAA is also applied to Cuprite dataset and the results are compared visually with the available geological map provided for this dataset. The qualitative assessment demonstrate
    
[^40]: Tram-FL: 基于路由的去中心化联邦学习的模型训练

    Tram-FL: Routing-based Model Training for Decentralized Federated Learning. (arXiv:2308.04762v1 [cs.LG])

    [http://arxiv.org/abs/2308.04762](http://arxiv.org/abs/2308.04762)

    Tram-FL是一种基于路由的去中心化联邦学习（DFL）模型训练方法，通过逐步传输全局模型而不是本地模型的交换和汇聚，以及动态模型路由算法，实现在非IID条件下高模型准确性和降低通信成本。

    

    在去中心化联邦学习（DFL）中，频繁的节点间通信和非独立和同分布（non-IID）数据导致高精度模型获取的挑战。我们提出了一种名为Tram-FL的新型DFL方法，该方法通过逐个将全局模型传输到节点之间进行逐步细化，而不是通过交换和汇聚本地模型。我们还引入了一种动态模型路由算法，用于选择最佳路径，旨在在最小转发的情况下增强模型精度。我们使用MNIST、CIFAR-10和IMDb数据集进行的实验证明，Tram-FL在非IID条件下提供了高模型准确性，优于基准并降低了通信成本。

    In decentralized federated learning (DFL), substantial traffic from frequent inter-node communication and non-independent and identically distributed (non-IID) data challenges high-accuracy model acquisition. We propose Tram-FL, a novel DFL method, which progressively refines a global model by transferring it sequentially amongst nodes, rather than by exchanging and aggregating local models. We also introduce a dynamic model routing algorithm for optimal route selection, aimed at enhancing model precision with minimal forwarding. Our experiments using MNIST, CIFAR-10, and IMDb datasets demonstrate that Tram-FL with the proposed routing delivers high model accuracy under non-IID conditions, outperforming baselines while reducing communication costs.
    
[^41]: 非独立同分布联邦学习的特征匹配数据合成

    Feature Matching Data Synthesis for Non-IID Federated Learning. (arXiv:2308.04761v1 [cs.LG])

    [http://arxiv.org/abs/2308.04761](http://arxiv.org/abs/2308.04761)

    本文提出了一种用于非独立同分布联邦学习的特征匹配数据合成方法，通过生成合成数据来解决联邦学习中的非独立同分布问题，并提出了一种隐私保护的特征增强方法。这种方法能够改善模型的泛化能力并抹去真实特征的信息。

    

    联邦学习是一种隐私保护的范式，它在边缘设备上训练神经网络而不在中央服务器上收集数据。然而，联邦学习在处理设备之间的非独立同分布（non-IID）数据方面面临着固有的挑战。为了解决这个挑战，本文提出了一种硬特征匹配数据合成（HFMDS）方法，除了本地模型外，还共享辅助数据。具体而言，通过学习真实样本的基本类相关特征并丢弃多余的特征，生成了合成数据，这有助于有效处理非独立同分布问题。为了更好地保护隐私，我们提出了一种硬特征增强方法，将真实特征转移到决策边界附近，从而合成数据不仅改善了模型的泛化能力，而且抹去了真实特征的信息。通过将提出的HFMDS方法与联邦学习相结合，我们提出了一种新的联邦学习框架，其中包含数据增强的方法。

    Federated learning (FL) has emerged as a privacy-preserving paradigm that trains neural networks on edge devices without collecting data at a central server. However, FL encounters an inherent challenge in dealing with non-independent and identically distributed (non-IID) data among devices. To address this challenge, this paper proposes a hard feature matching data synthesis (HFMDS) method to share auxiliary data besides local models. Specifically, synthetic data are generated by learning the essential class-relevant features of real samples and discarding the redundant features, which helps to effectively tackle the non-IID issue. For better privacy preservation, we propose a hard feature augmentation method to transfer real features towards the decision boundary, with which the synthetic data not only improve the model generalization but also erase the information of real features. By integrating the proposed HFMDS method with FL, we present a novel FL framework with data augmentati
    
[^42]: 使用差分隐私合成对数据进行分布式协作学习

    Collaborative Learning From Distributed Data With Differentially Private Synthetic Twin Data. (arXiv:2308.04755v1 [cs.LG])

    [http://arxiv.org/abs/2308.04755](http://arxiv.org/abs/2308.04755)

    本研究提出了一种差分隐私合成对数据进行分布式协作学习的框架，在真实世界健康数据的实验中验证了其可行性。合作学习方通过共享合成数据可以获得更准确的统计估计，尤其对于包含少数群体的数据方有帮助。

    

    考虑到多个拥有敏感数据的方在合作学习群体级别统计数据时，无法合并敏感数据集的情况。我们提出了一个框架，其中每个方分享其数据的差分隐私合成对。我们研究了在来自英国生物数据库的真实世界健康数据上结合这种合成对数据进行协作学习的可行性。我们发现通过共享合成数据进行协作学习的方，相比仅使用本地数据，可以获得更准确的目标统计估计。这一发现同样适用于小型异构数据集的复杂情况。此外，参与方越多，改进的规模和一致性越大。最后，我们发现数据共享可以帮助包含少数群体的数据的方更好地进行调整分析。基于我们的结果，我们得出结论：共享合成数据可以在保护隐私的同时促进分布式协作学习。

    Consider a setting where multiple parties holding sensitive data aim to collaboratively learn population level statistics, but pooling the sensitive data sets is not possible. We propose a framework in which each party shares a differentially private synthetic twin of their data. We study the feasibility of combining such synthetic twin data sets for collaborative learning on real-world health data from the UK Biobank. We discover that parties engaging in the collaborative learning via shared synthetic data obtain more accurate estimates of target statistics compared to using only their local data. This finding extends to the difficult case of small heterogeneous data sets. Furthermore, the more parties participate, the larger and more consistent the improvements become. Finally, we find that data sharing can especially help parties whose data contain underrepresented groups to perform better-adjusted analysis for said groups. Based on our results we conclude that sharing of synthetic 
    
[^43]: 通过大型语言模型实现通用模糊测试

    Universal Fuzzing via Large Language Models. (arXiv:2308.04748v1 [cs.SE])

    [http://arxiv.org/abs/2308.04748](http://arxiv.org/abs/2308.04748)

    本文介绍了Fuzz4All，这是第一个能够针对许多不同的输入语言和这些语言的许多不同功能进行模糊测试的通用工具。

    

    模糊测试在发现各种软件系统中的漏洞和脆弱性方面取得了巨大成功。接受编程或形式语言作为输入的测试系统（SUTs），如编译器，运行时引擎，约束求解器和具有可访问API的软件库，尤其重要，因为它们是软件开发的基本构建块。然而，针对这些系统的现有模糊测试工具通常针对特定语言，因此无法轻易应用于其他语言甚至同一语言的其他版本。此外，现有模糊测试工具生成的输入通常局限于输入语言的特定功能，因此很难揭示与其他功能相关的漏洞或新功能。本文提出了Fuzz4All，这是第一个通用的模糊测试工具，它可以针对许多不同的输入语言和这些语言的许多不同功能进行测试。Fuzz4All的关键思想是利用大型语言模型（LLMs）作为输入生成器。

    Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input genera
    
[^44]: 优化基于Transformer的深度学习地震处理工作流程的网络

    Optimizing a Transformer-based network for a deep learning seismic processing workflow. (arXiv:2308.04739v1 [physics.geo-ph])

    [http://arxiv.org/abs/2308.04739](http://arxiv.org/abs/2308.04739)

    这项研究对基于Transformer的地震处理网络StorSeismic进行优化，通过改进位置编码和注意力机制，提高了其效率和表现力，并在实际数据上取得了良好的结果。

    

    StorSeismic是一个基于Transformer的模型，通过其预训练和微调训练策略适应各种地震处理任务。在原始实现中，StorSeismic利用了从自然语言处理(NLP)应用中借鉴的正弦位置编码和传统的自注意机制。对于地震处理，它们取得了良好的结果，但也提到了效率和表现力的限制。我们提出了对这两个关键组件的修改，通过使用相对位置编码和低秩注意力矩阵来替代原始组件。我们在实际的Marmousi和海上场数据上对这些改进进行了测试，作为一个序列策略，从降噪、直达波去除、多次衰减，最后到均方根速度(VRMS)预测用于正常移动补偿(NMO)校正。我们观察到预训练速度更快，并且能够达到竞争性的效果。

    StorSeismic is a recently introduced model based on the Transformer to adapt to various seismic processing tasks through its pretraining and fine-tuning training strategy. In the original implementation, StorSeismic utilized a sinusoidal positional encoding and a conventional self-attention mechanism, both borrowed from the natural language processing (NLP) applications. For seismic processing they admitted good results, but also hinted to limitations in efficiency and expressiveness. We propose modifications to these two key components, by utilizing relative positional encoding and low-rank attention matrices as replacements to the vanilla ones. The proposed changes are tested on processing tasks applied to a realistic Marmousi and offshore field data as a sequential strategy, starting from denoising, direct arrival removal, multiple attenuation, and finally root-mean-squared velocity ($V_{RMS}$) prediction for normal moveout (NMO) correction. We observe faster pretraining and competi
    
[^45]: 用五点卷积进行反应-扩散方程的深入研究

    Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations. (arXiv:2308.04735v1 [cs.LG])

    [http://arxiv.org/abs/2308.04735](http://arxiv.org/abs/2308.04735)

    该论文提出了使用五点卷积神经网络进行反应-扩散方程深入研究的方法，并提出了具有较大感受野的深层FCNNs，可以预测具有大于CFL条件阈值的时间演化。

    

    物理信息神经网络被广泛应用于偏微分方程，并取得了巨大的成功，因为物理信息损失基本上不需要观测或离散化。然而，优化模型参数很困难，并且这些参数必须针对每个不同的初始条件进行训练。为了克服这些挑战，在二阶反应-扩散类型方程中，一种可能的方法是使用五点卷积神经网络（FCNNs）。FCNNs使用两个连续的快照进行训练，其中时间步长对应于给定快照的步长。因此，FCNNs的时间演化取决于时间步长，并且时间步长必须满足其CFL条件，以避免爆炸解。在这项工作中，我们提出具有较大感受野的深层FCNNs，以预测具有大于CFL条件阈值的时间演化。为了评估我们的模型，我们考虑了热量、费舍尔和Allen-Cahn方程。

    Physics-informed neural networks have been widely applied to partial differential equations with great success because the physics-informed loss essentially requires no observations or discretization. However, it is difficult to optimize model parameters, and these parameters must be trained for each distinct initial condition. To overcome these challenges in second-order reaction-diffusion type equations, a possible way is to use five-point stencil convolutional neural networks (FCNNs). FCNNs are trained using two consecutive snapshots, where the time step corresponds to the step size of the given snapshots. Thus, the time evolution of FCNNs depends on the time step, and the time step must satisfy its CFL condition to avoid blow-up solutions. In this work, we propose deep FCNNs that have large receptive fields to predict time evolutions with a time step larger than the threshold of the CFL condition. To evaluate our models, we consider the heat, Fisher's, and Allen-Cahn equations with
    
[^46]: JEN-1：具有全向扩散模型的文本引导通用音乐生成

    JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models. (arXiv:2308.04729v1 [cs.SD])

    [http://arxiv.org/abs/2308.04729](http://arxiv.org/abs/2308.04729)

    JEN-1是一个高保真度通用音乐生成模型，通过结合自回归和非自回归训练，实现了文本引导的音乐生成、音乐修补和延续等生成任务，在文本音乐对齐和音乐质量方面表现出优越性，同时保持计算效率。

    

    随着深度生成模型的进步，音乐生成引起了越来越多的关注。然而，基于文本描述生成音乐（即文本到音乐）仍然具有挑战性，原因是音乐结构的复杂性和高采样率的要求。尽管任务的重要性，当前的生成模型在音乐质量、计算效率和泛化能力方面存在局限性。本文介绍了JEN-1，这是一个用于文本到音乐生成的通用高保真模型。JEN-1是一个结合了自回归和非自回归训练的扩散模型。通过上下文学习，JEN-1可以执行各种生成任务，包括文本引导的音乐生成、音乐修补以及延续。评估结果表明，JEN-1在文本音乐对齐和音乐质量方面表现出优越性，同时保持计算效率。我们的演示可在此网址获取：http://URL

    Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at this http URL
    
[^47]: 通过预训练语言模型探测和多层对比学习进行槽位归纳

    Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning. (arXiv:2308.04712v1 [cs.CL])

    [http://arxiv.org/abs/2308.04712](http://arxiv.org/abs/2308.04712)

    本研究通过利用预训练语言模型的探测和对比学习机制，在槽位归纳任务中，成功地诱导了槽位边界，并在两个NLU基准数据集上表现出与令牌级监督模型相当的性能，同时也能提供增强的槽位标签表示。

    

    最近在面向任务的对话系统（如意图识别和槽位填充）的自然语言理解中，需要大量的标注数据才能达到竞争性的性能。在现实中，标记的时间级别（槽位标签）耗时且难以获取。在本文中，我们研究了槽位归纳(SI)任务，其目标是在没有显式了解的情况下诱导槽位边界。我们提出了利用无监督预训练语言模型(PLM)探测和对比学习机制来利用(1)从PLM中提取的无监督语义知识，和(2)从TOD中可用的额外句子级意图标签信号。我们的方法在槽位归纳任务中证明了其有效性，并能够弥补与基于令牌级监督模型在两个NLU基准数据集上的差距。当推广到新出现的意图时，我们的SI目标也提供了增强的槽位标签表示，从而提高了性能。

    Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved per
    
[^48]: 生成扰动分析用于概率黑盒异常归因

    Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution. (arXiv:2308.04708v1 [cs.LG])

    [http://arxiv.org/abs/2308.04708](http://arxiv.org/abs/2308.04708)

    本文提出了一种新颖的概率异常归因框架，通过生成扰动来计算每个输入变量的归因得分的概率分布，并量化这些得分的不确定性。

    

    我们针对黑盒回归模型设置中的概率异常归因任务，旨在计算每个输入变量的归因得分的概率分布，给定一个观察到的异常。假设训练数据集不可用。与标准的可解释人工智能（XAI）情景不同，这个任务希望解释与黑盒预测的异常偏差，而不是解释黑盒模型本身。我们首先证明了主流的模型无关解释方法，如Shapley值，对于这个任务不适用，因为它们具有“偏差无关属性”。然后，我们提出了一种新颖的概率异常归因框架，不仅可以计算归因得分作为预测均值，还可以量化这些得分的不确定性。这是通过考虑一个生成过程来实现的，该过程对扰动进行反事实地将观测到的异常观察恢复到正常状态。

    We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probability distribution of the attribution score of each input variable, given an observed anomaly. The training dataset is assumed to be unavailable. This task differs from the standard XAI (explainable AI) scenario, since we wish to explain the anomalous deviation from a black-box prediction rather than the black-box model itself.  We begin by showing that mainstream model-agnostic explanation methods, such as the Shapley values, are not suitable for this task because of their ``deviation-agnostic property.'' We then propose a novel framework for probabilistic anomaly attribution that allows us to not only compute attribution scores as the predictive mean but also quantify the uncertainty of those scores. This is done by considering a generative process for perturbations that counter-factually bring the observed anomalous observation back to normalcy. We int
    
[^49]: Pareto不变表示学习在多媒体推荐中的应用

    Pareto Invariant Representation Learning for Multimedia Recommendation. (arXiv:2308.04706v1 [cs.IR])

    [http://arxiv.org/abs/2308.04706](http://arxiv.org/abs/2308.04706)

    本文介绍了一种名为Pareto Invariant Representation Learning（PaInvRL）的框架，应用于多媒体推荐。该框架通过学习不变表示和变体表示的同时来缓解通用表示引入的错误相关性问题。从IID-OOD多目标优化的角度，PaInvRL减少了错误相关性对用户偏好的影响。

    

    多媒体推荐涉及个性化排序任务，通常使用通用编码器表示多媒体内容。然而，这些通用表示引入了错误的相关性，无法揭示用户的真实偏好。现有的工作尝试通过学习不变表示来缓解这个问题，但忽视了独立同分布（IID）和非分布（OOD）广义化之间的平衡。本文提出了一个名为Pareto Invariant Representation Learning（PaInvRL）的框架，从IID-OOD多目标优化的角度减少了错误相关性的影响，同时学习不变表示（吸引用户注意的内在因素）和变体表示（其他因素）。具体而言，PaInvRL包括三个迭代执行的模块：（i）非同质识别模块，用于识别反映分布转移

    Multimedia recommendation involves personalized ranking tasks, where multimedia content is usually represented using a generic encoder. However, these generic representations introduce spurious correlations that fail to reveal users' true preferences. Existing works attempt to alleviate this problem by learning invariant representations, but overlook the balance between independent and identically distributed (IID) and out-of-distribution (OOD) generalization. In this paper, we propose a framework called Pareto Invariant Representation Learning (PaInvRL) to mitigate the impact of spurious correlations from an IID-OOD multi-objective optimization perspective, by learning invariant representations (intrinsic factors that attract user attention) and variant representations (other factors) simultaneously. Specifically, PaInvRL includes three iteratively executed modules: (i) heterogeneous identification module, which identifies the heterogeneous environments to reflect distributional shift
    
[^50]: 一种用于PDF恶意软件检测的小尺寸特征集

    A Feature Set of Small Size for the PDF Malware Detection. (arXiv:2308.04704v1 [cs.CR])

    [http://arxiv.org/abs/2308.04704](http://arxiv.org/abs/2308.04704)

    该研究提出了一种小型特征集，用于检测PDF恶意软件，无需太多领域知识。在六种机器学习模型中，使用Random Forest模型时可以达到99.75%的准确性。这个仅包含12个特征的特征集是最简洁的之一，尽管规模较小，但结果可与采用更大特征集的最先进方法相媲美。

    

    随着恶意软件威胁的增加和更加复杂化，基于机器学习（ML）的恶意软件检测系统变得越来越重要。PDF文件通常被用作钓鱼攻击的矢量，因为它们被广泛认为是可信的数据资源，并且在不同平台上都可以访问。因此，研究人员开发了许多不同的PDF恶意软件检测方法。特征选择对于检测PDF恶意软件的性能有很大影响。在这项研究中，我们提出了一个小型特征集，不需要太多关于PDF文件的领域知识。我们使用六种不同的机器学习模型评估了提出的特征。在使用Random Forest模型时，我们报告了最高99.75%的准确性。我们提出的仅包含12个特征的特征集是PDF恶意软件检测领域中最简洁的之一。尽管规模较小，但我们获得了与采用更大特征集的最先进方法相当的结果。

    Machine learning (ML)-based malware detection systems are becoming increasingly important as malware threats increase and get more sophisticated. PDF files are often used as vectors for phishing attacks because they are widely regarded as trustworthy data resources, and are accessible across different platforms. Therefore, researchers have developed many different PDF malware detection methods. Performance in detecting PDF malware is greatly influenced by feature selection. In this research, we propose a small features set that don't require too much domain knowledge of the PDF file. We evaluate proposed features with six different machine learning models. We report the best accuracy of 99.75% when using Random Forest model. Our proposed feature set, which consists of just 12 features, is one of the most conciseness in the field of PDF malware detection. Despite its modest size, we obtain comparable results to state-of-the-art that employ a much larger set of features.
    
[^51]: 使用基于图的聚类算法对Covid-19数据集进行分析的研究

    An Analytical Study of Covid-19 Dataset using Graph-Based Clustering Algorithms. (arXiv:2308.04697v1 [cs.LG])

    [http://arxiv.org/abs/2308.04697](http://arxiv.org/abs/2308.04697)

    该研究使用基于图的聚类算法对Covid-19数据集进行了分析。研究结果对疾病药物研发和人们免受大流行病伤害具有重要意义。

    

    冠状病毒疾病(COVID-19)是一种新型病毒，最早在2019年12月在中国武汉发现，现在这种致命疾病已经传播到世界各地。根据世界卫生组织（WHO）的数据，从2019年到2021年4月，共有312万4905人死亡。在这种情况下，许多方法、人工智能技术和机器学习算法被研究并用于拯救人们免受这一大流行病的伤害。SARS-CoV和2019-nCoV、SARS-CoV-2病毒侵入我们的身体，导致细胞蛋白质结构上的一些差异。蛋白质-蛋白质相互作用(PPI)是我们细胞中的一个重要过程，在药物研发和疾病方面起着非常重要的作用。在这项研究中，我们对由Covi-19数据集的92个基因生成的PPI网络进行了聚类分析。我们使用了三种基于图的聚类算法，以对聚类分析提供直观理解。

    Corona VIrus Disease abbreviated as COVID-19 is a novel virus which is initially identified in Wuhan of China in December of 2019 and now this deadly disease has spread all over the world. According to World Health Organization (WHO), a total of 3,124,905 people died from 2019 to 2021, April. In this case, many methods, AI base techniques, and machine learning algorithms have been researched and are being used to save people from this pandemic. The SARS-CoV and the 2019-nCoV, SARS-CoV-2 virus invade our bodies, causing some differences in the structure of cell proteins. Protein-protein interaction (PPI) is an essential process in our cells and plays a very important role in the development of medicines and gives ideas about the disease. In this study, we performed clustering on PPI networks generated from 92 genes of the Covi-19 dataset. We have used three graph-based clustering algorithms to give intuition to the analysis of clusters.
    
[^52]: 解释性人工智能在骨科中的应用: 挑战、机遇和前景

    Explainable AI in Orthopedics: Challenges, Opportunities, and Prospects. (arXiv:2308.04696v1 [cs.AI])

    [http://arxiv.org/abs/2308.04696](http://arxiv.org/abs/2308.04696)

    解释性人工智能在骨科中的应用面临挑战，但也带来机遇。为了实现透明度和可解释性，需要开发注重人工智能模型的透明性和可解释性的算法，并促进跨学科合作。

    

    虽然人工智能在各个领域取得了许多成功应用，但在医疗保健领域的应用却略显滞后。其中一些因素包括监管框架、患者隐私问题和数据异质性等影响了人工智能在医疗保健中的推广。然而，在骨科领域，特别是解释性和可解释性方面的挑战阻碍了人工智能的实施。解决解释性人工智能（XAI）在骨科中的问题需要开发注重透明度和可解释性的人工智能模型和算法，使临床医生、外科医生和患者能够理解任何依赖人工智能预测或描述模型的因素。本文概述了解释性人工智能在骨科实践中的几个关键挑战和机遇，并强调了人工智能从业者之间的跨学科合作的需求。

    While artificial intelligence (AI) has made many successful applications in various domains, its adoption in healthcare lags a little bit behind other high-stakes settings. Several factors contribute to this slower uptake, including regulatory frameworks, patient privacy concerns, and data heterogeneity. However, one significant challenge that impedes the implementation of AI in healthcare, particularly in orthopedics, is the lack of explainability and interpretability around AI models. Addressing the challenge of explainable AI (XAI) in orthopedics requires developing AI models and algorithms that prioritize transparency and interpretability, allowing clinicians, surgeons, and patients to understand the contributing factors behind any AI-powered predictive or descriptive models. The current contribution outlines several key challenges and opportunities that manifest in XAI in orthopedic practice. This work emphasizes the need for interdisciplinary collaborations between AI practitione
    
[^53]: 用于解决参数PDE的有限元算子网络

    Finite Element Operator Network for Solving Parametric PDEs. (arXiv:2308.04690v1 [math.NA])

    [http://arxiv.org/abs/2308.04690](http://arxiv.org/abs/2308.04690)

    本文提出了一种新方法，通过有限元算子网络（FEONet）解决参数PDE。它结合了深度学习和传统数值方法，展示了在没有输入-输出训练数据的情况下解决参数PDE的有效性，并在准确度、泛化性和计算灵活性方面优于现有方法。

    

    偏微分方程（PDE）是我们理解和预测物理、工程和金融等众多领域自然现象的基础。然而，解决参数PDE是一项复杂的任务，需要高效的数值方法。在本文中，我们提出了一种通过有限元算子网络（FEONet）解决参数PDE的新方法。我们的方法结合了深度学习和传统数值方法，特别是有限元法，以在没有任何配对的输入-输出训练数据的情况下解决参数PDE。我们在几个基准问题上展示了我们方法的效果，并且表明它在准确度、泛化性和计算灵活性方面优于现有的最先进方法。我们的FEONet框架在模拟具有不同边界条件和复杂域的各种领域中显示出潜力。

    Partial differential equations (PDEs) underlie our understanding and prediction of natural phenomena across numerous fields, including physics, engineering, and finance. However, solving parametric PDEs is a complex task that necessitates efficient numerical methods. In this paper, we propose a novel approach for solving parametric PDEs using a Finite Element Operator Network (FEONet). Our proposed method leverages the power of deep learning in conjunction with traditional numerical methods, specifically the finite element method, to solve parametric PDEs in the absence of any paired input-output training data. We demonstrate the effectiveness of our approach on several benchmark problems and show that it outperforms existing state-of-the-art methods in terms of accuracy, generalization, and computational flexibility. Our FEONet framework shows potential for application in various fields where PDEs play a crucial role in modeling complex domains with diverse boundary conditions and sin
    
[^54]: 快速NeRF合成和渲染的通用隐式框架

    A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v1 [cs.CV])

    [http://arxiv.org/abs/2308.04669](http://arxiv.org/abs/2308.04669)

    本研究提出了一个通用的隐式框架，可以快速合成和渲染NeRF对象。通过引入神经深度场的新的表面表示方法，可以实现动态阴影并允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。

    

    最近，各种神经辐射场方法在高渲染速度方面取得了显著成功。然而，当前的加速方法专门化并且不适用于各种隐式方法，这阻碍了对不同类型的NeRF作品进行实时合成。由于NeRF依赖于沿光线采样，因此可以提供一般性的指导。我们提出了一个通用的隐式流水线来快速合成NeRF对象。这种新方法使得动态阴影可以使用解析光源在对象内部或对象之间进行投射，同时允许多个NeRF对象以任意刚体变换无缝地放置和渲染在一起。主要地，我们的工作引入了一种称为神经深度场（NeDF）的新的表面表示方法，它通过允许光线和隐式表面之间的直接相交计算来快速确定对象之间的空间关系。它利用一个交点神经网络来加速查询NeRF。

    Recently, a variety of Neural radiance fields methods have garnered remarkable success in high render speed. However, current accelerating methods is specialized and not compatible for various implicit method, which prevent a real-time composition over different kinds of NeRF works. Since NeRF relies on sampling along rays, it's possible to provide a guidance generally. We propose a general implicit pipeline to rapidly compose NeRF objects. This new method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration inste
    
[^55]: 使用合成病理学先验知识在CT图像上对肺癌亚型进行分类

    Classification of lung cancer subtypes on CT images with synthetic pathological priors. (arXiv:2308.04663v1 [eess.IV])

    [http://arxiv.org/abs/2308.04663](http://arxiv.org/abs/2308.04663)

    本文提出了一种使用合成病理学先验知识的分类方法，通过深度神经网络从CT图像中提取信息并将其与病理学知识结合，实现了对肺癌亚型的准确分类。

    

    为了准确地对肺癌的病理亚型进行诊断，本文提出了一种自生成混合特征网络（SGHF-Net）来在计算机断层扫描（CT）图像上准确地对肺癌亚型进行分类。借鉴了研究结果表明在同一例CT图像和其病理图像之间存在跨尺度关联的观点，我们创新地开发了一种具有病理特征合成模块（PFSM）的方法，通过深度神经网络定量映射跨模态关联，从CT图像中获取对应病理图像中所包含的“黄金标准”信息。此外，我们设计了一个放射学特征提取模块（RFEM）来直接获取CT图像信息，并将其与病理学知识在一个有效的特征融合框架下集成，使整个分类模型能够生成更具指示性的结果。

    The accurate diagnosis on pathological subtypes for lung cancer is of significant importance for the follow-up treatments and prognosis managements. In this paper, we propose self-generating hybrid feature network (SGHF-Net) for accurately classifying lung cancer subtypes on computed tomography (CT) images. Inspired by studies stating that cross-scale associations exist in the image patterns between the same case's CT images and its pathological images, we innovatively developed a pathological feature synthetic module (PFSM), which quantitatively maps cross-modality associations through deep neural networks, to derive the "gold standard" information contained in the corresponding pathological images from CT images. Additionally, we designed a radiological feature extraction module (RFEM) to directly acquire CT image information and integrated it with the pathological priors under an effective feature fusion framework, enabling the entire classification model to generate more indicative
    
[^56]: 高效的贝叶斯优化方法：使用基于变换器预训练的深度核学习的多元异构数据集

    Efficient Bayesian Optimization with Deep Kernel Learning and Transformer Pre-trained on Multiple Heterogeneous Datasets. (arXiv:2308.04660v1 [cs.LG])

    [http://arxiv.org/abs/2308.04660](http://arxiv.org/abs/2308.04660)

    本文提出了一种高效的贝叶斯优化方法，使用变换器预训练的深度核学习和多元异构数据集。通过从先前任务中学习来共同预训练替代模型，并通过简单有效的混合初始化策略加速新任务的收敛，实验结果证明了方法的有效性。

    

    贝叶斯优化广泛应用于黑盒优化问题，它依赖于一个替代模型来近似黑盒响应函数。随着解决黑盒优化任务的数量增加，学习多个先前任务以共同预训练替代模型，以进一步提高优化效率的能力备受期待。本文提出了一种简单的方法来预训练替代模型，该模型是一个基于变换器编码器学习的深度特征上定义的高斯过程（GP），使用来自可能存在异构输入空间的先前任务的数据集。此外，我们提供了一种简单而有效的混合初始化策略，用于对应于未见过的输入变量的输入令牌，从而加速新任务的收敛。在合成和真实基准问题上的实验证明了我们提出的预训练和迁移贝叶斯优化策略相对于现有方法的有效性

    Bayesian optimization (BO) is widely adopted in black-box optimization problems and it relies on a surrogate model to approximate the black-box response function. With the increasing number of black-box optimization tasks solved and even more to solve, the ability to learn from multiple prior tasks to jointly pre-train a surrogate model is long-awaited to further boost optimization efficiency. In this paper, we propose a simple approach to pre-train a surrogate, which is a Gaussian process (GP) with a kernel defined on deep features learned from a Transformer-based encoder, using datasets from prior tasks with possibly heterogeneous input spaces. In addition, we provide a simple yet effective mix-up initialization strategy for input tokens corresponding to unseen input variables and therefore accelerate new tasks' convergence. Experiments on both synthetic and real benchmark problems demonstrate the effectiveness of our proposed pre-training and transfer BO strategy over existing metho
    
[^57]: 使用不确定性评分评估基于深度学习的前列腺癌分割模型的性能

    Assessing the performance of deep learning-based models for prostate cancer segmentation using uncertainty scores. (arXiv:2308.04653v1 [eess.IV])

    [http://arxiv.org/abs/2308.04653](http://arxiv.org/abs/2308.04653)

    本研究比较了多种基于深度学习的模型来进行前列腺癌分割，并评估了不确定性。最佳模型Attention R2U-Net在分割所有区域时具有最高的IoU和DSC，并且在过渡区域和肿瘤边界具有最低的不确定性值。

    

    本研究重点比较深度学习方法在MRI图像中进行前列腺分割和不确定性量化方面的表现。目标是改善前列腺癌检测和诊断的工作流程。评估了七种基于U-Net的架构，搭配Monte-Carlo dropout进行自动分割中心区域、外围区域、过渡区域和肿瘤，并进行不确定性估计。本研究的最佳模型是Attention R2U-Net，在分割所有区域时，平均交集联合比(IoU)达到76.3%，Dice相似度系数(DSC)达到85%。此外，与其他模型相比，Attention R2U-Net在过渡区域和肿瘤边界表现出最低的不确定性值。

    This study focuses on comparing deep learning methods for the segmentation and quantification of uncertainty in prostate segmentation from MRI images. The aim is to improve the workflow of prostate cancer detection and diagnosis. Seven different U-Net-based architectures, augmented with Monte-Carlo dropout, are evaluated for automatic segmentation of the central zone, peripheral zone, transition zone, and tumor, with uncertainty estimation. The top-performing model in this study is the Attention R2U-Net, achieving a mean Intersection over Union (IoU) of 76.3% and Dice Similarity Coefficient (DSC) of 85% for segmenting all zones. Additionally, Attention R2U-Net exhibits the lowest uncertainty values, particularly in the boundaries of the transition zone and tumor, when compared to the other models.
    
[^58]: 用心电图信号进行血流动力学推理的深度度量学习

    Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals. (arXiv:2308.04650v1 [cs.LG])

    [http://arxiv.org/abs/2308.04650](http://arxiv.org/abs/2308.04650)

    本研究提出了一种利用心电图信号进行血流动力学推理的深度度量学习方法。通过使用非侵入性信号进行心脏压力的评估，既可以在住院环境中用于诊断和治疗预后，也可以应用于门诊设置中。

    

    心力衰竭是一种影响全球数百万人的致残疾患，对他们的生活质量和死亡率具有重大影响。心脏压力的客观评估仍然是诊断和治疗预后的重要方法。尽管心脏导管化验是估计中心血液动力学压力的黄金标准，但它是一种有潜在风险的侵入性操作，对某些患者来说可能是危险的。利用非侵入性信号（如心电图（ECG））的方法可以使常规估计心脏压力在住院和门诊环境中成为可能。先前训练用于以监督的方式估计心脏内压力（例如平均肺毛细血管楔压（mPCWP））的模型显示了良好的鉴别能力，但仅限于心力衰竭队列的标记数据集。

    Heart failure is a debilitating condition that affects millions of people worldwide and has a significant impact on their quality of life and mortality rates. An objective assessment of cardiac pressures remains an important method for the diagnosis and treatment prognostication for patients with heart failure. Although cardiac catheterization is the gold standard for estimating central hemodynamic pressures, it is an invasive procedure that carries inherent risks, making it a potentially dangerous procedure for some patients. Approaches that leverage non-invasive signals - such as electrocardiogram (ECG) - have the promise to make the routine estimation of cardiac pressures feasible in both inpatient and outpatient settings. Prior models trained to estimate intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP)) in a supervised fashion have shown good discriminatory ability but have been limited to the labeled dataset from the heart failure cohort. To address th
    
[^59]: 提高优化性能: 一种新颖的高斯崩溃搜索和Powell's Method的混合用于无导数优化

    Enhancing Optimization Performance: A Novel Hybridization of Gaussian Crunching Search and Powell's Method for Derivative-Free Optimization. (arXiv:2308.04649v1 [math.OC])

    [http://arxiv.org/abs/2308.04649](http://arxiv.org/abs/2308.04649)

    通过将高斯崩溃搜索(GCS)与传统无导数优化方法混合，可以提高优化性能并打开新的可能性。

    

    这篇研究论文介绍了一种通过高斯崩溃搜索(GCS)和Powell's Method的混合来增强优化性能的新方法。尽管GCS已经显示出克服传统无导数优化方法所面临挑战的潜力[1]，但它可能不总是在寻找局部最小值方面表现出色。另一方面，一些传统方法在这方面的表现可能更好。然而，GCS展示了其在逃离局部极小值陷阱和接近全局极小值方面的优势。通过实验证明，通过将GCS与某些传统的无导数优化方法相结合，我们可以在保留每种方法的各自优点的同时显著提高性能。这种混合方法为优化复杂系统和在各种应用中寻找最优解打开了新的可能性。

    This research paper presents a novel approach to enhance optimization performance through the hybridization of Gaussian Crunching Search (GCS) and Powell's Method for derivative-free optimization. While GCS has shown promise in overcoming challenges faced by traditional derivative-free optimization methods [1], it may not always excel in finding the local minimum. On the other hand, some traditional methods may have better performance in this regard. However, GCS demonstrates its strength in escaping the trap of local minima and approaching the global minima. Through experimentation, we discovered that by combining GCS with certain traditional derivative-free optimization methods, we can significantly boost performance while retaining the respective advantages of each method. This hybrid approach opens up new possibilities for optimizing complex systems and finding optimal solutions in a range of applications.
    
[^60]: 稀疏二值变压器用于多元时间序列建模

    Sparse Binary Transformers for Multivariate Time Series Modeling. (arXiv:2308.04637v1 [cs.LG])

    [http://arxiv.org/abs/2308.04637](http://arxiv.org/abs/2308.04637)

    本研究将稀疏二值变压器应用于多元时间序列问题，并展示了该轻量级模型在分类、异常检测和单步预测任务中取得了与密集浮点变压器相当的准确度。同时，通过两个修改降低了注意机制的计算复杂性。

    

    压缩神经网络有可能实现在新应用和较小的计算环境中进行深度学习。然而，目前对于这种模型在哪些学习任务中能够成功的了解不多。在这项工作中，我们将稀疏和二值权重的变压器应用于多元时间序列问题，结果显示这种轻量级模型的准确度与相同结构的密集浮点变压器相当。我们的模型在三个时间序列学习任务中取得了好的结果：分类、异常检测和单步预测。此外，为了降低注意机制的计算复杂性，我们进行了两个修改，这两个修改在模型性能上几乎没有下降：1) 在分类任务中，我们对查询、键和值激活应用了一个固定的掩码；2) 对于预测和异常检测，这些任务都依赖于对单个时间点的输出进行预测，我们提出了一种注意力机制的变体。

    Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attentio
    
[^61]: 多类在线学习在Bandit反馈下的研究

    Multiclass Online Learnability under Bandit Feedback. (arXiv:2308.04620v1 [cs.LG])

    [http://arxiv.org/abs/2308.04620](http://arxiv.org/abs/2308.04620)

    Bandit反馈下的在线多类学习的关键在于Bandit Littlestone维度的有限性，无论标签空间是否无界。

    

    我们研究了在Bandit反馈下的多类在线分类问题。我们扩展了(daniely2013price)的结果，通过展示Bandit Littlestone维度的有限性是多类在线学习的必要且充分条件，即使标签空间是无界的。我们的结果补充了(hanneke2023multiclass)的最近工作，他们在标签空间无界的全信息设置中，展示了Littlestone维度刻画了在线多类学习的能力。

    We study online multiclass classification under bandit feedback. We extend the results of (daniely2013price) by showing that the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability even when the label space is unbounded. Our result complements the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unbounded.
    
[^62]: 改进的激活剪裁方法用于普适性后门缓解和测试时间检测

    Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection. (arXiv:2308.04617v1 [cs.LG])

    [http://arxiv.org/abs/2308.04617](http://arxiv.org/abs/2308.04617)

    该论文提出了一种改进的激活剪裁方法用于普适性后门缓解和测试时间检测，该方法通过限制激活边界来提高对后门攻击的鲁棒性并在图像分类任务中表现出优越性能。

    

    深度神经网络易受到后门攻击（特洛伊），攻击者通过在训练集中注入后门触发器，使得神经网络在测试时将触发器分类到攻击者指定的目标类别。最近的研究表明，后门注入在受攻击模型中引起过拟合（异常大的激活），这促使我们提出了一种通用的、训练后的剪裁方法来缓解后门，即通过用一小组干净样本学习内部层的激活边界。我们设计了一种新的方法，选择激活边界来显式地限制分类边界。这种方法在CIFAR-10图像分类的对标方法中表现出优越性能。我们还展示了这种方法对于自适应攻击、X2X攻击和不同数据集的强大鲁棒性。最后，我们展示了基于原始和激活之间的输出差异的测试时间检测和纠正方法的扩展。

    Deep neural networks are vulnerable to backdoor attacks (Trojans), where an attacker poisons the training set with backdoor triggers so that the neural network learns to classify test-time triggers to the attacker's designated target class. Recent work shows that backdoor poisoning induces over-fitting (abnormally large activations) in the attacked model, which motivates a general, post-training clipping method for backdoor mitigation, i.e., with bounds on internal-layer activations learned using a small set of clean samples. We devise a new such approach, choosing the activation bounds to explicitly limit classification margins. This method gives superior performance against peer methods for CIFAR-10 image classification. We also show that this method has strong robustness against adaptive attacks, X2X attacks, and on different datasets. Finally, we demonstrate a method extension for test-time detection and correction based on the output differences between the original and activation
    
[^63]: 机器学习、深度学习和数据预处理技术在检测、预测和监测压力和压力相关精神障碍方面的应用：一项范围审查

    Machine Learning, Deep Learning and Data Preprocessing Techniques for Detection, Prediction, and Monitoring of Stress and Stress-related Mental Disorders: A Scoping Review. (arXiv:2308.04616v1 [cs.LG])

    [http://arxiv.org/abs/2308.04616](http://arxiv.org/abs/2308.04616)

    这项综述研究系统评估了机器学习在压力和压力相关精神障碍检测、预测和分析方面的应用。支持向量机、神经网络和随机森林模型展现出了更高的准确性和可靠性，并且生理参数如心率测量和皮肤反应在压力预测中被广泛使用。

    

    本综述系统评估了机器学习（ML）方法在检测、预测和分析精神压力及其相关精神障碍（MDs）方面的应用。通过严格的范围审查过程，研究探讨了应用于压力与压力相关MDs背景下的最新ML算法、预处理技术和数据类型。研究结果表明，支持向量机（SVM）、神经网络（NN）和随机森林（RF）模型在所有机器学习算法中一贯表现出更高的准确性和可靠性。此外，综述强调，生理参数如心率测量和皮肤反应是ML算法中常用的压力预测因子。这是因为它们能提供关于压力和压力相关MDs的丰富说明性信息，并且数据采集相对容易。

    This comprehensive review systematically evaluates Machine Learning (ML) methodologies employed in the detection, prediction, and analysis of mental stress and its consequent mental disorders (MDs). Utilizing a rigorous scoping review process, the investigation delves into the latest ML algorithms, preprocessing techniques, and data types employed in the context of stress and stress-related MDs. The findings highlight that Support Vector Machine (SVM), Neural Network (NN), and Random Forest (RF) models consistently exhibit superior accuracy and robustness among all machine learning algorithms examined. Furthermore, the review underscores that physiological parameters, such as heart rate measurements and skin response, are prevalently used as stress predictors in ML algorithms. This is attributed to their rich explanatory information concerning stress and stress-related MDs, as well as the relative ease of data acquisition. Additionally, the application of dimensionality reduction techn
    
[^64]: 使用深度学习进行方向找寻的稀疏数组设计

    Sparse Array Design for Direction Finding using Deep Learning. (arXiv:2308.04615v1 [eess.SP])

    [http://arxiv.org/abs/2308.04615](http://arxiv.org/abs/2308.04615)

    该论文介绍了使用深度学习设计稀疏数组的几个方向找寻应用，包括认知雷达、无线通信和综合感知和通信（ISAC）应用等。这些方法具有特征工程和低预测阶段复杂性的优势。

    

    在过去的几年中，引入了深度学习（DL）技术来设计稀疏数组。这些方法具有特征工程和低预测阶段复杂性的优势，在解决寻找稀疏数组的组合搜索问题方面非常有帮助。在本文中，我们提供了基于DL稀疏数组的多个方向找寻应用的概要。我们首先研究了在认知雷达应用中选择稀疏数组的监督和迁移学习技术。在这里，我们还讨论了使用模拟退火等元启发式学习算法来设计二维稀疏数组的情况。接下来，我们考虑了用于无线通信的DL基础天线选择，在其中稀疏数组问题也可以与信道估计、波束成形或定位相结合。最后，我们提供了一个用于综合感知和通信（ISAC）应用的深度稀疏数组技术的例子。

    In the past few years, deep learning (DL) techniques have been introduced for designing sparse arrays. These methods offer the advantages of feature engineering and low prediction-stage complexity, which is helpful in tackling the combinatorial search inherent to finding a sparse array. In this chapter, we provide a synopsis of several direction finding applications of DL-based sparse arrays. We begin by examining supervised and transfer learning techniques that have applications in selecting sparse arrays for a cognitive radar application. Here, we also discuss the use of meta-heuristic learning algorithms such as simulated annealing for the case of designing two-dimensional sparse arrays. Next, we consider DL-based antenna selection for wireless communications, wherein sparse array problem may also be combined with channel estimation, beamforming, or localization. Finally, we provide an example of deep sparse array technique for integrated sensing and communications (ISAC) applicatio
    
[^65]: 深度学习驱动的海啸相关内部重力波检测：通向开放海洋自然灾害检测的途径。

    Deep Learning Driven Detection of Tsunami Related Internal GravityWaves: a path towards open-ocean natural hazards detection. (arXiv:2308.04611v1 [cs.LG])

    [http://arxiv.org/abs/2308.04611](http://arxiv.org/abs/2308.04611)

    该论文提出了一个利用深度学习驱动的方法，通过检测电离层中的内部重力波来实现海啸的早期预警系统，提供了开放海洋覆盖。这个方法利用了大量的GNSS数据，通过处理成千上万个数据流之间的复杂非线性关系来增强海啸检测能力。

    

    海啸会在电离层中引发内部重力波 (IGWs)，扰动总电子含量 (TEC)，称为行进电离层扰动 (TIDs)，可通过全球导航卫星系统 (GNSS) 进行检测。GNSS是一组卫星，提供来自地球轨道的信号，包括欧洲的伽利略、美国的全球定位系统 (GPS)、俄罗斯的格洛纳斯 (GLONASS) 和中国的北斗。实时检测TIDs为海啸检测提供了一种方法，在不可由基于浮标的预警系统服务的地理区域内提供了开放海洋覆盖。深度学习利用大量的GNSS数据，有效处理成千上万个数据流之间的复杂非线性关系。我们描述了一个框架，利用来自VARION（实时电离层观测的变差方法）算法的斜向总电子含量（sTEC）。

    Tsunamis can trigger internal gravity waves (IGWs) in the ionosphere, perturbing the Total Electron Content (TEC) - referred to as Traveling Ionospheric Disturbances (TIDs) that are detectable through the Global Navigation Satellite System (GNSS). The GNSS are constellations of satellites providing signals from Earth orbit - Europe's Galileo, the United States' Global Positioning System (GPS), Russia's Global'naya Navigatsionnaya Sputnikovaya Sistema (GLONASS) and China's BeiDou. The real-time detection of TIDs provides an approach for tsunami detection, enhancing early warning systems by providing open-ocean coverage in geographic areas not serviceable by buoy-based warning systems. Large volumes of the GNSS data is leveraged by deep learning, which effectively handles complex non-linear relationships across thousands of data streams. We describe a framework leveraging slant total electron content (sTEC) from the VARION (Variometric Approach for Real-Time Ionosphere Observation) algor
    
[^66]: PSRFlow: 基于流式模型的科学数据概率超分辨率

    PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data. (arXiv:2308.04605v1 [eess.IV])

    [http://arxiv.org/abs/2308.04605](http://arxiv.org/abs/2308.04605)

    PSRFlow是一种基于流式模型的科学数据超分辨率算法，能够量化超分辨结果的误差和不确定性，以及生成多个可行的超分辨率输出。

    

    尽管近年来提出了许多基于深度学习的超分辨率方法，但由于推理阶段缺乏真值，很少能够量化超分辨结果的误差和不确定性。然而，对于科学可视化应用来说，将结果的不确定性传达给科学家是至关重要的，以避免生成误导或错误的信息。在本文中，我们提出了一种新颖的基于流式模型的生成模型PSRFlow，用于科学数据超分辨率，并将不确定性量化融入到超分辨过程中。PSRFlow基于低分辨率数据学习高分辨率数据的条件分布。通过从高分辨率数据中捕获的缺失信息的高斯潜在空间进行采样，可以生成不同的可行超分辨率输出。在高斯潜在空间中进行高效采样使得我们的模型能够进行不确定性量化。

    Although many deep-learning-based super-resolution approaches have been proposed in recent years, because no ground truth is available in the inference stage, few can quantify the errors and uncertainties of the super-resolved results. For scientific visualization applications, however, conveying uncertainties of the results to scientists is crucial to avoid generating misleading or incorrect information. In this paper, we propose PSRFlow, a novel normalizing flow-based generative model for scientific data super-resolution that incorporates uncertainty quantification into the super-resolution process. PSRFlow learns the conditional distribution of the high-resolution data based on the low-resolution counterpart. By sampling from a Gaussian latent space that captures the missing information in the high-resolution data, one can generate different plausible super-resolution outputs. The efficient sampling in the Gaussian latent space allows our model to perform uncertainty quantification 
    
[^67]: 分散式联邦学习综述

    A Survey on Decentralized Federated Learning. (arXiv:2308.04604v1 [cs.LG])

    [http://arxiv.org/abs/2308.04604](http://arxiv.org/abs/2308.04604)

    最近几年，联邦学习成为训练分布式、大规模、保护隐私的机器学习系统的流行范式。然而，其中一个关键挑战是克服集中式编排的单点故障问题。

    

    最近几年，联邦学习（FL）已经成为训练分布式、大规模、保护隐私的机器学习（ML）系统的流行范式。与标准ML不同，需要将数据收集在训练执行的确切位置，FL利用数百万边缘设备的计算能力来协同训练共享的全局模型，同时不会披露其本地私有数据。在典型的FL系统中，中央服务器只充当协调器的角色；它迭代地收集和汇总每个客户端在自己的私有数据上训练的本地模型，直到收敛。尽管FL在设计上具有许多优点（例如通过设计保护私有数据所有权），但也存在一些弱点。其中最关键的挑战之一是克服经典FL客户端-服务器架构的集中式编排，这被认为是易受单点故障攻击的。

    In recent years, federated learning (FL) has become a very popular paradigm for training distributed, large-scale, and privacy-preserving machine learning (ML) systems. In contrast to standard ML, where data must be collected at the exact location where training is performed, FL takes advantage of the computational capabilities of millions of edge devices to collaboratively train a shared, global model without disclosing their local private data. Specifically, in a typical FL system, the central server acts only as an orchestrator; it iteratively gathers and aggregates all the local models trained by each client on its private data until convergence. Although FL undoubtedly has several benefits over traditional ML (e.g., it protects private data ownership by design), it suffers from several weaknesses. One of the most critical challenges is to overcome the centralized orchestration of the classical FL client-server architecture, which is known to be vulnerable to single-point-of-failur
    
[^68]: 基于深度学习的图像水印技术：简要调查

    Deep Learning based Image Watermarking: A Brief Survey. (arXiv:2308.04603v1 [cs.MM])

    [http://arxiv.org/abs/2308.04603](http://arxiv.org/abs/2308.04603)

    该论文调查了基于深度学习的图像水印技术的最新研究进展，将其分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。对每个类别的研究方向进行了分析和总结，并讨论了未来的研究方向。

    

    图像水印是指在一张封面图像中秘密嵌入和提取水印以保护图像的行为。近年来，基于深度学习的图像水印技术层出不穷。为了研究最新的技术，本调查将前沿的基于深度学习的图像水印技术分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。还分析和总结了每个类别中的研究方向。此外，还讨论了潜在的未来研究方向，展望未来的研究。

    The act of secretly embedding and extracting a watermark on a cover image to protect it is known as image watermarking. In recent years, deep learning-based image watermarking techniques have been emerging one after another. To study the state-of-the-art, this survey categorizes cutting-edge deep learning-based image watermarking techniques into Embedder-Extractor Joint Training, Deep Networks as a Feature Transformation, and Hybrid schemes. Research directions in each category are also analyzed and summarized. Additionally, potential future research directions are discussed to envision future studies.
    
[^69]: 深度神经网络压缩中的量化感知分解方法

    Quantization Aware Factorization for Deep Neural Network Compression. (arXiv:2308.04595v1 [cs.LG])

    [http://arxiv.org/abs/2308.04595](http://arxiv.org/abs/2308.04595)

    本论文提出了一种量化感知分解方法，通过在量化因子上找到张量逼近，既实现了神经网络压缩，又保持了模型预测的准确性。该方法使用交替方向乘子法进行规范多线性分解，并与最新的后训练量化方法进行比较分析。

    

    张量分解是在神经网络中减少参数和FLOP的有效方法。由于移动或嵌入式设备的内存和功耗限制，当部署预训练模型时，通常需要量化步骤。对于具有分解权重的网络应用传统的后训练量化方法会导致准确度下降。针对这个问题，我们提出了一种算法，直接在量化因子上找到张量逼近，从而在保持模型预测品质的同时，同时从压缩技术中受益。具体而言，我们提出使用交替方向乘子法(ADMM)进行具有量化格点元素的规范多线性(CP)分解。我们使用一种设计的算法压缩神经网络权重，并评估其预测品质和性能。我们将我们的方法与最新的后训练量化方法进行比较。

    Tensor decomposition of convolutional and fully-connected layers is an effective way to reduce parameters and FLOP in neural networks. Due to memory and power consumption limitations of mobile or embedded devices, the quantization step is usually necessary when pre-trained models are deployed. A conventional post-training quantization approach applied to networks with decomposed weights yields a drop in accuracy. This motivated us to develop an algorithm that finds tensor approximation directly with quantized factors and thus benefit from both compression techniques while keeping the prediction quality of the model. Namely, we propose to use Alternating Direction Method of Multipliers (ADMM) for Canonical Polyadic (CP) decomposition with factors whose elements lie on a specified quantization grid. We compress neural network weights with a devised algorithm and evaluate it's prediction quality and performance. We compare our approach to state-of-the-art post-training quantization method
    
[^70]: ScatterUQ: 用于多类别深度学习问题的交互式不确定性可视化

    ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep Learning Problems. (arXiv:2308.04588v1 [cs.LG])

    [http://arxiv.org/abs/2308.04588](http://arxiv.org/abs/2308.04588)

    ScatterUQ是一个交互式系统，通过可视化展示模型在上下文驱动的不确定性设置下的性能，为用户提供了更好的理解模型预测的能力。

    

    最近，针对多类别标签问题开发了一种不确定性感知的深度学习方法，提供了校准的类别预测概率和超出分布指示器，使得机器学习的使用者和工程师可以评估模型对其预测的信心。然而，这种额外的神经网络预测信息在多个不确定性环境下对任意数据源进行可视化传达具有挑战性。为了解决这些挑战，我们提出了ScatterUQ，这是一个交互式系统，提供有针对性的可视化，让用户能够更好地理解模型在上下文驱动的不确定性设置下的性能。

    Recently, uncertainty-aware deep learning methods for multiclass labeling problems have been developed that provide calibrated class prediction probabilities and out-of-distribution (OOD) indicators, letting machine learning (ML) consumers and engineers gauge a model's confidence in its predictions. However, this extra neural network prediction information is challenging to scalably convey visually for arbitrary data sources under multiple uncertainty contexts. To address these challenges, we present ScatterUQ, an interactive system that provides targeted visualizations to allow users to better understand model performance in context-driven uncertainty settings. ScatterUQ leverages recent advances in distance-aware neural networks, together with dimensionality reduction techniques, to construct robust, 2-D scatter plots explaining why a model predicts a test example to be (1) in-distribution and of a particular class, (2) in-distribution but unsure of the class, and (3) out-of-distribu
    
[^71]: 决定性混淆下的内核单一代理控制

    Kernel Single Proxy Control for Deterministic Confounding. (arXiv:2308.04585v1 [stat.ML])

    [http://arxiv.org/abs/2308.04585](http://arxiv.org/abs/2308.04585)

    本研究考虑了具有未观测混淆因素的因果效应估计问题，在结果是确定性生成的情况下，提出了一种使用单一代理变量的内核方法，通过两阶段回归和最大矩约束的方法可以一致估计因果效应，并在合成数据集上成功恢复了因果效应。

    

    本文考虑具有未观测混淆因素的因果效应估计问题，其中我们观测到与混淆因素相关的代理变量。尽管代理因果学习（PCL）使用两个代理变量来恢复真实的因果效应，我们证明如果结果是确定性生成的，则使用单个代理变量就足以进行因果估计，并概括了控制结果校准法（COCA）。我们提出了两种基于内核的方法：一种基于两阶段回归方法，另一种基于最大矩约束方法。我们证明了这两种方法都可以一致地估计因果效应，并通过合成数据集的实证实验成功地恢复了因果效应。

    We consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. Although Proxy Causal Learning (PCL) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing Control Outcome Calibration Approach (COCA). We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on a synthetic dataset.
    
[^72]: RECipe: 多模态菜谱知识图谱适用于多用途推荐系统吗？

    RECipe: Does a Multi-Modal Recipe Knowledge Graph Fit a Multi-Purpose Recommendation System?. (arXiv:2308.04579v1 [cs.IR])

    [http://arxiv.org/abs/2308.04579](http://arxiv.org/abs/2308.04579)

    RECipe是一个多用途的菜谱推荐框架，使用多模态知识图谱作为支撑，通过在用户以自然语言查询或提供图像时向用户推荐菜谱，超越传统推荐系统的方法。

    

    在过去的两十年中，推荐系统（RS）已经使用机器学习（ML）的解决方案为商业或在线平台的客户推荐电影、图书和餐厅等物品。然而，相对于这些应用，菜谱推荐尚未得到很多关注。我们介绍了RECipe作为一个多用途的菜谱推荐框架，具有多模态知识图谱（MMKG）的支撑。RECipe的动机是通过在用户以自然语言查询或提供图像时向用户推荐菜谱，超越（深度）神经协同过滤（NCF）。RECipe由3个子系统组成：基于行为的推荐器、基于评论的推荐器和基于图像的推荐器。每个子系统都依赖于图中实体和关系的嵌入表示。我们首先从微软的MPNet模型的微调模型中获取文本实体（例如评论或成分）的（预训练）嵌入表示。

    Over the past two decades, recommendation systems (RSs) have used machine learning (ML) solutions to recommend items, e.g., movies, books, and restaurants, to clients of a business or an online platform. Recipe recommendation, however, has not yet received much attention compared to those applications. We introduce RECipe as a multi-purpose recipe recommendation framework with a multi-modal knowledge graph (MMKG) backbone. The motivation behind RECipe is to go beyond (deep) neural collaborative filtering (NCF) by recommending recipes to users when they query in natural language or by providing an image. RECipe consists of 3 subsystems: (1) behavior-based recommender, (2) review-based recommender, and (3) image-based recommender. Each subsystem relies on the embedding representations of entities and relations in the graph. We first obtain (pre-trained) embedding representations of textual entities, such as reviews or ingredients, from a fine-tuned model of Microsoft's MPNet. We initiali
    
[^73]: 从假到真（FFR）：一种用于减少与合成数据相关性错误的两阶段训练流程

    From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data. (arXiv:2308.04553v1 [cs.CV])

    [http://arxiv.org/abs/2308.04553](http://arxiv.org/abs/2308.04553)

    本文提出了一个两阶段训练流程，通过在一个平衡的合成数据集上进行预训练，然后在真实数据上进行微调，减少了视觉识别模型学习到与数据集偏差相关的错误的问题。

    

    视觉识别模型容易学习到由于训练集的不平衡导致的相关性错误，其中某些群体（如女性）在某些类别（如程序员）中代表性不足。生成模型通过为少数样本生成合成数据来减少这种偏差，从而平衡训练集。然而，先前使用这些方法的工作忽视了视觉识别模型往往能够学习区分真实图像和合成图像的能力，因此无法消除原始数据集中的偏差。在我们的工作中，我们提出了一种新颖的两阶段流程来减少这个问题，其中1）我们在平衡的合成数据集上进行预训练，然后2）在真实数据上进行微调。使用这个流程，我们避免了在真实数据和合成数据上的训练，从而避免了真实数据和合成数据之间的偏差。此外，在第一步中我们学习到了抵抗偏差的稳健特征，在第二步中减轻了偏差。

    Visual recognition models are prone to learning spurious correlations induced by an imbalanced training set where certain groups (\eg Females) are under-represented in certain classes (\eg Programmers). Generative models offer a promising direction in mitigating this bias by generating synthetic data for the minority samples and thus balancing the training set. However, prior work that uses these approaches overlooks that visual recognition models could often learn to differentiate between real and synthetic images and thus fail to unlearn the bias in the original dataset. In our work, we propose a novel two-stage pipeline to mitigate this issue where 1) we pre-train a model on a balanced synthetic dataset and then 2) fine-tune on the real data. Using this pipeline, we avoid training on both real and synthetic data, thus avoiding the bias between real and synthetic data. Moreover, we learn robust features against the bias in the first step that mitigate the bias in the second step. Mor
    
[^74]: 仅使用自监督预训练改善噪声标签下的医学图像分类

    Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining. (arXiv:2308.04551v1 [eess.IV])

    [http://arxiv.org/abs/2308.04551](http://arxiv.org/abs/2308.04551)

    本研究旨在探索在噪声标签下仅使用自监督预训练方法，以改善医学图像分类。通过对比和预训练任务为基础的自监督方法，我们发现这些方法在医学图像上也具有改进学习效果的潜力。

    

    噪声标签会对基于深度学习的监督图像分类性能造成负面影响，因为模型可能会过度拟合噪声并学习到有损的特征提取器。使用对比自监督预训练权重来初始化模型已被证明可以减少特征损坏并提高分类性能。然而，目前尚未有研究探索1）其他自监督方法（如预训练任务）如何影响噪声标签下的学习，以及2）在噪声标签设置下，仅使用自监督预训练方法对医学图像的影响。医学图像往往具有较小的数据集和微小的类间变化，需要人类专业知识来保证正确分类。因此，尚不清楚改善噪声标签下自然图像数据集学习的方法是否也对医学图像有帮助。在本研究中，我们探索了对比和预训练任务为基础的自监督预训练方法。

    Noisy labels hurt deep learning-based supervised image classification performance as the models may overfit the noise and learn corrupted feature extractors. For natural image classification training with noisy labeled data, model initialization with contrastive self-supervised pretrained weights has shown to reduce feature corruption and improve classification performance. However, no works have explored: i) how other self-supervised approaches, such as pretext task-based pretraining, impact the learning with noisy label, and ii) any self-supervised pretraining methods alone for medical images in noisy label settings. Medical images often feature smaller datasets and subtle inter class variations, requiring human expertise to ensure correct classification. Thus, it is not clear if the methods improving learning with noisy labels in natural image datasets such as CIFAR would also help with medical images. In this work, we explore contrastive and pretext task-based self-supervised pretr
    
[^75]: 利用生物启发的架构提高连续学习任务的性能

    Improving Performance in Continual Learning Tasks using Bio-Inspired Architectures. (arXiv:2308.04539v1 [cs.LG])

    [http://arxiv.org/abs/2308.04539](http://arxiv.org/abs/2308.04539)

    本研究提出了一种受生物启发的轻量级神经网络架构，通过本地误差信号实现在线连续学习，克服了传统方法的局限性，在多个数据集上展现出优秀的表现。

    

    在设计智能系统中，从连续的数据流中无间断地学习而不会发生灾难性遗忘的能力至关重要。许多连续学习方法依赖于随机梯度下降及其变体，这些方法采用全局误差更新，因此需要采取策略，如内存缓冲区或回放，以规避其稳定性、贪婪和短期记忆的限制。为了解决这个问题，我们开发了一种受生物启发的轻量级神经网络架构，它包括突触可塑性机制和神经调节，并通过本地误差信号进行学习，以实现在线连续学习而无需随机梯度下降。我们的方法在Split-MNIST、Split-CIFAR-10和Split-CIFAR-100数据集上比其他内存受限的学习方法表现出更好的在线连续学习性能，并且与最先进的内存密集型回放方法相匹配。

    The ability to learn continuously from an incoming data stream without catastrophic forgetting is critical to designing intelligent systems. Many approaches to continual learning rely on stochastic gradient descent and its variants that employ global error updates, and hence need to adopt strategies such as memory buffers or replay to circumvent its stability, greed, and short-term memory limitations. To address this limitation, we have developed a biologically inspired lightweight neural network architecture that incorporates synaptic plasticity mechanisms and neuromodulation and hence learns through local error signals to enable online continual learning without stochastic gradient descent.  Our approach leads to superior online continual learning performance on Split-MNIST, Split-CIFAR-10, and Split-CIFAR-100 datasets compared to other memory-constrained learning approaches and matches that of the state-of-the-art memory-intensive replay-based approaches. We further demonstrate the 
    
[^76]: 我应该和谁合作? 关于NLP学术界与工业界合作的比较研究。

    Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP. (arXiv:2308.04524v1 [cs.DL])

    [http://arxiv.org/abs/2308.04524](http://arxiv.org/abs/2308.04524)

    本研究调查了学术界和工业界在自然语言处理（NLP）领域的合作对研究的影响。结果显示，学术界和工业界的合作出版物数量有增长趋势，并且这些出版物往往比仅由学术界产生的出版物具有更高的影响力。

    

    我们的研究目标是调查学术界和工业界在自然语言处理（NLP）领域的合作对研究的影响。为了实现这一目标，我们创建了一个从NLP论文中提取机构和引用信息的流程，并将其分为三类：学术界、工业界和混合型（学术界与工业界的合作）。我们的实证分析发现，工业界和学术界-工业界合作出版物的数量呈增长趋势，并且这些类型的出版物与仅在学术界产生的出版物相比，往往具有更高的影响力。

    The goal of our research was to investigate the effects of collaboration between academia and industry on Natural Language Processing (NLP). To do this, we created a pipeline to extract affiliations and citations from NLP papers and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry). Our empirical analysis found that there is a trend towards an increase in industry and academia-industry collaboration publications and that these types of publications tend to have a higher impact compared to those produced solely within academia.
    
[^77]: 深度学习在不同数据类型隐写分析中的应用：综述

    Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])

    [http://arxiv.org/abs/2308.04522](http://arxiv.org/abs/2308.04522)

    本综述论文详细综述了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的最新研究进展。

    

    隐写术和隐写分析是信息安全领域的两个相关方面。隐写术旨在隐藏通信，而隐写分析则旨在找到这些隐藏信息，甚至尝试恢复其所包含的数据。隐写术和隐写分析引起了广泛的关注，特别受到执法部门的关注。隐写术常被网络犯罪分子甚至恐怖分子用来避免在拥有证据时被捕，即使加密也一样，因为在许多国家禁止或限制使用密码学。因此，了解揭示隐藏信息的尖端技术对揭露非法行为至关重要。在过去几年中，文献中引入了许多强大可靠的隐写术和隐写分析技术。本综述论文提供了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的全面概述。

    Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
    
[^78]: MT-IceNet - 一种用于北极海冰预测的空间和多时序深度学习模型

    MT-IceNet -- A Spatial and Multi-Temporal Deep Learning Model for Arctic Sea Ice Forecasting. (arXiv:2308.04511v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.04511](http://arxiv.org/abs/2308.04511)

    提出了一种名为MT-IceNet的深度学习模型，用于预测北极海冰浓度。该模型采用了空间和多时序结构，利用编码器-解码器架构和跳跃连接处理多时序输入流，并可生成未来时间步骤的空间地图。

    

    北极放大已经改变了区域和全球的气候模式，导致过去几十年中更频繁和更强烈的极端天气事件。北极放大的关键部分是卫星观测所证实的前所未有的海冰消失。准确地从次季节到季节性尺度预测北极海冰一直是一个重要的研究问题，存在根本性挑战。除了基于物理学的地球系统模型，研究者一直应用多种统计和机器学习模型进行海冰预测。回顾数据驱动方法研究海冰变化的潜力，我们提出了MT-IceNet - 一种基于UNet的空间和多时序深度学习模型，用于预测北极海冰浓度（SIC）。该模型采用编码器-解码器架构，具有跳跃连接，并处理多时序输入流以在未来的时间步骤中重新生成空间地图。

    Arctic amplification has altered the climate patterns both regionally and globally, resulting in more frequent and more intense extreme weather events in the past few decades. The essential part of Arctic amplification is the unprecedented sea ice loss as demonstrated by satellite observations. Accurately forecasting Arctic sea ice from sub-seasonal to seasonal scales has been a major research question with fundamental challenges at play. In addition to physics-based Earth system models, researchers have been applying multiple statistical and machine learning models for sea ice forecasting. Looking at the potential of data-driven approaches to study sea ice variations, we propose MT-IceNet - a UNet based spatial and multi-temporal (MT) deep learning model for forecasting Arctic sea ice concentration (SIC). The model uses an encoder-decoder architecture with skip connections and processes multi-temporal input streams to regenerate spatial maps at future timesteps. Using bi-monthly and m
    
[^79]: 基于物理信息神经网络的压缩机叶栅流场调查

    Investigation of compressor cascade flow based on physics-informed neural networks. (arXiv:2308.04501v1 [cs.LG])

    [http://arxiv.org/abs/2308.04501](http://arxiv.org/abs/2308.04501)

    本研究首次使用物理信息神经网络（PINNs）方法来预测压缩机叶栅的流场，结果表明PINNs在预测精度和处理反向问题方面比传统CFD方法具有明显优势。

    

    在本研究中，我们首次使用新兴的物理信息神经网络（PINNs）方法来预测压缩机叶栅的流场。该方法在一个二维问题上进行演示，同时结合正向和反向问题中的Navier-Stokes方程。在正向问题中，PINNs能够有效预测压缩机的流场。与深度神经网络（DNNs）相比，PINNs模型融合了相关量之间的物理关系，从而得到更精确的预测结果。在没有部分边界条件的情况下，PINNs在处理反向问题时显示出显著优势。PINNs仅基于部分速度向量和壁压信息成功重构了压缩机叶栅的流场。该研究提供了有力的证据，证明PINNs为涡轮机械设计人员提供了一种有前景的替代方法，取代了当前主导的CFD方法。

    In this study, we utilize the emerging Physics Informed Neural Networks (PINNs) approach for the first time to predict the flow field of a compressor cascade. The approach is demonstrated on a two-dimensional problem, incorporating Navier-Stokes equations in both the forward and inverse problems. In the forward problem, PINNs effectively predict the flow field of the compressor. The key advantage over Deep Neural Networks (DNNs) is that the PINNs model incorporates a physical relationship between the relevant quantities, resulting in more precise predictions. PINNs show obvious advantages over the traditional CFD approaches when dealing with inverse problems in the absence of partial boundary conditions. PINNs successfully reconstruct the flow field of the compressor cascade solely based on partial velocity vectors and wall pressure information. This research provides compelling evidence that PINNs offer turbomachinery designers a promising alternative to the current dominant CFD metho
    
[^80]: 基于一元光子计算芯片和生成对抗学习的高效期权定价

    Efficient option pricing with unary-based photonic computing chip and generative adversarial learning. (arXiv:2308.04493v1 [quant-ph])

    [http://arxiv.org/abs/2308.04493](http://arxiv.org/abs/2308.04493)

    本研究提出了一种基于一元光子计算芯片和生成对抗学习的高效期权定价方法，通过引入量子幅度估计算法，相比传统方法实现了二次加速。其中，生成对抗网络用于高效学习和加载资产分布，准确捕捉市场趋势。这项工作为金融应用中的专用光子处理器发展提供了新的思路。

    

    在现代金融行业系统中，产品结构变得越来越复杂，传统计算能力的瓶颈已经制约了金融行业的发展。本文提出了一种实现欧式期权定价的光子芯片，结合量子幅度估计算法，与经典蒙特卡洛方法相比，实现了二次加速。电路包括三个模块：加载资产价格分布的模块、计算预期收益的模块以及执行量子幅度估计算法以引入加速的模块。在分布模块中，嵌入了生成对抗网络以便高效地学习和加载资产分布，准确捕捉市场趋势。这项工作是专用光子处理器在金融应用中发展的一个新进展，有潜力改进期权定价的效率。

    In the modern financial industry system, the structure of products has become more and more complex, and the bottleneck constraint of classical computing power has already restricted the development of the financial industry. Here, we present a photonic chip that implements the unary approach to European option pricing, in combination with the quantum amplitude estimation algorithm, to achieve a quadratic speedup compared to classical Monte Carlo methods. The circuit consists of three modules: a module loading the distribution of asset prices, a module computing the expected payoff, and a module performing the quantum amplitude estimation algorithm to introduce speed-ups. In the distribution module, a generative adversarial network is embedded for efficient learning and loading of asset distributions, which precisely capture the market trends. This work is a step forward in the development of specialized photonic processors for applications in finance, with the potential to improve the
    
[^81]: D-Score:一种受突触启发的用于过滤修剪的方法

    D-Score: A Synapse-Inspired Approach for Filter Pruning. (arXiv:2308.04470v1 [cs.NE])

    [http://arxiv.org/abs/2308.04470](http://arxiv.org/abs/2308.04470)

    本文通过受突触启发的方法，提出了一种用于过滤修剪的方法，通过分析滤波器中正负权重的独立重要性，并排名进行修剪，从而显著减少FLOPs和Params数量并保持准确率稳定。

    

    本文介绍了一种确定卷积神经网络(CNNs)中不重要过滤器排名的新方面。在人类突触系统中，有两个重要的通道被称为兴奋性和抑制性神经递质，它们将信号从神经元传递到细胞。采用神经科学的视角，我们提出了一种受突触启发的过滤修剪方法，称为动态评分(D-Score)。D-Score分析滤波器中正负权重的独立重要性，并通过分配分数对独立重要性进行排名。修剪具有低总分和对神经网络准确性影响较低的滤波器。在CIFAR-10和ImageNet数据集上的实验结果表明，我们提出的方法能够显著减少FLOPs和Params的数量，并且对准确率几乎没有影响。

    This paper introduces a new aspect for determining the rank of the unimportant filters for filter pruning on convolutional neural networks (CNNs). In the human synaptic system, there are two important channels known as excitatory and inhibitory neurotransmitters that transmit a signal from a neuron to a cell. Adopting the neuroscientific perspective, we propose a synapse-inspired filter pruning method, namely Dynamic Score (D-Score). D-Score analyzes the independent importance of positive and negative weights in the filters and ranks the independent importance by assigning scores. Filters having low overall scores, and thus low impact on the accuracy of neural networks are pruned. The experimental results on CIFAR-10 and ImageNet datasets demonstrate the effectiveness of our proposed method by reducing notable amounts of FLOPs and Params without significant Acc. Drop.
    
[^82]: 通过深度学习神经网络相关医疗保险索赔服务

    Correlating Medi- Claim Service by Deep Learning Neural Networks. (arXiv:2308.04469v1 [cs.LG])

    [http://arxiv.org/abs/2308.04469](http://arxiv.org/abs/2308.04469)

    本论文通过使用深度学习神经网络，结合回归模型的相关性研究，利用卷积神经网络架构来检测医疗保险索赔中的欺诈行为，并使用有监督和无监督分类器来区分欺诈和非欺诈索赔。

    

    医疗保险索赔中存在与患者、医生、诊断中心和保险提供商相关的有组织犯罪，形成了必须不断监测的连锁反应。这些欺诈行为影响被保险人和健康保险公司的财务增长。通过对回归模型的相关性研究，利用卷积神经网络架构来检测欺诈索赔，从而有助于检测来自不同提供商的不同索赔的洗钱行为。使用了有监督和无监督分类器来检测欺诈和非欺诈索赔。

    Medical insurance claims are of organized crimes related to patients, physicians, diagnostic centers, and insurance providers, forming a chain reaction that must be monitored constantly. These kinds of frauds affect the financial growth of both insured people and health insurance companies. The Convolution Neural Network architecture is used to detect fraudulent claims through a correlation study of regression models, which helps to detect money laundering on different claims given by different providers. Supervised and unsupervised classifiers are used to detect fraud and non-fraud claims.
    
[^83]: 后门联邦学习：通过污染后门关键层

    Backdoor Federated Learning by Poisoning Backdoor-Critical Layers. (arXiv:2308.04466v1 [cs.CR])

    [http://arxiv.org/abs/2308.04466](http://arxiv.org/abs/2308.04466)

    该论文研究了后门联邦学习中后门关键层的存在，并提出了一种针对这些层的新型后门攻击方法，旨在在各种防御策略下实现攻击效果和隐蔽性之间的平衡。

    

    联邦学习（FL）已被广泛应用于在分布式设备上进行敏感数据的机器学习训练。然而，分散式学习范式和FL的异质性进一步扩展了后门攻击的攻击面。现有的FL攻击和防御方法通常会关注整个模型，但没有一个方法意识到后门关键（BC）层的存在，后门关键层是指控制模型漏洞的一小部分层。攻击BC层可以达到攻击整个模型的效果，但被最先进的防御手段发现的机会要小得多。本文提出了一个从攻击者的角度识别和验证BC层的普适性方法。基于识别出的BC层，我们精心设计了一种新的后门攻击方法，根据不同的防御策略自适应地寻求攻击效果和隐蔽性之间的平衡。大量实验表明，

    Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that 
    
[^84]: 通过基于强化学习的肌肉控制器对人类平衡进行特征化

    Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller. (arXiv:2308.04462v1 [cs.LG])

    [http://arxiv.org/abs/2308.04462](http://arxiv.org/abs/2308.04462)

    本研究通过基于强化学习的肌肉控制器，探索了利用质心状态空间监测人类平衡能力的可行性。

    

    在身体康复期间，平衡评估常常依赖于评分指标，从而导致主观性。虽然存在一些客观的平衡评估方法，但通常仅限于跟踪压力中心(COP)，无法完全捕捉全身姿势稳定性。本研究探讨了利用质心(COM)状态空间，并提出了一种有望用于监测人类平衡能力的方法。我们采用骨骼肌肉模型与平衡控制器相结合，通过强化学习(RL)进行训练，研究平衡能力。RL框架由两个相互连接的神经网络组成，分别控制平衡恢复和肌肉协调，使用近端策略优化(PPO)进行训练，包括参考状态初始化、早停和多训练策略。通过在随机初始COM状态(位置和速度)空间中恢复平衡。

    Balance assessment during physical rehabilitation often relies on rubric-oriented battery tests to score a patient's physical capabilities, leading to subjectivity. While some objective balance assessments exist, they are often limited to tracking the center of pressure (COP), which does not fully capture the whole-body postural stability. This study explores the use of the center of mass (COM) state space and presents a promising avenue for monitoring the balance capabilities in humans. We employ a musculoskeletal model integrated with a balance controller, trained through reinforcement learning (RL), to investigate balancing capabilities. The RL framework consists of two interconnected neural networks governing balance recovery and muscle coordination respectively, trained using Proximal Policy Optimization (PPO) with reference state initialization, early termination, and multiple training strategies. By exploring recovery from random initial COM states (position and velocity) space 
    
[^85]: Pangu天气预报模型与气象运行数据的兼容性

    The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data. (arXiv:2308.04460v1 [cs.LG])

    [http://arxiv.org/abs/2308.04460](http://arxiv.org/abs/2308.04460)

    Pangu天气预报模型与不同的数值模拟预报系统的模型初始条件兼容，并且具有相对稳定的预测能力。

    

    最近，基于机器学习的多个数据驱动模型用于天气预报。相比传统的数值模拟天气预报系统，这些模型在准确性上具有很高的竞争力。特别是Pangu天气模型是开源的非商业用途，已经通过欧洲中期天气预报中心（ECMWF）的验证其预测性能，并最近在《自然》杂志上发表。在本文中，我们通过案例研究评估了Pangu天气模型与几种常用的数值模拟天气预报运行分析的兼容性。结果表明，Pangu天气模型与不同的数值模拟预报系统的模型初始条件兼容，并且具有相对稳定的预测能力。此外，我们验证了改进全球或本地初始条件的质量对于增强预测能力的重要性。

    Recently, multiple data-driven models based on machine learning for weather forecasting have emerged. These models are highly competitive in terms of accuracy compared to traditional numerical weather prediction (NWP) systems. In particular, the Pangu-Weather model, which is open source for non-commercial use, has been validated for its forecasting performance by the European Centre for Medium-Range Weather Forecasts (ECMWF) and has recently been published in the journal "Nature". In this paper, we evaluate the compatibility of the Pangu-Weather model with several commonly used NWP operational analyses through case studies. The results indicate that the Pangu-Weather model is compatible with different operational analyses from various NWP systems as the model initial conditions, and it exhibits a relatively stable forecasting capability. Furthermore, we have verified that improving the quality of global or local initial conditions significantly contributes to enhancing the forecasting 
    
[^86]: MCTS引导的遗传算法用于神经网络权重优化

    MCTS guided Genetic Algorithm for optimization of neural network weights. (arXiv:2308.04459v1 [cs.NE])

    [http://arxiv.org/abs/2308.04459](http://arxiv.org/abs/2308.04459)

    本文研究了将MCTS引导的遗传算法应用于神经网络权重优化的可能性，并提出了一种有效的树搜索策略，用于探索整个基因树结构，以最优方式搜索使用遗传算法产生的最佳结果。

    

    在这项研究中，我们探讨了将搜索策略应用于遗传算法来探索整个基因树结构的可能性。虽然有很多方法可以帮助进行树搜索，但是简单的算法如广度优先、深度优先和迭代技术计算复杂度较高，常常导致执行时间较长。当进行概率搜索时，对手算法往往是首选机制，能够更快地得到最优结果。本文要解决的问题是利用遗传算法优化神经网络。遗传算法（GA）形成了一个可能状态的树，并通过适应度函数提供奖励机制。蒙特卡洛树搜索（MCTS）已经被证明是一种有效的树搜索策略，因此我们将结合这些方法来最优地搜索使用遗传算法产生的最佳结果。

    In this research, we investigate the possibility of applying a search strategy to genetic algorithms to explore the entire genetic tree structure. Several methods aid in performing tree searches; however, simpler algorithms such as breadth-first, depth-first, and iterative techniques are computation-heavy and often result in a long execution time. Adversarial techniques are often the preferred mechanism when performing a probabilistic search, yielding optimal results more quickly. The problem we are trying to tackle in this paper is the optimization of neural networks using genetic algorithms. Genetic algorithms (GA) form a tree of possible states and provide a mechanism for rewards via the fitness function. Monte Carlo Tree Search (MCTS) has proven to be an effective tree search strategy given states and rewards; therefore, we will combine these approaches to optimally search for the best result generated with genetic algorithms.
    
[^87]: 对地下能源系统中物理驱动机器学习应用的批判性评论

    A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems. (arXiv:2308.04457v1 [cs.LG])

    [http://arxiv.org/abs/2308.04457](http://arxiv.org/abs/2308.04457)

    本文批判性地审查了物理驱动的机器学习在地下能源系统中的应用，强调了PIML在地震应用、油藏模拟和油气生产等任务中的成功利用。

    

    机器学习已经成为各个领域的强大工具，包括计算机视觉、自然语言处理和语音识别。它可以揭示大数据集中隐藏的模式，并揭示无与伦比的洞察力，从而革新许多行业和学科。然而，机器和深度学习模型缺乏可解释性和有限的领域专业知识，特别是在物理和工程等应用中。相反，物理驱动的机器学习（PIML）技术将物理原理融入到数据驱动模型中。通过将深度学习与领域知识相结合，PIML提高了模型的泛化能力，遵守治理物理定律，以及可解释性。本文全面回顾了与地下能源系统相关的PIML应用，主要集中在石油和天然气行业。回顾突出了PIML在地震应用、油藏模拟和油气生产等任务中的成功利用。

    Machine learning has emerged as a powerful tool in various fields, including computer vision, natural language processing, and speech recognition. It can unravel hidden patterns within large data sets and reveal unparalleled insights, revolutionizing many industries and disciplines. However, machine and deep learning models lack interpretability and limited domain-specific knowledge, especially in applications such as physics and engineering. Alternatively, physics-informed machine learning (PIML) techniques integrate physics principles into data-driven models. By combining deep learning with domain knowledge, PIML improves the generalization of the model, abidance by the governing physical laws, and interpretability. This paper comprehensively reviews PIML applications related to subsurface energy systems, mainly in the oil and gas industry. The review highlights the successful utilization of PIML for tasks such as seismic applications, reservoir simulation, hydrocarbons production fo
    
[^88]: 使用深度学习对金属-绝缘体-金属介质布面的高精度预测

    High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning. (arXiv:2308.04450v1 [cs.LG])

    [http://arxiv.org/abs/2308.04450](http://arxiv.org/abs/2308.04450)

    本研究提出使用ResNets-10模型对金属-绝缘体-金属介质布面进行高精度预测。通过两阶段训练和小学习率进行训练，实现了超低误差值的预测结果。该网络可以取代传统的电磁计算方法，并可大幅降低设计过程时间。

    

    近年来，深度学习在电磁软件计算结果的预测方面一直是一个广泛讨论的问题。但预测准确率仍然是需要解决的挑战之一。在这项工作中，我们提出使用ResNets-10模型来预测等离子元布面的S11参数。通过k折交叉验证和小学习率进行两阶段训练。训练完成后，对于铝、金和银金属-绝缘体-金属布面的预测损失分别为-48.45、-46.47和-35.54。由于误差值极低，所提出的网络可以在一定结构范围内取代传统的电磁计算方法进行计算。此外，该网络的训练过程可以在不到1100个周期内完成。这意味着网络训练过程可以有效降低设计过程的时间。我们提出的ResNets-10模型还可以用于设计元-衍射装置和生物技术。

    Deep learning prediction of electromagnetic software calculation results has been a widely discussed issue in recent years. But the prediction accuracy was still one of the challenges to be solved. In this work, we proposed that the ResNets-10 model was used for predicting plasmonic metasurface S11 parameters. The two-stage training was performed by the k-fold cross-validation and small learning rate. After the training was completed, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace the traditional electromagnetic computing method for calculation within a certain structural range. Besides, this network can finish the training process less than 1,100 epochs. This means that the network training process can effectively lower the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and bios
    
[^89]: 双重治理：集中监管与众包安全机制在生成AI中的交集

    Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI. (arXiv:2308.04448v1 [cs.CY])

    [http://arxiv.org/abs/2308.04448](http://arxiv.org/abs/2308.04448)

    本文讨论了生成AI领域的治理问题，提出了将集中监管与众包安全机制相结合的双重治理方式。集中监管存在缺乏明确性和统一性等问题，而众包安全机制则存在缺乏统一性和合规性的不足。该研究旨在促进生成AI领域的负责任和道德发展。

    

    生成人工智能（AI）最近在消费者面向的、开放式的文本和图像生成模型方面得到了广泛应用。然而，使用这样的系统引发了重要的伦理和安全问题，包括隐私侵犯、虚假信息和知识产权盗窃。生成AI可能取代人类的创造力和谋生方式也受到了严格审查。为了缓解这些风险，在生成AI领域需要一种负责任和道德的发展政策和监管机制。现有和提议的政府集中监管AI的规定面临诸多批评，例如缺乏足够的明确性和统一性，在不同司法辖区之间缺乏互操作性，限制创新，阻碍自由市场竞争。分散的众包安全工具和机制是一个潜在的替代方案。然而，它们在缺乏统一性、合规性和透明性方面存在明显的不足。

    Generative Artificial Intelligence (AI) has seen mainstream adoption lately, especially in the form of consumer-facing, open-ended, text and image generating models. However, the use of such systems raises significant ethical and safety concerns, including privacy violations, misinformation and intellectual property theft. The potential for generative AI to displace human creativity and livelihoods has also been under intense scrutiny. To mitigate these risks, there is an urgent need of policies and regulations responsible and ethical development in the field of generative AI. Existing and proposed centralized regulations by governments to rein in AI face criticisms such as not having sufficient clarity or uniformity, lack of interoperability across lines of jurisdictions, restricting innovation, and hindering free market competition. Decentralized protections via crowdsourced safety tools and mechanisms are a potential alternative. However, they have clear deficiencies in terms of lac
    
[^90]: 从生成式AI走向可信赖的AI：LLM可以从Cyc中学到什么

    Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc. (arXiv:2308.04445v1 [cs.LG])

    [http://arxiv.org/abs/2308.04445](http://arxiv.org/abs/2308.04445)

    对于未来的AI，需要从生成式AI转向可信赖的AI。通过培养基于明确知识和经验规则的AI，可以解决当前方法的限制并实现推理过程的可信赖和可解释性。

    

    生成式AI是目前最流行的人工智能方法，由训练出的大型语言模型（LLMs）组成，用于生成可信，但不一定正确的输出。尽管它们的能力常常令人惊叹，但在推理方面它们存在缺陷，导致LLMs不完全可信赖。此外，它们的结果往往不可预测和不可解释。我们提出了未来人工智能的16个期望，并讨论了AI的另一种可能解决当前方法所面临的许多限制的方法：培养基于明确知识和经验规则的AI，使推理引擎能够自动推导出所有知识的逻辑蕴含。即使是通过这种方式产生的长论证也可以是可信且可解释的，因为完整的逐步推理过程始终可以获得，并且可以记录和审计每个步骤使用的知识的来源。

    Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable.  We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however
    
[^91]: 德国推特上COVID疫情期间政策偏好的变化

    Changes in Policy Preferences in German Tweets during the COVID Pandemic. (arXiv:2308.04444v1 [cs.CY])

    [http://arxiv.org/abs/2308.04444](http://arxiv.org/abs/2308.04444)

    这项研究提供了一种量化德国推特上COVID疫情期间政策偏好的方法，通过建立细粒度政治偏好的数据集，并使用文本分类模型进行分析，结果显示政治观点在疫情期间有所增加，研究还突出了不同政治类别的变化。

    

    在线社交媒体已成为交流政治观点的重要论坛。针对COVID措施，公民直接在这些平台上表达了自己的政策偏好。在在线社交媒体中量化政治偏好仍然具有挑战性：大量的内容需要可扩展的自动提取政治偏好--然而，由于缺乏数据集，使用当前的机器学习（ML）技术进行细粒度的政治偏好提取是困难的。在这里，我们提供了一个带有细粒度政治偏好注释的推文数据集。使用训练在这个数据上的文本分类模型，我们提取了从2019年到2022年的德国Twitter语料库中的政策偏好。我们的结果表明，对于COVID疫情，政治观点的表达增加了。我们使用一个成熟的政策偏好分类法分析了细粒度的政治观点，并突出了不同政治类别的变化。

    Online social media have become an important forum for exchanging political opinions. In response to COVID measures citizens expressed their policy preferences directly on these platforms. Quantifying political preferences in online social media remains challenging: The vast amount of content requires scalable automated extraction of political preferences -- however fine grained political preference extraction is difficult with current machine learning (ML) technology, due to the lack of data sets. Here we present a novel data set of tweets with fine grained political preference annotations. A text classification model trained on this data is used to extract policy preferences in a German Twitter corpus ranging from 2019 to 2022. Our results indicate that in response to the COVID pandemic, expression of political opinions increased. Using a well established taxonomy of policy preferences we analyse fine grained political views and highlight changes in distinct political categories. The
    
[^92]: 企业协作系统的事件抽象以支持社会流程挖掘

    Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining. (arXiv:2308.04396v1 [cs.LG])

    [http://arxiv.org/abs/2308.04396](http://arxiv.org/abs/2308.04396)

    本论文提出了一种定制的企业协作系统的事件抽象方法，通过比较实际用户活动和系统生成的低级别跟踪来训练模型，并将低级别跟踪转换为抽象的高级别日志，以支持社会流程挖掘。

    

    流程挖掘的一个目标是从信息系统的事件日志中发现流程模型。流程挖掘已成功应用于面向流程的企业系统，但对于面向通信和文档的企业协作系统（ECS）来说不太适用。ECS事件日志非常细粒度，对其日志应用流程挖掘会导致混乱的模型。一个常见的解决方案是事件抽象，即在运行发现算法之前将低级别日志转换为更抽象的高级别日志。迄今为止，既有的事件抽象方法尚未完全解决ECS日志的特殊特征。我们旨在通过定制的ECS事件抽象（ECSEA）方法来弥补这一差距，该方法通过比较记录的实际用户活动（高级别跟踪）与系统生成的低级别跟踪（从ECS中提取）来训练模型。该模型使我们能够自动将未来的低级别跟踪转换为抽象的高级别日志。

    One aim of Process Mining (PM) is the discovery of process models from event logs of information systems. PM has been successfully applied to process-oriented enterprise systems but is less suited for communication- and document-oriented Enterprise Collaboration Systems (ECS). ECS event logs are very fine-granular and PM applied to their logs results in spaghetti models. A common solution for this is event abstraction, i.e., converting low-level logs into more abstract high-level logs before running discovery algorithms. ECS logs come with special characteristics that have so far not been fully addressed by existing event abstraction approaches. We aim to close this gap with a tailored ECS event abstraction (ECSEA) approach that trains a model by comparing recorded actual user activities (high-level traces) with the system-generated low-level traces (extracted from the ECS). The model allows us to automatically convert future low-level traces into an abstracted high-level log that can 
    
[^93]: SLEM：机器学习用于路径建模和因果推断的超级学习者方程模型

    SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])

    [http://arxiv.org/abs/2308.04365](http://arxiv.org/abs/2308.04365)

    SLEM是一种路径建模技术，通过集成机器学习超级学习者，实现了一致且无偏的因果效应估计，并在处理非线性关系时超过了传统的结构方程模型。

    

    因果推断是科学的关键目标，使研究人员能够通过观察数据得出关于对假定干预的预测的有意义的结论。路径模型、结构方程模型(SEMs)以及更一般的有向无环图(DAGs)能够明确地指定关于现象背后的因果结构的假设。与DAGs不同，SEMs假设线性关系，这可能导致函数错误规范，从而阻碍研究人员进行可靠的效果大小估计。相反，我们提出了超级学习者方程模型（SLEM），一种集成了机器学习超级学习者集成的路径建模技术。我们通过实证研究，证明了SLEM能够提供一致且无偏的因果效应估计，在与SEMs进行线性模型比较时表现出竞争力，并且在处理非线性关系时优于SEMs。

    Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
    
[^94]: OpinionConv: 通过基于真实主观体验的观点实现对话式产品搜索

    OpinionConv: Conversational Product Search with Grounded Opinions. (arXiv:2308.04226v1 [cs.HC])

    [http://arxiv.org/abs/2308.04226](http://arxiv.org/abs/2308.04226)

    OpinionConv是第一个用于模拟销售对话的对话式AI，通过利用产品评论作为观点的丰富来源，实现了对话和决策中的真实性和信息基础。

    

    在搜索产品时，他人的观点在做出明智决策方面起着重要作用。对产品的主观体验可以是有价值的信息来源。这在销售对话中也是如此，在这种对话中，客户和销售助手交换有关产品的事实和观点。然而，训练一个用于此类对话的AI是复杂的，因为语言模型由于缺乏真实世界的经验没有真实的观点。我们通过利用产品评论作为产品观点的丰富来源来解决这个问题，以真实主观叙述支持对话式AI。通过OpinionConv，我们开发了第一个模拟销售对话的对话式AI。为了验证生成的对话，我们进行了多个用户研究，结果显示生成的观点被认为是真实的。我们的评估员也确认了观点对于决策的信息基础的重要性。

    When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
    
[^95]: 使用结构化背景知识和演绎推理理解CNN隐藏神经元激活

    Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v1 [cs.LG])

    [http://arxiv.org/abs/2308.03999](http://arxiv.org/abs/2308.03999)

    本文提供了一种使用结构化背景知识和演绎推理的方法，用于解释CNN隐藏神经元的激活。该方法能够提供有意义的解释，解决了深度学习系统黑盒特性的问题。

    

    Explainable AI中的一个主要挑战是准确解释隐藏神经元的激活：准确的解释将为深度学习系统内部检测到的输入相关内容提供洞察力，揭示深度学习系统的黑盒特性。现有技术表明，在某些情况下，隐藏节点的激活可以被人类理解，但是对隐藏神经元激活的解释进行假设和验证的系统化自动化方法尚未充分研究。在本文中，我们提供了这样一种方法，并证明它提供了有意义的解释。我们的方法基于使用大规模的背景知识，从维基百科概念层次结构中筛选出的约200万个类别，以及一个称为概念归纳的符号推理方法，这种方法最初是为语义Web领域的应用而开发的。

    A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, de-mystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Ou
    
[^96]: 用于具有大动作空间的离策略评估的双重稳健估计器

    Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces. (arXiv:2308.03443v1 [stat.ML])

    [http://arxiv.org/abs/2308.03443](http://arxiv.org/abs/2308.03443)

    本文提出了一种用于具有大动作空间的离策略评估的双重稳健估计器（MDR）。与现有的基准估计器相比，MDR能够在减小方差的同时保持无偏性，从而提高了估计的准确性。实验结果证实了MDR相对于现有估计器的优越性。

    

    本文研究了在具有大动作空间的背景下的离策略评估（OPE）。现有的基准估计器存在严重的偏差和方差折衷问题。参数化方法由于很难确定正确的模型而导致偏差，而重要性加权方法由于方差而产生问题。为了克服这些限制，本文提出了基于判别式的不良行为抑制器（MIPS）来通过对动作的嵌入来减小估计器的方差。为了使估计器更准确，我们提出了MIPS的双重稳健估计器——边际化双重稳健（MDR）估计器。理论分析表明，所提出的估计器在比MIPS更弱的假设下是无偏的，同时保持了对IPS的方差减小，这是MIPS的主要优势。经验实验证实了MDR相对于现有估计器的优越性。

    We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
    
[^97]: 本地一致增强的Siamese网络与循环损失用于双视图对应学习

    Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning. (arXiv:2308.03217v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2308.03217](http://arxiv.org/abs/2308.03217)

    本文提出了一种本地一致增强的Siamese网络用于双视图对应学习，通过增强特征的一致性强化内点相关性、抑制异常点的干扰，并使用正向和反向映射的信息来监督网络训练。

    

    最近的双视图对应学习研究通常建立一个端到端网络来共同预测对应可靠性和相对姿态。我们从两个方面改进了这样一个框架。首先，我们提出了一个本地特征一致性（LFC）插件块来增强现有模型的特征。给定一个对应特征，该块通过相互邻域一致性增强其相邻特征，并将它们聚合以产生增强特征。由于内点服从统一的跨视图变换，并且共享比异常点更一致的学习特征，特征一致性增强了内点之间的相关性，并抑制了异常点的干扰，从而使输出特征更具辨别力，可以用于分类内点/异常点。其次，现有的方法在网络训练中使用地面真实对应和本质矩阵将一个图像投影到另一个图像的输入图像对，而不考虑反向映射的信息。我们扩展现有的途径来同时利用正向和反向映射的信息来监督网络训练。

    Recent studies of two-view correspondence learning usually establish an end-to-end network to jointly predict correspondence reliability and relative pose. We improve such a framework from two aspects. First, we propose a Local Feature Consensus (LFC) plugin block to augment the features of existing models. Given a correspondence feature, the block augments its neighboring features with mutual neighborhood consensus and aggregates them to produce an enhanced feature. As inliers obey a uniform cross-view transformation and share more consistent learned features than outliers, feature consensus strengthens inlier correlation and suppresses outlier distraction, which makes output features more discriminative for classifying inliers/outliers. Second, existing approaches supervise network training with the ground truth correspondences and essential matrix projecting one image to the other for an input image pair, without considering the information from the reverse mapping. We extend existi
    
[^98]: 不规则采样时间序列的时间参数化卷积神经网络

    Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series. (arXiv:2308.03210v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03210](http://arxiv.org/abs/2308.03210)

    本研究提出了时间参数化卷积神经网络（TPCNN），通过时间显式初始化的核函数来参数化卷积层，以处理不规则采样的多变量时间序列。这一方法增强了对连续时间隐藏动态的学习。

    

    不规则采样的多变量时间序列在许多应用领域中普遍存在，导致观测值稀疏、不完全且不对齐。标准的顺序神经网络架构，如循环神经网络（RNN）和卷积神经网络（CNN），考虑了观测时间之间的正则间隙，对不规则时间序列建模带来了重大挑战。虽然大多数提出的架构都采用RNN变体来处理不规则时间间隔，但卷积神经网络在不规则采样环境下的研究还不充分。在本文中，我们通过使用时间显式初始化的核函数来参数化卷积层。时间的这种通用函数增强了连续时间隐藏动态的学习过程，并且可以高效地合并到卷积核权重中。因此，我们提出了时间参数化卷积神经网络（TPCNN），它共享了相同的...

    Irregularly sampled multivariate time series are ubiquitous in several application domains, leading to sparse, not fully-observed and non-aligned observations across different variables. Standard sequential neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), consider regular spacing between observation times, posing significant challenges to irregular time series modeling. While most of the proposed architectures incorporate RNN variants to handle irregular time intervals, convolutional neural networks have not been adequately studied in the irregular sampling setting. In this paper, we parameterize convolutional layers by employing time-explicitly initialized kernels. Such general functions of time enhance the learning process of continuous-time hidden dynamics and can be efficiently incorporated into convolutional kernel weights. We, thus, propose the time-parameterized convolutional neural network (TPCNN), which shares sim
    
[^99]: 通过领域适应的最少到最多提示的方式实现文本到SQL的高效泛化

    Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])

    [http://arxiv.org/abs/2308.02582](http://arxiv.org/abs/2308.02582)

    该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。

    

    跨领域和跨组合式的文本到SQL语义解析的泛化是一项具有挑战性的任务。现有的基于大型语言模型（LLM）的解决方案依赖于从训练集中推理出少量样本，以合成每个自然语言（NL）测试查询的运行时提示。与此相反，我们设计了一种算法，该算法通过离线抽样从训练数据中获取少量样本，完全覆盖SQL子句、运算符和函数，并在允许的令牌长度范围内实现最大领域覆盖。这样可以合成一个固定的通用提示（GP），其中包含NL测试查询之间共用的多样化样本集，避免了昂贵的测试时间样本检索。我们还将GP自适应到目标数据库领域（DA-GP），以更好地处理跨领域泛化；然后采用分解的最少到最多提示（LTMP-DA-GP）来处理跨组合泛化。LTMP-DA-GP的合成是离线任务，

    Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
    
[^100]: AutoML4ETC: 自动化神经架构搜索实现现实世界加密流量分类

    AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])

    [http://arxiv.org/abs/2308.02182](http://arxiv.org/abs/2308.02182)

    AutoML4ETC是一个自动设计高效且高性能神经架构的工具，用于加密流量分类。其通过定义新颖的搜索空间和使用不同的搜索策略，在多个数据集上优于当前最先进的加密流量分类器。

    

    在实验环境中，深度学习（DL）已成功应用于加密网络流量分类。然而，在实际应用中，DL分类器的性能随时间不可避免地下降。仅仅对新数据集进行模型重新训练只能部分提高其性能。手动调整模型架构以满足新数据集上的性能期望耗时且需要领域专业知识。本文提出了一种新颖的工具AutoML4ETC，用于自动设计高效且高性能的神经架构以进行加密流量分类。我们定义了一个新颖而强大的搜索空间，专门针对使用数据包头字节进行近实时加密流量分类。通过在搜索空间上使用不同的搜索策略，我们展示了AutoML4ETC生成的神经架构在多个数据集上均优于当前最先进的加密流量分类器，包括公共基准数据集。

    Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark dat
    
[^101]: 一种具有连续对称性的新型卷积神经网络架构

    A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])

    [http://arxiv.org/abs/2308.01621](http://arxiv.org/abs/2308.01621)

    本文介绍了一种新的卷积神经网络架构，通过连续的对称性修改权重，具有与传统模型不同的特点，希望将对称性作为神经网络的新期望特性，并推广将偏微分方程的视角应用于ConvNet的分析和解释。

    

    本文介绍了一种新的卷积神经网络(ConvNet)架构，其灵感来自于一类称为拟线性双曲系统的偏微分方程(PDEs)。在图像分类任务上具有可比较的性能，它允许通过连续的对称性修改权重。这是与传统模型中基本固定的架构和权重相比的重大转变。我们希望将(内部)对称性作为一种新的神经网络期望特性，并在更广泛的深度学习社区中吸引对PDE视角分析和解释ConvNet的关注。

    This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
    
[^102]: 利用运动增量进行时空分支的运动预测

    Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])

    [http://arxiv.org/abs/2308.01097](http://arxiv.org/abs/2308.01097)

    本论文提出了一种利用运动增量进行时空分支的运动预测网络，通过解耦时域和空域特征的学习，提取更多的运动信息。

    

    人体运动预测已成为一个热门的研究课题，但由于未来姿势的随机和不规则性质，这仍然是一个具有挑战性的任务。传统方法依赖于手工特征和机器学习技术，往往难以建模人体运动的复杂动力学。最近基于深度学习的方法通过学习运动的时空表示取得了成功，但这些模型常常忽视运动数据的可靠性。此外，骨架节点的时域和空域依赖性是不同的。时域关系捕捉到随时间的运动信息，而空域关系描述了身体结构和不同节点之间的关系。在本文中，我们提出了一种新颖的利用增量信息进行时空分支的运动预测网络，它解耦了时域和空域特征的学习，提取了更多的运动信息。

    Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and ac
    
[^103]: 使用Floss增强周期性时间序列的表示学习：一种频域正则化方法

    Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])

    [http://arxiv.org/abs/2308.01011](http://arxiv.org/abs/2308.01011)

    本文提出了一种叫做Floss的无监督方法，通过在频域上对学到的表示进行正则化来增强周期性时间序列的表示学习。Floss方法可以自动检测时间序列中的周期性并学习具有周期一致性的有意义的表示。

    

    时间序列分析是各个应用领域的基础任务，深度学习方法在这个领域表现出了非凡的性能。然而，许多现实世界的时间序列数据展现出重要的周期性或准周期性动态，这些动态往往不能被现有的基于深度学习的解决方案充分捕捉到。这导致对感兴趣的基础动态行为的表示不完整。为了解决这个问题，我们提出了一种无监督的方法叫做Floss，它通过自动化地在频域上调整学到的表示来进行正则化。Floss方法首先自动检测时间序列中的主要周期性。然后，它利用周期移位和谱密度相似度度量来学习具有周期一致性的有意义的表示。此外，Floss可以轻松地整合到有监督、半监督和无监督的学习框架中。

    Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
    
[^104]: 有限分类模型的精确核等价性

    An Exact Kernel Equivalence for Finite Classification Models. (arXiv:2308.00824v1 [cs.LG])

    [http://arxiv.org/abs/2308.00824](http://arxiv.org/abs/2308.00824)

    本研究推导出梯度下降训练的有限分类模型的精确核表示，揭示了神经网络和核方法之间的等价性，并通过实验证明了精确核对神经网络预测的指导作用。

    

    本研究通过推导梯度下降训练的任意有限大小参数分类模型的精确表示，探索神经网络和核方法之间的等价性。我们将我们的精确表示与著名的神经切向核（NTK）进行比较，并讨论相对于NTK和其他非精确路径核公式的近似误差。我们通过实验证明，可以计算出逼真网络的核到机器精度。我们利用这个精确核来展示我们的理论贡献如何为神经网络的预测提供有用的见解，特别是它们的泛化方式。

    We explore the equivalence between neural networks and kernel methods by deriving the first exact representation of any finite-size parametric classification model trained with gradient descent as a kernel machine. We compare our exact representation to the well-known Neural Tangent Kernel (NTK) and discuss approximation error relative to the NTK and other non-exact path kernel formulations. We experimentally demonstrate that the kernel can be computed for realistic networks up to machine precision. We use this exact kernel to show that our theoretical contribution can provide useful insights into the predictions made by neural networks, particularly the way in which they generalize.
    
[^105]: ESP:利用对称性先验知识进行多Agent强化学习

    ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning. (arXiv:2307.16186v1 [cs.MA])

    [http://arxiv.org/abs/2307.16186](http://arxiv.org/abs/2307.16186)

    本文提出了一种利用对称性先验知识的框架来解决多Agent强化学习中的数据效率问题，通过将数据增强和一致性损失集成到现有方法中，能够提高模型训练效率，并且泛化性能良好。

    

    近年来，多Agent强化学习（MARL）取得了令人期待的成果。然而，大多数现有的强化学习方法需要大量的数据进行模型训练。此外，数据高效的强化学习要求构建强大的归纳偏差，在当前的MARL方法中被忽视了。受多Agent系统中对称现象的启发，本文提出了一种框架，通过将数据增强和精心设计的一致性损失集成到现有的MARL方法中，来利用先验知识。此外，所提出的框架是模型无关的，可以应用于大多数当前的MARL算法。对多个具有挑战性的任务进行了实验测试，证明了所提出的框架的有效性。此外，将所提出的框架应用于一个物理多机器人实验平台，展示了它的优越性。

    Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
    
[^106]: Auto-Tables: 无需使用示例合成多步转换以使表格关系化

    Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples. (arXiv:2307.14565v1 [cs.DB])

    [http://arxiv.org/abs/2307.14565](http://arxiv.org/abs/2307.14565)

    Auto-Tables系统能自动合成多步转换的流水线，将非关系式表格转换为关系式表格，解决了非技术用户使用SQL分析工具的痛点。

    

    关系表格在关系数据库中是标准的，每一行对应一个实体，每一列对应一个属性。然而，在处理"野生"表格时，这样的标准无法保证。我们调查了真实的电子表格和网页表格，发现超过30%的表格不符合关系标准，这就需要进行复杂的表格重构转换，才能轻松地使用基于SQL的分析工具进行查询。然而，所需的转换编程并不简单，这已经成为技术和非技术用户的重要问题，从StackOverflow和Excel/Tableau论坛的大量问题可以证明。我们开发了一个Auto-Tables系统，可以自动合成具有多步转换（使用Python或其他语言）的流水线，将非关系型表转换为标准的关系形式。

    Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Tableau forums.  We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms fo
    
[^107]: 用度量的Dikin步骤有效地抽样PSD锥体

    Efficiently Sampling the PSD Cone with the Metric Dikin Walk. (arXiv:2307.12943v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2307.12943](http://arxiv.org/abs/2307.12943)

    本文通过对Dikin步行方法进行分析并适应一般度量，为带约束的PSD锥体抽样问题提供了一种有效的解决方案，并提出了优化的自共轭矩阵函数概念。

    

    半定规划代表了高效计算的前沿。尽管在半定最优化上已取得了很大进展，如今内点法能够在实践中解决中等规模的问题，但是抽样半定解的基本问题仍然是一个巨大的挑战。直接应用已知的多项式时间算法来抽样一般凸体的方法导致运行时间过长。此外，已知的通用方法需要昂贵的舍入阶段作为预处理。本文分析了Dikin步行，并首先将其适应于一般度量，然后为带有仿射约束的PSD锥体设计合适的度量。所得到的混合时间和每步复杂度相当小，并且通过适当选择度量，可以使其对约束的依赖关系变为多对数级的。我们介绍了一个优化的自共轭矩阵函数概念，并给出了组合规则。

    Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining
    
[^108]: WEPRO: 用于混合量子-经典算法高效优化的权重预测方法

    WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms. (arXiv:2307.12449v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2307.12449](http://arxiv.org/abs/2307.12449)

    本研究提出了一种名为WEPRO的新方法，通过利用参数权重中的规律趋势加速了混合量子-经典算法的收敛速度，相比标准训练方法，速度提高约2.25倍，准确性提高了2.3%，损失降低了6.1%。

    

    量子模拟器在经典计算机上的指数级运行时间、真实量子设备的长队列深度和高成本给变分量子算法(VQA)如量子神经网络(QNNs)、变分量子本征求解器(VQE)和量子近似优化算法(QAOA)的有效训练带来了巨大挑战。为了解决这些限制，我们提出了一种新方法WEPRO(权重预测)，通过利用参数权重中的规律趋势来加快VQA的收敛速度。我们引入了两种优化预测性能的技术，即Naive Prediction(NaP)和Adaptive Prediction(AdaP)。通过对各种数据集上多个QNN模型的广泛实验和训练，我们证明WEPRO相对于标准训练方法加快了大约2.25倍的速度，同时在存储和计算开销较低的情况下提供了更高的准确性(高达2.3%)和更低的损失(高达6.1%)。

    The exponential run time of quantum simulators on classical machines and long queue depths and high costs of real quantum devices present significant challenges in the effective training of Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA). To address these limitations, we propose a new approach, WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance namely, Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately $2.25\times$ compared to standard training methods, while also providing improved accuracy (up to $2.3\%$ higher) and loss (up to $6.1\%$ lower) with low storage and computational over
    
[^109]: 用物理信知的神经网络解决维度诅咒问题

    Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])

    [http://arxiv.org/abs/2307.12306](http://arxiv.org/abs/2307.12306)

    本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。

    

    维度诅咒(CoD)随着维度的增加，以指数级增长的计算成本来极度税费计算资源。这在解决高维偏微分方程(PDEs)中面临极大挑战，正如Richard Bellman在60年前首次指出的那样。尽管近年来在高维度上数值解决偏微分方程(PDEs)取得了一些成功，但这样的计算代价过高，而将一般非线性PDEs扩展到高维度从未实现过。本文提出了一种新方法，将物理信知的神经网络(PINNs)扩展到解决任意高维PDEs。该新方法称为随机维度梯度下降(SDGD)，将PDE的梯度分解为与不同维度对应的部分，并在训练PINNs的每次迭代中随机选择这些维度部分的子集进行采样。我们在理论上证明了所提出方法的收敛保证和其他期望属性。

    The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
    
[^110]: MSQNet: 无关演员的多模态动作识别

    MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])

    [http://arxiv.org/abs/2307.10763](http://arxiv.org/abs/2307.10763)

    MSQNet是一种无关演员的多模态多标签动作识别方法，通过使用视觉和文本模态来更好地表示动作类别，克服了现有方法中针对特定演员的限制。

    

    现有的动作识别方法通常是针对特定演员的，因为演员之间具有固有的拓扑和显着差异。这就需要特定演员的姿态估计（例如人类与动物），导致模型设计复杂性和高维护成本。此外，它们通常只关注学习视觉模态和单标签分类，忽视了其他可用信息源（例如类名文本）和多个动作的同时发生。为了克服这些限制，我们提出了一种新的方法，称为“无关演员的多模态多标签动作识别”，为包括人类和动物在内的各种类型的演员提供了统一的解决方案。我们进一步在基于Transformer的目标检测框架（例如DETR）中提出了一种新颖的多模态语义查询网络（MSQNet）模型，通过利用视觉和文本模态更好地表示动作类别。消除了演员特定性的限制。

    Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
    
[^111]: 一种风险厌恶策略梯度的方差替代：基尼离差

    An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])

    [http://arxiv.org/abs/2307.08873](http://arxiv.org/abs/2307.08873)

    本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。

    

    在风险厌恶的强化学习中，限制策略回报的方差是一种常见选择，因为它具有明确的数学定义和易于解释。传统方法直接限制总回报方差，而最近的方法通过限制每步奖励方差作为代理。本文彻底研究了这些基于方差的方法的局限性，如数字尺度的敏感性和阻碍策略学习，并提出使用替代风险衡量标准——基尼离差。我们研究了这种新风险衡量标准的各种属性，并导出了一种用于最小化基尼离差的策略梯度算法。在风险厌恶可以明确定义的领域进行实证评估时，我们的算法可以缓解基于方差的风险衡量标准的局限性，并在其他策略无法学到合理策略时实现高回报和低风险，以方差和基尼离差度量。

    Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
    
[^112]: Retentive Network: 作为大型语言模型的Transformer的继任者

    Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.08621](http://arxiv.org/abs/2307.08621)

    Retentive Network（RetNet）作为大型语言模型的基础架构，实现了训练并行、低成本推理和良好的性能。通过并行、循环和分块循环三种计算范式，RetNet具有训练并行化、低成本推理和高效的长序列建模的特点。

    

    在这项工作中，我们提出了Retentive Network (RetNet)作为大型语言模型的基础架构，同时实现了训练并行、低成本推理和良好的性能。我们从理论上推导出了循环和注意力之间的连接。然后，我们提出了序列建模的保留机制，支持三种计算范式，即并行、循环和分块循环。具体而言，并行表示允许进行训练并行化。循环表示能够实现低成本的$O(1)$推理，从而提高解码吞吐量、延迟和GPU内存，同时不损失性能。分块循环表示便于使用线性复杂度进行高效的长序列建模，其中每个块可以并行编码，同时进行循环摘要。语言建模实验结果表明，RetNet实现了良好的扩展结果、并行训练、低成本部署和高效的推理。

    In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient i
    
[^113]: INFLECT-DGNN: 动态图神经网络进行影响者预测

    INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2307.08131](http://arxiv.org/abs/2307.08131)

    INFLECT-DGNN是一个结合了图神经网络和递归神经网络的框架，使用加权损失函数、针对图数据适应的合成少数过采样技术和滚动窗口策略，用于影响者预测。实验结果显示，使用RNN来编码时间属性和GNN显著提高了预测性能。

    

    在许多领域中，利用网络信息进行预测建模已经变得非常普遍。在推荐和定向营销领域中，影响者检测是一个可以通过动态网络表示大大受益的领域，原因是不断发展的客户-品牌关系。为了阐述这一思想，我们引入了INFLECT-DGNN，这是一个使用加权损失函数的图神经网络（GNN）和递归神经网络（RNN），针对图数据适应的合成少数过采样技术（SMOTE）以及精心设计的滚动窗口策略的新框架。为了评估预测性能，我们利用一个包含三个城市网络的独特企业数据集，并制定了一个以利润为驱动的影响者预测评估方法。我们的结果表明，使用RNN来编码时间属性以及GNN显著改善了预测性能。

    Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive perform
    
[^114]: 一种新型的与平台无关的多模态深度学习模型，用于识别社交媒体上的促进饮食紊乱内容

    A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])

    [http://arxiv.org/abs/2307.06775](http://arxiv.org/abs/2307.06775)

    本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。

    

    在过去的十年中，饮食紊乱的诊断和与之相关的死亡数量大幅增加，尤其是在新冠疫情期间。这种巨大增长部分来源于疫情的压力，但也与社交媒体的暴露增加有关，社交媒体上充斥着促进饮食紊乱的内容。这些内容可以诱发观看者的饮食紊乱。本研究旨在创建一个多模态深度学习模型，能够基于视觉和文本数据的组合判断给定的社交媒体帖子是否促进饮食紊乱。从Twitter收集了一个带有标签的推文数据集，对其进行了十二个深度学习模型的训练和测试。根据模型的性能，最有效的深度学习模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的多模态融合模型，准确率和F1分数分别达到95.9%和0.959。RoBERTa和MaxViT融合模型可以有效地识别社交媒体上的促进饮食紊乱的内容。

    Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
    
[^115]: 使用大型语言模型实现无监督校准的文本分类方法的先验适应

    Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])

    [http://arxiv.org/abs/2307.06713](http://arxiv.org/abs/2307.06713)

    本文提出了一种使用大型语言模型进行文本分类的无监督校准方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行任务。

    

    当前有许多自然语言任务正在使用大规模语言模型（LLM）进行研究。这些模型通常通过大量无监督文本数据进行训练，并通过微调、校准或上下文学习等方法进行适应以执行下游自然语言任务。在本研究中，我们提出了一种方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行文本分类任务。该方法将LLM视为黑盒，在模型屏障中添加了一个阶段，用于校准模型后验以完成任务。结果表明，这些方法在不同数量的提示训练样本和无适应数据下的校准方法中优于未适应的模型。

    A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
    
[^116]: 一项关于时间序列的图神经网络综述：预测、分类、插值和异常检测

    A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])

    [http://arxiv.org/abs/2307.03759](http://arxiv.org/abs/2307.03759)

    这项综述介绍了图神经网络在时间序列分析中的应用，包括预测、分类、异常检测和插值。图神经网络能够显式地建模时间序列和变量之间的关系，为时间序列数据分析带来了新的方法和技术。

    

    时间序列是记录动态系统测量结果的主要数据类型，通过物理传感器和在线过程（虚拟传感器）生成大量数据。时间序列分析对于揭示可用数据中所蕴含的丰富信息至关重要。随着图神经网络（GNN）的最新进展，基于GNN的时间序列分析方法也大幅增加。这些方法能够显式地建模时间序列和变量之间的关系，而传统和其他深度神经网络方法则面临困难。在本综述中，我们提供了一份全面的基于图神经网络的时间序列分析综述（GNN4TS），包括四个基本维度：预测、分类、异常检测和插值。我们旨在指导设计师和实践者了解、构建应用和推动GNN4TS的研究。首先，我们提供了一个全面的面向任务的GNN4TS分类体系。接下来，我们展示了...

    Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present a
    
[^117]: 蒸馏修剪：使用合成数据赢得彩票的方法

    Distilled Pruning: Using Synthetic Data to Win the Lottery. (arXiv:2307.03364v1 [cs.LG])

    [http://arxiv.org/abs/2307.03364](http://arxiv.org/abs/2307.03364)

    该论文介绍了一种使用蒸馏数据来修剪深度学习模型的新方法，能够比传统方法更快地找到稀疏的可训练子网络，具有资源高效的神经网络修剪潜力。

    

    该论文介绍了一种通过使用蒸馏数据来修剪深度学习模型的新方法。与传统策略主要关注体系结构或算法优化不同，我们的方法重新考虑了数据在这些场景中的作用。蒸馏数据集捕捉了更大数据集中的重要模式，并且我们展示了如何利用这种能力来实现计算效率高的修剪过程。我们的方法可以在CIFAR-10上比迭代幅值修剪更快地找到稀疏的可训练子网络（也称为彩票票）。实验结果突显了使用蒸馏数据进行资源高效的神经网络修剪、模型压缩和神经网络架构搜索的潜力。

    This work introduces a novel approach to pruning deep learning models by using distilled data. Unlike conventional strategies which primarily focus on architectural or algorithmic optimization, our method reconsiders the role of data in these scenarios. Distilled datasets capture essential patterns from larger datasets, and we demonstrate how to leverage this capability to enable a computationally efficient pruning process. Our approach can find sparse, trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results highlight the potential of using distilled data for resource-efficient neural network pruning, model compression, and neural architecture search.
    
[^118]: 理解用于识别分子动力学的最新深度学习技术

    Understanding recent deep-learning techniques for identifying collective variables of molecular dynamics. (arXiv:2307.00365v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00365](http://arxiv.org/abs/2307.00365)

    这篇论文阐述了最近发展的深度学习方法在分子动力学中识别集体变量方面的应用。作者提供了两种不同的深度学习方法，一种基于微扰生成器和转移算符的主导特征函数，另一种通过学习自编码器来识别集体变量，并进行了比较性的数值研究。

    

    高维度的亚稳态分子系统常常可以通过系统的一些特征，即集体变量（CVs）来描述。由于机器学习和深度学习领域的快速发展，近年来已经开发出了各种基于深度学习的CV识别技术，可以对复杂分子系统进行准确建模和高效仿真。本文探讨了两种不同类别的基于深度学习的CV寻找方法，一种是通过计算与底层动力学相关的微扰生成器或转移算符的主导特征函数，另一种是通过学习自编码器，通过最小化重构误差来寻找集体变量。我们对这两种方法的数学原理做了简要概述，并在具体示例上进行了比较性的数值研究。

    High-dimensional metastable molecular system can often be characterised by a few features of the system, i.e. collective variables (CVs). Thanks to the rapid advance in the area of machine learning and deep learning, various deep learning-based CV identification techniques have been developed in recent years, allowing accurate modelling and efficient simulation of complex molecular systems. In this paper, we look at two different categories of deep learning-based approaches for finding CVs, either by computing leading eigenfunctions of infinitesimal generator or transfer operator associated to the underlying dynamics, or by learning an autoencoder via minimisation of reconstruction error. We present a concise overview of the mathematics behind these two approaches and conduct a comparative numerical study of these two approaches on illustrative examples.
    
[^119]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^120]: Diff-TTSG: 去噪概率集成语音和手势合成

    Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])

    [http://arxiv.org/abs/2306.09417](http://arxiv.org/abs/2306.09417)

    Diff-TTSG是第一个基于扩散的概率模型，用于联合学习合成语音和手势，相比于先前最新技术的非概率方法，它可以更好地捕捉人类讲话和运动的变化，产生更逼真和多样化的集成语音和手势合成。

    

    随着朗读语音合成实现高自然度评分，越来越多的研究开始关注合成自然言语。然而，人类面对面的自发对话既有口头的，也有非语言的（例如，共同言语手势）。最近才开始研究联合合成这两种模态在一个单一的系统中的好处。先前的最新技术使用非概率方法，无法捕捉人类讲话和运动的变化，并可能产生过度平滑的伪影和次优的合成质量。我们提出了第一个基于扩散的概率模型，称为 Diff-TTSG，共同学习合成语音和手势。我们的方法可以从头开始使用小型数据集进行训练。此外，我们描述了一组小心的单模态和多模态主观测试，用于评估集成语音和手势合成系统，并用它们来验证我们提出的方法。对于合成的样例而言，Diff-TTSG优于先前的最新技术，产生更逼真和多样化的集成语音和手势合成。

    With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
    
[^121]: GAD-NR: 通过邻域重构实现图形异常检测

    GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.01951](http://arxiv.org/abs/2306.01951)

    本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。

    

    图形异常检测（GAD）是一种技术，用于识别图中的异常节点，在网络安全、欺诈检测、社交媒体垃圾检测和其他各种领域中有应用。GAD的常见方法是图自编码器（GAEs），它将图形数据编码成节点表示，并根据这些表示来评估图形的重构质量，以识别异常。然而，现有的GAE模型主要针对直接链接重构进行优化，导致在潜在空间中连接图中的节点被聚类。因此，它们擅长检测聚类型结构异常，但对不符合聚类的更复杂的结构异常存在困难。为了解决这个限制，我们提出了一种新颖的解决方案，称为GAD-NR，它是GAE的一个新变体，融合邻域重构进行图形异常检测。GAD-NR的目标是重构节点的整个邻域，涵盖本地结构

    Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
    
[^122]: 脑肿瘤分割（BraTS）挑战赛2023：通过修复生成健康脑组织的局部合成。

    The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting. (arXiv:2305.08992v1 [eess.IV])

    [http://arxiv.org/abs/2305.08992](http://arxiv.org/abs/2305.08992)

    该论文介绍了BraTS 2023修复挑战，要求参与者使用修补技术从有病变的脑部扫描中合成健康脑扫描，以解决许多算法无法分析病变图像的问题。

    

    为了支持临床医生的决策，提供了许多自动分析脑部MR图像的算法。对于脑肿瘤患者，图像采集时间序列通常始于已经病理性的扫描。这会带来问题，因为许多算法是设计用于分析健康的大脑图像，并且没有为包含病变的图像提供保证。例如，进行脑部解剖分割、组织分割和脑部提取的算法。为解决这个问题，我们引入了BraTS 2023修复挑战。在这里，参与者需要探索修复技术，从有病变的扫描中合成健康的脑部扫描。下面的手稿包含了任务公式、数据集和提交程序。之后会更新以总结挑战的结果。这个挑战是作为BraTS 2023挑战的一部分，由加拿大温哥华MICCAI 2023会议主办。

    A myriad of algorithms for the automatic analysis of brain MR images is available to support clinicians in their decision-making. For brain tumor patients, the image acquisition time series typically starts with a scan that is already pathological. This poses problems, as many algorithms are designed to analyze healthy brains and provide no guarantees for images featuring lesions. Examples include but are not limited to algorithms for brain anatomy parcellation, tissue segmentation, and brain extraction. To solve this dilemma, we introduce the BraTS 2023 inpainting challenge. Here, the participants' task is to explore inpainting techniques to synthesize healthy brain scans from lesioned ones. The following manuscript contains the task formulation, dataset, and submission procedure. Later it will be updated to summarize the findings of the challenge. The challenge is organized as part of the BraTS 2023 challenge hosted at the MICCAI 2023 conference in Vancouver, Canada.
    
[^123]: DOCTOR：基于可穿戴医疗传感器的多疾病检测持续学习框架

    DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])

    [http://arxiv.org/abs/2305.05738](http://arxiv.org/abs/2305.05738)

    DOCTOR是一种基于可穿戴医疗传感器的多疾病检测持续学习框架，采用了多头深度神经网络和Exemplar-replay风格的CL算法。它可以不断地学习新任务，并在内存使用、电池消耗和检测复杂度方面优于传统的ML驱动疾病检测方法。

    

    现代机器学习（ML）和边缘设备中的可穿戴医疗传感器（WMS）的进步使得智能医疗的ML驱动疾病检测成为可能。传统的ML驱动疾病检测方法依赖于为每种疾病和相应的WMS数据定制个别模型。然而，这种方法缺乏对分布变化和新任务分类的适应性。同时，为了检测每个新疾病，需要从头开始重新构建和训练模型。针对这些挑战，我们提出了基于WMS的多疾病检测持续学习框架DOCTOR。它采用了多头深度神经网络（DNN）和一种Exemplar-replay风格的CL算法。CL算法使得框架能够不断地学习新任务，其中涉及不同的数据分布、分类类别和疾病检测任务。DOCTOR在使用来自实际WMS的公共数据集进行四种常见疾病检测方面取得了最先进的性能。同时，在内存使用、电池消耗和检测复杂度方面，DOCTOR也优于基线方法。

    Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
    
[^124]: AttentionViz：Transformer Attention的全局视图

    AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v1 [cs.HC])

    [http://arxiv.org/abs/2305.03210](http://arxiv.org/abs/2305.03210)

    这篇论文介绍了AttentionViz，一种以联合嵌入为基础的交互式可视化工具，用于帮助研究人员理解Transformer中的自我注意机制。该方法使得可以全局分析多个输入序列的注意力模式，提高对模型的理解并通过多个应用场景和专家反馈提供新的交互见解。

    

    Transformer模型正在革新机器学习，但它们的内部运作仍然神秘莫测。在本文中，我们提出了一种新的可视化技术，旨在帮助研究人员理解Transformer中的自我注意机制，使这些模型能够学习序列中元素之间丰富的上下文关系。我们方法的主要思想是可视化Transformer模型用于计算注意力的查询和键向量的联合嵌入。与以前的注意力可视化技术不同，我们的方法能够分析多个输入序列的全局模式。我们基于这些联合查询-键嵌入创建了一个交互式可视化工具AttentionViz，并将其用于研究语言和视觉变压器中的注意机制。通过几个应用场景和专家反馈，我们展示了我们的方法在提高模型理解和提供有关查询-键交互的新见解方面的实用性。

    Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz, based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.
    
[^125]: 特征工程能帮助量子机器学习进行恶意软件检测吗？

    Can Feature Engineering Help Quantum Machine Learning for Malware Detection?. (arXiv:2305.02396v1 [cs.LG])

    [http://arxiv.org/abs/2305.02396](http://arxiv.org/abs/2305.02396)

    本文通过量子机器学习与特征选择策略相结合的混合框架，以降低恶意软件分类器培训时间，初步结果表明在模拟器上可以达到78.91％的测试准确性。

    

    随着恶意软件攻击数量和复杂度的增加，基于机器学习（ML）的恶意软件检测系统变得越来越重要。同时，许多用于恶意软件分类的流行ML模型都是有监督学习。这些有监督分类器通常对新型恶意软件的推广效果不好。因此，需要经常重新训练它们以检测新的恶意软件样本，这可能非常耗时。本文通过理论量子ML与特征选择策略相结合的混合框架来解决这个问题，以降低数据大小和恶意软件分类器培训时间。初步结果表明，使用XGBoost选择的特征的VQC在模拟器上可以获得78.91％的测试准确性。使用XGBoost选择的特征训练的模型在IBM 5 qubits机器上的平均准确性为74％（+-11.35％）。

    With the increasing number and sophistication of malware attacks, malware detection systems based on machine learning (ML) grow in importance. At the same time, many popular ML models used in malware classification are supervised solutions. These supervised classifiers often do not generalize well to novel malware. Therefore, they need to be re-trained frequently to detect new malware specimens, which can be time-consuming. Our work addresses this problem in a hybrid framework of theoretical Quantum ML, combined with feature selection strategies to reduce the data size and malware classifier training time. The preliminary results show that VQC with XGBoost selected features can get a 78.91% test accuracy on the simulator. The average accuracy for the model trained using the features selected with XGBoost was 74% (+- 11.35%) on the IBM 5 qubits machines.
    
[^126]: 对称正定流形上低复杂度的子空间下降算法

    Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])

    [http://arxiv.org/abs/2305.02041](http://arxiv.org/abs/2305.02041)

    本文提出了一种基于黎曼子空间下降算法的对称正定流形上的函数最小化方法，其具有低复杂度和避免昂贵矩阵操作和计算黎曼梯度的优点。

    

    本文提出了一种低复杂度的黎曼子空间下降算法，用于在对称正定（SPD）流形上对函数进行最小化。与现有的黎曼梯度下降变体不同的是，所提出的方法利用 carefully chosen 的子空间，使得更新可以写成迭代的 Cholesky 因子和一个稀疏矩阵的乘积形式。由此产生的更新避免了昂贵的矩阵操作，如矩阵指数和密集矩阵乘法，这些操作通常在几乎所有其他 Riemannian 优化算法中都是必需的。

    This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
    
[^127]: 量子自然策略梯度：向样本高效增强学习迈进

    Quantum Natural Policy Gradients: Towards Sample-Efficient Reinforcement Learning. (arXiv:2304.13571v1 [quant-ph] CROSS LISTED)

    [http://arxiv.org/abs/2304.13571](http://arxiv.org/abs/2304.13571)

    本文提出了量子自然策略梯度(QNPG)算法，利用了量子费舍尔信息矩阵的高效近似方法，提高了强化学习的效率，实验结果表明，相比基于一阶梯度的训练，QNPG具有更快的收敛速度和稳定性，可以减少样本复杂度。

    

    强化学习是人工智能领域的一个快速发展的方向，但学习过程通常很耗费资源。使用变分量子电路作为函数逼近器可以减少成本，提高强化学习效率。本文中，我们提出了量子自然策略梯度(QNPG)算法，该算法利用了量子费舍尔信息矩阵的高效近似方法，是一种二阶梯度的基于策略的算法。在Contextual Bandits环境下的实验结果表明，QNPG 比基于一阶梯度的训练具有更快的收敛速度和稳定性，从而减少了样本复杂度，进一步展示了我们方法的实际可行性，并在12量子比特的硬件设备上进行了训练。

    Reinforcement learning is a growing field in AI with a lot of potential. Intelligent behavior is learned automatically through trial and error in interaction with the environment. However, this learning process is often costly. Using variational quantum circuits as function approximators can reduce this cost. In order to implement this, we propose the quantum natural policy gradient (QNPG) algorithm -- a second-order gradient-based routine that takes advantage of an efficient approximation of the quantum Fisher information matrix. We experimentally demonstrate that QNPG outperforms first-order based training on Contextual Bandits environments regarding convergence speed and stability and thereby reduces the sample complexity. Furthermore, we provide evidence for the practical feasibility of our approach by training on a 12-qubit hardware device.
    
[^128]: 用TiDE进行长期预测：时间序列稠密编码器

    Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])

    [http://arxiv.org/abs/2304.08424](http://arxiv.org/abs/2304.08424)

    TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。

    

    最近的研究表明，相比于基于Transformer的方法，简单的线性模型在长期时间序列预测中表现更好。鉴于此，我们提出了一种基于多层感知机(MLP)的编码器-解码器模型，即时间序列稠密编码器(TiDE)，用于长期时间序列预测。它既享有线性模型的简单性和速度，又能处理协变量和非线性依赖。从理论上讲，我们证明了我们模型的最简线性类比在一些假设下可以达到线性动态系统(LDS)的近乎最优误差率。实证上，我们表明，我们的方法可以在流行的长期时间序列预测基准测试中匹配或胜过以前的方法，同时比最佳的基于Transformer的模型快5-10倍。

    Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
    
[^129]: DiverseVul: 基于深度学习漏洞检测的新漏洞源代码数据集

    DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. (arXiv:2304.00409v1 [cs.CR])

    [http://arxiv.org/abs/2304.00409](http://arxiv.org/abs/2304.00409)

    这个论文提供了一个包含150个CWE的新的漏洞源代码数据集，覆盖比以前所有数据集加起来多305个项目。作者通过增加训练数据的多样性和数量改进了深度学习模型在漏洞检测方面的表现。通过结合已有数据集，作者还研究了使用深度学习检测软件漏洞的挑战和前途的分析，结果表明目前深度学习检测漏洞仍存在高误报率，低F1分数和难以检测严重CWE的问题。

    

    我们提出并发布了一个新的漏洞源代码数据集。我们通过爬取安全问题网站，提取相应项目的漏洞修复提交和源代码，筛选出了这个数据集。我们的新数据集包含了150个CWE，26,635个易受攻击的函数和352,606个不易受攻击的函数，提取自7,861个提交。我们的数据集覆盖了比以前所有数据集加起来多305个项目。我们展示了增加训练数据的多样性和数量可以提高深度学习模型在漏洞检测方面的表现。结合我们的新数据集和以前的数据集，我们提出了使用深度学习检测软件漏洞的挑战和有前途的研究方向的分析。我们研究了11个模型架构，属于4个家族。我们的结果表明，由于高误报率，低F1分数和难以检测严重CWE，深度学习仍未准备好用于漏洞检测。特别是，我们展示了......

    We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerable functions extracted from 7,861 commits. Our dataset covers 305 more projects than all previous datasets combined. We show that increasing the diversity and volume of training data improves the performance of deep learning models for vulnerability detection.  Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonst
    
[^130]: 关于初始化的影响：2层神经网络的缩放路径

    On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks. (arXiv:2303.17805v1 [cs.LG])

    [http://arxiv.org/abs/2303.17805](http://arxiv.org/abs/2303.17805)

    本文研究了具有不同尺度的非零权重的无限宽2层ReLU神经网络的正则化路径，展示该问题具有一个无限维度的凸对应，随着初始化的尺度在0到+∞范围内变化，关联路径在所谓的内核和丰富的区域之间连续插值。

    

    在监督学习中，正则化路径有时被用作梯度下降的优化路径的方便理论代理。本文研究了具有不同尺度的非零权重的无限宽2层ReLU神经网络的正则化路径的修改。通过利用与不平衡最优输运理论的联系，我们表明，尽管2层网络训练的非凸性，这个问题具有一个无限维度的凸对应。我们制定了相应的函数优化问题并调查了其主要特性。特别地，我们展示了随着初始化的尺度在0到+∞范围内变化，关联路径在所谓的内核和丰富的区域之间连续插值。数值实验证实，在我们的设置中，缩放路径和优化路径的最终状态行为类似，甚至超越了这些范围。

    In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized with zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with non-zero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite dimensional convex counterpart. We formulate the corresponding functional optimization problem and investigate its main properties. In particular, we show that as the scale of the initialization ranges between $0$ and $+\infty$, the associated path interpolates continuously between the so-called kernel and rich regimes. The numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly even beyond these ex
    
[^131]: MaMMUT: 一种用于多模态任务联合学习的简单架构

    MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])

    [http://arxiv.org/abs/2303.16839](http://arxiv.org/abs/2303.16839)

    提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。

    

    语言模型的发展已从编码-解码转向仅解码的设计。此外，普遍认为，最流行的两种多模态任务，生成任务和对比任务，往往互相冲突，难以在一个架构中容纳，并进一步需要用于下游任务的复杂调整。我们提出了一种新的培训范式，用于多模态任务的仅解码模型，这在联合学习这些不同的视觉语言任务方面非常有效。这是通过一个简单的模型MaMMUT实现的。它由单一的视觉编码器和一个文本解码器组成，并能够通过文本解码器上的新的两步方法容纳对比和生成学习。我们证明这些不同目标任务的联合训练是简单的，有效的，并最大化了模型的权重共享。此外，相同的架构使得对开放词汇对象检测的简单扩展成为可能。

    The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
    
[^132]: 基于深度学习的时间序列因果推断量化北极放大的原因

    Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.07122](http://arxiv.org/abs/2303.07122)

    该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。

    

    北极变暖，也称北极放大，由多种大气和海洋因素导致，但其基础热力因素的详细情况仍不清楚。使用固定治疗效应策略推断大气过程对海冰融化的因果效应会导致不现实的反事实估计。这样的模型也容易受到时间变化的混淆的影响而引起偏差。为了解决这些挑战，我们提出了TCINet - 一种基于循环神经网络的时间序列因果推断模型，以连续治疗方式推断因果关系。通过对合成和观测数据的实验，我们展示了我们的研究如何大大提高量化北极海冰融化的主要原因的能力。

    The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
    
[^133]: 大规模语言模型生成推理的成本效益超参数优化

    Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference. (arXiv:2303.04673v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04673](http://arxiv.org/abs/2303.04673)

    本文研究了优化大规模语言模型生成推理的成本效益超参数，通过经济的超参数优化和基于成本的修剪，提出了EcoOptiGen框架，该框架在使用GPT-3.5/GPT-4模型的任务中表现出有效性。

    

    大规模语言模型（LLM）在其生成能力方面引起了广泛关注，从而推动了各种商业应用的发展。使用这些模型的高成本驱使应用程序构建者在有限的推理预算下最大化生成价值。本文提出了一项关于优化推理超参数（如回复数量、温度和最大token数）的研究，这显著影响了文本生成的效用/成本。我们设计了一个名为EcoOptiGen的框架，它利用经济的超参数优化和基于成本的修剪。通过在各种任务上使用GPT-3.5/GPT-4模型进行实验，验证了其有效性。EcoOptiGen已在FLAML库的`autogen'包中实现：\url{https://aka.ms/autogen}。

    Large Language Models (LLMs) have sparked significant interest in their generative capabilities, leading to the development of various commercial applications. The high cost of using the models drives application builders to maximize the value of generation under a limited inference budget. This paper presents a study of optimizing inference hyperparameters such as the number of responses, temperature and max tokens, which significantly affects the utility/cost of text generation. We design a framework named EcoOptiGen which leverages economical hyperparameter optimization and cost-based pruning. Experiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its effectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML library: \url{https://aka.ms/autogen}.
    
[^134]: 将最大熵时刻法稳定应用于单精度下的稀疏气体动力学

    Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics at Single-Precision. (arXiv:2303.02898v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2303.02898](http://arxiv.org/abs/2303.02898)

    本文的创新在于将最大熵时刻法稳定应用于单精度下的稀疏气体动力学，使其能够在现代GPU上模拟非常强的正常激波。

    

    发展适用于密集和稀疏气体的扩展流体力学方程仍然是一个巨大的挑战。对于这个挑战的一个系统解决方案是利用气体分子速度分布的矩法描述密集和稀疏气体行为。在众多的矩法中，最大熵时刻法（MEM）因其具有良好的解决性和稳定性而突出，它利用了熵最大化的速度分布。然而，寻找这样的分布需要解决一个条件病态且计算需求较大的优化问题。这个问题在数值精度不足时会导致数值溢出和崩溃，尤其是对于高速激波等流动现象。它还阻碍了现代GPU利用其巨大的单精度计算能力来加速优化。本文旨在稳定MEM，使其在现代GPU上以单精度实用于模拟非常强的正常激波。

    Developing extended hydrodynamics equations valid for both dense and rarefied gases remains a great challenge. A systematical solution for this challenge is the moment method describing both dense and rarefied gas behaviors with moments of gas molecule velocity distributions. Among moment methods, the maximal entropy moment method (MEM) stands out for its well-posedness and stability, which utilizes velocity distributions with maximized entropy. However, finding such distributions requires solving an ill-conditioned and computation-demanding optimization problem. This problem causes numerical overflow and breakdown when the numerical precision is insufficient, especially for flows like high-speed shock waves. It also prevents modern GPUs from accelerating optimization with their enormous single floating-point precision computation power. This paper aims to stabilize MEM, making it practical for simulating very strong normal shock waves on modern GPUs at single precision. We propose the
    
[^135]: 连通性优化的嵌套图网络用于晶体结构

    Connectivity Optimized Nested Graph Networks for Crystal Structures. (arXiv:2302.14102v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14102](http://arxiv.org/abs/2302.14102)

    连通性优化和更深的消息函数在晶体结构研究中的嵌套图网络中能够显著提高计算效率和性能。

    

    图神经网络(GNNs)已应用于材料科学和化学领域的各种应用。在这里，我们重新总结了对结晶(周期性)材料的图构造，并研究其对GNN模型性能的影响。我们建议使用非对称单元格作为表示，通过利用系统的所有对称性来减少原子数量。这大大减少了计算成本，从而减少了训练大型图神经网络所需的时间，而不损失准确性。此外，通过基于消息传递和线图模板的简单但系统地构建GNN架构，我们介绍了一种通用架构(Nested Graph Network, NGN)，适用于各种任务。我们展示了我们建议的模型在MatBench基准测试的所有任务中系统地改善了最先进的结果。进一步的分析显示，优化的连接性和更深的消息函数是改进的原因。

    Graph neural networks (GNNs) have been applied to a large variety of applications in materials science and chemistry. Here, we recapitulate the graph construction for crystalline (periodic) materials and investigate its impact on the GNNs model performance. We suggest the asymmetric unit cell as a representation to reduce the number of atoms by using all symmetries of the system. This substantially reduced the computational cost and thus time needed to train large graph neural networks without any loss in accuracy. Furthermore, with a simple but systematically built GNN architecture based on message passing and line graph templates, we introduce a general architecture (Nested Graph Network, NGN) that is applicable to a wide range of tasks. We show that our suggested models systematically improve state-of-the-art results across all tasks within the MatBench benchmark. Further analysis shows that optimized connectivity and deeper message functions are responsible for the improvement. Asy
    
[^136]: 深度学习综述：从激活函数到Transformer

    A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00722](http://arxiv.org/abs/2302.00722)

    这篇综述调查了深度学习领域的重要进展，包括各种架构、层、目标和优化技术的发展，以及关注机制、归一化、跳跃连接、Transformer和自监督学习等方法的变体。总结了成功创新的关键策略，并讨论了最近的商业闭源模型。

    

    过去十年中，深度学习取得了显著的进展，得益于各种架构、层、目标和优化技术的涌现。这些包括关注机制、归一化、跳跃连接、Transformer和自监督学习等多种变体方法。我们的目标是向具有深度学习基本理解的人提供对这些领域中最新重要贡献的全面调查。我们的期望是通过对重要最新作品的综合和全面的探讨，促进不同深度学习领域之间形成新的联系。在我们的讨论中，我们总结了过去十年中许多成功创新的关键策略。我们还对最近一些商业闭源模型进行了讨论，例如OpenAI的GPT-4和Google的PaLM 2。

    The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.
    
[^137]: 深度学习中奇异值分解（SVD）作为可解释的因子化的出现在逆问题中

    Emergence of the SVD as an interpretable factorization in deep learning for inverse problems. (arXiv:2301.07820v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07820](http://arxiv.org/abs/2301.07820)

    深度学习中的奇异值分解（SVD）在逆问题中成为可解释的因子化工具，通过与解密变换结合，可以用来解释神经网络（NN）在噪声参数估计问题中编码信号模型的结构

    

    在深度学习框架下，我们展示了权重矩阵的奇异值分解（SVD）作为神经网络（NN）解释工具的出现，当与解密变换相结合时，这是一种最近针对噪声参数估计神经网络的解释技术。通过考虑传递给解密最小化问题的数据的平均效果，我们证明了在大数据极限下，解密变换可以用NN权重的SVD和输入自相关矩阵来表示。利用这个事实，我们展示了在噪声参数估计问题类中，SVD可以是训练网络编码信号模型的结构。我们用线性和非线性信号模型的实证证据进一步支持了我们的理论发现。我们的结果还揭示了数学理论和语义发展之间的联系

    Within the framework of deep learning we demonstrate the emergence of the singular value decomposition (SVD) of the weight matrix as a tool for interpretation of neural networks (NN) when combined with the descrambling transformation--a recently-developed technique for addressing interpretability in noisy parameter estimation neural networks \cite{amey2021neural}. By considering the averaging effect of the data passed to the descrambling minimization problem, we show that descrambling transformations--in the large data limit--can be expressed in terms of the SVD of the NN weights and the input autocorrelation matrix. Using this fact, we show that within the class of noisy parameter estimation problems the SVD may be the structure through which trained networks encode a signal model. We substantiate our theoretical findings with empirical evidence from both linear and non-linear signal models. Our results also illuminate the connections between a mathematical theory of semantic developm
    
[^138]: 模型美食：多样模型的回收利用以实现超出分布的泛化

    Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization. (arXiv:2212.10445v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10445](http://arxiv.org/abs/2212.10445)

    本论文提出了一种名为模型美食的新策略，通过回收基于同一基础模型在多样辅助任务上的多次微调，实现了在目标任务上的超出分布的泛化能力。通过利用辅助任务的多样性，这种策略旨在最大限度地提高模型权重的多样性。

    

    基础模型正在重新定义AI系统的构建方式。从一个预训练的基础模型开始，从业者现在都遵循一个标准的流程来构建他们的机器学习解决方案：在目标任务上微调权重。因此，互联网上充斥着许多在不同任务上微调的基础模型：这些单独的微调过程存在孤立，没有相互受益。在我们看来，这是一个被忽视的机会，因为这些专门的模型包含丰富多样的特征。因此，在本文中，我们提出了模型美食，这是一种在不同辅助任务上回收相同基础模型的多个微调的新策略。具体而言，我们将这些辅助权重重新用于目标任务上的多个并行微调的初始化；然后，我们对所有微调后的权重取平均值，得到最终模型。这种回收策略旨在通过利用辅助任务的多样性来最大化权重的多样性。

    Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks.
    
[^139]: 自毁模型：增加基础模型有害双重用途的成本

    Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models. (arXiv:2211.14946v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14946](http://arxiv.org/abs/2211.14946)

    本论文提出了一种名为自毁模型的方法，通过任务阻断范式实现对基础模型下游使用的更精确控制，从而降低基础模型的有害双重用途风险。

    

    一个日益增长的大规模开源基础模型生态系统，降低了应用机器学习解决许多新问题所需的标注数据和技术专长。然而，基础模型存在明显的双重用途风险，无差别地降低了构建有害和有益机器学习系统的成本。目前，政策工具如限制模型访问和出口管制是缓解此类双重用途风险的主要方法。在这项工作中，我们回顾了潜在的安全发布策略，并认为政策制定者和人工智能研究人员都将受益于能够更精确控制开源基础模型下游使用的基础新技术。我们提出了一种方法：任务阻断范式，即在基础模型训练时使用额外机制阻碍对有害任务的适应而不损失对理想任务的性能。我们称这种模型为自毁模型，受到机械设备的启发。

    A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanis
    
[^140]: RaLiBEV: 雷达和激光雷达的引导框自由物体检测系统的融合学习

    RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.06108](http://arxiv.org/abs/2211.06108)

    本论文提出了一种基于鸟瞰视角的引导框自由物体检测系统，通过雷达和激光雷达的特征融合学习，解决了在恶劣天气下物体检测的问题。

    

    在自动驾驶系统中，激光雷达和雷达在感知周围环境中起着重要作用。激光雷达提供精确的三维空间感知信息，但在雾等恶劣天气下无法工作。另一方面，雷达信号由于波长的特性在遇到雨滴或雾粒时会发生衍射，但它受到大噪声的干扰。最近的最新研究表明，雷达和激光雷达的融合可以在恶劣天气下实现强健的检测。现有的方法采用卷积神经网络架构从每个传感器数据流中提取特征，然后对齐和汇聚两个分支的特征以预测物体检测结果。然而，由于标签分配和融合策略的简单设计，这些方法对边界框估计的准确性较低。在本文中，我们提出了一种基于鸟瞰视角融合学习的引导框自由物体检测系统，该系统将来自雷达的距离-方位特征融合起来

    In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
    
[^141]: 插入后门元素的文本编码器对文本生成图像的影响

    Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis. (arXiv:2211.02408v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02408](http://arxiv.org/abs/2211.02408)

    本文证明了文本生成图像模型中使用的文本编码器存在重大的篡改风险，并提出了一种基于反向门攻击的方法，可以插入一个单一字符触发器进提示中，从而触发模型生成具有预定义属性的图像或遵循隐藏的、潜在的恶意描述的图像。

    

    尽管文本生成图像技术在研究者和公众中越来越受欢迎，但这些模型的安全性一直被忽视。许多文本生成图像模型依赖于外部来源的预训练文本编码器，并且它们的用户相信检索到的模型会像承诺的那样运行。不幸的是，这可能不是这种情况。我们引入了反向门攻击文本引导的生成模型，并证明它们的文本编码器构成了重大的篡改风险。我们的攻击只是轻微地改变了编码器，使得对于带有干净提示的图像生成没有可疑的模型行为。然后，通过将一个单一字符触发器插入提示中，例如一个非拉丁字符或表情符号，攻击者就可以触发模型生成具有预定义属性的图像或遵循隐藏的、潜在的恶意描述的图像。我们在Stable Diffusion和highligh上经验性地证明了我们攻击的高效性。

    While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highligh
    
[^142]: 一次性神经场用于3D对象理解

    One-Shot Neural Fields for 3D Object Understanding. (arXiv:2210.12126v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.12126](http://arxiv.org/abs/2210.12126)

    本文提出了一种一次性神经场方法，用于机器人学中的3D对象理解。这种方法利用单个RGB图像构建统一而紧凑的场景表示，可以用于多个任务，如新视角渲染、3D重建、碰撞检查和稳定抓取预测。研究结果表明，这种方法能够从新视角进行渲染并预测成功的抓取。

    

    我们提出了一种用于机器人学的统一且紧凑的场景表示方法，其中场景中的每个对象由捕捉几何和外观的潜在代码来描述。这种表示方法可以解码用于各种任务，例如新视角渲染，3D重建（例如恢复深度，点云或体素图），碰撞检查和稳定抓取预测。我们利用最新的神经辐射场（NeRF）在测试时从单个RGB输入图像构建我们的表示方法，该方法在大型多视图数据集上学习类别级先验知识，然后在少数或仅一个视图的新对象上进行微调。我们扩展了NeRF模型以获得额外的抓取输出，并探索了利用这种表示方法用于机器人学的方法。在测试时，我们从仅一个视点观察到的单个RGB输入图像构建表示方法。我们发现恢复的表示方法允许从新视角进行渲染，包括遮挡的物体部分，并且可以预测成功的抓取。

    We present a unified and compact scene representation for robotics, where each object in the scene is depicted by a latent code capturing geometry and appearance. This representation can be decoded for various tasks such as novel view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or voxel maps), collision checking, and stable grasp prediction. We build our representation from a single RGB input image at test time by leveraging recent advances in Neural Radiance Fields (NeRF) that learn category-level priors on large multiview datasets, then fine-tune on novel objects from one or few views. We expand the NeRF model for additional grasp outputs and explore ways to leverage this representation for robotics. At test-time, we build the representation from a single RGB input image observing the scene from only one viewpoint. We find that the recovered representation allows rendering from novel views, including of occluded object parts, and also for predicting successful 
    
[^143]: 基于贝叶斯神经网络先验不确定性的越域鉴别器

    An out-of-distribution discriminator based on Bayesian neural network epistemic uncertainty. (arXiv:2210.10780v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10780](http://arxiv.org/abs/2210.10780)

    本文研究了基于贝叶斯神经网络的越域鉴别器，并探讨了BNN中的不确定性类型以及如何计算。实验证明在训练数据集中较好表示的图像的先验不确定性较低，而在训练数据集中较差表示的图像中先验不确定性较高。文章还提出了一种基于BNN先验不确定性的越域检测算法，并通过实验证明了其有效性。

    

    神经网络在机器学习领域中具有增强的预测能力，但同时也需要可靠的不确定性量化。贝叶斯神经网络（BNN）是一种重要的神经网络类型，具有内建的不确定性量化能力。本文讨论了BNN中的不确定性类型以及如何计算。通过对一个图像数据集的例子进行实验，其中目标是识别图像中事件的幅度，结果表明在训练数据集中有很好表示的图像的先验不确定性较低，而在训练数据集中很差表示的图像中先验不确定性较高。文章介绍了一种基于BNN先验不确定性的越域检测算法，并通过多个实验展示了影响越域检测能力的因素。

    Neural networks have revolutionized the field of machine learning with increased predictive capability. In addition to improving the predictions of neural networks, there is a simultaneous demand for reliable uncertainty quantification on estimates made by machine learning methods such as neural networks. Bayesian neural networks (BNNs) are an important type of neural network with built-in capability for quantifying uncertainty. This paper discusses aleatoric and epistemic uncertainty in BNNs and how they can be calculated. With an example dataset of images where the goal is to identify the amplitude of an event in the image, it is shown that epistemic uncertainty tends to be lower in images which are well-represented in the training dataset and tends to be high in images which are not well-represented. An algorithm for out-of-distribution (OoD) detection with BNN epistemic uncertainty is introduced along with various experiments demonstrating factors influencing the OoD detection capa
    
[^144]: 有向无环图和偏序集上的因果傅里叶分析

    Causal Fourier Analysis on Directed Acyclic Graphs and Posets. (arXiv:2209.07970v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2209.07970](http://arxiv.org/abs/2209.07970)

    本文提出了一种用于有向无环图的新型傅里叶分析方法，通过特征分解转换信号，关联数据与其因果关系，扩展了组合数学中的经典模比乌斯反演理论。

    

    我们提出了一种新颖的傅里叶分析形式，并提出了与之相关的信号处理概念，用于由带权有向无环图（DAG）索引的信号（或数据）。这意味着我们的傅里叶基函数可以将适当的移位和卷积算子进行特征分解，我们定义了这种算子。 DAG是捕捉数据值之间因果关系的常见模型，在本文中，我们的傅里叶分析将数据与其因果关系联系起来，其中采用了我们定义的线性假设。傅里叶变换的定义需要对加权DAG进行传递闭包处理，根据边权重的解释，可以有几种形式。示例包括影响水平、距离或污染分布。我们的框架不同于先前的图信号处理：它专门针对DAG并利用并扩展了组合数学中的经典模比乌斯反演理论。对于一个典型的应用，我们考虑了建模动态网络的DAGs，其中边线表示时间顺序和因果关系的强度。

    We present a novel form of Fourier analysis, and associated signal processing concepts, for signals (or data) indexed by edge-weighted directed acyclic graphs (DAGs). This means that our Fourier basis yields an eigendecomposition of a suitable notion of shift and convolution operators that we define. DAGs are the common model to capture causal relationships between data values and in this case our proposed Fourier analysis relates data with its causes under a linearity assumption that we define. The definition of the Fourier transform requires the transitive closure of the weighted DAG for which several forms are possible depending on the interpretation of the edge weights. Examples include level of influence, distance, or pollution distribution. Our framework is different from prior GSP: it is specific to DAGs and leverages, and extends, the classical theory of Moebius inversion from combinatorics. For a prototypical application we consider DAGs modeling dynamic networks in which edge
    
[^145]: 3D感知视频生成

    3D-Aware Video Generation. (arXiv:2206.14797v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.14797](http://arxiv.org/abs/2206.14797)

    通过结合神经隐式表示和时间感知的判别器，我们开发了一个4D GAN框架，能够在没有条件的情况下生成3D感知视频，产生具有多视图和时态一致性的高质量图像，并学习了丰富的可分解的3D结构和运动。

    

    生成模型已成为许多图像合成和编辑任务的重要构建块。最近在这个领域的进展也使得能够生成具有多视图或时态一致性的高质量3D或视频内容。在我们的工作中，我们探索了学习无条件生成3D感知视频的4D生成对抗网络（GAN）。通过将神经隐式表示与时间感知的判别器相结合，我们开发了一个GAN框架，仅通过单目视频监督合成3D视频。我们展示了我们的方法学习了丰富的可分解的3D结构和动作嵌入，从而实现了新的时空渲染的视觉效果，同时产生了与现有3D或视频GAN质量相媲美的图像。

    Generative models have emerged as an essential building block for many image synthesis and editing tasks. Recent advances in this field have also enabled high-quality 3D or video content to be generated that exhibits either multi-view or temporal consistency. With our work, we explore 4D generative adversarial networks (GANs) that learn unconditional generation of 3D-aware videos. By combining neural implicit representations with time-aware discriminator, we develop a GAN framework that synthesizes 3D video supervised only with monocular videos. We show that our method learns a rich embedding of decomposable 3D structures and motions that enables new visual effects of spatio-temporal renderings while producing imagery with quality comparable to that of existing 3D or video GANs.
    
[^146]: 通过层次分块距离模型进行超低维图表示

    A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph Representations. (arXiv:2204.05885v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2204.05885](http://arxiv.org/abs/2204.05885)

    本文提出了一种层次分块距离模型（HBDM）用于图表示学习，可以处理大规模网络分析，考虑多尺度结构并且明确考虑到网络的同质性和传递性。

    

    图表示学习已经成为表征复杂网络结构和进行链接预测、节点分类、网络重构和社区检测等任务的核心。虽然已经提出了很多生成式图表示学习模型，但许多方法由于计算要求过高而限制了大规模网络分析，很少能够明确考虑多尺度结构的出现，并且只有很少一部分明确考虑到重要的网络属性，如同质性和传递性。本文提出了一种新的可扩展的图表示学习方法，称为层次分块距离模型（HBDM）。HBDM 强制实施了一种类似于随机分块建模（SBM）的多尺度分块结构，并通过准确逼近潜在距离模型（LDM）在推理的分层中考虑同质性和传递性。HBDM 自然地适用于单分区、有向和双分区网络。

    Graph Representation Learning (GRL) has become central for characterizing structures of complex networks and performing tasks such as link prediction, node classification, network reconstruction, and community detection. Whereas numerous generative GRL models have been proposed, many approaches have prohibitive computational requirements hampering large-scale network analysis, fewer are able to explicitly account for structure emerging at multiple scales, and only a few explicitly respect important network properties such as homophily and transitivity. This paper proposes a novel scalable graph representation learning method named the Hierarchical Block Distance Model (HBDM). The HBDM imposes a multiscale block structure akin to stochastic block modeling (SBM) and accounts for homophily and transitivity by accurately approximating the latent distance model (LDM) throughout the inferred hierarchy. The HBDM naturally accommodates unipartite, directed, and bipartite networks whereas the h
    
[^147]: BoMD：适用于嘈杂X光分类的多标签描述符包

    BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.01937](http://arxiv.org/abs/2203.01937)

    本文提出了一种适用于多标签、嘈杂CXR学习的方法，使用基于袋的多标签描述符平滑地重新标记数据集中的样本，并进行训练以提高模型性能。

    

    深度学习方法在医学图像问题的分类精度方面表现出色，这在很大程度上归功于具有清洁标签的大规模数据集的可用性。然而，考虑到这种手动注释的高成本，新的医学图像分类问题可能需要依赖于从放射学报告中提取的机器生成的嘈杂标签。事实上，许多胸部X光分类器已经从带有嘈杂标签的数据集中建模，但它们的训练过程通常不具有噪声标签样本的鲁棒性，导致次优模型。此外，CXR数据集大多是多标记的，因此当前设计用于多类问题的嘈杂标签学习方法不能轻松地进行调整。本文提出了一种新方法，用于嘈杂多标签CXR学习，其中检测并平滑地重新标记数据集中的样本，然后用于训练常见的多标签分类器。该方法优化了一个基于袋的多标签表示方法，以便有效地使用从放射学报告中提取的信息。

    Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
    
[^148]: 使用集成神经网络在修改的SIRD流行病模型中进行参数识别的反问题

    Inverse problem for parameters identification in a modified SIRD epidemic model using ensemble neural networks. (arXiv:2203.00407v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00407](http://arxiv.org/abs/2203.00407)

    本文提出了一个基于集成神经网络的方法，用于识别修改的SIRD流行病模型中的参数。该方法考虑了死亡人数作为一个独立类别，并引入了一个衡量实际感染总数与官方统计感染人数差异的参数。通过依赖前7天的数据进行估计，我们可以预测未来的流行病情况。

    

    本文提出了一种SIRD模型的参数识别方法，这是对经典SIR模型的扩展，将死亡人数作为一个独立的类别加以考虑。此外，我们的模型还包括一个参数，即实际感染总数与官方统计的感染人数之比。由于许多因素，如政府决策、多种变体的传播、学校的开闭等，模型参数在长时间内保持不变的典型假设是不现实的。因此，我们的目标是创建一种适用于短时间的方法。在这个范围内，我们通过依赖前7天的数据来进行估计，然后利用确定的参数进行预测。为了进行参数估计，我们提出了一个神经网络集成的平均值方法。每个神经网络是基于通过求解SIRD模型得到的7天数据集构建的。

    In this paper, we propose a parameter identification methodology of the SIRD model, an extension of the classical SIR model, that considers the deceased as a separate category. In addition, our model includes one parameter which is the ratio between the real total number of infected and the number of infected that were documented in the official statistics.  Due to many factors, like governmental decisions, several variants circulating, opening and closing of schools, the typical assumption that the parameters of the model stay constant for long periods of time is not realistic. Thus our objective is to create a method which works for short periods of time. In this scope, we approach the estimation relying on the previous 7 days of data and then use the identified parameters to make predictions.  To perform the estimation of the parameters we propose the average of an ensemble of neural networks. Each neural network is constructed based on a database built by solving the SIRD for 7 day
    
[^149]: 虚拟回顾经验重放：用于稀疏奖励任务的好奇模型基学习

    Imaginary Hindsight Experience Replay: Curious Model-based Learning for Sparse Reward Tasks. (arXiv:2110.02414v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.02414](http://arxiv.org/abs/2110.02414)

    这篇论文提出了一种虚拟回顾经验重放的简单基于模型的方法，适用于稀疏奖励的多目标任务，无需复杂的奖励工程。通过引入虚拟数据和好奇心驱动的内在奖励，此方法在OpenAI Gym Fetch Robotics任务上平均提高了一个数量级的数据效率。

    

    基于模型的强化学习是一种有望应用于实际机器人任务的学习策略，因为它相比于基于模型无关的方法能够提高数据效率。然而，当前的基于模型的方法都依赖于设计和实现困难的形状奖励信号。为了解决这个问题，我们提出了一种简单的基于模型的方法，专门针对稀疏奖励的多目标任务，避免了复杂的奖励工程的需求。这种方法被称为虚拟回顾经验重放，通过将虚拟数据纳入到策略更新中，以减少真实世界的交互。为了改善稀疏奖励环境下的探索，该策略采用标准的回顾经验重放训练，并配备了基于好奇心的内在奖励。评估结果显示，在OpenAI Gym Fetch Robotics任务的基准测试中，这种方法的数据效率平均提高了一个数量级，相比于当前最先进的基于模型无关方法。

    Model-based reinforcement learning is a promising learning strategy for practical robotic applications due to its improved data-efficiency versus model-free counterparts. However, current state-of-the-art model-based methods rely on shaped reward signals, which can be difficult to design and implement. To remedy this, we propose a simple model-based method tailored for sparse-reward multi-goal tasks that foregoes the need for complicated reward engineering. This approach, termed Imaginary Hindsight Experience Replay, minimises real-world interactions by incorporating imaginary data into policy updates. To improve exploration in the sparse-reward setting, the policy is trained with standard Hindsight Experience Replay and endowed with curiosity-based intrinsic rewards. Upon evaluation, this approach provides an order of magnitude increase in data-efficiency on average versus the state-of-the-art model-free method in the benchmark OpenAI Gym Fetch Robotics tasks.
    
[^150]: SANSformers: 无注意力模型在电子健康记录中的自我监督预测

    SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models. (arXiv:2108.13672v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.13672](http://arxiv.org/abs/2108.13672)

    使用无注意力的序列模型SANSformer在电子健康记录中进行自我监督预测，充分挖掘了Transformer在EHR应用中的优势。主要应用于预测未来的医疗资源利用，特别适用于处理不同患者子组，如罕见疾病患者。

    

    Transformer神经网络在电子健康记录（EHR）中的应用面临挑战，由于EHR数据具有独特的多维顺序结构，所以与简单的线性模型相比往往表现不佳。因此，Transformer的优势，如高效的迁移学习和改进的可扩展性，在EHR应用中没有得到充分利用。为了克服这些挑战，我们引入了SANSformer，这是一种新颖的无注意力序列模型，专门设计以适应EHR数据的独特特征。我们的主要应用领域是预测未来的医疗资源利用，这是有效分配医疗资源的关键任务。当处理不同的患者子组时，这个任务变得特别困难。这些被唯一的健康轨迹所特征化的子组，往往规模较小，如罕见疾病患者，需要特殊的建模方法。为了解决这个问题，我们采用了一种自我监督的方法，将多个子组的预测任务转化为一个整体的预测任务。

    The application of Transformer neural networks to Electronic Health Records (EHR) is challenging due to the distinct, multidimensional sequential structure of EHR data, often leading to underperformance when compared to simpler linear models. Thus, the advantages of Transformers, such as efficient transfer learning and improved scalability are not fully exploited in EHR applications. To overcome these challenges, we introduce SANSformer, a novel attention-free sequential model designed specifically with inductive biases to cater for the unique characteristics of EHR data.  Our main application area is predicting future healthcare utilization, a crucial task for effectively allocating healthcare resources. This task becomes particularly difficult when dealing with divergent patient subgroups. These subgroups, characterized by unique health trajectories and often small in size, such as patients with rare diseases, require specialized modeling approaches. To address this, we adopt a self-
    
[^151]: 基于模型的多智能体强化学习在零和马尔可夫博弈中具有接近最优样本复杂度

    Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity. (arXiv:2007.07461v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.07461](http://arxiv.org/abs/2007.07461)

    本文研究了基于模型的多智能体强化学习在零和马尔可夫博弈中的样本复杂度问题，并证明了其在找到纳什均衡值及具有平滑规划预言机的ε-NE策略方面具有接近最优的样本复杂度。

    

    基于模型的强化学习（RL）已经被认为是RL的基石之一，通过使用经验模型找到最优策略。它特别适用于多智能体RL（MARL），因为它自然地将学习和规划阶段解耦，并避免了在所有智能体同时使用样本改进策略时的非稳态问题。尽管直观且广泛使用，但基于模型的MARL算法的样本复杂度尚未得到全面研究。本文的目标是解决关于其样本复杂度的基本问题。我们研究了可能是最基本的MARL设置：只能访问一个生成模型的两人折扣零和马尔可夫博弈。我们证明了基于模型的MARL在寻找到某个ε误差的纳什均衡（NE）值以及具有平滑规划预言机的ε-NE策略方面达到了样本复杂度为$\tilde O(|S||A||B|(1-\gamma)^{-3}\epsilon^{-2})$的结果，其中γ是。

    Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the corner stones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has not been fully investigated. In this paper, our goal is to address the fundamental question about its sample complexity. We study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model. We show that model-based MARL achieves a sample complexity of $\tilde O(|S||A||B|(1-\gamma)^{-3}\epsilon^{-2})$ for finding the Nash equilibrium (NE) value up to some $\epsilon$ error, and the $\epsilon$-NE policies with a smooth planning oracle, where $\gamma$ is t
    
[^152]: 稀疏和低秩高阶张量回归通过并行近端方法

    Sparse and Low-Rank High-Order Tensor Regression via Parallel Proximal Method. (arXiv:1911.12965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.12965](http://arxiv.org/abs/1911.12965)

    本论文提出了一种通过并行近端方法实现稀疏和低秩高阶张量回归的模型，该模型通过直接应用$\ell_1$范数和张量核范数来保留张量的结构信息，并且在处理大规模数据和高阶结构时具有可扩展性和高效性。

    

    在许多现代应用中生成了张量数据（或多维数组），例如神经科学中的功能性磁共振成像（fMRI）以及视频分析中的视频。近年来，针对预测张量特征与单变量响应之间的关系提出了许多方法。然而，以往的方法要么丢失了张量数据中的结构信息，要么在处理高阶结构的大规模数据时时间成本过高。为了解决这些问题，我们提出了稀疏和低秩张量回归（SLTR）模型。我们的模型通过直接应用$\ell_1$范数和张量核范数来强制张量系数的稀疏性和低秩性，从而保留了张量的结构信息。为了使求解过程可扩展和高效，SLTR利用了近端梯度法，可以轻松地并行实现。我们在几个模拟数据集和一个视频数据集上评估了SLTR。

    Recently, tensor data (or multidimensional array) have been generated in many modern applications, such as functional magnetic resonance imaging (fMRI) in neuroscience and videos in video analysis. Many efforts are made in recent years to predict the relationship between tensor features and univariate responses. However, previously proposed methods either lose structural information within tensor data or have prohibitively expensive time costs, especially for large-scale data with high-order structures. To address such problems, we propose the Sparse and Low-rank Tensor Regression (SLTR) model. Our model enforces sparsity and low-rankness of the tensor coefficient by directly applying $\ell_1$ norm and tensor nuclear norm, such that it preserves structural information of the tensor. To make the solving procedure scalable and efficient, SLTR makes use of the proximal gradient method, which can be easily implemented parallelly. We evaluate SLTR on several simulated datasets and one video
    

