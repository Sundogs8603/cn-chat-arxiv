# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning to Augment Distributions for Out-of-Distribution Detection.](http://arxiv.org/abs/2311.01796) | 本研究提出了一种学习方法，通过增强分布的方式来区分在开放世界中的未知样本。具体来说，通过构造包含在辅助未知样本分布周围的Wasserstein球中所有分布的未知样本分布集，来减小分布差异，从而提升开放世界检测性能。 |
| [^2] | [Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching.](http://arxiv.org/abs/2311.01248) | 本研究利用视觉触觉传感器和模仿学习相结合，通过配对优化触觉力量曲线和简化传感器应用，对接触丰富的操作任务进行了研究。 |
| [^3] | [Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement.](http://arxiv.org/abs/2310.16979) | 该论文提出了一种无监督领域自适应方法，通过伪标签自我修正和噪声像素定位来改进语义分割模型的性能。 |
| [^4] | [Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels.](http://arxiv.org/abs/2310.16407) | 这项工作提出了一种基于信息论的拓扑感知联邦边缘学习的泛化分析方法，并提出了一种名为FedGMIR的正则化方法来增强模型性能。 |
| [^5] | [Improving Generalization of Alignment with Human Preferences through Group Invariant Learning.](http://arxiv.org/abs/2310.11971) | 该论文提出了一种通过强化学习实现在不同数据组或领域中学习一致策略的方法，该方法可以提高AI助手对不同领域的泛化能力，并更好地与人类偏好对齐。 |
| [^6] | [Can We Edit Multimodal Large Language Models?.](http://arxiv.org/abs/2310.08475) | 本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。 |
| [^7] | [Latent Diffusion Model for DNA Sequence Generation.](http://arxiv.org/abs/2310.06150) | 提出了一种新的潜在扩散模型DiscDiff，用于离散DNA序列生成。通过将离散DNA序列嵌入到连续的潜在空间中，利用连续扩散模型的强大生成能力来生成离散数据。同时，引入了一种新的度量标准FReD，用于评估DNA序列生成的样本质量。 |
| [^8] | [Enhancing Accuracy in Deep Learning Using Random Matrix Theory.](http://arxiv.org/abs/2310.03165) | 本研究探索了随机矩阵理论在深度神经网络训练中的应用，通过层剪枝和损失曲面优化，实现了对DNN架构的简化和准确性的增强。通过奇异值分解，并根据随机矩阵理论的标准丢弃小的奇异值，可减少DNN层的参数，简化DNN架构，同时保持或增强模型的准确性。 |
| [^9] | [Iterative Option Discovery for Planning, by Planning.](http://arxiv.org/abs/2310.01569) | 本文提出了一种迭代式选项发现方法，通过学习一组局部强策略来指导搜索算法，从而在强化学习和规划中应用于复杂领域。 |
| [^10] | [Seismogram Transformer: A generic deep learning backbone network for multiple earthquake monitoring tasks.](http://arxiv.org/abs/2310.01037) | 本文介绍了一种名为地震图变压器（SeisT）的通用深度学习骨干网络模型，用于多种地震监测任务。SeisT的高效网络架构使其在地震检测、地震相位拾取、首次运动极性分类、震级估计和反方位角估计等任务中表现优秀，特别是在泛化性能方面。 |
| [^11] | [Bayesian Design Principles for Frequentist Sequential Learning.](http://arxiv.org/abs/2310.00806) | 该论文提出了一种贝叶斯设计原则用于优化频率式序贯学习问题的通用理论，通过生成“算法信念”并使用贝叶斯后验进行决策，实现了无先验的贝叶斯类型算法在对抗性环境中的泛化和优化应用。 |
| [^12] | [Recurrent Hypernetworks are Surprisingly Strong in Meta-RL.](http://arxiv.org/abs/2309.14970) | 递归超网络和循环神经网络在元强化学习中的端到端学习表现出惊人的强大性能，相比于现有专门方法更为简单但效果更好。 |
| [^13] | [Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model.](http://arxiv.org/abs/2309.12334) | 本文将深度知识追踪视为一种编码器-解码器结构，通过在多个数据集上进行实验发现，一个更简单的解码器可以比DKT预测学生表现更好。 |
| [^14] | [FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning.](http://arxiv.org/abs/2309.08420) | 提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。 |
| [^15] | [Gaussian Processes with Linear Multiple Kernel: Spectrum Design and Distributed Learning for Multi-Dimensional Data.](http://arxiv.org/abs/2309.08201) | 本文研究了高斯过程与线性多核在多维数据上的应用，提出了一种新的格点谱混合核公式，减少了超参数数量，同时保留了优化结构和逼近能力。通过引入分布式算法，使大规模超参数优化变得可行。 |
| [^16] | [Beta Diffusion.](http://arxiv.org/abs/2309.07867) | beta扩散是一种新型生成模型方法，通过引入去掩盖和去噪的技术，利用缩放和偏移的beta分布进行乘法转换，实现在有界范围内生成数据。相比于传统的基于扩散的生成模型，它通过KL散度上界进行优化，证明了效果更好。 |
| [^17] | [Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism.](http://arxiv.org/abs/2308.13150) | 本文介绍了一种使用ResNet模型和轻量级注意机制框架的图像分类方法，通过优化特征表示、增强分类能力和改善特征可辨别性，在乳腺癌分类任务上显示出卓越性能和潜在应用前景。 |
| [^18] | [Data-driven decision-focused surrogate modeling.](http://arxiv.org/abs/2308.12161) | 这项研究提出了决策聚焦代理建模的概念，通过学习一个简化的凸优化模型来最小化原始和代理优化模型之间的决策预测误差，并在实时环境中解决计算上具有挑战性的非线性优化问题。 |
| [^19] | [Constrained Stein Variational Trajectory Optimization.](http://arxiv.org/abs/2308.12110) | CSVTO是一种受限斯坦变分轨迹优化算法，它通过斯坦变分梯度下降方法生成多样的约束满足轨迹集合，提高了在具有任意约束的问题中的优化性能和鲁棒性。 |
| [^20] | [Knowledge Graph Prompting for Multi-Document Question Answering.](http://arxiv.org/abs/2308.11730) | 这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。 |
| [^21] | [Fixed Integral Neural Networks.](http://arxiv.org/abs/2307.14439) | 本文介绍了一种方法来通过神经网络表示学习函数的解析积分，具有计算精确积分的能力，并能将约束直接应用于积分，而且还介绍了将学习函数约束为正的方法以及相关应用。 |
| [^22] | [Voting-based Multimodal Automatic Deception Detection.](http://arxiv.org/abs/2307.07516) | 本文提出了一种基于投票的多模态方法用于自动欺骗检测，通过视频的音频、视觉和文本特征进行检测。实验结果表明，我们的解决方案在欺骗检测中表现优于现有技术。 |
| [^23] | [Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation.](http://arxiv.org/abs/2307.05385) | 本文提出了一种通过学习核技术，具有解释性且参数较少的方法来评估和分割PPG信号的质量和伪影，与现有的深度神经网络方法相比有着类似甚至更好的性能。 |
| [^24] | [CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks.](http://arxiv.org/abs/2307.02813) | 这篇论文提出了一种对比式预训练方法（CPDG）用于动态图神经网络（DGNNs），通过灵活的结构-时序子图采样和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战，实验证明CPDG在各种下游任务中的动态图预训练方面优于现有方法。 |
| [^25] | [Large Language Models Empowered Autonomous Edge AI for Connected Intelligence.](http://arxiv.org/abs/2307.02779) | 大型语言模型赋能连接智能的自主边缘AI系统，通过云-边缘-客户端的分层架构和强大的GPT模型能力，实现高质量、低延迟、隐私保护的AI服务，满足用户个人需求并实现自动化。 |
| [^26] | [TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers.](http://arxiv.org/abs/2307.02588) | TransformerG2G是一种使用Transformer进行自适应时间步长的图嵌入模型，通过学习历史上的长程依赖关系，准确地捕捉时态图的动态特征。 |
| [^27] | [An efficient and straightforward online quantization method for a data stream through remove-birth updating.](http://arxiv.org/abs/2306.12574) | 本文提出了一种在线量化数据流的方法，通过移除生胎更新快速适应概念漂移，可以产生最小化的死单元，并为漂移检测提供了一些有用的度量指标。 |
| [^28] | [Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms.](http://arxiv.org/abs/2306.12383) | 本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。 |
| [^29] | [OpenGSL: A Comprehensive Benchmark for Graph Structure Learning.](http://arxiv.org/abs/2306.10280) | OpenGSL是第一个针对图结构学习的综合基准测试，旨在解决GSL领域中由于实验协议不一致而导致的进展不明确的问题。 |
| [^30] | [A Differential Testing Framework to Evaluate Image Recognition Model Robustness.](http://arxiv.org/abs/2306.06208) | 本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。 |
| [^31] | [Fault Localization for Framework Conversions of Image Recognition Models.](http://arxiv.org/abs/2306.06157) | 本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。 |
| [^32] | [Embedding Inequalities for Barron-type Spaces.](http://arxiv.org/abs/2305.19082) | 本文测量了Barron空间和谱Barron空间之间的关系，并提供了嵌入不等式。 |
| [^33] | [Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback.](http://arxiv.org/abs/2305.18341) | 本文提出了一种叫做RLCF的方法，使用代码编译器反馈进一步训练预训练的大型语言模型，以生成符合目标分布的代码，并通过所有静态正确性检查，显著提高了性能。 |
| [^34] | [Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective.](http://arxiv.org/abs/2305.15408) | 本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。 |
| [^35] | [Learning Rate Free Bayesian Inference in Constrained Domains.](http://arxiv.org/abs/2305.14943) | 我们的算法是学习率无关的约束域采样算法，并提出了一个统一框架，能够处理多种约束采样问题，实现了与现有算法相竞争的性能。 |
| [^36] | [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.](http://arxiv.org/abs/2305.13245) | 该论文介绍了一种将现有的多头语言模型检查点升级为具有多查询注意力（MQA）的模型的方法，并引入了群组查询注意力（GQA）来解决MQA可能导致的质量下降问题。通过升级后的GQA模型，实现了接近多头注意力的质量，并具备与MQA相当的速度。 |
| [^37] | [The Adversarial Consistency of Surrogate Risks for Binary Classification.](http://arxiv.org/abs/2305.09956) | 本文研究了二分类中代理风险的对抗一致性，并给出了代理损失函数集合的特征化，结果表明敌对一致代理的类与标准设置相比小得多。 |
| [^38] | [Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits.](http://arxiv.org/abs/2305.06743) | 本文提出了一种针对奖励分布重尾的MAB问题的隐式规范化预测器，证明该方法在线性和非线性重尾随机MAB问题上是最优的。 |
| [^39] | [Neural Lyapunov Control for Discrete-Time Systems.](http://arxiv.org/abs/2305.06547) | 该论文介绍了一种用于离散时间系统的神经李雅普诺夫控制方法，该方法利用混合整数线性规划来验证稳定性条件，计算子水平集刻画吸引域，有效地学习控制策略。 |
| [^40] | [Causal Discovery from Subsampled Time Series with Proxy Variables.](http://arxiv.org/abs/2305.05276) | 本研究提出了一种使用代理变量方法的无模型算法，可以从子采样时间序列中无需参数约束地识别整个因果结构。 |
| [^41] | [CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning.](http://arxiv.org/abs/2305.03148) | CAMEL提出了使用嵌入式DRAM作为主要存储介质的方法来解决设备端学习中存储和计算过程中占用大量内存的问题，从而使AI模型更加高效。 |
| [^42] | [Segment Anything Model for Medical Images?.](http://arxiv.org/abs/2304.14660) | “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。 |
| [^43] | [Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning.](http://arxiv.org/abs/2304.02711) | SPIRES是一种新的知识提取方法，利用大型语言模型进行零样本学习和通用查询回答，能够填充复杂的知识库而无需显式训练数据。 |
| [^44] | [Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models.](http://arxiv.org/abs/2304.01046) | 本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。 |
| [^45] | [Robust Risk-Aware Option Hedging.](http://arxiv.org/abs/2303.15216) | 本研究利用健壮的风险感知强化学习算法，优化期权对冲策略，特别应用于界限期权对冲，随着代理风险偏好变化，对冲策略发生扭曲，鲁棒策略优于非鲁棒策略。 |
| [^46] | [Learning with Explanation Constraints.](http://arxiv.org/abs/2303.14496) | 本文研究了解释约束下的学习问题，提出了EPAC模型，探讨了使用这些解释时模型的益处，并提供了一种基于变分近似的算法解决方案。 |
| [^47] | [FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs.](http://arxiv.org/abs/2303.00859) | FuNVol是一个多资产隐含波动率市场模拟器，使用函数主成分和神经SDE生成真实历史价格的IV表面序列，并在无静态套利的表面次流形内产生一致的市场情景。同时，使用模拟表面进行对冲可以生成与实现P＆L一致的损益分布。 |
| [^48] | [Quantum Learning Theory Beyond Batch Binary Classification.](http://arxiv.org/abs/2302.07409) | 这篇论文通过研究量子学习理论拓展了批处理多类学习、在线布尔学习和在线多类学习，并提出了第一个具有量子示例的在线学习模型。 |
| [^49] | [Private Statistical Estimation of Many Quantiles.](http://arxiv.org/abs/2302.06943) | 本文主要研究如何在差分隐私条件下估计一个分布的多个分位数。它提出了两种方法：一种是通过私有地估计样本的经验分位数来估计分布的分位数，另一种是使用密度估计技术进行分位数函数估计，并且展示了两种方法之间的权衡。 |
| [^50] | [DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets.](http://arxiv.org/abs/2302.04178) | DynGFN是一种借助RNA速度技术进行基因调控网络推断的方法，能够捕捉网络结构的不确定性，并在准确度上超过现有方法。 |
| [^51] | [Convergence Analysis of Sequencial Split Learning on Heterogeneous Data.](http://arxiv.org/abs/2302.01633) | 本文推导出了顺序分割学习在异构数据上收敛的保证，并且证明了它在异构数据上优于联邦平均算法。 |
| [^52] | [A Reduction-based Framework for Sequential Decision Making with Delayed Feedback.](http://arxiv.org/abs/2302.01477) | 我们提出了一个基于规约的框架，可以将任何多批次算法转化为处理顺序决策中的随机延迟的高效算法。我们不仅在赌博机、表格型MDPs和表格型MGs方面取得了与现有结果相匹配或改进的成果，还首次对顺序决策中的延迟与函数逼近进行了研究。 |
| [^53] | [A Comprehensive Survey of Dataset Distillation.](http://arxiv.org/abs/2301.05603) | 数据集压缩是处理海量数据的一种方法，通过综合一个小型典型数据集来提高数据处理效率。这一方法在压缩数据集方面取得了惊人的性能，但仍存在一些限制。 |
| [^54] | [Classification by sparse additive models.](http://arxiv.org/abs/2212.01792) | 这篇论文研究了非参数的稀疏加性模型用于分类，通过对分量系数施加稀疏组Lasso和稀疏组Slope型惩罚来设计分类器，实验证明了分类器在未知稀疏性和平滑性上的自适应性能。 |
| [^55] | [Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior: From Theory to Practice.](http://arxiv.org/abs/2211.07206) | 本研究使用PAC-Bayesian理论提供了元学习的泛化界限，并推导出了最佳性能保证的闭式优化超后验(PACOH)。通过理论分析和案例研究，我们展示了这些保证在元学习中相对于PAC-Bayesian每个任务学习界限的改进。 |
| [^56] | [Clustered Federated Learning based on Nonconvex Pairwise Fusion.](http://arxiv.org/abs/2211.04218) | 本研究提出了一种基于非凸配对融合的聚类联邦学习框架，能够自动识别聚类结构，降低通信成本，并确保隐私性。 |
| [^57] | [Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition.](http://arxiv.org/abs/2210.08415) | 本文引入了统一加倍条件，以确保在训练DNN期间准确性的稳定性，无论使用绝对值激活函数还是ReLU激活函数。在统一加倍条件下，准确率有很高的概率稳定，并提供了具体估计。 |
| [^58] | [MENLI: Robust Evaluation Metrics from Natural Language Inference.](http://arxiv.org/abs/2208.07316) | 本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。 |
| [^59] | [Ensemble forecasts in reproducing kernel Hilbert space family: dynamical systems in Wonderland.](http://arxiv.org/abs/2207.14653) | 本文提出了一种在动力系统中的集合预测和模拟的方法，将系统嵌入再生核希尔伯特空间族，并在该空间中使用简单的集合数据同化方法进行轨迹重构。 |
| [^60] | [Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees.](http://arxiv.org/abs/2206.02659) | 本文通过Hessian-based分析，提出一种基于距离的泛化度量方法，用于理解深度神经网络微调的泛化特性。通过PAC-Bayesian分析，给出了基于Hessian距离的微调模型泛化界。此外，还对微调面对标签噪声的问题进行了研究，并提出了一种相关算法和泛化误差保证。 |
| [^61] | [On-Device Learning: A Neural Network Based Field-Trainable Edge AI.](http://arxiv.org/abs/2203.01077) | 本研究介绍了一种基于神经网络的设备上学习方法，针对边缘人工智能应用中的环境因素对准确性造成的影响进行了解决。通过重新训练，在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。 |

# 详细

[^1]: 学习增强分布用于区分开放世界中的未知样本

    Learning to Augment Distributions for Out-of-Distribution Detection. (arXiv:2311.01796v1 [cs.LG])

    [http://arxiv.org/abs/2311.01796](http://arxiv.org/abs/2311.01796)

    本研究提出了一种学习方法，通过增强分布的方式来区分在开放世界中的未知样本。具体来说，通过构造包含在辅助未知样本分布周围的Wasserstein球中所有分布的未知样本分布集，来减小分布差异，从而提升开放世界检测性能。

    

    开放世界分类系统应该区分那些标签与内部分布情况不一致的未知样本，这促使了对未知样本检测的研究。尽管先进的方法在这方面取得了很大进展，但由于对未知样本缺乏预先了解，它们在开放世界中仍可能失败。虽然可以访问用于模型训练的辅助未知样本（与未知样本不同），但仍需分析这些辅助数据在开放世界中的效果。为此，我们从学习理论的角度研究这个问题，发现辅助未知样本和真实未知样本之间的分布差异是影响开放世界检测性能的关键。基于此，我们提出分布增强的未知样本学习(DAL)，通过构建一个包含在辅助未知样本分布周围的Wasserstein球中的所有分布的未知样本分布集来减小未知样本分布差异。我们证明了DAL在开放世界中具有较好性能。

    Open-world classification systems should discern out-of-distribution (OOD) data whose labels deviate from those of in-distribution (ID) cases, motivating recent studies in OOD detection. Advanced works, despite their promising progress, may still fail in the open world, owing to the lack of knowledge about unseen OOD data in advance. Although one can access auxiliary OOD data (distinct from unseen ones) for model training, it remains to analyze how such auxiliary data will work in the open world. To this end, we delve into such a problem from a learning theory perspective, finding that the distribution discrepancy between the auxiliary and the unseen real OOD data is the key to affecting the open-world detection performance. Accordingly, we propose Distributional-Augmented OOD Learning (DAL), alleviating the OOD distribution discrepancy by crafting an OOD distribution set that contains all distributions in a Wasserstein ball centered on the auxiliary OOD distribution. We justify that t
    
[^2]: 将其推向展示极限：多模态视觉触觉模仿学习与力匹配

    Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])

    [http://arxiv.org/abs/2311.01248](http://arxiv.org/abs/2311.01248)

    本研究利用视觉触觉传感器和模仿学习相结合，通过配对优化触觉力量曲线和简化传感器应用，对接触丰富的操作任务进行了研究。

    

    光学触觉传感器已经成为机器人操作过程中获取密集接触信息的有效手段。最近引入的“透视你的皮肤”（STS）型传感器具有视觉和触觉模式，通过利用半透明表面和可控照明实现。本文研究了视觉触觉传感与模仿学习在富有接触的操作任务中的好处。首先，我们使用触觉力测量和一种新的算法，在运动示范中产生更好匹配人体示范者的力曲线。其次，我们添加了视觉/触觉STS模式切换作为控制策略输出，简化传感器的应用。最后，我们研究了多种观察配置，比较和对比了视觉/触觉数据（包括模式切换和不切换）与手腕挂载的眼在手摄像机的视觉数据的价值。我们在一个广泛的实验系列上进行实验。

    Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
    
[^3]: 无监督领域自适应语义分割与伪标签自我修正

    Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v1 [cs.CV])

    [http://arxiv.org/abs/2310.16979](http://arxiv.org/abs/2310.16979)

    该论文提出了一种无监督领域自适应方法，通过伪标签自我修正和噪声像素定位来改进语义分割模型的性能。

    

    基于深度学习的语义分割解决方案在与训练时不同特征的数据上测试时性能显著下降。使用来自新域的注释数据来适应模型并不总是切实可行的。无监督领域自适应（UDA）方法在实际操作条件下部署这些模型是至关重要的。最近的最先进（SOTA）UDA方法采用教师-学生自训练方法，其中教师模型用于生成新数据的伪标签，进而指导学生模型的训练过程。尽管此方法取得了很大成功，但却存在伪标签在训练过程中传播的噪声问题。为了解决这个问题，我们提出了一个辅助伪标签精炼网络（PRN），用于在线精炼伪标签，并定位可能存在噪声的像素点的预测标签。

    Deep learning-based solutions for semantic segmentation suffer from significant performance degradation when tested on data with different characteristics than what was used during the training. Adapting the models using annotated data from the new domain is not always practical. Unsupervised Domain Adaptation (UDA) approaches are crucial in deploying these models in the actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ a teacher-student self-training approach, where a teacher model is used to generate pseudo-labels for the new data which in turn guide the training process of the student model. Though this approach has seen a lot of success, it suffers from the issue of noisy pseudo-labels being propagated in the training process. To address this issue, we propose an auxiliary pseudo-label refinement network (PRN) for online refining of the pseudo labels and also localizing the pixels whose predicted labels are likely to be noisy. Being able to improve the 
    
[^4]: 基于信息论的拓扑感知异构联邦边缘学习在噪声通道上的泛化分析

    Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels. (arXiv:2310.16407v1 [cs.IT])

    [http://arxiv.org/abs/2310.16407](http://arxiv.org/abs/2310.16407)

    这项工作提出了一种基于信息论的拓扑感知联邦边缘学习的泛化分析方法，并提出了一种名为FedGMIR的正则化方法来增强模型性能。

    

    随着边缘智能的快速发展，无线网络上的联邦学习部署越来越受关注，被称为联邦边缘学习（FEEL）。在FEEL中，移动设备通过噪声通道传输模型参数和在各种环境中收集数据，这给训练模型的泛化带来了挑战。此外，设备可以通过设备间通信进行分散式联邦学习，而连接设备的通信拓扑也影响模型的泛化能力。然而，大多数最近的理论研究在开展泛化分析时忽视了所有这些效应的纳入。与之相反，我们的工作提出了一种基于信息论的拓扑感知FEEL的泛化分析方法，考虑到了数据异构性和噪声通道的影响。此外，我们还提出了一种名为联邦全局互信息减少（FedGMIR）的新型正则化方法，以提高模型的性能。

    With the rapid growth of edge intelligence, the deployment of federated learning (FL) over wireless networks has garnered increasing attention, which is called Federated Edge Learning (FEEL). In FEEL, both mobile devices transmitting model parameters over noisy channels and collecting data in diverse environments pose challenges to the generalization of trained models. Moreover, devices can engage in decentralized FL via Device-to-Device communication while the communication topology of connected devices also impacts the generalization of models. Most recent theoretical studies overlook the incorporation of all these effects into FEEL when developing generalization analyses. In contrast, our work presents an information-theoretic generalization analysis for topology-aware FEEL in the presence of data heterogeneity and noisy channels. Additionally, we propose a novel regularization method called Federated Global Mutual Information Reduction (FedGMIR) to enhance the performance of models
    
[^5]: 通过群体不变性学习提高与人类偏好的对齐的泛化能力

    Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])

    [http://arxiv.org/abs/2310.11971](http://arxiv.org/abs/2310.11971)

    该论文提出了一种通过强化学习实现在不同数据组或领域中学习一致策略的方法，该方法可以提高AI助手对不同领域的泛化能力，并更好地与人类偏好对齐。

    

    基于语言模型(LLMs)的AI助手的成功在于强化学习从人类反馈中, 使生成的回答更加与人类偏好一致. 作为通用AI助手, 人们越来越期望它们在不同领域中表现一致. 然而, 先前的工作表明,强化学习(RL)经常利用捷径以获得较高的奖励, 忽略了具有挑战性的样本. 这种对快速奖励收益的关注不仅削弱了训练的稳定性, 也削弱了模型对新的未见数据的泛化能力. 在这项工作中, 我们提出了一种新颖的方法, 可以通过RL在不同数据组或领域中学习一致的策略. 鉴于获得群体标注的挑战, 我们的方法会自动将数据分类到不同的组中, 有意地最大化性能差异. 然后, 我们优化策略以在具有挑战性的组中表现良好. 最后, 利用已建立的

    The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
    
[^6]: 我们能编辑多模式大型语言模型吗？

    Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08475](http://arxiv.org/abs/2310.08475)

    本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。

    

    本文关注编辑多模式大型语言模型（MLLMs）。与编辑单模式LLMs相比，多模式模型的编辑更具挑战性，需要更高级别的审查和慎重考虑。为了促进这一领域的研究，我们构建了一个新的基准，称为MMEdit，用于编辑多模式LLMs，并建立了一套创新的度量标准进行评估。我们进行了包括各种模型编辑基线的综合实验，并分析了编辑多模式LLMs的不同组件的影响。根据经验，我们发现之前的基线在某种程度上可以实现编辑多模式LLMs，但效果仍然不理想，表明这个任务可能存在的困难。我们希望我们的工作能为NLP社区提供见解。代码和数据集可在https://github.com/zjunlp/EasyEdit获取。

    In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
    
[^7]: DNA序列生成的潜在扩散模型

    Latent Diffusion Model for DNA Sequence Generation. (arXiv:2310.06150v1 [cs.LG])

    [http://arxiv.org/abs/2310.06150](http://arxiv.org/abs/2310.06150)

    提出了一种新的潜在扩散模型DiscDiff，用于离散DNA序列生成。通过将离散DNA序列嵌入到连续的潜在空间中，利用连续扩散模型的强大生成能力来生成离散数据。同时，引入了一种新的度量标准FReD，用于评估DNA序列生成的样本质量。

    

    机器学习，尤其是深度生成模型的运用，为合成DNA序列生成领域打开了新的前景。虽然生成对抗网络（GANs）在这个应用中得到了广泛应用，但它们常常面临样本多样性有限和模式崩溃等问题。另一方面，扩散模型是一类有前景的新型生成模型，不受这些问题的困扰，使其能够在图像生成等领域达到最先进水平。基于此，我们提出了一种新颖的适用于离散DNA序列生成的潜在扩散模型DiscDiff。通过将离散DNA序列简单地嵌入到连续的潜在空间中使用自编码器，我们能够利用连续扩散模型的强大生成能力来生成离散数据。此外，我们引入了一种新的度量标准——Fr\'echet重建距离（FReD），用于评估DNA序列生成的样本质量。

    The harnessing of machine learning, especially deep generative models, has opened up promising avenues in the field of synthetic DNA sequence generation. Whilst Generative Adversarial Networks (GANs) have gained traction for this application, they often face issues such as limited sample diversity and mode collapse. On the other hand, Diffusion Models are a promising new class of generative models that are not burdened with these problems, enabling them to reach the state-of-the-art in domains such as image generation. In light of this, we propose a novel latent diffusion model, DiscDiff, tailored for discrete DNA sequence generation. By simply embedding discrete DNA sequences into a continuous latent space using an autoencoder, we are able to leverage the powerful generative abilities of continuous diffusion models for the generation of discrete data. Additionally, we introduce Fr\'echet Reconstruction Distance (FReD) as a new metric to measure the sample quality of DNA sequence gener
    
[^8]: 利用随机矩阵理论提高深度学习的准确性

    Enhancing Accuracy in Deep Learning Using Random Matrix Theory. (arXiv:2310.03165v1 [cs.LG])

    [http://arxiv.org/abs/2310.03165](http://arxiv.org/abs/2310.03165)

    本研究探索了随机矩阵理论在深度神经网络训练中的应用，通过层剪枝和损失曲面优化，实现了对DNN架构的简化和准确性的增强。通过奇异值分解，并根据随机矩阵理论的标准丢弃小的奇异值，可减少DNN层的参数，简化DNN架构，同时保持或增强模型的准确性。

    

    在本研究中，我们探索了随机矩阵理论在深度神经网络（DNN）训练中的应用，重点是通过层剪枝简化DNN架构和损失曲面。随机矩阵理论最近被用于解决深度学习中的过拟合问题，能够检查DNN的权重层谱。我们使用这些技术来在训练过程中通过奇异值分解（SVD）确定要从DNN的权重层中去除的奇异值的数量，这有助于简化DNN并提高准确性，在MNIST和Fashion MNIST数据集上培训简单的DNN模型得到了证明。我们的方法适用于预训练DNN的任何全连接或卷积层，减少了层的参数并简化了DNN的架构，同时保持甚至增强了模型的准确性。通过根据随机矩阵理论的标准丢弃小的奇异值，测试集的准确性保持一致，从而实现更有效的DNN训练。

    In this study, we explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning to simplify DNN architecture and loss landscape. RMT, recently used to address overfitting in deep learning, enables the examination of DNN's weight layer spectra. We use these techniques to optimally determine the number of singular values to be removed from the weight layers of a DNN during training via singular value decomposition (SVD). This process aids in DNN simplification and accuracy enhancement, as evidenced by training simple DNN models on the MNIST and Fashion MNIST datasets.  Our method can be applied to any fully connected or convolutional layer of a pretrained DNN, decreasing the layer's parameters and simplifying the DNN architecture while preserving or even enhancing the model's accuracy. By discarding small singular values based on RMT criteria, the accuracy of the test set remains consistent, facilitating more efficient DN
    
[^9]: 迭代式规划中的选项发现

    Iterative Option Discovery for Planning, by Planning. (arXiv:2310.01569v1 [cs.AI])

    [http://arxiv.org/abs/2310.01569](http://arxiv.org/abs/2310.01569)

    本文提出了一种迭代式选项发现方法，通过学习一组局部强策略来指导搜索算法，从而在强化学习和规划中应用于复杂领域。

    

    发现有用的时间抽象，也就是选项，被广泛认为是将强化学习和规划应用于日益复杂的领域的关键。在AlphaZero中使用的Expert Iteration策略学习方法的经验成功基础上，提出了Option Iteration，一种类似的选项发现方法。Option Iteration不是学习一个单一的强策略，而是学习一组选项策略，对于遇到的每个状态，至少有一种策略在某个未来的时间点与搜索结果吻合。直观地说，这可能更容易，因为它允许算法根据情况灵活调整，而不是学习一个在当前状态的细节上具有复杂依赖性的全局策略。通过学习这样一组局部强策略，我们可以使用它们来指导搜索算法，从而形成良性循环，获得更好的结果。

    Discovering useful temporal abstractions, in the form of options, is widely thought to be key to applying reinforcement learning and planning to increasingly complex domains. Building on the empirical success of the Expert Iteration approach to policy learning used in AlphaZero, we propose Option Iteration, an analogous approach to option discovery. Rather than learning a single strong policy that is trained to match the search results everywhere, Option Iteration learns a set of option policies trained such that for each state encountered, at least one policy in the set matches the search results for some horizon into the future. Intuitively, this may be significantly easier as it allows the algorithm to hedge its bets compared to learning a single globally strong policy, which may have complex dependencies on the details of the current state. Having learned such a set of locally strong policies, we can use them to guide the search algorithm resulting in a virtuous cycle where better 
    
[^10]: 地震图变压器：用于多种地震监测任务的通用深度学习骨干网络

    Seismogram Transformer: A generic deep learning backbone network for multiple earthquake monitoring tasks. (arXiv:2310.01037v2 [physics.geo-ph] UPDATED)

    [http://arxiv.org/abs/2310.01037](http://arxiv.org/abs/2310.01037)

    本文介绍了一种名为地震图变压器（SeisT）的通用深度学习骨干网络模型，用于多种地震监测任务。SeisT的高效网络架构使其在地震检测、地震相位拾取、首次运动极性分类、震级估计和反方位角估计等任务中表现优秀，特别是在泛化性能方面。

    

    地震记录，即地震图，是由地震事件引起的地面运动的重要记录，是地震研究和监测的基础。深度学习的最新进展极大地促进了各种地震信号处理任务。本文介绍了一种新颖的骨干神经网络模型，用于各种地震监测任务，名为地震图变压器（SeisT）。由于其高效的网络架构，SeisT在地震检测、地震相位拾取、首次运动极性分类、震级估计和反方位角估计等任务中能够与甚至超过最先进的模型匹配，特别是在超出分布的泛化性能方面。SeisT由多个网络层组成，这些层由不同的基础模块组成，帮助模型理解地震图的多层次特征表达，从低层次到高层次的复杂特征，有效地提取特征。

    Seismic records, known as seismograms, are crucial records of ground motion resulting from seismic events, constituting the backbone of earthquake research and monitoring. The latest advancements in deep learning have significantly facilitated various seismic signal processing tasks. This paper introduces a novel backbone neural network model designed for various seismic monitoring tasks, named Seismogram Transformer (SeisT). Thanks to its efficient network architecture, SeisT matches or even outperforms the state-of-the-art models in earthquake detection, seismic phase picking, first-motion polarity classification, magnitude estimation, and back-azimuth estimation tasks, particularly in terms of out-of-distribution generalization performance. SeisT consists of multiple network layers composed of different foundational blocks, which help the model understand multi-level feature representations of seismograms from low-level to high-level complex features, effectively extracting features
    
[^11]: 贝叶斯设计原则用于频率式序贯学习问题

    Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00806](http://arxiv.org/abs/2310.00806)

    该论文提出了一种贝叶斯设计原则用于优化频率式序贯学习问题的通用理论，通过生成“算法信念”并使用贝叶斯后验进行决策，实现了无先验的贝叶斯类型算法在对抗性环境中的泛化和优化应用。

    

    我们提出了一种通用理论，用于优化频率误差求和的序贯学习问题，可以通过统一的贝叶斯原则得到高效的强化学习和赌博机算法。我们提出了一种新颖的优化方法，在每一轮生成“算法信念”，并使用贝叶斯后验进行决策。我们提出的优化目标是创建“算法信念”，我们将其称为“算法信息比”，有效地表征了任何算法的频率误差的内在复杂度度量。据我们所知，这是第一种将贝叶斯式算法无先验地并且在对抗性环境中以通用和最优方式应用的系统方法。此外，这些算法简单且通常容易实现。作为一个重要应用，我们提出了一种新的多臂赌博机算法，在随机、对抗和非...

    We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non
    
[^12]: 递归超网络在元强化学习中表现出惊人的强大性能

    Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])

    [http://arxiv.org/abs/2309.14970](http://arxiv.org/abs/2309.14970)

    递归超网络和循环神经网络在元强化学习中的端到端学习表现出惊人的强大性能，相比于现有专门方法更为简单但效果更好。

    

    深度强化学习在实际应用时因样本效率低而不易部署。元强化学习通过学习在元训练时利用相关任务的分布来实现少样本学习，直接解决了这个样本效率问题。最近的研究表明，与专门的元强化学习方法相比，与一个通用的序列模型（如循环神经网络）结合的端到端学习是一个令人惊讶的强基准。然而，这样的观点由于有限的支持证据而引起了争议，特别是在之前的研究中确立了完全相反的观点。在本文中，我们进行了实证研究。虽然我们同样发现循环网络可以达到强大的性能，但我们证明了超网络的使用对于发挥循环基线的潜力至关重要。令人惊讶的是，与超网络相结合时，这种远比现有专门方法简单的循环基准实际上能取得更好的表现。

    Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
    
[^13]: 深度知识追踪是一个隐含的动态多维项目反应理论模型

    Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model. (arXiv:2309.12334v1 [cs.CY])

    [http://arxiv.org/abs/2309.12334](http://arxiv.org/abs/2309.12334)

    本文将深度知识追踪视为一种编码器-解码器结构，通过在多个数据集上进行实验发现，一个更简单的解码器可以比DKT预测学生表现更好。

    

    知识追踪是根据学生先前问题的表现来预测其在新问题上的表现，这可以作为优化评估和学习的先前步骤。深度知识追踪（DKT）是一种基于循环神经网络的知识追踪竞争模型，即使一些更简单的模型可能与其性能相匹配，但对于为什么DKT能够如此成功的了解还很少。在本文中，我们将深度知识追踪视为一种编码器-解码器结构。这个观点不仅使我们能够在性能、简单性或表达性方面提出更好的模型，还为未来的研究方向打开了有希望的途径。特别是，我们在几个小型和大型数据集上展示了一个更简单的解码器，可能比DKT使用的解码器参数更少，可以更好地预测学生表现。

    Knowledge tracing consists in predicting the performance of some students on new questions given their performance on previous questions, and can be a prior step to optimizing assessment and learning. Deep knowledge tracing (DKT) is a competitive model for knowledge tracing relying on recurrent neural networks, even if some simpler models may match its performance. However, little is known about why DKT works so well. In this paper, we frame deep knowledge tracing as a encoderdecoder architecture. This viewpoint not only allows us to propose better models in terms of performance, simplicity or expressivity but also opens up promising avenues for future research directions. In particular, we show on several small and large datasets that a simpler decoder, with possibly fewer parameters than the one used by DKT, can predict student performance better.
    
[^14]: FedDCSR: 通过解缠表示学习实现联邦跨领域顺序推荐

    FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])

    [http://arxiv.org/abs/2309.08420](http://arxiv.org/abs/2309.08420)

    提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。

    

    近年来，利用来自多个领域的用户序列数据的跨领域顺序推荐(CSR)受到了广泛关注。然而，现有的CSR方法需要在领域之间共享原始用户数据，这违反了《通用数据保护条例》(GDPR)。因此，有必要将联邦学习(FL)和CSR相结合，充分利用不同领域的知识，同时保护数据隐私。然而，不同领域之间的序列特征异质性对FL的整体性能有显著影响。在本文中，我们提出了FedDCSR，这是一种通过解缠表示学习的新型联邦跨领域顺序推荐框架。具体而言，为了解决不同领域之间的序列特征异质性，我们引入了一种称为领域内-领域间序列表示解缠(SRD)的方法，将用户序列特征解缠成领域共享和领域专属特征。

    Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
    
[^15]: 高斯过程与线性多核：频谱设计和多维数据的分布式学习

    Gaussian Processes with Linear Multiple Kernel: Spectrum Design and Distributed Learning for Multi-Dimensional Data. (arXiv:2309.08201v1 [cs.LG])

    [http://arxiv.org/abs/2309.08201](http://arxiv.org/abs/2309.08201)

    本文研究了高斯过程与线性多核在多维数据上的应用，提出了一种新的格点谱混合核公式，减少了超参数数量，同时保留了优化结构和逼近能力。通过引入分布式算法，使大规模超参数优化变得可行。

    

    高斯过程（GPs）已成为机器学习和信号处理的重要技术。GP建模的关键组成部分是核函数的选择，线性多核（LMKs）因其强大的建模能力和可解释性而成为一个吸引人的核函数类。本文重点研究格点谱混合（GSM）核，它是一种可以近似任意平稳核的LMK。具体来说，我们提出了一种新的GSM核公式，用于多维数据，相比现有公式减少了超参数的数量，同时保留了有利的优化结构和逼近能力。此外，为了使GSM核中的大规模超参数优化变得可行，我们首先引入了分布式SCA（DSCA）算法。在此基础上，我们基于交替方向乘子法（ADMM）框架提出了双重分布式SCA（D$^2$SCA）算法，使我们能够合作地进行优化。

    Gaussian processes (GPs) have emerged as a prominent technique for machine learning and signal processing. A key component in GP modeling is the choice of kernel, and linear multiple kernels (LMKs) have become an attractive kernel class due to their powerful modeling capacity and interpretability. This paper focuses on the grid spectral mixture (GSM) kernel, an LMK that can approximate arbitrary stationary kernels. Specifically, we propose a novel GSM kernel formulation for multi-dimensional data that reduces the number of hyper-parameters compared to existing formulations, while also retaining a favorable optimization structure and approximation capability. In addition, to make the large-scale hyper-parameter optimization in the GSM kernel tractable, we first introduce the distributed SCA (DSCA) algorithm. Building on this, we propose the doubly distributed SCA (D$^2$SCA) algorithm based on the alternating direction method of multipliers (ADMM) framework, which allows us to cooperativ
    
[^16]: Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])

    Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])

    [http://arxiv.org/abs/2309.07867](http://arxiv.org/abs/2309.07867)

    beta扩散是一种新型生成模型方法，通过引入去掩盖和去噪的技术，利用缩放和偏移的beta分布进行乘法转换，实现在有界范围内生成数据。相比于传统的基于扩散的生成模型，它通过KL散度上界进行优化，证明了效果更好。

    

    我们引入了beta扩散，一种将去掩盖和去噪集成到一起的新型生成建模方法，用于在有界范围内生成数据。使用了缩放和偏移的beta分布，beta扩散利用了随时间的乘法转换来创建正向和反向的扩散过程，同时维持着正向边缘分布和反向条件分布，给定任意时间点的数据。与传统的基于扩散的生成模型不同，传统模型依赖于加性高斯噪声和重新加权的证据下界（ELBO），beta扩散是乘法的，并且通过从KL散度的凸性推导出来的KL散度上界（KLUB）进行优化。我们证明了所提出的KLUB相对于负ELBO来说对于优化beta扩散更加有效，负ELBO也可以作为相同KL散度的KLUB，只是其两个参数交换了位置。beta扩散的损失函数以Bregman散度为指标来表示。

    We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
    
[^17]: 使用带轻量级注意机制的迁移ResNet增强乳腺癌分类

    Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])

    [http://arxiv.org/abs/2308.13150](http://arxiv.org/abs/2308.13150)

    本文介绍了一种使用ResNet模型和轻量级注意机制框架的图像分类方法，通过优化特征表示、增强分类能力和改善特征可辨别性，在乳腺癌分类任务上显示出卓越性能和潜在应用前景。

    

    深度学习模型通过学习原始像素数据中的复杂特征层次结构，彻底改变了图像分类。本文介绍了一种基于ResNet模型的图像分类方法，并引入了轻量级注意机制框架来提高性能。该框架优化特征表示，增强分类能力，改善特征可辨别性。我们在Breakhis数据集上验证了算法的有效性，在许多方面显示出卓越的性能。我们的方法不仅在传统模型方面表现优越，在当代视觉变换器等最新方法上也显示出优势。在诸如精度、准确度、召回率、F1分数和G-means等指标上取得了显著改进，同时在收敛时间方面表现良好。这些结果增强了算法的性能，巩固了其在实际图像分类任务中的应用前景。

    Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
    
[^18]: 数据驱动的决策聚焦代理建模

    Data-driven decision-focused surrogate modeling. (arXiv:2308.12161v1 [math.OC])

    [http://arxiv.org/abs/2308.12161](http://arxiv.org/abs/2308.12161)

    这项研究提出了决策聚焦代理建模的概念，通过学习一个简化的凸优化模型来最小化原始和代理优化模型之间的决策预测误差，并在实时环境中解决计算上具有挑战性的非线性优化问题。

    

    我们引入了决策聚焦代理建模的概念，用于在实时环境中解决计算上具有挑战性的非线性优化问题。提出的数据驱动框架旨在学习一个更简单的、例如凸优化模型，该模型经过训练以最小化决策预测误差，决策预测误差定义为原始和代理优化模型的最优解之间的差异。学习问题被构建为一个双层规划问题，可以看作是一个数据驱动的逆优化问题，我们采用了以前工作中的基于分解的求解算法。通过涉及常见非线性化学过程（如化学反应器、换热器网络和材料混合系统）的数值实验验证了我们的框架。我们还将决策聚焦代理建模与标准的数据驱动代理建模方法进行了详细比较，并证明了我们的方法的优越性。

    We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our appro
    
[^19]: 受限斯坦变分轨迹优化

    Constrained Stein Variational Trajectory Optimization. (arXiv:2308.12110v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2308.12110](http://arxiv.org/abs/2308.12110)

    CSVTO是一种受限斯坦变分轨迹优化算法，它通过斯坦变分梯度下降方法生成多样的约束满足轨迹集合，提高了在具有任意约束的问题中的优化性能和鲁棒性。

    

    我们提出了一种受限斯坦变分轨迹优化（CSVTO）算法，用于在一组轨迹上进行带约束的轨迹优化。我们将受限轨迹优化视为一种新颖的对轨迹分布约束的函数最小化形式，避免将约束视为目标函数的惩罚，从而使我们能够生成多样的满足约束的轨迹集合。我们的方法使用斯坦变分梯度下降（SVGD）寻找一组粒子，近似表示一个低成本轨迹的分布，并遵守约束。CSVTO适用于具有任意等式和不等式约束的问题，并包括一种新颖的粒子重新采样步骤来避免局部最小值。通过明确生成多样的轨迹集合，CSVTO能够更好地避免不良的局部最小值，并且对初始化更具鲁棒性。我们证明，CSVTO在具有高度约束的挑战性问题上优于基线方法。

    We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constr
    
[^20]: 多文档问答中的知识图谱引导

    Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])

    [http://arxiv.org/abs/2308.11730](http://arxiv.org/abs/2308.11730)

    这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。

    

    大型语言模型（LLMs）的“预训练、提示、预测”范式在开放域问答（OD-QA）中取得了显著的成功。然而，很少有工作在多文档问答（MD-QA）场景下探索这个范式，这是一个要求对不同文档的内容和结构之间的逻辑关联有深入理解的任务。为了填补这一重要的空白，我们提出了一种知识图谱引导（KGP）方法，用于在MD-QA中为LLMs提示正确的上下文，该方法包括图构建模块和图遍历模块。对于图构建，我们使用节点来表示文段或文档结构（例如，页面/表格），而使用边来表示文段之间的语义/词汇相似性或者文档内的结构关系。对于图遍历，我们设计了一个基于LM的图遍历器，它在节点之间导航并收集支持性的文段，以帮助LLMs在MD-QA中进行答案预测。

    The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
    
[^21]: 固定积分神经网络

    Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])

    [http://arxiv.org/abs/2307.14439](http://arxiv.org/abs/2307.14439)

    本文介绍了一种方法来通过神经网络表示学习函数的解析积分，具有计算精确积分的能力，并能将约束直接应用于积分，而且还介绍了将学习函数约束为正的方法以及相关应用。

    

    将学习函数通过神经网络进行积分是非常有用的，但是这种积分通常是通过数值方法来计算的，因为解析计算积分过程复杂，尤其是对于神经网络这样的学习函数。本文介绍了一种表示学习函数 $f$ 解析积分的方法。这允许精确计算神经网络的积分，并且通过将约束直接应用于积分来对约束神经网络进行参数化。关键的是，我们还介绍了一种将 $f$ 约束为正的方法，这是许多应用（例如概率分布、距离度量等）所必需的条件。最后，我们介绍了几个可以利用我们的固定积分神经网络（FINN）的应用领域。

    It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
    
[^22]: 基于投票的多模态自动欺骗检测

    Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])

    [http://arxiv.org/abs/2307.07516](http://arxiv.org/abs/2307.07516)

    本文提出了一种基于投票的多模态方法用于自动欺骗检测，通过视频的音频、视觉和文本特征进行检测。实验结果表明，我们的解决方案在欺骗检测中表现优于现有技术。

    

    自动欺骗检测一直是一个热门的研究课题，利用机器学习和深度学习自动检测欺骗给这一旧领域带来了新的光明。在本文中，我们提出了一种基于投票的方法，用于从视频中使用音频、视觉和文本特征进行自动欺骗检测。我们在两个数据集上进行了实验，分别是密歇根大学的真实试验数据集和迈阿密大学的欺骗检测数据集。视频样本被分成图像、音频和手稿的帧。我们提出的多模态投票解决方案包括三个模型。第一个模型是用于从图像中检测欺骗的卷积神经网络（CNN），第二个模型是用于从音频中检测欺骗的Mel频谱图上的支持向量机（SVM），第三个模型是用于从手稿中检测欺骗的支持向量机（SVM）上的Word2Vec。我们提出的解决方案优于现有技术水平。在图像、音频和文本上取得的最佳结果分别为97％、96％、9

    Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
    
[^23]: 学习核技术用于可解释和高效的PPG信号质量评估和伪影分割

    Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])

    [http://arxiv.org/abs/2307.05385](http://arxiv.org/abs/2307.05385)

    本文提出了一种通过学习核技术，具有解释性且参数较少的方法来评估和分割PPG信号的质量和伪影，与现有的深度神经网络方法相比有着类似甚至更好的性能。

    

    光电容抗(PPG)提供了一种低成本、非侵入性的方法来持续监测各种心血管参数。PPG信号由可穿戴设备产生，常常包含由外部因素(如人体运动)引起的大型伪影。为了确保对生理参数进行稳健和准确的提取，信号的损坏区域需要被正确地识别和处理。之前的方法依靠手工特征检测器或信号度量，结果性能不佳，或依靠深度神经网络(DNN)等机器学习技术，缺乏可解释性，计算和内存密集。在这项工作中，我们提出了一种新的方法，学习一小组可解释的卷积核，其性能与现有技术DNN方法相似，甚至更好，而参数数量比DNN方法少几个数量级。这项工作实现了高效、稳健和可解释的PPG信号质量评估和伪影分割。

    Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
    
[^24]: CPDG: 一种用于动态图神经网络的对比式预训练方法

    CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks. (arXiv:2307.02813v1 [cs.LG])

    [http://arxiv.org/abs/2307.02813](http://arxiv.org/abs/2307.02813)

    这篇论文提出了一种对比式预训练方法（CPDG）用于动态图神经网络（DGNNs），通过灵活的结构-时序子图采样和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战，实验证明CPDG在各种下游任务中的动态图预训练方面优于现有方法。

    

    近年来，由于动态图中蕴含丰富信息并在实际场景中得到广泛应用，动态图数据挖掘变得越来越流行。尽管动态图神经网络（DGNNs）取得了一定的进展，但其丰富的信息和多样的下游任务给在工业情景中实际应用带来了显著困难。因此，在本文中，我们提出了对比式预训练方法（CPDG）来解决这些问题。CPDG通过一种灵活的结构-时序子图采样器和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战。在大规模的研究和工业动态图数据集上进行的大量实验表明，CPDG在各种下游任务中的动态图预训练方面优于现有方法。

    Dynamic graph data mining has gained popularity in recent years due to the rich information contained in dynamic graphs and their widespread use in the real world. Despite the advances in dynamic graph neural networks (DGNNs), the rich information and diverse downstream tasks have posed significant difficulties for the practical application of DGNNs in industrial scenarios. To this end, in this paper, we propose to address them by pre-training and present the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG). CPDG tackles the challenges of pre-training for DGNNs, including generalization and long-short term modeling capability, through a flexible structural-temporal subgraph sampler along with structural-temporal contrastive pre-training schemes. Extensive experiments conducted on both large-scale research and industrial dynamic graph datasets show that CPDG outperforms existing methods in dynamic graph pre-training for various downstream tasks under three transf
    
[^25]: 大型语言模型赋能连接智能的自主边缘AI

    Large Language Models Empowered Autonomous Edge AI for Connected Intelligence. (arXiv:2307.02779v1 [cs.IT])

    [http://arxiv.org/abs/2307.02779](http://arxiv.org/abs/2307.02779)

    大型语言模型赋能连接智能的自主边缘AI系统，通过云-边缘-客户端的分层架构和强大的GPT模型能力，实现高质量、低延迟、隐私保护的AI服务，满足用户个人需求并实现自动化。

    

    无线网络的发展朝着连接智能的方向发展，这一概念设想了在超连接的网络物理世界中，人类、物体和智能之间实现无缝互联。边缘AI作为实现连接智能的有希望的解决方案，通过在网络边缘提供高质量、低延迟和隐私保护的AI服务。在本文中，我们介绍了一种自主边缘AI系统，该系统自动组织、适应和优化自己以满足用户的各种需求。该系统采用云-边缘-客户端的分层架构，其中大型语言模型——生成式预训练变换器（GPT）存放在云端，其他AI模型被共同部署在设备和边缘服务器上。通过利用GPT在语言理解、规划和代码生成方面的强大能力，我们提出了一个多功能的框架，有效协调边缘AI模型以满足用户的个人需求，同时实现自动化。

    The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automati
    
[^26]: TransformerG2G：使用Transformer进行自适应时间步长的学习时态图嵌入

    TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])

    [http://arxiv.org/abs/2307.02588](http://arxiv.org/abs/2307.02588)

    TransformerG2G是一种使用Transformer进行自适应时间步长的图嵌入模型，通过学习历史上的长程依赖关系，准确地捕捉时态图的动态特征。

    

    动态图嵌入已成为处理不同时间图分析任务（如链路预测、节点分类、推荐系统、异常检测和图生成）的一种非常有效的技术，广泛应用于各种应用领域。这些时态图展现了异质的瞬时动态、不同的时间间隔以及在演化过程中高度变化的节点特征。因此，将历史图上的长程依赖融入到学习时态动态的过程中至关重要。本文提出了一个带有不确定性量化的图嵌入模型TransformerG2G，通过利用先进的Transformer编码器从当前状态（$t$）和之前的上下文（时间戳[$t-1, t-l$]，$l$是上下文的长度）中首先学习中间节点表示。此外，我们采用两个投影层来生成每个节点的低维多元高斯分布，作为其潜在嵌入。

    Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at ti
    
[^27]: 一种通过移除生胎更新流数据在线量化的高效简单方法

    An efficient and straightforward online quantization method for a data stream through remove-birth updating. (arXiv:2306.12574v1 [cs.LG])

    [http://arxiv.org/abs/2306.12574](http://arxiv.org/abs/2306.12574)

    本文提出了一种在线量化数据流的方法，通过移除生胎更新快速适应概念漂移，可以产生最小化的死单元，并为漂移检测提供了一些有用的度量指标。

    

    网络设备的增长正在创造出大量数据，即所谓的大数据，并对有效数据分析提出了重要挑战。这些数据不断地产生，形成了动态流数据。流数据的特征可能会动态变化，这种变化被称为概念漂移。因此，处理流数据的方法必须在动态适应这些变化的同时，高效地缩减它们的体积。本文提出了一种简单的概念漂移在线向量量化方法。该方法通过移除生胎更新识别并替换概率低的单元，从而快速适应概念漂移。此外，研究结果表明，即使在概念漂移的情况下，该方法也可以产生最小化的死单元。本研究还表明一些通过所提出方法计算出来的度量指标对于漂移检测将是有益的。

    The growth of network-connected devices is creating an explosion of data, known as big data, and posing significant challenges to efficient data analysis. This data is generated continuously, creating a dynamic flow known as a data stream. The characteristics of a data stream may change dynamically, and this change is known as concept drift. Consequently, a method for handling data streams must efficiently reduce their volume while dynamically adapting to these changing characteristics. This paper proposes a simple online vector quantization method for concept drift. The proposed method identifies and replaces units with low win probability through remove-birth updating, thus achieving a rapid adaptation to concept drift. Furthermore, the results of this study show that the proposed method can generate minimal dead units even in the presence of concept drift. This study also suggests that some metrics calculated from the proposed method will be helpful for drift detection.
    
[^28]: 二次型赌臂机的样本复杂度：Hessian相关性界限和最优算法

    Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms. (arXiv:2306.12383v1 [cs.LG])

    [http://arxiv.org/abs/2306.12383](http://arxiv.org/abs/2306.12383)

    本文针对随机零阶优化的问题，提出了二次型目标函数及其局部几何结构的最优Hessian相关样本复杂度，从信息论角度提供Hessian相关复杂度的下界，并提供了一种Hessian无关的算法可普遍实现所有Hessian实例的渐近最优样本复杂度。

    

    在随机零阶优化中，了解如何充分利用底层目标函数的局部几何结构是一个实际相关的问题。我们考虑一种基本情况，即目标函数是二次型的，并且提供了最优Hessian相关样本复杂度的第一个紧密刻画。我们的贡献具有双重性质。首先，从信息论的角度出发，通过引入一种称为能量分配的概念来捕捉搜索算法和目标函数几何结构之间的交互，证明了Hessian相关复杂度的紧密下界。通过解决最优能量谱，得到了配套的上限。其次，算法方面，我们展示了存在一种Hessian无关的算法，能够普遍实现所有Hessian实例的渐近最优样本复杂度。我们算法能够实现的渐近最优样本复杂度对于重尾噪声分布仍然有效。

    In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called energy allocation, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributio
    
[^29]: OpenGSL: 一项针对图结构学习的综合基准测试研究

    OpenGSL: A Comprehensive Benchmark for Graph Structure Learning. (arXiv:2306.10280v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10280](http://arxiv.org/abs/2306.10280)

    OpenGSL是第一个针对图结构学习的综合基准测试，旨在解决GSL领域中由于实验协议不一致而导致的进展不明确的问题。

    

    图神经网络(GNNs)已成为图表示学习的事实标准，因为它们能有效地整合图的拓扑结构和节点属性。然而，由于图的复杂和依赖形成过程导致的节点连接的固有次优性质，在对其进行建模方面存在着重大挑战。为了解决这个问题，近年来图结构学习(GSL)作为一种数据中心化学习方法已经引起了广泛关注。GSL的核心概念是同时优化图结构和对应的GNN模型。尽管提出了许多GSL方法，但由于实验协议不一致，包括数据集的变化、数据处理技术和分割策略的差异，该领域的进展仍不清楚。在本文中，我们介绍了OpenGSL，这是第一个针对GSL的综合基准测试，并旨在填补这一空白。OpenGSL提供了一个公平的比较平台，使研究人员能够在统一的设置下评估不同的GSL方法。

    Graph Neural Networks (GNNs) have emerged as the de facto standard for representation learning on graphs, owing to their ability to effectively integrate graph topology and node attributes. However, the inherent suboptimal nature of node connections, resulting from the complex and contingent formation process of graphs, presents significant challenges in modeling them effectively. To tackle this issue, Graph Structure Learning (GSL), a family of data-centric learning approaches, has garnered substantial attention in recent years. The core concept behind GSL is to jointly optimize the graph structure and the corresponding GNN models. Despite the proposal of numerous GSL methods, the progress in this field remains unclear due to inconsistent experimental protocols, including variations in datasets, data processing techniques, and splitting strategies. In this paper, we introduce OpenGSL, the first comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables a fair compa
    
[^30]: 一种用于评估图像识别模型鲁棒性的差分测试框架

    A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])

    [http://arxiv.org/abs/2306.06208](http://arxiv.org/abs/2306.06208)

    本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。

    

    图像识别任务通常使用深度学习，并需要巨大的处理能力，因此依赖于GPU和TPU等硬件加速器进行快速、及时的处理。在模型部署过程中，硬件加速器上的子优映射可能会导致实时图像识别任务失败，从而导致时间不确定性和错误行为。硬件加速器上的映射是通过多个软件组件进行的，例如深度学习框架、编译器、设备库等，我们称之为计算环境。随着图像识别任务在自动驾驶和医疗成像等安全关键应用中的增加，评估它们对计算环境变化的鲁棒性至关重要，因为深度学习框架、编译器优化和硬件设备等参数对模型性能和正确性的影响还不太清楚。在本文中，我们提出了一种差分测试框架，用于评估图像识别模型对计算环境变化的鲁棒性。我们的框架使用一组参考图像，并通过更改软件组件来扰动计算环境，生成具有已知预测输出差异的图像。通过比较原始图像和扰动图像的预测输出，我们可以确定模型性能是否受到计算环境变化的影响。我们通过测试三个图像识别模型的鲁棒性来证明我们框架的有效性，并确定其在计算环境变化下的性能受到影响的情况。

    Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
    
[^31]: 图像识别模型框架转换的故障定位

    Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])

    [http://arxiv.org/abs/2306.06157](http://arxiv.org/abs/2306.06157)

    本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。

    

    在部署深度神经网络（DNNs）时，开发人员经常将模型从一个深度学习框架转换为另一个（例如，从TensorFlow到PyTorch）。然而，这个过程容易出错，并可能影响目标模型的准确性。为了确定这种影响的程度，我们对三个用于图像识别的DNNs（MobileNetV2、ResNet101和InceptionV3）进行了不同的分析，这些模型在四个深度学习框架（PyTorch、Keras、TensorFlow（TF）和TFLite）之间进行了转换，并发现了许多模型崩溃和输出标签差异高达100％。为了缓解这种错误，我们提出了一种新的方法来定位故障和修复有缺陷的深度学习框架转换，重点放在预训练的图像识别模型上。我们的技术包括四个主要分析阶段：1）转换工具，2）模型参数，3）模型超参数，4）图表示。此外，我们提出了许多针对故障定位和修复的策略，包括转换工具的推荐、调试技巧以及模型超参数的微调。我们通过成功修复所有测试的深度学习框架中MobileNetV2，ResNet101和InceptionV3 的有缺陷的转换来展示我们方法的有效性。

    When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
    
[^32]: Barron型空间的嵌入不等式

    Embedding Inequalities for Barron-type Spaces. (arXiv:2305.19082v1 [stat.ML])

    [http://arxiv.org/abs/2305.19082](http://arxiv.org/abs/2305.19082)

    本文测量了Barron空间和谱Barron空间之间的关系，并提供了嵌入不等式。

    

    深度学习理论中的一个基本问题是理解高维条件下两层神经网络的逼近和泛化性质。为了解决这个问题，研究人员引入了Barron空间$\mathcal{B}_s(\Omega)$和谱Barron空间$\mathcal{F}_s(\Omega)$，其中指数$s$表征了这些空间中函数的平滑性，$\Omega\subset\mathbb{R}^d$表示输入域。然而，两种类型的Barron空间之间的关系仍不清楚。本文通过以下不等式建立了这些空间之间的连续嵌入：对于任意$\delta\in(0,1),s\in\mathbb{N}^{+}$和$f:\Omega \mapsto \mathbb{R}$，都有\[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \]其中$\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$，$\lesssim_s$表示仅与平滑参数$s$有关的常数。

    One of the fundamental problems in deep learning theory is understanding the approximation and generalization properties of two-layer neural networks in high dimensions. In order to tackle this issue, researchers have introduced the Barron space $\mathcal{B}_s(\Omega)$ and the spectral Barron space $\mathcal{F}_s(\Omega)$, where the index $s$ characterizes the smoothness of functions within these spaces and $\Omega\subset\mathbb{R}^d$ represents the input domain. However, it is still not clear what is the relationship between the two types of Barron spaces. In this paper, we establish continuous embeddings between these spaces as implied by the following inequality: for any $\delta\in (0,1), s\in \mathbb{N}^{+}$ and $f: \Omega \mapsto\mathbb{R}$, it holds that \[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \] where $\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$ and notab
    
[^33]: 使用编译器生成的强化学习反馈调整代码模型

    Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback. (arXiv:2305.18341v1 [cs.PL])

    [http://arxiv.org/abs/2305.18341](http://arxiv.org/abs/2305.18341)

    本文提出了一种叫做RLCF的方法，使用代码编译器反馈进一步训练预训练的大型语言模型，以生成符合目标分布的代码，并通过所有静态正确性检查，显著提高了性能。

    

    最近，对代码进行预训练的大型语言模型成为程序合成的主要方法。然而，这些模型生成的代码可能违反基本的语言级别不变性，从而降低下游任务的性能。本文提出了一种称为RLCF的方法，它使用代码编译器的反馈进一步训练预训练的大型语言模型。 RLCF将LLM视为通过RL代理逐步生成代码，并接收以下反馈：（i）编译器派生的反馈与所生成的代码是否通过一组正确性检查有关; （ii）不同LLM的反馈，与训练语料库中一组参考程序相似。这些反馈机制帮助所生成的代码在通过所有静态正确性检查的同时保持在目标分布中。RLCF是模型和语言无关的。我们在Java的MBJP和MathQA任务上进行了实证评估，实验结果表明，RLCF显著提高了性能。

    Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called RLCF, that further trains a pre-trained LLM using feedback from a code compiler. RLCF views the LLM as an RL agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF significantly raise
    
[^34]: 从理论角度揭示“思维链”背后的奥秘

    Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])

    [http://arxiv.org/abs/2305.15408](http://arxiv.org/abs/2305.15408)

    本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。

    

    最近的研究发现，"思维链"提示能够显著提高大型语言模型（LLMs）的性能，特别是在涉及数学或推理的复杂任务中。尽管获得了巨大的实证成功，但“思维链”背后的机制以及它如何释放LLMs的潜力仍然是神秘的。本文首次从理论上回答了这些问题。具体而言，我们研究了LLMs带有“思维链”在解决基本数学和决策问题中的能力。我们首先给出一个不可能的结果，表明任何有限深度的Transformer都不能直接输出正确的基本算术/方程任务的答案，除非模型大小随着输入长度的增加呈超多项式增长。相反，我们通过构造证明，大小恒定的自回归Transformer足以通过使用常用的数学语言形式生成“思维链”推导来解决这两个任务。

    Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
    
[^35]: 学习率无关的约束域Bayesian推断

    Learning Rate Free Bayesian Inference in Constrained Domains. (arXiv:2305.14943v1 [stat.ML])

    [http://arxiv.org/abs/2305.14943](http://arxiv.org/abs/2305.14943)

    我们的算法是学习率无关的约束域采样算法，并提出了一个统一框架，能够处理多种约束采样问题，实现了与现有算法相竞争的性能。

    

    我们引入了一套新的基于粒子的算法，用于在约束域内进行采样，这是完全与学习率无关的。我们的方法利用了凸优化中的硬币投注思想，以及约束采样作为概率测度空间上镜像优化问题的观点。基于这个观点，我们还提出了一个统一框架，用于几种现有的约束采样算法，包括镜像Langevin动力学和镜像Stein变分梯度下降。我们在一系列的数值实验中展示了算法的性能，包括从单纯形目标进行采样、带公平性约束进行采样以及后选择推断中的约束采样问题。我们的结果表明，我们的算法在不需要调整任何超参数的情况下，实现了与现有约束采样方法相竞争的性能。

    We introduce a suite of new particle-based algorithms for sampling on constrained domains which are entirely learning rate free. Our approach leverages coin betting ideas from convex optimisation, and the viewpoint of constrained sampling as a mirrored optimisation problem on the space of probability measures. Based on this viewpoint, we also introduce a unifying framework for several existing constrained sampling algorithms, including mirrored Langevin dynamics and mirrored Stein variational gradient descent. We demonstrate the performance of our algorithms on a range of numerical examples, including sampling from targets on the simplex, sampling with fairness constraints, and constrained sampling problems in post-selection inference. Our results indicate that our algorithms achieve competitive performance with existing constrained sampling methods, without the need to tune any hyperparameters.
    
[^36]: GQA:从多头检查点训练广义多查询Transformer模型

    GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13245](http://arxiv.org/abs/2305.13245)

    该论文介绍了一种将现有的多头语言模型检查点升级为具有多查询注意力（MQA）的模型的方法，并引入了群组查询注意力（GQA）来解决MQA可能导致的质量下降问题。通过升级后的GQA模型，实现了接近多头注意力的质量，并具备与MQA相当的速度。

    

    多查询注意力（MQA）仅使用一个键值头，大大加快了解码器推理速度。然而，MQA可能导致质量下降，并且为了更快地推理而训练一个单独的模型可能不是理想的。我们（1）提出了一个方法，利用原始预训练计算量的5％，将现有的多头语言模型检查点升级为具有MQA的模型，并（2）引入了群组查询注意力（GQA），它是多查询注意力的广义形式，使用中间数量的键值头（多于一个，少于查询头的数量）。我们表明，经过升级的GQA实现了与多头注意力相当的速度，并且具有接近的质量。

    Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.
    
[^37]: 二分类中代理风险的对抗一致性

    The Adversarial Consistency of Surrogate Risks for Binary Classification. (arXiv:2305.09956v1 [cs.LG])

    [http://arxiv.org/abs/2305.09956](http://arxiv.org/abs/2305.09956)

    本文研究了二分类中代理风险的对抗一致性，并给出了代理损失函数集合的特征化，结果表明敌对一致代理的类与标准设置相比小得多。

    

    我们研究用于鲁棒二分类的代理风险的一致性。常见的做法是通过对抗性训练来学习鲁棒分类器，该方法试图在每个示例可以在小球内被恶意损坏的情况下最小化期望的$0$-$1$损失。我们给出了一种简单而完整的代理损失函数集的特征化，这些集是“一致”的，即可以替换$0$-$1$损失而不影响原始对抗风险的最小化序列的任何数据分布。我们还证明了用于$\rho$-margin损失的对抗一致性的量化版本。我们的结果显示，与标准设置相比，敌对一致代理的类明显较小，在标准设置中，许多常见的代理都被认为是一致的。

    We study the consistency of surrogate risks for robust binary classification. It is common to learn robust classifiers by adversarial training, which seeks to minimize the expected $0$-$1$ loss when each example can be maliciously corrupted within a small ball. We give a simple and complete characterization of the set of surrogate loss functions that are \emph{consistent}, i.e., that can replace the $0$-$1$ loss without affecting the minimizing sequences of the original adversarial risk, for any data distribution. We also prove a quantitative version of adversarial consistency for the $\rho$-margin loss. Our results reveal that the class of adversarially consistent surrogates is substantially smaller than in the standard setting, where many common surrogates are known to be consistent.
    
[^38]: 针对线性和非线性重尾多臂老虎机的隐式范数预测器的修剪

    Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits. (arXiv:2305.06743v1 [cs.LG])

    [http://arxiv.org/abs/2305.06743](http://arxiv.org/abs/2305.06743)

    本文提出了一种针对奖励分布重尾的MAB问题的隐式规范化预测器，证明该方法在线性和非线性重尾随机MAB问题上是最优的。

    

    已知隐式范数预测器（在线镜像下降，以Tsallis熵作为prox函数）是对抗性多臂老虎机问题（MAB）的最佳算法。但是，大多数复杂性结果都依赖于有界奖励或其他限制性假设。最近有关最佳二者结合算法的研究已经针对对手性和随机重尾MAB设置进行了探讨。这个算法在这两种情况下都是最优的，但不能充分利用数据。在本文中，我们针对奖励分布重尾的MAB问题提出了带剪辑的隐式规范化预测器。我们在奖励分布上提出渐进收敛性结果，并证明所提出的方法对于线性和非线性重尾随机MAB问题是最优的。我们还证明了与最好的二者结合算法相比，该算法通常表现更好。

    Implicitly Normalized Forecaster (online mirror descent with Tsallis entropy as prox-function) is known to be an optimal algorithm for adversarial multi-armed problems (MAB). However, most of the complexity results rely on bounded rewards or other restrictive assumptions. Recently closely related best-of-both-worlds algorithm were proposed for both adversarial and stochastic heavy-tailed MAB settings. This algorithm is known to be optimal in both settings, but fails to exploit data fully. In this paper, we propose Implicitly Normalized Forecaster with clipping for MAB problems with heavy-tailed distribution on rewards. We derive convergence results under mild assumptions on rewards distribution and show that the proposed method is optimal for both linear and non-linear heavy-tailed stochastic MAB problems. Also we show that algorithm usually performs better compared to best-of-two-worlds algorithm.
    
[^39]: 离散系统的神经李雅普诺夫控制

    Neural Lyapunov Control for Discrete-Time Systems. (arXiv:2305.06547v1 [cs.LG])

    [http://arxiv.org/abs/2305.06547](http://arxiv.org/abs/2305.06547)

    该论文介绍了一种用于离散时间系统的神经李雅普诺夫控制方法，该方法利用混合整数线性规划来验证稳定性条件，计算子水平集刻画吸引域，有效地学习控制策略。

    

    尽管线性系统的稳定性已经被充分了解，但对于具有非线性动力学的系统仍然是一个主要的挑战。在这种情况下的一般方法是利用李雅普诺夫稳定性理论来计算李雅普诺夫控制函数和相关控制策略的组合。然而，对于一般的非线性系统寻找李雅普诺夫函数是一个具有挑战性的任务。为了解决这个挑战，最近提出了几种使用神经网络表示李雅普诺夫函数的方法。然而，这些方法仅针对连续时间系统设计。我们提出了第一种适用于离散时间系统学习神经李雅普诺夫控制的方法。三个关键要素使我们能够有效地学习可证明稳定的控制策略。第一个是通过混合整数线性规划来验证离散时间系统中的稳定性条件，这是一种新的方法。第二个是计算子水平集的一种新方法，该方法刻画了吸引域的区域

    While ensuring stability for linear systems is well understood, it remains a major challenge for systems with nonlinear dynamics. A general approach in such cases is to leverage Lyapunov stability theory to compute a combination of a Lyapunov control function and an associated control policy. However, finding Lyapunov functions for general nonlinear systems is a challenging task. To address this challenge, several methods have been recently proposed that represent Lyapunov functions using neural networks. However, such approaches have been designed exclusively for continuous-time systems. We propose the first approach for learning neural Lyapunov control in discrete-time systems. Three key ingredients enable us to effectively learn provably stable control policies. The first is a novel mixed-integer linear programming approach for verifying the stability conditions in discrete-time systems. The second is a novel approach for computing sub-level sets which characterize the region of att
    
[^40]: 从子采样时间序列中使用代理变量进行因果推断

    Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])

    [http://arxiv.org/abs/2305.05276](http://arxiv.org/abs/2305.05276)

    本研究提出了一种使用代理变量方法的无模型算法，可以从子采样时间序列中无需参数约束地识别整个因果结构。

    

    从时间序列数据推断因果结构是许多科学研究的核心兴趣。采样频率远低于因果影响频率是此类推断的主要障碍。为了克服这个问题，已经提出了许多基于模型和非模型的方法，但是要么局限于线性情况，要么无法建立可识别性。在本研究中，我们提出了一种无模型的算法，可以在没有任何参数约束的情况下从子采样时间序列识别整个因果结构。该方法的思想是，子采样的挑战主要来自于“未观察到”的时间步，因此应使用为未观察到变量设计的工具处理此问题。在这些工具中，我们发现代理变量方法特别适合，因为未观察到变量的代理变量自然是在观察到的时间步上本身。根据这种直觉，我们建立了全面的结构可识别性。

    Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
    
[^41]: CAMEL：面向高效设备端学习的AI模型和嵌入式DRAM的共同设计

    CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning. (arXiv:2305.03148v1 [cs.AR])

    [http://arxiv.org/abs/2305.03148](http://arxiv.org/abs/2305.03148)

    CAMEL提出了使用嵌入式DRAM作为主要存储介质的方法来解决设备端学习中存储和计算过程中占用大量内存的问题，从而使AI模型更加高效。

    

    物联网的兴起导致边缘设备产生了大量数据，通常使用人工智能算法进行处理。设备端学习使边缘平台能够不断地根据用户个人数据调整AI模型，从而实现更好的服务质量。然而，在资源受限的设备上进行AI训练非常困难，因为深度神经网络（DNN）会带来密集的计算工作量和占用大量芯片内存的问题。为了缓解这个问题，我们建议使用嵌入式动态随机存取存储器（eDRAM）作为训练数据的主要存储介质。与静态随机访问存储器（SRAM）相比，eDRAM在存储密度上引入了超过2倍的改进，从而减少了芯片外存储器的流量。然而，为了保持存储的数据完整，eDRAM需要执行耗电的数据刷新操作。如果数据存储一段时间，就可以避免eDRAM刷新。

    The emergence of the Internet of Things (IoT) has resulted in a remarkable amount of data generated on edge devices, which are often processed using AI algorithms. On-device learning enables edge platforms to continually adapt the AI models to user personal data and further allows for a better service quality. However, AI training on resource-limited devices is extremely difficult because of the intensive computing workload and the significant amount of on-chip memory consumption exacted by deep neural networks (DNNs). To mitigate this, we propose to use embedded dynamic random-access memory (eDRAM) as the main storage medium of training data. Compared with static random-access memory (SRAM), eDRAM introduces more than $2\times$ improvement on storage density, enabling reduced off-chip memory traffic. However, to keep the stored data intact, eDRAM is required to perform the power-hungry data refresh operations.  eDRAM refresh can be eliminated if the data is stored for a period of time
    
[^42]: 医学图像的“Segment Anything Model”模型？

    Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])

    [http://arxiv.org/abs/2304.14660](http://arxiv.org/abs/2304.14660)

    “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。

    

    “Segment Anything Model”（SAM）是第一个适用于常规图像分割的基础模型。它设计了一种新颖的可推广分割任务，通过自动和手动两种模式实现了使用预训练模型进行零样本图像分割。SAM在各种自然图像分割任务中取得了显着的成果。然而，由于复杂的模态、细微的解剖结构、不确定的复杂对象边界和广泛的对象尺度，医学图像分割（MIS）更具挑战性。SAM在各种自然图像分割任务中取得了显着的成果。同时，零样本和高效的MIS可以很好地减少注释时间并促进医学图像分析的发展。因此，SAM似乎是一个潜在的工具，并且其在大型医学数据集上的表现应该进一步验证。我们收集和整理了52个开源数据集，并建立了一个具有16个模态和68个对象的大型医学分割数据集。

    The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
    
[^43]: 结构化提示询问与递归语义提取（SPIRES）：使用零样本学习填充知识库的方法

    Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. (arXiv:2304.02711v1 [cs.AI])

    [http://arxiv.org/abs/2304.02711](http://arxiv.org/abs/2304.02711)

    SPIRES是一种新的知识提取方法，利用大型语言模型进行零样本学习和通用查询回答，能够填充复杂的知识库而无需显式训练数据。

    

    创建知识库和本体是一项耗时的任务，依赖于手动管理。AI / NLP方法可以帮助专业策展人填充这些知识库，但当前方法依赖于大量训练数据，并且不能填充任意复杂的嵌套知识模式。在这里我们提出了Structured Prompt Interrogation and Recursive Extraction of Semantics（SPIRES），一种知识提取方法，该方法依赖于大型语言模型（LLM）执行零样本学习（ZSL）和通用查询回答，以及从灵活提示返回符合指定模式的信息。 SPIRES针对给定的详细用户定义的知识模式和输入文本，对GPT-3+执行递归提示询问，以获得与提供的模式匹配的一组响应。 SPIRES使用现有的本体和词汇表为所有匹配元素提供标识符。 我们提供了在不同领域（包括音乐，体育和政治）中使用SPIRES的示例，展示了其能够填充复杂的知识库而无需显式训练数据。

    Creating knowledge bases and ontologies is a time consuming task that relies on a manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrary complex nested knowledge schemas.  Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against GPT-3+ to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for all matched elements.  We present examples of use of SPIRES in different domains, inc
    
[^44]: “Polytuplet Loss: 训练阅读理解和逻辑推理模型的反向方法”

    Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2304.01046](http://arxiv.org/abs/2304.01046)

    本文研究了一种训练阅读理解和逻辑推理模型的反向方法，利用相对准确性的策略来训练模型，通过Polytuplet Loss函数来确保优先学习答案选择的相对正确性，获得了不错的成果，提出了具有一般性的训练方法和模型架构。

    

    在整个学校教育过程中，学生们将受到阅读理解和逻辑推理的考验。学生们已经开发了各种策略来完成此类考试，其中有些被认为是通常表现优于其他策略的。这样一种策略涉及强调相对准确性而非绝对准确性，理论上可以在不完全掌握解题所需信息的情况下得出正确答案。本文研究了应用这种策略来训练迁移学习模型以解决阅读理解和逻辑推理问题的有效性。这些模型在具有挑战性的阅读理解和逻辑推理基准数据集ReClor上进行了评估。尽管以前的研究集中于逻辑推理技能，但我们专注于一种通用的训练方法和模型架构。我们提出了Polytuplet Loss函数，是三元组损失函数的扩展，以确保优先学习答案选择的相对正确性而非学习绝对正确性。

    Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
    
[^45]: 健壮的风险感知期权对冲

    Robust Risk-Aware Option Hedging. (arXiv:2303.15216v1 [q-fin.CP])

    [http://arxiv.org/abs/2303.15216](http://arxiv.org/abs/2303.15216)

    本研究利用健壮的风险感知强化学习算法，优化期权对冲策略，特别应用于界限期权对冲，随着代理风险偏好变化，对冲策略发生扭曲，鲁棒策略优于非鲁棒策略。

    

    期权对冲/交易的目标不仅仅是为了保护下行风险，还希望寻求收益，驱动策略。本研究展示了健壮的风险感知强化学习(RL)在减轻与路径相关的金融衍生品风险方面的潜力。我们利用Jaimungal、Pesenti、Wang、Tatsat(2022)的策略梯度方法，优化健壮的风险感知绩效标准，具体应用于界限期权对冲，并强调随着代理从风险规避转变为风险寻求，最优对冲策略会发生扭曲，以及代理如何强化其策略。我们进一步研究了当数据生成过程(DGP)与训练DGP不同时，对冲的表现，并证明了鲁棒策略优于非鲁棒策略。

    The objectives of option hedging/trading extend beyond mere protection against downside risks, with a desire to seek gains also driving agent's strategies. In this study, we showcase the potential of robust risk-aware reinforcement learning (RL) in mitigating the risks associated with path-dependent financial derivatives. We accomplish this by leveraging the Jaimungal, Pesenti, Wang, Tatsat (2022) and their policy gradient approach, which optimises robust risk-aware performance criteria. We specifically apply this methodology to the hedging of barrier options, and highlight how the optimal hedging strategy undergoes distortions as the agent moves from being risk-averse to risk-seeking. As well as how the agent robustifies their strategy. We further investigate the performance of the hedge when the data generating process (DGP) varies from the training DGP, and demonstrate that the robust strategies outperform the non-robust ones.
    
[^46]: 解释约束下的学习

    Learning with Explanation Constraints. (arXiv:2303.14496v1 [cs.LG])

    [http://arxiv.org/abs/2303.14496](http://arxiv.org/abs/2303.14496)

    本文研究了解释约束下的学习问题，提出了EPAC模型，探讨了使用这些解释时模型的益处，并提供了一种基于变分近似的算法解决方案。

    

    尽管监督学习假设存在标注数据，但我们可能有关于模型应如何运行的先验信息。本文将其形式化为从解释约束中学习，并提供了一个学习理论框架，分析了这些解释如何提高模型的学习能力。本文的第一项关键贡献是通过定义我们称之为EPAC模型（在新数据期望中满足这些约束的模型）来回答哪些模型会受益于解释这一问题。我们使用标准的学习理论工具分析了这类模型。第二个关键贡献是对于由线性模型和两层神经网络的梯度信息给出的规范解释的限制（以其Rademacher复杂度为衡量标准）进行了表征。最后，我们通过一种变分近似提供了我们的框架的算法解决方案，它能够实现更好的性能并满足这些约束。

    While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraint
    
[^47]: FuNVol：使用函数主成分和神经SDE的多资产隐含波动率市场模拟器

    FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs. (arXiv:2303.00859v2 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2303.00859](http://arxiv.org/abs/2303.00859)

    FuNVol是一个多资产隐含波动率市场模拟器，使用函数主成分和神经SDE生成真实历史价格的IV表面序列，并在无静态套利的表面次流形内产生一致的市场情景。同时，使用模拟表面进行对冲可以生成与实现P＆L一致的损益分布。

    

    我们介绍了一种新的方法，使用函数数据分析和神经随机微分方程，结合概率积分变换惩罚来生成多个资产的隐含波动率表面序列，该方法忠实于历史价格。我们证明了学习IV表面和价格的联合动态产生的市场情景与历史特征一致，并且在没有静态套利的表面次流形内。最后，我们证明使用模拟表面进行对冲会生成与实现P＆L一致的损益分布。

    Here, we introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&L) distributions that are consistent with realised P&Ls.
    
[^48]: 《超越批处理二元分类的量子学习理论》

    Quantum Learning Theory Beyond Batch Binary Classification. (arXiv:2302.07409v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07409](http://arxiv.org/abs/2302.07409)

    这篇论文通过研究量子学习理论拓展了批处理多类学习、在线布尔学习和在线多类学习，并提出了第一个具有量子示例的在线学习模型。

    

    Arunachalam和de Wolf（2018）证明了在可实现和糊涂设置下，量子批处理学习布尔函数的样本复杂性与相应的经典样本复杂性具有相同的形式和数量级。在本文中，我们将这个明显令人惊讶的结果推广到了批处理多类学习、在线布尔学习和在线多类学习。对于我们的在线学习结果，我们首先考虑了Dawid和Tewari（2022）经典模型的自适应对手变体。然后，我们引入了第一个（据我们所知）具有量子示例的在线学习模型。

    Arunachalam and de Wolf (2018) showed that the sample complexity of quantum batch learning of boolean functions, in the realizable and agnostic settings, has the same form and order as the corresponding classical sample complexities. In this paper, we extend this, ostensibly surprising, message to batch multiclass learning, online boolean learning, and online multiclass learning. For our online learning results, we first consider an adaptive adversary variant of the classical model of Dawid and Tewari (2022). Then, we introduce the first (to the best of our knowledge) model of online learning with quantum examples.
    
[^49]: 多个分位数的私有统计估计

    Private Statistical Estimation of Many Quantiles. (arXiv:2302.06943v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06943](http://arxiv.org/abs/2302.06943)

    本文主要研究如何在差分隐私条件下估计一个分布的多个分位数。它提出了两种方法：一种是通过私有地估计样本的经验分位数来估计分布的分位数，另一种是使用密度估计技术进行分位数函数估计，并且展示了两种方法之间的权衡。

    

    本文研究在差分隐私条件下估计许多统计分位数的问题。更具体地，给定一个分布并且能够访问来自其独立同分布样本，我们考虑在特定点上估计其累积分布函数的逆函数（分位数函数）。例如，这项任务在私有数据生成中非常重要。我们提出了两种不同的方法。第一种方法是私下估计样本的经验分位数，并将此结果用作分布的分位数估计器。特别地，我们研究了 Kaplan等人最近发表的递归估计分位数的隐私算法的统计性质。第二种方法是使用密度估计技术进行均匀间隔内的分位数函数估计。特别地，我们展示了两种方法之间的权衡。当我们想要估计许多分位数时，最好使用第一种方法单独估计它们。另一方面，当我们想要在大区间上估计分位数函数时，第二种方法更有效。

    This work studies the estimation of many statistical quantiles under differential privacy. More precisely, given a distribution and access to i.i.d. samples from it, we study the estimation of the inverse of its cumulative distribution function (the quantile function) at specific points. For instance, this task is of key importance in private data generation. We present two different approaches. The first one consists in privately estimating the empirical quantiles of the samples and using this result as an estimator of the quantiles of the distribution. In particular, we study the statistical properties of the recently published algorithm introduced by Kaplan et al. 2022 that privately estimates the quantiles recursively. The second approach is to use techniques of density estimation in order to uniformly estimate the quantile function on an interval. In particular, we show that there is a tradeoff between the two methods. When we want to estimate many quantiles, it is better to estim
    
[^50]: DynGFN: 借助RNA速度技术进行贝叶斯基因调控网络推断的GFlowNets方法

    DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04178](http://arxiv.org/abs/2302.04178)

    DynGFN是一种借助RNA速度技术进行基因调控网络推断的方法，能够捕捉网络结构的不确定性，并在准确度上超过现有方法。

    

    细胞生物学的一个重要挑战是推断基因调控网络（GRN），该网络描述了控制基因表达和细胞功能的基因及其产物之间的相互作用。本文借助RNA速度技术开发了一种方法DynGFN，该方法训练生成流网络，使用RNA速度数据执行基因调控网络的贝叶斯推断，并捕捉网络结构的不确定性。

    One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
    
[^51]: 异构数据上顺序分割学习的收敛性分析

    Convergence Analysis of Sequencial Split Learning on Heterogeneous Data. (arXiv:2302.01633v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01633](http://arxiv.org/abs/2302.01633)

    本文推导出了顺序分割学习在异构数据上收敛的保证，并且证明了它在异构数据上优于联邦平均算法。

    

    联邦学习（FL）和分割学习（SL）是分布式机器学习的两种流行范例。通过将计算密集部分卸载到服务器，SL对于在资源受限设备上进行深层模型训练非常有前途，但仍缺乏严格的收敛性分析。在本文中，我们推导出了顺序SL（SSL，进行顺序模型训练的SL基本情形）在异构数据上对于强化/一般/非凸目标的收敛保证。值得注意的是，所得到的保证表明，在异构数据上，SSL比联邦平均（FedAvg，FL中最流行的算法）更好。我们在极端异构数据上通过实验证实了这个反直觉的分析结果。

    Federated Learning (FL) and Split Learning (SL) are two popular paradigms of distributed machine learning. By offloading the computation-intensive portions to the server, SL is promising for deep model training on resource-constrained devices, yet still lacking of rigorous convergence analysis. In this paper, we derive the convergence guarantees of Sequential SL (SSL, the vanilla case of SL that conducts the model training in sequence) for strongly/general/non-convex objectives on heterogeneous data. Notably, the derived guarantees suggest that SSL is better than Federated Averaging (FedAvg, the most popular algorithm in FL) on heterogeneous data. We validate the counterintuitive analysis result empirically on extremely heterogeneous data.
    
[^52]: 基于规约的延迟反馈顺序决策框架

    A Reduction-based Framework for Sequential Decision Making with Delayed Feedback. (arXiv:2302.01477v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01477](http://arxiv.org/abs/2302.01477)

    我们提出了一个基于规约的框架，可以将任何多批次算法转化为处理顺序决策中的随机延迟的高效算法。我们不仅在赌博机、表格型MDPs和表格型MGs方面取得了与现有结果相匹配或改进的成果，还首次对顺序决策中的延迟与函数逼近进行了研究。

    

    我们研究了一般多智能体顺序决策中的随机延迟反馈，包括赌博机问题、单智能体马尔可夫决策过程（MDPs）和马尔可夫博弈（MGs）。我们提出了一种新颖的基于规约的框架，将任何多批次算法转化为能处理顺序决策中的随机延迟的高效算法。通过将不同的多批次算法插入我们的框架中，我们提供了几个示例，证明我们的框架不仅匹配或改进了现有的赌博机、表格型MDPs和表格型MGs的结果，还首次对顺序决策中的延迟与函数逼近进行了研究。总之，我们为多智能体顺序决策中的延迟反馈提供了一套完整的尖锐结果。

    We study stochastic delayed feedback in general multi-agent sequential decision making, which includes bandits, single-agent Markov decision processes (MDPs), and Markov games (MGs). We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision making. By plugging different multi-batched algorithms into our framework, we provide several examples demonstrating that our framework not only matches or improves existing results for bandits, tabular MDPs, and tabular MGs, but also provides the first line of studies on delays in sequential decision making with function approximation. In summary, we provide a complete set of sharp results for multi-agent sequential decision making with delayed feedback.
    
[^53]: 数据集压缩的综合调查

    A Comprehensive Survey of Dataset Distillation. (arXiv:2301.05603v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05603](http://arxiv.org/abs/2301.05603)

    数据集压缩是处理海量数据的一种方法，通过综合一个小型典型数据集来提高数据处理效率。这一方法在压缩数据集方面取得了惊人的性能，但仍存在一些限制。

    

    在过去的十年中，深度学习技术得到了前所未有的发展，并在许多应用领域成为首选。这一进展主要归功于快速发展的计算资源与先进算法的系统协同，用以处理海量数据。然而，处理有限计算能力下无限增长的数据逐渐变得具有挑战性。为此，提出了各种方法来提高数据处理效率。数据集压缩方法是一种数据集缩减方法，通过从大规模数据中综合出一个小型典型数据集来解决这个问题，并引起了深度学习界的广泛关注。现有的数据集压缩方法可以根据是否明确模仿目标数据的性能将其分类为元学习和数据匹配框架。尽管数据集压缩在压缩数据集方面表现出惊人的性能，但仍存在一些限制。

    Deep learning technology has developed unprecedentedly in the last decade and has become the primary choice in many application domains. This progress is mainly attributed to a systematic collaboration in which rapidly growing computing resources encourage advanced algorithms to deal with massive data. However, it has gradually become challenging to handle the unlimited growth of data with limited computing power. To this end, diverse approaches are proposed to improve data processing efficiency. Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community. Existing dataset distillation methods can be taxonomized into meta-learning and data matching frameworks according to whether they explicitly mimic the performance of target data. Although dataset distillation has shown surprising performance in compressing datasets, there are still several limi
    
[^54]: 稀疏加性模型的分类

    Classification by sparse additive models. (arXiv:2212.01792v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2212.01792](http://arxiv.org/abs/2212.01792)

    这篇论文研究了非参数的稀疏加性模型用于分类，通过对分量系数施加稀疏组Lasso和稀疏组Slope型惩罚来设计分类器，实验证明了分类器在未知稀疏性和平滑性上的自适应性能。

    

    我们考虑了用于分类的非参数稀疏加性模型（SpAM）。SpAM分类器的设计基于最小化logistic损失，通过对分量展开系数施加稀疏组Lasso和更一般的稀疏组Slope型惩罚（例如，傅里叶或小波）。所得的分类器对未知的稀疏性和平滑性具有固有的自适应性。我们证明，在某些稀疏组受限特征值条件下，稀疏组Lasso分类器在整个解析、Sobolev和Besov类范围内几乎是最小化极小（加上对数因子），而稀疏组Slope分类器在稀疏和适度稠密设定下达到了确切的最小化极小阶数（不含额外的对数因子）。该分类器的性能在实际数据例子中得到了证明。

    We consider (nonparametric) sparse additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso and more general sparse group Slope-type penalties on the coefficients of univariate components' expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifiers are inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition the sparse group Lasso classifier is nearly-minimax (up to log-factors) within the entire range of analytic, Sobolev and Besov classes while the sparse group Slope classifier achieves the exact minimax order (without the extra log-factors) for sparse and moderately dense setups. The performance of the proposed classifier is illustrated on the real-data example.
    
[^55]: 可扩展的PAC-Bayesian元学习：从理论到实践的PAC-Optimal超后验

    Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior: From Theory to Practice. (arXiv:2211.07206v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07206](http://arxiv.org/abs/2211.07206)

    本研究使用PAC-Bayesian理论提供了元学习的泛化界限，并推导出了最佳性能保证的闭式优化超后验(PACOH)。通过理论分析和案例研究，我们展示了这些保证在元学习中相对于PAC-Bayesian每个任务学习界限的改进。

    

    元学习旨在通过从相关学习任务的数据集中获取有用的归纳偏好，加速对新任务的学习过程。然而，在实践中，可用的相关任务数量通常很小，大多数现有方法假设任务数量丰富，使它们不切实际且容易过拟合。元学习文献中的一个核心问题是如何进行正则化以确保对未见任务的泛化。在这项工作中，我们使用PAC-Bayesian理论提供了一种理论分析，并提出了元学习的泛化界限，这是由Rothfuss等人（2021）首次推导出来的。关键是，该界限使我们能够得到最佳性能保证的闭式优化超后验，称为PACOH。我们提供了一种理论分析和实证案例研究，在哪些条件下以及在多大程度上这些元学习的保证改进了PAC-Bayesian每个任务学习界限。

    Meta-Learning aims to speed up the learning process on new tasks by acquiring useful inductive biases from datasets of related learning tasks. While, in practice, the number of related tasks available is often small, most of the existing approaches assume an abundance of tasks; making them unrealistic and prone to overfitting. A central question in the meta-learning literature is how to regularize to ensure generalization to unseen tasks. In this work, we provide a theoretical analysis using the PAC-Bayesian theory and present a generalization bound for meta-learning, which was first derived by Rothfuss et al. (2021). Crucially, the bound allows us to derive the closed form of the optimal hyper-posterior, referred to as PACOH, which leads to the best performance guarantees. We provide a theoretical analysis and empirical case study under which conditions and to what extent these guarantees for meta-learning improve upon PAC-Bayesian per-task learning bounds. The closed-form PACOH inspi
    
[^56]: 基于非凸配对融合的聚类联邦学习

    Clustered Federated Learning based on Nonconvex Pairwise Fusion. (arXiv:2211.04218v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04218](http://arxiv.org/abs/2211.04218)

    本研究提出了一种基于非凸配对融合的聚类联邦学习框架，能够自动识别聚类结构，降低通信成本，并确保隐私性。

    

    本研究探讨了聚类联邦学习（FL），即具有非i.i.d.数据的FL的一种形式。在聚类联邦学习中，设备被分成聚类，并且每个聚类通过使用本地模型来最优地匹配其数据。我们提出了一种结合了非凸惩罚以配对参数差异的聚类联邦学习框架。该框架可以自动识别聚类结构，无需预先知道聚类的数量和每个聚类中的设备集合。为了实现所提出的框架，我们引入了一种称为融合惩罚联邦聚类（FPFC）的新型聚类联邦学习方法。基于标准的交替方向乘子方法（ADMM），FPFC在并行中实施，仅在每轮通信中更新设备的子集，并允许每个设备的可变工作负载。这些策略显著降低了通信成本，同时确保隐私性，使其在FL中实际可行。我们还提出了一种新的预热策略以提高性能。

    This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. This framework can automatically identify cluster structures without a priori knowledge of the number of clusters and the set of devices in each cluster. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC is implemented in parallel, updates only a subset of devices at each communication round, and allows for variable workload per device. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hype
    
[^57]: 通过统一加倍条件稳定神经网络训练的准确性

    Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition. (arXiv:2210.08415v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08415](http://arxiv.org/abs/2210.08415)

    本文引入了统一加倍条件，以确保在训练DNN期间准确性的稳定性，无论使用绝对值激活函数还是ReLU激活函数。在统一加倍条件下，准确率有很高的概率稳定，并提供了具体估计。

    

    本文研究深度神经网络（DNNs）训练过程中准确性的稳定性。在这个背景下，DNNs的训练通过最小化交叉熵损失函数来进行，其性能指标是准确性（正确分类的对象比例）。虽然训练会导致损失减少，但是准确性不一定随着过程增加，有时甚至会下降。实现准确性稳定性的目标是确保如果初始时准确性很高，在整个训练过程中保持高水平。本文引入了训练数据的加倍条件，以确保使用绝对值激活函数的DNN的训练期间的准确性稳定。对于在$R^n$中的训练数据，这种加倍条件使用$R^n$中的板块进行制定，并且取决于板块的选择。本文的目标是两方面。首先，通过引入更简单、更通用的条件——统一加倍条件，使加倍条件方法更易于理解。其次，利用这个新条件证明在训练过程中，无论是使用绝对值激活函数还是ReLU激活函数的DNN，其准确性都保持不变。我们的主要结果是，在统一加倍条件下，准确性在训练期间有很高的概率保持稳定，并提供了具体的概率估计。

    We study the stability of accuracy during the training of deep neural networks (DNNs). In this context, the training of a DNN is performed via the minimization of a cross-entropy loss function, and the performance metric is accuracy (the proportion of objects that are classified correctly). While training results in a decrease of loss, the accuracy does not necessarily increase during the process and may sometimes even decrease. The goal of achieving stability of accuracy is to ensure that if accuracy is high at some initial time, it remains high throughout training.  A recent result by Berlyand, Jabin, and Safsten introduces a doubling condition on the training data, which ensures the stability of accuracy during training for DNNs using the absolute value activation function. For training data in $\mathbb{R}^n$, this doubling condition is formulated using slabs in $\mathbb{R}^n$ and depends on the choice of the slabs. The goal of this paper is twofold. First, to make the doubling cond
    
[^58]: MENLI: 自然语言推理的鲁棒性评估指标

    MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07316](http://arxiv.org/abs/2208.07316)

    本文提出了一种基于自然语言推理的鲁棒性评估指标，比现有的摘要评估指标更好，在标准基准测试中表现良好且更加鲁棒，与现有指标相结合可以使评估效果进一步提高。

    

    最近被提出的基于BERT的文本生成评估指标在标准基准测试中表现良好，但易受到对信息正确性的攻击。我们认为这部分原因是此类模型是基于语义相似性建模的。相反，我们提出一种基于自然语言推理（NLI）的鲁棒性评估指标，这种指标更适合建模。我们设计了一种基于偏好的对抗性攻击框架，并表明我们的NLI基础指标比最近的BERT基础指标更具鲁棒性。在标准基准测试中，我们的NLI基础指标优于现有的摘要评估指标，但低于SOTA MT指标。然而，在现有指标与我们的NLI指标相结合时，我们既获得了更高的对抗鲁棒性（15％-30％），又获得了标准基准测试中更高的质量指标（+5％至30％）。

    Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
    
[^59]: 在再生核希尔伯特空间系列中的集成预测：仙境中的动力系统。

    Ensemble forecasts in reproducing kernel Hilbert space family: dynamical systems in Wonderland. (arXiv:2207.14653v2 [math-ph] UPDATED)

    [http://arxiv.org/abs/2207.14653](http://arxiv.org/abs/2207.14653)

    本文提出了一种在动力系统中的集合预测和模拟的方法，将系统嵌入再生核希尔伯特空间族，并在该空间中使用简单的集合数据同化方法进行轨迹重构。

    

    提出了一种针对海洋或大气流等高维动力系统的集合估计和模拟的方法框架。为此，将该动力系统嵌入由动力驱动的再生核希尔伯特空间族中。这个家族因其吸引人的特性而被命名为仙境。在仙境中，Koopman和Perron-Frobenius算子是酉的和一致连续的。这个属性保证它们可以用对角化有界无穷小生成器的指数级级数表示。此外，可以直接获得对Lyapunov指数和切线线性动态的精确集合式表达式。仙境使我们能够设计出极其简单的集合数据同化方法，用于以轨迹样本的恒定时间线性组合来进行轨迹重构。这种令人尴尬的简单策略得以实现，是通过Hilbert空间设置中切线线性动力的完全合理的叠加原理实现的。

    A methodological framework for ensemble-based estimation and simulation of high dimensional dynamical systems such as the oceanic or atmospheric flows is proposed. To that end, the dynamical system is embedded in a family of reproducing kernel Hilbert spaces with kernel functions driven by the dynamics. This family is nicknamed Wonderland for its appealing properties. In Wonderland the Koopman and Perron-Frobenius operators are unitary and uniformly continuous. This property warrants they can be expressed in exponential series of diagonalizable bounded infinitesimal generators. Access to Lyapunov exponents and to exact ensemble based expressions of the tangent linear dynamics are directly available as well. Wonderland enables us the devise of strikingly simple ensemble data assimilation methods for trajectory reconstructions in terms of constant-in-time linear combinations of trajectory samples. Such an embarrassingly simple strategy is made possible through a fully justified superposi
    
[^60]: 具有基于Hessian的泛化保证的深度神经网络鲁棒微调

    Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees. (arXiv:2206.02659v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02659](http://arxiv.org/abs/2206.02659)

    本文通过Hessian-based分析，提出一种基于距离的泛化度量方法，用于理解深度神经网络微调的泛化特性。通过PAC-Bayesian分析，给出了基于Hessian距离的微调模型泛化界。此外，还对微调面对标签噪声的问题进行了研究，并提出了一种相关算法和泛化误差保证。

    

    本文考虑在目标任务上对预训练的深度神经网络进行微调。我们研究微调的泛化特性，以理解过拟合问题，这在目标数据集较小或训练标签噪声时经常观察到。现有的深度网络泛化度量依赖于与微调模型的初始化（即预训练网络）距离和深度网络的噪声稳定性等概念。本文通过PAC-Bayesian分析确定了一种基于Hessian的距离度量，它与微调模型的观察到的泛化差距相关性很强。从理论上我们证明了基于Hessian距离的微调模型的泛化界。我们还对微调对抗标签噪声进行了扩展研究，过拟合是一个关键问题；我们提出了一种算法，并在类条件独立假设下给出了该算法的泛化误差保证。

    We consider fine-tuning a pretrained deep neural network on a target task. We study the generalization properties of fine-tuning to understand the problem of overfitting, which has often been observed (e.g., when the target dataset is small or when the training labels are noisy). Existing generalization measures for deep networks depend on notions such as distance from the initialization (i.e., the pretrained network) of the fine-tuned model and noise stability properties of deep networks. This paper identifies a Hessian-based distance measure through PAC-Bayesian analysis, which is shown to correlate well with observed generalization gaps of fine-tuned models. Theoretically, we prove Hessian distance-based generalization bounds for fine-tuned models. We also describe an extended study of fine-tuning against label noise, where overfitting is against a critical problem; We present an algorithm and a generalization error guarantee for this algorithm under a class conditional independent 
    
[^61]: 设备上学习：基于神经网络的可训练边缘人工智能

    On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01077](http://arxiv.org/abs/2203.01077)

    本研究介绍了一种基于神经网络的设备上学习方法，针对边缘人工智能应用中的环境因素对准确性造成的影响进行了解决。通过重新训练，在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。

    

    在真实世界的边缘人工智能应用中，其准确性经常受到各种环境因素的影响，如噪声、传感器的位置/校准和时间相关的变化。本文介绍了一种基于神经网络的设备上学习方法，以解决这个问题而不需要深入了解。我们的方法与事实上的反向传播训练有很大区别，而是专为低端边缘设备量身定制。本文介绍了其算法和在由树莓派Pico和低功耗无线模块组成的无线传感器节点上的实现。通过使用旋转机器的振动模式进行实验证明，通过设备上学习的重新训练在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。

    In real-world edge AI applications, their accuracy is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes. This article introduces a neural network based on-device learning approach to address this issue without going deep. Our approach is quite different from de facto backpropagation based training but tailored for low-end edge devices. This article introduces its algorithm and implementation on a wireless sensor node consisting of Raspberry Pi Pico and low-power wireless module. Experiments using vibration patterns of rotating machines demonstrate that retraining by the on-device learning significantly improves an anomaly detection accuracy at a noisy environment while saving computation and communication costs for low power.
    

