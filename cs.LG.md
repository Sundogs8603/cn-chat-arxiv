# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CBQ: Cross-Block Quantization for Large Language Models](https://rss.arxiv.org/abs/2312.07950) | CBQ是一种用于大型语言模型的跨块重构型后训练量化方法。CBQ通过使用同源重构方案来建立块间的长程依赖关系，最小化误差积累。CBQ还采用了粗到精的预处理策略和自适应的取整技术，使其能够有效处理极端异常值并提高整体量化精度。 |
| [^2] | [LightningNet: Distributed Graph-based Cellular Network Performance Forecasting for the Edge](https://arxiv.org/abs/2403.18810) | LightningNet提出了一种轻量级分布式图形框架，用于预测细胞网络性能，实现了稳定的性能提升，并在支持物联网和边缘设备方面取得了重大进展。 |
| [^3] | [ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation](https://arxiv.org/abs/2403.18807) | 通过使用预训练的ViT模型生成的全局图像先验，为单图深度估计模型提供更详细的上下文信息，并提出了一种新的使用扩散骨干且受ViT嵌入条件约束的深度估计模型。 |
| [^4] | [Long-form factuality in large language models](https://arxiv.org/abs/2403.18802) | 该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。 |
| [^5] | [ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object](https://arxiv.org/abs/2403.18775) | 本研究引入生成模型作为数据源，通过扩散模型生成了更多样化的背景、纹理和材料图像，建立了ImageNet-D基准评估深度模型的鲁棒性，在一系列视觉模型中显著降低了准确性高达60%。 |
| [^6] | [RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees](https://arxiv.org/abs/2403.18774) | 提出了一个新的数字水印框架RAW，将可学习的水印直接引入到原始图片数据中，支持各种生成架构并提供了对特定对抗攻击具有可证明保证的防御措施。 |
| [^7] | [Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means](https://arxiv.org/abs/2403.18766) | 该论文介绍了一种新型K-means聚类算法，通过竞争性随机样本大小优化，有效提高了Big-means中的并行大数据聚类效率。 |
| [^8] | [CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning](https://arxiv.org/abs/2403.18765) | CaT是一种新颖的受限RL算法，通过将约束集成到机器人学习中，通过随机终结的方式重新制定约束，达到了出色的约束依从性。 |
| [^9] | [Detection of subclinical atherosclerosis by image-based deep learning on chest x-ray](https://arxiv.org/abs/2403.18756) | 该研究开发了一种基于深度学习的系统，可以在胸部X射线上识别亚临床动脉硬化，为检测心血管疾病提供了新方法。 |
| [^10] | [Understanding the Learning Dynamics of Alignment with Human Feedback](https://arxiv.org/abs/2403.18742) | 本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。 |
| [^11] | [Usage-Specific Survival Modeling Based on Operational Data and Neural Networks](https://arxiv.org/abs/2403.18739) | 基于神经网络和运行数据的生存建模方法，提出了一种针对快照数据训练的生存模型，针对非同质采样数据，通过同质采样使得最大似然训练能够应用并产生理想结果。 |
| [^12] | [Nonlinear model reduction for operator learning](https://arxiv.org/abs/2403.18735) | 提出了一种结合神经网络与核主成分分析(KPCA)的高效框架用于运算学习，KPCA-DeepONet在准确性方面表现出优越性。 |
| [^13] | [Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods](https://arxiv.org/abs/2403.18731) | 该研究提出了一种利用可解释性技术来增强制造质量预测模型性能的方法，通过消除无关特征进行微调，提高了性能，为降低制造成本和更好理解训练后的模型铺平了道路。 |
| [^14] | [Semi-Supervised Learning for Deep Causal Generative Models](https://arxiv.org/abs/2403.18717) | 首次开发了一种利用变量之间因果关系的半监督深度因果生成模型，以最大限度地利用所有可用数据。 |
| [^15] | [Deep Learning for Traffic Flow Prediction using Cellular Automata-based Model and CNN-LSTM architecture](https://arxiv.org/abs/2403.18710) | 该论文提出使用基于元胞自动机模型和CNN-LSTM架构的深度学习方法成功预测交通流，充分利用交通流动态领域知识。 |
| [^16] | [Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching](https://arxiv.org/abs/2403.18705) | 本文介绍了一种通过一组受限耦合引入的条件Wasserstein距离，它等于后验Wasserstein距离的期望，推导了其性质，并提出了近似速度场的方法。 |
| [^17] | [Fpga-Based Neural Thrust Controller for UAVs](https://arxiv.org/abs/2403.18703) | 本研究探索了将 FPGA 作为一种解决方案，通过实现神经网络控制器以提高无人机适应性和性能，尤其在未知环境中。 |
| [^18] | [Contrastive Learning with Orthonormal Anchors (CLOA)](https://arxiv.org/abs/2403.18699) | 该研究提出了一种新的损失函数称为正交锚点回归损失，用于解开嵌入聚类，显著增强嵌入的独特性 |
| [^19] | [InceptionTime vs. Wavelet -- A comparison for time series classification](https://arxiv.org/abs/2403.18687) | 比较了在时间序列分类中使用InceptionTime直接分类和使用小波变换图像再分类的两种方法，均获得90%以上的分类准确率，其中直接分类方法达到了95.2%。 |
| [^20] | [Representatividad Muestral en la Incertidumbre Sim\'etrica Multivariada para la Selecci\'on de Atributos](https://arxiv.org/abs/2403.18685) | 提出了一种启发式条件，可以在属性数量、基数和样本大小的不同组合下保持多变量对称不确定性的良好质量，为属性选择过程提供了新的有用标准 |
| [^21] | [TransFusion: Contrastive Learning with Transformers](https://arxiv.org/abs/2403.18681) | TransFusion的主要创新在于定义了对比学习领域中的两个基本问题的理论极限，并成功实现了从复杂的现实世界数据中提取特征以改善分类精度。 |
| [^22] | [NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method](https://arxiv.org/abs/2403.18680) | NL-ITI通过引入非线性探测和多令牌干预，成功改进了ITI方法，在多个多选基准上取得了显著的性能改进。 |
| [^23] | [Fact Checking Beyond Training Set](https://arxiv.org/abs/2403.18671) | 论文探讨了超越训练集的事实核查问题，通过提出新颖的对抗算法和训练方法，使事实核查流程在不同领域中更加稳健。 |
| [^24] | [Aiming for Relevance](https://arxiv.org/abs/2403.18668) | 引入了与临床背景相一致的新颖生命体征预测性能指标，通过捕捉与临床规范的偏差、整体趋势和趋势偏差，为早期发现不良事件铺平道路。 |
| [^25] | [Neural Network-Based Piecewise Survival Models](https://arxiv.org/abs/2403.18664) | 本文提出了一类基于神经网络的生存模型，通过分段定义风险函数和密度函数，有效扩展了标准模型并展现出较好的性能。 |
| [^26] | [Dependency Aware Incident Linking in Large Cloud Systems](https://arxiv.org/abs/2403.18639) | 开发高效的事件关联模型对将相关事件分组到簇中以快速解决主要故障并降低现场工程师的疲劳至关重要。 |
| [^27] | [Transformers-based architectures for stroke segmentation: A review](https://arxiv.org/abs/2403.18637) | 本综述深入探讨了基于Transformer的创新架构在卒中分割领域的应用，旨在改善卒中的诊断与分割准确性。 |
| [^28] | [Fusion approaches for emotion recognition from speech using acoustic and text-based features](https://arxiv.org/abs/2403.18635) | 该研究提出了使用BERT获取上下文化的词嵌入来表征语音转录信息，并展示这比使用Glove嵌入的性能更好，同时对结合音频和文本模态的策略进行了比较，发现在两个数据集上融合声学和基于文本的系统是有益的。 |
| [^29] | [First Experiences with the Identification of People at Risk for Diabetes in Argentina using Machine Learning Techniques](https://arxiv.org/abs/2403.18631) | 在本研究中，针对阿根廷地区的情况，发展和评估了用于识别2型糖尿病（T2D）和糖尿病前期（PD）风险人群的预测模型，并展示了部分模型在两个特定数据集上取得了非常好的性能。 |
| [^30] | [Scalable Lipschitz Estimation for CNNs](https://arxiv.org/abs/2403.18613) | 提出了一种加速CNN Lipschitz常数估计的方法，通过分割大卷积块为一系列较小块，并通过调节划分因子以优先考虑准确性或可扩展性。 |
| [^31] | [Heterogeneous Peridynamic Neural Operators: Discover Biotissue Constitutive Law and Microstructure From Digital Image Correlation Measurements](https://arxiv.org/abs/2403.18597) | 提出了异质Peridynamic神经算子（HeteroPNO）方法，用于从实验测量数据中学习非局部本构定律和材料微结构，以便捕获生物组织中的纤维方向分布。 |
| [^32] | [The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision](https://arxiv.org/abs/2403.18587) | 统一输入会降低计算机视觉任务中的激活稀疏度，增加能量消耗和决策延迟，攻击者利用这一点提交海绵示例对神经网络进行攻击。 |
| [^33] | [One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch](https://arxiv.org/abs/2403.18582) | 提出了一种使用单一归一化流和布尔条件的简单架构，将高能物理模拟事件的分布有效地转换为观测数据分布的校正方法，并在玩具数据集上证明了该方法的有效性 |
| [^34] | [On Optimizing Hyperparameters for Quantum Neural Networks](https://arxiv.org/abs/2403.18579) | 本研究确定了对量子机器学习模型性能影响最大的超参数，并收集了相关数据。 |
| [^35] | [SteinGen: Generating Fidelitous and Diverse Graph Samples](https://arxiv.org/abs/2403.18578) | SteinGen是一种生成高质量图样本的新方法，结合了Stein方法和MCMC动力学，适用于只有一次观察到的图形，避免了参数估计的需求。 |
| [^36] | [Physics-Informed Graph Neural Networks for Water Distribution Systems](https://arxiv.org/abs/2403.18570) | 提出了一种用于水配系统的物理信息图神经网络模型，利用水力原理以无监督方式重建水力状态特征。 |
| [^37] | [PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction](https://arxiv.org/abs/2403.18569) | 提出了面向动态IR掉电预测的PDN感知GNN-CNN异构网络，引入了PDNGraph图结构和双分支异构网络PDNNet，以同时考虑PDN结构和单元-PDN关系，有助于更好地预测IR掉电。 |
| [^38] | [Noise-Robust Keyword Spotting through Self-supervised Pretraining](https://arxiv.org/abs/2403.18560) | 本研究探索了如何利用自监督学习预训练来增强关键词检测模型在嘈杂条件下的稳健性，发现在干净数据上进行预训练和微调优于监督学习方法。 |
| [^39] | [Attention-aware semantic relevance predicting Chinese sentence reading](https://arxiv.org/abs/2403.18542) | 提出了一种基于“关注语境”的方法，可以更准确地预测中文阅读任务中的凝视持续时间。 |
| [^40] | [skscope: Fast Sparsity-Constrained Optimization in Python](https://arxiv.org/abs/2403.18540) | skscope是一个Python库，通过只需编写目标函数，就能快速实现稀疏约束优化问题的解决，并且在高维参数空间下，其高效实现使得求解器能够迅速获得稀疏解，速度比基准凸求解器快80倍。 |
| [^41] | [Safe and Robust Reinforcement-Learning: Principles and Practice](https://arxiv.org/abs/2403.18539) | 本文通过对安全和稳健RL领域的探索，识别并进一步理解了在实际场景中部署RL系统面临的重大挑战，提出了不同算法方法的综合评述以增强RL代理的安全性和稳健性。 |
| [^42] | [Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs](https://arxiv.org/abs/2403.18535) | 提出了一种理论界限指导的层次化VAE（BG-VAE），用于改善神经图像编解码器的性能，通过实验证明其优于现有方法，提供了可变速率NIC。 |
| [^43] | [Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP](https://arxiv.org/abs/2403.18525) | 本研究发现，通过大型数据集训练的CLIP模型在对象-属性组合泛化中表现出明显优势，为泛化和数据集规模之间的关系提供了重要见解。 |
| [^44] | [Improving Line Search Methods for Large Scale Neural Network Training](https://arxiv.org/abs/2403.18519) | 本文改进了大规模神经网络训练的线搜索方法，通过将ADAM的动量项集成到Armijo线搜索中，实现了高效的大规模训练，并且优于以往的方法和Adam的调整学习率。 |
| [^45] | [Efficient Algorithms for Regularized Nonnegative Scale-invariant Low-rank Approximation Models](https://arxiv.org/abs/2403.18517) | 通过研究称为均匀正则化尺度不变的更一般模型，揭示了低秩逼近模型中尺度不变性导致隐式正则化的效果，有助于更好理解正则化函数的作用并指导正则化超参数的选择。 |
| [^46] | [CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection of Pathological Pulmonary CT scans](https://arxiv.org/abs/2403.18514) | 该研究利用3D正规化流技术开发出一种新型模型CT-3DFlow，通过在健康数据上进行无监督训练，在患者级别的胸部CT数据中实现病理性肺部病变的检测。 |
| [^47] | [Distributed Maximum Consensus over Noisy Links](https://arxiv.org/abs/2403.18509) | 提出了一种噪声鲁棒分布式最大一致性算法（RD-MC），通过将最大一致性问题重新定义为分布式优化问题，并使用交替方向乘法器方法求解，仅使用单组估计从而提高了鲁棒性和效率，在多智能体网络中更加抗噪声。 |
| [^48] | [Faster Convergence for Transformer Fine-tuning with Line Search Methods](https://arxiv.org/abs/2403.18506) | 将Armijo线搜索与Adam优化器相结合，通过在本地单元执行线搜索，实现了在Transformer微调中收敛速度更快的优化方法。 |
| [^49] | [Direct mineral content prediction from drill core images via transfer learning](https://arxiv.org/abs/2403.18495) | 该研究通过卷积神经网络，尝试直接从钻孔图像中评估岩石的岩性和矿物含量，以支持和加速地下地质勘探。 |
| [^50] | [Learning in PINNs: Phase transition, total diffusion, and generalization](https://arxiv.org/abs/2403.18494) | 该研究通过梯度信噪比（SNR）的视角研究了全连接神经网络的学习动态，并在信息瓶颈理论中发现了第三阶段"总扩散"，其特征是均匀的学习速率和梯度，这一阶段标志着快速的训练收敛和增强泛化能力。 |
| [^51] | [Impact of Employing Weather Forecast Data as Input to the Estimation of Evapotranspiration by Deep Neural Network Models](https://arxiv.org/abs/2403.18489) | 利用天气预报数据作为深度神经网络模型估计蒸散发的输入，解决了使用FAO56-PM方法计算ET0时太阳辐射参数不易获取的问题 |
| [^52] | [Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models](https://arxiv.org/abs/2403.18486) | 通过引入使用无分类器引导的条件扩散模型，可以直接生成特定主体、会话和类别的EEG数据，结果表明该模型生成的数据与真实数据相似。 |
| [^53] | [SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model](https://arxiv.org/abs/2403.18452) | 该论文提出了SingularTrajectory，一种基于扩散模型的通用轨迹预测框架，旨在统一不同任务的人体运动动态表示。 |
| [^54] | [CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT](https://arxiv.org/abs/2403.18451) | CoRAST提出了一种新颖的学习框架，利用基础模型(FMs)增强了分布式、相关的异构数据的分析。 |
| [^55] | [Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction](https://arxiv.org/abs/2403.18447) | 本文提出了LMTraj（基于语言的多模态轨迹预测器），将轨迹预测任务转化为一种类似于问答问题的形式，通过将数值和图像数据转换为自然语言空间来引导语言模型继续生成与轨迹相关的多模态预测。 |
| [^56] | [FRESCO: Federated Reinforcement Energy System for Cooperative Optimization](https://arxiv.org/abs/2403.18444) | FRESCO是一个旨在通过使用强化学习代理的分层控制架构来简化能源市场实施的框架，核心概念是创建一个合作设置，使各个个体目标得以实现。 |
| [^57] | [Generalized Policy Learning for Smart Grids: FL TRPO Approach](https://arxiv.org/abs/2403.18439) | 该研究提出了将联邦学习与Trust Region Policy Optimization相结合的框架，旨在降低智能电网的能源排放和成本，实验结果证实了该方法的有效性。 |
| [^58] | [Global Vegetation Modeling with Pre-Trained Weather Transformers](https://arxiv.org/abs/2403.18438) | 该研究针对全球植被活动建模，在中期天气预测成功使用的Transformer模型基础上，通过调整预训练FourCastNet模型，展示了利用预训练天气模型能提升全球植被活动估算效果。 |
| [^59] | [Collaborative Active Learning in Conditional Trust Environment](https://arxiv.org/abs/2403.18436) | 该论文介绍了一种新的协作式主动学习框架，使多个合作者能够在不披露已有数据和模型的情况下，通过分享新领域的预测结果和新获得的标签来探索新领域，从而实现隐私安全、资源共享和成本效益的优势。 |
| [^60] | [U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models](https://arxiv.org/abs/2403.18425) | 引入了U-Sketch框架，具有U-Net类型的潜在边缘预测器，能够有效地改进草图到图像扩散模型的空间布局生成。 |
| [^61] | [SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks](https://arxiv.org/abs/2403.18423) | 提出了一种称为Semantic Robust Defence (SemRoDe)的新方法，通过宏观对抗训练策略增强了语言模型（LMs）的鲁棒性，学习了能够连接对抗领域和基本领域的鲁棒表示。 |
| [^62] | [The Topos of Transformer Networks](https://arxiv.org/abs/2403.18415) | 通过拓扑理论的视角，我们对Transformer架构的表达能力进行了理论分析，发现它具有高阶推理的特点，并与其他常见神经网络架构进行了对比。 |
| [^63] | [An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM](https://arxiv.org/abs/2403.18406) | 本研究提出了一种新颖的策略，使用单一的Vision Language Model (VLM) 来进行零样本视频问答，将视频转换为单个合成图像以实现视频理解。 |
| [^64] | [On Spectrogram Analysis in a Multiple Classifier Fusion Framework for Power Grid Classification Using Electric Network Frequency](https://arxiv.org/abs/2403.18402) | 该论文提出了一种利用电网频率进行电网分类的新方法，通过生成频谱图并融合多个分类器，最终设计出一个专门用于融合过程的神经网络，实验结果展示其优于当前最先进分类器的准确性。 |
| [^65] | [Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/2403.18397) | 本文通过引入改进的深度卷积生成对抗网络(mDCGAN)，针对高质量艺术品生成进行了研究，解决了普遍训练问题，有效探索抽象绘画中的颜色和笔触模式。 |
| [^66] | [Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering](https://arxiv.org/abs/2403.18393) | 本文提出了一种新颖的基于张量的图学习框架，同时考虑了多视图聚类的一致性和特异性，解决了现有方法在相似性测量和图信息利用方面的局限性。 |
| [^67] | [Generative Multi-modal Models are Good Class-Incremental Learners](https://arxiv.org/abs/2403.18383) | 多模态生成模型为类增量学习提出了一种新的框架，可以直接为图像生成标签。 |
| [^68] | [IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction](https://arxiv.org/abs/2403.18379) | 提出了一种基于MLPs的IIP-Mixer架构，旨在通过在内部补丁和跨补丁维度进行混合操作，实现锂离子电池剩余寿命预测。 |
| [^69] | [Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates](https://arxiv.org/abs/2403.18375) | 提出了一种基于层次的关注Stragglers的低延迟同步联邦学习方法，允许stragglers同步传递部分梯度，从而改善在训练延迟约束下的模型性能。 |
| [^70] | [Ship in Sight: Diffusion Models for Ship-Image Super Resolution](https://arxiv.org/abs/2403.18370) | 该研究探索了船只图像超分辨率问题，提出了一种基于扩散模型的架构，利用文本条件在训练期间，并在类别感知的情况下，以最佳方式保存船只的关键细节。 |
| [^71] | [Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR](https://arxiv.org/abs/2403.18364) | 提出了一种面向工业物联网用户设备（IIoT UEs）的意图感知DRL上行动态调度器，通过深度强化学习（DRL）学习如何调度通信资源，并利用图结构的简化方案加速收敛，相较于传统调度方案，能有效保证IIoT UEs的意图表达。 |
| [^72] | [Supervised Multiple Kernel Learning approaches for multi-omics data integration](https://arxiv.org/abs/2403.18355) | MKL方法提供了一种灵活有效的多组学数据集成方法，可以与复杂的监督式多组学整合方法竞争 |
| [^73] | [Generating Diverse Agricultural Data for Vision-Based Farming Applications](https://arxiv.org/abs/2403.18351) | 提出了一种专门用于生成合成农业场景的程序化模型，能够模拟植物的生长阶段、土壤条件和光照变化，为精准农业中的计算机视觉任务提供了全面资源，验证了该模型在提供机器学习训练数据方面的潜力 |
| [^74] | [A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal Holes](https://arxiv.org/abs/2403.18347) | 本论文提出了一种基于量子计算的快速模糊c-均值技术，用于快速检测太阳日冕空洞（CHs）的区域。 |
| [^75] | [The Artificial Neural Twin -- Process Optimization and Continual Learning in Distributed Process Chains](https://arxiv.org/abs/2403.18343) | 提出了人工神经孪生体，结合了模型预测控制、深度学习和传感器网络的概念，通过可微分数据融合和反向传播损失梯度来实现分布式过程链中的流程优化和持续学习。 |
| [^76] | [Macroscale fracture surface segmentation via semi-supervised learning considering the structural similarity](https://arxiv.org/abs/2403.18337) | 通过半监督学习方法，在考虑结构相似性的情况下，建立了一种用于宏观裂纹表面分割的方法。 |
| [^77] | [A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages](https://arxiv.org/abs/2403.18336) | 本研究构建了一个多语言的药物不良反应语料库，包括德语、法语和日语文本，包含了12种实体类型、四种属性类型和13种关系类型的注释，为医疗保健领域开发现实世界多语言语言模型做出贡献。 |
| [^78] | [Tracking-Assisted Object Detection with Event Cameras](https://arxiv.org/abs/2403.18330) | 本文利用跟踪策略和自动标记算法，针对事件相机中的难以辨识的对象，揭示其特征信息。 |
| [^79] | [Privacy-Preserving Distributed Nonnegative Matrix Factorization](https://arxiv.org/abs/2403.18326) | 提出一种隐私保护算法，实现了全分布式NMF，通过Paillier密码系统保护了代理之间的信息交换，避免了原始数据暴露。 |
| [^80] | [Quantum Algorithms: A New Frontier in Financial Crime Prevention](https://arxiv.org/abs/2403.18322) | 量子算法在金融犯罪预防中展现了强大的解决方案，包括量子机器学习和量子人工智能，有效克服了传统方法的局限，并提供支持改进的金融风险管理分析。 |
| [^81] | [Implementation of the Principal Component Analysis onto High-Performance Computer Facilities for Hyperspectral Dimensionality Reduction: Results and Comparisons](https://arxiv.org/abs/2403.18321) | 该研究实现了主成分分析算法在NVIDIA GPU和Kalray多核处理器上，提出了利用高性能计算平台并行性的技巧，以减少处理高光谱图像所需时间。 |
| [^82] | [Multi-Modal Contrastive Learning for Online Clinical Time-Series Applications](https://arxiv.org/abs/2403.18316) | 该论文利用自监督多模态对比学习技术处理临床时间序列数据，提出了多模态邻域对比损失（MM-NCL）函数，展现出优秀的线性探测和零-shot性能。 |
| [^83] | [A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites](https://arxiv.org/abs/2403.18310) | 提出了一种基于物理信息深度学习的热力学一致材料模型，用于研究短纤维/聚合物纳米复合材料的行为，通过组合深度学习网络预测内部变量并定义整个系统的热力学状态。 |
| [^84] | [Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using SDO/HMI Data and an Attention-Aided Convolutional Neural Network](https://arxiv.org/abs/2403.18302) | 提出了一种SolarCNN方法，利用辅助注意力卷积神经网络，提升太阳活动区磁图的超分辨率，以增强对极端空间天气事件的预测。 |
| [^85] | [Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives](https://arxiv.org/abs/2403.18301) | 提出了SelMix，一种选择性混合的廉价微调技术，用于优化预训练模型以实现所需的非可分解目标。 |
| [^86] | [GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm](https://arxiv.org/abs/2403.18296) | GeNet提出了一种基于图神经网络的语义通信范式，通过将数据转换为图结构、利用编码器提取语义信息并利用解码器重建信息的方法来实现抗噪声任务导向通信。 |
| [^87] | [Few-Shot Recalibration of Language Models](https://arxiv.org/abs/2403.18286) | 提出了一种新框架，用于少样本特定切片重校准语言模型，实现在任意分布片段上获得校准的置信度估计。 |
| [^88] | [Clustering Change Sign Detection by Fusing Mixture Complexity](https://arxiv.org/abs/2403.18269) | 通过融合多个模型，提出了一种用于在渐变变化期间准确捕获集群结构并检测集群结构变化的方法。 |
| [^89] | [DSF-GAN: DownStream Feedback Generative Adversarial Network](https://arxiv.org/abs/2403.18267) | DSF-GAN提出了一种新架构，通过在训练中结合下游预测模型的反馈信息，增强了生成器的损失函数，从而提高了合成样本的实用性。 |
| [^90] | [Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning](https://arxiv.org/abs/2403.18266) | 本文提出了一种名为分支调整的方法，在持续自监督学习中实现了稳定性和可塑性的平衡，通过分支扩展和压缩来实现。 |
| [^91] | [Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models](https://arxiv.org/abs/2403.18252) | 提出了Visual Table，一种为MLLMs量身定制的新型视觉表示，通过提供层次化文本描述的全面视觉场景来弥补现有视觉表示的不足。 |
| [^92] | [NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation](https://arxiv.org/abs/2403.18241) | 提出一种新颖的空间感知3D形状生成框架，利用2D平面表示增强建模，并结合混合形状表示技术直接学习连续有向距离场表示，从而确保空间一致性和降低内存使用。 |
| [^93] | [Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data](https://arxiv.org/abs/2403.18233) | 该研究比较了图像变压器和卷积神经网络在超声前列腺癌分类中的表现，并设计了一种多目标学习策略。 |
| [^94] | [Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification](https://arxiv.org/abs/2403.18228) | 本文提出了在Spikformer中使用傅立叶或小波基代替传统的自注意力机制，提出的FWformer在视觉分类任务中表现出可比甚至更高的准确率。 |
| [^95] | [A Transformer-Based Framework for Payload Malware Detection and Classification](https://arxiv.org/abs/2403.18223) | 本文提出了一种基于Transformer的DPI算法，旨在检测恶意流量，通过学习复杂的序列数据内容并推广到类似场景中。 |
| [^96] | [Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies](https://arxiv.org/abs/2403.18222) | 提出一种不确定性感知的预训练语言条件模仿学习代理的部署方法，通过温度缩放和本地信息聚合做出不确定性感知决策，显著提升任务完成率。 |
| [^97] | [From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries](https://arxiv.org/abs/2403.18219) | 本研究通过强化学习算法在二维和三维环境中的表现，探讨了在没有预制库的情况下通过计算数学开发算法的可行性。 |
| [^98] | [Minimax Optimal Fair Classification with Bounded Demographic Disparity](https://arxiv.org/abs/2403.18216) | 本文研究了在公平二元分类中控制人口差异的统计基础，提出了极小极小最优分类错误限制人口差异到用户指定阈值的方法。 |
| [^99] | [NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation](https://arxiv.org/abs/2403.18211) | NeuroPictor通过直接调制扩散模型的生成过程，实现了fMRI到图像的重建，在多个个体预训练和多层次的引导条件下，实现了更详细的图像控制。 |
| [^100] | [Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2403.18209) | 本文提出了一种基于长期和短期约束的新算法用于安全强化学习，在自动驾驶任务中可以同时保证车辆的短期和长期安全性。 |
| [^101] | [Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities](https://arxiv.org/abs/2403.18196) | 通过考虑社会决定因素中的复杂相互作用，提出了一种简单而强大的方法，实现跨受保护群体的准确诊断结果和公平性，为高维胸部X射线多标签分类带来创新。 |
| [^102] | [Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples](https://arxiv.org/abs/2403.18192) | 多标签数据中困难和不平衡样本的突出，引入了一种自适应批次选择方法，以解决训练过程中的类别不平衡问题。 |
| [^103] | [Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering](https://arxiv.org/abs/2403.18181) | 该论文提出了一种通过分层聚类压缩非线性物理模型的Koopman矩阵的方法，相比传统SVD压缩，分层聚类表现更好。 |
| [^104] | [Mistake, Manipulation and Margin Guarantees in Online Strategic Classification](https://arxiv.org/abs/2403.18176) | 新算法旨在在存在战略代理行为的情况下恢复最大边际分类器，并提供了收敛性、有限错误和有限操纵保证。 |
| [^105] | [Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models](https://arxiv.org/abs/2403.18159) | 通过信号传播分析，提出了一种改进大型语言模型的量化知识蒸馏方法，并提供了ov-freeze稳定KD-QAT过程的洞察。 |
| [^106] | [Divide, Conquer, Combine Bayesian Decision Tree Sampling](https://arxiv.org/abs/2403.18147) | 该论文旨在通过贝叶斯推断方法量化决策树预测的不确定性，并提出了一种改变树结构和决策参数的新方法。 |
| [^107] | [HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks](https://arxiv.org/abs/2403.18142) | HERTA是针对展开图神经网络提出的一种高效且严格的训练算法，加速整个训练过程并保留模型的可解释性，同时实现了近线性时间最坏情况训练保证。 |
| [^108] | [Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs](https://arxiv.org/abs/2403.18136) | 提出了一种基于解释的方法来识别GNN中的后门训练图，设计了七种新的度量指标以更有效地检测后门攻击，并且通过自适应攻击进行了方法评估。 |
| [^109] | [AE SemRL: Learning Semantic Association Rules with Autoencoders](https://arxiv.org/abs/2403.18133) | 使用自动编码器学习时间序列数据中的语义关联关系，实验表明这种方法速度快上百倍 |
| [^110] | [Recommendation of data-free class-incremental learning algorithms by simulating future data](https://arxiv.org/abs/2403.18132) | 通过模拟未来数据流，推荐无数据的类递增学习算法，实验结果表明方法胜过竞争基线 |
| [^111] | [HealthGAT: Node Classifications in Electronic Health Records using Graph Attention Networks](https://arxiv.org/abs/2403.18128) | HealthGAT是一个新颖的图注意力网络框架，通过分层方法生成EHR嵌入，并引入定制的以EHR为中心的辅助预训练任务，从而在医学代码嵌入和复杂医学关系分析方面取得重大进展。 |
| [^112] | [A Correction of Pseudo Log-Likelihood Method](https://arxiv.org/abs/2403.18127) | 本文纠正了伪对数似然方法的最大似然估计失败问题，并提出了一种解决方案。 |
| [^113] | [Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization](https://arxiv.org/abs/2403.18120) | 通过将非正式的数学陈述翻译为形式的Isabelle代码并进行自动验证，我们提供了一种机制，可以自动拒绝在内部一致性方面与形式化问题陈述不一致的解决方案。 |
| [^114] | [Mathematical Foundation and Corrections for Full Range Head Pose Estimation](https://arxiv.org/abs/2403.18104) | 该论文研究了头部姿势估计中的坐标系和欧拉角顺序定义问题，验证了之前作品中使用的姿势输出的正确性和绘图例程的有效性。 |
| [^115] | [Tutorial on Diffusion Models for Imaging and Vision](https://arxiv.org/abs/2403.18103) | 该教程讨论了图像和视觉领域中扩散模型的基本理念，适合对扩散模型研究或应用感兴趣的本科生和研究生。 |
| [^116] | [Towards Explainable Clustering: A Constrained Declarative based Approach](https://arxiv.org/abs/2403.18101) | 该论文提出了一种新颖的可解释的约束聚类方法ECS，旨在找到高质量且可解释的聚类，强调在构建聚类时应考虑经典聚类标准和可解释性。 |
| [^117] | [Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning](https://arxiv.org/abs/2403.18100) | 通过边缘计算和物联网的组合，可以减少延迟，提高效率。 |
| [^118] | [A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury](https://arxiv.org/abs/2403.18094) | 本研究旨在开发一种个性化的视频基于手部分类系统，通过语义聚类自动识别颈椎脊髓损伤患者家庭中的主要手部握持动作。 |
| [^119] | [Paths to Equilibrium in Normal-Form Games](https://arxiv.org/abs/2403.18079) | 本文研究在多智体强化学习中的策略序列，探讨满足特定约束的策略路径，即满足路径，对于构建终止于均衡策略的路径具有重要意义。 |
| [^120] | [Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo](https://arxiv.org/abs/2403.18072) | 提出了一种适用于非线性模型的预测目标导向最优实验设计方法，通过最大化QoIs的期望信息增益来确定实验设计。 |
| [^121] | [State of the art applications of deep learning within tracking and detecting marine debris: A survey](https://arxiv.org/abs/2403.18067) | 深度学习技术在海洋垃圾领域取得了长足进展，特别是YOLO系列在目标检测方面表现优异，但研究显示当前缺乏全面的水下垃圾数据库，这成为了进一步研究的瓶颈。 |
| [^122] | [Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer](https://arxiv.org/abs/2403.18063) | 该论文提出了光谱卷积变压器 (SCT)，通过结合局部信息的卷积操作和全局信息的复杂傅里叶基础，实现了对视觉变压器中实部和复部多视图光谱算子的协调，从而实现了更好的性能。 |
| [^123] | [R2D2 image reconstruction with model uncertainty quantification in radio astronomy](https://arxiv.org/abs/2403.18052) | 本论文研究了射电天文学中的R2D2图像重建方法，通过研究学习模型系列的不确定性，提出了一种使用集成平均方法进行不确定性量化的方式。 |
| [^124] | [Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design](https://arxiv.org/abs/2403.18044) | 该研究开发了一种用于控制应用的深度多面体自编码器，在大规模系统的计算非线性控制器设计中展现出比标准线性方法更好的性能，其特定架构使得实现更高阶级数展开几乎没有额外计算负担。 |
| [^125] | [Bidirectional Consistency Models](https://arxiv.org/abs/2403.18035) | 提出了双向一致性模型（BCM），学习一个神经网络，能够实现沿着概率流常微分方程前向和后向遍历，从而有效地统一了生成和编辑图像等任务。 |
| [^126] | [Predicting species occurrence patterns from partial observations](https://arxiv.org/abs/2403.18028) | 提出了一个问题，即采用卫星图像和其他物种出现信息来预测物种出现模式，并提出了一个通用模型R-Tran，可以利用部分观测数据进行预测，优于其他方法。 |
| [^127] | [Cross-system biological image quality enhancement based on the generative adversarial network as a foundation for establishing a multi-institute microscopy cooperative network](https://arxiv.org/abs/2403.18026) | 通过生成对抗网络实现了共聚焦显微镜和广场荧光显微镜之间的图像质量转换，提供了低均方误差、高结构相似性和高峰值信噪比的高质量图像。 |
| [^128] | [Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER](https://arxiv.org/abs/2403.18025) | 提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。 |
| [^129] | [DORE: A Dataset For Portuguese Definition Generation](https://arxiv.org/abs/2403.18018) | DORE是第一个用于葡萄牙语的定义生成数据集，填补了这一领域的空白，包含超过10万个定义，并评估了多种基于深度学习的模型。 |
| [^130] | [Semi-Supervised Image Captioning Considering Wasserstein Graph Matching](https://arxiv.org/abs/2403.17995) | 提出了一种考虑Wasserstein图匹配的半监督图像字幕生成方法，用于解决半监督图像字幕生成中的困境 |
| [^131] | [Solution for Point Tracking Task of ICCV 1st Perception Test Challenge 2023](https://arxiv.org/abs/2403.17994) | 提出了一种改进的方法TAPIR+，专注于纠正静态摄像机拍摄的视频中静态点的跟踪，并在ICCV 2023第1届感知测试挑战的点跟踪任务中取得了第一名。 |
| [^132] | [Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence](https://arxiv.org/abs/2403.17993) | 人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。 |
| [^133] | [Interpretable cancer cell detection with phonon microscopy using multi-task conditional neural networks for inter-batch calibration](https://arxiv.org/abs/2403.17992) | 提出了一个多任务条件神经网络框架，用于解释性癌细胞检测的声子显微镜，实现了批间校准和时间分辨声子信号精准细胞分类，实现了89.22%的平衡精度和0.5秒的快速分类。 |
| [^134] | [Is Watermarking LLM-Generated Code Robust?](https://arxiv.org/abs/2403.17983) | 该研究探讨了现有水印技术在大型语言模型生成的Python代码上的鲁棒性，发现容易通过保留语义转换来移除这些水印。 |
| [^135] | [Markov chain models for inspecting response dynamics in psychological testing](https://arxiv.org/abs/2403.17982) | 本研究提倡利用第一阶马尔可夫链建模来捕捉和预测调查和测试答复中的序贯依赖关系，避免忽略上下文概率在塑造心理测试答复模式中的重要性。 |
| [^136] | [EG-ConMix: An Intrusion Detection Method based on Graph Contrastive Learning](https://arxiv.org/abs/2403.17980) | 提出了一种基于对比学习的入侵检测方法EG-ConMix，结合数据增强和E-GraphSAGE模块解决了数据不平衡问题。 |
| [^137] | [Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection](https://arxiv.org/abs/2403.17978) | 提出了一种利用全息全局卷积网络（HGConv）和全息简化表示（HRR）属性的方法，不需要复杂的核计算或设计，在恶意软件检测领域取得了新的SOTA结果。 |
| [^138] | [Deep Generative Domain Adaptation with Temporal Attention for Cross-User Activity Recognition](https://arxiv.org/abs/2403.17958) | 深度生成式领域自适应方法 DGDATA 独特地在领域自适应过程中识别和整合了时间关系。 |
| [^139] | [Sort & Slice: A Simple and Superior Alternative to Hash-Based Folding for Extended-Connectivity Fingerprints](https://arxiv.org/abs/2403.17954) | Sort & Slice 是一种易于实现且无位碰撞的替代方案，用于池化扩展连接指纹的亚结构 |
| [^140] | [A Note On Lookahead In Real Life And Computing](https://arxiv.org/abs/2403.17942) | 本文旨在学习、理解和探索前瞻概念，并设计新颖的模型作为解决现实世界问题的解决方案。 |
| [^141] | [Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2](https://arxiv.org/abs/2403.17905) | 通过引入R2D2方法，提出了一种可扩展的非笛卡尔磁共振图像重建方法。 |
| [^142] | [Empowering Data Mesh with Federated Learning](https://arxiv.org/abs/2403.17878) | 数据网格提出了一种去中心化的数据范式，通过将数据所有权分布到每个数据领域，同时保持联合治理，以克服数据源激增和及时分析处理需求增长的挑战。 |
| [^143] | [Asymptotic Bayes risk of semi-supervised learning with uncertain labeling](https://arxiv.org/abs/2403.17767) | 论文研究了具有不确定标签的半监督学习中的渐近贝叶斯风险计算，并通过与最佳算法比较得出新的见解。 |
| [^144] | [Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice](https://arxiv.org/abs/2403.17458) | 论文通过实证比较不同入侵检测系统，发现最佳解决方案取决于外部变量，如攻击类型、复杂性和网络环境，深度神经网络在某些数据集上表现最佳，但并非始终是最佳选择。 |
| [^145] | [Imitating Cost-Constrained Behaviors in Reinforcement Learning](https://arxiv.org/abs/2403.17456) | 该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。 |
| [^146] | [Language Models are Free Boosters for Biomedical Imaging Tasks](https://arxiv.org/abs/2403.17343) | 本研究揭示了基于残差的大型语言模型在生物医学成像任务中作为编码器的意想不到的有效性，利用冻结的变压器块进行直接处理视觉令牌，从而提高各种生物医学成像应用的性能。 |
| [^147] | [Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language](https://arxiv.org/abs/2403.17143) | 本文应用引导远程监督方法，为德语创建了最大的传记关系抽取数据集，同时发布了手动标注的评估数据集。 |
| [^148] | [Visual Whole-Body Control for Legged Loco-Manipulation](https://arxiv.org/abs/2403.16967) | 这项研究提出了一种利用视觉全身控制的框架，使腿式机器人能够同时控制腿部和手臂，以扩展操作能力，并通过仿真训练和Sim2Real转移实现了在捡起不同物体方面取得显著改进。 |
| [^149] | [Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models](https://arxiv.org/abs/2403.16915) | 本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。 |
| [^150] | [DeepMachining: Online Prediction of Machining Errors of Lathe Machines](https://arxiv.org/abs/2403.16451) | DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。 |
| [^151] | [MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline](https://arxiv.org/abs/2403.16335) | MEDDAP通过稳定扩散（SD）模型自动生成新的信息标记样本，提升医疗数据集的效用 |
| [^152] | [Centered Masking for Language-Image Pre-Training](https://arxiv.org/abs/2403.15837) | 使用中心掩蔽的GLIP技术在语言-图像预训练中取代了随机掩蔽，利用高斯分布提高了性能，并且易于获得且适用于不具有明显中心焦点的数据集。 |
| [^153] | [The opportunities and risks of large language models in mental health](https://arxiv.org/abs/2403.14814) | 大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。 |
| [^154] | [Simplified Diffusion Schr\"odinger Bridge](https://arxiv.org/abs/2403.14623) | 介绍了简化后的扩散薛定谔桥（DSB），通过与基于得分的生成模型（SGM）的统一解决了复杂数据生成中的限制，提高了性能并加快了收敛速度。 |
| [^155] | [Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity](https://arxiv.org/abs/2403.13374) | 通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。 |
| [^156] | [A Physics-embedded Deep Learning Framework for Cloth Simulation](https://arxiv.org/abs/2403.12820) | 该论文提出了一种基于物理的深度学习框架，可以直接编码布料模拟的物理特征，实现快速和实时模拟，并在不使用新数据训练的情况下通过测试表现出与基线的一致性。 |
| [^157] | [Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights](https://arxiv.org/abs/2403.10158) | 该论文提出了一个新颖的函数图卷积网络框架，结合了函数数据分析和图卷积网络，解决了数字健康和纵向研究中的多任务和多模态学习复杂性，关键创新包括任务特定嵌入组件、执行分类、回归和预测的能力，以及创建知识图进行数据解释。 |
| [^158] | [Deep Limit Order Book Forecasting](https://arxiv.org/abs/2403.09267) | 该研究利用深度学习方法预测纳斯达克交易所股票的限价订单簿中间价格变动，提出了一个创新的操作框架来评估预测的实用性。 |
| [^159] | [Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation](https://arxiv.org/abs/2403.08579) | 本研究利用机器学习的方法对正交基分段多项式逼近进行优化。 |
| [^160] | [Decoupled Data Consistency with Diffusion Purification for Image Restoration](https://arxiv.org/abs/2403.06054) | 通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。 |
| [^161] | [Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference](https://arxiv.org/abs/2403.05465) | 引入了对数正定编码（LP）和LP量化（LPQ）框架，采用基因算法寻找最优的LP参数，设计了统一的混合精度LP加速器（LPA）体系结构，可动态适应DNN参数分布，减少量化和完整精度模型之间的表示性差异。 |
| [^162] | [NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](https://arxiv.org/abs/2403.03100) | NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音 |
| [^163] | [Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling](https://arxiv.org/abs/2402.11800) | 延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。 |
| [^164] | [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283) | 这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。 |
| [^165] | [Nesting Particle Filters for Experimental Design in Dynamical Systems](https://arxiv.org/abs/2402.07868) | 本文提出了一种新颖的方法来解决动态系统中的贝叶斯实验设计问题，利用嵌套粒子滤波器和立体蒙特卡洛方法来进行基于梯度的策略优化，相比于其他方法具有更好的性能。 |
| [^166] | [Foundation Model Makes Clustering a Better Initialization for Active Learning](https://arxiv.org/abs/2402.02561) | 本研究提出了一种基于基础模型和聚类方法的主动学习初始化方案，用于选择最具信息量的样本。基础模型是通过自监督训练在大规模数据集上训练得到的，并能生成适用于各种下游任务的紧凑嵌入表示。 |
| [^167] | [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739) | OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。 |
| [^168] | [Simple Policy Optimization](https://arxiv.org/abs/2401.16025) | SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。 |
| [^169] | [Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge](https://arxiv.org/abs/2312.12558) | 研究了具有部分动态知识的在线Q学习的样本复杂度，并提出了一种乐观的Q学习算法，在有限的分集马尔可夫决策过程设置下，实现了较低的遗憾。 |
| [^170] | [Compositional Inductive Invariant Based Verification of Neural Network Controlled Systems](https://arxiv.org/abs/2312.10842) | 本文提出了一种新方法，使用构成方法将神经网络控制系统的安全验证问题拆分为较小、更易处理的子问题，并引入一种能够自动验证候选归纳不变式的算法。 |
| [^171] | [Learning to Act without Actions](https://arxiv.org/abs/2312.10812) | 通过从视频中恢复潜在动作信息，LAPO能够训练可以迅速微调为专家级策略的潜在动作策略。 |
| [^172] | [World Models via Policy-Guided Trajectory Diffusion](https://arxiv.org/abs/2312.08533) | 这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。 |
| [^173] | [Batched Low-Rank Adaptation of Foundation Models](https://arxiv.org/abs/2312.05677) | 提出了Fast LoRA（FLoRA）框架，使得基础模型的低秩调整可以高效批处理异构请求，并在绩效上保持竞争性。 |
| [^174] | [CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models](https://arxiv.org/abs/2312.03256) | CAFE提出了一种紧凑、自适应和快速的嵌入压缩框架，通过动态分配内存资源给重要特征并引入HotSketch数据结构实时捕获热门特征，解决了嵌入表在大规模推荐模型中内存需求增长的挑战。 |
| [^175] | [Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation](https://arxiv.org/abs/2311.12028) | 提出了一种名为Hourglass Tokenizer（HoT）的修剪和恢复框架，用于从视频中高效地基于Transformer进行3D人体姿势估计，通过动态选择具有高语义多样性的代表性标记并消除视频帧的冗余，最终提高了模型的效率。 |
| [^176] | [Challenging Common Paradigms in Multi-Task Learning](https://arxiv.org/abs/2311.04698) | 我们挑战了多任务学习中的常见范式，通过研究在单任务学习中的影响，揭示了优化器选择在MTL中的关键作用，并理论推导出了梯度冲突的角色。 |
| [^177] | [Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks](https://arxiv.org/abs/2311.03683) | 通过在神经网络输出中添加额外类别的对数几率项，设计使其在远离训练数据时支配原始类别的对数几率，从而防止在远处测试数据上出现任意高的置信度。 |
| [^178] | [Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2310.05723) | 将在线到离线强化学习框定为一个探索问题，并研究了内在奖励和UCB在该设置中的效果。 |
| [^179] | [Approximate and Weighted Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2308.06822) | 提出了一种基于插值的近似方法和层次加权损失函数，用于攻击FedAvg场景中的数据重构攻击。 |
| [^180] | [Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning](https://arxiv.org/abs/2102.12920) | 联邦学习作为一种新的学习范式，不仅有助于改进算法和模型融合方法，还展示了与其他学习框架相结合的潜力，特别是在联邦 X 学习中，为多任务学习、元学习、迁移学习等提供了新的研究视角。 |
| [^181] | [CharNet: Generalized Approach for High-Complexity Character Classification.](http://arxiv.org/abs/2401.17098) | 这是一篇关于手写字符识别问题的论文，提出了一个广义的方法来解决高复杂性字符分类的挑战。 |
| [^182] | [Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?.](http://arxiv.org/abs/2401.12492) | 本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。 |
| [^183] | [Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks.](http://arxiv.org/abs/2401.07494) | 通过结合输入凸性和Lipschitz连续性的优势，我们开发了一种名为输入凸性Lipschitz循环神经网络的新型网络结构，在计算效率和对抗鲁棒性方面优于现有的循环单元，并适用于多种工程任务。 |
| [^184] | [Few-Shot Detection of Machine-Generated Text using Style Representations.](http://arxiv.org/abs/2401.06712) | 本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。 |
| [^185] | [FedSN: A General Federated Learning Framework over LEO Satellite Networks.](http://arxiv.org/abs/2311.01483) | FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。 |
| [^186] | [VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification.](http://arxiv.org/abs/2311.01191) | VIGraph是一个基于自我监督学习的模型，通过利用自编码器生成少数类节点来解决图数据中的类别不平衡问题，并通过引入孪生对比策略提高生成节点的质量。 |
| [^187] | [Isometric Motion Manifold Primitives.](http://arxiv.org/abs/2310.17072) | Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods. |
| [^188] | [No-Regret Learning in Bilateral Trade via Global Budget Balance.](http://arxiv.org/abs/2310.12370) | 本文引入全局预算平衡的概念，提出了首个针对具有对抗输入的双边贸易的无悔算法，并在不同的反馈模型下进行了验证。 |
| [^189] | [Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning.](http://arxiv.org/abs/2310.03325) | 本文提出了一种面向视觉规划的可解释和可推广的框架，通过将视觉输入转化为概念表示、符号抽象和推理以及将视觉因果转换与真实世界行为关联，实现了目标条件的视觉规划。 |
| [^190] | [ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models.](http://arxiv.org/abs/2310.00117) | ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。 |
| [^191] | [A Comprehensive Review of Community Detection in Graphs.](http://arxiv.org/abs/2309.11798) | 本综述对图中的社区检测进行了全面回顾。社区结构是真实世界图的重要特征，社区检测方法的研究具有社会学、生物学和计算机科学方面的应用。尽管科学家们做出了努力，但尚未找到一个令人满意的解决方案。本综述介绍了社区结构的概念，各种社区检测方法，以及在各种网络中的实际应用。 |
| [^192] | [Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing.](http://arxiv.org/abs/2309.11427) | 本研究提出了TRACE-GPT模型，用于半导体制造业中无监督故障检测。通过使用时间卷积嵌入和生成式预训练Transformer来预训练时间序列数据，并使用交叉熵损失函数进行异常序列和正常序列的分类，实验结果表明模型具有更好的性能。 |
| [^193] | [A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation.](http://arxiv.org/abs/2309.06075) | A2V是一种半监督领域自适应框架，用于通过图像到图像转换实现脑血管的跨模态分割。它通过离散化和语义丰富的潜在空间实现源域到目标域的图像级自适应，并提高了计算效率和训练稳定性。 |
| [^194] | [Generalization Bounds: Perspectives from Information Theory and PAC-Bayes.](http://arxiv.org/abs/2309.04381) | 该论文介绍了一般化界限的两个视角：信息论和PAC-Bayesian，并探讨了它们之间的联系和共同点。这对于理论机器学习的进一步发展和新算法的设计具有重要意义。 |
| [^195] | [LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition.](http://arxiv.org/abs/2308.12882) | LCANets++是一种使用多层神经网络和层间竞争的鲁棒性音频分类方法，通过稀疏编码来提高对扰动和对抗攻击的抵抗力。 |
| [^196] | [HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar.](http://arxiv.org/abs/2308.02396) | 本文提出了一种名为HOOD的实时稳健的人员存在和离群检测方法，通过利用低成本的60 GHz FMCW雷达实现。该方法可同时解决存在检测和离群检测问题，通过重构架构和雷达图像实现准确检测人类存在，同时在人类不存在时检测移动或静止干扰物。 |
| [^197] | [High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers.](http://arxiv.org/abs/2307.13352) | 本文提出了一种适用于高维问题、在任意数量拜占庭攻击者下的新方法，核心是一种直接的高维半验证均值估计方法，具有极小极值统计率。 |
| [^198] | [Harpa: High-Rate Phase Association with Travel Time Neural Fields.](http://arxiv.org/abs/2307.07572) | 本论文提出了一种名为Harpa的高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，即使在波速未知的情况下也能实现相位关联。这种方法能够处理较小、高速率的地震事件，提供了关于地下弹性介质属性的宝贵描述。 |
| [^199] | [Learning from Synthetic Human Group Activities.](http://arxiv.org/abs/2306.16772) | 提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器，通过Unity引擎驱动实现。该生成器具有大规模数据生成、多模态和高质量注释等特点，能够用于研究复杂的人类互动和团队活动。 |
| [^200] | [Simulating counterfactuals.](http://arxiv.org/abs/2306.15328) | 该论文提出了一种算法，可以模拟反事实分布中的值，可对离散和连续变量设定条件，并应用于信用评分中的公平性分析。 |
| [^201] | [Recurrent Memory Decision Transformer.](http://arxiv.org/abs/2306.09459) | 本文提出了循环记忆决策变压器（RMDT）模型，用于处理强化学习中的长序列问题。在Atari游戏和MoJoCo控制问题上的实验表明，采用循环记忆机制的RMDT模型显着优于其没有循环记忆机制的对应模型。 |
| [^202] | [Weakly Supervised AUC Optimization: A Unified Partial AUC Approach.](http://arxiv.org/abs/2305.14258) | 本文提出了WSAUC，一种解决弱监督下AUC优化问题的统一框架，它包括噪声标签学习、正-无标签学习、多实例学习和半监督学习场景，并提出了一种新型的部分AUC——反转部分AUC（rpAUC），作为鲁棒的AUC最大化训练目标，为各种弱监督场景下的AUC优化提供了一种通用解决方案。 |
| [^203] | [Communication-minimizing Asynchronous Tensor Parallelism.](http://arxiv.org/abs/2305.13525) | 本文提出了Tensor3D，一种最小化通信消耗的三维张量计算并行化方法。它利用智能分布神经网络参数、新颖超分解方法以及通信模型，使训练速度提高了约3倍，GPU空闲时间降低了50％以上。 |
| [^204] | [ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review.](http://arxiv.org/abs/2305.03123) | 本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。 |
| [^205] | [Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space.](http://arxiv.org/abs/2305.02151) | 探索了语言特征对多语言表示空间中的跨语言传递性能的影响，初步提供了方法以增强对语言上相距较远的语言的传递能力。 |
| [^206] | [In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection.](http://arxiv.org/abs/2304.06427) | 本文系统研究了自监督学习方法在ECG表征学习上的应用，首次对三个常用ECG心律失常数据集进行了分布分析，实验发现SwAV方法表现最佳，能够超越传统的有监督学习方法，还具有较强的鲁棒性，有望在大规模和多样化人群中检测心律失常。 |
| [^207] | [ERM++: An Improved Baseline for Domain Generalization.](http://arxiv.org/abs/2304.01973) | ERM++是一个用于域通用性的改进基准方法，通过更好地利用训练数据、模型参数选择和权重空间正则化等关键技术，在多个数据集上比标准ERM更有效，同时计算复杂度更低，表现也优于最先进方法。 |
| [^208] | [Demystifying Misconceptions in Social Bots Research.](http://arxiv.org/abs/2303.17251) | 这篇文章揭示了关于社交机器人研究的普遍误解，强调需要以严谨、公正和负责任的方式讨论虚假信息研究。 |
| [^209] | [Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning.](http://arxiv.org/abs/2303.12091) | 本文提出了ANEDL框架，应用证据深度学习量化不同类型的不确定性，并设计了新颖的适应性负优化策略，有效应对在未标记数据集中包含内部值和异常值的开放式半监督学习。 |
| [^210] | [CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning.](http://arxiv.org/abs/2303.10365) | CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。 |
| [^211] | [HIVE: Harnessing Human Feedback for Instructional Visual Editing.](http://arxiv.org/abs/2303.09618) | 本文提出了一种新的框架，利用人类反馈进行指导性视觉编辑。通过收集被编辑图像的人类反馈，并学习奖励函数捕捉用户的偏好，可以缓解数据限制所带来的偏差，并提高模型性能。 |
| [^212] | [CrystalBox: Future-Based Explanations for DRL Network Controllers.](http://arxiv.org/abs/2302.13483) | CrystalBox是一种解释DRL网络控制器行为的框架，它能够使用未来关键网络性能指标的影响来生成简明而富有表现力的解释。 |
| [^213] | [Regret-Based Optimization for Robust Reinforcement Learning.](http://arxiv.org/abs/2302.06912) | 本论文提出了一种基于后悔的优化方法，用于使强化学习算法更加鲁棒，以应对观测中的对抗性噪声。 |
| [^214] | [Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation.](http://arxiv.org/abs/2301.11104) | 本文提出了基于语言解释的去偏见(B2T)框架，通过分析图像标题中的关键词，比较关键词和图像之间的相似性，识别和减缓视觉模型中的偏见，并提出了针对零样本分类器和文本到图像扩散模型的去偏见策略。 |
| [^215] | [Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram.](http://arxiv.org/abs/2301.10856) | 本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。 |

# 详细

[^1]: 跨块量化：用于大型语言模型的跨块量化方法

    CBQ: Cross-Block Quantization for Large Language Models

    [https://rss.arxiv.org/abs/2312.07950](https://rss.arxiv.org/abs/2312.07950)

    CBQ是一种用于大型语言模型的跨块重构型后训练量化方法。CBQ通过使用同源重构方案来建立块间的长程依赖关系，最小化误差积累。CBQ还采用了粗到精的预处理策略和自适应的取整技术，使其能够有效处理极端异常值并提高整体量化精度。

    

    后训练量化（PTQ）在以极低成本压缩大型语言模型（LLM）方面起着重要作用。然而，现有的PTQ方法只关注处理单个层或单个块内的异常值，忽略了块之间的依赖关系，在低位设置中导致严重的性能下降。本文提出了一种基于块间重构的跨块PTQ方法CBQ。CBQ采用了一种同源重构方案来实现块间的长程依赖关系，以最小化误差积累。此外，CBQ还结合了一种粗到精的预处理策略（CFP）来抑制权重和激活值的异常值，并配合一种自适应的LoRA取整技术实现精确的权重量化。这些创新使CBQ不仅能够有效处理极端异常值，还能提高整体量化精度。广泛的实验证明，CBQ在低位量化（W4A4，W4A8等）方面具有优越性能。

    Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
    
[^2]: LightningNet：基于分布式图的边缘细胞网络性能预测

    LightningNet: Distributed Graph-based Cellular Network Performance Forecasting for the Edge

    [https://arxiv.org/abs/2403.18810](https://arxiv.org/abs/2403.18810)

    LightningNet提出了一种轻量级分布式图形框架，用于预测细胞网络性能，实现了稳定的性能提升，并在支持物联网和边缘设备方面取得了重大进展。

    

    手机网络在提供互联网接入方面扮演着至关重要的角色，因为它是唯一具有普遍移动性支持的全球规模基础设施。为了管理和维护大规模网络，移动网络运营商需要及时信息，甚至准确的性能预测。本文提出了一种名为LightningNet的轻量级分布式基于图的框架，用于预测细胞网络性能，可以捕获网络流量中出现的时空依赖关系。LightningNet相比最先进的预测技术实现了稳定的性能提升，同时保持了类似的资源使用配置文件。我们的架构思想还在于特别设计为支持物联网和边缘设备，在与NVIDIA Jetson进行性能实验后，我们的表现甚至超过了当前领先技术。

    arXiv:2403.18810v1 Announce Type: cross  Abstract: The cellular network plays a pivotal role in providing Internet access, since it is the only global-scale infrastructure with ubiquitous mobility support. To manage and maintain large-scale networks, mobile network operators require timely information, or even accurate performance forecasts. In this paper, we propose LightningNet, a lightweight and distributed graph-based framework for forecasting cellular network performance, which can capture spatio-temporal dependencies that arise in the network traffic. LightningNet achieves a steady performance increase over state-of-the-art forecasting techniques, while maintaining a similar resource usage profile. Our architecture ideology also excels in the respect that it is specifically designed to support IoT and edge devices, giving us an even greater step ahead of the current state-of-the-art, as indicated by our performance experiments with NVIDIA Jetson.
    
[^3]: ECoDepth: 有效调整扩散模型以用于单目深度估计

    ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation

    [https://arxiv.org/abs/2403.18807](https://arxiv.org/abs/2403.18807)

    通过使用预训练的ViT模型生成的全局图像先验，为单图深度估计模型提供更详细的上下文信息，并提出了一种新的使用扩散骨干且受ViT嵌入条件约束的深度估计模型。

    

    在缺乏视差线索的情况下，基于学习的单图深度估计（SIDE）模型严重依赖图像中的阴影和上下文线索。我们从已有研究的启发中探讨使用从预训练的ViT模型生成的全局图像先验，以提供更详细的上下文信息。基于这一想法，我们提出了一种新的使用扩散骨干的SIDE模型，其受到ViT嵌入的条件约束。

    arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
    
[^4]: 大型语言模型中的长篇事实性

    Long-form factuality in large language models

    [https://arxiv.org/abs/2403.18802](https://arxiv.org/abs/2403.18802)

    该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。

    

    大型语言模型（LLMs）在回答开放性主题的事实性提示时，经常生成包含事实错误的内容。为了在开放领域中对模型的长篇事实性进行基准测试，我们首先使用GPT-4生成了一个名为LongFact的提示集，其中包含数千个囊括38个主题的问题。然后，我们提出LLM代理可以通过一种名为Search-Augmented Factuality Evaluator（SAFE）的方法作为长篇事实性的自动评估器。SAFE利用LLM将长篇回应分解为一组单独的事实，并通过发送搜索查询到Google搜索以及确定一个事实是否得到搜索结果支持的多步推理过程来评估每个事实的准确性。此外，我们还提议将F1分数扩展为长篇事实性的聚合度量。为此，我们平衡了回应中支持事实的百分比（精度）与

    arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
    
[^5]: ImageNet-D: 在扩散合成对象上评估神经网络的鲁棒性基准

    ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object

    [https://arxiv.org/abs/2403.18775](https://arxiv.org/abs/2403.18775)

    本研究引入生成模型作为数据源，通过扩散模型生成了更多样化的背景、纹理和材料图像，建立了ImageNet-D基准评估深度模型的鲁棒性，在一系列视觉模型中显著降低了准确性高达60%。

    

    我们为视觉感知鲁棒性建立了严格的基准。合成图像，如ImageNet-C、ImageNet-9和Stylized ImageNet，提供了对合成破坏、背景和纹理的特定类型评估，然而这些鲁棒性基准受限于指定的变体，并具有较低的合成质量。在这项工作中，我们引入生成模型作为合成难图像的数据源来评估深度模型的鲁棒性。利用扩散模型，我们能够生成比任何先前工作更多样化的背景、纹理和材料图像，我们将这个基准称为ImageNet-D。实验结果表明，ImageNet-D导致了一系列视觉模型的显著准确性下降，从标准ResNet视觉分类器到最新的基础模型，如CLIP和MiniGPT-4，将它们的准确性显著降低了高达60\%。我们的工作表明，扩散模型...

    arXiv:2403.18775v1 Announce Type: cross  Abstract: We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models 
    
[^6]: 一个具有可证明保证的稳健而灵活的即插即用数字水印框架，用于AI生成的图片

    RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees

    [https://arxiv.org/abs/2403.18774](https://arxiv.org/abs/2403.18774)

    提出了一个新的数字水印框架RAW，将可学习的水印直接引入到原始图片数据中，支持各种生成架构并提供了对特定对抗攻击具有可证明保证的防御措施。

    

    保护知识产权并防止对AI生成的图片的潜在滥用至关重要。本文介绍了一个名为RAW的稳健而灵活的即插即用数字水印检测框架。与传统的编码-解码方法不同，传统方法将固定的二进制代码作为水印嵌入到潜在表示中，我们的方法直接将可学习的水印引入到原始图片数据中。随后，我们使用一个与水印一起训练的分类器来检测水印的存在。提出的框架与各种生成架构兼容，并支持训练后即时进行水印注入。通过结合最先进的平滑技术，我们展示了该框架在特定对抗攻击针对水印的情况下，对于将带有水印的图片误分类的误报率提供了可证明的保证。

    arXiv:2403.18774v1 Announce Type: cross  Abstract: Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, dubbed as RAW. As a departure from traditional encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark. The proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-of-the-art smoothing techniques, we show that the framework provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of certain adversarial attacks targeting wa
    
[^7]: 通过竞争性随机样本大小优化在Big-means中实现优越的并行大数据聚类

    Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means

    [https://arxiv.org/abs/2403.18766](https://arxiv.org/abs/2403.18766)

    该论文介绍了一种新型K-means聚类算法，通过竞争性随机样本大小优化，有效提高了Big-means中的并行大数据聚类效率。

    

    这篇论文介绍了一种新颖的K-means聚类算法，是对传统Big-means方法的进步。所提出的方法有效地整合了并行处理、随机抽样和竞争性优化，创建了一个专为大数据应用设计的可扩展变体。它解决了传统技术通常面临的可伸缩性和计算时间挑战。该算法在执行过程中动态调整每个工作人员的样本大小，优化性能。这些样本大小的数据不断被分析，促进了找到最有效配置的识别。通过在使用不同样本大小的工作人员之间引入竞争因素，进一步刺激了Big-means算法内的效率。本质上，该算法通过在并行计算环境中采用随机、竞争性抽样策略，平衡了计算时间和聚类质量。

    arXiv:2403.18766v1 Announce Type: cross  Abstract: This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology. The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications. It addresses scalability and computation time challenges typically faced with traditional techniques. The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance. Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration. By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated. In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting.
    
[^8]: CaT: 约束作为四足行走强化学习的终结

    CaT: Constraints as Terminations for Legged Locomotion Reinforcement Learning

    [https://arxiv.org/abs/2403.18765](https://arxiv.org/abs/2403.18765)

    CaT是一种新颖的受限RL算法，通过将约束集成到机器人学习中，通过随机终结的方式重新制定约束，达到了出色的约束依从性。

    

    深度强化学习（RL）在解决复杂的机器人任务（如四足动态）方面取得了令人印象深刻的结果。然而，当前的解算器未能产生遵守严格约束的有效策略。在这项工作中，我们主张将约束集成到机器人学习中，并提出了约束作为终结（CaT），这是一种新颖的受限RL算法。我们离开传统的受限RL公式，通过策略学习中的随机终结来重新制定约束：任何约束违规都会触发RL代理可以获得潜在未来奖励的终结概率。我们提出了这种公式的算法方法，通过对机器人学习中广泛使用的现成RL算法（如近端策略优化）进行最小化修改。我们的方法在不引入不必要的复杂性和计算开销的情况下，导致出色的约束依从性，从而减轻了障碍。

    arXiv:2403.18765v1 Announce Type: cross  Abstract: Deep Reinforcement Learning (RL) has demonstrated impressive results in solving complex robotic tasks such as quadruped locomotion. Yet, current solvers fail to produce efficient policies respecting hard constraints. In this work, we advocate for integrating constraints into robot learning and present Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing from classical constrained RL formulations, we reformulate constraints through stochastic terminations during policy learning: any violation of a constraint triggers a probability of terminating potential future rewards the RL agent could attain. We propose an algorithmic approach to this formulation, by minimally modifying widely used off-the-shelf RL algorithms in robot learning (such as Proximal Policy Optimization). Our approach leads to excellent constraint adherence without introducing undue complexity and computational overhead, thus mitigating barriers 
    
[^9]: 基于图像深度学习在胸部X射线上检测亚临床动脉硬化

    Detection of subclinical atherosclerosis by image-based deep learning on chest x-ray

    [https://arxiv.org/abs/2403.18756](https://arxiv.org/abs/2403.18756)

    该研究开发了一种基于深度学习的系统，可以在胸部X射线上识别亚临床动脉硬化，为检测心血管疾病提供了新方法。

    

    目的是开发一个基于深度学习的系统，用于识别胸部正位X射线上的亚临床动脉硬化。通过460例初级预防患者（58.4%男性，中间年龄63 [51-74]岁）的胸部X射线（80%训练队列，20%内部验证队列），开发了一个用于预测冠状动脉钙化（CAC）评分的深度学习算法（AI-CAC模型），这些患者在临床原因下有可用的成对胸部X射线和胸部计算机断层扫描（CT），而且这两项检查在3个月内完成。在来自相同机构的90名病人的时间独立队列上对模型进行了验证（外部验证）。通过曲线下面积（AUC）评估AI-CAC模型的诊断准确性为主要结果。总体来说，中位AI-CAC评分为35（0-388），28.9%的患者没有AI-CAC。

    arXiv:2403.18756v1 Announce Type: cross  Abstract: Aims. To develop a deep-learning based system for recognition of subclinical atherosclerosis on a plain frontal chest x-ray. Methods and Results. A deep-learning algorithm to predict coronary artery calcium (CAC) score (the AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20% internal validation cohort) of primary prevention patients (58.4% male, median age 63 [51-74] years) with available paired chest x-ray and chest computed tomography (CT) indicated for any clinical reason and performed within 3 months. The CAC score calculated on chest CT was used as ground truth. The model was validated on an temporally-independent cohort of 90 patients from the same institution (external validation). The diagnostic accuracy of the AI-CAC model assessed by the area under the curve (AUC) was the primary outcome. Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC. AUC of the AI-CAC model to identify a CA
    
[^10]: 理解人类反馈对齐学习动态的研究

    Understanding the Learning Dynamics of Alignment with Human Feedback

    [https://arxiv.org/abs/2403.18742](https://arxiv.org/abs/2403.18742)

    本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。

    

    大型语言模型（LLMs）与人类意图对齐已成为安全部署模型在实际系统中的关键任务。现有的对齐方法虽然在经验上取得了成功，但理论上了解这些方法如何影响模型行为仍然是一个悬而未决的问题。我们的工作首次尝试在理论上分析人类偏好对齐的学习动态。我们正式展示了偏好数据集的分布如何影响模型更新速度，并对训练准确度提供了严格的保证。我们的理论还揭示了一个复杂现象，即优化易于优先考虑具有更高偏好可区分性的行为。我们在当代LLMs和对齐任务上在实证上验证了我们的发现，强化了我们的理论见解，并为未来的对齐方法提供了启示。免责声明：本文包含有效

    arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
    
[^11]: 基于运行数据和神经网络的特定用途生存建模

    Usage-Specific Survival Modeling Based on Operational Data and Neural Networks

    [https://arxiv.org/abs/2403.18739](https://arxiv.org/abs/2403.18739)

    基于神经网络和运行数据的生存建模方法，提出了一种针对快照数据训练的生存模型，针对非同质采样数据，通过同质采样使得最大似然训练能够应用并产生理想结果。

    

    当规划维护时，准确预测零部件故障时间至关重要，通过对这些故障时间分布进行建模，生存模型在这方面被证明特别有用。该方法基于基于神经网络的传统生存模型，使用在特定时间连续收集和存储的数据进行训练，称为快照。这种类型的训练数据的一个重要特性是它可以包含来自特定个体的多个快照，这导致标准的最大似然训练无法直接应用，因为数据不是独立的。然而，该论文表明，如果数据以所有个体的所有快照时间相同的特定格式存在，称为同质采样，则可以应用最大似然训练并产生理想的结果。在许多情况下，数据并非同质采样，在这种情况下

    arXiv:2403.18739v1 Announce Type: new  Abstract: Accurate predictions of when a component will fail are crucial when planning maintenance, and by modeling the distribution of these failure times, survival models have shown to be particularly useful in this context. The presented methodology is based on conventional neural network-based survival models that are trained using data that is continuously gathered and stored at specific times, called snapshots. An important property of this type of training data is that it can contain more than one snapshot from a specific individual which results in that standard maximum likelihood training can not be directly applied since the data is not independent. However, the papers show that if the data is in a specific format where all snapshot times are the same for all individuals, called homogeneously sampled, maximum likelihood training can be applied and produce desirable results. In many cases, the data is not homogeneously sampled and in this
    
[^12]: 面向运算学习的非线性模型简化

    Nonlinear model reduction for operator learning

    [https://arxiv.org/abs/2403.18735](https://arxiv.org/abs/2403.18735)

    提出了一种结合神经网络与核主成分分析(KPCA)的高效框架用于运算学习，KPCA-DeepONet在准确性方面表现出优越性。

    

    运算学习提供了一种逼近无限维函数空间之间映射的方法。深度运算网络(DeepONets)是该领域的一个显著架构。最近，基于模型简化和神经网络的DeepONet扩展，proper orthogonal decomposition (POD)-DeepONet，已能够在几个基准测试中的准确度方面胜过其他架构。我们将这一想法扩展到非线性模型阶降的领域，提出了一个将神经网络与核主成分分析(KPCA)结合的高效框架用于运算学习。我们的结果表明，KPCA-DeepONet在准确性方面优于POD-DeepONet。

    arXiv:2403.18735v1 Announce Type: new  Abstract: Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests. We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning. Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet.
    
[^13]: 通过集成可解释性方法增强制造质量预测模型

    Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods

    [https://arxiv.org/abs/2403.18731](https://arxiv.org/abs/2403.18731)

    该研究提出了一种利用可解释性技术来增强制造质量预测模型性能的方法，通过消除无关特征进行微调，提高了性能，为降低制造成本和更好理解训练后的模型铺平了道路。

    

    这项研究提出了一种利用可解释性技术来增强机器学习（ML）模型在预测铣削过程质量方面表现的方法，通过一个制造业案例展示了这一点。该方法包括首先训练ML模型，然后通过可解释性方法识别并消除无关特征进行微调。这种过程的完善结果导致性能的提升，为降低制造成本和更好理解训练后的ML模型铺平了道路。该研究突出了可解释性技术在解释和优化制造领域预测模型中的用处。

    arXiv:2403.18731v1 Announce Type: new  Abstract: This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case. The methodology entails the initial training of ML models, followed by a fine-tuning phase where irrelevant features identified through explainability methods are eliminated. This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models. This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm.
    
[^14]: 深度因果生成模型的半监督学习

    Semi-Supervised Learning for Deep Causal Generative Models

    [https://arxiv.org/abs/2403.18717](https://arxiv.org/abs/2403.18717)

    首次开发了一种利用变量之间因果关系的半监督深度因果生成模型，以最大限度地利用所有可用数据。

    

    开发能够回答“如果$y$变为$z$，$x$会如何变化？”这类问题的模型对于推动医学图像分析至关重要。然而，训练能够解决这类反事实问题的因果生成模型目前要求所有相关变量均已被观察到，并且相应的标签在训练数据中可用。我们首次开发了一种利用变量之间因果关系的半监督深度因果生成模型，以最大限度地利用所有可用数据。

    arXiv:2403.18717v1 Announce Type: cross  Abstract: Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference t
    
[^15]: 使用基于元胞自动机模型和CNN-LSTM架构进行交通流预测的深度学习

    Deep Learning for Traffic Flow Prediction using Cellular Automata-based Model and CNN-LSTM architecture

    [https://arxiv.org/abs/2403.18710](https://arxiv.org/abs/2403.18710)

    该论文提出使用基于元胞自动机模型和CNN-LSTM架构的深度学习方法成功预测交通流，充分利用交通流动态领域知识。

    

    近期的研究尝试使用深度学习来预测交通流的未来状态，但取得了不同的结果。这些方法面临两个关键挑战。首先，训练深度学习神经网络需要大量的训练数据，而对于交通流系统来说这样的数据目前并不容易获得。其次，即使有数据，神经网络也需要访问涵盖大多数可能的交通流动态的历史数据才能成功预测未来的交通状态。具体来说，这些深度学习方法没有充分利用关于交通流动态的领域知识，尽管已经有相当多的现有知识库。在这项工作中，我们提出使用卷积神经网络（CNNs）与长短期记忆（LSTM）深度学习架构成功预测交通流，并利用基于元胞自动机的交通流统计力学模型生成交通流数据。

    arXiv:2403.18710v1 Announce Type: new  Abstract: Recent works have attempted to use deep learning to predict future states of traffic flow, but have met with mixed results. These approaches face two key challenges. First, training deep learning neural networks requires large amounts of training data which are not yet easily available for traffic flow systems. Second, even when data is available, the neural networks require access to historical data that covers most possible traffic flow dynamics to successfully predict future traffic states. Specifically, these deep learning approaches do not fully leverage domain-knowledge about traffic flow dynamics, despite a significant existing knowledge-base. In this work, we propose to solve both issues using a Convolutional Neural Network (CNNs) with Long Short Term Memory (LSTM) deep learning architecture to successfully predict traffic flow, while leveraging a cellular automata-based statistical mechanics model of traffic flow to generate tra
    
[^16]: 具有贝叶斯OT流匹配应用的条件Wasserstein距离

    Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching

    [https://arxiv.org/abs/2403.18705](https://arxiv.org/abs/2403.18705)

    本文介绍了一种通过一组受限耦合引入的条件Wasserstein距离，它等于后验Wasserstein距离的期望，推导了其性质，并提出了近似速度场的方法。

    

    在逆问题中，许多条件生成模型通过最小化联合度量与其学习逼近之间的距离来近似后验测度。尽管这种方法在Kullback--Leibler分歧的情况下也控制后验测度之间的距离，但一般来说对于Wasserstein距离并不成立。在本文中，我们通过一组受限耦合引入了一种条件Wasserstein距离，它等于后验Wasserstein距离的期望。有趣的是，条件Wasserstein-1流的对偶形式以一种非常自然的方式类似于条件Wasserstein GAN文献中的损失。我们推导了条件Wasserstein距离的理论性质，表征相应的测地线和速度场以及流ODE。随后，我们建议通过放宽条件Wasserstein距离来近似速度场。

    arXiv:2403.18705v1 Announce Type: new  Abstract: In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein dista
    
[^17]: 基于 FPGA 的无人机神经推力控制器

    Fpga-Based Neural Thrust Controller for UAVs

    [https://arxiv.org/abs/2403.18703](https://arxiv.org/abs/2403.18703)

    本研究探索了将 FPGA 作为一种解决方案，通过实现神经网络控制器以提高无人机适应性和性能，尤其在未知环境中。

    

    无人机（UAVs）的出现通过提供多功能、成本效益和可访问平台，改善了各种领域实施最先进算法的可能性。为了完成更广泛的任务，越来越需要增强机载计算性能以应对不断增加的复杂性和动态环境条件。最近的进展看到深度神经网络（DNNs）的应用，特别是与强化学习一起，以提高无人机的适应性和性能，尤其是在未知环境中。然而，DNNs 的计算要求对许多 UAVs 上有限的计算资源构成挑战。本文探讨了将现场可编程门阵列（FPGAs）作为应对这一挑战的可行解决方案，提供了灵活性、高性能、能量和时间效率。我们提出了一种配备 Artix-7 FPGA 的新型硬件板。

    arXiv:2403.18703v1 Announce Type: cross  Abstract: The advent of unmanned aerial vehicles (UAVs) has improved a variety of fields by providing a versatile, cost-effective and accessible platform for implementing state-of-the-art algorithms. To accomplish a broader range of tasks, there is a growing need for enhanced on-board computing to cope with increasing complexity and dynamic environmental conditions. Recent advances have seen the application of Deep Neural Networks (DNNs), particularly in combination with Reinforcement Learning (RL), to improve the adaptability and performance of UAVs, especially in unknown environments. However, the computational requirements of DNNs pose a challenge to the limited computing resources available on many UAVs. This work explores the use of Field Programmable Gate Arrays (FPGAs) as a viable solution to this challenge, offering flexibility, high performance, energy and time efficiency. We propose a novel hardware board equipped with an Artix-7 FPGA 
    
[^18]: 具有正交锚点的对比学习（CLOA）

    Contrastive Learning with Orthonormal Anchors (CLOA)

    [https://arxiv.org/abs/2403.18699](https://arxiv.org/abs/2403.18699)

    该研究提出了一种新的损失函数称为正交锚点回归损失，用于解开嵌入聚类，显著增强嵌入的独特性

    

    本研究关注解决对比学习中普遍存在的不稳定性问题，特别是检查InfoNCE损失函数及其导数。我们揭示了一个关键观察，即这些损失函数表现出限制性行为，导致嵌入趋于融合为一个奇异点的收敛现象。这种“过度融合”效应对后续监督学习任务中的分类准确性产生不利影响。通过理论分析，我们证明了嵌入在等于或局限于秩-1线性子空间时表示InfoNCE的局部最小值。针对这一挑战，我们的研究提出了一种创新策略，利用与微调阶段典型使用的相同或更少的标记数据。我们提出的损失函数，即正交锚点回归损失，旨在解开嵌入聚类，显著增强每个嵌入的独特性。

    arXiv:2403.18699v1 Announce Type: cross  Abstract: This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This "over-fusion" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding 
    
[^19]: InceptionTime与小波--时间序列分类的比较

    InceptionTime vs. Wavelet -- A comparison for time series classification

    [https://arxiv.org/abs/2403.18687](https://arxiv.org/abs/2403.18687)

    比较了在时间序列分类中使用InceptionTime直接分类和使用小波变换图像再分类的两种方法，均获得90%以上的分类准确率，其中直接分类方法达到了95.2%。

    

    本文使用神经网络对次声数据进行分类，比较了两种不同的方法。一种是基于直接对时间序列数据进行分类，使用了InceptionTime网络的自定义实现。另一种方法是生成信号的小波变换的2D图像，然后使用ResNet实现进行分类。通过选择适当的超参数设置，两种方法的分类准确率均在90%以上，其中直接方法达到了95.2%。

    arXiv:2403.18687v1 Announce Type: new  Abstract: Neural networks were used to classify infrasound data. Two different approaches were compared. One based on the direct classification of time series data, using a custom implementation of the InceptionTime network. For the other approach, we generated 2D images of the wavelet transformation of the signals, which were subsequently classified using a ResNet implementation. Choosing appropriate hyperparameter settings, both achieve a classification accuracy of above 90 %, with the direct approach reaching 95.2 %.
    
[^20]: 用于属性选择的多变量对称不确定性中的样本代表性

    Representatividad Muestral en la Incertidumbre Sim\'etrica Multivariada para la Selecci\'on de Atributos

    [https://arxiv.org/abs/2403.18685](https://arxiv.org/abs/2403.18685)

    提出了一种启发式条件，可以在属性数量、基数和样本大小的不同组合下保持多变量对称不确定性的良好质量，为属性选择过程提供了新的有用标准

    

    在这项工作中，我们通过使用统计模拟技术分析了多变量对称不确定性（MSU）测量的行为，对含有信息和非信息随机生成特征的各种混合进行了实验。实验证明了属性数量、它们的基数和样本大小如何影响MSU。通过观察结果，在这个论文中提出了一种启发式条件，可以在这三个因素的不同组合下保持MSU的良好质量，提供了一个有用的新标准，有助于推动降维过程。

    arXiv:2403.18685v1 Announce Type: cross  Abstract: In this work, we analyze the behavior of the multivariate symmetric uncertainty (MSU) measure through the use of statistical simulation techniques under various mixes of informative and non-informative randomly generated features. Experiments show how the number of attributes, their cardinalities, and the sample size affect the MSU. In this thesis, through observation of results, it is proposed an heuristic condition that preserves good quality in the MSU under different combinations of these three factors, providing a new useful criterion to help drive the process of dimension reduction.   -  En el presente trabajo hemos analizado el comportamiento de una versi\'on multivariada de la incertidumbre sim\'etrica a trav\'es de t\'ecnicas de simulaci\'on estad\'isticas sobre varias combinaciones de atributos informativos y no-informativos generados de forma aleatoria. Los experimentos muestran como el n\'umero de atributos, sus cardinali
    
[^21]: TransFusion：具有变压器的对比学习

    TransFusion: Contrastive Learning with Transformers

    [https://arxiv.org/abs/2403.18681](https://arxiv.org/abs/2403.18681)

    TransFusion的主要创新在于定义了对比学习领域中的两个基本问题的理论极限，并成功实现了从复杂的现实世界数据中提取特征以改善分类精度。

    

    这篇论文提出了一个新的框架，TransFusion，旨在使对比学习的过程更具分析性和可解释性。 TransFusion由注意力块组成，其中的softmax被替换为ReLU，并且其最终块的加权和操作被截断，以使邻接矩阵成为输出。该模型通过最小化其输出与目标关联矩阵之间的Jensen-Shannon散度来进行训练，该矩阵指示每对样本是否属于相同类别或不同类别。 TransFusion的主要贡献在于定义了回答该领域两个基本问题的理论极限：数据增强的最大级别和有效对比学习所需的最小批量大小。 此外，实验结果表明，TransFusion成功地提取出能够从复杂的现实世界数据中分离集群的特征，从而提高了分类精度。

    arXiv:2403.18681v1 Announce Type: cross  Abstract: This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy 
    
[^22]: NL-ITI：优化探测和干预以改进ITI方法

    NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method

    [https://arxiv.org/abs/2403.18680](https://arxiv.org/abs/2403.18680)

    NL-ITI通过引入非线性探测和多令牌干预，成功改进了ITI方法，在多个多选基准上取得了显著的性能改进。

    

    大语言模型(LLM)容易返回虚假信息，这是人工智能领域的一个主要挑战。本文探讨了推理时干预(Inference-Time-Intervention, ITI)方法引入的范式。首先，ITI方法识别包含最多所需知识类型(例如真实信息)的注意力头。随后，在推理过程中，LLM激活被移动到所选注意力头的子集。我们通过引入非线性探测和多令牌干预-非线性ITI(NL-ITI)进一步改进了ITI框架。NL-ITI在多个多选基准上进行了测试，包括TruthfulQA，我们在这项基准上相对于基线ITI结果报告了约14%的MC1指标改进。NL-ITI还在其他测试集上取得了令人鼓舞的成绩-在MMLU的商业伦理子领域上，比基线LLaMA2-7B有约18%的MC1改进。此外，NL-ITI在效果更好的同时也更少侵入。

    arXiv:2403.18680v1 Announce Type: new  Abstract: Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in t
    
[^23]: 超越训练集的事实核查

    Fact Checking Beyond Training Set

    [https://arxiv.org/abs/2403.18671](https://arxiv.org/abs/2403.18671)

    论文探讨了超越训练集的事实核查问题，通过提出新颖的对抗算法和训练方法，使事实核查流程在不同领域中更加稳健。

    

    评估日常言论的真实性耗时且在某些情况下需要领域专业知识。我们通过实证证明，常用的事实核查流程，即检索器-阅读器，在使用一个领域的标记数据进行训练后，当在另一个领域中使用时性能会下降。之后，我们深入研究了流程的每个组成部分，并提出了新颖的算法来解决这个问题。我们提出了一种对抗算法，使检索器组件能够抵御分布转移。我们的核心思想是首先在标记的源数据上训练一个双编码器，然后通过使用未标记的目标数据对两个独立的文档编码器和言论编码器进行对抗训练。然后我们专注于阅读器组件，并建议训练它以对言论和证据文档的顺序不敏感。我们的实证评估支持这样一个阅读器显示出更高的性能。

    arXiv:2403.18671v1 Announce Type: new  Abstract: Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher
    
[^24]: 以相关性为目标

    Aiming for Relevance

    [https://arxiv.org/abs/2403.18668](https://arxiv.org/abs/2403.18668)

    引入了与临床背景相一致的新颖生命体征预测性能指标，通过捕捉与临床规范的偏差、整体趋势和趋势偏差，为早期发现不良事件铺平道路。

    

    在重症监护病房（ICU）中，生命体征至关重要。它们用于跟踪患者的状态，并识别临床上显著的变化。预测生命体征轨迹对于早期发现不良事件具有重要价值。然而，传统的机器学习指标如RMSE往往无法捕捉这些预测的真正临床相关性。我们引入了新颖的生命体征预测性能指标，与临床背景相一致，关注与临床规范的偏差、整体趋势和趋势偏差。这些指标源自通过与ICU临床医生的访谈获得的实证效用曲线。我们使用模拟和真实临床数据集（MIMIC和eICU）验证了这些指标的有用性。此外，我们将这些指标作为神经网络的损失函数，从而得到在预测临床重要事件方面表现出色的模型。这项研究为临床实践铺平了道路。

    arXiv:2403.18668v1 Announce Type: cross  Abstract: Vital signs are crucial in intensive care units (ICUs). They are used to track the patient's state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clin
    
[^25]: 基于神经网络的分段生存模型

    Neural Network-Based Piecewise Survival Models

    [https://arxiv.org/abs/2403.18664](https://arxiv.org/abs/2403.18664)

    本文提出了一类基于神经网络的生存模型，通过分段定义风险函数和密度函数，有效扩展了标准模型并展现出较好的性能。

    

    本文提出了一类基于神经网络的生存模型。这些模型是基于对时间进行分割的风险函数和密度函数的分段定义而指定的；文中展示了常数和线性分段定义，得到了四个模型的系列。这些模型可以被看作是常用的离散时间和分段指数模型的延伸，从而为这组标准模型增加了灵活性。使用模拟数据集表明，这些模型表现良好，相较于高度表达能力的最新能量模型，仅需要一小部分计算时间。

    arXiv:2403.18664v1 Announce Type: cross  Abstract: In this paper, a family of neural network-based survival models is presented. The models are specified based on piecewise definitions of the hazard function and the density function on a partitioning of the time; both constant and linear piecewise definitions are presented, resulting in a family of four models. The models can be seen as an extension of the commonly used discrete-time and piecewise exponential models and thereby add flexibility to this set of standard models. Using a simulated dataset the models are shown to perform well compared to the highly expressive, state-of-the-art energy-based model, while only requiring a fraction of the computation time.
    
[^26]: 大型云系统中的依赖感知事件关联

    Dependency Aware Incident Linking in Large Cloud Systems

    [https://arxiv.org/abs/2403.18639](https://arxiv.org/abs/2403.18639)

    开发高效的事件关联模型对将相关事件分组到簇中以快速解决主要故障并降低现场工程师的疲劳至关重要。

    

    尽管进行了重大可靠性改进，但大规模云服务不可避免地会遭遇可能显著影响服务可用性和客户满意度的生产事故。更糟糕的是，在许多情况下，一个事件可能导致多个下游故障，由于级联效应而在不同的依赖服务之间创建几个相关事件。通常情况下，现场工程师(OCEs)在孤立情况下检查这些事件，这导致大量手动操作并增加了事件缓解的总时间。因此，开发高效的事件关联模型对于将相关事件分组到簇中以快速解决主要故障并降低现场工程师的疲劳至关重要。现有的事件关联方法主要利用事件的文本和上下文信息（如标题、描述、严重性、受影响组件），从而未能利用服务之间的相互依赖关系。

    arXiv:2403.18639v1 Announce Type: cross  Abstract: Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer's satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that creates several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amount of manual toil and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverages textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between serv
    
[^27]: 基于Transformer架构的卒中分割：综述

    Transformers-based architectures for stroke segmentation: A review

    [https://arxiv.org/abs/2403.18637](https://arxiv.org/abs/2403.18637)

    本综述深入探讨了基于Transformer的创新架构在卒中分割领域的应用，旨在改善卒中的诊断与分割准确性。

    

    卒中仍然是一个重要的全球健康问题，需要精确高效的诊断工具以便及时干预和改善患者预后。深度学习方法的出现改变了医学图像分析的格局。最近，最初设计用于自然语言处理的Transformer在各种计算机视觉应用中展现出卓越的能力，包括医学图像分析。本综述旨在深入探讨应用于卒中分割领域的最新Transformer架构。它首先探讨了卒中病理学、成像方法以及准确诊断和分割面临的挑战。随后，综述深入探讨了Transformer的基本思想，提供了关于它们架构复杂性和赋予其能力的基本机制的详细见解。

    arXiv:2403.18637v1 Announce Type: cross  Abstract: Stroke remains a significant global health concern, necessitating precise and efficient diagnostic tools for timely intervention and improved patient outcomes. The emergence of deep learning methodologies has transformed the landscape of medical image analysis. Recently, Transformers, initially designed for natural language processing, have exhibited remarkable capabilities in various computer vision applications, including medical image analysis. This comprehensive review aims to provide an in-depth exploration of the cutting-edge Transformer-based architectures applied in the context of stroke segmentation. It commences with an exploration of stroke pathology, imaging modalities, and the challenges associated with accurate diagnosis and segmentation. Subsequently, the review delves into the fundamental ideas of Transformers, offering detailed insights into their architectural intricacies and the underlying mechanisms that empower the
    
[^28]: 使用声学和基于文本的特征进行情感识别的融合方法研究

    Fusion approaches for emotion recognition from speech using acoustic and text-based features

    [https://arxiv.org/abs/2403.18635](https://arxiv.org/abs/2403.18635)

    该研究提出了使用BERT获取上下文化的词嵌入来表征语音转录信息，并展示这比使用Glove嵌入的性能更好，同时对结合音频和文本模态的策略进行了比较，发现在两个数据集上融合声学和基于文本的系统是有益的。

    

    在本文中，我们研究了使用声学和基于文本特征对语音进行情感分类的不同方法。我们提出使用BERT获取上下文化的词嵌入来表征语音转录中包含的信息，并展示这比使用Glove嵌入的性能更好。我们还提出并比较了不同策略来结合音频和文本模态，对IEMOCAP和MSP-PODCAST数据集进行评估。我们发现融合声学和基于文本的系统在两个数据集上都是有益的，尽管在评估的融合方法中观察到的差异很细微。最后，针对IEMOCAP，我们展示了用于定义交叉验证折叠的标准标准对结果的巨大影响效果。特别是，为该数据集创建折叠的标准方法导致文本系统性能的高度乐观估计，这表明一些先前的工作可能...

    arXiv:2403.18635v1 Announce Type: new  Abstract: In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works ma
    
[^29]: 首次在阿根廷使用机器学习技术识别糖尿病风险人群的初步体验

    First Experiences with the Identification of People at Risk for Diabetes in Argentina using Machine Learning Techniques

    [https://arxiv.org/abs/2403.18631](https://arxiv.org/abs/2403.18631)

    在本研究中，针对阿根廷地区的情况，发展和评估了用于识别2型糖尿病（T2D）和糖尿病前期（PD）风险人群的预测模型，并展示了部分模型在两个特定数据集上取得了非常好的性能。

    

    识别2型糖尿病（T2D）和糖尿病前期（PD）对医学是一个真正的挑战，因为缺乏病原性症状和已知相关危险因素。尽管有些机器学习模型的提议使得识别患病风险的人成为可能，但这种病症的性质使得一个适用于一种人群的模型未必适用于另一种人群。本文讨论了在阿根廷特别发展和评估用于识别T2D和PD风险人群的预测模型。首先，数据库经过彻底预处理，生成了三个特定的数据集，考虑到记录数和可用变量的权衡。应用了5种不同的分类模型后，结果显示其中一些模型在两个数据集上表现出很好的性能。特别地，RF、DT和ANN展现了很大的表现。

    arXiv:2403.18631v1 Announce Type: new  Abstract: Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for medicine due to the absence of pathogenic symptoms and the lack of known associated risk factors. Even though some proposals for machine learning models enable the identification of people at risk, the nature of the condition makes it so that a model suitable for one population may not necessarily be suitable for another. In this article, the development and assessment of predictive models to identify people at risk for T2D and PD specifically in Argentina are discussed. First, the database was thoroughly preprocessed and three specific datasets were generated considering a compromise between the number of records and the amount of available variables. After applying 5 different classification models, the results obtained show that a very good performance was observed for two datasets with some of these models. In particular, RF, DT, and ANN demonstrated great c
    
[^30]: 可扩展的CNN的Lipschitz估计

    Scalable Lipschitz Estimation for CNNs

    [https://arxiv.org/abs/2403.18613](https://arxiv.org/abs/2403.18613)

    提出了一种加速CNN Lipschitz常数估计的方法，通过分割大卷积块为一系列较小块，并通过调节划分因子以优先考虑准确性或可扩展性。

    

    估计深度神经网络的Lipschitz常数正日益受到关注，因为它对通用性和对抗鲁棒性的评估很有用。卷积神经网络（CNNs）特别是在计算机视觉相关应用的成功中起着关键作用。然而，尽管现有的估计Lipschitz常数的方法可能很紧致，但当应用于CNNs时，它们的可扩展性有限。为了解决这个问题，我们提出了一种加速CNNs Lipschitz常数估计的新方法。其核心思想是通过联合层和宽度划分将大卷积块分割为一系列较小的块。我们证明了大块的Lipschitz常数上界与较小块的Lipschitz常数之间的关系。通过变化划分因子，得到的方法可以调节以优先考虑准确性或可扩展性，并支持并行化。我们展示

    arXiv:2403.18613v1 Announce Type: new  Abstract: Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness. Convolutional neural networks (CNNs) in particular, underpin much of the recent success in computer vision related applications. However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to CNNs. To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for CNNs. The core idea is to divide a large convolutional block via a joint layer and width-wise partition, into a collection of smaller blocks. We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks. Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation. We demonstrate
    
[^31]: 异质Peridynamic神经算子: 从数字图像相关测量中发现生物组织本构定律和微结构

    Heterogeneous Peridynamic Neural Operators: Discover Biotissue Constitutive Law and Microstructure From Digital Image Correlation Measurements

    [https://arxiv.org/abs/2403.18597](https://arxiv.org/abs/2403.18597)

    提出了异质Peridynamic神经算子（HeteroPNO）方法，用于从实验测量数据中学习非局部本构定律和材料微结构，以便捕获生物组织中的纤维方向分布。

    

    人体组织是高度有机化的结构，具有特定的胶原纤维排列，从点到点都有所不同。这种异质性的影响对组织功能起着重要作用，因此发现和理解这种纤维方向的分布从实验测量数据，如数字图像相关数据中尤为关键。为此，我们引入了异质Peridynamic神经算子（HeteroPNO）方法，用于基于数据的异质各向异性材料本构建模。旨在从加载场位移场测量中学习非局部本构定律以及材料微结构，以异质纤维定向场的形式。为此，我们提出了一个两阶段学习方法。首先，我们学习一个以神经网络为基础的核函数和非局部键力的均匀本构定律，以捕捉完整的

    arXiv:2403.18597v1 Announce Type: cross  Abstract: Human tissues are highly organized structures with specific collagen fiber arrangements varying from point to point. The effects of such heterogeneity play an important role for tissue function, and hence it is of critical to discover and understand the distribution of such fiber orientations from experimental measurements, such as the digital image correlation data. To this end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO) approach, for data-driven constitutive modeling of heterogeneous anisotropic materials. The goal is to learn both a nonlocal constitutive law together with the material microstructure, in the form of a heterogeneous fiber orientation field, from loading field-displacement field measurements. To this end, we propose a two-phase learning approach. Firstly, we learn a homogeneous constitutive law in the form of a neural network-based kernel function and a nonlocal bond force, to capture comple
    
[^32]: 统一输入对计算机视觉中激活稀疏度和能量-延迟攻击的影响

    The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency Attacks in Computer Vision

    [https://arxiv.org/abs/2403.18587](https://arxiv.org/abs/2403.18587)

    统一输入会降低计算机视觉任务中的激活稀疏度，增加能量消耗和决策延迟，攻击者利用这一点提交海绵示例对神经网络进行攻击。

    

    资源效率在当前机器学习中扮演着重要角色。能量消耗和决策延迟是保证可持续和实际应用的两个关键方面。不幸的是，能量消耗和决策延迟不具备对抗性。研究人员最近表明，攻击者可以在推断时计算并提交所谓的海绵示例，从而增加神经网络的能量消耗和决策延迟。在计算机视觉中，所提出的策略制作了具有较低激活稀疏度的输入，而这些输入本来可以用于加速计算。本文分析了这些能量-延迟攻击是如何减少激活稀疏度的机理。特别地，我们发现输入的统一性是关键的推动因素。统一图像，也就是大部分为平坦、颜色均匀的表面构成的图像，由于卷积、批量规范化等特定相互作用，会触发更多的激活。

    arXiv:2403.18587v1 Announce Type: cross  Abstract: Resource efficiency plays an important role for machine learning nowadays. The energy and decision latency are two critical aspects to ensure a sustainable and practical application. Unfortunately, the energy consumption and decision latency are not robust against adversaries. Researchers have recently demonstrated that attackers can compute and submit so-called sponge examples at inference time to increase the energy consumption and decision latency of neural networks. In computer vision, the proposed strategy crafts inputs with less activation sparsity which could otherwise be used to accelerate the computation. In this paper, we analyze the mechanism how these energy-latency attacks reduce activation sparsity. In particular, we find that input uniformity is a key enabler. A uniform image, that is, an image with mostly flat, uniformly colored surfaces, triggers more activations due to a specific interplay of convolution, batch normal
    
[^33]: 用单一归一化流和一个开关改进高能物理模拟

    One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch

    [https://arxiv.org/abs/2403.18582](https://arxiv.org/abs/2403.18582)

    提出了一种使用单一归一化流和布尔条件的简单架构，将高能物理模拟事件的分布有效地转换为观测数据分布的校正方法，并在玩具数据集上证明了该方法的有效性

    

    模拟事件是几乎所有高能物理分析中的关键要素。然而，模拟中的不完美可能导致观测数据和模拟事件之间存在相当大的差异。这种模拟不完全性对相关可观测量的影响必须通过比例因子、权重或修改可观测量及其相关性的分布来有效地进行校正。我们引入了一种校正方法，使用基于单一正常化流和布尔条件的简单架构将一个多维分布（模拟）转换为另一个多维分布（数据）。我们在一个受物理启发的玩具数据集上展示了该方法的有效性，该数据集存在几个可观测量及其相关性的非平凡模拟问题。

    arXiv:2403.18582v1 Announce Type: cross  Abstract: Simulated events are key ingredients in almost all high-energy physics analyses. However, imperfections in the simulation can lead to sizeable differences between the observed data and simulated events. The effects of such mismodelling on relevant observables must be corrected either effectively via scale factors, with weights or by modifying the distributions of the observables and their correlations. We introduce a correction method that transforms one multidimensional distribution (simulation) into another one (data) using a simple architecture based on a single normalising flow with a boolean condition. We demonstrate the effectiveness of the method on a physics-inspired toy dataset with non-trivial mismodelling of several observables and their correlations.
    
[^34]: 关于量子神经网络超参数优化的研究

    On Optimizing Hyperparameters for Quantum Neural Networks

    [https://arxiv.org/abs/2403.18579](https://arxiv.org/abs/2403.18579)

    本研究确定了对量子机器学习模型性能影响最大的超参数，并收集了相关数据。

    

    机器学习模型日益增强的能力与训练所需的海量数据和计算能力密不可分。因此，训练通常被外包到高性能计算设施中，我们在那里开始经历到传统高性能计算硬件扩展的限制，正如摩尔定律所预测的那样。尽管进行了大量的并行化和优化工作，当前最先进的机器学习模型需要数周的训练时间，这与巨大的 $CO_2$ 排放有关。量子计算，特别是量子机器学习（QML），可以提供显著的理论加速和增强的表现力。然而，训练 QML 模型需要调整各种超参数，这是一项非常困难的任务，次优选择可能会极大影响模型的可训练性和性能。在这项研究中，我们确定了最具影响力的超参数，并收集了关于 QML 模型性能的数据。

    arXiv:2403.18579v1 Announce Type: new  Abstract: The increasing capabilities of Machine Learning (ML) models go hand in hand with an immense amount of data and computational power required for training. Therefore, training is usually outsourced into HPC facilities, where we have started to experience limits in scaling conventional HPC hardware, as theorized by Moore's law. Despite heavy parallelization and optimization efforts, current state-of-the-art ML models require weeks for training, which is associated with an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum Machine Learning (QML), can offer significant theoretical speed-ups and enhanced expressive power. However, training QML models requires tuning various hyperparameters, which is a nontrivial task and suboptimal choices can highly affect the trainability and performance of the models. In this study, we identify the most impactful hyperparameters and collect data about the performance of QML models. We co
    
[^35]: SteinGen: 生成忠实和多样化的图样本

    SteinGen: Generating Fidelitous and Diverse Graph Samples

    [https://arxiv.org/abs/2403.18578](https://arxiv.org/abs/2403.18578)

    SteinGen是一种生成高质量图样本的新方法，结合了Stein方法和MCMC动力学，适用于只有一次观察到的图形，避免了参数估计的需求。

    

    生成保留特征结构并促进样本多样性的图形可能具有挑战性，特别是当图形观察数量较少时。在这里，我们解决了仅从一个观察到的图形生成图形的问题。通过在图形的设置中以指数随机图形模型的形式表达，我们提出的生成过程SteinGen结合了Stein方法和基于MCMC的马尔可夫动力学的思想，该动力学基于目标模型的Stein算子。SteinGen使用与e相关联的Glauber动力学

    arXiv:2403.18578v1 Announce Type: cross  Abstract: Generating graphs that preserve characteristic structures while promoting sample diversity can be challenging, especially when the number of graph observations is small. Here, we tackle the problem of graph generation from only one observed graph. The classical approach of graph generation from parametric models relies on the estimation of parameters, which can be inconsistent or expensive to compute due to intractable normalisation constants. Generative modelling based on machine learning techniques to generate high-quality graph samples avoids parameter estimation but usually requires abundant training samples. Our proposed generating procedure, SteinGen, which is phrased in the setting of graphs as realisations of exponential random graph models, combines ideas from Stein's method and MCMC by employing Markovian dynamics which are based on a Stein operator for the target model. SteinGen uses the Glauber dynamics associated with an e
    
[^36]: 物理信息图神经网络用于水配系统

    Physics-Informed Graph Neural Networks for Water Distribution Systems

    [https://arxiv.org/abs/2403.18570](https://arxiv.org/abs/2403.18570)

    提出了一种用于水配系统的物理信息图神经网络模型，利用水力原理以无监督方式重建水力状态特征。

    

    水配系统（WDS）是城市发展至关重要的关键基础设施。由于世界70%的人口可能会在2050年生活在城市环境中，因此对于WDS的高效仿真和规划工具在实现联合国可持续发展目标（SDG）6 - “为所有人提供清洁水和卫生设施”中发挥着至关重要的作用。在这个领域中，我们提出了一个新颖而高效的机器学习仿真器，更确切地说，是一个用于WDS中的水力状态估计的物理信息深度学习（DL）模型。我们的模型使用了一种递归方法，只需要几个图卷积神经网络（GCN）层，并采用了一种基于消息传递的创新算法。与传统的机器学习任务不同，该模型利用水力原理在无监督方式下推断出两个额外的水力状态特征，从而重建可用的地面实况特征。

    arXiv:2403.18570v1 Announce Type: cross  Abstract: Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - "Clean water and sanitation for all". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our k
    
[^37]: PDNNet：面向动态IR掉电预测的PDN感知GNN-CNN异构网络

    PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction

    [https://arxiv.org/abs/2403.18569](https://arxiv.org/abs/2403.18569)

    提出了面向动态IR掉电预测的PDN感知GNN-CNN异构网络，引入了PDNGraph图结构和双分支异构网络PDNNet，以同时考虑PDN结构和单元-PDN关系，有助于更好地预测IR掉电。

    

    电源供应网络（PDN）上的IR掉电与PDN的配置和电流消耗密切相关。随着集成电路（IC）设计的不断增大，动态IR掉电仿真变得计算成本高昂，基于机器学习的IR掉电预测被探索为一种有前途的解决方案。本文考虑不仅如何正确表示单元-PDN关系，还考虑如何在特征聚合过程中模拟IR掉电遵循其物理特性。因此，我们提出了一种新颖的图结构，PDNGraph，统一了PDN结构和细粒度单元-PDN关系的表示。我们进一步提出了一种双分支异构网络，PDNNet，将两个并行的GNN-CNN分支整合在一起，有利于捕捉上述特征。

    arXiv:2403.18569v1 Announce Type: cross  Abstract: IR drop on the power delivery network (PDN) is closely related to PDN's configuration and cell current consumption. As the integrated circuit (IC) design is growing larger, dynamic IR drop simulation becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution. Although CNN-based methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible. In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure. Thus, we propose a novel graph structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation. We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN branches to favorably capture the above feat
    
[^38]: 通过自监督预训练实现抗噪声关键词检测

    Noise-Robust Keyword Spotting through Self-supervised Pretraining

    [https://arxiv.org/abs/2403.18560](https://arxiv.org/abs/2403.18560)

    本研究探索了如何利用自监督学习预训练来增强关键词检测模型在嘈杂条件下的稳健性，发现在干净数据上进行预训练和微调优于监督学习方法。

    

    语音助手现在已经广泛可用，为了启动它们，使用关键词检测（KWS）算法。现代KWS系统主要使用监督学习方法进行训练，并需要大量标注数据才能达到良好性能。利用自监督学习（SSL）通过未标记数据已经被证明可以提高在干净条件下的准确性。本文探讨了如何利用Data2Vec等SSL预训练方法来增强KWS模型在嘈杂条件下的稳健性，这方面研究尚未充分探讨。对三种不同规模的模型使用不同的预训练方法进行预训练，然后进行KWS的微调。这些模型然后被测试并与使用两种基准监督学习方法训练的模型进行比较，其中一种是使用干净数据进行标准训练，另一种是多样式训练（MTR）。结果表明，在干净数据上进行预训练和微调优于监督学习方法。

    arXiv:2403.18560v1 Announce Type: cross  Abstract: Voice assistants are now widely available, and to activate them a keyword spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using supervised learning methods and require a large amount of labelled data to achieve a good performance. Leveraging unlabelled data through self-supervised learning (SSL) has been shown to increase the accuracy in clean conditions. This paper explores how SSL pretraining such as Data2Vec can be used to enhance the robustness of KWS models in noisy conditions, which is under-explored.   Models of three different sizes are pretrained using different pretraining approaches and then fine-tuned for KWS. These models are then tested and compared to models trained using two baseline supervised learning methods, one being standard training using clean data and the other one being multi-style training (MTR). The results show that pretraining and fine-tuning on clean data is superior to supervised 
    
[^39]: 关注语境语义相关性预测中文句子阅读

    Attention-aware semantic relevance predicting Chinese sentence reading

    [https://arxiv.org/abs/2403.18542](https://arxiv.org/abs/2403.18542)

    提出了一种基于“关注语境”的方法，可以更准确地预测中文阅读任务中的凝视持续时间。

    

    近年来，已经提出了几种有影响力的计算模型和度量标准，用于预测人类如何理解和处理句子。本研究受Transformer中的注意力算法和人类记忆机制启发，提出了一种“关注语境”的方法来计算语境语义相关性。这种新方法考虑了语境部分的不同贡献和期望效应，使其能够充分整合语境信息。关注语境方法还有助于模拟现有的阅读模型并对其进行评估。最终得到的“关注语境”语义相关性度量标准比现有方法更准确地预测了记录在眼动追踪语料库中的中文阅读任务中凝视持续时间。该研究的发现进一步为现有的研究提供了有力支持。

    arXiv:2403.18542v1 Announce Type: new  Abstract: In recent years, several influential computational models and metrics have been proposed to predict how humans comprehend and process sentence. One particularly promising approach is contextual semantic similarity. Inspired by the attention algorithm in Transformer and human memory mechanisms, this study proposes an ``attention-aware'' approach for computing contextual semantic relevance. This new approach takes into account the different contributions of contextual parts and the expectation effect, allowing it to incorporate contextual information fully. The attention-aware approach also facilitates the simulation of existing reading models and evaluate them. The resulting ``attention-aware'' metrics of semantic relevance can more accurately predict fixation durations in Chinese reading tasks recorded in an eye-tracking corpus than those calculated by existing approaches. The study's findings further provide strong support for the prese
    
[^40]: skscope：Python中的快速稀疏约束优化

    skscope: Fast Sparsity-Constrained Optimization in Python

    [https://arxiv.org/abs/2403.18540](https://arxiv.org/abs/2403.18540)

    skscope是一个Python库，通过只需编写目标函数，就能快速实现稀疏约束优化问题的解决，并且在高维参数空间下，其高效实现使得求解器能够迅速获得稀疏解，速度比基准凸求解器快80倍。

    

    在稀疏约束优化（SCO）上应用迭代求解器需要繁琐的数学推导和仔细的编程/调试，这限制了这些求解器的广泛影响。本文介绍了库skscope，以克服此障碍。借助skscope，用户只需编写目标函数即可解决SCO问题。本文通过两个例子演示了skscope的方便之处，其中只需四行代码就可以解决稀疏线性回归和趋势过滤。更重要的是，skscope的高效实现使得最先进的求解器可以快速获得稀疏解，而无需考虑参数空间的高维度。数值实验显示，skscope中的可用求解器可以实现比基准凸求解器获得的竞争松弛解高达80倍的加速度。skscope已经发布在Python软件包索引（PyPI）和Conda上。

    arXiv:2403.18540v1 Announce Type: cross  Abstract: Applying iterative solvers on sparsity-constrained optimization (SCO) requires tedious mathematical deduction and careful programming/debugging that hinders these solvers' broad impact. In the paper, the library skscope is introduced to overcome such an obstacle. With skscope, users can solve the SCO by just programming the objective function. The convenience of skscope is demonstrated through two examples in the paper, where sparse linear regression and trend filtering are addressed with just four lines of code. More importantly, skscope's efficient implementation allows state-of-the-art solvers to quickly attain the sparse solution regardless of the high dimensionality of parameter space. Numerical experiments reveal the available solvers in skscope can achieve up to 80x speedup on the competing relaxation solutions obtained via the benchmarked convex solver. skscope is published on the Python Package Index (PyPI) and Conda, and its 
    
[^41]: 安全稳健的强化学习: 原则与实践

    Safe and Robust Reinforcement-Learning: Principles and Practice

    [https://arxiv.org/abs/2403.18539](https://arxiv.org/abs/2403.18539)

    本文通过对安全和稳健RL领域的探索，识别并进一步理解了在实际场景中部署RL系统面临的重大挑战，提出了不同算法方法的综合评述以增强RL代理的安全性和稳健性。

    

    强化学习（RL）在解决相对复杂的任务方面取得了显著成功，然而在实际场景中部署RL系统面临与安全和稳健性有关的重大挑战。本文旨在通过探索安全和稳健RL领域的主要维度，涵盖算法、伦理和实际考虑因素，识别并进一步理解这些挑战。我们对近年来致力于解决与RL应用相关固有风险的方法和开放问题进行了全面审查。在讨论并提出安全和稳健RL的定义后，本文将现有研究工作归类为不同的算法方法，以增强RL代理的安全性和稳健性。我们考察了诸如不确定性估计、优化方法学、探索和利用的权衡以及对抗性等技术。

    arXiv:2403.18539v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness. This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations. We conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with RL applications.   After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents. We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and adversari
    
[^42]: 层次化VAE指导的理论界限用于神经图像编解码器

    Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs

    [https://arxiv.org/abs/2403.18535](https://arxiv.org/abs/2403.18535)

    提出了一种理论界限指导的层次化VAE（BG-VAE），用于改善神经图像编解码器的性能，通过实验证明其优于现有方法，提供了可变速率NIC。

    

    最近的研究揭示了变分自动编码器（VAEs）与率失真理论之间的重要理论联系，特别是在利用VAEs来估计图像信息率失真函数的理论上限。这些估计的理论界限大大超过了现有神经图像编解码器（NICs）的性能。为了缩小这一差距，我们提出了一种理论界限引导的层次化VAE（BG-VAE）用于NIC。所提出的BG-VAE利用理论界限来指导NIC模型以提升性能。我们使用层次化VAE实现了BG-VAE，并通过大量实验证明了其有效性。除了先进的神经网络模块外，我们提供了一种性能优越的、可变速率的NIC，无论是在考虑率失真性能还是计算复杂性时都优于现有方法。代码可在BG-VAE中获得。

    arXiv:2403.18535v1 Announce Type: cross  Abstract: Recent studies reveal a significant theoretical link between variational autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to estimate the theoretical upper bound of the information rate-distortion function of images. Such estimated theoretical bounds substantially exceed the performance of existing neural image codecs (NICs). To narrow this gap, we propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The proposed BG-VAE leverages the theoretical bound to guide the NIC model towards enhanced performance. We implement the BG-VAE using Hierarchical VAEs and demonstrate its effectiveness through extensive experiments. Along with advanced neural network blocks, we provide a versatile, variable-rate NIC that outperforms existing methods when considering both rate-distortion performance and computational complexity. The code is available at BG-VAE.
    
[^43]: 语言在CLIP对象-属性组合泛化中发挥关键作用

    Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP

    [https://arxiv.org/abs/2403.18525](https://arxiv.org/abs/2403.18525)

    本研究发现，通过大型数据集训练的CLIP模型在对象-属性组合泛化中表现出明显优势，为泛化和数据集规模之间的关系提供了重要见解。

    

    arXiv:2403.18525v1 公告类型:跨领域 摘要:视觉-语言模型，如CLIP，在各种类型的分布变化下展现了令人鼓舞的超出分布的泛化能力。最近的研究尝试调查这种能力的主要原因。在这项工作中，我们沿用了相同的思路，但专注于特定类型的超出分布数据 - 具有新颖属性-对象对组合的图像 - 并研究这样的模型是否能够成功地将这些图像分类到组合类别中。我们精心设计了一个名为ImageNet-AO的真实图像测试数据集，其中包含了CLIP训练集中不太可能遇到的对象的属性。我们发现，通过大型数据集训练的CLIP模型（如OpenAI CLIP，LAION-400M和LAION-2B）在有效的组合超出分布泛化方面比受监督模型和通过较小数据集训练的CLIP模型（如CC-12M和YFCC-15M）表现出数量级的改进。我们的结果证明了该方法提供了关于规模、数据集和泛化之间关系的见解。

    arXiv:2403.18525v1 Announce Type: cross  Abstract: Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the sca
    
[^44]: 改进大规模神经网络训练的线搜索方法

    Improving Line Search Methods for Large Scale Neural Network Training

    [https://arxiv.org/abs/2403.18519](https://arxiv.org/abs/2403.18519)

    本文改进了大规模神经网络训练的线搜索方法，通过将ADAM的动量项集成到Armijo线搜索中，实现了高效的大规模训练，并且优于以往的方法和Adam的调整学习率。

    

    在最近的研究中，线搜索方法在传统随机梯度下降技术的性能方面取得了显著进展，消除了需要特定学习率调度的需求。本文识别了现有最先进线搜索方法中存在的问题，提出了增强措施，并对其效果进行了严格评估。我们在比以往更大的数据集和更复杂的数据领域上测试了这些方法。具体来说，我们通过将ADAM的动量项集成到Armijo线搜索中的搜索方向中，改进了Armijo线搜索，实现了高效的大规模训练，这是以前使用Armijo线搜索方法容易失败的任务。我们的优化方法胜过以前的Armijo实现和Adam的调整学习率调度。我们的评估重点放在NLP和图像数据领域的Transformer和CNN上。我们的工作以Python包的形式公开发布，可以下载使用。

    arXiv:2403.18519v1 Announce Type: cross  Abstract: In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data. Our work is publicly available as a Python package, which prov
    
[^45]: 针对正则化非负尺度不变低秩逼近模型的高效算法

    Efficient Algorithms for Regularized Nonnegative Scale-invariant Low-rank Approximation Models

    [https://arxiv.org/abs/2403.18517](https://arxiv.org/abs/2403.18517)

    通过研究称为均匀正则化尺度不变的更一般模型，揭示了低秩逼近模型中尺度不变性导致隐式正则化的效果，有助于更好理解正则化函数的作用并指导正则化超参数的选择。

    

    正则化非负低秩逼近，如稀疏的非负矩阵分解或稀疏的非负Tucker分解，是具有增强可解释性的降维模型中的一个重要分支。然而，从实践角度来看，由于这些模型的多因素特性以及缺乏支持这些选择的理论，正则化函数和正则化系数的选择，以及高效算法的设计仍然具有挑战性。本文旨在改进这些问题。通过研究一个称为均匀正则化尺度不变的更一般模型，我们证明低秩逼近模型中固有的尺度不变性导致了隐式正则化，具有意想不到的有益和有害效果。这一发现使我们能够更好地理解低秩逼近模型中正则化函数的作用，指导正则化超参数的选择。

    arXiv:2403.18517v1 Announce Type: new  Abstract: Regularized nonnegative low-rank approximations such as sparse Nonnegative Matrix Factorization or sparse Nonnegative Tucker Decomposition are an important branch of dimensionality reduction models with enhanced interpretability. However, from a practical perspective, the choice of regularizers and regularization coefficients, as well as the design of efficient algorithms, is challenging because of the multifactor nature of these models and the lack of theory to back these choices. This paper aims at improving upon these issues. By studying a more general model called the Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance inherent to low-rank approximation models causes an implicit regularization with both unexpected beneficial and detrimental effects. This observation allows to better understand the effect of regularization functions in low-rank approximation models, to guide the choice of the regularization hyp
    
[^46]: 利用3D正规化流进行无监督检测病理性肺部CT扫描

    CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection of Pathological Pulmonary CT scans

    [https://arxiv.org/abs/2403.18514](https://arxiv.org/abs/2403.18514)

    该研究利用3D正规化流技术开发出一种新型模型CT-3DFlow，通过在健康数据上进行无监督训练，在患者级别的胸部CT数据中实现病理性肺部病变的检测。

    

    无监督的病理性检测可以通过仅在健康数据上训练模型并在推断时测量与训练集的偏差来实现，例如基于CNN的特征提取和单类分类器，或基于重构分数的方法，如自编码器，GAN和扩散模型。 正规化流（NF）具有直接通过可逆架构学习训练样本的概率分布的能力。 我们利用这一性质在一种名为CT-3DFlow的新型3D NF模型中，专门为胸部CT数据中的患者级肺部病理检测进行定制。 我们在健康的3D肺部CT块上进行无监督训练，并检测其对数似然分布的偏差作为异常。 我们从患者的CT扫描中汇总块级似然值，以提供患者级的'正常'/'异常'预测。 使用外分布检测性能进行评估。

    arXiv:2403.18514v1 Announce Type: cross  Abstract: Unsupervised pathology detection can be implemented by training a model on healthy data only and measuring the deviation from the training set upon inference, for example with CNN-based feature extraction and one-class classifiers, or reconstruction-score-based methods such as AEs, GANs and Diffusion models. Normalizing Flows (NF) have the ability to directly learn the probability distribution of training examples through an invertible architecture. We leverage this property in a novel 3D NF-based model named CT-3DFlow, specifically tailored for patient-level pulmonary pathology detection in chest CT data. Our model is trained unsupervised on healthy 3D pulmonary CT patches, and detects deviations from its log-likelihood distribution as anomalies. We aggregate patches-level likelihood values from a patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction. Out-of-distribution detection performance is evaluated using e
    
[^47]: 在噪声链路上的分布式最大一致性

    Distributed Maximum Consensus over Noisy Links

    [https://arxiv.org/abs/2403.18509](https://arxiv.org/abs/2403.18509)

    提出了一种噪声鲁棒分布式最大一致性算法（RD-MC），通过将最大一致性问题重新定义为分布式优化问题，并使用交替方向乘法器方法求解，仅使用单组估计从而提高了鲁棒性和效率，在多智能体网络中更加抗噪声。

    

    我们引入了一种名为噪声鲁棒分布式最大一致性（RD-MC）的分布式算法，用于在存在嘈杂通信链路的多智能体网络中估计最大值。我们的方法重新定义了最大一致性问题，将其作为分布式优化问题进行求解，从而可以利用交替方向乘法器方法的解决方案。与依赖多组噪声污染估计的现有算法不同，RD-MC采用单组估计，提高了鲁棒性和效率。为了进一步减轻链路噪声的影响并提高鲁棒性，我们将移动平均应用于本地估计。通过大量的仿真实验，我们证明了RD-MC相对于现有的最大一致性算法在通信链路噪声方面具有显著的鲁棒性。

    arXiv:2403.18509v1 Announce Type: cross  Abstract: We introduce a distributed algorithm, termed noise-robust distributed maximum consensus (RD-MC), for estimating the maximum value within a multi-agent network in the presence of noisy communication links. Our approach entails redefining the maximum consensus problem as a distributed optimization problem, allowing a solution using the alternating direction method of multipliers. Unlike existing algorithms that rely on multiple sets of noise-corrupted estimates, RD-MC employs a single set, enhancing both robustness and efficiency. To further mitigate the effects of link noise and improve robustness, we apply moving averaging to the local estimates. Through extensive simulations, we demonstrate that RD-MC is significantly more robust to communication link noise compared to existing maximum-consensus algorithms.
    
[^48]: 使用线搜索方法加速Transformer微调的收敛速度

    Faster Convergence for Transformer Fine-tuning with Line Search Methods

    [https://arxiv.org/abs/2403.18506](https://arxiv.org/abs/2403.18506)

    将Armijo线搜索与Adam优化器相结合，通过在本地单元执行线搜索，实现了在Transformer微调中收敛速度更快的优化方法。

    

    最近的研究表明，线搜索方法极大地提高了传统随机梯度下降方法在各种数据集和架构上的性能。在这项工作中，我们成功将线搜索方法扩展到了新颖且备受欢迎的Transformer架构和自然语言处理领域的数据集。具体来说，我们将Armijo线搜索与Adam优化器相结合，并通过将网络架构细分为合理的单元，在这些本地单元上分别执行线搜索。我们的优化方法优于传统的Adam优化器，在小数据集或小训练预算的情况下实现了显著的性能改进，同时在其他测试案例中表现相等或更好。我们的工作作为一个Python包公开可用，提供了一个无需超参数的PyTorch优化器，与任意网络架构兼容。

    arXiv:2403.18506v1 Announce Type: cross  Abstract: Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.
    
[^49]: 通过迁移学习直接预测钻孔图像中的矿物含量

    Direct mineral content prediction from drill core images via transfer learning

    [https://arxiv.org/abs/2403.18495](https://arxiv.org/abs/2403.18495)

    该研究通过卷积神经网络，尝试直接从钻孔图像中评估岩石的岩性和矿物含量，以支持和加速地下地质勘探。

    

    深部地下勘探对于矿业、石油和天然气工业至关重要，同时也在化学或核废物处置、以及地热能系统可行性评估中扮演着重要角色。本研究探讨了利用机器学习，特别是卷积神经网络(CNN)，仅通过钻孔图像的分析来评估岩石的岩性和矿物含量的潜力，旨在支持和加速地下地质勘探。

    arXiv:2403.18495v1 Announce Type: cross  Abstract: Deep subsurface exploration is important for mining, oil and gas industries, as well as in the assessment of geological units for the disposal of chemical or nuclear waste, or the viability of geothermal energy systems. Typically, detailed examinations of subsurface formations or units are performed on cuttings or core materials extracted during drilling campaigns, as well as on geophysical borehole data, which provide detailed information about the petrophysical properties of the rocks. Depending on the volume of rock samples and the analytical program, the laboratory analysis and diagnostics can be very time-consuming. This study investigates the potential of utilizing machine learning, specifically convolutional neural networks (CNN), to assess the lithology and mineral content solely from analysis of drill core images, aiming to support and expedite the subsurface geological exploration. The paper outlines a comprehensive methodolo
    
[^50]: 在PINNs中学习：相变、总扩散和泛化

    Learning in PINNs: Phase transition, total diffusion, and generalization

    [https://arxiv.org/abs/2403.18494](https://arxiv.org/abs/2403.18494)

    该研究通过梯度信噪比（SNR）的视角研究了全连接神经网络的学习动态，并在信息瓶颈理论中发现了第三阶段"总扩散"，其特征是均匀的学习速率和梯度，这一阶段标志着快速的训练收敛和增强泛化能力。

    

    我们通过梯度信噪比（SNR）的视角研究全连接神经网络的学习动态，探讨了Adam等一阶优化器在非凸目标中的行为。通过在信息瓶颈理论中解释漂移/扩散相，聚焦梯度均一性，我们确定了一个被称为“总扩散”的第三阶段，其特征是学习速率和梯度均匀，这一阶段标志着SNR急剧增加，样本空间中残差均匀且训练收敛最快。我们提出了一种基于残差的重新加权方案来加速二次损失函数中的扩散，从而增强泛化能力。我们还探索了信息压缩现象，确定在总扩散阶段激活发生显著饱和诱导的压缩，更深的层次经历可以忽略的信息损失。

    arXiv:2403.18494v1 Announce Type: new  Abstract: We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives. By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion", characterized by equilibrium in the learning rates and homogeneous gradients. This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence. We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization. We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss. Supporte
    
[^51]: 利用天气预报数据作为深度神经网络模型估计蒸散发的输入的影响

    Impact of Employing Weather Forecast Data as Input to the Estimation of Evapotranspiration by Deep Neural Network Models

    [https://arxiv.org/abs/2403.18489](https://arxiv.org/abs/2403.18489)

    利用天气预报数据作为深度神经网络模型估计蒸散发的输入，解决了使用FAO56-PM方法计算ET0时太阳辐射参数不易获取的问题

    

    参考蒸散发（ET0）是设计智能灌溉调度的关键参数，因为它通过系数与作物的水需求相关。联合国粮食和农业组织提出了一种标准的ET0计算方法（FAO56PM），基于Penman-Monteith方程的参数化，该方法在文献中被广泛采用。使用FAO56-PM方法计算ET0需要四个主要的天气参数：温度、湿度、风速和太阳辐射（SR）。一种预测未来几天的每日ET0值的方法是利用免费提供的天气预报服务（WFSs），这些服务可估计多种气象参数长达未来15天。这种方法的问题在于当前大多数在线服务没有提供SR作为免费的预测参数，通常这样的预测需要支付金钱。因此，出现了几种使用

    arXiv:2403.18489v1 Announce Type: new  Abstract: Reference Evapotranspiration (ET0) is a key parameter for designing smart irrigation scheduling, since it is related by a coefficient to the water needs of a crop. The United Nations Food and Agriculture Organization, proposed a standard method for ET0 computation (FAO56PM), based on the parameterization of the Penman-Monteith equation, that is widely adopted in the literature. To compute ET0 using the FAO56-PM method, four main weather parameters are needed: temperature, humidity, wind, and solar radiation (SR). One way to make daily ET0 estimations for future days is to use freely available weather forecast services (WFSs), where many meteorological parameters are estimated up to the next 15 days. A problem with this method is that currently, SR is not provided as a free forecast parameter on most of those online services or, normally, such forecasts present a financial cost penalty. For this reason, several ET0 estimation models using
    
[^52]: 使用条件扩散模型从事件相关电位范式中合成脑电图信号

    Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models

    [https://arxiv.org/abs/2403.18486](https://arxiv.org/abs/2403.18486)

    通过引入使用无分类器引导的条件扩散模型，可以直接生成特定主体、会话和类别的EEG数据，结果表明该模型生成的数据与真实数据相似。

    

    大脑-计算机接口领域中的数据稀缺问题可以通过使用生成模型，特别是扩散模型得以缓解。虽然扩散模型先前已成功应用于脑电图（EEG）数据，但现有模型在采样灵活性方面存在限制或需要EEG数据的替代表示。为了克服这些限制，我们介绍了一种新颖的条件扩散模型方法，利用无分类器引导直接生成特定主体、会话和类别的EEG数据。除了常用的指标外，还使用领域特定的指标来评估生成样本的特定性。结果表明，所提出的模型可以生成与每个主体、会话和类别的真实数据相似的EEG数据。

    arXiv:2403.18486v1 Announce Type: cross  Abstract: Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models. While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.
    
[^53]: SingularTrajectory: 使用扩散模型的通用轨迹预测器

    SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model

    [https://arxiv.org/abs/2403.18452](https://arxiv.org/abs/2403.18452)

    该论文提出了SingularTrajectory，一种基于扩散模型的通用轨迹预测框架，旨在统一不同任务的人体运动动态表示。

    

    本文提出了SingularTrajectory，一种基于扩散的通用轨迹预测框架，旨在减少五种任务之间的性能差距。该框架的核心是统一各种人类动力学表示在相关任务上的应用。

    arXiv:2403.18452v1 Announce Type: cross  Abstract: There are five types of trajectory prediction tasks: deterministic, stochastic, domain adaptation, momentary observation, and few-shot. These associated tasks are defined by various factors, such as the length of input paths, data split and pre-processing methods. Interestingly, even though they commonly take sequential coordinates of observations as input and infer future paths in the same coordinates as output, designing specialized architectures for each task is still necessary. For the other task, generality issues can lead to sub-optimal performances. In this paper, we propose SingularTrajectory, a diffusion-based universal trajectory prediction framework to reduce the performance gap across the five tasks. The core of SingularTrajectory is to unify a variety of human dynamics representations on the associated tasks. To do this, we first build a Singular space to project all types of motion patterns from each task into one embeddi
    
[^54]: CoRAST：面向资源受限的CPS和IoT中基于基础模型的相关数据分析

    CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT

    [https://arxiv.org/abs/2403.18451](https://arxiv.org/abs/2403.18451)

    CoRAST提出了一种新颖的学习框架，利用基础模型(FMs)增强了分布式、相关的异构数据的分析。

    

    基础模型(FMs)作为一种有前途的解决方案出现，通过利用先前的知识来理解异构数据集中复杂的时间和空间相关性，从而利用分布式和多样化的环境数据。与联合学习等分布式学习框架不同，它们通常难以处理多模态数据，FMs可以将不同的输入转换为嵌入。这一过程有助于整合各种模态的信息，并将先前的学习应用于新的领域。然而，在资源受限的边缘系统中部署FMs会带来重大挑战。为此，我们引入了CoRAST，这是一个新颖的学习框架，利用FMs增强了分布式、相关的异构数据的分析。通过利用基于服务器的FM，CoRAST可以利用现有的环境信息来提取传感器数据之间的时间、空间和跨模态相关性。

    arXiv:2403.18451v1 Announce Type: cross  Abstract: Foundation models (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets. Unlike distributed learning frameworks such as federated learning, which often struggle with multimodal data, FMs can transform diverse inputs into embeddings. This process facilitates the integration of information from various modalities and the application of prior learning to new domains. However, deploying FMs in resource-constrained edge systems poses significant challenges. To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data. Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data. This enables CoRAST to o
    
[^55]: 语言能否胜过数值回归？基于语言的多模态轨迹预测

    Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction

    [https://arxiv.org/abs/2403.18447](https://arxiv.org/abs/2403.18447)

    本文提出了LMTraj（基于语言的多模态轨迹预测器），将轨迹预测任务转化为一种类似于问答问题的形式，通过将数值和图像数据转换为自然语言空间来引导语言模型继续生成与轨迹相关的多模态预测。

    

    arXiv:2403.18447v1 公告类型：新摘要：语言模型展示了在上下文理解和生成性能方面令人印象深刻的能力。受到近期语言基础模型成功的启发，在本文中，我们提出了LMTraj（基于语言的多模态轨迹预测器），将轨迹预测任务重新设计为一种类似于问答问题的形式。与传统的将轨迹坐标序列视为连续信号的数值回归模型不同，我们将其视为类似于文本提示的离散信号。特别地，我们首先将轨迹坐标的输入空间转换成自然语言空间。在这里，行人的整个时间序列轨迹被转换为一个文本提示，场景图像通过图像字幕被描述为文本信息。然后，转换后的数值和图像数据被包装进问答模板中，供语言模型使用。接下来，为了引导语言模型继续生成与轨迹相关的多模态预测，我们提出了一种联合训练策略，同时优化文本和图像的编码器和解码器。

    arXiv:2403.18447v1 Announce Type: new  Abstract: Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language mo
    
[^56]: FRESCO：合作优化的联邦强化能源系统

    FRESCO: Federated Reinforcement Energy System for Cooperative Optimization

    [https://arxiv.org/abs/2403.18444](https://arxiv.org/abs/2403.18444)

    FRESCO是一个旨在通过使用强化学习代理的分层控制架构来简化能源市场实施的框架，核心概念是创建一个合作设置，使各个个体目标得以实现。

    

    随着可再生能源的兴起，能源网格中出现了新的动态，承诺创造一个更清洁、更参与式的能源网格，在这个网格中，技术在实现所需灵活性方面起着至关重要的作用，以实现下一代网格的愿景。本文介绍了FRESCO，一个旨在通过使用强化学习代理的分层控制架构来简化能源市场实施的框架。我们证明的核心概念是，让贪婪代理受高级代理变化的条件支配，创造出一个合作设置，这将允许实现所有个体目标。

    arXiv:2403.18444v1 Announce Type: new  Abstract: The rise in renewable energy is creating new dynamics in the energy grid that promise to create a cleaner and more participative energy grid, where technology plays a crucial part in making the required flexibility to achieve the vision of the next-generation grid. This work presents FRESCO, a framework that aims to ease the implementation of energy markets using a hierarchical control architecture of reinforcement learning agents trained using federated learning. The core concept we are proving is that having greedy agents subject to changing conditions from a higher level agent creates a cooperative setup that will allow for fulfilling all the individual objectives. This paper presents a general overview of the framework, the current progress, and some insights we obtained from the recent results.
    
[^57]: 智能电网的广义策略学习：FL TRPO方法

    Generalized Policy Learning for Smart Grids: FL TRPO Approach

    [https://arxiv.org/abs/2403.18439](https://arxiv.org/abs/2403.18439)

    该研究提出了将联邦学习与Trust Region Policy Optimization相结合的框架，旨在降低智能电网的能源排放和成本，实验结果证实了该方法的有效性。

    

    智能电网领域需要增强现有能源管理系统的能力；联邦学习（FL）与此目标一致，因为它展示了在异构数据集上训练模型的显着能力，同时保持数据隐私，使其适用于智能电网应用，这些应用往往涉及不同的数据分布和特征之间的相互依赖，这些因素阻碍了线性模型的适用性。本文介绍了将FL与Trust Region Policy Optimization (FL TRPO)相结合的框架，旨在降低能源相关排放和成本。我们的方法揭示了潜在的相互关系，并采用个性化的编码方法来捕捉独特的见解，从而理解特征之间的关系和最佳策略，使我们的模型能够泛化到以前未见的数据。实验结果验证了我们方法的稳健性，证实了其在有效学习方面的高效能力。

    arXiv:2403.18439v1 Announce Type: new  Abstract: The smart grid domain requires bolstering the capabilities of existing energy management systems; Federated Learning (FL) aligns with this goal as it demonstrates a remarkable ability to train models on heterogeneous datasets while maintaining data privacy, making it suitable for smart grid applications, which often involve disparate data distributions and interdependencies among features that hinder the suitability of linear models. This paper introduces a framework that combines FL with a Trust Region Policy Optimization (FL TRPO) aiming to reduce energy-associated emissions and costs. Our approach reveals latent interconnections and employs personalized encoding methods to capture unique insights, understanding the relationships between features and optimal strategies, allowing our model to generalize to previously unseen data. Experimental results validate the robustness of our approach, affirming its proficiency in effectively learn
    
[^58]: 全球植被建模与预训练天气Transformer

    Global Vegetation Modeling with Pre-Trained Weather Transformers

    [https://arxiv.org/abs/2403.18438](https://arxiv.org/abs/2403.18438)

    该研究针对全球植被活动建模，在中期天气预测成功使用的Transformer模型基础上，通过调整预训练FourCastNet模型，展示了利用预训练天气模型能提升全球植被活动估算效果。

    

    准确的植被模型可以更深入地了解植被活动与生态系统过程之间复杂的相互作用。由于长期趋势和温度、降水的短期变化对植被活动有影响，我们受到基于Transformer的深度学习模型在中期天气预测中取得的成功启发，将公开可用的预训练FourCastNet模型进行调整，以模拟植被活动并考虑气候变化的短期动态。我们探讨了学习的大气状态的全球表示如何转移，以模拟归一化差异植被指数（NDVI）。我们的模型以\SI{0.25}{\degree}的分辨率全球估计植被活动，只依赖于气象数据。我们证明利用预训练的天气模型能提升NDVI估算效果，相较于单独学习NDVI。

    arXiv:2403.18438v1 Announce Type: new  Abstract: Accurate vegetation models can produce further insights into the complex interaction between vegetation activity and ecosystem processes. Previous research has established that long-term trends and short-term variability of temperature and precipitation affect vegetation activity. Motivated by the recent success of Transformer-based Deep Learning models for medium-range weather forecasting, we adapt the publicly available pre-trained FourCastNet to model vegetation activity while accounting for the short-term dynamics of climate variability. We investigate how the learned global representation of the atmosphere's state can be transferred to model the normalized difference vegetation index (NDVI). Our model globally estimates vegetation activity at a resolution of \SI{0.25}{\degree} while relying only on meteorological data. We demonstrate that leveraging pre-trained weather models improves the NDVI estimates compared to learning an NDVI 
    
[^59]: 有条件信任环境中的协作式主动学习

    Collaborative Active Learning in Conditional Trust Environment

    [https://arxiv.org/abs/2403.18436](https://arxiv.org/abs/2403.18436)

    该论文介绍了一种新的协作式主动学习框架，使多个合作者能够在不披露已有数据和模型的情况下，通过分享新领域的预测结果和新获得的标签来探索新领域，从而实现隐私安全、资源共享和成本效益的优势。

    

    在这篇论文中，我们研究了在有条件信任环境中的协作式主动学习，这是一种多个合作者通过利用其结合的机器学习能力来探索一个新领域的范例，而不需透露他们现有的数据和模型。相反，合作者们分享来自新领域的预测结果和新获得的标签。这种协作提供了几个优势：(a) 通过消除直接的模型和数据披露需求，解决了隐私和安全问题；(b) 无需直接数据交换，实现了利用不同数据源和洞见；以及(c) 通过共享标记成本，促进了成本效益和资源效率。为了实现这些好处，我们引入了一个设计用于实现上述目标的协作式主动学习框架。我们通过仿真验证了所提出框架的有效性。结果表明，协作会导致更高的AUC分数。

    arXiv:2403.18436v1 Announce Type: new  Abstract: In this paper, we investigate collaborative active learning, a paradigm in which multiple collaborators explore a new domain by leveraging their combined machine learning capabilities without disclosing their existing data and models. Instead, the collaborators share prediction results from the new domain and newly acquired labels. This collaboration offers several advantages: (a) it addresses privacy and security concerns by eliminating the need for direct model and data disclosure; (b) it enables the use of different data sources and insights without direct data exchange; and (c) it promotes cost-effectiveness and resource efficiency through shared labeling costs. To realize these benefits, we introduce a collaborative active learning framework designed to fulfill the aforementioned objectives. We validate the effectiveness of the proposed framework through simulations. The results demonstrate that collaboration leads to higher AUC sco
    
[^60]: U-Sketch: 一种用于草图到图像扩散模型的高效方法

    U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models

    [https://arxiv.org/abs/2403.18425](https://arxiv.org/abs/2403.18425)

    引入了U-Sketch框架，具有U-Net类型的潜在边缘预测器，能够有效地改进草图到图像扩散模型的空间布局生成。

    

    扩散模型在文本到图像合成中表现出色，产生了符合相应文本提示的逼真高分辨率图像。尽管取得了巨大成功，但在草图到图像合成任务中仍有所欠缺，生成图像的空间布局不仅要遵循文本提示，还必须紧密跟随某些参考草图的轮廓。最近提出了使用MLP潜在边缘预测器来引导合成图像的空间布局，通过在每个去噪步骤预测边缘地图。尽管取得了有希望的结果，但MLP的逐像素操作并未将空间布局作为整体考虑进来，需要大量去噪迭代才能生成令人满意的图像，导致时间效率低下。因此，我们引入了U-Sketch，这是一个引入了U-Net类型潜在边缘预测器的框架，能够高效地

    arXiv:2403.18425v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable performance in text-to-image synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts. Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches. Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed. Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency. To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently
    
[^61]: SemRoDe: 宏观对抗训练以学习对单词级攻击具有鲁棒性的表示

    SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks

    [https://arxiv.org/abs/2403.18423](https://arxiv.org/abs/2403.18423)

    提出了一种称为Semantic Robust Defence (SemRoDe)的新方法，通过宏观对抗训练策略增强了语言模型（LMs）的鲁棒性，学习了能够连接对抗领域和基本领域的鲁棒表示。

    

    语言模型（LMs）是自然语言处理任务中不可或缺的工具，但它们对对抗攻击的脆弱性仍然令人担忧。尽管当前的研究已经探索了对抗训练技术，但其在防御单词级攻击方面的改进仍然有限。在这项工作中，我们提出了一种名为语义鲁棒防御（SemRoDe）的新方法，这是一种宏观对抗训练策略，旨在增强LMs的鲁棒性。受到图像领域最近研究的启发，我们探讨并确认在像语言这样的离散数据设置中，通过单词替换生成的对抗样本确实属于一个展现与基本域具有高Wasserstein距离的对抗领域。我们的方法学习了一个能够连接这两个领域的鲁棒表示。我们假设如果样本被投影到一个非对抗领域，而是投影到一个具有最小偏移的领域，那么可能会...

    arXiv:2403.18423v1 Announce Type: new  Abstract: Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it w
    
[^62]: Transformer网络的拓扑结构

    The Topos of Transformer Networks

    [https://arxiv.org/abs/2403.18415](https://arxiv.org/abs/2403.18415)

    通过拓扑理论的视角，我们对Transformer架构的表达能力进行了理论分析，发现它具有高阶推理的特点，并与其他常见神经网络架构进行了对比。

    

    Transformer神经网络已经远远超越所有其他神经网络架构，成为大型语言模型背后的引擎。我们通过拓扑理论的视角提供了对Transformer架构表达能力的理论分析。从这个观点出发，我们展示了许多常见的神经网络架构，如卷积网络、循环网络和图卷积网络，可以嵌入在分段线性函数的预拓扑中，但Transformer必然存在于其拓扑完备性中。特别地，这表明这两个网络家族实例化了不同的逻辑片段：前者是一阶的，而Transformers是高阶推理机。此外，我们还将拓扑理论与架构搜索和梯度下降进行了类比，将我们的分析纳入了控制论代理的框架中。

    arXiv:2403.18415v1 Announce Type: new  Abstract: The transformer neural network has significantly out-shined all other neural network architectures as the engine behind large language models. We provide a theoretical analysis of the expressivity of the transformer architecture through the lens of topos theory. From this viewpoint, we show that many common neural network architectures, such as the convolutional, recurrent and graph convolutional networks, can be embedded in a pretopos of piecewise-linear functions, but that the transformer necessarily lives in its topos completion. In particular, this suggests that the two network families instantiate different fragments of logic: the former are first order, whereas transformers are higher-order reasoners. Furthermore, we draw parallels with architecture search and gradient descent, integrating our analysis in the framework of cybernetic agents.
    
[^63]: 图像网格可能比视频更有价值：使用VLM进行零样本视频问答

    An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM

    [https://arxiv.org/abs/2403.18406](https://arxiv.org/abs/2403.18406)

    本研究提出了一种新颖的策略，使用单一的Vision Language Model (VLM) 来进行零样本视频问答，将视频转换为单个合成图像以实现视频理解。

    

    受最近大型语言模型（LLM）复杂推理能力的启发，人们提出了各种用于连接视频模态的策略。其中一种突出的策略涉及视频语言模型（VideoLMs），通过训练一个可学习的接口将先进的视觉编码器与LLMs连接起来。最近，出现了另一种旨在通过多个阶段跨模态进行模态桥接的策略，利用现成的基础模型，如VideoLMs和LLMs。本研究中，我们介绍了一种简单却新颖的策略，只使用单一的视觉语言模型（VLM）。我们的出发点是视频包含一系列图像或帧，这些图像与时间信息交织在一起的简单洞察。视频理解的精髓在于巧妙地管理每个帧的时间方面以及空间细节。初始时，我们通过排列多个帧将视频转换为单个合成图像。

    arXiv:2403.18406v1 Announce Type: cross  Abstract: Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging mul
    
[^64]: 在多分类器融合框架中对电网分类使用电网频率的频谱图分析

    On Spectrogram Analysis in a Multiple Classifier Fusion Framework for Power Grid Classification Using Electric Network Frequency

    [https://arxiv.org/abs/2403.18402](https://arxiv.org/abs/2403.18402)

    该论文提出了一种利用电网频率进行电网分类的新方法，通过生成频谱图并融合多个分类器，最终设计出一个专门用于融合过程的神经网络，实验结果展示其优于当前最先进分类器的准确性。

    

    电网频率（ENF）作为供电系统固有的独特标识。本文开发了一种利用ENF的电网分类新方法。从不同电网的音频和功率记录中生成频谱图，揭示有助于通过多个分类器融合进行电网分类的独特ENF模式。针对One-vs-All分类，开发了四种传统机器学习分类器以及使用神经结构搜索进行优化的卷积神经网络（CNN）。该过程对每个样本生成大量预测结果，然后编制并用于训练一个浅层多标签神经网络，该网络专门设计用于模拟融合过程，最终导致对每个样本的确定类别预测。实验结果表明，验证和测试准确性均优于当前最先进分类器的准确性，突显了其优势。

    arXiv:2403.18402v1 Announce Type: new  Abstract: The Electric Network Frequency (ENF) serves as a unique signature inherent to power distribution systems. Here, a novel approach for power grid classification is developed, leveraging ENF. Spectrograms are generated from audio and power recordings across different grids, revealing distinctive ENF patterns that aid in grid classification through a fusion of classifiers. Four traditional machine learning classifiers plus a Convolutional Neural Network (CNN), optimized using Neural Architecture Search, are developed for One-vs-All classification. This process generates numerous predictions per sample, which are then compiled and used to train a shallow multi-label neural network specifically designed to model the fusion process, ultimately leading to the conclusive class prediction for each sample. Experimental findings reveal that both validation and testing accuracy outperform those of current state-of-the-art classifiers, underlining the
    
[^65]: 使用改进的深度卷积生成对抗网络在抽象艺术中进行颜色和笔触模式识别

    Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks

    [https://arxiv.org/abs/2403.18397](https://arxiv.org/abs/2403.18397)

    本文通过引入改进的深度卷积生成对抗网络(mDCGAN)，针对高质量艺术品生成进行了研究，解决了普遍训练问题，有效探索抽象绘画中的颜色和笔触模式。

    

    抽象艺术是一种广受欢迎、被广泛讨论的艺术形式，通常能够描绘出艺术家的情感。许多研究人员尝试使用机器学习和深度学习的边缘检测、笔触和情感识别算法来研究抽象艺术。本文描述了使用生成对抗神经网络(GAN)对广泛分布的抽象绘画进行研究。 GAN具有学习和再现分布的能力，使研究人员能够有效地探索和研究生成的图像空间。然而，挑战在于开发一种能够克服常见训练问题的高效GAN架构。本文通过引入专门设计用于高质量艺术品生成的改进DCGAN(mDCGAN)来解决这一挑战。该方法涉及对所做修改的深入探讨，深入研究DCGAN的复杂工作。

    arXiv:2403.18397v1 Announce Type: cross  Abstract: Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, opt
    
[^66]: 基于张量的一致性和特异性多视角聚类图学习

    Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering

    [https://arxiv.org/abs/2403.18393](https://arxiv.org/abs/2403.18393)

    本文提出了一种新颖的基于张量的图学习框架，同时考虑了多视图聚类的一致性和特异性，解决了现有方法在相似性测量和图信息利用方面的局限性。

    

    图学习被广泛认为是多视角聚类中的关键技术。现有的图学习方法通常涉及基于概率邻居构建自适应邻居图，然后学习一致性图进行聚类，然而，它们面临两个局限性。首先，它们通常依赖欧氏距离来衡量相似性，这在许多真实场景中捕捉数据点间的内在结构时证明是不足够的。其次，大多数这些方法仅关注一致性图，忽略了特定视图的图信息。针对上述缺点，本文提出了一种新颖的基于张量的图学习框架，同时考虑了多视图聚类的一致性和特异性。具体地，我们在斯蒂弗尔流形上计算相似距离以保留内在str

    arXiv:2403.18393v1 Announce Type: new  Abstract: Graph learning is widely recognized as a crucial technique in multi-view clustering. Existing graph learning methods typically involve constructing an adaptive neighbor graph based on probabilistic neighbors and then learning a consensus graph to for clustering, however, they are confronted with two limitations. Firstly, they often rely on Euclidean distance to measure similarity when constructing the adaptive neighbor graph, which proves inadequate in capturing the intrinsic structure among data points in many real-world scenarios. Secondly, most of these methods focus solely on consensus graph, ignoring view-specific graph information. In response to the aforementioned drawbacks, we in this paper propose a novel tensor-based graph learning framework that simultaneously considers consistency and specificity for multi-view clustering. Specifically, we calculate the similarity distance on the Stiefel manifold to preserve the intrinsic str
    
[^67]: 多模态生成模型是良好的类增量学习器

    Generative Multi-modal Models are Good Class-Incremental Learners

    [https://arxiv.org/abs/2403.18383](https://arxiv.org/abs/2403.18383)

    多模态生成模型为类增量学习提出了一种新的框架，可以直接为图像生成标签。

    

    在类增量学习（CIL）场景中，由于分类器对当前任务的偏见而导致的灾难性遗忘现象长期以来一直是一个重大挑战。这主要是由判别模型的特性所致。随着多模态生成模型越来越受欢迎，我们将探讨将判别模型替换为生成模型以用于CIL。然而，从判别模型过渡到生成模型需要解决两个关键挑战。主要挑战在于将生成的文本信息转移到不同类别的分类中。此外，它还需要将CIL任务置于生成框架中。为此，我们提出了一种用于类增量学习的新型多模态生成模型（GMM）框架。我们的方法直接使用经调整的生成模型为图像生成标签。在获得详细文本后，我们使用

    arXiv:2403.18383v1 Announce Type: cross  Abstract: In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use 
    
[^68]: IIP-Mixer：用于电池剩余寿命预测的Intra-Inter Patch混合架构

    IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction

    [https://arxiv.org/abs/2403.18379](https://arxiv.org/abs/2403.18379)

    提出了一种基于MLPs的IIP-Mixer架构，旨在通过在内部补丁和跨补丁维度进行混合操作，实现锂离子电池剩余寿命预测。

    

    准确估计锂离子电池的剩余寿命对于维持可充电电池管理系统的安全稳定运行至关重要。然而，由于涉及复杂的时间动态，这项任务通常具有挑战性。最近，基于注意力的网络，如变压器和Informer，在时间序列预测中已经成为流行的架构。尽管它们有效，但这些具有丰富参数的模型必须耗费大量训练时间才能揭示时间模式。为了解决这些挑战，我们提出了一个简单的基于MLP-Mixer的架构，名为“Intra-Inter Patch Mixer”（IIP-Mixer），它是一种仅基于多层感知器（MLPs）的架构，通过沿着内部补丁和跨补丁维度进行混合操作来提取有关电池剩余寿命预测的信息。所提出的IIP-Mixer包括并行的双头混合器层：内部补丁混合ML

    arXiv:2403.18379v1 Announce Type: cross  Abstract: Accurately estimating the Remaining Useful Life (RUL) of lithium-ion batteries is crucial for maintaining the safe and stable operation of rechargeable battery management systems. However, this task is often challenging due to the complex temporal dynamics involved. Recently, attention-based networks, such as Transformers and Informer, have been the popular architecture in time series forecasting. Despite their effectiveness, these models with abundant parameters necessitate substantial training time to unravel temporal patterns. To tackle these challenges, we propose a simple MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which is an architecture based exclusively on multi-layer perceptrons (MLPs), extracting information by mixing operations along both intra-patch and inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer comprises parallel dual-head mixer layers: the intra-patch mixing ML
    
[^69]: 基于层次模型更新的关注Stragglers的低延迟同步联邦学习

    Stragglers-Aware Low-Latency Synchronous Federated Learning via Layer-Wise Model Updates

    [https://arxiv.org/abs/2403.18375](https://arxiv.org/abs/2403.18375)

    提出了一种基于层次的关注Stragglers的低延迟同步联邦学习方法，允许stragglers同步传递部分梯度，从而改善在训练延迟约束下的模型性能。

    

    同步联邦学习(FL)是协作边缘学习的一种流行范式。它通常涉及一组异构设备在本地并行训练神经网络(NN)模型，并定期进行集中聚合。由于一些设备可能具有有限的计算资源和可变的可用性，FL的延迟对stragglers非常敏感。传统方法会丢弃stragglers执行的不完整的模型内更新，改变本地工作量和体系结构的数量，或者转而使用异步设置；所有这些都会影响在严格的训练延迟约束下的训练模型性能。在这项工作中，我们提出了关注stragglers的基于层次的联邦学习(SALF)，通过反向传播利用神经网络(NNs)的优化过程以层次方式更新全局模型。SALF允许stragglers同步传递部分梯度，使得全局模型的每一层都可以更新

    arXiv:2403.18375v1 Announce Type: new  Abstract: Synchronous federated learning (FL) is a popular paradigm for collaborative edge learning. It typically involves a set of heterogeneous devices locally training neural network (NN) models in parallel with periodic centralized aggregations. As some of the devices may have limited computational resources and varying availability, FL latency is highly sensitive to stragglers. Conventional approaches discard incomplete intra-model updates done by stragglers, alter the amount of local workload and architecture, or resort to asynchronous settings; which all affect the trained model performance under tight training latency constraints. In this work, we propose straggler-aware layer-wise federated learning (SALF) that leverages the optimization procedure of NNs via backpropagation to update the global model in a layer-wise fashion. SALF allows stragglers to synchronously convey partial gradients, having each layer of the global model be updated 
    
[^70]: 视野中的船只：船只图像超分辨率的扩散模型

    Ship in Sight: Diffusion Models for Ship-Image Super Resolution

    [https://arxiv.org/abs/2403.18370](https://arxiv.org/abs/2403.18370)

    该研究探索了船只图像超分辨率问题，提出了一种基于扩散模型的架构，利用文本条件在训练期间，并在类别感知的情况下，以最佳方式保存船只的关键细节。

    

    近年来，图像生成领域取得了显著进展，主要受到不断增长的对各种图像生成子任务（如修补、去噪和超分辨率）高质量结果的需求驱动。我们的方法深入探讨了船只图像超分辨率问题，这对沿海和港口监视至关重要。我们研究了文本到图像扩散模型的应用机会，利用这些基础模型已经学到的先验知识。具体来说，我们提出了一种基于扩散模型的架构，利用文本条件在训练期间，并在类别感知的情况下，以最佳方式保存船只的关键细节在超分辨率生成过程中。

    arXiv:2403.18370v1 Announce Type: cross  Abstract: In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resolut
    
[^71]: 面向5G-NR的意图感知DRL上行动态调度器

    Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR

    [https://arxiv.org/abs/2403.18364](https://arxiv.org/abs/2403.18364)

    提出了一种面向工业物联网用户设备（IIoT UEs）的意图感知DRL上行动态调度器，通过深度强化学习（DRL）学习如何调度通信资源，并利用图结构的简化方案加速收敛，相较于传统调度方案，能有效保证IIoT UEs的意图表达。

    

    我们研究了支持工业物联网用户设备（IIoT UEs）具有意图（即所请求的服务质量（QoS））和随机流量到达的问题。提出了一种基于深度强化学习（DRL）的集中动态调度器，用于学习如何在IIoT UEs之间调度可用通信资源的时间频率资源。所提出的调度器利用RL框架来适应无线通信系统和流量到达中的动态变化。此外，提出了一种基于图的简化方案，以减少RL框架的状态和动作空间，以实现快速收敛和更好的学习策略。仿真结果表明，与几种传统调度方案（如轮询、半静态和启发式方法）相比，所提出的智能调度器在保证IIoT UEs所表达的意图方面具有有效性。

    arXiv:2403.18364v1 Announce Type: cross  Abstract: We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival. A deep reinforcement learning (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs. The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals. Moreover, a graph-based reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy. Simulation results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches. The proposed sche
    
[^72]: 监督多核学习方法用于多组学数据集成

    Supervised Multiple Kernel Learning approaches for multi-omics data integration

    [https://arxiv.org/abs/2403.18355](https://arxiv.org/abs/2403.18355)

    MKL方法提供了一种灵活有效的多组学数据集成方法，可以与复杂的监督式多组学整合方法竞争

    

    高通量技术的进展导致越来越多的组学数据集的可用性。多种异质数据源的集成目前是生物学和生物信息学领域的一个问题。多核学习（MKL）已被证明是一种灵活和有效的方法，可以考虑多组学输入的多样性，尽管它在基因组数据挖掘中是一种不常用的工具。我们提供了基于不同核融合策略的新颖MKL方法。为了从输入核的元核中学习，我们将无监督集成算法调整为支持向量机的监督任务。我们还测试了用于核融合和分类的深度学习架构。结果显示，基于MKL的模型可以与更复杂、最先进的监督式多组学整合方法竞争。多核学习为多组学基因组数据中的预测模型提供了一个自然的框架。

    arXiv:2403.18355v1 Announce Type: cross  Abstract: Advances in high-throughput technologies have originated an ever-increasing availability of omics datasets. The integration of multiple heterogeneous data sources is currently an issue for biology and bioinformatics. Multiple kernel learning (MKL) has shown to be a flexible and valid approach to consider the diverse nature of multi-omics inputs, despite being an underused tool in genomic data mining.We provide novel MKL approaches based on different kernel fusion strategies.To learn from the meta-kernel of input kernels, we adaptedunsupervised integration algorithms for supervised tasks with support vector machines.We also tested deep learning architectures for kernel fusion and classification.The results show that MKL-based models can compete with more complex, state-of-the-art, supervised multi-omics integrative approaches. Multiple kernel learning offers a natural framework for predictive models in multi-omics genomic data. Our resu
    
[^73]: 生成用于基于视觉的农业应用的多样化农业数据

    Generating Diverse Agricultural Data for Vision-Based Farming Applications

    [https://arxiv.org/abs/2403.18351](https://arxiv.org/abs/2403.18351)

    提出了一种专门用于生成合成农业场景的程序化模型，能够模拟植物的生长阶段、土壤条件和光照变化，为精准农业中的计算机视觉任务提供了全面资源，验证了该模型在提供机器学习训练数据方面的潜力

    

    我们提出了一个专门的程序化模型，用于生成合成的农业场景，重点是大豆作物以及各种杂草。该模型能够模拟这些植物的不同生长阶段、多样化的土壤条件，以及在不同光照条件下的随机田地布局。将现实世界的纹理和环境因素整合到程序化生成过程中，增强了合成数据的逼真度和适用性。我们的数据集包括带有语义标签的12,000张图像，为精准农业中的计算机视觉任务提供了全面的资源，如用于自主除草的语义分割。通过将合成数据与真实农业图像进行比较，我们验证了我们模型的有效性，展示了它在为农业中的机器学习模型提供训练数据方面的潜力。这种方法不仅提供了一种成本-effective的方式

    arXiv:2403.18351v1 Announce Type: cross  Abstract: We present a specialized procedural model for generating synthetic agricultural scenes, focusing on soybean crops, along with various weeds. This model is capable of simulating distinct growth stages of these plants, diverse soil conditions, and randomized field arrangements under varying lighting conditions. The integration of real-world textures and environmental factors into the procedural generation process enhances the photorealism and applicability of the synthetic data. Our dataset includes 12,000 images with semantic labels, offering a comprehensive resource for computer vision tasks in precision agriculture, such as semantic segmentation for autonomous weed control. We validate our model's effectiveness by comparing the synthetic data against real agricultural images, demonstrating its potential to significantly augment training data for machine learning models in agriculture. This approach not only provides a cost-effective s
    
[^74]: 一种基于量子模糊的方法用于实时检测太阳日冕空洞

    A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal Holes

    [https://arxiv.org/abs/2403.18347](https://arxiv.org/abs/2403.18347)

    本论文提出了一种基于量子计算的快速模糊c-均值技术，用于快速检测太阳日冕空洞（CHs）的区域。

    

    太阳日冕空洞（CHs）的检测和分析是太阳物理领域的重要研究领域。对于准确预测地磁风暴，这对各种空间和地面系统都产生直接或间接影响至关重要。迄今为止，太阳科学家依赖手动绘制方法来检测CHs。然而，随着图像处理技术的进步，一些自动图像分割方法已被用于检测CHs。尽管如此，快速准确检测CHs仍然是一个主要问题。在这项工作中，开发了一种新型基于量子计算的快速模糊c-均值技术，用于快速检测CHs区域。任务分为两个阶段进行，在第一阶段使用基于量子计算的快速模糊c-均值（QCFFCM）对太阳图像进行了分割，然后在后续阶段从中提取出CHs。

    arXiv:2403.18347v1 Announce Type: cross  Abstract: The detection and analysis of the solar coronal holes (CHs) is an important field of study in the domain of solar physics. Mainly, it is required for the proper prediction of the geomagnetic storms which directly or indirectly affect various space and ground-based systems. For the detection of CHs till date, the solar scientist depends on manual hand-drawn approaches. However, with the advancement of image processing technologies, some automated image segmentation methods have been used for the detection of CHs. In-spite of this, fast and accurate detection of CHs are till a major issues. Here in this work, a novel quantum computing-based fast fuzzy c-mean technique has been developed for fast detection of the CHs region. The task has been carried out in two stages, in first stage the solar image has been segmented using a quantum computing based fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted out from the 
    
[^75]: 人工神经孪生体--分布式过程链中的流程优化和持续学习

    The Artificial Neural Twin -- Process Optimization and Continual Learning in Distributed Process Chains

    [https://arxiv.org/abs/2403.18343](https://arxiv.org/abs/2403.18343)

    提出了人工神经孪生体，结合了模型预测控制、深度学习和传感器网络的概念，通过可微分数据融合和反向传播损失梯度来实现分布式过程链中的流程优化和持续学习。

    

    工业过程优化和控制对于提高经济和生态效率至关重要。然而，数据主权、不同的目标，或者实施所需的专业知识阻碍了整体实现。此外，越来越多地使用基于数据的AI方法在过程模型和工业传感器中经常需要定期微调以适应分布漂移。我们提出了人工神经孪生体，它结合了模型预测控制、深度学习和传感器网络的概念，以解决这些问题。我们的方法引入了可微分数据融合来估计分布式过程步骤的状态及其对输入数据的依赖。通过将相互连接的过程步骤视为准神经网络，我们可以反向传播损失梯度，用于流程优化或模型微调到过程参数或AI模型中。该概念在虚拟机组模拟中进行了演示。

    arXiv:2403.18343v1 Announce Type: new  Abstract: Industrial process optimization and control is crucial to increase economic and ecologic efficiency. However, data sovereignty, differing goals, or the required expert knowledge for implementation impede holistic implementation. Further, the increasing use of data-driven AI-methods in process models and industrial sensory often requires regular fine-tuning to accommodate distribution drifts. We propose the Artificial Neural Twin, which combines concepts from model predictive control, deep learning, and sensor networks to address these issues. Our approach introduces differentiable data fusion to estimate the state of distributed process steps and their dependence on input data. By treating the interconnected process steps as a quasi neural-network, we can backpropagate loss gradients for process optimization or model fine-tuning to process parameters or AI models respectively. The concept is demonstrated on a virtual machine park simulat
    
[^76]: 通过考虑结构相似性的半监督学习进行宏观裂纹表面分割

    Macroscale fracture surface segmentation via semi-supervised learning considering the structural similarity

    [https://arxiv.org/abs/2403.18337](https://arxiv.org/abs/2403.18337)

    通过半监督学习方法，在考虑结构相似性的情况下，建立了一种用于宏观裂纹表面分割的方法。

    

    截至目前，用于核能领域等领域的材料的安全评估通常依赖于利用宏观概念的断裂力学分析，其中全局载荷量K或J与材料的断裂韧性曲线进行比较。本研究范围内建立了一种半监督训练深度学习模型进行宏观级别裂纹表面分割的方法。因此，创建了三个不同且独特的数据集来分析结构相似性对分割能力的影响。由于评估材料和试样的结构相似性不同，以及不同实验室图像获取中的图像引起的波动，结构相似性各不相同。数据集对应典型的独立实验室条件

    arXiv:2403.18337v1 Announce Type: new  Abstract: To this date the safety assessment of materials, used for example in the nuclear power sector, commonly relies on a fracture mechanical analysis utilizing macroscopic concepts, where a global load quantity K or J is compared to the materials fracture toughness curve. Part of the experimental effort involved in these concepts is dedicated to the quantitative analysis of fracture surfaces. Within the scope of this study a methodology for the semi-supervised training of deep learning models for fracture surface segmentation on a macroscopic level was established. Therefore, three distinct and unique datasets were created to analyze the influence of structural similarity on the segmentation capability. The structural similarity differs due to the assessed materials and specimen, as well as imaging-induced variance due to fluctuations in image acquisition in different laboratories. The datasets correspond to typical isolated laboratory condit
    
[^77]: 面向德语、法语和日语的药物警戒数据集：跨语言标注药物不良反应

    A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages

    [https://arxiv.org/abs/2403.18336](https://arxiv.org/abs/2403.18336)

    本研究构建了一个多语言的药物不良反应语料库，包括德语、法语和日语文本，包含了12种实体类型、四种属性类型和13种关系类型的注释，为医疗保健领域开发现实世界多语言语言模型做出贡献。

    

    用户生成的数据来源在揭示药物不良反应（ADRs）方面变得越来越重要，数字世界中的讨论数量也在增加。然而，现有的临床语料库主要围绕英文科学文章展开。本研究提出了一个包括德语、法语和日语中关于ADR的文本的多语言语料库，来源包括患者论坛、社交媒体和临床报告。我们的语料库包含了12种实体类型、四种属性类型和13种关系类型的注释。这有助于为医疗保健开发现实世界的多语言语言模型。我们提供了一些统计数据以凸显与语料库相关的某些挑战，并进行了初步实验，为在单一语言内部和跨语言之间提取实体和实体之间关系提供了强大的基线。

    arXiv:2403.18336v1 Announce Type: new  Abstract: User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages.
    
[^78]: 使用跟踪辅助的事件相机目标检测

    Tracking-Assisted Object Detection with Event Cameras

    [https://arxiv.org/abs/2403.18330](https://arxiv.org/abs/2403.18330)

    本文利用跟踪策略和自动标记算法，针对事件相机中的难以辨识的对象，揭示其特征信息。

    

    最近，由于事件相机具有高动态范围和无动态模糊等特殊属性，事件驱动目标检测在计算机视觉领域引起了关注。然而，由于特征的不同步性和稀疏性导致了由于相机与之没有相对运动而导致的看不见的对象，这对任务构成了重大挑战。本文将这些看不见的对象视为伪遮挡对象，并旨在揭示其特征。首先，我们引入了对象的可见性属性，并提出了一个自动标记算法，用于在现有的事件相机数据集上附加额外的可见性标签。其次，我们利用跟踪策略来

    arXiv:2403.18330v1 Announce Type: cross  Abstract: Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features. Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies fo
    
[^79]: 隐私保护的分布式非负矩阵分解

    Privacy-Preserving Distributed Nonnegative Matrix Factorization

    [https://arxiv.org/abs/2403.18326](https://arxiv.org/abs/2403.18326)

    提出一种隐私保护算法，实现了全分布式NMF，通过Paillier密码系统保护了代理之间的信息交换，避免了原始数据暴露。

    

    非负矩阵分解（NMF）是一种有效的数据表示工具，在信号处理和机器学习等领域有着众多应用。然而，在自组网络中分布式部署NMF会引入隐私问题，因为传统方法涉及在网络代理之间共享原始数据。为了解决这一问题，我们提出了一种隐私保护算法，用于全分布式NMF，将分布式大数据矩阵分解为左右矩阵因子，同时保护每个代理的本地数据隐私。它促进了代理之间左矩阵因子的协作估计，并使它们能够估计各自的右因子而不暴露原始数据。为了确保数据隐私，我们利用Paillier密码系统在相邻代理之间安全地进行信息交换，这是一种用于公钥加密的概率性非对称算法，允许对加密数据进行计算而不解密。

    arXiv:2403.18326v1 Announce Type: cross  Abstract: Nonnegative matrix factorization (NMF) is an effective data representation tool with numerous applications in signal processing and machine learning. However, deploying NMF in a decentralized manner over ad-hoc networks introduces privacy concerns due to the conventional approach of sharing raw data among network agents. To address this, we propose a privacy-preserving algorithm for fully-distributed NMF that decomposes a distributed large data matrix into left and right matrix factors while safeguarding each agent's local data privacy. It facilitates collaborative estimation of the left matrix factor among agents and enables them to estimate their respective right factors without exposing raw data. To ensure data privacy, we secure information exchanges between neighboring agents utilizing the Paillier cryptosystem, a probabilistic asymmetric algorithm for public-key cryptography that allows computations on encrypted data without decr
    
[^80]: 量子算法：金融犯罪预防的新前沿

    Quantum Algorithms: A New Frontier in Financial Crime Prevention

    [https://arxiv.org/abs/2403.18322](https://arxiv.org/abs/2403.18322)

    量子算法在金融犯罪预防中展现了强大的解决方案，包括量子机器学习和量子人工智能，有效克服了传统方法的局限，并提供支持改进的金融风险管理分析。

    

    金融犯罪迅速蔓延且愈发复杂，需要提供稳健有效的新方法。本文探讨了量子算法在打击金融犯罪中的潜力。通过比较传统方法、机器学习技术和量子方法，突出了量子计算的优势。研究展示了量子机器学习（QML）和量子人工智能（QAI）等先进方法作为强大的解决方案，用于检测和预防金融犯罪，包括洗钱、金融犯罪检测、加密货币攻击和市场操纵。这些量子方法利用量子计算机固有的计算能力来克服受传统方法制约的局限。此外，本文阐明了量子计算如何支持改进的金融风险管理分析。金融机构可以提高它们的

    arXiv:2403.18322v1 Announce Type: new  Abstract: Financial crimes fast proliferation and sophistication require novel approaches that provide robust and effective solutions. This paper explores the potential of quantum algorithms in combating financial crimes. It highlights the advantages of quantum computing by examining traditional and Machine Learning (ML) techniques alongside quantum approaches. The study showcases advanced methodologies such as Quantum Machine Learning (QML) and Quantum Artificial Intelligence (QAI) as powerful solutions for detecting and preventing financial crimes, including money laundering, financial crime detection, cryptocurrency attacks, and market manipulation. These quantum approaches leverage the inherent computational capabilities of quantum computers to overcome limitations faced by classical methods. Furthermore, the paper illustrates how quantum computing can support enhanced financial risk management analysis. Financial institutions can improve thei
    
[^81]: 将主成分分析应用于高性能计算设备进行高光谱降维：结果与比较

    Implementation of the Principal Component Analysis onto High-Performance Computer Facilities for Hyperspectral Dimensionality Reduction: Results and Comparisons

    [https://arxiv.org/abs/2403.18321](https://arxiv.org/abs/2403.18321)

    该研究实现了主成分分析算法在NVIDIA GPU和Kalray多核处理器上，提出了利用高性能计算平台并行性的技巧，以减少处理高光谱图像所需时间。

    

    高光谱成像算法的高效性能关键在于降维作为一个关键的预处理步骤。然而，诸如主成分分析（PCA）之类的降维算法由于计算密集型的特性，建议将其应用于高性能计算架构用于应用在严格的延迟限制下。本研究介绍了PCA算法在两种不同高性能设备上的实现，分别为NVIDIA图形处理单元（GPU）和Kalray多核处理器，揭示了一系列有价值的技巧，以充分利用这些高性能计算平台固有的并行性，从而减少处理给定高光谱图像所需的时间。

    arXiv:2403.18321v1 Announce Type: new  Abstract: Dimensionality reduction represents a critical preprocessing step in order to increase the efficiency and the performance of many hyperspectral imaging algorithms. However, dimensionality reduction algorithms, such as the Principal Component Analysis (PCA), suffer from their computationally demanding nature, becoming advisable for their implementation onto high-performance computer architectures for applications under strict latency constraints. This work presents the implementation of the PCA algorithm onto two different high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and a Kalray manycore, uncovering a highly valuable set of tips and tricks in order to take full advantage of the inherent parallelism of these high-performance computing platforms, and hence, reducing the time that is required to process a given hyperspectral image. Moreover, the achieved results obtained with different hyperspectral images have
    
[^82]: 用于在线临床时间序列应用的多模态对比学习

    Multi-Modal Contrastive Learning for Online Clinical Time-Series Applications

    [https://arxiv.org/abs/2403.18316](https://arxiv.org/abs/2403.18316)

    该论文利用自监督多模态对比学习技术处理临床时间序列数据，提出了多模态邻域对比损失（MM-NCL）函数，展现出优秀的线性探测和零-shot性能。

    

    电子健康记录（EHR）来自重症监护病房（ICU）的数据集包含多种数据形式。尽管先前的工作在监督设置中成功利用了多种形式，我们将先进的自监督多模态对比学习技术应用于ICU数据，特别关注临床笔记和时间序列，用于临床相关的在线预测任务。我们引入了损失函数多模态邻域对比损失（MM-NCL），一个软邻域函数，并展示了我们方法的出色线性探测和零-shot性能。

    arXiv:2403.18316v1 Announce Type: new  Abstract: Electronic Health Record (EHR) datasets from Intensive Care Units (ICU) contain a diverse set of data modalities. While prior works have successfully leveraged multiple modalities in supervised settings, we apply advanced self-supervised multi-modal contrastive learning techniques to ICU data, specifically focusing on clinical notes and time-series for clinically relevant online prediction tasks. We introduce a loss function Multi-Modal Neighborhood Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the excellent linear probe and zero-shot performance of our approach.
    
[^83]: 一种热力学一致的基于物理信息深度学习材料模型，用于短纤维/聚合物纳米复合材料

    A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites

    [https://arxiv.org/abs/2403.18310](https://arxiv.org/abs/2403.18310)

    提出了一种基于物理信息深度学习的热力学一致材料模型，用于研究短纤维/聚合物纳米复合材料的行为，通过组合深度学习网络预测内部变量并定义整个系统的热力学状态。

    

    这项工作提出了一种基于物理信息深度学习（PIDL）的本构模型，用于研究在各种环境条件下短纤维增强纳米粒子填充环氧树脂的粘弹-粘塑行为。深度学习模型经过训练以强制执行热力学原理，从而得到热力学一致的本构模型。为实现此目标，将长短期记忆网络与前馈神经网络相结合，以预测表征纳米复合材料内部耗散所需的内部变量。此外，另一个前馈神经网络用于指示自由能函数，从而定义整个系统的热力学状态。PIDL模型最初针对三维情况进行开发，通过从经典本构模型中生成合成数据来训练模型，然后通过直接提取循环加载数据进行训练。

    arXiv:2403.18310v1 Announce Type: cross  Abstract: This work proposes a physics-informed deep learning (PIDL)-based constitutive model for investigating the viscoelastic-viscoplastic behavior of short fiber-reinforced nanoparticle-filled epoxies under various ambient conditions. The deep-learning model is trained to enforce thermodynamic principles, leading to a thermodynamically consistent constitutive model. To accomplish this, a long short-term memory network is combined with a feed-forward neural network to predict internal variables required for characterizing the internal dissipation of the nanocomposite materials. In addition, another feed-forward neural network is used to indicate the free-energy function, which enables defining the thermodynamic state of the entire system. The PIDL model is initially developed for the three-dimensional case by generating synthetic data from a classical constitutive model. The model is then trained by extracting the data directly from cyclic lo
    
[^84]: 利用SDO/HMI数据和辅助注意力卷积神经网络提升SOHO/MDI太阳活动区磁图的超分辨率

    Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using SDO/HMI Data and an Attention-Aided Convolutional Neural Network

    [https://arxiv.org/abs/2403.18302](https://arxiv.org/abs/2403.18302)

    提出了一种SolarCNN方法，利用辅助注意力卷积神经网络，提升太阳活动区磁图的超分辨率，以增强对极端空间天气事件的预测。

    

    图像超分辨率一直是图像处理和识别中的重要课题。本文提出了一种辅助注意力卷积神经网络（CNN），用于太阳图像的超分辨率。我们的方法命名为SolarCNN，旨在增强由Michelson Doppler Imager（MDI）在太阳和太阳圈观测卫星（SOHO）上收集的太阳活动区（AR）的顺视磁图的质量。用于训练SolarCNN的地面实况标签是由太阳动力学观测卫星（SDO）上搭载的Helioseismic和Magnetic Imager（HMI）收集的顺视磁图。太阳AR由强磁场组成，在其中磁能量可以突然释放，从而产生极端空间天气事件，如太阳耀斑、日冕物质抛射和太阳高能粒子。SOHO/MDI覆盖太阳第23个周期，这个周期比第24个周期更强烈，包含更多喷发事件。增强的SOHO/MDI磁图可以更好地

    arXiv:2403.18302v1 Announce Type: cross  Abstract: Image super-resolution has been an important subject in image processing and recognition. Here, we present an attention-aided convolutional neural network (CNN) for solar image super-resolution. Our method, named SolarCNN, aims to enhance the quality of line-of-sight (LOS) magnetograms of solar active regions (ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and Heliospheric Observatory (SOHO). The ground-truth labels used for training SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist of strong magnetic fields in which magnetic energy can suddenly be released to produce extreme space weather events, such as solar flares, coronal mass ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI magnetograms allow for bette
    
[^85]: 选择性混合微调以优化非可分解目标

    Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives

    [https://arxiv.org/abs/2403.18301](https://arxiv.org/abs/2403.18301)

    提出了SelMix，一种选择性混合的廉价微调技术，用于优化预训练模型以实现所需的非可分解目标。

    

    互联网使用的增加导致了大量数据的生成，从而采用了各种监督和半监督机器学习算法，这些算法可以有效利用大量数据来训练模型。然而，在将这些模型部署到现实世界之前，必须严格评估它们在诸如最坏情况召回率之类的性能指标上的表现，并满足公平性等约束条件。我们发现，当前最先进的经验技术在这些实际的、非可分解的性能目标上提供了次优性能。另一方面，理论技术需要为每个性能目标从头开始训练一个新模型。为了弥合这一差距，我们提出了SelMix，这是一种基于选择性混合的廉价微调技术，用于针对所需目标进行优化。

    arXiv:2403.18301v1 Announce Type: cross  Abstract: The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a
    
[^86]: GeNet:一种基于图神经网络的抗噪声任务导向语义通信范式

    GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm

    [https://arxiv.org/abs/2403.18296](https://arxiv.org/abs/2403.18296)

    GeNet提出了一种基于图神经网络的语义通信范式，通过将数据转换为图结构、利用编码器提取语义信息并利用解码器重建信息的方法来实现抗噪声任务导向通信。

    

    传统的语义通信任务方法依赖于了解信噪比（SNR）来减轻通道噪声。然而，这些方法需要在特定的SNR条件下进行训练，需要大量时间和计算资源。在本文中，我们提出了GeNet，这是一种基于图神经网络（GNN）的语义通信范式，旨在抵抗噪声，从而促进任务导向通信（TOC）。我们提出了一种新颖的方法，首先将输入数据图像转换为图结构。然后利用基于GNN的编码器从源数据中提取语义信息。这些提取的语义信息然后通过通道传输。在接收端，使用基于GNN的解码器从源数据中重建相关的语义信息以用于TOC。通过实验评估，我们展示了GeNet在抗噪声TOC中的有效性。

    arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
    
[^87]: 少样本重校准语言模型

    Few-Shot Recalibration of Language Models

    [https://arxiv.org/abs/2403.18286](https://arxiv.org/abs/2403.18286)

    提出了一种新框架，用于少样本特定切片重校准语言模型，实现在任意分布片段上获得校准的置信度估计。

    

    最近的研究发现了一些有希望的方法，可以从语言模型（LMs）中提取出校准良好的置信度估计，其中模型的置信度分数反映了其正确性可能性。然而，虽然LMs在广泛分布上可能具有良好的校准性，但这往往隐藏在更窄分片内存在显著的校准不准确性（例如，在数学中存在系统性过度自信可能会平衡历史中的系统性不足自信，从而在总体上实现完美校准）。为了获得任何分布片段的校准良好的置信度估计，我们提出了一种用于少样本特定切片重校准的新框架。具体来说，我们训练一个重新校准模型，该模型接受来自任何给定切片的少量无标签示例，并预测一条重新映射置信度分数以使其对该切片更准确的曲线。我们训练的模型可以重新校准任意新的切片，而无需使用该切片的任何标记数据。这使我们能够识别d

    arXiv:2403.18286v1 Announce Type: cross  Abstract: Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify d
    
[^88]: 通过融合混合复杂度进行聚类变化符号检测

    Clustering Change Sign Detection by Fusing Mixture Complexity

    [https://arxiv.org/abs/2403.18269](https://arxiv.org/abs/2403.18269)

    通过融合多个模型，提出了一种用于在渐变变化期间准确捕获集群结构并检测集群结构变化的方法。

    

    本文提出了一种用于检测集群结构变化的早期方法。集群结构是指使用有限混合模型表示数据时的离散结构特征，例如高斯混合模型中集群的数量。我们关注的是集群结构逐渐随时间变化的情况。对于有限混合模型，混合复杂度（MC）的概念通过考虑集群比例偏差和集群之间的重叠来度量连续的集群大小。在本文中，我们提出了MC融合作为MC的扩展，以处理有限混合模型中可能存在多个混合数字的情况。通过合并多个模型的融合，我们的方法在渐变变化的过渡期间准确捕获了集群结构。此外，我们介绍了一种通过检查MC融合的过渡来检测集群结构变化的方法。

    arXiv:2403.18269v1 Announce Type: cross  Abstract: This paper proposes an early detection method for cluster structural changes. Cluster structure refers to discrete structural characteristics, such as the number of clusters, when data are represented using finite mixture models, such as Gaussian mixture models. We focused on scenarios in which the cluster structure gradually changed over time. For finite mixture models, the concept of mixture complexity (MC) measures the continuous cluster size by considering the cluster proportion bias and overlap between clusters. In this paper, we propose MC fusion as an extension of MC to handle situations in which multiple mixture numbers are possible in a finite mixture model. By incorporating the fusion of multiple models, our approach accurately captured the cluster structure during transitional periods of gradual change. Moreover, we introduce a method for detecting changes in the cluster structure by examining the transition of MC fusion. We
    
[^89]: DSF-GAN: 下游反馈生成对抗网络

    DSF-GAN: DownStream Feedback Generative Adversarial Network

    [https://arxiv.org/abs/2403.18267](https://arxiv.org/abs/2403.18267)

    DSF-GAN提出了一种新架构，通过在训练中结合下游预测模型的反馈信息，增强了生成器的损失函数，从而提高了合成样本的实用性。

    

    实用性和隐私性是衡量合成表格数据质量的两个关键指标。尽管在隐私措施方面已经取得了显著进展，但生成具有高实用性的合成样本仍然具有挑战性。为了提高合成样本的实用性，我们提出了一种名为DownStream Feedback生成对抗网络（DSF-GAN）的新架构。该方法在训练过程中结合了下游预测模型的反馈，用有价值的信息增强生成器的损失函数。因此，DSF-GAN利用下游预测任务来增强合成样本的实用性。为了评估我们的方法，我们使用了两个流行的数据集进行测试。我们的实验表明，在使用DSF-GAN生成的合成样本进行训练时，模型性能得到了改善，相比于没有反馈的相同GAN架构生成的样本。评估是在同一个验证集上进行的，其中包含了re

    arXiv:2403.18267v1 Announce Type: cross  Abstract: Utility and privacy are two crucial measurements of the quality of synthetic tabular data. While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging. To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This approach incorporates feedback from a downstream prediction model during training to augment the generator's loss function with valuable information. Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples. To evaluate our method, we tested it using two popular datasets. Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same GAN architecture without feedback. The evaluation was conducted on the same validation set comprising re
    
[^90]: 分支调整：在持续自监督学习中平衡稳定性和可塑性

    Branch-Tuning: Balancing Stability and Plasticity for Continual Self-Supervised Learning

    [https://arxiv.org/abs/2403.18266](https://arxiv.org/abs/2403.18266)

    本文提出了一种名为分支调整的方法，在持续自监督学习中实现了稳定性和可塑性的平衡，通过分支扩展和压缩来实现。

    

    自监督学习（SSL）已经成为从大量未标记数据中得出通用表示的有效范式。然而，随着现实应用不断整合新内容，SSL的高计算和资源需求需要持续学习而不是完全重新训练。本文利用中心核对齐来定量分析模型的稳定性和可塑性，揭示了批归一化层对稳定性和卷积层对可塑性的关键作用。在此基础上，我们提出了一种高效简单的方法，即分支调整，实现了在持续SSL中稳定性和可塑性之间的平衡。分支调整由分支扩展和压缩组成，并且可以轻松应用于各种SSL方法而不需要

    arXiv:2403.18266v1 Announce Type: new  Abstract: Self-supervised learning (SSL) has emerged as an effective paradigm for deriving general representations from vast amounts of unlabeled data. However, as real-world applications continually integrate new content, the high computational and resource demands of SSL necessitate continual learning rather than complete retraining. This poses a challenge in striking a balance between stability and plasticity when adapting to new information. In this paper, we employ Centered Kernel Alignment for quantitatively analyzing model stability and plasticity, revealing the critical roles of batch normalization layers for stability and convolutional layers for plasticity. Motivated by this, we propose Branch-tuning, an efficient and straightforward method that achieves a balance between stability and plasticity in continual SSL. Branch-tuning consists of branch expansion and compression, and can be easily applied to various SSL methods without the need
    
[^91]: 超越嵌入：多模态模型中视觉表格的价值

    Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models

    [https://arxiv.org/abs/2403.18252](https://arxiv.org/abs/2403.18252)

    提出了Visual Table，一种为MLLMs量身定制的新型视觉表示，通过提供层次化文本描述的全面视觉场景来弥补现有视觉表示的不足。

    

    视觉表示学习一直是计算机视觉中的基石，从具有人类注释标签的监督学习发展到对齐来自互联网的图像-文本对。尽管多模态大语言模型（MLLMs）方面取得了近期的进展，但它们依赖的视觉表示（如CLIP嵌入）通常缺乏关键的外部世界知识，这对于现实世界的视觉推理至关重要。在这项工作中，我们提出了Visual Table，这是为MLLMs量身定制的新型视觉表示。它提供全面视觉场景的层次化文本描述，包括场景描述和涵盖类别、属性和实例级别知识的多个以对象为中心的描述。我们进一步开发了一个可扩展的生成器，用于从GPT4V的小规模注释中生成视觉表格，并训练它。广泛的评估表明，通过将生成的视觉表格作为额外视觉表示，我们

    arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
    
[^92]: NeuSDFusion: 一种空间感知的生成模型，用于3D形状的完成、重建和生成

    NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation

    [https://arxiv.org/abs/2403.18241](https://arxiv.org/abs/2403.18241)

    提出一种新颖的空间感知3D形状生成框架，利用2D平面表示增强建模，并结合混合形状表示技术直接学习连续有向距离场表示，从而确保空间一致性和降低内存使用。

    

    3D形状生成旨在生成符合特定条件和约束的创新性3D内容。现有方法通常将3D形状分解为一系列局部组件，将每个元素孤立处理而不考虑空间一致性。因此，这些方法在3D数据表示和形状生成方面表现出有限的多样性，阻碍了它们生成高度多样化且符合指定约束的3D形状的能力。为此，我们引入了一种新颖的空间感知3D形状生成框架，利用2D平面表示来增强3D形状建模。为确保空间一致性并减少内存使用，我们结合了一种混合形状表示技术，直接使用正交的2D平面学习3D形状的连续有向距离场表示。此外，我们通过传

    arXiv:2403.18241v1 Announce Type: cross  Abstract: 3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a tra
    
[^93]: 用于前列腺癌超声数据检测的图像变压器基准测试

    Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data

    [https://arxiv.org/abs/2403.18233](https://arxiv.org/abs/2403.18233)

    该研究比较了图像变压器和卷积神经网络在超声前列腺癌分类中的表现，并设计了一种多目标学习策略。

    

    目的：通常使用卷积网络（CNN）在穿刺痕迹区域的小感兴趣区域（ROI）中检测前列腺癌（PCa）的深度学习方法，然而，这种方法由于地面实况组织病理学标签未描述个别ROI的特性而存在标签弱化问题。最近，多尺度方法试图通过将变压器的上下文感知与CNN特征提取器相结合，利用多实例学习（MIL）从多个ROI中检测癌症以缓解这一问题。在这项工作中，我们对几种图像变压器架构进行了详细研究，用于ROI尺度和多尺度分类，并比较了基于超声的前列腺癌分类中CNN和变压器的性能。我们还设计了一种将ROI和核心预测结合起来的新型多目标学习策略以进一步

    arXiv:2403.18233v1 Announce Type: cross  Abstract: PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in ultrasound images typically employ convolutional networks (CNNs) to detect cancer in small regions of interest (ROI) along a needle trace region. However, this approach suffers from weak labelling, since the ground-truth histopathology labels do not describe the properties of individual ROIs. Recently, multi-scale approaches have sought to mitigate this issue by combining the context awareness of transformers with a CNN feature extractor to detect cancer from multiple ROIs using multiple-instance learning (MIL). In this work, we present a detailed study of several image transformer architectures for both ROI-scale and multi-scale classification, and a comparison of the performance of CNNs and transformers for ultrasound-based prostate cancer classification. We also design a novel multi-objective learning strategy that combines both ROI and core predictions to furth
    
[^94]: 傅立叶或小波基作为Spikformer中高效视觉分类的自注意力对应物

    Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification

    [https://arxiv.org/abs/2403.18228](https://arxiv.org/abs/2403.18228)

    本文提出了在Spikformer中使用傅立叶或小波基代替传统的自注意力机制，提出的FWformer在视觉分类任务中表现出可比甚至更高的准确率。

    

    能效高的Spikformer通过将生物合理的脉冲神经网络（SNN）和人工Transformer集成提出，其中使用脉冲自注意力（SSA）既可提高准确率又可降低计算成本。然而，在稀疏脉冲形式计算方式中，自注意力并非总是必要的。本文创新性地将传统SSA（使用来自查询和键的动态基函数计算）替换为脉冲形式的傅立叶变换、小波变换以及它们的组合（使用固定的三角形或小波基函数），基于一个关键假设，即它们都使用一组基函数进行信息变换。因此，提出了基于傅立叶或小波的Spikformer（FWformer），并在视觉分类任务中进行了验证，包括静态图像和基于事件的视频数据集。FWformer在准确率方面能够达到可比甚至更高的水平（0.4%-$

    arXiv:2403.18228v1 Announce Type: cross  Abstract: Energy-efficient spikformer has been proposed by integrating the biologically plausible spiking neural network (SNN) and artificial Transformer, whereby the Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower computational cost. However, it seems that self-attention is not always necessary, especially in sparse spike-form calculation manners. In this paper, we innovatively replace vanilla SSA (using dynamic bases calculating from Query and Key) with spike-form Fourier Transform, Wavelet Transform, and their combinations (using fixed triangular or wavelets bases), based on a key hypothesis that both of them use a set of basis functions for information transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is proposed and verified in visual classification tasks, including both static image and event-based video datasets. The FWformer can achieve comparable or even higher accuracies ($0.4\%$-$
    
[^95]: 基于Transformer的有效载荷恶意软件检测和分类框架

    A Transformer-Based Framework for Payload Malware Detection and Classification

    [https://arxiv.org/abs/2403.18223](https://arxiv.org/abs/2403.18223)

    本文提出了一种基于Transformer的DPI算法，旨在检测恶意流量，通过学习复杂的序列数据内容并推广到类似场景中。

    

    随着恶意网络威胁在入侵计算机网络方面变得更加复杂，有效的入侵检测系统（IDSs）的需求变得至关重要。传统上，IDSs依赖于异常检测和基于特征库的检测技术来检测未识别和可疑活动。深度学习技术在DPI方面展现出了巨大潜力，因为它们能够从通过网络传输的数据包内容中学习复杂的模式。在本文中，我们提出了一种革命性的基于Transformer的DPI算法，旨在通过带有分类器头的转变器检测恶意流量。

    arXiv:2403.18223v1 Announce Type: cross  Abstract: As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head. Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attent
    
[^96]: 预训练语言条件模仿学习策略的不确定性感知部署

    Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies

    [https://arxiv.org/abs/2403.18222](https://arxiv.org/abs/2403.18222)

    提出一种不确定性感知的预训练语言条件模仿学习代理的部署方法，通过温度缩放和本地信息聚合做出不确定性感知决策，显著提升任务完成率。

    

    大规模机器人策略在来自不同任务和机器人平台的数据上训练，为实现通用机器人带来很大希望；然而，可靠地泛化到新的环境条件仍然是一个主要挑战。为解决这一挑战，我们提出了一种新颖的方法，用于不确定性感知的预训练语言条件模仿学习代理的部署。具体来说，我们使用温度缩放来校准这些模型，并利用校准的模型通过聚合候选动作的本地信息来做出不确定性感知决策。我们在仿真环境中使用三个这样的预训练模型来实现我们的方法，并展示其潜力显著提升任务完成率。附带的代码可以在以下链接找到：https://github.com/BobWu1998/uncertainty_quant_all.git

    arXiv:2403.18222v1 Announce Type: cross  Abstract: Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git
    
[^97]: 从二维到三维环境的Q学习：使用强化学习建模自主导航而无需库

    From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries

    [https://arxiv.org/abs/2403.18219](https://arxiv.org/abs/2403.18219)

    本研究通过强化学习算法在二维和三维环境中的表现，探讨了在没有预制库的情况下通过计算数学开发算法的可行性。

    

    强化学习（RL）算法已成为人工智能中不可或缺的工具，使代理通过与环境和反馈机制的交互获得最优决策策略。本研究探讨了RL代理在二维（2D）和三维（3D）环境中的表现，旨在研究跨不同空间维度学习的动态。这项研究的一个关键方面是没有用于学习的预制库，算法完全通过计算数学开发。方法论框架集中在RL原则上，采用Q学习代理类和针对每个空间维度量身定制的不同环境类。研究旨在探讨一个问题：强化学习代理在不同空间维度的环境中如何适应和表现，特别是在2D和3D设置中？通过实证研究

    arXiv:2403.18219v1 Announce Type: cross  Abstract: Reinforcement learning (RL) algorithms have become indispensable tools in artificial intelligence, empowering agents to acquire optimal decision-making policies through interactions with their environment and feedback mechanisms. This study explores the performance of RL agents in both two-dimensional (2D) and three-dimensional (3D) environments, aiming to research the dynamics of learning across different spatial dimensions. A key aspect of this investigation is the absence of pre-made libraries for learning, with the algorithm developed exclusively through computational mathematics. The methodological framework centers on RL principles, employing a Q-learning agent class and distinct environment classes tailored to each spatial dimension. The research aims to address the question: How do reinforcement learning agents adapt and perform in environments of varying spatial dimensions, particularly in 2D and 3D settings? Through empirical
    
[^98]: 具有有界人口差异的极小极小公平分类

    Minimax Optimal Fair Classification with Bounded Demographic Disparity

    [https://arxiv.org/abs/2403.18216](https://arxiv.org/abs/2403.18216)

    本文研究了在公平二元分类中控制人口差异的统计基础，提出了极小极小最优分类错误限制人口差异到用户指定阈值的方法。

    

    缓解统计机器学习方法的不公平影响对确保公平至关重要。尽管大量研究旨在减少差异，但使用\emph{有限数据集}的效果 -- 而不是整个人口 -- 仍不清楚。本文探讨了具有两个受保护群体的公平二元分类的统计基础，重点是控制人口差异，即群体之间的接受率差异。尽管即使有无限数据，公平可能会以准确性为代价，但我们表明使用有限样本会由于需要估计特定于群体的接受阈值而产生额外成本。我们研究了再将人口差异限制到用户指定阈值时的极小极小最优分类错误。为了量化公平约束的影响，我们引入了一种称为\emph{公平感知超额风险}的新颖度量，并推导了一个极小极小下界。

    arXiv:2403.18216v1 Announce Type: cross  Abstract: Mitigating the disparate impact of statistical machine learning methods is crucial for ensuring fairness. While extensive research aims to reduce disparity, the effect of using a \emph{finite dataset} -- as opposed to the entire population -- remains unclear. This paper explores the statistical foundations of fair binary classification with two protected groups, focusing on controlling demographic disparity, defined as the difference in acceptance rates between the groups. Although fairness may come at the cost of accuracy even with infinite data, we show that using a finite sample incurs additional costs due to the need to estimate group-specific acceptance thresholds. We study the minimax optimal classification error while constraining demographic disparity to a user-specified threshold. To quantify the impact of fairness constraints, we introduce a novel measure called \emph{fairness-aware excess risk} and derive a minimax lower bou
    
[^99]: NeuroPictor: 通过多个个体的预训练和多层调制优化fMRI到图像的重建

    NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation

    [https://arxiv.org/abs/2403.18211](https://arxiv.org/abs/2403.18211)

    NeuroPictor通过直接调制扩散模型的生成过程，实现了fMRI到图像的重建，在多个个体预训练和多层次的引导条件下，实现了更详细的图像控制。

    

    最近的fMRI到图像方法主要集中在将fMRI信号与预先训练的扩散模型的特定条件关联起来。相比之下，本文提出直接调制扩散模型的生成过程，将fMRI到图像过程分为三个步骤：i) fMRI校准编码，用于处理共享潜在空间的多个体预训练，以最小化个体差异并实现后续的跨主体训练；ii) fMRI到图像跨个体预训练，感知地学习如何引导不同个体之间高低层次条件的扩散模型；iii) fMRI到图像单个体细化，类似于步骤ii，但侧重于适应特定个体。

    arXiv:2403.18211v1 Announce Type: cross  Abstract: Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particula
    
[^100]: 长短期约束驱动的安全强化学习用于自动驾驶

    Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving

    [https://arxiv.org/abs/2403.18209](https://arxiv.org/abs/2403.18209)

    本文提出了一种基于长期和短期约束的新算法用于安全强化学习，在自动驾驶任务中可以同时保证车辆的短期和长期安全性。

    

    强化学习（RL）在决策任务中被广泛使用，但由于需要与环境交互，无法保证代理的安全性，这严重限制了其在自动驾驶等工业应用中的应用。本文提出了一种基于长期和短期约束（LSTC）的新算法用于安全RL。短期约束旨在确保车辆探测到的短期状态安全，而长期约束则确保整体安全性。

    arXiv:2403.18209v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the
    
[^101]: 超越表面所见：基于种族健康不平等社会决定因素的多标签胸部X射线分类的子组交叉公平的实证分析

    Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities

    [https://arxiv.org/abs/2403.18196](https://arxiv.org/abs/2403.18196)

    通过考虑社会决定因素中的复杂相互作用，提出了一种简单而强大的方法，实现跨受保护群体的准确诊断结果和公平性，为高维胸部X射线多标签分类带来创新。

    

    在使用胸部X射线进行疾病诊断方面，实施深度学习模型取得了显著进展。尽管取得了这些进展，但这些模型中固有的偏见可能导致跨受保护群体的预测准确性差异。在本研究中，我们提出了一个框架，以实现准确的诊断结果，并确保在高维胸部X射线多标签分类中跨交叉群体的公平性。我们不拘一格地考虑社会决定因素中的复杂相互作用，实现了更精细的公平性基准和评估。我们提出了一种简单而强大的方法，涉及使用跨组平衡数据集重新训练预先训练模型的最后分类层。此外，我们考虑了公平性约束，并为多标签设置整合了类平衡微调。我们在MIMIC-CXR数据集上评估了我们的方法，结果表明

    arXiv:2403.18196v1 Announce Type: cross  Abstract: There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that
    
[^102]: 通过突出困难和不平衡样本进行多标签自适应批次选择

    Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples

    [https://arxiv.org/abs/2403.18192](https://arxiv.org/abs/2403.18192)

    多标签数据中困难和不平衡样本的突出，引入了一种自适应批次选择方法，以解决训练过程中的类别不平衡问题。

    

    arXiv：2403.18192v1 公告类型：新 深度神经网络模型已经证明了它们在分类来自各个领域的多标签数据方面的有效性。通常，它们采用组合小批次和优化器的训练模式，在构建小批次时每个样本被随机选择，概率相等。然而，在多标签数据中固有的类别不平衡可能会使模型倾向于主导标签，因为与少数标签相关的样本可能在每个小批次中代表不足。同时，在训练过程中，我们观察到与少数标签相关的实例倾向于引起更大的损失。现有的启发式批次选择方法，例如选择对目标函数具有很高贡献度的样本，即损失很高的样本，已经被证明可以加快收敛速度，同时减少单标签数据中的损失和测试误差。然而，批次选择方法尚未被应用和验证。

    arXiv:2403.18192v1 Announce Type: new  Abstract: Deep neural network models have demonstrated their effectiveness in classifying multi-label data from various domains. Typically, they employ a training mode that combines mini-batches with optimizers, where each sample is randomly selected with equal probability when constructing mini-batches. However, the intrinsic class imbalance in multi-label data may bias the model towards majority labels, since samples relevant to minority labels may be underrepresented in each mini-batch. Meanwhile, during the training process, we observe that instances associated with minority labels tend to induce greater losses. Existing heuristic batch selection methods, such as priority selection of samples with high contribution to the objective function, i.e., samples with high loss, have been proven to accelerate convergence while reducing the loss and test error in single-label data. However, batch selection methods have not yet been applied and validate
    
[^103]: 通过分层聚类压缩非线性物理模型的Koopman矩阵

    Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering

    [https://arxiv.org/abs/2403.18181](https://arxiv.org/abs/2403.18181)

    该论文提出了一种通过分层聚类压缩非线性物理模型的Koopman矩阵的方法，相比传统SVD压缩，分层聚类表现更好。

    

    机器学习方法允许仅从数据预测非线性动力系统。 Koopman算子是其中之一，它使我们能够对非线性动力系统使用线性分析。 Koopman算子的线性特征有望理解非线性动态并进行快速预测。 扩展的动态模态分解（EDMD）是一种近似Koopman算子的有限维矩阵的方法。 在这项工作中，我们提出了一种使用分层聚类压缩Koopman矩阵的方法。 通过小车摆模型的数值演示和与传统奇异值分解（SVD）的比较，结果表明分层聚类比朴素SVD压缩效果更好。

    arXiv:2403.18181v1 Announce Type: new  Abstract: Machine learning methods allow the prediction of nonlinear dynamical systems from data alone. The Koopman operator is one of them, which enables us to employ linear analysis for nonlinear dynamical systems. The linear characteristics of the Koopman operator are hopeful to understand the nonlinear dynamics and perform rapid predictions. The extended dynamic mode decomposition (EDMD) is one of the methods to approximate the Koopman operator as a finite-dimensional matrix. In this work, we propose a method to compress the Koopman matrix using hierarchical clustering. Numerical demonstrations for the cart-pole model and comparisons with the conventional singular value decomposition (SVD) are shown; the results indicate that the hierarchical clustering performs better than the naive SVD compressions.
    
[^104]: 在线战略分类中的错误、操纵和边际保证

    Mistake, Manipulation and Margin Guarantees in Online Strategic Classification

    [https://arxiv.org/abs/2403.18176](https://arxiv.org/abs/2403.18176)

    新算法旨在在存在战略代理行为的情况下恢复最大边际分类器，并提供了收敛性、有限错误和有限操纵保证。

    

    我们考虑了一个在线战略分类问题，其中每个到达的代理可以操纵他们真实的特征向量以获取一个正预测标签，同时承担取决于操纵量的成本。学习者试图在只有操纵特征的情况下预测代理的真实标签。在学习者发布他们的预测后，代理的真实标签会被揭示。先前的算法如战略感知器在关于代理真实特征向量的边际假设下保证了有限的错误次数。然而，这些算法并不能保证鼓励代理诚实。促进诚实与获得充足边际的预测密切相关，因此我们提供了两个旨在在存在战略代理行为的情况下恢复最大边际分类器的新算法。我们证明了多种代理成本结构下的收敛性、有限错误和有限操纵保证。

    arXiv:2403.18176v1 Announce Type: new  Abstract: We consider an online strategic classification problem where each arriving agent can manipulate their true feature vector to obtain a positive predicted label, while incurring a cost that depends on the amount of manipulation. The learner seeks to predict the agent's true label given access to only the manipulated features. After the learner releases their prediction, the agent's true label is revealed. Previous algorithms such as the strategic perceptron guarantee finitely many mistakes under a margin assumption on agents' true feature vectors. However, these are not guaranteed to encourage agents to be truthful. Promoting truthfulness is intimately linked to obtaining adequate margin on the predictions, thus we provide two new algorithms aimed at recovering the maximum margin classifier in the presence of strategic agent behavior. We prove convergence, finite mistake and finite manipulation guarantees for a variety of agent cost struct
    
[^105]: 噢！我们冷冻：通过信号传播分析改进大型语言模型的量化知识蒸馏

    Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models

    [https://arxiv.org/abs/2403.18159](https://arxiv.org/abs/2403.18159)

    通过信号传播分析，提出了一种改进大型语言模型的量化知识蒸馏方法，并提供了ov-freeze稳定KD-QAT过程的洞察。

    

    大型生成模型，如大型语言模型（LLMs）和扩散模型分别在NLP和计算机视觉领域引起了革命。然而，它们的推理速度慢，计算和内存需求高，这使得在边缘设备上部署它们变得具有挑战性。在这项研究中，我们提出了一种轻量级的量化感知微调技术，使用知识蒸馏（KD-QAT）来改善使用常用数据集改进4位重量量化的LLMs的性能，以实现流行的语言使用案例，在设备聊天应用中。为了改进这种微调范式，作为主要贡献，我们通过经验研究训练过程中的梯度传播，提供对KD-QAT稳定性的洞察，以更好地理解基于KD-QAT的方法对低位量化误差的脆弱性。根据我们的见解，我们提出了ov-freeze，一种稳定KD-QAT过程的简单技术。最后，我们进行了实验

    arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
    
[^106]: 划分、征服、结合贝叶斯决策树抽样

    Divide, Conquer, Combine Bayesian Decision Tree Sampling

    [https://arxiv.org/abs/2403.18147](https://arxiv.org/abs/2403.18147)

    该论文旨在通过贝叶斯推断方法量化决策树预测的不确定性，并提出了一种改变树结构和决策参数的新方法。

    

    决策树是常用的预测模型，因其灵活性和解释性。本文旨在通过采用贝叶斯推断方法量化决策树预测的不确定性。这是具有挑战性的，因为这些方法需要探索树结构空间和与每个树结构相关联的决策参数空间。通过使用马尔可夫链蒙特卡洛（MCMC）方法来处理这一问题，其中构建马尔可夫链以提供所需的贝叶斯估计样本。重要的是，结构和决策参数紧密耦合；树结构的微小变化可能需要大不同的决策参数以提供准确的预测。现有MCMC方法面临的挑战是提出同时改变树结构和决策参数的建议，以实现高效抽样。本文采取了不同的方法，

    arXiv:2403.18147v1 Announce Type: new  Abstract: Decision trees are commonly used predictive models due to their flexibility and interpretability. This paper is directed at quantifying the uncertainty of decision tree predictions by employing a Bayesian inference approach. This is challenging because these approaches need to explore both the tree structure space and the space of decision parameters associated with each tree structure. This has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a Markov Chain is constructed to provide samples from the desired Bayesian estimate. Importantly, the structure and the decision parameters are tightly coupled; small changes in the tree structure can demand vastly different decision parameters to provide accurate predictions. A challenge for existing MCMC approaches is proposing joint changes in both the tree structure and the decision parameters that result in efficient sampling. This paper takes a different approach, where ea
    
[^107]: HERTA：一种高效且严格的展开图神经网络训练算法

    HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks

    [https://arxiv.org/abs/2403.18142](https://arxiv.org/abs/2403.18142)

    HERTA是针对展开图神经网络提出的一种高效且严格的训练算法，加速整个训练过程并保留模型的可解释性，同时实现了近线性时间最坏情况训练保证。

    

    作为图神经网络（GNNs）的一种变体，展开GNNs在传统设计上提供了更强的可解释性和灵活性。然而，当涉及到训练成本时，它们仍然面临着可扩展性挑战。尽管已经提出了许多方法来解决这些可扩展性问题，但它们大多集中在每次迭代的效率上，没有最坏情况收敛保证。此外，这些方法通常会向原模型添加组件或进行修改，可能会破坏展开GNNs的可解释性。在本文中，我们提出了HERTA：一种用于加速整个训练过程的高效且严格的展开GNNs训练算法，实现了近线性时间最坏情况训练保证。至关重要的是，HERTA收敛到原始模型的最优解，从而保持了展开GNNs的可解释性。此外，作为HERTA的副产品，我们提出了一种新的谱稀疏化方法。

    arXiv:2403.18142v1 Announce Type: new  Abstract: As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs. In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we propose a new spectral sparsification met
    
[^108]: 保护GNN：基于解释的后门训练图识别

    Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs

    [https://arxiv.org/abs/2403.18136](https://arxiv.org/abs/2403.18136)

    提出了一种基于解释的方法来识别GNN中的后门训练图，设计了七种新的度量指标以更有效地检测后门攻击，并且通过自适应攻击进行了方法评估。

    

    Graph Neural Networks (GNNs)已经在许多领域流行起来，但它们容易受到后门攻击，这可能会损害它们的性能和道德应用。检测这些攻击对于保持GNN分类任务的可靠性和安全性至关重要，但有效的检测技术并不多见。我们观察到，尽管图级解释能够提供一些有限的见解，但它们在检测后门触发器方面的有效性是不一致且不完整的。为弥补这一差距，我们提取并转换GNN解释机制的次要输出，设计了七种更有效地检测后门攻击的新度量。此外，我们还开发了一种自适应攻击来严格评估我们的方法。我们在多个基准数据集上测试了我们的方法，并检查其对各种攻击模型的有效性。我们的结果表明，我们的方法可以取得较高的效果。

    arXiv:2403.18136v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high de
    
[^109]: AE SemRL：使用自动编码器学习语义关联规则

    AE SemRL: Learning Semantic Association Rules with Autoencoders

    [https://arxiv.org/abs/2403.18133](https://arxiv.org/abs/2403.18133)

    使用自动编码器学习时间序列数据中的语义关联关系，实验表明这种方法速度快上百倍

    

    关联规则挖掘（ARM）是学习数据特征之间逻辑规则关联的任务。从高维数值数据中挖掘关联规则，例如从智能环境中大量传感器的时间序列数据，是一项计算密集型的任务。在本研究中，我们提出了一种基于自动编码器的方法，用于从时间序列数据中学习和提取关联规则（AE SemRL）。此外，我们认为，在与时间序列数据源相关的语义信息存在时，语义可以促进学习可推广和可解释的关联规则。尽管通过额外的语义特征丰富时间序列数据，AE SemRL使得从高维数据中学习关联规则变得可行。我们的实验表明，可以从自动编码器生成的潜在表示中提取语义关联规则，并且此方法速度快上百倍。

    arXiv:2403.18133v1 Announce Type: cross  Abstract: Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules. Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task. In this study, we propose an Autoencoder-based approach to learn and extract association rules from time series data (AE SemRL). Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules. Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible. Our experiments show that semantic association rules can be extracted from a latent representation created by an Autoencoder and this method has in the order of hundreds of times faster
    
[^110]: 通过模拟未来数据推荐无数据的类递增学习算法

    Recommendation of data-free class-incremental learning algorithms by simulating future data

    [https://arxiv.org/abs/2403.18132](https://arxiv.org/abs/2403.18132)

    通过模拟未来数据流，推荐无数据的类递增学习算法，实验结果表明方法胜过竞争基线

    

    类递增学习处理由类别批次组成的顺序数据流。已经提出了各种算法来解决从无法存储过去类别的样本的具有挑战性的情况。然而，为用户定义的设置选择适当的算法是一个未解之谜，因为这些算法的相对性能取决于递增设置。为了解决这个问题，我们引入了一种模拟未来数据流的算法推荐方法。给定一个初始类别集合，它利用生成模型从相同的视觉域模拟未来类别。我们在模拟流上评估了最近的算法，并推荐在用户定义的递增设置中表现最佳的算法。我们使用六种算法和六个递增设置，在三个大型数据集上展示了我们方法的有效性。我们的方法胜过了竞争基线，并且性能接近

    arXiv:2403.18132v1 Announce Type: cross  Abstract: Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close 
    
[^111]: 使用图注意力网络对电子健康记录进行节点分类的HealthGAT

    HealthGAT: Node Classifications in Electronic Health Records using Graph Attention Networks

    [https://arxiv.org/abs/2403.18128](https://arxiv.org/abs/2403.18128)

    HealthGAT是一个新颖的图注意力网络框架，通过分层方法生成EHR嵌入，并引入定制的以EHR为中心的辅助预训练任务，从而在医学代码嵌入和复杂医学关系分析方面取得重大进展。

    

    虽然电子健康记录（EHRs）在医疗保健的各种应用中被广泛使用，但大多数应用程序使用原始（表格）格式的EHRs。依赖原始或简单的数据预处理可能会大大限制利用EHR进行下游任务的性能或适用性。为解决这一挑战，我们提出了HealthGAT，这是一个新颖的图注意力网络框架，采用分层方法从EHR生成嵌入，超越了传统的基于图的方法。我们的模型通过迭代改进医疗代码的嵌入，从而改进了EHR数据分析。我们还引入了定制的以EHR为中心的辅助预训练任务，以利用数据中嵌入的丰富医学知识。这种方法提供了对复杂医学关系的全面分析，并且在标准数据表示技术上取得了显著进展。HealthGAT已经证明了其在va方面的有效性

    arXiv:2403.18128v1 Announce Type: new  Abstract: While electronic health records (EHRs) are widely used across various applications in healthcare, most applications use the EHRs in their raw (tabular) format. Relying on raw or simple data pre-processing can greatly limit the performance or even applicability of downstream tasks using EHRs. To address this challenge, we present HealthGAT, a novel graph attention network framework that utilizes a hierarchical approach to generate embeddings from EHR, surpassing traditional graph-based methods. Our model iteratively refines the embeddings for medical codes, resulting in improved EHR data analysis. We also introduce customized EHR-centric auxiliary pre-training tasks to leverage the rich medical knowledge embedded within the data. This approach provides a comprehensive analysis of complex medical relationships and offers significant advancement over standard data representation techniques. HealthGAT has demonstrated its effectiveness in va
    
[^112]: 修正伪对数似然方法

    A Correction of Pseudo Log-Likelihood Method

    [https://arxiv.org/abs/2403.18127](https://arxiv.org/abs/2403.18127)

    本文纠正了伪对数似然方法的最大似然估计失败问题，并提出了一种解决方案。

    

    伪对数似然是一种用于各个领域的最大似然估计（MLE）方法，包括上下文乐队、社交网络的影响最大化和因果乐队。然而，在先前的文献中，对数似然函数可能没有界限，这可能导致他们提出的算法没有定义。本文给出了一个最大伪对数似然估计失败的反例，然后提供了一个解决方案来纠正\citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}中的算法。

    arXiv:2403.18127v1 Announce Type: new  Abstract: Pseudo log-likelihood is a type of maximum likelihood estimation (MLE) method used in various fields including contextual bandits, influence maximization of social networks, and causal bandits. However, in previous literature \citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}, the log-likelihood function may not be bounded, which may result in the algorithm they proposed not well-defined. In this paper, we give a counterexample that the maximum pseudo log-likelihood estimation fails and then provide a solution to correct the algorithms in \citep{li2017provably, zhang2022online, xiong2022combinatorial, feng2023combinatorial1, feng2023combinatorial2}.
    
[^113]: 不要相信：验证--用自动形式化为基础的LLM定量推理

    Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization

    [https://arxiv.org/abs/2403.18120](https://arxiv.org/abs/2403.18120)

    通过将非正式的数学陈述翻译为形式的Isabelle代码并进行自动验证，我们提供了一种机制，可以自动拒绝在内部一致性方面与形式化问题陈述不一致的解决方案。

    

    大型语言模型（LLM），如Google的Minerva和OpenAI的GPT系列，正在越来越能够解决数学定量推理问题。然而，它们在推理步骤和答案中仍然存在没有理由的逻辑和计算错误。本文利用LLMs的训练语料库包含足够多的形式化数学示例（例如在Isabelle中，一个形式定理证明环境），它们可以被提示将非正式的数学陈述翻译即自动形式化为形式的Isabelle代码--该代码可以被自动验证内部一致性。这提供了一个机制，可以自动拒绝那些其形式化版本在其内部或与形式化问题陈述不一致的解决方案。我们在GSM8K、MATH和MultiArith数据集上评估了我们的方法，并证明我们的方法提供了一个一直比

    arXiv:2403.18120v1 Announce Type: new  Abstract: Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than 
    
[^114]: 数学基础和完整头部姿势估计的修正

    Mathematical Foundation and Corrections for Full Range Head Pose Estimation

    [https://arxiv.org/abs/2403.18104](https://arxiv.org/abs/2403.18104)

    该论文研究了头部姿势估计中的坐标系和欧拉角顺序定义问题，验证了之前作品中使用的姿势输出的正确性和绘图例程的有效性。

    

    许多有关头部姿势估计（HPE）的作品提供了用于从面部关键点或直接从头部区域图像中提取欧拉角的算法或基于神经网络的方法。然而，许多作品未提供所使用的坐标系和欧拉或Tait-Bryan角度顺序的清晰定义。本文彻底检查了300W-LP数据集中定义的欧拉角，头部姿势估计如3DDFA-v2、6D-RepNet、WHENet等，以及它们的欧拉角绘制例程的有效性。必要时，我们推断它们的坐标系和偏航、横滚、俯仰的顺序。

    arXiv:2403.18104v1 Announce Type: cross  Abstract: Numerous works concerning head pose estimation (HPE) offer algorithms or proposed neural network-based approaches for extracting Euler angles from either facial key points or directly from images of the head region. However, many works failed to provide clear definitions of the coordinate systems and Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation matrices depend on coordinate systems, and yaw, roll, and pitch angles are sensitive to their application order. Without precise definitions, it becomes challenging to validate the correctness of the output head pose and drawing routines employed in prior works. In this paper, we thoroughly examined the Euler angles defined in the 300W-LP dataset, head pose estimation such as 3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of the Euler angles. When necessary, we infer their coordinate system and sequence of yaw, roll, pitch from pro
    
[^115]: 关于图像和视觉扩散模型的教程

    Tutorial on Diffusion Models for Imaging and Vision

    [https://arxiv.org/abs/2403.18103](https://arxiv.org/abs/2403.18103)

    该教程讨论了图像和视觉领域中扩散模型的基本理念，适合对扩散模型研究或应用感兴趣的本科生和研究生。

    

    近年来生成工具的惊人增长使得文本到图像生成和文本到视频生成等许多令人兴奋的应用成为可能。这些生成工具背后的基本原理是扩散概念，一种特殊的采样机制，克服了以前方法中被认为困难的一些缺点。本教程的目标是讨论扩散模型的基本理念。本教程的目标受众包括对研究扩散模型或将这些模型应用于解决其他问题感兴趣的本科生和研究生。

    arXiv:2403.18103v1 Announce Type: new  Abstract: The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.
    
[^116]: 迈向可解释的聚类：一种基于约束声明的方法

    Towards Explainable Clustering: A Constrained Declarative based Approach

    [https://arxiv.org/abs/2403.18101](https://arxiv.org/abs/2403.18101)

    该论文提出了一种新颖的可解释的约束聚类方法ECS，旨在找到高质量且可解释的聚类，强调在构建聚类时应考虑经典聚类标准和可解释性。

    

    可解释AI领域在所有机器学习领域都备受关注，而在聚类中更为重要，聚类是一项无监督任务，其结果必须由领域专家验证。我们的目标是找到一个在经典聚类标准方面具有高质量且可解释的聚类，我们认为在构建聚类时必须考虑这两个维度。我们认为，聚类的一个良好的全局解释应该考虑每个簇的特征，考虑到它们描述其对象的能力（覆盖率），同时将其与其他簇区分开（区分度）。此外，我们旨在利用知识专家在期望聚类的结构或其解释方面的不同级别的知识。在我们的框架中，一个簇的解释是一组模式，我们提出了一种新颖的可解释的约束聚类方法，称为ECS来支持声明性聚类。

    arXiv:2403.18101v1 Announce Type: new  Abstract: The domain of explainable AI is of interest in all Machine Learning fields, and it is all the more important in clustering, an unsupervised task whose result must be validated by a domain expert. We aim at finding a clustering that has high quality in terms of classic clustering criteria and that is explainable, and we argue that these two dimensions must be considered when building the clustering. We consider that a good global explanation of a clustering should give the characteristics of each cluster taking into account their abilities to describe its objects (coverage) while distinguishing it from the other clusters (discrimination). Furthermore, we aim at leveraging expert knowledge, at different levels, on the structure of the expected clustering or on its explanations. In our framework an explanation of a cluster is a set of patterns, and we propose a novel interpretable constrained clustering method called ECS for declarative clu
    
[^117]: 通过云计算和机器学习驱动智能物联网监控与控制

    Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning

    [https://arxiv.org/abs/2403.18100](https://arxiv.org/abs/2403.18100)

    通过边缘计算和物联网的组合，可以减少延迟，提高效率。

    

    这篇文章探讨了如何通过云计算和机器学习驱动智能物联网的监控和控制。随着物联网和云继续在网络中生成大量和多样化的数据作为传感器设备，收集到的数据被发送到云端进行统计分析、预测和数据分析以实现业务目标。然而，由于云计算模型受限于距离，对于网络连接质量不理想的环境可能在关键操作上存在问题。因此，边缘计算作为分布式计算架构，将处理应用程序、数据和服务的位置从网络的中心节点移动到网络的逻辑边缘节点，以减少对云处理和数据分析的依赖，实现近端数据处理和分析。物联网和边缘计算的结合可以减少延迟，提高效率。

    arXiv:2403.18100v1 Announce Type: new  Abstract: This article explores how to drive intelligent iot monitoring and control through cloud computing and machine learning. As iot and the cloud continue to generate large and diverse amounts of data as sensor devices in the network, the collected data is sent to the cloud for statistical analysis, prediction, and data analysis to achieve business objectives. However, because the cloud computing model is limited by distance, it can be problematic in environments where the quality of the Internet connection is not ideal for critical operations. Therefore, edge computing, as a distributed computing architecture, moves the location of processing applications, data and services from the central node of the network to the logical edge node of the network to reduce the dependence on cloud processing and analysis of data, and achieve near-end data processing and analysis. The combination of iot and edge computing can reduce latency, improve efficie
    
[^118]: 个性化基于视频的手部分类：脊髓损伤患者的应用

    A Personalized Video-Based Hand Taxonomy: Application for Individuals with Spinal Cord Injury

    [https://arxiv.org/abs/2403.18094](https://arxiv.org/abs/2403.18094)

    本研究旨在开发一种个性化的视频基于手部分类系统，通过语义聚类自动识别颈椎脊髓损伤患者家庭中的主要手部握持动作。

    

    手部功能对人类的互动和生活质量至关重要。脊髓损伤（SCI）可能损害手部功能，降低独立性。在家庭和社区环境中对功能进行全面评估需要针对手部功能受损个体的手握分类。由于标准分类中未代表的握持类型、伤害水平间数据分布不均匀和有限数据，开发这样一种分类是具有挑战性的。本研究旨在利用语义聚类自动识别主要的独特手部握持动作，在主体视角视频中。19名颈椎SCI个体家中收集的主体视角视频被用来对具有语义意义的握持动作进行聚类。采用整合姿势和外观数据的深度学习模型来创建个性化的手部分类。定量分析显示，聚类纯度为67.6%+-24.2%，冗余度为18.0%+-21.8%。

    arXiv:2403.18094v1 Announce Type: cross  Abstract: Hand function is critical for our interactions and quality of life. Spinal cord injuries (SCI) can impair hand function, reducing independence. A comprehensive evaluation of function in home and community settings requires a hand grasp taxonomy for individuals with impaired hand function. Developing such a taxonomy is challenging due to unrepresented grasp types in standard taxonomies, uneven data distribution across injury levels, and limited data. This study aims to automatically identify the dominant distinct hand grasps in egocentric video using semantic clustering. Egocentric video recordings collected in the homes of 19 individual with cervical SCI were used to cluster grasping actions with semantic significance. A deep learning model integrating posture and appearance data was employed to create a personalized hand taxonomy. Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with with 18.0% +- 21.8% redundancy. Qua
    
[^119]: 正态形式博弈中的均衡路径

    Paths to Equilibrium in Normal-Form Games

    [https://arxiv.org/abs/2403.18079](https://arxiv.org/abs/2403.18079)

    本文研究在多智体强化学习中的策略序列，探讨满足特定约束的策略路径，即满足路径，对于构建终止于均衡策略的路径具有重要意义。

    

    在多智体强化学习（MARL）中，智体会反复在时间上交互，并随着新数据的到来修订他们的策略，从而产生一系列策略概况。本文研究满足一种由强化学习中政策更新启发的成对约束的策略序列，其中在第 $t$ 期最优应答的智体在下一期 $t+1$ 不会改变其策略。这种约束仅要求优化智体不更改策略，但并不以任何方式限制其他非最优化智体，因此允许探索。具有此属性的序列被称为满足路径，并在许多 MARL 算法中自然出现。关于战略动态的一个基本问题是：对于给定的博弈和初始策略概况，是否总可以构建一个终止于均衡策略的满足路径？这个问题的解决对应着一些重要含义。

    arXiv:2403.18079v1 Announce Type: cross  Abstract: In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implication
    
[^120]: 非线性模型的目标导向贝叶斯最优实验设计与马尔可夫链蒙特卡洛方法

    Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo

    [https://arxiv.org/abs/2403.18072](https://arxiv.org/abs/2403.18072)

    提出了一种适用于非线性模型的预测目标导向最优实验设计方法，通过最大化QoIs的期望信息增益来确定实验设计。

    

    最优实验设计（OED）提供了一种系统化的方法来量化和最大化实验数据的价值。在贝叶斯方法下，传统的OED会最大化对模型参数的期望信息增益（EIG）。然而，我们通常感兴趣的不是参数本身，而是依赖于参数的非线性方式的预测感兴趣量（QoIs）。我们提出了一个适用于非线性观测和预测模型的预测目标导向OED（GO-OED）的计算框架，该框架寻求提供对QoIs的最大EIG的实验设计。具体地，我们提出了用于QoI EIG的嵌套蒙特卡洛估计器，其中采用马尔可夫链蒙特卡洛进行后验采样，利用核密度估计来评估后验预测密度及其与先验预测之间的Kullback-Leibler散度。GO-OED设计通过在设计空间中最大化EIG来获得。

    arXiv:2403.18072v1 Announce Type: cross  Abstract: Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the des
    
[^121]: 深度学习在海洋垃圾跟踪和检测中的最新应用：一项调查

    State of the art applications of deep learning within tracking and detecting marine debris: A survey

    [https://arxiv.org/abs/2403.18067](https://arxiv.org/abs/2403.18067)

    深度学习技术在海洋垃圾领域取得了长足进展，特别是YOLO系列在目标检测方面表现优异，但研究显示当前缺乏全面的水下垃圾数据库，这成为了进一步研究的瓶颈。

    

    深度学习技术在海洋垃圾问题中已经被探索了大约20年，但大多数研究在过去五年内快速发展。我们提供了对最近28项深度学习在海洋垃圾领域中最新和最重要贡献的深入、最新的总结和分析。通过交叉引用研究论文结果，YOLO系列明显优于所有其他目标检测方法，但许多受尊敬的贡献者明确表示，目前没有现成的用于机器学习的全面水下垃圾数据库。我们使用由我们精心策划和标记的小型数据集，在一个二分类任务上测试了YOLOv5，并发现准确率较低，假阳性率较高，突显了全面数据库的重要性。我们在这项调查中总结了超过40项未来的研究建议。

    arXiv:2403.18067v1 Announce Type: cross  Abstract: Deep learning techniques have been explored within the marine litter problem for approximately 20 years but the majority of the research has developed rapidly in the last five years. We provide an in-depth, up to date, summary and analysis of 28 of the most recent and significant contributions of deep learning in marine debris. From cross referencing the research paper results, the YOLO family significantly outperforms all other methods of object detection but there are many respected contributions to this field that have categorically agreed that a comprehensive database of underwater debris is not currently available for machine learning. Using a small dataset curated and labelled by us, we tested YOLOv5 on a binary classification task and found the accuracy was low and the rate of false positives was high; highlighting the importance of a comprehensive database. We conclude this survey with over 40 future research recommendations an
    
[^122]: 光谱卷积变压器：协调视觉变压器中的实部和复部多视图光谱算子

    Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer

    [https://arxiv.org/abs/2403.18063](https://arxiv.org/abs/2403.18063)

    该论文提出了光谱卷积变压器 (SCT)，通过结合局部信息的卷积操作和全局信息的复杂傅里叶基础，实现了对视觉变压器中实部和复部多视图光谱算子的协调，从而实现了更好的性能。

    

    视觉中使用的Transformer已经通过各种结构进行了研究 - 如ViT、PVT和Swin。这些工作旨在改进注意力机制并使其更加高效。与此不同的是，人们感受到了包含局部信息的需要，这导致在Transformer中引入卷积，如CPVT和CvT。我们使用复杂傅立叶基础捕捉全局信息，通过各种方法，如AFNO、GFNet和Spectformer实现全局令牌混合。我们提倡结合数据的三种不同视图 - 局部、全局和长程依赖性。我们还研究了仅使用实域光谱表示的最简单全局表示 - 通过Hartley变换获得。我们在初始层中使用卷积算子捕捉局部信息。通过这两个贡献，我们能够优化并获得一个提供改进性能的光谱卷积变压器（SCT）。

    arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
    
[^123]: 在射电天文学中量化模型不确定性的R2D2图像重建

    R2D2 image reconstruction with model uncertainty quantification in radio astronomy

    [https://arxiv.org/abs/2403.18052](https://arxiv.org/abs/2403.18052)

    本论文研究了射电天文学中的R2D2图像重建方法，通过研究学习模型系列的不确定性，提出了一种使用集成平均方法进行不确定性量化的方式。

    

    最近提出了用于天文学中射电干涉(RI)成像的“高动态范围成像的残差到残差DNN系列”(R2D2)方法。R2D2的重建形成为一系列残差图像，迭代地估计为以前迭代的图像估计和相关数据残差为输入的深度神经网络(DNN)的输出。在这项工作中，我们通过研究其学习模型系列的不确定性来研究R2D2图像估计过程的稳健性。采用集成平均方法，可以训练多个系列，来自在每次迭代过程中对训练过程进行不同随机DNN初始化。由此产生的多个R2D2实例也可用于产生“R2D2样本”，从中经验均值和标准差赋予算法联合估计和不确定性量化功能。重点放在RI imag

    arXiv:2403.18052v1 Announce Type: cross  Abstract: The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2) approach was recently introduced for Radio-Interferometric (RI) imaging in astronomy. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the previous iteration's image estimate and associated data residual as inputs. In this work, we investigate the robustness of the R2D2 image estimation process, by studying the uncertainty associated with its series of learned models. Adopting an ensemble averaging approach, multiple series can be trained, arising from different random DNN initializations of the training process at each iteration. The resulting multiple R2D2 instances can also be leveraged to generate ``R2D2 samples'', from which empirical mean and standard deviation endow the algorithm with a joint estimation and uncertainty quantification functionality. Focusing on RI imag
    
[^124]: 深度多面体自编码器用于低维线性参数变化逼近和非线性反馈设计

    Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design

    [https://arxiv.org/abs/2403.18044](https://arxiv.org/abs/2403.18044)

    该研究开发了一种用于控制应用的深度多面体自编码器，在大规模系统的计算非线性控制器设计中展现出比标准线性方法更好的性能，其特定架构使得实现更高阶级数展开几乎没有额外计算负担。

    

    多面体自编码器提供了多面体中状态的低维参数化。对于非线性PDE，这很容易应用于低维线性参数变化(LPV)逼近，因为它们已被用于通过状态相关Riccati方程的级数展开实现有效的非线性控制器设计。在这项工作中，我们开发了一种用于控制应用的多面体自编码器，并展示了它如何在视图非线性系统的LPV逼近方面优于标准线性方法，以及特定架构如何在几乎没有额外计算的情况下实现更高阶级数展开。我们通过彻底的数值研究展示了该方法在大规模系统的计算非线性控制器设计中的性质和潜力。

    arXiv:2403.18044v1 Announce Type: cross  Abstract: Polytopic autoencoders provide low-dimensional parametrizations of states in a polytope. For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation. In this work, we develop a polytopic autoencoder for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort. We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study.
    
[^125]: 双向一致性模型

    Bidirectional Consistency Models

    [https://arxiv.org/abs/2403.18035](https://arxiv.org/abs/2403.18035)

    提出了双向一致性模型（BCM），学习一个神经网络，能够实现沿着概率流常微分方程前向和后向遍历，从而有效地统一了生成和编辑图像等任务。

    

    扩散模型（DMs）通过迭代去噪一个随机向量能够生成非常高质量的样本，这个过程对应于沿着概率流常微分方程（PF ODE）移动。有趣的是，DMs还可以通过沿着PF ODE向后移动将输入图像转换为噪声，这是下游任务（如插值和图像编辑）的关键操作。然而，这一过程的迭代性质限制了其速度，阻碍了其更广泛的应用。最近，一致性模型（CMs）已经出现，以解决这一挑战，通过近似PF ODE的积分，从而避免了需要迭代。然而，缺乏显式ODE求解器使得反演过程复杂化。为了解决这个问题，我们引入了双向一致性模型（BCM），学习单个神经网络，能够同时实现沿着PF ODE的前向和后向遍历，有效地统一生成和

    arXiv:2403.18035v1 Announce Type: new  Abstract: Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation an
    
[^126]: 从部分观测预测物种出现模式

    Predicting species occurrence patterns from partial observations

    [https://arxiv.org/abs/2403.18028](https://arxiv.org/abs/2403.18028)

    提出了一个问题，即采用卫星图像和其他物种出现信息来预测物种出现模式，并提出了一个通用模型R-Tran，可以利用部分观测数据进行预测，优于其他方法。

    

    为了应对生物多样性和气候危机，我们需要了解物种分布的位置以及这些模式如何变化。然而，大多数物种的观测数据仍然非常有限，可用数据的量在不同分类群之间差异很大。我们提出了一个问题，即在给定卫星图像和其他物种出现信息的情况下预测物种出现模式。为了在此任务上评估算法，我们介绍了SatButterfly数据集，其中包含了蝴蝶的卫星图像、环境数据和观测数据，旨在与现有的鸟类观测数据集SatBird配对。为了解决这一任务，我们提出了一个通用模型R-Tran，用于预测物种出现模式，可以在任何地方使用部分观测数据。我们发现，R-Tran在预测物种遭遇率方面优于其他方法。

    arXiv:2403.18028v1 Announce Type: cross  Abstract: To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial
    
[^127]: 基于生成对抗网络的交叉系统生物图像质量增强，用作建立多机构显微镜合作网络的基础

    Cross-system biological image quality enhancement based on the generative adversarial network as a foundation for establishing a multi-institute microscopy cooperative network

    [https://arxiv.org/abs/2403.18026](https://arxiv.org/abs/2403.18026)

    通过生成对抗网络实现了共聚焦显微镜和广场荧光显微镜之间的图像质量转换，提供了低均方误差、高结构相似性和高峰值信噪比的高质量图像。

    

    生物系统的高质量荧光成像受限于光漂白和光毒性等过程，以及在许多情况下，受限于最新一代显微镜的有限访问。此外，低时间分辨率可能导致活体系统中的运动模糊效果。我们的工作提出了一种基于深度学习生成对抗方法的解决方案，用于基于低质量（LQ）图像获得高质量（HQ）图像的问题。我们提出了一种生成对抗网络（GAN），用于两种不同独立显微系统之间的对比传递：共聚焦显微镜（产生HQ图像）和广场荧光显微镜（产生LQ图像）。我们的模型证明了这种传递是可能的，使我们能够收到具有低均方误差（MSE）值、高结构相似性指数（SSIM）和高峰值信噪比（PSNR）值的HQ生成图像。对于我们最好的模型

    arXiv:2403.18026v1 Announce Type: cross  Abstract: High-quality fluorescence imaging of biological systems is limited by processes like photobleaching and phototoxicity, and also in many cases, by limited access to the latest generations of microscopes. Moreover, low temporal resolution can lead to a motion blur effect in living systems. Our work presents a deep learning (DL) generative-adversarial approach to the problem of obtaining high-quality (HQ) images based on their low-quality (LQ) equivalents. We propose a generative-adversarial network (GAN) for contrast transfer between two different separate microscopy systems: a confocal microscope (producing HQ images) and a wide-field fluorescence microscope (producing LQ images). Our model proves that such transfer is possible, allowing us to receive HQ-generated images characterized by low mean squared error (MSE) values, high structural similarity index (SSIM), and high peak signal-to-noise ratio (PSNR) values. For our best model in 
    
[^128]: 通过特定掩码损失改善预训练语言模型的敏感性：以生物医学实体识别为例

    Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER

    [https://arxiv.org/abs/2403.18025](https://arxiv.org/abs/2403.18025)

    提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。

    

    将语言模型（LMs）调整到新领域通常通过在特定领域数据上微调预训练LM（PLM）来实现。微调将新知识引入LM，使它能够理解和有效执行目标域任务。然而，微调可能会无意中变得不够敏感，如果它忽视了源域和目标域之间的广泛差异（例如在词义上）。为了解决微调不敏感的问题，我们提出了Mask Specific Language Modeling（MSLM），一种通过在微调过程中适当加权领域特定术语（DS-terms）的重要性来有效获取目标领域知识的方法。MSLM同时屏蔽DS术语和通用词，然后通过确保LM受到更大惩罚来学习特定于掩码的损失。

    arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
    
[^129]: DORE：一份用于葡萄牙语定义生成的数据集

    DORE: A Dataset For Portuguese Definition Generation

    [https://arxiv.org/abs/2403.18018](https://arxiv.org/abs/2403.18018)

    DORE是第一个用于葡萄牙语的定义生成数据集，填补了这一领域的空白，包含超过10万个定义，并评估了多种基于深度学习的模型。

    

    arXiv:2403.18018v1 公告类型：新的 摘要：定义建模（DM）是自动为特定单词生成词典定义的任务。具有DM能力的计算系统可以在多个受众中受益，因为DM被视为监督自然语言生成问题，这些系统需要大量带注释的数据集来训练机器学习（ML）模型。已经发布了一些用于英语和其他高资源语言的DM数据集。尽管葡萄牙语在大多数自然语言处理任务中被认为是一种中/高资源语言，且被2亿多母语人口使用，但目前尚无葡萄牙语的DM数据集。在这项研究中，我们通过引入DORE填补了这一空白；这是第一个用于葡萄牙语的定义建模数据集，包含超过10万个定义。我们还在DORE上评估了几种基于深度学习的DM模型，并报告了结果。

    arXiv:2403.18018v1 Announce Type: new  Abstract: Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings o
    
[^130]: 考虑Wasserstein图匹配的半监督图像字幕生成

    Semi-Supervised Image Captioning Considering Wasserstein Graph Matching

    [https://arxiv.org/abs/2403.17995](https://arxiv.org/abs/2403.17995)

    提出了一种考虑Wasserstein图匹配的半监督图像字幕生成方法，用于解决半监督图像字幕生成中的困境

    

    图像字幕生成可以自动生成给定图像的字幕，其关键挑战是学习从视觉特征到自然语言特征的映射函数。为了解决对描述图像需要大量人力的困境，提出了一种考虑Wasserstein图匹配的新型半监督图像字幕生成方法（SSIC-WGM），以监督生成的句子。

    arXiv:2403.17995v1 Announce Type: cross  Abstract: Image captioning can automatically generate captions for the given images, and the key challenge is to learn a mapping function from visual features to natural language features. Existing approaches are mostly supervised ones, i.e., each image has a corresponding sentence in the training set. However, considering that describing images always requires a huge of manpower, we usually have limited amount of described images (i.e., image-text pairs) and a large number of undescribed images in real-world applications. Thereby, a dilemma is the "Semi-Supervised Image Captioning". To solve this problem, we propose a novel Semi-Supervised Image Captioning method considering Wasserstein Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to supervise the generated sentences. Different from traditional single modal semi-supervised methods, the difficulty of semi-supervised cross-modal learning lies in constructing intermediately
    
[^131]: ICCV 2023第1届感知测试挑战中点跟踪任务的解决方案

    Solution for Point Tracking Task of ICCV 1st Perception Test Challenge 2023

    [https://arxiv.org/abs/2403.17994](https://arxiv.org/abs/2403.17994)

    提出了一种改进的方法TAPIR+，专注于纠正静态摄像机拍摄的视频中静态点的跟踪，并在ICCV 2023第1届感知测试挑战的点跟踪任务中取得了第一名。

    

    这份报告提出了一种改进的方法，用于追踪视频中的任意物理表面，以应对现有方法中存在的由时间预测引起的累积误差问题。我们提出了一种简单而有效的方法，称为TAP with confident static points (TAPIR+)，专注于纠正静态摄像机拍摄的视频中静态点的跟踪。我们的方法包含两个关键组件：多粒度摄像机运动检测和基于CMR的点轨迹预测。通过使用这种方法，我们在最终测试中排名第一，得分为0.46。

    arXiv:2403.17994v1 Announce Type: cross  Abstract: This report proposes an improved method for the Tracking Any Point (TAP) task, which tracks any physical surface through a video. Several existing approaches have explored the TAP by considering the temporal relationships to obtain smooth point motion trajectories, however, they still suffer from the cumulative error caused by temporal prediction. To address this issue, we propose a simple yet effective approach called TAP with confident static points (TAPIR+), which focuses on rectifying the tracking of the static point in the videos shot by a static camera. To clarify, our approach contains two key components: (1) Multi-granularity Camera Motion Detection, which could identify the video sequence by the static camera shot. (2) CMR-based point trajectory prediction with one moving object segmentation approach to isolate the static point from the moving object. Our approach ranked first in the final test with a score of 0.46.
    
[^132]: 将人工智能与自然智能相融合：从统计力学到人工智能再到湍流

    Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence

    [https://arxiv.org/abs/2403.17993](https://arxiv.org/abs/2403.17993)

    人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。

    

    这篇论文反思了人工智能在科学研究中的未来角色，特别关注了湍流研究，并通过根植于非平衡统计力学的扩散模型来检验人工智能的发展，强调了人工智能通过创新性地利用深度神经网络推动减少的拉格朗日湍流模型的重要影响。此外，论文审查了湍流研究中的各种其他人工智能应用，并概述了在人工智能和统计流体力学的同时发展中的潜在挑战和机会。

    arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
    
[^133]: 使用多任务条件神经网络进行解释性癌细胞检测的声子显微镜

    Interpretable cancer cell detection with phonon microscopy using multi-task conditional neural networks for inter-batch calibration

    [https://arxiv.org/abs/2403.17992](https://arxiv.org/abs/2403.17992)

    提出了一个多任务条件神经网络框架，用于解释性癌细胞检测的声子显微镜，实现了批间校准和时间分辨声子信号精准细胞分类，实现了89.22%的平衡精度和0.5秒的快速分类。

    

    人工智能的进展在揭示声子显微镜（高频超声波）数据中的基本信息以识别癌细胞方面显示出巨大潜力。然而，这项技术受到“批次效应”的影响，这是由于每次实验之间无法避免的技术变化所造成的，这些变化会产生混淆变量，使人工智能模型可能会无意中学习到。因此，我们提出了一个多任务条件神经网络框架，旨在同时实现批间校准，通过消除混淆变量，并对基于时间分辨声子信号的细胞准确分类。我们通过在不同的实验批次上进行训练和验证来验证我们的方法，实现了背景、健康和癌症区域分类的平衡精度为89.22％，交叉验证平均精度为89.07％。分类可在0.5秒内完成，只需要简单的先前批次信息。

    arXiv:2403.17992v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) show great potential in revealing underlying information from phonon microscopy (high-frequency ultrasound) data to identify cancerous cells. However, this technology suffers from the 'batch effect' that comes from unavoidable technical variations between each experiment, creating confounding variables that the AI model may inadvertently learn. We therefore present a multi-task conditional neural network framework to simultaneously achieve inter-batch calibration, by removing confounding variables, and accurate cell classification of time-resolved phonon-derived signals. We validate our approach by training and validating on different experimental batches, achieving a balanced precision of 89.22% and an average cross-validated precision of 89.07% for classifying background, healthy and cancerous regions. Classification can be performed in 0.5 seconds with only simple prior batch information requ
    
[^134]: LLM生成代码的水印技术是否具有鲁棒性？

    Is Watermarking LLM-Generated Code Robust?

    [https://arxiv.org/abs/2403.17983](https://arxiv.org/abs/2403.17983)

    该研究探讨了现有水印技术在大型语言模型生成的Python代码上的鲁棒性，发现容易通过保留语义转换来移除这些水印。

    

    我们首次研究了现有水印技术在大型语言模型生成的Python代码上的鲁棒性。尽管现有作品表明水印技术对自然语言具有鲁棒性，但我们发现通过保留语义的转换很容易移除代码上的这些水印。

    arXiv:2403.17983v1 Announce Type: cross  Abstract: We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.
    
[^135]: 马尔可夫链模型用于检查心理测试中的响应动态

    Markov chain models for inspecting response dynamics in psychological testing

    [https://arxiv.org/abs/2403.17982](https://arxiv.org/abs/2403.17982)

    本研究提倡利用第一阶马尔可夫链建模来捕捉和预测调查和测试答复中的序贯依赖关系，避免忽略上下文概率在塑造心理测试答复模式中的重要性。

    

    在心理测试中考虑上下文概率的重要性被强调，尽管方法论文献中广泛讨论了顺序效应的无处不在的性质。本研究借鉴了路径依赖性、一阶自相关性、状态依赖性和磁滞效应等概念，试图解决早期答复如何成为测试、调查和问卷中后续答案的锚点。引入从量子物理学中导出的不交换观测的概念，突出了它们在表征心理过程以及测量仪器对参与者答复的影响中的作用。我们主张利用一阶马尔可夫链建模来捕捉和预测调查和测试答复中的序贯依赖关系。第一阶马尔可夫链模型的应用在于个体倾向于表现为部分

    arXiv:2403.17982v1 Announce Type: cross  Abstract: The importance of considering contextual probabilities in shaping response patterns within psychological testing is underscored, despite the ubiquitous nature of order effects discussed extensively in methodological literature. Drawing from concepts such as path-dependency, first-order autocorrelation, state-dependency, and hysteresis, the present study is an attempt to address how earlier responses serve as an anchor for subsequent answers in tests, surveys, and questionnaires. Introducing the notion of non-commuting observables derived from quantum physics, I highlight their role in characterizing psychological processes and the impact of measurement instruments on participants' responses. We advocate for the utilization of first-order Markov chain modeling to capture and forecast sequential dependencies in survey and test responses. The employment of the first-order Markov chain model lies in individuals' propensity to exhibit parti
    
[^136]: 基于图对比学习的入侵检测方法EG-ConMix

    EG-ConMix: An Intrusion Detection Method based on Graph Contrastive Learning

    [https://arxiv.org/abs/2403.17980](https://arxiv.org/abs/2403.17980)

    提出了一种基于对比学习的入侵检测方法EG-ConMix，结合数据增强和E-GraphSAGE模块解决了数据不平衡问题。

    

    随着物联网设备数量的增加，安全问题变得更加突出。部署网络入侵检测系统（NIDS）可以通过监控网络流量、检测和发现入侵并及时发出安全警报来最小化威胁的影响。本文提出了一种基于E-GraphSAGE的EG-ConMix方法，结合数据增强模块来解决数据不平衡问题，并融合对比学习以区分正常和异常网络流量。

    arXiv:2403.17980v1 Announce Type: cross  Abstract: As the number of IoT devices increases, security concerns become more prominent. The impact of threats can be minimized by deploying Network Intrusion Detection System (NIDS) by monitoring network traffic, detecting and discovering intrusions, and issuing security alerts promptly. Most intrusion detection research in recent years has been directed towards the pair of traffic itself without considering the interrelationships among them, thus limiting the monitoring of complex IoT network attack events. Besides, anomalous traffic in real networks accounts for only a small fraction, which leads to a severe imbalance problem in the dataset that makes algorithmic learning and prediction extremely difficult. In this paper, we propose an EG-ConMix method based on E-GraphSAGE, incorporating a data augmentation module to fix the problem of data imbalance. In addition, we incorporate contrastive learning to discern the difference between normal 
    
[^137]: 用于恶意软件检测中的长程预测任务的全息全局卷积网络

    Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection

    [https://arxiv.org/abs/2403.17978](https://arxiv.org/abs/2403.17978)

    提出了一种利用全息全局卷积网络（HGConv）和全息简化表示（HRR）属性的方法，不需要复杂的核计算或设计，在恶意软件检测领域取得了新的SOTA结果。

    

    恶意软件检测是一个有趣且有价值的领域，因为它对现实世界有重要影响并具有独特的机器学习挑战。本文研究现有的长程技术和基准，并发现它们在这个问题领域不太适用。我们引入了利用全息简化表示（HRR）属性来编码和解码序列元素特征的全息全局卷积网络（HGConv）。与其他全局卷积方法不同，我们的方法不需要任何复杂的核计算或精心设计的核。HGConv核被定义为通过反向传播学习的简单参数。该方法在Microsoft恶意软件分类挑战赛、Drebin和EMBER恶意软件基准测试中取得了新的SOTA结果。在序列长度的对数级复杂度下，实证结果表明HGConv的运行时间明显更快。

    arXiv:2403.17978v1 Announce Type: cross  Abstract: Malware detection is an interesting and valuable domain to work in because it has significant real-world impact and unique machine-learning challenges. We investigate existing long-range techniques and benchmarks and find that they're not very suitable in this problem area. In this paper, we introduce Holographic Global Convolutional Networks (HGConv) that utilize the properties of Holographic Reduced Representations (HRR) to encode and decode features from sequence elements. Unlike other global convolutional methods, our method does not require any intricate kernel computation or crafted kernel design. HGConv kernels are defined as simple parameters learned through backpropagation. The proposed method has achieved new SOTA results on Microsoft Malware Classification Challenge, Drebin, and EMBER malware benchmarks. With log-linear complexity in sequence length, the empirical results demonstrate substantially faster run-time by HGConv c
    
[^138]: 深度生成式领域自适应与时间注意力跨用户活动识别

    Deep Generative Domain Adaptation with Temporal Attention for Cross-User Activity Recognition

    [https://arxiv.org/abs/2403.17958](https://arxiv.org/abs/2403.17958)

    深度生成式领域自适应方法 DGDATA 独特地在领域自适应过程中识别和整合了时间关系。

    

    在人体活动识别（HAR）中，一个主要的假设是用于训练和评估目的的数据来自相同的分布。然而，在实际实现中，往往挑战这一概念，尤其是在跨用户HAR等场景中表现为数据分布差异。领域自适应是应对跨用户HAR任务中这些挑战的有效方法。然而，领域自适应技术中一个明显的缺陷是忽视了在调整数据分布阶段嵌入在时间序列数据中的时间关系。针对这一疏漏，我们的研究提出了带有时间注意力的深度生成式领域自适应（DGDATA）方法。

    arXiv:2403.17958v1 Announce Type: cross  Abstract: In Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed ($\displaystyle i.i.d.$). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognises and integrates temporal relations during the domain adaptation proce
    
[^139]: Sort & Slice：一种简单且优越的扩展连接指纹的非哈希折叠备选方案

    Sort & Slice: A Simple and Superior Alternative to Hash-Based Folding for Extended-Connectivity Fingerprints

    [https://arxiv.org/abs/2403.17954](https://arxiv.org/abs/2403.17954)

    Sort & Slice 是一种易于实现且无位碰撞的替代方案，用于池化扩展连接指纹的亚结构

    

    arXiv:2403.17954v1 公告类型：新摘要：扩展连接指纹（ECFPs）是当前化学信息学和分子机器学习中的一个普遍工具，是用于化学预测的最常见的分子特征提取技术之一。由图神经网络学习的原子特征可以使用各种图池化方法聚合到化合物级表示中；相比之下，一组检测到的ECFP亚结构默认情况下只使用一个简单的基于哈希的折叠过程转换为位向量。我们引入了一个通用的数学框架，用于通过一种称为亚结构池化的形式操作来对结构指纹进行向量化，该框架包括基于哈希的折叠、算法性亚结构选择和各种其他潜在技术。我们继续描述Sort & Slice，这是一种易于实现且无位碰撞的替代方案，用于池化ECFP亚结构。

    arXiv:2403.17954v1 Announce Type: new  Abstract: Extended-connectivity fingerprints (ECFPs) are a ubiquitous tool in current cheminformatics and molecular machine learning, and one of the most prevalent molecular feature extraction techniques used for chemical prediction. Atom features learned by graph neural networks can be aggregated to compound-level representations using a large spectrum of graph pooling methods; in contrast, sets of detected ECFP substructures are by default transformed into bit vectors using only a simple hash-based folding procedure. We introduce a general mathematical framework for the vectorisation of structural fingerprints via a formal operation called substructure pooling that encompasses hash-based folding, algorithmic substructure-selection, and a wide variety of other potential techniques. We go on to describe Sort & Slice, an easy-to-implement and bit-collision-free alternative to hash-based folding for the pooling of ECFP substructures. Sort & Slice fi
    
[^140]: 关于现实生活和计算中的前瞻性的注解

    A Note On Lookahead In Real Life And Computing

    [https://arxiv.org/abs/2403.17942](https://arxiv.org/abs/2403.17942)

    本文旨在学习、理解和探索前瞻概念，并设计新颖的模型作为解决现实世界问题的解决方案。

    

    过去、现在和未来被认为是人类为了他们的存在和成长而定义的三个时间和逻辑概念。我们作为人类有幸能够利用我们的智慧在真实世界中的物理发生之前在头脑中执行某项活动。过去的知识、现在的冷静和未来的展望对应于现实生活以及计算的各个领域中的三个概念，分别是回顾、观察和前瞻。前瞻（LA）处理信息的未来预测和处理输入以提前生成输出。在本文中，我们的主要目标是学习、理解和探索LA概念，并设计新颖的模型作为解决现实世界问题的解决方案。我们提出了三种实践中使用的基于输入信息可用性的知名算法框架，例如脱机、在线和半在线。

    arXiv:2403.17942v1 Announce Type: cross  Abstract: Past, Present and Future are considered to be three temporal and logical concepts which are well defined by human beings for their existence and growth. We, as human beings, have the privilege of using our intelligence to mentally execute an activity before physical occurrence of the same in the real world. Knowledge of the past, aplomb of present and visualisation for the future correspond to three concepts such as look-back, look-at and look-ahead respectively in real life as well as in diversified domains of computing. Look-Ahead(LA) deals with the future prediction of information and processing of input to produce the output in advance. In this article, our main objective is to learn, understand and explore the concept of LA and design novel models as solution for real world problems. We present three well known algorithmic frameworks used in practice based on availability of input information such as offline, online and semi-onlin
    
[^141]: 具有R2D2的可扩展非笛卡尔磁共振成像方法

    Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2

    [https://arxiv.org/abs/2403.17905](https://arxiv.org/abs/2403.17905)

    通过引入R2D2方法，提出了一种可扩展的非笛卡尔磁共振图像重建方法。

    

    我们提出了一种新的非笛卡尔磁共振图像重建方法。我们利用最近在天文成像中引入的“用于高动态范围成像的残差级联DNN系列（R2D2）”方法，解决了可扩展性挑战。R2D2的重建被形成为残差图像的系列，被迭代地估计为接受上一次迭代的图像估计和相关数据残差作为输入的DNN的输出。

    arXiv:2403.17905v1 Announce Type: cross  Abstract: We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)" approach recently introduced in astronomical imaging. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algo
    
[^142]: 用联邦学习增强数据网格

    Empowering Data Mesh with Federated Learning

    [https://arxiv.org/abs/2403.17878](https://arxiv.org/abs/2403.17878)

    数据网格提出了一种去中心化的数据范式，通过将数据所有权分布到每个数据领域，同时保持联合治理，以克服数据源激增和及时分析处理需求增长的挑战。

    

    数据架构的演变见证了数据湖的兴起，旨在解决数据管理的瓶颈并推动智能决策。然而，这种集中化架构受制于数据源的激增和对及时分析处理的日益增长需求。为了克服这些挑战，提出了一种新的数据范式，数据网格。数据网格将领域视为首要关注点，通过将数据所有权从中央团队分发到每个数据领域，同时保持联合治理来监控领域及其数据产品。像Paypal、Netflix和Zalando等许多亿美元组织已经基于这种新架构转变了他们的数据分析流程。在这种去中心化架构中，数据由每个领域团队本地保存，传统的集中式机器学习无法在多个领域之间进行有效分析。

    arXiv:2403.17878v1 Announce Type: new  Abstract: The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains,
    
[^143]: 在具有不确定标签的半监督学习中的渐近贝叶斯风险

    Asymptotic Bayes risk of semi-supervised learning with uncertain labeling

    [https://arxiv.org/abs/2403.17767](https://arxiv.org/abs/2403.17767)

    论文研究了具有不确定标签的半监督学习中的渐近贝叶斯风险计算，并通过与最佳算法比较得出新的见解。

    

    本文考虑了高斯混合模型上的半监督分类设置，其中数据的标签不像通常那样严格，而是带有不确定标签。我们的主要目标是计算该模型的贝叶斯风险。我们比较了该模型的贝叶斯风险与目前已知的最佳算法的行为。这种比较最终为该算法提供了新的见解。

    arXiv:2403.17767v1 Announce Type: cross  Abstract: This article considers a semi-supervised classification setting on a Gaussian mixture model, where the data is not labeled strictly as usual, but instead with uncertain labels. Our main aim is to compute the Bayes risk for this model. We compare the behavior of the Bayes risk and the best known algorithm for this model. This comparison eventually gives new insights over the algorithm.
    
[^144]: 期望与现实：实践中评估入侵检测系统

    Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice

    [https://arxiv.org/abs/2403.17458](https://arxiv.org/abs/2403.17458)

    论文通过实证比较不同入侵检测系统，发现最佳解决方案取决于外部变量，如攻击类型、复杂性和网络环境，深度神经网络在某些数据集上表现最佳，但并非始终是最佳选择。

    

    我们的论文通过实证比较最近的入侵检测系统，为用户提供客观比较，以帮助用户根据其需求选择最适合的解决方案。我们的结果显示，没有一种解决方案是最好的，而是取决于外部变量，如攻击类型、复杂性和数据集中的网络环境。例如，BoT_IoT和Stratosphere IoT数据集都捕获了与物联网相关的攻击，但深度神经网络在使用BoT_IoT数据集进行测试时表现最佳，而在使用Stratosphere IoT数据集进行测试时HELAD表现最佳。因此，尽管我们发现深度神经网络解决方案在测试数据集上具有最高的平均F1分数，但并不总是表现最好的。我们进一步讨论了使用文献和项目存储库中的IDS的困难，这使得就IDS选择得出明确结论变得复杂。

    arXiv:2403.17458v1 Announce Type: cross  Abstract: Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements. Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset. For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset. So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one. We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection.
    
[^145]: 在强化学习中模仿受成本约束的行为

    Imitating Cost-Constrained Behaviors in Reinforcement Learning

    [https://arxiv.org/abs/2403.17456](https://arxiv.org/abs/2403.17456)

    该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。

    

    长期以来，复杂的计划和调度问题一直通过各种优化或启发式方法来解决。最近，提出了从专家演示中学习的模仿学习作为解决这些问题的一种可行替代方法。模仿学习旨在通过观察专家的行为来学习奖励（或偏好）模型或直接行为策略。现有的模仿学习和逆向强化学习工作主要集中在无限制设置下的模仿（例如，车辆消耗的燃油量没有限制）。然而，在许多实际应用中，专家的行为不仅受奖励（或偏好）的影响，还受约束的影响。例如，自动驾驶送货车的决策不仅取决于路径偏好/奖励（根据过去的需求数据），还取决于车辆内的燃油和送达时间等约束。

    arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
    
[^146]: 语言模型是生物医学成像任务的免费助推器

    Language Models are Free Boosters for Biomedical Imaging Tasks

    [https://arxiv.org/abs/2403.17343](https://arxiv.org/abs/2403.17343)

    本研究揭示了基于残差的大型语言模型在生物医学成像任务中作为编码器的意想不到的有效性，利用冻结的变压器块进行直接处理视觉令牌，从而提高各种生物医学成像应用的性能。

    

    在这项研究中，我们揭示了基于残差的大型语言模型（LLMs）在生物医学成像任务中作为编码器的意想不到的有效性，这是传统上缺乏语言或文本数据的领域。该方法不同于已建立的方法，通过利用从预训练的LLMs中提取的冻结变压器块作为创新的编码器层，直接处理视觉令牌。这种策略与通常依赖于语言驱动提示和输入的标准多模态视觉语言框架有着显著的不同。我们发现这些LLMs能够提升各种生物医学成像应用的性能，包括2D和3D视觉分类任务，充当即插即用的助推器。更有趣的是，作为副产品，我们发现所提出的框架实现了卓越的性能，在M的广泛、标准化数据集中取得了新的最先进结果。

    arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
    
[^147]: 引导远程监督用于多语言关系抽取数据：适应新语言

    Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language

    [https://arxiv.org/abs/2403.17143](https://arxiv.org/abs/2403.17143)

    本文应用引导远程监督方法，为德语创建了最大的传记关系抽取数据集，同时发布了手动标注的评估数据集。

    

    摘要：关系抽取对于在数字人文学和相关学科背景下提取和理解传记信息至关重要。社区对构建能够训练机器学习模型提取关系的数据集越来越感兴趣。然而，标注这样的数据集可能既昂贵又耗时，而且仅限于英语。本文应用了引导式远程监督方法，为德语创建了一个大型传记关系抽取数据集。我们的数据集包含了超过80,000个实例，涵盖了九种关系类型，是最大的德语传记关系抽取数据集。我们还创建了一个手动标注的数据集，包含2000个实例用于评估模型，并与利用引导式远程监督方法编制的数据集一起发布。我们在自动生成的数据集上训练了几种最先进的机器学习模型，并将其发布。

    arXiv:2403.17143v1 Announce Type: new  Abstract: Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as 
    
[^148]: 用于腿式定点机器人运动操作的视觉全身控制

    Visual Whole-Body Control for Legged Loco-Manipulation

    [https://arxiv.org/abs/2403.16967](https://arxiv.org/abs/2403.16967)

    这项研究提出了一种利用视觉全身控制的框架，使腿式机器人能够同时控制腿部和手臂，以扩展操作能力，并通过仿真训练和Sim2Real转移实现了在捡起不同物体方面取得显著改进。

    

    我们研究了使用配备手臂的腿式机器人进行移动操作的问题，即腿式定点操作。尽管机器人的腿通常用于移动，但通过进行全身控制，可以扩大其操作能力。也就是说，机器人可以同时控制腿部和手臂，以扩展其工作空间。我们提出了一个能够使用视觉观测自主进行全身控制的框架。我们的方法称为\ourFull~(\our)，由一个低级策略和一个高级策略组成。低级策略使用所有自由度来跟踪末端执行器的位置，高级策略根据视觉输入提出末端执行器位置。我们在仿真中训练了两个级别的策略，并进行了从Sim到实物的转移以进行实际机器人部署。我们进行了大量实验证明，在不同配置下（高度、）捡起不同物体方面，相对基线方法取得了显著改进。

    arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
    
[^149]: 利用预训练语言模型进行粗调优的专题文档检索

    Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

    [https://arxiv.org/abs/2403.16915](https://arxiv.org/abs/2403.16915)

    本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。

    

    在信息检索系统中，利用预训练语言模型（PLM-based IR）进行微调需要学习查询表示和查询-文档关系，除了下游任务特定的学习。本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调。通过在粗调优学习查询表示和查询-文档关系，我们旨在减少微调的负担，提高下游IR任务的学习效果。我们提出了用于粗调优的查询-文档对预测（QDPP），其预测查询-文档对的适当性。评估实验显示，所提出的方法显著改善了四个专题文档检索数据集中的MRR和/或nDCG@5。此外，查询预测任务的结果表明，粗调优促进了查询表示和查询-文档关系的学习。

    arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
    
[^150]: DeepMachining: 铣床机床加工误差在线预测

    DeepMachining: Online Prediction of Machining Errors of Lathe Machines

    [https://arxiv.org/abs/2403.16451](https://arxiv.org/abs/2403.16451)

    DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。

    

    我们描述了DeepMachining，这是一种基于深度学习的人工智能系统，用于在线预测车床加工操作的加工误差。我们基于工厂的制造数据构建并评估了DeepMachining。具体来说，我们首先对特定车床机床操作预训练深度学习模型，以学习加工状态的显著特征。然后，我们微调预训练模型以适应特定加工任务。我们展示了DeepMachining在涉及不同工件和刀具的多个任务中实现了高预测准确性。据我们所知，这项工作是使用预训练深度学习模型预测车床机床加工误差的首批工厂实验之一。

    arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
    
[^151]: 通过多样化增强管道提升医疗数据集的效用：MEDDAP

    MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline

    [https://arxiv.org/abs/2403.16335](https://arxiv.org/abs/2403.16335)

    MEDDAP通过稳定扩散（SD）模型自动生成新的信息标记样本，提升医疗数据集的效用

    

    深度神经网络（DNNs）的有效性严重依赖于丰富和准确的训练数据。然而，在医疗案例中，收集和注释大规模数据通常昂贵且耗时，尤其是当从业者已被工作占据。为了解决这一挑战，我们引入了一种名为MEDDAP的新型管道，利用稳定扩散（SD）模型通过自动生成新的信息标记样本来增强现有的小数据集。

    arXiv:2403.16335v1 Announce Type: cross  Abstract: The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data. However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties. Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer. To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples. Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy 
    
[^152]: 语言-图像预训练的中心掩蔽技术

    Centered Masking for Language-Image Pre-Training

    [https://arxiv.org/abs/2403.15837](https://arxiv.org/abs/2403.15837)

    使用中心掩蔽的GLIP技术在语言-图像预训练中取代了随机掩蔽，利用高斯分布提高了性能，并且易于获得且适用于不具有明显中心焦点的数据集。

    

    我们引入了用于语言-图像预训练（GLIP）的高斯掩蔽，这是一种新颖、直接和有效的技术，用于在视觉-语言模型的预训练过程中对图像补丁进行掩蔽。GLIP基于快速语言-图像预训练（FLIP），该方法在训练CLIP模型时随机屏蔽图像补丁。GLIP将随机屏蔽替换为中心掩蔽，使用高斯分布，并受到图像中心重要性的启发。在一系列下游数据集和任务中，GLIP保留了与FLIP相同的计算节省能力，同时改善了性能，这是由我们的实验结果所证实的。我们展示了GLIP的好处很容易获得，无需精细调整高斯，也适用于包含无明显中心焦点图片的数据集。

    arXiv:2403.15837v1 Announce Type: cross  Abstract: We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel, straightforward, and effective technique for masking image patches during pre-training of a vision-language model. GLIP builds on Fast Language-Image Pre-Training (FLIP), which randomly masks image patches while training a CLIP model. GLIP replaces random masking with centered masking, that uses a Gaussian distribution and is inspired by the importance of image patches at the center of the image. GLIP retains the same computational savings as FLIP, while improving performance across a range of downstream datasets and tasks, as demonstrated by our experimental results. We show the benefits of GLIP to be easy to obtain, requiring no delicate tuning of the Gaussian, and also applicable to data sets containing images without an obvious center focus.
    
[^153]: 大型语言模型在心理健康领域的机会和风险

    The opportunities and risks of large language models in mental health

    [https://arxiv.org/abs/2403.14814](https://arxiv.org/abs/2403.14814)

    大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。

    

    全球心理健康问题的发生率正在上升，人们越来越意识到现有的心理保健模式无法充分扩展以满足需求。随着大型语言模型（LLMs）的出现，人们对它们具有创造新颖、大规模解决方案以支持心理健康的承诺感到乐观。尽管它们还处于初期阶段，LLMs已被应用于与心理健康相关的任务。本综述总结了已有文献中关于利用LLMs提供心理健康教育、评估和干预的努力，并突出了每个领域中产生积极影响的关键机会。然后，我们强调了将LLMs应用于心理健康领域所伴随的风险，并鼓励采用策略来减轻这些风险。对于心理健康支持的迫切需求必须与负责任的心理健康LLMs的开发、测试和部署相平衡。特别关键的是确保心理健康...

    arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
    
[^154]: 简化扩散薛定谔桥

    Simplified Diffusion Schr\"odinger Bridge

    [https://arxiv.org/abs/2403.14623](https://arxiv.org/abs/2403.14623)

    介绍了简化后的扩散薛定谔桥（DSB），通过与基于得分的生成模型（SGM）的统一解决了复杂数据生成中的限制，提高了性能并加快了收敛速度。

    

    这篇论文介绍了一种新的理论简化扩散薛定谔桥（DSB），便于将其与基于得分的生成模型（SGM）统一起来，解决了DSB在复杂数据生成方面的局限性，实现更快的收敛速度和增强的性能。通过将SGM作为DSB的初始解决方案，我们的方法利用了这两个框架的优势，确保了更高效的训练过程，并改进了SGM的性能。我们还提出了一种重新参数化技术，尽管存在理论近似，但实际上提高了网络的拟合能力。我们进行了大量的实验证实，证实了简化的DSB的有效性，展示了其显著的改进。我们相信这项工作的贡献为先进的生成建模铺平了道路。

    arXiv:2403.14623v1 Announce Type: new  Abstract: This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
    
[^155]: 具有对数据异构性的自适应的拜占庭弹性联邦学习

    Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity

    [https://arxiv.org/abs/2403.13374](https://arxiv.org/abs/2403.13374)

    通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。

    

    本文处理了在存在恶意拜占庭攻击和数据异构性的情况下的联邦学习（FL）。提出了一种新颖的鲁棒平均梯度算法（RAGA），该算法利用几何中位数进行聚合，并可以自由选择本地更新的轮数。与大多数现有的弹性方法不同，这些方法基于强凸损失函数或均匀分布的数据集进行收敛分析，我们进行了对强凸和非凸损失函数在异构数据集上的收敛分析。根据我们的理论分析，只要恶意用户数据集的比例小于一半，RAGA就可以以$\mathcal{O}({1}/{T^{2/3- \delta}})$的速度实现非凸损失函数的收敛，其中$T$为迭代次数，$\delta \in (0, 2/3)$，对于强凸损失函数则呈线性收敛。此外，稳定点或全局最优解

    arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
    
[^156]: 一种基于物理的深度学习框架用于布料模拟

    A Physics-embedded Deep Learning Framework for Cloth Simulation

    [https://arxiv.org/abs/2403.12820](https://arxiv.org/abs/2403.12820)

    该论文提出了一种基于物理的深度学习框架，可以直接编码布料模拟的物理特征，实现快速和实时模拟，并在不使用新数据训练的情况下通过测试表现出与基线的一致性。

    

    精细的布料模拟长期以来一直是计算机图形学中所期望的。为改进受力交互、碰撞处理和数值积分，提出了各种方法。深度学习有潜力实现快速和实时模拟，但常见的神经网络结构通常需要大量参数来捕获布料动力学。本文提出了一种直接编码布料模拟物理特征的物理嵌入学习框架。卷积神经网络用于表示质点-弹簧系统的空间相关性，之后设计了三个分支来学习布料物理的线性、非线性和时间导数特征。该框架还可以通过传统模拟器或子神经网络与其他外部力和碰撞处理进行集成。模型在不使用新数据进行训练的情况下，在不同的布料动画案例中进行了测试。与基线的一致性

    arXiv:2403.12820v1 Announce Type: cross  Abstract: Delicate cloth simulations have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation. The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselin
    
[^157]: 函数图卷积网络：一个统一的多任务和多模态学习框架，促进健康和社会关怀洞见

    Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights

    [https://arxiv.org/abs/2403.10158](https://arxiv.org/abs/2403.10158)

    该论文提出了一个新颖的函数图卷积网络框架，结合了函数数据分析和图卷积网络，解决了数字健康和纵向研究中的多任务和多模态学习复杂性，关键创新包括任务特定嵌入组件、执行分类、回归和预测的能力，以及创建知识图进行数据解释。

    

    本文介绍了一种新颖的函数图卷积网络（funGCN）框架，将函数数据分析和图卷积网络相结合，以解决数字健康和纵向研究中的多任务和多模态学习的复杂性。随着健康解决方案对改善医疗保健和社会支持的重要性日益增长，确保各年龄段的健康生活和促进幸福感，funGCN提供了一种统一的方法来处理多个实体的多元纵向数据，并确保即使在样本量较小的情况下也具有可解释性。关键创新包括管理不同数据类型的任务特定嵌入组件、执行分类、回归和预测的能力，以及创建知识图以获取洞察性数据解释。通过模拟实验和实际数据应用验证了funGCN的有效性。

    arXiv:2403.10158v1 Announce Type: cross  Abstract: This paper introduces a novel Functional Graph Convolutional Network (funGCN) framework that combines Functional Data Analysis and Graph Convolutional Networks to address the complexities of multi-task and multi-modal learning in digital health and longitudinal studies. With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small sample sizes. Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation. The efficacy of funGCN is validated through simulation experiments and a real-data application.
    
[^158]: 深度限价订单簿预测

    Deep Limit Order Book Forecasting

    [https://arxiv.org/abs/2403.09267](https://arxiv.org/abs/2403.09267)

    该研究利用深度学习方法预测纳斯达克交易所股票的限价订单簿中间价格变动，提出了一个创新的操作框架来评估预测的实用性。

    

    我们利用尖端的深度学习方法探索了在纳斯达克交易所上交易的一组异质股票的高频限价订单簿中间价格变动的可预测性。在此过程中，我们发布了“LOBFrame”，一个开源代码库，可以高效处理大规模限价订单簿数据，并定量评估最先进的深度学习模型的预测能力。我们的结果是双重的。我们证明股票的微观结构特征影响深度学习方法的有效性，并且它们的高预测能力不一定对应可操作的交易信号。我们认为传统的机器学习指标未能充分评估限价订单簿环境中预测的质量。作为替代，我们提出了一个创新的操作框架，通过专注于准确预测的概率来评估预测的实用性。

    arXiv:2403.09267v1 Announce Type: cross  Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting com
    
[^159]: 机器学习优化的正交基分段多项式逼近

    Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation

    [https://arxiv.org/abs/2403.08579](https://arxiv.org/abs/2403.08579)

    本研究利用机器学习的方法对正交基分段多项式逼近进行优化。

    

    分段多项式（PPs）在几个工程学科中被使用，比如在轨迹规划中，用来逼近以一组点给出的位置轮廓。 鉴于逼近目标以及特定领域要求，比如Ck-连续性，可以被构造为一个方程组，结果可以直接计算，这样的闭式解对于多项式次数、多项式基础或者添加进一步的特定领域要求方面具有有限的灵活性。足够复杂的优化目标很快要求使用数值方法，比如梯度下降。由于梯度下降是训练人工神经网络（ANNs）的核心，现代机器学习（ML）框架比如TensorFlow提供了一套基于梯度的优化器，可能适用于训练任务之外的广泛优化问题。我们的方法是利用PP模式的多样性

    arXiv:2403.08579v1 Announce Type: new  Abstract: Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP mode
    
[^160]: 具有扩散净化的分离数据一致性的图像恢复

    Decoupled Data Consistency with Diffusion Purification for Image Restoration

    [https://arxiv.org/abs/2403.06054](https://arxiv.org/abs/2403.06054)

    通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。

    

    最近，扩散模型作为一种强大的深度生成先验类别已经引起了人们的关注，由于其出色地建模数据分布的能力，在各种图像恢复任务中表现出色。为了解决图像恢复问题，许多现有技术通过将额外的似然梯度步骤纳入到扩散模型的反向采样过程中来实现数据一致性。然而，这些额外的梯度步骤对于实际应用中存在挑战，因为它们造成了巨大的计算开销，从而增加了推理时间。当使用加速的扩散模型采样器时，这些额外的步骤还会导致额外的困难，因为数据一致性步骤的数量受限于反向采样步骤的数量。在这项工作中，我们提出了一种新颖的基于扩散的图像恢复求解器，通过将反向过程与数据一致性步骤分离来解决这些问题。我们的方法涉及

    arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
    
[^161]: 分布感知对数正定编码的算法硬件协同设计，用于高效的DNN推断

    Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference

    [https://arxiv.org/abs/2403.05465](https://arxiv.org/abs/2403.05465)

    引入了对数正定编码（LP）和LP量化（LPQ）框架，采用基因算法寻找最优的LP参数，设计了统一的混合精度LP加速器（LPA）体系结构，可动态适应DNN参数分布，减少量化和完整精度模型之间的表示性差异。

    

    传统的深度神经网络（DNN）量化方法使用整数、定点或浮点数据类型时，往往难以在低精度下捕捉不同的DNN参数分布，通常需要大量硅开销和密集的量化感知训练。在本研究中，我们引入了对数正定（LP）编码，这是一种受到正定启发的自适应、硬件友好的数据类型，通过参数化LP位域动态适应DNN权重/激活分布。我们还开发了一种基于遗传算法的新颖框架，LP量化（LPQ），用于寻找最优的逐层LP参数，同时通过一种新颖的全局-局部对比目标减少量化和完整精度模型之间的表示性差异。此外，我们设计了一个统一的混合精度LP加速器（LPA）体系结构，包括将LP纳入计算数据通路中的处理单元（PEs）。

    arXiv:2403.05465v1 Announce Type: cross  Abstract: Traditional Deep Neural Network (DNN) quantization methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive quantization-aware training. In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields. We also develop a novel genetic-algorithm based framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between quantized and full-precision models through a novel global-local contrastive objective. Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath. Our algorithm-hardware co-design
    
[^162]: NaturalSpeech 3: 利用分解编解码器和扩散模型实现零-shot语音合成

    NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models

    [https://arxiv.org/abs/2403.03100](https://arxiv.org/abs/2403.03100)

    NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音

    

    近期大规模文本到语音（TTS）模型取得了显著进展，然而在语音质量、相似度和韵律方面仍存在不足。鉴于语音复杂地包含各种属性（例如内容、韵律、音色和声学细节），给生成带来了重大挑战，一个自然的想法是将语音因子分解为代表不同属性的各个子空间，并单独生成它们。在此基础上，我们提出了NaturalSpeech 3，这是一个具有新颖的分解扩散模型的TTS系统，可以以零-shot方式生成自然语音。具体来说，1) 我们设计了一个具有分解向量量化（FVQ）的神经编解码器，将语音波形分解为内容、韵律、音色和声学细节的子空间；2) 我们提出了一个分解扩散模型，根据其相应的提示生成每个子空间中的属性。借助这种分解设计，NaturalSpeech 3能够ef

    arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
    
[^163]: 具有延迟更新的随机逼近：马尔科夫采样下的有限时间速率

    Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling

    [https://arxiv.org/abs/2402.11800](https://arxiv.org/abs/2402.11800)

    延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。

    

    受大规模和多智能体强化学习应用的启发，我们研究了在马尔科夫采样下具有延迟更新的随机逼近（SA）方案的非渐近性能。虽然延迟的影响在优化中得到了广泛研究，但它们与底层马尔科夫过程相互作用以塑造SA的有限时间性能的方式仍然不太清楚。在这个背景下，我们的第一个主要贡献是证明在时间变化有界延迟下，延迟的SA更新规则确保最后迭代收敛到SA运算符固定点周围的球体具有指数快速的速度。值得注意的是，我们的界限在依赖于最大延迟$\tau_{max}$和混合时间$\tau_{mix}$方面是\emph{紧致的}。为了实现这一紧密界限，我们开发了一种新颖的归纳证明技术，与各种现有延迟优化分析不同，它依赖于建立未...

    arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
    
[^164]: 攻击、防御和评估LLM对话安全性的调查

    Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey

    [https://arxiv.org/abs/2402.09283](https://arxiv.org/abs/2402.09283)

    这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。

    

    arXiv:2402.09283v1 公告类型: 新的摘要: 大型语言模型（LLMs）在对话应用中已经很常见。然而，它们可能被误用生成有害回复的风险引起了严重的社会关切，并激发了LLM对话安全性的最新研究。因此，在此调查中，我们提供了最近研究的全面概述，涵盖了LLM对话安全性的三个关键方面：攻击、防御和评估。我们的目标是提供一个结构化的摘要，增进对LLM对话安全性的理解，并鼓励进一步研究这一重要课题。为了方便参考，我们根据我们的分类法对所有在此调查中提到的研究进行了分类，可在以下网址找到：https://github.com/niconi19/LLM-conversation-safety。

    arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
    
[^165]: 动态系统中的实验设计的嵌套粒子滤波器

    Nesting Particle Filters for Experimental Design in Dynamical Systems

    [https://arxiv.org/abs/2402.07868](https://arxiv.org/abs/2402.07868)

    本文提出了一种新颖的方法来解决动态系统中的贝叶斯实验设计问题，利用嵌套粒子滤波器和立体蒙特卡洛方法来进行基于梯度的策略优化，相比于其他方法具有更好的性能。

    

    本文提出了一种新颖的贝叶斯实验设计方法，用于非交换数据，并将其形式化为风险敏感的策略优化。我们开发了内外SMC^2算法，使用嵌套顺序蒙特卡洛（SMC）估计器来预测期望的信息增益，并将其嵌入到粒子马尔可夫链蒙特卡洛（pMCMC）框架中进行基于梯度的策略优化。与最近依赖于偏估计器来摊销先前学习设计策略的成本的方法相比，我们的方法具有更好的性能。在一组动态系统的数值验证中展示了我们方法的有效性。

    In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.
    
[^166]: 基于基础模型的聚类优化主动学习初始化方法

    Foundation Model Makes Clustering a Better Initialization for Active Learning

    [https://arxiv.org/abs/2402.02561](https://arxiv.org/abs/2402.02561)

    本研究提出了一种基于基础模型和聚类方法的主动学习初始化方案，用于选择最具信息量的样本。基础模型是通过自监督训练在大规模数据集上训练得到的，并能生成适用于各种下游任务的紧凑嵌入表示。

    

    主动学习是从未标记的数据集中选择最具信息量的样本进行标注，以满足有限的标注预算。尽管已经有许多方法针对初始化模型后的样本选择进行了研究，但对于主动学习必不可少的初始化阶段，却没有引起足够的关注。先前的研究中大多数都采用随机抽样或者简单的聚类方法。然而，随机抽样容易产生波动，而简单聚类在处理高维数据（如图像数据）时收敛速度慢。在本研究中，我们提出将基础模型与聚类方法结合，用于选择主动学习初始化阶段的样本。基础模型是指在自监督范式下在大规模数据集上训练的模型，能够生成信息丰富且紧凑的嵌入表示，用于各种下游任务。

    Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget. While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization. Most of the previous studies resort to random sampling or naive clustering. However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data. In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization. Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks. Leveraging these embeddings to replace raw features such as p
    
[^167]: OpenMoE：开源混合专家语言模型的早期努力

    OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models

    [https://arxiv.org/abs/2402.01739](https://arxiv.org/abs/2402.01739)

    OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。

    

    为了帮助开源社区更好地理解基于混合专家(MoE)的大型语言模型(LLM)，我们训练并发布了OpenMoE，一系列完全开放源码和可复现的仅解码器MoE LLM，参数范围从650M到34B，训练数据超过1T个标记。我们的研究证实，MoE-based LLM可以提供比密集LLM更有利的成本效益平衡，突出了未来LLM开发的潜在有效性。本研究的另一个重要贡献是对我们的OpenMoE模型中的路由机制进行深入分析，得到了三个重要发现：上下文无关专业化、早期路由学习和末尾降低。我们发现，MoE模型中的路由决策主要基于标记ID，与上下文相关性很小。标记到专家的分配在预训练阶段早期确定，并且基本保持不变。这种不完全的路由可能导致...

    To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
    
[^168]: 简单策略优化

    Simple Policy Optimization

    [https://arxiv.org/abs/2401.16025](https://arxiv.org/abs/2401.16025)

    SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。

    

    PPO（Proximal Policy Optimization）算法在许多领域表现出色，被认为是TRPO（Trust Region Policy Optimization）算法的简化版本。然而，PPO中的比率剪切操作并不总是有效地强制执行信任区域约束，这可能会影响算法的稳定性。本文提出了一种新颖的剪切方法，即Simple Policy Optimization（SPO）算法，用于旧策略和当前策略之间的KL散度。在Atari 2600环境中进行的大量实验结果表明，与PPO的主流变体相比，SPO实现了更好的样本效率，极低的KL散度和更高的策略熵，并且对网络深度或复杂度的增加具有鲁棒性。更重要的是，SPO保持了无约束一阶算法的简单性。

    arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co
    
[^169]: 具有部分动态知识的样本高效强化学习

    Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge

    [https://arxiv.org/abs/2312.12558](https://arxiv.org/abs/2312.12558)

    研究了具有部分动态知识的在线Q学习的样本复杂度，并提出了一种乐观的Q学习算法，在有限的分集马尔可夫决策过程设置下，实现了较低的遗憾。

    

    在本文中，我们研究了在线Q学习方法的样本复杂度，当某些关于动态的先前知识可用或可以有效学习时。我们专注于按照加性干扰模型演变的系统，在有限的分集马尔可夫决策过程设置下，我们提出了一种乐观的Q学习算法，在对$f$的完美知识条件下实现了$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$的遗憾，其中$T$是与系统进行交互的总次数。

    arXiv:2312.12558v2 Announce Type: replace  Abstract: The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$ represents the underlying system dynamics, and $W_h$ are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with $S$ states, $A$ actions, and episode length $H$, we present an optimistic Q-learning algorithm that achieves $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of $f$, where $T$ is the total number of interactions with
    
[^170]: 神经网络控制系统的构成归纳不变式验证

    Compositional Inductive Invariant Based Verification of Neural Network Controlled Systems

    [https://arxiv.org/abs/2312.10842](https://arxiv.org/abs/2312.10842)

    本文提出了一种新方法，使用构成方法将神经网络控制系统的安全验证问题拆分为较小、更易处理的子问题，并引入一种能够自动验证候选归纳不变式的算法。

    

    近年来，将神经网络集成到安全关键系统中展示了巨大的潜力。然而，有效验证神经网络控制系统（NNCS）的安全性仍然具有挑战性。本文介绍了一种基于归纳不变式方法的NNCS安全验证新方法。在神经网络规模大、非线性的情况下，验证候选归纳不变式的归纳性是困难的。我们的构成方法通过将归纳性证明义务分解成更小、更易处理的子问题，使这一验证过程变得可管理。除了高层方法外，我们还提出了一种能够通过自动推断必要的分解谓词自动验证给定候选归纳不变式的算法。该算法明显优于基准方法，并在执行时间上显示出明显的缩短。

    arXiv:2312.10842v2 Announce Type: replace-cross  Abstract: The integration of neural networks into safety-critical systems has shown great potential in recent years. However, the challenge of effectively verifying the safety of Neural Network Controlled Systems (NNCS) persists. This paper introduces a novel approach to NNCS safety verification, leveraging the inductive invariant method. Verifying the inductiveness of a candidate inductive invariant in the context of NNCS is hard because of the scale and nonlinearity of neural networks. Our compositional method makes this verification process manageable by decomposing the inductiveness proof obligation into smaller, more tractable subproblems. Alongside the high-level method, we present an algorithm capable of automatically verifying the inductiveness of given candidates by automatically inferring the necessary decomposition predicates. The algorithm significantly outperforms the baseline method and shows remarkable reductions in execut
    
[^171]: 无需动作的行为学习

    Learning to Act without Actions

    [https://arxiv.org/abs/2312.10812](https://arxiv.org/abs/2312.10812)

    通过从视频中恢复潜在动作信息，LAPO能够训练可以迅速微调为专家级策略的潜在动作策略。

    

    在大规模网络数据上进行预训练已被证明是获取强大通用模型的有效方法，例如在语言和视觉领域。但是，这种范式尚未在强化学习中得以推广。我们介绍了Latent Action Policies（LAPO），这是一种从视频中纯粹恢复潜在动作信息的方法，从而产生潜在动作策略、世界模型和逆动力学模型。LAPO是第一个能够仅通过观察到的动态从视频中恢复真实动作空间结构的方法，即使在具有挑战性的过程生成环境中也是如此。

    arXiv:2312.10812v2 Announce Type: replace-cross  Abstract: Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of embodied behavioral data on the web, lack the action labels required by existing methods for imitating behavior from demonstrations. We introduce Latent Action Policies (LAPO), a method for recovering latent action information, and thereby latent-action policies, world models, and inverse dynamics models, purely from videos. LAPO is the first method able to recover the structure of the true action space just from observed dynamics, even in challenging procedurally-generated environments. LAPO enables training latent-action policies that can be rapidly fine-tuned into expert-level policies, either offline using a small action-labeled datas
    
[^172]: 通过策略引导的轨迹扩散实现世界模型

    World Models via Policy-Guided Trajectory Diffusion

    [https://arxiv.org/abs/2312.08533](https://arxiv.org/abs/2312.08533)

    这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。

    

    世界模型是开发智能agent的强大工具。通过预测一系列行动的结果，世界模型使得可以通过在“想象中”使用合成数据来优化策略，即通过在线策略增强学习（RL）来实现。现有的世界模型是自回归的，因为它们在预测下一个状态的同时从策略中采样下一个行动。随着轨迹长度的增长，预测误差必然会累积。在这项工作中，我们提出了一种新颖的世界建模方法，不是自回归的，而是通过扩散模型一次生成整个在线策略轨迹。我们的方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，利用了除了策略的动作分布梯度之外的一个去噪模型，将最初随机状态和动作的轨迹扩散成一个在线合成轨迹。我们分析了PolyGRAD与

    arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,
    
[^173]: 基于批处理的基础模型低秩调整

    Batched Low-Rank Adaptation of Foundation Models

    [https://arxiv.org/abs/2312.05677](https://arxiv.org/abs/2312.05677)

    提出了Fast LoRA（FLoRA）框架，使得基础模型的低秩调整可以高效批处理异构请求，并在绩效上保持竞争性。

    

    最近，低秩适应（LoRA）因通过引入可训练的低秩矩阵微调基础模型并减少可训练参数的数量而引起关注。虽然LoRA提供了许多优点，但其在实时为各种全球用户提供服务时无法高效处理多个特定任务适配器的能力受到限制。这为需要为每个传入请求个性化、特定任务适应的场景中造成了性能瓶颈。为了减轻这一约束，我们提出了快速LoRA（FLoRA）框架，其中批处理中的每个输入示例都可以与其独特的低秩适应权重相关联，从而实现对异构请求的高效批处理。我们通过实证表明，FLoRA保留了LoRA的绩效优点，在跨越8种语言的MultiPL-E代码生成基准测试上展示出竞争结果。

    arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
    
[^174]: CAFE：面向大规模推荐模型的紧凑、自适应和快速嵌入

    CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models

    [https://arxiv.org/abs/2312.03256](https://arxiv.org/abs/2312.03256)

    CAFE提出了一种紧凑、自适应和快速的嵌入压缩框架，通过动态分配内存资源给重要特征并引入HotSketch数据结构实时捕获热门特征，解决了嵌入表在大规模推荐模型中内存需求增长的挑战。

    

    最近，深度学习推荐模型（DLRM）中嵌入表的不断增长的内存需求给模型训练和部署带来了巨大挑战。现有的嵌入压缩解决方案无法同时满足内存效率、低延迟和适应动态数据分布等三个关键设计要求。本文提出了CAFE，一种紧凑、自适应和快速的嵌入压缩框架，以解决上述要求。CAFE的设计理念是动态分配更多内存资源给重要特征（称为热门特征），并为不重要的特征分配更少内存。在CAFE中，我们提出了一种快速轻量级的草图数据结构，命名为HotSketch，用于捕获特征重要性并实时报告热门特征。对于每个报告的热门特征，我们为其分配唯一的嵌入。对于非热门特征，我们允许多个特征共享一个嵌入，使用哈希embedd。

    arXiv:2312.03256v2 Announce Type: replace  Abstract: Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive, and Fast Embedding compression framework that addresses the above requirements. The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones. In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time. For each reported hot feature, we assign it a unique embedding. For the non-hot features, we allow multiple features to share one embedding by using hash embedd
    
[^175]: 高效基于Transformer的3D人体姿势估计的Hourglass Tokenizer

    Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation

    [https://arxiv.org/abs/2311.12028](https://arxiv.org/abs/2311.12028)

    提出了一种名为Hourglass Tokenizer（HoT）的修剪和恢复框架，用于从视频中高效地基于Transformer进行3D人体姿势估计，通过动态选择具有高语义多样性的代表性标记并消除视频帧的冗余，最终提高了模型的效率。

    

    Transformers已经成功应用于基于视频的3D人体姿势估计领域。然而，这些视频姿势Transformer（VPTs）的高计算成本使得它们在资源受限设备上不切实际。在本文中，我们提出了一种名为Hourglass Tokenizer（HoT）的即插即用的修剪和恢复框架，用于从视频中高效地基于Transformer进行3D人体姿势估计。我们的HoT首先通过修剪冗余帧的姿势标记开始，然后以恢复全长度标记结束，从而在中间的Transformer块中产生少量姿势标记，从而提高模型的效率。为了有效实现这一目标，我们提出了一个标记修剪集群（TPC），动态选择一些具有高语义多样性的代表性标记，同时消除视频帧的冗余。此外，我们开发了一个标记恢复注意力（TRA）来恢复详细的时空信息。

    arXiv:2311.12028v2 Announce Type: replace-cross  Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information b
    
[^176]: 在多任务学习中挑战常见范式

    Challenging Common Paradigms in Multi-Task Learning

    [https://arxiv.org/abs/2311.04698](https://arxiv.org/abs/2311.04698)

    我们挑战了多任务学习中的常见范式，通过研究在单任务学习中的影响，揭示了优化器选择在MTL中的关键作用，并理论推导出了梯度冲突的角色。

    

    尽管近年来多任务学习（MTL）受到了极大关注，但其基本机制仍然知之甚少。最近的方法并未带来一致的性能改进，相比单任务学习（STL）基线，强调了更深入了解MTL特定挑战的重要性。在我们的研究中，我们挑战了MTL中的范式，提出了几点关于STL的重要影响：首先，优化器的选择对MTL的影响只受到了轻微的调查。我们通过各种实验的实证方法展示了常见STL工具（例如Adam优化器）在MTL中的关键作用。为了进一步研究Adam的有效性，我们在一定的假设下从理论上推导出部分损失尺度不变性。其次，梯度冲突的概念经常被描述为MTL中的一个特定问题。我们深入探讨了梯度冲突在MTL中的作用，并将其与STL进行比较。在角度梯度对齐方面，我们没有找到

    arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
    
[^177]: 在点估计的判别式神经网络中防止远处数据的任意高置信度

    Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks

    [https://arxiv.org/abs/2311.03683](https://arxiv.org/abs/2311.03683)

    通过在神经网络输出中添加额外类别的对数几率项，设计使其在远离训练数据时支配原始类别的对数几率，从而防止在远处测试数据上出现任意高的置信度。

    

    判别式训练、确定性神经网络是解决分类问题的事实上的选择。然而，尽管它们在域内测试集上取得了最先进的结果，但它们往往在域外（OOD）数据上过于自信。我们通过向神经网络的输出添加一个项，该项对应于额外类别的对数几率，设计为当我们远离训练数据时支配原始类别的对数几率。这种技术可证明防止远处测试数据的任意高置信度，同时保持简单的判别点估计训练。在各种基准测试上的评估表明，该方法表现出强大的性能。

    arXiv:2311.03683v2 Announce Type: replace  Abstract: Discriminatively trained, deterministic neural networks are the de facto choice for classification problems. However, even though they achieve state-of-the-art results on in-domain test sets, they tend to be overconfident on out-of-distribution (OOD) data. For instance, ReLU networks - a popular class of neural network architectures - have been shown to almost always yield high confidence predictions when the test data are far away from the training set, even when they are trained with OOD data. We overcome this problem by adding a term to the output of the neural network that corresponds to the logit of an extra class, that we design to dominate the logits of the original classes as we move away from the training data.This technique provably prevents arbitrarily high confidence on far-away test data while maintaining a simple discriminative point-estimate training. Evaluation on various benchmarks demonstrates strong performance aga
    
[^178]: 在线到离线强化学习中的超领域规划

    Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning

    [https://arxiv.org/abs/2310.05723](https://arxiv.org/abs/2310.05723)

    将在线到离线强化学习框定为一个探索问题，并研究了内在奖励和UCB在该设置中的效果。

    

    离线预训练配合静态数据集，然后在线微调（离线到在线，即OtO）是一个与现实世界RL部署过程很匹配的范式。在这种情况下，我们的目标是在有限的在线交互预算内找到性能最佳的策略。以前在OtO设置中的工作集中在纠正由离线RL算法的策略约束机制引入的偏差上。这些约束使得学习的策略接近收集数据集的行为策略，但我们表明，如果行为策略远非最优，则这可能会不必要地限制策略性能。相反，我们放弃约束，把OtO RL作为一个探索问题来框定，旨在最大化在线数据采集的好处。我们首先研究了基于内在奖励和UCB的主要在线RL探索方法在OtO设置中的效果，显示内在奖励通过奖励函数的修改增加了训练的不稳定性。

    arXiv:2310.05723v2 Announce Type: replace  Abstract: Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm well matched to a real-world RL deployment process. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but we show this can unnecessarily limit policy performance if the behavior policy is far from optimal. Instead, we forgo constraints and frame OtO RL as an exploration problem that aims to maximize the benefit of online data-collection. We first study the major online RL exploration methods based on intrinsic rewards and UCB in the OtO setting, showing that intrinsic rewards add training instability through reward-function modif
    
[^179]: 联邦学习中的近似和加权数据重构攻击

    Approximate and Weighted Data Reconstruction Attack in Federated Learning

    [https://arxiv.org/abs/2308.06822](https://arxiv.org/abs/2308.06822)

    提出了一种基于插值的近似方法和层次加权损失函数，用于攻击FedAvg场景中的数据重构攻击。

    

    联邦学习（FL）是一种分布式学习范例，使多个客户端能够在不共享私人数据的情况下合作构建机器学习模型。虽然FL被认为是通过设计保护隐私的，但最近的数据重构攻击表明，攻击者可以基于在FL中共享的参数恢复客户端的训练数据。然而，大多数现有方法未能攻击最广泛使用的水平联邦平均（FedAvg）场景，在此场景中，客户端在多个局部训练步骤之后共享模型参数。为了解决这个问题，我们提出了一种基于插值的近似方法，通过生成客户端局部训练过程的中间模型更新，使攻击FedAvg场景变得可行。然后，我们设计了一种层次加权损失函数来改善重构的数据质量。我们为不同层次的模型更新分配不同的权重

    arXiv:2308.06822v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers conc
    
[^180]: 新兴趋势：从模型融合到联邦 X 学习的研究

    Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning

    [https://arxiv.org/abs/2102.12920](https://arxiv.org/abs/2102.12920)

    联邦学习作为一种新的学习范式，不仅有助于改进算法和模型融合方法，还展示了与其他学习框架相结合的潜力，特别是在联邦 X 学习中，为多任务学习、元学习、迁移学习等提供了新的研究视角。

    

    联邦学习是一种新的学习范式，通过多方计算和模型聚合来解耦数据收集与模型训练。作为一种灵活的学习设置，联邦学习有潜力与其他学习框架整合。本文针对联邦学习与其他学习算法的结合进行了重点调查。具体来说，我们探索了各种学习算法以改进原始的联邦平均算法，并回顾了模型融合方法，例如自适应聚合、正则化、聚类方法和贝叶斯方法。根据新兴趋势，我们还讨论了联邦学习与其他学习范式的交集，称为联邦 X 学习，其中 X 包括多任务学习、元学习、迁移学习、无监督学习和强化学习。这项调查回顾了现状、挑战和未来方向。

    arXiv:2102.12920v4 Announce Type: replace  Abstract: Federated learning is a new learning paradigm that decouples data collection and model training via multi-party computation and model aggregation. As a flexible learning setting, federated learning has the potential to integrate with other learning frameworks. We conduct a focused survey of federated learning in conjunction with other learning algorithms. Specifically, we explore various learning algorithms to improve the vanilla federated averaging algorithm and review model fusion methods such as adaptive aggregation, regularization, clustered methods, and Bayesian methods. Following the emerging trends, we also discuss federated learning in the intersection with other learning paradigms, termed federated X learning, where X includes multitask learning, meta-learning, transfer learning, unsupervised learning, and reinforcement learning. This survey reviews the state of the art, challenges, and future directions.
    
[^181]: CharNet：高复杂性字符分类的广义方法

    CharNet: Generalized Approach for High-Complexity Character Classification. (arXiv:2401.17098v1 [cs.CV])

    [http://arxiv.org/abs/2401.17098](http://arxiv.org/abs/2401.17098)

    这是一篇关于手写字符识别问题的论文，提出了一个广义的方法来解决高复杂性字符分类的挑战。

    

    对于机器学习研究人员来说，手写字符识别（HCR）是一个具有挑战性的问题。与打印文本数据不同，手写字符数据集由于人类导致的偏差而具有更多的变化。基于独特字符类的数据，例如象形文字或汉字韩字字符序列，给HCR问题带来新的复杂性。在这种数据集上的分类任务要求模型学习具有相似特征的图像的高复杂性细节。随着计算资源的可用性的最新进展以及进一步的计算机视觉理论的发展，一些研究团队已经有效地解决了出现的挑战。尽管以高效率而闻名，但许多常见的方法仍然不具有通用性，并使用数据集特定的解决方案来取得更好的结果。由于复杂的结构和高计算需求，现有方法经常阻止解决方案流行起来。本文提出了一种方法。

    Handwritten character recognition (HCR) is a challenging problem for machine learning researchers. Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias. With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem. The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features. With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges. Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results. Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity. This paper proposes a str
    
[^182]: 比较以人为中心的语言建模：模拟群体、个体特点还是两者兼顾？

    Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])

    [http://arxiv.org/abs/2401.12492](http://arxiv.org/abs/2401.12492)

    本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。

    

    自然语言处理在将人的上下文纳入其模型中取得了进展，但使用群体属性（如45岁以上的人群）还是模拟个体人物更有效的问题尚未确定。群体属性在技术上更容易实现，但是过于粗糙：并非所有45岁以上的人都以相同的方式书写。相反，模拟个体人物能够捕捉每个人身份的复杂性，允许更个性化的表示，但我们可能需要模拟无限数量的用户并且需要可能无法获取的数据。我们比较了通过群体属性、个体用户和组合方法来模拟人的上下文。将群体和个体特征结合起来，显著提高了基于用户文档的用户级回归任务（如年龄估计或个性评估）的性能。模拟个体用户显著提高了单个文档级分类任务（如立场和主题检测）的性能。

    Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
    
[^183]: 输入凸性Lipschitz RNN: 一种用于工程任务的快速和鲁棒的方法

    Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks. (arXiv:2401.07494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.07494](http://arxiv.org/abs/2401.07494)

    通过结合输入凸性和Lipschitz连续性的优势，我们开发了一种名为输入凸性Lipschitz循环神经网络的新型网络结构，在计算效率和对抗鲁棒性方面优于现有的循环单元，并适用于多种工程任务。

    

    计算效率和对抗鲁棒性是真实世界工程应用中的关键因素。然而，传统的神经网络往往在同时或分别解决这两个问题方面存在不足。通过从自然物理系统和现有文献中获取的见解，已知输入凸性结构增强了计算效率，而Lipschitz约束结构增强了对抗鲁棒性。通过利用凸性和Lipschitz连续性的优点，我们开发了一种新的网络结构，称为输入凸性Lipschitz循环神经网络。该模型在计算效率和对抗鲁棒性方面表现优于现有的循环单元，适用于一系列工程任务，包括基准MNIST图像分类、新加坡LHT Holdings公司的实际太阳能光伏系统规划中的实时太阳辐射预测，以及化学反应器的实时模型预测控制优化等。

    Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Networks. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor.
    
[^184]: 使用样式表示进行机器生成文本的小样本检测

    Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])

    [http://arxiv.org/abs/2401.06712](http://arxiv.org/abs/2401.06712)

    本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。

    

    受到指导调整的语言模型的出现使得人类写作的逼真模仿面临着重大滥用风险。然而，我们可以通过检测一段文本是由语言模型还是人类撰写而成来对抗此类滥用行为。本文提出了一种基于样式表示的小样本检测方法，避免了神经网络检测器在面对数据转换时的规约不足的挑战，同时也避免了在推理或检测时需要访问可能生成文档的模型的问题。

    The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
    
[^185]: FedSN：一个适用于LEO卫星网络的通用联邦学习框架

    FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])

    [http://arxiv.org/abs/2311.01483](http://arxiv.org/abs/2311.01483)

    FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。

    

    最近，许多低地球轨道（LEO）卫星已经由商业公司成功地发射和部署到太空中，如SpaceX。由于LEO卫星配备了多模传感器，它们不仅用于通信，还用于各种机器学习应用，如空间调制识别、遥感图像分类等。然而，由于与LEO卫星的有限接触时间（例如5分钟），地面站（GS）可能无法下载如此大量的原始感测数据进行集中模型训练。因此，联邦学习（FL）已经成为解决这个问题的有希望的解决方案，通过在设备上进行训练。不幸的是，要在LEO卫星上使用FL，我们仍然面临三个关键挑战，即i）异构计算和存储能力，ii）有限的上行速率，以及iii）模型陈旧问题。为此，我们提出了一种名为FedSN的通用FL框架来解决上述挑战，一

    Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
    
[^186]: VIGraph：自我监督学习用于类别不平衡节点分类

    VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])

    [http://arxiv.org/abs/2311.01191](http://arxiv.org/abs/2311.01191)

    VIGraph是一个基于自我监督学习的模型，通过利用自编码器生成少数类节点来解决图数据中的类别不平衡问题，并通过引入孪生对比策略提高生成节点的质量。

    

    图数据中的类别不平衡为节点分类带来了重大挑战。现有方法，如基于SMOTE的方法，在不平衡场景构建过程中仍存在局限性。自我监督学习（SSL）通过从数据中合成少数类节点提供了一个有前景的解决方案，然而其潜力尚未被探索。本文分析了基于SMOTE的方法的限制，并引入了VIGraph，这是一种基于自我监督变分图自编码器（VGAE）的新型SSL模型，利用变分推断（VI）生成少数类节点。具体而言，VIGraph在构建不平衡图时严格遵循不平衡的概念，并利用生成型VGAE生成少数类节点。此外，VIGraph在解码阶段引入了一种新颖的孪生对比策略，以提高生成节点的整体质量。VIGraph能够生成高质量的节点，无需重新集成。

    Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrat
    
[^187]: 等距运动流形基元

    Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])

    [http://arxiv.org/abs/2310.17072](http://arxiv.org/abs/2310.17072)

    Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.

    

    运动流形基元（MMP）为给定任务生成一系列连续轨迹流形，每一个轨迹流形都能成功完成任务。它由对流形进行参数化的解码函数以及潜在坐标空间中的概率密度组成。本文首先展示了由于潜在空间中的几何扭曲，MMP的性能可能会显著降低--通过变形，我们指的是相似的运动在潜在空间中无法相邻。然后，我们提出了等距运动流形基元（IMMP），其潜在坐标空间保持了流形的几何结构。为此，我们建立和使用了一个Riemannian度量，用于运动空间（即，参数化曲线空间），我们称之为CurveGeom Riemannian度量。对于平面障碍避让运动和推动操纵任务的实验表明，IMMP明显优于现有的MMP方法。代码可在https://github.com/Gabe-YHLee/IMMP找到。

    The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
    
[^188]: 双边贸易中基于全局预算平衡的无悔学习

    No-Regret Learning in Bilateral Trade via Global Budget Balance. (arXiv:2310.12370v1 [cs.GT])

    [http://arxiv.org/abs/2310.12370](http://arxiv.org/abs/2310.12370)

    本文引入全局预算平衡的概念，提出了首个针对具有对抗输入的双边贸易的无悔算法，并在不同的反馈模型下进行了验证。

    

    双边贸易涉及在两个战略代理人之间促进交易的挑战，一个是卖家，一个是买家，两者都对物品有私人估值。我们研究了该问题的在线版本，在每个时间步长，都会出现一个新的卖家和买家。学习者的任务是在不了解他们估值的情况下为每个代理人设置价格。卖家和买家的序列由一个遗忘性对手选择。在这个设置下，已知的负面结果排除了在每次迭代中学习者必须保证预算平衡的可能性。在本文中，我们引入了全局预算平衡的概念，该概念仅要求代理人在整个时间范围内保持预算平衡。通过要求全局预算平衡，我们提供了首个针对具有对抗输入的双边贸易的无悔算法，并在不同的反馈模型下进行了验证。首先，我们证明在全反馈模型中，学习者可以实现最小遗憾。

    Bilateral trade revolves around the challenge of facilitating transactions between two strategic agents -- a seller and a buyer -- both of whom have a private valuations for the item. We study the online version of the problem, in which at each time step a new seller and buyer arrive. The learner's task is to set a price for each agent, without any knowledge about their valuations. The sequence of sellers and buyers is chosen by an oblivious adversary. In this setting, known negative results rule out the possibility of designing algorithms with sublinear regret when the learner has to guarantee budget balance for each iteration. In this paper, we introduce the notion of global budget balance, which requires the agent to be budget balance only over the entire time horizon. By requiring global budget balance, we provide the first no-regret algorithms for bilateral trade with adversarial inputs under various feedback models. First, we show that in the full-feedback model the learner can g
    
[^189]: 学习基于概念的视觉因果转换和符号推理用于视觉规划

    Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])

    [http://arxiv.org/abs/2310.03325](http://arxiv.org/abs/2310.03325)

    本文提出了一种面向视觉规划的可解释和可推广的框架，通过将视觉输入转化为概念表示、符号抽象和推理以及将视觉因果转换与真实世界行为关联，实现了目标条件的视觉规划。

    

    视觉规划模拟了人类在搜索初始视觉状态和最终视觉目标状态之间的视觉因果转换来实现期望目标时所做的决策过程。在以自我为中心的视觉中，视觉规划越来越重要，因为它在引导智能体在复杂环境中执行日常任务方面具有优势。本文提出了一个可解释和可推广的视觉规划框架，包括：i）一种新颖的基于替代的概念学习器（SCL），将视觉输入转化为分解的概念表示；ii）通过自学符号进行任务规划的符号抽象和推理；iii）将视觉因果转换与语义相似的真实世界行为进行关联的视觉因果转换模型（ViCT）。给定一个初始状态，我们通过学习到的表示和因果转换的符号推理方法进行目标条件的视觉规划，以达到目标状态。

    Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiv
    
[^190]: ABScribe: 使用大型语言模型在人工智能与人类共同写作任务中快速探索多种写作变化

    ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])

    [http://arxiv.org/abs/2310.00117](http://arxiv.org/abs/2310.00117)

    ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。

    

    通过重新书写文本来探索替代想法是写作过程的关键。最先进的大型语言模型（LLM）可以简化写作变化生成的过程。然而，当前的界面存在同时考虑多种变化的挑战：在不覆盖文本的情况下创建新的版本可能很困难，而按顺序粘贴它们可能会使文档变得杂乱，增加工作量，并打断作者的流程。为了解决这个问题，我们提出了ABScribe，一种支持在人工智能与人类共同写作任务中快速且结构化地探索写作变化的界面。通过ABScribe，用户可以使用LLM提示快速产生多个变体，这些变体会自动转换成可重用的按钮形式。变体在文本段落中被存储在相邻位置，通过在上下文工具栏上的鼠标悬停交互进行快速的就地比较。我们对12名撰写人员进行的用户研究表明，ABScribe能显著减轻任务负荷（d = 1.20, p < 0.001），提高用户的认知程度。

    Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions o
    
[^191]: 图中社区检测的综合评述

    A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])

    [http://arxiv.org/abs/2309.11798](http://arxiv.org/abs/2309.11798)

    本综述对图中的社区检测进行了全面回顾。社区结构是真实世界图的重要特征，社区检测方法的研究具有社会学、生物学和计算机科学方面的应用。尽管科学家们做出了努力，但尚未找到一个令人满意的解决方案。本综述介绍了社区结构的概念，各种社区检测方法，以及在各种网络中的实际应用。

    

    复杂网络研究显著促进了我们对真实世界图的社区结构的理解，这是一个具有挑战性的问题，在社会学、生物学和计算机科学中具有应用价值。尽管跨学科科学家社区的努力，但尚未找到一个令人满意的解决方案。本综述详细介绍了图中社区检测的主题，这对于理解复杂系统的组织和功能起着关键的作用。首先，我们介绍社区结构的概念，它指的是将顶点划分为具有强内部连接和较弱连接的集群。然后，我们对各种社区检测方法进行了彻底的阐述，包括我们设计的一种新方法。此外，我们还探讨了社区检测在各种网络中的真实应用。

    The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
    
[^192]: 半导体制造业中无监督故障检测的时间序列数据生成预训练

    Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])

    [http://arxiv.org/abs/2309.11427](http://arxiv.org/abs/2309.11427)

    本研究提出了TRACE-GPT模型，用于半导体制造业中无监督故障检测。通过使用时间卷积嵌入和生成式预训练Transformer来预训练时间序列数据，并使用交叉熵损失函数进行异常序列和正常序列的分类，实验结果表明模型具有更好的性能。

    

    本文介绍了TRACE-GPT（Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers），它是用于半导体制造中的未标记数据集上预训练单变量时间序列传感器数据并检测故障的模型。研究表明，通过使用时间卷积嵌入和生成式预训练Transformer（GPT）来提取时间序列数据的特征，并使用交叉熵损失函数对异常序列和正常序列进行分类，我们的模型表现出更好的性能。

    This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Ch
    
[^193]: A2V: 一种半监督领域自适应框架用于不同图像模态的脑血管分割，通过二阶段训练从血管造影到静脉造影的转换

    A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel Segmentation via Two-Phase Training Angiography-to-Venography Translation. (arXiv:2309.06075v1 [eess.IV])

    [http://arxiv.org/abs/2309.06075](http://arxiv.org/abs/2309.06075)

    A2V是一种半监督领域自适应框架，用于通过图像到图像转换实现脑血管的跨模态分割。它通过离散化和语义丰富的潜在空间实现源域到目标域的图像级自适应，并提高了计算效率和训练稳定性。

    

    我们提出了一种半监督领域自适应框架，用于从不同图像模态中分割脑血管。现有的最先进方法集中于单一模态，忽视了广泛可用的脑血管成像技术。这可能导致显著的分布变化，对跨模态的泛化产生负面影响。通过依赖注释的血管造影和有限数量的注释的静脉造影，我们的框架实现图像到图像的转换和语义分割，利用离散化和语义丰富的潜在空间来表示异构数据，并进行源域到目标域的图像级自适应。此外，我们减少了基于循环的架构的典型复杂性，最小化了对抗性训练的使用，这使我们能够构建一个稳定训练的高效直观模型。我们在磁共振血管造影和静脉造影上评估了我们的方法。在实现最先进水平的同时，我们减少了所提出方法的计算复杂度和模型稳定性的改进。

    We present a semi-supervised domain adaptation framework for brain vessel segmentation from different image modalities. Existing state-of-the-art methods focus on a single modality, despite the wide range of available cerebrovascular imaging techniques. This can lead to significant distribution shifts that negatively impact the generalization across modalities. By relying on annotated angiographies and a limited number of annotated venographies, our framework accomplishes image-to-image translation and semantic segmentation, leveraging a disentangled and semantically rich latent space to represent heterogeneous data and perform image-level adaptation from source to target domains. Moreover, we reduce the typical complexity of cycle-based architectures and minimize the use of adversarial training, which allows us to build an efficient and intuitive model with stable training. We evaluate our method on magnetic resonance angiographies and venographies. While achieving state-of-the-art pe
    
[^194]: 一般化界限：信息论和PAC-Bayesian的视角

    Generalization Bounds: Perspectives from Information Theory and PAC-Bayes. (arXiv:2309.04381v1 [cs.LG])

    [http://arxiv.org/abs/2309.04381](http://arxiv.org/abs/2309.04381)

    该论文介绍了一般化界限的两个视角：信息论和PAC-Bayesian，并探讨了它们之间的联系和共同点。这对于理论机器学习的进一步发展和新算法的设计具有重要意义。

    

    在理论机器学习中，一个基本问题是一般化。在过去的几十年里，PAC-Bayesian方法已经被确定为一个灵活的框架，用来解决机器学习算法的一般化能力，并设计新的算法。最近，由于其对多种学习算法（包括深度神经网络）的潜在适用性，它引起了越来越多的关注。与此同时，还发展了一种信息论的视角，其中建立了一般化与各种信息度量之间的关系。这个框架与PAC-Bayesian方法密切相关，并且在两个方面都有独立发现的很多结果。在本文中，我们强调这种强连接，并提出一种统一的一般化处理方法。我们介绍了两个视角共同拥有的技术和结果，并讨论了不同的方法和解释。特别是，我们展示了这种连接如何产生新的洞见和理论的发展，并展示了这两个领域的交叉应用和潜在的进一步研究方向。

    A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demons
    
[^195]: LCANets++: 使用多层神经网络和层间竞争的鲁棒性音频分类

    LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition. (arXiv:2308.12882v1 [cs.SD])

    [http://arxiv.org/abs/2308.12882](http://arxiv.org/abs/2308.12882)

    LCANets++是一种使用多层神经网络和层间竞争的鲁棒性音频分类方法，通过稀疏编码来提高对扰动和对抗攻击的抵抗力。

    

    音频分类旨在识别音频信号，包括语音命令或声音事件。然而，当前的音频分类器容易受到扰动和对抗攻击的影响。此外，现实世界中的音频分类任务通常受到有限的标记数据的限制。为了填补这些差距，先前的工作在计算机视觉中通过局部竞争算法（LCA）在第一层使用了神经启发式卷积神经网络（CNNs）进行稀疏编码，即LCANets。LCANets通过监督和无监督学习的组合来学习，减少对标记样本的依赖性。受到听觉皮层也是稀疏的事实的启发，我们将LCANets扩展到音频识别任务，并引入了LCANets++，它们是CNNs，通过LCA在多层次上进行稀疏编码。我们证明LCANets++对于扰动（如背景噪声）以及黑盒和白盒攻击（如逃避和破坏）比标准CNN和LCANets更加鲁棒。

    Audio classification aims at recognizing audio signals, including speech commands or sound events. However, current audio classifiers are susceptible to perturbations and adversarial attacks. In addition, real-world audio classification tasks often suffer from limited labeled data. To help bridge these gaps, previous work developed neuro-inspired convolutional neural networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA) in the first layer (i.e., LCANets) for computer vision. LCANets learn in a combination of supervised and unsupervised learning, reducing dependency on labeled samples. Motivated by the fact that auditory cortex is also sparse, we extend LCANets to audio recognition tasks and introduce LCANets++, which are CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that LCANets++ are more robust than standard CNNs and LCANets against perturbations, e.g., background noise, as well as black-box and white-box attacks, e.g., evasion an
    
[^196]: HOOD: 实时稳健的低成本FMCW雷达人员存在和离群检测

    HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar. (arXiv:2308.02396v1 [eess.SP])

    [http://arxiv.org/abs/2308.02396](http://arxiv.org/abs/2308.02396)

    本文提出了一种名为HOOD的实时稳健的人员存在和离群检测方法，通过利用低成本的60 GHz FMCW雷达实现。该方法可同时解决存在检测和离群检测问题，通过重构架构和雷达图像实现准确检测人类存在，同时在人类不存在时检测移动或静止干扰物。

    

    在室内环境中使用毫米波频率调制连续波（FMCW）雷达进行人员存在检测是具有挑战性的，主要是因为室内空间中存在移动和静止的杂波。本文提出了一种名为“HOOD”的实时稳健的人员存在和离群检测方法，通过利用60 GHz短距离FMCW雷达。我们将存在检测应用视为离群检测问题，并使用单一流程同时解决这两个问题。我们的解决方案依赖于基于重构的架构，并使用雷达宏观和微观范围-Doppler图像（RDI）。HOOD旨在在存在或不存在移动和静止干扰物的情况下准确检测人类的“存在”。由于它也是一个离群检测器，它旨在将移动或静止杂波作为人类不存在的离群检测，并将当前场景的输出预测为“无人存在”。HOOD是一种无需任何活动的方法，适用于不同的人类场景。

    Human presence detection in indoor environments using millimeter-wave frequency-modulated continuous-wave (FMCW) radar is challenging due to the presence of moving and stationary clutters in indoor places. This work proposes "HOOD" as a real-time robust human presence and out-of-distribution (OOD) detection method by exploiting 60 GHz short-range FMCW radar. We approach the presence detection application as an OOD detection problem and solve the two problems simultaneously using a single pipeline. Our solution relies on a reconstruction-based architecture and works with radar macro and micro range-Doppler images (RDIs). HOOD aims to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On 
    
[^197]: 高维分布式梯度下降算法在任意数量拜占庭攻击者下的研究

    High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers. (arXiv:2307.13352v1 [cs.LG])

    [http://arxiv.org/abs/2307.13352](http://arxiv.org/abs/2307.13352)

    本文提出了一种适用于高维问题、在任意数量拜占庭攻击者下的新方法，核心是一种直接的高维半验证均值估计方法，具有极小极值统计率。

    

    近年来，具有拜占庭故障的强鲁棒分布式学习引起了广泛关注。然而，现有方法大多受到维度诅咒的限制，随着现代机器学习模型复杂性的增加，这个问题变得越来越严重。在本文中，我们设计了一种适用于高维问题、在任意数量拜占庭攻击者下的新方法。我们的设计核心是一种直接的高维半验证均值估计方法。我们的想法是首先识别一个子空间，通过工作机上传的梯度向量估计与该子空间垂直的均值分量，而通过辅助数据集估计该子空间内的均值分量。然后，我们将我们的新方法用作分布式学习问题的聚合器。我们的理论分析表明，新方法具有极小极值统计率。特别地，对维度的依赖性得到了显著改善。

    Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compar
    
[^198]: Harpa: 高速率下的相位关联与走时神经场

    Harpa: High-Rate Phase Association with Travel Time Neural Fields. (arXiv:2307.07572v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.07572](http://arxiv.org/abs/2307.07572)

    本论文提出了一种名为Harpa的高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，即使在波速未知的情况下也能实现相位关联。这种方法能够处理较小、高速率的地震事件，提供了关于地下弹性介质属性的宝贵描述。

    

    相位关联是根据其起源地震分组地震波到达的任务。它是地震数据处理流程中的基本任务，但对于较小、高速率的地震事件来说是具有挑战性的，这些事件携带有关地震动力学的基本信息，尤其是在常常假定不准确的波速模型下。因此，大多数关联方法都专注于发生率较低且容易关联的较大事件，尽管微地震活动提供了井下弹性介质属性的宝贵描述。在本文中，我们展示了即使在波速未知的情况下，也可以以比以前报告的更高的速率进行关联。我们提出了Harpa，这是一种高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，并首先解决联合时空源定位和波速恢复问题，然后进行相位关联。

    Phase association groups seismic wave arrivals according to their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by ass
    
[^199]: 从合成的人类团队活动中学习

    Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])

    [http://arxiv.org/abs/2306.16772](http://arxiv.org/abs/2306.16772)

    提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器，通过Unity引擎驱动实现。该生成器具有大规模数据生成、多模态和高质量注释等特点，能够用于研究复杂的人类互动和团队活动。

    

    在以人为中心的计算机视觉中，对复杂的人类互动和团队活动的理解引起了人们的关注。然而，相关任务的进展受到了获取大规模标记的真实世界数据集的困难的限制。为了缓解这个问题，我们提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器。M3Act采用Unity引擎驱动，包含可供仿真使用的三维场景和人物资源，可配置的照明和摄像系统，高度参数化的模块化团队活动，以及在数据生成过程中具有大量领域随机化的特点。我们的数据生成器能够生成具有多个视图、模态（RGB图像、2D姿势、3D动作）和高质量注释的大规模人类活动数据集（2D边界框、实例分割掩模、个体动作和团队活动类别）。利用M3Act，我们可以生成大规模的人类活动数据集，用于研究人类互动和团队活动。

    The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
    
[^200]: 模拟反事实情况

    Simulating counterfactuals. (arXiv:2306.15328v1 [stat.ML])

    [http://arxiv.org/abs/2306.15328](http://arxiv.org/abs/2306.15328)

    该论文提出了一种算法，可以模拟反事实分布中的值，可对离散和连续变量设定条件，并应用于信用评分中的公平性分析。

    

    反事实推断考虑了在与实际世界存在一些证据的平行世界中进行的假设性干预。如果证据在流形上指定了条件分布，反事实可能是解析难解的。我们提出了一种算法，用于从反事实分布中模拟值，其中可以对离散和连续变量设定条件。我们表明，所提出的算法可以被呈现为粒子滤波器，从而导致渐近有效的推断。该算法被应用于信用评分中的公平性分析。

    Counterfactual inference considers a hypothetical intervention in a parallel world that shares some evidence with the factual world. If the evidence specifies a conditional distribution on a manifold, counterfactuals may be analytically intractable. We present an algorithm for simulating values from a counterfactual distribution where conditions can be set on both discrete and continuous variables. We show that the proposed algorithm can be presented as a particle filter leading to asymptotically valid inference. The algorithm is applied to fairness analysis in credit scoring.
    
[^201]: 循环记忆决策变压器

    Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])

    [http://arxiv.org/abs/2306.09459](http://arxiv.org/abs/2306.09459)

    本文提出了循环记忆决策变压器（RMDT）模型，用于处理强化学习中的长序列问题。在Atari游戏和MoJoCo控制问题上的实验表明，采用循环记忆机制的RMDT模型显着优于其没有循环记忆机制的对应模型。

    

    变革性模型最初是为自然语言问题而开发的，最近在离线强化学习任务中得到广泛应用。这是因为代理的历史可以表示为序列，并且整个任务可以缩减为序列建模任务。然而，变压器操作的二次复杂性限制了上下文的潜在增加。因此，为了在自然语言中处理长序列，使用了不同版本的记忆机制。在本文中，我们提出了循环记忆决策变压器（RMDT），这是一种在强化学习问题中使用循环记忆机制的模型。我们在Atari游戏和MoJoCo控制问题上进行了彻底的实验，并表明我们提出的模型在Atari游戏上显着优于没有循环记忆机制的对应模型。我们还仔细研究了记忆对所提出的模型绩效的影响。这些发现为开发更高效和更有效的处理长序列的强化学习模型提供了启示。

    Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
    
[^202]: 弱监督下AUC优化：统一的部分AUC方法

    Weakly Supervised AUC Optimization: A Unified Partial AUC Approach. (arXiv:2305.14258v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2305.14258](http://arxiv.org/abs/2305.14258)

    本文提出了WSAUC，一种解决弱监督下AUC优化问题的统一框架，它包括噪声标签学习、正-无标签学习、多实例学习和半监督学习场景，并提出了一种新型的部分AUC——反转部分AUC（rpAUC），作为鲁棒的AUC最大化训练目标，为各种弱监督场景下的AUC优化提供了一种通用解决方案。

    

    由于获取完美的监督通常很困难，现实中的机器学习任务通常面临不准确、不完整或不精确的监督，统称为弱监督。在本文中，我们提出了WSAUC，一种用于解决弱监督下AUC优化问题的统一框架，它涵盖了噪声标签学习、正-无标签学习、多实例学习和半监督学习场景。在WSAUC框架内，我们首先将各种弱监督场景下的AUC优化问题框架化为最小化受污染集合上AUC风险的常见形式，并证明经验风险最小化问题与真实AUC一致。然后，我们介绍一种新型的部分AUC，即反转部分AUC（rpAUC），它作为鲁棒的AUC最大化训练目标，在存在污染标签的情况下发挥作用。WSAUC为各种弱监督场景下的AUC优化提供了一种通用解决方案。

    Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scena
    
[^203]: 最小化通信的异步张量并行性

    Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])

    [http://arxiv.org/abs/2305.13525](http://arxiv.org/abs/2305.13525)

    本文提出了Tensor3D，一种最小化通信消耗的三维张量计算并行化方法。它利用智能分布神经网络参数、新颖超分解方法以及通信模型，使训练速度提高了约3倍，GPU空闲时间降低了50％以上。

    

    随着现代神经网络规模扩大到数十亿个参数，设计能够在多GPU集群上高效训练这些网络的并行算法变得至关重要。本文提出了Tensor3D，一种全新的三维（3D）张量计算并行化方法，旨在最小化大型多十亿参数模型的并行训练中由通信引起的空闲时间。首先，我们引入了一种智能的神经网络参数分布方式，消除了为满足各层数据依赖而需要的通信。然后，我们提出了一种新颖的并行训练过程超分解方法，利用它可以显著提高通信与计算的重叠度，从而减少GPU空闲时间。最后，我们提出了一种通信模型，帮助用户为给定的神经网络识别通信最优的可用硬件资源分解。 对于256 A100 GPU上的28B参数CNN，在本文的 Tensor3D 方法下，训练速度提高了约3倍，与以前的方法相比 GPU 空闲时间也降低了约50％以上。

    As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
    
[^204]: ChatGPT 需要进行SPADE（可持续性、隐私、数字鸿沟和伦理）评估：一项综述。

    ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])

    [http://arxiv.org/abs/2305.03123](http://arxiv.org/abs/2305.03123)

    本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。

    

    ChatGPT是另一个大型语言模型（LLM），由于其性能和有效的对话能力，在研究和工业界中得到了巨大的关注。最近，许多研究已经发表，以展示ChatGPT和其他LLMs的有效性、效率、集成和情感。相反，本研究关注的是大多数被忽视的重要方面，即可持续性、隐私、数字鸿沟和伦理，并建议不仅仅是ChatGPT，而是在对话机器人类别中的每一个后续入口都应该进行SPADE评估。本文详细讨论了关于ChatGPT的问题和关注点与上述特征一致。我们通过一些初步的数据收集和可视化以及假设的事实来支持我们的假设。我们还为每个问题提出了缓解和建议。此外，我们还提供了一些未来方向和开放问题的探讨。

    ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
    
[^205]: 语言距离与多语言表示空间中的跨语言传递的相关性研究

    Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])

    [http://arxiv.org/abs/2305.02151](http://arxiv.org/abs/2305.02151)

    探索了语言特征对多语言表示空间中的跨语言传递性能的影响，初步提供了方法以增强对语言上相距较远的语言的传递能力。

    

    先前的研究已经探讨了不同语言特征对跨语言传递性能的影响。本研究探讨了这种效应如何映射到表示空间中。过去的研究集中在微调期间多语言语言模型的跨语言对齐上的影响，而本研究研究的是由MLLMs生成的相应语言表示空间的绝对演变。我们特别强调语言特征的作用，并调查其与表示空间和跨语言传递性能的影响之间的相互关系。此外，本文提供了初步证据，说明如何利用这些发现增强对语言上相距较远的语言的传递能力。

    Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
    
[^206]: 自监督ECG表征学习在心律失常检测中的应用研究：分布分析及实验探究

    In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection. (arXiv:2304.06427v1 [cs.LG])

    [http://arxiv.org/abs/2304.06427](http://arxiv.org/abs/2304.06427)

    本文系统研究了自监督学习方法在ECG表征学习上的应用，首次对三个常用ECG心律失常数据集进行了分布分析，实验发现SwAV方法表现最佳，能够超越传统的有监督学习方法，还具有较强的鲁棒性，有望在大规模和多样化人群中检测心律失常。

    

    本文针对心电图(ECG)心律失常检测问题，系统地研究了自监督学习(Self-Supervised Learning, SSL)方法的有效性。我们首先对三个常用的ECG心律失常数据集进行了分布分析，并进行了综合性实验，使用不同增强和参数评估了各种SSL方法（如SimCRL、BYOL和SwAV）在ECG表征学习方面的有效性。实验结果表明，SwAV方法表现最佳。我们进一步进行了针对In-Distribution (ID)和Out-of-Distribution (OOD) ECG数据的交叉数据集训练和测试实验，结果表明SSL方法，特别是SwAV，在ECG表征学习方面具有很高的竞争力，并且对不同种类的ECG数据具有较强的鲁棒性，从而有望在大规模和多样化人群中检测心律失常。

    This paper presents a systematic investigation into the effectiveness of Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia detection. We begin by conducting a novel distribution analysis on three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To the best of our knowledge, our study is the first to quantify these distributions in this area. We then perform a comprehensive set of experiments using different augmentations and parameters to evaluate the effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG representation learning, where we observe the best performance achieved by SwAV. Furthermore, our analysis shows that SSL methods achieve highly competitive results to those achieved by supervised state-of-the-art methods. To further assess the performance of these methods on both In-Distribution (ID) and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and testing experiments. Our comprehensive
    
[^207]: ERM++：用于域通用性的改进基准方法

    ERM++: An Improved Baseline for Domain Generalization. (arXiv:2304.01973v1 [cs.LG])

    [http://arxiv.org/abs/2304.01973](http://arxiv.org/abs/2304.01973)

    ERM++是一个用于域通用性的改进基准方法，通过更好地利用训练数据、模型参数选择和权重空间正则化等关键技术，在多个数据集上比标准ERM更有效，同时计算复杂度更低，表现也优于最先进方法。

    

    多源域通用性（DG）衡量分类器对于它没有接受过训练的新数据分布的泛化能力，并考虑了多个训练域。虽然已经提出了几种多源DG方法，但是它们在训练过程中使用域标签增加了额外的复杂性。最近的研究表明，经过良好调整的经验风险最小化（ERM）训练过程，即在源域上简单地最小化经验风险，可以胜过大多数现有的DG方法。我们确定了几个关键候选技术，以进一步提高ERM的性能，例如更好地利用训练数据、模型参数选择和权重空间正则化。我们将结果称为ERM ++，并展示它相对于标准ERM在五个多源数据集上将DG的性能显着提高了5％以上，并且尽管计算复杂度更低，但击败了最先进的方法。此外，我们还证明了ERM ++在WILDS-FMOW数据集上的有效性。

    Multi-source Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on, given several training domains. While several multi-source DG methods have been proposed, they incur additional complexity during training by using domain labels. Recent work has shown that a well-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. We identify several key candidate techniques to further improve ERM performance, such as better utilization of training data, model parameter selection, and weight-space regularization. We call the resulting method ERM++, and show it significantly improves the performance of DG on five multi-source datasets by over 5% compared to standard ERM, and beats state-of-the-art despite being less computationally expensive. Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset,
    
[^208]: 揭开对社交机器人研究的误解

    Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])

    [http://arxiv.org/abs/2303.17251](http://arxiv.org/abs/2303.17251)

    这篇文章揭示了关于社交机器人研究的普遍误解，强调需要以严谨、公正和负责任的方式讨论虚假信息研究。

    

    社交机器人科学寻求解决网络虚假信息最受争议的形式之一的知识和解决方案。然而，社交机器人研究受到普遍的偏见、夸大的结果和误解的困扰，这些都为歧义、不切实际的期望和看似无法调和的发现打下了基础。克服这些问题对于确保可靠的解决方案和重申科学方法的有效性至关重要。在这篇文章中，我们修订了社交机器人研究中的一些最新结果，强调和纠正了事实错误以及方法论和概念问题。更重要的是，我们揭开了普遍的误解，解决了有关如何讨论社交机器人研究的基本问题。我们的分析揭示了以严谨、公正和负责任的方式讨论虚假信息研究的必要性。本文通过确定并驳斥社交机器人研究的支持者和反对者常用的谬误论证，支持这种努力。

    The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
    
[^209]: 适应性负证据深度学习用于开放式半监督学习

    Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])

    [http://arxiv.org/abs/2303.12091](http://arxiv.org/abs/2303.12091)

    本文提出了ANEDL框架，应用证据深度学习量化不同类型的不确定性，并设计了新颖的适应性负优化策略，有效应对在未标记数据集中包含内部值和异常值的开放式半监督学习。

    

    半监督学习方法假设标记数据、未标记数据和测试数据来自同一分布。开放式半监督学习考虑到一个更实际的情况，即未标记数据和测试数据包含标记数据中未观察到的新类别（异常值）。本文提出了一种新颖的框架——适应性负证据深度学习（ANEDL），以应对二元分类器的不足之处，如缺乏可扩展性和无法区分不同类型的不确定性。具体而言，我们首先介绍证据深度学习（EDL）作为一种异常检测器来量化不同类型的不确定性，并设计不同的不确定性度量方法进行自我训练和推理。此外，我们提出了一种新颖的适应性负优化策略，使EDL更加适合包含内部值和异常值的未标记数据集。通过在基准数据集上的实验验证，我们的ANEDL显著优于现有的开放式半监督学习方法。

    Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
    
[^210]: CroSel: 用于部分标签学习的自信伪标签的跨选择

    CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])

    [http://arxiv.org/abs/2303.10365](http://arxiv.org/abs/2303.10365)

    CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。

    

    部分标签学习(PLL)是一个重要的弱监督学习问题，它允许每个训练示例有一个候选标签集，而不是一个单一的ground-truth标签。已经广泛探索了基于识别的方法来解决PLL中的标签歧义问题，这些方法将真实标签视为要识别的潜在变量。然而，准确和完整地识别真实标签仍然具有挑战性，这会在模型训练过程中导致伪标签中的噪声。本文提出了一种名为CroSel的新方法，该方法利用模型的历史预测信息来识别大多数训练示例的真实标签。首先，我们引入了一种交叉选择策略，使得两个深度模型可以相互选择部分标记数据的真实标签。此外，我们提出了一种新颖的一致性正则化项co-mix，以避免因虚假选择而引起的样本浪费和微小噪声。通过这种方式，CroSel能够挑选出大多数示例的真实标签。

    Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
    
[^211]: HIVE：利用人类反馈进行指导性视觉编辑

    HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])

    [http://arxiv.org/abs/2303.09618](http://arxiv.org/abs/2303.09618)

    本文提出了一种新的框架，利用人类反馈进行指导性视觉编辑。通过收集被编辑图像的人类反馈，并学习奖励函数捕捉用户的偏好，可以缓解数据限制所带来的偏差，并提高模型性能。

    

    研究表明，将人类反馈纳入大型语言模型生成的文本对齐到人类偏好至关重要。本文假设，最先进的指导性图像编辑模型，其输出基于输入图像和编辑指令，同样可以从人类反馈中受益，因为其输出可能不符合用户的正确指令和偏好。本文提出了一种利用人类反馈进行指导性视觉编辑（HIVE）的新框架。具体而言，我们在编辑的图像上收集人类反馈并学习奖励函数以捕捉基础用户偏好。随后，我们引入可扩展的扩散模型微调方法，可根据估计的奖励值融入人类偏好。此外，为减轻数据限制带来的偏差，我们贡献了1M训练数据集，3.6K奖励数据集以用于奖励学习，以及1K评估数据集，以提高指导性图像编辑模型的性能。

    Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
    
[^212]: CrystalBox：基于未来的DRL网络控制器的解释器

    CrystalBox: Future-Based Explanations for DRL Network Controllers. (arXiv:2302.13483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13483](http://arxiv.org/abs/2302.13483)

    CrystalBox是一种解释DRL网络控制器行为的框架，它能够使用未来关键网络性能指标的影响来生成简明而富有表现力的解释。

    

    解释性不足是限制高效深度强化学习(DRL)控制器实际采用的关键因素。网络解释性强化学习迄今使用引人注目的输入特征来解释控制器的行为。然而，这些基于特征的解决方案不能完全解释控制器的决策过程。通常，运营商有兴趣了解控制器对未来性能的影响，而基于特征的解决方案无法捕捉。在本文中，我们提出了CrystalBox，这是一个框架，通过关键网络性能指标的未来影响来解释控制器的行为。CrystalBox采用一种新颖的基于学习的方法，生成简洁而富有表现力的解释。我们使用DRL网络控制器的奖励组件作为解释的基础，这是运营商有意义的关键绩效指标。CrystalBox是通用的，可以适用于离散和连续控制环境。

    Lack of explainability is a key factor limiting the practical adoption of high-performant Deep Reinforcement Learning (DRL) controllers. Explainable RL for networking hitherto used salient input features to interpret a controller's behavior. However, these feature-based solutions do not completely explain the controller's decision-making process. Often, operators are interested in understanding the impact of a controller's actions on performance in the future, which feature-based solutions cannot capture.  In this paper, we present CrystalBox, a framework that explains a controller's behavior in terms of the future impact on key network performance metrics. CrystalBox employs a novel learning-based approach to generate succinct and expressive explanations. We use reward components of the DRL network controller, which are key performance metrics meaningful to operators, as the basis for explanations. CrystalBox is generalizable and can work across both discrete and continuous control en
    
[^213]: 强化学习的后悔优化方法

    Regret-Based Optimization for Robust Reinforcement Learning. (arXiv:2302.06912v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06912](http://arxiv.org/abs/2302.06912)

    本论文提出了一种基于后悔的优化方法，用于使强化学习算法更加鲁棒，以应对观测中的对抗性噪声。

    

    深度强化学习策略对观测中的微小对抗性噪声容易受到攻击。这种对抗性噪声在安全关键环境中可能造成灾难性后果。现有的使强化学习算法对观测扰动的对抗策略主要集中在迭代改进每个迭代中生成的对抗示例。虽然这些方法已经显示出对普通强化学习方法的改进，但它们是被动性的，如果某些类别的对抗性示例在训练中没有产生，它们可能会表现得更差。因此，我们追求一种更积极的方法，依赖于直接优化一个经过充分研究的鲁棒指标。

    Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable to small adversarial noise in observations. Such adversarial noise can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about nearby signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) or objects (e.g., cars altered to be recognized as trees) can be fatal. Existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on reactive approaches that iteratively improve against adversarial examples generated at each iteration. While such approaches have been shown to provide improvements over regular RL methods, they are reactive and can fare significantly worse if certain categories of adversarial examples are not generated during training. To that end, we pursue a more proactive approach that relies on directly optimizing a well-studied robustn
    
[^214]: 基于语言解释的去偏见: 通过语言解释消除未知的视觉偏见

    Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation. (arXiv:2301.11104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11104](http://arxiv.org/abs/2301.11104)

    本文提出了基于语言解释的去偏见(B2T)框架，通过分析图像标题中的关键词，比较关键词和图像之间的相似性，识别和减缓视觉模型中的偏见，并提出了针对零样本分类器和文本到图像扩散模型的去偏见策略。

    

    模型中的偏见在部署机器学习系统时构成重要问题，但以可解释的方式诊断这些偏见可能具有挑战性。为了解决这个问题，我们引入了去偏见(B2T)框架，该框架利用语言解释来识别和缓解视觉模型中的偏见，例如图象分类器和文本生成模型。我们对视觉偏差的语言描述提供了可解释的形式，使得能够发现新的偏见并有效地对模型进行去偏见。为了实现这一点，我们分析了被误预测或生成的图像标题中的常见关键词。在这里，我们提出了新的评分函数，通过比较偏见关键词和图像之间的相似性来避免标题中的偏见。此外，我们还提出了使用B2T框架中的偏见关键词对零样本分类器和文本到图像扩散模型进行去偏见的策略。我们展示了我们的框架在各种图像分类和生成任务上的有效性。

    Biases in models pose a critical issue when deploying machine learning systems, but diagnosing them in an explainable manner can be challenging. To address this, we introduce the bias-to-text (B2T) framework, which uses language interpretation to identify and mitigate biases in vision models, such as image classifiers and text-to-image generative models. Our language descriptions of visual biases provide explainable forms that enable the discovery of novel biases and effective model debiasing. To achieve this, we analyze common keywords in the captions of mispredicted or generated images. Here, we propose novel score functions to avoid biases in captions by comparing the similarities between bias keywords and those images. Additionally, we present strategies to debias zero-shot classifiers and text-to-image diffusion models using the bias keywords from the B2T framework. We demonstrate the effectiveness of our framework on various image classification and generation tasks. For classifi
    
[^215]: 部分动员：跟踪俄罗斯媒体和电报之间的多语言信息流

    Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2301.10856](http://arxiv.org/abs/2301.10856)

    本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。

    

    在俄罗斯入侵乌克兰后，针对俄罗斯在线媒体的虚假信息和宣传，包括俄罗斯之声和卫星新闻在内的俄罗斯媒体在欧洲遭到禁止。为了保持观众数量，许多俄罗斯媒体开始在电报等消息服务上大力宣传其内容。在这项工作中，我们研究了2022年期间16家俄罗斯媒体机构如何与732个电报频道互动和利用。利用基础模型MPNet、DP-means聚类和Hawkes过程，我们跟踪新闻网站和电报频道之间的叙事传播情况。我们表明，新闻媒体不仅通过电报传播现有的叙事，而且他们会从电报平台源材料。在我们研究的网站中，2.3％（ura.news）至26.7％（ukraina.ru）的文章讨论了源于/导致电报活动的内容。最后，通过跟踪个别主题的扩散，我们测量新闻网站发表文章的速率。

    In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
    

