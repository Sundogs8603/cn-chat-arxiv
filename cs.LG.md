# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation.](http://arxiv.org/abs/2305.19798) | 本文提出了一种基于不对称核奇异值分解的自注意力机制，即Primal-Attention，来优化注意力机制，提高注意力输出的投影方差。 |
| [^2] | [Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya.](http://arxiv.org/abs/2305.19779) | 本研究提出了一种利用aggVAE进行深度学习和MCMC处理行政边界变化的解决方案，可以更准确地映射以县为层级的聚合级别数据，并处理行政边界的变化，相比最先进的模型表现更好。 |
| [^3] | [Off-By-One Implementation Error in J-UNIWARD.](http://arxiv.org/abs/2305.19776) | J-UNIWARD 是一种将秘密信息隐藏在JPEG图像中的隐写方法，本文发现了其实现中存在的一个 off-by-one 错误，使一些图像块被高估，另一些被低估，同时提供了一个概念验证用于检测此种错误。 |
| [^4] | [Ambiguity in solving imaging inverse problems with deep learning based operators.](http://arxiv.org/abs/2305.19774) | 本文提出了通过使用非常小的神经结构、提出一种基于正则化项的网络损失函数修改策略和对图像进行低秩表示的预处理步骤来提高基于深度学习的去模糊方法的稳定性和鲁棒性。研究结果表明，这些策略有效地提高了图像去模糊方法的稳定性和准确性。 |
| [^5] | [Quality In / Quality Out: Assessing Data quality in an Anomaly Detection Benchmark.](http://arxiv.org/abs/2305.19770) | 本文发现，在进行异常检测基准测试时，基准数据集的质量对于机器学习模型的性能影响很大，比特定的机器学习算法更重要。 |
| [^6] | [Attention-Based Methods For Audio Question Answering.](http://arxiv.org/abs/2305.19769) | 本文提出了一种基于自我注意力和交叉注意力的神经网络体系结构，用于音频问答任务。实验结果表明，相较于参考方法，该方法在Clotho-AQA数据集上的表现有明显提高。 |
| [^7] | [A Bayesian Perspective On Training Data Attribution.](http://arxiv.org/abs/2305.19765) | 本文介绍了一种TDA任务的贝叶斯视角，从中发现个别训练样本的影响常被噪声掩盖，TDA只能用于解释对模型预测影响稳定、独立于其他噪声因素的训练数据。 |
| [^8] | [Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models.](http://arxiv.org/abs/2305.19761) | 本文提出了一种递归 Metropolis-Hastings 命名游戏 (RMHNG) 模型，用于多智能体系统中的符号出现，具有较快的收敛速度和鲁棒性，并在多个场景下进行了验证。 |
| [^9] | [The Tunnel Effect: Building Data Representations in Deep Neural Networks.](http://arxiv.org/abs/2305.19753) | 本文研究表明，深度神经网络中存在一种名为“隧道”的现象，它在网络的训练早期就出现，并且对最终的数据表示起到了压缩作用。其中，初始层构建了线性可分表示形式，而随后的层压缩这些表示形式并对整体性能影响不大。然而，隧道会削弱网络在超出分布的泛化性能。 |
| [^10] | [Neural Markov Jump Processes.](http://arxiv.org/abs/2305.19744) | 介绍了一种基于神经常微分方程的马尔可夫跳跃过程的变分推断算法，可通过反向传播进行训练，用于近似后验马尔可夫跳跃过程的初始分布和时间相关的转移概率率，同时在先验过程的时间无关率上也有很好的表现。 |
| [^11] | [Reliable Off-Policy Learning for Dosage Combinations.](http://arxiv.org/abs/2305.19742) | 本文提出了一种用于剂量组合的新颖可靠的脱机学习方法，通过三个步骤实现：开发神经网络估计个性化的剂量-反应，估计倾向得分检测共享协变量-治疗空间中的重叠有限区域，然后基于梯度的学习算法找到最佳的个性化剂量组合。 |
| [^12] | [Bures-Wasserstein Means of Graphs.](http://arxiv.org/abs/2305.19738) | 该论文提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。实验结果表明，在各种任务中都有很好的表现。 |
| [^13] | [APPRAISER: DNN Fault Resilience Analysis Employing Approximation Errors.](http://arxiv.org/abs/2305.19733) | 提出了一种新的容错评估方法，称为APPRAISER，它采用近似误差进行DNN故障容忍性分析。APPRAISER提供了成千上万倍的评估速度提升，同时保持了分析的高准确性。 |
| [^14] | [Data Representations' Study of Latent Image Manifolds.](http://arxiv.org/abs/2305.19730) | 本文研究了图像流形的曲率，其中最先进的卷积神经网络在层间具有特征曲率剖面，曲率差异与网络的泛化能力有强烈的相关性，且mixup等常见的规范化方法产生更平的表示。 |
| [^15] | [Unbalanced Low-rank Optimal Transport Solvers.](http://arxiv.org/abs/2305.19727) | 本论文提出了首个据我们所知的融合低秩近似和非平衡公式的O(n)优化输运求解器，并展示了其在各种任务中的优越性能。 |
| [^16] | [Learning Representations without Compositional Assumptions.](http://arxiv.org/abs/2305.19726) | 本文提出了一种无组合假设的数据驱动方法，通过表示不同特征集之间的依赖关系来进行无监督表示学习，并引入了LEGATO分层图自编码器来捕获局部信息。 |
| [^17] | [A rule-general abductive learning by rough sets.](http://arxiv.org/abs/2305.19718) | 本文提出了一种基于粗糙集理论的规则通用逆推学习方法，通过转化目标概念和子概念为信息表，以更低的成本解决领域知识获取和规则的修正、减少、生成问题。 |
| [^18] | [Is Rewiring Actually Helpful in Graph Neural Networks?.](http://arxiv.org/abs/2305.19717) | 本文研究了图神经网络中的改连方法是否有用，提出了一种新的评估设置，并在真实世界的节点和图分类任务上进行了系统的实验比较。 |
| [^19] | [Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming.](http://arxiv.org/abs/2305.19706) | 本研究提出了一种通用的动态规划方法来优化任何组合的可分离目标和约束条件，这种方法在可扩展性方面比通用求解器表现得更好。 |
| [^20] | [An Efficient Machine Learning-based Channel Prediction Technique for OFDM Sub-Bands.](http://arxiv.org/abs/2305.19696) | 这篇论文提出了一种基于机器学习训练信道衰落样本的OFDM子带信道预测技术，可用于估计选择性衰落中未来信道行为。 |
| [^21] | [Causal discovery for time series with constraint-based model and PMIME measure.](http://arxiv.org/abs/2305.19695) | 本文提出了一种新方法，将因果发现算法与信息理论度量结合，以解决通过相关性发现虚假关系的问题。这种方法在几个模拟数据集上呈现出有希望的结果。 |
| [^22] | [Hypothesis Transfer Learning with Surrogate Classification Losses.](http://arxiv.org/abs/2305.19694) | 本文研究了使用代理分类损失的假设迁移学习的学习理论，通过算法稳定性提供了在温和假设下的学习保证，适用于机器学习算法。 |
| [^23] | [Spontaneous symmetry breaking in generative diffusion models.](http://arxiv.org/abs/2305.19693) | 本文揭示生成式扩散模型存在自发对称破缺现象，这将其生成动力学分为两种不同的“相”，提出了高斯后期初始化方案，能够显著提高模型性能。 |
| [^24] | [Constant or logarithmic regret in asynchronous multiplayer bandits.](http://arxiv.org/abs/2305.19691) | 本文解决了异步多人赌博问题的集中式情况，推出了Cautious Greedy算法，可以保证常数遗憾，同时UCB算法的自然扩展展现出了 $\mathcal{O}(\sqrt{T\log(T)})$ 极小化遗憾。 |
| [^25] | [Deep Stochastic Mechanics.](http://arxiv.org/abs/2305.19685) | 本文提出了一种基于深度学习的方法，用于数值模拟时间演化薛定谔方程，利用马尔可夫扩散采样来适应波函数的潜在低维结构，并提出了新的随机量子力学方程，具有线性的计算复杂度。数值模拟显示出显着的优势。 |
| [^26] | [End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization.](http://arxiv.org/abs/2305.19684) | 本研究提出一种基于Metropolis-Hastings耦合和局部模态初始化的方法，解决了深度玻尔兹曼机中的偏差梯度估计问题，使得DBMs可以端到端地训练，实验结果表明与其他深度生成模型相当的生成性能。 |
| [^27] | [Smooth-Trajectron++: Augmenting the Trajectron++ behaviour prediction model with smooth attention.](http://arxiv.org/abs/2305.19678) | 本文中，研究者将平滑关注机制引入最先进的Trajectron++轨迹预测模型，提高其性能并比较其性能表现。该研究表明将人类的认知因素纳入轨迹预测模型中具有潜在的优势。 |
| [^28] | [Online-to-PAC Conversions: Generalization Bounds via Regret Analysis.](http://arxiv.org/abs/2305.19674) | 本文提出了在线学习游戏“泛化游戏”的框架，将在线学习算法的表现和统计学习算法的泛化界限联系了起来，并得出了一些标准的泛化限制。 |
| [^29] | [Signal Is Harder To Learn Than Bias: Debiasing with Focal Loss.](http://arxiv.org/abs/2305.19671) | 这篇论文提出了一种同时训练有偏和无偏分类器的方法，使用分解重新加权方案和Focal Loss启发的方法去偏差，相比于其他方法性能更好，同时提出了一种基于潜空间的可视化偏差方案，可以帮助实践者认识虚假相关的来源。 |
| [^30] | [Vandermonde Neural Operators.](http://arxiv.org/abs/2305.19663) | 本文提出了一种新的神经网络结构，Vandermonde神经算子，能够有效地处理非均匀分布点上的输入数据，同时在速度和准确性上相较于以前的方法有所提升。 |
| [^31] | [Improving Expressivity of Graph Neural Networks using Localization.](http://arxiv.org/abs/2305.19659) | 本文提出了Weisfeiler-Leman (WL)算法的局部版本，用于解决子图计数问题并提高图神经网络的表达能力，同时，也给出了一些时间和空间效率更高的$k-$WL变体和分裂技术。 |
| [^32] | [Optimal Estimates for Pairwise Learning with Deep ReLU Networks.](http://arxiv.org/abs/2305.19640) | 本文研究了深度ReLU网络中的成对学习，提出了一个针对一般损失函数的误差估计的尖锐界限，并基于成对最小二乘损失得出几乎最优的过度泛化误差界限。 |
| [^33] | [Explainable AI for Malnutrition Risk Prediction from m-Health and Clinical Data.](http://arxiv.org/abs/2305.19636) | 本文提出了一个基于异构m-health数据的早期和可解释的营养不良风险检测的新型AI框架，通过模型评估发现，随机森林（RF）和梯度上升是表现最好的分类器，尤其是在纳入身体组成评估数据时。 |
| [^34] | [Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast.](http://arxiv.org/abs/2305.19623) | Point-GCC提出了一种通过几何-颜色对比进行通用自监督三维场景预训练的方法，并设计了分层监督和架构无关的骨干网络，以缩小预训练和下游任务之间的差距。 |
| [^35] | [MSMix:An Interpolation-Based Text Data Augmentation Method Manifold Swap Mixup.](http://arxiv.org/abs/2305.19617) | 提出了一种基于插值的文本数据增强方法——流形交换Mixup(MSMix)，通过在网络的特定层部分替换隐藏特征来从两个不同的样本中获得更丰富的隐藏表示，并在三个中文意图识别数据集上实验证明了MSMix在完整样本和小样本配置下均表现出更好的性能。 |
| [^36] | [Underwater-Art: Expanding Information Perspectives With Text Templates For Underwater Acoustic Target Recognition.](http://arxiv.org/abs/2305.19612) | 本文提出了一种基于文本模板的水下声学目标识别方法（UART），从不同视角整合相关信息，通过音频-频谱图-文本三模态对比学习框架，赋予UART用自然语言指导声学表示学习的能力，显著提高识别模型的可解释性和鲁棒性。 |
| [^37] | [Medication Recommendation via Domain Knowledge Informed Deep Learning.](http://arxiv.org/abs/2305.19604) | 提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。 |
| [^38] | [Learning Music Sequence Representation from Text Supervision.](http://arxiv.org/abs/2305.19602) | 该论文提出一种新颖的文本监督预训练方法MUSER，能够更灵活地适应下游任务，且只需要使用0.056%的预训练数据即可实现最先进的性能。 |
| [^39] | [Federated Learning on Heterogeneous Data via Adaptive Self-Distillation.](http://arxiv.org/abs/2305.19600) | 本文提出一种基于自适应自蒸馏的新型正则化技术来训练客户端模型，该正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。实验结果表明，该方法在各种基准数据集上优于目前流行的联邦学习方法。 |
| [^40] | [Towards Semi-supervised Universal Graph Classification.](http://arxiv.org/abs/2305.19598) | 该论文提出了一种新型图神经网络框架UGNN， 解决了半监督普适图分类问题，通过估计未标记图的确定性解决了类别偏移，具有最新性能。 |
| [^41] | [Exploring the Vulnerabilities of Machine Learning and Quantum Machine Learning to Adversarial Attacks using a Malware Dataset: A Comparative Analysis.](http://arxiv.org/abs/2305.19593) | 本研究比较了传统神经网络和量子神经网络在恶意软件数据集上对于攻击的易感性，并发现QNN表现出更高的易感性。 |
| [^42] | [Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities.](http://arxiv.org/abs/2305.19591) | 该论文综述了交通预测方法的发展，重点介绍了基于人工智能的交通预测方法在多元交通时间序列模型研究方面的进展和机遇。 |
| [^43] | [Active causal structure learning with advice.](http://arxiv.org/abs/2305.19588) | 本研究提出了带建议的主动因果结构学习问题，并设计了一个自适应搜索算法，可以从建议中受益，即使建议是任意糟糕的情况下，仍然具有最坏情况下的保证。 |
| [^44] | [Towards Omni-generalizable Neural Methods for Vehicle Routing Problems.](http://arxiv.org/abs/2305.19587) | 提出了一个通用meta-learning框架，使得模型可以在推理过程中快速适应新任务。通过在多个合成和基准数据集上的实验表明，该方法可以有效地解决大小和分布变化的车辆路径问题（VRP）。 |
| [^45] | [LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction.](http://arxiv.org/abs/2305.19585) | LAIT是一种通过多段编码实现跨段注意力的新框架，使用Layer-Adjustable Interactions技术实现分段编码和逐层交互，提高了模型的效率和精度，并极大地简化了模型设计的复杂度。 |
| [^46] | [Causal Discovery with Latent Confounders Based on Higher-Order Cumulants.](http://arxiv.org/abs/2305.19582) | 本文提出了一种利用高阶累积量实现潜在混淆因素因果关系发现的新方法，这种方法能够使单潜在成分结构的OICA问题得到闭合形式解决方案，同时还提出了可测试的单潜在成分条件，通过迭代删除共享识别的潜在成分成功扩展了结果到多潜在成分结构。 |
| [^47] | [Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms.](http://arxiv.org/abs/2305.19570) | 本文提出了新算法来解决在线标签移位问题，在无需先验知识的情况下通过在线回归保证了最优动态遗憾，并在模拟和真实场景中表现出卓越的性能。 |
| [^48] | [Zero-Shot Automatic Pronunciation Assessment.](http://arxiv.org/abs/2305.19563) | 本文提出了一种基于预训练声学模型的零样本自动发音评估方法，该方法不需要标注数据，使用掩码模块破坏语音输入，应用k均值聚类获得标记序列并使用评分模块测量错误恢复标记的数量，在实验中取得了与监督回归基线相当的性能以及优于非回归基线的结果。 |
| [^49] | [Replicability in Reinforcement Learning.](http://arxiv.org/abs/2305.19562) | 这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。 |
| [^50] | [Dictionary Learning under Symmetries via Group Representations.](http://arxiv.org/abs/2305.19557) | 本文研究在预定变换群下学习不变的字典问题。利用非阿贝尔傅里叶分析，提供了算法，建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。 |
| [^51] | [Large Language Models Are Not Abstract Reasoners.](http://arxiv.org/abs/2305.19555) | 本文通过对最先进的大型语言模型进行抽象推理任务评估，发现它们在这方面的表现十分有限，揭示了其在推理方面的局限性。 |
| [^52] | [Catalysis distillation neural network for the few shot open catalyst challenge.](http://arxiv.org/abs/2305.19545) | 本研究介绍了2023年少样本开放式催化剂挑战赛，旨在推进机器学习技术在催化表面上预测催化反应，特别关注双原子催化剂的应用。 |
| [^53] | [Automatic Illumination Spectrum Recovery.](http://arxiv.org/abs/2305.19538) | 本文介绍了利用深度学习网络估计多光谱图像不同光照条件下照明谱的方法，并展示了模型推断的准确性。 |
| [^54] | [Low-rank extended Kalman filtering for online learning of neural networks from streaming data.](http://arxiv.org/abs/2305.19535) | 本文提出一种基于低秩扩展卡尔曼滤波的高效在线学习算法，其能够估计非线性函数的参数，具有更快的适应性和更快的奖励积累。 |
| [^55] | [Recasting Self-Attention with Holographic Reduced Representations.](http://arxiv.org/abs/2305.19534) | 本文提出了一种使用HRR的神经符号方法重新构建自注意力的方法，可以实现较低的时间和空间复杂度，并在LRA基准测试中获得了接近于最先进的准确度。 |
| [^56] | [Offline Meta Reinforcement Learning with In-Distribution Online Adaptation.](http://arxiv.org/abs/2305.19529) | 本文提出了一种带有不确定性量化的内部分布在线适应(IDAQ)的框架，利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务，在离线元强化学习上具有竞争性表现。 |
| [^57] | [Discovering New Interpretable Conservation Laws as Sparse Invariants.](http://arxiv.org/abs/2305.19525) | 这篇论文介绍了一种名为Sparse Invariant Detector（SID）的算法，它能够自动发现微分方程中的保守律。该算法可以重新发现已知的保守律，甚至发现新的保守律，并且已发现的保守律具有稳健性和可解释性。 |
| [^58] | [Incremental Randomized Smoothing Certification.](http://arxiv.org/abs/2305.19521) | 本文提出了渐进式随机平滑认证方法（IRS），可通过重用原始平滑模型的认证保证来认证近似模型，从而显著降低认证修改DNN的计算成本同时保持强大的鲁棒性保证。 |
| [^59] | [Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels.](http://arxiv.org/abs/2305.19518) | 本论文提出了一种基于标签检索的扩散模型，用于有效构造并利用伪干净标签，以减小嘈杂标签对分类影响。 |
| [^60] | [Fine-grained Text Style Transfer with Diffusion-Based Language Models.](http://arxiv.org/abs/2305.19512) | 本文提出了一种基于扩散式语言模型的细粒度文本风格转换方法，在不依赖外部信息的情况下取得了比之前利用预训练权重、嵌入和外部语法分析器更好的效果，表明扩散概率模型在文本生成领域具有广泛的应用前景。 |
| [^61] | [Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape.](http://arxiv.org/abs/2305.19510) | 本文研究了略微超参数化的ReLU网络在有限输入数据集上的损失景观，证明了大多数激活模式对应的参数区域没有坏的可微局部极小值，对于一维输入数据，网络可以通过大多数激活模式实现高维全局极小值集合而不具有坏的局部极小值。 |
| [^62] | [Graph Entropy Minimization for Semi-supervised Node Classification.](http://arxiv.org/abs/2305.19502) | 本研究提出一种名为图熵最小化的半监督学习方法，可同时解决预测误差、训练资源和推理延迟等三个问题。其采用从大量未分类节点中的一跳聚合进行预测，使得其预测精度与具有两个或更多跳消息传递的GNN相当。而且它还支持随机训练和在线知识蒸馏等加速技术，提高了性能和效率。 |
| [^63] | [Deep into The Domain Shift: Transfer Learning through Dependence Regularization.](http://arxiv.org/abs/2305.19499) | 本文提出了一种新的领域适应方法，可以分别衡量内部相关结构的差异和边缘分布的差异， significantly improves the transfer learning performance. |
| [^64] | [Is Learning in Games Good for the Learners?.](http://arxiv.org/abs/2305.19496) | 我们提出了“广义均衡”的概念，通过学习可以在某些游戏中获得更好的结果。如果没有纯纳什均衡，则一名玩家可以从不同策略中受益，结果捕获了Stackelberg均衡的扩展。 |
| [^65] | [Adaptive False Discovery Rate Control with Privacy Guarantee.](http://arxiv.org/abs/2305.19482) | 本文提出了一种带隐私保障的自适应FDR控制方法，采用新颖的p值转换方法和镜像剥离算法，可在用户指定的水平α下确切地控制经典的FDR指标，表现更好且可减小隐私泄露风险。 |
| [^66] | [Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration.](http://arxiv.org/abs/2305.19476) | 本文提出了一种新的探索技术，使用值条件状态熵来解决强化学习中探索不足的问题，可以均衡地覆盖低价值和高价值状态，相较于现有基于熵的探索方法，该方法在MuJoCo基准测试和Atari游戏上有着显著的提升。 |
| [^67] | [Doubly Constrained Fair Clustering.](http://arxiv.org/abs/2305.19475) | 本论文关注公平聚类问题中的人口统计学公平概念，提出一种同时满足不同公平要求的快速算法。 |
| [^68] | [Chain of Log-Concave Markov Chains.](http://arxiv.org/abs/2305.19473) | 该论文提出了一种新的采样算法，基于对数凹条件概率密度，使用等向性高斯平滑来解决高维下抽样难题。 |
| [^69] | [PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning.](http://arxiv.org/abs/2305.19472) | PlaSma提出了一种使用小型语言模型进行过程知识和计划能力的新方法， |
| [^70] | [Label Embedding by Johnson-Lindenstrauss Matrices.](http://arxiv.org/abs/2305.19470) | 这篇论文提出了基于JLMs的标签嵌入方法，将多元分类问题转化为有限回归问题，具有较高的计算效率和预测准确性。 |
| [^71] | [The Impact of Positional Encoding on Length Generalization in Transformers.](http://arxiv.org/abs/2305.19466) | 本文通过实证研究 Transformer 模型中位置编码对于长度推广的影响，结果表明常用的位置编码方法并不适合用于下游任务的长度推广，并且使用位置编码甚至可能会损害长度推广的能力。 |
| [^72] | [A Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition.](http://arxiv.org/abs/2305.19458) | 该论文提出了一种统一的音视频学习框架，用于联合定位、分离和识别，包括共享的音视频编码器和针对不同任务的解码器，通过局部音视频对应损失、混合分离框架和强化视觉识别能力来优化不同任务之间的相互依赖性能。 |
| [^73] | [Dynamic Sparsity Is Channel-Level Sparsity Learner.](http://arxiv.org/abs/2305.19454) | 本文提出了一种名为Channel-aware dynamic sparse (Chase)的方法，使用端到端训练实现了GPU友好的通道级别稀疏，不需要任何特殊操作，并且可以直接在通用硬件上加速，显著减小模型大小，同时保持性能。 |
| [^74] | [Bigger, Better, Faster: Human-level Atari with human-level efficiency.](http://arxiv.org/abs/2305.19452) | 引入BBF基于价值函数的RL代理，在Atari 100K基准测试上实现超人类表现，具有人类效率，提出了在样本高效RL研究的ALE中更新目标的可能。 |
| [^75] | [OWAdapt: An adaptive loss function for deep learning using OWA operators.](http://arxiv.org/abs/2305.19443) | 本文提出了一种基于OWA算子的模糊自适应损失函数，通过迭代加权策略应对类别级别噪声条件，提高深度学习在分类任务中的性能。实验证明该方法在各种分类任务中均优于传统的常用损失函数。 |
| [^76] | [SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning.](http://arxiv.org/abs/2305.19442) | SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。 |
| [^77] | [Machine learning with tree tensor networks, CP rank constraints, and tensor dropout.](http://arxiv.org/abs/2305.19440) | 本文介绍了一种新的机器学习方法，通过基于树状张量网络的CP秩约束和张量丢弃，来构建低秩分类器，并在时尚-MNIST图像分类中展示出了优异的表现。 |
| [^78] | [Adapting Fairness Interventions to Missing Values.](http://arxiv.org/abs/2305.19429) | 本文研究了如何在缺失值的情况下实现公平的分类。传统方法会加剧歧视。本文证明从插补数据训练分类器会恶化组公平性和平均准确性。作者提出可扩展和适应性的算法，可以与其他公平干预算法结合使用，以处理所有可能的缺失模式。 |
| [^79] | [Evaluating geospatial context information for travel mode detection.](http://arxiv.org/abs/2305.19428) | 本研究确定了与相关工作有关的背景表示法，并基于随机森林模型和SHAP方法评估了地理空间背景信息在出行方式检测中的贡献，实验结果表明描述与基础设施网络关系的特征对预测有显着贡献。 |
| [^80] | [ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning.](http://arxiv.org/abs/2305.19426) | 本文提出了Scoped Negation NLI (ScoNe-NLI)基准测试以评估微调和上下文学习策略对语言模型中否定推理表现的影响。研究结果表明，进行许多次微调后，RoBERTa和DeBERTa模型可以成功解决ScoNe-NLI。对于上下文学习方面，大多数提示策略都无法成功，但在嵌入否定推理的短故事的句子完成测试中，InstructGPT是成功的。 |
| [^81] | [Quantifying Overfitting: Evaluating Neural Network Performance through Analysis of Null Space.](http://arxiv.org/abs/2305.19424) | 本研究量化了过拟合问题，通过神经网络最后一层的零空间分析来评估模型性能并保证了神经网络的隐私和泛化性能。 |
| [^82] | [Data and Knowledge for Overtaking Scenarios in Autonomous Driving.](http://arxiv.org/abs/2305.19421) | 本文提出了一个新的超车场景数据集和知识表示模型，以帮助自动驾驶车辆进行规划和控制。 |
| [^83] | [KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned Stochastic Optimization.](http://arxiv.org/abs/2305.19416) | 本文提出了一种名为KrAD的新的Kronecker分解预处理方法，用于降低深度学习中二阶优化器的内存和计算资源要求。通过KrADagrad方法，避免了64位精度要求，并在32位精度下表现更好。 |
| [^84] | [Efficient Training of Energy-Based Models Using Jarzynski Equality.](http://arxiv.org/abs/2305.19414) | 本文介绍了一种通过使用Jarzynski平等式和顺序蒙特卡罗采样绕过标准对比散度算法中的不可控逼近误差，有效训练能量基模型的方法。 |
| [^85] | [FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection.](http://arxiv.org/abs/2305.19407) | 本文提出了一个深度强化学习框架FRAMM，用于公平的临床试验选址，该框架可以解决数据缺失和优化招募和多样性之间的权衡。在真实的临床试验数据集上，实验结果表明FRAMM能够实现公平的试验选址，并在不降低招募率的情况下提高多样性。 |
| [^86] | [Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI.](http://arxiv.org/abs/2305.19404) | 本文提出了一种“分歧感知”的双流增量学习框架，适用于针对不断演化的目标域数据进行分割任务，解决了分布变化、未见过结构和训练数据缺失等挑战。 |
| [^87] | [DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling.](http://arxiv.org/abs/2305.19395) | DyGen是一个动态增强的生成模型，使用嵌入空间中的动态模式可以改善从噪声标签中学习的精度，同时使用共规正则化机制来最小化潜在噪声标签和先验的影响，展示了最先进的性能。 |
| [^88] | [Synaptic Weight Distributions Depend on the Geometry of Plasticity.](http://arxiv.org/abs/2305.19394) | 计算神经科学的研究表明，突触权重分布取决于突触可塑性的几何形态，进而表明实验观测到的对数正态权重分布与标准的梯度下降模型不一致，可能说明大脑中使用的是非欧几里得距离。 |
| [^89] | [Deep Clustering with Incomplete Noisy Pairwise Annotations: A Geometric Regularization Approach.](http://arxiv.org/abs/2305.19391) | 本文通过研究逻辑DCC损失函数的理论性质，提出了一种基于几何因子分析的新损失函数用以抵御嘈杂的注释并进一步提高聚类性能。 |
| [^90] | [Inter Subject Emotion Recognition Using Spatio-Temporal Features From EEG Signal.](http://arxiv.org/abs/2305.19379) | 本研究提出了一种可跨主体对EEG信号进行情感分类的模型，使用了常规、深度和可分离卷积层，达到了73.04％的准确性。 |
| [^91] | [Benign Overfitting in Deep Neural Networks under Lazy Training.](http://arxiv.org/abs/2305.19377) | 本论文证明了在惰性训练期间，超参数化深度神经网络存在良性过拟合现象，可在数据分布良好分离时实现贝叶斯最优测试误差。通过插值平滑可以带来更好的泛化性能。 |
| [^92] | [Sensitivity Analysis of RF+clust for Leave-one-problem-out Performance Prediction.](http://arxiv.org/abs/2305.19375) | 本文提出了一种新的方法来解决机器学习中的LOPO问题，通过调整距离加权和引进基于特征的重要性，实验结果表明在预测准确性和推广能力方面比RF + clust更优。 |
| [^93] | [Compositional diversity in visual concept learning.](http://arxiv.org/abs/2305.19374) | 本文研究了人类如何利用组合性进行视觉概念学习，开发了一个程序归纳模型来生成候选视觉图形，发现人类和模型都可以进行多样的组合泛化。 |
| [^94] | [Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure.](http://arxiv.org/abs/2305.19373) | 本文采用主题建模技术对1200例心力衰竭患者的诊断编码和程序报告中存在的主题进行识别，旨在从中识别心力衰竭的临床表型并预测病人住院时间。 |
| [^95] | [Blockwise Parallel Transformer for Long Context Large Models.](http://arxiv.org/abs/2305.19370) | 本文提出了块级并行Transformer方法，以最小化内存成本，能够处理长序列，并且可以处理比先前的内存高效方法更长32倍的训练序列。 |
| [^96] | [Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network.](http://arxiv.org/abs/2305.19366) | 本文提出了在单一生成流网络中联合建模贝叶斯网络结构和参数的方法，包括非离散样本空间，提高了贝叶斯网络局部概率模型的灵活性。 |
| [^97] | [Stable Anisotropic Regularization.](http://arxiv.org/abs/2305.19358) | 本文提出了一种新颖的正则化方法I-STAR，可以增加模型的稳定性，提高性能，并改善自然语言处理中的组合表示问题。 |
| [^98] | [Uncovering multifunctional mechano-intelligence in and through phononic metastructures harnessing physical reservoir computing.](http://arxiv.org/abs/2305.19354) | 在机械领域实现智能的新方法——利用声子元结构实现多功能机械-智能；通过物理储库计算框架，在机械领域直接实现计算能力和各种智能元素，提供了具体的整合不同智能元素的系统基础。 |
| [^99] | [Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte Carlo.](http://arxiv.org/abs/2305.19350) | 提出了一种基于随机梯度马尔科夫链蒙特卡罗的方法来解决非凸贝叶斯学习问题，具有理论保证。 |
| [^100] | [On Riemannian Projection-free Online Learning.](http://arxiv.org/abs/2305.19349) | 本文提出了一种针对非凸约束集情况下的曲线空间在线测地凸优化的无投影算法，获得了次线性遗憾保证。 |
| [^101] | [Epilepsy Seizure Detection: Anatomy and Analysis.](http://arxiv.org/abs/2305.19347) | 该研究提出了一种通用、经济、非侵入性的癫痫检测系统，基于简单的实时kNN机器学习，可在不到四秒的训练时间内定制和适应个人用户，并具有94.5%的平均准确率。 |
| [^102] | [HiGen: Hierarchical Graph Generative Networks.](http://arxiv.org/abs/2305.19337) | HiGen是一种新颖的图形生成网络，能够以粗到细的方式捕捉图形的层次结构，并使用多项式分布来生成具有整数值的子图的边权。它能够有效地捕捉图形的局部和全局属性，并实现了最先进的性能。 |
| [^103] | [Mitigating Test-Time Bias for Fair Image Retrieval.](http://arxiv.org/abs/2305.19329) | 本文提出了后置偏差缓解（PBM）技术，解决了如何在中性文本查询的情况下实现公平的图像检索。该方法在实际数据集中实现了最低的偏差。 |
| [^104] | [A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks.](http://arxiv.org/abs/2305.19306) | 本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。 |
| [^105] | [Audio classification using ML methods.](http://arxiv.org/abs/2305.19304) | 本文介绍了如何利用机器学习方法对音乐类型进行分类，并展示了使用不同算法的分类结果。分类器将音乐类型分为古典音乐和金属乐。 |
| [^106] | [MAGNet: Motif-Agnostic Generation of Molecules from Shapes.](http://arxiv.org/abs/2305.19303) | MAGNet是一种基于图的分子生成技术，能够解决现有方法无法代表超出已知motif集之外的分子结构的问题，并在标准基准测试中表现出竞争力。 |
| [^107] | [On the Choice of Perception Loss Function for Learned Video Compression.](http://arxiv.org/abs/2305.19301) | 本文研究了在学习视频压缩时，感知损失函数的选择对于重建效果的影响，发现选择 PLF-JD 可以更好地保留跨帧时序相关性，但同时会带来更大的失真惩罚和更难以纠正早期输出帧中的错误。 |
| [^108] | [MLOps: A Step Forward to Enterprise Machine Learning.](http://arxiv.org/abs/2305.19298) | 本文讨论了机器学习运维（MLOps）在企业级机器学习中的重要性和适用性，并详细解释了MLOps工作流程、自动化流程的不同成熟度水平以及各种底层技术。最后，用一个物体检测服务的企业级MLOps项目的详细示例来解释技术在现实场景中的工作流程。 |
| [^109] | [Pointwise Representational Similarity.](http://arxiv.org/abs/2305.19294) | 本文介绍了 PNKA，一种可以量化单个输入在两个表示空间中的相似度的方法，填补了全局相似性度量不能局部调查表示的空白。 |
| [^110] | [Revisiting Random Forests in a Comparative Evaluation of Graph Convolutional Neural Network Variants for Traffic Prediction.](http://arxiv.org/abs/2305.19292) | 本论文比较了图卷积神经网络和传统方法随机森林在交通预测中的性能，证明将矩阵分解、注意力和位置特定的模型权重单独或集体加入GCNN可以提高其整体性能，且在准确性和鲁棒性方面GCNN表现更出色。 |
| [^111] | [Perimeter Control Using Deep Reinforcement Learning: A Model-free Approach towards Homogeneous Flow Rate Optimization.](http://arxiv.org/abs/2305.19291) | 该论文提出了一种基于深度强化学习的模型无周界控制框架，通过交通信号控制代理与模拟车辆进行交互，实现同质流量优化，比传统的基于模型的方法更有效。 |
| [^112] | [Global Layers: Non-IID Tabular Federated Learning.](http://arxiv.org/abs/2305.19290) | 本文提出了一个新颖的偏模型个性化方法Global Layers (GL)，该方法是目前唯一一种能够支持客户端专有特征和类别的FL方法，在两个新的基准实验中，GL的性能优于联邦平均和仅本地训练的性能，甚至有些客户端的性能比他们的集中式基线还要好。 |
| [^113] | [Large language models improve Alzheimer's disease diagnosis using multi-modality data.](http://arxiv.org/abs/2305.19280) | 本研究使用大型语言模型提高对非影像数据的应用能力，并在ADNI数据集上实现了SOTA结果。 |
| [^114] | [Data-Driven Games in Computational Mechanics.](http://arxiv.org/abs/2305.19279) | 本文提出了一种新型的非合作数据驱动游戏，其可以从数据中确定有效的材料定律，简化实际实施，并且与监督式机器学习方法不同，没有假设和参数。 |
| [^115] | [Probabilistic Computation with Emerging Covariance: Towards Efficient Uncertainty Quantification.](http://arxiv.org/abs/2305.19265) | 本文开发了一个高效、可解释的概率计算框架，通过监督平均值优化任务目标，从非线性耦合中自发出现的无监督协方差忠实地捕捉了与模型预测的不确定性相关的信息。 |
| [^116] | [Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks.](http://arxiv.org/abs/2305.19130) | 采用空间转换网络模块，可使舌部超声图像为基础的无声语音接口模型快速适应到不同的用户和会话，且能显著降低均方误差。 |
| [^117] | [IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design Algorithms in Nanophotonics.](http://arxiv.org/abs/2305.18978) | 提出了一个纳米光子学器件反向设计基准测试，以帮助人们进行易于理解和可重复的科学设计。开发了一个开源工具箱IDToolkit，其中包含了模拟和优化器模块以及后处理结果的函数，可用于与基准方法比较算法。 |
| [^118] | [Convolutional Monge Mapping Normalization for learning on biosignals.](http://arxiv.org/abs/2305.18831) | 本研究提出了一种新的方法：基于卷积蒙日映射归一化 (CMMN)，用于信号众多但变异性较大的生物医学数据上，能自适应调整幅度、滤波器的功率谱密度，通过最优输运映射和 barycenters 实现个体测试时间适应新数据，且显著提升信号分类和检测性能。 |
| [^119] | [HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance.](http://arxiv.org/abs/2305.18766) | 该论文提出了一种高保真度的文本到3D图像合成方法，并引入了先进的扩散引导策略。通过对NeRF渲染图像进行辅助深度监督和规范化密度场来提高3D几何表示。实验证明该方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。 |
| [^120] | [Task-Equivariant Graph Few-shot Learning.](http://arxiv.org/abs/2305.18758) | 本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。 |
| [^121] | [Dimensionality Reduction for General KDE Mode Finding.](http://arxiv.org/abs/2305.18755) | 本文提出了一种降维算法，可以适用于高斯混合模型和更广泛的核函数，结合梯度下降可制定出有效的实用启发式算法。 |
| [^122] | [NUNO: A General Framework for Learning Parametric PDEs with Non-Uniform Data.](http://arxiv.org/abs/2305.18694) | NUNO 是一个用于处理非均匀数据的高效算子学习框架，在三维 PDE 问题中取得了显著的效果提升。 |
| [^123] | [An Analytic End-to-End Deep Learning Algorithm based on Collaborative Learning.](http://arxiv.org/abs/2305.18594) | 本文提出了一种平滑激活函数的全连接神经网络端到端深度学习收敛分析方法，避免了潜在的抖动问题，并且可以使用协作学习进一步结合多个网络的优势。 |
| [^124] | [Trompt: Towards a Better Deep Neural Network for Tabular Data.](http://arxiv.org/abs/2305.18446) | Trompt是一种新颖的深度神经网络结构，其中分离了表格数据学习策略，它通过提示学习的方式来调整大型预先训练的模型，以便更好地能够处理表格数据，并取得了很好的效果。 |
| [^125] | [Dink-Net: Neural Clustering on Large Graphs.](http://arxiv.org/abs/2305.18405) | Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。 |
| [^126] | [Understanding Predictive Coding as an Adaptive Trust-Region Method.](http://arxiv.org/abs/2305.18188) | 研究将预测编码（PC）作为自适应信任区域（TR）算法的理论模型，并发现它可以比反向传播（BP）更快地逃脱鞍点。 |
| [^127] | [Modeling Dynamic Environments with Scene Graph Memory.](http://arxiv.org/abs/2305.17537) | 本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。 |
| [^128] | [The Curse of Recursion: Training on Generated Data Makes Models Forget.](http://arxiv.org/abs/2305.17493) | 使用生成数据进行训练会导致模型不可逆的缺陷并且使得原始内容分布的尾部消失，这种效应称为模型折叠。我们证明了这种现象在所有学习生成模型中都存在，必须认真对待。 |
| [^129] | [A Framework For Refining Text Classification and Object Recognition from Academic Articles.](http://arxiv.org/abs/2305.17401) | 本文提出了一种结合基于规则的方法和机器学习的框架，旨在解决从学术论文中提炼文本分类和对象识别的问题。 |
| [^130] | [On the Computational Power of Decoder-Only Transformer Language Models.](http://arxiv.org/abs/2305.17026) | 本篇论文研究了解码器Transformer语言模型的计算普适性，表明即使只有单层和单注意力头，仍然具有图灵完备性，其中单词嵌入的稀疏性/可压缩性是必要条件。 |
| [^131] | [HUB: Guiding Learned Optimizers with Continuous Prompt Tuning.](http://arxiv.org/abs/2305.16823) | 本文提出了一种名为HUB的混合更新策略，通过结合学习优化器和手工设计的优化器，提高了学习优化器泛化性能。 |
| [^132] | [AdaPlanner: Adaptive Planning from Feedback with Language Models.](http://arxiv.org/abs/2305.16653) | LLM代理可以通过Adaplanner自适应改进自己的计划以应对环境反馈，为此提出计划内外的改进策略以及代码风格的LLM提示结构和技能发现机制。 |
| [^133] | [Reverse Engineering Self-Supervised Learning.](http://arxiv.org/abs/2305.15614) | 本文逆向工程了自监督学习（SSL）训练表示，发现SSL训练过程中的正则化项本质上促进了样本基于语义标签的聚类。SSL训练的表示与语义类别更加接近，对齐在训练过程中增加，而且在网络深度加深时增加。 |
| [^134] | [Timeseries-aware Uncertainty Wrappers for Uncertainty Quantification of Information-Fusion-Enhanced AI Models based on Machine Learning.](http://arxiv.org/abs/2305.14872) | 本论文提出了一种基于机器学习的信息融合增强人工智能模型的时序感知不确定性封装器，可用于对时序数据进行可靠的不确定性估计，同时通过信息融合和时序感知的输入质量特征提高模型准确性和不确定性估计的质量。 |
| [^135] | [Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?.](http://arxiv.org/abs/2305.14035) | 本文研究了利用人类语音自监督神经表示学习来分析生物声学信号的交叉可迁移性，在狨猴叫声识别和检测中取得了成功，未来该方法可有效应用于该领域的研究。 |
| [^136] | [Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction.](http://arxiv.org/abs/2305.13059) | 研究提出了一种简单的序列到序列模型KGT5-context，通过加入查询实体的直接邻居信息实现知识图谱链接预测的高性能，并与其他方法相比取得了最先进的表现。 |
| [^137] | [Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems.](http://arxiv.org/abs/2305.12102) | 本文介绍了一种名为“特征复用”的框架，它使用单一的表示空间 能够高效有效地学习高质量的特征嵌入，同时区分不同的分类特征。通过在多个公共数据集和新数据集“Web-Available Image Search (WAIS)”上的测试，我们展示了这种方法的优于现有技术的表现。 |
| [^138] | [A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism.](http://arxiv.org/abs/2305.10631) | 提出了一种基于多尺度特征金字塔网络和双重注意力机制的子腹部MRI图像分割算法，使用空洞卷积和多尺度特征金字塔编码以避免语义差距，设计双重注意力机制以保持空间信息并减少错位。 |
| [^139] | [ZeroFlow: Fast Zero Label Scene Flow via Distillation.](http://arxiv.org/abs/2305.10424) | ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。 |
| [^140] | [Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation.](http://arxiv.org/abs/2305.07247) | 本论文提出了一种基于近似投影的Schr\"odinger bridge算法，它能够应用于概率时间序列填充，并在医疗保健和环境数据方面实现最先进的结果。 |
| [^141] | [ImageBind: One Embedding Space To Bind Them All.](http://arxiv.org/abs/2305.05665) | ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。 |
| [^142] | [Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling.](http://arxiv.org/abs/2305.04111) | 本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。 |
| [^143] | [On the nonlinear correlation of ML performance between data subpopulations.](http://arxiv.org/abs/2305.02995) | 在不同数据子群体间，机器学习模型的内部准确性和外部准确性之间的相关性是非线性的，呈现出“月亮形”的相关性。 |
| [^144] | [Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows.](http://arxiv.org/abs/2305.02164) | 本文提出了两个重要贡献：一是提出了条件切片Wasserstein流（CSWF），可以实现非参数条件建模，二是将局部连接和多尺度表示等视觉研究启发的技术引入到SWF中，从而大大提高了图像建模的效率和质量。 |
| [^145] | [Moccasin: Efficient Tensor Rematerialization for Neural Networks.](http://arxiv.org/abs/2304.14463) | 本文提出了一种名为Moccasin的新型约束编程形式，用于实现在内存预算下最小化计算图的执行时间，相较于最近的研究，该方法显著提高了效率，并成功应用于神经网络的高效张量重算。 |
| [^146] | [Regression with Sensor Data Containing Incomplete Observations.](http://arxiv.org/abs/2304.13415) | 本文提出了一种能处理不完整观测传感器数据的回归算法，解决了标签值由于不完整观测导致学习结果偏低的问题。 |
| [^147] | [Provable benefits of general coverage conditions in efficient online RL with function approximation.](http://arxiv.org/abs/2304.12886) | 研究者对在线强化学习提出了一种新的一般覆盖条件，并发现更多的覆盖条件，提高了在线强化学习的样本效率和表现，同时阐明良好的覆盖条件仍然有益于获得最优解。 |
| [^148] | [Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning.](http://arxiv.org/abs/2304.03916) | 本文提出了一种使用多模态对比损失函数的方法，通过在微调期间检测和明确区分受影响类别的错误属性，缓解多模态模型的错误相关性，同时提高模型精度和指向目标领域的有意义特征。 |
| [^149] | [Conformal Regression in Calorie Prediction for Team Jumbo-Visma.](http://arxiv.org/abs/2304.03778) | 本文提出一种新的符合性回归方法，通过预测动力和速度来为自行车比赛中的每个骑手提供卡路里需求的估计。 |
| [^150] | [Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning.](http://arxiv.org/abs/2303.17649) | 本文介绍了一种将英文GPT模型对齐到西班牙语的小封闭领域中的方法，该方法使用了奖励模型来改进答案的解码和生成，在问答任务中取得了良好的结果。 |
| [^151] | [Polarity is all you need to learn and transfer faster.](http://arxiv.org/abs/2303.17589) | 本文从权重极性的角度提出了一个思路：发育过程会初始化有优势极性配置的自然智能，当自然智能成长和学习时，突触的大小发生变化，但极性基本保持不变，如果权重极性被适当地设置在先，那么网络学习所需的时间和数据将会减少，从而增加学习和转移的效率。 |
| [^152] | [Adaptive Conformal Prediction by Reweighting Nonconformity Score.](http://arxiv.org/abs/2303.12695) | 该论文提出了一种新方法，利用分位数回归森林来学习非拟合分数的分布，并利用其权重分配更多的重要性给残差与测试点相似的样本，从而实现更符合模型的不确定性的预测区间。 |
| [^153] | [Unit Scaling: Out-of-the-Box Low-Precision Training.](http://arxiv.org/abs/2303.11257) | 本论文提出了一种称为“单位缩放”的方法，该方法可简化低精度数字格式的操作并提高训练效率。该方法通过在初始化时将所有权重、激活函数和梯度的单位差变为1来解决低精度训练范围的问题，与其他方法不同，它不需要多次运行也没有显著的计算开销。 |
| [^154] | [A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges.](http://arxiv.org/abs/2303.07275) | 该论文从图的角度审查了提示方法，将提示函数与图知识相结合，以解决在复杂任务中设计提示的挑战，在此基础上组织现有工作，并描述了应用和未来挑战。 |
| [^155] | [Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?.](http://arxiv.org/abs/2303.04143) | 该论文提出了一个可以预测其他神经网络高质量ImageNet参数的神经网络，通过使用预测参数进行初始化，能够提高多种ImageNet模型的训练速度，并且在转移到其他数据集时可以更快地收敛并达到竞争力的最终性能。 |
| [^156] | [On Hierarchical Multi-Resolution Graph Generative Models.](http://arxiv.org/abs/2303.03293) | 本文提出一种新颖的分层多分辨率图生成模型，能递归地生成多个层次的社区结构，并符合训练数据分布。该方法由粗到细地生成图，同时具有高度的可扩展性，提升了生成性能。 |
| [^157] | [DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network.](http://arxiv.org/abs/2303.02165) | DeepMAD是一种基于数学的新框架，能够在系统化的方式中设计出高性能的CNN模型，解决了CNN网络设计过程中的挑战性问题。 |
| [^158] | [Dropout Reduces Underfitting.](http://arxiv.org/abs/2303.01500) | 本研究证明dropout不仅可以防止神经网络过拟合，还可以缓解欠拟合问题。在训练初期采用early dropout方法，可以减少小批次梯度的方向差异，缓解SGD中的随机性，从而提高模型的训练效果。 |
| [^159] | [Consistency Models.](http://arxiv.org/abs/2303.01469) | 提出了一种支持一步生成且支持零样本编辑的生成模型——一致性模型，它们能够通过直接将噪声映射到数据来生成高质量样本，支持快速的一步生成，且仍然支持多步抽样以提高样本质量。 |
| [^160] | [Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation.](http://arxiv.org/abs/2303.00529) | 提出了一种将基于DNN的乘法遮罩扩展到深度子带滤波以在时频域中实现语音恢复的方案，可通用于提供在时频域中的遮罩的任何深度神经网络，相比乘法遮罩去混响性能更好，在去噪性能方面几乎没有影响。 |
| [^161] | [GNOT: A General Neural Operator Transformer for Operator Learning.](http://arxiv.org/abs/2302.14376) | 提出了一种通用神经运算符Transformer——GNOT，用于解决机器学习中学习偏微分方程的解算子的问题，并通过设计新颖的异构归一化注意力层和引入几何门控机制来增强模型的灵活性和解决多尺度问题。在多个领域的具有挑战性的数据集上进行广泛实验，取得了显着的改进。 |
| [^162] | [On Differentially Private Federated Linear Contextual Bandits.](http://arxiv.org/abs/2302.13945) | 本文提出了一种新的算法框架，可以解决跨桶差分隐私联邦线性背景下的上下文匹配问题，并修复了现有算法中存在的隐私保护失效、因噪声计算错误而导致的不正确的遗憾界和不可信的通信成本等问题。 |
| [^163] | [Domain Adaptive Decision Trees: Implications for Accuracy and Fairness.](http://arxiv.org/abs/2302.13846) | 研究提出领域适应的决策树(DADT)来提高机器学习模型在目标人群中的精确性，避免偏见和降低某些人群在目标人群中受到不利影响。 |
| [^164] | [DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule.](http://arxiv.org/abs/2302.12022) | 我们提出了一个参数-free 的动态 SGD 步长公式，称为梯度距离公式（DoG）， 它没有“学习率”参数，但是在局部有界的随机梯度优化中拥有强大的无参数收敛性，并在广泛的视觉和语言转移学习任务中的表现与有调整学习率的 SGD 相当接近。 |
| [^165] | [K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs.](http://arxiv.org/abs/2302.11996) | 本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。 |
| [^166] | [IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness.](http://arxiv.org/abs/2302.10896) | 本文提出了一种名为IB-RAR的正则化方法，利用信息瓶颈来增强对抗性训练和非对抗性训练方法的鲁棒性，并通过过滤不必要的特征来提高准确性。 |
| [^167] | [MaskedKD: Efficient Distillation of Vision Transformers with Masked Images.](http://arxiv.org/abs/2302.10494) | MaskedKD提出了一种通过遮蔽图像块来显著降低Vision Transformer (ViT)蒸馏成本的方法，而不影响学生模型的预测准确性。 |
| [^168] | [Simple Disentanglement of Style and Content in Visual Representations.](http://arxiv.org/abs/2302.09795) | 该论文提出了一个简单的后处理框架，用于分离学习到的表征中的内容和风格，在领域泛化中表现出显著的提升。 |
| [^169] | [Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most.](http://arxiv.org/abs/2302.09195) | 该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。 |
| [^170] | [Measuring Equality in Machine Learning Security Defenses.](http://arxiv.org/abs/2302.08973) | 本文研究了机器学习安全防御方法的平等性能问题，提出了一种简单的平等度量和分析框架，鼓励进一步探索公平性在该领域的应用。 |
| [^171] | [Federated Auto-weighted Domain Adaptation.](http://arxiv.org/abs/2302.05049) | 这篇论文提出了一种用于联邦领域自适应的新聚合规则-联邦梯度投影。在此基础上，开发了一个自动加权方案，用于最优地结合源和目标梯度，以解决在数据稀缺和领域转移时常见的技术失败的问题。 |
| [^172] | [Cooperative Open-ended Learning Framework for Zero-shot Coordination.](http://arxiv.org/abs/2302.04831) | 该论文提出了一个COLE框架，通过构建合作游戏的开放式目标，从图论的角度评估和确定每个策略的协作能力，以有效地解决零样本协调中的合作不兼容性问题。 |
| [^173] | [On Sampling with Approximate Transport Maps.](http://arxiv.org/abs/2302.04763) | 本研究探讨了两种基于传输映射的抽样方法，研究结果表明，基于流的提议可以处理多峰目标，在高维度和训练不良的情况下使用依赖于重新参数化的方法更加稳健。 |
| [^174] | [Generalizing Neural Wave Functions.](http://arxiv.org/abs/2302.04168) | 本文提出了一种名为“图学习轨道嵌入”的神经网络重新参数化方法，可以泛化神经波函数到不同的分子，同时还提出了一个大小一致的波函数Ansatz，名为“分子轨道网络”，可以联合求解不同分子的Schrödinger方程，提高了求解速度和精度。 |
| [^175] | [Efficient Online Reinforcement Learning with Offline Data.](http://arxiv.org/abs/2302.02948) | 本文研究了利用离线数据进行高效在线强化学习的方法，证明了现有的离线策略方法能够应用于在线学习，提出了一些最少但重要的更改，来实现可靠的性能并提供了实践中可应用的建议。 |
| [^176] | [CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets.](http://arxiv.org/abs/2302.02551) | 本文提出了 CHiLS，它利用分层标签集进行零样本图像分类，通过产生更具信息性的类别名称，在获得更精细的分类效果的同时，还能使用更少的有标签数据。 |
| [^177] | [Multi-View Masked World Models for Visual Robotic Manipulation.](http://arxiv.org/abs/2302.02408) | 本文介绍了一个多视图遮蔽自编码器，用于重构出随机遮蔽视点的像素，并学习基于自编码器表示的世界模型。该方法在多视图控制和单视图控制场景中展示了其有效性，能够训练出具有强视点随机化的策略，并应用于实际机器人任务中。 |
| [^178] | [Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models.](http://arxiv.org/abs/2301.13826) | 该论文提出了一种基于注意力的文本到图像扩散模型的语义引导方法，名为参与兴奋，在推理时间内干预生成过程以改善生成图像的信实性和完整性，并解决了传统扩散模型在图像语义生成中可能存在的失灵现象。 |
| [^179] | [UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers.](http://arxiv.org/abs/2301.13741) | UPop是一种通用的视觉语言Transformer压缩框架，采用统一和渐进式剪枝方法，可自动分配剪枝比率，实现更高的压缩比率。 |
| [^180] | [Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs.](http://arxiv.org/abs/2301.12950) | 本论文提出了一种名为HPRL的分层编程强化学习框架，通过学习程序组合的方法实现，能够产生具有人类可解释性并且在评估候选方案时可以准确奖励和惩罚的策略。 |
| [^181] | [On Enhancing Expressive Power via Compositions of Single Fixed-Size ReLU Network.](http://arxiv.org/abs/2301.12353) | 本文探讨了深度神经网络的表达能力，通过组合单个固定大小RELU网络，证明了其具有惊人的表达能力，尤其是在逼近具有$1-$Lipschitz连续性和一般连续性的函数时。 |
| [^182] | [Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases.](http://arxiv.org/abs/2301.12017) | 本文研究了在语言模型中采用INT4权重和激活量化的可行性，并开发了高度优化的W4A4编码器推断管道，支持不同的量化策略。使用W4A4可以实现模型在延迟方面的显著提高。 |
| [^183] | [Pre-training for Speech Translation: CTC Meets Optimal Transport.](http://arxiv.org/abs/2301.11716) | 本文提出了一种基于CTC和最优传输的语音翻译预训练方法，可以有效减小语音和文本模态之间的差距，提高最终的ST准确性。 |
| [^184] | [Image Restoration with Mean-Reverting Stochastic Differential Equations.](http://arxiv.org/abs/2301.11699) | 本文提出了一种通用图像恢复的方法，通过使用均值回归随机微分方程将一个高质量图像转换为降级图像并加入高斯噪声，然后使用最大似然目标来学习逆转轨迹，能够在不依赖于任何特定先验知识的情况下恢复低质量图像的原始状态。 |
| [^185] | [Learning the Dynamics of Sparsely Observed Interacting Systems.](http://arxiv.org/abs/2301.11647) | 本论文解决了学习稀疏观测交互系统的动力学问题，将其作为解的学习控制微分方程（CDE），利用签名理论将非线性问题转化为高维线性回归，具有明确的依赖于个体特定采样方案的预测误差的oracle界限。证明了该方法优于现有算法回收完整时间序列，且计算成本较低。 |
| [^186] | [Solving Richly Constrained Reinforcement Learning through State Augmentation and Reward Penalties.](http://arxiv.org/abs/2301.11592) | 本文提出了一种基于状态增强和奖励惩罚的约束强化学习新方法，相较于现有技术，在保证安全性和目标性能方面表现更好。 |
| [^187] | [Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification.](http://arxiv.org/abs/2301.11562) | 在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。 |
| [^188] | [SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning.](http://arxiv.org/abs/2301.11520) | 本文提出了一种称为SNeRL的语义感知神经辐射场，它通过学习3D-aware的隐式表示来进行强化学习，并在基于像素的以及最新的3D感知表示方法中表现出更好的性能。 |
| [^189] | [Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning.](http://arxiv.org/abs/2301.11321) | 提出一种轨迹感知的资格追踪多步运算符，可以同时表达每个决策和轨迹感知的方法，并解决了被完全裁剪的资格追踪无法逆转的问题。 |
| [^190] | [Improving Graph Generation by Restricting Graph Bandwidth.](http://arxiv.org/abs/2301.10857) | 本论文提出一种通过限制图的带宽来提高现有图生成模型输出空间的方法，该方法可以提高生成可扩展性和质量，并在不增加复杂性或减少表达能力的情况下与现有图生成方法兼容。 |
| [^191] | [Explaining the effects of non-convergent sampling in the training of Energy-Based Models.](http://arxiv.org/abs/2301.09428) | 本文证明，使用非持续运行的EBM训练可以通过一个精确的动态过程完美地复制数据集的一组统计信息，而不是通过平衡测度的水平，这为使用EBM作为扩散模型奠定了基础。 |
| [^192] | [Computationally Efficient 3D MRI Reconstruction with Adaptive MLP.](http://arxiv.org/abs/2301.08868) | 本文提出了自适应多层感知器的计算高效三维MRI重建方法，混合使用CNN和dMLP模块。相比于当前基于CNN的方法，本方法可以更好地捕捉长距离信息，并且能够适应较大的图像尺寸和GPU内存限制。 |
| [^193] | [Uncertainty in Real-Time Semantic Segmentation on Embedded Systems.](http://arxiv.org/abs/2301.01201) | 本文提出了一种结合贝叶斯回归和动量传播的预测方法，能够实时在嵌入式硬件上产生有意义的不确定性，从而使语义分割模型在自动驾驶和人机交互等领域的实时应用成为可能。 |
| [^194] | [Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing.](http://arxiv.org/abs/2301.00006) | 该论文提出了一种多项选择众包任务的模型，该模型可以恢复最令人困惑的答案和混淆概率。在该模型下，提出了一个两阶段推断算法来推断最有可能的答案和混淆概率。 |
| [^195] | [Representation Learning in Deep RL via Discrete Information Bottleneck.](http://arxiv.org/abs/2212.13835) | 本研究提出了一种名为RepDIB的表征学习方法，在强化学习中通过离散信息瓶颈构建隐状态，可以有效地去除感知信息中的噪声和无关信息，实验表明该方法能够提高性能。 |
| [^196] | [Continual Contrastive Finetuning Improves Low-Resource Relation Extraction.](http://arxiv.org/abs/2212.10823) | 本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。 |
| [^197] | [Transformers learn in-context by gradient descent.](http://arxiv.org/abs/2212.07677) | 本文提出，训练Transformer模型应用于自回归目标问题时，与基于梯度的元学习的形式密切相关，通过梯度下降学习模型的“底层优化程序”的机制，在回归问题的领域中从机械的角度理解了Transformers模型中上下文学习的内部机制。 |
| [^198] | [Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator.](http://arxiv.org/abs/2212.06751) | 本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。 |
| [^199] | [The Stable Artist: Steering Semantics in Diffusion Latent Space.](http://arxiv.org/abs/2212.06013) | 本文介绍了一种名为"稳定的美术家"的图像编辑方法，其中包含SEGA以及潜在遍历等组成部分，使得用户可以实现在图像生成过程中的细粒度控制，可以微妙地编辑图像、改变构图和风格，达到艺术构思优化等目的，实现了在各种文本到图像合成任务中的最先进的定量和定性结果。 |
| [^200] | [Elixir: Train a Large Language Model on a Small GPU Cluster.](http://arxiv.org/abs/2212.05339) | Elixir 提出了一种基于预运行模型分析的自动化高效大模型训练方案，可以将内存使用卸载到 CPU 和 NVMe 存储器中，充分发挥硬件的潜力，实验中表现优于最先进的基准模型。 |
| [^201] | [X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion.](http://arxiv.org/abs/2212.03863) | 本文利用零样本识别和text2image模型，重新思考了可扩展的复制粘贴，实现了利用不同物体类别的图像进行实例分割，以获得更高的性能 |
| [^202] | [Concentration Phenomenon for Random Dynamical Systems: An Operator Theoretic Approach.](http://arxiv.org/abs/2212.03670) | 本文提供了一种算子理论方法，解决了离散时间马尔可夫链的浓度现象问题，并证明了可逆性在浓度现象中的作用。 |
| [^203] | [Yggdrasil Decision Forests: A Fast and Extensible Decision Forests Library.](http://arxiv.org/abs/2212.02934) | Yggdrasil Decision Forests是一种快速且可扩展的决策森林库，旨在为研究和生产工作提供支持。该库的设计原则为易用性、安全性、模块化和高层抽象以及与其他机器学习库的集成，并在经典机器学习问题上展示了其使用，并在基准测试中取得了成功。 |
| [^204] | [Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models.](http://arxiv.org/abs/2212.02024) | 本文提出了一种基于扩散模型的像素级指导的细粒度图像编辑方法，通过少量带注释数据训练像素分类器和映射编辑操作快速生成满足用户意图的图像，并且编辑结果质量和速度均优于基于GAN的方法。 |
| [^205] | [Pareto Regret Analyses in Multi-objective Multi-armed Bandit.](http://arxiv.org/abs/2212.00884) | 本文研究了多目标多臂赌博机中的Pareto最优性，提出了对抗性多目标多臂赌博机的表述和定义了Pareto后悔，提出了新算法，分析证明算法在对抗性环境最优，在随机环境中也接近最优，并将对抗性攻击机制从赌徒推广到多目标领域。 |
| [^206] | [On the Power of Foundation Models.](http://arxiv.org/abs/2211.16327) | 本文通过范畴论探究了基础模型的能力，提出了具有最小所需能力的基础模型可以通过微调和足够的资源来解决前置任务所定义的类别中的下游任务，并且这种能力可以扩展到任何下游任务，只要允许微调且下游任务可在前置任务定义的范畴中表示。 |
| [^207] | [Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature.](http://arxiv.org/abs/2211.15779) | 研究发现图神经网络模型存在过度平滑和过度压缩问题，这一问题与图形曲率相关，作者利用Ollivier-Ricci曲率提出了Batch Ollivier-Ricci Flow算法，解决了这一问题。 |
| [^208] | [Unifying Label-inputted Graph Neural Networks with Deep Equilibrium Models.](http://arxiv.org/abs/2211.10629) | 本文将标签输入的GNN和隐式GNN统一起来，并提出了一种IGNN中的隐式微分方法，使得标签无限传播变得可行。 |
| [^209] | [PAD-Net: An Efficient Framework for Dynamic Networks.](http://arxiv.org/abs/2211.05528) | PAD-Net是一个部分动态网络的框架，将冗余的动态参数转换为静态参数，提高了动态网络的效率和适用性。 |
| [^210] | [Forecasting Local Behavior of Self-organizing Many-agent System without Reconstruction.](http://arxiv.org/abs/2210.17289) | 本文提出了一种CNN-LSTM模型，可以在不需要重建所有代理状态的情况下，预测自组织多代理系统中特定代理的状态。所提出的模型在森林火灾模型中的实验中表现出更好的性能，可以提高自组织众智系统的效率和可扩展性。 |
| [^211] | [GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity.](http://arxiv.org/abs/2210.16402) | 本文研究了一类分布式优化算法，通过允许具有“次要”数据的客户端在本地执行较少的训练步骤来减轻高通信成本，这一方法可在强凸区域内实现可证明的通信加速。 |
| [^212] | [E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty.](http://arxiv.org/abs/2210.13455) | 本文提出了一种新的方法E-MCTS，通过在MCTS预测中应用表观不确定性估计，实现了模型基强化学习中的深度探索，以及规划探索策略。通过实验证明这种方法在成功的表观不确定性估计和深度探索方面表现优异。 |
| [^213] | [CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models.](http://arxiv.org/abs/2210.09223) | 该论文介绍了一种新的 Correlation Aware Pruner (CAP) 框架，能够在处理高度准确的稀疏视觉模型剪枝方面推动压缩界限，并且能够通过有效的紧缩恢复过程实现压缩后的性能提升。 |
| [^214] | [RARR: Researching and Revising What Language Models Say, Using Language Models.](http://arxiv.org/abs/2210.08726) | RARR是一个可以对不确定信息进行研究和修订的系统，它可以自动找到文本生成模型输出的归因并修正不支持的内容。 |
| [^215] | [Hierarchical Policy Blending as Inference for Reactive Robot Control.](http://arxiv.org/abs/2210.07890) | 该论文提出了一种分层运动生成的方法，结合了反应式策略和规划的优点，在多目标决策问题中提供了可行的路径。 |
| [^216] | [Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization.](http://arxiv.org/abs/2210.07658) | 该论文提出了一种解决长时间跨度机器人策略泛化问题的方法，通过把计划生成和计划执行分开，解决领域差异的问题，并将其分为建立抽象环境、生成抽象轨迹和通过翻译器解决原始任务三个步骤。 |
| [^217] | [How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?.](http://arxiv.org/abs/2210.06516) | 外部数据可能引入攻击者篡改的有毒数据，为了提高毒性防御性能，需要准确地从数据集中筛选出干净的子集。 |
| [^218] | [Are Sample-Efficient NLP Models More Robust?.](http://arxiv.org/abs/2210.06456) | 较低采样效率的NLP模型在特定情况下可能比较高采样效率的模型更为鲁棒，表明通用的提高采样效率方法不太可能改善自然语言处理中的OOD鲁棒性。 |
| [^219] | [Variational Open-Domain Question Answering.](http://arxiv.org/abs/2210.06345) | 本文介绍了变分开放领域（VOD）框架，提出了一种新的自归一化的Rényi变分界的估计方法，可用于训练具有检索增强功能的模型，例如阅读器-检索器BERT-sized模型，并实现了在多项选择医学考试问题上的优异表现。 |
| [^220] | [On the Forward Invariance of Neural ODEs.](http://arxiv.org/abs/2210.04763) | 该论文提出了一种利用控制障碍函数使神经ODE满足输出规范的方法，该方法保证了输出规范，且在训练和推理过程中可以通过改变受限参数/输入进行操作，此外，该方法还创造了额外的鲁棒性。 |
| [^221] | [Why Random Pruning Is All We Need to Start Sparse.](http://arxiv.org/abs/2210.02412) | 本文发现，如果随机掩码的宽度比稀疏性的倒数的对数因子大，则它们可以近似任意目标网络，因此随机剪枝足以启动稀疏神经网络，而且任何从密集到稀疏的训练方案都可以转化为计算上更有效的从稀疏到稀疏的训练方案。 |
| [^222] | [Implicit Neural Spatial Representations for Time-dependent PDEs.](http://arxiv.org/abs/2210.00124) | 本文探讨了使用隐式神经空间表示作为空间离散化的方法来求解时变偏微分方程，该方法不需要任何现有求解器生成的训练数据，具有高精度和稳定性，同时在内存使用方面实现了显著的节省。 |
| [^223] | [Topological Singularity Detection at Multiple Scales.](http://arxiv.org/abs/2210.00069) | 本文提出了一种多尺度拓扑奇异性检测方法，可以评估数据的局部固有维度，并量化点的“流形度”，能够检测复杂空间和图像中的奇异性。 |
| [^224] | [FusionRetro: Molecule Representation Fusion via In-context Reactions for Retrosynthetic Planning.](http://arxiv.org/abs/2209.15315) | 本文提出了一种新的反合成规划框架，能够利用上下文信息来改善预测准确性，从而实现更实用和准确的反合成规划结果。 |
| [^225] | [Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies.](http://arxiv.org/abs/2209.14568) | 本文提出了一种概率框架，为每个观测值提供稀疏的局部反事实规则，并将这些规则聚合成区域反事实规则，以适应不稳定的实现环境，并产生稳健的救济措施。 |
| [^226] | [Forecasting Evolution of Clusters in Game Agents with Hebbian Learning.](http://arxiv.org/abs/2209.06904) | 本文研究了如何通过聚类和预测模型来学习游戏智能体群集的演化，提出了一种基于 Hebbian 学习的无监督聚类方法，结合 LSTM 预测模型，能够在多个场景下准确预测群集演化。 |
| [^227] | [Bayesian Complementary Kernelized Learning for Multidimensional Spatiotemporal Data.](http://arxiv.org/abs/2208.09978) | 本文提出了一种贝叶斯互补核学习（BCKL）框架，它将核化低秩张量分解和短程时空高斯过程相结合，可有效地建模多维时空数据的复杂相关性。 |
| [^228] | [Near-Optimal $\Phi$-Regret Learning in Extensive-Form Games.](http://arxiv.org/abs/2208.09747) | 本文介绍了一种有效和解耦的学习动力学，在多人博弈中能够使每个玩家的触发后悔按$O(\log T)$增长，达到接近最优的收敛速度。并且构建利用了一个更为一般的由具有多项式次数的有理函数导出的不动点结果，以及一个凸包的更细致的后悔电路，保留了RVU合适性的属性。 |
| [^229] | [ILLUME: Rationalizing Vision-Language Models through Human Interactions.](http://arxiv.org/abs/2208.08241) | 本文提出了一种新的调整范例，名为ILLUME，通过人机交互来合理化视觉-语言模型，从而使模型的输出更符合人的思维方式。在使用相对较少的训练数据和最少的人类反馈下，ILLUME表现出与标准监督微调相当的竞争力。 |
| [^230] | [What Can Be Learnt With Wide Convolutional Neural Networks?.](http://arxiv.org/abs/2208.01003) | 本文研究在内核环境下的无限宽卷积神经网络，证明了深层CNN能够适应目标函数的空间尺度，即使数据没有局部结构，深层CNN也可以学习，只要全局结构可以被利用。 |
| [^231] | [Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting.](http://arxiv.org/abs/2207.07827) | 本文提出了一种通用记忆驱动变压器，通过集成多个时间序列特征来驱动预测过程，逐步引入噪声以增强泛化能力，在多个数据集上实现了更优秀的预测性能。 |
| [^232] | [IBP Regularization for Verified Adversarial Robustness via Branch-and-Bound.](http://arxiv.org/abs/2206.14772) | 本文提出了一种基于区间传播的IBP正则化算法，通过在扩大的领域上进行对抗性攻击并结合一种基于廉价区间传播的正则化项来引入网络的可验证性，从而实现对抗训练网络的验证稳健性。 |
| [^233] | [Understanding convolution on graphs via energies.](http://arxiv.org/abs/2206.10991) | 本论文结合能量的概念，证明了带对称滤波器的线性图卷积可以增强高频率，使图神经网络在同质和异质任务中表现更好。 |
| [^234] | [OmniMAE: Single Model Masked Pretraining on Images and Videos.](http://arxiv.org/abs/2206.08356) | 该论文提出了一种基于遮蔽自编码的方法，可以在图像和视频上训练一个简单的单一Vision Transformer模型，而不需要标记数据，该模型的视觉表示可与单模态表示在基准测试上相当或更好，并且使用更简单的架构。 |
| [^235] | [Fair Classification via Domain Adaptation: A Dual Adversarial Learning Approach.](http://arxiv.org/abs/2206.03656) | 通过利用类似域的辅助信息，本论文提出了一种双重对抗学习方法，以实现没有敏感属性的目标域的公平分类。 |
| [^236] | [Shedding a PAC-Bayesian Light on Adaptive Sliced-Wasserstein Distances.](http://arxiv.org/abs/2206.03230) | 本文利用 PAC-Bayesian 理论，提出了自适应切片瓦砾斯坦距离的概括特性界限和一种基于界限的切片分布学习流程，以提高 SW 的判别度。 |
| [^237] | [Saliency Cards: A Framework to Characterize and Compare Saliency Methods.](http://arxiv.org/abs/2206.02958) | 本文介绍了显著性卡片，即结构化文档，描述了显著性方法的操作方式及其在多项评估指标上的性能，并确定了用户选择方法时应考虑的10个属性。 |
| [^238] | [Faster Rates of Convergence to Stationary Points in Differentially Private Optimization.](http://arxiv.org/abs/2206.00846) | 本文研究了在差分隐私下近似利普希茨和平滑函数的静态点问题。提供了新的高效算法和构造，分别在有限和随机情况下比现有算法更快的收敛速度。 |
| [^239] | [Static Scheduling with Predictions Learned through Efficient Exploration.](http://arxiv.org/abs/2205.15695) | 本文研究了单机作业调度的问题，提出了一种基于学习预测的静态调度算法，在类型未知的情况下实现了次线性的过剩成本，尤其在抢占式问题中表现出色，可以在不同作业类型持续时间相差很大时优于非抢占匹配。 |
| [^240] | [Happenstance: Utilizing Semantic Search to Track Russian State Media Narratives about the Russo-Ukrainian War On Reddit.](http://arxiv.org/abs/2205.14484) | 本论文通过分析俄罗斯国家媒体在英语读者中传播的叙事，比较Reddit上有关俄乌战争的讨论，展示了俄罗斯媒体对叙事的影响以及讨论的发展。 |
| [^241] | [Personalized Algorithmic Recourse with Preference Elicitation.](http://arxiv.org/abs/2205.13743) | 研究提出了PEAR方法，这是一个首个能够针对最终用户需求提供个性化算法补救成本的人机交互方法。该方法利用贝叶斯偏好引导的见解，通过最大化原则性信息增益度量来计算目标用户选择的预期效用，然后将偏好引导整合到强化学习框架中。该方法显著提高了算法干预的经济实用性和用户友好性。 |
| [^242] | [Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher.](http://arxiv.org/abs/2205.08096) | 该论文提出了一种新颖的机器取消学习方法，通过在学生-教师框架中利用有能力和无能力的教师来引导遗忘，以便随时从已经训练好的机器学习模型中删除某个集合或类别的数据，而无需重新训练。 |
| [^243] | [RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states.](http://arxiv.org/abs/2205.07229) | 本文提出了RoMFAC算法，通过新的训练目标和重复的正则化损失函数，使其对于异常状态干扰具有鲁棒性并获得出色性能表现。 |
| [^244] | [BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck.](http://arxiv.org/abs/2205.03612) | BrainIB是一种基于图信息瓶颈原理开发的图神经网络框架，在分析fMRI图像中的功能连接时能够识别最具信息量的边缘，具有良好的泛化能力和可解释性，适用于未见样本的场景。 |
| [^245] | [Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs).](http://arxiv.org/abs/2205.01059) | 本文提出一种增广拉格朗日松弛方法(AL-PINNs)用于物理信息神经网络(PINNs)的训练，该方法通过自适应平衡每个损失组件，能够有效地解决非线性偏微分方程问题。 |
| [^246] | [Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees.](http://arxiv.org/abs/2205.00359) | 该研究将深度学习模型的影响估计方法改编到了梯度提升决策树上，命名为TREX和BoostIn，旨在帮助更好地理解GBDT的预测和改进性能。 |
| [^247] | [A neural network-supported two-stage algorithm for lightweight dereverberation on hearing devices.](http://arxiv.org/abs/2204.02978) | 论文提出了一种基于神经网络的两阶段算法，通过直接优化准则，使得多通道线性滤波器阶段输出处效果更好，可与后滤波器阶段结合有效去除残留混响。 |
| [^248] | [Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models.](http://arxiv.org/abs/2201.12675) | 该论文提出了一种针对联邦学习中文本的隐私攻击方法，通过部署恶意参数向量来揭示私人用户文本，并成功地进行mini-batches训练，适用于多个用户和长序列，提示了文本领域的FL比先前认为的更脆弱。 |
| [^249] | [Zero-Shot Machine Unlearning.](http://arxiv.org/abs/2201.05629) | 零样本机器遗忘是一个新兴的研究问题，允许从已经训练好的ML模型中删除数据。因为这些请求可能会涉及到无法访问的训练数据，因此需要新的解决方法。 |
| [^250] | [Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation.](http://arxiv.org/abs/2112.12218) | 本文提出了在最大熵和错误预测上使用的训练策略（MEEP），通过惩罚自信度过高的预测来提高医学图像分割模型的校准性和分割准确性。 |
| [^251] | [Causal Inference Despite Limited Global Confounding via Mixture Models.](http://arxiv.org/abs/2112.11602) | 本论文提出了一种基于混合模型的因果推断方法，通过解决混合问题和恢复概率分布，可以确定原本无法确定的因果关系。 |
| [^252] | [Controlling Wasserstein Distances by Kernel Norms with Application to Compressive Statistical Learning.](http://arxiv.org/abs/2112.00423) | 本文提供了用MMD范数控制Wasserstein距离的条件，针对压缩统计学习提出了HLRIP属性，通过导出的新核范数提供了计算上高效的Wasserstein距离压缩统计学习保证。 |
| [^253] | [Fast Yet Effective Machine Unlearning.](http://arxiv.org/abs/2111.08947) | 本文提出了一种快速且有效的机器数据遗忘框架，该框架采用误差最大化的噪声生成和损伤-修复的权重操作来删除机器学习模型中的特定数据，同时具有较高的适用性和效率。 |
| [^254] | [Optimum-statistical Collaboration Towards General and Efficient Black-box Optimization.](http://arxiv.org/abs/2106.09215) | 本文提出了最优统计协作算法框架，管理优化误差通量和演化中的统计误差通量。该框架通用性强且适用于多种函数和分区族，并启发提出了一种方差自适应算法。 |
| [^255] | [Accurate Shapley Values for explaining tree-based models.](http://arxiv.org/abs/2106.03820) | 本文提出了在树模型中计算Shapley值的两种更准确的估计器，相比于现有方法可以更高效地利用树结构，并探讨了Shapley值作为局部解释的局限性。 |
| [^256] | [Learning Diverse Options via InfoMax Termination Critic.](http://arxiv.org/abs/2010.02756) | 本文提出了一种通过最大化状态和动作选项之间的互信息来学习选项的终止条件的方法，从而提高学习到的选项的多样性和可重用性，在实验中取得了显著的效果。 |
| [^257] | [How Powerful are Shallow Neural Networks with Bandlimited Random Weights?.](http://arxiv.org/abs/2008.08427) | 本文研究了深度为2的带限制随机神经网络的表达能力，通过数学证明确定了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。 |

# 详细

[^1]: 基于原始表达的不对称核奇异值分解的自注意力机制

    Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation. (arXiv:2305.19798v1 [cs.LG])

    [http://arxiv.org/abs/2305.19798](http://arxiv.org/abs/2305.19798)

    本文提出了一种基于不对称核奇异值分解的自注意力机制，即Primal-Attention，来优化注意力机制，提高注意力输出的投影方差。

    

    近期，一系列工作将自注意力机制视为核机器，以此来理解和改进Transformers。然而，现有的方法只适用于对称核而不适用于不对称的自注意力，导致了理论和实际的差距。在本文中，我们提出了一种基于不对称核奇异值分解（KSVD）来表达和优化自注意力的新视角。通过不对称KSVD，我们得到了：i）自注意力的一种原始-对偶表达，其中优化目标被转化为最大化注意力输出中的投影方差；ii）一种新的注意力机制-Primal-Attention，通过KSVD的原始表达式避免了在对偶中显式计算核矩阵的问题；iii）通过KKT条件，我们证明了Primal-Attention的状态最小化问题的解与之前的对偶算法具有一致性。

    Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stati
    
[^2]: 利用aggVAE进行深度学习和MCMC以处理行政边界变化：以肯尼亚的疟疾患病率为例

    Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v1 [cs.LG])

    [http://arxiv.org/abs/2305.19779](http://arxiv.org/abs/2305.19779)

    本研究提出了一种利用aggVAE进行深度学习和MCMC处理行政边界变化的解决方案，可以更准确地映射以县为层级的聚合级别数据，并处理行政边界的变化，相比最先进的模型表现更好。

    

    基于模型的疾病映射是公共卫生和疾病监测中基本的政策信息工具，分层贝叶斯模型是当前最先进的方法。当处理区域数据，如行政区划单位（例如县或省）的聚合数据时，常用的模型依赖于区域单元的相邻结构以考虑空间相关性。疾病监测系统的目标是随时间跟踪疾病结果，但在危机情况下（例如政治变化导致行政边界更改），这将带来挑战。我们提出了一种新颖、实用和易于实施的解决方案，该方案依赖于组合深层生成模型和全贝叶斯推断。我们建立在现有的变分自编码器(VAE) 工作上，并展示我们提出的聚合VAE(aggVAE)体系结构可用于在以县为层级的聚合级别处理数据，以映射肯尼亚的疟疾患病率。我们的模型可以以连续的方式考虑空间相关性，而不依赖于相邻性假设，并且能够处理行政边界的变化。结果表明，相比最先进的模型，我们的模型表现出更好的性能和更准确的疟疾患病率映射。

    Model-based disease mapping remains a fundamental policy-informing tool in public health and disease surveillance with hierarchical Bayesian models being the current state-of-the-art approach. When working with areal data, e.g. aggregates at the administrative unit level such as district or province, routinely used models rely on the adjacency structure of areal units to account for spatial correlations. The goal of disease surveillance systems is to track disease outcomes over time, but this provides challenging in situations of crises, such as political changes, leading to changes of administrative boundaries. Kenya is an example of such country. Moreover, adjacency-based approach ignores the continuous nature of spatial processes and cannot solve the change-of-support problem, i.e. when administrative boundaries change. We present a novel, practical, and easy to implement solution relying on a methodology combining deep generative modelling and fully Bayesian inference. We build on 
    
[^3]: J-UNIWARD中的一个实现错误

    Off-By-One Implementation Error in J-UNIWARD. (arXiv:2305.19776v1 [cs.CR])

    [http://arxiv.org/abs/2305.19776](http://arxiv.org/abs/2305.19776)

    J-UNIWARD 是一种将秘密信息隐藏在JPEG图像中的隐写方法，本文发现了其实现中存在的一个 off-by-one 错误，使一些图像块被高估，另一些被低估，同时提供了一个概念验证用于检测此种错误。

    

    J-UNIWARD是一种将秘密信息隐藏在JPEG盖板图像中的流行隐写术方法。作为一种内容自适应方法，J-UNIWARD旨在嵌入到纹理图像区域，这些区域的变化难以检测。为此，J-UNIWARD首先为每个DCT系数分配一个嵌入成本，该成本基于图像的小波残差计算，然后使用一种编码方法，该方法在嵌入所需的有效载荷的同时，最小化成本。更改一个DCT系数会影响23x23个小波系数窗口。为了加速成本图的计算，原始实现预先计算小波残差，然后对于每个更改的DCT系数，考虑一个23x23的小波残差窗口。然而，该实现错误地将窗口偏移了一个像素到右下方。在这份报告中，我们评估了这个off-by-one错误对生成的成本图的影响。一些图像块被高估，而其他图像块则被低估，但差异相对较小。我们提供了一个概念验证，说明如何检测使用带有偏移错误的J-UNIWARD隐藏的隐写术信息。

    J-UNIWARD is a popular steganography method for hiding secret messages in JPEG cover images. As a content-adaptive method, J-UNIWARD aims to embed into textured image regions where changes are difficult to detect. To this end, J-UNIWARD first assigns to each DCT coefficient an embedding cost calculated based on the image's Wavelet residual, and then uses a coding method that minimizes the cost while embedding the desired payload. Changing one DCT coefficient affects a 23x23 window of Wavelet coefficients. To speed up the costmap computation, the original implementation pre-computes the Wavelet residual and then considers per changed DCT coefficient a 23x23 window of the Wavelet residual. However, the implementation accesses a window accidentally shifted by one pixel to the bottom right. In this report, we evaluate the effect of this off-by-one error on the resulting costmaps. Some image blocks are over-priced while other image blocks are under-priced, but the difference is relatively s
    
[^4]: 利用深度学习算子解决成像反问题时存在的歧义

    Ambiguity in solving imaging inverse problems with deep learning based operators. (arXiv:2305.19774v1 [cs.CV])

    [http://arxiv.org/abs/2305.19774](http://arxiv.org/abs/2305.19774)

    本文提出了通过使用非常小的神经结构、提出一种基于正则化项的网络损失函数修改策略和对图像进行低秩表示的预处理步骤来提高基于深度学习的去模糊方法的稳定性和鲁棒性。研究结果表明，这些策略有效地提高了图像去模糊方法的稳定性和准确性。

    

    近年来，大型卷积神经网络被广泛用作图像去模糊工具，因为它们能够非常精确地恢复图像。众所周知，图像去模糊在数学上被建模为反问题，当噪声影响数据时，其解决方案难以近似。然而，神经网络在去模糊时的局限性在于其对噪声和其他扰动的敏感性，这可能会导致不稳定性并产生较差的重建结果。此外，当进行端到端的训练时，网络不一定考虑底层成像问题的数字化形式。在本文中，我们提出了一些策略，通过基于深度学习的方法来将图像去模糊的稳定性提高而不至于失去太多准确性。首先，我们建议使用非常小的神经结构，这将减少训练的执行时间，同时满足绿色人工智能的需求，并且不会极大地放大计算图像中的噪声。其次，我们提出了一种修改网络损失函数的方法，基于正则化项以稳定训练并降低对扰动的敏感度。最后，我们提出采用基于低秩表示的预处理步骤，为迭代优化过程提供更稳定和准确的起点。我们在合成和现实数据上验证了我们策略的有效性，证明它们可以提高基于深度学习的去模糊方法的稳定性和鲁棒性。

    In recent years, large convolutional neural networks have been widely used as tools for image deblurring, because of their ability in restoring images very precisely. It is well known that image deblurring is mathematically modeled as an ill-posed inverse problem and its solution is difficult to approximate when noise affects the data. Really, one limitation of neural networks for deblurring is their sensitivity to noise and other perturbations, which can lead to instability and produce poor reconstructions. In addition, networks do not necessarily take into account the numerical formulation of the underlying imaging problem, when trained end-to-end. In this paper, we propose some strategies to improve stability without losing to much accuracy to deblur images with deep-learning based methods. First, we suggest a very small neural architecture, which reduces the execution time for training, satisfying a green AI need, and does not extremely amplify noise in the computed image. Second, 
    
[^5]: “质量进/质量出：评估异常检测基准数据的数据质量”

    Quality In / Quality Out: Assessing Data quality in an Anomaly Detection Benchmark. (arXiv:2305.19770v1 [cs.LG])

    [http://arxiv.org/abs/2305.19770](http://arxiv.org/abs/2305.19770)

    本文发现，在进行异常检测基准测试时，基准数据集的质量对于机器学习模型的性能影响很大，比特定的机器学习算法更重要。

    

    自主或自动驾驶网络被期望成为未来互联网中极富挑战和需求的新型应用的解决方案。处理复杂性的关键在于通过最少的人工干预执行网络优化和故障恢复的任务。为此，社区依赖于新的机器学习模型和技术的开发。然而，机器学习的好坏取决于它所拟合的数据。为研究目的提供的数据集（对研究结果和方向有重要影响的基准数据集）通常被认为是默认具有良好质量的。本文表明，对同一基准数据集（UGR'16，用于异常检测的基于流量的实时数据集）进行相对较小的修改，比所考虑的具体机器学习技术更显著地影响了模型性能。为了理解这一发现，我们提出了一种研究这些差异根本原因的方法。

    Autonomous or self-driving networks are expected to provide a solution to the myriad of extremely demanding new applications in the Future Internet. The key to handle complexity is to perform tasks like network optimization and failure recovery with minimal human supervision. For this purpose, the community relies on the development of new Machine Learning (ML) models and techniques. However, ML can only be as good as the data it is fitted with. Datasets provided to the community as benchmarks for research purposes, which have a relevant impact in research findings and directions, are often assumed to be of good quality by default. In this paper, we show that relatively minor modifications on the same benchmark dataset (UGR'16, a flow-based real-traffic dataset for anomaly detection) cause significantly more impact on model performance than the specific ML technique considered. To understand this finding, we contribute a methodology to investigate the root causes for those differences,
    
[^6]: 基于注意力机制的音频问答方法

    Attention-Based Methods For Audio Question Answering. (arXiv:2305.19769v1 [cs.CL])

    [http://arxiv.org/abs/2305.19769](http://arxiv.org/abs/2305.19769)

    本文提出了一种基于自我注意力和交叉注意力的神经网络体系结构，用于音频问答任务。实验结果表明，相较于参考方法，该方法在Clotho-AQA数据集上的表现有明显提高。

    

    音频问答(AQA)是对于音频和自然语言提出问题时，生成自然语言回答的任务。本文提出了基于自注意力和交叉注意力的神经网络体系结构，用于音频问答任务。自我注意力层可以提取强大的音频和文本表示。交叉注意力将与文本特征相关的音频特征映射到答案中。我们的所有模型都在最近提出的Clotho-AQA数据集上进行了训练，用于二进制是/否问题和单词回答问题。结果清楚地显示出相对于原始论文中的参考方法改进。在是/否二进制分类任务中，我们的提出的模型相对于参考模型的准确率从62.7％提高到了68.3％。对于单词答案多类分类器，我们的模型分别产生了57.9％和99.8％的top-1和top-5准确率，相对于参考模型的54.2％和93.7％有了明显提高。

    Audio question answering (AQA) is the task of producing natural language answers when a system is provided with audio and natural language questions. In this paper, we propose neural network architectures based on self-attention and cross-attention for the AQA task. The self-attention layers extract powerful audio and textual representations. The cross-attention maps audio features that are relevant to the textual features to produce answers. All our models are trained on the recently proposed Clotho-AQA dataset for both binary yes/no questions and single-word answer questions. Our results clearly show improvement over the reference method reported in the original paper. On the yes/no binary classification task, our proposed model achieves an accuracy of 68.3% compared to 62.7% in the reference model. For the single-word answers multiclass classifier, our model produces a top-1 and top-5 accuracy of 57.9% and 99.8% compared to 54.2% and 93.7% in the reference model respectively. We fur
    
[^7]: 训练数据归因的贝叶斯视角

    A Bayesian Perspective On Training Data Attribution. (arXiv:2305.19765v1 [cs.LG])

    [http://arxiv.org/abs/2305.19765](http://arxiv.org/abs/2305.19765)

    本文介绍了一种TDA任务的贝叶斯视角，从中发现个别训练样本的影响常被噪声掩盖，TDA只能用于解释对模型预测影响稳定、独立于其他噪声因素的训练数据。

    

    训练数据归因（TDA）技术可找出影响模型对所关注的测试数据的预测的重要的训练数据，并估计减少或增加特定训练样本的影响。本文引入了一种TDA任务的贝叶斯视角，将学习的模型视为贝叶斯后验，TDA估计作为随机变量。从这个新的视角出发，我们发现个别训练样本的影响常会被模型初始化和SGD批次组合产生的噪声掩盖。基于这个发现，我们认为TDA只能可靠地用于解释一些训练数据对模型预测的影响是稳定的，独立于其他噪声因素。我们的实验证实了这种独立于噪声的训练-测试数据是很罕见的。

    Training data attribution (TDA) techniques find influential training data for the model's prediction on the test data of interest. They approximate the impact of down- or up-weighting a particular training sample. While conceptually useful, they are hardly applicable in practice, particularly because of their sensitivity to different model initialisation. In this paper, we introduce a Bayesian perspective on the TDA task, where the learned model is treated as a Bayesian posterior and the TDA estimates as random variables. From this novel viewpoint, we observe that the influence of an individual training sample is often overshadowed by the noise stemming from model initialisation and SGD batch composition. Based on this observation, we argue that TDA can only be reliably used for explaining model predictions that are consistently influenced by certain training data, independent of other noise factors. Our experiments demonstrate the rarity of such noise-independent training-test data pa
    
[^8]: 递归 Metropolis-Hastings 命名游戏：基于概率生成模型的多智能体系统中的符号出现

    Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models. (arXiv:2305.19761v1 [cs.CL])

    [http://arxiv.org/abs/2305.19761](http://arxiv.org/abs/2305.19761)

    本文提出了一种递归 Metropolis-Hastings 命名游戏 (RMHNG) 模型，用于多智能体系统中的符号出现，具有较快的收敛速度和鲁棒性，并在多个场景下进行了验证。

    

    在代理人群体中研究符号的出现和紧急通信，使用了一种计算模型，其中代理人参与各种语言游戏。其中，Metropolis-Hastings 命名游戏 (MHNG) 具有一个显著的数学属性：通过 MHNG 的符号出现被证明是分散式贝叶斯推理，是代理人共享的表征。然而，以前提出的 MHNG 仅在两个代理人场景中使用。本文将 MHNG 扩展到 N 代理人场景中。本文的主要贡献有两个：(1) 我们将递归 Metropolis-Hastings 命名游戏 (RMHNG) 提出为 MHNG 的 N 代理人版本，并证明 RMHNG 是一种近似贝叶斯推理方法，类似于 MHNG，用于代理人共享的潜在变量的后验分布；(2) 我们在合成和真实图像数据上进行了 RMHNG 的性能实证评估，使多个代理可以开发和共享符号系统。此外，我们还将 RMHNG 的性能与其他现有模型进行了比较，并展示了其在收敛速度和鲁棒性方面的优越性。

    In the studies on symbol emergence and emergent communication in a population of agents, a computational model was employed in which agents participate in various language games. Among these, the Metropolis-Hastings naming game (MHNG) possesses a notable mathematical property: symbol emergence through MHNG is proven to be a decentralized Bayesian inference of representations shared by the agents. However, the previously proposed MHNG is limited to a two-agent scenario. This paper extends MHNG to an N-agent scenario. The main contributions of this paper are twofold: (1) we propose the recursive Metropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and demonstrate that RMHNG is an approximate Bayesian inference method for the posterior distribution over a latent variable shared by agents, similar to MHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and real image data, enabling multiple agents to develop and share a symbol system. Furthermore, we
    
[^9]: 隧道效应：深度神经网络中的数据表示构建

    The Tunnel Effect: Building Data Representations in Deep Neural Networks. (arXiv:2305.19753v1 [cs.LG])

    [http://arxiv.org/abs/2305.19753](http://arxiv.org/abs/2305.19753)

    本文研究表明，深度神经网络中存在一种名为“隧道”的现象，它在网络的训练早期就出现，并且对最终的数据表示起到了压缩作用。其中，初始层构建了线性可分表示形式，而随后的层压缩这些表示形式并对整体性能影响不大。然而，隧道会削弱网络在超出分布的泛化性能。

    

    深度神经网络以其在各种任务上的卓越表现而闻名，人们普遍认为更深的网络隐含着对更复杂数据表示的理解。本文表明，训练有素的用于监督图像分类的深度网络分为两个不同的部分，它们对最终数据表示的形成起着不同的作用：最初的层构建了线性可分的表示形式，而随后的层（我们称之为“隧道”）则压缩这些表示形式，并对整体性能影响不大。我们通过全面的实证研究探讨了隧道的行为，发现它会在训练过程中的早期出现，隧道的深度取决于网络容量与任务复杂度之间的关系。此外，我们表明，隧道会削弱网络在超出分布的泛化性能，并讨论了这对于持续学习的影响。

    Deep neural networks are widely known for their remarkable effectiveness across various tasks, with the consensus that deeper networks implicitly learn more complex data representations. This paper shows that sufficiently deep networks trained for supervised image classification split into two distinct parts that contribute to the resulting data representations differently. The initial layers create linearly-separable representations, while the subsequent layers, which we refer to as \textit{the tunnel}, compress these representations and have a minimal impact on the overall performance. We explore the tunnel's behavior through comprehensive empirical studies, highlighting that it emerges early in the training process. Its depth depends on the relation between the network's capacity and task complexity. Furthermore, we show that the tunnel degrades out-of-distribution generalization and discuss its implications for continual learning.
    
[^10]: 神经马尔可夫跳跃过程

    Neural Markov Jump Processes. (arXiv:2305.19744v1 [cs.LG])

    [http://arxiv.org/abs/2305.19744](http://arxiv.org/abs/2305.19744)

    介绍了一种基于神经常微分方程的马尔可夫跳跃过程的变分推断算法，可通过反向传播进行训练，用于近似后验马尔可夫跳跃过程的初始分布和时间相关的转移概率率，同时在先验过程的时间无关率上也有很好的表现。

    

    马尔可夫跳跃过程是具有广泛应用的连续时间随机过程，被广泛应用于自然和社会科学领域。尽管它们被广泛使用，但这些模型中的推断是非常复杂的，通常需要通过蒙特卡罗或期望最大化方法进行。本文介绍了一种基于神经常微分方程的马尔可夫跳跃过程的变分推断算法，并可通过反向传播进行训练。该方法学习了观测数据的神经连续时间表示，用于近似后验马尔可夫跳跃过程的初始分布和时间相关的转移概率率。相比之下，先验过程的时间无关率则像生成对抗网络一样进行训练。我们在合成数据上测试了我们的方法，这些数据是从真实的马尔可夫跳跃过程、实验性开关离子通道数据和分子动力学模拟中采样得到的。

    Markov jump processes are continuous-time stochastic processes with a wide range of applications in both natural and social sciences. Despite their widespread use, inference in these models is highly non-trivial and typically proceeds via either Monte Carlo or expectation-maximization methods. In this work we introduce an alternative, variational inference algorithm for Markov jump processes which relies on neural ordinary differential equations, and is trainable via back-propagation. Our methodology learns neural, continuous-time representations of the observed data, that are used to approximate the initial distribution and time-dependent transition probability rates of the posterior Markov jump process. The time-independent rates of the prior process are in contrast trained akin to generative adversarial networks. We test our approach on synthetic data sampled from ground-truth Markov jump processes, experimental switching ion channel data and molecular dynamics simulations. Source c
    
[^11]: 用于剂量组合的可靠脱机学习

    Reliable Off-Policy Learning for Dosage Combinations. (arXiv:2305.19742v1 [cs.LG])

    [http://arxiv.org/abs/2305.19742](http://arxiv.org/abs/2305.19742)

    本文提出了一种用于剂量组合的新颖可靠的脱机学习方法，通过三个步骤实现：开发神经网络估计个性化的剂量-反应，估计倾向得分检测共享协变量-治疗空间中的重叠有限区域，然后基于梯度的学习算法找到最佳的个性化剂量组合。

    

    个性化医学领域的决策制定，如癌症治疗或危重护理，通常必须对剂量组合进行选择，即多种连续治疗。现有的这项任务的工作已经独立地建模了多种治疗的效果，而估计联合效果却受到了很少的关注，并且面临着非平凡的挑战。在本文中，我们提出了一种新颖的方法，用于剂量组合的可靠脱机学习。我们的方法分为三个步骤：（1）我们开发了一个特定的神经网络，估计个性化的剂量-反应函数，同时考虑多个相关剂量的联合效应。（2）我们使用条件正态化流量估计广义倾向得分，以检测共享协变量-治疗空间中重叠有限的区域。（3）我们提供一种基于梯度的学习算法，以找到最佳的个性化剂量组合。在此，我们确保可靠地估计策略价值。

    Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments independently, while estimating the joint effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy val
    
[^12]: 图的Bures-Wasserstein平均值

    Bures-Wasserstein Means of Graphs. (arXiv:2305.19738v1 [stat.ML])

    [http://arxiv.org/abs/2305.19738](http://arxiv.org/abs/2305.19738)

    该论文提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。实验结果表明，在各种任务中都有很好的表现。

    

    在机器学习和统计学中，找到采样数据的平均值是一项基本任务。然而，在数据样本为图对象的情况下，定义平均值是一项困难的任务。我们提出了一个新颖的框架，通过在平滑图信号分布空间中嵌入图来定义图的平均值，其中可以使用Wasserstein度量衡量图相似性。通过在此嵌入空间中找到平均值，我们可以恢复保留结构信息的平均图。我们确定了新的图平均值的存在和唯一性，并提供了一个迭代算法来计算它。为了展示我们的框架作为机器学习中的一个有价值的工具，我们在各种任务中进行了评估，包括结构化图的k-means聚类、功能性脑网络的分类以及多层图的半监督节点分类。我们的实验结果表明，我们的方法实现了一致的p。

    Finding the mean of sampled data is a fundamental task in machine learning and statistics. However, in cases where the data samples are graph objects, defining a mean is an inherently difficult task. We propose a novel framework for defining a graph mean via embeddings in the space of smooth graph signal distributions, where graph similarity can be measured using the Wasserstein metric. By finding a mean in this embedding space, we can recover a mean graph that preserves structural information. We establish the existence and uniqueness of the novel graph mean, and provide an iterative algorithm for computing it. To highlight the potential of our framework as a valuable tool for practical applications in machine learning, it is evaluated on various tasks, including k-means clustering of structured graphs, classification of functional brain networks, and semi-supervised node classification in multi-layer graphs. Our experimental results demonstrate that our approach achieves consistent p
    
[^13]: APPRAISER: 利用近似误差进行DNN故障容忍性分析

    APPRAISER: DNN Fault Resilience Analysis Employing Approximation Errors. (arXiv:2305.19733v1 [cs.LG])

    [http://arxiv.org/abs/2305.19733](http://arxiv.org/abs/2305.19733)

    提出了一种新的容错评估方法，称为APPRAISER，它采用近似误差进行DNN故障容忍性分析。APPRAISER提供了成千上万倍的评估速度提升，同时保持了分析的高准确性。

    

    如今，深度神经网络在安全关键应用中的广泛应用引起了新的可靠性关注。实际上，通过硬件仿真进行故障注入的方法是研究DNN架构容错性的有效和广泛使用的方法，早期设计阶段已经减轻了可靠性问题。然而，目前的故障注入方法在时间、设计和控制复杂性方面存在一系列问题。为了克服这些问题，提出了一种新的容错评估方法，称为APPRAISER，它将函数近似用于非传统用途，并利用近似计算误差。通过在容错评估领域采用这个概念，APPRAISER提供了成千上万倍的评估速度提升，同时保持了分析的高准确性。本文通过与最先进的故障注入方法进行比较，验证了APPRAISER的有效性。

    Nowadays, the extensive exploitation of Deep Neural Networks (DNNs) in safety-critical applications raises new reliability concerns. In practice, methods for fault injection by emulation in hardware are efficient and widely used to study the resilience of DNN architectures for mitigating reliability issues already at the early design stages. However, the state-of-the-art methods for fault injection by emulation incur a spectrum of time-, design- and control-complexity problems. To overcome these issues, a novel resiliency assessment method called APPRAISER is proposed that applies functional approximation for a non-conventional purpose and employs approximate computing errors for its interest. By adopting this concept in the resiliency assessment domain, APPRAISER provides thousands of times speed-up in the assessment process, while keeping high accuracy of the analysis. In this paper, APPRAISER is validated by comparing it with state-of-the-art approaches for fault injection by emulat
    
[^14]: 潜在图像流形的数据表示研究

    Data Representations' Study of Latent Image Manifolds. (arXiv:2305.19730v1 [cs.LG])

    [http://arxiv.org/abs/2305.19730](http://arxiv.org/abs/2305.19730)

    本文研究了图像流形的曲率，其中最先进的卷积神经网络在层间具有特征曲率剖面，曲率差异与网络的泛化能力有强烈的相关性，且mixup等常见的规范化方法产生更平的表示。

    

    深度神经网络在许多领域取得了惊人的成功，但其内在机制尚未得到很好的理解。本文研究了图像流形的曲率，即在其主方向上的流形偏离平坦的程度。我们发现，用于图像分类的最先进的卷积神经网络在层间具有特征曲率剖面：一个初始急剧增加，接着是长时间的平台期，然后是另一个增加。相反，在未经训练的网络中不出现这种行为，其中曲率变平。我们还表明，最后两层之间的曲率差异与网络的泛化能力有强烈的相关性。此外，我们发现，潜在编码的内在维度并非必然表征曲率。最后，我们观察到，mixup等常见的规范化方法在与其他方法相比时产生更平的表示。

    Deep neural networks have been demonstrated to achieve phenomenal success in many domains, and yet their inner mechanisms are not well understood. In this paper, we investigate the curvature of image manifolds, i.e., the manifold deviation from being flat in its principal directions. We find that state-of-the-art trained convolutional neural networks for image classification have a characteristic curvature profile along layers: an initial steep increase, followed by a long phase of a plateau, and followed by another increase. In contrast, this behavior does not appear in untrained networks in which the curvature flattens. We also show that the curvature gap between the last two layers has a strong correlation with the generalization capability of the network. Moreover, we find that the intrinsic dimension of latent codes is not necessarily indicative of curvature. Finally, we observe that common regularization methods such as mixup yield flatter representations when compared to other m
    
[^15]: 不平衡低秩最优输运求解器

    Unbalanced Low-rank Optimal Transport Solvers. (arXiv:2305.19727v1 [cs.LG])

    [http://arxiv.org/abs/2305.19727](http://arxiv.org/abs/2305.19727)

    本论文提出了首个据我们所知的融合低秩近似和非平衡公式的O(n)优化输运求解器，并展示了其在各种任务中的优越性能。

    

    优化输运方法长期以来都受到了两个显着限制的影响：第一，标准样本求解器（当用于n个样本批次时）的O(n^3)计算成本是禁止的；第二，质量守恒约束使得OT解算器在实践中太过严格，因为它们必须匹配来自两种度量的所有点，其输出可能会受到异常值的严重影响。本文旨在将这两种方法合并起来，实现首个据我们所知融合了低秩近似和非平衡公式的O(n)优化输运求解器。我们展示了我们的求解器在各种任务中如何优于最先进的速度，允许更轻松地并入先前的知识，并且对异常值更有弹性。

    The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations. First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive. Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers. A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods: While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further. On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation. The goal of this paper is to merge these two strains, to achieve th
    
[^16]: 无组合假设的表示学习

    Learning Representations without Compositional Assumptions. (arXiv:2305.19726v1 [cs.LG])

    [http://arxiv.org/abs/2305.19726](http://arxiv.org/abs/2305.19726)

    本文提出了一种无组合假设的数据驱动方法，通过表示不同特征集之间的依赖关系来进行无监督表示学习，并引入了LEGATO分层图自编码器来捕获局部信息。

    

    本文针对包含由不同的测量来源生成的多个视图的表格数据进行无监督表示学习。传统方法使用多视图框架解决此问题，但受到预定义的假设的限制，这些假设认为特征集共享相同的信息，表示应该学习全局共享的因素。然而，这种假设并不总是适用于复杂特征集之间存在的依赖关系的真实世界表格数据，这导致了较难学习的局部信息。为了克服这个限制，我们提出了一种数据驱动的方法，通过将特征集表示为图节点，将它们之间的关系作为可学习的边来学习特征集之间的依赖关系。此外，我们引入了一个新的分层图自编码器LEGATO，学习一个较小的潜在图来动态地整合多个视图的信息。这种方法产生了专门捕获局部信息的潜在图组件。

    This paper addresses unsupervised representation learning on tabular data containing multiple views generated by distinct sources of measurement. Traditional methods, which tackle this problem using the multi-view framework, are constrained by predefined assumptions that assume feature sets share the same information and representations should learn globally shared factors. However, this assumption is not always valid for real-world tabular datasets with complex dependencies between feature sets, resulting in localized information that is harder to learn. To overcome this limitation, we propose a data-driven approach that learns feature set dependencies by representing feature sets as graph nodes and their relationships as learnable edges. Furthermore, we introduce LEGATO, a novel hierarchical graph autoencoder that learns a smaller, latent graph to aggregate information from multiple views dynamically. This approach results in latent graph components that specialize in capturing local
    
[^17]: 粗糙集下一种规则通用逆推学习方法

    A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])

    [http://arxiv.org/abs/2305.19718](http://arxiv.org/abs/2305.19718)

    本文提出了一种基于粗糙集理论的规则通用逆推学习方法，通过转化目标概念和子概念为信息表，以更低的成本解决领域知识获取和规则的修正、减少、生成问题。

    

    在现实任务中，通常存在大量未标记数据和标记数据。将两者组合起来进行学习的任务被称为半监督学习。专家可以使用逻辑规则来标记未标记数据，但这个操作很昂贵。感知和推理的结合在处理具有领域知识的半监督任务方面具有良好的效果。然而，获取领域知识以及规则的修正、减少和生成仍然是需要解决的复杂问题。粗糙集理论是解决信息系统中知识处理的重要方法。本文提出了一种粗糙集下的规则通用逆推学习方法（RS-ABL）。通过将规则的目标概念和子概念转化为信息表，利用粗糙集理论来解决以更低的成本获取领域知识和修正、减少、生成规则的问题。该框架还可以生成更广泛的负规则，以增强规则范围。

    In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
    
[^18]: 图神经网络中的改连是否真正有用？

    Is Rewiring Actually Helpful in Graph Neural Networks?. (arXiv:2305.19717v1 [cs.LG])

    [http://arxiv.org/abs/2305.19717](http://arxiv.org/abs/2305.19717)

    本文研究了图神经网络中的改连方法是否有用，提出了一种新的评估设置，并在真实世界的节点和图分类任务上进行了系统的实验比较。

    

    图神经网络通过执行多个消息传递步骤来计算节点表示，这些步骤包括节点特征的本地聚合。但是，深层模型能够利用节点之间更长距离的交互的问题受到了过度平滑和过度压缩的影响。而后者归因于指导消息传递的图拓扑，导致节点表示对包含在远程节点上的信息不敏感。许多改连方法已被提出来解决或减轻这个问题。然而，由于过度压缩与其他与模型训练密切相关的问题（如消失的梯度）相耦合，所以正确评估这些方法的好处是困难的。因此，我们提出了一种基于消息传递模型的评估设置，这些模型不需要训练即可计算节点和图表示。我们在真实世界的节点和图分类任务上进行了系统的实验比较。

    Graph neural networks compute node representations by performing multiple message-passing steps that consist in local aggregations of node features. Having deep models that can leverage longer-range interactions between nodes is hindered by the issues of over-smoothing and over-squashing. In particular, the latter is attributed to the graph topology which guides the message-passing, causing a node representation to become insensitive to information contained at distant nodes. Many graph rewiring methods have been proposed to remedy or mitigate this problem. However, properly evaluating the benefits of these methods is made difficult by the coupling of over-squashing with other issues strictly related to model training, such as vanishing gradients. Therefore, we propose an evaluation setting based on message-passing models that do not require training to compute node and graph representations. We perform a systematic experimental comparison on real-world node and graph classification ta
    
[^19]: 可分目标的最优决策树：推动动态规划的极限

    Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])

    [http://arxiv.org/abs/2305.19706](http://arxiv.org/abs/2305.19706)

    本研究提出了一种通用的动态规划方法来优化任何组合的可分离目标和约束条件，这种方法在可扩展性方面比通用求解器表现得更好。

    

    决策树的全局优化在准确性，大小和人类可理解性方面表现出良好的前景。然而，许多方法仍然依赖于通用求解器，可扩展性仍然是一个问题。动态规划方法已被证明具有更好的可扩展性，因为它们通过将子树作为独立的子问题解决来利用树结构。然而，这仅适用于可以分别优化子树的任务。我们详细研究了这种关系，并展示了实现这种可分离约束和目标任意组合的动态规划方法。在四个应用领域的实验表明了这种方法的普适性，同时也比通用求解器具有更好的可扩展性。

    Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
    
[^20]: 一种高效的基于机器学习的OFDM子带信道预测技术

    An Efficient Machine Learning-based Channel Prediction Technique for OFDM Sub-Bands. (arXiv:2305.19696v1 [cs.IT])

    [http://arxiv.org/abs/2305.19696](http://arxiv.org/abs/2305.19696)

    这篇论文提出了一种基于机器学习训练信道衰落样本的OFDM子带信道预测技术，可用于估计选择性衰落中未来信道行为。

    

    准确的信道状态信息（CSI）的获取对于提高无线通信系统的性能至关重要。但是，由于无线环境的时变性和频率选择性的复杂性，获得精确的CSI（可以通过信道估计或信道预测实现）是一项复杂的任务。为此，我们提出了一种在正交频分复用（OFDM）子带中进行信道预测的高效机器学习（ML）技术。所提出的方法的创新之处在于训练用于估计选择性衰落中未来信道行为的信道衰落样本。

    The acquisition of accurate channel state information (CSI) is of utmost importance since it provides performance improvement of wireless communication systems. However, acquiring accurate CSI, which can be done through channel estimation or channel prediction, is an intricate task due to the complexity of the time-varying and frequency selectivity of the wireless environment. To this end, we propose an efficient machine learning (ML)-based technique for channel prediction in orthogonal frequency-division multiplexing (OFDM) sub-bands. The novelty of the proposed approach lies in the training of channel fading samples used to estimate future channel behaviour in selective fading.
    
[^21]: 基于约束模型和PMIME度量的时间序列因果发现

    Causal discovery for time series with constraint-based model and PMIME measure. (arXiv:2305.19695v1 [stat.ME])

    [http://arxiv.org/abs/2305.19695](http://arxiv.org/abs/2305.19695)

    本文提出了一种新方法，将因果发现算法与信息理论度量结合，以解决通过相关性发现虚假关系的问题。这种方法在几个模拟数据集上呈现出有希望的结果。

    

    因果关系定义了因果和效应之间的关系。在多变量时间序列领域，这个概念允许考虑时间滞后来表征多个时间序列之间的联系。这些现象在医学上分析药物效应、在制造业中检测复杂系统中异常情况的原因或在社会科学中特别重要......大多数情况下，研究这些复杂系统仅通过相关性来实现。但是，相关性可能导致虚假的关系。为了解决这个问题，我们在本文中提出了一种新的方法，将因果发现算法与基于信息论的度量结合起来，从而允许推断线性和非线性关系并构建潜在的因果图。我们在几个模拟数据集上评估了该方法的性能，显示出有希望的结果。

    Causality defines the relationship between cause and effect. In multivariate time series field, this notion allows to characterize the links between several time series considering temporal lags. These phenomena are particularly important in medicine to analyze the effect of a drug for example, in manufacturing to detect the causes of an anomaly in a complex system or in social sciences... Most of the time, studying these complex systems is made through correlation only. But correlation can lead to spurious relationships. To circumvent this problem, we present in this paper a novel approach for discovering causality in time series data that combines a causal discovery algorithm with an information theoretic-based measure. Hence the proposed method allows inferring both linear and non-linear relationships and building the underlying causal graph. We evaluate the performance of our approach on several simulated data sets, showing promising results.
    
[^22]: 利用代理分类损失的假设迁移学习

    Hypothesis Transfer Learning with Surrogate Classification Losses. (arXiv:2305.19694v1 [stat.ML])

    [http://arxiv.org/abs/2305.19694](http://arxiv.org/abs/2305.19694)

    本文研究了使用代理分类损失的假设迁移学习的学习理论，通过算法稳定性提供了在温和假设下的学习保证，适用于机器学习算法。

    

    假设迁移学习（HTL）通过允许先前任务（即源任务）向一个新任务（目标任务）转移学习，而无需访问源数据，与领域自适应相对应。事实上，HTL仅依赖于从源数据学习到的假设，免除了大量数据存储的障碍，并提供了巨大的实际利益。因此，HTL对于依赖于大数据的实际应用非常有利。本文通过算法稳定性研究HTL的学习理论，这是一种用于分析机器学习算法的有吸引力的理论框架，特别是在二分类情况下感兴趣。我们的稳定性分析提供了在温和假设下的学习保证。因此，我们得出了几个比以前更紧密的理论界限，这些界限可以实际应用于机器学习算法。

    Hypothesis transfer learning (HTL) contrasts domain adaptation by allowing for a previous task leverage, named the source, into a new one, the target, without requiring access to the source data. Indeed, HTL relies only on a hypothesis learnt from such source data, relieving the hurdle of expansive data storage and providing great practical benefits. Hence, HTL is highly beneficial for real-world applications relying on big data. The analysis of such a method from a theoretical perspective faces multiple challenges, particularly in classification tasks. This paper deals with this problem by studying the learning theory of HTL through algorithmic stability, an attractive theoretical framework for machine learning algorithms analysis. In particular, we are interested in the statistical behaviour of the regularized empirical risk minimizers in the case of binary classification. Our stability analysis provides learning guarantees under mild assumptions. Consequently, we derive several comp
    
[^23]: 生成式扩散模型中的自发对称破缺

    Spontaneous symmetry breaking in generative diffusion models. (arXiv:2305.19693v1 [cs.LG])

    [http://arxiv.org/abs/2305.19693](http://arxiv.org/abs/2305.19693)

    本文揭示生成式扩散模型存在自发对称破缺现象，这将其生成动力学分为两种不同的“相”，提出了高斯后期初始化方案，能够显著提高模型性能。

    

    生成式扩散模型近期成为了生成高维数据的主要方法。在本文中，我们展示了这些模型的动力学存在自发对称破缺，将生成式动力学分为两个不同的“相”：1）中心固定点周围的线性稳态动力学，2）朝向数据流形的吸引子动力学。这两种“相”由中心固定点稳定性变化所分隔，而不稳定窗口负责生成样本的多样性。我们使用理论和实证证据表明，准确地模拟早期动力学并不会对最终生成产生重要影响，因为早期涨落会回到中心固定点。为了利用这一见解，我们提出了一个高斯后期初始化方案，这显著提高了模型的性能，在快速取样器上实现了长达3倍的FID改进。

    Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, whi
    
[^24]: 异步多人赌博问题中的常数或对数遗憾

    Constant or logarithmic regret in asynchronous multiplayer bandits. (arXiv:2305.19691v1 [cs.LG])

    [http://arxiv.org/abs/2305.19691](http://arxiv.org/abs/2305.19691)

    本文解决了异步多人赌博问题的集中式情况，推出了Cautious Greedy算法，可以保证常数遗憾，同时UCB算法的自然扩展展现出了 $\mathcal{O}(\sqrt{T\log(T)})$ 极小化遗憾。

    

    最近，由于在认知无线电网络中的应用，多人赌博问题得到了广泛研究。虽然文献大多考虑同步玩家，但无线电网络（例如物联网） tend to have asynchronous devices。这引发了更加困难的异步多人赌博问题，首先用探索然后承诺（ETC）算法解决（请参见 Dakdouk，2022），遗憾上限为 $\mathcal{O}(T^{\frac{2}{3}})$。甚至在考虑分散化之前，理解集中式情况仍然是一个挑战，因为不知道是否可能得到小于 $\Omega(T^\frac{2}{3})$ 的遗憾。 我们对这个问题作出了肯定回答，因为UCB的自然扩展展现出了 $\mathcal{O}(\sqrt{T\log(T)})$ 极小化遗憾。更重要的是，我们介绍了一个叫做“谨慎贪婪”的集中式算法，如果最优策略至少将一个玩家指定在每个武器上（被证明会出现这种情况），则可以产生常数保证的遗憾。

    Multiplayer bandits have recently been extensively studied because of their application to cognitive radio networks.  While the literature mostly considers synchronous players, radio networks (e.g. for IoT) tend to have asynchronous devices. This motivates the harder, asynchronous multiplayer bandits problem, which was first tackled with an explore-then-commit (ETC) algorithm (see Dakdouk, 2022), with a regret upper-bound in $\mathcal{O}(T^{\frac{2}{3}})$. Before even considering decentralization, understanding the centralized case was still a challenge as it was unknown whether getting a regret smaller than $\Omega(T^{\frac{2}{3}})$ was possible.  We answer positively this question, as a natural extension of UCB exhibits a $\mathcal{O}(\sqrt{T\log(T)})$ minimax regret.  More importantly, we introduce Cautious Greedy, a centralized algorithm that yields constant instance-dependent regret if the optimal policy assigns at least one player on each arm (a situation that is proved to occur 
    
[^25]: 深度随机力学

    Deep Stochastic Mechanics. (arXiv:2305.19685v1 [cs.LG])

    [http://arxiv.org/abs/2305.19685](http://arxiv.org/abs/2305.19685)

    本文提出了一种基于深度学习的方法，用于数值模拟时间演化薛定谔方程，利用马尔可夫扩散采样来适应波函数的潜在低维结构，并提出了新的随机量子力学方程，具有线性的计算复杂度。数值模拟显示出显着的优势。

    

    本文引入了一种基于深度学习的方法，用于数值模拟时间演化薛定谔方程，受随机力学和生成性扩散模型的启发。与现有方法不同的是，我们的方法允许我们通过从马尔可夫扩散中采样来适应波函数潜在的低维结构，因此可以在更高的维度上降低计算复杂度。此外，我们提出了新的随机量子力学方程，结果具有与维数数量线性的计算复杂度。数值模拟验证了我们的理论发现，并显示出我们的方法与其他用于量子力学的基于深度学习的方法相比具有显着优势。

    This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schr\"odinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.
    
[^26]: 通过局部模态初始化和无偏差对比散度实现深度玻尔兹曼机的端到端训练

    End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization. (arXiv:2305.19684v1 [cs.LG])

    [http://arxiv.org/abs/2305.19684](http://arxiv.org/abs/2305.19684)

    本研究提出一种基于Metropolis-Hastings耦合和局部模态初始化的方法，解决了深度玻尔兹曼机中的偏差梯度估计问题，使得DBMs可以端到端地训练，实验结果表明与其他深度生成模型相当的生成性能。

    

    本研究解决了深度玻尔兹曼机（DBMs）中的偏差梯度估计问题。现有的获取无偏估计量的方法使用基于Gibbs采样的最大耦合，但当状态是高维时，其收敛需要很长时间。因此，我们提出了基于Metropolis-Hastings的耦合，并围绕目标分布的局部模态初始化状态。由于MH倾向于拒绝提案，这种耦合有很高的概率在一步内收敛，因此具有高效性。我们发现，我们的方法可以在不进行贪心预训练的情况下端到端地训练DBMs。我们还提出了一些实用技术，以进一步提高DBMs的性能。我们通过实验证明，我们的训练算法使DBMs能够展现出与其他深度生成模型相当的生成性能，在MNIST上达到了10.33的FID分数。

    We address the problem of biased gradient estimation in deep Boltzmann machines (DBMs). The existing method to obtain an unbiased estimator uses a maximal coupling based on a Gibbs sampler, but when the state is high-dimensional, it takes a long time to converge. In this study, we propose to use a coupling based on the Metropolis-Hastings (MH) and to initialize the state around a local mode of the target distribution. Because of the propensity of MH to reject proposals, the coupling tends to converge in only one step with a high probability, leading to high efficiency. We find that our method allows DBMs to be trained in an end-to-end fashion without greedy pretraining. We also propose some practical techniques to further improve the performance of DBMs. We empirically demonstrate that our training algorithm enables DBMs to show comparable generative performance to other deep generative models, achieving the FID score of 10.33 for MNIST.
    
[^27]: Smooth-Trajectron++: 将平滑关注机制引入Trajectron++行为预测模型

    Smooth-Trajectron++: Augmenting the Trajectron++ behaviour prediction model with smooth attention. (arXiv:2305.19678v1 [cs.LG])

    [http://arxiv.org/abs/2305.19678](http://arxiv.org/abs/2305.19678)

    本文中，研究者将平滑关注机制引入最先进的Trajectron++轨迹预测模型，提高其性能并比较其性能表现。该研究表明将人类的认知因素纳入轨迹预测模型中具有潜在的优势。

    

    理解交通参与者的行为对于预测其未来轨迹至关重要，为自动驾驶车辆的开发提供安全可靠的规划系统。本文研究了最先进的Trajectron++轨迹预测模型，并通过在其注意力模块中加入平滑项以进一步提高其性能。该关注机制模仿了认知科学研究所启发的人类注意力，表明人的注意力在切换时存在局限性。我们对结果进行了评估，并将其与原始模型在各种基准测试中进行了比较，揭示了将人类认知洞察力纳入轨迹预测模型中的潜力。

    Understanding traffic participants' behaviour is crucial for predicting their future trajectories, aiding in developing safe and reliable planning systems for autonomous vehicles. Integrating cognitive processes and machine learning models has shown promise in other domains but is lacking in the trajectory forecasting of multiple traffic agents in large-scale autonomous driving datasets. This work investigates the state-of-the-art trajectory forecasting model Trajectron++ which we enhance by incorporating a smoothing term in its attention module. This attention mechanism mimics human attention inspired by cognitive science research indicating limits to attention switching. We evaluate the performance of the resulting Smooth-Trajectron++ model and compare it to the original model on various benchmarks, revealing the potential of incorporating insights from human cognition into trajectory prediction models.
    
[^28]: 在线到PAC的转换: 通过遗憾分析得出泛化界限

    Online-to-PAC Conversions: Generalization Bounds via Regret Analysis. (arXiv:2305.19674v1 [stat.ML])

    [http://arxiv.org/abs/2305.19674](http://arxiv.org/abs/2305.19674)

    本文提出了在线学习游戏“泛化游戏”的框架，将在线学习算法的表现和统计学习算法的泛化界限联系了起来，并得出了一些标准的泛化限制。

    

    我们提出了一个新的框架，通过在线学习的视角推导出统计学习算法的泛化界限。具体而言，我们构建了一个在线学习游戏称为“泛化游戏”，其中在线学习器试图与固定的统计学习算法竞争，预测独立同分布数据点训练集上的泛化间隙序列。我们通过展示在这个游戏中存在有界遗憾的在线学习算法与统计学习设置之间的联系来建立这种关联，这意味着统计学习算法的泛化错误存在一个界限，直到与统计学习方法的复杂性无关的鞅浓度项。这种技术允许我们恢复几个标准的泛化限制，包括一系列的PAC-Bayesian保证和信息理论保证，以及它们的推广。

    We present a new framework for deriving bounds on the generalization bound of statistical learning algorithms from the perspective of online learning. Specifically, we construct an online learning game called the "generalization game", where an online learner is trying to compete with a fixed statistical learning algorithm in predicting the sequence of generalization gaps on a training set of i.i.d. data points. We establish a connection between the online and statistical learning setting by showing that the existence of an online learning algorithm with bounded regret in this game implies a bound on the generalization error of the statistical learning algorithm, up to a martingale concentration term that is independent of the complexity of the statistical learning method. This technique allows us to recover several standard generalization bounds including a range of PAC-Bayesian and information-theoretic guarantees, as well as generalizations thereof.
    
[^29]: 信号比偏差更难学习：使用Focal Loss进行去偏差

    Signal Is Harder To Learn Than Bias: Debiasing with Focal Loss. (arXiv:2305.19671v1 [cs.LG])

    [http://arxiv.org/abs/2305.19671](http://arxiv.org/abs/2305.19671)

    这篇论文提出了一种同时训练有偏和无偏分类器的方法，使用分解重新加权方案和Focal Loss启发的方法去偏差，相比于其他方法性能更好，同时提出了一种基于潜空间的可视化偏差方案，可以帮助实践者认识虚假相关的来源。

    

    伪相关性无处不在。 尽管人类常常不会察觉它们，但神经网络以学习不需要的关联而出了名，也就是所谓的偏差，而不是基础决策规则。 因此，实践者经常不知道分类器的有偏决策。 基于虚假关联的这种有偏模型可能无法推广到未观察到的数据，导致意外的负面影响。 我们提出了一个名为Signal is Harder（SiH）的变分自动编码器方法，使用受Focal Loss启发的新型分解重新加权方案同时训练有偏和无偏分类器。 使用无偏分类器，SiH匹配或优于最先进的去偏差方法的性能。 为了提高我们技术的可解释性，我们提出了一种在潜空间中的扰动方案来可视化偏差，这有助于实践者认识到虚假相关的来源。

    Spurious correlations are everywhere. While humans often do not perceive them, neural networks are notorious for learning unwanted associations, also known as biases, instead of the underlying decision rule. As a result, practitioners are often unaware of the biased decision-making of their classifiers. Such a biased model based on spurious correlations might not generalize to unobserved data, leading to unintended, adverse consequences. We propose Signal is Harder (SiH), a variational-autoencoder-based method that simultaneously trains a biased and unbiased classifier using a novel, disentangling reweighting scheme inspired by the focal loss. Using the unbiased classifier, SiH matches or improves upon the performance of state-of-the-art debiasing methods. To improve the interpretability of our technique, we propose a perturbation scheme in the latent space for visualizing the bias that helps practitioners become aware of the sources of spurious correlations.
    
[^30]: Vandermonde神经算子

    Vandermonde Neural Operators. (arXiv:2305.19663v1 [cs.LG])

    [http://arxiv.org/abs/2305.19663](http://arxiv.org/abs/2305.19663)

    本文提出了一种新的神经网络结构，Vandermonde神经算子，能够有效地处理非均匀分布点上的输入数据，同时在速度和准确性上相较于以前的方法有所提升。

    

    Fourier神经算子（FNO）已成为非常受欢迎的机器学习体系结构，用于学习操作符，特别是那些在PDE中出现的操作符。然而，由于FNO依赖于快速傅里叶变换以实现计算效率，所以该体系结构可能仅限于笛卡尔网格上的输入数据。在这里，我们将FNO推广到处理分布在非均匀点分布上的输入数据。我们提出的模型称为Vandermonde神经运算符（VNO），利用Vandermonde结构矩阵来高效地计算正向和反向的傅里叶变换，即使在任意分布的点上也可以如此。我们进行了数值实验，证明VNO可以比FNO快得多，同时保持可比的准确性，并改进了可比的非均匀方法（如Geo-FNO）的准确性。

    Fourier Neural Operators (FNOs) have emerged as very popular machine learning architectures for learning operators, particularly those arising in PDEs. However, as FNOs rely on the fast Fourier transform for computational efficiency, the architecture can be limited to input data on equispaced Cartesian grids. Here, we generalize FNOs to handle input data on non-equispaced point distributions. Our proposed model, termed as Vandermonde Neural Operator (VNO), utilizes Vandermonde-structured matrices to efficiently compute forward and inverse Fourier transforms, even on arbitrarily distributed points. We present numerical experiments to demonstrate that VNOs can be significantly faster than FNOs, while retaining comparable accuracy, and improve upon accuracy of comparable non-equispaced methods such as the Geo-FNO.
    
[^31]: 利用局部化提高图神经网络的表达能力

    Improving Expressivity of Graph Neural Networks using Localization. (arXiv:2305.19659v1 [cs.LG])

    [http://arxiv.org/abs/2305.19659](http://arxiv.org/abs/2305.19659)

    本文提出了Weisfeiler-Leman (WL)算法的局部版本，用于解决子图计数问题并提高图神经网络的表达能力，同时，也给出了一些时间和空间效率更高的$k-$WL变体和分裂技术。

    

    本文提出了Weisfeiler-Leman (WL)算法的局部版本，旨在增加表达能力并减少计算负担。我们专注于子图计数问题，并为任意$k$给出$k-$WL的局部版本。我们分析了Local $k-$WL的作用，并证明其比$k-$WL更具表现力，并且至多与$(k+1)-$WL一样具有表现力。我们给出了一些模式的特征，如果两个图是Local $k-$WL等价的，则它们的子图和诱导子图的计数是不变的。我们还介绍了$k-$WL的两个变体：层$k-$WL和递归$k-$WL。这些方法的时间和空间效率比在整个图上应用$k-$WL更高。我们还提出了一种分裂技术，使用$1-$WL即可保证所有大小不超过4的诱导子图的准确计数。相同的方法可以使用$k>1$进一步扩展到更大的模式。我们还将Local $k-$WL的表现力与其他GNN层次结构进行了比较。

    In this paper, we propose localized versions of Weisfeiler-Leman (WL) algorithms in an effort to both increase the expressivity, as well as decrease the computational overhead. We focus on the specific problem of subgraph counting and give localized versions of $k-$WL for any $k$. We analyze the power of Local $k-$WL and prove that it is more expressive than $k-$WL and at most as expressive as $(k+1)-$WL. We give a characterization of patterns whose count as a subgraph and induced subgraph are invariant if two graphs are Local $k-$WL equivalent. We also introduce two variants of $k-$WL: Layer $k-$WL and recursive $k-$WL. These methods are more time and space efficient than applying $k-$WL on the whole graph. We also propose a fragmentation technique that guarantees the exact count of all induced subgraphs of size at most 4 using just $1-$WL. The same idea can be extended further for larger patterns using $k>1$. We also compare the expressive power of Local $k-$WL with other GNN hierarc
    
[^32]: 深度ReLU网络中的成对学习最优估计

    Optimal Estimates for Pairwise Learning with Deep ReLU Networks. (arXiv:2305.19640v1 [stat.ML])

    [http://arxiv.org/abs/2305.19640](http://arxiv.org/abs/2305.19640)

    本文研究了深度ReLU网络中的成对学习，提出了一个针对一般损失函数的误差估计的尖锐界限，并基于成对最小二乘损失得出几乎最优的过度泛化误差界限。

    

    成对学习指的是在损失函数中考虑一对样本的学习任务。本文研究了深度ReLU网络中的成对学习，并估计了过度泛化误差。对于满足某些温和条件的一般损失函数，建立了误差估计的尖锐界限，其误差估计的阶数为O（（Vlog（n）/ n）1 /（2-β））。特别地，对于成对最小二乘损失，我们得到了过度泛化误差的几乎最优界限，在真实的预测器满足某些光滑性正则性时，最优界限达到了最小化界限，差距仅为对数项。

    Pairwise learning refers to learning tasks where a loss takes a pair of samples into consideration. In this paper, we study pairwise learning with deep ReLU networks and estimate the excess generalization error. For a general loss satisfying some mild conditions, a sharp bound for the estimation error of order $O((V\log(n) /n)^{1/(2-\beta)})$ is established. In particular, with the pairwise least squares loss, we derive a nearly optimal bound of the excess generalization error which achieves the minimax lower bound up to a logrithmic term when the true predictor satisfies some smoothness regularities.
    
[^33]: 从移动健康和临床数据中预测营养不良风险的可解释人工智能

    Explainable AI for Malnutrition Risk Prediction from m-Health and Clinical Data. (arXiv:2305.19636v1 [cs.LG])

    [http://arxiv.org/abs/2305.19636](http://arxiv.org/abs/2305.19636)

    本文提出了一个基于异构m-health数据的早期和可解释的营养不良风险检测的新型AI框架，通过模型评估发现，随机森林（RF）和梯度上升是表现最好的分类器，尤其是在纳入身体组成评估数据时。

    

    营养不良是老年人中普遍存在的严重健康问题，尤其是在住院或机构化的个体中更加普遍。准确和早期的风险检测对于营养不良的管理和预防至关重要。配备人工智能（AI）的m-health服务可以在更自动化，客观和持续的监测和评估方面带来重要的改进。此外，最新的可解释人工智能（XAI）方法可以使AI决策可解释和可信任。本文提出了一个基于异构m-health数据的早期和可解释的营养不良风险检测的新型AI框架。我们执行了广泛的模型评估，包括独立个体和个性化预测，并且获得的结果表明，随机森林（RF）和梯度上升是表现最好的分类器，尤其是在纳入身体组成评估数据时。我们还调查了几种基准XAI方法。

    Malnutrition is a serious and prevalent health problem in the older population, and especially in hospitalised or institutionalised subjects. Accurate and early risk detection is essential for malnutrition management and prevention. M-health services empowered with Artificial Intelligence (AI) may lead to important improvements in terms of a more automatic, objective, and continuous monitoring and assessment. Moreover, the latest Explainable AI (XAI) methodologies may make AI decisions interpretable and trustworthy for end users. This paper presents a novel AI framework for early and explainable malnutrition risk detection based on heterogeneous m-health data. We performed an extensive model evaluation including both subject-independent and personalised predictions, and the obtained results indicate Random Forest (RF) and Gradient Boosting as the best performing classifiers, especially when incorporating body composition assessment data. We also investigated several benchmark XAI metho
    
[^34]: Point-GCC: 基于几何-颜色对比的通用自监督三维场景预训练

    Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast. (arXiv:2305.19623v1 [cs.CV])

    [http://arxiv.org/abs/2305.19623](http://arxiv.org/abs/2305.19623)

    Point-GCC提出了一种通过几何-颜色对比进行通用自监督三维场景预训练的方法，并设计了分层监督和架构无关的骨干网络，以缩小预训练和下游任务之间的差距。

    

    点云提供的几何和颜色信息对于三维场景理解都非常重要，然而现有方法在区分和相关性方面缺乏精细设计，因此提出了一种可以更好地利用点云信息关系的三维自监督范式。具体来说，提出了一种通过 Siamese 网络对齐几何和颜色信息的通用三维场景预训练框架 Point-GCC。为了照顾实际应用任务，设计了（i）基于新颖的深度聚类模块的点级对比和重建和物体级对比的分层监督，来缩小预训练和下游任务之间的差距；（ii）架构无关的骨干网络，以适应各种下游模型。由于与下游任务相关联的物体级表示，Point-GCC 可以直接评估。

    Geometry and color information provided by the point clouds are both crucial for 3D scene understanding. Two pieces of information characterize the different aspects of point clouds, but existing methods lack an elaborate design for the discrimination and relevance. Hence we explore a 3D self-supervised paradigm that can better utilize the relations of point cloud information. Specifically, we propose a universal 3D scene pre-training framework via Geometry-Color Contrast (Point-GCC), which aligns geometry and color information using a Siamese network. To take care of actual application tasks, we design (i) hierarchical supervision with point-level contrast and reconstruct and object-level contrast based on the novel deep clustering module to close the gap between pre-training and downstream tasks; (ii) architecture-agnostic backbone to adapt for various downstream models. Benefiting from the object-level representation associated with downstream tasks, Point-GCC can directly evaluate 
    
[^35]: MSMix: 基于插值的文本数据增强方法——流形交换Mixup

    MSMix:An Interpolation-Based Text Data Augmentation Method Manifold Swap Mixup. (arXiv:2305.19617v1 [cs.LG])

    [http://arxiv.org/abs/2305.19617](http://arxiv.org/abs/2305.19617)

    提出了一种基于插值的文本数据增强方法——流形交换Mixup(MSMix)，通过在网络的特定层部分替换隐藏特征来从两个不同的样本中获得更丰富的隐藏表示，并在三个中文意图识别数据集上实验证明了MSMix在完整样本和小样本配置下均表现出更好的性能。

    

    为了解决深度神经网络模型由于数据不足而表现不佳的问题，提出了一种简单而有效的基于插值的数据增强方法：MSMix(流形交换Mixup)。该方法将两个不同的样本输入同一个深度神经网络模型，随机选择一个特定的层，部分替换该层一个样本的隐藏特征为另一个样本的对应特征。混合后的隐藏特征输入模型，并通过网络的其余部分。还提出了两种不同的选择策略，以获取更丰富的隐藏表示。在三个中文意图识别数据集上进行了实验，结果表明MSMix方法在完整样本和小样本配置下均获得了比其他方法更好的结果。

    To solve the problem of poor performance of deep neural network models due to insufficient data, a simple yet effective interpolation-based data augmentation method is proposed: MSMix (Manifold Swap Mixup). This method feeds two different samples to the same deep neural network model, and then randomly select a specific layer and partially replace hidden features at that layer of one of the samples by the counterpart of the other. The mixed hidden features are fed to the model and go through the rest of the network. Two different selection strategies are also proposed to obtain richer hidden representation. Experiments are conducted on three Chinese intention recognition datasets, and the results show that the MSMix method achieves better results than other methods in both full-sample and small-sample configurations.
    
[^36]: 拓展水下声学目标识别信息视角的文本模板方法

    Underwater-Art: Expanding Information Perspectives With Text Templates For Underwater Acoustic Target Recognition. (arXiv:2305.19612v1 [cs.SD])

    [http://arxiv.org/abs/2305.19612](http://arxiv.org/abs/2305.19612)

    本文提出了一种基于文本模板的水下声学目标识别方法（UART），从不同视角整合相关信息，通过音频-频谱图-文本三模态对比学习框架，赋予UART用自然语言指导声学表示学习的能力，显著提高识别模型的可解释性和鲁棒性。

    

    水下声学目标识别是一项棘手的任务，由于复杂的声源特征和声波传播模式而变得十分困难。基于深度学习的识别模型，受限于数据量不足和狭窄信息视角，在实际水下场景中似乎远未达到满意程度。尽管水下声学信号受距离、水深或其他因素的严重影响，但相关信息的注释往往是不均匀的、不完整的、难以使用的。本文提出了一种基于文本模板的水下声学目标识别方法（UART），通过将来自不同视角的相关信息整合成自然语言来设计模板。UART采用音频-频谱图-文本三模态对比学习框架，赋予UART用自然语言指导声学表示学习的能力。实验结果表明，UART可以显著提高各种水下声学数据集上识别模型的可解释性和鲁棒性。

    Underwater acoustic target recognition is an intractable task due to the complex acoustic source characteristics and sound propagation patterns. Limited by insufficient data and narrow information perspective, recognition models based on deep learning seem far from satisfactory in practical underwater scenarios. Although underwater acoustic signals are severely influenced by distance, channel depth, or other factors, annotations of relevant information are often non-uniform, incomplete, and hard to use. In our work, we propose to implement Underwater Acoustic Recognition based on Templates made up of rich relevant information (hereinafter called "UART"). We design templates to integrate relevant information from different perspectives into descriptive natural language. UART adopts an audio-spectrogram-text tri-modal contrastive learning framework, which endows UART with the ability to guide the learning of acoustic representations by descriptive natural language. Our experiments reveal
    
[^37]: 通过领域知识启示的深度学习进行药物推荐

    Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])

    [http://arxiv.org/abs/2305.19604](http://arxiv.org/abs/2305.19604)

    提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。

    

    药物推荐是医疗保健的基本但至关重要的分支，提供机会为复杂健康状况的患者支持临床医生更精确的药物处方。从电子健康记录（EHR）中学习推荐药物是先前研究中最常见的方法。然而，大多数研究忽视了根据患者的EHR中的临床表现纳入领域知识的问题。为了解决这些问题，我们提出了一种新颖的基于动态领域知识的药物推荐框架，即领域知识启示网络（DKINet），用于将领域知识与可观察的患者临床表现相结合。特别是，我们首先设计了一个基于领域知识的编码器来捕捉领域信息，然后开发了一个数据驱动的编码器将领域知识整合到可观察的EHR中。

    Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
    
[^38]: 从文本监督中学习音乐序列表示

    Learning Music Sequence Representation from Text Supervision. (arXiv:2305.19602v1 [cs.SD])

    [http://arxiv.org/abs/2305.19602](http://arxiv.org/abs/2305.19602)

    该论文提出一种新颖的文本监督预训练方法MUSER，能够更灵活地适应下游任务，且只需要使用0.056%的预训练数据即可实现最先进的性能。

    

    音乐表示学习因其在数字信号序列中包含复杂的与人类相关的概念而闻名困难。为了从有标签的音频中挖掘更好的音乐序列表示，我们提出了一种新颖的文本监督预训练方法，名为MUSER。MUSER采用音频-频谱-文本三模态对比学习框架，在该框架中，文本输入可以是任何形式的元数据，借助文本模板进行帮助，而频谱是从音频序列中派生出来的。我们的实验揭示出，与目前的数据密集型预训练方法相比，MUSER可以更灵活地适应下游任务，且只需要使用0.056%的预训练数据即可实现最先进的性能。

    Music representation learning is notoriously difficult for its complex human-related concepts contained in the sequence of numerical signals. To excavate better MUsic SEquence Representation from labeled audio, we propose a novel text-supervision pre-training method, namely MUSER. MUSER adopts an audio-spectrum-text tri-modal contrastive learning framework, where the text input could be any form of meta-data with the help of text templates while the spectrum is derived from an audio sequence. Our experiments reveal that MUSER could be more flexibly adapted to downstream tasks compared with the current data-hungry pre-training method, and it only requires 0.056% of pre-training data to achieve the state-of-the-art performance.
    
[^39]: 自适应自蒸馏下的异构数据联邦学习

    Federated Learning on Heterogeneous Data via Adaptive Self-Distillation. (arXiv:2305.19600v1 [cs.LG])

    [http://arxiv.org/abs/2305.19600](http://arxiv.org/abs/2305.19600)

    本文提出一种基于自适应自蒸馏的新型正则化技术来训练客户端模型，该正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。实验结果表明，该方法在各种基准数据集上优于目前流行的联邦学习方法。

    

    联邦学习是一种机器学习范式，它使得客户机可以聚合本地训练模型而无需共享任何本地训练数据从而训练全局模型。然而，实践中发现，每个客户端观察到的本地数据分布之间可能存在显著的不均匀性（例如类别不平衡）。在这种不均匀的数据分布下，联邦学习会出现“客户机漂移”问题，导致每个客户端收敛到其自己的局部最优解，这会降低模型的收敛速度并降低模型性能。为了解决这个问题，我们提出了一种基于自适应自蒸馏的新型正则化技术来训练客户端模型。我们的正则化方案基于客户端本地模型预测和全局模型的相似性以及客户端的标签分布来自适应地调整客户端的训练数据。该正则化技术可以轻松地集成在现有的联邦学习算法之上，而不需要对客户端或服务器代码进行任何更改，因此具有高度的可部署性。我们在各种基准数据集上验证了我们的方法，并展示了在非独立同分布数据下的优越性。

    Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid data distributions across clients, FL suffers from the 'client-drift' problem where every client converges to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to the client's training data based on: (1) the closeness of the local model's predictions with that of the global model and (2) the client's label distribution. The proposed regularization can be easily integrated atop exis
    
[^40]: 半监督普适图分类

    Towards Semi-supervised Universal Graph Classification. (arXiv:2305.19598v1 [cs.LG])

    [http://arxiv.org/abs/2305.19598](http://arxiv.org/abs/2305.19598)

    该论文提出了一种新型图神经网络框架UGNN， 解决了半监督普适图分类问题，通过估计未标记图的确定性解决了类别偏移，具有最新性能。

    

    近年来，图神经网络推动了图分类的最新技术。本文提出了一种名为UGNN的新型图神经网络框架，以从子图角度利用未标记数据来解决半监督普适图分类问题。该问题的挑战在于缺乏标签数据和可能的类别偏移。UGNN通过估计未标记图的确定性来解决类别偏移，使其在具有挑战性的数据集上具有最新性能。

    Graph neural networks have pushed state-of-the-arts in graph classifications recently. Typically, these methods are studied within the context of supervised end-to-end training, which necessities copious task-specific labels. However, in real-world circumstances, labeled data could be limited, and there could be a massive corpus of unlabeled data, even from unknown classes as a complementary. Towards this end, we study the problem of semi-supervised universal graph classification, which not only identifies graph samples which do not belong to known classes, but also classifies the remaining samples into their respective classes. This problem is challenging due to a severe lack of labels and potential class shifts. In this paper, we propose a novel graph neural network framework named UGNN, which makes the best of unlabeled data from the subgraph perspective. To tackle class shifts, we estimate the certainty of unlabeled graphs using multiple subgraphs, which facilities the discovery of
    
[^41]: 使用恶意软件数据集探究机器学习和量子机器学习的易受攻击漏洞: 一项比较分析

    Exploring the Vulnerabilities of Machine Learning and Quantum Machine Learning to Adversarial Attacks using a Malware Dataset: A Comparative Analysis. (arXiv:2305.19593v1 [cs.LG])

    [http://arxiv.org/abs/2305.19593](http://arxiv.org/abs/2305.19593)

    本研究比较了传统神经网络和量子神经网络在恶意软件数据集上对于攻击的易感性，并发现QNN表现出更高的易感性。

    

    机器学习和量子机器学习已经展现了在各领域中处理复杂问题的巨大潜力。然而，当在安全敏感的应用中部署这些系统时，其易受攻击的特性引起了人们的担忧。本研究使用恶意软件数据集进行比较分析，探究传统神经网络和量子神经网络对于攻击的易感性。我们使用名为ClaMP的软件供应链攻击数据集，分别为QNN和NN开发两个不同的模型，并使用Pennylane进行量子实现，使用TensorFlow和Keras进行传统实现。我们的方法是通过向数据集的一小部分引入随机噪声来创建对抗性样本，并使用准确度、精确度、召回率和F1分数指标评估其对模型性能的影响。基于我们的观察结果，我们得出结论，当训练使用恶意软件数据集时，机器学习和量子机器学习模型都容易受到攻击。在我们的比较分析中发现，与传统NN相比，QNN表现出更高的易感性。

    The burgeoning fields of machine learning (ML) and quantum machine learning (QML) have shown remarkable potential in tackling complex problems across various domains. However, their susceptibility to adversarial attacks raises concerns when deploying these systems in security sensitive applications. In this study, we present a comparative analysis of the vulnerability of ML and QML models, specifically conventional neural networks (NN) and quantum neural networks (QNN), to adversarial attacks using a malware dataset. We utilize a software supply chain attack dataset known as ClaMP and develop two distinct models for QNN and NN, employing Pennylane for quantum implementations and TensorFlow and Keras for traditional implementations. Our methodology involves crafting adversarial samples by introducing random noise to a small portion of the dataset and evaluating the impact on the models performance using accuracy, precision, recall, and F1 score metrics. Based on our observations, both M
    
[^42]: 基于人工智能的交通预测：近期进展与新机遇综述

    Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities. (arXiv:2305.19591v1 [cs.LG])

    [http://arxiv.org/abs/2305.19591](http://arxiv.org/abs/2305.19591)

    该论文综述了交通预测方法的发展，重点介绍了基于人工智能的交通预测方法在多元交通时间序列模型研究方面的进展和机遇。

    

    交通预测在缓解全球性的交通拥堵问题中起着关键作用，其负面影响包括额外旅行时间的损失和燃料消耗的增加。将新兴技术融入交通系统可以显著改善交通预测，并带来新的研究问题。为了了解交通预测中的开放性研究挑战，本综述旨在提供交通预测方法的综合概述。具体而言，我们侧重于基于人工智能（AI）的交通预测方法在多变量交通时间序列建模方面的近期进展和新的研究机遇，这是由于近年来这类方法在交通预测中具有潜在的成功和潜力。

    Traffic prediction plays a crucial role in alleviating traffic congestion which represents a critical problem globally, resulting in negative consequences such as lost hours of additional travel time and increased fuel consumption. Integrating emerging technologies into transportation systems provides opportunities for improving traffic prediction significantly and brings about new research problems. In order to lay the foundation for understanding the open research challenges in traffic prediction, this survey aims to provide a comprehensive overview of traffic prediction methodologies. Specifically, we focus on the recent advances and emerging research opportunities in Artificial Intelligence (AI)-based traffic prediction methods, due to their recent success and potential in traffic prediction, with an emphasis on multivariate traffic time series modeling. We first provide a list and explanation of the various data types and resources used in the literature. Next, the essential data 
    
[^43]: 带建议的主动因果结构学习

    Active causal structure learning with advice. (arXiv:2305.19588v1 [cs.LG])

    [http://arxiv.org/abs/2305.19588](http://arxiv.org/abs/2305.19588)

    本研究提出了带建议的主动因果结构学习问题，并设计了一个自适应搜索算法，可以从建议中受益，即使建议是任意糟糕的情况下，仍然具有最坏情况下的保证。

    

    我们引入了带建议的主动因果结构学习问题。在典型的研究中，学习算法针对观测分布获得本质图，并被要求在最小化干预次数的同时恢复出潜在的因果有向无环图(DAG) $G^*$。在我们的问题设定中，除了关于 $G^*$的必要信息外，例如一个声称是 $G^*$的DAG $G$，我们还会额外获得关于 $G^*$的侧面信息。我们想知道，当建议接近正确时，学习算法是否可以从建议中受益，同时即使建议是任意糟糕的情况下，仍然具有最坏情况下的保证。我们的工作与关于带预测算法的不断增加的研究领域相同。当建议是有向无环图$G$时，我们设计了自适应搜索算法来恢复 $G^*$，其干预成本最多为验证$G^*$的成本的$O(max\{1, \log \psi\})$倍。这里，$\psi$是$G$和$G^*$之间的距离度量，它被上界约束。

    We introduce the problem of active causal structure learning with advice. In the typical well-studied setting, the learning algorithm is given the essential graph for the observational distribution and is asked to recover the underlying causal directed acyclic graph (DAG) $G^*$ while minimizing the number of interventions made. In our setting, we are additionally given side information about $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the learning algorithm can benefit from the advice when it is close to being correct, while still having worst-case guarantees even when the advice is arbitrarily bad. Our work is in the same space as the growing body of research on algorithms with predictions. When the advice is a DAG $G$, we design an adaptive search algorithm to recover $G^*$ whose intervention cost is at most $O(\max\{1, \log \psi\})$ times the cost for verifying $G^*$; here, $\psi$ is a distance measure between $G$ and $G^*$ that is upper bounded by the numb
    
[^44]: 面向车辆路径问题的全通用神经方法

    Towards Omni-generalizable Neural Methods for Vehicle Routing Problems. (arXiv:2305.19587v1 [cs.LG])

    [http://arxiv.org/abs/2305.19587](http://arxiv.org/abs/2305.19587)

    提出了一个通用meta-learning框架，使得模型可以在推理过程中快速适应新任务。通过在多个合成和基准数据集上的实验表明，该方法可以有效地解决大小和分布变化的车辆路径问题（VRP）。

    

    由于避免了对手工规则的依赖，学习车辆路径问题（VRP）的启发式方法受到了广泛关注。然而，现有方法通常在固定大小和节点分布的同一任务上进行训练和测试，因此具有有限的泛化性能。本文研究了一个具有挑战性但又现实的场景，该场景考虑了VRP在大小和分布方面的一般性。我们提出了一种通用的元学习框架，在推理期间能够快速适应新任务的能力下对初始化模型进行有效训练。我们进一步开发了一种简单而有效的近似方法来减少训练开销。对旅行商问题（TSP）和容量车辆路径问题（CVRP）的合成和基准实例进行了广泛的实验证明了我们方法的有效性。代码可在https://github.com/RoyalSkye/Omni-VRP得到。

    Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.
    
[^45]: 基于可调层交互的Transformer高效多段编码

    LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction. (arXiv:2305.19585v1 [cs.CL])

    [http://arxiv.org/abs/2305.19585](http://arxiv.org/abs/2305.19585)

    LAIT是一种通过多段编码实现跨段注意力的新框架，使用Layer-Adjustable Interactions技术实现分段编码和逐层交互，提高了模型的效率和精度，并极大地简化了模型设计的复杂度。

    

    Transformer编码器通过对各个令牌的注意力进行编码，使其上下文得以建立，但对于长文本二次计算成本较高。本文提出了名为LAIT的新框架，通过多段编码实现跨段注意力，极大地提升了模型的效率和精度。该框架通过Layer-Adjustable Interactions技术实现分段编码和逐层交互，不仅有效地利用了预训练的Transformers模型，而且极大地简化了模型设计的复杂度。

    Transformer encoders contextualize token representations by attending to all other tokens at each layer, leading to quadratic increase in compute effort with the input length. In practice, however, the input text of many NLP tasks can be seen as a sequence of related segments (e.g., the sequence of sentences within a passage, or the hypothesis and premise in NLI). While attending across these segments is highly beneficial for many tasks, we hypothesize that this interaction can be delayed until later encoding stages.  To this end, we introduce Layer-Adjustable Interactions in Transformers (LAIT). Within LAIT, segmented inputs are first encoded independently, and then jointly. This partial two-tower architecture bridges the gap between a Dual Encoder's ability to pre-compute representations for segments and a fully self-attentive Transformer's capacity to model cross-segment attention. The LAIT framework effectively leverages existing pretrained Transformers and converts them into the h
    
[^46]: 基于高阶累积量的潜在混淆因素因果关系发现

    Causal Discovery with Latent Confounders Based on Higher-Order Cumulants. (arXiv:2305.19582v1 [cs.LG])

    [http://arxiv.org/abs/2305.19582](http://arxiv.org/abs/2305.19582)

    本文提出了一种利用高阶累积量实现潜在混淆因素因果关系发现的新方法，这种方法能够使单潜在成分结构的OICA问题得到闭合形式解决方案，同时还提出了可测试的单潜在成分条件，通过迭代删除共享识别的潜在成分成功扩展了结果到多潜在成分结构。

    

    在许多科学领域中，具有潜在混淆因素的因果关系发现是一项重要但具有挑战性的任务。尽管某些过完备独立分量分析（OICA）方法在某些领域取得了成功，但它们计算成本高，容易陷入局部最优解。我们发现有趣的是，通过利用高阶累积量，在特定情况下（例如，混合过程遵循单潜在分量结构），存在OICA的闭合形式解决方案。鉴于OICA闭合形式解决方案与单潜在成分结构相对应的强大功能，我们制定了一种使用高阶累积量估计混合矩阵的方法，并进一步提出可测试的单潜在成分条件，以识别潜在变量并确定因果顺序。通过迭代删除共享识别的潜在成分，我们成功地将单潜在成分结构的结果扩展到多潜在成分结构。

    Causal discovery with latent confounders is an important but challenging task in many scientific areas. Despite the success of some overcomplete independent component analysis (OICA) based methods in certain domains, they are computationally expensive and can easily get stuck into local optima. We notice that interestingly, by making use of higher-order cumulants, there exists a closed-form solution to OICA in specific cases, e.g., when the mixing procedure follows the One-Latent-Component structure. In light of the power of the closed-form solution to OICA corresponding to the One-Latent-Component structure, we formulate a way to estimate the mixing matrix using the higher-order cumulants, and further propose the testable One-Latent-Component condition to identify the latent variables and determine causal orders. By iteratively removing the share identified latent components, we successfully extend the results on the One-Latent-Component structure to the Multi-Latent-Component structu
    
[^47]: 在线标签移位：最优动态遗憾相遇实用算法

    Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms. (arXiv:2305.19570v1 [stat.ML])

    [http://arxiv.org/abs/2305.19570](http://arxiv.org/abs/2305.19570)

    本文提出了新算法来解决在线标签移位问题，在无需先验知识的情况下通过在线回归保证了最优动态遗憾，并在模拟和真实场景中表现出卓越的性能。

    

    本文关注监督和无监督在线标签移位，其中类边际 $Q(y)$ 变化，但类条件 $Q(x|y)$ 保持不变。在无监督设置中，我们的目标是适应一个从某些离线标记数据训练的学习器，以适应给定未标记在线数据下的变化标签分布。在监督设置中，我们必须学习分类器并适应只给定有标记在线数据的动态演化类边际。我们开发了新算法，将适应问题减少到在线回归并保证无先验知识下的最优动态遗憾。我们的解决方案基于引导在线回归预测器估计，以跟踪漂移比例。在众多模拟和真实世界的在线标签移位场景中的实验证明了我们提出方法的卓越性能，通常实现1-3％的准确性提高，同时具有更小的计算和内存消耗。

    This paper focuses on supervised and unsupervised online label shift, where the class marginals $Q(y)$ varies but the class-conditionals $Q(x|y)$ remain invariant. In the unsupervised setting, our goal is to adapt a learner, trained on some offline labeled data, to changing label distributions given unlabeled online data. In the supervised setting, we must both learn a classifier and adapt to the dynamically evolving class marginals given only labeled online data. We develop novel algorithms that reduce the adaptation problem to online regression and guarantee optimal dynamic regret without any prior knowledge of the extent of drift in the label distribution. Our solution is based on bootstrapping the estimates of \emph{online regression oracles} that track the drifting proportions. Experiments across numerous simulated and real-world online label shift scenarios demonstrate the superior performance of our proposed approaches, often achieving 1-3\% improvement in accuracy while being s
    
[^48]: 无需标注数据的零样本自动发音评估

    Zero-Shot Automatic Pronunciation Assessment. (arXiv:2305.19563v1 [cs.SD])

    [http://arxiv.org/abs/2305.19563](http://arxiv.org/abs/2305.19563)

    本文提出了一种基于预训练声学模型的零样本自动发音评估方法，该方法不需要标注数据，使用掩码模块破坏语音输入，应用k均值聚类获得标记序列并使用评分模块测量错误恢复标记的数量，在实验中取得了与监督回归基线相当的性能以及优于非回归基线的结果。

    

    自动发音评估对于计算机辅助语言学习至关重要。传统方法依赖于带注释的语音文本数据来训练自动语音识别模型或依赖于带分数的语音数据来训练回归模型。本文提出了一种基于预训练声学模型HuBERT的全新零样本自动发音评估方法。我们的方法涉及对语音输入进行编码并通过掩码模块进行破坏。然后使用Transformer编码器并应用k均值聚类以获得标记序列。最后，设计了评分模块来测量错误恢复标记的数量。在speechocean762上的实验结果表明，所提出的方法在皮尔逊相关系数（PCC）方面与监督回归基线方法具有可比性，并且在非回归基线方法方面表现更优。此外，我们还分析了掩码策略对自动发音评估性能的影响。

    Automatic Pronunciation Assessment (APA) is vital for computer-assisted language learning. Prior methods rely on annotated speech-text data to train Automatic Speech Recognition (ASR) models or speech-score data to train regression models. In this work, we propose a novel zero-shot APA method based on the pre-trained acoustic model, HuBERT. Our method involves encoding speech input and corrupting them via a masking module. We then employ the Transformer encoder and apply k-means clustering to obtain token sequences. Finally, a scoring module is designed to measure the number of wrongly recovered tokens. Experimental results on speechocean762 demonstrate that the proposed method achieves comparable performance to supervised regression baselines and outperforms non-regression baselines in terms of Pearson Correlation Coefficient (PCC). Additionally, we analyze how masking strategies affect the performance of APA.
    
[^49]: 强化学习中的可复现性研究

    Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])

    [http://arxiv.org/abs/2305.19562](http://arxiv.org/abs/2305.19562)

    这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。

    

    我们在强化学习 (RL) 的背景下，将可复现性作为算法属性进行了数学研究。我们关注的是具有生成模型访问权的带折扣表格MDP的基本设置。受Impagliazzo等人 [2022]的启发，如果在内部随机性相同时，RL算法在从生成器抽取的两个独立和同分布的样本上执行两次并输出完全相同的策略，则表示该RL算法是可复制的。我们首先提供一个有效的$\rho$-可复制算法，用于$(\varepsilon,\delta)$-最优策略估计，其样本和时间复杂度为 $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$，其中$N$是状态-动作对的数量。然后，对于确定性算法的子类，我们提供了 $ \Omega\left(\frac {N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right) $ 阶的下限。接下来，我们研究了Kalavasis等人[2019]提出的可复制性的松弛版本，其中仅要求算法的输出接近复制算法的输出，而不是相同。我们提供了一种有效算法，其时间和样本复杂度为 $\widetilde O\left(\frac{N^5\cdot\log(1/\delta)}{(1-\gamma)^9\cdot\varepsilon^4\cdot\rho^2}\right)$，用于$(\varepsilon,\delta)$意义下的可复制性，这比先前与相关问题的界限更好。最后，我们讨论了我们的结果对RL算法设计和可重复性研究的未来方向的影响。

    We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\rho$-replicable algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and time complexity $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et 
    
[^50]: 通过群表示学习对称下的字典学习

    Dictionary Learning under Symmetries via Group Representations. (arXiv:2305.19557v1 [math.OC])

    [http://arxiv.org/abs/2305.19557](http://arxiv.org/abs/2305.19557)

    本文研究在预定变换群下学习不变的字典问题。利用非阿贝尔傅里叶分析，提供了算法，建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。

    

    字典学习问题可以被看作是一个数据驱动的过程，旨在学习一个合适的变换，以便通过示例数据直接表示数据的稀疏性。本文研究了在预定的变换群下学习不变的字典问题。自然的应用领域包括冷冻电镜、多目标跟踪、同步和姿态估计等。我们特别从数学表示理论的角度研究了这个问题。通过利用非阿贝尔傅里叶分析，我们为符合这些不变性的字典学习提供了算法。我们将自然界中的字典学习问题，其自然被建模为无限维度的问题，与相关的计算问题，这必然是有限维度的问题，联系起来。我们建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。

    The dictionary learning problem can be viewed as a data-driven process to learn a suitable transformation so that data is sparsely represented directly from example data. In this paper, we examine the problem of learning a dictionary that is invariant under a pre-specified group of transformations. Natural settings include Cryo-EM, multi-object tracking, synchronization, pose estimation, etc. We specifically study this problem under the lens of mathematical representation theory. Leveraging the power of non-abelian Fourier analysis for functions over compact groups, we prescribe an algorithmic recipe for learning dictionaries that obey such invariances. We relate the dictionary learning problem in the physical domain, which is naturally modelled as being infinite dimensional, with the associated computational problem, which is necessarily finite dimensional. We establish that the dictionary learning problem can be effectively understood as an optimization instance over certain matrix o
    
[^51]: 大型语言模型不能作为抽象推理器

    Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])

    [http://arxiv.org/abs/2305.19555](http://arxiv.org/abs/2305.19555)

    本文通过对最先进的大型语言模型进行抽象推理任务评估，发现它们在这方面的表现十分有限，揭示了其在推理方面的局限性。

    

    大型语言模型在自然语言处理任务上表现出极好的性能，包括文本理解和常识推理等。然而，这些成功的机制尚不清楚，LLMs是否能够达到人类的认知能力或这些模型是否还存在根本性的局限性也不确定。抽象推理是认知的基本任务，包括从少量数据中找到和应用一般模式。评估深度神经结构在这个任务上的表现可以揭示它们在推理方面的潜在局限性和广泛的泛化能力，这是一个目前未被探索的领域。本文对最先进的LLMs进行了大量评估，发现它们在抽象推理任务中的表现非常有限，并探究了造成这种差异的原因。

    Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
    
[^52]: 催化馏分神经网络用于少样本开放式催化剂挑战

    Catalysis distillation neural network for the few shot open catalyst challenge. (arXiv:2305.19545v1 [physics.chem-ph])

    [http://arxiv.org/abs/2305.19545](http://arxiv.org/abs/2305.19545)

    本研究介绍了2023年少样本开放式催化剂挑战赛，旨在推进机器学习技术在催化表面上预测催化反应，特别关注双原子催化剂的应用。

    

    人工智能和科学的整合使得计算化学方法在设计和发现新型催化剂方面取得了实质性的进展。然而，电催化反应和开展大规模语言模型在催化领域中的挑战依然存在。此外，ChatGPT的少样本方法超过BERT的最近成功突显了研究中有限的数据、高昂的计算、时间限制和结构活性关系处理的重要性。因此，催化的少样本技术的开发是至关重要的，无论现在和未来的需求是什么。本文介绍了2023年少样本开放式催化剂挑战赛，该赛事旨在推进机器学习技术在预测催化表面上的催化反应，特别关注双原子催化剂的应用。

    The integration of artificial intelligence and science has resulted in substantial progress in computational chemistry methods for the design and discovery of novel catalysts. Nonetheless, the challenges of electrocatalytic reactions and developing a large-scale language model in catalysis persist, and the recent success of ChatGPT's (Chat Generative Pre-trained Transformer) few-shot methods surpassing BERT (Bidirectional Encoder Representation from Transformers) underscores the importance of addressing limited data, expensive computations, time constraints and structure-activity relationship in research. Hence, the development of few-shot techniques for catalysis is critical and essential, regardless of present and future requirements. This paper introduces the Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the application of machine learning technology for predicting catalytic reactions on catalytic surfaces, with a specific focus on dual-atom catalysts in hy
    
[^53]: 自动光谱恢复

    Automatic Illumination Spectrum Recovery. (arXiv:2305.19538v1 [cs.CV])

    [http://arxiv.org/abs/2305.19538](http://arxiv.org/abs/2305.19538)

    本文介绍了利用深度学习网络估计多光谱图像不同光照条件下照明谱的方法，并展示了模型推断的准确性。

    

    我们开发了一个深度学习网络来估计多光谱图像在各种光照条件下的照明谱。为此，我们创建了一个数据集IllumNet。使用Specim IQ相机拍摄各种光照条件下的图像，包括室内外。室外图像在晴天、阴天和不同时间拍摄。室内图像使用卤素和LED光源以及混合光源，主要是卤素或LED和荧光。本研究采用ResNet18网络，但将2D内核改为3D内核以适应数据的光谱特性。除了很好地适应实际照明谱外，预测的照明谱还应平滑，这是通过立方平滑样条误差成本函数实现的。实验结果表明，训练好的模型可以推断出准确的照明谱估计值。

    We develop a deep learning network to estimate the illumination spectrum of hyperspectral images under various lighting conditions. To this end, a dataset, IllumNet, was created. Images were captured using a Specim IQ camera under various illumination conditions, both indoor and outdoor. Outdoor images were captured in sunny, overcast, and shady conditions and at different times of the day. For indoor images, halogen and LED light sources were used, as well as mixed light sources, mainly halogen or LED and fluorescent. The ResNet18 network was employed in this study, but with the 2D kernel changed to a 3D kernel to suit the spectral nature of the data. As well as fitting the actual illumination spectrum well, the predicted illumination spectrum should also be smooth, and this is achieved by the cubic smoothing spline error cost function. Experimental results indicate that the trained model can infer an accurate estimate of the illumination spectrum.
    
[^54]: 基于流数据的神经网络在线学习的低秩扩展卡尔曼滤波算法

    Low-rank extended Kalman filtering for online learning of neural networks from streaming data. (arXiv:2305.19535v1 [stat.ML])

    [http://arxiv.org/abs/2305.19535](http://arxiv.org/abs/2305.19535)

    本文提出一种基于低秩扩展卡尔曼滤波的高效在线学习算法，其能够估计非线性函数的参数，具有更快的适应性和更快的奖励积累。

    

    本文提出了一种高效的在线近似贝叶斯推理算法，用于从可能非平稳的数据流中估计非线性函数的参数。该方法基于扩展卡尔曼滤波器（EKF），但使用了一种新颖的低秩加对角线的后验精度矩阵分解，其每步的成本与模型参数数量成线性关系。与基于随机变分推理的方法不同，我们的方法是完全确定的，并且不需要步长调整。我们通过实验证明，这导致更快（更高效）的学习，从而在用作上下文赌博算法的一部分时实现更快速的适应性和更快的奖励积累。

    We propose an efficient online approximate Bayesian inference algorithm for estimating the parameters of a nonlinear function from a potentially non-stationary data stream. The method is based on the extended Kalman filter (EKF), but uses a novel low-rank plus diagonal decomposition of the posterior precision matrix, which gives a cost per step which is linear in the number of model parameters. In contrast to methods based on stochastic variational inference, our method is fully deterministic, and does not require step-size tuning. We show experimentally that this results in much faster (more sample efficient) learning, which results in more rapid adaptation to changing distributions, and faster accumulation of reward when used as part of a contextual bandit algorithm.
    
[^55]: 用全息约化表示重新建模自注意力

    Recasting Self-Attention with Holographic Reduced Representations. (arXiv:2305.19534v1 [cs.LG])

    [http://arxiv.org/abs/2305.19534](http://arxiv.org/abs/2305.19534)

    本文提出了一种使用HRR的神经符号方法重新构建自注意力的方法，可以实现较低的时间和空间复杂度，并在LRA基准测试中获得了接近于最先进的准确度。

    

    近年来，自注意力已经成为各个领域序列建模的主要范例。然而，在序列长度非常长的领域中，复杂度为$\mathcal{O}(T^2)$的内存和$\mathcal{O}(T^2 \cdot H)$的计算成本可能会使得使用变形金刚网络不可行。受惊物检测中$T \geq 100,000$的序列长度成为深度学习的拦路虎的问题的启发，我们使用全息约化表示（HRR）的神经符号化方法重新构建自注意力。这样我们执行相同的高级策略，即标准自 注意力的查询匹配钥匙，返回每个键的值的加权响应。通过实现“Hrrformer”，我们获得了一些好处，包括$\mathcal{O}(T H \log H)$的时间复杂度、$\mathcal{O}(T H)$的空间复杂度和收敛于$10\times$更少的迭代次数。然而，Hrrformer在LRA基准测试中实现了接近于最先进的准确度，我们能够学习到深度模型。

    In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $\mathcal{O}(T^2)$ memory and $\mathcal{O}(T^2 H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \geq 100,000$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including $\mathcal{O}(T H \log H)$ time complexity, $\mathcal{O}(T H)$ space complexity, and convergence in $10\times$ fewer epochs. Nevertheless, the Hrrformer achieves near state-of-the-art accuracy on LRA benchmarks and we are able to learn wi
    
[^56]: 带有内部分布在线适应的离线元强化学习

    Offline Meta Reinforcement Learning with In-Distribution Online Adaptation. (arXiv:2305.19529v1 [cs.LG])

    [http://arxiv.org/abs/2305.19529](http://arxiv.org/abs/2305.19529)

    本文提出了一种带有不确定性量化的内部分布在线适应(IDAQ)的框架，利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务，在离线元强化学习上具有竞争性表现。

    

    近期的离线元强化学习方法通常利用任务相关的行为策略(例如，对每个个体任务进行RL智能体的训练)来收集多任务数据集。然而，这些方法总是需要额外的信息进行快速调整，例如测试任务的离线上下文。为了解决这个问题，我们首先正式地表征了离线元强化学习中的一个独特挑战：离线数据集和在线适应之间的转换-奖励分布偏移。我们的理论发现，来自分布之外的适应情况可能会导致不可靠的策略评估，并且使用分布内的情况进行在线适应可以确保适应性能保证。基于这些理论洞察，我们提出了一种新的适应框架，称为带有不确定性量化的内部分布在线适应(IDAQ)，它利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务。 实验结果表明，IDAQ的效果优于现有的离线元强化学习方法，并且达到了最先进的在线元强化学习方法的竞争性表现。

    Recent offline meta-reinforcement learning (meta-RL) methods typically utilize task-dependent behavior policies (e.g., training RL agents on each individual task) to collect a multi-task dataset. However, these methods always require extra information for fast adaptation, such as offline context for testing tasks. To address this problem, we first formally characterize a unique challenge in offline meta-RL: transition-reward distribution shift between offline datasets and online adaptation. Our theory finds that out-of-distribution adaptation episodes may lead to unreliable policy evaluation and that online adaptation with in-distribution episodes can ensure adaptation performance guarantee. Based on these theoretical insights, we propose a novel adaptation framework, called In-Distribution online Adaptation with uncertainty Quantification (IDAQ), which generates in-distribution context using a given uncertainty quantification and performs effective task belief inference to address new
    
[^57]: 发现新的可解释保守律作为稀疏不变量

    Discovering New Interpretable Conservation Laws as Sparse Invariants. (arXiv:2305.19525v1 [math.DS])

    [http://arxiv.org/abs/2305.19525](http://arxiv.org/abs/2305.19525)

    这篇论文介绍了一种名为Sparse Invariant Detector（SID）的算法，它能够自动发现微分方程中的保守律。该算法可以重新发现已知的保守律，甚至发现新的保守律，并且已发现的保守律具有稳健性和可解释性。

    

    发现给定动力系统的保守律是重要但具有挑战性的任务。在理论设置（已知微分方程和基函数）中，我们提出了Sparse Invariant Detector（SID），这是一种从微分方程中自动发现保守律的算法。其算法简单性确保了已发现保守数量的稳健性和可解释性。我们展示了SID能够在各种系统中重新发现已知保守律，甚至发现新的保守律。在流体力学和大气化学的两个例子中，SID分别发现了14个和3个守恒量，而这些领域专家先前只知道12个和2个。

    Discovering conservation laws for a given dynamical system is important but challenging. In a theorist setup (differential equations and basis functions are both known), we propose the Sparse Invariant Detector (SID), an algorithm that auto-discovers conservation laws from differential equations. Its algorithmic simplicity allows robustness and interpretability of the discovered conserved quantities. We show that SID is able to rediscover known and even discover new conservation laws in a variety of systems. For two examples in fluid mechanics and atmospheric chemistry, SID discovers 14 and 3 conserved quantities, respectively, where only 12 and 2 were previously known to domain experts.
    
[^58]: 渐进式随机平滑认证。

    Incremental Randomized Smoothing Certification. (arXiv:2305.19521v1 [cs.LG])

    [http://arxiv.org/abs/2305.19521](http://arxiv.org/abs/2305.19521)

    本文提出了渐进式随机平滑认证方法（IRS），可通过重用原始平滑模型的认证保证来认证近似模型，从而显著降低认证修改DNN的计算成本同时保持强大的鲁棒性保证。

    

    随机平滑认证是一种有效的方法，用于获取深度神经网络（DNN）对抗攻击的鲁棒性证书。该方法构建了一个平滑的DNN模型，并通过统计抽样来证明其鲁棒性，但计算代价较高，特别是当使用大量样本进行证明时。此外，当修改平滑模型（例如，量化或修剪）时，认证保证可能不适用于修改的DNN，并且从头开始重新认证可能代价太高。我们提出了第一种渐进式鲁棒性认证随机平滑方法（IRS）。我们展示了如何重复使用原始平滑模型的认证保证，以利用很少的样本认证近似模型。IRS显著降低了认证修改DNN的计算成本，同时保持强大的鲁棒性保证。我们通过实验验证了我们方法的有效性，展示了IRS在多个数据集上的优越性能。

    Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive.  We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showin
    
[^59]: 基于标签检索的扩散模型用于学习嘈杂标签

    Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels. (arXiv:2305.19518v1 [cs.LG])

    [http://arxiv.org/abs/2305.19518](http://arxiv.org/abs/2305.19518)

    本论文提出了一种基于标签检索的扩散模型，用于有效构造并利用伪干净标签，以减小嘈杂标签对分类影响。

    

    学习如何从带有嘈杂标签的数据集中获取信息，一直是机器学习领域的一个重要课题。传统方法通常依赖于比较严格的假设，而且受限于特定类型的标签噪声。本文提出了一种标签噪声问题的生成模型，用于学习和判断随机的标签。作者利用这一模型，提出了基于标签检索的扩散模型，来有效的构造和利用伪干净标签，从而减小嘈杂标签的影响。

    Learning from noisy labels is an important and long-standing problem in machine learning for real applications. One of the main research lines focuses on learning a label corrector to purify potential noisy labels. However, these methods typically rely on strict assumptions and are limited to certain types of label noise. In this paper, we reformulate the label-noise problem from a generative-model perspective, $\textit{i.e.}$, labels are generated by gradually refining an initial random guess. This new perspective immediately enables existing powerful diffusion models to seamlessly learn the stochastic generative process. Once the generative uncertainty is modeled, we can perform classification inference using maximum likelihood estimation of labels. To mitigate the impact of noisy labels, we propose the $\textbf{L}$abel-$\textbf{R}$etrieval-$\textbf{A}$ugmented (LRA) diffusion model, which leverages neighbor consistency to effectively construct pseudo-clean labels for diffusion train
    
[^60]: 基于扩散式语言模型的细粒度文本风格转换

    Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])

    [http://arxiv.org/abs/2305.19512](http://arxiv.org/abs/2305.19512)

    本文提出了一种基于扩散式语言模型的细粒度文本风格转换方法，在不依赖外部信息的情况下取得了比之前利用预训练权重、嵌入和外部语法分析器更好的效果，表明扩散概率模型在文本生成领域具有广泛的应用前景。

    

    扩散式概率模型已经在可控制地生成高质量图像上显示出了巨大的成功，研究人员已经试图将这种可控性运用到文本生成领域。以前的扩散式语言模型研究表明，它们可以在不需要外部知识（如预训练权重）的情况下进行训练，并且仍然可以实现稳定的性能和可控性。 在本文中，我们在StylePTB数据集上训练了一个扩散式模型，这是细粒度文本风格转换的标准基准。与以前的工作评估任务相比，StylePTB中的任务需要对输出文本进行更加精细的控制，我们的模型能够在StylePTB上实现卓越的性能，包括个别和组合转换。此外，我们的模型在没有外部知识的情况下使用StylePTB的有限数据进行训练，其表现优于以前利用预训练权重、嵌入和外部语法分析器的工作，这可能表明扩散概率模型在文本生成领域具有巨大的潜力。

    Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
    
[^61]: 略微超参数化的ReLU网络具有有利的损失景观

    Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape. (arXiv:2305.19510v1 [cs.LG])

    [http://arxiv.org/abs/2305.19510](http://arxiv.org/abs/2305.19510)

    本文研究了略微超参数化的ReLU网络在有限输入数据集上的损失景观，证明了大多数激活模式对应的参数区域没有坏的可微局部极小值，对于一维输入数据，网络可以通过大多数激活模式实现高维全局极小值集合而不具有坏的局部极小值。

    

    本文研究了有限输入数据集上，二层略微超参数化ReLU神经网络的损失景观，使用了参数映射的Jacobian矩阵的秩来估计局部和全局极小值集的维度。使用随机二进制矩阵的结果，我们证明大多数激活模式对应的参数区域没有坏的可微局部极小值。此外，对于一维输入数据，我们证明了网络可以通过大多数的激活模式实现高维全局极小值集合而不具有坏的局部极小值。我们通过发现大多数区域具有完整的秩或缺乏秩，以实验的方式证实了这些结果，这取决于超参数的数量。

    We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parameterization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization.
    
[^62]: 图熵最小化用于半监督节点分类

    Graph Entropy Minimization for Semi-supervised Node Classification. (arXiv:2305.19502v1 [cs.LG])

    [http://arxiv.org/abs/2305.19502](http://arxiv.org/abs/2305.19502)

    本研究提出一种名为图熵最小化的半监督学习方法，可同时解决预测误差、训练资源和推理延迟等三个问题。其采用从大量未分类节点中的一跳聚合进行预测，使得其预测精度与具有两个或更多跳消息传递的GNN相当。而且它还支持随机训练和在线知识蒸馏等加速技术，提高了性能和效率。

    

    工业领域需要综合降低预测误差、训练资源和推理延迟，而节点分类器实现这些方面往往只关注其中一两个方面。因此，妥协的方面成为最短的短板，阻碍了它们在工业级任务中的实际部署。本研究提出了一种新的半监督学习方法，称为图熵最小化（GEM），可同时解决这三个问题。GEM从大量未分类节点中受益于其一跳聚合，使其预测精度与具有两个或更多跳消息传递的GNN相当。它可以被分解为支持最小边缘独立样本的随机训练，实现极快的采样和节省空间的训练。虽然它的一跳聚合在推理中比深度GNN快，但通过在线知识蒸馏可以进一步加速到极致，从而得到非跳节点分类器。

    Node classifiers are required to comprehensively reduce prediction errors, training resources, and inference latency in the industry. However, most graph neural networks (GNN) concentrate only on one or two of them. The compromised aspects thus are the shortest boards on the bucket, hindering their practical deployments for industrial-level tasks. This work proposes a novel semi-supervised learning method termed Graph Entropy Minimization (GEM) to resolve the three issues simultaneously. GEM benefits its one-hop aggregation from massive uncategorized nodes, making its prediction accuracy comparable to GNNs with two or more hops message passing. It can be decomposed to support stochastic training with mini-batches of independent edge samples, achieving extremely fast sampling and space-saving training. While its one-hop aggregation is faster in inference than deep GNNs, GEM can be further accelerated to an extreme by deriving a non-hop classifier via online knowledge distillation. Thus,
    
[^63]: 基于相关性正则化的迁移学习：深入探究领域变换

    Deep into The Domain Shift: Transfer Learning through Dependence Regularization. (arXiv:2305.19499v1 [cs.LG])

    [http://arxiv.org/abs/2305.19499](http://arxiv.org/abs/2305.19499)

    本文提出了一种新的领域适应方法，可以分别衡量内部相关结构的差异和边缘分布的差异， significantly improves the transfer learning performance.

    

    传统的领域自适应方法通过规范化源域（标记）中的特征和目标域（未标记）中特征之间的整体分布差异来获得可转移性。然而，它们往往无法区分领域差异是来自边缘分布还是相关结构。在许多业务和金融应用中，标记函数通常对与边缘分布变化和相关结构变化的敏感程度不同。仅仅测量整体分布差异在获得可转移性方面不够具有判别力。没有必要的结构分辨率，学到的转移效果就会不够优化。本文提出了一种新的领域适应方法，可以分别衡量内部相关结构的差异和边缘分布的差异。通过优化它们之间的相对权重，新的规范化策略大大放松了现有方法的严格性，并显著提高了转移学习性能。具体来说，本文开发了一种新的相关性正则化（DR）方法框架，其中包含一个新的相关性损失和一个融合变量。这种新方法不假设任何特定的分布形式，可以有效地处理高维数据，并可扩展到任何现有的深度转移学习方法。在实验中，DR 方法在几个基准数据集上优于现有的最先进领域适应方法。

    Classical Domain Adaptation methods acquire transferability by regularizing the overall distributional discrepancies between features in the source domain (labeled) and features in the target domain (unlabeled). They often do not differentiate whether the domain differences come from the marginals or the dependence structures. In many business and financial applications, the labeling function usually has different sensitivities to the changes in the marginals versus changes in the dependence structures. Measuring the overall distributional differences will not be discriminative enough in acquiring transferability. Without the needed structural resolution, the learned transfer is less optimal. This paper proposes a new domain adaptation approach in which one can measure the differences in the internal dependence structure separately from those in the marginals. By optimizing the relative weights among them, the new regularization strategy greatly relaxes the rigidness of the existing ap
    
[^64]: 游戏中的学习是否对学习者有益？

    Is Learning in Games Good for the Learners?. (arXiv:2305.19496v1 [cs.GT])

    [http://arxiv.org/abs/2305.19496](http://arxiv.org/abs/2305.19496)

    我们提出了“广义均衡”的概念，通过学习可以在某些游戏中获得更好的结果。如果没有纯纳什均衡，则一名玩家可以从不同策略中受益，结果捕获了Stackelberg均衡的扩展。

    

    我们考虑与奖励和后悔在两个代理之间重复玩游戏相关的一些问题。为了实现这一点，我们引入了广义均衡的概念，该概念允许不对称的后悔约束，并为每个代理和一对后悔约束派生可行值的多面体。作为核心案例，我们强调了一方是禁止交换的，另一方的后悔没有限制。我们证明了这一点，它捕获了与Stackelberg均衡的一种扩展，可匹配最优值，并且存在一大类游戏，在这些游戏中，一名玩家可以通过从禁止交换的后悔算法中偏离，显著增加自己的效用（实际上，几乎所有没有纯纳什均衡的游戏都是这种形式）。

    We consider a number of questions related to tradeoffs between reward and regret in repeated gameplay between two agents. To facilitate this, we introduce a notion of {\it generalized equilibrium} which allows for asymmetric regret constraints, and yields polytopes of feasible values for each agent and pair of regret constraints, where we show that any such equilibrium is reachable by a pair of algorithms which maintain their regret guarantees against arbitrary opponents. As a central example, we highlight the case one agent is no-swap and the other's regret is unconstrained. We show that this captures an extension of {\it Stackelberg} equilibria with a matching optimal value, and that there exists a wide class of games where a player can significantly increase their utility by deviating from a no-swap-regret algorithm against a no-swap learner (in fact, almost any game without pure Nash equilibria is of this form). Additionally, we make use of generalized equilibria to consider tradeo
    
[^65]: 带隐私保障的自适应FDR控制

    Adaptive False Discovery Rate Control with Privacy Guarantee. (arXiv:2305.19482v1 [stat.ML])

    [http://arxiv.org/abs/2305.19482](http://arxiv.org/abs/2305.19482)

    本文提出了一种带隐私保障的自适应FDR控制方法，采用新颖的p值转换方法和镜像剥离算法，可在用户指定的水平α下确切地控制经典的FDR指标，表现更好且可减小隐私泄露风险。

    

    差分隐私的多重检验程序可在保证假阳性率的同时保护用于假设检验的个体信息。本文提出了一种差分隐私的自适应FDR控制方法，可以在用户指定的水平α下确切地控制经典的FDR指标，并提供隐私保障。我们的分析基于两个关键洞见：1）一种新颖的p值转换方法，既保护隐私，同时又保持了镜像保守特性；2）一种镜像剥离算法，允许构建过滤器，并应用最优停止技术。数值研究表明，所提出的DP-AdaPT相比现有的差分隐私FDR控制方法表现更好。与非隐私的AdaPT相比，它会产生一些精度损失，但显著地减小了隐私泄露风险。

    Differentially private multiple testing procedures can protect the information of individuals used in hypothesis tests while guaranteeing a small fraction of false discoveries. In this paper, we propose a differentially private adaptive FDR control method that can control the classic FDR metric exactly at a user-specified level $\alpha$ with privacy guarantee, which is a non-trivial improvement compared to the differentially private Benjamini-Hochberg method proposed in Dwork et al. (2021). Our analysis is based on two key insights: 1) a novel p-value transformation that preserves both privacy and the mirror conservative property, and 2) a mirror peeling algorithm that allows the construction of the filtration and application of the optimal stopping technique. Numerical studies demonstrate that the proposed DP-AdaPT performs better compared to the existing differentially private FDR control methods. Compared to the non-private AdaPT, it incurs a small accuracy loss but significantly re
    
[^66]: 使用值条件状态熵探索加速强化学习

    Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])

    [http://arxiv.org/abs/2305.19476](http://arxiv.org/abs/2305.19476)

    本文提出了一种新的探索技术，使用值条件状态熵来解决强化学习中探索不足的问题，可以均衡地覆盖低价值和高价值状态，相较于现有基于熵的探索方法，该方法在MuJoCo基准测试和Atari游戏上有着显著的提升。

    

    探索的一种有效技术是通过鼓励对访问状态空间的均匀覆盖来最大化已访问状态分布的熵，即状态熵。然而，它在有任务奖励的监督设置中往往难以应对，其中代理趋向于访问高价值状态以利用任务奖励。这个偏好会导致高价值状态和低价值状态的分布不平衡，当分布变得更加均匀时，状态熵会增加，从而偏向于探索低价值区域。当高价值状态在状态空间中分布狭窄时，这个问题会进一步恶化，使得代理完成任务变得更加困难。在本文中，我们提出了一种新颖的探索技术，最大化值条件状态熵，它分别估计每个状态价值估计条件下的状态熵，然后最大化它们的加权和。值条件状态熵量化了低价值和高价值状态区域的覆盖范围，从而使其对不平衡问题更加健壮。我们展示了我们的方法在一系列具有挑战性的MuJoCo基准测试和Atari游戏上显著优于现有的基于熵的探索方法。

    A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
    
[^67]: 双重约束公平聚类

    Doubly Constrained Fair Clustering. (arXiv:2305.19475v1 [cs.LG])

    [http://arxiv.org/abs/2305.19475](http://arxiv.org/abs/2305.19475)

    本论文关注公平聚类问题中的人口统计学公平概念，提出一种同时满足不同公平要求的快速算法。

    

    在过去的几年中，公平聚类吸引了显着的关注，导致出现了大量不同的公平概念。虽然这些概念是有充分理据的，但它们通常在独立的情况下被考虑和研究，其中一个公平要求被独立地考虑而不考虑其他的。这导致了在公平聚类中理解不同公平概念之间的关系成为一个重要的开放问题。在本文中，我们朝向这个方向迈出了第一步。具体来说，我们考虑了聚类中最突出的两种人口统计学公平概念:(1)集团公正(GF)，在这种情况下，不同的人口统计学群体在每个聚类中应该有接近人口水平的代表;(2)中心选择中的多样性(DS)，在这种情况下，所选择的中心应该接近每个群体的人口水平表示。我们展示了在给定一个恒定的近似算法的情况下，一个快速的算法可以同时满足这两个概念。

    The remarkable attention which fair clustering has received in the last few years has resulted in a significant number of different notions of fairness. Despite the fact that these notions are well-justified, they are often motivated and studied in a disjoint manner where one fairness desideratum is considered exclusively in isolation from the others. This leaves the understanding of the relations between different fairness notions as an important open problem in fair clustering. In this paper, we take the first step in this direction. Specifically, we consider the two most prominent demographic representation fairness notions in clustering: (1) Group Fairness (GF), where the different demographic groups are supposed to have close to population-level representation in each cluster and (2) Diversity in Center Selection (DS), where the selected centers are supposed to have close to population-level representation of each group. We show that given a constant approximation algorithm for on
    
[^68]: 对数凹马尔可夫链之链

    Chain of Log-Concave Markov Chains. (arXiv:2305.19473v1 [stat.ML])

    [http://arxiv.org/abs/2305.19473](http://arxiv.org/abs/2305.19473)

    该论文提出了一种新的采样算法，基于对数凹条件概率密度，使用等向性高斯平滑来解决高维下抽样难题。

    

    马尔科夫链蒙特卡罗（MCMC）是一种从未标准化密度中抽样的通用算法类。在高维情况下，MCMC面临两个众所周知的问题：(i)感兴趣的分布在由小概率块隔开的区域中集中;(ii)对数凹性的小概率块本身通常存在病态问题。我们引入了一种采用等向性高斯平滑来解决这些问题的框架。我们证明，无论密度函数的最小假设是什么，从密度函数中采样总是可以分解为通过等噪声测量的累积，从对数凹性条件密度中采样的序列。该构造跟踪了样本历史，因此作为一个整体而言是非马尔可夫的，但历史仅以经验均值的形式出现，从而保证了内存印迹的最小化。我们的采样算法推广了步行跳跃采样（1）。"走"阶段变成了对数凹链的(非马尔可夫)链。

    Markov chain Monte Carlo (MCMC) is a class of general-purpose algorithms for sampling from unnormalized densities. There are two well-known problems facing MCMC in high dimensions: (i) The distributions of interest are concentrated in pockets separated by large regions with small probability mass, and (ii) The log-concave pockets themselves are typically ill-conditioned. We introduce a framework to tackle these problems using isotropic Gaussian smoothing. We prove one can always decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling from log-concave conditional densities via accumulation of noisy measurements with equal noise levels. This construction keeps track of a history of samples, making it non-Markovian as a whole, but the history only shows up in the form of an empirical mean, making the memory footprint minimal. Our sampling algorithm generalizes walk-jump sampling [1]. The "walk" phase becomes a (non-Markovian) chain of log-co
    
[^69]: PlaSma: 为 (反事实) 计划制定增强过程知识模型的小型语言模型

    PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])

    [http://arxiv.org/abs/2305.19472](http://arxiv.org/abs/2305.19472)

    PlaSma提出了一种使用小型语言模型进行过程知识和计划能力的新方法，

    

    过程规划是机器的一项重要而又复杂的任务，它将一个高级目标分解为一系列时间顺序的步骤。它需要整合常识知识以推理出常常是反事实的复杂情境，例如 "没有电话时安排医生的约会"。当前的方法使用大型语言模型 (LLM) 取得了令人鼓舞的结果，但受到昂贵的 API 调用和可复现性问题的限制。本文提出使用更小的语言模型来进行规划，我们介绍了 PlaSma，这是一种新的双重方法，使小型语言模型具有过程知识和 (反事实) 计划能力。更具体地说，我们开发了符号过程知识蒸馏来增强小型语言模型中的隐含知识，以及一种推理算法来促进更结构化和准确的推理。此外，我们还引入了一个新的任务，反事实规划。

    Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
    
[^70]: 用Johnson-Lindenstrauss矩阵进行标签嵌入

    Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])

    [http://arxiv.org/abs/2305.19470](http://arxiv.org/abs/2305.19470)

    这篇论文提出了基于JLMs的标签嵌入方法，将多元分类问题转化为有限回归问题，具有较高的计算效率和预测准确性。

    

    我们提出了一个基于Johnson-Lindenstrauss矩阵（JLMs）的简单且可扩展的极端多元分类框架。利用JLM的列来嵌入标签，将一个C类分类问题转化为具有$\cO(\log C)$输出维度的回归问题。我们得出了一个超量风险限制，阐明了计算效率和预测准确性之间的权衡，并进一步表明，在Massart噪声条件下，降维的惩罚会消失。我们的方法易于并行化，并且实验结果展示了在大规模应用中其有效性和可扩展性。

    We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
    
[^71]: 位置编码对 Transformer 模型长度推广的影响

    The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])

    [http://arxiv.org/abs/2305.19466](http://arxiv.org/abs/2305.19466)

    本文通过实证研究 Transformer 模型中位置编码对于长度推广的影响，结果表明常用的位置编码方法并不适合用于下游任务的长度推广，并且使用位置编码甚至可能会损害长度推广的能力。

    

    Transformer-based 语言模型的开发中，长度推广是一个关键的挑战，它是指从小的训练文本范围到更大范围的泛化能力。位置编码（PE）被发现是影响长度推广的主要因素之一，但不同的 PE 方案对下游任务的外推影响还不清楚。本文通过对比评估五种不同位置编码方法（包括绝对位置嵌入、T5 的相对 PE、ALiBi、Rotary 和无位置编码）的解码器 Transformer 的长度推广能力，对推理和数学任务进行了系统的实证研究。研究发现，常用的位置编码方法，如 ALiBi、Rotary 和 APE，并不适合用于下游任务的长度推广。更重要的是，无 PE 的 Transformer 在推理任务中的表现优于其他显式 PE 方法，这意味着使用位置编码实际上可能会损害长度推广的能力。这些发现揭示了位置编码在有效 Transformer 模型开发中的重要作用。

    Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
    
[^72]: 一种统一的音视频学习框架，用于定位、分离和识别

    A Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition. (arXiv:2305.19458v1 [cs.SD])

    [http://arxiv.org/abs/2305.19458](http://arxiv.org/abs/2305.19458)

    该论文提出了一种统一的音视频学习框架，用于联合定位、分离和识别，包括共享的音视频编码器和针对不同任务的解码器，通过局部音视频对应损失、混合分离框架和强化视觉识别能力来优化不同任务之间的相互依赖性能。

    

    准确识别、定位和分离声源对于任何音视频感知任务来说都至关重要。历史上，这些能力分别被解决，为每个任务单独开发了多种方法。然而，由于源定位、分离和识别具有相互关联的性质，独立模型可能会产生次优性能，因为它们未能捕捉这些任务之间的相互依赖关系。为解决这个问题，我们提出了一个统一的音视频学习框架(称为OneAVM)，它集成了音频和视觉线索，用于联合定位、分离和识别。OneAVM包括一个共享的音视频编码器和针对不同任务的解码器，使用三个目标进行训练。第一个目标通过局部音视频对应损失对齐音频和视觉表示。第二个目标利用传统的混合分离框架来解决视觉源分离问题。最后，第三个目标强化视觉识别能力，使其能够协同进行定位、分离和识别。

    The ability to accurately recognize, localize and separate sound sources is fundamental to any audio-visual perception task. Historically, these abilities were tackled separately, with several methods developed independently for each task. However, given the interconnected nature of source localization, separation, and recognition, independent models are likely to yield suboptimal performance as they fail to capture the interdependence between these tasks. To address this problem, we propose a unified audio-visual learning framework (dubbed OneAVM) that integrates audio and visual cues for joint localization, separation, and recognition. OneAVM comprises a shared audio-visual encoder and task-specific decoders trained with three objectives. The first objective aligns audio and visual representations through a localized audio-visual correspondence loss. The second tackles visual source separation using a traditional mix-and-separate framework. Finally, the third objective reinforces vis
    
[^73]: 动态稀疏是通道级稀疏的学习者

    Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])

    [http://arxiv.org/abs/2305.19454](http://arxiv.org/abs/2305.19454)

    本文提出了一种名为Channel-aware dynamic sparse (Chase)的方法，使用端到端训练实现了GPU友好的通道级别稀疏，不需要任何特殊操作，并且可以直接在通用硬件上加速，显著减小模型大小，同时保持性能。

    

    稀疏训练由于在整个训练过程和推理中具有诱人的节省能力而受到机器学习的广泛关注。动态稀疏训练(DST)作为一种领先的稀疏训练方法，可以从零开始训练深度神经网络，以达到与密集对应物性能相匹配的高稀疏性能。然而，大多数DST之前的研究都表明它们的有效性是在高度不规则的稀疏模式下的非结构化稀疏性上，这在常见硬件上得到了有限的支持。这种限制阻碍了DST在实践中的使用。在本文中，我们提出了一种名为通道感知动态稀疏（Chase）的方法，它将非结构化动态稀疏的性能转换为适合GPU友好的通道级别稀疏，在一个端到端的训练过程中实现，而不需要任何特殊的操作。所得到的小型稀疏网络可以直接通过通用硬件加速，而无需使用专用的稀疏硬件加速器。我们在各种数据集上的实验结果表明，Chase可以在深度神经网络上实现高通道级稀疏性，同时保持其性能，并显着减小模型的大小。

    Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a
    
[^74]: 更大、更好、更快：具有人类效率的人类级Atari游戏

    Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])

    [http://arxiv.org/abs/2305.19452](http://arxiv.org/abs/2305.19452)

    引入BBF基于价值函数的RL代理，在Atari 100K基准测试上实现超人类表现，具有人类效率，提出了在样本高效RL研究的ALE中更新目标的可能。

    

    我们引入了一个名为BBF的基于价值函数的强化学习代理来实现Atari 100K基准测试的超人类表现。BBF依靠神经网络的价值估计扩展以及其他设计选择，在遵循样本高效的情况下，能够实现这种扩展。我们对这些设计选择进行了广泛的分析并为未来的工作提供了洞见。最后，我们讨论了更新ALE上样本高效的RL研究目标的问题。我们将我们的代码和数据公开在https://github.com/google-research/google-research/tree/master/bigger_better_faster。

    We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
    
[^75]: OWAdapt：使用OWA算子的深度学习自适应损失函数

    OWAdapt: An adaptive loss function for deep learning using OWA operators. (arXiv:2305.19443v1 [cs.LG])

    [http://arxiv.org/abs/2305.19443](http://arxiv.org/abs/2305.19443)

    本文提出了一种基于OWA算子的模糊自适应损失函数，通过迭代加权策略应对类别级别噪声条件，提高深度学习在分类任务中的性能。实验证明该方法在各种分类任务中均优于传统的常用损失函数。

    

    本文提出了一种模糊自适应损失函数，用于提升分类任务中深度学习的性能。具体而言，我们重新定义了交叉熵损失，以有效应对类别级别噪声条件，包括类别不平衡这一难题。我们的方法引入了聚合算子，利用模糊逻辑的能力提高了分类精度。我们提出的方法的理论基础在于损失函数内类别级别组件的迭代加权，重点关注那些存在较大误差的组件。为此，我们采用了有序加权平均（OWA）算子，并将其与基于梯度的学习的自适应方案结合在一起。通过大量实验，我们的方法在各种二元和多元分类任务中优于其他常用的损失函数，如标准交叉熵或聚焦损失。此外，我们还探讨了与OWA算子相关的超参数的影响。

    In this paper, we propose a fuzzy adaptive loss function for enhancing deep learning performance in classification tasks. Specifically, we redefine the cross-entropy loss to effectively address class-level noise conditions, including the challenging problem of class imbalance. Our approach introduces aggregation operators, leveraging the power of fuzzy logic to improve classification accuracy. The rationale behind our proposed method lies in the iterative up-weighting of class-level components within the loss function, focusing on those with larger errors. To achieve this, we employ the ordered weighted average (OWA) operator and combine it with an adaptive scheme for gradient-based learning. Through extensive experimentation, our method outperforms other commonly used loss functions, such as the standard cross-entropy or focal loss, across various binary and multiclass classification tasks. Furthermore, we explore the influence of hyperparameters associated with the OWA operators and 
    
[^76]: SimFBO：简单、灵活且通信高效的联邦双层学习

    SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])

    [http://arxiv.org/abs/2305.19442](http://arxiv.org/abs/2305.19442)

    SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。

    

    近来，由于元学习、微调、超参数调整等领域中嵌套优化结构的出现，联邦双层优化（FBO）在机器学习和边缘计算中显示了巨大的潜力。然而，现有的FBO算法往往涉及复杂的计算，并需要每次迭代多个子循环，每个子循环包含多个通信轮。在本文中，我们提出了一个名为SimFBO的简单灵活的FBO框架，它易于实现，不需要子循环，并包括一种广义的服务器端聚合和更新以提高通信效率。我们进一步提出了系统级异构鲁棒FBO（ShroFBO）作为SimFBO的变体，其对本地计算的异构有更强的鲁棒性。我们证明了在部分客户端参与和无替换的客户端采样下，SimFBO和ShroFBO可以实现线性收敛加速，同时改进了样本和通信复杂度。实验证明了它们在图像分类数据集的元学习和真实世界数据集上的超参数优化任务中的有效性。

    Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
    
[^77]: 基于树张量网络、CP秩约束和张量丢弃的机器学习方法。

    Machine learning with tree tensor networks, CP rank constraints, and tensor dropout. (arXiv:2305.19440v1 [cs.LG])

    [http://arxiv.org/abs/2305.19440](http://arxiv.org/abs/2305.19440)

    本文介绍了一种新的机器学习方法，通过基于树状张量网络的CP秩约束和张量丢弃，来构建低秩分类器，并在时尚-MNIST图像分类中展示出了优异的表现。

    

    张量网络可以通过降低自由度来近似表示$N$阶张量，并构成一系列压缩的小张量网络。在[arXiv:2205.15296]文章中，作者提出可以通过对张量网络中的张量的CP秩附加约束，进一步降低计算成本。本文旨在展示如何利用基于树状张量网络(TTN)的CP秩约束和张量丢弃的方法来进行机器学习，并表明该方法在时尚-MNIST图像分类中优于其他基于张量网络的方法。当分支系数$b=4$时，低秩TTN分类器达到了测试集准确率90.3\%，同时拥有较低的计算成本。基于线性元素构成的张量网络分类器避免了深度神经网络的梯度消失问题。CP秩约束还有其他优点：可以减少和调整模型参数数量。

    Tensor networks approximate order-$N$ tensors with a reduced number of degrees of freedom that is only polynomial in $N$ and arranged as a network of partially contracted smaller tensors. As suggested in [arXiv:2205.15296] in the context of quantum many-body physics, computation costs can be further substantially reduced by imposing constraints on the canonical polyadic (CP) rank of the tensors in such networks. Here we demonstrate how tree tensor networks (TTN) with CP rank constraints and tensor dropout can be used in machine learning. The approach is found to outperform other tensor-network based methods in Fashion-MNIST image classification. A low-rank TTN classifier with branching ratio $b=4$ reaches test set accuracy 90.3\% with low computation costs. Consisting of mostly linear elements, tensor network classifiers avoid the vanishing gradient problem of deep neural networks. The CP rank constraints have additional advantages: The number of parameters can be decreased and tuned m
    
[^78]: 缺失值下的公平性干预措施的适应性研究

    Adapting Fairness Interventions to Missing Values. (arXiv:2305.19429v1 [cs.LG])

    [http://arxiv.org/abs/2305.19429](http://arxiv.org/abs/2305.19429)

    本文研究了如何在缺失值的情况下实现公平的分类。传统方法会加剧歧视。本文证明从插补数据训练分类器会恶化组公平性和平均准确性。作者提出可扩展和适应性的算法，可以与其他公平干预算法结合使用，以处理所有可能的缺失模式。

    

    真实世界中数据的缺失值对算法公平性提出了显著而独特的挑战。不同的族群可能不会同等地受到缺失数据的影响，而处理缺失值的标准程序，即先对数据进行插补，然后使用插补的数据进行分类，这个过程被称为“插补再分类”，会加剧歧视。本文分析了缺失值如何影响算法公平性。我们首先证明了从插补数据训练分类器会显著恶化可以实现的组公平性和平均准确性的值。这是因为插补数据会导致数据缺失模式的丢失，数据缺失模式通常会传达有关预测标签的信息。我们提出了可扩展和适应性的算法，用于处理缺失值的公平分类。这些算法可以与任何现有的公平干预算法结合使用，以处理所有可能的缺失模式，并保留信息。

    Missing values in real-world data pose a significant and unique challenge to algorithmic fairness. Different demographic groups may be unequally affected by missing data, and the standard procedure for handling missing values where first data is imputed, then the imputed data is used for classification -- a procedure referred to as "impute-then-classify" -- can exacerbate discrimination. In this paper, we analyze how missing values affect algorithmic fairness. We first prove that training a classifier from imputed data can significantly worsen the achievable values of group fairness and average accuracy. This is because imputing data results in the loss of the missing pattern of the data, which often conveys information about the predictive label. We present scalable and adaptive algorithms for fair classification with missing values. These algorithms can be combined with any preexisting fairness-intervention algorithm to handle all possible missing patterns while preserving informatio
    
[^79]: 评估地理空间背景信息在出行方式检测中的应用

    Evaluating geospatial context information for travel mode detection. (arXiv:2305.19428v1 [physics.soc-ph])

    [http://arxiv.org/abs/2305.19428](http://arxiv.org/abs/2305.19428)

    本研究确定了与相关工作有关的背景表示法，并基于随机森林模型和SHAP方法评估了地理空间背景信息在出行方式检测中的贡献，实验结果表明描述与基础设施网络关系的特征对预测有显着贡献。

    

    利用全球定位卫星系统（GNSS）轨迹检测出行方式对了解个人出行行为至关重要，是实现可持续交通系统的先决条件。虽然研究已经认识到将地理空间背景信息纳入出行方式检测模型中的好处，但很少有文章总结了背景建模方法，并分析了这些背景特征的重要性，这妨碍了高效模型的开发。本文确定了与相关工作有关的背景表示法，并提出了一种分析管道，基于随机森林模型和SHapley Additive exPlanation（SHAP）方法评估地理空间背景信息在出行方式检测中的贡献。通过在大规模GNSS跟踪数据集上的实验，我们发现描述与基础设施网络关系的特征，例如到铁路或道路网络的距离，对模型的预测有显着贡献。

    Detecting travel modes from global navigation satellite system (GNSS) trajectories is essential for understanding individual travel behaviour and a prerequisite for achieving sustainable transport systems. While studies have acknowledged the benefits of incorporating geospatial context information into travel mode detection models, few have summarized context modelling approaches and analyzed the significance of these context features, hindering the development of an efficient model. Here, we identify context representations from related work and propose an analytical pipeline to assess the contribution of geospatial context information for travel mode detection based on a random forest model and the SHapley Additive exPlanation (SHAP) method. Through experiments on a large-scale GNSS tracking dataset, we report that features describing relationships with infrastructure networks, such as the distance to the railway or road network, significantly contribute to the model's prediction. Mo
    
[^80]: ScoNe: 用微调和上下文学习评估语言模型中的否定推理表现的基准测试

    ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning. (arXiv:2305.19426v1 [cs.CL])

    [http://arxiv.org/abs/2305.19426](http://arxiv.org/abs/2305.19426)

    本文提出了Scoped Negation NLI (ScoNe-NLI)基准测试以评估微调和上下文学习策略对语言模型中否定推理表现的影响。研究结果表明，进行许多次微调后，RoBERTa和DeBERTa模型可以成功解决ScoNe-NLI。对于上下文学习方面，大多数提示策略都无法成功，但在嵌入否定推理的短故事的句子完成测试中，InstructGPT是成功的。

    

    最近的一些基准测试试图评估模型处理自然语言否定的能力。然而，这些基准测试缺乏受控的示例范例，无法推断模型是否已经学会了否定语素的语义作用。为了填补这些分析上的空白，我们提出了Scoped Negation NLI (ScoNe-NLI)基准测试，其中包含六个对比示例组成的集合，其中包括多达两个否定语素，其中零个、一个或两个否定语素影响NLI标签。我们使用ScoNe-NLI评估微调和上下文学习策略。我们发现RoBERTa和DeBERTa模型在进行许多次微调之后可以解决ScoNe-NLI。在上下文学习方面，我们测试了InstructGPT模型，并发现大多数提示策略都不成功，包括那些使用逐步推理的策略。为了更好地理解这个结果，我们将ScoNe扩展为ScoNe-NLG，这是一个嵌入了否定推理的短故事的句子完成测试集。在这里，InstructGPT是成功的，揭示了...

    A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveal
    
[^81]: 量化过拟合：通过零空间分析评估神经网络性能

    Quantifying Overfitting: Evaluating Neural Network Performance through Analysis of Null Space. (arXiv:2305.19424v1 [cs.LG])

    [http://arxiv.org/abs/2305.19424](http://arxiv.org/abs/2305.19424)

    本研究量化了过拟合问题，通过神经网络最后一层的零空间分析来评估模型性能并保证了神经网络的隐私和泛化性能。

    

    机器学习模型若过拟合/训练，则更容易受到知识泄漏的威胁，从而对隐私构成风险。本文针对如何在不知道模型的训练准确性的情况下，确定模型是否过拟合或过训练而提出了解决方案。通过分析神经网络最后一层的零空间，我们可以量化过拟合，而不需要访问训练数据或知道那些数据的准确性，从而保证了方法的隐私性和泛化性能。

    Machine learning models that are overfitted/overtrained are more vulnerable to knowledge leakage, which poses a risk to privacy. Suppose we download or receive a model from a third-party collaborator without knowing its training accuracy. How can we determine if it has been overfitted or overtrained on its training data? It's possible that the model was intentionally over-trained to make it vulnerable during testing. While an overfitted or overtrained model may perform well on testing data and even some generalization tests, we can't be sure it's not over-fitted. Conducting a comprehensive generalization test is also expensive. The goal of this paper is to address these issues and ensure the privacy and generalization of our method using only testing data. To achieve this, we analyze the null space in the last layer of neural networks, which enables us to quantify overfitting without access to training data or knowledge of the accuracy of those data. We evaluated our approach on variou
    
[^82]: 自动驾驶中的超车场景数据和知识

    Data and Knowledge for Overtaking Scenarios in Autonomous Driving. (arXiv:2305.19421v1 [cs.RO])

    [http://arxiv.org/abs/2305.19421](http://arxiv.org/abs/2305.19421)

    本文提出了一个新的超车场景数据集和知识表示模型，以帮助自动驾驶车辆进行规划和控制。

    

    自动驾驶已成为人工智能中最流行的研究方向之一。自动驾驶车辆被理解为一个系统，它结合了感知、决策制定、规划和控制等任务，所有这些任务都需要车辆收集周围的数据，以便做出良好的决策和行动。本文的贡献在于提出了一个专注于超车场景的新的合成数据集，以及一个超车动作的知识表示模型，除了用于感知和决策制定任务之外，还可以用于规划和控制算法的开发。

    Autonomous driving has become one of the most popular research topics within Artificial Intelligence. An autonomous vehicle is understood as a system that combines perception, decision-making, planning, and control. All of those tasks require that the vehicle collects surrounding data in order to make a good decision and action. In particular, the overtaking maneuver is one of the most critical actions of driving. The process involves lane changes, acceleration and deceleration actions, and estimation of the speed and distance of the vehicle in front or in the lane in which it is moving. Despite the amount of work available in the literature, just a few handle overtaking maneuvers and, because overtaking can be risky, no real-world dataset is available. This work contributes in this area by presenting a new synthetic dataset whose focus is the overtaking maneuver. We start by performing a thorough review of the state of the art in autonomous driving and then explore the main datasets f
    
[^83]: KrADagrad：Kronecker近似-主导梯度预处理随机优化

    KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned Stochastic Optimization. (arXiv:2305.19416v1 [stat.ML])

    [http://arxiv.org/abs/2305.19416](http://arxiv.org/abs/2305.19416)

    本文提出了一种名为KrAD的新的Kronecker分解预处理方法，用于降低深度学习中二阶优化器的内存和计算资源要求。通过KrADagrad方法，避免了64位精度要求，并在32位精度下表现更好。

    

    二阶随机优化器允许参数更新步长和方向适应损失曲率，但传统上对于深度学习而言需要太多的内存和计算资源。最近，Shampoo [Gupta et al.，2018]引入了Kronecker分解的预处理方法来减少这些要求： 它用于大型深度模型[Anil et al.，2020]并且在生产中[Anil et al.，2022]。 但是，它需要求解病态矩阵的逆矩阵根。这需要64位精度，会产生强硬件限制。本文中，我们提出了一种新的分解方法，即Kronecker近似-主导（KrAD）。 使用KrAD，我们更新一个矩阵，直接近似逆经验Fisher矩阵（类似于完整矩阵AdaGrad），避免求逆矩阵，因此不需要64位精度。我们随后提出KrADagrad$^\star$，其计算成本与Shampoo相似并具有相同的后悔值。在32位精度下，合成的病态实验表现优于Shampoo，同时在64位精度下实现了可比较的结果。我们为KrADagrad的收敛性提供了理论分析，并在标准深度学习基准测试中展示了其有效性。

    Second order stochastic optimizers allow parameter update step size and direction to adapt to loss curvature, but have traditionally required too much memory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018] introduced a Kronecker factored preconditioner to reduce these requirements: it is used for large deep models [Anil et al., 2020] and in production [Anil et al., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices. This requires 64-bit precision, imposing strong hardware constraints. In this paper, we propose a novel factorization, Kronecker Approximation-Domination (KrAD). Using KrAD, we update a matrix that directly approximates the inverse empirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and hence 64-bit precision. We then propose KrADagrad$^\star$, with similar computational costs to Shampoo and the same regret. Synthetic ill-conditioned experiments show improved performance over Shampoo for 32-bit precision, while
    
[^84]: 利用Jarzynski平等式高效训练能量基模型

    Efficient Training of Energy-Based Models Using Jarzynski Equality. (arXiv:2305.19414v1 [cs.LG])

    [http://arxiv.org/abs/2305.19414](http://arxiv.org/abs/2305.19414)

    本文介绍了一种通过使用Jarzynski平等式和顺序蒙特卡罗采样绕过标准对比散度算法中的不可控逼近误差，有效训练能量基模型的方法。

    

    能量基模型是受统计物理启发的生成模型，在无监督学习中有广泛应用。模型分布相对于数据分布的交叉熵是衡量它们性能的最佳指标。然而，使用交叉熵作为训练目标挑战重重，因为它对于模型参数的梯度计算需要对模型分布进行采样。在这里，我们展示了基于Jarzynski等式的非平衡热力学结果，结合顺序蒙特卡罗采样工具，可以有效地进行计算，避免使用标准对比散度算法所产生的不可控逼近误差。具体而言，我们介绍了未调整Langevin算法的修改版本，在其中每个Walker都会获得一个权重，使得能够在任何步骤时估计交叉熵的梯度，从而规避由采样偏差导致的问题。

    Energy-based models (EBMs) are generative models inspired by statistical physics with a wide range of applications in unsupervised learning. Their performance is best measured by the cross-entropy (CE) of the model distribution relative to the data distribution. Using the CE as the objective for training is however challenging because the computation of its gradient with respect to the model parameters requires sampling the model distribution. Here we show how results for nonequilibrium thermodynamics based on Jarzynski equality together with tools from sequential Monte-Carlo sampling can be used to perform this computation efficiently and avoid the uncontrolled approximations made using the standard contrastive divergence algorithm. Specifically, we introduce a modification of the unadjusted Langevin algorithm (ULA) in which each walker acquires a weight that enables the estimation of the gradient of the cross-entropy at any step during GD, thereby bypassing sampling biases induced by
    
[^85]: FRAMM：针对临床试验招募不足的公平排名和缺失数据的解决方案

    FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection. (arXiv:2305.19407v1 [cs.AI])

    [http://arxiv.org/abs/2305.19407](http://arxiv.org/abs/2305.19407)

    本文提出了一个深度强化学习框架FRAMM，用于公平的临床试验选址，该框架可以解决数据缺失和优化招募和多样性之间的权衡。在真实的临床试验数据集上，实验结果表明FRAMM能够实现公平的试验选址，并在不降低招募率的情况下提高多样性。

    

    尽管许多努力已经做出来，但性别、种族和少数民族在临床试验中的代表性不足仍然是一个问题，并且削弱了少数民族的治疗效果。本文针对试验选址任务提出FRAMM，这是一个深度强化学习框架，用于公平的试验选址。我们专注于解决影响公平试验选址的两个现实挑战：许多潜在试验场地的数据模式经常不完整，而且试验选址需要同时优化招募和多样性，因为问题必然是二者之间的权衡，唯一可能通过限制招募数量来增加多样性。为了解决缺失数据的挑战，FRAMM具有一个模态编码器和一个掩码交叉关注机制，可处理缺失数据，无需进行数据填充和完整数据进行训练。为了处理优化招募和多样性的需求，FRAMM提出了一种新颖的公平度量和可调节的限制招募机制，以进行权衡。实验结果表明，在真实的临床试验数据集上，FRAMM能够有效实现公平的试验选址，并在不牺牲入选率的情况下提高多样性。

    Despite many efforts to address the disparities, the underrepresentation of gender, racial, and ethnic minorities in clinical trials remains a problem and undermines the efficacy of treatments on minorities. This paper focuses on the trial site selection task and proposes FRAMM, a deep reinforcement learning framework for fair trial site selection. We focus on addressing two real-world challenges that affect fair trial sites selection: the data modalities are often not complete for many potential trial sites, and the site selection needs to simultaneously optimize for both enrollment and diversity since the problem is necessarily a trade-off between the two with the only possible way to increase diversity post-selection being through limiting enrollment via caps. To address the missing data challenge, FRAMM has a modality encoder with a masked cross-attention mechanism for handling missing data, bypassing data imputation and the need for complete data in training. To handle the need fo
    
[^86]: 脑部肿瘤MRI异质结构分割的增量学习

    Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI. (arXiv:2305.19404v1 [cs.CV])

    [http://arxiv.org/abs/2305.19404](http://arxiv.org/abs/2305.19404)

    本文提出了一种“分歧感知”的双流增量学习框架，适用于针对不断演化的目标域数据进行分割任务，解决了分布变化、未见过结构和训练数据缺失等挑战。

    

    通过单源域训练的静态深度学习模型，在不同的解剖结构分割任务中取得了巨大的成功。但是，在不断演化的环境中，静态模型的表现可能会变差，需要适当地更新模型。在增量学习环境下，我们希望能够更新好的静态模型，跟随不断演化的目标域数据（例如，来自不同站点的额外损伤或感兴趣结构），而不会发生灾难性遗忘。然而，这也带来了一些挑战：分布的变化、在初始模型训练期间未见过的其他结构以及源域中缺乏训练数据。为了解决这些挑战，本文致力于以统一的方式逐步演化“即插即用”训练的分割模型，以应对不断增加的解剖分类数据集。具体地，我们首先提出了一种“分歧感知”的双流增量学习框架

    Deep learning (DL) models for segmenting various anatomical structures have achieved great success via a static DL model that is trained in a single source domain. Yet, the static DL model is likely to perform poorly in a continually evolving environment, requiring appropriate model updates. In an incremental learning setting, we would expect that well-trained static models are updated, following continually evolving target domain data -- e.g., additional lesions or structures of interest -- collected from different sites, without catastrophic forgetting. This, however, poses challenges, due to distribution shifts, additional structures not seen during the initial model training, and the absence of training data in a source domain. To address these challenges, in this work, we seek to progressively evolve an ``off-the-shelf" trained segmentation model to diverse datasets with additional anatomical categories in a unified manner. Specifically, we first propose a divergence-aware dual-fl
    
[^87]: DyGen: 通过动态增强的生成建模从噪声标签中学习

    DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling. (arXiv:2305.19395v1 [cs.CL])

    [http://arxiv.org/abs/2305.19395](http://arxiv.org/abs/2305.19395)

    DyGen是一个动态增强的生成模型，使用嵌入空间中的动态模式可以改善从噪声标签中学习的精度，同时使用共规正则化机制来最小化潜在噪声标签和先验的影响，展示了最先进的性能。

    

    在许多实际应用中，训练数据可能包含不正确或已损坏的标签，从噪声标签中学习是一个挑战。当使用带有噪声标签的语言模型进行微调时，模型很容易过度拟合标签噪声，导致性能下降。大多数现有的从噪声标签中学习的方法使用静态输入特征进行去噪，但这些方法受限于它们在真实标签分布方面提供的信息，可能导致有偏的或不正确的预测。在这项工作中，我们提出了一个名为DyGen的动态增强生成模型，该模型在语言模型的微调过程中利用嵌入空间中的动态模式来改善噪声标签预测。DyGen使用变分自动编码框架从噪声标签和训练动态中推断真实标签的后验分布。此外，使用共规正则化机制来最小化潜在噪声标签和先验的影响。在存在不同级别的标签噪声情况下，DyGen在两个大规模文本分类数据集上展示了最先进的性能。

    Learning from noisy labels is a challenge that arises in many real-world applications where training data can contain incorrect or corrupted labels. When fine-tuning language models with noisy labels, models can easily overfit the label noise, leading to decreased performance. Most existing methods for learning from noisy labels use static input features for denoising, but these methods are limited by the information they can provide on true label distributions and can result in biased or incorrect predictions. In this work, we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic patterns in the embedding space during the fine-tuning process of language models to improve noisy label predictions. DyGen uses the variational auto-encoding framework to infer the posterior distributions of true labels from noisy labels and training dynamics. Additionally, a co-regularization mechanism is used to minimize the impact of potentially noisy labels and priors. DyGen demonstr
    
[^88]: 突触权重分布取决于可塑性的几何形态

    Synaptic Weight Distributions Depend on the Geometry of Plasticity. (arXiv:2305.19394v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.19394](http://arxiv.org/abs/2305.19394)

    计算神经科学的研究表明，突触权重分布取决于突触可塑性的几何形态，进而表明实验观测到的对数正态权重分布与标准的梯度下降模型不一致，可能说明大脑中使用的是非欧几里得距离。

    

    机器学习中的大多数学习算法都依赖于梯度下降调整模型参数，计算神经科学中日益增长的文献利用这些思想研究突触可塑性。然而，绝大部分此类研究忽略了一个关键的基本假设：突触变化的距离选择（即突触可塑性的几何形态）。梯度下降假定距离为欧几里得距离，但许多其他距离也是可能的，并且生物学不一定使用欧几里得几何形态。在这里，我们使用镜像下降提供的理论工具表明，无论最小化的损失为何，突触权重的分布都取决于突触可塑性的几何形态。我们利用这些结果表明，在几个大脑区域中发现的实验观测到的对数正态权重分布与标准的梯度下降（即欧几里得几何形态）不一致，而是与非欧几里得距离一致。

    Most learning algorithms in machine learning rely on gradient descent to adjust model parameters, and a growing literature in computational neuroscience leverages these ideas to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes (i.e. the geometry of synaptic plasticity). Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that, regardless of the loss being minimized, the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with non-Euclidean distances.
    
[^89]: 带有不完整噪声配对注释的深度聚类：一种几何正则化方法

    Deep Clustering with Incomplete Noisy Pairwise Annotations: A Geometric Regularization Approach. (arXiv:2305.19391v1 [cs.LG])

    [http://arxiv.org/abs/2305.19391](http://arxiv.org/abs/2305.19391)

    本文通过研究逻辑DCC损失函数的理论性质，提出了一种基于几何因子分析的新损失函数用以抵御嘈杂的注释并进一步提高聚类性能。

    

    近年来，深度学习和基于注释相似性对限制聚类的结合，即深度限制聚类（DCC），已被证明对于将弱监督纳入大规模数据聚类是有效的：少于1％的成对相似性注释通常可以显著提高聚类精度。然而，除了经验性成功外，对DCC缺乏理解。此外，许多DCC范例对注释噪声敏感，但具有性能保证的嘈杂DCC方法在很大程度上难以捉摸。本文首先对最近出现的DCC逻辑损失函数进行了深入研究，并表征了其理论性质。我们的结果表明，逻辑DCC损失确保在合理条件下数据成员的可识别性，这可能为其在实践中的有效性提供了启示。在此基础上，提出了一种基于几何因子分析的新损失函数，以抵御嘈杂的注释并进一步提高聚类性能。在几个数据集上的实验证明了所提出方法的有效性。

    The recent integration of deep learning and pairwise similarity annotation-based constrained clustering -- i.e., $\textit{deep constrained clustering}$ (DCC) -- has proven effective for incorporating weak supervision into massive data clustering: Less than 1% of pair similarity annotations can often substantially enhance the clustering accuracy. However, beyond empirical successes, there is a lack of understanding of DCC. In addition, many DCC paradigms are sensitive to annotation noise, but performance-guaranteed noisy DCC methods have been largely elusive. This work first takes a deep look into a recently emerged logistic loss function of DCC, and characterizes its theoretical properties. Our result shows that the logistic DCC loss ensures the identifiability of data membership under reasonable conditions, which may shed light on its effectiveness in practice. Building upon this understanding, a new loss function based on geometric factor analysis is proposed to fend against noisy an
    
[^90]: EEG信号中时空特征用于跨主体情感识别

    Inter Subject Emotion Recognition Using Spatio-Temporal Features From EEG Signal. (arXiv:2305.19379v1 [cs.HC])

    [http://arxiv.org/abs/2305.19379](http://arxiv.org/abs/2305.19379)

    本研究提出了一种可跨主体对EEG信号进行情感分类的模型，使用了常规、深度和可分离卷积层，达到了73.04％的准确性。

    

    跨主体/个体无关情感识别一直是情感计算中的难题。本研究提出了一种易于实现的情感识别模型，可对EEG信号进行跨主体的情感分类。该模型基于著名的EEGNet架构，该架构常用于与EEG相关的BCI中。作者采用了DENS数据集，其中包含情感事件的精确信息，用于情感分类。该模型结合了常规、深度和可分离卷积层，可对情感进行分类。该模型具有学习EEG通道的空间特征和随时间变化的EEG信号变异的时间特征的能力。该模型对情感评分进行了评估。模型达到了73.04％的准确性。

    Inter-subject or subject-independent emotion recognition has been a challenging task in affective computing. This work is about an easy-to-implement emotion recognition model that classifies emotions from EEG signals subject independently. It is based on the famous EEGNet architecture, which is used in EEG-related BCIs. We used the Dataset on Emotion using Naturalistic Stimuli (DENS) dataset. The dataset contains the Emotional Events -- the precise information of the emotion timings that participants felt. The model is a combination of regular, depthwise and separable convolution layers of CNN to classify the emotions. The model has the capacity to learn the spatial features of the EEG channels and the temporal features of the EEG signals variability with time. The model is evaluated for the valence space ratings. The model achieved an accuracy of 73.04%.
    
[^91]: 惰性训练下深层神经网络中的良性过拟合

    Benign Overfitting in Deep Neural Networks under Lazy Training. (arXiv:2305.19377v1 [cs.LG])

    [http://arxiv.org/abs/2305.19377](http://arxiv.org/abs/2305.19377)

    本论文证明了在惰性训练期间，超参数化深度神经网络存在良性过拟合现象，可在数据分布良好分离时实现贝叶斯最优测试误差。通过插值平滑可以带来更好的泛化性能。

    

    本文研究了采用ReLU激活函数的超参数化深度神经网络，并证明了在数据分布良好分离时，采用惰性训练方法可以使DNN分类实现贝叶斯最优测试误差，同时获得（几乎）零训练误差。为此，我们统一了超参数化、良性过拟合和DNN Lipschitz常数这三个相互关联的概念。研究结果表明，插值平滑可以带来更好的泛化性能。此外，我们还探讨了神经切向核法进行插值平滑的情况，证明了泛化误差收敛到仅取决于标签噪声和初始化噪声的常数阶，理论上证实了良性过拟合现象。我们的分析还提供了非平滑激活函数下归一化边缘的严格下界。

    This paper focuses on over-parameterized deep neural networks (DNNs) with ReLU activation functions and proves that when the data distribution is well-separated, DNNs can achieve Bayes-optimal test error for classification while obtaining (nearly) zero-training error under the lazy training regime. For this purpose, we unify three interrelated concepts of overparameterization, benign overfitting, and the Lipschitz constant of DNNs. Our results indicate that interpolating with smoother functions leads to better generalization. Furthermore, we investigate the special case where interpolating smooth ground-truth functions is performed by DNNs under the Neural Tangent Kernel (NTK) regime for generalization. Our result demonstrates that the generalization error converges to a constant order that only depends on label noise and initialization noise, which theoretically verifies benign overfitting. Our analysis provides a tight lower bound on the normalized margin under non-smooth activation 
    
[^92]: LOPO性能预测的RF + clust灵敏度分析

    Sensitivity Analysis of RF+clust for Leave-one-problem-out Performance Prediction. (arXiv:2305.19375v1 [cs.LG])

    [http://arxiv.org/abs/2305.19375](http://arxiv.org/abs/2305.19375)

    本文提出了一种新的方法来解决机器学习中的LOPO问题，通过调整距离加权和引进基于特征的重要性，实验结果表明在预测准确性和推广能力方面比RF + clust更优。

    

    Leave-one-problem-out（LOPO）性能预测需要机器学习（ML）模型将算法的性能从一组训练问题推广到之前未见过的问题上。即使对于最先进的方法，LOPO也是一项非常具有挑战性的任务。在更简单的leave-one-instance-out场景中表现良好的模型通常未能很好地推广到LOPO设置中。为了解决LOPO问题，最近的研究建议使用加权算法性能的随机森林（RF）性能回归模型对标准模型进行扩展，这些算法性能被认为与测试问题相似。更准确地说，在这个RF + clust方法中，权重是根据某些特征空间中问题的距离成比例选择的。在这项工作中，我们通过调整基于特征的重要性对性能回归的距离加权来扩展RF + clust方法。也就是说，我们不再考虑特征空间中的余弦距离，而是考虑加权余弦距离，其中权重是基于RF中的特征重要性度量自动计算的。我们在不同领域的23个数据集上进行了大量实验，以比较RF + clust与所提出方法的性能。结果表明，所提出的方法在预测准确性和对新问题的推广能力方面比RF + clust显着更优，特别是当训练问题的数量较少时。

    Leave-one-problem-out (LOPO) performance prediction requires machine learning (ML) models to extrapolate algorithms' performance from a set of training problems to a previously unseen problem. LOPO is a very challenging task even for state-of-the-art approaches. Models that work well in the easier leave-one-instance-out scenario often fail to generalize well to the LOPO setting. To address the LOPO problem, recent work suggested enriching standard random forest (RF) performance regression models with a weighted average of algorithms' performance on training problems that are considered similar to a test problem. More precisely, in this RF+clust approach, the weights are chosen proportionally to the distances of the problems in some feature space. Here in this work, we extend the RF+clust approach by adjusting the distance-based weights with the importance of the features for performance regression. That is, instead of considering cosine distance in the feature space, we consider a weig
    
[^93]: 视觉概念学习中的组合多样性

    Compositional diversity in visual concept learning. (arXiv:2305.19374v1 [cs.CV])

    [http://arxiv.org/abs/2305.19374](http://arxiv.org/abs/2305.19374)

    本文研究了人类如何利用组合性进行视觉概念学习，开发了一个程序归纳模型来生成候选视觉图形，发现人类和模型都可以进行多样的组合泛化。

    

    人类利用组合性来有效地学习新概念，理解熟悉部件如何组合在一起形成新颖对象。相比之下，流行的计算机视觉模型难以进行相同类型的推理，需要更多数据，并且不如人类具有灵活的泛化能力。本文研究这些人类特有的能力在不同类型的视觉组合中的表现，研究人类如何对具有丰富关系结构的“外星人图形”进行分类和生成。我们还开发了一个贝叶斯程序归纳模型，搜索生成候选视觉图形的最佳程序，利用包含不同组合机制和抽象的大型程序空间。在少样本分类任务中，我们发现人类和程序归纳模型可以进行许多有意义的组合泛化，模型提供了实验数据的强有力解释以及显示人类式组合推理的可解释参数。

    Humans leverage compositionality to efficiently learn new concepts, understanding how familiar parts can combine together to form novel objects. In contrast, popular computer vision models struggle to make the same types of inferences, requiring more data and generalizing less flexibly than people do. Here, we study these distinctively human abilities across a range of different types of visual composition, examining how people classify and generate ``alien figures'' with rich relational structure. We also develop a Bayesian program induction model which searches for the best programs for generating the candidate visual figures, utilizing a large program space containing different compositional mechanisms and abstractions. In few shot classification tasks, we find that people and the program induction model can make a range of meaningful compositional generalizations, with the model providing a strong account of the experimental data as well as interpretable parameters that reveal huma
    
[^94]: 通过挖掘临床笔记中的主题识别心力衰竭患者的表型并预测住院时间

    Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure. (arXiv:2305.19373v1 [cs.LG])

    [http://arxiv.org/abs/2305.19373](http://arxiv.org/abs/2305.19373)

    本文采用主题建模技术对1200例心力衰竭患者的诊断编码和程序报告中存在的主题进行识别，旨在从中识别心力衰竭的临床表型并预测病人住院时间。

    

    心力衰竭是一种综合征，当心脏无法泵血和输送氧气以支持体内其他器官时出现。识别接受心力衰竭治疗的病人的诊断编码和程序报告中的潜在主题，可以揭示与心力衰竭相关的临床表型，并根据相似的特征对病人进行分组，这也有助于预测病人的住院时间。由于这些临床表型通常具有概率的潜在结构，并且之前没有关于使用概率框架在心力衰竭患者的临床笔记中识别表型和使用数据驱动的基于人工智能的方法预测这些患者的住院时间的研究，因此我们采用自然语言处理技术——主题建模，对伊利诺伊大学医院1200例心力衰竭患者的诊断编码和程序报告中存在的主题进行识别。

    Heart failure is a syndrome which occurs when the heart is not able to pump blood and oxygen to support other organs in the body. Identifying the underlying themes in the diagnostic codes and procedure reports of patients admitted for heart failure could reveal the clinical phenotypes associated with heart failure and to group patients based on their similar characteristics which could also help in predicting patient outcomes like length of stay. These clinical phenotypes usually have a probabilistic latent structure and hence, as there has been no previous work on identifying phenotypes in clinical notes of heart failure patients using a probabilistic framework and to predict length of stay of these patients using data-driven artificial intelligence-based methods, we apply natural language processing technique, topic modeling, to identify the themes present in diagnostic codes and in procedure reports of 1,200 patients admitted for heart failure at the University of Illinois Hospital 
    
[^95]: 大型长序列模型的块级并行Transformer

    Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])

    [http://arxiv.org/abs/2305.19370](http://arxiv.org/abs/2305.19370)

    本文提出了块级并行Transformer方法，以最小化内存成本，能够处理长序列，并且可以处理比先前的内存高效方法更长32倍的训练序列。

    

    Transformer已经成为最先进的自然语言处理模型的基石，在各种AI应用中展现出出色的性能。然而，Transformer中的自我注意机制和大型前馈网络所需的内存容量限制了它们处理长序列的能力，从而为涉及多个长序列或长期依赖的任务带来了挑战。我们提出了一种独特的方法，块级并行Transformer（BPT），它利用块级计算自我注意和前馈网络融合以最小化内存成本。通过在保持内存效率的同时处理更长的输入序列，BPT使训练序列的长度比原始的Transformer长32倍，比先前的内存高效方法长2到4倍。对语言建模和强化学习任务进行的大量实验证明了BPT在减少内存需求和提高性能方面的有效性。

    Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
    
[^96]: 单一生成流网络中的图结构与参数的联合贝叶斯推理

    Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network. (arXiv:2305.19366v1 [cs.LG])

    [http://arxiv.org/abs/2305.19366](http://arxiv.org/abs/2305.19366)

    本文提出了在单一生成流网络中联合建模贝叶斯网络结构和参数的方法，包括非离散样本空间，提高了贝叶斯网络局部概率模型的灵活性。

    

    生成流网络是一类对离散和结构化样本空间进行建模的生成模型。先前的研究已将其应用于推断给定观测数据的贝叶斯网络的有向无环图（DAG）的边缘后验分布。本文基于最近的研究进展，在非离散样本空间上将此框架扩展到联合后验分布的建模，不仅包括贝叶斯网络的结构，还考虑了其条件概率分布的参数。

    Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized
    
[^97]: 稳健的各向异性正则化

    Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])

    [http://arxiv.org/abs/2305.19358](http://arxiv.org/abs/2305.19358)

    本文提出了一种新颖的正则化方法I-STAR，可以增加模型的稳定性，提高性能，并改善自然语言处理中的组合表示问题。

    

    鉴于大型语言模型（LLMs）的成功，研究模型激活的属性已引起了相当大的兴趣。文献普遍认为LLMs表示由少数具有极高方差和幅度的“异常维度”主导。自然语言处理（NLP）中的几项研究试图减轻这些异常维度的影响，并迫使LLMs成为各向同性（即在嵌入空间中所有维度具有均匀方差）的。各向同性被认为是LLMs的一种理想属性，可以提高模型性能并更加贴近人类直觉的文本表示。然而，关于NLP中各向同性的许多观点都是基于嵌入的平均余弦相似度，最近已经表明这是一种有缺陷的各向同性度量。在本文中，我们提出了I-STAR：基于IsoScore$^{\star}$的稳定各向异性正则化，这是一种新颖的正则化方法，可以用于增加模型的稳定性并提高性能。

    Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
    
[^98]: 利用声子元结构实现多功能机械-智能的发现与挖掘

    Uncovering multifunctional mechano-intelligence in and through phononic metastructures harnessing physical reservoir computing. (arXiv:2305.19354v1 [physics.app-ph])

    [http://arxiv.org/abs/2305.19354](http://arxiv.org/abs/2305.19354)

    在机械领域实现智能的新方法——利用声子元结构实现多功能机械-智能；通过物理储库计算框架，在机械领域直接实现计算能力和各种智能元素，提供了具体的整合不同智能元素的系统基础。

    

    自主系统的最新进展促使下一代自适应结构和材料在机械领域拥有更多内置智能（MI）需求强烈。以前的MI尝试主要集中在特定设计和案例研究上，以实现有限的MI方面，并且缺乏在有效和高效方式下构建和整合不同智能元素的系统基础。在这里，我们提出了一种新的方法来创建实现综合多功能MI所需基础的物理储库计算（PRC）框架。也就是说，直接在机械领域中同时体现计算能力和各种智能元素，即感知、决策和命令，从传统依靠外加数字计算机和大规模电子实现智能的自适应结构中发展。作为示例平台，我们使用一款

    The recent advances in autonomous systems have prompted a strong demand for the next generation of adaptive structures and materials to possess more built-in intelligence in their mechanical domain, the so-called mechano-intelligence (MI). Previous MI attempts mainly focused on specific designs and case studies to realize limited aspects of MI, and there is a lack of a systematic foundation in constructing and integrating the different elements of intelligence in an effective and efficient manner. Here, we propose a new approach to create the needed foundation in realizing integrated multifunctional MI via a physical reservoir computing (PRC) framework. That is, to concurrently embody computing power and the various elements of intelligence, namely perception, decision-making, and commanding, directly in the mechanical domain, advancing from conventional adaptive structures that rely solely on add-on digital computers and massive electronics to achieve intelligence. As an exemplar plat
    
[^99]: 基于随机梯度马尔科夫链蒙特卡罗的非凸贝叶斯学习

    Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte Carlo. (arXiv:2305.19350v1 [stat.CO])

    [http://arxiv.org/abs/2305.19350](http://arxiv.org/abs/2305.19350)

    提出了一种基于随机梯度马尔科夫链蒙特卡罗的方法来解决非凸贝叶斯学习问题，具有理论保证。

    

    人工智能的兴起取决于现代深度神经网络的有效训练，这涉及到非凸优化和不确定性量化，归结为非凸贝叶斯学习问题。为了解决这个问题，本文提出了一种基于随机梯度马尔科夫链蒙特卡罗的方法，用于近似后验分布并具有理论保证。

    The rise of artificial intelligence (AI) hinges on the efficient training of modern deep neural networks (DNNs) for non-convex optimization and uncertainty quantification, which boils down to a non-convex Bayesian learning problem. A standard tool to handle the problem is Langevin Monte Carlo, which proposes to approximate the posterior distribution with theoretical guarantees. In this thesis, we start with the replica exchange Langevin Monte Carlo (also known as parallel tempering), which proposes appropriate swaps between exploration and exploitation to achieve accelerations. However, the na\"ive extension of swaps to big data problems leads to a large bias, and bias-corrected swaps are required. Such a mechanism leads to few effective swaps and insignificant accelerations. To alleviate this issue, we first propose a control variates method to reduce the variance of noisy energy estimators and show a potential to accelerate the exponential convergence. We also present the population-
    
[^100]: 关于黎曼流形上无投影在线学习的研究

    On Riemannian Projection-free Online Learning. (arXiv:2305.19349v1 [cs.LG])

    [http://arxiv.org/abs/2305.19349](http://arxiv.org/abs/2305.19349)

    本文提出了一种针对非凸约束集情况下的曲线空间在线测地凸优化的无投影算法，获得了次线性遗憾保证。

    

    投影操作是许多优化算法（例如在线梯度下降[OGD]）中强制约束和实现最优遗憾边界所必需的关键组成部分。然而，当处理高维设置或具有病态约束集时，它会受到计算复杂度限制。无投影算法通过用更有效的优化子程序取代投影预测来解决此问题。但到目前为止，这些方法主要在欧几里得设置中开发，并且虽然越来越多地关注黎曼流形上的优化，但在尝试利用无投影工具方面基本上没有工作。一个明显的问题是，在这些领域中，非平凡的仿射函数通常是非凸的。在本文中，我们提出了一种方法，在曲线空间上进行在线测地凸优化，以获得两种情况下的次线性遗憾保证：当我们访问（a）时

    The projection operation is a critical component in a wide range of optimization algorithms, such as online gradient descent (OGD), for enforcing constraints and achieving optimal regret bounds. However, it suffers from computational complexity limitations in high-dimensional settings or when dealing with ill-conditioned constraint sets. Projection-free algorithms address this issue by replacing the projection oracle with more efficient optimization subroutines. But to date, these methods have been developed primarily in the Euclidean setting, and while there has been growing interest in optimization on Riemannian manifolds, there has been essentially no work in trying to utilize projection-free tools here. An apparent issue is that non-trivial affine functions are generally non-convex in such domains. In this paper, we present methods for obtaining sub-linear regret guarantees in online geodesically convex optimization on curved spaces for two scenarios: when we have access to (a) a s
    
[^101]: 癫痫发作检测：解剖与分析。

    Epilepsy Seizure Detection: Anatomy and Analysis. (arXiv:2305.19347v1 [cs.LG])

    [http://arxiv.org/abs/2305.19347](http://arxiv.org/abs/2305.19347)

    该研究提出了一种通用、经济、非侵入性的癫痫检测系统，基于简单的实时kNN机器学习，可在不到四秒的训练时间内定制和适应个人用户，并具有94.5%的平均准确率。

    

    癫痫跟踪系统对于监测和评估癫痫治疗非常重要。目前癫痫护理中使用的护理记录可能会错过癫痫发作。可穿戴的监测设备可能更容易被耐受，并且更适合长期进行。许多技术和方法被提出用于癫痫发作检测；然而，在保持检测精度的同时，简单性和费用都是日常使用的关键概念。在本研究中，我们提出了一种基于简单实时k-最近邻（kNN）机器学习的通用、经济、非侵入性的癫痫检测系统，并可在不到四秒的训练时间内定制和适应个人用户；该系统经过了500个被试验者的验证和验证，抽样频率为178 Hz，其平均准确率为94.5％。

    A seizure tracking system is crucial for monitoring and evaluating epilepsy treatments. Caretaker seizure diaries are used in epilepsy care today, but clinical seizure monitoring may miss seizures. Monitoring devices that can be worn may be better tolerated and more suitable for long-term ambulatory use. Many techniques and methods are proposed for seizure detection; However, simplicity and affordability are key concepts for daily use while preserving the accuracy of the detection. In this study, we propose a versal, affordable noninvasive based on a simple real-time k-Nearest-Neighbors (kNN) machine learning that can be customized and adapted to individual users in less than four (4) seconds of training time; the system was verified and validated using 500 subjects, with seizure detection data sampled at 178 Hz, the operated with a mean accuracy of (94.5%).
    
[^102]: HiGen：层次图生成网络

    HiGen: Hierarchical Graph Generative Networks. (arXiv:2305.19337v1 [cs.LG])

    [http://arxiv.org/abs/2305.19337](http://arxiv.org/abs/2305.19337)

    HiGen是一种新颖的图形生成网络，能够以粗到细的方式捕捉图形的层次结构，并使用多项式分布来生成具有整数值的子图的边权。它能够有效地捕捉图形的局部和全局属性，并实现了最先进的性能。

    

    大多数真实世界的图表现出层次结构，这通常被现有的图形生成方法所忽视。为了解决这个限制，我们提出了一种新颖的图形生成网络，能够以粗到细的方式捕捉图形的层次结构并成功地生成图形子结构。在每个层次上，该模型并行生成社区，使用独立的模型预测社区之间的跨边。这种模块化方法使生成的图形网络高度可扩展。此外，我们用多项式分布建模层次图形的输出分布，并针对此分布推导了递归分解，使我们能够以自回归的方式生成具有整数值的子图的边权。实证研究证明，所提出的生成模型能够有效地捕捉图形的局部和全局属性，并实现了最先进的性能。

    Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using a separate model. This modular approach results in a highly scalable graph generative network. Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution, enabling us to generate sub-graphs with integer-valued edge weights in an autoregressive approach. Empirical studies demonstrate that the proposed generative model can effectively capture both local and global properties of graphs and achieves state-of-the-art performanc
    
[^103]: 缓解测试时间偏差，实现公平的图像检索

    Mitigating Test-Time Bias for Fair Image Retrieval. (arXiv:2305.19329v1 [cs.CV])

    [http://arxiv.org/abs/2305.19329](http://arxiv.org/abs/2305.19329)

    本文提出了后置偏差缓解（PBM）技术，解决了如何在中性文本查询的情况下实现公平的图像检索。该方法在实际数据集中实现了最低的偏差。

    

    本文解决了如何在中性文本查询的情况下（没有明确的性别或种族内涵）生成公平和无偏见的图像检索结果，同时保持底层视觉语言（VL）模型的效用（性能）的挑战。先前的方法旨在将图像和文本查询的学习表示与性别和种族特征分离。然而，我们发现这些方法不能减轻测试集中的偏差，从而实现所需的平等表示结果。出于这个动机，我们提出了一个简单的技术，后置偏差缓解（PBM），来后处理预训练视觉语言模型的输出。我们在实际图像搜索数据集Occupation 1和2，以及两个大规模的图像文本数据集MS-COCO和Flickr30k上评估了我们的算法。与各种现有的偏差缓解方法相比，我们的方法在基于文本的图像检索结果中实现了最低的偏差。

    We address the challenge of generating fair and unbiased image retrieval results given neutral textual queries (with no explicit gender or race connotations), while maintaining the utility (performance) of the underlying vision-language (VL) model. Previous methods aim to disentangle learned representations of images and text queries from gender and racial characteristics. However, we show these are inadequate at alleviating bias for the desired equal representation result, as there usually exists test-time bias in the target retrieval set. So motivated, we introduce a straightforward technique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from the pre-trained vision-language model. We evaluate our algorithm on real-world image search datasets, Occupation 1 and 2, as well as two large-scale image-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest bias, compared with various existing bias-mitigation methods, in text-based image retrieval result whi
    
[^104]: 一张图值得一比特的差异性：当图的对比学习遇到脉冲神经网络时。

    A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])

    [http://arxiv.org/abs/2305.19306](http://arxiv.org/abs/2305.19306)

    本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。

    

    虽然对比自监督学习已经成为图神经网络的事实上的学习范式，但对高任务准确性的追求需要大的隐藏维度来学习信息丰富、有区别性的全精度表示，这引发了对计算、存储和能源消耗负担（在现实世界应用中大多被忽略）的担忧。本文探索了一种有前途的方向，即用脉冲神经网络（SNNs）进行图的对比学习（GCL），利用稀疏和二元特性来学习更具生物可行性和紧凑性的表示。我们提出了SpikeGCL，一种学习图的二值化1比特表示的新型GCL框架，平衡了效率和性能之间的权衡。我们提供了理论保证，证明SpikeGCL在表达能力上与其全精度对应物具有可比性。实验结果表明，通过将表示存储压缩近32倍，SpikeGCL在保持高准确性的同时可以实现高效的学习。

    While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio
    
[^105]: 使用机器学习方法进行音频分类

    Audio classification using ML methods. (arXiv:2305.19304v1 [cs.SD])

    [http://arxiv.org/abs/2305.19304](http://arxiv.org/abs/2305.19304)

    本文介绍了如何利用机器学习方法对音乐类型进行分类，并展示了使用不同算法的分类结果。分类器将音乐类型分为古典音乐和金属乐。

    

    机器学习系统在不同领域中取得了出色的性能。本文将机器学习方法应用于音乐类型分类任务。该代码展示了如何从音频文件中提取特征，并使用监督学习将它们分为两个类型，即古典音乐和金属乐。所使用的算法包括逻辑回归、使用不同核（线性、sigmoid、rbf和poly）的支持向量机、K近邻分类器、随机森林分类器、决策树分类器和高斯贝叶斯分类器。

    Machine Learning systems have achieved outstanding performance in different domains. In this paper machine learning methods have been applied to classification task to classify music genre. The code shows how to extract features from audio files and classify them using supervised learning into 2 genres namely classical and metal. Algorithms used are LogisticRegression, SVC using different kernals (linear, sigmoid, rbf and poly), KNeighborsClassifier , RandomForestClassifier, DecisionTreeClassifier and GaussianNB.
    
[^106]: MAGNet：基于形状的分子生成技术

    MAGNet: Motif-Agnostic Generation of Molecules from Shapes. (arXiv:2305.19303v1 [physics.chem-ph])

    [http://arxiv.org/abs/2305.19303](http://arxiv.org/abs/2305.19303)

    MAGNet是一种基于图的分子生成技术，能够解决现有方法无法代表超出已知motif集之外的分子结构的问题，并在标准基准测试中表现出竞争力。

    

    机器学习在分子领域中的应用极有潜力，可为药物发现提供便利。现有的分子生成模型主要将分子分解为常见的子结构（motifs），之后生成新的化合物。尽管motif的表示法极大地帮助学习分子分布，但是这种方法无法代表超出已知motif集之外的分子结构。我们提出MAGNet，一种基于图的模型，可在分配原子和键类型之前，生成抽象的形状，以减轻这个问题并增加对数据集的灵活性。为此，我们提出了一种新的分子数据分布的因子分解方法，考虑了分子的全局上下文，并促进了原子和键的适当分配。虽然将抽象形状引入分布学习增加了复杂性，但我们证明MAGNet在标准基准测试中具有竞争力的表现。

    Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from in silico predictions. Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods struggle to represent substructures beyond their known motif set. To alleviate this issue and increase flexibility across datasets, we propose MAGNet, a graph-based model that generates abstract shapes before allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that accounts for the molecules' global context and facilitates learning adequate assignments of atoms and bonds onto shapes. While the abstraction to shapes introduces greater complexity for distribution learning, we show the competitive performance of MAGNet on standard benchmarks.
    
[^107]: 关于学习视频压缩中感知损失函数的选择

    On the Choice of Perception Loss Function for Learned Video Compression. (arXiv:2305.19301v1 [eess.IV])

    [http://arxiv.org/abs/2305.19301](http://arxiv.org/abs/2305.19301)

    本文研究了在学习视频压缩时，感知损失函数的选择对于重建效果的影响，发现选择 PLF-JD 可以更好地保留跨帧时序相关性，但同时会带来更大的失真惩罚和更难以纠正早期输出帧中的错误。

    

    本文针对低延迟、顺序视频压缩下受到均方误差（MSE）失真损失和感知损失（以实现真实感为目标）的输出设计了研究，并考虑了两种不同的感知损失函数（PLFs），分别是PLF-JD和PLF-FMD。通过信息论分析和基于深度学习的实验，我们演示了 PLF 的选择可能对重建效果产生显著影响，特别是在低比特率的情况下。具体而言，在保留跨帧时序相关性方面，基于PLF-JD 的重建效果会更好，但与 PLF-FMD 相比，也会带来更大的失真惩罚，并使其更难以恢复早期输出帧中出现的错误。

    We study causal, low-latency, sequential video compression when the output is subjected to both a mean squared-error (MSE) distortion loss as well as a perception loss to target realism. Motivated by prior approaches, we consider two different perception loss functions (PLFs). The first, PLF-JD, considers the joint distribution (JD) of all the video frames up to the current one, while the second metric, PLF-FMD, considers the framewise marginal distributions (FMD) between the source and reconstruction. Using information theoretic analysis and deep-learning based experiments, we demonstrate that the choice of PLF can have a significant effect on the reconstruction, especially at low-bit rates. In particular, while the reconstruction based on PLF-JD can better preserve the temporal correlation across frames, it also imposes a significant penalty in distortion compared to PLF-FMD and further makes it more difficult to recover from errors made in the earlier output frames. Although the cho
    
[^108]: MLOps：企业级机器学习迈进一步

    MLOps: A Step Forward to Enterprise Machine Learning. (arXiv:2305.19298v1 [cs.SE])

    [http://arxiv.org/abs/2305.19298](http://arxiv.org/abs/2305.19298)

    本文讨论了机器学习运维（MLOps）在企业级机器学习中的重要性和适用性，并详细解释了MLOps工作流程、自动化流程的不同成熟度水平以及各种底层技术。最后，用一个物体检测服务的企业级MLOps项目的详细示例来解释技术在现实场景中的工作流程。

    

    机器学习运维（MLOps）正成为希望利用人工智能和机器学习模型获益的企业非常关键的一部分。本研究详细回顾了MLOps及其好处、困难、进化以及重要的底层技术，如MLOps框架、Docker、GitHub操作和Kubernetes。详细解释了MLOps工作流程，包括模型设计、部署和操作，以及为模型和数据探索和部署所必需的各种工具。此外，该文章还强调了使用各种自动化流程的不同成熟度水平来端到端生产ML项目的重要性。最低为无自动化，最高为具有完整的CI/CD和CT功能。此外，使用物体检测服务的企业级MLOps项目的详细示例，用于解释技术在现实场景中的工作流程。为此，使用TensorFlow Object Detection API训练了预训练模型，并使用了Flask Web Framework在Web应用程序中托管该模型。

    Machine Learning Operations (MLOps) is becoming a highly crucial part of businesses looking to capitalize on the benefits of AI and ML models. This research presents a detailed review of MLOps, its benefits, difficulties, evolutions, and important underlying technologies such as MLOps frameworks, Docker, GitHub actions, and Kubernetes. The MLOps workflow, which includes model design, deployment, and operations, is explained in detail along with the various tools necessary for both model and data exploration and deployment. This article also puts light on the end-to-end production of ML projects using various maturity levels of automated pipelines, with the least at no automation at all and the highest with complete CI/CD and CT capabilities. Furthermore, a detailed example of an enterprise-level MLOps project for an object detection service is used to explain the workflow of the technology in a real-world scenario. For this purpose, a web application hosting a pre-trained model from Te
    
[^109]: 个别点表示相似性

    Pointwise Representational Similarity. (arXiv:2305.19294v1 [cs.LG])

    [http://arxiv.org/abs/2305.19294](http://arxiv.org/abs/2305.19294)

    本文介绍了 PNKA，一种可以量化单个输入在两个表示空间中的相似度的方法，填补了全局相似性度量不能局部调查表示的空白。

    

    随着对深度神经网络的依赖性越来越大，发展更好地理解它们所学表示方式的方法变得越来越重要。表征相似性度量已经成为了一种常用工具，用于检查学习到的表示。然而，现有的度量只能在全局层面上对相似性进行汇总估计，即在N个输入示例的一组表示中进行。因此，这些度量不适合于在局部层面上调查表示，即单个输入示例的表示。例如，我们需要局部相似性度量，以了解哪些单个输入表示受到了模型训练干预的影响（例如，更公平和无偏）或更容易被错误分类。在本文中，我们填补了这一空白并提出了PNKA（Pointwise Normalized Kernel Alignment），它对比度量了在两个表示空间中一个个别输入的表示相似程度。

    With the increasing reliance on deep neural networks, it is important to develop ways to better understand their learned representations. Representation similarity measures have emerged as a popular tool for examining learned representations However, existing measures only provide aggregate estimates of similarity at a global level, i.e. over a set of representations for N input examples. As such, these measures are not well-suited for investigating representations at a local level, i.e. representations of a single input example. Local similarity measures are needed, for instance, to understand which individual input representations are affected by training interventions to models (e.g. to be more fair and unbiased) or are at greater risk of being misclassified. In this work, we fill in this gap and propose Pointwise Normalized Kernel Alignment (PNKA), a measure that quantifies how similarly an individual input is represented in two representation spaces. Intuitively, PNKA compares the
    
[^110]: 重新审视随机森林：比较图卷积神经网络变体在交通预测中的性能

    Revisiting Random Forests in a Comparative Evaluation of Graph Convolutional Neural Network Variants for Traffic Prediction. (arXiv:2305.19292v1 [cs.LG])

    [http://arxiv.org/abs/2305.19292](http://arxiv.org/abs/2305.19292)

    本论文比较了图卷积神经网络和传统方法随机森林在交通预测中的性能，证明将矩阵分解、注意力和位置特定的模型权重单独或集体加入GCNN可以提高其整体性能，且在准确性和鲁棒性方面GCNN表现更出色。

    

    交通预测是智能交通系统中至关重要的时空预测任务。目前，图卷积神经网络（GCNN）已成为交通预测领域主流模型，因其擅长提取空间相关性。本文对成功的GCNN预测模型进行组成部分的分类并分析矩阵分解、注意力机制和权重共享对其性能的影响。此外，我们将这些变体与传统回归方法——随机森林进行比较。我们使用多伦多两个地区的模拟数据以及选定的加州高速公路的真实世界传感器数据来评估这些方法。我们发现将矩阵分解、注意力和位置特定的模型权重单独或集体加入GCNN中可以实现更好的整体性能。此外，尽管随机森林回归仍具有一定的竞争力，但在准确性和鲁棒性方面，GCNN表现更出色。

    Traffic prediction is a spatiotemporal predictive task that plays an essential role in intelligent transportation systems. Today, graph convolutional neural networks (GCNNs) have become the prevailing models in the traffic prediction literature since they excel at extracting spatial correlations. In this work, we classify the components of successful GCNN prediction models and analyze the effects of matrix factorization, attention mechanism, and weight sharing on their performance. Furthermore, we compare these variations against random forests, a traditional regression method that predates GCNNs by over 15 years. We evaluated these methods using simulated data of two regions in Toronto as well as real-world sensor data from selected California highways. We found that incorporating matrix factorization, attention, and location-specific model weights either individually or collectively into GCNNs can result in a better overall performance. Moreover, although random forest regression is 
    
[^111]: 使用深度强化学习的周界控制: 一种无模型方法实现同质流量优化

    Perimeter Control Using Deep Reinforcement Learning: A Model-free Approach towards Homogeneous Flow Rate Optimization. (arXiv:2305.19291v1 [cs.LG])

    [http://arxiv.org/abs/2305.19291](http://arxiv.org/abs/2305.19291)

    该论文提出了一种基于深度强化学习的模型无周界控制框架，通过交通信号控制代理与模拟车辆进行交互，实现同质流量优化，比传统的基于模型的方法更有效。

    

    周界控制通过控制不同区域之间的交通转移流量，以保证其交通密度低于临界值，从而保持受保护区域内交通高效。现有方法可以分为基于网络传输模型（NTMs）和宏观基本图（MFDs）的模型基方法和无模型方法。虽然基于模型的方法具有更高的数据效率和性能保证，但它们天生容易受到模型偏差和不准确性的影响。此外，现有研究中没有一项研究在微观模拟中使用强化学习进行同质流量优化，微观模拟考虑了空间特征、车辆级别信息和计量实现，这些在宏观模拟中经常被忽略。为了应对这些挑战，我们提出了一种基于深度强化学习（DRL）的无模型周界控制框架。我们的框架使用交通信号控制代理与模拟车辆进行交互，学习如何优化信号时序以控制受保护区域的流入和流出。我们展示了基于DRL的方法在合成和真实交通情况下均能优于传统的基于模型的方法。

    Perimeter control maintains high traffic efficiency within protected regions by controlling transfer flows among regions to ensure that their traffic densities are below critical values. Existing approaches can be categorized as either model-based or model-free, depending on whether they rely on network transmission models (NTMs) and macroscopic fundamental diagrams (MFDs). Although model-based approaches are more data efficient and have performance guarantees, they are inherently prone to model bias and inaccuracy. For example, NTMs often become imprecise for a large number of protected regions, and MFDs can exhibit scatter and hysteresis that are not captured in existing model-based works. Moreover, no existing studies have employed reinforcement learning for homogeneous flow rate optimization in microscopic simulation, where spatial characteristics, vehicle-level information, and metering realizations -- often overlooked in macroscopic simulations -- are taken into account. To circu
    
[^112]: 全局层：非独立同分布的表格联邦学习

    Global Layers: Non-IID Tabular Federated Learning. (arXiv:2305.19290v1 [cs.LG])

    [http://arxiv.org/abs/2305.19290](http://arxiv.org/abs/2305.19290)

    本文提出了一个新颖的偏模型个性化方法Global Layers (GL)，该方法是目前唯一一种能够支持客户端专有特征和类别的FL方法，在两个新的基准实验中，GL的性能优于联邦平均和仅本地训练的性能，甚至有些客户端的性能比他们的集中式基线还要好。

    

    客户端之间的数据异质性仍然是联邦学习（FL）中一个重要的挑战，尤其是在表格数据的情况下。本研究提出Global Layers（GL），这是一种新颖的偏模型个性化方法，能够在客户端之间具有联合分布 $P(X,Y)$ 转变和混合输入/输出空间 $X \times Y$ 的情况下实现鲁棒性。据我们所知，GL是第一种能够支持客户端专有特征和类别的方法。我们从现有的真实数据集中自然地对表格FL进行了两个新的基准实验：i）将UCI Covertype分为4个具有“野外区域”特征的客户端，以及ii）将UCI Heart Disease、SAHeart、UCI Heart Failure分别作为客户端。在全员参与设置的实验中，实验结果显示GL的性能优于联邦平均（FedAvg）和仅本地训练的性能，甚至有些客户端的性能比他们的集中式基线还要好。

    Data heterogeneity between clients remains a key challenge in Federated Learning (FL), particularly in the case of tabular data. This work presents Global Layers (GL), a novel partial model personalization method robust in the presence of joint distribution $P(X,Y)$ shift and mixed input/output spaces $X \times Y$ across clients. To the best of our knowledge, GL is the first method capable of supporting both client-exclusive features and classes. We introduce two new benchmark experiments for tabular FL naturally partitioned from existing real world datasets: i) UCI Covertype split into 4 clients by "wilderness area" feature, and ii) UCI Heart Disease, SAHeart, UCI Heart Failure, each as clients. Empirical results in these experiments in the full-participant setting show that GL achieves better outcomes than Federated Averaging (FedAvg) and local-only training, with some clients even performing better than their centralized baseline.
    
[^113]: 大型语言模型改善多模态数据诊断阿尔茨海默病

    Large language models improve Alzheimer's disease diagnosis using multi-modality data. (arXiv:2305.19280v1 [cs.LG])

    [http://arxiv.org/abs/2305.19280](http://arxiv.org/abs/2305.19280)

    本研究使用大型语言模型提高对非影像数据的应用能力，并在ADNI数据集上实现了SOTA结果。

    

    在诊断像阿尔茨海默病（AD）这样的艰难病症时，影像是一个重要的参考。非影像患者数据（例如患者信息、遗传数据、药物信息、认知和记忆测试）在诊断中也起着非常重要的作用。然而，由于人工智能模型挖掘这些信息的能力受限，大部分现有模型只能使用多模态影像数据，而不能充分利用非影像数据。我们使用目前非常流行的预训练大型语言模型（LLM），以增强模型使用非影像数据的能力，并在ADNI数据集上实现了SOTA的结果。

    In diagnosing challenging conditions such as Alzheimer's disease (AD), imaging is an important reference. Non-imaging patient data such as patient information, genetic data, medication information, cognitive and memory tests also play a very important role in diagnosis. Effect. However, limited by the ability of artificial intelligence models to mine such information, most of the existing models only use multi-modal image data, and cannot make full use of non-image data. We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.
    
[^114]: 计算力学中的数据驱动游戏

    Data-Driven Games in Computational Mechanics. (arXiv:2305.19279v1 [cs.CE])

    [http://arxiv.org/abs/2305.19279](http://arxiv.org/abs/2305.19279)

    本文提出了一种新型的非合作数据驱动游戏，其可以从数据中确定有效的材料定律，简化实际实施，并且与监督式机器学习方法不同，没有假设和参数。

    

    我们采用博弈论来构建数据驱动的实体力学方法，其中应力和应变玩家追求不同的目标。应力玩家的目标是将其与材料数据集的差异最小化，而应变玩家的目标是确保机械状态的可允许性，即相容性和平衡性。与过去提出的合作数据驱动游戏不同，我们展示了这些新型的非合作数据驱动游戏可以从数据中确定有效的材料定律，并简化为传统位移边界值问题，这有助于实际实施。提出的非合作数据驱动游戏与监督式机器学习方法不同，它没有假设和参数，特别是有效的材料定律是直接从数据中学习的，而无需回归到神经网络等参数化函数类。

    We resort to game theory in order to formulate Data-Driven methods for solid mechanics in which stress and strain players pursue different objectives. The objective of the stress player is to minimize the discrepancy to a material data set, whereas the objective of the strain player is to ensure the admissibility of the mechanical state, in the sense of compatibility and equilibrium. We show that, unlike the cooperative Data-Driven games proposed in the past, the new non-cooperative Data-Driven games identify an effective material law from the data and reduce to conventional displacement boundary-value problems, which facilitates their practical implementation. However, unlike supervised machine learning methods, the proposed non-cooperative Data-Driven games are unsupervised, ansatz-free and parameter-free. In particular, the effective material law is learned from the data directly, without recourse to regression to a parameterized class of functions such as neural networks. We presen
    
[^115]: 利用新兴协方差进行概率计算：走向高效的不确定性量化

    Probabilistic Computation with Emerging Covariance: Towards Efficient Uncertainty Quantification. (arXiv:2305.19265v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19265](http://arxiv.org/abs/2305.19265)

    本文开发了一个高效、可解释的概率计算框架，通过监督平均值优化任务目标，从非线性耦合中自发出现的无监督协方差忠实地捕捉了与模型预测的不确定性相关的信息。

    

    建立鲁棒性、可解释性和安全性强的人工智能系统需要通过概率视角量化和表示不确定性，因为这可以模仿人类的认知能力。然而，概率计算由于其固有的复杂性而面临重大挑战。本文通过截断概率表示的前两个矩，即平均值和协方差，开发了一个高效、可解释的概率计算框架。我们通过训练随机网络的确定性替代品来实例化该框架，该网络通过简单激活的组合学习复杂的概率表示，封装了平均值和协方差的非线性耦合。我们表明，当平均值受到监督以优化任务目标时，从其与协方差的非线性耦合中自发出现的无监督协方差忠实地捕捉了与模型预测的不确定性相关的信息。

    Building robust, interpretable, and secure artificial intelligence system requires some degree of quantifying and representing uncertainty via a probabilistic perspective, as it allows to mimic human cognitive abilities. However, probabilistic computation presents significant challenges due to its inherent complexity. In this paper, we develop an efficient and interpretable probabilistic computation framework by truncating the probabilistic representation up to its first two moments, i.e., mean and covariance. We instantiate the framework by training a deterministic surrogate of a stochastic network that learns the complex probabilistic representation via combinations of simple activations, encapsulating the non-linearities coupling of the mean and covariance. We show that when the mean is supervised for optimizing the task objective, the unsupervised covariance spontaneously emerging from the non-linear coupling with the mean faithfully captures the uncertainty associated with model p
    
[^116]: 基于空间转换网络的舌部超声无声语音接口的适应性研究

    Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks. (arXiv:2305.19130v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2305.19130](http://arxiv.org/abs/2305.19130)

    采用空间转换网络模块，可使舌部超声图像为基础的无声语音接口模型快速适应到不同的用户和会话，且能显著降低均方误差。

    

    最新的深度学习算法能够在一定条件下，从发音运动数据中合成可懂的语音，这些成果来自于无声语音接口（SSI）。然而，所得到的模型往往特定于某一说话人，使得在不同用户之间进行快速切换变得麻烦。即使是同一个讲者，在不同时间进行模型应用效果也较差。为了帮助舌部超声图像为基础的 SSI 模型快速适应到不同的用户和会话，我们扩展了深度网络，并利用空间转换网络 (STN) 模块，能够对输入图像进行仿射变换。虽然 STN 只占网络的约 10%，但实验表明，仅适应 STN 模块就可以将均方误差平均减少 88%，而不重新训练整个网络。当将网络适应到同一讲者的不同记录会话时，改进效果更大（大约 92%）。

    Thanks to the latest deep learning algorithms, silent speech interfaces (SSI) are now able to synthesize intelligible speech from articulatory movement data under certain conditions. However, the resulting models are rather speaker-specific, making a quick switch between users troublesome. Even for the same speaker, these models perform poorly cross-session, i.e. after dismounting and re-mounting the recording equipment. To aid quick speaker and session adaptation of ultrasound tongue imaging-based SSI models, we extend our deep networks with a spatial transformer network (STN) module, capable of performing an affine transformation on the input images. Although the STN part takes up only about 10% of the network, our experiments show that adapting just the STN module might allow to reduce MSE by 88% on the average, compared to retraining the whole network. The improvement is even larger (around 92%) when adapting the network to different recording sessions from the same speaker.
    
[^117]: IDToolkit: 用于纳米光子学反向设计算法基准测试和开发的工具箱

    IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design Algorithms in Nanophotonics. (arXiv:2305.18978v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.18978](http://arxiv.org/abs/2305.18978)

    提出了一个纳米光子学器件反向设计基准测试，以帮助人们进行易于理解和可重复的科学设计。开发了一个开源工具箱IDToolkit，其中包含了模拟和优化器模块以及后处理结果的函数，可用于与基准方法比较算法。

    

    帮助人类进行科学设计是人工智能（AI）和机器学习（ML）最令人兴奋的领域之一，因为它们具有发现新药物、设计新材料和化合物等潜力。然而，科学设计通常需要熟悉领域知识的专业技能，这些对于AI研究人员来说并不熟悉。此外，科学研究需要专业的实验和评估技能。这些障碍阻碍了AI研究人员开发专门用于科学设计的方法。为迈向易于理解和可重复研究的科学设计，我们提出了一个用于纳米光子学器件反向设计的基准测试，可在计算上和准确地验证。具体而言，我们实现了三种不同的纳米光子学设计问题，分别是辐射冷却器，适用于热光伏选择性发射体以及结构色滤光器，它们在设计参数空间、复杂度和物理性质上都不同。此外，我们开发了IDToolkit，一个开源的Python工具箱，以为用户提供与基准方法比较其算法的简单接口。IDToolkit包含了反向设计所需的几个模块，例如模拟模块和优化器模块，以及后处理结果的函数。我们希望所提出的基准测试和工具包将加速纳米光子学科学设计新方法的发展。

    Aiding humans with scientific designs is one of the most exciting of artificial intelligence (AI) and machine learning (ML), due to their potential for the discovery of new drugs, design of new materials and chemical compounds, etc. However, scientific design typically requires complex domain knowledge that is not familiar to AI researchers. Further, scientific studies involve professional skills to perform experiments and evaluations. These obstacles prevent AI researchers from developing specialized methods for scientific designs. To take a step towards easy-to-understand and reproducible research of scientific design, we propose a benchmark for the inverse design of nanophotonic devices, which can be verified computationally and accurately. Specifically, we implemented three different nanophotonic design problems, namely a radiative cooler, a selective emitter for thermophotovoltaics, and structural color filters, all of which are different in design parameter spaces, complexity, an
    
[^118]: 基于卷积蒙日映射归一化的生物信号学习方法

    Convolutional Monge Mapping Normalization for learning on biosignals. (arXiv:2305.18831v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2305.18831](http://arxiv.org/abs/2305.18831)

    本研究提出了一种新的方法：基于卷积蒙日映射归一化 (CMMN)，用于信号众多但变异性较大的生物医学数据上，能自适应调整幅度、滤波器的功率谱密度，通过最优输运映射和 barycenters 实现个体测试时间适应新数据，且显著提升信号分类和检测性能。

    

    在许多信号与生物医学数据的机器学习应用中，尤其是在脑电图 (EEG) 中，一个主要的挑战是数据在受试者、会话和硬件设备上的变异性。本文提出了一种新方法，称为卷积蒙日映射归一化 (CMMN)，其核心是根据训练数据估计Wasserstein barycenter，通过滤波信号以适应其功率谱密度 (PSD)。CMMN 基于新的闭式解，提供了最优输运映射和 barycenters，并提供了个体测试时间适应新数据的功能，而无需重新训练预测模型。通过对睡眠 EEG 数据的数值实验表明，CMMN 在适应受试者、会话甚至在使用不同硬件收集的数据集之间，都能带来显著且一致的性能提升，且其性能提升与数值密集的域适应 (DA) 相当。

    In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization (CMMN), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. CMMN relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that CMMN leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) m
    
[^119]: HiFA: 高保真度的文本到3D图像合成及其先进的扩散引导策略

    HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.18766](http://arxiv.org/abs/2305.18766)

    该论文提出了一种高保真度的文本到3D图像合成方法，并引入了先进的扩散引导策略。通过对NeRF渲染图像进行辅助深度监督和规范化密度场来提高3D几何表示。实验证明该方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。

    

    通过优化3D模型，自动文本到3D合成在提升中已经取得了显著进展。现有方法通常依赖于预训练的文本到图像生成模型（如扩散模型），提供神经辐射场（NeRFs）的2D渲染得分并用于优化NeRFs。然而，由于其对3D几何的有限理解，这些方法经常遇到多个视角上的伪影和不一致现象。为了解决这些限制，我们提出了使用扩散先验重新制定优化损失的方法。此外，我们引入了一种新的训练方法，释放了扩散先验的潜力。为了提高3D几何表示，我们对NeRF渲染图像进行辅助深度监督，并规范化NeRF的密度场。大量实验证明了我们的方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。

    Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
    
[^120]: 任务等变图Few-shot学习

    Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18758](http://arxiv.org/abs/2305.18758)

    本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。

    

    虽然图神经网络（GNN）在节点分类任务中取得了成功，但其性能严重依赖于每类具有足够标记节点的可用性。在现实情况下，不是所有类都有许多标记节点，模型可能需要分类新类别，使得手动标记变得困难。为了解决这个问题，GNN需要能够用有限数量的标记节点对节点进行分类，称为Few-shot节点分类。先前的基于剧集元学习的方法在Few-shot节点分类中取得了成功，但我们的发现表明仅有多样的训练元任务才能实现最佳性能。为了应对基于元学习的Few-shot学习的挑战，我们提出了一种新的方法，即任务等变图Few-shot学习（TEG）框架。我们的TEG框架通过利用图神经网络的等变性质来使模型学习可转移的任务适应策略。我们在各种Few-shot分类基准上展示了我们提出的方法的有效性，实现了最先进的性能。

    Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
    
[^121]: 通用高斯核密度估计模型的降维算法研究

    Dimensionality Reduction for General KDE Mode Finding. (arXiv:2305.18755v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18755](http://arxiv.org/abs/2305.18755)

    本文提出了一种降维算法，可以适用于高斯混合模型和更广泛的核函数，结合梯度下降可制定出有效的实用启发式算法。

    

    在统计学和数据分析中，寻找高维概率分布的模是一项重要的算法问题。本文针对高斯混合模型的模估计结果进行了通用扩展，提出了一种降维算法，可以适用于更广泛的核函数，包括流行的逻辑、Sigmoid和广义高斯核。通过结合梯度下降，该算法可制定出有效的实用启发式算法。

    Finding the mode of a high dimensional probability distribution $D$ is a fundamental algorithmic problem in statistics and data analysis. There has been particular interest in efficient methods for solving the problem when $D$ is represented as a mixture model or kernel density estimate, although few algorithmic results with worst-case approximation and runtime guarantees are known.  In this work, we significantly generalize a result of (LeeLiMusco:2021) on mode approximation for Gaussian mixture models. We develop randomized dimensionality reduction methods for mixtures involving a broader class of kernels, including the popular logistic, sigmoid, and generalized Gaussian kernels. As in Lee et al.'s work, our dimensionality reduction results yield quasi-polynomial algorithms for mode finding with multiplicative accuracy $(1-\epsilon)$ for any $\epsilon > 0$. Moreover, when combined with gradient descent, they yield efficient practical heuristics for the problem.  In addition to our po
    
[^122]: NUNO: 用于学习非均匀数据 Parametric PDEs 的通用框架

    NUNO: A General Framework for Learning Parametric PDEs with Non-Uniform Data. (arXiv:2305.18694v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18694](http://arxiv.org/abs/2305.18694)

    NUNO 是一个用于处理非均匀数据的高效算子学习框架，在三维 PDE 问题中取得了显著的效果提升。

    

    神经算子已经成为求解偏微分方程中函数空间映射的有力工具。但是，当面临现实世界的物理数据时，这些数据往往分布非常不均匀，使用基于网格的技术（如FFT）很具有挑战性。为了解决这个问题，我们引入了Non-Uniform Neural Operator (NUNO)，这是一个专门用于处理非均匀数据的高效算子学习框架。通过利用基于K-D树的域分解，我们将非均匀数据转换成均匀网格，同时有效地控制插值误差，从而可以与从非均匀数据学习的速度和准确性并行。我们在二维弹性、(2+1)D河道流和三维多物理 heatsink 上进行了大量的实验，这些实验涉及复杂几何结构的三维偏微分方程问题。我们的框架将误差率降低了最多60％，并将训练速度提高了2倍至30倍。代码现在在https://github.com 上可用。

    The neural operator has emerged as a powerful tool in learning mappings between function spaces in PDEs. However, when faced with real-world physical data, which are often highly non-uniformly distributed, it is challenging to use mesh-based techniques such as the FFT. To address this, we introduce the Non-Uniform Neural Operator (NUNO), a comprehensive framework designed for efficient operator learning with non-uniform data. Leveraging a K-D tree-based domain decomposition, we transform non-uniform data into uniform grids while effectively controlling interpolation error, thereby paralleling the speed and accuracy of learning from non-uniform data. We conduct extensive experiments on 2D elasticity, (2+1)D channel flow, and a 3D multi-physics heatsink, which, to our knowledge, marks a novel exploration into 3D PDE problems with complex geometries. Our framework has reduced error rates by up to 60% and enhanced training speeds by 2x to 30x. The code is now available at https://github.co
    
[^123]: 基于协作学习的分析端到端深度学习算法

    An Analytic End-to-End Deep Learning Algorithm based on Collaborative Learning. (arXiv:2305.18594v1 [cs.LG])

    [http://arxiv.org/abs/2305.18594](http://arxiv.org/abs/2305.18594)

    本文提出了一种平滑激活函数的全连接神经网络端到端深度学习收敛分析方法，避免了潜在的抖动问题，并且可以使用协作学习进一步结合多个网络的优势。

    

    在大多数控制应用中，系统的理论分析对确保稳定性或收敛至关重要，以确保安全可靠的运行并获得更好的系统理解以进行进一步的发展。然而，大多数当前的深度学习方法是黑盒方法，更多地关注经验性研究。本文提出了一种基于平滑激活函数的全连接神经网络（FNN）端到端深度学习收敛分析方法，避免了潜在的抖动问题，同时也不易导致梯度消失问题。该算法可以同时训练多个两层全连接网络，并使用协作学习进一步结合它们的优势。

    In most control applications, theoretical analysis of the systems is crucial in ensuring stability or convergence, so as to ensure safe and reliable operations and also to gain a better understanding of the systems for further developments. However, most current deep learning methods are black-box approaches that are more focused on empirical studies. Recently, some results have been obtained for convergence analysis of end-to end deep learning based on non-smooth ReLU activation functions, which may result in chattering for control tasks. This paper presents a convergence analysis for end-to-end deep learning of fully connected neural networks (FNN) with smooth activation functions. The proposed method therefore avoids any potential chattering problem, and it also does not easily lead to gradient vanishing problems. The proposed End-to-End algorithm trains multiple two-layer fully connected networks concurrently and collaborative learning can be used to further combine their strengths
    
[^124]: Trompt：面向表格数据的更好的深度神经网络

    Trompt: Towards a Better Deep Neural Network for Tabular Data. (arXiv:2305.18446v1 [cs.LG])

    [http://arxiv.org/abs/2305.18446](http://arxiv.org/abs/2305.18446)

    Trompt是一种新颖的深度神经网络结构，其中分离了表格数据学习策略，它通过提示学习的方式来调整大型预先训练的模型，以便更好地能够处理表格数据，并取得了很好的效果。

    

    表格数据可谓是各种实际领域中最常用的数据结构之一，包括金融、医疗和电子商务。内在的异质性允许表格数据存储丰富的信息。然而，根据最近发布的表格基准，我们可以看到深度神经网络在表格数据集上仍然落后于树状模型。本文提出了Trompt，它是一个新颖的架构，灵感来自于语言模型的提示学习。提示学习的本质是通过一组模型外的提示来调整一个大型预训练模型，而不是直接修改模型。基于这个思想，Trompt将表格数据的学习策略分为两部分。第一部分类似于预训练模型，注重学习表格的内在信息。第二部分类似于提示，注重学习样本之间的差异。 Trompt在上述基准中进行了评估。

    Tabular data is arguably one of the most commonly used data structures in various practical domains, including finance, healthcare and e-commerce. The inherent heterogeneity allows tabular data to store rich information. However, based on a recently published tabular benchmark, we can see deep neural networks still fall behind tree-based models on tabular datasets. In this paper, we propose Trompt--which stands for Tabular Prompt--a novel architecture inspired by prompt learning of language models. The essence of prompt learning is to adjust a large pre-trained model through a set of prompts outside the model without directly modifying the model. Based on this idea, Trompt separates the learning strategy of tabular data into two parts. The first part, analogous to pre-trained models, focus on learning the intrinsic information of a table. The second part, analogous to prompts, focus on learning the variations among samples. Trompt is evaluated with the benchmark mentioned above. The ex
    
[^125]: Dink-Net: 大规模图形神经聚类方法

    Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])

    [http://arxiv.org/abs/2305.18405](http://arxiv.org/abs/2305.18405)

    Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。

    

    近年来，深度图聚类通过深度神经网络将图形的节点进行分组取得了很大的进展，但现有方法无法处理百万节点的大图。为了解决这个问题，我们提出了一种可扩展的Dink-Net深度图聚类方法，利用了膨胀和收缩的思想。首先，通过区分带增强的跟不带增强的节点，自我监督方式学习表示形式。同时，将聚类中心初始化为可学习的神经网络参数。随后，通过对抗性方式最小化提出的集群膨胀损失和集群收缩损失，优化聚类分布。通过这些设置，我们将表示学习和聚类优化两个步骤统一为一个端到端框架，引导网络学习聚类友好的特征。此外，Dink-Net能很好地扩展到大规模的图形上，因为设计的膨胀收缩操作可以有效地减少计算和内存消耗。实验结果表明，Dink-Net在处理百万节点图形的各种基准数据集上优于现有的最先进方法，证明了该方法在大图聚类中的可扩展性和有效性。

    Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
    
[^126]: 将预测编码理解为自适应信任区域方法

    Understanding Predictive Coding as an Adaptive Trust-Region Method. (arXiv:2305.18188v1 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2305.18188](http://arxiv.org/abs/2305.18188)

    研究将预测编码（PC）作为自适应信任区域（TR）算法的理论模型，并发现它可以比反向传播（BP）更快地逃脱鞍点。

    

    预测编码（PC）是一种类脑本地学习算法，最近被提出在生物相关场景中提供比反向传播（BP）更好的优势。虽然理论研究主要集中在展示PC如何在各种极限中逼近BP，但“自然”的PC的潜在优势尚不完全理解。在本文中，我们将PC开发为使用二阶信息的自适应信任区域（TR）算法的理论模型。

    Predictive coding (PC) is a brain-inspired local learning algorithm that has recently been suggested to provide advantages over backpropagation (BP) in biologically relevant scenarios. While theoretical work has mainly focused on showing how PC can approximate BP in various limits, the putative benefits of "natural" PC are less understood. Here we develop a theory of PC as an adaptive trust-region (TR) algorithm that uses second-order information. We show that the learning dynamics of PC can be interpreted as interpolating between BP's loss gradient direction and a TR direction found by the PC inference dynamics. Our theory suggests that PC should escape saddle points faster than BP, a prediction which we prove in a shallow linear model and support with experiments on deeper networks. This work lays a foundation for understanding PC in deep and wide networks.
    
[^127]: 使用场景图记忆建模动态环境

    Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17537](http://arxiv.org/abs/2305.17537)

    本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。

    

    在大型环境中，如居室等，寻找物品的具有行动能力的AI代理需要基于部分信息预测物品位置来做出有效决策。我们将其形式化为一种新类型的链路预测问题：部分可观察动态图上的链路预测。我们的图表达了一个场景，其中房间和物品是节点，在边缘中编码它们之间的关系；在每个时间步骤上，代理人仅知道更改图的部分。这种部分可观测性对于现有的链路预测方法构成了挑战，我们进行了解决。我们提出了一种新颖的状态表示 - 场景图记忆（SGM） - 其中包括代理人的累积观察集合，以及一种名为节点边缘预测器（NEP）的神经网络架构，该架构从SGM中提取信息以进行高效搜索。我们在动态房屋模拟器中评估了我们的方法，这是一个新的基准，它按照语义模式创建不同的动态图形。

    Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
    
[^128]: 递归的诅咒：使用生成数据进行训练会让模型忘记

    The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17493](http://arxiv.org/abs/2305.17493)

    使用生成数据进行训练会导致模型不可逆的缺陷并且使得原始内容分布的尾部消失，这种效应称为模型折叠。我们证明了这种现象在所有学习生成模型中都存在，必须认真对待。

    

    稳定扩散技术革命性地改变了从描述性文本中生成图像的方法。GPT-2、GPT-3(.5)和GPT-4在各种语言任务中表现惊人。ChatGPT将这些语言模型引入了大众视野。大语言模型(LLMs)已经不可避免并将彻底改变在线文本和图像的整个生态系统。本文考虑了未来可能发生的事情。当LLMs占据了在线语言的大部分时，GPT-{n}会发生什么？我们发现，在训练中使用模型生成的内容会导致所得模型中不可逆缺陷，原始内容分布的尾部消失。我们将这种效应称为模型折叠，并显示它可以发生在变分自编码器、高斯混合模型和LLMs中。我们建立了现象背后的理论直觉，并展示了这种现象在所有学习生成模型中的普遍性。我们证明，如果我们要在实践中使用生成数据进行训练，就必须认真对待这一问题。

    Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we ar
    
[^129]: 一种从学术论文中提炼文本分类和对象识别的框架

    A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.17401](http://arxiv.org/abs/2305.17401)

    本文提出了一种结合基于规则的方法和机器学习的框架，旨在解决从学术论文中提炼文本分类和对象识别的问题。

    

    随着互联网的广泛使用，高效地从大量学术论文中提取特定信息变得越来越重要。数据挖掘技术通常用于解决这个问题。然而，挖掘学术论文的数据具有挑战性，因为它需要自动从复杂的非结构化布局文档中提取特定模式。当前的学术论文数据挖掘方法使用基于规则的（RB）或机器学习（ML）方法。然而，使用基于规则的方法需要编写复杂排版论文的高昂成本。另一方面，仅使用机器学习方法需要对文章中复杂内容类型进行注释工作，这可能成本高昂。此外，仅使用机器学习可能会导致基于规则的方法容易识别的模式被错误提取的情况。为了解决这些问题，本文从分析指定著作中使用的标准布局和排版角度出发，提出了一种结合基于规则的方法和机器学习的框架。

    With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
    
[^130]: 论解码器Transformer语言模型的计算能力

    On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17026](http://arxiv.org/abs/2305.17026)

    本篇论文研究了解码器Transformer语言模型的计算普适性，表明即使只有单层和单注意力头，仍然具有图灵完备性，其中单词嵌入的稀疏性/可压缩性是必要条件。

    

    本文章对解码器Transformer模型的计算普适性进行了理论评估。我们扩展了Transformer模型的理论文献，并表明仅使用单层和单注意力头的解码器Transformer结构，在合理假设下具备图灵完备性。从理论分析中，我们证明了单词嵌入的稀疏性/可压缩性是图灵完备性成立的必要条件。

    This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
    
[^131]: HUB: 用持续提示调整引导学习优化器

    HUB: Guiding Learned Optimizers with Continuous Prompt Tuning. (arXiv:2305.16823v1 [cs.LG])

    [http://arxiv.org/abs/2305.16823](http://arxiv.org/abs/2305.16823)

    本文提出了一种名为HUB的混合更新策略，通过结合学习优化器和手工设计的优化器，提高了学习优化器泛化性能。

    

    学习优化器是元学习的关键组成部分，但其在处理未见过的任务和网络架构时有限。为了解决此问题，本文提出了一种基于混合更新策略的优化方法（HUB），该方法受到了大型语言和视觉模型中硬提示调整和结果选择技术的启发。通过将手工设计的优化器作为我们混合方法的第二个组件，我们能够在稳定训练的同时保留学习优化器的好处。

    Learned optimizers are a crucial component of meta-learning. Recent advancements in scalable learned optimizers have demonstrated their superior performance over hand-designed optimizers in various tasks. However, certain characteristics of these models, such as an unstable learning curve, limited ability to handle unseen tasks and network architectures, difficult-to-control behaviours, and poor performance in fine-tuning tasks impede their widespread adoption. To tackle the issue of generalization in scalable learned optimizers, we propose a hybrid-update-based (HUB) optimization strategy inspired by recent advancements in hard prompt tuning and result selection techniques used in large language and vision models. This approach can be easily applied to any task that involves hand-designed or learned optimizer. By incorporating hand-designed optimizers as the second component in our hybrid approach, we are able to retain the benefits of learned optimizers while stabilizing the training
    
[^132]: AdaPlanner:自适应规划与语言模型的反馈。 （arXiv：2305.16653v1 [cs.CL]）

    AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])

    [http://arxiv.org/abs/2305.16653](http://arxiv.org/abs/2305.16653)

    LLM代理可以通过Adaplanner自适应改进自己的计划以应对环境反馈，为此提出计划内外的改进策略以及代码风格的LLM提示结构和技能发现机制。

    

    最近的大型语言模型（LLM）展示了在序列决策任务中作为自主代理的潜力。然而，大多数现有方法要么贪婪地采取行动而没有计划，要么依赖于不可适应环境反馈的静态计划。因此，随着问题复杂性和计划水平的增加，LLM代理的顺序决策性能会退化。我们提出了一种闭环方法AdaPlanner，它允许LLM代理根据环境反馈自适应地改进其自动生成的计划。在AdaPlanner中，LLM代理通过计划内和计划外的改进策略自适应地改进其计划。为了减轻幻觉，我们开发了一种代码风格的LLM提示结构，促进了跨各种任务，环境和代理能力的计划生成。此外，我们提出了一种技能发现机制，利用成功的计划作为少量示例，使计划更具普适性。

    Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the
    
[^133]: 自监督学习的逆向工程

    Reverse Engineering Self-Supervised Learning. (arXiv:2305.15614v1 [cs.LG])

    [http://arxiv.org/abs/2305.15614](http://arxiv.org/abs/2305.15614)

    本文逆向工程了自监督学习（SSL）训练表示，发现SSL训练过程中的正则化项本质上促进了样本基于语义标签的聚类。SSL训练的表示与语义类别更加接近，对齐在训练过程中增加，而且在网络深度加深时增加。

    

    自监督学习（SSL）是机器学习中有力的工具，但理解学习表示及其基础机制仍然是一个挑战。本文对SSL训练表示进行了深入的实证分析，包括多种模型、架构和超参数。我们的研究揭示了SSL训练过程的一个有趣方面：它本质上促进了样本基于语义标签的聚类，这令人惊讶的是，这是由SSL目标的正则化项驱动的。这种聚类过程不仅增强了下游分类，而且压缩了数据信息。此外，我们发现SSL训练的表示与语义类别更加接近，而不是随机类别。值得注意的是，我们展示了学习表示与各种层次的语义类别对齐，并且这种对齐在训练过程中增加，而且在网络深度加深时增加。

    Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provid
    
[^134]: 基于机器学习的信息融合增强人工智能模型的时序感知不确定性封装器

    Timeseries-aware Uncertainty Wrappers for Uncertainty Quantification of Information-Fusion-Enhanced AI Models based on Machine Learning. (arXiv:2305.14872v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14872](http://arxiv.org/abs/2305.14872)

    本论文提出了一种基于机器学习的信息融合增强人工智能模型的时序感知不确定性封装器，可用于对时序数据进行可靠的不确定性估计，同时通过信息融合和时序感知的输入质量特征提高模型准确性和不确定性估计的质量。

    

    随着人工智能组件在物理系统中的使用越来越常见，需要可靠的系统架构。虽然数据驱动模型在感知任务上表现出色，但模型结果通常不足以应用于安全关键应用。本文提出了一种时序感知的不确定性封装器，用于对时序数据进行可靠的不确定性估计。不确定性封装器与模型的连续预测信息融合相结合。不确定性封装器的应用以交通标志识别为例进行演示。我们展示了通过信息融合可以提高模型的准确性，同时通过时序感知的输入质量特征还可以提高不确定性估计的质量。

    As the use of Artificial Intelligence (AI) components in cyber-physical systems is becoming more common, the need for reliable system architectures arises. While data-driven models excel at perception tasks, model outcomes are usually not dependable enough for safety-critical applications. In this work,we present a timeseries-aware uncertainty wrapper for dependable uncertainty estimates on timeseries data. The uncertainty wrapper is applied in combination with information fusion over successive model predictions in time. The application of the uncertainty wrapper is demonstrated with a traffic sign recognition use case. We show that it is possible to increase model accuracy through information fusion and additionally increase the quality of uncertainty estimates through timeseries-aware input quality features.
    
[^135]: 自监督神经表示是否能够在预训练人类语音后区分动物呼叫者？

    Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?. (arXiv:2305.14035v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14035](http://arxiv.org/abs/2305.14035)

    本文研究了利用人类语音自监督神经表示学习来分析生物声学信号的交叉可迁移性，在狨猴叫声识别和检测中取得了成功，未来该方法可有效应用于该领域的研究。

    

    自监督学习模型仅使用给定信号的内在结构，独立于声学领域，将信号转换为嵌入空间中的基本信息。这意味着这些表示的效用不仅局限于人类语音建模。本文基于此探讨了通过预训练人类语音自监督神经表示学习来分析生物声学信号的交叉可迁移性。我们使用11个预先训练的各种预训练任务的自监督模型对叫声进行辨别和检测研究，结果表明嵌入空间包含有意义的呼叫者信息，可成功区分不同狨猴叫的个体身份，而无需微调。这表明在生物声学领域，预先训练于人类语音的表示能够被有效地应用于该领域，并为该领域的未来研究提供有价值的见解。

    Self-supervised learning (SSL) models use only the intrinsic structure of a given signal, independent of its acoustic domain, to extract essential information from the input to an embedding space. This implies that the utility of such representations is not limited to modeling human speech alone. Building on this understanding, this paper explores the cross-transferability of SSL neural representations learned from human speech to analyze bio-acoustic signals. We conduct a caller discrimination analysis and a caller detection study on Marmoset vocalizations using eleven SSL models pre-trained with various pretext tasks. The results show that the embedding spaces carry meaningful caller information and can successfully distinguish the individual identities of Marmoset callers without fine-tuning. This demonstrates that representations pre-trained on human speech can be effectively applied to the bio-acoustics domain, providing valuable insights for future investigations in this field.
    
[^136]: 友好的邻居：语境化序列到序列链接预测

    Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction. (arXiv:2305.13059v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13059](http://arxiv.org/abs/2305.13059)

    研究提出了一种简单的序列到序列模型KGT5-context，通过加入查询实体的直接邻居信息实现知识图谱链接预测的高性能，并与其他方法相比取得了最先进的表现。

    

    我们提出了 KGT5-context，这是一个用于知识图谱（KG）中链接预测（LP）的简单序列到序列模型。我们的工作在最近的LP模型KGT5的基础上拓展，KGT5利用了KG的文本特征，模型规模小且可扩展，但为了达到好的预测性能，KGT5依赖于与之配合的知识图嵌入模型，该模型本身非常大且使用成本高昂。在这篇短文中，我们通过实验证明，加入语境信息，即关于查询实体的直接邻居信息，可以减轻对独立KGE模型的需求以获得良好的性能。由此产生的KGT5-context模型简单，显著缩小了模型大小，并在我们的实验研究中获得了最先进的性能。

    We propose KGT5-context, a simple sequence-to-sequence model for link prediction (LP) in knowledge graphs (KG). Our work expands on KGT5, a recent LP model that exploits textual features of the KG, has small model size, and is scalable. To reach good predictive performance, however, KGT5 relies on an ensemble with a knowledge graph embedding model, which itself is excessively large and costly to use. In this short paper, we show empirically that adding contextual information - i.e., information about the direct neighborhood of the query entity - alleviates the need for a separate KGE model to obtain good performance. The resulting KGT5-context model is simple, reduces model size significantly, and obtains state-of-the-art performance in our experimental study.
    
[^137]: 统一嵌入：面向 Web 规模 ML 系统的经过验证的特征表示

    Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems. (arXiv:2305.12102v1 [cs.LG])

    [http://arxiv.org/abs/2305.12102](http://arxiv.org/abs/2305.12102)

    本文介绍了一种名为“特征复用”的框架，它使用单一的表示空间 能够高效有效地学习高质量的特征嵌入，同时区分不同的分类特征。通过在多个公共数据集和新数据集“Web-Available Image Search (WAIS)”上的测试，我们展示了这种方法的优于现有技术的表现。

    

    高效、有效地学习高质量的特征嵌入对于 Web 规模的机器学习系统的性能至关重要。标准方法是将每个特征值表示为一个 d 维嵌入，引入数百亿个参数，而这些特征的基数非常高。这个瓶颈导致了备选嵌入算法的重大进展。本文介绍了一个简单但非常有效的框架，即“特征复用”，在许多不同的分类特征之间使用一个单一的表示空间。我们的理论和实证分析表明，复用的嵌入可以分解为每个组成特征的组件，使得模型可以区分特征。我们展示了复用的嵌入在几个公共数据集上优于现有技术。此外，我们引入了一个名为“Web-Available Image Search (WAIS)”的新数据集，以严格评估 Web 规模下的新嵌入算法。我们邀请社区通过提出可以准确、高效地将数百万张图像嵌入和分类到成千上万个类别的新模型来贡献 WAIS 挑战。

    Learning high-quality feature embeddings efficiently and effectively is critical for the performance of web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on the order of millions to billions of tokens. The standard approach is to represent each feature value as a d-dimensional embedding, introducing hundreds of billions of parameters for extremely high-cardinality features. This bottleneck has led to substantial progress in alternative embedding algorithms. Many of these methods, however, make the assumption that each feature uses an independent embedding table. This work introduces a simple yet highly effective framework, Feature Multiplexing, where one single representation space is used across many different categorical features. Our theoretical and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each constituent feature, allowing models to distinguish between features. We show that multip
    
[^138]: 基于多尺度特征金字塔网络和双重注意力机制的腹部MRI图像分割算法

    A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism. (arXiv:2305.10631v1 [eess.IV])

    [http://arxiv.org/abs/2305.10631](http://arxiv.org/abs/2305.10631)

    提出了一种基于多尺度特征金字塔网络和双重注意力机制的子腹部MRI图像分割算法，使用空洞卷积和多尺度特征金字塔编码以避免语义差距，设计双重注意力机制以保持空间信息并减少错位。

    

    本研究旨在解决U-Net在分割直肠癌治疗期间的子腹部MRI图像时，由于多次卷积和池化操作导致编码和解码之间存在语义差距和错位问题。提出了一种基于多尺度特征金字塔网络和双重注意力机制的MRI图像分割方法。我们的创新在于设计了两个模块：1）在编码中使用了空洞卷积和多尺度特征金字塔网络以避免语义差距。2）设计了双重注意力机制，以保持U-Net的空间信息并减少错位。对子腹部MRI图像数据集的实验表明，该方法比其他方法表现更好。总之，多尺度特征金字塔网络可以减少语义差距，双重注意力机制可以使编码和解码之间的特征对齐。

    This study aimed to solve the semantic gap and misalignment issue between encoding and decoding because of multiple convolutional and pooling operations in U-Net when segmenting subabdominal MRI images during rectal cancer treatment. A MRI Image Segmentation is proposed based on a multi-scale feature pyramid network and dual attention mechanism. Our innovation is the design of two modules: 1) a dilated convolution and multi-scale feature pyramid network are used in the encoding to avoid the semantic gap. 2) a dual attention mechanism is designed to maintain spatial information of U-Net and reduce misalignment. Experiments on a subabdominal MRI image dataset show the proposed method achieves better performance than others methods. In conclusion, a multi-scale feature pyramid network can reduce the semantic gap, and the dual attention mechanism can make an alignment of features between encoding and decoding.
    
[^139]: ZeroFlow: 通过蒸馏实现快速零标签场景流

    ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])

    [http://arxiv.org/abs/2305.10424](http://arxiv.org/abs/2305.10424)

    ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。

    

    场景流估计是描述连续点云之间的三维运动场的任务。最先进的方法使用强大的先验知识和测试时优化技术，但对于大规模点云需要数十秒的时间，使其无法作为实时应用程序（如开放世界目标检测）的计算机视觉基元使用。前向传递方法相对快速，对于大规模点云的运行时间在数十至数百毫秒之间，但需要昂贵的人力监督。为了解决这两个限制，我们提出了一种简单的蒸馏框架 Scene Flow via Distillation，使用无标签优化方法来生成伪标签以监督前向传递模型。我们实现了这个框架中的 ZeroFlow，使用零人工标签，在大规模点云上实时生成场景流估计结果，同时质量竞争状态下的最先进方法。值得注意的是，在测试时 ZeroFlow

    Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
    
[^140]: 应用于概率时间序列填充的Schr\"odinger bridge问题的收敛性分析和算法

    Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])

    [http://arxiv.org/abs/2305.07247](http://arxiv.org/abs/2305.07247)

    本论文提出了一种基于近似投影的Schr\"odinger bridge算法，它能够应用于概率时间序列填充，并在医疗保健和环境数据方面实现最先进的结果。

    

    Schr\"odinger bridge问题（SBP）在生成建模中引起了越来越多的关注。然而，近似的投影是唯一可用的，其收敛性还不是十分清楚。我们提出了一种基于近似投影的Schr\"odinger bridge算法的第一个收敛分析。我们将SBP应用于概率时间序列填充，展示了优化传输成本可以提高性能，提出的算法在医疗保健和环境数据方面实现了最先进的结果。

    The Schr\"odinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schr\"odinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time ser
    
[^141]: ImageBind:一个共同嵌入空间绑定所有模态的方法

    ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])

    [http://arxiv.org/abs/2305.05665](http://arxiv.org/abs/2305.05665)

    ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。

    

    我们提出了ImageBind，这是一种跨越图像、文本、音频、深度、热传感和IMU数据的六种不同模态的联合嵌入方法。我们展示，不需要训练所有配对数据，只需要图像配对数据就足以将这些模态绑定在一起。ImageBind可以利用最近的大规模视觉-语言模型，并通过使用它们与图像的自然配对，将它们的零样本能力扩展到新的模态。它可以实现“开箱即用”的新型应用程序，包括跨模态检索、用算术组合模态、跨模态检测和生成。新型应用随着图像编码器的强度而不断改进，我们在跨模态的零样本识别任务上取得了新的最优成绩，超过了专家监督模型。最后，我们还展示了强的几何识别结果，超过了以前的工作，ImageBind成为了评估视觉模态联合学习的一种新方法。

    We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio
    
[^142]: 离散扩散建模下的高效和度数引导图生成

    Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])

    [http://arxiv.org/abs/2305.04111](http://arxiv.org/abs/2305.04111)

    本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。

    

    基于扩散的生成图模型已被证明在生成高质量小图方面非常有效。然而，它们需要更可扩展性，以生成包含数千个节点的大图并满足图统计。本文提出了EDGE，一种新的基于扩散的生成图模型，用于生成大型图的生成任务。为了提高计算效率，我们通过在每个时间步长随机删除边来鼓励图的稀疏性，并最终获得一张空白图。EDGE仅在每个去噪步骤中关注图中一部分节点。它比以前的基于扩散的模型更少地进行边预测。此外，EDGE明确地允许对图的节点度数进行建模，进一步提高了模型的性能。实证研究表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。它还在生成质量方面优于基准模型。

    Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
    
[^143]: 关于数据子群体间机器学习模型性能的非线性相关性

    On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])

    [http://arxiv.org/abs/2305.02995](http://arxiv.org/abs/2305.02995)

    在不同数据子群体间，机器学习模型的内部准确性和外部准确性之间的相关性是非线性的，呈现出“月亮形”的相关性。

    

    理解机器学习模型在不同数据分布下的性能对于可靠的应用至关重要。尽管最新的经验研究认为训练数据内部的准确性和新数据外部的准确性之间存在近乎完美的线性相关性，但我们在各种数据集、模型和训练时期进行了严格的实验和分析，发现在子群体转移下，内部准确性和外部准确性之间的相关性更为微妙，并且在上升阶段存在“月亮形”的相关性（抛物线上升曲线）。

    Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
    
[^144]: 无参数条件和局部连接切片Wasserstein流量的生成建模

    Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows. (arXiv:2305.02164v1 [cs.LG])

    [http://arxiv.org/abs/2305.02164](http://arxiv.org/abs/2305.02164)

    本文提出了两个重要贡献：一是提出了条件切片Wasserstein流（CSWF），可以实现非参数条件建模，二是将局部连接和多尺度表示等视觉研究启发的技术引入到SWF中，从而大大提高了图像建模的效率和质量。

    

    切片Wasserstein流（SWF）是一种非参数生成建模的有前途的方法，但由于其发生生成质量的亚优性和缺乏条件建模能力而未被广泛采用。我们在这项工作中，为弥合这一差距做出了两个重要贡献。首先，基于一个愉悦的观察（在某些条件下），联合分布的SWF与条件分布的SWF相符，我们提出了条件切片Wasserstein流（CSWF），这是SWF的一个简单但有效的扩展，可实现非参数条件建模。其次，我们引入了适当的图像归纳偏置到SWF中，用两个技术受到视觉研究中的局部连接和多尺度表示的启发，大大提高了图像建模的效率和质量。通过全部改进，在进行纯非参数建模的同时，在有条件和无条件任务上实现了与许多深度参数化生成模型相当的生成性能。

    Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametric generative modeling but has not been widely adopted due to its suboptimal generative quality and lack of conditional modeling capabilities. In this work, we make two major contributions to bridging this gap. First, based on a pleasant observation that (under certain conditions) the SWF of joint distributions coincides with those of conditional distributions, we propose Conditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension of SWF that enables nonparametric conditional modeling. Second, we introduce appropriate inductive biases of images into SWF with two techniques inspired by local connectivity and multiscale representation in vision research, which greatly improve the efficiency and quality of modeling images. With all the improvements, we achieve generative performance comparable with many deep parametric generative models on both conditional and unconditional tasks in a purely nonparametric
    
[^145]: Moccasin：神经网络的高效张量重算技术

    Moccasin: Efficient Tensor Rematerialization for Neural Networks. (arXiv:2304.14463v1 [cs.LG])

    [http://arxiv.org/abs/2304.14463](http://arxiv.org/abs/2304.14463)

    本文提出了一种名为Moccasin的新型约束编程形式，用于实现在内存预算下最小化计算图的执行时间，相较于最近的研究，该方法显著提高了效率，并成功应用于神经网络的高效张量重算。

    

    在边缘计算设备上部署和训练神经网络面临许多挑战，其中较低的内存是部署大型神经网络模型时经常遇到的最大限制因素之一。张量重算是解决神经网络训练和推理所需高内存需求的一种方式。本文考虑在内存预算下最小化计算图的执行时间问题。具体来说，我们开发了一种新的约束编程形式，称为Moccasin，其中只有$O(n)$个整数变量，$n$是计算图中节点的数量。这相对于最近文献中提出的具有$O(n^2)$布尔变量的公式提出了显着的改进。我们展示了数值研究结果，表明我们的方法在大规模图上比最近的工作快一个数量级。

    The deployment and training of neural networks on edge computing devices pose many challenges. The low memory nature of edge devices is often one of the biggest limiting factors encountered in the deployment of large neural network models. Tensor rematerialization or recompute is a way to address high memory requirements for neural network training and inference. In this paper we consider the problem of execution time minimization of compute graphs subject to a memory budget. In particular, we develop a new constraint programming formulation called \textsc{Moccasin} with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute graph. This is a significant improvement over the works in the recent literature that propose formulations with $O(n^2)$ Boolean variables. We present numerical studies that show that our approach is up to an order of magnitude faster than recent work especially for large-scale graphs.
    
[^146]: 包含不完整观测的传感器数据回归问题

    Regression with Sensor Data Containing Incomplete Observations. (arXiv:2304.13415v1 [cs.LG])

    [http://arxiv.org/abs/2304.13415](http://arxiv.org/abs/2304.13415)

    本文提出了一种能处理不完整观测传感器数据的回归算法，解决了标签值由于不完整观测导致学习结果偏低的问题。

    

    本文解决了一个回归问题，其中输出标签值是感应现象幅度的结果。这种标签值低可能意味着现象的实际幅度低或传感器做出了不完整的观测。这导致标签值偏低，学习结果也偏低，因为标签值可能由于不完整的观测而偏低，即使现象的实际幅度高。为了解决这个问题，我们提出了一个学习算法，显式地模拟了带有负值的不对称噪声的不完整观测。我们证明了我们的算法是无偏的，就像从不包含不完整观测的未损坏数据学习一样。我们通过数值实验证明了我们算法的优势。

    This paper addresses a regression problem in which output label values are the results of sensing the magnitude of a phenomenon. A low value of such labels can mean either that the actual magnitude of the phenomenon was low or that the sensor made an incomplete observation. This leads to a bias toward lower values in labels and its resultant learning because labels may have lower values due to incomplete observations, even if the actual magnitude of the phenomenon was high. Moreover, because an incomplete observation does not provide any tags indicating incompleteness, we cannot eliminate or impute them. To address this issue, we propose a learning algorithm that explicitly models incomplete observations corrupted with an asymmetric noise that always has a negative value. We show that our algorithm is unbiased as if it were learned from uncorrupted data that does not involve incomplete observations. We demonstrate the advantages of our algorithm through numerical experiments.
    
[^147]: 针对函数逼近的在线强化学习的一般覆盖条件的可证明优势

    Provable benefits of general coverage conditions in efficient online RL with function approximation. (arXiv:2304.12886v1 [stat.ML])

    [http://arxiv.org/abs/2304.12886](http://arxiv.org/abs/2304.12886)

    研究者对在线强化学习提出了一种新的一般覆盖条件，并发现更多的覆盖条件，提高了在线强化学习的样本效率和表现，同时阐明良好的覆盖条件仍然有益于获得最优解。

    

    在线强化学习中，与其使用马尔可夫决策过程（MDPs）的标准结构假设，使用某种覆盖条件（源自离线强化学习）足以确保样本有效保证（Xie等人，2023）。本文关注这个新方向，挖掘更多可能和更普遍的覆盖条件，并研究它们在高效在线强化学习中的潜力和用途。我们鉴定了更多概念，包括$L^p$功能集中度、密度比实现性以及部分/全覆盖条件的权衡，这些概念也有益于实现样本有效的在线强化学习，从而实现改进的遗憾边界。此外，如果利用探索性的离线数据，在我们的覆盖条件下，可以为在线强化学习实现统计和计算上高效的保证。此外，即使MDP结构已经给出，例如线性MDP，我们也阐明了良好的覆盖条件仍然有益于获得最优解。

    In online reinforcement learning (RL), instead of employing standard structural assumptions on Markov decision processes (MDPs), using a certain coverage condition (original from offline RL) is enough to ensure sample-efficient guarantees (Xie et al. 2023). In this work, we focus on this new direction by digging more possible and general coverage conditions, and study the potential and the utility of them in efficient online RL. We identify more concepts, including the $L^p$ variant of concentrability, the density ratio realizability, and trade-off on the partial/rest coverage condition, that can be also beneficial to sample-efficient online RL, achieving improved regret bound. Furthermore, if exploratory offline data are used, under our coverage conditions, both statistically and computationally efficient guarantees can be achieved for online RL. Besides, even though the MDP structure is given, e.g., linear MDP, we elucidate that, good coverage conditions are still beneficial to obtai
    
[^148]: 在微调时缓解多模态模型中的错误相关性

    Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])

    [http://arxiv.org/abs/2304.03916](http://arxiv.org/abs/2304.03916)

    本文提出了一种使用多模态对比损失函数的方法，通过在微调期间检测和明确区分受影响类别的错误属性，缓解多模态模型的错误相关性，同时提高模型精度和指向目标领域的有意义特征。

    

    损害模型泛化能力或导致模型基于错误原因的错误相关性是实际部署面临的主要鲁棒性问题之一。然而，在预训练大型模型期间缓解这些相关性可能成本高昂且不切实际，特别是对于没有高性能计算资源的人来说。本文提出了一种新方法，以解决特定领域的微调期间的错误相关性。针对多模态模型（例如CLIP），所提出的方法利用这些模型中的不同模态来检测并明确区分受影响类别的错误属性，通过表达语言的多模态对比损失函数来实现。我们在CLIP上进行的实验证明和深入的可视化显示，这种介入能够有效地提高模型精度，而不存在错误属性，并将模型指向目标领域的有意义特征。

    Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the 
    
[^149]: 针对Jumbo-Visma队卡路里预测的符合性回归

    Conformal Regression in Calorie Prediction for Team Jumbo-Visma. (arXiv:2304.03778v1 [cs.LG])

    [http://arxiv.org/abs/2304.03778](http://arxiv.org/abs/2304.03778)

    本文提出一种新的符合性回归方法，通过预测动力和速度来为自行车比赛中的每个骑手提供卡路里需求的估计。

    

    UCI WorldTour赛事是男子精英公路自行车比赛的顶级赛事，对骑行者的体能和耐力进行考验。Jumbo-Visma队的教练们长期以来一直负责预测每个比赛日历中荷兰队的每位骑手的能量需求，以确保骑手在比赛过程中有足够的能量和资源来保持高水平表现。本文提出了一种新的更有效的预测骑行比赛能量需求的方法。通过回归模型预测速度和动力，为每个阶段的每个个体骑手提供卡路里需求估计。

    UCI WorldTour races, the premier men's elite road cycling tour, are grueling events that put riders' physical fitness and endurance to the test. The coaches of Team Jumbo-Visma have long been responsible for predicting the energy needs of each rider of the Dutch team for every race on the calendar. Those must be estimated to ensure riders have the energy and resources necessary to maintain a high level of performance throughout a race. This task, however, is both time-consuming and challenging, as it requires precise estimates of race speed and power output. Traditionally, the approach to predicting energy needs has relied on coaches' judgement and experience, but this method has its limitations and often leads to inaccurate predictions. In this paper, we propose a new, more effective approach to predicting energy needs for cycling races. By predicting the speed and power with regression models, we provide the coaches with calorie needs estimate for each individual rider per stage inst
    
[^150]: 利用强化学习将一个中等大小的英文GPT模型对齐到西班牙语的小封闭领域中

    Aligning a medium-size GPT model in English to a small closed domain in Spanish using reinforcement learning. (arXiv:2303.17649v1 [cs.CL])

    [http://arxiv.org/abs/2303.17649](http://arxiv.org/abs/2303.17649)

    本文介绍了一种将英文GPT模型对齐到西班牙语的小封闭领域中的方法，该方法使用了奖励模型来改进答案的解码和生成，在问答任务中取得了良好的结果。

    

    本文提出了一种方法，将原本用于开放领域的中等大小英文GPT模型，对齐到西班牙语的小封闭领域。该模型被精细调整用于问答任务。为了实现这一目标，我们还需要训练和实现另一个神经网络（我们称之为奖励模型），以评分并确定答案是否适用于给定的问题。该组件有助于改进系统回答的解码和生成。 BLEU和perplexity等数字度量标准被用于评估模型，同时也使用人类判断来比较解码技术与其他技术。最终，结果支持了所提出的方法，并确定使用奖励模型来对齐生成回答是可行的。

    In this paper, we propose a methodology to align a medium-sized GPT model, originally trained in English for an open domain, to a small closed domain in Spanish. The application for which the model is finely tuned is the question answering task. To achieve this we also needed to train and implement another neural network (which we called the reward model) that could score and determine whether an answer is appropriate for a given question. This component served to improve the decoding and generation of the answers of the system. Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others. Finally, the results favored the proposed method, and it was determined that it is feasible to use a reward model to align the generation of responses.
    
[^151]: 极性是您学习和快速传递所需的全部

    Polarity is all you need to learn and transfer faster. (arXiv:2303.17589v1 [cs.LG])

    [http://arxiv.org/abs/2303.17589](http://arxiv.org/abs/2303.17589)

    本文从权重极性的角度提出了一个思路：发育过程会初始化有优势极性配置的自然智能，当自然智能成长和学习时，突触的大小发生变化，但极性基本保持不变，如果权重极性被适当地设置在先，那么网络学习所需的时间和数据将会减少，从而增加学习和转移的效率。

    

    自然智能在动态世界中茁壮成长——它们可以很快地学习，有时仅需要少量样本。相比之下，人工智能通常需要大量的训练样本和计算能力才能学习。什么设计原则使得自然智能和人工智能之间存在如此明显的差异？在这里，我们从权重极性的角度提出了一个思路：发育过程会初始化有优势极性配置的自然智能，当自然智能成长和学习时，突触的大小发生变化，但极性基本保持不变。通过模拟和图像分类任务，我们证明了如果权重极性被适当地设置在先，那么网络学习所需的时间和数据将会减少。我们还明确说明了某些情况下，先验设置权重极性会对网络产生不利的影响。我们的工作从学习的统计和计算效率的角度阐述了权重极性的价值。

    Natural intelligences (NIs) thrive in a dynamic world - they learn quickly, sometimes with only a few samples. In contrast, Artificial intelligences (AIs) typically learn with prohibitive amount of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we propose an angle from weight polarity: development processes initialize NIs with advantageous polarity configurations; as NIs grow and learn, synapse magnitudes update yet polarities are largely kept unchanged. We demonstrate with simulation and image classification tasks that if weight polarities are adequately set $\textit{a priori}$, then networks learn with less time and data. We also explicitly illustrate situations in which $\textit{a priori}$ setting the weight polarities is disadvantageous for networks. Our work illustrates the value of weight polarities from the perspective of statistical and computational efficiency during learning.
    
[^152]: 非拟合分数重新权重实现自适应一致性预测

    Adaptive Conformal Prediction by Reweighting Nonconformity Score. (arXiv:2303.12695v1 [stat.ML])

    [http://arxiv.org/abs/2303.12695](http://arxiv.org/abs/2303.12695)

    该论文提出了一种新方法，利用分位数回归森林来学习非拟合分数的分布，并利用其权重分配更多的重要性给残差与测试点相似的样本，从而实现更符合模型的不确定性的预测区间。

    

    尽管具有吸引人的理论保证和实际成功，但由一致性预测（CP）给出的预测区间（PI）可能无法反映给定模型的不确定性。这种限制源于CP方法对所有测试点使用常数修正，无视它们的不确定性，以确保覆盖特性。为了解决这个问题，我们提出使用分位数回归森林（QRF）来学习非拟合分数的分布，并利用QRF的权重将更多的重要性分配给残差与测试点相似的样本。这种方法导致的PI长度更符合模型的不确定性。此外，QRF学习到的权重提供了特征空间的划分，通过组合一致化可以实现更高效的计算和改进PI的适应性。我们的方法享有基于样本和基于训练条件的无假设有限覆盖率，并在适当的假设下，也可以

    Despite attractive theoretical guarantees and practical successes, Predictive Interval (PI) given by Conformal Prediction (CP) may not reflect the uncertainty of a given model. This limitation arises from CP methods using a constant correction for all test points, disregarding their individual uncertainties, to ensure coverage properties. To address this issue, we propose using a Quantile Regression Forest (QRF) to learn the distribution of nonconformity scores and utilizing the QRF's weights to assign more importance to samples with residuals similar to the test point. This approach results in PI lengths that are more aligned with the model's uncertainty. In addition, the weights learnt by the QRF provide a partition of the features space, allowing for more efficient computations and improved adaptiveness of the PI through groupwise conformalization. Our approach enjoys an assumption-free finite sample marginal and training-conditional coverage, and under suitable assumptions, it also
    
[^153]: 单位缩放：开箱即用的低精度训练方法。

    Unit Scaling: Out-of-the-Box Low-Precision Training. (arXiv:2303.11257v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11257](http://arxiv.org/abs/2303.11257)

    本论文提出了一种称为“单位缩放”的方法，该方法可简化低精度数字格式的操作并提高训练效率。该方法通过在初始化时将所有权重、激活函数和梯度的单位差变为1来解决低精度训练范围的问题，与其他方法不同，它不需要多次运行也没有显著的计算开销。

    

    本论文提出了一种名为“单位缩放”的范式，用于设计神经网络模型，该方法简化了使用低精度数字格式的操作。在FP16或FP8格式下进行训练可以大大提高效率，但可能缺乏足够的范围进行训练。单位缩放通过引入基于模型数字的原则方法，寻求在初始化时所有权重、激活函数和梯度的单位方差来解决这个问题。与其他方法不同，这种方法既不需要多次运行以找到合适的比例，也没有显著的计算开销。我们展示了单位缩放在各种模型和优化器上的有效性。我们进一步展示了现有模型可以被调整为单位缩放，例如使用FP16对BERT-Large进行训练，然后使用FP8进行训练，而不会影响网络精度。

    We present unit scaling, a paradigm for designing deep learning models that simplifies the use of low-precision number formats. Training in FP16 or the recently proposed FP8 formats offers substantial efficiency gains, but can lack sufficient range for out-of-the-box training. Unit scaling addresses this by introducing a principled approach to model numerics: seeking unit variance of all weights, activations and gradients at initialisation. Unlike alternative methods, this approach neither requires multiple training runs to find a suitable scale nor has significant computational overhead. We demonstrate the efficacy of unit scaling across a range of models and optimisers. We further show that existing models can be adapted to be unit-scaled, training BERT-Large in FP16 and then FP8 with no degradation in accuracy.
    
[^154]: 图形提示方法综述：技术，应用和挑战

    A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges. (arXiv:2303.07275v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07275](http://arxiv.org/abs/2303.07275)

    该论文从图的角度审查了提示方法，将提示函数与图知识相结合，以解决在复杂任务中设计提示的挑战，在此基础上组织现有工作，并描述了应用和未来挑战。

    

    最近，“预训练，提示，预测训练”范例已经成为一种用有限标记数据学习普适模型的方式。这种方法涉及使用预训练模型和一个提示函数，该函数将模板应用于输入样本，添加指示背景并将目标任务重构为预训练任务。然而，在复杂任务中设计提示可能是一个具有挑战性和耗时的过程。这种限制可以通过使用图数据来解决，因为图形作为显式地建模实体之间交互的结构化知识库。在这个调查中，我们从图的角度审查提示方法，其中提示函数使用图知识进行扩展。特别是，我们介绍了图形提示学习的基本概念，组织了设计图形提示函数的现有工作，并描述了它们的应用和未来挑战。本调查将弥合图形和提示设计之间的差距。

    The recent "pre-train, prompt, predict training" paradigm has gained popularity as a way to learn generalizable models with limited labeled data. The approach involves using a pre-trained model and a prompting function that applies a template to input samples, adding indicative context and reformulating target tasks as the pre-training task. However, the design of prompts could be a challenging and time-consuming process in complex tasks. The limitation can be addressed by using graph data, as graphs serve as structured knowledge repositories by explicitly modeling the interaction between entities. In this survey, we review prompting methods from the graph perspective, where prompting functions are augmented with graph knowledge. In particular, we introduce the basic concepts of graph prompt learning, organize the existing work of designing graph prompting functions, and describe their applications and future challenges. This survey will bridge the gap between graphs and prompt design 
    
[^155]: 我们能否将Transformer应用到多种ImageNet模型的参数预测中进行扩展？

    Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?. (arXiv:2303.04143v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04143](http://arxiv.org/abs/2303.04143)

    该论文提出了一个可以预测其他神经网络高质量ImageNet参数的神经网络，通过使用预测参数进行初始化，能够提高多种ImageNet模型的训练速度，并且在转移到其他数据集时可以更快地收敛并达到竞争力的最终性能。

    

    在大规模数据集上对神经网络进行预训练已成为机器学习中的基石，但这只能由一些拥有充足资源的社区实现。我们旨在实现一个雄心勃勃的目标：民主化预训练。为此，我们训练并发布了一个单一的神经网络，可以预测其他神经网络的高质量ImageNet参数。通过使用预测参数进行初始化，我们可以提高PyTorch中可用的各种ImageNet模型的训练速度。在转移到其他数据集时，使用预测参数初始化的模型也会更快地收敛并达到竞争力的最终性能。

    Pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. We aim at an ambitious goal of democratizing pretraining. Towards that goal, we train and release a single neural network that can predict high quality ImageNet parameters of other neural networks. By using predicted parameters for initialization we are able to boost training of diverse ImageNet models available in PyTorch. When transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.
    
[^156]: 关于分层多分辨率图生成模型的研究

    On Hierarchical Multi-Resolution Graph Generative Models. (arXiv:2303.03293v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03293](http://arxiv.org/abs/2303.03293)

    本文提出一种新颖的分层多分辨率图生成模型，能递归地生成多个层次的社区结构，并符合训练数据分布。该方法由粗到细地生成图，同时具有高度的可扩展性，提升了生成性能。

    

    在现实世界中，大部分的图都具有层次结构。然而，数据驱动的图生成仍然没有有效地捕捉到这种结构。为了解决这个问题，我们提出了一种新颖的方法，以多个分辨率递归地生成社区结构，生成的结构在每个层次结构上，都符合训练数据的分布。图的生成被设计为一系列由粗到细的生成模型，允许所有子结构的并行生成，从而实现高度的可扩展性。我们的方法在多个图数据集上展示了生成性能的提升。

    In real world domains, most graphs naturally exhibit a hierarchical structure. However, data-driven graph generation is yet to effectively capture such structures. To address this, we propose a novel approach that recursively generates community structures at multiple resolutions, with the generated structures conforming to training data distribution at each level of the hierarchy. The graphs generation is designed as a sequence of coarse-to-fine generative models allowing for parallel generation of all sub-structures, resulting in a high degree of scalability. Our method demonstrates generative performance improvement on multiple graph datasets.
    
[^157]: DeepMAD: 基于数学的深度卷积神经网络架构设计

    DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network. (arXiv:2303.02165v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.02165](http://arxiv.org/abs/2303.02165)

    DeepMAD是一种基于数学的新框架，能够在系统化的方式中设计出高性能的CNN模型，解决了CNN网络设计过程中的挑战性问题。

    

    近期视觉Transformer（ViT）的快速发展使其在各种视觉任务中刷新了最先进的性能, 超过了传统的基于卷积神经网络的模型。这引发了一些最近在卷积神经网络领域的反击性研究，表明当仔细调整时，纯CNN模型可以实现与ViT模型一样出色的性能。然而，设计这样高性能的CNN模型是具有挑战性的，需要对网络设计具有非平凡的先验知识。为此，提出了一种新的框架，称为深度卷积神经网络的数学架构设计（DeepMAD）, 以系统的方式设计高性能的CNN模型。在DeepMAD中，CNN网络被建模为一个信息处理系统，其表现力和效果可以通过结构参数进行分析和计算。然后，提出了一个约束的数学规划（MP）问题来优化这些结构参数。MP问题可以通过现成的MP求解器在CPU上轻松解决，并具有较小的记忆量。

    The rapid advances in Vision Transformer (ViT) refresh the state-of-the-art performances in various vision tasks, overshadowing the conventional CNN-based models. This ignites a few recent striking-back research in the CNN world showing that pure CNN models can achieve as good performance as ViT models when carefully tuned. While encouraging, designing such high-performance CNN models is challenging, requiring non-trivial prior knowledge of network design. To this end, a novel framework termed Mathematical Architecture Design for Deep CNN (DeepMAD) is proposed to design high-performance CNN models in a principled way. In DeepMAD, a CNN network is modeled as an information processing system whose expressiveness and effectiveness can be analytically formulated by their structural parameters. Then a constrained mathematical programming (MP) problem is proposed to optimize these structural parameters. The MP problem can be easily solved by off-the-shelf MP solvers on CPUs with a small memo
    
[^158]: 《Dropout Reduces Underfitting》

    Dropout Reduces Underfitting. (arXiv:2303.01500v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01500](http://arxiv.org/abs/2303.01500)

    本研究证明dropout不仅可以防止神经网络过拟合，还可以缓解欠拟合问题。在训练初期采用early dropout方法，可以减少小批次梯度的方向差异，缓解SGD中的随机性，从而提高模型的训练效果。

    

    《Dropout Reduces Underfitting》是一篇关于神经网络正则化方法Dropout的研究，证明了Dropout可以在训练初期防止欠拟合的效果。在研究中，发现Dropout可以减少小批次梯度的方向差异，并有助于将小批次梯度与整个数据集的梯度对齐，从而缓解SGD中的随机性，减少每个批次对模型训练的影响。因此，我们提出了一个解决欠拟合问题的方案——early dropout: 在训练的初始阶段应用dropout，并在训练后关闭dropout。相比于没有dropout的模型，采用early dropout的模型能够获得更低的最终训练损失。另外，我们还探讨了一种用于正则化过拟合模型的对称技术——late dropout。

    Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations an
    
[^159]: 一种新的生成模型：一步生成且支持零样本编辑——一致性模型

    Consistency Models. (arXiv:2303.01469v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01469](http://arxiv.org/abs/2303.01469)

    提出了一种支持一步生成且支持零样本编辑的生成模型——一致性模型，它们能够通过直接将噪声映射到数据来生成高质量样本，支持快速的一步生成，且仍然支持多步抽样以提高样本质量。

    

    扩散模型在图像、音频和视频生成领域有了显著的进展，但它们依赖于一个迭代抽样过程，导致生成速度缓慢。为了克服这个限制，我们提出了一致性模型，这是一族通过直接将噪声映射到数据来生成高质量样本的新模型。它们通过设计支持快速的一步生成，同时仍允许多步抽样来以计算换取样本质量。它们还支持零样本数据编辑，如图像修复、上色和超分辨率，而无需明确训练这些任务。一致性模型可以通过蒸馏预训练的扩散模型来训练，也可以作为独立的生成模型进行训练。

    Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generatio
    
[^160]: 将基于DNN的乘法遮罩扩展到深度子带滤波以改善去混响

    Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation. (arXiv:2303.00529v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2303.00529](http://arxiv.org/abs/2303.00529)

    提出了一种将基于DNN的乘法遮罩扩展到深度子带滤波以在时频域中实现语音恢复的方案，可通用于提供在时频域中的遮罩的任何深度神经网络，相比乘法遮罩去混响性能更好，在去噪性能方面几乎没有影响。

    

    本文提出了一种方案，将基于深度神经网络的乘法遮罩扩展到深度子带滤波中，以在时频域中实现语音恢复。结果方法可通用于提供在时频域中的遮罩的任何深度神经网络，只需要更少的可训练参数和对于最先进的神经网络可忽略的计算开销。我们证明，结果深度子带滤波方案优于乘法遮罩去混响，而几乎不影响去噪性能。我们认为，这是因为时频域中的深度子带滤波适用于混响消除文献中常用的子带近似，而乘法遮罩则对于一般用于去噪的窄带近似。

    In this paper, we present a scheme for extending deep neural network-based multiplicative maskers to deep subband filters for speech restoration in the time-frequency domain. The resulting method can be generically applied to any deep neural network providing masks in the time-frequency domain, while requiring only few more trainable parameters and a computational overhead that is negligible for state-of-the-art neural networks. We demonstrate that the resulting deep subband filtering scheme outperforms multiplicative masking for dereverberation, while leaving the denoising performance virtually the same. We argue that this is because deep subband filtering in the time-frequency domain fits the subband approximation often assumed in the dereverberation literature, whereas multiplicative masking corresponds to the narrowband approximation generally employed for denoising.
    
[^161]: GNOT: 一种用于运算符学习的通用神经运算符Transformer

    GNOT: A General Neural Operator Transformer for Operator Learning. (arXiv:2302.14376v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14376](http://arxiv.org/abs/2302.14376)

    提出了一种通用神经运算符Transformer——GNOT，用于解决机器学习中学习偏微分方程的解算子的问题，并通过设计新颖的异构归一化注意力层和引入几何门控机制来增强模型的灵活性和解决多尺度问题。在多个领域的具有挑战性的数据集上进行广泛实验，取得了显着的改进。

    

    在机器学习中，学习偏微分方程的解算子是一个重要的问题。然而，在实际应用中，学习算子存在一些挑战，例如不规则的网格、多个输入函数和解决PDE解的复杂性。为了解决这些挑战，我们提出了GNOT，一种可扩展且有效的基于Transformer的算子学习框架。通过设计新颖的异构归一化注意力层，我们的模型高度灵活，能够处理多个输入函数和不规则网格。此外，我们引入了一种几何门控机制，可以看作是软域分解以解决多尺度问题。Transformer体系结构的大模型容量使我们的模型能够应用于大型数据集和实际问题。我们在不同领域的多个具有挑战性的数据集上进行了广泛的实验，并取得了显着的改进。

    Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improv
    
[^162]: 关于差分隐私联邦线性背景下的上下文匹配问题

    On Differentially Private Federated Linear Contextual Bandits. (arXiv:2302.13945v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13945](http://arxiv.org/abs/2302.13945)

    本文提出了一种新的算法框架，可以解决跨桶差分隐私联邦线性背景下的上下文匹配问题，并修复了现有算法中存在的隐私保护失效、因噪声计算错误而导致的不正确的遗憾界和不可信的通信成本等问题。

    

    本文考虑跨桶差分隐私联邦线性背景下的上下文匹配问题，在不损害每个用户隐私的情况下，多个沉积物（代理）与本地用户互动，并通过中央服务器进行通信以实现协作。本文识别了现有技术中的三个问题：(i)声明的隐私保护失效和(ii)由于噪声计算错误而导致的不正确的遗憾界，以及(iii)不可信的通信成本。为了解决这些问题，我们采取了一个两步原则性的方法。首先，我们设计了一个算法框架，包括一个通用的联邦LCB算法和灵活的隐私协议。然后，利用所提出的框架，我们研究了两种不同隐私约束下的联邦LCB。我们首先在桶级本地差分隐私下建立了隐私和遗憾保证，修复了现有技术中存在的问题。为了进一步提高遗憾表现，我们接下来考虑了差分privacy的洗牌模型。

    We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos (agents) interact with the local users and communicate via a central server to realize collaboration while without sacrificing each user's privacy. We identify three issues in the state-of-the-art: (i) failure of claimed privacy protection and (ii) incorrect regret bound due to noise miscalculation and (iii) ungrounded communication cost. To resolve these issues, we take a two-step principled approach. First, we design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, leveraging the proposed framework, we study federated LCBs under two different privacy constraints. We first establish privacy and regret guarantees under silo-level local differential privacy, which fix the issues present in state-of-the-art algorithm. To further improve the regret performance, we next consider shuffle model of differential p
    
[^163]: 领域适应决策树：对精确度和公平性的影响。

    Domain Adaptive Decision Trees: Implications for Accuracy and Fairness. (arXiv:2302.13846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13846](http://arxiv.org/abs/2302.13846)

    研究提出领域适应的决策树(DADT)来提高机器学习模型在目标人群中的精确性，避免偏见和降低某些人群在目标人群中受到不利影响。

    

    在使用预先训练的机器学习模型时，已知一个问题是该模型所部署的目标人群可能与训练模型时使用的源人群并不相同。这可能导致模型的偏见，降低模型的性能。风险之一是，随着人群的变化，某些人群将被该模型低估，或者在目标人群中受到不利影响，即使他们在目标人群中变得更具代表性。领域适应提出了一些技术，用于解决目标人群不存在标签数据，但某些有关目标分布的信息存在的情况。本文通过引入领域适应的决策树(DADT)，为领域适应文献做出贡献。鉴于决策树因其可解释性和相对于其他更复杂模型的性能而越来越受到欢迎，我们将重点放在决策树上。通过DADT，我们旨在提高模型精确度。

    In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy
    
[^164]: DoG是SGD最好的朋友：一个无需参数调整的动态步长大小计划

    DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule. (arXiv:2302.12022v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12022](http://arxiv.org/abs/2302.12022)

    我们提出了一个参数-free 的动态 SGD 步长公式，称为梯度距离公式（DoG）， 它没有“学习率”参数，但是在局部有界的随机梯度优化中拥有强大的无参数收敛性，并在广泛的视觉和语言转移学习任务中的表现与有调整学习率的 SGD 相当接近。

    

    我们提出了一个无需参数调整的动态SGD步长公式，称为梯度距离公式（DoG）。DoG步长依赖于简单的经验量（初始点距离和梯度范数），并且没有“学习率”参数。在理论上，我们证明了DoG公式的一个略微变化可以保证具有强大的无参数收敛性，假定只有局部有界的随机梯度优化。在实践中，我们考虑了广泛的视觉和语言转移学习任务，并显示DoG的性能接近具有调整的学习率的SGD。我们还提出了一种逐层变量的DoG变体，通常优于调整的SGD，并接近调整的Adam的性能。PyTorch实现可在https://github.com/formll/dog获取。

    We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no ``learning rate'' parameter. Theoretically, we show that a slight variation of the DoG formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only \emph{locally bounded} stochastic gradients. Empirically, we consider a broad range of vision and language transfer learning tasks, and show that DoG's performance is close to that of SGD with tuned learning rate. We also propose a per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance of tuned Adam. A PyTorch implementation is available at https://github.com/formll/dog
    
[^165]: K-SHAP: 一种用于匿名状态-动作对的策略聚类算法

    K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11996](http://arxiv.org/abs/2302.11996)

    本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。

    

    从观测数据中学习智能体行为已经被证明可以提高我们对它们决策过程的理解，从而增强我们解释它们与环境和其他智能体之间交互的能力。尽管文献中已经提出了多种学习技术，但还有一种特定的情况尚未被探索，那就是智能体身份保持匿名的多智能体系统。例如，在金融市场中，标记数据通常是专有的，仅公开多个市场参与者交互而产生的匿名状态-动作对。因此，智能体行动序列不可观测，限制了现有工作的适用性。本文提出了一种策略聚类算法K-SHAP，它学习根据智能体策略对匿名状态-动作对进行分组。我们将该问题作为模仿学习(IL)任务，学习一个w...

    Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
    
[^166]: IB-RAR：信息瓶颈作为对抗性鲁棒性的正则化方法

    IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness. (arXiv:2302.10896v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10896](http://arxiv.org/abs/2302.10896)

    本文提出了一种名为IB-RAR的正则化方法，利用信息瓶颈来增强对抗性训练和非对抗性训练方法的鲁棒性，并通过过滤不必要的特征来提高准确性。

    

    本文提出了一种新颖的方法IB-RAR，利用信息瓶颈（IB）来增强对抗性训练和非对抗性训练方法的对抗性鲁棒性。首先，本文使用IB理论在损失函数中构建正则化器作为学习目标。然后，根据中间表示与标签之间的互信息（MI）过滤掉不必要的特征，因为使用IB训练的网络提供易于区分的MI特征。实验结果表明，我们的方法可以自然地与对抗性训练相结合，并在新的对抗性例子上提供始终更好的准确性。我们的方法在针对VGG16网络进行三次对抗性训练基准和CIFAR-10数据集的五种对抗性攻击方案中，平均提高了3.07％的准确率。此外，我们的方法也为无防御方法提供了良好的鲁棒性，例如仅使用交叉熵损失进行训练。最后，在无人干预的情况下，我们的方法可以在处理缺失标签的情况下提供更好的学习效果。

    In this paper, we propose a novel method, IB-RAR, which uses Information Bottleneck (IB) to strengthen adversarial robustness for both adversarial training and non-adversarial-trained methods. We first use the IB theory to build regularizers as learning objectives in the loss function. Then, we filter out unnecessary features of intermediate representation according to their mutual information (MI) with labels, as the network trained with IB provides easily distinguishable MI for its features. Experimental results show that our method can be naturally combined with adversarial training and provides consistently better accuracy on new adversarial examples. Our method improves the accuracy by an average of 3.07% against five adversarial attacks for the VGG16 network, trained with three adversarial training benchmarks and the CIFAR-10 dataset. In addition, our method also provides good robustness for undefended methods, such as training with cross-entropy loss only. Finally, in the absenc
    
[^167]: MaskedKD：使用遮蔽图像的高效Vision Transformer蒸馏

    MaskedKD: Efficient Distillation of Vision Transformers with Masked Images. (arXiv:2302.10494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10494](http://arxiv.org/abs/2302.10494)

    MaskedKD提出了一种通过遮蔽图像块来显著降低Vision Transformer (ViT)蒸馏成本的方法，而不影响学生模型的预测准确性。

    

    知识蒸馏对于训练轻量级模型是一种有效的方法，但它会在训练成本中引入大量的计算开销，因为该方法需要在训练样本上获取教师监督。当使用大规模的Vision Transformer（ViTs）等教师模型时，这种附加成本——蒸馏成本——最为明显。我们提出了MaskedKD，这是一种简单但有效的策略，可以显着降低蒸馏ViTs的成本，同时不损失学生模型的预测准确性。具体来说，MaskedKD通过遮蔽一部分输入到教师模型的图像块令教师模型的推理成本减少，因此可以跳过处理这些块所需的计算。所选的遮罩位置旨在防止屏蔽学生模型用于预测的图像的核心特征。该遮罩选择机制基于学生模型的某些注意力分数操作。

    Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model
    
[^168]: 视觉表征中风格与内容的简单分离

    Simple Disentanglement of Style and Content in Visual Representations. (arXiv:2302.09795v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09795](http://arxiv.org/abs/2302.09795)

    该论文提出了一个简单的后处理框架，用于分离学习到的表征中的内容和风格，在领域泛化中表现出显著的提升。

    

    学习具有可解释特征的视觉表征，即分离的表征，仍然是一个具有挑战性的问题。现有方法在某些情况下表现出成功，但要应用于像ImageNet这样的大规模视觉数据集则很困难。在这项工作中，我们提出了一个简单的后处理框架，用于从预训练的视觉模型中分离学习到的表征中的内容和风格。我们将预训练特征概率地建模为潜在内容和风格因素的线性综合，并基于概率模型开发了一个简单的分离算法。我们证明了该方法能够可靠地分离内容和风格特征，并在实践中验证了其有效性。我们处理后的特征在样式变化或与样式相关的虚假相关性导致分布偏移时，可以显著提高领域泛化性能。

    Learning visual representations with interpretable features, i.e., disentangled representations, remains a challenging problem. Existing methods demonstrate some success but are hard to apply to large-scale vision datasets like ImageNet. In this work, we propose a simple post-processing framework to disentangle content and style in learned representations from pre-trained vision models. We model the pre-trained features probabilistically as linearly entangled combinations of the latent content and style factors and develop a simple disentanglement algorithm based on the probabilistic model. We show that the method provably disentangles content and style features and verify its efficacy empirically. Our post-processed features yield significant domain generalization performance improvements when the distribution shift occurs due to style changes or style-related spurious correlations.
    
[^169]: 数据高效的对比自监督学习：易于学习的样本起到最大的作用。

    Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09195](http://arxiv.org/abs/2302.09195)

    该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。

    

    自监督学习（SSL）从大量的无标签训练数据中学习高质量的表示。随着数据集变得越来越大，识别对学习此类表示最有用的示例变得至关重要。这可以通过减少学习高质量表示所需的数据量来实现有效的SSL。然而，对于SSL的价值如何量化一直是一个悬而未决的问题。在本文中，我们首次解决了这个问题，证明在期望意义下，对比SSL中对学习做出最大贡献的示例是具有最相似数据增强的示例。我们对这些子集的SSL的广义性能提供了严格的保证。实验证明，令人惊讶的是，对SSL做出最大贡献的子集是对监督学习做出最小贡献的子集。通过广泛的实验，我们证明了我们的子集在CIFAR100、CIFAR中的表现优于随机子集3%以上。

    Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
    
[^170]: 机器学习安全防御中的平等度量

    Measuring Equality in Machine Learning Security Defenses. (arXiv:2302.08973v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08973](http://arxiv.org/abs/2302.08973)

    本文研究了机器学习安全防御方法的平等性能问题，提出了一种简单的平等度量和分析框架，鼓励进一步探索公平性在该领域的应用。

    

    在过去的十年中，机器学习安全社区已经发展了许多对抗攻击的防御方法。但这个社区中鲜有研究一个问题：这些防御方法为谁提供保护呢？本文考虑了一些常见的防御方法，并研究了当这些防御方法被不同的子群体使用时，它们是否会产生意想不到的平等性能问题。我们提出了一种简单的平等度量和分析框架，通过机器学习安全方法的公平性实证结果来回答这个问题。许多方法可能会直接造成伤害，我们称之为有偏漏洞和有偏排斥。我们的框架和度量方法可以应用于强化训练模型、基于预处理的方法和拒绝方法，以捕捉在安全预算上的行为。我们确定了一个实际的数据集，具有合理的计算成本，适合于测量防御的平等性。通过一个案例研究，我们展示了现代防御方法的准确性和平等性性能的衡量价值。我们希望我们提出的指标和方法能够鼓励和促进机器学习安全和防御领域的公平性探索。

    The machine learning security community has developed myriad defenses for evasion attacks over the past decade. An understudied question in that community is: for whom do these defenses defend? In this work, we consider some common approaches to defending learned systems and whether those approaches may offer unexpected performance inequities when used by different sub-populations. We outline simple parity metrics and a framework for analysis that can begin to answer this question through empirical results of the fairness implications of machine learning security methods. Many methods have been proposed that can cause direct harm, which we describe as biased vulnerability and biased rejection. Our framework and metric can be applied to robustly trained models, preprocessing-based methods, and rejection methods to capture behavior over security budgets. We identify a realistic dataset with a reasonable computational cost suitable for measuring the equality of defenses. Through a case st
    
[^171]: 联邦自加权领域自适应

    Federated Auto-weighted Domain Adaptation. (arXiv:2302.05049v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05049](http://arxiv.org/abs/2302.05049)

    这篇论文提出了一种用于联邦领域自适应的新聚合规则-联邦梯度投影。在此基础上，开发了一个自动加权方案，用于最优地结合源和目标梯度，以解决在数据稀缺和领域转移时常见的技术失败的问题。

    

    联邦领域自适应（FDA）是描述多个源客户端协作改善目标客户端性能的联邦学习设置，其中目标领域数据有限。源领域和目标领域之间的领域转移，加上目标领域的稀疏数据，使得FDA成为一个具有挑战性的问题，例如，常见的技术（如FedAvg和微调）在存在显著领域转移和数据稀缺性时通常会失败。为了全面了解这个问题，我们介绍了表征FDA设置的度量标准，并提出了用于分析聚合规则性能的理论框架。我们还提出了用于FDA的一种新的聚合规则，称为联邦梯度投影（$\texttt{FedGP}$），用于在训练期间聚合源梯度和目标梯度。重要的是，我们的框架使得开发一个自动加权方案成为可能，这个方案能够最优地结合源和目标梯度。

    Federated Domain Adaptation (FDA) describes the federated learning setting where a set of source clients work collaboratively to improve the performance of a target client where limited data is available. The domain shift between the source and target domains, coupled with sparse data in the target domain, makes FDA a challenging problem, e.g., common techniques such as FedAvg and fine-tuning, often fail with the presence of significant domain shift and data scarcity. To comprehensively understand the problem, we introduce metrics that characterize the FDA setting and put forth a theoretical framework for analyzing the performance of aggregation rules. We also propose a novel aggregation rule for FDA, Federated Gradient Projection ($\texttt{FedGP}$), used to aggregate the source gradients and target gradient during training. Importantly, our framework enables the development of an $\textit{auto-weighting scheme}$ that optimally combines the source and target gradients. This scheme impr
    
[^172]: 零样本协同合作学习框架的合作开放式学习

    Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.04831](http://arxiv.org/abs/2302.04831)

    该论文提出了一个COLE框架，通过构建合作游戏的开放式目标，从图论的角度评估和确定每个策略的协作能力，以有效地解决零样本协调中的合作不兼容性问题。

    

    协作人工智能中的零样本协调仍然是一个重大挑战，有效地协调一系列看不见的合作伙伴。先前的算法试图通过优化种群中的固定目标来改善策略或行为的多样性来解决这一挑战。然而，这些方法可能导致学习损失和与种群中某些策略无法合作，即合作不兼容性。为了解决这个问题，我们提出了合作开放式学习（COLE）框架，该框架从图论的角度构建了协作游戏的开放式目标，以评估和确定每个策略的协作能力。我们进一步明确了框架并提出了一种实用的算法，该算法利用了博弈论和图论的知识。此外，对算法的学习过程进行的分析显示，它可以有效地克服学习困难。

    Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
    
[^173]: 关于使用近似传输映射进行抽样的研究

    On Sampling with Approximate Transport Maps. (arXiv:2302.04763v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04763](http://arxiv.org/abs/2302.04763)

    本研究探讨了两种基于传输映射的抽样方法，研究结果表明，基于流的提议可以处理多峰目标，在高维度和训练不良的情况下使用依赖于重新参数化的方法更加稳健。

    

    通过将分布转化为易于处理的分布，传输映射可以简化具有非平凡几何结构的分布的抽样。随着深度神经网络参数化的传统流（NF）的发展，这种方法的潜力不断提高。NF增强采样器最近提出了将马尔可夫链蒙特卡罗方法与（i）来自流的提议绘制或（ii）基于流的重新参数化相结合。在这两种情况下，学习到的传输的质量会影响性能。本研究首次阐明了这两种方法的相对优势和劣势。我们的研究得出结论：直到中等维度，可以可靠地使用基于流的提议处理多峰目标。相比之下，在高维度和训练不良的情况下，依赖于重新参数化的方法在多模式方面存在困难，但其他方面更为稳健。

    Transport maps can ease the sampling of distributions with non-trivial geometries by transforming them into distributions that are easier to handle. The potential of this approach has risen with the development of Normalizing Flows (NF) which are maps parameterized with deep neural networks trained to push a reference distribution towards a target. NF-enhanced samplers recently proposed blend (Markov chain) Monte Carlo methods with either (i) proposal draws from the flow or (ii) a flow-based reparametrization. In both cases, the quality of the learned transport conditions performance. The present work clarifies for the first time the relative strengths and weaknesses of these two approaches. Our study concludes that multimodal targets can be reliably handled with flow-based proposals up to moderately high dimensions. In contrast, methods relying on reparametrization struggle with multimodality but are more robust otherwise in high-dimensional settings and under poor training. To furthe
    
[^174]: 泛化神经波函数

    Generalizing Neural Wave Functions. (arXiv:2302.04168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04168](http://arxiv.org/abs/2302.04168)

    本文提出了一种名为“图学习轨道嵌入”的神经网络重新参数化方法，可以泛化神经波函数到不同的分子，同时还提出了一个大小一致的波函数Ansatz，名为“分子轨道网络”，可以联合求解不同分子的Schrödinger方程，提高了求解速度和精度。

    

    最近，基于神经网络的波函数在建模从头算基态势能曲面方面取得了最新的状态-of-the-art精度。然而，这些网络只能解决相同原子组的不同空间排列。为了克服这个限制，我们提出了一种名为“图学习轨道嵌入(Globe)”的神经网络重新参数化方法，可以将神经波函数适应于不同的分子。Globe通过连接分子轨道和共价键以实现空间信息传递，学习局部电子结构的表示形式，从而可以在分子之间泛化。此外，我们提出了一个大小一致的波函数Ansatz，分子轨道网络(Moon)，专门用于联合求解不同分子的Schrödinger方程。在我们的实验中，我们发现Moon在4.5倍的迭代步数内即可收敛于以前的方法相似的精度，或在相同的时间内给出更低的能量。进一步分析表明，Moon的能量估算具有可加性。

    Recent neural network-based wave functions have achieved state-of-the-art accuracies in modeling ab-initio ground-state potential energy surface. However, these networks can only solve different spatial arrangements of the same set of atoms. To overcome this limitation, we present Graph-learned orbital embeddings (Globe), a neural network-based reparametrization method that can adapt neural wave functions to different molecules. Globe learns representations of local electronic structures that generalize across molecules via spatial message passing by connecting molecular orbitals to covalent bonds. Further, we propose a size-consistent wave function Ansatz, the Molecular orbital network (Moon), tailored to jointly solve Schr\"odinger equations of different molecules. In our experiments, we find Moon converging in 4.5 times fewer steps to similar accuracy as previous methods or to lower energies given the same time. Further, our analysis shows that Moon's energy estimate scales additive
    
[^175]: 利用离线数据进行高效在线强化学习

    Efficient Online Reinforcement Learning with Offline Data. (arXiv:2302.02948v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02948](http://arxiv.org/abs/2302.02948)

    本文研究了利用离线数据进行高效在线强化学习的方法，证明了现有的离线策略方法能够应用于在线学习，提出了一些最少但重要的更改，来实现可靠的性能并提供了实践中可应用的建议。

    

    样本效率和探索仍然是在线强化学习中的主要挑战。一种强有力的方法是包括离线数据，如来自人类专家或次优探索策略的先前轨迹。以前的方法依赖于广泛的修改和额外的复杂性来确保有效使用这些数据。相反，我们想问：我们是否可以简单地应用现有的离线策略方法来利用离线数据进行在线学习？在这项工作中，我们证明了答案是肯定的。但是，为了实现可靠的性能，需要对现有的离线策略强化学习算法进行一些最少但重要的更改。我们广泛地测试了这些设计选择，并展示了对性能影响最大的关键因素，得出了一组实践者可以轻松应用的建议，无论其数据包括少量专家演示或大量次优轨迹。

    Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see
    
[^176]: CHiLS：具有分层标签集的零样本图像分类

    CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets. (arXiv:2302.02551v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02551](http://arxiv.org/abs/2302.02551)

    本文提出了 CHiLS，它利用分层标签集进行零样本图像分类，通过产生更具信息性的类别名称，在获得更精细的分类效果的同时，还能使用更少的有标签数据。

    

    开放词汇模型（例如CLIP）通过能够基于它们（自然语言）的名称为每个类别生成嵌入向量，在零样本分类方面表现出强大的性能。先前的工作着重于通过提示工程或通过包含少量标记的下游数据（通过微调）来提高这些模型的准确性。但是，在改进类别名称的丰富性方面却鲜有研究，这在类别标签被粗略定义并且不具信息性的情况下可能会有问题。我们提出了具有分层标签集（CHiLS）的分类，这是一种专门为具有隐含语义层次结构的数据集设计的零样本分类替代策略。CHiLS分三步进行：（i）为每个类别生成一组子类，使用现有的标签层次结构或通过查询GPT-3实现；（ii）执行标准的零样本CLIP过程，就像这些子类别是感兴趣的标签那样进行；（iii）将预测的类别嵌入映射回原始类别层次结构，以在期望的粒度水平上进行预测。我们用四个具有不同语义层次的基准测试数据集评估了CHiLS，并表明它在需要更少的下游标签的同时优于最先进的零样本分类器。

    Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predi
    
[^177]: 多视图遮蔽世界模型用于视觉机器人操作

    Multi-View Masked World Models for Visual Robotic Manipulation. (arXiv:2302.02408v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.02408](http://arxiv.org/abs/2302.02408)

    本文介绍了一个多视图遮蔽自编码器，用于重构出随机遮蔽视点的像素，并学习基于自编码器表示的世界模型。该方法在多视图控制和单视图控制场景中展示了其有效性，能够训练出具有强视点随机化的策略，并应用于实际机器人任务中。

    

    视觉机器人操作研究和应用通常使用多个摄像头或视角来更好地感知世界。那么我们如何利用多视图数据的丰富性呢？本文研究如何利用多视图数据学习良好的表示，并将其用于视觉机器人操作。具体而言，我们训练一个多视图遮蔽自编码器，该自编码器重构出随机遮蔽视点的像素，然后学习一个基于自编码器表示的世界模型。我们在一系列场景中展示了我们方法的有效性，包括多视图控制、为表示学习使用辅助摄像头的单视图控制。我们还表明，使用多个随机视点训练的多视图遮蔽自编码器能够训练具有强视点随机化的策略，并将该策略转移到解决实际机器人任务中，而无需相机校准和适应程序。视频演示可在以下网址查看：https:/ / 2302.02408v2 [cs.RO] UPDATED

    Visual robotic manipulation research and applications often use multiple cameras, or views, to better perceive the world. How else can we utilize the richness of multi-view data? In this paper, we investigate how to learn good representations with multi-view data and utilize them for visual robotic manipulation. Specifically, we train a multi-view masked autoencoder which reconstructs pixels of randomly masked viewpoints and then learn a world model operating on the representations from the autoencoder. We demonstrate the effectiveness of our method in a range of scenarios, including multi-view control and single-view control with auxiliary cameras for representation learning. We also show that the multi-view masked autoencoder trained with multiple randomized viewpoints enables training a policy with strong viewpoint randomization and transferring the policy to solve real-robot tasks without camera calibration and an adaptation procedure. Video demonstrations are available at: https:/
    
[^178]: 参与兴奋：一种基于注意力的文本到图像扩散模型的语义引导方法

    Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. (arXiv:2301.13826v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13826](http://arxiv.org/abs/2301.13826)

    该论文提出了一种基于注意力的文本到图像扩散模型的语义引导方法，名为参与兴奋，在推理时间内干预生成过程以改善生成图像的信实性和完整性，并解决了传统扩散模型在图像语义生成中可能存在的失灵现象。

    

    最近的文本到图像生成模型展示了一种无与伦比的通过目标文本提示进行指导生成多种多样和富有创造性的形象的能力。虽然具有革命性，但目前最先进的扩散模型仍可能在生成完全传达给定文本提示中的语义的图像方面失败。我们分析了公开的稳定扩散模型，并评估了灾难性忽视的存在，即模型无法生成输入提示中的一个或多个主题。此外，我们发现在某些情况下，模型还未能将属性（例如颜色）正确绑定到其相应的主题上。为了帮助减轻这些失败情况，我们引入了产生式语义护理（GSN）的概念，在推理时间内寻求干预生成过程以改善所生成图像的信实性。使用基于注意力的 GSN 公式，被称为参与兴奋，我们引导模型改进跨注意力的不确定性。

    Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention un
    
[^179]: UPop：用于压缩视觉语言Transformer模型的统一和渐进式剪枝方法

    UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers. (arXiv:2301.13741v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13741](http://arxiv.org/abs/2301.13741)

    UPop是一种通用的视觉语言Transformer压缩框架，采用统一和渐进式剪枝方法，可自动分配剪枝比率，实现更高的压缩比率。

    

    真实世界的数据包含大量的多模态信息，其中视觉和语言是最具代表性的两种模态。此外，越来越重的模型，例如Transformer，已经引起了研究人员对模型压缩的关注。然而，如何压缩多模态模型，特别是视觉语言Transformer，仍然未被充分探索。本文提出了一种名为UPop的通用视觉语言Transformer压缩框架，它包括1）在原始模型中在连续优化空间中统一搜索多模态子网，从而实现可压缩模态和结构之间自动分配剪枝比率；2）渐进式搜索和微调子网，从而保持搜索和微调之间的收敛，以实现更高的压缩比率。

    Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \textit{e}.\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \textbf{U}nified and \textbf{P}r\textbf{o}gressive \textbf{P}runing (\textbf{\emph{UPop}}) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model archit
    
[^180]: 通过学习程序组合的方法实现分层编程强化学习

    Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs. (arXiv:2301.12950v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12950](http://arxiv.org/abs/2301.12950)

    本论文提出了一种名为HPRL的分层编程强化学习框架，通过学习程序组合的方法实现，能够产生具有人类可解释性并且在评估候选方案时可以准确奖励和惩罚的策略。

    

    Trivedi等人(2021)提出了一种方法(LEAPS)，旨在产生具有人类可解释性并且可以更好地推广到新场景的加强学习（RL）策略。方法先学习一个程序嵌入空间，以连续参数化来自预生成的程序数据集的多样化程序，然后在给定任务时在学习的程序嵌入空间中搜索解决任务的程序。

    Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pre-generated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite the encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program p
    
[^181]: 通过单个固定大小RELU网络的组合来增强表达能力

    On Enhancing Expressive Power via Compositions of Single Fixed-Size ReLU Network. (arXiv:2301.12353v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12353](http://arxiv.org/abs/2301.12353)

    本文探讨了深度神经网络的表达能力，通过组合单个固定大小RELU网络，证明了其具有惊人的表达能力，尤其是在逼近具有$1-$Lipschitz连续性和一般连续性的函数时。

    

    本文探讨了深度神经网络的表达能力，通过函数组合的框架，我们证明了重复组合单个固定大小RELU网络的惊人表达能力，尽管单个网络的表达能力有限。我们进一步将此结果扩展到了$[0,1]^d$上的一般连续函数，其逼近误差由连续性模量表征。

    This paper explores the expressive power of deep neural networks through the framework of function compositions. We demonstrate that the repeated compositions of a single fixed-size ReLU network exhibit surprising expressive power, despite the limited expressive capabilities of the individual network itself. Specifically, we prove by construction that $\mathcal{L}_2\circ \boldsymbol{g}^{\circ r}\circ \boldsymbol{\mathcal{L}}_1$ can approximate $1$-Lipschitz continuous functions on $[0,1]^d$ with an error $\mathcal{O}(r^{-1/d})$, where $\boldsymbol{g}$ is realized by a fixed-size ReLU network, $\boldsymbol{\mathcal{L}}_1$ and $\mathcal{L}_2$ are two affine linear maps matching the dimensions, and $\boldsymbol{g}^{\circ r}$ denotes the $r$-times composition of $\boldsymbol{g}$. Furthermore, we extend such a result to generic continuous functions on $[0,1]^d$ with the approximation error characterized by the modulus of continuity. Our results reveal that a continuous-depth network generat
    
[^182]: 理解Transformer模型的INT4量化：延迟速度提升、可组合性和故障案例

    Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. (arXiv:2301.12017v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12017](http://arxiv.org/abs/2301.12017)

    本文研究了在语言模型中采用INT4权重和激活量化的可行性，并开发了高度优化的W4A4编码器推断管道，支持不同的量化策略。使用W4A4可以实现模型在延迟方面的显著提高。

    

    鉴于Transformer基于语言模型的高计算和内存成本，提高其部署效率一直是一个挑战。尽管最近已经证明了INT8量化在减少内存成本和延迟方面的有效性，同时还保持了模型的准确性，但我们是否可以利用INT4（可以使硬件峰值吞吐量增加一倍）来实现进一步的延迟改进还不清楚。在这项研究中，我们探讨了在语言模型中采用INT4权重和激活（W4A4）量化的可行性。我们的发现表明，对于仅编码器和编码器-解码器模型，W4A4量化引入的准确性降低可以忽略不计，但对于仅解码器模型而言，会导致显著的准确性下降。为了实现使用W4A4的性能增益，我们开发了一个高度优化的端到端W4A4编码器推断管道，支持不同的量化策略。我们的INT4管道在面向延迟的场景下的速度可以提高8.5倍，在其他场景下最多可以提高3倍。

    Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\times$ faster for latency-oriented scenarios and up to $3\times$ for t
    
[^183]: 基于CTC和最优传输的语音翻译预训练方法

    Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11716](http://arxiv.org/abs/2301.11716)

    本文提出了一种基于CTC和最优传输的语音翻译预训练方法，可以有效减小语音和文本模态之间的差距，提高最终的ST准确性。

    

    语音到文本翻译(ST)中的模态差距是一个重要挑战，该文提出了一种预训练方法来减轻这个问题，无需改变ST模型的架构。首先，本文表明连接时序分类(CTC)损失可以通过设计来减小模态差距。通过与更常见的交叉熵损失的定量比较，我们证明了使用CTC进行预训练可以始终实现更好的最终ST准确性。其次，我们提出了一种结合CTC和最优传输的新型预训练方法以进一步减小这种差距。我们的实验证明了使用CTC和最优传输进行预训练相对于仅使用CTC进行预训练和没有进行预训练的基线模型均能够提供持续改进。

    The gap between speech and text modalities is a major challenge in speech-to-text translation (ST). Different methods have been proposed to reduce this gap, but most of them require architectural changes in ST training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the ST model. First, we show that the connectionist temporal classification (CTC) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with CTC consistently achieves better final ST accuracy. Nevertheless, CTC is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining CTC and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wassers
    
[^184]: 使用均值回归随机微分方程进行图像恢复

    Image Restoration with Mean-Reverting Stochastic Differential Equations. (arXiv:2301.11699v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11699](http://arxiv.org/abs/2301.11699)

    本文提出了一种通用图像恢复的方法，通过使用均值回归随机微分方程将一个高质量图像转换为降级图像并加入高斯噪声，然后使用最大似然目标来学习逆转轨迹，能够在不依赖于任何特定先验知识的情况下恢复低质量图像的原始状态。

    

    本文提出了一种通用图像恢复的随机微分方程（SDE）方法。主要构造是使用一种均值回归SDE将高质量图像转换为均值状态下的降级图像并加入固定高斯噪声。然后通过模拟相应的逆转时间SDE，我们能够在不依赖于任何特定先验知识的情况下恢复低质量图像的原始状态。关键是，所提出的均值回归SDE有一个封闭形式的解，使我们能够计算出真实的时间依赖得分并使用神经网络进行学习。另外，我们提出了一个最大似然目标来学习一个最优的逆转轨迹，从而稳定训练并改进恢复结果。

    This paper presents a stochastic differential equation (SDE) approach for general-purpose image restoration. The key construction consists in a mean-reverting SDE that transforms a high-quality image into a degraded counterpart as a mean state with fixed Gaussian noise. Then, by simulating the corresponding reverse-time SDE, we are able to restore the origin of the low-quality image without relying on any task-specific prior knowledge. Crucially, the proposed mean-reverting SDE has a closed-form solution, allowing us to compute the ground truth time-dependent score and learn it with a neural network. Moreover, we propose a maximum likelihood objective to learn an optimal reverse trajectory that stabilizes the training and improves the restoration results. The experiments show that our proposed method achieves highly competitive performance in quantitative comparisons on image deraining, deblurring, and denoising, setting a new state-of-the-art on two deraining datasets. Finally, the ge
    
[^185]: 学习稀疏观测交互系统的动力学

    Learning the Dynamics of Sparsely Observed Interacting Systems. (arXiv:2301.11647v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11647](http://arxiv.org/abs/2301.11647)

    本论文解决了学习稀疏观测交互系统的动力学问题，将其作为解的学习控制微分方程（CDE），利用签名理论将非线性问题转化为高维线性回归，具有明确的依赖于个体特定采样方案的预测误差的oracle界限。证明了该方法优于现有算法回收完整时间序列，且计算成本较低。

    

    我们解决了一个问题，即学习未知的非参数系统的动力学，该系统将目标时间序列和特征时间序列联系起来。特征时间序列在稀疏和不规则的网格上测量，而我们只能访问目标时间序列的一些点。学习后，我们可以使用这些动态将特征时间序列的前几个时间点来预测目标的值。我们将这个任务作为控制微分方程（CDE）解的学习。通过利用签名理论的丰富理论，我们能够将这个非线性问题转化为高维线性回归。我们提供了一个预测误差的oracle界限，其具有明确的依赖于个体特定采样方案的依赖。我们的理论结果通过模拟得到了证明，表明我们的方法在回收完整时间序列时优于现有算法，同时计算成本较低。我们最后展示了它在现实世界流行病学中的潜力。

    We address the problem of learning the dynamics of an unknown non-parametric system linking a target and a feature time series. The feature time series is measured on a sparse and irregular grid, while we have access to only a few points of the target time series. Once learned, we can use these dynamics to predict values of the target from the previous values of the feature time series. We frame this task as learning the solution map of a controlled differential equation (CDE). By leveraging the rich theory of signatures, we are able to cast this non-linear problem as a high-dimensional linear regression. We provide an oracle bound on the prediction error which exhibits explicit dependencies on the individual-specific sampling schemes. Our theoretical results are illustrated by simulations which show that our method outperforms existing algorithms for recovering the full time series while being computationally cheap. We conclude by demonstrating its potential on real-world epidemiologi
    
[^186]: 通过状态增强和奖励惩罚解决复杂约束强化学习问题

    Solving Richly Constrained Reinforcement Learning through State Augmentation and Reward Penalties. (arXiv:2301.11592v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11592](http://arxiv.org/abs/2301.11592)

    本文提出了一种基于状态增强和奖励惩罚的约束强化学习新方法，相较于现有技术，在保证安全性和目标性能方面表现更好。

    

    约束强化学习通过使用预期成本约束来执行策略的安全约束。其主要挑战在于处理使用策略积累的预期成本，而不仅仅是单个步骤中的成本。现有方法已经开发出了将整个策略的成本约束转换为本地决策（每个时间步骤）约束的创新方法。虽然这些方法在客观上提供了好的解决方案，但是它们可能在成本方面过于激进或过于保守。这是由于在本地成本约束中使用了“未来”或“后向”成本的估计。为此，我们提出了一个等效的无约束RL公式，其中包括增强状态空间和奖励惩罚。这种直观的公式具有广泛的适用性和有趣的理论属性。更重要的是，这为有效解决约束RL问题提供了新的范例。正如我们在实验结果中展示的那样，我们提出的方法在安全性和目标性能方面优于现有技术。

    Constrained Reinforcement Learning has been employed to enforce safety constraints on policy through the use of expected cost constraints. The key challenge is in handling expected cost accumulated using the policy and not just in a single step. Existing methods have developed innovative ways of converting this cost constraint over entire policy to constraints over local decisions (at each time step). While such approaches have provided good solutions with regards to objective, they can either be overly aggressive or conservative with respect to costs. This is owing to use of estimates for "future" or "backward" costs in local cost constraints.  To that end, we provide an equivalent unconstrained formulation to constrained RL that has an augmented state space and reward penalties. This intuitive formulation is general and has interesting theoretical properties. More importantly, this provides a new paradigm for solving constrained RL problems effectively. As we show in our experimental
    
[^187]: 预测是否随意？在公平分类中评估自洽性

    Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11562](http://arxiv.org/abs/2301.11562)

    在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。

    

    在公平分类中，不同经过训练的模型之间的预测方差是一个重要但鲜为人知的误差来源问题。 实证表明，某些情况下，预测的方差差异非常大，以至于决策实际上是随意的。 为了研究这个问题，我们进行了大规模的实证研究，并做出了四个总体贡献：我们1）定义了一种基于方差的度量标准，称为自洽性，在测量和减少随意性时使用； 2）开发了一种合理的算法，当预测无法做出决策时，可以放弃分类； 3）进行了迄今为止有关公平分类中方差（相对于自洽性和随意性）作用的最大规模实证研究； 4）推出了一个工具包，使美国住房抵押贷款披露法案（HMDA）数据集易于用于未来研究。 总的来说，我们的实证结果揭示了关于可重复性的令人震惊的见解。当考虑到方差和随意预测的可能性时，大多数公平分类基准接近公平。 但是，一小部分实例显示出极大的随意性水平，这表明当前的模型可能无法处理某些类型的数据。

    Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
    
[^188]: SNeRL: 语义感知的神经辐射场用于强化学习

    SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning. (arXiv:2301.11520v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11520](http://arxiv.org/abs/2301.11520)

    本文提出了一种称为SNeRL的语义感知神经辐射场，它通过学习3D-aware的隐式表示来进行强化学习，并在基于像素的以及最新的3D感知表示方法中表现出更好的性能。

    

    传统的强化学习表示方法很难有效地融合人类直观的3D环境理解，因此经常表现出次优性能。本文提出了一种称为SNeRL的语义感知神经辐射场，它通过联合优化卷积编码器和语义感知神经辐射场（NeRF）来从多视角图像中学习3D感知神经隐式表示。我们在NeRF中引入了3D语义和蒸馏特征场，并与RGB辐射场并行用于强化学习中的语义和对象中心表示学习。SNeRL在无模型和有模型强化学习中不仅优于以往的基于像素的表示方法，还优于最近的3D感知表示方法。

    As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.
    
[^189]: 轨迹感知的资格追踪在离线强化学习中的应用

    Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning. (arXiv:2301.11321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11321](http://arxiv.org/abs/2301.11321)

    提出一种轨迹感知的资格追踪多步运算符，可以同时表达每个决策和轨迹感知的方法，并解决了被完全裁剪的资格追踪无法逆转的问题。

    

    离线多步返回的非政策学习对于节约样本的强化学习至关重要，但抵消偏差的同时不加剧方差是具有挑战性的。一般来说，非政策偏差是通过资格追踪的方法来进行修正的，资格追踪通过通吃因子(Impotance Sampling)比例对过去的时间差分误差进行重新加权以纠正。许多离线算法都依赖这种机制，不同的是针对IS的统计估计方法所采用的“裁剪IS比例”协议的不同。不幸的是，一旦资格追踪被完全裁剪，其影响就无法逆转。这已经导致了将多个过去经历同时考虑在内的信用分配策略的出现。这些轨迹感知的方法尚未得到广泛的分析，它们的理论依据仍然不确定。本文提出了一种多步运算符，可以同时表达每个决策和轨迹感知的方法，并证明它们的收敛条件。

    Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios to combat the variance of the IS estimator. Unfortunately, once a trace has been fully cut, the effect cannot be reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time. These trajectory-aware methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose a multistep operator that can express both per-decision and trajectory-aware methods. We prove convergence conditi
    
[^190]: 限制图带宽来提高图生成技术

    Improving Graph Generation by Restricting Graph Bandwidth. (arXiv:2301.10857v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10857](http://arxiv.org/abs/2301.10857)

    本论文提出一种通过限制图的带宽来提高现有图生成模型输出空间的方法，该方法可以提高生成可扩展性和质量，并在不增加复杂性或减少表达能力的情况下与现有图生成方法兼容。

    

    深度图生成建模已被证明能够学习表征真实世界图形的复杂多尺度结构的分布。然而，现有方法的主要限制之一是它们的大输出空间，这限制了生成可扩展性并阻碍了基础分布的准确建模。为了克服这些限制，我们提出了一种新颖的方法，显着减少了现有图生成模型的输出空间。具体地，从许多真实世界图形具有低图带宽的观察出发，我们在训练和生成过程中限制图带宽。我们的策略提高了生成的可扩展性和质量，同时不增加架构复杂性或减小表达能力。我们的方法与现有的图生成方法兼容，并描述了它在自回归和单次模型上的应用。我们对合成和真实数据集进行了广泛的验证，包括分子结构生成数据集。

    Deep graph generative modeling has proven capable of learning the distribution of complex, multi-scale structures characterizing real-world graphs. However, one of the main limitations of existing methods is their large output space, which limits generation scalability and hinders accurate modeling of the underlying distribution. To overcome these limitations, we propose a novel approach that significantly reduces the output space of existing graph generative models. Specifically, starting from the observation that many real-world graphs have low graph bandwidth, we restrict graph bandwidth during training and generation. Our strategy improves both generation scalability and quality without increasing architectural complexity or reducing expressiveness. Our approach is compatible with existing graph generative methods, and we describe its application to both autoregressive and one-shot models. We extensively validate our strategy on synthetic and real datasets, including molecular grap
    
[^191]: 解释非收敛采样对基于能量模型(Energy-Based Models, EBMs)训练的影响

    Explaining the effects of non-convergent sampling in the training of Energy-Based Models. (arXiv:2301.09428v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09428](http://arxiv.org/abs/2301.09428)

    本文证明，使用非持续运行的EBM训练可以通过一个精确的动态过程完美地复制数据集的一组统计信息，而不是通过平衡测度的水平，这为使用EBM作为扩散模型奠定了基础。

    

    本文量化了使用非收敛马尔可夫链(Markov chains)训练EBMs的影响，证明了使用短的非持续运行来估计梯度进行训练的EBMs能够通过精确的动态过程完美复制数据的一组统计信息，而不是通过平衡测度的水平。我们的结果为最近提出的使用起始于随机初态的短跑作为EBMs中生成高质量样本的有效策略提供了一个根据，为使用EBMs作为扩散模型奠定了基础。在解释了这种效果在通用EBMs中的情况后，我们分析了两个可解的模型，其中描述了训练参数中非收敛采样的效果。最后，我们在ConvNet EBM和Boltzmann机器上进行了数字预测的测试。

    In this paper, we quantify the impact of using non-convergent Markov chains to train Energy-Based models (EBMs). In particular, we show analytically that EBMs trained with non-persistent short runs to estimate the gradient can perfectly reproduce a set of empirical statistics of the data, not at the level of the equilibrium measure, but through a precise dynamical process. Our results provide a first-principles explanation for the observations of recent works proposing the strategy of using short runs starting from random initial conditions as an efficient way to generate high-quality samples in EBMs, and lay the groundwork for using EBMs as diffusion models. After explaining this effect in generic EBMs, we analyze two solvable models in which the effect of the non-convergent sampling in the trained parameters can be described in detail. Finally, we test these predictions numerically on a ConvNet EBM and a Boltzmann machine.
    
[^192]: 自适应多层感知器的计算高效三维MRI重建

    Computationally Efficient 3D MRI Reconstruction with Adaptive MLP. (arXiv:2301.08868v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.08868](http://arxiv.org/abs/2301.08868)

    本文提出了自适应多层感知器的计算高效三维MRI重建方法，混合使用CNN和dMLP模块。相比于当前基于CNN的方法，本方法可以更好地捕捉长距离信息，并且能够适应较大的图像尺寸和GPU内存限制。

    

    相比于二维MRI，三维MRI提供了更好的体积空间分辨率和信噪比。然而，重建三维MRI图像更具挑战性。当前的方法主要基于带有小内核的卷积神经网络（CNN），但由于图像尺寸大和GPU内存限制，这很难扩展到足够适应三维MRI重建的规模。此外，MRI重建是一个反卷积问题，需要捕捉CNN难以捕捉的长距离信息。 多层感知器（MLP）可以模拟这种长距离信息，但它需要固定的输入尺寸。在本文中，我们提出了Recon3DMLP，这是一个CNN模块和小内核用于低频重建，自适应MLP（dMLP）模块和大内核用于提升高频重建的混合体，用于三维MRI重建。我们进一步利用了基于MRI物理学的循环位移操作，从而...（原文未完）

    Compared with 2D MRI, 3D MRI provides superior volumetric spatial resolution and signal-to-noise ratio. However, it is more challenging to reconstruct 3D MRI images. Current methods are mainly based on convolutional neural networks (CNN) with small kernels, which are difficult to scale up to have sufficient fitting power for 3D MRI reconstruction due to the large image size and GPU memory constraint. Furthermore, MRI reconstruction is a deconvolution problem, which demands long-distance information that is difficult to capture by CNNs with small convolution kernels. The multi-layer perceptron (MLP) can model such long-distance information, but it requires a fixed input size. In this paper, we proposed Recon3DMLP, a hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction, for 3D MRI reconstruction. We further utilized the circular shift operation based on MRI physics such that
    
[^193]: 嵌入式系统实时语义分割中的不确定性

    Uncertainty in Real-Time Semantic Segmentation on Embedded Systems. (arXiv:2301.01201v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.01201](http://arxiv.org/abs/2301.01201)

    本文提出了一种结合贝叶斯回归和动量传播的预测方法，能够实时在嵌入式硬件上产生有意义的不确定性，从而使语义分割模型在自动驾驶和人机交互等领域的实时应用成为可能。

    

    语义分割模型在自动驾驶和人机交互等领域的应用需要实时预测能力。实时应用的挑战被资源受限的硬件所加剧。虽然这些平台上实时方法的开发得到了增加，但这些模型无法足够地考虑到存在的不确定性。本文通过将预先训练模型的深层特征提取与贝叶斯回归和动量传播相结合，提出了一种关注不确定性的预测方法。我们演示了该方法如何在嵌入式硬件上实时产生有意义的不确定性，同时保持预测性能。

    Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful uncertainty on embedded hardware in real-time whilst maintaining predictive performance.
    
[^194]: 在多项选择众包中恢复前两个答案和混淆概率

    Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing. (arXiv:2301.00006v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2301.00006](http://arxiv.org/abs/2301.00006)

    该论文提出了一种多项选择众包任务的模型，该模型可以恢复最令人困惑的答案和混淆概率。在该模型下，提出了一个两阶段推断算法来推断最有可能的答案和混淆概率。

    

    众包已经成为一种标记大量数据的有效平台，具有成本和时间效益。大部分先前的工作都集中在设计一种有效的算法，仅恢复数据的真实标签。在本文中，我们考虑了多项选择众包任务，目标不仅是恢复真实标签，还包括最令人困惑的答案和混淆概率。最令人困惑的答案提供了关于任务的有用信息，揭示了除真实答案以外最可信的答案以及它的可信度。为了理论分析这样的情况，我们提出了一个模型，每个任务有两个最可信的答案，与其他选择有所不同。任务难度由前两个答案之间的混淆概率量化，工作可靠性由给出前两个答案的概率量化。在此模型下，我们提出了一个两阶段推断算法来推断前两个答案和混淆概率。

    Crowdsourcing has emerged as an effective platform for labeling large amounts of data in a cost- and time-efficient manner. Most previous work has focused on designing an efficient algorithm to recover only the ground-truth labels of the data. In this paper, we consider multi-choice crowdsourcing tasks with the goal of recovering not only the ground truth, but also the most confusing answer and the confusion probability. The most confusing answer provides useful information about the task by revealing the most plausible answer other than the ground truth and how plausible it is. To theoretically analyze such scenarios, we propose a model in which there are the top two plausible answers for each task, distinguished from the rest of the choices. Task difficulty is quantified by the probability of confusion between the top two, and worker reliability is quantified by the probability of giving an answer among the top two. Under this model, we propose a two-stage inference algorithm to infe
    
[^195]: 通过离散信息瓶颈在深度强化学习中进行表征学习

    Representation Learning in Deep RL via Discrete Information Bottleneck. (arXiv:2212.13835v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13835](http://arxiv.org/abs/2212.13835)

    本研究提出了一种名为RepDIB的表征学习方法，在强化学习中通过离散信息瓶颈构建隐状态，可以有效地去除感知信息中的噪声和无关信息，实验表明该方法能够提高性能。

    

    针对强化学习中感知信息带有噪声和无关信息的情况，本文提出了一种利用信息瓶颈方法构建隐状态的表征学习方法，名为RepDIB。该方法利用可变离散信息瓶颈学习结构化分解表示，并将简单而有效的瓶颈与现有自监督目标相结合。在多个在线和离线RL基准测试以及真实机器人臂任务中，实验证明使用RepDIB进行压缩表征可以有效提高性能。

    Several self-supervised representation learning methods have been proposed for reinforcement learning (RL) with rich observations. For real-world applications of RL, recovering underlying latent states is crucial, particularly when sensory inputs contain irrelevant and exogenous information. In this work, we study how information bottlenecks can be used to construct latent states efficiently in the presence of task-irrelevant information. We propose architectures that utilize variational and discrete information bottlenecks, coined as RepDIB, to learn structured factorized representations. Exploiting the expressiveness bought by factorized representations, we introduce a simple, yet effective, bottleneck that can be integrated with any existing self-supervised objective for RL. We demonstrate this across several online and offline RL benchmarks, along with a real robot arm task, where we find that compressed representations with RepDIB can lead to strong performance improvements, as th
    
[^196]: 连续对比微调改进低资源关系提取

    Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2212.10823](http://arxiv.org/abs/2212.10823)

    本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。

    

    关系提取（RE）依赖结构化注释语料库进行模型训练，尤其在低资源情况和领域中，该任务具有挑战性。近期研究通过自监督学习来解决低资源的RE，其中解决方案包括通过RE目标预训练关系嵌入，并通过分类为基础的目标对有标签数据进行微调。然而，这种方法的一个关键挑战是目标之间的差距，它阻止RE模型充分利用预训练表示中的知识。本文旨在弥合差距，并提出使用一致的对比学习目标预训练和微调RE模型。由于在这种表示学习范式中，一个关系可能在表示空间中轻松形成多个聚类，因此我们进一步提出了多中心对比损失，允许一个关系形成多个聚类以更好地对齐预训练。在两个文档中的实验表明，所提出的方法可以在低资源情况和领域中显着提高关系提取性能。

    Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
    
[^197]: Transformer模型通过梯度下降实现上下文学习

    Transformers learn in-context by gradient descent. (arXiv:2212.07677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07677](http://arxiv.org/abs/2212.07677)

    本文提出，训练Transformer模型应用于自回归目标问题时，与基于梯度的元学习的形式密切相关，通过梯度下降学习模型的“底层优化程序”的机制，在回归问题的领域中从机械的角度理解了Transformers模型中上下文学习的内部机制。

    

    目前，Transformers模型中上下文学习的机制尚未得到很好的理解，大多只停留在直觉上。本文提出，训练Transformer模型应用于自回归目标问题时，与基于梯度的元学习的形式密切相关。我们首先提供一个简单的权重构造，证明了由单个线性自注意力层引发的数据转换与由具有回归损失的梯度下降（GD）获得的转换具有等价性。在此基础上，我们通过实验证明，当仅训练自注意力Transformer模型进行简单的回归任务时，通过GD优化得到的模型与模型权重十分相似，或者在某些情况下，GD优化的权重与构造的权重相同。因此，我们展示了经过训练的Transformer模型是如何在前向传递中通过梯度下降学习模型的“底层优化程序”的。在回归问题的领域中，这使我们能够从机械的角度理解Transformers模型中上下文学习的内部机制。

    At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context
    
[^198]: 基于任务相似度元学习加速多目标非分层超参数最优化的树形结构Parzen估计

    Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06751](http://arxiv.org/abs/2212.06751)

    本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。

    

    超参数优化是提高深度学习性能的关键步骤。实践者通常面临多个方面的权衡，如准确性和延迟时间。在深度学习的高计算需求和对高效超参数优化的不断增长需求下，加速多目标优化变得越来越重要。本文将TPE的收购函数扩展到元学习设置中，使用由任务之间顶级域之间的重叠度定义的任务相似性。我们也从理论上分析并解决了任务相似性的局限性。在实验中，我们展示了我们的方法在表格HPO基准上加速了MO-TPE，并获得了最先进的性能。我们的方法还通过赢得AutoML 2022来得到外部验证。

    Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
    
[^199]: 稳定的美术家：扭曲漫流空间中的语义驾驭

    The Stable Artist: Steering Semantics in Diffusion Latent Space. (arXiv:2212.06013v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.06013](http://arxiv.org/abs/2212.06013)

    本文介绍了一种名为"稳定的美术家"的图像编辑方法，其中包含SEGA以及潜在遍历等组成部分，使得用户可以实现在图像生成过程中的细粒度控制，可以微妙地编辑图像、改变构图和风格，达到艺术构思优化等目的，实现了在各种文本到图像合成任务中的最先进的定量和定性结果。

    

    最近，基于大规模的文本条件下的生成扩散模型因其惊人的性能而受到了广泛关注，可以仅通过文本生成高保真度的图像。然而，一次性获得高质量的结果几乎是不可行的。相反，文本指导的图像生成包括用户对输入进行多次微小的更改，以迭代地雕刻出所想象的图像。然而，输入提示的微小更改通常会导致生成完全不同的图像，因此艺术家的控制受限于其粒度。为了提供灵活性，我们提出了稳定的美术家，这是一种图像编辑方法，可使图像生成过程中的细粒度控制更加容易。主要组成部分是语义引导(SEGA)，它沿着不同数量的语义方向引导扩散过程。这允许对图像进行微妙的编辑、改变构图和风格，以及优化整体艺术构思。此外，我们提出了一种名为潜在遍历的新技术，它可以实现对图像特定区域的局部和有导向地修改。我们的方法在各种文本到图像合成任务中实现了最先进的定量和定性结果，为用户提供了对所生成的图像前所未有的控制。

    Large, text-conditioned generative diffusion models have recently gained a lot of attention for their impressive performance in generating high-fidelity images from text alone. However, achieving high-quality results is almost unfeasible in a one-shot fashion. On the contrary, text-guided image generation involves the user making many slight changes to inputs in order to iteratively carve out the envisioned image. However, slight changes to the input prompt often lead to entirely different images being generated, and thus the control of the artist is limited in its granularity. To provide flexibility, we present the Stable Artist, an image editing approach enabling fine-grained control of the image generation process. The main component is semantic guidance (SEGA) which steers the diffusion process along variable numbers of semantic directions. This allows for subtle edits to images, changes in composition and style, as well as optimization of the overall artistic conception. Furthermo
    
[^200]: Elixir: 在小型 GPU 集群上训练大型语言模型

    Elixir: Train a Large Language Model on a Small GPU Cluster. (arXiv:2212.05339v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2212.05339](http://arxiv.org/abs/2212.05339)

    Elixir 提出了一种基于预运行模型分析的自动化高效大模型训练方案，可以将内存使用卸载到 CPU 和 NVMe 存储器中，充分发挥硬件的潜力，实验中表现优于最先进的基准模型。

    

    近年来，由于其规模前所未有的大小，大型语言模型取得了巨大的成功。然而，训练这些模型对于大多数研究人员来说是一项挑战，因为它需要大量的 GPU。为了减少 GPU 内存使用，提出了内存分区和内存卸载。这些方法消除了内存冗余，并将内存使用卸载到 CPU 和 NVMe 存储器中，使得可以在小型 GPU 集群上进行训练。然而，直接部署这些解决方案通常会导致次优效率。只有经验丰富的专家才能通过仔细调整分布式配置来充分发挥硬件的潜力。因此，我们提出了一种新颖的解决方案 Elixir，它基于预运行模型分析自动化高效的大模型训练。Elixir 的目标是确定分区和卸载技术的最佳组合，以最大化训练吞吐量。在我们的实验中，Elixir 显著优于当前最先进的基准

    In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art basel
    
[^201]: X-Paste：利用CLIP和StableDiffusion重新思考可扩展的实例分割复制粘贴(arXiv:2212.03863v2 [cs.CV] 修订版)

    X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion. (arXiv:2212.03863v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.03863](http://arxiv.org/abs/2212.03863)

    本文利用零样本识别和text2image模型，重新思考了可扩展的复制粘贴，实现了利用不同物体类别的图像进行实例分割，以获得更高的性能

    

    复制粘贴是一种简单且有效的实例分割数据增强策略。通过将对象实例随机粘贴到新的背景图像中，可以免费创建新的训练数据并显著提高分割性能，特别是对于罕见的物体类别。本文利用新出现的零样本识别模型（例如CLIP）和text2image模型（例如StableDiffusion）的能力，重新思考了可扩展的复制粘贴。我们首次展示了使用text2image模型生成图像或使用零样本识别模型过滤嘈杂爬取的图像以获取不同物体类别的可行性。

    Copy-Paste is a simple and effective data augmentation strategy for instance segmentation. By randomly pasting object instances onto new background images, it creates new training data for free and significantly boosts the segmentation performance, especially for rare object categories. Although diverse, high-quality object instances used in Copy-Paste result in more performance gain, previous works utilize object instances either from human-annotated instance segmentation datasets or rendered from 3D object models, and both approaches are too expensive to scale up to obtain good diversity. In this paper, we revisit Copy-Paste at scale with the power of newly emerged zero-shot recognition models (e.g., CLIP) and text2image models (e.g., StableDiffusion). We demonstrate for the first time that using a text2image model to generate images or zero-shot recognition model to filter noisily crawled images for different object categories is a feasible way to make Copy-Paste truly scalable. To 
    
[^202]: 随机动态系统的浓度现象：一种算子理论方法

    Concentration Phenomenon for Random Dynamical Systems: An Operator Theoretic Approach. (arXiv:2212.03670v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03670](http://arxiv.org/abs/2212.03670)

    本文提供了一种算子理论方法，解决了离散时间马尔可夫链的浓度现象问题，并证明了可逆性在浓度现象中的作用。

    

    本文通过算子理论方法，对具有不变的遗传测度$\mu_{\pi}$的离散时间马尔可夫链的给定观测量“$r$”的浓度现象进行了形式化，可能具有支持无界状态空间。本文的主要贡献是通过对Markov转移算子$P$和由$e^{r}$定义的乘法算子的组合研究，避免了繁琐的概率方法。结果表明，即使可观测/奖励函数是无界的，但对于某些$q>2$，$\|e^{r}\|_{q \rightarrow 2} \propto \exp\big(\mu_{\pi}(r) +\frac{2q}{q-2}\big) $，并且$P$具有上确界控制$\|P\|_{2 \rightarrow q }< e^{\frac{1}{2}[\frac{1}{2}-\frac{1}{q}]}$，则可以得到尖锐的非渐进浓度界限。由于输运-熵不等式，对于所有$q>2$，该乘法算子的上界得到保证。本文论证了可逆性在浓度现象中的作用。

    Via operator theoretic methods, we formalize the concentration phenomenon for a given observable `$r$' of a discrete time Markov chain with `$\mu_{\pi}$' as invariant ergodic measure, possibly having support on an unbounded state space. The main contribution of this paper is circumventing tedious probabilistic methods with a study of a composition of the Markov transition operator $P$ followed by a multiplication operator defined by $e^{r}$. It turns out that even if the observable/ reward function is unbounded, but for some for some $q>2$, $\|e^{r}\|_{q \rightarrow 2} \propto \exp\big(\mu_{\pi}(r) +\frac{2q}{q-2}\big) $ and $P$ is hyperbounded with norm control $\|P\|_{2 \rightarrow q }< e^{\frac{1}{2}[\frac{1}{2}-\frac{1}{q}]}$, sharp non-asymptotic concentration bounds follow. \emph{Transport-entropy} inequality ensures the aforementioned upper bound on multiplication operator for all $q>2$. The role of \emph{reversibility} in concentration phenomenon is demystified. These results a
    
[^203]: Yggdrasil Decision Forests：一种快速且可扩展的决策森林库

    Yggdrasil Decision Forests: A Fast and Extensible Decision Forests Library. (arXiv:2212.02934v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02934](http://arxiv.org/abs/2212.02934)

    Yggdrasil Decision Forests是一种快速且可扩展的决策森林库，旨在为研究和生产工作提供支持。该库的设计原则为易用性、安全性、模块化和高层抽象以及与其他机器学习库的集成，并在经典机器学习问题上展示了其使用，并在基准测试中取得了成功。

    

    Yggdrasil Decision Forests是一个用于训练、服务和解释决策森林模型的库，旨在为研究和生产工作提供支持，使用C++实现，并提供C++命令行界面、Python（名称为TensorFlow Decision Forests）、JavaScript、Go和Google Sheets（名称为Simple ML for Sheets）等语言接口。该库自2018年以来按照四个机器学习库和框架的设计原则进行有机开发：易用性、安全性、模块化和高层抽象以及与其他机器学习库的集成。在本文中，我们详细描述了这些原则，并展示了它们如何指导库的设计。然后，我们在一组经典机器学习问题上展示了我们的库的使用。最后，我们报告了一个将我们的库与相关解决方案进行比较的基准测试结果。

    Yggdrasil Decision Forests is a library for the training, serving and interpretation of decision forest models, targeted both at research and production work, implemented in C++, and available in C++, command line interface, Python (under the name TensorFlow Decision Forests), JavaScript, Go, and Google Sheets (under the name Simple ML for Sheets). The library has been developed organically since 2018 following a set of four design principles applicable to machine learning libraries and frameworks: simplicity of use, safety of use, modularity and high-level abstraction, and integration with other machine learning libraries. In this paper, we describe those principles in detail and present how they have been used to guide the design of the library. We then showcase the use of our library on a set of classical machine learning problems. Finally, we report a benchmark comparing our library to related solutions.
    
[^204]: 基于扩散模型的像素级指导的细粒度图像编辑方法

    Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models. (arXiv:2212.02024v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02024](http://arxiv.org/abs/2212.02024)

    本文提出了一种基于扩散模型的像素级指导的细粒度图像编辑方法，通过少量带注释数据训练像素分类器和映射编辑操作快速生成满足用户意图的图像，并且编辑结果质量和速度均优于基于GAN的方法。

    

    本文旨在开发适用于实际应用的细粒度真实图像编辑方法。我们首先总结了这些方法的四个要求，并提出了一种新的基于扩散的带有像素级指导的图像编辑框架，以满足这些要求。具体而言，我们使用少量带注释数据训练像素分类器，然后推断目标图像的分割映射。然后用户通过操作映射来指导编辑操作。我们利用预训练的扩散模型以像素级指导生成与用户意图相关联的已编辑图像。所提出的指导和其他技术的有效组合实现了高度可控的编辑，并保留编辑区域之外，从而满足我们的要求。实验结果表明，我们的方法在编辑质量和速度方面均优于基于GAN的方法。

    Our goal is to develop fine-grained real-image editing methods suitable for real-world applications. In this paper, we first summarize four requirements for these methods and propose a novel diffusion-based image editing framework with pixel-wise guidance that satisfies these requirements. Specifically, we train pixel-classifiers with a few annotated data and then infer the segmentation map of a target image. Users then manipulate the map to instruct how the image will be edited. We utilize a pre-trained diffusion model to generate edited images aligned with the user's intention with pixel-wise guidance. The effective combination of proposed guidance and other techniques enables highly controllable editing with preserving the outside of the edited area, which results in meeting our requirements. The experimental results demonstrate that our proposal outperforms the GAN-based method for editing quality and speed.
    
[^205]: 多目标多臂赌博机中的Pareto后悔分析

    Pareto Regret Analyses in Multi-objective Multi-armed Bandit. (arXiv:2212.00884v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00884](http://arxiv.org/abs/2212.00884)

    本文研究了多目标多臂赌博机中的Pareto最优性，提出了对抗性多目标多臂赌博机的表述和定义了Pareto后悔，提出了新算法，分析证明算法在对抗性环境最优，在随机环境中也接近最优，并将对抗性攻击机制从赌徒推广到多目标领域。

    

    本文研究了多目标多臂赌博机中的Pareto最优性。通过提出对抗性多目标多臂赌博机的表述并定义其Pareto后悔，可以应用于随机和对抗性环境。这些后悔不依赖于任何标量化函数，并反映了与标量化后悔相比的Pareto最优性。同时，我们提出了在有和无先验信息的多目标多臂赌博机环境下的新算法。通过我们对Pareto后悔的上下界分析证明，在对抗性环境中算法是最优的，在随机环境中也接近最优。此外，下界分析表明，新的后悔与随机环境下的现有Pareto后悔一致，并将对抗性攻击机制从赌徒推广到了多目标领域。

    We study Pareto optimality in multi-objective multi-armed bandit by providing a formulation of adversarial multi-objective multi-armed bandit and defining its Pareto regrets that can be applied to both stochastic and adversarial settings. The regrets do not rely on any scalarization functions and reflect Pareto optimality compared to scalarized regrets. We also present new algorithms assuming both with and without prior information of the multi-objective multi-armed bandit setting. The algorithms are shown optimal in adversarial settings and nearly optimal up to a logarithmic factor in stochastic settings simultaneously by our established upper bounds and lower bounds on Pareto regrets. Moreover, the lower bound analyses show that the new regrets are consistent with the existing Pareto regret for stochastic settings and extend an adversarial attack mechanism from bandit to the multi-objective one.
    
[^206]: 基础模型的能力探究

    On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16327](http://arxiv.org/abs/2211.16327)

    本文通过范畴论探究了基础模型的能力，提出了具有最小所需能力的基础模型可以通过微调和足够的资源来解决前置任务所定义的类别中的下游任务，并且这种能力可以扩展到任何下游任务，只要允许微调且下游任务可在前置任务定义的范畴中表示。

    

    如果基础模型具有无限高质量的数据点、无限计算能力、一个无限大的完美训练算法、以及在预设任务上保证零泛化误差，那么它可以用于一切吗？传统的表示、优化或泛化理论无法回答这个问题，因为它们主要探讨的问题在这里都是不存在的。本文提出范畴论提供了强大的理论工具，以回答这个问题。我们证明了三个结果，第一个限制了基于提示的学习的能力，即仅当任务可表示时，模型才能用提示解决下游任务；第二个结果表明，微调不受这个限制，因为一个具有最小所需能力（对称性）的基础模型可以通过微调和足够的资源来理论上解决前置任务所定义的类别中的下游任务。我们的最终结果可以看作是第二个结果的一般化，表明如果允许微调并且下游任务可在前置任务定义的范畴中表示，则基础模型的最小能力也足以解决任何下游任务。

    With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
    
[^207]: 使用Ollivier-Ricci曲率重新审视过度平滑和过度压缩问题

    Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature. (arXiv:2211.15779v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15779](http://arxiv.org/abs/2211.15779)

    研究发现图神经网络模型存在过度平滑和过度压缩问题，这一问题与图形曲率相关，作者利用Ollivier-Ricci曲率提出了Batch Ollivier-Ricci Flow算法，解决了这一问题。

    

    图神经网络(GNN)天生容易出现过度平滑和过度压缩问题。这些问题限制了GNN在进行远距离信息处理时对复杂图形相互作用建模的效力。我们的研究揭示了局部图形空间和这两个问题之间的关键联系，从而提供了一个统一的框架来使用Ollivier-Ricci曲率在局部尺度上研究它们。具体来说，我们证明过度平滑与正图曲率相联系，而过度压缩则与负图曲率相联系。基于我们的理论，我们提出了Batch Ollivier-Ricci Flow，这是一种新颖的重连算法，能够同时解决过度平滑和过度压缩问题。

    Graph Neural Networks (GNNs) had been demonstrated to be inherently susceptible to the problems of over-smoothing and over-squashing. These issues prohibit the ability of GNNs to model complex graph interactions by limiting their effectiveness in taking into account distant information. Our study reveals the key connection between the local graph geometry and the occurrence of both of these issues, thereby providing a unified framework for studying them at a local scale using the Ollivier-Ricci curvature. Specifically, we demonstrate that over-smoothing is linked to positive graph curvature while over-squashing is linked to negative graph curvature. Based on our theory, we propose the Batch Ollivier-Ricci Flow, a novel rewiring algorithm capable of simultaneously addressing both over-smoothing and over-squashing.
    
[^208]: 用深度均衡模型统一标签输入图神经网络

    Unifying Label-inputted Graph Neural Networks with Deep Equilibrium Models. (arXiv:2211.10629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10629](http://arxiv.org/abs/2211.10629)

    本文将标签输入的GNN和隐式GNN统一起来，并提出了一种IGNN中的隐式微分方法，使得标签无限传播变得可行。

    

    图神经网络在学习非欧几里得数据方面的成功引发了许多子课题，例如标签输入的GNN(LGNN)和隐式GNN(IGNN)。本文通过将LGNN解释为IGNN理论并将流行的LGNN归约为IGNN的形式来统一这两个子域。该统一简化了两个子域之间的交流并启发了更多研究。具体来说，介绍了IGNN的隐式微分到LGNN中，以常数内存微分其无限范围的标签传播，使传播成为可行的选择。

    The success of Graph Neural Networks (GNN) in learning on non-Euclidean data arouses many subtopics, such as Label-inputted GNN (LGNN) and Implicit GNN (IGNN). LGNN, explicitly inputting supervising information (a.k.a. labels) in GNN, integrates label propagation to achieve superior performance, but with the dilemma between its propagating distance and adaptiveness. IGNN, outputting an equilibrium point by iterating its network infinite times, exploits information in the entire graph to capture long-range dependencies, but with its network constrained to guarantee the existence of the equilibrium. This work unifies the two subdomains by interpreting LGNN in the theory of IGNN and reducing prevailing LGNNs to the form of IGNN. The unification facilitates the exchange between the two subdomains and inspires more studies. Specifically, implicit differentiation of IGNN is introduced to LGNN to differentiate its infinite-range label propagation with constant memory, making the propagation b
    
[^209]: PAD-Net：用于动态网络的高效框架

    PAD-Net: An Efficient Framework for Dynamic Networks. (arXiv:2211.05528v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05528](http://arxiv.org/abs/2211.05528)

    PAD-Net是一个部分动态网络的框架，将冗余的动态参数转换为静态参数，提高了动态网络的效率和适用性。

    

    动态网络，例如动态卷积（DY-Conv）和专家混合模型（MoE），已被广泛探索，因为它们可以显着提高模型的表示能力，并具有可接受的计算成本。实现动态网络的常见做法是将给定的静态层转换为完全动态的层，其中所有参数都是动态的（至少在单个层内）并随输入变化。但是，这种完全动态的设置可能会导致冗余参数和高部署成本，从而限制了动态网络在更广泛的任务和模型中的适用性。我们工作的主要贡献是挑战动态网络的基本常识，并提出部分动态网络，即PAD-Net，以将冗余动态参数转换为静态参数。此外，我们进一步设计迭代模式分区来有效地分区动态和静态参数。我们的方法受到大规模实验的全面支持。

    Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments wi
    
[^210]: 不需要重建的预测自组织众智系统的局部行为

    Forecasting Local Behavior of Self-organizing Many-agent System without Reconstruction. (arXiv:2210.17289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17289](http://arxiv.org/abs/2210.17289)

    本文提出了一种CNN-LSTM模型，可以在不需要重建所有代理状态的情况下，预测自组织多代理系统中特定代理的状态。所提出的模型在森林火灾模型中的实验中表现出更好的性能，可以提高自组织众智系统的效率和可扩展性。

    

    大型多代理系统通常由局部定义的代理交互驱动，即自组织。本文的首要目标是确定这种局部交互扩散到特定感兴趣代理的时间。虽然可以使用重建所有代理状态的传统方法，但可能会带来不必要的计算成本。我们研究了一种CNN-LSTM模型，可以在不重建的情况下预测大型自组织多代理系统中特定代理的状态。所提出的模型包括CNN编码器以低维度向量表示系统，LSTM模块学习向量空间中的代理动态，并预测未来代理状态的MLP解码器。我们以森林火灾模型为例，旨在预测特定树代理何时开始燃烧。我们将所提出的模型与重建型方法（如CNN-LSTM和ConvLSTM）进行比较。所提出的模型表现出更好的性能，可以预测局部代理的行为而无需重建所有代理状态，从而实现更高效和可扩展的自组织众智系统应用。

    Large multi-agent systems are often driven by locally defined agent interactions, which is referred to as self-organization. Our primary objective is to determine when the propagation of such local interactions will reach a specific agent of interest. Although conventional approaches that reconstruct all agent states can be used, they may entail unnecessary computational costs. In this paper, we investigate a CNN-LSTM model to forecast the state of a particular agent in a large self-organizing multi-agent system without the reconstruction. The proposed model comprises a CNN encoder to represent the system in a low-dimensional vector, a LSTM module to learn agent dynamics in the vector space, and a MLP decoder to predict the future state of an agent. As an example, we consider a forest fire model where we aim to predict when a particular tree agent will start burning. We compare the proposed model with reconstruction-based approaches such as CNN-LSTM and ConvLSTM. The proposed model exh
    
[^211]: GradSkip：具有更好计算复杂度的通信加速局部梯度方法

    GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity. (arXiv:2210.16402v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16402](http://arxiv.org/abs/2210.16402)

    本文研究了一类分布式优化算法，通过允许具有“次要”数据的客户端在本地执行较少的训练步骤来减轻高通信成本，这一方法可在强凸区域内实现可证明的通信加速。

    

    我们研究了一类分布式优化算法，旨在通过允许客户端在通信之前执行多个本地梯度类型的训练步骤来减轻高通信成本。虽然这种方法已经研究了约十年，但本地训练的加速性质在理论上还未得到完全解释。最近，Mishchenko等人(2022 International Conference on Machine Learning)取得了重大突破，证明了当本地训练得到正确执行时，会导致可证明的通信加速，在强凸区域内这一点成立，而且不依赖于任何数据相似性假设。然而，他们的方法ProxSkip要求所有客户端在每次通信轮中执行相同数量的本地训练步骤。灵感来自常识的直觉，我们通过猜测认为拥有“次要”数据的客户端应该能够用较少的本地训练步骤就能完成，而不影响整体通信

    We study a class of distributed optimization algorithms that aim to alleviate high communication costs by allowing the clients to perform multiple local gradient-type training steps prior to communication. While methods of this type have been studied for about a decade, the empirically observed acceleration properties of local training eluded all attempts at theoretical understanding. In a recent breakthrough, Mishchenko et al. (ICML 2022) proved that local training, when properly executed, leads to provable communication acceleration, and this holds in the strongly convex regime without relying on any data similarity assumptions. However, their method ProxSkip requires all clients to take the same number of local training steps in each communication round. Inspired by a common sense intuition, we start our investigation by conjecturing that clients with ``less important'' data should be able to get away with fewer local training steps without this impacting the overall communication c
    
[^212]: E-MCTS：通过规划表观不确定性进行深度探索的模型基强化学习

    E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13455](http://arxiv.org/abs/2210.13455)

    本文提出了一种新的方法E-MCTS，通过在MCTS预测中应用表观不确定性估计，实现了模型基强化学习中的深度探索，以及规划探索策略。通过实验证明这种方法在成功的表观不确定性估计和深度探索方面表现优异。

    

    模拟退火树搜索（MCTS）是模型基强化学习中应用最广泛、性能最优秀的规划方法之一。MCTS的关键挑战在于深度探索和面对未知时的可靠性，这两个挑战可以通过在MCTS预测中使用原则性的表观不确定性估计来缓解。本文提出了两个主要贡献：首先，我们开发了一种在MCTS中传播表观不确定性的方法，使智能体能够估计其预测的表观不确定性。其次，我们利用传播的不确定性提出了一种新的深度探索算法，通过明确规划探索策略。我们将这种方法应用于基于MCTS的模型基强化学习方法中，包括使用学习和提供的模型，通过实验证明了我们的方法实现了成功的表观不确定性估计并进行了深度探索。我们将其与基于非规划的深度探索基线进行了比较，并表明...

    One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
    
[^213]: CAP: 针对高度准确的稀疏视觉模型的相关性感知剪枝

    CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models. (arXiv:2210.09223v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09223](http://arxiv.org/abs/2210.09223)

    该论文介绍了一种新的 Correlation Aware Pruner (CAP) 框架，能够在处理高度准确的稀疏视觉模型剪枝方面推动压缩界限，并且能够通过有效的紧缩恢复过程实现压缩后的性能提升。

    

    在架构设计和训练流程显著改进的推动下，计算机视觉在 ImageNet 等经典基准测试上取得了显著的准确性提高。然而，这些高度准确的模型部署具有挑战性，因为它们似乎更难使用标准的压缩技术（例如剪枝技术）进行压缩。我们通过引入基于两个技术进展的 Correlation Aware Pruner (CAP) 来解决这个问题：一种新的理论上被证明的剪枝器，可以在剪枝过程中精确、有效地处理复杂的权重相关性；以及一种用于压缩后恢复的高效微调过程。我们通过对多个现代视觉模型（如 Vision Transformers (ViT)、现代 CNN 和 ViT-CNN 混合模型）进行大量实验验证了我们的方法，首次展示了这些模型可以进行高效压缩。

    Driven by significant improvements in architectural design and training pipelines, computer vision has recently experienced dramatic progress in terms of accuracy on classic benchmarks such as ImageNet. These highly-accurate models are challenging to deploy, as they appear harder to compress using standard techniques such as pruning. We address this issue by introducing the Correlation Aware Pruner (CAP), a new unstructured pruning framework which significantly pushes the compressibility limits for state-of-the-art architectures. Our method is based on two technical advancements: a new theoretically-justified pruner, which can handle complex weight correlations accurately and efficiently during the pruning process itself, and an efficient finetuning procedure for post-compression recovery. We validate our approach via extensive experiments on several modern vision models such as Vision Transformers (ViT), modern CNNs, and ViT-CNN hybrids, showing for the first time that these can be pr
    
[^214]: RARR: 使用语言模型研究和修正其输出结果中的不确定信息

    RARR: Researching and Revising What Language Models Say, Using Language Models. (arXiv:2210.08726v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.08726](http://arxiv.org/abs/2210.08726)

    RARR是一个可以对不确定信息进行研究和修订的系统，它可以自动找到文本生成模型输出的归因并修正不支持的内容。

    

    现在的语言模型在诸如少样本学习、问答、推理和对话等许多任务上表现出色。然而，它们有时会生成无支持或误导性的内容。由于大多数语言模型没有任何内置的归因外部证据的机制，用户很难确定它们的输出是否可靠。为了在保留最新一代模型的所有强大优势的同时实现归因，我们提出了 RARR (使用研究和修订进行改进归因)系统，它 1) 自动找到任何文本生成模型输出的归因并 2) 在尽可能保留原始输出的同时，修正不支持的内容。当应用于几个最先进的语言模型在各种输出任务上的结果时，我们发现RARR在显著提高归因率的同时，比以前探索的编辑模型更能保留原始输入。

    Language models (LMs) now excel at many tasks such as few-shot learning, question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, 
    
[^215]: 分层策略混合作为反应式机器人控制的推理

    Hierarchical Policy Blending as Inference for Reactive Robot Control. (arXiv:2210.07890v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.07890](http://arxiv.org/abs/2210.07890)

    该论文提出了一种分层运动生成的方法，结合了反应式策略和规划的优点，在多目标决策问题中提供了可行的路径。

    

    在杂乱、密集和动态的环境中进行运动生成是机器人领域的一个核心问题，被视为一个多目标决策问题。当前的方法在安全和性能之间进行权衡。一方面，反应式策略保证了对环境变化的快速响应，但以次优的行为作为代价。另一方面，基于规划的运动生成提供可行的轨迹，但高计算成本可能会限制控制频率，从而牺牲安全性。为了结合反应式策略和规划的优点，我们提出了一种分层运动生成方法。此外，我们采用概率推理方法来正式化分层模型和随机优化。我们将这种方法实现为随机反应式专家策略的加权乘积，其中规划被用于自适应计算任务周期内的最优权重。这种随机优化避免了局部最优，并提出了可行的反应式计划，找到路径。

    Motion generation in cluttered, dense, and dynamic environments is a central topic in robotics, rendered as a multi-objective decision-making problem. Current approaches trade-off between safety and performance. On the one hand, reactive policies guarantee fast response to environmental changes at the risk of suboptimal behavior. On the other hand, planning-based motion generation provides feasible trajectories, but the high computational cost may limit the control frequency and thus safety. To combine the benefits of reactive policies and planning, we propose a hierarchical motion generation method. Moreover, we adopt probabilistic inference methods to formalize the hierarchical model and stochastic optimization. We realize this approach as a weighted product of stochastic, reactive expert policies, where planning is used to adaptively compute the optimal weights over the task horizon. This stochastic optimization avoids local optima and proposes feasible reactive plans that find path
    
[^216]: 《一次性任务泛化的抽象到可执行轨迹翻译》

    Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization. (arXiv:2210.07658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07658](http://arxiv.org/abs/2210.07658)

    该论文提出了一种解决长时间跨度机器人策略泛化问题的方法，通过把计划生成和计划执行分开，解决领域差异的问题，并将其分为建立抽象环境、生成抽象轨迹和通过翻译器解决原始任务三个步骤。

    

    在复杂物理环境中训练长时间跨度的机器人策略对于许多应用非常重要，比如机器人操作。然而，学习可以泛化到未知任务的策略是具有挑战性的。在这项工作中，我们提出通过拆分计划生成和计划执行来实现一次性任务泛化。具体而言，我们的方法分为三步解决复杂的长时间跨度任务：通过简化几何和物理学建立成对的抽象环境，生成抽象轨迹，然后通过抽象到可执行轨迹翻译器来解决原始任务。在抽象环境中，去除了诸如物理操纵等复杂动力学，使得抽象轨迹更容易生成。然而，这引入了大的领域差异，因为抽象轨迹缺乏低级细节并且没有与被执行的轨迹一一对应。类似于语言翻译的方式，

    Training long-horizon robotic policies in complex physical environments is essential for many applications, such as robotic manipulation. However, learning a policy that can generalize to unseen tasks is challenging. In this work, we propose to achieve one-shot task generalization by decoupling plan generation and plan execution. Specifically, our method solves complex long-horizon tasks in three steps: build a paired abstract environment by simplifying geometry and physics, generate abstract trajectories, and solve the original task by an abstract-to-executable trajectory translator. In the abstract environment, complex dynamics such as physical manipulation are removed, making abstract trajectories easier to generate. However, this introduces a large domain gap between abstract trajectories and the actual executed trajectories as abstract trajectories lack low-level details and are not aligned frame-to-frame with the executed trajectory. In a manner reminiscent of language translatio
    
[^217]: 如何在数据污染的情况下筛选出干净的数据子集？

    How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?. (arXiv:2210.06516v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.06516](http://arxiv.org/abs/2210.06516)

    外部数据可能引入攻击者篡改的有毒数据，为了提高毒性防御性能，需要准确地从数据集中筛选出干净的子集。

    

    鉴于现代机器学习模型所需的大量数据，越来越多地使用外部供应商。然而，合并外部数据会带来数据污染的风险，攻击者可以操纵他们的数据以降低模型的效用或完整性。大多数毒化防御都假定可以访问一组干净的数据（或基础集）。然而，鉴于隐蔽性毒化攻击的快速增长研究，一个问题出现了：防御者真的能够在被污染的数据集中确定一个干净的子集以支持防御吗？本文从研究有毒样本错误地混入基础集后对防御的影响开始。我们分析了五种防御方法并发现它们的性能会在基础集中污染点少于1％时急剧下降。这些发现表明，在这些防御的性能方面，精确地筛选出一个基础集是关键。受这些观察的启发，我们研究了如何精确确定现有的自动化方法，以在污染数据中鉴别一个干净的子集。

    Given the volume of data needed to train modern machine learning models, external suppliers are increasingly used. However, incorporating external data poses data poisoning risks, wherein attackers manipulate their data to degrade model utility or integrity. Most poisoning defenses presume access to a set of clean data (or base set). While this assumption has been taken for granted, given the fast-growing research on stealthy poisoning attacks, a question arises: can defenders really identify a clean subset within a contaminated dataset to support defenses?  This paper starts by examining the impact of poisoned samples on defenses when they are mistakenly mixed into the base set. We analyze five defenses and find that their performance deteriorates dramatically with less than 1% poisoned points in the base set. These findings suggest that sifting out a base set with high precision is key to these defenses' performance. Motivated by these observations, we study how precise existing auto
    
[^218]: 采样效率更高的NLP模型更加鲁棒吗？

    Are Sample-Efficient NLP Models More Robust?. (arXiv:2210.06456v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06456](http://arxiv.org/abs/2210.06456)

    较低采样效率的NLP模型在特定情况下可能比较高采样效率的模型更为鲁棒，表明通用的提高采样效率方法不太可能改善自然语言处理中的OOD鲁棒性。

    

    最近在图像分类和抽取式问答中的研究表明，预训练模型在更少的内部数据上训练可以获得更好的外部评测性能。然而，这些趋势的普适性还不清楚。在三个任务、三个广泛适用的建模干预（增加模型大小、使用不同的适应方法和在更多数据上进行预训练）和14个不同数据集上，我们进行了大规模的实证研究，以研究样本效率（达到给定ID准确度所需的数据量）和鲁棒性（模型在OOD评估中的表现）之间的关系。我们发现，较高的样本效率仅在某些建模干预和任务上与更好的平均OOD鲁棒性相关，而在其他情况下则不然。在个别数据集上，样本效率较低的模型甚至更为健壮。这些结果表明，提高样本效率的通用方法不太可能改善自然语言处理中的普遍OOD鲁棒性。

    Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-of-distribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks, three broadly-applicable modeling interventions (increasing model size, using a different adaptation method, and pre-training on more data), and 14 diverse datasets to investigate the relationship between sample efficiency (amount of data needed to reach a given ID accuracy) and robustness (how models fare on OOD evaluation). We find that higher sample efficiency is only correlated with better average OOD robustness on some modeling interventions and tasks, but not others. On individual datasets, models with lower sample efficiency can even be more robust. These results suggest that general-purpose methods for improving sample efficiency are unlikely to yield universal OOD robustness impro
    
[^219]: 变分开放领域问答

    Variational Open-Domain Question Answering. (arXiv:2210.06345v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06345](http://arxiv.org/abs/2210.06345)

    本文介绍了变分开放领域（VOD）框架，提出了一种新的自归一化的Rényi变分界的估计方法，可用于训练具有检索增强功能的模型，例如阅读器-检索器BERT-sized模型，并实现了在多项选择医学考试问题上的优异表现。

    

    检索增强模型在自然语言处理任务中已被证明是有效的，但是对它们进行变分推断的优化研究仍然不足。我们引入了变分开放领域（VOD）框架，用于检索增强模型的端到端训练和评估，重点放在开放领域问答和语言建模方面。VOD目标是一种自归一化的Rényi变分界的估计，近似于任务边缘似然，并在一个辅助采样分布（缓存的检索器和/或近似后验）中进行样本抽取评估。它仍然是可行的，即使是在对大量语料库定义的检索器分布下。我们通过训练针对多项选择医学考试问题的阅读器-检索器BERT-sized模型，展示了VOD的多功能性。在MedMCQA数据集上，我们超过了领域微调的Med-PaLM 5.3％，尽管使用的参数少了2500倍。我们的检索增强BioLinkBERT模型得分为62.9％。

    Retrieval-augmented models have proven to be effective in natural language processing tasks, yet there remains a lack of research on their optimization using variational inference. We introduce the Variational Open-Domain (VOD) framework for end-to-end training and evaluation of retrieval-augmented models, focusing on open-domain question answering and language modelling. The VOD objective, a self-normalized estimate of the R\'enyi variational bound, approximates the task marginal likelihood and is evaluated under samples drawn from an auxiliary sampling distribution (cached retriever and/or approximate posterior). It remains tractable, even for retriever distributions defined on large corpora. We demonstrate VOD's versatility by training reader-retriever BERT-sized models on multiple-choice medical exam questions. On the MedMCQA dataset, we outperform the domain-tuned Med-PaLM by +5.3% despite using 2.500$\times$ fewer parameters. Our retrieval-augmented BioLinkBERT model scored 62.9%
    
[^220]: 关于神经ODE的正向不变性

    On the Forward Invariance of Neural ODEs. (arXiv:2210.04763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04763](http://arxiv.org/abs/2210.04763)

    该论文提出了一种利用控制障碍函数使神经ODE满足输出规范的方法，该方法保证了输出规范，且在训练和推理过程中可以通过改变受限参数/输入进行操作，此外，该方法还创造了额外的鲁棒性。

    

    我们提出了一种新的方法，通过使用不变集传播来确保神经常微分方程(ODE)满足输出规范。 我们的方法使用一类控制障碍函数将输出规范转换为对学习系统的参数和输入的约束条件。 这个设置使我们能够通过在训练和推理过程中改变受限参数/输入来实现输出规范保证。 此外，我们证明我们通过数据控制的神经ODE的不变集传播不仅保持了概括性能，而且通过启用对系统参数/输入的因果操作创造了额外的鲁棒性。 我们在一系列表示学习任务上测试了我们的方法，包括物理动力学和凸性画像，以及自主车辆的安全避碰。

    We propose a new method to ensure neural ordinary differential equations (ODEs) satisfy output specifications by using invariance set propagation. Our approach uses a class of control barrier functions to transform output specifications into constraints on the parameters and inputs of the learning system. This setup allows us to achieve output specification guarantees simply by changing the constrained parameters/inputs both during training and inference. Moreover, we demonstrate that our invariance set propagation through data-controlled neural ODEs not only maintains generalization performance but also creates an additional degree of robustness by enabling causal manipulation of the system's parameters/inputs. We test our method on a series of representation learning tasks, including modeling physical dynamics and convexity portraits, as well as safe collision avoidance for autonomous vehicles.
    
[^221]: 为什么随机剪枝足以启动稀疏神经网络

    Why Random Pruning Is All We Need to Start Sparse. (arXiv:2210.02412v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02412](http://arxiv.org/abs/2210.02412)

    本文发现，如果随机掩码的宽度比稀疏性的倒数的对数因子大，则它们可以近似任意目标网络，因此随机剪枝足以启动稀疏神经网络，而且任何从密集到稀疏的训练方案都可以转化为计算上更有效的从稀疏到稀疏的训练方案。

    

    实验证明，随机剪枝可以定义出非常有效的稀疏神经网络模型。得到的稀疏网络通常可以与密集结构和最先进的“中彩票”剪枝算法竞争，尽管它们不依赖于计算昂贵的剪枝-训练迭代，并且可以在初始阶段绘制而不需要过多的计算开销。本文提供了一个理论解释，即如果随机掩码的宽度比稀疏性的倒数的对数因子大，则它们可以近似任意目标网络。这种超过参数化因子至少对于三层随机网络是必要的，这解释了随机网络在更高的稀疏度下观察到的性能下降。然而，在中等到高稀疏度水平下，我们的结果暗示着更稀疏的网络包含在随机的源网络中，因此任何从密集到稀疏的训练方案都可以转化为计算上更有效的从稀疏到稀疏的训练方案。

    Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity $1 / \log(1/\text{sparsity})$. This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one
    
[^222]: 针对时变偏微分方程的隐式神经空间表示

    Implicit Neural Spatial Representations for Time-dependent PDEs. (arXiv:2210.00124v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00124](http://arxiv.org/abs/2210.00124)

    本文探讨了使用隐式神经空间表示作为空间离散化的方法来求解时变偏微分方程，该方法不需要任何现有求解器生成的训练数据，具有高精度和稳定性，同时在内存使用方面实现了显著的节省。

    

    隐式神经空间表示 (INSR) 已被证明是空间依赖向量场的有效表示方法。本文探讨了使用INSR求解时变偏微分方程。传统的PDE求解器引入了时间和空间离散化。常见的空间离散化包括网格和无网格点云，其中每个自由度对应于空间中的一个位置。虽然这些显式空间对应关系可以直观地建模和理解，但这些表示方法不一定是最优的，对于精度、内存使用和适应性都存在问题。在保持经典的时间离散化不变的情况下（如显式/隐式Euler），我们探讨了INSR作为替代空间离散化的方法，其中空间信息隐含在神经网络权重中。然后通过时间积分，网络权重随时间演化。我们的方法不需要任何现有求解器生成的训练数据，因为我们的方法本身就是求解器。我们在各种基准测试问题上验证了我们的方法，证明了其高精度和稳定性，同时在内存使用方面实现了显著的节省。

    Implicit Neural Spatial Representation (INSR) has emerged as an effective representation of spatially-dependent vector fields. This work explores solving time-dependent PDEs with INSR. Classical PDE solvers introduce both temporal and spatial discretizations. Common spatial discretizations include meshes and meshless point clouds, where each degree-of-freedom corresponds to a location in space. While these explicit spatial correspondences are intuitive to model and understand, these representations are not necessarily optimal for accuracy, memory usage, or adaptivity. Keeping the classical temporal discretization unchanged (e.g., explicit/implicit Euler), we explore INSR as an alternative spatial discretization, where spatial information is implicitly stored in the neural network weights. The network weights then evolve over time via time integration. Our approach does not require any training data generated by existing solvers because our approach is the solver itself. We validate our
    
[^223]: 多尺度拓扑奇异性检测

    Topological Singularity Detection at Multiple Scales. (arXiv:2210.00069v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00069](http://arxiv.org/abs/2210.00069)

    本文提出了一种多尺度拓扑奇异性检测方法，可以评估数据的局部固有维度，并量化点的“流形度”，能够检测复杂空间和图像中的奇异性。

    

    流形假设是现代机器学习研究的一个基本假设，它假定数据位于或接近于低固有维度的未知流形上。然而，最近的研究表明，现实世界的数据表现出明显的非流形结构，即奇异性，这可能导致错误的发现。因此，检测这种奇异性在插值和推断任务之前是至关重要的。我们通过开发一个拓扑框架来解决这个问题，该框架能够（i）量化局部固有维度，以及（ii）在多个尺度上产生“欧几里得性”评分，用以评估点的“流形度”。我们的方法可以在图像数据中捕获复杂空间的奇异性，同时捕捉奇异结构和局部几何复杂性。

    The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.
    
[^224]: 通过上下文反应实现分子表示融合的反合成规划技术

    FusionRetro: Molecule Representation Fusion via In-context Reactions for Retrosynthetic Planning. (arXiv:2209.15315v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15315](http://arxiv.org/abs/2209.15315)

    本文提出了一种新的反合成规划框架，能够利用上下文信息来改善预测准确性，从而实现更实用和准确的反合成规划结果。

    

    反合成规划的目标是从起始物质到目标分子设计出一个完整的多步合成路线。当前策略采用一种解耦的方法，即单步反合成模型和搜索算法，只将产物作为输入来预测每个规划步骤的反应物，并忽略了沿着合成路线的有价值的上下文信息。本文提出了一种新颖的框架，利用上下文信息来改善反合成规划。我们将合成路线视为反应图，并提出通过三个原则步骤来整合信息和预测反应物。我们的方法是利用上下文反应进行反合成规划的首次尝试。整个框架可以高效地端到端地优化，产生更实用和准确的预测。全面的实验证明，通过在上下文中融合分子表示，我们的方法可以取得更好的反合成规划结果。

    Retrosynthetic planning aims to devise a complete multi-step synthetic route from starting materials to a target molecule. Current strategies use a decoupled approach of single-step retrosynthesis models and search algorithms, taking only the product as the input to predict the reactants for each planning step and ignoring valuable context information along the synthetic route. In this work, we propose a novel framework that utilizes context information for improved retrosynthetic planning. We view synthetic routes as reaction graphs and propose to incorporate context through three principled steps: encode molecules into embeddings, aggregate information over routes, and readout to predict reactants. Our approach is the first attempt to utilize in-context reactions for retrosynthetic planning. The entire framework can be efficiently optimized in an end-to-end fashion and produce more practical and accurate predictions. Comprehensive experiments demonstrate that by fusing in the context
    
[^225]: 重新思考反事实解释：作为局部和区域反事实政策的反事实解释

    Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies. (arXiv:2209.14568v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.14568](http://arxiv.org/abs/2209.14568)

    本文提出了一种概率框架，为每个观测值提供稀疏的局部反事实规则，并将这些规则聚合成区域反事实规则，以适应不稳定的实现环境，并产生稳健的救济措施。

    

    反事实解释（CE）面临着许多未解决的挑战，如确保稳定性、综合多个CE以及提供合理性和稀疏性保证。从更实际的角度来看，最近的研究表明，所规定的反事实救济措施通常不会被个体完全实施，并证明大多数最先进的CE算法在这种嘈杂的环境中很可能失败。为了解决这些问题，我们提出了一个概率框架，为每个观测值提供稀疏的局部反事实规则，提供能够以高概率改变决策的值范围的规则。这些规则作为多样的反事实解释的总结，并产生稳健的救济措施。我们进一步将这些局部规则聚合成区域反事实规则，识别数据子组的共享救济措施。我们的局部和区域规则来自于随机森林算法。

    Counterfactual Explanations (CE) face several unresolved challenges, such as ensuring stability, synthesizing multiple CEs, and providing plausibility and sparsity guarantees. From a more practical point of view, recent studies [Pawelczyk et al., 2022] show that the prescribed counterfactual recourses are often not implemented exactly by individuals and demonstrate that most state-of-the-art CE algorithms are very likely to fail in this noisy environment. To address these issues, we propose a probabilistic framework that gives a sparse local counterfactual rule for each observation, providing rules that give a range of values capable of changing decisions with high probability. These rules serve as a summary of diverse counterfactual explanations and yield robust recourses. We further aggregate these local rules into a regional counterfactual rule, identifying shared recourses for subgroups of the data. Our local and regional rules are derived from the Random Forest algorithm, which of
    
[^226]: 基于赫比学习的游戏智能体群集演化预测研究

    Forecasting Evolution of Clusters in Game Agents with Hebbian Learning. (arXiv:2209.06904v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2209.06904](http://arxiv.org/abs/2209.06904)

    本文研究了如何通过聚类和预测模型来学习游戏智能体群集的演化，提出了一种基于 Hebbian 学习的无监督聚类方法，结合 LSTM 预测模型，能够在多个场景下准确预测群集演化。

    

    大型多智能体系统例如实时策略游戏通常被智能体的集体行为所驱动。例如在星际争霸II中，人类玩家会将空间接近的智能体分组成团队，并控制团队击败敌人。在这种情况下，将游戏中的智能体进行聚类已被用于多智能体强化学习的高效控制以及提供给游戏用户的游戏分析工具等等多个方面。然而，尽管聚类提供了有用的信息，但是在研究多智能体系统在群集级别上的动态学习方面还很少被研究。在本文中，我们提出了一种混合型AI模型，将无监督学习和自监督学习相结合，来预测星际争霸II中群集的演化。我们开发了一种无监督的 Hebbian 学习方法，用于在 Set-to-Cluster 模块中高效地创建可变数量的群集，其推理时间复杂度低于 K-means 聚类。同时，采用了基于长短期记忆（LSTM）的预测模型学习群集的复杂动态，并预测未来群集的归属。通过实验结果，我们证明了所提出的方法优于现有的聚类方法，并能够在游戏的多个场景下准确预测群集演化。

    Large multi-agent systems such as real-time strategy games are often driven by collective behavior of agents. For example, in StarCraft II, human players group spatially near agents into a team and control the team to defeat opponents. In this light, clustering the agents in the game has been used for various purposes such as the efficient control of the agents in multi-agent reinforcement learning and game analytic tools for the game users. However, despite the useful information provided by clustering, learning the dynamics of multi-agent systems at a cluster level has been rarely studied yet. In this paper, we present a hybrid AI model that couples unsupervised and self-supervised learning to forecast evolution of the clusters in StarCraft II. We develop an unsupervised Hebbian learning method in a set-to-cluster module to efficiently create a variable number of the clusters with lower inference time complexity than K-means clustering. Also, a long short-term memory based prediction
    
[^227]: 多维时空数据的贝叶斯互补核学习

    Bayesian Complementary Kernelized Learning for Multidimensional Spatiotemporal Data. (arXiv:2208.09978v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.09978](http://arxiv.org/abs/2208.09978)

    本文提出了一种贝叶斯互补核学习（BCKL）框架，它将核化低秩张量分解和短程时空高斯过程相结合，可有效地建模多维时空数据的复杂相关性。

    

    多维时空数据的概率建模对许多现实应用至关重要。由于现实世界中的时空数据往往表现出非平稳和非可分离的复杂依赖关系，因此开发有效且计算效率高的统计模型以适应同时包含长程和短程变化的非稳态/不可分离过程变得具有挑战性，尤其是对于具有不同破坏/缺失结构的大规模数据集。在本文中，我们提出了一种新的统计框架 - 贝叶斯互补核学习（BCKL） - 用于实现多维时空数据的可扩展概率建模。为了有效地表征复杂依赖关系，BCKL集成了两个互补方法——核低秩张量分解和短程时空高斯过程。具体而言，我们使用多线性低秩因子分解组件来捕获全局/长程相关性，并使用短程时空高斯过程来捕获局部/短程相关性。

    Probabilistic modeling of multidimensional spatiotemporal data is critical to many real-world applications. As real-world spatiotemporal data often exhibits complex dependencies that are nonstationary and nonseparable, developing effective and computationally efficient statistical models to accommodate nonstationary/nonseparable processes containing both long-range and short-scale variations becomes a challenging task, in particular for large-scale datasets with various corruption/missing structures. In this paper, we propose a new statistical framework -- Bayesian Complementary Kernelized Learning (BCKL) -to achieve scalable probabilistic modeling for multidimensional spatiotemporal data. To effectively characterize complex dependencies, BCKL integrates two complementary approaches -- kernelized low-rank tensor factorization and short-range spatiotemporal Gaussian Processes. Specifically, we use a multi-linear low-rank factorization component to capture the global/long-range correla
    
[^228]: 博弈树中的NEAR-OPTIMAL PHI-REGRET学习

    Near-Optimal $\Phi$-Regret Learning in Extensive-Form Games. (arXiv:2208.09747v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2208.09747](http://arxiv.org/abs/2208.09747)

    本文介绍了一种有效和解耦的学习动力学，在多人博弈中能够使每个玩家的触发后悔按$O(\log T)$增长，达到接近最优的收敛速度。并且构建利用了一个更为一般的由具有多项式次数的有理函数导出的不动点结果，以及一个凸包的更细致的后悔电路，保留了RVU合适性的属性。

    

    本文在多人完美回忆不完美信息博弈中建立了有效和解耦的学习动力学，以便每个玩家的触发后悔在T次游戏重复后按$O(\log T)$增长。这相对于先前已知的触发后悔边界$O(T^{1/4})$有指数级的改进，并解决了Bai等人（2022）提出的一个最近的开放问题。作为直接的结果，我们保证以接近最优的速度$\frac{\log T}{T}$收敛到广泛形式的相关均衡和粗略的相关均衡。在现有工作的基础上，我们的构建的核心是一个更为一般的由具有多项式次数的有理函数导出的不动点结果，这是我们为触发偏差函数（粗略的）固定点所建立的属性。此外，我们的构建利用了凸包的更细致的后悔电路，与先前的保证不同，它保留了RVU合适性的属性。

    In this paper, we establish efficient and uncoupled learning dynamics so that, when employed by all players in multiplayer perfect-recall imperfect-information extensive-form games, the trigger regret of each player grows as $O(\log T)$ after $T$ repetitions of play. This improves exponentially over the prior best known trigger-regret bound of $O(T^{1/4})$, and settles a recent open question by Bai et al. (2022). As an immediate consequence, we guarantee convergence to the set of extensive-form correlated equilibria and coarse correlated equilibria at a near-optimal rate of $\frac{\log T}{T}$.  Building on prior work, at the heart of our construction lies a more general result regarding fixed points deriving from rational functions with polynomial degree, a property that we establish for the fixed points of (coarse) trigger deviation functions. Moreover, our construction leverages a refined regret circuit for the convex hull, which -- unlike prior guarantees -- preserves the RVU proper
    
[^229]: ILLUME：通过人机交互来合理化视觉-语言模型

    ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08241](http://arxiv.org/abs/2208.08241)

    本文提出了一种新的调整范例，名为ILLUME，通过人机交互来合理化视觉-语言模型，从而使模型的输出更符合人的思维方式。在使用相对较少的训练数据和最少的人类反馈下，ILLUME表现出与标准监督微调相当的竞争力。

    

    基于预训练语言模型的引导已被证明是构建视觉-语言模型（VLM）的有效方法，可用于图像字幕或视觉问题回答等任务。然而，这些模型的输出很少与用户对特定答案的理性相一致。为了改善这种对齐并加强常识原因，我们提出了一种基于人机生成数据的调整范例。我们的ILLUME执行以下循环：给定一个图像-问题-答案提示，VLM样本多个候选原理，人类评论家通过偏好选择提供反馈，用于微调。这个循环增加了训练数据，并逐渐雕刻出与人类意图相一致的VLM的理性能力。我们的详尽实验表明，ILLUME在使用 significantly 更少的训练数据仅需要 minimal 反馈的同时，与标准监督微调具有竞争力。

    Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.
    
[^230]: 宽卷积神经网络能够学到什么？

    What Can Be Learnt With Wide Convolutional Neural Networks?. (arXiv:2208.01003v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.01003](http://arxiv.org/abs/2208.01003)

    本文研究在内核环境下的无限宽卷积神经网络，证明了深层CNN能够适应目标函数的空间尺度，即使数据没有局部结构，深层CNN也可以学习，只要全局结构可以被利用。

    

    理解卷积神经网络（CNN）如何高效地学习高维函数仍然是一个基本挑战。人们普遍认为，这些模型利用了自然数据（如图像）的局部和分层结构。然而，我们缺乏如此结构如何影响性能的量化理解，如泛化误差随训练样本数量的衰减速率。本文研究了在内核环境下的无限宽卷积神经网络，并展示了相应核的谱沿袭了网络的分层结构，并表征了其渐进性。然后，我们将这个结果与泛化误差界限结合起来，证明了深层CNN能够适应目标函数的空间尺度。特别是，我们发现，如果目标函数依赖于相邻输入变量的低维子集，则误差的衰减受到这些子集的有效维数的控制。相反，如果函数依赖于高维结构，则有效维数受网络宽度的影响。我们的结果表明，在过度参数化的情况下，即使数据没有局部结构，深层CNN也可以学习，只要全局结构可以被利用。

    Understanding how convolutional neural networks (CNNs) can efficiently learn high-dimensional functions remains a fundamental challenge. A popular belief is that these models harness the local and hierarchical structure of natural data such as images. Yet, we lack a quantitative understanding of how such structure affects performance, e.g., the rate of decay of the generalisation error with the number of training samples. In this paper, we study infinitely-wide deep CNNs in the kernel regime. First, we show that the spectrum of the corresponding kernel inherits the hierarchical structure of the network, and we characterise its asymptotics. Then, we use this result together with generalisation bounds to prove that deep CNNs adapt to the spatial scale of the target function. In particular, we find that if the target function depends on low-dimensional subsets of adjacent input variables, then the decay of the error is controlled by the effective dimensionality of these subsets. Conversel
    
[^231]: 多元长序列时间序列预测的通用记忆驱动变压器

    Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting. (arXiv:2207.07827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07827](http://arxiv.org/abs/2207.07827)

    本文提出了一种通用记忆驱动变压器，通过集成多个时间序列特征来驱动预测过程，逐步引入噪声以增强泛化能力，在多个数据集上实现了更优秀的预测性能。

    

    多元长序列时间序列预测(M-LSTF)是一个实际但具有挑战性的问题。与传统的时间序列预测任务不同，M-LSTF任务从两个方面更具挑战性：1) M-LSTF模型需要同时学习多个时间特征之间的时间序列模式；2)在滚动预测设置中，两个连续训练样本之间的相似度随着预测长度的增加而增加，这使得模型更易于过拟合。本文提出了一种通用记忆驱动变压器来解决M-LSTF问题。具体而言，我们首先提出了一个全局层面的记忆组件，通过集成多个时间序列特征来驱动预测过程。此外，我们采用渐进式的方式来训练我们的模型，以增强其泛化能力，逐步在训练样本中引入伯努利噪声。在多个领域的五个不同数据集上进行了大量实验。实验结果表明，我们提出的模型优于现有的方法，并在所有数据集上实现了更优异的预测性能。

    Multivariate long sequence time-series forecasting (M-LSTF) is a practical but challenging problem. Unlike traditional timer-series forecasting tasks, M-LSTF tasks are more challenging from two aspects: 1) M-LSTF models need to learn time-series patterns both within and between multiple time features; 2) Under the rolling forecasting setting, the similarity between two consecutive training samples increases with the increasing prediction length, which makes models more prone to overfitting. In this paper, we propose a generalizable memory-driven Transformer to target M-LSTF problems. Specifically, we first propose a global-level memory component to drive the forecasting procedure by integrating multiple time-series features. In addition, we adopt a progressive fashion to train our model to increase its generalizability, in which we gradually introduce Bernoulli noises to training samples. Extensive experiments have been performed on five different datasets across multiple fields. Exper
    
[^232]: 基于区间传播的IBP正则化方法提高对抗训练网络的验证稳健性

    IBP Regularization for Verified Adversarial Robustness via Branch-and-Bound. (arXiv:2206.14772v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14772](http://arxiv.org/abs/2206.14772)

    本文提出了一种基于区间传播的IBP正则化算法，通过在扩大的领域上进行对抗性攻击并结合一种基于廉价区间传播的正则化项来引入网络的可验证性，从而实现对抗训练网络的验证稳健性。

    

    近期的工作尝试通过在扩大的领域上运行攻击并向目标函数中添加各种正则项来增加对抗训练网络的可验证性。然而，这些算法要么性能不佳，要么需要复杂和昂贵的分阶段训练过程，从而影响其实际应用。本文提出了一种新颖的验证训练算法IBP-R，它既简单又有效。IBP-R通过在扩大的领域上进行对抗性攻击并结合一种基于廉价区间传播的正则化项来引入网络的可验证性，从而最小化非凸验证问题与其近似之间的差距。通过利用最近的分支定界框架，我们展示了IBP-R在CIFAR-10小扰动上获得了最先进的验证稳健性-准确性平衡，同时比相关先前工作训练速度更快。此外，我们还提出了一种新的分支算法UPB。

    Recent works have tried to increase the verifiability of adversarially trained networks by running the attacks over domains larger than the original perturbations and adding various regularization terms to the objective. However, these algorithms either underperform or require complex and expensive stage-wise training procedures, hindering their practical applicability. We present IBP-R, a novel verified training algorithm that is both simple and effective. IBP-R induces network verifiability by coupling adversarial attacks on enlarged domains with a regularization term, based on inexpensive interval bound propagation, that minimizes the gap between the non-convex verification problem and its approximations. By leveraging recent branch-and-bound frameworks, we show that IBP-R obtains state-of-the-art verified robustness-accuracy trade-offs for small perturbations on CIFAR-10 while training significantly faster than relevant previous work. Additionally, we present UPB, a novel branching
    
[^233]: 通过图上的能量理解卷积

    Understanding convolution on graphs via energies. (arXiv:2206.10991v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10991](http://arxiv.org/abs/2206.10991)

    本论文结合能量的概念，证明了带对称滤波器的线性图卷积可以增强高频率，使图神经网络在同质和异质任务中表现更好。

    

    图神经网络（GNN）通常通过消息传递操作，其中节点的状态是基于其邻居收到的信息进行更新的。大多数消息传递模型都是作为图卷积进行操作的，其中特征在被传播到边缘之前通过共享的线性变换混合。在节点分类任务中，图卷积已经表现出两个限制：在heterophilic图上表现欠佳，并且过度平滑。常见的看法是，这两种现象的发生是因为这种模型表现为低通滤波器，意味着在图层间特征的Dirichlet能量会减少，导致平滑效应，最终特征不再可区分。在这项工作中，我们严谨地证明了简单的图卷积模型实际上可以增强高频率甚至引导一种我们所称的过度锐化的渐近行为，与过度平滑相反。我们通过表明对称滤波器的线性图卷积可以被解释为在图形上的能量最小化问题来做到这一点。具体而言，能量函数惩罚高能信号，有效地抑制低频，同时促进相关的高频。我们的结果表明，精心设计的图卷积模型可以在同质和异质任务上提供更好的性能。

    Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with sy
    
[^234]: OmniMAE: 图片和视频上的单一模型遮蔽预训练

    OmniMAE: Single Model Masked Pretraining on Images and Videos. (arXiv:2206.08356v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.08356](http://arxiv.org/abs/2206.08356)

    该论文提出了一种基于遮蔽自编码的方法，可以在图像和视频上训练一个简单的单一Vision Transformer模型，而不需要标记数据，该模型的视觉表示可与单模态表示在基准测试上相当或更好，并且使用更简单的架构。

    

    基于Transformer的体系结构在各种视觉领域中已变得竞争力十足，其中最著名的是图像和视频。之前的工作通常是研究这些模态之间的隔离，但是具有相同的架构意味着可以为多个视觉模态训练一个单一的统一模型。之前的统一建模尝试通常使用专门为视觉任务量身定制的架构，或与单模态模型相比表现更差。在这项工作中，我们展示了遮蔽自编码可以用于在图像和视频上训练一个简单的Vision Transformer模型，而无需任何标记数据。这个单一的模型学习的视觉表示与单模态表示在图像和视频基准测试上相当或更好，同时使用更简单的架构。此外，通过删除90％的图像和95％的视频补丁，可以学习该模型，从而实现极快的大型模型架构训练。特别地，我们展示了我们的单一ViT-Hu

    Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Hu
    
[^235]: 通过领域适应实现公平分类：一种双重对抗学习方法

    Fair Classification via Domain Adaptation: A Dual Adversarial Learning Approach. (arXiv:2206.03656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03656](http://arxiv.org/abs/2206.03656)

    通过利用类似域的辅助信息，本论文提出了一种双重对抗学习方法，以实现没有敏感属性的目标域的公平分类。

    

    现代的机器学习模型越来越受欢迎，在决策系统中得到广泛应用。然而，研究表明机器学习歧视和不公的问题十分严重，这些问题阻碍了高风险应用的采用。公平分类器的最近研究引起了人们的广泛关注，旨在开发有效的算法实现公平和良好的分类性能。尽管这些公平感知的机器学习模型取得了极大的成功，但大多数现有模型需要使用敏感属性对数据进行预处理，正则化模型学习或后处理预测，以实现公平预测。然而，由于隐私、法律或监管限制，敏感属性常常是不完整甚至不可用的。虽然我们没有敏感属性来训练目标域的公平模型，但可能存在具有敏感属性的类似域。因此，利用类似域的辅助信息非常重要。

    Modern machine learning (ML) models are becoming increasingly popular and are widely used in decision-making systems. However, studies have shown critical issues of ML discrimination and unfairness, which hinder their adoption on high-stake applications. Recent research on fair classifiers has drawn significant attention to developing effective algorithms to achieve fairness and good classification performance. Despite the great success of these fairness-aware machine learning models, most of the existing models require sensitive attributes to pre-process the data, regularize the model learning or post-process the prediction to have fair predictions. However, sensitive attributes are often incomplete or even unavailable due to privacy, legal or regulation restrictions. Though we lack the sensitive attribute for training a fair model in the target domain, there might exist a similar domain that has sensitive attributes. Thus, it is important to exploit auxiliary information from a simil
    
[^236]: 自适应切片瓦砾斯坦距离的 PAC-Bayesian 光照

    Shedding a PAC-Bayesian Light on Adaptive Sliced-Wasserstein Distances. (arXiv:2206.03230v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.03230](http://arxiv.org/abs/2206.03230)

    本文利用 PAC-Bayesian 理论，提出了自适应切片瓦砾斯坦距离的概括特性界限和一种基于界限的切片分布学习流程，以提高 SW 的判别度。

    

    切片瓦砾斯坦距离（SW）是瓦砾斯坦距离的一种计算有效且理论基础良好的替代方法。然而，关于其统计特性（或更准确地说，关于其相对于“切片”的分布的概括特性，超越均匀分布），文献资料极为有限。为了为这一研究方向带来新的贡献，本文利用 PAC-Bayesian 理论和一个核心观察结果：SW 可以被解释为平均风险，PAC-Bayesian 界定其特性的量。本文提供了三种结果：i）PAC-Bayesian 的概括性界限，适用于我们所称的自适应切片瓦砾斯坦距离，即相对于任意分布的切片（包括数据相关分布）定义的 SW；ii）一种基于理论界限的原则性流程，用于学习切片分布，以得到最大判别 SW；iii）我们的理论和算法的实证说明。

    The Sliced-Wasserstein distance (SW) is a computationally efficient and theoretically grounded alternative to the Wasserstein distance. Yet, the literature on its statistical properties -- or, more accurately, its generalization properties -- with respect to the distribution of slices, beyond the uniform measure, is scarce. To bring new contributions to this line of research, we leverage the PAC-Bayesian theory and a central observation that SW may be interpreted as an average risk, the quantity PAC-Bayesian bounds have been designed to characterize. We provide three types of results: i) PAC-Bayesian generalization bounds that hold on what we refer as adaptive Sliced-Wasserstein distances, i.e. SW defined with respect to arbitrary distributions of slices (among which data-dependent distributions), ii) a principled procedure to learn the distribution of slices that yields maximally discriminative SW, by optimizing our theoretical bounds, and iii) empirical illustrations of our theoretic
    
[^237]: Saliency Cards: 一个用于表征和比较显著性方法的框架

    Saliency Cards: A Framework to Characterize and Compare Saliency Methods. (arXiv:2206.02958v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02958](http://arxiv.org/abs/2206.02958)

    本文介绍了显著性卡片，即结构化文档，描述了显著性方法的操作方式及其在多项评估指标上的性能，并确定了用户选择方法时应考虑的10个属性。

    

    显著性方法是一类常见的机器学习可解释性技术，计算每个输入特征对模型输出的重要性。然而，随着快速发展，用户很难了解新方法的优缺点，因此会根据不合理的原因（如流行度）选择方法。为此，我们引入了显著性卡片：结构化文档，描述显著性方法的操作方式及其在多项评估指标上的性能。通过对25个显著性方法论文和33个方法评估的回顾，我们确定了用户选择方法时应考虑的10个属性。我们将这些属性分成三个类别，涵盖计算和解释显著性的过程。

    Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model's output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency: methodology, or how the
    
[^238]: 差分隐私优化中更快的收敛速度到静态点

    Faster Rates of Convergence to Stationary Points in Differentially Private Optimization. (arXiv:2206.00846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00846](http://arxiv.org/abs/2206.00846)

    本文研究了在差分隐私下近似利普希茨和平滑函数的静态点问题。提供了新的高效算法和构造，分别在有限和随机情况下比现有算法更快的收敛速度。

    

    我们研究了在$(\varepsilon,\delta)$-差分隐私（DP）下，近似利普希茨和平滑函数的静态点的问题，涉及了有限和和随机情况。如果$\|\nabla F(\widehat{w})\|\leq \alpha$，则称点$\widehat{w}$是函数$F:\mathbb{R}^d\rightarrow\mathbb{R}$的$\alpha$-静态点。我们提供了一个新的高效算法，在有限和设置中找到一个$\tilde{O}\big(\big[\frac{\sqrt{d}}{n\varepsilon}\big]^{2/3}\big)$的静态点，其中$n$是样本数。这优于以前最佳速率$\tilde{O}\big(\big[\frac{\sqrt{d}}{n\varepsilon}\big]^{1/2}\big)$。我们还提供了一种新的构造，改进了随机优化设置中现有的速率，在该设置中，目标是找到人口风险的近似静态点。我们的构造找到了一个$\tilde{O}\big(\frac{1}{n^{1/3}} + \big[\frac{\sqrt{d}}{n\varepsilon}\big]^{1/2}\big)$的人口风险静态点。

    We study the problem of approximating stationary points of Lipschitz and smooth functions under $(\varepsilon,\delta)$-differential privacy (DP) in both the finite-sum and stochastic settings. A point $\widehat{w}$ is called an $\alpha$-stationary point of a function $F:\mathbb{R}^d\rightarrow\mathbb{R}$ if $\|\nabla F(\widehat{w})\|\leq \alpha$. We provide a new efficient algorithm that finds an $\tilde{O}\big(\big[\frac{\sqrt{d}}{n\varepsilon}\big]^{2/3}\big)$-stationary point in the finite-sum setting, where $n$ is the number of samples. This improves on the previous best rate of $\tilde{O}\big(\big[\frac{\sqrt{d}}{n\varepsilon}\big]^{1/2}\big)$. We also give a new construction that improves over the existing rates in the stochastic optimization setting, where the goal is to find approximate stationary points of the population risk. Our construction finds a $\tilde{O}\big(\frac{1}{n^{1/3}} + \big[\frac{\sqrt{d}}{n\varepsilon}\big]^{1/2}\big)$-stationary point of the population risk 
    
[^239]: 通过高效探索学习预测的静态调度

    Static Scheduling with Predictions Learned through Efficient Exploration. (arXiv:2205.15695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15695](http://arxiv.org/abs/2205.15695)

    本文研究了单机作业调度的问题，提出了一种基于学习预测的静态调度算法，在类型未知的情况下实现了次线性的过剩成本，尤其在抢占式问题中表现出色，可以在不同作业类型持续时间相差很大时优于非抢占匹配。

    

    我们研究了单机作业调度，每个作业都属于决定其持续时间分布的作业类型。我们首先分析了类型特征已知的情况，然后转向两种学习情景，其中类型未知：非抢占式问题，它要求完成已启动的作业，然后才能移动到另一个作业；和抢占式问题，这里作业执行可以暂停以优先转移到另一个作业。在两种情况下，我们设计的算法相对于已知类型的性能实现了次线性的过剩成本，并证明了非抢占式情况的下限。值得注意的是，我们展示了抢占算法在不同作业类型持续时间相差很大时，理论上和通过模拟的方式可以优于非抢占匹配，在类型持续时间已知时并不存在这种现象。

    We study single-machine scheduling of jobs, each belonging to a job type that determines its duration distribution. We start by analyzing the scenario where the type characteristics are known and then move to two learning scenarios where the types are unknown: non-preemptive problems, where each started job must be completed before moving to another job; and preemptive problems, where job execution can be paused in the favor of moving to a different job. In both cases, we design algorithms that achieve sublinear excess cost, compared to the performance with known types, and prove lower bounds for the non-preemptive case. Notably, we demonstrate, both theoretically and through simulations, how preemptive algorithms can greatly outperform non-preemptive ones when the durations of different job types are far from one another, a phenomenon that does not occur when the type durations are known.
    
[^240]: 偶然性：利用语义搜索在Reddit上跟踪俄罗斯国家媒体对俄乌战争的叙事

    Happenstance: Utilizing Semantic Search to Track Russian State Media Narratives about the Russo-Ukrainian War On Reddit. (arXiv:2205.14484v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2205.14484](http://arxiv.org/abs/2205.14484)

    本论文通过分析俄罗斯国家媒体在英语读者中传播的叙事，比较Reddit上有关俄乌战争的讨论，展示了俄罗斯媒体对叙事的影响以及讨论的发展。

    

    在俄罗斯联邦侵略乌克兰之前和之后的几周，俄罗斯国家媒体发布了大量误导性和彻底错误的信息。本文研究了这种协调信息运动，以便了解俄罗斯政府向英语读者宣传的最突出的国家媒体叙事。为此，我们首先使用大型语言模型MPNet对由包括俄罗斯新的“事实核查”网站waronfakes.com在内的十个不同亲俄宣传网站发布的文章进行句级主题分析。在这个生态系统中，我们展示了像katehon.com这样的小型网站在发布稍后由其他俄罗斯网站回响的主题方面非常有效。在分析了这组俄罗斯信息叙事之后，我们再分析它们与r/Russia和其他10个政治子reddit上的叙事和讨论主题的对应关系。使用MPNet和语义搜索算法将这些子reddit上的前10,000条评论分别翻译成俄语和英语，然后使用MUSE跨语言嵌入找到语义上类似的评论。最后，我们展示了Reddit上有关俄乌战争的讨论如何随时间演变以及这些讨论如何受到国家媒体叙事的影响的交互式可视化。

    In the buildup to and in the weeks following the Russian Federation's invasion of Ukraine, Russian state media outlets output torrents of misleading and outright false information. In this work, we study this coordinated information campaign in order to understand the most prominent state media narratives touted by the Russian government to English-speaking audiences. To do this, we first perform sentence-level topic analysis using the large-language model MPNet on articles published by ten different pro-Russian propaganda websites including the new Russian "fact-checking" website waronfakes.com. Within this ecosystem, we show that smaller websites like katehon.com were highly effective at publishing topics that were later echoed by other Russian sites. After analyzing this set of Russian information narratives, we then analyze their correspondence with narratives and topics of discussion on the r/Russia and 10 other political subreddits. Using MPNet and a semantic search algorithm, we
    
[^241]: 带有偏好引导的个性化算法干预研究

    Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13743](http://arxiv.org/abs/2205.13743)

    研究提出了PEAR方法，这是一个首个能够针对最终用户需求提供个性化算法补救成本的人机交互方法。该方法利用贝叶斯偏好引导的见解，通过最大化原则性信息增益度量来计算目标用户选择的预期效用，然后将偏好引导整合到强化学习框架中。该方法显著提高了算法干预的经济实用性和用户友好性。

    

    算法干预（AR）的问题是计算用户执行一系列操作以颠覆不良机器决策的过程。该过程的操作序列不应该对用户的实施提出过高的要求。然而，大多数AR方法都假设所有用户的操作成本相同，因此可能会向某些用户推荐昂贵的补救计划。为了解决这个问题，我们提出了PEAR，这是一种首个可提供个性化算法补救成本的人机交互方法，以满足任何最终用户的需求。PEAR利用贝叶斯偏好引导的见解，通过向目标用户发出选择集查询来迭代地改善对操作成本的估计值。这些查询的计算是通过最大化选择的预期效用来计算的，这是一种能够考虑成本估计和用户响应不确定性的原则性信息增益度量。PEAR将偏好引导整合到强化学习框架中，同时考虑用户实现AR任务所需达成目标的偏好，以及执行每个操作所涉及的成本。我们通过引入更具挑战性的AR任务来评估PEAR，并显示其比现有的方法找到了更为经济实用且用户友好的补救计划。

    Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
    
[^242]: 糟糕的教学会导致遗忘吗？使用无能教师在深度网络中进行取消学习。

    Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher. (arXiv:2205.08096v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.08096](http://arxiv.org/abs/2205.08096)

    该论文提出了一种新颖的机器取消学习方法，通过在学生-教师框架中利用有能力和无能力的教师来引导遗忘，以便随时从已经训练好的机器学习模型中删除某个集合或类别的数据，而无需重新训练。

    

    由于越来越多的机器学习应用需要遵守新兴的数据隐私法规，机器取消学习已成为研究的重要领域。它便于从已经训练好的机器学习模型中删除某个集合或类别的数据，而无需重新训练。我们提出了一种新颖的机器取消学习方法，通过在学生-教师框架中利用有能力和无能力的教师引导遗忘。我们选择性地将有能力和无能力教师的知识传递给学生，以获得一个不包含任何被遗忘数据信息的模型。我们实验表明，这种方法具有良好的泛化性和快速有效性。此外，我们引入“零重新训练遗忘（ZRF）”指标来评估任何取消学习方法。与现有的取消学习指标不同，ZRF得分...

    Machine unlearning has become an important area of research due to an increasing need for machine learning (ML) applications to comply with the emerging data privacy regulations. It facilitates the provision for removal of certain set or class of data from an already trained ML model without requiring retraining from scratch. Recently, several efforts have been put in to make unlearning to be effective and efficient. We propose a novel machine unlearning method by exploring the utility of competent and incompetent teachers in a student-teacher framework to induce forgetfulness. The knowledge from the competent and incompetent teachers is selectively transferred to the student to obtain a model that doesn't contain any information about the forget data. We experimentally show that this method generalizes well, is fast and effective. Furthermore, we introduce the zero retrain forgetting (ZRF) metric to evaluate any unlearning method. Unlike the existing unlearning metrics, the ZRF score 
    
[^243]: RoMFAC: 一种对于状态异常干扰具有鲁棒性的均场演员-评论家强化学习算法

    RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states. (arXiv:2205.07229v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.07229](http://arxiv.org/abs/2205.07229)

    本文提出了RoMFAC算法，通过新的训练目标和重复的正则化损失函数，使其对于异常状态干扰具有鲁棒性并获得出色性能表现。

    

    多智能体强化学习需要基于观察到的系统状态做出最优决策，但是观察中的不确定性可能会导致智能体做出错误的行动。本文提出一种新的算法RoMFAC，它通过一个策略梯度函数和一个代表状态干扰影响的行动损失函数训练演员。同时，RoMFAC还引入一个重复的正则化行动损失函数的方法，以确保训练演员具有出色的性能表现。

    Multi-agent deep reinforcement learning makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement learning (MFAC) is well-known in the multi-agent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two innovations: 1) a new objective function of training actors, composed of a \emph{policy gradient function} that is related to the expected cumulative discount reward on sampled clean states and an \emph{action loss function} that represents the difference between actions taken on clean and adversarial states; and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a 
    
[^244]: BrainIB：基于图信息瓶颈的可解释性脑网络精神疾病诊断

    BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck. (arXiv:2205.03612v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2205.03612](http://arxiv.org/abs/2205.03612)

    BrainIB是一种基于图信息瓶颈原理开发的图神经网络框架，在分析fMRI图像中的功能连接时能够识别最具信息量的边缘，具有良好的泛化能力和可解释性，适用于未见样本的场景。

    

    发展一种基于潜在生物机制而非主观症状对精神障碍进行诊断的新模型已成为一种新的共识。最近，使用功能连接（FC）进行精神障碍和健康对照的机器学习分类器被开发出来，用于确定大脑标记物。然而，现有的基于机器学习的诊断模型容易出现过拟合的情况（由于训练样本不足），在新的测试环境中表现差。此外，难以获得可解释的、可靠的大脑生物标记物来解释潜在的诊断决策。这些问题阻碍了其可能的临床应用。在本研究中，我们提出了BrainIB，一种新的图神经网络（GNN）框架，通过利用著名的信息瓶颈（IB）原理来分析功能磁共振图像（fMRI）。BrainIB能够识别大脑中最具信息量的边缘（即子图）并具有良好的泛化能力，适用于未见样本的场景。

    Developing a new diagnostic models based on the underlying biological mechanisms rather than subjective symptoms for psychiatric disorders is an emerging consensus. Recently, machine learning-based classifiers using functional connectivity (FC) for psychiatric disorders and healthy controls are developed to identify brain markers. However, existing machine learningbased diagnostic models are prone to over-fitting (due to insufficient training samples) and perform poorly in new test environment. Furthermore, it is difficult to obtain explainable and reliable brain biomarkers elucidating the underlying diagnostic decisions. These issues hinder their possible clinical applications. In this work, we propose BrainIB, a new graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI), by leveraging the famed Information Bottleneck (IB) principle. BrainIB is able to identify the most informative edges in the brain (i.e., subgraph) and generalizes well to unseen 
    
[^245]: 增强物理信息神经网络的增广拉格朗日松弛方法（AL-PINNs）

    Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs). (arXiv:2205.01059v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.01059](http://arxiv.org/abs/2205.01059)

    本文提出一种增广拉格朗日松弛方法(AL-PINNs)用于物理信息神经网络(PINNs)的训练，该方法通过自适应平衡每个损失组件，能够有效地解决非线性偏微分方程问题。

    

    物理信息神经网络（PINNs）已成为科学计算中深度学习的杰出应用，它们是非线性偏微分方程（PDE）解的强大逼近器。本文提出了一种增广拉格朗日松弛方法（AL-PINNs）用于PINNs的训练。我们将初始和边界条件视为PDE残差优化问题的约束条件。通过应用增广拉格朗日松弛方法，约束优化问题变成了一个顺序的最大-最小问题，使可以学习的参数λ能自适应平衡每个损失组件。我们的理论分析表明，所提出的损失函数的最小化序列收敛于Helmholtz、粘性Burgers和Klein-Gordon方程的实际解。

    Physics-Informed Neural Networks (PINNs) have become a prominent application of deep learning in scientific computation, as they are powerful approximators of solutions to nonlinear partial differential equations (PDEs). There have been numerous attempts to facilitate the training process of PINNs by adjusting the weight of each component of the loss function, called adaptive loss-balancing algorithms. In this paper, we propose an Augmented Lagrangian relaxation method for PINNs (AL-PINNs). We treat the initial and boundary conditions as constraints for the optimization problem of the PDE residual. By employing Augmented Lagrangian relaxation, the constrained optimization problem becomes a sequential max-min problem so that the learnable parameters $\lambda$ adaptively balance each loss component. Our theoretical analysis reveals that the sequence of minimizers of the proposed loss functions converges to an actual solution for the Helmholtz, viscous Burgers, and Klein--Gordon equations
    
[^246]: 适应并评估用于梯度提升决策树的影响估计方法

    Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees. (arXiv:2205.00359v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.00359](http://arxiv.org/abs/2205.00359)

    该研究将深度学习模型的影响估计方法改编到了梯度提升决策树上，命名为TREX和BoostIn，旨在帮助更好地理解GBDT的预测和改进性能。

    

    影响估计分析了对训练数据的更改如何导致不同的模型预测；这种分析可以帮助我们更好地理解这些预测、做出这些预测的模型以及它们训练的数据集。然而，大多数影响估计技术都是为具有连续参数的深度学习模型设计的。梯度提升决策树（GBDT）是一种强大且广泛使用的模型类别；然而，这些模型是黑盒子，具有不透明的决策过程。为了更好地理解 GBDT 预测并普遍改进这些模型，我们将最近和流行的深度学习模型的影响估计方法适应到了 GBDT 上。具体而言，我们使用 representer-point 方法和 TracIn 方法改编了这些方法，分别命名为 TREX 和 BoostIn；源代码可在 https://github.com/jjbrophy47/tree_influence 上找到。我们使用五种不同的评估方法将这些方法与 LeafInfluence 和其他基准线进行了比较。

    Influence estimation analyzes how changes to the training data can lead to different model predictions; this analysis can help us better understand these predictions, the models making those predictions, and the data sets they're trained on. However, most influence-estimation techniques are designed for deep learning models with continuous parameters. Gradient-boosted decision trees (GBDTs) are a powerful and widely-used class of models; however, these models are black boxes with opaque decision-making processes. In the pursuit of better understanding GBDT predictions and generally improving these models, we adapt recent and popular influence-estimation methods designed for deep learning models to GBDTs. Specifically, we adapt representer-point methods and TracIn, denoting our new methods TREX and BoostIn, respectively; source code is available at https://github.com/jjbrophy47/tree_influence. We compare these methods to LeafInfluence and other baselines using 5 different evaluation mea
    
[^247]: 一种基于神经网络的听力设备轻量级去混响两阶段算法

    A neural network-supported two-stage algorithm for lightweight dereverberation on hearing devices. (arXiv:2204.02978v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2204.02978](http://arxiv.org/abs/2204.02978)

    论文提出了一种基于神经网络的两阶段算法，通过直接优化准则，使得多通道线性滤波器阶段输出处效果更好，可与后滤波器阶段结合有效去除残留混响。

    

    本文提出了一种基于深度神经网络的多通道多帧线性滤波器结合单通道单帧后滤波器的轻量级在线去混响算法。通过分析不同时间范围下去混响性能的新指标，证明了在多通道线性滤波器阶段输出处直接优化准则结果比放置在DNN输出处优化PSD估计更有效。我们展示了端到端训练模型有助于进一步去除可被滤波器访问的范围内的混响，从而提高早期到中期混响比率。

    A two-stage lightweight online dereverberation algorithm for hearing devices is presented in this paper. The approach combines a multi-channel multi-frame linear filter with a single-channel single-frame post-filter. Both components rely on power spectral density (PSD) estimates provided by deep neural networks (DNNs). By deriving new metrics analyzing the dereverberation performance in various time ranges, we confirm that directly optimizing for a criterion at the output of the multi-channel linear filtering stage results in a more efficient dereverberation as compared to placing the criterion at the output of the DNN to optimize the PSD estimation. More concretely, we show that training this stage end-to-end helps further remove the reverberation in the range accessible to the filter, thus increasing the \textit{early-to-moderate} reverberation ratio. We argue and demonstrate that it can then be well combined with a post-filtering stage to efficiently suppress the residual late rever
    
[^248]: 破坏者：训练语言模型的联邦学习中的变形金刚隐私泄露

    Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models. (arXiv:2201.12675v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12675](http://arxiv.org/abs/2201.12675)

    该论文提出了一种针对联邦学习中文本的隐私攻击方法，通过部署恶意参数向量来揭示私人用户文本，并成功地进行mini-batches训练，适用于多个用户和长序列，提示了文本领域的FL比先前认为的更脆弱。

    

    联邦学习(Federated learning, FL)的核心原则是在不集中用户数据的情况下训练模型，这种方法强调隐私保护。然而，先前的研究表明，FL中使用的梯度更新可能泄露用户信息。尽管FL在文本应用领域（例如击键预测）中很常见，但对于FL隐私的几乎所有攻击都集中在简单的图像分类器上。我们提出了一种新的攻击方法，通过部署恶意参数向量来揭示私人用户文本，这种攻击可以成功地进行mini-batches训练，适用于多个用户和长序列。与以往针对FL的攻击不同的是，这种攻击利用了Transformer架构和标记嵌入(token embedding)的特性，分别提取标记和位置嵌入以检索高保真度文本。这项工作表明，文本领域的FL在历史上一直能够抵御隐私攻击，但比先前认为的更加脆弱。

    A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.
    
[^249]: 零样本机器遗忘

    Zero-Shot Machine Unlearning. (arXiv:2201.05629v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.05629](http://arxiv.org/abs/2201.05629)

    零样本机器遗忘是一个新兴的研究问题，允许从已经训练好的ML模型中删除数据。因为这些请求可能会涉及到无法访问的训练数据，因此需要新的解决方法。

    

    现代隐私法规赋予公民被产品、服务和公司遗忘的权利。对于机器学习（ML）应用而言，这需要从存储归档和ML模型中删除数据。由于ML应用需要越来越多的监管合规性，机器遗忘已成为一个不断出现的研究问题。被遗忘请求以删除已经训练好的ML模型中的一定集合或类别的数据的形式提出。实际考虑阻止丢弃删除的数据后从头重新训练模型。现有少数研究使用整个训练数据、训练数据子集或在训练期间存储的一些元数据更新遗忘的模型权重。然而，在许多情况下，训练过程或训练样本相关的数据可能无法访问，我们因此提出问题：是否可能通过零样本学习实现遗忘。

    Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, in many cases, no data related to the training process or training samples may be accessible for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning wit
    
[^250]: 在错误预测上使用最大熵(MEEP)：提高医学图像分割模型的校准性。

    Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation. (arXiv:2112.12218v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.12218](http://arxiv.org/abs/2112.12218)

    本文提出了在最大熵和错误预测上使用的训练策略（MEEP），通过惩罚自信度过高的预测来提高医学图像分割模型的校准性和分割准确性。

    

    现代深度神经网络在医学图像分割任务上取得了显著进展。然而，最近观察到，它们倾向于产生过度自信的估计结果，甚至在高不确定性情况下也是如此，导致模型校准不良、不可靠。在这项工作中，我们介绍了一种基于最大熵的错误预测（MEEP）训练策略，用于分割网络，它有选择地对自信度过高的预测进行惩罚，仅关注错误分类的像素。我们的方法对神经结构不加偏见，不增加模型复杂度，可与多个分割损失函数配对使用。我们在两个具有挑战性的分割任务中进行了基准测试：脑磁共振图像中的白质高信号病变和心脏磁共振图像中的心房分割。实验结果表明，将MEEP与标准分割损失相结合，不仅在模型校准方面，而且在分割准确性方面，相比于基线模型都会带来改进。

    Modern deep neural networks achieved remarkable progress in medical image segmentation tasks. However, it has recently been observed that they tend to produce overconfident estimates, even in situations of high uncertainty, leading to poorly calibrated and unreliable models. In this work we introduce Maximum Entropy on Erroneous Predictions (MEEP), a training strategy for segmentation networks which selectively penalizes overconfident predictions, focusing only on misclassified pixels. Our method is agnostic to the neural architecture, does not increase model complexity and can be coupled with multiple segmentation loss functions. We benchmark the proposed strategy in two challenging segmentation tasks: white matter hyperintensity lesions in magnetic resonance images (MRI) of the brain, and atrial segmentation in cardiac MRI. The experimental results demonstrate that coupling MEEP with standard segmentation losses leads to improvements not only in terms of model calibration, but also i
    
[^251]: 通过混合模型进行有限全局混淆的因果推断

    Causal Inference Despite Limited Global Confounding via Mixture Models. (arXiv:2112.11602v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11602](http://arxiv.org/abs/2112.11602)

    本论文提出了一种基于混合模型的因果推断方法，通过解决混合问题和恢复概率分布，可以确定原本无法确定的因果关系。

    

    贝叶斯网络是一组$n$个随机变量（图的顶点）上的有向无环图（DAG）; 贝叶斯网络分布（BND）是在图上马尔可夫的随机变量的概率分布。这种模型的有限$k$-混合由一个更大的图形式化表示，该图具有一个额外的“隐藏”（或“潜在”）随机变量$U$，其范围为$\{1,\ldots,k\}$，并且$U$到每个其他顶点都有一个有向边。这种类型的模型在因果推断中是基本的，其中$U$模拟了多个群体的未观察到的混淆效应，使得可观察的DAG中的因果关系变得模糊不清。通过解决混合问题并恢复$U$上的联合概率分布，传统上无法确定的因果关系变得可确定。通过将其约化为更为研究的“空”图中的“乘积”情况，我们提出了第一个学习非空DAG的混合算法。

    A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ random variables (the vertices); a Bayesian Network Distribution (BND) is a probability distribution on the random variables that is Markovian on the graph. A finite $k$-mixture of such models is graphically represented by a larger graph which has an additional "hidden" (or "latent") random variable $U$, ranging in $\{1,\ldots,k\}$, and a directed edge from $U$ to every other vertex. Models of this type are fundamental to causal inference, where $U$ models an unobserved confounding effect of multiple populations, obscuring the causal relationships in the observable DAG. By solving the mixture problem and recovering the joint probability distribution on $U$, traditionally unidentifiable causal relationships become identifiable. Using a reduction to the more well-studied "product" case on empty graphs, we give the first algorithm to learn mixtures of non-empty DAGs.
    
[^252]: 用核范数控制Wasserstein距离，并在压缩统计学习中应用

    Controlling Wasserstein Distances by Kernel Norms with Application to Compressive Statistical Learning. (arXiv:2112.00423v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.00423](http://arxiv.org/abs/2112.00423)

    本文提供了用MMD范数控制Wasserstein距离的条件，针对压缩统计学习提出了HLRIP属性，通过导出的新核范数提供了计算上高效的Wasserstein距离压缩统计学习保证。

    

    在许多机器学习算法中，比较概率分布是关键。最大均值差异（MMD）和Wasserstein距离是两类概率分布距离，近年来受到了广泛关注。本文建立了一些条件，使得可以通过MMD范数来控制Wasserstein距离。我们的工作受到压缩统计学习（CSL）理论的启发，这是一种资源有效的大规模学习通用框架，在其中训练数据在单个向量（称为草图）中进行总结，以捕捉与考虑的学习任务相关的信息。我们在现有的CSL结果的启发下引入了H\"older Lower Restricted Isometric Property，并展示了这种属性对于压缩统计学习具有有趣的保证。基于MMD和Wasserstein距离之间的关系，我们提供了一种基于MMD导出的新核范数的压缩统计学习保证。我们的理论为在压缩统计学习中使用Wasserstein距离的学习提供了一种计算上高效的两步算法。我们将我们的方法应用于合成和真实数据集，展示了Wasserstein距离相对于CSL中其他常用距离的实质性改进。

    Comparing probability distributions is at the crux of many machine learning algorithms. Maximum Mean Discrepancies (MMD) and Wasserstein distances are two classes of distances between probability distributions that have attracted abundant attention in past years. This paper establishes some conditions under which the Wasserstein distance can be controlled by MMD norms. Our work is motivated by the compressive statistical learning (CSL) theory, a general framework for resource-efficient large scale learning in which the training data is summarized in a single vector (called sketch) that captures the information relevant to the considered learning task. Inspired by existing results in CSL, we introduce the H\"older Lower Restricted Isometric Property and show that this property comes with interesting guarantees for compressive statistical learning. Based on the relations between the MMD and the Wasserstein distances, we provide guarantees for compressive statistical learning by introduci
    
[^253]: 快速有效的机器数据遗忘

    Fast Yet Effective Machine Unlearning. (arXiv:2111.08947v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08947](http://arxiv.org/abs/2111.08947)

    本文提出了一种快速且有效的机器数据遗忘框架，该框架采用误差最大化的噪声生成和损伤-修复的权重操作来删除机器学习模型中的特定数据，同时具有较高的适用性和效率。

    

    对机器学习模型在训练中观测到的数据进行遗忘是一项重要任务，可以在加强基于机器学习的应用程序的隐私和安全方面发挥关键作用。本文提出了以下问题：（i）我们能否在不查看完整训练数据的情况下，从机器学习模型中删除单个或多个类别的数据？（ii）我们能否使快速的遗忘过程适用于大型数据集，并推广到不同的深度网络？我们引入了一种新的机器数据遗忘框架，采用误差最大化的噪声生成和损伤-修复的权重操作，为上述问题提供了高效的解决方案。通过使用原始模型学习一个针对待遗忘类别的误差最大化噪声矩阵，并利用该噪声矩阵操作模型权重以遗忘目标类别的数据。我们引入损伤和修复步骤来控制网络权重的操作。在损伤步骤中，将噪声矩阵应用于与目标类别相对应的神经元的权重。在修复步骤中，使用剩余的训练数据对操作后的权重进行微调。各种基准实验表明了我们所提出的方法的有效性和高效性。

    Unlearning the data observed during the training of a machine learning (ML) model is an important task that can play a pivotal role in fortifying the privacy and security of ML-based applications. This paper raises the following questions: (i) can we unlearn a single or multiple class(es) of data from a ML model without looking at the full training data even once? (ii) can we make the process of unlearning fast and scalable to large datasets, and generalize it to different deep networks? We introduce a novel machine unlearning framework with error-maximizing noise generation and impair-repair based weight manipulation that offers an efficient solution to the above questions. An error-maximizing noise matrix is learned for the class to be unlearned using the original model. The noise matrix is used to manipulate the model weights to unlearn the targeted class of data. We introduce impair and repair steps for a controlled manipulation of the network weights. In the impair step, the noise
    
[^254]: 面向通用和高效黑盒优化的最优统计协作

    Optimum-statistical Collaboration Towards General and Efficient Black-box Optimization. (arXiv:2106.09215v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.09215](http://arxiv.org/abs/2106.09215)

    本文提出了最优统计协作算法框架，管理优化误差通量和演化中的统计误差通量。该框架通用性强且适用于多种函数和分区族，并启发提出了一种方差自适应算法。

    

    本文中，我们针对分层赌博机式黑盒优化算法中分辨率和统计不确定性的作用进行关键阐述，引导更为通用的分析和更高效的算法设计。我们引入了最优统计协作，一种管理优化误差通量和优化过程中演化的统计误差通量相互作用的算法框架。我们提供了此框架的通用分析，而不需要指定统计误差和不确定性量化器的形式。由于其通用性，我们的框架和分析可应用于满足不同局部平滑性假设和具有不同局部最优值数量的大量函数和分区族，这比之前的作品所研究的函数类要丰富得多。该框架还启发我们提出更好的统计不确定性测量方法，因此提出了一种方差自适应算法。

    In this paper, we make the key delineation on the roles of resolution and statistical uncertainty in hierarchical bandits-based black-box optimization algorithms, guiding a more general analysis and a more efficient algorithm design. We introduce the \textit{optimum-statistical collaboration}, an algorithm framework of managing the interaction between optimization error flux and statistical error flux evolving in the optimization process. We provide a general analysis of this framework without specifying the forms of statistical error and uncertainty quantifier. Our framework and its analysis, due to their generality, can be applied to a large family of functions and partitions that satisfy different local smoothness assumptions and have different numbers of local optimums, which is much richer than the class of functions studied in prior works. Our framework also inspires us to propose a better measure of the statistical uncertainty and consequently a variance-adaptive algorithm \text
    
[^255]: 解释树模型的准确Shapley值

    Accurate Shapley Values for explaining tree-based models. (arXiv:2106.03820v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.03820](http://arxiv.org/abs/2106.03820)

    本文提出了在树模型中计算Shapley值的两种更准确的估计器，相比于现有方法可以更高效地利用树结构，并探讨了Shapley值作为局部解释的局限性。

    

    Shapley值广泛用于可解释的人工智能，但它们的估计和解释可能具有挑战性，导致不准确的推论和解释。本文提出了两种基于树结构的Shapley值估计器，利用树结构高效地计算Shapley值，比现有方法更准确。通过与最先进的算法进行模拟和比较，展示了我们方法的实际收益。最后，我们讨论了Shapley值作为局部解释的局限性。这些方法可以作为Python包使用。

    Shapley Values (SV) are widely used in explainable AI, but their estimation and interpretation can be challenging, leading to inaccurate inferences and explanations. As a starting point, we remind an invariance principle for SV and derive the correct approach for computing the SV of categorical variables that are particularly sensitive to the encoding used. In the case of tree-based models, we introduce two estimators of Shapley Values that exploit the tree structure efficiently and are more accurate than state-of-the-art methods. Simulations and comparisons are performed with state-of-the-art algorithms and show the practical gain of our approach. Finally, we discuss the limitations of Shapley Values as a local explanation. These methods are available as a Python package.
    
[^256]: 通过信息最大化的终止评估学习多样化选项

    Learning Diverse Options via InfoMax Termination Critic. (arXiv:2010.02756v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.02756](http://arxiv.org/abs/2010.02756)

    本文提出了一种通过最大化状态和动作选项之间的互信息来学习选项的终止条件的方法，从而提高学习到的选项的多样性和可重用性，在实验中取得了显著的效果。

    

    本文考虑了深度强化学习中自动学习可重用的持续性动作选项的问题。虽然选项可以作为可重用的构建模块来加速迁移学习，但是为未知任务分布学习可重用选项仍然具有挑战性。本文受到基于互信息技术的技能学习的启发，提出通过最大化选项和对应状态转换之间的互信息来学习选项的终止条件。我们通过梯度上升导出了这种互信息最大化的可扩展近似方法，并称之为InfoMax Termination Critic（IMTC）算法。本文的实验结果表明，IMTC显著提高了学习选项的多样性，且不需要外部奖励，仅结合内在的选项学习方法。此外，我们通过将学习到的选项转移到各种任务中测试其可重用性，证实了其可行性。

    We consider the problem of autonomously learning reusable temporally extended actions, or options, in reinforcement learning. While options can speed up transfer learning by serving as reusable building blocks, learning reusable options for unknown task distribution remains challenging. Motivated by the recent success of mutual information (MI) based skill learning, we hypothesize that more diverse options are more reusable. To this end, we propose a method for learning termination conditions of options by maximizing MI between options and corresponding state transitions. We derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. Our experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards combined with an intrinsic option learning method. Moreover, we test the reusability of learned options by transferring options into various tasks, confirming that
    
[^257]: 浅层神经网络带限制的随机权重有多大的能力？

    How Powerful are Shallow Neural Networks with Bandlimited Random Weights?. (arXiv:2008.08427v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.08427](http://arxiv.org/abs/2008.08427)

    本文研究了深度为2的带限制随机神经网络的表达能力，通过数学证明确定了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。

    

    本文探讨了深度为2的带限制随机神经网络的表达能力。随机网络是指隐藏层参数被冻结并赋予随机分配的神经网络，只有输出层参数通过损失最小化进行训练。使用随机权重的隐藏层是避免标准梯度下降学习中的非凸优化的有效方法，并已被近期深度学习理论所采用。尽管神经网络是普适逼近器的众所周知的事实，在这项研究中，我们数学上证明了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。我们特别导出了一个新的非平凡逼近误差下界。证明利用了Ridgelet分析技术，这是一种为神经网络设计的谐波分析方法。这种方法受到了经典信号处理中的基本原理的启发，特别是信号在某种限制下的采样。

    We investigate the expressive power of depth-2 bandlimited random neural networks. A random net is a neural network where the hidden layer parameters are frozen with random assignment, and only the output layer parameters are trained by loss minimization. Using random weights for a hidden layer is an effective method to avoid non-convex optimization in standard gradient descent learning. It has also been adopted in recent deep learning theories. Despite the well-known fact that a neural network is a universal approximator, in this study, we mathematically show that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error. In particular, we derive a new nontrivial approximation error lower bound. The proof utilizes the technique of ridgelet analysis, a harmonic analysis method designed for neural networks. This method is inspired by fundamental principles in classical signal processing, specifically the idea that signals with limit
    

