# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Statistical Inference for Fairness Auditing.](http://arxiv.org/abs/2305.03712) | 本文介绍了一种公平性审计的统计推断方法，可用于评估黑盒模型在敏感子群体上的表现，并通过自举方法限制多个群体表现差异，方法通用性强且适用面广。 |
| [^2] | [Is dataset condensation a silver bullet for healthcare data sharing?.](http://arxiv.org/abs/2305.03711) | 数据集压缩为医疗数据共享提供了一种既保护了隐私又保存了实用程序的新方法。 |
| [^3] | [Data Encoding For Healthcare Data Democratisation and Information Leakage Prevention.](http://arxiv.org/abs/2305.03710) | 本文提出了一种通过不可逆数据编码实现医疗数据民主化和信息泄露预防的方法。利用随机投影和随机量子编码实现密集和时间序列数据的编码框架，在保持原始数据语义信息的同时有效提取信息瓶颈原则，为深度学习在医疗保健领域的发展提供了新思路。 |
| [^4] | [Fine-Grained Product Classification on Leaflet Advertisements.](http://arxiv.org/abs/2305.03706) | 本文介绍了首个公开细粒度产品识别数据集，通过图像和文本结合的模型可以提高对于细节相似的产品分类精确度，最终模型准确率为96.4%。 |
| [^5] | [Mining bias-target Alignment from Voronoi Cells.](http://arxiv.org/abs/2305.03691) | 本文提出了一种偏见无关的方法来减轻深度神经网络中偏差的影响，在目标类别上量化“偏见对齐/不对齐”，以避免通过网络传播偏见-目标对齐信息。 |
| [^6] | [On Preimage Approximation for Neural Networks.](http://arxiv.org/abs/2305.03686) | 本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。 |
| [^7] | [Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models.](http://arxiv.org/abs/2305.03660) | 该研究提出了一种检索增强的方法，利用对比预训练的视觉语言模型的多模态对齐嵌入来检索相应的候选放射学文本，并使用通用领域生成模型来生成报告，可抑制虚构的生成，实现更好的临床指标。 |
| [^8] | [White-Box Multi-Objective Adversarial Attack on Dialogue Generation.](http://arxiv.org/abs/2305.03655) | 该论文提出了一种新的白盒多目标对话生成对抗攻击方法，DGSlow。通过平衡生成准确性和长度两个目标，DGSlow利用生成更长的输出来提高攻击效果。 |
| [^9] | [On the Effectiveness of Equivariant Regularization for Robust Online Continual Learning.](http://arxiv.org/abs/2305.03648) | 论文提出了一种名为CLER的OCL方法，它利用等变任务进行自监督，避免了CSSL在OCL中的限制，并与连续学习相结合。 |
| [^10] | [Verifiable Learning for Robust Tree Ensembles.](http://arxiv.org/abs/2305.03626) | 本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。 |
| [^11] | [Optimizing Hyperparameters with Conformal Quantile Regression.](http://arxiv.org/abs/2305.03623) | 该论文提出了利用合服量化回归优化超参数，相比高斯过程，该方法对观测噪声做出最少的假设，更真实鲁棒。此外，作者还提出了在多保真度设置中聚合结果的方法，在实际任务中优于传统方法。 |
| [^12] | [Segmentation of fundus vascular images based on a dual-attention mechanism.](http://arxiv.org/abs/2305.03617) | 本文提出了基于双重注意机制的眼底血管图像分割方法，可以从空间和通道维度提取图像信息，并通过引入空间注意机制和Dropout层解决光照变化和不均匀对比度等问题，实验结果表明，该方法可以产生令人满意的结果。 |
| [^13] | [Differentially Private Topological Data Analysis.](http://arxiv.org/abs/2305.03609) | 本文尝试使用差分隐私实现拓扑数据分析并生成接近最优的私有持久图，提出使用 $L^1$-距离计算持久图并采用指数机制保护隐私，成功实现在隐私保护和数据分析之间的平衡。 |
| [^14] | [On the Optimality, Stability, and Feasibility of Control Barrier Functions: An Adaptive Learning-Based Approach.](http://arxiv.org/abs/2305.03608) | 本文提出了自适应多步控制屏障函数(AM-CBF)来解决当前控制屏障函数(CBF)的基本局限性，通过神经网络参数化类-$\mathcal{K}$函数并与强化学习策略一起训练，同时提出了一种新的“多步训练和单步执行”范式来减轻CBF的短视性。 |
| [^15] | [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.](http://arxiv.org/abs/2305.03598) | 本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。 |
| [^16] | [A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning.](http://arxiv.org/abs/2305.03582) | 本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习，该方法在中间表示上进行了静态与动态信息、模态特异与共同信息的分离，并且在实验中表现出优越性。 |
| [^17] | [Scope Restriction for Scalable Real-Time Railway Rescheduling: An Exploratory Study.](http://arxiv.org/abs/2305.03574) | 本文提出了一种限制重新调度范围的核心问题定义，旨在解决铁路重新调度问题，初步使用Flatland模拟环境得到结果。 |
| [^18] | [Model-free Reinforcement Learning of Semantic Communication by Stochastic Policy Gradient.](http://arxiv.org/abs/2305.03571) | 本论文利用随机策略梯度（SPG）强化学习，成功设计了一种无需通道模型的语义通信系统，能够传输意义而非精确版本，达到了信息速率节省的目的。 |
| [^19] | [A vector quantized masked autoencoder for audiovisual speech emotion recognition.](http://arxiv.org/abs/2305.03568) | 本文提出了一种特别为音视频言语自监督表示学习设计的矢量量化MAE模型，采用了基于离散音频和视觉言语表示的自监督范式，并在标准情感音视频言语数据集上取得了较好的效果。 |
| [^20] | [The geometry of financial institutions -- Wasserstein clustering of financial data.](http://arxiv.org/abs/2305.03565) | 本文提出了一种新的算法，Wasserstein聚类，用于处理金融机构的复杂数据，有效地解决了缺失值和基于特定特征识别聚类所面临的挑战。该算法可用于监管者的监管工作，并在其领域取得了良好的效果。 |
| [^21] | [Contrastive Graph Clustering in Curvature Spaces.](http://arxiv.org/abs/2305.03555) | 该论文提出了一种基于异构曲率空间的对比图聚类方法CONGREGATE，该方法在各种基准数据集上都取得了最先进的性能，展示了其有效性。 |
| [^22] | [Over-the-Air Federated Averaging with Limited Power and Privacy Budgets.](http://arxiv.org/abs/2305.03547) | 本文研究了一种有限功耗和隐私预算的差分隐私OTA联邦平均系统，通过对齐系数对梯度进行聚合并使用信道噪声来保护隐私，以提高学习性能。 |
| [^23] | [Random Smoothing Regularization in Kernel Gradient Descent Learning.](http://arxiv.org/abs/2305.03531) | 本文提出了一种随机平滑正则化的框架，能够自适应地、有效地学习属于经典Sobolev空间范围内的各种真实函数，通过引入噪声避免过拟合，该方法可以在较快的速度下实现最优收敛率。 |
| [^24] | [Exploring Softly Masked Language Modelling for Controllable Symbolic Music Generation.](http://arxiv.org/abs/2305.03530) | 本文探索了软掩模语言建模在符号音乐生成中的应用。使用变压器编码器架构，成功将SMLM应用于受限符号音乐生成。 |
| [^25] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^26] | [Can Large Language Models Transform Computational Social Science?.](http://arxiv.org/abs/2305.03514) | 本文研究了大型语言模型作为计算社会科学工具的潜力。虽然在分类任务上没有优势，但在自由形式编码任务上表现优异，今后可以作为零-shot检测工具进行使用， |
| [^27] | [ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs.](http://arxiv.org/abs/2305.03513) | ChatGraph通过将ChatGPT的知识转换为图形，提高了文本分类的可解释性和性能 |
| [^28] | [Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion.](http://arxiv.org/abs/2305.03509) | Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。 |
| [^29] | [CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing.](http://arxiv.org/abs/2305.03508) | 本研究旨在解决法律写作中引用值的鉴别问题，通过使用一个包含 178M 句子的标记数据集并测试多种深度学习模型的性能，结果表明专门针对该领域预训练的模型优于其他模型。 |
| [^30] | [Automatic Prompt Optimization with "Gradient Descent" and Beam Search.](http://arxiv.org/abs/2305.03495) | 在基于大型语言模型的自然语言处理中，使用梯度下降和 beam search 的自动提示优化方法可以自动改进提示，提高性能。 |
| [^31] | [Zoo Guide to Network Embedding.](http://arxiv.org/abs/2305.03474) | 本文章综述了网络嵌入文献和当前趋势的用户友好指南，为网络推理的基本问题提供了有效的应用，例如链接预测、节点分类和社区检测。 |
| [^32] | [A technical note on bilinear layers for interpretability.](http://arxiv.org/abs/2305.03452) | 本论文讨论了一种比标准的多层感知器更易于分析、同时表现更好的双线性层，这为深入的安全洞察提供了可能性。 |
| [^33] | [Adaptive Graph Convolutional Subspace Clustering.](http://arxiv.org/abs/2305.03414) | 本研究提出了自适应图卷积子空间聚类方法（AGCSC），将特征提取方法和系数矩阵约束相结合，实现更真实的子空间结构揭示，并在大量实验中证明了其有效性和效率。 |
| [^34] | [Domain-agnostic segmentation of thalamic nuclei from joint structural and diffusion MRI.](http://arxiv.org/abs/2305.03413) | 本研究提出了一种基于深度学习的方法，可以从任何分辨率的T1和扩散数据中分割出丘脑核，无需重新训练或微调，提高了丘脑核分割的准确性和普适性。 |
| [^35] | [GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering.](http://arxiv.org/abs/2305.03403) | 介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。 |
| [^36] | [Sparsifying Bayesian neural networks with latent binary variables and normalizing flows.](http://arxiv.org/abs/2305.03395) | 本论文介绍了一种新的方法来稀疏化贝叶斯神经网络，使用潜在二进制变量和归一化流，实现了网络在测试时的自动稀疏化，而且结果表明这个方法在准确性上能够与现有的稀疏化方法相媲美。 |
| [^37] | [Towards Effective Collaborative Learning in Long-Tailed Recognition.](http://arxiv.org/abs/2305.03378) | 本文提出了一种重新加权的蒸馏损失来处理长尾分布下的协作学习不平衡问题，并建立了一个有效的协作学习框架以提高模型的性能。 |
| [^38] | [The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation.](http://arxiv.org/abs/2305.03369) | MuSe 2023是一组共享任务，涉及三个当代多模态情感和情绪分析问题：模拟情感、跨文化幽默和个性化。参与者需要在各自的子挑战中预测情感目标、跨文化幽默和个性化的信号。 |
| [^39] | [A Survey on Offline Model-Based Reinforcement Learning.](http://arxiv.org/abs/2305.03360) | 本文是一篇关于离线基于模型的强化学习最新进展的综述性文章，讨论了该领域的最新概念、方法和挑战。文献综述介绍了当前离线基于模型的强化学习领域的关键文献，并重点讨论了如何解决分布变化问题。该领域面临的关键挑战也被讨论和总结。 |
| [^40] | [From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base.](http://arxiv.org/abs/2305.03356) | 本文提出了一种名为解析-执行-优化（Parse-Execute-Refine）的范式，通过向知识库问题应答模型演示执行中间推理步骤，可以提高复杂推理的能力。 |
| [^41] | [A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness.](http://arxiv.org/abs/2305.03355) | 本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。 |
| [^42] | [Reconstructing Training Data from Multiclass Neural Networks.](http://arxiv.org/abs/2305.03350) | 本文研究了从多类神经网络中重建训练数据的问题，比以往的二元分类情况更容易实现，并且发现在训练过程中使用权重衰减会增加样本重建的易受性。 |
| [^43] | [Generic and Robust Root Cause Localization for Multi-Dimensional Data in Online Service Systems.](http://arxiv.org/abs/2305.03331) | 本文提出了一种通用且鲁棒的PSqueeze多维数据的根本原因定位方法，该方法基于广义串扰效应和一种新的概率聚类方法和鲁棒启发式搜索方法，并在两个真实世界数据集中的5400个故障中验证了其性能优于基线。 |
| [^44] | [Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices.](http://arxiv.org/abs/2305.03308) | Tiny-PPG是一个用于边缘设备上实时检测PPG信号中运动伪影的轻量级深度神经网络，通过使用深度可分离卷积和空洞空间金字塔池化模块，以及通道注意机制，能够在平衡检测精度和速度的情况下实现最先进的检测效果，并可以在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。 |
| [^45] | [Decentralized diffusion-based learning under non-parametric limited prior knowledge.](http://arxiv.org/abs/2305.03295) | 本文提出在非参数情况下，仅通过相邻节点之间的信息传播，避免数据交换的分散扩散学习算法。 |
| [^46] | [FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding.](http://arxiv.org/abs/2305.03292) | 本文提出FedNC，一个联合学习的通信框架，结合了网络编码技术，能够提高系统的隐私、吞吐量和鲁棒性。 |
| [^47] | [Demystifying Softmax Gating in Gaussian Mixture of Experts.](http://arxiv.org/abs/2305.03288) | 本文提出了新的参数Vononoi损失函数并建立了MLE的收敛速度来解决高斯混合专家模型中的Softmax门控问题，研究表明该门控与高斯分布中的专家函数通过偏微分方程相互作用，是一个复杂依赖关系。 |
| [^48] | [Composite Motion Learning with Task Control.](http://arxiv.org/abs/2305.03286) | 该论文介绍了一种使用多个判别器在类 GAN 设置中直接学习来自多个参考动作的特定身体部位的解耦动作的深度学习方法，以实现复合和任务驱动的动作控制，同时考虑了多个任务的奖励和训练一个单一的多目标控制策略。 |
| [^49] | [Semantic Segmentation using Vision Transformers: A survey.](http://arxiv.org/abs/2305.03273) | 本文介绍了使用视觉变换器（ViT）进行语义分割的不同架构，讨论了ViT在分块划分方案方面的局限性，并回顾和比较了不同ViT架构的性能表现。ViT架构的崛起在计算机视觉任务中替代传统的卷积神经网络的趋势不断增强。 |
| [^50] | [Bayesian Reinforcement Learning with Limited Cognitive Load.](http://arxiv.org/abs/2305.03263) | 本文综述了限制认知负荷的贝叶斯强化学习的最新算法和理论成果，并讨论了这些思想如何应用于研究认知和行为科学中的问题。 |
| [^51] | [Data-driven and Physics Informed Modelling of Chinese Hamster Ovary Cell Bioreactors.](http://arxiv.org/abs/2305.03257) | 本文提出了一种基于数据和物理模型的混合方法，以学习中国仓鼠卵巢（CHO）细胞生物反应器的动态演化模型。 |
| [^52] | [PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors.](http://arxiv.org/abs/2305.03249) | 该论文提出了一种使用多个分部运动先验(PMP)动画化角色的方法，可以创造出不同的动作集合，并提供了采用广泛的分部先验训练代理的方法。 |
| [^53] | [Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts.](http://arxiv.org/abs/2305.03237) | 本文提出了一个上下文感知的OOD意图检测框架（Caro），用于模拟OOD意图检测任务中的多轮对话上下文，并在提取稳健的表示时删除与意图检测无关的多余信息。Caro在多个标准数据集上表现出最先进的性能，并超越了先前方法。 |
| [^54] | [A Survey on Out-of-Distribution Detection in NLP.](http://arxiv.org/abs/2305.03236) | 这篇论文首次综述了最新的OOD检测方法在自然语言处理中的应用。根据算法使用的数据，将方法分成三类，并介绍了相关数据集、应用和度量方法。 |
| [^55] | [Carbon Price Forecasting with Quantile Regression and Feature Selection.](http://arxiv.org/abs/2305.03224) | 研究使用分位回归和特征选择等方法进行炭价预测，显著提高了预测效果。 |
| [^56] | [Algorithms for Social Justice: Affirmative Action in Social Networks.](http://arxiv.org/abs/2305.03223) | 本文介绍了一个新的基于谱图理论的链接推荐算法ERA-Link，旨在缓解现有推荐算法带来的信息孤岛和社会成见，实现社交网络平台的社会正义目标。 |
| [^57] | [All models are local: time to replace external validation with recurrent local validation.](http://arxiv.org/abs/2305.03219) | 本文认为外部验证无法确保机器学习模型的安全性或实用性，提出了循环本地验证的MLOps启发式范式作为新的黄金标准，强调对各个本地部署的模型进行监测和更新，从而更好地对齐临床和医疗特定需求与机器学习模型验证策略，提高临床决策支持工具的安全性和实用性。 |
| [^58] | [AttentionViz: A Global View of Transformer Attention.](http://arxiv.org/abs/2305.03210) | 这篇论文介绍了AttentionViz，一种以联合嵌入为基础的交互式可视化工具，用于帮助研究人员理解Transformer中的自我注意机制。该方法使得可以全局分析多个输入序列的注意力模式，提高对模型的理解并通过多个应用场景和专家反馈提供新的交互见解。 |
| [^59] | [Enhancing Pashto Text Classification using Language Processing Techniques for Single And Multi-Label Analysis.](http://arxiv.org/abs/2305.03201) | 研究旨在建立一个普什图语文本的自动分类系统，最终通过应用多层感知器分类器获得了94％的平均测试准确率。 |
| [^60] | [Emulation Learning for Neuromimetic Systems.](http://arxiv.org/abs/2305.03196) | 本文提出了一种符合神经模仿模式的通用仿真问题，为了解决优化步骤的整数规划组合复杂性影响，提出了一种深度Q网络算法来学习轨迹和对信道中断具有优势；并且可以使用基于映射的迁移学习法来将模型应用于其他仿真问题。 |
| [^61] | [A Generative Modeling Framework for Inferring Families of Biomechanical Constitutive Laws in Data-Sparse Regimes.](http://arxiv.org/abs/2305.03184) | 该论文提出了一种利用生成式深度学习与贝叶斯推断相结合的方法，在数据稀疏情况下，能够有效地推断本构关系族群。 |
| [^62] | [Contrastive Learning for Sleep Staging based on Inter Subject Correlation.](http://arxiv.org/abs/2305.03178) | 该论文提出了基于主观间相关性的对比学习方法(MViTime)，用于睡眠分期中的跨个体问题，实现了最先进的性能。 |
| [^63] | [Deep Learning-Assisted Simultaneous Targets Sensing and Super-Resolution Imaging.](http://arxiv.org/abs/2305.03177) | 本研究展示了一种多功能深度神经网络，用于重建超材料目标交互系统中的目标信息，能同时感知目标的数量和介电常数，生成高精度的超分辨率图像。 |
| [^64] | [New Adversarial Image Detection Based on Sentiment Analysis.](http://arxiv.org/abs/2305.03173) | 本文提出了一种新的对抗性样本检测器，通过使用情感分析并检测对神经网络的隐藏层特征图造成的影响逐渐显现来检测对抗性样本，克服了现有技术的局限，且实验结果表明其在识别最新的对抗性攻击方面优于现有技术。 |
| [^65] | [A CSI Dataset for Wireless Human Sensing on 80 MHz Wi-Fi Channels.](http://arxiv.org/abs/2305.03170) | 该文提出了一个CSI数据集，提供了80MHz Wi-fi信道上与环境、人员和Wi-Fi硬件相关的数据，这为开发领域自适应算法提供了重要支持。 |
| [^66] | [Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records.](http://arxiv.org/abs/2305.03169) | 该论文使用机器学习算法来识别结构化数据中的敏感变量，以便便于去身份化过程。该算法可以解决不同数据集PHI字段异质性的问题。 |
| [^67] | [G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer.](http://arxiv.org/abs/2305.03153) | G-MATT是一个结合数据驱动模型与化学知识的化学感知回溯合成预测框架，在分层SMILES语法树输入的基础上采用树到序列变换器架构，能够显著提高单步回溯合成的准确率。 |
| [^68] | [Communication-Efficient Graph Neural Networks with Probabilistic Neighborhood Expansion Analysis and Caching.](http://arxiv.org/abs/2305.03152) | 本文针对分布式设置中GNN的小批量训练和推断提出了一种缓存策略，基于顶点的多跳邻域抽样中的点包含概率分析（VIP），可以显著减少通信量而不影响预测准确度。 |
| [^69] | [CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning.](http://arxiv.org/abs/2305.03148) | CAMEL提出了使用嵌入式DRAM作为主要存储介质的方法来解决设备端学习中存储和计算过程中占用大量内存的问题，从而使AI模型更加高效。 |
| [^70] | [Influence of various text embeddings on clustering performance in NLP.](http://arxiv.org/abs/2305.03144) | 研究探索了不同文本嵌入对聚类算法（KMeans、单链接聚合等级、DBSCAN和HDBSCAN）性能的影响，并应用于评论聚类领域。 |
| [^71] | [Towards Invertible Semantic-Preserving Embeddings of Logical Formulae.](http://arxiv.org/abs/2305.03143) | 本研究提出了一种基于 Graph VAE 的深度学习模型ISPE，能够在连续空间中保持语义的嵌入逻辑公式，并且该方法是可逆的。在语句补全和推理等任务上的表现优于现有方法。 |
| [^72] | [Contrastive losses as generalized models of global epistasis.](http://arxiv.org/abs/2305.03136) | 对比损失函数可以用于提取全局上位联系模型所隐含的稀疏潜在函数，这种方法可能为蛋白质工程和相关领域的适应性函数推断提供有用的通用框架。 |
| [^73] | [ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review.](http://arxiv.org/abs/2305.03123) | 本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。 |
| [^74] | [Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability.](http://arxiv.org/abs/2305.03100) | 本文提出了一种统一的框架，用于游戏理论驱动的归因和k阶交互方法，通过假设，可以在连续输入设置中得到唯一全面的特征交互解释，即协同作用。 |
| [^75] | [A Bootstrap Algorithm for Fast Supervised Learning.](http://arxiv.org/abs/2305.03099) | 本文探讨了一种Bootstrap算法，该算法可以通过自助法、重抽样和线性回归来更新隐藏层的加权连接，从而达到更快的收敛速度。 |
| [^76] | [Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion.](http://arxiv.org/abs/2305.03098) | 本文提出了一种利用深度多元图像完成方法进行乳腺扫描异常定位的无监督方法，该方法通过探索完成的多元性来提高评估标准的精度。 |
| [^77] | [Federated Ensemble-Directed Offline Reinforcement Learning.](http://arxiv.org/abs/2305.03097) | 本文开发了一种名为FEDORA的联邦集成指导的离线强化学习算法，通过集成学习方法提炼客户群体的集体智慧，显著优于其他方法，包括在合并的数据汇总中进行离线强化学习，在各种复杂的连续控制环境和真实世界数据集中进行了实验。 |
| [^78] | [Explaining dark matter halo density profiles with neural networks.](http://arxiv.org/abs/2305.03077) | 使用可解释的神经网络将暗物质晕的演化历史与其密度分布相连接，网络发现超过渗透半径的轮廓由一个单一参数描述，这展示了在复杂的天体物理数据集中机器协助科学发现的潜力。 |
| [^79] | [Neuro-symbolic model for cantilever beams damage detection.](http://arxiv.org/abs/2305.03063) | 本文提出了一种神经符号模型用于悬臂梁损伤检测，该模型通过将卷积网络的处理能力与逻辑查询交互控制相结合，不仅能够准确检测损伤，而且还能够提供解释和定位，使其在操作条件下更可靠和可信。 |
| [^80] | [Plug-and-Play Multilingual Few-shot Spoken Words Recognition.](http://arxiv.org/abs/2305.03058) | PLiX是一种跨语种的关键词检测系统，使用少样本学习实现了对未见过的口语单词的识别，可适用于低资源语言。 |
| [^81] | [Leveraging gradient-derived metrics for data selection and valuation in differentially private training.](http://arxiv.org/abs/2305.02942) | 研究如何在隐私增强技术下，利用梯度信息识别训练中感兴趣的数据样本进行数据选择和估价。 |
| [^82] | [Hierarchical Transformer for Scalable Graph Learning.](http://arxiv.org/abs/2305.02866) | 本文提出了分层可扩展图Transformer (HSGT)用于解决图表示学习中的规模问题和上下文信息捕获不足问题，通过构建多尺度图分层结构，HSGT实现了对大型图的快速和内存高效处理，并在基准数据集上展现了卓越的性能表现。 |
| [^83] | [Cuttlefish: Low-rank Model Training without All The Tuning.](http://arxiv.org/abs/2305.02538) | Cuttlefish 是一种新的自动化低秩训练方法，可以有效地减少可训练参数的数量，而无需调整因式分解超参数即可实现加速，可生成比完全秩训练小多达 5.6 倍的模型。 |
| [^84] | [Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner.](http://arxiv.org/abs/2305.02470) | 该论文介绍了一种可提高探测行星信号分类器性能的框架，称为多重性增强分类器，基于现有的分类器并使用多重性信息来验证69个新的系外行星。 |
| [^85] | [Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge.](http://arxiv.org/abs/2305.02459) | 本文提出并探究了基于转移和主动学习的稀有类问题的解决方案，包括利用在密切相关任务上训练的模型和评估获取策略来解决共振检测的稀有类问题，并且发现了一种名为PRC的有效的策略来指导注释。 |
| [^86] | [Unsupervised Improvement of Audio-Text Cross-Modal Representations.](http://arxiv.org/abs/2305.01864) | 本研究探索了无监督的方法来改进跨模态音频-文本表征学习，通过使用领域特定的筛选和软标注对比性损失，成功提高了零-shot分类性能。 |
| [^87] | [Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features.](http://arxiv.org/abs/2305.01807) | 本研究首次从理论上研究了基于协方差神经网络的可转移性，证明了当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。 |
| [^88] | [LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application.](http://arxiv.org/abs/2305.01095) | 该论文提出了一种基于LSTM的ACC系统，可以学习过去的驾驶经验，适应和预测新情况，并且在模拟驾驶环境中表现良好。 |
| [^89] | [Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization.](http://arxiv.org/abs/2304.13761) | 通过独热编码和正则化提高梯度提升决策树的鲁棒性，研究表明对带有$L_1$或$L_2$正则化的线性回归形式进行拟合可提高GBDT模型的鲁棒性。 |
| [^90] | [Directed Chain Generative Adversarial Networks.](http://arxiv.org/abs/2304.13131) | 本文提出了有向链生成对抗网络（DC-GANs），使用邻域过程作为关键步骤生成同样分布的多模态时间序列数据。 |
| [^91] | [Exploring the Connection between Robust and Generative Models.](http://arxiv.org/abs/2304.04033) | 本文探究鲁棒性判别分类器与生成模型之间的联系，并发现在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量，提出了一种名为高能量PGD的新攻击。 |
| [^92] | [Prevalence and major risk factors of non-communicable diseases: A Hospital-based Cross-Sectional Study in Dhaka, Bangladesh.](http://arxiv.org/abs/2303.04808) | 该研究调查了孟加拉国达卡市成年患者中多种非传染性疾病的患病率及其风险因素，其中心血管疾病最为普遍。男性参与者较女性更容易患有心血管疾病，但糖尿病不具有性别倾向。CVD和DM都会随着年龄的增长而增加。患有肥胖症的住院病人占五分之一。 |
| [^93] | [On the Implicit Bias of Linear Equivariant Steerable Networks.](http://arxiv.org/abs/2303.04198) | 本文的研究发现了基于参数化预测器的梯度流收敛于最大余量定义的唯一群不变分类器的方向, 同时，我们还发现了可旋网络在较少变情况下相对于不变网络的改善余量与广义界的优势。 |
| [^94] | [Language Models are Few-shot Learners for Prognostic Prediction.](http://arxiv.org/abs/2302.12692) | 本研究探索了语言模型在免疫治疗预后预测中的应用，研究了小样本学习面临的挑战，对比了基线和语言模型的有效性，并发现在准确度方面有显著的改进，突出了自然语言处理在临床研究中改善早期检测和干预不同疾病的潜力。 |
| [^95] | [Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications.](http://arxiv.org/abs/2302.05763) | 本研究通过收集单个用户数据并在后处理中合并数据的方法，实现了多用户活动的识别，有望用于人机协作领域。 |
| [^96] | [Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.](http://arxiv.org/abs/2302.03857) | 该研究提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法，能够有效地加速对抗性对比学习（ACL）并维持其强鲁棒性和泛化性能。 |
| [^97] | [Toward Large Kernel Models.](http://arxiv.org/abs/2302.02605) | 本文提出了一种构建大规模通用核模型的方法，这解决了传统核机器中模型大小与数据大小相互耦合的问题，使其能够在大数据集上进行训练。 |
| [^98] | [Can In-context Learners Learn a Reasoning Concept from Demonstrations?.](http://arxiv.org/abs/2212.01692) | 本文介绍了一种概念性少样本学习方法，以帮助在场学习者学习新技能。通过选择与预测示例共享可能信息的演示，这个方法可以在模型记忆独立的情况下区分模型的在场学习能力。 |
| [^99] | [xTrimoABFold: De novo Antibody Structure Prediction without MSA.](http://arxiv.org/abs/2212.00735) | xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。 |
| [^100] | [A Comprehensive Survey on Enterprise Financial Risk Analysis from Big Data Perspective.](http://arxiv.org/abs/2211.14997) | 本文从大数据角度综述了企业财务风险分析的研究现状，回顾了250多篇代表性文章。 |
| [^101] | [Multi-Step Short-Term Wind Speed Prediction with Rank Pooling and Fast Fourier Transformation.](http://arxiv.org/abs/2211.14434) | 本文提出了一种新型深度混合模型LR-FFT-RP-MLP/LSTM，利用排序池化和快速傅里叶变换进行多步短期风速预测，实验结果表明其优于几个基准模型并达到最先进水平。 |
| [^102] | [Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks.](http://arxiv.org/abs/2211.10024) | 本文提出了一种基于嵌入方法的SNAFUE技术，用于寻找复制/粘贴攻击，并使用该技术对ImageNet分类器进行了测试并发现许多易于描述的漏洞。 |
| [^103] | [Predicting air quality via multimodal AI and satellite imagery.](http://arxiv.org/abs/2211.00780) | 本文旨在利用卫星和地面数据创建一个多模态机器学习模型，以在没有监测站的情况下预测空气污染物分布和改变社会工业行为，并提供了一个新的欧洲污染监测数据集。 |
| [^104] | [Improving Graph Neural Networks with Learnable Propagation Operators.](http://arxiv.org/abs/2210.17224) | 本文通过引入可学习的通道加权因子ω，学习和混合多个平滑和锐化滤波器，改进了图神经网络（GNNs），避免了全局平滑现象。 |
| [^105] | [Rethinking the Event Coding Pipeline with Prompt Entailment.](http://arxiv.org/abs/2210.05257) | 提出了一种名为PR-ENT的事件编码方法，用于从非结构化全文事件描述中提取事件类型。该方法利用预训练语言模型填充事件描述中的模板，并在文本隐含关系任务中选择答案候选项，同时保持了高准确性和资源效率。 |
| [^106] | [FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings.](http://arxiv.org/abs/2210.04620) | 这篇论文介绍了FLamby，一个跨设备联合学习的逼真健康医疗数据集和基准，以帮助促进跨设备FL的应用和研究。 |
| [^107] | [Tree species classification from hyperspectral data using graph-regularized neural networks.](http://arxiv.org/abs/2208.08675) | 本文提出了一种基于图正则化神经网络的树种分类方法，该方法在稀疏注释的数据集上使用标签传播技术，能够达到高精度的分类效果；同时，该方法在半监督方法方面具有竞争力。 |
| [^108] | [Deep Learning for Classification of Thyroid Nodules on Ultrasound: Validation on an Independent Dataset.](http://arxiv.org/abs/2207.13765) | 该研究使用深度学习算法对甲状腺结节超声图像进行分类，结果与放射科医生相似，并验证了该算法的泛化能力。 |
| [^109] | [BigIssue: A Realistic Bug Localization Benchmark.](http://arxiv.org/abs/2207.10739) | BigIssue是一个真实的漏洞定位基准测试，旨在提高模型的漏洞定位能力和自动程序修复的性能，从而帮助开发人员编写更好的代码。 |
| [^110] | [Multi-scale Sinusoidal Embeddings Enable Learning on High Resolution Mass Spectrometry Data.](http://arxiv.org/abs/2207.02980) | 本文介绍了一种新方法，通过对高分辨质谱数据进行多尺度正弦嵌入训练，展示了一种新的对 MS2 数据的光谱库搜索和新的化学性质预测任务。同时还介绍了一种新颖的高吞吐实验以及医药化学家关注的 10 种化学性质的预测。在这些任务中，本文提出了一种具有最先进性能的模型。 |
| [^111] | [Deep Multi-View Semi-Supervised Clustering with Sample Pairwise Constraints.](http://arxiv.org/abs/2206.04949) | 本文提出了一种新型的深度多视图半监督聚类方法（DMSC），它通过共同优化多种损失来提高聚类性能，包括多视图聚类损失、半监督成对约束损失和多个自动编码器重构损失。其中，KL散度基础的多视图聚类损失被特别强调用于多视图数据的共同表示，以达到异构特征优化、多视图加权和聚类预测的目的。 |
| [^112] | [U-NO: U-shaped Neural Operators.](http://arxiv.org/abs/2204.11127) | 本文提出了U-NO，一种U形记忆增强架构，允许更深的神经算子，通过利用函数预测中的问题结构，在解决偏微分方程方面表现出快速训练、数据效率和对超参数选择的鲁棒性。 |
| [^113] | [Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes.](http://arxiv.org/abs/2204.03376) | 本论文研究了离线强化学习在控制1型糖尿病患者血糖水平中的应用，采用BCQ、CQL和TD3-BC算法有效地管理血糖水平，克服了在线学习过程的不稳定性和不安全性。 |
| [^114] | [Investigating the Properties of Neural Network Representations in Reinforcement Learning.](http://arxiv.org/abs/2203.15955) | 本文探究了深度强化学习系统学习到的表示特征及其对迁移学习的支持能力，并在一个像素导航环境中考虑了具有不同辅助损失的深度Q学习代理。 |
| [^115] | [ADATIME: A Benchmarking Suite for Domain Adaptation on Time Series Data.](http://arxiv.org/abs/2203.08321) | ADATIME是一个用于时间序列领域自适应的评估套件，其所提供的标准化骨干神经网络架构和数据集、现实的模型选择方法可以无监督地进行领域自适应。 |
| [^116] | [PyNET-QxQ: An Efficient PyNET Variant for QxQ Bayer Pattern Demosaicing in CMOS Image Sensors.](http://arxiv.org/abs/2203.04314) | PyNET-QxQ是一种用于QxQ Bayer CFA模式的轻量级解析模型，参数少于原始PyNET的2.5％，并采用了一种称为渐进蒸馏的知识蒸馏方法来训练网络。实验表明，PyNET-QxQ在保持图像质量的同时，具有较高的效率和较低的计算成本。 |
| [^117] | [Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features.](http://arxiv.org/abs/2203.01881) | 从自监督学习模型中提取区分特征，并使用它们压缩表示空间，提出了一种Q-Score自监督表示质量分数，可以可靠地预测线性评估期间的错误分类。 |
| [^118] | [Survey and Systematization of 3D Object Detection Models and Methods.](http://arxiv.org/abs/2201.09354) | 本文就3D目标检测领域进行了全面的调查和系统化研究，提出了一种实用框架用于比较3D目标检测方法，并帮助研究人员和实践者快速了解该领域。 |
| [^119] | [Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models.](http://arxiv.org/abs/2112.03860) | 该论文提出了一种使用可微数据相关层进行重新参数化和高斯化潜在张量的方法，以约束逆问题为获得高保真度的分布内解决方案，有效解决深度生成模型逆问题中潜在张量偏离期望高斯分布的问题。 |
| [^120] | [LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training.](http://arxiv.org/abs/2112.01404) | 本文提出了一种基于少样本的逻辑知识条件下文本生成的统一框架LOGEN，通过自训练和基于内容和结构一致性抽样伪逻辑形式，实现了在少量样本下的文本生成。 |
| [^121] | [PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding.](http://arxiv.org/abs/2111.08792) | 本文提出了PredProp方法，它通过随机梯度下降和自适应加权参数更新来优化预测编码网络中的权重和状态。通过使用传播误差协方差实现近似自然梯度下降，使得PredProp在测试中表现良好，比Adam表现更优。此外，PredProp可以通过误差精度提高权重参数的优化效果，并且在层次结构中可以分解精度需求。 |
| [^122] | [CaloFlow II: Even Faster and Still Accurate Generation of Calorimeter Showers with Normalizing Flows.](http://arxiv.org/abs/2110.11377) | CaloFlow v2是一种基于归一化流的高保真量能器淋浴仿真生成模型，相对于原版将淋浴生成速度进一步提高了500倍，并且在保真度方面大大超过了以前的最先进水平。 |
| [^123] | [Generating Symbolic Reasoning Problems with Transformer GANs.](http://arxiv.org/abs/2110.10054) | 本文使用Transformer GAN生成符号推理问题的训练数据，生成的数据可用于替代真实训练数据，并且可用于修改目标分布，达到更好的训练效果。 |
| [^124] | [CaloFlow: Fast and Accurate Generation of Calorimeter Showers with Normalizing Flows.](http://arxiv.org/abs/2106.05285) | CaloFlow提出了一种利用正则化流技术实现快速准确模拟能量量能器淋浴，相对于其他最新方法具有多种优势，包括更高的准确性和更稳定的训练，并引入了新的质量评估指标。 |
| [^125] | [Posterior Regularization on Bayesian Hierarchical Mixture Clustering.](http://arxiv.org/abs/2105.06903) | 本文提出了一种后验正则化方法来改进贝叶斯分层混合聚类模型，在每个层级对节点实施最大间隔约束以增强集群的分离。 |
| [^126] | [Learning Node Representations against Perturbations.](http://arxiv.org/abs/2008.11416) | 本文讨论如何在GNN中针对扰动学习节点表示，并提出了稳定-可识别GNN反对扰动 (SIGNNAP) 模型以无监督形式学习可靠的节点表示。 |
| [^127] | [Fast and Robust Rank Aggregation against Model Misspecification.](http://arxiv.org/abs/1905.12341) | 本文提出了CoarsenRank，其具有鲁棒性，适用于模型错误特化。它采用由粗到精的方案来处理用户收集的信息，并利用排名空间中的几何结构来更好地模拟聚合过程。在实验中验证了CoarsenRank的有效性。 |

# 详细

[^1]: 公平性审计的统计推断方法

    Statistical Inference for Fairness Auditing. (arXiv:2305.03712v1 [stat.ME])

    [http://arxiv.org/abs/2305.03712](http://arxiv.org/abs/2305.03712)

    本文介绍了一种公平性审计的统计推断方法，可用于评估黑盒模型在敏感子群体上的表现，并通过自举方法限制多个群体表现差异，方法通用性强且适用面广。

    

    在将黑盒模型用于高风险问题之前，评估模型在敏感子群体上的表现非常重要。本文提出了一种“公平性审计”的任务框架，并通过多重假设检验将其表述。我们展示了如何使用自举方法以统计保证的方式同时限制多个群体表现差异。我们的方法适用于几乎任何性能度量或群体公平性标准，并且可以处理非常丰富的、甚至是无限的子群体集合。此外，我们将方法推广到了多个潜在重叠标准下的模型表现审计。我们在合成和真实数据集上展示了我们方法的有效性。

    Before deploying a black-box model in high-stakes problems, it is important to evaluate the model's performance on sensitive subpopulations. For example, in a recidivism prediction task, we may wish to identify demographic groups for which our prediction model has unacceptably high false positive rates or certify that no such groups exist. In this paper, we frame this task, often referred to as "fairness auditing," in terms of multiple hypothesis testing. We show how the bootstrap can be used to simultaneously bound performance disparities over a collection of groups with statistical guarantees. Our methods can be used to flag subpopulations affected by model underperformance, and certify subpopulations for which the model performs adequately. Crucially, our audit is model-agnostic and applicable to nearly any performance metric or group fairness criterion. Our methods also accommodate extremely rich -- even infinite -- collections of subpopulations. Further, we generalize beyond subpo
    
[^2]: 数据集压缩是医疗数据共享的银弹吗？

    Is dataset condensation a silver bullet for healthcare data sharing?. (arXiv:2305.03711v1 [cs.LG])

    [http://arxiv.org/abs/2305.03711](http://arxiv.org/abs/2305.03711)

    数据集压缩为医疗数据共享提供了一种既保护了隐私又保存了实用程序的新方法。

    

    对于医疗数据共享，保护个人信息至关重要，但目前并没有一个万无一失的方法。本文研究了一种深度学习新技术——数据集压缩，以在医疗人工智能研究中共享数据，结果表明前景广阔。压缩后的数据摘要了原始记录，不可逆地隐藏了个体级别的信息，达到了真正的去识别化，允许自由共享。此外，压缩数据中保留了原始深度学习实用程序，且数据量较小且模型收敛加速。在PhysioNet-2012中，20个样本的压缩数据能够使深度模型达到了80.3%的死亡预测测试AUC（而原始记录为5120个样本的85.8%），这一发现也适用于MIMIC-III和Coswara等数据集。我们还通过理论分析和经验证明了DC的隐私保护性。数据集压缩为共享医疗数据提供了一种新的方法，既保护了隐私又保存了实用程序。

    Safeguarding personal information is paramount for healthcare data sharing, a challenging issue without any silver bullet thus far. We study the prospect of a recent deep-learning advent, dataset condensation (DC), in sharing healthcare data for AI research, and the results are promising. The condensed data abstracts original records and irreversibly conceals individual-level knowledge to achieve a bona fide de-identification, which permits free sharing. Moreover, the original deep-learning utilities are well preserved in the condensed data with compressed volume and accelerated model convergences. In PhysioNet-2012, a condensed dataset of 20 samples can orient deep models attaining 80.3% test AUC of mortality prediction (versus 85.8% of 5120 original records), an inspiring discovery generalised to MIMIC-III and Coswara datasets. We also interpret the inhere privacy protections of DC through theoretical analysis and empirical evidence. Dataset condensation opens a new gate to sharing h
    
[^3]: 健康数据民主化和信息泄露预防的数据编码研究

    Data Encoding For Healthcare Data Democratisation and Information Leakage Prevention. (arXiv:2305.03710v1 [cs.LG])

    [http://arxiv.org/abs/2305.03710](http://arxiv.org/abs/2305.03710)

    本文提出了一种通过不可逆数据编码实现医疗数据民主化和信息泄露预防的方法。利用随机投影和随机量子编码实现密集和时间序列数据的编码框架，在保持原始数据语义信息的同时有效提取信息瓶颈原则，为深度学习在医疗保健领域的发展提供了新思路。

    

    数据民主化不足和已训练模型的信息泄露妨碍了深度学习在医疗保健领域的健全发展和广泛应用。本文认为，不可逆数据编码可以提供有效解决方案，实现数据民主化，同时又不违反对医疗数据和临床模型的隐私约束。经过理论分析，本文提出所需编码框架的特征，并利用随机投影和随机量子编码实现密集和长期时间序列数据的编码框架。实验结果表明，模型使用编码时间序列数据有效提取信息的瓶颈原则，同时保持了原始数据的语义信息。

    The lack of data democratization and information leakage from trained models hinder the development and acceptance of robust deep learning-based healthcare solutions. This paper argues that irreversible data encoding can provide an effective solution to achieve data democratization without violating the privacy constraints imposed on healthcare data and clinical models. An ideal encoding framework transforms the data into a new space where it is imperceptible to a manual or computational inspection. However, encoded data should preserve the semantics of the original data such that deep learning models can be trained effectively. This paper hypothesizes the characteristics of the desired encoding framework and then exploits random projections and random quantum encoding to realize this framework for dense and longitudinal or time-series data. Experimental evaluation highlights that models trained on encoded time-series data effectively uphold the information bottleneck principle and hen
    
[^4]: 传单广告上的细粒度产品分类研究

    Fine-Grained Product Classification on Leaflet Advertisements. (arXiv:2305.03706v1 [cs.CV])

    [http://arxiv.org/abs/2305.03706](http://arxiv.org/abs/2305.03706)

    本文介绍了首个公开细粒度产品识别数据集，通过图像和文本结合的模型可以提高对于细节相似的产品分类精确度，最终模型准确率为96.4%。

    

    本文描述了一个基于传单图片的首个公开细粒度产品识别数据集。通过从不同欧洲零售商收集广告传单，我们提供了832个类别中的41600个手动注释的产品图像。进一步，我们尝试了三种不同的方法来进行细粒度产品分类任务，分别是基于图像、基于文本以及基于图像和文本的分类。"基于文本的分类"方法直接使用从传单中提取的商品文本信息。我们展示了将图像和文本作为输入的结合方法可以提高对于外观相似但难以区分的产品的分类性能。最终模型的准确率为96.4%，Top-3得分为99.2%。我们将代码公开在 https://github.com/ladwigd/Leaflet-Product-Classification 上。

    In this paper, we describe a first publicly available fine-grained product recognition dataset based on leaflet images. Using advertisement leaflets, collected over several years from different European retailers, we provide a total of 41.6k manually annotated product images in 832 classes. Further, we investigate three different approaches for this fine-grained product classification task, Classification by Image, by Text, as well as by Image and Text. The approach "Classification by Text" uses the text extracted directly from the leaflet product images. We show, that the combination of image and text as input improves the classification of visual difficult to distinguish products. The final model leads to an accuracy of 96.4% with a Top-3 score of 99.2%. We release our code at https://github.com/ladwigd/Leaflet-Product-Classification.
    
[^5]: 从泰森多边形中挖掘偏见-目标对齐

    Mining bias-target Alignment from Voronoi Cells. (arXiv:2305.03691v1 [cs.LG])

    [http://arxiv.org/abs/2305.03691](http://arxiv.org/abs/2305.03691)

    本文提出了一种偏见无关的方法来减轻深度神经网络中偏差的影响，在目标类别上量化“偏见对齐/不对齐”，以避免通过网络传播偏见-目标对齐信息。

    

    尽管进行了大量的研究，深度神经网络仍然容易出现偏差:这引起了他们公正性的担忧并限制了它们的泛化能力。在本文中，我们提出了一种偏见无关的方法来减轻深度神经网络中偏差的影响。与传统的去偏见方法不同，我们依靠一个指标来量化目标类别上的“偏见对齐/不对齐”，并利用此信息来避免通过网络传播偏见-目标对齐信息。我们在几个常用的去偏见数据集上进行实验，并将我们的方法与有监督和偏见特定的方法进行比较。我们的结果表明，尽管存在多个样本中的偏见，所提出的方法在不存在偏见的情况下实现了与最先进的有监督方法相当的性能。

    Despite significant research efforts, deep neural networks are still vulnerable to biases: this raises concerns about their fairness and limits their generalization. In this paper, we propose a bias-agnostic approach to mitigate the impact of bias in deep neural networks. Unlike traditional debiasing approaches, we rely on a metric to quantify ``bias alignment/misalignment'' on target classes, and use this information to discourage the propagation of bias-target alignment information through the network. We conduct experiments on several commonly used datasets for debiasing and compare our method to supervised and bias-specific approaches. Our results indicate that the proposed method achieves comparable performance to state-of-the-art supervised approaches, although it is bias-agnostic, even in presence of multiple biases in the same sample.
    
[^6]: 关于神经网络原像近似的研究

    On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])

    [http://arxiv.org/abs/2305.03686](http://arxiv.org/abs/2305.03686)

    本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。

    

    神经网络验证主要关注局部鲁棒性，然而，通常需要知道给定属性是否在整个输入域内全局成立，如果不成立，则需要知道属性成立的输入比例是多少。尽管精确的原像生成可以构建神经网络的等价表示，但在规模上是难以处理的。本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似。我们的算法通过将输入区域划分为子区域，其中神经网络松弛边界变得更紧，迭代地最小化体积逼近误差。我们进一步采用采样和可微体积逼近来优先划分区域，并优化松弛的参数，从而实现更快的改进和更高的压缩度。

    Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa
    
[^7]: 利用OpenAI GPT模型的检索增强的胸部X射线报告生成

    Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])

    [http://arxiv.org/abs/2305.03660](http://arxiv.org/abs/2305.03660)

    该研究提出了一种检索增强的方法，利用对比预训练的视觉语言模型的多模态对齐嵌入来检索相应的候选放射学文本，并使用通用领域生成模型来生成报告，可抑制虚构的生成，实现更好的临床指标。

    

    我们提出了一种名为Retrieval Augmented Generation (RAG) 的方法来自动生成放射学报告，该方法利用对比预训练的视觉语言模型的多模态对齐嵌入来检索相应的候选放射学文本，并使用像OpenAI text-davinci-003、gpt-3.5-turbo和gpt-4这样的通用领域生成模型来生成报告。该方法可以抑制虚构的生成并提供指令跟随能力，以我们所需的格式生成报告内容。我们的方法实现了更好的临床指标，BERTScore为0.2865（Δ+25.88%），Semb Score为0.4026（Δ+6.31%）。我们的方法可以广泛应用于不同的临床设置，因为它允许增强自动生成的放射学报告过程，同时具备适合该设置的相关内容的能力。

    We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the abilit
    
[^8]: 白盒多目标对话生成对抗攻击

    White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])

    [http://arxiv.org/abs/2305.03655](http://arxiv.org/abs/2305.03655)

    该论文提出了一种新的白盒多目标对话生成对抗攻击方法，DGSlow。通过平衡生成准确性和长度两个目标，DGSlow利用生成更长的输出来提高攻击效果。

    

    预训练的转换器在最先进的对话生成系统中很受欢迎。 然而，这种语言模型容易受到各种对抗样本的攻击，这在传统任务（如文本分类）中已经得到了研究，这激发了我们对它们在DG系统中的鲁棒性的好奇心。 其中一个主要的挑战是攻击DG模型的时候，对当前句子的扰动几乎不会降低响应的准确性，因为未改变的聊天记录也会被考虑进行决策。我们观察到，通过精心制作对抗样本来迫使生成更长的输出，有利于攻击的有效性 - 生成的响应通常是不相关、冗长和重复的。因此，我们提出了一种名为DGSlow的白盒多目标攻击方法。具体来说，DGSlow通过基于梯度的多目标优化器平衡两个目标 - 生成准确度和长度，并使用自适应方法实施攻击。

    Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness -- the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adapti
    
[^9]: 关于等变正则化对于鲁棒在线连续学习的有效性研究

    On the Effectiveness of Equivariant Regularization for Robust Online Continual Learning. (arXiv:2305.03648v1 [cs.LG])

    [http://arxiv.org/abs/2305.03648](http://arxiv.org/abs/2305.03648)

    论文提出了一种名为CLER的OCL方法，它利用等变任务进行自监督，避免了CSSL在OCL中的限制，并与连续学习相结合。

    

    人类能够增量学习，而神经网络会在学习新任务时灾难性地忘记以前学到的知识。连续学习方法试图通过在训练过程中促进知识向旧任务（向后转移）和未来任务（向前转移）的传递来弥补这个差距。最近的研究表明，自监督能够产生多才多艺的模型，可以很好地推广到不同的下游任务。然而，对比自监督学习（CSSL）是一种流行的自监督技术，在在线连续学习中的效果有限。OCL只允许输入数据集的一次迭代，而CSSL的低样本效率阻碍了它在输入数据流上的使用。在这项工作中，我们提出了一种名为等变正则化连续学习（CLER）的OCL方法，它利用等变任务进行自监督，避免了CSSL的限制。我们的方法代表了将等变知识与CL相结合的第一次尝试，并且可以很容易地集成到现有的OCL架构中。

    Humans can learn incrementally, whereas neural networks forget previously acquired information catastrophically. Continual Learning (CL) approaches seek to bridge this gap by facilitating the transfer of knowledge to both previous tasks (backward transfer) and future ones (forward transfer) during training.  Recent research has shown that self-supervision can produce versatile models that can generalize well to diverse downstream tasks. However, contrastive self-supervised learning (CSSL), a popular self-supervision technique, has limited effectiveness in online CL (OCL). OCL only permits one iteration of the input dataset, and CSSL's low sample efficiency hinders its use on the input data-stream.  In this work, we propose Continual Learning via Equivariant Regularization (CLER), an OCL approach that leverages equivariant tasks for self-supervision, avoiding CSSL's limitations. Our method represents the first attempt at combining equivariant knowledge with CL and can be easily integrat
    
[^10]: 鲁棒决策树集成的可验证学习

    Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])

    [http://arxiv.org/abs/2305.03626](http://arxiv.org/abs/2305.03626)

    本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。

    

    在测试时间内验证机器学习模型对抗攻击的鲁棒性是一个重要的研究问题。不幸的是，先前的研究确定，对于决策树集成，这个问题是 NP-hard ，因此对于特定的输入来说是不可解的。在本文中，我们确定了一类受限决策树集成，称为 large-spread 集成，其允许在多项式时间内运行安全验证算法。然后，我们提出了一种新方法，称为可验证学习，该方法倡导训练这种易于验证的受限模型类。我们通过设计一种新的训练算法，从标记数据中自动学习 large-spread 决策树集成来展示这种方法的益处，从而使其能够在多项式时间内进行安全验证。公开可用数据集上的实验结果证实，使用我们的算法训练的 large-spread 集成可以在几秒钟内使用标准半定编程求解器进行验证，同时对抗当前最先进的攻击具有竞争力的性能。

    Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
    
[^11]: 利用合服量化回归优化超参数

    Optimizing Hyperparameters with Conformal Quantile Regression. (arXiv:2305.03623v1 [cs.LG])

    [http://arxiv.org/abs/2305.03623](http://arxiv.org/abs/2305.03623)

    该论文提出了利用合服量化回归优化超参数，相比高斯过程，该方法对观测噪声做出最少的假设，更真实鲁棒。此外，作者还提出了在多保真度设置中聚合结果的方法，在实际任务中优于传统方法。

    

    很多现有的超参数优化算法依赖于基于模型的优化工具，它们可以学习代理模型来指导搜索。高斯过程是默认的代理模型，因为它们可以捕捉不确定性，但是它们对观测噪声做出强烈的假设，这在实践中可能是不合理的。在这项工作中，我们提出利用合服量化回归，该方法对观测噪声做出最少的假设，因此更真实和鲁棒地建模目标函数，并在实证基准上快速实现超参数优化收敛。为了在多保真度设置中应用我们的方法，我们提出了一种简单而有效的技术，通过聚合不同资源水平上观察到的结果，在许多实际任务中优于传统方法。

    Many state-of-the-art hyperparameter optimization (HPO) algorithms rely on model-based optimizers that learn surrogate models of the target function to guide the search. Gaussian processes are the de facto surrogate model due to their ability to capture uncertainty but they make strong assumptions about the observation noise, which might not be warranted in practice. In this work, we propose to leverage conformalized quantile regression which makes minimal assumptions about the observation noise and, as a result, models the target function in a more realistic and robust fashion which translates to quicker HPO convergence on empirical benchmarks. To apply our method in a multi-fidelity setting, we propose a simple, yet effective, technique that aggregates observed results across different resource levels and outperforms conventional methods across many empirical tasks.
    
[^12]: 基于双重注意机制的眼底血管图像分割

    Segmentation of fundus vascular images based on a dual-attention mechanism. (arXiv:2305.03617v1 [eess.IV])

    [http://arxiv.org/abs/2305.03617](http://arxiv.org/abs/2305.03617)

    本文提出了基于双重注意机制的眼底血管图像分割方法，可以从空间和通道维度提取图像信息，并通过引入空间注意机制和Dropout层解决光照变化和不均匀对比度等问题，实验结果表明，该方法可以产生令人满意的结果。

    

    精确地分割视网膜眼底图像中的血管对于早期筛查、诊断和评估某些眼部疾病至关重要。然而，这些图像中存在明显的光照变化和不均匀对比度，这使得分割变得非常具挑战性。因此，本文采用了一种注意融合机制，该机制结合了Transformer构建的通道注意和空间注意机制，从空间和通道维度提取视网膜眼底图像的信息。为了消除编码器图像中的噪声，引入了一个空间注意机制在跳跃连接中。此外，使用Dropout层随机舍弃一些神经元，以防止神经网络过度拟合并提高其泛化性能。在公共数据集DERIVE、STARE和CHASEDB1上进行了实验。结果显示，与一些最近的视网膜眼底图像分割算法相比，我们的方法产生了令人满意的结果。

    Accurately segmenting blood vessels in retinal fundus images is crucial in the early screening, diagnosing, and evaluating some ocular diseases. However, significant light variations and non-uniform contrast in these images make segmentation quite challenging. Thus, this paper employ an attention fusion mechanism that combines the channel attention and spatial attention mechanisms constructed by Transformer to extract information from retinal fundus images in both spatial and channel dimensions. To eliminate noise from the encoder image, a spatial attention mechanism is introduced in the skip connection. Moreover, a Dropout layer is employed to randomly discard some neurons, which can prevent overfitting of the neural network and improve its generalization performance. Experiments were conducted on publicly available datasets DERIVE, STARE, and CHASEDB1. The results demonstrate that our method produces satisfactory results compared to some recent retinal fundus image segmentation algor
    
[^13]: 差分隐私在拓扑数据分析中的应用

    Differentially Private Topological Data Analysis. (arXiv:2305.03609v1 [stat.ML])

    [http://arxiv.org/abs/2305.03609](http://arxiv.org/abs/2305.03609)

    本文尝试使用差分隐私实现拓扑数据分析并生成接近最优的私有持久图，提出使用 $L^1$-距离计算持久图并采用指数机制保护隐私，成功实现在隐私保护和数据分析之间的平衡。

    

    本文是首篇尝试使用差分隐私实现拓扑数据分析并生成接近最优的私有持久图。我们通过瓶颈距离分析持久图的灵敏度，发现常用的 \v{C}ech 复形的灵敏度并不会随着样本量 $n$ 的增加而降低，这使得 \v{C}ech 复形持久图难以隐私化。作为替代方法，我们提出使用 $L^1$-距离来计算持久图，发现其灵敏度为 $O(1/n)$。基于灵敏度分析，我们提出采用指数机制，其效用函数定义为 $L^1$-DTM 持久图的瓶颈距离。同时，我们还推导出了我们隐私机制的精度上下界；得到的界限表明我们的机制隐私误差接近最优。我们展示了我们的私有持久图方法的性能。

    This paper is the first to attempt differentially private (DP) topological data analysis (TDA), producing near-optimal private persistence diagrams. We analyze the sensitivity of persistence diagrams in terms of the bottleneck distance, and we show that the commonly used \v{C}ech complex has sensitivity that does not decrease as the sample size $n$ increases. This makes it challenging for the persistence diagrams of \v{C}ech complexes to be privatized. As an alternative, we show that the persistence diagram obtained by the $L^1$-distance to measure (DTM) has sensitivity $O(1/n)$. Based on the sensitivity analysis, we propose using the exponential mechanism whose utility function is defined in terms of the bottleneck distance of the $L^1$-DTM persistence diagrams. We also derive upper and lower bounds of the accuracy of our privacy mechanism; the obtained bounds indicate that the privacy error of our mechanism is near-optimal. We demonstrate the performance of our privatized persistence
    
[^14]: 关于控制屏障函数的最优性、稳定性和可行性：一种自适应学习方法的探索

    On the Optimality, Stability, and Feasibility of Control Barrier Functions: An Adaptive Learning-Based Approach. (arXiv:2305.03608v1 [cs.LG])

    [http://arxiv.org/abs/2305.03608](http://arxiv.org/abs/2305.03608)

    本文提出了自适应多步控制屏障函数(AM-CBF)来解决当前控制屏障函数(CBF)的基本局限性，通过神经网络参数化类-$\mathcal{K}$函数并与强化学习策略一起训练，同时提出了一种新的“多步训练和单步执行”范式来减轻CBF的短视性。

    

    安全性一直是应用于实际场景的基于学习的方法部署的关键问题。为了解决这个问题，控制屏障函数(CBF)及其变体已受到广泛关注，用于关键安全控制。然而，由于CBF的短视单步性质以及缺乏设计类-$\mathcal{K}$函数的原则方法，目前的CBF仍存在基本局限性：最优性、稳定性和可行性。本文提出了一种新的和统一的方法来解决这些限制，即自适应多步控制屏障函数(AM-CBF)，其中我们将类-$\mathcal{K}$函数通过神经网络进行参数化，并与强化学习策略一起训练。此外，为了减轻短视性，我们提出一种新的“多步训练和单步执行”范式，使CBF具有远见性，而执行仍然解决单步凸二次规划问题。我们的方法进行了评估。

    Safety has been a critical issue for the deployment of learning-based approaches in real-world applications. To address this issue, control barrier function (CBF) and its variants have attracted extensive attention for safety-critical control. However, due to the myopic one-step nature of CBF and the lack of principled methods to design the class-$\mathcal{K}$ functions, there are still fundamental limitations of current CBFs: optimality, stability, and feasibility. In this paper, we proposed a novel and unified approach to address these limitations with Adaptive Multi-step Control Barrier Function (AM-CBF), where we parameterize the class-$\mathcal{K}$ function by a neural network and train it together with the reinforcement learning policy. Moreover, to mitigate the myopic nature, we propose a novel \textit{multi-step training and single-step execution} paradigm to make CBF farsighted while the execution remains solving a single-step convex quadratic program. Our method is evaluated 
    
[^15]: NLI4CT：面向临床试验报告的多证据自然语言推理

    NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])

    [http://arxiv.org/abs/2305.03598](http://arxiv.org/abs/2305.03598)

    本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。

    

    如何解释和检索用于支持临床决策的医学证据？多年来，积累下来的临床试验报告包含了发展个性化医学所必需的信息。然而，为了找到最佳的实验治疗证据，手动检查超过400,000个临床试验报告是实际上不可行的。自然语言推理（NLI）提供了一个潜在的解决方案，通过允许可扩展计算文本蕴含关系。然而，现有的NLI模型在生物医学语料库上表现不佳，之前发布的数据集无法捕捉CTR推理的全部复杂性。在本文中，我们提出了一种新的资源，以推进关于CTR推理的NLI研究。该资源包括两个主要任务。首先，确定自然语言陈述和CTR之间的推理关系。其次，检索支持事实以证明预测的关系。我们提供了NLI4CT，一个基于CTR的语料库。

    How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
    
[^16]: 一种用于音视频语音表示学习的多模态动态变分自编码器

    A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning. (arXiv:2305.03582v1 [cs.SD])

    [http://arxiv.org/abs/2305.03582](http://arxiv.org/abs/2305.03582)

    本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习，该方法在中间表示上进行了静态与动态信息、模态特异与共同信息的分离，并且在实验中表现出优越性。

    

    本文提出了一种多模态动态自编码器（MDVAE），用于无监督音视频语音表示学习。潜在空间被构造为将在各个模态之间共享的潜在动态因素与每个模态特定的因素区分开来。同时，引入一个静态潜变量来编码音视频语音序列中随时间恒定的信息。模型在一个音视频情感语音数据集上进行无监督训练分两个阶段。在第一阶段，对于每个模态，首先独立学习一个向量量化自编码器（VQ-VAE），而没有时间建模。第二阶段则在向量量化自编码器（VQ-VAEs）的中间表示上学习MDVAE模型。该方法的实验结果表明，在语音表示学习方面，提出的方法优于现有方法。

    In this paper, we present a multimodal \textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how a
    
[^17]: 可扩展实时铁路重新调度的范围限制：一项探索性研究

    Scope Restriction for Scalable Real-Time Railway Rescheduling: An Exploratory Study. (arXiv:2305.03574v1 [math.OC])

    [http://arxiv.org/abs/2305.03574](http://arxiv.org/abs/2305.03574)

    本文提出了一种限制重新调度范围的核心问题定义，旨在解决铁路重新调度问题，初步使用Flatland模拟环境得到结果。

    

    本文描述了一项铁路重新调度问题的探索性研究，旨在刺激未来的研究。我们提出了一种核心问题的定义，即在响应扰动时仅限制需要重新调度的列车，从而在时间和空间上限制范围，而不是采用地理范围的分解方法。在这种情况下，困难在于定义一个可以预测给定扰动会影响哪些列车服务子集的范围规定器。我们报告了使用Flatland模拟环境得出的初步结果，突出了这个想法的潜力和挑战。我们提供了一个可扩展的游乐场开源实现，基于Flatland铁路环境和Answer-Set编程。

    With the aim to stimulate future research, we describe an exploratory study of a railway rescheduling problem. A widely used approach in practice and state of the art is to decompose these complex problems by geographical scope. Instead, we propose defining a core problem that restricts a rescheduling problem in response to a disturbance to only trains that need to be rescheduled, hence restricting the scope in both time and space. In this context, the difficulty resides in defining a scoper that can predict a subset of train services that will be affected by a given disturbance. We report preliminary results using the Flatland simulation environment that highlights the potential and challenges of this idea. We provide an extensible playground open-source implementation based on the Flatland railway environment and Answer-Set Programming.
    
[^18]: 基于随机策略梯度的模型无关语义通信强化学习

    Model-free Reinforcement Learning of Semantic Communication by Stochastic Policy Gradient. (arXiv:2305.03571v1 [eess.SP])

    [http://arxiv.org/abs/2305.03571](http://arxiv.org/abs/2305.03571)

    本论文利用随机策略梯度（SPG）强化学习，成功设计了一种无需通道模型的语义通信系统，能够传输意义而非精确版本，达到了信息速率节省的目的。

    

    受机器学习工具在无线通信方面的成功启发，韦弗（Weaver）于1949年提出的语义通信概念引起了人们的关注。它打破了香农经典的设计范例，旨在传输消息的意义，即语义，而不是精确版本，从而实现信息速率节省。在这项工作中，我们应用了随机策略梯度（SPG）来设计一种基于强化学习的语义通信系统，不需要已知或可微分通道模型，这是实际部署的关键步骤。此外，我们从最大化接收和目标变量之间的互信息出发，激发了将SPG用于经典和语义通信的动机。数值结果表明，我们的方法达到了与基于重新参数化技巧的模型感知方法相当的性能，尽管收敛速度有所降低。

    Motivated by the recent success of Machine Learning tools in wireless communications, the idea of semantic communication by Weaver from 1949 has gained attention. It breaks with Shannon's classic design paradigm by aiming to transmit the meaning, i.e., semantics, of a message instead of its exact version, allowing for information rate savings. In this work, we apply the Stochastic Policy Gradient (SPG) to design a semantic communication system by reinforcement learning, not requiring a known or differentiable channel model a crucial step towards deployment in practice. Further, we motivate the use of SPG for both classic and semantic communication from the maximization of the mutual information between received and target variables. Numerical results show that our approach achieves comparable performance to a model-aware approach based on the reparametrization trick, albeit with a decreased convergence rate.
    
[^19]: 一种用于音视频言语情感识别的矢量量化掩码自编码器

    A vector quantized masked autoencoder for audiovisual speech emotion recognition. (arXiv:2305.03568v1 [cs.SD])

    [http://arxiv.org/abs/2305.03568](http://arxiv.org/abs/2305.03568)

    本文提出了一种特别为音视频言语自监督表示学习设计的矢量量化MAE模型，采用了基于离散音频和视觉言语表示的自监督范式，并在标准情感音视频言语数据集上取得了较好的效果。

    

    尽管全面监督模型已被证明对于音视频言语情感识别（SER）非常有效，但标记数据的有限性仍然是该领域的主要挑战。为了解决这个问题，自监督学习方法，如掩码自编码器（MAEs），已成为潜在解决方案。本文提出了一种特别为音视频言语自监督表示学习设计的矢量量化MAE模型（VQ-MAE-AV）。与现有的依赖于原始音视频言语数据处理的多模态MAEs不同，该方法采用了基于两个预先训练的矢量量化变分自编码器学习的离散音频和视觉言语表示的自监督范式。实验结果表明，该方法在VoxCeleb2数据库上进行预训练，并在标准情感音视频言语数据集上进行微调，优于现有的音视频SER方法。

    While fully-supervised models have been shown to be effective for audiovisual speech emotion recognition (SER), the limited availability of labeled data remains a major challenge in the field. To address this issue, self-supervised learning approaches, such as masked autoencoders (MAEs), have gained popularity as potential solutions. In this paper, we propose the VQ-MAE-AV model, a vector quantized MAE specifically designed for audiovisual speech self-supervised representation learning. Unlike existing multimodal MAEs that rely on the processing of the raw audiovisual speech data, the proposed method employs a self-supervised paradigm based on discrete audio and visual speech representations learned by two pre-trained vector quantized variational autoencoders. Experimental results show that the proposed approach, which is pre-trained on the VoxCeleb2 database and fine-tuned on standard emotional audiovisual speech datasets, outperforms the state-of-the-art audiovisual SER methods.
    
[^20]: 金融机构的几何形态--金融数据的Wasserstein聚类

    The geometry of financial institutions -- Wasserstein clustering of financial data. (arXiv:2305.03565v1 [stat.ML])

    [http://arxiv.org/abs/2305.03565](http://arxiv.org/abs/2305.03565)

    本文提出了一种新的算法，Wasserstein聚类，用于处理金融机构的复杂数据，有效地解决了缺失值和基于特定特征识别聚类所面临的挑战。该算法可用于监管者的监管工作，并在其领域取得了良好的效果。

    

    不断增加的各种有趣对象的细节和大数据的可用性使得有必要开发将这些信息压缩成代表性和可理解的地图的方法。金融监管是一个展示这种需求的领域，因为监管机构需要从金融机构获取多样化的数据，有时是高度细粒度的，以监督和评估他们的活动。然而，处理和分析这样的数据可能是一项艰巨的任务，尤其是考虑到处理缺失值和基于特定特征识别聚类所面临的挑战。为了解决这些挑战，我们提出了一种适用于概率分布的Lloyd算法变体，并使用广义Wasserstein重心构建表示不同对象上的给定数据的度量空间，从而应对金融监管背景下监管者面临的具体挑战。我们相信这种方法在金融监管领域具有实用价值。

    The increasing availability of granular and big data on various objects of interest has made it necessary to develop methods for condensing this information into a representative and intelligible map. Financial regulation is a field that exemplifies this need, as regulators require diverse and often highly granular data from financial institutions to monitor and assess their activities. However, processing and analyzing such data can be a daunting task, especially given the challenges of dealing with missing values and identifying clusters based on specific features.  To address these challenges, we propose a variant of Lloyd's algorithm that applies to probability distributions and uses generalized Wasserstein barycenters to construct a metric space which represents given data on various objects in condensed form. By applying our method to the financial regulation context, we demonstrate its usefulness in dealing with the specific challenges faced by regulators in this domain. We beli
    
[^21]: 曲率空间中对比图聚类

    Contrastive Graph Clustering in Curvature Spaces. (arXiv:2305.03555v1 [cs.LG])

    [http://arxiv.org/abs/2305.03555](http://arxiv.org/abs/2305.03555)

    该论文提出了一种基于异构曲率空间的对比图聚类方法CONGREGATE，该方法在各种基准数据集上都取得了最先进的性能，展示了其有效性。

    

    图聚类一直是一个长期研究的话题，在近年来的深度学习方法中取得了显着的成功。尽管如此，我们观察到仍然存在一些重要问题尚未得到解决。一方面，从几何角度进行图聚类具有吸引力，但很少涉及到它的几何聚类空间。另一方面，对比学习可以提高深度图聚类的效果，但通常会在图增强或难例挖掘方面面临困难。为了填补这一空白，我们重新思考图聚类问题，并尝试首次引入异构曲率空间到图聚类问题中。相应地，我们提出了一个名为CONGREGATE的新颖的端到端对比图聚类模型，用Ricci曲率解决几何图聚类。为了支持几何聚类，我们构建了一个理论上支撑的异构曲率空间框架，可以捕捉图的各种曲率特征。我们提出的方法在几个基准数据集上实现了最先进的性能，展示了我们基于曲率的几何图聚类方法的有效性。

    Graph clustering is a longstanding research topic, and has achieved remarkable success with the deep learning methods in recent years. Nevertheless, we observe that several important issues largely remain open. On the one hand, graph clustering from the geometric perspective is appealing but has rarely been touched before, as it lacks a promising space for geometric clustering. On the other hand, contrastive learning boosts the deep graph clustering but usually struggles in either graph augmentation or hard sample mining. To bridge this gap, we rethink the problem of graph clustering from geometric perspective and, to the best of our knowledge, make the first attempt to introduce a heterogeneous curvature space to graph clustering problem. Correspondingly, we present a novel end-to-end contrastive graph clustering model named CONGREGATE, addressing geometric graph clustering with Ricci curvatures. To support geometric clustering, we construct a theoretically grounded Heterogeneous Curv
    
[^22]: 有限功耗和隐私预算的OTA联邦平均

    Over-the-Air Federated Averaging with Limited Power and Privacy Budgets. (arXiv:2305.03547v1 [cs.LG])

    [http://arxiv.org/abs/2305.03547](http://arxiv.org/abs/2305.03547)

    本文研究了一种有限功耗和隐私预算的差分隐私OTA联邦平均系统，通过对齐系数对梯度进行聚合并使用信道噪声来保护隐私，以提高学习性能。

    

    为了共同克服无线联邦学习(FL)的通信瓶颈和隐私泄漏问题，本文研究了一种具有有限总功率预算的差分隐私OTA联邦平均(DP-OTA-FedAvg)系统。使用DP-OTA-FedAvg，梯度通过一个对齐系数进行对齐并在空中聚合，同时利用信道噪声来保护隐私。我们的目标是在满足总功率和隐私约束条件下，联合设计设备调度、对齐系数和联邦平均(FedAvg)的聚合轮数，以改善学习性能。我们首先基于差分隐私(DP)提供了隐私分析，以量化对隐私保护的影响。此外，为了研究设备调度、对齐系数和全局聚合数量对学习过程的影响，我们对DP-OTA-FedAvg的收敛性进行了分析。

    To jointly overcome the communication bottleneck and privacy leakage of wireless federated learning (FL), this paper studies a differentially private over-the-air federated averaging (DP-OTA-FedAvg) system with a limited sum power budget. With DP-OTA-FedAvg, the gradients are aligned by an alignment coefficient and aggregated over the air, and channel noise is employed to protect privacy. We aim to improve the learning performance by jointly designing the device scheduling, alignment coefficient, and the number of aggregation rounds of federated averaging (FedAvg) subject to sum power and privacy constraints. We first present the privacy analysis based on differential privacy (DP) to quantify the impact of the alignment coefficient on privacy preservation in each communication round. Furthermore, to study how the device scheduling, alignment coefficient, and the number of the global aggregation affect the learning process, we conduct the convergence analysis of DP-OTA-FedAvg in the cas
    
[^23]: 核梯度下降学习中的随机平滑正则化

    Random Smoothing Regularization in Kernel Gradient Descent Learning. (arXiv:2305.03531v1 [stat.ML])

    [http://arxiv.org/abs/2305.03531](http://arxiv.org/abs/2305.03531)

    本文提出了一种随机平滑正则化的框架，能够自适应地、有效地学习属于经典Sobolev空间范围内的各种真实函数，通过引入噪声避免过拟合，该方法可以在较快的速度下实现最优收敛率。

    

    随机平滑数据增强是一种独特的正则化形式，可以通过向输入数据引入噪声来防止过拟合，鼓励模型学习更广泛的特征。尽管在各种应用中都取得了成功，但随机平滑的正则化能力缺乏系统的研究。在本文中，我们旨在通过提出一个随机平滑正则化的框架，能够自适应地、有效地学习属于经典 Sobolev 空间范围内的各种真实函数。具体而言，我们研究了两种基础的函数空间：低固有维度的 Sobolev 空间，其中包括 $D$ 维欧几里德空间或低维子流形作为特例，以及具有张量结构的混合平滑 Sobolev 空间。通过使用随机平滑正则化作为新型卷积平滑核，我们可以在这些情况下实现最优收敛率。

    Random smoothing data augmentation is a unique form of regularization that can prevent overfitting by introducing noise to the input data, encouraging the model to learn more generalized features. Despite its success in various applications, there has been a lack of systematic study on the regularization ability of random smoothing. In this paper, we aim to bridge this gap by presenting a framework for random smoothing regularization that can adaptively and effectively learn a wide range of ground truth functions belonging to the classical Sobolev spaces. Specifically, we investigate two underlying function spaces: the Sobolev space of low intrinsic dimension, which includes the Sobolev space in $D$-dimensional Euclidean space or low-dimensional sub-manifolds as special cases, and the mixed smooth Sobolev space with a tensor structure. By using random smoothing regularization as novel convolution-based smoothing kernels, we can attain optimal convergence rates in these cases using a ke
    
[^24]: 探索软掩模语言建模在可控符号音乐生成中的应用

    Exploring Softly Masked Language Modelling for Controllable Symbolic Music Generation. (arXiv:2305.03530v1 [cs.SD])

    [http://arxiv.org/abs/2305.03530](http://arxiv.org/abs/2305.03530)

    本文探索了软掩模语言建模在符号音乐生成中的应用。使用变压器编码器架构，成功将SMLM应用于受限符号音乐生成。

    

    本文介绍了将软掩模语言建模（SMLM）应用于符号音乐生成的早期探索。SMLM可视为掩模语言建模（MLM）的一种推广，其中输入集合的每个元素可以是部分已知的，而不是已知或未知的。我们使用变压器编码器架构展示了将SMLM应用于受限符号音乐生成的结果。可在https://erl-j.github.io/smlm-web-supplement/上找到若干音频示例。

    This document presents some early explorations of applying Softly Masked Language Modelling (SMLM) to symbolic music generation. SMLM can be seen as a generalisation of masked language modelling (MLM), where instead of each element of the input set being either known or unknown, elements can be partly known. We demonstrate some results of applying SMLM to constrained symbolic music generation using a transformer encoder architecture. Several audio examples are available at https://erl-j.github.io/smlm-web-supplement/
    
[^25]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^26]: 大型语言模型能否改变计算社会科学？

    Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])

    [http://arxiv.org/abs/2305.03514](http://arxiv.org/abs/2305.03514)

    本文研究了大型语言模型作为计算社会科学工具的潜力。虽然在分类任务上没有优势，但在自由形式编码任务上表现优异，今后可以作为零-shot检测工具进行使用，

    

    ChatGPT等大型语言模型(LLMs)能够成功地在许多语言处理任务中进行零-shot操作（无需训练数据）。如果这种能力也适用于对说服力和政治意识形态等社会现象的编码，那么LLMs就可以有效地改变计算社会科学(CSS)。本研究提供了使用LLMs作为CSS工具的路线图。为此，我们提供了一组优秀的提示实践以及一个广泛的评估流程，以测量13种语言模型在24个代表性的CSS基准测试上的零-shot性能。在分类任务上，LLMs无法超越最佳微调模型，但仍然与人类达成了公平的协议水平。在自由形式的编码任务（生成）上，LLMs生成的解释常常超过了工作者的黄金参考的质量。我们得出结论，今天的LLMs可以通过两种方式从根本上增强CSS研究流程：(1)作为零-shot检测工具进行无缝工作。

    Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot d
    
[^27]: ChatGraph: 通过将ChatGPT的知识转换为图形来实现可解释的文本分类

    ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])

    [http://arxiv.org/abs/2305.03513](http://arxiv.org/abs/2305.03513)

    ChatGraph通过将ChatGPT的知识转换为图形，提高了文本分类的可解释性和性能

    

    ChatGPT作为最近推出的大型语言模型（LLM），在各种自然语言处理（NLP）任务中展现出卓越的性能。然而，存在两个主要限制阻碍了它的潜在应用：（1）在下游任务上微调的不灵活性和（2）在决策过程中缺乏可解释性。为了解决这些限制，我们提出了一种新颖的框架，利用ChatGPT的能力来进行特定任务（如文本分类），同时提高其可解释性。

    ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing Cha
    
[^28]: Diffusion Explainer：用于文本到图像稳定扩散的可视化解释工具

    Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])

    [http://arxiv.org/abs/2305.03509](http://arxiv.org/abs/2305.03509)

    Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。

    

    基于扩散的生成模型通过创造逼真的图像而获得了全球关注。然而，它们复杂的内部结构和操作往往使得非专业人员难以理解。我们提出了 Diffusion Explainer，这是第一个交互式可视化工具，用于解释稳定扩散如何将文本提示转化为图像。Diffusion Explainer紧密地将稳定扩散的复杂组件的视觉概述与其潜在操作的详细说明相结合，通过动画和交互元素使用户可以流畅地在多个抽象级别之间过渡。通过比较由两个相关文本提示引导的图像表示的演变来指导精细时间步长，用户可以发现提示对图像生成的影响。Diffusion Explainer在用户的Web浏览器中本地运行，无需安装或专门的硬件，扩大了公众对现代人工智能技术的教育获取。

    Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
    
[^29]: CiteCaseLAW: 用于法律辅助写作的判例法引用值检测

    CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing. (arXiv:2305.03508v1 [cs.CL])

    [http://arxiv.org/abs/2305.03508](http://arxiv.org/abs/2305.03508)

    本研究旨在解决法律写作中引用值的鉴别问题，通过使用一个包含 178M 句子的标记数据集并测试多种深度学习模型的性能，结果表明专门针对该领域预训练的模型优于其他模型。

    

    在法律文件撰写中，正确引用案例法和其他来源以证明声明和论点是其中的一个关键要素。理解法律领域并识别适当的引用上下文或值得引用的句子是需要昂贵的手动注释的具有挑战性的任务。法律术语、语义和高度特异性的存在使得法律语言变得复杂，从而使任何相关的法律任务难以自动化。本研究关注的是引用值鉴别的问题。它旨在成为当今引文推荐系统中的初始步骤，以减轻提取足够的引文上下文的负担。为此，我们从 Caselaw 访问项目 (CAP) 中引入了一个包含 178M 句子的标记数据集，用于法律领域中的引文值检测。在这个全新的数据集上，我们检查了各种深度学习模型的性能。专门针对该领域预训练的模型往往优于其他模型。

    In legal document writing, one of the key elements is properly citing the case laws and other sources to substantiate claims and arguments. Understanding the legal domain and identifying appropriate citation context or cite-worthy sentences are challenging tasks that demand expensive manual annotation. The presence of jargon, language semantics, and high domain specificity makes legal language complex, making any associated legal task hard for automation. The current work focuses on the problem of citation-worthiness identification. It is designed as the initial step in today's citation recommendation systems to lighten the burden of extracting an adequate set of citation contexts. To accomplish this, we introduce a labeled dataset of 178M sentences for citation-worthiness detection in the legal domain from the Caselaw Access Project (CAP). The performance of various deep learning models was examined on this novel dataset. The domain-specific pre-trained model tends to outperform other
    
[^30]: 基于“梯度下降”与 beam search 的自动提示优化

    Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])

    [http://arxiv.org/abs/2305.03495](http://arxiv.org/abs/2305.03495)

    在基于大型语言模型的自然语言处理中，使用梯度下降和 beam search 的自动提示优化方法可以自动改进提示，提高性能。

    

    大型语言模型（LLM）在通用智能方面展现了出色性能，但其能力仍高度依赖于手写的提示，需要大量的试错尝试。我们提出了一个简单而非参数化的解决方案——自动提示优化（APO），其灵感来自于使用数值梯度下降自动改进提示。

    Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
    
[^31]: Zoo Guide to Network Embedding.（网络嵌入动物园指南）

    Zoo Guide to Network Embedding. (arXiv:2305.03474v1 [cs.SI])

    [http://arxiv.org/abs/2305.03474](http://arxiv.org/abs/2305.03474)

    本文章综述了网络嵌入文献和当前趋势的用户友好指南，为网络推理的基本问题提供了有效的应用，例如链接预测、节点分类和社区检测。

    

    网络已经成为了数据和复杂系统的极为成功的建模方式。然而，作为组合对象，网络通常没有固有的坐标，也不典型地位于一个环境空间中。为网络分配嵌入空间的过程在过去几十年中引起了广泛关注，并且已被有效地应用于网络推理的基本问题，例如链接预测、节点分类和社区检测。本文回顾了网络嵌入文献和当前趋势的用户友好指南，使读者能够在这些主题的充满活力的研究活动中浏览方法和方法的复杂领域。

    Networks have provided extremely successful models of data and complex systems. Yet, as combinatorial objects, networks do not have in general intrinsic coordinates and do not typically lie in an ambient space. The process of assigning an embedding space to a network has attracted lots of interest in the past few decades, and has been efficiently applied to fundamental problems in network inference, such as link prediction, node classification, and community detection. In this review, we provide a user-friendly guide to the network embedding literature and current trends in this field which will allow the reader to navigate through the complex landscape of methods and approaches emerging from the vibrant research activity on these subjects.
    
[^32]: 关于可解释性的双线性层技术注释

    A technical note on bilinear layers for interpretability. (arXiv:2305.03452v1 [cs.LG])

    [http://arxiv.org/abs/2305.03452](http://arxiv.org/abs/2305.03452)

    本论文讨论了一种比标准的多层感知器更易于分析、同时表现更好的双线性层，这为深入的安全洞察提供了可能性。

    

    神经网络具有超越神经元数量的表示特征的能力，这使得解释它们变得具有挑战性。这种现象被称为叠加效应，已经激发了寻找比具有元素级激活函数的标准多层感知器（MLP）更易于解释的架构的努力。在这个注释中，我研究了双线性层，这是一种数学上更容易分析而且同时表现比标准 MLP 更好的 MLP 层的类型。虽然它们是其输入的非线性函数，但我证明双线性层可以仅使用线性操作和三阶张量来表示。我们可以将双线性层的这种表达式集成到转换器电路的数学框架中，该数学框架以前仅适用于仅具有注意力的转换器。这些结果表明，相对于当前的架构，双线性层数学上更易于分析，因此可能通过允许我们更深入地讨论来为更深入的安全洞察力作出贡献。

    The ability of neural networks to represent more features than neurons makes interpreting them challenging. This phenomenon, known as superposition, has spurred efforts to find architectures that are more interpretable than standard multilayer perceptrons (MLPs) with elementwise activation functions. In this note, I examine bilinear layers, which are a type of MLP layer that are mathematically much easier to analyze while simultaneously performing better than standard MLPs. Although they are nonlinear functions of their input, I demonstrate that bilinear layers can be expressed using only linear operations and third order tensors. We can integrate this expression for bilinear layers into a mathematical framework for transformer circuits, which was previously limited to attention-only transformers. These results suggest that bilinear layers are easier to analyze mathematically than current architectures and thus may lend themselves to deeper safety insights by allowing us to talk more f
    
[^33]: 自适应图卷积子空间聚类

    Adaptive Graph Convolutional Subspace Clustering. (arXiv:2305.03414v1 [cs.LG])

    [http://arxiv.org/abs/2305.03414](http://arxiv.org/abs/2305.03414)

    本研究提出了自适应图卷积子空间聚类方法（AGCSC），将特征提取方法和系数矩阵约束相结合，实现更真实的子空间结构揭示，并在大量实验中证明了其有效性和效率。

    

    光谱型子空间聚类算法在许多子空间聚类应用中表现出很好的性能。现有的光谱型子空间聚类算法要么专注于设计重构系数矩阵的约束条件，要么使用特征提取方法找到原始数据样本的潜在特征。本文受到图卷积网络的启发，使用图卷积技术同时开发特征提取方法和系数矩阵约束。我们在提出的算法中迭代和自适应地更新图卷积算子。因此，我们将该方法称为自适应图卷积子空间聚类（AGCSC）。我们声称，通过使用AGCSC，原始数据样本的聚合特征表示适合于子空间聚类，系数矩阵可以更真实地揭示原始数据集的子空间结构。最后，进行了大量子空间聚类实验，以验证AGCSC的有效性和效率。

    Spectral-type subspace clustering algorithms have shown excellent performance in many subspace clustering applications. The existing spectral-type subspace clustering algorithms either focus on designing constraints for the reconstruction coefficient matrix or feature extraction methods for finding latent features of original data samples. In this paper, inspired by graph convolutional networks, we use the graph convolution technique to develop a feature extraction method and a coefficient matrix constraint simultaneously. And the graph-convolutional operator is updated iteratively and adaptively in our proposed algorithm. Hence, we call the proposed method adaptive graph convolutional subspace clustering (AGCSC). We claim that by using AGCSC, the aggregated feature representation of original data samples is suitable for subspace clustering, and the coefficient matrix could reveal the subspace structure of the original data set more faithfully. Finally, plenty of subspace clustering ex
    
[^34]: 基于结构和扩散MRI的通用领域下丘脑核分割

    Domain-agnostic segmentation of thalamic nuclei from joint structural and diffusion MRI. (arXiv:2305.03413v1 [eess.IV])

    [http://arxiv.org/abs/2305.03413](http://arxiv.org/abs/2305.03413)

    本研究提出了一种基于深度学习的方法，可以从任何分辨率的T1和扩散数据中分割出丘脑核，无需重新训练或微调，提高了丘脑核分割的准确性和普适性。

    

    人类丘脑是大脑内高度连接的皮层下灰质结构，包括数十个具有不同功能和连接性的核。这些核因疾病而受到不同程度的影响，因此越来越多的人对其进行MRI研究。虽然有工具可以从1mm T1扫描中分割出丘脑，但是边界的对比度太低而无法生成可靠的分割。一些工具尝试将扩散MRI中的信息整合到分割中以优化这些边界，但是这些方法在不同扩散MRI获取时不具有普适性。在这里，我们提出了第一个可以从任何分辨率的T1和扩散数据中分割丘脑核的CNN，而无需重新训练或微调。我们的方法基于公共组织学的丘脑核图谱和具有高质量扩散数据的银标准分割的最新贝叶斯自适应分割工具。

    The human thalamus is a highly connected subcortical grey-matter structure within the brain. It comprises dozens of nuclei with different function and connectivity, which are affected differently by disease. For this reason, there is growing interest in studying the thalamic nuclei in vivo with MRI. Tools are available to segment the thalamus from 1 mm T1 scans, but the contrast of the lateral and internal boundaries is too faint to produce reliable segmentations. Some tools have attempted to incorporate information from diffusion MRI in the segmentation to refine these boundaries, but do not generalise well across diffusion MRI acquisitions. Here we present the first CNN that can segment thalamic nuclei from T1 and diffusion data of any resolution without retraining or fine tuning. Our method builds on a public histological atlas of the thalamic nuclei and silver standard segmentations on high-quality diffusion data obtained with a recent Bayesian adaptive segmentation tool. We combin
    
[^35]: GPT用于半自动化数据科学：引入CAAFE实现上下文感知自动特征工程

    GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])

    [http://arxiv.org/abs/2305.03403](http://arxiv.org/abs/2305.03403)

    介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。

    

    随着自动化机器学习（AutoML）领域的发展，将领域知识纳入这些系统中变得越来越重要。我们利用大型语言模型（LLMs）的强大功能提出了一种方法来实现这一目标。具体地，我们介绍了一种用于表格数据的特征工程方法，名为上下文感知自动特征工程（CAAFE），它利用LLM根据数据集的描述生成更多具有语义意义的特征。该方法产生用于创建新特征的Python代码，并提供生成特征的效用说明。尽管方法论上很简单，但CAAFE提高了14个数据集中11个数据集的性能，与2个数据集并列，只有1个数据集性能下降，从而使所有数据集的平均ROC AUC表现从0.798提升至0.822。对于所评估的数据集，这一改进与使用随机森林（AUC 0.782）代替逻辑回归（AUC 0.754）所获得的平均改进相似。此外，

    As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
    
[^36]: 用潜在二进制变量和归一化流来稀疏化贝叶斯神经网络

    Sparsifying Bayesian neural networks with latent binary variables and normalizing flows. (arXiv:2305.03395v1 [stat.ML])

    [http://arxiv.org/abs/2305.03395](http://arxiv.org/abs/2305.03395)

    本论文介绍了一种新的方法来稀疏化贝叶斯神经网络，使用潜在二进制变量和归一化流，实现了网络在测试时的自动稀疏化，而且结果表明这个方法在准确性上能够与现有的稀疏化方法相媲美。

    

    人工神经网络（ANN）是现代许多应用中强大的机器学习方法，如面部识别、机器翻译和癌症诊断。ANN的一个常见问题是它们通常具有数百万或数十亿个可训练参数，并且因此倾向于过度拟合训练数据。这在需要可靠的不确定性估计的应用中特别有问题。贝叶斯神经网络（BNN）可以改善这一问题，因为它们包含参数不确定性。此外，潜在二进制贝叶斯神经网络（LBBNN）通过允许将权重打开或关闭，从而在权重和结构的联合空间中启用推断，也考虑了结构不确定性。本文将考虑LBBNN方法的两个扩展：首先，通过使用局部重参数化技巧（LRT）直接采样隐藏单元，我们得到了更加计算有效的算法。更重要的是，通过使用归一化流，我们可以近似潜在二进制变量的后验分布，从而在测试时实现网络的稀疏化。我们实验证明，我们提出的方法与现有的稀疏化技术相比，能够获得竞争性的结果，同时保持类似的准确性。

    Artificial neural networks (ANNs) are powerful machine learning methods used in many modern applications such as facial recognition, machine translation, and cancer diagnostics. A common issue with ANNs is that they usually have millions or billions of trainable parameters, and therefore tend to overfit to the training data. This is especially problematic in applications where it is important to have reliable uncertainty estimates. Bayesian neural networks (BNN) can improve on this, since they incorporate parameter uncertainty. In addition, latent binary Bayesian neural networks (LBBNN) also take into account structural uncertainty by allowing the weights to be turned on or off, enabling inference in the joint space of weights and structures. In this paper, we will consider two extensions to the LBBNN method: Firstly, by using the local reparametrization trick (LRT) to sample the hidden units directly, we get a more computationally efficient algorithm. More importantly, by using normal
    
[^37]: 面向长尾识别的有效协作学习

    Towards Effective Collaborative Learning in Long-Tailed Recognition. (arXiv:2305.03378v1 [cs.CV])

    [http://arxiv.org/abs/2305.03378](http://arxiv.org/abs/2305.03378)

    本文提出了一种重新加权的蒸馏损失来处理长尾分布下的协作学习不平衡问题，并建立了一个有效的协作学习框架以提高模型的性能。

    

    实际数据通常面临着严重的类别不平衡和长尾分布，其中少数类与多数类相比显着不足。最近的研究倾向于利用多专家体系结构来减轻在少数类中的模型不确定性，其中采用协作学习来汇总专家的知识，即在线蒸馏。在本文中，我们观察到专家之间的知识转移在类别分布方面是不平衡的，这导致少数类的性能提升有限。为了解决这个问题，我们提出了一种重新加权的蒸馏损失，通过比较在线蒸馏和标签注释分别监督的两个分类器的预测来实现。我们还强调，特征级蒸馏将显着提高模型性能并增加特征的鲁棒性。最后，我们提出了一个有效的协作学习（ECL）框架，该框架将对比学习和蒸馏机制相结合，以实现最佳表现。

    Real-world data usually suffers from severe class imbalance and long-tailed distributions, where minority classes are significantly underrepresented compared to the majority ones. Recent research prefers to utilize multi-expert architectures to mitigate the model uncertainty on the minority, where collaborative learning is employed to aggregate the knowledge of experts, i.e., online distillation. In this paper, we observe that the knowledge transfer between experts is imbalanced in terms of class distribution, which results in limited performance improvement of the minority classes. To address it, we propose a re-weighted distillation loss by comparing two classifiers' predictions, which are supervised by online distillation and label annotations, respectively. We also emphasize that feature-level distillation will significantly improve model performance and increase feature robustness. Finally, we propose an Effective Collaborative Learning (ECL) framework that integrates a contrastiv
    
[^38]: MuSe 2023多模态情感分析挑战赛：模拟情感、跨文化幽默和个性化

    The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation. (arXiv:2305.03369v1 [cs.LG])

    [http://arxiv.org/abs/2305.03369](http://arxiv.org/abs/2305.03369)

    MuSe 2023是一组共享任务，涉及三个当代多模态情感和情绪分析问题：模拟情感、跨文化幽默和个性化。参与者需要在各自的子挑战中预测情感目标、跨文化幽默和个性化的信号。

    

    MuSe 2023是一组共享任务，涉及三个当代多模态情感和情绪分析问题：在模拟情感子挑战（MuSe-Mimic）中，参与者预测三个连续情感目标。这个子挑战利用了Hume-Vidmimic数据集，其中包含用户生成的视频。对于跨文化幽默检测子挑战（MuSe-Humour），提供了Passau Spontaneous Football Coach Humour（Passau-SFCH）数据集的扩展。参与者需要预测跨文化环境中自发幽默的出现。个性化子挑战（MuSe-Personalisation）基于Ulm-Trier Social Stress Test（Ulm-TSST）数据集，该数据集包含处于紧张状态下的被试录音。在这里，需要预测唤醒和价值信号，而部分测试标签则可用于促进个性化。MuSe 2023旨在将来自不同研究社区的广泛受众汇聚在一起。

    The MuSe 2023 is a set of shared tasks addressing three different contemporary multimodal affect and sentiment analysis problems: In the Mimicked Emotions Sub-Challenge (MuSe-Mimic), participants predict three continuous emotion targets. This sub-challenge utilises the Hume-Vidmimic dataset comprising of user-generated videos. For the Cross-Cultural Humour Detection Sub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous Football Coach Humour (Passau-SFCH) dataset is provided. Participants predict the presence of spontaneous humour in a cross-cultural setting. The Personalisation Sub-Challenge (MuSe-Personalisation) is based on the Ulm-Trier Social Stress Test (Ulm-TSST) dataset, featuring recordings of subjects in a stressed situation. Here, arousal and valence signals are to be predicted, whereas parts of the test labels are made available in order to facilitate personalisation. MuSe 2023 seeks to bring together a broad audience from different research communities such as
    
[^39]: 离线基于模型的强化学习综述

    A Survey on Offline Model-Based Reinforcement Learning. (arXiv:2305.03360v1 [cs.LG])

    [http://arxiv.org/abs/2305.03360](http://arxiv.org/abs/2305.03360)

    本文是一篇关于离线基于模型的强化学习最新进展的综述性文章，讨论了该领域的最新概念、方法和挑战。文献综述介绍了当前离线基于模型的强化学习领域的关键文献，并重点讨论了如何解决分布变化问题。该领域面临的关键挑战也被讨论和总结。

    

    基于模型的方法在离线强化学习领域变得越来越流行，由于模型能够充分利用监督学习技术中大规模历史数据集的优势，在实际应用中具有非常高的潜力。本文对目前离线基于模型的强化学习研究进行了文献综述。综述简要介绍了离线强化学习和基于模型的强化学习的概念和最新发展，并探讨了两个领域的交叉点。接着，本文介绍了离线基于模型的强化学习领域的关键文献，并讨论了它们的方法，特别是它们在解决分布变化问题方面的方法，这是当前所有离线基于模型的强化学习方法面临的主要问题。本文还进一步讨论了该领域面临的关键挑战。

    Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by
    
[^40]: 从解析-执行到解析-执行-优化：提高复杂问题答案基于知识库的语义解析器

    From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base. (arXiv:2305.03356v1 [cs.CL])

    [http://arxiv.org/abs/2305.03356](http://arxiv.org/abs/2305.03356)

    本文提出了一种名为解析-执行-优化（Parse-Execute-Refine）的范式，通过向知识库问题应答模型演示执行中间推理步骤，可以提高复杂推理的能力。

    

    把问题解析成可执行的逻辑形式对于知识库问题应答有了显著的结果。然而，复杂的知识库问题应答是一项更具挑战性的任务，需要进行复杂的多步推理。最近，提出了一种名为KoPL的新型语义解析器，旨在显式地模拟推理过程，在复杂知识库问题应答领域实现了最先进的结果。本文进一步探讨了如何通过一种简单的解析-执行-优化范式来开发语义解析器的推理能力。我们通过向知识库问题应答模型演示执行中间推理步骤来完善和改进KoPL解析器。我们表明，这样简单的策略可以显著提高复杂推理的能力。具体而言，我们提出了三个组成部分：解析阶段，执行阶段和优化阶段，以增强复杂推理的能力。解析器使用KoPL生成透明的逻辑形式。然后，执行阶段对齐和执行这些逻辑形式。

    Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and execute
    
[^41]: 数据集压缩综合研究：性能、隐私、鲁棒性以及公平性

    A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])

    [http://arxiv.org/abs/2305.03355](http://arxiv.org/abs/2305.03355)

    本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。

    

    数据集压缩旨在将原始数据集的丰富特征编码成小型数据集，是一种加速神经网络训练和相关研究的有前途的方法。已经提出了不同的方法来改善压缩图像的信息性和泛化性能。然而，目前还没有从安全性角度全面分析这一技术的工作，并且对潜在风险缺乏系统理解。在本文中，我们进行了大量实验，评估了当前最先进的数据集压缩方法。我们成功使用成员推理攻击来显示仍然存在隐私风险。本文还表明，数据集压缩在模型鲁棒性方面可能会产生不同程度的影响，并在进行预测时放大类别间的模型不公平性。本研究为数据集压缩评估提供了大规模的基准测试框架。

    The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
    
[^42]: 从多类神经网络中重建训练数据

    Reconstructing Training Data from Multiclass Neural Networks. (arXiv:2305.03350v1 [cs.LG])

    [http://arxiv.org/abs/2305.03350](http://arxiv.org/abs/2305.03350)

    本文研究了从多类神经网络中重建训练数据的问题，比以往的二元分类情况更容易实现，并且发现在训练过程中使用权重衰减会增加样本重建的易受性。

    

    从经过训练的神经网络的训练集中重建样本是一项重要的隐私问题。Haim等人最近表明，基于梯度方法的隐含偏差的理论结果，可以从神经网络二元分类器中重建训练样本。在本文中，我们对这项先前工作进行了多项改进和新的见解。作为我们的主要改进，我们表明在多类别设置下训练数据的重建是可能的，而且重建质量甚至比二元分类情况更高。此外，我们展示了在训练过程中使用权重衰减会增加样本重建的易受性。最后，虽然先前的工作中，训练集的大小最多只有来自10个类别的1000个样本，但我们展示了初步证据表明能够从从100个类别的5000个样本训练的模型中进行重建。

    Reconstructing samples from the training set of trained neural networks is a major privacy concern. Haim et al. (2022) recently showed that it is possible to reconstruct training samples from neural network binary classifiers, based on theoretical results about the implicit bias of gradient methods. In this work, we present several improvements and new insights over this previous work. As our main improvement, we show that training-data reconstruction is possible in the multi-class setting and that the reconstruction quality is even higher than in the case of binary classification. Moreover, we show that using weight-decay during training increases the vulnerability to sample reconstruction. Finally, while in the previous work the training set was of size at most $1000$ from $10$ classes, we show preliminary evidence of the ability to reconstruct from a model trained on $5000$ samples from $100$ classes.
    
[^43]: 在在线服务系统中针对多维数据的通用且鲁棒的根本原因定位

    Generic and Robust Root Cause Localization for Multi-Dimensional Data in Online Service Systems. (arXiv:2305.03331v1 [cs.SE])

    [http://arxiv.org/abs/2305.03331](http://arxiv.org/abs/2305.03331)

    本文提出了一种通用且鲁棒的PSqueeze多维数据的根本原因定位方法，该方法基于广义串扰效应和一种新的概率聚类方法和鲁棒启发式搜索方法，并在两个真实世界数据集中的5400个故障中验证了其性能优于基线。

    

    定位多维数据的根本原因对于确保在线服务系统的可靠性至关重要。当出现故障时，只有特定属性组合内的测量值异常。这些属性组合是下层根本原因的重要线索，因此被称为多维数据的根本原因。本论文提出了一种多维数据的通用且鲁棒的根本原因定位方法PSqueeze。我们提出了一种多维数据根本原因的通用特性——广义串扰效应(GRE)。基于此，我们提出了一种新的概率聚类方法和鲁棒启发式搜索方法。此外，我们确定了确定外部根本原因的重要性，并首次在文献中提出了一种有效的方法。我们对两个真实世界的数据集进行了实验，涉及5400个故障，在所有情况下，PSqueeze的F1得分优于基线32.89％，定位时间约为10秒。外部根本原因的F1得分比基线高39.86％。

    Localizing root causes for multi-dimensional data is critical to ensure online service systems' reliability. When a fault occurs, only the measure values within specific attribute combinations are abnormal. Such attribute combinations are substantial clues to the underlying root causes and thus are called root causes of multidimensional data. This paper proposes a generic and robust root cause localization approach for multi-dimensional data, PSqueeze. We propose a generic property of root cause for multi-dimensional data, generalized ripple effect (GRE). Based on it, we propose a novel probabilistic cluster method and a robust heuristic search method. Moreover, we identify the importance of determining external root causes and propose an effective method for the first time in literature. Our experiments on two real-world datasets with 5400 faults show that the F1-score of PSqueeze outperforms baselines by 32.89%, while the localization time is around 10 seconds across all cases. The F
    
[^44]: Tiny-PPG: 用于边缘设备上实时检测光电容积脉搏图信号中运动伪影的轻量级深度神经网络

    Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices. (arXiv:2305.03308v1 [eess.SP])

    [http://arxiv.org/abs/2305.03308](http://arxiv.org/abs/2305.03308)

    Tiny-PPG是一个用于边缘设备上实时检测PPG信号中运动伪影的轻量级深度神经网络，通过使用深度可分离卷积和空洞空间金字塔池化模块，以及通道注意机制，能够在平衡检测精度和速度的情况下实现最先进的检测效果，并可以在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。

    

    尽管光电容积脉搏图（PPG）信号已经被广泛应用于基于物联网的可穿戴设备和智能健康设备中进行心血管健康监护，但在现实世界中，PPG信号很容易受到运动伪影的污染。本研究提出了一种名为“Tiny-PPG”的轻量级深度神经网络，用于在物联网边缘设备上准确实时地分割PPG伪影。该模型在公开数据集PPG DaLiA上进行了训练和测试，该数据集使用手表式设备（Empatica E4）对15名受试者在各种日常活动中的PPG信号，具有不同长度和形态的复杂伪影。该模型结构、训练方法和损失函数特别设计，以平衡检测精度和速度，适用于资源受限的嵌入式设备上进行实时PPG伪影检测。为了优化多尺度特征表示的模型大小和能力，该模型采用深度可分离卷积和空洞空间金字塔池化模块。此外，还提出了一个通道注意机制，以跨通道学习区分性和信息丰富的PPG信号特征。实验结果表明，Tiny-PPG在检测精度和计算效率方面均实现了最先进的性能，并可在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。

    Photoplethysmogram (PPG) signals are easily contaminated by motion artifacts in real-world settings, despite their widespread use in Internet-of-Things (IoT) based wearable and smart health devices for cardiovascular health monitoring. This study proposed a lightweight deep neural network, called Tiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edge devices. The model was trained and tested on a public dataset, PPG DaLiA, which featured complex artifacts with diverse lengths and morphologies during various daily activities of 15 subjects using a watch-type device (Empatica E4). The model structure, training method and loss function were specifically designed to balance detection accuracy and speed for real-time PPG artifact detection in resource-constrained embedded devices. To optimize the model size and capability in multi-scale feature representation, the model employed deep separable convolution and atrous spatial pyramid pooling modules, respectively. Addition
    
[^45]: 非参数有限先验知识下的分散扩散学习

    Decentralized diffusion-based learning under non-parametric limited prior knowledge. (arXiv:2305.03295v1 [stat.ML])

    [http://arxiv.org/abs/2305.03295](http://arxiv.org/abs/2305.03295)

    本文提出在非参数情况下，仅通过相邻节点之间的信息传播，避免数据交换的分散扩散学习算法。

    

    我们研究在噪声环境中，从局部代理的测量结果中学习非线性现象 m 的扩散网络学习问题。对于分散的网络，仅在直接相邻节点之间传播信息，我们提出了一种非参数学习算法，避免了原始数据交换，仅需要对 m 有轻微的先验知识。对所提出的方法进行了非渐近估计误差界的导出，并通过模拟实验说明了它的潜在应用。

    We study the problem of diffusion-based network learning of a nonlinear phenomenon, $m$, from local agents' measurements collected in a noisy environment. For a decentralized network and information spreading merely between directly neighboring nodes, we propose a non-parametric learning algorithm, that avoids raw data exchange and requires only mild \textit{a priori} knowledge about $m$. Non-asymptotic estimation error bounds are derived for the proposed method. Its potential applications are illustrated through simulation experiments.
    
[^46]: FedNC：基于网络编码启发的安全高效联合学习方法

    FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding. (arXiv:2305.03292v1 [cs.LG])

    [http://arxiv.org/abs/2305.03292](http://arxiv.org/abs/2305.03292)

    本文提出FedNC，一个联合学习的通信框架，结合了网络编码技术，能够提高系统的隐私、吞吐量和鲁棒性。

    

    联合学习是一种有前途的分布式学习机制，但仍然面临两个主要挑战，即隐私泄漏和系统效率。在本文中，我们从网络信息理论的角度重新构思了联合学习系统，并制定了一个原创的联合学习通信框架FedNC，该框架受到网络编码的启发。 FedNC的主要思想是通过对原始数据包进行随机线性组合，将本地模型的信息混合在一起，然后再上传进行进一步的聚合，从而使FL系统更加安全，吞吐量更高，鲁棒性更好。据我们所知，这是第一个将NC引入FL的框架。随着FL在实际网络框架中的不断演变，可以基于FedNC进一步设计更多的应用和变体。

    Federated Learning (FL) is a promising distributed learning mechanism which still faces two major challenges, namely privacy breaches and system efficiency. In this work, we reconceptualize the FL system from the perspective of network information theory, and formulate an original FL communication framework, FedNC, which is inspired by Network Coding (NC). The main idea of FedNC is mixing the information of the local models by making random linear combinations of the original packets, before uploading for further aggregation. Due to the benefits of the coding scheme, both theoretical and experimental analysis indicate that FedNC improves the performance of traditional FL in several important ways, including security, throughput, and robustness. To the best of our knowledge, this is the first framework where NC is introduced in FL. As FL continues to evolve within practical network frameworks, more applications and variants can be further designed based on FedNC.
    
[^47]: 解密高斯混合专家模型中的Softmax门控问题

    Demystifying Softmax Gating in Gaussian Mixture of Experts. (arXiv:2305.03288v1 [stat.ML])

    [http://arxiv.org/abs/2305.03288](http://arxiv.org/abs/2305.03288)

    本文提出了新的参数Vononoi损失函数并建立了MLE的收敛速度来解决高斯混合专家模型中的Softmax门控问题，研究表明该门控与高斯分布中的专家函数通过偏微分方程相互作用，是一个复杂依赖关系。

    

    理解Softmax门控高斯混合专家模型的参数估计一直是文献中长期未解决的问题。这主要是由于三个基本理论挑战与Softmax门控相关：（i）只能识别参数的平移；（ii）Softmax门控和高斯分布中专家函数之间通过偏微分方程的内在相互作用；（iii）Softmax门控高斯混合专家模型的条件密度的分子和分母之间的复杂依赖关系。我们通过提出新的参数Vononoi损失函数并建立MLE的收敛速度来解决这些挑战，用于解决这些模型的参数估计。当专家数量未知且超额指定时，我们的发现表明MLE的速率与一组多项式方程的可解性问题有关。

    Understanding parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating: (i) the identifiability only up to the translation of the parameters; (ii) the intrinsic interaction via partial differential equation between the softmax gating and the expert functions in Gaussian distribution; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Vononoi loss functions among parameters and establishing the convergence rates of the maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the number of experts is unknown and over-specified, our findings show a connection between the rate of MLE and a solvability problem of a system of polynomial equations.
    
[^48]: 使用任务控制的复合动作学习

    Composite Motion Learning with Task Control. (arXiv:2305.03286v1 [cs.GR])

    [http://arxiv.org/abs/2305.03286](http://arxiv.org/abs/2305.03286)

    该论文介绍了一种使用多个判别器在类 GAN 设置中直接学习来自多个参考动作的特定身体部位的解耦动作的深度学习方法，以实现复合和任务驱动的动作控制，同时考虑了多个任务的奖励和训练一个单一的多目标控制策略。

    

    我们提出了一种深度学习方法，用于物理模拟角色的复合和任务驱动的动作控制。与使用强化学习模仿全身动作的现有数据驱动方法不同，我们通过在类 GAN 设置中利用多个判别器直接同时学习来自多个参考动作的特定身体部位的解耦动作。在这个过程中，不需要手动制作用于学习的复合参考动作。相反，控制策略自行探索如何自动地组合复合动作。我们进一步考虑了多个任务特定奖励，并训练一个单一的多目标控制策略。为此，我们提出了一个新的多目标学习框架，自适应平衡来自多个来源和多个目标定向控制目标的不同运动的学习。此外，由于复合动作通常是更简单行为的增强。

    We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behavio
    
[^49]: 使用视觉变换器的语义分割：一项调查

    Semantic Segmentation using Vision Transformers: A survey. (arXiv:2305.03273v1 [cs.CV])

    [http://arxiv.org/abs/2305.03273](http://arxiv.org/abs/2305.03273)

    本文介绍了使用视觉变换器（ViT）进行语义分割的不同架构，讨论了ViT在分块划分方案方面的局限性，并回顾和比较了不同ViT架构的性能表现。ViT架构的崛起在计算机视觉任务中替代传统的卷积神经网络的趋势不断增强。

    

    语义分割在各个领域中有广泛的应用，包括土地覆盖分析、自动驾驶和医学图像分析。卷积神经网络（CNN）和视觉变换器（ViT）为语义分割提供了架构模型。尽管ViT在图像分类方面取得了成功，但由于其分块划分方案，无法直接应用于像图像分割和目标检测这样的密集预测任务，因此ViT不是一种通用的主干架构。本调查讨论了一些可用于语义分割的不同ViT架构以及它们如何应对上述挑战。ViT的崛起以及其高成功率的表现，促使社区逐渐在各种计算机视觉任务中取代传统的卷积神经网络。本调查旨在回顾和比较设计用于语义分割的ViT架构的性能。

    Semantic segmentation has a broad range of applications in a variety of domains including land coverage analysis, autonomous driving, and medical image analysis. Convolutional neural networks (CNN) and Vision Transformers (ViTs) provide the architecture models for semantic segmentation. Even though ViTs have proven success in image classification, they cannot be directly applied to dense prediction tasks such as image segmentation and object detection since ViT is not a general purpose backbone due to its patch partitioning scheme. In this survey, we discuss some of the different ViT architectures that can be used for semantic segmentation and how their evolution managed the above-stated challenge. The rise of ViT and its performance with a high success rate motivated the community to slowly replace the traditional convolutional neural networks in various computer vision tasks. This survey aims to review and compare the performances of ViT architectures designed for semantic segmentati
    
[^50]: 限制认知负荷的贝叶斯强化学习

    Bayesian Reinforcement Learning with Limited Cognitive Load. (arXiv:2305.03263v1 [cs.LG])

    [http://arxiv.org/abs/2305.03263](http://arxiv.org/abs/2305.03263)

    本文综述了限制认知负荷的贝叶斯强化学习的最新算法和理论成果，并讨论了这些思想如何应用于研究认知和行为科学中的问题。

    

    所有的生物和人工智能代理都必须在信息处理能力上有限制下进行学习和决策。因此，一般的自适应行为理论应该能够说明代理的学习历史、决策和能力限制之间的复杂互动。最近，计算机科学领域的研究开始通过将强化学习、贝叶斯决策和速率-失真理论的思想融合，澄清塑造这些动态的原则。这一领域的工作提供了一种会考虑处理限制对学习和动作选择的影响的统一规范框架，即容量有限的贝叶斯强化学习。在这里，我们提供了一篇易于理解的综述，介绍了这一领域的最新算法和理论成果，特别关注这些思想如何应用于研究认知和行为科学中的问题。

    All biological and artificial agents must learn and make decisions given limits on their ability to process information. As such, a general theory of adaptive behavior should be able to account for the complex interactions between an agent's learning history, decisions, and capacity constraints. Recent work in computer science has begun to clarify the principles that shape these dynamics by bridging ideas from reinforcement learning, Bayesian decision-making, and rate-distortion theory. This body of work provides an account of capacity-limited Bayesian reinforcement learning, a unifying normative framework for modeling the effect of processing constraints on learning and action selection. Here, we provide an accessible review of recent algorithms and theoretical results in this setting, paying special attention to how these ideas can be applied to studying questions in the cognitive and behavioral sciences.
    
[^51]: 基于数据和物理模型的中国仓鼠卵巢细胞生物反应器建模

    Data-driven and Physics Informed Modelling of Chinese Hamster Ovary Cell Bioreactors. (arXiv:2305.03257v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.03257](http://arxiv.org/abs/2305.03257)

    本文提出了一种基于数据和物理模型的混合方法，以学习中国仓鼠卵巢（CHO）细胞生物反应器的动态演化模型。

    

    滴定培养是利用哺乳动物细胞培养生产生物制品的一种已经建立的操作模式。量化建模整合了一些关键反应步骤的动力学以及优化驱动的代谢通量分配，使用通量平衡分析。这已知会导致某些数学不一致性。本文提出了一种基于物理的数据驱动混合模型（“灰盒”）来从过程数据中学习中国仓鼠卵巢（CHO）细胞生物反应器的动态演化模型。该方法结合了物理定律（例如物质平衡）以及代谢通量的动力学表达式。然后使用机器学习（ML）来直接学习演化方程（黑盒建模）；恢复未知的物理参数（“白盒”参数拟合）或者，更重要的是，学习部分未知动力学表达式（灰盒建模）。我们将过度决定的代谢生物物理系统的凸优化步骤编码为差分方程。

    Fed-batch culture is an established operation mode for the production of biologics using mammalian cell cultures. Quantitative modeling integrates both kinetics for some key reaction steps and optimization-driven metabolic flux allocation, using flux balance analysis; this is known to lead to certain mathematical inconsistencies. Here, we propose a physically-informed data-driven hybrid model (a "gray box") to learn models of the dynamical evolution of Chinese Hamster Ovary (CHO) cell bioreactors from process data. The approach incorporates physical laws (e.g. mass balances) as well as kinetic expressions for metabolic fluxes. Machine learning (ML) is then used to (a) directly learn evolution equations (black-box modelling); (b) recover unknown physical parameters ("white-box" parameter fitting) or -- importantly -- (c) learn partially unknown kinetic expressions (gray-box modelling). We encode the convex optimization step of the overdetermined metabolic biophysical system as a differe
    
[^52]: PMP：使用分部运动先验学习与环境进行物理交互

    PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors. (arXiv:2305.03249v1 [cs.GR])

    [http://arxiv.org/abs/2305.03249](http://arxiv.org/abs/2305.03249)

    该论文提出了一种使用多个分部运动先验(PMP)动画化角色的方法，可以创造出不同的动作集合，并提供了采用广泛的分部先验训练代理的方法。

    

    我们提出了一种使用多个分部运动先验（PMP）来动画化角色的方法。之前的研究允许根据参考数据创建逼真的关节运动，但运动范围很大程度上受到可用样本的限制。特别是对于充满交互的场景，试图获取每种可能的交互运动是不现实的，因为物理参数的组合呈指数增长。所提出的PMP允许我们组装多个分部技能以动画化角色，创建具有不同现有数据组合的多种动作集。在我们的管道中，我们可以使用广泛的分部先验训练代理。因此，每个身体部位可以从运动捕捉中获得一种动力学视角的风格，或者同时从附加的部分特定模拟中提取与动力学相关的信息。例如，我们可以先仅为器械手部分训练一般交互技能（例如抓握），然后再结合其他部分的动作。

    We present a method to animate a character incorporating multiple part-wise motion priors (PMP). While previous works allow creating realistic articulated motions from reference data, the range of motion is largely limited by the available samples. Especially for the interaction-rich scenarios, it is impractical to attempt acquiring every possible interacting motion, as the combination of physical parameters increases exponentially. The proposed PMP allows us to assemble multiple part skills to animate a character, creating a diverse set of motions with different combinations of existing data. In our pipeline, we can train an agent with a wide range of part-wise priors. Therefore, each body part can obtain a kinematic insight of the style from the motion captures, or at the same time extract dynamics-related information from the additional part-specific simulation. For example, we can first train a general interaction skill, e.g. grasping, only for the dexterous part, and then combine 
    
[^53]: 考虑多轮对话上下文的领域外意图检测

    Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])

    [http://arxiv.org/abs/2305.03237](http://arxiv.org/abs/2305.03237)

    本文提出了一个上下文感知的OOD意图检测框架（Caro），用于模拟OOD意图检测任务中的多轮对话上下文，并在提取稳健的表示时删除与意图检测无关的多余信息。Caro在多个标准数据集上表现出最先进的性能，并超越了先前方法。

    

    领域外（OOD）意图检测对于实用的对话系统非常重要，通常需要考虑多轮对话上下文。然而，大多数先前的OOD意图检测方法仅限于单轮对话。在本文中，我们介绍了一个上下文感知的OOD意图检测（Caro）框架，用于对OOD意图检测任务中的多轮上下文进行建模。具体地，我们遵循信息瓶颈原则从多轮对话上下文中提取稳健的表示。每个输入样本构建了两个不同的视角，使用多视图信息瓶颈损失删除与意图检测无关的多余信息。此外，我们还探索了在Caro中利用未标记的数据。引入了一个两阶段训练过程来从这些未标记的数据中挖掘OOD样本，并使用自举方法用这些OOD样本来训练生成的模型。全面的实验表明，Caro在OOD意图检测任务的几个基准数据集上建立了最先进的性能，并超越了仅考虑单轮上下文的先前方法。

    Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
    
[^54]: 自然语言处理中基于外部分布检测的综述

    A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])

    [http://arxiv.org/abs/2305.03236](http://arxiv.org/abs/2305.03236)

    这篇论文首次综述了最新的OOD检测方法在自然语言处理中的应用。根据算法使用的数据，将方法分成三类，并介绍了相关数据集、应用和度量方法。

    

    在现实世界中，基于外部分布（OOD）的检测对于机器学习系统的可靠和安全的部署至关重要。过去几年取得了极大进展。本文重点关注自然语言处理方法，并首次综述了OOD检测方面的最新进展。首先，我们给出OOD检测的正式定义，并讨论了几个相关领域。然后，根据算法使用的数据，将最近的算法分成三类：（1）可用OOD数据，（2）OOD数据不可用+内部分布（ID）标签可用，（3）OOD数据不可用+ID标签不可用。第三，介绍数据集、应用和度量方法。最后，总结现有工作并提出潜在的未来研究课题。

    Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
    
[^55]: 采用分位回归和特征选择的碳价预测

    Carbon Price Forecasting with Quantile Regression and Feature Selection. (arXiv:2305.03224v1 [cs.LG])

    [http://arxiv.org/abs/2305.03224](http://arxiv.org/abs/2305.03224)

    研究使用分位回归和特征选择等方法进行炭价预测，显著提高了预测效果。

    

    炭价以期货形式近期成为欧盟和中国等交易市场中新兴的金融资产，其波动性和非线性使得精确预测碳价通常是一项困难的任务。本研究提出使用分位回归和特征选择等方法来进行碳价预测，并展示了显著的效果。

    Carbon futures has recently emerged as a novel financial asset in the trading markets such as the European Union and China. Monitoring the trend of the carbon price has become critical for both national policy-making as well as industrial manufacturing planning. However, various geopolitical, social, and economic factors can impose substantial influence on the carbon price. Due to its volatility and non-linearity, predicting accurate carbon prices is generally a difficult task. In this study, we propose to improve carbon price forecasting with several novel practices. First, we collect various influencing factors, including commodity prices, export volumes such as oil and natural gas, and prosperity indices. Then we select the most significant factors and disclose their optimal grouping for explainability. Finally, we use the Sparse Quantile Group Lasso and Adaptive Sparse Quantile Group Lasso for robust price predictions. We demonstrate through extensive experimental studies that our 
    
[^56]: 社会正义算法：社交网络中的平权行动

    Algorithms for Social Justice: Affirmative Action in Social Networks. (arXiv:2305.03223v1 [cs.SI])

    [http://arxiv.org/abs/2305.03223](http://arxiv.org/abs/2305.03223)

    本文介绍了一个新的基于谱图理论的链接推荐算法ERA-Link，旨在缓解现有推荐算法带来的信息孤岛和社会成见，实现社交网络平台的社会正义目标。

    

    链接推荐算法对于世界各地数十亿用户的人际关系产生了影响。为了最大化相关性，它们通常建议连接相互相似的用户。然而，这被发现会产生信息孤岛，加剧弱势突出群体所遭受的孤立，并延续社会成见。为了缓解这些限制，大量研究致力于实现公平的链接推荐方法。然而，大多数方法并不质疑链接推荐算法的最终目标，即数据交易的复杂商业模型中用户参与的货币化。本文主张实现社交网络平台玩家和目的的多样化，以实现社会正义。为了说明这一概念目标，我们提出了ERA-Link，这是一种基于谱图理论的新型链接推荐算法，可以抵消系统性的社会歧视。

    Link recommendation algorithms contribute to shaping human relations of billions of users worldwide in social networks. To maximize relevance, they typically propose connecting users that are similar to each other. This has been found to create information silos, exacerbating the isolation suffered by vulnerable salient groups and perpetuating societal stereotypes. To mitigate these limitations, a significant body of work has been devoted to the implementation of fair link recommendation methods. However, most approaches do not question the ultimate goal of link recommendation algorithms, namely the monetization of users' engagement in intricate business models of data trade. This paper advocates for a diversification of players and purposes of social network platforms, aligned with the pursue of social justice. To illustrate this conceptual goal, we present ERA-Link, a novel link recommendation algorithm based on spectral graph theory that counteracts the systemic societal discriminat
    
[^57]: 所有的模型都是局部的: 用循环本地验证取代外部验证

    All models are local: time to replace external validation with recurrent local validation. (arXiv:2305.03219v1 [cs.LG])

    [http://arxiv.org/abs/2305.03219](http://arxiv.org/abs/2305.03219)

    本文认为外部验证无法确保机器学习模型的安全性或实用性，提出了循环本地验证的MLOps启发式范式作为新的黄金标准，强调对各个本地部署的模型进行监测和更新，从而更好地对齐临床和医疗特定需求与机器学习模型验证策略，提高临床决策支持工具的安全性和实用性。

    

    外部验证经常被推荐用于确保机器学习模型的泛化能力。然而，它既不能保证泛化能力，也不能等价于模型的临床实用性（任何临床决策支持工具的最终目标）。外部验证与当前医疗保健机器学习的需要不一致。其次，新的机器学习技术、当前的市场力量和更新的监管框架正在促进对个体部署的模型实例的频繁更新和监控。我们认为，外部验证不足以确保机器学习模型的安全性或实用性。修复外部验证范式的建议不够彻底。继续依赖它作为最终测试很可能会使我们走上错误道路。我们提出了 MLOps 启发式范式的循环本地验证作为新的黄金标准，强调监测和更新各个本地部署的模型。采用这种范式将更好地对齐临床和医疗特定需求与机器学习模型验证策略，提高临床决策支持工具的安全性和实用性。

    External validation is often recommended to ensure the generalizability of ML models. However, it neither guarantees generalizability nor equates to a model's clinical usefulness (the ultimate goal of any clinical decision-support tool). External validation is misaligned with current healthcare ML needs. First, patient data changes across time, geography, and facilities. These changes create significant volatility in the performance of a single fixed model (especially for deep learning models, which dominate clinical ML). Second, newer ML techniques, current market forces, and updated regulatory frameworks are enabling frequent updating and monitoring of individual deployed model instances. We submit that external validation is insufficient to establish ML models' safety or utility. Proposals to fix the external validation paradigm do not go far enough. Continued reliance on it as the ultimate test is likely to lead us astray. We propose the MLOps-inspired paradigm of recurring local v
    
[^58]: AttentionViz：Transformer Attention的全局视图

    AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v1 [cs.HC])

    [http://arxiv.org/abs/2305.03210](http://arxiv.org/abs/2305.03210)

    这篇论文介绍了AttentionViz，一种以联合嵌入为基础的交互式可视化工具，用于帮助研究人员理解Transformer中的自我注意机制。该方法使得可以全局分析多个输入序列的注意力模式，提高对模型的理解并通过多个应用场景和专家反馈提供新的交互见解。

    

    Transformer模型正在革新机器学习，但它们的内部运作仍然神秘莫测。在本文中，我们提出了一种新的可视化技术，旨在帮助研究人员理解Transformer中的自我注意机制，使这些模型能够学习序列中元素之间丰富的上下文关系。我们方法的主要思想是可视化Transformer模型用于计算注意力的查询和键向量的联合嵌入。与以前的注意力可视化技术不同，我们的方法能够分析多个输入序列的全局模式。我们基于这些联合查询-键嵌入创建了一个交互式可视化工具AttentionViz，并将其用于研究语言和视觉变压器中的注意机制。通过几个应用场景和专家反馈，我们展示了我们的方法在提高模型理解和提供有关查询-键交互的新见解方面的实用性。

    Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz, based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.
    
[^59]: 利用语言处理技术增强普什图语的单标签和多标签文本分类

    Enhancing Pashto Text Classification using Language Processing Techniques for Single And Multi-Label Analysis. (arXiv:2305.03201v1 [cs.CL])

    [http://arxiv.org/abs/2305.03201](http://arxiv.org/abs/2305.03201)

    研究旨在建立一个普什图语文本的自动分类系统，最终通过应用多层感知器分类器获得了94％的平均测试准确率。

    

    文本分类已成为各个领域中至关重要的任务，导致了大量研究开发自动化文本分类系统以支持国内外的语言。然而，需要自动化文本分类系统来处理本地语言的需求越来越大。本研究旨在建立一个普什图语文本的自动分类系统。为了实现这一目标，我们构建了一个普什图语文档数据集，并应用了各种模型，包括统计和神经机器学习模型，如DistilBERT-base-multilingual-cased、多层感知器、支持向量机、K最近邻、决策树、高斯朴素贝叶斯、多项式朴素贝叶斯、随机森林和逻辑回归，来确定最有效的方法。我们还评估了两种不同的特征提取方法，即词袋和词频逆向文档频率。研究表明，使用多层感知器分类器，测试准确率平均值达到了94％。

    Text classification has become a crucial task in various fields, leading to a significant amount of research on developing automated text classification systems for national and international languages. However, there is a growing need for automated text classification systems that can handle local languages. This study aims to establish an automated classification system for Pashto text. To achieve this goal, we constructed a dataset of Pashto documents and applied various models, including statistical and neural machine learning models such as DistilBERT-base-multilingual-cased, Multilayer Perceptron, Support Vector Machine, K Nearest Neighbor, decision tree, Gaussian na\"ive Bayes, multinomial na\"ive Bayes, random forest, and logistic regression, to identify the most effective approach. We also evaluated two different feature extraction methods, bag of words and Term Frequency Inverse Document Frequency. The study achieved an average testing accuracy rate of 94% using the MLP class
    
[^60]: 模仿神经系统的仿真学习

    Emulation Learning for Neuromimetic Systems. (arXiv:2305.03196v1 [eess.SY])

    [http://arxiv.org/abs/2305.03196](http://arxiv.org/abs/2305.03196)

    本文提出了一种符合神经模仿模式的通用仿真问题，为了解决优化步骤的整数规划组合复杂性影响，提出了一种深度Q网络算法来学习轨迹和对信道中断具有优势；并且可以使用基于映射的迁移学习法来将模型应用于其他仿真问题。

    

    基于我们最近在神经启发式量化系统上的研究，我们报告了学习量化运动和对信道中断的弹性的结果。我们提出了一种符合神经模仿模式的通用仿真问题。这个最优量化问题可以通过模型预测控制（MPC）来解决，但由于优化步骤涉及整数规划，当输入通道数量变大时，方法会受到组合复杂性的影响。即使我们同时收集数据点来训练神经网络，数据收集和训练本身仍然耗时。因此，我们提出了一个通用的深度Q网络（DQN）算法，不仅可以学习轨迹，还可以展示对信道中断的优势。此外，为了将模型转移到其他仿真问题，可以直接在当前模型上使用基于映射的迁移学习方法来获取最优方向。

    Building on our recent research on neural heuristic quantization systems, results on learning quantized motions and resilience to channel dropouts are reported. We propose a general emulation problem consistent with the neuromimetic paradigm. This optimal quantization problem can be solved by model predictive control (MPC), but because the optimization step involves integer programming, the approach suffers from combinatorial complexity when the number of input channels becomes large. Even if we collect data points to train a neural network simultaneously, collection of training data and the training itself are still time-consuming. Therefore, we propose a general Deep Q Network (DQN) algorithm that can not only learn the trajectory but also exhibit the advantages of resilience to channel dropout. Furthermore, to transfer the model to other emulation problems, a mapping-based transfer learning approach can be used directly on the current model to obtain the optimal direction for the ne
    
[^61]: 一种生成建模框架用于推断在数据稀少情况下的生物力学本构定律族群

    A Generative Modeling Framework for Inferring Families of Biomechanical Constitutive Laws in Data-Sparse Regimes. (arXiv:2305.03184v1 [cs.LG])

    [http://arxiv.org/abs/2305.03184](http://arxiv.org/abs/2305.03184)

    该论文提出了一种利用生成式深度学习与贝叶斯推断相结合的方法，在数据稀疏情况下，能够有效地推断本构关系族群。

    

    量化人体血管组织的生物力学性质有助于深入了解心血管疾病。常规的本构建模非线性回归要求充足高质量数据和一个本构模型作为先验知识。相比之下，我们提出一种新方法，将生成式深度学习与贝叶斯推断相结合，以在数据稀疏情况下有效地推断本构关系族群。受函数先验概念启发，我们开发了一个生成对抗网络（GAN），其中包括一个神经算子作为生成器和一个全连接神经网络作为判别器。生成器将以测量数据为条件的噪声向量作为输入并生成预测的本构关系，接下来判别器会对其进行验证。我们证明，该框架可以准确地估计本构关系的均值和标准差。

    Quantifying biomechanical properties of the human vasculature could deepen our understanding of cardiovascular diseases. Standard nonlinear regression in constitutive modeling requires considerable high-quality data and an explicit form of the constitutive model as prior knowledge. By contrast, we propose a novel approach that combines generative deep learning with Bayesian inference to efficiently infer families of constitutive relationships in data-sparse regimes. Inspired by the concept of functional priors, we develop a generative adversarial network (GAN) that incorporates a neural operator as the generator and a fully-connected neural network as the discriminator. The generator takes a vector of noise conditioned on measurement data as input and yields the predicted constitutive relationship, which is scrutinized by the discriminator in the following step. We demonstrate that this framework can accurately estimate means and standard deviations of the constitutive relationships of
    
[^62]: 基于主观间相关性的对比学习在睡眠分期中的应用

    Contrastive Learning for Sleep Staging based on Inter Subject Correlation. (arXiv:2305.03178v1 [eess.SP])

    [http://arxiv.org/abs/2305.03178](http://arxiv.org/abs/2305.03178)

    该论文提出了基于主观间相关性的对比学习方法(MViTime)，用于睡眠分期中的跨个体问题，实现了最先进的性能。

    

    近年来，许多研究将深度学习应用于自动睡眠分期。然而，这些研究在睡眠分期的跨个体问题上关注较少。同时，新兴的神经科学理论对主观间相关性的研究可以为跨主体分析提供新的视角。本文提出了MViTime模型，并通过对比学习实现了跨主体相关性理论，提供了一个解决睡眠分期中跨个体问题的可行方案。最后，通过实验结果和结论表明所开发的方法在睡眠分期上实现了最先进的性能。剖析实验的结果也证明了基于对比学习的跨主体方法的有效性。

    In recent years, multitudes of researches have applied deep learning to automatic sleep stage classification. Whereas actually, these works have paid less attention to the issue of cross-subject in sleep staging. At the same time, emerging neuroscience theories on inter-subject correlations can provide new insights for cross-subject analysis. This paper presents the MViTime model that have been used in sleep staging study. And we implement the inter-subject correlation theory through contrastive learning, providing a feasible solution to address the cross-subject problem in sleep stage classification. Finally, experimental results and conclusions are presented, demonstrating that the developed method has achieved state-of-the-art performance on sleep staging. The results of the ablation experiment also demonstrate the effectiveness of the cross-subject approach based on contrastive learning.
    
[^63]: 深度学习辅助下的同时目标感知和超分辨率成像研究

    Deep Learning-Assisted Simultaneous Targets Sensing and Super-Resolution Imaging. (arXiv:2305.03177v1 [eess.SP])

    [http://arxiv.org/abs/2305.03177](http://arxiv.org/abs/2305.03177)

    本研究展示了一种多功能深度神经网络，用于重建超材料目标交互系统中的目标信息，能同时感知目标的数量和介电常数，生成高精度的超分辨率图像。

    

    近年来，由于亚波长电磁波调控能力的引入，超材料在感知和超分辨率成像领域有了革命性的发展。然而，超材料的添加使得从检测场中获取目标信息的复杂度变得非常高。此外，虽然深度学习方法为一系列电磁问题提供了强大的平台，但许多研究主要集中在解决单一功能，并限制了研究的多样性。本研究展示了一种多功能深度神经网络，用于重建超材料目标交互系统中的目标信息。首先，在初步验证实验中确认了交互场景可以容忍系统噪声。然后，通过电场分布输入，多任务深度神经网络不仅可以感知目标的数量和介电常数，而且还可以生成高精度的超分辨率图像。

    Recently, metasurfaces have experienced revolutionary growth in the sensing and superresolution imaging field, due to their enabling of subwavelength manipulation of electromagnetic waves. However, the addition of metasurfaces multiplies the complexity of retrieving target information from the detected fields. Besides, although the deep learning method affords a compelling platform for a series of electromagnetic problems, many studies mainly concentrate on resolving one single function and limit the research's versatility. In this study, a multifunctional deep neural network is demonstrated to reconstruct target information in a metasurface targets interactive system. Firstly, the interactive scenario is confirmed to tolerate the system noises in a primary verification experiment. Then, fed with the electric field distributions, the multitask deep neural network can not only sense the quantity and permittivity of targets but also generate superresolution images with high precision. Th
    
[^64]: 基于情感分析的新型对抗性图像检测方法

    New Adversarial Image Detection Based on Sentiment Analysis. (arXiv:2305.03173v1 [cs.CR])

    [http://arxiv.org/abs/2305.03173](http://arxiv.org/abs/2305.03173)

    本文提出了一种新的对抗性样本检测器，通过使用情感分析并检测对神经网络的隐藏层特征图造成的影响逐渐显现来检测对抗性样本，克服了现有技术的局限，且实验结果表明其在识别最新的对抗性攻击方面优于现有技术。

    

    深度神经网络（DNN）易受到对抗样本的攻击，而对抗攻击模型（例如DeepFool）正在崛起并超越对抗性样本检测技术。本文提出了一种新的对抗性样本检测器，可在识别图像数据集上的最新对抗性攻击方面超越现有技术。具体而言，我们提出使用情感分析来检测对抗性样本，这是通过检测对神经网络的隐藏层特征图造成的影响逐渐显现来实现的。因此，我们设计一个具有最少可学习参数的模块化嵌入层，将隐藏层特征映射到词向量中，组成可以进行情感分析的句子。大量实验表明，这种新型检测器在检测针对CIFAR-10、SVHN和ImageNet数据集上的ResNet和Inception中性网络发起的最新攻击方面均能一致超越最先进的检测算法。

    Deep Neural Networks (DNNs) are vulnerable to adversarial examples, while adversarial attack models, e.g., DeepFool, are on the rise and outrunning adversarial example detection techniques. This paper presents a new adversarial example detector that outperforms state-of-the-art detectors in identifying the latest adversarial attacks on image datasets. Specifically, we propose to use sentiment analysis for adversarial example detection, qualified by the progressively manifesting impact of an adversarial perturbation on the hidden-layer feature maps of a DNN under attack. Accordingly, we design a modularized embedding layer with the minimum learnable parameters to embed the hidden-layer feature maps into word vectors and assemble sentences ready for sentiment analysis. Extensive experiments demonstrate that the new detector consistently surpasses the state-of-the-art detection algorithms in detecting the latest attacks launched against ResNet and Inception neutral networks on the CIFAR-1
    
[^65]: 80 MHz Wi-Fi信道上的无线人体感应CSI数据集

    A CSI Dataset for Wireless Human Sensing on 80 MHz Wi-Fi Channels. (arXiv:2305.03170v1 [eess.SP])

    [http://arxiv.org/abs/2305.03170](http://arxiv.org/abs/2305.03170)

    该文提出了一个CSI数据集，提供了80MHz Wi-fi信道上与环境、人员和Wi-Fi硬件相关的数据，这为开发领域自适应算法提供了重要支持。

    

    在过去的几年中，已经提出了几种基于机器学习的技术来监测从Wi-Fi信道读数的人类动作。然而，开发稳健地在不同环境下工作的领域自适应算法仍然是一个开放的问题，其解决方案需要大规模的数据集，这些数据集具有强大的领域多样性，包括环境、人员和Wi-Fi硬件。迄今为止，少数公共数据集大多已经过时 - 因为是通过20或40 MHz频段运作的Wi-Fi设备得到的 - 并且几乎没有领域多样性，从而极大地限制了传感算法设计的进展。本文旨在通过提供一个IEEE 802.11ac信道测量的数据集来填补这一差距，在80 MHz带宽信道上具有显着的领域多样性，通过涉及不同环境、日子和不同硬件的十三个主题的测量活动提供新的实验数据。

    In the last years, several machine learning-based techniques have been proposed to monitor human movements from Wi-Fi channel readings. However, the development of domain-adaptive algorithms that robustly work across different environments is still an open problem, whose solution requires large datasets characterized by strong domain diversity, in terms of environments, persons and Wi-Fi hardware. To date, the few public datasets available are mostly obsolete - as obtained via Wi-Fi devices operating on 20 or 40 MHz bands - and contain little or no domain diversity, thus dramatically limiting the advancements in the design of sensing algorithms. The present contribution aims to fill this gap by providing a dataset of IEEE 802.11ac channel measurements over an 80 MHz bandwidth channel featuring notable domain diversity, through measurement campaigns that involved thirteen subjects across different environments, days, and with different hardware. Novel experimental data is provided by bl
    
[^66]: 机器学习模型在电子健康记录中的敏感数据检测

    Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records. (arXiv:2305.03169v1 [cs.CR])

    [http://arxiv.org/abs/2305.03169](http://arxiv.org/abs/2305.03169)

    该论文使用机器学习算法来识别结构化数据中的敏感变量，以便便于去身份化过程。该算法可以解决不同数据集PHI字段异质性的问题。

    

    在大数据时代，医疗保健提供者、社区和研究人员需要分享数据并合作改善健康结果、获取有价值的见解和推进研究。1996年《健康保险流通与责任法案》(HIPAA)是一项联邦法律，旨在通过制定保护健康信息的规定来保护敏感健康信息。然而，在数据共享之前，HIPAA没有提供有效的检测或删除PHI的工具。本文旨在探讨使用机器学习算法来识别结构化数据中的敏感变量，从而便于去身份化过程。

    In the era of big data, there is an increasing need for healthcare providers, communities, and researchers to share data and collaborate to improve health outcomes, generate valuable insights, and advance research. The Health Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law designed to protect sensitive health information by defining regulations for protected health information (PHI). However, it does not provide efficient tools for detecting or removing PHI before data sharing. One of the challenges in this area of research is the heterogeneous nature of PHI fields in data across different parties. This variability makes rule-based sensitive variable identification systems that work on one database fail on another. To address this issue, our paper explores the use of machine learning algorithms to identify sensitive variables in structured data, thus facilitating the de-identification process. We made a key observation that the distributions of metadata of
    
[^67]: G-MATT: 分子语法树变换器的单步回溯合成预测

    G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer. (arXiv:2305.03153v1 [cs.LG])

    [http://arxiv.org/abs/2305.03153](http://arxiv.org/abs/2305.03153)

    G-MATT是一个结合数据驱动模型与化学知识的化学感知回溯合成预测框架，在分层SMILES语法树输入的基础上采用树到序列变换器架构，能够显著提高单步回溯合成的准确率。

    

    近年来，已经报道了几种基于反应模板和基于自由模板的单步回溯合成预测方法。尽管这些方法中的许多在传统数据驱动指标方面表现良好，但使用的模型架构与支配反向合成的底层化学原则之间存在脱节。在这里，我们提出了一种新颖的化学感知回溯合成预测框架，将强大的数据驱动模型与化学知识相结合。我们报告了一种基于分层SMILES语法树的树到序列变换器架构，其中包含被纯SMILES表示法的模型忽略的底层化学信息。所提出的框架，基于语法的分子注意力树变换器（G-MATT），与基线回溯合成模型相比，实现了显着的性能提高。 G-MATT的准确率排名前1为51％（前10为79.1％），无效率为1.5％，

    In recent years, several reaction templates-based and template-free approaches have been reported for single-step retrosynthesis prediction. Even though many of these approaches perform well from traditional data-driven metrics standpoint, there is a disconnect between model architectures used and underlying chemistry principles governing retrosynthesis. Here, we propose a novel chemistry-aware retrosynthesis prediction framework that combines powerful data-driven models with chemistry knowledge. We report a tree-to-sequence transformer architecture based on hierarchical SMILES grammar trees as input containing underlying chemistry information that is otherwise ignored by models based on purely SMILES-based representations. The proposed framework, grammar-based molecular attention tree transformer (G-MATT), achieves significant performance improvements compared to baseline retrosynthesis models. G-MATT achieves a top-1 accuracy of 51% (top-10 accuracy of 79.1%), invalid rate of 1.5%, a
    
[^68]: 通信高效的概率邻域扩展分析与缓存的图神经网络

    Communication-Efficient Graph Neural Networks with Probabilistic Neighborhood Expansion Analysis and Caching. (arXiv:2305.03152v1 [cs.LG])

    [http://arxiv.org/abs/2305.03152](http://arxiv.org/abs/2305.03152)

    本文针对分布式设置中GNN的小批量训练和推断提出了一种缓存策略，基于顶点的多跳邻域抽样中的点包含概率分析（VIP），可以显著减少通信量而不影响预测准确度。

    

    自图神经网络（GNNs）诞生以来，如何在规模庞大的图上训练和推断一直受到广泛关注，因为GNN在推荐系统和金融取证等应用中被广泛使用且取得了成功。本文针对在分布式设置中使用节点抽样的GNN进行小批量训练和推断，其中跨分布式存储的顶点特征的必要分区会导致特征通信成为制约可扩展性的主要瓶颈。为了显著减少通信量而不影响预测准确性，我们提出了一种策略，即缓存远程分区中与访问频率高的顶点相关的数据。所提出的策略基于顶点的多跳邻域抽样中的点包含概率（VIP）分析，这可能会将邻域扩展到图的分区边界之外。VIP分析不仅能够消除离线计算，而且能够将通信开销降到近似线性，同时保持预测准确度不变。

    Training and inference with graph neural networks (GNNs) on massive graphs has been actively studied since the inception of GNNs, owing to the widespread use and success of GNNs in applications such as recommendation systems and financial forensics. This paper is concerned with minibatch training and inference with GNNs that employ node-wise sampling in distributed settings, where the necessary partitioning of vertex features across distributed storage causes feature communication to become a major bottleneck that hampers scalability. To significantly reduce the communication volume without compromising prediction accuracy, we propose a policy for caching data associated with frequently accessed vertices in remote partitions. The proposed policy is based on an analysis of vertex-wise inclusion probabilities (VIP) during multi-hop neighborhood sampling, which may expand the neighborhood far beyond the partition boundaries of the graph. VIP analysis not only enables the elimination of th
    
[^69]: CAMEL：面向高效设备端学习的AI模型和嵌入式DRAM的共同设计

    CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning. (arXiv:2305.03148v1 [cs.AR])

    [http://arxiv.org/abs/2305.03148](http://arxiv.org/abs/2305.03148)

    CAMEL提出了使用嵌入式DRAM作为主要存储介质的方法来解决设备端学习中存储和计算过程中占用大量内存的问题，从而使AI模型更加高效。

    

    物联网的兴起导致边缘设备产生了大量数据，通常使用人工智能算法进行处理。设备端学习使边缘平台能够不断地根据用户个人数据调整AI模型，从而实现更好的服务质量。然而，在资源受限的设备上进行AI训练非常困难，因为深度神经网络（DNN）会带来密集的计算工作量和占用大量芯片内存的问题。为了缓解这个问题，我们建议使用嵌入式动态随机存取存储器（eDRAM）作为训练数据的主要存储介质。与静态随机访问存储器（SRAM）相比，eDRAM在存储密度上引入了超过2倍的改进，从而减少了芯片外存储器的流量。然而，为了保持存储的数据完整，eDRAM需要执行耗电的数据刷新操作。如果数据存储一段时间，就可以避免eDRAM刷新。

    The emergence of the Internet of Things (IoT) has resulted in a remarkable amount of data generated on edge devices, which are often processed using AI algorithms. On-device learning enables edge platforms to continually adapt the AI models to user personal data and further allows for a better service quality. However, AI training on resource-limited devices is extremely difficult because of the intensive computing workload and the significant amount of on-chip memory consumption exacted by deep neural networks (DNNs). To mitigate this, we propose to use embedded dynamic random-access memory (eDRAM) as the main storage medium of training data. Compared with static random-access memory (SRAM), eDRAM introduces more than $2\times$ improvement on storage density, enabling reduced off-chip memory traffic. However, to keep the stored data intact, eDRAM is required to perform the power-hungry data refresh operations.  eDRAM refresh can be eliminated if the data is stored for a period of time
    
[^70]: 文本嵌入对NLP聚类性能的影响。

    Influence of various text embeddings on clustering performance in NLP. (arXiv:2305.03144v1 [cs.LG])

    [http://arxiv.org/abs/2305.03144](http://arxiv.org/abs/2305.03144)

    研究探索了不同文本嵌入对聚类算法（KMeans、单链接聚合等级、DBSCAN和HDBSCAN）性能的影响，并应用于评论聚类领域。

    

    随着电子商务平台的出现，评论对于顾客评估产品的可信度至关重要。但是，星级评分并不总是与顾客编写的评论文本相匹配。在本研究中，我们探索了选择不同文本嵌入来表示这些评论的任务，并探究了嵌入选择对各种类型聚类算法性能的影响。我们使用上下文（BERT）和非上下文（Word2Vec）文本嵌入来表示文本，并测量它们对三种聚类算法（基于分区的KMeans、单链接聚合等级和密度基础的DBSCAN和HDBSCAN）在不同实验设置下的影响。

    With the advent of e-commerce platforms, reviews are crucial for customers to assess the credibility of a product. The star ratings do not always match the review text written by the customer. For example, a three star rating (out of five) may be incongruous with the review text, which may be more suitable for a five star review. A clustering approach can be used to relabel the correct star ratings by grouping the text reviews into individual groups. In this work, we explore the task of choosing different text embeddings to represent these reviews and also explore the impact the embedding choice has on the performance of various classes of clustering algorithms. We use contextual (BERT) and non-contextual (Word2Vec) text embeddings to represent the text and measure their impact of three classes on clustering algorithms - partitioning based (KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN and HDBSCAN), each with various experimental settings. We use the sil
    
[^71]: 构建可逆的语义保持的逻辑公式嵌入：一种基于 Graph VAE 的深度学习方法

    Towards Invertible Semantic-Preserving Embeddings of Logical Formulae. (arXiv:2305.03143v1 [cs.AI])

    [http://arxiv.org/abs/2305.03143](http://arxiv.org/abs/2305.03143)

    本研究提出了一种基于 Graph VAE 的深度学习模型ISPE，能够在连续空间中保持语义的嵌入逻辑公式，并且该方法是可逆的。在语句补全和推理等任务上的表现优于现有方法。

    

    逻辑是自动推理的主要形式语言，同时也是一种可解释的语言。学习和优化逻辑规则一直是人工智能领域的重要问题。然而，现有的机器学习方法是基于连续空间的梯度下降优化，而学习逻辑则处于离散的语法空间中。解决这个挑战，需要提出一种能在连续空间中保持语义的嵌入方法，目前的方法虽然能保持语义，但是不可逆。本文提出了一种新的模型，即可逆语义保持嵌入（ISPE），利用基于 Graph VAE 的深度结构实现了嵌入。该方法在语句补全和推理等任务上表现优异，超过了现有的方法。

    Logic is the main formal language to perform automated reasoning, and it is further a human-interpretable language, at least for small formulae. Learning and optimising logic requirements and rules has always been an important problem in Artificial Intelligence. State of the art Machine Learning (ML) approaches are mostly based on gradient descent optimisation in continuous spaces, while learning logic is framed in the discrete syntactic space of formulae. Using continuous optimisation to learn logic properties is a challenging problem, requiring to embed formulae in a continuous space in a meaningful way, i.e. preserving the semantics. Current methods are able to construct effective semantic-preserving embeddings via kernel methods (for linear temporal logic), but the map they define is not invertible. In this work we address this problem, learning how to invert such an embedding leveraging deep architectures based on the Graph Variational Autoencoder framework. We propose a novel mod
    
[^72]: 对比损失作为全局上位联系模型的广义模型

    Contrastive losses as generalized models of global epistasis. (arXiv:2305.03136v1 [q-bio.PE])

    [http://arxiv.org/abs/2305.03136](http://arxiv.org/abs/2305.03136)

    对比损失函数可以用于提取全局上位联系模型所隐含的稀疏潜在函数，这种方法可能为蛋白质工程和相关领域的适应性函数推断提供有用的通用框架。

    

    适应性函数将生物序列的大组合空间映射到所关注的特性上。从实验数据中推断这些多模态函数是现代蛋白质工程中的核心任务。全局上位联系模型是一类有效且有物理基础的模型，可用于估计从观察数据中推断适应性函数。这些模型假设稀疏的潜在函数通过单调非线性变换以发射可测的适应性。在这里，我们展示了最小化对比损失函数（如 Bradley-Terry 损失）是提取全局上位联系所隐示的稀疏潜在函数的一种简单灵活的技术。我们通过适应性-上位联系不确定性原理争辩，全局上位联系模型中的非线性可以产生不具备稀疏表示的观察适应性函数，因此可能不适合使用均方误差（MSE）损失（一种常见的做法）从观察中学习。我们表明，对比损失可用于推断不适合 MSE 损失的适应性函数，并且全局上位联系模型可以解释为一种规则化的对比损失模型。我们的结果表明，这种方法可能为蛋白质工程和相关领域的适应性函数推断提供有用的通用框架。

    Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrasti
    
[^73]: ChatGPT 需要进行SPADE（可持续性、隐私、数字鸿沟和伦理）评估：一项综述。

    ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])

    [http://arxiv.org/abs/2305.03123](http://arxiv.org/abs/2305.03123)

    本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。

    

    ChatGPT是另一个大型语言模型（LLM），由于其性能和有效的对话能力，在研究和工业界中得到了巨大的关注。最近，许多研究已经发表，以展示ChatGPT和其他LLMs的有效性、效率、集成和情感。相反，本研究关注的是大多数被忽视的重要方面，即可持续性、隐私、数字鸿沟和伦理，并建议不仅仅是ChatGPT，而是在对话机器人类别中的每一个后续入口都应该进行SPADE评估。本文详细讨论了关于ChatGPT的问题和关注点与上述特征一致。我们通过一些初步的数据收集和可视化以及假设的事实来支持我们的假设。我们还为每个问题提出了缓解和建议。此外，我们还提供了一些未来方向和开放问题的探讨。

    ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
    
[^74]: 分布式协同功能：统一博弈论交互方法来解释机器学习

    Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability. (arXiv:2305.03100v1 [cs.LG])

    [http://arxiv.org/abs/2305.03100](http://arxiv.org/abs/2305.03100)

    本文提出了一种统一的框架，用于游戏理论驱动的归因和k阶交互方法，通过假设，可以在连续输入设置中得到唯一全面的特征交互解释，即协同作用。

    

    深度学习已经彻底改变了机器学习的许多领域，从计算机视觉到自然语言处理，但这些高性能模型通常是“黑盒子”。解释此类模型将提高AI决策透明度和信任，并且对于理解其他实际需求（如鲁棒性和公平性）是必要的。增强模型透明度的一种流行方法是量化单个输入对模型输出的贡献（称为归因）以及群组输入之间的相互作用的强度。越来越多的这些方法导入博弈论的概念和结果来产生归因和交互作用。本文提出了一个统一的框架，用于博弈论驱动的归因和k阶交互方法。我们展示了在连续输入设置中，假设适度，可以得到特征之间交互的唯一全面说明，即协同作用。我们确定了各种方法的特征。

    Deep learning has revolutionized many areas of machine learning, from computer vision to natural language processing, but these high-performance models are generally "black box." Explaining such models would improve transparency and trust in AI-powered decision making and is necessary for understanding other practical needs such as robustness and fairness. A popular means of enhancing model transparency is to quantify how individual inputs contribute to model outputs (called attributions) and the magnitude of interactions between groups of inputs. A growing number of these methods import concepts and results from game theory to produce attributions and interactions. This work presents a unifying framework for game-theory-inspired attribution and $k^\text{th}$-order interaction methods. We show that, given modest assumptions, a unique full account of interactions between features, called synergies, is possible in the continuous input setting. We identify how various methods are characte
    
[^75]: 一种用于快速有监督学习的Bootstrap算法

    A Bootstrap Algorithm for Fast Supervised Learning. (arXiv:2305.03099v1 [cs.LG])

    [http://arxiv.org/abs/2305.03099](http://arxiv.org/abs/2305.03099)

    本文探讨了一种Bootstrap算法，该算法可以通过自助法、重抽样和线性回归来更新隐藏层的加权连接，从而达到更快的收敛速度。

    

    训练神经网络（NN）通常依赖某种类型的曲线跟随方法，例如梯度下降（GD）（和随机梯度下降（SGD）），ADADELTA，ADAM或有限内存算法。这些算法的收敛通常依赖于访问大量的观测值以实现高精度，并且对于某些函数类，这些算法可能需要多个epoch的数据点才能进行。本文探讨了一种不同的技术，可以实现更快的收敛速度，尤其是对于浅层的网络而言。它不是曲线跟随，而是依赖于“分离”隐藏层并通过自助法、重抽样和线性回归来更新它们的加权连接。通过利用重抽样的观测值，本方法的收敛被实证地显示出快速和需要更少的数据点：特别是，我们的实验表明，我们只需要少量的数据点即可。

    Training a neural network (NN) typically relies on some type of curve-following method, such as gradient descent (GD) (and stochastic gradient descent (SGD)), ADADELTA, ADAM or limited memory algorithms. Convergence for these algorithms usually relies on having access to a large quantity of observations in order to achieve a high level of accuracy and, with certain classes of functions, these algorithms could take multiple epochs of data points to catch on. Herein, a different technique with the potential of achieving dramatically better speeds of convergence, especially for shallow networks, is explored: it does not curve-follow but rather relies on 'decoupling' hidden layers and on updating their weighted connections through bootstrapping, resampling and linear regression. By utilizing resampled observations, the convergence of this process is empirically shown to be remarkably fast and to require a lower amount of data points: in particular, our experiments show that one needs a fra
    
[^76]: 使用深度多元图像完成进行高分辨率乳腺扫描的无监督异常定位

    Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion. (arXiv:2305.03098v1 [eess.IV])

    [http://arxiv.org/abs/2305.03098](http://arxiv.org/abs/2305.03098)

    本文提出了一种利用深度多元图像完成方法进行乳腺扫描异常定位的无监督方法，该方法通过探索完成的多元性来提高评估标准的精度。

    

    数字乳腺摄影中的自动肿瘤检测是一项困难的任务，由于肿瘤很少出现，乳房组织变异和高分辨率。针对这个问题，鉴于异常图像的稀缺性和正常图像的丰富性，异常检测/定位方法可能非常适合。然而，机器学习中大部分异常定位研究集中在非医学数据集上，我们发现这些方法在医学图像数据集上适应性不足。这个问题可以通过解决图像完成视角下的任务得到缓解，其中异常的存在可以通过原始外观与其环境条件下自动完成之间的差异来指示。然而，在DBT数据集中，往往有很多相同环境条件下的有效的正常完成，使这个评估标准不太精确。为了解决这个问题，我们考虑通过探索完成的多元性来进行图像完成。

    Automated tumor detection in Digital Breast Tomosynthesis (DBT) is a difficult task due to natural tumor rarity, breast tissue variability, and high resolution. Given the scarcity of abnormal images and the abundance of normal images for this problem, an anomaly detection/localization approach could be well-suited. However, most anomaly localization research in machine learning focuses on non-medical datasets, and we find that these methods fall short when adapted to medical imaging datasets. The problem is alleviated when we solve the task from the image completion perspective, in which the presence of anomalies can be indicated by a discrepancy between the original appearance and its auto-completion conditioned on the surroundings. However, there are often many valid normal completions given the same surroundings, especially in the DBT dataset, making this evaluation criterion less precise. To address such an issue, we consider pluralistic image completion by exploring the distributi
    
[^77]: 联邦集成指导的离线强化学习算法

    Federated Ensemble-Directed Offline Reinforcement Learning. (arXiv:2305.03097v1 [cs.LG])

    [http://arxiv.org/abs/2305.03097](http://arxiv.org/abs/2305.03097)

    本文开发了一种名为FEDORA的联邦集成指导的离线强化学习算法，通过集成学习方法提炼客户群体的集体智慧，显著优于其他方法，包括在合并的数据汇总中进行离线强化学习，在各种复杂的连续控制环境和真实世界数据集中进行了实验。

    

    本文考虑了联邦离线强化学习问题。在这一场景下，分布式的学习代理必须仅使用由不同的未知的行为策略生成的小型预先收集的数据集协作学习出高质量的控制策略。笨拙地将标准离线强化学习方法与标准联邦学习方法组合来解决这个问题可能会导致表现不佳的策略。我们因此设计了Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA)，通过集成学习方法提炼客户群体的集体智慧。我们开发了FEDORA代码库，利用联邦学习平台上的分布式计算资源。我们证明了FEDORA在各种复杂的连续控制环境和真实世界数据集中均显著优于其他方法，包括在合并的数据汇总中进行离线强化学习。最后，我们展示了FEDORA在真实世界中的表现。

    We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Naively combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies. In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach. We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on 
    
[^78]: 用神经网络解释暗物质晕的密度分布

    Explaining dark matter halo density profiles with neural networks. (arXiv:2305.03077v1 [astro-ph.CO])

    [http://arxiv.org/abs/2305.03077](http://arxiv.org/abs/2305.03077)

    使用可解释的神经网络将暗物质晕的演化历史与其密度分布相连接，网络发现超过渗透半径的轮廓由一个单一参数描述，这展示了在复杂的天体物理数据集中机器协助科学发现的潜力。

    

    我们使用可解释的神经网络将暗物质晕的演化历史与其密度分布相连接。该网络捕获密度分布中独立的变化因素，在低维表示中物理地解释了它们，使用了互信息。不需要先验知识，网络恢复了早期组装与内部轮廓之间的已知关系，并发现超过渗透半径的轮廓由一个单一参数描述，该参数表示最近的质量吸积率。结果展示了在复杂的天体物理数据集中机器协助科学发现的潜力。

    We use explainable neural networks to connect the evolutionary history of dark matter halos with their density profiles. The network captures independent factors of variation in the density profiles within a low-dimensional representation, which we physically interpret using mutual information. Without any prior knowledge of the halos' evolution, the network recovers the known relation between the early time assembly and the inner profile, and discovers that the profile beyond the virial radius is described by a single parameter capturing the most recent mass accretion rate. The results illustrate the potential for machine-assisted scientific discovery in complicated astrophysical datasets.
    
[^79]: 悬臂梁损伤检测的神经符号模型

    Neuro-symbolic model for cantilever beams damage detection. (arXiv:2305.03063v1 [cs.LG])

    [http://arxiv.org/abs/2305.03063](http://arxiv.org/abs/2305.03063)

    本文提出了一种神经符号模型用于悬臂梁损伤检测，该模型通过将卷积网络的处理能力与逻辑查询交互控制相结合，不仅能够准确检测损伤，而且还能够提供解释和定位，使其在操作条件下更可靠和可信。

    

    在过去的十年中，损伤检测方法迅速从先进的信号处理方法转变为机器学习，尤其是深度学习模型，以准确地、非侵入性地估计梁结构状态。但随着深度学习模型达到巅峰表现，人们也观察到了它们适用性的限制和易受攻击的弱点。其中最重要的一个原因是深度学习系统内在可解释性的缺失，由于知识编码在张量值中而没有包含逻辑约束。本文提出了一种神经符号模型，基于新颖的认知架构，将卷积网络的处理能力与直接将实际逻辑包含到模型中的查询交互控制结合起来，用于悬臂梁损伤检测。该混合判别模型不仅能够精确地检测损伤，而且能够提供损伤的解释和定位，使其在操作条件下更可靠和可信。

    In the last decade, damage detection approaches swiftly changed from advanced signal processing methods to machine learning and especially deep learning models, to accurately and non-intrusively estimate the state of the beam structures. But as the deep learning models reached their peak performances, also their limitations in applicability and vulnerabilities were observed. One of the most important reason for the lack of trustworthiness in operational conditions is the absence of intrinsic explainability of the deep learning system, due to the encoding of the knowledge in tensor values and without the inclusion of logical constraints. In this paper, we propose a neuro-symbolic model for the detection of damages in cantilever beams based on a novel cognitive architecture in which we join the processing power of convolutional networks with the interactive control offered by queries realized through the inclusion of real logic directly into the model. The hybrid discriminative model is 
    
[^80]: 跨语种即插即用少样本语音识别技术

    Plug-and-Play Multilingual Few-shot Spoken Words Recognition. (arXiv:2305.03058v1 [eess.AS])

    [http://arxiv.org/abs/2305.03058](http://arxiv.org/abs/2305.03058)

    PLiX是一种跨语种的关键词检测系统，使用少样本学习实现了对未见过的口语单词的识别，可适用于低资源语言。

    

    随着技术的进步和数字设备的普及，人机无缝通讯的重要性日益增加。移动、可穿戴和其他物联网设备的广泛采用已经改变了我们与这些智能设备的互动方式，使得准确的语音识别成为有效互动的关键组成部分。然而，构建能够处理新关键字的强大的语音识别系统仍然具有挑战性，特别是对于训练数据有限的低资源语言。在本文中，我们提出PLiX，一种跨语种的、即插即用的关键词检测系统，利用少样本学习来利用海量现实世界数据，在测试时实现对未见过的口语单词的识别。我们使用20种语言的数百万个一秒音频剪辑来学习少样本深度模型，在高效率的同时实现了最先进的性能。广泛的评估表明，PLiX能够适应新的口语单词。

    As technology advances and digital devices become prevalent, seamless human-machine communication is increasingly gaining significance. The growing adoption of mobile, wearable, and other Internet of Things (IoT) devices has changed how we interact with these smart devices, making accurate spoken words recognition a crucial component for effective interaction. However, building robust spoken words detection system that can handle novel keywords remains challenging, especially for low-resource languages with limited training data. Here, we propose PLiX, a multilingual and plug-and-play keyword spotting system that leverages few-shot learning to harness massive real-world data and enable the recognition of unseen spoken words at test-time. Our few-shot deep models are learned with millions of one-second audio clips across 20 languages, achieving state-of-the-art performance while being highly efficient. Extensive evaluations show that PLiX can generalize to novel spoken words given as fe
    
[^81]: 利用梯度衡量数据选择和估价在差分隐私训练中的应用

    Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])

    [http://arxiv.org/abs/2305.02942](http://arxiv.org/abs/2305.02942)

    研究如何在隐私增强技术下，利用梯度信息识别训练中感兴趣的数据样本进行数据选择和估价。

    

    由于监管担忧和参与度的不足，为机器学习模型进行协作训练获取高质量数据可能是一项具有挑战性的任务。隐私增强技术（PET）是解决监管问题的一种常用方法，差分隐私（DP）训练是其中最常用的一种方法。本文研究了如何使用梯度信息来识别隐私训练中感兴趣的训练样本。我们展示了在最严格的隐私设置中，存在着能够为客户提供有原则的数据选择工具的技术。

    Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
    
[^82]: 分层Transformer用于可扩展图学习

    Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])

    [http://arxiv.org/abs/2305.02866](http://arxiv.org/abs/2305.02866)

    本文提出了分层可扩展图Transformer (HSGT)用于解决图表示学习中的规模问题和上下文信息捕获不足问题，通过构建多尺度图分层结构，HSGT实现了对大型图的快速和内存高效处理，并在基准数据集上展现了卓越的性能表现。

    

    图Transformer在机器学习领域中越来越受到关注，并在图表示学习的基准测试中展现出了最先进的性能。然而，由于当前实现的图Transformer主要集中在学习小规模图的表示上，全局自注意机制的二次复杂度对于应用于较大规模图的全批量训练构成了挑战。此外，传统的基于采样的方法无法捕捉必要的高层次上下文信息，导致性能严重下降。在本文中，我们引入了分层可扩展图Transformer (HSGT)作为这些挑战的解决方案。HSGT成功地将Transformer架构扩展到大规模图上的节点表示学习任务中，同时保持高性能。通过利用通过粗化技术构建的图分层结构，HSGT有效地更新和存储多尺度信息，从而实现对大型图的快速和内存高效处理。我们在几个基准数据集上评估了HSGT，并展示了它相对于现有方法的卓越性能。

    Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale informat
    
[^83]: Cuttlefish: 无需调整超参数的低秩模型训练

    Cuttlefish: Low-rank Model Training without All The Tuning. (arXiv:2305.02538v1 [cs.LG])

    [http://arxiv.org/abs/2305.02538](http://arxiv.org/abs/2305.02538)

    Cuttlefish 是一种新的自动化低秩训练方法，可以有效地减少可训练参数的数量，而无需调整因式分解超参数即可实现加速，可生成比完全秩训练小多达 5.6 倍的模型。

    

    最近的研究表明，训练低秩神经网络可以有效减少可训练参数的总数，而不会影响预测准确率，从而实现端到端的加速。但是，低秩模型的训练需要调整多个因式分解超参数。在本文中，我们通过引入 Cuttlefish，一种自动化的低秩训练方法，消除了调整因式分解超参数的需要。Cuttlefish 利用了一种观察结果，即在几个 epoch 的完全秩训练后，每个层的稳定秩稳定在一个常数值。一旦所有层的稳定秩都收敛，Cuttlefish 就从完全秩训练切换到低秩训练，将每个因式分解的维数设置为其相应的稳定秩。我们的实验结果表明，使用 Cuttlefish 生成的模型比完全秩训练小多达 5.6 倍。

    Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank 
    
[^84]: 探测行星信号的多重性增强分类器：使用ExoMiner的多重性增强验证69个新行星

    Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner. (arXiv:2305.02470v1 [astro-ph.EP])

    [http://arxiv.org/abs/2305.02470](http://arxiv.org/abs/2305.02470)

    该论文介绍了一种可提高探测行星信号分类器性能的框架，称为多重性增强分类器，基于现有的分类器并使用多重性信息来验证69个新的系外行星。

    

    大多数已知的系外行星是通过验证技术而不是通过补充观测进行确认的。这些技术生成的分数通常代表了有关信号的某些信息（用x表示）给出的探测行星信号的概率（y（x）=行星）。这项工作引入了一种框架，即在给定的探测行星信号确认器的基础上，利用多重性信息改善其性能。我们将此框架应用于几个现有的分类器，包括vespa（Morton等人2016）、Robovetter（Coughlin等人2017）、AstroNet（Shallue和Vanderburg 2018）、ExoNet（Ansdel等人2018）、GPC和RFC（Armstrong等人2020）以及ExoMiner（Valizadegan等人2022），以支持我们的分类结果的有效性。

    Most existing exoplanets are discovered using validation techniques rather than being confirmed by complementary observations. These techniques generate a score that is typically the probability of the transit signal being an exoplanet (y(x)=exoplanet) given some information related to that signal (represented by x). Except for the validation technique in Rowe et al. (2014) that uses multiplicity information to generate these probability scores, the existing validation techniques ignore the multiplicity boost information. In this work, we introduce a framework with the following premise: given an existing transit signal vetter (classifier), improve its performance using multiplicity information. We apply this framework to several existing classifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin et al. 2017), AstroNet (Shallue & Vanderburg 2018), ExoNet (Ansdel et al. 2018), GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), to support our cl
    
[^85]: 转移学习与主动学习用于共鸣检测：解决稀有类挑战

    Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])

    [http://arxiv.org/abs/2305.02459](http://arxiv.org/abs/2305.02459)

    本文提出并探究了基于转移和主动学习的稀有类问题的解决方案，包括利用在密切相关任务上训练的模型和评估获取策略来解决共振检测的稀有类问题，并且发现了一种名为PRC的有效的策略来指导注释。

    

    尽管基于变压器的系统使得使用更少的训练样例能够得到更高的准确性，但对于稀有类任务（即类别标签非常少见的情况，例如<5%的样本），数据采集障碍仍然存在。主动学习一般被提出用于缓解这种挑战，但选择策略，即选择稀有类示例的标准，尚未得到系统评估。此外，变压器可以实现迭代迁移学习方法。我们提出并研究了转移和主动学习解决了通过利用在密切相关任务上训练的模型和评估获取策略来解决共振检测的稀有类问题，其中包括一种提出的稀有类概率（PRC）方法。我们针对特定的稀有类问题（从社交媒体中收集认知共振的语言样本）进行了这些实验。我们发现PRC是指导注释的简单而有效的策略，最终可以提高性能，并且转移学习可以在稀缺数据情况下提供显著的改进。

    While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks -- when the class label is very infrequent (e.g. < 5% of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We propose and investigate transfer- and active learning solutions to the rare class problem of dissonance detection through utilizing models trained on closely related tasks and the evaluation of acquisition strategies, including a proposed probability-of-rare-class (PRC) approach. We perform these experiments for a specific rare class problem: collecting language samples of cognitive dissonance from social media. We find that PRC is a simple and effective strategy to guide annotations and ultimately
    
[^86]: 无监督改进音频-文本跨模态表征

    Unsupervised Improvement of Audio-Text Cross-Modal Representations. (arXiv:2305.01864v1 [cs.SD])

    [http://arxiv.org/abs/2305.01864](http://arxiv.org/abs/2305.01864)

    本研究探索了无监督的方法来改进跨模态音频-文本表征学习，通过使用领域特定的筛选和软标注对比性损失，成功提高了零-shot分类性能。

    

    最近通过使用语言模型来获得跨模态音频-文本表征取得了进展，克服了使用预定义标签的传统训练方法的局限性。这使得社区能够在零-shot分类等任务上取得进展，否则是不可能的。然而，学习这样的表征需要大量的人工注释的音频-文本对。本文研究了使用未配对文本和音频改进这些表征学习框架的无监督方法。我们探索了领域非特定和领域特定的筛选方法，创建我们用于进一步改进模型的音频-文本对。我们还表明，当与软标注对比性损失结合使用领域特定筛选时，我们能够在下游声音事件分类或声学场景分类任务的零-shot分类性能方面取得显着的改进。

    Recent advances in using language models to obtain cross-modal audio-text representations have overcome the limitations of conventional training approaches that use predefined labels. This has allowed the community to make progress in tasks like zero-shot classification, which would otherwise not be possible. However, learning such representations requires a large amount of human-annotated audio-text pairs. In this paper, we study unsupervised approaches to improve the learning framework of such representations with unpaired text and audio. We explore domain-unspecific and domain-specific curation methods to create audio-text pairs that we use to further improve the model. We also show that when domain-specific curation is used in conjunction with a soft-labeled contrastive loss, we are able to obtain significant improvement in terms of zero-shot classification performance on downstream sound event classification or acoustic scene classification tasks.
    
[^87]: 基于协方差神经网络的可转移学习和应用于解释性脑龄预测

    Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features. (arXiv:2305.01807v1 [cs.LG])

    [http://arxiv.org/abs/2305.01807](http://arxiv.org/abs/2305.01807)

    本研究首次从理论上研究了基于协方差神经网络的可转移性，证明了当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。

    

    图卷积网络（GCN）利用基于拓扑图的卷积操作来组合图上的信息进行推理任务。我们最近的工作中，通过使用协方差矩阵作为图来设计了一种类似于传统PCA数据分析方法的协方差神经网络（VNN），并具有显著的优势。本文首先从理论上研究了VNN的可转移性。可转移性的概念是从学习模型可以在“兼容”的数据集上泛化的直观期望中产生的。我们展示了VNN从GCN继承的无标度数据处理架构，并证明当数据集的协方差矩阵收敛到一个极限对象时，VNN能够展现出性能可转移性。多尺度神经影像数据集可以在多个尺度上研究脑部，并且可以验证VNN的可转移性。

    Graph convolutional networks (GCN) leverage topology-driven graph convolutional operations to combine information across the graph for inference tasks. In our recent work, we have studied GCNs with covariance matrices as graphs in the form of coVariance neural networks (VNNs) that draw similarities with traditional PCA-driven data analysis approaches while offering significant advantages over them. In this paper, we first focus on theoretically characterizing the transferability of VNNs. The notion of transferability is motivated from the intuitive expectation that learning models could generalize to "compatible" datasets (possibly of different dimensionalities) with minimal effort. VNNs inherit the scale-free data processing architecture from GCNs and here, we show that VNNs exhibit transferability of performance over datasets whose covariance matrices converge to a limit object. Multi-scale neuroimaging datasets enable the study of the brain at multiple scales and hence, can validate
    
[^88]: 一种基于LSTM的自适应巡航控制器，在车道变换时可以预测先前车辆的行为

    LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application. (arXiv:2305.01095v1 [cs.RO])

    [http://arxiv.org/abs/2305.01095](http://arxiv.org/abs/2305.01095)

    该论文提出了一种基于LSTM的ACC系统，可以学习过去的驾驶经验，适应和预测新情况，并且在模拟驾驶环境中表现良好。

    

    自适应巡航控制（ACC）系统的发展旨在通过自动调节车速来确保与前车安全距离，从而增强车辆的安全性和舒适性。然而，传统的ACC系统无法适应不断变化的驾驶条件和驾驶者的行为。为了解决这个问题，我们提出了一种基于LSTM的ACC系统，可以从以往的驾驶经验中学习，并实时地适应和预测新的情况。该模型是基于实际的高速公路高D数据集构建的，该数据集是利用装备有摄像头的无人机在德国高速公路上获取的。我们在侧面车道前车剪切并强制目标驾驶员减速的激进车道变化时评估了ACC系统。为此，将所提出的系统在模拟驾驶环境中进行了评估，并与馈送前人工神经网络（ANN）模型和模型预测控制（MPC）模型进行了比较。

    The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The 
    
[^89]: 通过独热编码和正则化提高梯度提升决策树的鲁棒性

    Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization. (arXiv:2304.13761v1 [stat.ML])

    [http://arxiv.org/abs/2304.13761](http://arxiv.org/abs/2304.13761)

    通过独热编码和正则化提高梯度提升决策树的鲁棒性，研究表明对带有$L_1$或$L_2$正则化的线性回归形式进行拟合可提高GBDT模型的鲁棒性。

    

    梯度提升决策树(GBDT)是一种广泛应用的高效机器学习方法，用于表格数据建模。然而，它们复杂的结构可能导致模型对未见数据中的小协变量扰动的鲁棒性较低。本研究应用独热编码将GBDT模型转换为线性框架，通过将每个树叶编码为一个虚拟变量。这允许使用线性回归技术，以及一种新颖的风险分解方法来评估GBDT模型对协变量扰动的鲁棒性。我们建议通过重新拟合其带有$L_1$或$L_2$正则化的线性回归形式，提高GBDT模型的鲁棒性。理论结果表明了正则化对模型性能和鲁棒性的影响。在数值实验中，证明了所提出的正则化方法可以提高独热编码GBDT模型的鲁棒性。

    Gradient-boosted decision trees (GBDT) are widely used and highly effective machine learning approach for tabular data modeling. However, their complex structure may lead to low robustness against small covariate perturbation in unseen data. In this study, we apply one-hot encoding to convert a GBDT model into a linear framework, through encoding of each tree leaf to one dummy variable. This allows for the use of linear regression techniques, plus a novel risk decomposition for assessing the robustness of a GBDT model against covariate perturbations. We propose to enhance the robustness of GBDT models by refitting their linear regression forms with $L_1$ or $L_2$ regularization. Theoretical results are obtained about the effect of regularization on the model performance and robustness. It is demonstrated through numerical experiments that the proposed regularization approach can enhance the robustness of the one-hot-encoded GBDT models.
    
[^90]: 有向链生成对抗网络

    Directed Chain Generative Adversarial Networks. (arXiv:2304.13131v1 [cs.LG])

    [http://arxiv.org/abs/2304.13131](http://arxiv.org/abs/2304.13131)

    本文提出了有向链生成对抗网络（DC-GANs），使用邻域过程作为关键步骤生成同样分布的多模态时间序列数据。

    

    现实世界中的数据分布通常是多模态的，例如描述社区意见分歧、神经元的间隔分布以及振荡器的固有频率的数据。生成多模态分布的现实世界数据已成为现有生成对抗网络（GAN）的挑战。例如，将神经随机微分方程（Neural SDEs）视为无限维GAN的情况下，它们已经展示了成功的性能，主要用于生成单峰时间序列数据。在本文中，我们提出了一种新的时间序列生成器——有向链GAN（DC-GAN），它将时间序列数据集（称为有向链的邻域过程或输入）插入具有分布约束的有向链SDE的漂移和扩散系数中。DC-GAN可以生成与邻域过程相同分布的新时间序列，而邻域过程将提供学习和生成多模态分布数据的关键步骤。

    Real-world data can be multimodal distributed, e.g., data describing the opinion divergence in a community, the interspike interval distribution of neurons, and the oscillators natural frequencies. Generating multimodal distributed real-world data has become a challenge to existing generative adversarial networks (GANs). For example, neural stochastic differential equations (Neural SDEs), treated as infinite-dimensional GANs, have demonstrated successful performance mainly in generating unimodal time series data. In this paper, we propose a novel time series generator, named directed chain GANs (DC-GANs), which inserts a time series dataset (called a neighborhood process of the directed chain or input) into the drift and diffusion coefficients of the directed chain SDEs with distributional constraints. DC-GANs can generate new time series of the same distribution as the neighborhood process, and the neighborhood process will provide the key step in learning and generating multimodal di
    
[^91]: 探究鲁棒性模型与生成模型之间的联系

    Exploring the Connection between Robust and Generative Models. (arXiv:2304.04033v1 [cs.LG])

    [http://arxiv.org/abs/2304.04033](http://arxiv.org/abs/2304.04033)

    本文探究鲁棒性判别分类器与生成模型之间的联系，并发现在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量，提出了一种名为高能量PGD的新攻击。

    

    本研究将通过分解鲁棒性判别分类器的损失函数来探究鲁棒性判别分类器与能量基模型(EBM)形式的生成模型之间的联系。我们发现，尽管常见的假设是对抗点离开了输入数据的流形，但是在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量。我们提出了两个证据:非定向攻击的概率甚至比自然数据还要高，并且随着攻击强度的增加，其概率也会增加。这使我们能够轻松地检测它们并设计一种名为高能量PGD的新攻击，能够欺骗分类器但具有与数据集相似的能量。

    We offer a study that connects robust discriminative classifiers trained with adversarial training (AT) with generative modeling in the form of Energy-based Models (EBM). We do so by decomposing the loss of a discriminative classifier and showing that the discriminative model is also aware of the input data density. Though a common assumption is that adversarial points leave the manifold of the input data, our study finds out that, surprisingly, untargeted adversarial points in the input space are very likely under the generative model hidden inside the discriminative classifier -- have low energy in the EBM. We present two evidence: untargeted attacks are even more likely than the natural data and their likelihood increases as the attack strength increases. This allows us to easily detect them and craft a novel attack called High-Energy PGD that fools the classifier yet has energy similar to the data set.
    
[^92]: 孟加拉达卡市基于医院的横断面研究：非传染性疾病的患病率及主要风险因素。

    Prevalence and major risk factors of non-communicable diseases: A Hospital-based Cross-Sectional Study in Dhaka, Bangladesh. (arXiv:2303.04808v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2303.04808](http://arxiv.org/abs/2303.04808)

    该研究调查了孟加拉国达卡市成年患者中多种非传染性疾病的患病率及其风险因素，其中心血管疾病最为普遍。男性参与者较女性更容易患有心血管疾病，但糖尿病不具有性别倾向。CVD和DM都会随着年龄的增长而增加。患有肥胖症的住院病人占五分之一。

    

    该研究旨在确定孟加拉国达卡市寻求营养指导的成年患者中多种非传染性疾病（NCD）的患病率，分析其风险因素。结果显示，性别、年龄组、肥胖与NCD（糖尿病、CKD、IBS、心血管疾病、慢性肾脏疾病、甲状腺疾病）之间有关联。NCD中最常见的是心血管问题（CVD），在所有参与者中占83.56%。CVD在男性参与者中更为普遍。相应地，男性参与者的血压分布比女性更高。另一方面，糖尿病并没有性别倾向。无论CVD还是DM，都具有年龄上的进展。慢性呼吸系统疾病在中年参与者中比年轻或老年人更为常见。基于数据，五分之一的住院患者患有肥胖症。我们对合并症进行了分析，发现31.5%的人口仅患有一种NCD，30.1%的人患有两种或两种以上的NCD。

    Objective: The study aimed to determine the prevalence of several non-communicable diseases (NCD) and analyze risk factors among adult patients seeking nutritional guidance in Dhaka, Bangladesh. Result: Our study observed the relationships between gender, age groups, obesity, and NCDs (DM, CKD, IBS, CVD, CRD, thyroid). The most frequently reported NCD was cardiovascular issues (CVD), which was present in 83.56% of all participants. CVD was more common in male participants. Consequently, male participants had a higher blood pressure distribution than females. Diabetes mellitus (DM), on the other hand, did not have a gender-based inclination. Both CVD and DM had an age-based progression. Our study showed that chronic respiratory illness was more frequent in middle-aged participants than in younger or elderly individuals. Based on the data, every one in five hospitalized patients was obese. We analyzed the co-morbidities and found that 31.5% of the population has only one NCD, 30.1% has t
    
[^93]: 关于线性等变可旋神经网络的隐式偏差

    On the Implicit Bias of Linear Equivariant Steerable Networks. (arXiv:2303.04198v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04198](http://arxiv.org/abs/2303.04198)

    本文的研究发现了基于参数化预测器的梯度流收敛于最大余量定义的唯一群不变分类器的方向, 同时，我们还发现了可旋网络在较少变情况下相对于不变网络的改善余量与广义界的优势。

    

    本论文研究了群不变二进制分类中的线性等变可旋神经网络隐式偏差。我们的研究发现，基于参数化预测器的梯度流收敛于最大余量定义的唯一群不变分类器的方向，该方向由输入群操作定义。在输入表示的酉假设下，我们建立了可旋网络和数据增广之间的等效性。此外，我们证明了可旋网络在较少变情况下相对于不变网络的改善余量与广义界的优势。

    We study the implicit bias of gradient flow on linear equivariant steerable networks in group-invariant binary classification. Our findings reveal that the parameterized predictor converges in direction to the unique group-invariant classifier with a maximum margin defined by the input group action. Under a unitary assumption on the input representation, we establish the equivalence between steerable networks and data augmentation. Furthermore, we demonstrate the improved margin and generalization bound of steerable networks over their non-invariant counterparts.
    
[^94]: 语言模型在预后预测中的小样本学习能力

    Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12692](http://arxiv.org/abs/2302.12692)

    本研究探索了语言模型在免疫治疗预后预测中的应用，研究了小样本学习面临的挑战，对比了基线和语言模型的有效性，并发现在准确度方面有显著的改进，突出了自然语言处理在临床研究中改善早期检测和干预不同疾病的潜力。

    

    临床预测是医疗保健行业中的关键任务。然而，迄今为止，建立在理论框架Transformers之上的大型语言模型的成功并未延伸到这个领域。本研究利用真实世界中患者的临床数据和分子特征探索了Transformers和语言模型在免疫治疗预后预测中的应用。本文研究了Transformers相对于常规机器学习方法在临床预测中的潜力，并发现了在预测罕见疾病领域下小样本学习面临的挑战。该研究对比了基线和语言模型在多种癌症类型的预后预测中的有效性，并研究了在小样本情况下不同预先训练的语言模型的影响。结果表明在准确度方面有显著的改进，并突出了自然语言处理在临床研究中改善早期检测和干预不同疾病的潜力。

    Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.
    
[^95]: 基于深度学习和辅助训练数据的多用户活动识别实现人机协作

    Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications. (arXiv:2302.05763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05763](http://arxiv.org/abs/2302.05763)

    本研究通过收集单个用户数据并在后处理中合并数据的方法，实现了多用户活动的识别，有望用于人机协作领域。

    

    人机交互研究逐渐关注多方面场景，即机器人与多个人用户同时交互的场景。 然而，在人机协作方面，研究仍处于早期阶段。处理此类合作的机器学习技术需要的数据比典型的人机交互设置中更不可行。本研究提出了非二元人机协作应用的并行任务场景，并提议一种替代方法来收集与多用户活动相关的数据，即收集与单个用户相关的数据并在后处理中合并它们，以减少产生成双设置录制的努力。收集了单个用户的活动三维骨架姿势并将它们合并成一对来验证该语句，随后，这些数据点被用于分别训练由LSTM网络和VAE 混合而成的模型。

    Human-robot interaction (HRI) research is progressively addressing multi-party scenarios, where a robot interacts with more than one human user at the same time. Conversely, research is still at an early stage for human-robot collaboration. The use of machine learning techniques to handle such type of collaboration requires data that are less feasible to produce than in a typical HRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRC applications. Based upon these concepts, this study also proposes an alternative way of gathering data regarding multi-user activity, by collecting data related to single users and merging them in post-processing, to reduce the effort involved in producing recordings of pair settings. To validate this statement, 3D skeleton poses of activity of single users were collected and merged in pairs. After this, such datapoints were used to separately train a long short-term memory (LSTM) network and a variational autoencoder (VAE) composed
    
[^96]: 高效的对抗性对比学习：基于鲁棒性感知的数据核心集选择

    Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection. (arXiv:2302.03857v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03857](http://arxiv.org/abs/2302.03857)

    该研究提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法，能够有效地加速对抗性对比学习（ACL）并维持其强鲁棒性和泛化性能。

    

    对抗性对比学习（ACL）不需要昂贵的数据注释，但可以输出抵抗对抗性攻击并且适用于广泛下游任务的强鲁棒性表示。然而，ACL需要巨大的运行时间才能生成所有训练数据的对抗变体，这限制了其在大型数据集上的可扩展性。为了加速ACL，本文提出了一种基于鲁棒性感知的数据核心集选择（RCS）方法。RCS不需要标签信息，搜索最小化表示分歧的信息子集，即自然数据和其虚拟对抗变体之间表示的距离。RCS的基本解法是遍历所有可能的子集，计算复杂度高。因此，我们在理论上将RCS转化为子模最大化的替代问题，利用贪心搜索是原问题的有效解决方案，同时具有原问题的最优性保证。实验结果表明，RCS可以通过减少训练数据量有效地加速ACL，并且仍然保持其强鲁棒性和泛化性能。

    Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our compr
    
[^97]: 向大核模型迈进

    Toward Large Kernel Models. (arXiv:2302.02605v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02605](http://arxiv.org/abs/2302.02605)

    本文提出了一种构建大规模通用核模型的方法，这解决了传统核机器中模型大小与数据大小相互耦合的问题，使其能够在大数据集上进行训练。

    

    最近的研究表明，与深度神经网络（DNN）相比，核机器在小数据集上的表现通常可以达到或超过DNN。核机器的兴趣受到其在某些情况下等效于宽神经网络的发现的推动。然而，DNN的一个关键特征是它们能够独立地扩展模型大小和训练数据量，而在传统的核机器中，模型大小与数据大小是相互耦合的。由于这种耦合，将核机器扩展到大数据是计算上具有挑战性的。在本文中，我们提供了一种构建大规模通用核模型的方法，这是核机器的一般化，通过解耦模型和数据，允许在大数据集上进行训练。具体地，我们引入了基于投影双重预处理SGD的EigenPro 3.0算法，并展示了使用现有核方法不可能实现的模型和数据规模的扩展。

    Recent studies indicate that kernel machines can often perform similarly or better than deep neural networks (DNNs) on small datasets. The interest in kernel machines has been additionally bolstered by the discovery of their equivalence to wide neural networks in certain regimes. However, a key feature of DNNs is their ability to scale the model size and training data size independently, whereas in traditional kernel machines model size is tied to data size. Because of this coupling, scaling kernel machines to large data has been computationally challenging. In this paper, we provide a way forward for constructing large-scale general kernel models, which are a generalization of kernel machines that decouples the model and data, allowing training on large datasets. Specifically, we introduce EigenPro 3.0, an algorithm based on projected dual preconditioned SGD and show scaling to model and data sizes which have not been possible with existing kernel methods.
    
[^98]: 在场学习者能否从演示中学习推理概念？

    Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01692](http://arxiv.org/abs/2212.01692)

    本文介绍了一种概念性少样本学习方法，以帮助在场学习者学习新技能。通过选择与预测示例共享可能信息的演示，这个方法可以在模型记忆独立的情况下区分模型的在场学习能力。

    

    大型语言模型展示了从少量输入-输出演示中学习新任务的新能力。然而，最近的研究表明，在场学习者大部分依赖于他们的预训练知识，如标签的情感，而不是在输入中找到新的关联性。然而，常用的少样本评估设置使用随机选择的在场演示无法区分模型从演示中学习新技能的能力，因为大部分随机选择的演示并不呈现超越暴露于新任务分布的预测的关系。为了在模型记忆独立的情况下区分模型的在场学习能力，我们引入了一个概念性少样本学习方法，选择与预测示例共享可能信息的演示。我们从注释解释中提取了一组这样的概念，并测量了模型展示这些概念可以获得多少好处。

    Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
    
[^99]: xTrimoABFold：无多序列比对的新型抗体结构预测方法

    xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)

    [http://arxiv.org/abs/2212.00735](http://arxiv.org/abs/2212.00735)

    xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。

    

    在抗体工程领域，设计一个新型抗体以正确地结合特定抗原的表位是一项重要的任务。了解抗体结构和其表位可以促进对其功能的机制理解。因此，从其序列预测抗体结构一直是一项高度有价值的任务，而AlphaFold2提供了一种基于蛋白质序列预测蛋白质结构的解决方案，但对于抗体，特别是对于抗体的互补决定区（CDRs），其预测效率和准确性有限制。

    In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
    
[^100]: 从大数据角度看企业财务风险分析的综述研究

    A Comprehensive Survey on Enterprise Financial Risk Analysis from Big Data Perspective. (arXiv:2211.14997v3 [q-fin.RM] UPDATED)

    [http://arxiv.org/abs/2211.14997](http://arxiv.org/abs/2211.14997)

    本文从大数据角度综述了企业财务风险分析的研究现状，回顾了250多篇代表性文章。

    

    企业财务风险分析旨在预测企业未来的财务风险。由于其广泛而重要的应用，企业财务风险分析一直是金融和管理领域的核心研究主题。基于先进的计算机科学和人工智能技术，企业风险分析研究正在经历快速发展并取得重要进展。因此，全面评估相关研究既有必要性又具挑战性。虽然已经存在一些有价值和令人印象深刻的关于企业风险分析的综述，但这些综述单独介绍了方法，缺乏企业财务风险分析的最新进展。相反，本文尝试从大数据的角度提供企业风险分析方法的系统文献综述，回顾了超过250篇代表性文章。

    Enterprise financial risk analysis aims at predicting the future financial risk of enterprises. Due to its wide and significant application, enterprise financial risk analysis has always been the core research topic in the fields of Finance and Management. Based on advanced computer science and artificial intelligence technologies, enterprise risk analysis research is experiencing rapid developments and making significant progress. Therefore, it is both necessary and challenging to comprehensively review the relevant studies. Although there are already some valuable and impressive surveys on enterprise risk analysis from the perspective of Finance and Management, these surveys introduce approaches in a relatively isolated way and lack recent advances in enterprise financial risk analysis. In contrast, this paper attempts to provide a systematic literature survey of enterprise risk analysis approaches from Big Data perspective, which reviews more than 250 representative articles in the 
    
[^101]: 基于排序池化和快速傅里叶变换的多步短期风速预测

    Multi-Step Short-Term Wind Speed Prediction with Rank Pooling and Fast Fourier Transformation. (arXiv:2211.14434v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14434](http://arxiv.org/abs/2211.14434)

    本文提出了一种新型深度混合模型LR-FFT-RP-MLP/LSTM，利用排序池化和快速傅里叶变换进行多步短期风速预测，实验结果表明其优于几个基准模型并达到最先进水平。

    

    短期风速预测对于经济利用风能至关重要。现实世界中的风速数据通常是间歇性和波动性的，对现有浅层模型提出了很大的挑战。本文提出了一种新型深度混合模型用于多步风速预测，名为LR-FFT-RP-MLP/LSTM（线性快速傅里叶变换排序池化多层感知/长短期记忆）。我们的混合模型同时处理本地和全局输入特征。我们利用排序池化（RP）进行本地特征提取以捕捉时间结构同时保持时间顺序。此外，为了了解风的周期性模式，我们利用快速傅里叶变换（FFT）提取全局特征和风速数据中相关的频率分量。产生的本地和全局特征分别与原始数据集成，然后输入MLP/LSTM层进行初始的风速预测。最后，预测结果基于先前的多步预测进行更新。我们在两个真实世界数据集上进行了实验，证明我们提出的模型优于几个基准模型，达到了最先进的结果。

    Short-term wind speed prediction is essential for economical wind power utilization. The real-world wind speed data is typically intermittent and fluctuating, presenting great challenges to existing shallow models. In this paper, we present a novel deep hybrid model for multi-step wind speed prediction, namely LR-FFT-RP-MLP/LSTM (Linear Fast Fourier Transformation Rank Pooling Multiple-Layer Perception/Long Short-Term Memory). Our hybrid model processes the local and global input features simultaneously. We leverage Rank Pooling (RP) for the local feature extraction to capture the temporal structure while maintaining the temporal order. Besides, to understand the wind periodic patterns, we exploit Fast Fourier Transformation (FFT) to extract global features and relevant frequency components in the wind speed data. The resulting local and global features are respectively integrated with the original data and are fed into an MLP/LSTM layer for the initial wind speed predictions. Finally,
    
[^102]: 自动化复制/粘贴攻击对深度神经网络的诊断

    Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks. (arXiv:2211.10024v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10024](http://arxiv.org/abs/2211.10024)

    本文提出了一种基于嵌入方法的SNAFUE技术，用于寻找复制/粘贴攻击，并使用该技术对ImageNet分类器进行了测试并发现许多易于描述的漏洞。

    

    本文研究了如何帮助人类监督深度神经网络（DNN）。对抗样本可以通过揭示DNN的弱点来帮助，但很难解释或从中得出可实施的结论。先前的工作提出了使用人类可解释的对抗攻击，包括复制/粘贴攻击，其中一张自然图像粘贴到另一张图像会导致意外的分类错误。本文在此基础上提出了两个贡献。首先，我们介绍了使用嵌入的SNAFUE（Search for Natural Adversarial Features Using Embeddings）寻找复制/粘贴攻击的全自动方法。其次，我们使用SNAFUE来测试ImageNet分类器。我们重现了先前工作中的复制/粘贴攻击，并发现了数百个其他易于描述的漏洞，全过程无需人为干预。代码已在 https://github.com/thestephencasper/snafue 上公开。

    This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue
    
[^103]: 基于多模态AI和卫星影像预测空气质量

    Predicting air quality via multimodal AI and satellite imagery. (arXiv:2211.00780v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00780](http://arxiv.org/abs/2211.00780)

    本文旨在利用卫星和地面数据创建一个多模态机器学习模型，以在没有监测站的情况下预测空气污染物分布和改变社会工业行为，并提供了一个新的欧洲污染监测数据集。

    

    气候变化是当前地球面临的最重要的环境问题，影响着地球上的所有生物。由于空气质量监测站通常是地面站，它们检测污染物分布的能力通常受到限制。然而，卫星具有在大范围内研究大气的潜力。欧洲空间局的哥白尼计划卫星“Sentinel-5P”是一颗新近发射的卫星，能够测量各种污染物信息并提供公开的数据输出。本文旨在创建一个多模态机器学习模型，用于在不存在监测站的情况下预测空气质量指标。该模型的输入将包括地面测量和卫星数据的融合，旨在突出污染物分布并促进社会和工业行为的改变。一个新的欧洲污染监测站测量数据集也被创建。

    Climate change may be classified as the most important environmental problem that the Earth is currently facing, and affects all living species on Earth. Given that air-quality monitoring stations are typically ground-based their abilities to detect pollutant distributions are often restricted to wide areas. Satellites however have the potential for studying the atmosphere at large; the European Space Agency (ESA) Copernicus project satellite, "Sentinel-5P" is a newly launched satellite capable of measuring a variety of pollutant information with publicly available data outputs. This paper seeks to create a multi-modal machine learning model for predicting air-quality metrics where monitoring stations do not exist. The inputs of this model will include a fusion of ground measurements and satellite data with the goal of highlighting pollutant distribution and motivating change in societal and industrial behaviors. A new dataset of European pollution monitoring station measurements is cr
    
[^104]: 通过学习传播算子，改进图神经网络

    Improving Graph Neural Networks with Learnable Propagation Operators. (arXiv:2210.17224v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17224](http://arxiv.org/abs/2210.17224)

    本文通过引入可学习的通道加权因子ω，学习和混合多个平滑和锐化滤波器，改进了图神经网络（GNNs），避免了全局平滑现象。

    

    图神经网络（GNNs）的传播算子受到限制。在许多情况下，这些算子通常只含有非负元素，并且在通道之间共享，限制了GNNs的表现力。此外，一些GNNs受到全局平滑的限制，而无法很好地表达复杂的网络结构。另一方面，卷积神经网络（CNNs）可以学习多样的传播滤波器，并且通常不会表现出全局平滑的现象。本文通过将可学习的通道加权因子ω纳入每层中，学习和混合多个平滑和锐化滤波器，来填补这些差距。我们制定了一个通用方法，称为ωGNN，并且易于实现。我们研究了两种变体：ωGCN和ωGAT。对于ωGCN，我们从理论上分析了其行为和ω对所得到的节点特征的影响。我们的实验结果证实了这些发现，并解释了这两种变体如何避免全局平滑现象。

    Graph Neural Networks (GNNs) are limited in their propagation operators. In many cases, these operators often contain non-negative elements only and are shared across channels, limiting the expressiveness of GNNs. Moreover, some GNNs suffer from over-smoothing, limiting their depth. On the other hand, Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and phenomena like over-smoothing are typically not apparent in CNNs. In this paper, we bridge these gaps by incorporating trainable channel-wise weighting factors $\omega$ to learn and mix multiple smoothing and sharpening propagation operators at each layer. Our generic method is called $\omega$GNN, and is easy to implement. We study two variants: $\omega$GCN and $\omega$GAT. For $\omega$GCN, we theoretically analyse its behaviour and the impact of $\omega$ on the obtained node features. Our experiments confirm these findings, demonstrating and explaining how both variants do not over-smooth. Additionally, we ex
    
[^105]: 用提示蕴涵重新思考事件编码管道

    Rethinking the Event Coding Pipeline with Prompt Entailment. (arXiv:2210.05257v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.05257](http://arxiv.org/abs/2210.05257)

    提出了一种名为PR-ENT的事件编码方法，用于从非结构化全文事件描述中提取事件类型。该方法利用预训练语言模型填充事件描述中的模板，并在文本隐含关系任务中选择答案候选项，同时保持了高准确性和资源效率。

    

    为了监测危机，政治事件从新闻中提取。庞大的非结构化全文事件描述使得逐案分析难以控制，尤其是对于低资源的人道主义援助组织来说。这就需要对事件进行分类，这一任务被称为事件编码。在这项工作中，我们提出了PR-ENT，这是一种新的事件编码方法，更加灵活和资源高效，同时保持了竞争性的准确性。

    For monitoring crises, political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable, particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types, a task referred to as event coding. Typically, domain experts craft an event type ontology, annotators label a large dataset and technical experts develop a supervised coding system. In this work, we propose PR-ENT, a new event coding approach that is more flexible and resource-efficient, while maintaining competitive accuracy: first, we extend an event description such as "Military injured two civilians'' by a template, e.g. "People were [Z]" and prompt a pre-trained (cloze) language model to fill the slot Z. Second, we select answer candidates Z* = {"injured'', "hurt"...} by treating the event description as premise and the filled templates as hypothesis in a textual entailment task. This allo
    
[^106]: FLamby：适用于现实医疗环境中跨设备联合学习的数据集和基准

    FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings. (arXiv:2210.04620v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04620](http://arxiv.org/abs/2210.04620)

    这篇论文介绍了FLamby，一个跨设备联合学习的逼真健康医疗数据集和基准，以帮助促进跨设备FL的应用和研究。

    

    联合学习（FL）是一种新颖的方法，使持有敏感数据的多个客户端能够协作地训练机器学习模型，而不需要集中数据。跨设备联合学习是指少量（2-50）可靠客户端的情况，每个客户端持有中到大型的数据集，并且通常出现在医疗、金融或工业等应用中。虽然之前的工作已经为跨设备FL提出了代表性数据集，但很少有逼真的健康医疗跨设备FL数据集存在，从而减缓了这一关键应用的算法研究。在这项工作中，我们提出了一个新颖的跨设备数据集套件，FLamby（您的跨设备联合学习充足基准），专注于医疗保健，以弥合跨设备FL理论和实践之间的差距。FLamby包括7个医疗保健数据集，具有自然拆分，涵盖多个任务、模态和数据量，每个数据集都配有基线训练代码。作为示例，我们还对FLamby进行了详细的性能评估。

    Federated Learning (FL) is a novel approach enabling several clients holding sensitive data to collaboratively train machine learning models, without centralizing data. The cross-silo FL setting corresponds to the case of few ($2$--$50$) reliable clients, each holding medium to large datasets, and is typically found in applications such as healthcare, finance, or industry. While previous works have proposed representative datasets for cross-device FL, few realistic healthcare cross-silo FL datasets exist, thereby slowing algorithmic research in this critical application. In this work, we propose a novel cross-silo dataset suite focused on healthcare, FLamby (Federated Learning AMple Benchmark of Your cross-silo strategies), to bridge the gap between theory and practice of cross-silo FL. FLamby encompasses 7 healthcare datasets with natural splits, covering multiple tasks, modalities, and data volumes, each accompanied with baseline training code. As an illustration, we additionally ben
    
[^107]: 基于图正则化神经网络的高光谱数据树种分类方法

    Tree species classification from hyperspectral data using graph-regularized neural networks. (arXiv:2208.08675v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.08675](http://arxiv.org/abs/2208.08675)

    本文提出了一种基于图正则化神经网络的树种分类方法，该方法在稀疏注释的数据集上使用标签传播技术，能够达到高精度的分类效果；同时，该方法在半监督方法方面具有竞争力。

    

    本文提出了一种新颖的图正则化神经网络（GRNN）算法，用于树种分类。该算法包括基于超像素的分割用于图像构建、基于像素的神经网络分类器，以及标签传播技术，用于在稀疏注释的数据集上生成准确且真实的分类地图（模拟树冠）。与多种先进技术相比，GRNN在标准的印度玉米高光谱图像中表现出更优的性能，并且在新的在法属圭亚那森林收集的高光谱图像上（当标注像素少于1％时），达到高达92％的分类精度。此外，我们进一步证明了GRNN在半监督方法方面的竞争力，并且在不同数量的训练样本和多次随机抽样训练标记像素的重复试验中表现出小的精度偏差。

    We propose a novel graph-regularized neural network (GRNN) algorithm for tree species classification. The proposed algorithm encompasses superpixel-based segmentation for graph construction, a pixel-wise neural network classifier, and the label propagation technique to generate an accurate and realistic (emulating tree crowns) classification map on a sparsely annotated data set. GRNN outperforms several state-of-the-art techniques not only for the standard Indian Pines HSI but also achieves a high classification accuracy (approx. 92%) on a new HSI data set collected over the heterogeneous forests of French Guiana (FG) when less than 1% of the pixels are labeled. We further show that GRNN is competitive with the state-of-the-art semi-supervised methods and exhibits a small deviation in accuracy for different numbers of training samples and over repeated trials with randomly sampled labeled pixels for training.
    
[^108]: 甲状腺结节超声图像的深度学习分类：基于独立数据集的验证

    Deep Learning for Classification of Thyroid Nodules on Ultrasound: Validation on an Independent Dataset. (arXiv:2207.13765v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2207.13765](http://arxiv.org/abs/2207.13765)

    该研究使用深度学习算法对甲状腺结节超声图像进行分类，结果与放射科医生相似，并验证了该算法的泛化能力。

    

    目的：将先前验证的深度学习算法应用于新的甲状腺结节超声图像数据集，并将其性能与放射科医生进行比较。方法：先前的研究呈现了一种算法，该算法能够检测甲状腺结节，然后使用两个超声图像进行恶性分类。多任务深度卷积神经网络是从1278个结节中训练出来的，最初使用99个不同的结节进行测试。结果与放射科医生的结果相当。该算法随后使用了378个结节的超声机成像数据进行测试，这些数据与训练集案例的制造商和产品类型不同。要求4名经验丰富的放射科医生评估结节，以与深度学习进行比较。结果：使用参数化、双正态估计计算了深度学习算法和4名放射科医生的AUC。对于深度学习算法，AUC为0.69（95％CI：0.64 0.75）。放射科医生的AUC值为...

    Objectives: The purpose is to apply a previously validated deep learning algorithm to a new thyroid nodule ultrasound image dataset and compare its performances with radiologists. Methods: Prior study presented an algorithm which is able to detect thyroid nodules and then make malignancy classifications with two ultrasound images. A multi-task deep convolutional neural network was trained from 1278 nodules and originally tested with 99 separate nodules. The results were comparable with that of radiologists. The algorithm was further tested with 378 nodules imaged with ultrasound machines from different manufacturers and product types than the training cases. Four experienced radiologists were requested to evaluate the nodules for comparison with deep learning. Results: The Area Under Curve (AUC) of the deep learning algorithm and four radiologists were calculated with parametric, binormal estimation. For the deep learning algorithm, the AUC was 0.69 (95% CI: 0.64 0.75). The AUC of ra
    
[^109]: BigIssue：一个真实的漏洞定位基准测试

    BigIssue: A Realistic Bug Localization Benchmark. (arXiv:2207.10739v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10739](http://arxiv.org/abs/2207.10739)

    BigIssue是一个真实的漏洞定位基准测试，旨在提高模型的漏洞定位能力和自动程序修复的性能，从而帮助开发人员编写更好的代码。

    

    随着机器学习工具的进步，不可避免地产生了一个问题：机器学习如何帮助我们编写更好的代码？随着GPT-3和Bert等模型在自然语言处理方面取得的重大进展，开始探索将自然语言处理技术应用于代码。大多数研究集中在自动程序修复（APR）上，虽然在合成或高度过滤的数据集上的结果很有前途，但这些模型很难应用于现实场景，因为存在定位漏洞不足的问题。我们提出BigIssue：一个真实的漏洞定位基准测试。基准测试的目标是两个方面。首先，我们提供了一个包含多种实际和合成Java漏洞的通用基准；第二，通过注意到完整存储库上下文，激励提高模型的漏洞定位能力。引入BigIssue后，我们希望推进漏洞定位技术的最新进展，从而提高APR的性能，并最终帮助开发人员编写更好的代码。

    As machine learning tools progress, the inevitable question arises: How can machine learning help us write better code? With significant progress being achieved in natural language processing with models like GPT-3 and Bert, the applications of natural language processing techniques to code are starting to be explored. Most of the research has been focused on automatic program repair (APR), and while the results on synthetic or highly filtered datasets are promising, such models are hard to apply in real-world scenarios because of inadequate bug localization. We propose BigIssue: a benchmark for realistic bug localization. The goal of the benchmark is two-fold. We provide (1) a general benchmark with a diversity of real and synthetic Java bugs and (2) a motivation to improve bug localization capabilities of models through attention to the full repository context. With the introduction of BigIssue, we hope to advance the state of the art in bug localization, in turn improving APR perfor
    
[^110]: 小分子质谱多尺度正弦嵌入能够提高高分辨质谱数据的学习能力

    Multi-scale Sinusoidal Embeddings Enable Learning on High Resolution Mass Spectrometry Data. (arXiv:2207.02980v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02980](http://arxiv.org/abs/2207.02980)

    本文介绍了一种新方法，通过对高分辨质谱数据进行多尺度正弦嵌入训练，展示了一种新的对 MS2 数据的光谱库搜索和新的化学性质预测任务。同时还介绍了一种新颖的高吞吐实验以及医药化学家关注的 10 种化学性质的预测。在这些任务中，本文提出了一种具有最先进性能的模型。

    

    生物样本中的小分子可用于提供关于疾病状态、环境毒素、天然产物药物发现及许多其他应用的信息。串联质谱（MS2）是了解小分子混合物组成的主要界面，它产生的数据具有高灵敏度和百万分之一（ppm级别）的分辨率。我们采用针对 MS2 质谱数据中质量数据的多尺度正弦嵌入，旨在满足从完整 MS2 数据进行学习的挑战。利用这些嵌入，我们为光谱图书馆搜索提供了一种新的最先进模型，这是评估 MS2 数据的标准任务。我们还引入了一个新的任务——通过 MS2 数据进行化学性质预测——该任务在高通量 MS2 实验中具有自然应用，并且展示了在医药化学家优先考虑的 10 种化学性质中，可以实现对新化合物平均 R² 值为 80％。我们使用降维技术并进行实验来研究这些任务。

    Small molecules in biological samples are studied to provide information about disease states, environmental toxins, natural product drug discovery, and many other applications. The primary window into the composition of small molecule mixtures is tandem mass spectrometry (MS2), which produces data that are of high sensitivity and part per million resolution. We adopt multi-scale sinusoidal embeddings of the mass data in MS2 designed to meet the challenge of learning from the full resolution of MS2 data. Using these embeddings, we provide a new state of the art model for spectral library search, the standard task for initial evaluation of MS2 data. We also introduce a new task, chemical property prediction from MS2 data, that has natural applications in high-throughput MS2 experiments and show that an average $R^2$ of 80\% for novel compounds can be achieved across 10 chemical properties prioritized by medicinal chemists. We use dimensionality reduction techniques and experiments with 
    
[^111]: 带有样本成对约束的深度多视图半监督聚类

    Deep Multi-View Semi-Supervised Clustering with Sample Pairwise Constraints. (arXiv:2206.04949v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.04949](http://arxiv.org/abs/2206.04949)

    本文提出了一种新型的深度多视图半监督聚类方法（DMSC），它通过共同优化多种损失来提高聚类性能，包括多视图聚类损失、半监督成对约束损失和多个自动编码器重构损失。其中，KL散度基础的多视图聚类损失被特别强调用于多视图数据的共同表示，以达到异构特征优化、多视图加权和聚类预测的目的。

    

    多视图聚类因其集成多源信息的能力而受到了广泛关注。虽然在过去的几十年中提出了许多先进的方法，但它们中的大多数通常忽视了弱监督信息的重要性，并且不能保留多个视图的特征属性，从而导致聚类性能不理想。为了解决这些问题，本文提出了一种新型的深度多视图半监督聚类方法（DMSC），在网络微调期间共同优化三种损失，包括多视图聚类损失、半监督成对约束损失和多个自动编码器重构损失。具体而言，KL散度基础的多视图聚类损失被强加在多视图数据的共同表示上，同时执行异构特征优化、多视图加权和聚类预测。然后，我们创新地提出了将成对约束整合在损失函数中的方法。

    Multi-view clustering has attracted much attention thanks to the capacity of multi-source information integration. Although numerous advanced methods have been proposed in past decades, most of them generally overlook the significance of weakly-supervised information and fail to preserve the feature properties of multiple views, thus resulting in unsatisfactory clustering performance. To address these issues, in this paper, we propose a novel Deep Multi-view Semi-supervised Clustering (DMSC) method, which jointly optimizes three kinds of losses during networks finetuning, including multi-view clustering loss, semi-supervised pairwise constraint loss and multiple autoencoders reconstruction loss. Specifically, a KL divergence based multi-view clustering loss is imposed on the common representation of multi-view data to perform heterogeneous feature optimization, multi-view weighting and clustering prediction simultaneously. Then, we innovatively propose to integrate pairwise constraints
    
[^112]: U-NO：U形神经算子

    U-NO: U-shaped Neural Operators. (arXiv:2204.11127v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.11127](http://arxiv.org/abs/2204.11127)

    本文提出了U-NO，一种U形记忆增强架构，允许更深的神经算子，通过利用函数预测中的问题结构，在解决偏微分方程方面表现出快速训练、数据效率和对超参数选择的鲁棒性。

    

    神经算子将经典神经网络推广到了无限维空间之间的映射，例如函数空间。先前的神经算子研究提出了一系列新方法来学习这样的映射，并在学习偏微分方程的解算子方面取得了空前的成功。由于它们与全连接架构非常接近，这些模型主要受到高内存使用率的困扰，并且通常仅限于浅层的深度学习模型。在本文中，我们提出了U-NO（U形神经算子），这是一种U形记忆增强架构，允许更深的神经算子。U-NO利用函数预测中的问题结构，并展示了快速训练、数据效率和对超参数选择的鲁棒性。我们研究了U-NO在PDE基准测试中的表现，即Darcy流动定律和Navier-Stokes方程。我们展示了U-NO在Darcy流和Tu的平均预测改进分别达到26％和44％。

    Neural operators generalize classical neural networks to maps between infinite-dimensional spaces, e.g., function spaces. Prior works on neural operators proposed a series of novel methods to learn such maps and demonstrated unprecedented success in learning solution operators of partial differential equations. Due to their close proximity to fully connected architectures, these models mainly suffer from high memory usage and are generally limited to shallow deep learning models. In this paper, we propose U-shaped Neural Operator (U-NO), a U-shaped memory enhanced architecture that allows for deeper neural operators. U-NOs exploit the problem structures in function predictions and demonstrate fast training, data efficiency, and robustness with respect to hyperparameters choices. We study the performance of U-NO on PDE benchmarks, namely, Darcy's flow law and the Navier-Stokes equations. We show that U-NO results in an average of 26% and 44% prediction improvement on Darcy's flow and tu
    
[^113]: 离线增强学习用于更安全地控制1型糖尿病患者的血糖水平

    Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes. (arXiv:2204.03376v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03376](http://arxiv.org/abs/2204.03376)

    本论文研究了离线强化学习在控制1型糖尿病患者血糖水平中的应用，采用BCQ、CQL和TD3-BC算法有效地管理血糖水平，克服了在线学习过程的不稳定性和不安全性。

    

    有效的混合闭环系统的广泛应用将是1型糖尿病患者护理中的一个重要里程碑。这些设备通常利用简单的控制算法选择最佳胰岛素剂量来维持血糖水平在健康范围内。在线强化学习已被用作进一步增强这些设备的血糖控制方法。先前的方法已被证明与传统控制算法相比降低了患者风险，并改善了在目标范围内的时间，但往往在学习过程中不稳定，导致选择不安全的行为。本文提出了一种离线强化学习评估方法，以开发有效的剂量策略，无需在训练过程中进行潜在危险的患者交互。本文研究了BCQ、CQL和TD3-BC在管理FDA批准的30名虚拟病人的血糖水平中的效用。

    The widespread adoption of effective hybrid closed loop systems would represent an important milestone of care for people living with type 1 diabetes (T1D). These devices typically utilise simple control algorithms to select the optimal insulin dose for maintaining blood glucose levels within a healthy range. Online reinforcement learning (RL) has been utilised as a method for further enhancing glucose control in these devices. Previous approaches have been shown to reduce patient risk and improve time spent in the target range when compared to classical control algorithms, but are prone to instability in the learning process, often resulting in the selection of unsafe actions. This work presents an evaluation of offline RL for developing effective dosing policies without the need for potentially dangerous patient interaction during training. This paper examines the utility of BCQ, CQL and TD3-BC in managing the blood glucose of the 30 virtual patients available within the FDA-approved
    
[^114]: 探究强化学习中神经网络表示的特征

    Investigating the Properties of Neural Network Representations in Reinforcement Learning. (arXiv:2203.15955v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.15955](http://arxiv.org/abs/2203.15955)

    本文探究了深度强化学习系统学习到的表示特征及其对迁移学习的支持能力，并在一个像素导航环境中考虑了具有不同辅助损失的深度Q学习代理。

    

    本文研究了深度强化学习系统学习到的表示特征。在强化学习表示方面的早期工作主要集中在设计固定基础架构上，以达到理想的特征，如正交性和稀疏性。相比之下，深度强化学习方法的理念是代理设计者不应编码表示特征，而应该让数据流决定表示的特征——在适当的训练方案下，良好的表示会显现出来。本文将这两个视角结合起来，通过对超过25,000个代理任务设置的实证研究，探究支持强化学习中迁移性的表示特征。我们引入并测量了六个表征特征。我们在一个基于像素的导航环境中考虑了具有不同辅助损失的深度Q学习代理，包括源和传输。

    In this paper we investigate the properties of representations learned by deep reinforcement learning systems. Much of the early work on representations for reinforcement learning focused on designing fixed-basis architectures to achieve properties thought to be desirable, such as orthogonality and sparsity. In contrast, the idea behind deep reinforcement learning methods is that the agent designer should not encode representational properties, but rather that the data stream should determine the properties of the representation -- good representations emerge under appropriate training schemes. In this paper we bring these two perspectives together, empirically investigating the properties of representations that support transfer in reinforcement learning. We introduce and measure six representational properties over more than 25 thousand agent-task settings. We consider Deep Q-learning agents with different auxiliary losses in a pixel-based navigation environment, with source and tran
    
[^115]: ADATIME：针对时间序列数据的领域自适应基准测试套件

    ADATIME: A Benchmarking Suite for Domain Adaptation on Time Series Data. (arXiv:2203.08321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.08321](http://arxiv.org/abs/2203.08321)

    ADATIME是一个用于时间序列领域自适应的评估套件，其所提供的标准化骨干神经网络架构和数据集、现实的模型选择方法可以无监督地进行领域自适应。

    

    无监督的领域自适应方法旨在针对可能与训练数据不同（转移）的未标记测试数据具有良好的泛化能力。这样的方法通常是在图像数据上开发的，它们在时间序列数据上的应用还不够深入。现有的时间序列领域自适应工作存在评估方案、数据集和骨干神经网络架构不一致的问题。此外，标记的目标数据经常用于模型选择，这违反了无监督领域自适应的基本假设。为解决这些问题，我们开发了一个评估套件（AdaTime）来系统地和公正地评估不同的时间序列领域自适应方法。具体而言，我们标准化了骨干神经网络架构和基准测试数据集，同时探索更现实的模型选择方法，可以在没有标记数据或仅有少量标记样本的情况下工作。我们的评估包括对不同领域自适应方法进行的适应性测试。

    Unsupervised domain adaptation methods aim to generalize well on unlabeled test data that may have a different (shifted) distribution from the training data. Such methods are typically developed on image data, and their application to time series data is less explored. Existing works on time series domain adaptation suffer from inconsistencies in evaluation schemes, datasets, and backbone neural network architectures. Moreover, labeled target data are often used for model selection, which violates the fundamental assumption of unsupervised domain adaptation. To address these issues, we develop a benchmarking evaluation suite (AdaTime) to systematically and fairly evaluate different domain adaptation methods on time series data. Specifically, we standardize the backbone neural network architectures and benchmarking datasets, while also exploring more realistic model selection approaches that can work with no labeled data or just a few labeled samples. Our evaluation includes adapting st
    
[^116]: PyNET-QxQ: 一种高效的用于CMOS图像传感器中QxQ Bayer模式解析的PyNET变体（arXiv:2203.04314v2 [eess.IV] UPDATED）

    PyNET-QxQ: An Efficient PyNET Variant for QxQ Bayer Pattern Demosaicing in CMOS Image Sensors. (arXiv:2203.04314v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.04314](http://arxiv.org/abs/2203.04314)

    PyNET-QxQ是一种用于QxQ Bayer CFA模式的轻量级解析模型，参数少于原始PyNET的2.5％，并采用了一种称为渐进蒸馏的知识蒸馏方法来训练网络。实验表明，PyNET-QxQ在保持图像质量的同时，具有较高的效率和较低的计算成本。

    

    基于深度学习的移动相机图像信号处理（ISP）模型可以生成与专业DSLR相机相媲美的高质量图像。然而，它们的计算要求通常使它们不适合移动设置。此外，现代移动相机采用非Bayer色彩滤色阵列（CFA），如Quad Bayer，Nona Bayer和QxQ Bayer，以提高图像质量，然而，大多数现有的基于深度学习的ISP（或解码）模型主要专注于标准Bayer CFAs。在本研究中，我们提出了PyNET-QxQ，这是一种专门针对QxQ Bayer CFA模式设计的轻量级解析模型，它是从原始的PyNET导出的。我们还提出了一种知识蒸馏方法，称为渐进蒸馏，以更有效地训练减少的网络。因此，PyNET-QxQ的参数少于原始PyNET的2.5％，同时保持其性能。使用原型QxQ相机传感器捕获的QxQ图像的实验表明，PyNET-QxQ在保持图像质量的同时，具有较高的效率和较低的计算成本。

    Deep learning-based image signal processor (ISP) models for mobile cameras can generate high-quality images that rival those of professional DSLR cameras. However, their computational demands often make them unsuitable for mobile settings. Additionally, modern mobile cameras employ non-Bayer color filter arrays (CFA) such as Quad Bayer, Nona Bayer, and QxQ Bayer to enhance image quality, yet most existing deep learning-based ISP (or demosaicing) models focus primarily on standard Bayer CFAs. In this study, we present PyNET-QxQ, a lightweight demosaicing model specifically designed for QxQ Bayer CFA patterns, which is derived from the original PyNET. We also propose a knowledge distillation method called progressive distillation to train the reduced network more effectively. Consequently, PyNET-QxQ contains less than 2.5% of the parameters of the original PyNET while preserving its performance. Experiments using QxQ images captured by a proto type QxQ camera sensor show that PyNET-QxQ o
    
[^117]: 使用区分特征度量下游分类的自监督表示质量

    Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01881](http://arxiv.org/abs/2203.01881)

    从自监督学习模型中提取区分特征，并使用它们压缩表示空间，提出了一种Q-Score自监督表示质量分数，可以可靠地预测线性评估期间的错误分类。

    

    自监督学习在下游分类任务中展现出了惊人的结果。然而，对于它们的失败模式和学习表示的解释，存在着有限的研究。本文研究了 SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg 和 Barlow Twins 等最先进的自监督模型的表示空间。在不使用类标签信息的情况下，我们发现了对应于图像中独特物理属性的区分特征，这些区分特征主要存在于正确分类的表示中。使用这些特征，我们可以将表示空间压缩多达 40%，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或 Q-Score），这是一种模型无关、无监督的分数，可以可靠地预测一个给定样本在线性评估期间是否可能被错误分类，并在 ImageNet-100 和 ImageNet-1K 上实现了 AUPRC 分别为 91.45 和 78.78。

    Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
    
[^118]: 3D Object Detection Models and Methods 的调查和系统化研究

    Survey and Systematization of 3D Object Detection Models and Methods. (arXiv:2201.09354v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.09354](http://arxiv.org/abs/2201.09354)

    本文就3D目标检测领域进行了全面的调查和系统化研究，提出了一种实用框架用于比较3D目标检测方法，并帮助研究人员和实践者快速了解该领域。

    

    自动驾驶车辆的强烈需求和3D传感器广泛普及，不断推动着3D目标检测方法的提出。本文全面调研了2012年至2021年间3D目标检测领域的最新发展，包括输入数据、数据表示和特征提取以及实际检测模块的完整流程。介绍了基本概念，关注了过去十年里涌现的各种不同方法，并提出了一种系统化的方法，为比较这些方法提供了实用框架，以指导未来的开发、评估和应用活动。具体而言，本调查和系统化研究可以帮助研究人员和实践者快速了解该领域，将3D目标检测的解决方案分解为更易处理的部分。

    Strong demand for autonomous vehicles and the wide availability of 3D sensors are continuously fueling the proposal of novel methods for 3D object detection. In this paper, we provide a comprehensive survey of recent developments from 2012-2021 in 3D object detection covering the full pipeline from input data, over data representation and feature extraction to the actual detection modules. We introduce fundamental concepts, focus on a broad range of different approaches that have emerged over the past decade, and propose a systematization that provides a practical framework for comparing these approaches with the goal of guiding future development, evaluation and application activities. Specifically, our survey and systematization of 3D object detection models and methods can help researchers and practitioners to get a quick overview of the field by decomposing 3DOD solutions into more manageable pieces.
    
[^119]: 可微分高斯化层用于深度生成模型正则化的逆问题

    Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models. (arXiv:2112.03860v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.03860](http://arxiv.org/abs/2112.03860)

    该论文提出了一种使用可微数据相关层进行重新参数化和高斯化潜在张量的方法，以约束逆问题为获得高保真度的分布内解决方案，有效解决深度生成模型逆问题中潜在张量偏离期望高斯分布的问题。

    

    深度生成模型如GAN、标准化流和扩散模型是逆问题的强大正则化器，可以帮助减小不适定性并获得高质量的结果。然而，在逆推过程中，这些模型的潜在张量可能会从期望的高维标准高斯分布中脱离，特别是在数据噪声和不准确的正向模型存在的情况下，会导致低保真度的解决方案。为解决这个问题，我们提出使用新颖的可微数据相关层重新参数化和高斯化潜在张量，其中使用自定义操作符解决优化问题。这些拟议的层将逆问题约束为获得高保真度的分布内解决方案。我们在三个反演任务（压缩感知MRI、图像去模糊和准确度受限的非线性偏微分方程反演问题“eikonal tomography”）上使用两种典型的深度生成模型进行了验证。

    Deep generative models such as GANs, normalizing flows, and diffusion models are powerful regularizers for inverse problems. They exhibit great potential for helping reduce ill-posedness and attain high-quality results. However, the latent tensors of such deep generative models can fall out of the desired high-dimensional standard Gaussian distribution during inversion, particularly in the presence of data noise and inaccurate forward models, leading to low-fidelity solutions. To address this issue, we propose to reparameterize and Gaussianize the latent tensors using novel differentiable data-dependent layers wherein custom operators are defined by solving optimization problems. These proposed layers constrain inverse problems to obtain high-fidelity in-distribution solutions. We validate our technique on three inversion tasks: compressive-sensing MRI, image deblurring, and eikonal tomography (a nonlinear PDE-constrained inverse problem) using two representative deep generative models
    
[^120]: LOGEN：基于逻辑知识条件的自训练文本生成在少样本下的应用

    LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.01404](http://arxiv.org/abs/2112.01404)

    本文提出了一种基于少样本的逻辑知识条件下文本生成的统一框架LOGEN，通过自训练和基于内容和结构一致性抽样伪逻辑形式，实现了在少量样本下的文本生成。

    

    结构化数据的自然语言生成主要集中在表面层面描述，其存在控制内容选择困难和低保真度的问题。先前的研究利用逻辑形式来促进逻辑知识条件下的文本生成。虽然取得了显著进展，但是它们对数据的需求量较大，这使得在有限数据情况下应用于现实世界应用变得具有挑战性。为此，本文提出了一种基于少样本的逻辑知识条件下文本生成的统一框架。我们的方法只使用少量种子逻辑形式（如20/100种子） ，并利用自训练和基于内容和结构一致性抽样伪逻辑形式。实验结果表明，我们的方法可以比基准方法获得更好的少样本性能。

    Natural language generation from structured data mainly focuses on surface-level descriptions, suffering from uncontrollable content selection and low fidelity. Previous works leverage logical forms to facilitate logical knowledge-conditioned text generation. Though achieving remarkable progress, they are data-hungry, which makes the adoption for real-world applications challenging with limited data. To this end, this paper proposes a unified framework for logical knowledge-conditioned text generation in the few-shot setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach leverages self-training and samples pseudo logical forms based on content and structure consistency. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.
    
[^121]: PredProp: 带精度加权预测编码的双向随机优化方法

    PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding. (arXiv:2111.08792v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08792](http://arxiv.org/abs/2111.08792)

    本文提出了PredProp方法，它通过随机梯度下降和自适应加权参数更新来优化预测编码网络中的权重和状态。通过使用传播误差协方差实现近似自然梯度下降，使得PredProp在测试中表现良好，比Adam表现更优。此外，PredProp可以通过误差精度提高权重参数的优化效果，并且在层次结构中可以分解精度需求。

    

    本文提出了PredProp方法，它基于传播的误差和神经活动的精度，通过随机梯度下降和自适应加权参数更新来优化预测编码网络(PCN)中的权重和状态。由于传播误差协方差与Fisher信息矩阵之间的关系，PredProp实现了近似自然梯度下降。我们在密集解码器网络和简单图像基准数据集的情况下展示了PredProp的有效性。我们发现，在测试配置中，PredProp表现优于Adam，一种广泛使用的自适应学习率优化器。此外，可用于权重参数的优化方法在推理过程中受益于使用PredProp的误差精度。由于层次预测编码层是通过局部误差单独优化的，所以所需精度在层次结构上分解。

    We present PredProp, a method for optimization of weights and states in predictive coding networks (PCNs) based on the precision of propagated errors and neural activity. PredProp jointly addresses inference and learning via stochastic gradient descent and adaptively weights parameter updates by approximate curvature. Due to the relation between propagated error covariance and the Fisher information matrix, PredProp implements approximate Natural Gradient Descent. We demonstrate PredProp's effectiveness in the context of dense decoder networks and simple image benchmark datasets. We found that PredProp performs favorably over Adam, a widely used adaptive learning rate optimizer in the tested configurations. Furthermore, available optimization methods for weight parameters benefit from using PredProp's error precision during inference. Since hierarchical predictive coding layers are optimised individually using local errors, the required precisions factorize over hierarchical layers. Ex
    
[^122]: CaloFlow II: 采用归一化流进一步提高速度并保持准确性的量能器淋浴生成器

    CaloFlow II: Even Faster and Still Accurate Generation of Calorimeter Showers with Normalizing Flows. (arXiv:2110.11377v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2110.11377](http://arxiv.org/abs/2110.11377)

    CaloFlow v2是一种基于归一化流的高保真量能器淋浴仿真生成模型，相对于原版将淋浴生成速度进一步提高了500倍，并且在保真度方面大大超过了以前的最先进水平。

    

    最近，我们介绍了一种基于归一化流的高保真量能器淋浴仿真生成模型CaloFlow。现在，我们介绍了CaloFlow v2，它是我们原来框架的改进版，相对于原版将淋浴生成速度进一步提高了500倍。这种改进基于一种称为概率密度蒸馏的技术，这种技术最初用于ML领域的语音合成，我们通过引入一组强大的新损失项进一步开发了这种技术。我们通过定性（平均图像、高级特征的直方图）和定量（GEANT4和生成样本之间的分类器指标）度量证明CaloFlow v2保持了与原版相同的高保真度。结果是一种量能器淋浴的生成模型，其速度达到了最先进水平（比GEANT4快$10^4$倍），并且在保真度方面大大超过了以前的最先进水平。

    Recently, we introduced CaloFlow, a high-fidelity generative model for GEANT4 calorimeter shower emulation based on normalizing flows. Here, we present CaloFlow v2, an improvement on our original framework that speeds up shower generation by a further factor of 500 relative to the original. The improvement is based on a technique called Probability Density Distillation, originally developed for speech synthesis in the ML literature, and which we develop further by introducing a set of powerful new loss terms. We demonstrate that CaloFlow v2 preserves the same high fidelity of the original using qualitative (average images, histograms of high level features) and quantitative (classifier metric between GEANT4 and generated samples) measures. The result is a generative model for calorimeter showers that matches the state-of-the-art in speed (a factor of $10^4$ faster than GEANT4) and greatly surpasses the previous state-of-the-art in fidelity.
    
[^123]: 利用Transformer GAN生成符号推理问题

    Generating Symbolic Reasoning Problems with Transformer GANs. (arXiv:2110.10054v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.10054](http://arxiv.org/abs/2110.10054)

    本文使用Transformer GAN生成符号推理问题的训练数据，生成的数据可用于替代真实训练数据，并且可用于修改目标分布，达到更好的训练效果。

    

    本文研究了使用Transformer编码器的GAN和Wasserstein GAN的能力，在符号推理领域生成有意义且具有挑战性的训练数据。我们在两个问题领域上进行了实验：符号数学和验证中的时间规范。即使没有自回归，我们的GAN模型也会产生语法正确的实例。我们展示了生成的数据可用作分类器训练的替代真实训练数据，并且，尤其是当数据集太小无法直接训练时，可以从数据集中生成训练数据。使用GAN的设置还允许我们修改目标分布：我们展示了通过在生成器目标中添加分类器不确定性部分，我们获得的数据集比原始数据集更难以为一个时间逻辑分类器所解决。

    We study the capabilities of GANs and Wasserstein GANs equipped with Transformer encoders to generate sensible and challenging training data for symbolic reasoning domains. We conduct experiments on two problem domains where Transformers have been successfully applied recently: symbolic mathematics and temporal specifications in verification. Even without autoregression, our GAN models produce syntactically correct instances. We show that the generated data can be used as a substitute for real training data when training a classifier, and, especially, that training data can be generated from a dataset that is too small to be trained on directly. Using a GAN setting also allows us to alter the target distribution: We show that by adding a classifier uncertainty part to the generator objective, we obtain a dataset that is even harder to solve for a temporal logic classifier than our original dataset.
    
[^124]: CaloFlow: 利用正则化流实现快速准确模拟能量量能器淋浴

    CaloFlow: Fast and Accurate Generation of Calorimeter Showers with Normalizing Flows. (arXiv:2106.05285v3 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2106.05285](http://arxiv.org/abs/2106.05285)

    CaloFlow提出了一种利用正则化流技术实现快速准确模拟能量量能器淋浴，相对于其他最新方法具有多种优势，包括更高的准确性和更稳定的训练，并引入了新的质量评估指标。

    

    我们提出了一种基于正则化流的快速探测器模拟框架，称为CaloFlow。我们首次证明，正则化流能够以极高的准确性重新产生出多通道能量量能器淋浴，提供了一种与计算代价昂贵的GEANT4模拟以及基于GANs和VAE的最新快速模拟框架不同的新选择。除了原有的物理特征的直方图和能量量能器淋浴的图像外，我们引入了一个新的评估生成建模质量的指标：对区分真实与虚构图像的分类器性能。我们证明，GAN生成的图像可以被分类器以接近100％的准确率识别，而由CaloFlow生成的图像则更容易欺骗分类器。总体而言，与其他最新方法（GAN和VAE）相比，正则化流具有多种优势，包括：可追踪的似然函数，稳定和收敛的训练以及主要的优化流程。

    We introduce CaloFlow, a fast detector simulation framework based on normalizing flows. For the first time, we demonstrate that normalizing flows can reproduce many-channel calorimeter showers with extremely high fidelity, providing a fresh alternative to computationally expensive GEANT4 simulations, as well as other state-of-the-art fast simulation frameworks based on GANs and VAEs. Besides the usual histograms of physical features and images of calorimeter showers, we introduce a new metric for judging the quality of generative modeling: the performance of a classifier trained to differentiate real from generated images. We show that GAN-generated images can be identified by the classifier with nearly 100% accuracy, while images generated from CaloFlow are better able to fool the classifier. More broadly, normalizing flows offer several advantages compared to other state-of-the-art approaches (GANs and VAEs), including: tractable likelihoods; stable and convergent training; and princ
    
[^125]: 贝叶斯分层混合聚类中的后验正则化

    Posterior Regularization on Bayesian Hierarchical Mixture Clustering. (arXiv:2105.06903v7 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.06903](http://arxiv.org/abs/2105.06903)

    本文提出了一种后验正则化方法来改进贝叶斯分层混合聚类模型，在每个层级对节点实施最大间隔约束以增强集群的分离。

    

    贝叶斯分层混合聚类通过在生成过程中用层级狄利克雷过程混合模型(HDPMM)替换传统的高斯-高斯核来实现从父节点到子节点的扩散，从而改进了传统的贝叶斯分层聚类。然而，BHMC可能会产生具有高节点方差的树，表明在较高层级之间的节点之间存在较弱的分离。为了解决这个问题，我们采用了后验正则化(Posterior Regularization)，它对每个层级的节点实施最大间隔约束以增强集群的分离。我们阐述了如何将PR应用于BHMC，并证明了它在改进BHMC模型方面的有效性。

    Bayesian hierarchical mixture clustering (BHMC) improves traditionalBayesian hierarchical clustering by replacing conventional Gaussian-to-Gaussian kernels with a Hierarchical Dirichlet Process Mixture Model(HDPMM) for parent-to-child diffusion in the generative process. However,BHMC may produce trees with high nodal variance, indicating weak separation between nodes at higher levels. To address this issue, we employ Posterior Regularization, which imposes max-margin constraints on nodes at every level to enhance cluster separation. We illustrate how to apply PR toBHMC and demonstrate its effectiveness in improving the BHMC model.
    
[^126]: 针对扰动学习节点表示

    Learning Node Representations against Perturbations. (arXiv:2008.11416v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.11416](http://arxiv.org/abs/2008.11416)

    本文讨论如何在GNN中针对扰动学习节点表示，并提出了稳定-可识别GNN反对扰动 (SIGNNAP) 模型以无监督形式学习可靠的节点表示。

    

    最近的图神经网络 (GNN) 在节点表示学习方面取得了显著的表现。GNN的成功关键因素之一是节点表示上的“平滑”属性。尽管如此，大多数GNN模型对图输入的扰动很脆弱，可能会学习到不可靠的节点表示。本文研究如何在GNN中针对扰动学习节点表示。具体而言，我们认为节点表示应在输入略微扰动时保持稳定，并且应能够识别不同结构的节点表示，这两者分别被称为节点表示的“稳定性”和“可识别性”。为此，我们提出了一种名为稳定-可识别GNN反对扰动 (SIGNNAP) 的新模型，该模型以无监督的方式学习可靠的节点表示。SIGNNAP通过对比目标来形式化“稳定性”和“可识别性”，并保留了...

    Recent graph neural networks (GNN) has achieved remarkable performance in node representation learning. One key factor of GNN's success is the \emph{smoothness} property on node representations. Despite this, most GNN models are fragile to the perturbations on graph inputs and could learn unreliable node representations. In this paper, we study how to learn node representations against perturbations in GNN. Specifically, we consider that a node representation should remain stable under slight perturbations on the input, and node representations from different structures should be identifiable, which two are termed as the \emph{stability} and \emph{identifiability} on node representations, respectively. To this end, we propose a novel model called Stability-Identifiability GNN Against Perturbations (SIGNNAP) that learns reliable node representations in an unsupervised manner. SIGNNAP formalizes the \emph{stability} and \emph{identifiability} by a contrastive objective and preserves the 
    
[^127]: 快速、鲁棒的排名聚合算法在模型错误特化方面的应用

    Fast and Robust Rank Aggregation against Model Misspecification. (arXiv:1905.12341v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1905.12341](http://arxiv.org/abs/1905.12341)

    本文提出了CoarsenRank，其具有鲁棒性，适用于模型错误特化。它采用由粗到精的方案来处理用户收集的信息，并利用排名空间中的几何结构来更好地模拟聚合过程。在实验中验证了CoarsenRank的有效性。

    

    排名聚合算法（RA）用于将来自不同用户的偏好总结成一个总排序。在假设用户同质化的情况下，这项工作基本无误。但在复杂的实际情况下，由于同质假设无法验证，RA的模型错误特化出现了。现有的健壮RA通常采用排名模型扩充来解释额外的噪声，其中收集到的偏好可以被视为单个附加到理想偏好上的扰动。由于健壮闯闯亮RAs大多依赖于某些扰动假设，因此它们不能很好地推广到真实世界中对不确定噪声的偏好。在本文中，我们提出了CoarsenRank，它对模型的错误特化具有鲁棒性。具体而言，我们的CoarsenRank具有以下特性：（1）CoarsenRank是针对轻微的模型错误特化而设计的，假设与模型假设一致的理想偏好位于收集到的偏好的附近。（2）CoarsenRank采用由粗到精的方案来捕捉收集到的来自不同用户的细微差异， 并利用聚类技术。（3）CoarsenRank利用排名空间的几何结构来更好地模拟聚合过程，并进一步增强其鲁棒性。我们进行了广泛的实验，涵盖合成和真实世界的数据集，验证了CoarsenRank的有效性。

    In rank aggregation (RA), a collection of preferences from different users are summarized into a total order under the assumption of homogeneity of users. Model misspecification in RA arises since the homogeneity assumption fails to be satisfied in the complex real-world situation. Existing robust RAs usually resort to an augmentation of the ranking model to account for additional noises, where the collected preferences can be treated as a noisy perturbation of idealized preferences. Since the majority of robust RAs rely on certain perturbation assumptions, they cannot generalize well to agnostic noise-corrupted preferences in the real world. In this paper, we propose CoarsenRank, which possesses robustness against model misspecification. Specifically, the properties of our CoarsenRank are summarized as follows: (1) CoarsenRank is designed for mild model misspecification, which assumes there exist the ideal preferences (consistent with model assumption) that locates in a neighborhood o
    

