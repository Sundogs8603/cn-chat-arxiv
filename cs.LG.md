# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation.](http://arxiv.org/abs/2307.12219) | 本文提出了一种通过生成插值方法，利用生成模型合成多样化超领域样本来改进分类器的鲁棒性。通过在一个源域上训练StyleGAN模型，然后在其他域上微调，得到了多个相关的生成器，从而提升了性能。 |
| [^2] | [Adversarial Agents For Attacking Inaudible Voice Activated Devices.](http://arxiv.org/abs/2307.12204) | 本研究分析了对语音激活设备进行无声攻击的风险，并发现存在重大安全漏洞。我们提出了一种基线网络模型，并模拟了多种攻击场景，揭示了通过无需添加硬件或增加设备技能的物理访问来发现和拥有特权信息的潜力。使用深度Q学习算法，我们在少数步骤中快速拥有了所有节点。这些研究结果强调了对非传统网络和新的网络安全措施的重要性。 |
| [^3] | [NCART: Neural Classification and Regression Tree for Tabular Data.](http://arxiv.org/abs/2307.12198) | 这篇论文介绍了一种名为NCART的可解释性神经网络，它利用多个可微性决策树替代全连接层，从而在保持可解释性的同时充分利用神经网络的优势。这种方法可以解决深度学习在大规模或高维数据集上的计算复杂性，并适用于小规模数据集。 |
| [^4] | [Monadic Deep Learning.](http://arxiv.org/abs/2307.12187) | DeepLearning.scala 2解决了在静态类型语言中使用神经网络进行训练时的自动求导问题，并提供了一组单子和单子变换器。 |
| [^5] | [Machine learning discovers invariants of braids and flat braids.](http://arxiv.org/abs/2307.12185) | 本研究使用机器学习对辫子和平面辫子进行分类，得出了新的便利不变量，包括平面辫子的完全不变量。 |
| [^6] | [Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation.](http://arxiv.org/abs/2307.12180) | 本文提出了一种基于原型驱动和多专家集成的多模态MR脑肿瘤分割方法，通过脑肿瘤原型的指导，突出显示了每个肿瘤亚区的特征，并采用相互转移机制解决了单一模态特征信息不足的问题。 |
| [^7] | [Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics.](http://arxiv.org/abs/2307.12171) | 本文介绍了一种名为LtC的协作框架，可以通过学习来有效减小流媒体视频的流量，提高流媒体视频分析的效率。 |
| [^8] | [Optimized Network Architectures for Large Language Model Training with Billions of Parameters.](http://arxiv.org/abs/2307.12169) | 本文提出了一种优化的网络架构，用于训练拥有数十亿参数的大型语言模型。这个架构根据语言模型的通信需求，将集群分割成一组通过非阻塞高带宽互连的GPU集合，并通过轨道连接仅连接具有通信需求的GPU，从而降低网络成本高达75％，同时不影响训练性能。 |
| [^9] | [Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification.](http://arxiv.org/abs/2307.12159) | 该论文提出了一种用于自动识别肌萎缩侧索硬化（ALS）的面部关键点图方法。通过分析患者的面部表情，可以更简单和更便宜地诊断和检测ALS。实验结果表明，该方法在多伦多神经脸数据集中表现出色，为该领域的进一步发展带来了希望。 |
| [^10] | [DIP-RL: Demonstration-Inferred Preference Learning in Minecraft.](http://arxiv.org/abs/2307.12158) | DIP-RL是一种利用人类演示的算法，在非结构化和开放的环境中通过多种方式推导偏好并学习奖励函数，其在Minecraft中的砍树任务中表现出了竞争力。 |
| [^11] | [Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach.](http://arxiv.org/abs/2307.12157) | 研究介绍了一种去中心化方法，利用可解释的人工智能对多层级供应链中感兴趣指标的估计贡献进行计算，无需数据共享。经验证实，该方法在检测质量变化的原因方面有效。 |
| [^12] | [Real-Time Neural Video Recovery and Enhancement on Mobile Devices.](http://arxiv.org/abs/2307.12152) | 本文提出了一种在移动设备上实现实时神经视频恢复和增强的新方法，包括视频帧恢复方案、超分辨率算法和增强感知的视频比特率自适应算法。实验表明该方法能够支持每秒30帧，并在不同网络环境下实现实时增强。 |
| [^13] | [CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment.](http://arxiv.org/abs/2307.12149) | 本论文提出了一种基于相关性的神经网络架构来解决联邦学习中的异质模型和不可用节点的问题，通过将模型权重投影到共同的潜空间，并通过最小化重构损失和最大化相关性来实现模型的填补和预测。 |
| [^14] | [A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter.](http://arxiv.org/abs/2307.12145) | 通过利用快照高光谱成像技术和机器学习分类方法，研究人员提出了一种在河流环境中高效自动化监测宏观塑料垃圾的方法，实现了高检测精度，为解决全球范围的漂浮垃圾问题做出了贡献。 |
| [^15] | [Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning.](http://arxiv.org/abs/2307.12143) | 本文研究了深度强化学习智能体中自适应昼夜节律的出现，通过在周期性变化环境中进行觅食任务，证明了智能体能够内化环境信号并适应相位变化，这一自适应过程是通过人工神经元的动力学来实现的。 |
| [^16] | [Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem.](http://arxiv.org/abs/2307.12136) | 该论文提出了一个利用强化学习方法解决三维装载有限车辆路径问题的模型，该方法能够在线性时间内有效计算共载和路径的可行解决方案，从而释放了碳减排的潜力。 |
| [^17] | [The Sample Complexity of Multi-Distribution Learning for VC Classes.](http://arxiv.org/abs/2307.12135) | 这个论文研究了多分布学习对VC类的样本复杂度，发现现有的上下界存在显著差距，并讨论了一些涉及统计学习中博弈动态的困难。 |
| [^18] | [AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities.](http://arxiv.org/abs/2307.12128) | 本文分析了智能城市中交通事故及事故检测系统的综合方法。通过利用交通监控摄像头和行为识别系统，可以实时检测和响应交通事故，并将其与紧急服务整合，以减少人为错误和改善交通安全。 |
| [^19] | [Synthesis of Batik Motifs using a Diffusion -- Generative Adversarial Network.](http://arxiv.org/abs/2307.12122) | 本研究使用了StyleGAN2-Ada和扩散技术，通过调整模型架构和使用精选数据集，成功产生了逼真且高质量的合成巴蒂克图案，为巴蒂克设计师提供了重要的辅助工具。 |
| [^20] | [A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC.](http://arxiv.org/abs/2307.12115) | 移动AIGC技术可以推动人类数字孪生在个性化医疗中的应用，包括生成罕见疾病数据、建模高保真数字孪生以及提供24/7定制医疗服务。 |
| [^21] | [A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks.](http://arxiv.org/abs/2307.12114) | 这项研究评估了四种指导细调大型语言模型在临床和生物医学任务上的表现，并发现它们在零样本和少样本情况下接近最先进模型的性能，尤其在问答任务上表现良好。然而，在分类和关系抽取任务上的表现稍逊于特定训练于医学领域的模型。没有一个模型在所有研究任务上胜过其他模型，有些模型更适合特定任务。 |
| [^22] | [Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning.](http://arxiv.org/abs/2307.12083) | 本研究使用深度强化学习算法结合旋转和多个可控喷口，通过优化喷口数量和位置，传感器位置以及每个动作可允许的最大流量和每个episode中允许的总喷口数的形式，实现对旋转圆柱体流动的主动控制，抑制涡流脱落和稳定卡门涡流。 |
| [^23] | [Spectral Normalized-Cut Graph Partitioning with Fairness Constraints.](http://arxiv.org/abs/2307.12065) | 本文提出了一种谱归一化切图分割的公平约束变种问题，通过两阶段的谱算法FNM，实现了在最小化归一化切值的同时保证每个聚类中各个人口群体的近似比例。 |
| [^24] | [Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs.](http://arxiv.org/abs/2307.12063) | 本文提出了一种通过动态构建潜在地标图来平衡探索和利用，解决了目标条件分层强化学习中的时间一致性和子目标选择策略的问题。 |
| [^25] | [Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations.](http://arxiv.org/abs/2307.12062) | 这篇论文提出了一种新的鲁棒强化学习方法，通过将时间耦合的鲁棒强化学习问题视为两人零和游戏来处理问题，并通过找到近似均衡来确保代理对时间耦合干扰的鲁棒性。实验结果显示，该方法在各种连续控制任务中相比基准方法表现出显著的鲁棒性优势。 |
| [^26] | [Fast Knowledge Graph Completion using Graphics Processing Units.](http://arxiv.org/abs/2307.12059) | 本文提出了一种在GPU上高效完成知识图谱补全的框架，通过使用知识图谱嵌入矢量来添加新关系。该框架将知识图谱补全问题转化为相似性连接问题，并提供了一种高效处理相似性连接问题的方法。 |
| [^27] | [External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback.](http://arxiv.org/abs/2307.12057) | 本文提出通过从外部存储库中选择性地集成知识来增强大型语言模型，提出了一种外部推理的新方法，例子是ChatPDF。 |
| [^28] | [Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space.](http://arxiv.org/abs/2307.12032) | 本论文引入创新的增强迁移学习模型，以及通过在霍夫空间中使用新的SR损失函数改善飞行轨迹线检测的方法。这些方法能够准确地检测飞行轨迹并且对数据需求较少，为基于机器学习的航空研究中的飞行轨迹检测提供了新的解决方案。 |
| [^29] | [A Flexible Framework for Incorporating Patient Preferences Into Q-Learning.](http://arxiv.org/abs/2307.12022) | 这个论文提出了一种称为潜在效用Q学习的方法，能够将患者偏好纳入复合结果的动态治疗方案中，解决了传统方法对时间点和结果数量的限制，能够实现强大的性能。 |
| [^30] | [Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors.](http://arxiv.org/abs/2307.12015) | 本文介绍了一种通过数据驱动的多步血糖预测器与线性时变模型预测控制（MPC）相结合的闭环胰岛素输送算法设计，用于治疗1型糖尿病。作者通过直接拟合整个血糖预测曲线，结合非线性和线性模型，与传统线性MPC进行对比，评估了算法的优劣。 |
| [^31] | [NLCUnet: Single-Image Super-Resolution Network with Hairline Details.](http://arxiv.org/abs/2307.12014) | 本文提出了一种具有发际线细节的单图像超分辨网络（NLCUnet）。该网络使用非局部注意机制恢复局部细节，并提出了一种不需要模糊核估计的新网络架构。此外，还提出了一种在中心裁剪内部进行随机裁剪的方法来提高语义信息的利用效率。实验结果表明，NLCUnet相较于其他方法表现更好。 |
| [^32] | [Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering.](http://arxiv.org/abs/2307.11986) | 本研究提出了一个新的医学视觉问答任务，名为MIMIC-Diff-VQA，为自动化医学视觉语言模型做出了贡献。与现有数据集相比，该任务旨在回答关于疾病和图像差异的问题，并应用了专家知识感知的图表示学习模型。 |
| [^33] | [Collaborative Graph Neural Networks for Attributed Network Embedding.](http://arxiv.org/abs/2307.11981) | 本文提出了一种协作式图神经网络用于属性网络嵌入，通过深入参与节点属性来增强节点连接并改善对非活动节点的感受域限制。 |
| [^34] | [Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model.](http://arxiv.org/abs/2307.11980) | 本研究提出了一种基于全局变换器的迭代建模方法，用于在MRI中合成具有任意对比增强水平的图像。该方法通过引入子采样的注意机制和旋转平移模块来捕捉各种对比度相关的特征。定量评估结果显示，该模型在剂量降低和肿瘤分割等任务中表现优于其他最先进的方法。 |
| [^35] | [Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?.](http://arxiv.org/abs/2307.11978) | 视觉-语言模型通过少样本提示调参的方式适应新的分类任务，且对于噪声标签具有鲁棒性。关键原因包括固定的类名标记对模型优化的正则化作用以及从多样且通用的网络数据中学习到的强大预训练图像-文本嵌入提供的先验知识。 |
| [^36] | [Out-of-Distribution Optimality of Invariant Risk Minimization.](http://arxiv.org/abs/2307.11972) | 本文旨在提供IRM的理论验证，严格证明了解决方案可以最小化区外风险。 |
| [^37] | [DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation.](http://arxiv.org/abs/2307.11960) | DHC是一种用于类别不平衡的半监督医学图像分割的双去偏异构联合训练框架，通过引入分布感知和难度感知的去偏权重策略，以动态引导模型解决数据和学习偏见，显著提高了性能。 |
| [^38] | [Implicit Interpretation of Importance Weight Aware Updates.](http://arxiv.org/abs/2307.11955) | 本文首次证明了在在线学习环境中，重要性权重感知（IWA）更新对于凸优化机器学习算法具有更好的遗憾上界，优于普通梯度更新。 |
| [^39] | [On-Robot Bayesian Reinforcement Learning for POMDPs.](http://arxiv.org/abs/2307.11954) | 本研究提供了解决机器人学习中大量数据需求的方法，通过将专家知识捕捉并形式化为贝叶斯框架，使用基于样本的在线解决方法来推动基于贝叶斯强化学习在机器人中的应用。 |
| [^40] | [HIQL: Offline Goal-Conditioned RL with Latent States as Actions.](http://arxiv.org/abs/2307.11949) | 本文提出了一个基于离线数据的目标导向强化学习的分层算法，通过利用目标达成问题的结构，使用一个无动作的价值函数学习了两个策略，从而在学习过程中更有效地利用离线数据。 |
| [^41] | [The instabilities of large learning rate training: a loss landscape view.](http://arxiv.org/abs/2307.11948) | 本论文研究了大学习率下网络训练的损失景观，并发现了梯度下降的不稳定性现象，包括景观变平和景观变移。 |
| [^42] | [Collaboratively Learning Linear Models with Structured Missing Data.](http://arxiv.org/abs/2307.11947) | 本论文研究了协同学习线性模型的问题，提出了一种分布式、半监督的算法Collab，该算法在无法访问标记数据的情况下具有通信效率和实用性，同时在渐近局部极小极值方面也表现出优异的性能。 |
| [^43] | [Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent.](http://arxiv.org/abs/2307.11939) | 这篇论文提出了一种用于差分隐私随机梯度下降的批次剪裁和自适应逐层剪裁方法。通过引入批次剪裁，可以解决深度神经网络无法使用批次正则化层的问题。同时，通过自适应逐层剪裁，可以根据不同层的敏感性调整剪裁常数。 |
| [^44] | [Mercer Large-Scale Kernel Machines from Ridge Function Perspective.](http://arxiv.org/abs/2307.11925) | 本文从岭函数的角度介绍了Mercer大规模核机器，通过研究哪些核函数可以被余弦函数乘积逼近，解决了大规模核机器中的难题。这些结果对深度学习，特别是图像处理有潜在应用。 |
| [^45] | [Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors.](http://arxiv.org/abs/2307.11922) | 本研究提出了一种名为BLINDER的方法，通过学习任务条件下状态描述的值函数，自动选择简明的状态描述，以优化大型语言模型(LLM)演员在顺序决策任务中的性能和效率。 |
| [^46] | [Poverty rate prediction using multi-modal survey and earth observation data.](http://arxiv.org/abs/2307.11921) | 该研究提出了一种结合多模式调查和地球观测数据的方法来预测贫困率。通过使用从卫星图像中提取的可视特征和调查问题，我们的方法在预测贫困率时表现出更低的平均误差。 |
| [^47] | [Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks.](http://arxiv.org/abs/2307.11906) | 该论文提出了一种通过查询有效的黑盒攻击方法，揭示了可解释的深度学习系统中的漏洞。这种攻击方法是基于微生物遗传算法，无需先验知识，结合了转移和评分方法，攻击成功率高且难以被检测到。 |
| [^48] | [Model Compression Methods for YOLOv5: A Review.](http://arxiv.org/abs/2307.11904) | 本文综述了针对YOLOv5的模型压缩方法，重点关注了网络裁剪和量化。这些方法在降低内存使用和推理时间方面取得了积极的效果。 |
| [^49] | [Project Florida: Federated Learning Made Easy.](http://arxiv.org/abs/2307.11899) | 项目佛罗里达是一个简化联邦学习的系统架构和SDK，能够在异构设备生态系统中部署大规模的联邦学习解决方案。它通过保护数据的隐私和安全性，将模型训练分散在设备和云存储之间，同时实现模型更新的集中聚合，解决了复杂的隐私和安全问题，提高了规模和性能。 |
| [^50] | [Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning.](http://arxiv.org/abs/2307.11897) | 本研究提出了Hindsight-DICE算法，利用重要抽样比率估计技术改善了深度强化学习中的信用分配问题。 |
| [^51] | [On the Vulnerability of Fairness Constrained Learning to Malicious Noise.](http://arxiv.org/abs/2307.11892) | 这项研究考虑了公正约束学习对恶意噪声的脆弱性，发现使用随机分类器可以在精度上只损失$\Theta(\alpha)$和$O(\sqrt{\alpha})$，对应不同的公正约束要求。 |
| [^52] | [On the Universality of Linear Recurrences Followed by Nonlinear Projections.](http://arxiv.org/abs/2307.11888) | 本论文展示了一种基于循环线性层和多层感知器的序列模型可以逼近任何规则的非线性序列到序列映射。 |
| [^53] | [MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification.](http://arxiv.org/abs/2307.11875) | 本文提出了一个高效的量子多分类器MORE，通过测量和相关性来实现多分类任务。该方法利用了单个读出量子位的量子信息，而不需要复杂的后处理过程。 |
| [^54] | [The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention.](http://arxiv.org/abs/2307.11864) | 本文介绍了一种在领英平台上检测虚假和大规模语言模型生成的个人资料的新方法，该方法使用领英个人资料中的文本信息，并引入“部分和子部分标签嵌入”（SSTE）方法以增强数据的特征。研究还通过收集3600个领英个人资料建立了一个公开可用的数据集。 |
| [^55] | [Data-Induced Interactions of Sparse Sensors.](http://arxiv.org/abs/2307.11838) | 本研究通过采用热力学观点，用统计物理学中的Ising模型来计算由训练数据引发的稀疏传感器之间的相互作用，从而优化传感器的空间配置和重构复杂系统的完整状态。 |
| [^56] | [PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks.](http://arxiv.org/abs/2307.11833) | PINNsFormer是一种基于Transformer的框架，通过捕捉时间依赖性准确逼近求解偏微分方程，相比传统方法具有更好的性能。 |
| [^57] | [Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks.](http://arxiv.org/abs/2307.11807) | 本文提出了一种以本地核归一化为机制的理论框架，解释了全连接和卷积神经网络在特征学习方面的差异。 |
| [^58] | [Unsupervised Embedding Learning for Human Activity Recognition Using Wearable Sensor Data.](http://arxiv.org/abs/2307.11796) | 本文提出了一种无监督的方法，使用可穿戴传感器数据进行人体活动识别。该方法将人体活动投影到一个嵌入空间中，从而帮助聚类算法实现更好的性能。 |
| [^59] | [Prompting Large Language Models with Speech Recognition Abilities.](http://arxiv.org/abs/2307.11795) | 本研究通过为大型语言模型添加音频编码器，使其具备了语音识别能力。在多语言数据集上的实验证明，这样的扩展能够提高模型的性能，并且在多语言环境下实现了语音识别。通过消融研究，我们还发现可以冻结模型以保持其原有功能，并且提升音频编码器的规模有助于提高性能。 |
| [^60] | [Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks.](http://arxiv.org/abs/2307.11794) | 本文提出了改进的Transformer和CGAN神经网络用于反向设计石墨烯超表面，通过对太赫兹多谐振吸收谱进行S2V和S2I设计，实现了更高的精确度和全面性能。 |
| [^61] | [Leveraging arbitrary mobile sensor trajectories with shallow recurrent decoder networks for full-state reconstruction.](http://arxiv.org/abs/2307.11793) | 本论文提出了一种利用浅层递归解码网络和移动传感器轨迹实现全状态重建的方法。 |
| [^62] | [Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data.](http://arxiv.org/abs/2307.11792) | 本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。 |
| [^63] | [LLM Cognitive Judgements Differ From Human.](http://arxiv.org/abs/2307.11787) | 这项研究调查了大型语言模型在认知任务中的表现，并发现它们的认知判断与人类不同。 |
| [^64] | [Adversarial Conversational Shaping for Intelligent Agents.](http://arxiv.org/abs/2307.11785) | 本文研究了通过对抗对话塑造来增强智能对话代理的两个模型：GANPG和REGS。这些模型能够改进当前的自动拨号系统，提高聊天机器人的性能。 |
| [^65] | [What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems.](http://arxiv.org/abs/2307.11784) | 本文讨论了学习机制在安全关键领域的挑战，提出了一种两步验证方法来实现可证明的统计保障。 |
| [^66] | [Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case.](http://arxiv.org/abs/2307.11782) | 该论文研究了Adam算法在非凸目标中的收敛性，通过探索超参数设置和解决非遗传性收敛的挑战，提出了几乎确定性的遗传收敛速率，证明了非遗传性收敛优于遗传性收敛。 |
| [^67] | [The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative Language Models.](http://arxiv.org/abs/2307.11779) | 该论文提出了一个名为提取-摘要轴的概念，用于衡量生成语言模型中内容的"借用"程度，并提出了开发相应度量标准、数据集和注释指南的需求。 |
| [^68] | [Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths.](http://arxiv.org/abs/2307.11777) | 通过统计增强学习模型预测手球比赛，准确率超过80%，可以为手球队教练提供有价值的统计和预测洞察力。 |
| [^69] | [A Topical Approach to Capturing Customer Insight In Social Media.](http://arxiv.org/abs/2307.11775) | 本研究通过嵌入狄利克雷过程，嵌入层次狄利克雷过程和面向时间的动态嵌入三种方法，解决了在嘈杂的大数据环境中完全无监督的主题提取挑战。 |
| [^70] | [AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models.](http://arxiv.org/abs/2307.11772) | AutoAlign是一种全自动的知识图谱对齐方法，不需要手工制作的种子对齐。它利用大型语言模型自动捕捉谓词相似性，并使用TransE计算实体嵌入来实现实体对齐。 |
| [^71] | [Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization.](http://arxiv.org/abs/2307.11770) | 通过大规模计算评估，我们研究了用于2D文本空间化的主题模型和降维方法的有效性，为语料库分析提供了具有高质量布局的解决方案。 |
| [^72] | [Question Decomposition Improves the Faithfulness of Model-Generated Reasoning.](http://arxiv.org/abs/2307.11768) | 通过将问题分解为子问题，可以显著提高大型语言模型生成推理的忠实度。 |
| [^73] | [Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model.](http://arxiv.org/abs/2307.11765) | 该研究提出了一种通过调取用户的心智模型来测量可解释人工智能模型的感知信任度的方法。通过使用模糊认知图来评估解释对感知信任度的影响，从而为解释人工智能决策提供参考和改进方向。 |
| [^74] | [Similarity-based Memory Enhanced Joint Entity and Relation Extraction.](http://arxiv.org/abs/2307.11762) | 本文提出了一种基于相似性的记忆增强联合实体和关系抽取的方法，通过在任务之间建立双向内存依赖关系，从而更准确地执行文档级联合实体和关系抽取问题。实证研究表明，该方法优于现有方法，并在BioCreative V CDR语料库上达到了最先进的结果。 |
| [^75] | [Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization.](http://arxiv.org/abs/2307.11007) | 本文研究发现，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。我们的结果表明，尖锐性与泛化之间的关系取决于数据分布和模型架构。 |
| [^76] | [LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?.](http://arxiv.org/abs/2307.10719) | 本文讨论了大型语言模型(LLM)的审查问题，指出现有的语义审查方法存在理论上的限制，由于LLM的程序化和遵循指令的能力，语义审查可以被认为是一个不可判定的问题。同时，有知识的攻击者可以重构不可容许的输出。 |
| [^77] | [TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars.](http://arxiv.org/abs/2307.10705) | 本文提出了一种轻量级模型TwinLiteNet，用于自动驾驶车辆中的可驱动区域和车道分割。该模型成本低廉且高效准确，并在实验中表现出明显的计算资源节约。 |
| [^78] | [Detecting deceptive reviews using text classification.](http://arxiv.org/abs/2307.10617) | 这篇论文提出了一种使用机器学习模型的方法来识别虚假评论，并通过在餐馆评论的数据集上进行实验验证了其性能。 |
| [^79] | [Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning.](http://arxiv.org/abs/2307.10559) | 本研究提出了一种使用合规化动态图学习来预测空中交通管制员工作负荷水平的方法，通过对退休空中交通管制员进行人机交互模拟，利用空中交通数据和工作负荷标签进行预测和评估。 |
| [^80] | [(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs.](http://arxiv.org/abs/2307.10490) | 本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。 |
| [^81] | [A data science axiology: the nature, value, and risks of data science.](http://arxiv.org/abs/2307.10460) | 这篇论文介绍了数据科学的价值取向，探讨了其特征和作用。数据科学不是一门科学，而是一种研究范式，具有广泛的应用和重大的影响，但也存在着未知的风险。这一领域仍然处于初级阶段，需要进一步的研究。 |
| [^82] | [Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model.](http://arxiv.org/abs/2307.10443) | 本文提出了一种新的注意力模式，使用图增强自注意力机制将从异构图中导出的推理知识整合到变压器架构中，从而克服了变压器模型在复杂推理任务中的限制。通过全局-局部注意力、图注意力和关系类型考虑，优化了实体和单词之间的注意力。该模式与相对位置标签相结合，能够与LUKE的实体感知自注意力机制相集成。 |
| [^83] | [SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning.](http://arxiv.org/abs/2307.10234) | 本研究通过利用GPT进行高级情感分析，并考察其与当前机器学习方法的差异，发现GPT方法相较于其他模型在预测性能上具有显著优势，并有效解决了情感分析任务中的一些挑战，如理解上下文和检测讽刺。 |
| [^84] | [Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla.](http://arxiv.org/abs/2307.09458) | 本论文研究了电路分析在最先进的语言模型中的可扩展性，通过对70B毛丫鼠模型进行多项选择题的分析，发现现有的逻辑层归因和激活修补技术具有可扩展性，并进一步研究了注意力头的语义特征。 |
| [^85] | [Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study.](http://arxiv.org/abs/2307.08572) | 本研究重新审视了最小错误熵准则在处理非高斯噪声中的鲁棒性，并探讨了其在实际转移学习回归任务中的可行性和有用性。实验证明，在基本转移学习算法中，通过用最小错误熵代替均方误差损失，可以取得与现有方法相媲美的性能表现。 |
| [^86] | [Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models.](http://arxiv.org/abs/2307.08303) | 本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。 |
| [^87] | [Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression.](http://arxiv.org/abs/2307.08044) | 本研究提出了一种深度AFT排名回归模型，用于灵活地进行时间事件建模，从而改善预测性能并减轻严格的假设。 |
| [^88] | [Neural Video Recovery for Cloud Gaming.](http://arxiv.org/abs/2307.07847) | 本文提出了一个新方法，用于在云游戏中恢复丢失或损坏的视频帧。与传统方法不同的是，我们利用游戏状态和部分解码帧来提高恢复准确性。 |
| [^89] | [Generalizable Embeddings with Cross-batch Metric Learning.](http://arxiv.org/abs/2307.07620) | 通过跨批次度量学习，我们提出了一种基于可学习原型的全局平均汇聚方法，用于学习通用实体以表示未见过的类别。 |
| [^90] | [Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems.](http://arxiv.org/abs/2307.06538) | 本论文以张量分解方法为基础，提出了学习线性动力系统混合模型的新方法。算法成功地应用于没有组件分离条件的情况，并可以与贝叶斯最优聚类竞争。此外，算法可以在部分观测设置下工作。 |
| [^91] | [Local Conditional Neural Fields for Versatile and Generalizable Large-Scale Reconstructions in Computational Imaging.](http://arxiv.org/abs/2307.06207) | 本文引入了一种新颖的本地条件神经场（LCNF）框架，利用连续的隐式神经表示来解决传统像素表示在捕捉对象连续、多尺度细节方面的限制。LCNF在多尺度信息重建中展现了出色的能力，并成功解决了傅立叶相位显微镜中的逆问题，实现了大规模相位恢复。与传统神经场框架不同的是，LCNF结合了本地条件表示，促进了模型泛化、学习多尺度信息以及高效处理大规模成像数据的能力。 |
| [^92] | [NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services.](http://arxiv.org/abs/2307.06148) | NetGPT是一个能够在边缘和云端部署适当的大型语言模型的本地AI网络架构，实现了个性化生成服务，并通过协作云边方法论来优化资源协调和互动效果。 |
| [^93] | [Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies.](http://arxiv.org/abs/2307.04893) | 这篇论文介绍了一种名为2L的算法，该算法能够提供引导合成程序化策略的参考策略，通过在实验中的表现和在MicroRTS锦标赛中的胜利，证明了2L算法相对于其他学习算法的优势。 |
| [^94] | [CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks.](http://arxiv.org/abs/2307.02813) | 这篇论文提出了一种对比式预训练方法（CPDG）用于动态图神经网络（DGNNs），通过灵活的结构-时序子图采样和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战，实验证明CPDG在各种下游任务中的动态图预训练方面优于现有方法。 |
| [^95] | [Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning.](http://arxiv.org/abs/2307.02620) | 本文研究了在观测代价敏感强化学习中，强化学习代理在每个时间步不需要昂贵的测量，提出了一种新的方法DMSOA，并在多个环境中进行了评估，结果表明DMSOA能够以更少的决策步骤和测量次数学到更好的策略。 |
| [^96] | [Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions.](http://arxiv.org/abs/2307.02478) | 本文研究了应用于流形结构化数据上的线性回归，揭示了外在几何对回归解唯一性以及对超出分布推断稳定性的影响。 |
| [^97] | [Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series.](http://arxiv.org/abs/2307.01482) | 提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。 |
| [^98] | [ManimML: Communicating Machine Learning Architectures with Animation.](http://arxiv.org/abs/2306.17108) | ManimML是一个开源Python库，通过动画演示自动生成的ML算法，为机器学习从业者提供了一种简单且熟悉的方式来沟通和可视化ML算法。 |
| [^99] | [Deep Unfolded Simulated Bifurcation for Massive MIMO Signal Detection.](http://arxiv.org/abs/2306.16264) | 本文提出了一种基于量子启发式算法的模拟双分岔用于大规模MIMO信号检测的方法，并通过修改算法和使用深度展开技术，显著改善了检测性能。 |
| [^100] | [Defining data science: a new field of inquiry.](http://arxiv.org/abs/2306.16177) | 数据科学是一种新的研究范式，具有潜力和应用广泛性，在40多个学科、数百个研究领域和成千上万个应用中出现。然而，由于其起步阶段，目前存在许多定义的冗余和不一致性的问题。 |
| [^101] | [Deep Learning Models for Water Stage Predictions in South Florida.](http://arxiv.org/abs/2306.15907) | 本论文利用深度学习模型训练代理模型，快速预测南佛罗里达州迈阿密河下游的水位，并与基于物理的模型进行比较。 |
| [^102] | [Augmenting Control over Exploration Space in Molecular Dynamics Simulators to Streamline De Novo Analysis through Generative Control Policies.](http://arxiv.org/abs/2306.14705) | 本研究引入了P5模型，利用强化学习增强了分子动力学模拟的控制能力。通过优化目标聚合物链构象的采样，我们实现了超过37.1%的效率提升。强化学习产生的控制策略扩展了传统分子动力学模拟的配置空间的探索范围，生成了更多样化的构象集合，并针对特定特性进行目标化。这一技术在研究新系统时具有显著优势，并为复杂模拟问题的解决提供了新的方法学。 |
| [^103] | [Learning from Pixels with Expert Observations.](http://arxiv.org/abs/2306.13872) | 本文提出了一种使用专家观察数据的新方法，在像素观测中进行稀疏奖励的机器人操作任务学习。通过使用专家观察数据作为目标条件，该方法能显著改进两种最先进的智能体的性能，同时训练过程中所需的专家行动次数减少了4-20倍，并且优于分层基线模型。 |
| [^104] | [FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning.](http://arxiv.org/abs/2306.13264) | 本文提出了一种名为FedSelect的新联邦学习框架，通过寻找最佳客户端子网络从而直接个性化客户端子网络结构和参数，同时保留了全局知识，提高了客户端性能。 |
| [^105] | [Predicting protein variants with equivariant graph neural networks.](http://arxiv.org/abs/2306.12231) | 本文比较了等变图神经网络和基于序列的方法在预测蛋白质变体方面的能力，并发现我们提出的结构方法在训练更少的分子的情况下可以达到与基于序列的方法相竞争的性能。 |
| [^106] | [Transformer Training Strategies for Forecasting Multiple Load Time Series.](http://arxiv.org/abs/2306.10891) | 转换器模型在预测多负载时间序列方面使用全局训练策略比多变量和本地训练策略具有更好的性能，平均降低了21.8%和12.8%的预测误差。 |
| [^107] | [Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects.](http://arxiv.org/abs/2306.10125) | 自监督学习（SSL）在时间序列分析中的应用取得了显著性能，通过减少对标注数据的依赖，即使只有少量标注数据，也能实现高性能。 |
| [^108] | [CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models.](http://arxiv.org/abs/2306.09635) | 本文提出了一种使用未标注视频和预训练语言视觉模型进行文本合成音频的方法。通过利用视觉模态作为桥梁来学习文本-音频对应关系，提出了有条件的扩散模型，生成视频的音轨。使用CLIP图像查询条件进行零样本模态转换。并采用预训练的扩散先验模型，生成对应于CLIP文本嵌入的CLIP图像嵌入。实验证明了该方法的有效性。 |
| [^109] | [A Survey of Some Density Based Clustering Techniques.](http://arxiv.org/abs/2306.09256) | 这项研究调查了一些基于密度的聚类技术，分析了它们的特点、优点和缺点，以及它们在不同类型的数据集上挖掘模式的适用性。 |
| [^110] | [Deep learning based Meta-modeling for Multi-objective Technology Optimization of Electrical Machines.](http://arxiv.org/abs/2306.09087) | 本文利用变分自编码器（VAE）在高维设计空间中同时优化了异步电机和永磁同步电机两种不同的电机技术，通过深度神经网络和解码器作为元模型，预测关键性能指标并生成新的设计。与经典的基于深度学习的直接方法相比，VAE方法表现出更好的性能。 |
| [^111] | [NuCLR: Nuclear Co-Learned Representations.](http://arxiv.org/abs/2306.06099) | NuCLR是一种可以预测各种核可观测量，包括结合能、衰变能和核电荷半径的深度学习模型，使用共享表示法进行训练，具有最先进的性能，可以捕捉到核物理的基础物理原理，并有潜力为核理论提供有价值的见解。 |
| [^112] | [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.](http://arxiv.org/abs/2306.03341) | 本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。 |
| [^113] | [How Do ConvNets Understand Image Intensity?.](http://arxiv.org/abs/2306.00360) | 该论文研究了ConvNets需要依赖图像亮度信息的情况，并通过可视化进行了展示。 |
| [^114] | [Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach.](http://arxiv.org/abs/2305.18088) | 本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。 |
| [^115] | [PFNs Are Flexible Models for Real-World Bayesian Optimization.](http://arxiv.org/abs/2305.17535) | 本文使用灵活的PFN作为BO代理建模，该模型能够允许进一步信息纳入以进行非远视BO。在三种不同的问题上得到了很好的结果。 |
| [^116] | [Dropout Drops Double Descent.](http://arxiv.org/abs/2305.16179) | 本研究发现通过在全连接线性层之前添加一个dropout层，可以缓解双下降现象，从而提高模型的预测准确性。 |
| [^117] | [Revenge of MLP in Sequential Recommendation.](http://arxiv.org/abs/2305.14675) | 本文提出了一种纯MLP顺序推荐架构TriMLP，其中加入了新颖的三角形混合器以实现标记有序的交互，以提高顺序推荐的性能表现。 |
| [^118] | [OL-Transformer: A Fast and Universal Surrogate Simulator for Optical Multilayer Thin Film Structures.](http://arxiv.org/abs/2305.11984) | 该论文提出了OL-Transformer用于光学多层薄膜结构，可以预测多达$10^{25}$种不同多层结构的精确反射和透射光谱，同时具有快速的计算速度。 |
| [^119] | [A New Class of Explanations for Classifiers with Non-Binary Features.](http://arxiv.org/abs/2304.14760) | 本文提出了一种适用于具有非二元特征的分类器的新型解释方法，可以提供更多关于决策和基础分类器的信息。 |
| [^120] | [Typical and atypical solutions in non-convex neural networks with discrete and continuous weights.](http://arxiv.org/abs/2304.13871) | 本文研究了二元和连续的负边距感知器作为非凸神经网络模型，发现它们都存在着极为平坦和宽广的亚优解，这对于二元情况中的算法行为有着很强的影响。 |
| [^121] | [Stochastic MPC for energy hubs using data driven demand forecasting.](http://arxiv.org/abs/2304.12438) | 本文提出了一种基于数据驱动需求预测的随机MPC控制器，采用机会约束来最小化不确定的电力和热需求，使用历史数据构建预测模型，并通过情景法采样多步需求轨迹。在模拟和实际数据上验证了该控制器的性能。 |
| [^122] | [A Transfer Principle: Universal Approximators Between Metric Spaces From Euclidean Universal Approximators.](http://arxiv.org/abs/2304.12231) | 本论文提出使用欧几里得空间通用逼近器为构建块，构建了在任意波兰度量空间 $\mathcal{X}$ 和 $\mathcal{Y}$ 之间的通用逼近器，并通过随机化输出离散概率测度来克服某些限制。在适当的结构下提供了概率和定量保证。 |
| [^123] | [Classification of US Supreme Court Cases using BERT-Based Techniques.](http://arxiv.org/abs/2304.08649) | 本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。 |
| [^124] | [Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?.](http://arxiv.org/abs/2304.07840) | 本研究探究使用大型预训练语言模型，结合自然语言和编程语言，来改善自动程序修复的效果，发现预训练模型通过使用代码审查和代码更改的数据集微调，能显著优于传统方法，并具有更好的泛化能力。 |
| [^125] | [S3M: Scalable Statistical Shape Modeling through Unsupervised Correspondences.](http://arxiv.org/abs/2304.07515) | 该论文提出一种无监督的方法来建立统计形状模型，使用深度几何特征和功能对应来同时学习复杂解剖结构中的局部和全局形状结构，并且该方法足够健壮，能够从噪声神经网络预测中学习。 |
| [^126] | [Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective.](http://arxiv.org/abs/2304.06833) | 本文提出，当模型类足够丰富以涵盖真实情况时，非线性问题的“先估计再优化”方法优于集成方法，包括优化间隙的渐进优势的均值，所有其他时刻和整个渐进分布。 |
| [^127] | [Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification.](http://arxiv.org/abs/2304.03981) | 提出了基于不确定性的开放集(UIOS)模型，用于处理训练中未见过的类别样本，该模型通过计算不确定性得分来表达其置信度，并在多个测试数据集中表现优异，为真实世界的视网膜异常筛查提供了一个强大的方法。 |
| [^128] | [RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging.](http://arxiv.org/abs/2304.03483) | 本文提出了一种称为RED-PSM的方法，将部分可分模型与去噪正则化相结合，用于解决动态成像问题，数值实验证明其优越性。 |
| [^129] | [Exact Characterization of the Convex Hulls of Reachable Sets.](http://arxiv.org/abs/2303.17674) | 本文精确地刻画了具有有界扰动的非线性系统的可达集的凸包为一阶常微分方程的解的凸包，提出了一种低成本、高精度的估计算法，可用于过逼近可达集。 |
| [^130] | [NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions.](http://arxiv.org/abs/2303.12865) | 本文提出了一种基于神经辐射场（NeRF）和生成对抗网络（GAN）的新方法，通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，实现了基于姿态的2D GAN的高效3D感知生成。 |
| [^131] | [Deployment of Image Analysis Algorithms under Prevalence Shifts.](http://arxiv.org/abs/2303.12540) | 本文通过实证研究表明，在机器学习在医学图像分析领域的实际应用中，流行病变化对算法的部署效果有重要影响。 |
| [^132] | [Efficient Multi-stage Inference on Tabular Data.](http://arxiv.org/abs/2303.11580) | 该论文通过将推断算法简化并嵌入产品代码中，以减少网络通信，在处理表格数据的实时平台上可将推断延迟降低1.3倍，CPU资源减少30％，并将应用程序前端和后端之间的网络通信减少60％。 |
| [^133] | [Vibration Signal Denoising Using Deep Learning.](http://arxiv.org/abs/2303.11413) | 本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。 |
| [^134] | [Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction.](http://arxiv.org/abs/2303.09340) | 本文提出了一种基于深度卷积神经网络的伪影降噪方法，用于改善稀疏视图下自动出血检测的图像质量，并证明其能够与完全采样的图像进行同等精确度的分类和检测。 |
| [^135] | [Automated patent extraction powers generative modeling in focused chemical spaces.](http://arxiv.org/abs/2303.08272) | 本研究通过开发自动化管道，使用专利数据源训练领域特定的生成模型，利用专利中的弱标记应用类别中尽可能多的信息实现化学空间内生成建模。 |
| [^136] | [Opening Up the Neural Network Classifier for Shap Score Computation.](http://arxiv.org/abs/2303.06516) | 本文提出了一种高效计算机器学习模型分类中Shap解释分数的方法，通过将二进制神经网络转换为布尔电路，并使用知识编译技术，将电路视为开放式模型，通过最近的高效算法计算Shap分数，相比于将BNN视为黑盒模型直接计算Shap，性能有了显著的提高。 |
| [^137] | [Exphormer: Sparse Transformers for Graphs.](http://arxiv.org/abs/2303.06147) | Exphormer是一个稀疏Transformer框架，通过虚拟全局节点和扩张图的稀疏注意机制，在保持准确性的同时能够扩展到大型图形，并证明其拥有理想的理论特性。 |
| [^138] | [How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding.](http://arxiv.org/abs/2303.04245) | 本文提供了对Transformer学习语义结构的机制性理解，通过数学分析和实验证明了嵌入层和自注意力层如何对词汇的共现结构进行编码。 |
| [^139] | [Contrastive Learning and the Emergence of Attributes Associations.](http://arxiv.org/abs/2302.10763) | 对比学习方案通过对物体输入表示进行身份保持的变换，不仅有助于物体的分类，还可以提供关于属性的有无决策的有价值信息。 |
| [^140] | [BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image.](http://arxiv.org/abs/2302.09629) | 本文提出了一种名为BiofilmScanner的方法，该方法利用基于Yolact的深度学习方法和不变矩来高效地检测和分割SRB图像中的细菌细胞，并准确地提取其几何特性。 |
| [^141] | [From Graph Generation to Graph Classification.](http://arxiv.org/abs/2302.07989) | 本文提出了一种新的图分类方法，通过利用图生成模型，推导出了给定图的类标签概率的分类公式，并提出了一种新的条件 ELBO 用于训练生成图自编码器模型。这是一种在图分类中具有创新性的方法。 |
| [^142] | [DNArch: Learning Convolutional Neural Architectures by Backpropagation.](http://arxiv.org/abs/2302.05400) | DNArch是一种通过反向传播同时学习卷积神经网络的权重和架构的方法，它不仅可以学习每一层的卷积核大小和通道数，还可以学习网络的深度和下采样层的位置和值。与现有方法不同的是，DNArch不限于预定义的神经组件，能够发现各种核大小、宽度、深度和下采样组合中的整个CNN架构。在实验中，DNArch在多个分类和密集预测任务上找到了高性能的CNN架构。 |
| [^143] | [Detecting Harmful Agendas in News Articles.](http://arxiv.org/abs/2302.00102) | 这项研究提出了一种新的任务，即在新闻文章中检测有害议程，并发布了一个新闻文章注释数据集以供研究使用。研究者展示了可解释系统在这一任务上的有效性，并证明它们可以和黑盒模型有相当的表现。 |
| [^144] | [Toward Efficient Gradient-Based Value Estimation.](http://arxiv.org/abs/2301.13757) | 本研究研究了梯度为基础的值估计方法慢的根本原因，并提出了一种低复杂度的方法以解决损失函数带来的不良影响，该方法在效率上比剩余梯度方法更快，几乎具有相同的计算复杂度，并且在经典问题上与TD具有竞争力。 |
| [^145] | [ScaDLES: Scalable Deep Learning over Streaming data at the Edge.](http://arxiv.org/abs/2301.08897) | ScaDLES是一种用于边缘端流式数据上的可扩展深度学习方法，解决了系统和统计异质性的挑战。 |
| [^146] | [CLIPTER: Looking at the Bigger Picture in Scene Text Recognition.](http://arxiv.org/abs/2301.07464) | 本研究提出了CLIPTER（CLIP文本识别）框架，利用现代视觉语言模型提供整个图像的信息，通过门控交叉注意机制融合到裁剪的文本图像识别器中。结果表明，这种方法在多个基准测试上取得了最先进的效果。 |
| [^147] | [Deep Injective Prior for Inverse Scattering.](http://arxiv.org/abs/2301.03092) | 本文提出了一种基于深度生成模型的反散射数据驱动框架，通过学习低维流形作为正则化器，只需要目标介电常数进行训练，并引入了贝叶斯框架来近似后验分布。 |
| [^148] | [Provable Reset-free Reinforcement Learning by No-Regret Reduction.](http://arxiv.org/abs/2301.02389) | 该论文提出了一个通用的无懊悔规约方法，用于解决强化学习中重置机制带来的实际应用问题，提出了第一个可证明正确性的无重置强化学习算法。 |
| [^149] | [Backward Curriculum Reinforcement Learning.](http://arxiv.org/abs/2212.14214) | 这项工作提出了一种新颖的反向课程强化学习方法，通过使用回放轨迹而不是原始的前向轨迹来训练智能体。这种方法通过提供强有力的奖励信号实现了更高效的学习，而且只需要进行微小的算法改变。 |
| [^150] | [Shuffled Multi-Channel Sparse Signal Recovery.](http://arxiv.org/abs/2212.07368) | 本文研究了混合多通道稀疏信号恢复问题，将其视为信号重建问题并建立了唯一恢复的条件。对于这个问题，尚未考虑过相关的采样结果，并且现有的无标签感知方法无法直接应用。 |
| [^151] | [Can Evolutionary Clustering Have Theoretical Guarantees?.](http://arxiv.org/abs/2212.01771) | 本文证明了进化聚类算法 GSEMO 可以在理论上保证解决四种聚类问题，并介绍了在离散 k-中位数聚类中的公平聚类问题。 |
| [^152] | [Offline Reinforcement Learning with Closed-Form Policy Improvement Operators.](http://arxiv.org/abs/2211.15956) | 本文提出了基于行为约束的离线强化学习中的闭合形式策略改进算子，该算子将行为策略建模为高斯混合，利用LogSumExp的下界和Jensen不等式克服了优化困难，能有效处理实际数据集中的异构策略。 |
| [^153] | [Fast Sampling of Diffusion Models via Operator Learning.](http://arxiv.org/abs/2211.13449) | 本文通过使用神经算子加速扩散模型的采样过程，并提出了一种并行解码方法来生成图像。通过引入时间卷积层建模轨迹上的时间相关性，我们展示了方法的实现。 |
| [^154] | [Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph.](http://arxiv.org/abs/2211.10890) | 该论文介绍了一种单次对比学习方法，可适用于同构和异构图，并给出了性能保证。 |
| [^155] | [Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material.](http://arxiv.org/abs/2211.09710) | 本文提出了一种拉比文学的分类系统，可以通过其风格来检测Midrash Tanhuma中的失落材料。 |
| [^156] | [Learning-Augmented B-Trees.](http://arxiv.org/abs/2211.09251) | 这是一个学习增强的B树，通过使用具有复合优先级的Treaps，每个项目的深度由其预测权重确定，推广了最近的学习增强BST，并且是第一个可以利用访问序列中的局部性的B树数据结构。 |
| [^157] | [Toward expanding the scope of radiology report summarization to multiple anatomies and modalities.](http://arxiv.org/abs/2211.08584) | 本论文针对放射学报告摘要存在的限制，提出了一个新的数据集MIMIC-RRS，包含多个解剖学和模态。通过在数据集上进行实验和临床评估，我们旨在扩大放射学报告摘要的应用范围。 |
| [^158] | [C3: Cross-instance guided Contrastive Clustering.](http://arxiv.org/abs/2211.07136) | C3是一种引入了交叉实例关系的对比聚类方法，通过考虑实例之间的关系增加了正对数，从而提高了聚类性能。 |
| [^159] | [Near-optimal multiple testing in Bayesian linear models with finite-sample FDR control.](http://arxiv.org/abs/2211.02778) | 本文针对高维贝叶斯线性模型的多重检验问题，开发了近似最优的多重检验程序，能够在有限样本下控制频率FDR，并能达到近似最优功率。 |
| [^160] | [Neural Active Learning on Heteroskedastic Distributions.](http://arxiv.org/abs/2211.00928) | 这项研究展示了在异方差分布上进行神经主动学习可能导致灾难性失败，并提出了一种利用微调来减轻这种失败的方法。 |
| [^161] | [Lipschitz-regularized gradient flows and generative particle algorithms for high-dimensional scarce data.](http://arxiv.org/abs/2210.17230) | 构建了一种新的生成算法类，能够有效地学习稀缺高维数据的任意目标分布并生成新样本，具有很好的数据整合能力。 |
| [^162] | [A picture of the space of typical learnable tasks.](http://arxiv.org/abs/2210.17011) | 我们使用信息几何技术研究了在不同任务上训练时深度网络学习到的表示，发现任务空间的结构与Wordnet系统进化树的某些部分一致，并且监督学习在一个任务上的进展可以在其他任务上产生一定的影响。 |
| [^163] | [Physics-aware Graph Neural Network for Accurate RNA 3D Structure Prediction.](http://arxiv.org/abs/2210.16392) | 本研究提出了一种基于图神经网络的评分函数PaxNet，通过训练已解决的RNA 3D结构上的原子类型和坐标来预测准确的结构模型。PaxNet模拟了局部和非局部相互作用，并包含注意力融合模块，它能学习每种相互作用类型的个体贡献以进行最终预测。结果表明，PaxNet在RNA 3D结构预测任务中明显优于其他方法。 |
| [^164] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^165] | [Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model Predictive Control.](http://arxiv.org/abs/2210.12583) | 本文提出了一种用于主动学习非线性机器人系统动力学的方法，结合了离线和在线学习，能够在实时中准确推断模型动力学，并设计了一种不确定性感知模型预测控制器。 |
| [^166] | [Log-linear Guardedness and its Implications.](http://arxiv.org/abs/2210.10012) | 本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。 |
| [^167] | [Minimax Optimal Kernel Operator Learning via Multilevel Training.](http://arxiv.org/abs/2209.14430) | 本文研究了学习两个无穷维Sobolev再生核希尔伯特空间之间的Hilbert-Schmidt算子的统计极限，在多层级训练方法下，通过学习偏差以下的谱分量和忽略方差以上的分量，可以达到最优的学习速率。 |
| [^168] | [Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2209.14344) | 本文提出了Pareto Actor-Critic（Pareto-AC）算法来解决多智能体强化学习中的均衡选择问题，该算法利用无冲突游戏的性质，即Pareto最优均衡最大化了所有智能体的回报。实验结果显示Pareto-AC相比其他七种最先进的算法更能收敛到更高的回合回报。 |
| [^169] | [Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy.](http://arxiv.org/abs/2209.11531) | 该论文提出了一种基于深度学习的胸部X射线匿名化方法（PriCheXy-Net），可以有针对性地去除生物特征信息，同时保持数据的诊断和机器学习效用。 |
| [^170] | [Improving Generalizability of Graph Anomaly Detection Models via Data Augmentation.](http://arxiv.org/abs/2209.10168) | 通过数据增强技术，本文针对图异常检测中存在的泛化能力差问题，提出了一种通用且新颖的研究问题：广义图异常检测，旨在同时有效识别训练领域图和未训练领域图上的异常。 |
| [^171] | [Prediction of the outcome of a Twenty-20 Cricket Match : A Machine Learning Approach.](http://arxiv.org/abs/2209.06346) | 本研究使用机器学习方法针对Twenty20板球比赛的结果进行预测，通过考虑球员的历史表现统计、评级和聚类等因素，提出了一种新颖的基于ELO方法的球员评级方法，并使用不同的机器学习算法进行了对比分析。 |
| [^172] | [R\'{e}nyi Divergence Deep Mutual Learning.](http://arxiv.org/abs/2209.05732) | 本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。 |
| [^173] | [Segmenting Known Objects and Unseen Unknowns without Prior Knowledge.](http://arxiv.org/abs/2209.05407) | 本研究提出一种不需要先验知识的整体分割方法，可以将已知的物体和未知的物体进行准确分割，并在处理已知类别时对未知类别进行鲁棒处理。 |
| [^174] | [EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks.](http://arxiv.org/abs/2208.14003) | EchoGNN是一种基于图神经网络的模型，用于从超声心动图视频中可解释地估计射血分数。它推断出一个潜在的超声心动图，并通过节点和边的权重来确定帧的重要性，以帮助估计射血分数。该模型在定性和定量方面展现出良好的效果。 |
| [^175] | [Approximate blocked Gibbs sampling for Bayesian neural networks.](http://arxiv.org/abs/2208.11389) | 本文提出了一种近似阻塞Gibbs采样方法，可以更可行地进行小批量MCMC采样，提高了前馈神经网络的预测准确性和预测不确定性的量化能力。 |
| [^176] | [Frouros: A Python library for drift detection in machine learning systems.](http://arxiv.org/abs/2208.06868) | Frouros是一个开源的Python库，可以检测任何机器学习框架中的概念和数据漂移，易于维护和扩展。 |
| [^177] | [Efficient Algorithms for Sparse Moment Problems without Separation.](http://arxiv.org/abs/2207.13008) | 本文提出了一个无需分离条件的稀疏矩问题的高效算法，主要贡献在于引入了全面且紧密的分析方法，并且通过Vandermonde矩阵和Schur多项式之间的联系，提供了与分离无关的紧密摄动界限。 |
| [^178] | [Reducing Training Time in Cross-Silo Federated Learning using Multigraph Topology.](http://arxiv.org/abs/2207.09657) | 本文提出了一种新的多图拓扑结构，用于跨数据仓库联邦学习，通过孤立节点实现模型聚合，从而有效地减少了训练时间。 |
| [^179] | [GANDALF: Gated Adaptive Network for Deep Automated Learning of Features.](http://arxiv.org/abs/2207.08548) | GANDALF是一种用于表格数据的高性能深度学习架构，具有解释性和计算效率高的特点。通过引入门控特征学习单元，GANDALF能够实现更好的性能，并且在多个公开基准测试中优于或与其他最先进的方法持平。 |
| [^180] | [Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning.](http://arxiv.org/abs/2207.05018) | 本文提出了一种新颖的分层强化学习代理SEADS，将连续控制的时间延展技能与环境状态的符号离散抽象的前向模型相连接，实现了在连续领域中的长程规划和控制能力的学习。 |
| [^181] | [TF-GNN: Graph Neural Networks in TensorFlow.](http://arxiv.org/abs/2207.03522) | TF-GNN是一个可扩展的 TensorFlow 图神经网络库，用于支持丰富的异构图数据。它提供了低代码解决方案，并广泛应用于谷歌的生产模型中。这个库最近作为开源项目发布，为图学习提供了强大的工具。 |
| [^182] | [TabText: A Flexible and Contextual Approach to Tabular Data Representation.](http://arxiv.org/abs/2206.10381) | TabText是一种处理和特征提取框架，通过转换内容为语言并利用预训练的大型语言模型，从表格数据中提取上下文信息。通过应用TabText框架可以生成高性能且简单的机器学习基准模型，减少数据预处理的工作量。该框架在医疗预测任务中展现出良好的效果。 |
| [^183] | [Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data.](http://arxiv.org/abs/2206.02909) | 本研究利用700,000人日的未标记可穿戴传感器数据，通过自监督学习技术，成功构建了一种能够在多个数据集上泛化且效果显著优于基线模型的人体活动识别模型，有望帮助研究人员和开发者开发高性能的可定制和泛化的活动分类器。 |
| [^184] | [When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction.](http://arxiv.org/abs/2206.02058) | 本研究发现，使用群体属性个性化机器学习模型可能降低群体水平的性能。为了确保在预测任务中公平使用群体属性，我们提出了形式化条件，并提供了相应的解决方法。我们的实证研究表明，在临床预测任务中普遍存在公平使用违规的情况，但我们也找到了简单干预手段来减轻其伤害。 |
| [^185] | [Representation Power of Graph Neural Networks: Improved Expressivity via Algebraic Analysis.](http://arxiv.org/abs/2205.09801) | 本文通过代数分析改进了图神经网络（GNN）的表达能力，证明了GNN能够比Weisfeiler-Lehman（WL）算法更好地产生区分性表示，特别是在具有不同特征值的图上。此外，我们还发现简单的卷积结构与无信息输入产生的等变特征比WL表示更具表达能力。 |
| [^186] | [Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching.](http://arxiv.org/abs/2205.03447) | 本文介绍了五个新的生物医学本体匹配任务，通过引入机器学习技术并解决现有评估方法的限制，提供了综合评估框架来衡量本体匹配系统的性能。 |
| [^187] | [AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation.](http://arxiv.org/abs/2204.13170) | AdaBest提出了一种自适应算法，用于准确估计联邦学习中的客户端漂移。与之前的方法相比，AdaBest所需的存储和通信带宽较少，计算成本也较低。此外，AdaBest通过限制估计值的范数来提供稳定性。 |
| [^188] | [Generalizing similarity in noisy setups: the DIBS phenomenon.](http://arxiv.org/abs/2201.12803) | 本研究揭示了数据密度、噪声和相似性学习之间的相互作用，证明了数据对的密度对于泛化至关重要，并发现了一种在密集数据集上比对称标签噪声更差的泛化性能的现象，称为密度诱导的相似性破坏（DIBS）。 |
| [^189] | [LAnoBERT: System Log Anomaly Detection based on BERT Masked Language Model.](http://arxiv.org/abs/2111.09564) | 我们提出了一种名为LAnoBERT的无解析器系统日志异常检测方法，它使用了BERT模型进行掩码语言建模，从而实现了无人工干预的高效异常检测。 |
| [^190] | [Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects.](http://arxiv.org/abs/2110.08693) | 本文提出了一种扩展平方根速度函数的新表示方法和度量方法，用于分析和比较树状三维物体，从而提高物体形状差异计算的精度和效率。 |
| [^191] | [Compositional Clustering: Applications to Multi-Label Object Recognition and Speaker Identification.](http://arxiv.org/abs/2109.04160) | 该论文提出了一种新的组合聚类任务，针对具有组合关系的聚类研究，通过提出三种新算法可以将示例分成连贯的群组并推断其中的组合结构。与传统的层次聚类不同，该任务关注的是找到表示组成聚类的属性并集的组合聚类。 |
| [^192] | [Learning Optimal Prescriptive Trees from Observational Data.](http://arxiv.org/abs/2108.13628) | 该论文介绍了一种从观察数据中学习最优处方树的方法，可以在不需要数据随机化和对树有严格假设的情况下，通过混合整数优化技术进行学习，并具有建模领域特定问题的能力。 |
| [^193] | [Approximating Pandora's Box with Correlations.](http://arxiv.org/abs/2108.12976) | 在这项工作中，我们通过研究相关分布下的潘多拉魔盒问题，将其与均匀决策树和最小和集覆盖问题进行了近似转化。我们的主要结果表明，这些问题都可以在亚指数时间内实现常数近似比。 |
| [^194] | [Holistic Guidance for Occluded Person Re-Identification.](http://arxiv.org/abs/2104.06524) | 本文提出了一种新颖的整体指导方法来解决遮挡行人再识别问题，通过匹配遮挡样本与整体样本的距离分布来训练模型，无需额外监督。 |
| [^195] | [Co-Imitation Learning without Expert Demonstration.](http://arxiv.org/abs/2103.14823) | 本文提出了一种名为共同模仿学习（CoIL）的新型学习框架，通过利用代理自身的良好经验，而无需专家示范，来改善强化学习的效率。实验结果表明，该方法在各种任务上都具有显著的优越性。 |
| [^196] | [Moderately Supervised Learning: Definition, Framework and Generality.](http://arxiv.org/abs/2008.11945) | 适度监督学习包括标准有监督学习和弱监督学习，已有的一些标准有监督学习任务表明，给定的标签并不容易学习，从给定的标签转换为易于学习的目标的过程会显著影响最终的性能，有监督学习的定义隐藏了可能对解决方案影响重大的细节。 |
| [^197] | [Declarative Mechanism Design.](http://arxiv.org/abs/1912.13122) | 本文介绍了声明性机制设计的研究，提出了机构神经网络作为一种受管制的人工神经网络，引起人们对人工教学的关注，并提供了初步的答案。 |
| [^198] | [Attention Is All You Need.](http://arxiv.org/abs/1706.03762) | Transformer是一种新的简单网络架构，完全基于注意力机制，取代了复杂的循环神经网络或卷积神经网络。实验证明Transformer在机器翻译任务中的质量更好、并行化效果更佳，且训练时间更短。它在英译德和英译法任务中取得了比其他模型更好的结果。 |

# 详细

[^1]: 通过生成插值改进分类器的超领域鲁棒性

    Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation. (arXiv:2307.12219v1 [cs.LG])

    [http://arxiv.org/abs/2307.12219](http://arxiv.org/abs/2307.12219)

    本文提出了一种通过生成插值方法，利用生成模型合成多样化超领域样本来改进分类器的鲁棒性。通过在一个源域上训练StyleGAN模型，然后在其他域上微调，得到了多个相关的生成器，从而提升了性能。

    

    深度神经网络在学习独立同分布（i.i.d.）数据时表现出优越的性能。然而，当处理超领域（OoD）数据时，其性能显著下降，即训练和测试数据来自不同的分布。本文通过利用生成模型作为数据增强源，探索了改进神经分类器的超领域鲁棒性的方法。具体而言，我们提出了一种简单而有效的方法，称为生成插值，用于合成多个域的生成模型训练的多样化OoD样本。直接在源域上训练生成模型往往容易出现模式崩溃，并且有时会放大数据偏差。因此，我们首先在一个源域上训练StyleGAN模型，然后在其他域上微调，从而得到许多相关的生成器，它们的模型参数具有相同的初始化，因此是对齐的。

    Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly 
    
[^2]: 对无声语音激活设备进行对抗性攻击的对策

    Adversarial Agents For Attacking Inaudible Voice Activated Devices. (arXiv:2307.12204v1 [cs.LG])

    [http://arxiv.org/abs/2307.12204](http://arxiv.org/abs/2307.12204)

    本研究分析了对语音激活设备进行无声攻击的风险，并发现存在重大安全漏洞。我们提出了一种基线网络模型，并模拟了多种攻击场景，揭示了通过无需添加硬件或增加设备技能的物理访问来发现和拥有特权信息的潜力。使用深度Q学习算法，我们在少数步骤中快速拥有了所有节点。这些研究结果强调了对非传统网络和新的网络安全措施的重要性。

    

    我们对无声攻击对语音激活设备的分析确认了7.6的风险因素，强调了由NIST国家漏洞数据库（NVD）独立评分的重大安全漏洞。我们的基线网络模型展示了一种攻击者使用无声语音命令未经授权访问受保护笔记本上机密信息的场景。我们在该基线网络模型上模拟了许多攻击场景，揭示了通过物理访问而无需添加新硬件或增强设备技能的互联设备的大规模利用潜力来发现和拥有特权信息。使用微软的CyberBattleSim框架，我们评估了六种强化学习算法，发现深度Q学习与开发证明是最优的，可以在更少的步骤中迅速拥有所有节点。我们的研究结果强调了对非传统网络和新的网络安全措施的重要需求。

    Our analysis of inaudible attacks on voice-activated devices confirms the alarming risk factor of 7.6 out of 10, underlining significant security vulnerabilities scored independently by NIST National Vulnerability Database (NVD). Our baseline network model showcases a scenario in which an attacker uses inaudible voice commands to gain unauthorized access to confidential information on a secured laptop. We simulated many attack scenarios on this baseline network model, revealing the potential for mass exploitation of interconnected devices to discover and own privileged information through physical access without adding new hardware or amplifying device skills. Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps. Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measure
    
[^3]: NCART: 用于表格数据的神经分类与回归树

    NCART: Neural Classification and Regression Tree for Tabular Data. (arXiv:2307.12198v1 [cs.LG])

    [http://arxiv.org/abs/2307.12198](http://arxiv.org/abs/2307.12198)

    这篇论文介绍了一种名为NCART的可解释性神经网络，它利用多个可微性决策树替代全连接层，从而在保持可解释性的同时充分利用神经网络的优势。这种方法可以解决深度学习在大规模或高维数据集上的计算复杂性，并适用于小规模数据集。

    

    深度学习模型在表格数据分析中变得流行，因为它们解决了决策树的局限性，并实现了半监督学习、在线学习和迁移学习等有价值的应用。然而，这些深度学习方法通常会遇到一个折衷。一方面，当处理大规模或高维数据集时，它们可能计算量很大。另一方面，它们可能缺乏可解释性，不适用于小规模数据集。在本研究中，我们提出了一种新颖的可解释性神经网络，称为神经分类与回归树（NCART），以克服这些挑战。NCART是残差网络的变体，它将全连接层替换为多个可微性的无视决策树。通过将决策树集成到架构中，NCART保持其可解释性，同时又能从神经网络的端到端能力中受益。NCART架构的简洁性

    Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally expensive when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture 
    
[^4]: 单子化深度学习

    Monadic Deep Learning. (arXiv:2307.12187v1 [cs.PL])

    [http://arxiv.org/abs/2307.12187](http://arxiv.org/abs/2307.12187)

    DeepLearning.scala 2解决了在静态类型语言中使用神经网络进行训练时的自动求导问题，并提供了一组单子和单子变换器。

    

    Java和Scala社区建立了一个非常成功的大数据生态系统。然而，大部分在其上运行的神经网络是用动态类型编程语言建模的。这些动态类型的深度学习框架将神经网络视为包含许多可训练变量的可微分表达式，并在训练时对这些表达式进行自动求导。直到2019年，静态类型语言的学习框架没有提供传统框架的表达能力。除非创建大量样板代码进行硬编码的反向传播，否则用户无法使用自定义算法。我们在DeepLearning.scala 2中解决了这个问题。我们的贡献是：1.我们发现了一种新的方法，可以对包含多个可训练变量的静态类型函数进行反向模式的自动求导，并且可以自由地与元语言互操作。2.我们设计了一组单子和单子变换器，

    The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.  Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.  We solved this problem in DeepLearning.scala 2. Our contributions are:  1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.  2. We designed a set of monads and monad transformers, whic
    
[^5]: 机器学习发现辫子和平面辫子的不变量

    Machine learning discovers invariants of braids and flat braids. (arXiv:2307.12185v1 [math.GT])

    [http://arxiv.org/abs/2307.12185](http://arxiv.org/abs/2307.12185)

    本研究使用机器学习对辫子和平面辫子进行分类，得出了新的便利不变量，包括平面辫子的完全不变量。

    

    我们使用机器学习将辫子（或平面辫子）的示例分类为平凡或非平凡。我们的机器学习采用了神经网络（多层感知器）的监督学习形式。当它们在分类任务上取得良好的结果时，我们能够将它们的结构解释为数学猜想，然后证明这些猜想成为定理。结果，在辫子中找到了新的便利不变量，包括平面辫子的完全不变量。

    We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
    
[^6]: 基于原型驱动和多专家集成的多模态MR脑肿瘤图像分割

    Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation. (arXiv:2307.12180v1 [eess.IV])

    [http://arxiv.org/abs/2307.12180](http://arxiv.org/abs/2307.12180)

    本文提出了一种基于原型驱动和多专家集成的多模态MR脑肿瘤分割方法，通过脑肿瘤原型的指导，突出显示了每个肿瘤亚区的特征，并采用相互转移机制解决了单一模态特征信息不足的问题。

    

    对于多模态磁共振（MR）脑肿瘤图像分割，目前的方法通常直接从输入图像中提取判别特征，用于确定和定位肿瘤亚区类别。然而，由于肿瘤亚区的相互包含导致的信息重叠往往被忽视。此外，现有方法通常不会针对单一肿瘤亚区特征进行量身定制的努力。为此，提出了一种基于脑肿瘤原型驱动和多专家集成的多模态MR脑肿瘤分割方法。它可以在肿瘤原型的指导下突出显示每个肿瘤亚区的特征。具体而言，为了获得具有完整信息的原型，我们提出了一种相互传输机制，将不同模态的特征相互转移，以解决单一模态特征信息不足引起的问题。此外，我们设计了一种基于原型驱动的特征表示和融合方法。

    For multi-modal magnetic resonance (MR) brain tumor image segmentation, current methods usually directly extract the discriminative features from input images for tumor sub-region category determination and localization. However, the impact of information aliasing caused by the mutual inclusion of tumor sub-regions is often ignored. Moreover, existing methods usually do not take tailored efforts to highlight the single tumor sub-region features. To this end, a multi-modal MR brain tumor segmentation method with tumor prototype-driven and multi-expert integration is proposed. It could highlight the features of each tumor sub-region under the guidance of tumor prototypes. Specifically, to obtain the prototypes with complete information, we propose a mutual transmission mechanism to transfer different modal features to each other to address the issues raised by insufficient information on single-modal features. Furthermore, we devise a prototype-driven feature representation and fusion me
    
[^7]: 学习压缩（LtC）：高效的基于学习的流媒体视频分析

    Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics. (arXiv:2307.12171v1 [eess.IV])

    [http://arxiv.org/abs/2307.12171](http://arxiv.org/abs/2307.12171)

    本文介绍了一种名为LtC的协作框架，可以通过学习来有效减小流媒体视频的流量，提高流媒体视频分析的效率。

    

    视频分析通常作为边缘设置中的云服务进行，主要是为了卸载计算负载，并且在视频传感器不直接使用结果的情况下进行。从边缘设备发送高质量的视频数据在带宽和功耗方面都会很昂贵。为了构建一个高效利用这些资源的流媒体视频分析流水线，因此有必要减小视频流的大小。传统的视频压缩算法对视频的语义不敏感，可能既低效又对分析性能有害。在本文中，我们介绍了LtC，这是视频源和分析服务器之间的协作框架，它通过在分析流水线中高效地学习来减小视频流。具体而言，LtC将服务器上的全功能分析算法作为教师，训练一个轻量级的学生神经网络，然后部署在视频源上。

    Video analytics are often performed as cloud services in edge settings, mainly to offload computation, and also in situations where the results are not directly consumed at the video sensors. Sending high-quality video data from the edge devices can be expensive both in terms of bandwidth and power use. In order to build a streaming video analytics pipeline that makes efficient use of these resources, it is therefore imperative to reduce the size of the video stream. Traditional video compression algorithms are unaware of the semantics of the video, and can be both inefficient and harmful for the analytics performance. In this paper, we introduce LtC, a collaborative framework between the video source and the analytics server, that efficiently learns to reduce the video streams within an analytics pipeline. Specifically, LtC uses the full-fledged analytics algorithm at the server as a teacher to train a lightweight student neural network, which is then deployed at the video source. The
    
[^8]: 用于训练拥有数十亿参数的大型语言模型的优化网络架构

    Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])

    [http://arxiv.org/abs/2307.12169](http://arxiv.org/abs/2307.12169)

    本文提出了一种优化的网络架构，用于训练拥有数十亿参数的大型语言模型。这个架构根据语言模型的通信需求，将集群分割成一组通过非阻塞高带宽互连的GPU集合，并通过轨道连接仅连接具有通信需求的GPU，从而降低网络成本高达75％，同时不影响训练性能。

    

    本文挑战了为训练大型语言模型（LLMs）构建任意到任意网络的传统范式。我们展示了LLMs呈现出一种独特的通信模式，在其中，只有小组的GPU需要高带宽的任意到任意通信，以实现接近最优的训练性能。在这些GPU小组之间，通信非常微不足道、稀疏且均匀。我们提出了一个新的网络架构，紧密匹配LLMs的通信需求。我们的架构将集群分割为一组通过非阻塞任意到任意高带宽互连的GPU集合，我们称之为HB域。在HB域之间，网络只连接具有通信需求的GPU。我们将这种网络连接称为“仅轨道连接”，并展示了我们的架构相对于最先进的任意到任意Clos网络可以将网络成本降低高达75％，同时不损害LLM训练的性能。

    This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
    
[^9]: 面部关键点图用于肌萎缩侧索硬化的识别

    Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification. (arXiv:2307.12159v1 [cs.CV])

    [http://arxiv.org/abs/2307.12159](http://arxiv.org/abs/2307.12159)

    该论文提出了一种用于自动识别肌萎缩侧索硬化（ALS）的面部关键点图方法。通过分析患者的面部表情，可以更简单和更便宜地诊断和检测ALS。实验结果表明，该方法在多伦多神经脸数据集中表现出色，为该领域的进一步发展带来了希望。

    

    在肌萎缩侧索硬化（ALS）的早期阶段识别肌萎缩侧索硬化（ALS）对于确定治疗的开始、丰富前景和提高受影响个体的整体幸福感至关重要。然而，早期诊断和检测疾病的迹象并不直接。通过计算方法分析患者的面部表情可以提供一种更简单和更便宜的方式。当ALS患者进行特定的动作，例如张嘴，特定面部肌肉的运动与健康个体中观察到的不同。本文提出了面部关键点图以从面部图像的几何学中学习信息以自动识别ALS。多伦多神经脸数据集中的实验结果表明，所提出的方法优于最先进的结果，促进了该领域的有希望的发展。

    Identifying Amyotrophic Lateral Sclerosis (ALS) in its early stages is essential for establishing the beginning of treatment, enriching the outlook, and enhancing the overall well-being of those affected individuals. However, early diagnosis and detecting the disease's signs is not straightforward. A simpler and cheaper way arises by analyzing the patient's facial expressions through computational methods. When a patient with ALS engages in specific actions, e.g., opening their mouth, the movement of specific facial muscles differs from that observed in a healthy individual. This paper proposes Facial Point Graphs to learn information from the geometry of facial images to identify ALS automatically. The experimental outcomes in the Toronto Neuroface dataset show the proposed approach outperformed state-of-the-art results, fostering promising developments in the area.
    
[^10]: DIP-RL：在Minecraft中的演示推导偏好学习

    DIP-RL: Demonstration-Inferred Preference Learning in Minecraft. (arXiv:2307.12158v1 [cs.LG])

    [http://arxiv.org/abs/2307.12158](http://arxiv.org/abs/2307.12158)

    DIP-RL是一种利用人类演示的算法，在非结构化和开放的环境中通过多种方式推导偏好并学习奖励函数，其在Minecraft中的砍树任务中表现出了竞争力。

    

    在机器学习中的顺序决策过程中，算法代理通过接收奖励信号的反馈来与环境进行交互学习。然而，在许多非结构化的现实世界环境中，这样的奖励信号是未知的，并且人类无法可靠地构建一个正确捕捉所需行为的奖励信号。为了在这样的非结构化和开放的环境中完成任务，我们提出了Demonstration-Inferred Preference Reinforcement Learning (DIP-RL)，这是一种利用人类演示的算法，包括训练自编码器，用演示数据种子强化学习 (RL)训练批次，并推导出偏好以学习引导RL的奖励函数。我们在Minecraft中的砍树任务中评估了DIP-RL。结果表明，该方法能够指导RL代理学习一个反映人类偏好的奖励函数，并且相对于基准模型，DIP-RL表现出了竞争力。

    In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspi
    
[^11]: 在多层级环境中识别供应链结果的贡献者：一种去中心化方法

    Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach. (arXiv:2307.12157v1 [cs.LG])

    [http://arxiv.org/abs/2307.12157](http://arxiv.org/abs/2307.12157)

    研究介绍了一种去中心化方法，利用可解释的人工智能对多层级供应链中感兴趣指标的估计贡献进行计算，无需数据共享。经验证实，该方法在检测质量变化的原因方面有效。

    

    组织经常难以确定产品质量和交货时间等指标变化的原因。当原因位于公司边界之外，处于部分可观察的多层级供应链中时，这项任务变得越来越具有挑战性。尽管传统的供应链管理主张通过数据共享获得更好的洞察力，但由于数据隐私问题，这在实践中并不常见。我们提出使用可解释的人工智能来实现对多阶段生产过程中感兴趣指标的估计贡献的去中心化计算。这种方法减轻了说服供应链参与者共享数据的需求，因为所有计算都是以去中心化的方式进行的。我们的方法是通过对真实多阶段制造过程进行数据收集进行经验证实的。结果显示，与使用集中化方法相比，我们的方法在检测质量变化的原因方面具有有效性。

    Organisations often struggle to identify the causes of change in metrics such as product quality and delivery duration. This task becomes increasingly challenging when the cause lies outside of company borders in multi-echelon supply chains that are only partially observable. Although traditional supply chain management has advocated for data sharing to gain better insights, this does not take place in practice due to data privacy concerns. We propose the use of explainable artificial intelligence for decentralised computing of estimated contributions to a metric of interest in a multi-stage production process. This approach mitigates the need to convince supply chain actors to share data, as all computations occur in a decentralised manner. Our method is empirically validated using data collected from a real multi-stage manufacturing process. The results demonstrate the effectiveness of our approach in detecting the source of quality variations compared to a centralised approach using
    
[^12]: 移动设备上的实时神经视频恢复和增强

    Real-Time Neural Video Recovery and Enhancement on Mobile Devices. (arXiv:2307.12152v1 [cs.NI])

    [http://arxiv.org/abs/2307.12152](http://arxiv.org/abs/2307.12152)

    本文提出了一种在移动设备上实现实时神经视频恢复和增强的新方法，包括视频帧恢复方案、超分辨率算法和增强感知的视频比特率自适应算法。实验表明该方法能够支持每秒30帧，并在不同网络环境下实现实时增强。

    

    随着移动设备在视频流媒体方面的受欢迎程度不断增加，优化这些设备的流媒体体验至关重要。虽然基于深度学习的视频增强技术引起了广泛关注，但大多数技术不能在移动设备上支持实时增强。此外，许多技术仅关注超分辨率，无法处理视频帧的部分或完全丢失或损坏，而这在互联网和无线网络上非常常见。为了克服这些挑战，本文提出了一种新颖的方法。我们的方法包括（i）一种新颖的视频帧恢复方案，（ii）一种新的超分辨率算法，以及（iii）一种接收增强感知的视频比特率自适应算法。我们在iPhone 12上实现了我们的方法，并且支持每秒30帧（FPS）。我们在WiFi、3G、4G和5G网络等不同网络上评估了我们的方法。评估结果表明，我们的方法能够实现实时增强。

    As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.  To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancem
    
[^13]: CorrFL: 异质IoT环境中基于相关性的神经网络架构，用于处理不可用问题

    CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment. (arXiv:2307.12149v1 [cs.LG])

    [http://arxiv.org/abs/2307.12149](http://arxiv.org/abs/2307.12149)

    本论文提出了一种基于相关性的神经网络架构来解决联邦学习中的异质模型和不可用节点的问题，通过将模型权重投影到共同的潜空间，并通过最小化重构损失和最大化相关性来实现模型的填补和预测。

    

    联邦学习(Federated Learning, FL)范式在现实环境中面临一些限制，包括本地模型架构的异质性以及由于连接问题导致分布式物联网(IoT)节点不可用的挑战。这些因素提出了一个问题：“如何填补不可用模型的训练差距？”这个问题被称为“斜线联合学习”问题。本文提出了受表征学习领域影响的基于相关性的FL (CorrFL)方法来解决这个问题。CorrFL将各种模型权重投影到一个共同的潜空间中，以解决模型的异质性问题。其损失函数在模型缺失时最小化重构损失，并最大化生成模型之间的相关性。

    The Federated Learning (FL) paradigm faces several challenges that limit its application in real-world environments. These challenges include the local models' architecture heterogeneity and the unavailability of distributed Internet of Things (IoT) nodes due to connectivity problems. These factors posit the question of "how can the available models fill the training gap of the unavailable models?". This question is referred to as the "Oblique Federated Learning" problem. This problem is encountered in the studied environment that includes distributed IoT nodes responsible for predicting CO2 concentrations. This paper proposes the Correlation-based FL (CorrFL) approach influenced by the representational learning field to address this problem. CorrFL projects the various model weights to a common latent space to address the model heterogeneity. Its loss function minimizes the reconstruction loss when models are absent and maximizes the correlation between the generated models. The latte
    
[^14]: 一个清洁河流的愿景：利用快照高光谱成像技术检测宏观塑料垃圾

    A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter. (arXiv:2307.12145v1 [cs.CV])

    [http://arxiv.org/abs/2307.12145](http://arxiv.org/abs/2307.12145)

    通过利用快照高光谱成像技术和机器学习分类方法，研究人员提出了一种在河流环境中高效自动化监测宏观塑料垃圾的方法，实现了高检测精度，为解决全球范围的漂浮垃圾问题做出了贡献。

    

    进入河流的塑料废物对当地生态系统造成了伤害，导致了负面的生态和经济影响。大量的塑料废物从内陆运输到海洋，形成了全球范围的漂浮垃圾场问题。在这种情况下，高效自动化地监测管理不善的塑料废物至关重要。为了解决这个问题，我们分析了在类似河流的环境中利用计算成像方法检测宏观塑料垃圾的可行性。我们利用快照可见-短波红外高光谱成像实现了部分浸没塑料的近实时跟踪。我们的实验表明，结合机器学习分类方法的成像策略可以在具有挑战性的场景中实现高检测精度，尤其是在利用高光谱数据和非线性分类器时。所有的代码、数据和模型都可以在线上获取：https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection

    Plastic waste entering the riverine harms local ecosystems leading to negative ecological and economic impacts. Large parcels of plastic waste are transported from inland to oceans leading to a global scale problem of floating debris fields. In this context, efficient and automatized monitoring of mismanaged plastic waste is paramount. To address this problem, we analyze the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy even in challenging scenarios, especially when leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online: https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection.
    
[^15]: 强化学习中自适应昼夜节律的出现

    Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning. (arXiv:2307.12143v1 [cs.AI])

    [http://arxiv.org/abs/2307.12143](http://arxiv.org/abs/2307.12143)

    本文研究了深度强化学习智能体中自适应昼夜节律的出现，通过在周期性变化环境中进行觅食任务，证明了智能体能够内化环境信号并适应相位变化，这一自适应过程是通过人工神经元的动力学来实现的。

    

    适应环境的规律对生物有重要意义，使其能够预测事件和制定计划。其中一个显著的例子是生物对地球自转24小时周期的内化，即昼夜节律。本文研究了深度强化学习智能体中类似昼夜节律的出现。具体而言，我们在一个周期性变化可靠的环境中部署了智能体，并解决了一个觅食任务。我们系统地表征了智能体在学习过程中的行为，并证明了一个内源性且可调匹的节律的出现。有趣的是，内部节律适应了环境信号相位的变化，而无需重新训练。此外，我们通过分岔和相位响应曲线分析展示了人工神经元如何发展出支持环境节律内化的动力学。从动力学系统视角看，我们证明了此自适应过程的实现方式。

    Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergenc
    
[^16]: 利用强化学习释放三维装载有限车辆路径问题的碳减排潜力

    Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem. (arXiv:2307.12136v1 [cs.LG])

    [http://arxiv.org/abs/2307.12136](http://arxiv.org/abs/2307.12136)

    该论文提出了一个利用强化学习方法解决三维装载有限车辆路径问题的模型，该方法能够在线性时间内有效计算共载和路径的可行解决方案，从而释放了碳减排的潜力。

    

    重型货车是供应链交付系统的重要支柱，但在英国仅具有60％的装载效率，对碳排放有显著贡献。协同车辆路径规划被提出作为提高效率的解决方案，但仍面临挑战。一个关键挑战是有效计算共载和路径的可行解决方案。当前的运筹学方法在问题规模增大时存在非线性扩展，因此只能限于在有限的地理范围内计算结果以满足日常运营的时间要求。这仅允许在路径规划上寻找局部最优解，无法实现全局优化潜力。我们开发了一个强化学习模型来近线性时间解决三维装载有限车辆路径问题。虽然此问题在运筹学中得到了广泛研究，但没有关于用强化学习解决该问题的出版物存在。

    Heavy goods vehicles are vital backbones of the supply chain delivery system but also contribute significantly to carbon emissions with only 60% loading efficiency in the United Kingdom. Collaborative vehicle routing has been proposed as a solution to increase efficiency, but challenges remain to make this a possibility. One key challenge is the efficient computation of viable solutions for co-loading and routing. Current operations research methods suffer from non-linear scaling with increasing problem size and are therefore bound to limited geographic areas to compute results in time for day-to-day operations. This only allows for local optima in routing and leaves global optimisation potential untouched. We develop a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. While this problem has been studied extensively in operations research, no publications on solving it with reinforcement learning exist.
    
[^17]: 多分布学习对VC类的样本复杂度

    The Sample Complexity of Multi-Distribution Learning for VC Classes. (arXiv:2307.12135v1 [cs.LG])

    [http://arxiv.org/abs/2307.12135](http://arxiv.org/abs/2307.12135)

    这个论文研究了多分布学习对VC类的样本复杂度，发现现有的上下界存在显著差距，并讨论了一些涉及统计学习中博弈动态的困难。

    

    多分布学习是将PAC学习推广到具有多个数据分布的情境。我们对于PAC可学习类的已知上下界仍然存在显著差距。尽管我们了解在$k$个分布上学习具有VC维度d的类的样本复杂度为$O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$，但最好的下界是$\Omega(\epsilon^{-2}(d + k \ln(k)))$。我们讨论了这个问题的最新进展以及在统计学习中使用博弈动态的一些困难。

    Multi-distribution learning is a natural generalization of PAC learning to settings with multiple data distributions. There remains a significant gap between the known upper and lower bounds for PAC-learnable classes. In particular, though we understand the sample complexity of learning a VC dimension d class on $k$ distributions to be $O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$, the best lower bound is $\Omega(\epsilon^{-2}(d + k \ln(k)))$. We discuss recent progress on this problem and some hurdles that are fundamental to the use of game dynamics in statistical learning.
    
[^18]: AI在道路上的应用：智能城市中交通事故及事故检测系统的综合分析

    AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities. (arXiv:2307.12128v1 [cs.CV])

    [http://arxiv.org/abs/2307.12128](http://arxiv.org/abs/2307.12128)

    本文分析了智能城市中交通事故及事故检测系统的综合方法。通过利用交通监控摄像头和行为识别系统，可以实时检测和响应交通事故，并将其与紧急服务整合，以减少人为错误和改善交通安全。

    

    事故检测和交通分析是智能城市和自动化交通系统的关键组成部分，可以减少事故频率和严重程度，改善交通管理。本文利用美国国家公路交通安全管理局（NHTSA）的Crash Report Sampling System（CRSS）的数据，对美国不同地区的交通事故进行了全面分析。为了应对事故检测和交通分析的挑战，本文提出了一个框架，利用交通监控摄像头和行为识别系统来即时检测和响应交通事故。将所提出的框架与紧急服务整合，将交通摄像头和机器学习算法的力量用于响应交通事故和减少人为错误。智能城市中的事故检测系统等先进智能技术将改善交通安全，提高交通效率。

    Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve tra
    
[^19]: 用扩散 - 生成对抗网络合成巴蒂克图案

    Synthesis of Batik Motifs using a Diffusion -- Generative Adversarial Network. (arXiv:2307.12122v1 [cs.CV])

    [http://arxiv.org/abs/2307.12122](http://arxiv.org/abs/2307.12122)

    本研究使用了StyleGAN2-Ada和扩散技术，通过调整模型架构和使用精选数据集，成功产生了逼真且高质量的合成巴蒂克图案，为巴蒂克设计师提供了重要的辅助工具。

    

    巴蒂克是印度尼西亚社会的一种独特的艺术与工艺品的结合，研究巴蒂克图案主要集中在分类方面，但是进一步的研究可以扩展到巴蒂克图案的合成上。生成对抗网络（GANs）是一种重要的深度学习模型，用于生成合成数据，但通常面临结果的稳定性和一致性方面的挑战。本研究集中在使用StyleGAN2-Ada和扩散技术来产生逼真且高质量的合成巴蒂克图案。StyleGAN2-Ada是GAN模型的一种变体，将图像中的风格和内容分离，而扩散技术则向数据中引入随机噪声。在巴蒂克的语境中，使用StyleGAN2-Ada和扩散技术来产生逼真的合成巴蒂克图案。本研究还对模型架构进行了调整，并使用了精选的巴蒂克数据集。主要目标是帮助巴蒂克设计师。

    Batik, a unique blend of art and craftsmanship, is a distinct artistic and technological creation for Indonesian society. Research on batik motifs is primarily focused on classification. However, further studies may extend to the synthesis of batik patterns. Generative Adversarial Networks (GANs) have been an important deep learning model for generating synthetic data, but often face challenges in the stability and consistency of results. This research focuses on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN model that separates the style and content aspects in an image, whereas diffusion techniques introduce random noise into the data. In the context of batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic batik patterns. This study also made adjustments to the model architecture and used a well-curated batik dataset. The main goal is to assist batik designe
    
[^20]: 个性化医疗革命：利用移动AIGC实现人类数字孪生

    A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC. (arXiv:2307.12115v1 [cs.NI])

    [http://arxiv.org/abs/2307.12115](http://arxiv.org/abs/2307.12115)

    移动AIGC技术可以推动人类数字孪生在个性化医疗中的应用，包括生成罕见疾病数据、建模高保真数字孪生以及提供24/7定制医疗服务。

    

    移动人工智能生成内容（AIGC）技术指的是在移动边缘网络上采用AI算法自动化信息创建过程，同时满足终端用户的需求。移动AIGC最近引起了极大的关注，并且可以成为人类数字孪生（HDT）的关键技术。移动AIGC驱动的HDT有望通过生成罕见疾病数据、建模高保真数字孪生、构建多功能试验平台和提供全天候定制医疗服务来革命性地改变个性化医疗。为了推动这一新型范式的发展，在本文中，我们提出了一个移动AIGC驱动的HDT系统架构，并强调了相应的设计要求和挑战。此外，我们还举例说明了两种用例，即在定制手术规划和个性化药物治疗中使用移动AIGC驱动的HDT。此外，我们进行了一项实验性研究。

    Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental stud
    
[^21]: 零样本和少样本情况下应用于临床和生物医学任务的指导细调大型语言模型的研究

    A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])

    [http://arxiv.org/abs/2307.12114](http://arxiv.org/abs/2307.12114)

    这项研究评估了四种指导细调大型语言模型在临床和生物医学任务上的表现，并发现它们在零样本和少样本情况下接近最先进模型的性能，尤其在问答任务上表现良好。然而，在分类和关系抽取任务上的表现稍逊于特定训练于医学领域的模型。没有一个模型在所有研究任务上胜过其他模型，有些模型更适合特定任务。

    

    我们评估了四种最先进的指导细调大型语言模型（LLM）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在13个实际世界的临床和生物医学自然语言处理（NLP）任务中的表现，例如命名实体识别（NER）、问答（QA）、关系抽取（RE）等。我们的综合结果表明，在大多数任务的零样本和少样本情况下，评估的LLM开始接近最先进模型的性能，尤其对于QA任务表现得特别好，即使它们之前没有见过这些任务的示例。然而，我们观察到分类和关系抽取任务的表现低于特定训练于医学领域的模型（如PubMedBERT）可以达到的水平。最后，我们注意到没有一个LLM在所有研究任务上都胜过其他模型，有些模型更适合于特定的任务。

    We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
    
[^22]: 使用深度强化学习的多喷口对旋转圆柱体流动进行主动控制

    Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning. (arXiv:2307.12083v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.12083](http://arxiv.org/abs/2307.12083)

    本研究使用深度强化学习算法结合旋转和多个可控喷口，通过优化喷口数量和位置，传感器位置以及每个动作可允许的最大流量和每个episode中允许的总喷口数的形式，实现对旋转圆柱体流动的主动控制，抑制涡流脱落和稳定卡门涡流。

    

    人工智能的真正威力体现在强化学习中，由于其动态性质，强化学习在计算和物理方面更为复杂。旋转和喷射已被证实是减少钝体所受阻力的一种有效的主动流动控制方式。本研究将使用深度强化学习算法(DRL)来添加旋转，并利用多个可控喷口以实现最大可能的阻力抑制。将介绍DRL代码的特点，包括控制参数、其限制以及用于旋转的DRL网络的优化。本研究将重点优化喷口的数量和位置、传感器位置以及每个动作可允许的最大流量和每个episode中允许的总喷口数的形式。研究结果表明，将旋转与DRL工具相结合是有希望的，因为它可以抑制涡流脱落，稳定卡门涡流。

    The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection have been a proven way of active flow control to reduce the drag force exerted on blunt bodies. Rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, sensors location, and maximum allowed flow rate to jets in the form of maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation with the DRL tools is promising, since it suppresses the vortex shedding, stabilizes the Karman vortex stre
    
[^23]: 具有公平约束的谱归一化切图分割

    Spectral Normalized-Cut Graph Partitioning with Fairness Constraints. (arXiv:2307.12065v1 [cs.LG])

    [http://arxiv.org/abs/2307.12065](http://arxiv.org/abs/2307.12065)

    本文提出了一种谱归一化切图分割的公平约束变种问题，通过两阶段的谱算法FNM，实现了在最小化归一化切值的同时保证每个聚类中各个人口群体的近似比例。

    

    归一化切图分割的目标是将图中的节点集合划分为$k$个不相交的聚类，以最小化任意聚类与其他聚类之间的总边数占比。本文考虑了一种公平的划分问题变种，在此问题中，节点由一个分类敏感属性（例如性别或种族）表征，指示其属于不同人口群体。我们的目标是在最小化归一化切值的同时，确保每个群体在每个聚类中近似按比例表示。为了解决这个问题，我们提出了一种称为FNM的两阶段谱算法。在第一阶段，我们根据公平标准向目标函数中添加了一个增广Lagrangian项，以获得更公平的谱节点嵌入。然后，在第二阶段，我们设计了一个舍入方案，从公平嵌入中生成$k$个聚类，有效地权衡了公平性和划分质量。通过对九个基准数据集的全面实验

    Normalized-cut graph partitioning aims to divide the set of nodes in a graph into $k$ disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we consider a fair variant of the partitioning problem wherein nodes are characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal is to ensure that each group is approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we propose a two-phase spectral algorithm called FNM. In the first phase, we add an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we design a rounding scheme to produce $k$ clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine bench
    
[^24]: 通过动态构建潜在地标图在分层强化学习中平衡探索和利用

    Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs. (arXiv:2307.12063v1 [cs.LG])

    [http://arxiv.org/abs/2307.12063](http://arxiv.org/abs/2307.12063)

    本文提出了一种通过动态构建潜在地标图来平衡探索和利用，解决了目标条件分层强化学习中的时间一致性和子目标选择策略的问题。

    

    目标条件分层强化学习(GCHRL)是解决强化学习中探索-利用困境的一种有前途的范例。它将源任务分解为子目标条件子任务，并在子目标空间中进行探索和利用。GCHRL的有效性在很大程度上依赖于子目标表示函数和子目标选择策略。然而，现有的工作常常忽视了GCHRL中学习潜在子目标表示的时间一致性，并且缺乏一种平衡探索和利用的高效子目标选择策略。本文提出了通过动态构建Latent Landmark图的分层强化学习（HILL）来克服这些局限性。HILL使用对比表示学习目标学习满足时间一致性的潜在子目标表示。基于这些表示，HILL动态构建潜在地标图，并在节点和边上使用新颖性度量。

    Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into subgoal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on subgoal representation functions and subgoal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent subgoal representations and lack an efficient subgoal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and
    
[^25]: 游戏理论的鲁棒强化学习处理时间耦合的干扰

    Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations. (arXiv:2307.12062v1 [cs.LG])

    [http://arxiv.org/abs/2307.12062](http://arxiv.org/abs/2307.12062)

    这篇论文提出了一种新的鲁棒强化学习方法，通过将时间耦合的鲁棒强化学习问题视为两人零和游戏来处理问题，并通过找到近似均衡来确保代理对时间耦合干扰的鲁棒性。实验结果显示，该方法在各种连续控制任务中相比基准方法表现出显著的鲁棒性优势。

    

    鲁棒强化学习旨在训练能够在环境干扰或对抗攻击下表现良好的策略。现有方法通常假设可能干扰的空间在各个时间步骤保持不变。然而，在许多情况下，给定时间步骤上可能干扰的空间取决于过去的干扰。我们正式引入时间耦合干扰，对现有的鲁棒强化学习方法提出了新的挑战。为了应对这个挑战，我们提出了GRAD，一种新的游戏理论方法，将时间耦合鲁棒强化学习问题视为部分可观察的两人零和游戏。通过在这个游戏中找到一个近似均衡，GRAD确保了代理的对时间耦合干扰的鲁棒性。对各种连续控制任务的实证实验表明，我们提出的方法相比基准方法在标准和时间耦合干扰下具有显著的鲁棒性优势。

    Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupl
    
[^26]: 使用图形处理单元快速完成知识图谱

    Fast Knowledge Graph Completion using Graphics Processing Units. (arXiv:2307.12059v1 [cs.AI])

    [http://arxiv.org/abs/2307.12059](http://arxiv.org/abs/2307.12059)

    本文提出了一种在GPU上高效完成知识图谱补全的框架，通过使用知识图谱嵌入矢量来添加新关系。该框架将知识图谱补全问题转化为相似性连接问题，并提供了一种高效处理相似性连接问题的方法。

    

    知识图谱可以在与数据语义相关的多个领域中使用，例如问答系统和基于知识的系统。然而，目前构建的知识图谱需要通过补充关系来获得更好的知识。这被称为知识图谱补全。为了通过使用知识图谱嵌入模型向现有知识图谱添加新关系，我们必须评估N×N×R个矢量操作，其中N是实体数量，R是关系类型数量。这非常昂贵。在本文中，我们提供了一个在GPU上高效完成知识图谱补全任务的框架，以获取使用知识图谱嵌入矢量的新关系。在提出的框架中，我们首先定义了“可以转换为度量空间”，然后提供了一种将知识图谱补全问题转化为“可以转换为度量空间”的模型的相似性连接问题的方法。之后，为了高效处理相似性连接问题，我们

    Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.  In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we
    
[^27]: 外部推理：朝着多种大型语言模型可互换辅助与人类反馈的方向前进

    External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])

    [http://arxiv.org/abs/2307.12057](http://arxiv.org/abs/2307.12057)

    本文提出通过从外部存储库中选择性地集成知识来增强大型语言模型，提出了一种外部推理的新方法，例子是ChatPDF。

    

    记忆被认为是使海马体和脑神经元内保持视觉和语言信息、随后用于解决通过学习一生中遇到的现实挑战的关键人类能力。通过应用已获得的知识解决复杂的人工智能任务是实现人工通用智能的一大进展。然而，尽管像GPT-3.5和GPT-4这样的大型语言模型在语言理解、生成、交互和推理方面显示了卓越的能力，但由于上下文长度的限制，它们无法处理广泛、不断演变的知识库。本文提出通过从外部存储库中选择性地集成知识来增强LLMs，并介绍了一种外部推理的新方法，例子是ChatPDF。

    Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to
    
[^28]: 通过在霍夫空间中采用新的SR损失函数进行增强的迁移学习的飞行轨迹划分

    Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space. (arXiv:2307.12032v1 [cs.CV])

    [http://arxiv.org/abs/2307.12032](http://arxiv.org/abs/2307.12032)

    本论文引入创新的增强迁移学习模型，以及通过在霍夫空间中使用新的SR损失函数改善飞行轨迹线检测的方法。这些方法能够准确地检测飞行轨迹并且对数据需求较少，为基于机器学习的航空研究中的飞行轨迹检测提供了新的解决方案。

    

    空中交通对环境产生了重大挑战，特别是飞行轨迹造成的对气候变化的贡献，由于其潜在的全球变暖影响。从卫星图像中检测飞行轨迹一直以来都是一个长期存在的难题。传统的计算机视觉技术在不同的图像条件下存在局限性，而使用典型的卷积神经网络的机器学习方法受到手工标注的飞行轨迹数据集和专门的学习过程的限制。在本文中，我们引入了一种创新的模型，基于增强的迁移学习，能够准确地检测飞行轨迹，并且数据需求较少。我们还提出了一种新的损失函数，SR Loss，通过将图像空间转换为霍夫空间来改善飞行轨迹线的检测。我们的研究为基于机器学习的航空研究中的飞行轨迹检测打开了新的途径，提供了解决大型手工标注数据集缺乏问题，并显著增强了飞行轨迹检测能力。

    Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detecti
    
[^29]: 将患者偏好纳入Q学习的灵活框架

    A Flexible Framework for Incorporating Patient Preferences Into Q-Learning. (arXiv:2307.12022v1 [cs.LG])

    [http://arxiv.org/abs/2307.12022](http://arxiv.org/abs/2307.12022)

    这个论文提出了一种称为潜在效用Q学习的方法，能够将患者偏好纳入复合结果的动态治疗方案中，解决了传统方法对时间点和结果数量的限制，能够实现强大的性能。

    

    在现实世界的医疗问题中，通常存在多个竞争性的关注点，如治疗疗效和副作用严重程度。然而，用于估计动态治疗方案 (DTRs) 的统计方法通常假设只有一个关注点，而处理复合结果的方法很少，存在重要限制，包括对单个时间点和两个结果的限制、无法纳入患者的自述偏好以及有限的理论保证。为此，我们提出了一个新的方法来解决这些限制，我们称之为潜在效用Q学习(LUQ-Learning)。LUQ-Learning采用潜在模型方法，自然地将Q学习扩展到复合结果设置，并为每个患者选择理想的结果权衡。与之前的方法不同，我们的框架允许任意数量的时间点和结果，纳入陈述的偏好，并实现强大的渐近性能。

    In real-world healthcare problems, there are often multiple competing outcomes of interest, such as treatment efficacy and side effect severity. However, statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a single outcome of interest, and the few methods that deal with composite outcomes suffer from important limitations. This includes restrictions to a single time point and two outcomes, the inability to incorporate self-reported patient preferences and limited theoretical guarantees. To this end, we propose a new method to address these limitations, which we dub Latent Utility Q-Learning (LUQ-Learning). LUQ-Learning uses a latent model approach to naturally extend Q-learning to the composite outcome setting and adopt the ideal trade-off between outcomes to each patient. Unlike previous approaches, our framework allows for an arbitrary number of time points and outcomes, incorporates stated preferences and achieves strong asymptotic performance with rea
    
[^30]: 通过数据驱动的多步预测血糖预测器的模型预测控制（MPC）人工胰腺

    Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors. (arXiv:2307.12015v1 [eess.SY])

    [http://arxiv.org/abs/2307.12015](http://arxiv.org/abs/2307.12015)

    本文介绍了一种通过数据驱动的多步血糖预测器与线性时变模型预测控制（MPC）相结合的闭环胰岛素输送算法设计，用于治疗1型糖尿病。作者通过直接拟合整个血糖预测曲线，结合非线性和线性模型，与传统线性MPC进行对比，评估了算法的优劣。

    

    我们提出了一个封闭式胰岛素输送算法的设计和\textit{in-silico}评估，用于治疗1型糖尿病（T1D），该算法包括一个基于数据驱动的多步前瞻血糖(BG)预测器，与一个线性时变（LTV）模型预测控制（MPC）框架集成。我们建议直接拟合整个BG预测曲线，作为MPC中预定义预测时间范围内的非线性函数，这个非线性函数由过去的输入-输出数据和未来的胰岛素控制输入的线性函数组成。对于非线性部分，我们提出了一个长短期记忆（LSTM）网络，而对于线性部分，选择了线性回归模型。为了评估与从数据中识别出的传统线性MPC（基于自回归外生（ARX）输入模型）相比的优劣，我们在三个模拟场景中评估了提议的LSTM-MPC控制器：一个正常情况wit

    We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case wit
    
[^31]: NLCUnet: 具有发际线细节的单图像超分辨网络

    NLCUnet: Single-Image Super-Resolution Network with Hairline Details. (arXiv:2307.12014v1 [eess.IV])

    [http://arxiv.org/abs/2307.12014](http://arxiv.org/abs/2307.12014)

    本文提出了一种具有发际线细节的单图像超分辨网络（NLCUnet）。该网络使用非局部注意机制恢复局部细节，并提出了一种不需要模糊核估计的新网络架构。此外，还提出了一种在中心裁剪内部进行随机裁剪的方法来提高语义信息的利用效率。实验结果表明，NLCUnet相较于其他方法表现更好。

    

    对于单图像超分辨任务来说，追求超分辨图像的精细细节是具有挑战性的。本文提出了一种具有发际线细节的单图像超分辨网络（称为NLCUnet），包括三个核心设计。首先，我们引入了非局部注意机制，通过从整个图像区域学习来恢复局部细节。然后，我们发现通过现有工作训练的模糊核是不必要的。基于这一发现，我们创建了一个新的网络架构，将深度卷积与通道注意力集成，而不需要估计模糊核，结果导致性能提升。最后，为了使裁剪区域包含尽可能多的语义信息，我们提出了一种在中心512x512裁剪内部进行随机64x64裁剪的方法，而不是直接在整个2K大小的图像内进行随机裁剪。在基准DF2K数据集上进行的大量实验表明，我们的NLCUnet优于其他方法。

    Pursuing the precise details of super-resolution images is challenging for single-image super-resolution tasks. This paper presents a single-image super-resolution network with hairline details (termed NLCUnet), including three core designs. Specifically, a non-local attention mechanism is first introduced to restore local pieces by learning from the whole image region. Then, we find that the blur kernel trained by the existing work is unnecessary. Based on this finding, we create a new network architecture by integrating depth-wise convolution with channel attention without the blur kernel estimation, resulting in a performance improvement instead. Finally, to make the cropped region contain as much semantic information as possible, we propose a random 64$\times$64 crop inside the central 512$\times$512 crop instead of a direct random crop inside the whole image of 2K size. Numerous experiments conducted on the benchmark DF2K dataset demonstrate that our NLCUnet performs better than t
    
[^32]: Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering.（专家知识感知的图像变化图表示学习用于关注差异的医学视觉问答）

    Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering. (arXiv:2307.11986v1 [cs.CV])

    [http://arxiv.org/abs/2307.11986](http://arxiv.org/abs/2307.11986)

    本研究提出了一个新的医学视觉问答任务，名为MIMIC-Diff-VQA，为自动化医学视觉语言模型做出了贡献。与现有数据集相比，该任务旨在回答关于疾病和图像差异的问题，并应用了专家知识感知的图表示学习模型。

    

    为了为自动化医学视觉语言模型做出贡献，我们提出了一个新颖的胸部X光图像差异视觉问答（VQA）任务。该任务旨在回答几个关于疾病以及更重要的是它们之间差异的问题。这与放射科医生的诊断实践相一致，放射科医生在得出报告之前会对当前图像与参考图像进行比较。我们收集了一个新的数据集，称为MIMIC-Diff-VQA，包括来自164,324对主图像和参考图像的700,703个问题-答案配对。与现有的医学VQA数据集相比，我们的问题针对了临床专业人员使用的评估-诊断-干预-评估治疗过程。同时，我们还提出了一种新的专家知识感知的图表示学习模型来解决这个任务。

    To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a mul
    
[^33]: 协作图神经网络用于属性网络嵌入

    Collaborative Graph Neural Networks for Attributed Network Embedding. (arXiv:2307.11981v1 [cs.LG])

    [http://arxiv.org/abs/2307.11981](http://arxiv.org/abs/2307.11981)

    本文提出了一种协作式图神经网络用于属性网络嵌入，通过深入参与节点属性来增强节点连接并改善对非活动节点的感受域限制。

    

    图神经网络（GNNs）在属性网络嵌入方面显示出了显著的性能。然而，现有的研究主要集中在利用网络结构，而对节点属性的利用相对较少，它们只在初始层作为节点特征。这种简单的策略限制了节点属性在增强节点连接方面的潜力，导致了对具有少量甚至没有邻居的非活动节点的有限感受域。此外，大多数GNN的训练目标（即重构网络结构）也不包括节点属性，尽管研究表明重构节点属性是有益的。因此，深入参与图卷积操作和训练目标等GNN关键组件中的节点属性是很有前景的。然而，这是一个非常重要的任务，因为需要适当的集成方式来保持GNN的优点。为了填补这个差距，在本文中，我们提出了协作式图神经网络...

    Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborat
    
[^34]: 使用迭代全局变换器模型在MRI中模拟任意级别对比剂剂量的仿真

    Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model. (arXiv:2307.11980v1 [eess.IV])

    [http://arxiv.org/abs/2307.11980](http://arxiv.org/abs/2307.11980)

    本研究提出了一种基于全局变换器的迭代建模方法，用于在MRI中合成具有任意对比增强水平的图像。该方法通过引入子采样的注意机制和旋转平移模块来捕捉各种对比度相关的特征。定量评估结果显示，该模型在剂量降低和肿瘤分割等任务中表现优于其他最先进的方法。

    

    基于深度学习的MRI图像中对比剂剂量的降低和消除得到了广泛关注，考虑到钆基对比剂的不良影响。然而，这些深度学习算法受到高质量低剂量数据集的限制。此外，不同类型的对比剂和病理需要不同剂量水平的深度学习算法才能可靠工作。在本研究中，我们提出了一种基于全局变换器(Gformer)的新型迭代建模方法，用于合成具有与不同剂量水平相对应的任意对比增强图像。所提出的Gformer结合了基于子采样的注意机制和旋转平移模块，捕捉了各种与对比度相关的特征。定量评估表明，所提出的模型优于其他最先进的方法。我们还对剂量降低和肿瘤分割等下游任务进行了定量评估，以展示其临床-

    Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical
    
[^35]: 为什么视觉-语言模型的提示调参对于噪声标签具有鲁棒性？

    Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?. (arXiv:2307.11978v1 [cs.CV])

    [http://arxiv.org/abs/2307.11978](http://arxiv.org/abs/2307.11978)

    视觉-语言模型通过少样本提示调参的方式适应新的分类任务，且对于噪声标签具有鲁棒性。关键原因包括固定的类名标记对模型优化的正则化作用以及从多样且通用的网络数据中学习到的强大预训练图像-文本嵌入提供的先验知识。

    

    视觉-语言模型（如CLIP）通过大规模训练数据学习了通用的文本-图像嵌入。通过少样本提示调参的方式，可以使视觉-语言模型适应新的分类任务。我们发现，这种提示调参过程对于噪声标签具有很强的鲁棒性。这激发了我们研究提示调参范式鲁棒性的关键原因。我们进行了大量实验证明，关键因素包括：1）固定的类名标记对模型的优化提供了强大的正则化作用，减少了噪声样本引起的梯度；2）从多样且通用的网络数据中学习到的强大的预训练图像-文本嵌入为图像分类提供了强大的先验知识。此外，我们证明可以利用CLIP中的噪声零样本预测来调整其自身的提示，显著提高了在无监督设置下的预测准确性。代码可在https://github.com/CE找到。

    Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CE
    
[^36]: 不变风险最小化的区外优化性

    Out-of-Distribution Optimality of Invariant Risk Minimization. (arXiv:2307.11972v1 [stat.ML])

    [http://arxiv.org/abs/2307.11972](http://arxiv.org/abs/2307.11972)

    本文旨在提供IRM的理论验证，严格证明了解决方案可以最小化区外风险。

    

    深度神经网络经常继承训练数据中嵌入的虚假相关性，因此可能无法泛化到具有与提供训练数据的领域不同的未知域。M. Arjovsky等人（2019年）引入了区外（o.o.d.）风险的概念，即所有域中的最大风险，并将由虚假相关性引起的问题规定为最小化区外风险的问题。不变风险最小化（IRM）被认为是最小化区外风险的一种有前途的方法：IRM通过解决一个双层优化问题来估计最小化的区外风险。尽管IRM以实证成功吸引了相当多的关注，但它缺乏一些理论保证。特别是，还没有确立双层优化问题给出最小化区外风险的坚实理论保证。本文旨在提供IRM的理论验证，严格证明了解决方案可以通过在大仿真跟踪数据库中进行实时仿真，其包括对周围环境的直接感知，对潜在路线规划的策略认识，同时考虑到多车辆交互，以实现该问题的全局优化目标。

    Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to
    
[^37]: DHC: 双去偏异构联合训练框架用于类别不平衡的半监督医学图像分割

    DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation. (arXiv:2307.11960v1 [eess.IV])

    [http://arxiv.org/abs/2307.11960](http://arxiv.org/abs/2307.11960)

    DHC是一种用于类别不平衡的半监督医学图像分割的双去偏异构联合训练框架，通过引入分布感知和难度感知的去偏权重策略，以动态引导模型解决数据和学习偏见，显著提高了性能。

    

    对于三维医学图像的体积标记需要专业知识且耗时，因此对于有限标记数据，半监督学习（SSL）是非常理想的。不平衡的类别分布是一个严重问题，制约了这些方法在现实世界应用中的进展，但却没有得到充分解决。为了解决这个问题，我们提出了一种新颖的双去偏异构联合训练（DHC）框架，用于半监督三维医学图像分割。具体而言，我们提出了两种损失加权策略，即分布感知去偏权重（DistDW）和难度感知去偏权重（DiffDW），利用伪标签动态引导模型解决数据和学习偏见。通过联合训练这两个多样且准确的子模型，框架显著提高了性能。我们还引入了更具代表性的类别不平衡半监督医学图像分割基准，充分展示了该框架的有效性。

    The volume-wise labeling of 3D medical images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Co-training (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-
    
[^38]: 隐式解释重要性权重感知更新

    Implicit Interpretation of Importance Weight Aware Updates. (arXiv:2307.11955v1 [cs.LG])

    [http://arxiv.org/abs/2307.11955](http://arxiv.org/abs/2307.11955)

    本文首次证明了在在线学习环境中，重要性权重感知（IWA）更新对于凸优化机器学习算法具有更好的遗憾上界，优于普通梯度更新。

    

    鉴于其速度和简单性，子梯度下降是凸优化机器学习算法中最常用的优化算法之一。然而，调整其学习率可能是实现一致良好性能的最严重瓶颈。减少对学习率的依赖的常见方法是使用隐式/近端更新。其中一种变体是重要性权重感知（IWA）更新，其由每个损失函数上无限多个无穷小更新组成。然而，IWA更新的经验成功并不能完全通过其理论来解释。在本文中，我们首次展示了IWA更新在在线学习设置中具有严格更好的遗憾上界，优于普通梯度更新。我们的分析基于新框架：广义隐式Follow-the-Regularized-Leader（FTRL）（Chen和Orabona, 2023），使用对偶表述来分析广义隐式更新。特别地，我们的结果暗示了IWA更新可以被视为

    Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as
    
[^39]: 关于基于贝叶斯强化学习解决部分可观测马尔可夫决策过程的机器人的研究

    On-Robot Bayesian Reinforcement Learning for POMDPs. (arXiv:2307.11954v1 [cs.RO])

    [http://arxiv.org/abs/2307.11954](http://arxiv.org/abs/2307.11954)

    本研究提供了解决机器人学习中大量数据需求的方法，通过将专家知识捕捉并形式化为贝叶斯框架，使用基于样本的在线解决方法来推动基于贝叶斯强化学习在机器人中的应用。

    

    由于获取数据的成本较高，机器人学习往往困难重重。然而，通过有效的算法和充分利用专家对机器人动态的信息，我们可以解决大量数据的需求。基于贝叶斯强化学习（BRL）由于其样本效率和对先验知识的利用能力，在这一问题上独具优势。不幸的是，由于表达专家知识和解决后续推理问题的困难，BRL的应用受到了限制。本文通过提出一个专门针对物理系统的框架，推动了机器人中的BRL。具体而言，我们以分解表示的形式捕捉这些知识，然后展示了后验概率的分解形式，并最终在贝叶斯框架下将模型形式化。然后，我们引入了一种基于蒙特卡洛树搜索和粒子滤波的基于样本的在线解决方法，专门用于解决所得到的模型。

    Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach c
    
[^40]: HIQL: 以潜在状态作为动作的离线目标导向强化学习

    HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])

    [http://arxiv.org/abs/2307.11949](http://arxiv.org/abs/2307.11949)

    本文提出了一个基于离线数据的目标导向强化学习的分层算法，通过利用目标达成问题的结构，使用一个无动作的价值函数学习了两个策略，从而在学习过程中更有效地利用离线数据。

    

    无监督预训练最近已成为计算机视觉和自然语言处理的基石。在强化学习中，目标导向强化学习可以潜在地利用大量未标记的（无奖励）数据，提供类似于自我监督的方法。然而，构建有效的目标导向强化学习算法并直接从多样化的离线数据中进行学习是具有挑战性的，因为准确估计远期目标的价值函数很困难。然而，目标达成问题表现出一定的结构，即达到远期目标需要首先通过较近子目标。这种结构非常有用，因为评估邻近目标的动作质量通常比更远目标容易。基于这一思想，我们提出了一个基于离线数据的目标导向强化学习的分层算法。利用一个没有动作的价值函数，我们学习了两个策略，允许我们利用这种结构：一个高层策略

    Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
    
[^41]: 大学习率训练的不稳定性: 一个损失景观的视角

    The instabilities of large learning rate training: a loss landscape view. (arXiv:2307.11948v1 [cs.LG])

    [http://arxiv.org/abs/2307.11948](http://arxiv.org/abs/2307.11948)

    本论文研究了大学习率下网络训练的损失景观，并发现了梯度下降的不稳定性现象，包括景观变平和景观变移。

    

    现代神经网络无疑取得了巨大的成功。许多研究探讨了损失景观的曲率如何影响解的质量。本研究通过考虑Hessian矩阵来研究损失景观，在使用大学习率进行网络训练时，我们发现了（非）著名的不稳定性。我们对梯度下降的不稳定性进行了表征，并观察到了损失景观的变平和变移的引人注目现象，这两者都与训练的不稳定性密切相关。

    Modern neural networks are undeniably successful. Numerous works study how the curvature of loss landscapes can affect the quality of solutions. In this work we study the loss landscape by considering the Hessian matrix during network training with large learning rates - an attractive regime that is (in)famously unstable. We characterise the instabilities of gradient descent, and we observe the striking phenomena of \textit{landscape flattening} and \textit{landscape shift}, both of which are intimately connected to the instabilities of training.
    
[^42]: 通过结构化缺失数据协同学习线性模型

    Collaboratively Learning Linear Models with Structured Missing Data. (arXiv:2307.11947v1 [stat.ML])

    [http://arxiv.org/abs/2307.11947](http://arxiv.org/abs/2307.11947)

    本论文研究了协同学习线性模型的问题，提出了一种分布式、半监督的算法Collab，该算法在无法访问标记数据的情况下具有通信效率和实用性，同时在渐近局部极小极值方面也表现出优异的性能。

    

    我们研究了$m$个代理协同学习最小二乘估计的问题。每个代理观察到不同的特征子集，例如从不同分辨率的传感器收集的数据。我们的目标是确定如何协调代理以产生最佳的估计器。我们提出了一种分布式、半监督的算法Collab，包括三个步骤：本地训练、聚合和分布。我们的方法不需要通信标记数据，使其在无法访问标记数据的情况下具有通信效率和实用性。尽管存在这个障碍，我们的方法几乎是渐近局部极小极值$\unicode{x2013}$即使在允许通信标记数据的估计器中，如插补方法。我们在真实数据和合成数据上测试了我们的方法。

    We study the problem of collaboratively learning least squares estimates for $m$ agents. Each agent observes a different subset of the features$\unicode{x2013}$e.g., containing data collected from sensors of varying resolution. Our goal is to determine how to coordinate the agents in order to produce the best estimator for each agent. We propose a distributed, semi-supervised algorithm Collab, consisting of three steps: local training, aggregation, and distribution. Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible. Despite this handicap, our procedure is nearly asymptotically local minimax optimal$\unicode{x2013}$even among estimators allowed to communicate the labeled data such as imputation methods. We test our method on real and synthetic data.
    
[^43]: 差分隐私随机梯度下降的批次剪裁和自适应逐层剪裁

    Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent. (arXiv:2307.11939v1 [cs.LG])

    [http://arxiv.org/abs/2307.11939](http://arxiv.org/abs/2307.11939)

    这篇论文提出了一种用于差分隐私随机梯度下降的批次剪裁和自适应逐层剪裁方法。通过引入批次剪裁，可以解决深度神经网络无法使用批次正则化层的问题。同时，通过自适应逐层剪裁，可以根据不同层的敏感性调整剪裁常数。

    

    在差分隐私随机梯度下降（DPSGD）中，每一轮传输的剪裁梯度之和被高斯噪声混淆，用于更新全局模型，常表示为深度神经网络。由于剪裁梯度是分别计算的，我们称之为个体剪裁（IC），深度神经网络如resnet-18 无法使用批次正则化层（BNL），这是实现高准确性的深度神经网络的重要组成部分。为了利用BNL，我们引入了批次剪裁（BC），在原始的DPSGD中，我们将单个梯度进行平均和剪裁。此外，不同层的模型条目对添加的高斯噪声具有不同的敏感性。因此，引入了自适应逐层剪裁方法（ALC），其中每一层都有自己适应调整的剪裁常数，但迄今为止没有严密的差分隐私证明。在本文中，我们提出了一种新的方法

    Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18 cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\em a ne
    
[^44]: 基于岭函数角度的Mercer大规模核机器

    Mercer Large-Scale Kernel Machines from Ridge Function Perspective. (arXiv:2307.11925v1 [cs.LG])

    [http://arxiv.org/abs/2307.11925](http://arxiv.org/abs/2307.11925)

    本文从岭函数的角度介绍了Mercer大规模核机器，通过研究哪些核函数可以被余弦函数乘积逼近，解决了大规模核机器中的难题。这些结果对深度学习，特别是图像处理有潜在应用。

    

    为了从岭函数的角度介绍Mercer大规模核机器，我们回顾了Lin和Pinkus在岭函数的基本性上的结果。我们考虑了Rachimi和Recht在近似理论中的最近一篇论文的主要定理，即大规模核机器的随机特征。我们研究了哪些核函数可以被$x$和$y$的余弦函数乘积的和逼近，并提出了这种方法的障碍。本文的结果可能在深度学习中有各种应用，尤其是与图像处理相关的问题。

    To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from Fundamentality of ridge functions. We consider the main theorem of the recent paper by Rachimi and Recht, 2008, Random features for large-scale kernel machines in terms of the Approximation Theory. We study which kernels can be approximated by a sum of cosine function products with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article may have various applications in Deep Learning, especially in problems related to Image Processing.
    
[^45]: 选择性感知：利用强化学习为语言模型演员优化状态描述

    Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors. (arXiv:2307.11922v1 [cs.LG])

    [http://arxiv.org/abs/2307.11922](http://arxiv.org/abs/2307.11922)

    本研究提出了一种名为BLINDER的方法，通过学习任务条件下状态描述的值函数，自动选择简明的状态描述，以优化大型语言模型(LLM)演员在顺序决策任务中的性能和效率。

    

    大型语言模型(LLM)被应用于机器人和游戏等顺序决策任务的演员中，利用其丰富的世界知识和规划能力。然而，以往的研究很少探索通过语言向LLM演员提供什么环境状态信息。详尽描述高维状态可能会影响性能并增加LLM演员的推理成本。以前的LLM演员通过依赖手工设计的任务特定协议来确定该状态的哪些特征需要进行传递，哪些不需要。在本工作中，我们提出了一种名为BLINDER的方法，通过学习任务条件下状态描述的值函数，自动选择简明的状态描述。我们在具有挑战性的视频游戏NetHack和机器人操作任务中评估了BLINDER。我们的方法提高了任务成功率，减少了输入大小和计算成本，并且提高了生成的结果。

    Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and gen
    
[^46]: 使用多模式调查和地球观测数据预测贫困率

    Poverty rate prediction using multi-modal survey and earth observation data. (arXiv:2307.11921v1 [cs.LG])

    [http://arxiv.org/abs/2307.11921](http://arxiv.org/abs/2307.11921)

    该研究提出了一种结合多模式调查和地球观测数据的方法来预测贫困率。通过使用从卫星图像中提取的可视特征和调查问题，我们的方法在预测贫困率时表现出更低的平均误差。

    

    本研究提出了一种将家庭人口统计和生活水平调查问题与从卫星图像中提取的特征相结合的方法，以预测一个地区的贫困率。我们的方法利用从单一步骤提取特征的方法获取的可视特征，该方法应用于公开可用的10米/像素Sentinel-2地表反射卫星图像。这些可视特征与十个调查问题结合在一起，通过代理手段测试（PMT）估计一个家庭是否低于贫困线。我们展示了包含可视特征可以将贫困率估计的平均误差从4.09％降低到3.88％。除了在代理手段测试中包含卫星图像特征外，我们还提出了一种选择与从卫星图像中提取的可视特征互补的调查问题子集的方法。具体来说，我们设计了一种由全面的调查和图像指导的调查变量选择方法。

    This work presents an approach for combining household demographic and living standards survey questions with features derived from satellite imagery to predict the poverty rate of a region. Our approach utilizes visual features obtained from a single-step featurization method applied to freely available 10m/px Sentinel-2 surface reflectance satellite imagery. These visual features are combined with ten survey questions in a proxy means test (PMT) to estimate whether a household is below the poverty line. We show that the inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% over a nationally representative out-of-sample test set. In addition to including satellite imagery features in proxy means tests, we propose an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. Specifically, we design a survey variable selection approach guided by the full survey and image 
    
[^47]: 通过查询有效的黑盒攻击揭示可解释的深度学习系统中的漏洞

    Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks. (arXiv:2307.11906v1 [cs.CV])

    [http://arxiv.org/abs/2307.11906](http://arxiv.org/abs/2307.11906)

    该论文提出了一种通过查询有效的黑盒攻击方法，揭示了可解释的深度学习系统中的漏洞。这种攻击方法是基于微生物遗传算法，无需先验知识，结合了转移和评分方法，攻击成功率高且难以被检测到。

    

    深度学习已迅速应用于许多应用领域，革新了许多行业，但已知其容易受到对抗性攻击的威胁。这种攻击对基于深度学习的系统构成了严重威胁，损害了其的完整性、可靠性和信任性。可解释的深度学习系统（IDLSes）旨在使系统更加透明和可解释，但同时也容易受到攻击。在这项工作中，我们提出了一种基于微生物遗传算法的黑盒攻击方法，用于攻击IDLSes，该方法不需要对目标模型及其解释模型有任何先验知识。所提出的攻击是一种查询有效的方法，结合了基于转移和基于评分的方法，使其成为揭示IDLS漏洞的强大工具。我们的攻击实验展示了使用具有与良性样本非常相似的属性图的对抗性样本的高攻击成功率，这使得即使是人工分析人员也很难检测到。

    Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-based black-box attack against IDLSes that requires no prior knowledge of the target model and its interpretation model. The proposed attack is a query-efficient approach that combines transfer-based and score-based methods, making it a powerful tool to unveil IDLS vulnerabilities. Our experiments of the attack show high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples which makes it difficult to detect even by human analysts. 
    
[^48]: YOLOv5的模型压缩方法综述

    Model Compression Methods for YOLOv5: A Review. (arXiv:2307.11904v1 [cs.CV])

    [http://arxiv.org/abs/2307.11904](http://arxiv.org/abs/2307.11904)

    本文综述了针对YOLOv5的模型压缩方法，重点关注了网络裁剪和量化。这些方法在降低内存使用和推理时间方面取得了积极的效果。

    

    在过去几年中，人们对提升YOLO物体检测器进行了大量研究。自引入以来，已经推出了八个主要版本的YOLO，旨在提高其准确性和效率。尽管YOLO的明显优点使其在许多领域被广泛使用，但将其部署在资源有限的设备上仍面临挑战。为了解决这个问题，研究人员开发了各种神经网络压缩方法，分为网络裁剪、量化和知识蒸馏三类。利用模型压缩方法取得的成果，如降低内存使用和推理时间，使其在硬件受限的边缘设备上部署大型神经网络变得可行甚至必要。在本综述论文中，我们重点研究网络裁剪和量化方法，因为它们具有相对较好的模块性。我们将它们进行分类，并分析将这些方法应用于YOLOv5的实际结果。

    Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we ident
    
[^49]: 项目佛罗里达: 简化联邦学习

    Project Florida: Federated Learning Made Easy. (arXiv:2307.11899v1 [cs.LG])

    [http://arxiv.org/abs/2307.11899](http://arxiv.org/abs/2307.11899)

    项目佛罗里达是一个简化联邦学习的系统架构和SDK，能够在异构设备生态系统中部署大规模的联邦学习解决方案。它通过保护数据的隐私和安全性，将模型训练分散在设备和云存储之间，同时实现模型更新的集中聚合，解决了复杂的隐私和安全问题，提高了规模和性能。

    

    我们提出了项目佛罗里达，这是一个系统架构和软件开发工具包（SDK），能够在异构设备生态系统中部署大规模的联邦学习（FL）解决方案。联邦学习是一种基于数据主权原则的机器学习方法，即通过将数据存储在其原始位置（无论是在终端设备上还是在分离的云存储分区中）来实现数据的隐私和安全性。联邦学习通过将模型快照分发给在安全边界内运行的客户端，并运行客户端代码更新模型，然后在中央协调者中聚合更新的快照，从而在设备和分区之间进行模型训练，而训练数据仍保留在其安全边界内。部署联邦学习解决方案需要实现复杂的隐私和安全机制以及可扩展的协调基础设施。规模和性能是一个重要问题，因为模型训练过程的效益受到这方面的影响。

    We present Project Florida, a system architecture and software development kit (SDK) enabling deployment of large-scale Federated Learning (FL) solutions across a heterogeneous device ecosystem. Federated learning is an approach to machine learning based on a strong data sovereignty principle, i.e., that privacy and security of data is best enabled by storing it at its origin, whether on end-user devices or in segregated cloud storage silos. Federated learning enables model training across devices and silos while the training data remains within its security boundary, by distributing a model snapshot to a client running inside the boundary, running client code to update the model, and then aggregating updated snapshots across many clients in a central orchestrator. Deploying a FL solution requires implementation of complex privacy and security mechanisms as well as scalable orchestration infrastructure. Scale and performance is a paramount concern, as the model training process benefit
    
[^50]: Hindsight-DICE：稳定信用分配用于深度强化学习

    Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning. (arXiv:2307.11897v1 [cs.LG])

    [http://arxiv.org/abs/2307.11897](http://arxiv.org/abs/2307.11897)

    本研究提出了Hindsight-DICE算法，利用重要抽样比率估计技术改善了深度强化学习中的信用分配问题。

    

    在顺序决策问题中，环境往往提供很少的评估反馈来指导强化学习代理。在极端情况下，行为的长时间轨迹仅以一个终止信号标记，导致观察到非平凡奖励和触发此类反馈的个体步骤之间存在显著的时间延迟。解决这种信用分配挑战是强化学习的重要特征之一，本研究利用现有的重要抽样比率估计技术来显著改善策略梯度方法中的信用分配处理。虽然使用所谓的事后策略为观察到的轨迹返回返回数据重新加权提供了一个有原则的机制，但是简单地应用重要抽样会导致不稳定或过度滞后的学习。

    Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, engendering a significant temporal delay between the observation of non-trivial reward and the individual steps of behavior culpable for eliciting such feedback. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning and, in this work, we capitalize on existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the handling of credit assignment with policy-gradient methods. While the use of so-called hindsight policies offers a principled mechanism for reweighting on-policy data by saliency to the observed trajectory return, naively applying importance sampling results in unstable or excessively lagged learning.
    
[^51]: 关于受恶意噪声影响的公正约束学习的脆弱性

    On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])

    [http://arxiv.org/abs/2307.11892](http://arxiv.org/abs/2307.11892)

    这项研究考虑了公正约束学习对恶意噪声的脆弱性，发现使用随机分类器可以在精度上只损失$\Theta(\alpha)$和$O(\sqrt{\alpha})$，对应不同的公正约束要求。

    

    我们考虑了公正约束学习对训练数据中微小恶意噪声的脆弱性。Konstantinov和Lampert (2021)在这个问题上进行了研究，并展示了负面结果，表明在不平衡的群组大小下存在一些数据分布，任何适当的学习器都会表现出较高的脆弱性。在这里，我们展示了更乐观的观点，如果允许随机分类器，则情况更加细致。例如，对于人口统计学平等性，我们显示只会产生$\Theta(\alpha)$的精度损失，其中$\alpha$是恶意噪声率，甚至可以与没有公正约束的情况完全匹配。对于机会均等性，我们显示只会产生$O(\sqrt{\alpha})$的损失，并给出一个匹配的$\Omega(\sqrt{\alpha})$的下界。相比之下，Konstantinov和Lampert (2021)示范了对于适当的学习器，这两个概念的精度损失都是$\Omega(1)$。关键的技术创新是

    We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
    
[^52]: 关于线性递推和非线性投影的普遍性

    On the Universality of Linear Recurrences Followed by Nonlinear Projections. (arXiv:2307.11888v1 [cs.LG])

    [http://arxiv.org/abs/2307.11888](http://arxiv.org/abs/2307.11888)

    本论文展示了一种基于循环线性层和多层感知器的序列模型可以逼近任何规则的非线性序列到序列映射。

    

    在这篇注释（作为一篇全文论文的工作进展）中，我们展示了一族基于循环线性层（包括S4、S5和LRU）和位置逐元素多层感知器（MLPs）的序列模型可以很好地逼近任意规则的非线性序列到序列映射。我们的结果背后的主要思想是将循环层视为可以忠实地存储输入序列信息到内部状态的压缩算法，然后由高度表达能力的MLP进行处理。

    In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
    
[^53]: MORE: 基于测量和相关性的变分量子电路用于多分类问题

    MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification. (arXiv:2307.11875v1 [quant-ph])

    [http://arxiv.org/abs/2307.11875](http://arxiv.org/abs/2307.11875)

    本文提出了一个高效的量子多分类器MORE，通过测量和相关性来实现多分类任务。该方法利用了单个读出量子位的量子信息，而不需要复杂的后处理过程。

    

    近年来，量子计算在计算密集型任务方面显示出相当大的潜力。例如，基于量子神经网络（QNN）的分类任务引起了研究人员的极大兴趣，并在各种场景中进行了评估。然而，由于量子计算资源受限或需要大量经典后处理，目前大多数量子分类器仅适用于二分类任务。在本文中，我们提出了一个高效的量子多分类器MORE，该分类器以测量和相关性为基础构建了变分量子多分类器。MORE使用与二分类器相同的变分式设计并通过充分利用单个读出量子位的量子信息来执行多分类。为了从读出量子位提取完整信息，我们选择了三个可观测量作为二维希尔伯特空间的基。然后，我们使用量子态重构技术对量子状态进行了重建。

    Quantum computing has shown considerable promise for compute-intensive tasks in recent years. For instance, classification tasks based on quantum neural networks (QNN) have garnered significant interest from researchers and have been evaluated in various scenarios. However, the majority of quantum classifiers are currently limited to binary classification tasks due to either constrained quantum computing resources or the need for intensive classical post-processing. In this paper, we propose an efficient quantum multi-classifier called MORE, which stands for measurement and correlation based variational quantum multi-classifier. MORE adopts the same variational ansatz as binary classifiers while performing multi-classification by fully utilizing the quantum information of a single readout qubit. To extract the complete information from the readout qubit, we select three observables that form the basis of a two-dimensional Hilbert space. We then use the quantum state tomography techniqu
    
[^54]: 虚假和大规模语言模型生成的领英个人资料的潜在威胁：检测和预防的挑战与机遇

    The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention. (arXiv:2307.11864v1 [cs.SI])

    [http://arxiv.org/abs/2307.11864](http://arxiv.org/abs/2307.11864)

    本文介绍了一种在领英平台上检测虚假和大规模语言模型生成的个人资料的新方法，该方法使用领英个人资料中的文本信息，并引入“部分和子部分标签嵌入”（SSTE）方法以增强数据的特征。研究还通过收集3600个领英个人资料建立了一个公开可用的数据集。

    

    本文提出了一种新的方法，用于在领英在线社交网络注册时和建立连接之前立即检测虚假和大规模语言模型生成的个人资料。早期识别虚假资料对于维护平台的完整性至关重要，它可以防止冒名顶替者获取合法用户的私密和敏感信息，并防止他们获得增加未来钓鱼和欺诈活动可信度的机会。本研究利用领英个人资料中提供的文本信息，并引入了“部分和子部分标签嵌入”（SSTE）方法，以增强区分合法资料和冒名顶替者手动或使用大规模语言模型生成的资料的特征。此外，鉴于目前公开可用的领英数据集较少，我们为研究收集了3600个领英个人资料，并将公开发布我们的数据集供研究使用。

    In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research pur
    
[^55]: 稀疏传感器的数据引发的相互作用

    Data-Induced Interactions of Sparse Sensors. (arXiv:2307.11838v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2307.11838](http://arxiv.org/abs/2307.11838)

    本研究通过采用热力学观点，用统计物理学中的Ising模型来计算由训练数据引发的稀疏传感器之间的相互作用，从而优化传感器的空间配置和重构复杂系统的完整状态。

    

    在科学和工程中，大维度的经验数据经常具有低秩结构，并且可以表示为仅由几个特征模式的组合。由于这种结构，我们可以使用仅有少数局部化的传感器测量来重新构建复杂系统的完整状态。这种重构的质量，特别是在传感器噪声存在的情况下，显著取决于传感器的空间配置。已经提出了多种基于缺失插值和QR分解的算法来优化传感器位置。在这里，我们采用热力学观点计算由训练数据引发的传感器相互作用的完整地形。该地形采用统计物理学中的Ising模型的形式，考虑到每个传感器位置捕获的数据方差以及传感器之间的串扰。绘制出这些数据引发的传感器相互作用的图景允许

    Large-dimensional empirical data in science and engineering frequently has low-rank structure and can be represented as a combination of just a few eigenmodes. Because of this structure, we can use just a few spatially localized sensor measurements to reconstruct the full state of a complex system. The quality of this reconstruction, especially in the presence of sensor noise, depends significantly on the spatial configuration of the sensors. Multiple algorithms based on gappy interpolation and QR factorization have been proposed to optimize sensor placement. Here, instead of an algorithm that outputs a singular "optimal" sensor configuration, we take a thermodynamic view to compute the full landscape of sensor interactions induced by the training data. The landscape takes the form of the Ising model in statistical physics, and accounts for both the data variance captured at each sensor location and the crosstalk between sensors. Mapping out these data-induced sensor interactions allow
    
[^56]: PINNsFormer: 基于Transformer的物理信息神经网络框架

    PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks. (arXiv:2307.11833v1 [cs.CE])

    [http://arxiv.org/abs/2307.11833](http://arxiv.org/abs/2307.11833)

    PINNsFormer是一种基于Transformer的框架，通过捕捉时间依赖性准确逼近求解偏微分方程，相比传统方法具有更好的性能。

    

    物理信息神经网络（PINNs）已经成为一种有效的深度学习框架，用于近似求解偏微分方程（PDEs）的数值解。然而，传统的PINNs和大多数相关研究采用全连接的多层感知机（MLP）作为核心结构，忽略了PDEs中的时间关系，无法准确逼近真解。在本文中，我们提出了一种新的基于Transformer的框架，即PINNsFormer，通过Transformer-based模型中的多头注意力机制捕捉时间依赖性，准确逼近PDEs的解。PINNsFormer不仅适应输入向量以伪序列的形式进行近似预测，还将逐点的PINNs损失改为了顺序的PINNs损失。此外，PINNsFormer还配备了一种新的激活函数，即小波函数，通过深度神经网络实现对傅里叶分解的预测。我们通过实验证明了PINNsFormer捕捉时间依赖关系的能力。

    Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions for partial differential equations (PDEs). While conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. In this paper, we propose a novel Transformer-based framework, namely PINNsFormer, that accurately approximates PDEs' solutions by capturing the temporal dependencies with multi-head attention mechanisms in Transformer-based models. Instead of approximating point predictions, PINNsFormer adapts input vectors to pseudo sequences and point-wise PINNs loss to a sequential PINNs loss. In addition, PINNsFormer is equipped with a novel activation function, namely Wavelet, which anticipates the Fourier decomposition through deep neural networks. We empirically demonstrate PINNsFormer's ability to captu
    
[^57]: 以本地核归一化为机制的过度参数化卷积神经网络中的特征学习

    Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks. (arXiv:2307.11807v1 [cs.LG])

    [http://arxiv.org/abs/2307.11807](http://arxiv.org/abs/2307.11807)

    本文提出了一种以本地核归一化为机制的理论框架，解释了全连接和卷积神经网络在特征学习方面的差异。

    

    特征学习是深度神经网络自动从原始数据中学习相关特征的能力，是其解决复杂任务的优秀能力的基础。然而，在全连接或卷积架构中，特征学习似乎以不同的方式实现。经验证据显示，无限宽度极限下的全连接神经网络最终优于有限宽度的对应网络。由于描述无限宽度网络的核在训练过程中不会改变，所以在深度全连接架构中出现的任何形式的特征学习对于改善泛化没有太大帮助。另一方面，具有卷积层的最先进架构在有限宽度范围内实现了最佳性能，这表明在这种情况下出现了一种有效的特征学习形式。在这项工作中，我们提出了一个简单的理论框架，以解释这些差异，以及在一个隐藏层网络中的理由。

    Feature learning, or the ability of deep neural networks to automatically learn relevant features from raw data, underlies their exceptional capability to solve complex tasks. However, feature learning seems to be realized in different ways in fully-connected (FC) or convolutional architectures (CNNs). Empirical evidence shows that FC neural networks in the infinite-width limit eventually outperform their finite-width counterparts. Since the kernel that describes infinite-width networks does not evolve during training, whatever form of feature learning occurs in deep FC architectures is not very helpful in improving generalization. On the other hand, state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime, suggesting that an effective form of feature learning emerges in this case. In this work, we present a simple theoretical framework that provides a rationale for these differences, in one hidden layer networks. First, we show t
    
[^58]: 使用可穿戴传感器数据进行无监督嵌入学习的人体活动识别

    Unsupervised Embedding Learning for Human Activity Recognition Using Wearable Sensor Data. (arXiv:2307.11796v1 [cs.LG])

    [http://arxiv.org/abs/2307.11796](http://arxiv.org/abs/2307.11796)

    本文提出了一种无监督的方法，使用可穿戴传感器数据进行人体活动识别。该方法将人体活动投影到一个嵌入空间中，从而帮助聚类算法实现更好的性能。

    

    手机和其他可穿戴设备中的嵌入式传感器使得人体活动数据更易获取。然而，从可穿戴传感器数据中识别不同的人体活动仍然是普适计算中的一个具有挑战性的问题。其中一个原因是获取的数据大部分没有标签。本文提出了一种基于人体活动特性的无监督方法，将人体活动投影到一个嵌入空间中，相似的活动会被紧密地聚集在一起。利用这个嵌入空间，后续的聚类算法可以从中受益，形成代表一个人进行的不同活动的行为簇。在三个带有标签的基准数据集上的实验结果证明了该框架的有效性，并且显示我们的方法可以帮助聚类算法在识别和分类潜在人体活动方面实现改进的性能。

    The embedded sensors in widely used smartphones and other wearable devices make the data of human activities more accessible. However, recognizing different human activities from the wearable sensor data remains a challenging research problem in ubiquitous computing. One of the reasons is that the majority of the acquired data has no labels. In this paper, we present an unsupervised approach, which is based on the nature of human activity, to project the human activities into an embedding space in which similar activities will be located closely together. Using this, subsequent clustering algorithms can benefit from the embeddings, forming behavior clusters that represent the distinct activities performed by a person. Results of experiments on three labeled benchmark datasets demonstrate the effectiveness of the framework and show that our approach can help the clustering algorithm achieve improved performance in identifying and categorizing the underlying human activities compared to 
    
[^59]: 用语音识别能力促进大型语言模型

    Prompting Large Language Models with Speech Recognition Abilities. (arXiv:2307.11795v1 [eess.AS])

    [http://arxiv.org/abs/2307.11795](http://arxiv.org/abs/2307.11795)

    本研究通过为大型语言模型添加音频编码器，使其具备了语音识别能力。在多语言数据集上的实验证明，这样的扩展能够提高模型的性能，并且在多语言环境下实现了语音识别。通过消融研究，我们还发现可以冻结模型以保持其原有功能，并且提升音频编码器的规模有助于提高性能。

    

    大型语言模型已证明其高度灵活，能够解决各种生成任务，如概括性摘要和开放性问答。本文通过直接附加一个小型音频编码器来扩展LLM的功能，使其能够执行语音识别。通过将一系列声音嵌入直接预置到文本令牌嵌入之前，LLM可以转换为自动语音识别（ASR）系统，并且可以与其文本对应物以完全相同的方式使用。在多语言LibriSpeech（MLS）上的实验证明，将一个conformer编码器融入到开源的LLaMA-7B中，使其在单一语言基准上的表现超过18%，并能够执行多语言语音识别，尽管LLaMA的训练主要依赖于英文文本。此外，我们进行了消融研究，以调查LLM在训练过程中是否可以完全冻结以保持其原有功能，以及提升音频编码器的规模。

    Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, a
    
[^60]: 通过改进Transformer和CGAN神经网络生成人工智能产生的太赫兹多谐振超表面

    Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks. (arXiv:2307.11794v1 [physics.optics])

    [http://arxiv.org/abs/2307.11794](http://arxiv.org/abs/2307.11794)

    本文提出了改进的Transformer和CGAN神经网络用于反向设计石墨烯超表面，通过对太赫兹多谐振吸收谱进行S2V和S2I设计，实现了更高的精确度和全面性能。

    

    众所周知，利用传统的深度神经网络（DNN）进行太赫兹多谐振石墨烯超表面的反向设计具有有限的泛化能力。本文提出了改进的Transformer和条件生成对抗神经网络（CGAN）来基于太赫兹多谐振吸收谱进行石墨烯超表面的反向设计。改进的Transformer在Spectrum to Vector（S2V）设计中相比传统的多层感知器（MLP）神经网络能够获得更高的精确度和泛化性能；而通过CGAN实现的Spectrum to Image（S2I）设计可以提供比MLP获得的S2V设计更全面的信息和更高的准确度。此外，改进的CGAN还可以直接从所需的多谐振吸收谱中实现石墨烯超表面图像的反向设计。研究表明，这项工作可以促进人工智能生成的超表面设计过程。

    It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-g
    
[^61]: 利用浅层递归解码网络将任意移动传感器轨迹用于全状态重建

    Leveraging arbitrary mobile sensor trajectories with shallow recurrent decoder networks for full-state reconstruction. (arXiv:2307.11793v1 [cs.LG])

    [http://arxiv.org/abs/2307.11793](http://arxiv.org/abs/2307.11793)

    本论文提出了一种利用浅层递归解码网络和移动传感器轨迹实现全状态重建的方法。

    

    感知是监测、预测和控制复杂时空系统最基本的任务之一。在许多应用中，有限数量的传感器是移动的，并随着动态的改变位置，例如可穿戴技术、海洋监测浮标和天气气球。在这些动态系统中（没有统计独立的区域），测量时间历史编码了大量可用于关键任务的信息。大多数无模型感知范例的目标是将当前稀疏传感器测量映射到高维状态空间，完全忽略时间历史。利用现代深度学习架构，我们展示了通过使用序列到向量模型（如LSTM网络）和解码网络，动态轨迹信息可以映射到完整的状态空间估计中。实际上，我们证明通过利用浅层递归解码网络和移动传感器轨迹，我们可以实现全状态重建。

    Sensing is one of the most fundamental tasks for the monitoring, forecasting and control of complex, spatio-temporal systems. In many applications, a limited number of sensors are mobile and move with the dynamics, with examples including wearable technology, ocean monitoring buoys, and weather balloons. In these dynamic systems (without regions of statistical-independence), the measurement time history encodes a significant amount of information that can be extracted for critical tasks. Most model-free sensing paradigms aim to map current sparse sensor measurements to the high-dimensional state space, ignoring the time-history all together. Using modern deep learning architectures, we show that a sequence-to-vector model, such as an LSTM (long, short-term memory) network, with a decoder network, dynamic trajectory information can be mapped to full state-space estimates. Indeed, we demonstrate that by leveraging mobile sensor trajectories with shallow recurrent decoder networks, we can
    
[^62]: 具有交互层的量子卷积神经网络用于经典数据的分类

    Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data. (arXiv:2307.11792v1 [quant-ph])

    [http://arxiv.org/abs/2307.11792](http://arxiv.org/abs/2307.11792)

    本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。

    

    由于量子计算机具有异常的计算能力，量子机器学习（QML）引起了广泛关注。在不久的将来，几乎没有错误的量子计算机的承诺之下，对量子神经网络中多量子位相互作用的影响进行广泛研究非常重要。本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。该方法在三个公开可用的数据集MNIST、Fashion MNIST和Iris数据集上进行了测试，用于进行二元和多类别分类，并发现超越了现有最先进方法的性能。

    Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely MNIST, Fashion MNIST, and Iris datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods.
    
[^63]: LLM认知判断与人类有所不同

    LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])

    [http://arxiv.org/abs/2307.11787](http://arxiv.org/abs/2307.11787)

    这项研究调查了大型语言模型在认知任务中的表现，并发现它们的认知判断与人类不同。

    

    最近，大型语言模型(LLMs)成为研究人员、企业和消费者关注的焦点。虽然这类模型的语言能力已经得到了广泛的研究，但对它们作为认知主体的调查越来越受关注。在本研究中，我对GPT-3和ChatGPT在一个来自认知科学文献的有限数据归纳推理任务上的能力进行了研究。结果表明，这些模型的认知判断与人类不同。

    Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
    
[^64]: 智能代理的对抗对话塑造

    Adversarial Conversational Shaping for Intelligent Agents. (arXiv:2307.11785v1 [cs.CL])

    [http://arxiv.org/abs/2307.11785](http://arxiv.org/abs/2307.11785)

    本文研究了通过对抗对话塑造来增强智能对话代理的两个模型：GANPG和REGS。这些模型能够改进当前的自动拨号系统，提高聊天机器人的性能。

    

    深度学习方法的出现使得研究界在自然语言处理等多个领域取得了最先进的成果。然而，当前的自动拨号系统仍然不稳定且不准确：文本生成器和聊天机器人可能会迟钝并误解人类对话。在这项工作中，我们研究了两种模型的性能，它们通过对抗对话塑造来增强智能对话代理：使用策略梯度的生成对抗网络（GANPG）和基于Li等人提出的REGS模型的每一代生成步骤都有奖励的生成对抗网络（REGS）。该模型能够为部分和完整的生成文本序列分配奖励。我们在强化学习框架中讨论了使用不同训练细节的性能：seq2seq [36]和transformers [37]。

    The recent emergence of deep learning methods has enabled the research community to achieve state-of-the art results in several domains including natural language processing. However, the current robocall system remains unstable and inaccurate: text generator and chat-bots can be tedious and misunderstand human-like dialogue. In this work, we study the performance of two models able to enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model is able to assign rewards to both partially and fully generated text sequences. We discuss performance with different training details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning framework.
    
[^65]: 究竟什么是可实现的可证明保障的学习机制安全关键系统

    What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems. (arXiv:2307.11784v1 [cs.LG])

    [http://arxiv.org/abs/2307.11784](http://arxiv.org/abs/2307.11784)

    本文讨论了学习机制在安全关键领域的挑战，提出了一种两步验证方法来实现可证明的统计保障。

    

    机器学习取得了显著的进展，但在安全关键领域自信地利用学习机制仍然面临挑战。在这些挑战中，实现安全保证的一种严谨但实用的方法是最为突出的。本文首先讨论了设计和验证这种系统所涉及的工程和研究挑战。然后，基于对现有工作无法实现可证明保证的观察，我们提出了一种两步验证方法，以最终实现可证明的统计保障。

    Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses challenges. Among the challenges, it is known that a rigorous, yet practical, way of achieving safety guarantees is one of the most prominent. In this paper, we first discuss the engineering and research challenges associated with the design and verification of such systems. Then, based on the observation that existing works cannot actually achieve provable guarantees, we promote a two-step verification method for the ultimate achievement of provable statistical guarantees.
    
[^66]: Adam算法在非凸目标中的收敛性：放松的超参数和非遗传性情况

    Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case. (arXiv:2307.11782v1 [math.OC])

    [http://arxiv.org/abs/2307.11782](http://arxiv.org/abs/2307.11782)

    该论文研究了Adam算法在非凸目标中的收敛性，通过探索超参数设置和解决非遗传性收敛的挑战，提出了几乎确定性的遗传收敛速率，证明了非遗传性收敛优于遗传性收敛。

    

    Adam是机器学习中常用的随机优化算法。然而，其收敛性仍然没有得到完全理解，尤其是在非凸设置下。本文主要关注探索Adam的收敛性超参数设置，并解决与实际应用相关的非遗传性收敛的挑战。主要贡献总结如下：首先，我们引入了遗传性和非遗传性收敛的精确定义，涵盖了几乎所有随机优化算法收敛的形式。同时，我们强调非遗传性收敛优于遗传性收敛。其次，我们建立了Adam遗传性收敛保证的一个较弱的充分条件，允许更放松的超参数选择。在此基础上，我们实现了Adam的几乎确定性遗传收敛速度，这个速度可以任意接近$o(1/\sqrt{K})$。更重要的是，我们首次证明了las

    Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical application. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $o(1/\sqrt{K})$. More importantly, we prove, for the first time, that the las
    
[^67]: Generate的语言模型中的提取-摘要轴:测量内容"借用"

    The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative Language Models. (arXiv:2307.11779v1 [cs.CL])

    [http://arxiv.org/abs/2307.11779](http://arxiv.org/abs/2307.11779)

    该论文提出了一个名为提取-摘要轴的概念，用于衡量生成语言模型中内容的"借用"程度，并提出了开发相应度量标准、数据集和注释指南的需求。

    

    生成语言模型通过设计产生高度摘要的输出，与搜索引擎中的提取式响应形成对比。鉴于LLMs的这一特点及其对内容许可和归属的影响，我们提出了所谓的提取-摘要轴，用于基准测试生成模型，并强调开发相应的度量标准、数据集和注释指南的需要。我们将讨论限制在文本形式上。

    Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing & Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
    
[^68]: 通过估计团队实力的统计增强学习来预测手球比赛

    Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths. (arXiv:2307.11777v1 [cs.LG])

    [http://arxiv.org/abs/2307.11777](http://arxiv.org/abs/2307.11777)

    通过统计增强学习模型预测手球比赛，准确率超过80%，可以为手球队教练提供有价值的统计和预测洞察力。

    

    我们提出了一种统计增强学习模型（即 SEL 模型）来预测手球比赛。我们的机器学习模型通过 SEL 特征的增强表现超过了最先进的模型，准确率超过80%。在这项工作中，我们展示了如何构建数据集，以在过去的女子俱乐部比赛上训练机器学习模型。然后我们比较不同的模型并评估它们来评估其性能能力。最后，可解释性方法使我们能够将我们的工具从纯粹的预测解决方案转变为高度有见解的分析工具。这对于手球队的教练来说可以成为宝贵的资产，提供有价值的统计和预测洞察力，以准备未来的比赛。

    We propose a Statistically Enhanced Learning (aka. SEL) model to predict handball games. Our Machine Learning model augmented with SEL features outperforms state-of-the-art models with an accuracy beyond 80%. In this work, we show how we construct the data set to train Machine Learning models on past female club matches. We then compare different models and evaluate them to assess their performance capabilities. Finally, explainability methods allow us to change the scope of our tool from a purely predictive solution to a highly insightful analytical tool. This can become a valuable asset for handball teams' coaches providing valuable statistical and predictive insights to prepare future competitions.
    
[^69]: 捕捉社交媒体中客户见解的主题方法

    A Topical Approach to Capturing Customer Insight In Social Media. (arXiv:2307.11775v1 [cs.CL])

    [http://arxiv.org/abs/2307.11775](http://arxiv.org/abs/2307.11775)

    本研究通过嵌入狄利克雷过程，嵌入层次狄利克雷过程和面向时间的动态嵌入三种方法，解决了在嘈杂的大数据环境中完全无监督的主题提取挑战。

    

    社交媒体时代为企业带来了新的机遇。这种繁荣的信息财富超出了传统营销研究的渠道和框架，包括营销组合建模(MMM)。特别是，文本数据提出了许多数据分析从业人员必须应对的挑战。社交媒体构成了大规模、异构和嘈杂的文档来源。工业数据采集过程包括一定量的ETL。然而，数据中噪声的变异性和不同来源引入的异构性给予了临时工具的需求。换句话说，在完全无监督、嘈杂的环境中提取客户见解是一项艰巨的任务。本研究解决了在嘈杂的大数据环境中完全无监督的主题提取挑战。我们在变分自动编码器框架上提出了三种方法：嵌入狄利克雷过程、嵌入层次狄利克雷过程和面向时间的动态嵌入

    The age of social media has opened new opportunities for businesses. This flourishing wealth of information is outside traditional channels and frameworks of classical marketing research, including that of Marketing Mix Modeling (MMM). Textual data, in particular, poses many challenges that data analysis practitioners must tackle. Social media constitute massive, heterogeneous, and noisy document sources. Industrial data acquisition processes include some amount of ETL. However, the variability of noise in the data and the heterogeneity induced by different sources create the need for ad-hoc tools. Put otherwise, customer insight extraction in fully unsupervised, noisy contexts is an arduous task. This research addresses the challenge of fully unsupervised topic extraction in noisy, Big Data contexts. We present three approaches we built on the Variational Autoencoder framework: the Embedded Dirichlet Process, the Embedded Hierarchical Dirichlet Process, and the time-aware Dynamic Embe
    
[^70]: AutoAlign：基于大型语言模型的全自动有效知识图谱对齐方法

    AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])

    [http://arxiv.org/abs/2307.11772](http://arxiv.org/abs/2307.11772)

    AutoAlign是一种全自动的知识图谱对齐方法，不需要手工制作的种子对齐。它利用大型语言模型自动捕捉谓词相似性，并使用TransE计算实体嵌入来实现实体对齐。

    

    知识图谱间的实体对齐任务旨在识别出两个不同知识图谱中表示相同实体的每对实体。许多基于机器学习的方法已被提出用于这个任务。然而，据我们所知，现有的方法都需要手工制作的种子对齐，这是非常昂贵的。在本文中，我们提出了第一个名为AutoAlign的完全自动对齐方法，它不需要任何手工制作的种子对齐。具体而言，对于谓词嵌入，AutoAlign使用大型语言模型构建谓词近邻图，自动捕捉两个知识图谱中谓词的相似性。对于实体嵌入，AutoAlign首先使用TransE独立计算每个知识图谱的实体嵌入，然后通过计算基于实体属性的实体相似性，将两个知识图谱的实体嵌入移动到相同的向量空间中。因此，AutoAlign实现了谓词对齐和实体对齐。

    The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
    
[^71]: 大规模评估用于2D文本空间化的主题模型和降维方法

    Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization. (arXiv:2307.11770v1 [cs.CL])

    [http://arxiv.org/abs/2307.11770](http://arxiv.org/abs/2307.11770)

    通过大规模计算评估，我们研究了用于2D文本空间化的主题模型和降维方法的有效性，为语料库分析提供了具有高质量布局的解决方案。

    

    主题模型是一类无监督学习算法，用于检测文本语料库中的语义结构。与后续的降维算法一起，主题模型可以用于为文本语料库导出空间化的二维散点图，反映文档之间的语义相似性并支持语料库分析。尽管主题模型、降维算法及其底层超参数的选择对产生的布局有着重要影响，但目前尚不清楚哪种特定组合能够得到具有高质量的布局，准确度和感知度指标方面。为了研究主题模型和降维方法在作为二维散点图的语料库空间化（或作为景观类型可视化的基础）方面的有效性，我们提出了一个基于基准的大规模计算评估。我们的评估包括（1）一组语料库，（2）一组布局算法。

    Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that a
    
[^72]: 问题分解提高了模型生成推理的忠实度

    Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])

    [http://arxiv.org/abs/2307.11768](http://arxiv.org/abs/2307.11768)

    通过将问题分解为子问题，可以显著提高大型语言模型生成推理的忠实度。

    

    随着大型语言模型（LLM）执行越来越复杂的任务，验证其行为的正确性和安全性变得越来越困难。其中一种解决方法是要求LLM在回答问题时以逐步推理的方式外化其推理过程（思维链；CoT）。推理过程可以让我们检查模型执行任务的过程。然而，这种方法依赖于所陈述的推理能够忠实地反映模型的实际推理，而这并非总是如此。为了提高CoT推理的忠实度，我们通过将问题分解为子问题来生成推理。基于分解的方法在问答任务上取得了较好的性能，有时接近CoT，并在几个最近提出的度量标准中提高了模型所陈述推理的忠实度。通过强制模型在单独的上下文中回答简单的子问题，我们大大增加了模型的忠实度。

    As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithf
    
[^73]: 使用调取心智模型的方式来测量对可解释人工智能辅助决策的感知信任度

    Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model. (arXiv:2307.11765v1 [cs.HC])

    [http://arxiv.org/abs/2307.11765](http://arxiv.org/abs/2307.11765)

    该研究提出了一种通过调取用户的心智模型来测量可解释人工智能模型的感知信任度的方法。通过使用模糊认知图来评估解释对感知信任度的影响，从而为解释人工智能决策提供参考和改进方向。

    

    该实证研究提出了一种新的方法来测量用户对可解释人工智能模型的感知信任度。利用模糊认知图来获取用户的心智模型。首先，我们采用一个可解释的机器学习模型将疑似COVID-19患者分类为阳性或阴性。然后，医疗专家根据他们的专业知识以及可解释人工智能模型提供的预测和解释进行诊断决策任务。为了评估解释对感知信任度的影响，医疗专家通过调查对解释满意度进行评分。然后，将其作为模糊认知图的概念，以确定它们对彼此以及最终对感知信任度的影响。此外，为了考虑医疗专家的主观性，使用模糊语言变量来确定影响的强度。在模糊认知图达到稳定状态后，将获得一个量化值来衡量感知信任度。

    This empirical study proposes a novel methodology to measure users' perceived trust in an Explainable Artificial Intelligence (XAI) model. To do so, users' mental models are elicited using Fuzzy Cognitive Maps (FCMs). First, we exploit an interpretable Machine Learning (ML) model to classify suspected COVID-19 patients into positive or negative cases. Then, Medical Experts' (MEs) conduct a diagnostic decision-making task based on their knowledge and then prediction and interpretations provided by the XAI model. In order to evaluate the impact of interpretations on perceived trust, explanation satisfaction attributes are rated by MEs through a survey. Then, they are considered as FCM's concepts to determine their influences on each other and, ultimately, on the perceived trust. Moreover, to consider MEs' mental subjectivity, fuzzy linguistic variables are used to determine the strength of influences. After reaching the steady state of FCMs, a quantified value is obtained to measure the 
    
[^74]: 基于相似性的记忆增强联合实体和关系抽取

    Similarity-based Memory Enhanced Joint Entity and Relation Extraction. (arXiv:2307.11762v1 [cs.CL])

    [http://arxiv.org/abs/2307.11762](http://arxiv.org/abs/2307.11762)

    本文提出了一种基于相似性的记忆增强联合实体和关系抽取的方法，通过在任务之间建立双向内存依赖关系，从而更准确地执行文档级联合实体和关系抽取问题。实证研究表明，该方法优于现有方法，并在BioCreative V CDR语料库上达到了最先进的结果。

    

    文档级联合实体和关系抽取是一个具有挑战性的信息提取问题，需要一个统一的方法，在其中一个单一的神经网络执行四个子任务：提及检测，共指解析，实体分类和关系抽取。现有方法通常采用顺序多任务学习方法，在其中任意分解导致当前任务仅依赖于前一个任务，忽略了它们之间可能存在的更复杂关系的可能性。在本文中，我们提出了一个具有双向内存依赖关系的多任务学习框架，以解决这些缺点并更准确地执行联合问题。我们的实证研究表明，所提出的方法优于现有方法，并在BioCreative V CDR语料库上实现了最先进的结果。

    Document-level joint entity and relation extraction is a challenging information extraction problem that requires a unified approach where a single neural network performs four sub-tasks: mention detection, coreference resolution, entity classification, and relation extraction. Existing methods often utilize a sequential multi-task learning approach, in which the arbitral decomposition causes the current task to depend only on the previous one, missing the possible existence of the more complex relationships between them. In this paper, we present a multi-task learning framework with bidirectional memory-like dependency between tasks to address those drawbacks and perform the joint problem more accurately. Our empirical studies show that the proposed approach outperforms the existing methods and achieves state-of-the-art results on the BioCreative V CDR corpus.
    
[^75]: 尖锐性最小化算法不仅仅是为了更好地泛化而最小化尖锐性

    Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization. (arXiv:2307.11007v1 [cs.LG])

    [http://arxiv.org/abs/2307.11007](http://arxiv.org/abs/2307.11007)

    本文研究发现，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。我们的结果表明，尖锐性与泛化之间的关系取决于数据分布和模型架构。

    

    尽管进行了广泛的研究，但过参数化的神经网络能够泛化的基本原因仍然不明确。现有的理论表明，常见的随机优化器更倾向于训练损失更平坦的最小化器，因此自然而然的解释是平坦性意味着泛化。本文对这一解释进行了批判性的研究。通过理论和实证调查，我们发现对于两层ReLU网络存在以下三种情况：(1) 平坦性确实暗示泛化；(2) 存在最平坦的非泛化模型，尖锐性最小化算法无法泛化；(3) 更加令人惊讶的是，存在非泛化最平坦的模型，但尖锐性最小化算法仍然能够泛化。我们的研究结果表明，尖锐性与泛化之间的关系在一定程度上取决于数据分布和模型架构，尖锐性最小化算法不仅仅是为了最小化尖锐性而达到更好的泛化。

    Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize, and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve bet
    
[^76]: LLM审查：机器学习挑战还是计算机安全问题？

    LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?. (arXiv:2307.10719v1 [cs.AI])

    [http://arxiv.org/abs/2307.10719](http://arxiv.org/abs/2307.10719)

    本文讨论了大型语言模型(LLM)的审查问题，指出现有的语义审查方法存在理论上的限制，由于LLM的程序化和遵循指令的能力，语义审查可以被认为是一个不可判定的问题。同时，有知识的攻击者可以重构不可容许的输出。

    

    大型语言模型(LLM)在理解复杂指令方面展现了令人印象深刻的能力。然而，它们对提供的指令的盲目遵循引发了对恶意使用风险的担忧。现有的防御机制，如LLM的模型微调或使用LLM进行输出审查，已证明是有缺陷的，因为LLM仍然可以生成有问题的回答。常用的审查方法将这个问题视为机器学习问题，并依赖于另一个语言模型来检测LLM输出中的不良内容。在本文中，我们呈现了这种语义审查方法的理论限制。具体来说，我们证明了语义审查可以被认为是一个不可判定的问题，突出了由于LLM的程序化和遵循指令的能力而引起的审查中的固有挑战。此外，我们认为这些挑战不仅限于语义审查，因为有知识的攻击者可以重构不可容许的输出。

    Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs 
    
[^77]: TwinLiteNet：自动驾驶汽车中可驱动区域和车道分割的高效轻量模型

    TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])

    [http://arxiv.org/abs/2307.10705](http://arxiv.org/abs/2307.10705)

    本文提出了一种轻量级模型TwinLiteNet，用于自动驾驶车辆中的可驱动区域和车道分割。该模型成本低廉且高效准确，并在实验中表现出明显的计算资源节约。

    

    语义分割是自动驾驶中一个常见的任务，用于理解周围环境。对于道路上的安全和高效导航来说，可驱动区域分割和车道检测尤为重要。然而，原始的语义分割模型计算开销大，需要高端硬件，这对于嵌入式系统的自动驾驶车辆来说是不可行的。本文提出了一种轻量级的可驱动区域和车道线分割模型。TwinLiteNet设计成成本低廉，但能够实现准确和高效的分割结果。我们在BDD100K数据集上评估了TwinLiteNet，并与现代模型进行了比较。实验结果表明，我们的TwinLiteNet与现有方法表现相似，但所需的计算资源显著减少。具体而言，TwinLiteNet在可驱动区域任务上实现了91.3%的mIoU评分，在车道检测任务上实现了31.08%的IoU评分，仅使用了40万个参数，在GPU RTX上实现了415 FPS。

    Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
    
[^78]: 使用文本分类检测虚假评论

    Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])

    [http://arxiv.org/abs/2307.10617](http://arxiv.org/abs/2307.10617)

    这篇论文提出了一种使用机器学习模型的方法来识别虚假评论，并通过在餐馆评论的数据集上进行实验验证了其性能。

    

    近年来，在线评论在推广任何产品或服务方面发挥着重要作用。企业可能会嵌入虚假评论以吸引客户购买他们的产品。他们甚至可能突出强调自己产品的优点或批评竞争对手的产品。市场营销人员、广告商和其他在线商业用户有动机为他们想要推广的产品编写虚假的正面评论，或者为他们真正不喜欢的产品提供虚假的负面评论。因此，识别虚假评论是一个紧迫且持续的研究领域。本研究论文提出了一种机器学习模型方法来识别虚假评论。论文调查了在一个餐馆评论的虚假意见垃圾语料库数据集上进行的多次实验的性能。我们采用了n-gram模型和最大特征来识别虚假评论。

    In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
    
[^79]: 使用合规化动态图学习预测空中交通管制员工作负荷水平

    Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning. (arXiv:2307.10559v1 [cs.LG])

    [http://arxiv.org/abs/2307.10559](http://arxiv.org/abs/2307.10559)

    本研究提出了一种使用合规化动态图学习来预测空中交通管制员工作负荷水平的方法，通过对退休空中交通管制员进行人机交互模拟，利用空中交通数据和工作负荷标签进行预测和评估。

    

    空中交通管制是一个安全关键的服务系统，要求地面空中交通管制员（ATCo）时刻关注以维持日常航空运营。ATCo的工作负荷可能对运营安全和空域使用产生负面影响。为了避免过载并确保ATCo的可接受工作负荷水平，准确预测ATCo的工作负荷对于采取缓解措施非常重要。在本文中，我们首先对ATCo工作负荷的研究进行了回顾，主要从空中交通的角度。然后，我们简要介绍了与退休ATCo进行人机交互模拟的设置，其中获得了空中交通数据和工作负荷标签。模拟在三种菲尼克斯接近场景下进行，要求人类ATCo自我评估其工作负荷评级（即，低-1到高-7）。进行了初步的数据分析。接下来，我们提出了一个基于图的深度学习框架，结合合规化预测，来对ATCo的工作负荷进行预测。

    Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to ide
    
[^80]: 图像和声音的滥用用于在多模态LLMs中进行间接指令注入

    (Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])

    [http://arxiv.org/abs/2307.10490](http://arxiv.org/abs/2307.10490)

    本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。

    

    我们展示了如何利用图像和声音在多模态LLMs中进行间接提示和指令注入。攻击者生成与提示相对应的对抗扰动，并将其融入图像或音频录音中。当用户向（未修改的良性）模型询问被扰动的图像或音频时，扰动会引导模型输出攻击者选择的文本和/或使后续对话遵循攻击者的指令。我们用几个概念验证示例针对LLaVa和PandaGPT来说明这种攻击。

    We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
    
[^81]: 数据科学的价值取向：数据科学的本质、价值和风险

    A data science axiology: the nature, value, and risks of data science. (arXiv:2307.10460v1 [cs.AI])

    [http://arxiv.org/abs/2307.10460](http://arxiv.org/abs/2307.10460)

    这篇论文介绍了数据科学的价值取向，探讨了其特征和作用。数据科学不是一门科学，而是一种研究范式，具有广泛的应用和重大的影响，但也存在着未知的风险。这一领域仍然处于初级阶段，需要进一步的研究。

    

    数据科学不是一门科学，而是一种研究范式，具有无法预测的范围、规模、复杂性和知识发现能力，这是其他方式无法实现的，并且可能超出人类推理的能力。它已经在AI军备竞赛中广泛应用于数以万计的应用程序中，已经实质性地改变了我们的世界，但由于其不可思议的复杂性，可能带来未知的风险。本文介绍了数据科学的价值取向，探讨和评估了其显著而决定性的特征，以便了解和定义数据科学，认识到其潜在的益处、风险和开放性研究挑战。基于AI的数据科学本质上涉及不确定性，这可能比我们对科学确定性的偏好更加现实。数据科学将产生远远超出知识发现的影响。

    Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery
    
[^82]: 使用相对位置标签将异构图与实体感知自注意力相结合的阅读理解模型

    Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])

    [http://arxiv.org/abs/2307.10443](http://arxiv.org/abs/2307.10443)

    本文提出了一种新的注意力模式，使用图增强自注意力机制将从异构图中导出的推理知识整合到变压器架构中，从而克服了变压器模型在复杂推理任务中的限制。通过全局-局部注意力、图注意力和关系类型考虑，优化了实体和单词之间的注意力。该模式与相对位置标签相结合，能够与LUKE的实体感知自注意力机制相集成。

    

    尽管变压器模型在机器阅读理解任务中取得了重大进展，但由于输入序列中缺少显式知识，它们仍然面临处理复杂推理任务的限制。本文提出了一种新颖的注意力模式来克服这个限制，它利用增强图自注意力机制将由异构图导出的推理知识整合到变压器架构中。提出的注意力模式包括三个关键要素：单词标记的全局-局部注意力，对实体标记的图注意力，实体标记对相关联的标记显示强烈的注意力而对不相关的标记显示较弱的注意力，以及考虑每个实体标记与单词标记之间的关系类型。这样，如果存在关系，则可以优化两者之间的注意力。该模式与特殊的相对位置标签相结合，使其能够与LUKE的实体感知自注意力机制相集成。

    Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
    
[^83]: SentimentGPT：利用GPT进行高级情感分析及其与当前机器学习方法的差异

    SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])

    [http://arxiv.org/abs/2307.10234](http://arxiv.org/abs/2307.10234)

    本研究通过利用GPT进行高级情感分析，并考察其与当前机器学习方法的差异，发现GPT方法相较于其他模型在预测性能上具有显著优势，并有效解决了情感分析任务中的一些挑战，如理解上下文和检测讽刺。

    

    本研究对情感分析中各种生成预训练转换器（GPT）方法进行了全面的考察，特别是在SemEval 2017数据集的任务4中。采用了三种主要策略：1）使用GPT-3.5 Turbo进行提示工程，2）对GPT模型进行微调，3）采用创新的嵌入分类方法。研究结果揭示了这些策略和个别GPT模型之间的详细比较见解，展示了它们独特的优势和潜在的局限性。此外，本研究将这些基于GPT的方法与其他同时代、高性能的模型在相同数据集上进行比较。结果表明，GPT方法在预测性能方面具有显著的优势，相较于最先进技术，F1分数增加了22%以上。此外，本论文还探讨了情感分析任务中的常见挑战，如理解上下文和检测讽刺。研究强调了GPT方法的重要价值和潜力。

    This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other contemporary, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22% in F1-score compared to the state-of-the-art. Further, the paper addresses common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores
    
[^84]: 电路分析的可解释性是否具有可扩展性？来自毛丫鼠中多项选择能力的证据。

    Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])

    [http://arxiv.org/abs/2307.09458](http://arxiv.org/abs/2307.09458)

    本论文研究了电路分析在最先进的语言模型中的可扩展性，通过对70B毛丫鼠模型进行多项选择题的分析，发现现有的逻辑层归因和激活修补技术具有可扩展性，并进一步研究了注意力头的语义特征。

    

    电路分析是一种理解语言模型内部机制的有前途的技术。然而，现有的分析都是在远离最先进技术的小型模型中进行的。为了解决这个问题，我们在70B毛丫鼠模型中进行了一项案例研究，旨在测试电路分析的可扩展性。具体而言，我们研究了多项选择题，调查了毛丫鼠在知道正确答案文本的情况下是否能够识别出正确答案标签。我们发现已有的逻辑层归因、注意力模式可视化和激活修补技术在毛丫鼠模型中具有自然的可扩展性，使我们能够识别和分类一小组“输出节点”（注意力头和多层感知机）。我们进一步研究了“正确字母”类别的注意力头，旨在了解其特征的语义，结果有所不同。对于正常的多项选择问题，我们显著压缩了查询。

    \emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).  We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, ke
    
[^85]: 重新审视最小错误熵准则的鲁棒性：转移学习案例研究

    Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study. (arXiv:2307.08572v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08572](http://arxiv.org/abs/2307.08572)

    本研究重新审视了最小错误熵准则在处理非高斯噪声中的鲁棒性，并探讨了其在实际转移学习回归任务中的可行性和有用性。实验证明，在基本转移学习算法中，通过用最小错误熵代替均方误差损失，可以取得与现有方法相媲美的性能表现。

    

    应对数据分布转移是转移学习方法的重要组成部分，以便在实际任务中表现出色。然而，现有方法要么假设数据不包含噪声，要么采用复杂的训练范式或模型设计来处理数据分布转移。本文重新审视了在统计信号处理中广泛使用的最小错误熵（MEE）准则的鲁棒性，并研究其在实际转移学习回归任务中的可行性和有用性，其中分布转移是常见的。具体来说，我们提出了一个新的理论结果，展示了MEE对协变量转移的鲁棒性。我们还表明，通过简单地将均方误差（MSE）损失替换为MEE，在基本的转移学习算法（如微调和线性探测）中，我们可以获得与现有方法竞争力相当的性能。

    Coping with distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-
    
[^86]: 使用大型语言模型增强密集检索的软提示调优

    Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2307.08303](http://arxiv.org/abs/2307.08303)

    本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。

    

    密集检索（DR）将查询和文档转化为密集向量表示，并在向量空间中测量查询与文档之间的相似性。DR的一个挑战是缺乏领域特定的训练数据。虽然DR模型可以通过迁移学习从大规模公共数据集（如MS MARCO）中学习，但证据表明，并非所有DR模型和领域都能同等受益于迁移学习。最近，一些研究人员转向使用大型语言模型（LLMs）来改进零样本和少样本的DR模型。然而，这些方法中采用的硬提示或人工编写的提示无法保证生成的弱查询的质量。为了解决这个问题，我们提出了用于增强DR的软提示调优（SPTAR）：对于每个任务，我们利用软提示调优在有限的真实数据上优化任务特定的软提示，然后用这些提示引导LLMs为未标记的文档标记弱查询，从而得到足够的弱文档-查询对来训练任务特定的模型。

    Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
    
[^87]: 柔性时间事件建模：通过排名回归优化神经网络

    Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])

    [http://arxiv.org/abs/2307.08044](http://arxiv.org/abs/2307.08044)

    本研究提出了一种深度AFT排名回归模型，用于灵活地进行时间事件建模，从而改善预测性能并减轻严格的假设。

    

    时间事件分析，也被称为生存分析，旨在根据一组特征预测事件发生的时间。这个领域面临的一个主要挑战是处理被截尾的数据，这可能使学习算法更加复杂。传统方法如Cox比例风险模型和加速失效时间（AFT）模型在这个领域很受欢迎，但它们经常需要一些假设，如比例风险和线性。特别是，AFT模型通常需要预先指定的参数分布假设。为了提高预测性能和减轻严格的假设，近年来出现了许多基于深度学习的危险模型方法。然而，神经网络文献中对于AFT的表示学习尚未广泛探索，尽管相对于以危险为重点的方法而言，它更加简单和可解释。在这项工作中，我们引入了深度AFT排名回归模型来进行时间事件预测。

    Time-to-event analysis, also known as survival analysis, aims to predict the time of occurrence of an event, given a set of features. One of the major challenges in this area is dealing with censored data, which can make learning algorithms more complex. Traditional methods such as Cox's proportional hazards model and the accelerated failure time (AFT) model have been popular in this field, but they often require assumptions such as proportional hazards and linearity. In particular, the AFT models often require pre-specified parametric distributional assumptions. To improve predictive performance and alleviate strict assumptions, there have been many deep learning approaches for hazard-based models in recent years. However, representation learning for AFT has not been widely explored in the neural network literature, despite its simplicity and interpretability in comparison to hazard-focused methods. In this work, we introduce the Deep AFT Rank-regression model for Time-to-event predic
    
[^88]: 云游戏中的神经视频恢复

    Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])

    [http://arxiv.org/abs/2307.07847](http://arxiv.org/abs/2307.07847)

    本文提出了一个新方法，用于在云游戏中恢复丢失或损坏的视频帧。与传统方法不同的是，我们利用游戏状态和部分解码帧来提高恢复准确性。

    

    云游戏是一个价值数十亿美元的行业。在云游戏中，客户端将自己的移动发送到互联网上的游戏服务器，服务器将渲染并传输结果视频回来。为了提供良好的游戏体验，需要低于80毫秒的延迟。这意味着视频的渲染、编码、传输、解码和显示必须在这个时间范围内完成，由于服务器过载、网络拥塞和丢包等因素，这一点特别具有挑战性。在本文中，我们提出了一种在云游戏中恢复丢失或损坏视频帧的新方法。与传统视频帧恢复不同，我们的方法利用游戏状态显著提升恢复准确性，并利用部分解码的帧来恢复丢失的部分。我们开发了一个综合性的系统，包括(i)高效提取游戏状态，(ii)修改 H.264 视频解码器生成一个指示需要恢复视频帧哪些部分的掩码，和 (iii)设计一个新颖的神经网络进行视频帧恢复。

    Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural networ
    
[^89]: 通过跨批次度量学习实现通用嵌入

    Generalizable Embeddings with Cross-batch Metric Learning. (arXiv:2307.07620v1 [cs.LG])

    [http://arxiv.org/abs/2307.07620](http://arxiv.org/abs/2307.07620)

    通过跨批次度量学习，我们提出了一种基于可学习原型的全局平均汇聚方法，用于学习通用实体以表示未见过的类别。

    

    全局平均汇聚（GAP）是深度度量学习（DML）中常用的组件，用于聚合特征。其有效性通常归因于将每个特征向量视为独立的语义实体，并将GAP视为它们的组合。尽管经过证实，但这种解释在学习可用于表示未见过的类别的通用实体的算法意义上仍不清楚，这是DML的关键目标。为了解决这个问题，我们将GAP定义为可学习原型的凸组合。然后，我们展示了原型学习可以被表达为将线性预测器拟合到一批样本的递归过程。基于这个观点，我们在每次迭代中考虑两个不相交类别的批次，并通过使用适应于另一个批次的原型来规范化学习。我们在4个热门DML基准上验证了我们的方法。

    Global average pooling (GAP) is a popular component in deep metric learning (DML) for aggregating features. Its effectiveness is often attributed to treating each feature vector as a distinct semantic entity and GAP as a combination of them. Albeit substantiated, such an explanation's algorithmic implications to learn generalizable entities to represent unseen classes, a crucial DML goal, remain unclear. To address this, we formulate GAP as a convex combination of learnable prototypes. We then show that the prototype learning can be expressed as a recursive process fitting a linear predictor to a batch of samples. Building on that perspective, we consider two batches of disjoint classes at each iteration and regularize the learning by expressing the samples of a batch with the prototypes that are fitted to the other batch. We validate our approach on 4 popular DML benchmarks.
    
[^90]: 张量分解与控制理论的结合：学习线性动力系统的混合模型

    Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems. (arXiv:2307.06538v1 [cs.LG])

    [http://arxiv.org/abs/2307.06538](http://arxiv.org/abs/2307.06538)

    本论文以张量分解方法为基础，提出了学习线性动力系统混合模型的新方法。算法成功地应用于没有组件分离条件的情况，并可以与贝叶斯最优聚类竞争。此外，算法可以在部分观测设置下工作。

    

    最近，Chen和Poor开始研究学习线性动力系统的混合模型。虽然线性动力系统已经在建模时间序列数据方面有广泛的应用，但使用混合模型可以带来更好的拟合或者对数据中表示的基础子群体有更丰富的理解。在这项工作中，我们提出了一种基于张量分解的学习线性动力系统混合模型的新方法。因此，我们的算法在组件无强分离条件的情况下成功，并可以用于与轨迹的贝叶斯最优聚类竞争。此外，我们的算法适用于具有挑战性的部分观测设置。我们的起点是简单但强大的观察，即经典的何-卡尔曼算法是学习潜变量模型的现代张量分解方法的近亲。这为我们提供了一个扩展到更复杂的生成模型的操作指南。

    Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems. While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data. In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions. As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories. Moreover our algorithm works in the challenging partially-observed setting. Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a close relative of modern tensor decomposition methods for learning latent variable models. This gives us a playbook for how to extend it to work with more complicated generative models.
    
[^91]: 用于计算成像中多样化和通用大规模重建的本地条件神经场

    Local Conditional Neural Fields for Versatile and Generalizable Large-Scale Reconstructions in Computational Imaging. (arXiv:2307.06207v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.06207](http://arxiv.org/abs/2307.06207)

    本文引入了一种新颖的本地条件神经场（LCNF）框架，利用连续的隐式神经表示来解决传统像素表示在捕捉对象连续、多尺度细节方面的限制。LCNF在多尺度信息重建中展现了出色的能力，并成功解决了傅立叶相位显微镜中的逆问题，实现了大规模相位恢复。与传统神经场框架不同的是，LCNF结合了本地条件表示，促进了模型泛化、学习多尺度信息以及高效处理大规模成像数据的能力。

    

    深度学习已经改变了计算成像，但传统的基于像素的表示限制了它们捕捉对象的连续、多尺度细节的能力。在这里，我们引入了一种新颖的本地条件神经场（LCNF）框架，利用连续的隐式神经表示来解决这个限制。LCNF实现了灵活的对象表示，并促进了多尺度信息的重建。我们在傅立叶相位显微镜（FPM）中展示了LCNF的能力，通过多路复用测量解决了高度病态的逆问题，并实现了强大、可扩展和通用的大规模相位恢复。与传统的神经场框架不同，LCNF集成了一个促进模型泛化、学习多尺度信息和高效处理大规模成像数据的本地条件表示。通过将编码器和解码器与学习的潜在向量相关联，LCNF实现了多功能连续成像重建。

    Deep learning has transformed computational imaging, but traditional pixel-based representations limit their ability to capture continuous, multiscale details of objects. Here we introduce a novel Local Conditional Neural Fields (LCNF) framework, leveraging a continuous implicit neural representation to address this limitation. LCNF enables flexible object representation and facilitates the reconstruction of multiscale information. We demonstrate the capabilities of LCNF in solving the highly ill-posed inverse problem in Fourier ptychographic microscopy (FPM) with multiplexed measurements, achieving robust, scalable, and generalizable large-scale phase retrieval. Unlike traditional neural fields frameworks, LCNF incorporates a local conditional representation that promotes model generalization, learning multiscale information, and efficient processing of large-scale imaging data. By combining an encoder and a decoder conditioned on a learned latent vector, LCNF achieves versatile conti
    
[^92]: NetGPT: 超越提供个性化生成服务的本地AI网络架构

    NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])

    [http://arxiv.org/abs/2307.06148](http://arxiv.org/abs/2307.06148)

    NetGPT是一个能够在边缘和云端部署适当的大型语言模型的本地AI网络架构，实现了个性化生成服务，并通过协作云边方法论来优化资源协调和互动效果。

    

    大型语言模型（LLMs）通过生成信息在日常生活中取得了巨大成功，LLMs的个性化可能进一步促进它们在应用中的作用，因为它们能更好地与人类意图对齐。针对个性化生成服务，协作云边方法论听起来很有前景，因为它有助于有效协调异构分布式通信和计算资源。在本文中，我们讨论了几种候选的云边协作技术的利弊，提出了NetGPT，根据其计算能力在边缘和云端部署适当的LLMs。此外，边缘LLMs可以高效利用基于位置的信息进行个性化提示完成，从而有益于与云端LLMs的互动。在边缘和云端部署代表性的开源LLMs（例如GPT-2-base和LLaMA模型）之后，我们展示了NetGPT的可行性。

    Large language models (LLMs) have triggered tremendous success to empower daily life by generative information, and the personalization of LLMs could further contribute to their applications due to better alignment with human intents. Towards personalized generative services, a collaborative cloud-edge methodology sounds promising, as it facilitates the effective orchestration of heterogeneous distributed communication and computing resources. In this article, after discussing the pros and cons of several candidate cloud-edge collaboration techniques, we put forward NetGPT to capably deploy appropriate LLMs at the edge and the cloud in accordance with their computing capacity. In addition, edge LLMs could efficiently leverage location-based information for personalized prompt completion, thus benefiting the interaction with cloud LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on th
    
[^93]: 选择好对手：如何指导程序化策略的合成

    Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies. (arXiv:2307.04893v1 [cs.LG])

    [http://arxiv.org/abs/2307.04893](http://arxiv.org/abs/2307.04893)

    这篇论文介绍了一种名为2L的算法，该算法能够提供引导合成程序化策略的参考策略，通过在实验中的表现和在MicroRTS锦标赛中的胜利，证明了2L算法相对于其他学习算法的优势。

    

    本论文介绍了一种名为Local Learner (2L)的算法，用于提供一组参考策略，以指导在两人零和博弈中搜索程序化策略。之前的学习算法，如迭代最佳响应算法(IBR)，虚构游戏算法(FP)和双正交算法(DO)，计算复杂度较高或会漏掉指导搜索算法的重要信息。2L主动选择一组参考策略以提高搜索信号。我们通过在三个游戏中引导局部搜索算法来实证我们的方法优势，其中包括MicroRTS，一个具有挑战性的实时战略游戏。结果表明，2L学习到的参考策略提供了比IBR，FP和DO更强的搜索信号。我们还模拟了一场MicroRTS锦标赛，其中使用2L合成器的表现超过了两个最新MicroRTS比赛的胜者，这些胜者均为人类编程员编写的程序化策略。

    This paper introduces Local Learner (2L), an algorithm for providing a set of reference strategies to guide the search for programmatic strategies in two-player zero-sum games. Previous learning algorithms, such as Iterated Best Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be computationally expensive or miss important information for guiding search algorithms. 2L actively selects a set of reference strategies to improve the search signal. We empirically demonstrate the advantages of our approach while guiding a local search algorithm for synthesizing strategies in three games, including MicroRTS, a challenging real-time strategy game. Results show that 2L learns reference strategies that provide a stronger search signal than IBR, FP, and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L outperformed the winners of the two latest MicroRTS competitions, which were programmatic strategies written by human programmers.
    
[^94]: CPDG: 一种用于动态图神经网络的对比式预训练方法

    CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks. (arXiv:2307.02813v1 [cs.LG])

    [http://arxiv.org/abs/2307.02813](http://arxiv.org/abs/2307.02813)

    这篇论文提出了一种对比式预训练方法（CPDG）用于动态图神经网络（DGNNs），通过灵活的结构-时序子图采样和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战，实验证明CPDG在各种下游任务中的动态图预训练方面优于现有方法。

    

    近年来，由于动态图中蕴含丰富信息并在实际场景中得到广泛应用，动态图数据挖掘变得越来越流行。尽管动态图神经网络（DGNNs）取得了一定的进展，但其丰富的信息和多样的下游任务给在工业情景中实际应用带来了显著困难。因此，在本文中，我们提出了对比式预训练方法（CPDG）来解决这些问题。CPDG通过一种灵活的结构-时序子图采样器和结构-时序对比式预训练方案，解决了DGNNs预训练中的泛化和长短期建模能力等挑战。在大规模的研究和工业动态图数据集上进行的大量实验表明，CPDG在各种下游任务中的动态图预训练方面优于现有方法。

    Dynamic graph data mining has gained popularity in recent years due to the rich information contained in dynamic graphs and their widespread use in the real world. Despite the advances in dynamic graph neural networks (DGNNs), the rich information and diverse downstream tasks have posed significant difficulties for the practical application of DGNNs in industrial scenarios. To this end, in this paper, we propose to address them by pre-training and present the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG). CPDG tackles the challenges of pre-training for DGNNs, including generalization and long-short term modeling capability, through a flexible structural-temporal subgraph sampler along with structural-temporal contrastive pre-training schemes. Extensive experiments conducted on both large-scale research and industrial dynamic graph datasets show that CPDG outperforms existing methods in dynamic graph pre-training for various downstream tasks under three transf
    
[^95]: 在观测代价敏感强化学习中的动态观测策略

    Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])

    [http://arxiv.org/abs/2307.02620](http://arxiv.org/abs/2307.02620)

    本文研究了在观测代价敏感强化学习中，强化学习代理在每个时间步不需要昂贵的测量，提出了一种新的方法DMSOA，并在多个环境中进行了评估，结果表明DMSOA能够以更少的决策步骤和测量次数学到更好的策略。

    

    强化学习已被证明可以学习复杂任务的高级控制策略，包括游戏、机器人、供暖与制冷系统和文本生成。然而，强化学习中的动作-感知循环通常假设在每个时间步都可以获得对环境状态的测量，且不产生成本。然而，在深海和行星机器人探索、材料设计和医学等应用中，测量或者近似环境状态可能会产生高昂的成本。本文调查了近来不断增长的文献，采取了RL代理可能不需要或者不想在每个时间步进行昂贵测量的观点。在这个背景下，我们提出了Deep Dynamic Multi-Step Observationless Agent (DMSOA)，并将其与文献进行对比，并在OpenAI gym和Atari Pong环境中进行了实证评估。我们的结果显示，DMSOA能够以更少的决策步骤和测量次数学到更好的策略。

    Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
    
[^96]: 在流形结构化数据上的线性回归：外在几何对解的影响

    Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions. (arXiv:2307.02478v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02478](http://arxiv.org/abs/2307.02478)

    本文研究了应用于流形结构化数据上的线性回归，揭示了外在几何对回归解唯一性以及对超出分布推断稳定性的影响。

    

    本文研究了应用于流形结构化数据上的线性回归。我们假设数据流形是光滑的，并嵌入在一个欧氏空间中，我们的目标是揭示数据流形的外在几何对回归的影响。具体而言，我们分析了流形的曲率（或当曲率局部为零时的高阶非线性参数化）对回归解唯一性的影响。我们的研究发现，当嵌入的子流形在某些维度上是平坦的时，相应的线性回归解不是唯一的。否则，流形的曲率（或嵌入中的高阶非线性度量）可能在与流形的法方向相关的解中有显著贡献。因此，我们的研究揭示了数据流形几何在确保回归模型对于超出分布的推断的稳定性方面的作用。

    In this paper, we study linear regression applied to data structured on a manifold. We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression. Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution. Our findings suggest that the corresponding linear regression does not have a unique solution when the embedded submanifold is flat in some dimensions. Otherwise, the manifold's curvature (or higher order nonlinearity in the embedding) may contribute significantly, particularly in the solution associated with the normal directions of the manifold. Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.
    
[^97]: Nexus sine qua non：基于节点识别的神经网络连接的时空预测多变量时间序列

    Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])

    [http://arxiv.org/abs/2307.01482](http://arxiv.org/abs/2307.01482)

    提出了一种紧凑的神经网络预测模型，通过节点识别、密集编码器-解码器和消息传递层来实现，不需要复杂的顺序模块。该模型在实证评估中表现出了有效性和高效性。

    

    建模和预测多变量时间序列不仅有助于从业者的决策，还加深我们对底层动态系统的科学理解。时空图神经网络（STGNNs）已经成为强大的预测器，并成为学习时空表示的事实标准模型。然而，现有的STGNNs的架构往往通过堆叠一系列复杂的层次而变得复杂。设计的模型可能多余或难以理解，这给复杂性和可扩展性带来了巨大挑战。这些问题促使我们重新审视现代STGNNs的设计，并确定对强大和高效的神经预测器有所贡献的核心原则。在这里，我们提出了一个紧凑的预测模型，完全由密集编码器-解码器和消息传递层来定义，基于节点识别，没有任何复杂的顺序模块，例如TCNs，RNNs和Transformers。通过实证重新评估该模型的性能，我们证明了该模型的有效性和高效性。

    Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
    
[^98]: ManimML：用动画演示机器学习架构

    ManimML: Communicating Machine Learning Architectures with Animation. (arXiv:2306.17108v1 [cs.LG])

    [http://arxiv.org/abs/2306.17108](http://arxiv.org/abs/2306.17108)

    ManimML是一个开源Python库，通过动画演示自动生成的ML算法，为机器学习从业者提供了一种简单且熟悉的方式来沟通和可视化ML算法。

    

    近年来，由于机器学习在科学和工程领域的应用，对机器学习（ML）的兴趣激增。然而，随着ML技术的发展，解释和可视化新颖的ML算法的工具还远远落后。动画已被证明是一种强大的工具，可以制作出随时间动态变化的系统的吸引人可视化效果，非常适合用于沟通ML算法的任务。然而，目前动画化ML算法的方法是手工制作突出特定算法或使用复杂的通用动画软件。我们开发了ManimML，这是一个开源的Python库，可以直接从代码中轻松生成ML算法的动画。我们旨在利用ML从业者对编程的现有知识，而不是要求他们学习复杂的动画软件。ManimML具有熟悉的语法，用于指定模仿流行的深度学习框架如Pytorch的神经网络。

    There has been an explosion in interest in machine learning (ML) in recent years due to its applications to science and engineering. However, as ML techniques have advanced, tools for explaining and visualizing novel ML algorithms have lagged behind. Animation has been shown to be a powerful tool for making engaging visualizations of systems that dynamically change over time, which makes it well suited to the task of communicating ML algorithms. However, the current approach to animating ML algorithms is to handcraft applications that highlight specific algorithms or use complex generalized animation software. We developed ManimML, an open-source Python library for easily generating animations of ML algorithms directly from code. We sought to leverage ML practitioners' preexisting knowledge of programming rather than requiring them to learn complex animation software. ManimML has a familiar syntax for specifying neural networks that mimics popular deep learning frameworks like Pytorch.
    
[^99]: 深度展开模拟的双分岔用于大规模MIMO信号检测

    Deep Unfolded Simulated Bifurcation for Massive MIMO Signal Detection. (arXiv:2306.16264v1 [cs.IT])

    [http://arxiv.org/abs/2306.16264](http://arxiv.org/abs/2306.16264)

    本文提出了一种基于量子启发式算法的模拟双分岔用于大规模MIMO信号检测的方法，并通过修改算法和使用深度展开技术，显著改善了检测性能。

    

    多输入多输出（MIMO）是下一代无线通信的关键组成部分。最近，基于深度学习技术和量子（启发式）算法的各种MIMO信号检测器已被提出，以改善与传统检测器相比的检测性能。本文关注量子启发式算法中的模拟双分岔（SB）算法，并提出了两种技术来改善其检测性能。第一种是修改受Levenberg-Marquardt算法启发的算法，以消除最大似然检测的局部最小值。第二种是使用深度展开，一种深度学习技术，来训练迭代算法的内部参数。我们提出了一种深度展开的SB，通过使SB的更新规则可微分。数值结果表明，这些提出的检测器显著改善了大规模MIMO系统的信号检测性能。

    Multiple-input multiple-output (MIMO) is a key ingredient of next-generation wireless communications. Recently, various MIMO signal detectors based on deep learning techniques and quantum(-inspired) algorithms have been proposed to improve the detection performance compared with conventional detectors. This paper focuses on the simulated bifurcation (SB) algorithm, a quantum-inspired algorithm. This paper proposes two techniques to improve its detection performance. The first is modifying the algorithm inspired by the Levenberg-Marquardt algorithm to eliminate local minima of maximum likelihood detection. The second is the use of deep unfolding, a deep learning technique to train the internal parameters of an iterative algorithm. We propose a deep-unfolded SB by making the update rule of SB differentiable. The numerical results show that these proposed detectors significantly improve the signal detection performance in massive MIMO systems.
    
[^100]: 定义数据科学：一种新的研究范式

    Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])

    [http://arxiv.org/abs/2306.16177](http://arxiv.org/abs/2306.16177)

    数据科学是一种新的研究范式，具有潜力和应用广泛性，在40多个学科、数百个研究领域和成千上万个应用中出现。然而，由于其起步阶段，目前存在许多定义的冗余和不一致性的问题。

    

    数据科学不是一门科学，而是一种研究范式。它的力量、范围和规模将超越科学，成为促使知识发现并改变世界的重要手段。我们尚未理解和定义它，这对于实现其潜力和管理其风险至关重要。现代数据科学处于起步阶段。自1962年以来缓慢发展，并且自2000年以来发展迅速，它是一种根本性的新的研究领域，是21世纪最活跃、最强大和发展最快的创新之一。由于其价值、力量和适用性，它正在40多个学科、数百个研究领域和成千上万个应用中出现。数以百万计的数据科学出版物中包含了无数关于数据科学和数据科学问题解决的定义。由于其起步阶段，许多定义是独立的、应用特定的、相互不完整的、冗余的或不一致的，因此数据科学也是如此。本研究通过提出解决数据科学多重定义挑战的方法来解决这个问题。

    Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
    
[^101]: 南佛罗里达州水位预测的深度学习模型

    Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])

    [http://arxiv.org/abs/2306.15907](http://arxiv.org/abs/2306.15907)

    本论文利用深度学习模型训练代理模型，快速预测南佛罗里达州迈阿密河下游的水位，并与基于物理的模型进行比较。

    

    模拟和预测河流系统的水位对于洪水警报、水力操作和洪水减轻至关重要。在工程领域中，使用HEC-RAS、MIKE和SWMM等工具建立详细的基于物理的水文和水力计算模型来模拟整个流域，从而预测系统中任意点的水位。然而，这些基于物理的模型计算量大，尤其对于大流域和长时间模拟来说。为了解决这个问题，我们训练了几个深度学习（DL）模型作为代理模型，快速预测水位。本文以南佛罗里达州迈阿密河的下游水位为案例研究。数据集来自南佛罗里达水管理区（SFWMD）的DBHYDRO数据库，时间跨度为2010年1月1日至2020年12月31日。广泛的实验表明，DL模型的性能与基于物理的模型相当。

    Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
    
[^102]: 扩大分子动力学模拟器探索空间的控制以通过生成控制策略简化De Novo分析

    Augmenting Control over Exploration Space in Molecular Dynamics Simulators to Streamline De Novo Analysis through Generative Control Policies. (arXiv:2306.14705v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14705](http://arxiv.org/abs/2306.14705)

    本研究引入了P5模型，利用强化学习增强了分子动力学模拟的控制能力。通过优化目标聚合物链构象的采样，我们实现了超过37.1%的效率提升。强化学习产生的控制策略扩展了传统分子动力学模拟的配置空间的探索范围，生成了更多样化的构象集合，并针对特定特性进行目标化。这一技术在研究新系统时具有显著优势，并为复杂模拟问题的解决提供了新的方法学。

    

    本研究引入了P5模型，该模型利用强化学习来增强分子动力学模拟中的控制、效率和可扩展性。我们的创新策略优化了目标聚合物链构象的采样，使效率提高了37.1%以上。强化学习引导的控制策略作为归纳偏好，调节布朗力以将系统引导到优选状态，从而扩展了传统分子动力学所允许的配置空间的探索范围。这种扩大的探索生成了更多样化的构象集合，并针对特定特性进行目标化，这对于聚合物开发、药物发现和材料设计的进展至关重要。我们的技术在研究具有有限先验知识的新系统时具有显著优势，为利用生成技术解决复杂的模拟问题开辟了新的方法学。

    This study introduces the P5 model - a foundational method that utilizes reinforcement learning (RL) to augment control, effectiveness, and scalability in molecular dynamics simulations (MD). Our innovative strategy optimizes the sampling of target polymer chain conformations, marking an efficiency improvement of over 37.1%. The RL-induced control policies function as an inductive bias, modulating Brownian forces to steer the system towards the preferred state, thereby expanding the exploration of the configuration space beyond what traditional MD allows. This broadened exploration generates a more varied set of conformations and targets specific properties, a feature pivotal for progress in polymer development, drug discovery, and material design. Our technique offers significant advantages when investigating new systems with limited prior knowledge, opening up new methodologies for tackling complex simulation problems with generative techniques.
    
[^103]: 使用专家观察数据进行像素学习

    Learning from Pixels with Expert Observations. (arXiv:2306.13872v2 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2306.13872](http://arxiv.org/abs/2306.13872)

    本文提出了一种使用专家观察数据的新方法，在像素观测中进行稀疏奖励的机器人操作任务学习。通过使用专家观察数据作为目标条件，该方法能显著改进两种最先进的智能体的性能，同时训练过程中所需的专家行动次数减少了4-20倍，并且优于分层基线模型。

    

    在强化学习中，稀疏奖励可能是一个重要的挑战。幸运的是，可以利用专家行为来克服这个问题。然而，获取明确的专家行动可能是昂贵的，而专家观察数据通常更容易获得。本文提出了一种新的方法，利用专家观察数据在稀疏奖励的像素观测中进行机器人操作任务的学习。具体而言，我们的技术将专家观察数据作为目标条件的强化学习智能体的中间视觉目标，使其通过连续达到一系列目标来完成任务。我们在仿真中展示了我们方法在五个具有挑战性的块堆叠任务中的有效性，并且实验证明，当与两种最先进的智能体相结合时，我们的方法可以显著提高它们的性能，同时在训练过程中需要的专家行动次数减少了4-20倍。此外，我们的方法也优于一个分层的基线模型。

    In reinforcement learning (RL), sparse rewards can present a significant challenge. Fortunately, expert actions can be utilized to overcome this issue. However, acquiring explicit expert actions can be costly, and expert observations are often more readily available. This paper presents a new approach that uses expert observations for learning in robot manipulation tasks with sparse rewards from pixel observations. Specifically, our technique involves using expert observations as intermediate visual goals for a goal-conditioned RL agent, enabling it to complete a task by successively reaching a series of goals. We demonstrate the efficacy of our method in five challenging block construction tasks in simulation and show that when combined with two state-of-the-art agents, our approach can significantly improve their performance while requiring 4-20 times fewer expert actions during training. Moreover, our method is also superior to a hierarchical baseline.
    
[^104]: FedSelect: 个性化联邦学习中参数自定义选择的细调方法

    FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])

    [http://arxiv.org/abs/2306.13264](http://arxiv.org/abs/2306.13264)

    本文提出了一种名为FedSelect的新联邦学习框架，通过寻找最佳客户端子网络从而直接个性化客户端子网络结构和参数，同时保留了全局知识，提高了客户端性能。

    

    联邦学习旨在通过在本地数据上微调客户端参数或针对本地任务个性化架构来提高客户端性能。然而，现有的方法要么在牺牲重要的全局知识的情况下进行个性化，要么在预先确定网络层以进行微调的情况下导致客户端模型中全局知识储存的不足。本文提出了一种新的联邦学习框架FedSelect，通过同时搜索并获得个性化最佳参数和用于全局聚合的其余参数，从而直接个性化客户子网络结构和参数。

    Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
    
[^105]: 利用等变图神经网络预测蛋白质变体

    Predicting protein variants with equivariant graph neural networks. (arXiv:2306.12231v1 [cs.LG])

    [http://arxiv.org/abs/2306.12231](http://arxiv.org/abs/2306.12231)

    本文比较了等变图神经网络和基于序列的方法在预测蛋白质变体方面的能力，并发现我们提出的结构方法在训练更少的分子的情况下可以达到与基于序列的方法相竞争的性能。

    

    预训练模型在许多蛋白质工程任务中取得了成功。其中，基于序列的模型在蛋白质适应性预测方面取得了最先进的性能，而基于结构的模型则已被实验性地用于开发具有增强功能的蛋白质。然而，在预测优于野生型蛋白质的蛋白质变体方面，基于结构和基于序列的方法比较这一研究领域存在研究空白。本文旨在通过比较等变图神经网络（EGNN）和基于序列的方法识别有前途的氨基酸突变的能力，来解决这一空白。结果表明，我们提出的结构方法在训练更少的分子的情况下，可以达到与基于序列的方法相竞争的性能。此外，我们发现将测定标记数据与结构预训练模型相结合，与序列预训练模型的效果类似。

    Pre-trained models have been successful in many protein engineering tasks. Most notably, sequence-based models have achieved state-of-the-art performance on protein fitness prediction while structure-based models have been used experimentally to develop proteins with enhanced functions. However, there is a research gap in comparing structure- and sequence-based methods for predicting protein variants that are better than the wildtype protein. This paper aims to address this gap by conducting a comparative study between the abilities of equivariant graph neural networks (EGNNs) and sequence-based approaches to identify promising amino-acid mutations. The results show that our proposed structural approach achieves a competitive performance to sequence-based methods while being trained on significantly fewer molecules. Additionally, we find that combining assay labelled data with structure pre-trained models yields similar trends as with sequence pre-trained models.
    
[^106]: 转换器训练策略用于预测多个负载时间序列

    Transformer Training Strategies for Forecasting Multiple Load Time Series. (arXiv:2306.10891v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10891](http://arxiv.org/abs/2306.10891)

    转换器模型在预测多负载时间序列方面使用全局训练策略比多变量和本地训练策略具有更好的性能，平均降低了21.8%和12.8%的预测误差。

    

    在未来的智能电网中，准确的负载预测可以帮助在本地平衡供需，并防止电网故障。尽管被监测的客户数量将随着不断推进的智能电表安装而增加，但每个客户的数据量始终是有限的。我们评估了转换器负载预测模型是否受益于转移学习策略，即在多个客户的负载时间序列上训练全局的单变量模型。在使用两个包含数百个客户的数据集进行的实验中，我们发现全局训练策略优于相关工作中使用的多变量和本地训练策略。平均而言，与其他两种策略相比，全局训练策略在从未来一天到一个月的预测时间范围内，预测误差降低了21.8%和12.8%。与线性模型、多层感知机和LSTM模型的比较显示，转换器训练策略效果更好。

    In the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that T
    
[^107]: 自监督学习在时间序列分析中的应用：分类、进展和前景

    Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10125](http://arxiv.org/abs/2306.10125)

    自监督学习（SSL）在时间序列分析中的应用取得了显著性能，通过减少对标注数据的依赖，即使只有少量标注数据，也能实现高性能。

    

    自监督学习（SSL）最近在各种时间序列任务上取得了令人瞩目的性能。SSL最突出的优势是减少对标注数据的依赖。基于预训练和微调策略，即使只有少量标注数据，也可以实现高性能。与许多关于计算机视觉和自然语言处理的自监督学习综述相比，目前还缺乏针对时间序列SSL的综述。为了填补这一空白，本文回顾了当前时间序列数据中的自监督学习（SSL）方法的最新研究进展。为此，我们首先全面回顾了与自监督学习（SSL）和时间序列相关的现有综述，然后通过总结从生成型、对比型和对抗型三个角度对现有时间序列自监督学习方法进行了新的分类。这些方法进一步细分为十个子类，详细回顾和讨论了它们的关键直觉、主要框架、优势和限制。

    Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
    
[^108]: CLIPSonic：使用未标注视频和预训练语言视觉模型的文本合成音频

    CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models. (arXiv:2306.09635v1 [cs.SD])

    [http://arxiv.org/abs/2306.09635](http://arxiv.org/abs/2306.09635)

    本文提出了一种使用未标注视频和预训练语言视觉模型进行文本合成音频的方法。通过利用视觉模态作为桥梁来学习文本-音频对应关系，提出了有条件的扩散模型，生成视频的音轨。使用CLIP图像查询条件进行零样本模态转换。并采用预训练的扩散先验模型，生成对应于CLIP文本嵌入的CLIP图像嵌入。实验证明了该方法的有效性。

    

    最近的研究探讨了使用大量成对的文本和音频数据进行文本合成音频。 然而，带有高质量文本注释的音频录音可能难以获取。 在本文中，我们使用未标记的视频和预训练语言视觉模型来进行文本合成音频的研究。 我们建议利用视觉模态作为桥梁来学习所需的文本-音频对应关系。 我们训练一个有条件的扩散模型，以生成视频的音频轨道，给定由预训练的对比语言图像预训练（CLIP）模型编码的视频帧。 在测试时间，我们首先探索执行零样本模态转换，并使用CLIP编码的文本查询条件扩散模型。 但是，我们观察到与图像查询相比存在明显的性能下降。 为了弥合这一差距，我们进一步采用预训练的扩散先验模型，生成给定CLIP文本嵌入的CLIP图像嵌入。 我们的结果显示了所提出方法的有效性，并且

    Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that 
    
[^109]: 一项关于一些基于密度的聚类技术的调研

    A Survey of Some Density Based Clustering Techniques. (arXiv:2306.09256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09256](http://arxiv.org/abs/2306.09256)

    这项研究调查了一些基于密度的聚类技术，分析了它们的特点、优点和缺点，以及它们在不同类型的数据集上挖掘模式的适用性。

    

    基于密度的聚类是数据挖掘中使用的一种聚类方法，用于从数据集中提取以前未知的模式。有许多基于密度的聚类方法，如DBSCAN，OPTICS，DENCLUE，VDBSCAN，DVBSCAN，DBCLASD和ST-DBSCAN。本文对这些方法进行了研究，并分析了它们的特点、优点和缺点，最重要的是它们对不同类型的数据集挖掘有用和适当的模式的适用性。

    Density Based Clustering are a type of Clustering methods using in data mining for extracting previously unknown patterns from data sets. There are a number of density based clustering methods such as DBSCAN, OPTICS, DENCLUE, VDBSCAN, DVBSCAN, DBCLASD and ST-DBSCAN. In this paper, a study of these methods is done along with their characteristics, advantages and disadvantages and most importantly, their applicability to different types of data sets to mine useful and appropriate patterns.
    
[^110]: 基于深度学习的多目标电机技术优化的元模型建模

    Deep learning based Meta-modeling for Multi-objective Technology Optimization of Electrical Machines. (arXiv:2306.09087v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09087](http://arxiv.org/abs/2306.09087)

    本文利用变分自编码器（VAE）在高维设计空间中同时优化了异步电机和永磁同步电机两种不同的电机技术，通过深度神经网络和解码器作为元模型，预测关键性能指标并生成新的设计。与经典的基于深度学习的直接方法相比，VAE方法表现出更好的性能。

    

    旋转电机的优化既耗时又需要计算资源。由于参数化不同，设计优化通常对每种电机技术分别执行。本文介绍了利用变分自编码器（VAE）同时优化两种不同的电机技术，即异步电机和永磁同步电机。在训练之后，我们利用深度神经网络和解码器作为元模型，通过统一的潜在空间在优化循环中预测全局关键性能指标（KPI）并生成相关新设计。数值结果表明了在高维设计空间中的并行参数化多目标技术优化。VAE方法在KPI预测上与经典的基于深度学习的直接方法进行了定量比较。

    Optimization of rotating electrical machines is both time- and computationally expensive. Because of the different parametrization, design optimization is commonly executed separately for each machine technology. In this paper, we present the application of a variational auto-encoder (VAE) to optimize two different machine technologies simultaneously, namely an asynchronous machine and a permanent magnet synchronous machine. After training, we employ a deep neural network and a decoder as meta-models to predict global key performance indicators (KPIs) and generate associated new designs, respectively, through unified latent space in the optimization loop. Numerical results demonstrate concurrent parametric multi-objective technology optimization in the high-dimensional design space. The VAE-based approach is quantitatively compared to a classical deep learning-based direct approach for KPIs prediction.
    
[^111]: NuCLR：核共学习表示

    NuCLR: Nuclear Co-Learned Representations. (arXiv:2306.06099v1 [nucl-th])

    [http://arxiv.org/abs/2306.06099](http://arxiv.org/abs/2306.06099)

    NuCLR是一种可以预测各种核可观测量，包括结合能、衰变能和核电荷半径的深度学习模型，使用共享表示法进行训练，具有最先进的性能，可以捕捉到核物理的基础物理原理，并有潜力为核理论提供有价值的见解。

    

    我们引入了核共学习表示（NuCLR），这是一种深度学习模型，可以预测各种核可观测量，包括结合能、衰变能和核电荷半径。该模型采用多任务学习方法进行训练，使用共享表示法，获得了最先进的性能，达到了理解核（天体）物理基本现象所必需的精度水平。我们还报告了一个有趣的发现，即NuCLR学习的表示展现出核壳模型的重要方面，包括壳层结构，包括众所周知的魔数和泡利排斥原理。这表明该模型能够捕捉到基础物理原理，我们的方法有潜力为核理论提供有价值的见解。

    We introduce Nuclear Co-Learned Representations (NuCLR), a deep learning model that predicts various nuclear observables, including binding and decay energies, and nuclear charge radii. The model is trained using a multi-task approach with shared representations and obtains state-of-the-art performance, achieving levels of precision that are crucial for understanding fundamental phenomena in nuclear (astro)physics. We also report an intriguing finding that the learned representation of NuCLR exhibits the prominent emergence of crucial aspects of the nuclear shell model, namely the shell structure, including the well-known magic numbers, and the Pauli Exclusion Principle. This suggests that the model is capable of capturing the underlying physical principles and that our approach has the potential to offer valuable insights into nuclear theory.
    
[^112]: 推理时间干预：从语言模型中引导出真实的答案

    Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])

    [http://arxiv.org/abs/2306.03341](http://arxiv.org/abs/2306.03341)

    本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。

    

    我们介绍了推理时间干预（ITI）技术，旨在增强大型语言模型（LLMs）的真实性。ITI通过在推理过程中沿着一组方向移动模型激活，跨越有限数量的注意力头。这种干预显着提高了LLaMA模型在TruthfulQA基准上的表现。在指令微调的LLaMA Alpaca上，ITI将其真实性从32.5％提高到65.1％。我们确定了真实性和可用性之间的权衡，并演示了如何通过调整干预强度来平衡它。ITI 取得了最低程度的干扰且计算廉价。此外，该技术在数据效率上表现优异：虽然像RLHF这样的方法需要广泛注释，但是ITI仅使用了几百个例子就能定位真实的方向。我们的研究结果表明，LLMs可能具有某种内部表示方法来表示某事是真实的可能性，即使它们在表面上产生了虚假的结果。

    We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
    
[^113]: ConvNets是如何理解图像亮度的？

    How Do ConvNets Understand Image Intensity?. (arXiv:2306.00360v1 [cs.CV])

    [http://arxiv.org/abs/2306.00360](http://arxiv.org/abs/2306.00360)

    该论文研究了ConvNets需要依赖图像亮度信息的情况，并通过可视化进行了展示。

    

    卷积神经网络(ConvNets)通常依赖边缘/形状信息来分类图像。过去十年间开发的可视化方法确认ConvNets依靠边缘信息。我们研究了ConvNets需要除形状信息外还需依赖图像亮度的情况。我们通过可视化展示了ConvNets依赖图像亮度信息。

    Convolutional Neural Networks (ConvNets) usually rely on edge/shape information to classify images. Visualization methods developed over the last decade confirm that ConvNets rely on edge information. We investigate situations where the ConvNet needs to rely on image intensity in addition to shape. We show that the ConvNet relies on image intensity information using visualization.
    
[^114]: 利用分子对接和机器学习回归方法的药物重用以靶向COVID-19 3CL Protease

    Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.18088](http://arxiv.org/abs/2305.18088)

    本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。

    

    COVID-19疫情已经成为全球健康危机，迫切需要快速鉴定潜在的治疗药物。为了应对这一挑战，药物重用是省时省力的唯一解决方案。本研究使用Zinc数据库对全球已批准（包括FDA批准）的5903种药物进行筛选，作为潜在的COVID-19治疗药物，以靶向SARS-CoV-2的主要蛋白酶3CL。我们使用Autodock-Vina进行分子对接，检查药物分子的功效。为了提高药物重用的效率，我们采用决策树、额外树、MLP、KNN、XGBoost和梯度提升等多个机器学习回归方法建模结合药物的结合亲和力。计算结果表明，决策树回归（DTR）模型具有改进的统计措施R2和RMSE。这些模拟结果有助于识别具有高结合亲和力和有利的结合能的药物。

    The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
    
[^115]: PFN是适用于实际贝叶斯优化的灵活模型。

    PFNs Are Flexible Models for Real-World Bayesian Optimization. (arXiv:2305.17535v1 [cs.LG])

    [http://arxiv.org/abs/2305.17535](http://arxiv.org/abs/2305.17535)

    本文使用灵活的PFN作为BO代理建模，该模型能够允许进一步信息纳入以进行非远视BO。在三种不同的问题上得到了很好的结果。

    

    本文使用先验数据拟合网络(PFNs)作为贝叶斯优化(BO)的灵活代理。PFN是一种神经过程，被训练用于近似后验预测分布(PPD)，适用于任何可有效采样的先验分布。我们描述了如何利用这种灵活性来进行BO的代理建模。我们使用PFN来模拟一个朴素高斯过程(GP)，一个先进的GP和一个贝叶斯神经网络(BNN)。此外，我们展示了如何将进一步的信息纳入先验，例如允许有关最优位置的提示(用户先验)，忽略不相关的维度，并通过学习获取函数来执行非远视BO。这些扩展的灵活性为使用PFN进行BO开辟了广阔的可能性。我们在人工高斯过程样本和三个不同的超参数优化测试平台上展示了PFN对BO的有用性：HPO-B、Bayesmark和PD1。

    In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) for any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1. We publish code 
    
[^116]: Dropout可以缓解双下降现象的研究

    Dropout Drops Double Descent. (arXiv:2305.16179v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16179](http://arxiv.org/abs/2305.16179)

    本研究发现通过在全连接线性层之前添加一个dropout层，可以缓解双下降现象，从而提高模型的预测准确性。

    

    本研究发现并分析，通过在全连接线性层之前添加一个dropout层，可以轻松地缓解双下降现象。双下降现象在近年来引起了公众的关注，即随着样本或模型规模的增加，预测误差会先上升再下降。本文从理论和实证两个方面证明，通过在线性回归模型和非线性随机特征回归中使用最佳的dropout，可以缓解这些现象。

    In this paper, we find and analyze that we can easily drop the double descent by only adding one dropout layer before the fully-connected linear layer. The surprising double-descent phenomenon has drawn public attention in recent years, making the prediction error rise and drop as we increase either sample or model size. The current paper shows that it is possible to alleviate these phenomena by using optimal dropout in the linear regression model and the nonlinear random feature regression, both theoretically and empirically. % ${y}=X{\beta}^0+{\epsilon}$ with $X\in\mathbb{R}^{n\times p}$. We obtain the optimal dropout hyperparameter by estimating the ground truth ${\beta}^0$ with generalized ridge typed estimator $\hat{{\beta}}=(X^TX+\alpha\cdot\mathrm{diag}(X^TX))^{-1}X^T{y}$. Moreover, we empirically show that optimal dropout can achieve a monotonic test error curve in nonlinear neural networks using Fashion-MNIST and CIFAR-10. Our results suggest considering dropout for risk curve
    
[^117]: MLP在顺序推荐中的复仇

    Revenge of MLP in Sequential Recommendation. (arXiv:2305.14675v1 [cs.LG])

    [http://arxiv.org/abs/2305.14675](http://arxiv.org/abs/2305.14675)

    本文提出了一种纯MLP顺序推荐架构TriMLP，其中加入了新颖的三角形混合器以实现标记有序的交互，以提高顺序推荐的性能表现。

    

    顺序推荐模型对历史用户-物品交互行为序列进行建模，以更好地推断动态偏好。近年来，得益于改进的神经网络架构，如RNN、CNN和Transformer，这个领域已经迎来了快速的性能提升。最近，全MLP模型的研究成果引发了人们对一种高效的方法的注意，即通过混合历史行为的MLP学习转换模式。然而，由于这种全连接结构允许不受限制的跨行为间通信并忽略了时间顺序，我们发现直接将混合MLP应用于顺序推荐会导致较差的性能。本文提出了一种纯MLP顺序推荐架构TriMLP，其中包括一种新颖的三角形混合器，改进后的MLP赋予了标记有序的交互。由于MLP中的跨标记交互实际上是矩阵...

    Sequential recommendation models sequences of historical user-item interactive behaviors (or referred as token) to better infer dynamic preferences. Fueled by the improved neural network architectures such as RNN, CNN and Transformer, this field has enjoyed rapid performance boost in the past years. Recent progress on all-MLP models lights on an efficient method with less intensive computation, token-mixing MLP, to learn the transformation patterns among historical behaviors. However, due to the inherent fully-connection design that allows the unrestricted cross-token communication and ignores the chronological order, we find that directly applying token-mixing MLP into sequential recommendation leads to subpar performance. In this paper, we present a purely MLP-based sequential recommendation architecture TriMLP with a novel \underline{Tri}angular Mixer where the modified \underline{MLP} endows tokens with ordered interactions. As the cross-token interaction in MLP is actually matrix 
    
[^118]: OL-Transformer：用于光学多层薄膜结构的快速通用代理模拟器

    OL-Transformer: A Fast and Universal Surrogate Simulator for Optical Multilayer Thin Film Structures. (arXiv:2305.11984v1 [cs.LG])

    [http://arxiv.org/abs/2305.11984](http://arxiv.org/abs/2305.11984)

    该论文提出了OL-Transformer用于光学多层薄膜结构，可以预测多达$10^{25}$种不同多层结构的精确反射和透射光谱，同时具有快速的计算速度。

    

    基于深度学习的方法最近被证明是用于光学多层薄膜结构的快速准确的代理模拟器。然而，现有的方法仅适用于具有不同材料排列方式的有限类型的结构，限制了它们向多样化和通用化结构的应用。在这里，我们提出了Opto-Layer（OL）Transformer作为巨量结构的通用替代模拟器。结合结构序列化技术，我们的模型可以预测多达$10^{25}$种不同多层结构的精确反射和透射光谱，同时相较于物理求解器仍然实现了6倍时间加速。进一步的研究表明，普遍的学习能力来自于我们的模型首先学习物理嵌入，然后使用自我注意机制来捕捉每层之间的光物质相互作用的隐藏关系。

    Deep learning-based methods have recently been established as fast and accurate surrogate simulators for optical multilayer thin film structures. However, existing methods only work for limited types of structures with different material arrangements, preventing their applications towards diverse and universal structures. Here, we propose the Opto-Layer (OL) Transformer to act as a universal surrogate simulator for enormous types of structures. Combined with the technique of structure serialization, our model can predict accurate reflection and transmission spectra for up to $10^{25}$ different multilayer structures, while still achieving a six-fold time speedup compared to physical solvers. Further investigation reveals that the general learning ability comes from the fact that our model first learns the physical embeddings and then uses the self-attention mechanism to capture the hidden relationship of light-matter interaction between each layer.
    
[^119]: 具有非二元特征的分类器的新型解释方法

    A New Class of Explanations for Classifiers with Non-Binary Features. (arXiv:2304.14760v1 [cs.AI])

    [http://arxiv.org/abs/2304.14760](http://arxiv.org/abs/2304.14760)

    本文提出了一种适用于具有非二元特征的分类器的新型解释方法，可以提供更多关于决策和基础分类器的信息。

    

    近来，当分析分类器决策时，已经有两种类型的解释受到了文献中的重视。第一种解释是为决策提供充分理由的解释，即缩写为PI解释的诱导式解释；第二种解释是为何不做出其他决策的解释，即对照式或反事实解释的必要理由。这些解释是为二元、离散和在某些情况下为连续特征的分类器定义的。我们展示了当存在非二元特征时，这些解释可以得到显著的改进，从而导致了一类新的解释方法，可以提供更多关于决策和基础分类器的信息。必要和充分原因也被证明是完整原因的主要蕴含项和被蕴含项，可以使用量化算子获得。我们的结果表明，我们改进的必要和充分原因的概念比现有方法更好地适用于具有非二元特征的分类器。

    Two types of explanations have received significant attention in the literature recently when analyzing the decisions made by classifiers. The first type explains why a decision was made and is known as a sufficient reason for the decision, also an abductive or PI-explanation. The second type explains why some other decision was not made and is known as a necessary reason for the decision, also a contrastive or counterfactual explanation. These explanations were defined for classifiers with binary, discrete and, in some cases, continuous features. We show that these explanations can be significantly improved in the presence of non-binary features, leading to a new class of explanations that relay more information about decisions and the underlying classifiers. Necessary and sufficient reasons were also shown to be the prime implicates and implicants of the complete reason for a decision, which can be obtained using a quantification operator. We show that our improved notions of necessa
    
[^120]: 连续和离散权重下非凸神经网络中的典型与非典型解析解

    Typical and atypical solutions in non-convex neural networks with discrete and continuous weights. (arXiv:2304.13871v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2304.13871](http://arxiv.org/abs/2304.13871)

    本文研究了二元和连续的负边距感知器作为非凸神经网络模型，发现它们都存在着极为平坦和宽广的亚优解，这对于二元情况中的算法行为有着很强的影响。

    

    本论文研究了二元和连续的负边距感知器作为简单的非凸神经网络模型在学习随机规则和关联时的情况。我们分析了两种模型的解空间几何形态，并找到了重要的相似性和差异性。这两种模型都表现出极为平坦和宽广的亚优解。这些亚优解与二元情况下无法递归算法访问的大量蔓延小集群（冻结1-RSB相）组成的主要解集共存，或者球面情况下不同大小聚类的分层结构（完全RSB相）。在两种情况下，当交叉一定密度约束的阈值时，宽广平坦极小值的局部熵变得非单调，表明鲁棒解的空间被分成了不连通的组件。这对二元模型中算法的行为产生了很强的影响，因为它无法访问剩余的孤立集群。

    We study the binary and continuous negative-margin perceptrons as simple non-convex neural network models learning random rules and associations. We analyze the geometry of the landscape of solutions in both models and find important similarities and differences. Both models exhibit subdominant minimizers which are extremely flat and wide. These minimizers coexist with a background of dominant solutions which are composed by an exponential number of algorithmically inaccessible small clusters for the binary case (the frozen 1-RSB phase) or a hierarchical structure of clusters of different sizes for the spherical case (the full RSB phase). In both cases, when a certain threshold in constraint density is crossed, the local entropy of the wide flat minima becomes non-monotonic, indicating a break-up of the space of robust solutions into disconnected components. This has a strong impact on the behavior of algorithms in binary models, which cannot access the remaining isolated clusters. For
    
[^121]: 数据驱动需求预测的能源中心随机MPC

    Stochastic MPC for energy hubs using data driven demand forecasting. (arXiv:2304.12438v1 [eess.SY])

    [http://arxiv.org/abs/2304.12438](http://arxiv.org/abs/2304.12438)

    本文提出了一种基于数据驱动需求预测的随机MPC控制器，采用机会约束来最小化不确定的电力和热需求，使用历史数据构建预测模型，并通过情景法采样多步需求轨迹。在模拟和实际数据上验证了该控制器的性能。

    

    能源中心通过多种转换和储能组件将能源资源进行转换和分配。能源中心的最佳运行利用其灵活性来增加能源效率并减少运营成本。然而，需求中的不确定性给能源中心的优化带来了挑战。本文提出了一种随机MPC控制器，使用机会约束来最小化不确定的电力和热需求，采用历史数据构建基于高斯过程的需求预测模型，并生成未来电力和热需求的预测。通过从预测模型中采样多步需求轨迹，采用“情景法”解决随机优化问题。在模拟能源中心模型和来自实际建筑的需求数据上验证了所提出预测器和随机控制器的性能。

    Energy hubs convert and distribute energy resources by combining different energy inputs through multiple conversion and storage components. The optimal operation of the energy hub exploits its flexibility to increase the energy efficiency and reduce the operational costs. However, uncertainties in the demand present challenges to energy hub optimization. In this paper, we propose a stochastic MPC controller to minimize energy costs using chance constraints for the uncertain electricity and thermal demands. Historical data is used to build a demand prediction model based on Gaussian processes to generate a forecast of the future electricity and heat demands. The stochastic optimization problem is solved via the Scenario Approach by sampling multi-step demand trajectories from the derived prediction model. The performance of the proposed predictor and of the stochastic controller is verified on a simulated energy hub model and demand data from a real building.
    
[^122]: 一种转移原理：从欧几里得通用逼近器到度量空间之间的通用逼近器

    A Transfer Principle: Universal Approximators Between Metric Spaces From Euclidean Universal Approximators. (arXiv:2304.12231v1 [cs.LG])

    [http://arxiv.org/abs/2304.12231](http://arxiv.org/abs/2304.12231)

    本论文提出使用欧几里得空间通用逼近器为构建块，构建了在任意波兰度量空间 $\mathcal{X}$ 和 $\mathcal{Y}$ 之间的通用逼近器，并通过随机化输出离散概率测度来克服某些限制。在适当的结构下提供了概率和定量保证。

    

    我们使用欧几里得空间通用逼近器作为构建块，构建了连续映射的度量空间之间的通用逼近器。早期结果假定输出空间 $\mathcal{Y}$ 是拓扑向量空间。我们通过“随机化”来克服这种限制：我们的逼近器输出 $\mathcal{Y}$ 上的离散概率测度。当 $\mathcal{X}$ 和 $\mathcal{Y}$ 没有附加结构时，我们证明了非常通用的定性保证；当它们具有适当的组合结构时，我们证明了 H\"older 类映射的定量保证，包括有限图之间的映射，在某些 Carnot 群之间的粗微分方程的解算子以及反问题中出现的 Banach 空间之间的连续非线性算子。特别地，我们展示了所需的 Dirac 测度数量由 $\mathcal{X}$ 和 $\mathcal{Y}$ 的组合结构决定。

    We build universal approximators of continuous maps between arbitrary Polish metric spaces $\mathcal{X}$ and $\mathcal{Y}$ using universal approximators between Euclidean spaces as building blocks. Earlier results assume that the output space $\mathcal{Y}$ is a topological vector space. We overcome this limitation by "randomization": our approximators output discrete probability measures over $\mathcal{Y}$. When $\mathcal{X}$ and $\mathcal{Y}$ are Polish without additional structure, we prove very general qualitative guarantees; when they have suitable combinatorial structure, we prove quantitative guarantees for H\"older-like maps, including maps between finite graphs, solution operators to rough differential equations between certain Carnot groups, and continuous non-linear operators between Banach spaces arising in inverse problems. In particular, we show that the required number of Dirac measures is determined by the combinatorial structure of $\mathcal{X}$ and $\mathcal{Y}$. For b
    
[^123]: 基于BERT的技术对美国最高法院案例进行分类

    Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])

    [http://arxiv.org/abs/2304.08649](http://arxiv.org/abs/2304.08649)

    本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。

    

    基于双向编码器表示来自变压器的模型（BERT）在许多自然语言处理（NLP）任务（如命名实体识别（NER），词性（POS）标记等）上产生了最新技术（SOTA）结果。当分类长文档（例如来自美国最高法院的文档）时，使用BERT模型可能比较困难。本文中，我们尝试了几种基于BERT的分类技术，用于对美国最高法院决定或最高法院数据库（SCDB）进行分类，并将其与先前的SOTA结果进行了比较。我们还将我们的结果与针对长文档的SOTA模型进行了比较。我们对两个分类任务进行了比较：（1）广泛的分类任务，具有15个类别；（2）细粒度的分类任务，具有279个类别。我们的最佳结果在15个广泛类别上产生80％的准确度，在279个细粒度类别上产生60％的准确度。

    Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
    
[^124]: 基于代码审查的自动程序修复: 预训练Transformer模型表现如何？

    Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?. (arXiv:2304.07840v1 [cs.LG])

    [http://arxiv.org/abs/2304.07840](http://arxiv.org/abs/2304.07840)

    本研究探究使用大型预训练语言模型，结合自然语言和编程语言，来改善自动程序修复的效果，发现预训练模型通过使用代码审查和代码更改的数据集微调，能显著优于传统方法，并具有更好的泛化能力。

    

    序列到序列模型已被用于将错误的程序转换为正确的程序，当它们被训练使用足够大的数据集时。一些最近的研究也证明了代码审查(关于代码中建议性更改的自然语言指令)可以进一步改善程序修复。使用自然语言和计算机程序语料库训练的大型语言模型具有包含两种语言的固有知识的能力。在本研究中，我们探究了是否可以利用代码和自然语言的固有知识来提高自动程序修复的效果。我们将PLBART和CodeT5这两个最先进的语言模型，应用于两个基于自然语言的程序修复数据集，并发现使用包含代码审查和随后代码更改的数据集微调的预训练语言模型，明显优于之前的模型。我们观察到预训练模型具有更好的泛化能力，可以从之前未见过的数据中学习并得到更好的结果，相比之前的方法，我们的方法具有更高的修复能力，相对于之前的SOTA技术，CODET5取得了3%左右的提升

    Sequence-to-sequence models have been used to transform erroneous programs into correct ones when trained with a large enough dataset. Some recent studies also demonstrated strong empirical evidence that code review (natural language instruction about suggestive changes in code) can improve the program repair further. Large language models, trained with Natural Language (NL) and computer program corpora, have the capacity to contain inherent knowledge of both. In this study, we investigate if this inherent knowledge of code and NL can be utilized to improve automated program repair. We applied PLBART and CodeT5, two state-of-the-art language models that are pre-trained with both Programming Language (PL) and Natural Language (NL), on two such natural language-based program repair datasets and found that the pre-trained language models fine-tuned with datasets containing both code review and subsequent code changes notably outperform each of the previous models. We observed that the pre
    
[^125]: S3M：通过无监督对应实现可扩展的统计形状建模

    S3M: Scalable Statistical Shape Modeling through Unsupervised Correspondences. (arXiv:2304.07515v1 [cs.CV])

    [http://arxiv.org/abs/2304.07515](http://arxiv.org/abs/2304.07515)

    该论文提出一种无监督的方法来建立统计形状模型，使用深度几何特征和功能对应来同时学习复杂解剖结构中的局部和全局形状结构，并且该方法足够健壮，能够从噪声神经网络预测中学习。

    

    统计形状模型(SSM)是一种几何上表示人群解剖学结构的方法，在临床诊断领域具有广泛应用。但是，通常需要领域专业知识和费力的手动分割或地标注释来生成。对于SSM的对应估计方法通常需要使用这些标签作为监督信号进行学习。我们提出了一种无监督的方法，利用深度几何特征和功能对应来同时学习复杂解剖结构中的局部和全局形状结构。我们的流程显著改进了SSM的无监督对应估计方法，即使在高度不规则的表面拓扑上也是如此。我们用甲状腺和多室心脏数据集展示了这一点。此外，我们的方法足够健壮，能够从噪声神经网络预测中学习，从而实现扩大SSMs规模的目标。

    Statistical shape models (SSMs) are an established way to geometrically represent the anatomy of a population with various clinically relevant applications. However, they typically require domain expertise and labor-intensive manual segmentations or landmark annotations to generate. Methods to estimate correspondences for SSMs typically learn with such labels as supervision signals. We address these shortcomings by proposing an unsupervised method that leverages deep geometric features and functional correspondences to learn local and global shape structures across complex anatomies simultaneously. Our pipeline significantly improves unsupervised correspondence estimation for SSMs compared to baseline methods, even on highly irregular surface topologies. We demonstrate this for two different anatomical structures: the thyroid and a multi-chamber heart dataset. Furthermore, our method is robust enough to learn from noisy neural network predictions, enabling scaling SSMs to larger patien
    
[^126]: 评估-优化方法与集成评估优化法：基于随机优势的观点

    Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective. (arXiv:2304.06833v1 [stat.ML])

    [http://arxiv.org/abs/2304.06833](http://arxiv.org/abs/2304.06833)

    本文提出，当模型类足够丰富以涵盖真实情况时，非线性问题的“先估计再优化”方法优于集成方法，包括优化间隙的渐进优势的均值，所有其他时刻和整个渐进分布。

    

    在数据驱动的随机优化中，除了需要优化任务，还需要从数据中估计潜在分布的模型参数。最近的文献表明，通过选择导致最佳经验目标性能的模型参数，可以集成估计和优化过程。当模型被错误地指定时，这种集成方法可以很容易地显示出优于简单的“先估计再优化”的方法。本文认为，在模型类足够丰富以涵盖真实情况的情况下，对于非线性问题，两种方法之间的性能排序在强烈的意义下被颠倒。在受限条件和当上下文特征可用时，类似的结果也成立。

    In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature suggests the integration of the estimation and optimization processes, by selecting model parameters that lead to the best empirical objective performance. Such an integrated approach can be readily shown to outperform simple ``estimate then optimize" when the model is misspecified. In this paper, we argue that when the model class is rich enough to cover the ground truth, the performance ordering between the two approaches is reversed for nonlinear problems in a strong sense. Simple ``estimate then optimize" outperforms the integrated approach in terms of stochastic dominance of the asymptotic optimality gap, i,e, the mean, all other moments, and the entire asymptotic distribution of the optimality gap is always better. Analogous results also hold under constrained settings and when contextual features are availa
    
[^127]: 基于不确定性的开放集学习用于视网膜异常识别

    Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification. (arXiv:2304.03981v1 [cs.LG])

    [http://arxiv.org/abs/2304.03981](http://arxiv.org/abs/2304.03981)

    提出了基于不确定性的开放集(UIOS)模型，用于处理训练中未见过的类别样本，该模型通过计算不确定性得分来表达其置信度，并在多个测试数据集中表现优异，为真实世界的视网膜异常筛查提供了一个强大的方法。

    

    人工智能在实际视网膜异常分类应用中的一个主要限制是无法识别训练过程中未见过的类别样本。为解决这一障碍，我们提出了一种基于不确定性的开放集(UIOS)模型，该模型使用了9个常见视网膜病变的眼底图像进行训练。除了每个类别的概率，UIOS还计算了不确定性得分来表达其置信度。我们的UIOS模型通过设置阈值策略，在内部测试数据集、外部测试数据集和非典型测试数据集中的F1分别达到了99.55％、97.01％和91.91％，相比标准人工智能模型的F1分别为92.20％、80.69％和64.74％。此外，UIOS正确预测了罕见视网膜疾病、低质量眼底图像和非眼底图像等数据集中的高不确定性分数，提示需要手动检查。这项工作为真实世界的视网膜异常筛查提供了一个强大的方法。

    Failure to recognize samples from the classes unseen during training is a major limit of artificial intelligence (AI) in real-world implementation of retinal anomaly classification. To resolve this obstacle, we propose an uncertainty-inspired open-set (UIOS) model which was trained with fundus images of 9 common retinal conditions. Besides the probability of each category, UIOS also calculates an uncertainty score to express its confidence. Our UIOS model with thresholding strategy achieved an F1 score of 99.55%, 97.01% and 91.91% for the internal testing set, external testing set and non-typical testing set, respectively, compared to the F1 score of 92.20%, 80.69% and 64.74% by the standard AI model. Furthermore, UIOS correctly predicted high uncertainty scores, which prompted the need for a manual check, in the datasets of rare retinal diseases, low-quality fundus images, and non-fundus images. This work provides a robust method for real-world screening of retinal anomalies.
    
[^128]: RED-PSM: 带去噪正则化的部分可分模型用于动态成像

    RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging. (arXiv:2304.03483v1 [eess.IV])

    [http://arxiv.org/abs/2304.03483](http://arxiv.org/abs/2304.03483)

    本文提出了一种称为RED-PSM的方法，将部分可分模型与去噪正则化相结合，用于解决动态成像问题，数值实验证明其优越性。

    

    动态成像是指利用被欠采样的测量数据恢复每个时间点上的时变二维或三维物体。尤其是在动态断层扫描中，每个时间点上只有一个视角下的单个投影可用，使得问题严重算不可逆。本文提出了一种称为RED-PSM的方法，首次将两种强大的技术结合起来解决这个具有挑战性的成像问题。第一种技术是部分可分模型，已经用于高效地为时空目标引入低秩先验。第二种是最近提出的去噪正则化(RED)，它提供了一种灵活的框架，利用最先进的图像去噪算法处理各种反问题。我们提出了一个带正则化的部分可分目标，通过变量分裂和ADMM优化方案，并证明了我们的目标收敛于一个满足优化问题的稳定点。我们通过在具有挑战性的动态断层扫描问题上进行了大量的数值实验，证明了所提出的RED-PSM方法的优越性，相较于现有的动态成像技术。

    Dynamic imaging addresses the recovery of a time-varying 2D or 3D object at each time instant using its undersampled measurements. In particular, in the case of dynamic tomography, only a single projection at a single view angle may be available at a time, making the problem severely ill-posed. In this work, we propose an approach, RED-PSM, which combines for the first time two powerful techniques to address this challenging imaging problem. The first, are partially separable models, which have been used to efficiently introduce a low-rank prior for the spatio-temporal object. The second is the recent Regularization by Denoising (RED), which provides a flexible framework to exploit the impressive performance of state-of-the-art image denoising algorithms, for various inverse problems. We propose a partially separable objective with RED and an optimization scheme with variable splitting and ADMM, and prove convergence of our objective to a value corresponding to a stationary point satis
    
[^129]: 可达集的凸包的精确刻画

    Exact Characterization of the Convex Hulls of Reachable Sets. (arXiv:2303.17674v1 [math.OC])

    [http://arxiv.org/abs/2303.17674](http://arxiv.org/abs/2303.17674)

    本文精确地刻画了具有有界扰动的非线性系统的可达集的凸包为一阶常微分方程的解的凸包，提出了一种低成本、高精度的估计算法，可用于过逼近可达集。

    

    本文研究了具有有界扰动的非线性系统的可达集的凸包。可达集在控制中起着至关重要的作用，但计算起来仍然非常具有挑战性，现有的过逼近工具往往过于保守或计算代价高昂。本文精确地刻画了可达集的凸包，将其表示成一阶常微分方程的解的凸包，这个有限维的刻画开启了一种紧密的估计算法，可用于过逼近可达集，且成本比现有方法更低、更精准。本文还提出了神经反馈环分析和鲁棒模型预测控制的应用。

    We study the convex hulls of reachable sets of nonlinear systems with bounded disturbances. Reachable sets play a critical role in control, but remain notoriously challenging to compute, and existing over-approximation tools tend to be conservative or computationally expensive. In this work, we exactly characterize the convex hulls of reachable sets as the convex hulls of solutions of an ordinary differential equation from all possible initial values of the disturbances. This finite-dimensional characterization unlocks a tight estimation algorithm to over-approximate reachable sets that is significantly faster and more accurate than existing methods. We present applications to neural feedback loop analysis and robust model predictive control.
    
[^130]: NeRF-GAN蒸馏：基于卷积的高效3D感知生成

    NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions. (arXiv:2303.12865v1 [cs.CV])

    [http://arxiv.org/abs/2303.12865](http://arxiv.org/abs/2303.12865)

    本文提出了一种基于神经辐射场（NeRF）和生成对抗网络（GAN）的新方法，通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，实现了基于姿态的2D GAN的高效3D感知生成。

    

    基于姿态的卷积生成模型在从单视图数据集进行高质量的3D一致图像生成方面存在困难，因为它们缺乏足够的3D先验知识。最近，将神经辐射场（NeRF）和生成对抗网络（GAN）等生成模型相结合，从单视图图像中生成3D感知图像，已经引起了广泛关注。NeRF-GAN利用了三维神经表示和体积渲染的强归纳偏差，但也带来了更高的计算复杂性。这项研究旨在通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，重审基于姿态的二维GAN，在推理时间内实现高效的3D感知生成。我们提出了一种简单有效的方法，基于在基于姿态的卷积网络中重用预训练的NeRF-GAN的良好解耦潜在空间，直接生成与潜在的3D表达相对应的3D一致图片。在多个数据集上的实验表明，所提出的方法实现了更高效的3D感知生成。

    Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed met
    
[^131]: “图像分析算法在流行病变化下的部署”

    Deployment of Image Analysis Algorithms under Prevalence Shifts. (arXiv:2303.12540v1 [cs.CV])

    [http://arxiv.org/abs/2303.12540](http://arxiv.org/abs/2303.12540)

    本文通过实证研究表明，在机器学习在医学图像分析领域的实际应用中，流行病变化对算法的部署效果有重要影响。

    

    领域差距是医学图像分析的机器学习解决方案临床转化中最重要的障碍之一。尽管当前的研究重点是新的训练范式和网络架构，但很少关注算法在实践中部署时流行病发生率变化的特定影响。对于人工智能民主化的背景，由于疾病流行率可能在时间和地点上变化很大，因此开发/验证方法中使用的数据和部署环境中的数据的类别频率之间的差异非常重要。我们的贡献有两个方面。首先，我们通过分析（i）校准的程度，（ii）决策阈值与最优值的偏差，以及（iii）验证指标反映神经网络性能在部署人口的能力，实证地证明了缺少流行病处理的潜在严重后果。

    Domain gaps are among the most relevant roadblocks in the clinical translation of machine learning (ML)-based solutions for medical image analysis. While current research focuses on new training paradigms and network architectures, little attention is given to the specific effect of prevalence shifts on an algorithm deployed in practice. Such discrepancies between class frequencies in the data used for a method's development/validation and that in its deployment environment(s) are of great importance, for example in the context of artificial intelligence (AI) democratization, as disease prevalences may vary widely across time and location. Our contribution is twofold. First, we empirically demonstrate the potentially severe consequences of missing prevalence handling by analyzing (i) the extent of miscalibration, (ii) the deviation of the decision threshold from the optimum, and (iii) the ability of validation metrics to reflect neural network performance on the deployment population a
    
[^132]: 基于表格数据的高效多级推断

    Efficient Multi-stage Inference on Tabular Data. (arXiv:2303.11580v1 [cs.LG])

    [http://arxiv.org/abs/2303.11580](http://arxiv.org/abs/2303.11580)

    该论文通过将推断算法简化并嵌入产品代码中，以减少网络通信，在处理表格数据的实时平台上可将推断延迟降低1.3倍，CPU资源减少30％，并将应用程序前端和后端之间的网络通信减少60％。

    

    许多机器学习应用和产品通过中等数量的输入数据进行训练，但在实时推断时被瓶颈所困。传统智慧在实现机器学习系统时，倾向于将ML代码分割成服务，并通过远程过程调用（RPC）API被产品代码查询。这种方法澄清了整体软件架构，并通过抽象ML内部简化了产品代码。然而，这种分离增加了网络延迟，并带来了额外的CPU开销。因此，我们简化推断算法并将其嵌入产品代码中，以减少网络通信。针对公共数据集和处理表格数据的高性能实时平台，我们表明通常有超过一半的输入可以适应这种优化，而其余部分可以由原始模型处理。通过将AutoML应用于训练和推断，我们将推断延迟降低了1.3倍，CPU资源减少了30％，应用程序前端和后端之间的网络通信减少了60％。

    Many ML applications and products train on medium amounts of input data but get bottlenecked in real-time inference. When implementing ML systems, conventional wisdom favors segregating ML code into services queried by product code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall software architecture and simplifies product code by abstracting away ML internals. However, the separation adds network latency and entails additional CPU overhead. Hence, we simplify inference algorithms and embed them into the product code to reduce network communication. For public datasets and a high-performance real-time platform that deals with tabular data, we show that over half of the inputs are often amenable to such optimization, while the remainder can be handled by the original model. By applying our optimization with AutoML to both training and inference, we reduce inference latency by 1.3x, CPU resources by 30%, and network communication between application front-end an
    
[^133]: 基于深度学习的振动信号去噪方法

    Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])

    [http://arxiv.org/abs/2303.11413](http://arxiv.org/abs/2303.11413)

    本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。

    

    由脚步引起的结构振动信号被广泛用于人员识别、定位、人类活动推断、结构健康监测等任务。然而，由于环境噪声、电磁干扰等因素的影响，实际采集的信号通常会带有噪声。噪声的存在影响了信号处理过程，从而影响了最终任务的准确性和误差。本文主要探讨了基于深度学习的去除脚步引起的振动信号的噪声的方法。我们考虑了不同类型的噪声，包括高斯噪声和非平稳噪声等。

    Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
    
[^134]: 基于深度卷积神经网络伪影降噪的稀疏视图CT图像自动出血检测的改进

    Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v1 [eess.IV])

    [http://arxiv.org/abs/2303.09340](http://arxiv.org/abs/2303.09340)

    本文提出了一种基于深度卷积神经网络的伪影降噪方法，用于改善稀疏视图下自动出血检测的图像质量，并证明其能够与完全采样的图像进行同等精确度的分类和检测。

    

    颅内出血是一种严重的健康问题，需要快速且常常非常密集的医疗治疗。为了诊断，通常要进行颅部计算机断层扫描（CCT）扫描。然而，由于辐射引起的增加的健康风险是一个问题。降低这种潜在风险的最重要策略是尽可能保持辐射剂量低，并与诊断任务一致。 稀疏视图CT可以通过减少所采集的视图总数，从而降低剂量，是一种有效的策略，但代价是降低图像质量。在这项工作中，我们使用U-Net架构来减少稀疏视图CCT的伪影，从稀疏视图中预测完全采样的重建图像。我们使用一个卷积神经网络对出血的检测和分类进行评估，并在完全采样的CCT上进行训练。我们的结果表明，伪影降噪后的CCT图像进行自动分类和检测的准确性与完全采样的CCT图像没有明显差异。

    Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. However, the increased health risk caused by radiation is a concern. The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality. In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. Our results suggest that the automated classification and detection accuracy of hemo
    
[^135]: 自动化专利提取支持聚焦化学空间内的生成建模

    Automated patent extraction powers generative modeling in focused chemical spaces. (arXiv:2303.08272v1 [physics.chem-ph])

    [http://arxiv.org/abs/2303.08272](http://arxiv.org/abs/2303.08272)

    本研究通过开发自动化管道，使用专利数据源训练领域特定的生成模型，利用专利中的弱标记应用类别中尽可能多的信息实现化学空间内生成建模。

    

    深度生成模型已成为反向分子设计的一种令人兴奋的手段，其进展来自于训练算法和分子表示之间的相互作用。应用于材料科学和化学领域时，其中一个主要挑战是缺乏具有属性标签的大规模训练数据集。已发布的专利包含在其在期刊上发表之前披露新材料的信息，是一种相对未被充分利用的科学知识广泛来源。由于专利被提交是为了保护特定用途，因此专利中的分子可以被视为弱标记的应用类别。此外，由美国专利与商标局（USPTO）发布的专利具有可下载的机器可读文本和分子结构。在本研究中，我们通过开发自动化管道，使用专利数据源训练领域特定的生成模型。

    Deep generative models have emerged as an exciting avenue for inverse molecular design, with progress coming from the interplay between training algorithms and molecular representations. One of the key challenges in their applicability to materials science and chemistry has been the lack of access to sizeable training datasets with property labels. Published patents contain the first disclosure of new materials prior to their publication in journals, and are a vast source of scientific knowledge that has remained relatively untapped in the field of data-driven molecular design. Because patents are filed seeking to protect specific uses, molecules in patents can be considered to be weakly labeled into application classes. Furthermore, patents published by the US Patent and Trademark Office (USPTO) are downloadable and have machine-readable text and molecular structures. In this work, we train domain-specific generative models using patent data sources by developing an automated pipeline
    
[^136]: 打开神经网络分类器以计算Shap分数

    Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])

    [http://arxiv.org/abs/2303.06516](http://arxiv.org/abs/2303.06516)

    本文提出了一种高效计算机器学习模型分类中Shap解释分数的方法，通过将二进制神经网络转换为布尔电路，并使用知识编译技术，将电路视为开放式模型，通过最近的高效算法计算Shap分数，相比于将BNN视为黑盒模型直接计算Shap，性能有了显著的提高。

    This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.

    我们解决了使用机器学习模型进行分类的Shap解释分数的高效计算问题。为此，我们展示了将二进制神经网络（BNN）转换为确定性和可分解的布尔电路，使用知识编译技术。所得到的电路被视为开放式模型，通过最近的高效算法计算Shap分数。详细的实验表明，与将BNN视为黑盒模型直接计算Shap相比，性能有了显著的提高。

    We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
    
[^137]: Exphormer: 稀疏Transformer用于图形

    Exphormer: Sparse Transformers for Graphs. (arXiv:2303.06147v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06147](http://arxiv.org/abs/2303.06147)

    Exphormer是一个稀疏Transformer框架，通过虚拟全局节点和扩张图的稀疏注意机制，在保持准确性的同时能够扩展到大型图形，并证明其拥有理想的理论特性。

    

    图形转换器已经成为各种图学习和表示任务的一种有前途的架构。尽管取得了成功，但是将图形转换器扩展到大型图形并同时保持与消息传递网络相媲美的准确性仍然具有挑战性。在本文中，我们介绍了Exphormer，一个构建强大且可扩展的图形转换器的框架。Exphormer包括基于两个机制的稀疏注意机制：虚拟全局节点和扩张图，其数学特性（如谱扩展、伪随机性和稀疏性）使得图形转换器的复杂度仅与图形大小线性相关，并且能够证明生成的转换器模型具有理想的理论特性。我们展示了将Exphormer纳入最近提出的GraphGPS框架中，可以在各种图数据集上获得具有竞争力的实证结果，包括在三个数据集上的最新结果。

    Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three d
    
[^138]: Transformers如何学习主题结构：走向对其机制的理解

    How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding. (arXiv:2303.04245v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04245](http://arxiv.org/abs/2303.04245)

    本文提供了对Transformer学习语义结构的机制性理解，通过数学分析和实验证明了嵌入层和自注意力层如何对词汇的共现结构进行编码。

    

    尽管Transformer在许多领域都取得了成功，但对其学习机制的准确理解仍然存在较大的缺乏。虽然它们在包括各种结构化和推理任务在内的基准测试中表现出了强大的能力，但对数学理解的研究仍然滞后。最近的研究开始从表示方面研究了这个问题：即基于注意力的网络的大小/深度/复杂性用于执行某些任务。然而，并不能保证学习动态会收敛到所提出的结构上。在本文中，我们提供了细致入微的机制理解，阐明了Transformer如何学习“语义结构”，即捕捉词汇的共现结构。准确地说，我们通过数学分析和对维基百科数据以及由潜在狄利克雷分配（LDA）建模的合成数据进行的实验，展示了嵌入层和自注意力层如何对主题进行编码。

    While the successes of transformers across many domains are indisputable, accurate understanding of the learning mechanics is still largely lacking. Their capabilities have been probed on benchmarks which include a variety of structured and reasoning tasks -- but mathematical understanding is lagging substantially behind. Recent lines of work have begun studying representational aspects of this question: that is, the size/depth/complexity of attention-based networks to perform certain tasks. However, there is no guarantee the learning dynamics will converge to the constructions proposed. In our paper, we provide fine-grained mechanistic understanding of how transformers learn "semantic structure", understood as capturing co-occurrence structure of words. Precisely, we show, through a combination of mathematical analysis and experiments on Wikipedia data and synthetic data modeled by Latent Dirichlet Allocation (LDA), that the embedding layer and the self-attention layer encode the topi
    
[^139]: 对比学习与属性关联的出现

    Contrastive Learning and the Emergence of Attributes Associations. (arXiv:2302.10763v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10763](http://arxiv.org/abs/2302.10763)

    对比学习方案通过对物体输入表示进行身份保持的变换，不仅有助于物体的分类，还可以提供关于属性的有无决策的有价值信息。

    

    对于物体呈现，监督学习方案通常会给出一个简洁的标签。而人类在类似的呈现下，除了给出一个标签外，还会被大量的关联信息所淹没，其中包括了呈现物体的属性。对比学习是一种半监督学习方案，基于对物体输入表示进行保持身份的变换。本研究推测，这些变换不仅可以保持呈现物体的身份，还可以保持其语义上有意义的属性的身份。这意味着对比学习方案的输出表示不仅对于呈现物体的分类有价值，还对于任何感兴趣属性的有无决策有价值。通过模拟实验证明了这一观点的可行性。

    In response to an object presentation, supervised learning schemes generally respond with a parsimonious label. Upon a similar presentation we humans respond again with a label, but are flooded, in addition, by a myriad of associations. A significant portion of these consist of the presented object attributes. Contrastive learning is a semi-supervised learning scheme based on the application of identity preserving transformations on the object input representations. It is conjectured in this work that these same applied transformations preserve, in addition to the identity of the presented object, also the identity of its semantically meaningful attributes. The corollary of this is that the output representations of such a contrastive learning scheme contain valuable information not only for the classification of the presented object, but also for the presence or absence decision of any attribute of interest. Simulation results which demonstrate this idea and the feasibility of this co
    
[^140]: BiofilmScanner: 一种从生物被膜图像中获取细菌细胞形态属性的计算智能方法

    BiofilmScanner: A Computational Intelligence Approach to Obtain Bacterial Cell Morphological Attributes from Biofilm Image. (arXiv:2302.09629v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.09629](http://arxiv.org/abs/2302.09629)

    本文提出了一种名为BiofilmScanner的方法，该方法利用基于Yolact的深度学习方法和不变矩来高效地检测和分割SRB图像中的细菌细胞，并准确地提取其几何特性。

    

    Desulfovibrio alaskensis G20 (DA-G20)被用作硫酸盐还原细菌（SRB）的模型，SRB与由微生物引起的腐蚀问题有关。SRB基生物膜被认为是导致金属基础设施每年损失数十亿美元的生物腐蚀的原因。了解SRB生物膜中不同生长阶段中细菌细胞的形状和大小属性的提取将有助于设计防腐技术。然而，当前方法存在许多问题，包括耗时的几何特性提取、低效率和高误差率。本文提出了一种名为BiofilmScanner的方法，它是基于Yolact的深度学习方法，并结合了不变矩来解决这些问题。我们的方法可以高效地检测和分割SRB图像中的细菌细胞，同时利用不变矩对分割的细胞的几何特性进行测量，误差较低。所提出方法的数值实验证明了其有效性。

    Desulfovibrio alaskensis G20 (DA-G20) is utilized as a model for sulfate-reducing bacteria (SRB) that are associated with corrosion issues caused by microorganisms. SRB-based biofilms are thought to be responsible for the billion-dollar-per-year bio-corrosion of metal infrastructure. Understanding the extraction of the bacterial cells' shape and size properties in the SRB-biofilm at different growth stages will assist with the design of anti-corrosion techniques. However, numerous issues affect current approaches, including time-consuming geometric property extraction, low efficiency, and high error rates. This paper proposes BiofilScanner, a Yolact-based deep learning method integrated with invariant moments to address these problems. Our approach efficiently detects and segments bacterial cells in an SRB image while simultaneously invariant moments measure the geometric characteristics of the segmented cells with low errors. The numerical experiments of the proposed method demonstrat
    
[^141]: 从图生成到图分类

    From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07989](http://arxiv.org/abs/2302.07989)

    本文提出了一种新的图分类方法，通过利用图生成模型，推导出了给定图的类标签概率的分类公式，并提出了一种新的条件 ELBO 用于训练生成图自编码器模型。这是一种在图分类中具有创新性的方法。

    

    本文描述了一种利用图生成模型 (GGM) 进行图分类的新方法。假设一个定义了图及其类标签的联合概率分布的 GGM，我推导了计算给定图的类标签概率的分类公式。可以使用新的条件 ELBO 来训练生成图自编码器模型进行区分。虽然利用生成模型进行分类在非关系 i.i.d. 数据中已经得到了很好的研究，但据我们所知，这是一种图分类的新方法。

    This note describes a new approach to classifying graphs that leverages graph generative models (GGM). Assuming a GGM that defines a joint probability distribution over graphs and their class labels, I derive classification formulas for the probability of a class label given a graph. A new conditional ELBO can be used to train a generative graph auto-encoder model for discrimination. While leveraging generative models for classification has been well explored for non-relational i.i.d. data, to our knowledge it is a novel approach to graph classification.
    
[^142]: DNArch: 通过反向传播学习卷积神经网络的可学习架构

    DNArch: Learning Convolutional Neural Architectures by Backpropagation. (arXiv:2302.05400v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05400](http://arxiv.org/abs/2302.05400)

    DNArch是一种通过反向传播同时学习卷积神经网络的权重和架构的方法，它不仅可以学习每一层的卷积核大小和通道数，还可以学习网络的深度和下采样层的位置和值。与现有方法不同的是，DNArch不限于预定义的神经组件，能够发现各种核大小、宽度、深度和下采样组合中的整个CNN架构。在实验中，DNArch在多个分类和密集预测任务上找到了高性能的CNN架构。

    

    我们提出了Differentiable Neural Architectures (DNArch)，一种通过反向传播同时学习卷积神经网络(CNNs)的权重和架构的方法。具体而言，DNArch允许学习(i)每一层的卷积核大小，(ii)每一层的通道数，(iii)下采样层的位置和值，以及(iv)网络的深度。为此，DNArch将神经架构视为连续的多维实体，并使用可学习的可微掩码来控制其大小。与现有方法不同，DNArch不限于预定义的可能神经组件集，而是能够发现在所有可行的核大小、宽度、深度和下采样组合中的整个CNN架构。实验证明，DNArch能够为顺序和图像数据的多个分类和密集预测任务找到有效的CNN架构。当与控制架构大小的损失项相结合时，DNArch还能在运行时间和模型性能之间实现有效的权衡。

    We present Differentiable Neural Architectures (DNArch), a method that jointly learns the weights and the architecture of Convolutional Neural Networks (CNNs) by backpropagation. In particular, DNArch allows learning (i) the size of convolutional kernels at each layer, (ii) the number of channels at each layer, (iii) the position and values of downsampling layers, and (iv) the depth of the network. To this end, DNArch views neural architectures as continuous multidimensional entities, and uses learnable differentiable masks along each dimension to control their size. Unlike existing methods, DNArch is not limited to a predefined set of possible neural components, but instead it is able to discover entire CNN architectures across all feasible combinations of kernel sizes, widths, depths and downsampling. Empirically, DNArch finds performant CNN architectures for several classification and dense prediction tasks on sequential and image data. When combined with a loss term that controls t
    
[^143]: 在新闻文章中检测有害议程

    Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.00102](http://arxiv.org/abs/2302.00102)

    这项研究提出了一种新的任务，即在新闻文章中检测有害议程，并发布了一个新闻文章注释数据集以供研究使用。研究者展示了可解释系统在这一任务上的有效性，并证明它们可以和黑盒模型有相当的表现。

    

    在线上操纵新闻是一个日益严重的问题，需要使用自动化系统来遏制其传播。我们认为，虽然误导信息和虚假信息的检测已经得到研究，但在检测新闻文章中的有害议程这一重要挑战方面缺乏投资；识别有害议程对于识别具有最大潜在现实危害的新闻运动至关重要。此外，由于对审查制度存在真实的担忧，有害议程检测器必须具有可解释性才能发挥作用。在这项工作中，我们提出了这一全新的任务，并发布了一个名为NewsAgendas的新闻文章注释数据集，用于议程识别。我们展示了可解释系统在这一任务上的有效性，并证明它们可以与黑盒模型具有相当的表现。

    Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NewsAgendas, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.
    
[^144]: 面向高效梯度为基础的值估计

    Toward Efficient Gradient-Based Value Estimation. (arXiv:2301.13757v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13757](http://arxiv.org/abs/2301.13757)

    本研究研究了梯度为基础的值估计方法慢的根本原因，并提出了一种低复杂度的方法以解决损失函数带来的不良影响，该方法在效率上比剩余梯度方法更快，几乎具有相同的计算复杂度，并且在经典问题上与TD具有竞争力。

    

    强化学习中基于梯度的值估计方法具有良好的稳定性，但通常比时间差异（TD）学习方法慢得多。我们研究了这种缓慢的根本原因，并表明均方贝尔曼误差（MSBE）是一种病态的损失函数，其黑塞矩阵具有较大的条件数。为了解决MSBE的不良条件对基于梯度的方法的负面影响，我们提出了一种低复杂度的无批处理近端方法，它近似遵循高斯牛顿方向，并在参数化方面渐近鲁棒。我们的主要算法称为RANS，它在效率上比剩余梯度方法更快，几乎具有相同的计算复杂度，并且在我们测试的经典问题上与TD具有竞争力。

    Gradient-based methods for value estimation in reinforcement learning have favorable stability properties, but they are typically much slower than Temporal Difference (TD) learning methods. We study the root causes of this slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned loss function in the sense that its Hessian has large condition-number. To resolve the adverse effect of poor conditioning of MSBE on gradient based methods, we propose a low complexity batch-free proximal method that approximately follows the Gauss-Newton direction and is asymptotically robust to parameterization. Our main algorithm, called RANS, is efficient in the sense that it is significantly faster than the residual gradient methods while having almost the same computational complexity, and is competitive with TD on the classic problems that we tested.
    
[^145]: ScaDLES: 边缘端流式数据上的可扩展深度学习

    ScaDLES: Scalable Deep Learning over Streaming data at the Edge. (arXiv:2301.08897v1 [cs.DC] CROSS LISTED)

    [http://arxiv.org/abs/2301.08897](http://arxiv.org/abs/2301.08897)

    ScaDLES是一种用于边缘端流式数据上的可扩展深度学习方法，解决了系统和统计异质性的挑战。

    

    分布式深度学习（DDL）训练系统设计用于云和数据中心环境，假设具有均匀计算资源、高网络带宽、足够的内存和存储，以及在所有节点上独立和同分布的数据。然而，这些假设在边缘端不一定适用，特别是在在线方式下训练神经网络时。边缘计算面临系统和统计异质性的挑战。系统异质性归因于每个设备的计算资源和带宽的差异，而统计异质性则来自于边缘端的不平衡和偏斜数据。在处理流式数据时，设备之间的不同流速也可以成为异质性的另一个来源。如果流速低于训练批量大小，设备需要等待足够的样本流入后才能执行一次随机梯度下降（SGD）迭代。

    Distributed deep learning (DDL) training systems are designed for cloud and data-center environments that assumes homogeneous compute resources, high network bandwidth, sufficient memory and storage, as well as independent and identically distributed (IID) data across all nodes. However, these assumptions don't necessarily apply on the edge, especially when training neural networks on streaming data in an online manner. Computing on the edge suffers from both systems and statistical heterogeneity. Systems heterogeneity is attributed to differences in compute resources and bandwidth specific to each device, while statistical heterogeneity comes from unbalanced and skewed data on the edge. Different streaming-rates among devices can be another source of heterogeneity when dealing with streaming data. If the streaming rate is lower than training batch-size, device needs to wait until enough samples have streamed in before performing a single iteration of stochastic gradient descent (SGD).
    
[^146]: CLIPTER: 在场景文本识别中关注更大的背景信息

    CLIPTER: Looking at the Bigger Picture in Scene Text Recognition. (arXiv:2301.07464v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07464](http://arxiv.org/abs/2301.07464)

    本研究提出了CLIPTER（CLIP文本识别）框架，利用现代视觉语言模型提供整个图像的信息，通过门控交叉注意机制融合到裁剪的文本图像识别器中。结果表明，这种方法在多个基准测试上取得了最先进的效果。

    

    在现实世界的场景中阅读文本通常需要理解其周围的上下文，尤其是在处理质量较差的文本时。然而，当前的场景文本识别器在处理裁剪的文本图像时并不了解更大的背景信息。在这项研究中，我们利用现代视觉语言模型（如CLIP）的代表性能力，将场景级别的信息提供给基于裁剪的识别器。我们通过一个门控交叉注意机制，将整个图像的丰富表示（来自视觉语言模型）与识别器的单词级特征融合。该组件逐渐转向增强上下文的表示，从而实现对预训练识别器的稳定微调。我们在领先的文本识别架构上展示了我们的模型无关框架CLIPTER（CLIP文本识别）的有效性，并在多个基准测试中取得了最先进的结果。此外，我们的分析突出了改进了的...

    Reading text in real-world scenarios often requires understanding the context surrounding it, especially when dealing with poor-quality text. However, current scene text recognizers are unaware of the bigger picture as they operate on cropped text images. In this study, we harness the representative capabilities of modern vision-language models, such as CLIP, to provide scene-level information to the crop-based recognizer. We achieve this by fusing a rich representation of the entire image, obtained from the vision-language model, with the recognizer word-level features via a gated cross-attention mechanism. This component gradually shifts to the context-enhanced representation, allowing for stable fine-tuning of a pretrained recognizer. We demonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIP TExt Recognition), on leading text recognition architectures and achieve state-of-the-art results across multiple benchmarks. Furthermore, our analysis highlights improved 
    
[^147]: 深度注入先验用于反散射

    Deep Injective Prior for Inverse Scattering. (arXiv:2301.03092v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03092](http://arxiv.org/abs/2301.03092)

    本文提出了一种基于深度生成模型的反散射数据驱动框架，通过学习低维流形作为正则化器，只需要目标介电常数进行训练，并引入了贝叶斯框架来近似后验分布。

    

    在电磁反散射中，目标是利用散射波来重建物体介电常数。深度学习已经显示出作为迭代求解器替代方法的潜力，但它主要应用于散射场的分布漂移敏感的监督框架。此外，这些方法通常只提供一个介电常数模式的估计，可能因噪声和问题的病态性而不足或引导错误。本文提出了一种基于深度生成模型的反散射数据驱动框架。我们的方法学习一个低维流形作为恢复目标介电常数的正则化器。与需要散射场和目标介电常数作为训练输入的监督方法不同，我们的方法只需要目标介电常数进行训练，然后可以与任何实验设置一起使用。我们还引入了一种贝叶斯框架来近似后验分布。

    In electromagnetic inverse scattering, the goal is to reconstruct object permittivity using scattered waves. While deep learning has shown promise as an alternative to iterative solvers, it is primarily used in supervised frameworks which are sensitive to distribution drift of the scattered fields, common in practice. Moreover, these methods typically provide a single estimate of the permittivity pattern, which may be inadequate or misleading due to noise and the ill-posedness of the problem. In this paper, we propose a data-driven framework for inverse scattering based on deep generative models. Our approach learns a low-dimensional manifold as a regularizer for recovering target permittivities. Unlike supervised methods that necessitate both scattered fields and target permittivities, our method only requires the target permittivities for training; it can then be used with any experimental setup. We also introduce a Bayesian framework for approximating the posterior distribution of t
    
[^148]: 可证明的无重置强化学习——基于无懊悔规约的方法

    Provable Reset-free Reinforcement Learning by No-Regret Reduction. (arXiv:2301.02389v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02389](http://arxiv.org/abs/2301.02389)

    该论文提出了一个通用的无懊悔规约方法，用于解决强化学习中重置机制带来的实际应用问题，提出了第一个可证明正确性的无重置强化学习算法。

    

    迄今为止，强化学习在现实世界中的应用非常有限。一个关键挑战是，典型的强化学习算法严重依赖于重置机制来采样合适的初始状态；在实践中，由于需要人工干预或者复杂的环境工程，这种重置机制的实现成本较高。为了使学习更加实用，我们提出了一个通用的无懊悔规约方法来系统地设计无重置强化学习算法。我们的规约将无重置强化学习问题转化为一个两人博弈问题。我们证明，在这个两人博弈中实现亚线性懊悔将意味着在原始的强化学习问题中学习到具有亚线性性能悔恨和亚线性重置总次数的策略。这意味着智能体最终学会了最优化行为并避免重置。为了展示这个方法的有效性，我们设计了一个线性马尔可夫决策过程的实例，这是第一个具有可证明正确性的无重置强化学习算法。

    Reinforcement learning (RL) so far has limited real-world applications. One key challenge is that typical RL algorithms heavily rely on a reset mechanism to sample proper initial states; these reset mechanisms, in practice, are expensive to implement due to the need for human intervention or heavily engineered environments. To make learning more practical, we propose a generic no-regret reduction to systematically design reset-free RL algorithms. Our reduction turns the reset-free RL problem into a two-player game. We show that achieving sublinear regret in this two-player game would imply learning a policy that has both sublinear performance regret and sublinear total number of resets in the original RL problem. This means that the agent eventually learns to perform optimally and avoid resets. To demonstrate the effectiveness of this reduction, we design an instantiation for linear Markov decision processes, which is the first provably correct reset-free RL algorithm.
    
[^149]: 反向课程强化学习

    Backward Curriculum Reinforcement Learning. (arXiv:2212.14214v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.14214](http://arxiv.org/abs/2212.14214)

    这项工作提出了一种新颖的反向课程强化学习方法，通过使用回放轨迹而不是原始的前向轨迹来训练智能体。这种方法通过提供强有力的奖励信号实现了更高效的学习，而且只需要进行微小的算法改变。

    

    当前强化学习算法使用前向生成轨迹来训练智能体，这种方法提供的指导不足以使智能体进行尽可能多的探索。尽管我们认识到强化学习结果来自充分的探索，但这种方法在样本效率上存在折衷，这是影响算法性能的重要因素。以往的方法使用奖励塑造技术和网络结构修改来增加样本效率，但这些方法需要很多步骤来实现。在本文中，我们提出了一种新颖的反向课程强化学习方法，即通过使用回放轨迹而不是原始的前向轨迹来训练智能体。这种方法为智能体提供了强有力的奖励信号，从而实现更高效的学习。此外，我们的方法只需要在智能体训练之前对轨迹的顺序进行微小的改变，使得实现起来更加直接。

    Current reinforcement learning algorithms train an agent using forward-generated trajectories, which provide little guidance so that the agent can explore as much as possible. While realizing the value of reinforcement learning results from sufficient exploration, this approach leads to a trade-off in losing sample efficiency, an essential factor impacting algorithm performance. Previous tasks use reward-shaping techniques and network structure modification to increase sample efficiency. However, these methods require many steps to implement. In this work, we propose novel backward curriculum reinforcement learning that begins training the agent using the backward trajectory of the episode instead of the original forward trajectory. This approach provides the agent with a strong reward signal, enabling more sample-efficient learning. Moreover, our method only requires a minor change in the algorithm of reversing the order of the trajectory before agent training, allowing a straightforw
    
[^150]: 混合多通道稀疏信号恢复

    Shuffled Multi-Channel Sparse Signal Recovery. (arXiv:2212.07368v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2212.07368](http://arxiv.org/abs/2212.07368)

    本文研究了混合多通道稀疏信号恢复问题，将其视为信号重建问题并建立了唯一恢复的条件。对于这个问题，尚未考虑过相关的采样结果，并且现有的无标签感知方法无法直接应用。

    

    多个真实应用中常常存在样本与其对应的通道或目标之间的不匹配。为了系统地解决这个问题，我们将其作为一个信号重建问题来提出，其中样本与其对应的通道之间的对应关系丢失了。在假设我们有一个感知矩阵用于底层信号的情况下，我们展示了该问题等价于一个结构化的无标签感知问题，并建立了唯一恢复的充分条件。据我们所知，文献中尚未考虑过混合多通道信号重建的采样结果，现有的无标签感知方法也无法直接应用。我们将结果扩展到信号允许稀疏表示的情况。

    Mismatches between samples and their respective channel or target commonly arise in several real-world applications. For instance, whole-brain calcium imaging of freely moving organisms, multiple-target tracking or multi-person contactless vital sign monitoring may be severely affected by mismatched sample-channel assignments. To systematically address this fundamental problem, we pose it as a signal reconstruction problem where we have lost correspondences between the samples and their respective channels. Assuming that we have a sensing matrix for the underlying signals, we show that the problem is equivalent to a structured unlabeled sensing problem, and establish sufficient conditions for unique recovery. To the best of our knowledge, a sampling result for the reconstruction of shuffled multi-channel signals has not been considered in the literature and existing methods for unlabeled sensing cannot be directly applied. We extend our results to the case where the signals admit a spa
    
[^151]: 进化聚类是否有理论保证?

    Can Evolutionary Clustering Have Theoretical Guarantees?. (arXiv:2212.01771v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2212.01771](http://arxiv.org/abs/2212.01771)

    本文证明了进化聚类算法 GSEMO 可以在理论上保证解决四种聚类问题，并介绍了在离散 k-中位数聚类中的公平聚类问题。

    

    聚类是许多领域中的一个基本问题，其目标是基于某种距离度量将给定的数据集分成不同的组，使得同一组中的数据点相似而不同组中的数据点不相似。由于其重要性和 NP 困难性，已经提出了许多方法，其中进化算法是一类常用的方法。进化聚类已经在许多应用中取得了成功，但所有的结果都是经验主义的，缺乏理论支持。本文通过证明 GSEMO（一种简单的多目标进化算法）在解决四种聚类形式：k-总距离最小化、k-中心、离散 k-中位数和 k-均值时，其近似性能可以在理论上得到保证，填补了这一空白。此外，我们考虑了公平的聚类问题，即尽量避免算法的偏见，这在机器学习中最近成为一个重要的研究课题。我们证明了在离散 k-中位数聚类下的公平聚类。

    Clustering is a fundamental problem in many areas, which aims to partition a given data set into groups based on some distance measure, such that the data points in the same group are similar while that in different groups are dissimilar. Due to its importance and NP-hardness, a lot of methods have been proposed, among which evolutionary algorithms are a class of popular ones. Evolutionary clustering has found many successful applications, but all the results are empirical, lacking theoretical support. This paper fills this gap by proving that the approximation performance of the GSEMO (a simple multi-objective evolutionary algorithm) for solving four formulations of clustering, i.e., $k$-tMM, $k$-center, discrete $k$-median and $k$-means, can be theoretically guaranteed. Furthermore, we consider clustering under fairness, which tries to avoid algorithmic bias, and has recently been an important research topic in machine learning. We prove that for discrete $k$-median clustering under 
    
[^152]: 基于闭合形式策略改进算子的离线强化学习

    Offline Reinforcement Learning with Closed-Form Policy Improvement Operators. (arXiv:2211.15956v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15956](http://arxiv.org/abs/2211.15956)

    本文提出了基于行为约束的离线强化学习中的闭合形式策略改进算子，该算子将行为策略建模为高斯混合，利用LogSumExp的下界和Jensen不等式克服了优化困难，能有效处理实际数据集中的异构策略。

    

    行为约束策略优化已被证明是解决离线强化学习问题的一种成功的范式。本文提出了我们的闭合形式策略改进算子。我们发现，行为约束自然地激励了使用一阶泰勒近似，从而导致了策略目标的线性近似。此外，由于实际数据集通常由异构策略收集而来，我们将行为策略建模为高斯混合，并利用LogSumExp的下界和Jensen不等式克服了引起优化困难的问题，从而得到了闭式策略改进算子。我们使用我们的新颖策略改进算子来实例化离线RL算法，并在实验中展示了它们的效果。

    Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate offline RL algorithms with our novel policy improvement operators and empirically demonstrate their e
    
[^153]: 快速采样扩散模型通过算子学习

    Fast Sampling of Diffusion Models via Operator Learning. (arXiv:2211.13449v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13449](http://arxiv.org/abs/2211.13449)

    本文通过使用神经算子加速扩散模型的采样过程，并提出了一种并行解码方法来生成图像。通过引入时间卷积层建模轨迹上的时间相关性，我们展示了方法的实现。

    

    扩散模型在各个领域都得到了广泛应用。然而，由于扩散模型需要通过数百到数千次网络评估来模拟由微分方程定义的连续过程，所以其采样过程较慢。在这项工作中，我们使用神经算子，一种高效解决概率流微分方程的方法，来加速扩散模型的采样过程。与其他具有顺序性质的快速采样方法不同，我们首次提出了一种并行解码方法，只需进行一次模型前向传递即可生成图像。我们提出了扩散模型采样与神经算子（DSNO），将初始条件（即高斯分布）映射到逆扩散过程的连续时间解轨迹。为了建模轨迹上的时间相关性，我们将参数化傅立叶空间中的时间卷积层引入给定扩散模型的主干。我们展示了我们的方法的实现。

    Diffusion models have found widespread adoption in various areas. However, their sampling process is slow because it requires hundreds to thousands of network evaluations to emulate a continuous process defined by differential equations. In this work, we use neural operators, an efficient method to solve the probability flow differential equations, to accelerate the sampling process of diffusion models. Compared to other fast sampling methods that have a sequential nature, we are the first to propose a parallel decoding method that generates images with only one model forward pass. We propose diffusion model sampling with neural operator (DSNO) that maps the initial condition, i.e., Gaussian distribution, to the continuous-time solution trajectory of the reverse diffusion process. To model the temporal correlations along the trajectory, we introduce temporal convolution layers that are parameterized in the Fourier space into the given diffusion model backbone. We show our method achiev
    
[^154]: 单次对比学习可以适用于同构和异构图

    Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph. (arXiv:2211.10890v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10890](http://arxiv.org/abs/2211.10890)

    该论文介绍了一种单次对比学习方法，可适用于同构和异构图，并给出了性能保证。

    

    现有的图对比学习（GCL）技术通常需要两次前向传递才能构建产生对比的损失，这对于捕捉节点特征的低频信号是有效的。这种双通道设计已经在同构图上表现出实证的成功，但它在异构图上的有效性，其中直接连接的节点通常具有不同的标签，尚不清楚。此外，现有的GCL方法无法提供强有力的性能保证。受到GCL方法在异构图上的不可预测性的影响，它们在实际应用中的适用性受到限制。于是，一个自然的问题就出现了：我们能否设计一种适用于同构和异构图的GCL方法，并具有性能保证？为了回答这个问题，我们在理论上研究了在同构和异构图上通过邻域聚合得到的特征的集中性质，引入了单通道图对比学习损失。

    Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss
    
[^155]: 翻译：利用风格分类来检测失落的《Midrash Tanhuma》材料

    Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09710](http://arxiv.org/abs/2211.09710)

    本文提出了一种拉比文学的分类系统，可以通过其风格来检测Midrash Tanhuma中的失落材料。

    

    Midrash集合是复杂的拉比文献作品，由多种语言的文本组成，经过不稳定的口头和书面传递过程演变而来。确定这种合集中的一个给定段落的起源并不总是直观的，常常是学者之间的争议，然而对于学者们理解段落及其与拉比文集中其他文本的关系至关重要。为了解决这个问题，我们提出了一个基于风格的拉比文学分类系统，利用最近发布的针对希伯来语的预训练Transformer模型。此外，我们展示了如何利用我们的方法来发现失落的Midrash Tanhuma材料。

    Midrash collections are complex rabbinic works that consist of text in multiple languages, which evolved through long processes of unstable oral and written transmission. Determining the origin of a given passage in such a compilation is not always straightforward and is often a matter of dispute among scholars, yet it is essential for scholars' understanding of the passage and its relationship to other texts in the rabbinic corpus.  To help solve this problem, we propose a system for classification of rabbinic literature based on its style, leveraging recently released pretrained Transformer models for Hebrew. Additionally, we demonstrate how our method can be applied to uncover lost material from Midrash Tanhuma.
    
[^156]: 学习增强的B树

    Learning-Augmented B-Trees. (arXiv:2211.09251v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2211.09251](http://arxiv.org/abs/2211.09251)

    这是一个学习增强的B树，通过使用具有复合优先级的Treaps，每个项目的深度由其预测权重确定，推广了最近的学习增强BST，并且是第一个可以利用访问序列中的局部性的B树数据结构。

    

    本研究通过使用具有复合优先级的Treaps来研究学习增强的二叉搜索树（BST）和B树。结果是一个简单的搜索树，其中每个项目的深度由其预测权重$w_x$确定。为了实现这个结果，每个项目$x$都有其复合优先级$-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$，其中$U(0, 1)$是均匀分布的随机变量。这将最近的学习增强BST（Lin-Luo-Woodruff ICML`22）推广到任意输入和预测，而不仅仅适用于Zipfian分布。它还提供了第一个可以根据访问序列中的局部性进行在线自我重组的B树数据结构。该数据结构对于预测错误是健壮的，可以处理插入、删除以及预测更新。

    We study learning-augmented binary search trees (BSTs) and B-Trees via Treaps with composite priorities. The result is a simple search tree where the depth of each item is determined by its predicted weight $w_x$. To achieve the result, each item $x$ has its composite priority $-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$ where $U(0, 1)$ is the uniform random variable. This generalizes the recent learning-augmented BSTs [Lin-Luo-Woodruff ICML`22], which only work for Zipfian distributions, to arbitrary inputs and predictions. It also gives the first B-Tree data structure that can provably take advantage of localities in the access sequence via online self-reorganization. The data structure is robust to prediction errors and handles insertions, deletions, as well as prediction updates.
    
[^157]: 扩大放射学报告摘要范围：多个解剖学和模态的综述

    Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. (arXiv:2211.08584v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08584](http://arxiv.org/abs/2211.08584)

    本论文针对放射学报告摘要存在的限制，提出了一个新的数据集MIMIC-RRS，包含多个解剖学和模态。通过在数据集上进行实验和临床评估，我们旨在扩大放射学报告摘要的应用范围。

    

    放射学报告摘要（RRS）是一个不断发展的研究领域。给定放射学报告的发现部分，目标是生成一个概述（称为印象部分），突出放射学研究的关键观察和结论。然而，RRS目前面临着重要的限制。首先，许多先前的研究使用私有数据集进行实验，无法重现结果并在不同系统和解决方案之间进行公平比较。其次，大多数先前的方法仅在胸部X射线上进行评估。为了解决这些限制，我们提出了一个数据集（MIMIC-RRS），涉及MIMIC-III和MIMIC-CXR数据集的三种新的模态和七种新的解剖学。然后，我们进行了大量实验，评估了模型在MIMIC-RRS中的模态-解剖学对内和对外的性能。此外，我们通过RadGraph评估它们的临床功效，这是一个事实正确性指标。

    Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential limitations.First, many prior studies conduct experiments on private datasets, preventing reproduction of results and fair comparisons across different systems and solutions. Second, most prior approaches are evaluated solely on chest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS) involving three new modalities and seven new anatomies based on the MIMIC-III and MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the performance of models both within and across modality-anatomy pairs in MIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a factual correctness metric.
    
[^158]: C3: 跨实例引导对比聚类

    C3: Cross-instance guided Contrastive Clustering. (arXiv:2211.07136v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07136](http://arxiv.org/abs/2211.07136)

    C3是一种引入了交叉实例关系的对比聚类方法，通过考虑实例之间的关系增加了正对数，从而提高了聚类性能。

    

    聚类是将相似的数据样本聚集到无需预定义标签的簇中的任务。它在机器学习文献中得到了广泛研究，最近深度学习的进展重新引起了人们对该领域的兴趣。对比聚类（CC）模型是深度聚类的一个重要组成部分，通过数据增强生成每个数据实例的正负对。CC模型的目标是学习一个将正对实例的实例级和聚类级表示分组在一起的特征空间。尽管提高了SOTA，但这些算法忽略了携带有改进聚类性能的交叉实例模式。这增加了模型的错误负对数，同时降低了它的真正正对数。在本文中，我们提出了一种新颖的对比聚类方法，Cross-instance guided Contrastive Clustering (C3)，考虑了样本之间的关系以增加正对数。

    Clustering is the task of gathering similar data samples into clusters without using any predefined labels. It has been widely studied in machine learning literature, and recent advancements in deep learning have revived interest in this field. Contrastive clustering (CC) models are a staple of deep clustering in which positive and negative pairs of each data instance are generated through data augmentation. CC models aim to learn a feature space where instance-level and cluster-level representations of positive pairs are grouped together. Despite improving the SOTA, these algorithms ignore the cross-instance patterns, which carry essential information for improving clustering performance. This increases the false-negative-pair rate of the model while decreasing its true-positive-pair rate. In this paper, we propose a novel contrastive clustering method, Cross-instance guided Contrastive Clustering (C3), that considers the cross-sample relationships to increase the number of positive p
    
[^159]: 贝叶斯线性模型中具有有限样本FDR控制的近似最优多重检验

    Near-optimal multiple testing in Bayesian linear models with finite-sample FDR control. (arXiv:2211.02778v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.02778](http://arxiv.org/abs/2211.02778)

    本文针对高维贝叶斯线性模型的多重检验问题，开发了近似最优的多重检验程序，能够在有限样本下控制频率FDR，并能达到近似最优功率。

    

    在高维变量选择问题中，统计学家通常致力于设计能控制虚警发现率（FDR）的多重检验过程，同时识别更多相关变量。模型-X方法（如Knockoffs和条件随机化检验）在假设已知协变量分布的情况下，实现了有限样本FDR控制的主要目标。然而，这些方法是否还能实现最大化发现的次要目标仍不确定。事实上，设计具有有限样本FDR控制并发现更多相关变量的程序，在可能是最简单的线性模型中，仍然是一个基本开放问题。本文针对高维贝叶斯线性模型与各向同性协变量，开发了接近最优的多重检验程序。我们引入了能在有限样本下保证频率FDR控制的模型-X程序，即使模型被错误指定，也能达到近似最优功率。

    In high dimensional variable selection problems, statisticians often seek to design multiple testing procedures that control the False Discovery Rate (FDR), while concurrently identifying a greater number of relevant variables. Model-X methods, such as Knockoffs and conditional randomization tests, achieve the primary goal of finite-sample FDR control, assuming a known distribution of covariates. However, whether these methods can also achieve the secondary goal of maximizing discoveries remains uncertain. In fact, designing procedures to discover more relevant variables with finite-sample FDR control is a largely open question, even within the arguably simplest linear models.  In this paper, we develop near-optimal multiple testing procedures for high dimensional Bayesian linear models with isotropic covariates. We introduce Model-X procedures that provably control the frequentist FDR from finite samples, even when the model is misspecified, and conjecturally achieve near-optimal powe
    
[^160]: 异方差分布上的神经主动学习

    Neural Active Learning on Heteroskedastic Distributions. (arXiv:2211.00928v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00928](http://arxiv.org/abs/2211.00928)

    这项研究展示了在异方差分布上进行神经主动学习可能导致灾难性失败，并提出了一种利用微调来减轻这种失败的方法。

    

    能够主动寻找最佳质量训练数据的模型承诺着更准确、适应性强和高效的机器学习。主动学习技术通常倾向于选择最难分类的例子。尽管这在同质数据集上效果良好，但我们发现在多个具有不同程度标签噪声或异方差性的分布上进行主动学习可能会导致灾难性失败。这些主动学习算法强烈倾向于从噪声更大的分布中选择，即使这些例子没有信息结构（例如具有随机标签的纯色图像）。为此，我们展示了这些主动学习算法在异方差分布上的灾难性失败，并提出了一种基于微调的方法来减轻这些失败。此外，我们还提出了一种新的算法，该算法为每个数据点引入了模型差异评分函数，用于去除噪声例子并采样清晰的例子。

    Models that can actively seek out the best quality training data hold the promise of more accurate, adaptable, and efficient machine learning. Active learning techniques often tend to prefer examples that are the most difficult to classify. While this works well on homogeneous datasets, we find that it can lead to catastrophic failures when performed on multiple distributions with different degrees of label noise or heteroskedasticity. These active learning algorithms strongly prefer to draw from the distribution with more noise, even if their examples have no informative structure (such as solid color images with random labels). To this end, we demonstrate the catastrophic failure of these active learning algorithms on heteroskedastic distributions and propose a fine-tuning-based approach to mitigate these failures. Further, we propose a new algorithm that incorporates a model difference scoring function for each data point to filter out the noisy examples and sample clean examples th
    
[^161]: Lipschitz正则化梯度流和高维稀缺数据的生成粒子算法

    Lipschitz-regularized gradient flows and generative particle algorithms for high-dimensional scarce data. (arXiv:2210.17230v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.17230](http://arxiv.org/abs/2210.17230)

    构建了一种新的生成算法类，能够有效地学习稀缺高维数据的任意目标分布并生成新样本，具有很好的数据整合能力。

    

    我们构建了一种新的生成算法类，能够有效地从可能稀缺、高维的数据中学习任意目标分布，并生成新的样本。这些生成算法是基于粒子的，并且是通过Lipschitz正则化Kullback-Leibler或其他f-散度的梯度流来构造的，其中来自源分布的数据可以稳定地作为粒子传输到目标分布的附近。作为数据整合的一个突出结果，我们证明了所提出的算法可以正确传输维数超过54K的基因表达数据点，而样本量通常只有几百个。

    We build a new class of generative algorithms capable of efficiently learning an arbitrary target distribution from possibly scarce, high-dimensional data and subsequently generate new samples. These generative algorithms are particle-based and are constructed as gradient flows of Lipschitz-regularized Kullback-Leibler or other $f$-divergences, where data from a source distribution can be stably transported as particles, towards the vicinity of the target distribution. As a highlighted result in data integration, we demonstrate that the proposed algorithms correctly transport gene expression data points with dimension exceeding 54K, while the sample size is typically only in the hundreds.
    
[^162]: 典型可学习任务空间的图像

    A picture of the space of typical learnable tasks. (arXiv:2210.17011v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17011](http://arxiv.org/abs/2210.17011)

    我们使用信息几何技术研究了在不同任务上训练时深度网络学习到的表示，发现任务空间的结构与Wordnet系统进化树的某些部分一致，并且监督学习在一个任务上的进展可以在其他任务上产生一定的影响。

    

    我们利用信息几何技术来理解深度网络在使用监督学习、元学习、半监督学习和对比学习训练在不同任务上学到的表示。我们揭示了与任务空间结构相关的以下现象：(1)使用不同表示学习方法在不同任务上训练的概率模型流形实际上是低维的；(2)在一个任务上进行监督学习即使在表面上看起来是不相似的任务上也能取得出乎意料的进展；如果训练任务具有多样的类别，则其在其他任务上的进展更大；(3)通过我们的分析所指示的任务空间结构与Wordnet系统进化树中的某些部分一致；(4)在训练过程中，情境元学习算法和监督学习遵循不同的轨迹，但最终适应相似的模型；(5)对比学习和半监督学习方法遵循类似的轨迹。

    We develop information geometric techniques to understand the representations learned by deep networks when they are trained on different tasks using supervised, meta-, semi-supervised and contrastive learning. We shed light on the following phenomena that relate to the structure of the space of tasks: (1) the manifold of probabilistic models trained on different tasks using different representation learning methods is effectively low-dimensional; (2) supervised learning on one task results in a surprising amount of progress even on seemingly dissimilar tasks; progress on other tasks is larger if the training task has diverse classes; (3) the structure of the space of tasks indicated by our analysis is consistent with parts of the Wordnet phylogenetic tree; (4) episodic meta-learning algorithms and supervised learning traverse different trajectories during training but they fit similar models eventually; (5) contrastive and semi-supervised learning methods traverse trajectories similar
    
[^163]: 物理感知图神经网络用于精确的RNA 3D结构预测

    Physics-aware Graph Neural Network for Accurate RNA 3D Structure Prediction. (arXiv:2210.16392v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16392](http://arxiv.org/abs/2210.16392)

    本研究提出了一种基于图神经网络的评分函数PaxNet，通过训练已解决的RNA 3D结构上的原子类型和坐标来预测准确的结构模型。PaxNet模拟了局部和非局部相互作用，并包含注意力融合模块，它能学习每种相互作用类型的个体贡献以进行最终预测。结果表明，PaxNet在RNA 3D结构预测任务中明显优于其他方法。

    

    RNA的生物功能由其三维（3D）结构决定。因此，鉴于实验确定的RNA结构有限，预测RNA结构将有助于阐明RNA功能和RNA靶向药物发现，但仍然是一项具有挑战性的任务。在本研究中，我们提出了一种基于图神经网络（GNN）的评分函数，仅通过已解决的RNA 3D结构上的原子类型和坐标进行训练，以区分准确的结构模型。所提出的物理感知多重图神经网络（PaxNet）分别模拟了局部和非局部相互作用，受分子力学启发。此外，PaxNet还包含基于注意力的融合模块，用于学习每种相互作用类型的个体贡献，以进行最终预测。我们在两个基准测试上对PaxNet的性能进行了严格评估，并将其与几个最先进的基线方法进行了比较。结果表明，PaxNet明显优于其他方法。

    Biological functions of RNAs are determined by their three-dimensional (3D) structures. Thus, given the limited number of experimentally determined RNA structures, the prediction of RNA structures will facilitate elucidating RNA functions and RNA-targeted drug discovery, but remains a challenging task. In this work, we propose a Graph Neural Network (GNN)-based scoring function trained only with the atomic types and coordinates on limited solved RNA 3D structures for distinguishing accurate structural models. The proposed Physics-aware Multiplex Graph Neural Network (PaxNet) separately models the local and non-local interactions inspired by molecular mechanics. Furthermore, PaxNet contains an attention-based fusion module that learns the individual contribution of each interaction type for the final prediction. We rigorously evaluate the performance of PaxNet on two benchmarks and compare it with several state-of-the-art baselines. The results show that PaxNet significantly outperforms
    
[^164]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^165]: 用于不确定性感知模型预测控制的离散时间动力学的主动学习

    Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model Predictive Control. (arXiv:2210.12583v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.12583](http://arxiv.org/abs/2210.12583)

    本文提出了一种用于主动学习非线性机器人系统动力学的方法，结合了离线和在线学习，能够在实时中准确推断模型动力学，并设计了一种不确定性感知模型预测控制器。

    

    模型驱动的控制需要对系统动力学进行准确建模，以便在复杂和动态环境中精确且安全地控制机器人。此外，在操作条件变化的情况下，模型应该不断调整以弥补动力学变化。本文提出了一种主动学习方法来主动建模非线性机器人系统的动力学。我们结合了离线学习以往经验和在线学习当前机器人与未知环境的交互。这两个因素使得学习过程高效且自适应，能够在实时中准确推断模型动力学，即使在大大不同于训练分布的操作范围内也可行。此外，我们设计了一种对学习到的动力学的aleatoric（数据）不确定性启发式条件的不确定性感知模型预测控制器。该控制器可以主动选择最优的控制动作。

    Model-based control requires an accurate model of the system dynamics for precisely and safely controlling the robot in complex and dynamic environments. Moreover, in the presence of variations in the operating conditions, the model should be continuously refined to compensate for dynamics changes. In this paper, we present a self-supervised learning approach that actively models the dynamics of nonlinear robotic systems. We combine offline learning from past experience and online learning from current robot interaction with the unknown environment. These two ingredients enable a highly sample-efficient and adaptive learning process, capable of accurately inferring model dynamics in real-time even in operating regimes that greatly differ from the training distribution. Moreover, we design an uncertainty-aware model predictive controller that is heuristically conditioned to the aleatoric (data) uncertainty of the learned dynamics. This controller actively chooses the optimal control act
    
[^166]: 对数线性保护性及其影响的研究

    Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10012](http://arxiv.org/abs/2210.10012)

    本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。

    

    已经发现，在假设可线性的神经表示中，从中删除可人解释的概念的方法是可行和有用的。然而，这种删除对于基于修改后表示进行训练的下游分类器行为的影响尚未完全理解。在这项工作中，我们正式定义了对数线性保护性的概念，即对手无法直接从表示中预测概念的能力，并研究其影响。我们证明，在二元情况下，在某些假设下，下游对数线性模型无法恢复被删除的概念。然而，我们证明，在某些情况下，可以构建一个多类对数线性模型，间接恢复概念，这指出了对数线性保护性作为下游偏差缓解技术的内在局限性。这些发现揭示了线性删除方法的理论限制，并强调了进一步研究可解释神经表示与分类器之间的联系的需要。

    Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
    
[^167]: 最小化损失的多层训练方法下的最优核算子学习

    Minimax Optimal Kernel Operator Learning via Multilevel Training. (arXiv:2209.14430v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14430](http://arxiv.org/abs/2209.14430)

    本文研究了学习两个无穷维Sobolev再生核希尔伯特空间之间的Hilbert-Schmidt算子的统计极限，在多层级训练方法下，通过学习偏差以下的谱分量和忽略方差以上的分量，可以达到最优的学习速率。

    

    在无穷维函数空间中学习映射已经在机器学习的许多领域中取得了经验上的成功，包括生成模型、函数数据分析、因果推断和多智能体强化学习。本文研究了学习两个无穷维Sobolev再生核希尔伯特空间之间的Hilbert-Schmidt算子的统计极限。我们建立了使用Sobolev Hilbert-Schmidt范数的信息理论下界，并展示了一个规则化方法，通过学习偏差轮廓以下的谱分量并忽略方差轮廓以上的分量，可以达到最优的学习速率。同时，偏差和方差轮廓之间的谱分量给我们在设计计算上可行的机器学习算法时提供了灵活性。基于这一观察，我们开发了一个多层级的核算子学习算法，当学习线性算子时它是最优的。

    Learning mappings between infinite-dimensional function spaces has achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones that are above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators betwee
    
[^168]: Pareto Actor-Critic用于多智能体强化学习中的均衡选择

    Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning. (arXiv:2209.14344v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14344](http://arxiv.org/abs/2209.14344)

    本文提出了Pareto Actor-Critic（Pareto-AC）算法来解决多智能体强化学习中的均衡选择问题，该算法利用无冲突游戏的性质，即Pareto最优均衡最大化了所有智能体的回报。实验结果显示Pareto-AC相比其他七种最先进的算法更能收敛到更高的回合回报。

    

    本文关注于在无冲突多智能体博弈中的均衡选择问题，具体研究了在多个现有均衡中选择Pareto最优均衡的问题。已经表明，许多最先进的多智能体强化学习算法由于每个智能体在训练过程中对其他智能体政策的不确定性而容易收敛到Pareto支配的均衡状态。为了解决次优均衡选择问题，我们提出了Pareto Actor-Critic（Pareto-AC），这是一种用于无冲突游戏（合作游戏的超集）的演员-评论家算法，其利用了一个简单的性质：无冲突游戏中的Pareto最优均衡最大化了所有智能体的回报，因此对于所有智能体来说是首选结果。我们在各种多智能体博弈中评估了Pareto-AC，并显示它收敛到更高的回合回报，与七种最先进的多智能体强化学习算法相比，Pareto-AC成功地收敛到了一个

    This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal equilibrium among several existing equilibria. It has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and therefore is the preferred outcome for all agents. We evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a
    
[^169]: 深度学习为基础的胸部X射线匿名化：维护患者隐私的效用保护措施

    Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy. (arXiv:2209.11531v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.11531](http://arxiv.org/abs/2209.11531)

    该论文提出了一种基于深度学习的胸部X射线匿名化方法（PriCheXy-Net），可以有针对性地去除生物特征信息，同时保持数据的诊断和机器学习效用。

    

    在发布用于研究目的的大型胸部X射线数据集之前，强大且可靠的胸部X射线匿名化是一个必要的步骤。传统的匿名化过程是通过在图像中模糊个人信息并删除或替换元信息来完成的。然而，这种简单的措施仍然保留了胸部X射线中的生物特征信息，使得患者可以通过关联攻击进行重新识别。因此，迫切需要混淆图像中出现的生物特征信息。我们提出了第一个基于深度学习的方法（PriCheXy-Net），可以有针对性地对胸部X射线进行匿名化，同时保持诊断和机器学习的数据效用。我们的模型架构由三个独立的神经网络组成，当集体使用时，可以学习一个变形场，以防止患者重新识别。在ChestX-ray14数据集上的定量结果显示了一定的降低。

    Robust and reliable anonymization of chest radiographs constitutes an essential step before publishing large datasets of such for research purposes. The conventional anonymization process is carried out by obscuring personal information in the images with black boxes and removing or replacing meta-information. However, such simple measures retain biometric information in the chest radiographs, allowing patients to be re-identified by a linkage attack. Therefore, there is an urgent need to obfuscate the biometric information appearing in the images. We propose the first deep learning-based approach (PriCheXy-Net) to targetedly anonymize chest radiographs while maintaining data utility for diagnostic and machine learning purposes. Our model architecture is a composition of three independent neural networks that, when collectively used, allow for learning a deformation field that is able to impede patient re-identification. Quantitative results on the ChestX-ray14 dataset show a reduction
    
[^170]: 通过数据增强提高图异常检测模型的泛化能力

    Improving Generalizability of Graph Anomaly Detection Models via Data Augmentation. (arXiv:2209.10168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10168](http://arxiv.org/abs/2209.10168)

    通过数据增强技术，本文针对图异常检测中存在的泛化能力差问题，提出了一种通用且新颖的研究问题：广义图异常检测，旨在同时有效识别训练领域图和未训练领域图上的异常。

    

    图异常检测（GAD）是一项重要的任务，因为即使少数异常都可能对良性用户造成巨大威胁。最近的半监督GAD方法，可以有效地利用可用的标签作为先验知识，比无监督方法取得了更好的性能。在实践中，人们通常需要在新的（子）图上识别异常以确保业务安全，但他们可能缺乏标签来训练有效的检测模型。一个自然的想法是直接将训练好的GAD模型应用于新的（子）图进行测试。然而，我们发现现有的半监督GAD方法存在泛化能力差的问题，即训练良好的模型在同一图的未训练区域（即训练中无法访问的区域）上表现不佳。这可能会带来很大的麻烦。在本文中，我们基于这一现象，提出了一个通用且新颖的研究问题，即广义图异常检测，旨在有效地识别训练领域图和未训练领域图上的异常。

    Graph anomaly detection (GAD) is a vital task since even a few anomalies can pose huge threats to benign users. Recent semi-supervised GAD methods, which can effectively leverage the available labels as prior knowledge, have achieved superior performances than unsupervised methods. In practice, people usually need to identify anomalies on new (sub)graphs to secure their business, but they may lack labels to train an effective detection model. One natural idea is to directly adopt a trained GAD model to the new (sub)graph for testing. However, we find that existing semi-supervised GAD methods suffer from poor generalization issue, i.e., well-trained models could not perform well on an unseen area (i.e., not accessible in training) of the same graph. It may cause great troubles. In this paper, we base on the phenomenon and propose a general and novel research problem of generalized graph anomaly detection that aims to effectively identify anomalies on both the training-domain graph and u
    
[^171]: 一种基于机器学习的预测Twenty20板球比赛结果的方法

    Prediction of the outcome of a Twenty-20 Cricket Match : A Machine Learning Approach. (arXiv:2209.06346v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06346](http://arxiv.org/abs/2209.06346)

    本研究使用机器学习方法针对Twenty20板球比赛的结果进行预测，通过考虑球员的历史表现统计、评级和聚类等因素，提出了一种新颖的基于ELO方法的球员评级方法，并使用不同的机器学习算法进行了对比分析。

    

    Twenty20板球是板球的一种短式形式，每个队伍有一个局，局限于最多20个回合。本文尝试了四种不同的机器学习方法，用于预测Twenty20板球比赛的结果。具体来说，我们考虑了参与竞争队伍的球员的以往表现统计、从著名板球统计网站获得的球员评级、将具有类似表现统计的球员进行分类以及提出了一种基于ELO方法的新颖评级球员的方法。我们通过使用不同的机器学习算法，包括逻辑回归、支持向量机、贝叶斯网络、决策树和随机森林，比较了每种特征工程方法的表现。

    Twenty20 cricket, sometimes written Twenty-20, and often abbreviated to T20, is a short form of cricket. In a Twenty20 game the two teams of 11 players have a single innings each, which is restricted to a maximum of 20 overs. This version of cricket is especially unpredictable and is one of the reasons it has gained popularity over recent times. However, in this paper we try four different machine learning approaches for predicting the results of T20 Cricket Matches. Specifically we take in to account: previous performance statistics of the players involved in the competing teams, ratings of players obtained from reputed cricket statistics websites, clustering the players' with similar performance statistics and propose a novel method using an ELO based approach to rate players. We compare the performances of each of these feature engineering approaches by using different ML algorithms, including logistic regression, support vector machines, bayes network, decision tree, random forest.
    
[^172]: R\'{e}nyi散度深度互相学习

    R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05732](http://arxiv.org/abs/2209.05732)

    本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。

    

    本文重审了一种简单而有效的计算范式——深度互相学习（DML）。我们提出使用R\'{e}nyi散度而不是KL散度，这种做法更加灵活、可调，以改善vanilla DML。这种修改能够在有限的附加复杂性下不断提高性能。该范例的收敛性进行了理论分析，并且表明具有恒定学习率的随机梯度下降在非凸优化任务的最坏情况下收敛的偏差为$\mathcal{O}(1)$。

    This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
    
[^173]: 不需要先验知识的分割已知物体和未知物体

    Segmenting Known Objects and Unseen Unknowns without Prior Knowledge. (arXiv:2209.05407v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.05407](http://arxiv.org/abs/2209.05407)

    本研究提出一种不需要先验知识的整体分割方法，可以将已知的物体和未知的物体进行准确分割，并在处理已知类别时对未知类别进行鲁棒处理。

    

    泛视域分割方法会根据输入将已知类别分配给每个像素。即使在最先进的方法中，这必然会导致在训练类别之外的物体上产生错误的预测。然而，在安全关键的环境中，对于未知样本和极端情况的鲁棒性至关重要，以避免危险后果。由于现实世界的数据集无法包含足够的数据点来充分采样底层分布的长尾部分，模型必须能够处理未见过和未知的情况。先前的方法通过重新识别已看到的未标记对象来解决这个问题。在这项工作中，我们提出了扩展分割的必要步骤，我们将其称为整体分割。整体分割旨在从未见过的未知类别中识别和分离对象实例，而无需任何先验知识，同时执行已知类别的泛视域分割。

    Panoptic segmentation methods assign a known class to each pixel given in input. Even for state-of-the-art approaches, this inevitably enforces decisions that systematically lead to wrong predictions for objects outside the training categories. However, robustness against out-of-distribution samples and corner cases is crucial in safety-critical settings to avoid dangerous consequences. Since real-world datasets cannot contain enough data points to adequately sample the long tail of the underlying distribution, models must be able to deal with unseen and unknown scenarios as well. Previous methods targeted this by re-identifying already-seen unlabeled objects. In this work, we propose the necessary step to extend segmentation with a new setting which we term holistic segmentation. Holistic segmentation aims to identify and separate objects of unseen unknown categories into instances, without any prior knowledge about them, while performing panoptic segmentation of known classes. We tac
    
[^174]: EchoGNN: 基于图神经网络的可解释性射血分数估计

    EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks. (arXiv:2208.14003v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2208.14003](http://arxiv.org/abs/2208.14003)

    EchoGNN是一种基于图神经网络的模型，用于从超声心动图视频中可解释地估计射血分数。它推断出一个潜在的超声心动图，并通过节点和边的权重来确定帧的重要性，以帮助估计射血分数。该模型在定性和定量方面展现出良好的效果。

    

    射血分数（EF）是心脏功能的关键指标，可用于识别容易出现心功能障碍（如心衰）的患者。EF是通过手动追踪左心室并估计特定帧上其体积的心脏超声视频（超声心动图）来估计的。由于手动过程和视频质量的差异，这些估计存在高度的观察者差异。这种不准确性和快速评估的需求需要可靠且可解释的机器学习技术。在这项工作中，我们提出了EchoGNN，一种基于图神经网络（GNNs）的模型，用于从超声心动图视频中估计EF。我们的模型首先从一个或多个超声心动图序列的帧中推断出潜在的超声心动图。然后，它估计这个图的节点和边的权重，表示对于帮助估计EF的个别帧的重要性。一个GNN回归器使用这个加权图来预测EF。我们通过定性和定量的方法展示了EchoGNN的效果。

    Ejection fraction (EF) is a key indicator of cardiac function, allowing identification of patients prone to heart dysfunctions such as heart failure. EF is estimated from cardiac ultrasound videos known as echocardiograms (echo) by manually tracing the left ventricle and estimating its volume on certain frames. These estimations exhibit high inter-observer variability due to the manual process and varying video quality. Such sources of inaccuracy and the need for rapid assessment necessitate reliable and explainable machine learning techniques. In this work, we introduce EchoGNN, a model based on graph neural networks (GNNs) to estimate EF from echo videos. Our model first infers a latent echo-graph from the frames of one or multiple echo cine series. It then estimates weights over nodes and edges of this graph, indicating the importance of individual frames that aid EF estimation. A GNN regressor uses this weighted graph to predict EF. We show, qualitatively and quantitatively, that t
    
[^175]: Bayesian神经网络中的近似阻塞Gibbs采样

    Approximate blocked Gibbs sampling for Bayesian neural networks. (arXiv:2208.11389v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.11389](http://arxiv.org/abs/2208.11389)

    本文提出了一种近似阻塞Gibbs采样方法，可以更可行地进行小批量MCMC采样，提高了前馈神经网络的预测准确性和预测不确定性的量化能力。

    

    本文提出了一种更可行的前馈神经网络的小批量MCMC采样方法。为此，文中提出了通过阻塞Gibbs采样方案对参数进行子组抽样的方法。通过对参数空间进行划分，无论层宽如何，都能进行采样。同时，通过在深层减小建议方差，可以减轻递增深度时消失的接受率问题。在分类任务中，增加非收敛链的长度可以提高预测准确性，因此避免消失的接受率和允许更长的链运行具有实际好处。此外，非收敛链的实现有助于量化预测不确定性。一个未解决的问题是在存在增广数据的情况下如何进行前馈神经网络的小批量MCMC采样。

    In this work, minibatch MCMC sampling for feedforward neural networks is made more feasible. To this end, it is proposed to sample subgroups of parameters via a blocked Gibbs sampling scheme. By partitioning the parameter space, sampling is possible irrespective of layer width. It is also possible to alleviate vanishing acceptance rates for increasing depth by reducing the proposal variance in deeper layers. Increasing the length of a non-convergent chain increases the predictive accuracy in classification tasks, so avoiding vanishing acceptance rates and consequently enabling longer chain runs have practical benefits. Moreover, non-convergent chain realizations aid in the quantification of predictive uncertainty. An open problem is how to perform minibatch MCMC sampling for feedforward neural networks in the presence of augmented data.
    
[^176]: Frouros: 一个用于机器学习系统中漂移检测的Python库

    Frouros: A Python library for drift detection in machine learning systems. (arXiv:2208.06868v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06868](http://arxiv.org/abs/2208.06868)

    Frouros是一个开源的Python库，可以检测任何机器学习框架中的概念和数据漂移，易于维护和扩展。

    

    Frouros是一个开源的Python库，能够检测机器学习系统中的漂移。它提供了传统和最近算法的组合来检测概念和数据漂移。我们的设计目标是使它与任何机器学习框架兼容，并轻松适应实际应用场景。该库遵循一系列最佳开发和持续集成实践，以确保易于维护和扩展。源代码可在https://github.com/IFCA/frouros上获取。

    Frouros is an open-source Python library capable of detecting drift in machine learning systems. It provides a combination of classical and more recent algorithms for drift detection: both concept and data drift. We have designed it with the objective of making it compatible with any machine learning framework and easily adaptable to real-world use cases. The library is developed following a set of best development and continuous integration practices to ensure ease of maintenance and extensibility. The source code is available at https://github.com/IFCA/frouros.
    
[^177]: 无需分离的稀疏矩问题的高效算法

    Efficient Algorithms for Sparse Moment Problems without Separation. (arXiv:2207.13008v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13008](http://arxiv.org/abs/2207.13008)

    本文提出了一个无需分离条件的稀疏矩问题的高效算法，主要贡献在于引入了全面且紧密的分析方法，并且通过Vandermonde矩阵和Schur多项式之间的联系，提供了与分离无关的紧密摄动界限。

    

    本文针对高维空间中从含噪声的矩信息中学习$k$-尖峰混合的稀疏矩问题进行了研究。我们使用运输距离来衡量学习混合模型的准确性。先前的算法要么假设了特定的分离条件，要么使用更多的恢复矩，要么运行在（超）指数时间。我们针对一维问题提出了一个鲁棒的算法（也称为稀疏Hausdorff矩问题），其基础是经典的Prony方法，我们的贡献主要在于分析。我们采用了比先前工作更全面且更紧密的分析（先前工作分析了Prony方法中间结果的摄动）。一个有用的技术要素是Vandermonde矩阵定义的线性系统与Schur多项式之间的联系，它使得我们能够提供与分离无关的紧密摄动界限，并且在其他情境中也可能有用。为了解决高维问题，我们首先求解了一维问题的情况。

    We consider the sparse moment problem of learning a $k$-spike mixture in high-dimensional space from its noisy moment information in any dimension. We measure the accuracy of the learned mixtures using transportation distance. Previous algorithms either assume certain separation assumptions, use more recovery moments, or run in (super) exponential time. Our algorithm for the one-dimensional problem (also called the sparse Hausdorff moment problem) is a robust version of the classic Prony's method, and our contribution mainly lies in the analysis. We adopt a global and much tighter analysis than previous work (which analyzes the perturbation of the intermediate results of Prony's method). A useful technical ingredient is a connection between the linear system defined by the Vandermonde matrix and the Schur polynomial, which allows us to provide tight perturbation bound independent of the separation and may be useful in other contexts. To tackle the high-dimensional problem, we first sol
    
[^178]: 使用多图拓扑在跨数据仓库联邦学习中减少训练时间

    Reducing Training Time in Cross-Silo Federated Learning using Multigraph Topology. (arXiv:2207.09657v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09657](http://arxiv.org/abs/2207.09657)

    本文提出了一种新的多图拓扑结构，用于跨数据仓库联邦学习，通过孤立节点实现模型聚合，从而有效地减少了训练时间。

    

    联邦学习是一个活跃的研究课题，因为它使得多个参与者能够共同训练一个模型而无需共享本地数据。目前，跨数据仓库联邦学习是一种常见的训练设置，它利用几百个可靠的数据仓库和高速访问链路来训练模型。虽然这种方法在实际场景中被广泛应用，但设计一个稳健的拓扑结构以减少训练时间仍然是一个未解决的问题。本文提出了一种新的用于跨数据仓库联邦学习的多图拓扑。我们首先使用覆盖图构建多图，然后将这个多图解析成具有孤立节点的不同简单图。孤立节点的存在使得我们可以在等待其他节点的情况下进行模型聚合，从而有效地减少训练时间。对三个公共数据集进行的大量实验表明，我们提出的方法与最新的拓扑结构相比，显著减少了训练时间。

    Federated learning is an active research topic since it enables several participants to jointly train a model without sharing local data. Currently, cross-silo federated learning is a popular training setting that utilizes a few hundred reliable data silos with high-speed access links to training a model. While this approach has been widely applied in real-world scenarios, designing a robust topology to reduce the training time remains an open problem. In this paper, we present a new multigraph topology for cross-silo federated learning. We first construct the multigraph using the overlay graph. We then parse this multigraph into different simple graphs with isolated nodes. The existence of isolated nodes allows us to perform model aggregation without waiting for other nodes, hence effectively reducing the training time. Intensive experiments on three public datasets show that our proposed method significantly reduces the training time compared with recent state-of-the-art topologies w
    
[^179]: GANDALF: 用于深度自动化特征学习的门控自适应网络

    GANDALF: Gated Adaptive Network for Deep Automated Learning of Features. (arXiv:2207.08548v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08548](http://arxiv.org/abs/2207.08548)

    GANDALF是一种用于表格数据的高性能深度学习架构，具有解释性和计算效率高的特点。通过引入门控特征学习单元，GANDALF能够实现更好的性能，并且在多个公开基准测试中优于或与其他最先进的方法持平。

    

    我们提出了一种新颖的高性能、可解释、参数和计算效率高的深度学习架构，用于表格数据，称为GANDALF（Gated Adaptive Network for Deep Automated Learning of Features）。GANDALF依赖于一个具有门控机制和内置特征选择的新的表格处理单元，称为门控特征学习单元（GFLU），作为特征表示学习单元。通过对多个公开的基准测试进行实验证明，GANDALF在性能上优于或与XGBoost、SAINT、FT-Transformers等最先进的方法持平。我们已经将代码在github.com/manujosephv/pytorch_tabular上以MIT许可证的形式提供。

    We propose a novel high-performance, interpretable, and parameter \& computationally efficient deep learning architecture for tabular data, Gated Adaptive Network for Deep Automated Learning of Features (GANDALF). GANDALF relies on a new tabular processing unit with a gating mechanism and in-built feature selection called Gated Feature Learning Unit (GFLU) as a feature representation learning unit. We demonstrate that GANDALF outperforms or stays at-par with SOTA approaches like XGBoost, SAINT, FT-Transformers, etc. by experiments on multiple established public benchmarks. We have made available the code at github.com/manujosephv/pytorch_tabular under MIT License.
    
[^180]: 在连续领域中将时间延展技能作为符号动作进行规划的学习

    Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning. (arXiv:2207.05018v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.05018](http://arxiv.org/abs/2207.05018)

    本文提出了一种新颖的分层强化学习代理SEADS，将连续控制的时间延展技能与环境状态的符号离散抽象的前向模型相连接，实现了在连续领域中的长程规划和控制能力的学习。

    

    对于既需要长程规划又需要连续控制能力的问题，现有的强化学习代理面临着重大挑战。在本文中，我们引入了一种新颖的分层强化学习代理，它将连续控制的时间延展技能与环境状态的符号离散抽象的前向模型相连接，用于规划。我们将我们的代理称为SEADS，即Symbolic Effect-Aware Diverse Skills。我们制定了一个目标和相应的算法，通过内在动机在已知状态抽象下进行无监督学习，从而学习到一组多样化的技能。这些技能与捕捉技能执行在状态抽象中的影响的符号前向模型共同学习。训练完成后，我们可以利用这些技能作为符号动作使用前向模型进行长程规划，并随后使用学习到的连续动作控制技能执行计划。所提出的算法实现了技能的学习。

    Problems which require both long-horizon planning and continuous control capabilities pose significant challenges to existing reinforcement learning agents. In this paper we introduce a novel hierarchical reinforcement learning agent which links temporally extended skills for continuous control with a forward model in a symbolic discrete abstraction of the environment's state for planning. We term our agent SEADS for Symbolic Effect-Aware Diverse Skills. We formulate an objective and corresponding algorithm which leads to unsupervised learning of a diverse set of skills through intrinsic motivation given a known state abstraction. The skills are jointly learned with the symbolic forward model which captures the effect of skill execution in the state abstraction. After training, we can leverage the skills as symbolic actions using the forward model for long-horizon planning and subsequently execute the plan using the learned continuous-action control skills. The proposed algorithm learn
    
[^181]: TF-GNN: TensorFlow中的图神经网络

    TF-GNN: Graph Neural Networks in TensorFlow. (arXiv:2207.03522v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03522](http://arxiv.org/abs/2207.03522)

    TF-GNN是一个可扩展的 TensorFlow 图神经网络库，用于支持丰富的异构图数据。它提供了低代码解决方案，并广泛应用于谷歌的生产模型中。这个库最近作为开源项目发布，为图学习提供了强大的工具。

    

    TensorFlow-GNN (TF-GNN) 是一个在 TensorFlow 中可扩展的图神经网络库。它从底层设计来支持当今信息生态系统中出现的各种丰富的异构图数据。除了为机器学习研究人员和高级开发人员提供支持，TF-GNN还提供低代码解决方案，以赋能更广泛的开发者社区进行图学习。谷歌的许多生产模型都在使用 TF-GNN，并且它最近作为一个开源项目发布。在本文中，我们描述了 TF-GNN 的数据模型、其 Keras 消息传递 API 以及相关的能力，如图采样和分布式训练。

    TensorFlow-GNN (TF-GNN) is a scalable library for Graph Neural Networks in TensorFlow. It is designed from the bottom up to support the kinds of rich heterogeneous graph data that occurs in today's information ecosystems. In addition to enabling machine learning researchers and advanced developers, TF-GNN offers low-code solutions to empower the broader developer community in graph learning. Many production models at Google use TF-GNN, and it has been recently released as an open source project. In this paper we describe the TF-GNN data model, its Keras message passing API, and relevant capabilities such as graph sampling and distributed training.
    
[^182]: TabText:一种灵活和上下文化的表格数据表示方法

    TabText: A Flexible and Contextual Approach to Tabular Data Representation. (arXiv:2206.10381v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10381](http://arxiv.org/abs/2206.10381)

    TabText是一种处理和特征提取框架，通过转换内容为语言并利用预训练的大型语言模型，从表格数据中提取上下文信息。通过应用TabText框架可以生成高性能且简单的机器学习基准模型，减少数据预处理的工作量。该框架在医疗预测任务中展现出良好的效果。

    

    表格数据对于在各个行业中应用机器学习任务至关重要。然而，传统的数据处理方法并没有充分利用表格中所有可用的信息，忽视了重要的上下文信息，如列标题描述。此外，将数据预处理成表格格式仍然是模型开发中一项耗时的瓶颈。本工作引入了TabText，一种处理和特征提取框架，将上下文信息从表格数据结构中提取出来。TabText通过将内容转换为语言，并利用预训练的大型语言模型(LLMs)来解决处理困难。我们在涵盖患者出院、ICU入院和死亡等九个医疗预测任务上评估了我们的框架。我们展示了：1) 应用我们的TabText框架可以生成性能优秀且简单的机器学习基准模型，只需最少的数据预处理；2) 增强预处理后的数据利用预训练语言模型能够提升模型效果

    Tabular data is essential for applying machine learning tasks across various industries. However, traditional data processing methods do not fully utilize all the information available in the tables, ignoring important contextual information such as column header descriptions. In addition, pre-processing data into a tabular format can remain a labor-intensive bottleneck in model development. This work introduces TabText, a processing and feature extraction framework that extracts contextual information from tabular data structures. TabText addresses processing difficulties by converting the content into language and utilizing pre-trained large language models (LLMs). We evaluate our framework on nine healthcare prediction tasks ranging from patient discharge, ICU admission, and mortality. We show that 1) applying our TabText framework enables the generation of high-performing and simple machine learning baseline models with minimal data pre-processing, and 2) augmenting pre-processed t
    
[^183]: 使用70万人日的可穿戴数据进行自监督学习的人体活动识别

    Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data. (arXiv:2206.02909v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2206.02909](http://arxiv.org/abs/2206.02909)

    本研究利用700,000人日的未标记可穿戴传感器数据，通过自监督学习技术，成功构建了一种能够在多个数据集上泛化且效果显著优于基线模型的人体活动识别模型，有望帮助研究人员和开发者开发高性能的可定制和泛化的活动分类器。

    

    由于缺乏大规模标记数据集，深度学习在人体活动识别方面的进展相对有限。在本研究中，我们利用自监督学习技术对UK-Biobank活动追踪器数据集进行了分析，这是迄今为止最大的数据集，包含超过70万人日的未标记可穿戴传感器数据。我们得到的活动识别模型在七个基准数据集上始终优于强基线模型，F1相对改善2.5%-100%（中位数18.4%），改进最大的是规模较小的数据集。与之前的研究不同的是，我们的结果可以泛化到外部数据集、设备和环境。我们的开源模型将帮助研究人员和开发者构建具有高性能的可定制和泛化的活动分类器。

    Advances in deep learning for human activity recognition have been relatively limited due to the lack of large labelled datasets. In this study, we leverage self-supervised learning techniques on the UK-Biobank activity tracker dataset--the largest of its kind to date--containing more than 700,000 person-days of unlabelled wearable sensor data. Our resulting activity recognition model consistently outperformed strong baselines across seven benchmark datasets, with an F1 relative improvement of 2.5%-100% (median 18.4%), the largest improvements occurring in the smaller datasets. In contrast to previous studies, our results generalise across external datasets, devices, and environments. Our open-source model will help researchers and developers to build customisable and generalisable activity classifiers with high performance.
    
[^184]: 当个性化造成伤害时：重新考虑在预测中使用群体属性

    When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction. (arXiv:2206.02058v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.02058](http://arxiv.org/abs/2206.02058)

    本研究发现，使用群体属性个性化机器学习模型可能降低群体水平的性能。为了确保在预测任务中公平使用群体属性，我们提出了形式化条件，并提供了相应的解决方法。我们的实证研究表明，在临床预测任务中普遍存在公平使用违规的情况，但我们也找到了简单干预手段来减轻其伤害。

    

    机器学习模型通常会使用受保护、敏感、自报告或者昂贵的分类属性进行个性化。本研究指出，使用群体属性进行个性化会降低群体水平的性能。我们提出了一种形式化条件，以确保在预测任务中“公平使用”群体属性，方法是通过训练一个额外的模型，即保证每个提供个人数据的群体会获得相对应的性能提升。我们提出了足够的条件，以确保在经验风险最小化中的公平使用，并描述了导致公平使用违规的故障模式，这是由于模型开发和部署中的标准做法所导致的。我们对临床预测任务进行了全面的实证研究。我们的结果表明，在实践中普遍存在公平使用违规，并说明了减轻其伤害的简单干预手段。

    Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level. We propose formal conditions to ensure the "fair use" of group attributes in prediction tasks by training one additional model -- i.e., collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We present a comprehensive empirical study of fair use in clinical prediction tasks. Our results demonstrate the prevalence of fair use violations in practice and illustrate simple interventions to mitigate their harm.
    
[^185]: 图神经网络的表达能力：通过代数分析改进表达性能

    Representation Power of Graph Neural Networks: Improved Expressivity via Algebraic Analysis. (arXiv:2205.09801v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09801](http://arxiv.org/abs/2205.09801)

    本文通过代数分析改进了图神经网络（GNN）的表达能力，证明了GNN能够比Weisfeiler-Lehman（WL）算法更好地产生区分性表示，特别是在具有不同特征值的图上。此外，我们还发现简单的卷积结构与无信息输入产生的等变特征比WL表示更具表达能力。

    

    尽管图神经网络（GNN）取得了显著的成功，但普遍认为它们的表达能力有限，并且它们最多与Weisfeiler-Lehman（WL）算法一样具有表达能力。本文与此相反，我们证明了标准的GNN（匿名输入）产生的表示比WL算法更具有区分性。我们使用线性代数工具对GNN的表示能力进行了全新的分析，并将其与图操作符的特征值分解相关联。我们证明了GNN能够从无信息输入产生独特的输出，至少对于所有具有不同特征值的图。我们还展示了简单的卷积结构与无信息输入产生的等变特征，它们计算图中的闭合路径并且明显比WL表示具有更高的表达能力。在图同构和图分类数据集上进行了彻底的实验分析，验证了我们的理论。

    Despite the remarkable success of Graph Neural Networks (GNNs), the common belief is that their representation power is limited and that they are at most as expressive as the Weisfeiler-Lehman (WL) algorithm. In this paper, we argue the opposite and show that standard GNNs, with anonymous inputs, produce more discriminative representations than the WL algorithm. Our novel analysis employs linear algebraic tools and characterizes the representation power of GNNs with respect to the eigenvalue decomposition of the graph operators. We prove that GNNs are able to generate distinctive outputs from white uninformative inputs, for, at least, all graphs that have different eigenvalues. We also show that simple convolutional architectures with white inputs, produce equivariant features that count the closed paths in the graph and are provably more expressive than the WL representations. Thorough experimental analysis on graph isomorphism and graph classification datasets corroborates our theore
    
[^186]: 机器学习友好的生物医学数据集用于等价和包含关系本体匹配

    Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v7 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.03447](http://arxiv.org/abs/2205.03447)

    本文介绍了五个新的生物医学本体匹配任务，通过引入机器学习技术并解决现有评估方法的限制，提供了综合评估框架来衡量本体匹配系统的性能。

    

    本体匹配在生物信息学和语义网等许多领域中扮演着重要角色，随着机器学习技术的应用，其研究越来越受到关注。然而，现有的本体匹配评估方法仍存在一些限制，包括对包含关系映射的有限评估、参考映射的亚优解以及对基于机器学习的系统评估的有限支持。为了解决这些限制，我们提出了五个新的生物医学本体匹配任务，涉及从Mondo和UMLS中提取的本体。每个任务包括等价和包含关系匹配，并通过人工筛选、本体修剪等方式确保参考映射的质量，并提出了一个综合评估框架，来从不同的角度评估基于机器学习和非机器学习的本体匹配系统性能。我们报告了评估结果。

    Ontology Matching (OM) plays an important role in many domains such as bioinformatics and the Semantic Web, and its research is becoming increasingly popular, especially with the application of machine learning (ML) techniques. Although the Ontology Alignment Evaluation Initiative (OAEI) represents an impressive effort for the systematic evaluation of OM systems, it still suffers from several limitations including limited evaluation of subsumption mappings, suboptimal reference mappings, and limited support for the evaluation of ML-based systems. To tackle these limitations, we introduce five new biomedical OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes both equivalence and subsumption matching; the quality of reference mappings is ensured by human curation, ontology pruning, etc.; and a comprehensive evaluation framework is proposed to measure OM performance from various perspectives for both ML-based and non-ML-based OM systems. We report evaluation r
    
[^187]: AdaBest: 通过自适应偏差估计最小化联邦学习中的客户漂移

    AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.13170](http://arxiv.org/abs/2204.13170)

    AdaBest提出了一种自适应算法，用于准确估计联邦学习中的客户端漂移。与之前的方法相比，AdaBest所需的存储和通信带宽较少，计算成本也较低。此外，AdaBest通过限制估计值的范数来提供稳定性。

    

    在联邦学习中，许多客户端或设备在不共享数据的情况下协作训练模型。模型在每个客户端进行本地优化，然后传输到集中中心进行聚合。尽管联邦学习是一种吸引人的分散式训练范式，但来自不同客户端的数据的异质性可能导致局部优化偏离全局目标。为了估计和消除这种偏离，近期在联邦学习优化中引入了方差减少技术。然而，这些方法对客户端漂移进行了不准确的估计，并最终未能正确地消除它。在这项工作中，我们提出了一种精确估计客户端漂移的自适应算法。与之前的方法相比，我们的方法需要更少的存储和通信带宽，以及更低的计算成本。此外，我们提议的方法通过限制客户端漂移估计的范数来引入稳定性。

    In Federated Learning (FL), a number of clients or devices collaborate to train a model without sharing their data. Models are optimized locally at each client and further communicated to a central hub for aggregation. While FL is an appealing decentralized training paradigm, heterogeneity among data from different clients can cause the local optimization to drift away from the global objective. In order to estimate and therefore remove this drift, variance reduction techniques have been incorporated into FL optimization recently. However, these approaches inaccurately estimate the clients' drift and ultimately fail to remove it properly. In this work, we propose an adaptive algorithm that accurately estimates drift across clients. In comparison to previous works, our approach necessitates less storage and communication bandwidth, as well as lower compute costs. Additionally, our proposed methodology induces stability by constraining the norm of estimates for client drift, making it mo
    
[^188]: 噪声设置中的相似性泛化：DIBS现象

    Generalizing similarity in noisy setups: the DIBS phenomenon. (arXiv:2201.12803v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12803](http://arxiv.org/abs/2201.12803)

    本研究揭示了数据密度、噪声和相似性学习之间的相互作用，证明了数据对的密度对于泛化至关重要，并发现了一种在密集数据集上比对称标签噪声更差的泛化性能的现象，称为密度诱导的相似性破坏（DIBS）。

    

    本文揭示了数据密度、噪声和相似性学习的普适性之间的相互作用。我们考虑了暹罗神经网络（SNNs），这是对比学习的基本形式，并探索了两种可能影响SNNs的噪声类型，即对比标签噪声（PLN）和单标签噪声（SLN）。我们的研究发现，不论训练设置如何，SNNs都表现出双降行为，并且噪声进一步加剧了这种行为。我们证明数据对的密度对于泛化至关重要。当SNNs在稀疏数据集上训练时，具有相同数量的PLN或SLN，它们的泛化性能是可比较的。然而，当使用密集数据集时，在过参数化区域中，PLN案例的泛化性能较差，相对于SLN案例，这导致了一种我们称为密度诱导的相似性破坏（DIBS）的现象。在这个情况下，PLN相似性违规变得宏观化，使得数据集被损坏到无法实现完全插值的程度。

    This work uncovers an interplay among data density, noise, and the generalization ability in similarity learning. We consider Siamese Neural Networks (SNNs), which are the basic form of contrastive learning, and explore two types of noise that can impact SNNs, Pair Label Noise (PLN) and Single Label Noise (SLN). Our investigation reveals that SNNs exhibit double descent behaviour regardless of the training setup and that it is further exacerbated by noise. We demonstrate that the density of data pairs is crucial for generalization. When SNNs are trained on sparse datasets with the same amount of PLN or SLN, they exhibit comparable generalization properties. However, when using dense datasets, PLN cases generalize worse than SLN ones in the overparametrized region, leading to a phenomenon we call Density-Induced Break of Similarity (DIBS). In this regime, PLN similarity violation becomes macroscopical, corrupting the dataset to the point where complete interpolation cannot be achieved, 
    
[^189]: LAnoBERT: 基于BERT掩码语言模型的系统日志异常检测方法

    LAnoBERT: System Log Anomaly Detection based on BERT Masked Language Model. (arXiv:2111.09564v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.09564](http://arxiv.org/abs/2111.09564)

    我们提出了一种名为LAnoBERT的无解析器系统日志异常检测方法，它使用了BERT模型进行掩码语言建模，从而实现了无人工干预的高效异常检测。

    

    计算机系统中生成的系统日志指的是大规模同时收集的数据，用作确定错误、入侵和异常行为的基础数据。系统日志异常检测的目标是及时识别异常，同时最小化人工干预，这是行业中的一个关键问题。先前的研究通过将各种形式的日志数据转换为标准化模板，使用解析器进行算法进行异常检测。特别是，应预先定义与特定事件对应的模板，以便所有日志数据使用该模板，其中日志关键信息可能会丢失。在本研究中，我们提出了一种名为LAnoBERT的无解析器系统日志异常检测方法，该方法使用了BERT模型，具有出色的自然语言处理性能。所提出的方法LAnoBERT通过掩码语言建模进行模型学习，进而进行无监督的异常检测。

    The system log generated in a computer system refers to large-scale data that are collected simultaneously and used as the basic data for determining errors, intrusion and abnormal behaviors. The aim of system log anomaly detection is to promptly identify anomalies while minimizing human intervention, which is a critical problem in the industry. Previous studies performed anomaly detection through algorithms after converting various forms of log data into a standardized template using a parser. Particularly, a template corresponding to a specific event should be defined in advance for all the log data using which the information within the log key may get lost. In this study, we propose LAnoBERT, a parser free system log anomaly detection method that uses the BERT model, exhibiting excellent natural language processing performance. The proposed method, LAnoBERT, learns the model through masked language modeling, which is a BERT-based pre-training method, and proceeds with unsupervised 
    
[^190]: 学习树状三维物体的几何和拓扑的生成模型

    Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects. (arXiv:2110.08693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08693](http://arxiv.org/abs/2110.08693)

    本文提出了一种扩展平方根速度函数的新表示方法和度量方法，用于分析和比较树状三维物体，从而提高物体形状差异计算的精度和效率。

    

    如何分析展现出复杂几何和拓扑变化的详细三维生物物体，例如神经元和植物树？本文提出了一个新的数学框架，用于表示、比较和计算这些树状三维对象的形状差异，并定义了一种新的度量方法来量化将一个树状物体变形为另一个物体所需的弯曲、拉伸和分支滑动。

    How can one analyze detailed 3D biological objects, such as neurons and botanical trees, that exhibit complex geometrical and topological variation? In this paper, we develop a novel mathematical framework for representing, comparing, and computing geodesic deformations between the shapes of such tree-like 3D objects. A hierarchical organization of subtrees characterizes these objects -- each subtree has the main branch with some side branches attached -- and one needs to match these structures across objects for meaningful comparisons. We propose a novel representation that extends the Square-Root Velocity Function (SRVF), initially developed for Euclidean curves, to tree-shaped 3D objects. We then define a new metric that quantifies the bending, stretching, and branch sliding needed to deform one tree-shaped object into the other. Compared to the current metrics, such as the Quotient Euclidean Distance (QED) and the Tree Edit Distance (TED), the proposed representation and metric cap
    
[^191]: 组合聚类：在多标签物体识别和说话者识别中的应用

    Compositional Clustering: Applications to Multi-Label Object Recognition and Speaker Identification. (arXiv:2109.04160v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.04160](http://arxiv.org/abs/2109.04160)

    该论文提出了一种新的组合聚类任务，针对具有组合关系的聚类研究，通过提出三种新算法可以将示例分成连贯的群组并推断其中的组合结构。与传统的层次聚类不同，该任务关注的是找到表示组成聚类的属性并集的组合聚类。

    

    我们考虑一种新颖的聚类任务，其中聚类可以具有组合关系，例如，一个聚类包含矩形图像，一个聚类包含圆形图像，一个（组合性）聚类包含同时拥有两种对象的图像。与层次聚类不同，其中父聚类表示子聚类属性的交集，我们的问题是找到表示组成聚类的属性并集的组合聚类。这个任务的动机来自于最近开发的少样本学习和嵌入模型，可以区分分配给示例的标签集，而不仅仅是单个标签。我们提出了三种新的算法--组合亲和传播（CAP）、组合k均值（CKM）和贪婪组合重新分配（GCR）--可以将示例分成连贯的群组并推断其中的组合结构。我们展示了有希望的结果，与流行的算法（如高斯m）

    We consider a novel clustering task in which clusters can have compositional relationships, e.g., one cluster contains images of rectangles, one contains images of circles, and a third (compositional) cluster contains images with both objects. In contrast to hierarchical clustering in which a parent cluster represents the intersection of properties of the child clusters, our problem is about finding compositional clusters that represent the union of the properties of the constituent clusters. This task is motivated by recently developed few-shot learning and embedding models can distinguish the label sets, not just the individual labels, assigned to the examples. We propose three new algorithms -- Compositional Affinity Propagation (CAP), Compositional k-means (CKM), and Greedy Compositional Reassignment (GCR) -- that can partition examples into coherent groups and infer the compositional structure among them. We show promising results, compared to popular algorithms such as Gaussian m
    
[^192]: 从观察数据中学习最优的处方树

    Learning Optimal Prescriptive Trees from Observational Data. (arXiv:2108.13628v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.13628](http://arxiv.org/abs/2108.13628)

    该论文介绍了一种从观察数据中学习最优处方树的方法，可以在不需要数据随机化和对树有严格假设的情况下，通过混合整数优化技术进行学习，并具有建模领域特定问题的能力。

    

    我们考虑从观察数据中学习一个适度深度的最优处方树（即，以二叉树形式表示的可解释的治疗分配策略）的问题。这个问题在许多社会重要领域（如公共卫生和个性化医学）中是存在的，这些领域中通过被动收集数据来寻找基于数据的可解释和数据驱动干预，而不是通过随机试验。我们提出了一种使用混合整数优化（MIO）技术来学习最优处方树的方法。我们证明，在温和条件下，我们的方法是渐近精确的，即随着历史数据样本的数量趋向于无穷大，它收敛到一个最优的样本外治疗分配策略。与现有文献相反，我们的方法：1）不需要数据随机化，2）不对学习到的树施加严格的假设，3）具有建模领域特定问题的能力。

    We consider the problem of learning an optimal prescriptive tree (i.e., an interpretable treatment assignment policy in the form of a binary tree) of moderate depth, from observational data. This problem arises in numerous socially important domains such as public health and personalized medicine, where interpretable and data-driven interventions are sought based on data gathered in deployment -- through passive collection of data -- rather than from randomized trials. We propose a method for learning optimal prescriptive trees using mixed-integer optimization (MIO) technology. We show that under mild conditions our method is asymptotically exact in the sense that it converges to an optimal out-of-sample treatment assignment policy as the number of historical data samples tends to infinity. Contrary to existing literature, our approach: 1) does not require data to be randomized, 2) does not impose stringent assumptions on the learned trees, and 3) has the ability to model domain specif
    
[^193]: 用相关性逼近潘多拉魔盒

    Approximating Pandora's Box with Correlations. (arXiv:2108.12976v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2108.12976](http://arxiv.org/abs/2108.12976)

    在这项工作中，我们通过研究相关分布下的潘多拉魔盒问题，将其与均匀决策树和最小和集覆盖问题进行了近似转化。我们的主要结果表明，这些问题都可以在亚指数时间内实现常数近似比。

    

    我们重新审视了在相关分布下的经典潘多拉魔盒（PB）问题。最近的研究得出了一种针对问题中按固定顺序访问盒子的一类限制策略的常数近似算法。在这项工作中，我们研究了近似最优策略的复杂性，该策略可以根据迄今为止看到的数值自适应地选择下一个要访问的盒子。我们的主要结果将PB与经典的均匀决策树（UDT）问题以及变体的最小和集覆盖（MSSC_f）问题进行了近似转化。对于具有支持度$m$的分布，UDT具有$\log m$的近似比，尽管在多项式时间内实现常数近似比一直是一个长期存在的开放问题，但在亚指数时间内实现常数近似比是可能的（arXiv:1906.11385）。我们的主要结果表明，PB和MSSC_f也具有相同的性质。我们还研究了c

    We revisit the classic Pandora's Box (PB) problem under correlated distributions on the box values. Recent work of arXiv:1911.01632 obtained constant approximate algorithms for a restricted class of policies for the problem that visit boxes in a fixed order. In this work, we study the complexity of approximating the optimal policy which may adaptively choose which box to visit next based on the values seen so far.  Our main result establishes an approximation-preserving equivalence of PB to the well studied Uniform Decision Tree (UDT) problem from stochastic optimization and a variant of the Min-Sum Set Cover ($\text{MSSC}_f$) problem. For distributions of support $m$, UDT admits a $\log m$ approximation, and while a constant factor approximation in polynomial time is a long-standing open problem, constant factor approximations are achievable in subexponential time (arXiv:1906.11385). Our main result implies that the same properties hold for PB and $\text{MSSC}_f$.  We also study the c
    
[^194]: 遮挡行人再识别的整体指导方法

    Holistic Guidance for Occluded Person Re-Identification. (arXiv:2104.06524v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2104.06524](http://arxiv.org/abs/2104.06524)

    本文提出了一种新颖的整体指导方法来解决遮挡行人再识别问题，通过匹配遮挡样本与整体样本的距离分布来训练模型，无需额外监督。

    

    在实际视频监控应用中，人员再识别（ReID）受到遮挡和检测错误的影响。尽管近年来取得了一些进展，但遮挡仍然会破坏最先进的CNN骨干提取的特征，并因此降低了ReID系统的准确性。为了解决这个问题，文献中的方法使用额外的昂贵过程，如姿态估计，其中姿态图提供监督来排除遮挡的区域。相反，我们提出了一种新颖的整体指导（HG）方法，只依赖于人的身份标签和数据集的配对匹配距离分布，以减轻遮挡问题，而无需额外的监督。因此，我们提出的学生-教师框架通过将遮挡样本的距离分布（DCD）与整体（非遮挡）样本的距离分布匹配来训练，从而解决了遮挡问题，并利用后者作为软约束。

    In real-world video surveillance applications, person re-identification (ReID) suffers from the effects of occlusions and detection errors. Despite recent advances, occlusions continue to corrupt the features extracted by state-of-art CNN backbones, and thereby deteriorate the accuracy of ReID systems. To address this issue, methods in the literature use an additional costly process such as pose estimation, where pose maps provide supervision to exclude occluded regions. In contrast, we introduce a novel Holistic Guidance (HG) method that relies only on person identity labels, and on the distribution of pairwise matching distances of datasets to alleviate the problem of occlusion, without requiring additional supervision. Hence, our proposed student-teacher framework is trained to address the occlusion problem by matching the distributions of between- and within-class distances (DCDs) of occluded samples with that of holistic (non-occluded) samples, thereby using the latter as a soft l
    
[^195]: 无需专家示范的共同模仿学习

    Co-Imitation Learning without Expert Demonstration. (arXiv:2103.14823v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.14823](http://arxiv.org/abs/2103.14823)

    本文提出了一种名为共同模仿学习（CoIL）的新型学习框架，通过利用代理自身的良好经验，而无需专家示范，来改善强化学习的效率。实验结果表明，该方法在各种任务上都具有显著的优越性。

    

    模仿学习是一种通过利用专家示范来提高强化学习效率的主要方法。然而，在许多实际场景中，获取专家示范可能非常昂贵甚至不可能。为了解决这个挑战，本文提出了一种名为共同模仿学习（CoIL）的新型学习框架，以利用代理选择探索环境并利用同伴代理的经验。尽管这些经验可能有价值，也可能误导人，我们提出通过期望增益的价值函数来估计每个经验的潜在效用。因此，代理可以通过强调更有用的经验并过滤掉噪声来选择性地互相模仿。各种任务的实验结果显示了所提出的共同模仿学习方法的显著优越性。

    Imitation learning is a primary approach to improve the efficiency of reinforcement learning by exploiting the expert demonstrations. However, in many real scenarios, obtaining expert demonstrations could be extremely expensive or even impossible. To overcome this challenge, in this paper, we propose a novel learning framework called Co-Imitation Learning (CoIL) to exploit the past good experiences of the agents themselves without expert demonstration. Specifically, we train two different agents via letting each of them alternately explore the environment and exploit the peer agent's experience. While the experiences could be valuable or misleading, we propose to estimate the potential utility of each piece of experience with the expected gain of the value function. Thus the agents can selectively imitate from each other by emphasizing the more useful experiences while filtering out noisy ones. Experimental results on various tasks show significant superiority of the proposed Co-Imitat
    
[^196]: 适度监督学习：定义、框架和普适性。

    Moderately Supervised Learning: Definition, Framework and Generality. (arXiv:2008.11945v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2008.11945](http://arxiv.org/abs/2008.11945)

    适度监督学习包括标准有监督学习和弱监督学习，已有的一些标准有监督学习任务表明，给定的标签并不容易学习，从给定的标签转换为易于学习的目标的过程会显著影响最终的性能，有监督学习的定义隐藏了可能对解决方案影响重大的细节。

    

    适度监督学习包括标准有监督学习和弱监督学习，前者训练数据集标签是理想的（完全、精确和准确），后者训练数据集标签不完美（不完整，不精确或不准确）。经过多次实验，已有的一些标准有监督学习任务的解决方案表明，有时候给定的标签并不容易学习，从给定的标签转换为易于学习的目标的过程会显著影响最终标准有监督学习解决方案的性能。因此，在未考虑从给定标签到易于学习的目标的转化过程属性的情况下，有监督学习的定义隐藏了一些可能对建立解决方案影响重大的细节。

    Learning with supervision has achieved remarkable success in numerous artificial intelligence (AI) applications. In the current literature, by referring to the properties of the labels prepared for the training dataset, learning with supervision is categorized as supervised learning (SL) and weakly supervised learning (WSL). SL concerns the situation where the training data set is assigned with ideal (complete, exact and accurate) labels, while WSL concerns the situation where the training data set is assigned with non-ideal (incomplete, inexact or inaccurate) labels. However, various solutions for SL tasks have shown that the given labels are not always easy to learn, and the transformation from the given labels to easy-to-learn targets can significantly affect the performance of the final SL solutions. Without considering the properties of the transformation from the given labels to easy-to-learn targets, the definition of SL conceals some details that can be critical to building the
    
[^197]: 声明性机制设计

    Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1912.13122](http://arxiv.org/abs/1912.13122)

    本文介绍了声明性机制设计的研究，提出了机构神经网络作为一种受管制的人工神经网络，引起人们对人工教学的关注，并提供了初步的答案。

    

    多智能体系统（MAS）和声明性电子机构（DEIs）的调控是过去十年涉及物理和软件智能体以及法律的多学科研究课题，但近年来逐渐演变为2016年起被称为新闻的机器律师。其中一种首次提出限制软件智能体行为的方案是电子机构。然而，随着人工神经网络（ANNs）被重新定义为深度学习（DL），有关DL使用的安全、隐私、伦理和法律问题引起了人工智能（AI）社区的关注。现在，MAS的规范几乎得到正确处理，我们提出将人工神经网络的规范作为一种特殊类型的受管制的人工神经网络，称之为机构神经网络（INN）。本文的主旨是引起人们对人工教学（AT）的关注，并给出一个初步的答案，展示了一种证明性的方法。

    Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
    
[^198]: 注意力就是一切（arXiv:1706.03762v6 [cs.CL]已更新）

    Attention Is All You Need. (arXiv:1706.03762v6 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762)

    Transformer是一种新的简单网络架构，完全基于注意力机制，取代了复杂的循环神经网络或卷积神经网络。实验证明Transformer在机器翻译任务中的质量更好、并行化效果更佳，且训练时间更短。它在英译德和英译法任务中取得了比其他模型更好的结果。

    

    目前主要的序列转换模型基于复杂的循环神经网络或卷积神经网络的编码器-解码器配置。表现最好的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，Transformer，完全基于注意力机制，不再使用循环和卷积。在两个机器翻译任务上的实验证明，这些模型在质量上优于其他模型，同时更易于并行化，训练时间显著减少。我们的模型在WMT 2014英译德任务上达到28.4的BLEU分数，比现有最好结果（包括集成模型）提高了2个BLEU分。在WMT 2014英译法任务上，在8个GPU上训练了3.5天后，我们的模型获得了41.8的单模型最新BLEU分数，训练成本仅为文献中最好模型的一小部分。我们展示了Transformer架构的优势，并证明了其在机器翻译任务中的重要贡献。

    The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transforme
    

