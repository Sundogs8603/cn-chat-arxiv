# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models.](http://arxiv.org/abs/2308.07922) | RAVEN是一种结合了检索增强的蒙特卡洛语言建模和前缀语言建模的模型，通过引入上下文融合学习，它能够在上下文学习方面取得比ATLAS更好的性能。 |
| [^2] | [The Regular Expression Inference Challenge.](http://arxiv.org/abs/2308.07899) | 正则表达式推理挑战是一个以找到最小正则表达式为目标的任务，具有应用广泛、难度可调、适用于代码/语言建模和机器学习的特点。 |
| [^3] | [SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation.](http://arxiv.org/abs/2308.07896) | SciRE-Solver是一种高效的采样器，通过引入得分积分求解器和递归导数估计方法，它解决了扩散概率模型采样过程缓慢的挑战，并实现了最先进的采样性能。 |
| [^4] | [On regularized Radon-Nikodym differentiation.](http://arxiv.org/abs/2308.07887) | 本文讨论了估计Radon-Nikodym导数的问题，并提出了基于正则化方案的解决方法。通过考虑导数的平滑度和估计空间的容量，建立了相应算法的收敛速度。数值模拟进一步验证了理论结果。 |
| [^5] | [Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms.](http://arxiv.org/abs/2308.07886) | 对现代时间序列分类算法的回归基础进行了合理检查，发现在某些问题上经典的无时间顺序的表格方法表现更好。 |
| [^6] | [The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning.](http://arxiv.org/abs/2308.07885) | 本研究利用深度学习网络重建了动态无呼吸胎儿心脏MRI的kt-SENSE style采集数据，通过恢复欠采样数据提高了重建质量和采集策略的优化。 |
| [^7] | [Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations.](http://arxiv.org/abs/2308.07883) | 本文研究了国家间农业贸易的时态边缘回归任务，探索了使用图神经网络（GNNs）进行边缘回归的应用。实验结果显示现有GNNs的不足，提出了TGN作为边缘回归任务的更合适选择。 |
| [^8] | [Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT.](http://arxiv.org/abs/2308.07876) | 该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。 |
| [^9] | [Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets.](http://arxiv.org/abs/2308.07871) | 这篇论文提出了一种统一的计算模型，通过学习情感嵌入，独立于不同的语言、交流方式、媒体或标签形式，从而将以往对不同类型异质情感数据的研究整合起来。 |
| [^10] | [Brain-Inspired Computational Intelligence via Predictive Coding.](http://arxiv.org/abs/2308.07870) | 这项研究介绍了一种通过预测编码的脑启发式计算智能方法，它可以解决现有人工智能方法的一些重要限制，并具有在机器学习领域有希望的应用潜力。 |
| [^11] | [Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes.](http://arxiv.org/abs/2308.07867) | 本文提出了一种图结构核设计，用于使用高斯过程进行功率流学习，通过顶点度核和网络扫描主动学习方案，实现了更高效的学习和样本复杂度降低。 |
| [^12] | [Impression-Aware Recommender Systems.](http://arxiv.org/abs/2308.07857) | 基于印象的推荐系统利用印象数据源提升推荐质量，通过综述分类推荐系统、数据集和评估方法，揭示开放性问题和未来研究方向。 |
| [^13] | [Dyadic Reinforcement Learning.](http://arxiv.org/abs/2308.07843) | 该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。 |
| [^14] | [Simple and Efficient Partial Graph Adversarial Attack: A New Perspective.](http://arxiv.org/abs/2308.07834) | 提出了一种名为部分图攻击 (PGA) 的全新方法，它选取脆弱节点作为攻击目标，相比现有的全局攻击方法有更高的效率和改进的空间。 |
| [^15] | [REFORMS: Reporting Standards for Machine Learning Based Science.](http://arxiv.org/abs/2308.07832) | REFORMS是一个基于机器学习的科学报告标准，旨在解决使用机器学习方法在科学研究中出现的有效性、可重复性和可推广性失败问题。这个标准由32个问题和一套指导方针组成，可作为研究人员设计和实施科研时的参考资源。 |
| [^16] | [Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves.](http://arxiv.org/abs/2308.07824) | 本论文提出了一个基于深度学习的混合模型，通过提取与锂离子电池老化相关的特征，动态地估计当前容量和预测未来容量。这种模型有助于准确快速估计和预测锂离子电池的老化状态。 |
| [^17] | [Deep reinforcement learning for process design: Review and perspective.](http://arxiv.org/abs/2308.07822) | 深度强化学习在流程设计中的综述和展望。该研究调查了强化学习在流程设计中的最新研究，并探讨了潜在挑战和未来工作，以充分发挥其在化学工程中的潜力。 |
| [^18] | [Quantifying the Cost of Learning in Queueing Systems.](http://arxiv.org/abs/2308.07817) | 本文提出了一种新的度量方法，即学习队列中的成本 (CLQ)，用于量化由参数不确定性导致的时间平均队列长度最大增加量。该度量方法可以捕捉学习队列系统的统计复杂性，不局限于渐近性能。 |
| [^19] | [Fairness and Privacy in Federated Learning and Their Implications in Healthcare.](http://arxiv.org/abs/2308.07805) | 联邦学习在解决分布式学习中的安全和隐私问题方面具有重要的应用价值，特别是在医疗保健领域。 |
| [^20] | [Adaptive Noise Covariance Estimation under Colored Noise using Dynamic Expectation Maximization.](http://arxiv.org/abs/2308.07797) | 本文提出了一种基于脑启发的算法，用于自适应估计受到有色噪声影响的动态系统中的噪声协方差矩阵。通过扩展动态期望最大化算法，我们的估计器能够同时进行在线噪声协方差和状态估计，并达到全局最优解。 |
| [^21] | [Informed Named Entity Recognition Decoding for Generative Language Models.](http://arxiv.org/abs/2308.07791) | 该论文提出了一种利用生成式语言模型进行命名实体识别解码的方法，通过在信息提取过程中应用生成模型的语言理解能力，提高了性能并消除了幻觉的风险。 |
| [^22] | [DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding.](http://arxiv.org/abs/2308.07787) | 本文提出了一种基于扩散的视频转语音合成方法，采用视觉引导说话人嵌入的方式，可以在推断过程中不需要额外的音频信息，仅通过输入的视觉信息即可产生丰富的说话人嵌入信息。 |
| [^23] | [Hierarchical generative modelling for autonomous robots.](http://arxiv.org/abs/2308.07775) | 该论文研究了自主机器人操作中运动控制的分层生成建模方法，通过模仿人类的深层时间结构来实现多级规划和协调肢体运动，以实现复杂的整体身体动作。 |
| [^24] | [A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection.](http://arxiv.org/abs/2308.07774) | 本文提出了一种无监督图编码解码模型，用于检测图中的异常节点。在编码阶段，通过设计一种新的池化机制，该模型能够根据节点的异常程度对节点进行排序。模型的池化过程具有较低的计算复杂度和更高的可解释性。 |
| [^25] | [MOLE: MOdular Learning FramEwork via Mutual Information Maximization.](http://arxiv.org/abs/2308.07772) | MOLE是一种异步和本地化的神经网络学习框架，它通过层次化的模块化方式和最大化互信息的方法来进行训练，具有局部优化和梯度隔离的特性，并且在不同类型的数据上都有良好的应用效果。 |
| [^26] | [NeFL: Nested Federated Learning for Heterogeneous Clients.](http://arxiv.org/abs/2308.07761) | NeFL是一个嵌套联邦学习框架，通过深度和宽度缩放将模型有效地划分为子模型，解决了在联邦学习中由于慢或能力有限的客户端导致的训练时间延长和性能下降的问题。 |
| [^27] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^28] | [Exploiting Sparsity in Automotive Radar Object Detection Networks.](http://arxiv.org/abs/2308.07748) | 本文研究了在汽车雷达目标检测网络中利用稀疏性的方法，提出了稀疏核心点柱体和双体素点卷积来解决网格渲染和稀疏骨干结构问题。实验结果显示，这种方法在Car AP4.0上优于基准模型5.89%和先前的最优模型4.19%，并将平均尺度误差相对于基准模型减少了21.41%。 |
| [^29] | [Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World.](http://arxiv.org/abs/2308.07741) | Real Robot Challenge 2022为RL和机器人学界之间的桥梁，允许参与者在真实机器人上从离线数据中学习灵巧操作任务，解决了在模拟中得到的见解不能转化到真实机器人的问题。 |
| [^30] | [Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability.](http://arxiv.org/abs/2308.07728) | 本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。 |
| [^31] | [Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening.](http://arxiv.org/abs/2308.07707) | 选择性突触减弱（SSD）是快速、性能优越且无需重新训练的机器遗忘方法，能够同时保护模型的性能并遗忘特定信息。 |
| [^32] | [Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models.](http://arxiv.org/abs/2308.07706) | 本论文提出使用视觉-语言模型进行医学图像分割的迁移学习，并评估了其在医学领域的可迁移性。通过捕捉语义信息和引入新的图像描述变化，实现了对多样化医学图像的分割。 |
| [^33] | [Parametric entropy based Cluster Centriod Initialization for k-means clustering of various Image datasets.](http://arxiv.org/abs/2308.07705) | 本文提出了一种基于参数熵的k-means聚类中心初始化方法，在图像数据集上进行了分析，并得出了适用于不同数据集的最佳熵度量。 |
| [^34] | [Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images.](http://arxiv.org/abs/2308.07688) | 该研究通过在非医学图像上利用自监督学习（SSL）预训练，取得了优于基于ImageNet的预训练的结果，从而实现了在医疗AI模型中增强网络初始化的目的。 |
| [^35] | [DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models.](http://arxiv.org/abs/2308.07687) | 本论文提出了一种名为DiffGuard的方法，使用预训练的扩散模型进行语义不匹配引导的带外分布检测。实验证明，DiffGuard在小规模数据集上表现出色，但在ImageNet规模的数据集上无法应用。 |
| [^36] | [Gradient-Based Post-Training Quantization: Challenging the Status Quo.](http://arxiv.org/abs/2308.07662) | 本文挑战了基于梯度的模型训练后量化(GPTQ)方法中的常见选择，并且展示了该方法在一定程度上对多个变量具有鲁棒性。 |
| [^37] | [Attention Is Not All You Need Anymore.](http://arxiv.org/abs/2308.07661) | 本文提出了一种名为Extractor的插入替代器，用于取代Transformer中的自注意机制，实验证明使用Extractor可以提高Transformer的性能，并且具有更短的计算关键路径。 |
| [^38] | [From Commit Message Generation to History-Aware Commit Message Completion.](http://arxiv.org/abs/2308.07655) | 本文提出并评估了从提交信息生成转向基于历史的提交信息补全的新思路，使用先前的提交历史作为额外上下文可以显著提高生成的提交信息的质量和个性化程度。 |
| [^39] | [Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping.](http://arxiv.org/abs/2308.07641) | 本文提出了一种名为三元奇异值分解 (TSVD) 的新颖线性映射参数化形式，通过限制奇异值分解中的矩阵为三元矩阵形式，它在网络压缩性能方面表现出色。实验证明，TSVD在各种类型的网络和任务中都能实现最先进的网络压缩性能。 |
| [^40] | [Backpropagation Path Search On Adversarial Transferability.](http://arxiv.org/abs/2308.07625) | 本研究提出了一种backPropagation pAth Search (PAS)方法来增强对抗迁移性，通过调整卷积模块的反向传播路径和构建基于有向无环图的搜索空间来解决了现有基于结构攻击者的局限性。 |
| [^41] | [A Multilayer Perceptron-based Fast Sunlight Assessment for the Conceptual Design of Residential Neighborhoods under Chinese Policy.](http://arxiv.org/abs/2308.07616) | 本研究提出了一种基于多层感知器的方法，用于快速评估中国住宅小区的阳光条件。该方法通过一阶段预测，输出输入立方体形状建筑导致的遮挡时间间隔，从而得到建筑的阳光小时数。这种方法比传统软件更快速，可以在概念设计阶段使用，有助于优化建筑阳光性能。 |
| [^42] | [Searching for Novel Chemistry in Exoplanetary Atmospheres using Machine Learning for Anomaly Detection.](http://arxiv.org/abs/2308.07604) | 本文研究使用机器学习方法进行异常检测，在系外行星中寻找新的化学物质。通过在合成光谱上进行实验证明了两种常见的异常检测方法的可行性，并用ROC曲线比较了它们的性能。 |
| [^43] | [Generating Personas for Games with Multimodal Adversarial Imitation Learning.](http://arxiv.org/abs/2308.07598) | 本论文介绍了一种使用多模态对抗模仿学习的方法，可以生成多个游戏角色策略，以用于游戏测试。这种方法通过使用多个判别器作为奖励模型，并根据辅助输入对每个判别器的奖励进行加权，有效地建模各种人类游戏风格。 |
| [^44] | [High-Probability Risk Bounds via Sequential Predictors.](http://arxiv.org/abs/2308.07588) | 通过在线到批次转换和对损失函数的二阶校正，我们展示了一种方法可以在统计学习中获得几乎最优的高概率风险上界。 |
| [^45] | [Story Visualization by Online Text Augmentation with Context Memory.](http://arxiv.org/abs/2308.07575) | 本论文提出了一种通过在线文本增强和上下文记忆来进行故事可视化的方法。通过使用新颖的记忆架构和多个伪描述作为训练过程的补充监督，该方法在两个故事可视化基准测试中取得了显著优于现有方法的结果。 |
| [^46] | [Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition.](http://arxiv.org/abs/2308.07571) | Ske2Grid是一种用于改进基于骨架的动作识别的新型表示学习框架，通过定义规则卷积操作和新颖设计的网格表示，实现了更好的识别效果。 |
| [^47] | [Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels.](http://arxiv.org/abs/2308.07562) | 这篇论文提出了两种新的多重插补模型组合方法，实现了半监督学习中非随机缺失标签问题的更高准确性和更小偏差。 |
| [^48] | [A User-Centered Evaluation of Spanish Text Simplification.](http://arxiv.org/abs/2308.07556) | 通过对西班牙文文本简化的用户评估，我们发现神经网络在预测用户偏好方面表现更好，同时也发现多语言模型不如西班牙文模型在同一任务上的表现。我们发布了评估中的语料库，希望推动西班牙语自然语言处理的最新技术。 |
| [^49] | [Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks.](http://arxiv.org/abs/2308.07553) | 本论文提出了一种改进的点对点认证方法，通过利用差分隐私和采样高斯机制，能够确保对有限数量中毒样本的预测具有不变性，提供更大的对抗鲁棒性保证。 |
| [^50] | [Domain Adaptation via Minimax Entropy for Real/Bogus Classification of Astronomical Alerts.](http://arxiv.org/abs/2308.07538) | 本文研究了用于天文警报真伪分类的领域适应方法，通过使用微调方法和半监督深度领域适应（MME）来提高分类模型的性能。实验证明，微调模型和MME模型都能在只有少数标记数据的情况下显著改进基本模型。 |
| [^51] | [FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction.](http://arxiv.org/abs/2308.07527) | FeatGeNN是一种使用基于相关性的汇集方法提取和创建新特征的卷积方法，它在多个基准数据集上的评估中表现优于现有的自动特征工程方法。 |
| [^52] | [Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System.](http://arxiv.org/abs/2308.07523) | 深层操作符网络（DeepONet）作为一种强大的替代建模方法，在核系统数字孪生技术中展示出了显著的预测精度和计算效率。然而，挑战仍然存在，包括最佳传感器放置和模型评估。 |
| [^53] | [Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning.](http://arxiv.org/abs/2308.07520) | 这篇论文研究了非线性、反馈和因果结构学习中的一致性问题，并提出了一个弱于强可靠性的k-Triangle Faithfulness的替代定义。 |
| [^54] | [Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach.](http://arxiv.org/abs/2308.07511) | 本文提出了一种将传统基于模型的信干噪比优化方法应用于神经网络训练的知识蒸馏方法，以提高神经网络的性能和收敛速度。 |
| [^55] | [Data Race Detection Using Large Language Models.](http://arxiv.org/abs/2308.07505) | 本研究探索了一种基于大型语言模型的数据竞争检测方法，通过结合提示工程和微调技术，发现LLMs可以作为数据竞争检测的可行方法，但在需要详细信息时仍不如传统工具竞争。 |
| [^56] | [ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting.](http://arxiv.org/abs/2308.07496) | ST-MLP是一种基于级联多层感知器（MLP）模块和线性层的时空模型，通过成功实现分组独立策略的时间序列预测技术，将时间信息、空间信息和预定义的图结构结合起来。实验结果表明ST-MLP在精度和计算效率方面优于其他模型。 |
| [^57] | [Adaptive Tracking of a Single-Rigid-Body Character in Various Environments.](http://arxiv.org/abs/2308.07491) | 本研究提出了一种基于单刚体角色仿真的深度强化学习方法，通过训练一个能够自适应各种环境变化的策略，实现在不需要额外学习的情况下完成各种任务。 |
| [^58] | [O-1: Self-training with Oracle and 1-best Hypothesis.](http://arxiv.org/abs/2308.07486) | 提出了一种新的自我训练目标O-1，通过增强神谕假设来减少训练偏差，并统一训练和评估指标。通过实验证明了O-1在语音识别任务中的有效性，并取得了相对于EMBR的显著改进。 |
| [^59] | [OCDaf: Ordered Causal Discovery with Autoregressive Flows.](http://arxiv.org/abs/2308.07480) | 我们提出了一种新的有序因果推断方法，可以从观测数据中学习因果图，并在多种基准测试中展示出了最先进的性能。 |
| [^60] | [Symphony: Optimized Model Serving using Centralized Orchestration.](http://arxiv.org/abs/2308.07470) | Symphony是一个集中式调度系统，可以优化深度神经网络模型服务，在满足高加速器效率和延迟SLO的同时适应工作负载变化。通过非工作保持调度算法和模型分配算法，Symphony能够实现高批处理效率和强大的自动缩放功能，比之前的系统提供高达4.7倍的吞吐量。 |
| [^61] | [Omega-Regular Reward Machines.](http://arxiv.org/abs/2308.07469) | 本文引入了ω-正规奖励机器，将奖励机器与ω-正规语言集成在一起，为RL提供了一种表达能力强且有效的奖励机制。 |
| [^62] | [There Is a Digital Art History.](http://arxiv.org/abs/2308.07464) | 该论文重点探讨了数字艺术史的存在与发展，并指出大规模视觉模型对数字艺术史具有重要影响，打开了提取和自动化视觉逻辑的新可能性。 |
| [^63] | [GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction.](http://arxiv.org/abs/2308.07452) | GRU-D-Weibull是一种新型的实时个体化终点预测方法，结合了门控循环单元和衰减技术，用于建模威布尔分布。通过在慢性肾脏疾病患者队列中的评估，发现该方法在终点预测方面的性能优于其他竞争方法。 |
| [^64] | [Open-set Face Recognition using Ensembles trained on Clustered Data.](http://arxiv.org/abs/2308.07445) | 本研究提出了一种使用在聚类数据上训练的集成模型进行开放集合人脸识别的方法，能够准确识别感兴趣的个体，同时有效处理陌生的面孔。实验结果表明即使在大规模图库中也能取得竞争性的性能。 |
| [^65] | [Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides.](http://arxiv.org/abs/2308.07441) | 该论文提出了一个物理先验的深度学习框架，通过编码平流-扩散机制和流体动力学约束来联合预测NO2和NOx，并成功降低了机器学习模型的偏差。 |
| [^66] | [Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks.](http://arxiv.org/abs/2308.07439) | 本研究提出了一种交互感知的个性化车辆轨迹预测方法，利用时间图神经网络来建模目标车辆与周围交通之间的时空交互，并通过迁移学习来个性化预测。实验结果表明，该方法能够更准确地预测车辆的轨迹。 |
| [^67] | [A Hybrid Deep Spatio-Temporal Attention-Based Model for Parkinson's Disease Diagnosis Using Resting State EEG Signals.](http://arxiv.org/abs/2308.07436) | 本研究提出了一种基于深度学习的模型，利用静息状态的脑电图信号进行帕金森病的诊断。该模型能够从EEG中提取复杂的非线性特征，并在多个数据集上实现准确的诊断。 |
| [^68] | [Addressing Distribution Shift in RTB Markets via Exponential Tilting.](http://arxiv.org/abs/2308.07424) | 本文介绍了一种名为ExTRA的算法，用于解决机器学习模型中的分布偏移问题。通过确定源数据上的重要性权重，该方法能够最小化加权源数据和目标数据集之间的KL散度。通过实验验证，证明了这种方法的适用性。 |
| [^69] | [U-Turn Diffusion.](http://arxiv.org/abs/2308.07421) | U-Turn扩散是一种用于生成合成图像的AI模型，通过引入U-Turn Diffusion技术来改进生成图像的质量。这种技术结合了前向、U-Turn和反向过程，通过解构快速相关性来提高生成过程的效率。 |
| [^70] | [Locally Adaptive and Differentiable Regression.](http://arxiv.org/abs/2308.07418) | 本文提出了一种本地自适应可微回归模型，通过对局部学习模型进行加权平均，在不同本地区域处理数据时具有竞争力，并在理论上实现更快的统计收敛以及在实际应用中改善了性能。 |
| [^71] | [Text Injection for Capitalization and Turn-Taking Prediction in Speech Models.](http://arxiv.org/abs/2308.07395) | 本研究探讨了在语音模型中利用文本注入进行非ASR任务的辅助，并使用联合训练算法提升大写准确率和交替检测召回率。 |
| [^72] | [DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks.](http://arxiv.org/abs/2308.07387) | DISBELIEVE 是一种本地模型污染攻击，通过在参数或梯度之间创建恶意距离与良性客户端的距离较小的恶意参数或梯度，破坏联合系统的性能。 |
| [^73] | [Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity.](http://arxiv.org/abs/2308.07359) | 本研究通过考虑记录合并疾病的比例项，改进了基于ICD码的语义相似度计算方法。 |
| [^74] | [Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks.](http://arxiv.org/abs/2308.07358) | 本研究提出了一种机器学习的方案，结合图神经网络和专家指导自动化生成计算流体力学网格，并引入了一种新的三维分割算法，用于表面分类。 |
| [^75] | [Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach.](http://arxiv.org/abs/2308.07356) | 本研究通过sMRI和机器学习方法研究了自闭症谱系障碍患者中的形态连接模式在不同年龄组之间的差异，并发现了一些显著特征。 |
| [^76] | [Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer.](http://arxiv.org/abs/2308.07352) | 本研究使用基于贝叶斯物理信息神经网络（B-PINN）的框架来模拟含水层中纳米颗粒的运动性，并为理解和开发高效的修复策略提供了预测性工具。 |
| [^77] | [IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse.](http://arxiv.org/abs/2308.07351) | 该论文提出了一种新颖的传递强化学习方法，通过在actor-critic框架中使用Q函数来选择源策略，解决了在有限样本下选择适当的源策略的挑战。 |
| [^78] | [Efficient Neural PDE-Solvers using Quantization Aware Training.](http://arxiv.org/abs/2308.07350) | 使用量化感知训练的高效神经PDE求解器研究了在减少计算成本方面的潜力，并证明了量化网络权重和激活可以成功降低计算成本而不损害性能。 |
| [^79] | [Conic Descent Redux for Memory-Efficient Optimization.](http://arxiv.org/abs/2308.07343) | 这项工作提出了一种内存高效的锥下降优化算法，该算法从对偶问题中得到几何推导，提出了动量锥下降（MOCO）变体，并探索了提速对偶收敛的可能性。 |
| [^80] | [Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic.](http://arxiv.org/abs/2308.07336) | 本研究研究了一种从合成语料库中学习演绎推理能力的方法，通过采用基于形式逻辑理论的演绎规则，训练的语言模型具有更泛化的推理能力。 |
| [^81] | [An Encoder-Decoder Approach for Packing Circles.](http://arxiv.org/abs/2308.07335) | 本文提出了一个编码-解码的方法，用于将相同的圆形放置在一个较大的圆形中。该方法通过编码和解码过程，以及添加受控扰动来有效解决装填问题。 |
| [^82] | [Variations on the Reinforcement Learning performance of Blackjack.](http://arxiv.org/abs/2308.07329) | 本研究探讨了在不同的牌堆大小下，强化学习代理在Blackjack游戏中的表现变化。研究发现算法的学习收敛速度与牌堆大小有关，并展示了使用基本策略和HI-LO系统的牌数计数器如何使庄家破产。这项工作的创新之处在于认识到牌堆大小是影响Blackjack表现的关键因素。 |
| [^83] | [AI Text-to-Behavior: A Study In Steerability.](http://arxiv.org/abs/2308.07326) | 本研究探讨了大型语言模型的可操控性，通过使用行为心理学框架，模型在生成文本时能够根据提示展现特定的行为特征。研究发现，模型在不同特征上的表现具有灵活性和区分度，并能够准确复制历史人物的个性和对话风格。 |
| [^84] | [Redesigning Out-of-Distribution Detection on 3D Medical Images.](http://arxiv.org/abs/2308.07324) | 该论文重设计了医学图像的区分概率检测问题，引入了Expected Performance Drop (EPD)度量，通过下游模型的性能来定义异常样本，并且能够根据临床影响对方法进行排名。 |
| [^85] | [AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes.](http://arxiv.org/abs/2308.07221) | AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。 |
| [^86] | [A Time-aware tensor decomposition for tracking evolving patterns.](http://arxiv.org/abs/2308.07126) | 提出了一种适用于跟踪演变模式的时空张量分解方法tPARAFAC2，通过时间正则化器从时间数据中提取逐渐演变的模式。 |
| [^87] | [#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models.](http://arxiv.org/abs/2308.07074) | 本研究提出了InsTag，一种用于标记基于语义和意图的监督微调（SFT）数据集样本的开放式细粒度标注器。通过分析开源SFT数据集，发现模型能力会随着更多多样化和复杂化的数据而增长。基于这一观察结果，使用InsTag选择的数据进行模型微调，得到的TagLM模型在大规模SFT数据上优于开源模型，验证了查询多样性和复杂性的重要性。 |
| [^88] | [SAILOR: Structural Augmentation Based Tail Node Representation Learning.](http://arxiv.org/abs/2308.06801) | SAILOR是一种基于结构增强的尾节点表示学习框架，针对真实场景中长尾分布的图节点度数，通过学习增强图结构和提取更有信息的尾节点表示，解决了GNN在尾节点表示上的性能退化问题。 |
| [^89] | [Private Distribution Learning with Public Data: The View from Sample Compression.](http://arxiv.org/abs/2308.06239) | 本论文研究了在具有公共数据的情况下的私有分布学习问题，通过压缩样本和列表学习的方式，我们对高斯分布以及高斯混合分布进行了学习上限的分析，并提出了对不可知学习和分布变化抵抗学习的新结果。 |
| [^90] | [Diffusion Model in Causal Inference with Unmeasured Confounders.](http://arxiv.org/abs/2308.03669) | 本研究扩展了扩散模型的使用，提出了一种新的模型BDCM，可以在存在无法测量的混淆因素的情况下更准确地回答因果问题。 |
| [^91] | [SynJax: Structured Probability Distributions for JAX.](http://arxiv.org/abs/2308.03291) | SynJax是一个针对JAX的结构化概率分布库，通过提供高效的向量化实现解决了对于结构化对象的难以实现的问题。 |
| [^92] | [Source-free Domain Adaptive Human Pose Estimation.](http://arxiv.org/abs/2308.03202) | 提出了无源域自适应的人体姿势估计任务，旨在解决在适应过程中无法访问源数据的跨域学习挑战。通过提出的新框架，源保护模块更有效地保留源信息并抵抗噪声。 |
| [^93] | [MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixied Sample Data Augmentation Method.](http://arxiv.org/abs/2308.02804) | MiAMix是一种新型的混合样本数据增强方法，通过将图像增强集成到混合框架中并利用多种多样的混合方法来提升图像分类模型的性能和泛化能力。 |
| [^94] | [Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites.](http://arxiv.org/abs/2308.01246) | Tirtha是一个自动化平台，用于众包文化遗址的图像并创建它们的三维模型。这个平台利用了最先进的结构运动和多视图立体技术，具有模块化、可扩展和成本效益的特点，在资源有限的发展中国家具有重要意义。 |
| [^95] | [Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather.](http://arxiv.org/abs/2308.00924) | 本文研究了在空中图像上进行连续适应的问题，特别关注了模型在逐渐恶化的天气条件下的适应能力。在实验中，作者合成了多个数据集模拟逐渐恶化的天气条件，并评估了不同的领域适应模型的性能。研究结果表明，不断适应的约束对模型的性能有重要影响。 |
| [^96] | [PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization.](http://arxiv.org/abs/2307.15199) | 提出了PromptStyler，通过使用提示合成样式特征，解决了无源域泛化的问题。该方法通过学习样式词向量生成多样的样式，并通过强制样式内容特征与内容特征靠近来保证样式不会扭曲内容信息。在多个数据集上取得了最先进的结果。 |
| [^97] | [Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items.](http://arxiv.org/abs/2307.13709) | 本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。 |
| [^98] | [Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations.](http://arxiv.org/abs/2307.07893) | 本文提出了一种在数据有限的情况下，通过自动编码器进行异常检测的方法，利用纤维层片的深度图进行二分类，并使用重构误差作为量化指标。 |
| [^99] | [Stack More Layers Differently: High-Rank Training Through Low-Rank Updates.](http://arxiv.org/abs/2307.05695) | 本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。 |
| [^100] | [Principles and Guidelines for Evaluating Social Robot Navigation Algorithms.](http://arxiv.org/abs/2306.16740) | 本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。 |
| [^101] | [Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization.](http://arxiv.org/abs/2306.13050) | 本研究提出了一种基于最大边际矩阵分解的半监督方法来增广和细化协同过滤算法的评级预测。该方法利用自我训练来评估评分的置信度，并通过系统的数据增广策略来提高算法性能。 |
| [^102] | [Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork.](http://arxiv.org/abs/2306.10698) | 人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。 |
| [^103] | [Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics.](http://arxiv.org/abs/2306.10656) | 本论文提出了一种名为VHGM的深度生成模型，基于掩码建模的方法来学习健康属性、生活方式和人格之间的关系。通过使用异构表格数据集，VHGM有效地学习了超过1,800个属性。该模型具有潜在的应用前景，例如用于医疗属性的虚拟测量和生活方式的假设验证。 |
| [^104] | [GCformer: An Efficient Framework for Accurate and Scalable Long-Term Multivariate Time Series Forecasting.](http://arxiv.org/abs/2306.08325) | GCformer是一个结合了全局卷积和局部Transformer分支的框架，旨在解决长期多元时间序列预测的精确性和可扩展性问题。通过引入具有亚线性复杂度的结构化卷积核，GCformer在各种基准数据集上实现了优于其他方法的性能。 |
| [^105] | [Time-aware Graph Structure Learning via Sequence Prediction on Temporal Graphs.](http://arxiv.org/abs/2306.07699) | 该论文提出了一种基于时间图序列预测的时态图结构学习方法，通过添加潜在的时间边来学习更好的图像结构，提高下游任务的性能。 |
| [^106] | [Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals.](http://arxiv.org/abs/2306.07071) | 这篇论文提出了一种名为ω-UCB的新上置信区间抽样策略，使用不对称置信区间以更准确、更紧密地估计奖励成本比，解决了现有预算多臂老虎机问题策略存在的问题，并在合成和真实环境中表现出色。 |
| [^107] | [Policy Regularization with Dataset Constraint for Offline Reinforcement Learning.](http://arxiv.org/abs/2306.06569) | 本文提出了一种离线强化学习方法，称为PRDC，它使用数据集约束来正则化学习策略。相比于现有方法，PRDC可以更有效地指导策略的更新，并提高其性能。 |
| [^108] | [Explainable Representation Learning of Small Quantum States.](http://arxiv.org/abs/2306.05694) | 该论文探讨了一种无监督机器学习模型在小量子态上的可解释表示学习方法，得到了一个与潜在结构相联系的可理解表示。 |
| [^109] | [A Bio-Inspired Chaos Sensor Model Based on the Perceptron Neural Network: Machine Learning Concept and Application for Computational Neuro-Science.](http://arxiv.org/abs/2306.01991) | 该论文介绍了一种基于感知器神经网络的生物灵感混沌传感器模型，用于估计神经动力学系统中脉冲序列的熵。经过训练，该模型能够以高精度近似一个短时间序列的模糊熵，即使隐藏层只有一个神经元，也能够达到良好的结果。 |
| [^110] | [Disentanglement via Latent Quantization.](http://arxiv.org/abs/2305.18378) | 本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。 |
| [^111] | [A Framework For Refining Text Classification and Object Recognition from Academic Articles.](http://arxiv.org/abs/2305.17401) | 本文提出了一种结合基于规则的方法和机器学习的框架，旨在解决从学术论文中提炼文本分类和对象识别的问题。 |
| [^112] | [Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation.](http://arxiv.org/abs/2305.05424) | 该论文提出了一种基于DDPM的方法来生成合成超声图像，用于医学图像分析任务中的有效替代品。在左心室和左房分割任务中，使用仅合成图像训练的神经网络相对于之前最先进方法提高了9.09％、3.7％和15.0％的Dice分数。 |
| [^113] | [ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification.](http://arxiv.org/abs/2305.04003) | 本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。 |
| [^114] | [Proportionally Representative Clustering.](http://arxiv.org/abs/2304.13917) | 本文提出了一个新的公平性准则——比例代表性公平性（PRF），并设计了有效的算法满足该准则。 |
| [^115] | [Regularized Complete Cycle Consistent GAN for Anomaly Detection.](http://arxiv.org/abs/2304.07769) | 本研究提出了RCALAD方法，通过循环一致性和新的鉴别器增强了GAN在异常检测中的效果。同时，利用补充分布引导重建和引入新的异常评分进一步提高了模型性能。 |
| [^116] | [Fairness through Aleatoric Uncertainty.](http://arxiv.org/abs/2304.03646) | 研究不确定性与公平性的关系，通过贝叶斯学习估算样本预测不确定性，发现低不确定性的数据更准确和公平，提出一种基于不确定性量化定义的新的公平性-效用目标。 |
| [^117] | [Mixed Regression via Approximate Message Passing.](http://arxiv.org/abs/2304.02229) | 本文提出了一种新的近似消息传递算法来解决在广义线性模型中的回归问题，该算法适用于混合线性回归、最大仿射回归和专家混合模型等问题。 |
| [^118] | [Multiscale Attention via Wavelet Neural Operators for Vision Transformers.](http://arxiv.org/abs/2303.12398) | 本文介绍了一种基于小波神经算子的多尺度注意力机制，它通过使用小波神经算子将注意力机制从局部扩展到全域和多尺度范围内，取得了比ViT和AFNO更显著的性能提高。 |
| [^119] | [Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches.](http://arxiv.org/abs/2303.11582) | 本文提出了一个基于贝叶斯算法的自适应实验框架，可灵活处理任何批处理大小。通过正态近似指导可扩展自适应设计，采用残余时限优化选择采样分配，实现了最先进的性能。 |
| [^120] | [FUSQA: Fetal Ultrasound Segmentation Quality Assessment.](http://arxiv.org/abs/2303.04418) | FUSQA是一个用于胎儿超声分割质量评估的简化模型，通过将分割质量评估过程定为自动化分类任务，实现了更准确的孕龄估计。 |
| [^121] | [Subset-Based Instance Optimality in Private Estimation.](http://arxiv.org/abs/2303.01262) | 本论文提出了一个新的定义来评估差分隐私估计算法的实例优化。我们的定义要求最优算法与一个最佳的已知数据集并在其较大的子集上进行性能评估相竞争，从而使基准算法比以前的工作更强大。我们还展示了在实值数据集上如何构建能够实现实例优化的隐私算法，并对均值进行了详细分析，证明我们的算法在估计一类广泛的数据集属性时能达到或超过渐近性能。 |
| [^122] | [Semantic-aware Node Synthesis for Imbalanced Heterogeneous Information Networks.](http://arxiv.org/abs/2302.14061) | 这项研究提出了一种解决不平衡异构信息网络中语义不平衡的方法，通过合成节点来应对少数类别的样本不足和偏倚的问题。 |
| [^123] | [Achieving High Accuracy with PINNs via Energy Natural Gradients.](http://arxiv.org/abs/2302.13163) | 我们提出了能量自然梯度下降算法，用于优化物理约束神经网络（PINNs）和深度Ritz方法。实验结果表明，与标准优化器相比，能量自然梯度下降算法能够产生更高准确度的解决方案。 |
| [^124] | [SGL-PT: A Strong Graph Learner with Graph Prompt Tuning.](http://arxiv.org/abs/2302.12449) | SGL-PT是一个具有图形提示调优的强大图形学习器，以缩小预训练和下游图形任务之间的差距，并提供一致的训练目标来增强预训练模型的能力。 |
| [^125] | [Robust expected improvement for Bayesian optimization.](http://arxiv.org/abs/2302.08612) | 本论文提出了一种鲁棒的期望改进（REI）方法，将对抗性方法引入贝叶斯优化框架，以解决在广阔的吸引域中寻找解决方案的问题。 |
| [^126] | [Label-efficient Time Series Representation Learning: A Review.](http://arxiv.org/abs/2302.06433) | 这篇综述介绍了针对时间序列数据中标记数据稀缺性问题的现有方法，并提供了一个新颖的分类系统来归纳这些方法。该综述总结了每种方法的最新进展并提出了未来的研究方向。 |
| [^127] | [Undersampling and Cumulative Class Re-decision Methods to Improve Detection of Agitation in People with Dementia.](http://arxiv.org/abs/2302.03224) | 本文对样本不平衡及标记不精确问题进行改进，仅用正常行为数据的20％即可训练具竞争力的痴呆症患者情绪激动检测模型。 |
| [^128] | [A deep complementary energy method for solid mechanics using minimum complementary energy principle.](http://arxiv.org/abs/2302.01538) | 本文提出了一种名为深度互补能量方法(DCEM)的新方法，它基于最小互补能量原理，在固体力学中解决偏微分方程(PDE)，并扩展为DCEM-P以满足更多方程的要求。 |
| [^129] | [InfiniCity: Infinite-Scale City Synthesis.](http://arxiv.org/abs/2301.09637) | InfiniCity是一个无限规模的城市合成框架，可以从随机噪音构建和渲染一个无限制大小的3D环境，具有可扩展的功能和交互式编辑。 |
| [^130] | [A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics.](http://arxiv.org/abs/2301.04900) | 本文介绍了一种行为良好的图神经网络近似复杂动力学的方法，包括必要的偏置和适当的神经网络结构，并提出了评估泛化能力和推断时预测置信度的方法。 |
| [^131] | [Why Batch Normalization Damage Federated Learning on Non-IID Data?.](http://arxiv.org/abs/2301.02982) | 本文通过首次的收敛性分析发现，非独立同分布数据中，在批归一化中局部和全局统计参数不匹配导致了梯度偏差，从而影响了联邦学习的收敛性。 |
| [^132] | [FedICT: Federated Multi-task Distillation for Multi-access Edge Computing.](http://arxiv.org/abs/2301.00389) | FedICT是一种用于多接入边缘计算的联邦多任务蒸馏方法，可以在个性化服务和异构机器学习模型的同时实现高效通信和模型异构性。 |
| [^133] | [Quality at the Tail.](http://arxiv.org/abs/2212.13925) | 本研究发现深度学习推理质量存在波动，引入了“尾部质量”的概念来描述这一现象。 |
| [^134] | [End-to-end AI framework for interpretable prediction of molecular and crystal properties.](http://arxiv.org/abs/2212.11317) | 我们引入了一个端到端的AI框架，利用最先进的模型和计算环境，可以预测分子和晶体的属性，提供了可解释的推断功能，并在领先的计算设施中进行了部署和测试。 |
| [^135] | [FedALA: Adaptive Local Aggregation for Personalized Federated Learning.](http://arxiv.org/abs/2212.01197) | FedALA是一种用于个性化联邦学习的方法，通过自适应局部聚合（ALA）模块来解决统计异质性问题，并在广泛的实验证明中超过了11种最先进的基准模型。 |
| [^136] | [Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge.](http://arxiv.org/abs/2211.15377) | 本文介绍了一种新方法MELD-FAIR来解决情感识别中的挑战，通过使用主动说话者检测和自动语音识别模型，重新对齐了MELD视频，并成功捕获了讲话者的面部表情。 |
| [^137] | [{\mu}Split: efficient image decomposition for microscopy data.](http://arxiv.org/abs/2211.12872) | uSplit是一种适用于荧光显微镜图像的高效图像分解方法，集成了横向上下文化，帮助训练更深的分层模型，并有效地减少平铺伪影问题。 |
| [^138] | [Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges.](http://arxiv.org/abs/2211.08413) | 本文提出去中心化联邦学习（DFL）来解决传统中心化FL（CFL）模型中的问题，主要研究DFL与CFL的差异、DFL的基础理论、DFL框架的设计与评估以及DFL在实际应用中的场景。 |
| [^139] | [Faster variational quantum algorithms with quantum kernel-based surrogate models.](http://arxiv.org/abs/2211.01134) | 提出了一种基于量子内核的代理模型，用于加速小规模至中等规模的变分量子算法，通过将计算负担转移到经典优化器组件上，大大减少了对量子处理器的查询次数。实验结果显示，这种代理模型相比传统经典内核具有更好的性能表现。 |
| [^140] | [Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization.](http://arxiv.org/abs/2210.17550) | 我们提出了一种新的一阶优化算法AG-OG，用于可分离的凸-凹极小极大优化问题。通过精细地利用问题结构，我们实现了最优收敛速率，可以适用于多种设置，包括双线性耦合的强凸-强凹、凸-强凹极小极大优化和双线性博弈。该算法还在随机设置下达到了最优收敛速率。这是第一个在双线性耦合的极小极大优化问题中在确定性和随机设置下都具有最优收敛速率的单次调用算法。 |
| [^141] | [Bayesian Hyperbolic Multidimensional Scaling.](http://arxiv.org/abs/2210.15081) | 这是一篇关于提出了一种贝叶斯双曲多维标度方法的论文，通过在双曲空间中表示低维图形来处理高维相关数据，从而适用于具有树状结构的数据，并且提供了有效的后验采样方法，降低了计算复杂性。 |
| [^142] | [MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos.](http://arxiv.org/abs/2210.09887) | 本文提出了MotionDeltaCNN，一个支持移动摄像机的稀疏CNN推断框架，通过引入球面缓冲区和填充卷积来高效融合新揭示的图像区域和已处理区域，从而实现对帧差异的推断。在移动摄像机视频中，相对于DeltaCNN，我们的方法性能提升了高达90%。 |
| [^143] | [Rigorous dynamical mean field theory for stochastic gradient descent methods.](http://arxiv.org/abs/2210.06591) | 本研究通过证明的闭式方程，描述了一类基于梯度的方法在高维情况下的精确渐进性能，为随机梯度下降等算法提供了理论支持，并提供了数值实现。 |
| [^144] | [R2C-GAN: Restore-to-Classify GANs for Blind X-Ray Restoration and COVID-19 Classification.](http://arxiv.org/abs/2209.14770) | R2C-GAN是一种用于盲目X射线恢复和COVID-19分类的生成对抗网络，通过图像恢复提高X射线图像质量，并实现更高的诊断性能。 |
| [^145] | [Catastrophic overfitting can be induced with discriminative non-robust features.](http://arxiv.org/abs/2206.08242) | 本研究通过控制性修改典型的自然图像数据集，研究了对抗训练中灾难性过度拟合的出现。通过注入看似无害的特征，可以在较小的epsilon值下引发灾难性过度拟合。 |
| [^146] | [Positive Unlabeled Contrastive Learning.](http://arxiv.org/abs/2206.01206) | 我们提出了一种正样本未标记对比学习的新方法，通过扩展对比损失和使用PU特定聚类方案，该方法在PU任务中学习到了优秀的表示，并在多个标准数据集上明显优于现有方法。 |
| [^147] | [FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata.](http://arxiv.org/abs/2203.16331) | FlexFringe提供了高效的概率有限自动机学习方法，可用于建模软件行为。该方法在实践中通过实现改进的状态合并策略实现了显著性能提升，并且能够从软件日志中学习可解释的模型，用于异常检测。与基于神经网络的解决方案相比，学习更小更复杂的模型能够提高FlexFringe在异常检测中的性能。 |
| [^148] | [Multi-task Representation Learning with Stochastic Linear Bandits.](http://arxiv.org/abs/2202.10066) | 本研究通过跨任务共享低维线性表示，提出了一种基于迹范数正则化的高效贪婪策略，在多任务学习中学习低维表示，无需知道潜在矩阵的秩。实验结果表明，该策略相比基线在多任务遗憾上有明显的优势。 |
| [^149] | [Multi-Scale Hybrid Vision Transformer for Learning Gastric Histology: AI-Based Decision Support System for Gastric Cancer Treatment.](http://arxiv.org/abs/2202.08510) | 提出了一个基于AI的决策支持系统，该系统能够对胃癌进行五种不同的病理学子分类，通过多尺度混合视觉Transformer网络进行胃癌诊断，在多中心队列上展示了可靠的诊断性能。 |
| [^150] | [Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data.](http://arxiv.org/abs/2111.13180) | 这项研究提出了一种名为变分吉布斯推断（VGI）的方法，用于解决使用不完全数据进行统计模型估计时的挑战。与标准的潜变量模型不同，VGI能够处理估计指数多个缺失变量的条件分布，从而为实际的数据集提供了更准确的模型估计。 |
| [^151] | [Clustering and Structural Robustness in Causal Diagrams.](http://arxiv.org/abs/2111.04513) | 本研究提出了一种聚类方法，通过定义过渡聚类并应用完备的算法，可以简化因果图中的变量关系，并保留因果效应的可辨识性属性。 |
| [^152] | [Non-stationary Online Learning with Memory and Non-stochastic Control.](http://arxiv.org/abs/2102.03758) | 本文研究了具有记忆的非平稳在线凸优化问题，引入了动态策略遗憾作为性能度量，并提出了一种算法，通过新颖的切换成本感知在线合奏方法解决了切换成本的关键技术挑战。 |
| [^153] | [Fair Densities via Boosting the Sufficient Statistics of Exponential Families.](http://arxiv.org/abs/2012.00188) | 本文介绍了一种利用增强算法来实现公平密度的方法，该方法通过学习指数族的充分统计量，以改善数据拟合，并确保最小的公平性保证。实验证明了该方法在真实数据上的有效性。 |

# 详细

[^1]: RAVEN：上下文学习与检索增强的编码器-解码器语言模型

    RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])

    [http://arxiv.org/abs/2308.07922](http://arxiv.org/abs/2308.07922)

    RAVEN是一种结合了检索增强的蒙特卡洛语言建模和前缀语言建模的模型，通过引入上下文融合学习，它能够在上下文学习方面取得比ATLAS更好的性能。

    

    本文研究了检索增强的编码器-解码器语言模型在上下文学习方面的能力。我们首先对现有的ATLAS模型进行全面分析，发现其在上下文学习方面存在限制，主要原因是预训练和测试之间存在不匹配，以及上下文长度受限。为了解决这些问题，我们提出了RAVEN模型，该模型结合了检索增强的蒙特卡洛语言建模和前缀语言建模。我们还引入了上下文融合学习，通过使模型能够利用更多上下文示例而无需额外训练或模型修改来提高少样本性能。通过大量实验，我们证明了RAVEN在某些场景下明显优于ATLAS，并达到了与最先进的语言模型相当的结果，尽管参数数量显著较少。我们的工作强调了检索增强的编码器-解码器语言模型的潜力。

    In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
    
[^2]: 正则表达式推理挑战

    The Regular Expression Inference Challenge. (arXiv:2308.07899v1 [cs.LG])

    [http://arxiv.org/abs/2308.07899](http://arxiv.org/abs/2308.07899)

    正则表达式推理挑战是一个以找到最小正则表达式为目标的任务，具有应用广泛、难度可调、适用于代码/语言建模和机器学习的特点。

    

    我们提出将正则表达式推理（REI）作为代码/语言建模以及更广泛的机器学习社区的挑战。REI是一个有监督的机器学习和程序合成任务，它提出了从示例中找到最小正则表达式的问题：给定两个有限字符串集合P和N以及一个成本函数cost(·)，任务是生成一个接受P中所有字符串并拒绝N中所有字符串的表达式r，而不存在其他表达式r'，使得cost(r')<cost(r)。REI作为一个挑战问题具有以下优势：（i）正则表达式是众所周知、广泛使用的，是代码的自然理想化；（ii）REI的渐近最坏情况复杂性已被充分理解；（iii）REI具有一小部分易于理解的参数（例如P或N的基数、示例的字符串长度或成本函数），这使得我们可以轻松调整REI的难度；（iv）对于基于深度学习的M模型而言，REI是一个未解决的问题。

    We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')<\text{cost}(r)$.  REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based M
    
[^3]: SciRE-Solver: 用得分积分求解器和递归导数估计快速采样扩散概率模型

    SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation. (arXiv:2308.07896v1 [stat.ML])

    [http://arxiv.org/abs/2308.07896](http://arxiv.org/abs/2308.07896)

    SciRE-Solver是一种高效的采样器，通过引入得分积分求解器和递归导数估计方法，它解决了扩散概率模型采样过程缓慢的挑战，并实现了最先进的采样性能。

    

    扩散概率模型(DPMs)是一类强大的生成模型，以其生成高保真图像样本的能力而闻名。DPMs的实现面临的主要挑战是采样过程缓慢。在这项工作中，我们提出了一种高效的DPMs采样器。具体而言，我们针对与DPMs采样过程对应的扩散ODE提出了一个基于得分的精确解决方案范式，该范式为求解扩散ODE的数值算法开发提供了新的视角。为了实现高效的采样器，我们提出了一种递归导数估计(RDE)方法来减小估计误差。通过我们提出的解决方案范式和RDE方法，我们提出了具有收敛顺序保证的得分积分求解器(SciRE-Solver)来解决扩散ODEs。SciRE-Solver在离散时间和连续时间DPMs上获得了最先进的采样性能，并且仅需有限数量的得分函数评估(NFE)。

    Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs
    
[^4]: 关于正则化的Radon-Nikodym导数

    On regularized Radon-Nikodym differentiation. (arXiv:2308.07887v1 [math.ST])

    [http://arxiv.org/abs/2308.07887](http://arxiv.org/abs/2308.07887)

    本文讨论了估计Radon-Nikodym导数的问题，并提出了基于正则化方案的解决方法。通过考虑导数的平滑度和估计空间的容量，建立了相应算法的收敛速度。数值模拟进一步验证了理论结果。

    

    本文讨论了估计Radon-Nikodym导数的问题。这个问题在各种应用中出现，比如协变量偏移适应、似然比检验、互信息估计和条件概率估计。为了解决上述问题，我们采用再生核希尔伯特空间中的一般正则化方案。通过考虑导数的平滑度和估计它的空间的容量，建立了相应正则化算法的收敛速度。这是以一般源条件和正则化的Christoffel函数为基础的。我们还发现，在任何特定点上重建Radon-Nikodym导数可以具有高精度。我们的理论结果通过数值模拟进行了说明。

    We discuss the problem of estimating Radon-Nikodym derivatives. This problem appears in various applications, such as covariate shift adaptation, likelihood-ratio testing, mutual information estimation, and conditional probability estimation. To address the above problem, we employ the general regularization scheme in reproducing kernel Hilbert spaces. The convergence rate of the corresponding regularized algorithm is established by taking into account both the smoothness of the derivative and the capacity of the space in which it is estimated. This is done in terms of general source conditions and the regularized Christoffel functions. We also find that the reconstruction of Radon-Nikodym derivatives at any particular point can be done with high order of accuracy. Our theoretical results are illustrated by numerical simulations.
    
[^5]: 回归基础：对现代时间序列分类算法的合理检查

    Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms. (arXiv:2308.07886v1 [cs.LG])

    [http://arxiv.org/abs/2308.07886](http://arxiv.org/abs/2308.07886)

    对现代时间序列分类算法的回归基础进行了合理检查，发现在某些问题上经典的无时间顺序的表格方法表现更好。

    

    时间序列分类的最新发展，从1NN-DTW算法到ROCKET分类器家族，已经取得了长足的进步。然而，在目前快速发展新分类器的过程中，退一步进行简单的基准检查是必要的。这些检查经常被忽视，因为研究人员专注于建立新的最先进结果，开发可扩展的算法，并使模型能够解释。然而，有许多数据集一开始看起来像是时间序列，但经典的无时间顺序的表格方法可能在这些问题上表现更好。例如，对于光谱数据集，表格方法往往能明显优于最近的时间序列方法。在本研究中，我们将经典的机器学习方法（例如Ridge、LDA、RandomForest）与ROCKET分类器家族（例如Rocket、MiniRocket、MultiRocket）的效果进行比较。表格模型简单且性能很好。

    The state-of-the-art in time series classification has come a long way, from the 1NN-DTW algorithm to the ROCKET family of classifiers. However, in the current fast-paced development of new classifiers, taking a step back and performing simple baseline checks is essential. These checks are often overlooked, as researchers are focused on establishing new state-of-the-art results, developing scalable algorithms, and making models explainable. Nevertheless, there are many datasets that look like time series at first glance, but classic algorithms such as tabular methods with no time ordering may perform better on such problems. For example, for spectroscopy datasets, tabular methods tend to significantly outperform recent time series methods. In this study, we compare the performance of tabular models using classic machine learning approaches (e.g., Ridge, LDA, RandomForest) with the ROCKET family of classifiers (e.g., Rocket, MiniRocket, MultiRocket). Tabular models are simple and very e
    
[^6]: 利用深度学习进行胎儿心脏MRI重建的挑战

    The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning. (arXiv:2308.07885v1 [eess.IV])

    [http://arxiv.org/abs/2308.07885](http://arxiv.org/abs/2308.07885)

    本研究利用深度学习网络重建了动态无呼吸胎儿心脏MRI的kt-SENSE style采集数据，通过恢复欠采样数据提高了重建质量和采集策略的优化。

    

    动态无呼吸胎儿心脏MRI是最具挑战性的模态之一，需要高时空分辨率来描述胎儿心脏的快速变化。深度学习方法恢复欠采样数据的能力可以帮助优化kt-SENSE采集策略，并提高非门控kt-SENSE重建质量。本研究使用大量体内数据探索了监督式深度学习网络用于kt-SENSE风格采集数据的重建。通过访问全采样低分辨率多线圈胎儿心脏MRI，我们研究网络从欠采样数据中恢复全采样数据的性能。我们考虑模型架构以及训练策略，考虑到它们在用于收集数据的真实临床设置中的应用，以使网络能够预测欠采样数据。我们探索了一系列修改来形成动态胎儿心脏重建性能的基准评估。

    Dynamic free-breathing fetal cardiac MRI is one of the most challenging modalities, which requires high temporal and spatial resolution to depict rapid changes in a small fetal heart. The ability of deep learning methods to recover undersampled data could help to optimise the kt-SENSE acquisition strategy and improve non-gated kt-SENSE reconstruction quality. In this work, we explore supervised deep learning networks for reconstruction of kt-SENSE style acquired data using an extensive in vivo dataset. Having access to fully-sampled low-resolution multi-coil fetal cardiac MRI, we study the performance of the networks to recover fully-sampled data from undersampled data. We consider model architectures together with training strategies taking into account their application in the real clinical setup used to collect the dataset to enable networks to recover prospectively undersampled data. We explore a set of modifications to form a baseline performance evaluation for dynamic fetal cardi
    
[^7]: 迈向时态边缘回归：关于国家间农业贸易的案例研究

    Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations. (arXiv:2308.07883v1 [cs.LG])

    [http://arxiv.org/abs/2308.07883](http://arxiv.org/abs/2308.07883)

    本文研究了国家间农业贸易的时态边缘回归任务，探索了使用图神经网络（GNNs）进行边缘回归的应用。实验结果显示现有GNNs的不足，提出了TGN作为边缘回归任务的更合适选择。

    

    最近，图神经网络（GNNs）在动态图任务如节点分类、链接预测和图回归等方面表现出了很有前景的性能。然而，很少有工作研究了具有重要现实应用的时态边缘回归任务。在本文中，我们探索了GNNs在静态和动态设置下边缘回归任务中的应用，重点是预测国家之间的食品和农业贸易价值。我们引入了三个简单但强大的基准线，并使用联合国贸易数据集全面评估了一个静态和三个动态GNN模型。我们的实验结果表明，这些基线在各种设置下表现出异常强大的性能，突显了现有GNNs的不足之处。我们还发现，TGN优于其他GNN模型，这表明TGN是边缘回归任务的更合适选择。此外，我们注意到训练样本中负边的比例显著影响了测试的结果。

    Recently, Graph Neural Networks (GNNs) have shown promising performance in tasks on dynamic graphs such as node classification, link prediction and graph regression. However, few work has studied the temporal edge regression task which has important real-world applications. In this paper, we explore the application of GNNs to edge regression tasks in both static and dynamic settings, focusing on predicting food and agriculture trade values between nations. We introduce three simple yet strong baselines and comprehensively evaluate one static and three dynamic GNN models using the UN Trade dataset. Our experimental results reveal that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs. We also find that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks. Moreover, we note that the proportion of negative edges in the training samples significantly affects the test p
    
[^8]: 通过编码本知识、自然语言推理和ChatGPT来合成政治零样本关系分类

    Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])

    [http://arxiv.org/abs/2308.07876](http://arxiv.org/abs/2308.07876)

    该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。

    

    最近的事件编码的监督模型在性能方面远远超过模式匹配方法。然而，它们仅仅依赖于新的注释，忽视了专家数据库中的大量知识，限制了它们在细粒度分类中的适用性。为了解决这些限制，我们通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类。我们的研究涵盖了ChatGPT和一种新颖的基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，将任务分解为上下文、语态和类别消歧的不同层次。该框架提高了解释性、效率和对模式更改的适应性。通过在我们新策划的数据集上进行大量实验，我们指出了ChatGPT中的不稳定性问题，并突出了ZSP的卓越性能。ZSP在细粒度根代码分类的F1得分上取得了令人印象深刻的提高40%。

    Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
    
[^9]: 情感嵌入——从异质情感数据中学习稳定且均匀的抽象表示

    Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets. (arXiv:2308.07871v1 [cs.LG])

    [http://arxiv.org/abs/2308.07871](http://arxiv.org/abs/2308.07871)

    这篇论文提出了一种统一的计算模型，通过学习情感嵌入，独立于不同的语言、交流方式、媒体或标签形式，从而将以往对不同类型异质情感数据的研究整合起来。

    

    人类情感通过多种交流方式和媒体格式表达，因此它们的计算研究同样多样化，涉及到自然语言处理、音频信号分析、计算机视觉等等。在先前的研究中，情感被以不同的形式进行描述（极性尺度、基本情感类别、维度方法、评价理论等），导致数据集、预测模型和情感分析软件工具的多样化增长。由于这两种不同类型的异质性，在表达和表示层面上，迫切需要统一以往对越来越分散的数据和标签类型的研究成果。本文提出了一个统一的计算模型。我们提出了一种训练过程，可以学习一种共享的情感潜在表示，即所谓情感嵌入，不依赖于不同的自然语言、交流方式、媒体或表示标签形式。

    Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc. Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis. Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types. This article presents such a unifying computational model. We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label form
    
[^10]: 通过预测编码实现脑启发式计算智能

    Brain-Inspired Computational Intelligence via Predictive Coding. (arXiv:2308.07870v1 [cs.AI])

    [http://arxiv.org/abs/2308.07870](http://arxiv.org/abs/2308.07870)

    这项研究介绍了一种通过预测编码的脑启发式计算智能方法，它可以解决现有人工智能方法的一些重要限制，并具有在机器学习领域有希望的应用潜力。

    

    人工智能（AI）正在迅速成为本世纪的关键技术之一。到目前为止，在AI领域取得的大部分成果都是使用误差反向传播学习算法训练的深度神经网络所实现的。然而，这种方法的普及应用已经凸显出了一些重要的局限性，例如计算成本高、难以量化不确定性、缺乏鲁棒性、不可靠性和生物学上的不合理性。解决这些限制可能需要受到神经科学理论的启发和指导的方案。其中一种理论称为预测编码（PC），在机器智能任务中表现出有希望的性能，具有令人兴奋的特性，使其在机器学习社区中具有潜在的价值：PC可以模拟不同脑区的信息处理，可以用于认知控制和机器人技术，并在变分推理方面具有坚实的数学基础，提供了一个强大的工具。

    Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a pow
    
[^11]: 使用高斯过程进行功率流学习的图结构核设计

    Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes. (arXiv:2308.07867v1 [eess.SY])

    [http://arxiv.org/abs/2308.07867](http://arxiv.org/abs/2308.07867)

    本文提出了一种图结构核设计，用于使用高斯过程进行功率流学习，通过顶点度核和网络扫描主动学习方案，实现了更高效的学习和样本复杂度降低。

    

    本文提出了一种基于物理启发的图结构核设计，用于使用高斯过程进行功率流学习。该核被命名为顶点度核（VDK），它依赖于基于网络图或拓扑的电压注入关系的潜在分解。值得注意的是，VDK设计避免了需要解决核搜索的优化问题。为了提高效率，我们还探索了一种图缩减方法，以获得具有较少项的VDK表示。此外，我们提出了一种新颖的网络扫描主动学习方案，它智能地选择顺序训练输入，加速VDK的学习。利用VDK的可加性结构，主动学习算法对GP的预测方差进行了块下降类型的过程，作为信息增益的代理。仿真结果表明，所提出的VDK-GP与中等规模500个节点和大规模1354个节点的完整GP相比，实现了超过两倍的样本复杂度降低。

    This paper presents a physics-inspired graph-structured kernel designed for power flow learning using Gaussian Process (GP). The kernel, named the vertex-degree kernel (VDK), relies on latent decomposition of voltage-injection relationship based on the network graph or topology. Notably, VDK design avoids the need to solve optimization problems for kernel search. To enhance efficiency, we also explore a graph-reduction approach to obtain a VDK representation with lesser terms. Additionally, we propose a novel network-swipe active learning scheme, which intelligently selects sequential training inputs to accelerate the learning of VDK. Leveraging the additive structure of VDK, the active learning algorithm performs a block-descent type procedure on GP's predictive variance, serving as a proxy for information gain. Simulations demonstrate that the proposed VDK-GP achieves more than two fold sample complexity reduction, compared to full GP on medium scale 500-Bus and large scale 1354-Bus 
    
[^12]: 基于印象的推荐系统

    Impression-Aware Recommender Systems. (arXiv:2308.07857v1 [cs.IR])

    [http://arxiv.org/abs/2308.07857](http://arxiv.org/abs/2308.07857)

    基于印象的推荐系统利用印象数据源提升推荐质量，通过综述分类推荐系统、数据集和评估方法，揭示开放性问题和未来研究方向。

    

    新型数据源为改进推荐系统的质量带来了新的机遇。印象是一种包含过去推荐（展示的项目）和传统互动的新型数据源。研究人员可以利用印象来优化用户偏好并克服当前推荐系统研究中的限制。印象的相关性和兴趣度逐年增加，因此需要对这类推荐系统中相关工作进行综述。我们提出了一篇关于使用印象的推荐系统的系统文献综述，侧重于研究中的三个基本方面：推荐系统、数据集和评估方法。我们对使用印象的推荐系统的论文进行了三个分类，详细介绍了每篇综述论文，描述了具有印象的数据集，并分析了现有的评估方法。最后，我们提出了值得关注的开放性问题和未来的研究方向，强调了文献中缺失的方面。

    Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that
    
[^13]: Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) 该论文标题已翻译：二元强化学习。

    Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])

    [http://arxiv.org/abs/2308.07843](http://arxiv.org/abs/2308.07843)

    该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。

    

    移动医疗旨在通过在个人日常生活中提供干预来提高健康结果。照顾伴侣和社会支持网络的参与经常在帮助个人管理繁重的医疗条件方面起着关键作用。这为移动医疗提供了机会，设计针对二元关系——目标人和其照顾伴侣之间关系——以提高社会支持的干预措施。在本文中，我们开发了二元强化学习（Dyadic RL），这是一种基于环境因素和目标人及其照顾伴侣的过去反馈个性化干预措施的在线强化学习算法。在这里，多组干预措施影响着二元关系在多个时间间隔内。开发的二元强化学习是贝叶斯和层次的。我们正式介绍了问题设定，开发了二元强化学习并确定了遗憾边界。通过模拟，我们展示了二元强化学习的实证效果。

    Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
    
[^14]: 简单高效的部分图对抗攻击: 一个新的视角

    Simple and Efficient Partial Graph Adversarial Attack: A New Perspective. (arXiv:2308.07834v1 [cs.LG])

    [http://arxiv.org/abs/2308.07834](http://arxiv.org/abs/2308.07834)

    提出了一种名为部分图攻击 (PGA) 的全新方法，它选取脆弱节点作为攻击目标，相比现有的全局攻击方法有更高的效率和改进的空间。

    

    随着图神经网络的研究变得更加深入和全面，它们的稳健性和安全性也受到了广泛的关注。现有的全局攻击方法将图中的所有节点视为攻击目标。尽管现有方法已经取得了很好的结果，但仍有很大的改进空间。关键问题是当前方法过于僵化地遵循全局攻击的定义。它们忽视了一个重要问题，即不同的节点具有不同的稳健性，对攻击不具有相等的韧性。从全局攻击者的角度来看，我们应该明智地安排攻击预算，而不是浪费在具有高稳健性的节点上。为此，我们提出了一种全新的方法，命名为部分图攻击(PGA)，它选取了脆弱的节点作为攻击目标。首先，为了选择脆弱的节点，我们提出了一种层次化的目标选择策略，允许攻击者只关注易于攻击的节点。然后，我们提出了一个成本效率的…

    As the study of graph neural networks becomes more intensive and comprehensive, their robustness and security have received great research interest. The existing global attack methods treat all nodes in the graph as their attack targets. Although existing methods have achieved excellent results, there is still considerable space for improvement. The key problem is that the current approaches rigidly follow the definition of global attacks. They ignore an important issue, i.e., different nodes have different robustness and are not equally resilient to attacks. From a global attacker's view, we should arrange the attack budget wisely, rather than wasting them on highly robust nodes. To this end, we propose a totally new method named partial graph attack (PGA), which selects the vulnerable nodes as attack targets. First, to select the vulnerable items, we propose a hierarchical target selection policy, which allows attackers to only focus on easy-to-attack nodes. Then, we propose a cost-e
    
[^15]: REFORMS: 基于机器学习的科学报告标准

    REFORMS: Reporting Standards for Machine Learning Based Science. (arXiv:2308.07832v1 [cs.LG])

    [http://arxiv.org/abs/2308.07832](http://arxiv.org/abs/2308.07832)

    REFORMS是一个基于机器学习的科学报告标准，旨在解决使用机器学习方法在科学研究中出现的有效性、可重复性和可推广性失败问题。这个标准由32个问题和一套指导方针组成，可作为研究人员设计和实施科研时的参考资源。

    

    机器学习方法在科学研究中得到了广泛应用。然而，这些方法的采用也伴随着有效性、可重复性和可推广性的失败。这些失败可能会阻碍科学进展，导致对无效结论的错误共识，并削弱基于机器学习的科学的可信度。机器学习方法在不同学科中常常以相似的方式应用且失败。出于这个观察，我们的目标是为基于机器学习的科学提供清晰的报告标准。基于对过去文献的广泛评论，我们提出了REFORMS检查表（$\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience）。它由32个问题和一套配套的指导方针组成。REFORMS是基于19位研究人员的共识开发的，这些人来自计算机科学、数据科学、数学、社会科学和生物医学科学领域。REFORMS可以为研究人员在设计和实施科研时提供参考。

    Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing 
    
[^16]: Cerberus:基于放松电压曲线的深度学习混合模型用于锂离子电池老化估计和预测

    Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves. (arXiv:2308.07824v1 [cs.LG])

    [http://arxiv.org/abs/2308.07824](http://arxiv.org/abs/2308.07824)

    本论文提出了一个基于深度学习的混合模型，通过提取与锂离子电池老化相关的特征，动态地估计当前容量和预测未来容量。这种模型有助于准确快速估计和预测锂离子电池的老化状态。

    

    锂离子电池的退化过程与其作为动力源和储能设备的整个生命周期密切相关，涵盖了性能交付和循环利用等方面。因此，对锂离子电池老化状态的准确快速估计或预测引起了广泛关注。然而，现有研究主要集中在老化估计或预测上，忽视了两个方面的动态融合。本文提出了一个基于深度学习的容量老化估计和预测的混合模型，从充电和放电放松过程中提取与老化密切相关的显著特征。通过融合历史容量衰减数据，该模型动态提供锂离子电池当前容量的估计和未来容量的预测。我们的方法在包含不同充放电循环的新数据集上进行了验证。

    The degradation process of lithium-ion batteries is intricately linked to their entire lifecycle as power sources and energy storage devices, encompassing aspects such as performance delivery and cycling utilization. Consequently, the accurate and expedient estimation or prediction of the aging state of lithium-ion batteries has garnered extensive attention. Nonetheless, prevailing research predominantly concentrates on either aging estimation or prediction, neglecting the dynamic fusion of both facets. This paper proposes a hybrid model for capacity aging estimation and prediction based on deep learning, wherein salient features highly pertinent to aging are extracted from charge and discharge relaxation processes. By amalgamating historical capacity decay data, the model dynamically furnishes estimations of the present capacity and forecasts of future capacity for lithium-ion batteries. Our approach is validated against a novel dataset involving charge and discharge cycles at varying
    
[^17]: 深度强化学习在流程设计中的应用: 综述与展望

    Deep reinforcement learning for process design: Review and perspective. (arXiv:2308.07822v1 [cs.LG])

    [http://arxiv.org/abs/2308.07822](http://arxiv.org/abs/2308.07822)

    深度强化学习在流程设计中的综述和展望。该研究调查了强化学习在流程设计中的最新研究，并探讨了潜在挑战和未来工作，以充分发挥其在化学工程中的潜力。

    

    化学工业向可再生能源和原料供应的转型需要新的概念性流程设计方法。最近，人工智能方面取得的突破为加速这一转变提供了机会。具体而言，深度强化学习作为机器学习的一个子类，已经展示出解决复杂决策问题和帮助可持续流程设计的潜力。我们通过三个主要要素对流程设计中强化学习的最新研究进行了综述：（i）信息表示、（ii）代理架构，以及（iii）环境和奖励。此外，我们讨论了潜在挑战和有前景的未来工作，以充分发挥强化学习在化学工程的流程设计中的潜力。

    The transformation towards renewable energy and feedstock supply in the chemical industry requires new conceptual process design approaches. Recently, breakthroughs in artificial intelligence offer opportunities to accelerate this transition. Specifically, deep reinforcement learning, a subclass of machine learning, has shown the potential to solve complex decision-making problems and aid sustainable process design. We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward. Moreover, we discuss perspectives on underlying challenges and promising future works to unfold the full potential of reinforcement learning for process design in chemical engineering.
    
[^18]: 量化队列系统中的学习成本

    Quantifying the Cost of Learning in Queueing Systems. (arXiv:2308.07817v1 [cs.LG])

    [http://arxiv.org/abs/2308.07817](http://arxiv.org/abs/2308.07817)

    本文提出了一种新的度量方法，即学习队列中的成本 (CLQ)，用于量化由参数不确定性导致的时间平均队列长度最大增加量。该度量方法可以捕捉学习队列系统的统计复杂性，不局限于渐近性能。

    

    队列系统是广泛应用的随机模型，应用于通信网络、医疗保健、服务系统等等。虽然它们的最优控制已经得到了广泛研究，但大多数现有方法都假设系统参数的完美知识。然而，在实践中，参数不确定性很常见，因此最近一系列关于队列系统的学习的研究产生了。这个新兴的研究方向主要关注所提算法的渐近性能。本文中，我们认为渐近度量，即着眼于后期性能的度量，无法捕捉学习队列系统中固有的统计复杂性，这种复杂性通常出现在早期阶段。相反，我们提出了学习队列中的成本 (CLQ)，这是一种新的度量方法，可以衡量由参数不确定性导致的时间平均队列长度的最大增加量。我们对单队列多服务器系统的CLQ进行了表征。

    Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.  In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system,
    
[^19]: 在联邦学习中的公平性和隐私问题及其在医疗保健中的影响

    Fairness and Privacy in Federated Learning and Their Implications in Healthcare. (arXiv:2308.07805v1 [cs.CR])

    [http://arxiv.org/abs/2308.07805](http://arxiv.org/abs/2308.07805)

    联邦学习在解决分布式学习中的安全和隐私问题方面具有重要的应用价值，特别是在医疗保健领域。

    

    目前存在许多情境，分布式学习由于安全和通信限制而变得困难。一个常见的考虑因素是医疗保健领域，在这个领域中，数据通常受到HIPAA等数据使用条例的管制。另一方面，更大的样本量和共享数据模型是必要的，以允许模型更好地推广，考虑到更多的变异性和平衡不足的类别。联邦学习是一种分布式学习模型，它允许数据以分散的方式进行训练。这反过来解决了数据安全、隐私和漏洞考虑，因为数据本身并不在给定的学习网络节点之间共享。联邦学习面临的三个主要挑战包括节点数据不是独立同分布的(iid)，客户机需要在对等节点之间进行高水平的通信开销，以及网络中不同客户机的异构性。

    Currently, many contexts exist where distributed learning is difficult or otherwise constrained by security and communication limitations. One common domain where this is a consideration is in Healthcare where data is often governed by data-use-ordinances like HIPAA. On the other hand, larger sample sizes and shared data models are necessary to allow models to better generalize on account of the potential for more variability and balancing underrepresented classes. Federated learning is a type of distributed learning model that allows data to be trained in a decentralized manner. This, in turn, addresses data security, privacy, and vulnerability considerations as data itself is not shared across a given learning network nodes. Three main challenges to federated learning include node data is not independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and there is the heterogeneity of different clients within a network with re
    
[^20]: 使用动态期望最大化方法在有色噪声下自适应估计噪声协方差

    Adaptive Noise Covariance Estimation under Colored Noise using Dynamic Expectation Maximization. (arXiv:2308.07797v1 [eess.SY])

    [http://arxiv.org/abs/2308.07797](http://arxiv.org/abs/2308.07797)

    本文提出了一种基于脑启发的算法，用于自适应估计受到有色噪声影响的动态系统中的噪声协方差矩阵。通过扩展动态期望最大化算法，我们的估计器能够同时进行在线噪声协方差和状态估计，并达到全局最优解。

    

    在动态系统中准确估计噪声协方差矩阵（NCM）对状态估计和控制至关重要，因为它对其最优性有重要影响。尽管已经开发了大量的NCM估计方法，但大多数方法假设噪声是白噪声。然而，在许多实际应用中，噪声是有色的（例如，它们展现出时间自相关性），导致次优解。本文介绍了一种新颖的类脑算法，能够准确自适应地估计受到有色噪声影响的动态系统的NCM。具体而言，我们将动态期望最大化算法扩展为通过优化自由能目标同时进行在线噪声协方差和状态估计。我们在数值模拟实验中数学证明了我们的NCM估计器收敛到该自由能目标的全局最优解。使用随机化的数值模拟，我们展示了我们的估计器在最小噪声协方差的情况下优于其他九个基线方法。

    The accurate estimation of the noise covariance matrix (NCM) in a dynamic system is critical for state estimation and control, as it has a major influence in their optimality. Although a large number of NCM estimation methods have been developed, most of them assume the noises to be white. However, in many real-world applications, the noises are colored (e.g., they exhibit temporal autocorrelations), resulting in suboptimal solutions. Here, we introduce a novel brain-inspired algorithm that accurately and adaptively estimates the NCM for dynamic systems subjected to colored noise. Particularly, we extend the Dynamic Expectation Maximization algorithm to perform both online noise covariance and state estimation by optimizing the free energy objective. We mathematically prove that our NCM estimator converges to the global optimum of this free energy objective. Using randomized numerical simulations, we show that our estimator outperforms nine baseline methods with minimal noise covarianc
    
[^21]: 为生成式语言模型提供信息的命名实体识别解码

    Informed Named Entity Recognition Decoding for Generative Language Models. (arXiv:2308.07791v1 [cs.CL])

    [http://arxiv.org/abs/2308.07791](http://arxiv.org/abs/2308.07791)

    该论文提出了一种利用生成式语言模型进行命名实体识别解码的方法，通过在信息提取过程中应用生成模型的语言理解能力，提高了性能并消除了幻觉的风险。

    

    越来越大的语言模型具有越来越强的能力，已成为被广泛应用的文本处理工具。然而，信息提取任务，如命名实体识别，仍然受到之前一代仅编码器的转换器模型的影响。在这里，我们提出了一种简单但有效的方法，称为Informed Named Entity Recognition Decoding（iNERD），它将命名实体识别视为一种生成过程。它以面向未来的方式利用最近生成模型的语言理解能力，并采用了一种基于信息提取的有限文本生成方法，提高了性能并消除了任何幻觉的风险。我们在合并的命名实体语料库上粗调优化了模型，评估了五个生成式语言模型在八个命名实体识别数据集上的表现，并取得了显著的结果。

    Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, espec
    
[^22]: DiffV2S: 基于扩散的视频转语音合成与视觉引导的说话人嵌入

    DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding. (arXiv:2308.07787v1 [cs.SD])

    [http://arxiv.org/abs/2308.07787](http://arxiv.org/abs/2308.07787)

    本文提出了一种基于扩散的视频转语音合成方法，采用视觉引导说话人嵌入的方式，可以在推断过程中不需要额外的音频信息，仅通过输入的视觉信息即可产生丰富的说话人嵌入信息。

    

    最近的研究在视频转语音合成方面取得了令人印象深刻的结果，即仅通过视觉输入重建语音。然而，由于缺乏足够的指导来正确推断出适当声音的内容，以前的工作在合成语音方面仍存在困难。为了解决这个问题，他们采用额外的说话人嵌入作为来自参考听觉信息的说话风格指导。然而，在推断过程中，往往无法从相应的视频输入中获得音频信息。本文提出了一种新颖的使用自我监督预训练模型和提示调整技术的视觉引导说话人嵌入提取器。通过这样做，可以仅通过输入的视觉信息产生丰富的说话人嵌入信息，在推断过程中不需要额外的音频信息。

    Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations
    
[^23]: 自主机器人的分层生成建模

    Hierarchical generative modelling for autonomous robots. (arXiv:2308.07775v1 [cs.RO])

    [http://arxiv.org/abs/2308.07775](http://arxiv.org/abs/2308.07775)

    该论文研究了自主机器人操作中运动控制的分层生成建模方法，通过模仿人类的深层时间结构来实现多级规划和协调肢体运动，以实现复杂的整体身体动作。

    

    人类在与周围环境互动时可以产生复杂的整体身体动作，通过计划、执行和组合各个肢体的运动。我们在自主机器人操作环境中探索了运动控制的这一基本方面。我们采用分层生成建模的方法来处理这个问题，该方法配备了多级规划，以模仿人类运动控制的深层时间结构。在这里，时间深度是指前向或生成模型的连续层次的嵌套时间尺度，例如，交付一个物体需要一个全局计划来上下文化多个肢体的快速协调。这种时间尺度的分离也推动了机器人和控制领域的发展。具体来说，为了实现多功能的感知动作控制，以分层结构化规划和低层肢体运动控制是有优势的。我们使用数值和物理模拟进行实验。

    Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements. We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations. We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control. Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs. This separation of temporal scales also motivates robotics and control. Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs. We use numerical and physical simulation to conduct e
    
[^24]: 无监督异常检测的图编码解码网络

    A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection. (arXiv:2308.07774v1 [cs.LG])

    [http://arxiv.org/abs/2308.07774](http://arxiv.org/abs/2308.07774)

    本文提出了一种无监督图编码解码模型，用于检测图中的异常节点。在编码阶段，通过设计一种新的池化机制，该模型能够根据节点的异常程度对节点进行排序。模型的池化过程具有较低的计算复杂度和更高的可解释性。

    

    许多图神经网络中的关键组件是池化操作，它旨在减小图的大小同时保留重要的结构信息。然而，大多数现有的图池化策略依赖于通过使用图神经网络层获得的分配矩阵，该矩阵具有可训练的参数，往往导致显著的计算复杂性和池化过程的缺乏可解释性。本文提出了一种无监督图编码解码模型，通过学习一种异常评分函数对节点进行排序，从而检测出图中的异常节点。在编码阶段，我们设计了一种新的池化机制，命名为LCPool，它利用局部约束线性编码进行特征编码，通过求解带有局部正则化项的最小二乘优化问题来找到聚类分配矩阵。通过在编码过程中强制执行局部约束，LCPool被设计成免费

    A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free fr
    
[^25]: MOLE: MOdular Learning FramEwork通过最大化互信息引入了一种异步和本地化的神经网络学习框架

    MOLE: MOdular Learning FramEwork via Mutual Information Maximization. (arXiv:2308.07772v1 [cs.LG])

    [http://arxiv.org/abs/2308.07772](http://arxiv.org/abs/2308.07772)

    MOLE是一种异步和本地化的神经网络学习框架，它通过层次化的模块化方式和最大化互信息的方法来进行训练，具有局部优化和梯度隔离的特性，并且在不同类型的数据上都有良好的应用效果。

    

    本文介绍了一种名为MOLE（MOdular Learning Framework）的异步和本地化学习框架，该框架通过层次化的方式对神经网络进行模块化，并通过最大化互信息的方式定义了每个模块的训练目标，并按顺序通过最大化互信息的方法对每个模块进行训练。MOLE使得训练变得具有局部优化和模块之间梯度隔离的特性，这种方案在生物学上更具可行性，比反向传播更加合理。我们在向量、网格和图形类型的数据上进行了实验。特别地，这个框架能够解决图形类型数据的图形级和节点级任务。因此，通过实验证明MOLE可以广泛适用于不同类型的数据。

    This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.
    
[^26]: NeFL: 针对异构客户端的嵌套联邦学习

    NeFL: Nested Federated Learning for Heterogeneous Clients. (arXiv:2308.07761v1 [cs.LG])

    [http://arxiv.org/abs/2308.07761](http://arxiv.org/abs/2308.07761)

    NeFL是一个嵌套联邦学习框架，通过深度和宽度缩放将模型有效地划分为子模型，解决了在联邦学习中由于慢或能力有限的客户端导致的训练时间延长和性能下降的问题。

    

    联邦学习是一种有希望的分布式学习方法，可以保持隐私。然而，在联邦学习的训练过程中，慢或能力有限的客户端（即阻塞者）会减慢总体训练时间并降低性能。系统的异构性，包括异构计算和网络带宽，已经被用来减轻阻塞者的影响。以往的研究将模型分割来解决这个问题，但在模型架构方面的自由度较小。我们提出了嵌套联邦学习（NeFL），这是一个通用的框架，可以使用深度和宽度缩放将模型有效地分成子模型。NeFL通过将模型解释为解决常微分方程（ODE）并使用自适应步长来实现。为了解决训练具有不同架构的多个子模型时出现的不一致性问题，我们解耦了一些参数。NeFL使资源受限的客户端能够有效地加入联邦学习流程，并使模型能够被训练。

    Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be 
    
[^27]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^28]: 在汽车雷达目标检测网络中利用稀疏性

    Exploiting Sparsity in Automotive Radar Object Detection Networks. (arXiv:2308.07748v1 [cs.CV])

    [http://arxiv.org/abs/2308.07748](http://arxiv.org/abs/2308.07748)

    本文研究了在汽车雷达目标检测网络中利用稀疏性的方法，提出了稀疏核心点柱体和双体素点卷积来解决网格渲染和稀疏骨干结构问题。实验结果显示，这种方法在Car AP4.0上优于基准模型5.89%和先前的最优模型4.19%，并将平均尺度误差相对于基准模型减少了21.41%。

    

    精确感知环境对于确保自动驾驶系统的安全可靠性至关重要。雷达目标检测网络是这类系统的一个基本组成部分。基于CNN的目标检测器在这一背景下表现良好，但需要大量计算资源。本文研究了稀疏卷积目标检测网络，它将强大的基于网格的检测与低计算资源相结合。我们研究了雷达特定的挑战，并提出了稀疏核心点柱体（SKPP）和双体素点卷积（DVPC）来解决网格渲染和稀疏骨干结构问题。我们在nuScenes数据集上评估了SKPP-DPVCN架构，在Car AP4.0上优于基准模型5.89%，优于先前的最优模型4.19%。此外，SKPP-DPVCN还将平均尺度误差（ASE）相对于基准模型减少了21.41%。

    Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems. Radar object detection networks are one fundamental part of such systems. CNN-based object detectors showed good performance in this context, but they require large compute resources. This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources. We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures. We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.
    
[^29]: Real Robot Challenge 2022: 在真实世界中从离线数据中学习灵巧操作

    Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World. (arXiv:2308.07741v1 [cs.RO])

    [http://arxiv.org/abs/2308.07741](http://arxiv.org/abs/2308.07741)

    Real Robot Challenge 2022为RL和机器人学界之间的桥梁，允许参与者在真实机器人上从离线数据中学习灵巧操作任务，解决了在模拟中得到的见解不能转化到真实机器人的问题。

    

    在真实机器人上进行实验在时间和成本上要求很高。因此，增强学习（RL）社区的很大一部分使用模拟器来开发和评估算法。然而，从模拟中得到的见解不一定能够转化到真实机器人上，尤其是对于涉及复杂环境交互的任务。因此，Real Robot Challenge 2022作为RL和机器人学界之间的桥梁，让参与者能够像在模拟中一样轻松地远程实验真实机器人。在过去几年中，离线增强学习已经成熟为一种从预先收集的数据集中学习的有希望的范式，减轻了对昂贵在线交互的依赖.因此，我们要求参与者从提供的真实机器人数据集中学习两个灵巧操作任务，包括推动、抓取和手内定位。进行了广泛的软件文档化，并在基于仿真的初步阶段进行了实验。

    Experimentation on real robots is demanding in terms of time and costs. For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms. However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment. The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.  In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions. We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets. An extensive software documentation and an initial stage based on a simulation of the re
    
[^30]: 领域感知微调：增强神经网络的适应性能力

    Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])

    [http://arxiv.org/abs/2308.07728](http://arxiv.org/abs/2308.07728)

    本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。

    

    微调预训练神经网络模型已成为各个领域广泛采用的方法。然而，微调可能导致已具备强大泛化能力的预训练特征提取器发生畸变。在适应新目标领域时减轻特征畸变至关重要。最近的研究表明，在进行微调之前，在分布数据集上对头层进行对齐处理可以处理特征畸变问题取得有希望的结果。然而，在微调过程中，批归一化层的处理存在显著局限性，导致性能不佳。在本文中，我们提出了领域感知微调（DAFT），一种新的方法，它结合了批归一化转换、线性探测和微调的特性。我们的批归一化转换方法通过减少对神经网络的修改来有效减轻特征畸变。此外，我们还引入了线性探测和微调的集成方法。

    Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
    
[^31]: 通过选择性突触减弱实现快速的机器遗忘无需重新训练

    Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening. (arXiv:2308.07707v1 [cs.LG])

    [http://arxiv.org/abs/2308.07707](http://arxiv.org/abs/2308.07707)

    选择性突触减弱（SSD）是快速、性能优越且无需重新训练的机器遗忘方法，能够同时保护模型的性能并遗忘特定信息。

    

    机器遗忘，即机器学习模型的遗忘能力，正在越来越重要，以便符合数据隐私法规，并删除有害、篡改或过时的信息。关键挑战在于在保护模型在其余数据上的性能的同时遗忘特定信息。虽然目前的最先进方法表现良好，但通常需要在保留数据上进行某种程度的重新训练，以保护或恢复模型性能。这增加了计算开销，并要求训练数据保持可用和可访问，这可能并不可行。相反，其他方法采用无需重新训练的范式，但这些方法在计算上代价过高，并且性能不及重新训练的对应方法。我们提出选择性突触减弱（SSD），一种新颖的两步后验、无需重新训练的机器遗忘方法，快速、性能优越，并且不需要长时间的模型训练。

    Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require lon
    
[^32]: 利用视觉-语言模型在医学图像分割中探索迁移学习

    Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])

    [http://arxiv.org/abs/2308.07706](http://arxiv.org/abs/2308.07706)

    本论文提出使用视觉-语言模型进行医学图像分割的迁移学习，并评估了其在医学领域的可迁移性。通过捕捉语义信息和引入新的图像描述变化，实现了对多样化医学图像的分割。

    

    医学图像分割在医学领域的各种临床应用中至关重要。尽管最先进的分割模型已被证明有效，但在这个任务中整合文本指导以增强视觉特征仍然是一个进展有限的领域。现有利用文本指导的分割模型主要在开放领域图像上训练，这引发了在医学领域直接应用的难题，需要手动介入或进行微调。为了解决这些挑战，我们提出使用多模态的视觉-语言模型从图像描述和图像中捕捉语义信息，使得能够对多样化的医学图像进行分割。该研究全面评估了现有的视觉-语言模型在多个数据集上的可迁移性，以评估其从开放领域向医学领域的迁移能力。此外，我们对数据集中以前未见图像的图像描述引入了变化，揭示了显著的变异。

    Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
    
[^33]: 基于参数熵的k-means聚类中心初始化方法在不同图像数据集上的应用

    Parametric entropy based Cluster Centriod Initialization for k-means clustering of various Image datasets. (arXiv:2308.07705v1 [cs.IT])

    [http://arxiv.org/abs/2308.07705](http://arxiv.org/abs/2308.07705)

    本文提出了一种基于参数熵的k-means聚类中心初始化方法，在图像数据集上进行了分析，并得出了适用于不同数据集的最佳熵度量。

    

    k-means算法是一种被广泛应用且简单的聚类分析算法。它在人工智能、市场细分、欺诈检测、数据挖掘、心理学等领域取得了成功的应用。然而，k-means算法并不总是能产生最优质的结果。它的性能严重依赖于提供的聚类数和合适的聚类中心或种子的初始化方法。本文通过使用参数熵进行熵-based聚类中心初始化方法，并提出了适用于一般图像数据集的最佳匹配熵度量，对k-means在图像数据上的性能进行了分析。我们使用了多种熵度量，如Taneja熵、Kapur熵、Aczel Daroczy熵、Sharma Mittal熵。观察到对于不同的数据集，不同的熵度量比传统方法提供了更好的结果。我们在以下数据集上应用了我们提出的算法：Satellite...

    One of the most employed yet simple algorithm for cluster analysis is the k-means algorithm. k-means has successfully witnessed its use in artificial intelligence, market segmentation, fraud detection, data mining, psychology, etc., only to name a few. The k-means algorithm, however, does not always yield the best quality results. Its performance heavily depends upon the number of clusters supplied and the proper initialization of the cluster centroids or seeds. In this paper, we conduct an analysis of the performance of k-means on image data by employing parametric entropies in an entropy based centroid initialization method and propose the best fitting entropy measures for general image datasets. We use several entropies like Taneja entropy, Kapur entropy, Aczel Daroczy entropy, Sharma Mittal entropy. We observe that for different datasets, different entropies provide better results than the conventional methods. We have applied our proposed algorithm on these datasets: Satellite, To
    
[^34]: 使用大规模未标记的自然图像增强医疗AI模型的网络初始化

    Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])

    [http://arxiv.org/abs/2308.07688](http://arxiv.org/abs/2308.07688)

    该研究通过在非医学图像上利用自监督学习（SSL）预训练，取得了优于基于ImageNet的预训练的结果，从而实现了在医疗AI模型中增强网络初始化的目的。

    

    预训练数据集（如ImageNet）已成为医学图像分析的黄金标准。然而，自监督学习（SSL）的出现提供了通过利用未标记数据来学习强大特征的机会，从而可以绕过繁重的标注过程。在这项研究中，我们探索了SSL预训练在非医学图像上是否可以应用于胸部X射线，并与非医学图像和医学图像上的监督预训练进行了比较。我们利用视觉变换器，并根据以下方式初始化其权重：（i）基于自然图像的SSL预训练（DINOv2）、（ii）基于自然图像的监督预训练（ImageNet数据集），以及（iii）基于MIMIC-CXR数据库中的胸部X射线的监督预训练。我们在来自六个全球大型数据集的800,000多张胸部X射线上测试了我们的方法，诊断了20多种不同的影像所见。我们的SSL预训练在经过筛选的图像上不仅表现出色，而且超过了基于ImageNet的预训练（对所有数据集，P<0.001），而且在某些数据集上还超过了基于医学图像的预训练。

    Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in cert
    
[^35]: DiffGuard：使用预训练扩散模型进行语义不匹配引导的带外分布检测

    DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])

    [http://arxiv.org/abs/2308.07687](http://arxiv.org/abs/2308.07687)

    本论文提出了一种名为DiffGuard的方法，使用预训练的扩散模型进行语义不匹配引导的带外分布检测。实验证明，DiffGuard在小规模数据集上表现出色，但在ImageNet规模的数据集上无法应用。

    

    本论文针对语义带外（OOD）样本与合法类别内容在语义上的不匹配特征，提出了一种语义不匹配引导的带外分布检测方法DiffGuard。与其他方法相比，DiffGuard直接使用预训练的扩散模型进行语义不匹配引导，相较于条件生成对抗网络，扩散模型更易于训练且适用于各种条件。实验结果表明，在小规模数据集上，DiffGuard取得了显著的带外分布检测性能。然而，由于使用图像和标签作为条件训练条件生成对抗网络的困难性，DiffGuard在ImageNet规模的数据集上无法应用。

    Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and t
    
[^36]: 基于梯度的模型训练后量化：挑战现状

    Gradient-Based Post-Training Quantization: Challenging the Status Quo. (arXiv:2308.07662v1 [cs.LG])

    [http://arxiv.org/abs/2308.07662](http://arxiv.org/abs/2308.07662)

    本文挑战了基于梯度的模型训练后量化(GPTQ)方法中的常见选择，并且展示了该方法在一定程度上对多个变量具有鲁棒性。

    

    量化对于高效部署深度神经网络已经变得至关重要，其中浮点运算被转化为简化的定点运算。在最简单的形式中，它仅仅是由缩放和舍入转换组成，导致压缩率有限或者准确性显著下降。最近，基于梯度的模型训练后量化(GPTQ)方法在简单方法和更强大但昂贵的量化感知训练 (QAT)方法之间提供了一个合适的折衷，特别是在尝试量化大型语言模型(LLM)时，量化过程的可扩展性非常重要。GPTQ主要是通过使用一个小的校准集合来学习舍入操作。在这项工作中，我们挑战了GPTQ方法中常见的选择。特别是，我们展示了这个过程在一定程度上对多个变量(权重选择、特征增强、校准选择)具有鲁棒性。

    Quantization has become a crucial step for the efficient deployment of deep neural networks, where floating point operations are converted to simpler fixed point operations. In its most naive form, it simply consists in a combination of scaling and rounding transformations, leading to either a limited compression rate or a significant accuracy drop. Recently, Gradient-based post-training quantization (GPTQ) methods appears to be constitute a suitable trade-off between such simple methods and more powerful, yet expensive Quantization-Aware Training (QAT) approaches, particularly when attempting to quantize LLMs, where scalability of the quantization process is of paramount importance. GPTQ essentially consists in learning the rounding operation using a small calibration set. In this work, we challenge common choices in GPTQ methods. In particular, we show that the process is, to a certain extent, robust to a number of variables (weight selection, feature augmentation, choice of calibrat
    
[^37]: 注意力不再是唯一需要的东西了。

    Attention Is Not All You Need Anymore. (arXiv:2308.07661v1 [cs.LG])

    [http://arxiv.org/abs/2308.07661](http://arxiv.org/abs/2308.07661)

    本文提出了一种名为Extractor的插入替代器，用于取代Transformer中的自注意机制，实验证明使用Extractor可以提高Transformer的性能，并且具有更短的计算关键路径。

    

    在最近几年中，流行的Transformer架构在自然语言处理和计算机视觉等许多应用领域取得了巨大成功。许多现有的工作旨在通过性能平衡来减少Transformer中自注意机制的计算和存储复杂度。然而，性能对于Transformer的持续成功至关重要。本文提出了一种用于取代Transformer中自注意机制的插入替代器（Extractor）。实验结果表明，使用Extractor替换自注意机制可以提高Transformer的性能。此外，Extractor具有更短的计算关键路径，因此有潜力比自注意更快。此外，本文还使用可变长离散时间马尔可夫链对文本生成中的序列预测问题进行了建模，并针对我们的插入替代器对Transformer进行了评估。

    In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our unders
    
[^38]: 从提交信息生成到基于历史的提交信息补全

    From Commit Message Generation to History-Aware Commit Message Completion. (arXiv:2308.07655v1 [cs.SE])

    [http://arxiv.org/abs/2308.07655](http://arxiv.org/abs/2308.07655)

    本文提出并评估了从提交信息生成转向基于历史的提交信息补全的新思路，使用先前的提交历史作为额外上下文可以显著提高生成的提交信息的质量和个性化程度。

    

    提交信息对软件开发至关重要，可以帮助开发人员跟踪改动并有效地协作。尽管有用，但大多数提交信息缺乏重要信息，因为编写高质量的提交信息是繁琐且耗时的。目前关于提交信息生成的研究尚未在实践中得到广泛应用。本文提出并评估了从提交信息生成转向提交信息补全，并使用先前的提交历史作为额外上下文的新思路，可以显著提高生成的提交信息的质量和个性化程度。我们收集并共享了一个名为CommitChronicle的新数据集，其中包含20种编程语言的1070万个提交。我们使用这个数据集来评估补全设置和历史上下文对于最先进的提交信息生成模型和GPT-3的实用性。

    Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could significantly improve the quality and the personal nature of the resulting commit messages.  In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT-3
    
[^39]: 三元奇异值分解作为线性映射中更好的参数化形式

    Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping. (arXiv:2308.07641v1 [cs.LG])

    [http://arxiv.org/abs/2308.07641](http://arxiv.org/abs/2308.07641)

    本文提出了一种名为三元奇异值分解 (TSVD) 的新颖线性映射参数化形式，通过限制奇异值分解中的矩阵为三元矩阵形式，它在网络压缩性能方面表现出色。实验证明，TSVD在各种类型的网络和任务中都能实现最先进的网络压缩性能。

    

    我们提出了一种简单但新颖的线性映射参数化形式，称为三元奇异值分解 (TSVD)，以实现卓越的网络压缩性能。与传统的奇异值分解不同，TSVD将奇异值分解中的$U$和$V$矩阵限制为三元矩阵形式，即$\{ \pm 1, 0 \}$。这意味着在计算$U(\cdot)$和$V(\cdot)$时，TSVD只需要加法指令，而不需要昂贵的乘法指令。我们提供了直接转换算法和训练过渡算法，如后训练量化和量化感知训练。此外，我们在理论上分析了直接转换算法的收敛性。在实验中，我们证明了TSVD在各种类型的网络和任务中都能实现最先进的网络压缩性能，包括当前的基准模型，如ConvNext、Swim、BERT以及类似OPT的大型语言模型。

    We present a simple yet novel parameterized form of linear mapping to achieves remarkable network compression performance: a pseudo SVD called Ternary SVD (TSVD).  Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive multiplication instructions, TSVD only requires addition instructions when computing $U(\cdot)$ and $V(\cdot)$.  We provide direct and training transition algorithms for TSVD like Post Training Quantization and Quantization Aware Training respectively. Additionally, we analyze the convergence of the direct transition algorithms in theory.  In experiments, we demonstrate that TSVD can achieve state-of-the-art network compression performance in various types of networks and tasks, including current baseline models such as ConvNext, Swim, BERT, and large language model like OPT.
    
[^40]: 运用反向传播路径搜索增强对抗迁移性

    Backpropagation Path Search On Adversarial Transferability. (arXiv:2308.07625v1 [cs.CV])

    [http://arxiv.org/abs/2308.07625](http://arxiv.org/abs/2308.07625)

    本研究提出了一种backPropagation pAth Search (PAS)方法来增强对抗迁移性，通过调整卷积模块的反向传播路径和构建基于有向无环图的搜索空间来解决了现有基于结构攻击者的局限性。

    

    深度神经网络容易受到对抗性样本的攻击，因此在部署之前测试模型的鲁棒性变得至关重要。基于迁移的攻击者会针对替代模型制作对抗样本，并将其转移到黑盒情况下部署的受害者模型上。为了增强对抗迁移性，基于结构的攻击者调整反向传播路径以避免攻击过度拟合替代模型。然而，现有的基于结构的攻击者未能探索卷积模块，并且启发式地修改反向传播图，导致效果有限。在本文中，我们提出了backPropagation pAth Search (PAS)，解决了上述两个问题。我们首先通过结构重参数化提出了SkipConv来调整卷积的反向传播路径。为了克服启发式设计的反向传播路径的缺点，我们进一步构建了基于有向无环图的搜索空间，并利用一步近似来实现路径的搜索。

    Deep neural networks are vulnerable to adversarial examples, dictating the imperativeness to test the model's robustness before deployment. Transfer-based attackers craft adversarial examples against surrogate models and transfer them to victim models deployed in the black-box situation. To enhance the adversarial transferability, structure-based attackers adjust the backpropagation path to avoid the attack from overfitting the surrogate model. However, existing structure-based attackers fail to explore the convolution module in CNNs and modify the backpropagation graph heuristically, leading to limited effectiveness. In this paper, we propose backPropagation pAth Search (PAS), solving the aforementioned two problems. We first propose SkipConv to adjust the backpropagation path of convolution by structural reparameterization. To overcome the drawback of heuristically designed backpropagation paths, we further construct a DAG-based search space, utilize one-step approximation for path e
    
[^41]: 基于多层感知器的快速阳光评估法用于中国住宅小区的概念设计

    A Multilayer Perceptron-based Fast Sunlight Assessment for the Conceptual Design of Residential Neighborhoods under Chinese Policy. (arXiv:2308.07616v1 [cs.LG])

    [http://arxiv.org/abs/2308.07616](http://arxiv.org/abs/2308.07616)

    本研究提出了一种基于多层感知器的方法，用于快速评估中国住宅小区的阳光条件。该方法通过一阶段预测，输出输入立方体形状建筑导致的遮挡时间间隔，从而得到建筑的阳光小时数。这种方法比传统软件更快速，可以在概念设计阶段使用，有助于优化建筑阳光性能。

    

    在中国的建筑规范中，要求住宅建筑在指定的冬季日子上接收到最低小时数的自然直射阳光，这代表了一年中最差的阳光条件。这一要求是在住宅项目的概念设计阶段获取建筑许可的先决条件。因此，通常使用官方认可的软件来评估建筑的阳光性能。这些软件根据重复的遮挡计算来预测阳光小时数，这是一项耗时的工作。本文提出了一种基于多层感知器的方法，即一阶段预测方法，该方法输出由输入的长方体形式建筑所引起的遮挡时间间隔。通过计算所有建筑的阳光时间间隔的并集（遮挡时间间隔的补集），可以得到一个地块的阳光小时数。进行了三个数值实验，即水平水平和坡度分析以及基于模拟的优化。

    In Chinese building codes, it is required that residential buildings receive a minimum number of hours of natural, direct sunlight on a specified winter day, which represents the worst sunlight condition in a year. This requirement is a prerequisite for obtaining a building permit during the conceptual design of a residential project. Thus, officially sanctioned software is usually used to assess the sunlight performance of buildings. These software programs predict sunlight hours based on repeated shading calculations, which is time-consuming. This paper proposed a multilayer perceptron-based method, a one-stage prediction approach, which outputs a shading time interval caused by the inputted cuboid-form building. The sunlight hours of a site can be obtained by calculating the union of the sunlight time intervals (complement of shading time interval) of all the buildings. Three numerical experiments, i.e., horizontal level and slope analysis, and simulation-based optimization are carr
    
[^42]: 使用机器学习进行异常检测，在系外行星大气中寻找新的化学物质

    Searching for Novel Chemistry in Exoplanetary Atmospheres using Machine Learning for Anomaly Detection. (arXiv:2308.07604v1 [astro-ph.EP])

    [http://arxiv.org/abs/2308.07604](http://arxiv.org/abs/2308.07604)

    本文研究使用机器学习方法进行异常检测，在系外行星中寻找新的化学物质。通过在合成光谱上进行实验证明了两种常见的异常检测方法的可行性，并用ROC曲线比较了它们的性能。

    

    下一代望远镜将提供大量高分辨率光谱数据，用于分析数千个系外行星。庞大的数据量和需要分析的行星数量极大地推动了开发新的、快速高效的方法，用于标记有趣的行星以供重新观测和详细分析。我们提倡将机器学习技术应用于系外行星凌星光谱的异常（新颖性）检测，目标是识别具有异常化学成分甚至寻找未知的生物标记。我们成功地展示了两种常见的异常检测方法（局部离群因子和一类支持向量机）在大型公共数据库的合成光谱上的可行性。我们考虑了几种测试情况，每种情况都有不同水平的仪器噪声。在每种情况下，我们使用ROC曲线来量化和比较两种机器学习技术的性能。

    The next generation of telescopes will yield a substantial increase in the availability of high-resolution spectroscopic data for thousands of exoplanets. The sheer volume of data and number of planets to be analyzed greatly motivate the development of new, fast and efficient methods for flagging interesting planets for reobservation and detailed analysis. We advocate the application of machine learning (ML) techniques for anomaly (novelty) detection to exoplanet transit spectra, with the goal of identifying planets with unusual chemical composition and even searching for unknown biosignatures. We successfully demonstrate the feasibility of two popular anomaly detection methods (Local Outlier Factor and One Class Support Vector Machine) on a large public database of synthetic spectra. We consider several test cases, each with different levels of instrumental noise. In each case, we use ROC curves to quantify and compare the performance of the two ML techniques.
    
[^43]: 使用多模态对抗模仿学习生成游戏角色

    Generating Personas for Games with Multimodal Adversarial Imitation Learning. (arXiv:2308.07598v1 [cs.LG])

    [http://arxiv.org/abs/2308.07598](http://arxiv.org/abs/2308.07598)

    本论文介绍了一种使用多模态对抗模仿学习的方法，可以生成多个游戏角色策略，以用于游戏测试。这种方法通过使用多个判别器作为奖励模型，并根据辅助输入对每个判别器的奖励进行加权，有效地建模各种人类游戏风格。

    

    强化学习在游戏中取得了广泛的成功，能够生成具有人类水平的玩家智能体。然而，这需要复杂的奖励工程，并且智能体的策略往往是不可预测的。为了建模各种人类游戏风格，超越强化学习是必要的，而这往往很难用奖励函数表示。本文提出了一种新颖的模仿学习方法，用于生成多个角色策略用于游戏测试。多模态生成对抗模仿学习（MultiGAIL）使用辅助输入参数，使用单智能体模型学习不同的角色。MultiGAIL基于生成对抗模仿学习，并使用多个判别器作为奖励模型，通过比较智能体和不同的专家策略来推断环境奖励。每个判别器的奖励根据辅助输入进行加权。我们的实验分析证明了我们的技术的有效性。

    Reinforcement learning has been widely successful in producing agents capable of playing games at a human level. However, this requires complex reward engineering, and the agent's resulting policy is often unpredictable. Going beyond reinforcement learning is necessary to model a wide range of human playstyles, which can be difficult to represent with a reward function. This paper presents a novel imitation learning approach to generate multiple persona policies for playtesting. Multimodal Generative Adversarial Imitation Learning (MultiGAIL) uses an auxiliary input parameter to learn distinct personas using a single-agent model. MultiGAIL is based on generative adversarial imitation learning and uses multiple discriminators as reward models, inferring the environment reward by comparing the agent and distinct expert policies. The reward from each discriminator is weighted according to the auxiliary input. Our experimental analysis demonstrates the effectiveness of our technique in two
    
[^44]: 高概率风险上界通过顺序预测器实现

    High-Probability Risk Bounds via Sequential Predictors. (arXiv:2308.07588v1 [cs.LG])

    [http://arxiv.org/abs/2308.07588](http://arxiv.org/abs/2308.07588)

    通过在线到批次转换和对损失函数的二阶校正，我们展示了一种方法可以在统计学习中获得几乎最优的高概率风险上界。

    

    在最小假设下，在线学习方法产生顺序遗憾上界，并为统计学习提供期望风险上界。然而，尽管在线保证相对于统计保证明显有优势，但最近的研究结果表明，在许多重要情况下，遗憾上界可能无法保证统计设置中的紧密高概率风险上界。在这项工作中，我们展示了应用于一般在线学习算法的在线到批次转换可以绕过此限制。通过对定义遗憾的损失函数进行一般的二阶校正，我们获得了几种经典统计估计问题的几乎最优高概率风险上界，例如离散分布估计，线性回归，逻辑回归和条件密度估计。我们的分析依赖于许多在线学习算法是不适当的事实，因为它们不受限于使用给定参考类别的预测器。

    Online learning methods yield sequential regret bounds under minimal assumptions and provide in-expectation risk bounds for statistical learning. However, despite the apparent advantage of online guarantees over their statistical counterparts, recent findings indicate that in many important cases, regret bounds may not guarantee tight high-probability risk bounds in the statistical setting. In this work we show that online to batch conversions applied to general online learning algorithms can bypass this limitation. Via a general second-order correction to the loss function defining the regret, we obtain nearly optimal high-probability risk bounds for several classical statistical estimation problems, such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation. Our analysis relies on the fact that many online learning algorithms are improper, as they are not restricted to use predictors from a given reference class. The improper 
    
[^45]: 通过在线文本增强和上下文记忆的方式进行故事可视化

    Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])

    [http://arxiv.org/abs/2308.07575](http://arxiv.org/abs/2308.07575)

    本论文提出了一种通过在线文本增强和上下文记忆来进行故事可视化的方法。通过使用新颖的记忆架构和多个伪描述作为训练过程的补充监督，该方法在两个故事可视化基准测试中取得了显著优于现有方法的结果。

    

    故事可视化是一个具有挑战性的文本到图像生成任务，难点在于不仅需要从文本描述中呈现视觉细节，还需要对跨多个句子的长期上下文进行编码。以往的工作主要关注为每个句子生成语义相关的图像，但在给定段落中编码上下文以生成具有上下文说服力的图像（例如，正确的角色或适当的场景背景）仍然是一个挑战。为此，我们提出了一种新颖的记忆架构，用于双向Transformer，并通过在线文本增强生成多个伪描述作为训练过程中的补充监督，以更好地适应推理中的语言变化。在两个流行的故事可视化基准测试中进行了大量实验证明，即Pororo-SV和Flintstones-SV，所提出的方法在包括FID、字符...

    Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, char
    
[^46]: Ske2Grid：骨架到网格表示学习的动作识别

    Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition. (arXiv:2308.07571v1 [cs.CV])

    [http://arxiv.org/abs/2308.07571](http://arxiv.org/abs/2308.07571)

    Ske2Grid是一种用于改进基于骨架的动作识别的新型表示学习框架，通过定义规则卷积操作和新颖设计的网格表示，实现了更好的识别效果。

    

    本文提出了Ske2Grid，一种用于改进基于骨架的动作识别的新型表示学习框架。在Ske2Grid中，我们定义了一个规则的卷积操作，用于人体骨架的一种新颖的网格表示，它是通过三个新颖设计构建和学习的紧凑的类似图像的网格图像块。具体来说，我们提出了一个图节点索引变换（GIT），通过一一将骨架图中的节点分配到所需的网格单元，构建一个规则的网格图像块。为了确保GIT是一个双射，并丰富网格表示的表达能力，我们学习了一个上采样变换（UPT），以插值填充网格图像块的骨架图节点，使其达到完整。为了解决当单步UPT过于激进时的问题，并进一步发掘随着空间尺寸增加的网格图像块的表示能力，我们提出了一种渐进学习策略（PLS），将UPT分解为多个步骤并将其与多个配对对齐。

    This paper presents Ske2Grid, a new representation learning framework for improved skeleton-based action recognition. In Ske2Grid, we define a regular convolution operation upon a novel grid representation of human skeleton, which is a compact image-like grid patch constructed and learned through three novel designs. Specifically, we propose a graph-node index transform (GIT) to construct a regular grid patch through assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, an up-sampling transform (UPT) is learned to interpolate the skeleton graph nodes for filling the grid patch to the full. To resolve the problem when the one-step UPT is aggressive and further exploit the representation capability of the grid patch with increasing spatial size, a progressive learning strategy (PLS) is proposed which decouples the UPT into multiple steps and aligns them to multiple paired
    
[^47]: 非随机缺失标签的多重插补半监督学习

    Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels. (arXiv:2308.07562v1 [cs.LG])

    [http://arxiv.org/abs/2308.07562](http://arxiv.org/abs/2308.07562)

    这篇论文提出了两种新的多重插补模型组合方法，实现了半监督学习中非随机缺失标签问题的更高准确性和更小偏差。

    

    半监督学习是在训练算法时同时使用有标签和无标签数据的技术。这是机器学习的一个常见应用，因为获得完全标记的数据集是不现实的。研究人员已经解决了三个主要问题：随机缺失、完全随机缺失和非随机缺失。非随机缺失问题是三者中最具挑战性的，因为不能安全地假设所有类别分布相等。现有的方法，包括类感知插补和类似概率，大多忽视了无标签数据中的非随机性。本文提出了两种新的多重插补模型组合方法，以实现更高的准确性和更小的偏差。1）我们使用多重插补模型，创建置信区间，并应用阈值来忽略置信度较低的伪标签。2）我们的新方法，去偏插补半监督学习（SSL-DI），旨在通过过滤不准确的数据并找到一种子数据集，来减小偏差。

    Semi-Supervised Learning (SSL) is implemented when algorithms are trained on both labeled and unlabeled data. This is a very common application of ML as it is unrealistic to obtain a fully labeled dataset. Researchers have tackled three main issues: missing at random (MAR), missing completely at random (MCAR), and missing not at random (MNAR). The MNAR problem is the most challenging of the three as one cannot safely assume that all class distributions are equal. Existing methods, including Class-Aware Imputation (CAI) and Class-Aware Propensity (CAP), mostly overlook the non-randomness in the unlabeled data. This paper proposes two new methods of combining multiple imputation models to achieve higher accuracy and less bias. 1) We use multiple imputation models, create confidence intervals, and apply a threshold to ignore pseudo-labels with low confidence. 2) Our new method, SSL with De-biased Imputations (SSL-DI), aims to reduce bias by filtering out inaccurate data and finding a subs
    
[^48]: 西班牙文文本简化的用户中心评估

    A User-Centered Evaluation of Spanish Text Simplification. (arXiv:2308.07556v1 [cs.CL])

    [http://arxiv.org/abs/2308.07556](http://arxiv.org/abs/2308.07556)

    通过对西班牙文文本简化的用户评估，我们发现神经网络在预测用户偏好方面表现更好，同时也发现多语言模型不如西班牙文模型在同一任务上的表现。我们发布了评估中的语料库，希望推动西班牙语自然语言处理的最新技术。

    

    我们通过两个语料库，一个针对复杂句子和一个针对复杂词语的识别，对西班牙文文本简化进行了一个生产系统的评估。我们将最流行的西班牙特定可读性评分与神经网络进行了比较，并表明后者在预测用户对文本简化的偏好方面始终更好。作为我们分析的一部分，我们发现多语言模型在相同任务上表现不如仅限于西班牙文的模型，但所有模型过于频繁地关注统计特征，如句子长度。我们将我们评估中的语料库发布给更广泛的社区，希望推动西班牙语自然语言处理的最新技术。

    We present an evaluation of text simplification (TS) in Spanish for a production system, by means of two corpora focused in both complex-sentence and complex-word identification. We compare the most prevalent Spanish-specific readability scores with neural networks, and show that the latter are consistently better at predicting user preferences regarding TS. As part of our analysis, we find that multilingual models underperform against equivalent Spanish-only models on the same task, yet all models focus too often on spurious statistical features, such as sentence length. We release the corpora in our evaluation to the broader community with the hopes of pushing forward the state-of-the-art in Spanish natural language processing.
    
[^49]: 加强解药：针对中毒攻击的改进点对点认证

    Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks. (arXiv:2308.07553v1 [cs.LG])

    [http://arxiv.org/abs/2308.07553](http://arxiv.org/abs/2308.07553)

    本论文提出了一种改进的点对点认证方法，通过利用差分隐私和采样高斯机制，能够确保对有限数量中毒样本的预测具有不变性，提供更大的对抗鲁棒性保证。

    

    中毒攻击可以通过对训练语料进行微小改动来对模型行为产生不成比例的影响。虽然存在对特定中毒攻击的防御方法，但它们通常不提供任何保证，可能被新型攻击所对抗。相比之下，通过考察最坏情况下的行为，认证防御可以提供针对有限数量训练样本被敌对攻击修改的样本的鲁棒性保证，称为点对点认证。我们通过利用差分隐私和采样高斯机制，确保每个测试实例对有限数量中毒样本的预测具有不变性，从而实现了这一点。通过这样做，我们的模型提供的对抗鲁棒性保证比之前的认证方法提供的保证更大两倍以上。

    Poisoning attacks can disproportionately influence model behaviour by making small changes to the training corpus. While defences against specific poisoning attacks do exist, they in general do not provide any guarantees, leaving them potentially countered by novel attacks. In contrast, by examining worst-case behaviours Certified Defences make it possible to provide guarantees of the robustness of a sample against adversarial attacks modifying a finite number of training samples, known as pointwise certification. We achieve this by exploiting both Differential Privacy and the Sampled Gaussian Mechanism to ensure the invariance of prediction for each testing instance against finite numbers of poisoned examples. In doing so, our model provides guarantees of adversarial robustness that are more than twice as large as those provided by prior certifications.
    
[^50]: 通过极小极大熵进行领域适应，用于天文警报的真伪分类

    Domain Adaptation via Minimax Entropy for Real/Bogus Classification of Astronomical Alerts. (arXiv:2308.07538v1 [astro-ph.IM])

    [http://arxiv.org/abs/2308.07538](http://arxiv.org/abs/2308.07538)

    本文研究了用于天文警报真伪分类的领域适应方法，通过使用微调方法和半监督深度领域适应（MME）来提高分类模型的性能。实验证明，微调模型和MME模型都能在只有少数标记数据的情况下显著改进基本模型。

    

    时间域天文学正朝着实时分析多个大规模数据集的方向发展，促使多流机器学习模型的发展。本文使用四个不同的数据集（HiTS、DES、ATLAS和ZTF）研究了天文警报的真伪分类的领域适应（DA）。我们研究了这些数据集之间的领域变化，并通过使用微调方法和半监督深度领域适应（MME）改进了一个简单的深度学习分类模型。我们比较了这些模型在不同源目标情景下的平衡准确性。我们发现，微调模型和MME模型都能显著改进基本模型，即使从目标数据集中只有一个标记的项，而且MME模型不会损害源数据集上的性能。

    Time domain astronomy is advancing towards the analysis of multiple massive datasets in real time, prompting the development of multi-stream machine learning models. In this work, we study Domain Adaptation (DA) for real/bogus classification of astronomical alerts using four different datasets: HiTS, DES, ATLAS, and ZTF. We study the domain shift between these datasets, and improve a naive deep learning classification model by using a fine tuning approach and semi-supervised deep DA via Minimax Entropy (MME). We compare the balanced accuracy of these models for different source-target scenarios. We find that both the fine tuning and MME models improve significantly the base model with as few as one labeled item per class coming from the target dataset, but that the MME does not compromise its performance on the source dataset.
    
[^51]: FeatGeNN: 使用基于相关性的特征提取方法改进表格数据模型性能

    FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction. (arXiv:2308.07527v1 [cs.LG])

    [http://arxiv.org/abs/2308.07527](http://arxiv.org/abs/2308.07527)

    FeatGeNN是一种使用基于相关性的汇集方法提取和创建新特征的卷积方法，它在多个基准数据集上的评估中表现优于现有的自动特征工程方法。

    

    自动特征工程（AutoFE）已成为任何机器学习项目中重要的任务，因为它可以帮助提高模型性能并为统计分析提供更多信息。然而，目前大多数自动特征工程方法要么依靠手动特征创建，要么使用可能生成大量特征的方法，这会耗费大量计算资源并导致过拟合。为了解决这些挑战，我们提出了一种新颖的卷积方法，称为FeatGeNN，它使用相关性作为汇集函数来提取和创建新特征。与传统的最大汇集（max-pooling）等汇集函数不同，基于相关性的汇集考虑了数据矩阵中特征之间的线性关系，使之更适用于表格数据。我们在多个基准数据集上评估了我们的方法，并证明了FeatGeNN在模型性能方面优于现有的自动特征工程方法。我们的结果表明，基于相关性的汇集可以成为最大汇集的一种有前途的替代方法。

    Automated Feature Engineering (AutoFE) has become an important task for any machine learning project, as it can help improve model performance and gain more information for statistical analysis. However, most current approaches for AutoFE rely on manual feature creation or use methods that can generate a large number of features, which can be computationally intensive and lead to overfitting. To address these challenges, we propose a novel convolutional method called FeatGeNN that extracts and creates new features using correlation as a pooling function. Unlike traditional pooling functions like max-pooling, correlation-based pooling considers the linear relationship between the features in the data matrix, making it more suitable for tabular data. We evaluate our method on various benchmark datasets and demonstrate that FeatGeNN outperforms existing AutoFE approaches regarding model performance. Our results suggest that correlation-based pooling can be a promising alternative to max-p
    
[^52]: 深层操作符网络在核系统数字孪生技术中的潜力

    Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System. (arXiv:2308.07523v1 [stat.ML])

    [http://arxiv.org/abs/2308.07523](http://arxiv.org/abs/2308.07523)

    深层操作符网络（DeepONet）作为一种强大的替代建模方法，在核系统数字孪生技术中展示出了显著的预测精度和计算效率。然而，挑战仍然存在，包括最佳传感器放置和模型评估。

    

    本研究在核工程的数字孪生系统中引入了深层操作符网络（DeepONet）作为一种强大的替代建模方法。随着核能作为一种碳中和解决方案的重要性不断增加，采用数字孪生技术对于提高核工程应用中的运营效率、安全性和预测能力变得至关重要。DeepONet具有显著的预测精度，优于传统的机器学习方法。通过广泛的基准测试和评估，本研究展示了DeepONet在解决复杂粒子传输问题中的可扩展性和计算效率。通过将函数作为输入数据并使用训练数据构建操作符G，DeepONet能够有效处理多样化和复杂的场景。然而，DeepONet的应用也揭示了与最佳传感器放置和模型评估相关的挑战，这是实际实施中的关键问题。

    This research introduces the Deep Operator Network (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) systems for nuclear engineering. With the increasing importance of nuclear energy as a carbon-neutral solution, adopting DT technology has become crucial to enhancing operational efficiencies, safety, and predictive capabilities in nuclear engineering applications. DeepONet exhibits remarkable prediction accuracy, outperforming traditional ML methods. Through extensive benchmarking and evaluation, this study showcases the scalability and computational efficiency of DeepONet in solving a challenging particle transport problem. By taking functions as input data and constructing the operator $G$ from training data, DeepONet can handle diverse and complex scenarios effectively. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing 
    
[^53]: 非线性、反馈和因果结构学习中的一致性问题研究

    Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning. (arXiv:2308.07520v1 [stat.ML])

    [http://arxiv.org/abs/2308.07520](http://arxiv.org/abs/2308.07520)

    这篇论文研究了非线性、反馈和因果结构学习中的一致性问题，并提出了一个弱于强可靠性的k-Triangle Faithfulness的替代定义。

    

    因果发现的目标是从观测数据中找到学习因果结构的自动化搜索方法。有些情况下，感兴趣的因果机制的所有变量都已经被测量，任务是预测一个变量对另一个变量的影响。相反，有时主要关注的变量并非直接可观察，而是通过它们在数据中的表现来推理出来的。这些被称为潜在变量。一个广泛被知道的例子是心理构造的智商，因为无法直接测量，所以研究人员尝试通过各种指标如智商测试来评估。在这种情况下，因果发现算法可以揭示潜在变量之间和潜在变量与观察变量之间的因果连接，从而发现潜在的模式和结构。这篇论文主要研究因果发现中的两个问题：提供了一个弱于强可靠性的k-Triangle Faithfulness的替代定义，并提出了对统计一致性的新要求。

    The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfu
    
[^54]: 将资源管理算法的知识融入神经网络：一种统一的训练辅助方法

    Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach. (arXiv:2308.07511v1 [cs.LG])

    [http://arxiv.org/abs/2308.07511](http://arxiv.org/abs/2308.07511)

    本文提出了一种将传统基于模型的信干噪比优化方法应用于神经网络训练的知识蒸馏方法，以提高神经网络的性能和收敛速度。

    

    作为一个基本问题，在多用户环境中，有很多方法致力于优化信干噪比（SINR）。尽管传统的基于模型的优化方法取得了很好的性能，但高复杂性催生了基于神经网络（NN）的方法来权衡性能和复杂性。为了充分利用传统基于模型的方法的高性能和NN方法的低复杂性，本文提出了一种基于知识蒸馏（KD）的算法蒸馏（AD）方法，以提高NN方法的性能和收敛速度。在这个方法中，采用传统的SINR优化方法作为“老师”来辅助训练NNs，即“学生”，从而增强无监督和强化学习技术的性能。该方法旨在缓解每种训练范例中遇到的常见问题，包括无法获得准确解的问题。

    As a fundamental problem, numerous methods are dedicated to the optimization of signal-to-interference-plus-noise ratio (SINR), in a multi-user setting. Although traditional model-based optimization methods achieve strong performance, the high complexity raises the research of neural network (NN) based approaches to trade-off the performance and complexity. To fully leverage the high performance of traditional model-based methods and the low complexity of the NN-based method, a knowledge distillation (KD) based algorithm distillation (AD) method is proposed in this paper to improve the performance and convergence speed of the NN-based method, where traditional SINR optimization methods are employed as ``teachers" to assist the training of NNs, which are ``students", thus enhancing the performance of unsupervised and reinforcement learning techniques. This approach aims to alleviate common issues encountered in each of these training paradigms, including the infeasibility of obtaining o
    
[^55]: 使用大型语言模型进行数据竞争检测

    Data Race Detection Using Large Language Models. (arXiv:2308.07505v1 [cs.LG])

    [http://arxiv.org/abs/2308.07505](http://arxiv.org/abs/2308.07505)

    本研究探索了一种基于大型语言模型的数据竞争检测方法，通过结合提示工程和微调技术，发现LLMs可以作为数据竞争检测的可行方法，但在需要详细信息时仍不如传统工具竞争。

    

    大型语言模型（LLMs）作为一种替代策略，展示了在分析和优化高性能计算程序方面的显著优势，避免了资源密集型手动工具的创建。本文中，我们探讨了一种新颖的基于LLM的数据竞争检测方法，结合了提示工程和微调技术。我们创建了一个名为DRB-ML的专用数据集，该数据集源自DataRaceBench，并具有精细的标签，显示了数据竞争对及其相关变量、行号和读/写信息的存在。然后，我们使用DRB-ML评估了代表性的LLMs并微调了开源模型。我们的实验证明，LLMs可以作为数据竞争检测的可行方法。然而，当我们需要有关引起数据竞争的变量对的详细信息时，它们仍无法与传统的数据竞争检测工具竞争。

    Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
    
[^56]: ST-MLP：一种基于分组独立策略的级联时空线性框架用于交通预测

    ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting. (arXiv:2308.07496v1 [cs.LG])

    [http://arxiv.org/abs/2308.07496](http://arxiv.org/abs/2308.07496)

    ST-MLP是一种基于级联多层感知器（MLP）模块和线性层的时空模型，通过成功实现分组独立策略的时间序列预测技术，将时间信息、空间信息和预定义的图结构结合起来。实验结果表明ST-MLP在精度和计算效率方面优于其他模型。

    

    在智能交通系统中，准确预测交通流量对于优化交通流管理至关重要。时空图神经网络（STGNNs）因其适应道路图结构的能力而备受赞誉。然而，当前关于STGNNs架构的研究常常优先考虑复杂的设计，导致计算负担加重，仅在精度上有少许提升。为解决这个问题，我们提出了ST-MLP，这是一个简洁的基于级联多层感知器（MLP）模块和线性层的时空模型。具体而言，我们通过成功实现分组独立策略的时间序列预测技术，将时间信息、空间信息和预定义的图结构结合起来。实证结果表明，ST-MLP在精度和计算效率方面优于最先进的STGNNs和其他模型。我们的发现鼓励了有关交通预测的进一步研究和应用。

    The criticality of prompt and precise traffic forecasting in optimizing traffic flow management in Intelligent Transportation Systems (ITS) has drawn substantial scholarly focus. Spatio-Temporal Graph Neural Networks (STGNNs) have been lauded for their adaptability to road graph structures. Yet, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model solely based on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encou
    
[^57]: 在不同环境中对单刚体角色的自适应跟踪

    Adaptive Tracking of a Single-Rigid-Body Character in Various Environments. (arXiv:2308.07491v1 [cs.RO])

    [http://arxiv.org/abs/2308.07491](http://arxiv.org/abs/2308.07491)

    本研究提出了一种基于单刚体角色仿真的深度强化学习方法，通过训练一个能够自适应各种环境变化的策略，实现在不需要额外学习的情况下完成各种任务。

    

    自从DeepMimic的引入以来，后续研究一直致力于在不同情景下扩展模拟动作的范畴。在本研究中，我们提出了一个替代方法，一种基于单刚体角色仿真的深度强化学习方法。利用质心动力学模型（CDM）将全身角色表示为单刚体（SRB），并训练一个跟踪参考动作的策略，我们可以得到一个能够适应各种未观测环境变化和控制器转换的策略，而不需要额外的学习。由于状态和动作空间的降维，学习过程具有高样本效率。最终的全身动作以物理合理的方式基于模拟SRB角色的状态进行运动生成。SRB仿真被制定为一个二次规划问题，策略输出一个动作，允许角色在不同环境中完成任务。

    Since the introduction of DeepMimic [Peng et al. 2018], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows th
    
[^58]: O-1: 自动标注与1-best假设的自我训练

    O-1: Self-training with Oracle and 1-best Hypothesis. (arXiv:2308.07486v1 [cs.LG])

    [http://arxiv.org/abs/2308.07486](http://arxiv.org/abs/2308.07486)

    提出了一种新的自我训练目标O-1，通过增强神谕假设来减少训练偏差，并统一训练和评估指标。通过实验证明了O-1在语音识别任务中的有效性，并取得了相对于EMBR的显著改进。

    

    我们引入了O-1，这是一个新的自我训练目标，旨在减少训练偏差，统一语音识别的训练和评估指标。O-1是期望最小贝叶斯风险（EMBR）的一种更快速的变体，它增强了神谕假设，并且可以适应有监督和无监督数据。我们通过对公开可用的SpeechStew数据集和大规模内部数据集进行识别效果的实证，证明了我们方法的有效性。在Speechstew上，相对于将实际性能与神谕性能之间的差距缩小43%的EMBR，O-1目标通过80%的相对缩小了这一差距。在SpeechStew的各个数据集上，O-1相对于EMBR实现了13%到25%的相对改进，并且相对于在内部数据集上使用EMBR训练的神谕词错误率相对缩小了12%的差距。总的来说，O-1相对于EMBR在词错误率上实现了9%的相对改进，这表明了该目标在大规模数据集上的可扩展性。

    We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\% relative compared to EMBR which bridges the gap by 43\% relative. O-1 achieves 13\% to 25\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.
    
[^59]: OCDaf: 有序因果推断与自回归流

    OCDaf: Ordered Causal Discovery with Autoregressive Flows. (arXiv:2308.07480v1 [cs.LG])

    [http://arxiv.org/abs/2308.07480](http://arxiv.org/abs/2308.07480)

    我们提出了一种新的有序因果推断方法，可以从观测数据中学习因果图，并在多种基准测试中展示出了最先进的性能。

    

    我们提出了一种新的基于顺序的方法，用于从观测数据中学习因果图。我们在多变量异方差噪声模型中建立了因果图的可识别性，这是对加性噪声模型的推广，允许非常数噪声方差。借鉴这些模型与仿射自回归归一互补规范流的结构相似性，我们引入了一种连续搜索算法来寻找因果结构。我们的实验结果表明，在Sachs和SynTReN基准测试中，在结构汉明距离（SHD）和结构干预距离（SID）方面具有最先进的性能。此外，我们验证了我们的可识别性理论在各种参数和非参数合成数据集上，并展示了与现有基线方法相比的卓越性能。

    We propose OCDaf, a novel order-based method for learning causal graphs from observational data. We establish the identifiability of causal graphs within multivariate heteroscedastic noise models, a generalization of additive noise models that allow for non-constant noise variances. Drawing upon the structural similarities between these models and affine autoregressive normalizing flows, we introduce a continuous search algorithm to find causal structures. Our experiments demonstrate state-of-the-art performance across the Sachs and SynTReN benchmarks in Structural Hamming Distance (SHD) and Structural Intervention Distance (SID). Furthermore, we validate our identifiability theory across various parametric and nonparametric synthetic datasets and showcase superior performance compared to existing baselines.
    
[^60]: Symphony: 使用集中式协调来优化模型服务

    Symphony: Optimized Model Serving using Centralized Orchestration. (arXiv:2308.07470v1 [cs.DC])

    [http://arxiv.org/abs/2308.07470](http://arxiv.org/abs/2308.07470)

    Symphony是一个集中式调度系统，可以优化深度神经网络模型服务，在满足高加速器效率和延迟SLO的同时适应工作负载变化。通过非工作保持调度算法和模型分配算法，Symphony能够实现高批处理效率和强大的自动缩放功能，比之前的系统提供高达4.7倍的吞吐量。

    

    在GPU集群上进行深度神经网络(DNN)模型推理的协调存在两个重要挑战：在满足批处理属性的模型推理同时实现高加速器效率，并满足延迟服务级别目标(SLO)，以及适应工作负载的变化，无论是短期波动还是长期资源分配。为了解决这些挑战，我们提出了Symphony，这是一个可以扩展到每秒数百万个请求并协调数万个GPU的集中式调度系统。我们的系统使用非工作保持调度算法，能够实现高批处理效率，同时也能实现强大的自动缩放。此外，我们还开发了一个基于模型的计算和内存需求分配子集群的算法。通过广泛的实验，我们证明Symphony的性能优于之前的系统，最多可以提高4.7倍的吞吐量。

    The orchestration of deep neural network (DNN) model inference on GPU clusters presents two significant challenges: achieving high accelerator efficiency given the batching properties of model inference while meeting latency service level objectives (SLOs), and adapting to workload changes both in terms of short-term fluctuations and long-term resource allocation. To address these challenges, we propose Symphony, a centralized scheduling system that can scale to millions of requests per second and coordinate tens of thousands of GPUs. Our system utilizes a non-work-conserving scheduling algorithm capable of achieving high batch efficiency while also enabling robust autoscaling. Additionally, we developed an epoch-scale algorithm that allocates models to sub-clusters based on the compute and memory needs of the models. Through extensive experiments, we demonstrate that Symphony outperforms prior systems by up to 4.7x higher goodput.
    
[^61]: Omega-Regular Reward Machines. (arXiv:2308.07469v1 [cs.LG])

    Omega-Regular Reward Machines. (arXiv:2308.07469v1 [cs.LG])

    [http://arxiv.org/abs/2308.07469](http://arxiv.org/abs/2308.07469)

    本文引入了ω-正规奖励机器，将奖励机器与ω-正规语言集成在一起，为RL提供了一种表达能力强且有效的奖励机制。

    

    强化学习（RL）是一种强大的训练代理执行任务的方法，但设计适当的奖励机制对其成功至关重要。然而，在许多情况下，学习目标的复杂性超出了马尔可夫假设的能力范围，需要更复杂的奖励机制。奖励机器和ω-正规语言是用于表示定量和定性目标的非马尔可夫奖励的两种形式化方法。本文引入了ω-正规奖励机器，将奖励机器与ω-正规语言集成在一起，为RL提供了一种表达能力强且有效的奖励机制。我们提出了一种无模型RL算法来计算针对ω-正规奖励机器的ε-最优策略，并通过实验证明了所提算法的有效性。

    Reinforcement learning (RL) is a powerful approach for training agents to perform tasks, but designing an appropriate reward mechanism is critical to its success. However, in many cases, the complexity of the learning objectives goes beyond the capabilities of the Markovian assumption, necessitating a more sophisticated reward mechanism. Reward machines and omega-regular languages are two formalisms used to express non-Markovian rewards for quantitative and qualitative objectives, respectively. This paper introduces omega-regular reward machines, which integrate reward machines with omega-regular languages to enable an expressive and effective reward mechanism for RL. We present a model-free RL algorithm to compute epsilon-optimal strategies against omega-egular reward machines and evaluate the effectiveness of the proposed algorithm through experiments.
    
[^62]: 数字艺术史的存在

    There Is a Digital Art History. (arXiv:2308.07464v1 [cs.CV])

    [http://arxiv.org/abs/2308.07464](http://arxiv.org/abs/2308.07464)

    该论文重点探讨了数字艺术史的存在与发展，并指出大规模视觉模型对数字艺术史具有重要影响，打开了提取和自动化视觉逻辑的新可能性。

    

    在这篇论文中，我们重新审视了约翰娜·德鲁克(Johanna Drucker)十年前提出的“是否存在数字艺术史？”这个问题，这是基于大规模的、基于transformer的视觉模型的出现之后的。虽然传统型神经网络早已成为数字艺术史的一部分，数字人文项目最近开始使用transformer模型，但它们的认识论影响和方法学可负担性尚未得到系统化的分析。我们将分析重点放在两个主要方面，这两个方面共同表明了向德鲁克所说的“数字”艺术史的一种新的范式转变的趋势。一方面，大规模视觉模型中新编码的视觉文化代表作品对数字艺术史产生了巨大影响。包含大量非摄影影像使得可以提取和自动化不同形式的视觉逻辑。大规模视觉模型“看到了”通过网络中介的西方视觉经典的大部分内容。

    In this paper, we revisit Johanna Drucker's question, "Is there a digital art history?" -- posed exactly a decade ago -- in the light of the emergence of large-scale, transformer-based vision models. While more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net vi
    
[^63]: GRU-D-Weibull:一种新型的实时个体化终点预测方法

    GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction. (arXiv:2308.07452v1 [cs.LG])

    [http://arxiv.org/abs/2308.07452](http://arxiv.org/abs/2308.07452)

    GRU-D-Weibull是一种新型的实时个体化终点预测方法，结合了门控循环单元和衰减技术，用于建模威布尔分布。通过在慢性肾脏疾病患者队列中的评估，发现该方法在终点预测方面的性能优于其他竞争方法。

    

    在临床实践中，准确的个体级终点和终点时间预测模型至关重要。本研究提出了一种新的方法，GRU-D-Weibull，它将门控循环单元与衰减技术（GRU-D）结合，来建模威布尔分布。我们的方法实现了实时个体化终点预测和群体级风险管理。我们使用一个包含6,879名慢性肾脏疾病（CKD4）患者的队列，评估了GRU-D-Weibull在终点预测中的性能。在指标日期，GRU-D-Weibull的C-指数约为0.7，随着4.3年的随访，C-指数提高到约0.77，类似于随机生存森林。我们的方法在CKD4指标日期的绝对L1损失约为1.1年（标准差0.95），随访4年后的最小L1损失约为0.45年（标准差0.3），明显优于其他竞争方法。相比之下，GRU-D-Weibull在事件发生时的预测生存概率范围更小且更固定。

    Accurate prediction models for individual-level endpoints and time-to-endpoints are crucial in clinical practice. In this study, we propose a novel approach, GRU-D-Weibull, which combines gated recurrent units with decay (GRU-D) to model the Weibull distribution. Our method enables real-time individualized endpoint prediction and population-level risk management. Using a cohort of 6,879 patients with stage 4 chronic kidney disease (CKD4), we evaluated the performance of GRU-D-Weibull in endpoint prediction. The C-index of GRU-D-Weibull was ~0.7 at the index date and increased to ~0.77 after 4.3 years of follow-up, similar to random survival forest. Our approach achieved an absolute L1-loss of ~1.1 years (SD 0.95) at the CKD4 index date and a minimum of ~0.45 years (SD0.3) at 4 years of follow-up, outperforming competing methods significantly. GRU-D-Weibull consistently constrained the predicted survival probability at the time of an event within a smaller and more fixed range compared 
    
[^64]: 使用在聚类数据上训练的集成模型进行开放集合人脸识别

    Open-set Face Recognition using Ensembles trained on Clustered Data. (arXiv:2308.07445v1 [cs.CV])

    [http://arxiv.org/abs/2308.07445](http://arxiv.org/abs/2308.07445)

    本研究提出了一种使用在聚类数据上训练的集成模型进行开放集合人脸识别的方法，能够准确识别感兴趣的个体，同时有效处理陌生的面孔。实验结果表明即使在大规模图库中也能取得竞争性的性能。

    

    开放集合人脸识别描述了在测试时出现未知的主题，而在训练阶段未见过。它不仅需要准确识别感兴趣的个体的方法，还需要有效处理陌生的面孔的方法。本文详细介绍了一个可扩展的开放集合人脸识别方法，适用于包含数百和数千个主题的图库。它由聚类和一组二进制学习算法组成，用于估计查询人脸样本是否属于人脸图库，并检索其正确的身份。该方法选择最合适的图库主题，并使用集成模型改善预测性能。我们在知名的LFW和YTF基准测试上进行实验。结果表明，即使针对可扩展性，也可以达到竞争性的性能。

    Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
    
[^65]: 使用物理先验的深度学习降低联合预测氮氧化物的偏差

    Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides. (arXiv:2308.07441v1 [cs.LG])

    [http://arxiv.org/abs/2308.07441](http://arxiv.org/abs/2308.07441)

    该论文提出了一个物理先验的深度学习框架，通过编码平流-扩散机制和流体动力学约束来联合预测NO2和NOx，并成功降低了机器学习模型的偏差。

    

    大气中的氮氧化物（NOx）主要来自燃料燃烧，对健康和环境有明显的急性和慢性影响。机器学习（ML）方法显著增强了我们在高时空分辨率下预测地面上NOx浓度的能力，但由于缺乏有关大气污染动力学的物理和化学知识，可能存在高估偏差。化学传输模型（CTMs）利用这些知识；然而，准确预测地面浓度通常需要进行广泛的后校准。在这里，我们提出了一个物理先验的深度学习框架，通过编码平流-扩散机制和流体动力学约束来联合预测NO2和NOx，并将ML模型的偏差降低了21-42％。我们的方法捕捉到了NO2和NOx的细粒度传输，生成了强大的空间外推，并提供了明确的不确定性估计。该框架将CTM的知识驱动的物理化学原理与深度学习相融合。

    Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have recognized acute and chronic health and environmental effects. Machine learning (ML) methods have significantly enhanced our capacity to predict NOx concentrations at ground-level with high spatiotemporal resolution but may suffer from high estimation bias since they lack physical and chemical knowledge about air pollution dynamics. Chemical transport models (CTMs) leverage this knowledge; however, accurate predictions of ground-level concentrations typically necessitate extensive post-calibration. Here, we present a physics-informed deep learning framework that encodes advection-diffusion mechanisms and fluid dynamics constraints to jointly predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures fine-scale transport of NO2 and NOx, generates robust spatial extrapolation, and provides explicit uncertainty estimation. The framework fuses knowledge-driven physicochemical principles of CTMs with th
    
[^66]: 使用时间图神经网络的交互感知个性化车辆轨迹预测

    Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks. (arXiv:2308.07439v1 [cs.LG])

    [http://arxiv.org/abs/2308.07439](http://arxiv.org/abs/2308.07439)

    本研究提出了一种交互感知的个性化车辆轨迹预测方法，利用时间图神经网络来建模目标车辆与周围交通之间的时空交互，并通过迁移学习来个性化预测。实验结果表明，该方法能够更准确地预测车辆的轨迹。

    

    准确预测车辆轨迹对于先进驾驶辅助系统和自动驾驶汽车至关重要。现有方法主要依赖于从大型数据集中推导出的通用轨迹预测，忽视了个别驾驶员的个性化驾驶模式。为了弥补这一差距，我们提出了一种交互感知的个性化车辆轨迹预测方法，该方法采用了时间图神经网络。我们的方法利用图卷积神经网络（GCN）和长短期记忆（LSTM）来建模目标车辆与周围交通之间的时空交互。为了个性化预测，我们建立了一个利用迁移学习的流程：模型首先在大规模轨迹数据集上进行预训练，然后使用特定驾驶数据针对每个驾驶员进行微调。我们采用人机协同仿真来收集个性化的自然驾驶轨迹及其相应的周围车辆轨迹。

    Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories
    
[^67]: 一种基于深度时空注意力的混合模型用于帕金森病的诊断——利用静息状态脑电图信号

    A Hybrid Deep Spatio-Temporal Attention-Based Model for Parkinson's Disease Diagnosis Using Resting State EEG Signals. (arXiv:2308.07436v1 [eess.SP])

    [http://arxiv.org/abs/2308.07436](http://arxiv.org/abs/2308.07436)

    本研究提出了一种基于深度学习的模型，利用静息状态的脑电图信号进行帕金森病的诊断。该模型能够从EEG中提取复杂的非线性特征，并在多个数据集上实现准确的诊断。

    

    帕金森病（PD）是一种严重且进行性的神经疾病，影响着全球数百万人。为了实现帕金森病的有效治疗和管理，准确和早期的诊断至关重要。本研究提出了一种基于深度学习的模型，用于利用静息状态的脑电图（EEG）信号进行帕金森病的诊断。研究的目标是开发一个自动化模型，能够从EEG中提取复杂的隐藏非线性特征，并在未见数据上展示其泛化能力。该模型采用了卷积神经网络（CNN）、双向门控循环单元（Bi-GRU）和注意机制构建的混合模型。所提出的方法在三个公开数据集（UC San Diego数据集、PRED-CT数据集和University of Iowa (UI)数据集）上进行了评估，其中一个数据集用于训练，另外两个用于评估。结果表明，所提出的模型可以在训练和测试数据集上高性能地准确诊断帕金森病。

    Parkinson's disease (PD), a severe and progressive neurological illness, affects millions of individuals worldwide. For effective treatment and management of PD, an accurate and early diagnosis is crucial. This study presents a deep learning-based model for the diagnosis of PD using resting state electroencephalogram (EEG) signal. The objective of the study is to develop an automated model that can extract complex hidden nonlinear features from EEG and demonstrate its generalizability on unseen data. The model is designed using a hybrid model, consists of convolutional neural network (CNN), bidirectional gated recurrent unit (Bi-GRU), and attention mechanism. The proposed method is evaluated on three public datasets (Uc San Diego Dataset, PRED-CT, and University of Iowa (UI) dataset), with one dataset used for training and the other two for evaluation. The results show that the proposed model can accurately diagnose PD with high performance on both the training and hold-out datasets. T
    
[^68]: 通过指数倾斜解决RTB市场中的分布偏移问题

    Addressing Distribution Shift in RTB Markets via Exponential Tilting. (arXiv:2308.07424v1 [stat.ML])

    [http://arxiv.org/abs/2308.07424](http://arxiv.org/abs/2308.07424)

    本文介绍了一种名为ExTRA的算法，用于解决机器学习模型中的分布偏移问题。通过确定源数据上的重要性权重，该方法能够最小化加权源数据和目标数据集之间的KL散度。通过实验验证，证明了这种方法的适用性。

    

    机器学习模型中的分布偏移可能是性能下降的主要原因。本文深入探讨了这些偏移的特性，主要针对实时竞价（RTB）市场模型的特点。我们强调了类别不平衡和样本选择偏差所带来的挑战，这两者均是分布偏移的强有力诱因。本文介绍了一种名为ExTRA（Exponential Tilt Reweighting Alignment）的算法，该算法由Marty等人（2023）提出，用于解决数据中的分布偏移问题。ExTRA方法旨在确定源数据上的重要性权重，以最小化加权源数据和目标数据集之间的KL散度。该方法的一个显著优点是它能够使用有标签的源数据和无标签的目标数据进行操作。通过模拟真实世界数据，我们研究了分布偏移的性质，并评估了所提出模型的适用性。

    Distribution shift in machine learning models can be a primary cause of performance degradation. This paper delves into the characteristics of these shifts, primarily motivated by Real-Time Bidding (RTB) market models. We emphasize the challenges posed by class imbalance and sample selection bias, both potent instigators of distribution shifts. This paper introduces the Exponential Tilt Reweighting Alignment (ExTRA) algorithm, as proposed by Marty et al. (2023), to address distribution shifts in data. The ExTRA method is designed to determine the importance weights on the source data, aiming to minimize the KL divergence between the weighted source and target datasets. A notable advantage of this method is its ability to operate using labeled source data and unlabeled target data. Through simulated real-world data, we investigate the nature of distribution shift and evaluate the applicacy of the proposed model.
    
[^69]: U-Turn扩散

    U-Turn Diffusion. (arXiv:2308.07421v1 [cs.LG])

    [http://arxiv.org/abs/2308.07421](http://arxiv.org/abs/2308.07421)

    U-Turn扩散是一种用于生成合成图像的AI模型，通过引入U-Turn Diffusion技术来改进生成图像的质量。这种技术结合了前向、U-Turn和反向过程，通过解构快速相关性来提高生成过程的效率。

    

    我们对基于分数的扩散模型进行了全面研究，用于生成合成图像的AI模型。这些模型依赖于由随机微分方程驱动的动态辅助时间机制，在输入图像中获取分数函数。我们的研究揭示了评估基于分数的扩散模型效率的标准：生成过程的能力取决于在反向/去噪阶段解构快速相关性的能力。为了提高生成的合成图像质量，我们引入了一种被称为“U-Turn Diffusion”的方法。U-Turn Diffusion技术从标准的前向扩散过程开始，尽管相对于传统设置，它的持续时间更短。随后，我们执行标准的反向动力学，以前向过程的最终配置为初始值。这种结合了前向、U-Turn和反向过程的U-Turn Diffusion过程创建一个合成图像。

    We present a comprehensive examination of score-based diffusion models of AI for generating synthetic images. These models hinge upon a dynamic auxiliary time mechanism driven by stochastic differential equations, wherein the score function is acquired from input images. Our investigation unveils a criterion for evaluating efficiency of the score-based diffusion models: the power of the generative process depends on the ability to de-construct fast correlations during the reverse/de-noising phase. To improve the quality of the produced synthetic images, we introduce an approach coined "U-Turn Diffusion". The U-Turn Diffusion technique starts with the standard forward diffusion process, albeit with a condensed duration compared to conventional settings. Subsequently, we execute the standard reverse dynamics, initialized with the concluding configuration from the forward process. This U-Turn Diffusion procedure, combining forward, U-turn, and reverse processes, creates a synthetic image 
    
[^70]: 本地自适应可微回归

    Locally Adaptive and Differentiable Regression. (arXiv:2308.07418v1 [cs.LG])

    [http://arxiv.org/abs/2308.07418](http://arxiv.org/abs/2308.07418)

    本文提出了一种本地自适应可微回归模型，通过对局部学习模型进行加权平均，在不同本地区域处理数据时具有竞争力，并在理论上实现更快的统计收敛以及在实际应用中改善了性能。

    

    过度参数化模型，如深度神经网络和随机森林，在机器学习中变得非常受欢迎。然而，在现代超参数化的本地自适应模型中，常见的连续性和可微性目标往往被忽视。我们提出了一个通用框架，通过在对应的本地区域中对局部学习模型进行加权平均来构建全局连续可微模型。该模型在处理具有不同密度或不同本地区域中的函数值尺度的数据时具有竞争力。我们证明，当我们在本地模型中混合使用核岭和多项式回归项，并对它们进行连续拼接时，在理论上实现更快的统计收敛，并在各种实际环境中实现改进的性能。

    Over-parameterized models like deep nets and random forests have become very popular in machine learning. However, the natural goals of continuity and differentiability, common in regression models, are now often ignored in modern overparametrized, locally-adaptive models. We propose a general framework to construct a global continuous and differentiable model based on a weighted average of locally learned models in corresponding local regions. This model is competitive in dealing with data with different densities or scales of function values in different local regions. We demonstrate that when we mix kernel ridge and polynomial regression terms in the local models, and stitch them together continuously, we achieve faster statistical convergence in theory and improved performance in various practical settings.
    
[^71]: 语音模型中用于大写和交替预测的文本注入

    Text Injection for Capitalization and Turn-Taking Prediction in Speech Models. (arXiv:2308.07395v1 [cs.CL])

    [http://arxiv.org/abs/2308.07395](http://arxiv.org/abs/2308.07395)

    本研究探讨了在语音模型中利用文本注入进行非ASR任务的辅助，并使用联合训练算法提升大写准确率和交替检测召回率。

    

    文本注入用于自动语音识别（ASR），其中非配对的纯文本数据用于补充音频-文本数据，已显示出对词错误率有显著改善。该研究研究了文本注入用于辅助任务，在E2E模型中常用于非ASR任务。在本研究中，我们使用联合端到端和内部语言模型训练（JEIT）作为我们的文本注入算法，训练一个ASR模型来完成两个辅助任务。第一个任务是大写，是一种去标准化任务。第二个任务是交替预测，尝试识别用户是否已完成对话交替，在数字助理互动中。我们展示了我们的文本注入方法能提升长尾数据的大写性能，并改善了交替检测的召回率。

    Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.
    
[^72]: DISBELIEVE：客户模型之间的距离对于有效的本地模型污染攻击非常重要

    DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks. (arXiv:2308.07387v1 [cs.LG])

    [http://arxiv.org/abs/2308.07387](http://arxiv.org/abs/2308.07387)

    DISBELIEVE 是一种本地模型污染攻击，通过在参数或梯度之间创建恶意距离与良性客户端的距离较小的恶意参数或梯度，破坏联合系统的性能。

    

    联合学习是解决分享患者敏感数据相关隐私问题的一种有前景的方向。在医学图像分析领域中，联合系统通常假设参与的本地客户端是诚实的。然而，一些研究报告了一些恶意客户端可以通过的机制，这些机制可以污染联合设置，破坏全局模型的性能。为了克服这个问题，已经提出了一些坚固的聚合方法，可以防御这些攻击。我们观察到，大多数最先进的坚固聚合方法非常依赖于恶意客户端和良性客户端的参数或梯度之间的距离，这使得它们容易受到本地模型污染攻击的影响，当恶意和良性客户端的参数或梯度很接近时。基于这一点，我们介绍了一种名为DISBELIEVE的本地模型污染攻击，它创建恶意参数或梯度，使得它们与良性客户端之间的距离很小。

    Federated learning is a promising direction to tackle the privacy issues related to sharing patients' sensitive data. Often, federated systems in the medical image analysis domain assume that the participating local clients are \textit{honest}. Several studies report mechanisms through which a set of malicious clients can be introduced that can poison the federated setup, hampering the performance of the global model. To overcome this, robust aggregation methods have been proposed that defend against those attacks. We observe that most of the state-of-the-art robust aggregation methods are heavily dependent on the distance between the parameters or gradients of malicious clients and benign clients, which makes them prone to local model poisoning attacks when the parameters or gradients of malicious and benign clients are close. Leveraging this, we introduce DISBELIEVE, a local model poisoning attack that creates malicious parameters or gradients such that their distance to benign clien
    
[^73]: 使用不同程度的合并疾病信息改进基于ICD的语义相似度

    Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity. (arXiv:2308.07359v1 [cs.LG])

    [http://arxiv.org/abs/2308.07359](http://arxiv.org/abs/2308.07359)

    本研究通过考虑记录合并疾病的比例项，改进了基于ICD码的语义相似度计算方法。

    

    在精准医学中，寻找相似的患者是常见的目标，有助于治疗结果评估和临床决策支持。选择广泛可用的患者特征和适当的数学方法来计算相似度是至关重要的。国际疾病和相关健康问题统计分类（ICD）码被全球范围内用于编码疾病，并且几乎适用于所有患者。将其聚合为包含主要和次要诊断的集合，它们可以显示出一定程度的合并疾病并揭示合并疾病模式。可以使用语义相似度算法根据患者的ICD码计算相似度。这些算法通常使用单术语专家评分的数据集进行评估。然而，实际患者数据往往显示出不同程度的记录合并疾病，可能会影响算法的性能。为了解决这个问题，我们提出了一个考虑记录合并疾病的比例项。

    Finding similar patients is a common objective in precision medicine, facilitating treatment outcome assessment and clinical decision support. Choosing widely-available patient features and appropriate mathematical methods for similarity calculations is crucial. International Statistical Classification of Diseases and Related Health Problems (ICD) codes are used worldwide to encode diseases and are available for nearly all patients. Aggregated as sets consisting of primary and secondary diagnoses they can display a degree of comorbidity and reveal comorbidity patterns. It is possible to compute the similarity of patients based on their ICD codes by using semantic similarity algorithms. These algorithms have been traditionally evaluated using a single-term expert rated data set.  However, real-word patient data often display varying degrees of documented comorbidities that might impair algorithm performance. To account for this, we present a scale term that considers documented comorbid
    
[^74]: 使用图神经网络增强专家引导的网格划分的确认性预测

    Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks. (arXiv:2308.07358v1 [cs.GR])

    [http://arxiv.org/abs/2308.07358](http://arxiv.org/abs/2308.07358)

    本研究提出了一种机器学习的方案，结合图神经网络和专家指导自动化生成计算流体力学网格，并引入了一种新的三维分割算法，用于表面分类。

    

    计算流体力学在不同的工程领域广泛应用，但准确的模拟依赖于仿真域的适当网格划分。虽然高度精细的网格可以确保精度，但也带来了高计算成本。类似地，自适应重网格技术需要多次仿真，计算成本高。这意味着网格划分过程依赖于专业知识和多年的经验。自动化网格生成可以节省大量的时间和精力，并带来更快速和高效的设计过程。本文提出了一种基于机器学习的方案，利用图神经网络（GNN）和专家指导来自动生成飞机模型的计算流体力学网格。在这项工作中，我们引入了一种优于两个先进模型（PointNet++和PointMLP）的新的三维分割算法，用于表面分类。我们还提出了一种新颖的方法，用于从三维网格分割模型中投射预测结果。

    Computational Fluid Dynamics (CFD) is widely used in different engineering fields, but accurate simulations are dependent upon proper meshing of the simulation domain. While highly refined meshes may ensure precision, they come with high computational costs. Similarly, adaptive remeshing techniques require multiple simulations and come at a great computational cost. This means that the meshing process is reliant upon expert knowledge and years of experience. Automating mesh generation can save significant time and effort and lead to a faster and more efficient design process. This paper presents a machine learning-based scheme that utilizes Graph Neural Networks (GNN) and expert guidance to automatically generate CFD meshes for aircraft models. In this work, we introduce a new 3D segmentation algorithm that outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. We also present a novel approach to project predictions from 3D mesh segmentation model
    
[^75]: 自闭症谱系障碍中形态连接模式的年龄分层差异：一种sMRI和机器学习方法的探索

    Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach. (arXiv:2308.07356v1 [eess.IV])

    [http://arxiv.org/abs/2308.07356](http://arxiv.org/abs/2308.07356)

    本研究通过sMRI和机器学习方法研究了自闭症谱系障碍患者中的形态连接模式在不同年龄组之间的差异，并发现了一些显著特征。

    

    目的：年龄偏见已被确定为自闭症诊断的一个重要因素。本研究的目标是比较不同年龄组在使用形态特征（MF）和形态连接特征（MCF）进行自闭症分类方面的影响。方法：本研究使用了来自两个公开可用数据库ABIDE-I和ABIDE-II的sMRI数据。在我们的分析中，我们考虑了三个年龄组：6至11岁、11至18岁和6至18岁。sMRI数据经过标准处理流程预处理，然后根据Destrieux视图将其划分为148个不同的区域。然后提取每个区域的面积、厚度、体积和平均曲率信息，用于创建每个受试者的总共592个MF和10,878个MCF。使用统计t检验（p<0.05）确定了显著特征，并用其进行随机森林（RF）分类器的训练。结果：我们的研究结果表明，

    Purpose: Age biases have been identified as an essential factor in the diagnosis of ASD. The objective of this study was to compare the effect of different age groups in classifying ASD using morphological features (MF) and morphological connectivity features (MCF). Methods: The structural magnetic resonance imaging (sMRI) data for the study was obtained from the two publicly available databases, ABIDE-I and ABIDE-II. We considered three age groups, 6 to 11, 11 to 18, and 6 to 18, for our analysis. The sMRI data was pre-processed using a standard pipeline and was then parcellated into 148 different regions according to the Destrieux atlas. The area, thickness, volume, and mean curvature information was then extracted for each region which was used to create a total of 592 MF and 10,878 MCF for each subject. Significant features were identified using a statistical t-test (p<0.05) which was then used to train a random forest (RF) classifier. Results: The results of our study suggested th
    
[^76]: 基于贝叶斯物理信息神经网络的工程纳米颗粒在受污染含水层中的正向和反向模拟

    Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer. (arXiv:2308.07352v1 [cs.LG])

    [http://arxiv.org/abs/2308.07352](http://arxiv.org/abs/2308.07352)

    本研究使用基于贝叶斯物理信息神经网络（B-PINN）的框架来模拟含水层中纳米颗粒的运动性，并为理解和开发高效的修复策略提供了预测性工具。

    

    在全球范围内，有许多受污染的地下水场地需要积极的修复计划来恢复当地生态系统和环境。工程纳米颗粒（ENPs）已被证明是地下水中污染物原位降解的有效反应剂。虽然这些ENPs在实验室阶段表现出色，但在实际场地条件下的应用仍然有限。ENPs的复杂输运和滞留机制阻碍了高效的修复策略的开发。因此，需要一种预测性工具来理解ENPs的输运和滞留行为。现有文献中的工具主要是以数值模拟器为主，对稀疏数据集和含水层异质性的存在具有有限的灵活性和准确性。本研究使用基于贝叶斯物理信息神经网络（B-PINN）的框架来模拟含水层中纳米颗粒的运动性。

    Globally, there are many polluted groundwater sites that need an active remediation plan for the restoration of local ecosystem and environment. Engineered nanoparticles (ENPs) have proven to be an effective reactive agent for the in-situ degradation of pollutants in groundwater. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool to comprehend the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets and the aquifer heterogeneity. This work uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the nano-particles mobility within an aquifer. The result from the f
    
[^77]: IOB：集成优化传递和行为传递用于多策略复用

    IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse. (arXiv:2308.07351v1 [cs.LG])

    [http://arxiv.org/abs/2308.07351](http://arxiv.org/abs/2308.07351)

    该论文提出了一种新颖的传递强化学习方法，通过在actor-critic框架中使用Q函数来选择源策略，解决了在有限样本下选择适当的源策略的挑战。

    

    人类有能力重复利用之前学习的策略来快速解决新任务，而强化学习（RL）代理也可以通过从源策略向相关目标任务传递知识来做到同样的事情。传递RL方法可以通过改变策略优化目标（优化传递）或者影响行为策略（行为传递）来利用源策略。然而，在有限的样本下选择适当的源策略来引导目标策略学习一直是一个挑战。之前的方法引入了额外的组件，比如层次策略或者源策略的值函数估计，这可能导致非平稳的策略优化或者大量的采样成本，从而降低了传递的有效性。为了解决这个挑战，我们提出了一种新颖的传递RL方法，选择源策略而无需训练额外的组件。我们的方法利用了actor-critic框架中的Q函数来指导策略选择，选择源策略。

    Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source poli
    
[^78]: 使用量化感知训练的高效神经PDE求解器

    Efficient Neural PDE-Solvers using Quantization Aware Training. (arXiv:2308.07350v1 [cs.LG])

    [http://arxiv.org/abs/2308.07350](http://arxiv.org/abs/2308.07350)

    使用量化感知训练的高效神经PDE求解器研究了在减少计算成本方面的潜力，并证明了量化网络权重和激活可以成功降低计算成本而不损害性能。

    

    过去几年中，将神经网络应用作为解决偏微分方程的经典数值方法的替代方案已经成为这个有着百年历史的数学领域潜在范式转变。然而，在实际可行性方面，计算成本仍然是一个重大瓶颈。传统的方法通过限制PDE定义的空间分辨率来减轻这个挑战。对于神经PDE求解器，我们可以做得更好：在这里，我们研究了最先进的量化方法在降低计算成本方面的潜力。我们展示了对网络权重和激活进行量化可以成功降低推断的计算成本，同时保持性能。我们在四个标准PDE数据集和三个网络架构上的结果表明，量化感知训练适用于各种设置和三个数量级的FLOPs。最后，我们以实证的方式证明了计算成本与模型性能之间的帕累托最优关系。

    In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs p
    
[^79]: Conic Descent Redux for Memory-Efficient Optimization （内存高效优化的锥下降重返）

    Conic Descent Redux for Memory-Efficient Optimization. (arXiv:2308.07343v1 [math.OC])

    [http://arxiv.org/abs/2308.07343](http://arxiv.org/abs/2308.07343)

    这项工作提出了一种内存高效的锥下降优化算法，该算法从对偶问题中得到几何推导，提出了动量锥下降（MOCO）变体，并探索了提速对偶收敛的可能性。

    

    锥编程在信号处理和机器学习任务中有着众所周知的优点。这项工作重访了最近发展起来的一阶锥下降（CD）求解器，并在直觉、理论和算法实现方面进行了改进。发现CD可以从对偶问题中得到一个直观的几何推导，这为新的算法设计提供了可能，其中一种是CD的动量变体——动量锥下降（MOCO）。对CD和MOCO的对偶行为进行更深入的研究揭示出：i）一个经过分析验证的停止准则；ii）设计预处理器加速对偶收敛的潜力。最后，为了针对低秩解尤其是半定规划（SDP）进行规模化，提出了一种内存高效的MOCO变体，并进行了数值验证。

    Conic programming has well-documented merits in a gamut of signal processing and machine learning tasks. This contribution revisits a recently developed first-order conic descent (CD) solver, and advances it in three aspects: intuition, theory, and algorithmic implementation. It is found that CD can afford an intuitive geometric derivation that originates from the dual problem. This opens the door to novel algorithmic designs, with a momentum variant of CD, momentum conic descent (MOCO) exemplified. Diving deeper into the dual behavior CD and MOCO reveals: i) an analytically justified stopping criterion; and, ii) the potential to design preconditioners to speed up dual convergence. Lastly, to scale semidefinite programming (SDP) especially for low-rank solutions, a memory efficient MOCO variant is developed and numerically validated.
    
[^80]: 从合成语料库和形式逻辑学习演绎推理

    Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])

    [http://arxiv.org/abs/2308.07336](http://arxiv.org/abs/2308.07336)

    本研究研究了一种从合成语料库中学习演绎推理能力的方法，通过采用基于形式逻辑理论的演绎规则，训练的语言模型具有更泛化的推理能力。

    

    我们研究了一种从合成语料库中学习演绎推理能力的语言模型（LMs）方法。之前的研究使用了具体的演绎规则来生成演绎示例，但这些规则受限或者是任意的。这可能限制了所获得演绎推理能力的泛化能力。我们重新思考并采用基于形式逻辑理论的一组良好基础的演绎规则，当这些规则以多步方式组合时，可以推导出任何其他演绎规则。我们通过实验证明，在提出的语料库上训练的LMs，即$\textbf{FLD}$（$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction），获得了更具泛化性的演绎推理能力。此外，我们确定了演绎推理语料库可以增强LMs的推理能力的方面，以及不同方面无法增强的方面。最后，基于这些结果，我们讨论了将演绎语料库或其他方法应用于每个方面的未来方向。

    We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
    
[^81]: 一个编码-解码的方法用于装填圆形

    An Encoder-Decoder Approach for Packing Circles. (arXiv:2308.07335v1 [cs.AI])

    [http://arxiv.org/abs/2308.07335](http://arxiv.org/abs/2308.07335)

    本文提出了一个编码-解码的方法，用于将相同的圆形放置在一个较大的圆形中。该方法通过编码和解码过程，以及添加受控扰动来有效解决装填问题。

    

    自从几十年以来，将较小的物体完全放置在较大的物体内一直是一个有趣的问题。在这些问题中，除了要求较小的物体必须完全位于较大的物体内，还要求它们不重叠或尽量最小化重叠面积。因此，装填问题成为一个非凸问题，其最优解的获取具有挑战性。因此，在一般情况下，我们使用了一些启发式方法来获取亚最优解，并针对某些特殊情况获得了可证明最优解。在本文中，我们提出了一种新颖的编码-解码架构，包括编码器块、扰动块和解码器块，用于将相同的圆形放置在一个较大的圆形中。在我们的方法中，编码器以要放置的圆的索引作为输入，并通过标准化层输出其中心，扰动层对中心添加受控扰动，以确保它不会超出边界。解码器通过反转编码器过程来恢复圆的位置。通过训练集进行训练和优化，我们的方法可以有效地解决圆形装填问题，并取得了很好的效果。

    The problem of packing smaller objects within a larger object has been of interest since decades. In these problems, in addition to the requirement that the smaller objects must lie completely inside the larger objects, they are expected to not overlap or have minimum overlap with each other. Due to this, the problem of packing turns out to be a non-convex problem, obtaining whose optimal solution is challenging. As such, several heuristic approaches have been used for obtaining sub-optimal solutions in general, and provably optimal solutions for some special instances. In this paper, we propose a novel encoder-decoder architecture consisting of an encoder block, a perturbation block and a decoder block, for packing identical circles within a larger circle. In our approach, the encoder takes the index of a circle to be packed as an input and outputs its center through a normalization layer, the perturbation layer adds controlled perturbations to the center, ensuring that it does not de
    
[^82]: Blackjack的强化学习性能变化

    Variations on the Reinforcement Learning performance of Blackjack. (arXiv:2308.07329v1 [cs.AI])

    [http://arxiv.org/abs/2308.07329](http://arxiv.org/abs/2308.07329)

    本研究探讨了在不同的牌堆大小下，强化学习代理在Blackjack游戏中的表现变化。研究发现算法的学习收敛速度与牌堆大小有关，并展示了使用基本策略和HI-LO系统的牌数计数器如何使庄家破产。这项工作的创新之处在于认识到牌堆大小是影响Blackjack表现的关键因素。

    

    Blackjack或称为“21点”是一种流行的基于扑克牌的运气和技巧游戏。游戏的目标是在不超过21点的情况下，获得比庄家更高的手牌总数。理想的黑杰克策略将在长期内最大化财务回报，同时避免赌徒的破产。由于黑杰克的随机环境和固有的奖励结构，这为我们更好地理解强化学习代理在环境变化下的表现提供了一个吸引人的问题。在这里，我们考虑了一种用于最佳玩法的Q-learning解决方案，并研究了算法学习收敛速度与牌堆大小的关系。我们还实现了一个允许使用通用黑杰克规则的模拟器，以展示一个使用基本策略和HI-LO系统的牌数完美计数器如何使庄家破产，以及环境变化对这个结果的影响。我们的工作的创新之处在于将牌堆大小的影响概念性地理解为黑杰克中的因素。

    Blackjack or "21" is a popular card-based game of chance and skill. The objective of the game is to win by obtaining a hand total higher than the dealer's without exceeding 21. The ideal blackjack strategy will maximize financial return in the long run while avoiding gambler's ruin. The stochastic environment and inherent reward structure of blackjack presents an appealing problem to better understand reinforcement learning agents in the presence of environment variations. Here we consider a q-learning solution for optimal play and investigate the rate of learning convergence of the algorithm as a function of deck size. A blackjack simulator allowing for universal blackjack rules is also implemented to demonstrate the extent to which a card counter perfectly using the basic strategy and hi-lo system can bring the house to bankruptcy and how environment variations impact this outcome. The novelty of our work is to place this conceptual understanding of the impact of deck size in the con
    
[^83]: AI文本-行为：可操控性研究

    AI Text-to-Behavior: A Study In Steerability. (arXiv:2308.07326v1 [cs.AI])

    [http://arxiv.org/abs/2308.07326](http://arxiv.org/abs/2308.07326)

    本研究探讨了大型语言模型的可操控性，通过使用行为心理学框架，模型在生成文本时能够根据提示展现特定的行为特征。研究发现，模型在不同特征上的表现具有灵活性和区分度，并能够准确复制历史人物的个性和对话风格。

    

    该研究探讨了大型语言模型（LLM），特别是OpenAI的ChatGPT迭代版本的可操控性。通过采用一种名为OCEAN（开放性，责任心，外向性，宜人性，神经质）的行为心理学框架，我们量化评估了模型对定制提示的响应能力。当要求生成类似于外向人格的文本时，OCEAN得分对齐到了该行为特质。在我们的分析中，“开放性”呈现了语言的模糊性，而“责任心”和“神经质”在OCEAN框架中明确地被唤起，而“外向性”和“宜人性”则展示了与其他特征明显重叠但又有明显分离的特点。我们的发现强调了GPT的多功能性和识别以及适应微妙指导的能力。此外，历史人物模拟突出了LLM内化和投射可指导个性的能力，精确复制了他们的哲学和对话风格。

    The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. How
    
[^84]: 改进3D医学图像的区分概率检测

    Redesigning Out-of-Distribution Detection on 3D Medical Images. (arXiv:2308.07324v1 [eess.IV])

    [http://arxiv.org/abs/2308.07324](http://arxiv.org/abs/2308.07324)

    该论文重设计了医学图像的区分概率检测问题，引入了Expected Performance Drop (EPD)度量，通过下游模型的性能来定义异常样本，并且能够根据临床影响对方法进行排名。

    

    对于可信的医学图像分割中的区分概率（OOD）样本的检测仍然是一个重要挑战。关键问题在于缺乏对异常数据的严格定义，这经常导致无法衡量临床影响的人为问题设置。在本文中，我们根据体积医学成像和相关下游任务（例如分割）的特定情况重新设计了OOD检测问题。我们提出使用下游模型的性能作为图像之间异常样本的伪度量。这种方法使我们能够根据性能的影响权衡不同的样本，而无需显式的ID/OOD区分。我们在新的度量标准Expected Performance Drop (EPD) 中加入了这种权重。 EPD是我们对新问题设计的核心贡献，它使我们能够根据其临床影响对方法进行排名。我们在11个CT和MRI OOD检测挑战中展示了EPD评估的有效性。

    Detecting out-of-distribution (OOD) samples for trusted medical image segmentation remains a significant challenge. The critical issue here is the lack of a strict definition of abnormal data, which often results in artificial problem settings without measurable clinical impact. In this paper, we redesign the OOD detection problem according to the specifics of volumetric medical imaging and related downstream tasks (e.g., segmentation). We propose using the downstream model's performance as a pseudometric between images to define abnormal samples. This approach enables us to weigh different samples based on their performance impact without an explicit ID/OOD distinction. We incorporate this weighting in a new metric called Expected Performance Drop (EPD). EPD is our core contribution to the new problem design, allowing us to rank methods based on their clinical impact. We demonstrate the effectiveness of EPD-based evaluation in 11 CT and MRI OOD detection challenges.
    
[^85]: AudioFormer: 通过离散的声学代码学习音频特征表示的音频变换器

    AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2308.07221](http://arxiv.org/abs/2308.07221)

    AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。

    

    我们提出了一种名为AudioFormer的方法，通过获取离散的声学代码来学习音频特征表示，并随后对其进行微调以用于音频分类任务。我们首先将音频分类任务视为一种自然语言理解 (NLU) 的形式，借助现有的神经音频编解码模型，我们生成了离散的声学代码，并利用它们来训练一个掩码语言模型 (MLM)，从而获得音频特征表示。此外，我们首创了一种多正样本对比 (MPC) 学习方法的整合，该方法能够学习同一音频输入中多个离散声学代码间的联合表示。在实验中，我们将离散的声学代码视为文本数据，并使用类似填空题的方法训练一个掩码语言模型，最终得到高质量的音频表示。值得注意的是，MPC学习技术能够有效捕捉到音频中的相关性。

    We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
    
[^86]: 一种适用于跟踪演变模式的时空张量分解方法

    A Time-aware tensor decomposition for tracking evolving patterns. (arXiv:2308.07126v1 [cs.LG])

    [http://arxiv.org/abs/2308.07126](http://arxiv.org/abs/2308.07126)

    提出了一种适用于跟踪演变模式的时空张量分解方法tPARAFAC2，通过时间正则化器从时间数据中提取逐渐演变的模式。

    

    时间演变的数据集通常可以组织成一个高阶张量，其中的一个模式是时间模式。虽然张量分解已经成功地用于捕捉这类高阶数据集中的潜在模式，但往往忽略了时间的因素，允许时间点的重新排序。在最近的研究中，引入了时间正则化器来解决这个问题。然而，现有方法仍然不允许潜在模式在时间上发生变化（例如，大脑中的空间变化，主题中的上下文变化）。本文中，我们提出了一种基于PARAFAC2的时空张量分解方法tPARAFAC2，通过时间正则化器从时间数据中提取逐渐演变的模式。通过对合成数据的大量实验，我们证明了tPARAFAC2能够准确地捕捉到演变中的潜在模式，表现优于PARAFAC2和带有时间平滑正则化的耦合矩阵分解方法。

    Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regulariza
    
[^87]: #InsTag:针对大型语言模型监督微调的指令标注分析

    #InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models. (arXiv:2308.07074v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07074](http://arxiv.org/abs/2308.07074)

    本研究提出了InsTag，一种用于标记基于语义和意图的监督微调（SFT）数据集样本的开放式细粒度标注器。通过分析开源SFT数据集，发现模型能力会随着更多多样化和复杂化的数据而增长。基于这一观察结果，使用InsTag选择的数据进行模型微调，得到的TagLM模型在大规模SFT数据上优于开源模型，验证了查询多样性和复杂性的重要性。

    

    基于监督微调（SFT），基础语言模型获得了遵循指令的能力。多样性和复杂性被认为是成功的SFT数据集的重要因素，但其定义仍然模糊不清，缺乏定量分析。在这项工作中，我们提出了InsTag，一种开放的细粒度标注器，根据语义和意图对SFT数据集中的样本进行标记，并且通过标签来定义指令的多样性和复杂性。我们获得了6.6K个标签来描述综合用户查询。然后，我们分析了一些流行的开源SFT数据集，并发现模型的能力随着更多多样化和复杂化的数据而增长。基于这一观察结果，我们提出了一个基于InsTag的数据选择器，从开源数据集中选择6K个多样性和复杂性样本，并在InsTag选择的数据上进行模型微调。结果表明，TagLM模型在MT-Bench评估的大规模SFT数据上优于开源模型，验证了查询多样性和复杂性的重要性。

    Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and compl
    
[^88]: SAILOR: 基于结构增强的尾节点表示学习

    SAILOR: Structural Augmentation Based Tail Node Representation Learning. (arXiv:2308.06801v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06801](http://arxiv.org/abs/2308.06801)

    SAILOR是一种基于结构增强的尾节点表示学习框架，针对真实场景中长尾分布的图节点度数，通过学习增强图结构和提取更有信息的尾节点表示，解决了GNN在尾节点表示上的性能退化问题。

    

    近期，图神经网络（GNN）在图表示学习方面取得了最先进的性能。然而，GNN的有效性主要依赖于拓扑结构的质量，而大多数真实场景中的图节点度数呈现长尾分布，即图中大部分节点都是只有少数连接边的尾节点。由于缺乏结构信息，GNN对尾节点产生较差的节点表示。为了提升GNN对尾节点的表达能力，我们研究了结构信息不足如何恶化尾节点的性能，并提出了一种名为SAILOR的通用结构增强的尾节点表示学习框架，该框架可以同时学习增强图结构和提取更多有信息的尾节点表示。

    Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on pu
    
[^89]: 具有公共数据的私有分布学习：基于样本压缩的视角

    Private Distribution Learning with Public Data: The View from Sample Compression. (arXiv:2308.06239v1 [cs.LG])

    [http://arxiv.org/abs/2308.06239](http://arxiv.org/abs/2308.06239)

    本论文研究了在具有公共数据的情况下的私有分布学习问题，通过压缩样本和列表学习的方式，我们对高斯分布以及高斯混合分布进行了学习上限的分析，并提出了对不可知学习和分布变化抵抗学习的新结果。

    

    我们研究了在可以访问公共数据的情况下的私有分布学习问题。在这个设置中，我们称之为公私学习，学习器被给予来自未知分布p的属于类$\mathcal Q$的公共样本和私有样本，目标是输出一个对p的估计，同时遵守与私有样本相关的隐私约束（这里是纯差分隐私）。我们展示了类$\mathcal Q$的公私可学习性与$\mathcal Q$的样本压缩方案以及中间概念——列表学习的存在性有关。利用这个联系：（1）近似恢复了关于$\mathbb R^d$上高斯分布的先前结果；（2）得出了新的结果，包括对任意$k$-高斯混合分布在$\mathbb R^d$上的样本复杂度上界，以及对不可知和分布变化抵抗学习器的结果，以及公私可学习性的闭包性质。

    We study the problem of private distribution learning with access to public data. In this setup, which we refer to as public-private learning, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.  We show that the public-private learnability of a class $\mathcal Q$ is connected to the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we refer to as list learning. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability
    
[^90]: 无法测量混淆因素下因果推断中的扩散模型

    Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])

    [http://arxiv.org/abs/2308.03669](http://arxiv.org/abs/2308.03669)

    本研究扩展了扩散模型的使用，提出了一种新的模型BDCM，可以在存在无法测量的混淆因素的情况下更准确地回答因果问题。

    

    我们研究了如何在无法测量的混淆因素存在的情况下，扩展扩散模型的使用，以从观测数据中回答因果问题。在Pearl的使用有向无环图（DAG）捕捉因果干预的框架中，提出了一种基于扩散模型的因果模型（DCM），可以更准确地回答因果问题，假设所有混淆因素都是可以观察到的。然而，实际中存在无法测量的混淆因素，这使得DCM无法应用。为了缓解DCM的这一局限性，我们提出了一个扩展模型，称为基于反门准则的DCM（BDCM），其思想根植于在DAG中找到要包括在扩散模型解码过程中的变量的反门准则，这样我们可以将DCM扩展到存在无法测量的混淆因素的情况。合成数据实验表明，我们提出的模型在无法测量混淆因素的情况下更精确地捕捉到了反事实分布。

    We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
    
[^91]: SynJax: JAX的结构化概率分布

    SynJax: Structured Probability Distributions for JAX. (arXiv:2308.03291v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03291](http://arxiv.org/abs/2308.03291)

    SynJax是一个针对JAX的结构化概率分布库，通过提供高效的向量化实现解决了对于结构化对象的难以实现的问题。

    

    深度学习软件库的发展使得该领域取得了显著的进展，它使用户能够专注于建模，同时让库来处理针对现代硬件加速器进行优化执行的繁琐和耗时的任务。然而，这仅对特定类型的深度学习模型有益，例如Transformer，其基本操作易于映射到向量化计算。而对于显式考虑结构化对象（如树和分割）的模型，并没有同样的受益，因为它们需要定制的难以以向量化形式实现的算法。SynJax通过提供用于结构化分布的推理算法的高效向量化实现来直接解决这个问题，包括对齐、标记、分割、组成树和生成树的处理。使用SynJax，我们可以构建大规模的可微分模型，显式地对数据的结构进行建模。代码可在https://g中获得。

    The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.  SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://g
    
[^92]: 无源域自适应的人体姿势估计

    Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.03202](http://arxiv.org/abs/2308.03202)

    提出了无源域自适应的人体姿势估计任务，旨在解决在适应过程中无法访问源数据的跨域学习挑战。通过提出的新框架，源保护模块更有效地保留源信息并抵抗噪声。

    

    人体姿势估计广泛应用于运动分析、医疗保健和虚拟现实等领域。然而，标注实际场景的数据集的巨大开销对姿势估计构成了重要挑战。为了解决这个问题，一种方法是在合成数据集上训练姿势估计模型，然后在真实世界数据上进行域自适应(DA)。然而，现有的HPE的DA方法在适应过程中忽略了数据隐私和安全问题，因为使用了源数据和目标数据。为此，我们提出了一种新的任务，名为无源域自适应的HPE，旨在解决在适应过程中无法访问源数据的HPE的跨域学习挑战。我们进一步提出了一个由三个模型组成的新框架：源模型、中间模型和目标模型，从源数据和目标数据的角度探索该任务。源保护模块更有效地保留源信息并抵抗噪声。

    Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
    
[^93]: MiAMix: 通过多阶段增强混合样本数据增强方法提升图像分类

    MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixied Sample Data Augmentation Method. (arXiv:2308.02804v1 [cs.CV])

    [http://arxiv.org/abs/2308.02804](http://arxiv.org/abs/2308.02804)

    MiAMix是一种新型的混合样本数据增强方法，通过将图像增强集成到混合框架中并利用多种多样的混合方法来提升图像分类模型的性能和泛化能力。

    

    尽管深度学习领域取得了相当大的进展，但过拟合依然是一个关键的挑战。数据增强作为一种十分有前景的方法，因其能够增强模型在各种计算机视觉任务中的泛化能力而备受关注。虽然已经提出了各种各样的策略，但混合样本数据增强（MSDA）在增强模型性能和泛化能力方面显示出了巨大潜力。我们引入了一种称为MiAMix的新型混合方法，即多阶段增强混合。MiAMix将图像增强集成到混合框架中，同时利用多种多样的混合方法，并通过随机选择混合掩模增强方法来改进混合方法。最近的方法利用了显著性信息，而MiAMix的设计也考虑到了计算效率，减少了额外的开销，并且可以轻松集成到现有的训练流程中。我们对MiAMix进行了全面的评估，使用了四个图像基准和进行了比较。

    Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pit
    
[^94]: Tirtha -- 一个自动化平台，用于众包图像并创建历史遗址的三维模型

    Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites. (arXiv:2308.01246v1 [cs.CV])

    [http://arxiv.org/abs/2308.01246](http://arxiv.org/abs/2308.01246)

    Tirtha是一个自动化平台，用于众包文化遗址的图像并创建它们的三维模型。这个平台利用了最先进的结构运动和多视图立体技术，具有模块化、可扩展和成本效益的特点，在资源有限的发展中国家具有重要意义。

    

    数字化保存文化遗产（CH）遗址对于保护它们免受自然灾害或人类活动的损害至关重要。创建文化遗产遗址的三维模型已经成为数字化保存的流行方法，得益于计算机视觉和摄影测量的进展。然而，这个过程耗时、昂贵，并且通常需要专门的设备和专业知识，在资源有限的发展中国家存在挑战。此外，缺乏一个开放的三维模型存储库阻碍了对遗产的研究和公众参与。为了解决这些问题，我们提出了Tirtha，一个用于众包文化遗址图像和三维模型创建的网络平台。Tirtha利用了最先进的结构运动（SfM）和多视图立体（MVS）技术。它是模块化、可扩展和具有成本效益的，可以随着摄影测量的进展而引入新技术。Tirtha可以通过https://tirtha.niser.ac.in作为网页界面访问。

    Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in a
    
[^95]: 不断适应陆地影像中逐渐恶化的天气条件的论文

    Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather. (arXiv:2308.00924v1 [cs.CV])

    [http://arxiv.org/abs/2308.00924](http://arxiv.org/abs/2308.00924)

    本文研究了在空中图像上进行连续适应的问题，特别关注了模型在逐渐恶化的天气条件下的适应能力。在实验中，作者合成了多个数据集模拟逐渐恶化的天气条件，并评估了不同的领域适应模型的性能。研究结果表明，不断适应的约束对模型的性能有重要影响。

    

    领域适应旨在减少模型在训练数据（源域）与使用数据（目标域）之间的领域差异。当一个深度学习模型部署在空中平台上时，它在运行过程中可能会遇到逐渐恶化的天气条件，导致训练数据与实际评估数据之间的领域差距扩大。本文在两个现有的空中图像数据集上从实际图像中合成了两种逐渐恶化的天气条件，生成了四个基准数据集。在连续或测试时间适应的设置下，我们评估了三种领域适应模型：一个基线标准领域适应模型和两个连续领域适应模型。在这种设置下，模型每次只能访问一小部分或一个批次的目标数据，并且适应只发生在一次数据迭代中。不断适应的约束和逐渐恶化的天气条件的结合，对模型的性能有重要影响。

    Domain adaptation (DA) strives to mitigate the domain gap between the source domain where a model is trained, and the target domain where the model is deployed. When a deep learning model is deployed on an aerial platform, it may face gradually degrading weather conditions during operation, leading to widening domain gaps between the training data and the encountered evaluation data. We synthesize two such gradually worsening weather conditions on real images from two existing aerial imagery datasets, generating a total of four benchmark datasets. Under the continual, or test-time adaptation setting, we evaluate three DA models on our datasets: a baseline standard DA model and two continual DA models. In such setting, the models can access only one small portion, or one batch of the target data at a time, and adaptation takes place continually, and over only one epoch of the data. The combination of the constraints of continual adaptation, and gradually deteriorating weather conditions
    
[^96]: PromptStyler：基于提示的无源域泛化风格生成

    PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])

    [http://arxiv.org/abs/2307.15199](http://arxiv.org/abs/2307.15199)

    提出了PromptStyler，通过使用提示合成样式特征，解决了无源域泛化的问题。该方法通过学习样式词向量生成多样的样式，并通过强制样式内容特征与内容特征靠近来保证样式不会扭曲内容信息。在多个数据集上取得了最先进的结果。

    

    在联合视觉语言空间中，文本特征（如“一张狗的照片”）可以有效地表示其相关的图像特征（如狗的照片）。受此启发，我们提出了PromptStyler，通过使用提示来合成各种样式，而不使用任何图像来处理无源域泛化中的分布偏移。我们的方法通过可学习的样式词向量为伪词S*生成多样的样式特征（如“a S* style of a”）。为了确保学习到的样式不会扭曲内容信息，我们强制要求样式内容特征（如“a S* style of a [class]”）在联合视觉语言空间中靠近其对应的内容特征（如“[class]”）。在学习样式词向量之后，我们使用合成的样式内容特征训练一个线性分类器。尽管PromptStyler不需要使用任何图像，并且需要额外的训练，但在PACS、VLCS、OfficeHome和DomainNet上取得了最先进的结果。

    In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Inspired by this, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. Our method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and take
    
[^97]: 深度布拉德利-特里评分：在没有具体评价标准的情况下估计物品的属性

    Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])

    [http://arxiv.org/abs/2307.13709](http://arxiv.org/abs/2307.13709)

    本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。

    

    在现实世界中，许多属性，如竞争环境中的可取性或强度，无法直接观测，这使得它们难以评估。为了解决这个具有挑战性的问题，先前的研究主要集中在估计已知物品的这些属性，特别是出现在配对比较数据集中的运动员的实力。在本文中，我们介绍了深度布拉德利-特里评分（DBTR），这是一个新颖的机器学习框架，用于评估不一定存在于数据集中的未知物品的任何属性。我们的方法无缝地将传统的布拉德利-特里模型与神经网络结构相结合。我们还进一步推广了这个架构，用于具有不公平性的非对称环境，这在现实世界中更为常见。在我们的实验分析中，DBTR成功地学习了这些属性的预期量化。

    Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
    
[^98]: 自动化纤维成型中的异常检测：数据有限的学习

    Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])

    [http://arxiv.org/abs/2307.07893](http://arxiv.org/abs/2307.07893)

    本文提出了一种在数据有限的情况下，通过自动编码器进行异常检测的方法，利用纤维层片的深度图进行二分类，并使用重构误差作为量化指标。

    

    当前自动化纤维成型(AFP)的缺陷检测系统主要基于端到端的监督学习方法，需要大量标记的有缺陷样本，而这些样本很难生成足够数量。为了解决这个数据稀缺的问题，我们引入了一种基于自动编码器的方法，适用于小型数据集。幸运的是，从基础的角度来看，这个问题可以简化为正常样本和异常样本之间的二分类。所提出的方法使用纤维层片（tow）对纤维铺设表面的深度图进行分割成小窗口。其中不包含异常的窗口子集传递给自动编码器来重构输入。因为自动编码器是用正常样本进行训练的，对于这些样本，它产生的重构比对于异常样本更精确。因此，重构误差的值被用作一个量化指标，用于判断是否存在潜在的异常。

    Current defect detection systems for Automated Fibre Placement (AFP) are mostly based on end-to-end supervised learning methods requiring abundant labelled defective samples, which are not easily generated in sufficient numbers. To address this data scarcity problem, we introduce an autoencoder-based approach compatible with small datasets. Fortunately, the problem from a foundational point of view can be simplified as a binary classification between normal and abnormal samples. The proposed approach uses a depth map of the fibre layup surface, split into small windows aligned to each composite strip (tow). A subset of these windows that do not contain anomalies is passed to an autoencoder to reconstruct the input. Because the autoencoder is trained with normal samples, it produces more accurate reconstructions for these samples than for abnormal ones. Therefore, the value of reconstruction error is used as a quantitative metric for whether there are potential anomalies. These values a
    
[^99]: 以不同方式堆叠更多层：通过低秩更新进行高秩训练

    Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])

    [http://arxiv.org/abs/2307.05695](http://arxiv.org/abs/2307.05695)

    本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。

    

    尽管大规模网络拥有数百亿个参数的规模已经占主导地位并且效果显著，但对于过度参数化模型的训练必要性仍然缺乏清晰的理解，而替代方法不一定能够降低训练高性能模型的成本。本文探索了低秩训练技术作为训练大型神经网络的替代方法。我们引入了一种称为ReLoRA的新方法，它利用低秩更新来训练高秩网络。我们将ReLoRA应用于预训练的Transformer语言模型，参数量高达350M，并且证明了与常规神经网络训练相当的性能。此外，我们观察到ReLoRA的效率随着模型大小的增加而提高，这使得它成为高效训练千亿级参数网络的有希望的方法。我们的研究结果揭示了低秩训练技术的潜力及其对于缩放定律的影响。

    Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
    
[^100]: 评估社交机器人导航算法的原则与指南

    Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])

    [http://arxiv.org/abs/2306.16740](http://arxiv.org/abs/2306.16740)

    本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。

    

    在人类居住环境中导航是部署机器人广泛应用的主要挑战，通常被称为社交机器人导航。虽然社交导航领域近年来取得了很大进展，但评估解决社交导航的算法仍然困难，因为它不仅涉及机器人在静态环境中移动，还涉及到动态的人类参与者及其对机器人行为的感知适应性。相比之下，清晰、可重复、易于获得的基准在计算机视觉、自然语言处理和传统机器人导航等领域加速了进展，使研究人员能够公平比较算法，揭示现有解决方案的局限性，并呈现有前途的新方向。我们相信相同的方法可以有助于社交导航。在本文中，我们为评估社交机器人导航建立了共同、广泛可用且可重复的基准标准，并提出了自己的创新点。

    A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
    
[^101]: 推荐系统的数据增广：一种基于最大边际矩阵分解的半监督方法

    Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization. (arXiv:2306.13050v1 [cs.IR])

    [http://arxiv.org/abs/2306.13050](http://arxiv.org/abs/2306.13050)

    本研究提出了一种基于最大边际矩阵分解的半监督方法来增广和细化协同过滤算法的评级预测。该方法利用自我训练来评估评分的置信度，并通过系统的数据增广策略来提高算法性能。

    

    协同过滤已成为推荐系统开发的常用方法，其中，根据用户的过去喜好和其他用户的可用偏好信息预测其对新物品的评分。尽管CF方法很受欢迎，但其性能通常受观察到的条目的稀疏性的极大限制。本研究探讨最大边际矩阵分解（MMMF）的数据增广和细化方面，该方法是广泛接受的用于评级预测的CF技术，之前尚未进行研究。我们利用CF算法的固有特性来评估单个评分的置信度，并提出了一种基于自我训练的半监督评级增强方法。我们假设任何CF算法的预测低置信度是由于训练数据的某些不足，因此，通过采用系统的数据增广策略，可以提高算法的性能。

    Collaborative filtering (CF) has become a popular method for developing recommender systems (RS) where ratings of a user for new items is predicted based on her past preferences and available preference information of other users. Despite the popularity of CF-based methods, their performance is often greatly limited by the sparsity of observed entries. In this study, we explore the data augmentation and refinement aspects of Maximum Margin Matrix Factorization (MMMF), a widely accepted CF technique for the rating predictions, which have not been investigated before. We exploit the inherent characteristics of CF algorithms to assess the confidence level of individual ratings and propose a semi-supervised approach for rating augmentation based on self-training. We hypothesize that any CF algorithm's predictions with low confidence are due to some deficiency in the training data and hence, the performance of the algorithm can be improved by adopting a systematic data augmentation strategy
    
[^102]: 基于任务条件化超网络的多任务记忆深度强化学习

    Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10698](http://arxiv.org/abs/2306.10698)

    人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。

    

    深度强化学习算法通常受到采样效率低下的限制，严重依赖与环境的多次交互才能获得准确的决策能力。相比之下，人类似乎依赖海马体从过去有关任务的经历中检索相关信息，在学习新任务时指导其决策，而不是仅仅依赖于环境交互。然而，为代理设计类似海马体的模块以将过去的经历融入既有的强化学习算法面临两个挑战。第一个挑战涉及选择当前任务最相关的过去经验，第二个是将这些经验与决策网络相结合。为了解决这些问题，我们提出了一种新算法，利用基于任务条件化超网络的检索网络，根据任务调整检索网络的参数。

    Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
    
[^103]: 虚拟人类生成模型：基于掩码建模的方法来学习人类特征

    Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics. (arXiv:2306.10656v1 [cs.LG])

    [http://arxiv.org/abs/2306.10656](http://arxiv.org/abs/2306.10656)

    本论文提出了一种名为VHGM的深度生成模型，基于掩码建模的方法来学习健康属性、生活方式和人格之间的关系。通过使用异构表格数据集，VHGM有效地学习了超过1,800个属性。该模型具有潜在的应用前景，例如用于医疗属性的虚拟测量和生活方式的假设验证。

    

    识别医疗属性、生活方式和人格之间的关系对于理解和改善身体和精神状况至关重要。本文提出了一种名为虚拟人类生成模型（VHGM）的机器学习模型，用于估计有关医疗保健、生活方式和个性的属性。VHGM是一个深度生成模型，使用掩码建模训练，在已知属性的条件下学习属性的联合分布。利用异构表格数据集，VHGM高效地学习了超过1,800个属性。我们数值评估了VHGM及其训练技术的性能。作为VHGM的概念验证，我们提出了几个应用程序，演示了用户情境，例如医疗属性的虚拟测量和生活方式的假设验证。

    Identifying the relationship between healthcare attributes, lifestyles, and personality is vital for understanding and improving physical and mental conditions. Machine learning approaches are promising for modeling their relationships and offering actionable suggestions. In this paper, we propose Virtual Human Generative Model (VHGM), a machine learning model for estimating attributes about healthcare, lifestyles, and personalities. VHGM is a deep generative model trained with masked modeling to learn the joint distribution of attributes conditioned on known ones. Using heterogeneous tabular datasets, VHGM learns more than 1,800 attributes efficiently. We numerically evaluate the performance of VHGM and its training techniques. As a proof-of-concept of VHGM, we present several applications demonstrating user scenarios, such as virtual measurements of healthcare attributes and hypothesis verifications of lifestyles.
    
[^104]: GCformer:一种用于准确和可扩展的长期多元时间序列预测的高效框架

    GCformer: An Efficient Framework for Accurate and Scalable Long-Term Multivariate Time Series Forecasting. (arXiv:2306.08325v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08325](http://arxiv.org/abs/2306.08325)

    GCformer是一个结合了全局卷积和局部Transformer分支的框架，旨在解决长期多元时间序列预测的精确性和可扩展性问题。通过引入具有亚线性复杂度的结构化卷积核，GCformer在各种基准数据集上实现了优于其他方法的性能。

    

    基于Transformer的模型已经成为时间序列预测的有希望的工具。然而，这些模型对于长输入时间序列的预测不够准确。一方面，它们未能捕捉时间序列数据中的全局依赖关系。另一方面，长输入序列通常导致模型尺寸大和时间复杂度高。为了解决这些限制，我们提出了GCformer，它将用于处理长输入序列的结构化全局卷积分支与用于捕捉短期近期信号的局部Transformer分支相结合。引入了一个全局卷积核的连贯框架，利用了三种不同的参数化方法。全局分支中选择的结构化卷积核经过特殊设计，具有亚线性复杂度，从而可以高效和有效地处理长而嘈杂的输入信号。对六个基准数据集进行的实证研究表明，GCformer的性能优于其他方法。

    Transformer-based models have emerged as promising tools for time series forecasting.  However, these model cannot make accurate prediction for long input time series. On the one hand, they failed to capture global dependencies within time series data. On the other hand, the long input sequence usually leads to large model size and high time complexity.  To address these limitations, we present GCformer, which combines a structured global convolutional branch for processing long input sequences with a local Transformer-based branch for capturing short, recent signals. A cohesive framework for a global convolution kernel has been introduced, utilizing three distinct parameterization methods. The selected structured convolutional kernel in the global branch has been specifically crafted with sublinear complexity, thereby allowing for the efficient and effective processing of lengthy and noisy input signals. Empirical studies on six benchmark datasets demonstrate that GCformer outperforms
    
[^105]: 基于时间图序列预测的时态图结构学习

    Time-aware Graph Structure Learning via Sequence Prediction on Temporal Graphs. (arXiv:2306.07699v1 [cs.LG])

    [http://arxiv.org/abs/2306.07699](http://arxiv.org/abs/2306.07699)

    该论文提出了一种基于时间图序列预测的时态图结构学习方法，通过添加潜在的时间边来学习更好的图像结构，提高下游任务的性能。

    

    近年来，旨在模拟图像的传递性质的时间图学习变得越来越受关注并取得了显著的性能。然而，实际上，图像结构往往是不完整和嘈杂的，这阻碍了时间图网络（TGN）学习信息丰富的表示。为了解决这些问题，我们提出了一种基于时间图序列预测的时态图结构学习（TGSL）方法，通过添加潜在的时间边学习更好的图像结构，提高了下游任务的性能。

    Temporal Graph Learning, which aims to model the time-evolving nature of graphs, has gained increasing attention and achieved remarkable performance recently. However, in reality, graph structures are often incomplete and noisy, which hinders temporal graph networks (TGNs) from learning informative representations. Graph contrastive learning uses data augmentation to generate plausible variations of existing data and learn robust representations. However, rule-based augmentation approaches may be suboptimal as they lack learnability and fail to leverage rich information from downstream tasks. To address these issues, we propose a Time-aware Graph Structure Learning (TGSL) approach via sequence prediction on temporal graphs, which learns better graph structures for downstream tasks through adding potential temporal edges. In particular, it predicts time-aware context embedding based on previously observed interactions and uses the Gumble-Top-K to select the closest candidate edges to th
    
[^106]: 具有不对称置信区间的有限预算多臂老虎机问题

    Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals. (arXiv:2306.07071v1 [cs.LG])

    [http://arxiv.org/abs/2306.07071](http://arxiv.org/abs/2306.07071)

    这篇论文提出了一种名为ω-UCB的新上置信区间抽样策略，使用不对称置信区间以更准确、更紧密地估计奖励成本比，解决了现有预算多臂老虎机问题策略存在的问题，并在合成和真实环境中表现出色。

    

    我们研究了随机预算多臂老虎机（MAB）问题，其中玩家选择具有未知期望奖励和成本的K个臂。目标是在预算约束下最大化总奖励。因此，玩家试图尽可能经常地选择具有最高奖励成本比的臂。当前针对此问题的最先进策略存在一些问题，我们予以说明。为了克服这些问题，我们提出了一种新的上置信区间（UCB）抽样策略，称为ω-UCB，并使用不对称置信区间。这些区间尺度随着样本均值和随机变量边界之间的距离而变化，相对于我们的竞争对手，可以更准确、更紧密地估计奖励成本比。我们证明了我们的方法具有对数后悔，并在合成和真实环境中始终优于现有策略。

    We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from $K$ arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current state-of-the-art policies for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, $\omega$-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has logarithmic regret and consistently outperforms existing policies in synthetic and real settings.
    
[^107]: 离线强化学习中具有数据集约束的策略正则化

    Policy Regularization with Dataset Constraint for Offline Reinforcement Learning. (arXiv:2306.06569v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06569](http://arxiv.org/abs/2306.06569)

    本文提出了一种离线强化学习方法，称为PRDC，它使用数据集约束来正则化学习策略。相比于现有方法，PRDC可以更有效地指导策略的更新，并提高其性能。

    

    本文考虑从固定数据集中学习最佳策略的问题，即离线强化学习。现有的离线强化学习方法通常采用策略正则化来约束学习策略的分布或行为策略的支持。然而，分布和支持约束过于保守，因为它们都要求学习策略在特定状态下选择与行为策略相似的动作。这将限制学习策略的性能，特别是当行为策略是次优的情况下。本文发现，将策略正则化指向最近的状态-动作对可能更加有效，因此提出了具有数据集约束的策略正则化（PRDC）。在给定状态下更新策略时，PRDC会在整个数据集中搜索最接近的状态-动作样本，然后用该样本的动作来约束策略。与现有方法不同，PRDC可以指导策略根据最相关的状态-动作来进行更新，从而提高策略的性能。

    We consider the problem of learning the best possible policy from a fixed dataset, known as offline Reinforcement Learning (RL). A common taxonomy of existing offline RL works is policy regularization, which typically constrains the learned policy by distribution or support of the behavior policy. However, distribution and support constraints are overly conservative since they both force the policy to choose similar actions as the behavior policy when considering particular states. It will limit the learned policy's performance, especially when the behavior policy is sub-optimal. In this paper, we find that regularizing the policy towards the nearest state-action pair can be more effective and thus propose Policy Regularization with Dataset Constraint (PRDC). When updating the policy in a given state, PRDC searches the entire dataset for the nearest state-action sample and then restricts the policy with the action of this sample. Unlike previous works, PRDC can guide the policy with pr
    
[^108]: 小量子态的可解释表示学习

    Explainable Representation Learning of Small Quantum States. (arXiv:2306.05694v1 [quant-ph])

    [http://arxiv.org/abs/2306.05694](http://arxiv.org/abs/2306.05694)

    该论文探讨了一种无监督机器学习模型在小量子态上的可解释表示学习方法，得到了一个与潜在结构相联系的可理解表示。

    

    无监督的机器学习模型能够在没有明确人类指导或特征工程的情况下建立起对训练数据的内部表示。通过这种学习到的表示，我们可以进一步了解到关于处理任务所需的数据特征信息。在量子物理学的背景下，训练模型以描述量子态又未经人类干预地学习信息是获得深入了解机器如何呈现复杂量子态的重要途径。解释学习到的表示将为非平凡的量子系统及其有效表示带来新的视角。我们针对由参数化量子电路生成的两量子比特密度矩阵训练了一个生成模型。通过一系列计算实验，我们调查了该模型所学习到的表示以及其内部对数据信息的理解。我们观察到模型学习了一种可解释的表示方式，并将量子态与其潜在结构相联系。

    Unsupervised machine learning models build an internal representation of their training data without the need for explicit human guidance or feature engineering. This learned representation provides insights into which features of the data are relevant for the task at hand. In the context of quantum physics, training models to describe quantum states without human intervention offers a promising approach to gaining insight into how machines represent complex quantum states. The ability to interpret the learned representation may offer a new perspective on non-trivial features of quantum systems and their efficient representation. We train a generative model on two-qubit density matrices generated by a parameterized quantum circuit. In a series of computational experiments, we investigate the learned representation of the model and its internal understanding of the data. We observe that the model learns an interpretable representation which relates the quantum states to their underlying
    
[^109]: 基于感知器神经网络的生物灵感混沌传感器模型：机器学习概念与计算神经科学应用

    A Bio-Inspired Chaos Sensor Model Based on the Perceptron Neural Network: Machine Learning Concept and Application for Computational Neuro-Science. (arXiv:2306.01991v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.01991](http://arxiv.org/abs/2306.01991)

    该论文介绍了一种基于感知器神经网络的生物灵感混沌传感器模型，用于估计神经动力学系统中脉冲序列的熵。经过训练，该模型能够以高精度近似一个短时间序列的模糊熵，即使隐藏层只有一个神经元，也能够达到良好的结果。

    

    本研究提出了一种基于感知器神经网络的生物灵感混沌传感器模型，用于估计神经动力学系统中脉冲序列的熵。经过训练，感知器模型，隐藏层有50个神经元，输出层有1个神经元，能够以高精度近似一个短时间序列的模糊熵，决定系数R2约为0.9。使用Hindmarsh-Rose脉冲模型生成脉冲间隔的时间序列，并使用这些数据集来训练和测试感知器。使用K-block交叉验证方法选择感知器模型的超参数和估计传感器的准确性。即使隐藏层只有一个神经元，该模型也能够以良好的结果近似模糊熵，并且决定系数R2约为0.5-0.8。在一个简化的模型中，隐藏层只有一个神经元，并且第一层的权重相等，近似的原理基于时间序列的平均值的线性变换。

    The study presents a bio-inspired chaos sensor model based on the perceptron neural network for the estimation of entropy of spike train in neurodynamic systems. After training, the sensor on perceptron, having 50 neurons in the hidden layer and 1 neuron at the output, approximates the fuzzy entropy of a short time series with high accuracy, with a determination coefficient of R2 ~ 0.9. The Hindmarsh-Rose spike model was used to generate time series of spike intervals, and datasets for training and testing the perceptron. The selection of the hyperparameters of the perceptron model and the estimation of the sensor accuracy were performed using the K-block cross-validation method. Even for a hidden layer with one neuron, the model approximates the fuzzy entropy with good results and the metric R2 ~ 0.5-0.8. In a simplified model with one neuron and equal weights in the first layer, the principle of approximation is based on the linear transformation of the average value of the time seri
    
[^110]: 通过潜在量化进行解缠

    Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])

    [http://arxiv.org/abs/2305.18378](http://arxiv.org/abs/2305.18378)

    本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。

    

    在解缠表示学习中，模型需要将数据集的基础变化因素分开并独立地表示出来，而模型并没有提供有关这些因素的真实信息，归纳偏见在实现解缠方面发挥着重要作用。在本文中，我们通过施加严格的交流瓶颈和强大的模型规范化，构建了一种朝着组合编码和解码数据的归纳偏见。具体来说，我们对潜在维度进行可学习的离散编码，并为每个维度应用一个单独的标量码书。潜在量化迫使编码器在许多数据点上使用少量潜在值，从而使解码器能够为每个值分配一致的含义。规范化有助于将模型引向这种简明策略。我们在多个基准数据集上展示了该方法的广泛应用性，并且展示了我们的方法显著提高了一系列标准VAE模型学习的表征的可解释性。

    In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
    
[^111]: 一种从学术论文中提炼文本分类和对象识别的框架

    A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.17401](http://arxiv.org/abs/2305.17401)

    本文提出了一种结合基于规则的方法和机器学习的框架，旨在解决从学术论文中提炼文本分类和对象识别的问题。

    

    随着互联网的广泛使用，高效地从大量学术论文中提取特定信息变得越来越重要。数据挖掘技术通常用于解决这个问题。然而，挖掘学术论文的数据具有挑战性，因为它需要自动从复杂的非结构化布局文档中提取特定模式。当前的学术论文数据挖掘方法使用基于规则的（RB）或机器学习（ML）方法。然而，使用基于规则的方法需要编写复杂排版论文的高昂成本。另一方面，仅使用机器学习方法需要对文章中复杂内容类型进行注释工作，这可能成本高昂。此外，仅使用机器学习可能会导致基于规则的方法容易识别的模式被错误提取的情况。为了解决这些问题，本文从分析指定著作中使用的标准布局和排版角度出发，提出了一种结合基于规则的方法和机器学习的框架。

    With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
    
[^112]: 来自噪声的回音：基于扩散模型的合成超声图像生成用于实际图像分割

    Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation. (arXiv:2305.05424v1 [eess.IV])

    [http://arxiv.org/abs/2305.05424](http://arxiv.org/abs/2305.05424)

    该论文提出了一种基于DDPM的方法来生成合成超声图像，用于医学图像分析任务中的有效替代品。在左心室和左房分割任务中，使用仅合成图像训练的神经网络相对于之前最先进方法提高了9.09％、3.7％和15.0％的Dice分数。

    

    我们提出了一种新的方法，通过使用基于去噪扩散概率模型（DDPM）和心脏超声语义标签地图生成合成图像。我们展示出这些合成图像可以作为深度学习模型在医学图像分析任务中的有效替代品，如图像分割。为了证明这种方法的有效性，我们生成了合成的2D心脏超声图像并训练了一个神经网络对左心室和左房进行分割。在仅使用合成图像进行训练的情况下，所训练的神经网络在未见过的真实图像数据集上对左心室内膜、心外膜和左心房的分割分别产生了88.5±6.0％、92.3±3.9％和86.3±10.7％的平均Dice分数。这比先前最先进的方法相比提高了9.09％、3.7％和15.0％。该提议的方法在应用领域具有潜力。

    We propose a novel pipeline for the generation of synthetic images via Denoising Diffusion Probabilistic Models (DDPMs) guided by cardiac ultrasound semantic label maps. We show that these synthetic images can serve as a viable substitute for real data in the training of deep-learning models for medical image analysis tasks such as image segmentation. To demonstrate the effectiveness of this approach, we generated synthetic 2D echocardiography images and trained a neural network for segmentation of the left ventricle and left atrium. The performance of the network trained on exclusively synthetic images was evaluated on an unseen dataset of real images and yielded mean Dice scores of 88.5 $\pm 6.0$ , 92.3 $\pm 3.9$, 86.3 $\pm 10.7$ \% for left ventricular endocardial, epicardial and left atrial segmentation respectively. This represents an increase of $9.09$, $3.7$ and $15.0$ \% in Dice scores compared to the previous state-of-the-art. The proposed pipeline has the potential for applic
    
[^113]: ANTONIO:面向NLP验证的系统化基准生成方法

    ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])

    [http://arxiv.org/abs/2305.04003](http://arxiv.org/abs/2305.04003)

    本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    

    自然语言处理（NLP）中使用的机器学习模型的验证被认为是一个难题。现有的神经网络验证方法常用于计算机视觉和其他数字数据集，但并不适用于NLP。本研究探讨了造成这一问题的技术原因，并在此基础上提出了实用的方法和启发式规则，以便将NLP数据集和模型准备为适合基于抽象解释的已知验证方法。我们将这些方法实现为一个名为ANTONIO的Python库，该库连接到神经网络验证器ERAN和Marabou。我们使用一个名为R-U-A-Robot的NLP数据集对工具进行了评估，该数据集被提议作为验证具有法律重要性的NLP应用的基准。我们希望，由于其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
    
[^114]: 比例代表性聚类

    Proportionally Representative Clustering. (arXiv:2304.13917v1 [cs.LG])

    [http://arxiv.org/abs/2304.13917](http://arxiv.org/abs/2304.13917)

    本文提出了一个新的公平性准则——比例代表性公平性（PRF），并设计了有效的算法满足该准则。

    

    近年来，机器学习领域对公平概念的形式化表述越来越受关注。本文关注于聚类，是无监督机器学习中最基础的任务之一。我们提出了一个新的公平性准则——比例代表性公平性（PRF），我们认为该概念以一种更有说服力的方式达到了文献中几个现存概念的理由。但现有的公平聚类算法不能满足我们的公平性概念。我们设计了高效的算法，以满足无约束聚类和离散聚类问题的PRF。

    In recent years, there has been a surge in effort to formalize notions of fairness in machine learning. We focus on clustering -- one of the fundamental tasks in unsupervised machine learning. We propose a new axiom that captures proportional representation fairness (PRF). We make a case that the concept achieves the raison d'{\^{e}}tre of several existing concepts in the literature in an arguably more convincing manner. Our fairness concept is not satisfied by existing fair clustering algorithms. We design efficient algorithms to achieve PRF both for unconstrained and discrete clustering problems.
    
[^115]: 正则化完整循环一致性GAN用于异常检测

    Regularized Complete Cycle Consistent GAN for Anomaly Detection. (arXiv:2304.07769v1 [cs.LG])

    [http://arxiv.org/abs/2304.07769](http://arxiv.org/abs/2304.07769)

    本研究提出了RCALAD方法，通过循环一致性和新的鉴别器增强了GAN在异常检测中的效果。同时，利用补充分布引导重建和引入新的异常评分进一步提高了模型性能。

    

    本研究提出了一种用于实际应用中异常检测的对抗性方法，通过重建误差中循环一致性利用生成对抗神经网络（GAN）的威力。以往的方法由于类别间精度高度差异而未被应用于所有类型的异常情况。RCALAD是一种旨在通过将新的鉴别器引入到结构中来解决这个问题的方法，从而实现更高效的训练过程。此外，RCALAD在输入空间中使用补充分布，将重建引导到正常数据分布，有效地将异常样本与其重建分离，进而实现更准确的异常检测。为了进一步提高模型的性能，引入了两个新的异常评分。在六个不同数据集的大量实验中对所提出的模型进行了全面评估，得出了展示出其优越性能的结果。

    This study presents an adversarial method for anomaly detection in real-world applications, leveraging the power of generative adversarial neural networks (GANs) through cycle consistency in reconstruction error. Previous methods suffer from the high variance between class-wise accuracy which leads to not being applicable for all types of anomalies. The proposed method named RCALAD tries to solve this problem by introducing a novel discriminator to the structure, which results in a more efficient training process. Additionally, RCALAD employs a supplementary distribution in the input space to steer reconstructions toward the normal data distribution, effectively separating anomalous samples from their reconstructions and facilitating more accurate anomaly detection. To further enhance the performance of the model, two novel anomaly scores are introduced. The proposed model has been thoroughly evaluated through extensive experiments on six various datasets, yielding results that demonst
    
[^116]: 通过不确定性实现公平性

    Fairness through Aleatoric Uncertainty. (arXiv:2304.03646v1 [cs.LG])

    [http://arxiv.org/abs/2304.03646](http://arxiv.org/abs/2304.03646)

    研究不确定性与公平性的关系，通过贝叶斯学习估算样本预测不确定性，发现低不确定性的数据更准确和公平，提出一种基于不确定性量化定义的新的公平性-效用目标。

    

    我们提出了一种独特的解决方案，以解决机器学习分类任务中公平性和效用通常相互竞争的目标。 公平性确保模型的预测不带偏见地针对任何特定群体，而效用则专注于最大化模型预测的准确性。我们的目标是研究不确定性与公平性之间的关系。我们的方法利用贝叶斯学习来估算样本预测的不确定性，其中估算与受保护属性相关的混淆效应无关。通过实证证据，我们表明具有低分类不确定性的样本比具有高不确定性的样本更准确和公平地建模，可能具有偏差的表示和更高的预测误差。为了解决平衡公平性和效用的挑战，我们提出了一种基于不确定性量化定义的新的公平性-效用目标。

    We propose a unique solution to tackle the often-competing goals of fairness and utility in machine learning classification tasks. While fairness ensures that the model's predictions are unbiased and do not discriminate against any particular group, utility focuses on maximizing the accuracy of the model's predictions. Our aim is to investigate the relationship between uncertainty and fairness. Our approach leverages this concept by employing Bayesian learning to estimate the uncertainty in sample predictions where the estimation is independent of confounding effects related to the protected attribute. Through empirical evidence, we show that samples with low classification uncertainty are modeled more accurately and fairly than those with high uncertainty, which may have biased representations and higher prediction errors. To address the challenge of balancing fairness and utility, we propose a novel fairness-utility objective that is defined based on uncertainty quantification. The w
    
[^117]: 通过近似消息传递的混合回归（Mixed Regression via Approximate Message Passing）

    Mixed Regression via Approximate Message Passing. (arXiv:2304.02229v1 [stat.ML])

    [http://arxiv.org/abs/2304.02229](http://arxiv.org/abs/2304.02229)

    本文提出了一种新的近似消息传递算法来解决在广义线性模型中的回归问题，该算法适用于混合线性回归、最大仿射回归和专家混合模型等问题。

    

    本文研究了广义线性模型（GLM）中具有多个信号和潜变量的回归问题。该模型被称为矩阵GLM，涵盖了许多在统计学习中广泛研究的问题，包括混合线性回归、最大仿射回归和专家混合模型等。我们提出了一种新的近似消息传递（AMP）算法来估计矩阵GLM中的信号和潜变量，并在高维极限中对其性能进行了严格的表征。该表征是通过状态演化递归来计算的，从而可以精确计算渐近性能度量，例如信噪比下降阈值（threshold）。

    We study the problem of regression in a generalized linear model (GLM) with multiple signals and latent variables. This model, which we call a matrix GLM, covers many widely studied problems in statistical learning, including mixed linear regression, max-affine regression, and mixture-of-experts. In mixed linear regression, each observation comes from one of $L$ signal vectors (regressors), but we do not know which one; in max-affine regression, each observation comes from the maximum of $L$ affine functions, each defined via a different signal vector. The goal in all these problems is to estimate the signals, and possibly some of the latent variables, from the observations. We propose a novel approximate message passing (AMP) algorithm for estimation in a matrix GLM and rigorously characterize its performance in the high-dimensional limit. This characterization is in terms of a state evolution recursion, which allows us to precisely compute performance measures such as the asymptotic 
    
[^118]: 通过小波神经算子实现Transformers的多尺度注意力机制

    Multiscale Attention via Wavelet Neural Operators for Vision Transformers. (arXiv:2303.12398v1 [cs.CV])

    [http://arxiv.org/abs/2303.12398](http://arxiv.org/abs/2303.12398)

    本文介绍了一种基于小波神经算子的多尺度注意力机制，它通过使用小波神经算子将注意力机制从局部扩展到全域和多尺度范围内，取得了比ViT和AFNO更显著的性能提高。

    

    Transformer在计算机视觉中取得了广泛的成功。其核心是自注意机制（SA），它是一种归纳偏见，通过加权基础将输入中的每个token与每个其他token相关联。标准的SA机制具有二次复杂度，难以处理高分辨率图像中出现的长序列。为此，我们引入了基于小波神经算子的Multiscale Wavelet Attention（MWA），使用小波神经算子将注意力机制从局部扩展到全域和多尺度范围内。我们用CIFAR和ImageNet进行了实验，结果表明，MWA比ViT和AFNO都表现出显著的性能提高。

    Transformers have achieved widespread success in computer vision. At their heart, there is a Self-Attention (SA) mechanism, an inductive bias that associates each token in the input with every other token through a weighted basis. The standard SA mechanism has quadratic complexity with the sequence length, which impedes its utility to long sequences appearing in high resolution vision. Recently, inspired by operator learning for PDEs, Adaptive Fourier Neural Operators (AFNO) were introduced for high resolution attention based on global convolution that is efficiently implemented via FFT. However, the AFNO global filtering cannot well represent small and moderate scale structures that commonly appear in natural images. To leverage the coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention (MWA) by leveraging wavelet neural operators which incurs linear complexity in the sequence size. We replace the attention in ViT with MWA and our experiments with CIFAR and ImageN
    
[^119]: 大规模适应性实验：灵活批处理的贝叶斯算法

    Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches. (arXiv:2303.11582v1 [cs.LG])

    [http://arxiv.org/abs/2303.11582](http://arxiv.org/abs/2303.11582)

    本文提出了一个基于贝叶斯算法的自适应实验框架，可灵活处理任何批处理大小。通过正态近似指导可扩展自适应设计，采用残余时限优化选择采样分配，实现了最先进的性能。

    

    标准的贝叶斯算法假定持续重新分配测量工作，这在实现过程中存在延迟反馈和基础设施/组织难题等挑战。本文针对仅有少数重新分配阶段的实际情况，其中测量结果是以批处理形式测量的，提出了一种新的适应性实验框架，可灵活处理任何批处理大小。我们的主要观察是，在统计推断中普遍使用的正态近似也可以指导可扩展自适应设计。通过推导渐进顺序实验，我们制定了一种动态规划，可以利用平均回报的先验信息。动态规划的状态转移相对于采样分配是可微的，允许使用基于梯度的方法进行规划和策略优化。我们提出了一种简单的迭代规划方法，即残余时限优化，通过优化平衡探索和利用的规划目标来选择采样分配。在合成和真实世界基准测试问题上的实验结果表明，我们的框架实现了最先进的性能，同时具有模块化和易用性。

    Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a new adaptive experimentation framework that can flexibly handle any batch size. Our main observation is that normal approximations universal in statistical inference can also guide the design of scalable adaptive designs. By deriving an asymptotic sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. State transitions of the dynamic program are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and policy optimization. We propose a simple iterative planning method, Residual Horizon Optimization, which selects sampling allocations by optimizing a planning obj
    
[^120]: FUSQA: 胎儿超声分割质量评估

    FUSQA: Fetal Ultrasound Segmentation Quality Assessment. (arXiv:2303.04418v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.04418](http://arxiv.org/abs/2303.04418)

    FUSQA是一个用于胎儿超声分割质量评估的简化模型，通过将分割质量评估过程定为自动化分类任务，实现了更准确的孕龄估计。

    

    深度学习模型在各种胎儿超声分割任务中取得了很好的效果。然而，对于新的未见数据的泛化引发了关于它们在临床应用中的效果的问题。通常，向新的未见数据的转换需要耗时且昂贵的质量保证流程来验证转换后的分割性能。分割质量评估的工作集中在自然图像上，其中问题通常被形式化为一个dice分数回归任务。在本文中，我们提出了一个简化的胎儿超声分割质量评估（FUSQA）模型，用于处理当不存在用于比较的掩码时的分割质量评估。我们将分割质量评估过程定为一个自动化分类任务，以区分优质和低质量的分割掩码，以实现更准确的孕龄估计。我们在从两个医院收集的两个数据集上验证了我们提出方法的性能。

    Deep learning models have been effective for various fetal ultrasound segmentation tasks. However, generalization to new unseen data has raised questions about their effectiveness for clinical adoption. Normally, a transition to new unseen data requires time-consuming and costly quality assurance processes to validate the segmentation performance post-transition. Segmentation quality assessment efforts have focused on natural images, where the problem has been typically formulated as a dice score regression task. In this paper, we propose a simplified Fetal Ultrasound Segmentation Quality Assessment (FUSQA) model to tackle the segmentation quality assessment when no masks exist to compare with. We formulate the segmentation quality assessment process as an automated classification task to distinguish between good and poor-quality segmentation masks for more accurate gestational age estimation. We validate the performance of our proposed approach on two datasets we collect from two hosp
    
[^121]: 隐私估计中基于子集的实例优化

    Subset-Based Instance Optimality in Private Estimation. (arXiv:2303.01262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01262](http://arxiv.org/abs/2303.01262)

    本论文提出了一个新的定义来评估差分隐私估计算法的实例优化。我们的定义要求最优算法与一个最佳的已知数据集并在其较大的子集上进行性能评估相竞争，从而使基准算法比以前的工作更强大。我们还展示了在实值数据集上如何构建能够实现实例优化的隐私算法，并对均值进行了详细分析，证明我们的算法在估计一类广泛的数据集属性时能达到或超过渐近性能。

    

    我们提出了一个针对差分隐私估计算法的实例优化的新定义。我们的定义要求最优算法在每个数据集$D$上都与最佳的已知$D$并以最坏情况性能对$D$的大子集进行评估的隐私基准算法同时竞争。也就是说，基准算法在潜在的极端点被添加到$D$时可能表现不好；它只需要处理删除已经存在的一小部分真实数据点。这使得我们的基准算法比之前提出的基准算法显著更强大。尽管如此，我们仍然展示了对于实值数据集，如何构造能够实现我们的实例优化概念的隐私算法，以估计包括均值、分位数和$\ell_p$-范数最小化器在内的广泛类别的数据集属性。特别是对于均值，我们提供了一种详细分析，并展示了我们的算法同时匹配或超过了渐近的p

    We propose a new definition of instance optimality for differentially private estimation algorithms. Our definition requires an optimal algorithm to compete, simultaneously for every dataset $D$, with the best private benchmark algorithm that (a) knows $D$ in advance and (b) is evaluated by its worst-case performance on large subsets of $D$. That is, the benchmark algorithm need not perform well when potentially extreme points are added to $D$; it only has to handle the removal of a small number of real data points that already exist. This makes our benchmark significantly stronger than those proposed in prior work. We nevertheless show, for real-valued datasets, how to construct private algorithms that achieve our notion of instance optimality when estimating a broad class of dataset properties, including means, quantiles, and $\ell_p$-norm minimizers. For means in particular, we provide a detailed analysis and show that our algorithm simultaneously matches or exceeds the asymptotic p
    
[^122]: 用于不平衡异构信息网络的语义感知节点合成

    Semantic-aware Node Synthesis for Imbalanced Heterogeneous Information Networks. (arXiv:2302.14061v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14061](http://arxiv.org/abs/2302.14061)

    这项研究提出了一种解决不平衡异构信息网络中语义不平衡的方法，通过合成节点来应对少数类别的样本不足和偏倚的问题。

    

    异构图神经网络（HGNNs）在建模异构信息网络（HINs）中的复杂异质性方面显示出了非凡的效能。HGNNs的关键优势在于通过提取和利用丰富的语义信息进行有效的表示学习，以处理HINs中不同的节点和边类型。然而，作为许多现实场景中普遍存在的现象，HINs中的类别不平衡分布为现有的HGNNs创建了性能瓶颈。除了节点数量的不平衡外，HINs中更关键和独特的挑战是语义不平衡。HINs中的少数类别往往缺乏多样化和足够的邻居节点，导致偏倚和不完整的语义信息。这种语义不平衡进一步加剧了准确分类少数节点的困难，导致HGNNs的性能下降。为了解决少数类别的不平衡和补充其不足的情况。

    Heterogeneous graph neural networks (HGNNs) have exhibited exceptional efficacy in modeling the complex heterogeneity in heterogeneous information networks (HINs). The critical advantage of HGNNs is their ability to handle diverse node and edge types in HINs by extracting and utilizing the abundant semantic information for effective representation learning. However, as a widespread phenomenon in many real-world scenarios, the class-imbalance distribution in HINs creates a performance bottleneck for existing HGNNs. Apart from the quantity imbalance of nodes, another more crucial and distinctive challenge in HINs is semantic imbalance. Minority classes in HINs often lack diverse and sufficient neighbor nodes, resulting in biased and incomplete semantic information. This semantic imbalance further compounds the difficulty of accurately classifying minority nodes, leading to the performance degradation of HGNNs. To tackle the imbalance of minority classes and supplement their inadequate se
    
[^123]: 利用能量自然梯度实现PINNs的高准确度

    Achieving High Accuracy with PINNs via Energy Natural Gradients. (arXiv:2302.13163v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13163](http://arxiv.org/abs/2302.13163)

    我们提出了能量自然梯度下降算法，用于优化物理约束神经网络（PINNs）和深度Ritz方法。实验结果表明，与标准优化器相比，能量自然梯度下降算法能够产生更高准确度的解决方案。

    

    我们提出了能量自然梯度下降，这是一种基于Hessian诱导的黎曼度量的自然梯度方法，作为物理约束神经网络（PINNs）和深度Ritz方法的优化算法。作为主要动机，我们展示了能量自然梯度的更新方向在函数空间的结果与牛顿方向模除模型切空间的正交投影相对应。我们在实验中证明，即使允许标准优化器如梯度下降或Adam具有更长的计算时间，能量自然梯度下降仍然能产生比他们的误差小几个数量级的高精度解决方案。

    We propose energy natural gradient descent, a natural gradient method with respect to a Hessian-induced Riemannian metric as an optimization algorithm for physics-informed neural networks (PINNs) and the deep Ritz method. As a main motivation we show that the update direction in function space resulting from the energy natural gradient corresponds to the Newton direction modulo an orthogonal projection onto the model's tangent space. We demonstrate experimentally that energy natural gradient descent yields highly accurate solutions with errors several orders of magnitude smaller than what is obtained when training PINNs with standard optimizers like gradient descent or Adam, even when those are allowed significantly more computation time.
    
[^124]: SGL-PT: 一种具有图形提示调优的强大图形学习器

    SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. (arXiv:2302.12449v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12449](http://arxiv.org/abs/2302.12449)

    SGL-PT是一个具有图形提示调优的强大图形学习器，以缩小预训练和下游图形任务之间的差距，并提供一致的训练目标来增强预训练模型的能力。

    

    最近，人们付出了很多努力来设计图形自监督方法，以获得通用的预训练模型，并通过微调将预训练模型应用于下游任务。然而，前文任务和下游图形任务之间存在固有差距，这不充分发挥了预训练模型的能力，甚至导致负传递。同时，通过将预训练和微调与一致的训练目标对齐，提示调优在自然语言处理中取得了成功。在本文中，我们确定了图形提示调优的挑战：首先，图领域中各种预训练方法之间缺乏强大且通用的预训练任务。第二个挑战在于设计一致的训练目标，既适用于预训练任务，也适用于下游任务。为了克服上述障碍，我们提出了一种名为SGL-PT的新框架，该框架遵循学习策略“预训练、提示和预测”。

    Recently, much exertion has been paid to design graph self-supervised methods to obtain generalized pre-trained models, and adapt pre-trained models onto downstream tasks through fine-tuning. However, there exists an inherent gap between pretext and downstream graph tasks, which insufficiently exerts the ability of pre-trained models and even leads to negative transfer. Meanwhile, prompt tuning has seen emerging success in natural language processing by aligning pre-training and fine-tuning with consistent training objectives. In this paper, we identify the challenges for graph prompt tuning: The first is the lack of a strong and universal pre-training task across sundry pre-training methods in graph domain. The second challenge lies in the difficulty of designing a consistent training objective for both pre-training and downstream tasks. To overcome above obstacles, we propose a novel framework named SGL-PT which follows the learning strategy ``Pre-train, Prompt, and Predict''. Specif
    
[^125]: 鲁棒的期望改进用于贝叶斯优化

    Robust expected improvement for Bayesian optimization. (arXiv:2302.08612v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08612](http://arxiv.org/abs/2302.08612)

    本论文提出了一种鲁棒的期望改进（REI）方法，将对抗性方法引入贝叶斯优化框架，以解决在广阔的吸引域中寻找解决方案的问题。

    

    贝叶斯优化（BO）将高斯过程（GP）拟合与顺序设计相结合，以优化昂贵的黑盒函数。例如设计启发式方法，如期望改进（EI），在严格的评估预算下平衡勘探和开发，以提供全局解决方案。然而，当解决鲁棒最优解时，它们表现不佳，意味着更广阔的吸引域中优先考虑解决方案。当输入不精确指定或需要一系列解决方案时，鲁棒解决方案很有用。在这种情况下，常见的数学规划技术涉及对抗性目标，将局部求解器从“尖锐”的低谷偏离。在描述方法之后，我们提出了一种称为鲁棒期望改进（REI）的拟合模型和主动学习技术，将对抗方法引入了BO / GP框架。我们展示并对比了几个竞争对手在基准上的效果。

    Bayesian Optimization (BO) links Gaussian Process (GP) surrogates with sequential design toward optimizing expensive-to-evaluate black-box functions. Example design heuristics, or so-called acquisition functions, like expected improvement (EI), balance exploration and exploitation to furnish global solutions under stringent evaluation budgets. However, they fall short when solving for robust optima, meaning a preference for solutions in a wider domain of attraction. Robust solutions are useful when inputs are imprecisely specified, or where a series of solutions is desired. A common mathematical programming technique in such settings involves an adversarial objective, biasing a local solver away from ``sharp'' troughs. Here we propose a surrogate modeling and active learning technique called robust expected improvement (REI) that ports adversarial methodology into the BO/GP framework. After describing the methods, we illustrate and draw comparisons to several competitors on benchmark s
    
[^126]: 标签效率的时间序列表示学习：一项综述

    Label-efficient Time Series Representation Learning: A Review. (arXiv:2302.06433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06433](http://arxiv.org/abs/2302.06433)

    这篇综述介绍了针对时间序列数据中标记数据稀缺性问题的现有方法，并提供了一个新颖的分类系统来归纳这些方法。该综述总结了每种方法的最新进展并提出了未来的研究方向。

    

    标记数据的稀缺性是在现实世界中应用深度学习模型于时间序列数据时的主要挑战之一。因此，最近已经开发了几种方法，例如迁移学习，自监督学习和半监督学习，以促进深度学习模型从有限的时间序列标签中获取学习能力。在本综述中，我们首次提供了一个新颖的分类系统，用来根据它们对外部数据源的依赖，对解决时间序列数据中标记数据稀缺性问题的现有方法进行分类。此外，我们对每种方法的最新进展进行了审查，并总结了当前工作的局限性，并提供了可能在这一领域取得更好进展的未来方向。

    The scarcity of labeled data is one of the main challenges of applying deep learning models on time series data in the real world. Therefore, several approaches, e.g., transfer learning, self-supervised learning, and semi-supervised learning, have been recently developed to promote the learning capability of deep learning models from the limited time series labels. In this survey, for the first time, we provide a novel taxonomy to categorize existing approaches that address the scarcity of labeled data problem in time series data based on their dependency on external data sources. Moreover, we present a review of the recent advances in each approach and conclude the limitations of the current works and provide future directions that could yield better progress in the field.
    
[^127]: 用于改善痴呆症患者情绪激动检测的下采样和累积类决策方法

    Undersampling and Cumulative Class Re-decision Methods to Improve Detection of Agitation in People with Dementia. (arXiv:2302.03224v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03224](http://arxiv.org/abs/2302.03224)

    本文对样本不平衡及标记不精确问题进行改进，仅用正常行为数据的20％即可训练具竞争力的痴呆症患者情绪激动检测模型。

    

    情绪激动是痴呆症患者最常见的症状之一，可能会对他们和看护者的安全造成威胁。开发客观的情绪激动检测方法对支持在居住环境中生活的痴呆症患者的健康和安全非常重要。本文旨在通过实现不同的下采样方法消除样本不平衡问题，并设计加权下采样方法来评估手动标记机制的有效性，以改善痴呆症患者情绪激动检测的问题。研究结果表明，只有正常行为数据的20％足以训练具有竞争力的情绪激动检测模型。

    Agitation is one of the most prevalent symptoms in people with dementia (PwD) that can place themselves and the caregiver's safety at risk. Developing objective agitation detection approaches is important to support health and safety of PwD living in a residential setting. In a previous study, we collected multimodal wearable sensor data from 17 participants for 600 days and developed machine learning models for predicting agitation in one-minute windows. However, there are significant limitations in the dataset, such as imbalance problem and potential imprecise labels as the occurrence of agitation is much rarer in comparison to the normal behaviours. In this paper, we first implement different undersampling methods to eliminate the imbalance problem, and come to the conclusion that only 20\% of normal behaviour data are adequate to train a competitive agitation detection model. Then, we design a weighted undersampling method to evaluate the manual labeling mechanism given the ambiguo
    
[^128]: 一种使用最小互补能量原理的深度互补能量方法应用于固体力学的研究

    A deep complementary energy method for solid mechanics using minimum complementary energy principle. (arXiv:2302.01538v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01538](http://arxiv.org/abs/2302.01538)

    本文提出了一种名为深度互补能量方法(DCEM)的新方法，它基于最小互补能量原理，在固体力学中解决偏微分方程(PDE)，并扩展为DCEM-P以满足更多方程的要求。

    

    近年来，深度学习的快速发展在各个领域，特别是在固体力学中解决偏微分方程(PDE)方面产生了显著影响，极大地受益于神经网络的优异逼近能力。物理启发神经网络(PINNs)和深度能量方法(DEM)在解决PDE方面受到了广泛关注。最小势能原理和互补能量原理是固体力学中两个重要的变分原理。然而，DEM是基于最小势能原理，但它缺乏最小互补能量的重要形式。为了弥补这一差距，我们提出了基于最小互补能量原理的深度互补能量方法(DCEM)。DCEM的输出函数是应力函数。我们将DCEM扩展到DCEM-Plus (DCEM-P)，添加满足偏微分方程的项。此外，我们还提出了一种深度互补能量算子的方法。

    In recent years, the rapid advancement of deep learning has significantly impacted various fields, particularly in solving partial differential equations (PDEs) in solid mechanics, benefiting greatly from the remarkable approximation capabilities of neural networks. In solving PDEs, Physics-Informed Neural Networks (PINNs) and the Deep Energy Method (DEM) have garnered substantial attention. The principle of minimum potential energy and complementary energy are two important variational principles in solid mechanics. However,DEM is based on the principle of minimum potential energy, but it lacks the important form of minimum complementary energy. To bridge this gap, we propose the deep complementary energy method (DCEM) based on the principle of minimum complementary energy. The output function of DCEM is the stress function. We extend DCEM to DCEM-Plus (DCEM-P), adding terms that satisfy partial differential equations. Furthermore, we propose a deep complementary energy operator metho
    
[^129]: InfiniCity: 无限规模城市合成

    InfiniCity: Infinite-Scale City Synthesis. (arXiv:2301.09637v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.09637](http://arxiv.org/abs/2301.09637)

    InfiniCity是一个无限规模的城市合成框架，可以从随机噪音构建和渲染一个无限制大小的3D环境，具有可扩展的功能和交互式编辑。

    

    为了实现无限规模的3D城市合成，我们提出了一个新颖的框架InfiniCity，它可以从随机噪音构建和渲染一个无限制大小的3D环境。InfiniCity将看似不可行的任务分解为三个可行模块，充分利用了2D和3D数据。首先，一个无限像素图像合成模块从鸟瞰图生成任意尺度的2D地图。接下来，一个基于八叉树的体素完成模块将生成的2D地图转换为3D八叉树。最后，一个基于体素的神经渲染模块给体素贴上纹理并渲染2D图像。InfiniCity能够合成任意尺度和可遍历的3D城市环境，并允许用户进行灵活和交互式编辑。我们定量和定性地证明了该框架的有效性。

    Toward infinite-scale 3D city synthesis, we propose a novel framework, InfiniCity, which constructs and renders an unconstrainedly large and 3D-grounded environment from random noises. InfiniCity decomposes the seemingly impractical task into three feasible modules, taking advantage of both 2D and 3D data. First, an infinite-pixel image synthesis module generates arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel completion module lifts the generated 2D map to 3D octrees. Finally, a voxel-based neural rendering module texturizes the voxels and renders 2D images. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city environments, and allow flexible and interactive editing from users. We quantitatively and qualitatively demonstrate the efficacy of the proposed framework. Project page: https://hubert0527.github.io/infinicity/
    
[^130]: 一种行为良好的图神经近似复杂动力学的方法

    A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics. (arXiv:2301.04900v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2301.04900](http://arxiv.org/abs/2301.04900)

    本文介绍了一种行为良好的图神经网络近似复杂动力学的方法，包括必要的偏置和适当的神经网络结构，并提出了评估泛化能力和推断时预测置信度的方法。

    

    数据驱动的常微分方程近似提供了一种有前景的方法来发现动力系统模型，特别是对于缺乏明确原理的复杂系统。本文着重研究了一类由网络邻接矩阵耦合的常微分方程系统描述的复杂系统。许多现实世界中的系统，包括金融、社交和神经系统，属于这类动力学模型。我们提出了使用神经网络近似这种动力系统的关键要素，包括必要的偏置和适当的神经网络结构。强调与静态监督学习的区别，我们提倡在统计学习理论的经典假设之外评估泛化能力。为了在推断时估计预测的置信度，我们引入了一个专用的空模型。通过研究各种复杂网络动力学，我们展示了神经网络的能力。

    Data-driven approximations of ordinary differential equations offer a promising alternative to classical methods in discovering a dynamical system model, particularly in complex systems lacking explicit first principles. This paper focuses on a complex system whose dynamics is described with a system of ordinary differential equations, coupled via a network adjacency matrix. Numerous real-world systems, including financial, social, and neural systems, belong to this class of dynamical models. We propose essential elements for approximating such dynamical systems using neural networks, including necessary biases and an appropriate neural architecture. Emphasizing the differences from static supervised learning, we advocate for evaluating generalization beyond classical assumptions of statistical learning theory. To estimate confidence in prediction during inference time, we introduce a dedicated null model. By studying various complex network dynamics, we demonstrate the neural network'
    
[^131]: 为什么批归一化会损害非独立同分布数据上的联邦学习？

    Why Batch Normalization Damage Federated Learning on Non-IID Data?. (arXiv:2301.02982v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02982](http://arxiv.org/abs/2301.02982)

    本文通过首次的收敛性分析发现，非独立同分布数据中，在批归一化中局部和全局统计参数不匹配导致了梯度偏差，从而影响了联邦学习的收敛性。

    

    作为一种有前景的分布式学习范式，联邦学习（FL）涉及在网络边缘训练深度神经网络（DNN）模型，同时保护边缘客户端的隐私。为了训练大规模的DNN模型，批归一化（BN）被认为是一种简单有效的加速训练和改善泛化能力的方法。然而，最近的研究发现，在非独立同分布数据的情况下，BN会显著损害FL的性能。尽管已经提出了一些FL算法来解决这个问题，但它们的性能仍然明显低于集中式方案。此外，它们没有提供关于BN如何损害FL收敛性的理论解释。在本文中，我们提出了第一个收敛性分析，以展示在非独立同分布数据下，BN中局部和全局统计参数之间的不匹配导致了局部和全局模型之间的梯度偏差，从而影响了联邦学习的收敛性。

    As a promising distributed learning paradigm, federated learning (FL) involves training deep neural network (DNN) models at the network edge while protecting the privacy of the edge clients. To train a large-scale DNN model, batch normalization (BN) has been regarded as a simple and effective means to accelerate the training and improve the generalization capability. However, recent findings indicate that BN can significantly impair the performance of FL in the presence of non-i.i.d. data. While several FL algorithms have been proposed to address this issue, their performance still falls significantly when compared to the centralized scheme. Furthermore, none of them have provided a theoretical explanation of how the BN damages the FL convergence. In this paper, we present the first convergence analysis to show that under the non-i.i.d. data, the mismatch between the local and global statistical parameters in BN causes the gradient deviation between the local and global models, which, 
    
[^132]: FedICT:用于多接入边缘计算的联邦多任务蒸馏

    FedICT: Federated Multi-task Distillation for Multi-access Edge Computing. (arXiv:2301.00389v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00389](http://arxiv.org/abs/2301.00389)

    FedICT是一种用于多接入边缘计算的联邦多任务蒸馏方法，可以在个性化服务和异构机器学习模型的同时实现高效通信和模型异构性。

    

    对于移动设备智能服务和隐私保护的日益关注，联邦学习在多接入边缘计算（MEC）中得到了广泛应用。多样的用户行为要求在不同设备上使用个性化服务和异构机器学习（ML）模型。提出了联邦多任务学习（FMTL）来为不同设备训练相关但个性化的ML模型，然而之前的工作在训练过程中存在过多的通信开销，并忽视了MEC中设备之间的模型异构性。将知识蒸馏引入FMTL可以同时实现高效的通信和客户端之间的模型异构性，而现有方法依赖于公共数据集，这在实际中是不切实际的。为了解决这个困境，提出了用于多接入边缘计算的联邦多任务蒸馏（FedICT）。

    The growing interest in intelligent services and privacy protection for mobile devices has given rise to the widespread application of federated learning in Multi-access Edge Computing (MEC). Diverse user behaviors call for personalized services with heterogeneous Machine Learning (ML) models on different devices. Federated Multi-task Learning (FMTL) is proposed to train related but personalized ML models for different devices, whereas previous works suffer from excessive communication overhead during training and neglect the model heterogeneity among devices in MEC. Introducing knowledge distillation into FMTL can simultaneously enable efficient communication and model heterogeneity among clients, whereas existing methods rely on a public dataset, which is impractical in reality. To tackle this dilemma, Federated MultI-task Distillation for Multi-access Edge CompuTing (FedICT) is proposed. FedICT direct local-global knowledge aloof during bi-directional distillation processes between 
    
[^133]: 质量的尾部

    Quality at the Tail. (arXiv:2212.13925v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13925](http://arxiv.org/abs/2212.13925)

    本研究发现深度学习推理质量存在波动，引入了“尾部质量”的概念来描述这一现象。

    

    对深度学习模型和系统进行基准测试和评估需要一种细致入微的方法，以确保全面评估。在实际应用中，考虑到推理质量和推理时间是至关重要的，特别是在严苛的环境下，要求同时满足两个指标的要求。忽视其中任何一个方面都可能导致严重和不可逆的后果，包括人员伤亡和财产损失。不幸的是，许多研究缺乏对这些指标的全面考虑，通常在理想或宽松条件下进行，从而导致评估方法不完整或不直观。本研究揭示了深度学习推理质量的波动，进一步给基准测试和评估带来了复杂性和挑战。为了更好地描述这一现象，引入了“尾部质量”的概念，表示分布尾部的质量。

    Benchmarking and evaluating deep learning models and systems necessitate a meticulous approach to ensure comprehensive assessment. In practical applications, it is paramount to consider both the inference quality and the inference time, particularly within critical contexts, where stringent requirements demand the simultaneous satisfaction of both metrics. Neglecting either aspect can result in severe and irreversible consequences, including loss of human life and property damage. Unfortunately, many studies lack a comprehensive consideration of these metrics, often conducted under ideal or permissive conditions, thereby leading to incomplete or non-intuitive evaluation methodologies.  This study reveals that deep learning inference quality exhibits fluctuations, which further introduces complications and challenges to the benchmarking and evaluation. To better characterize the phenomenon, the concept of "tail quality" is introduced, which indicates the quality at the tail of distribut
    
[^134]: 可解释预测分子和晶体属性的端到端AI框架

    End-to-end AI framework for interpretable prediction of molecular and crystal properties. (arXiv:2212.11317v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2212.11317](http://arxiv.org/abs/2212.11317)

    我们引入了一个端到端的AI框架，利用最先进的模型和计算环境，可以预测分子和晶体的属性，提供了可解释的推断功能，并在领先的计算设施中进行了部署和测试。

    

    我们引入了一个端到端的计算框架，利用DeepHyper库进行超参数优化，加速模型训练，并进行可解释的AI推断。该框架基于最先进的AI模型，包括CGCNN、PhysNet、SchNet、MPNN、MPNN-transformer和TorchMD-NET。我们利用QM9、hMOF和MD17等基准数据集展示了这些模型如何在现代计算环境中预测用户指定的材料属性。我们演示了该统一、独立框架在小分子、无机晶体和纳米多孔金属有机框架建模中的可迁移应用。我们在Argonne领导计算设施的ThetaGPU超级计算机和国家超级计算应用中心的Delta超级计算机上部署和测试了这个框架，为研究人员提供了进行加速的AI驱动发现的现代工具。

    We introduce an end-to-end computational framework that allows for hyperparameter optimization using the DeepHyper library, accelerated model training, and interpretable AI inference. The framework is based on state-of-the-art AI models including CGCNN, PhysNet, SchNet, MPNN, MPNN-transformer, and TorchMD-NET. We employ these AI models along with the benchmark QM9, hMOF, and MD17 datasets to showcase how the models can predict user-specified material properties within modern computing environments. We demonstrate transferable applications in the modeling of small molecules, inorganic crystals and nanoporous metal organic frameworks with a unified, standalone framework. We have deployed and tested this framework in the ThetaGPU supercomputer at the Argonne Leadership Computing Facility, and in the Delta supercomputer at the National Center for Supercomputing Applications to provide researchers with modern tools to conduct accelerated AI-driven discovery in leadership-class computing env
    
[^135]: FedALA: 自适应局部聚合用于个性化联邦学习

    FedALA: Adaptive Local Aggregation for Personalized Federated Learning. (arXiv:2212.01197v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01197](http://arxiv.org/abs/2212.01197)

    FedALA是一种用于个性化联邦学习的方法，通过自适应局部聚合（ALA）模块来解决统计异质性问题，并在广泛的实验证明中超过了11种最先进的基准模型。

    

    联邦学习中的一个关键挑战是统计异质性，这会影响全局模型在每个客户端上的泛化能力。为了解决这个问题，我们提出了一种名为Federated learning with Adaptive Local Aggregation（FedALA）的方法，通过在个性化联邦学习中捕捉全局模型对客户端模型中所需的信息。FedALA的关键组成部分是自适应局部聚合（ALA）模块，它可以根据每个客户端上的局部目标自适应聚合下载的全局模型和本地模型以在每次迭代中初始化本地模型。为了评估FedALA的有效性，我们在计算机视觉和自然语言处理领域使用了五个基准数据集进行了大量的实验证明。FedALA在测试准确性方面比十一种最先进的基准模型取得了最多3.27%的改进。此外，我们还将ALA模块应用于其他联邦学习方法，并在测试准确性方面取得了最多24.19%的改进。

    A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.
    
[^136]: 谁的情绪更重要？没有先前知识的说话活动定位。

    Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge. (arXiv:2211.15377v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.15377](http://arxiv.org/abs/2211.15377)

    本文介绍了一种新方法MELD-FAIR来解决情感识别中的挑战，通过使用主动说话者检测和自动语音识别模型，重新对齐了MELD视频，并成功捕获了讲话者的面部表情。

    

    会话情感识别（ERC）的任务受益于多种模态的可用性，例如在基于视频的多模态情感线数据集（MELD）中提供的信息。然而，只有少数研究方法使用了MELD视频中的声学和视觉信息。这有两个原因：首先，MELD中的标签到视频的对齐是有噪声的，这使得那些视频成为了情感语音数据的不可靠来源。其次，会话可以涉及到同一场景中的几个人，这需要定位话语来源。在本文中，我们通过使用最近的主动说话者检测和自动语音识别模型，引入了带有固定音频视觉信息的MELD-FAIR，能够重新对齐MELD视频并捕获96.92％的MELD中提供的话语的讲话者面部表情。使用自我监督的声音识别模型进行的实验表明，重新对齐的MELD-FAIR视频更清晰地显示了讲话者的面部表情。

    The task of emotion recognition in conversations (ERC) benefits from the availability of multiple modalities, as provided, for example, in the video-based Multimodal EmotionLines Dataset (MELD). However, only a few research approaches use both acoustic and visual information from the MELD videos. There are two reasons for this: First, label-to-video alignments in MELD are noisy, making those videos an unreliable source of emotional speech data. Second, conversations can involve several people in the same scene, which requires the localisation of the utterance source. In this paper, we introduce MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using recent active speaker detection and automatic speech recognition models, we are able to realign the videos of MELD and capture the facial expressions from speakers in 96.92% of the utterances provided in MELD. Experiments with a self-supervised voice recognition model indicate that the realigned MELD-FAIR videos more cl
    
[^137]: {\mu}Split: 显微镜数据的高效图像分解方法

    {\mu}Split: efficient image decomposition for microscopy data. (arXiv:2211.12872v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12872](http://arxiv.org/abs/2211.12872)

    uSplit是一种适用于荧光显微镜图像的高效图像分解方法，集成了横向上下文化，帮助训练更深的分层模型，并有效地减少平铺伪影问题。

    

    我们提出了 uSplit，一种专门用于荧光显微镜图像中的训练图像分解的方法。我们发现，使用常规的深度结构体系结构在训练时使用大图像块会获得最佳结果，使内存消耗成为进一步提高性能的限制因素。因此，我们引入了横向上下文化（LC），一种内存高效的方法来训练强大的网络，并展示LC在处理任务时始终带来了显著的改进。我们将LC与U-Nets、分层自编码器和分层VAEs集成，为此我们制定了一种改进的ELBO loss。此外，LC使得训练比原本更深的分层模型成为可能，并且有助于减少使用分割VAE预测时不可避免的平铺伪影。我们将uSplit应用于五个分解任务，一个是合成数据集，另外四个来自实际显微镜数据。LC实现了SOTA的结果（平均im）

    We present uSplit, a dedicated approach for trained image decomposition in the context of fluorescence microscopy images. We find that best results using regular deep architectures are achieved when large image patches are used during training, making memory consumption the limiting factor to further improving performance. We therefore introduce lateral contextualization (LC), a memory efficient way to train powerful networks and show that LC leads to consistent and significant improvements on the task at hand. We integrate LC with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a modified ELBO loss. Additionally, LC enables training deeper hierarchical models than otherwise possible and, interestingly, helps to reduce tiling artefacts that are inherently impossible to avoid when using tiled VAE predictions. We apply uSplit to five decomposition tasks, one on a synthetic dataset, four others derived from real microscopy data. LC achieves SOTA results (average im
    
[^138]: 基于去中心化的联邦学习: 基础、现状、框架、趋势和挑战 (arXiv:2211.08413v2 [cs.LG] UPDATED)

    Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08413](http://arxiv.org/abs/2211.08413)

    本文提出去中心化联邦学习（DFL）来解决传统中心化FL（CFL）模型中的问题，主要研究DFL与CFL的差异、DFL的基础理论、DFL框架的设计与评估以及DFL在实际应用中的场景。

    

    在过去的十年中，联邦学习（FL）已经成为在不共享敏感数据的情况下训练协作模型的一种重要方法。自问世以来，中心化FL（CFL）一直是文献中最常见的方法，其中一个中心化实体创建全局模型。然而，中心化方法会导致瓶颈增加、系统故障风险增高，影响负责创建全局模型的实体的可信度。去中心化联邦学习（DFL）应运而生，通过推广去中心化模型聚合并最小化对中心化架构的依赖，解决了这些问题。但是，尽管在DFL方面有所努力，文献还没有研究(i)DFL和CFL之间的主要差异;(ii)分析DFL框架以创建和评估新解决方案;(iii)回顾使用DFL的应用场景。因此，本文在联邦架构、安全性、通信等方面识别并分析了DFL的主要基础。

    In the last decade, Federated Learning (FL) has gained relevance in training collaborative models without sharing sensitive data. Since its birth, Centralized FL (CFL) has been the most common approach in the literature, where a central entity creates a global model. However, a centralized approach leads to increased latency due to bottlenecks, heightened vulnerability to system failures, and trustworthiness concerns affecting the entity responsible for the global model creation. Decentralized Federated Learning (DFL) emerged to address these concerns by promoting decentralized model aggregation and minimizing reliance on centralized architectures. However, despite the work done in DFL, the literature has not (i) studied the main aspects differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and evaluate new solutions; and (iii) reviewed application scenarios using DFL. Thus, this article identifies and analyzes the main fundamentals of DFL in terms of federation architect
    
[^139]: 基于量子内核的代理模型加速变分量子算法

    Faster variational quantum algorithms with quantum kernel-based surrogate models. (arXiv:2211.01134v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.01134](http://arxiv.org/abs/2211.01134)

    提出了一种基于量子内核的代理模型，用于加速小规模至中等规模的变分量子算法，通过将计算负担转移到经典优化器组件上，大大减少了对量子处理器的查询次数。实验结果显示，这种代理模型相比传统经典内核具有更好的性能表现。

    

    我们提出了一种新的优化方法，用于在噪声近期量子处理器上进行小规模至中等规模的变分算法，该方法使用配备经典评估的量子内核的高斯过程代理模型。传统上，变分算法使用基于梯度的方法进行优化，但在当前的噪声设备上实施起来很困难，需要大量的目标函数评估。我们的方案将这个计算负担转移到这些混合算法的经典优化器组件上，大大减少了对量子处理器的查询次数。我们重点研究变分量子特征值求解器（VQE）算法，并经过数值验证表明，这种代理模型特别适用于该算法的目标函数。接下来，我们将这些模型应用到无噪声和有噪声的VQE模拟中，并展示它们在最终准确度和收敛速度方面比广泛使用的经典内核表现更好。

    We present a new optimization method for small-to-intermediate scale variational algorithms on noisy near-term quantum processors which uses a Gaussian process surrogate model equipped with a classically-evaluated quantum kernel. Variational algorithms are typically optimized using gradient-based approaches however these are difficult to implement on current noisy devices, requiring large numbers of objective function evaluations. Our scheme shifts this computational burden onto the classical optimizer component of these hybrid algorithms, greatly reducing the number of queries to the quantum processor. We focus on the variational quantum eigensolver (VQE) algorithm and demonstrate numerically that such surrogate models are particularly well suited to the algorithm's objective function. Next, we apply these models to both noiseless and noisy VQE simulations and show that they exhibit better performance than widely-used classical kernels in terms of final accuracy and convergence speed.
    
[^140]: Nesterov遇见乐观主义：速率最优的可分离极小极大优化

    Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization. (arXiv:2210.17550v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.17550](http://arxiv.org/abs/2210.17550)

    我们提出了一种新的一阶优化算法AG-OG，用于可分离的凸-凹极小极大优化问题。通过精细地利用问题结构，我们实现了最优收敛速率，可以适用于多种设置，包括双线性耦合的强凸-强凹、凸-强凹极小极大优化和双线性博弈。该算法还在随机设置下达到了最优收敛速率。这是第一个在双线性耦合的极小极大优化问题中在确定性和随机设置下都具有最优收敛速率的单次调用算法。

    

    我们提出了一种新的一阶优化算法 - 加速梯度-乐观梯度（AG-OG）下降上升法，用于可分离的凸-凹极小极大优化问题。我们算法的主要思想是精细地利用极小极大问题的结构，在个体组件上进行Nesterov加速，并在耦合组件上进行乐观梯度。我们展示了AG-OG在各种设置下（包括双线性耦合的强凸-强凹极小极大优化，双线性耦合的凸-强凹极小极大优化和双线性博弈）实现了最优收敛速率（常数因子之内）。我们还将我们的算法扩展到随机设置，并在双线性耦合的强凸-强凹和凸-强凹设置下达到最优收敛速率。AG-OG是第一个在双线性耦合的极小极大优化问题的确定性和随机设置下都具有最优收敛速率的单次调用算法。

    We propose a new first-order optimization algorithm -AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent -- for separable convex-concave minimax optimization. The main idea of our algorithm is to carefully leverage the structure of the minimax problem, performing Nesterov acceleration on the individual component and optimistic gradient on the coupling component. Equipped with proper restarting, we show that AG-OG achieves the optimal convergence rate (up to a constant) for a variety of settings, including bilinearly coupled strongly convex-strongly concave minimax optimization (bi-SC-SC), bilinearly coupled convex-strongly concave minimax optimization (bi-C-SC), and bilinear games. We also extend our algorithm to the stochastic setting and achieve the optimal convergence rate in both bi-SC-SC and bi-C-SC settings. AG-OG is the first single-call algorithm with optimal convergence rates in both deterministic and stochastic settings for bilinearly coupled minimax optimization 
    
[^141]: Bayesian双曲多维标度

    Bayesian Hyperbolic Multidimensional Scaling. (arXiv:2210.15081v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.15081](http://arxiv.org/abs/2210.15081)

    这是一篇关于提出了一种贝叶斯双曲多维标度方法的论文，通过在双曲空间中表示低维图形来处理高维相关数据，从而适用于具有树状结构的数据，并且提供了有效的后验采样方法，降低了计算复杂性。

    

    多维标度（MDS）是一种广泛应用于表示高维、相关数据的方法。MDS通过为每个观测分配一个低维几何图形上的位置来工作，图形上的距离表示相似性。我们提出了一种贝叶斯方法来处理低维图形为双曲线的多维标度。使用双曲空间有助于表示许多场景中常见的树状结构（例如具有层次结构的文本或遗传数据）。贝叶斯方法提供了对观测数据中测量误差的最小化影响和不确定性评估的正则化。我们还提出了一种情况对照似然估计近似值，允许在更大的数据设置中从后验分布中高效采样，将计算复杂性从近似$O(n^2)$降低到$O(n)$。我们使用模拟、经典参考数据集、印度vil进行了对所提出的方法与业界最先进的方法进行了评估

    Multidimensional scaling (MDS) is a widely used approach to representing high-dimensional, dependent data. MDS works by assigning each observation a location on a low-dimensional geometric manifold, with distance on the manifold representing similarity. We propose a Bayesian approach to multidimensional scaling when the low-dimensional manifold is hyperbolic. Using hyperbolic space facilitates representing tree-like structures common in many settings (e.g. text or genetic data with hierarchical structure). A Bayesian approach provides regularization that minimizes the impact of measurement error in the observed data and assesses uncertainty. We also propose a case-control likelihood approximation that allows for efficient sampling from the posterior distribution in larger data settings, reducing computational complexity from approximately $O(n^2)$ to $O(n)$. We evaluate the proposed method against state-of-the-art alternatives using simulations, canonical reference datasets, Indian vil
    
[^142]: MotionDeltaCNN：移动摄像机视频中稀疏CNN对帧差异的推断

    MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos. (arXiv:2210.09887v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09887](http://arxiv.org/abs/2210.09887)

    本文提出了MotionDeltaCNN，一个支持移动摄像机的稀疏CNN推断框架，通过引入球面缓冲区和填充卷积来高效融合新揭示的图像区域和已处理区域，从而实现对帧差异的推断。在移动摄像机视频中，相对于DeltaCNN，我们的方法性能提升了高达90%。

    

    在视频输入上进行卷积神经网络推断计算代价高昂，并且需要高内存带宽。最近，DeltaCNN通过仅处理与上一帧相比有显著更新的像素来减少成本。然而，DeltaCNN依赖于静态摄像机输入。移动摄像机给如何高效融合新揭示的图像区域与已处理区域带来了新的挑战，以最小化更新率 - 同时不增加内存开销且无需知道未来帧的摄像机外参数。在这项工作中，我们提出了MotionDeltaCNN，这是一个支持移动摄像机的稀疏CNN推断框架。我们引入了球面缓冲区和填充卷积，以便无缝融合新揭示的区域和以前处理的区域 - 同时不增加内存占用。我们的评估结果表明，对于移动摄像头视频，我们的性能超过DeltaCNN多达90%。

    Convolutional neural network inference on video input is computationally expensive and requires high memory bandwidth. Recently, DeltaCNN managed to reduce the cost by only processing pixels with significant updates over the previous frame. However, DeltaCNN relies on static camera input. Moving cameras add new challenges in how to fuse newly unveiled image regions with already processed regions efficiently to minimize the update rate - without increasing memory overhead and without knowing the camera extrinsics of future frames. In this work, we propose MotionDeltaCNN, a sparse CNN inference framework that supports moving cameras. We introduce spherical buffers and padded convolutions to enable seamless fusion of newly unveiled regions and previously processed regions -- without increasing memory footprint. Our evaluation shows that we outperform DeltaCNN by up to 90% for moving camera videos.
    
[^143]: 严格的动力学均场理论用于随机梯度下降方法

    Rigorous dynamical mean field theory for stochastic gradient descent methods. (arXiv:2210.06591v2 [math-ph] UPDATED)

    [http://arxiv.org/abs/2210.06591](http://arxiv.org/abs/2210.06591)

    本研究通过证明的闭式方程，描述了一类基于梯度的方法在高维情况下的精确渐进性能，为随机梯度下降等算法提供了理论支持，并提供了数值实现。

    

    我们证明了一类基于梯度的方法在高维情况下的精确渐进性能闭式方程，该方法从高斯数据的经验风险最小化学习估计器（例如M-估计器，浅层神经网络...）。这包括了广泛使用的算法，如随机梯度下降（SGD）或Nesterov加速。得到的方程与将动力学均场理论（DMFT）方程离散化后应用于梯度流时产生的方程相匹配。我们的证明方法允许我们明确描述记忆核在有效动力学中如何构建，并且包括非可分离的更新函数，允许具有非单位协方差矩阵的数据集。最后，我们提供了具有通用批处理大小和恒定学习率的SGD方程的数值实现。

    We prove closed-form equations for the exact high-dimensional asymptotics of a family of first order gradient-based methods, learning an estimator (e.g. M-estimator, shallow neural network, ...) from observations on Gaussian data with empirical risk minimization. This includes widely used algorithms such as stochastic gradient descent (SGD) or Nesterov acceleration. The obtained equations match those resulting from the discretization of dynamical mean-field theory (DMFT) equations from statistical physics when applied to gradient flow. Our proof method allows us to give an explicit description of how memory kernels build up in the effective dynamics, and to include non-separable update functions, allowing datasets with non-identity covariance matrices. Finally, we provide numerical implementations of the equations for SGD with generic extensive batch-size and with constant learning rates.
    
[^144]: R2C-GAN: 用于盲目X射线恢复和COVID-19分类的恢复到分类生成对抗网络

    R2C-GAN: Restore-to-Classify GANs for Blind X-Ray Restoration and COVID-19 Classification. (arXiv:2209.14770v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.14770](http://arxiv.org/abs/2209.14770)

    R2C-GAN是一种用于盲目X射线恢复和COVID-19分类的生成对抗网络，通过图像恢复提高X射线图像质量，并实现更高的诊断性能。

    

    探索了针对盲目X射线恢复的联合模型：恢复到分类生成对抗网络(R2C-GANs)。该模型在保持疾病完整性的同时进行图像恢复，从而提高X射线图像的质量并实现更高的诊断性能。将恢复任务定义为从质量较差包含有噪声、模糊或过/欠曝图片到高质量图像领域的图像到图像翻译问题。R2C-GAN模型能够学习两个领域之间的正向和反向转换。

    Restoration of poor quality images with a blended set of artifacts plays a vital role for a reliable diagnosis. Existing studies have focused on specific restoration problems such as image deblurring, denoising, and exposure correction where there is usually a strong assumption on the artifact type and severity. As a pioneer study in blind X-ray restoration, we propose a joint model for generic image restoration and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). Such a jointly optimized model keeps any disease intact after the restoration. Therefore, this will naturally lead to a higher diagnosis performance thanks to the improved X-ray image quality. To accomplish this crucial objective, we define the restoration task as an Image-to-Image translation problem from poor quality having noisy, blurry, or over/under-exposed images to high quality image domain. The proposed R2C-GAN model is able to learn forward and inverse transforms between the two domains
    
[^145]: 非鲁棒性特征会导致灾难性的过度拟合

    Catastrophic overfitting can be induced with discriminative non-robust features. (arXiv:2206.08242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08242](http://arxiv.org/abs/2206.08242)

    本研究通过控制性修改典型的自然图像数据集，研究了对抗训练中灾难性过度拟合的出现。通过注入看似无害的特征，可以在较小的epsilon值下引发灾难性过度拟合。

    

    对抗训练是构建鲁棒神经网络的事实方法，但计算成本较高。为了减轻这一问题，可以使用快速单步攻击，但这可能导致灾难性的过度拟合。本文通过对典型的自然图像数据集进行控制性修改，研究了单步对抗训练方法中灾难性过度拟合的出现。特别地，我们表明，通过注入看似无害的特征，可以在比之前观察到的较小的epsilon值下引发灾难性过度拟合。这些特征有助于非鲁棒性分类，但不能单独实现鲁棒性。通过大量实验，我们分析了这一新现象，并发现了这种失败模式的机制还不够清楚。

    Adversarial training (AT) is the de facto method for building robust neural networks, but it can be computationally expensive. To mitigate this, fast single-step attacks can be used, but this may lead to catastrophic overfitting (CO). This phenomenon appears when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. The mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in single-step AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced at much smaller $\epsilon$ values than it was observed before just by injecting images with seemingly innocuous features. These features aid non-robust classification but are not enough to achieve robustness on their own. Through extensive experiments we analyze this novel phenomenon and discover that the presence of thes
    
[^146]: 正样本未标记对比学习

    Positive Unlabeled Contrastive Learning. (arXiv:2206.01206v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01206](http://arxiv.org/abs/2206.01206)

    我们提出了一种正样本未标记对比学习的新方法，通过扩展对比损失和使用PU特定聚类方案，该方法在PU任务中学习到了优秀的表示，并在多个标准数据集上明显优于现有方法。

    

    自我监督预训练无标签数据，然后在标记数据上进行监督微调是一种常见的从有限标记样本中学习的方法。我们将这个方法扩展到经典的正样本未标记（PU）设置，其中的任务是仅通过一些标记为正样本和（通常）大量的未标记样本（可以是正样本或负样本）来学习二分类器。我们首先对标准infoNCE对比损失的家族提出了一个简单的扩展，适用于PU设置；并且证明相比于现有的无监督和有监督方法，这种方法学习到了更好的表示。然后，我们开发了一种简单的方法，使用新的PU特定聚类方案为未标记样本构建伪标签；这些伪标签可以用来训练最终的（正样本 vs. 负样本）分类器。我们的方法在几个标准PU基准数据集上明显优于现有的PU方法，并且不需要任何类别的先验知识。

    Self-supervised pretraining on unlabeled data followed by supervised fine-tuning on labeled data is a popular paradigm for learning from limited labeled examples. We extend this paradigm to the classical positive unlabeled (PU) setting, where the task is to learn a binary classifier given only a few labeled positive samples, and (often) a large amount of unlabeled samples (which could be positive or negative).  We first propose a simple extension of standard infoNCE family of contrastive losses, to the PU setting; and show that this learns superior representations, as compared to existing unsupervised and supervised approaches. We then develop a simple methodology to pseudo-label the unlabeled samples using a new PU-specific clustering scheme; these pseudo-labels can then be used to train the final (positive vs. negative) classifier. Our method handily outperforms state-of-the-art PU methods over several standard PU benchmark datasets, while not requiring a-priori knowledge of any clas
    
[^147]: FlexFringe:通过学习概率有限自动机来建模软件行为

    FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata. (arXiv:2203.16331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16331](http://arxiv.org/abs/2203.16331)

    FlexFringe提供了高效的概率有限自动机学习方法，可用于建模软件行为。该方法在实践中通过实现改进的状态合并策略实现了显著性能提升，并且能够从软件日志中学习可解释的模型，用于异常检测。与基于神经网络的解决方案相比，学习更小更复杂的模型能够提高FlexFringe在异常检测中的性能。

    

    我们介绍了FlexFringe中可用的概率确定性有限自动机学习方法的高效实现。这些实现了众所周知的状态合并策略，包括几种修改以提高它们在实践中的性能。我们通过实验证明这些算法能够获得有竞争力的结果，并在默认实现上实现了显著的改进。我们还展示了如何使用FlexFringe从软件日志中学习可解释的模型，并将其用于异常检测。虽然这些模型较难解释，但我们展示了学习更小、更复杂的模型如何提高FlexFringe在异常检测中的性能，优于基于神经网络的现有解决方案。

    We present the efficient implementations of probabilistic deterministic finite automaton learning methods available in FlexFringe. These implement well-known strategies for state-merging including several modifications to improve their performance in practice. We show experimentally that these algorithms obtain competitive results and significant improvements over a default implementation. We also demonstrate how to use FlexFringe to learn interpretable models from software logs and use these for anomaly detection. Although less interpretable, we show that learning smaller more convoluted models improves the performance of FlexFringe on anomaly detection, outperforming an existing solution based on neural nets.
    
[^148]: 多任务表示学习与随机线性赌博机

    Multi-task Representation Learning with Stochastic Linear Bandits. (arXiv:2202.10066v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.10066](http://arxiv.org/abs/2202.10066)

    本研究通过跨任务共享低维线性表示，提出了一种基于迹范数正则化的高效贪婪策略，在多任务学习中学习低维表示，无需知道潜在矩阵的秩。实验结果表明，该策略相比基线在多任务遗憾上有明显的优势。

    

    我们研究了在随机线性赌博机任务中的迁移学习问题。我们考虑跨任务共享低维线性表示，并研究在多任务学习中学习这种表示的益处。根据最新的随机赌博机策略设计结果，我们提出了一种基于迹范数正则化的高效贪婪策略。它通过鼓励任务回归向量形成的矩阵具有低秩来隐式地学习低维表示。与文献中的先前工作不同，我们的策略不需要知道潜在矩阵的秩。我们导出了我们策略的多任务遗憾的上界，该上界在对数因子上是$O(\sqrt{NdT(T+d)r})$，其中$T$是任务数，$r$是秩，$d$是变量数，$N$是每个任务的回合数。我们展示了与基线$Td\sqrt{N}$相比，我们策略的益处。

    We study the problem of transfer-learning in the setting of stochastic linear bandit tasks. We consider that a low dimensional linear representation is shared across the tasks, and study the benefit of learning this representation in the multi-task learning setting. Following recent results to design stochastic bandit policies, we propose an efficient greedy policy based on trace norm regularization. It implicitly learns a low dimensional representation by encouraging the matrix formed by the task regression vectors to be of low rank. Unlike previous work in the literature, our policy does not need to know the rank of the underlying matrix. We derive an upper bound on the multi-task regret of our policy, which is, up to logarithmic factors, of order $\sqrt{NdT(T+d)r}$, where $T$ is the number of tasks, $r$ the rank, $d$ the number of variables and $N$ the number of rounds per task. We show the benefit of our strategy compared to the baseline $Td\sqrt{N}$ obtained by solving each task i
    
[^149]: 学习胃组织的多尺度混合视觉Transformer：胃癌治疗的基于AI的决策支持系统

    Multi-Scale Hybrid Vision Transformer for Learning Gastric Histology: AI-Based Decision Support System for Gastric Cancer Treatment. (arXiv:2202.08510v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.08510](http://arxiv.org/abs/2202.08510)

    提出了一个基于AI的决策支持系统，该系统能够对胃癌进行五种不同的病理学子分类，通过多尺度混合视觉Transformer网络进行胃癌诊断，在多中心队列上展示了可靠的诊断性能。

    

    胃内窥镜筛查是早期确定适当的胃癌治疗的有效方法，有效降低胃癌相关死亡率。虽然人工智能（AI）为病理学家筛查数字化全切片图像带来了巨大的希望，但现有的AI系统在细粒度癌症分类和癌症治疗规划方面受限。我们提出了一个实用的AI系统，可以实现五种胃癌病理学子分类，并直接匹配到常规胃癌治疗指南。该AI系统利用两阶段混合Vision Transformer（ViT）网络通过模仿人类病理学家理解组织学的方式，通过多尺度自注意机制有效区分多类胃癌。该AI系统在来自多中心队列的1,212张切片上实现了高于0.85的平均敏感性，展示了可靠的诊断性能。

    Gastric endoscopic screening is an effective way to decide appropriate gastric cancer (GC) treatment at an early stage, reducing GC-associated mortality rate. Although artificial intelligence (AI) has brought a great promise to assist pathologist to screen digitalized whole slide images, existing AI systems are limited in fine-grained cancer subclassifications and have little usability in planning cancer treatment. We propose a practical AI system that enables five subclassifications of GC pathology, which can be directly matched to general GC treatment guidance. The AI system is designed to efficiently differentiate multi-classes of GC through multi-scale self-attention mechanism using 2-stage hybrid Vision Transformer (ViT) networks, by mimicking the way how human pathologists understand histology. The AI system demonstrates reliable diagnostic performance by achieving class-average sensitivity of above 0.85 on a total of 1,212 slides from multicentric cohort. Furthermore, AI-assiste
    
[^150]: 不完全数据统计模型估计的变分吉布斯推断

    Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data. (arXiv:2111.13180v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.13180](http://arxiv.org/abs/2111.13180)

    这项研究提出了一种名为变分吉布斯推断（VGI）的方法，用于解决使用不完全数据进行统计模型估计时的挑战。与标准的潜变量模型不同，VGI能够处理估计指数多个缺失变量的条件分布，从而为实际的数据集提供了更准确的模型估计。

    

    统计模型在机器学习中具有广泛的适用性，可用于各种下游任务。这些模型由自由参数控制，通常通过最大似然估计或其近似方法从数据中估计。然而，当面对真实世界的数据集时，许多模型都会遇到一个关键问题：它们是以完全观测的数据为基础的，而实际上数据集中存在缺失数据。从不完全数据中进行统计模型估计的理论在概念上类似于潜变量模型的估计，其中存在诸如变分推断（VI）之类的强大工具。然而，与标准的潜变量模型不同，使用不完全数据的参数估计通常需要估计指数多个缺失变量的条件分布，因此使得标准的VI方法难以处理。我们通过引入变分吉布斯推断（VGI），一种新的通用方法，来填补这一差距。

    Statistical models are central to machine learning with broad applicability across a range of downstream tasks. The models are controlled by free parameters that are typically estimated from data by maximum-likelihood estimation or approximations thereof. However, when faced with real-world data sets many of the models run into a critical issue: they are formulated in terms of fully-observed data, whereas in practice the data sets are plagued with missing data. The theory of statistical model estimation from incomplete data is conceptually similar to the estimation of latent-variable models, where powerful tools such as variational inference (VI) exist. However, in contrast to standard latent-variable models, parameter estimation with incomplete data often requires estimating exponentially-many conditional distributions of the missing variables, hence making standard VI methods intractable. We address this gap by introducing variational Gibbs inference (VGI), a new general-purpose meth
    
[^151]: 聚类和因果图中的结构稳健性

    Clustering and Structural Robustness in Causal Diagrams. (arXiv:2111.04513v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.04513](http://arxiv.org/abs/2111.04513)

    本研究提出了一种聚类方法，通过定义过渡聚类并应用完备的算法，可以简化因果图中的变量关系，并保留因果效应的可辨识性属性。

    

    图表常用于表示和可视化因果关系。对于少量变量，这种方法提供了一个简洁清晰的场景视图。但是随着研究变量数量的增加，图表方法可能变得不可行，并且表示的清晰度丧失。变量聚类是减小因果图大小的一种自然方式，但如果随意实施，可能会错误地改变因果关系的重要属性。我们定义了一种特定类型的聚类，称为过渡聚类，在某些条件下保证保留因果效应的可辨识性属性。我们提供了一个完备的算法来寻找给定图表中的所有过渡聚类，并展示了聚类如何简化因果效应的识别。我们还研究了反向问题，即我们从一个聚类图表开始，寻找满足因果效应可辨识性属性的扩展图表。

    Graphs are commonly used to represent and visualize causal relations. For a small number of variables, this approach provides a succinct and clear view of the scenario at hand. As the number of variables under study increases, the graphical approach may become impractical, and the clarity of the representation is lost. Clustering of variables is a natural way to reduce the size of the causal diagram, but it may erroneously change the essential properties of the causal relations if implemented arbitrarily. We define a specific type of cluster, called transit cluster, that is guaranteed to preserve the identifiability properties of causal effects under certain conditions. We provide a sound and complete algorithm for finding all transit clusters in a given graph and demonstrate how clustering can simplify the identification of causal effects. We also study the inverse problem, where one starts with a clustered graph and looks for extended graphs where the identifiability properties of ca
    
[^152]: 非平稳在线学习中的记忆与非随机控制问题

    Non-stationary Online Learning with Memory and Non-stochastic Control. (arXiv:2102.03758v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.03758](http://arxiv.org/abs/2102.03758)

    本文研究了具有记忆的非平稳在线凸优化问题，引入了动态策略遗憾作为性能度量，并提出了一种算法，通过新颖的切换成本感知在线合奏方法解决了切换成本的关键技术挑战。

    

    本文研究了具有记忆的在线凸优化问题（OCO），其中损失函数可以依赖于过去的决策，从而捕捉到学习问题的时间效应。我们引入了动态策略遗憾作为性能度量，以设计在非平稳环境下鲁棒的算法，该算法将算法的决策与一系列变化的比较器进行竞争。我们提出了一种新的OCO记忆算法，它在时间跨度、非平稳度量和记忆长度方面保证了最佳的动态策略遗憾。关键技术挑战是如何控制切换成本，即参与者决策的累积移动量，这个问题通过一种新颖的切换成本感知在线合奏方法得到了巧妙解决，该方法采用了动态策略遗憾的新的元基分解和一个精心设计的元学习器和基学习器，以显式地规范化切换成本。

    We study the problem of Online Convex Optimization (OCO) with memory, which allows loss functions to depend on past decisions and thus captures temporal effects of learning problems. In this paper, we introduce dynamic policy regret as the performance measure to design algorithms robust to non-stationary environments, which competes algorithms' decisions with a sequence of changing comparators. We propose a novel algorithm for OCO with memory that provably enjoys an optimal dynamic policy regret in terms of time horizon, non-stationarity measure, and memory length. The key technical challenge is how to control the switching cost, the cumulative movements of player's decisions, which is neatly addressed by a novel switching-cost-aware online ensemble approach equipped with a new meta-base decomposition of dynamic policy regret and a careful design of meta-learner and base-learner that explicitly regularizes the switching cost. The results are further applied to tackle non-stationarity i
    
[^153]: 通过增强指数族的充分统计量来实现公平密度。

    Fair Densities via Boosting the Sufficient Statistics of Exponential Families. (arXiv:2012.00188v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.00188](http://arxiv.org/abs/2012.00188)

    本文介绍了一种利用增强算法来实现公平密度的方法，该方法通过学习指数族的充分统计量，以改善数据拟合，并确保最小的公平性保证。实验证明了该方法在真实数据上的有效性。

    

    我们介绍了一种利用增强算法对数据进行公平预处理的方法。从一个初始的公平但不准确的分布开始，我们的方法在确保最小公平性保证的同时朝着更好的数据拟合方向进行转移。为此，它学习了一个具有增强收敛性的指数族的充分统计量。重要的是，我们能够从理论上证明学习的分布将具有表示率和统计率的数据公平性保证。与最近的基于优化的预处理方法不同，我们的方法可以轻松适应连续域特征。此外，当弱学习者被指定为决策树时，可以检查学习分布的充分统计量，以提供（不）公平性的来源线索。通过实证结果展示了在真实数据上的结果质量。

    We introduce a boosting algorithm to pre-process data for fairness. Starting from an initial fair but inaccurate distribution, our approach shifts towards better data fitting while still ensuring a minimal fairness guarantee. To do so, it learns the sufficient statistics of an exponential family with boosting-compliant convergence. Importantly, we are able to theoretically prove that the learned distribution will have a representation rate and statistical rate data fairness guarantee. Unlike recent optimization based pre-processing methods, our approach can be easily adapted for continuous domain features. Furthermore, when the weak learners are specified to be decision trees, the sufficient statistics of the learned distribution can be examined to provide clues on sources of (un)fairness. Empirical results are present to display the quality of result on real-world data.
    

