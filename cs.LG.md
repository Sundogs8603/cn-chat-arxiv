# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Differentially Private Wireless Federated Learning Using Orthogonal Sequences.](http://arxiv.org/abs/2306.08280) | 本文提出了一种使用正交序列的FLORAS方法，可消除发送端的信道状态信息，同时提供了项目级和客户级的差分隐私保证。FLORAS可以灵活地实现不同的差分隐私等级，并且通过推导收敛界限，实现了收敛速度和隐私保证之间的平稳权衡。 |
| [^2] | [FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks.](http://arxiv.org/abs/2306.08277) | FRIGATE是一种使用时空GNN进行公路网络预测的方法，可以在不依赖每个节点的感知、完整的感知历史记录或静态网络假设的情况下工作，在真实交通数据集上表现出更高的预测性能。 |
| [^3] | [Why Using Either Aggregated Features or Adjacency Lists in Directed or Undirected Graph? Empirical Study and Simple Classification Method.](http://arxiv.org/abs/2306.08274) | 本文实证研究了在节点分类任务中使用聚合特征或邻接表以及边方向对分类效果的影响，并提出了一个简单全面的分类方法A2DUG，该方法利用了有向和无向图中节点表示变量的所有组合，其在各种数据集上稳定表现良好，并在几个数据集上优于当前的最先进方法。 |
| [^4] | [Imagery Tracking of Sun Activity Using 2D Circular Kernel Time Series Transformation, Entropy Measures and Machine Learning Approaches.](http://arxiv.org/abs/2306.08270) | 本文开发了一种基于2D圆形核时间序列转换、熵度量和机器学习方法的太阳活动成像跟踪技术，可以将太阳观测图像转换为1维时间序列，并提取特征进行机器学习分类，用于追踪太阳的活动情况，尤其在识别“太阳风暴”方面准确性较高。 |
| [^5] | [LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting.](http://arxiv.org/abs/2306.08259) | LargeST数据集是一个更为现实和具有挑战性的交通预测基准，包括8600个传感器、覆盖5年时间和包括细致元数据。 |
| [^6] | [Data Augmentation for Seizure Prediction with Generative Diffusion Model.](http://arxiv.org/abs/2306.08256) | 该论文提出了一种基于扩散模型的数据增强方法DiffEEG，可以有效地提高癫痫预测的性能，超过了现有的数据扩增方法。 |
| [^7] | [MMASD: A Multimodal Dataset for Autism Intervention Analysis.](http://arxiv.org/abs/2306.08243) | 提出了一个名为MMASD的自闭症多模态数据集，收集自治疗干预。它包括从32名自闭症患儿的干预录音中分段的1,315个数据样本，每个样本包含四种隐私保护模式的数据。 |
| [^8] | [Curricular Subgoals for Inverse Reinforcement Learning.](http://arxiv.org/abs/2306.08232) | 本论文提出一种新的逆强化学习框架，即基于课程子目标的逆强化学习 (CSIRL)。与现有方法不同的是，该框架将一个任务分解为多个局部子目标，以引导智能体进行模仿，这有助于解决全局设计中存在的问题。 |
| [^9] | [Unbiased Learning of Deep Generative Models with Structured Discrete Representations.](http://arxiv.org/abs/2306.08230) | 该论文提出了一种名为结构化变分自编码器的深度生成模型，它通过图像模型的结构和可解释性以及深度学习的适用于高维数据的灵活似然，结合两种框架的优势。同时，该论文还提出了一种学习SVAE的新算法，与此同时，推导出了一种计算自然梯度的方法，这些优化创新使得SVAE首次能与最先进的时间序列模型进行比较。 |
| [^10] | [Expanding Versatility of Agile Locomotion through Policy Transitions Using Latent State Representation.](http://arxiv.org/abs/2306.08224) | 本文提出了一种稳健的过渡策略——过渡网络，将不同步态的复杂性分为适用于真实世界机器人的专用策略，并通过潜在状态表征拓展机器人的多样性，实现了在技能库中任意策略对之间稳健过渡。 |
| [^11] | [Uncertainty-Aware Robust Learning on Noisy Graphs.](http://arxiv.org/abs/2306.08210) | 本文提出了一种基于分布式鲁棒优化的不确定性感知图学习框架，利用图神经网络嵌入节点特征，并通过极小极大形式最小化最坏情况风险来找到最优节点嵌入，从而缓解现实世界图中噪声测量挑战对图数据的负面影响。 |
| [^12] | [Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer.](http://arxiv.org/abs/2306.08204) | 该论文使用双重策略解决ARC任务，通过决策Transformer模仿人类解决方案和引入物体检测算法来提高AI的问题解决能力，同时揭示了数据和模型结构等方面的需求，为未来AGI研究提供了见解。 |
| [^13] | [POP: Prompt Of Prompts for Continual Learning.](http://arxiv.org/abs/2306.08200) | 本研究提出了一种称为POP的模型，通过逐步学习一组任务指定的提示和一组全局提示，以解决连续学习中的信息集成问题。 |
| [^14] | [Learning on Graphs under Label Noise.](http://arxiv.org/abs/2306.08194) | 该论文介绍了一种新方法 CGNN，它利用一致性图神经网络和基于同质性假设的样本选择技术，在标签噪声的情况下对节点分类进行建模，实现过滤出噪声节点和增强节点表示的鲁棒性。 |
| [^15] | [Operationalising Representation in Natural Language Processing.](http://arxiv.org/abs/2306.08193) | 本文介绍了一个框架，通过使用探测分类器来评估组件是否表示属性，填补了自然语言处理中关于“表示”的哲学空白。 |
| [^16] | [Inductive Linear Probing for Few-shot Node Classification.](http://arxiv.org/abs/2306.08192) | 本文研究了在归纳少样本节点分类任务中现有框架的局限性，并提出了一种针对这种任务的简单而高效的基线方法。 |
| [^17] | [Contextual Font Recommendations based on User Intent.](http://arxiv.org/abs/2306.08188) | Adobe的20,000多种字体库是个选择恐惧症患者的噩梦，本文通过创造一个基于用户意图的系统来自动给用户提供合适的字体推荐，目前已被数百万 Adobe Express 用户使用，点击率高达 25% 以上。 |
| [^18] | [DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer.](http://arxiv.org/abs/2306.08175) | 提出了一种基于Conformer的新型动态上下文传递机制DCTX-Conformer，解决了流式识别性能差距的问题，相比于现有最优解，识别结果的误差率提高了25%，但对延迟影响可以忽略不计。 |
| [^19] | [Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training.](http://arxiv.org/abs/2306.08173) | 本文提出了一种差分隐私的CLIP模型（Dp-CLIP），旨在保护多模态AI任务中的数据隐私，同时保持模型准确性。该方法在基准数据集上得到了验证，并表明其与标准非私有CLIP模型相比具有同等的性能。 |
| [^20] | [Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms.](http://arxiv.org/abs/2306.08167) | 机器学习模型在语义连贯的数据子集上表现不佳仍然会出现问题，但是确定这些问题片段可能很困难，自动生成的片段并不是确定人工从业者问题性片段的银弹。 |
| [^21] | [Reinforcement Learning-Driven Linker Design via Fast Attention-based Point Cloud Alignment.](http://arxiv.org/abs/2306.08166) | 该论文提出了一种强化学习驱动的链接物设计方法，成功地生成满足2D和3D要求的连接剂，实现了新型连接剂的制备。 |
| [^22] | [INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation.](http://arxiv.org/abs/2306.08162) | 该论文提出了一种通过低秩自适应纠错的方法，从而可以显著地减少精细调整VRAM需求，并纠正量化大语言模型中的量化误差，使消费者笔记本电脑可以对70亿个参数的大语言模型进行精细调整，生成连贯的英文文本。 |
| [^23] | [h2oGPT: Democratizing Large Language Models.](http://arxiv.org/abs/2306.08161) | 本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。 |
| [^24] | [Survey on Sociodemographic Bias in Natural Language Processing.](http://arxiv.org/abs/2306.08158) | 本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。 |
| [^25] | [Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks.](http://arxiv.org/abs/2306.08157) | 本文提出了一种基于动态贝叶斯网络的方法，来预测加密货币价格方向，以帮助投资者做出明智的投资决策。 |
| [^26] | [(Amplified) Banded Matrix Factorization: A unified approach to private training.](http://arxiv.org/abs/2306.08153) | 本文提出了利用带状矩阵构建的矩阵分解机制，该机制能够在所有隐私预算中将先前最先进的算法纳入分散和联合训练设置中。对于跨设备联合学习，这意味着可以使用一种放松的设备参与模式，与实际的FL基础设施相容。在集中式设置中，带状矩阵具有与 ubiquitous DP-SGD algorithm 相同的隐私放大结果。 |
| [^27] | [Neural Mixed Effects for Nonlinear Personalized Predictions.](http://arxiv.org/abs/2306.08149) | 本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。 |
| [^28] | [Multi-market Energy Optimization with Renewables via Reinforcement Learning.](http://arxiv.org/abs/2306.08147) | 本文提出了一种基于深度强化学习的功率厂优化方法，通过储能与可再生能源的组合最大化能源市场的收入，最小化储能损耗成本以及可再生能源的削减。通过部件级模拟器处理了储能过程中的时间耦合、不确定性以及非线性存储模型等问题，并通过RL方法处理复杂储能模型，实现了策略动作与系统约束的平衡。 |
| [^29] | [ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations.](http://arxiv.org/abs/2306.08141) | 为研究人工智能与人类交互，研究者创建了ArtWhisperer数据集，这是一个在线游戏，人们通过反复尝试不同的提示词，来生成和目标图像类似的图像，并记录了50,000多个交互记录。在初步分析中，研究者发现人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。 |
| [^30] | [Self-supervised Deep Hyperspectral Inpainting with the Sparsity and Low-Rank Considerations.](http://arxiv.org/abs/2306.08128) | 本论文提出了两种新的自监督高光谱图像修复算法，具有强大的学习能力，且在不需要外部训练数据的情况下有效地解决高光谱图像修复的问题，实验结果也显示其效果优于现有算法。 |
| [^31] | [Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD.](http://arxiv.org/abs/2306.08125) | 本研究提出了一种简单的SGD修改方法，使训练出的神经网络输出可被证明为可压缩，而不需要任何非平凡假设。 |
| [^32] | [Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level.](http://arxiv.org/abs/2306.08122) | 该论文提出了一种新的自然语言处理方法，可对学术写作中的抄袭行为进行有效检测，不仅在句子级别上进行评估，还在文档级别上提供可量化指标。该方法的准确率高达94％，具有较强的适应性和可靠性，能够不断随着LLM技术的发展而不断改进。 |
| [^33] | [Better Generalization with Semantic IDs: A case study in Ranking for Recommendations.](http://arxiv.org/abs/2306.08121) | 本文提出使用语义ID解决推荐系统中的物品冷启动问题，这些ID是从内容嵌入中学习的，可以捕捉概念的层次关系，相较于完全消除ID特征的方法，语义ID能更好地提高推荐质量。 |
| [^34] | [CipherSniffer: Classifying Cipher Types.](http://arxiv.org/abs/2306.08116) | 本文将解密任务作为分类问题来解决，并创建了一个包含各种类型的密码数据集，最终评估了各种Tokenizer-Model组合在此任务中的性能。 |
| [^35] | [Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity.](http://arxiv.org/abs/2306.08109) | 本研究证明了针对一类新的目标函数，只有部分参数满足强凸性，Nesterov动量法在深度神经网络中实现了加速收敛。 |
| [^36] | [AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks.](http://arxiv.org/abs/2306.08107) | 论文探讨了AutoML和LLMs之间的共生关系，并指出这两个领域的融合有望颠覆NLP和AutoML两个领域，同时也存在风险。 |
| [^37] | [Model-Free Market Risk Hedging Using Crowding Networks.](http://arxiv.org/abs/2306.08105) | 本文采用基金持仓的网络分析来计算股票的拥挤分数，构建出的多头头寸和空头头寸的对冲组合具有负相关和正市场回报下凸性的特性，能提供一种无需昂贵的期权策略或复杂的数值优化的对冲投资组合，用于对冲投资组合风险，包括尾部风险。 |
| [^38] | [Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise Reduction.](http://arxiv.org/abs/2306.08102) | 本文提出了一种少样本监督学习框架，用于光学相干断层扫描噪音降低，并成功推广到不同成像领域。 |
| [^39] | [Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning.](http://arxiv.org/abs/2306.08094) | 本文研究探讨使用 ChatGPT 解决混合交通流控制问题，通过大规模用户研究发现 ChatGPT 在某些环境下能够提高成功策略数量 |
| [^40] | [Safe Use of Neural Networks.](http://arxiv.org/abs/2306.08086) | 本文介绍了一种用于神经网络的错误检测和纠正方法，使用数字编码可以显著提升其可靠性。 |
| [^41] | [Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization.](http://arxiv.org/abs/2306.08076) | 本文提出一种非欧几里得空间线性外推设计，通过外推结构和特征空间来生成超出分布的图数据， 然后实现图的超出分布泛化。 |
| [^42] | [DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$.](http://arxiv.org/abs/2306.08068) | DORSal提出了一种基于扩散模型的物体中心场景表示方法，可以呈现高保真新视图，并在较大程度上保留了诸如基于物体的场景编辑之类的优点。 |
| [^43] | [Symbolic Regression via Control Variable Genetic Programming.](http://arxiv.org/abs/2306.08057) | CVGP是一种通过控制变量实验设计加快符号表达式发现的方法，能够学习复杂符号表达式，扩展了现有方法的能力。 |
| [^44] | [Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training.](http://arxiv.org/abs/2306.08055) | 该论文提出了一种名为“CARBS”的算法，它利用贝叶斯优化算法在性能-计算 Pareto 前沿附近执行局部搜索来解决大型深度学习模型超参数调整的问题。该方法适用于具有许多超参数的无界搜索空间，学习缩放关系，并自动化了许多调整中的“黑魔法”。此外，该方法对计算受限制的情况尤其有效。 |
| [^45] | [Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care.](http://arxiv.org/abs/2306.08044) | 该论文介绍了一种深度Q学习方法，通过剪枝动作集来实现将中间生物标志物信号整合到奖励规范中，提高了重症护理策略的可靠性。 |
| [^46] | [On Faking a Nash Equilibrium.](http://arxiv.org/abs/2306.08041) | 本文研究多智能体强化学习中的数据污染攻击，提出了唯一纳什集的概念，并设计了一个线性规划方案来计算最优污染攻击策略。 |
| [^47] | [Flexible Channel Dimensions for Differentiable Architecture Search.](http://arxiv.org/abs/2306.08021) | 本文提出了一种新颖的可微神经架构搜索方法，使用动态通道分配算法实现通道维度的灵活搜索空间，能够有效地找到等同于以前方法在CIFAR-10数据集上任务准确性和推理延迟方面的DNN架构，并且不需要手动设计或架构工程专业知识。 |
| [^48] | [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models.](http://arxiv.org/abs/2306.08018) | Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。 |
| [^49] | [Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language.](http://arxiv.org/abs/2306.08014) | 本文介绍了自由能原理和主动推理的理论框架，并推导了适用于任意图形模型的主动推理版本。同时引入了一种新的图形说明语言（GSL）来明确规定系统目标。 |
| [^50] | [TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models.](http://arxiv.org/abs/2306.08013) | 本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。 |
| [^51] | [Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios.](http://arxiv.org/abs/2306.08011) | 本文提出了一个针对非独立同分布场景下联邦学习的隐蔽后门攻击方案，其中通过生成对抗网络提供补充数据集，有效应对数据异构问题和隐私推断攻击风险。 |
| [^52] | [DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation.](http://arxiv.org/abs/2306.08009) | 本文提出了一种数据无关的全面后门擦除（DHBE）框架，该框架将后门擦除任务视为一个统一的对抗过程，在转移干净数据的知识的同时保证擦除后门。通过对抗蒸馏和后门正则化两个不同的竞争过程，DHBE实现了一种统一的、无需数据，并且有效的后门擦除框架。 |
| [^53] | [Dynamic Interval Restrictions on Action Spaces in Deep Reinforcement Learning for Obstacle Avoidance.](http://arxiv.org/abs/2306.08008) | 本文提出两种在动态障碍物场景下处理任意数量区间限制的方法，能较好地完成避障任务，表现优于现有方法。 |
| [^54] | [Leveraging dendritic properties to advance machine learning and neuro-inspired computing.](http://arxiv.org/abs/2306.08007) | 树突机制为AI领域提供了启发性的创新解决方案，包括信用分配、灾难性遗忘和高能耗等问题，为构建更强大、更节能的人工学习系统提供了有前途的新途径。 |
| [^55] | [Detection and classification of faults aimed at preventive maintenance of PV systems.](http://arxiv.org/abs/2306.08004) | 本论文提出了一种随机森林算法的创新方法，用于检测细微的光伏故障并进行分类，提高了故障分类的计算时间，同时保持了高精度。 |
| [^56] | [DTW k-means clustering for fault detection in photovoltaic modules.](http://arxiv.org/abs/2306.08003) | 本文提出了一种无监督聚类技术的方法，用于在大规模光伏电厂中检测和识别故障类型，采用动态时间规整（DTW）距离度量，能够提高聚类性能。 |
| [^57] | [A Markovian Formalism for Active Querying.](http://arxiv.org/abs/2306.08001) | 本文提出了一个基于马尔可夫系统的形式化主义对主动学习进行概述，将主动学习过程作为一个部分可观察的马尔可夫系统的整体处理，并且阐述如何将查询、数据集增强、奖励更新等过程视为马尔可夫系统中元状态之间的转移。 |
| [^58] | [Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models.](http://arxiv.org/abs/2306.08000) | 该论文研究了如何利用领域预训练语言模型CX-BERT、BlueBERT和ClinicalBERT提高CLIP-like模型对低患病率胸部病症的零样本检测。实验结果表明，预训练的文本塔对于低患病率疾病的检测有显著的性能提升。这提示了未来可使用不同训练语言模型的集成模型进行进一步研究。 |
| [^59] | [Machine Learning Approach on Multiclass Classification of Internet Firewall Log Files.](http://arxiv.org/abs/2306.07997) | 该研究通过应用多种分类算法，分析互联网防火墙日志文件，以便更好地防御网络攻击并了解恶意操作何时以及如何影响互联网。 |
| [^60] | [Semantic-Based Neural Network Repair.](http://arxiv.org/abs/2306.07995) | 本文提出了一种自动修复神经网络错误的方法，基于深度学习层的可执行语义，并专注于实践中常见的四种错误。 |
| [^61] | [MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer.](http://arxiv.org/abs/2306.07994) | 本文提出了一种新方法，通过为每个词汇分配单独的风格向量来对文本进行风格转换，并引入基于教师-学生学习的对抗性培训框架，以提高培训稳定性，实现了双风格转换和多风格转换两种情况下的明显提高的风格转换准确性和内容保留。 |
| [^62] | [Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework.](http://arxiv.org/abs/2306.07992) | 本文提出了一种对抗图像重构及检测框架来保护视觉感知推荐系统，能够防御局部扰动为特征的对抗攻击并且能够在干净和对抗性图像上进行训练来检测对抗性图像。 |
| [^63] | [A Survey on Cross-Architectural IoT Malware Threat Hunting.](http://arxiv.org/abs/2306.07989) | 本文综述了跨体系结构IoT恶意软件检测和分类方法的最新发展和实际挑战。 |
| [^64] | [Chainlet Orbits: Topological Address Embedding for the Bitcoin Blockchain.](http://arxiv.org/abs/2306.07974) | 本研究利用拓扑特征嵌入比特币地址，提出了一种新方法“链环轨道”，用于探测比特币网络中的电子犯罪行为。 |
| [^65] | [PrivaScissors: Enhance the Privacy of Collaborative Inference through the Lens of Mutual Information.](http://arxiv.org/abs/2306.07973) | PrivaScissors是一种防御策略，用于减少模型中间结果和设备数据、预测之间的互信息，以增强边缘-云协作推理的隐私保护。 |
| [^66] | [Leveraging Machine Learning for Multichain DeFi Fraud Detection.](http://arxiv.org/abs/2306.07972) | 该论文提出了一种利用机器学习进行多链DeFi欺诈检测的方法，并在最广泛使用的23个DeFi协议的交易数据集上进行了评估。 |
| [^67] | [Symmetry & Critical Points for Symmetric Tensor Decompositions Problems.](http://arxiv.org/abs/2306.07886) | 本文研究了将一个实对称张量分解成秩为1项之和的非凸优化问题，得到了精确的分析估计，并发现了各种阻碍局部优化方法的几何障碍和由于对称性导致的丰富的临界点集合。 |
| [^68] | [BeliefPPG: Uncertainty-aware Heart Rate Estimation from PPG signals via Belief Propagation.](http://arxiv.org/abs/2306.07730) | 通过离散时间随机过程将心率演变表示为隐藏马尔科夫模型，使用训练的神经网络计算PPG信号窗口内的可能心率值分布，然后通过置信传播结合统计分布监测心率变化以优化这些估计，并获得涵盖心率值范围的量化概率分布以捕获固有预测不确定性的良好校准估计。 |
| [^69] | [Hyperbolic Graph Diffusion Model for Molecule Generation.](http://arxiv.org/abs/2306.07618) | 本文提出了基于双曲图扩散模型的分子生成方法，可以更全面地捕捉分子的内部非欧几里德结构，实现数据生成，并提取复杂几何特征的能力。 |
| [^70] | [Incentivizing High-Quality Content in Online Recommender Systems.](http://arxiv.org/abs/2306.07479) | 本文研究了在线推荐系统中激励高质量内容的算法问题，经典的在线学习算法会激励生产者创建低质量的内容，但本文提出的一种算法通过惩罚低质量内容的创建者，成功地激励了生产者创造高质量的内容。 |
| [^71] | [DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills.](http://arxiv.org/abs/2306.07419) | 本文通过深度强化学习和机器人工具的互动研究，证明了可行性是四足动物步态转换的重要标准。其中，步-小跑步态转换能够在平坦地形上同时提高可行性和节能效果。 |
| [^72] | [Self-Supervised Hyperspectral Inpainting with the Optimisation inspired Deep Neural Network Prior.](http://arxiv.org/abs/2306.07308) | 本文提出了一种自监督的高光谱图像修复算法LRS-PnP-DIP，该算法能够在高光谱图像中精确预测缺失像素和带，其在实验中表现优异，达到或超过了其他学习方法。 |
| [^73] | [Unprocessing Seven Years of Algorithmic Fairness.](http://arxiv.org/abs/2306.07261) | 该论文取消了算法公平性中的后处理方法，并发现后处理实现的公平性-准确性Pareto边界包含了可评估的所有其他方法。 |
| [^74] | [Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches.](http://arxiv.org/abs/2306.07220) | 本文介绍了Strokes2Surface，它可从建筑师的笔画中恢复出曲线网络，对于建筑设计中的概念设计和数字建模之间的桥梁具有重要意义。 |
| [^75] | [NF4 Isn't Information Theoretically Optimal (and that's Good).](http://arxiv.org/abs/2306.06965) | 论文讨论了基于absmax的分块量化，推出该方法不是信息理论上的最优选择。作者优化L1重构误差，提出改进方法，适用于大块量化。在小块量化里，两种方法性能类似。 |
| [^76] | [Localised Adaptive Spatial-Temporal Graph Neural Network.](http://arxiv.org/abs/2306.06930) | 本文提出了自适应图稀疏化算法，成功将自适应空间-时间图神经网络（ASTGNN）本地化至极致，无需空间图即可达到同样的测试准确性。 |
| [^77] | [TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI.](http://arxiv.org/abs/2306.06924) | 本文探讨了人工智能可能带来的社会规模风险的分类和分析，其中提出了一种基于问责制的分类法。针对可能出现的风险类型提供了实际的证明，并指出需要联合技术和政策解决方案。 |
| [^78] | [Enhancing COVID-19 Diagnosis through Vision Transformer-Based Analysis of Chest X-ray Images.](http://arxiv.org/abs/2306.06914) | 本研究提出了一种基于ViT视觉转换器处理胸部X-ray图像来自动诊断COVID-19的创新方法， 在二元和三元分类表现方面都取得了良好的成果。 |
| [^79] | [TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models.](http://arxiv.org/abs/2306.06815) | 本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。 |
| [^80] | [Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization.](http://arxiv.org/abs/2306.06674) | 该论文提出了一种自监督等式嵌入深度Lagrange对偶算法，用于解决不带标签的逼近限制优化问题。此方法通过在神经网络中嵌入等式约束来确保可行解，并使用原始-对偶方法进行训练，同时DeepLDE取得了最好的优化结果。 |
| [^81] | [Fast light-field 3D microscopy with out-of-distribution detection and adaptation through Conditional Normalizing Flows.](http://arxiv.org/abs/2306.06408) | 本文提出了一种基于条件归一化流框架的快速三维重建方法，并在其中融入分布外检测和适应性机制，以确保重建结果的稳健性和可靠性，该方法可以实现实时重建速度和高质量的重建效果，是分析活体生物的有前途的工具。 |
| [^82] | [Error Feedback Can Accurately Compress Preconditioners.](http://arxiv.org/abs/2306.06098) | 本论文提出一种错误反馈技术，可以通过在馈入预处理器之前对梯度信息进行压缩（稀疏化或低秩压缩），将预处理器的存储成本压缩多达两个数量级，而不会丢失收敛性。 |
| [^83] | [CARSO: Counter-Adversarial Recall of Synthetic Observations.](http://arxiv.org/abs/2306.06081) | 本文提出了一种新的图像分类的对抗性防御机制CARSO，该方法可以比最先进的对抗性训练更好地保护分类器，通过利用生成模型进行对抗净化来进行最终分类，并成功地保护自己免受未预见的威胁和最终攻击。 |
| [^84] | [Self-Interpretable Time Series Prediction with Counterfactual Explanations.](http://arxiv.org/abs/2306.06024) | 本文提出了一种自我解释的时间序列预测模型CounTS，该模型可以生成反事实和可操作的解释，适用于关键领域如医疗和自动驾驶等。与现有方法不同，该模型为可解释性建模做出了贡献。 |
| [^85] | [Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees.](http://arxiv.org/abs/2306.05739) | 本研究首次在连续空间中进行树形系统探索和推断，用于有根和无根树，优于当前最佳方法并在实验中证明了其效果，可用于加速生命科学的新进化发现。 |
| [^86] | [Specifying and Solving Robust Empirical Risk Minimization Problems Using CVXPY.](http://arxiv.org/abs/2306.05649) | 本文介绍了如何使用CVXPY以用户友好的方式自动化鲁棒经验风险最小化问题的对偶化过程，使得用户可以方便地解决各种回归和分类问题。 |
| [^87] | [Island-based Random Dynamic Voltage Scaling vs ML-Enhanced Power Side-Channel Attacks.](http://arxiv.org/abs/2306.04859) | 本文介绍了一种基于岛屿的随机动态电压调节（iRDVS）方法，用于防范功率侧信道攻击，并通过实验验证了其有效性。 |
| [^88] | [Reinforcement Learning-Based Control of CrazyFlie 2.X Quadrotor.](http://arxiv.org/abs/2306.03951) | 该论文介绍了如何使用强化学习算法来实现CrazyFlie 2.X四轴飞行器的控制和导航，同时结合PID控制和灯塔定位系统，以实现更加精确的控制。 |
| [^89] | [A survey of Generative AI Applications.](http://arxiv.org/abs/2306.02781) | 本篇论文对350多个生成式人工智能应用进行了全面调查，总结了不同单模和多模生成式人工智能的应用。该调查为研究人员和从业者提供了宝贵的资源，帮助他们更好地了解生成式人工智能领域目前的最先进技术，并促进该领域的进一步创新。 |
| [^90] | [How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?.](http://arxiv.org/abs/2306.01248) | 这篇论文探讨了是否可以使用预训练的抽象模型和大型语言模型来自动生成法律案例判决的摘要，并在印度的法庭案例判决中进行了相关实验分析。 |
| [^91] | [Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders.](http://arxiv.org/abs/2305.19259) | 本论文研究了一种允许任意数据排序的普通SGD算法,并表明在非凸函数情况下，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。 |
| [^92] | [Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization.](http://arxiv.org/abs/2305.17323) | 本文提出了一种强凸优化的次梯度法原始对偶理论，可以实现简单的、最佳的停止准则和优化证明，同时可以适用于各种步长的选择和非Lipschitz病态问题，保证了这些方法次线性收敛速度。 |
| [^93] | [Generating Images with Multimodal Language Models.](http://arxiv.org/abs/2305.17216) | 该论文提出了一种方法，将大型语言模型与预训练的图像编码器和解码器模型进行融合，能生成具有连贯性的图像输出，同时也能进行图像检索和多模态对话。 |
| [^94] | [Scaling Data-Constrained Language Models.](http://arxiv.org/abs/2305.16264) | 研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。 |
| [^95] | [OVO: Open-Vocabulary Occupancy.](http://arxiv.org/abs/2305.16133) | 本文提出了开放词汇占据（OVO）方法，可以允许任意类别的语义占据预测，无需三维注释，其关键技术包括预训练二维开放词汇分割模型到三维占据网络的知识蒸馏和像素-体素过滤以生成高质量的训练数据。 |
| [^96] | [Multi-BVOC Super-Resolution Exploiting Compounds Inter-Connection.](http://arxiv.org/abs/2305.14180) | 本文提出了一种利用多种化合物贡献对粗糙BVOC排放地图进行超分辨的策略，实验结果表明该方法可以提高超分辨率性能。 |
| [^97] | [Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks.](http://arxiv.org/abs/2305.14065) | 本文提出了一种新的图神经网络结构搜索方法——神经结构编码（NAC），它通过稀疏编码寻找最优结构参数，无需训练就能发挥表现力，在多个基准数据集上实现了最先进性能，并且运算速度比强基线方法快了200倍，精度提高了18.8％。 |
| [^98] | [A Meta-learning based Generalizable Indoor Localization Model using Channel State Information.](http://arxiv.org/abs/2305.13453) | 本文提出了一种基于元学习和信道状态信息的室内定位模型，以解决深度学习定位模型中持续存在的通用性缺失问题。 |
| [^99] | [Road Planning for Slums via Deep Reinforcement Learning.](http://arxiv.org/abs/2305.13060) | 本文介绍了一种基于深度强化学习的方法，用于自动布局贫民窟道路。通过掩码策略优化，可使可达性提高14.3％，对现有基线方法具有明显改进。 |
| [^100] | [Graph Propagation Transformer for Graph Representation Learning.](http://arxiv.org/abs/2305.11424) | 本文提出了一种新的变换器架构 GPTrans，以图传播注意力为基础，可以更好地学习图形模型，并在多个基准测试集上超过了其他最先进的基于变换器的图形模型。 |
| [^101] | [Meta-Polyp: a baseline for efficient Polyp segmentation.](http://arxiv.org/abs/2305.07848) | 本研究提出Meta-Polyp，将Meta-Former与UNet融合并引入多尺度上采样块和Convformer块，解决了CNN和Vision Transformer在处理分布外数据集、缺失边界和小息肉时遇到的困难，提高了息肉分割的效率。 |
| [^102] | [How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?.](http://arxiv.org/abs/2305.06587) | 该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。 |
| [^103] | [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.](http://arxiv.org/abs/2305.06500) | 本文对视觉语言指令调整进行了系统全面的研究，引入了指令感知的视觉特征提取这种关键的方法，使模型能够提取适合于给定指令的信息特征。 |
| [^104] | [Segmentation of fundus vascular images based on a dual-attention mechanism.](http://arxiv.org/abs/2305.03617) | 本文提出了基于双重注意机制的眼底血管图像分割方法，可以从空间和通道维度提取图像信息，并通过引入空间注意机制和Dropout层解决光照变化和不均匀对比度等问题，实验结果表明，该方法可以产生令人满意的结果。 |
| [^105] | [The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold.](http://arxiv.org/abs/2305.01604) | 本文展示了多个深度网络的训练过程探索相同的低维流形，这些网络包括不同体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化，并揭示了网络初始化位置、体系结构和大小对流形的影响。 |
| [^106] | [Contextual Multilingual Spellchecker for User Queries.](http://arxiv.org/abs/2305.01082) | 本文提出了一个上下文多语种用户查询拼写检查器，它非常快速、可扩展，并根据特定产品的需求调整其词汇表和拼写输出，以满足用户的需求。 |
| [^107] | [Class-Balancing Diffusion Models.](http://arxiv.org/abs/2305.00562) | 这项工作探究了扩散模型在类别不平衡的数据上的表现，并提出了一种解决方案“类平衡扩散模型”通过使用分布调整正则化器进行训练。 |
| [^108] | [Spherical Inducing Features for Orthogonally-Decoupled Gaussian Processes.](http://arxiv.org/abs/2304.14034) | 本文研究了解耦高斯过程的正交分解问题，提出了一种扩展方法，即引入球形跨域特征，构建更灵活的数据依赖基函数来缓解限制，并展示了其有效性。 |
| [^109] | [Towards Mode Balancing of Generative Models via Diversity Weights.](http://arxiv.org/abs/2304.11961) | 本研究提出了通过平衡训练数据集中的模式来增加模型输出多样性的多样性权重训练方案，以更好地适应需要多样化输出的创意应用，并在受控环境中进行的初步实验展示了其潜力。 |
| [^110] | [Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing.](http://arxiv.org/abs/2304.11839) | 本文提出了一种基于局部能量分布的随机模拟退火超参数确定方法，该方法通过中心极限定理估计局部能量的分布，将超参数搜索的时间复杂度从O(n^3)降低到O(1)，在解决最大割问题中的实验中表现良好。 |
| [^111] | [B-Learner: Quasi-Oracle Bounds on Heterogeneous Causal Effects Under Hidden Confounding.](http://arxiv.org/abs/2304.10577) | 本文提出了一种元学习器 B-Learner，它可以在限制隐藏混淆水平的情况下高效地学习 CATE 函数的尖锐界限。 |
| [^112] | [Tool Learning with Foundation Models.](http://arxiv.org/abs/2304.08354) | 基于基础模型的工具学习结合了专用工具和基础模型的优势，实现了问题解决的增强精度、效率和自动化。本文对工具学习进行了系统研究，提出了涵盖两种类型学习的通用工具学习框架，并分析了它们的独特挑战、机会和未来方向。 |
| [^113] | [Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'.](http://arxiv.org/abs/2304.08297) | 对陈等人发表在《自然—生物医学工程》杂志上的“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”一文的评论和关切。 |
| [^114] | [Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca.](http://arxiv.org/abs/2304.08177) | 这篇论文提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以改善LLaMA对中文的理解和生成能力。 |
| [^115] | [Joint optimization of a $\beta$-VAE for ECG task-specific feature extraction.](http://arxiv.org/abs/2304.06476) | 本文研究了使用$\beta$-VAE作为可解释特征提取器，并联合优化信号重建和心脏功能预测。在7255名患者的数据上进行测试，显示出相比先前方法，该方法显著改善了预测能力和可解释性。 |
| [^116] | [Collaborative Machine Learning Model Building with Families Using Co-ML.](http://arxiv.org/abs/2304.05444) | Co-ML是一个基于平板电脑的应用程序，用于协同构建ML图像分类器，可以帮助学习者在合作中发掘新的想法和方法，解决数据表现和多样性等关键问题。 |
| [^117] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^118] | [ASPEST: Bridging the Gap Between Active Learning and Selective Prediction.](http://arxiv.org/abs/2304.03870) | 本文提出了一种新的学习范式——主动选择性预测（ASPEST），它可以在转移目标领域中学习查询更多有信息的样本，从而实现减少人工标注工作的同时增加准确性和覆盖率。 |
| [^119] | [Re-thinking Model Inversion Attacks Against Deep Neural Networks.](http://arxiv.org/abs/2304.01669) | 本文重新审视深度学习中的模型逆推攻击，提出了一种改进的优化目标和一个新型的“模型增强”思路，可以显著提高攻击性能。 |
| [^120] | [Learning the Delay Using Neural Delay Differential Equations.](http://arxiv.org/abs/2304.01329) | 本文提出了一种基于时滞微分方程的连续时间神经网络方法，使用伴随灵敏度方法直接学习模型参数和时滞，具有学习DDE参数的能力。 |
| [^121] | [Kernel Affine Hull Machines for Differentially Private Learning.](http://arxiv.org/abs/2304.01300) | 本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。 |
| [^122] | [Machine-Learned Premise Selection for Lean.](http://arxiv.org/abs/2304.00994) | 该论文介绍了一种基于机器学习的工具，可为 Lean 证明助手建议与用户正在证明的定理相关的前提条件。 |
| [^123] | [FP8 versus INT8 for efficient deep learning inference.](http://arxiv.org/abs/2303.17951) | 本文比较了FP8和INT8在设备高效推理中的性能，展示了量化和量化感知训练的成果，为选择正确的数字格式提供了参考。 |
| [^124] | [Contextual Combinatorial Bandits with Probabilistically Triggered Arms.](http://arxiv.org/abs/2303.17110) | 本文研究了带有概率触发臂的情境组合赌博机，在不同条件下设计了C$^2$-UCB-T算法和VAC$^2$-UCB算法，并分别导出了对应的遗憾值上限，为相关应用提供了理论支持。 |
| [^125] | [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention.](http://arxiv.org/abs/2303.16199) | 本文提出了一种基于适应提示和零初始化注意力机制的轻量级语言模型调整方法，可高效微调LLaMA为指令跟随模型，具有比Alpaca更短的微调时间并具有近似的响应质量。 |
| [^126] | [MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation.](http://arxiv.org/abs/2303.09975) | MedNeXt是一个定制化的现代化可扩展卷积神经网络，用于解决数据稀缺的医学环境挑战。该网络包含：完全ConvNeXt 3D编码器-解码器网络、残差ConvNeXt上下采样块和一种新的迭代增加核大小的技术。 |
| [^127] | [Fast exploration and learning of latent graphs with aliased observations.](http://arxiv.org/abs/2303.07397) | 本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。 |
| [^128] | [Bounding the Optimal Value Function in Compositional Reinforcement Learning.](http://arxiv.org/abs/2303.02557) | 本文在组合强化学习中提出了一种通用的框架，将感兴趣的组合任务的最优解与已知的原始任务解决方案相关联，并提出了双侧不等式将最优组合值函数与原始任务的值函数相关联。 |
| [^129] | [iSAGE: An Incremental Version of SAGE for Online Explanation on Data Streams.](http://arxiv.org/abs/2303.01181) | iSAGE是一种基于增量学习的SAGE在线解释方法，具备快速、内存高效的特点。该方法能够对模型变化以及数据生成过程中的漂移进行反应，同时提供了有效的特征移除方法，具有和SAGE类似的理论性质。 |
| [^130] | [GNOT: A General Neural Operator Transformer for Operator Learning.](http://arxiv.org/abs/2302.14376) | 提出了一种通用神经运算符Transformer——GNOT，用于解决机器学习中学习偏微分方程的解算子的问题，并通过设计新颖的异构归一化注意力层和引入几何门控机制来增强模型的灵活性和解决多尺度问题。在多个领域的具有挑战性的数据集上进行广泛实验，取得了显着的改进。 |
| [^131] | [Reconstruction-based Out-of-Distribution Detection for Short-Range FMCW Radar.](http://arxiv.org/abs/2302.14192) | 本篇论文提出了一种新颖的基于重构的OOD检测器，以在雷达领域进行操作，利用自动编码器及其潜在表示来检测OOD样本，在实验数据集上获得了90.72%的AUROC。 |
| [^132] | [Optimistic Planning by Regularized Dynamic Programming.](http://arxiv.org/abs/2302.14004) | 本文提出了一种基于正则化动态规划的乐观规划方法，可用于学习折扣线性混合MDPs中的最优策略，且具有近乎最优的统计保证 |
| [^133] | [DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining.](http://arxiv.org/abs/2302.12445) | DeAR提出了一种新的调度算法，将All-Reduce原语分解成两个连续操作，与反向传播和前向计算同时重叠。使用实用的张量融合算法可以提高训练性能，实验结果表明DeAR在训练速度上可达到83%和15%的加速。 |
| [^134] | [Revisiting the Gumbel-Softmax in MADDPG.](http://arxiv.org/abs/2302.11793) | 本文探索了多种Gumbel-Softmax的替代方法，并将其应用于MADDPG中，以解决离散动作空间下的性能问题。 |
| [^135] | [Learning Manifold Dimensions with Conditional Variational Autoencoders.](http://arxiv.org/abs/2302.11756) | 该论文证明全局最优变分自编码器(CVAE)可以学习正确的流形维度，同时提出了一种新方法可以共同学习流形维度和条件分布，以在多个数据集上实现更好的特征分离和样本质量。 |
| [^136] | [MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection.](http://arxiv.org/abs/2302.10739) | 本文提出了一种专门为恶意软件检测领域设计的状态防御技术MalProtect，它通过实现一种新颖的查询分类方法来检测查询攻击，实验结果证明其有效性。 |
| [^137] | [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat.](http://arxiv.org/abs/2302.10289) | 本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。 |
| [^138] | [Multivariate Systemic Risk Measures and Computation by Deep Learning Algorithms.](http://arxiv.org/abs/2302.10183) | 本文提出了基于深度学习算法计算多元系统性缺口风险度量的方法，可以学习原始最优化程序、对偶表示的最优解以及公平风险分配。 |
| [^139] | [Improving Training Stability for Multitask Ranking Models in Recommender Systems.](http://arxiv.org/abs/2302.09178) | 本文研究了推荐系统中多任务排名模型训练的稳定性问题，提出了解决方案，这些方案旨在提高模型的可使用性，节省资源和促进模型的发展 |
| [^140] | [Mixed Traffic Control and Coordination from Pixels.](http://arxiv.org/abs/2302.09167) | 本研究考虑利用图像观察作为替代方法来进行混合交通控制。 |
| [^141] | [Pretraining Language Models with Human Preferences.](http://arxiv.org/abs/2302.08582) | 本论文探索了用人类反馈替代传统互联网文本来预训练语言模型的方法，其中条件训练是最优和简单的方法，可将不良内容的生成速率降低一个数量级，同时保持语言模型在下游任务上的性能。 |
| [^142] | [LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation.](http://arxiv.org/abs/2302.08191) | LightGCL是一种新的图对比学习方法，旨在解决现有方法中存在的不足。该模型采用奇异值分解进行对比增强，更好地保留了内在的语义结构，并提高了模型的通用性和鲁棒性。 |
| [^143] | [Bandit Social Learning: Exploration under Myopic Behavior.](http://arxiv.org/abs/2302.07425) | 该论文研究了自私行为下的劫匪社交学习问题，发现存在一种探索激励权衡，即武器探索和社交探索之间的权衡，受到代理的短视行为的限制会加剧这种权衡，并导致遗憾率与代理数量成线性关系。 |
| [^144] | [B-BACN: Bayesian Boundary-Aware Convolutional Network for Crack Characterization.](http://arxiv.org/abs/2302.06827) | 提出了一种贝叶斯边界感知卷积网络（B-BACN），用于生成精确可靠的裂纹边界检测，并同时量化认识不确定性和归属不确定性。 |
| [^145] | [Debiasing Recommendation by Learning Identifiable Latent Confounders.](http://arxiv.org/abs/2302.05052) | 本研究提出了一种新方法 iDCF，通过学习可识别的混淆因素来消除推荐偏差，该方法在真实和合成数据集上的实验表明有效性和理论保证。 |
| [^146] | [Towards Model-Agnostic Federated Learning over Networks.](http://arxiv.org/abs/2302.04363) | 该论文提出了一种适用于网络环境中多种数据和模型的模型无关关联学习方法，旨在通过网络结构反映本地数据集的相似性并保证本地模型产生一致的预测结果。 |
| [^147] | [Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs.](http://arxiv.org/abs/2302.03655) | 本文将SO(3)卷积降维至SO(2)，以减少等变卷积在高阶张量上的计算复杂度，并通过提出的等变球形通道网络（eSCN）在大规模OC-2数据集上获得最先进的结果。 |
| [^148] | [Stop overkilling simple tasks with black-box models and use transparent models instead.](http://arxiv.org/abs/2302.02804) | 过度使用黑匣子模型会导致简单任务的浪费，透明模型可以提高效率和精度。 |
| [^149] | [Tighter Information-Theoretic Generalization Bounds from Supersamples.](http://arxiv.org/abs/2302.02432) | 本文介绍了一种新颖的信息论泛化界限，利用投影损失对，与Rademacher序列相关联来源于超取样的设置，这些界限比同一超取样设置中迄今已知的所有信息理论界限都更紧密。 |
| [^150] | [Large language models predict human sensory judgments across six modalities.](http://arxiv.org/abs/2302.01308) | 本研究表明，最先进的大型语言模型能预测人类在六个感官模态下的感知评判，并能提供从语言中提取感知信息的下限。 |
| [^151] | [Mnemosyne: Learning to Train Transformers with Transformers.](http://arxiv.org/abs/2302.01128) | Mnemosyne优化器使用Performers方法来学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节，并成功训练ViTs和应用于机器人领域中，具有更好的泛化能力与快速收敛。 |
| [^152] | [Grounding Language Models to Images for Multimodal Inputs and Outputs.](http://arxiv.org/abs/2301.13823) | 该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。 |
| [^153] | [SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces.](http://arxiv.org/abs/2301.11832) | SOBER算法是一种在离散和混合空间上进行高并行贝叶斯优化的方法，能够进行可扩展和多样化的批量全局优化和积分，且优于11个竞争基线方法。 |
| [^154] | [Diffusion-based Conditional ECG Generation with Structured State Space Models.](http://arxiv.org/abs/2301.08227) | 本研究提出了将扩散模型和结构化状态空间模型相结合的新技术SSSD-ECG，在根据70多个心电图语句生成合成12导联心电图方面表现出色。 |
| [^155] | [Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks.](http://arxiv.org/abs/2301.06683) | 本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。 |
| [^156] | [A Theoretical Framework for AI Models Explainability with Application in Biomedicine.](http://arxiv.org/abs/2212.14447) | 该论文提出了一种新的解释定义和理论框架，用于评估AI模型的可解释性，特别是在生物医学领域。这个框架可帮助确定模型决策中最具影响力的输入特征，支持对模型行为的解释。 |
| [^157] | [MixupE: Understanding and Improving Mixup from Directional Derivative Perspective.](http://arxiv.org/abs/2212.13381) | 本文从方向导数的角度分析了深度神经网络中常用的数据增强技术Mixup，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一思路，作者提出了改进版的Mixup，理论上证明具有更好的泛化性能，并在各种领域的数据集上进行了验证，表现出比Mixup更好的效果。 |
| [^158] | [Scalable Adaptive Computation for Iterative Generation.](http://arxiv.org/abs/2212.11972) | 本文提出了循环接口网络（RINs）结构，将核心计算与数据维数分离，实现了自适应计算，解决了高维数据生成的可伸缩性问题。这一结构将大部分计算集中在潜在标记上，使用交叉注意力在潜在标记和数据标记之间路由信息，具有很好的生成效果并可扩展到数十万维的数据集上。 |
| [^159] | [Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners.](http://arxiv.org/abs/2212.10873) | 本论文提出了一种混合线性探测和上下文学习的方法，结合了两者的优点，旨在提高模型在少样本和零样本情况下的性能表现。 |
| [^160] | [Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language.](http://arxiv.org/abs/2212.07525) | 本文提出了一种上下文化目标表示自监督学习的高效性改进方法，称为data2vec 2.0，它在多项任务中取得了和其他算法相当的准确率，但需要的预训练时间较短。 |
| [^161] | [TIDE: Time Derivative Diffusion for Deep Learning on Graphs.](http://arxiv.org/abs/2212.02483) | 本文提出了一种新方法 TIDE，通过时间导数图扩散克服了图神经网络中消息传递框架的结构限制，实现了高效地中长距离通信，并在图神经网络任务中达到了 state-of-the-art 的性能表现。 |
| [^162] | [Second-order optimization with lazy Hessians.](http://arxiv.org/abs/2212.00781) | 本论文提出了一种用于解决可能为非凸问题的二阶优化算法，使用懒惰Hessian更新和重用先前看到的Hessian，可显著降低总算术复杂度，并提高了效率。 |
| [^163] | [Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations.](http://arxiv.org/abs/2211.17244) | 本文提出了一种新的，基于低秩半正定松弛技术实现对对抗性训练神经网络的严格认证方法，它能够实现采用更便宜的SDP方法相当的强认证。 |
| [^164] | [Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs.](http://arxiv.org/abs/2211.16468) | 在因果图中，提出了解决前门调整的线性时间算法，通过观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。 |
| [^165] | [GREAD: Graph Neural Reaction-Diffusion Networks.](http://arxiv.org/abs/2211.14208) | 本文介绍了一种基于反应扩散方程的GNN方法，考虑了所有流行的反应方程类型和一种特殊的反应方程，是目前其中最全面的研究之一，并在实验中表现出更好的性能。 |
| [^166] | [Where Will Players Move Next? Dynamic Graphs and Hierarchical Fusion for Movement Forecasting in Badminton.](http://arxiv.org/abs/2211.12217) | 该研究提出了一种新颖的动态图和层次融合模型，用于预测羽毛球比赛中球员移动，该模型将基于序列的模型和基于图形的模型相结合，并具有一个层次融合层，以考虑时空依赖性。实验结果表明了该方法在真实数据上的有效性。 |
| [^167] | [Adversarial Cheap Talk.](http://arxiv.org/abs/2211.11030) | 本文提出了一种新型对抗性设置，在其中对手只能将信息附加到受害者的观察中，从而产生最小的影响范围，并提出对抗性廉价交流（ACT）算法进行对手训练。在高度受限的情况下，使用ACT训练的对手仍会对受害者的训练和测试表现产生显著影响，揭示了强化学习算法中的一种新的攻击向量。 |
| [^168] | [HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization.](http://arxiv.org/abs/2211.10250) | 本文提出了一种采用人工蜂群优化的神经架构搜索框架HiveNAS，它在极短的时间内就能超越其他基于群智的NAS框架，成为最优。 |
| [^169] | [Physics-informed neural networks for gravity currents reconstruction from limited data.](http://arxiv.org/abs/2211.09715) | 本文研究了物理信息神经网络在受限数据情况下重建重力气流方面的应用，通过光衰减技术(LAT)平均空间密度测量进行训练，结果表明该方法可以在少量测量值下快速、精确地重建流场，并且与实验数据比较表明了其可靠性。 |
| [^170] | [Bayesian Fixed-Budget Best-Arm Identification.](http://arxiv.org/abs/2211.08572) | 本文提出一种贝叶斯消除算法，用于解决固定预算下最佳臂识别问题，并且推导了其与先验相关的误识别上界，此算法优于频率学派方法，与无保证的贝叶斯算法相竞争。 |
| [^171] | [SPADE4: Sparsity and Delay Embedding based Forecasting of Epidemics.](http://arxiv.org/abs/2211.08277) | SPADE4是一种基于稀疏性和时滞嵌入的流行病预测方法，利用随机特征模型和Takens的时滞嵌入定理处理数据稀缺和捕捉基础系统的性质。 |
| [^172] | [Improved disentangled speech representations using contrastive learning in factorized hierarchical variational autoencoder.](http://arxiv.org/abs/2211.08191) | 本文引入对比学习在因式分层变分自编码器中，实现更好的语音表征，通过捕获说话者身份改善模型性能，达到最新水平。 |
| [^173] | [Directional Privacy for Deep Learning.](http://arxiv.org/abs/2211.04686) | 本文采用基于von Mises-Fisher分布的机制应用方向隐私来保护深度学习模型训练隐私，并提供了$\epsilon d$-隐私保证，可根据输入梯度的差异平滑退化。 |
| [^174] | [LMD: A Learnable Mask Network to Detect Adversarial Examples for Speaker Verification.](http://arxiv.org/abs/2211.00825) | LMD是一种攻击者独立和可解释的方法，用于检测语音验证中的对抗性样本，其核心是通过神经网络生成掩蔽谱图，利用ASV分数的绝对差异来检测对抗性样本。 |
| [^175] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^176] | [Stable Deep MRI Reconstruction using Generative Priors.](http://arxiv.org/abs/2210.13834) | 本文提出了一种基于生成图像先验的深度神经网络正则化器，通过在参考图像上训练编码高层次的域统计信息，可以实现在不受子采样方式的影响下高质量稳定的MRI重建，并提供了概率解释进行不确定性量化。 |
| [^177] | [Continual Vision-based Reinforcement Learning with Group Symmetries.](http://arxiv.org/abs/2210.12301) | 本论文提出了一种名为COVERS的连续视觉强化学习方法，它能够识别基本群操作下等价的任务，并为每组等价任务培养一种策略，提高样本效率，减少唯一策略的数量，并实现了更强的泛化性能。 |
| [^178] | [On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning.](http://arxiv.org/abs/2210.10763) | 本文研究了是否可以利用现代基于模型的强化学习算法学习的内部模型来更加快速地解决新的、明显不同的任务。我们提出了一种基于模型的跨任务转移框架，通过离线多任务预训练和在线跨任务微调，获得了在各种模拟机器人操作任务中显著的样本效率改进。 |
| [^179] | [Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems.](http://arxiv.org/abs/2210.10636) | 本文提出了使用干预方法来提高文本匹配推荐系统的跨领域泛化能力。研究发现，常用的基于精调模型的方法在具有新领域数据时有反效果，为此，提出了基于干预的重要性度量来解释泛化失败的原因。 |
| [^180] | [Pareto Manifold Learning: Tackling multiple tasks via ensembles of single-task models.](http://arxiv.org/abs/2210.09759) | 提出了Pareto流形学习方法，通过在参数空间中进行线性参数化，实现持续的Pareto前沿产生，并可以在一个模型中实现多个任务的优化，同时提高所有任务的效果。 |
| [^181] | [PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting.](http://arxiv.org/abs/2210.08964) | 提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。 |
| [^182] | [How Does Pseudo-Labeling Affect the Generalization Error of the Semi-Supervised Gibbs Algorithm?.](http://arxiv.org/abs/2210.08188) | 本文研究了Gibbs算法下的伪标记半监督学习的泛化误差，发现泛化性能受到标记和伪标记数据样本之间共享的信息的影响，这对于选择伪标记方法提供了指导。 |
| [^183] | [CORL: Research-oriented Deep Offline Reinforcement Learning Library.](http://arxiv.org/abs/2210.07105) | CORL是一个面向研究的深度强化学习离线库，提供了经过充分基准测试的单文件实现离线和离线到在线强化学习算法，并具有简单的开发体验和实验跟踪功能。 |
| [^184] | [FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation.](http://arxiv.org/abs/2210.04296) | 本文提出了FP-Diffusion方法来改进基于得分的扩散模型，通过强制底层得分的福克-普朗克方程来正则化DSM目标函数，以提高模型似然度和守恒程度。 |
| [^185] | [MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning.](http://arxiv.org/abs/2210.04183) | 本文提出了一种联合掩膜多模态建模方法，以学习细粒度的多模态表示，通过隐式和显式目标来恢复联合掩膜信号以提高细化的图像-文本交互。 |
| [^186] | [Topological Singularity Detection at Multiple Scales.](http://arxiv.org/abs/2210.00069) | 本文提出了一种多尺度拓扑奇异性检测方法，可以评估数据的局部固有维度，并量化点的“流形度”，能够检测复杂空间和图像中的奇异性。 |
| [^187] | [Analysis and Comparison of Classification Metrics.](http://arxiv.org/abs/2209.05355) | 本文回顾并比较了常用于度量分类系统表现的各种指标，发现期望成本指标具有更广泛的适用性和直观性，并可用于解决从连续得分生成分类决策的实践问题。 |
| [^188] | [White-Box Adversarial Policies in Deep Reinforcement Learning.](http://arxiv.org/abs/2209.02167) | 本文研究了在强化学习中训练对抗策略的方法，提出了基于白盒攻击的策略，能够访问目标代理的内部状态，从而识别其漏洞，攻击成功率更高。 |
| [^189] | [Early heart disease prediction using hybrid quantum classification.](http://arxiv.org/abs/2208.08882) | 本文提出了两种量子机器学习方法，分别适用于高维和低维问题，并在Cleveland和Statlog数据集上实验表明这些方法更适合于早期心脏疾病预测，可获得高达96.43％和97.78％的曲线下面积。 |
| [^190] | [Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise.](http://arxiv.org/abs/2208.08003) | 本文研究发现标签噪声会导致双丘降曲线出现“最终上升”，即在足够大的噪声样本比率下，中等宽度下实现最佳泛化性能。随机丢弃可训练参数来减少密度可在标签噪声下改善泛化性能。 |
| [^191] | [How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies.](http://arxiv.org/abs/2207.04581) | 本文提出了一个新的标准来衡量公平优化策略的稳健性——稳健比率，并使用三种公平策略在五个公平数据集上进行了多次广泛的实验。结果表明，公平策略的稳健性在不同数据集之间和不同公平性定义之间存在显着差异。 |
| [^192] | [Using Neural Networks for Novelty-based Test Selection to Accelerate Functional Coverage Closure.](http://arxiv.org/abs/2207.00445) | 本文提出了一个基于神经网络的新颖测试选择框架，可显著加速覆盖闭合，三个测试配置明显优于随机测试，最大节省率达49.37%。 |
| [^193] | [HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection.](http://arxiv.org/abs/2206.15157) | HRFuser是一种多分辨率传感器融合架构，可用于二维物体检测，基于高分辨率网络和新型多窗口交叉注意力块进行多模态多分辨率融合。在nuScenes和DENSE上的实验证明了其有效性。 |
| [^194] | [Subject Granular Differential Privacy in Federated Learning.](http://arxiv.org/abs/2206.03617) | 本文提出了两个新算法，可以在每个联邦用户本地实现主体级别的差分隐私保护。用户级别的局部差分隐私可以自然地保证主体级别的差分隐私，并通过真实世界数据集上的实验验证了算法的有效性。 |
| [^195] | [The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization.](http://arxiv.org/abs/2206.02768) | 本文研究了前馈神经网络初始化时的随机协方差矩阵分布，发现对激活函数进行形状塑造可以使协方差矩阵是非退化的，而随机协方差矩阵受到神经协方差SDE的随机微分方程的控制。 |
| [^196] | [AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning.](http://arxiv.org/abs/2205.15319) | 本文提出了一种名为AdaProp的方法，该方法可自适应地过滤掉不相关的实体，同时保留有前途的目标，从而高效、强大地进行知识图谱推理。 |
| [^197] | [PLAtE: A Large-scale Dataset for List Page Web Extraction.](http://arxiv.org/abs/2205.12386) | 这个工作介绍了一个名为PLAtE的大规模列表页网络抽取数据集，用于从产品评论页面中提取商品列表和产品属性。数据集由52,898个项目和156,014个属性组成，是第一个大规模的列表页网络抽取数据集。 |
| [^198] | [Automated Scoring for Reading Comprehension via In-context BERT Tuning.](http://arxiv.org/abs/2205.09864) | 本文介绍了一种基于预训练语言模型的文本表示和上下文BERT微调的阅读理解自动评分模型，解决了单个问题/题目模型无法利用题目间关联性以及存储模型困难的问题。 |
| [^199] | [Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows.](http://arxiv.org/abs/2204.13939) | 本文提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，用于短期低压负荷预测，相比传统方法表现更好，可用于规划和运营低碳能源系统。 |
| [^200] | [Bilinear value networks.](http://arxiv.org/abs/2204.13695) | 提出了一种通过点积低秩近似来表示Q值的双线性分解方法，其中第一个向量场捕捉状态的局部动态，第二个部分捕捉当前状态和目标之间的全局关系，该方法能够显著提高数据效率，并具有很好的泛化性能。 |
| [^201] | [Detection of sepsis during emergency department triage using machine learning.](http://arxiv.org/abs/2204.07657) | 本研究利用机器学习开发出一种检测急诊科分诊前败血症的模型，其性能优于标准败血症筛查算法。 |
| [^202] | [Visualizing Deep Neural Networks with Topographic Activation Maps.](http://arxiv.org/abs/2204.03528) | 本文提出使用拓扑激活图来可视化深度神经网络及其决策过程，提高了 DNN 的可解释性。 |
| [^203] | [Topological Experience Replay.](http://arxiv.org/abs/2203.15845) | 本文提出了一种拓扑经验回放的方法，通过构建图来明确状态的 Q 值之间的依赖关系，解决了传统采样策略忽视状态间依赖关系的问题，提高了学习深度 Q 函数时的性能和准确性。 |
| [^204] | [SC2 Benchmark: Supervised Compression for Split Computing.](http://arxiv.org/abs/2203.08875) | 该研究提出了一种基于监督式压缩的划分式计算方案（SC2），以有效地将神经网络计算分配给移动设备和边缘服务器；使用新的度量标准对其进行全面评估，发现在压缩数据大小和性能之间获得了更好的平衡。 |
| [^205] | [On the Omnipresence of Spurious Local Minima in Certain Neural Network Training Problems.](http://arxiv.org/abs/2202.12262) | 研究了具有一维实数输出且激活函数包含仿射段以及隐藏层至少有两个节点的深层人工神经网络的训练问题损失景观，发现这类问题对于所有不是仿射的目标函数，都存在一系列虚假的局部极小值，这是由通用逼近定理直接推导得出的。 |
| [^206] | [Compositional Scene Representation Learning via Reconstruction: A Survey.](http://arxiv.org/abs/2202.07135) | 这篇论文综述了利用深度神经网络学习组合场景表示并通过重构的各种方法，这些方法能够利用大规模未标记数据，避免昂贵和费时的数据注释。 |
| [^207] | [Distributionally Robust Data Join.](http://arxiv.org/abs/2202.05797) | 本论文提出了一个解决数据合并问题的方法，该方法通过最大化所有概率分布上的最大损失，来构建分布式稳健预测器。 |
| [^208] | [Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks.](http://arxiv.org/abs/2202.02947) | 本文提出了并行连续学习（PSL），通过网络、异构性和接近性三个维度的扩展，实现了在无线设备集合上的动态分布式模型训练。 |
| [^209] | [Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks.](http://arxiv.org/abs/2202.00293) | 本文研究了高维两层神经网络中随机梯度下降的相图，探究了窄网络和过参数化浅层网络之间的交界处，并研究了三个方面变量之间的相互作用，工作建立在统计物理的框架下。 |
| [^210] | [Cognitive Ledger Project: Towards Building Personal Digital Twins Through Cognitive Blockchain.](http://arxiv.org/abs/2201.08163) | 该论文提出了一个认知数字孪生体的认知架构，即通过基于区块链基础设施的方式将用户的个人数据转化为结构化信息和机器学习模型，最终能够一起组成用户的认知数字孪生体。 |
| [^211] | [$m^\ast$ of two-dimensional electron gas: a neural canonical transformation study.](http://arxiv.org/abs/2201.03156) | 本研究开发了一种神经网络方法，直接计算低温下的热熵来提取二维自旋极化电子气体的有效质量$m^\ast$，其揭示了有效质量在低密度强耦合区域的更加明显的抑制作用。该预测需要在实验中进行验证。 |
| [^212] | [Towards Green Automated Machine Learning: Status Quo and Future Directions.](http://arxiv.org/abs/2111.05850) | 本文提出了一个名为 "绿色自动化机器学习" 的概念，旨在使整个 AutoML 过程更加环保。本文重点研究如何量化 AutoML 工具的环境足迹，并总结了有关如何针对其 "绿色性"，即可持续性，设计和基准测试 AutoML 工具的不同策略。 |
| [^213] | [Focusing on Potential Named Entities During Active Label Acquisition.](http://arxiv.org/abs/2111.03837) | 本文提出了几个AL句子查询评估函数，关注潜在正面标记，并使用更好的数据驱动的正常化方法，以最小化NER注释成本。 |
| [^214] | [QuantumNAT: Quantum Noise-Aware Training with Noise Injection, Quantization and Normalization.](http://arxiv.org/abs/2110.11331) | QuantumNAT是一个PQC特定框架，可以在训练和推断阶段执行噪声感知优化，提高鲁棒性，缓解量子噪声 |
| [^215] | [Entropic Issues in Likelihood-Based OOD Detection.](http://arxiv.org/abs/2109.10794) | 本文研究了最大似然训练的深度生成模型的OOD检测问题，提出了一种新的观察角度，即将平均似然分解为KL散度项和熵项。后者可以解释模型可能会给OOD数据高似然值的现象，因为它抑制具有更高熵的数据集上的似然值。 |
| [^216] | [Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses.](http://arxiv.org/abs/2106.09779) | 本文研究了无需信任服务器或其他数据源的跨 silo 联邦学习，考虑了跨 silo 记录级差分隐私 ISRL-DP。该算法可以确保来自每个人的数据都不会被泄漏。 |
| [^217] | [Autoencoding Under Normalization Constraints.](http://arxiv.org/abs/2105.05735) | 本文介绍了一种基于自编码机的规范化概率模型，称为规范化自编码机（NAE）。该模型通过抑制负样本的重构来强制执行正则化，有效提高了异常检测性能。 |
| [^218] | [Robust Sample Weighting to Facilitate Individualized Treatment Rule Learning for a Target Population.](http://arxiv.org/abs/2105.00581) | 本文研究了个体化治疗规则的泛化问题，提出一种加权框架方法以缓解因规定的函数类不包含最优规则而导致的影响。 |
| [^219] | [WARM: A Weakly (+Semi) Supervised Model for Solving Math word Problems.](http://arxiv.org/abs/2104.06722) | 研究提出了一种弱监督模型WARM来解决用于自然语言处理中的数学题。通过仅用期望答案作为监督，该方法通过学习生成方程来解决问题，并在无需使用方程作为监督的情况下，成功实现了相比最先进的弱监督方法更高的精度提升。 |
| [^220] | [Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information.](http://arxiv.org/abs/2011.07442) | 本文提出了一种新的语音增强方法，通过利用广义语音类别序列的损失来提高SE性能，实验证明上下文BPC信息可以提高性能。 |
| [^221] | [Investigating Membership Inference Attacks under Data Dependencies.](http://arxiv.org/abs/2010.12112) | 本文研究了在数据具有依赖性的情况下成员推断攻击的影响，并表明了DP无法在这种情况下提供有意义的保护，需要探索替代的防御策略来提供更强隐私保证。 |
| [^222] | [Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?.](http://arxiv.org/abs/2007.00112) | 本文通过观察不同种类的图像转换后深度卷积神经网络（DCNNs）的表现，探讨了不变神经表示是否促进了跨类别的图像鲁棒性。实验表明，跨类别变换的鲁棒性是由不变表示所促进的，支持鲁棒性是由不变性驱动而不是专门的子网络的假设。 |
| [^223] | [We Should at Least Be Able to Design Molecules That Dock Well.](http://arxiv.org/abs/2006.16955) | 该文提出了一个基于对接的基准测试来评估分子结合蛋白质的流行计算方法，探究了目前基于图形的生成模型在新型药物设计上的局限性，并提出了新的基准测试版本。 |
| [^224] | [Volterra Neural Networks (VNNs).](http://arxiv.org/abs/1910.09616) | 该论文提出了一种Volterra滤波器启发的神经网络架构，旨在通过输入数据的延迟采样之间的交互引入受控的非线性，以降低卷积神经网络的复杂性。通过串行实现Volterra滤波器，减少了进行与传统神经网络相同的分类任务所需的参数数量。此外，作者还展示了该网络对视频的RGB信息和光流信息进行非线性融合的自适应。 |

# 详细

[^1]: 使用正交序列的差分隐私无线联合学习方法

    Differentially Private Wireless Federated Learning Using Orthogonal Sequences. (arXiv:2306.08280v1 [cs.IT])

    [http://arxiv.org/abs/2306.08280](http://arxiv.org/abs/2306.08280)

    本文提出了一种使用正交序列的FLORAS方法，可消除发送端的信道状态信息，同时提供了项目级和客户级的差分隐私保证。FLORAS可以灵活地实现不同的差分隐私等级，并且通过推导收敛界限，实现了收敛速度和隐私保证之间的平稳权衡。

    

    本文提出了一种新的隐私保护上行空中计算方法FLORAS，用于单输入单输出（SISO）无线联合学习（FL）系统。FLORAS从通信设计的角度出发，利用正交序列的性质消除了发送端的信道状态信息（CSIT）要求。从隐私保护的角度来看，我们证明FLORAS可以提供项目级和客户级差分隐私（DP）保证。此外，通过调整系统参数，FLORAS可以在不增加成本的情况下灵活地实现不同的DP等级。我们推导出了一个新的FL收敛界限，结合隐私保证，可以在收敛速度和差分隐私级别之间实现平稳的权衡。数值结果证明了FLORAS相对于基准AirComp方法的优势，并验证了我们的分析结果可以指导不同权衡条件下的隐私保护FL的设计。

    We propose a novel privacy-preserving uplink over-the-air computation (AirComp) method, termed FLORAS, for single-input single-output (SISO) wireless federated learning (FL) systems. From the communication design perspective, FLORAS eliminates the requirement of channel state information at the transmitters (CSIT) by leveraging the properties of orthogonal sequences. From the privacy perspective, we prove that FLORAS can offer both item-level and client-level differential privacy (DP) guarantees. Moreover, by adjusting the system parameters, FLORAS can flexibly achieve different DP levels at no additional cost. A novel FL convergence bound is derived which, combined with the privacy guarantees, allows for a smooth tradeoff between convergence rate and differential privacy levels. Numerical results demonstrate the advantages of FLORAS compared with the baseline AirComp method, and validate that our analytical results can guide the design of privacy-preserving FL with different tradeoff 
    
[^2]: FRIGATE:公路网络的节约时空预测

    FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks. (arXiv:2306.08277v1 [cs.LG])

    [http://arxiv.org/abs/2306.08277](http://arxiv.org/abs/2306.08277)

    FRIGATE是一种使用时空GNN进行公路网络预测的方法，可以在不依赖每个节点的感知、完整的感知历史记录或静态网络假设的情况下工作，在真实交通数据集上表现出更高的预测性能。

    

    建模公路网络上的时空过程是一个越来越重要的任务。虽然在发展时空图神经网络（GNNs）方面取得了重要进展，但现有的工作建立在三个不适用于实际公路网络的假设基础上。首先，它们假定每个节点都配备传感器进行感知。然而在现实中，由于预算限制或传感器故障，所有位置（节点）可能没有配备传感器。其次，它们假定所有安装的传感器都具有感知历史记录。然而由于传感器故障、通信中的数据丢失等原因，这也是不现实的。最后，假设静态的道路网络。网络内连通性会由于道路封闭、新路修建等原因而发生变化。在这项工作中，我们开发了FRIGATE来解决所有这些缺点。FRIGATE由时空GNN驱动，将位置、拓扑和时间信息集成到节点表示中，用于预测。通过利用图神经网络的表现力，FRIGATE可以学习预测公路网络上时空过程的演变，而无需每个节点的感知、完整的感知历史记录或静态网络的假设。我们在真实交通数据集上评估FRIGATE，并证明其相对于现有最先进的方法具有更优异的预测性能。

    Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop FRIGATE to address all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representatio
    
[^3]: 使用聚合特征或邻接表在有向或无向图中，为什么要选择哪个？实证研究和简单分类方法。

    Why Using Either Aggregated Features or Adjacency Lists in Directed or Undirected Graph? Empirical Study and Simple Classification Method. (arXiv:2306.08274v1 [cs.LG])

    [http://arxiv.org/abs/2306.08274](http://arxiv.org/abs/2306.08274)

    本文实证研究了在节点分类任务中使用聚合特征或邻接表以及边方向对分类效果的影响，并提出了一个简单全面的分类方法A2DUG，该方法利用了有向和无向图中节点表示变量的所有组合，其在各种数据集上稳定表现良好，并在几个数据集上优于当前的最先进方法。

    

    节点分类是图分析中最热门的任务之一。本文关注节点表示（聚合特征 vs. 邻接表）和输入图的边方向（有向 vs. 无向）选择对分类结果的影响。我们进行了第一项实证研究，以基准测试使用不同节点表示和边方向的各种GNNs的性能。我们的实验表明，在数据集中没有单一的组合稳定地实现了最先进的结果，这表明我们需要根据数据集的特征选择合适的组合。为此，我们提出了一个简单但全面的分类方法A2DUG，该方法利用有向和无向图中节点表示变量的所有组合。我们表明，A2DUG在各种数据集上稳定表现良好。令人惊讶的是，在几个数据集上，它在很大程度上优于当前的最先进方法。

    Node classification is one of the hottest tasks in graph analysis. In this paper, we focus on the choices of node representations (aggregated features vs. adjacency lists) and the edge direction of an input graph (directed vs. undirected), which have a large influence on classification results. We address the first empirical study to benchmark the performance of various GNNs that use either combination of node representations and edge directions. Our experiments demonstrate that no single combination stably achieves state-of-the-art results across datasets, which indicates that we need to select appropriate combinations depending on the characteristics of datasets. In response, we propose a simple yet holistic classification method A2DUG which leverages all combinations of node representation variants in directed and undirected graphs. We demonstrate that A2DUG stably performs well on various datasets. Surprisingly, it largely outperforms the current state-of-the-art methods in several
    
[^4]: 基于2D圆形核时间序列转换、熵度量和机器学习方法的太阳活动成像跟踪

    Imagery Tracking of Sun Activity Using 2D Circular Kernel Time Series Transformation, Entropy Measures and Machine Learning Approaches. (arXiv:2306.08270v1 [astro-ph.SR])

    [http://arxiv.org/abs/2306.08270](http://arxiv.org/abs/2306.08270)

    本文开发了一种基于2D圆形核时间序列转换、熵度量和机器学习方法的太阳活动成像跟踪技术，可以将太阳观测图像转换为1维时间序列，并提取特征进行机器学习分类，用于追踪太阳的活动情况，尤其在识别“太阳风暴”方面准确性较高。

    

    太阳的性质非常复杂，其观测图像特征是了解太阳活动、空间和地球天气条件最重要的信息来源之一。本研究开发了一种技术，使用2D圆形核时间序列转换、统计和熵度量以及机器学习方法追踪太阳活动。该技术将太阳观测图像转换为1维时间序列，然后使用统计和熵度量或直接分类等方法从中提取特征，用于机器学习分类为“太阳风暴”和“非风暴”。实验结果表明，该模型追踪太阳活动的潜在准确性为约.

    The sun is highly complex in nature and its observatory imagery features is one of the most important sources of information about the sun activity, space and Earth's weather conditions. The NASA, solar Dynamics Observatory captures approximately 70,000 images of the sun activity in a day and the continuous visual inspection of this solar observatory images is challenging. In this study, we developed a technique of tracking the sun's activity using 2D circular kernel time series transformation, statistical and entropy measures, with machine learning approaches. The technique involves transforming the solar observatory image section into 1-Dimensional time series (1-DTS) while the statistical and entropy measures (Approach 1) and direct classification (Approach 2) is used to capture the extraction features from the 1-DTS for machine learning classification into 'solar storm' and 'no storm'. We found that the potential accuracy of the model in tracking the activity of the sun is approxim
    
[^5]: LargeST: 一个面向大规模交通预测的基准数据集

    LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting. (arXiv:2306.08259v1 [cs.LG])

    [http://arxiv.org/abs/2306.08259](http://arxiv.org/abs/2306.08259)

    LargeST数据集是一个更为现实和具有挑战性的交通预测基准，包括8600个传感器、覆盖5年时间和包括细致元数据。

    

    交通预测在智慧城市项目中扮演着至关重要的角色，并通过深度学习捕捉交通数据的非线性模式取得了显著的进展。然而，目前公共数据集上取得的有前途的结果可能不适用于实际场景，因为这些数据集存在局限性。为了解决这些问题，我们引入了一个名为LargeST的基准数据集，包括8600个传感器、覆盖5年时间和包括细致元数据。我们使用LargeST进行深入数据分析并演示了它相对于现有数据集而言是一个更为现实和具有挑战性的交通预测基准。

    Traffic forecasting plays a critical role in smart city initiatives and has experienced significant advancements thanks to the power of deep learning in capturing non-linear patterns of traffic data. However, the promising results achieved on current public datasets may not be applicable to practical scenarios due to limitations within these datasets. First, the limited sizes of them may not reflect the real-world scale of traffic networks. Second, the temporal coverage of these datasets is typically short, posing hurdles in studying long-term patterns and acquiring sufficient samples for training deep models. Third, these datasets often lack adequate metadata for sensors, which compromises the reliability and interpretability of the data. To mitigate these limitations, we introduce the LargeST benchmark dataset. It encompasses a total number of 8,600 sensors with a 5-year time coverage and includes comprehensive metadata. Using LargeST, we perform in-depth data analysis to extract dat
    
[^6]: 基于生成扩散模型的癫痫预测数据增强方法

    Data Augmentation for Seizure Prediction with Generative Diffusion Model. (arXiv:2306.08256v1 [eess.SP])

    [http://arxiv.org/abs/2306.08256](http://arxiv.org/abs/2306.08256)

    该论文提出了一种基于扩散模型的数据增强方法DiffEEG，可以有效地提高癫痫预测的性能，超过了现有的数据扩增方法。

    

    目标：癫痫预测对于改善患者生活质量具有重要意义，重点在于区分发作前状态与发作后状态。随着机器学习的发展，癫痫预测方法取得了显著进展。然而，发作前与发作后状态数据之间的严重不平衡仍然是一个巨大的挑战，限制了分类器的性能。数据扩增是解决这个问题的一个直观方法。现有的数据扩增方法通过重叠或重新组合数据来生成样本。由于这些转换无法完全探索特征空间并提供新信息，所以生成的样本分布受到原始数据的限制。由于癫痫脑电图表示在不同发作之间具有差异性，这些生成的样本不能提供足够的多样性以在新的癫痫发作中实现高性能。因此，我们提出了一种使用扩散模型的新型数据增强方法DiffEEG。方法：扩散模型是一种建模数据分布的强大工具，我们使用此模型来对原始脑电图数据进行转换以生成多样性的样本，进而提高分类器的性能。结果：DiffEEG在神经网络和SVM模型上进行的实验表明，它可以有效地提高癫痫预测的性能，超过了现有的数据扩增方法。

    Objective: Seizure prediction is of great importance to improve the life of patients. The focal point is to distinguish preictal states from interictal ones. With the development of machine learning, seizure prediction methods have achieved significant progress. However, the severe imbalance problem between preictal and interictal data still poses a great challenge, restricting the performance of classifiers. Data augmentation is an intuitive way to solve this problem. Existing data augmentation methods generate samples by overlapping or recombining data. The distribution of generated samples is limited by original data, because such transformations cannot fully explore the feature space and offer new information. As the epileptic EEG representation varies among seizures, these generated samples cannot provide enough diversity to achieve high performance on a new seizure. As a consequence, we propose a novel data augmentation method with diffusion model called DiffEEG. Methods: Diffusi
    
[^7]: 一种用于自闭症干预分析的多模态数据集MMASD。

    MMASD: A Multimodal Dataset for Autism Intervention Analysis. (arXiv:2306.08243v1 [cs.CV])

    [http://arxiv.org/abs/2306.08243](http://arxiv.org/abs/2306.08243)

    提出了一个名为MMASD的自闭症多模态数据集，收集自治疗干预。它包括从32名自闭症患儿的干预录音中分段的1,315个数据样本，每个样本包含四种隐私保护模式的数据。

    

    自闭症谱系障碍（ASD）是一种发育性疾病，其特征是重大的社交沟通障碍和困难的知觉和表达沟通提示。机器学习技术已广泛应用于促进自闭症研究和评估。然而，计算模型主要集中于特定分析，并在自闭症社区的私有数据集上进行验证，这限制了由于数据共享复杂性而跨模型的比较。本研究提出了一种新的隐私保护开源数据集MMASD作为多模式ASD基准数据集，收集自患有自闭症儿童的游戏治疗干预。MMASD包括32名患有ASD的儿童的数据，以及从100多小时的干预录音中分段的1,315个数据样本。为促进公共访问，每个数据样本包含四种隐私保护模式的数据：（1）光流，（2）2D骨架，（3）3D骨架和（4）临床医生ASD评估。

    Autism spectrum disorder (ASD) is a developmental disorder characterized by significant social communication impairments and difficulties perceiving and presenting communication cues. Machine learning techniques have been broadly adopted to facilitate autism studies and assessments. However, computational models are primarily concentrated on specific analysis and validated on private datasets in the autism community, which limits comparisons across models due to privacy-preserving data sharing complications. This work presents a novel privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions of children with Autism. MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from over 100 hours of intervention recordings. To promote public access, each data sample consists of four privacy-preserving modalities of data: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evalua
    
[^8]: 逆强化学习中的课程子目标

    Curricular Subgoals for Inverse Reinforcement Learning. (arXiv:2306.08232v1 [cs.LG])

    [http://arxiv.org/abs/2306.08232](http://arxiv.org/abs/2306.08232)

    本论文提出一种新的逆强化学习框架，即基于课程子目标的逆强化学习 (CSIRL)。与现有方法不同的是，该框架将一个任务分解为多个局部子目标，以引导智能体进行模仿，这有助于解决全局设计中存在的问题。

    

    逆强化学习（IRL）旨在从专家演示中重构奖励函数以促进策略学习，并已在模仿学习中展示出卓越的成功。现有的IRL方法主要集中于学习全局奖励函数以最小化模仿者和专家之间的轨迹差异，但这些全局设计仍然存在冗余噪声和误差传播问题，导致不适当的奖励分配，从而降低代理在复杂的多阶段任务中的能力。

    Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function from expert demonstrations to facilitate policy learning, and has demonstrated its remarkable success in imitation learning. To promote expert-like behavior, existing IRL methods mainly focus on learning global reward functions to minimize the trajectory difference between the imitator and the expert. However, these global designs are still limited by the redundant noise and error propagation problems, leading to the unsuitable reward assignment and thus downgrading the agent capability in complex multi-stage tasks. In this paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement Learning (CSIRL) framework, that explicitly disentangles one task with several local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces decision uncertainty of the trained agent over expert trajectories to dynamically select subgoals, which directly determines the exploration boundary of differen
    
[^9]: 结构化离散表示的深度生成模型的无偏学习

    Unbiased Learning of Deep Generative Models with Structured Discrete Representations. (arXiv:2306.08230v1 [cs.LG])

    [http://arxiv.org/abs/2306.08230](http://arxiv.org/abs/2306.08230)

    该论文提出了一种名为结构化变分自编码器的深度生成模型，它通过图像模型的结构和可解释性以及深度学习的适用于高维数据的灵活似然，结合两种框架的优势。同时，该论文还提出了一种学习SVAE的新算法，与此同时，推导出了一种计算自然梯度的方法，这些优化创新使得SVAE首次能与最先进的时间序列模型进行比较。

    

    通过将图形模型与深度学习架构组合，我们学习具有两种框架优势的生成模型。 结构化变分自编码器（SVAE）从图形模型继承结构和可解释性，从深度学习中继承了适用于高维数据的灵活似然，但是会带来相当大的优化挑战。 我们提出了学习SVAE的新算法，并且首次证明了SVAE在含有缺失数据且包含离散潜变量时处理多模态不确定性的能力。我们的内存高效隐式微分方案使得SVAE可以通过梯度下降来学习，并且证明了鲁棒性。为了更快地学习准确的图形模型参数，我们推导了一种计算自然梯度的方法，而不需要手动进行导出，从而避免了先前工作中发现的偏差。这些优化创新使得首次能够将SVAE与最先进的时间序列模型进行比较。

    By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges. We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables. Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work. These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time s
    
[^10]: 利用潜在状态表征通过策略过渡拓展敏捷机器人的多样性

    Expanding Versatility of Agile Locomotion through Policy Transitions Using Latent State Representation. (arXiv:2306.08224v1 [cs.RO])

    [http://arxiv.org/abs/2306.08224](http://arxiv.org/abs/2306.08224)

    本文提出了一种稳健的过渡策略——过渡网络，将不同步态的复杂性分为适用于真实世界机器人的专用策略，并通过潜在状态表征拓展机器人的多样性，实现了在技能库中任意策略对之间稳健过渡。

    

    本文提出了一种稳健的过渡策略——过渡网络，通过将不同步态的复杂性分为适用于真实世界机器人的专用运动策略，然后将这些策略与稳健的过渡方式统一为单个连贯的元控制器，并检查潜在状态表征，拓展机器人的多样性。我们的方法使机器人能够迭代地扩展其技能库和在库中任意策略对之间稳健地过渡。在我们的框架中，添加新技能不会引入任何改变之前学习技能的过程。此外，使用单个消费级GPU不到一小时即可完成单个运动策略的培训。我们的方法在真实世界中很有效，相比现有方法，在我们的实验中最具挑战性的转换对中，平均成功率比现有方法高19%。

    This paper proposes the transition-net, a robust transition strategy that expands the versatility of robot locomotion in the real-world setting. To this end, we start by distributing the complexity of different gaits into dedicated locomotion policies applicable to real-world robots. Next, we expand the versatility of the robot by unifying the policies with robust transitions into a single coherent meta-controller by examining the latent state representations. Our approach enables the robot to iteratively expand its skill repertoire and robustly transition between any policy pair in a library. In our framework, adding new skills does not introduce any process that alters the previously learned skills. Moreover, training of a locomotion policy takes less than an hour with a single consumer GPU. Our approach is effective in the real-world and achieves a 19% higher average success rate for the most challenging transition pairs in our experiments compared to existing approaches.
    
[^11]: 基于不确定性的噪声图上的鲁棒学习

    Uncertainty-Aware Robust Learning on Noisy Graphs. (arXiv:2306.08210v1 [cs.LG])

    [http://arxiv.org/abs/2306.08210](http://arxiv.org/abs/2306.08210)

    本文提出了一种基于分布式鲁棒优化的不确定性感知图学习框架，利用图神经网络嵌入节点特征，并通过极小极大形式最小化最坏情况风险来找到最优节点嵌入，从而缓解现实世界图中噪声测量挑战对图数据的负面影响。

    

    图神经网络在解决各种图计算任务方面展现了出色的能力，特别是在节点分类方面表现出色。然而，在现实世界的图中，拓扑或节点信息中普遍存在的噪声测量挑战可能会影响其效果。观测中的这些不准确性可能会破坏图数据中的关键模式，最终导致实际应用中不良性能。为了解决这些问题，本文提出了一种新颖的基于分布式鲁棒优化的不确定性感知图学习框架。具体而言，我们使用基于图神经网络的编码器来嵌入节点特征，并通过极小极大形式最小化最坏情况风险来找到最优节点嵌入。这种不确定性感知的学习过程导致了改进的节点表示和更强壮的图预测模型，有效地缓解了图数据中噪声测量的负面影响。

    Graph neural networks have shown impressive capabilities in solving various graph learning tasks, particularly excelling in node classification. However, their effectiveness can be hindered by the challenges arising from the widespread existence of noisy measurements associated with the topological or nodal information present in real-world graphs. These inaccuracies in observations can corrupt the crucial patterns within the graph data, ultimately resulting in undesirable performance in practical applications. To address these issues, this paper proposes a novel uncertainty-aware graph learning framework motivated by distributionally robust optimization. Specifically, we use a graph neural network-based encoder to embed the node features and find the optimal node embeddings by minimizing the worst-case risk through a minimax formulation. Such an uncertainty-aware learning process leads to improved node representations and a more robust graph predictive model that effectively mitigates
    
[^12]: 解谜ARC：用物体为中心的决策Transformer模仿人类解决方案

    Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer. (arXiv:2306.08204v1 [cs.AI])

    [http://arxiv.org/abs/2306.08204](http://arxiv.org/abs/2306.08204)

    该论文使用双重策略解决ARC任务，通过决策Transformer模仿人类解决方案和引入物体检测算法来提高AI的问题解决能力，同时揭示了数据和模型结构等方面的需求，为未来AGI研究提供了见解。

    

    在追求人工通用智能（AGI）的过程中，我们采用了一种新颖的双重方法来解决抽象推理文本集（ARC）任务。我们使用决策Transformer在模仿学习范式下建模人类问题解决，并引入了物体检测算法Push and Pull聚类方法。这种双重策略增强了AI的ARC问题解决能力，并为AGI进展提供了见解。然而，我们的工作揭示了高级数据收集工具、强大的培训数据集和精细的模型结构需求。这项研究突显了决策Transformer的潜在改进，并推动未来AGI研究。

    In the pursuit of artificial general intelligence (AGI), we tackle Abstraction and Reasoning Corpus (ARC) tasks using a novel two-pronged approach. We employ the Decision Transformer in an imitation learning paradigm to model human problem-solving, and introduce an object detection algorithm, the Push and Pull clustering method. This dual strategy enhances AI's ARC problem-solving skills and provides insights for AGI progression. Yet, our work reveals the need for advanced data collection tools, robust training datasets, and refined model structures. This study highlights potential improvements for Decision Transformers and propels future AGI research.
    
[^13]: POP: Prompt Of Prompts for Continual Learning

    POP: Prompt Of Prompts for Continual Learning. (arXiv:2306.08200v1 [cs.CV])

    [http://arxiv.org/abs/2306.08200](http://arxiv.org/abs/2306.08200)

    本研究提出了一种称为POP的模型，通过逐步学习一组任务指定的提示和一组全局提示，以解决连续学习中的信息集成问题。

    

    连续学习（CL）近年来越来越受到关注。它旨在模仿人类学习新概念的能力而不会发生灾难性遗忘。虽然现有的CL方法在某种程度上已经实现了这一目标，但它们仍然容易受到学习到的特征空间的语义漂移的影响。拥有健壮特征表示的基础模型，通过从非常大的数据集中获得的学习提供了解决CL问题的有趣基础。最近的工作还表明，它们可以通过提示调整技术适应特定任务，而大部分的表示的一般性基本不受影响。然而，一个待解决的问题是如何学习既为任务特定的提示又为全局提示的模型，即捕捉跨任务的信息。在这项工作中，我们提出了Prompt Of Prompts（POP）模型，通过逐步学习一组任务指定的提示和一组全局提示（即POP）来实现该目标，以集成信息。

    Continual learning (CL) has attracted increasing attention in the recent past. It aims to mimic the human ability to learn new concepts without catastrophic forgetting. While existing CL methods accomplish this to some extent, they are still prone to semantic drift of the learned feature space. Foundation models, which are endowed with a robust feature representation, learned from very large datasets, provide an interesting substrate for the solution of the CL problem. Recent work has also shown that they can be adapted to specific tasks by prompt tuning techniques that leave the generality of the representation mostly unscathed. An open question is, however, how to learn both prompts that are task specific and prompts that are global, i.e. capture cross-task information. In this work, we propose the Prompt Of Prompts (POP) model, which addresses this goal by progressively learning a group of task-specified prompts and a group of global prompts, denoted as POP, to integrate information
    
[^14]: 在标签噪声下的图上学习

    Learning on Graphs under Label Noise. (arXiv:2306.08194v1 [cs.LG])

    [http://arxiv.org/abs/2306.08194](http://arxiv.org/abs/2306.08194)

    该论文介绍了一种新方法 CGNN，它利用一致性图神经网络和基于同质性假设的样本选择技术，在标签噪声的情况下对节点分类进行建模，实现过滤出噪声节点和增强节点表示的鲁棒性。

    

    图上的节点分类是一项重要的任务，具有广泛的应用，包括社交分析和异常检测。虽然图神经网络（GNN）在这项任务上已经取得了一些有希望的结果，但目前的技术通常假设节点的标签信息是准确的，这在现实应用中可能并不成立。为了解决这个问题，我们研究了如何在标签噪声下的图上学习，并开发了一种名为一致性图神经网络（CGNN）的新方法来解决它。具体而言，我们采用了图对比学习作为正则化项，促进增强节点的两个视角具有一致的表示。由于这个正则化项不能利用标签信息，它可以增强节点表示对标签噪声的鲁棒性。此外，为了在图上检测噪声标签，我们提出了一种基于同质性假设的样本选择技术，通过测量嵌入和它们的邻居之间的一致性来识别噪声节点。各种真实世界数据集上的实验结果表明，CGNN可以有效地缓解标签噪声对节点分类的负面影响，并在对称和非对称标签噪声模型下优于最先进的基线。

    Node classification on graphs is a significant task with a wide range of applications, including social analysis and anomaly detection. Even though graph neural networks (GNNs) have produced promising results on this task, current techniques often presume that label information of nodes is accurate, which may not be the case in real-world applications. To tackle this issue, we investigate the problem of learning on graphs with label noise and develop a novel approach dubbed Consistent Graph Neural Network (CGNN) to solve it. Specifically, we employ graph contrastive learning as a regularization term, which promotes two views of augmented nodes to have consistent representations. Since this regularization term cannot utilize label information, it can enhance the robustness of node representations to label noise. Moreover, to detect noisy labels on the graph, we present a sample selection technique based on the homophily assumption, which identifies noisy nodes by measuring the consisten
    
[^15]: 自然语言处理中的表示实践

    Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])

    [http://arxiv.org/abs/2306.08193](http://arxiv.org/abs/2306.08193)

    本文介绍了一个框架，通过使用探测分类器来评估组件是否表示属性，填补了自然语言处理中关于“表示”的哲学空白。

    

    尽管“表示”在认知科学哲学中具有核心地位，但在当代自然语言处理实践中，几乎没有哲学领域的先前研究与之涉及。本文旨在填补这一空白：结合认知科学的思想，提出了一个框架来评估神经自然语言处理模型组件所作出的表示性声明，并提出三个评估组件是否表示属性的标准，并使用探测分类器来实现这些标准的操作化，探测分类器是NLP（和更广泛的深度学习）中流行的分析技术。操作化一个在哲学上受到启发的“表示”概念的项目应该引起科学哲学家和自然语言处理实践者的兴趣。对于哲学家来说，这提供了一个测试有关表示的本质的论据的新颖场地，并帮助NLPers组织有关探测实验的大量文献，提出了新的经验研究方向。

    Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
    
[^16]: 针对少样本节点分类的归纳线性探测方法

    Inductive Linear Probing for Few-shot Node Classification. (arXiv:2306.08192v1 [cs.LG])

    [http://arxiv.org/abs/2306.08192](http://arxiv.org/abs/2306.08192)

    本文研究了在归纳少样本节点分类任务中现有框架的局限性，并提出了一种针对这种任务的简单而高效的基线方法。

    

    元学习已成为少样本节点分类的有效训练策略，在转导设置下展示其有效性。然而，现有文献主要关注转导少样本节点分类，忽略了更广泛的少样本学习社区中广泛研究的归纳设置。这个疏忽限制了我们对基于元学习方法在图形数据上的性能的全面理解。在这项工作中，我们进行了实证研究，突出了当前框架在归纳少样本节点分类设置中的局限性。此外，我们提出了一个针对归纳少样本节点分类任务特别设计的简单但有竞争力的基线方法。我们希望我们的工作能够为更好地理解元学习范式在图形领域的工作方式提供新的道路。

    Meta-learning has emerged as a powerful training strategy for few-shot node classification, demonstrating its effectiveness in the transductive setting. However, the existing literature predominantly focuses on transductive few-shot node classification, neglecting the widely studied inductive setting in the broader few-shot learning community. This oversight limits our comprehensive understanding of the performance of meta-learning based methods on graph data. In this work, we conduct an empirical study to highlight the limitations of current frameworks in the inductive few-shot node classification setting. Additionally, we propose a simple yet competitive baseline approach specifically tailored for inductive few-shot node classification tasks. We hope our work can provide a new path forward to better understand how the meta-learning paradigm works in the graph domain.
    
[^17]: 基于用户意图的上下文字体推荐

    Contextual Font Recommendations based on User Intent. (arXiv:2306.08188v1 [cs.HC])

    [http://arxiv.org/abs/2306.08188](http://arxiv.org/abs/2306.08188)

    Adobe的20,000多种字体库是个选择恐惧症患者的噩梦，本文通过创造一个基于用户意图的系统来自动给用户提供合适的字体推荐，目前已被数百万 Adobe Express 用户使用，点击率高达 25% 以上。

    

    Adobe Fonts 拥有超过 20,000 种独特的字体库，用于创造图形、海报、复合物等。由于字体库的特殊性，选择合适的字体是一项需要大量经验的艰巨任务。对于大多数 Adobe 产品用户，特别是 Adobe Express 的普通用户，这通常意味着选择默认字体而不是使用丰富多样的可用字体。本研究创造了一个基于用户意图的系统，为用户提供上下文字体推荐，以协助他们的创作之旅。我们的系统接受多语言文本输入，并根据用户的意图推荐适当的字体。根据用户的资格，免费和付费字体的混合比例将会做出相应的调整。该功能目前已被数百万 Adobe Express 用户使用，点击率高达 25% 以上。

    Adobe Fonts has a rich library of over 20,000 unique fonts that Adobe users utilize for creating graphics, posters, composites etc. Due to the nature of the large library, knowing what font to select can be a daunting task that requires a lot of experience. For most users in Adobe products, especially casual users of Adobe Express, this often means choosing the default font instead of utilizing the rich and diverse fonts available. In this work, we create an intent-driven system to provide contextual font recommendations to users to aid in their creative journey. Our system takes in multilingual text input and recommends suitable fonts based on the user's intent. Based on user entitlements, the mix of free and paid fonts is adjusted. The feature is currently used by millions of Adobe Express users with a CTR of >25%.
    
[^18]: DCTX-Conformer：针对低延迟统一流式和非流式Conformer的动态上下文传递

    DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer. (arXiv:2306.08175v1 [eess.AS])

    [http://arxiv.org/abs/2306.08175](http://arxiv.org/abs/2306.08175)

    提出了一种基于Conformer的新型动态上下文传递机制DCTX-Conformer，解决了流式识别性能差距的问题，相比于现有最优解，识别结果的误差率提高了25%，但对延迟影响可以忽略不计。

    

    基于Conformer的端到端模型现在已经变得普遍，被广泛用于流式和非流式自动语音识别（ASR）中。诸如双模式和动态分块训练等技术有助于统一流式和非流式系统。然而，在完整和有限的过去上下文的情况下，流式识别之间仍然存在着性能差距。为了解决这个问题，我们提出了一种新颖的动态上下文传递机制，将其与现有的最先进的统一ASR系统进行集成。我们的提议——动态上下文Conformer（DCTX-Conformer）利用了一个非重叠的上下文传递机制，同时考虑了一块的左上下文和一个或多个先前的上下文嵌入。由于额外的上下文嵌入，我们相对于目前最优解提升了25.0%的词错误率，而延迟影响可以忽略不计。

    Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings.
    
[^19]: 在多模态人工智能中保护数据：一种差分隐私方法用于CLIP训练

    Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training. (arXiv:2306.08173v1 [cs.LG])

    [http://arxiv.org/abs/2306.08173](http://arxiv.org/abs/2306.08173)

    本文提出了一种差分隐私的CLIP模型（Dp-CLIP），旨在保护多模态AI任务中的数据隐私，同时保持模型准确性。该方法在基准数据集上得到了验证，并表明其与标准非私有CLIP模型相比具有同等的性能。

    

    多模态人工智能的成功引发了视觉和语言任务中数据隐私的关注。虽然CLIP通过对图像和文本的联合训练彻底改变了多模态学习，但其可能无意中披露敏感信息的潜力需要集成保护隐私的机制。我们引入了对比语言-图像预训练（CLIP）模型的差分隐私改进，有效地解决了隐私问题，同时保持准确性。我们提出的方法Dp-CLIP在包括图像分类和视觉问答等多样的视觉和语言任务的基准数据集上进行了严格评估。我们证明了我们的方法保持了与标准的非私有CLIP模型同等的性能。此外，我们在线性表示设置下分析了我们提出的算法。我们推导了算法的收敛速度，并展示了在梯度被剪辑时实用性和隐私之间的权衡。

    The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks. While CLIP has revolutionized multimodal learning through joint training on images and text, its potential to unintentionally disclose sensitive information necessitates the integration of privacy-preserving mechanisms. We introduce a differentially private adaptation of the Contrastive Language-Image Pretraining (CLIP) model that effectively addresses privacy concerns while retaining accuracy. Our proposed method, Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse vision-and-language tasks such as image classification and visual question answering. We demonstrate that our approach retains performance on par with the standard non-private CLIP model. Furthermore, we analyze our proposed algorithm under linear representation settings. We derive the convergence rate of our algorithm and show a trade-off between utility and privacy when gradients are clipped pe
    
[^20]: 我的模型性能为什么会下降？对片段发现算法的人工评估

    Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms. (arXiv:2306.08167v1 [cs.HC])

    [http://arxiv.org/abs/2306.08167](http://arxiv.org/abs/2306.08167)

    机器学习模型在语义连贯的数据子集上表现不佳仍然会出现问题，但是确定这些问题片段可能很困难，自动生成的片段并不是确定人工从业者问题性片段的银弹。

    

    机器学习（ML）模型可以在语义连贯的数据子集（即“片段”）上表现不佳，而高平均准确率的模型仍然会出现这种问题。这种行为可能对模型的安全性或偏见在部署中产生重大影响，但在实践中确定这些性能下降的片段可能很困难，特别是在从业者缺乏访问群组注释以定义其数据的连贯子集的领域。受到这些挑战的驱动，ML研究人员开发了新的片段发现算法，旨在将数据的连贯和高误差子集分组在一起。然而，评估这些工具是否帮助人类正确形成他们的模型性能下降的假设还很少。我们进行了一项受控用户研究（N = 15），向用户展示两种最先进的片段发现算法输出的40个片段，并要求他们形成有关对象检测模型性能下降的假设。我们的响应变量是参与者正确按错误率对片段进行排名的能力。我们的主要发现是：（1）两种片段发现算法都不会让参与者在假设上表现出系统性的优势；（2）即使在同一种片段发现算法中，参与者在正确对片段进行排序的能力上也存在显着变异；（3）对象类别的错误率比对象大小或位置等隐含语义更好地预测了片段难度。总体而言，我们的结果表明自动生成的片段并非确定人工从业者问题性片段的银弹，而在实践中使用这些算法必须小心。

    Machine learning (ML) models that achieve high average accuracy can still underperform on semantically coherent subsets (i.e. "slices") of data. This behavior can have significant societal consequences for the safety or bias of the model in deployment, but identifying these underperforming slices can be difficult in practice, especially in domains where practitioners lack access to group annotations to define coherent subsets of their data. Motivated by these challenges, ML researchers have developed new slice discovery algorithms that aim to group together coherent and high-error subsets of data. However, there has been little evaluation focused on whether these tools help humans form correct hypotheses about where (for which groups) their model underperforms. We conduct a controlled user study (N = 15) where we show 40 slices output by two state-of-the-art slice discovery algorithms to users, and ask them to form hypotheses about where an object detection model underperforms. Our res
    
[^21]: 强化学习驱动的快速基于注意力的点云对齐的连接物设计

    Reinforcement Learning-Driven Linker Design via Fast Attention-based Point Cloud Alignment. (arXiv:2306.08166v1 [cs.LG])

    [http://arxiv.org/abs/2306.08166](http://arxiv.org/abs/2306.08166)

    该论文提出了一种强化学习驱动的链接物设计方法，成功地生成满足2D和3D要求的连接剂，实现了新型连接剂的制备。

    

    Proteolysis-Targeting Chimeras(PROTACs)是一类新型小分子，设计成桥梁，将E3连接酶和与疾病相关的蛋白质连接起来，从而促进其后续降解。 PROTACs由两个蛋白质结合的“活性”区域组成，由“连接”区域连接。 连接区域的设计由其相互作用给出几何和化学约束条件，并需要最大化药物样本。 为了解决这些挑战，我们介绍了ShapeLinker，一种用于全新设计链接物的方法。 它使用强化学习在自回归SMILES生成器上进行片段连接。 该方法通过结合相关的物理化学性质和一种新颖的基于注意力的点云对齐得分来对其进行优化。 这种新方法成功地生成满足相关2D和3D要求的连接剂，并在生成假定目标连接剂构造的新型连接剂方面取得了最先进的成果。

    Proteolysis-Targeting Chimeras (PROTACs) represent a novel class of small molecules which are designed to act as a bridge between an E3 ligase and a disease-relevant protein, thereby promoting its subsequent degradation. PROTACs are composed of two protein binding "active" domains, linked by a "linker" domain. The design of the linker domain is challenging due to geometric and chemical constraints given by its interactions, and the need to maximize drug-likeness. To tackle these challenges, we introduce ShapeLinker, a method for de novo design of linkers. It performs fragment-linking using reinforcement learning on an autoregressive SMILES generator. The method optimizes for a composite score combining relevant physicochemical properties and a novel, attention-based point cloud alignment score. This new method successfully generates linkers that satisfy both relevant 2D and 3D requirements, and achieves state-of-the-art results in producing novel linkers assuming a target linker confor
    
[^22]: INT2.1：通过低秩自适应纠错实现精细可调的量化大语言模型

    INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation. (arXiv:2306.08162v1 [cs.CL])

    [http://arxiv.org/abs/2306.08162](http://arxiv.org/abs/2306.08162)

    该论文提出了一种通过低秩自适应纠错的方法，从而可以显著地减少精细调整VRAM需求，并纠正量化大语言模型中的量化误差，使消费者笔记本电脑可以对70亿个参数的大语言模型进行精细调整，生成连贯的英文文本。

    

    我们提出了一种方法，可以显著地减少精细调整VRAM需求，并纠正量化大语言模型中的量化误差。首先，我们使用低秩自适应（LoRA）开发了一种极其内存高效的量化模型精细调整方法（EMEF），并根据它构建了一个错误修正算法，旨在最小化量化过程中引起的误差。我们的方法可以将内存要求降低多达5.6倍，从而使消费者笔记本电脑可以对70亿个参数的大语言模型进行精细调整。同时，我们提出了一种低秩纠错（LREC）方法，利用增加的LoRA层来改善量化模型与其浮点数对应物之间的差距。我们的纠错框架可以生成连贯的英文文本，实现了完全功能的INT2量化大语言模型。据我们所知，这是第一个能够达到这种性能的INT2大语言模型。

    We introduce a method that dramatically reduces fine-tuning VRAM requirements and rectifies quantization errors in quantized Large Language Models. First, we develop an extremely memory-efficient fine-tuning (EMEF) method for quantized models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an error-correcting algorithm designed to minimize errors induced by the quantization process. Our method reduces the memory requirements by up to 5.6 times, which enables fine-tuning a 7 billion parameter Large Language Model (LLM) on consumer laptops. At the same time, we propose a Low-Rank Error Correction (LREC) method that exploits the added LoRA layers to ameliorate the gap between the quantized model and its float point counterpart. Our error correction framework leads to a fully functional INT2 quantized LLM with the capacity to generate coherent English text. To the best of our knowledge, this is the first INT2 Large Language Model that has been able to reach such a perfo
    
[^23]: h2oGPT：民主化大语言模型

    h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])

    [http://arxiv.org/abs/2306.08161](http://arxiv.org/abs/2306.08161)

    本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。

    

    基于生成预训练变压器（GPTs），大语言模型（LLMs）如GPT-4因其在自然语言处理方面的现实应用而成为人工智能革命的一部分。然而，它们也带来了许多重大的风险，如存在有偏见、私人或有害文本和未经授权的版权材料。本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs）。该项目的目标是创建世界上最好的真正开源的替代封闭源GPTs。与开源社区合作，作为其一部分，我们开源了几个LLM，其参数从7亿到400亿，可在完全自由的Apache 2.0许可下商用。我们的发布包括使用自然语言的100％私有文档搜索。开源语言模型有助于促进人工智能的发展并使其更加可靠。

    Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
    
[^24]: 自然语言处理中社会人口统计偏见的调查

    Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])

    [http://arxiv.org/abs/2306.08158](http://arxiv.org/abs/2306.08158)

    本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。

    

    深度神经网络在训练过程中往往会学习到非预期的偏见，这在实际应用中可能会产生有害的影响。本文对209篇关于NLP模型中偏见的论文进行了调查，其中大部分论文涉及社会人口统计偏见。为了更好地理解偏见与真实世界的危害之间的区别，我们借鉴心理学和行为经济学的思想，提出了社会人口统计偏见的定义。我们确定了NLP偏见研究的三个主要类别：偏见类型、量化偏见和去偏见。我们认为当前对于量化偏见的方法存在可靠性问题，许多偏见度量并不涉及真实世界中的偏见，当前的去偏见技术是表面的，只是隐藏了偏见，而不是真正去除它。最后，我们提供了未来工作的建议。

    Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
    
[^25]: 使用动态贝叶斯网络进行加密货币价格方向因果特征工程

    Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks. (arXiv:2306.08157v1 [cs.LG])

    [http://arxiv.org/abs/2306.08157](http://arxiv.org/abs/2306.08157)

    本文提出了一种基于动态贝叶斯网络的方法，来预测加密货币价格方向，以帮助投资者做出明智的投资决策。

    

    加密货币在各个领域，特别是金融和投资领域中越来越受到关注。其独特的区块链相关特性，如隐私、去中心化和不可追踪性，部分原因是其受欢迎的原因。然而，由于加密货币价格的波动性和不确定性，加密货币仍然是一种高风险投资。本文提出了一个动态贝叶斯网络（DBN）方法，可以在多元设置下模拟复杂系统，以预测五种流行加密货币的价格运动方向，以解决这个问题。

    Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular a
    
[^26]: （扩大）带状矩阵分解：一种统一的隐私训练方法。

    (Amplified) Banded Matrix Factorization: A unified approach to private training. (arXiv:2306.08153v1 [cs.LG])

    [http://arxiv.org/abs/2306.08153](http://arxiv.org/abs/2306.08153)

    本文提出了利用带状矩阵构建的矩阵分解机制，该机制能够在所有隐私预算中将先前最先进的算法纳入分散和联合训练设置中。对于跨设备联合学习，这意味着可以使用一种放松的设备参与模式，与实际的FL基础设施相容。在集中式设置中，带状矩阵具有与 ubiquitous DP-SGD algorithm 相同的隐私放大结果。

    

    差分隐私（DP）下的矩阵分解（MF）机制在许多场景下显著改进了隐私-效用-计算折衷的最新技术。但是在分散和联合设置中，仍存在MF不易适用的实例，或者其他算法提供更好的折衷（通常随着 ε 变小）。在这项工作中，我们展示了如何使用带状矩阵构建MF机制，在所有隐私预算中将先前最先进的算法纳入分散和联合训练设置中。关键技术是带状矩阵的构造。对于跨设备联合学习（FL），这使得多个设备可以使用一种放松的设备参与模式，与实际的FL基础设施相容（如产品部署所示）。在集中式设置中，我们证明带状矩阵具有与 ubiquitous DP-SGD algorithm 相同的隐私放大结果。

    Matrix factorization (MF) mechanisms for differential privacy (DP) have substantially improved the state-of-the-art in privacy-utility-computation tradeoffs for ML applications in a variety of scenarios, but in both the centralized and federated settings there remain instances where either MF cannot be easily applied, or other algorithms provide better tradeoffs (typically, as $\epsilon$ becomes small).  In this work, we show how MF can subsume prior state-of-the-art algorithms in both federated and centralized training settings, across all privacy budgets. The key technique throughout is the construction of MF mechanisms with banded matrices. For cross-device federated learning (FL), this enables multiple-participations with a relaxed device participation schema compatible with practical FL infrastructure (as demonstrated by a production deployment). In the centralized setting, we prove that banded matrices enjoy the same privacy amplification results as for the ubiquitous DP-SGD algo
    
[^27]: 非线性个性化预测的神经混合效应

    Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])

    [http://arxiv.org/abs/2306.08149](http://arxiv.org/abs/2306.08149)

    本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。

    

    个性化预测是一种机器学习方法，根据过去标记观测预测一个人未来的观测值，通常用于连续任务，例如预测日常情绪评分。在进行个性化预测时，模型可以结合两种趋势：（a）跨人共享的趋势，即个人通用趋势，例如周末更开心，和（b）每个人独特的趋势，即个人特定的趋势，例如每周有一次压力大的会议。混合效应模型是一种流行的统计模型，用于通过组合个人通用和个人特定参数来研究这两种趋势。尽管现在线性混合效应模型通过将其与神经网络整合而变得越来越流行，但这种整合目前仅限于线性个人特定参数：排除非线性个人特定趋势。在本文中，我们提出了神经混合效应（NME）模型，以优化非线性个人特定参数。

    Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
    
[^28]: 多市场能源优化与强化学习中的可再生能源

    Multi-market Energy Optimization with Renewables via Reinforcement Learning. (arXiv:2306.08147v1 [cs.LG])

    [http://arxiv.org/abs/2306.08147](http://arxiv.org/abs/2306.08147)

    本文提出了一种基于深度强化学习的功率厂优化方法，通过储能与可再生能源的组合最大化能源市场的收入，最小化储能损耗成本以及可再生能源的削减。通过部件级模拟器处理了储能过程中的时间耦合、不确定性以及非线性存储模型等问题，并通过RL方法处理复杂储能模型，实现了策略动作与系统约束的平衡。

    

    本文介绍了一种基于深度强化学习框架的功率厂优化方法，通过储能与可再生能源的组合，旨在最大化能源市场的收入、最小化储能损耗成本以及可再生能源数量的削减。该框架处理了储能设备的时间耦合复杂性、可再生能源发电量和能源价格的不确定性以及非线性储能模型等问题。本研究将问题视为分层马尔可夫决策过程，并使用部件级模拟器来处理储能。它利用强化学习的方法来整合复杂的储能模型，克服了需要凸性和可微分的部件模型的优化方法的限制。该方法的重要特点是确保策略动作尊重系统约束，通过将潜在非法动作投影到安全状态动作集上来实现。本文通过广泛的实践证明了该方法的有效性。

    This paper introduces a deep reinforcement learning (RL) framework for optimizing the operations of power plants pairing renewable energy with storage. The objective is to maximize revenue from energy markets while minimizing storage degradation costs and renewable curtailment. The framework handles complexities such as time coupling by storage devices, uncertainty in renewable generation and energy prices, and non-linear storage models. The study treats the problem as a hierarchical Markov Decision Process (MDP) and uses component-level simulators for storage. It utilizes RL to incorporate complex storage models, overcoming restrictions of optimization-based methods that require convex and differentiable component models. A significant aspect of this approach is ensuring policy actions respect system constraints, achieved via a novel method of projecting potentially infeasible actions onto a safe state-action set. The paper demonstrates the efficacy of this approach through extensive 
    
[^29]: ArtWhisperer：一个用于描述艺术创作中人工智能与人类交互的数据集

    ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])

    [http://arxiv.org/abs/2306.08141](http://arxiv.org/abs/2306.08141)

    为研究人工智能与人类交互，研究者创建了ArtWhisperer数据集，这是一个在线游戏，人们通过反复尝试不同的提示词，来生成和目标图像类似的图像，并记录了50,000多个交互记录。在初步分析中，研究者发现人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。

    

    随着生成型人工智能越来越普及，研究人类用户如何与这些模型交互变得越来越重要。在这项工作中，我们研究了人们如何使用文本到图像的模型生成所需的目标图像。为了研究这种交互，我们创建了ArtWhisperer，这是一个在线游戏，用户会得到一个目标图像，并需要反复尝试不同的提示词，以便生成类似目标图像的图像。通过这个游戏，我们记录了50,000多个人工智能-人类交互的记录；每个交互都对应着用户创建的一个提示词和相应生成的图像。大多数记录都是重复的交互，用户通过反复尝试找到最佳的提示词以生成目标图像，这使得这个数据集成为研究人工智能与人类协作的独特连续数据集。在对这个数据集的初步分析中，我们发现了一些提示词交互和用户策略的特征。人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。

    As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
    
[^30]: 自监督深度高光谱修复算法考虑稀疏性和低秩性

    Self-supervised Deep Hyperspectral Inpainting with the Sparsity and Low-Rank Considerations. (arXiv:2306.08128v1 [eess.IV])

    [http://arxiv.org/abs/2306.08128](http://arxiv.org/abs/2306.08128)

    本论文提出了两种新的自监督高光谱图像修复算法，具有强大的学习能力，且在不需要外部训练数据的情况下有效地解决高光谱图像修复的问题，实验结果也显示其效果优于现有算法。

    

    高光谱图像通常由数百个窄且连续的光谱波段组成，每个波段包含了被成像场景的物质组成信息。然而，这些图像容易受到多种噪声、失真或数据丢失的影响，导致其质量和实际应用价值显著下降。为了解决这些问题，我们提出了两种新的自监督高光谱图像修复算法：低秩和稀疏约束“插入-播放” (LRS-PnP) 以及其扩展算法 LRS-PnP-DIP。该算法具有强大的学习能力，但仍无需外部训练数据。我们在某些温和假设下进行稳定性分析，保证了算法的收敛性，有助于实际应用。大量实验表明，所提出的解决方案能够产生外观和质量上优于现有方法的修复结果，并达到最先进水平。

    Hyperspectral images are typically composed of hundreds of narrow and contiguous spectral bands, each containing information about the material composition of the imaged scene. However, these images can be affected by various sources of noise, distortions, or data losses, which can significantly degrade their quality and usefulness. To address these problems, we introduce two novel self-supervised Hyperspectral Images (HSI) inpainting algorithms: Low Rank and Sparsity Constraint Plug-and-Play (LRS-PnP), and its extension LRS-PnP-DIP, which features the strong learning capability, but is still free of external training data. We conduct the stability analysis under some mild assumptions which guarantees the algorithm to converge. It is specifically very helpful for the practical applications. Extensive experiments demonstrate that the proposed solution is able to produce visually and qualitatively superior inpainting results, achieving state-of-the-art performance. The code for reproduci
    
[^31]: 带有沉重尾部SGD训练的过参数化神经网络的隐式可压缩性

    Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD. (arXiv:2306.08125v1 [stat.ML])

    [http://arxiv.org/abs/2306.08125](http://arxiv.org/abs/2306.08125)

    本研究提出了一种简单的SGD修改方法，使训练出的神经网络输出可被证明为可压缩，而不需要任何非平凡假设。

    

    由于减少计算需求和压缩与泛化误差之间的显式关系，神经网络压缩成为越来越重要的研究对象。最近的研究表明，随机梯度下降(SGD)的超参数选择可以影响学习参数向量的压缩性。虽然这些结果揭示了训练动态对压缩性的影响，但是它们依赖于不可验证的假设，由于隐含性质，得出的理论并没有提供实用的指导方针。在本研究中，我们提出了一种简单的SGD修改方法，使得算法的输出能够被证明是可压缩的，而不需要任何非平凡假设。我们考虑了一个使用SGD训练的单隐藏层神经网络，并在每次迭代中注入附加的沉重尾部噪声。

    Neural network compression has been an increasingly important subject, due to its practical implications in terms of reducing the computational requirements and its theoretical implications, as there is an explicit connection between compressibility and the generalization error. Recent studies have shown that the choice of the hyperparameters of stochastic gradient descent (SGD) can have an effect on the compressibility of the learned parameter vector. Even though these results have shed some light on the role of the training dynamics over compressibility, they relied on unverifiable assumptions and the resulting theory does not provide a practical guideline due to its implicitness. In this study, we propose a simple modification for SGD, such that the outputs of the algorithm will be provably compressible without making any nontrivial assumptions. We consider a one-hidden-layer neural network trained with SGD and we inject additive heavy-tailed noise to the iterates at each iteration.
    
[^32]: 从句子到文档层面的AI生成抄袭检测：超越黑匣子方法

    Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level. (arXiv:2306.08122v1 [cs.CL])

    [http://arxiv.org/abs/2306.08122](http://arxiv.org/abs/2306.08122)

    该论文提出了一种新的自然语言处理方法，可对学术写作中的抄袭行为进行有效检测，不仅在句子级别上进行评估，还在文档级别上提供可量化指标。该方法的准确率高达94％，具有较强的适应性和可靠性，能够不断随着LLM技术的发展而不断改进。

    

    学术写作中对大型语言模型（LLMs）越来越依赖导致了抄袭现象的增加。现有的AI生成文本分类器准确性有限，往往产生误报。我们提出一种新方法，利用自然语言处理（NLP）技术，提供句子和文档级别的可量化指标，方便人类评估者进行解释。我们的方法采用多方面的方法，生成给定问题的多个释义版本，将它们输入LLM以生成答案。通过使用基于余弦相似度的对比损失函数，将生成的句子与学生回答中的句子匹配。我们的方法在分类人类和AI文本方面的准确性达到了94％，为学术环境中的抄袭检测提供了强大而适应性强的解决方案。该方法随着LLM技术的发展而不断改进，减少了新模型训练或重新配置的需求，并提供了比传统黑匣子方法更透明的评估方式。

    The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of ev
    
[^33]: 使用语义ID进行更好的泛化：推荐排名的案例研究

    Better Generalization with Semantic IDs: A case study in Ranking for Recommendations. (arXiv:2306.08121v1 [cs.IR])

    [http://arxiv.org/abs/2306.08121](http://arxiv.org/abs/2306.08121)

    本文提出使用语义ID解决推荐系统中的物品冷启动问题，这些ID是从内容嵌入中学习的，可以捕捉概念的层次关系，相较于完全消除ID特征的方法，语义ID能更好地提高推荐质量。

    

    在推荐模型中，训练好的物品表示是至关重要的。通常，一项商品会被分配一个唯一的随机生成的ID，并且通常会通过学习与随机ID值相对应的嵌入来表示。虽然这种方法被广泛使用，但在物品数量大且物品服从幂律分布的情况下——这是真实世界推荐系统的典型特征——会有一定局限性。这会导致物品冷启动问题，模型无法对尾部和以前未见过的物品进行可靠的推荐。完全消除这些ID特征及其学习的嵌入以解决冷启动问题会严重降低推荐质量。基于内容的物品嵌入更为可靠，但对于用户过去的物品交互序列来说，它们成本高且使用困难。本文中，我们使用语义ID来表示离散的物品，这些ID是通过使用RQ-VAE从内容嵌入中学习的，可以捕捉概念的层次关系。

    Training good representations for items is critical in recommender models. Typically, an item is assigned a unique randomly generated ID, and is commonly represented by learning an embedding corresponding to the value of the random ID. Although widely used, this approach have limitations when the number of items are large and items are power-law distributed -- typical characteristics of real-world recommendation systems. This leads to the item cold-start problem, where the model is unable to make reliable inferences for tail and previously unseen items. Removing these ID features and their learned embeddings altogether to combat cold-start issue severely degrades the recommendation quality. Content-based item embeddings are more reliable, but they are expensive to store and use, particularly for users' past item interaction sequence. In this paper, we use Semantic IDs, a compact discrete item representations learned from content embeddings using RQ-VAE that captures hierarchy of concep
    
[^34]: CipherSniffer: 分类密码类型

    CipherSniffer: Classifying Cipher Types. (arXiv:2306.08116v1 [cs.CL])

    [http://arxiv.org/abs/2306.08116](http://arxiv.org/abs/2306.08116)

    本文将解密任务作为分类问题来解决，并创建了一个包含各种类型的密码数据集，最终评估了各种Tokenizer-Model组合在此任务中的性能。

    

    密码是加密通信的强有力工具。有很多不同的密码类型，这使得使用暴力破解来解密密码的计算费用昂贵。本文将解密任务作为分类问题来框架化。首先，我们创建了一个置换、替换、文本反转、单词反转、句子移位和未加密文本的数据集。然后，我们评估了各种Tokenizer-Model组合在此任务中的表现。

    Ciphers are a powerful tool for encrypting communication. There are many different cipher types, which makes it computationally expensive to solve a cipher using brute force. In this paper, we frame the decryption task as a classification problem. We first create a dataset of transpositions, substitutions, text reversals, word reversals, sentence shifts, and unencrypted text. Then, we evaluate the performance of various tokenizer-model combinations on this task.
    
[^35]: 针对部分强凸性，在Nesterov动量法下加速收敛深度神经网络

    Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity. (arXiv:2306.08109v1 [cs.LG])

    [http://arxiv.org/abs/2306.08109](http://arxiv.org/abs/2306.08109)

    本研究证明了针对一类新的目标函数，只有部分参数满足强凸性，Nesterov动量法在深度神经网络中实现了加速收敛。

    

    当前最先进的神经网络梯度下降收敛分析聚焦于表征损失函数的特性，例如Polyak-Lojaciewicz（PL）条件和受限强凸性。虽然在这些条件下梯度下降具有线性收敛性，但是Nesterov动量法是否在类似的条件和假设下具有加速收敛仍然是一个未解决的问题。在这项研究中，我们考虑了一类新的目标函数，只有部分参数满足强凸性，并证明了Nesterov动量法在这种目标函数下实现了加速收敛。我们提供了两种问题类别的实现，其中一种是深度ReLU网络，这是我们所知道的第一个证明非平凡神经网络体系结构具有加速收敛率的论文。

    Current state-of-the-art analyses on the convergence of gradient descent for training neural networks focus on characterizing properties of the loss landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted strong convexity. While gradient descent converges linearly under such conditions, it remains an open question whether Nesterov's momentum enjoys accelerated convergence under similar settings and assumptions. In this work, we consider a new class of objective functions, where only a subset of the parameters satisfies strong convexity, and show Nesterov's momentum achieves acceleration in theory for this objective class. We provide two realizations of the problem class, one of which is deep ReLU networks, which --to the best of our knowledge--constitutes this work the first that proves accelerated convergence rate for non-trivial neural network architectures.
    
[^36]: 巨型语言模型时代的AutoML：当前挑战，未来机遇和风险。

    AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])

    [http://arxiv.org/abs/2306.08107](http://arxiv.org/abs/2306.08107)

    论文探讨了AutoML和LLMs之间的共生关系，并指出这两个领域的融合有望颠覆NLP和AutoML两个领域，同时也存在风险。

    

    在过去的几年中，自然语言处理（NLP）和自动化机器学习（AutoML）领域取得了显著的成果。特别是在NLP领域，巨型语言模型（LLMs）最近经历了一系列突破。我们设想，两个领域通过紧密的融合可以彼此推动极限。为了展示这一愿景，我们探索了AutoML和LLMs之间的共生关系潜力，着重探讨了它们如何互相受益。我们特别研究了从不同角度增强LLMs的AutoML方法的机会以及利用AutoML进一步改进LLMs的挑战。为此，我们调查了现有工作，并对其中的风险进行了批判性评估。我们坚信，两个领域的融合有可能颠覆NLP和AutoML两个领域。通过强调可想象的协同作用和风险，我们旨在促进在交叉点的进一步探索。

    The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
    
[^37]: 无模型市场风险对冲与拥挤网络

    Model-Free Market Risk Hedging Using Crowding Networks. (arXiv:2306.08105v1 [q-fin.PM])

    [http://arxiv.org/abs/2306.08105](http://arxiv.org/abs/2306.08105)

    本文采用基金持仓的网络分析来计算股票的拥挤分数，构建出的多头头寸和空头头寸的对冲组合具有负相关和正市场回报下凸性的特性，能提供一种无需昂贵的期权策略或复杂的数值优化的对冲投资组合，用于对冲投资组合风险，包括尾部风险。

    

    拥挤被广泛认为是设计投资组合策略中最重要的风险因素之一。本文使用基金持仓的网络分析来计算股票的拥挤分数，构建出无需使用任何数值优化的成本不变的多头头寸和空头头寸的对冲组合，以模型自由的方式为市场风险提供保护机制，并且具有负相关和正市场回报下的凸性特性，能够在小幅和大幅市场价格波动时提供保护。通过将我们的多头头寸和空头头寸对冲组合添加到基础组合中，例如传统的60/40组合，我们的方法提供了一种无需昂贵的期权策略或复杂的数值优化的对冲组合，来对冲投资组合风险，包括尾部风险。此类对冲的总成本相当低。

    Crowding is widely regarded as one of the most important risk factors in designing portfolio strategies. In this paper, we analyze stock crowding using network analysis of fund holdings, which is used to compute crowding scores for stocks. These scores are used to construct costless long-short portfolios, computed in a distribution-free (model-free) way and without using any numerical optimization, with desirable properties of hedge portfolios. More specifically, these long-short portfolios provide protection for both small and large market price fluctuations, due to their negative correlation with the market and positive convexity as a function of market returns. By adding our long-short portfolio to a baseline portfolio such as a traditional 60/40 portfolio, our method provides an alternative way to hedge portfolio risk including tail risk, which does not require costly option-based strategies or complex numerical optimization. The total cost of such hedging amounts to the total cost
    
[^38]: 面向域感知的光学相干断层扫描噪音降低的少样本学习

    Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise Reduction. (arXiv:2306.08102v1 [eess.IV])

    [http://arxiv.org/abs/2306.08102](http://arxiv.org/abs/2306.08102)

    本文提出了一种少样本监督学习框架，用于光学相干断层扫描噪音降低，并成功推广到不同成像领域。

    

    散斑噪声一直是医学成像中一个被广泛研究的问题。近年来，利用深度学习方法进行降噪方面取得了重大进展。然而，将有监督的学习模型适应到新领域仍然是一个具有挑战性的问题。本文提出了一种少样本监督学习框架，用于光学相干断层扫描噪音降低，大大提高了训练速度，只需要单张图像或部分图像以及相应的去斑地面实况进行训练。此外，我们对光学相干断层扫描不同成像系统的域转移问题进行了制定，并证明了我们提出的方法适应不同成像领域的有效性。我们的方法在合成和真实的光学相干断层扫描数据集上均优于现有方法，展示了它很好地推广到新领域的能力。

    Speckle noise has long been an extensively studied problem in medical imaging. In recent years, there have been significant advances in leveraging deep learning methods for noise reduction. Nevertheless, adaptation of supervised learning models to unseen domains remains a challenging problem. Specifically, deep neural networks (DNNs) trained for computational imaging tasks are vulnerable to changes in the acquisition system's physical parameters, such as: sampling space, resolution, and contrast. Even within the same acquisition system, performance degrades across datasets of different biological tissues. In this work, we propose a few-shot supervised learning framework for optical coherence tomography (OCT) noise reduction, that offers a dramatic increase in training speed and requires only a single image, or part of an image, and a corresponding speckle suppressed ground truth, for training. Furthermore, we formulate the domain shift problem for OCT diverse imaging systems, and prove
    
[^39]: 能否通过 ChatGPT 实现智能交通系统？使用强化学习实现混合交通流控制的案例研究

    Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning. (arXiv:2306.08094v1 [cs.AI])

    [http://arxiv.org/abs/2306.08094](http://arxiv.org/abs/2306.08094)

    本文研究探讨使用 ChatGPT 解决混合交通流控制问题，通过大规模用户研究发现 ChatGPT 在某些环境下能够提高成功策略数量

    

    强化学习在智能交通系统中的应用不断增多，同时也凸显了一些关键问题。本文研究使用大型语言模型 ChatGPT 研究是否可以帮助解决复杂的混合交通流控制问题，通过一个大规模的用户研究，发现 ChatGPT 在某些环境下能够增加成功策略数量

    The surge in Reinforcement Learning (RL) applications in Intelligent Transportation Systems (ITS) has contributed to its growth as well as highlighted key challenges. However, defining objectives of RL agents in traffic control and management tasks, as well as aligning policies with these goals through an effective formulation of Markov Decision Process (MDP), can be challenging and often require domain experts in both RL and ITS. Recent advancements in Large Language Models (LLMs) such as GPT-4 highlight their broad general knowledge, reasoning capabilities, and commonsense priors across various domains. In this work, we conduct a large-scale user study involving 70 participants to investigate whether novices can leverage ChatGPT to solve complex mixed traffic control problems. Three environments are tested, including ring road, bottleneck, and intersection. We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 
    
[^40]: 神经网络的安全使用

    Safe Use of Neural Networks. (arXiv:2306.08086v1 [eess.SP])

    [http://arxiv.org/abs/2306.08086](http://arxiv.org/abs/2306.08086)

    本文介绍了一种用于神经网络的错误检测和纠正方法，使用数字编码可以显著提升其可靠性。

    

    现代通信系统中的神经网络可能容易受到内部数字误差的影响，从而严重影响决策结果。本文提出了使用基于数字编码的方法来检测神经网络处理过程中的算术错误以确保其安全使用。研究表明，该方法的可靠性得到了有效提升。

    Neural networks in modern communication systems can be susceptible to internal numerical errors that can drastically effect decision results. Such structures are composed of many sections each of which generally contain weighting operations and activation function evaluations. The safe use comes from methods employing number based codes that can detect arithmetic errors in the network's processing steps. Each set of operations generates parity values dictated by a code in two ways. One set of parities is obtained from a section's outputs while a second comparable set is developed directly from the original inputs. The parity values protecting the activation functions involve a Taylor series approximation to the activation functions. We focus on using long numerically based convolutional codes because of the large size of data sets. The codes are based on Discrete Fourier Transform kernels and there are many design options available. Mathematical program simulations show our error-detec
    
[^41]: 图结构和特征外推在超出分布泛化中的应用

    Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization. (arXiv:2306.08076v1 [cs.LG])

    [http://arxiv.org/abs/2306.08076](http://arxiv.org/abs/2306.08076)

    本文提出一种非欧几里得空间线性外推设计，通过外推结构和特征空间来生成超出分布的图数据， 然后实现图的超出分布泛化。

    

    超出分布泛化涉及测试数据分布不同于训练数据分布的学习场景。本文提出一种新颖的非欧几里得空间线性外推设计，通过外推结构和特征空间来生成超出分布的图数据，从而实现图超出分布泛化。理论分析和实证结果证明了我们的方法在解决目标转移过程中的有效性，在各种图超出分布任务中都显示出了显著且持续的改进。

    Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates both structure and feature spaces to generate OOD graph data. Our design tailors OOD samples for specific shifts without corrupting underlying causal mechanisms. Theoretical analysis and empirical results evidence the effectiveness of our method in solving target shifts, showing substantial and constant improvements across various graph OOD tasks.
    
[^42]: DORSal: 基于扩散的物体中心场景表示

    DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])

    [http://arxiv.org/abs/2306.08068](http://arxiv.org/abs/2306.08068)

    DORSal提出了一种基于扩散模型的物体中心场景表示方法，可以呈现高保真新视图，并在较大程度上保留了诸如基于物体的场景编辑之类的优点。

    

    最近在三维场景理解方面取得的进展使跨大量不同场景的数据集的可扩展表示学习成为可能。因此，对于未见过的场景和物体的泛化，仅通过单个或少数图像渲染新视图，以及支持编辑的可控场景生成现在成为可能。然而，联合训练大量场景通常会在渲染质量上妥协，而与单个场景优化模型（如NeRF）相比。在本文中，我们利用最近扩散模型的进展，使三维场景表示学习模型具备呈现高保真新视图的能力，同时在较大程度上保留了诸如基于物体的场景编辑之类的优点。特别地，我们提出了DORSal，它基于扩散视频架构，为基于物体中心的场景插槽表示的三维场景生成提供适应性。我们在复杂的合成多物体场景和现实世界大规模街景数据集上证明，我们的模型能够生成高质量的场景新视图，同时支持物体级别的编辑，并保留细粒度的纹理和反射等细节。

    Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
    
[^43]: 通过控制变量基因表达式编程进行符号回归

    Symbolic Regression via Control Variable Genetic Programming. (arXiv:2306.08057v1 [cs.NE])

    [http://arxiv.org/abs/2306.08057](http://arxiv.org/abs/2306.08057)

    CVGP是一种通过控制变量实验设计加快符号表达式发现的方法，能够学习复杂符号表达式，扩展了现有方法的能力。

    

    直接从实验数据中学习符号表达式是人工智能驱动的科学发现的重要步骤。然而，现有的方法仅限于学习简单的表达式。回归涉及许多自变量的表达式仍然难以实现。受科学界广泛使用的控制变量实验的启发，我们提出了一种基于控制变量的基因表达式编程（CVGP）方法，用于多个自变量的符号回归。CVGP通过定制实验设计，而不是从预先收集的固定数据集中学习，加快了符号表达式的发现过程。它首先使用基因表达式编程拟合涉及少量自变量的简单表达式，在控制变量实验中，其中其他变量被保持为常量。然后通过增加新的自变量扩展以前学习到的表达式，使用新的控制变量实验，在这些实验中，这些变量被允许变化。理论上，我们展示了CVGP可以有效地探索潜在解决方案的空间，并在基准数据集上验证了其有效性。我们的结果表明，CVGP能够学习涉及许多自变量的复杂符号表达式，而现有的方法无法处理。

    Learning symbolic expressions directly from experiment data is a vital step in AI-driven scientific discovery. Nevertheless, state-of-the-art approaches are limited to learning simple expressions. Regressing expressions involving many independent variables still remain out of reach. Motivated by the control variable experiments widely utilized in science, we propose Control Variable Genetic Programming (CVGP) for symbolic regression over many independent variables. CVGP expedites symbolic expression discovery via customized experiment design, rather than learning from a fixed dataset collected a priori. CVGP starts by fitting simple expressions involving a small set of independent variables using genetic programming, under controlled experiments where other variables are held as constants. It then extends expressions learned in previous generations by adding new independent variables, using new control variable experiments in which these variables are allowed to vary. Theoretically, we
    
[^44]: 在伸缩性训练中优化超参数：计算效率训练的超参数优化

    Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training. (arXiv:2306.08055v1 [cs.LG])

    [http://arxiv.org/abs/2306.08055](http://arxiv.org/abs/2306.08055)

    该论文提出了一种名为“CARBS”的算法，它利用贝叶斯优化算法在性能-计算 Pareto 前沿附近执行局部搜索来解决大型深度学习模型超参数调整的问题。该方法适用于具有许多超参数的无界搜索空间，学习缩放关系，并自动化了许多调整中的“黑魔法”。此外，该方法对计算受限制的情况尤其有效。

    

    深度学习模型的超参数调整可以使相同的计算量获得数量级的性能提升。尽管如此，系统调整还不普遍，尤其是对于大型模型更是如此，这些模型评估昂贵，超参数较多，需要进行难以把握的折中、预算和搜索边界决策。为了解决这些问题并提出一种实用的方法来稳健地调整大型模型，我们提出了成本感知 Pareto 区域贝叶斯搜索（CARBS），这是一种贝叶斯优化算法，它在性能-计算 Pareto 前沿附近执行局部搜索。CARBS 在具有许多超参数的无界搜索空间中表现良好，学习缩放关系，因此即使在模型缩放的同时也可以调整模型，并自动化了许多“黑魔法”调整。在我们的结果中，我们通过调整简单的基线（ProcGen 论文中提供的 PPO 方法）有效地解决了整个 ProcGen 基准测试。我们还在使用更少的评估时复制了 Bertinetto 等人的模型选择结果。我们的方法通常适用，但特别适合计算受限的情况，其中设计师可以轻松评估或限制培训成本。

    Hyperparameter tuning of deep learning models can lead to order-of-magnitude performance gains for the same amount of compute. Despite this, systematic tuning is uncommon, particularly for large models, which are expensive to evaluate and tend to have many hyperparameters, necessitating difficult judgment calls about tradeoffs, budgets, and search bounds. To address these issues and propose a practical method for robustly tuning large models, we present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian optimization algorithm that performs local search around the performance-cost Pareto frontier. CARBS does well even in unbounded search spaces with many hyperparameters, learns scaling relationships so that it can tune models even as they are scaled up, and automates much of the "black magic" of tuning. Among our results, we effectively solve the entire ProcGen benchmark just by tuning a simple baseline (PPO, as provided in the original ProcGen paper). We also reproduce the mo
    
[^45]: 剪枝方式提高可靠策略：一种多目标深度Q学习方法应用于重症护理

    Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v1 [cs.LG])

    [http://arxiv.org/abs/2306.08044](http://arxiv.org/abs/2306.08044)

    该论文介绍了一种深度Q学习方法，通过剪枝动作集来实现将中间生物标志物信号整合到奖励规范中，提高了重症护理策略的可靠性。

    

    大多数医疗决策具有连续性，因此，强化学习可能有望制定精确的数据驱动治疗计划。然而，该领域的主要挑战之一是主要基于死亡率的奖励函数的稀疏性，导致离线估计的稳定性降低。本研究引入了一种深度Q学习方法，能够获得更可靠的重症护理策略。该方法将相关但嘈杂的中间生物标志物信号整合到奖励规范中，同时不会损害感兴趣的主要结果（例如患者生存率）的优化。通过根据所有可用奖励对动作集进行剪枝，然后基于稀疏主要奖励，使用受限动作集进行最终模型训练，通过解离准确和近似奖励来最小化主要目标的潜在扭曲，实现了上述目标。

    Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all whi
    
[^46]: 关于伪造纳什均衡的研究

    On Faking a Nash Equilibrium. (arXiv:2306.08041v1 [cs.MA])

    [http://arxiv.org/abs/2306.08041](http://arxiv.org/abs/2306.08041)

    本文研究多智能体强化学习中的数据污染攻击，提出了唯一纳什集的概念，并设计了一个线性规划方案来计算最优污染攻击策略。

    

    本文研究了多智能体强化学习中的数据污染攻击，攻击者试图更改数据集以安装（潜在虚假的）唯一马尔可夫完美纳什均衡点(Nash equilibrium)。我们提出了唯一纳什集的概念，即由其Q函数规定的游戏的集合，其具有唯一的联合策略作为唯一的纳什均衡点。唯一纳什集对于污染攻击非常重要，因为只有当数据污染使所有合理的游戏都在其中时，攻击才成功。唯一纳什集将常用于逆强化学习中的奖励多面体推广到多智能体强化学习中。对于零和马尔科夫博弈，逆纳什集以及由数据引起的合理游戏集都是Q函数空间中的多面体。我们提出了一个线性规划方案以有效地计算最优的污染攻击策略。我们的工作为设计更加鲁棒的多智能体强化学习算法之前必要的步骤揭示了离线MARL数据污染攻击结构的一些特点。

    We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL alg
    
[^47]: 可变通道维度的可微架构搜索方法

    Flexible Channel Dimensions for Differentiable Architecture Search. (arXiv:2306.08021v1 [cs.LG])

    [http://arxiv.org/abs/2306.08021](http://arxiv.org/abs/2306.08021)

    本文提出了一种新颖的可微神经架构搜索方法，使用动态通道分配算法实现通道维度的灵活搜索空间，能够有效地找到等同于以前方法在CIFAR-10数据集上任务准确性和推理延迟方面的DNN架构，并且不需要手动设计或架构工程专业知识。

    

    在计算资源有限的条件下设计表现良好的深度神经网络，找到最优的通道维度（即DNN层中的过滤器数量）至关重要。最近的神经架构搜索工作旨在自动化DNN模型实现的优化。然而，现有的通道维度神经架构搜索方法依赖于固定的搜索空间，这阻碍了实现高效且完全自动化的解决方案。在这项工作中，我们提出了一种新颖的可微神经架构搜索方法，配备有效的动态通道分配算法，以实现通道维度的灵活搜索空间。我们展示了所提出的框架能够找到等同于以前方法在CIFAR-10数据集上任务准确性和推理延迟方面的DNN架构，architecture search阶段GPU-hours提高了1.3-1.7倍，内存要求提高了1.5-1.7倍。此外，所提出的框架不需要任何手动设计或架构工程专业知识，并且可以轻松扩展到各种DNN架构和不同的数据集。

    Finding optimal channel dimensions (i.e., the number of filters in DNN layers) is essential to design DNNs that perform well under computational resource constraints. Recent work in neural architecture search aims at automating the optimization of the DNN model implementation. However, existing neural architecture search methods for channel dimensions rely on fixed search spaces, which prevents achieving an efficient and fully automated solution. In this work, we propose a novel differentiable neural architecture search method with an efficient dynamic channel allocation algorithm to enable a flexible search space for channel dimensions. We show that the proposed framework is able to find DNN architectures that are equivalent to previous methods in task accuracy and inference latency for the CIFAR-10 dataset with an improvement of $1.3-1.7\times$ in GPU-hours and $1.5-1.7\times$ in the memory requirements during the architecture search stage. Moreover, the proposed frameworks do not re
    
[^48]: Mol-Instructions: 一个大规模生物分子指令数据集，为大语言模型提供支持

    Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.08018](http://arxiv.org/abs/2306.08018)

    Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。

    

    大语言模型（LLM）以其卓越的任务处理能力和创新的输出，在许多领域推动了重大进展。然而，它们在生物分子研究等专业领域的熟练应用还受到限制。为了解决这个挑战，我们介绍了Mol-Instructions，这是一个经过精心策划、专门针对生物分子领域设计的综合指令数据集。Mol-Instructions由三个关键组成部分组成：分子导向指令、蛋白质导向指令和生物分子文本指令，每个部分都被策划用于增强LLM对生物分子特性和行为的理解和预测能力。通过对代表性LLM的广泛指令调整实验，我们强调了Mol-Instructions在增强大模型在生物分子研究复杂领域内的适应能力和认知敏锐度方面的潜力，从而促进生物分子领域的进一步发展。

    Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
    
[^49]: 实现合成主动推理代理，第一部分：认识目标和图形说明语言

    Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language. (arXiv:2306.08014v1 [cs.AI])

    [http://arxiv.org/abs/2306.08014](http://arxiv.org/abs/2306.08014)

    本文介绍了自由能原理和主动推理的理论框架，并推导了适用于任意图形模型的主动推理版本。同时引入了一种新的图形说明语言（GSL）来明确规定系统目标。

    

    自由能原理（FEP）是一种描述系统如何通过最小化自由能泛函而自组织成具有连贯性、稳定结构（智能）的理论框架。主动推理（AIF）是FEP的一个推论，它明确了能够为未来进行规划（代理）的系统是如何通过最小化包含信息寻求组件的特定自由能泛函来运作的。本文是一个系列中的第一篇，我们在自由形式因子图上推导了AIF的合成版本。本文重点推导了AIF所使用的自由能泛函的局部版本。这使我们能够构造一个适用于任意图形模型并与有关消息传递算法的先前工作接口的AIF版本。结果消息是在我们的伴侣论文中得出的。我们还发现因子图形式中存在一个缺口。虽然因子图表达了生成模型，但在指定系统目标方面缺乏一个图形化语言。我们引入了一个因子图描述法的新扩展，称为图形说明语言（GSL），它使系统目标得到明确规定。

    The Free Energy Principle (FEP) is a theoretical framework for describing how (intelligent) systems self-organise into coherent, stable structures by minimising a free energy functional. Active Inference (AIF) is a corollary of the FEP that specifically details how systems that are able to plan for the future (agents) function by minimising particular free energy functionals that incorporate information seeking components. This paper is the first in a series of two where we derive a synthetic version of AIF on free form factor graphs. The present paper focuses on deriving a local version of the free energy functionals used for AIF. This enables us to construct a version of AIF which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms. The resulting messages are derived in our companion paper. We also identify a gap in the graphical notation used for factor graphs. While factor graphs are great at expressing a generative model, they have so
    
[^50]: TopP\&R: 具有鲁棒性的支持估计方法，用于评估生成模型中的保真度和多样性

    TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])

    [http://arxiv.org/abs/2306.08013](http://arxiv.org/abs/2306.08013)

    本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。

    

    本文提出了一种鲁棒可靠的生成模型评估指标，通过引入拓扑和统计处理进行严格的支持估计。现有的度量标准，如Inception Score（IS），Fr\'echet Inception Distance（FID）以及Precision and Recall（P\&R）的变体，严重依赖于从样本特征估计的支持。然而，尽管评估的质量完全取决于其可靠性，但其估计的可靠性并没有得到严肃的讨论（并被忽视）。本文提出了拓扑精度和召回率（TopP\&R，发音为“topper”），它提供了一种系统的方法来估计支持，仅保留具有一定置信水平的具有拓扑和统计上重要性的特征。这不仅使TopP\&R对于噪声特征具有强大的鲁棒性，而且还提供了统计一致性。我们的理论和实验结果表明，TopP\&R对于离群值和非独立同分布具有鲁棒性。

    We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&R is robust to outliers and non-independent and identically distributed
    
[^51]: 在非独立同分布场景下基于隐蔽后门的联邦学习隐私推断攻击

    Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios. (arXiv:2306.08011v1 [cs.LG])

    [http://arxiv.org/abs/2306.08011](http://arxiv.org/abs/2306.08011)

    本文提出了一个针对非独立同分布场景下联邦学习的隐蔽后门攻击方案，其中通过生成对抗网络提供补充数据集，有效应对数据异构问题和隐私推断攻击风险。

    

    联邦学习（FL）在现实情况下面临数据异构问题，但是这个问题往往被研究FL安全和隐私的研究所忽视。本文针对非独立同分布场景下的FL，提出了一个新型的私密推理加强的隐蔽后门攻击（PI-SBA）方案。该方案包括通过生成对抗网络（GAN）提供补充数据集的机制和基于源特定的后门学习策略。此外，该方案还能有效应对恶意客户通过隐私推断攻击窃取私有数据的风险。

    Federated learning (FL) naturally faces the problem of data heterogeneity in real-world scenarios, but this is often overlooked by studies on FL security and privacy. On the one hand, the effectiveness of backdoor attacks on FL may drop significantly under non-IID scenarios. On the other hand, malicious clients may steal private data through privacy inference attacks. Therefore, it is necessary to have a comprehensive perspective of data heterogeneity, backdoor, and privacy inference. In this paper, we propose a novel privacy inference-empowered stealthy backdoor attack (PI-SBA) scheme for FL under non-IID scenarios. Firstly, a diverse data reconstruction mechanism based on generative adversarial networks (GANs) is proposed to produce a supplementary dataset, which can improve the attacker's local data distribution and support more sophisticated strategies for backdoor attacks. Based on this, we design a source-specified backdoor learning (SSBL) strategy as a demonstration, allowing th
    
[^52]: DHBE: 通过受限对抗蒸馏的数据无关全面后门擦除来保护深度神经网络

    DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation. (arXiv:2306.08009v1 [cs.LG])

    [http://arxiv.org/abs/2306.08009](http://arxiv.org/abs/2306.08009)

    本文提出了一种数据无关的全面后门擦除（DHBE）框架，该框架将后门擦除任务视为一个统一的对抗过程，在转移干净数据的知识的同时保证擦除后门。通过对抗蒸馏和后门正则化两个不同的竞争过程，DHBE实现了一种统一的、无需数据，并且有效的后门擦除框架。

    

    后门攻击已成为深度神经网络（DNN）的紧急威胁。为防御后门攻击，许多工作都建立了一个分阶段的流程来从受害DNN中移除后门：检查、定位和擦除。然而，在只有少量干净数据可用的情况下，这种流程是脆弱的，而且不能在不牺牲模型准确性的情况下完全擦除后门。因此，本文提出了一种全新的数据无关全面后门擦除（DHBE）框架。该框架将后门擦除任务视为一个统一的对抗过程，在转移干净数据的知识的同时保证擦除后门。通过对抗蒸馏和后门正则化两个不同的竞争过程，DHBE实现了一种统一的、无需数据，并且有效的后门擦除框架。大量的实验证明，DHBE可以高成功率地擦除后门，同时保持模型准确性，胜过现有技术。

    Backdoor attacks have emerged as an urgent threat to Deep Neural Networks (DNNs), where victim DNNs are furtively implanted with malicious neurons that could be triggered by the adversary. To defend against backdoor attacks, many works establish a staged pipeline to remove backdoors from victim DNNs: inspecting, locating, and erasing. However, in a scenario where a few clean data can be accessible, such pipeline is fragile and cannot erase backdoors completely without sacrificing model accuracy. To address this issue, in this paper, we propose a novel data-free holistic backdoor erasing (DHBE) framework. Instead of the staged pipeline, the DHBE treats the backdoor erasing task as a unified adversarial procedure, which seeks equilibrium between two different competing processes: distillation and backdoor regularization. In distillation, the backdoored DNN is distilled into a proxy model, transferring its knowledge about clean data, yet backdoors are simultaneously transferred. In backdo
    
[^53]: 基于动态区间限制的深度强化学习在避障中的应用

    Dynamic Interval Restrictions on Action Spaces in Deep Reinforcement Learning for Obstacle Avoidance. (arXiv:2306.08008v1 [cs.LG])

    [http://arxiv.org/abs/2306.08008](http://arxiv.org/abs/2306.08008)

    本文提出两种在动态障碍物场景下处理任意数量区间限制的方法，能较好地完成避障任务，表现优于现有方法。

    

    深度强化学习算法通常在同一组动作上执行，但这在许多真实场景下都不够充分，因为每个步骤可用的动作组不同。本文考虑了动态障碍物场景中出现的区间限制问题。当需要避免导致碰撞的动作时，连续动作空间将被划分为变量部分。最近的研究对区间数量做出了较强假设，仅限于凸子集，并且可用的动作是根据观察结果学习的。因此，本文提出了两种方法，基于参数化强化学习和ConstraintNet，能处理任意数量的区间而不会受环境状态的影响。我们将这两种方法应用于避障任务，并将它们与文献中的惩罚、投影、替换以及离散和连续屏蔽方法进行比较。

    Deep reinforcement learning algorithms typically act on the same set of actions. However, this is not sufficient for a wide range of real-world applications where different subsets are available at each step. In this thesis, we consider the problem of interval restrictions as they occur in pathfinding with dynamic obstacles. When actions that lead to collisions are avoided, the continuous action space is split into variable parts. Recent research learns with strong assumptions on the number of intervals, is limited to convex subsets, and the available actions are learned from the observations. Therefore, we propose two approaches that are independent of the state of the environment by extending parameterized reinforcement learning and ConstraintNet to handle an arbitrary number of intervals. We demonstrate their performance in an obstacle avoidance task and compare the methods to penalties, projection, replacement, as well as discrete and continuous masking from the literature. The res
    
[^54]: 利用树突的特性推进机器学习和神经启发式计算

    Leveraging dendritic properties to advance machine learning and neuro-inspired computing. (arXiv:2306.08007v1 [cs.NE])

    [http://arxiv.org/abs/2306.08007](http://arxiv.org/abs/2306.08007)

    树突机制为AI领域提供了启发性的创新解决方案，包括信用分配、灾难性遗忘和高能耗等问题，为构建更强大、更节能的人工学习系统提供了有前途的新途径。

    

    大脑是一个非常能干和高效的系统。它可以使用极少的能量处理和存储大量嘈杂、非结构化的信息。相比之下，当前的人工智能系统需要巨大的资源进行训练，而仍然很难在生物代理器件轻松完成的任务中竞争。因此，脑启发式工程已经成为设计可持续的，新一代人工智能系统的有前途的新途径。在本文中，我们介绍了生物神经元的树突机制如何激发创新的解决方案来解决重要的人工智能问题，包括多层网络中的信用分配、灾难性遗忘和高能耗。这些发现为现有体系结构提供了令人兴奋的替代方案，展示了树突研究如何为构建更强大、更节能的人工学习系统铺平道路。

    The brain is a remarkably capable and efficient system. It can process and store huge amounts of noisy and unstructured information using minimal energy. In contrast, current artificial intelligence (AI) systems require vast resources for training while still struggling to compete in tasks that are trivial for biological agents. Thus, brain-inspired engineering has emerged as a promising new avenue for designing sustainable, next-generation AI systems. Here, we describe how dendritic mechanisms of biological neurons have inspired innovative solutions for significant AI problems, including credit assignment in multilayer networks, catastrophic forgetting, and high energy consumption. These findings provide exciting alternatives to existing architectures, showing how dendritic research can pave the way for building more powerful and energy-efficient artificial learning systems.
    
[^55]: 检测与分类旨在预防光伏系统故障的论文。

    Detection and classification of faults aimed at preventive maintenance of PV systems. (arXiv:2306.08004v1 [cs.LG])

    [http://arxiv.org/abs/2306.08004](http://arxiv.org/abs/2306.08004)

    本论文提出了一种随机森林算法的创新方法，用于检测细微的光伏故障并进行分类，提高了故障分类的计算时间，同时保持了高精度。

    

    光伏系统的诊断旨在检测、定位和识别故障。诊断这些故障对于保证能源生产和延长光伏发电厂的使用寿命至关重要。在文献中，已经为此提出了多种机器学习方法。然而，这些研究中很少有关注细微故障的检测和专门的特征提取和分类选择过程。细微故障是一种其特征签名与健康面板的特征签名难以区别的故障。作为检测细微故障的贡献，本研究提出了一种基于随机森林（RF）算法的创新方法。这种方法使用复杂的特征提取和选择方法，提高了故障分类的计算时间，同时保持了高精度。

    Diagnosis in PV systems aims to detect, locate and identify faults. Diagnosing these faults is vital to guarantee energy production and extend the useful life of PV power plants. In the literature, multiple machine learning approaches have been proposed for this purpose. However, few of these works have paid special attention to the detection of fine faults and the specialized process of extraction and selection of features for their classification. A fine fault is one whose characteristic signature is difficult to distinguish to that of a healthy panel. As a contribution to the detection of fine faults (especially of the snail trail type), this article proposes an innovative approach based on the Random Forest (RF) algorithm. This approach uses a complex feature extraction and selection method that improves the computational time of fault classification while maintaining high accuracy.
    
[^56]: DTW k-means聚类用于光伏模块故障检测

    DTW k-means clustering for fault detection in photovoltaic modules. (arXiv:2306.08003v1 [cs.LG])

    [http://arxiv.org/abs/2306.08003](http://arxiv.org/abs/2306.08003)

    本文提出了一种无监督聚类技术的方法，用于在大规模光伏电厂中检测和识别故障类型，采用动态时间规整（DTW）距离度量，能够提高聚类性能。

    

    光伏能源在全球的应用增加表明，光伏电厂的寿命和维护直接依赖于快速检测光伏电厂的严重故障的能力。为了解决这个检测问题，文献中提出了基于数据的方法。然而，这些之前的解决方案只考虑了一个或少数几个故障的具体行为。其中大多数方法可以被归为监督学习，需要大量标记努力（每种技术中明确识别的故障类型）。此外，大多数方法在PV电池或一个PV模块中验证。这在考虑它们的复杂性的大规模PV电厂中很难应用。相反，一些基于数据的无监督知名方法尝试检测异常，但不能精确地识别故障类型。其中表现最好的方法成功地将健康面板有效地分组并将其与故障面板分开。因此，本文提出了一个无监督聚类技术的方法，用于大规模PV电厂的一般故障检测。具体而言，对PV电厂中每个面板的时间序列数据集应用了k-means聚类算法。为了提高聚类性能，采用了一种广泛使用的相似性度量，即动态时间规整（DTW）距离，以克服PV面板对不同环境和操作条件的响应的变异性。实验结果表明，所提出的方法可以成功检测PV电厂中的故障并识别其类型。

    The increase in the use of photovoltaic (PV) energy in the world has shown that the useful life and maintenance of a PV plant directly depend on theability to quickly detect severe faults on a PV plant. To solve this problem of detection, data based approaches have been proposed in the literature.However, these previous solutions consider only specific behavior of one or few faults. Most of these approaches can be qualified as supervised, requiring an enormous labelling effort (fault types clearly identified in each technology). In addition, most of them are validated in PV cells or one PV module. That is hardly applicable in large-scale PV plants considering their complexity. Alternatively, some unsupervised well-known approaches based on data try to detect anomalies but are not able to identify precisely the type of fault. The most performant of these methods do manage to efficiently group healthy panels and separate them from faulty panels. In that way, this article presents an unsu
    
[^57]: 主动查询的马尔可夫形式主义

    A Markovian Formalism for Active Querying. (arXiv:2306.08001v1 [cs.LG])

    [http://arxiv.org/abs/2306.08001](http://arxiv.org/abs/2306.08001)

    本文提出了一个基于马尔可夫系统的形式化主义对主动学习进行概述，将主动学习过程作为一个部分可观察的马尔可夫系统的整体处理，并且阐述如何将查询、数据集增强、奖励更新等过程视为马尔可夫系统中元状态之间的转移。

    

    主动学习算法是人工智能领域中最近进展的重要部分。然而，该领域的研究变化很大，缺乏整体性的组织结构。我们提出了一个基于马尔可夫系统的形式化主义，用于对主动学习领域进行概述，并通过文献综述来展示我们所提出的形式化主义的组织能力。我们的形式化主义将主动学习过程作为一个部分可观察的马尔可夫系统的整体来处理。我们具体概述了查询、数据集增强、奖励更新以及其他主动学习方面如何视为马尔可夫系统中元状态之间的转移，并指导其他主动学习方面如何适应我们的形式化主义。

    Active learning algorithms have been an integral part of recent advances in artificial intelligence. However, the research in the field is widely varying and lacks an overall organizing leans. We outline a Markovian formalism for the field of active learning and survey the literature to demonstrate the organizing capability of our proposed formalism. Our formalism takes a partially observable Markovian system approach to the active learning process as a whole. We specifically outline how querying, dataset augmentation, reward updates, and other aspects of active learning can be viewed as a transition between meta-states in a Markovian system, and give direction into how other aspects of active learning can fit into our formalism.
    
[^58]: 利用领域预训练语言模型改进低患病率胸部病症的零样本检测

    Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models. (arXiv:2306.08000v1 [physics.med-ph])

    [http://arxiv.org/abs/2306.08000](http://arxiv.org/abs/2306.08000)

    该论文研究了如何利用领域预训练语言模型CX-BERT、BlueBERT和ClinicalBERT提高CLIP-like模型对低患病率胸部病症的零样本检测。实验结果表明，预训练的文本塔对于低患病率疾病的检测有显著的性能提升。这提示了未来可使用不同训练语言模型的集成模型进行进一步研究。

    

    零样本学习的最新进展使得可以利用成对的图像识别标签数据替代结构化标签，消除了对专家注释数据集的需求。像CLIP-based CheXzero这样的模型利用了这些在胸部X射线解释领域的进步。我们假设，使用CX-BERT、BlueBERT和ClinicalBERT等领域预训练模型，通过替换BERT权重来增加特定领域知识，有可能提高类似于CLIP的模型的性能，但代价是打破原始模型的对齐性。我们评估了具有特定领域预训练的零样本分类模型在检测低患病率病理方面的性能。尽管替换原始CLIP-BERT权重会降低模型在常见病理方面的性能，但我们发现预训练文本塔在低患病率疾病的检测中表现出色。这激发了未来使用不同训练语言模型组合的集成模型的可能性。

    Recent advances in zero-shot learning have enabled the use of paired image-text data to replace structured labels, replacing the need for expert annotated datasets. Models such as CLIP-based CheXzero utilize these advancements in the domain of chest X-ray interpretation. We hypothesize that domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model's alignment. We evaluate the performance of zero-shot classification models with domain-specific pre-training for detecting low-prevalence pathologies. Even though replacing the weights of the original CLIP-BERT degrades model performance on commonly found pathologies, we show that pre-trained text towers perform exceptionally better on low-prevalence diseases. This motivates future ensemble models with a combination of differently trained language models for maxima
    
[^59]: 互联网防火墙日志文件的多类分类的机器学习方法

    Machine Learning Approach on Multiclass Classification of Internet Firewall Log Files. (arXiv:2306.07997v1 [cs.CR])

    [http://arxiv.org/abs/2306.07997](http://arxiv.org/abs/2306.07997)

    该研究通过应用多种分类算法，分析互联网防火墙日志文件，以便更好地防御网络攻击并了解恶意操作何时以及如何影响互联网。

    

    防火墙是通过筛选所有进入（以及偶尔出现的）数据包来保护通信网络安全的关键组件。过滤是通过将传入的数据包与一组旨在防止恶意代码进入网络的规则进行比较来进行的。为了调节进入和离开网络的数据包流，互联网防火墙跟踪所有活动。日志文件的主要功能是帮助故障排除和诊断，同时也与系统审计和取证相关。防火墙的主要功能是防止发送恶意数据包。为了更好地防御网络攻击并了解恶意操作何时以及如何影响互联网，有必要检查日志文件。因此，防火墙决定是否'允许'、'拒绝'、'丢弃'或'重置-双方'传入和传出的数据包。在这项研究中，我们应用各种分类算法进行多类分类以便更好地理解和分析互联网防火墙日志文件。

    Firewalls are critical components in securing communication networks by screening all incoming (and occasionally exiting) data packets. Filtering is carried out by comparing incoming data packets to a set of rules designed to prevent malicious code from entering the network. To regulate the flow of data packets entering and leaving a network, an Internet firewall keeps a track of all activity. While the primary function of log files is to aid in troubleshooting and diagnostics, the information they contain is also very relevant to system audits and forensics. Firewalls primary function is to prevent malicious data packets from being sent. In order to better defend against cyberattacks and understand when and how malicious actions are influencing the internet, it is necessary to examine log files. As a result, the firewall decides whether to 'allow,' 'deny,' 'drop,' or 'reset-both' the incoming and outgoing packets. In this research, we apply various categorization algorithms to make se
    
[^60]: 基于语义的神经网络修复

    Semantic-Based Neural Network Repair. (arXiv:2306.07995v1 [cs.LG])

    [http://arxiv.org/abs/2306.07995](http://arxiv.org/abs/2306.07995)

    本文提出了一种自动修复神经网络错误的方法，基于深度学习层的可执行语义，并专注于实践中常见的四种错误。

    

    近年来，神经网络已经广泛应用于多个领域，其中包括许多安全关键系统。神经网络是通过在 TensorFlow 和 PyTorch 等框架中进行编程构建（和训练）的。开发人员可以应用丰富的预定义层手动编写神经网络或通过 AutoML 自动生成网络。由于必须满足使用这些层的非平凡约束条件，所以使用不同层来组合神经网络容易出现错误。在本文中，我们提出了一种自动修复错误神经网络的方法。挑战在于识别出对网络进行最小修改以使其变为有效的修改。修改一层可能会对随后的层产生级联效应，因此我们的方法必须递归地搜索以识别“全局”最小修改。我们的方法基于深度学习层的可执行语义，并专注于实践中常见的四种错误。我们对我们的方法进行了验证。

    Recently, neural networks have spread into numerous fields including many safety-critical systems. Neural networks are built (and trained) by programming in frameworks such as TensorFlow and PyTorch. Developers apply a rich set of pre-defined layers to manually program neural networks or to automatically generate them (e.g., through AutoML). Composing neural networks with different layers is error-prone due to the non-trivial constraints that must be satisfied in order to use those layers. In this work, we propose an approach to automatically repair erroneous neural networks. The challenge is in identifying a minimal modification to the network so that it becomes valid. Modifying a layer might have cascading effects on subsequent layers and thus our approach must search recursively to identify a "globally" minimal modification. Our approach is based on an executable semantics of deep learning layers and focuses on four kinds of errors which are common in practice. We evaluate our appro
    
[^61]: MSSRNet: 无监督文本风格转换中的顺序风格表示操作

    MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer. (arXiv:2306.07994v1 [cs.CL])

    [http://arxiv.org/abs/2306.07994](http://arxiv.org/abs/2306.07994)

    本文提出了一种新方法，通过为每个词汇分配单独的风格向量来对文本进行风格转换，并引入基于教师-学生学习的对抗性培训框架，以提高培训稳定性，实现了双风格转换和多风格转换两种情况下的明显提高的风格转换准确性和内容保留。

    

    无监督文本风格转移任务旨在将文本重写为目标风格，同时保留其主要内容。传统方法依赖于使用固定大小的向量来调节文本风格，这很难准确传达每个单独令牌的风格强度。事实上，文本的每个令牌都包含不同的风格强度，并对整体风格产生不同的贡献。我们提出的方法通过为文本中的每个令牌分配单独的风格向量来解决这个问题，允许对风格强度进行细粒度的控制和操作。此外，我们引入了一个基于教师-学生学习的对抗性培训框架，以增强培训稳定性并减轻高维优化的复杂性。我们实验的结果证明了我们的方法在双风格转换和多风格转换设置中，具有明显提高的风格转换准确性和内容保留的效果。

    Unsupervised text style transfer task aims to rewrite a text into target style while preserving its main content. Traditional methods rely on the use of a fixed-sized vector to regulate text style, which is difficult to accurately convey the style strength for each individual token. In fact, each token of a text contains different style intensity and makes different contribution to the overall style. Our proposed method addresses this issue by assigning individual style vector to each token in a text, allowing for fine-grained control and manipulation of the style strength. Additionally, an adversarial training framework integrated with teacher-student learning is introduced to enhance training stability and reduce the complexity of high-dimensional optimization. The results of our experiments demonstrate the efficacy of our method in terms of clearly improved style transfer accuracy and content preservation in both two-style transfer and multi-style transfer settings.
    
[^62]: 安全的视觉感知推荐系统：一种对抗图像重构及检测框架

    Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework. (arXiv:2306.07992v1 [cs.CV])

    [http://arxiv.org/abs/2306.07992](http://arxiv.org/abs/2306.07992)

    本文提出了一种对抗图像重构及检测框架来保护视觉感知推荐系统，能够防御局部扰动为特征的对抗攻击并且能够在干净和对抗性图像上进行训练来检测对抗性图像。

    

    随着富含图片等视觉数据与物品关联度增加，视觉感知推荐系统（VARS）已被广泛应用于不同应用领域。最近的研究表明，VARS易受到物品-图像对抗攻击的攻击，这些攻击向与这些物品关联的干净图像添加人类无法感知的扰动。对VARS的攻击为广泛使用VARS的许多应用（如电子商务和社交网络）带来新的安全挑战。如何保护VARS免受此类对抗攻击成为一个关键的问题。目前，尚缺乏系统地研究如何设计针对VARS视觉攻击的安全防御策略。本文提出了一种对抗图像重构及检测框架来保护VARS，我们的方法可以同时(1)通过基于全局视觉传输的图像重构来防御以局部扰动为特征的对抗攻击，(2)使用在少量干净和对抗性图像上训练的检测模型来检测对抗性图像。实验结果表明，我们的框架能够有效地防御各种物品-图像对抗攻击对VARS的影响。

    With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision tra
    
[^63]: 跨体系结构物联网恶意软件威胁调查综述

    A Survey on Cross-Architectural IoT Malware Threat Hunting. (arXiv:2306.07989v1 [cs.CR])

    [http://arxiv.org/abs/2306.07989](http://arxiv.org/abs/2306.07989)

    本文综述了跨体系结构IoT恶意软件检测和分类方法的最新发展和实际挑战。

    

    近年来，非Windows恶意软件威胁的增加已经成为网络安全界的焦点。围绕Hunting Windows PE-Based Malwares的研究工作正在成熟，而针对Linux恶意软件威胁的发展相对较少。随着物联网（IoT）时代的到来，融入人类生活的智能设备已经成为黑客进行恶意活动的一条通道。IoT设备采用各种基于Unix的架构，遵循ELF（可执行和可链接格式）作为它们的标准二进制文件规范。本研究旨在提供关于跨体系结构IoT恶意软件检测和分类方法的最新发展的全面调查。在现代分类法的帮助下，我们讨论了在调查工作中采用的特征表示，特征提取技术和机器学习模型。我们进一步提供了关于跨体系结构IoT恶意软件威胁调查中的实际挑战的更多见解。

    In recent years, the increase in non-Windows malware threats had turned the focus of the cybersecurity community. Research works on hunting Windows PE-based malwares are maturing, whereas the developments on Linux malware threat hunting are relatively scarce. With the advent of the Internet of Things (IoT) era, smart devices that are getting integrated into human life have become a hackers highway for their malicious activities. The IoT devices employ various Unix-based architectures that follow ELF (Executable and Linkable Format) as their standard binary file specification. This study aims at providing a comprehensive survey on the latest developments in cross-architectural IoT malware detection and classification approaches. Aided by a modern taxonomy, we discuss the feature representations, feature extraction techniques, and machine learning models employed in the surveyed works. We further provide more insights on the practical challenges involved in cross-architectural IoT malwar
    
[^64]: 链环轨道：比特币区块链的拓扑地址嵌入

    Chainlet Orbits: Topological Address Embedding for the Bitcoin Blockchain. (arXiv:2306.07974v1 [cs.CR])

    [http://arxiv.org/abs/2306.07974](http://arxiv.org/abs/2306.07974)

    本研究利用拓扑特征嵌入比特币地址，提出了一种新方法“链环轨道”，用于探测比特币网络中的电子犯罪行为。

    

    比特币等加密货币的兴起使得具有一定匿名性的交易变得普遍，同时也带来了各种非法活动的激增，包括勒索软件支付和暗网市场上的交易等。这些非法活动通常使用比特币作为首选支付方式。本文提出了一种称为“链环轨道”的有效解决方案，通过利用交易中比特币地址的拓扑特征来进行地址嵌入。 通过使用我们创新的地址嵌入方案，我们研究了比特币网络中的电子犯罪，着重关注从非法行为中产生的特定子结构。我们的节点分类实验的结果证明了我们的方法的有效性和准确性。

    The rise of cryptocurrencies like Bitcoin, which enable transactions with a degree of pseudonymity, has led to a surge in various illicit activities, including ransomware payments and transactions on darknet markets. These illegal activities often utilize Bitcoin as the preferred payment method. However, current tools for detecting illicit behavior either rely on a few heuristics and laborious data collection processes or employ computationally inefficient graph neural network (GNN) models that are challenging to interpret.  To overcome the computational and interpretability limitations of existing techniques, we introduce an effective solution called Chainlet Orbits. This approach embeds Bitcoin addresses by leveraging their topological characteristics in transactions. By employing our innovative address embedding, we investigate e-crime in Bitcoin networks by focusing on distinctive substructures that arise from illicit behavior.  The results of our node classification experiments de
    
[^65]: 基于互信息的协作推理隐私保护方法

    PrivaScissors: Enhance the Privacy of Collaborative Inference through the Lens of Mutual Information. (arXiv:2306.07973v1 [cs.CR])

    [http://arxiv.org/abs/2306.07973](http://arxiv.org/abs/2306.07973)

    PrivaScissors是一种防御策略，用于减少模型中间结果和设备数据、预测之间的互信息，以增强边缘-云协作推理的隐私保护。

    

    边缘-云协作推理为物联网设备支持深度学习应用提供了强大的计算能力，同时也保护了原始数据的隐私。然而，之前的研究表明，协作推理仍然会暴露设备的数据和预测结果。为了增强协作推理的隐私保护，我们提出了一种叫做PrivaScissors的防御策略，该策略旨在减少模型中间结果和设备数据、预测之间的互信息。我们在多个数据集上评估了PrivaScissors在不同攻击情况下的性能，并提供了理论上的鲁棒性保证。

    Edge-cloud collaborative inference empowers resource-limited IoT devices to support deep learning applications without disclosing their raw data to the cloud server, thus preserving privacy. Nevertheless, prior research has shown that collaborative inference still results in the exposure of data and predictions from edge devices. To enhance the privacy of collaborative inference, we introduce a defense strategy called PrivaScissors, which is designed to reduce the mutual information between a model's intermediate outcomes and the device's data and predictions. We evaluate PrivaScissors's performance on several datasets in the context of diverse attacks and offer a theoretical robustness guarantee.
    
[^66]: 利用机器学习进行多链DeFi欺诈检测

    Leveraging Machine Learning for Multichain DeFi Fraud Detection. (arXiv:2306.07972v1 [q-fin.GN])

    [http://arxiv.org/abs/2306.07972](http://arxiv.org/abs/2306.07972)

    该论文提出了一种利用机器学习进行多链DeFi欺诈检测的方法，并在最广泛使用的23个DeFi协议的交易数据集上进行了评估。

    

    自2008年比特币推出以来，随着无需许可的区块链的出现，人们意识到它们最适合的用例与使金融系统及其优势可供每个人无缝使用而无需依赖任何受信任的中介有关。跨链智能合约提供了一个去中心化金融（DeFi）生态系统，用户可以与借贷池、自动市场制造商（AMM）交易所、稳定币、衍生品等进行交互，其累计锁定价值已超过1600亿美元。尽管DeFi带来了高额回报，但也带来了许多风险。多年来发生了许多金融罪案，使得恶意活动的早期检测成为了高度优先考虑的问题。该提议框架介绍了一种有效的方法，从不同链中提取一组功能，包括最大的以太坊，并使用我们收集的最广泛DeFi协议的交易数据集进行评估（23个）。

    Since the inception of permissionless blockchains with Bitcoin in 2008, it became apparent that their most well-suited use case is related to making the financial system and its advantages available to everyone seamlessly without depending on any trusted intermediaries. Smart contracts across chains provide an ecosystem of decentralized finance (DeFi), where users can interact with lending pools, Automated Market Maker (AMM) exchanges, stablecoins, derivatives, etc. with a cumulative locked value which had exceeded 160B USD. While DeFi comes with high rewards, it also carries plenty of risks. Many financial crimes have occurred over the years making the early detection of malicious activity an issue of high priority. The proposed framework introduces an effective method for extracting a set of features from different chains, including the largest one, Ethereum and it is evaluated over an extensive dataset we gathered with the transactions of the most widely used DeFi protocols (23 in t
    
[^67]: 对称张量分解问题的对称性与临界点

    Symmetry & Critical Points for Symmetric Tensor Decompositions Problems. (arXiv:2306.07886v1 [math.OC])

    [http://arxiv.org/abs/2306.07886](http://arxiv.org/abs/2306.07886)

    本文研究了将一个实对称张量分解成秩为1项之和的非凸优化问题，得到了精确的分析估计，并发现了各种阻碍局部优化方法的几何障碍和由于对称性导致的丰富的临界点集合。

    

    本文考虑了将一个实对称张量分解成秩为1项之和的非凸优化问题。利用其丰富的对称结构，导出Puiseux级数表示的一系列临界点，并获得了关于临界值和Hessian谱的精确分析估计。这些结果揭示了各种几何障碍，阻碍了局部优化方法的使用，最后，利用一个牛顿多面体论证了固定对称性的所有临界点的完全枚举，并证明了与全局最小值的集合相比，由于对称性的存在，临界点的集合可能会显示出组合的丰富性。

    We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains 
    
[^68]: BeliefPPG: 通过置信传播从PPG信号中获得具有不确定性感知的心率估计

    BeliefPPG: Uncertainty-aware Heart Rate Estimation from PPG signals via Belief Propagation. (arXiv:2306.07730v1 [cs.LG])

    [http://arxiv.org/abs/2306.07730](http://arxiv.org/abs/2306.07730)

    通过离散时间随机过程将心率演变表示为隐藏马尔科夫模型，使用训练的神经网络计算PPG信号窗口内的可能心率值分布，然后通过置信传播结合统计分布监测心率变化以优化这些估计，并获得涵盖心率值范围的量化概率分布以捕获固有预测不确定性的良好校准估计。

    

    我们提出了一种新颖的基于学习的方法，通过离散时间随机过程将心率的演变表示为隐藏马尔可夫模型，并通过训练的神经网络为给定的PPG信号窗口导出可能心率值的分布。使用置信传播，在时间上下文中结合心率变化的统计分布以优化这些估计。从此，我们获得了一种量化的概率分布，涵盖了可能心率值的这个范围，这可以捕获固有预测不确定性的有意义且良好校准的估计。我们在八个公共数据集上进行了三个不同的交叉验证实验，证明了我们方法的鲁棒性。

    We present a novel learning-based method that achieves state-of-the-art performance on several heart rate estimation benchmarks extracted from photoplethysmography signals (PPG). We consider the evolution of the heart rate in the context of a discrete-time stochastic process that we represent as a hidden Markov model. We derive a distribution over possible heart rate values for a given PPG signal window through a trained neural network. Using belief propagation, we incorporate the statistical distribution of heart rate changes to refine these estimates in a temporal context. From this, we obtain a quantized probability distribution over the range of possible heart rate values that captures a meaningful and well-calibrated estimate of the inherent predictive uncertainty. We show the robustness of our method on eight public datasets with three different cross-validation experiments.
    
[^69]: 基于双曲图扩散模型的分子生成

    Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])

    [http://arxiv.org/abs/2306.07618](http://arxiv.org/abs/2306.07618)

    本文提出了基于双曲图扩散模型的分子生成方法，可以更全面地捕捉分子的内部非欧几里德结构，实现数据生成，并提取复杂几何特征的能力。

    

    最近，扩散模型在数据生成方面取得了显著的成果，例如生成高质量的图像。然而，化学分子通常具有复杂的非欧几里德空间结构，其行为动态变化且难以预测。大多数现有的扩散模型高度依赖于计算欧几里德空间中的概率分布，即高斯分布，不能捕捉分子的内部非欧几里德结构，特别是分子所表示的隐式流形表面的分层结构。观察到，双曲嵌入空间中的复杂分层结构变得更加明显且更容易被捕捉。为了充分利用扩散模型的数据生成能力和提取复杂几何特征的双曲嵌入的强大能力，我们提出将扩散模型扩展到双曲流形上进行分子生成，即基于双曲图扩散模型的分子生成。

    Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
    
[^70]: 在在线推荐系统中激励高质量内容

    Incentivizing High-Quality Content in Online Recommender Systems. (arXiv:2306.07479v1 [cs.GT])

    [http://arxiv.org/abs/2306.07479](http://arxiv.org/abs/2306.07479)

    本文研究了在线推荐系统中激励高质量内容的算法问题，经典的在线学习算法会激励生产者创建低质量的内容，但本文提出的一种算法通过惩罚低质量内容的创建者，成功地激励了生产者创造高质量的内容。

    

    对于像TikTok和YouTube这样的内容推荐系统，平台的决策算法塑造了内容生产者的激励，包括生产者在内容质量上投入多少努力。许多平台采用在线学习，这会产生跨时间的激励，因为今天生产的内容会影响未来内容的推荐。在本文中，我们研究了在线学习产生的激励，分析了在纳什均衡下生产的内容质量。我们发现，像Hedge和EXP3这样的经典在线学习算法会激励生产者创建低质量的内容。特别地，内容质量在学习率方面有上限，并且随着典型学习率进展而趋近于零。在这一负面结果的基础上，我们设计了一种不同的学习算法——基于惩罚创建低质量内容的生产者——正确激励生产者创建高质量内容。我们的算法依赖于新颖的策略性赌博机问题，并克服了在组合设置中应用对抗性技术的挑战。在模拟和真实数据的实验中，我们的算法成功地激励生产者创建高质量内容。

    For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality co
    
[^71]: DeepTransition：可行性导致步态转换的出现

    DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills. (arXiv:2306.07419v1 [cs.RO])

    [http://arxiv.org/abs/2306.07419](http://arxiv.org/abs/2306.07419)

    本文通过深度强化学习和机器人工具的互动研究，证明了可行性是四足动物步态转换的重要标准。其中，步-小跑步态转换能够在平坦地形上同时提高可行性和节能效果。

    

    四足动物在改变运动速度时能够无缝地转换步态。本文提出可行性（即避免跌倒）代表步态转换的一个重要标准。通过利用深度强化学习和机器人工具，我们研究了步态转换的出现。一致于四足动物数据，我们证明了在平坦地形上，四足机器人的步-小跑步态转换能同时提高可行性和节能效果。此外，我们研究了离散地形（即穿越连续间隔）对强制步态转换的影响，并找到足-蹦步态的出现。

    Quadruped animals seamlessly transition between gaits as they change locomotion speeds. While the most widely accepted explanation for gait transitions is energy efficiency, there is no clear consensus on the determining factor, nor on the potential effects from terrain properties. In this article, we propose that viability, i.e. the avoidance of falls, represents an important criterion for gait transitions. We investigate the emergence of gait transitions through the interaction between supraspinal drive (brain), the central pattern generator in the spinal cord, the body, and exteroceptive sensing by leveraging deep reinforcement learning and robotics tools. Consistent with quadruped animal data, we show that the walk-trot gait transition for quadruped robots on flat terrain improves both viability and energy efficiency. Furthermore, we investigate the effects of discrete terrain (i.e. crossing successive gaps) on imposing gait transitions, and find the emergence of trot-pronk transit
    
[^72]: 基于优化启发式深度神经网络的自监督高光谱图像修复

    Self-Supervised Hyperspectral Inpainting with the Optimisation inspired Deep Neural Network Prior. (arXiv:2306.07308v1 [eess.IV])

    [http://arxiv.org/abs/2306.07308](http://arxiv.org/abs/2306.07308)

    本文提出了一种自监督的高光谱图像修复算法LRS-PnP-DIP，该算法能够在高光谱图像中精确预测缺失像素和带，其在实验中表现优异，达到或超过了其他学习方法。

    

    高光谱图像具有成百上千个窄带谱段，传递了大量的空间和谱信息。然而，由于仪器误差和大气变化，实践中得到的高光谱图像常常被噪声和坏点污染，导致缺失信息可能严重破坏后续应用。本文提出了一种新的高光谱图像取样点修复算法，称为低秩稀疏约束插入播放算法（LRS-PnP）。结果表明，即使图像的所有光谱带都丢失，LRS-PnP也能够预测缺失的像素和带。将LRS-PnP与Deep Image Prior（DIP）相结合，进一步扩展了一种自监督模型，称为LRS-PnP-DIP。在一系列真实数据实验中，结果表明，与其他基于学习的方法相比，LRS-PnP-DIP具有最先进的修复性能或胜过它们。

    Hyperspectral Image (HSI)s cover hundreds or thousands of narrow spectral bands, conveying a wealth of spatial and spectral information. However, due to the instrumental errors and the atmospheric changes, the HSI obtained in practice are often contaminated by noise and dead pixels(lines), resulting in missing information that may severely compromise the subsequent applications. We introduce here a novel HSI missing pixel prediction algorithm, called Low Rank and Sparsity Constraint Plug-and-Play (LRS-PnP). It is shown that LRS-PnP is able to predict missing pixels and bands even when all spectral bands of the image are missing. The proposed LRS-PnP algorithm is further extended to a self-supervised model by combining the LRS-PnP with the Deep Image Prior (DIP), called LRS-PnP-DIP. In a series of experiments with real data, It is shown that the LRS-PnP-DIP either achieves state-of-the-art inpainting performance compared to other learning-based methods, or outperforms them.
    
[^73]: 取消七年的算法公平性后处理

    Unprocessing Seven Years of Algorithmic Fairness. (arXiv:2306.07261v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07261](http://arxiv.org/abs/2306.07261)

    该论文取消了算法公平性中的后处理方法，并发现后处理实现的公平性-准确性Pareto边界包含了可评估的所有其他方法。

    

    七年前，研究人员提出了一种后处理方法，以使模型在不同人口群体中的误差率相等。这项工作启动了数百篇论文，声称能够改进后处理基线。我们通过对几个表格数据集上数千个模型评估的实证评估来评估这些声明。我们发现，后处理实现的公平性-准确性Pareto边界包含我们可以评估的所有其他方法。这样做，我们解决了两个常见的方法论错误，这些错误困扰了以前的观察结果。一个与使用不同的无约束基础模型比较方法有关。另一个涉及实现不同的约束放松水平的方法。我们研究的核心是一种简单的想法，我们称之为取消处理，大致对应于后处理的反演。取消处理允许直接比较使用不同基础模型和放松级别的方法。解读我们的发现。

    Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Interpreting our findi
    
[^74]: Strokes2Surface：从四维建筑设计素描中恢复曲线网络

    Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2306.07220](http://arxiv.org/abs/2306.07220)

    本文介绍了Strokes2Surface，它可从建筑师的笔画中恢复出曲线网络，对于建筑设计中的概念设计和数字建模之间的桥梁具有重要意义。

    

    本文介绍了一个离线几何重建管道Strokes2Surface，它是基于4D Sketching Interface，MR.Sketch的目标是面向建筑设计的。该管道从设计师绘制的笔画中恢复曲线网络，因此在建筑设计的概念设计和数字建模阶段之间建立了桥梁。我们的管道的输入包括3D笔画的折线顶点及其相应的时间戳（作为第四个维度），以及额外的几何和笔触相关的记录属性。基于素描合并和基于素描建模方法的启发，我们的管道利用这些数据并组合三个机器学习（ML）模型；一个分类器和两个聚类模型。特别是，根据建筑设计素描中设计师通常采用的实践观察，我们解决了一个二元分类问题，以识别一笔画是描绘边界和边缘还是用于填充所需建筑物的封闭区域和表面。

    We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
    
[^75]: NF4不是信息理论上最优的（并且这是好事）

    NF4 Isn't Information Theoretically Optimal (and that's Good). (arXiv:2306.06965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06965](http://arxiv.org/abs/2306.06965)

    论文讨论了基于absmax的分块量化，推出该方法不是信息理论上的最优选择。作者优化L1重构误差，提出改进方法，适用于大块量化。在小块量化里，两种方法性能类似。

    

    本文分享了一些关于基于absmax的分块量化的简单计算和实验，该方法被Dettmers等人在2023年提出，他们提出的NF4数据类型被认为是用于表示正态分布权重时的信息理论最优。我展示了这不完全是情况，因为要量化的值的分布取决于块大小。我试图应用这些见解，基于最小化期望L1重构误差而不是分位数方法推导出一个改进的编码。这导致在更大的量化块大小时性能得到改善，而在较小的块大小时两个编码表现类似。

    This note shares some simple calculations and experiments related to absmax-based blockwise quantization, as used in Dettmers et al., 2023. Their proposed NF4 data type is said to be information theoretically optimal for representing normally distributed weights. I show that this can't quite be the case, as the distribution of the values to be quantized depends on the block-size. I attempt to apply these insights to derive an improved code based on minimizing the expected L1 reconstruction error, rather than the quantile based method. This leads to improved performance for larger quantization block sizes, while both codes perform similarly at smaller block sizes.
    
[^76]: 局部自适应时空图神经网络

    Localised Adaptive Spatial-Temporal Graph Neural Network. (arXiv:2306.06930v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06930](http://arxiv.org/abs/2306.06930)

    本文提出了自适应图稀疏化算法，成功将自适应空间-时间图神经网络（ASTGNN）本地化至极致，无需空间图即可达到同样的测试准确性。

    

    空间-时间图模型广泛用于抽象和模拟空间和时间依赖性。本文探讨了以下问题：我们是否以及在何种程度上可以本地化空间-时间图模型？我们将范围限定在自适应空间-时间图神经网络（ASTGNN）上，这是最先进的模型架构。我们的本地化方法涉及将空间图邻接矩阵稀疏化。为此，我们提出了自适应图稀疏化（AGS），一种成功实现ASTGNN本地化的图稀疏化算法（完全本地化）。我们将AGS应用于两种不同的ASTGNN架构和9个空间-时间数据集。有趣的是，我们观察到ASTGNN中的空间图可以稀疏化超过99.5％，而没有任何测试准确性的下降。此外，即使ASTGNN完全本地化，变得无图且纯粹的时间，大多数测试数据集的准确性也没有下降。

    Spatial-temporal graph models are prevailing for abstracting and modelling spatial and temporal dependencies. In this work, we ask the following question: whether and to what extent can we localise spatial-temporal graph models? We limit our scope to adaptive spatial-temporal graph neural networks (ASTGNNs), the state-of-the-art model architecture. Our approach to localisation involves sparsifying the spatial graph adjacency matrices. To this end, we propose Adaptive Graph Sparsification (AGS), a graph sparsification algorithm which successfully enables the localisation of ASTGNNs to an extreme extent (fully localisation). We apply AGS to two distinct ASTGNN architectures and nine spatial-temporal datasets. Intriguingly, we observe that spatial graphs in ASTGNNs can be sparsified by over 99.5\% without any decline in test accuracy. Furthermore, even when ASTGNNs are fully localised, becoming graph-less and purely temporal, we record no drop in accuracy for the majority of tested datase
    
[^77]: TASRA: 一份关于人工智能可能带来的社会规模风险的分类与分析

    TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI. (arXiv:2306.06924v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.06924](http://arxiv.org/abs/2306.06924)

    本文探讨了人工智能可能带来的社会规模风险的分类和分析，其中提出了一种基于问责制的分类法。针对可能出现的风险类型提供了实际的证明，并指出需要联合技术和政策解决方案。

    

    虽然最近一些作品已经确认了人工智能可能给人类带来的社会规模和灭绝级的风险，但几乎没有人尝试过进行全面归纳这些风险。许多全面性的分类法都是可能的，并且有些是有用的——尤其是如果它们揭示了新的风险或安全的实际方法。本文探讨了一种基于问责制的分类法：哪些行为导致了风险，行动者是否统一，他们是否是蓄意的。我们还提供了故事来说明各种风险类型如何发挥作用，包括来自许多人工智能系统意外相互作用的风险，以及来自蓄意滥用的风险，需要联合技术和政策解决方案。

    While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\em exhaustive taxonomy} of such risks. Many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.
    
[^78]: 基于ViT视觉转换器分析胸部X-ray图像以增强COVID-19诊断

    Enhancing COVID-19 Diagnosis through Vision Transformer-Based Analysis of Chest X-ray Images. (arXiv:2306.06914v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2306.06914](http://arxiv.org/abs/2306.06914)

    本研究提出了一种基于ViT视觉转换器处理胸部X-ray图像来自动诊断COVID-19的创新方法， 在二元和三元分类表现方面都取得了良好的成果。

    

    新冠肺炎的爆发引起了全球性的公共卫生危机，需要通过不同的诊断方式对个体进行诊断。放射学影像学，特别是X-ray成像，已被公认为是检测和描述COVID-19的重要工具。最近的研究揭示了X-ray图像中关于病毒的宝贵见解，引发了探索利用人工智能技术提高诊断精度的方法学。本研究提出了一种创新的框架，旨在自动诊断 COVID-19，通过精调预训练的 Vision Transformer (ViT) 模型来处理原始胸部 X-ray 图像。所开发的模型在二元分类表现（区分COVID-19和正常情况）和三元分类表现（区分COVID-19、病毒性肺炎和正常情况）方面进行了评估。

    The advent of 2019 Coronavirus (COVID-19) has engendered a momentous global health crisis, necessitating the identification of the ailment in individuals through diverse diagnostic modalities. Radiological imaging, particularly the deployment of X-ray imaging, has been recognized as a pivotal instrument in the detection and characterization of COVID-19. Recent investigations have unveiled invaluable insights pertaining to the virus within X-ray images, instigating the exploration of methodologies aimed at augmenting diagnostic accuracy through the utilization of artificial intelligence (AI) techniques. The current research endeavor posits an innovative framework for the automated diagnosis of COVID-19, harnessing raw chest X-ray images, specifically by means of fine-tuning pre-trained Vision Transformer (ViT) models. The developed models were appraised in terms of their binary classification performance, discerning COVID-19 from Normal cases, as well as their ternary classification per
    
[^79]: TrojPrompt：基于黑盒方式的预训练语言模型木马攻击

    TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2306.06815](http://arxiv.org/abs/2306.06815)

    本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。

    

    Prompt学习被证明在提高预训练语言模型（PLM）适应性方面非常有效，超越了传统的微调范式，并在专为少样本学习场景量身定制的应用程序和API中展现了杰出的前景。但是，尽管prompt学习的API越来越受欢迎，但它们的安全问题仍未得到充分探索。本文在prompt学习的PLM API的特洛伊易感性方面进行了开创性研究。我们发现，离散提示，少样本和黑盒设置是几个关键挑战，限制了现有后门攻击的适用性。为了解决这些挑战，我们提出了TrojPrompt，这是一种自动的黑盒框架，可有效生成通用的和隐秘的触发器，并将特洛伊木马插入硬提示。具体而言，我们提出了一种API驱动的通用触发器发现算法，通过查询受害者PLM API，为各种输入生成通用触发器。

    Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
    
[^80]: 自监督等式嵌入深度Lagrange对偶算法优化逼近限制优化问题

    Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization. (arXiv:2306.06674v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.06674](http://arxiv.org/abs/2306.06674)

    该论文提出了一种自监督等式嵌入深度Lagrange对偶算法，用于解决不带标签的逼近限制优化问题。此方法通过在神经网络中嵌入等式约束来确保可行解，并使用原始-对偶方法进行训练，同时DeepLDE取得了最好的优化结果。

    

    在限制优化问题中，传统求解方法通常计算量较大，特别是在规模较大、时间敏感的问题上更是如此。因此，使用神经网络作为快速最优解逼近器引起了人们的越来越大兴趣，但是将约束条件与神经网络结合起来是具有挑战性的。为此，我们提出了一种称为DeepLDE的深度Lagrange对偶算法，该框架学习在不使用标签的情况下寻找最优解，通过将等式约束嵌入神经网络来确保可行解，并使用原始-对偶方法对不等式约束进行训练。此外，我们证明了DeepLDE的收敛性，并表明仅靠原始-对偶学习方法无法确保等式约束，需要等式嵌入的帮助。在凸、非凸和交流最优潮流（AC-OPF）问题的模拟结果中，我们展示了DeepLDE的最优性能而且始终保证可行解。

    Conventional solvers are often computationally expensive for constrained optimization, particularly in large-scale and time-critical problems. While this leads to a growing interest in using neural networks (NNs) as fast optimal solution approximators, incorporating the constraints with NNs is challenging. In this regard, we propose deep Lagrange dual with equality embedding (DeepLDE), a framework that learns to find an optimal solution without using labels. To ensure feasible solutions, we embed equality constraints into the NNs and train the NNs using the primal-dual method to impose inequality constraints. Furthermore, we prove the convergence of DeepLDE and show that the primal-dual learning method alone cannot ensure equality constraints without the help of equality embedding. Simulation results on convex, non-convex, and AC optimal power flow (AC-OPF) problems show that the proposed DeepLDE achieves the smallest optimality gap among all the NN-based approaches while always ensuri
    
[^81]: 基于条件归一化流的快速光场三维显微镜及其分布外检测和适应性

    Fast light-field 3D microscopy with out-of-distribution detection and adaptation through Conditional Normalizing Flows. (arXiv:2306.06408v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2306.06408](http://arxiv.org/abs/2306.06408)

    本文提出了一种基于条件归一化流框架的快速三维重建方法，并在其中融入分布外检测和适应性机制，以确保重建结果的稳健性和可靠性，该方法可以实现实时重建速度和高质量的重建效果，是分析活体生物的有前途的工具。

    

    实时三维荧光显微镜对于监测神经活动等生物学研究至关重要。本文提出了一种基于条件归一化流框架的快速三维重建方法，并将分布外检测和适应性机制融入其中，以确保重建结果的稳健性和可靠性。实验结果表明，该方法可以实现实时重建速度和高质量的重建效果，是分析活体生物的有前途的工具。

    Real-time 3D fluorescence microscopy is crucial for the spatiotemporal analysis of live organisms, such as neural activity monitoring. The eXtended field-of-view light field microscope (XLFM), also known as Fourier light field microscope, is a straightforward, single snapshot solution to achieve this. The XLFM acquires spatial-angular information in a single camera exposure. In a subsequent step, a 3D volume can be algorithmically reconstructed, making it exceptionally well-suited for real-time 3D acquisition and potential analysis. Unfortunately, traditional reconstruction methods (like deconvolution) require lengthy processing times (0.0220 Hz), hampering the speed advantages of the XLFM. Neural network architectures can overcome the speed constraints at the expense of lacking certainty metrics, which renders them untrustworthy for the biomedical realm. This work proposes a novel architecture to perform fast 3D reconstructions of live immobilized zebrafish neural activity based on a 
    
[^82]: 错误反馈可以准确地压缩预处理器。

    Error Feedback Can Accurately Compress Preconditioners. (arXiv:2306.06098v1 [cs.LG])

    [http://arxiv.org/abs/2306.06098](http://arxiv.org/abs/2306.06098)

    本论文提出一种错误反馈技术，可以通过在馈入预处理器之前对梯度信息进行压缩（稀疏化或低秩压缩），将预处理器的存储成本压缩多达两个数量级，而不会丢失收敛性。

    

    利用深度网络规模的二阶信息是改进当前用于深度学习优化器性能的主要途径之一。然而，现有的精确全矩阵预处理方法，如全矩阵Adagrad（GGT）或无矩阵近似曲率（M-FAC），即使应用于中等规模模型，也会遇到巨大的存储成本问题，因为它们必须存储梯度的滑动窗口，其存储需求在模型维度中是成倍增加的。本文通过一种高效且易于实现的错误反馈技术来解决这个问题，该技术可以在实践中将预处理器压缩多达两个数量级，而不会丢失收敛性。具体而言，我们的方法在将梯度信息馈入预处理器之前通过稀疏化或低秩压缩压缩梯度信息，将压缩误差反馈到未来的迭代中。对深度神经网络进行了大量实验。

    Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks
    
[^83]: CARSO: 对抗性合成观测的反对抗性召回

    CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])

    [http://arxiv.org/abs/2306.06081](http://arxiv.org/abs/2306.06081)

    本文提出了一种新的图像分类的对抗性防御机制CARSO，该方法可以比最先进的对抗性训练更好地保护分类器，通过利用生成模型进行对抗净化来进行最终分类，并成功地保护自己免受未预见的威胁和最终攻击。

    

    本文提出了一种新的对抗性防御机制CARSO，用于图像分类，灵感来自认知神经科学的线索。该方法与对抗训练具有协同互补性，并依赖于被攻击分类器的内部表示的知识。通过利用生成模型进行对抗净化，该方法采样输入的重构来进行最终分类。在各种图像数据集和分类器体系结构上进行的实验评估表明，CARSO能够比最先进的对抗性训练更好地保护分类器——同时具有可接受的清洁准确度损失。此外，防御体系结构成功地保护自己免受未预见的威胁和最终攻击。代码和预训练模型可在https://github.com/获得。

    In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
    
[^84]: 可自我解释的时间序列预测与反事实解释

    Self-Interpretable Time Series Prediction with Counterfactual Explanations. (arXiv:2306.06024v1 [cs.LG])

    [http://arxiv.org/abs/2306.06024](http://arxiv.org/abs/2306.06024)

    本文提出了一种自我解释的时间序列预测模型CounTS，该模型可以生成反事实和可操作的解释，适用于关键领域如医疗和自动驾驶等。与现有方法不同，该模型为可解释性建模做出了贡献。

    

    可解释的时间序列预测对于像医疗和自动驾驶等安全关键领域至关重要。本文提出了一种不同于现有方法的思路，旨在开发出一种自我解释的模型，被称为Counterfactual Time Series（CounTS），该模型针对时间序列预测生成反事实和可操作的解释。具体而言，我们形式化了时间序列反事实解释的问题，建立了相应的评估协议，并提出了一种带有时间序列绑架、行动和预测反事实推理能力的变分贝叶斯深度学习模型。与最先进的基线相比，我们的自我解释模型可以生成更好的反事实解释，同时保持相当的预测准确性。

    Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.
    
[^85]: 跃迁于树空间：连续的树形系统推断方法用于有根和无根树

    Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees. (arXiv:2306.05739v1 [q-bio.PE])

    [http://arxiv.org/abs/2306.05739](http://arxiv.org/abs/2306.05739)

    本研究首次在连续空间中进行树形系统探索和推断，用于有根和无根树，优于当前最佳方法并在实验中证明了其效果，可用于加速生命科学的新进化发现。

    

    生物进化系统学现在是生命科学中的一个基础，可以阐明生命早期支系和传染病的起源和传播。然而，从可能的树的广阔空间中找到合适的系统树仍然具有挑战性。为了解决这个问题，我们首次在连续空间中进行了树形系统探索和推断，使梯度计算成为可能。这种连续的放松方式允许在有根和无根树中跨越树空间，且不易收敛到局部最小值。我们的方法优于当前最佳的无根树推断方法，并且在模拟中准确地推断出树和树根。该方法在实际数据中也很有效，我们在颌口动物的系统发育中证明了这一点。事实上，仅具有超指数信号的少数基因通常足以分辨脊椎动物的主要谱系。通过我们的方法，我们希望加速发现生命科学中的新进化发现。

    Phylogenetics is now fundamental in life sciences, providing insights into the earliest branches of life and the origins and spread of epidemics. However, finding suitable phylogenies from the vast space of possible trees remains challenging. To address this problem, for the first time, we perform both tree exploration and inference in a continuous space where the computation of gradients is possible. This continuous relaxation allows for major leaps across tree space in both rooted and unrooted trees, and is less susceptible to convergence to local minima. Our approach outperforms the current best methods for inference on unrooted trees and, in simulation, accurately infers the tree and root in ultrametric cases. The approach is effective in cases of empirical data with negligible amounts of data, which we demonstrate on the phylogeny of jawed vertebrates. Indeed, only a few genes with an ultrametric signal were generally sufficient for resolving the major lineages of vertebrate. With
    
[^86]: 使用CVXPY规定和解决鲁棒经验风险最小化问题

    Specifying and Solving Robust Empirical Risk Minimization Problems Using CVXPY. (arXiv:2306.05649v1 [math.OC])

    [http://arxiv.org/abs/2306.05649](http://arxiv.org/abs/2306.05649)

    本文介绍了如何使用CVXPY以用户友好的方式自动化鲁棒经验风险最小化问题的对偶化过程，使得用户可以方便地解决各种回归和分类问题。

    

    我们考虑鲁棒性经验风险最小化（ERM），其中模型参数被选为使得每个数据点在给定的凸不确定性集内变化时最小化最坏情况下的经验损失。在一些简单的情况下，这些问题可以表达为解析形式。一般情况下，可以通过对偶化使问题变得可行，这将一个min-max问题转换为一个min-min问题。对偶化需要专业知识，很烦琐也容易出现错误。我们展示了如何使用CVXPY以用户友好的方式自动化这个对偶化过程。我们的框架允许从一个一般的凸损失类中捕捉许多标准的回归和分类问题，并且用户可以轻松地指定任何可以用纪律化凸规划（DCP）约束表示的复杂不确定性集合。

    We consider robust empirical risk minimization (ERM), where model parameters are chosen to minimize the worst-case empirical loss when each data point varies over a given convex uncertainty set. In some simple cases, such problems can be expressed in an analytical form. In general the problem can be made tractable via dualization, which turns a min-max problem into a min-min problem. Dualization requires expertise and is tedious and error-prone. We demonstrate how CVXPY can be used to automate this dualization procedure in a user-friendly manner. Our framework allows practitioners to specify and solve robust ERM problems with a general class of convex losses, capturing many standard regression and classification problems. Users can easily specify any complex uncertainty set that is representable via disciplined convex programming (DCP) constraints.
    
[^87]: 基于岛屿的随机动态电压调节与ML增强型功率侧信道攻击

    Island-based Random Dynamic Voltage Scaling vs ML-Enhanced Power Side-Channel Attacks. (arXiv:2306.04859v1 [cs.CR])

    [http://arxiv.org/abs/2306.04859](http://arxiv.org/abs/2306.04859)

    本文介绍了一种基于岛屿的随机动态电压调节（iRDVS）方法，用于防范功率侧信道攻击，并通过实验验证了其有效性。

    

    本文描述和分析了一种基于岛屿的随机动态电压调节（iRDVS）方法，用于防范功率侧信道攻击。我们首先分析了独立电压岛的数量对信噪比和轨迹错位的影响。作为我们对错位的分析的一部分，我们提出了一种新颖的基于无监督机器学习（ML）的攻击，对于具有三个或更少独立电压的系统很有效。我们的结果表明，带有四个电压岛的iRDVS在200k加密跟踪下无法被破解，说明iRDVS可以有效。我们最后通过描述一个12纳米FinFet工艺下的iRDVS测试芯片来结束讲话，其中包括三个变体的AES-256加速器，所有这些加速器均源自同一RTL。这包括一个同步核心，一个没有保护的异步核心，以及一个使用异步逻辑采用iRDVS技术的核心。芯片的实验室测量表明，两个未受保护的变体都失败了。

    In this paper, we describe and analyze an island-based random dynamic voltage scaling (iRDVS) approach to thwart power side-channel attacks. We first analyze the impact of the number of independent voltage islands on the resulting signal-to-noise ratio and trace misalignment. As part of our analysis of misalignment, we propose a novel unsupervised machine learning (ML) based attack that is effective on systems with three or fewer independent voltages. Our results show that iRDVS with four voltage islands, however, cannot be broken with 200k encryption traces, suggesting that iRDVS can be effective. We finish the talk by describing an iRDVS test chip in a 12nm FinFet process that incorporates three variants of an AES-256 accelerator, all originating from the same RTL. This included a synchronous core, an asynchronous core with no protection, and a core employing the iRDVS technique using asynchronous logic. Lab measurements from the chips indicated that both unprotected variants failed 
    
[^88]: 基于强化学习的CrazyFlie 2.X四轴飞行器控制

    Reinforcement Learning-Based Control of CrazyFlie 2.X Quadrotor. (arXiv:2306.03951v1 [cs.RO])

    [http://arxiv.org/abs/2306.03951](http://arxiv.org/abs/2306.03951)

    该论文介绍了如何使用强化学习算法来实现CrazyFlie 2.X四轴飞行器的控制和导航，同时结合PID控制和灯塔定位系统，以实现更加精确的控制。

    

    本项目旨在探索经典控制算法（如PID）和现代强化学习算法之间的协同作用，以开发一种实用的控制机制来控制CrazyFlie 2.X四轴飞行器。主要目标是使用强化学习策略来执行PID调整，次要目标是利用第一个任务的经验，通过与灯塔定位系统集成实现导航控制。在导航方面考虑了两种方法：一种是使用预定义的有限运动基元进行深度Q学习的离散导航问题，另一种是使用深度强化学习进行连续导航。 RL训练的模拟将在gym-pybullet-drones上进行，该平台是一种基于强化学习的开源gym环境，并使用stable-baselines3提供RL实施。

    The objective of the project is to explore synergies between classical control algorithms such as PID and contemporary reinforcement learning algorithms to come up with a pragmatic control mechanism to control the CrazyFlie 2.X quadrotor. The primary objective would be performing PID tuning using reinforcement learning strategies. The secondary objective is to leverage the learnings from the first task to implement control for navigation by integrating with the lighthouse positioning system. Two approaches are considered for navigation, a discrete navigation problem using Deep Q-Learning with finite predefined motion primitives, and deep reinforcement learning for a continuous navigation approach. Simulations for RL training will be performed on gym-pybullet-drones, an open-source gym-based environment for reinforcement learning, and the RL implementations are provided by stable-baselines3
    
[^89]: 生成式人工智能应用调查

    A survey of Generative AI Applications. (arXiv:2306.02781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02781](http://arxiv.org/abs/2306.02781)

    本篇论文对350多个生成式人工智能应用进行了全面调查，总结了不同单模和多模生成式人工智能的应用。该调查为研究人员和从业者提供了宝贵的资源，帮助他们更好地了解生成式人工智能领域目前的最先进技术，并促进该领域的进一步创新。

    

    近年来，生成式人工智能有了显著增长，并在各个领域展示了广泛的应用。本文对350多个生成式人工智能应用进行了全面调查，提供了分类结构和对不同单模和多模生成式人工智能的简洁描述。该调查分成多个部分，覆盖了文本、图像、视频、游戏和脑信息等单模生成式人工智能的广泛应用。我们的调研旨在为研究人员和从业者提供宝贵的资源，帮助他们更好地了解生成式人工智能领域目前的最先进技术，并促进该领域的进一步创新。

    Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.
    
[^90]: 预训练的抽象模型和LLMs在法律案例判决摘要中的应用准备情况？

    How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])

    [http://arxiv.org/abs/2306.01248](http://arxiv.org/abs/2306.01248)

    这篇论文探讨了是否可以使用预训练的抽象模型和大型语言模型来自动生成法律案例判决的摘要，并在印度的法庭案例判决中进行了相关实验分析。

    

    自动摘要法律案例判决一直是采用抽取式摘要方法尝试解决的问题。然而，近年来，具有生成更自然和连贯摘要能力的抽象摘要模型受到越来越多的关注。现在已经有了专门用于法律领域的预训练抽象摘要模型。此外，众所周知，如ChatGPT这样的通用领域预训练大型语言模型(LLMs)能够生成高质量的文本，并具有文本摘要的能力。因此，值得问的是，这些模型是否已准备好用于自动生成案例判决的抽象摘要。为了探讨这个问题，我们将几种最先进的领域特定的抽象性摘要模型和通用领域的LLMs应用于印度法庭案例判决中，并检查所生成摘要的质量。除了摘要质量的标准度量，我们还检查了生成的摘要中可能存在的不一致性和虚构现象。

    Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the 
    
[^91]: Shuffle SGD总是比SGD更好：对具有任意数据顺序的SGD进行改进分析

    Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders. (arXiv:2305.19259v1 [cs.LG])

    [http://arxiv.org/abs/2305.19259](http://arxiv.org/abs/2305.19259)

    本论文研究了一种允许任意数据排序的普通SGD算法,并表明在非凸函数情况下，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。

    

    随机梯度下降（SGD）算法被广泛用于优化神经网络，随机重排（RR）和单次洗牌（SS）是通过循环遍历训练数据的随机或单个排列的常见选择，然而这些算法在非凸情况下的收敛性质尚未完全理解。现有结果表明，在实际的训练场景中，当时代的数量小于训练集大小时，RR可能表现不如SGD。本文分析了一种允许任意数据排序的普通SGD算法，并展示了在非凸函数情况下的改进收敛速度。具体而言，我们的分析表明，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。总的来说，我们的研究凸显了使用随机/单次洗牌的SGD的好处，并为其非凸收敛性质提供了新的见解。

    Stochastic Gradient Descent (SGD) algorithms are widely used in optimizing neural networks, with Random Reshuffling (RR) and Single Shuffle (SS) being popular choices for cycling through random or single permutations of the training data. However, the convergence properties of these algorithms in the non-convex case are not fully understood. Existing results suggest that, in realistic training scenarios where the number of epochs is smaller than the training set size, RR may perform worse than SGD.  In this paper, we analyze a general SGD algorithm that allows for arbitrary data orderings and show improved convergence rates for non-convex functions. Specifically, our analysis reveals that SGD with random and single shuffling is always faster or at least as good as classical SGD with replacement, regardless of the number of iterations. Overall, our study highlights the benefits of using SGD with random/single shuffling and provides new insights into its convergence properties for non-co
    
[^92]: 强凸优化的次梯度法的原始对偶理论

    Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])

    [http://arxiv.org/abs/2305.17323](http://arxiv.org/abs/2305.17323)

    本文提出了一种强凸优化的次梯度法原始对偶理论，可以实现简单的、最佳的停止准则和优化证明，同时可以适用于各种步长的选择和非Lipschitz病态问题，保证了这些方法次线性收敛速度。

    

    本文考虑强凸但潜在非光滑非Lipschitz优化的（随机）次梯度法。我们提供了新的等价对偶描述（类似于对偶平均）来描述经典的次梯度法，近端次梯度法和切换次梯度法。这些等价性能够以 $O(1/T)$ 的速度收敛，同时能够在强凸优化问题上分别还提供了经典原始间隙和前人未曾分析的对偶间隙保证。因此，我们的理论为这些经典方法提供了简单的、最佳的停止准则和优化证明，而不需要额外的计算成本。我们的结果适用于近乎所有的步长选择和一系列的非Lipschitz病态问题，对于在这些情况下，次梯度法的早期迭代可能会出现指数级的发散，而之前的研究没有处理过这种问题。即使在这种不良操作的情况下，我们的理论仍然确保和 bounds 了这些方法的次线性收敛速度。

    We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
    
[^93]: 用多模态语言模型生成图片

    Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])

    [http://arxiv.org/abs/2305.17216](http://arxiv.org/abs/2305.17216)

    该论文提出了一种方法，将大型语言模型与预训练的图像编码器和解码器模型进行融合，能生成具有连贯性的图像输出，同时也能进行图像检索和多模态对话。

    

    我们提出了一种方法，将仅包含文本的大型语言模型（LLMs）与预训练的图像编码器和解码器模型进行融合，通过映射它们的嵌入空间。我们的模型展示了广泛的多模态能力：图像检索、新颖图像生成和多模态对话。这是第一种能够在任意交错的图像和文本输入之间进行条件调节，生成连贯图像（和文本）输出的方法。为了在图像生成任务中取得强大的性能，我们提出了一种有效的映射网络，将LLM基于现成的文本到图像生成模型，将文本的隐藏表示转换为视觉模型的嵌入空间，利用LLM强大的文本表示来生成视觉输出。我们的方法在长且复杂语言的任务上优于基准生成模型。除了新颖图像生成之外，我们的模型还能够从文本描述中检索图像，并进行多模态对话。

    We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
    
[^94]: 缩放数据受限的语言模型

    Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16264](http://arxiv.org/abs/2305.16264)

    研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。

    

    现在扩展语言模型的趋势涉及增加参数计数和训练数据集大小。推断这个趋势表明，训练数据集大小可能很快就会受到互联网上可用文本数据的限制。出于此限制的动机，我们研究在数据受限制的情况下缩放语言模型。具体而言，我们运行了大量的实验，变化数据重复程度和计算预算，范围达到了9000亿个训练令牌和9亿参数模型。我们发现，在有限的数据的情况下，使用高达4次重复数据的训练与使用唯一数据相比对损失的贡献微不足道。然而，使用更多的重复数据，添加计算的价值最终会衰减为零。我们提出并经验证了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。最后，我们尝试了缓解数据稀缺的方法。

    The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
    
[^95]: OVO: 开放词汇占据

    OVO: Open-Vocabulary Occupancy. (arXiv:2305.16133v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16133](http://arxiv.org/abs/2305.16133)

    本文提出了开放词汇占据（OVO）方法，可以允许任意类别的语义占据预测，无需三维注释，其关键技术包括预训练二维开放词汇分割模型到三维占据网络的知识蒸馏和像素-体素过滤以生成高质量的训练数据。

    

    语义占据预测旨在推断出自主代理在三维环境中操作的密集几何和语义。现有的占据预测方法几乎完全是基于人工注释的体积数据进行训练的。虽然这些数据有高质量，但是生成这些三维注释是费力且成本高昂的，限制了它们仅可以训练少量特定物体类别的数据集。为了解决这一限制，本文提出了一种新颖的方法：开放词汇占据（OVO），允许任意类别的语义占据预测，但在训练过程中无需三维注释。我们方法的关键是：（1）从预训练的二维开放词汇分割模型到三维占据网络的知识蒸馏，（2）像素 - 体素过滤以生成高质量的训练数据。该框架简单、紧凑且兼容大多数最先进的语义占据预测模型。在 NYUv2 和 SemanticKIT 上的实验显示出该方法的有效性。

    Semantic occupancy prediction aims to infer dense geometry and semantics of surroundings for an autonomous agent to operate safely in the 3D environment. Existing occupancy prediction methods are almost entirely trained on human-annotated volumetric data. Although of high quality, the generation of such 3D annotations is laborious and costly, restricting them to a few specific object categories in the training dataset. To address this limitation, this paper proposes Open Vocabulary Occupancy (OVO), a novel approach that allows semantic occupancy prediction of arbitrary classes but without the need for 3D annotations during training. Keys to our approach are (1) knowledge distillation from a pre-trained 2D open-vocabulary segmentation model to the 3D occupancy network, and (2) pixel-voxel filtering for high-quality training data generation. The resulting framework is simple, compact, and compatible with most state-of-the-art semantic occupancy prediction models. On NYUv2 and SemanticKIT
    
[^96]: 利用化合物互连的多种挥发性有机化合物的超分辨率

    Multi-BVOC Super-Resolution Exploiting Compounds Inter-Connection. (arXiv:2305.14180v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.14180](http://arxiv.org/abs/2305.14180)

    本文提出了一种利用多种化合物贡献对粗糙BVOC排放地图进行超分辨的策略，实验结果表明该方法可以提高超分辨率性能。

    

    陆地生态系统排放到地球大气中的生物挥发性有机化合物（BVOC）是大气化学的重要组成部分。由于测量的稀缺性，可靠的BVOC排放地图可以提供更密集的数据，以供大气化学、气候和空气质量模型使用。在本研究中，我们提出了一种利用不同化合物贡献同时超分辨粗糙BVOC排放地图的策略。为此，我们首先准确地调查几种BVOC物种之间的空间相互作用。然后，我们利用发现的相似性建立了一个Multi-Image Super-Resolution（MISR）系统，其中与不同化合物相关联的多个排放地图被集成以提高超分辨率（SR）性能。我们比较了不同配置的物种和合并BVOC数量的方法。我们的实验结果表明，将BVOC关系纳入过程中可以提高SR性能。

    Biogenic Volatile Organic Compounds (BVOCs) emitted from the terrestrial ecosystem into the Earth's atmosphere are an important component of atmospheric chemistry. Due to the scarcity of measurement, a reliable enhancement of BVOCs emission maps can aid in providing denser data for atmospheric chemical, climate, and air quality models. In this work, we propose a strategy to super-resolve coarse BVOC emission maps by simultaneously exploiting the contributions of different compounds. To this purpose, we first accurately investigate the spatial inter-connections between several BVOC species. Then, we exploit the found similarities to build a Multi-Image Super-Resolution (MISR) system, in which a number of emission maps associated with diverse compounds are aggregated to boost Super-Resolution (SR) performance. We compare different configurations regarding the species and the number of joined BVOCs. Our experimental results show that incorporating BVOCs' relationship into the process can 
    
[^97]: 不要训练它：图神经网络的线性神经架构搜索

    Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14065](http://arxiv.org/abs/2305.14065)

    本文提出了一种新的图神经网络结构搜索方法——神经结构编码（NAC），它通过稀疏编码寻找最优结构参数，无需训练就能发挥表现力，在多个基准数据集上实现了最先进性能，并且运算速度比强基线方法快了200倍，精度提高了18.8％。

    

    图神经网络的神经架构搜索（NAS-GNN）已经显著地提高了手动设计的图神经网络的性能。然而，这些方法继承了传统NAS方法的问题，如高计算成本和优化难度。更重要的是，以前的NAS方法忽视了GNN的独特性，即GNN具有无需训练就具有表现力的特点。采用随机初始化的权重，我们可以通过稀疏编码目标寻找最优的架构参数，并得出一种新的NAS-GNN方法，即神经结构编码（NAC）。因此，我们的NAC在GNN上实现了无更新方案，可以在线性时间内高效计算。在多个GNN基准数据集上的实证评估表明，我们的方法导致了最先进的性能，比强基线方法快200倍，精度提高了18.8％。

    Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\times$ faster and $18.8\%$ more accurate than the strong baselines.
    
[^98]: 一种基于元学习和信道状态信息的可推广室内定位模型

    A Meta-learning based Generalizable Indoor Localization Model using Channel State Information. (arXiv:2305.13453v1 [cs.LG])

    [http://arxiv.org/abs/2305.13453](http://arxiv.org/abs/2305.13453)

    本文提出了一种基于元学习和信道状态信息的室内定位模型，以解决深度学习定位模型中持续存在的通用性缺失问题。

    

    近年来，室内定位因其在智能家居、工业自动化和医疗保健等领域的广泛应用而受到了重视，特别是随着越来越多的人依赖其无线设备进行基于位置的服务。基于深度学习的解决方案利用无线参数（如信道状态信息（CSI）和接收信号强度指示器（RSSI））在室内环境中准确估计无线设备的位置已经取得了很好的效果。然而，尽管深度学习模型在实现高准确度的定位方面取得了成功，但这些模型缺乏通用性，在不重新训练的情况下无法轻松部署到新环境或在动态环境中运行。本文提出了基于元学习的定位模型来解决传统深度学习定位模型中持续存在的通用性缺失问题。此外，由于元学习算法需要多元化数据。

    Indoor localization has gained significant attention in recent years due to its various applications in smart homes, industrial automation, and healthcare, especially since more people rely on their wireless devices for location-based services. Deep learning-based solutions have shown promising results in accurately estimating the position of wireless devices in indoor environments using wireless parameters such as Channel State Information (CSI) and Received Signal Strength Indicator (RSSI). However, despite the success of deep learning-based approaches in achieving high localization accuracy, these models suffer from a lack of generalizability and can not be readily-deployed to new environments or operate in dynamic environments without retraining. In this paper, we propose meta-learning-based localization models to address the lack of generalizability that persists in conventionally trained DL-based localization models. Furthermore, since meta-learning algorithms require diverse dat
    
[^99]: 借助深度强化学习的贫民窟道路规划

    Road Planning for Slums via Deep Reinforcement Learning. (arXiv:2305.13060v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.13060](http://arxiv.org/abs/2305.13060)

    本文介绍了一种基于深度强化学习的方法，用于自动布局贫民窟道路。通过掩码策略优化，可使可达性提高14.3％，对现有基线方法具有明显改进。

    

    数百万贫民窟居民由于贫民窟内不足的道路基础设施而遭受城市服务无法访问的困境，而贫民窟道路规划对城市的可持续发展至关重要。现有的重组或启发式方法要么耗时，不能推广到不同的贫民窟，要么在可达性和建设成本方面产生次优的道路规划。本文提出了一种基于深度强化学习的方法，用于自动布局贫民窟道路。我们提出了一个通用图模型，用于捕获贫民窟的拓扑结构，并设计了一种新颖的图神经网络，用于选择计划道路的位置。通过掩码策略优化，我们的模型可以生成连接贫民窟地点的道路规划，以最小的建设成本。对不同国家的真实贫民窟进行大量实验验证了我们模型的有效性，可使可达性提高14.3％，对现有基线方法具有明显改进。

    Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3% against existing baseline metho
    
[^100]: 图传播变换器用于图表示学习

    Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])

    [http://arxiv.org/abs/2305.11424](http://arxiv.org/abs/2305.11424)

    本文提出了一种新的变换器架构 GPTrans，以图传播注意力为基础，可以更好地学习图形模型，并在多个基准测试集上超过了其他最先进的基于变换器的图形模型。

    

    本文提出了一种用于图表示学习的新型变换器架构。我们的方法的核心见解是在构建变换器块中的注意力模块时，充分考虑图中节点和边之间的信息传播。具体而言，我们提出了一种新的注意力机制称为图传播注意力（GPA），它将信息在节点和边之间以三种方式明确传递，即从节点到节点，从节点到边和从边到节点，这对于学习图结构数据至关重要。在此基础上，我们设计了一种名为图传播变换器（GPTrans）的有效变换器架构，进一步帮助学习图数据。我们在几个基准数据集上的广泛图学习实验中验证了GPTrans的性能。这些结果表明，我们的方法以更好的性能超过了许多最先进的基于变换器的图形模型。代码将在https://github.com/czczup/GPTrans上发布。

    This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
    
[^101]: Meta-Polyp：高效息肉分割的基准线。

    Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v1 [eess.IV])

    [http://arxiv.org/abs/2305.07848](http://arxiv.org/abs/2305.07848)

    本研究提出Meta-Polyp，将Meta-Former与UNet融合并引入多尺度上采样块和Convformer块，解决了CNN和Vision Transformer在处理分布外数据集、缺失边界和小息肉时遇到的困难，提高了息肉分割的效率。

    

    近年来，息肉分割变得越来越重要，并且许多方法利用CNN、Vision Transformer和Transformer技术开发以实现竞争性结果。然而，这些方法在处理分布外数据集、缺失边界和小息肉时经常遇到困难。在2022年，Meta-Former作为一种新的视觉基准线被引入，它不仅提高了多任务计算机视觉的性能，而且解决了Vision Transformer和CNN家族骨架的局限性。为了进一步增强分割，我们提出了Meta-Former与UNet的融合，同时在解码器阶段引入了多尺度上采样块与级联组合，以增强纹理；此外，我们提出了Convformer块，基于Meta-Former的思想，以加强局部特征的关键信息。这些块将全局信息（例如息肉的整体形状）与局部信息相结合，提高了进行息肉分割的效率。

    In recent years, polyp segmentation has gained significant importance, and many methods have been developed using CNN, Vision Transformer, and Transformer techniques to achieve competitive results. However, these methods often face difficulties when dealing with out-of-distribution datasets, missing boundaries, and small polyps. In 2022, Meta-Former was introduced as a new baseline for vision, which not only improved the performance of multi-task computer vision but also addressed the limitations of the Vision Transformer and CNN family backbones. To further enhance segmentation, we propose a fusion of Meta-Former with UNet, along with the introduction of a Multi-scale Upsampling block with a level-up combination in the decoder stage to enhance the texture, also we propose the Convformer block base on the idea of the Meta-former to enhance the crucial information of the local feature. These blocks enable the combination of global information, such as the overall shape of the polyp, wit
    
[^102]: 面向时间序列预测的谱时图神经网络的表达能力研究

    How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])

    [http://arxiv.org/abs/2305.06587](http://arxiv.org/abs/2305.06587)

    该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。

    

    谱时图神经网络是大多数基于图神经网络(GNN)的时间序列预测模型的一个有前途的抽象。然而，我们需要更多关于这种方法的基础知识。本文建立了一个理论框架，揭示了谱时GNN的表现力。我们的结果表明，具有线性谱时GNN是普适的，在温和的假设下，它们的表现力受到我们的离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。为了使我们的发现在实践中有用，我们详细讨论了相关限制，并概述了在谱域中设计空间和时间模块的理论蓝图。基于这些见解，并为了展示基于我们的框架，谱时GNN有多么强大，我们提出了一个名为 Temporal Graph GegenConv (TGC) 的简单实例，显著优于大多数已有的模型。

    Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
    
[^103]: InstructBLIP: 通过指令调整实现通用视觉语言模型

    InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. (arXiv:2305.06500v1 [cs.CV])

    [http://arxiv.org/abs/2305.06500](http://arxiv.org/abs/2305.06500)

    本文对视觉语言指令调整进行了系统全面的研究，引入了指令感知的视觉特征提取这种关键的方法，使模型能够提取适合于给定指令的信息特征。

    

    驱动了预训练和指令调整流程的通用语言模型已经出现，可以解决各种语言领域的任务。然而，由于增加了额外的视觉输入，建立通用视觉语言模型仍然具有挑战性。尽管视觉语言预训练已经广泛研究，但视觉语言指令调整仍然相对较少探讨。在本文中，我们基于预训练的BLIP-2模型对视觉语言指令调整进行了系统全面的研究。我们收集了26个公开可用的数据集，并将它们转换为指令调整格式并分类为两个集群，用于保持指令调整和保持零-shot评估。此外，我们引入了指令感知的视觉特征提取，这是一种关键的方法，使模型能够提取适合于给定指令的信息特征。结果，InstructBLIP模型实现

    General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction. The resulting InstructBLIP models a
    
[^104]: 基于双重注意机制的眼底血管图像分割

    Segmentation of fundus vascular images based on a dual-attention mechanism. (arXiv:2305.03617v1 [eess.IV])

    [http://arxiv.org/abs/2305.03617](http://arxiv.org/abs/2305.03617)

    本文提出了基于双重注意机制的眼底血管图像分割方法，可以从空间和通道维度提取图像信息，并通过引入空间注意机制和Dropout层解决光照变化和不均匀对比度等问题，实验结果表明，该方法可以产生令人满意的结果。

    

    精确地分割视网膜眼底图像中的血管对于早期筛查、诊断和评估某些眼部疾病至关重要。然而，这些图像中存在明显的光照变化和不均匀对比度，这使得分割变得非常具挑战性。因此，本文采用了一种注意融合机制，该机制结合了Transformer构建的通道注意和空间注意机制，从空间和通道维度提取视网膜眼底图像的信息。为了消除编码器图像中的噪声，引入了一个空间注意机制在跳跃连接中。此外，使用Dropout层随机舍弃一些神经元，以防止神经网络过度拟合并提高其泛化性能。在公共数据集DERIVE、STARE和CHASEDB1上进行了实验。结果显示，与一些最近的视网膜眼底图像分割算法相比，我们的方法产生了令人满意的结果。

    Accurately segmenting blood vessels in retinal fundus images is crucial in the early screening, diagnosing, and evaluating some ocular diseases. However, significant light variations and non-uniform contrast in these images make segmentation quite challenging. Thus, this paper employ an attention fusion mechanism that combines the channel attention and spatial attention mechanisms constructed by Transformer to extract information from retinal fundus images in both spatial and channel dimensions. To eliminate noise from the encoder image, a spatial attention mechanism is introduced in the skip connection. Moreover, a Dropout layer is employed to randomly discard some neurons, which can prevent overfitting of the neural network and improve its generalization performance. Experiments were conducted on publicly available datasets DERIVE, STARE, and CHASEDB1. The results demonstrate that our method produces satisfactory results compared to some recent retinal fundus image segmentation algor
    
[^105]: 多个深度网络的训练过程探索相同的低维流形

    The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold. (arXiv:2305.01604v1 [cs.LG])

    [http://arxiv.org/abs/2305.01604](http://arxiv.org/abs/2305.01604)

    本文展示了多个深度网络的训练过程探索相同的低维流形，这些网络包括不同体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化，并揭示了网络初始化位置、体系结构和大小对流形的影响。

    

    我们开发了信息几何技术来分析深度网络训练过程中预测轨迹。通过检查底层高维概率模型，我们揭示了训练过程探索的有效低维流形。具有各种体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化的网络在预测空间内位于同一流形上。我们研究了这种流形的细节，发现具有不同体系结构的网络遵循可区分的轨迹，但其他因素影响极小; 更大的网络沿着与较小的网络相似的流形训练，只是更快; 不同部分的初始化网络在相似的流形上向解决方案收敛。

    We develop information-geometric techniques to analyze the trajectories of the predictions of deep networks during training. By examining the underlying high-dimensional probabilistic models, we reveal that the training process explores an effectively low-dimensional manifold. Networks with a wide range of architectures, sizes, trained using different optimization methods, regularization techniques, data augmentation techniques, and weight initializations lie on the same manifold in the prediction space. We study the details of this manifold to find that networks with different architectures follow distinguishable trajectories but other factors have a minimal influence; larger networks train along a similar manifold as that of smaller networks, just faster; and networks initialized at very different parts of the prediction space converge to the solution along a similar manifold.
    
[^106]: 上下文多语种用户查询拼写检查器

    Contextual Multilingual Spellchecker for User Queries. (arXiv:2305.01082v1 [cs.CL])

    [http://arxiv.org/abs/2305.01082](http://arxiv.org/abs/2305.01082)

    本文提出了一个上下文多语种用户查询拼写检查器，它非常快速、可扩展，并根据特定产品的需求调整其词汇表和拼写输出，以满足用户的需求。

    

    拼写检查是最基本和广泛使用的搜索功能之一。纠正拼写错误的用户查询不仅增强了用户体验，而且用户也期望能够实现。然而，大多数广泛可用的拼写检查解决方案要么比最新的解决方案精度低，要么速度太慢，无法用于延迟是关键要求的搜索用例。此外，大多数最新的创新架构集中在英语上，并且没有以多语言方式进行培训，并且是针对较长文本的拼写纠正进行培训，这是与对用户查询的拼写纠正不同的范式，其中上下文很少(大多数查询只有1-2个单词)。最后，由于大多数企业有独特的词汇，例如产品名称，现成的拼写解决方案无法满足用户的需求。在这项工作中，我们构建了一个多语言拼写检查器，它非常快速和可扩展，并根据特定产品的需求调整其词汇表和拼写输出。

    Spellchecking is one of the most fundamental and widely used search features. Correcting incorrectly spelled user queries not only enhances the user experience but is expected by the user. However, most widely available spellchecking solutions are either lower accuracy than state-of-the-art solutions or too slow to be used for search use cases where latency is a key requirement. Furthermore, most innovative recent architectures focus on English and are not trained in a multilingual fashion and are trained for spell correction in longer text, which is a different paradigm from spell correction for user queries, where context is sparse (most queries are 1-2 words long). Finally, since most enterprises have unique vocabularies such as product names, off-the-shelf spelling solutions fall short of users' needs. In this work, we build a multilingual spellchecker that is extremely fast and scalable and that adapts its vocabulary and hence speller output based on a specific product's needs. Fu
    
[^107]: 类平衡扩散模型

    Class-Balancing Diffusion Models. (arXiv:2305.00562v1 [cs.CV])

    [http://arxiv.org/abs/2305.00562](http://arxiv.org/abs/2305.00562)

    这项工作探究了扩散模型在类别不平衡的数据上的表现，并提出了一种解决方案“类平衡扩散模型”通过使用分布调整正则化器进行训练。

    

    最近的研究表明，基于扩散的模型在生成高质量视觉数据同时保持更好的多样性方面有优势。但这种观察结果只适用于策划好的数据分布，即数据样本经过精心处理以在标签方面均匀分布。在实践中，长尾数据分布似乎更为普遍，而扩散模型在这种类别不平衡的数据上的表现还未知。在这项工作中，我们首先研究了这个问题，观察到当扩散模型在类别不平衡的数据集上训练时，多样性和保真度显著降低。尤其是在尾部类别，生成的样本严重缺乏多样性，我们观察到严重的模式崩溃问题。为了解决这个问题，我们假设数据分布不是类平衡的，提出了一种名为“类平衡扩散模型”的解决方案，使用分布调整正则化器进行训练。

    Diffusion-based models have shown the merits of generating high-quality visual data while preserving better diversity in recent studies. However, such observation is only justified with curated data distribution, where the data samples are nicely pre-processed to be uniformly distributed in terms of their labels. In practice, a long-tailed data distribution appears more common and how diffusion models perform on such class-imbalanced data remains unknown. In this work, we first investigate this problem and observe significant degradation in both diversity and fidelity when the diffusion model is trained on datasets with class-imbalanced distributions. Especially in tail classes, the generations largely lose diversity and we observe severe mode-collapse issues. To tackle this problem, we set from the hypothesis that the data distribution is not class-balanced, and propose Class-Balancing Diffusion Models (CBDM) that are trained with a distribution adjustment regularizer as a solution. E
    
[^108]: 正交解耦高斯过程的球形感应特征

    Spherical Inducing Features for Orthogonally-Decoupled Gaussian Processes. (arXiv:2304.14034v1 [cs.LG])

    [http://arxiv.org/abs/2304.14034](http://arxiv.org/abs/2304.14034)

    本文研究了解耦高斯过程的正交分解问题，提出了一种扩展方法，即引入球形跨域特征，构建更灵活的数据依赖基函数来缓解限制，并展示了其有效性。

    

    尽管高斯过程（GPs）具有许多优点，但它们缺乏学习表征的能力，因此经常与深度神经网络（NNs）进行比较。最近的工作通过在诱导变量与前馈NN的隐藏单元之间建立联系的跨域变分GPs来弥合 GPs和深度NN之间的差距。本文在研究此方法与实际应用中的一些实际问题，并提出一种扩展方法，利用GPs的正交分解来减轻这些限制。具体地，我们引入球形跨域特征，构建更灵活的数据依赖基函数，用于GP逼近的主要和正交分量，结果表明在此框架下加入NN激活特征，不仅可以缓解这些问题，而且比其他策略更具有可扩展性。在多个基准数据集上的实验表明了我们方法的有效性。

    Despite their many desirable properties, Gaussian processes (GPs) are often compared unfavorably to deep neural networks (NNs) for lacking the ability to learn representations. Recent efforts to bridge the gap between GPs and deep NNs have yielded a new class of inter-domain variational GPs in which the inducing variables correspond to hidden units of a feedforward NN. In this work, we examine some practical issues associated with this approach and propose an extension that leverages the orthogonal decomposition of GPs to mitigate these limitations. In particular, we introduce spherical inter-domain features to construct more flexible data-dependent basis functions for both the principal and orthogonal components of the GP approximation and show that incorporating NN activation features under this framework not only alleviates these shortcomings but is more scalable than alternative strategies. Experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.
    
[^109]: 通过多样性权重实现生成模型的模式平衡

    Towards Mode Balancing of Generative Models via Diversity Weights. (arXiv:2304.11961v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11961](http://arxiv.org/abs/2304.11961)

    本研究提出了通过平衡训练数据集中的模式来增加模型输出多样性的多样性权重训练方案，以更好地适应需要多样化输出的创意应用，并在受控环境中进行的初步实验展示了其潜力。

    

    大型数据驱动的图像模型被广泛用于支持创意和艺术作品。在当前主导的分布拟合范式下，数据集被视为要尽可能接近的真实值。然而，许多创意应用需要多样化的输出，创作者经常努力从给定的数据分布中积极分离出来。我们认为，从纯模式覆盖转向模式平衡的建模目标调整是必要的，以适应更高的输出多样性目标。我们提出了多样性权重，这是一种通过平衡训练数据集中的模式来增加模型输出多样性的训练方案。在受控环境中进行的初步实验展示了我们方法的潜力。我们讨论了我们方法与多样性、公平和包容在生成式机器学习以及计算机创意中的联系。我们的算法实现可以在https://github.com/找到。

    Large data-driven image models are extensively used to support creative and artistic work. Under the currently predominant distribution-fitting paradigm, a dataset is treated as ground truth to be approximated as closely as possible. Yet, many creative applications demand a diverse range of output, and creators often strive to actively diverge from a given data distribution. We argue that an adjustment of modelling objectives, from pure mode coverage towards mode balancing, is necessary to accommodate the goal of higher output diversity. We present diversity weights, a training scheme that increases a model's output diversity by balancing the modes in the training dataset. First experiments in a controlled setting demonstrate the potential of our method. We discuss connections of our approach to diversity, equity, and inclusion in generative machine learning more generally, and computational creativity specifically. An implementation of our algorithm is available at https://github.com/
    
[^110]: 基于局部能量分布的随机模拟退火超参数确定

    Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing. (arXiv:2304.11839v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11839](http://arxiv.org/abs/2304.11839)

    本文提出了一种基于局部能量分布的随机模拟退火超参数确定方法，该方法通过中心极限定理估计局部能量的分布，将超参数搜索的时间复杂度从O(n^3)降低到O(1)，在解决最大割问题中的实验中表现良好。

    

    本文提出了一种基于局部能量分布的随机模拟退火（SSA）超参数确定方法。 SSA能够比典型的模拟退火（SA）更快地解决组合优化问题，但需要耗费时间进行超参数搜索。所提出的方法基于自旋（概率比特）的局部能量分布来确定超参数。自旋是SSA的基本计算元素，并通过权重与其他自旋进行图形连接。局部能量的分布可以基于中心极限定理（CLT）进行估计。基于CLT的正态分布用于确定超参数，其将超参数搜索的时间复杂度从传统方法的O(n^3)降低到O(1)。使用确定的超参数评估了SSA在Gset和K2000基准上的性能，用于最大割问题。结果表明，所提出的方法实现了平均割值的近似值。

    This paper presents a local energy distribution based hyperparameter determination for stochastic simulated annealing (SSA). SSA is capable of solving combinatorial optimization problems faster than typical simulated annealing (SA), but requires a time-consuming hyperparameter search. The proposed method determines hyperparameters based on the local energy distributions of spins (probabilistic bits). The spin is a basic computing element of SSA and is graphically connected to other spins with its weights. The distribution of the local energy can be estimated based on the central limit theorem (CLT). The CLT-based normal distribution is used to determine the hyperparameters, which reduces the time complexity for hyperparameter search from O(n^3) of the conventional method to O(1). The performance of SSA with the determined hyperparameters is evaluated on the Gset and K2000 benchmarks for maximum-cut problems. The results show that the proposed method achieves mean cut values of approxim
    
[^111]: B-Learner：隐藏混淆下异质因果效应的准神谕界限

    B-Learner: Quasi-Oracle Bounds on Heterogeneous Causal Effects Under Hidden Confounding. (arXiv:2304.10577v1 [cs.LG])

    [http://arxiv.org/abs/2304.10577](http://arxiv.org/abs/2304.10577)

    本文提出了一种元学习器 B-Learner，它可以在限制隐藏混淆水平的情况下高效地学习 CATE 函数的尖锐界限。

    

    从观察数据中估计异质治疗效应是许多领域中的重要任务，有助于政策和决策者做出更好的行动。近年来，在估计条件平均治疗效应（CATE）函数方面取得了鲁棒且高效的方法，但这些方法通常未考虑隐藏混淆的风险，这可能会对基于观察数据的任何因果估计造成任意和不知情的偏差。我们提出了一种名为B-Learner的元学习器，它可以在限制隐藏混淆水平的情况下高效地学习CATE函数的尖锐界限。我们通过将最近针对平均治疗效应的尖锐且有效边界结果（Dorn等人，2021）调整为Kallus＆Oprescu（2022）所提供的稳健和模型无关的分布式治疗效应学习框架，派生出B-Learner。B-Learner可以使用任何函数估计器，例如随机森林和深度神经网络，我们证明了它的。

    Estimating heterogeneous treatment effects from observational data is a crucial task across many fields, helping policy and decision-makers take better actions. There has been recent progress on robust and efficient methods for estimating the conditional average treatment effect (CATE) function, but these methods often do not take into account the risk of hidden confounding, which could arbitrarily and unknowingly bias any causal estimate based on observational data. We propose a meta-learner called the B-Learner, which can efficiently learn sharp bounds on the CATE function under limits on the level of hidden confounding. We derive the B-Learner by adapting recent results for sharp and valid bounds of the average treatment effect (Dorn et al., 2021) into the framework given by Kallus & Oprescu (2022) for robust and model-agnostic learning of distributional treatment effects. The B-Learner can use any function estimator such as random forests and deep neural networks, and we prove its 
    
[^112]: 基于基础模型的工具学习

    Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08354](http://arxiv.org/abs/2304.08354)

    基于基础模型的工具学习结合了专用工具和基础模型的优势，实现了问题解决的增强精度、效率和自动化。本文对工具学习进行了系统研究，提出了涵盖两种类型学习的通用工具学习框架，并分析了它们的独特挑战、机会和未来方向。

    

    人类拥有非凡的创造和利用工具的能力，使得他们能够克服物理限制并探索新的领域。随着基础模型的出现，AI系统有望像人类一样熟练地使用工具。这种范式即基于基础模型的工具学习，结合了专用工具和基础模型的优势，实现了问题解决的增强精度、效率和自动化。尽管具有巨大潜力，但该领域仍缺乏对关键挑战、机会和未来发展的全面理解。针对这一问题，本文对工具学习进行了系统研究。首先介绍了工具学习的背景，包括其认知起源、基础模型的范式转换和工具和模型的互补作用。然后，我们回顾了现有的工具学习研究，包括基于工具和面向工具的学习。我们制定了一个涵盖两种类型学习的通用工具学习框架，并分析了它们的独特挑战、机会和未来方向。我们预计这种系统的探索将为未来开发具有复杂工具学习能力的AI系统提供一个跳板。

    Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool l
    
[^113]: 对“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”的评论（arXiv: 2304.08297v2 [eess.IV] UPDATED）

    Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'. (arXiv:2304.08297v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.08297](http://arxiv.org/abs/2304.08297)

    对陈等人发表在《自然—生物医学工程》杂志上的“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”一文的评论和关切。

    

    最近，陈等人（Chen2022）在《自然—生物医学工程》杂志上发表了题为“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”的文章。该文章作者称其方法为“组织学自监督图像搜索，简称SISH。”我们对SISH表示了关切，因为它是Yottixel的增量修改，使用了MinMax二值化但未引用原始作品，并基于一个误称“自监督图像搜索”的概念。此外，我们还指出了陈等人进行实验和比较时存在的几个问题。

    Chen et al. [Chen2022] recently published the article 'Fast and scalable search of whole-slide images via self-supervised deep learning' in Nature Biomedical Engineering. The authors call their method 'self-supervised image search for histology', short SISH. We express our concerns that SISH is an incremental modification of Yottixel, has used MinMax binarization but does not cite the original works, and is based on a misnomer 'self-supervised image search'. As well, we point to several other concerns regarding experiments and comparisons performed by Chen et al.
    
[^114]: 中文LLaMA和Alpaca的高效有效的文本编码

    Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. (arXiv:2304.08177v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08177](http://arxiv.org/abs/2304.08177)

    这篇论文提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以改善LLaMA对中文的理解和生成能力。

    

    大型语言模型（LLM）已经彻底改变了自然语言处理研究，并显示出朝着人工通用智能（AGI）的有希望的进展。然而，训练和部署LLM的高成本对透明、可访问的学术研究构成了重大障碍。在这篇论文中，我们提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以便更好地理解和生成中文文本及其指令。

    Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, signifi
    
[^115]: 基于$\beta$-VAE的心电图任务特定特征提取联合优化

    Joint optimization of a $\beta$-VAE for ECG task-specific feature extraction. (arXiv:2304.06476v1 [eess.SP])

    [http://arxiv.org/abs/2304.06476](http://arxiv.org/abs/2304.06476)

    本文研究了使用$\beta$-VAE作为可解释特征提取器，并联合优化信号重建和心脏功能预测。在7255名患者的数据上进行测试，显示出相比先前方法，该方法显著改善了预测能力和可解释性。

    

    心电图是研究心脏情况的最常用方法，通过观察心脏节律和电活动进行诊断和监测。本文研究了使用$\beta$-变分自动编码器作为可解释的特征提取器，并通过联合优化信号重建和心脏功能预测来改进其预测能力。然后，使用逻辑回归对提取的特征进行心脏功能预测。该方法在7255名患者的数据上进行训练和测试，并显示出与先前方法相比，我们的方法显著改善了预测能力和可解释性。

    Electrocardiography is the most common method to investigate the condition of the heart through the observation of cardiac rhythm and electrical activity, for both diagnosis and monitoring purposes. Analysis of electrocardiograms (ECGs) is commonly performed through the investigation of specific patterns, which are visually recognizable by trained physicians and are known to reflect cardiac (dis)function. In this work we study the use of $\beta$-variational autoencoders (VAEs) as an explainable feature extractor, and improve on its predictive capacities by jointly optimizing signal reconstruction and cardiac function prediction. The extracted features are then used for cardiac function prediction using logistic regression. The method is trained and tested on data from 7255 patients, who were treated for acute coronary syndrome at the Leiden University Medical Center between 2010 and 2021. The results show that our method significantly improved prediction and explainability compared to 
    
[^116]: 使用Co-ML协同机器学习模型构建家庭的合作模型构建

    Collaborative Machine Learning Model Building with Families Using Co-ML. (arXiv:2304.05444v1 [cs.HC])

    [http://arxiv.org/abs/2304.05444](http://arxiv.org/abs/2304.05444)

    Co-ML是一个基于平板电脑的应用程序，用于协同构建ML图像分类器，可以帮助学习者在合作中发掘新的想法和方法，解决数据表现和多样性等关键问题。

    

    现有的针对新手友好的机器学习（ML）建模工具，侧重于单一用户体验，一个单一用户仅收集自己的数据来构建模型。然而，单独建模经历限制了学习者共同工作时会遇到的交替想法和方法的宝贵机会。因此，当不同的观点体现在群体构建的数据集中时，往往排除了ML围绕数据表现和多样性的关键问题。为解决这个问题，我们创建了Co-ML——一个面向学习者的基于平板电脑的应用程序，通过端对端的迭代模型构建流程，协同构建ML图像分类器。在本文中，我们通过介绍一个家庭（由两个11和14岁的孩子与父母一起工作）在家中使用Co-ML进行引导性介绍ML活动的深入案例研究，展示了协作建模的可行性和潜在丰富性。我们分享了Co-ML系统的d。

    Existing novice-friendly machine learning (ML) modeling tools center around a solo user experience, where a single user collects only their own data to build a model. However, solo modeling experiences limit valuable opportunities for encountering alternative ideas and approaches that can arise when learners work together; consequently, it often precludes encountering critical issues in ML around data representation and diversity that can surface when different perspectives are manifested in a group-constructed data set. To address this issue, we created Co-ML -- a tablet-based app for learners to collaboratively build ML image classifiers through an end-to-end, iterative model-building process. In this paper, we illustrate the feasibility and potential richness of collaborative modeling by presenting an in-depth case study of a family (two children 11 and 14-years-old working with their parents) using Co-ML in a facilitated introductory ML activity at home. We share the Co-ML system d
    
[^117]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^118]: ASPEST：主动学习和选择预测之间的弥合

    ASPEST: Bridging the Gap Between Active Learning and Selective Prediction. (arXiv:2304.03870v1 [cs.LG])

    [http://arxiv.org/abs/2304.03870](http://arxiv.org/abs/2304.03870)

    本文提出了一种新的学习范式——主动选择性预测（ASPEST），它可以在转移目标领域中学习查询更多有信息的样本，从而实现减少人工标注工作的同时增加准确性和覆盖率。

    

    选择性预测旨在学习一个可靠的模型，当模型不确定性很高时，可以避免进行预测。随后，可以将这些预测推迟给人类专家进行进一步评估。然而，在许多实际场景中，测试数据的分布与训练数据不同。这导致更不准确的预测，需要增加人工标注，这在许多场景中都是困难和昂贵的。主动学习通过仅查询最信息量丰富的示例来避免这种困难，并且在多个案例中已被证明可以降低总体的标注工作。在这项工作中，我们弥合了选择性预测和主动学习之间的差距，提出了一种新的学习范式，称为主动选择性预测（active selective prediction），它可以在增加准确性和覆盖率的同时在转移目标领域中学习查询更多有信息的样本。对于这个新问题，我们提出了一个简单但有效的解决方案ASPEST，它训练模型快照的集合。

    Selective prediction aims to learn a reliable model that abstains from making predictions when the model uncertainty is high. These predictions can then be deferred to a human expert for further evaluation. In many real-world scenarios, however, the distribution of test data is different from the training data. This results in more inaccurate predictions, necessitating increased human labeling, which is difficult and expensive in many scenarios. Active learning circumvents this difficulty by only querying the most informative examples and, in several cases, has been shown to lower the overall labeling effort. In this work, we bridge the gap between selective prediction and active learning, proposing a new learning paradigm called active selective prediction which learns to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new problem, we propose a simple but effective solution, ASPEST, that trains ensembles of model snapshots
    
[^119]: 重新审视针对深度神经网络的模型逆推攻击

    Re-thinking Model Inversion Attacks Against Deep Neural Networks. (arXiv:2304.01669v1 [cs.LG])

    [http://arxiv.org/abs/2304.01669](http://arxiv.org/abs/2304.01669)

    本文重新审视深度学习中的模型逆推攻击，提出了一种改进的优化目标和一个新型的“模型增强”思路，可以显著提高攻击性能。

    

    模型逆推（MI）攻击旨在通过滥用对模型的访问来推断和重构私有培训数据。MI攻击引起了有关泄露敏感信息（例如用于训练人脸识别系统的私人面部图像）的担忧。最近，已经提出了几种算法来改善MI的攻击表现。在这项工作中，我们重新审视MI，研究了所有最先进（SOTA） MI算法所涉及的两个基本问题，并提出了解决这些问题的解决方案，这些解决方案可以显著提高所有SOTA MI的攻击表现。特别是，我们的贡献有两个方面：1）我们分析了SOTA MI算法的优化目标，认为该目标对于实现MI是次优的，并提出了一种改进的优化目标，显著提高了攻击性能。2）我们分析了“MI过度拟合”，展示了它会阻止重构图像从学习培训数据的语义，提出了一种新型的“模型增强”思路。

    Model inversion (MI) attacks aim to infer and reconstruct private training data by abusing access to a model. MI attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for MI have been proposed to improve the attack performance. In this work, we revisit MI, study two fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all SOTA MI. In particular, our contributions are two-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue that the objective is sub-optimal for achieving MI, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze "MI overfitting", show that it would prevent reconstructed images from learning semantics of training data, and propose a novel "model augmentation" ide
    
[^120]: 使用神经时滞微分方程学习延迟

    Learning the Delay Using Neural Delay Differential Equations. (arXiv:2304.01329v1 [math.OC])

    [http://arxiv.org/abs/2304.01329](http://arxiv.org/abs/2304.01329)

    本文提出了一种基于时滞微分方程的连续时间神经网络方法，使用伴随灵敏度方法直接学习模型参数和时滞，具有学习DDE参数的能力。

    

    机器学习和动力系统的交叉点最近引起了人们的广泛兴趣。神经常微分方程（NODEs）代表了这些领域之间丰富的交叠。本文提出了一种基于时滞微分方程（DDEs）的连续时间神经网络方法。我们的模型使用伴随灵敏度方法从数据中直接学习模型参数和时滞。我们的方法受到NODEs的启发，并扩展了早期的神经DDE模型，后者假设时滞的值是已知的。我们对我们提出的方法进行了灵敏度分析，并展示了它从基准系统中学习DDE参数的能力。我们在讨论中得出结论，提出未来可能的方向和应用。

    The intersection of machine learning and dynamical systems has generated considerable interest recently. Neural Ordinary Differential Equations (NODEs) represent a rich overlap between these fields. In this paper, we develop a continuous time neural network approach based on Delay Differential Equations (DDEs). Our model uses the adjoint sensitivity method to learn the model parameters and delay directly from data. Our approach is inspired by that of NODEs and extends earlier neural DDE models, which have assumed that the value of the delay is known a priori. We perform a sensitivity analysis on our proposed approach and demonstrate its ability to learn DDE parameters from benchmark systems. We conclude our discussion with potential future directions and applications.
    
[^121]: 基于核凸包机的差分隐私学习研究

    Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])

    [http://arxiv.org/abs/2304.01300](http://arxiv.org/abs/2304.01300)

    本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。

    

    本文探讨了通过学习再生核希尔伯特空间中的点的凸包来表示数据的方法，旨在将数据空间划分为几何体，从而隐藏有关单个数据点的隐私信息，同时保留原始学习问题的结构。为此，我们引入了核凸包机（KAHM），它提供了一种有效的方法来计算从结果有界几何体中的距离度量。KAHM是广泛和深入的自编码器的关键构建块，它们使数据表示学习用于分类应用。为了确保隐私保护学习，我们提出了一种新颖的生成虚假数据的方法，该方法涉及将差分隐私数据样本通过转换过程进行平滑处理。生成的虚假数据不仅保证差分隐私，而且确保KAHM建模误差不大于原始数据误差。

    This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
    
[^122]: Lean 的机器学习前提选择工具

    Machine-Learned Premise Selection for Lean. (arXiv:2304.00994v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2304.00994](http://arxiv.org/abs/2304.00994)

    该论文介绍了一种基于机器学习的工具，可为 Lean 证明助手建议与用户正在证明的定理相关的前提条件。

    

    我们介绍了一种基于机器学习的工具，可为 Lean 证明助手建议与用户正在证明的定理相关的前提条件。该工具的设计原则为：（1）与证明助手紧密集成，（2）易于使用和安装，（3）采用轻量级且快速的方法。为此，我们设计了一个在线学习的自定义随机森林模型，直接在 Lean 中实现，这得益于 Lean 4 丰富而高效的元编程功能。随机森林的训练数据来自于 mathlib -- Lean 的数学库。我们尝试了各种选项来产生训练特征和标签。经过训练的模型的建议可以通过“suggest_premises策略”传达给用户，在交互式构建证明过程中可以在编辑器中调用该策略。

    We introduce a machine-learning-based tool for the Lean proof assistant that suggests relevant premises for theorems being proved by a user. The design principles for the tool are (1) tight integration with the proof assistant, (2) ease of use and installation, (3) a lightweight and fast approach. For this purpose, we designed a custom version of the random forest model, trained in an online fashion. It is implemented directly in Lean, which was possible thanks to the rich and efficient metaprogramming features of Lean 4. The random forest is trained on data extracted from mathlib -- Lean's mathematics library. We experiment with various options for producing training features and labels. The advice from a trained model is accessible to the user via the suggest_premises tactic which can be called in an editor while constructing a proof interactively.
    
[^123]: FP8和INT8在高效深度学习推理中的比较

    FP8 versus INT8 for efficient deep learning inference. (arXiv:2303.17951v1 [cs.LG])

    [http://arxiv.org/abs/2303.17951](http://arxiv.org/abs/2303.17951)

    本文比较了FP8和INT8在设备高效推理中的性能，展示了量化和量化感知训练的成果，为选择正确的数字格式提供了参考。

    

    最近，使用FP8作为神经网络训练的数字格式的想法在深度学习世界中流传。鉴于目前大部分训练都是使用完整网络的FP32或者有时使用混合精度的FP16进行的，部分网络使用8位重量级的FP8可以加快深度学习中通常耗时昂贵的训练过程。这引发了人们对于此发展对于边缘设备高效推理的影响的自然问题。在高效推理设备中，工作负载通常在INT8中执行。有时为了保证效率，甚至低至INT4。在本文中，我们比较了FP8和INT格式在设备高效推理中的性能。我们理论上展示了神经网络中INT和FP格式之间的差异，并提供了大量的训练后量化和训练时量化结果来展示如何在不同格式下实现高效推理。

    Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how 
    
[^124]: 带有概率触发臂的情境组合赌博机

    Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])

    [http://arxiv.org/abs/2303.17110](http://arxiv.org/abs/2303.17110)

    本文研究了带有概率触发臂的情境组合赌博机，在不同条件下设计了C$^2$-UCB-T算法和VAC$^2$-UCB算法，并分别导出了对应的遗憾值上限，为相关应用提供了理论支持。

    

    本研究探讨了在捕捉广泛应用范围的一系列平滑条件下的带有概率触发臂的情境组合赌博机(C$^2$MAB-T)，例如情境级联赌博机和情境最大化赌博机。在模拟触发概率(TPM)的条件下，我们设计了C$^2$-UCB-T算法，并提出了一种新的分析方法，实现了一个$\tilde{O}(d\sqrt{KT})$的遗憾值上限，消除了一个可能指数级增长的因子$O(1/p_{\min})$，其中$d$是情境的维数，$p_{\min}$是能被触发的任何臂的最小正概率，批大小$K$是每轮能被触发的臂的最大数量。在方差调制(VM)或触发概率和方差调制(TPVM)条件下，我们提出了一种新的方差自适应算法VAC$^2$-UCB，并导出了一个$\tilde{O}(d\sqrt{T})$的遗憾值上限，该上限与批大小$K$无关。作为一个有价值的副产品，我们发现我们的一个...

    We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
    
[^125]: LLaMA-Adapter: 零初始化注意力下的语言模型精细调整的高效方法

    LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])

    [http://arxiv.org/abs/2303.16199](http://arxiv.org/abs/2303.16199)

    本文提出了一种基于适应提示和零初始化注意力机制的轻量级语言模型调整方法，可高效微调LLaMA为指令跟随模型，具有比Alpaca更短的微调时间并具有近似的响应质量。

    

    本文提出了LLaMA-Adapter这一轻量级适应方法，用于将LLaMA高效地微调为一个指令跟随模型。利用52K个自我指导示范，LLaMA-Adapter仅在冻结的LLaMA 7B模型上引入了1.2M个可学习参数，并且在8个A100 GPU上仅耗时不到一个小时进行微调。具体而言，我们采用一组可学习的适应提示，并在较高的变压器层中将它们预置于输入文本令牌之前。然后，提出了一种零初始化注意力机制和零门控机制，该机制可以自适应地将新的指令提示注入LLaMA，并有效地保留了其预先训练的知识。通过高效训练，LLaMA-Adapter能够产生高质量的响应，与完全微调的7B参数的Alpaca相似。此外，我们的方法还可以简单地扩展到多模态输入，例如图像，用于图像相关的LLaMA，在ScienceQA上实现了更强的推理能力。我们在https://github.com/ZrrSkywalker/LLaMA-Adapt发布了我们的代码。

    We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
    
[^126]: MedNeXt：用于医学图像分割的变压器驱动卷积神经网络的可扩展性

    MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v1 [eess.IV])

    [http://arxiv.org/abs/2303.09975](http://arxiv.org/abs/2303.09975)

    MedNeXt是一个定制化的现代化可扩展卷积神经网络，用于解决数据稀缺的医学环境挑战。该网络包含：完全ConvNeXt 3D编码器-解码器网络、残差ConvNeXt上下采样块和一种新的迭代增加核大小的技术。

    

    近年来，在医学图像分割中使用基于 Transformer 的架构越来越多，但是由于缺乏大规模标注的医学数据集，使得其性能远不如自然图像。相比之下，卷积神经网络具有更高的归纳偏差，因此更容易训练到高性能水平。最近，ConvNeXt 架构尝试通过镜像变压器块来现代化标准卷积神经网络。在这项工作中，我们改进了这一架构，设计了一种现代化且可扩展的卷积神经网络，以应对数据稀缺的医学环境的挑战。我们引入 MedNeXt，这是一个受变压器启发的大核分割网络，其中包括：1）用于医学图像分割的完全 ConvNeXt 3D 编码器 - 解码器网络，2）残差 ConvNeXt 上下采样块，以在各个尺度上保留语义信息，3）一种新的技术，通过上采样小核来迭代增加核大小。

    There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel 
    
[^127]: 具有别名观测的潜在图的快速探索与学习

    Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])

    [http://arxiv.org/abs/2303.07397](http://arxiv.org/abs/2303.07397)

    本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。

    

    考虑这种场景：一个智能体通过执行操作从一个节点到另一个节点来导航潜在图。所选操作确定了下一个访问节点上的概率分布。在每个节点处，智能体收到一个观测，但该观测不是唯一的，因此它不能唯一地标识节点，这使得问题别名化。本文旨在提供一个政策，该政策约等于最大化探索效率（即在给定的探索预算下如何恢复图表）。在非别名化的情况下，我们展示了相对于现有最先进强化学习基线的改进性能。对于别名化的情况，我们不知道适用的基线，而是展示了在各种拓扑结构下相对于随机策略更快的恢复速度，并且对于具有挑战性的拓扑结构，恢复速度比随机策略快指数倍。我们将该算法称为 eFeX（来自于 efficient exploration 的缩写）。

    Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
    
[^128]: 在组合强化学习中限制最优值函数的界限

    Bounding the Optimal Value Function in Compositional Reinforcement Learning. (arXiv:2303.02557v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02557](http://arxiv.org/abs/2303.02557)

    本文在组合强化学习中提出了一种通用的框架，将感兴趣的组合任务的最优解与已知的原始任务解决方案相关联，并提出了双侧不等式将最优组合值函数与原始任务的值函数相关联。

    

    在强化学习领域，智能体通常需要解决一系列仅在奖励函数上有所不同的问题。为了快速获得适用于新奖励函数的未知问题的解决方案，一种流行的方法涉及到以前解决任务的功能组合。然而，以前使用这种功能组合的工作主要集中在组合函数的具体实例上，这些实例的极限假设允许进行精确的零-shot组合。我们的工作统一了这些示例，并为标准和熵正则化RL中的组合性提供了更一般的框架。我们发现，对于一类广泛的函数，感兴趣的组合任务的最优解可以与已知的原始任务解决方案相关联。具体而言，我们提出了双侧不等式，将最优组合值函数与原始任务的值函数相关联。我们还展示了使用零-shot策略的遗憾可以得到界限。

    In the field of reinforcement learning (RL), agents are often tasked with solving a variety of problems differing only in their reward functions. In order to quickly obtain solutions to unseen problems with new reward functions, a popular approach involves functional composition of previously solved tasks. However, previous work using such functional composition has primarily focused on specific instances of composition functions whose limiting assumptions allow for exact zero-shot composition. Our work unifies these examples and provides a more general framework for compositionality in both standard and entropy-regularized RL. We find that, for a broad class of functions, the optimal solution for the composite task of interest can be related to the known primitive task solutions. Specifically, we present double-sided inequalities relating the optimal composite value function to the value functions for the primitive tasks. We also show that the regret of using a zero-shot policy can be
    
[^129]: iSAGE：一种基于增量学习的SAGE在线解释方法

    iSAGE: An Incremental Version of SAGE for Online Explanation on Data Streams. (arXiv:2303.01181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01181](http://arxiv.org/abs/2303.01181)

    iSAGE是一种基于增量学习的SAGE在线解释方法，具备快速、内存高效的特点。该方法能够对模型变化以及数据生成过程中的漂移进行反应，同时提供了有效的特征移除方法，具有和SAGE类似的理论性质。

    

    现有的可解释人工智能（XAI）方法，包括SAGE等流行的特征重要性测量，大多限于批量学习场景。然而，机器学习通常应用于动态环境中，数据持续到达，必须以在线方式进行学习。因此，我们提出了iSAGE，一种快速、内存高效的SAGE增量方法，它能够对模型的变化以及数据生成过程中的漂移进行反应。我们提供了有效的特征移除方法，破坏（干预）和保留（观测）特征之间的依赖关系。此外，我们正式分析了我们的解释方法，展示了iSAGE与SAGE具有类似的理论性质。最后，我们基于广泛使用的数据集和具有概念漂移的数据流对我们的方法进行了彻底的实验分析。

    Existing methods for explainable artificial intelligence (XAI), including popular feature importance measures such as SAGE, are mostly restricted to the batch learning scenario. However, machine learning is often applied in dynamic environments, where data arrives continuously and learning must be done in an online manner. Therefore, we propose iSAGE, a time- and memory-efficient incrementalization of SAGE, which is able to react to changes in the model as well as to drift in the data-generating process. We further provide efficient feature removal methods that break (interventional) and retain (observational) feature dependencies. Moreover, we formally analyze our explanation method to show that iSAGE adheres to similar theoretical properties as SAGE. Finally, we evaluate our approach in a thorough experimental analysis based on well-established data sets and data streams with concept drift.
    
[^130]: GNOT: 一种用于运算符学习的通用神经运算符Transformer

    GNOT: A General Neural Operator Transformer for Operator Learning. (arXiv:2302.14376v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14376](http://arxiv.org/abs/2302.14376)

    提出了一种通用神经运算符Transformer——GNOT，用于解决机器学习中学习偏微分方程的解算子的问题，并通过设计新颖的异构归一化注意力层和引入几何门控机制来增强模型的灵活性和解决多尺度问题。在多个领域的具有挑战性的数据集上进行广泛实验，取得了显着的改进。

    

    在机器学习中，学习偏微分方程的解算子是一个重要的问题。然而，在实际应用中，学习算子存在一些挑战，例如不规则的网格、多个输入函数和解决PDE解的复杂性。为了解决这些挑战，我们提出了GNOT，一种可扩展且有效的基于Transformer的算子学习框架。通过设计新颖的异构归一化注意力层，我们的模型高度灵活，能够处理多个输入函数和不规则网格。此外，我们引入了一种几何门控机制，可以看作是软域分解以解决多尺度问题。Transformer体系结构的大模型容量使我们的模型能够应用于大型数据集和实际问题。我们在不同领域的多个具有挑战性的数据集上进行了广泛的实验，并取得了显着的改进。

    Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improv
    
[^131]: 基于重构的短距离FMCW雷达外分布检测方法

    Reconstruction-based Out-of-Distribution Detection for Short-Range FMCW Radar. (arXiv:2302.14192v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.14192](http://arxiv.org/abs/2302.14192)

    本篇论文提出了一种新颖的基于重构的OOD检测器，以在雷达领域进行操作，利用自动编码器及其潜在表示来检测OOD样本，在实验数据集上获得了90.72%的AUROC。

    

    最近，由于外分布检测（OOD）在现实应用中的关键作用，引起了人们的关注。OOD检测器旨在区分培训分布之外的样本，以避免机器学习模型对OOD数据的过度自信预测。现有的检测器主要依赖于logit、中间特征空间、softmax分数或重构损失，它们能够产生有希望的结果。但是，这些方法大多数是针对图像领域开发的。在这项研究中，我们提出了一种新颖的基于重构的OOD检测器，以在雷达领域进行操作。我们的方法利用自动编码器（AE）及其潜在表示来检测OOD样本。我们提出了两种得分，包括基于修补程序的重构损失和从每个修补程序的潜在表示计算得出的能量值。在使用60 GHz sh收集的数据集上，我们实现了90.72％的AUROC。

    Out-of-distribution (OOD) detection recently has drawn attention due to its critical role in the safe deployment of modern neural network architectures in real-world applications. The OOD detectors aim to distinguish samples that lie outside the training distribution in order to avoid the overconfident predictions of machine learning models on OOD data. Existing detectors, which mainly rely on the logit, intermediate feature space, softmax score, or reconstruction loss, manage to produce promising results. However, most of these methods are developed for the image domain. In this study, we propose a novel reconstruction-based OOD detector to operate on the radar domain. Our method exploits an autoencoder (AE) and its latent representation to detect the OOD samples. We propose two scores incorporating the patch-based reconstruction loss and the energy value calculated from the latent representations of each patch. We achieve an AUROC of 90.72% on our dataset collected by using 60 GHz sh
    
[^132]: 基于正则化动态规划的乐观规划方法

    Optimistic Planning by Regularized Dynamic Programming. (arXiv:2302.14004v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14004](http://arxiv.org/abs/2302.14004)

    本文提出了一种基于正则化动态规划的乐观规划方法，可用于学习折扣线性混合MDPs中的最优策略，且具有近乎最优的统计保证

    

    我们提出了一种新的无限时段折扣马尔可夫决策过程中乐观规划的方法，基于在近似值迭代过程的更新中添加正则化的思想。此技术使我们能够避免萎缩和单调性论证，这通常是现有近似动态规划方法分析所要求的，特别是可以在具有线性函数逼近的MDPs中使用通过最小二乘法估计的近似转移函数。我们使用该方法恢复了表格MDPs中已知的保证，并提供了一种从单个流经验中学习折扣线性混合MDPs中接近最优策略的计算有效算法，并证明它实现了近乎最优的统计保证。

    We propose a new method for optimistic planning in infinite-horizon discounted Markov decision processes based on the idea of adding regularization to the updates of an otherwise standard approximate value iteration procedure. This technique allows us to avoid contraction and monotonicity arguments typically required by existing analyses of approximate dynamic programming methods, and in particular to use approximate transition functions estimated via least-squares procedures in MDPs with linear function approximation. We use our method to recover known guarantees in tabular MDPs and to provide a computationally efficient algorithm for learning near-optimal policies in discounted linear mixture MDPs from a single stream of experience, and show it achieves near-optimal statistical guarantees.
    
[^133]: DeAR：细粒度All-Reduce管道加速分布式深度学习

    DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining. (arXiv:2302.12445v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12445](http://arxiv.org/abs/2302.12445)

    DeAR提出了一种新的调度算法，将All-Reduce原语分解成两个连续操作，与反向传播和前向计算同时重叠。使用实用的张量融合算法可以提高训练性能，实验结果表明DeAR在训练速度上可达到83%和15%的加速。

    

    通信调度在加速分布式训练中已被证明是有效的，它使得All-Reduce通信与反向传播计算重叠。这在流行的分布式深度学习框架中已被广泛采用。然而，存在两个基本问题：（1）针对每个All-Reduce操作，启动延迟与工作节点数成正比；（2）由于下一次迭代中前向计算的依赖和同步要求，它仅能实现次优的训练性能。我们提出了一种新的调度算法DeAR，将All-Reduce原语分解成两个连续操作，它们与反向传播和前向计算同时重叠而无需额外通信。我们还设计了一个实用的张量融合算法来提高训练性能。使用五种流行的模型进行的实验结果表明，DeAR在训练速度上实现了多达83%和15%的加速。

    Communication scheduling has been shown to be effective in accelerating distributed training, which enables all-reduce communications to be overlapped with backpropagation computations. This has been commonly adopted in popular distributed deep learning frameworks. However, there exist two fundamental problems: (1) excessive startup latency proportional to the number of workers for each all-reduce operation; (2) it only achieves sub-optimal training performance due to the dependency and synchronization requirement of the feed-forward computation in the next iteration. We propose a novel scheduling algorithm, DeAR, that decouples the all-reduce primitive into two continuous operations, which overlaps with both backpropagation and feed-forward computations without extra communications. We further design a practical tensor fusion algorithm to improve the training performance. Experimental results with five popular models show that DeAR achieves up to 83% and 15% training speedup over the 
    
[^134]: 重访MADDPG中的Gumbel-Softmax

    Revisiting the Gumbel-Softmax in MADDPG. (arXiv:2302.11793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11793](http://arxiv.org/abs/2302.11793)

    本文探索了多种Gumbel-Softmax的替代方法，并将其应用于MADDPG中，以解决离散动作空间下的性能问题。

    

    MADDPG是一种适用于多智能体强化学习的算法，它将单智能体方法DDPG推广到多智能体场景中。重要的是，DDPG是一种针对连续动作空间设计的算法，在其中状态-动作价值函数的梯度存在。为了使该算法适用于离散动作空间，必须进行离散的梯度估计。对于MADDPG算法，使用了Gumbel-Softmax（GS）估算器--一种将离散分布松弛到类似连续分布的再参数化方法。然而，该方法具有统计偏差，最近的多智能体强化学习算法基准测试论文表明，这种偏差使得MADDPG在格子世界等离散动作空间下表现不佳。幸运的是，GS的许多替代方法存在，具有各种各样的性能。本文探讨了其中几种替代方法，并将它们整合到离散格子世界场景中的MADDPG中。然后对各种性能指标的相应影响进行了测量。

    MADDPG is an algorithm in multi-agent reinforcement learning (MARL) that extends the popular single-agent method, DDPG, to multi-agent scenarios. Importantly, DDPG is an algorithm designed for continuous action spaces, where the gradient of the state-action value function exists. For this algorithm to work in discrete action spaces, discrete gradient estimation must be performed. For MADDPG, the Gumbel-Softmax (GS) estimator is used -- a reparameterisation which relaxes a discrete distribution into a similar continuous one. This method, however, is statistically biased, and a recent MARL benchmarking paper suggests that this bias makes MADDPG perform poorly in grid-world situations, where the action space is discrete. Fortunately, many alternatives to the GS exist, boasting a wide range of properties. This paper explores several of these alternatives and integrates them into MADDPG for discrete grid-world scenarios. The corresponding impact on various performance metrics is then measur
    
[^135]: 条件变分自编码器学习流形维度

    Learning Manifold Dimensions with Conditional Variational Autoencoders. (arXiv:2302.11756v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11756](http://arxiv.org/abs/2302.11756)

    该论文证明全局最优变分自编码器(CVAE)可以学习正确的流形维度，同时提出了一种新方法可以共同学习流形维度和条件分布，以在多个数据集上实现更好的特征分离和样本质量。

    

    虽然变分自编码器（VAE）及其条件扩展（CVAE）在多个领域中能够实现最先进的结果，但它们的精确行为仍未完全理解，特别是在数据（如图像）在或接近低维流形上的情况下。我们证明了VAE全局最小值确实能够恢复正确的流形维度，并通过引入一种新方法来共同学习流形维度和条件分布，进一步扩展了这一结果到更一般的CVAEs。我们在各种数据集上证明了我们方法的有效性，包括MNIST和CelebA，实现了表现最好的视觉质量和特征分离。

    Although the variational autoencoder (VAE) and its conditional extension (CVAE) are capable of state-of-the-art results across multiple domains, their precise behavior is still not fully understood, particularly in the context of data (like images) that lie on or near a low-dimensional manifold. For example, while prior work has suggested that the globally optimal VAE solution can learn the correct manifold dimension, a necessary (but not sufficient) condition for producing samples from the true data distribution, this has never been rigorously proven. Moreover, it remains unclear how such considerations would change when various types of conditioning variables are introduced, or when the data support is extended to a union of manifolds (e.g., as is likely the case for MNIST digits and related). In this work, we address these points by first proving that VAE global minima are indeed capable of recovering the correct manifold dimension. We then extend this result to more general CVAEs, 
    
[^136]: MalProtect：针对机器学习恶意软件检测领域中的对抗查询攻击的状态防御

    MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection. (arXiv:2302.10739v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10739](http://arxiv.org/abs/2302.10739)

    本文提出了一种专门为恶意软件检测领域设计的状态防御技术MalProtect，它通过实现一种新颖的查询分类方法来检测查询攻击，实验结果证明其有效性。

    

    众所周知，机器学习模型容易受到对抗性查询攻击的影响。在这些攻击中，查询会被不断扰动，以期达到特定的分类目的，而且没有关于目标模型的任何了解，仅凭其输出。远程托管的机器学习分类模型和面向服务的机器学习平台的普遍存在意味着查询攻击对这些系统的安全构成了真正的威胁。为了解决这个问题，已经提出了状态防御机制，通过监视和分析系统接收到的查询序列来检测查询攻击并防止生成对抗性样本。近年来已经提出了几种状态防御机制。但是，这些机制仅依赖于相似性或超出分布检测方法，这些方法可能在其他领域有效。在恶意软件检测领域，生成对抗性样本的方法本质上有所不同，因此我们发现这种检测机制的有效性明显较低。因此，在本文中，我们提出了MalProtect，这是一种专为恶意软件检测领域设计的状态防御技术。MalProtect实现了一种新颖的查询分类方法，利用关于恶意和良性查询分布的知识来检测查询攻击。我们在实际数据集上的实验表明，MalProtect有效地检测到了查询攻击，并提高了基于机器学习的恶意软件检测系统对对抗性攻击的抵抗力。

    ML models are known to be vulnerable to adversarial query attacks. In these attacks, queries are iteratively perturbed towards a particular class without any knowledge of the target model besides its output. The prevalence of remotely-hosted ML classification models and Machine-Learning-as-a-Service platforms means that query attacks pose a real threat to the security of these systems. To deal with this, stateful defenses have been proposed to detect query attacks and prevent the generation of adversarial examples by monitoring and analyzing the sequence of queries received by the system. Several stateful defenses have been proposed in recent years. However, these defenses rely solely on similarity or out-of-distribution detection methods that may be effective in other domains. In the malware detection domain, the methods to generate adversarial examples are inherently different, and therefore we find that such detection mechanisms are significantly less effective. Hence, in this paper
    
[^137]: 将黑匣子分解为可解释模型的混合物：路线规划，解释，重复。

    Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10289](http://arxiv.org/abs/2302.10289)

    本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。

    

    机器学习模型设计要么从解释性模型开始，要么从黑盒开始并事后解释。黑盒模型灵活但难以解释，而解释性模型本质上是可解释的。然而，解释性模型需要广泛的机器学习知识，并且往往比它们的黑盒变体不够灵活和表现不佳。本文旨在模糊黑盒的事后解释和构建可解释模型之间的界限。我们从黑盒开始，迭代地Carve出一种混合解释模型（MoIE）和一个残余网络。每个可解释模型专门处理一个样本子集，并使用一阶逻辑(FOL)对其进行解释，从黑盒中提供基本推理概念。我们通过灵活的残差路由其余的样本。我们在残转网络上重复该方法，直到所有可解释模型解释所需比例的数据。我们进行了大量实验，结果表明我们的路线规划，解释和重复方法在各种数据集上优于目前几种黑匣子模型解释方法，并产生高度可解释的模型。

    ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
    
[^138]: 多元系统风险度量及基于深度学习算法的计算

    Multivariate Systemic Risk Measures and Computation by Deep Learning Algorithms. (arXiv:2302.10183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10183](http://arxiv.org/abs/2302.10183)

    本文提出了基于深度学习算法计算多元系统性缺口风险度量的方法，可以学习原始最优化程序、对偶表示的最优解以及公平风险分配。

    

    本文提出了基于深度学习算法计算通过多元效用函数定义的系统性缺口风险度量的方法。我们讨论了相关的理论方面，特别关注原始最优解的公平性及相应的风险分配。我们提供的算法允许学习原始最优化程序、对偶表示的最优解以及相应的公平风险分配。我们通过与基于配对指数效用函数的基准模型进行比较来测试我们的算法，该基准模型可提供显式公式。我们还展示了在无法提供显式公式的情况下的收敛证据。

    In this work we propose deep learning-based algorithms for the computation of systemic shortfall risk measures defined via multivariate utility functions. We discuss the key related theoretical aspects, with a particular focus on the fairness properties of primal optima and associated risk allocations. The algorithms we provide allow for learning primal optimizers, optima for the dual representation and corresponding fair risk allocations. We test our algorithms by comparison to a benchmark model, based on a paired exponential utility function, for which we can provide explicit formulas. We also show evidence of convergence in a case for which explicit formulas are not available.
    
[^139]: 改进推荐系统中多任务排名模型的训练稳定性

    Improving Training Stability for Multitask Ranking Models in Recommender Systems. (arXiv:2302.09178v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09178](http://arxiv.org/abs/2302.09178)

    本文研究了推荐系统中多任务排名模型训练的稳定性问题，提出了解决方案，这些方案旨在提高模型的可使用性，节省资源和促进模型的发展

    

    推荐系统在许多内容平台中扮演着重要角色。虽然大部分推荐研究致力于设计更好的模型以提高用户体验，但我们发现，针对这些模型训练稳定性的研究严重不足。随着推荐模型变得越来越大、越来越复杂，它们越容易发生训练不稳定性问题，例如损失发散，这会使模型无法使用、浪费大量资源并阻碍模型的发展。在本文中，我们分享了我们为提高 YouTube 推荐实际多任务排名模型的训练稳定性所学习到的发现和最佳实践。我们展示了一些导致训练不稳定的模型特性，并推断了原因。此外，基于我们对训练不稳定性点附近训练动态的观察，我们假设了现有解决方案失败的原因，并提出了一种新算法来缓解现有解决方案的局限

    Recommender systems play an important role in many content platforms. While most recommendation research is dedicated to designing better models to improve user experience, we found that research on stabilizing the training for such models is severely under-explored. As recommendation models become larger and more sophisticated, they are more susceptible to training instability issues, i.e., loss divergence, which can make the model unusable, waste significant resources and block model developments. In this paper, we share our findings and best practices we learned for improving the training stability of a real-world multitask ranking model for YouTube recommendations. We show some properties of the model that lead to unstable training and conjecture on the causes. Furthermore, based on our observations of training dynamics near the point of training instability, we hypothesize why existing solutions would fail, and propose a new algorithm to mitigate the limitations of existing soluti
    
[^140]: 基于像素的混合交通控制与协调方法

    Mixed Traffic Control and Coordination from Pixels. (arXiv:2302.09167v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2302.09167](http://arxiv.org/abs/2302.09167)

    本研究考虑利用图像观察作为替代方法来进行混合交通控制。

    

    交通拥堵是我们社会中一直存在的问题。传统的交通控制方法在缓解当前拥堵程度方面已经失效，因此研究人员开始探索通过机器人车辆进行交通控制的想法，考虑到不同级别自主性车辆的不断涌现。这引起了混合交通控制的出现，其中机器人车辆通过强化学习算法来调节人驾驶车辆。本研究考虑利用图像观察作为混合交通控制的替代方法：1）图像通过卫星图像、车内摄像系统和交通监控系统普遍存在；2）图像不需要更新现有道路基础设施，并且不需要向可能不愿意配合的人类驾驶员传递信息。

    Traffic congestion is a persistent problem in our society. Existing methods for traffic control have proven futile in alleviating current congestion levels leading researchers to explore ideas with robot vehicles given the increased emergence of vehicles with different levels of autonomy on our roads. This gives rise to mixed traffic control, where robot vehicles regulate human-driven vehicles through reinforcement learning (RL). However, most existing studies use precise observations that involve global information, such as environment outflow, and local information, i.e., vehicle positions and velocities. Obtaining this information requires updating existing road infrastructure with vast sensor environments and communication to potentially unwilling human drivers. We consider image observations as the alternative for mixed traffic control via RL: 1) images are ubiquitous through satellite imagery, in-car camera systems, and traffic monitoring systems; 2) images do not require a compl
    
[^141]: 用人类偏好预训练语言模型

    Pretraining Language Models with Human Preferences. (arXiv:2302.08582v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08582](http://arxiv.org/abs/2302.08582)

    本论文探索了用人类反馈替代传统互联网文本来预训练语言模型的方法，其中条件训练是最优和简单的方法，可将不良内容的生成速率降低一个数量级，同时保持语言模型在下游任务上的性能。

    

    语言模型（LMs）的预训练是为了模仿互联网文本，其中包括如果由LMs生成而违反人类偏好的内容:虚假信息，冒犯性评论，个人可识别信息，质量较低或有缺陷的代码等。在这里，我们探讨了预训练LMs的备选目标，以引导它们生成与人类偏好一致的文本。我们在三项任务中针对人类反馈对五个预训练目标进行基准测试，并研究它们如何影响预训练LMs的一致性和能力之间的权衡。我们发现在我们探索的方法中有一种帕累托最优且简单的方法：条件训练，或学习在奖励模型给出的人类偏好得分条件下的令牌分布。条件训练将不良内容的生成速率降低了一个数量级，无论是在没有提示的情况下生成还是在对抗选择的提示下生成，都如此。此外，条件训练保持了LMs的下游任务性能，表明它是预训练LMs生成与人类偏好一致的文本的可行方法。

    Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task per
    
[^142]: LightGCL: 简单而有效的用于推荐的图对比学习

    LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. (arXiv:2302.08191v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2302.08191](http://arxiv.org/abs/2302.08191)

    LightGCL是一种新的图对比学习方法，旨在解决现有方法中存在的不足。该模型采用奇异值分解进行对比增强，更好地保留了内在的语义结构，并提高了模型的通用性和鲁棒性。

    

    图神经网络是一种强大的基于图的推荐系统学习方法。最近，将对比学习与GNN结合在推荐系统中使用，在处理高度稀疏的数据方面采取数据增强方案，已经显示出超越其他方法的性能。尽管在其成功的基础上，现有的图对比学习方法大多要么在用户-物品交互图上执行随机扰动(例如节点/边扰动)，要么依赖于启发式的增强技术(例如用户聚类)来生成对比视图。本文认为，这些方法不能很好地保持内在的语义结构，并且很容易受到噪音扰动的影响。为此，本文提出了一种称为LightGCL的简单而有效的图对比学习范式，解决了对比学习模型因噪音而失去通用性和鲁棒性的问题。我们的模型仅使用奇异值分解进行对比增强，使其更好地保留了内在的语义结构。

    Graph neural network (GNN) is a powerful learning approach for graph-based recommender systems. Recently, GNNs integrated with contrastive learning have shown superior performance in recommendation with their data augmentation schemes, aiming at dealing with highly sparse data. Despite their success, most existing graph contrastive learning methods either perform stochastic augmentation (e.g., node/edge perturbation) on the user-item interaction graph, or rely on the heuristic-based augmentation techniques (e.g., user clustering) for generating contrastive views. We argue that these methods cannot well preserve the intrinsic semantic structures and are easily biased by the noise perturbation. In this paper, we propose a simple yet effective graph contrastive learning paradigm LightGCL that mitigates these issues impairing the generality and robustness of CL-based recommenders. Our model exclusively utilizes singular value decomposition for contrastive augmentation, which enables the un
    
[^143]: 自私行为下的劫匪社交学习

    Bandit Social Learning: Exploration under Myopic Behavior. (arXiv:2302.07425v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2302.07425](http://arxiv.org/abs/2302.07425)

    该论文研究了自私行为下的劫匪社交学习问题，发现存在一种探索激励权衡，即武器探索和社交探索之间的权衡，受到代理的短视行为的限制会加剧这种权衡，并导致遗憾率与代理数量成线性关系。

    

    我们研究了一种社交学习动态，其中代理按照简单的多臂劫匪协议共同行动。代理以顺序方式到达，选择武器并接收相关奖励。每个代理观察先前代理的完整历史记录（武器和奖励），不存在私有信号。尽管代理共同面临开发和利用的探索折衷，但每个代理人都是一见钟情的，无需考虑探索。我们允许一系列与（参数化）置信区间一致的自私行为，包括“无偏”行为和各种行为偏差。虽然这些行为的极端版本对应于众所周知的劫匪算法，但我们证明了更温和的版本会导致明显的探索失败，因此遗憾率与代理数量成线性关系。我们通过分析“温和乐观”的代理提供匹配的遗憾上界。因此，我们建立了两种类型的探索激励之间的基本权衡：武器探索是固有于劫匪问题的，只受当前代理的行动影响，而社交探索是由先前代理行为驱动的，因此有利于未来代理。由于代理的短视行为限制了社交探索，这种权衡被加剧。

    We study social learning dynamics where the agents collectively follow a simple multi-armed bandit protocol. Agents arrive sequentially, choose arms and receive associated rewards. Each agent observes the full history (arms and rewards) of the previous agents, and there are no private signals. While collectively the agents face exploration-exploitation tradeoff, each agent acts myopically, without regards to exploration. Motivating scenarios concern reviews and ratings on online platforms.  We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals, including the "unbiased" behavior as well as various behaviorial biases. While extreme versions of these behaviors correspond to well-known bandit algorithms, we prove that more moderate versions lead to stark exploration failures, and consequently to regret rates that are linear in the number of agents. We provide matching upper bounds on regret by analyzing "moderately optimistic" agents.  As a
    
[^144]: B-BACN：贝叶斯边界感知卷积网络，用于裂纹表征

    B-BACN: Bayesian Boundary-Aware Convolutional Network for Crack Characterization. (arXiv:2302.06827v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.06827](http://arxiv.org/abs/2302.06827)

    提出了一种贝叶斯边界感知卷积网络（B-BACN），用于生成精确可靠的裂纹边界检测，并同时量化认识不确定性和归属不确定性。

    

    精确地检测裂纹边界对于结构和材料的可靠度评估和风险管理至关重要，例如结构健康监测、诊断、预测和维护安排。由于各种随机因素，例如测量噪声、信号处理和模型简化，裂纹检测的不确定性量化具有挑战性。我们提出了一种基于机器学习的方法来同时量化认识不确定性和归属不确定性。我们引入了一种贝叶斯边界感知卷积网络（B-BACN），强调不确定性感知边界细化，以生成精确可靠的裂纹边界检测。所提出的方法采用了多任务学习方法，其中我们使用蒙特卡罗dropout来学习认识不确定性和高斯采样函数来预测每个样本的归属不确定性。此外，我们将边界细化损失加入到B-BACN中，以增强检测缺陷边界的确定性。

    Accurately detecting crack boundaries is crucial for reliability assessment and risk management of structures and materials, such as structural health monitoring, diagnostics, prognostics, and maintenance scheduling. Uncertainty quantification of crack detection is challenging due to various stochastic factors, such as measurement noises, signal processing, and model simplifications. A machine learning-based approach is proposed to quantify both epistemic and aleatoric uncertainties concurrently. We introduce a Bayesian Boundary-Aware Convolutional Network (B-BACN) that emphasizes uncertainty-aware boundary refinement to generate precise and reliable crack boundary detections. The proposed method employs a multi-task learning approach, where we use Monte Carlo Dropout to learn the epistemic uncertainty and a Gaussian sampling function to predict each sample's aleatoric uncertainty. Moreover, we include a boundary refinement loss to B-BACN to enhance the determination of defect boundari
    
[^145]: 通过学习可识别的潜在混淆因素消除推荐偏差

    Debiasing Recommendation by Learning Identifiable Latent Confounders. (arXiv:2302.05052v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05052](http://arxiv.org/abs/2302.05052)

    本研究提出了一种新方法 iDCF，通过学习可识别的混淆因素来消除推荐偏差，该方法在真实和合成数据集上的实验表明有效性和理论保证。

    

    推荐系统旨在预测用户对未被曝光的物品的反馈。混淆偏差是由于存在未测量的变量（例如，用户的社会经济状况）可能会影响用户的曝光和反馈。现有的方法要么对这些未测量变量做出不可行的假设，要么直接推断用户的潜在混淆因素。然而，它们无法保证识别出反事实的反馈，这可能导致预测带有偏见。在本文中，我们提出了一种新方法，即可识别的去混淆（iDCF），它利用一组代理变量（例如，观察到的用户特征）来解决上述的非识别问题。所提出的iDCF是一个通用的去混淆的推荐框架，它应用近端因果推断来推断未测量的混淆因素并识别反事实的反馈，具有理论保证。在各种真实世界和合成数据集上进行的广泛实验证明了我们所提出的方法在减少混淆偏差和提高推荐准确性方面的有效性。

    Recommendation systems aim to predict users' feedback on items not exposed to them.  Confounding bias arises due to the presence of unmeasured variables (e.g., the socio-economic status of a user) that can affect both a user's exposure and feedback. Existing methods either (1) make untenable assumptions about these unmeasured variables or (2) directly infer latent confounders from users' exposure. However, they cannot guarantee the identification of counterfactual feedback, which can lead to biased predictions. In this work, we propose a novel method, i.e., identifiable deconfounder (iDCF), which leverages a set of proxy variables (e.g., observed user features) to resolve the aforementioned non-identification issue. The proposed iDCF is a general deconfounded recommendation framework that applies proximal causal inference to infer the unmeasured confounders and identify the counterfactual feedback with theoretical guarantees. Extensive experiments on various real-world and synthetic da
    
[^146]: 探索网络上的模型无关联合学习

    Towards Model-Agnostic Federated Learning over Networks. (arXiv:2302.04363v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04363](http://arxiv.org/abs/2302.04363)

    该论文提出了一种适用于网络环境中多种数据和模型的模型无关关联学习方法，旨在通过网络结构反映本地数据集的相似性并保证本地模型产生一致的预测结果。

    

    我们提出了一种适用于异构数据和模型网络的模型无关联合学习方法。网络结构反映了本地数据集（统计数据）和它们相关的本地模型之间的相似性。我们的方法是经验风险最小化的一种实例，其中正则化项是从数据的网络结构导出的。特别地，我们要求良好连接的本地模型形成聚类，在一个公共测试集上产生相似的预测结果。所提出的方法允许使用各种各样的本地模型。 对这些本地模型唯一的限制是它们允许有效实现正则化的经验风险最小化（训练）。对于各种模型，这样的实现都可以在高级编程库（包括scikit-learn、Keras或PyTorch）中找到。

    We present a model-agnostic federated learning method for networks of heterogeneous data and models. The network structure reflects similarities between the (statistics of) local datasets and, in turn, their associated local("personal") models. Our method is an instance of empirical risk minimization, with the regularization term derived from the network structure of data. In particular, we require well-connected local models, forming clusters, to yield similar predictions on a common test set. The proposed method allows for a wide range of local models. The only restriction on these local models is that they allow for efficient implementation of regularized empirical risk minimization (training). For a wide range of models, such implementations are available in high-level programming libraries including scikit-learn, Keras or PyTorch.
    
[^147]: 将SO(3)卷积降维至SO(2)以实现高效等变GNN

    Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs. (arXiv:2302.03655v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03655](http://arxiv.org/abs/2302.03655)

    本文将SO(3)卷积降维至SO(2)，以减少等变卷积在高阶张量上的计算复杂度，并通过提出的等变球形通道网络（eSCN）在大规模OC-2数据集上获得最先进的结果。

    

    模拟点云或原子等3D数据的图神经网络通常需要是SO(3)等变的，即对3D旋转等变。然而，等变卷积（是等变网络的基本操作）随着更高阶张量的使用，在计算复杂度上显著增加。本文通过将SO(3)卷积或张量积降维至SO(2)，从而将节点嵌入的主轴与边向量对齐，从而稀疏化张量积并将计算复杂度从O(L^6)降至O(L^3)，其中L为表示的度。通过提出利用我们新的等变卷积方法实现等变球形通道网络（eSCN）的图神经网络且在大规模OC-2数据集上获得最先进的结果，我们展示了这一改进的潜在影响。

    Graph neural networks that model 3D data, such as point clouds or atoms, are typically desired to be $SO(3)$ equivariant, i.e., equivariant to 3D rotations. Unfortunately equivariant convolutions, which are a fundamental operation for equivariant networks, increase significantly in computational complexity as higher-order tensors are used. In this paper, we address this issue by reducing the $SO(3)$ convolutions or tensor products to mathematically equivalent convolutions in $SO(2)$ . This is accomplished by aligning the node embeddings' primary axis with the edge vectors, which sparsifies the tensor product and reduces the computational complexity from $O(L^6)$ to $O(L^3)$, where $L$ is the degree of the representation. We demonstrate the potential implications of this improvement by proposing the Equivariant Spherical Channel Network (eSCN), a graph neural network utilizing our novel approach to equivariant convolutions, which achieves state-of-the-art results on the large-scale OC-2
    
[^148]: 不要再过度使用黑匣子模型进行简单任务，转而使用透明模型。

    Stop overkilling simple tasks with black-box models and use transparent models instead. (arXiv:2302.02804v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02804](http://arxiv.org/abs/2302.02804)

    过度使用黑匣子模型会导致简单任务的浪费，透明模型可以提高效率和精度。

    

    近年来，深度学习方法在人工智能领域中取得了重大突破。与传统机器学习模型不同，基于深度学习的方法能够自主从原始数据中提取特征。这允许跳过通常被认为是容易出错和烦琐的特征工程过程。此外，深度学习策略在精度方面通常优于传统模型。

    In recent years, the employment of deep learning methods has led to several significant breakthroughs in artificial intelligence. Different from traditional machine learning models, deep learning-based approaches are able to extract features autonomously from raw data. This allows for bypassing the feature engineering process, which is generally considered to be both error-prone and tedious. Moreover, deep learning strategies often outperform traditional models in terms of accuracy.
    
[^149]: 源于超取样的信息论泛化界限更紧密

    Tighter Information-Theoretic Generalization Bounds from Supersamples. (arXiv:2302.02432v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02432](http://arxiv.org/abs/2302.02432)

    本文介绍了一种新颖的信息论泛化界限，利用投影损失对，与Rademacher序列相关联来源于超取样的设置，这些界限比同一超取样设置中迄今已知的所有信息理论界限都更紧密。

    

    本文介绍了针对学习算法的各种新颖的信息论泛化界限，源于Steinke＆Zakynthinou（2020）的超取样设置-“条件互信息”框架的设置。我们的开发利用将损失对（从训练实例和测试实例获得）投影到单个数字，并将损失值与Rademacher序列（及其移动变体）相关联。所呈现的界限包括平方根界限，快速率界限，包括基于方差和尖锐度的界限以及插值算法的界限等。我们理论上或经验上证明，这些界限比同一超取样设置中迄今已知的所有信息理论界限都更紧密。

    In this work, we present a variety of novel information-theoretic generalization bounds for learning algorithms, from the supersample setting of Steinke & Zakynthinou (2020)-the setting of the "conditional mutual information" framework. Our development exploits projecting the loss pair (obtained from a training instance and a testing instance) down to a single number and correlating loss values with a Rademacher sequence (and its shifted variants). The presented bounds include square-root bounds, fast-rate bounds, including those based on variance and sharpness, and bounds for interpolating algorithms etc. We show theoretically or empirically that these bounds are tighter than all information-theoretic bounds known to date on the same supersample setting.
    
[^150]: 大型语言模型可以预测人类在六个感官模态下的感知评判

    Large language models predict human sensory judgments across six modalities. (arXiv:2302.01308v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.01308](http://arxiv.org/abs/2302.01308)

    本研究表明，最先进的大型语言模型能预测人类在六个感官模态下的感知评判，并能提供从语言中提取感知信息的下限。

    

    确定从语言中可以恢复感知世界的程度是哲学和认知科学中长期存在的问题。本研究展示了，最先进的大型语言模型通过提供从语言中提取感知信息的下限，可以为解决这个问题提供新的见解。具体而言，我们从GPT模型中引出了六个心理物理数据集的成对相似度评估结果。我们发现这些评估结果在所有领域中均与人类数据显著相关，回复了众所周知的表现，如颜色环和音高螺旋。令人惊讶的是，我们发现一个在视觉和语言上共同训练的模型（GPT-4）并不一定会导致对视觉模态的特定改进。为了研究特定语言对感知的影响，我们还将这些模型应用于多语言颜色命名任务。我们发现，GPT-4在英语和俄语中复制了跨语言差异，阐明了它们之间的相互作用。

    Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality. To study the influence of specific languages on perception, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interacti
    
[^151]: Mnemosyne: 使用Transformers来训练Transformers

    Mnemosyne: Learning to Train Transformers with Transformers. (arXiv:2302.01128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01128](http://arxiv.org/abs/2302.01128)

    Mnemosyne优化器使用Performers方法来学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节，并成功训练ViTs和应用于机器人领域中，具有更好的泛化能力与快速收敛。

    

    训练复杂的机器学习(ML)架构需要耗费大量计算和时间来选择合适的优化器并调节其超参数。从数据中学习优化器的新学习范式已经成为手动设计ML优化器的更好选择。我们提出了Mnemosyne优化器，它使用Performers: 隐式低秩attention Transformers。它可以学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节。我们展示了Mnemosyne：(a)比流行的LSTM优化器具有更好的泛化能力；(b)特别地，可以在标准MLPs上进行元训练后成功地训练Vision Transformers(ViTs) (c)可以初始化优化器以实现机器人应用中更快的收敛。我们相信这些结果开启了使用Transformers构建基础优化模型的可能性，可以应对常规的Transformer训练挑战。我们通过广泛的理论分析来补充我们的结果。

    Training complex machine learning (ML) architectures requires a compute and time consuming process of selecting the right optimizer and tuning its hyper-parameters. A new paradigm of learning optimizers from data has emerged as a better alternative to hand-designed ML optimizers. We propose Mnemosyne optimizer, that uses Performers: implicit low-rank attention Transformers. It can learn to train entire neural network architectures including other Transformers without any task-specific optimizer tuning. We show that Mnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in particular can successfully train Vision Transformers (ViTs) while meta--trained on standard MLPs and (c) can initialize optimizers for faster convergence in Robotics applications. We believe that these results open the possibility of using Transformers to build foundational optimization models that can address the challenges of regular Transformer training. We complement our results with an extensive theo
    
[^152]: 将语言模型与图像进行联系以处理多模态信息

    Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13823](http://arxiv.org/abs/2301.13823)

    该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。

    

    我们提出了一种有效的方法，将预训练的仅文本语言模型与视觉领域联系起来，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的文本。我们利用从大规模文本预训练中学到的语言模型的能力，例如上下文学习和自由形式文本生成。我们保持语言模型冻结，并微调输入和输出线性层以实现跨模态交互。这使得我们的模型能够处理任意交错的图像和文本输入，并生成与检索图像交错的自由形式文本。我们在环境相关的图像检索和多模态对话等任务中取得了强大的零-shot表现，并展示了引人入胜的交互能力。我们的方法适用于任何现成的语言模型，为在视觉场景下利用预训练语言模型提供了一个有效且通用的解决方案。

    We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
    
[^153]: SOBER：离散和混合空间上高并行贝叶斯优化和贝叶斯积分

    SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces. (arXiv:2301.11832v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11832](http://arxiv.org/abs/2301.11832)

    SOBER算法是一种在离散和混合空间上进行高并行贝叶斯优化的方法，能够进行可扩展和多样化的批量全局优化和积分，且优于11个竞争基线方法。

    

    批处理贝叶斯优化和贝叶斯积分已被证明是在需并行查询昂贵的目标函数时执行优化和积分的高效方法。然而，当前的方法不适用于大批量操作。我们提出了一种新算法——SOBER，它允许在离散和混合空间上使用任意采集函数和内核进行可扩展和多样化的批量全局优化和积分。我们的方法的关键在于将全局优化的批量选择重新定义为积分问题，并将采集函数的最大化（非凸）松弛为内核重组（凸），从而有效地解决了两个任务。我们展示SOBER优于11个竞争基线方法。

    Batch Bayesian optimisation and Bayesian quadrature have been shown to be sample-efficient methods of performing optimisation and quadrature where expensive-to-evaluate objective functions can be queried in parallel. However, current methods do not scale to large batch sizes -- a frequent desideratum in practice (e.g. drug discovery or simulation-based inference). We present a novel algorithm, SOBER, which permits scalable and diversified batch global optimisation and quadrature with arbitrary acquisition functions and kernels over discrete and mixed spaces. The key to our approach is to reformulate batch selection for global optimisation as a quadrature problem, which relaxes acquisition function maximisation (non-convex) to kernel recombination (convex). Bridging global optimisation and quadrature can efficiently solve both tasks by balancing the merits of exploitative Bayesian optimisation and explorative Bayesian quadrature. We show that SOBER outperforms 11 competitive baselines o
    
[^154]: 基于扩散的结构化状态空间模型的条件心电信号生成

    Diffusion-based Conditional ECG Generation with Structured State Space Models. (arXiv:2301.08227v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2301.08227](http://arxiv.org/abs/2301.08227)

    本研究提出了将扩散模型和结构化状态空间模型相结合的新技术SSSD-ECG，在根据70多个心电图语句生成合成12导联心电图方面表现出色。

    

    合成数据生成是解决敏感健康数据分布时的隐私问题的一种有前途的解决方案。最近，扩散模型为不同的数据模式设定了新的生成模型标准。最近，结构化状态空间模型出现，成为捕捉时间序列中长期依赖关系的强大建模范例。我们提出了SSSD-ECG，将这两种技术相结合，用于根据70多个心电图语句生成合成12导联心电图的条件生成。由于没有可靠的基准，我们还提出了两种最先进的无条件生成模型的条件变体。我们通过评估在生成数据上预训练的分类器的性能来彻底评估所生成样本的质量，并评估了仅在合成数据上训练的分类器的性能，在这方面，SSSD-ECG明显优于其基于GAN的竞争对手。我们通过进一步的实验证明了我们方法的合理性。

    Synthetic data generation is a promising solution to address privacy issues with the distribution of sensitive health data. Recently, diffusion models have set new standards for generative models for different data modalities. Also very recently, structured state space models emerged as a powerful modeling paradigm to capture long-term dependencies in time series. We put forward SSSD-ECG, as the combination of these two technologies, for the generation of synthetic 12-lead electrocardiograms conditioned on more than 70 ECG statements. Due to a lack of reliable baselines, we also propose conditional variants of two state-of-the-art unconditional generative models. We thoroughly evaluate the quality of the generated samples, by evaluating pretrained classifiers on the generated data and by evaluating the performance of a classifier trained only on synthetic data, where SSSD-ECG clearly outperforms its GAN-based competitors. We demonstrate the soundness of our approach through further exp
    
[^155]: 手术聚合：一种用于协同学习的分布式医学影像数据和多样任务协调框架

    Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.06683](http://arxiv.org/abs/2301.06683)

    本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。

    

    大规模的胸部X光数据集已经通过深度学习进行异常检测，并有潜力为许多临床应用提供巨大的益处。然而，每个数据集仅专注于检测患者可能同时出现的一部分发现，从而限制了其临床效用。因此，数据协调对于聚合这些数据集来训练具有完整胸部内可能出现的所有异常的临床实用、强大模型至关重要。为此，我们提出了手术聚合，一种协同学习框架，用于协调和聚合分布式异构数据集的知识，并带有部分疾病注释。我们在合成的iid数据集和具有部分注释的真实大规模非iid数据集上评估了手术聚合。我们的结果表明，手术聚合显著优于当前的策略，具有更好的通用性。

    Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
    
[^156]: 一种AI模型解释性的理论框架及其在生物医学领域的应用

    A Theoretical Framework for AI Models Explainability with Application in Biomedicine. (arXiv:2212.14447v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.14447](http://arxiv.org/abs/2212.14447)

    该论文提出了一种新的解释定义和理论框架，用于评估AI模型的可解释性，特别是在生物医学领域。这个框架可帮助确定模型决策中最具影响力的输入特征，支持对模型行为的解释。

    

    可解释性人工智能（XAI）是人工智能社区中一个充满活力的研究课题，受到不同方法和领域的越来越多的关注。虽然有很多关于该主题的文章，但XAI仍缺乏共享的术语和框架，无法为解释提供结构上的完整性。在我们的工作中，我们通过提出一个新的解释定义来解决这些问题，该定义是文献中可以找到的综合体。我们认为，解释不是原子性的，而是来自于模型和其输入输出映射的证据，以及人类对这些证据的解释组合而成。此外，我们将解释纳入到真实性（即解释是否是模型内部工作和决策过程的真实描述）和可信度（即解释对用户的说服力）的特性中。使用我们提出的理论框架简化了这些特性的操作，并提供了评估AI模型可解释性的综合方法。我们将该框架应用于生物医学领域，并展示其使得我们能够确定模型决策中最具影响力的输入特征，支持对模型行为的解释。我们相信，我们的贡献将促进可解释性工具和方法的发展，从而推进AI算法在关键领域的可信部署。

    EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the artificial intelligence community, with growing interest across methods and domains. Much has been written about the subject, yet XAI still lacks shared terminology and a framework capable of providing structural soundness to explanations. In our work, we address these issues by proposing a novel definition of explanation that is a synthesis of what can be found in the literature. We recognize that explanations are not atomic but the combination of evidence stemming from the model and its input-output mapping, and the human interpretation of this evidence. Furthermore, we fit explanations into the properties of faithfulness (i.e., the explanation being a true description of the model's inner workings and decision-making process) and plausibility (i.e., how much the explanation looks convincing to the user). Using our proposed theoretical framework simplifies how these properties are operationalized and it prov
    
[^157]: MixupE：从方向导数角度理解和改进Mixup技术

    MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13381](http://arxiv.org/abs/2212.13381)

    本文从方向导数的角度分析了深度神经网络中常用的数据增强技术Mixup，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一思路，作者提出了改进版的Mixup，理论上证明具有更好的泛化性能，并在各种领域的数据集上进行了验证，表现出比Mixup更好的效果。

    

    Mixup是一种深度神经网络中流行的数据增强技术，通过线性插值输入和它们的标签生成额外的样本。该技术已被证实在许多学习范式和应用中提高了泛化性能。本文首先对Mixup进行分析，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一新的洞见，我们提出了一种改进版本的Mixup，理论上证明它可以比原始版本具有更好的泛化性能。为了证明这种方法的有效性，我们在各种领域进行了实验，例如图像、表格数据、语音和图形。我们的结果表明，所提出的方法改进了Mixup在多个数据集上的表现，在使用各种架构时都表现出比Mixup更好的性能，例如在ImageNet的top-1精度上比Mixup提高了0.8%。

    Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.
    
[^158]: 可伸缩自适应计算用于迭代生成

    Scalable Adaptive Computation for Iterative Generation. (arXiv:2212.11972v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11972](http://arxiv.org/abs/2212.11972)

    本文提出了循环接口网络（RINs）结构，将核心计算与数据维数分离，实现了自适应计算，解决了高维数据生成的可伸缩性问题。这一结构将大部分计算集中在潜在标记上，使用交叉注意力在潜在标记和数据标记之间路由信息，具有很好的生成效果并可扩展到数十万维的数据集上。

    

    自然数据是冗余的，但主要的结构仍然在其输入和输出空间上统一切割计算。本文提出了循环接口网络（RINs），这是一种基于注意力的结构，它将其核心计算与数据的维数分离，实现了更可伸缩的高维数据生成的自适应计算。RINs将大部分计算（即全局自注意力）集中在一组潜在标记上，使用交叉注意力在潜在标记和数据标记之间读取和写入（即路由）信息。堆叠RIN模块允许自下而上（从数据到潜在）和自上而下（从潜在到数据）反馈，从而实现更深层和更具表现力的路由。虽然这种路由引入了挑战，但在任务（和路由问题）逐渐变化的循环计算设置中，这个问题就不那么棘手了，比如扩散模型的迭代生成。我们展示了如何通过在每次修订过程的前向传递中调节潜在标记，并引入了一种灵活的适配器结构，可以实现高维数据和结构的有效扩展。我们通过生成高质量的图像和多元时间序列展示了RINs的有效性，并表明RINs可以优雅地扩展到具有数十万维度的数据集。

    Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the rev
    
[^159]: 提升线性探测：超越少样本学习的极限

    Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners. (arXiv:2212.10873v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10873](http://arxiv.org/abs/2212.10873)

    本论文提出了一种混合线性探测和上下文学习的方法，结合了两者的优点，旨在提高模型在少样本和零样本情况下的性能表现。

    

    通过上下文学习，大规模语言模型可以高效地进行少样本学习，而无需进行额外的模型微调。然而，由于底层语言模型的固有输入长度限制，上下文学习的性能在可用训练样本数量上并不具有可伸缩性。与此同时，许多研究表明语言模型也是强大的特征提取器，使其能够以黑匣子的方式使用，并实现线性探测范式，即在预先提取的输入表示之上训练轻量级鉴别器。本文提出了prompt-augmented linear probing（PALP），它是线性探测和上下文学习的混合体，兼具二者的优点。PALP继承了线性探测的可伸缩性和使语言模型通过将输入定制为更易理解的形式来派生更有意义表示的能力。在各种数据集的深入调查中，我们验证了PALP在少样本和零样本情况下均胜过最先进的方法。

    Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified th
    
[^160]: 面向视觉、语音和语言的上下文化目标表示自监督学习高效性改进

    Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language. (arXiv:2212.07525v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07525](http://arxiv.org/abs/2212.07525)

    本文提出了一种上下文化目标表示自监督学习的高效性改进方法，称为data2vec 2.0，它在多项任务中取得了和其他算法相当的准确率，但需要的预训练时间较短。

    

    当前的自监督学习算法通常是模态特定的，需要大量的计算资源。为了解决这些问题，我们提高了data2vec的训练效率，该学习目标可以推广到多种模态。我们不对掩蔽标记进行编码，使用快速卷积解码器，并分摊构建教师表示的工作。data2vec 2.0受到data2vec引入的丰富上下文化目标表示的益处，可以实现快速自监督学习。在ImageNet-1K图像分类实验中，data2vec 2.0在低16.4倍的预训练时间内与蒙版自编码器的准确率相匹配，在Librispeech语音识别中，它的表现与wav2vec 2.0相当，时间少10.6倍，在GLUE自然语言理解方面，它与重新训练的RoBERTa模型的时间相比减半。在牺牲一定的速度以换取准确性的情况下，使用训练了150个epochs的ViT-L模型可以得到86.8\%的ImageNet-1K top-1准确率。

    Current self-supervised learning algorithms are often modality-specific and require large amounts of computational resources. To address these issues, we increase the training efficiency of data2vec, a learning objective that generalizes across several modalities. We do not encode masked tokens, use a fast convolutional decoder and amortize the effort to build teacher representations. data2vec 2.0 benefits from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner. Experiments on ImageNet-1K image classification show that data2vec 2.0 matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time, on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x less time, and on GLUE natural language understanding it matches a retrained RoBERTa model in half the time. Trading some speed for accuracy results in ImageNet-1K top-1 accuracy of 86.8\% with a ViT-L model trained for 150 epochs.
    
[^161]: TIDE：用于图上深度学习的时间导数扩散

    TIDE: Time Derivative Diffusion for Deep Learning on Graphs. (arXiv:2212.02483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02483](http://arxiv.org/abs/2212.02483)

    本文提出了一种新方法 TIDE，通过时间导数图扩散克服了图神经网络中消息传递框架的结构限制，实现了高效地中长距离通信，并在图神经网络任务中达到了 state-of-the-art 的性能表现。

    

    图神经网络的一个重要范式是基于消息传递框架的。在这个框架中，信息通信仅在相邻节点之间实现。使用这种范式的方法的挑战是确保节点之间的高效和准确的长距离通信，因为深度卷积网络容易产生过度平滑。在本文中，我们提出了一种基于时间导数图扩散（TIDE）的新方法，以克服消息传递框架的这些结构限制。我们的方法允许优化扩散的空间范围，适用于各种任务和网络通道，从而实现中长距离通信的高效率。此外，我们还展示了我们的架构设计也使本地消息传递成为可能，从而继承了本地消息传递方法的能力。我们在广泛使用的图基准和合成网格和图数据集上展示，所提出的框架优于	state-of-the-art 方法。

    A prominent paradigm for graph neural networks is based on the message-passing framework. In this framework, information communication is realized only between neighboring nodes. The challenge of approaches that use this paradigm is to ensure efficient and accurate long-distance communication between nodes, as deep convolutional networks are prone to oversmoothing. In this paper, we present a novel method based on time derivative graph diffusion (TIDE) to overcome these structural limitations of the message-passing framework. Our approach allows for optimizing the spatial extent of diffusion across various tasks and network channels, thus enabling medium and long-distance communication efficiently. Furthermore, we show that our architecture design also enables local message-passing and thus inherits from the capabilities of local message-passing approaches. We show that on both widely used graph benchmarks and synthetic mesh and graph datasets, the proposed framework outperforms state-
    
[^162]: 懒惰Hessian的二阶优化方法

    Second-order optimization with lazy Hessians. (arXiv:2212.00781v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2212.00781](http://arxiv.org/abs/2212.00781)

    本论文提出了一种用于解决可能为非凸问题的二阶优化算法，使用懒惰Hessian更新和重用先前看到的Hessian，可显著降低总算术复杂度，并提高了效率。

    

    我们分析了用于解决一般可能是非凸优化问题的懒惰Hessian更新的牛顿法。我们建议在计算方法的每个步骤中重复使用先前看到的Hessian，同时计算新的梯度。这显着降低了二阶优化方案的总算术复杂度。通过使用立方正则化技术，我们建立了我们的方法到二阶稳定点的快速全局收敛性，而Hessian在每个迭代中不需要更新。对于凸问题，我们证明了具有二次正则化的懒惰牛顿步骤具有全局和局部超线性速率，这更容易计算。更新Hessian的最佳频率是每$d$个迭代一次，其中$d$是问题的维度。这可以证明将二阶算法的总算术复杂度提高了$\sqrt{d}$倍。

    We analyze Newton's method with lazy Hessian updates for solving general possibly non-convex optimization problems. We propose to reuse a previously seen Hessian for several iterations while computing new gradients at each step of the method. This significantly reduces the overall arithmetical complexity of second-order optimization schemes. By using the cubic regularization technique, we establish fast global convergence of our method to a second-order stationary point, while the Hessian does not need to be updated each iteration. For convex problems, we justify global and local superlinear rates for lazy Newton steps with quadratic regularization, which is easier to compute. The optimal frequency for updating the Hessian is once every $d$ iterations, where $d$ is the dimension of the problem. This provably improves the total arithmetical complexity of second-order algorithms by a factor $\sqrt{d}$.
    
[^163]: 通过非凸低秩半正定松弛实现对对抗训练神经网络的严格认证

    Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations. (arXiv:2211.17244v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.17244](http://arxiv.org/abs/2211.17244)

    本文提出了一种新的，基于低秩半正定松弛技术实现对对抗性训练神经网络的严格认证方法，它能够实现采用更便宜的SDP方法相当的强认证。

    

    众所周知，对抗训练可以产生高质量的神经网络模型，这些模型在经验上对抗性扰动具有鲁棒性。然而，一旦进行了对抗性训练，人们通常希望证明该模型在未来的所有攻击中真正具有鲁棒性。不幸的是，面对对抗训练模型时，所有现有方法都难以做出足够有效的证明。特别是线性规划（LP）技术，即使经过混合整数线性规划（MILP）和分支定界（BnB）技术的改进，也会面临"凸松弛壁垒"，使得它们难以进行高质量的证明。因此，本文提出了一种基于低秩半正定松弛的非凸认证技术。非凸松弛可以进行与更昂贵的半正定规划（SDP）方法相媲美的强认证，同时优化范围更广。

    Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming (LP) techniques in particular face a "convex relaxation barrier" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming (MILP) and branch-and-bound (BnB) techniques. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming (SDP) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive SDP methods, while optimizing over dramatica
    
[^164]: 因果图中前门调整的线性时间算法

    Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16468](http://arxiv.org/abs/2211.16468)

    在因果图中，提出了解决前门调整的线性时间算法，通过观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。

    

    从观测数据中估计因果效应是实证科学中的基本任务。当系统中涉及未观察到的混淆因素时，这变得尤为具有挑战性。本文侧重于前门调整——一种经典技术，它使用观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。虽然前门估计的统计特性已经很好地理解了，但它的算法方面长期以来一直未得到探究。最近，Jeong，Tian和Barenboim [NeurIPS 2022]提出了一种第一个多项式时间算法，用于在给定的有向无环图（DAG）中找到满足前门准则的集合，其运行时间为$O（n^3（n+m））$，其中$n$表示变量的数量，$m$表示因果图的边的数量。在我们的工作中，我们提供了第一个具有线性时间复杂度的算法，即$O（n+m）$，用于这项任务，从而达到了渐近最优的时间复杂性。

    Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
    
[^165]: GREAD: 基于图神经反应扩散网络的研究

    GREAD: Graph Neural Reaction-Diffusion Networks. (arXiv:2211.14208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14208](http://arxiv.org/abs/2211.14208)

    本文介绍了一种基于反应扩散方程的GNN方法，考虑了所有流行的反应方程类型和一种特殊的反应方程，是目前其中最全面的研究之一，并在实验中表现出更好的性能。

    

    图神经网络是深度学习中最受欢迎的研究课题之一。GNN方法通常是基于图信号处理理论进行设计的。特别地，扩散方程被广泛用于设计GNN的核心处理层，因此不可避免地容易出现过度平滑问题。最近，有几篇论文注意到反应方程和扩散方程的结合。不过，它们都只考虑了有限的反应方程形式。因此，我们提出了一种基于反应扩散方程的GNN方法，考虑了所有流行的反应方程类型和我们设计的一种特殊反应方程。据我们所知，我们的论文是关于基于反应扩散方程的GNN的最全面的研究之一。在我们使用9个数据集和28个基准模型进行的实验中，我们的方法 GREAD 在大多数情况下性能优于它们。进一步的人工数据实验显示……

    Graph neural networks (GNNs) are one of the most popular research topics for deep learning. GNN methods typically have been designed on top of the graph signal processing theory. In particular, diffusion equations have been widely used for designing the core processing layer of GNNs, and therefore they are inevitably vulnerable to the notorious oversmoothing problem. Recently, a couple of papers paid attention to reaction equations in conjunctions with diffusion equations. However, they all consider limited forms of reaction equations. To this end, we present a reaction-diffusion equation-based GNN method that considers all popular types of reaction equations in addition to one special reaction equation designed by us. To our knowledge, our paper is one of the most comprehensive studies on reaction-diffusion equation-based GNNs. In our experiments with 9 datasets and 28 baselines, our method, called GREAD, outperforms them in a majority of cases. Further synthetic data experiments show
    
[^166]: 动态图和层次融合用于羽毛球球员移动预测

    Where Will Players Move Next? Dynamic Graphs and Hierarchical Fusion for Movement Forecasting in Badminton. (arXiv:2211.12217v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12217](http://arxiv.org/abs/2211.12217)

    该研究提出了一种新颖的动态图和层次融合模型，用于预测羽毛球比赛中球员移动，该模型将基于序列的模型和基于图形的模型相结合，并具有一个层次融合层，以考虑时空依赖性。实验结果表明了该方法在真实数据上的有效性。

    

    运动分析已经吸引了越来越多的关注，因为对各种数据的分析可以为训练策略、球员评估等提供洞察。本文着重于预测返回技能的类型以及基于前一次击球球员将移动到哪里。由于此问题尚未得到解决，因此可以通过将其构造为序列预测任务，并且采用基于序列和基于图形的模型来解决移动预测问题。然而，现有的基于序列的模型忽略了球员之间交互的影响，基于图形的模型仍然存在关于下一次移动的多方面观点。此外，还没有现有工作可以表示球员射击类型和移动之间的战略关系。为了解决这些挑战，我们首先介绍了球员移动图（PM）图的过程，以利用具有战略关系的球员的结构移动。基于PM图，我们提出了一种新颖的动态图和层次融合（DGHF）模型，用于预测羽毛球比赛中的球员移动。DGHF模型将基于序列的模型和基于图形的模型相结合，并具有一个层次融合层，以考虑时空依赖性。实验结果表明了所提出方法在真实数据上的有效性。

    Sports analytics has captured increasing attention since analysis of the various data enables insights for training strategies, player evaluation, etc. In this paper, we focus on predicting what types of returning strokes will be made, and where players will move to based on previous strokes. As this problem has not been addressed to date, movement forecasting can be tackled through sequence-based and graph-based models by formulating as a sequence prediction task. However, existing sequence-based models neglect the effects of interactions between players, and graph-based models still suffer from multifaceted perspectives on the next movement. Moreover, there is no existing work on representing strategic relations among players' shot types and movements. To address these challenges, we first introduce the procedure of the Player Movements (PM) graph to exploit the structural movements of players with strategic relations. Based on the PM graph, we propose a novel Dynamic Graphs and Hier
    
[^167]: 对抗性廉价交流

    Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11030](http://arxiv.org/abs/2211.11030)

    本文提出了一种新型对抗性设置，在其中对手只能将信息附加到受害者的观察中，从而产生最小的影响范围，并提出对抗性廉价交流（ACT）算法进行对手训练。在高度受限的情况下，使用ACT训练的对手仍会对受害者的训练和测试表现产生显著影响，揭示了强化学习算法中的一种新的攻击向量。

    

    强化学习中的对抗性攻击通常假定攻击者可以高度特权地访问受害者的参数、环境或数据。本文提出了一种称为廉价交流MDP的新型对抗性设置，其中对手只能将确定性信息附加到受害者的观察中，从而产生最小的影响范围。对手不能掩盖地面事实，影响基本环境动态或奖励信号，引入不稳定性，增加随机性，看到受害者的动作或访问他们的参数。此外，我们提出了一种简单的元学习算法，称为对抗性廉价交流（ACT），在这种设置中对对手进行训练。我们证明，即使在高度受限的情况下，使用ACT训练的对手仍会显着影响受害者的训练和测试表现。影响训练时间表现揭示了一种新的攻击向量，并为现有强化学习算法的成功和失败模式提供了见解。

    Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
    
[^168]: HiveNAS: 采用人工蜂群优化的神经架构搜索

    HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization. (arXiv:2211.10250v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2211.10250](http://arxiv.org/abs/2211.10250)

    本文提出了一种采用人工蜂群优化的神经架构搜索框架HiveNAS，它在极短的时间内就能超越其他基于群智的NAS框架，成为最优。

    

    传统的神经网络开发过程需要大量专业知识，并且严重依赖于直觉和试错。神经架构搜索（NAS）框架被引入来稳健地搜索网络拓扑，并促进神经网络的自动开发。在NAS上，虽然一些优化方法（如遗传算法）已经被广泛探究，但其他元启发式优化算法尚未被研究。在本研究中，我们评估了采用人工蜂群优化进行神经架构搜索的可行性。我们提出的框架HiveNAS在一小部分时间内就超越了现有的基于群智的NAS框架，成为最优。

    The traditional Neural Network-development process requires substantial expert knowledge and relies heavily on intuition and trial-and-error. Neural Architecture Search (NAS) frameworks were introduced to robustly search for network topologies, as well as facilitate the automated development of Neural Networks. While some optimization approaches -- such as Genetic Algorithms -have been extensively explored in the NAS context, other Metaheuristic Optimization algorithms have not yet been investigated. In this study, we evaluate the viability of Artificial Bee Colony optimization for Neural Architecture Search. Our proposed framework, HiveNAS, outperforms existing state-of-the-art Swarm Intelligence-based NAS frameworks in a fraction of the time.
    
[^169]: 物理信息神经网络用于受限数据情况下重建重力气流

    Physics-informed neural networks for gravity currents reconstruction from limited data. (arXiv:2211.09715v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2211.09715](http://arxiv.org/abs/2211.09715)

    本文研究了物理信息神经网络在受限数据情况下重建重力气流方面的应用，通过光衰减技术(LAT)平均空间密度测量进行训练，结果表明该方法可以在少量测量值下快速、精确地重建流场，并且与实验数据比较表明了其可靠性。

    

    本文研究了物理信息神经网络(PINNs)在三维重建不稳定重力气流方面的应用。在PINN框架下，通过训练神经网络重建流场，使其目标函数惩罚网络预测与实际观测数据之间的不匹配并使用自动微分嵌入基础方程。本研究依赖于规范锁定交换配置的高保真数值实验，可在几个训练数据库上定量测试PINNs重建能力，这些数据库模拟了密度和速度的最新实验测量技术。特别地，采用光衰减技术(LAT)平均空间密度测量进行训练过程。根据两个标准提出了流重建的最佳实验设置: 实施复杂度和重建准确性。结果表明，PINNs可以从少量LAT测量值中快速、精确地重建不稳定流场，并且与粒子图像测速术(PIV)和荧光颗粒跟踪测速术(FPTV)的实验数据比较表明了该方法的可靠性和稳健性。

    The present work investigates the use of physics-informed neural networks (PINNs) for the 3D reconstruction of unsteady gravity currents from limited data. In the PINN context, the flow fields are reconstructed by training a neural network whose objective function penalizes the mismatch between the network predictions and the observed data and embeds the underlying equations using automatic differentiation. This study relies on a high-fidelity numerical experiment of the canonical lock-exchange configuration. This allows us to benchmark quantitatively the PINNs reconstruction capabilities on several training databases that mimic state-of-the-art experimental measurement techniques for density and velocity. Notably, spatially averaged density measurements by light attenuation technique (LAT) are employed for the training procedure. An optimal experimental setup for flow reconstruction by PINNs is proposed according to two criteria : the implementation complexity and the accuracy of the 
    
[^170]: 贝叶斯固定预算最佳臂识别

    Bayesian Fixed-Budget Best-Arm Identification. (arXiv:2211.08572v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08572](http://arxiv.org/abs/2211.08572)

    本文提出一种贝叶斯消除算法，用于解决固定预算下最佳臂识别问题，并且推导了其与先验相关的误识别上界，此算法优于频率学派方法，与无保证的贝叶斯算法相竞争。

    

    固定预算最佳臂识别是一种赌博问题，代理人最大化识别最佳臂的概率，在一个固定的观察预算内。在本文中，我们研究了这个问题的贝叶斯设置。我们提出了一种贝叶斯消除算法，并推导出其误识别最优臂的概率的上界。这个上界反映了先验的质量，并且是此设置中第一个与分布相关的上界。我们使用类似于频率学派的论证证明了它，我们一直使用先验，然后在最后将赌徒实例积分掉，也为2个臂的贝叶斯赌徒提供了误识别概率的下界，并且展示了我们的上界在任何预算下（几乎）匹配。我们的实验证明了，贝叶斯消除优于频率学派方法，并且在此设置中，与无法保证的最先进的贝叶斯算法相竞争。

    Fixed-budget best-arm identification (BAI) is a bandit problem where the agent maximizes the probability of identifying the optimal arm within a fixed budget of observations. In this work, we study this problem in the Bayesian setting. We propose a Bayesian elimination algorithm and derive an upper bound on its probability of misidentifying the optimal arm. The bound reflects the quality of the prior and is the first distribution-dependent bound in this setting. We prove it using a frequentist-like argument, where we carry the prior through, and then integrate out the bandit instance at the end. We also provide a lower bound on the probability of misidentification in a $2$-armed Bayesian bandit and show that our upper bound (almost) matches it for any budget. Our experiments show that Bayesian elimination is superior to frequentist methods and competitive with the state-of-the-art Bayesian algorithms that have no guarantees in our setting.
    
[^171]: SPADE4: 基于稀疏性和时滞嵌入的流行病预测

    SPADE4: Sparsity and Delay Embedding based Forecasting of Epidemics. (arXiv:2211.08277v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08277](http://arxiv.org/abs/2211.08277)

    SPADE4是一种基于稀疏性和时滞嵌入的流行病预测方法，利用随机特征模型和Takens的时滞嵌入定理处理数据稀缺和捕捉基础系统的性质。

    

    预测疾病的发展是具有挑战性的，尤其是当数据可用性有限和不完整时。最受欢迎的建模和预测传染病流行的工具是隔离模型。它们将人群根据健康状况分成不同的隔离群体，并使用动力系统模型来描述这些隔离群体的动态。然而，由于疾病传播和人类互动的复杂性，这些预定义系统可能无法捕捉流行病的真实动态。为了克服这个缺点，我们提出了基于稀疏性和时滞嵌入的流行病预测（SPADE4）。SPADE4预测可观察变量的未来轨迹，而不需要其他变量或基础系统的知识。我们使用具有稀疏回归的随机特征模型来处理数据稀缺问题，并采用Takens的时滞嵌入定理，从观察变量中捕捉基础系统的性质。

    Predicting the evolution of diseases is challenging, especially when the data availability is scarce and incomplete. The most popular tools for modelling and predicting infectious disease epidemics are compartmental models. They stratify the population into compartments according to health status and model the dynamics of these compartments using dynamical systems. However, these predefined systems may not capture the true dynamics of the epidemic due to the complexity of the disease transmission and human interactions. In order to overcome this drawback, we propose Sparsity and Delay Embedding based Forecasting (SPADE4) for predicting epidemics. SPADE4 predicts the future trajectory of an observable variable without the knowledge of the other variables or the underlying system. We use random features model with sparse regression to handle the data scarcity issue and employ Takens' delay embedding theorem to capture the nature of the underlying system from the observed variable. We sho
    
[^172]: 基于因式分层变分自编码器的对比学习改进语音表征

    Improved disentangled speech representations using contrastive learning in factorized hierarchical variational autoencoder. (arXiv:2211.08191v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.08191](http://arxiv.org/abs/2211.08191)

    本文引入对比学习在因式分层变分自编码器中，实现更好的语音表征，通过捕获说话者身份改善模型性能，达到最新水平。

    

    利用说话者身份和语音内容在不同时间尺度上变化的事实，因式分层变分自编码器使用不同的潜在变量来表示这两个属性。通过不同潜在变量的先验设置来实现这些属性的分离。对于说话者身份变量的先验设置，因式分层变分自编码器假设它是具有变化的均值和固定方差的高斯分布。通过设置一个较小的固定方差，训练过程促进了相同句子内的身份变量靠近其先验均值。为了使同一说话者的表示时身份变量相聚而与其他说话者的身份变量相隔，我们在因式分层变分自编码器框架中引入对比学习。模型结构没有发生改变。引入对比损失后，属性分离表示更有效地捕获了说话者身份。在TIMIT数据集和VCTK数据集上的实验表明，相对于传统的没有对比学习的因式分层变分自编码器，在说话者识别任务上，所提出的方法表现优异，达到了最新水平。

    Leveraging the fact that speaker identity and content vary on different time scales, \acrlong{fhvae} (\acrshort{fhvae}) uses different latent variables to symbolize these two attributes. Disentanglement of these attributes is carried out by different prior settings of the corresponding latent variables. For the prior of speaker identity variable, \acrshort{fhvae} assumes it is a Gaussian distribution with an utterance-scale varying mean and a fixed variance. By setting a small fixed variance, the training process promotes identity variables within one utterance gathering close to the mean of their prior. However, this constraint is relatively weak, as the mean of the prior changes between utterances. Therefore, we introduce contrastive learning into the \acrshort{fhvae} framework, to make the speaker identity variables gathering when representing the same speaker, while distancing themselves as far as possible from those of other speakers. The model structure has not been changed in th
    
[^173]: 深度学习的方向隐私

    Directional Privacy for Deep Learning. (arXiv:2211.04686v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04686](http://arxiv.org/abs/2211.04686)

    本文采用基于von Mises-Fisher分布的机制应用方向隐私来保护深度学习模型训练隐私，并提供了$\epsilon d$-隐私保证，可根据输入梯度的差异平滑退化。

    

    差分隐私随机梯度下降（DP-SGD）是保护深度学习模型训练隐私的关键方法。它在训练过程中向梯度加入各向同性高斯噪声，可能会破坏其效用。度量差分隐私可以提供基于任意度量的替代机制，这可能更适合于维护其效用。本文通过基于von Mises-Fisher（VMF）分布的机制，采用\textit{角距离} 扰动梯度，从而广泛保留梯度方向，应用\textit{方向隐私}。我们证明，这提供深度学习训练的$\epsilon$-DP和$\epsilon d$-隐私，而不是高斯机制的$(\epsilon,\delta)$-隐私。我们观察到$\epsilon d$-隐私保证不需要$\delta>0$项，但会根据输入梯度的差异而平滑退化。随着$\epsilon$在其中的变化，我们展示了使用方向隐私的深度学习训练的实验结果和隐私分析，并且我们探究了这种技术的某些应用。

    Differentially Private Stochastic Gradient Descent (DP-SGD) is a key method for applying privacy in the training of deep learning models. This applies isotropic Gaussian noise to gradients during training, which can perturb these gradients in any direction, damaging utility. Metric DP, however, can provide alternative mechanisms based on arbitrary metrics that might be more suitable for preserving utility. In this paper, we apply \textit{directional privacy}, via a mechanism based on the von Mises-Fisher (VMF) distribution, to perturb gradients in terms of \textit{angular distance} so that gradient direction is broadly preserved. We show that this provides both $\epsilon$-DP and $\epsilon d$-privacy for deep learning training, rather than the $(\epsilon, \delta)$-privacy of the Gaussian mechanism; we observe that the $\epsilon d$-privacy guarantee does not require a $\delta>0$ term but degrades smoothly according to the dissimilarity of the input gradients.  As $\epsilon$s between thes
    
[^174]: LMD：一种可学习的掩蔽网络，用于检测语音验证中的对抗性样本

    LMD: A Learnable Mask Network to Detect Adversarial Examples for Speaker Verification. (arXiv:2211.00825v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.00825](http://arxiv.org/abs/2211.00825)

    LMD是一种攻击者独立和可解释的方法，用于检测语音验证中的对抗性样本，其核心是通过神经网络生成掩蔽谱图，利用ASV分数的绝对差异来检测对抗性样本。

    

    虽然自动语音验证（ASV）的安全性受到最近出现的对抗性攻击的严重威胁，但已经有一些对策来缓解这种威胁。然而，许多防御方法不仅需要攻击者的先前知识，还有弱的可解释性。为了解决这个问题，本文提出了一种攻击者独立和可解释的方法，称为可以学习的掩蔽检测器（LMD），以区分对抗性样本和真正的样本。它利用分数变化作为指标来检测对抗性样本，其中分数变化是原始音频记录及其从掩蔽复杂谱图合成的转换音频的ASV分数之间的绝对差异。分数变化检测的核心组成部分是通过神经网络生成掩蔽谱图。神经网络只需要真实的样本进行训练，这使其成为一种攻击者独立的方法。

    Although the security of automatic speaker verification (ASV) is seriously threatened by recently emerged adversarial attacks, there have been some countermeasures to alleviate the threat. However, many defense approaches not only require the prior knowledge of the attackers but also possess weak interpretability. To address this issue, in this paper, we propose an attacker-independent and interpretable method, named learnable mask detector (LMD), to separate adversarial examples from the genuine ones. It utilizes score variation as an indicator to detect adversarial examples, where the score variation is the absolute discrepancy between the ASV scores of an original audio recording and its transformed audio synthesized from its masked complex spectrogram. A core component of the score variation detector is to generate the masked spectrogram by a neural network. The neural network needs only genuine examples for training, which makes it an attacker-independent approach. Its interpretab
    
[^175]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^176]: 基于生成先验的稳定深度MRI重建

    Stable Deep MRI Reconstruction using Generative Priors. (arXiv:2210.13834v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.13834](http://arxiv.org/abs/2210.13834)

    本文提出了一种基于生成图像先验的深度神经网络正则化器，通过在参考图像上训练编码高层次的域统计信息，可以实现在不受子采样方式的影响下高质量稳定的MRI重建，并提供了概率解释进行不确定性量化。

    

    近年来，数据驱动方法在磁共振成像(MRI)重建方面取得了显著的成功，但由于缺乏普适性和可解释性，其集成到临床常规中仍然存在挑战。在本文中，我们提出了一个基于生成图像先验的统一框架，以解决这些挑战。我们提出了一种新颖的深度神经网络正则化器，它在仅有参考幅值图像的生成环境下进行训练。训练后，正则化器编码了高层次的域统计信息，我们通过合成无数据图像来证明这一点。将训练好的模型嵌入到经典的变分方法中，可以得到高质量的重建结果，不受子采样方式的影响。此外，当面临对比度变化的分布外数据时，该模型呈现出稳定的行为。此外，概率解释提供了重建结果的分布，从而允许不确定性量化。

    Data-driven approaches recently achieved remarkable success in magnetic resonance imaging (MRI) reconstruction, but integration into clinical routine remains challenging due to a lack of generalizability and interpretability. In this paper, we address these challenges in a unified framework based on generative image priors. We propose a novel deep neural network based regularizer which is trained in a generative setting on reference magnitude images only. After training, the regularizer encodes higher-level domain statistics which we demonstrate by synthesizing images without data. Embedding the trained model in a classical variational approach yields high-quality reconstructions irrespective of the sub-sampling pattern. In addition, the model shows stable behavior when confronted with out-of-distribution data in the form of contrast variation. Furthermore, a probabilistic interpretation provides a distribution of reconstructions and hence allows uncertainty quantification. To reconstr
    
[^177]: 具有群对称的视觉连续强化学习

    Continual Vision-based Reinforcement Learning with Group Symmetries. (arXiv:2210.12301v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12301](http://arxiv.org/abs/2210.12301)

    本论文提出了一种名为COVERS的连续视觉强化学习方法，它能够识别基本群操作下等价的任务，并为每组等价任务培养一种策略，提高样本效率，减少唯一策略的数量，并实现了更强的泛化性能。

    

    连续强化学习旨在顺序学习各种任务，保留执行先前遇到的任务的能力，同时为新任务开发新策略。但是，目前的连续RL方法忽略了某些任务在基本群运算（如旋转或平移）下具有相同的特征，特别是在视觉输入中。他们可能会为每个类似的任务不必要地学习和维护新策略，导致样本效率和泛化能力弱。为了解决这个问题，我们引入了一种名为COVERS的唯一具有群对称性的连续视觉强化学习方法，它为每组等价任务培养一种策略，而不是为每个任务单独制定策略。COVERS采用基于近端策略优化的强化学习算法，配备等变特征提取器和一种依赖于提取的不变特征的新任务分组机制。我们在桌面操作任务序列上评估COVERS并与最先进的连续RL方法进行比较。结果表明COVERS提高了样本效率，减少了唯一策略的数量，并实现了更强的泛化性能。

    Continual reinforcement learning aims to sequentially learn a variety of tasks, retaining the ability to perform previously encountered tasks while simultaneously developing new policies for novel tasks. However, current continual RL approaches overlook the fact that certain tasks are identical under basic group operations like rotations or translations, especially with visual inputs. They may unnecessarily learn and maintain a new policy for each similar task, leading to poor sample efficiency and weak generalization capability. To address this, we introduce a unique Continual Vision-based Reinforcement Learning method that recognizes Group Symmetries, called COVERS, cultivating a policy for each group of equivalent tasks rather than individual tasks. COVERS employs a proximal policy optimization-based RL algorithm with an equivariant feature extractor and a novel task grouping mechanism that relies on the extracted invariant features. We evaluate COVERS on sequences of table-top mani
    
[^178]: 关于采用基于模型的强化学习进行跨任务转移的可行性研究

    On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning. (arXiv:2210.10763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10763](http://arxiv.org/abs/2210.10763)

    本文研究了是否可以利用现代基于模型的强化学习算法学习的内部模型来更加快速地解决新的、明显不同的任务。我们提出了一种基于模型的跨任务转移框架，通过离线多任务预训练和在线跨任务微调，获得了在各种模拟机器人操作任务中显著的样本效率改进。

    

    强化学习算法可以直接从图像观测中解决具有挑战性的控制问题，但通常需要数百万的环境交互才能做到。最近，基于模型的强化学习算法通过同时学习内部世界模型并补充真实环境交互以进行策略改进，极大地提高了样本效率。但是，从零开始学习有效的世界模型是具有挑战性的，与人类依赖于理解世界和视觉线索学习新技能形成鲜明对比。在本研究中，我们调查了现代基于模型的强化学习算法所学习的内部模型是否可以利用来更快地解决新的、明显不同的任务。我们提出了基于模型的跨任务转移（XTRA）框架，该框架具有可扩展的预训练和微调学习的世界模型。通过离线多任务预训练和在线跨任务微调，我们在各种模拟机器人操作任务中实现了显著的样本效率改进。我们的结果表明，XTRA是一种有效和可推广的RL方向。

    Reinforcement Learning (RL) algorithms can solve challenging control problems directly from image observations, but they often require millions of environment interactions to do so. Recently, model-based RL algorithms have greatly improved sample-efficiency by concurrently learning an internal model of the world, and supplementing real environment interactions with imagined rollouts for policy improvement. However, learning an effective model of the world from scratch is challenging, and in stark contrast to humans that rely heavily on world understanding and visual cues for learning new skills. In this work, we investigate whether internal models learned by modern model-based RL algorithms can be leveraged to solve new, distinctly different tasks faster. We propose Model-Based Cross-Task Transfer (XTRA), a framework for sample-efficient online RL with scalable pretraining and finetuning of learned world models. By offline multi-task pretraining and online cross-task finetuning, we ach
    
[^179]: 使用干预方法提高文本匹配推荐系统的跨领域泛化能力

    Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems. (arXiv:2210.10636v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.10636](http://arxiv.org/abs/2210.10636)

    本文提出了使用干预方法来提高文本匹配推荐系统的跨领域泛化能力。研究发现，常用的基于精调模型的方法在具有新领域数据时有反效果，为此，提出了基于干预的重要性度量来解释泛化失败的原因。

    

    给定用户的输入文本，文本匹配推荐系统通过将输入文本与可用商品的描述进行比较来输出相关商品，例如在电子商务平台上的商品推荐。由于用户的兴趣和物品库存预计会发生变化，因此文本匹配系统具有泛化至数据变化的能力，这是一项称为跨领域（OOD）泛化的任务。然而，我们发现，精调大型基础语言模型相对于已配对的商品相关数据（例如用户点击）的流行方法可能对OOD泛化具有反效果。对于商品推荐任务，在推荐新类别或未来时间段的商品时，微调获得的准确性比基础模型更差。为了解释这种泛化失败，我们考虑了基于干预的重要性指标，该指标显示微调模型捕捉了虚假相关性，并未学习确定任何两个文本之间相关性的因果特征。

    Given a user's input text, text-matching recommender systems output relevant items by comparing the input text to available items' description, such as product-to-product recommendation on e-commerce platforms. As users' interests and item inventory are expected to change, it is important for a text-matching system to generalize to data shifts, a task known as out-of-distribution (OOD) generalization. However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization. For a product recommendation task, fine-tuning obtains worse accuracy than the base model when recommending items in a new category or for a future time period. To explain this generalization failure, we consider an intervention-based importance metric, which shows that a fine-tuned model captures spurious correlations and fails to learn the causal features that determine the relevance between any two tex
    
[^180]: Pareto流形学习：通过单任务模型集成解决多任务问题

    Pareto Manifold Learning: Tackling multiple tasks via ensembles of single-task models. (arXiv:2210.09759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09759](http://arxiv.org/abs/2210.09759)

    提出了Pareto流形学习方法，通过在参数空间中进行线性参数化，实现持续的Pareto前沿产生，并可以在一个模型中实现多个任务的优化，同时提高所有任务的效果。

    

    这篇论文介绍了一种多任务学习的方法，即Pareto流形学习。这种方法通过集成单任务模型，实现同时解决多个任务的效果。在这种方法中，通过在参数空间中进行线性参数化，实现持续的Pareto前沿产生，其可以在一个模型中实现多个任务的优化，同时提高所有任务的效果。

    In Multi-Task Learning (MTL), tasks may compete and limit the performance achieved on each other, rather than guiding the optimization to a solution, superior to all its single-task trained counterparts. Since there is often not a unique solution optimal for all tasks, practitioners have to balance tradeoffs between tasks' performance, and resort to optimality in the Pareto sense. Most MTL methodologies either completely neglect this aspect, and instead of aiming at learning a Pareto Front, produce one solution predefined by their optimization schemes, or produce diverse but discrete solutions. Recent approaches parameterize the Pareto Front via neural networks, leading to complex mappings from tradeoff to objective space. In this paper, we conjecture that the Pareto Front admits a linear parameterization in parameter space, which leads us to propose \textit{Pareto Manifold Learning}, an ensembling method in weight space. Our approach produces a continuous Pareto Front in a single trai
    
[^181]: PromptCast：一种新的基于提示的时间序列预测范式

    PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.08964](http://arxiv.org/abs/2210.08964)

    提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。

    

    本文提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast）。在这种新的任务中，将原来的数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，使得语言模型可以直接应用于预测的目的。为了支持和促进这个任务的研究，我们还提出了一个大规模的数据集（PISA）。

    This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
    
[^182]: 伪标记对半监督Gibbs算法的泛化误差有何影响？

    How Does Pseudo-Labeling Affect the Generalization Error of the Semi-Supervised Gibbs Algorithm?. (arXiv:2210.08188v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2210.08188](http://arxiv.org/abs/2210.08188)

    本文研究了Gibbs算法下的伪标记半监督学习的泛化误差，发现泛化性能受到标记和伪标记数据样本之间共享的信息的影响，这对于选择伪标记方法提供了指导。

    

    本文通过Gibbs算法对半监督学习中的伪标记进行了精确刻画，给出了泛化误差的对称化KL信息表达式。同时，获得了泛化误差的不依赖于数据分布的上下界。我们的发现表明，伪标记半监督学习的泛化性能不仅受到输出假设和输入训练数据之间的信息影响，还受到标记和伪标记数据样本之间共享的信息的影响，这为从给定的方法族中选择合适的伪标记方法提供了指导。为了加深我们的理解，我们进一步探讨了两个例子——均值估计和逻辑回归，特别是分析了未标记数据与标记数据比例λ对泛化误差的影响。

    We provide an exact characterization of the expected generalization error (gen-error) for semi-supervised learning (SSL) with pseudo-labeling via the Gibbs algorithm. The gen-error is expressed in terms of the symmetrized KL information between the output hypothesis, the pseudo-labeled dataset, and the labeled dataset. Distribution-free upper and lower bounds on the gen-error can also be obtained. Our findings offer new insights that the generalization performance of SSL with pseudo-labeling is affected not only by the information between the output hypothesis and input training data but also by the information {\em shared} between the {\em labeled} and {\em pseudo-labeled} data samples. This serves as a guideline to choose an appropriate pseudo-labeling method from a given family of methods. To deepen our understanding, we further explore two examples -- mean estimation and logistic regression. In particular, we analyze how the ratio of the number of unlabeled to labeled data $\lambda
    
[^183]: CORL: 面向研究的深度强化学习离线库

    CORL: Research-oriented Deep Offline Reinforcement Learning Library. (arXiv:2210.07105v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07105](http://arxiv.org/abs/2210.07105)

    CORL是一个面向研究的深度强化学习离线库，提供了经过充分基准测试的单文件实现离线和离线到在线强化学习算法，并具有简单的开发体验和实验跟踪功能。

    

    CORL是一个开源库，提供了经过充分基准测试的单文件实现深度离线和离线到在线强化学习算法。它强调简单的开发体验，具有直观的代码库和现代分析跟踪工具。在CORL中，我们将方法实现隔离到单独的单个文件中，使性能相关的细节更容易识别。此外，实验跟踪功能可用于帮助记录指标、超参数、依赖项等到云端。最后，我们通过基准测试常用的D4RL数据集，确保了实现的可靠性，提供了透明的结果源，可用于强大的评估工具，例如性能概要、改进概率或预期在线性能。

    CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.
    
[^184]: FP-Diffusion: 通过强制底层得分的福克-普朗克方程来改进基于得分的扩散模型

    FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation. (arXiv:2210.04296v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04296](http://arxiv.org/abs/2210.04296)

    本文提出了FP-Diffusion方法来改进基于得分的扩散模型，通过强制底层得分的福克-普朗克方程来正则化DSM目标函数，以提高模型似然度和守恒程度。

    

    基于得分的生成模型（SGM）学习一组与数据密度相对应的、噪声条件得分函数。这些扰动的数据密度通过福克-普朗克方程（FPE）相互联系，该方程是一种描述物质扩散过程的偏微分方程。在本文中，我们推导了一个对应的方程，称为得分FPE，其特征是扰动的数据密度（即它们的梯度）的噪声条件得分。令人惊讶的是，虽然DSM得分学习方法具有令人印象深刻的实证性能，但我们观察到学习到的得分未能满足底层的得分FPE，这是地面真实得分的固有自一致性属性。我们证明满足得分FPE是可取的，因为它可以提高似然度和守恒程度。因此，我们提出对DSM目标进行正则化，以强制满足分数FPE的要求。

    Score-based generative models (SGMs) learn a family of noise-conditional score functions corresponding to the data density perturbed with increasingly large amounts of noise. These perturbed data densities are linked together by the Fokker-Planck equation (FPE), a partial differential equation (PDE) governing the spatial-temporal evolution of a density undergoing a diffusion process. In this work, we derive a corresponding equation called the score FPE that characterizes the noise-conditional scores of the perturbed data densities (i.e., their gradients). Surprisingly, despite the impressive empirical performance, we observe that scores learned through denoising score matching (DSM) fail to fulfill the underlying score FPE, which is an inherent self-consistency property of the ground truth score. We prove that satisfying the score FPE is desirable as it improves the likelihood and the degree of conservativity. Hence, we propose to regularize the DSM objective to enforce satisfaction of
    
[^185]: MAMO：面向细粒度视觉-语言表示学习的掩膜多模态建模方法

    MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04183](http://arxiv.org/abs/2210.04183)

    本文提出了一种联合掩膜多模态建模方法，以学习细粒度的多模态表示，通过隐式和显式目标来恢复联合掩膜信号以提高细化的图像-文本交互。

    

    多模态表示学习在各种视觉-语言任务中展现了很大的潜力。然而，当前大部分方法都主要致力于建立全局级别的图像与语言对齐，缺乏有效的细粒度图像-文本交互。为此，本文提出了一种联合掩膜多模态建模方法，以学习细粒度的多模态表示。我们的方法对图像-文本输入进行联合掩膜，并集成了显式和隐式目标来恢复掩膜信号。其中，隐式目标为视觉和语言提供统一且无偏差的目标，模型预测未掩膜输入的潜在多模态表示；显式目标则通过恢复图像块的动量视觉特征和单词标记的概念，进一步丰富多模态表示。通过这样的掩膜建模过程，我们的模型不仅可以学习到细粒度的多模态交互，还能学习到有意义的语义信息。

    Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also a
    
[^186]: 多尺度拓扑奇异性检测

    Topological Singularity Detection at Multiple Scales. (arXiv:2210.00069v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00069](http://arxiv.org/abs/2210.00069)

    本文提出了一种多尺度拓扑奇异性检测方法，可以评估数据的局部固有维度，并量化点的“流形度”，能够检测复杂空间和图像中的奇异性。

    

    流形假设是现代机器学习研究的一个基本假设，它假定数据位于或接近于低固有维度的未知流形上。然而，最近的研究表明，现实世界的数据表现出明显的非流形结构，即奇异性，这可能导致错误的发现。因此，检测这种奇异性在插值和推断任务之前是至关重要的。我们通过开发一个拓扑框架来解决这个问题，该框架能够（i）量化局部固有维度，以及（ii）在多个尺度上产生“欧几里得性”评分，用以评估点的“流形度”。我们的方法可以在图像数据中捕获复杂空间的奇异性，同时捕捉奇异结构和局部几何复杂性。

    The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.
    
[^187]: 分类指标的分析与比较

    Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05355](http://arxiv.org/abs/2209.05355)

    本文回顾并比较了常用于度量分类系统表现的各种指标，发现期望成本指标具有更广泛的适用性和直观性，并可用于解决从连续得分生成分类决策的实践问题。

    

    在机器学习领域，常用各种性能指标来评估分类系统的表现。本文介绍了一些最常用的用于衡量硬决策质量的标准和平衡准确率、标准和平衡错误率、F-beta分数和Matthews相关系数（MCC）等指标。我们回顾了这些和其他指标的定义，并将它们与期望成本（EC）进行比较，后者是每个统计学习课程中都介绍但在机器学习文献中很少使用的指标。我们表明标准和平衡错误率都是EC的特殊情况，进一步展示了EC与F分数和MCC的关系，并认为EC指标优于传统指标，因其更具有优雅性、通用性和直观性，且基于统计学的基本原理。本文中介绍的指标均用于度量硬决策的质量。然而，大多数现代分类系统输出连续得分，而有一个重要的实践问题是如何从这些连续得分中生成分类决策。

    A variety of different performance metrics are commonly used in the machine learning literature for the evaluation of classification systems. Some of the most common ones for measuring quality of hard decisions are standard and balanced accuracy, standard and balanced error rate, F-beta score, and Matthews correlation coefficient (MCC). In this document, we review the definition of these and other metrics and compare them with the expected cost (EC), a metric introduced in every statistical learning course but rarely used in the machine learning literature. We show that both the standard and balanced error rates are special cases of the EC. Further, we show its relation with F-score and MCC and argue that EC is superior to these traditional metrics, being more elegant, general, and intuitive, as well as being based on basic principles from statistics.  The metrics above measure the quality of hard decisions. Yet, most modern classification systems output continuous scores for the class
    
[^188]: 深度强化学习中的白盒对抗策略研究

    White-Box Adversarial Policies in Deep Reinforcement Learning. (arXiv:2209.02167v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.02167](http://arxiv.org/abs/2209.02167)

    本文研究了在强化学习中训练对抗策略的方法，提出了基于白盒攻击的策略，能够访问目标代理的内部状态，从而识别其漏洞，攻击成功率更高。

    

    在强化学习中，对抗策略可以通过训练对抗代理来最小化目标代理的奖励来开发。之前的研究研究了黑盒版本的这些攻击，其中对手仅观察世界状态，并将目标代理视为环境的任何其他部分。然而，这并没有考虑问题中的附加结构。在这项工作中，我们从白盒攻击的文献中获得灵感，以训练更有效的对抗策略。我们研究了白盒对抗策略，并显示访问目标代理的内部状态可以用于识别其漏洞。我们做出了两个贡献。(1)我们介绍了白盒对抗策略，其中攻击者在每个时间步观察目标的内部状态和世界状态。我们制定了使用这些策略攻击2人游戏和生成文本语言模型中的代理的方法。(2)我们证明了与黑盒攻击相比，这些策略可以实现更高的攻击成功率，特别是当目标代理的内部状态比较复杂时。

    In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we take inspiration from the literature on white-box attacks to train more effective adversarial policies. We study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can ach
    
[^189]: 早期心脏疾病预测的混合量子分类方法

    Early heart disease prediction using hybrid quantum classification. (arXiv:2208.08882v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2208.08882](http://arxiv.org/abs/2208.08882)

    本文提出了两种量子机器学习方法，分别适用于高维和低维问题，并在Cleveland和Statlog数据集上实验表明这些方法更适合于早期心脏疾病预测，可获得高达96.43％和97.78％的曲线下面积。

    

    心脏发病率和心脏死亡率的增加明显影响了全球公共健康和世界经济。早期预测对于减少心脏发病率和死亡率至关重要。本文提出了两种量子机器学习方法：混合量子神经网络和混合随机森林量子神经网络，用于早期检测心脏疾病。这些方法在Cleveland和Statlog数据集上应用。结果表明，混合量子神经网络和混合随机森林量子神经网络分别适用于高维和低维问题。混合量子神经网络对异常数据敏感，而混合随机森林对异常数据具有鲁棒性。与不同机器学习方法的比较表明，所提出的量子方法更适合早期心脏疾病预测，在Cleveland和Statlog数据集上分别获得了96.43％和97.78％的曲线下面积。

    The rate of heart morbidity and heart mortality increases significantly which affect the global public health and world economy. Early prediction of heart disease is crucial for reducing heart morbidity and mortality. This paper proposes two quantum machine learning methods i.e. hybrid quantum neural network and hybrid random forest quantum neural network for early detection of heart disease. The methods are applied on the Cleveland and Statlog datasets. The results show that hybrid quantum neural network and hybrid random forest quantum neural network are suitable for high dimensional and low dimensional problems respectively. The hybrid quantum neural network is sensitive to outlier data while hybrid random forest is robust on outlier data. A comparison between different machine learning methods shows that the proposed quantum methods are more appropriate for early heart disease prediction where 96.43% and 97.78% area under curve are obtained for Cleveland and Statlog dataset respect
    
[^190]: 研究模型宽度和密度对标签噪声下泛化性能的影响

    Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise. (arXiv:2208.08003v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08003](http://arxiv.org/abs/2208.08003)

    本文研究发现标签噪声会导致双丘降曲线出现“最终上升”，即在足够大的噪声样本比率下，中等宽度下实现最佳泛化性能。随机丢弃可训练参数来减少密度可在标签噪声下改善泛化性能。

    

    扩大过参数化神经网络的规模是实现最先进性能的关键。这是通过双丘降现象捕捉的，其中测试损失随着模型宽度的增加呈现出降低-增加-降低的模式。然而，标签噪声对测试损失曲线的影响尚未被充分探索。在本文中，我们揭示了一个有趣的现象，即标签噪声导致原本观察到的双丘降曲线出现了“最终上升”。具体而言，在足够大的噪声样本比率下，中等宽度下实现最佳泛化性能。通过理论分析，我们将这种现象归因于标签噪声引起的测试损失方差形状转换。此外，我们将最终上升现象扩展到模型密度，并提供了第一个理论表征，表明随机丢弃可训练参数来减少密度可在标签噪声下改善泛化性能。

    Increasing the size of overparameterized neural networks has been a key in achieving state-of-the-art performance. This is captured by the double descent phenomenon, where the test loss follows a decreasing-increasing-decreasing pattern as model width increases. However, the effect of label noise on the test loss curve has not been fully explored. In this work, we uncover an intriguing phenomenon where label noise leads to a \textit{final ascent} in the originally observed double descent curve. Specifically, under a sufficiently large noise-to-sample-size ratio, optimal generalization is achieved at intermediate widths. Through theoretical analysis, we attribute this phenomenon to the shape transition of test loss variance induced by label noise. Furthermore, we extend the final ascent phenomenon to model density and provide the first theoretical characterization showing that reducing density by randomly dropping trainable parameters improves generalization under label noise. We also t
    
[^191]: 你的公平模型有多稳健？探索不同公平策略的稳健性。

    How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies. (arXiv:2207.04581v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04581](http://arxiv.org/abs/2207.04581)

    本文提出了一个新的标准来衡量公平优化策略的稳健性——稳健比率，并使用三种公平策略在五个公平数据集上进行了多次广泛的实验。结果表明，公平策略的稳健性在不同数据集之间和不同公平性定义之间存在显着差异。

    

    随着机器学习在高风险决策中的应用，确保算法公平性已成为一个愈发重要的问题。为此，已经提出了许多数学上的公平性定义，并开发了各种优化技术，旨在最大化定义的公平性概念。然而，公平解决方案依赖于训练数据的质量，而且对噪声非常敏感。最近的研究表明，稳健性（模型在未知数据上表现良好的能力）在应对新问题时应使用的策略类型中起着重要作用，因此衡量这些策略的稳健性已成为一个基本问题。因此，在本文中，我们提出了一个新的标准来衡量各种公平优化策略的稳健性 - 稳健比率。我们使用三种最常用的公平策略对五个基准公平数据集进行了多次广泛的实验，并表明这些策略的稳健性在数据集之间和不同公平性定义之间存在显着差异。我们的结果表明，在设计和选择公平算法时，应更加谨慎地考虑稳健性，以确保它们在实际场景中始终有效可靠。

    With the introduction of machine learning in high-stakes decision making, ensuring algorithmic fairness has become an increasingly important problem to solve. In response to this, many mathematical definitions of fairness have been proposed, and a variety of optimisation techniques have been developed, all designed to maximise a defined notion of fairness. However, fair solutions are reliant on the quality of the training data, and can be highly sensitive to noise. Recent studies have shown that robustness (the ability for a model to perform well on unseen data) plays a significant role in the type of strategy that should be used when approaching a new problem and, hence, measuring the robustness of these strategies has become a fundamental problem. In this work, we therefore propose a new criterion to measure the robustness of various fairness optimisation strategies - the robustness ratio. We conduct multiple extensive experiments on five bench mark fairness data sets using three of 
    
[^192]: 使用神经网络进行基于新颖性的测试选择以加速功能覆盖闭合

    Using Neural Networks for Novelty-based Test Selection to Accelerate Functional Coverage Closure. (arXiv:2207.00445v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2207.00445](http://arxiv.org/abs/2207.00445)

    本文提出了一个基于神经网络的新颖测试选择框架，可显著加速覆盖闭合，三个测试配置明显优于随机测试，最大节省率达49.37%。

    

    已经证明，在基于仿真的验证中使用新颖的测试选择器可以显著加速覆盖闭合，无论覆盖洞的数量如何。本文提出了一个配置灵活且高度自动化的基于神经网络的新颖测试选择框架。使用商业信号处理单元测试了框架的三个配置。所有三个配置都明显优于随机测试选择，最大的仿真节省率为49.37%，覆盖率达到99.5%。与仿真减少量相比，配置的计算开销可以忽略不计。我们比较了实验结果并讨论了与配置性能相关的重要特征。

    Novel test selectors used in simulation-based verification have been shown to significantly accelerate coverage closure regardless of the number of coverage holes. This paper presents a configurable and highly-automated framework for novel test selection based on neural networks. Three configurations of this framework are tested with a commercial signal processing unit. All three convincingly outperform random test selection with the largest saving of simulation being 49.37% to reach 99.5% coverage. The computational expense of the configurations is negligible compared to the simulation reduction. We compare the experimental results and discuss important characteristics related to the performance of the configurations.
    
[^193]: HRFuser: 一种用于二维物体检测的多分辨率传感器融合架构

    HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection. (arXiv:2206.15157v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.15157](http://arxiv.org/abs/2206.15157)

    HRFuser是一种多分辨率传感器融合架构，可用于二维物体检测，基于高分辨率网络和新型多窗口交叉注意力块进行多模态多分辨率融合。在nuScenes和DENSE上的实验证明了其有效性。

    

    自动驾驶车辆除了标准相机外，通常还包括多个其他传感器，如激光雷达和雷达，这些传感器帮助获取感知驾驶场景的更丰富信息。虽然最近有几项工作着重于将某些传感器对进行融合，如相机与激光雷达或雷达，但缺乏一种通用且模块化的传感器融合架构。在本文中，我们提出了HRFuser，一种多模态二维物体检测的模块化架构。它以多分辨率方式融合多个传感器并可扩展到任意数量的输入模式。HRFuser的设计基于用于仅图像密集预测的最先进高分辨率网络，并采用新型的多窗口交叉注意力块作为执行多模态多分辨率融合的手段。通过对nuScenes和恶劣条件DENSE进行的大量实验，我们证明了HRFuser的有效性。

    Besides standard cameras, autonomous vehicles typically include multiple additional sensors, such as lidars and radars, which help acquire richer information for perceiving the content of the driving scene. While several recent works focus on fusing certain pairs of sensors - such as camera with lidar or radar - by using architectural components specific to the examined setting, a generic and modular sensor fusion architecture is missing from the literature. In this work, we propose HRFuser, a modular architecture for multi-modal 2D object detection. It fuses multiple sensors in a multi-resolution fashion and scales to an arbitrary number of input modalities. The design of HRFuser is based on state-of-the-art high-resolution networks for image-only dense prediction and incorporates a novel multi-window cross-attention block as the means to perform fusion of multiple modalities at multiple resolutions. We demonstrate via extensive experiments on nuScenes and the adverse conditions DENSE
    
[^194]: 基于主体的差分隐私在联邦学习中的应用

    Subject Granular Differential Privacy in Federated Learning. (arXiv:2206.03617v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03617](http://arxiv.org/abs/2206.03617)

    本文提出了两个新算法，可以在每个联邦用户本地实现主体级别的差分隐私保护。用户级别的局部差分隐私可以自然地保证主体级别的差分隐私，并通过真实世界数据集上的实验验证了算法的有效性。

    

    本文考虑在联邦学习设置中实现主体级别的隐私保护。主体是一个个体，其私有信息由单个联邦用户内部或分布在多个联邦用户之间的多个数据项所体现。我们提出了两个新算法，可以在每个联邦用户本地实现主体级别的差分隐私保护。第一个算法称为LocalGroupDP，是在流行的DP-SGD算法中应用组差分隐私的直接应用。我们的第二个算法是基于一种新颖的想法——针对参加训练迷你批次的主体使用分层渐进平均梯度（HiGradAvgDP）。我们还证明了用户级别的局部差分隐私可以自然地保证主体级别的差分隐私。我们观察到，FL中主体级隐私损失的横向组合问题——各个用户产生的主体级隐私损失在联邦中进行组合。我们正式证明了我们算法对主体级差分隐私的保证，并通过真实世界数据集上的实验证明了其有效性。

    This paper considers subject level privacy in the FL setting, where a subject is an individual whose private information is embodied by several data items either confined within a single federation user or distributed across multiple federation users. We propose two new algorithms that enforce subject level DP at each federation user locally. Our first algorithm, called LocalGroupDP, is a straightforward application of group differential privacy in the popular DP-SGD algorithm. Our second algorithm is based on a novel idea of hierarchical gradient averaging (HiGradAvgDP) for subjects participating in a training mini-batch. We also show that user level Local Differential Privacy (LDP) naturally guarantees subject level DP. We observe the problem of horizontal composition of subject level privacy loss in FL - subject level privacy loss incurred at individual users composes across the federation. We formally prove the subject level DP guarantee for our algorithms, and also show their effe
    
[^195]: 神经协方差SDE：初始化时具有无限深度和宽度的网络的形状。

    The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization. (arXiv:2206.02768v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.02768](http://arxiv.org/abs/2206.02768)

    本文研究了前馈神经网络初始化时的随机协方差矩阵分布，发现对激活函数进行形状塑造可以使协方差矩阵是非退化的，而随机协方差矩阵受到神经协方差SDE的随机微分方程的控制。

    

    在初始化时，前馈神经网络的logit输出在给定由次表层定义的随机协方差矩阵的条件下是条件高斯分布的。本文研究了这种随机矩阵的分布。最近的研究表明，当网络深度增加时，对激活函数进行形状塑造是必要的，以使得这个协方差矩阵是非退化的。然而，当前无限宽度样式的这种理解在大深度时存在不足：无限宽度分析忽略了从层到层的微观波动，但这些波动在许多层上积累。为了克服这个问题，我们研究了由形状塑造的无限深度和宽度极限中的随机协方差矩阵。我们确定了到达非平凡极限所需的激活函数的精确缩放，并表明随机协方差矩阵受到我们称之为神经协方差SDE的随机微分方程的控制。使用模拟，我们证明了我们对神经协方差SDE的理解是准确的。

    The logit outputs of a feedforward neural network at initialization are conditionally Gaussian, given a random covariance matrix defined by the penultimate layer. In this work, we study the distribution of this random matrix. Recent work has shown that shaping the activation function as network depth grows large is necessary for this covariance matrix to be non-degenerate. However, the current infinite-width-style understanding of this shaping method is unsatisfactory for large depth: infinite-width analyses ignore the microscopic fluctuations from layer to layer, but these fluctuations accumulate over many layers.  To overcome this shortcoming, we study the random covariance matrix in the shaped infinite-depth-and-width limit. We identify the precise scaling of the activation function necessary to arrive at a non-trivial limit, and show that the random covariance matrix is governed by a stochastic differential equation (SDE) that we call the Neural Covariance SDE. Using simulations, w
    
[^196]: AdaProp：基于图神经网络的知识图谱推理中学习自适应传播

    AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning. (arXiv:2205.15319v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15319](http://arxiv.org/abs/2205.15319)

    本文提出了一种名为AdaProp的方法，该方法可自适应地过滤掉不相关的实体，同时保留有前途的目标，从而高效、强大地进行知识图谱推理。

    

    由于图神经网络（GNN）的广泛应用，已经有许多基于GNN的方法被设计用于知识图谱(KG)推理。GNN-based KG推理方法中一个重要的组成部分是传播路径，它包含每个传播步骤中所涉及的一组实体。现有方法使用手工设计的传播路径，忽略了实体与查询关系之间的相关性。此外，在更大的传播步骤中，涉及的实体数量会爆炸性增长。本文旨在学习一个自适应的传播路径，以过滤掉不相关的实体，同时保留有前途的目标。首先，我们设计了一种增量采样机制，它能够以线性复杂度保留附近目标和分层连接。其次，我们设计了一个基于学习的采样分布来识别语义相关的实体。大量实验证明，我们的方法强大、高效，并且是语义感知的。

    Due to the popularity of Graph Neural Networks (GNNs), various GNN-based methods have been designed to reason on knowledge graphs (KGs). An important design component of GNN-based KG reasoning methods is called the propagation path, which contains a set of involved entities in each propagation step. Existing methods use hand-designed propagation paths, ignoring the correlation between the entities and the query relation. In addition, the number of involved entities will explosively grow at larger propagation steps. In this work, we are motivated to learn an adaptive propagation path in order to filter out irrelevant entities while preserving promising targets. First, we design an incremental sampling mechanism where the nearby targets and layer-wise connections can be preserved with linear complexity. Second, we design a learning-based sampling distribution to identify the semantically related entities. Extensive experiments show that our method is powerful, efficient, and semantic-awa
    
[^197]: PLAtE: 一个大规模的列表页网络抽取数据集

    PLAtE: A Large-scale Dataset for List Page Web Extraction. (arXiv:2205.12386v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12386](http://arxiv.org/abs/2205.12386)

    这个工作介绍了一个名为PLAtE的大规模列表页网络抽取数据集，用于从产品评论页面中提取商品列表和产品属性。数据集由52,898个项目和156,014个属性组成，是第一个大规模的列表页网络抽取数据集。

    

    最近，神经模型被利用来显著提高从半结构化网站中提取信息的性能。然而，继续进步的障碍是训练这些模型的数据集数量太少。在这项工作中，我们介绍了 PLAtE （Pages of Lists Attribute Extraction）基准数据集作为一个具有挑战性的新网络抽取任务。PLAtE 主要关注购物数据，特别是从包含多个项目的产品评论页面中提取，包含两个任务：（1）查找产品列表分割边界和（2）提取每个产品的属性。PLAtE由来自6,694个页面的52,898个项目和156,014个属性组成，是第一个大规模的列表页网络抽取数据集。我们使用多阶段方法来收集和注释数据集，并将三个最先进的网络抽取模型适应于两个任务，定量和定性比较它们的优缺点。

    Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product-list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52, 898 items collected from 6, 694 pages and 156, 014 attributes, making it the first largescale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively.
    
[^198]: 基于上下文BERT调整的阅读理解自动评分模型

    Automated Scoring for Reading Comprehension via In-context BERT Tuning. (arXiv:2205.09864v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09864](http://arxiv.org/abs/2205.09864)

    本文介绍了一种基于预训练语言模型的文本表示和上下文BERT微调的阅读理解自动评分模型，解决了单个问题/题目模型无法利用题目间关联性以及存储模型困难的问题。

    

    自动评分模型有着极大的潜力可以降低人工评分的成本。最近的一些自动评分方法利用了基于预训练语言模型（例如BERT和GPT）的文本表示作为评分模型的输入。然而，大多数方法为每个问题/题目训练一个单独的模型，这适用于试题种类千差万别的作文评分等场景。但是这种方法存在两个问题：1）在阅读理解等与多个问题/题目相关的场景下，无法利用题目之间的关联性；2）当模型参数数量庞大时，存储每个题目独立模型变得很困难。本文介绍了我们在国家教育进步评估（NAEP）阅读理解自动评分挑战赛中获得的（一等奖）解决方案。我们的方法——基于上下文BERT微调，产生一个共用的评分器。

    Automated scoring of open-ended student responses has the potential to significantly reduce human grader effort. Recent advances in automated scoring often leverage textual representations based on pre-trained language models such as BERT and GPT as input to scoring models. Most existing approaches train a separate model for each item/question, which is suitable for scenarios such as essay scoring where items can be quite different from one another. However, these approaches have two limitations: 1) they fail to leverage item linkage for scenarios such as reading comprehension where multiple items may share a reading passage; 2) they are not scalable since storing one model per item becomes difficult when models have a large number of parameters. In this paper, we report our (grand prize-winning) solution to the National Assessment of Education Progress (NAEP) automated scoring challenge for reading comprehension. Our approach, in-context BERT fine-tuning, produces a single shared scor
    
[^199]: 低压负荷伯恩斯坦多项式归一化流的短期密度预测

    Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows. (arXiv:2204.13939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.13939](http://arxiv.org/abs/2204.13939)

    本文提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，用于短期低压负荷预测，相比传统方法表现更好，可用于规划和运营低碳能源系统。

    This paper proposes a flexible conditional density forecasting method based on Bernstein polynomial normalizing flows for short-term low-voltage load forecasting, which outperforms traditional methods and can be used for planning and operating low-carbon energy systems.

    实现全面可再生能源电网的转型需要更好地预测低压水平的需求，以提高效率并确保可靠的控制。然而，高波动性和不断增加的电气化导致巨大的预测变异性，这在传统的点估计中没有反映出来。概率负载预测考虑未来的不确定性，因此允许更明智的决策，用于规划和运营低碳能源系统。我们提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，其中神经网络控制流的参数。在一项包括363个智能电表客户的实证研究中，我们的密度预测与高斯和高斯混合密度相比表现出优势。此外，对于两种不同的神经网络架构，它们在24小时前的负载预测中优于基于针球损失的非参数方法。

    The transition to a fully renewable energy grid requires better forecasting of demand at the low-voltage level to increase efficiency and ensure reliable control. However, high fluctuations and increasing electrification cause huge forecast variability, not reflected in traditional point estimates. Probabilistic load forecasts take future uncertainties into account and thus allow more informed decision-making for the planning and operation of low-carbon energy systems. We propose an approach for flexible conditional density forecasting of short-term load based on Bernstein polynomial normalizing flows, where a neural network controls the parameters of the flow. In an empirical study with 363 smart meter customers, our density predictions compare favorably against Gaussian and Gaussian mixture densities. Also, they outperform a non-parametric approach based on the pinball loss for 24h-ahead load forecasting for two different neural network architectures.
    
[^200]: 双线性价值网络

    Bilinear value networks. (arXiv:2204.13695v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.13695](http://arxiv.org/abs/2204.13695)

    提出了一种通过点积低秩近似来表示Q值的双线性分解方法，其中第一个向量场捕捉状态的局部动态，第二个部分捕捉当前状态和目标之间的全局关系，该方法能够显著提高数据效率，并具有很好的泛化性能。

    

    多目标强化学习的主流框架涉及估计目标条件下的Q值函数。当学习实现多个不同目标时，数据效率与Q函数对新目标的泛化密切相关。目前的范式是使用单一的神经网络来近似Q(s, a, g)。为了改进Q函数的泛化，我们提出了一种双线性分解方法，通过两个向量场之间的点积的低秩近似来表示Q值。第一个向量场f(s, a)捕捉状态s处的环境局部动态；而第二个部分{ϕ}(s, g)则捕捉当前状态和目标之间的全局关系。我们展示了双线性分解方案显著提高了数据效率，相比先前的方法，对于处于分布范围之外的目标具有更好的转移性。我们在模拟的Fetch机器人任务套件和DeepMind Control Suite上提供了实证证据。

    The dominant framework for off-policy multi-goal reinforcement learning involves estimating goal conditioned Q-value function. When learning to achieve multiple goals, data efficiency is intimately connected with the generalization of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks. To improve the generalization of the Q-function, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, {\phi}(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme substantially improves data efficiency, and has superior transfer to out-of-distribution goals compared to prior methods. Empirical evidence is provided on the simulated Fetch robot task-suite and d
    
[^201]: 利用机器学习在急诊科分诊中检测败血症

    Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07657](http://arxiv.org/abs/2204.07657)

    本研究利用机器学习开发出一种检测急诊科分诊前败血症的模型，其性能优于标准败血症筛查算法。

    

    身体器官功能障碍的败血症是全球死亡和危重疾病的主要原因之一。本研究的目的是比较标准败血症筛查算法和在电子健康记录分诊数据上训练的机器学习算法在急诊科分诊前（未使用实验室诊断）的败血症检测性能。研究得出机器学习模型的 AUC 为 0.9423，敏感性为 71.09%。

    Sepsis is a life-threatening condition with organ dysfunction and is a leading cause of death and critical illness worldwide. Even a few hours of delay in the treatment of sepsis results in increased mortality. Early detection of sepsis during emergency department triage would allow early initiation of lab analysis, antibiotic administration, and other sepsis treatment protocols. The purpose of this study was to compare sepsis detection performance at ED triage (prior to the use of laboratory diagnostics) of the standard sepsis screening algorithm (SIRS with source of infection) and a machine learning algorithm trained on EHR triage data. A machine learning model (KATE Sepsis) was developed using patient encounters with triage data from 16participating hospitals. KATE Sepsis and standard screening were retrospectively evaluated on the adult population of 512,949 medical records. KATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of 71.09% (70.12% - 71.98%) and
    
[^202]: 用拓扑激活图来可视化深度神经网络

    Visualizing Deep Neural Networks with Topographic Activation Maps. (arXiv:2204.03528v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03528](http://arxiv.org/abs/2204.03528)

    本文提出使用拓扑激活图来可视化深度神经网络及其决策过程，提高了 DNN 的可解释性。

    

    深度神经网络（DNN）在解决各领域任务中已成为成功工具。然而，DNN的复杂性使得了解其如何解决所学任务变得困难。为了提高 DNN 的可解释性，我们采用神经科学的方法来分析复杂的不透明系统。我们从神经科学如何使用拓扑图可视化脑活动中获取灵感，同样地，我们研究了布置 DNN 层中神经元的技术，使得具有类似激活的神经元在彼此附近。本文中我们介绍了并比较了获得 DNN 层中神经元拓扑结构的方法。此外，我们展示了如何使用拓扑激活图来识别错误或编码偏见，并可视化训练过程。我们的新型可视化技术提高了基于 DNN 的决策系统的透明度。

    Machine Learning with Deep Neural Networks (DNNs) has become a successful tool in solving tasks across various fields of application. However, the complexity of DNNs makes it difficult to understand how they solve their learned task. To improve the explainability of DNNs, we adapt methods from neuroscience that analyze complex and opaque systems. Here, we draw inspiration from how neuroscience uses topographic maps to visualize brain activity. To also visualize activations of neurons in DNNs as topographic maps, we research techniques to layout the neurons in a two-dimensional space such that neurons of similar activity are in the vicinity of each other. In this work, we introduce and compare methods to obtain a topographic layout of neurons in a DNN layer. Moreover, we demonstrate how to use topographic activation maps to identify errors or encoded biases and to visualize training processes. Our novel visualization technique improves the transparency of DNN-based decision-making syste
    
[^203]: 拓扑经验回放

    Topological Experience Replay. (arXiv:2203.15845v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.15845](http://arxiv.org/abs/2203.15845)

    本文提出了一种拓扑经验回放的方法，通过构建图来明确状态的 Q 值之间的依赖关系，解决了传统采样策略忽视状态间依赖关系的问题，提高了学习深度 Q 函数时的性能和准确性。

    

    最先进的深度 Q 学习方法使用从经验重放缓冲区中采样的状态转换元组更新 Q 值。这种策略通常均匀和随机地采样，或基于诸如时间差（TD）误差等度量优先。这样的采样策略在学习 Q 函数时可能效率低下，因为一个状态的 Q 值取决于继承状态的 Q 值。如果数据采样策略忽略了下一个状态的 Q 值估计的精度，它可能会导致无用和常常不正确的 Q 值更新。为了减轻这个问题，我们将智能体的经验组织成一个图，明确跟踪状态的 Q 值之间的依赖关系。图中的每条边代表通过执行单个操作在两个状态之间的转换。我们通过从一组终端状态开始扩展图中的顶点，并逐步向后移动的广度优先搜索来执行值备份。

    State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically sho
    
[^204]: SC2基准测试：面向划分计算的监督式压缩

    SC2 Benchmark: Supervised Compression for Split Computing. (arXiv:2203.08875v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.08875](http://arxiv.org/abs/2203.08875)

    该研究提出了一种基于监督式压缩的划分式计算方案（SC2），以有效地将神经网络计算分配给移动设备和边缘服务器；使用新的度量标准对其进行全面评估，发现在压缩数据大小和性能之间获得了更好的平衡。

    

    随着在移动设备上使用深度学习模型的需求增加，将神经网络计算分配给设备和更强大的边缘服务器已成为一种吸引人的解决方案。但是，现有的划分计算方法通常表现不如对压缩数据进行远程计算的朴素基线。最近的研究提出了学习压缩表示，这些表示包含更多用于监督下游任务的相关信息，展示了在压缩数据大小和受监督性能之间改进的权衡。然而，现有的评估指标只提供了划分计算的不完整图像。本研究介绍了面向划分计算的监督式压缩（SC2），并提出了新的评估标准：最小化移动设备上的计算，最小化传输数据大小，最大化模型准确性。我们使用10种基准方法、三个计算机视觉任务和180多个训练模型进行了全面的基准测试研究，并讨论了各种约束条件下方法的比较。

    With the increasing demand for deep learning models on mobile devices, splitting neural network computation between the device and a more powerful edge server has become an attractive solution. However, existing split computing approaches often underperform compared to a naive baseline of remote computation on compressed data. Recent studies propose learning compressed representations that contain more relevant information for supervised downstream tasks, showing improved tradeoffs between compressed data size and supervised performance. However, existing evaluation metrics only provide an incomplete picture of split computing. This study introduces supervised compression for split computing (SC2) and proposes new evaluation criteria: minimizing computation on the mobile device, minimizing transmitted data size, and maximizing model accuracy. We conduct a comprehensive benchmark study using 10 baseline methods, three computer vision tasks, and over 180 trained models, and discuss vario
    
[^205]: 关于某些神经网络训练问题中虚假局部极小值的普遍存在性研究

    On the Omnipresence of Spurious Local Minima in Certain Neural Network Training Problems. (arXiv:2202.12262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.12262](http://arxiv.org/abs/2202.12262)

    研究了具有一维实数输出且激活函数包含仿射段以及隐藏层至少有两个节点的深层人工神经网络的训练问题损失景观，发现这类问题对于所有不是仿射的目标函数，都存在一系列虚假的局部极小值，这是由通用逼近定理直接推导得出的。

    

    本文研究了具有一维实数输出且激活函数包含仿射段以及隐藏层至少有两个节点的深层人工神经网络的训练问题损失景观。结果表明，对于所有不是仿射的目标函数，这类问题存在一系列虚假的（即非全局最优的）局部极小值。与以往的研究不同，我们的分析涵盖了所有采样和参数化方案，一般可微损失函数，任意连续非多项式激活函数，以及有限维和无限维设置。进一步表明，所考虑的训练问题中虚假局部极小值的出现是通用逼近定理的直接结果，底层机制也导致例如$L^p$-最佳逼近问题对于没有密集图像的所有网络在Hadamard意义下都是不良 posed。后一结果也适用于无限维情况。

    We study the loss landscape of training problems for deep artificial neural networks with a one-dimensional real output whose activation functions contain an affine segment and whose hidden layers have width at least two. It is shown that such problems possess a continuum of spurious (i.e., not globally optimal) local minima for all target functions that are not affine. In contrast to previous works, our analysis covers all sampling and parameterization regimes, general differentiable loss functions, arbitrary continuous nonpolynomial activation functions, and both the finite- and infinite-dimensional setting. It is further shown that the appearance of the spurious local minima in the considered training problems is a direct consequence of the universal approximation theorem and that the underlying mechanisms also cause, e.g., $L^p$-best approximation problems to be ill-posed in the sense of Hadamard for all networks that do not have a dense image. The latter result also holds without 
    
[^206]: 基于重构的组合场景表示学习：综述

    Compositional Scene Representation Learning via Reconstruction: A Survey. (arXiv:2202.07135v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07135](http://arxiv.org/abs/2202.07135)

    这篇论文综述了利用深度神经网络学习组合场景表示并通过重构的各种方法，这些方法能够利用大规模未标记数据，避免昂贵和费时的数据注释。

    

    视觉场景是由视觉概念组成的，并具有组合爆炸的特性。人类能够有效学习各种视觉场景的重要原因是具备组合感知能力，而希望人工智能也具备类似的能力。组合场景表示学习是一项能够实现这种能力的任务。近年来，提出了各种方法，利用深度神经网络学习组合场景表示，并通过重构将这一研究方向推进到了深度学习时代。通过重构进行学习的优势在于可以利用大规模未标记数据，避免昂贵和费时的数据注释。在本综述中，我们首先概述基于重构的深度神经网络组合场景表示学习的当前进展，包括发展历程和现有分类。

    Visual scenes are composed of visual concepts and have the property of combinatorial explosion. An important reason for humans to efficiently learn from diverse visual scenes is the ability of compositional perception, and it is desirable for artificial intelligence to have similar abilities. Compositional scene representation learning is a task that enables such abilities. In recent years, various methods have been proposed to apply deep neural networks, which have been proven to be advantageous in representation learning, to learn compositional scene representations via reconstruction, advancing this research direction into the deep learning era. Learning via reconstruction is advantageous because it may utilize massive unlabeled data and avoid costly and laborious data annotation. In this survey, we first outline the current progress on reconstruction-based compositional scene representation learning with deep neural networks, including development history and categorizations of exi
    
[^207]: 分布式稳健数据合并

    Distributionally Robust Data Join. (arXiv:2202.05797v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05797](http://arxiv.org/abs/2202.05797)

    本论文提出了一个解决数据合并问题的方法，该方法通过最大化所有概率分布上的最大损失，来构建分布式稳健预测器。

    

    假设我们有两个数据集：一个带标签的数据集和一个带有额外辅助特征的未标记数据集。最合理的使用这些数据集构建预测器的方法应该取决于这些数据集是由相同的分布还是不同的分布生成的，以及测试分布与这些分布中的任一分布有多相似。在许多应用中，两个数据集可能遵循不同的分布，但两者都可能接近测试分布。我们介绍了建立一种预测器的问题，该预测器通过将其Wasserstein距离分别为$r_1$和$r_2$的概率分布最大化地减少所有原始特征、辅助特征和二元标签上的最大损失。这可以看作是分布式稳健优化的推广，用于处理数据合并的情况。

    Suppose we are given two datasets: a labeled dataset and unlabeled dataset which also has additional auxiliary features not present in the first dataset. What is the most principled way to use these datasets together to construct a predictor?  The answer should depend upon whether these datasets are generated by the same or different distributions over their mutual feature sets, and how similar the test distribution will be to either of those distributions. In many applications, the two datasets will likely follow different distributions, but both may be close to the test distribution. We introduce the problem of building a predictor which minimizes the maximum loss over all probability distributions over the original features, auxiliary features, and binary labels, whose Wasserstein distance is $r_1$ away from the empirical distribution over the labeled dataset and $r_2$ away from that of the unlabeled dataset. This can be thought of as a generalization of distributionally robust opti
    
[^208]: 并行连续学习用于异构无线网络上动态分布式模型训练

    Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks. (arXiv:2202.02947v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.02947](http://arxiv.org/abs/2202.02947)

    本文提出了并行连续学习（PSL），通过网络、异构性和接近性三个维度的扩展，实现了在无线设备集合上的动态分布式模型训练。

    

    联邦学习(FedL)已成为一种将模型训练分布在一组无线设备上的流行技术，通过设备上的迭代本地更新和服务器上的全局聚合。本文提出并行连续学习（PSL），将FedL架构沿三个维度进行扩展：（i）网络，通过设备间通信实现去中心化协作；（ii）异构性，在三个层次进行解释：（ii-a）学习：PSL考虑到设备上具有不同的小批量大小的异构数量的随机梯度下降迭代；（ii-b）数据：PSL假设数据到达和离开的动态环境中，本地数据集的分布随时间演变，通过新的模型/概念漂移度量来捕获；（ii-c）设备：PSL考虑到具有不同计算和通信能力的设备；（iii）接近性，设备之间以及访问点之间具有不同的距离。

    Federated learning (FedL) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). In this paper, we develop parallel successive learning (PSL), which expands the FedL architecture along three dimensions: (i) Network, allowing decentralized cooperation among the devices via device-to-device (D2D) communications. (ii) Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) Device: PSL considers devices with different computation and communication capabilities. (iii) Proximity, where devices have different distances to each other and the acces
    
[^209]: 高维两层神经网络中随机梯度下降的相图

    Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks. (arXiv:2202.00293v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.00293](http://arxiv.org/abs/2202.00293)

    本文研究了高维两层神经网络中随机梯度下降的相图，探究了窄网络和过参数化浅层网络之间的交界处，并研究了三个方面变量之间的相互作用，工作建立在统计物理的框架下。

    

    虽然非凸优化景观，在过参数化的浅层网络中，梯度下降能够实现全局收敛。但情况对于窄网络却可能完全不同，它们倾向于被困在具有糟糕泛化的局部最小值。在这里，我们研究这两个范畴之间的交界处，特别是我们调查了所谓的平均场/流体力学范畴与Saad和Solla 的开创性方法之间的联系。我们重点研究了在高维随机梯度下降的动态中，学习率、时间尺度和隐含层数量之间的相互作用，以高斯数据为例。我们的工作基于从统计物理学中对于高维度随机梯度下降的确定性描述，我们加以拓展并提供了严密的收敛速率证明。

    Despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. The picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. Here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of Saad & Solla. Focusing on the case of Gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (SGD). Our work builds on a deterministic description of SGD in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates.
    
[^210]: 认知账本项目：通过认知区块链构建个人数字化孪生体

    Cognitive Ledger Project: Towards Building Personal Digital Twins Through Cognitive Blockchain. (arXiv:2201.08163v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2201.08163](http://arxiv.org/abs/2201.08163)

    该论文提出了一个认知数字孪生体的认知架构，即通过基于区块链基础设施的方式将用户的个人数据转化为结构化信息和机器学习模型，最终能够一起组成用户的认知数字孪生体。

    

    认知账本项目旨在开发一个模块化系统，通过基于区块链基础设施的方式将用户的个人数据转化为结构化信息和机器学习模型。在这个正在进行的工作中，我们提出了一个认知数字孪生体的认知架构。所提出的设计在其核心采用了认知区块链（认知账本）。架构包括多个模块，可以将用户在数字环境中的活动转化为可重复使用的知识对象和人工智能，最终能够一起组成用户的认知数字孪生体。

    The Cognitive Ledger Project is an effort to develop a modular system for turning users' personal data into structured information and machine learning models based on a blockchain-based infrastructure. In this work-in-progress paper, we propose a cognitive architecture for cognitive digital twins. The suggested design embraces a cognitive blockchain (Cognitive ledger) at its core. The architecture includes several modules that turn users' activities in the digital environment into reusable knowledge objects and artificial intelligence that one day can work together to form the cognitive digital twin of users.
    
[^211]: 二维电子气体的有效质量$m^\ast$：一项神经规范变换研究

    $m^\ast$ of two-dimensional electron gas: a neural canonical transformation study. (arXiv:2201.03156v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2201.03156](http://arxiv.org/abs/2201.03156)

    本研究开发了一种神经网络方法，直接计算低温下的热熵来提取二维自旋极化电子气体的有效质量$m^\ast$，其揭示了有效质量在低密度强耦合区域的更加明显的抑制作用。该预测需要在实验中进行验证。

    

    相互作用电子的准粒子有效质量$m^\ast$是费米液体理论中的基本数量。然而，经过几十年的研究，均匀电子气体的有效质量的精确值仍然难以确定。新开发的神经规范变换方法提供了一种通过直接计算低温下的热熵来提取电子气体有效质量的原则方法。该方法使用两个生成神经网络对变分多电子密度矩阵进行建模：一个动量占据的自回归模型和一个电子坐标的归一化流模型。我们的计算揭示了二维自旋极化电子气体中有效质量的抑制作用，这比先前在低密度强耦合区域的报告更加明显。这一预测需要在二维电子气体实验中进行验证。

    The quasiparticle effective mass $m^\ast$ of interacting electrons is a fundamental quantity in the Fermi liquid theory. However, the precise value of the effective mass of uniform electron gas is still elusive after decades of research. The newly developed neural canonical transformation approach [Xie et al., J. Mach. Learn. 1, (2022)] offers a principled way to extract the effective mass of electron gas by directly calculating the thermal entropy at low temperature. The approach models a variational many-electron density matrix using two generative neural networks: an autoregressive model for momentum occupation and a normalizing flow for electron coordinates. Our calculation reveals a suppression of effective mass in the two-dimensional spin-polarized electron gas, which is more pronounced than previous reports in the low-density strong-coupling region. This prediction calls for verification in two-dimensional electron gas experiments.
    
[^212]: 绿色自动化机器学习：现状与未来方向

    Towards Green Automated Machine Learning: Status Quo and Future Directions. (arXiv:2111.05850v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.05850](http://arxiv.org/abs/2111.05850)

    本文提出了一个名为 "绿色自动化机器学习" 的概念，旨在使整个 AutoML 过程更加环保。本文重点研究如何量化 AutoML 工具的环境足迹，并总结了有关如何针对其 "绿色性"，即可持续性，设计和基准测试 AutoML 工具的不同策略。

    

    自动化机器学习 (AutoML) 旨在自动配置机器学习算法并将它们组合成一个适用于手头的学习任务 (数据集) 的整体 (软件) 解决方案 - 一个机器学习流水线。在过去的十年中，AutoML 已经发展成了一个独立的研究领域，有着数百篇的相关论文。同时，AutoML 因其高资源消耗而受到批评，因为许多方法依赖于对许多机器学习流水线的 (昂贵的) 评估，以及跨许多数据集和方法的昂贵的大规模实验。本文提出了绿色自动化机器学习 (Green AutoML) 的概念，旨在使整个 AutoML 过程更加环保。因此，我们首先阐述了如何量化 AutoML 工具的环境足迹。接着，总结了有关如何针对其 "绿色性"，即可持续性，设计和基准测试 AutoML 工具的不同策略。最后

    Automated machine learning (AutoML) strives for the automatic configuration of machine learning algorithms and their composition into an overall (software) solution - a machine learning pipeline - tailored to the learning task (dataset) at hand. Over the last decade, AutoML has developed into an independent research field with hundreds of contributions. At the same time, AutoML is being criticised for its high resource consumption as many approaches rely on the (costly) evaluation of many machine learning pipelines, as well as the expensive large scale experiments across many datasets and approaches. In the spirit of recent work on Green AI, this paper proposes Green AutoML, a paradigm to make the whole AutoML process more environmentally friendly. Therefore, we first elaborate on how to quantify the environmental footprint of an AutoML tool. Afterward, different strategies on how to design and benchmark an AutoML tool wrt. their "greenness", i.e. sustainability, are summarized. Finall
    
[^213]: 集中关注潜在命名实体的主动标注获取

    Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.03837](http://arxiv.org/abs/2111.03837)

    本文提出了几个AL句子查询评估函数，关注潜在正面标记，并使用更好的数据驱动的正常化方法，以最小化NER注释成本。

    

    命名实体识别(NER)旨在识别结构化文本中命名实体的提及并将其分类到预定义的命名实体类别中。虽然基于深度学习的预训练语言模型有助于在NER中实现良好的预测性能，但许多特定领域的NER应用仍需要大量标记数据。主动学习(AL)是解决标签获取问题的通用框架，已用于NER任务，以最小化注释成本而不牺牲模型性能。然而，标记的严重不均匀类分布引入了设计有效的NER主动学习查询方法的挑战。我们提出了几个AL句子查询评估函数，更多关注潜在的正面标记，并使用基于句子和标记成本评估策略来评估这些提议的函数。我们还提出了更好的数据驱动的正常化方法，以惩罚过长或过短的句子。

    Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
    
[^214]: QuantumNAT：注重量子噪声的噪声注入、量化和归一化的量子训练

    QuantumNAT: Quantum Noise-Aware Training with Noise Injection, Quantization and Normalization. (arXiv:2110.11331v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.11331](http://arxiv.org/abs/2110.11331)

    QuantumNAT是一个PQC特定框架，可以在训练和推断阶段执行噪声感知优化，提高鲁棒性，缓解量子噪声

    

    参数化量子电路是实现近期量子硬件优势的有希望方法。然而，由于存在较大的量子噪声（误差），在实际的量子设备上，PQC模型的性能会受到严重的降级。我们提出了QuantumNAT，一个可以在训练和推断阶段执行噪声感知优化的PQC特定框架，以提高其鲁棒性。通过实验我们发现，量子噪声对PQC测量结果的影响是从无噪声结果经过一个缩放和偏移因子得到的线性映射。基于此，我们提出了后测量归一化来缓解特征分布不一致的问题。

    Parameterized Quantum Circuits (PQC) are promising towards quantum advantage on near-term quantum hardware. However, due to the large quantum noises (errors), the performance of PQC models has a severe degradation on real quantum devices. Take Quantum Neural Network (QNN) as an example, the accuracy gap between noise-free simulation and noisy results on IBMQ-Yorktown for MNIST-4 classification is over 60%. Existing noise mitigation methods are general ones without leveraging unique characteristics of PQC; on the other hand, existing PQC work does not consider noise effect. To this end, we present QuantumNAT, a PQC-specific framework to perform noise-aware optimizations in both training and inference stages to improve robustness. We experimentally observe that the effect of quantum noise to PQC measurement outcome is a linear map from noise-free outcome with a scaling and a shift factor. Motivated by that, we propose post-measurement normalization to mitigate the feature distribution di
    
[^215]: 基于最大似然的OOD检测中的熵问题

    Entropic Issues in Likelihood-Based OOD Detection. (arXiv:2109.10794v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.10794](http://arxiv.org/abs/2109.10794)

    本文研究了最大似然训练的深度生成模型的OOD检测问题，提出了一种新的观察角度，即将平均似然分解为KL散度项和熵项。后者可以解释模型可能会给OOD数据高似然值的现象，因为它抑制具有更高熵的数据集上的似然值。

    

    最大似然训练的深度生成模型仍然是关于数据概率推理的流行方法。然而，观察到它们可能会分配比正向分布数据更高的可能性，因此质疑这些似然值的含义。本文提供了一种新的观察角度，将平均似然分解为KL散度项和熵项。我们认为后者可以解释上述奇怪的OOD行为，抑制具有更高熵的数据集上的似然值。虽然我们的思路很简单，但我们还没有看到它在文献中得到探讨。这种分析进一步解释了基于似然比的OOD检测方法成功的原因，因为问题熵项在期望中会抵消。最后，我们讨论了这一观察结果如何与最近在流形支持模型中的OOD检测成功相关。

    Deep generative models trained by maximum likelihood remain very popular methods for reasoning about data probabilistically. However, it has been observed that they can assign higher likelihoods to out-of-distribution (OOD) data than in-distribution data, thus calling into question the meaning of these likelihood values. In this work we provide a novel perspective on this phenomenon, decomposing the average likelihood into a KL divergence term and an entropy term. We argue that the latter can explain the curious OOD behaviour mentioned above, suppressing likelihood values on datasets with higher entropy. Although our idea is simple, we have not seen it explored yet in the literature. This analysis provides further explanation for the success of OOD detection methods based on likelihood ratios, as the problematic entropy term cancels out in expectation. Finally, we discuss how this observation relates to recent success in OOD detection with manifold-supported models, for which the above
    
[^216]: 无需信任的私有联邦学习：凸损失函数的最优算法

    Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses. (arXiv:2106.09779v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.09779](http://arxiv.org/abs/2106.09779)

    本文研究了无需信任服务器或其他数据源的跨 silo 联邦学习，考虑了跨 silo 记录级差分隐私 ISRL-DP。该算法可以确保来自每个人的数据都不会被泄漏。

    

    本文探讨了联邦学习（FL）的研究，特别是跨数据源（跨 silo）FL，这些数据源的数据主人都不信任服务器或其他 silos。在这种情况下，每个数据源（例如医院）都有来自不同人（例如患者）的数据，并且必须维护每个人（例如医疗记录）数据的隐私，即使服务器或其他数据源是恶意监听者。这种要求促进了对跨 silo 记录级差分隐私（ISRL-DP）的研究，它要求 silo i 的通信满足记录 / 项目级差分隐私 (DP)。ISRL-DP 确保 silo i 中每个人（例如患者）的数据都不会泄漏。ISRL-DP 不同于各种已有的隐私概念。中心和用户级差分隐私假定人们信任服务器/其他数据源。在极端情况下，本地DP 假定人们根本不信任任何人（甚至是他们自己的数据源）。ISRL-DP 处于中心和本地DP 之间，使得在跨 silo 的真实情况下具有现实意义。

    This paper studies federated learning (FL)--especially cross-silo FL--with data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) has data from different people (e.g. patients) and must maintain the privacy of each person's data (e.g. medical record), even if the server or other silos act as adversarial eavesdroppers. This requirement motivates the study of Inter-Silo Record-Level Differential Privacy (ISRL-DP), which requires silo i's communications to satisfy record/item-level differential privacy (DP). ISRL-DP ensures that the data of each person (e.g. patient) in silo i (e.g. hospital i) cannot be leaked. ISRL-DP is different from well-studied privacy notions. Central and user-level DP assume that people trust the server/other silos. On the other end of the spectrum, local DP assumes that people do not trust anyone at all (even their own silo). Sitting between central and local DP, ISRL-DP makes the realistic assumption (in cross-sil
    
[^217]: 正则化约束下的自编码机

    Autoencoding Under Normalization Constraints. (arXiv:2105.05735v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.05735](http://arxiv.org/abs/2105.05735)

    本文介绍了一种基于自编码机的规范化概率模型，称为规范化自编码机（NAE）。该模型通过抑制负样本的重构来强制执行正则化，有效提高了异常检测性能。

    

    概率似然是一种标准的异常检测估计方法。我们提出一种基于自编码机的标准化概率模型，称为规范化自编码机（NAE）。NAE的概率密度是通过对自编码机的重构误差进行定义的，该误差在传统的基于能量的模型中被不同地定义。在我们的模型中，通过抑制负样本的重构来强制执行正则化，从而显着提高了异常检测性能。我们的实验结果确认NAE的有效性，不仅可以检测异常值，还可以生成分布内的样本。

    Likelihood is a standard estimate for outlier detection. The specific role of the normalization constraint is to ensure that the out-of-distribution (OOD) regime has a small likelihood when samples are learned using maximum likelihood. Because autoencoders do not possess such a process of normalization, they often fail to recognize outliers even when they are obviously OOD. We propose the Normalized Autoencoder (NAE), a normalized probabilistic model constructed from an autoencoder. The probability density of NAE is defined using the reconstruction error of an autoencoder, which is differently defined in the conventional energy-based model. In our model, normalization is enforced by suppressing the reconstruction of negative samples, significantly improving the outlier detection performance. Our experimental results confirm the efficacy of NAE, both in detecting outliers and in generating in-distribution samples.
    
[^218]: 鲁棒的样本加权方法以便为目标人群学习个体化治疗规则

    Robust Sample Weighting to Facilitate Individualized Treatment Rule Learning for a Target Population. (arXiv:2105.00581v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.00581](http://arxiv.org/abs/2105.00581)

    本文研究了个体化治疗规则的泛化问题，提出一种加权框架方法以缓解因规定的函数类不包含最优规则而导致的影响。

    

    学习个体化治疗规则对于精准医疗至关重要。当前文献主要关注从单个来源人群中导出个体化治疗规则。然而，当来源人群与目标人群不同时，我们需要考虑观察数据设置。与平均治疗效应的因果推广不同，个体化治疗规则的推广由于需要基于一个预先指定的函数类对规则进行建模和推广，因此面临着新的挑战，该函数类可能不包含无约束真正最优个体化治疗规则。本文旨在开发一种加权框架，以缓解这种错误规定的影响，从而促进从来源人群到目标人群的最优个体化治疗规则的泛化。我们的方法寻求在由再生核希尔伯特空间表征的非参数函数类上进行协变量平衡，并且可以改进许多依赖于权重的个体化治疗规则学习方法。我们证明了样本加权估计的一致性。

    Learning individualized treatment rules (ITRs) is an important topic in precision medicine. Current literature mainly focuses on deriving ITRs from a single source population. We consider the observational data setting when the source population differs from a target population of interest. Compared with causal generalization for the average treatment effect which is a scalar quantity, ITR generalization poses new challenges due to the need to model and generalize the rules based on a prespecified class of functions which may not contain the unrestricted true optimal ITR. The aim of this paper is to develop a weighting framework to mitigate the impact of such misspecification and thus facilitate the generalizability of optimal ITRs from a source population to a target population. Our method seeks covariate balance over a non-parametric function class characterized by a reproducing kernel Hilbert space and can improve many ITR learning methods that rely on weights. We show that the prop
    
[^219]: 一种弱监督模型WARM用于解决数学题

    WARM: A Weakly (+Semi) Supervised Model for Solving Math word Problems. (arXiv:2104.06722v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.06722](http://arxiv.org/abs/2104.06722)

    研究提出了一种弱监督模型WARM来解决用于自然语言处理中的数学题。通过仅用期望答案作为监督，该方法通过学习生成方程来解决问题，并在无需使用方程作为监督的情况下，成功实现了相比最先进的弱监督方法更高的精度提升。

    

    解决数学问题是自然语言处理中的一个重要而具有挑战性的问题。现有的方法需要通过中间方程获得全部监督，而标注每个数学题的坑人代价昂贵。为了解决方程注释的挑战，我们提出了一种弱监督模型，通过仅需要期望答案作为监督来解决数学问题。

    Solving math word problems (MWPs) is an important and challenging problem in natural language processing. Existing approaches to solve MWPs require full supervision in the form of intermediate equations. However, labeling every MWP with its corresponding equations is a time-consuming and expensive task. In order to address this challenge of equation annotation, we propose a weakly supervised model for solving MWPs by requiring only the final answer as supervision. We approach this problem by first learning to generate the equation using the problem description and the final answer, which we subsequently use to train a supervised MWP solver. We propose and compare various weakly supervised techniques to learn to generate equations directly from the problem description and answer. Through extensive experiments, we demonstrate that without using equations for supervision, our approach achieves accuracy gains of 4.5% and 32% over the state-of-the-art weakly supervised approach, on the stan
    
[^220]: 利用背景音素类信息提高语音增强性能

    Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information. (arXiv:2011.07442v4 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2011.07442](http://arxiv.org/abs/2011.07442)

    本文提出了一种新的语音增强方法，通过利用广义语音类别序列的损失来提高SE性能，实验证明上下文BPC信息可以提高性能。

    

    先前的研究证实，通过用发音的位置和方式特征增强声学特征，语音增强(SE)过程可以指导考虑输入语音的广义语音属性，以获得性能改进。在本文中，我们探索了结构属性的背景信息，作为进一步受益的SE的附加信息。更具体地，我们建议通过利用端到端自动语音识别(E2E-ASR)模型中预测的广义语音类别序列的损失来提高SE性能。我们还开发了多目标训练，利用ASR和感知损失基于基于BPC的E2E-ASR来训练SE系统。来自语音降噪，语音去混响和受损语音增强任务的实验结果表明，上下文BPC信息可以提高SE性能。此外，使用基于BPC的E2E-ASR训练的SE模型胜过了其他基线模型。

    Previous studies have confirmed that by augmenting acoustic features with the place/manner of articulatory features, the speech enhancement (SE) process can be guided to consider the broad phonetic properties of the input speech when performing enhancement to attain performance improvements. In this paper, we explore the contextual information of articulatory attributes as additional information to further benefit SE. More specifically, we propose to improve the SE performance by leveraging losses from an end-to-end automatic speech recognition (E2E-ASR) model that predicts the sequence of broad phonetic classes (BPCs). We also developed multi-objective training with ASR and perceptual losses to train the SE system based on a BPC-based E2E-ASR. Experimental results from speech denoising, speech dereverberation, and impaired speech enhancement tasks confirmed that contextual BPC information improves SE performance. Moreover, the SE model trained with the BPC-based E2E-ASR outperforms th
    
[^221]: 研究在数据依赖性下的成员推断攻击

    Investigating Membership Inference Attacks under Data Dependencies. (arXiv:2010.12112v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2010.12112](http://arxiv.org/abs/2010.12112)

    本文研究了在数据具有依赖性的情况下成员推断攻击的影响，并表明了DP无法在这种情况下提供有意义的保护，需要探索替代的防御策略来提供更强隐私保证。

    

    在隐私敏感数据上训练机器学习模型已经成为一种流行的实践，推动着不断扩大的领域中的创新。这打开了新的攻击方式，可能会带来严重的隐私影响。一种这样的攻击是成员推断攻击（MIA），它揭示了特定数据点是否被用于训练模型。越来越多的文献使用差分隐私（DP）训练算法作为防御这种攻击的手段。然而，这些工作在评估防御策略时都使用了一种限制性假设，即训练集和非成员独立且满足相同分布。这种假设在很多现实应用中并不成立。在这种背景下，本文研究了样本之间的统计依赖性对成员推断的影响，并解释了为什么在这种更普遍的情况下DP不能提供有意义的保护（隐私参数$\epsilon$随训练集大小$n$的增加而增加）。我们进行了一系列实验证明了DP在数据依赖性下对MIA的脆弱性，突显出需要寻找可提供更强隐私保证的替代防御策略。

    Training machine learning models on privacy-sensitive data has become a popular practice, driving innovation in ever-expanding fields. This has opened the door to new attacks that can have serious privacy implications. One such attack, the Membership Inference Attack (MIA), exposes whether or not a particular data point was used to train a model. A growing body of literature uses Differentially Private (DP) training algorithms as a defence against such attacks. However, these works evaluate the defence under the restrictive assumption that all members of the training set, as well as non-members, are independent and identically distributed. This assumption does not hold for many real-world use cases in the literature. Motivated by this, we evaluate membership inference with statistical dependencies among samples and explain why DP does not provide meaningful protection (the privacy parameter $\epsilon$ scales with the training set size $n$) in this more general case. We conduct a series
    
[^222]: 跨类别变换的鲁棒性：鲁棒性是否由不变神经表示驱动？

    Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?. (arXiv:2007.00112v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2007.00112](http://arxiv.org/abs/2007.00112)

    本文通过观察不同种类的图像转换后深度卷积神经网络（DCNNs）的表现，探讨了不变神经表示是否促进了跨类别的图像鲁棒性。实验表明，跨类别变换的鲁棒性是由不变表示所促进的，支持鲁棒性是由不变性驱动而不是专门的子网络的假设。

    

    深度卷积神经网络（DCNNs）已经证明在识别物体在变换下的鲁棒性方面取得了令人印象深刻的结果（例如模糊或噪音），当这些变换被包含在训练集中时。一个解释这种鲁棒性的假设是，DCNNs发展出的不变神经表示在图像转换时不发生改变。然而，这个假设的真实程度是一个尚未解决的问题，因为鲁棒性可能是通过与不变性不同的特性实现的，例如，网络的某些部分可能专门用于识别转换或非转换的图像。本文通过利用不变神经表示促进对训练集之外的变换的鲁棒性，研究了不变神经表示出现的条件。具体而言，我们分析了一种训练范式，在该训练范式中，只有一些对象类别在训练期间被变换，然后评价DCNN是否对所有类别的变换具有鲁棒性，包括那些在训练期间从未见过的类别的变换。我们的实验表明，跨类别变换的鲁棒性是由不变表示所促进的，支持鲁棒性是由不变性驱动而不是专门的子网络的假设。

    Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (eg. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. However, to what extent this hypothesis holds true is an outstanding question, as robustness to transformations could be achieved with properties different from invariance, eg. parts of the network could be specialized to recognize either transformed or non-transformed images. This paper investigates the conditions under which invariant neural representations emerge by leveraging that they facilitate robustness to transformations beyond the training distribution. Concretely, we analyze a training paradigm in which only some object categories are seen transformed during training and evaluate whether the DCNN is robust to transformations a
    
[^223]: 我们至少应该能够设计出好的分子结合体

    We Should at Least Be Able to Design Molecules That Dock Well. (arXiv:2006.16955v5 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2006.16955](http://arxiv.org/abs/2006.16955)

    该文提出了一个基于对接的基准测试来评估分子结合蛋白质的流行计算方法，探究了目前基于图形的生成模型在新型药物设计上的局限性，并提出了新的基准测试版本。

    

    设计具有所需特性的化合物是药物发现过程的关键元素。然而，由于缺乏现实的回顾性基准和前瞻性验证的高昂成本，衡量该领域的进展一直是具有挑战性的。为了弥补这一差距，我们提出了一个基于对接的基准测试，这是一种用于评估分子结合蛋白质的流行计算方法。具体而言，目标是生成得分高的药物样分子，这些药物属于SMINA，一种流行的对接软件。我们观察到，在使用真实大小的训练集进行训练后，流行的基于图形的生成模型无法生成具有高结合得分的分子。这表明了当前的模型在新型药物设计中存在局限性。最后，我们提出了基于简化评分函数的基准测试版本，并展示测试模型能够部分解决该问题。我们将该基准测试作为一个易于使用的软件包发布，可在 https://github.com 上获得。

    Designing compounds with desired properties is a key element of the drug discovery process. However, measuring progress in the field has been challenging due to the lack of realistic retrospective benchmarks, and the large cost of prospective validation. To close this gap, we propose a benchmark based on docking, a popular computational method for assessing molecule binding to a protein. Concretely, the goal is to generate drug-like molecules that are scored highly by SMINA, a popular docking software. We observe that popular graph-based generative models fail to generate molecules with a high docking score when trained using a realistically sized training set. This suggests a limitation of the current incarnation of models for de novo drug design. Finally, we propose a simplified version of the benchmark based on a simpler scoring function, and show that the tested models are able to partially solve it. We release the benchmark as an easy to use package available at https://github.com
    
[^224]: Volterra神经网络（VNNs）

    Volterra Neural Networks (VNNs). (arXiv:1910.09616v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1910.09616](http://arxiv.org/abs/1910.09616)

    该论文提出了一种Volterra滤波器启发的神经网络架构，旨在通过输入数据的延迟采样之间的交互引入受控的非线性，以降低卷积神经网络的复杂性。通过串行实现Volterra滤波器，减少了进行与传统神经网络相同的分类任务所需的参数数量。此外，作者还展示了该网络对视频的RGB信息和光流信息进行非线性融合的自适应。

    

    机器学习（ML）中推理的重要性导致了ML的大量提案，特别是深度学习。为了降低卷积神经网络的复杂性，我们提出了一种Volterra滤波器启发的网络架构。该架构通过输入数据的延迟采样之间的交互引入了受控非线性。我们提出了Volterra滤波器的串级实现，以显著降低进行与传统神经网络相同的分类任务所需的参数数量。我们展示了这种Volterra神经网络（VNN）的高效并行实现，以及其在保持相对较简单且可能更易于处理的结构的同时表现出的显著性能。此外，我们展示了这个网络对视频的RGB（空间）信息和光流（时间）信息进行非线性融合的比较复杂的自适应。

    The importance of inference in Machine Learning (ML) has led to an explosive number of different proposals in ML, and particularly in Deep Learning. In an attempt to reduce the complexity of Convolutional Neural Networks, we propose a Volterra filter-inspired Network architecture. This architecture introduces controlled non-linearities in the form of interactions between the delayed input samples of data. We propose a cascaded implementation of Volterra Filtering so as to significantly reduce the number of parameters required to carry out the same classification task as that of a conventional Neural Network. We demonstrate an efficient parallel implementation of this Volterra Neural Network (VNN), along with its remarkable performance while retaining a relatively simpler and potentially more tractable structure. Furthermore, we show a rather sophisticated adaptation of this network to nonlinearly fuse the RGB (spatial) information and the Optical Flow (temporal) information of a video 
    

