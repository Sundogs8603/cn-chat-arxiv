# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation.](http://arxiv.org/abs/2307.05476) | 本文首次将Fisher合并方法应用于序列推荐中，通过合并多个模型的参数来改善整体性能，从而解决了实际挑战，具有推动最新技术的潜力。 |
| [^2] | [AdaptiveRec: Adaptively Construct Pairs for Contrastive Learning in Sequential Recommendation.](http://arxiv.org/abs/2307.05469) | 本文提出了AdaptiveRec，这是一种在顺序推荐中解决对比学习挑战的自适应方法，通过改善嵌入质量和减轻误判问题来提高效果，并在各种推荐场景中展示了其灵活性和适用性的价值。 |
| [^3] | [Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features.](http://arxiv.org/abs/2307.05454) | 这项研究提出了一个形态学感知框架 M2C，可以通过生成测试来评估 NLP 模型在不同语言特征下的行为。研究发现，在英语中，模型在大多数测试中表现出色，但在斯瓦希里语的时间表达和芬兰语的合成所有格等特定类型特征上泛化能力较差。这些结果促使我们开发能够解决这些盲点的模型。 |
| [^4] | [ISLTranslate: Dataset for Translating Indian Sign Language.](http://arxiv.org/abs/2307.05440) | ISLTranslate是一个包含31k个ISL-英语句子/短语对的最大连续印度手语翻译数据集，该数据集帮助开发手语翻译系统，解决印度手语资源匮乏的问题。 |
| [^5] | [Metropolis Sampling for Constrained Diffusion Models.](http://arxiv.org/abs/2307.05439) | 本文提出了一种基于城市采样的简单加噪方案，用于解决受约束的扩散模型的生成建模问题。相较于现有的采样器，该方案在计算效率和实证性能方面都有明显提升，并且被证明是反射布朗运动的有效离散化。 |
| [^6] | [Improving the Security of Smartwatch Payment with Deep Learning.](http://arxiv.org/abs/2307.05437) | 本论文通过深度学习方法提高了智能手表支付的安全性，包括构建了一个超越最先进技术的深度学习认证系统和开发一个生成合成手势的模型。这些方法可以减少用户在认证系统中提供手势的数量，从而提高系统性能。 |
| [^7] | [One-Versus-Others Attention: Scalable Multimodal Integration.](http://arxiv.org/abs/2307.05435) | 提出了一种可扩展的多模态集成方法，通过一对多（OvO）注意力机制解决了多模态学习中超过三个模态的注意力计算问题。 |
| [^8] | [Self-Supervised Learning with Lie Symmetries for Partial Differential Equations.](http://arxiv.org/abs/2307.05432) | 本研究通过自监督学习的方法，利用李对称将异构数据中的PDEs表示进行优化，提高了不变任务的性能并改进了神经求解器的时间推进性能。 |
| [^9] | [Geometric Neural Diffusion Processes.](http://arxiv.org/abs/2307.05431) | 本文将扩散模型的框架应用于无限维建模，并引入几何先验以处理在非欧几里得空间中带有对称性的数据。通过构建具有对称群变换的几何高斯过程和等变神经网络逼近得分，生成函数模型也具有相同的对称性。 |
| [^10] | [Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort.](http://arxiv.org/abs/2307.05426) | 该研究提出了一种使用卷积神经网络（CNN）和BOLD-fMRI计算人类连接组发展队列中呼吸体积与时间（RTV）和呼吸变异（RV）的方法。实验结果表明，CNN可以从静息状态的BOLD信号中捕捉有信息的特征，并重建真实的呼吸时序。这种方法可以降低fMRI研究的成本，并减轻参与者的负担。 |
| [^11] | [Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection.](http://arxiv.org/abs/2307.05422) | 本文提出了一种在黑盒情况下检测深度神经网络后门攻击的高效方法，通过量化触发器和良性特征对确定后门网络输出的影响，并使用训练好的新颖性检测器进行检测。 |
| [^12] | [Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores.](http://arxiv.org/abs/2307.05405) | 本文提出一种利用人类提供的分数来改进交互式强化学习的反馈效率的方法。通过使用分数代替成对偏好，我们可以获得更多的数据。我们通过对机器人任务的评估表明，该方法能够通过自适应学习分数有效地学习到接近最优的策略。 |
| [^13] | [Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform.](http://arxiv.org/abs/2307.05399) | 在文档处理平台中，我们提出了一种领域无关的神经架构，能够在每个类别的示例单独呈现时训练高性能分类器，无需使用记忆缓冲区，并在实验中取得了超越参考方法的结果。 |
| [^14] | [Simplicial Message Passing for Chemical Property Prediction.](http://arxiv.org/abs/2307.05392) | 这篇论文提出了一种Simplicial信息传递（SMP）框架，用于捕捉分子中隐藏的拓扑信息，并在化学性质预测方面取得了显著进展。 |
| [^15] | [CrysMMNet: Multimodal Representation for Crystal Property Prediction.](http://arxiv.org/abs/2307.05390) | CrysMMNet是一种多模态表示方法，通过使用文本描述建模全局结构信息来预测晶体属性。 |
| [^16] | [Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation.](http://arxiv.org/abs/2307.05385) | 本文提出了一种通过学习核技术，具有解释性且参数较少的方法来评估和分割PPG信号的质量和伪影，与现有的深度神经网络方法相比有着类似甚至更好的性能。 |
| [^17] | [Stochastic Nested Compositional Bi-level Optimization for Robust Feature Learning.](http://arxiv.org/abs/2307.05384) | 本文提出了一种用于解决嵌套构成双层优化问题的随机逼近算法，可以实现鲁棒特征学习，并且不依赖于矩阵求逆或小批量输入。 |
| [^18] | [Human Emotion Recognition Based On Galvanic Skin Response signal Feature Selection and SVM.](http://arxiv.org/abs/2307.05383) | 本论文提出了一种基于自动选择的皮肤电反应信号特征和支持向量机的人类情绪识别方法，实验结果表明该方法在人类情绪识别方面具有较高的准确率。 |
| [^19] | [Protecting the Future: Neonatal Seizure Detection with Spatial-Temporal Modeling.](http://arxiv.org/abs/2307.05382) | 本文提出了一种名为STATENet的深度学习框架，通过精心设计在时空和模型层面上解决了新生儿癫痫检测中的独特挑战，实验证明了该框架能显著提高癫痫检测性能。 |
| [^20] | [Optimized Crystallographic Graph Generation for Material Science.](http://arxiv.org/abs/2307.05380) | 我们提出了一种在GPU优化中生成晶体材料的图形表示的高效工具，并提供了一个与Pytorch兼容的框架。通过实时生成图，我们的工具能够更新结构的几何形状，并在神经网络的训练过程中处理更新后的图。 |
| [^21] | [M$^2$Hub: Unlocking the Potential of Machine Learning for Materials Discovery.](http://arxiv.org/abs/2307.05378) | M$^2$Hub是一个推动机器学习在材料发现中的工具包，通过集成平台提供多样化的材料发现任务、数据集、机器学习方法、评估和基准结果的便捷访问，涵盖虚拟筛选、逆向设计和分子模拟等关键阶段的多种材料类型和属性。 |
| [^22] | [Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems.](http://arxiv.org/abs/2307.05374) | 首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性，并且通过一个"单一"的基于神经网络的均衡器，在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。 |
| [^23] | [Classification of sleep stages from EEG, EOG and EMG signals by SSNet.](http://arxiv.org/abs/2307.05373) | 本研究提出了一个基于深度学习的SSNet框架，通过从EEG、EOG和EMG信号中提取特征，并将其输入到全连接层进行分类，成功实现了睡眠阶段的分类，并在两个公共数据集上获得了高准确度和Kappa系数的结果。 |
| [^24] | [Capafoldable: self-tracking foldable smart textiles with capacitive sensing.](http://arxiv.org/abs/2307.05370) | 这项工作提出了一种创新的自追踪可折叠智能纺织品，通过结合折叠织物结构和电容传感，利用深度学习技术来检测结构运动。实验结果显示，我们的方法可以从电容信号中准确重构出片段的几何形状。 |
| [^25] | [Neural network analysis of neutron and X-ray reflectivity data: Incorporating prior knowledge for tackling the phase problem.](http://arxiv.org/abs/2307.05364) | 本研究提出了一种利用先验知识的神经网络分析方法，用于解决通过中子和X射线反射率曲线确定多层薄膜物理参数的相位问题。实验证明了该方法在不同情景下的有效性，能够改善训练过程并解决问题的欠定性质。 |
| [^26] | [SleepEGAN: A GAN-enhanced Ensemble Deep Learning Model for Imbalanced Classification of Sleep Stages.](http://arxiv.org/abs/2307.05362) | 本文提出了一种名为SleepEGAN的生成对抗网络（GAN）增强的集成深度学习模型，用于不平衡的睡眠阶段分类。该模型通过使用新的GAN架构进行数据增强，并采用无成本的集成学习策略来解决类别不平衡和个体异质性问题，提高了分类准确性和鲁棒性。 |
| [^27] | [A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics.](http://arxiv.org/abs/2307.05361) | 本文提出了一种基于物理信息的低样本学习方法，用于基于sEMG的肌肉力和关节运动学估计。该方法将拉格朗日运动方程和反动力学肌肉模型整合到生成对抗网络中，实现对小样本数据的结构化特征解码和外推估计。 |
| [^28] | [Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators.](http://arxiv.org/abs/2307.05358) | 本文提出了一种带有双调节器的新型联邦半监督学习框架FedDure，解决了数据分布不平衡的问题。通过粗调节器和细调节器对本地模型的更新进行规范，以及学习适应性加权方案，适应不同的数据分布。 |
| [^29] | [VisText: A Benchmark for Semantically Rich Chart Captioning.](http://arxiv.org/abs/2307.05356) | 本研究介绍了VisText，一个丰富语义的图表标题评测基准，该数据集包含了12,441个图表和标题对，描述了图表的构造、报告了关键统计数据，并识别了感知和认知现象。通过在图表标题生成任务上微调语言模型并应用预处理，我们评估了VisText的影响。 |
| [^30] | [Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models.](http://arxiv.org/abs/2307.05350) | 本文模糊了后解释黑盒模型与构建可解释模型之间的界限，通过从灵活的黑盒模型开始，逐渐引入可解释模型和残差网络，实现了对样本的路由和解释。 |
| [^31] | [Tracking Most Significant Shifts in Nonparametric Contextual Bandits.](http://arxiv.org/abs/2307.05341) | 该论文研究了非参数情境赌博中的最显著变化，提出了一种只计算显著变化的方法，来解决局部性问题。 |
| [^32] | [A Self-Supervised Algorithm for Denoising Photoplethysmography Signals for Heart Rate Estimation from Wearables.](http://arxiv.org/abs/2307.05339) | 提出了一种自监督算法，用于从可穿戴设备收集的PPG信号中去噪以估计心率。该算法通过重构受损信号部分和保留干净信号部分的方式进行去噪，并通过利用干净PPG信号训练自动编码器来实现。实验结果表明我们的算法在PPG信号心率估计方面优于传统方法。 |
| [^33] | [Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach.](http://arxiv.org/abs/2307.05333) | 本文提出了一种基于多属性公平损失的CNN模型，通过考虑患者数据中的敏感属性，公平预测疼痛状态，致力于减少差异。 |
| [^34] | [The Value of Chess Squares.](http://arxiv.org/abs/2307.05330) | 本研究通过引入边际估值对国际象棋棋盘上的棋子和棋盘进行评价，提供了关于马、象和兵的有价值的见解。 |
| [^35] | [Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks.](http://arxiv.org/abs/2307.05318) | 这项工作提出了一种使用深度集成神经网络在端点设备上预测小分子溶解度的方法，通过静态网站运行，同时具备预测不确定性，并实现了令人满意的结果。 |
| [^36] | [Discovering Symbolic Laws Directly from Trajectories with Hamiltonian Graph Neural Networks.](http://arxiv.org/abs/2307.05299) | 本文介绍了一种哈密尔顿图神经网络（HGNN），它可以直接从轨迹中学习物理系统的动力学，在多个系统上表现出色，并能从少量数据中提取符合真实情况的定律。 |
| [^37] | [On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets.](http://arxiv.org/abs/2307.05284) | 该论文通过对表格数据集中的自然偏移进行研究，发现$Y|X$-偏移最为普遍。为了推动研究人员开发描述数据分布偏移的精细语言，作者构建了WhyShift实验平台，并讨论了$Y|X$-偏移对算法的影响。 |
| [^38] | [CareFall: Automatic Fall Detection through Wearable Devices and AI Methods.](http://arxiv.org/abs/2307.05275) | 本文介绍了CareFall，一种基于可穿戴设备和人工智能方法的自动跌倒检测系统。实验结果表明，机器学习方法在准确率、敏感性和特异性方面优于基于阈值的方法，为解决老年人跌倒问题提供了智能和用户友好的解决方案。 |
| [^39] | [U-CREAT: Unsupervised Case Retrieval using Events extrAcTion.](http://arxiv.org/abs/2307.05260) | U-CREAT是一个无监督案例检索系统，通过使用事件提取实现了更高的性能和更快的检索速度，适用于实时案例检索系统。 |
| [^40] | [MAP- and MLE-Based Teaching.](http://arxiv.org/abs/2307.05252) | 该论文研究了基于MAP和MLE的教学方法，其中学习者根据观察结果推断隐藏的概念，教师试图找到最小的观察集合以使得学习者返回特定的概念。 |
| [^41] | [DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image Synthesis.](http://arxiv.org/abs/2307.05249) | 本论文提出了一种通用模型，通过动态路由和跨层连接来解决多中心PET图像合成中的域转移和中心干扰问题。 |
| [^42] | [A Survey From Distributed Machine Learning to Distributed Deep Learning.](http://arxiv.org/abs/2307.05232) | 这篇综述论文介绍了分布式机器学习和分布式深度学习的研究现状和方法，强调了分布式深度学习在解决复杂问题方面取得的重要进展。 |
| [^43] | [Attribute Controlled Dialogue Prompting.](http://arxiv.org/abs/2307.05228) | 本文提出了一种新的基于实例级控制代码的对话引导算法，用于探索实例特定的提示对于控制对话生成的影响。实验结果表明，该方法优于提示基线，并且与仅使用总参数的微调相媲美。 |
| [^44] | [Supervised Attention Using Homophily in Graph Neural Networks.](http://arxiv.org/abs/2307.05217) | 本文提出了一种新技术，可以在任何图注意力模型中应用，以鼓励共享相同类别标签的节点获得更高的注意力分数，并在多个节点分类数据集上展示了比标准基线模型更高的性能。 |
| [^45] | [Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning.](http://arxiv.org/abs/2307.05213) | 本研究采用评分函数梯度估计方法，通过预测参数分布来计算决策焦点模型的更新，以扩大决策焦点学习的适用性。 |
| [^46] | [Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning.](http://arxiv.org/abs/2307.05209) | 我们提出了一种使用奖励机器抽象来表示当前任务，并在迁移学习中提升DRL代理的性能的方法，实验表明该方法能够提高样本效率并在多个领域中进行少样本迁移。 |
| [^47] | [Reject option models comprising out-of-distribution detection.](http://arxiv.org/abs/2307.05199) | 本文提出了三种拒绝选项模型来处理超出分布检测问题，并给出了一个简单的双评分超出分布方法。实验结果表明，该方法优于最先进的方法。 |
| [^48] | [Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling.](http://arxiv.org/abs/2307.05194) | 通过对数据生成过程和模型之间的$\beta$-分解进行后验采样，我们提出了$\beta$D-Bayes，一种能够实现差分机器学习的方法。 |
| [^49] | [Membership Inference Attacks on DNNs using Adversarial Perturbations.](http://arxiv.org/abs/2307.05193) | 提出了一种使用对抗扰动的DNN成员推断攻击，通过对现有MI攻击进行统一并提出两种新的攻击方法，提高了在简单数据集上的性能表现。 |
| [^50] | [Using Linear Regression for Iteratively Training Neural Networks.](http://arxiv.org/abs/2307.05189) | 我们提出了一种使用线性回归来迭代训练神经网络的方法，通过从输出向后计算神经元的理想总输入值，以线性最小二乘问题迭代更新参数和激活值。 |
| [^51] | [Decorrelation using Optimal Transport.](http://arxiv.org/abs/2307.05187) | 引入了一种利用凸神经最优输运求解器进行去相关的新方法，通过最优输运将连续特征空间与受保护属性去相关。在高能物理中，这种方法在喷注分类中表现出色，能更好地去相关多类输出特征空间。 |
| [^52] | [A Mapping Study of Machine Learning Methods for Remaining Useful Life Estimation of Lead-Acid Batteries.](http://arxiv.org/abs/2307.05163) | 本文是一项关于使用机器学习方法估计铅酸电池的SoH和RUL的映射研究。正确的估计这两个指标可以提高电池系统的预测性维护、可靠性和寿命。这对于电动汽车、可再生能源系统等依赖此电池技术的应用都至关重要。 |
| [^53] | [SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization.](http://arxiv.org/abs/2307.05162) | 本研究展示了一种名为LoRA的参数高效细调方法在临床对话摘要中的评估结果，并证明LoRA与对大型语言模型进行端到端细调效果相当。 |
| [^54] | [On the Effectiveness of Speech Self-supervised Learning for Music.](http://arxiv.org/abs/2307.05161) | 本研究探索了语音自我监督学习在音乐信息检索中的有效性。通过对两个与语音相关的模型进行自我监督学习适应，并在多个MIR任务上进行系统评估，结果显示在音乐数据上训练可以提高MIR任务的性能。 |
| [^55] | [Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders.](http://arxiv.org/abs/2307.05152) | 这项研究提出了两种机器学习算法，用于在CERN大型强子对撞机中选择中性长寿命粒子衰变的事件，并通过加速卡进行加速。实验结果表明，这些算法在加速的情况下仍然保持准确性，并符合第二级触发的延迟要求。 |
| [^56] | [Deep Probabilistic Movement Primitives with a Bayesian Aggregator.](http://arxiv.org/abs/2307.05141) | 该论文提出了一个统一的深度运动原理模型，具备多种操作，并展示了高样本效率和泛化能力。 |
| [^57] | [TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation.](http://arxiv.org/abs/2307.05134) | 本文提出了一种评估文本到图像生成中对齐性的新度量方法TIAM，该方法基于提示模板，可以更好地描述生成图像与提示中内容的对齐程度，包括对象类型、数量和颜色。研究结果表明，图像质量可以有很大的变化。 |
| [^58] | [On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis.](http://arxiv.org/abs/2307.05132) | 这项研究探讨了在自发语音合成中使用自监督式语音表示的方法，并发现了在自发语音转语音系统中最适合的SSL和SSL模型的哪一层。此外，该研究还扩展了基于SSL的MOS预测框架，成功地在合成的自发语音上进行了评估。 |
| [^59] | [Enhancing Continuous Time Series Modelling with a Latent ODE-LSTM Approach.](http://arxiv.org/abs/2307.05126) | 该论文提出了使用潜在ODE-LSTM方法增强连续时间序列建模的方法，解决了使用标准RNN进行建模时遇到的问题，包括不规则采样和缺失数据等。该方法使用ODE-RNN模型作为编码器，并使用神经ODE作为解码器，提供了更好的建模效果。 |
| [^60] | [Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer.](http://arxiv.org/abs/2307.05121) | 我们提出了一种名为STA-GT的新颖异构图神经网络用于交易欺诈检测，它能够有效学习空间-时间信息，并通过合并全局信息改进表示学习。 |
| [^61] | [$\ell_p$-Regression in the Arbitrary Partition Model of Communication.](http://arxiv.org/abs/2307.05117) | 这个论文研究了通信中的任意分区模型中的$\ell_p$回归问题的随机通信复杂度，给出了该问题的显著改进的界限。对于$p = 2$的情况，我们给出了最优$\tilde{\Theta}(sd^2 + sd/\epsilon)$比特的界限。对于$p \in (1,2)$的情况，我们得到了$\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$的界限。 |
| [^62] | [Conformalization of Sparse Generalized Linear Models.](http://arxiv.org/abs/2307.05109) | 本文研究了稀疏广义线性模型的合规化问题。通过利用选择变量在输入数据微小扰动下的不变性，我们使用数值延拓技术高效逼近解决方案路径，从而减少计算合规化集合的复杂度。 |
| [^63] | [A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI.](http://arxiv.org/abs/2307.05104) | 本研究深入探讨了使用扰动作为评估从时间序列模型中提取的归因的方法。通过对多种XAI技术的应用和在多个数据集上进行验证，结果表明扰动分析可以有效评估归因的质量，并为其优点和局限性提供了洞察力。 |
| [^64] | [Estimating label quality and errors in semantic segmentation data via any model.](http://arxiv.org/abs/2307.05080) | 通过任何模型，我们可以估计语义分割数据中标签的质量和错误。这导致了一种用于自动检测错误的标签质量评分方法，并且可以帮助确定需要重点检查的数据，以确保高质量的训练/评估数据集。 |
| [^65] | [A Theory of Bounded Inductive Rationality.](http://arxiv.org/abs/2307.05068) | 本论文的主要贡献是提出了一个不假设逻辑全知性的理性决策理论，该理论可用于解决在现实环境中存在计算复杂性和无法对自身进行全面分析的决策问题。 |
| [^66] | [Portfolio Optimization: A Comparative Study.](http://arxiv.org/abs/2307.05048) | 该论文进行了一项比较研究，研究了三种投资组合设计方法，包括均值方差投资组合（MVP）、层次风险均衡（HRP）基于投资组合和自编码器基于投资组合，并将其应用于印度国家证券交易所（NSE）上的十个主题行业的股票。结果显示，在样本外数据上，MVP投资组合的表现最佳。 |
| [^67] | [Number Systems for Deep Neural Network Architectures: A Survey.](http://arxiv.org/abs/2307.05035) | 这项论文调查了用于深度神经网络的替代数字系统，以提高计算效率和资源利用。 |
| [^68] | [FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms.](http://arxiv.org/abs/2307.05029) | 本论文研究了如何利用开源的机器学习模型解释工具，使普通人可以直观地理解和改善决策支持系统中的不公平问题。 |
| [^69] | [Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels.](http://arxiv.org/abs/2307.05025) | 我们研究表明，结合交叉熵损失和正则化策略（如学习率衰减、模型权重平均和数据增强）的简单基准方法可以超过最先进方法，证明了正则化策略的组合在学习嘈杂标签问题中的潜力。 |
| [^70] | [Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification.](http://arxiv.org/abs/2307.05017) | 本文提出了一种名为特征激活映射（FAM）的解释工具，可以解释没有FC层的深度学习模型作为分类器，使其更加可解释、透明和可信。 |
| [^71] | [Test-Time Training on Video Streams.](http://arxiv.org/abs/2307.05014) | 该论文扩展了测试时培训（TTT）到视频流的设置中，提出了在线TTT方法，相对于固定模型基线和离线TTT，在多个任务上都有显著的性能优势，包括实例和全景分割。 |
| [^72] | [Improving RNN-Transducers with Acoustic LookAhead.](http://arxiv.org/abs/2307.05006) | 本文提出了一种名为LookAhead的技术，通过提前观察音频输入的未来部分，使RNN-Transducers模型的文本表示更加与声学相符。该技术在准确率上相对降低了5%-20%。 |
| [^73] | [Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2307.05004) | 本文提出了一种将控制与概率推理结合的新颖的通信机制，应用于多智能体强化学习中。智能体通过推理控制其动作，并通过消息进行通信，从而实现协作任务。 |
| [^74] | [Selective Sampling and Imitation Learning via Online Regression.](http://arxiv.org/abs/2307.04998) | 本论文提出了一种通过在线回归实现选择性采样和模仿学习的方法，解决了在只有噪声专家反馈的情况下的问题。算法不需要大量样本即可成功，并取得了最佳的回归和查询次数界限。 |
| [^75] | [Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning.](http://arxiv.org/abs/2307.04996) | 本文介绍了两种基于知识图谱的方法，一种使用强化学习，另一种使用XGBoost算法，用于个性化文章推荐。这些方法利用自动生成的知识图谱，并在一个大型跨国金融服务公司的客户中进行了实证研究。 |
| [^76] | [PowerFusion: A Tensor Compiler with Explicit Data Movement Description and Instruction-level Graph IR.](http://arxiv.org/abs/2307.04995) | PowerFusion是一种张量编译器，通过考虑计算和数据移动优化，生成高性能内存密集运算符的高效代码。 |
| [^77] | [Uncertainty Quantification of the Virial Black Hole Mass with Conformal Prediction.](http://arxiv.org/abs/2307.04993) | 该研究提出了使用符合预测的方法在机器学习设置中量化黑洞预测的不确定性。与传统方法相比，该方法能够提供更有用的预测区间指标，并调整到黑洞质量及其相关属性。 |
| [^78] | [Monotone deep Boltzmann machines.](http://arxiv.org/abs/2307.04990) | 在这项工作中，我们提出了一种新的限制模型，即单调DBM，它允许每一层具有任意的自连接，但通过一种方式限制了权重，以保证存在和全局唯一的均场不动点。 |
| [^79] | [Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation.](http://arxiv.org/abs/2307.04988) | 该研究评估了六种基准因果发现方法和一种新提出的基于 GFlowNets 的方法在治疗效果估计任务中的表现，并发现 GFlowNets 具有捕捉各种有用和多样的平均处理效应模式的能力。 |
| [^80] | [Secrets of RLHF in Large Language Models Part I: PPO.](http://arxiv.org/abs/2307.04964) | 本论文研究了大型语言模型中RLHF的秘密，重点关注了奖励模型、PPO和进程监督等技术路径，探索如何解决RLHF的稳定训练问题。 |
| [^81] | [DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization.](http://arxiv.org/abs/2307.04963) | DyCL通过程序重写和图优化的方式，解决了现有DL编译器在编译具有动态特性的神经网络时的困难，提供了一种通用的方法来成功编译动态神经网络。 |
| [^82] | [Intrinsically motivated graph exploration using network theories of human curiosity.](http://arxiv.org/abs/2307.04962) | 在这项工作中，我们通过应用人类好奇心的两个理论，发展了一种内在驱动的图探索方法。我们利用图神经网络的强化学习将拓扑特征作为奖励，从而实现了对图结构数据的探索。在多类合成生成图上进行的实验证明，我们的方法不仅可以推广到更大的环境，还可以进行更长的探索步行。同时，我们的方法比传统的贪婪评估方法更高效。 |
| [^83] | [Reinforcement Learning with Non-Cumulative Objective.](http://arxiv.org/abs/2307.04957) | 本文研究了最优控制和强化学习中非累积目标的挑战，并提出了修改现有算法的方法来优化这些目标。研究结果表明，在贝尔曼最优性方程中使用广义运算可以更好地处理非累积目标。 |
| [^84] | [Hybrid hidden Markov LSTM for short-term traffic flow prediction.](http://arxiv.org/abs/2307.04954) | 该论文介绍了一种混合隐马尔可夫LSTM模型，用于短期交通流量预测。研究发现，深度学习方法在预测交通变量方面优于传统的参数模型。这种模型结合了循环神经网络和隐马尔可夫模型的优势，能够捕捉交通系统的复杂动态模式和非平稳性。 |
| [^85] | [Compact Twice Fusion Network for Edge Detection.](http://arxiv.org/abs/2307.04952) | 本文提出了一种紧凑型双重融合网络（CTFN），用于边缘检测，在保持模型紧凑性的同时完全整合多尺度特征。其中包括语义增强模块和伪像素级加权模块，还使用动态焦点损失函数处理纹理噪声带来的挑战。 |
| [^86] | [DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization.](http://arxiv.org/abs/2307.04946) | 本文提出了一种通过扩散去噪梯度加权最小化求解逆问题的方法。该方法将传统的基于梯度的最小化重构误差与去噪相结合，每一步都添加噪声，使得迭代动力学类似于Langevin或扩散过程。通过模拟倾斜视图进行实证研究，我们发现该方法可以在只有50个倾斜视图的情况下达到高精度。 |
| [^87] | [Benchmarking Algorithms for Federated Domain Generalization.](http://arxiv.org/abs/2307.04942) | 本论文介绍了一种针对联邦领域泛化的基准测试方法，并评估了13种联邦DG方法。研究结果表明，在大量客户端和高异质性的情况下，联邦DG仍存在显著的性能差距。 |
| [^88] | [Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective.](http://arxiv.org/abs/2307.04937) | 这项研究提出了以因果视角看待公平图学习问题的框架CAF，通过选择训练数据中的反事实来避免图神经网络中的偏见。 |
| [^89] | [Probabilistic Counterexample Guidance for Safer Reinforcement Learning.](http://arxiv.org/abs/2307.04927) | 本文提出了一种安全强化学习方法，通过引导训练中的反例来解决安全探索的问题，该方法将连续和离散状态空间系统抽象为紧凑的模型，并利用概率性反例生成构建最小化仿真子模型，以揭示安全需求的违反情况。 |
| [^90] | [SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation.](http://arxiv.org/abs/2307.04907) | SimpleMTOD是一个简单的语言模型，将多模态任务导向对话的子任务转化为序列预测任务，并引入了局部和非局部的对象标记来捕捉视觉场景的语义。它在SIMMC 2.0测试集的回应生成子任务中取得了最先进的BLEU分数，同时在其他多模态子任务中也表现出色。 |
| [^91] | [FedYolo: Augmenting Federated Learning with Pretrained Transformers.](http://arxiv.org/abs/2307.04905) | 本文研究了在联邦学习中使用预训练的Transformer模型来实现在设备上的学习目标，探讨了模型大小和模块化的作用，并证明了规模较大可以提高异构性的鲁棒性。 |
| [^92] | [Fast dynamic time warping and clustering in C++.](http://arxiv.org/abs/2307.04904) | 这种方法提出了一种计算效率高的动态时间规整（DTW）和时间序列数据聚类的方法，在保证全局最优性不是必需的情况下，使用混合整数规划（MIP）和k-medoids聚类来增加速度，同时实现了任务级并行化，通过测试发现比其他选项快33%，在大数据集上速度提高到64%。 |
| [^93] | [Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer.](http://arxiv.org/abs/2307.04895) | 循环Transformer是一种可行的方法来学习解决约束满足问题。相比于类似的方法，循环Transformer具有明显的优势，可以处理视觉输入，成功解决符号基础问题，并实现样本高效学习和半监督学习。 |
| [^94] | [Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies.](http://arxiv.org/abs/2307.04893) | 这篇论文介绍了一种名为2L的算法，该算法能够提供引导合成程序化策略的参考策略，通过在实验中的表现和在MicroRTS锦标赛中的胜利，证明了2L算法相对于其他学习算法的优势。 |
| [^95] | [Accelerated Discovery of Machine-Learned Symmetries: Deriving the Exceptional Lie Groups G2, F4 and E6.](http://arxiv.org/abs/2307.04891) | 本研究提出了两种改进算法，能够显著加快对称变换的发现速度，并成功地推导出了特殊李群G2，F4和E6的完整一组生成元。这种机器学习方法对于发现各种类型的对称性是通用的。 |
| [^96] | [Measuring and Mitigating Interference in Reinforcement Learning.](http://arxiv.org/abs/2307.04887) | 本文提供了一种衡量强化学习中干扰的新方法，并且提出了一类在线感知算法来减轻干扰，这些算法在经典控制环境中提高了稳定性和性能。 |
| [^97] | [Onion Universe Algorithm: Applications in Weakly Supervised Learning.](http://arxiv.org/abs/2307.04870) | 洋葱宇宙算法是一种新颖的集成学习分类方法，可作为弱监督学习的标签模型。它在实现上简单、计算高效，适用于无完全标记数据的情况。经验证明，它在常见的基准数据集上表现出色。 |
| [^98] | [Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning.](http://arxiv.org/abs/2307.04869) | 本文提出了一种名为Fed-CPrompt的方法，用于解决无重复学习的联邦持续学习中的遗忘问题。该方法通过异步提示学习和对比持续损失处理异步任务到达和异构数据分布，并在实验证明其在该领域取得了最先进的性能。 |
| [^99] | [Leveraging an Alignment Set in Tackling Instance-Dependent Label Noise.](http://arxiv.org/abs/2307.04868) | 本论文提出了一种两阶段的方法来应对实例相关的标签噪声。该方法利用了对齐集合来学习，并在多个任务中相对于现有方法实现了一致的性能改进。 |
| [^100] | [Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds.](http://arxiv.org/abs/2307.04866) | 该论文研究了使用腰部佩戴的加速计自动检测步态事件和行走距离的方法，通过分析市售智能手机加速计数据，实现了从广泛的步态速度范围中提取步态特征，可用于对Duchenne肌肉萎缩患儿和典型发育正常患者的评估。 |
| [^101] | [Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models.](http://arxiv.org/abs/2307.04859) | 本研究提出了一种文本引导的3D头像生成方法，通过直接在可关节化的3D模型上操作几何和纹理，并引入了新的优化过程，实现了对生成头像的精确控制。 |
| [^102] | [SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features.](http://arxiv.org/abs/2307.04850) | 本文提出了SHAP@k框架，旨在通过提高样本效率来解决Top-k特征识别问题，通过将问题转化为Explore-m问题并利用多臂赌博机的技术来实现。 |
| [^103] | [SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees.](http://arxiv.org/abs/2307.04849) | SigOpt Mulch是一种智能系统，用于自动化调整梯度提升树模型的超参数。与其他现有系统不同，SigOpt Mulch是“模型感知型”的，能够针对GBTs进行更优化的性能调整，并且无需领域知识，帮助实现自动化实验。 |
| [^104] | [Dynamics of Temporal Difference Reinforcement Learning.](http://arxiv.org/abs/2307.04841) | 我们使用统计物理学的概念，研究了时间差分学习在线性函数逼近器下的典型学习曲线。我们发现由于子采样可能的轨迹空间而产生的随机半梯度噪声会导致值误差出现显著的平台。 |
| [^105] | [CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction.](http://arxiv.org/abs/2307.04838) | 本文研究了使用CLIP模型提高视觉关系预测的可能性。通过在UVTransE框架中采用基于CLIP的表示方法，以及引入对比训练策略，我们提出了CREPE模型，简化了现有复杂的图形模型，取得了良好的效果。 |
| [^106] | [On Detecting Some Defective Items in Group Testing.](http://arxiv.org/abs/2307.04822) | 本文关注解决了群体测试中识别特定数量有缺陷项目的问题，并在自适应和非自适应设置下给出了相应的上下界。 |
| [^107] | [A physics-constrained machine learning method for mapping gapless land surface temperature.](http://arxiv.org/abs/2307.04817) | 本文提出了一种物理约束的机器学习模型，将机制与数据驱动模型的优点结合起来，以生成具有物理意义和高精度的无间断地表温度估计。模型利用了轻量梯度提升机作为纯机器学习模型，并通过物理约束进一步提升了模型的可解释性和外推能力。模型的输入变量包括遥感数据、关键Community Land Model（CLM）强制数据以及CLM模拟数据。 |
| [^108] | [Collaborative Score Distillation for Consistent Visual Synthesis.](http://arxiv.org/abs/2307.04787) | 本文介绍了一种名为协同分数蒸馏（Collaborative Score Distillation，CSD）的新方法，它基于斯坦变分梯度下降（SVGD），通过将多个样本作为“粒子”并结合它们的分数函数来实现对一组图像的生成先验进行同步蒸馏，从而在多个样本之间实现了一致的视觉合成。CSD在各种任务中展示了其有效性，包括全景图像、视频和3D场景的视觉编辑。 |
| [^109] | [Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation.](http://arxiv.org/abs/2307.04780) | 本文比较了使用点云和基于图像的模型来进行量热器快速模拟，发现点云更自然地表示了量热器淋浴，处理稀疏数据集更优秀，并且可以使用更紧凑的模型和数据文件进行实现。 |
| [^110] | [Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case.](http://arxiv.org/abs/2307.04778) | 本论文介绍了一个基于统计分析的战略计划，通过研究金融公司LendingClub的实际案例，探索引入大数据平台和先进特征选择能力的可能性，以增加收入并降低风险。 |
| [^111] | [MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment.](http://arxiv.org/abs/2307.04777) | 本论文提出了一个个性化的心理健康跟踪与情绪预测系统，利用个人健康设备收集的生理数据，通过分散学习机制实现精神病学治疗和管理的有效追踪，并以隐私意识和有责任感的方式提供精神科医生对患者心理健康状况的进一步了解。 |
| [^112] | [Digital Twins for Patient Care via Knowledge Graphs and Closed-Form Continuous-Time Liquid Neural Networks.](http://arxiv.org/abs/2307.04772) | 本文提出了一个新的框架，通过将患者健康数据构建为知识图谱，并使用闭式连续时间液体神经网络进行实时分析，以解决复杂疾病建模和计算复杂性的问题。这一框架有望推动数字双生技术在患者护理领域的应用。 |
| [^113] | [Predicting Outcomes in Long COVID Patients with Spatiotemporal Attention.](http://arxiv.org/abs/2307.04770) | 本研究提出了一种时空关注机制来预测长期COVID患者的结果，解决了其纵向数据预测的困难性，并通过与其他方法进行比较，证明了该方法的有效性。 |
| [^114] | [Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles.](http://arxiv.org/abs/2307.04679) | 本文提出了一种新的框架来分析使用一阶优化算法进行统计学习时的泛化误差，该框架适用于多个学习问题，并且可以推导出紧密匹配的上界和下界。这些结果适用于光滑、强凸和满足Polyak-Lojasiewicz假设的优化问题。 |
| [^115] | [Self Expanding Neural Networks.](http://arxiv.org/abs/2307.04526) | 这项研究提出了一种自扩展神经网络的方法，通过自然梯度来自动增加神经网络的宽度和深度，以在训练损失降低的情况下提高性能，并在分类和回归问题中展示了其优势。 |
| [^116] | [CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning.](http://arxiv.org/abs/2307.04390) | 本研究开发了一种名为SRRD的新方法，利用易于获得的CT图像进行膝关节骨性骨质分析，并通过配对的MR图像增强了模型的分析能力。该方法有望在膝关节骨关节炎的临床应用中具有潜在价值。 |
| [^117] | [DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification.](http://arxiv.org/abs/2307.03500) | DEFT是一种利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化的方法，可以减少分布式深度学习中的通信流量，并在计算成本和梯度累积方面具有优势。 |
| [^118] | [QI2 -- an Interactive Tool for Data Quality Assurance.](http://arxiv.org/abs/2307.03419) | 本文介绍了一种用于数据质量保证的交互工具QI2，该工具支持对多个数据质量方面的验证和定量数据质量要求的验证。通过在MNIST数据集上进行演示，展示了该方法的应用和优势。 |
| [^119] | [When Fair Classification Meets Noisy Protected Attributes.](http://arxiv.org/abs/2307.03306) | 这项研究是对公平分类算法进行的一次首次的头对头比较，研究了依赖属性、容忍噪声和盲目属性的算法在预测性和公平性方面的表现，结果显示盲目属性和容忍噪声的公平分类器具有潜力。 |
| [^120] | [FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout.](http://arxiv.org/abs/2307.02623) | FLuID提出了一种使用不变性丢失的方法来减轻联邦学习中性能较低设备导致的训练时间问题，并开发了一个自适应训练框架。通过动态平衡训练负载，FLuID能有效地减轻阻塞设备的工作负载，同时不影响模型质量。 |
| [^121] | [Application of data engineering approaches to address challenges in microbiome data for optimal medical decision-making.](http://arxiv.org/abs/2307.00033) | 本研究应用数据工程方法解决微生物组数据中的类别不平衡和高维度问题，针对囊性纤维化婴儿的数据集实施了四种机器学习分类器，并提高了医学决策的准确性。 |
| [^122] | [Defining data science: a new field of inquiry.](http://arxiv.org/abs/2306.16177) | 数据科学是一种新的研究范式，具有潜力和应用广泛性，在40多个学科、数百个研究领域和成千上万个应用中出现。然而，由于其起步阶段，目前存在许多定义的冗余和不一致性的问题。 |
| [^123] | [Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation.](http://arxiv.org/abs/2306.16170) | 本文介绍了一种名为多教师对抗鲁棒性蒸馏的方法，它通过使用强大的干净样本教师和鲁棒性教师来改进深度神经网络的对抗训练过程，以减轻准确性和鲁棒性之间的权衡。 |
| [^124] | [BayesFlow: Amortized Bayesian Workflows With Neural Networks.](http://arxiv.org/abs/2306.16015) | BayesFlow是一个Python库，提供了使用神经网络进行摊还贝叶斯推断的功能，用户可以在模型仿真上训练定制的神经网络，并将其用于任何后续应用。这种摊还贝叶斯推断能够快速准确地进行推断，并实现了对不可计算后验分布的近似。 |
| [^125] | [Physics-constrained Random Forests for Turbulence Model Uncertainty Estimation.](http://arxiv.org/abs/2306.13370) | 本文提出了一种物理约束的机器学习方法，用于考虑湍流模型的认识不确定性，同时在准确数据稀缺时能够实现前置的预测置信度估计。 |
| [^126] | [TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support.](http://arxiv.org/abs/2306.13339) | TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。 |
| [^127] | [Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes.](http://arxiv.org/abs/2306.12045) | 本研究提出 TeCoS-LVM 模型，使用脉冲神经元以模拟自然视觉刺激的神经响应。该模型能够自适应地探索和利用刺激序列中的时间依赖关系，避免丢失脉冲列中的信息。 |
| [^128] | [The Implicit Bias of Batch Normalization in Linear Models and Two-layer Linear Convolutional Neural Networks.](http://arxiv.org/abs/2306.11680) | 本文研究了使用批规范化训练线性模型和两层线性卷积神经网络时的隐式偏差，并证明批规范化对于均匀间隔具有隐含偏差。通过两个例子，我们发现在特定学习问题中，均匀间隔分类器的表现甚至优于最大间隔分类器。 |
| [^129] | [Single-Model Attribution via Final-Layer Inversion.](http://arxiv.org/abs/2306.06210) | 本文提出了一种利用最终层反演和异常检测的开放式单模型归因方法，解决了以往方法要么局限于封闭式环境、要么需要对生成模型进行不必要的改变的问题。实验结果表明该方法优于现有方法。 |
| [^130] | [Realising Synthetic Active Inference Agents, Part II: Variational Message Updates.](http://arxiv.org/abs/2306.02733) | 本文讨论了解决广义自由能（FE）目标的合成主动推理代理的变分信息更新和消息传递算法，通过对T形迷宫导航任务的模拟比较，表明AIF可引起认知行为。 |
| [^131] | [Smooth Monotonic Networks.](http://arxiv.org/abs/2306.01147) | 本文提出了一种新的神经网络模块--平滑min-max(SMM)网络，相比于传统的min-max(MM)神经网络结构简单易用，在单调建模方面表现优异。 |
| [^132] | [Distilling BlackBox to Interpretable models for Efficient Transfer Learning.](http://arxiv.org/abs/2305.17303) | 本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。 |
| [^133] | [Refocusing Is Key to Transfer Learning.](http://arxiv.org/abs/2305.15542) | 这篇论文提出了一种名为 TOAST 的迁移学习算法，通过重新聚焦注意力，选择与任务相关的元素并反馈回模型，有效地提高了细粒度视觉分类数据集的性能，同时具有小部分可调参数。 |
| [^134] | [Continual Learning on Dynamic Graphs via Parameter Isolation.](http://arxiv.org/abs/2305.13825) | 提出了Parameter Isolation GNN (PI-GNN)模型，用于处理动态图上的持续学习任务。该模型通过参数隔离和扩展来避免学习新模式和保留旧模式之间的权衡。 |
| [^135] | [Successive Affine Learning for Deep Neural Networks.](http://arxiv.org/abs/2305.07996) | 本文提出了一种连续仿射学习（SAL）模型，用于构建深度神经网络(DNNs)。 该模型通过解决涉及激活函数的二次/凸优化问题来学习仿射映射，但是在权重矩阵和偏差之后。 |
| [^136] | [Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins.](http://arxiv.org/abs/2305.04934) | 本研究使用基于语言模型的深度学习策略，在蛋白质建模中应用transformer和图卷积的结构预训练生成模型，进一步训练后能够设计具有特定性质的蛋白质，案例验证表明该方法可生成理想目标性质的蛋白质。 |
| [^137] | [Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models.](http://arxiv.org/abs/2305.03829) | 本研究采用贝叶斯深度学习估计多种治疗的因果后验分布，提高了基于图像的精准医疗的不确定性估计方法，以预测噪声多的医疗环境下的个体治疗效果。 |
| [^138] | [Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study.](http://arxiv.org/abs/2305.03017) | 本研究使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐的质量，重点关注于Stack Overflow上的Java编程语言。研究使用BERT将代码示例转换为数值向量。 |
| [^139] | [Hyper-parameter Tuning for Adversarially Robust Models.](http://arxiv.org/abs/2304.02497) | 本文探究对抗训练模型的超参数调节问题，明确对抗环境下需要额外调整的超参数，并提出利用廉价对抗训练方法的新方案降低调节成本。 |
| [^140] | [I2I: Initializing Adapters with Improvised Knowledge.](http://arxiv.org/abs/2304.02168) | 本文提出了一种称为ImprovisetoInitialize(I2I)的连续学习算法，通过提取先前学习的任务适配器的知识来为即将到来的任务初始化适配器。这使得从一个任务到另一个任务的知识传递更加高效。 |
| [^141] | [Diagnosing Model Performance Under Distribution Shift.](http://arxiv.org/abs/2303.02011) | 本研究提出一种名为 DISDE 的方法，用于分析模型在不同分布情况下的性能变化。该方法将性能下降分解为三个方面：难度更大但更频繁出现的示例增加、特征和结果之间关系的变化和在训练期间不频繁或未见过的示例性能差。 |
| [^142] | [Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB.](http://arxiv.org/abs/2303.00890) | 本研究比较了BBOB上五种高维贝叶斯优化算法与传统方法以及CMA-ES算法的性能，结果表明... (根据论文的具体内容进行总结) |
| [^143] | [Directed Diffusion: Direct Control of Object Placement through Attention Guidance.](http://arxiv.org/abs/2302.13153) | 本论文介绍了一种有导向性的扩散方法，通过关注引导在图像中直接控制对象的放置。通过观察提示词的交叉注意力映射，引入优化目标，在特定位置产生“激活”，从而改进了场景组合能力。 |
| [^144] | [Restoring the saturation response of a PMT using pulse-shape and artificial-neural-networks.](http://arxiv.org/abs/2302.06170) | 该论文提出了使用脉冲形状和人工神经网络恢复PMT饱和响应的方法，可以估计线性区域并提高光子计数和能量重建效率。 |
| [^145] | [Hierarchical Classification of Research Fields in the "Web of Science" Using Deep Learning.](http://arxiv.org/abs/2302.00390) | 本文提出了一个使用深度学习进行层次分类的系统，可以自动将学术出版物通过抽象进行分类，实现了对研究活动在不同层次结构中的全面分类，并允许跨学科和跨领域的单标签和多标签分类。 |
| [^146] | [Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays.](http://arxiv.org/abs/2301.12636) | 本研究系统地评估了不同的增强方法对学习到的胸部 X 光片异常检测的孪生表示的质量和鲁棒性的影响。结果显示，我们找到了一组能够产生良好泛化效果的鲁棒表示的增强方法。 |
| [^147] | [Adapting Neural Link Predictors for Complex Query Answering.](http://arxiv.org/abs/2301.12313) | 本文提出通过训练一个参数高效的分数适应模型来重新校准神经链接预测分数以解决神经链接预测器在复杂查询回答中的问题。 |
| [^148] | [ClimaX: A foundation model for weather and climate.](http://arxiv.org/abs/2301.10343) | ClimaX是一种灵活且可推广的深度学习模型，用于天气和气候科学，可以使用不同数据集进行训练。 |
| [^149] | [Solving PDEs with Unmeasurable Source Terms Using Coupled Physics-Informed Neural Network with Recurrent Prediction in Soft Sensor Modeling.](http://arxiv.org/abs/2301.08618) | 提出了一种带循环预测学习策略的耦合物理信息神经网络（CPINN），用于软传感建模中的含非测量源项的PDEs求解，证明了CPINN具有满足PDEs解的近似容量，提高了软传感建模在时空工业系统中的性能表现。 |
| [^150] | [Optimal Algorithms for Latent Bandits with Cluster Structure.](http://arxiv.org/abs/2301.07040) | 本文提出了一种名为LATTICE的方法，用于解决具有聚类结构的潜在bandit问题，并在最小化遗憾方面达到了最优解。 |
| [^151] | [Distributed Pruning Towards Tiny Neural Networks in Federated Learning.](http://arxiv.org/abs/2212.01977) | 该论文提出了FedTiny，一个用于联邦学习的分布式剪枝框架，可以为内存和计算受限的设备生成专门的小型模型。在这里，作者引入了自适应的批归一化选择模块来解决剪枝中的偏差问题。 |
| [^152] | [Adversarial Cheap Talk.](http://arxiv.org/abs/2211.11030) | 本文提出了一种新型对抗性设置，在其中对手只能将信息附加到受害者的观察中，从而产生最小的影响范围，并提出对抗性廉价交流（ACT）算法进行对手训练。在高度受限的情况下，使用ACT训练的对手仍会对受害者的训练和测试表现产生显著影响，揭示了强化学习算法中的一种新的攻击向量。 |
| [^153] | [Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges.](http://arxiv.org/abs/2211.08413) | 本文提出去中心化联邦学习（DFL）来解决传统中心化FL（CFL）模型中的问题，主要研究DFL与CFL的差异、DFL的基础理论、DFL框架的设计与评估以及DFL在实际应用中的场景。 |
| [^154] | [A Survey on Explainable Anomaly Detection.](http://arxiv.org/abs/2210.06959) | 该调查旨在提供关于最先进的可解释异常检测技术的全面和结构化的信息，帮助实践者和研究人员找到最适合他们需求的方法。 |
| [^155] | [Prediction intervals for neural network models using weighted asymmetric loss functions.](http://arxiv.org/abs/2210.04318) | 本论文提出了一种使用加权不对称损失函数的方法，生成可靠的预测区间，适用于复杂的机器学习情境，可扩展为参数化函数的PI预测。 |
| [^156] | [Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks.](http://arxiv.org/abs/2209.10200) | 本文提出了一种在联邦学习中应用模型量化的方案，通过联邦学习的可变位宽优化来提高无线通信和计算效率，在无线资源受限的情况下，采用多尺度量化联邦学习聚合算法，能够有效改善联邦学习的性能，具备更高的收敛速度和更少的训练损失。 |
| [^157] | [Robust Inference of Manifold Density and Geometry by Doubly Stochastic Scaling.](http://arxiv.org/abs/2209.08004) | 本论文提出了一种对高维噪声下的流形密度和几何进行稳健推断的方法，通过双重随机缩放高斯核进行标准化，以解决高维噪声对传统标准化方法的不准确性问题。 |
| [^158] | [Action-based Early Autism Diagnosis Using Contrastive Feature Learning.](http://arxiv.org/abs/2209.05379) | 本研究提出了一种基于对比特征学习的基于行为的早期自闭症诊断方法，通过简单且少量的被试动作视频剪辑自动化诊断自闭症，克服了可用数据量小和样本变化大的挑战。 |
| [^159] | [Forming Trees with Treeformers.](http://arxiv.org/abs/2207.06960) | 本文介绍了一种Treeformer模块，它借鉴了CKY算法，通过学习组合运算符和汇聚函数来构建短语和句子的层次编码，从而将层次结构纳入Transformer模型中。实验证明，这种模块在组合泛化和各种自然语言任务中取得了显著的改进。 |
| [^160] | [High Dimensional Quantum Machine Learning With Small Quantum Computers.](http://arxiv.org/abs/2203.13739) | 本研究探索了使用小型量子计算机进行高维量子机器学习的可能性，并且通过构建一个机器学习模型成功地在数字识别任务中实现了利用更少的电路评估来近似较大电路的输出。 |
| [^161] | [Isotuning With Applications To Scale-Free Online Learning.](http://arxiv.org/abs/2112.14586) | 我们提出了一种用于无标度在线学习的等调节技术，该技术具有快速、自适应、随时随地和无标度的特点，并可以自动适应遗憾的速率。同时，我们还引入了在线校正的方法来改进算法的性能。 |
| [^162] | [RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes.](http://arxiv.org/abs/2112.13934) | RELDEC是一种基于强化学习的中等长度LDPC码的解码方法，通过训练智能体顺序调度CN集群，以优化解码策略。同时，通过改进MDP的状态空间表示，适用于更大块长的LDPC码。为了解决不同信道条件下的解码问题，提出了AM-RELDEC算法。 |
| [^163] | [The Statistical Complexity of Interactive Decision Making.](http://arxiv.org/abs/2112.13487) | 本论文提出了一种复杂度度量，决策估计系数，用于解决交互式学习中的样本高效问题，并证明其为样本高效交互式学习的必要且充分条件。 |
| [^164] | [Responsive parallelized architecture for deploying deep learning models in production environments.](http://arxiv.org/abs/2112.08933) | 本研究设计和提出了一个可响应的并行化架构，用于在实时生产环境中部署深度学习模型。采用层次化细化的标签注意力网络预测CV实体，并使用多个深度学习模型并行预测。通过选择轻量级微型Web框架和使用微服务来部署大型深度学习模型管道，达到在少于700毫秒的时间内解析普通CV的目的。 |
| [^165] | [Hybrid quantum-classical machine learning for generative chemistry and drug design.](http://arxiv.org/abs/2108.11644) | 本研究构建了一个混合的量子经典机器学习模型，利用深度生成化学模型加速药物发现。通过在D-Wave量子退火器上训练，成功生成了具有药物化学和合成可及性特性的2331个新化学结构。 |
| [^166] | [To Raise or Not To Raise: The Autonomous Learning Rate Question.](http://arxiv.org/abs/2106.08767) | 自主学习率控制器是一个解决深度学习中学习率设置问题的新方法，旨在避免耗时且琐碎的选择和修改过程，以及对网络架构、优化器、数据集和初始条件的微小变化的敏感性。 |
| [^167] | [Automating Augmentation Through Random Unidimensional Search.](http://arxiv.org/abs/2106.08756) | 通过随机单维搜索的数据增强自动化方法，仅使用6次训练即可获得与使用100次训练相当的性能。 |
| [^168] | [Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches.](http://arxiv.org/abs/2105.06295) | 本研究使用单个加速度计对DMD患者和正常儿童的步态特征进行了比较，采取了传统机器学习和深度学习方法，从而实现了对两组之间的区分。 |
| [^169] | [Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN.](http://arxiv.org/abs/2011.12177) | 我们提出了一种名为GOTHIC的算法，用于检测双核星系，发现了许多双重活动星系核。我们在大样本中检测到了159个双重AGN，其中2个是三重AGN系统。 |
| [^170] | [Randomized Exploration in Generalized Linear Bandits.](http://arxiv.org/abs/1906.08947) | 本文研究了广义线性赌臂问题中的两种随机算法，GLM-TSL和GLM-FPL。GLM-TSL从后验分布中采样广义线性模型，GLM-FPL则将广义线性模型拟合到过去奖励的随机扰动历史中。我们分析了这两种算法并得出了它们的遗憾上界，此前的工作中的遗憾上界得到了改进，并且对于非线性模型中的高斯噪声扰动问题，GLM-FPL是首次尝试。我们在逻辑赌臂问题和神经网络赌臂问题上对这两种算法进行了实证评估。这项工作展示了随机化在探索中的作用，超越了仅仅进行后验采样。 |
| [^171] | [Perturbed-History Exploration in Stochastic Linear Bandits.](http://arxiv.org/abs/1903.09132) | 我们提出了一种在线算法，通过在训练于其干扰历史的线性模型上选择估计奖励最高的臂，用于在随机线性赌博机中最小化累积遗憾。我们推导出了一个关于算法遗憾的较好界限，并通过实证评估展示了算法的实用性。 |

# 详细

[^1]: 序列推荐中对比学习模型的Fisher加权合并

    Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation. (arXiv:2307.05476v1 [cs.IR])

    [http://arxiv.org/abs/2307.05476](http://arxiv.org/abs/2307.05476)

    本文首次将Fisher合并方法应用于序列推荐中，通过合并多个模型的参数来改善整体性能，从而解决了实际挑战，具有推动最新技术的潜力。

    

    随着在线平台和服务的指数增长，推荐系统已成为根据用户偏好识别相关物品的必备工具。序列推荐的领域旨在捕捉用户随时间变化的偏好。为了解决动态偏好，已提出了各种对比学习方法来应对推荐系统中由于有限的用户-物品交互而导致的数据稀疏性挑战。在本文中，我们首次将Fisher合并方法应用于序列推荐中，解决并解决了与之相关的实际挑战。这种方法通过合并多个模型的参数来确保鲁棒微调，从而改善整体性能。通过大量实验，我们证明了我们提出的方法的有效性，并突出了它们在序列学习和推荐系统中推动最新技术的潜力。

    Along with the exponential growth of online platforms and services, recommendation systems have become essential for identifying relevant items based on user preferences. The domain of sequential recommendation aims to capture evolving user preferences over time. To address dynamic preference, various contrastive learning methods have been proposed to target data sparsity, a challenge in recommendation systems due to the limited user-item interactions. In this paper, we are the first to apply the Fisher-Merging method to Sequential Recommendation, addressing and resolving practical challenges associated with it. This approach ensures robust fine-tuning by merging the parameters of multiple models, resulting in improved overall performance. Through extensive experiments, we demonstrate the effectiveness of our proposed methods, highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
    
[^2]: AdaptiveRec：在顺序推荐中自适应构建对比学习的解决方案

    AdaptiveRec: Adaptively Construct Pairs for Contrastive Learning in Sequential Recommendation. (arXiv:2307.05469v1 [cs.IR])

    [http://arxiv.org/abs/2307.05469](http://arxiv.org/abs/2307.05469)

    本文提出了AdaptiveRec，这是一种在顺序推荐中解决对比学习挑战的自适应方法，通过改善嵌入质量和减轻误判问题来提高效果，并在各种推荐场景中展示了其灵活性和适用性的价值。

    

    本文针对顺序推荐系统中对比学习所面临的挑战提出了一种解决方案。具体而言，它解决了误判的问题，该问题限制了推荐算法的有效性。通过引入先进的对比学习方法，所提出的方法改善了物品嵌入的质量，并减轻了将相似实例错误地归类为不相似的问题。实验证明，与现有系统相比，该方法提供了性能的提升。所提出的方法在各种推荐场景中的灵活性和适用性进一步凸显了它在增强顺序推荐系统中的价值。

    This paper presents a solution to the challenges faced by contrastive learning in sequential recommendation systems. In particular, it addresses the issue of false negative, which limits the effectiveness of recommendation algorithms. By introducing an advanced approach to contrastive learning, the proposed method improves the quality of item embeddings and mitigates the problem of falsely categorizing similar instances as dissimilar. Experimental results demonstrate performance enhancements compared to existing systems. The flexibility and applicability of the proposed approach across various recommendation scenarios further highlight its value in enhancing sequential recommendation systems.
    
[^3]: 通过语言类型特征增强跨语言行为测试的 NLP 模型

    Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features. (arXiv:2307.05454v1 [cs.CL])

    [http://arxiv.org/abs/2307.05454](http://arxiv.org/abs/2307.05454)

    这项研究提出了一个形态学感知框架 M2C，可以通过生成测试来评估 NLP 模型在不同语言特征下的行为。研究发现，在英语中，模型在大多数测试中表现出色，但在斯瓦希里语的时间表达和芬兰语的合成所有格等特定类型特征上泛化能力较差。这些结果促使我们开发能够解决这些盲点的模型。

    

    开发面向世界各语言的 NLP 系统的一个挑战是理解它们在与真实世界应用相关的类型上的泛化能力。为此，我们提出了 M2C，一个对 NLP 模型进行行为测试的形态学感知框架。我们使用 M2C 生成测试，以探究模型在12种类型多样的语言中针对特定语言特征表现的行为。我们在生成的测试上评估最先进的语言模型。虽然模型在英语上的大多数测试上表现出色，但我们强调了在斯瓦希里语的时间表达和芬兰语的合成所有格等特定类型特征的泛化失败。我们的发现促进了开发能够解决这些盲点的模型。

    A challenge towards developing NLP systems for the world's languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models' behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish. Our findings motivate the development of models that address these blind spots.
    
[^4]: ISLTranslate: 翻译印度手语的数据集

    ISLTranslate: Dataset for Translating Indian Sign Language. (arXiv:2307.05440v1 [cs.CL])

    [http://arxiv.org/abs/2307.05440](http://arxiv.org/abs/2307.05440)

    ISLTranslate是一个包含31k个ISL-英语句子/短语对的最大连续印度手语翻译数据集，该数据集帮助开发手语翻译系统，解决印度手语资源匮乏的问题。

    

    手语是全球许多听障人士的主要通信方式。最近，为了弥补听障社区与其他人群之间的沟通差距，提出了几个手语翻译数据集，以便开发统计手语翻译系统。然而，印度手语的资源匮乏。本资源论文介绍了ISLTranslate，一个用于连续印度手语（ISL）的翻译数据集，包含31k个ISL-英语句子/短语对。据我们所知，这是连续印度手语最大的翻译数据集。我们对数据集进行了详细分析。为了验证现有的端到端手语到口语翻译系统的性能，我们使用基于Transformer模型的ISL翻译对创建的数据集进行了基准测试。

    Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language. We provide a detailed analysis of the dataset. To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation.
    
[^5]: 受限扩散模型的城市采样

    Metropolis Sampling for Constrained Diffusion Models. (arXiv:2307.05439v1 [cs.LG])

    [http://arxiv.org/abs/2307.05439](http://arxiv.org/abs/2307.05439)

    本文提出了一种基于城市采样的简单加噪方案，用于解决受约束的扩散模型的生成建模问题。相较于现有的采样器，该方案在计算效率和实证性能方面都有明显提升，并且被证明是反射布朗运动的有效离散化。

    

    去噪扩散模型最近已经成为生成建模的主要范式。它们对黎曼流形的扩展使得它们能够在自然科学中的一系列问题上得到应用。然而，在许多实际情况中，这些流形由一组约束定义，并且不被现有的（黎曼）扩散模型方法所覆盖。最近的工作通过使用基于对数障碍方法或反射布朗运动的新型加噪过程来解决这个问题。然而，随着约束复杂度的增加，相关的采样器计算负担较重。在本文中，我们引入了一种基于城市采样的替代简单加噪方案，与早期的采样器相比，计算效率和实证性能都有很大提升。在独立的兴趣方面，我们证明了这个新过程对应于反射布朗运动的有效离散化。

    Denoising diffusion models have recently emerged as the predominant paradigm for generative modelling. Their extension to Riemannian manifolds has facilitated their application to an array of problems in the natural sciences. Yet, in many practical settings, such manifolds are defined by a set of constraints and are not covered by the existing (Riemannian) diffusion model methodology. Recent work has attempted to address this issue by employing novel noising processes based on logarithmic barrier methods or reflected Brownian motions. However, the associated samplers are computationally burdensome as the complexity of the constraints increases. In this paper, we introduce an alternative simple noising scheme based on Metropolis sampling that affords substantial gains in computational efficiency and empirical performance compared to the earlier samplers. Of independent interest, we prove that this new process corresponds to a valid discretisation of the reflected Brownian motion. We dem
    
[^6]: 用深度学习提高智能手表支付的安全性

    Improving the Security of Smartwatch Payment with Deep Learning. (arXiv:2307.05437v1 [cs.CR])

    [http://arxiv.org/abs/2307.05437](http://arxiv.org/abs/2307.05437)

    本论文通过深度学习方法提高了智能手表支付的安全性，包括构建了一个超越最先进技术的深度学习认证系统和开发一个生成合成手势的模型。这些方法可以减少用户在认证系统中提供手势的数量，从而提高系统性能。

    

    使用智能手表进行非接触式支付越来越受欢迎，但该支付方式缺乏传统的生物识别安全措施，如面部或指纹识别。在2022年，Sturgess等人提出了一种名为WatchAuth的系统，用于通过向支付终端伸手的物理手势来认证智能手表支付。虽然有效，但该系统需要用户经历繁重的注册过程才能达到可接受的错误率。在本论文中，我们探索深度学习应用是否可以减少用户在智能手表支付认证系统中必须提供的手势数量。我们首先构建了一个超越当前最先进技术的深度学习认证系统，包括在目标用户提供有限手势的情况下。然后，我们开发了一个正则化的自编码器模型来生成用户特定的合成手势。我们证明在训练中使用这些手势可以改善系统的性能。

    Making contactless payments using a smartwatch is increasingly popular, but this payment medium lacks traditional biometric security measures such as facial or fingerprint recognition. In 2022, Sturgess et al. proposed WatchAuth, a system for authenticating smartwatch payments using the physical gesture of reaching towards a payment terminal. While effective, the system requires the user to undergo a burdensome enrolment period to achieve acceptable error levels. In this dissertation, we explore whether applications of deep learning can reduce the number of gestures a user must provide to enrol into an authentication system for smartwatch payment. We firstly construct a deep-learned authentication system that outperforms the current state-of-the-art, including in a scenario where the target user has provided a limited number of gestures. We then develop a regularised autoencoder model for generating synthetic user-specific gestures. We show that using these gestures in training improve
    
[^7]: One-Versus-Others Attention: 可扩展的多模态集成

    One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])

    [http://arxiv.org/abs/2307.05435](http://arxiv.org/abs/2307.05435)

    提出了一种可扩展的多模态集成方法，通过一对多（OvO）注意力机制解决了多模态学习中超过三个模态的注意力计算问题。

    

    随着多模态学习模型在问题回答和自动驾驶等各种任务上超越单模态方法，多模态学习模型变得日益重要。尽管多模态学习的重要性，现有的工作仅关注于自然语言处理应用，其中模态数通常少于四个（音频、视频、文本、图像）。然而，在其他领域，如医疗领域，数据输入可能包括X射线、PET扫描、MRI、遗传筛查、临床笔记等，这就需要高效而准确的信息融合。许多最先进的模型依赖于两两跨模态注意力，但对于超过三个模态的应用，这种方法不会很好地扩展。对于$n$个模态，计算注意力将导致$n \choose 2$的复杂度，可能需要大量的计算资源。为了解决这个问题，我们提出了一种新的领域中立的注意力机制，即一对多（OvO）注意力，该机制随着模态数量线性扩展。

    Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with
    
[^8]: 利用李对称的自监督学习解决偏微分方程问题

    Self-Supervised Learning with Lie Symmetries for Partial Differential Equations. (arXiv:2307.05432v1 [cs.LG])

    [http://arxiv.org/abs/2307.05432](http://arxiv.org/abs/2307.05432)

    本研究通过自监督学习的方法，利用李对称将异构数据中的PDEs表示进行优化，提高了不变任务的性能并改进了神经求解器的时间推进性能。

    

    差分方程的机器学习为数值求解器提供了计算效率高的替代方法，可能在科学和工程领域产生广泛影响。本研究通过实施联合嵌入方法的自监督学习（SSL）框架，从异构数据中学习PDEs的通用表示，该框架是一种无监督表示学习方法，在计算机视觉领域取得了显著的成功。我们的表示优于基线方法在不变任务（如回归PDE的系数）上的表现，同时提高神经求解器的时间推进性能。我们希望我们提出的方法将在未来的通用基础模型的发展中发挥作用。

    Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation mode
    
[^9]: 几何神经扩散过程

    Geometric Neural Diffusion Processes. (arXiv:2307.05431v1 [stat.ML])

    [http://arxiv.org/abs/2307.05431](http://arxiv.org/abs/2307.05431)

    本文将扩散模型的框架应用于无限维建模，并引入几何先验以处理在非欧几里得空间中带有对称性的数据。通过构建具有对称群变换的几何高斯过程和等变神经网络逼近得分，生成函数模型也具有相同的对称性。

    

    降噪扩散模型已被证明是一种灵活且有效的生成建模范式。最近将其扩展到无限维欧氏空间使得可以对随机过程进行建模。然而，自然科学中的许多问题都涉及对称性和存在于非欧几里得空间中的数据。在本文中，我们将扩散模型的框架扩展到无限维建模中引入一系列几何先验。我们通过 a) 构建一个噪声过程，其极限分布是在感兴趣的对称群下变换的几何高斯过程，并 b) 使用对这个群具有等变性的神经网络来逼近得分。我们表明，在这些条件下，生成函数模型具有相同的对称性。我们使用一种新颖的基于 Langevin 的条件采样器展示了模型的可扩展性和容量性，以适应复杂的标量和向量场，这些场存在于欧氏空间和球形空间中。

    Denoising diffusion models have proven to be a flexible and effective paradigm for generative modelling. Their recent extension to infinite dimensional Euclidean spaces has allowed for the modelling of stochastic processes. However, many problems in the natural sciences incorporate symmetries and involve data living in non-Euclidean spaces. In this work, we extend the framework of diffusion models to incorporate a series of geometric priors in infinite-dimension modelling. We do so by a) constructing a noising process which admits, as limiting distribution, a geometric Gaussian process that transforms under the symmetry group of interest, and b) approximating the score with a neural network that is equivariant w.r.t. this group. We show that with these conditions, the generative functional model admits the same symmetry. We demonstrate scalability and capacity of the model, using a novel Langevin-based conditional sampler, to fit complex scalar and vector fields, with Euclidean and sph
    
[^10]: 使用卷积神经网络（CNN）和BOLD-fMRI计算人类连接组发展队列中的呼吸体积与时间（RTV）和呼吸变异（RV）

    Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort. (arXiv:2307.05426v1 [eess.SP])

    [http://arxiv.org/abs/2307.05426](http://arxiv.org/abs/2307.05426)

    该研究提出了一种使用卷积神经网络（CNN）和BOLD-fMRI计算人类连接组发展队列中呼吸体积与时间（RTV）和呼吸变异（RV）的方法。实验结果表明，CNN可以从静息状态的BOLD信号中捕捉有信息的特征，并重建真实的呼吸时序。这种方法可以降低fMRI研究的成本，并减轻参与者的负担。

    

    在许多fMRI研究中，呼吸信号不可用或质量不可接受。因此，无法直接从BOLD信号中去除低频呼吸变化。本研究提出了一种一维CNN模型，用于重建两个呼吸测量指标，RV和RVT。结果表明，CNN可以从静息状态的BOLD信号中捕捉有信息的特征，并重建真实的RV和RVT时序。预计这种方法的应用将降低fMRI研究的成本，减少复杂性，并减轻参与者的负担，因为他们不需要佩戴呼吸贝洛斯。

    In many fMRI studies, respiratory signals are unavailable or do not have acceptable quality. Consequently, the direct removal of low-frequency respiratory variations from BOLD signals is not possible. This study proposes a one-dimensional CNN model for reconstruction of two respiratory measures, RV and RVT. Results show that a CNN can capture informative features from resting BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows.
    
[^11]: 黑盒DNN后门检测的触发器和良性特征的差异分析

    Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. (arXiv:2307.05422v1 [cs.CR])

    [http://arxiv.org/abs/2307.05422](http://arxiv.org/abs/2307.05422)

    本文提出了一种在黑盒情况下检测深度神经网络后门攻击的高效方法，通过量化触发器和良性特征对确定后门网络输出的影响，并使用训练好的新颖性检测器进行检测。

    

    本文提出了一种在黑盒情况下，对深度神经网络进行高效检测的方法，用于防御后门攻击。该方法的思路是，与其他良性特征相比，与触发器相关的特征对确定后门网络输出具有更高的影响力。为了定量衡量触发器和良性特征对确定后门网络输出的影响，我们引入了五个度量指标。为了计算给定输入的五个度量值，我们首先通过将输入的部分内容注入到干净的验证样本中生成几个合成样本。然后，通过使用相应合成样本的输出标签计算出五个度量指标。本文的一个贡献是使用了一个小型的干净验证数据集。在计算出五个度量值后，我们从验证数据集中训练出五个新颖性检测器。一个元新颖性检测器将五个训练好的新颖性检测器的输出进行融合。

    This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario. The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features. To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics. To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples. Then, the five metrics are computed by using the output labels of the corresponding synthetic samples. One contribution of this work is the use of a tiny clean validation dataset. Having the computed five metrics, five novelty detectors are trained from the validation dataset. A meta novelty detector fuses the output of the five trained novelty detectors to 
    
[^12]: 提升自适应学习分数来增加交互式强化学习的反馈效率

    Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores. (arXiv:2307.05405v1 [cs.RO])

    [http://arxiv.org/abs/2307.05405](http://arxiv.org/abs/2307.05405)

    本文提出一种利用人类提供的分数来改进交互式强化学习的反馈效率的方法。通过使用分数代替成对偏好，我们可以获得更多的数据。我们通过对机器人任务的评估表明，该方法能够通过自适应学习分数有效地学习到接近最优的策略。

    

    交互式强化学习在学习复杂的机器人任务方面显示出潜力。然而，由于需要大量的交互反馈，这一过程可能需要人工参与。本文提出了一种新的方法，利用人类提供的分数而不是成对偏好，来提高交互式强化学习的反馈效率。我们的关键洞察是，分数可以产生比成对偏好更多的数据。具体而言，在稀疏奖励环境中，我们要求教师与代理交互评分全面的轨迹来训练行为策略。为了避免人类给出的不稳定分数对训练过程产生负面影响，我们提出了一种自适应学习方案。这使得学习范式对于不完美或不可靠的分数不敏感。我们对机器人运动和操作任务进行了广泛评估。结果表明，所提出的方法可以通过自适应学习分数有效地学习到接近最优的策略。

    Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adap
    
[^13]: 在文档处理平台中用于类增量连续学习的领域无关神经架构

    Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform. (arXiv:2307.05399v1 [cs.LG])

    [http://arxiv.org/abs/2307.05399](http://arxiv.org/abs/2307.05399)

    在文档处理平台中，我们提出了一种领域无关的神经架构，能够在每个类别的示例单独呈现时训练高性能分类器，无需使用记忆缓冲区，并在实验中取得了超越参考方法的结果。

    

    复杂系统中的生产部署要求机器学习架构对多个任务高效可用。特别需要注意的是分类问题，其中数据以流式方式到达，并且每个类别单独呈现。最近的随机梯度学习方法在这种设置中表现不佳，或者存在诸如内存缓冲区的限制，不能在现实场景中使用。因此，我们提出了一种基于专家混合模型的全可微架构，可以在每个类别的示例单独呈现时训练高性能分类器。我们进行了详尽的实验证明了其在各个领域的适用性和在线在生产环境中学习的能力。所提出的技术在没有记忆缓冲区的情况下达到了SOTA结果，并明显优于参考方法。

    Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately. We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods.
    
[^14]: 化学性质预测的Simplicial信息传递

    Simplicial Message Passing for Chemical Property Prediction. (arXiv:2307.05392v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.05392](http://arxiv.org/abs/2307.05392)

    这篇论文提出了一种Simplicial信息传递（SMP）框架，用于捕捉分子中隐藏的拓扑信息，并在化学性质预测方面取得了显著进展。

    

    最近，消息传递神经网络（MPNN）为处理分子图提供了一种有前途的工具，并在促进发现和设计具有所需性质的材料方面取得了显著成功。然而，经典的MPNN方法在捕捉分子结构中隐藏的强拓扑信息（如非同构图）方面也存在局限性。为了解决这个问题，本文提出了一种Simplicial信息传递（SMP）框架，以更好地捕捉分子中的拓扑信息，从而突破了传统信息传递范式的局限。在SMP中，建立了一个广义信息传递框架，用于聚集任意次序的simplicial复合体中的信息，并详细阐述了一个分层结构，允许不同次序的simplices之间的信息交流。我们将SMP框架应用于深度学习体系结构中的量子化学性质预测，并取得了状态。

    Recently, message-passing Neural networks (MPNN) provide a promising tool for dealing with molecular graphs and have achieved remarkable success in facilitating the discovery and materials design with desired properties. However, the classical MPNN methods also suffer from a limitation in capturing the strong topological information hidden in molecular structures, such as nonisomorphic graphs. To address this problem, this work proposes a Simplicial Message Passing (SMP) framework to better capture the topological information from molecules, which can break through the limitation within the vanilla message-passing paradigm. In SMP, a generalized message-passing framework is established for aggregating the information from arbitrary-order simplicial complex, and a hierarchical structure is elaborated to allow information exchange between different order simplices. We apply the SMP framework within deep learning architectures for quantum-chemical properties prediction and achieve state-o
    
[^15]: CrysMMNet: 多模态表示用于晶体属性预测

    CrysMMNet: Multimodal Representation for Crystal Property Prediction. (arXiv:2307.05390v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.05390](http://arxiv.org/abs/2307.05390)

    CrysMMNet是一种多模态表示方法，通过使用文本描述建模全局结构信息来预测晶体属性。

    

    机器学习模型已经成为快速准确预测不同晶体性质的强大工具。现有的最先进模型依赖于晶体数据的单一模态，即晶体图结构，在三维空间中建立相邻原子之间的边，并应用GNN学习材料表示。因此，它们成功地编码了原子周围的局部化学语义，但未能捕捉到影响不同晶体属性的重要的全局周期结构信息，如空间群数、晶体对称性、旋转信息等。在这项工作中，我们利用材料的文本描述将全局结构信息建模为图结构，并学习晶体材料更加稳健丰富的表示。为此，我们首先整理了一个包含每个材料描述的晶体材料数据库的文本数据集。此外，我们提出了CrysMMNet，一个简单的多模态表示方法。

    Machine Learning models have emerged as a powerful tool for fast and accurate prediction of different crystalline properties. Exiting state-of-the-art models rely on a single modality of crystal data i.e. crystal graph structure, where they construct multi-graph by establishing edges between nearby atoms in 3D space and apply GNN to learn materials representation. Thereby, they encode local chemical semantics around the atoms successfully but fail to capture important global periodic structural information like space group number, crystal symmetry, rotational information, etc, which influence different crystal properties. In this work, we leverage textual descriptions of materials to model global structural information into graph structure and learn a more robust and enriched representation of crystalline materials. To this effect, we first curate a textual dataset for crystalline material databases containing descriptions of each material. Further, we propose CrysMMNet, a simple multi
    
[^16]: 学习核技术用于可解释和高效的PPG信号质量评估和伪影分割

    Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])

    [http://arxiv.org/abs/2307.05385](http://arxiv.org/abs/2307.05385)

    本文提出了一种通过学习核技术，具有解释性且参数较少的方法来评估和分割PPG信号的质量和伪影，与现有的深度神经网络方法相比有着类似甚至更好的性能。

    

    光电容抗(PPG)提供了一种低成本、非侵入性的方法来持续监测各种心血管参数。PPG信号由可穿戴设备产生，常常包含由外部因素(如人体运动)引起的大型伪影。为了确保对生理参数进行稳健和准确的提取，信号的损坏区域需要被正确地识别和处理。之前的方法依靠手工特征检测器或信号度量，结果性能不佳，或依靠深度神经网络(DNN)等机器学习技术，缺乏可解释性，计算和内存密集。在这项工作中，我们提出了一种新的方法，学习一小组可解释的卷积核，其性能与现有技术DNN方法相似，甚至更好，而参数数量比DNN方法少几个数量级。这项工作实现了高效、稳健和可解释的PPG信号质量评估和伪影分割。

    Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
    
[^17]: 随机嵌套构成的双层优化用于鲁棒特征学习

    Stochastic Nested Compositional Bi-level Optimization for Robust Feature Learning. (arXiv:2307.05384v1 [math.OC])

    [http://arxiv.org/abs/2307.05384](http://arxiv.org/abs/2307.05384)

    本文提出了一种用于解决嵌套构成双层优化问题的随机逼近算法，可以实现鲁棒特征学习，并且不依赖于矩阵求逆或小批量输入。

    

    我们开发并分析了用于解决嵌套构成双层优化问题的随机逼近算法。这些问题涉及到上层的$T$个潜在非凸平滑函数的嵌套构造，以及下层的平滑且强凸函数。我们的算法不依赖于矩阵求逆或小批量输入，并且可以以近似$\tilde{O}_T(1/\epsilon^{2})$的预算复杂度实现$\epsilon$-稳定解，假设能够得到上层组成中的个体函数和下层函数的随机一阶诺埃尔，这些一阶诺埃尔是无偏且具有有界矩。这里，$\tilde{O}_T$可以隐藏多项对数系数和常数，依赖于$T$。

    We develop and analyze stochastic approximation algorithms for solving nested compositional bi-level optimization problems. These problems involve a nested composition of $T$ potentially non-convex smooth functions in the upper-level, and a smooth and strongly convex function in the lower-level. Our proposed algorithm does not rely on matrix inversions or mini-batches and can achieve an $\epsilon$-stationary solution with an oracle complexity of approximately $\tilde{O}_T(1/\epsilon^{2})$, assuming the availability of stochastic first-order oracles for the individual functions in the composition and the lower-level, which are unbiased and have bounded moments. Here, $\tilde{O}_T$ hides polylog factors and constants that depend on $T$. The key challenge we address in establishing this result relates to handling three distinct sources of bias in the stochastic gradients. The first source arises from the compositional nature of the upper-level, the second stems from the bi-level structure
    
[^18]: 基于皮肤电反应信号特征选择和支持向量机的人类情绪识别方法

    Human Emotion Recognition Based On Galvanic Skin Response signal Feature Selection and SVM. (arXiv:2307.05383v1 [eess.SP])

    [http://arxiv.org/abs/2307.05383](http://arxiv.org/abs/2307.05383)

    本论文提出了一种基于自动选择的皮肤电反应信号特征和支持向量机的人类情绪识别方法，实验结果表明该方法在人类情绪识别方面具有较高的准确率。

    

    本文提出了一种基于自动选择的皮肤电反应（GSR）信号特征和支持向量机（SVM）的新型人类情绪识别方法。通过 e-Health Sensor Platform V2.0 获取了皮肤电反应信号，并使用小波函数对数据进行去噪和归一化以消除个体差异。从归一化的数据中提取了30个特征，但直接使用这些特征会导致识别率较低。为了获得优化的特征，我们采用了基于协方差的特征选择方法。最后，利用包含优化特征的SVM实现了人类情绪识别。实验结果表明，所提出的方法能够实现良好的人类情绪识别，识别准确率超过66.67%。

    A novel human emotion recognition method based on automatically selected Galvanic Skin Response (GSR) signal features and SVM is proposed in this paper. GSR signals were acquired by e-Health Sensor Platform V2.0. Then, the data is de-noised by wavelet function and normalized to get rid of the individual difference. 30 features are extracted from the normalized data, however, directly using of these features will lead to a low recognition rate. In order to gain the optimized features, a covariance based feature selection is employed in our method. Finally, a SVM with input of the optimized features is utilized to achieve the human emotion recognition. The experimental results indicate that the proposed method leads to good human emotion recognition, and the recognition accuracy is more than 66.67%.
    
[^19]: 保护未来: 基于时空建模的新生儿癫痫发作检测

    Protecting the Future: Neonatal Seizure Detection with Spatial-Temporal Modeling. (arXiv:2307.05382v1 [eess.SP])

    [http://arxiv.org/abs/2307.05382](http://arxiv.org/abs/2307.05382)

    本文提出了一种名为STATENet的深度学习框架，通过精心设计在时空和模型层面上解决了新生儿癫痫检测中的独特挑战，实验证明了该框架能显著提高癫痫检测性能。

    

    在新生儿重症监护病房，及时检测具有电脑脑电图（EEG）的新生儿的癫痫发作是一项常见但能拯救生命的实践。然而，实时监测需要人力大量投入，因此需要自动化的新生儿癫痫发作检测解决方案。此外，当前针对成人癫痫监测的自动化方法通常会因为以下原因而失败：（i）人脑中癫痫发作起始位置的动态变化；（ii）新生儿脑电图的不同电极配置以及（iii）不同受试对象之间的巨大分布变化。本文提出了一种名为STATENet的深度学习框架，通过精心设计在时空和模型层面上解决了独特的挑战。对真实的大规模新生儿脑电图数据集的实验表明，我们的框架在癫痫检测性能上取得了显著优势。

    A timely detection of seizures for newborn infants with electroencephalogram (EEG) has been a common yet life-saving practice in the Neonatal Intensive Care Unit (NICU). However, it requires great human efforts for real-time monitoring, which calls for automated solutions to neonatal seizure detection. Moreover, the current automated methods focusing on adult epilepsy monitoring often fail due to (i) dynamic seizure onset location in human brains; (ii) different montages on neonates and (iii) huge distribution shift among different subjects. In this paper, we propose a deep learning framework, namely STATENet, to address the exclusive challenges with exquisite designs at the temporal, spatial and model levels. The experiments over the real-world large-scale neonatal EEG dataset illustrate that our framework achieves significantly better seizure detection performance.
    
[^20]: 优化的晶体学图生成用于材料科学

    Optimized Crystallographic Graph Generation for Material Science. (arXiv:2307.05380v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.05380](http://arxiv.org/abs/2307.05380)

    我们提出了一种在GPU优化中生成晶体材料的图形表示的高效工具，并提供了一个与Pytorch兼容的框架。通过实时生成图，我们的工具能够更新结构的几何形状，并在神经网络的训练过程中处理更新后的图。

    

    图神经网络在应用于化学和材料科学发现中广泛使用。然而，对于晶体材料而言，从几何信息生成基于图的表示对于神经网络来说并不是一项简单的任务。晶体的周期性需要在大规模并行环境下进行高效的实时处理。为了训练基于图的材料发现生成模型，我们提出了一种在GPU优化中生成截断图和k最近邻图的高效工具。我们提供了一个与Pytorch兼容的pyMatGraph框架，在神经网络架构训练期间实时生成图。我们的工具可以更新一个结构的图，使生成模型能够更新几何形状，并在GPU上进行前向传播时处理更新后的图。我们的代码公开可用于https://github.com/aklipf/mat

    Graph neural networks are widely used in machine learning applied to chemistry, and in particular for material science discovery. For crystalline materials, however, generating graph-based representation from geometrical information for neural networks is not a trivial task. The periodicity of crystalline needs efficient implementations to be processed in real-time under a massively parallel environment. With the aim of training graph-based generative models of new material discovery, we propose an efficient tool to generate cutoff graphs and k-nearest-neighbours graphs of periodic structures within GPU optimization. We provide pyMatGraph a Pytorch-compatible framework to generate graphs in real-time during the training of neural network architecture. Our tool can update a graph of a structure, making generative models able to update the geometry and process the updated graph during the forward propagation on the GPU side. Our code is publicly available at https://github.com/aklipf/mat
    
[^21]: M$^2$Hub：释放机器学习在材料发现中的潜力

    M$^2$Hub: Unlocking the Potential of Machine Learning for Materials Discovery. (arXiv:2307.05378v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.05378](http://arxiv.org/abs/2307.05378)

    M$^2$Hub是一个推动机器学习在材料发现中的工具包，通过集成平台提供多样化的材料发现任务、数据集、机器学习方法、评估和基准结果的便捷访问，涵盖虚拟筛选、逆向设计和分子模拟等关键阶段的多种材料类型和属性。

    

    我们引入了M$^2$Hub，这是一个推动机器学习在材料发现中的工具包。机器学习在建模分子结构方面取得了显著进展，尤其是在药物发现中的生物分子。然而，机器学习方法在建模材料结构方面的发展滞后，部分原因是缺乏一个集成平台，能够提供多样化的材料发现任务。为了弥补这一差距，M$^2$Hub将实现对材料发现任务、数据集、机器学习方法、评估和基准结果的便捷访问，涵盖整个工作流程。具体来说，M$^2$Hub的第一个版本重点关注材料发现中的三个关键阶段：虚拟筛选、逆向设计和分子模拟，包括包含56个任务的6种材料类型的9个数据集，涵盖8种材料属性。此外，我们还提供了2个合成数据集，用于材料的生成任务。

    We introduce M$^2$Hub, a toolkit for advancing machine learning in materials discovery. Machine learning has achieved remarkable progress in modeling molecular structures, especially biomolecules for drug discovery. However, the development of machine learning approaches for modeling materials structures lag behind, which is partly due to the lack of an integrated platform that enables access to diverse tasks for materials discovery. To bridge this gap, M$^2$Hub will enable easy access to materials discovery tasks, datasets, machine learning methods, evaluations, and benchmark results that cover the entire workflow. Specifically, the first release of M$^2$Hub focuses on three key stages in materials discovery: virtual screening, inverse design, and molecular simulation, including 9 datasets that covers 6 types of materials with 56 tasks across 8 types of material properties. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. In addition to random 
    
[^22]: 多任务学习以提高相干光系统中神经网络均衡器的泛化能力

    Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems. (arXiv:2307.05374v1 [eess.SP])

    [http://arxiv.org/abs/2307.05374](http://arxiv.org/abs/2307.05374)

    首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性，并且通过一个"单一"的基于神经网络的均衡器，在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。

    

    首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性。与常规数字时钟恢复方法相比，一个"单一"的基于神经网络的均衡器在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。

    For the first time, multi-task learning is proposed to improve the flexibility of NN-based equalizers in coherent systems. A "single" NN-based equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training, even with variations in launch power, symbol rate, or transmission distance.
    
[^23]: 通过SSNet从EEG、EOG和EMG信号中分类睡眠阶段 (arXiv:2307.05373v1 [eess.SP])

    Classification of sleep stages from EEG, EOG and EMG signals by SSNet. (arXiv:2307.05373v1 [eess.SP])

    [http://arxiv.org/abs/2307.05373](http://arxiv.org/abs/2307.05373)

    本研究提出了一个基于深度学习的SSNet框架，通过从EEG、EOG和EMG信号中提取特征，并将其输入到全连接层进行分类，成功实现了睡眠阶段的分类，并在两个公共数据集上获得了高准确度和Kappa系数的结果。

    

    睡眠阶段的分类在诊断睡眠相关疾病，包括睡眠呼吸障碍(SDB)疾病中起着重要作用。在本研究中，我们提出了一个端到端的深度学习架构，命名为SSNet，它包括基于卷积神经网络（CNN）和长短时记忆（LSTM）的两个深度学习网络。两个深度学习网络从电眼图（EOG）、脑电图（EEG）和肌电图（EMG）信号的组合中提取特征，因为每个信号具有不同的特征，有助于睡眠阶段的分类。两个深度学习网络产生的特征被连接起来传递到全连接层用于分类。我们提出的模型在使用两个公共数据集Sleep-EDF Expanded dataset和ISRUC-Sleep dataset进行评估。睡眠-EDF Expanded数据集的三种睡眠阶段分类的准确性和Kappa系数分别为96.36%和93.40%。

    Classification of sleep stages plays an essential role in diagnosing sleep-related diseases including Sleep Disorder Breathing (SDB) disease. In this study, we propose an end-to-end deep learning architecture, named SSNet, which comprises of two deep learning networks based on Convolutional Neuron Networks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks extract features from the combination of Electrooculogram (EOG), Electroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal has distinct features that help in the classification of sleep stages. The features produced by the two-deep learning networks are concatenated to pass to the fully connected layer for the classification. The performance of our proposed model is evaluated by using two public datasets Sleep-EDF Expanded dataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36% and 93.40% respectively, for classifying three classes of sleep stages using Sleep-EDF Expanded da
    
[^24]: Capafoldable: 具有电容传感能力的自追踪可折叠智能纺织品

    Capafoldable: self-tracking foldable smart textiles with capacitive sensing. (arXiv:2307.05370v1 [cs.HC])

    [http://arxiv.org/abs/2307.05370](http://arxiv.org/abs/2307.05370)

    这项工作提出了一种创新的自追踪可折叠智能纺织品，通过结合折叠织物结构和电容传感，利用深度学习技术来检测结构运动。实验结果显示，我们的方法可以从电容信号中准确重构出片段的几何形状。

    

    折叠是一种独特的结构技术，可以使平面材料具有运动或三维力学特性。基于纺织品的电容传感已经被证明对导电纺织品的几何形变和相对运动非常敏感。在这项工作中，我们提出了一种创新的自追踪可折叠智能纺织品，将折叠织物结构和电容传感相结合，利用先进的传感电路和深度学习技术来检测结构运动。我们创建了两种折叠模式，手风琴和齿形，每种模式中都有两种布局的电容传感器，以热粘附的导电纺织品片的形式存在。在手动移动折叠模式的片段的实验中，我们开发了深度神经网络来学习和重构片段的视觉跟踪形状。通过我们的方法，可以从电容信号中重构定义片段形状的几何原语，R-squared值可达95％，22.5cm长片段的追踪误差为1cm。

    Folding is an unique structural technique to enable planer materials with motion or 3D mechanical properties. Textile-based capacitive sensing has shown to be sensitive to the geometry deformation and relative motion of conductive textiles. In this work, we propose a novel self-tracking foldable smart textile by combining folded fabric structures and capacitive sensing to detect the structural motions using state-of-the-art sensing circuits and deep learning technologies. We created two folding patterns, Accordion and Chevron, each with two layouts of capacitive sensors in the form of thermobonded conductive textile patches. In an experiment of manually moving patches of the folding patterns, we developed deep neural network to learn and reconstruct the vision-tracked shape of the patches. Through our approach, the geometry primitives defining the patch shape can be reconstructed from the capacitive signals with R-squared value of up to 95\% and tracking error of 1cm for 22.5cm long pa
    
[^25]: 中子和X射线反射率数据的神经网络分析：融入先验知识解决相位问题

    Neural network analysis of neutron and X-ray reflectivity data: Incorporating prior knowledge for tackling the phase problem. (arXiv:2307.05364v1 [eess.SP])

    [http://arxiv.org/abs/2307.05364](http://arxiv.org/abs/2307.05364)

    本研究提出了一种利用先验知识的神经网络分析方法，用于解决通过中子和X射线反射率曲线确定多层薄膜物理参数的相位问题。实验证明了该方法在不同情景下的有效性，能够改善训练过程并解决问题的欠定性质。

    

    由于缺乏相位信息，从测量的中子和X射线反射率曲线中确定多层薄膜的物理参数，在基本水平上是一个欠定反问题。这个所谓的相位问题限制了标准神经网络的应用，限制了先前机器学习解决方案中考虑的参数范围和数量。为了克服这个问题，我们提出了一种利用先验知识对训练过程进行正则化的方法，以扩大参数空间。我们在多种情景下展示了我们方法的有效性，包括具有箱模型参数化和物理启发式特殊参数化的多层结构的散射长度密度剖面。通过利用先验知识的输入，我们可以改善训练动力学并解决问题的欠定特性。与先前的方法相比，我们的方法在增加参数空间时具有较好的可扩展性。

    Due to the lack of phase information, determining the physical parameters of multilayer thin films from measured neutron and X-ray reflectivity curves is, on a fundamental level, an underdetermined inverse problem. This so-called phase problem poses limitations on standard neural networks, constraining the range and number of considered parameters in previous machine learning solutions. To overcome this, we present an approach that utilizes prior knowledge to regularize the training process over larger parameter spaces. We demonstrate the effectiveness of our method in various scenarios, including multilayer structures with box model parameterization and a physics-inspired special parameterization of the scattering length density profile for a multilayer structure. By leveraging the input of prior knowledge, we can improve the training dynamics and address the underdetermined ("ill-posed") nature of the problem. In contrast to previous methods, our approach scales favorably when increa
    
[^26]: SleepEGAN:一种基于增强型生成对抗网络的集成深度学习模型，用于不平衡的睡眠阶段分类

    SleepEGAN: A GAN-enhanced Ensemble Deep Learning Model for Imbalanced Classification of Sleep Stages. (arXiv:2307.05362v1 [eess.SP])

    [http://arxiv.org/abs/2307.05362](http://arxiv.org/abs/2307.05362)

    本文提出了一种名为SleepEGAN的生成对抗网络（GAN）增强的集成深度学习模型，用于不平衡的睡眠阶段分类。该模型通过使用新的GAN架构进行数据增强，并采用无成本的集成学习策略来解决类别不平衡和个体异质性问题，提高了分类准确性和鲁棒性。

    

    深度神经网络在自动睡眠阶段分类中发挥了重要作用，因为它们具有强大的表示能力和内部特征转换能力。然而，原始睡眠数据中的类别不平衡和个体异质性往往会显著影响任何机器学习算法的分类性能。为了解决这两个问题，本文开发了一种名为SleepEGAN的生成对抗网络（GAN）增强的集成深度学习模型，用于不平衡睡眠阶段的分类。为了缓解类别不平衡问题，我们提出了一种针对EEG信号特征的新GAN架构（称为EGAN），用于数据增强。在训练过程中使用少数类别的生成样本。此外，我们设计了一种无成本的集成学习策略，以减少由验证集和测试集之间的异质性引起的模型估计方差，从而提高准确性和鲁棒性。

    Deep neural networks have played an important role in automatic sleep stage classification because of their strong representation and in-model feature transformation abilities. However, class imbalance and individual heterogeneity which typically exist in raw EEG signals of sleep data can significantly affect the classification performance of any machine learning algorithms. To solve these two problems, this paper develops a generative adversarial network (GAN)-powered ensemble deep learning model, named SleepEGAN, for the imbalanced classification of sleep stages. To alleviate class imbalance, we propose a new GAN (called EGAN) architecture adapted to the features of EEG signals for data augmentation. The generated samples for the minority classes are used in the training process. In addition, we design a cost-free ensemble learning strategy to reduce the model estimation variance caused by the heterogeneity between the validation and test sets, so as to enhance the accuracy and robus
    
[^27]: 通过物理信息的低样本学习对基于sEMG的肌肉力和关节运动学进行估计

    A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics. (arXiv:2307.05361v1 [eess.SP])

    [http://arxiv.org/abs/2307.05361](http://arxiv.org/abs/2307.05361)

    本文提出了一种基于物理信息的低样本学习方法，用于基于sEMG的肌肉力和关节运动学估计。该方法将拉格朗日运动方程和反动力学肌肉模型整合到生成对抗网络中，实现对小样本数据的结构化特征解码和外推估计。

    

    面向基于表面肌电图（sEMG）的肌肉力和关节运动学估计，是实时生物力学分析中神经肌肉刺激、肌肉动力学和动力学之间动态相互作用的关键。深度神经网络（DNN）的最新进展表明，它们有潜力以全自动和可重复的方式改进生物力学分析。然而，生物力学分析的小样本性质和物理可解释性限制了DNN的应用。本文提出了一种新颖的物理信息低样本学习方法，用于基于sEMG的肌肉力和关节运动学估计。该方法将拉格朗日运动方程和反动力学肌肉模型无缝地整合到生成对抗网络（GAN）框架中，以实现对小样本数据的结构化特征解码和外推估计。具体而言，引入拉格朗日运动方程到生成模型中，以限制高层结构化解码过程。

    Muscle force and joint kinematics estimation from surface electromyography (sEMG) are essential for real-time biomechanical analysis of the dynamic interplay among neural muscle stimulation, muscle dynamics, and kinetics. Recent advances in deep neural networks (DNNs) have shown the potential to improve biomechanical analysis in a fully automated and reproducible manner. However, the small sample nature and physical interpretability of biomechanical analysis limit the applications of DNNs. This paper presents a novel physics-informed low-shot learning method for sEMG-based estimation of muscle force and joint kinematics. This method seamlessly integrates Lagrange's equation of motion and inverse dynamic muscle model into the generative adversarial network (GAN) framework for structured feature decoding and extrapolated estimation from the small sample data. Specifically, Lagrange's equation of motion is introduced into the generative model to restrain the structured decoding of the hig
    
[^28]: 使用双调节器解决联邦半监督学习中的数据不平衡问题

    Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])

    [http://arxiv.org/abs/2307.05358](http://arxiv.org/abs/2307.05358)

    本文提出了一种带有双调节器的新型联邦半监督学习框架FedDure，解决了数据分布不平衡的问题。通过粗调节器和细调节器对本地模型的更新进行规范，以及学习适应性加权方案，适应不同的数据分布。

    

    联邦学习已经成为一种从分散异构数据中学习的流行方法。由于分散客户端上标签稀缺，联邦半监督学习（FSSL）出现以从少量标记数据中训练模型。现有的FSSL方法假设客户端之间的标签数据独立且具有相同分布，并且在客户端内部标记和未标记数据之间具有一致的类别分布。本文研究了FSSL的更实际和具有挑战性的情况，即数据分布不仅在客户端之间不同，在客户端内部标记和未标记数据之间也不同。为了解决这个挑战，本文提出了一种带有双调节器的新型FSSL框架，FedDure。FedDure通过粗调节器（C-reg）和细调节器（F-reg）解除了以前的假设：C-reg通过跟踪标记数据分布的学习效果来规范本地模型的更新；F-reg学习一个适应性加权方案，以适应客户端内不同的数据分布。

    Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure.} FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored f
    
[^29]: VisText：一个丰富语义的图表标题评测基准

    VisText: A Benchmark for Semantically Rich Chart Captioning. (arXiv:2307.05356v1 [cs.CV])

    [http://arxiv.org/abs/2307.05356](http://arxiv.org/abs/2307.05356)

    本研究介绍了VisText，一个丰富语义的图表标题评测基准，该数据集包含了12,441个图表和标题对，描述了图表的构造、报告了关键统计数据，并识别了感知和认知现象。通过在图表标题生成任务上微调语言模型并应用预处理，我们评估了VisText的影响。

    

    描述或解释图表的标题有助于提高对图表数据的回忆和理解，并为视觉障碍人士提供更易接触的媒介。然而，当前自动生成这类标题的方法难以表达图表的感知或认知特征（如复杂的趋势和模式）。为此，我们介绍了VisText：一个由12,441个图表和标题对组成的数据集，描述了图表的构造，报告了关键统计数据，并识别了感知和认知现象。在VisText中，一个图表有三种表示形式：光栅化图像、支持数据表格和场景图——类似于Web页面的文档对象模型（DOM）的图表可视元素的分层表示。为了评估VisText的影响，我们在图表标题生成任务上对最先进的语言模型进行了微调，并应用了预处理来产生传达语义内容变化的标题。

    Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts' construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph -- a hierarchical representation of a chart's visual elements akin to a web page's Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. O
    
[^30]: 路由、解释、重复：模糊后解释性与可解释模型之间的界限。

    Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models. (arXiv:2307.05350v1 [cs.LG])

    [http://arxiv.org/abs/2307.05350](http://arxiv.org/abs/2307.05350)

    本文模糊了后解释黑盒模型与构建可解释模型之间的界限，通过从灵活的黑盒模型开始，逐渐引入可解释模型和残差网络，实现了对样本的路由和解释。

    

    目前的机器学习模型设计方法要么选择一个灵活的黑盒模型并在后期解释它，要么从一个可解释的模型开始。黑盒模型灵活但难以解释，而可解释模型设计为可解释。然而，开发可解释模型需要深厚的机器学习知识，而得到的模型往往不够灵活，可能性能不及其黑盒模型的等价物。本文旨在模糊后解释黑盒模型与构建可解释模型之间的区别。我们提议从一个灵活的黑盒模型开始，并逐渐「雕刻」出一种混合了可解释模型和一个「残差网络」的架构。我们的设计通过可解释模型「路由」一部分样本，剩余的样本则通过灵活的残差网络进行路由。我们采用一阶逻辑（FOL）作为可解释模型的基础。

    The current approach to ML model design is either to choose a flexible Blackbox model and explain it post hoc or to start with an interpretable model. Blackbox models are flexible but difficult to explain, whereas interpretable models are designed to be explainable. However, developing interpretable models necessitates extensive ML knowledge, and the resulting models tend to be less flexible, offering potentially subpar performance compared to their Blackbox equivalents. This paper aims to blur the distinction between a post hoc explanation of a BlackBox and constructing interpretable models. We propose beginning with a flexible BlackBox model and gradually \emph{carving out} a mixture of interpretable models and a \emph{residual network}. Our design identifies a subset of samples and \emph{routes} them through the interpretable models. The remaining samples are routed through a flexible residual network. We adopt First Order Logic (FOL) as the interpretable model's backbone, which pro
    
[^31]: 跟踪非参数情境赌博中最显著变化

    Tracking Most Significant Shifts in Nonparametric Contextual Bandits. (arXiv:2307.05341v1 [stat.ML])

    [http://arxiv.org/abs/2307.05341](http://arxiv.org/abs/2307.05341)

    该论文研究了非参数情境赌博中的最显著变化，提出了一种只计算显著变化的方法，来解决局部性问题。

    

    我们研究了非参数情境赌博，其中Lipschitz均值奖励函数可能随时间变化。我们首先在这个较少被理解的情境下建立了动态遗憾率的极小极大值，这些值与变化数量L和总变差V有关，两者都可以捕捉到上下文空间的所有分布变化，并且证明了目前的方法在这个情境下是次优的。接下来，我们探讨了这种情境下的适应性问题，即在不知道L或V的情况下实现极小极大值。非常重要的是，我们认为，在给定的上下文X_t处，赌博问题在上下文空间其他部分中的奖励变化不应该产生影响。因此，我们提出了一种变化的概念，我们称之为经验显著变化，更好地考虑了局部性，因此比L和V计数更少。此外，类似于最近在非平稳多臂赌博机中的工作（Suk和Kpotufe，2022），经验显著变化只计算显著变化。

    We study nonparametric contextual bandits where Lipschitz mean reward functions may change over time. We first establish the minimax dynamic regret rate in this less understood setting in terms of number of changes $L$ and total-variation $V$, both capturing all changes in distribution over context space, and argue that state-of-the-art procedures are suboptimal in this setting.  Next, we tend to the question of an adaptivity for this setting, i.e. achieving the minimax rate without knowledge of $L$ or $V$. Quite importantly, we posit that the bandit problem, viewed locally at a given context $X_t$, should not be affected by reward changes in other parts of context space $\cal X$. We therefore propose a notion of change, which we term experienced significant shifts, that better accounts for locality, and thus counts considerably less changes than $L$ and $V$. Furthermore, similar to recent work on non-stationary MAB (Suk & Kpotufe, 2022), experienced significant shifts only count the m
    
[^32]: 用于从可穿戴设备中去噪光电容/容积描记信号以估计心率的自监督算法

    A Self-Supervised Algorithm for Denoising Photoplethysmography Signals for Heart Rate Estimation from Wearables. (arXiv:2307.05339v1 [eess.SP])

    [http://arxiv.org/abs/2307.05339](http://arxiv.org/abs/2307.05339)

    提出了一种自监督算法，用于从可穿戴设备收集的PPG信号中去噪以估计心率。该算法通过重构受损信号部分和保留干净信号部分的方式进行去噪，并通过利用干净PPG信号训练自动编码器来实现。实验结果表明我们的算法在PPG信号心率估计方面优于传统方法。

    

    智能手表和其他可穿戴设备配备了光电容/容积描记（PPG）传感器，用于监测心率和其他心血管健康指标。然而，从这些设备收集的PPG信号容易受到噪声和运动伪迹的干扰，导致心率估计误差。传统的去噪方法往往会以过滤或重构的方式处理信号，这样会导致信号的形态信息丢失，即使是对于原本干净的信号部分而言，也会有用的信息被丢失。在这项工作中，我们开发了一种用于去噪PPG信号的算法，可以重构受损信号的部分，同时保留PPG信号的干净部分。我们的创新框架基于自监督训练，利用大量的干净PPG信号数据库来训练去噪自动编码器。正如我们所展示的，我们重构的信号提供了比领先的心率估计方法更好的PPG信号心率估计结果。进一步的实验显示...

    Smart watches and other wearable devices are equipped with photoplethysmography (PPG) sensors for monitoring heart rate and other aspects of cardiovascular health. However, PPG signals collected from such devices are susceptible to corruption from noise and motion artifacts, which cause errors in heart rate estimation. Typical denoising approaches filter or reconstruct the signal in ways that eliminate much of the morphological information, even from the clean parts of the signal that would be useful to preserve. In this work, we develop an algorithm for denoising PPG signals that reconstructs the corrupted parts of the signal, while preserving the clean parts of the PPG signal. Our novel framework relies on self-supervised training, where we leverage a large database of clean PPG signals to train a denoising autoencoder. As we show, our reconstructed signals provide better estimates of heart rate from PPG signals than the leading heart rate estimation methods. Further experiments show
    
[^33]: 通过可穿戴设备和电子健康记录数据进行无偏见的疼痛评估：基于多属性公平损失的CNN方法

    Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach. (arXiv:2307.05333v1 [eess.SP])

    [http://arxiv.org/abs/2307.05333](http://arxiv.org/abs/2307.05333)

    本文提出了一种基于多属性公平损失的CNN模型，通过考虑患者数据中的敏感属性，公平预测疼痛状态，致力于减少差异。

    

    多样化的健康数据（物联网、电子健康记录和临床调查）与可扩展的适应性人工智能相结合，已经实现了对疼痛状态的身体、行为和心理社交指标的发现。尽管以技术进步改变医疗系统的热情和承诺，但临床疼痛评估中的人工智能应用受到了问题本身的多样性和个性化以及公平性等其他挑战的阻碍。研究表明，许多人工智能（如机器学习或深度学习）模型显示出偏见，并歧视特定人群（如基于性别或种族），这引起了医疗专业人员对人工智能适应性的怀疑。在本文中，我们提出了一种基于多属性公平损失的CNN模型，旨在考虑数据中包含的任何敏感属性，并公平预测患者的疼痛状态，同时尽量减少差异。

    The combination of diverse health data (IoT, EHR, and clinical surveys) and scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of physical, behavioral, and psycho-social indicators of pain status. Despite the hype and promise to fundamentally alter the healthcare system with technological advancements, much AI adoption in clinical pain evaluation has been hampered by the heterogeneity of the problem itself and other challenges, such as personalization and fairness. Studies have revealed that many AI (i.e., machine learning or deep learning) models display biases and discriminate against specific population segments (such as those based on gender or ethnicity), which breeds skepticism among medical professionals about AI adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL) based CNN model that aims to account for any sensitive attributes included in the data and fairly predict patients' pain status while attempting to minimize the discre
    
[^34]: 棋盘上的棋子价值。 (arXiv:2307.05330v1 [cs.AI])

    The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])

    [http://arxiv.org/abs/2307.05330](http://arxiv.org/abs/2307.05330)

    本研究通过引入边际估值对国际象棋棋盘上的棋子和棋盘进行评价，提供了关于马、象和兵的有价值的见解。

    

    我们的研究的主要目标是评估棋盘上棋子的价值，并确定棋子在棋盘上的摆放位置。随着国际象棋人工智能的出现，我们能够准确评估国际象棋局面的价值。传统方法对棋子赋予固定的价值$(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$。我们通过引入棋子和棋盘方面的边际估值来改进这种分析。我们通过研究马和象的位置，并提供有关兵的价值的宝贵见解来演示我们的方法。值得注意的是，尼姆佐维奇是倡导兵的结构和价值的先驱之一。最后，我们提出了未来研究的潜在方向。

    Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
    
[^35]: 使用深度集成神经网络在端点设备上预测小分子的溶解度

    Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])

    [http://arxiv.org/abs/2307.05318](http://arxiv.org/abs/2307.05318)

    这项工作提出了一种使用深度集成神经网络在端点设备上预测小分子溶解度的方法，通过静态网站运行，同时具备预测不确定性，并实现了令人满意的结果。

    

    水溶解度是一种有价值但难以预测的性质。使用一级原理方法计算溶解度需要考虑熵和焓的竞争效应，导致计算时间较长且准确性相对较差。基于数据驱动的方法，如深度学习，提供了更高的准确性和计算效率，但通常缺乏不确定性量化。此外，任何计算技术的易用性仍然是一个问题，导致群体贡献方法的持续流行。在这项工作中，我们使用一种具有预测不确定性的深度学习模型来解决这些问题，该模型在静态网站上运行（无需服务器）。这种方法将计算需求转移到网站访问者身上，而不需要安装，消除了支付和维护服务器的需求。我们的模型在溶解度预测上取得了令人满意的结果。此外，我们展示了如何创建平衡溶解度预测模型。

    Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balanc
    
[^36]: 用哈密尔顿图神经网络直接从轨迹中发现符号定律

    Discovering Symbolic Laws Directly from Trajectories with Hamiltonian Graph Neural Networks. (arXiv:2307.05299v1 [cs.LG])

    [http://arxiv.org/abs/2307.05299](http://arxiv.org/abs/2307.05299)

    本文介绍了一种哈密尔顿图神经网络（HGNN），它可以直接从轨迹中学习物理系统的动力学，在多个系统上表现出色，并能从少量数据中提取符合真实情况的定律。

    

    物理系统的时间演化由微分方程描述，这些方程依赖于能量和力等抽象量。传统上，这些量是基于位置和速度等可观测量的泛函导出的。发现这些控制符号定律是理解自然界相互作用的关键。本文介绍了一种哈密尔顿图神经网络（HGNN），它是一种通过轨迹直接学习系统动力学的物理强化图神经网络。我们展示了HGNN在n个弹簧、n个摆动系统、引力系统和二进制Lennard Jones系统上的性能，HGNN能够从少量数据中学习出与真实情况极佳一致的动力学。我们还评估了HGNN在更大系统大小以及混合弹簧-摆动系统上的推广能力，该系统是由两个独立训练的原始系统（弹簧和摆动）的组合构成。最后，我们还利用符号回归方法对学到的定律进行了推导和解释。

    The time evolution of physical systems is described by differential equations, which depend on abstract quantities like energy and force. Traditionally, these quantities are derived as functionals based on observables such as positions and velocities. Discovering these governing symbolic laws is the key to comprehending the interactions in nature. Here, we present a Hamiltonian graph neural network (HGNN), a physics-enforced GNN that learns the dynamics of systems directly from their trajectory. We demonstrate the performance of HGNN on n-springs, n-pendulums, gravitational systems, and binary Lennard Jones systems; HGNN learns the dynamics in excellent agreement with the ground truth from small amounts of data. We also evaluate the ability of HGNN to generalize to larger system sizes, and to hybrid spring-pendulum system that is a combination of two original systems (spring and pendulum) on which the models are trained independently. Finally, employing symbolic regression on the learn
    
[^37]: 关于需要描述分布偏移的语言：基于表格数据集的案例分析

    On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])

    [http://arxiv.org/abs/2307.05284](http://arxiv.org/abs/2307.05284)

    该论文通过对表格数据集中的自然偏移进行研究，发现$Y|X$-偏移最为普遍。为了推动研究人员开发描述数据分布偏移的精细语言，作者构建了WhyShift实验平台，并讨论了$Y|X$-偏移对算法的影响。

    

    不同的分布偏移需要不同的算法和操作干预。方法研究必须以其所涉及的具体偏移为基础。尽管新兴的基准数据为实证研究提供了有希望的基础，但它们隐含地关注协变量偏移，并且实证发现的有效性取决于偏移类型，例如，当$Y|X$分布发生变化时，之前关于算法性能的观察可能无效。我们对5个表格数据集中的自然偏移进行了深入研究，通过对86,000个模型配置进行实验，发现$Y|X$-偏移最为普遍。为了鼓励研究人员开发一种精细的描述数据分布偏移的语言，我们构建了WhyShift，一个由策划的真实世界偏移测试平台，在其中我们对我们基准性能的偏移类型进行了表征。由于$Y|X$-偏移在表格设置中很常见，我们确定了受到最大$Y|X$-偏移影响的协变量区域，并讨论了对算法的影响。

    Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
    
[^38]: CareFall: 基于可穿戴设备和人工智能方法的自动跌倒检测

    CareFall: Automatic Fall Detection through Wearable Devices and AI Methods. (arXiv:2307.05275v1 [cs.LG])

    [http://arxiv.org/abs/2307.05275](http://arxiv.org/abs/2307.05275)

    本文介绍了CareFall，一种基于可穿戴设备和人工智能方法的自动跌倒检测系统。实验结果表明，机器学习方法在准确率、敏感性和特异性方面优于基于阈值的方法，为解决老年人跌倒问题提供了智能和用户友好的解决方案。

    

    人口老龄化导致了全球范围内跌倒事件的增多，对全球公共健康产生了影响。本文提出了CareFall，一种基于可穿戴设备和人工智能方法的自动跌倒检测系统（FDS）。CareFall利用从智能手表提取的加速度计和陀螺仪时间信号。采用了两种不同的特征提取和分类方法：一是基于阈值的方法，二是基于机器学习的方法。在两个公共数据库上的实验结果表明，将加速度计和陀螺仪信息结合使用的机器学习方法在准确率、敏感性和特异性方面优于基于阈值的方法。该研究为设计智能且用户友好的解决方案，缓解老年人跌倒的负面后果作出了贡献。

    The aging population has led to a growing number of falls in our society, affecting global public health worldwide. This paper presents CareFall, an automatic Fall Detection System (FDS) based on wearable devices and Artificial Intelligence (AI) methods. CareFall considers the accelerometer and gyroscope time signals extracted from a smartwatch. Two different approaches are used for feature extraction and classification: i) threshold-based, and ii) machine learning-based. Experimental results on two public databases show that the machine learning-based approach, which combines accelerometer and gyroscope information, outperforms the threshold-based approach in terms of accuracy, sensitivity, and specificity. This research contributes to the design of smart and user-friendly solutions to mitigate the negative consequences of falls among older people.
    
[^39]: U-CREAT: 无监督事件提取的无监督案例检索系统

    U-CREAT: Unsupervised Case Retrieval using Events extrAcTion. (arXiv:2307.05260v1 [cs.IR])

    [http://arxiv.org/abs/2307.05260](http://arxiv.org/abs/2307.05260)

    U-CREAT是一个无监督案例检索系统，通过使用事件提取实现了更高的性能和更快的检索速度，适用于实时案例检索系统。

    

    在法律领域，先前案例检索的任务是自动引用与给定查询案例相关（基于事实和先例）的先前法律案例。为了进一步推动先前案例检索研究，本文提出了一个新的大型基准（以英文为主）用于先前案例检索任务：IL-PCR（印度法律先前案例检索）语料库。考虑到案例相关性的复杂性和法律文档的长度，BM25仍然是排名引用先前文档的强大基准。在这项工作中，我们探索了事件在法律案例检索中的作用，并提出一种基于无监督检索方法的管道系统U-CREAT（无监督事件提取的无监督案例检索系统）。我们发现，所提出的无监督检索方法与BM25相比显著提高了性能，并且使检索速度大大加快，使其适用于实时案例检索系统。我们的系统具有通用性，我们证明它适用于两个不同的法律体系（印度）。

    The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (India
    
[^40]: 基于MAP和MLE的教学

    MAP- and MLE-Based Teaching. (arXiv:2307.05252v1 [cs.LG])

    [http://arxiv.org/abs/2307.05252](http://arxiv.org/abs/2307.05252)

    该论文研究了基于MAP和MLE的教学方法，其中学习者根据观察结果推断隐藏的概念，教师试图找到最小的观察集合以使得学习者返回特定的概念。

    

    假设一个学习者L试图从一系列观察中推断出一个隐藏的概念。在Ferri等人的工作[4]的基础上，我们假设学习者由先验P(c)和条件概率P(z|c)参数化，其中c范围在给定类别C中的所有概念上，z范围在观察集合Z中的所有观察上。如果L将一组观察看作是随机样本，并返回具有最大后验概率的概念（相应地，返回最大化S的c条件概率的概念），则L被称为MAP学习器（resp. MLE学习器）。根据L是否假设S是从有序或无序采样（resp. 有替换或无替换采样）获得的，可以区分四种不同的采样模式。对于给定的目标概念c在C中，对于MAP学习器L来说，教师的目标是找到最小的观察集合，使得L返回c。这种方法自然地导致了各种MAP或MLE教学的概念。

    Imagine a learner L who tries to infer a hidden concept from a collection of observations. Building on the work [4] of Ferri et al., we assume the learner to be parameterized by priors P(c) and by c-conditional likelihoods P(z|c) where c ranges over all concepts in a given class C and z ranges over all observations in an observation set Z. L is called a MAP-learner (resp. an MLE-learner) if it thinks of a collection S of observations as a random sample and returns the concept with the maximum a-posteriori probability (resp. the concept which maximizes the c-conditional likelihood of S). Depending on whether L assumes that S is obtained from ordered or unordered sampling resp. from sampling with or without replacement, we can distinguish four different sampling modes. Given a target concept c in C, a teacher for a MAP-learner L aims at finding a smallest collection of observations that causes L to return c. This approach leads in a natural manner to various notions of a MAP- or MLE-teac
    
[^41]: DRMC: 一种具有动态路由的通用模型用于多中心PET图像合成

    DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image Synthesis. (arXiv:2307.05249v1 [eess.IV])

    [http://arxiv.org/abs/2307.05249](http://arxiv.org/abs/2307.05249)

    本论文提出了一种通用模型，通过动态路由和跨层连接来解决多中心PET图像合成中的域转移和中心干扰问题。

    

    多中心正电子发射断层扫描（PET）图像合成旨在从多个不同中心恢复低剂量PET图像。由于不同中心具有不同的成像系统/协议和数据分布，现有方法的普适性仍然不够理想，会导致域转移问题。虽然一些方法通过为每个中心训练专门的模型来解决域转移问题，但这些方法参数效率低，无法充分利用中心之间的共享知识。为了解决这个问题，我们开发了一个通用模型，该模型在中心之间共享架构和参数以利用共享知识。然而，通用模型可能会遇到中心干扰的问题，即由于非相同的数据分布，不同中心的梯度方向可能不一致甚至相反。为了减轻这种干扰，我们引入了一种新颖的动态路由策略和跨层连接。

    Multi-center positron emission tomography (PET) image synthesis aims at recovering low-dose PET images from multiple different centers. The generalizability of existing methods can still be suboptimal for a multi-center study due to domain shifts, which result from non-identical data distribution among centers with different imaging systems/protocols. While some approaches address domain shifts by training specialized models for each center, they are parameter inefficient and do not well exploit the shared knowledge across centers. To address this, we develop a generalist model that shares architecture and parameters across centers to utilize the shared knowledge. However, the generalist model can suffer from the center interference issue, \textit{i.e.} the gradient directions of different centers can be inconsistent or even opposite owing to the non-identical data distribution. To mitigate such interference, we introduce a novel dynamic routing strategy with cross-layer connections th
    
[^42]: 从分布式机器学习到分布式深度学习的调查

    A Survey From Distributed Machine Learning to Distributed Deep Learning. (arXiv:2307.05232v1 [cs.LG])

    [http://arxiv.org/abs/2307.05232](http://arxiv.org/abs/2307.05232)

    这篇综述论文介绍了分布式机器学习和分布式深度学习的研究现状和方法，强调了分布式深度学习在解决复杂问题方面取得的重要进展。

    

    近年来，人工智能在处理复杂任务方面取得了重大成功。这一成功归功于机器学习算法和硬件加速的进步。为了获得更准确的结果和解决更复杂的问题，算法必须用更多的数据进行训练。这么大量的数据可能需要消耗大量的时间和计算资源来处理。通过将数据和算法分布在多台机器上，可以实现这个解决方案，这被称为分布式机器学习。在分布式机器学习算法方面，已经投入了相当多的努力，目前已经提出了不同的方法。本文通过对这些算法的综述，对该领域的最新状态进行全面总结。我们将这些算法分成分类和聚类（传统机器学习）、深度学习和深度强化学习几组。分布式深度学习的研究也取得了显著的进展。

    Artificial intelligence has achieved significant success in handling complex tasks in recent years. This success is due to advances in machine learning algorithms and hardware acceleration. In order to obtain more accurate results and solve more complex problems, algorithms must be trained with more data. This huge amount of data could be time-consuming to process and require a great deal of computation. This solution could be achieved by distributing the data and algorithm across several machines, which is known as distributed machine learning. There has been considerable effort put into distributed machine learning algorithms, and different methods have been proposed so far. In this article, we present a comprehensive summary of the current state-of-the-art in the field through the review of these algorithms. We divide this algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has ga
    
[^43]: 属性控制的对话引导

    Attribute Controlled Dialogue Prompting. (arXiv:2307.05228v1 [cs.CL])

    [http://arxiv.org/abs/2307.05228](http://arxiv.org/abs/2307.05228)

    本文提出了一种新的基于实例级控制代码的对话引导算法，用于探索实例特定的提示对于控制对话生成的影响。实验结果表明，该方法优于提示基线，并且与仅使用总参数的微调相媲美。

    

    为了适应下游任务，提示调整已成为一种越来越受欢迎的参数高效方法。然而，离散提示和连续提示都假设任务中的所有数据样本使用相同的固定提示，忽略了某些任务（如开放域对话生成）中输入的巨大变化。本文提出了一种新颖的、基于实例级控制代码的对话引导算法。具体来说，我们基于实例级控制代码而不是对话历史生成提示，以探索实例特定的提示对于控制对话生成的影响。在流行的开放域对话数据集上进行的实验，在自动指标和人工评估方面都证明我们的方法优于提示基线，并且与仅使用总参数的5%-6%的微调相媲美。

    Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.
    
[^44]: 基于同质性的有监督注意力图神经网络

    Supervised Attention Using Homophily in Graph Neural Networks. (arXiv:2307.05217v1 [cs.LG])

    [http://arxiv.org/abs/2307.05217](http://arxiv.org/abs/2307.05217)

    本文提出了一种新技术，可以在任何图注意力模型中应用，以鼓励共享相同类别标签的节点获得更高的注意力分数，并在多个节点分类数据集上展示了比标准基线模型更高的性能。

    

    图神经网络已经成为处理图上学习问题的标准方法。在不同变种的图神经网络中，图注意力网络（GATs）被成功应用于不同的任务。在GAT模型中，每个节点使用注意力机制为其邻居分配重要性分数。然而，类似于其他图神经网络，GAT聚合来自属于不同类别的节点的信息，因此产生的节点表示在不同类别方面不够明确，这可能会影响它们的性能。在这项工作中，为了缓解这个问题，我们提出了一种新的技术，可以将其纳入到任何图注意力模型中，以鼓励共享相同类别标签的节点之间获得更高的注意力分数。我们在几个节点分类数据集上评估了所提出的方法，并展示了与标准基线模型相比的性能提升。

    Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.
    
[^45]: 评分函数梯度估计以扩大决策焦点学习的适用性

    Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning. (arXiv:2307.05213v1 [cs.LG])

    [http://arxiv.org/abs/2307.05213](http://arxiv.org/abs/2307.05213)

    本研究采用评分函数梯度估计方法，通过预测参数分布来计算决策焦点模型的更新，以扩大决策焦点学习的适用性。

    

    许多现实世界的优化问题都包含需要在解决之前进行预测的未知参数。为了训练涉及的预测机器学习（ML）模型，通常采用的方法是专注于最大化预测准确性。然而，这种方法并不总是导致下游任务损失的最小化。决策焦点学习（DFL）是一种最近提出的范式，其目标是通过直接最小化任务损失来训练ML模型。然而，最先进的DFL方法受到它们对优化问题结构的假设（例如，问题是线性的）以及只能预测出现在目标函数中的参数的限制。在这项工作中，我们通过相反地预测参数的分布，并采用评分函数梯度估计（SFGE）来计算决策焦点模型的更新，从而扩大DFL的适用性。我们的实验...

    Many real-world optimization problems contain unknown parameters that must be predicted prior to solving. To train the predictive machine learning (ML) models involved, the commonly adopted approach focuses on maximizing predictive accuracy. However, this approach does not always lead to the minimization of the downstream task loss. Decision-focused learning (DFL) is a recently proposed paradigm whose goal is to train the ML model by directly minimizing the task loss. However, state-of-the-art DFL methods are limited by the assumptions they make about the structure of the optimization problem (e.g., that the problem is linear) and by the fact that can only predict parameters that appear in the objective function. In this work, we address these limitations by instead predicting \textit{distributions} over parameters and adopting score function gradient estimation (SFGE) to compute decision-focused updates to the predictive model, thereby widening the applicability of DFL. Our experiment
    
[^46]: 强化学习中基于奖励机器抽象的上下文预规划以增强迁移学习

    Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])

    [http://arxiv.org/abs/2307.05209](http://arxiv.org/abs/2307.05209)

    我们提出了一种使用奖励机器抽象来表示当前任务，并在迁移学习中提升DRL代理的性能的方法，实验表明该方法能够提高样本效率并在多个领域中进行少样本迁移。

    

    最近的研究表明，深度强化学习（DRL）代理倾向于过拟合训练任务，并且无法适应轻微的环境变化。为了在转移到未见任务时加快学习，我们提出了一种使用奖励机器（RM）来表示当前任务的新方法，奖励机器是基于当前任务的奖励和动态生成子任务的状态机抽象。我们的方法为代理提供了当前抽象状态的符号表示，并奖励它们达成这些转换。这些表示在任务之间共享，使代理能够利用先前遇到的符号和转换的知识，从而增强迁移能力。我们的实证评估表明，我们的表示在各种领域中提高了样本效率和少样本迁移。

    Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
    
[^47]: 拒绝选项模型中的超出分布检测

    Reject option models comprising out-of-distribution detection. (arXiv:2307.05199v1 [cs.LG])

    [http://arxiv.org/abs/2307.05199](http://arxiv.org/abs/2307.05199)

    本文提出了三种拒绝选项模型来处理超出分布检测问题，并给出了一个简单的双评分超出分布方法。实验结果表明，该方法优于最先进的方法。

    

    超出分布检测的最优预测策略是机器学习中的一个基本问题。本文提出了三种超出分布检测的拒绝选项模型：基于成本的模型、有界TPR-FPR模型和有界精确度-召回率模型。这些模型扩展了非超出分布检测中使用的标准拒绝选项模型，并定义了最优超出分布选择分类器的概念。我们证明了尽管这些模型在公式上不同，但它们都共享一类最优策略。受到最优策略的启发，我们引入了双评分超出分布方法，利用来自两个选择的超出分布检测器的不确定度评分：一个专注于超出分布/内部分布的区分，另一个专注于误分类检测。实验结果一致表明，与最先进的方法相比，这种简单策略的性能更优。此外，我们还提出了新颖的评估方法。

    The optimal prediction strategy for out-of-distribution (OOD) setups is a fundamental question in machine learning. In this paper, we address this question and present several contributions. We propose three reject option models for OOD setups: the Cost-based model, the Bounded TPR-FPR model, and the Bounded Precision-Recall model. These models extend the standard reject option models used in non-OOD setups and define the notion of an optimal OOD selective classifier. We establish that all the proposed models, despite their different formulations, share a common class of optimal strategies. Motivated by the optimal strategy, we introduce double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection. The experimental results consistently demonstrate the superior performance of this simple strategy compared to state-of-the-art methods. Additionally, we propose novel evaluation met
    
[^48]: 通过$\beta$-分解一后验采样实现差分计算机学习

    Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])

    [http://arxiv.org/abs/2307.05194](http://arxiv.org/abs/2307.05194)

    通过对数据生成过程和模型之间的$\beta$-分解进行后验采样，我们提出了$\beta$D-Bayes，一种能够实现差分机器学习的方法。

    

    差分私密性确保了包含敏感数据的统计分析结果可以在不损害任何个体隐私的情况下进行发布。实现这种保证通常需要在参数估计或估计过程中直接注入噪音。而采样来自贝叶斯后验分布已被证明是指数机制的一种特殊情况，可以产生一致且高效的私密估计，而不会改变数据生成过程。然而，当前方法的应用受到较强的边界假设的限制，这些假设对于基本模型（如简单的线性回归器）并不成立。为了改善这一点，我们提出了$\beta$D-Bayes，一种从广义后验中进行后验采样的方案，目标是最小化模型与数据生成过程之间的$\beta$-分解。这提供了私密估计的方法。

    Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation t
    
[^49]: 使用对抗扰动的DNN成员推断攻击

    Membership Inference Attacks on DNNs using Adversarial Perturbations. (arXiv:2307.05193v1 [cs.LG])

    [http://arxiv.org/abs/2307.05193](http://arxiv.org/abs/2307.05193)

    提出了一种使用对抗扰动的DNN成员推断攻击，通过对现有MI攻击进行统一并提出两种新的攻击方法，提高了在简单数据集上的性能表现。

    

    已经提出了几种成员推断（MI）攻击方法来审计目标DNN。给定一组主题，MI攻击可以告诉我们目标DNN在训练过程中观察到了哪些主题。本文关注后期训练MI攻击，强调在低虚假阳性率（FPR）下的高置信度成员检测--真正阳性率（TPR）。目前在这个类别中的工作--似然比攻击（LiRA）和增强MI攻击（EMIA）--只在复杂的数据集（比如CIFAR-10和Imagenet）上表现良好，而在更简单的数据集上表现不佳（Fashion-MNIST上两种攻击都是0%的TPR，MNIST上分别是1% FPR的2%和0% TPR）。为了解决这个问题，首先我们通过提出一个分为三个阶段的框架（准备、指示和决策）来统一当前的MI攻击。其次，我们利用这个框架提出了两种新的攻击方法：（1）对抗成员推断攻击（AMIA）有效地利用成员分类中的特征进行攻击。(2) Emphasizing-on-Misclassified Samples Attack (EMSA) 通过强调错误分类的样本来提高攻击性能。

    Several membership inference (MI) attacks have been proposed to audit a target DNN. Given a set of subjects, MI attacks tell which subjects the target DNN has seen during training. This work focuses on the post-training MI attacks emphasizing high confidence membership detection -- True Positive Rates (TPR) at low False Positive Rates (FPR). Current works in this category -- likelihood ratio attack (LiRA) and enhanced MI attack (EMIA) -- only perform well on complex datasets (e.g., CIFAR-10 and Imagenet) where the target DNN overfits its train set, but perform poorly on simpler datasets (0% TPR by both attacks on Fashion-MNIST, 2% and 0% TPR respectively by LiRA and EMIA on MNIST at 1% FPR). To address this, firstly, we unify current MI attacks by presenting a framework divided into three stages -- preparation, indication and decision. Secondly, we utilize the framework to propose two novel attacks: (1) Adversarial Membership Inference Attack (AMIA) efficiently utilizes the membership 
    
[^50]: 使用线性回归迭代训练神经网络

    Using Linear Regression for Iteratively Training Neural Networks. (arXiv:2307.05189v1 [cs.LG])

    [http://arxiv.org/abs/2307.05189](http://arxiv.org/abs/2307.05189)

    我们提出了一种使用线性回归来迭代训练神经网络的方法，通过从输出向后计算神经元的理想总输入值，以线性最小二乘问题迭代更新参数和激活值。

    

    我们提出了一种基于简单线性回归的方法来学习神经网络的权重和偏置，作为标准梯度反向传播的替代方法。目前的工作是探索性的，并且我们将描述和实验限制在（i）简单的前馈神经网络，（ii）标量（单输出）回归问题，以及（iii）可逆激活函数上。然而，这种方法可以扩展到更大、更复杂的架构。关键思想是观察到神经网络中每个神经元的输入是前一层神经元的激活以及该层的参数（权重和偏置）的线性组合。如果我们能够通过从输出向后计算每个神经元的理想总输入值，我们可以将学习问题形式化为一个线性最小二乘问题，该问题在更新参数和激活值之间迭代。我们提出了一个明确的算法来实现这个方法。

    We present a simple linear regression based approach for learning the weights and biases of a neural network, as an alternative to standard gradient based backpropagation. The present work is exploratory in nature, and we restrict the description and experiments to (i) simple feedforward neural networks, (ii) scalar (single output) regression problems, and (iii) invertible activation functions. However, the approach is intended to be extensible to larger, more complex architectures. The key idea is the observation that the input to every neuron in a neural network is a linear combination of the activations of neurons in the previous layer, as well as the parameters (weights and biases) of the layer. If we are able to compute the ideal total input values to every neuron by working backwards from the output, we can formulate the learning problem as a linear least squares problem which iterates between updating the parameters and the activation values. We present an explicit algorithm tha
    
[^51]: 使用最优输运进行去相关

    Decorrelation using Optimal Transport. (arXiv:2307.05187v1 [hep-ph])

    [http://arxiv.org/abs/2307.05187](http://arxiv.org/abs/2307.05187)

    引入了一种利用凸神经最优输运求解器进行去相关的新方法，通过最优输运将连续特征空间与受保护属性去相关。在高能物理中，这种方法在喷注分类中表现出色，能更好地去相关多类输出特征空间。

    

    能够从受保护属性中将特征空间去相关是伦理学、公平性以及自然科学领域活跃研究和学习的一个领域。我们引入一种新的利用凸神经最优输运求解器（Cnots）进行去相关的方法，该方法能够通过最优输运使连续特征空间与受保护属性去相关。我们演示了在高能物理中进行喷注分类时，它的表现如何，其中分类器得分希望与喷注的质量去相关。在二进制分类中实现的去相关程度接近于使用条件归一化流的最新水平。当转向多类输出时，最优输运方法的性能显著优于最先进方法，表明在去相关多维特征空间方面有实质性的增益。

    Being able to decorrelate a feature space from protected attributes is an area of active research and study in ethics, fairness, and also natural sciences. We introduce a novel decorrelation method using Convex Neural Optimal Transport Solvers (Cnots), that is able to decorrelate continuous feature space against protected attributes with optimal transport. We demonstrate how well it performs in the context of jet classification in high energy physics, where classifier scores are desired to be decorrelated from the mass of a jet. The decorrelation achieved in binary classification approaches the levels achieved by the state-of-the-art using conditional normalising flows. When moving to multiclass outputs the optimal transport approach performs significantly better than the state-of-the-art, suggesting substantial gains at decorrelating multidimensional feature spaces.
    
[^52]: 剩余寿命估计中机器学习方法的映射研究：以铅酸电池为例

    A Mapping Study of Machine Learning Methods for Remaining Useful Life Estimation of Lead-Acid Batteries. (arXiv:2307.05163v1 [cs.LG])

    [http://arxiv.org/abs/2307.05163](http://arxiv.org/abs/2307.05163)

    本文是一项关于使用机器学习方法估计铅酸电池的SoH和RUL的映射研究。正确的估计这两个指标可以提高电池系统的预测性维护、可靠性和寿命。这对于电动汽车、可再生能源系统等依赖此电池技术的应用都至关重要。

    

    能源储存解决方案在现代基础设施中扮演着越来越重要的角色，而铅酸电池则是可充电类别中最常用的一种。由于随着时间的推移会逐渐退化，正确确定电池的健康状态（SoH）和剩余寿命（RUL）对于提高预测性维护、可靠性和电池系统的寿命有着重要的贡献。除了提高成本节约外，正确估计SoH还可以通过回收利用退役电池来减少污染。本文对机器学习方法在估计铅酸电池的SoH和RUL方面的最新研究进行了映射研究。这两个指标在电动汽车、可再生能源系统等广泛应用该电池技术的领域中的电池管理系统中至关重要。在本研究中，我们分析了用于估计SoH和RUL的机器学习算法的类型，并评估了其性能。

    Energy storage solutions play an increasingly important role in modern infrastructure and lead-acid batteries are among the most commonly used in the rechargeable category. Due to normal degradation over time, correctly determining the battery's State of Health (SoH) and Remaining Useful Life (RUL) contributes to enhancing predictive maintenance, reliability, and longevity of battery systems. Besides improving the cost savings, correct estimation of the SoH can lead to reduced pollution though reuse of retired batteries. This paper presents a mapping study of the state-of-the-art in machine learning methods for estimating the SoH and RUL of lead-acid batteries. These two indicators are critical in the battery management systems of electric vehicles, renewable energy systems, and other applications that rely heavily on this battery technology. In this study, we analyzed the types of machine learning algorithms employed for estimating SoH and RUL, and evaluated their performance in terms
    
[^53]: SuryaKiran在MEDIQA-Sum 2023中的应用：利用LoRA进行临床对话摘要

    SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization. (arXiv:2307.05162v1 [cs.CL])

    [http://arxiv.org/abs/2307.05162](http://arxiv.org/abs/2307.05162)

    本研究展示了一种名为LoRA的参数高效细调方法在临床对话摘要中的评估结果，并证明LoRA与对大型语言模型进行端到端细调效果相当。

    

    细调大型语言模型有助于改善特定领域用例的结果。对大型语言模型进行端到端的细调耗费时间和资源，并具有高存储需求以存储细调后的大型语言模型。参数高效细调（PEFT）方法通过保持大型语言模型为固定基准并添加额外层来解决时间和资源挑战，PEFT方法进行细调。本文展示了一个名为低秩适应（LoRA）的PEFT方法在临床对话摘要中的评估结果。评估结果显示，LoRA与对大型语言模型进行端到端细调的效果相当。本文还介绍了解决ImageCLEFmedical的Subtask A和B所进行的评估。

    Finetuning Large Language Models helps improve the results for domain-specific use cases. End-to-end finetuning of large language models is time and resource intensive and has high storage requirements to store the finetuned version of the large language model. Parameter Efficient Fine Tuning (PEFT) methods address the time and resource challenges by keeping the large language model as a fixed base and add additional layers, which the PEFT methods finetune. This paper demonstrates the evaluation results for one such PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization. The evaluation results show that LoRA works at par with end-to-end finetuning for a large language model. The paper presents the evaluations done for solving both the Subtask A and B from ImageCLEFmedical {https://www.imageclef.org/2023/medical}
    
[^54]: 关于语音自我监督学习在音乐中的有效性研究

    On the Effectiveness of Speech Self-supervised Learning for Music. (arXiv:2307.05161v1 [cs.SD])

    [http://arxiv.org/abs/2307.05161](http://arxiv.org/abs/2307.05161)

    本研究探索了语音自我监督学习在音乐信息检索中的有效性。通过对两个与语音相关的模型进行自我监督学习适应，并在多个MIR任务上进行系统评估，结果显示在音乐数据上训练可以提高MIR任务的性能。

    

    自我监督学习（SSL）已经在各种语音和自然语言处理应用中显示出有希望的结果。然而，它在音乐信息检索（MIR）中的有效性仍然很少被探索。虽然以前的在音乐记录上预训练的SSL模型可能主要是闭源的，但最近的语音模型如wav2vec2.0在音乐建模方面显示出了潜力。然而，对于将语音SSL模型应用于音乐记录的有效性的研究还很有限。我们探索了两个独特的与语音相关的模型在音乐中的自我监督学习适应，分别是data2vec1.0和Hubert，并将它们分别称为music2vec和musicHuBERT。我们使用不同的预训练配置下训练了12个具有95M参数的SSL模型，并系统评估了13个不同的MIR任务的表现。我们的研究结果表明，在音乐数据上训练通常可以提高MIR任务的性能，即使使用设计为语音模型的训练范式。

    Self-supervised learning (SSL) has shown promising results in various speech and natural language processing applications. However, its efficacy in music information retrieval (MIR) still remains largely unexplored. While previous SSL models pre-trained on music recordings may have been mostly closed-sourced, recent speech models such as wav2vec2.0 have shown promise in music modelling. Nevertheless, research exploring the effectiveness of applying speech SSL models to music recordings has been limited. We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL models with 95M parameters under various pre-training configurations and systematically evaluate the MIR task performances with 13 different MIR tasks. Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed f
    
[^55]: 在强子对撞机上用于长寿命粒子触发的FPGA快速神经网络推理

    Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders. (arXiv:2307.05152v1 [hep-ex])

    [http://arxiv.org/abs/2307.05152](http://arxiv.org/abs/2307.05152)

    这项研究提出了两种机器学习算法，用于在CERN大型强子对撞机中选择中性长寿命粒子衰变的事件，并通过加速卡进行加速。实验结果表明，这些算法在加速的情况下仍然保持准确性，并符合第二级触发的延迟要求。

    

    实验粒子物理学需要一个复杂的触发和采集系统，能够高效地保留感兴趣的碰撞事件以供进一步研究。利用FPGA加速卡进行异构计算可能成为CERN大型强子对撞机即将到来的高亮度计划中触发策略的一种新趋势技术。在这个背景下，我们提出了两种机器学习算法，用于选择在探测器体积内发生中性长寿命粒子衰变的事件，并研究了在商业可获得的Xilinx FPGA加速卡上进行加速的准确性和推理时间。推理时间还与基于CPU和GPU的硬件设置进行了对比。结果表明，所提出的新算法在考虑的基准物理场景中是高效的，并且在FPGA卡上加速时准确性没有降低。所有测试的架构都符合第二级触发的延迟要求。

    Experimental particle physics demands a sophisticated trigger and acquisition system capable to efficiently retain the collisions of interest for further investigation. Heterogeneous computing with the employment of FPGA cards may emerge as a trending technology for the triggering strategy of the upcoming high-luminosity program of the Large Hadron Collider at CERN. In this context, we present two machine-learning algorithms for selecting events where neutral long-lived particles decay within the detector volume studying their accuracy and inference time when accelerated on commercially available Xilinx FPGA accelerator cards. The inference time is also confronted with a CPU- and GPU-based hardware setup. The proposed new algorithms are proven efficient for the considered benchmark physics scenario and their accuracy is found to not degrade when accelerated on the FPGA cards. The results indicate that all tested architectures fit within the latency requirements of a second-level trigge
    
[^56]: 深度概率运动原理与贝叶斯聚合器

    Deep Probabilistic Movement Primitives with a Bayesian Aggregator. (arXiv:2307.05141v1 [cs.RO])

    [http://arxiv.org/abs/2307.05141](http://arxiv.org/abs/2307.05141)

    该论文提出了一个统一的深度运动原理模型，具备多种操作，并展示了高样本效率和泛化能力。

    

    运动原理是可训练的参数模型，可以从有限的演示集合中复制机器人的运动。先前的工作提出了简单的线性模型，通过允许时间调制运动（加速或减速复制运动）、混合（将两个运动合并为一个）、通过点调节（将运动约束到特定的通过点）和上下文调节（基于观察到的变量生成运动，例如物体的位置）展示出高样本效率和泛化能力。以前的工作已经提出了基于神经网络的运动原理模型，并展示了它们在一些形式的输入调节或时间调制表达中执行任务的能力。然而，迄今为止还没有提出一个单一统一的深度运动原理模型，它能够具备所有先前的操作，这限制了神经运动原理的潜在应用。本文提出了一个深度运动原理的架构。

    Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive arch
    
[^57]: TIAM -- 一种评估文本到图像生成中对齐性的度量方法

    TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])

    [http://arxiv.org/abs/2307.05134](http://arxiv.org/abs/2307.05134)

    本文提出了一种评估文本到图像生成中对齐性的新度量方法TIAM，该方法基于提示模板，可以更好地描述生成图像与提示中内容的对齐程度，包括对象类型、数量和颜色。研究结果表明，图像质量可以有很大的变化。

    

    合成图像生成的进展使得评估其质量变得至关重要。尽管已经提出了几种用于评估图像渲染的度量方法，但对于基于提示生成图像的文本到图像（T2I）模型而言，考虑到生成图像与提示中重要内容之间的相似程度等额外因素至关重要。此外，虽然生成的图像通常是从随机起始点开始的，但通常不考虑这一影响。本文提出了一种基于提示模板的新度量方法，用于研究提示中指定的内容与生成的图像之间的对齐性。它允许我们更好地描述对齐性，包括指定对象的类型、数量和颜色。我们对几个最近的T2I模型进行了研究，并获得了一个有趣的额外结果，即图像质量可以大幅度变化。

    The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically 
    
[^58]: 自监督式语音表示在自发语音合成中的应用

    On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis. (arXiv:2307.05132v1 [eess.AS])

    [http://arxiv.org/abs/2307.05132](http://arxiv.org/abs/2307.05132)

    这项研究探讨了在自发语音合成中使用自监督式语音表示的方法，并发现了在自发语音转语音系统中最适合的SSL和SSL模型的哪一层。此外，该研究还扩展了基于SSL的MOS预测框架，成功地在合成的自发语音上进行了评估。

    

    自监督学习（SSL）是从大量多样化、质量不一致的语音数据中学习得到的一种有效的语音表示方法，不需要转录。先前的研究表明，SSL是一个有效的中间表示方法，在两阶段的文本转语音（TTS）中，无论是针对朗读语音还是自发语音。然而，尚不清楚哪种SSL以及每个SSL模型中的哪一层适合于自发语音的TTS。为了弥补这个不足，我们扩大了对自发语音TTS中SSL的比较范围，包括6种不同的SSL和每种SSL中的3层。此外，SSL在预测合成语音的主观质量评分（MOS）方面也显示出潜力，但这只在朗读语音MOS预测中进行过研究。我们扩展了之前用于评分朗读语音合成的基于SSL的MOS预测框架，并在合成的自发语音上评估其性能。所有实验在两个不同的自发语音数据集上重复进行。

    Self-supervised learning (SSL) speech representations learned from large amounts of diverse, mixed-quality speech data without transcriptions are gaining ground in many speech technology applications. Prior work has shown that SSL is an effective intermediate representation in two-stage text-to-speech (TTS) for both read and spontaneous speech. However, it is still not clear which SSL and which layer from each SSL model is most suited for spontaneous TTS. We address this shortcoming by extending the scope of comparison for SSL in spontaneous TTS to 6 different SSLs and 3 layers within each SSL. Furthermore, SSL has also shown potential in predicting the mean opinion scores (MOS) of synthesized speech, but this has only been done in read-speech MOS prediction. We extend an SSL-based MOS prediction framework previously developed for scoring read speech synthesis and evaluate its performance on synthesized spontaneous speech. All experiments are conducted twice on two different spontaneou
    
[^59]: 用潜在ODE-LSTM方法增强连续时间序列建模

    Enhancing Continuous Time Series Modelling with a Latent ODE-LSTM Approach. (arXiv:2307.05126v1 [cs.LG])

    [http://arxiv.org/abs/2307.05126](http://arxiv.org/abs/2307.05126)

    该论文提出了使用潜在ODE-LSTM方法增强连续时间序列建模的方法，解决了使用标准RNN进行建模时遇到的问题，包括不规则采样和缺失数据等。该方法使用ODE-RNN模型作为编码器，并使用神经ODE作为解码器，提供了更好的建模效果。

    

    由于其不规则的采样率和高频采样等动态特性，连续时间序列（CTS）广泛应用于许多领域。由于具有不规则采样率的CTS难以使用标准循环神经网络（RNN）进行建模，因此RNN被推广为具有由神经常微分方程（Neural ODE）定义的连续时间隐藏动力学的ODE-RNN模型。另一种提供更好建模效果的方法是潜在ODE模型，该模型构建了一个连续时间模型，在所有时间点上定义了一个潜在状态。潜在ODE模型使用标准RNN作为编码器和神经ODE作为解码器。然而，由于RNN编码器存在缺失数据和不完全定义的潜在变量问题，最近提出了一种使用ODE-RNN模型作为编码器的潜在ODE-RNN模型。由于梯度消失和爆炸问题，潜在ODE模型和潜在ODE-RNN模型都难以训练。

    Due to their dynamic properties such as irregular sampling rate and high-frequency sampling, Continuous Time Series (CTS) are found in many applications. Since CTS with irregular sampling rate are difficult to model with standard Recurrent Neural Networks (RNNs), RNNs have been generalised to have continuous-time hidden dynamics defined by a Neural Ordinary Differential Equation (Neural ODE), leading to the ODE-RNN model. Another approach that provides a better modelling is that of the Latent ODE model, which constructs a continuous-time model where a latent state is defined at all times. The Latent ODE model uses a standard RNN as the encoder and a Neural ODE as the decoder. However, since the RNN encoder leads to difficulties with missing data and ill-defined latent variables, a Latent ODE-RNN model has recently been proposed that uses a ODE-RNN model as the encoder instead. Both the Latent ODE and Latent ODE-RNN models are difficult to train due to the vanishing and exploding gradie
    
[^60]: 通过空间-时间感知图转换器进行交易欺诈检测

    Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer. (arXiv:2307.05121v1 [cs.LG])

    [http://arxiv.org/abs/2307.05121](http://arxiv.org/abs/2307.05121)

    我们提出了一种名为STA-GT的新颖异构图神经网络用于交易欺诈检测，它能够有效学习空间-时间信息，并通过合并全局信息改进表示学习。

    

    如何获取交易的信息表示并进行欺诈交易的识别是确保金融安全的关键部分。最近的研究将图神经网络应用于交易欺诈检测问题。然而，由于结构限制，它们在有效学习空间-时间信息方面遇到了挑战。此外，很少有基于图神经网络的先前检测器意识到合并全局信息的重要性，该全局信息涵盖了相似的行为模式并为判别性表示学习提供了有价值的见解。因此，我们提出了一种新颖的异构图神经网络，称为空间-时间感知图转换器（STA-GT），用于交易欺诈检测问题。具体来说，我们设计了一种时间编码策略来捕捉时间依赖关系，并将其纳入图神经网络框架中，增强了空间-时间信息建模并改进了表达能力。

    How to obtain informative representations of transactions and then perform the identification of fraudulent transactions is a crucial part of ensuring financial security. Recent studies apply Graph Neural Networks (GNNs) to the transaction fraud detection problem. Nevertheless, they encounter challenges in effectively learning spatial-temporal information due to structural limitations. Moreover, few prior GNN-based detectors have recognized the significance of incorporating global information, which encompasses similar behavioral patterns and offers valuable insights for discriminative representation learning. Therefore, we propose a novel heterogeneous graph neural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for transaction fraud detection problems. Specifically, we design a temporal encoding strategy to capture temporal dependencies and incorporate it into the graph neural network framework, enhancing spatial-temporal information modeling and improving expressive
    
[^61]: 通信中的任意分区模型中的$\ell_p$回归问题的随机通信复杂度

    $\ell_p$-Regression in the Arbitrary Partition Model of Communication. (arXiv:2307.05117v1 [cs.DS])

    [http://arxiv.org/abs/2307.05117](http://arxiv.org/abs/2307.05117)

    这个论文研究了通信中的任意分区模型中的$\ell_p$回归问题的随机通信复杂度，给出了该问题的显著改进的界限。对于$p = 2$的情况，我们给出了最优$\tilde{\Theta}(sd^2 + sd/\epsilon)$比特的界限。对于$p \in (1,2)$的情况，我们得到了$\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$的界限。

    

    我们考虑协调器模型中分布式$\ell_p$回归问题的随机通信复杂度，其中$p\in(0,2]$。在这个问题中，有一个协调器和$s$个服务器。第$i$个服务器接收到$A^i\in\{-M, -M+1, \ldots, M\}^{n\times d}$和$b^i\in\{-M, -M+1, \ldots, M\}^n$，协调器想要找到一个$(1+\epsilon)$-近似解来求解$\min_{x\in\mathbb{R}^n} \|(\sum_i A^i)x - (\sum_i b^i)\|_p$。这里的$M \leq \mathrm{poly}(nd)$。数据以加法的方式在服务器间共享，这个模型通常被称为任意分区模型。我们获得了该问题的显著改进的界限。对于$p=2$的情况，即最小二乘回归，我们给出了第一个优化界限：$\tilde{\Theta}(sd^2 + sd/\epsilon)$比特。对于$p\in(1,2)$的情况，我们得到了$\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$的上界。值得注意的是，对于足够大的$d$，我们的主要项只与$1/\epsilon$成线性关系。

    We consider the randomized communication complexity of the distributed $\ell_p$-regression problem in the coordinator model, for $p\in (0,2]$. In this problem, there is a coordinator and $s$ servers. The $i$-th server receives $A^i\in\{-M, -M+1, \ldots, M\}^{n\times d}$ and $b^i\in\{-M, -M+1, \ldots, M\}^n$ and the coordinator would like to find a $(1+\epsilon)$-approximate solution to $\min_{x\in\mathbb{R}^n} \|(\sum_i A^i)x - (\sum_i b^i)\|_p$. Here $M \leq \mathrm{poly}(nd)$ for convenience. This model, where the data is additively shared across servers, is commonly referred to as the arbitrary partition model.  We obtain significantly improved bounds for this problem. For $p = 2$, i.e., least squares regression, we give the first optimal bound of $\tilde{\Theta}(sd^2 + sd/\epsilon)$ bits.  For $p \in (1,2)$,we obtain an $\tilde{O}(sd^2/\epsilon + sd/\mathrm{poly}(\epsilon))$ upper bound. Notably, for $d$ sufficiently large, our leading order term only depends linearly on $1/\epsilo
    
[^62]: 稀疏广义线性模型的合规化

    Conformalization of Sparse Generalized Linear Models. (arXiv:2307.05109v1 [cs.LG])

    [http://arxiv.org/abs/2307.05109](http://arxiv.org/abs/2307.05109)

    本文研究了稀疏广义线性模型的合规化问题。通过利用选择变量在输入数据微小扰动下的不变性，我们使用数值延拓技术高效逼近解决方案路径，从而减少计算合规化集合的复杂度。

    

    给定一系列可观测变量{(x1，y1)，…，(xn，yn)}，合规化预测方法通过仅假设数据的联合分布是置换不变的，为给定x_{n+1}估计y_{n+1}的置信区间，这个置信区间对于任何有限样本量都是有效的。尽管有吸引力，在大多数回归问题中计算这样的置信区间在计算上是不可行的。事实上，在这些情况下，未知变量y_{n+1}可以取无限多个可能的候选值，并且生成合规化集合需要为每个候选重新训练预测模型。在本文中，我们专注于仅使用子集变量进行预测的稀疏线性模型，并使用数值延拓技术高效逼近解决方案路径。我们利用的关键特性是所选变量集在输入数据的微小扰动下是不变的。因此，只需要在变化点枚举和重新拟合模型即可。

    Given a sequence of observable variables $\{(x_1, y_1), \ldots, (x_n, y_n)\}$, the conformal prediction method estimates a confidence set for $y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merely assuming that the joint distribution of the data is permutation invariant. Although attractive, computing such a set is computationally infeasible in most regression problems. Indeed, in these cases, the unknown variable $y_{n+1}$ can take an infinite number of possible candidate values, and generating conformal sets requires retraining a predictive model for each candidate. In this paper, we focus on a sparse linear model with only a subset of variables for prediction and use numerical continuation techniques to approximate the solution path efficiently. The critical property we exploit is that the set of selected variables is invariant under a small perturbation of the input data. Therefore, it is sufficient to enumerate and refit the model only at the change points of
    
[^63]: 对扰动作为时序XAI评估技术的深入探究

    A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI. (arXiv:2307.05104v1 [cs.LG])

    [http://arxiv.org/abs/2307.05104](http://arxiv.org/abs/2307.05104)

    本研究深入探讨了使用扰动作为评估从时间序列模型中提取的归因的方法。通过对多种XAI技术的应用和在多个数据集上进行验证，结果表明扰动分析可以有效评估归因的质量，并为其优点和局限性提供了洞察力。

    

    可解释的人工智能（XAI）近年来引起了极大的关注，因为对机器学习模型透明度和可解释性的需求增加了。特别是，在金融、医疗和气候科学领域，对时间序列数据的XAI变得越来越重要。然而，评估解释质量，如XAI技术提供的归因，仍然具有挑战性。本文对使用扰动来评估从时间序列模型中提取的归因进行了深入分析。扰动分析包括系统地修改输入数据，并评估对XAI方法生成的归因的影响。我们将这种方法应用于几种最先进的XAI技术，并在三个时间序列分类数据集上评估它们的性能。我们的结果表明，扰动分析方法可以有效评估归因的质量，并洞察力地揭示出其优点和局限性。

    Explainable Artificial Intelligence (XAI) has gained significant attention recently as the demand for transparency and interpretability of machine learning models has increased. In particular, XAI for time series data has become increasingly important in finance, healthcare, and climate science. However, evaluating the quality of explanations, such as attributions provided by XAI techniques, remains challenging. This paper provides an in-depth analysis of using perturbations to evaluate attributions extracted from time series models. A perturbation analysis involves systematically modifying the input data and evaluating the impact on the attributions generated by the XAI method. We apply this approach to several state-of-the-art XAI techniques and evaluate their performance on three time series classification datasets. Our results demonstrate that the perturbation analysis approach can effectively evaluate the quality of attributions and provide insights into the strengths and limitati
    
[^64]: 通过任何模型估计语义分割数据中的标签质量和错误

    Estimating label quality and errors in semantic segmentation data via any model. (arXiv:2307.05080v1 [cs.LG])

    [http://arxiv.org/abs/2307.05080](http://arxiv.org/abs/2307.05080)

    通过任何模型，我们可以估计语义分割数据中标签的质量和错误。这导致了一种用于自动检测错误的标签质量评分方法，并且可以帮助确定需要重点检查的数据，以确保高质量的训练/评估数据集。

    

    语义分割数据集的劳动密集型注释过程往往容易出现错误，因为人们很难正确标记每个像素。我们研究了自动检测此类注释错误的算法，尤其是评分标签质量的方法，从而使得得分最低的图像最不可能被正确标记。这有助于优先考虑要审查的数据，以确保高质量的训练/评估数据集，在敏感应用（如医学成像和自动驾驶）中至关重要。广泛适用，我们的标签质量评分依靠训练的分割模型的概率预测-任何模型架构和训练过程都可以利用。本文研究了与DeepLabV3+或FPN分割模型结合使用的7种不同的标签质量评分方法，以在SYNTHIA数据集的一个版本中检测注释错误。通过精确度-召回率评估揭示了一个得分-

    The labor-intensive annotation process of semantic segmentation datasets is often prone to errors, since humans struggle to label every pixel correctly. We study algorithms to automatically detect such annotation errors, in particular methods to score label quality, such that the images with the lowest scores are least likely to be correctly labeled. This helps prioritize what data to review in order to ensure a high-quality training/evaluation dataset, which is critical in sensitive applications such as medical imaging and autonomous vehicles. Widely applicable, our label quality scores rely on probabilistic predictions from a trained segmentation model -- any model architecture and training procedure can be utilized. Here we study 7 different label quality scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation model to detect annotation errors in a version of the SYNTHIA dataset. Precision-recall evaluations reveal a score -- the soft-minimum of the model-estimat
    
[^65]: 有界归纳理性的理论

    A Theory of Bounded Inductive Rationality. (arXiv:2307.05068v1 [cs.AI])

    [http://arxiv.org/abs/2307.05068](http://arxiv.org/abs/2307.05068)

    本论文的主要贡献是提出了一个不假设逻辑全知性的理性决策理论，该理论可用于解决在现实环境中存在计算复杂性和无法对自身进行全面分析的决策问题。

    

    理性选择的主流理论假设逻辑全知性，即当面临决策问题时，一个智能体可以执行所有相关计算，并确定所有相关的逻辑/数学命题的真值。然而，这种假设在某些情况下是不现实的，例如我们对圆周率的远程小数提供赌注，或者智能体面临计算复杂的规划问题时。此外，逻辑全知性的假设在环境中包含智能体自身描述的情况下会产生矛盾。重要的是，博弈理论研究的战略互动是决策问题，这种情况下一个理性智能体由其环境（其他玩家）预测。本文中，我们提出了一个不假设逻辑全知性的理性决策理论。我们考虑反复面临决策问题的智能体（包括对圆周率小数赌注的问题以及与其他智能体对战的游戏）。本文的主要贡献是...

    The dominant theories of rational choice assume logical omniscience. That is, they assume that when facing a decision problem, an agent can perform all relevant computations and determine the truth value of all relevant logical/mathematical claims. This assumption is unrealistic when, for example, we offer bets on remote digits of pi or when an agent faces a computationally intractable planning problem. Furthermore, the assumption of logical omniscience creates contradictions in cases where the environment can contain descriptions of the agent itself. Importantly, strategic interactions as studied in game theory are decision problems in which a rational agent is predicted by its environment (the other players). In this paper, we develop a theory of rational decision making that does not assume logical omniscience. We consider agents who repeatedly face decision problems (including ones like betting on digits of pi or games against other agents). The main contribution of this paper is t
    
[^66]: 投资组合优化：一项比较研究

    Portfolio Optimization: A Comparative Study. (arXiv:2307.05048v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.05048](http://arxiv.org/abs/2307.05048)

    该论文进行了一项比较研究，研究了三种投资组合设计方法，包括均值方差投资组合（MVP）、层次风险均衡（HRP）基于投资组合和自编码器基于投资组合，并将其应用于印度国家证券交易所（NSE）上的十个主题行业的股票。结果显示，在样本外数据上，MVP投资组合的表现最佳。

    

    投资组合优化一直是金融研究界关注的领域。设计一个盈利的投资组合是一个具有挑战性的任务，涉及到对未来股票收益和风险的精确预测。本章介绍了三种投资组合设计方法的比较研究，包括均值方差投资组合（MVP）、层次风险均衡（HRP）基于投资组合和自编码器基于投资组合。这三种投资组合设计方法应用于从印度国家证券交易所（NSE）上选择的十个主题行业的股票的历史价格。投资组合是使用2018年1月1日到2021年12月31日的股票价格数据进行设计的，并且它们的表现在2022年1月1日到2022年12月31日的样本外数据上进行了测试。对投资组合的性能进行了详细结果分析。观察到MVP投资组合在样本外数据上的表现最好，风险调整。

    Portfolio optimization has been an area that has attracted considerable attention from the financial research community. Designing a profitable portfolio is a challenging task involving precise forecasting of future stock returns and risks. This chapter presents a comparative study of three portfolio design approaches, the mean-variance portfolio (MVP), hierarchical risk parity (HRP)-based portfolio, and autoencoder-based portfolio. These three approaches to portfolio design are applied to the historical prices of stocks chosen from ten thematic sectors listed on the National Stock Exchange (NSE) of India. The portfolios are designed using the stock price data from January 1, 2018, to December 31, 2021, and their performances are tested on the out-of-sample data from January 1, 2022, to December 31, 2022. Extensive results are analyzed on the performance of the portfolios. It is observed that the performance of the MVP portfolio is the best on the out-of-sample data for the risk-adjust
    
[^67]: 深度神经网络体系结构的数字系统：一项调查

    Number Systems for Deep Neural Network Architectures: A Survey. (arXiv:2307.05035v1 [cs.NE])

    [http://arxiv.org/abs/2307.05035](http://arxiv.org/abs/2307.05035)

    这项论文调查了用于深度神经网络的替代数字系统，以提高计算效率和资源利用。

    

    深度神经网络（DNN）已成为各种人工智能应用的关键组件。在自动驾驶、健康应用等方面，DNN在某些情况下甚至能比人类表现更好。然而，由于计算复杂性，将DNN部署到资源受限的设备上仍面临着许多挑战，如计算复杂性、能源效率、延迟和成本方面。因此，学术界和工业界正在追求几个研究方向，以加速和高效地实现DNNs。其中一个重要方向是确定适用于DNN处理中涉及的大量数据的适当数据表示。使用传统的数字系统被发现对DNNs来说不够优化。相反，许多研究将重点放在探索合适的数字系统上。本文旨在提供关于更有效表示的替代数字系统的全面调查和讨论。

    Deep neural networks (DNNs) have become an enabling component for a myriad of artificial intelligence applications. DNNs have shown sometimes superior performance, even compared to humans, in cases such as self-driving, health applications, etc. Because of their computational complexity, deploying DNNs in resource-constrained devices still faces many challenges related to computing complexity, energy efficiency, latency, and cost. To this end, several research directions are being pursued by both academia and industry to accelerate and efficiently implement DNNs. One important direction is determining the appropriate data representation for the massive amount of data involved in DNN processing. Using conventional number systems has been found to be sub-optimal for DNNs. Alternatively, a great body of research focuses on exploring suitable number systems. This article aims to provide a comprehensive survey and discussion about alternative number systems for more efficient representation
    
[^68]: FairLay-ML：数据驱动社会关键算法中不公平的直观改善方法

    FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms. (arXiv:2307.05029v1 [cs.LG])

    [http://arxiv.org/abs/2307.05029](http://arxiv.org/abs/2307.05029)

    本论文研究了如何利用开源的机器学习模型解释工具，使普通人可以直观地理解和改善决策支持系统中的不公平问题。

    

    本论文探讨了开源的机器学习（ML）模型解释工具，以了解这些工具是否能够让普通人可视化、理解和建议直观的方法来改善ML决策支持系统中的不公平问题。针对少数群体受偏见数据训练的机器学习模型越来越被用于指导重大的社会决策，迫切需要研究它们在不公平方面的逻辑。由于这个问题对广大公众产生重大影响，理解这些算法中不公平性质和潜在的权衡是非常关键的，不仅仅是社会正义领域的专家或机器学习专家需要理解。现有研究主要集中在数学定义和工具上，以理解和解决不公平模型，其中一些直接提到用户交互工具是未来工作所必需的。本论文介绍了FairLay-ML，一个概念证明工具。

    This thesis explores open-sourced machine learning (ML) model explanation tools to understand whether these tools can allow a layman to visualize, understand, and suggest intuitive remedies to unfairness in ML-based decision-support systems. Machine learning models trained on datasets biased against minority groups are increasingly used to guide life-altering social decisions, prompting the urgent need to study their logic for unfairness. Due to this problem's impact on vast populations of the general public, it is critical for the layperson -- not just subject matter experts in social justice or machine learning experts -- to understand the nature of unfairness within these algorithms and the potential trade-offs. Existing research on fairness in machine learning focuses mostly on the mathematical definitions and tools to understand and remedy unfair models, with some directly citing user-interactive tools as necessary for future work. This thesis presents FairLay-ML, a proof-of-conce
    
[^69]: 发挥正则化策略在具有嘈杂标签的学习中的潜力

    Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels. (arXiv:2307.05025v1 [cs.LG])

    [http://arxiv.org/abs/2307.05025](http://arxiv.org/abs/2307.05025)

    我们研究表明，结合交叉熵损失和正则化策略（如学习率衰减、模型权重平均和数据增强）的简单基准方法可以超过最先进方法，证明了正则化策略的组合在学习嘈杂标签问题中的潜力。

    

    近年来，对于学习嘈杂标签的研究主要集中在设计新算法，以实现对嘈杂训练标签的鲁棒性，并在干净数据上进行泛化。这些算法通常包括复杂的技术，如噪声建模、标签校正和协同训练。在本研究中，我们展示了使用交叉熵损失和常用的正则化策略（如学习率衰减、模型权重平均和数据增强）的简单基准方法可以超过最先进方法。我们的发现表明，采用正则化策略的组合比复杂的算法更有效地应对学习嘈杂标签的挑战。虽然一些正则化策略在之前的学习嘈杂标签研究中已被使用，但它们的全部潜力尚未得到充分探索。我们的结果鼓励重新评估学习嘈杂标签的基准，并促使重新考虑该领域的研究重点。

    In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robustness to noisy training labels while generalizing to clean data. These algorithms often incorporate sophisticated techniques, such as noise modeling, label correction, and co-training. In this study, we demonstrate that a simple baseline using cross-entropy loss, combined with widely used regularization strategies like learning rate decay, model weights average, and data augmentations, can outperform state-of-the-art methods. Our findings suggest that employing a combination of regularization strategies can be more effective than intricate algorithms in tackling the challenges of learning with noisy labels. While some of these regularization strategies have been utilized in previous noisy label learning research, their full potential has not been thoroughly explored. Our results encourage a reevaluation of benchmarks for learning with noisy labels and prompt reconsider
    
[^70]: 特征激活映射：用于图像分类的深度学习模型可视解释

    Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification. (arXiv:2307.05017v1 [cs.CV])

    [http://arxiv.org/abs/2307.05017](http://arxiv.org/abs/2307.05017)

    本文提出了一种名为特征激活映射（FAM）的解释工具，可以解释没有FC层的深度学习模型作为分类器，使其更加可解释、透明和可信。

    

    通过可视化图像上的判别区域，可以理解和解释卷积神经网络（CNN）的决策。为此，提出了基于类激活映射（CAM）的方法作为强大的解释工具，使深度学习模型的预测更加可解释、透明和可信。然而，所有基于CAM的方法（如CAM、Grad-CAM和Relevance-CAM）只能用于解释具有全连接（FC）层作为分类器的CNN模型。值得注意的是，许多深度学习模型在没有FC层的情况下对图像进行分类，例如小样本学习图像分类、对比学习图像分类和图像检索任务。在这项工作中，提出了一种名为特征激活映射（FAM）的事后解释工具，可以解释没有FC层的深度学习模型作为分类器。在所提出的FAM算法中，通过两个图像嵌入之间的相似度得到通道权重的贡献。

    Decisions made by convolutional neural networks(CNN) can be understood and explained by visualizing discriminative regions on images. To this end, Class Activation Map (CAM) based methods were proposed as powerful interpretation tools, making the prediction of deep learning models more explainable, transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM, Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with fully-connected (FC) layers as a classifier. It is worth noting that many deep learning models classify images without FC layers, e.g., few-shot learning image classification, contrastive learning image classification, and image retrieval tasks. In this work, a post-hoc interpretation tool named feature activation map (FAM) is proposed, which can interpret deep learning models without FC layers as a classifier. In the proposed FAM algorithm, the channel-wise contribution weights are derived from the similarity scores between two image emb
    
[^71]: 视频流上的测试时培训

    Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])

    [http://arxiv.org/abs/2307.05014](http://arxiv.org/abs/2307.05014)

    该论文扩展了测试时培训（TTT）到视频流的设置中，提出了在线TTT方法，相对于固定模型基线和离线TTT，在多个任务上都有显著的性能优势，包括实例和全景分割。

    

    先前的研究已经将测试时培训（TTT）确定为一种在测试时进一步改进训练模型的通用框架。在对每个测试实例进行预测之前，模型会使用自监督任务（例如使用掩蔽自动编码器进行图像重建）在同一实例上进行训练。我们将TTT扩展到流式设置中，其中多个测试实例（在我们的情况下为视频帧）按时间顺序到达。我们的扩展是在线TTT：当前模型从上个模型初始化，然后在当前帧和前几个帧的小窗口上进行训练。在线TTT在四个任务上明显优于固定模型基线，在三个实际数据集上的相对改进分别为45%和66%。令人惊讶的是，在线TTT也优于其离线版本，后者访问更多信息，可以训练所有帧而不考虑时间顺序。这与先前的研究结果不同。

    Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using 
    
[^72]: 用声学预测改进RNN-Transducers模型

    Improving RNN-Transducers with Acoustic LookAhead. (arXiv:2307.05006v1 [cs.CL])

    [http://arxiv.org/abs/2307.05006](http://arxiv.org/abs/2307.05006)

    本文提出了一种名为LookAhead的技术，通过提前观察音频输入的未来部分，使RNN-Transducers模型的文本表示更加与声学相符。该技术在准确率上相对降低了5%-20%。

    

    RNN-Transducers（RNN-Ts）已经被广泛接受作为一种端到端的语音转文本模型，因为它们具有高准确率和流式处理能力。传统的RNN-T模型独立地编码输入音频和文本上下文，并通过一个薄型联合网络将两种编码结合起来。虽然这种架构提供了SOTA的流式处理准确率，但也使模型对强语言模型（LM）的偏见脆弱，这表现为在没有声学证据的情况下对文本进行多步幻觉生成。在本文中，我们提出了LookAhead技术，通过提前观察音频输入的未来部分，使文本表示更具有声学基础。这种技术在领域内和领域外的评估集上相对错误率有显著的5%-20%的降低。

    RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end model for speech to text conversion because of their high accuracy and streaming capabilities. A typical RNN-T independently encodes the input audio and the text context, and combines the two encodings by a thin joint network. While this architecture provides SOTA streaming accuracy, it also makes the model vulnerable to strong LM biasing which manifests as multi-step hallucination of text without acoustic evidence. In this paper we propose LookAhead that makes text representations more acoustically grounded by looking ahead into the future within the audio input. This technique yields a significant 5%-20% relative reduction in word error rate on both in-domain and out-of-domain evaluation sets.
    
[^73]: 多智能体强化学习中的控制作为概率推理的新兴通信机制

    Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning. (arXiv:2307.05004v1 [cs.AI])

    [http://arxiv.org/abs/2307.05004](http://arxiv.org/abs/2307.05004)

    本文提出了一种将控制与概率推理结合的新颖的通信机制，应用于多智能体强化学习中。智能体通过推理控制其动作，并通过消息进行通信，从而实现协作任务。

    

    本文提出了一种新型的生成概率模型，将新兴通信和多智能体强化学习进行了整合。智能体通过概率推理进行动作规划，称为控制作为推理，并使用潜在变量和根据规划的动作进行估计的消息进行通信。通过这些消息，每个智能体可以发送关于其动作的信息，并了解另一个智能体的动作信息。因此，智能体根据估计的消息来改变其动作，以实现协作任务。这种消息的推理可以被视为通信，并且可以通过Metropolis-Hasting命名游戏来进行形式化。通过在网格世界环境中的实验，我们展示了提出的概率图模型可以推断出有意义的消息，以实现协作任务。

    This paper proposes a generative probabilistic model integrating emergent communication and multi-agent reinforcement learning. The agents plan their actions by probabilistic inference, called control as inference, and communicate using messages that are latent variables and estimated based on the planned actions. Through these messages, each agent can send information about its actions and know information about the actions of another agent. Therefore, the agents change their actions according to the estimated messages to achieve cooperative tasks. This inference of messages can be considered as communication, and this procedure can be formulated by the Metropolis-Hasting naming game. Through experiments in the grid world environment, we show that the proposed PGM can infer meaningful messages to achieve the cooperative task.
    
[^74]: 通过在线回归进行选择性采样和模仿学习

    Selective Sampling and Imitation Learning via Online Regression. (arXiv:2307.04998v1 [cs.LG])

    [http://arxiv.org/abs/2307.04998](http://arxiv.org/abs/2307.04998)

    本论文提出了一种通过在线回归实现选择性采样和模仿学习的方法，解决了在只有噪声专家反馈的情况下的问题。算法不需要大量样本即可成功，并取得了最佳的回归和查询次数界限。

    

    我们考虑通过主动查询嘈杂的专家来进行模仿学习（IL）的问题。虽然模仿学习在实践中取得了成功，但大部分先前的工作都假设可以获得无噪声的专家反馈，而这在许多应用中是不切实际的。实际上，当只能获得嘈杂的专家反馈时，依赖纯离线数据的算法（非交互式IL）被证明需要大量的样本才能成功。相反，在这项工作中，我们提供了一种交互式IL算法，它使用选择性采样来主动查询嘈杂的专家反馈。我们的贡献有两个方面：首先，我们提供了一种适用于通用函数类和多个动作的新选择性采样算法，并获得了迄今为止最好的回归和查询次数界限。其次，我们将这个分析扩展到了具有嘈杂专家反馈的IL问题，并提供了一种新的IL算法来进行有限查询。

    We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.  Our algorithm for sele
    
[^75]: 利用自动生成的知识图谱和强化学习增强推荐系统

    Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])

    [http://arxiv.org/abs/2307.04996](http://arxiv.org/abs/2307.04996)

    本文介绍了两种基于知识图谱的方法，一种使用强化学习，另一种使用XGBoost算法，用于个性化文章推荐。这些方法利用自动生成的知识图谱，并在一个大型跨国金融服务公司的客户中进行了实证研究。

    

    个性化推荐在直接营销中越来越重要，激发了通过知识图谱（KG）应用来提升客户体验的研究动机。例如，在金融服务领域，公司可以通过向客户提供相关金融文章来培养关系，促进客户参与和促进知情的金融决策。尽管一些方法专注于基于KG的推荐系统以改进内容，但在本研究中，我们专注于可解释的基于KG的推荐系统来进行决策。为此，我们提出了两种基于知识图谱的个性化文章推荐方法，用于一家大型跨国金融服务公司的一组客户。第一种方法使用强化学习，第二种方法使用XGBoost算法来向客户推荐文章。这两种方法都利用从结构化（表格数据）和非结构化数据（大量文本数据）生成的KG。

    Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body o
    
[^76]: PowerFusion: 一种带有显式数据移动描述和指令级图形IR的张量编译器

    PowerFusion: A Tensor Compiler with Explicit Data Movement Description and Instruction-level Graph IR. (arXiv:2307.04995v1 [cs.LG])

    [http://arxiv.org/abs/2307.04995](http://arxiv.org/abs/2307.04995)

    PowerFusion是一种张量编译器，通过考虑计算和数据移动优化，生成高性能内存密集运算符的高效代码。

    

    深度神经网络（DNNs）在不同领域中非常重要。为了加速DNN计算，提出了张量编译器以在不同的领域特定加速器上生成高效的代码。现有的张量编译器主要关注优化计算效率。然而，由于加速器的计算性能远远超过内存性能，内存访问成为关键的性能瓶颈。目前张量编译器中间表示（IR）中对内存访问和数据依赖的直接描述不足，给生成内存高效代码带来了显著挑战。在本文中，我们提出了IntelliGen，一种可以通过考虑计算和数据移动优化来生成高性能内存密集运算符的张量编译器。IntelliGen使用GIR来表示DNN程序，其中包含指示其计算、数据移动和并行策略的基元。这些信息将被。。。。

    Deep neural networks (DNNs) are of critical use in different domains. To accelerate DNN computation, tensor compilers are proposed to generate efficient code on different domain-specific accelerators. Existing tensor compilers mainly focus on optimizing computation efficiency. However, memory access is becoming a key performance bottleneck because the computational performance of accelerators is increasing much faster than memory performance. The lack of direct description of memory access and data dependence in current tensor compilers' intermediate representation (IR) brings significant challenges to generate memory-efficient code.  In this paper, we propose IntelliGen, a tensor compiler that can generate high-performance code for memory-intensive operators by considering both computation and data movement optimizations. IntelliGen represent a DNN program using GIR, which includes primitives indicating its computation, data movement, and parallel strategies. This information will be 
    
[^77]: 使用符合预测的方法对维里黑洞质量的不确定性进行量化

    Uncertainty Quantification of the Virial Black Hole Mass with Conformal Prediction. (arXiv:2307.04993v1 [astro-ph.CO])

    [http://arxiv.org/abs/2307.04993](http://arxiv.org/abs/2307.04993)

    该研究提出了使用符合预测的方法在机器学习设置中量化黑洞预测的不确定性。与传统方法相比，该方法能够提供更有用的预测区间指标，并调整到黑洞质量及其相关属性。

    

    精确测量黑洞质量对于理解黑洞和宿主星系的共同演化至关重要。直接测量黑洞质量通常仅限于最近的星系，而高红移的物体则使用间接方法，即利用单次纪元的维里黑洞质量估计。然而，该方法受到偏差和不确定性的影响，因为其依赖于一个小样本的本地活动星系核的比例关系。在这项研究中，我们提出了将符合化分位数回归（CQR）应用于机器学习设置中，以量化黑洞预测的不确定性。我们将CQR与各种预测区间技术进行比较，并证明CQR可以提供更有用的预测区间指标。与基准方法相比，我们展示了CQR方法能够调整到黑洞质量及其相关属性。

    Precise measurements of the black hole mass are essential to gain insight on the black hole and host galaxy co-evolution. A direct measure of the black hole mass is often restricted to nearest galaxies and instead, an indirect method using the single-epoch virial black hole mass estimation is used for objects at high redshifts. However, this method is subjected to biases and uncertainties as it is reliant on the scaling relation from a small sample of local active galactic nuclei. In this study, we propose the application of conformalised quantile regression (CQR) to quantify the uncertainties of the black hole predictions in a machine learning setting. We compare CQR with various prediction interval techniques and demonstrated that CQR can provide a more useful prediction interval indicator. In contrast to baseline approaches for prediction interval estimation, we show that the CQR method provides prediction intervals that adjust to the black hole mass and its related properties. That
    
[^78]: 单调的深度Boltzmann机器

    Monotone deep Boltzmann machines. (arXiv:2307.04990v1 [cs.LG])

    [http://arxiv.org/abs/2307.04990](http://arxiv.org/abs/2307.04990)

    在这项工作中，我们提出了一种新的限制模型，即单调DBM，它允许每一层具有任意的自连接，但通过一种方式限制了权重，以保证存在和全局唯一的均场不动点。

    

    深度Boltzmann机器(DBMs)是最早研究的"深度"学习方法之一，它是由一个描述网络中所有变量/节点的可能性的成对能量函数所控制的多层概率模型。在实际应用中，为了实现更高效的推理，DBMs通常会受到一些限制，例如通过"限制性" Boltzmann机器(RBM)架构（不允许层间连接）。在这项工作中，我们重新审视了通用的DBM方法，并提出了一个问题：是否存在其他可能的设计限制，以实现高效的（近似）推理？具体地，我们开发了一种新的限制模型，即单调DBM，它允许每一层具有任意的自连接，但通过一种方式限制了权重，以保证存在和全局唯一的均场不动点。为此，我们利用了最近提出的单调深度均衡模型的工具，并展示了一个特定的...

    Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods ever studied, are multi-layered probabilistic models governed by a pairwise energy function that describes the likelihood of all variables/nodes in the network. In practice, DBMs are often constrained, i.e., via the \emph{restricted} Boltzmann machine (RBM) architecture (which does not permit intra-layer connections), in order to allow for more efficient inference. In this work, we revisit the generic DBM approach, and ask the question: are there other possible restrictions to their design that would enable efficient (approximate) inference? In particular, we develop a new class of restricted model, the monotone DBM, which allows for arbitrary self-connection in each layer, but restricts the \emph{weights} in a manner that guarantees the existence and global uniqueness of a mean-field fixed point. To do this, we leverage tools from the recently-proposed monotone Deep Equilibrium model and show that a particular 
    
[^79]: 用于下游治疗效果估计的贝叶斯因果发现方法的基准测试

    Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v1 [cs.LG])

    [http://arxiv.org/abs/2307.04988](http://arxiv.org/abs/2307.04988)

    该研究评估了六种基准因果发现方法和一种新提出的基于 GFlowNets 的方法在治疗效果估计任务中的表现，并发现 GFlowNets 具有捕捉各种有用和多样的平均处理效应模式的能力。

    

    决策中因果性的实际应用被广泛认可，因果发现和推理在本质上是相互交织的。然而，在因果发现方法的评估中存在明显的差距，对下游推理的重视程度不足。为了填补这一空白，我们评估了六种已建立的基准因果发现方法和一种基于 GFlowNets 的新方法在治疗效果估计的下游任务上的表现。通过实施一个稳健的评估过程，我们为治疗效果估计的这些因果发现方法的有效性提供了有价值的见解，考虑了合成和真实场景以及低数据场景。此外，我们研究的结果表明，GFlowNets 具有有效捕捉各种有用和多样的平均处理效应模式的能力。

    The practical utility of causality in decision-making is widely recognized, with causal discovery and inference being inherently intertwined. Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference. To address this gap, we evaluate six established baseline causal discovery methods and a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation. Through the implementation of a robust evaluation procedure, we offer valuable insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios. Furthermore, the results of our study demonstrate that GFlowNets possess the capability to effectively capture a wide range of useful and diverse ATE modes.
    
[^80]: 大型语言模型中RLHF的秘密 第一部分：PPO

    Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])

    [http://arxiv.org/abs/2307.04964](http://arxiv.org/abs/2307.04964)

    本论文研究了大型语言模型中RLHF的秘密，重点关注了奖励模型、PPO和进程监督等技术路径，探索如何解决RLHF的稳定训练问题。

    

    大型语言模型（LLMs）为推动人工通用智能的进展提供了蓝图。其主要目标是成为以人为中心的（有益、诚实和无害）助手。与人类的对齐具有至关重要的意义，强化学习与人类反馈（RLHF）成为支撑这一追求的关键技术范式。当前的技术路线通常包括用于衡量人类偏好的奖励模型、用于优化策略模型输出的近端策略优化（PPO）以及用于改善逐步推理能力的进程监督。然而，由于奖励设计、环境交互和代理训练的挑战，再加上大型语言模型的试验成本巨大，对于AI研究人员来说，激励技术对齐和LLMs的安全着陆存在重大障碍。RLHF的稳定训练仍然是一个难题。

    Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
    
[^81]: DyCL: 通过程序重写和图优化实现动态神经网络编译

    DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization. (arXiv:2307.04963v1 [cs.CL])

    [http://arxiv.org/abs/2307.04963](http://arxiv.org/abs/2307.04963)

    DyCL通过程序重写和图优化的方式，解决了现有DL编译器在编译具有动态特性的神经网络时的困难，提供了一种通用的方法来成功编译动态神经网络。

    

    DL编译器的主要功能是将使用高级DL框架（如PyTorch和TensorFlow）编写的DNN程序转换为可移植的可执行文件。然而，现有的DL编译器依赖于跟踪机制，该机制涉及向神经网络程序提供运行时输入，并跟踪程序执行路径以生成编译所需的计算图。然而，这种机制在处理具有根据输入变化的计算图的现代动态神经网络（DyNNs）时存在问题。因此，传统的DL编译器在将DyNNs准确编译为可执行代码方面遇到困难。为了解决这个问题，我们提出了\tool，这是一种通用方法，可以使任何现有的DL编译器成功编译DyNNs。\tool通过引入一种编译机制来解决DyNNs的动态特性，该机制重新分配原始控制和数据流的流程。

    DL compiler's primary function is to translate DNN programs written in high-level DL frameworks such as PyTorch and TensorFlow into portable executables. These executables can then be flexibly executed by the deployed host programs. However, existing DL compilers rely on a tracing mechanism, which involves feeding a runtime input to a neural network program and tracing the program execution paths to generate the computational graph necessary for compilation. Unfortunately, this mechanism falls short when dealing with modern dynamic neural networks (DyNNs) that possess varying computational graphs depending on the inputs. Consequently, conventional DL compilers struggle to accurately compile DyNNs into executable code. To address this limitation, we propose \tool, a general approach that enables any existing DL compiler to successfully compile DyNNs. \tool tackles the dynamic nature of DyNNs by introducing a compilation mechanism that redistributes the control and data flow of the origi
    
[^82]: 利用人类好奇心的网络理论进行内在驱动的图探索

    Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])

    [http://arxiv.org/abs/2307.04962](http://arxiv.org/abs/2307.04962)

    在这项工作中，我们通过应用人类好奇心的两个理论，发展了一种内在驱动的图探索方法。我们利用图神经网络的强化学习将拓扑特征作为奖励，从而实现了对图结构数据的探索。在多类合成生成图上进行的实验证明，我们的方法不仅可以推广到更大的环境，还可以进行更长的探索步行。同时，我们的方法比传统的贪婪评估方法更高效。

    

    内在驱动的探索在强化学习中已被证明具有用途，即使没有额外的外在奖励。当环境自然表示为图时，如何最好地引导探索仍是一个未解决的问题。在这项工作中，我们提出了一种新的方法，通过人类好奇心的两个理论：信息差理论和压缩进展理论，来激励对图结构数据进行探索。这些理论将好奇心视为对环境中访问节点所引发的子图的拓扑特征进行优化的内在动机。我们将这些提出的特征作为基于图神经网络的强化学习的奖励。在多个类别的合成生成图上，我们发现训练代理可以推广到更大的环境和比训练过程中更长的探索性步行。我们的方法的计算效率高于相关拓扑属性的贪婪评估。所提出的内在动机产生的奖励在多类合成生成图生成上推广良好，并且在训练期间能够在更大的环境中进行更长的探索步行。

    Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
    
[^83]: 非累积目标的强化学习

    Reinforcement Learning with Non-Cumulative Objective. (arXiv:2307.04957v1 [cs.LG])

    [http://arxiv.org/abs/2307.04957](http://arxiv.org/abs/2307.04957)

    本文研究了最优控制和强化学习中非累积目标的挑战，并提出了修改现有算法的方法来优化这些目标。研究结果表明，在贝尔曼最优性方程中使用广义运算可以更好地处理非累积目标。

    

    在强化学习中，目标几乎总是定义为沿过程中奖励的\emph{累积}函数。然而，在许多最优控制和强化学习问题中，尤其是在通信和网络领域中，目标并不自然地表达为奖励的求和。本文中，我们认识到各种问题中非累积目标的普遍存在，并提出了修改现有算法以优化这些目标的方法。具体来说，我们深入研究了许多最优控制和强化学习算法的基本构建模块：贝尔曼最优性方程。为了优化非累积目标，我们用与目标相对应的广义运算替换了贝尔曼更新规则中的原始求和运算。此外，我们提供了广义运算形式的足够条件以及对马尔可夫决策的假设。

    In reinforcement learning, the objective is almost always defined as a \emph{cumulative} function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision 
    
[^84]: 混合隐马尔可夫LSTM用于短期交通流量预测

    Hybrid hidden Markov LSTM for short-term traffic flow prediction. (arXiv:2307.04954v1 [cs.LG])

    [http://arxiv.org/abs/2307.04954](http://arxiv.org/abs/2307.04954)

    该论文介绍了一种混合隐马尔可夫LSTM模型，用于短期交通流量预测。研究发现，深度学习方法在预测交通变量方面优于传统的参数模型。这种模型结合了循环神经网络和隐马尔可夫模型的优势，能够捕捉交通系统的复杂动态模式和非平稳性。

    

    深度学习方法在预测交通变量的短期和近短期未来方面已经优于参数模型，如历史平均、ARIMA和其变体，这对于交通管理至关重要。具体来说，循环神经网络（RNN）及其变体（例如长短期记忆）被设计用于保留长期时序相关性，因此非常适用于建模序列。然而，多制度模型假设交通系统以不同特征的多个状态（例如畅通、拥堵）演变，因此需要训练不同模型以表征每个制度内的交通动态。例如，使用隐马尔可夫模型进行制度识别的马尔可夫切换模型能够捕捉复杂的动态模式和非平稳性。有趣的是，隐马尔可夫模型和LSTM都可以用于建模从一组潜在的或隐藏状态变量中的观察序列。在LSTM中，潜在变量可以从上一个时间步的隐藏状态变量传递过来。

    Deep learning (DL) methods have outperformed parametric models such as historical average, ARIMA and variants in predicting traffic variables into short and near-short future, that are critical for traffic management. Specifically, recurrent neural network (RNN) and its variants (e.g. long short-term memory) are designed to retain long-term temporal correlations and therefore are suitable for modeling sequences. However, multi-regime models assume the traffic system to evolve through multiple states (say, free-flow, congestion in traffic) with distinct characteristics, and hence, separate models are trained to characterize the traffic dynamics within each regime. For instance, Markov-switching models with a hidden Markov model (HMM) for regime identification is capable of capturing complex dynamic patterns and non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an observation sequence from a set of latent or, hidden state variables. In LSTM, the latent variable 
    
[^85]: 紧凑型双重融合网络用于边缘检测

    Compact Twice Fusion Network for Edge Detection. (arXiv:2307.04952v1 [cs.CV])

    [http://arxiv.org/abs/2307.04952](http://arxiv.org/abs/2307.04952)

    本文提出了一种紧凑型双重融合网络（CTFN），用于边缘检测，在保持模型紧凑性的同时完全整合多尺度特征。其中包括语义增强模块和伪像素级加权模块，还使用动态焦点损失函数处理纹理噪声带来的挑战。

    

    边缘检测社区逐渐认识到多尺度特征的重要性。然而，多尺度特征的融合增加了模型的复杂性，对实际应用不友好。本文提出了一种紧凑型双重融合网络（CTFN），以完全整合多尺度特征并保持模型的紧凑性。CTFN包括两个轻量级多尺度特征融合模块：语义增强模块（SEM），可以利用粗尺度特征中包含的语义信息来引导细尺度特征的学习；伪像素级加权（PPW）模块，通过给所有特征分配权重来聚合多尺度特征的互补优点。尽管如此，纹理噪声的干扰仍然使得一些像素的正确分类成为挑战。对于这些难样本，我们提出了一种新的损失函数，称为动态焦点损失，它重新塑造了标准的损失函数以解决这个问题。

    The significance of multi-scale features has been gradually recognized by the edge detection community. However, the fusion of multi-scale features increases the complexity of the model, which is not friendly to practical application. In this work, we propose a Compact Twice Fusion Network (CTFN) to fully integrate multi-scale features while maintaining the compactness of the model. CTFN includes two lightweight multi-scale feature fusion modules: a Semantic Enhancement Module (SEM) that can utilize the semantic information contained in coarse-scale features to guide the learning of fine-scale features, and a Pseudo Pixel-level Weighting (PPW) module that aggregate the complementary merits of multi-scale features by assigning weights to all features. Notwithstanding all this, the interference of texture noise makes the correct classification of some pixels still a challenge. For these hard samples, we propose a novel loss function, coined Dynamic Focal Loss, which reshapes the standard
    
[^86]: DDGM: 通过扩散去噪梯度加权最小化求解逆问题

    DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization. (arXiv:2307.04946v1 [cs.CV])

    [http://arxiv.org/abs/2307.04946](http://arxiv.org/abs/2307.04946)

    本文提出了一种通过扩散去噪梯度加权最小化求解逆问题的方法。该方法将传统的基于梯度的最小化重构误差与去噪相结合，每一步都添加噪声，使得迭代动力学类似于Langevin或扩散过程。通过模拟倾斜视图进行实证研究，我们发现该方法可以在只有50个倾斜视图的情况下达到高精度。

    

    逆问题一般需要一个正则化器或先验来得到一个良好的解决方案。最近一种趋势是通过训练卷积网络去降噪图像，并在求解逆问题时将这个网络作为先验。一些方法依赖于正向算子的奇异值分解，而其他一些在运行时通过去噪网络进行反向传播。我们在这里提出了一种更简单的方法，将传统的基于梯度的最小化重构误差与去噪相结合。每一步都添加噪声，使得迭代动力学类似于Langevin或扩散过程。添加的噪声水平和去噪步骤的大小都会随时间指数衰减。我们将这种方法应用于从多个倾角获取的电子显微图的层析重建问题。通过使用模拟倾斜视图进行实证研究，我们找到了适用于我们方法的参数设置，得到了良好的结果。我们证明，只需50个倾斜视图就可以达到高精度。

    Inverse problems generally require a regularizer or prior for a good solution. A recent trend is to train a convolutional net to denoise images, and use this net as a prior when solving the inverse problem. Several proposals depend on a singular value decomposition of the forward operator, and several others backpropagate through the denoising net at runtime. Here we propose a simpler approach that combines the traditional gradient-based minimization of reconstruction error with denoising. Noise is also added at each step, so the iterative dynamics resembles a Langevin or diffusion process. Both the level of added noise and the size of the denoising step decay exponentially with time. We apply our method to the problem of tomographic reconstruction from electron micrographs acquired at multiple tilt angles. With empirical studies using simulated tilt views, we find parameter settings for our method that produce good results. We show that high accuracy can be achieved with as few as 50 
    
[^87]: 基准测试针对联邦领域泛化的算法

    Benchmarking Algorithms for Federated Domain Generalization. (arXiv:2307.04942v1 [cs.LG])

    [http://arxiv.org/abs/2307.04942](http://arxiv.org/abs/2307.04942)

    本论文介绍了一种针对联邦领域泛化的基准测试方法，并评估了13种联邦DG方法。研究结果表明，在大量客户端和高异质性的情况下，联邦DG仍存在显著的性能差距。

    

    尽管先前的领域泛化（DG）基准考虑了训练-测试数据集的异质性，我们评估了引入联邦学习（FL）特定挑战的联邦DG。此外，我们还在客户端本地数据集中探索基于领域的异质性-一个现实的联邦DG场景。先前的联邦DG评估在客户端数量或异质性以及数据集多样性方面存在限制。为了填补这一差距，我们提出了一种联邦DG基准测试方法，可以控制客户端的数量和异质性，并提供数据集难度的度量标准。然后，我们应用我们的方法来评估13种联邦DG方法，包括适应FL环境的集中DG方法、处理客户端异质性的FL方法以及专为联邦DG设计的方法。我们的结果表明，尽管取得了一些进展，但在大量客户端和高客户端异质性的情况下，联邦DG仍存在显著的性能差距。

    While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges. Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario. Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity. To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty. We then apply our methodology to evaluate 13 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG. Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client h
    
[^88]: 提高图神经网络公平性：一种图反事实角度

    Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective. (arXiv:2307.04937v1 [cs.LG])

    [http://arxiv.org/abs/2307.04937](http://arxiv.org/abs/2307.04937)

    这项研究提出了以因果视角看待公平图学习问题的框架CAF，通过选择训练数据中的反事实来避免图神经网络中的偏见。

    

    图神经网络在图的表示学习中展现了出色的能力，促进了各种任务的进行。尽管它们在建模图中表现出色，但最近的研究表明，GNN倾向于从训练数据中继承和放大偏见，引起了在高风险场景中使用GNN的担忧。因此，已经做出了许多努力来实现公平感知的GNN。然而，大多数现有的公平GNN通过采用统计公平概念来学习公平节点表示，但在统计异常存在的情况下，这种方法可能无法减轻偏见。受因果理论的启发，有几种方法利用图反事实公平性来减轻不公平的根本原因。然而，这些方法会受到通过扰动或生成获得的非现实反事实的影响。在本文中，我们以因果视角看待公平图学习问题。在因果分析的指导下，我们提出了一种新的框架CAF，它可以从训练数据中选择反事实以避免偏见。

    Graph neural networks have shown great ability in representation (GNNs) learning on graphs, facilitating various tasks. Despite their great performance in modeling graphs, recent works show that GNNs tend to inherit and amplify the bias from training data, causing concerns of the adoption of GNNs in high-stake scenarios. Hence, many efforts have been taken for fairness-aware GNNs. However, most existing fair GNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies. Motivated by causal theory, there are several attempts utilizing graph counterfactual fairness to mitigate root causes of unfairness. However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation. In this paper, we take a causal view on fair graph learning problem. Guided by the casual analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid 
    
[^89]: 安全强化学习的概率性反例引导

    Probabilistic Counterexample Guidance for Safer Reinforcement Learning. (arXiv:2307.04927v1 [cs.LG])

    [http://arxiv.org/abs/2307.04927](http://arxiv.org/abs/2307.04927)

    本文提出了一种安全强化学习方法，通过引导训练中的反例来解决安全探索的问题，该方法将连续和离散状态空间系统抽象为紧凑的模型，并利用概率性反例生成构建最小化仿真子模型，以揭示安全需求的违反情况。

    

    安全探索旨在解决强化学习在安全关键场景中的局限性，其中在试错学习过程中的失败可能会导致高成本。存在多种方法来整合外部知识或使用近距离传感器数据来限制对不安全状态的探索。然而，在未知环境中减少探索风险仍然具有挑战性，因为代理必须在探索过程中发现安全威胁。本文通过采用安全需求的反例引导训练来解决安全探索问题。我们的方法将连续和离散状态空间系统抽象为紧凑的抽象模型，代表代理在探索过程中获得的与安全相关的知识。然后，我们利用概率性反例生成构建最小化仿真子模型，以揭示安全需求的违反情况，代理可以使用这些模型在离线环境中进行高效的训练，以优化其策略，减小安全风险。

    Safe exploration aims at addressing the limitations of Reinforcement Learning (RL) in safety-critical scenarios, where failures during trial-and-error learning may incur high costs. Several methods exist to incorporate external knowledge or to use proximal sensor data to limit the exploration of unsafe states. However, reducing exploration risks in unknown environments, where an agent must discover safety threats during exploration, remains challenging. In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement. Our method abstracts both continuous and discrete state-space systems into compact abstract models representing the safety-relevant knowledge acquired by the agent during exploration. We then exploit probabilistic counterexample generation to construct minimal simulation submodels eliciting safety requirement violations, where the agent can efficiently train offline to refine its policy towards minimising the 
    
[^90]: SimpleMTOD: 一种用于符号化场景表示的多模态任务导向对话的简易语言模型

    SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation. (arXiv:2307.04907v1 [cs.CL])

    [http://arxiv.org/abs/2307.04907](http://arxiv.org/abs/2307.04907)

    SimpleMTOD是一个简单的语言模型，将多模态任务导向对话的子任务转化为序列预测任务，并引入了局部和非局部的对象标记来捕捉视觉场景的语义。它在SIMMC 2.0测试集的回应生成子任务中取得了最先进的BLEU分数，同时在其他多模态子任务中也表现出色。

    

    SimpleMTOD是一个简易语言模型，将多模态任务导向对话中的几个子任务重新构建为序列预测任务。SimpleMTOD基于大规模的基于转换器的自回归架构构建而成，该架构已经在单模态任务导向对话中取得了成功，并且有效地利用了预训练的GPT-2进行迁移学习。为了捕捉视觉场景的语义，我们引入了局部和非局部的对象标记。非局部的对象标记表示对象的类型而不是具体的对象本身，在整个数据集中具有一致的含义。SimpleMTOD在SIMMC 2.0测试集中的回应生成子任务中取得了最先进的BLEU分数（0.327），同时在其他多模态子任务中表现出色：消歧、指代消解和对话状态跟踪。尽管采取了极简的方法来提取视觉（和非视觉）信息，但SimpleMTOD仍然取得了良好的成绩。

    SimpleMTOD is a simple language model which recasts several sub-tasks in multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is built on a large-scale transformer-based auto-regressive architecture, which has already proven to be successful in uni-modal task-oriented dialogues, and effectively leverages transfer learning from pre-trained GPT-2. In-order to capture the semantics of visual scenes, we introduce both local and de-localized tokens for objects within a scene. De-localized tokens represent the type of an object rather than the specific object itself and so possess a consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0 test-std dataset while performing on par in other multimodal sub-tasks: Disambiguation, Coreference Resolution, and Dialog State Tracking. This is despite taking a minimalist approach for extracting visual (and non-visual) information. In addi
    
[^91]: FedYolo: 使用预训练的Transformer模型增强联邦学习

    FedYolo: Augmenting Federated Learning with Pretrained Transformers. (arXiv:2307.04905v1 [cs.LG])

    [http://arxiv.org/abs/2307.04905](http://arxiv.org/abs/2307.04905)

    本文研究了在联邦学习中使用预训练的Transformer模型来实现在设备上的学习目标，探讨了模型大小和模块化的作用，并证明了规模较大可以提高异构性的鲁棒性。

    

    机器学习应用的增长和多样性促使我们重新思考使用移动和边缘设备进行学习的方式。我们如何解决不同客户目标和稀缺异构数据的学习问题？虽然联邦学习旨在解决这些问题，但它面临着统一解决方案的挑战。大型Transformer模型已被证明可以在各种任务中工作，并实现了显著的少样本适应能力。这引发了一个问题：客户能否使用单个通用模型，而不是为每个任务创建定制模型，并遵守设备和网络的限制？在这项工作中，我们使用预训练的Transformer模型来实现这些设备上的学习目标，并深入探讨模型大小和模块化的作用，其中模块化指的是通过提示或适配器等模块进行适应。在联邦学习方面，我们证明了：（1）规模较大可以缩小不同方法之间的准确性差距，并提高异构性的鲁棒性。

    The growth and diversity of machine learning applications motivate a rethinking of learning with mobile and edge devices. How can we address diverse client goals and learn with scarce heterogeneous data? While federated learning aims to address these issues, it has challenges hindering a unified solution. Large transformer models have been shown to work across a variety of tasks achieving remarkable few-shot adaptation. This raises the question: Can clients use a single general-purpose model, rather than custom models for each task, while obeying device and network constraints? In this work, we investigate pretrained transformers (PTF) to achieve these on-device learning goals and thoroughly explore the roles of model size and modularity, where the latter refers to adaptation through modules such as prompts or adapters. Focusing on federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy gaps between alternative approaches and improves heterogeneity robustness. Sc
    
[^92]: 快速动态时间规整和聚类在C++中的应用

    Fast dynamic time warping and clustering in C++. (arXiv:2307.04904v1 [eess.SP])

    [http://arxiv.org/abs/2307.04904](http://arxiv.org/abs/2307.04904)

    这种方法提出了一种计算效率高的动态时间规整（DTW）和时间序列数据聚类的方法，在保证全局最优性不是必需的情况下，使用混合整数规划（MIP）和k-medoids聚类来增加速度，同时实现了任务级并行化，通过测试发现比其他选项快33%，在大数据集上速度提高到64%。

    

    我们提出了一种计算效率高的动态时间规整（DTW）和时间序列数据聚类的方法。该方法将时间序列数据的动态规整问题视为使用动态规划求解的优化问题，然后使用混合整数规划（MIP）求解第二个优化问题来进行时间序列数据聚类。当全局最优性的证明不是必需时，还可以使用k-medoids聚类来增加速度。我们的方法的改进效率源于聚类和DTW的任务级并行化。我们使用UCR时间序列存档对我们的方法进行了测试，发现当使用相同的聚类方法时，我们的方法的速度平均比下一个最快的选项快33%。在考虑只有较大数据集（超过1000个时间序列）时，速度提高到64%。由于DTW计算速度较快，MIP聚类在少量较长时间序列上效果最好。

    We present an approach for computationally efficient dynamic time warping (DTW) and clustering of time-series data. The method frames the dynamic warping of time series datasets as an optimisation problem solved using dynamic programming, and then clusters time series data by solving a second optimisation problem using mixed-integer programming (MIP). There is also an option to use k-medoids clustering for increased speed, when a certificate for global optimality is not essential. The improved efficiency of our approach is due to task-level parallelisation of the clustering alongside DTW. Our approach was tested using the UCR Time Series Archive, and was found to be, on average, 33% faster than the next fastest option when using the same clustering method. This increases to 64% faster when considering only larger datasets (with more than 1000 time series). The MIP clustering is most effective on small numbers of longer time series, because the DTW computation is faster than other appro
    
[^93]: 使用循环Transformer学习解决约束满足问题

    Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer. (arXiv:2307.04895v1 [cs.AI])

    [http://arxiv.org/abs/2307.04895](http://arxiv.org/abs/2307.04895)

    循环Transformer是一种可行的方法来学习解决约束满足问题。相比于类似的方法，循环Transformer具有明显的优势，可以处理视觉输入，成功解决符号基础问题，并实现样本高效学习和半监督学习。

    

    约束满足问题（CSPs）是关于找到满足给定约束条件的变量值的问题。我们展示了使用增加循环性质的Transformer来学习以端到端的方式解决CSPs是可行的方法，相比于Graph神经网络、SATNet和一些神经符号模型等最先进的方法具有明显的优势。由于Transformer可以处理视觉输入的能力，提出的循环Transformer可以直接应用于视觉约束推理问题，并成功解决符号基础问题。我们还展示了如何利用离散约束的演绎知识来实现Transformer的归纳学习，从而实现CSPs的样本高效学习和半监督学习。

    Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that Transformer extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. With the ability of Transformer to handle visual input, the proposed Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems while successfully addressing the symbol grounding problem. We also show how to leverage deductive knowledge of discrete constraints in the Transformer's inductive learning to achieve sample-efficient learning and semi-supervised learning for CSPs.
    
[^94]: 选择好对手：如何指导程序化策略的合成

    Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies. (arXiv:2307.04893v1 [cs.LG])

    [http://arxiv.org/abs/2307.04893](http://arxiv.org/abs/2307.04893)

    这篇论文介绍了一种名为2L的算法，该算法能够提供引导合成程序化策略的参考策略，通过在实验中的表现和在MicroRTS锦标赛中的胜利，证明了2L算法相对于其他学习算法的优势。

    

    本论文介绍了一种名为Local Learner (2L)的算法，用于提供一组参考策略，以指导在两人零和博弈中搜索程序化策略。之前的学习算法，如迭代最佳响应算法(IBR)，虚构游戏算法(FP)和双正交算法(DO)，计算复杂度较高或会漏掉指导搜索算法的重要信息。2L主动选择一组参考策略以提高搜索信号。我们通过在三个游戏中引导局部搜索算法来实证我们的方法优势，其中包括MicroRTS，一个具有挑战性的实时战略游戏。结果表明，2L学习到的参考策略提供了比IBR，FP和DO更强的搜索信号。我们还模拟了一场MicroRTS锦标赛，其中使用2L合成器的表现超过了两个最新MicroRTS比赛的胜者，这些胜者均为人类编程员编写的程序化策略。

    This paper introduces Local Learner (2L), an algorithm for providing a set of reference strategies to guide the search for programmatic strategies in two-player zero-sum games. Previous learning algorithms, such as Iterated Best Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be computationally expensive or miss important information for guiding search algorithms. 2L actively selects a set of reference strategies to improve the search signal. We empirically demonstrate the advantages of our approach while guiding a local search algorithm for synthesizing strategies in three games, including MicroRTS, a challenging real-time strategy game. Results show that 2L learns reference strategies that provide a stronger search signal than IBR, FP, and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L outperformed the winners of the two latest MicroRTS competitions, which were programmatic strategies written by human programmers.
    
[^95]: 加速机器学习对称性的发现：推导出特殊李群G2，F4和E6

    Accelerated Discovery of Machine-Learned Symmetries: Deriving the Exceptional Lie Groups G2, F4 and E6. (arXiv:2307.04891v1 [hep-th])

    [http://arxiv.org/abs/2307.04891](http://arxiv.org/abs/2307.04891)

    本研究提出了两种改进算法，能够显著加快对称变换的发现速度，并成功地推导出了特殊李群G2，F4和E6的完整一组生成元。这种机器学习方法对于发现各种类型的对称性是通用的。

    

    近期的研究已经应用监督深度学习推导连续对称变换以保持数据标签，并获得相应的对称生成元代数。本文引入了两种改进的算法，显著加快了对称变换的发现速度。通过推导出酉群U(n)和特殊李群G2，F4和E6的完整一组生成元来演示这些新方法。第三个后处理算法将找到的生成元呈现为稀疏形式。我们对比了新算法与标准方法的性能改善。鉴于特殊李群的复杂性，我们的结果表明，这种机器学习方法用于发现对称性是完全通用的，可以应用于各种带标签的数据集。

    Recent work has applied supervised deep learning to derive continuous symmetry transformations that preserve the data labels and to obtain the corresponding algebras of symmetry generators. This letter introduces two improved algorithms that significantly speed up the discovery of these symmetry transformations. The new methods are demonstrated by deriving the complete set of generators for the unitary groups U(n) and the exceptional Lie groups $G_2$, $F_4$, and $E_6$. A third post-processing algorithm renders the found generators in sparse form. We benchmark the performance improvement of the new algorithms relative to the standard approach. Given the significant complexity of the exceptional Lie groups, our results demonstrate that this machine-learning method for discovering symmetries is completely general and can be applied to a wide variety of labeled datasets.
    
[^96]: 测量和缓解强化学习中的干扰

    Measuring and Mitigating Interference in Reinforcement Learning. (arXiv:2307.04887v1 [cs.LG])

    [http://arxiv.org/abs/2307.04887](http://arxiv.org/abs/2307.04887)

    本文提供了一种衡量强化学习中干扰的新方法，并且提出了一类在线感知算法来减轻干扰，这些算法在经典控制环境中提高了稳定性和性能。

    

    灾难性干扰在许多基于网络的学习系统中很常见，并且存在许多减轻干扰的建议。在克服干扰之前，我们必须更好地理解它。在这项工作中，我们为Fitted Q-Iteration和DQN等基于值的强化学习方法提供了干扰的定义和新型度量。我们系统地评估了我们的干扰度量，在各种网络架构上显示出它与控制性能的不稳定性相关。我们的新干扰度量使我们能够提出关于常用深度学习架构的新科学问题，并研究减轻干扰的学习算法。最后，我们概述了一类我们称为在线感知算法的算法，旨在减轻干扰，并且根据我们的度量显示它们减少了干扰，并在几个经典的控制环境中提高了稳定性和性能。

    Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. Before overcoming interference we must understand it better. In this work, we provide a definition and novel measure of interference for value-based reinforcement learning methods such as Fitted Q-Iteration and DQN. We systematically evaluate our measure of interference, showing that it correlates with instability in control performance, across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and study learning algorithms which mitigate interference. Lastly, we outline a class of algorithms which we call online-aware that are designed to mitigate interference, and show they do reduce interference according to our measure and that they improve stability and performance in several classic control environments.
    
[^97]: 洋葱宇宙算法：在弱监督学习中的应用

    Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])

    [http://arxiv.org/abs/2307.04870](http://arxiv.org/abs/2307.04870)

    洋葱宇宙算法是一种新颖的集成学习分类方法，可作为弱监督学习的标签模型。它在实现上简单、计算高效，适用于无完全标记数据的情况。经验证明，它在常见的基准数据集上表现出色。

    

    本文介绍了洋葱宇宙算法(OUA)，一种新颖的集成学习分类方法。特别地，我们展示了它作为弱监督学习标签模型的适用性。OUA在实现上简单，计算效率高，并且不依赖于数据或弱信号的任何假设。该模型非常适用于没有完全标记数据的情况。我们的方法基于对由弱信号所构成的空间的几何解释。经验证实，OUA在一般的弱信号集合下具有潜在的几何结构，并且在实践中表现良好。我们还通过实验证据展示，OUA在常见的基准数据集上相比现有的弱监督学习标签模型表现出色。

    We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
    
[^98]: Fed-CPrompt: 无重复学习的联邦持续学习的对比提示

    Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])

    [http://arxiv.org/abs/2307.04869](http://arxiv.org/abs/2307.04869)

    本文提出了一种名为Fed-CPrompt的方法，用于解决无重复学习的联邦持续学习中的遗忘问题。该方法通过异步提示学习和对比持续损失处理异步任务到达和异构数据分布，并在实验证明其在该领域取得了最先进的性能。

    

    联邦持续学习（FCL）从分布在客户端上的机密数据集中逐步学习任务。本文着重研究无重复学习的FCL，在学习新任务时存在因无法访问历史任务数据而导致严重遗忘的问题。为解决此问题，我们提出了基于提示学习技术的Fed-CPrompt，以一种高效的通信方式获得任务特定的提示。Fed-CPrompt引入了两个关键组件，异步提示学习和对比持续损失，以分别处理FCL中的异步任务到达和异构数据分布。大量实验证明了Fed-CPrompt在实现最先进的无重复学习FCL性能方面的有效性。

    Federated continual learning (FCL) learns incremental tasks over time from confidential datasets distributed across clients. This paper focuses on rehearsal-free FCL, which has severe forgetting issues when learning new tasks due to the lack of access to historical task data. To address this issue, we propose Fed-CPrompt based on prompt learning techniques to obtain task-specific prompts in a communication-efficient way. Fed-CPrompt introduces two key components, asynchronous prompt learning, and contrastive continual loss, to handle asynchronous task arrival and heterogeneous data distributions in FCL, respectively. Extensive experiments demonstrate the effectiveness of Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
    
[^99]: 使用对齐集合来解决实例相关的标签噪声

    Leveraging an Alignment Set in Tackling Instance-Dependent Label Noise. (arXiv:2307.04868v1 [cs.LG])

    [http://arxiv.org/abs/2307.04868](http://arxiv.org/abs/2307.04868)

    本论文提出了一种两阶段的方法来应对实例相关的标签噪声。该方法利用了对齐集合来学习，并在多个任务中相对于现有方法实现了一致的性能改进。

    

    噪声训练标签会影响模型性能。大多数解决标签噪声的方法都假设标签噪声与输入特征无关。然而，在实践中，标签噪声往往与特征或实例相关，因此是有偏见的（即某些实例更可能被错误标记）。例如，在临床护理中，女性患者相比男性患者更容易被误诊为心血管疾病。忽略这种相关性的方法可能会产生区分性能差的模型，并且在许多医疗保健环境中可能加剧健康不平等问题。鉴于这些限制，我们提出了一种两阶段的方法来学习实例相关的标签噪声。我们的方法利用了\anchor锚点，这是一个小的数据子集，我们知道其中的观测和真实标签。在多个任务中，我们的方法相对于现有方法实现了一致的性能改进。

    Noisy training labels can hurt model performance. Most approaches that aim to address label noise assume label noise is independent from the input features. In practice, however, label noise is often feature or \textit{instance-dependent}, and therefore biased (i.e., some instances are more likely to be mislabeled than others). E.g., in clinical care, female patients are more likely to be under-diagnosed for cardiovascular disease compared to male patients. Approaches that ignore this dependence can produce models with poor discriminative performance, and in many healthcare settings, can exacerbate issues around health disparities. In light of these limitations, we propose a two-stage approach to learn in the presence instance-dependent label noise. Our approach utilizes \textit{\anchor points}, a small subset of data for which we know the observed and ground truth labels. On several tasks, our approach leads to consistent improvements over the state-of-the-art in discriminative perfor
    
[^100]: 使用腰部佩戴的加速计在典型的行走和跑步速度范围内自动检测步态事件和行走距离

    Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])

    [http://arxiv.org/abs/2307.04866](http://arxiv.org/abs/2307.04866)

    该论文研究了使用腰部佩戴的加速计自动检测步态事件和行走距离的方法，通过分析市售智能手机加速计数据，实现了从广泛的步态速度范围中提取步态特征，可用于对Duchenne肌肉萎缩患儿和典型发育正常患者的评估。

    

    背景：估计步态（CFs）的时间空间临床特征，如步数和长度、步长、步频、步速和行走距离等，在使用可穿戴式加速计进行基于社区的移动性评估中是一个重要的组成部分。然而，由于设备复杂性和可用性、成本和分析方法学引起的挑战限制了此类工具的广泛应用。研究问题：能否使用市售智能手机的加速计数据来提取Duchenne肌肉萎缩（DMD）患儿和典型发育正常（TDs）患者在广泛步态速度范围内的步态CFs，并使用机器学习（ML）方法。方法：15名DMD患儿和15名TDs被要求在10MRW、25MRW、100MRW、6MWT和FW评估中以一系列步态速度进行监督性临床测试，同时佩戴手机基础加速计。

    Background: Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed and distance traveled is an important component of community-based mobility evaluation using wearable accelerometers. However, challenges arising from device complexity and availability, cost and analytical methodology have limited widespread application of such tools. Research Question: Can accelerometer data from commercially-available smartphones be used to extract gait CFs across a broad range of attainable gait velocities in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods Methods: Fifteen children with DMD and 15 TDs underwent supervised clinical testing across a range of gait speeds using 10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer at the wa
    
[^101]: 使用文本到图像扩散模型生成可关节化的3D头像

    Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models. (arXiv:2307.04859v1 [cs.CV])

    [http://arxiv.org/abs/2307.04859](http://arxiv.org/abs/2307.04859)

    本研究提出了一种文本引导的3D头像生成方法，通过直接在可关节化的3D模型上操作几何和纹理，并引入了新的优化过程，实现了对生成头像的精确控制。

    

    生成多样化的3D可关节化头像对于增强现实、电影制作和教育等众多应用至关重要。最近关于文本引导的3D物体生成的研究展示了很大的潜力来满足这些需求。这些方法直接利用预训练的2D文本到图像扩散模型生成3D多视角一致的通用物体辐射场。然而，由于缺乏几何和纹理先验知识，这些方法对生成的3D物体的控制能力有限，导致难以操作在特定领域内，比如人脸头像。在这项工作中，我们开发了一种新的方法来解决这个限制，实现文本引导的3D头像生成。我们的框架直接在一个可关节化的3D可变形模型（3DMM）的几何和纹理上操作，并引入了更新几何和纹理的新型优化过程，同时保持2D和3D面部特征的对齐。结果是一个3D头像。

    The ability to generate diverse 3D articulated head avatars is vital to a plethora of applications, including augmented reality, cinematography, and education. Recent work on text-guided 3D object generation has shown great promise in addressing these needs. These methods directly leverage pre-trained 2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance fields of generic objects. However, due to the lack of geometry and texture priors, these methods have limited control over the generated 3D objects, making it difficult to operate inside a specific domain, e.g., human heads. In this work, we develop a new approach to text-guided 3D head avatar generation to address this limitation. Our framework directly operates on the geometry and texture of an articulable 3D morphable model (3DMM) of a head, and introduces novel optimization procedures to update the geometry and texture while keeping the 2D and 3D facial features aligned. The result is a 3D head avatar tha
    
[^102]: SHAP@k：高效且可能近似正确（PAC）地识别Top-k特征

    SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features. (arXiv:2307.04850v1 [cs.LG])

    [http://arxiv.org/abs/2307.04850](http://arxiv.org/abs/2307.04850)

    本文提出了SHAP@k框架，旨在通过提高样本效率来解决Top-k特征识别问题，通过将问题转化为Explore-m问题并利用多臂赌博机的技术来实现。

    

    SHAP框架通过计算特征重要性提供了一种解释模型预测的方法。受金融应用的启发，我们引入了Top-k识别问题（TkIP），其目标是识别具有最高SHAP值的k个特征。虽然任何计算带有不确定性估计的SHAP值的方法（如KernelSHAP和SamplingSHAP）都可以轻松适应TkIP的解决，但这样做会导致样本效率低下。我们的工作目标是在解决TkIP的背景下提高现有方法的样本效率。我们的关键洞察是TkIP可以作为一个Explore-m问题进行建模，该问题与多臂赌博机（MAB）相关的问题已经得到了深入研究。这种联系使我们能够通过利用MAB文献中的两种技术来提高样本效率：（1）更好的停止条件（停止采样），识别PAC（可能近似正确）保证已经满足；（2）一种贪婪采样方案。

    The SHAP framework provides a principled method to explain the predictions of a model by computing feature importance. Motivated by applications in finance, we introduce the Top-k Identification Problem (TkIP), where the objective is to identify the k features with the highest SHAP values. While any method to compute SHAP values with uncertainty estimates (such as KernelSHAP and SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample inefficient. The goal of our work is to improve the sample efficiency of existing methods in the context of solving TkIP. Our key insight is that TkIP can be framed as an Explore-m problem--a well-studied problem related to multi-armed bandits (MAB). This connection enables us to improve sample efficiency by leveraging two techniques from the MAB literature: (1) a better stopping-condition (to stop sampling) that identifies when PAC (Probably Approximately Correct) guarantees have been met and (2) a greedy sampling scheme that judic
    
[^103]: SigOpt Mulch: 一种自动学习梯度提升树的智能系统

    SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees. (arXiv:2307.04849v1 [cs.LG])

    [http://arxiv.org/abs/2307.04849](http://arxiv.org/abs/2307.04849)

    SigOpt Mulch是一种智能系统，用于自动化调整梯度提升树模型的超参数。与其他现有系统不同，SigOpt Mulch是“模型感知型”的，能够针对GBTs进行更优化的性能调整，并且无需领域知识，帮助实现自动化实验。

    

    梯度提升树(GBTs)是研究人员、机器学习(ML)实践者和数据科学家普遍使用的模型，因为它们具有稳健的性能、可解释的行为和易于使用的特点。训练GBTs的一个关键挑战是调整超参数。在实践中，选择这些超参数通常是手动完成的。最近，ML社区提倡通过黑盒优化来调整超参数，并开发了最先进的系统来实现这一目标。然而，将这些系统应用于调整GBTs存在两个缺点。首先，这些系统不具备“模型感知性”，而是设计用于“通用”模型，这导致了优化性能的显著降低。其次，使用这些系统需要“领域知识”，比如超参数搜索空间的选择，这与黑盒优化旨在提供的自动实验相悖。在本文中，我们介绍了SigOpt Mulch

    Gradient boosted trees (GBTs) are ubiquitous models used by researchers, machine learning (ML) practitioners, and data scientists because of their robust performance, interpretable behavior, and ease-of-use. One critical challenge in training GBTs is the tuning of their hyperparameters. In practice, selecting these hyperparameters is often done manually. Recently, the ML community has advocated for tuning hyperparameters through black-box optimization and developed state-of-the-art systems to do so. However, applying such systems to tune GBTs suffers from two drawbacks. First, these systems are not \textit{model-aware}, rather they are designed to apply to a \textit{generic} model; this leaves significant optimization performance on the table. Second, using these systems requires \textit{domain knowledge} such as the choice of hyperparameter search space, which is an antithesis to the automatic experimentation that black-box optimization aims to provide. In this paper, we present SigOp
    
[^104]: 时间差分强化学习的动态

    Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])

    [http://arxiv.org/abs/2307.04841](http://arxiv.org/abs/2307.04841)

    我们使用统计物理学的概念，研究了时间差分学习在线性函数逼近器下的典型学习曲线。我们发现由于子采样可能的轨迹空间而产生的随机半梯度噪声会导致值误差出现显著的平台。

    

    强化学习在需要学习在反馈有限的环境中行动的多个应用中取得了成功。然而，尽管有这种经验上的成功，仍然没有对强化学习模型的参数和用于表示状态的特征如何相互作用控制学习动态的理论理解。在这项工作中，我们使用统计物理学的概念，研究线性函数逼近器下时间差分学习价值函数的典型学习曲线。我们的理论是在一个高斯等效假设下推导出来的，其中对随机轨迹的平均值被替换为时态相关的高斯特征平均值，并且我们在小规模马尔可夫决策过程上验证了我们的假设。我们发现，由于对可能的轨迹空间进行子采样而产生的随机半梯度噪声导致值误差出现显著的平台，这与传统的梯度下降不同。

    Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent 
    
[^105]: CREPE：使用CLIP的可学习提示提高视觉关系预测

    CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction. (arXiv:2307.04838v1 [cs.CV])

    [http://arxiv.org/abs/2307.04838](http://arxiv.org/abs/2307.04838)

    本文研究了使用CLIP模型提高视觉关系预测的可能性。通过在UVTransE框架中采用基于CLIP的表示方法，以及引入对比训练策略，我们提出了CREPE模型，简化了现有复杂的图形模型，取得了良好的效果。

    

    本文探讨了视觉语言模型（VLMs），特别是CLIP，在预测视觉目标之间的关系方面的潜力，其中涉及将图像的视觉特征解释为基于语言的关系。现有的最先进方法使用复杂的图形模型，利用语言线索和视觉特征来解决这一挑战。我们假设CLIP嵌入中的强语言先验可以简化这些图形模型，为更简单的方法铺平道路。我们采用了UVTransE关系预测框架，该框架通过场景中的主体、客体和并集框嵌入来学习关系作为一个平移嵌入。我们在UVTransE框架内系统地探索了基于CLIP的主体、客体和并集框表示的设计，并提出了CREPE（CLIP增强谓词估计）。CREPE利用所有三个边界框的基于文本的表示，并引入了一种新颖的对比训练策略，以自动学习视觉关系。

    In this paper, we explore the potential of Vision-Language Models (VLMs), specifically CLIP, in predicting visual object relationships, which involves interpreting visual features from images into language-based relations. Current state-of-the-art methods use complex graphical models that utilize language cues and visual features to address this challenge. We hypothesize that the strong language priors in CLIP embeddings can simplify these graphical models paving for a simpler approach. We adopt the UVTransE relation prediction framework, which learns the relation as a translational embedding with subject, object, and union box embeddings from a scene. We systematically explore the design of CLIP-based subject, object, and union-box representations within the UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate Estimation). CREPE utilizes text-based representations for all three bounding boxes and introduces a novel contrastive training strategy to automatically
    
[^106]: 关于检测群体测试中的某些有缺陷的项目

    On Detecting Some Defective Items in Group Testing. (arXiv:2307.04822v1 [cs.DS])

    [http://arxiv.org/abs/2307.04822](http://arxiv.org/abs/2307.04822)

    本文关注解决了群体测试中识别特定数量有缺陷项目的问题，并在自适应和非自适应设置下给出了相应的上下界。

    

    群体测试是一种旨在在总共n个元素中识别最多d个有缺陷的项目的方法。这是通过检查子集来确定是否至少存在一个有缺陷的项目来实现的。在我们的研究中，我们着重研究了识别最多$\ell \leq d$个有缺陷项目的问题。我们在自适应和非自适应设置下，在没有关于d的先验知识的情况下以及在有关于d的估计或至少一些非平凡的上界的情况下，开发了检测$\ell$个有缺陷项目所需的测试次数的上界和下界。当没有关于d的先验知识时，我们在随机非自适应设置下证明了一个下界：$\Omega(\frac{\ell \log^2n}{\log \ell +\log\log n})$的测试，并给出了相同设置的上界：$O(\ell \log^2 n)$。此外，我们证明任何非自适应确定性算法必须询问$\Theta(n)$次测试，这意味着存在一种根本限制。

    Group testing is an approach aimed at identifying up to $d$ defective items among a total of $n$ elements. This is accomplished by examining subsets to determine if at least one defective item is present. In our study, we focus on the problem of identifying a subset of $\ell\leq d$ defective items. We develop upper and lower bounds on the number of tests required to detect $\ell$ defective items in both the adaptive and non-adaptive settings while considering scenarios where no prior knowledge of $d$ is available, and situations where an estimate of $d$ or at least some non-trivial upper bound on $d$ is available.  When no prior knowledge on $d$ is available, we prove a lower bound of $ \Omega(\frac{\ell \log^2n}{\log \ell +\log\log n})$ tests in the randomized non-adaptive settings and an upper bound of $O(\ell \log^2 n)$ for the same settings. Furthermore, we demonstrate that any non-adaptive deterministic algorithm must ask $\Theta(n)$ tests, signifying a fundamental limitation in t
    
[^107]: 用于映射无间断地表温度的物理约束机器学习方法

    A physics-constrained machine learning method for mapping gapless land surface temperature. (arXiv:2307.04817v1 [physics.ao-ph])

    [http://arxiv.org/abs/2307.04817](http://arxiv.org/abs/2307.04817)

    本文提出了一种物理约束的机器学习模型，将机制与数据驱动模型的优点结合起来，以生成具有物理意义和高精度的无间断地表温度估计。模型利用了轻量梯度提升机作为纯机器学习模型，并通过物理约束进一步提升了模型的可解释性和外推能力。模型的输入变量包括遥感数据、关键Community Land Model（CLM）强制数据以及CLM模拟数据。

    

    在地球系统研究中，更准确、时空一致且具有物理一致性的地表温度估计一直是一个主要关注点。发展物理驱动机制模型和数据驱动的机器学习（ML）模型是无间断地表温度估计的两种主要范式，它们各自具有优缺点。本文提出了一个物理约束的ML模型，将机制模型和ML模型的优势结合起来，以生成具有物理意义和高精度的无间断地表温度。混合模型将ML作为主要框架，其中将输入变量的物理约束融入模型，以增强模型的可解释性和外推能力。具体而言，轻量梯度提升机（LGBM）模型，只使用遥感数据作为输入，作为纯ML模型。通过进一步整合关键Community Land Model（CLM）强制数据（原因）和CLM模拟数据（效果），物理约束（PCs）被耦合进来。

    More accurate, spatio-temporally, and physically consistent LST estimation has been a main interest in Earth system research. Developing physics-driven mechanism models and data-driven machine learning (ML) models are two major paradigms for gapless LST estimation, which have their respective advantages and disadvantages. In this paper, a physics-constrained ML model, which combines the strengths in the mechanism model and ML model, is proposed to generate gapless LST with physical meanings and high accuracy. The hybrid model employs ML as the primary architecture, under which the input variable physical constraints are incorporated to enhance the interpretability and extrapolation ability of the model. Specifically, the light gradient-boosting machine (LGBM) model, which uses only remote sensing data as input, serves as the pure ML model. Physical constraints (PCs) are coupled by further incorporating key Community Land Model (CLM) forcing data (cause) and CLM simulation data (effect)
    
[^108]: 协同分数蒸馏用于一致的视觉合成

    Collaborative Score Distillation for Consistent Visual Synthesis. (arXiv:2307.04787v1 [cs.CV])

    [http://arxiv.org/abs/2307.04787](http://arxiv.org/abs/2307.04787)

    本文介绍了一种名为协同分数蒸馏（Collaborative Score Distillation，CSD）的新方法，它基于斯坦变分梯度下降（SVGD），通过将多个样本作为“粒子”并结合它们的分数函数来实现对一组图像的生成先验进行同步蒸馏，从而在多个样本之间实现了一致的视觉合成。CSD在各种任务中展示了其有效性，包括全景图像、视频和3D场景的视觉编辑。

    

    大规模文本到图像扩散模型的生成先验使得在多样化的视觉模态上有了广泛的新生成和编辑应用。然而，当将这些先验适应于复杂的视觉模态，通常表示为多个图像（例如视频）时，实现一组图像的一致性是具有挑战性的。在本文中，我们提出了一种新颖的方法，协同分数蒸馏（CSD）。CSD基于斯坦变分梯度下降（SVGD）。具体而言，我们建议将多个样本视为SVGD更新中的“粒子”，并结合它们的分数函数以同时蒸馏一组图像上的生成先验。因此，CSD促进了2D图像之间信息的无缝集成，从而实现了多个样本的一致视觉合成。我们展示了CSD在多种任务中的有效性，包括全景图像、视频和3D场景的视觉编辑。我们的结果突出了CSD的能力。

    Generative priors of large-scale text-to-image diffusion models enable a wide range of new generation and editing applications on diverse visual modalities. However, when adapting these priors to complex visual modalities, often represented as multiple images (e.g., video), achieving consistency across a set of images is challenging. In this paper, we address this challenge with a novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein Variational Gradient Descent (SVGD). Specifically, we propose to consider multiple samples as "particles" in the SVGD update and combine their score functions to distill generative priors over a set of images synchronously. Thus, CSD facilitates seamless integration of information across 2D images, leading to a consistent visual synthesis across multiple samples. We show the effectiveness of CSD in a variety of tasks, encompassing the visual editing of panorama images, videos, and 3D scenes. Our results underline the competency of
    
[^109]: 点云与基于图像的模型在量热器快速模拟中的比较

    Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation. (arXiv:2307.04780v1 [cs.LG])

    [http://arxiv.org/abs/2307.04780](http://arxiv.org/abs/2307.04780)

    本文比较了使用点云和基于图像的模型来进行量热器快速模拟，发现点云更自然地表示了量热器淋浴，处理稀疏数据集更优秀，并且可以使用更紧凑的模型和数据文件进行实现。

    

    基于得分的生成模型是一种新的生成模型类别，已被证明能准确生成高维度的量热器数据集。生成模型的最新进展使用图像与3D体素表示和建模复杂的量热器淋浴。然而，点云可能是量热器淋浴更自然的表示方式，尤其是在具有高粒度的量热器中。点云保留了原始模拟的所有信息，更自然地处理稀疏数据集，并且可以使用更紧凑的模型和数据文件进行实现。在这项工作中，用同一组量热器模拟数据训练了两种最先进的基于得分的模型，并进行了直接比较。

    Score based generative models are a new class of generative models that have been shown to accurately generate high dimensional calorimeter datasets. Recent advances in generative models have used images with 3D voxels to represent and model complex calorimeter showers. Point clouds, however, are likely a more natural representation of calorimeter showers, particularly in calorimeters with high granularity. Point clouds preserve all of the information of the original simulation, more naturally deal with sparse datasets, and can be implemented with more compact models and data files. In this work, two state-of-the-art score based models are trained on the same set of calorimeter simulation and directly compared.
    
[^110]: 基于统计分析和应用程序在金融公司中制定战略计划的一个实际案例研究

    Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case. (arXiv:2307.04778v1 [cs.LG])

    [http://arxiv.org/abs/2307.04778](http://arxiv.org/abs/2307.04778)

    本论文介绍了一个基于统计分析的战略计划，通过研究金融公司LendingClub的实际案例，探索引入大数据平台和先进特征选择能力的可能性，以增加收入并降低风险。

    

    商业统计在企业级别实施数据驱动的战略计划中起着至关重要的作用，以运用各种分析手段，这样的计划的结果可以帮助企业改进决策过程或降低组织风险。本文介绍了一个基于统计分析的战略计划，针对一家金融公司LendingClub，计划包括探索引入大数据平台和先进的特征选择能力的可能性。这个计划的主要目标是增加公司的收入，同时减少向无法归还贷款的借款人授予贷款的风险。通过对公司担忧进行了不同假设的研究，结果显示贷款金额对无法归还贷款的借款人人数有深远影响。提出的战略计划还包括引入机器学习等先进分析方法的应用。

    Business statistics play a crucial role in implementing a data-driven strategic plan at the enterprise level to employ various analytics where the outcomes of such a plan enable an enterprise to enhance the decision-making process or to mitigate risks to the organization. In this work, a strategic plan informed by the statistical analysis is introduced for a financial company called LendingClub, where the plan is comprised of exploring the possibility of onboarding a big data platform along with advanced feature selection capacities. The main objectives of such a plan are to increase the company's revenue while reducing the risks of granting loans to borrowers who cannot return their loans. In this study, different hypotheses formulated to address the company's concerns are studied, where the results reveal that the amount of loans profoundly impacts the number of borrowers charging off their loans. Also, the proposed strategic plan includes onboarding advanced analytics such as machin
    
[^111]: MentalHealthAI: 利用个人健康设备数据优化精神病学治疗

    MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment. (arXiv:2307.04777v1 [cs.LG])

    [http://arxiv.org/abs/2307.04777](http://arxiv.org/abs/2307.04777)

    本论文提出了一个个性化的心理健康跟踪与情绪预测系统，利用个人健康设备收集的生理数据，通过分散学习机制实现精神病学治疗和管理的有效追踪，并以隐私意识和有责任感的方式提供精神科医生对患者心理健康状况的进一步了解。

    

    在现代医疗中，心理健康障碍仍然是一个重要挑战，诊断和治疗往往依赖于患者的主观描述和过去的病史。为了解决这个问题，我们提出了一个个性化的心理健康跟踪与情绪预测系统，利用通过个人健康设备收集的患者生理数据。我们的系统利用了分散学习机制，结合了转移学习和联邦机器学习的概念，使用智能合约，使数据保留在用户设备上，并以隐私意识和有责任感的方式有效跟踪精神病学治疗和管理中的心理健康状况。我们使用了一个流行的心理健康数据集对模型进行评估，结果显示了良好的效果。通过利用连接的健康系统和机器学习模型，我们的方法为提供精神科医生对患者的心理健康状况外部提供了新的解决方案。

    Mental health disorders remain a significant challenge in modern healthcare, with diagnosis and treatment often relying on subjective patient descriptions and past medical history. To address this issue, we propose a personalized mental health tracking and mood prediction system that utilizes patient physiological data collected through personal health devices. Our system leverages a decentralized learning mechanism that combines transfer and federated machine learning concepts using smart contracts, allowing data to remain on users' devices and enabling effective tracking of mental health conditions for psychiatric treatment and management in a privacy-aware and accountable manner. We evaluate our model using a popular mental health dataset that demonstrates promising results. By utilizing connected health systems and machine learning models, our approach offers a novel solution to the challenge of providing psychiatrists with further insight into their patients' mental health outside
    
[^112]: 基于知识图谱和闭式连续时间液体神经网络的患者护理数字双生技术

    Digital Twins for Patient Care via Knowledge Graphs and Closed-Form Continuous-Time Liquid Neural Networks. (arXiv:2307.04772v1 [cs.LG])

    [http://arxiv.org/abs/2307.04772](http://arxiv.org/abs/2307.04772)

    本文提出了一个新的框架，通过将患者健康数据构建为知识图谱，并使用闭式连续时间液体神经网络进行实时分析，以解决复杂疾病建模和计算复杂性的问题。这一框架有望推动数字双生技术在患者护理领域的应用。

    

    数字双生技术被预计将改变医疗保健，实现个性化药物和支持、早期诊断、模拟治疗结果和优化手术计划。数字双生技术在制造业、供应链物流和民用基础设施等行业已经获得了广泛关注，但在患者护理领域尚未得到应用。建模复杂疾病与多模态患者数据以及计算分析的复杂性是制约生物医学领域数字双生技术应用的重要障碍。然而，通过不同方法来处理这些模型可能能够解决这些障碍。本文提出了一个新框架，以解决由计算成本和建模复杂性造成的临床双生模型的障碍。我们提出将患者健康数据构建为知识图谱，并使用闭式连续时间液体神经网络进行实时分析。通过综合多模态患者数据并利用这一框架，可以实现患者护理的数字双生模型的应用。

    Digital twin technology has is anticipated to transform healthcare, enabling personalized medicines and support, earlier diagnoses, simulated treatment outcomes, and optimized surgical plans. Digital twins are readily gaining traction in industries like manufacturing, supply chain logistics, and civil infrastructure. Not in patient care, however. The challenge of modeling complex diseases with multimodal patient data and the computational complexities of analyzing it have stifled digital twin adoption in the biomedical vertical. Yet, these major obstacles can potentially be handled by approaching these models in a different way. This paper proposes a novel framework for addressing the barriers to clinical twin modeling created by computational costs and modeling complexities. We propose structuring patient health data as a knowledge graph and using closed-form continuous-time liquid neural networks, for real-time analytics. By synthesizing multimodal patient data and leveraging the fle
    
[^113]: 针对长期COVID病患者的时空关注机制预测结果的研究

    Predicting Outcomes in Long COVID Patients with Spatiotemporal Attention. (arXiv:2307.04770v1 [cs.LG])

    [http://arxiv.org/abs/2307.04770](http://arxiv.org/abs/2307.04770)

    本研究提出了一种时空关注机制来预测长期COVID患者的结果，解决了其纵向数据预测的困难性，并通过与其他方法进行比较，证明了该方法的有效性。

    

    长期COVID是指COVID-19后出现的长期后遗症的一般术语。长期COVID患者可能经历持久的症状，包括疲劳、头痛、呼吸困难和嗅觉丧失等。从长期COVID患者的纵向数据中预测其结果是困难的，因为这些患者呈现出异质的表型。在本研究中，我们提出了一种时空关注机制，从时间维度和特征空间共同权衡特征的重要性。考虑到医学检查在相邻时间点可能有可互换的顺序，我们使用了本地LSTM限制短期依赖性的学习，并使用联合时空关注机制来学习长期依赖性。我们还将所提出的方法与几种最先进的方法和一种临床实践方法进行了比较。我们在一个研究中对这些方法进行了评估。

    Long COVID is a general term of post-acute sequelae of COVID-19. Patients with long COVID can endure long-lasting symptoms including fatigue, headache, dyspnea and anosmia, etc. Identifying the cohorts with severe long-term complications in COVID-19 could benefit the treatment planning and resource arrangement. However, due to the heterogeneous phenotype presented in long COVID patients, it is difficult to predict their outcomes from their longitudinal data. In this study, we proposed a spatiotemporal attention mechanism to weigh feature importance jointly from the temporal dimension and feature space. Considering that medical examinations can have interchangeable orders in adjacent time points, we restricted the learning of short-term dependency with a Local-LSTM and the learning of long-term dependency with the joint spatiotemporal attention. We also compared the proposed method with several state-of-the-art methods and a method in clinical practice. The methods are evaluated on a ha
    
[^114]: 一阶方法在具有通用预测器的统计学习中的泛化误差分析

    Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles. (arXiv:2307.04679v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04679](http://arxiv.org/abs/2307.04679)

    本文提出了一种新的框架来分析使用一阶优化算法进行统计学习时的泛化误差，该框架适用于多个学习问题，并且可以推导出紧密匹配的上界和下界。这些结果适用于光滑、强凸和满足Polyak-Lojasiewicz假设的优化问题。

    

    本文提供了一个新的框架，用于分析在统计学习中使用一阶优化算法时，当梯度只能通过预测器给出的部分观测来访问时的泛化误差。我们的分析依赖于梯度对数据样本的规则性，并且可以推导出多个学习问题的泛化误差的紧密匹配的上界和下界，包括监督学习、迁移学习、鲁棒学习、分布式学习和使用梯度量化的通信效率学习。这些结果适用于光滑且强凸的优化问题，以及满足Polyak-Lojasiewicz假设的光滑非凸优化问题。特别地，我们的上界和下界依赖于一个扩展了条件标准差概念的新量，它衡量了通过访问预测器可以近似梯度的程度。

    In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle. Our analysis relies on the regularity of the gradient w.r.t. the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization. These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle. As a consequ
    
[^115]: 自扩展神经网络

    Self Expanding Neural Networks. (arXiv:2307.04526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04526](http://arxiv.org/abs/2307.04526)

    这项研究提出了一种自扩展神经网络的方法，通过自然梯度来自动增加神经网络的宽度和深度，以在训练损失降低的情况下提高性能，并在分类和回归问题中展示了其优势。

    

    神经网络的训练结果严重依赖于所选择的架构；即使只是对网络大小做微小修改，通常也需要重新开始训练过程。相比之下，我们以一个小的架构开始训练，只在问题需要时扩展其容量，并避免干扰先前的优化过程。因此，我们引入了一种基于自然梯度的方法，当这样做可能大幅降低假设收敛训练损失时，直观地扩展了神经网络的宽度和深度。我们证明了神经元添加的“速率”上界，并且给出了计算廉价的扩展评分的下界。我们在分类和回归问题中展示了自扩展神经网络的优势，包括那些合适的架构大小在先验上相当不确定的问题。

    The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the "rate" at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.
    
[^116]: 基于CT的膝关节骨性骨质分析在膝关节骨关节炎中的应用（通过MR引导的蒸馏学习）。

    CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning. (arXiv:2307.04390v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.04390](http://arxiv.org/abs/2307.04390)

    本研究开发了一种名为SRRD的新方法，利用易于获得的CT图像进行膝关节骨性骨质分析，并通过配对的MR图像增强了模型的分析能力。该方法有望在膝关节骨关节炎的临床应用中具有潜在价值。

    

    背景：基于MR的骨性骨质有效预测膝关节骨关节炎。然而，其临床应用受到MR成本和时间的限制。目的：我们旨在开发一种名为SRRD的基于蒸馏学习的新方法，使用易于获得的CT图像进行膝关节骨性骨质分析，该方法在训练过程中利用配对的MR图像增强CT图像分析模型。材料和方法：从2020年10月到2021年5月收集了CT和MR双模态的膝关节图像。首先，我们开发了基于GAN的生成模型，将MR图像转换为CT图像，该模型用于建立两种模态之间的解剖对应关系。然后，我们通过回归从相应的CT图像块中获取了大量骨性骨质区域的MR图像块以及其梁结构参数（BV / TV，Tb. Th，Tb. Sp，Tb. N）。蒸馏学习技术用于训练回归模型并传输MR结构。

    Background: MR-based subchondral bone effectively predicts knee osteoarthritis. However, its clinical application is limited by the cost and time of MR. Purpose: We aim to develop a novel distillation-learning-based method named SRRD for subchondral bone microstructural analysis using easily-acquired CT images, which leverages paired MR images to enhance the CT-based analysis model during training. Materials and Methods: Knee joint images of both CT and MR modalities were collected from October 2020 to May 2021. Firstly, we developed a GAN-based generative model to transform MR images into CT images, which was used to establish the anatomical correspondence between the two modalities. Next, we obtained numerous patches of subchondral bone regions of MR images, together with their trabecular parameters (BV / TV, Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression. The distillation-learning technique was used to train the regression model and transfer MR structu
    
[^117]: DEFT:利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化

    DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v1 [cs.LG])

    [http://arxiv.org/abs/2307.03500](http://arxiv.org/abs/2307.03500)

    DEFT是一种利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化的方法，可以减少分布式深度学习中的通信流量，并在计算成本和梯度累积方面具有优势。

    

    梯度稀疏化是减少分布式深度学习中过多通信流量的广泛应用解决方案。然而，大多数现有的梯度稀疏化方法由于梯度选择的计算成本相当大和梯度累积增加的通信流量，其可扩展性相对较差。为了解决这些挑战，我们提出了一种新颖的梯度稀疏化方案DEFT，将梯度选择任务分解为子任务并分配给工作节点。 DEFT与现有的稀疏化方法不同，每个工作节点仅从所有梯度中选择梯度。因此，随着工作节点数量的增加，计算成本可以降低。此外，DEFT允许工作节点在非交叉的分区中选择梯度，因此即使工作节点数量增加，通信流量也可以根据用户要求进行维持。

    Gradient sparsification is a widely adopted solution for reducing the excessive communication traffic in distributed deep learning. However, most existing gradient sparsifiers have relatively poor scalability because of considerable computational cost of gradient selection and/or increased communication traffic owing to gradient build-up. To address these challenges, we propose a novel gradient sparsification scheme, DEFT, that partitions the gradient selection task into sub tasks and distributes them to workers. DEFT differs from existing sparsifiers, wherein every worker selects gradients among all gradients. Consequently, the computational cost can be reduced as the number of workers increases. Moreover, gradient build-up can be eliminated because DEFT allows workers to select gradients in partitions that are non-intersecting (between workers). Therefore, even if the number of workers increases, the communication traffic can be maintained as per user requirement.  To avoid the loss 
    
[^118]: QI2 -- 一个用于数据质量保证的交互工具

    QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v1 [cs.CY])

    [http://arxiv.org/abs/2307.03419](http://arxiv.org/abs/2307.03419)

    本文介绍了一种用于数据质量保证的交互工具QI2，该工具支持对多个数据质量方面的验证和定量数据质量要求的验证。通过在MNIST数据集上进行演示，展示了该方法的应用和优势。

    

    高数据质量的重要性随着机器学习系统和大数据的增长影响和分布而增加。此外，欧洲委员会计划的AI法案为数据质量定义了具有挑战性的法律要求，特别是对于市场推出与安全相关的机器学习系统。本文介绍了一种支持多个数据质量方面的数据质量保证过程的新方法。这种方法可以验证定量数据质量要求。通过小例子数据集介绍和解释了该概念和优势。如何应用该方法在基于手写数字的知名MNIST数据集上进行了演示。

    The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well known MNIST data set based an handwritten digits.
    
[^119]: 当公平分类遇到嘈杂的保护属性

    When Fair Classification Meets Noisy Protected Attributes. (arXiv:2307.03306v1 [cs.LG])

    [http://arxiv.org/abs/2307.03306](http://arxiv.org/abs/2307.03306)

    这项研究是对公平分类算法进行的一次首次的头对头比较，研究了依赖属性、容忍噪声和盲目属性的算法在预测性和公平性方面的表现，结果显示盲目属性和容忍噪声的公平分类器具有潜力。

    

    算法公平性的实施面临着许多实际挑战，其中之一就是数据集中受保护属性的可用性或可靠性。在现实世界的环境中，实际和法律上的障碍可能会阻止收集和使用人口统计数据，使得确保算法公平性变得困难。尽管最初的公平算法没有考虑这些限制，但最近的提议旨在通过将受保护属性的嘈杂性纳入考虑或根本不使用受保护属性来实现分类的算法公平性。据我们所知，这是首次对比基于属性、容忍噪声和盲目属性的公平分类算法在预测性和公平性这两个方面进行头对头研究。我们通过对四个真实数据集和合成扰动的案例研究评估了这些算法。我们的研究表明，盲目属性和容忍噪声的公平分类器可能会在预测性和公平性的双重轴上有潜力。

    The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all.  To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-blind algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-blind and noise-tolerant fair classifiers can potentia
    
[^120]: FLuID: 使用不变性丢失减轻联邦学习中的阻塞问题

    FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout. (arXiv:2307.02623v1 [cs.LG])

    [http://arxiv.org/abs/2307.02623](http://arxiv.org/abs/2307.02623)

    FLuID提出了一种使用不变性丢失的方法来减轻联邦学习中性能较低设备导致的训练时间问题，并开发了一个自适应训练框架。通过动态平衡训练负载，FLuID能有效地减轻阻塞设备的工作负载，同时不影响模型质量。

    

    联邦学习（FL）允许机器学习模型在个体移动设备上进行本地训练，并通过共享服务器同步模型更新。这种方法保护用户隐私，但也由于不同设备的性能差异而产生了一个异构的训练环境。因此，在FL中，性能较低的阻塞设备经常决定整体训练时间。在这项工作中，我们旨在通过系统动态平衡训练负载来减轻由于阻塞器产生的性能瓶颈。我们引入了不变性丢失，一种基于权重更新阈值提取子模型的方法，从而最小化对准确性的潜在影响。在此丢失技术的基础上，我们开发了一种自适应训练框架FLuID。FLuID提供了一种轻量级的子模型提取方法来调节计算强度，从而减少阻塞设备的负载而不影响模型质量。

    Federated Learning (FL) allows machine learning models to train locally on individual mobile devices, synchronizing model updates via a shared server. This approach safeguards user privacy; however, it also generates a heterogeneous training environment due to the varying performance capabilities across devices. As a result, straggler devices with lower performance often dictate the overall training time in FL. In this work, we aim to alleviate this performance bottleneck due to stragglers by dynamically balancing the training load across the system. We introduce Invariant Dropout, a method that extracts a sub-model based on the weight update threshold, thereby minimizing potential impacts on accuracy. Building on this dropout technique, we develop an adaptive training framework, Federated Learning using Invariant Dropout (FLuID). FLuID offers a lightweight sub-model extraction to regulate computational intensity, thereby reducing the load on straggler devices without affecting model q
    
[^121]: 应用数据工程方法解决微生物组数据在医学决策中的挑战

    Application of data engineering approaches to address challenges in microbiome data for optimal medical decision-making. (arXiv:2307.00033v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.00033](http://arxiv.org/abs/2307.00033)

    本研究应用数据工程方法解决微生物组数据中的类别不平衡和高维度问题，针对囊性纤维化婴儿的数据集实施了四种机器学习分类器，并提高了医学决策的准确性。

    

    通过它们与多个器官的相互作用，人体肠道微生物群落已被证明对机体的众多生理功能起着贡献，并且还与一系列病理条件有关。过去几十年的大量研究工作已经提供了有关肠道微生物群落的相对分类分布的有价值的信息，可以实现个体化医学。然而，微生物组数据存在类别不平衡和高维度问题，这些问题必须加以解决。在本研究中，我们实施了数据工程算法来解决与微生物组数据相关的上述问题。我们在以囊性纤维化婴儿的先前发表的数据集上实现了四种标准机器学习分类器（逻辑回归 (LR)、支持向量机 (SVM)、随机森林 (RF) 和极限梯度提升 (XGB) 决策树），并比较了不同分类器的性能。本研究结果表明，数据工程方法能有效解决微生物组数据中的类别不平衡和高维度问题，从而提高医学决策的准确性。

    The human gut microbiota is known to contribute to numerous physiological functions of the body through their interplay with multiple organs and also implicated in a myriad of pathological conditions. Prolific research work in the past few decades have yielded valuable information regarding the relative taxonomic distribution of the gut microbiota that could enable personalized medicine. Unfortunately, the microbiome data suffers from class imbalance and high dimensionality issues that must be addressed. In this study, we have implemented data engineering algorithms to address the above-mentioned issues inherent to microbiome data. Four standard machine learning classifiers (logistic regression (LR), support vector machines (SVM), random forests (RF), and extreme gradient boosting (XGB) decision trees) were implemented on a previously published dataset of infants with cystic fibrosis exhibiting normal vs abnormal growth patterns. The issue of class imbalance and high dimensionality of 
    
[^122]: 定义数据科学：一种新的研究范式

    Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])

    [http://arxiv.org/abs/2306.16177](http://arxiv.org/abs/2306.16177)

    数据科学是一种新的研究范式，具有潜力和应用广泛性，在40多个学科、数百个研究领域和成千上万个应用中出现。然而，由于其起步阶段，目前存在许多定义的冗余和不一致性的问题。

    

    数据科学不是一门科学，而是一种研究范式。它的力量、范围和规模将超越科学，成为促使知识发现并改变世界的重要手段。我们尚未理解和定义它，这对于实现其潜力和管理其风险至关重要。现代数据科学处于起步阶段。自1962年以来缓慢发展，并且自2000年以来发展迅速，它是一种根本性的新的研究领域，是21世纪最活跃、最强大和发展最快的创新之一。由于其价值、力量和适用性，它正在40多个学科、数百个研究领域和成千上万个应用中出现。数以百万计的数据科学出版物中包含了无数关于数据科学和数据科学问题解决的定义。由于其起步阶段，许多定义是独立的、应用特定的、相互不完整的、冗余的或不一致的，因此数据科学也是如此。本研究通过提出解决数据科学多重定义挑战的方法来解决这个问题。

    Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
    
[^123]: 通过多教师对抗蒸馏减轻准确性与鲁棒性之间的权衡

    Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation. (arXiv:2306.16170v1 [cs.LG])

    [http://arxiv.org/abs/2306.16170](http://arxiv.org/abs/2306.16170)

    本文介绍了一种名为多教师对抗鲁棒性蒸馏的方法，它通过使用强大的干净样本教师和鲁棒性教师来改进深度神经网络的对抗训练过程，以减轻准确性和鲁棒性之间的权衡。

    

    对抗训练是一种改善深度神经网络对抗攻击鲁棒性的实用方法。虽然有效提高了鲁棒性，但对干净样本的性能却有所下降，这意味着准确性和鲁棒性之间存在一种权衡。最近的一些研究尝试在对抗训练中使用知识蒸馏方法，取得了提高鲁棒性的竞争性性能，但并没有显著改善对干净样本的准确性。为了减轻准确性和鲁棒性之间的权衡，本文引入了多教师对抗鲁棒性蒸馏（MTARD），通过应用强大的干净样本教师和强大的鲁棒样本教师来指导模型的对抗训练过程。在优化过程中，为了确保不同教师显示相似的知识水平，我们设计了基于熵的平衡算法进行调整。

    Adversarial training is a practical approach for improving the robustness of deep neural networks against adversarial attacks. Although bringing reliable robustness, the performance toward clean examples is negatively affected after adversarial training, which means a trade-off exists between accuracy and robustness. Recently, some studies have tried to use knowledge distillation methods in adversarial training, achieving competitive performance in improving the robustness but the accuracy for clean samples is still limited. In this paper, to mitigate the accuracy-robustness trade-off, we introduce the Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's adversarial training process by applying a strong clean teacher and a strong robust teacher to handle the clean examples and adversarial examples, respectively. During the optimization process, to ensure that different teachers show similar knowledge scales, we design the Entropy-Based Balance algorithm to adj
    
[^124]: BayesFlow: 使用神经网络的摊还贝叶斯工作流

    BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v1 [cs.LG])

    [http://arxiv.org/abs/2306.16015](http://arxiv.org/abs/2306.16015)

    BayesFlow是一个Python库，提供了使用神经网络进行摊还贝叶斯推断的功能，用户可以在模型仿真上训练定制的神经网络，并将其用于任何后续应用。这种摊还贝叶斯推断能够快速准确地进行推断，并实现了对不可计算后验分布的近似。

    

    现代贝叶斯推断涉及一系列计算技术，用于估计、验证和从概率模型中得出结论，作为数据分析中有原则的工作流的一部分。贝叶斯工作流中的典型问题包括近似不可计算后验分布以适应不同的模型类型，以及通过复杂性和预测性能比较同一过程的竞争模型。本文介绍了Python库BayesFlow，用于基于仿真训练已建立的神经网络架构，用于摊还数据压缩和推断。在BayesFlow中实现的摊还贝叶斯推断使用户能够在模型仿真上训练定制的神经网络，并将这些网络重用于模型的任何后续应用。由于训练好的网络可以几乎即时地执行推断，因此前期的神经网络训练很快就能够摊还。

    Modern Bayesian inference involves a mixture of computational techniques for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows for data analysis. Typical problems in Bayesian workflows are the approximation of intractable posterior distributions for diverse model types and the comparison of competing models of the same process in terms of their complexity and predictive performance. This manuscript introduces the Python library BayesFlow for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.
    
[^125]: 物理约束的随机森林用于湍流模型不确定性估计

    Physics-constrained Random Forests for Turbulence Model Uncertainty Estimation. (arXiv:2306.13370v1 [cs.LG])

    [http://arxiv.org/abs/2306.13370](http://arxiv.org/abs/2306.13370)

    本文提出了一种物理约束的机器学习方法，用于考虑湍流模型的认识不确定性，同时在准确数据稀缺时能够实现前置的预测置信度估计。

    

    在实现工业设计的虚拟认证过程中，对于模拟驱动过程中不确定性的量化是至关重要的。本文讨论了一种物理约束方法来考虑湍流模型的认识不确定性。为了消除用户输入，我们结合了数据驱动的机器学习策略。此外，我们的研究重点是在准确数据稀缺时开发先验估计预测置信度。

    To achieve virtual certification for industrial design, quantifying the uncertainties in simulation-driven processes is crucial. We discuss a physics-constrained approach to account for epistemic uncertainty of turbulence models. In order to eliminate user input, we incorporate a data-driven machine learning strategy. In addition to it, our study focuses on developing an a priori estimation of prediction confidence when accurate data is scarce.
    
[^126]: TrustGuard: 基于GNN的动态支持鲁棒且可解释的信任评估

    TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])

    [http://arxiv.org/abs/2306.13339](http://arxiv.org/abs/2306.13339)

    TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。

    

    信任评估评估实体之间的信任关系并促进决策。机器学习由于其学习能力而表现出巨大的潜力，因此对信任评估具有重要意义。近年来，作为一种新的机器学习范 paradigm，图神经网络（GNN）在处理图形数据方面表现出优越性。这激发了研究人员探索将其用于信任评估，因为实体之间的信任关系可以建模为图形。但是，使用GNN的当前信任评估方法未能完全满足信任的动态性，忽略了攻击对信任评估的不利影响，并且无法提供令人信服的评估结果解释。为解决这些问题，在本文中，我们提出了TrustGuard ：一种支持信任动态性、抗击鲁棒且通过可视化提供解释的精确信任评估模型。具体而言，TrustGuard 设计了一个由动态感知节点嵌入层、图卷积层、注意机制层和信任预测层组成的分层架构。为了评估提出的模型的有效性，我们对真实数据集进行了实验，并将TrustGuard与其他最先进的方法进行了比较。实验结果表明，TrustGuard 在准确性、鲁棒性和可解释性方面均优于其他方法。

    Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
    
[^127]: 处理自然视觉场景神经响应的时间条件脉冲潜变量模型

    Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.12045](http://arxiv.org/abs/2306.12045)

    本研究提出 TeCoS-LVM 模型，使用脉冲神经元以模拟自然视觉刺激的神经响应。该模型能够自适应地探索和利用刺激序列中的时间依赖关系，避免丢失脉冲列中的信息。

    

    发展神经响应的计算模型对于理解感知处理和神经计算至关重要。目前最先进的神经网络方法使用时间过滤器来处理时间依赖性，导致处理流程不现实且不灵活。同时，这些方法针对试验平均发放率，未能捕捉到脉冲列中的重要特征。本研究提出时间条件脉冲潜变量模型（TeCoS-LVM）来模拟自然视觉刺激的神经响应。我们使用脉冲神经元产生直接匹配记录脉冲列的脉冲输出。这种方法有助于避免丢失嵌入在原始脉冲列中的信息。我们从模型参数空间中排除时间维度，并引入时间条件操作，使模型能够在自然范式中自适应地探索和利用刺激序列中的时间依赖关系。我们展示了 TeCoS-LVM 模型能够产生...

    Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
    
[^128]: 批规范化在线性模型和两层线性卷积神经网络中的隐式偏差

    The Implicit Bias of Batch Normalization in Linear Models and Two-layer Linear Convolutional Neural Networks. (arXiv:2306.11680v1 [cs.LG])

    [http://arxiv.org/abs/2306.11680](http://arxiv.org/abs/2306.11680)

    本文研究了使用批规范化训练线性模型和两层线性卷积神经网络时的隐式偏差，并证明批规范化对于均匀间隔具有隐含偏差。通过两个例子，我们发现在特定学习问题中，均匀间隔分类器的表现甚至优于最大间隔分类器。

    

    本文研究了由梯度下降训练的批规范化的隐含偏差。我们证明，当使用批规范化训练二分类线性模型时，梯度下降会收敛到一个具有均匀间隔的分类器，收敛速度为$\exp（- \Omega（\log ^ 2 t））$。这将批规范化的线性模型与不使用批规范化的模型区分开来，其隐含偏差和收敛速度均不同。我们进一步将结果扩展到了一类两层单滤波器线性卷积神经网络中，并表明批规范化对于均匀间隔具有隐含偏差。通过两个例子，我们证明了特定学习问题中均匀间隔分类器的性能可以优于最大间隔分类器。我们的研究为更好地理解批规范化提供了理论基础。

    We study the implicit bias of batch normalization trained by gradient descent. We show that when learning a linear model with batch normalization for binary classification, gradient descent converges to a uniform margin classifier on the training data with an $\exp(-\Omega(\log^2 t))$ convergence rate. This distinguishes linear models with batch normalization from those without batch normalization in terms of both the type of implicit bias and the convergence rate. We further extend our result to a class of two-layer, single-filter linear convolutional neural networks, and show that batch normalization has an implicit bias towards a patch-wise uniform margin. Based on two examples, we demonstrate that patch-wise uniform margin classifiers can outperform the maximum margin classifiers in certain learning problems. Our results contribute to a better theoretical understanding of batch normalization.
    
[^129]: 通过最终层反演进行单模型归因

    Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])

    [http://arxiv.org/abs/2306.06210](http://arxiv.org/abs/2306.06210)

    本文提出了一种利用最终层反演和异常检测的开放式单模型归因方法，解决了以往方法要么局限于封闭式环境、要么需要对生成模型进行不必要的改变的问题。实验结果表明该方法优于现有方法。

    

    最近关于生成模型方面的开创性发展引起了人们对于实用单模型归因的兴趣。这些方法可以预测一个样本是由特定的生成器生成的还是不是，例如，为了证明知识产权盗窃行为。然而，以前的方法要么局限于封闭式环境，要么需要对生成模型进行不必要的改变。本文提出了FLIPAD，一种基于最终层反演和异常检测的开放式单模型归因方法，以解决这些问题。我们展示利用的最终层反演可以简化为一个凸的 Lasso 优化问题，从而使我们的方法在理论上可靠且计算效率高。理论结果还得到了实验研究的支持，证明本文方法的有效性，优于现有方法。

    Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
    
[^130]: 实现合成主动推理代理，第二部分：变分信息更新

    Realising Synthetic Active Inference Agents, Part II: Variational Message Updates. (arXiv:2306.02733v1 [stat.ML])

    [http://arxiv.org/abs/2306.02733](http://arxiv.org/abs/2306.02733)

    本文讨论了解决广义自由能（FE）目标的合成主动推理代理的变分信息更新和消息传递算法，通过对T形迷宫导航任务的模拟比较，表明AIF可引起认知行为。

    

    自由能原理（FEP）描述生物代理通过相应环境的生成模型最小化变分自由能（FE）。主动推理（AIF）是FEP的推论，描述了代理人通过最小化期望的FE目标来探索和利用其环境。在两篇相关论文中，我们通过自由形式Forney-style因子图（FFG）上的消息传递，描述了一种可扩展的合成AIF代理的认知方法。本文（第二部分）根据变分演算法，导出了最小化CFFG上（广义）FE目标的消息传递算法。比较了模拟Bethe和广义FE代理之间的差异，说明了合成AIF如何在T形迷宫导航任务上引起认知行为。通过对合成AIF代理的完整消息传递描述，可以推导和重用该代理在不同环境下的行为。

    The Free Energy Principle (FEP) describes (biological) agents as minimising a variational Free Energy (FE) with respect to a generative model of their environment. Active Inference (AIF) is a corollary of the FEP that describes how agents explore and exploit their environment by minimising an expected FE objective. In two related papers, we describe a scalable, epistemic approach to synthetic AIF agents, by message passing on free-form Forney-style Factor Graphs (FFGs). A companion paper (part I) introduces a Constrained FFG (CFFG) notation that visually represents (generalised) FE objectives for AIF. The current paper (part II) derives message passing algorithms that minimise (generalised) FE objectives on a CFFG by variational calculus. A comparison between simulated Bethe and generalised FE agents illustrates how synthetic AIF induces epistemic behaviour on a T-maze navigation task. With a full message passing account of synthetic AIF agents, it becomes possible to derive and reuse 
    
[^131]: 平滑单调网络

    Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])

    [http://arxiv.org/abs/2306.01147](http://arxiv.org/abs/2306.01147)

    本文提出了一种新的神经网络模块--平滑min-max(SMM)网络，相比于传统的min-max(MM)神经网络结构简单易用，在单调建模方面表现优异。

    

    单调性约束是统计建模中的强力正则化工具。它们可以在计算机支持的决策制定中支持公平性，并增加数据驱动科学模型的可信度。经典的min-max(MM)神经网络结构确保了单调性，但由于梯度消失而往往在训练过程中陷入不良局部最优。我们提出了对MM网络的简单修改，使用严格递增的平滑非线性函数来缓解这个问题。得到的平滑min-max(SMM)网络模块继承了MM架构的渐近逼近性质。它可以嵌入到更大的端到端深度学习系统中进行训练。在单调建模的神经网络方面，SMM模块要简单得多，计算需求也要少得多。尽管如此，在我们的实验中，它在泛化性能方面与替代神经和非神经方法相比表现得更为优异。

    Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
    
[^132]: 从黑盒模型到可解释模型的转化，用于高效的迁移学习

    Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])

    [http://arxiv.org/abs/2305.17303](http://arxiv.org/abs/2305.17303)

    本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。

    

    建立具有普适性的AI模型是医疗领域面临的主要挑战之一。神经网络（NN）模型即使输入分布轻微移位（例如扫描仪类型），也会受到影响，而放射科医生则依赖于异常性的通用描述性规则。微调模型以将知识从一个领域转移到另一个领域需要大量标记数据。在本文中，我们开发了一种可解释的模型，它可以在计算成本最小的情况下，高效地针对未知的目标域进行微调。我们认为NN的可解释组件大致是域不变的。然而，可解释模型通常表现不及它们的BB变体。在源域中我们先使用人类理解的概念从BB开始，将其提炼成一组浅显易懂的interpretable模型。由于每个interpretable模型都覆盖了数据的一个子集，具有一组interpretable模型的混合可以实现与BB相当的性能。

    Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
    
[^133]: 聚焦是迁移学习的关键。

    Refocusing Is Key to Transfer Learning. (arXiv:2305.15542v1 [cs.CV])

    [http://arxiv.org/abs/2305.15542](http://arxiv.org/abs/2305.15542)

    这篇论文提出了一种名为 TOAST 的迁移学习算法，通过重新聚焦注意力，选择与任务相关的元素并反馈回模型，有效地提高了细粒度视觉分类数据集的性能，同时具有小部分可调参数。

    

    迁移学习涉及将预先训练好的模型适应新的下游任务。然而，我们观察到当前的迁移学习方法常常无法聚焦于与任务相关的特征。在这项工作中，我们强调了在迁移学习中重新聚焦注意力的重要性。我们引入了一种新的迁移学习算法-Top-Down Attention Steering（TOAST），它保持预先训练的骨干结构不变，同时选择输出中与任务有关的元素，并将它们反馈回模型，以引导其注意任务特定的特征。仅通过重新聚焦注意力，TOAST在许多迁移学习基准测试中实现了最先进的结果，同时具有小部分可调参数。与完全微调、LoRA和提示微调相比，TOAST在一系列细粒度视觉分类数据集上（例如，在 FGVC 上从 81.1% 提高到 86.2%）显着提高了性能。TOAST在指令跟随方面也优于完全微调的 Alpaca 模型。

    Transfer learning involves adapting a pre-trained model to novel downstream tasks. However, we observe that current transfer learning methods often fail to focus on task-relevant features. In this work, we emphasize the importance of refocusing the attention in transfer learning. We introduce Top-Down Attention Steering (TOAST), a novel transfer learning algorithm that keeps the pre-trained backbone frozen, while selecting the task-relevant elements in the output and feeding them back to the model to steer its attention to the task-specific features. By refocusing the attention only, TOAST achieves state-of-the-art results on a number of transfer learning benchmarks, while having a small portion of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt tuning, TOAST substantially improves performance across a range of fine-grained visual classification datasets (e.g., 81.1% -> 86.2% on FGVC). TOAST also outperforms the fully fine-tuned Alpaca model on instruction-following
    
[^134]: 基于参数隔离的动态图上的持续学习

    Continual Learning on Dynamic Graphs via Parameter Isolation. (arXiv:2305.13825v1 [cs.LG])

    [http://arxiv.org/abs/2305.13825](http://arxiv.org/abs/2305.13825)

    提出了Parameter Isolation GNN (PI-GNN)模型，用于处理动态图上的持续学习任务。该模型通过参数隔离和扩展来避免学习新模式和保留旧模式之间的权衡。

    

    许多实际的图学习任务需要处理新节点和边出现的动态图。动态图学习方法通常遭遇灾难性遗忘问题，即为以前的图所学的知识会被新图的更新覆盖。为了缓解这个问题，提出了持续图学习方法。然而，现有的持续图学习方法旨在学习新的模式并维护旧的模式，但使用相同固定大小的参数集，因此面临两种目标之间的根本权衡。在本文中，我们提出了Parameter Isolation GNN (PI-GNN)，用于动态图上的持续学习，通过参数隔离和扩展来避免这种权衡。我们的动机在于不同的参数对于学习不同的图模式有贡献。基于这个想法，我们扩展模型参数以持续学习出现的图模式。与此同时，为了有效地保存未受影响模式的知识，我们找到参数。

    Many real-world graph learning tasks require handling dynamic graphs where new nodes and edges emerge. Dynamic graph learning methods commonly suffer from the catastrophic forgetting problem, where knowledge learned for previous graphs is overwritten by updates for new graphs. To alleviate the problem, continual graph learning methods are proposed. However, existing continual graph learning methods aim to learn new patterns and maintain old ones with the same set of parameters of fixed size, and thus face a fundamental tradeoff between both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN) for continual learning on dynamic graphs that circumvents the tradeoff via parameter isolation and expansion. Our motivation lies in that different parameters contribute to learning different graph patterns. Based on the idea, we expand model parameters to continually learn emerging graph patterns. Meanwhile, to effectively preserve knowledge for unaffected patterns, we find parameter
    
[^135]: 深度神经网络的连续仿射学习

    Successive Affine Learning for Deep Neural Networks. (arXiv:2305.07996v1 [cs.LG])

    [http://arxiv.org/abs/2305.07996](http://arxiv.org/abs/2305.07996)

    本文提出了一种连续仿射学习（SAL）模型，用于构建深度神经网络(DNNs)。 该模型通过解决涉及激活函数的二次/凸优化问题来学习仿射映射，但是在权重矩阵和偏差之后。

    

    本文介绍了一种用于构建深度神经网络(DNNs)的连续仿射学习(SAL)模型。传统上，DNN是通过解决非凸优化问题来构建的。由于其非凸性和层数众多，通常难以在数值上解决这种问题。为了解决这一挑战，本文作者启发于人类教育系统，最近提出了多级深度学习(MGDL)模型。MGDL模型以多个年级的形式学习DNN，在每个年级中构建由少量层数组成的浅层DNN。MGDL模型仍需要解决几个非凸优化问题。提出的SAL模型是在MGDL模型基础上演变而来。发现DNN的每层都由仿射映射和激活函数组成，我们建议通过解决涉及激活函数的二次/凸优化问题来学习仿射映射，但是在权重矩阵和偏差之后。

    This paper introduces a successive affine learning (SAL) model for constructing deep neural networks (DNNs). Traditionally, a DNN is built by solving a non-convex optimization problem. It is often challenging to solve such a problem numerically due to its non-convexity and having a large number of layers. To address this challenge, inspired by the human education system, the multi-grade deep learning (MGDL) model was recently initiated by the author of this paper. The MGDL model learns a DNN in several grades, in each of which one constructs a shallow DNN consisting of a small number of layers. The MGDL model still requires solving several non-convex optimization problems. The proposed SAL model mutates from the MGDL model. Noting that each layer of a DNN consists of an affine map followed by an activation function, we propose to learn the affine map by solving a quadratic/convex optimization problem which involves the activation function only {\it after} the weight matrix and the bias
    
[^136]: 应用基于生成式预训练自回归Transformer图神经网络的方法分析和发现新型蛋白质

    Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.04934](http://arxiv.org/abs/2305.04934)

    本研究使用基于语言模型的深度学习策略，在蛋白质建模中应用transformer和图卷积的结构预训练生成模型，进一步训练后能够设计具有特定性质的蛋白质，案例验证表明该方法可生成理想目标性质的蛋白质。

    

    本文报道了一种灵活的基于语言模型的深度学习策略，应用于解决蛋白质建模中的正向和反向问题，使用一个整合了transformer和图卷积的注意力神经网络结构，在因果多头图机制中实现预训练生成模型。该模型被用于预测二级结构内容（每个残基的水平和总体内容）、蛋白质可溶性和测序任务。进一步在反向任务上训练，该模型能够设计具有这些性质作为目标特征的蛋白质。该模型被制定为一个通用的框架，完全基于提示，可以为各种下游任务进行适应。我们发现添加额外任务会产生相互协同作用，使模型在整体性能上得到提高，超过仅在每个数据集上训练模型的可能性。案例研究用于验证该方法，生成具有理想目标性质，包括稳定性和可溶性的蛋白质，并进行实验性研究。

    We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
    
[^137]: 利用不确定性感知因果模型提高基于图像的精准医疗

    Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v1 [cs.LG])

    [http://arxiv.org/abs/2305.03829](http://arxiv.org/abs/2305.03829)

    本研究采用贝叶斯深度学习估计多种治疗的因果后验分布，提高了基于图像的精准医疗的不确定性估计方法，以预测噪声多的医疗环境下的个体治疗效果。

    

    基于图像的精准医疗旨在根据个体的独特成像特征个性化诊疗决策，以改善其临床结果。集成不确定性估计作为治疗建议的机器学习框架将更加安全可靠。然而，在精准医疗中，几乎没有研究适应不确定性估计技术和验证指标。本文采用贝叶斯深度学习估计多种治疗的因果后验分布，从而估计每种治疗选项的不确定性以及任意两种治疗之间的个体治疗效果。我们对患有多发性硬化症的患者进行训练和评估，以预测新的和扩大的T2病变数量，并评估模型的相关性。

    Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of 
    
[^138]: 使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐：一项比较研究

    Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])

    [http://arxiv.org/abs/2305.03017](http://arxiv.org/abs/2305.03017)

    本研究使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐的质量，重点关注于Stack Overflow上的Java编程语言。研究使用BERT将代码示例转换为数值向量。

    

    过去和最近一直在进行代码示例推荐的研究，以帮助开发人员完成软件开发任务。由于开发人员经常花费大量时间在互联网上寻找相关的代码示例，利用开源项目和非正式文档。为了找到有用的代码示例，非正式文档（如Stack Overflow讨论和论坛）可以非常宝贵。我们的研究重点是Stack Overflow，它是软件开发人员讨论不同主题的流行资源。为了提高推荐代码示例的质量，我们收集并推荐了Java编程语言中最佳的代码示例。我们采用了BERT来进行处理，它是一个大型语言模型（LLM），可以有效地从文本数据中提取语义信息。我们的第一步是使用BERT将代码示例转换为数值向量。

    The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
    
[^139]: 面向对抗鲁棒模型的超参数调节

    Hyper-parameter Tuning for Adversarially Robust Models. (arXiv:2304.02497v1 [cs.LG])

    [http://arxiv.org/abs/2304.02497](http://arxiv.org/abs/2304.02497)

    本文探究对抗训练模型的超参数调节问题，明确对抗环境下需要额外调整的超参数，并提出利用廉价对抗训练方法的新方案降低调节成本。

    

    本文关注对抗训练模型的超参数调节问题，旨在确定在对抗环境下哪些额外的超参数是需要调节的，同时降低对抗训练模型的调节成本。通过对3个广泛应用于先前有关对抗鲁棒性文献中的模型进行广泛的实验研究，我们对这一问题进行了探究，并发现该问题在对抗环境下的复杂性主要有两个方面：需要调整额外的超参数来平衡标准训练和对抗训练；需要独立调整标准训练和对抗训练阶段的超参数。同时，本文还提出了利用廉价的对抗训练方法来降低对抗训练模型超参数调节成本的新机会。

    This work focuses on the problem of hyper-parameter tuning (HPT) for robust (i.e., adversarially trained) models, with the twofold goal of i) establishing which additional HPs are relevant to tune in adversarial settings, and ii) reducing the cost of HPT for robust models. We pursue the first goal via an extensive experimental study based on 3 recent models widely adopted in the prior literature on adversarial robustness. Our findings show that the complexity of the HPT problem, already notoriously expensive, is exacerbated in adversarial settings due to two main reasons: i) the need of tuning additional HPs which balance standard and adversarial training; ii) the need of tuning the HPs of the standard and adversarial training phases independently. Fortunately, we also identify new opportunities to reduce the cost of HPT for robust models. Specifically, we propose to leverage cheap adversarial training methods to obtain inexpensive, yet highly correlated, estimations of the quality ach
    
[^140]: I2I: 用改进的知识初始化转接器

    I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])

    [http://arxiv.org/abs/2304.02168](http://arxiv.org/abs/2304.02168)

    本文提出了一种称为ImprovisetoInitialize(I2I)的连续学习算法，通过提取先前学习的任务适配器的知识来为即将到来的任务初始化适配器。这使得从一个任务到另一个任务的知识传递更加高效。

    

    转接器是延续学习中解决灾难性遗忘问题的一种有前途的解决方案。然而，为每个新任务训练独立的适配器模块错失了跨任务知识转移的机会。我们提出了一种称为 Improvise to Initialize (I2I) 的连续学习算法，通过提取先前学习的任务适配器的知识，为即将到来的任务初始化适配器。我们通过对视觉问答任务序列进行实验，评估了 I2I 在 CLiMB，一个多模态的连续学习基准上的表现。使用 I2I 训练的适配器始终比独立训练的适配器具有更好的任务精度，证明了我们的算法促进了任务适配器之间的知识转移，并且相对于先进的 AdapterFusion，I2I 也能实现更好的跨任务知识转移而不产生相关的参数成本。

    Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
    
[^141]: 在分布转移下诊断模型性能

    Diagnosing Model Performance Under Distribution Shift. (arXiv:2303.02011v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.02011](http://arxiv.org/abs/2303.02011)

    本研究提出一种名为 DISDE 的方法，用于分析模型在不同分布情况下的性能变化。该方法将性能下降分解为三个方面：难度更大但更频繁出现的示例增加、特征和结果之间关系的变化和在训练期间不频繁或未见过的示例性能差。

    

    当模型在不同于训练分布的目标分布下运行时，其性能可能会下降。为了理解这些操作失败模式，我们开发了一种方法，称为 DIstribution Shift DEcomposition（DISDE），将性能下降归因于不同类型的分布转移。我们的方法将性能下降分解为以下几个方面：1）来自训练的更难但更频繁的示例增加；2）特征和结果之间关系的变化；3）在训练期间不频繁或未见过的示例性能差。为了实现这一点，我们在固定 $X$ 的分布的同时改变 $Y \mid X$ 的条件分布，或在固定 $Y \mid X$ 的条件分布的同时改变 $X$ 的分布，从而定义了一个关于 $X$ 的假设分布，其中包含训练和目标中共同的值，可以轻松地比较 $Y \mid X$ 并进行预测。

    Prediction models can perform poorly when deployed to target distributions different from the training distribution. To understand these operational failure modes, we develop a method, called DIstribution Shift DEcomposition (DISDE), to attribute a drop in performance to different types of distribution shifts. Our approach decomposes the performance drop into terms for 1) an increase in harder but frequently seen examples from training, 2) changes in the relationship between features and outcomes, and 3) poor performance on examples infrequent or unseen during training. These terms are defined by fixing a distribution on $X$ while varying the conditional distribution of $Y \mid X$ between training and target, or by fixing the conditional distribution of $Y \mid X$ while varying the distribution on $X$. In order to do this, we define a hypothetical distribution on $X$ consisting of values common in both training and target, over which it is easy to compare $Y \mid X$ and thus predictive
    
[^142]: BBOB上高维贝叶斯优化算法的比较

    Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB. (arXiv:2303.00890v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00890](http://arxiv.org/abs/2303.00890)

    本研究比较了BBOB上五种高维贝叶斯优化算法与传统方法以及CMA-ES算法的性能，结果表明... (根据论文的具体内容进行总结)

    

    贝叶斯优化是一类基于黑盒、基于代理的启发式算法，可以有效地优化评估成本高、只能拥有有限的评估预算的问题。贝叶斯优化在解决工业界的数值优化问题中尤为受欢迎，因为目标函数的评估通常依赖耗时的模拟或物理实验。然而，许多工业问题涉及大量参数，这给贝叶斯优化算法带来了挑战，其性能在维度超过15个变量时常常下降。虽然已经提出了许多新算法来解决这个问题，但目前还不清楚哪种算法在哪种优化场景中表现最好。本研究比较了5种最新的高维贝叶斯优化算法与传统贝叶斯优化和CMA-ES算法在COCA环境下24个BBOB函数上的性能，在维度从10到60个变量不断增加的情况下进行了对比。我们的结果证实了...

    Bayesian Optimization (BO) is a class of black-box, surrogate-based heuristics that can efficiently optimize problems that are expensive to evaluate, and hence admit only small evaluation budgets. BO is particularly popular for solving numerical optimization problems in industry, where the evaluation of objective functions often relies on time-consuming simulations or physical experiments. However, many industrial problems depend on a large number of parameters. This poses a challenge for BO algorithms, whose performance is often reported to suffer when the dimension grows beyond 15 variables. Although many new algorithms have been proposed to address this problem, it is not well understood which one is the best for which optimization scenario.  In this work, we compare five state-of-the-art high-dimensional BO algorithms, with vanilla BO and CMA-ES on the 24 BBOB functions of the COCO environment at increasing dimensionality, ranging from 10 to 60 variables. Our results confirm the su
    
[^143]: 有导向性的扩散: 通过关注引导实现对象放置的直接控制

    Directed Diffusion: Direct Control of Object Placement through Attention Guidance. (arXiv:2302.13153v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.13153](http://arxiv.org/abs/2302.13153)

    本论文介绍了一种有导向性的扩散方法，通过关注引导在图像中直接控制对象的放置。通过观察提示词的交叉注意力映射，引入优化目标，在特定位置产生“激活”，从而改进了场景组合能力。

    

    文本引导的扩散模型（如DALLE-2、Imagen和稳定扩散）能够仅通过描述所需图像内容的简短文本提示生成各种形式的图像。在许多情况下，这些图像具有非常高的质量。然而，这些模型通常难以组合包含多个关键对象（如指定位置关系中的字符）的场景。在讲述故事中，能够“直接”指导人物和对象的放置，无论是在图像内还是跨图像内，都是至关重要的，这一点在电影和动画理论的文献中得到了认可。在本文中，我们采用一种特别简单的方法来提供所需的指导。基于一个观察结果，即提示词的交叉注意力映射反映出由这些词所表示的对象的空间布局，我们引入了一个优化目标，在这些交叉注意力映射中的期望位置产生“激活”。由此产生的方法是通向泛化的一步。

    Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion are able to generate an effectively endless variety of images given only a short text prompt describing the desired image content. In many cases the images are of very high quality. However, these models often struggle to compose scenes containing several key objects such as characters in specified positional relationships. The missing capability to "direct" the placement of characters and objects both within and across images is crucial in storytelling, as recognized in the literature on film and animation theory. In this work, we take a particularly straightforward approach to providing the needed direction. Drawing on the observation that the cross-attention maps for prompt words reflect the spatial layout of objects denoted by those words, we introduce an optimization objective that produces ``activation'' at desired positions in these cross-attention maps. The resulting approach is a step toward generalizin
    
[^144]: 使用脉冲形状和人工神经网络恢复PMT的饱和响应

    Restoring the saturation response of a PMT using pulse-shape and artificial-neural-networks. (arXiv:2302.06170v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2302.06170](http://arxiv.org/abs/2302.06170)

    该论文提出了使用脉冲形状和人工神经网络恢复PMT饱和响应的方法，可以估计线性区域并提高光子计数和能量重建效率。

    

    光电倍增管（PMT）的线性响应是光子计数和中微子能量重建的必要属性。使用基于线性烷基苯（LAB）的液体闪烁体研究了PMT的线性有效区域和饱和响应。观察到了两种不同饱和响应之间的相关性，即脉冲形状失真和脉冲面积减小。观察到的脉冲形状为估计脉冲面积相对线性区域提供了有用的信息。这种基于相关性的诊断允许原地估计线性范围，这在以前很具挑战性。利用测得的两个饱和响应之间的相关性，训练人工神经网络（ANN）来预测从观察到的脉冲形状中减小的脉冲面积。ANN预测的脉冲面积减小使得可以预测独立于饱和行为的理想光电子数。这种基于脉冲形状的方法估计PMT的线性范围并使用ANN恢复饱和响应是增强中微子探测器中光子计数和能量重建效率的重要贡献。

    The linear response of a photomultiplier tube (PMT) is a required property for photon counting and reconstruction of the neutrino energy. The linearity valid region and the saturation response of PMT were investigated using a linear-alkyl-benzene (LAB)-based liquid scintillator. A correlation was observed between the two different saturation responses, with pulse-shape distortion and pulse-area decrease. The observed pulse-shape provides useful information for the estimation of the linearity region relative to the pulse-area. This correlation-based diagnosis allows an ${in}$-${situ}$ estimation of the linearity range, which was previously challenging. The measured correlation between the two saturation responses was employed to train an artificial-neural-network (ANN) to predict the decrease in pulse-area from the observed pulse-shape. The ANN-predicted pulse-area decrease enables the prediction of the ideal number of photoelectrons irrelevant to the saturation behavior. This pulse-sha
    
[^145]: 使用深度学习在"Web of Science"中对研究领域进行层次分类

    Hierarchical Classification of Research Fields in the "Web of Science" Using Deep Learning. (arXiv:2302.00390v2 [cs.DL] UPDATED)

    [http://arxiv.org/abs/2302.00390](http://arxiv.org/abs/2302.00390)

    本文提出了一个使用深度学习进行层次分类的系统，可以自动将学术出版物通过抽象进行分类，实现了对研究活动在不同层次结构中的全面分类，并允许跨学科和跨领域的单标签和多标签分类。

    

    本文提出了一个层次分类系统，通过使用抽象将学术出版物自动分类到三级层次标签集（学科、领域、子领域）中，以多类别设置进行分类。该系统可以通过文章的知识生产和引用的影响，对研究活动在所述层次结构中进行全面的分类，并允许这些活动被归为多个类别。该分类系统在Microsoft Academic Graph (版本2018-05-17)的160 million份摘要片段中区分了44个学科、718个领域和1,485个子领域。我们以模块化和分布式方式进行批量训练，以解决和允许跨学科和跨领域的单标签和多标签分类。总共，我们在所有考虑的模型（卷积神经网络，循环神经网络，变形器）中进行了3,140次实验。分类准确率> 90％。

    This paper presents a hierarchical classification system that automatically categorizes a scholarly publication using its abstract into a three-tier hierarchical label set (discipline, field, subfield) in a multi-class setting. This system enables a holistic categorization of research activities in the mentioned hierarchy in terms of knowledge production through articles and impact through citations, permitting those activities to fall into multiple categories. The classification system distinguishes 44 disciplines, 718 fields and 1,485 subfields among 160 million abstract snippets in Microsoft Academic Graph (version 2018-05-17). We used batch training in a modularized and distributed fashion to address and allow for interdisciplinary and interfield classifications in single-label and multi-label settings. In total, we have conducted 3,140 experiments in all considered models (Convolutional Neural Networks, Recurrent Neural Networks, Transformers). The classification accuracy is > 90%
    
[^146]: 探究用于胸部 X 光片的孪生表示学习的图像增强方法

    Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays. (arXiv:2301.12636v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.12636](http://arxiv.org/abs/2301.12636)

    本研究系统地评估了不同的增强方法对学习到的胸部 X 光片异常检测的孪生表示的质量和鲁棒性的影响。结果显示，我们找到了一组能够产生良好泛化效果的鲁棒表示的增强方法。

    

    图像增强对于有效的自监督学习技术的视觉表示学习至关重要。虽然自然图像的增强策略已经得到了广泛研究，但医学图像与自然图像有很大的不同。因此，我们不清楚在孪生表示学习中常用的增强策略是否适用于医学图像，以及适用的程度。为了解决这个挑战，在本研究中，我们系统地评估了各种增强方法对学习到的表示的质量和鲁棒性的影响。我们在三个大型数据集（MIMIC-CXR、CheXpert 和 VinDR-CXR）上训练和评估了用于胸部 X 光片异常检测的孪生网络。我们通过线性探测、微调、零样本迁移和数据效率的实验来研究学习到的表示的有效性。最后，我们确定了一组增强方法，可以得到具有良好泛化能力的鲁棒表示。

    Image augmentations are quintessential for effective visual representation learning across self-supervised learning techniques. While augmentation strategies for natural imaging have been studied extensively, medical images are vastly different from their natural counterparts. Thus, it is unknown whether common augmentation strategies employed in Siamese representation learning generalize to medical images and to what extent. To address this challenge, in this study, we systematically assess the effect of various augmentations on the quality and robustness of the learned representations. We train and evaluate Siamese Networks for abnormality detection on chest X-Rays across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate the efficacy of the learned representations through experiments involving linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally, we identify a set of augmentations that yield robust representations that generalize well t
    
[^147]: 用于复杂查询回答的神经链接预测器的调整

    Adapting Neural Link Predictors for Complex Query Answering. (arXiv:2301.12313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12313](http://arxiv.org/abs/2301.12313)

    本文提出通过训练一个参数高效的分数适应模型来重新校准神经链接预测分数以解决神经链接预测器在复杂查询回答中的问题。

    

    在不完整知识图谱上回答复杂查询是一项具有挑战性的任务，模型需要在缺失知识的情况下回答复杂逻辑查询。最近，Arakelyan等人（2021）；Minervini等人（2022）表明，神经链接预测器也可以用于回答复杂查询：他们的连续查询分解（CQD）方法通过将复杂查询分解为原子子查询，使用神经链接预测器回答并通过t-范数来聚合其分数，以对每个复杂查询的答案进行排序。然而，CQD不处理否定并且仅使用原子训练查询的训练信号：在回答复杂查询期间，神经链接预测分数没有通过模糊逻辑t-范数进行校准以相互作用。在这项工作中，我们提出通过训练一个参数高效的分数适应模型来重新校准神经链接预测分数以解决这个问题：这个新组件通过反向传播法在复杂查询上进行训练。

    Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022) showed that neural link predictors could also be used for answering complex queries: their Continuous Query Decomposition (CQD) method works by decomposing complex queries into atomic sub-queries, answers them using neural link predictors and aggregates their scores via t-norms for ranking the answers to each complex query. However, CQD does not handle negations and only uses the training signal from atomic training queries: neural link prediction scores are not calibrated to interact together via fuzzy logic t-norms during complex query answering. In this work, we propose to address this problem by training a parameter-efficient score adaptation model to re-calibrate neural link prediction scores: this new component is trained on complex queries by back-propa
    
[^148]: ClimaX:一种用于天气和气候的基础模型

    ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10343](http://arxiv.org/abs/2301.10343)

    ClimaX是一种灵活且可推广的深度学习模型，用于天气和气候科学，可以使用不同数据集进行训练。

    

    目前大多数先进的天气和气候模型都是基于物理信息的数值模型。这些方法旨在模拟非线性动力学和多变量之间的复杂相互作用，这些相互作用很难近似。此外，许多这样的数值模型在模拟细粒度空间和时间分辨率的大气现象时计算量很大。最近的基于机器学习的数据驱动方法通过使用深度神经网络学习数据驱动的功能映射来直接解决下游预测或投射任务。然而，这些网络是使用为特定时空任务策划和同质化的气候数据集进行训练的，因此缺乏数值模型的普遍性。我们开发并演示了ClimaX，这是一个灵活且可推广的深度学习模型，可用于天气和气候科学，并可以使用跨越不同数据集的异构数据进行训练。

    Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
    
[^149]: 使用循环预测耦合物理信息神经网络在软传感建模中求解含非测量源项的PDEs

    Solving PDEs with Unmeasurable Source Terms Using Coupled Physics-Informed Neural Network with Recurrent Prediction in Soft Sensor Modeling. (arXiv:2301.08618v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08618](http://arxiv.org/abs/2301.08618)

    提出了一种带循环预测学习策略的耦合物理信息神经网络（CPINN），用于软传感建模中的含非测量源项的PDEs求解，证明了CPINN具有满足PDEs解的近似容量，提高了软传感建模在时空工业系统中的性能表现。

    

    针对含非测量源项的非齐次偏微分方程（PDEs）在软传感建模中难以很好解决的问题，本文提出了一种带循环预测（RP）学习策略的耦合物理信息神经网络（CPINN）来进行软传感建模。首先，提出了包含NetU和NetG的CPINN，其中NetU用于近似研究PDEs的解，NetG用于正则化NetU的训练，两个网络被集成到一个数据-物理混合损失函数中。随后，在理论上证明了所提出的CPINN具有满足PDEs解的近似容量。此外，我们提出了一种分层训练策略来优化和耦合两个网络以实现CPINN的参数。其次，通过在NetU中引入循环机制来进行未来状态的预测，提出了NetU-RP，以提高软传感建模在时空工业系统中的性能表现。最后，通过模拟振动位移系统的例子，演示了所提出的CPINN-RP方法在求解含非测量源项的PDEs和软传感建模应用中的有效性。

    Nonhomogeneous partial differential equations (PDEs) are an applicable model in soft sensor modeling for describing spatiotemporal industrial systems with unmeasurable source terms, which cannot be well solved by existing physics-informed neural networks (PINNs). To this end, a coupled PINN (CPINN) with a recurrent prediction (RP) learning strategy (CPINN-RP) is proposed for soft sensor modeling in spatiotemporal industrial processes, such as vibration displacement. First, CPINN containing NetU and NetG is proposed. NetU is used to approximate the solutions to PDEs under study and NetG is used to regularize the training of NetU. The two networks are integrated into a data-physics-hybrid loss function. Then, we theoretically prove that the proposed CPINN has a satisfying approximation capacity to the PDEs solutions. Besides the theoretical aspects, we propose a hierarchical training strategy to optimize and couple the two networks to achieve the parameters of CPINN. Secondly, NetU-RP is
    
[^150]: 具有聚类结构的潜在bandit问题的最优算法。

    Optimal Algorithms for Latent Bandits with Cluster Structure. (arXiv:2301.07040v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07040](http://arxiv.org/abs/2301.07040)

    本文提出了一种名为LATTICE的方法，用于解决具有聚类结构的潜在bandit问题，并在最小化遗憾方面达到了最优解。

    

    本文考虑具有聚类结构的潜在bandit问题，在这个问题中有多个用户，每个用户都有一个相关的多臂赌博机问题。这些用户被分成“潜在”簇，使得同一簇内的用户的平均奖励向量相同。在每一轮中，随机选择一个用户，拉动一个手臂并观察相应的噪声奖励。用户的目标是最大化累积奖励。这个问题对于实际的推荐系统非常重要，并且最近已经引起了广泛的关注。然而，如果每个用户都独立行动，他们将不得不独立探索每个手臂，并且不可避免地产生$\Omega(\sqrt{\mathsf{MNT}})$的遗憾，其中$\mathsf{M}$和$\mathsf{N}$分别是手臂和用户的数量。相反，我们提出了LATTICE (通过矩阵补全实现的潜在bandit问题)方法，该方法利用了潜在的聚类结构，提供了极小化最优遗憾。

    We consider the problem of latent bandits with cluster structure where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into \emph{latent} clusters such that the mean reward vectors of users within the same cluster are identical. At each round, a user, selected uniformly at random, pulls an arm and observes a corresponding noisy reward. The goal of the users is to maximize their cumulative rewards. This problem is central to practical recommendation systems and has received wide attention of late \cite{gentile2014online, maillard2014latent}. Now, if each user acts independently, then they would have to explore each arm independently and a regret of $\Omega(\sqrt{\mathsf{MNT}})$ is unavoidable, where $\mathsf{M}, \mathsf{N}$ are the number of arms and users, respectively. Instead, we propose LATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploitation of the latent cluster structure to provide the minimax optimal regret of
    
[^151]: 在联邦学习中实现小型神经网络的分布式剪枝

    Distributed Pruning Towards Tiny Neural Networks in Federated Learning. (arXiv:2212.01977v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01977](http://arxiv.org/abs/2212.01977)

    该论文提出了FedTiny，一个用于联邦学习的分布式剪枝框架，可以为内存和计算受限的设备生成专门的小型模型。在这里，作者引入了自适应的批归一化选择模块来解决剪枝中的偏差问题。

    

    神经网络剪枝是减小深度神经网络大小和复杂度的重要技术，使得在资源有限的设备上可以进行大规模模型。然而，现有的剪枝方法过于依赖训练数据来指导剪枝策略，使得它们对于联邦学习中的分布式和保密数据集效果不好。此外，内存和计算密集型的剪枝过程在资源受限的设备上变得不可行。为了解决这些挑战，我们提出了FedTiny，这是一个用于联邦学习的分布式剪枝框架，可以为内存和计算受限的设备生成专门的小型模型。我们在FedTiny中引入了两个关键模块，以适应稀疏和廉价的局部计算部署场景，自适应地搜索粗剪枝和细剪枝的专用模型。首先，设计了一个自适应的批归一化选择模块，来减轻剪枝中由异质性引起的偏差。

    Neural network pruning is an essential technique for reducing the size and complexity of deep neural networks, enabling large-scale models on devices with limited resources. However, existing pruning approaches heavily rely on training data for guiding the pruning strategies, making them ineffective for federated learning over distributed and confidential datasets. Additionally, the memory- and computation-intensive pruning process becomes infeasible for recourse-constrained devices in federated learning. To address these challenges, we propose FedTiny, a distributed pruning framework for federated learning that generates specialized tiny models for memory- and computing-constrained devices. We introduce two key modules in FedTiny to adaptively search coarse- and finer-pruned specialized models to fit deployment scenarios with sparse and cheap local computation. First, an adaptive batch normalization selection module is designed to mitigate biases in pruning caused by the heterogeneity
    
[^152]: 对抗性廉价交流

    Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11030](http://arxiv.org/abs/2211.11030)

    本文提出了一种新型对抗性设置，在其中对手只能将信息附加到受害者的观察中，从而产生最小的影响范围，并提出对抗性廉价交流（ACT）算法进行对手训练。在高度受限的情况下，使用ACT训练的对手仍会对受害者的训练和测试表现产生显著影响，揭示了强化学习算法中的一种新的攻击向量。

    

    强化学习中的对抗性攻击通常假定攻击者可以高度特权地访问受害者的参数、环境或数据。本文提出了一种称为廉价交流MDP的新型对抗性设置，其中对手只能将确定性信息附加到受害者的观察中，从而产生最小的影响范围。对手不能掩盖地面事实，影响基本环境动态或奖励信号，引入不稳定性，增加随机性，看到受害者的动作或访问他们的参数。此外，我们提出了一种简单的元学习算法，称为对抗性廉价交流（ACT），在这种设置中对对手进行训练。我们证明，即使在高度受限的情况下，使用ACT训练的对手仍会显着影响受害者的训练和测试表现。影响训练时间表现揭示了一种新的攻击向量，并为现有强化学习算法的成功和失败模式提供了见解。

    Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
    
[^153]: 基于去中心化的联邦学习: 基础、现状、框架、趋势和挑战 (arXiv:2211.08413v2 [cs.LG] UPDATED)

    Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08413](http://arxiv.org/abs/2211.08413)

    本文提出去中心化联邦学习（DFL）来解决传统中心化FL（CFL）模型中的问题，主要研究DFL与CFL的差异、DFL的基础理论、DFL框架的设计与评估以及DFL在实际应用中的场景。

    

    在过去的十年中，联邦学习（FL）已经成为在不共享敏感数据的情况下训练协作模型的一种重要方法。自问世以来，中心化FL（CFL）一直是文献中最常见的方法，其中一个中心化实体创建全局模型。然而，中心化方法会导致瓶颈增加、系统故障风险增高，影响负责创建全局模型的实体的可信度。去中心化联邦学习（DFL）应运而生，通过推广去中心化模型聚合并最小化对中心化架构的依赖，解决了这些问题。但是，尽管在DFL方面有所努力，文献还没有研究(i)DFL和CFL之间的主要差异;(ii)分析DFL框架以创建和评估新解决方案;(iii)回顾使用DFL的应用场景。因此，本文在联邦架构、安全性、通信等方面识别并分析了DFL的主要基础。

    In the last decade, Federated Learning (FL) has gained relevance in training collaborative models without sharing sensitive data. Since its birth, Centralized FL (CFL) has been the most common approach in the literature, where a central entity creates a global model. However, a centralized approach leads to increased latency due to bottlenecks, heightened vulnerability to system failures, and trustworthiness concerns affecting the entity responsible for the global model creation. Decentralized Federated Learning (DFL) emerged to address these concerns by promoting decentralized model aggregation and minimizing reliance on centralized architectures. However, despite the work done in DFL, the literature has not (i) studied the main aspects differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and evaluate new solutions; and (iii) reviewed application scenarios using DFL. Thus, this article identifies and analyzes the main fundamentals of DFL in terms of federation architect
    
[^154]: 《可解释异常检测调查》

    A Survey on Explainable Anomaly Detection. (arXiv:2210.06959v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06959](http://arxiv.org/abs/2210.06959)

    该调查旨在提供关于最先进的可解释异常检测技术的全面和结构化的信息，帮助实践者和研究人员找到最适合他们需求的方法。

    

    在过去的二十年中，大部分关于异常检测的研究都集中在提高检测的准确性上，而很大程度上忽视了相应方法的可解释性，因此将结果的解释留给了实践者。由于异常检测算法越来越广泛地应用于安全关键领域，在这些领域做出的高风险决策需要提供解释已成为一项道德和监管要求。因此，本研究对最先进的可解释异常检测技术进行了全面和结构化的调查。我们提出了一个基于每种可解释异常检测技术的主要特征的分类法，旨在帮助实践者和研究人员找到最适合他们需求的可解释异常检测方法。

    In the past two decades, most research on anomaly detection has focused on improving the accuracy of the detection, while largely ignoring the explainability of the corresponding methods and thus leaving the explanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-critical domains, providing explanations for the high-stakes decisions made in those domains has become an ethical and regulatory requirement. Therefore, this work provides a comprehensive and structured survey on state-of-the-art explainable anomaly detection techniques. We propose a taxonomy based on the main aspects that characterize each explainable anomaly detection technique, aiming to help practitioners and researchers find the explainable anomaly detection method that best suits their needs.
    
[^155]: 使用加权不对称损失函数的神经网络模型预测区间

    Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.04318](http://arxiv.org/abs/2210.04318)

    本论文提出了一种使用加权不对称损失函数的方法，生成可靠的预测区间，适用于复杂的机器学习情境，可扩展为参数化函数的PI预测。

    

    我们提出了一种简单而有效的方法来生成近似和预测趋势的预测区间（PIs）。我们利用加权不对称损失函数来估计PI的下限和上限，权重由区间宽度确定。我们提供了该方法的简洁数学证明，展示了如何将其扩展到为参数化函数推导PI，并论证了该方法为预测相关变量的PI而有效的原因。我们在基于神经网络的模型的真实世界预测任务上对该方法进行了测试，结果表明它在复杂的机器学习情境下可以产生可靠的PI。

    We propose a simple and efficient approach to generate prediction intervals (PIs) for approximated and forecasted trends. Our method leverages a weighted asymmetric loss function to estimate the lower and upper bounds of the PIs, with the weights determined by the interval width. We provide a concise mathematical proof of the method, show how it can be extended to derive PIs for parametrised functions and argue why the method works for predicting PIs of dependent variables. The presented tests of the method on a real-world forecasting task using a neural network-based model show that it can produce reliable PIs in complex machine learning scenarios.
    
[^156]: 无线网络中基于可变位宽的联邦学习性能优化

    Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks. (arXiv:2209.10200v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10200](http://arxiv.org/abs/2209.10200)

    本文提出了一种在联邦学习中应用模型量化的方案，通过联邦学习的可变位宽优化来提高无线通信和计算效率，在无线资源受限的情况下，采用多尺度量化联邦学习聚合算法，能够有效改善联邦学习的性能，具备更高的收敛速度和更少的训练损失。

    

    本文研究了通过模型量化来改善联邦学习（FL）中的无线通信和计算效率。在所提出的位宽FL方案中，边缘设备训练和传输其本地FL模型参数的量化版本到一个协调服务器，将它们聚合成一个量化的全局模型并同步设备。目标是共同确定用于本地FL模型量化的位宽和每次迭代参与FL训练的设备集合。我们将这视为一个优化问题，旨在在每次迭代的设备抽样预算和延迟要求下最小化量化FL的训练损失。然而，所制定的问题难以解决，没有(i)对量化如何影响全局ML性能的具体理解以及(ii)服务器构建这个过程估计的能力。为了解决第一个挑战，我们分析地表征了有限的无线资源如何影响量化FL性能，并推导出了最优的位宽分配策略。为了应对第二个挑战，我们提出了一种新颖的多尺度量化FL聚合算法，使服务器能够轻松地从分布式量化FL模型中重构全局量化FL模型。广泛的仿真验证了我们的方法的有效性，在培训损失和收敛速度方面比现有技术方案提高了高达35％。

    This paper considers improving wireless communication and computation efficiency in federated learning (FL) via model quantization. In the proposed bitwidth FL scheme, edge devices train and transmit quantized versions of their local FL model parameters to a coordinating server, which aggregates them into a quantized global model and synchronizes the devices. The goal is to jointly determine the bitwidths employed for local FL model quantization and the set of devices participating in FL training at each iteration. We pose this as an optimization problem that aims to minimize the training loss of quantized FL under a per-iteration device sampling budget and delay requirement. However, the formulated problem is difficult to solve without (i) a concrete understanding of how quantization impacts global ML performance and (ii) the ability of the server to construct estimates of this process efficiently. To address the first challenge, we analytically characterize how limited wireless resou
    
[^157]: 通过双重随机缩放方法对流形密度和几何的稳健推断 (arXiv:2209.08004v2 [math.ST] UPDATED)

    Robust Inference of Manifold Density and Geometry by Doubly Stochastic Scaling. (arXiv:2209.08004v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2209.08004](http://arxiv.org/abs/2209.08004)

    本论文提出了一种对高维噪声下的流形密度和几何进行稳健推断的方法，通过双重随机缩放高斯核进行标准化，以解决高维噪声对传统标准化方法的不准确性问题。

    

    高斯核及其传统的标准化方法（例如，行随机化）是评估数据点之间相似性的常用方法。然而，在高维噪声下，它们可能不准确，特别是当噪声的幅度在数据中变化较大时，例如在异方差性或异常值下。在这项工作中，我们研究了一种更稳健的替代方案--高斯核的双重随机标准化。我们考虑一种情况，即从高维空间中嵌入低维流形上的未知密度中采样的点，并且可能受到可能强烈的、非同分布的、亚高斯噪声的污染。我们证明了双重随机亲和矩阵及其缩放因子在某些种群形式附近集中，并提供相应的有限样本概率误差界。然后，我们利用这些结果开发了几种在一般高维噪声下的稳健推断工具。首先，我们推导出一个稳健密度...

    The Gaussian kernel and its traditional normalizations (e.g., row-stochastic) are popular approaches for assessing similarities between data points. Yet, they can be inaccurate under high-dimensional noise, especially if the noise magnitude varies considerably across the data, e.g., under heteroskedasticity or outliers. In this work, we investigate a more robust alternative -- the doubly stochastic normalization of the Gaussian kernel. We consider a setting where points are sampled from an unknown density on a low-dimensional manifold embedded in high-dimensional space and corrupted by possibly strong, non-identically distributed, sub-Gaussian noise. We establish that the doubly stochastic affinity matrix and its scaling factors concentrate around certain population forms, and provide corresponding finite-sample probabilistic error bounds. We then utilize these results to develop several tools for robust inference under general high-dimensional noise. First, we derive a robust density 
    
[^158]: 基于对比特征学习的基于行为的早期自闭症诊断

    Action-based Early Autism Diagnosis Using Contrastive Feature Learning. (arXiv:2209.05379v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.05379](http://arxiv.org/abs/2209.05379)

    本研究提出了一种基于对比特征学习的基于行为的早期自闭症诊断方法，通过简单且少量的被试动作视频剪辑自动化诊断自闭症，克服了可用数据量小和样本变化大的挑战。

    

    自闭症，也被称为自闭症谱系障碍（ASD），是一种神经性疾病。其主要症状包括(口语和/或非口语)沟通困难和僵化/重复行为。这些症状常常难以与正常（对照）个体区分，因此这种障碍在儿童早期仍然未被诊断出来，从而导致延迟治疗。由于在早期年龄段学习曲线陡峭，早期诊断自闭症可以在正确的时间采取适当的干预措施，可能对自闭症孩子的成长产生积极影响。此外，传统的自闭症诊断方法需要多次就诊于专科精神科医生，然而这一过程可能耗时。在本文中，我们提出了一种基于学习的方法，利用简单且小的被试动作视频剪辑来自动化自闭症诊断。这项任务特别具有挑战性，因为可用的注释数据量很小，并且样本之间存在变化。

    Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological disorder. Its main symptoms include difficulty in (verbal and/or non-verbal) communication, and rigid/repetitive behavior. These symptoms are often indistinguishable from a normal (control) individual, due to which this disorder remains undiagnosed in early childhood leading to delayed treatment. Since the learning curve is steep during the initial age, an early diagnosis of autism could allow to take adequate interventions at the right time, which might positively affect the growth of an autistic child. Further, the traditional methods of autism diagnosis require multiple visits to a specialized psychiatrist, however this process can be time-consuming. In this paper, we present a learning based approach to automate autism diagnosis using simple and small action video clips of subjects. This task is particularly challenging because the amount of annotated data available is small, and the variations among samples
    
[^159]: 用Treeformers生成树结构

    Forming Trees with Treeformers. (arXiv:2207.06960v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.06960](http://arxiv.org/abs/2207.06960)

    本文介绍了一种Treeformer模块，它借鉴了CKY算法，通过学习组合运算符和汇聚函数来构建短语和句子的层次编码，从而将层次结构纳入Transformer模型中。实验证明，这种模块在组合泛化和各种自然语言任务中取得了显著的改进。

    

    人类语言具有嵌套的层次结构，使我们能够从较小的片段中构建复杂的句子。然而，许多最先进的神经网络模型（如Transformers）在其架构中没有明确的层次结构，即它们对层次结构没有归纳偏差。此外，已知Transformers在需要这种结构的组合泛化任务上表现不佳。在本文中，我们引入了Treeformer，这是一个通用的编码器模块，受到CKY算法的启发，它学习了一个组合运算符和汇聚函数，用于构建短语和句子的层次编码。我们的大量实验表明，将层次结构纳入Transformer模型中的好处，并且在组合泛化以及机器翻译、抽象摘要和各种自然语言理解任务等下游任务中取得了显著的改进。

    Human language is known to exhibit a nested, hierarchical structure, allowing us to form complex sentences out of smaller pieces. However, many state-of-the-art neural networks models such as Transformers have no explicit hierarchical structure in its architecture -- that is, they don't have an inductive bias toward hierarchical structure. Additionally, Transformers are known to perform poorly on compositional generalization tasks which require such structures. In this paper, we introduce Treeformer, a general-purpose encoder module inspired by the CKY algorithm which learns a composition operator and pooling function to construct hierarchical encodings for phrases and sentences. Our extensive experiments demonstrate the benefits of incorporating hierarchical structure into the Transformer and show significant improvements in compositional generalization as well as in downstream tasks such as machine translation, abstractive summarization, and various natural language understanding tas
    
[^160]: 使用小型量子计算机进行高维量子机器学习

    High Dimensional Quantum Machine Learning With Small Quantum Computers. (arXiv:2203.13739v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.13739](http://arxiv.org/abs/2203.13739)

    本研究探索了使用小型量子计算机进行高维量子机器学习的可能性，并且通过构建一个机器学习模型成功地在数字识别任务中实现了利用更少的电路评估来近似较大电路的输出。

    

    量子计算机对于增强机器学习具有巨大潜力，但目前的量子比特数量限制了这一潜力的实现。为了缓解这个限制，可以采用一种技术，在比电路所需的比特少的机器上评估量子电路。这些技术通过在较小的机器上评估许多较小的电路，然后将它们组合成一个多项式来复制较大机器的输出。这种方案对于一般电路而言需要更多的电路评估，这是不切实际的。然而，我们探讨了一种可能性，即对于某些应用来说，许多这些子电路是多余的，并且一个更小的求和足以估计完整的电路。我们构建了一个机器学习模型，可以利用更少的电路评估来逼近较大电路的输出。我们成功地将该模型应用于数字识别任务，使用模拟的量子计算机。

    Quantum computers hold great promise to enhance machine learning, but their current qubit counts restrict the realisation of this promise. In an attempt to placate this limitation techniques can be applied for evaluating a quantum circuit using a machine with fewer qubits than the circuit naively requires. These techniques work by evaluating many smaller circuits on the smaller machine, that are then combined in a polynomial to replicate the output of the larger machine. This scheme requires more circuit evaluations than are practical for general circuits. However, we investigate the possibility that for certain applications many of these subcircuits are superfluous, and that a much smaller sum is sufficient to estimate the full circuit. We construct a machine learning model that may be capable of approximating the outputs of the larger circuit with much fewer circuit evaluations. We successfully apply our model to the task of digit recognition, using simulated quantum computers much s
    
[^161]: 使用于无标度在线学习的等调节技术

    Isotuning With Applications To Scale-Free Online Learning. (arXiv:2112.14586v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.14586](http://arxiv.org/abs/2112.14586)

    我们提出了一种用于无标度在线学习的等调节技术，该技术具有快速、自适应、随时随地和无标度的特点，并可以自动适应遗憾的速率。同时，我们还引入了在线校正的方法来改进算法的性能。

    

    我们扩展和结合了文献中的几种方法，设计了快速、自适应、随时随地和无标度的在线学习算法。无标度的遗憾界限必须与最大损失成线性关系，不论是对于大损失还是对于非常小的损失。自适应的遗憾界限表明算法可以利用简单的数据并可能具有常数遗憾。我们致力于开发尽可能少依赖参数的快速算法，特别是它们应该是随时可用的，因此不依赖于时间范围。我们的第一个和主要工具是等调节技术，它是平衡遗憾权衡的思想的推广。我们开发了一套工具来轻松设计和分析这样的学习速度，并展示它们能够自动适应遗憾的速率（无论是常数、$O(\log T)$、$O(\sqrt{T})$等），并且在同样的观察量上比在事后选择的最优学习速度高出2倍。第二个工具是在线校正，它使我们能够获得...

    We extend and combine several tools of the literature to design fast, adaptive, anytime and scale-free online learning algorithms. Scale-free regret bounds must scale linearly with the maximum loss, both toward large losses and toward very small losses. Adaptive regret bounds demonstrate that an algorithm can take advantage of easy data and potentially have constant regret. We seek to develop fast algorithms that depend on as few parameters as possible, in particular they should be anytime and thus not depend on the time horizon. Our first and main tool, isotuning, is a generalization of the idea of balancing the trade-off of the regret. We develop a set of tools to design and analyze such learning rates easily and show that they adapts automatically to the rate of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a factor 2 of the optimal learning rate in hindsight for the same observed quantities. The second tool is an online correction, which allows us to obtain
    
[^162]: RELDEC: 强化学习基于的中等长度LDPC码的解码方法

    RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes. (arXiv:2112.13934v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2112.13934](http://arxiv.org/abs/2112.13934)

    RELDEC是一种基于强化学习的中等长度LDPC码的解码方法，通过训练智能体顺序调度CN集群，以优化解码策略。同时，通过改进MDP的状态空间表示，适用于更大块长的LDPC码。为了解决不同信道条件下的解码问题，提出了AM-RELDEC算法。

    

    在这项工作中，我们提出了一种名为RELDEC的新方法，用于中等长度低密度奇偶校验（LDPC）码的顺序解码。 RELDEC的主要思想是基于马尔可夫决策过程（MDP）通过强化学习获得优化的解码策略。与我们先前的工作不同，在先前的工作中，智能体仅学习一次迭代中集群内的单个检查节点（CN）的调度，而在这项工作中，我们训练智能体调度每个集群中的所有CN以及每次迭代中的所有集群。也就是说，在RELDEC的每个学习步骤中，智能体根据与调度特定集群相关联的奖励来顺序学习调度CN集群。我们还修改了MDP的状态空间表示，使RELDEC适用于比我们之前的工作研究的较大块长的LDPC码。此外，为了解决在不同信道条件下的解码问题，我们提出了灵活的元-RELDEC（AM-RELDEC）方法，它采用的是敏捷的元-RELDEC算法。

    In this work we propose RELDEC, a novel approach for sequential decoding of moderate length low-density parity-check (LDPC) codes. The main idea behind RELDEC is that an optimized decoding policy is subsequently obtained via reinforcement learning based on a Markov decision process (MDP). In contrast to our previous work, where an agent learns to schedule only a single check node (CN) within a group (cluster) of CNs per iteration, in this work we train the agent to schedule all CNs in a cluster, and all clusters in every iteration. That is, in each learning step of RELDEC an agent learns to schedule CN clusters sequentially depending on a reward associated with the outcome of scheduling a particular cluster. We also modify the state space representation of the MDP, enabling RELDEC to be suitable for larger block length LDPC codes than those studied in our previous work. Furthermore, to address decoding under varying channel conditions, we propose agile meta-RELDEC (AM-RELDEC) that empl
    
[^163]: 交互式决策制定的统计复杂性

    The Statistical Complexity of Interactive Decision Making. (arXiv:2112.13487v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.13487](http://arxiv.org/abs/2112.13487)

    本论文提出了一种复杂度度量，决策估计系数，用于解决交互式学习中的样本高效问题，并证明其为样本高效交互式学习的必要且充分条件。

    

    交互式学习和决策制定中的一个基本挑战是提供样本高效、自适应的学习算法，以实现近乎最优的遗憾。这个问题类似于经典的最优（监督）统计学习问题，在那里有着被广泛认可的复杂度度量（例如VC维和Rademacher复杂度）来控制学习的统计复杂性。然而，由于问题的自适应性质，表征交互式学习的统计复杂性会更具挑战性。本研究的主要结果提供了一种复杂度度量，即决策估计系数，该度量被证明是样本高效的交互式学习的必要且充分条件。具体而言，我们提供了：1. 任何交互式决策制定问题的最优遗憾的下界，确立决策估计系数作为一个基本限制；2. 一种统计学习算法，该算法在样本效率和自适应性方面具有最佳表现。

    A fundamental challenge in interactive learning and decision making, ranging from bandit problems to reinforcement learning, is to provide sample-efficient, adaptive learning algorithms that achieve near-optimal regret. This question is analogous to the classical problem of optimal (supervised) statistical learning, where there are well-known complexity measures (e.g., VC dimension and Rademacher complexity) that govern the statistical complexity of learning. However, characterizing the statistical complexity of interactive learning is substantially more challenging due to the adaptive nature of the problem. The main result of this work provides a complexity measure, the Decision-Estimation Coefficient, that is proven to be both necessary and sufficient for sample-efficient interactive learning. In particular, we provide:  1. a lower bound on the optimal regret for any interactive decision making problem, establishing the Decision-Estimation Coefficient as a fundamental limit.  2. a un
    
[^164]: 可响应的并行化架构用于在生产环境中部署深度学习模型

    Responsive parallelized architecture for deploying deep learning models in production environments. (arXiv:2112.08933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08933](http://arxiv.org/abs/2112.08933)

    本研究设计和提出了一个可响应的并行化架构，用于在实时生产环境中部署深度学习模型。采用层次化细化的标签注意力网络预测CV实体，并使用多个深度学习模型并行预测。通过选择轻量级微型Web框架和使用微服务来部署大型深度学习模型管道，达到在少于700毫秒的时间内解析普通CV的目的。

    

    招聘人员可以通过查看求职者的简历(CV)文档轻松筛选候选人。无结构的文档CV包含候选人的作品集和命名实体列表的详细信息。本研究的主要目标是设计和提出一个面向Web的、高度响应的计算管道，使用层次化细化的标签注意力网络系统地预测CV实体。使用专门用于命名实体识别的深度学习模型在大型数据集上进行训练，以预测相关字段。本文提出了一种在并行中使用一定数量的深度学习模型并实时预测的最佳策略。我们采用分析层次处理算法选择了一个轻量级的微型Web框架，并专注于一种有助于在生产就绪环境中使用微服务部署大型深度学习模型管道的方法。部署的模型和提出的架构有助于在少于700毫秒的时间内解析普通CV。

    Recruiters can easily shortlist candidates for jobs via viewing their curriculum vitae (CV) document. Unstructured document CV beholds candidate's portfolio and named entities listing details. The main aim of this study is to design and propose a web oriented, highly responsive, computational pipeline that systematically predicts CV entities using hierarchically-refined label attention networks. Deep learning models specialized for named entity recognition were trained on large dataset to predict relevant fields. The article suggests an optimal strategy to use a number of deep learning models in parallel and predict in real time. We demonstrate selection of light weight micro web framework using Analytical Hierarchy Processing algorithm and focus on an approach useful to deploy large deep learning model-based pipelines in production ready environments using microservices. Deployed models and architecture proposed helped in parsing normal CV in less than 700 milliseconds for sequential 
    
[^165]: 混合量子经典机器学习用于生成化学和药物设计

    Hybrid quantum-classical machine learning for generative chemistry and drug design. (arXiv:2108.11644v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2108.11644](http://arxiv.org/abs/2108.11644)

    本研究构建了一个混合的量子经典机器学习模型，利用深度生成化学模型加速药物发现。通过在D-Wave量子退火器上训练，成功生成了具有药物化学和合成可及性特性的2331个新化学结构。

    

    深度生成化学模型成为加速药物发现的强大工具。然而，所有可能的药物分子的结构空间的巨大大小和复杂性构成了重大的障碍，可以通过将量子计算机与深度经典网络相结合来克服。作为实现这一目标的第一步，我们构建了一个紧凑的离散变分自动编码器（DVAE），其中潜在层中的受限玻尔兹曼机（RBM）的大小被减小。拟议模型的大小足够小，可以适应最先进的D-Wave量子退火器，并允许在ChEMBL生物活性化合物数据集子集上进行训练。最后，我们生成了2331个具有药物化学和合成可及性特性的新化学结构，其范围与来自ChEMBL的分子类似。所呈现的结果证明了使用已经存在或即将推出的量子计算设备作为未来测试平台的可行性。

    Deep generative chemistry models emerge as powerful tools to expedite drug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome with hybrid architectures combining quantum computers with deep classical networks. As the first step toward this goal, we built a compact discrete variational autoencoder (DVAE) with a Restricted Boltzmann Machine (RBM) of reduced size in its latent layer. The size of the proposed model was small enough to fit on a state-of-the-art D-Wave quantum annealer and allowed training on a subset of the ChEMBL dataset of biologically active compounds. Finally, we generated 2331 novel chemical structures with medicinal chemistry and synthetic accessibility properties in the ranges typical for molecules from ChEMBL. The presented results demonstrate the feasibility of using already existing or soon-to-be-available quantum computing devices as testbeds for futur
    
[^166]: 提升还是不提升：自主学习率问题研究

    To Raise or Not To Raise: The Autonomous Learning Rate Question. (arXiv:2106.08767v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.08767](http://arxiv.org/abs/2106.08767)

    自主学习率控制器是一个解决深度学习中学习率设置问题的新方法，旨在避免耗时且琐碎的选择和修改过程，以及对网络架构、优化器、数据集和初始条件的微小变化的敏感性。

    

    学习率是深度学习领域中无处不在的参数。同时也存在一个普遍的问题：学习率应该设置为多少？然而，这个问题的真正答案通常是琐碎且耗时的，并且近年来积累了大量关于如何选择和修改学习率以实现最佳训练性能的深奥知识。此外，花费数小时精心调整学习率的努力可能会因为网络架构、优化器、数据集或初始条件微小的改变而化为乌有。但事实上，我们可以改变这种情况。我们提出了一个新的答案来解决这个重要的学习率问题：自主学习率控制器。

    There is a parameter ubiquitous throughout the deep learning world: learning rate. There is likewise a ubiquitous question: what should that learning rate be? The true answer to this question is often tedious and time consuming to obtain, and a great deal of arcane knowledge has accumulated in recent years over how to pick and modify learning rates to achieve optimal training performance. Moreover, the long hours spent carefully crafting the perfect learning rate can come to nothing the moment your network architecture, optimizer, dataset, or initial conditions change ever so slightly. But it need not be this way. We propose a new answer to the great learning rate question: the Autonomous Learning Rate Controller. Find it at https://github.com/fastestimator/ARC/tree/v2.0
    
[^167]: 通过随机单维搜索实现数据增强的自动化

    Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.08756](http://arxiv.org/abs/2106.08756)

    通过随机单维搜索的数据增强自动化方法，仅使用6次训练即可获得与使用100次训练相当的性能。

    

    对于深度学习研究者来说，找到训练期间最佳的数据增强策略对于取得最先进的性能而言至关重要。为此，社区已经见证了许多努力来自动化找到适合任何任务的完美增强程序的过程。然而，即使是最新的尖端方法也会带来巨大的计算开销，可能需要多达100个完整模型的训练来确定理想的配置。我们展示了如何使用仅6次训练就能达到相同的性能，通过随机单维增强方法进行数据增强。源代码可在https://github.com/fastestimator/RUA/tree/v1.0找到。

    It is no secret amongst deep learning researchers that finding the optimal data augmentation strategy during training can mean the difference between state-of-the-art performance and a run-of-the-mill result. To that end, the community has seen many efforts to automate the process of finding the perfect augmentation procedure for any task at hand. Unfortunately, even recent cutting-edge methods bring massive computational overhead, requiring as many as 100 full model trainings to settle on an ideal configuration. We show how to achieve equivalent performance using just 6 trainings with Random Unidimensional Augmentation. Source code is available at https://github.com/fastestimator/RUA/tree/v1.0
    
[^168]: 使用单传感器加速度计对杜兴肌肉营养不良患者（DMD）的步态特征进行表征：传统机器学习和深度学习方法

    Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches. (arXiv:2105.06295v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2105.06295](http://arxiv.org/abs/2105.06295)

    本研究使用单个加速度计对DMD患者和正常儿童的步态特征进行了比较，采取了传统机器学习和深度学习方法，从而实现了对两组之间的区分。

    

    儿童杜兴肌肉营养不良患者（DMD）与正常发育的同龄人的步态模式存在明显差异，但在步态实验室之外对这些差异进行定量分析一直是困难的。本研究利用腰部佩戴的iPhone加速度计测量了垂直、左右和前后加速度，记录了DMD和正常儿童在典型速度范围内的行走情况。参与研究的三到十六岁的15名正常儿童和15名DMD患儿进行了八项步行/奔跑活动，包括五个25米走/跑速度校准测试（SC-L1到SC-L5）、6分钟步行试验（6MWT）、一百米快走/慢跑/跑步（100MRW）和自由行走（FW）。为了进行临床评估，参与者还完成了北方评估（NSAA）。我们提取了时空步态临床特征（CFs），并应用多种机器学习方法来区分DMD和正常儿童。

    Differences in gait patterns of children with Duchenne muscular dystrophy (DMD) and typically-developing (TD) peers are visible to the eye, but quantifications of those differences outside of the gait laboratory have been elusive. In this work, we measured vertical, mediolateral, and anteroposterior acceleration using a waist-worn iPhone accelerometer during ambulation across a typical range of velocities. Fifteen TD and fifteen DMD children from 3-16 years of age underwent eight walking/running activities, including five 25 meters walk/run speed-calibration tests at a slow walk to running speeds (SC-L1 to SC-L5), a 6-minute walk test (6MWT), a 100 meters fast-walk/jog/run (100MRW), and a free walk (FW). For clinical anchoring purposes, participants completed a Northstar Ambulatory Assessment (NSAA). We extracted temporospatial gait clinical features (CFs) and applied multiple machine learning (ML) approaches to differentiate between DMD and TD children using extracted temporospatial g
    
[^169]: 使用GOTHIC自动检测双核星系并发现一大批双重AGN

    Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN. (arXiv:2011.12177v3 [astro-ph.GA] UPDATED)

    [http://arxiv.org/abs/2011.12177](http://arxiv.org/abs/2011.12177)

    我们提出了一种名为GOTHIC的算法，用于检测双核星系，发现了许多双重活动星系核。我们在大样本中检测到了159个双重AGN，其中2个是三重AGN系统。

    

    我们提出了一种新颖的算法GOTHIC（图形增强迭代爬山算法），用于检测具有两个或多个密集分离核的星系。我们的目标是在星系中检测双重或多重活动星系核（AGN）的样本。尽管星系合并很常见，但双重AGN的检测很罕见。它们的检测非常重要，因为它们帮助我们理解超大质量黑洞（SMBH）双星的形成、SMBH的增长和多核系统中的AGN反馈效应。因此，需要一种算法对现有的成像数据进行系统调查，以发现双核星系和双重AGN。我们在已知的DNG样本上测试了GOTHIC，并随后将其应用于SDSS DR16中大约100万个处于红移范围0至0.75的星系样本，并具有可用的光谱数据。我们在这个样本中检测到159个双重AGN，其中2个是三重AGN系统。

    We present a novel algorithm to detect double nuclei galaxies (DNG) called GOTHIC (Graph BOosted iterated HIll Climbing) - that detects whether a given image of a galaxy has two or more closely separated nuclei. Our aim is to detect samples of dual or multiple active galactic nuclei (AGN) in galaxies. Although galaxy mergers are common, the detection of dual AGN is rare. Their detection is very important as they help us understand the formation of supermassive black hole (SMBH) binaries, SMBH growth and AGN feedback effects in multiple nuclei systems. There is thus a need for an algorithm to do a systematic survey of existing imaging data for the discovery of DNGs and dual AGN. We have tested GOTHIC on a known sample of DNGs and subsequently applied it to a sample of a million SDSS DR16 galaxies lying in the redshift range of 0 to 0.75 approximately, and have available spectroscopic data. We have detected 159 dual AGN in this sample, of which 2 are triple AGN systems. Our results show 
    
[^170]: 在广义线性赌臂问题中的随机探索

    Randomized Exploration in Generalized Linear Bandits. (arXiv:1906.08947v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1906.08947](http://arxiv.org/abs/1906.08947)

    本文研究了广义线性赌臂问题中的两种随机算法，GLM-TSL和GLM-FPL。GLM-TSL从后验分布中采样广义线性模型，GLM-FPL则将广义线性模型拟合到过去奖励的随机扰动历史中。我们分析了这两种算法并得出了它们的遗憾上界，此前的工作中的遗憾上界得到了改进，并且对于非线性模型中的高斯噪声扰动问题，GLM-FPL是首次尝试。我们在逻辑赌臂问题和神经网络赌臂问题上对这两种算法进行了实证评估。这项工作展示了随机化在探索中的作用，超越了仅仅进行后验采样。

    

    我们研究了两种广义线性赌臂问题的随机算法。第一种算法GLM-TSL从后验分布的拉普拉斯拟合中采样广义线性模型(GLM)。第二种算法GLM-FPL将一个广义线性模型拟合到过去奖励的随机扰动历史中。我们分析了这两种算法，并得出了它们在n轮中遗憾上界$\tilde{O}(d \sqrt{n \log K})$，其中$d$是特征的数量，$K$是臂的数量。前者改进了先前的工作，而后者是非线性模型中高斯噪声扰动的首次尝试。我们在逻辑赌臂问题中对GLM-TSL和GLM-FPL进行了实证评估，并将GLM-FPL应用于神经网络赌臂问题。我们的工作展示了探索中随机化的作用，不仅仅是后验采样。

    We study two randomized algorithms for generalized linear bandits. The first, GLM-TSL, samples a generalized linear model (GLM) from the Laplace approximation to the posterior distribution. The second, GLM-FPL, fits a GLM to a randomly perturbed history of past rewards. We analyze both algorithms and derive $\tilde{O}(d \sqrt{n \log K})$ upper bounds on their $n$-round regret, where $d$ is the number of features and $K$ is the number of arms. The former improves on prior work while the latter is the first for Gaussian noise perturbations in non-linear models. We empirically evaluate both GLM-TSL and GLM-FPL in logistic bandits, and apply GLM-FPL to neural network bandits. Our work showcases the role of randomization, beyond posterior sampling, in exploration.
    
[^171]: 在随机线性赌博机中的干扰历史探索

    Perturbed-History Exploration in Stochastic Linear Bandits. (arXiv:1903.09132v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1903.09132](http://arxiv.org/abs/1903.09132)

    我们提出了一种在线算法，通过在训练于其干扰历史的线性模型上选择估计奖励最高的臂，用于在随机线性赌博机中最小化累积遗憾。我们推导出了一个关于算法遗憾的较好界限，并通过实证评估展示了算法的实用性。

    

    我们提出了一种新的在线算法，用于在随机线性赌博机中最小化累积遗憾。该算法在训练于其干扰历史的线性模型上选择估计奖励最高的臂。因此，我们称之为线性赌博机中的干扰历史探索（LinPHE）。所谓干扰历史是指观察到的奖励和随机生成的独立同分布的伪奖励的混合。我们推导出对于LinPHE的$n$轮遗憾，其中$d$是特征数量，有一个$\tilde{O}(d \sqrt{n})$的间隙自由界。我们分析的关键步骤是关于伯努利随机变量的加权和的新的集中和反集中边界。为了展示我们设计的普遍性，我们将LinPHE推广到一个逻辑模型中。我们通过实证评估证明了我们的算法的实用性。

    We propose a new online algorithm for cumulative regret minimization in a stochastic linear bandit. The algorithm pulls the arm with the highest estimated reward in a linear model trained on its perturbed history. Therefore, we call it perturbed-history exploration in a linear bandit (LinPHE). The perturbed history is a mixture of observed rewards and randomly generated i.i.d. pseudo-rewards. We derive a $\tilde{O}(d \sqrt{n})$ gap-free bound on the $n$-round regret of LinPHE, where $d$ is the number of features. The key steps in our analysis are new concentration and anti-concentration bounds on the weighted sum of Bernoulli random variables. To show the generality of our design, we generalize LinPHE to a logistic model. We evaluate our algorithms empirically and show that they are practical.
    

