# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LightPath: Lightweight and Scalable Path Representation Learning.](http://arxiv.org/abs/2307.10171) | LightPath是一个轻量级和可伸缩的路径表示学习框架，旨在实现在资源受限的环境下降低资源消耗和实现可伸缩性，同时保持高准确性，从而实现更广泛的适用性。 |
| [^2] | [Challenges and Applications of Large Language Models.](http://arxiv.org/abs/2307.10169) | 本文旨在总结大型语言模型领域的挑战和应用成功案例，帮助机器学习研究人员快速了解该领域的当前状态并提高效率。 |
| [^3] | [VITS : Variational Inference Thomson Sampling for contextual bandits.](http://arxiv.org/abs/2307.10167) | VITS是一种基于高斯变分推理的新算法，用于情境背离问题的汤普森抽样。它提供了强大的后验近似，计算效率高，并且在线性情境背离问题中达到与传统TS相同阶数的次线性遗憾上界。 |
| [^4] | [Rethinking Backdoor Attacks.](http://arxiv.org/abs/2307.10163) | 本文重新思考了后门攻击问题，发现在没有关于训练数据分布的结构信息的情况下，后门攻击与数据中自然产生的特征是不可区分的，因此难以检测。作者还重新审视现有的抵御后门攻击的方法，并探索了一种关于后门攻击的替代视角。 |
| [^5] | [Robust Driving Policy Learning with Guided Meta Reinforcement Learning.](http://arxiv.org/abs/2307.10160) | 本文提出了一种通过引导元元策略学习方法来实现社交车辆多样驾驶策略的有效方法，并使用训练策略增强自主驾驶策略的鲁棒性。 |
| [^6] | [Curvature-based Clustering on Graphs.](http://arxiv.org/abs/2307.10155) | 本研究通过利用图的几何性质，实现了基于离散Ricci曲率的聚类算法，可以识别图结构中的密集连接子结构，包括单成员社区和混合成员社区，以及在线图上的社区检测，并提供了实验证据支持。 |
| [^7] | [Benchmarking Potential Based Rewards for Learning Humanoid Locomotion.](http://arxiv.org/abs/2307.10142) | 本文对人形机器人使用标准形式的奖励塑造和潜在基于奖励的塑造进行了基准测试。在高维系统中，潜在基于奖励的塑造（PBRS）对于收敛速度的提升效果较小。 |
| [^8] | [Quarl: A Learning-Based Quantum Circuit Optimizer.](http://arxiv.org/abs/2307.10120) | Quarl是一种基于学习的量子电路优化器，通过引入新颖的神经架构和强化学习训练过程，解决了量子电路优化中的动作空间较大和非均匀状态表示的挑战。实验证明，Quarl在几乎所有基准电路上表现显著优于其他优化器，并且在速度上更快。 |
| [^9] | [Extended Graph Assessment Metrics for Graph Neural Networks.](http://arxiv.org/abs/2307.10112) | 本论文提出了扩展的图评估指标（GAMs），适用于回归任务和连续邻接矩阵。主要关注的两个GAMs是同质性和跨类邻域相似度（CCNS）。这些扩展的指标能够在图神经网络中评估图结构，提高模型性能。 |
| [^10] | [Gradient Sparsification For Masked Fine-Tuning of Transformers.](http://arxiv.org/abs/2307.10098) | 本研究提出了一种使用渐进稀疏化方法对预训练语言模型进行正则化，以改善微调性能。GradDrop及其变体通过在训练过程中随机屏蔽梯度，有效地进行梯度稀疏化。 |
| [^11] | [Revisiting invariances and introducing priors in Gromov-Wasserstein distances.](http://arxiv.org/abs/2307.10093) | 本文提出了一种新的基于最优传输的距离，增强的Gromov-Wasserstein，它在Gromov-Wasserstein距离的基础上引入了对变换刚度的控制和特征对齐，并应用于单细胞多组学和迁移学习任务中，展示了其在机器学习中的实用性和改进性能。 |
| [^12] | [Android in the Wild: A Large-Scale Dataset for Android Device Control.](http://arxiv.org/abs/2307.10088) | 这个论文提出了一个名为Android in the Wild (AITW)的大规模数据集，用于研究设备控制系统，该数据集包括人类示范的设备交互、自然语言指令和多种Android版本和设备类型。这个数据集提供了一个新的挑战，需要从视觉外观中推断用户界面中可用的操作。 |
| [^13] | [A Dual Formulation for Probabilistic Principal Component Analysis.](http://arxiv.org/abs/2307.10078) | 本文探讨了概率主成分分析在希尔伯特空间中的双重表述方法，并发展了适用于核方法的生成框架。作者通过实验证明了该方法能兼容核主成分分析，并在虚拟和真实数据集上进行了验证。 |
| [^14] | [Scalable Deep Learning for RNA Secondary Structure Prediction.](http://arxiv.org/abs/2307.10073) | RNA二级结构预测取得了显著进展，我们提出了RNAformer模型，通过使用轴向注意力和潜空间回收的方法，以及在潜空间直接建模邻接矩阵的架构和扩展模型规模来获得性能改善。该方法在TS0基准数据集上取得最先进的性能，并可以学习RNA折叠过程的生物物理模型。 |
| [^15] | [Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples.](http://arxiv.org/abs/2307.10062) | 本研究提出了一种无需源数据的框架来估计深度视觉模型在未标记的目标数据上的准确性。该方法利用伪标签和无源域自适应算法进行准确性估计，通过对目标模型的输入进行自适应对抗性扰动来处理错误伪标签的影响。 |
| [^16] | [Accurate deep learning sub-grid scale models for large eddy simulations.](http://arxiv.org/abs/2307.10060) | 本文提出了两个用于大涡模拟的子网格尺度模型，采用深度学习算法并与分析模型相比能够处理更复杂的非线性关系。其中一个模型融入了多个不变性，另一个只融入了伽利略不变性。 |
| [^17] | [Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization.](http://arxiv.org/abs/2307.10053) | 本文研究了非平滑非凸优化中随机次梯度方法的收敛性质，并提出了一种新的框架，证明了其在单时间尺度和双时间尺度情况下的全局收敛性，包括了多种已知的SGD类型方法。对于有限和形式的目标函数，证明了这些方法能够在随机选择的步长和初始点上找到Clarke稳定点。 |
| [^18] | [Contextual Reliability: When Different Features Matter in Different Contexts.](http://arxiv.org/abs/2307.10026) | 本论文提出了一个新的概念，即上下文可靠性，它考虑到在不同上下文中使用的“正确”特征可能会有所变化。作者提出的两阶段框架ENP能够首先识别给定上下文中要使用的相关特征，然后训练模型，从而提高性能的鲁棒性。 |
| [^19] | [Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK.](http://arxiv.org/abs/2307.10022) | 本文提出了一个关于欧盟和英国的国家层面历史民意调查数据集，填补了现有数据的空白，为研究人员揭示多模态数据与选民行为之间的复杂互动提供了机会。 |
| [^20] | [TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction.](http://arxiv.org/abs/2307.10003) | 本文提出了一种名为TbExplain的框架，它利用XAI技术和预训练的对象检测器，通过文本形式解释场景分类模型，并引入了一种新的方法来纠正预测和进行文本解释。 |
| [^21] | [Impact of Disentanglement on Pruning Neural Networks.](http://arxiv.org/abs/2307.09994) | 本研究通过使用Beta-VAE框架来迫使网络学习解缠缠绕表示，并研究了其对剪枝神经网络的影响。实验结果表明，在分类任务中，解缠缠绕表示对剪枝过程具有重要作用。 |
| [^22] | [UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing.](http://arxiv.org/abs/2307.09989) | UniMatch是一个统一的用户-物品匹配框架，通过使用一个模型同时进行物品推荐和用户定位，减少了商家购买多个机器学习模型的成本。利用多项分布建模用户-物品交互矩阵，并通过双向偏差校正的损失函数指导模型学习用户-物品联合概率，实现了优化。 |
| [^23] | [TinyTrain: Deep Neural Network Training at the Extreme Edge.](http://arxiv.org/abs/2307.09988) | TinyTrain是一种在设备上进行训练的方法，通过选择性更新模型的部分并处理数据稀缺问题，大大缩短了训练时间。通过任务自适应的稀疏更新方法，TinyTrain能够在高准确性的同时减小计算和内存占用，对未知任务表现出色。 |
| [^24] | [Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks.](http://arxiv.org/abs/2307.09977) | 本文提出了联合学习者推荐辅助的联邦客户端选择(LRef-FedCS)方法，以最小化最坏情况下参与者所产生的成本，并确保在分层物联网网络中联邦学习的长期公平性。 |
| [^25] | [Towards green AI-based software systems: an architecture-centric approach (GAISSA).](http://arxiv.org/abs/2307.09964) | 该论文介绍了GAISSA项目的愿景、目标和预期成果，该项目旨在为数据科学家和软件工程师提供基于架构的方法和工具，以开发绿色人工智能系统。 |
| [^26] | [XSkill: Cross Embodiment Skill Discovery.](http://arxiv.org/abs/2307.09955) | 本研究提出了XSkill，一种跨体现的技能发现框架，能够从无标签的人类和机器人操纵视频中纯粹地发现跨体现技能原型，并通过条件扩散策略将这些技能转移到机器人动作中，在未见任务中完成学习到的技能的组合。仿真和真实环境中的实验结果表明，这些发现的技能原型能够有效地促进技能转移和组合，从而构建出更通用和可扩展的模仿学习框架。 |
| [^27] | [Impatient Bandits: Optimizing for the Long-Term Without Delay.](http://arxiv.org/abs/2307.09943) | 这里是中文总结出的一句话要点：这篇论文研究了在推荐系统中提高用户长期满意度的问题，通过开发一个预测延迟奖励的模型和设计一个利用该模型的赌博算法来解决了通过测量短期代理奖励反映实际长期目标不完美的挑战。 |
| [^28] | [TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network.](http://arxiv.org/abs/2307.09942) | TREEMENT是一种采用个性化动态树状记忆网络的可解释患者-试验匹配模型，利用层次化临床本体知识和合格标准嵌入学习，提供准确而有解释性的患者-试验匹配。 |
| [^29] | [Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features.](http://arxiv.org/abs/2307.09933) | 本研究通过理论证明和算法提出，展示了在没有标签的情况下如何利用不稳定特征来提高分类器的性能。 |
| [^30] | [DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration.](http://arxiv.org/abs/2307.09931) | 本文提出了一个通用框架，用于创建具有表达力的跨模态描述符，通过在卷积神经网络的特征空间中用点积逼近现有的度量，实现了快速的可变形全局配准。我们的方法可在临床环境中直接使用，仅需替换相似性度量。 |
| [^31] | [TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations.](http://arxiv.org/abs/2307.09916) | TimeTuner是一个新颖的可视化分析框架，旨在帮助分析人员理解时间序列预测中模型行为与时间表示的关系，并解决自动化特征学习方法的局限性。 |
| [^32] | [Deep projection networks for learning time-homogeneous dynamical systems.](http://arxiv.org/abs/2307.09912) | 这篇论文介绍了一种利用深度投影网络学习时间齐次动力系统的有意义表示的方法。通过优化类似于经典相关分析的目标函数，避免了矩阵求逆的稳定性问题，并通过两种正则化方案进一步增强学习效果。 |
| [^33] | [Repeated Observations for Classification.](http://arxiv.org/abs/2307.09896) | 本文研究了非参数分类中的重复观测问题，提出了一些简单的分类规则，并证明了当重复观测次数$t\to\infty$时，条件错误概率以指数速度收敛。 |
| [^34] | [Symmetric Equilibrium Learning of VAEs.](http://arxiv.org/abs/2307.09883) | 本文提出了一种对称均衡学习方法，允许在只能通过采样获得数据和潜在分布的情况下学习VAEs。实验证明该方法与传统ELBO学习方法获得的模型相当，并具有广泛的应用性。 |
| [^35] | [Adversarial Likelihood Estimation with One-way Flows.](http://arxiv.org/abs/2307.09882) | 本文提出了一种通过单向流进行对抗性似然估计的方法，并使用重要性采样解决了Wasserstein GAN中分区函数有偏估计的问题。同时，通过最大化生成器的熵，提高了模式覆盖效果。这种方法通过计算生成样本的密度来实现对分区函数的无偏估计和生成器熵的计算。 |
| [^36] | [Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network.](http://arxiv.org/abs/2307.09866) | 该论文使用图神经网络和强化学习对城市基础设施相互依赖网络中的脆弱节点进行了准确建模和分析。 |
| [^37] | [Towards a population-informed approach to the definition of data-driven models for structural dynamics.](http://arxiv.org/abs/2307.09862) | 本论文提出了一种基于人群信息的方法，用于定义结构动力学的数据驱动模型，以解决数据稀缺问题。这种方法结合了物理基础方法和机器学习算法，通过从相似物理现象的人群中学习关系，构建可转移、可解释、值得信赖的模型。 |
| [^38] | [Reinforcement Learning for Credit Index Option Hedging.](http://arxiv.org/abs/2307.09844) | 本文研究了使用强化学习寻找信用指数期权的最佳对冲策略。通过应用信任区域波动率优化算法，证明所得到的对冲策略优于传统的Black & Scholes的Delta对冲策略。 |
| [^39] | [Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders.](http://arxiv.org/abs/2307.09836) | 本文提出了一种投影算法，能够在几乎线性时间内将矩阵投影到 $\ell_{1,\infty}$ 球面。该算法易于实现，能够在有限时间内收敛到精确解。同时，将该算法应用于自编码器训练中可以实现特征选择和权重的稀疏化。 |
| [^40] | [Deep Operator Network Approximation Rates for Lipschitz Operators.](http://arxiv.org/abs/2307.09835) | 该论文研究了一类神经深度算子网络(DON)在对Lipschitz算子的逼近速率界上的普适性，不需要G是全纯的。 |
| [^41] | [What do neural networks learn in image classification? A frequency shortcut perspective.](http://arxiv.org/abs/2307.09829) | 本研究通过在合成数据集和自然图像上的实验，发现神经网络在图像分类中倾向于找到简单的解决方案，并且在训练过程中首先学到的内容取决于最具有区分性的频率特征，这可以是低频或高频。同时，研究也提出了一种度量标准和方法来识别频率快捷路径，并验证了其在不同类型的图像上的有效性。 |
| [^42] | [Multi-modal Learning based Prediction for Disease.](http://arxiv.org/abs/2307.09823) | 本文提出了一种基于多模态学习的NAFLD预测方法，结合了综合临床数据集和智能方法，为NAFLD的非侵入性诊断提供了一个有前景的解决方案。 |
| [^43] | [Deep unrolling Shrinkage Network for Dynamic MR imaging.](http://arxiv.org/abs/2307.09818) | 本文提出了一种新的运算符AST，来学习每个通道的阈值，在动态磁共振成像方面取得了更好的性能。 |
| [^44] | [Manifold Learning with Sparse Regularised Optimal Transport.](http://arxiv.org/abs/2307.09816) | 这篇论文介绍了一种利用稀疏正则最优传输进行流形学习的方法，该方法构建了一个稀疏自适应的亲和矩阵，并在连续极限下与拉普拉斯型算子一致。 |
| [^45] | [GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence.](http://arxiv.org/abs/2307.09810) | 提出了一个新的迭代框架GenKL，通过$(\alpha, \beta)$-广义KL散度来解决Web图像中的标签模糊和非符合实例的问题。 |
| [^46] | [Graph Federated Learning Based on the Decentralized Framework.](http://arxiv.org/abs/2307.09801) | 这项研究提出了一种基于分散框架的图形联邦学习方法，通过确定节点之间的置信度并聚合梯度信息来改进联邦学习的准确性和泛化能力，同时解决了中央服务器故障和网络拓扑扩展性差的问题。 |
| [^47] | [Probabilistic Forecasting with Coherent Aggregation.](http://arxiv.org/abs/2307.09797) | 该论文提出了一种新的模型，利用因子模型结构来产生遵守层次结构的概率预测。模型利用卷积神经网络生成参数，并通过优化样本损失函数实现预测优化。 |
| [^48] | [Forecasting Early with Meta Learning.](http://arxiv.org/abs/2307.09796) | 本文介绍了一种利用元学习方法预测早期时间序列的方法，通过对目标数据集进行对抗学习和利用额外样本增强时间序列，在多个数据集中进行元学习，并在预测性能上优于单任务学习、联合学习、多任务学习和经典预测基线方法。 |
| [^49] | [From West to East: Who can understand the music of the others better?.](http://arxiv.org/abs/2307.09795) | 这项研究探讨了基于迁移学习方法的音频嵌入模型在不同音乐文化和风格中的适用性，并提出了关于跨文化音乐理解的研究问题。 |
| [^50] | [A Note on Hardness of Computing Recursive Teaching Dimension.](http://arxiv.org/abs/2307.09792) | 本文研究了计算递归教学维度（RTD）问题的难度，证明了在指数时间假设（ETH）下，该问题的计算时间为$n^{\Omega(\log n)}$，与暴力算法的运行时间相匹配。 |
| [^51] | [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats.](http://arxiv.org/abs/2307.09782) | ZeroQuant-FP通过使用浮点格式进行LLMs训练后量化，解决了在大型语言模型中平衡计算效率和保持模型质量的挑战，并发现FP8激活优于INT8，并且FP4权重表现与INT4相当甚至更优。 |
| [^52] | [Text2Layer: Layered Image Generation using Latent Diffusion Model.](http://arxiv.org/abs/2307.09781) | 该论文提出了一种使用潜在扩散模型进行分层图像生成的方法，使得在生成高质量图像的同时，实现更好的合成工作流程和更高质量的图层遮罩生成。 |
| [^53] | [Beyond Single-Feature Importance with ICECREAM.](http://arxiv.org/abs/2307.09779) | 提出了一种名为ICECREAM的新方法，超越了单一特征重要性的统计方法。该方法利用信息论量化衡量了变量联合对目标变量分布的影响，能够确定一个特定结果所需的关键因素集合。 |
| [^54] | [A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices.](http://arxiv.org/abs/2307.09771) | 该论文提出了一种新的时空变分量子电路(ST-VQC)用于在NISQ设备上实现深度学习，通过整合非线性特征提高学习模型对噪声的鲁棒性，并通过独特的编码量子子电路和逐层计算的量子子电路实现基于时间的深度学习。 |
| [^55] | [Sig-Splines: universal approximation and convex calibration of time series generative models.](http://arxiv.org/abs/2307.09767) | 该论文提出了一种新颖的时间序列生成模型，通过将线性变换和签名变换作为传统神经网络的替代，既实现了神经网络的通用性，又引入了模型参数的凸性。 |
| [^56] | [Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition.](http://arxiv.org/abs/2307.09762) | 该论文提出了一种算法框架，通过将模式识别和随机滤波理论的技术结合起来，强化了基于POD的反应扩散复杂网络模型简化技术，在受扰动输入的情况下提高了代理模型的准确性。 |
| [^57] | [Constructing Extreme Learning Machines with zero Spectral Bias.](http://arxiv.org/abs/2307.09759) | 本研究验证了极限学习机（ELM）不存在谱偏差（SB），并通过实施傅立叶特征嵌入的变体消除了谱偏差，并且成功将ELM应用于高频分辨率至关重要的问题（如PINNs）。 |
| [^58] | [Improved Distribution Matching for Dataset Condensation.](http://arxiv.org/abs/2307.09742) | 本文提出了一种基于分布匹配的新型数据集压缩方法，通过解决朴素分布匹配的两个缺点，提出了三种新技术，实现了更高效和有前景的数据集压缩。 |
| [^59] | [RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap.](http://arxiv.org/abs/2307.09706) | RaTE是一种无标签的自动分类评分方法，它通过大型预训练的语言模型实现可重复的自动分类评估。结果表明，RaTE与人类评判具有较高的相关性，并且人为降低分类法会导致RaTE评分下降。 |
| [^60] | [Efficient Guided Generation for LLMs.](http://arxiv.org/abs/2307.09702) | 本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。 |
| [^61] | [STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization.](http://arxiv.org/abs/2307.09692) | 本文提出了STRAPPER方法，通过自训练增强和同伴正则化实现基于偏好的强化学习。与其他方法不同的是，作者发现基于偏好的强化学习中存在相似性陷阱现象，即相似的片段对可能会存在截然相反的偏好，对一致性正则化造成影响。 |
| [^62] | [Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach.](http://arxiv.org/abs/2307.09691) | 该论文提出了一个基于DRL的双时间尺度方案，旨在通过联合优化服务缓存、协作卸载和计算通信资源分配来提高MEC系统中的长期服务质量并降低缓存切换成本。 |
| [^63] | [Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation.](http://arxiv.org/abs/2307.09688) | Amazon-M2是一个多语言多区域购物会话数据集，可以增强个性化推荐和理解用户偏好能力。 |
| [^64] | [Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction.](http://arxiv.org/abs/2307.09672) | 本文研究了ReLU层在闭球和非负部分上的可逆性，并提供了使用凸几何视角和框架理论中的对偶概念来验证和重构ReLU层的计算方法。 |
| [^65] | [JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting.](http://arxiv.org/abs/2307.09670) | JAZZVAR是一份包含爵士乐标准曲独奏中变奏的数据集，为音乐重绘任务提供了基础。 |
| [^66] | [Towards A Unified Agent with Foundation Models.](http://arxiv.org/abs/2307.09668) | 本文研究如何在强化学习智能体中嵌入和利用语言模型和视觉语言模型的能力，设计了一个以语言为核心推理工具的框架，并在稀疏奖励的机器人操作环境中测试了该方法。结果显示，该方法在探索效率和数据复用方面具有显著性能提升，并展示了如何通过复用学到的技能解决新任务。 |
| [^67] | [Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers.](http://arxiv.org/abs/2307.09665) | 这项研究通过使用动态图转换器预测研究社区中的技术专长和能力演进，从而提高了全球安全和核不扩散等领域的预测能力。 |
| [^68] | [Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization.](http://arxiv.org/abs/2307.09661) | 本论文提出了一种基于机器学习的减阶建模方法，称为BO-ML-ROM，用于减少计算时间和成本，并同时实现引导波传播的不确定性量化。 |
| [^69] | [Neural Priority Queues for Graph Neural Networks.](http://arxiv.org/abs/2307.09660) | 本文提出了神经优先队列，这是一种用于图神经网络的可微分的模块，并通过在不同数据集上的实验结果验证了其在算法推理和捕捉长距离交互方面的有效性。 |
| [^70] | [HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning.](http://arxiv.org/abs/2307.09653) | HAT-CL是一个基于任务的硬注意力PyTorch库，以提供对连续学习中的灾难性遗忘现象的解决方案。它通过改善HAT的可用性和兼容性问题，并提供对现有网络复用的支持，实现了对PyTorch模块的自动化梯度操作和转换。此外，HAT-CL还引入了新颖的掩码操作技术。 |
| [^71] | [Application of BadNets in Spam Filters.](http://arxiv.org/abs/2307.09649) | 本研究利用BadNets设计了一种后门攻击，揭示了垃圾邮件过滤器中机器学习模型的潜在漏洞，强调了评估和改进的重要性。 |
| [^72] | [Promoting Exploration in Memory-Augmented Adam using Critical Momenta.](http://arxiv.org/abs/2307.09638) | 本研究提出了一种记忆增强型Adam方法，通过使用关键动量项的缓冲区来促进对更平坦最小值的探索。实验证明，该方法在标准的监督语言建模和图像分类任务中提高了几种Adam变体的性能。 |
| [^73] | [Deep Reinforcement Learning for ESG financial portfolio management.](http://arxiv.org/abs/2307.09631) | 本文研究了深度强化学习在ESG金融投资组合管理中的应用，结果显示在按公司ESG评分调整回报的市场中，DRL代理的表现优于标准市场设置。 |
| [^74] | [Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning.](http://arxiv.org/abs/2307.09619) | Dataset Grouper是一个库，用于创建大规模群组结构化数据集，并克服了内存限制、提供了灵活性，并且与不同的软件框架兼容。实验证明它可以实现比以前更大规模的联邦语言建模模拟。 |
| [^75] | [Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey.](http://arxiv.org/abs/2307.09615) | 本文对可解释的深度学习在神经影像学中的应用进行了全面调查，深度学习模型的易解释性仍然存在挑战，近年来的研究主要集中在如何理解模型决策的直觉，还需探索如何验证解释性方法的可靠性。 |
| [^76] | [Multi-view self-supervised learning for multivariate variable-channel time series.](http://arxiv.org/abs/2307.09614) | 本论文提出了一种多视角自监督学习方法，用于处理多变量通道时间序列数据，在不同数据集之间进行迁移学习。通过预训练和微调，结合传递神经网络和TS2Vec损失，该方法在大多数设置下表现优于其他方法。 |
| [^77] | [Retrieving Continuous Time Event Sequences using Neural Temporal Point Processes with Learnable Hashing.](http://arxiv.org/abs/2307.09613) | 提出了一个名为NeuroSeqRet的模型，用于解决大规模检索连续时间事件序列的任务。通过使用可学习的哈希和神经时间点过程，该模型能够为输入的查询序列返回一个相关序列的排序列表。 |
| [^78] | [Sequential Monte Carlo Learning for Time Series Structure Discovery.](http://arxiv.org/abs/2307.09607) | 本文提出了一种顺序蒙特卡洛学习的方法，用于自动发现复杂时间序列数据的准确模型。在实验中显示，该方法相对于之前的方法，具有较快的运行速度并能够发现合理的模型结构。 |
| [^79] | [A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation.](http://arxiv.org/abs/2307.09602) | 这项工作提出了一种将神经网络转化为样条表示的新算法，它不再需要凸多边形和分段线性网络操作符的限制，并且可以在整个网络上执行。这项工作不仅填补了神经网络和逼近理论之间的差距，还使得网络特征图的可视化成为可能。 |
| [^80] | [Gradient strikes back: How filtering out high frequencies improves explanations.](http://arxiv.org/abs/2307.09591) | 本研究发现，基于预测的属性方法与基于梯度的方法产生的属性图具有不同的高频内容，滤除高频率可以提高解释性。 |
| [^81] | [Causal Influences over Social Learning Networks.](http://arxiv.org/abs/2307.09575) | 本论文研究了社交学习网络中代理之间的因果影响，并提出了一种算法来评估整体影响力和发现高度有影响力的代理。 |
| [^82] | [Self-Compatibility: Evaluating Causal Discovery without Ground Truth.](http://arxiv.org/abs/2307.09552) | 本论文提出了一种在没有基准数据的情况下评估因果发现方法的新方法，通过在不同变量子集上学习的因果图之间的兼容性检测，来伪证因果关系的推断正确性。 |
| [^83] | [The semantic landscape paradigm for neural networks.](http://arxiv.org/abs/2307.09550) | 本研究引入了语义景观范式，用于描述神经网络的训练动力学，将其视为在图上的路径，图的节点对应于网络学习表示中的新算法。这种抽象使我们能够以统计物理学中研究过的问题来解释各种神经网络现象。 |
| [^84] | [DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI.](http://arxiv.org/abs/2307.09547) | DreaMR 是基于扩散的反事实解释方法，用于高特异性、合理性和一致性的解释功能性磁共振成像（fMRI）数据。 |
| [^85] | [Can Neural Network Memorization Be Localized?.](http://arxiv.org/abs/2307.09542) | 本文研究了深度神经网络中记忆化的局部化现象，通过实验证据表明，记忆化并不局限于个别层，而是在模型的多个层中的一小部分神经元中发生。 |
| [^86] | [Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing.](http://arxiv.org/abs/2307.09494) | 这篇论文提出了一个解释引导的联邦学习方案，通过利用可解释的人工智能策略产生透明和无偏的深度神经网络，从而确保可靠的预测。 |
| [^87] | [PLiNIO: A User-Friendly Library of Gradient-based Methods for Complexity-aware DNN Optimization.](http://arxiv.org/abs/2307.09488) | PLiNIO是一个用户友好的基于梯度优化的深度神经网络优化方法库，可以在边缘设备上实现高精度和高效的DNNs，并显著减少内存占用。 |
| [^88] | [Submodular Maximization under the Intersection of Matroid and Knapsack Constraints.](http://arxiv.org/abs/2307.09487) | 本文提出了一种在$k$-matroid约束和$m$-knapsack约束的交集下进行子模求解最大化问题的算法SPROUT，并引入了部分枚举和平滑技术以提高其效率，实现了更好的多项式时间逼近保证。 |
| [^89] | [MolFM: A Multimodal Molecular Foundation Model.](http://arxiv.org/abs/2307.09484) | MolFM是一种多模态分子基础模型，通过跨模态关注实现了分子结构、文本和知识图谱之间的联合表示学习。 |
| [^90] | [Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla.](http://arxiv.org/abs/2307.09458) | 本论文研究了电路分析在最先进的语言模型中的可扩展性，通过对70B毛丫鼠模型进行多项选择题的分析，发现现有的逻辑层归因和激活修补技术具有可扩展性，并进一步研究了注意力头的语义特征。 |
| [^91] | [A benchmark of categorical encoders for binary classification.](http://arxiv.org/abs/2307.09191) | 本研究是迄今为止最全面的分类编码器基准研究，通过对来自不同家族的32种编码器配置进行广泛评估，以及36种实验因素和50个数据集的组合，展示了数据集选择、实验因素和聚合策略对基准研究结论的深远影响。 |
| [^92] | [Towards the Sparseness of Projection Head in Self-Supervised Learning.](http://arxiv.org/abs/2307.08913) | 该论文研究了自监督学习中投影头的稀疏性，发现通过在投影子空间中执行对比损失可以提升表示的质量，建议只有一部分特征是必要的，而稀疏的投影头可以增强模型的泛化性能。 |
| [^93] | [Retentive Network: A Successor to Transformer for Large Language Models.](http://arxiv.org/abs/2307.08621) | Retentive Network（RetNet）作为大型语言模型的基础架构，实现了训练并行、低成本推理和良好的性能。通过并行、循环和分块循环三种计算范式，RetNet具有训练并行化、低成本推理和高效的长序列建模的特点。 |
| [^94] | [M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization.](http://arxiv.org/abs/2307.08347) | M-FLAG是一种新颖的医学视觉语言预训练方法，通过利用冻结语言模型和引入正交损失函数来优化潜空间几何关系。在医学影像分类、分割和目标检测任务上，M-FLAG在性能上显著优于现有方法，并且减少了78%的参数量。 |
| [^95] | [Towards Understanding Adversarial Transferability From Surrogate Training.](http://arxiv.org/abs/2307.07873) | 本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。 |
| [^96] | [Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0.](http://arxiv.org/abs/2307.06975) | 本论文提出了一种基于神经符号的强化去噪扩散概率模型，用于在工业4.0中实时预测异常。该方法集成了工业本体论，为智能制造提供了形式化知识，并且通过随机傅里叶特征提取扩散模型，实现了在嵌入式系统中直接集成。这种方法在以前从未被探索过。 |
| [^97] | [IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation.](http://arxiv.org/abs/2307.06698) | IntelliGraphs是一组新的知识图谱数据集，用于评估知识图谱生成。其中包含具有逻辑规则表达的语义的子图，用于评估子图推断的模型。 |
| [^98] | [Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks.](http://arxiv.org/abs/2307.06608) | 本文将对抗攻击重新设定为下游任务，通过生成图像噪声来满足新兴趋势，并将基础模型引入作为代理模型。虽然基础模型的表现不佳，但通过在特征空间中进行分析，我们发现缺乏对应的特征。 |
| [^99] | [Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization.](http://arxiv.org/abs/2307.06385) | 本文解决了弱监督条件下的音视频事件定位问题，通过使用基础模型在训练数据上以更细的时间分辨率估计标签，并提出辅助目标来处理合成视频的分布外特性。 |
| [^100] | [Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation.](http://arxiv.org/abs/2307.06333) | 提出了一种交互式框架，通过从用户那里直接获取反馈来识别个性化的无关紧要的概念，从而进行数据增强并获得适应个性化用户目标的政策。 |
| [^101] | [Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation.](http://arxiv.org/abs/2307.04988) | 该研究评估了六种基准因果发现方法和一种新提出的基于 GFlowNets 的方法在治疗效果估计任务中的表现，并发现 GFlowNets 具有捕捉各种有用和多样的平均处理效应模式的能力。 |
| [^102] | [CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction.](http://arxiv.org/abs/2307.04838) | 本文研究了使用CLIP模型提高视觉关系预测的可能性。通过在UVTransE框架中采用基于CLIP的表示方法，以及引入对比训练策略，我们提出了CREPE模型，简化了现有复杂的图形模型，取得了良好的效果。 |
| [^103] | [Multimodal brain age estimation using interpretable adaptive population-graph learning.](http://arxiv.org/abs/2307.04639) | 这项研究提出了一种利用自适应人群图学习的多模态脑龄估计框架，通过优化人群图结构，提高了图卷积网络的性能和准确性。 |
| [^104] | [Solvent: A Framework for Protein Folding.](http://arxiv.org/abs/2307.04603) | Solvent是一个用于蛋白质折叠的统一研究框架，支持最新模型重要组件的实现和基准测试，并提供对蛋白质结构建模领域的有用见解。 |
| [^105] | [Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks.](http://arxiv.org/abs/2307.04228) | 本论文提出了一种高效的贝叶斯行程时间层析成像方法，利用敏感性信息的多项式混沌展开和深度生成网络，以处理地质复杂性先验的挑战。 |
| [^106] | [BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits.](http://arxiv.org/abs/2307.03587) | BOF-UCB是一种用于非平稳环境下的背景线性赌博机的贝叶斯优化频率算法，其结合了贝叶斯和频率学派原则，提高了在动态环境中的性能。它利用贝叶斯更新推断后验分布，并使用频率学派方法计算上界信心界以平衡探索和开发。实验证明，BOF-UCB优于现有方法，是非平稳环境中顺序决策的有前途的解决方案。 |
| [^107] | [Distilling Large Vision-Language Model with Out-of-Distribution Generalizability.](http://arxiv.org/abs/2307.03135) | 本文研究了针对大型视觉语言模型的模型压缩方法，将教师模型的视觉表示压缩到学生模型中。研究重点在于超出分布可泛化的问题，并提出了两个原则来增强学生模型的性能。 |
| [^108] | [LongNet: Scaling Transformers to 1,000,000,000 Tokens.](http://arxiv.org/abs/2307.02486) | LongNet是一种可以扩展到10亿个标记的Transformer变体，通过扩张注意力解决了序列长度受限的问题，具有线性计算复杂度和对数依赖关系，可以作为分布式训练器使用并无缝集成到现有的Transformer优化中。实验证明LongNet在长序列和短序列上性能强大。 |
| [^109] | [SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation.](http://arxiv.org/abs/2307.01646) | 本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。 |
| [^110] | [Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2307.01158) | 本论文提出了一种方法来建立语义有意义、人类可解释的信念，并将其应用于多智能体强化学习中。研究发现，通过预测其他智能体的信念来作为内在奖励信号，可以在多智能体环境中产生良好的效果。 |
| [^111] | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.](http://arxiv.org/abs/2306.14048) | 本文提出了一种通过预测文本中的热门元素来减少GPU内存消耗的方法，在实验中表现良好。 |
| [^112] | [Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?.](http://arxiv.org/abs/2306.13197) | 在Gradient-based Attribution Methods中，使用Pre Softmax分数或Post Softmax分数的梯度的选择有各自的优缺点，需要根据具体情况进行权衡。 |
| [^113] | [The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions.](http://arxiv.org/abs/2306.10404) | 本文提出了一个高维RL模型，推导出该模型的典型动力学为一组闭式ODE方程组，并通过实验与神经RL代理进行了比较，结果表明该模型捕捉了现实世界RL的关键特征。 |
| [^114] | [Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions.](http://arxiv.org/abs/2306.09618) | 本研究发现在高维生成模型测度中使用的精度和召回指标存在不对称性，可能会导致误导性结论。我们提出了一些方法来修正这种错误。 |
| [^115] | [Multi-class Graph Clustering via Approximated Effective $p$-Resistance.](http://arxiv.org/abs/2306.08617) | 本文提出了一种近似计算有效$p$-阻抗并将其应用于多类图聚类，该方法可以通过参数$p$偏向于具有高内部连通性或者更小的簇内顶点之间的最短路径距离的聚类。 |
| [^116] | [Contagion Effect Estimation Using Proximal Embeddings.](http://arxiv.org/abs/2306.02479) | 本文介绍了一种使用近邻嵌入方法来估计社交网络中的传染效应。我们提出了ProEmb框架，通过将变分自动编码器（VAEs）和对抗网络集成在一起，生成高维代理变量的平衡低维表示，并解决了传染效应估计中的偏差问题。 |
| [^117] | [A Conceptual Model for End-to-End Causal Discovery in Knowledge Tracing.](http://arxiv.org/abs/2305.16165) | 本文提出了一个概念模型来解决知识追踪中的因果发现问题，通过学生反应数据找到不同技能之间的潜在因果关系。该模型引入了一个因果门循环单元模块，并使用了可学习的置换矩阵和下三角矩阵来表示因果顺序和因果结构。在NeurIPS 2022挑战赛中取得了优秀的成绩。 |
| [^118] | [On sampling determinantal and Pfaffian point processes on a quantum computer.](http://arxiv.org/abs/2305.15851) | 本文总结了在量子计算机上采样确定性行列式和Pfaffian点过程的状态及其优化方式。 |
| [^119] | [Off-Policy Average Reward Actor-Critic with Deterministic Policy Search.](http://arxiv.org/abs/2305.12239) | 本文介绍了带有确定性策略搜索的离策略平均回报行动者-评论家算法，并提出了基于策略和离策略的确定性策略梯度定理。使用这些定理，本文还提出了一种平均回报离策略深度确定性策略梯度算法（ARO-DDPG）。该算法在渐近收敛性分析和有限时间分析中展示了较好的性能，并获得了$\epsilon$-最优稳定策略。 |
| [^120] | [DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images.](http://arxiv.org/abs/2305.09946) | 提出了一种DeepMSS模型，采用新颖的Segmentated-to-Survival（STS）框架，使用多模态渐进聚合网络（MMPAN）来探索肿瘤内外的预后信息，并通过自我注意力机制增强的深度生存模型进行生存预测，取得了在两个公共PET/CT图像数据集上优于几种最先进的方法的结果。 |
| [^121] | [Probabilistic Distance-Based Outlier Detection.](http://arxiv.org/abs/2305.09446) | 本文提出了一种将距离法异常检测分数转化为可解释的概率估计的通用方法，该方法使用与其他数据点的距离建模距离概率分布，将距离法异常检测分数转换为异常概率，提高了正常点和异常点之间的对比度，而不会影响检测性能。 |
| [^122] | [CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images.](http://arxiv.org/abs/2305.09211) | CB-HVTNet 提出了一种 Channel Boosted Hybrid Vision Transformer 网络，利用迁移学习生成增强通道，并结合使用 Transformers 和 CNN，在组织病理学图像中高效准确地评估淋巴细胞。 |
| [^123] | [Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus.](http://arxiv.org/abs/2305.07898) | 本文提出了一种基于哈蒙莫特 Hessian 一致性的全分布式牛顿型优化算法 Network-GIANT，将梯度跟踪和牛顿型迭代算法相结合，经证明对严格凸和光滑损失函数有半全局和指数收敛到精确解的保证，实验证明 Network-GIANT 优于其他分布式学习算法（如 Network-DANE 和 Newton-Raphson Consensus）的收敛性能。 |
| [^124] | [Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation.](http://arxiv.org/abs/2305.00909) | 提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。 |
| [^125] | [Uncovering Bias in Personal Informatics.](http://arxiv.org/abs/2303.15592) | 该论文是第一个对个人信息学系统中的偏见进行实证和分析研究的工作，研究包括原始数据和整个机器学习周期中的偏见，并找出其中的实践和道德影响。 |
| [^126] | [Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing.](http://arxiv.org/abs/2303.15585) | 本文通过对IMWUT期刊上过去五年发表的论文进行系统回顾，发现UbiComp社区在算法公平方面的进展滞后，存在敏感属性偏差导致的歧视性结果，需要探索报告数据集的信息以解决这些偏差。 |
| [^127] | [Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling.](http://arxiv.org/abs/2303.11103) | Sionna RT是一个GPU加速的开源库，它集成了可微分的光线追踪器，可以用于模拟无线电波传播。这个功能使得可以计算与多个系统和环境参数有关的量的梯度，对于诸如学习无线电材料和优化发射机方向等应用具有重要价值。同时，可微分光线追踪对于新颖的研究方向如数字孪生也是一个关键的推动者。 |
| [^128] | [Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction.](http://arxiv.org/abs/2303.09340) | 本文提出了一种基于深度卷积神经网络的伪影降噪方法，用于改善稀疏视图下自动出血检测的图像质量，并证明其能够与完全采样的图像进行同等精确度的分类和检测。 |
| [^129] | [Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss.](http://arxiv.org/abs/2303.03027) | 本文使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型，并在有限秩矩阵空间内表征关键点和最小化问题，最终确定了梯度下降算法的收敛性。 |
| [^130] | [Graph Positional Encoding via Random Feature Propagation.](http://arxiv.org/abs/2303.02918) | 本文提出了一种新的位置编码方案，Random Feature Propagation (RFP)，通过串联迭代算法的中间步骤以计算传播矩阵的主特征向量，从而改进了现有的随机特征和谱位置编码方案。 |
| [^131] | [Generalization Error Bounds for Noisy, Iterative Algorithms via Maximal Leakage.](http://arxiv.org/abs/2302.14518) | 通过最大泄露分析噪声迭代算法的泛化误差界限，证明了如果更新函数在L2-范数下有界且加性噪声为各向同性高斯噪声，则可以得到一个半封闭形式下的最大泄露上界，同时展示了更新函数的假设如何影响噪声的最优选择。 |
| [^132] | [Robust Field-level Likelihood-free Inference with Galaxies.](http://arxiv.org/abs/2302.14101) | 通过训练图神经网络，我们实现了使用星系目录进行场地级别的无相似度推断，可以在不受天文物理学和子网格模型变化影响的情况下，以高精度推断出Ωm的值。 |
| [^133] | [CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design.](http://arxiv.org/abs/2302.14015) | CO-BED是一个通用的、与模型无关的框架，用于通过贝叶斯实验设计的信息理论来进行上下文优化。它采用黑箱变分方法同时估计和优化设计，可以适应离散动作，并在多个实验中展示出竞争性能。 |
| [^134] | [AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving.](http://arxiv.org/abs/2302.11665) | AlpaServe是一种新颖的服务系统，它利用模型并行和统计复用，在提供多个模型服务时降低延迟，提高处理速率和突发负载处理能力。 |
| [^135] | [The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus.](http://arxiv.org/abs/2302.07265) | 这项研究解决了可解释人工智能领域中关于在缺乏真实解释标签的情况下如何可靠估算解释方法质量的问题。通过对不同质量估计器进行元评估，利用MetaQuantus框架分析了估计器的韧性和反应特征，从而帮助实践者选择最佳的解释方法。 |
| [^136] | [ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction.](http://arxiv.org/abs/2302.05783) | 本文提出了一种基于对比学习的框架ConCerNet，用于提高DNN动力学建模的可靠性，实现对系统不变量的自动捕捉和保留，经实验证明其性能优于传统神经网络方法。 |
| [^137] | [Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples.](http://arxiv.org/abs/2302.05086) | 本文通过使替代模型更贝叶斯化，提出了一种攻击贝叶斯模型以实现理想的对抗性样本可迁移性的方法，通过实验证明了该方法的有效性，并在常见基准数据集上优于最新方法。 |
| [^138] | [Sequential Kernelized Independence Testing.](http://arxiv.org/abs/2212.07383) | 该论文介绍了顺序核独立性测试的方法，以解决传统批量测试在流数据上的问题，实现了根据任务复杂性自适应调整样本大小，并在收集新数据后持续监测和控制误报率。 |
| [^139] | [Can In-context Learners Learn a Reasoning Concept from Demonstrations?.](http://arxiv.org/abs/2212.01692) | 本文介绍了一种概念性少样本学习方法，以帮助在场学习者学习新技能。通过选择与预测示例共享可能信息的演示，这个方法可以在模型记忆独立的情况下区分模型的在场学习能力。 |
| [^140] | [An exponentially-growing family of universal quantum circuits.](http://arxiv.org/abs/2212.00736) | 该论文介绍了两种新的指数增长的量子机器学习体系结构，能够解决量子机器学习中的贫瘠高原问题，提高了量子编码的表达能力。 |
| [^141] | [Revisiting Softmax for Uncertainty Approximation in Text Classification.](http://arxiv.org/abs/2210.14037) | 本研究重新审视了Softmax在文本分类中的不确定性近似方法，并比较了基于MC Dropout的方法。实证分析发现，尽管MC dropout产生了最好的不确定性近似，但使用softmax也能产生相对准确的结果。 |
| [^142] | [Active Learning for Single Neuron Models with Lipschitz Non-Linearities.](http://arxiv.org/abs/2210.13601) | 该论文提出一种针对具有利普希茨非线性的单个神经元模型的主动学习策略，该策略在敌对标签噪声下拟合线性函数，并在逼近保证方面具有强有力的可证明性能。 |
| [^143] | [SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems.](http://arxiv.org/abs/2210.12547) | 该论文介绍了一种名为SurCo的方法，通过学习线性代理成本，将组合非线性优化问题转化为线性问题，并通过结合梯度方法和线性组合优化的结构提供了高效解决方案。 |
| [^144] | [Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics.](http://arxiv.org/abs/2210.06226) | 本文提出了VR-IWAE下界，该下界是IWAE下界的推广，采用无偏梯度估计器能够实现与VR下界相同的随机梯度下降过程，对该下界进行了理论分析，揭示了其优势和不足，并通过示例验证了理论观点。 |
| [^145] | [Prediction intervals for neural network models using weighted asymmetric loss functions.](http://arxiv.org/abs/2210.04318) | 本论文提出了一种使用加权不对称损失函数的方法，生成可靠的预测区间，适用于复杂的机器学习情境，可扩展为参数化函数的PI预测。 |
| [^146] | [Cooperation in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders.](http://arxiv.org/abs/2209.15514) | 本研究展示了在变分自编码器中添加混合成分的好处，并证明了混合成分的增加能够提高其在图像和单细胞数据集上的潜在表示能力。这表明使用混合VAE是获取更灵活变分逼近的标准方法。 |
| [^147] | [Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning.](http://arxiv.org/abs/2209.10901) | 本文研究了使用自监督方法预训练视觉转换器，并通过添加时间顺序验证任务来捕捉观测之间的时间关系。实验结果表明，这些方法在学习有用的表示和提高强化学习数据效率方面都很有效。 |
| [^148] | [Value Summation: A Novel Scoring Function for MPC-based Model-based Reinforcement Learning.](http://arxiv.org/abs/2209.08169) | 本文提出了一种新的评分函数，用于解决使用奖励函数评分轨迹时的偏差问题，该方法通过利用价值的折扣和求和来提高MPC-based强化学习的学习效率，并在实验中表现出优于当前最先进算法的结果。 |
| [^149] | [The Value of Out-of-Distribution Data.](http://arxiv.org/abs/2208.10967) | 不同分布的数据可以对任务的泛化误差产生非单调的影响，使用少量不同分布的数据进行训练是有价值的。 |
| [^150] | [Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success.](http://arxiv.org/abs/2208.07734) | 这项研究通过广泛的实验，证明数据增强与异常生成机制之间的对齐是自监督学习在无监督异常检测中取得成功的关键，并且在缺乏对齐时，自监督学习甚至可能降低准确性。 |
| [^151] | [Trustworthy Recommender Systems.](http://arxiv.org/abs/2208.06265) | 可信度推荐系统研究已经从以准确性为导向转变为以透明、公正、稳健性为特点的可信度推荐系统。本文提供了可信度推荐系统领域的文献综述和讨论。 |
| [^152] | [Primal Estimated Subgradient Solver for SVM for Imbalanced Classification.](http://arxiv.org/abs/2206.09311) | 本研究旨在实现对不平衡数据集的分类，并评估成本敏感的PEGASOS SVM的性能，同时将核函数纳入SVM中扩展Ding的工作。 |
| [^153] | [Alternately Optimized Graph Neural Networks.](http://arxiv.org/abs/2206.03638) | 本文提出了一种新的优化框架用于解决图上的半监督学习，通过交替优化算法显著提高了计算和内存效率，并在实验证明相比现有技术可以达到相当或更好的性能。 |
| [^154] | [Meta-Learning Parameterized Skills.](http://arxiv.org/abs/2206.03597) | 提出了一种元学习参数化技能的算法，通过学习可转移的参数化技能并将其综合到新的动作空间中，实现了长期任务的高效学习。实证研究表明，该算法能够使智能体在难以解决的长期任务中取得成功。 |
| [^155] | [Learning from time-dependent streaming data with online stochastic algorithms.](http://arxiv.org/abs/2205.12549) | 本文研究了处理时间相关的流式数据的在线随机算法，并通过非渐进分析建立了新颖的启发式算法，加速收敛。实验证明时间变化的小批量SGD方法可以打破依赖结构，有偏倚的SGD方法具有与无偏倚方法相当的性能，并且使用Polyak-Ruppert平均化方法能够加快随机优化算法的收敛。 |
| [^156] | [Tackling Provably Hard Representative Selection via Graph Neural Networks.](http://arxiv.org/abs/2205.10403) | 本文研究了属性图的代表性选择问题，并证明了在缺乏图结构的情况下，RS的学习困难性。同时，发现当存在或构建了同质图结构时，可以通过适当的建模方法有效地解决这一困难问题。 |
| [^157] | [ConceptEvo: Interpreting Concept Evolution in Deep Learning Training.](http://arxiv.org/abs/2203.16475) | ConceptEvo是一个统一的深度神经网络解释框架，可以在训练过程中揭示概念的产生和演变，并通过人机评估和实验证明其发现对模型和预测具有重要意义。 |
| [^158] | [Non-linear Embeddings in Hilbert Simplex Geometry.](http://arxiv.org/abs/2203.11434) | 本文研究了将图的距离矩阵嵌入到Hilbert Simplex几何中的表示能力，发现该几何结构在嵌入任务中与其他几何结构相媲美，同时具有快速和数值稳健的特点。 |
| [^159] | [Finite-Time Analysis of Natural Actor-Critic for POMDPs.](http://arxiv.org/abs/2202.09753) | 本文分析了部分观察的马尔科夫决策过程（POMDPs）下自然演员-评论家方法的有限时间特性，并对使用有限状态控制器产生的错误进行了明确的表征。 |
| [^160] | [Weisfeiler and Leman go Machine Learning: The Story so far.](http://arxiv.org/abs/2112.09992) | Weisfeiler-Leman算法被广泛应用于处理图和关系数据。本文全面介绍了该算法在监督学习中的应用，包括理论背景、扩展、与等变神经网格的联系、并列出了当前应用和未来研究方向。 |
| [^161] | [MAFAT: Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference.](http://arxiv.org/abs/2107.06960) | 本文提出了一种内存感知的神经网络融合和切片方法，用于在边缘设备上加速推断。通过将网络细分为多个独立的卷积层组合，并进行融合和切片操作，实现了在资源受限的设备上降低内存占用的目标。 |
| [^162] | [Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting.](http://arxiv.org/abs/2106.07677) | 本研究引入了一种概率公平策略ProbFair，如何在不确定性贪婪赌博问题中进行资源分配，并在满足预算约束的同时最大化总期望奖励。实验证明ProbFair在保持效用的同时提供公平性保证。 |
| [^163] | [Strong Optimal Classification Trees.](http://arxiv.org/abs/2103.15965) | 本文提出了一种基于流的混合整数优化形式来学习最优二进制分类树，该形式具有更强的线性优化能力，并能处理侧约束，实现可解释和公平的决策树设计。 |
| [^164] | [Evaluation of Complexity Measures for Deep Learning Generalization in Medical Image Analysis.](http://arxiv.org/abs/2103.03328) | 本文通过对乳腺超声图像进行实证研究，调查了25个复杂度度量与监督深度学习分类器的普适性能力之间的相关性，发现基于PAC-Bayes平坦性和路径范数的度量方法产生了最一致的结果。 |
| [^165] | [MixPath: A Unified Approach for One-shot Neural Architecture Search.](http://arxiv.org/abs/2001.05887) | 本论文提出了一种名为MixPath的统一的一次性神经架构搜索方法，通过训练一次性的多路径超网络来准确评估候选架构。采用一种新颖的机制称为Shadow Batch Normalization（SBN）来解决多路径结构的特征差异问题，稳定优化并提高排名性能。 |
| [^166] | [Declarative Mechanism Design.](http://arxiv.org/abs/1912.13122) | 本文介绍了声明性机制设计的研究，提出了机构神经网络作为一种受管制的人工神经网络，引起人们对人工教学的关注，并提供了初步的答案。 |

# 详细

[^1]: LightPath: 轻量级和可伸缩的路径表示学习

    LightPath: Lightweight and Scalable Path Representation Learning. (arXiv:2307.10171v1 [cs.LG])

    [http://arxiv.org/abs/2307.10171](http://arxiv.org/abs/2307.10171)

    LightPath是一个轻量级和可伸缩的路径表示学习框架，旨在实现在资源受限的环境下降低资源消耗和实现可伸缩性，同时保持高准确性，从而实现更广泛的适用性。

    

    移动路径广泛应用于智能交通和智能城市应用中。为了服务这些应用，路径表示学习旨在提供路径的紧凑表示，以在不同的下游任务（如路径排序和旅行成本估计）中实现高效准确的操作。在资源受限的环境下和绿色计算限制下，路径表示学习的轻量级和可伸缩性非常重要。然而，现有的路径表示学习研究主要关注准确性，对资源消耗和可伸缩性次要关注。我们提出了一个轻量级和可伸缩的路径表示学习框架，命名为LightPath，旨在降低资源消耗，实现可伸缩性，同时不影响准确性，从而实现更广泛的适用性。

    Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.  We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good sca
    
[^2]: 大型语言模型的挑战与应用

    Challenges and Applications of Large Language Models. (arXiv:2307.10169v1 [cs.CL])

    [http://arxiv.org/abs/2307.10169](http://arxiv.org/abs/2307.10169)

    本文旨在总结大型语言模型领域的挑战和应用成功案例，帮助机器学习研究人员快速了解该领域的当前状态并提高效率。

    

    大型语言模型在机器学习领域的讨论中从不存在到无处不在只用了几年的时间。由于领域的快速发展，很难确定剩余的挑战和已经取得的应用成功。本文旨在建立一个系统的一组未解决问题和应用成功案例，以便机器学习研究人员能够更快地了解该领域的当前状态并提高效率。

    Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
    
[^3]: VITS: 基于变分推理的汤普森抽样用于情境背离问题的算法

    VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])

    [http://arxiv.org/abs/2307.10167](http://arxiv.org/abs/2307.10167)

    VITS是一种基于高斯变分推理的新算法，用于情境背离问题的汤普森抽样。它提供了强大的后验近似，计算效率高，并且在线性情境背离问题中达到与传统TS相同阶数的次线性遗憾上界。

    

    本文介绍并分析了一种用于情境背离问题的汤普森抽样（TS）算法的变体。传统的TS算法在每轮需要从当前的后验分布中抽样，而这通常是难以计算的。为了解决这个问题，可以使用近似推理技术并提供接近后验分布的样本。然而，当前的近似技术要么估计不准确（拉普拉斯近似），要么计算开销较大（MCMC方法，集成抽样...）。在本文中，我们提出了一种新的算法，基于高斯变分推理的变分推理汤普森抽样（VITS）。这种方法提供了强大的后验近似，并且容易从中抽样，而且计算效率高，是TS的理想选择。此外，我们还证明了在线性情境背离问题中，VITS实现了与传统TS相同阶数的次线性遗憾上界，与维度和回合数成正比。

    In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we 
    
[^4]: 重新思考后门攻击

    Rethinking Backdoor Attacks. (arXiv:2307.10163v1 [cs.CR])

    [http://arxiv.org/abs/2307.10163](http://arxiv.org/abs/2307.10163)

    本文重新思考了后门攻击问题，发现在没有关于训练数据分布的结构信息的情况下，后门攻击与数据中自然产生的特征是不可区分的，因此难以检测。作者还重新审视现有的抵御后门攻击的方法，并探索了一种关于后门攻击的替代视角。

    

    在后门攻击中，对手会将恶意构造的后门示例插入训练集中，使得生成的模型容易受到操纵。防御这种攻击通常涉及将这些插入的示例视为训练集中的异常值，并使用鲁棒统计学的技术来检测和删除它们。在这项工作中，我们提出了一种不同的解决后门攻击问题的方法。具体而言，我们展示了在没有关于训练数据分布的结构信息的情况下，后门攻击与数据中自然产生的特征是不可区分的--因此无法在一般意义上“检测”它们。然后，根据这一观察，我们重新审视现有的抵御后门攻击的方法，并表征它们所做出的（常常是潜在的）假设以及它们依赖的假设。最后，我们探索了一种关于后门攻击的替代视角：假设这些攻击对应于训练数据中最强的特征。

    In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.  In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to "detect" in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this a
    
[^5]: 通过引导元元强化学习实现鲁棒的驾驶策略学习

    Robust Driving Policy Learning with Guided Meta Reinforcement Learning. (arXiv:2307.10160v1 [cs.RO])

    [http://arxiv.org/abs/2307.10160](http://arxiv.org/abs/2307.10160)

    本文提出了一种通过引导元元策略学习方法来实现社交车辆多样驾驶策略的有效方法，并使用训练策略增强自主驾驶策略的鲁棒性。

    

    尽管深度强化学习(DRL)在交互式交通场景中的自主导航方面取得了可喜的成果，但现有研究通常采用固定的行为策略来控制训练环境中的社交车辆。这可能导致学习到的驾驶策略过拟合环境，使其难以与具有不同、未见过行为的车辆良好交互。在这项工作中，我们引入了一种有效的方法，将多样的驾驶策略作为一个单一的元元策略进行训练。通过随机化社交车辆的基于交互的奖励函数，我们可以生成多样化的目标，并通过实现特定目标的引导策略有效地训练元元策略。我们进一步提出了一种训练策略，使用社交车辆由学习到的元元策略控制的环境，来增强自主车辆的驾驶策略的鲁棒性。我们的方法成功地学习了一种能够很好地适应未见过的情况的自主驾驶策略。

    Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unsee
    
[^6]: 基于曲率的图聚类

    Curvature-based Clustering on Graphs. (arXiv:2307.10155v1 [cs.SI])

    [http://arxiv.org/abs/2307.10155](http://arxiv.org/abs/2307.10155)

    本研究通过利用图的几何性质，实现了基于离散Ricci曲率的聚类算法，可以识别图结构中的密集连接子结构，包括单成员社区和混合成员社区，以及在线图上的社区检测，并提供了实验证据支持。

    

    无监督节点聚类（或社区检测）是经典的图学习任务。本文研究利用图的几何性质来识别密集连接的子结构以形成聚类或社区的算法。我们的方法实现了离散Ricci曲率及其相关的几何流，通过这些流，图的边权重演化以揭示其社区结构。我们考虑了几种离散曲率概念，并分析了相应算法的实用性。与之前的文献相比，我们不仅研究了单成员社区检测，即每个节点只属于一个社区，还研究了混合成员社区检测，即社区可能重叠。对于后者，我们认为在线图上执行社区检测有益处，即图的对偶图。我们提供了我们基于曲率的聚类算法的理论和实证证据。此外，我们还提供了几个重新实现和评估的实验。

    Unsupervised node clustering (or community detection) is a classical graph learning task. In this paper, we study algorithms, which exploit the geometry of the graph to identify densely connected substructures, which form clusters or communities. Our method implements discrete Ricci curvatures and their associated geometric flows, under which the edge weights of the graph evolve to reveal its community structure. We consider several discrete curvature notions and analyze the utility of the resulting algorithms. In contrast to prior literature, we study not only single-membership community detection, where each node belongs to exactly one community, but also mixed-membership community detection, where communities may overlap. For the latter, we argue that it is beneficial to perform community detection on the line graph, i.e., the graph's dual. We provide both theoretical and empirical evidence for the utility of our curvature-based clustering algorithms. In addition, we give several re
    
[^7]: 对于学习人形机械行走的潜在基于奖励的基准测试

    Benchmarking Potential Based Rewards for Learning Humanoid Locomotion. (arXiv:2307.10142v1 [cs.RO])

    [http://arxiv.org/abs/2307.10142](http://arxiv.org/abs/2307.10142)

    本文对人形机器人使用标准形式的奖励塑造和潜在基于奖励的塑造进行了基准测试。在高维系统中，潜在基于奖励的塑造（PBRS）对于收敛速度的提升效果较小。

    

    在开发有效的强化学习(RL)流程中，主要挑战往往是设计和调整奖励函数。良好设计的塑形奖励可以加快学习速度。然而，简单地制定奖励可能与期望的行为相冲突，如果没有适当调整，可能导致过度拟合甚至不稳定的性能。从理论上讲，潜在基于奖励的塑形(PBRS)可以在不影响最优策略的情况下指导学习过程。尽管有几项研究探索了使用潜在基于奖励的塑形来加快学习收敛的方法，但大多数研究局限于网格世界和低维系统，而在机器人强化学习中，主要依赖于标准形式的奖励塑造。在本文中，我们对使用PBRS的标准形式和塑形进行了人形机器人的基准测试。我们发现，在这个高维系统中，PBRS的收敛速度只有微小的好处。然而，PBRS奖励项具有重大的意义。

    The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly
    
[^8]: Quarl: 一种基于学习的量子电路优化器

    Quarl: A Learning-Based Quantum Circuit Optimizer. (arXiv:2307.10120v1 [quant-ph])

    [http://arxiv.org/abs/2307.10120](http://arxiv.org/abs/2307.10120)

    Quarl是一种基于学习的量子电路优化器，通过引入新颖的神经架构和强化学习训练过程，解决了量子电路优化中的动作空间较大和非均匀状态表示的挑战。实验证明，Quarl在几乎所有基准电路上表现显著优于其他优化器，并且在速度上更快。

    

    量子电路优化具有挑战性，因为功能等效的电路搜索空间非常大，并且必须应用暂时降低性能的转换来实现最终的性能改进。本文提出了一种名为Quarl的基于学习的量子电路优化器。将强化学习应用于量子电路优化面临两个主要挑战：庞大且变化的动作空间以及不均匀的状态表示。Quarl通过一种新颖的神经架构和强化学习训练过程来解决这些问题。我们的神经架构将动作空间分解为两个部分，并在其状态表示中利用图神经网络，这两个部分都受到本地推理主导全局电路推理的直觉指导。我们的评估表明，Quarl在几乎所有基准电路上明显优于现有的电路优化器。令人惊讶的是，Quarl不仅达到了此前的最佳水平，并且在速度上也比其他优化器更快。

    Optimizing quantum circuits is challenging due to the very large search space of functionally equivalent circuits and the necessity of applying transformations that temporarily decrease performance to achieve a final performance improvement. This paper presents Quarl, a learning-based quantum circuit optimizer. Applying reinforcement learning (RL) to quantum circuit optimization raises two main challenges: the large and varying action space and the non-uniform state representation. Quarl addresses these issues with a novel neural architecture and RL-training procedure. Our neural architecture decomposes the action space into two parts and leverages graph neural networks in its state representation, both of which are guided by the intuition that optimization decisions can be mostly guided by local reasoning while allowing global circuit-wide reasoning. Our evaluation shows that Quarl significantly outperforms existing circuit optimizers on almost all benchmark circuits. Surprisingly, Qu
    
[^9]: 扩展图神经网络的图评估指标

    Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])

    [http://arxiv.org/abs/2307.10112](http://arxiv.org/abs/2307.10112)

    本论文提出了扩展的图评估指标（GAMs），适用于回归任务和连续邻接矩阵。主要关注的两个GAMs是同质性和跨类邻域相似度（CCNS）。这些扩展的指标能够在图神经网络中评估图结构，提高模型性能。

    

    当将患者队列重组为所谓的人口图时，最初独立的数据点可以合并成一个相互连接的图结构。利用图神经网络（GNNs），可以使用这种人口图进行医学的下游任务。适合的图结构的构建是学习过程中的一个具有挑战性的步骤，它对模型的性能有着重要的影响。为此，已经引入了不同的图评估指标来评估图结构。然而，这些指标仅适用于分类任务和离散的邻接矩阵，只覆盖了一小部分实际应用。在这项工作中，我们引入了针对回归任务和连续邻接矩阵的扩展图评估指标（GAMs）。我们重点关注两个具体的GAMs：同质性和跨类邻域相似度（CCNS）。我们将GAMs的概念扩展到多个跳跃，并为回归任务定义了同质性。

    When re-structuring patient cohorts into so-called population graphs, initially independent data points can be incorporated into one interconnected graph structure. This population graph can then be used for medical downstream tasks using graph neural networks (GNNs). The construction of a suitable graph structure is a challenging step in the learning pipeline that can have severe impact on model performance. To this end, different graph assessment metrics have been introduced to evaluate graph structures. However, these metrics are limited to classification tasks and discrete adjacency matrices, only covering a small subset of real-world applications. In this work, we introduce extended graph assessment metrics (GAMs) for regression tasks and continuous adjacency matrices. We focus on two GAMs in specific: \textit{homophily} and \textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of GAMs to more than one hop, define homophily for regression tasks, as well as con
    
[^10]: 渐进稀疏化用于Transformer模型的遮罩微调

    Gradient Sparsification For Masked Fine-Tuning of Transformers. (arXiv:2307.10098v1 [cs.CL])

    [http://arxiv.org/abs/2307.10098](http://arxiv.org/abs/2307.10098)

    本研究提出了一种使用渐进稀疏化方法对预训练语言模型进行正则化，以改善微调性能。GradDrop及其变体通过在训练过程中随机屏蔽梯度，有效地进行梯度稀疏化。

    

    预训练的自监督语言模型的微调被广泛应用于向下游任务的迁移学习。微调可通过冻结预训练网络的梯度并只更新新添加的分类层的梯度，或通过对所有参数进行梯度更新来实现。渐进解冻在训练过程中逐渐解冻整个层的梯度，以在存储和训练速度与泛化性能之间进行权衡，这是一种有效的策略。然而，目前还不清楚渐进解冻整个训练是否是最优选择，相比之下，稀疏变体的渐进解冻可能可以提高微调性能。在本文中，我们提出了随机屏蔽梯度来正则化预训练语言模型，从而改善整体微调性能。我们介绍了GradDrop及其变体，一类梯度稀疏化方法，在训练过程中对梯度进行屏蔽。

    Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the b
    
[^11]: 重新审视不变性并引入先验知识在Gromov-Wasserstein距离中

    Revisiting invariances and introducing priors in Gromov-Wasserstein distances. (arXiv:2307.10093v1 [cs.LG])

    [http://arxiv.org/abs/2307.10093](http://arxiv.org/abs/2307.10093)

    本文提出了一种新的基于最优传输的距离，增强的Gromov-Wasserstein，它在Gromov-Wasserstein距离的基础上引入了对变换刚度的控制和特征对齐，并应用于单细胞多组学和迁移学习任务中，展示了其在机器学习中的实用性和改进性能。

    

    由于其能够比较度量空间中的测度并且对等度变换具有不变性，Gromov-Wasserstein距离在机器学习中有很多应用。然而，在某些应用中，这种不变性可能过于灵活而不可取。此外，Gromov-Wasserstein距离仅考虑输入数据集中的成对样本相似性，而忽略原始特征表示。我们提出了一种新的基于最优传输的距离，称为增强的Gromov-Wasserstein，它允许对变换的刚度有一定控制。它还结合了特征对齐，使我们能够更好地利用输入数据上的先验知识以提高性能。我们提出了对所提出的度量的理论洞察力。然后，我们展示了它在单细胞多组学对齐任务和机器学习中的迁移学习场景中的实用性。

    Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariance property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We present theoretical insights into the proposed metric. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.
    
[^12]: 在野外的Android：用于Android设备控制的大规模数据集

    Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])

    [http://arxiv.org/abs/2307.10088](http://arxiv.org/abs/2307.10088)

    这个论文提出了一个名为Android in the Wild (AITW)的大规模数据集，用于研究设备控制系统，该数据集包括人类示范的设备交互、自然语言指令和多种Android版本和设备类型。这个数据集提供了一个新的挑战，需要从视觉外观中推断用户界面中可用的操作。

    

    对于能够解释人类自然语言指令并直接控制数字设备用户界面执行的设备控制系统，人们越来越感兴趣。我们提出了一个用于设备控制研究的数据集，Android in the Wild (AITW)，该数据集比当前数据集大几个数量级。该数据集包含了设备交互的人类示范，包括屏幕和操作，以及相应的自然语言指令。它包括715k个剧集，涵盖30k个不同的指令，四个Android版本（v10-13），以及八种不同的设备类型（从Pixel 2 XL到Pixel 6）和不同的屏幕分辨率。它包含需要语言和视觉上下文的语义理解的多步骤任务。这个数据集提出了一个新的挑战：必须从它们的视觉外观中推断出用户界面中可用的操作。而且，行动空间不再是简单的基于用户界面元素的行动，而是包含精确的手势（例如，水平滚动）

    There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls 
    
[^13]: 概率主成分分析的双重表述

    A Dual Formulation for Probabilistic Principal Component Analysis. (arXiv:2307.10078v1 [cs.LG])

    [http://arxiv.org/abs/2307.10078](http://arxiv.org/abs/2307.10078)

    本文探讨了概率主成分分析在希尔伯特空间中的双重表述方法，并发展了适用于核方法的生成框架。作者通过实验证明了该方法能兼容核主成分分析，并在虚拟和真实数据集上进行了验证。

    

    本文中，我们在希尔伯特空间中对概率主成分分析进行了表述，并展示了最优解在对偶空间中的表示。这使得我们能够发展出一种适用于核方法的生成框架。此外，我们展示了它如何吸纳了核主成分分析，并在一个虚拟数据集和一个真实数据集上进行了演示。

    In this paper, we characterize Probabilistic Principal Component Analysis in Hilbert spaces and demonstrate how the optimal solution admits a representation in dual space. This allows us to develop a generative framework for kernel methods. Furthermore, we show how it englobes Kernel Principal Component Analysis and illustrate its working on a toy and a real dataset.
    
[^14]: 可伸缩的深度学习用于RNA二级结构预测

    Scalable Deep Learning for RNA Secondary Structure Prediction. (arXiv:2307.10073v1 [cs.LG])

    [http://arxiv.org/abs/2307.10073](http://arxiv.org/abs/2307.10073)

    RNA二级结构预测取得了显著进展，我们提出了RNAformer模型，通过使用轴向注意力和潜空间回收的方法，以及在潜空间直接建模邻接矩阵的架构和扩展模型规模来获得性能改善。该方法在TS0基准数据集上取得最先进的性能，并可以学习RNA折叠过程的生物物理模型。

    

    随着深度学习技术的应用，RNA二级结构预测领域取得了显著的进展。在这项工作中，我们提出了RNAformer，这是一个使用轴向注意力和潜空间回收的简洁深度学习模型。我们通过在潜空间直接设计邻接矩阵的架构，并扩展模型的规模来获得性能改进。我们的方法在流行的TS0基准数据集上实现了最先进的性能，并且超过了使用外部信息的方法。此外，我们通过实验证明RNAformer可以学习RNA折叠过程的生物物理模型。

    The field of RNA secondary structure prediction has made significant progress with the adoption of deep learning techniques. In this work, we present the RNAformer, a lean deep learning model using axial attention and recycling in the latent space. We gain performance improvements by designing the architecture for modeling the adjacency matrix directly in the latent space and by scaling the size of the model. Our approach achieves state-of-the-art performance on the popular TS0 benchmark dataset and even outperforms methods that use external information. Further, we show experimentally that the RNAformer can learn a biophysical model of the RNA folding process.
    
[^15]: 使用无源样本的领域自适应对抗扰动的无监督深度视觉模型准确性估计

    Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples. (arXiv:2307.10062v1 [cs.CV])

    [http://arxiv.org/abs/2307.10062](http://arxiv.org/abs/2307.10062)

    本研究提出了一种无需源数据的框架来估计深度视觉模型在未标记的目标数据上的准确性。该方法利用伪标签和无源域自适应算法进行准确性估计，通过对目标模型的输入进行自适应对抗性扰动来处理错误伪标签的影响。

    

    部署深度视觉模型可能导致性能下降，原因是源域和目标域之间存在差异。一些方法利用带标签的源数据来估计目标域的准确性，但由于数据保密性或服务设备上的资源限制，访问带标签的源数据通常非常困难。我们的工作提出了一种新的框架，可以在没有访问源数据的情况下对未标记的目标数据进行模型准确性估计。我们研究了使用伪标签进行准确性估计的可行性，并在此基础上采用了最近的无源域自适应算法的进展。我们的方法衡量了从源假设演化而来的目标伪标记函数与源假设之间的不一致率。为了减轻由于理想联合假设风险较高而可能产生的错误伪标签的影响，我们采用自适应对抗性扰动来处理目标模型的输入。

    Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. Our proposed source-fr
    
[^16]: 准确的深度学习子网格尺度模型用于大涡模拟

    Accurate deep learning sub-grid scale models for large eddy simulations. (arXiv:2307.10060v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.10060](http://arxiv.org/abs/2307.10060)

    本文提出了两个用于大涡模拟的子网格尺度模型，采用深度学习算法并与分析模型相比能够处理更复杂的非线性关系。其中一个模型融入了多个不变性，另一个只融入了伽利略不变性。

    

    我们提出了两个用于大涡模拟的子网格尺度（SGS）湍流模型系列。它们的开发需要制定经过物理验证的强大而高效的深度学习（DL）算法，与最先进的分析建模技术不同，这些算法可以产生输入和输出之间的高阶复杂非线性关系。通过从两个摩擦雷诺数约为395和590的典型通道流的直接模拟数据进行显式滤波，提供了用于训练和测试的准确数据。这两组模型使用不同的网络架构。其中一种架构使用张量基神经网络（TBNN），嵌入了简化的分析模型形式的一般有效粘性假设，从而融入了伽利略、旋转和反射不变性。而另一种架构是一个相对简单的网络，它只能融入伽利略不变性。

    We present two families of sub-grid scale (SGS) turbulence models developed for large-eddy simulation (LES) purposes. Their development required the formulation of physics-informed robust and efficient Deep Learning (DL) algorithms which, unlike state-of-the-art analytical modeling techniques can produce high-order complex non-linear relations between inputs and outputs. Explicit filtering of data from direct simulations of the canonical channel flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided accurate data for training and testing. The two sets of models use different network architectures. One of the architectures uses tensor basis neural networks (TBNN) and embeds the simplified analytical model form of the general effective-viscosity hypothesis, thus incorporating the Galilean, rotational and reflectional invariances. The other architecture is that of a relatively simple network, that is able to incorporate the Galilean invariance only. However, this simp
    
[^17]: 非平滑非凸优化中随机次梯度方法的收敛性保证

    Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])

    [http://arxiv.org/abs/2307.10053](http://arxiv.org/abs/2307.10053)

    本文研究了非平滑非凸优化中随机次梯度方法的收敛性质，并提出了一种新的框架，证明了其在单时间尺度和双时间尺度情况下的全局收敛性，包括了多种已知的SGD类型方法。对于有限和形式的目标函数，证明了这些方法能够在随机选择的步长和初始点上找到Clarke稳定点。

    

    本文研究了随机梯度下降（SGD）方法及其变种在训练由非平滑激活函数构建的神经网络中的收敛性质。我们提出了一种新颖的框架，为更新动量项和变量的步长分配了不同的时间尺度。在一些温和的条件下，我们证明了我们提出的框架在单时间尺度和双时间尺度情况下的全局收敛性。我们还证明了我们提出的框架包含了很多已知的SGD类型方法，包括heavy-ball SGD、SignSGD、Lion、normalized SGD和clipped SGD。此外，当目标函数采用有限和形式时，我们基于我们提出的框架证明了这些SGD类型方法的收敛性质。特别地，在温和的假设下，我们证明了这些SGD类型方法在随机选择的步长和初始点上能够找到目标函数的Clarke稳定点。

    In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
    
[^18]: 上下文可靠性：在不同上下文中不同特征的重要性

    Contextual Reliability: When Different Features Matter in Different Contexts. (arXiv:2307.10026v1 [cs.LG])

    [http://arxiv.org/abs/2307.10026](http://arxiv.org/abs/2307.10026)

    本论文提出了一个新的概念，即上下文可靠性，它考虑到在不同上下文中使用的“正确”特征可能会有所变化。作者提出的两阶段框架ENP能够首先识别给定上下文中要使用的相关特征，然后训练模型，从而提高性能的鲁棒性。

    

    深度神经网络经常因依赖噪声相关性而产生灾难性错误。大部分的研究假设有一个明确的虚假和可靠特征的二分法，然而这通常是不现实的。例如，大部分情况下我们不想让自动驾驶汽车简单地复制周围车辆的速度 -- 如果邻近的车辆违章，我们不想让我们的车也闯红灯。然而，我们不能简单地强制要求不变性以排除下一条车道的速度，因为这可能提供了关于一个无法观测到的人行横道上行人的宝贵信息。因此，普遍地忽视有时可靠（但并非总是）的特征会导致性能非鲁棒。我们提出了一个新的称为上下文可靠性的设定，考虑到使用“正确”特征可能会因上下文的不同而变化。我们提出并分析了一个称为显式非虚假特征预测（ENP）的两阶段框架，该框架首先识别给定上下文中要使用的相关特征，然后训练一个模型。

    Deep neural networks often fail catastrophically by relying on spurious correlations. Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic. For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so. However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk. Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance. We formalize a new setting called contextual reliability which accounts for the fact that the "right" features to use may vary depending on the context. We propose and analyze a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a mod
    
[^19]: Europepolls: 一个关于欧盟和英国的国家层面民意调查数据集

    Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK. (arXiv:2307.10022v1 [cs.LG])

    [http://arxiv.org/abs/2307.10022](http://arxiv.org/abs/2307.10022)

    本文提出了一个关于欧盟和英国的国家层面历史民意调查数据集，填补了现有数据的空白，为研究人员揭示多模态数据与选民行为之间的复杂互动提供了机会。

    

    本文提出了一个关于欧盟和英国国家层面历史民意调查数据的开放数据集。该数据集旨在填补现有欧盟民意调查数据的空白，一些现有数据集仅涵盖过去五年，限制了研究机会。同时，一些更大的专有数据集以视觉预处理时间序列格式提供，最后，虽然可能有其他大型国家的数据集存在，但由于语言障碍而无法获取。数据收集自维基百科，并使用pandas库进行预处理，原始数据和预处理数据均为.csv格式。我希望，鉴于最近在语言模型和深度学习等方面的进展，这个大型数据集将使研究人员能够揭示多模态数据（新闻文章、经济指标、社交媒体）与选民行为之间的复杂互动。

    I propose an open dataset of country-level historical opinion polling data for the European Union and the UK. The dataset aims to fill a gap in available opinion polling data for the European Union. Some existing datasets are restricted to the past five years, limiting research opportunities. At the same time, some larger proprietary datasets exist but are available only in a visual preprocessed time series format. Finally, while other large datasets for individual countries might exist, these could be inaccessible due to language barriers. The data was gathered from Wikipedia, and preprocessed using the pandas library. Both the raw and the preprocessed data are in the .csv format. I hope that given the recent advances in LLMs and deep learning in general, this large dataset will enable researchers to uncover complex interactions between multimodal data (news articles, economic indicators, social media) and voting behavior. The raw data, the preprocessed data, and the preprocessing scr
    
[^20]: TbExplain: 一种场景分类模型的基于文本的解释方法与统计预测校正

    TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])

    [http://arxiv.org/abs/2307.10003](http://arxiv.org/abs/2307.10003)

    本文提出了一种名为TbExplain的框架，它利用XAI技术和预训练的对象检测器，通过文本形式解释场景分类模型，并引入了一种新的方法来纠正预测和进行文本解释。

    

    可解释性人工智能(XAI)的领域旨在提高黑盒机器学习模型的可解释性。建立基于输入特征重要性值的热图是解释这些模型产生预测的基本方法之一。热图在人类中几乎可以理解，但并非没有缺陷。例如，非专业用户可能不完全理解热图的逻辑（即使用不同强度或颜色突出显示与模型预测相关的像素的逻辑）。此外，与模型预测相关的输入图像的对象和区域通常无法完全通过热图区分。本文提出了一种称为TbExplain的框架，采用XAI技术和预训练的对象检测器，以文本形式解释场景分类模型。此外，TbExplain还采用了一种新的方法来纠正预测和进行文本解释。

    The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually exp
    
[^21]: 剪枝神经网络对剪枝结果的影响

    Impact of Disentanglement on Pruning Neural Networks. (arXiv:2307.09994v1 [cs.LG])

    [http://arxiv.org/abs/2307.09994](http://arxiv.org/abs/2307.09994)

    本研究通过使用Beta-VAE框架来迫使网络学习解缠缠绕表示，并研究了其对剪枝神经网络的影响。实验结果表明，在分类任务中，解缠缠绕表示对剪枝过程具有重要作用。

    

    在边缘设备上部署深度学习神经网络以在真实世界中实现特定任务，需要减小其存储占用、功耗和延迟。通过高效的模型压缩可以实现这一目标。变分自编码器（VAE）网络产生的解缠缠绕的潜在表示是实现模型压缩的一种有前途的方法，因为它们主要保留了与任务相关的信息，丢弃了对该任务无用的信息。我们使用Beta-VAE框架结合标准剪枝准则，研究了迫使网络学习解缠缠绕表示对分类任务的剪枝过程的影响。具体而言，我们在MNIST和CIFAR10数据集上进行实验，研究了解缠缠绕的挑战，并提出了未来研究的路径。

    Deploying deep learning neural networks on edge devices, to accomplish task specific objectives in the real-world, requires a reduction in their memory footprint, power consumption, and latency. This can be realized via efficient model compression. Disentangled latent representations produced by variational autoencoder (VAE) networks are a promising approach for achieving model compression because they mainly retain task-specific information, discarding useless information for the task at hand. We make use of the Beta-VAE framework combined with a standard criterion for pruning to investigate the impact of forcing the network to learn disentangled representations on the pruning process for the task of classification. In particular, we perform experiments on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose a path forward for future works.
    
[^22]: UniMatch:一个统一的用户-物品匹配框架，用于多用途商家营销

    UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing. (arXiv:2307.09989v1 [cs.IR])

    [http://arxiv.org/abs/2307.09989](http://arxiv.org/abs/2307.09989)

    UniMatch是一个统一的用户-物品匹配框架，通过使用一个模型同时进行物品推荐和用户定位，减少了商家购买多个机器学习模型的成本。利用多项分布建模用户-物品交互矩阵，并通过双向偏差校正的损失函数指导模型学习用户-物品联合概率，实现了优化。

    

    在使用云服务进行私有领域营销时，商家通常需要为多个营销目的购买不同的机器学习模型，导致成本非常高。我们提出了一个统一的用户-物品匹配框架，可以通过一个模型同时进行物品推荐和用户定位。我们通过对用户-物品交互矩阵进行多项分布建模的实验验证了上述并发建模的可行性，并提出了一个双向偏差校正的NCE loss来实现。提出的损失函数通过纠正由于批次内负采样引起的用户和物品偏差，引导模型学习用户-物品联合概率p(u,i)，而不是条件概率p(i|u)或p(u|i)。此外，我们的框架对模型架构具有灵活的适应性。广泛的实验证明，我们的框架可以显著提高性能。

    When doing private domain marketing with cloud services, the merchants usually have to purchase different machine learning models for the multiple marketing purposes, leading to a very high cost. We present a unified user-item matching framework to simultaneously conduct item recommendation and user targeting with just one model. We empirically demonstrate that the above concurrent modeling is viable via modeling the user-item interaction matrix with the multinomial distribution, and propose a bidirectional bias-corrected NCE loss for the implementation. The proposed loss function guides the model to learn the user-item joint probability $p(u,i)$ instead of the conditional probability $p(i|u)$ or $p(u|i)$ through correcting both the users and items' biases caused by the in-batch negative sampling. In addition, our framework is model-agnostic enabling a flexible adaptation of different model architectures. Extensive experiments demonstrate that our framework results in significant perfo
    
[^23]: TinyTrain：在极端边缘进行深度神经网络训练

    TinyTrain: Deep Neural Network Training at the Extreme Edge. (arXiv:2307.09988v1 [cs.LG])

    [http://arxiv.org/abs/2307.09988](http://arxiv.org/abs/2307.09988)

    TinyTrain是一种在设备上进行训练的方法，通过选择性更新模型的部分并处理数据稀缺问题，大大缩短了训练时间。通过任务自适应的稀疏更新方法，TinyTrain能够在高准确性的同时减小计算和内存占用，对未知任务表现出色。

    

    设备上的训练对于用户个性化和隐私至关重要。随着物联网设备和微控制器单元（MCU）的普及，由于受限的内存和计算资源以及标注的用户数据的有限可用性，这项任务变得更加具有挑战性。尽管如此，先前的研究忽视了数据稀缺问题，需要过长的训练时间（例如几个小时），或者导致重大的准确性损失（≥10%）。我们提出了TinyTrain，一种设备上的训练方法，通过选择性更新模型的部分，并明确处理数据稀缺问题，大幅缩短了训练时间。TinyTrain引入了一种任务自适应的稀疏更新方法，根据多目标准则动态选择层/通道，同时捕捉用户数据、内存和目标设备的计算能力，从而在未知任务上获得高准确性，并减小计算和内存占用。TinyTrain在整体微调的基础上表现出色。

    On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\geq$10\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the enti
    
[^24]: 面向分层物联网网络的成本有效联邦学习的学习者推荐

    Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks. (arXiv:2307.09977v1 [cs.LG])

    [http://arxiv.org/abs/2307.09977](http://arxiv.org/abs/2307.09977)

    本文提出了联合学习者推荐辅助的联邦客户端选择(LRef-FedCS)方法，以最小化最坏情况下参与者所产生的成本，并确保在分层物联网网络中联邦学习的长期公平性。

    

    联邦学习是一种通过在资源受限的客户端上本地训练参数的分布式方式，以解决数据隐私问题的范例，受到了广泛关注。然而，当联邦学习服务器的覆盖范围内并非所有客户端都注册到联邦学习网络时，联邦学习就不适用。为了弥补这种差距，本文提出了联合学习者推荐辅助的联邦客户端选择(LRef-FedCS)，以及通信和计算资源调度，和本地模型准确性优化(LMAO)方法。这些方法旨在最小化最坏情况下参与者所产生的成本，确保在分层物联网网络中联邦学习的长期公平性。利用Lyapunov优化技术，我们将原始问题重新转化为逐步联合优化问题(JOP)。随后，为了解决混合整数非凸JOP，我们分别和迭代地解决LRef-FedCS和LMAO。

    The paradigm of federated learning (FL) to address data privacy concerns by locally training parameters on resource-constrained clients in a distributed manner has garnered significant attention. Nonetheless, FL is not applicable when not all clients within the coverage of the FL server are registered with the FL network. To bridge this gap, this paper proposes joint learner referral aided federated client selection (LRef-FedCS), along with communications and computing resource scheduling, and local model accuracy optimization (LMAO) methods. These methods are designed to minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things (HieIoT) networks. Utilizing the Lyapunov optimization technique, we reformulate the original problem into a stepwise joint optimization problem (JOP). Subsequently, to tackle the mixed-integer non-convex JOP, we separatively and iteratively address LRef-FedCS and LMAO through the central
    
[^25]: 迈向绿色人工智能软件系统：基于架构的方法（GAISSA）

    Towards green AI-based software systems: an architecture-centric approach (GAISSA). (arXiv:2307.09964v1 [cs.SE])

    [http://arxiv.org/abs/2307.09964](http://arxiv.org/abs/2307.09964)

    该论文介绍了GAISSA项目的愿景、目标和预期成果，该项目旨在为数据科学家和软件工程师提供基于架构的方法和工具，以开发绿色人工智能系统。

    

    如今，基于人工智能的系统在不同领域取得了卓越的成果，并且在某些方面超过了人类。然而，训练人工智能模型和从中推断所需的计算资源很高，这对当前能源效率社会需求构成了重大挑战。为了应对这一挑战，本研究论文描述了GAISSA项目的主要愿景、目标和预期成果。GAISSA项目旨在为数据科学家和软件工程师提供基于架构的方法和工具，以建模和开发绿色人工智能系统。虽然项目处于初期阶段，但我们描述了目前的研究结果，展示了实现GAISSA目标的潜力。

    Nowadays, AI-based systems have achieved outstanding results and have outperformed humans in different domains. However, the processes of training AI models and inferring from them require high computational resources, which pose a significant challenge in the current energy efficiency societal demand. To cope with this challenge, this research project paper describes the main vision, goals, and expected outcomes of the GAISSA project. The GAISSA project aims at providing data scientists and software engineers tool-supported, architecture-centric methods for the modelling and development of green AI-based systems. Although the project is in an initial stage, we describe the current research results, which illustrate the potential to achieve GAISSA objectives.
    
[^26]: XSkill：跨体现技能发现

    XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])

    [http://arxiv.org/abs/2307.09955](http://arxiv.org/abs/2307.09955)

    本研究提出了XSkill，一种跨体现的技能发现框架，能够从无标签的人类和机器人操纵视频中纯粹地发现跨体现技能原型，并通过条件扩散策略将这些技能转移到机器人动作中，在未见任务中完成学习到的技能的组合。仿真和真实环境中的实验结果表明，这些发现的技能原型能够有效地促进技能转移和组合，从而构建出更通用和可扩展的模仿学习框架。

    

    人类示范视频是机器人学习的广泛数据源，并且是表达所需行为的直观用户界面。然而，直接从非结构化的人类视频中提取可重用的机器人操纵技能面临着体现差异和未观察到的行动参数的挑战。为了弥合这种体现差距，本文介绍了XSkill，一种模仿学习框架，它从无标签的人类和机器人操纵视频中纯粹地发现名为技能原型的跨体现表示，使用条件扩散策略将技能表示转移到机器人动作，并最终使用人类提示视频完成学习到的技能来完成未见任务。我们在仿真和真实环境中的实验表明，发现的技能原型促进了未见任务的技能转移和组合，从而实现了更通用和可扩展的模仿学习框架。

    Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performan
    
[^27]: 这里是翻译过的论文标题: 过去曾翻译《Impatient Bandits: Optimizing for the Long-Term Without Delay》

    Impatient Bandits: Optimizing for the Long-Term Without Delay. (arXiv:2307.09943v1 [cs.LG])

    [http://arxiv.org/abs/2307.09943](http://arxiv.org/abs/2307.09943)

    这里是中文总结出的一句话要点：这篇论文研究了在推荐系统中提高用户长期满意度的问题，通过开发一个预测延迟奖励的模型和设计一个利用该模型的赌博算法来解决了通过测量短期代理奖励反映实际长期目标不完美的挑战。

    

    这里是翻译过的论文摘要：推荐系统在在线平台上是一个普遍存在的功能。越来越多的情况下，它们明确地被任务为提高用户的长期满意度。在这个背景下，我们研究了一个内容探索任务，将其形式化为一个具有延迟奖励的多臂赌博问题。我们观察到，在选择学习信号时存在明显的权衡：等待完全的奖励可能需要几周时间，这会影响学习发生的速度，而测量短期代理奖励则不完美地反映了实际的长期目标。我们通过两个步骤来解决这个挑战。首先，我们开发了一个预测延迟奖励的模型，该模型可以整合迄今所获得的所有信息。通过贝叶斯滤波器组合完整的观察结果以及部分（短期或中期）的结果，从而得到概率信念。其次，我们设计了一个利用这个新的预测模型的赌博算法。该算法可以快速学习识别内容。

    Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify conten
    
[^28]: TREEMENT: 可解释的患者-试验匹配模型通过个性化动态树状记忆网络

    TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network. (arXiv:2307.09942v1 [cs.LG])

    [http://arxiv.org/abs/2307.09942](http://arxiv.org/abs/2307.09942)

    TREEMENT是一种采用个性化动态树状记忆网络的可解释患者-试验匹配模型，利用层次化临床本体知识和合格标准嵌入学习，提供准确而有解释性的患者-试验匹配。

    

    临床试验对于药物开发至关重要，但往往面临昂贵而低效的患者招募问题。近年来，提出了机器学习模型，通过基于纵向患者电子健康记录（EHR）数据和临床试验的合格标准自动匹配患者与临床试验，以加速患者招募。然而，它们要么依赖于无法扩展到其他试验的专家规则，要么以黑箱模型进行非常通用的匹配，缺乏可解释性会导致模型结果难以采用。为了提供准确且可解释的患者-试验匹配，我们引入了一种名为TREEMENT的个性化动态树状记忆网络模型。它利用层次化临床本体知识扩展了从序列EHR数据中学到的个性化患者表示，然后使用基于合格标准嵌入学习的注意力束搜索查询。

    Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials. However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.  To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to o
    
[^29]: Spuriosity并没有导致分类器失败：利用不变的预测来利用虚假特征

    Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])

    [http://arxiv.org/abs/2307.09933](http://arxiv.org/abs/2307.09933)

    本研究通过理论证明和算法提出，展示了在没有标签的情况下如何利用不稳定特征来提高分类器的性能。

    

    为了避免在域外数据上的失败，最近的研究试图提取具有与标签在不同域之间稳定或不变关系的特征，舍弃与标签在不同域之间关系变化的"虚假"或不稳定特征。然而，不稳定特征常常携带关于标签的补充信息，如果在测试域中正确使用，可以提高性能。我们的主要贡献是显示在没有标签的情况下学习如何在测试域中使用这些不稳定特征是可能的。特别是，我们证明基于稳定特征的伪标签提供了足够的指导来做到这一点，前提是在给定标签的条件下，稳定特征和不稳定特征是条件独立的。基于这个理论洞见，我们提出了稳定特征增强（SFB）算法：(i)学习一个能够分离稳定特征和条件独立不稳定特征的预测器；(ii)使用稳定特征预测来适应测试域

    To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt t
    
[^30]: DISA: 可微分相似性逼近用于通用多模态配准

    DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration. (arXiv:2307.09931v1 [cs.CV])

    [http://arxiv.org/abs/2307.09931](http://arxiv.org/abs/2307.09931)

    本文提出了一个通用框架，用于创建具有表达力的跨模态描述符，通过在卷积神经网络的特征空间中用点积逼近现有的度量，实现了快速的可变形全局配准。我们的方法可在临床环境中直接使用，仅需替换相似性度量。

    

    多模态图像配准是许多图像引导过程中具有挑战性但又必不可少的一步。大多数配准算法依赖于计算复杂、频繁非可微的相似性度量，以应对影像模态之间解剖结构外观差异的问题。最近的基于机器学习的方法仅适用于特定的解剖学-模态组合，并不能推广到新的环境。本文提出了一个通用框架，用于创建具有表达力的跨模态描述符，从而实现快速的可变形全局配准。我们通过在一个小型卷积神经网络（CNN）的特征空间中用点积逼近现有的度量，实现了可微分训练，同时无需配准数据。我们的方法比基于局部块的度量快几个数量级，并可以直接在临床环境中使用，只需将相似性度量替换为我们提出的度量即可。

    Multimodal image registration is a challenging but essential step for numerous image-guided procedures. Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities. Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings. We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration. We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data. Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one. Experiments on three differe
    
[^31]: TimeTuner: 诊断时间序列预测中的时间表示的对照解释

    TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v1 [cs.HC])

    [http://arxiv.org/abs/2307.09916](http://arxiv.org/abs/2307.09916)

    TimeTuner是一个新颖的可视化分析框架，旨在帮助分析人员理解时间序列预测中模型行为与时间表示的关系，并解决自动化特征学习方法的局限性。

    

    深度学习方法在时间序列预测中的应用越来越多，许多努力致力于设计复杂的深度学习模型。最近的研究表明，深度学习的成功往往归因于有效的数据表示，促进了特征工程和表示学习领域的发展。然而，自动化特征学习方法通常在融入先验知识、识别变量间相互作用和选择评估指标以确保模型可靠性方面存在局限性。为了改善这些限制，本文提出了一种新颖的可视化分析框架，即TimeTuner，旨在帮助分析人员理解模型行为与时间序列表示的局部相关性、平稳性和粒度之间的关联。该系统主要包括以下两个阶段技术：我们首先利用对照解释来建立时间序列表示之间的关系，

    Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations,
    
[^32]: 用于学习时间齐次动力系统的深度投影网络

    Deep projection networks for learning time-homogeneous dynamical systems. (arXiv:2307.09912v1 [cs.LG])

    [http://arxiv.org/abs/2307.09912](http://arxiv.org/abs/2307.09912)

    这篇论文介绍了一种利用深度投影网络学习时间齐次动力系统的有意义表示的方法。通过优化类似于经典相关分析的目标函数，避免了矩阵求逆的稳定性问题，并通过两种正则化方案进一步增强学习效果。

    

    我们考虑了一般的时间齐次动力系统，包括离散和连续的，并研究了从观测数据中学习状态的有意义表示的问题。这对于学习系统的前向传输算子至关重要，该算子可以用于预测未来的状态或可观测量。表示通常通过神经网络参数化，与投影算子相关联，并通过优化类似于经典相关分析（CCA）的目标函数来学习。然而，与CCA不同，我们的目标函数避免了矩阵求逆，因此通常更稳定且适用于具有挑战性的场景。我们的目标函数是CCA的一个紧松弛，我们进一步通过提出两种正则化方案来增强它，一种鼓励表示的分量正交，而另一种利用了 Chapman-Kolmogorov 方程。我们将我们的方法应用于具有挑战性的离散动力系统

    We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical
    
[^33]: 重复观测进行分类

    Repeated Observations for Classification. (arXiv:2307.09896v1 [cs.IT])

    [http://arxiv.org/abs/2307.09896](http://arxiv.org/abs/2307.09896)

    本文研究了非参数分类中的重复观测问题，提出了一些简单的分类规则，并证明了当重复观测次数$t\to\infty$时，条件错误概率以指数速度收敛。

    

    我们研究了非参数分类问题中的重复观测问题。令$\bX$为$d$维特征向量，$Y$为取值在$\{1,\dots ,M\}$之间的标签。与通常样本量$n$较大，维度较低$d$的设置不同的是，本文处理的是当我们不是观测到一个单一的特征向量$\bX$，而是给出$t$个重复的特征向量$\bV_1,\dots ,\bV_t $的情况。我们提出了一些简单的分类规则，使得条件错误概率的收敛速度随着$t\to\infty$呈指数收敛。在分析中，我们研究了一些特定的模型，如基于名义密度的鲁棒检测，原型分类，线性变换，线性分类和尺度变换。

    We study the problem nonparametric classification with repeated observations. Let $\bX$ be the $d$ dimensional feature vector and let $Y$ denote the label taking values in $\{1,\dots ,M\}$. In contrast to usual setup with large sample size $n$ and relatively low dimension $d$, this paper deals with the situation, when instead of observing a single feature vector $\bX$ we are given $t$ repeated feature vectors $\bV_1,\dots ,\bV_t $. Some simple classification rules are presented such that the conditional error probabilities have exponential convergence rate of convergence as $t\to\infty$. In the analysis, we investigate particular models like robust detection by nominal densities, prototype classification, linear transformation, linear classification, scaling.
    
[^34]: VAE的对称均衡学习

    Symmetric Equilibrium Learning of VAEs. (arXiv:2307.09883v1 [cs.LG])

    [http://arxiv.org/abs/2307.09883](http://arxiv.org/abs/2307.09883)

    本文提出了一种对称均衡学习方法，允许在只能通过采样获得数据和潜在分布的情况下学习VAEs。实验证明该方法与传统ELBO学习方法获得的模型相当，并具有广泛的应用性。

    

    我们将变分自动编码器（VAE）视为解码器-编码器对，将数据空间中的分布映射到潜在空间中的分布，反之亦然。VAEs的标准学习方法，即最大化证据下界（ELBO），存在明显的不对称性。此外，它需要一个闭合形式的先验潜在分布。这限制了VAEs在更复杂的情况下的适用性，如一般的半监督学习和使用复杂的生成模型作为先验。我们提出了一种纳什均衡学习方法，放宽了这些限制，在只能通过采样获得数据和潜在分布的情况下学习VAEs。这种方法的灵活性和简单性使其适用于广泛的学习场景和下游任务。实验证明，通过这种方法学习的模型与ELBO学习获得的模型相当，并展示了其在实践中的适用性。

    We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability fo
    
[^35]: 通过单向流进行对抗性似然估计

    Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])

    [http://arxiv.org/abs/2307.09882](http://arxiv.org/abs/2307.09882)

    本文提出了一种通过单向流进行对抗性似然估计的方法，并使用重要性采样解决了Wasserstein GAN中分区函数有偏估计的问题。同时，通过最大化生成器的熵，提高了模式覆盖效果。这种方法通过计算生成样本的密度来实现对分区函数的无偏估计和生成器熵的计算。

    

    生成对抗网络（GAN）能够产生高质量的样本，但无法提供样本周围的概率密度估计。然而，已经注意到在能量模型的设置中，最大化对数似然可以导致判别器提供非归一化的密度（通常称为能量）的对抗性框架。我们进一步发展了这一观点，结合重要性采样，并展示了以下内容：1）Wasserstein GAN对分区函数进行了有偏估计，我们提出使用无偏估计方法；2）在最优化似然时，必须最大化生成器的熵。这被假设会提供更好的模式覆盖。与以前的工作不同，我们明确计算了生成样本的密度。这是设计无偏估计分区函数以及计算生成器熵的关键因素。生成密度是通过一种新型的流网络来获得的，称为单向流网络。

    Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
    
[^36]: 检测城市基础设施相互依赖网络中的脆弱节点

    Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])

    [http://arxiv.org/abs/2307.09866](http://arxiv.org/abs/2307.09866)

    该论文使用图神经网络和强化学习对城市基础设施相互依赖网络中的脆弱节点进行了准确建模和分析。

    

    理解和描述城市基础设施的脆弱性对我们具有重要价值，这些基础设施是城市正常运行所必需的工程设施，以网络的形式自然存在。潜在的应用包括保护脆弱设施和设计稳健的拓扑结构等。由于不同拓扑特性和基础设施脆弱性以及其复杂的演化机制之间的强关联，一些启发式分析和机器辅助分析在解决这种场景时存在局限性。在本文中，我们将相互依赖网络建模为异构图，并提出了一种基于图神经网络和强化学习的系统，可以在实际数据上进行训练，准确地描述城市系统的脆弱性。所提出的系统利用深度学习技术来理解和分析异构图，从而能够捕捉级联失败风险。

    Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failu
    
[^37]: 迈向基于人群信息的结构动力学数据驱动模型定义方法

    Towards a population-informed approach to the definition of data-driven models for structural dynamics. (arXiv:2307.09862v1 [cs.LG])

    [http://arxiv.org/abs/2307.09862](http://arxiv.org/abs/2307.09862)

    本论文提出了一种基于人群信息的方法，用于定义结构动力学的数据驱动模型，以解决数据稀缺问题。这种方法结合了物理基础方法和机器学习算法，通过从相似物理现象的人群中学习关系，构建可转移、可解释、值得信赖的模型。

    

    机器学习已经影响了许多领域中多种现象的建模方式，其中之一就是结构动力学领域。然而，由于机器学习算法是问题特定的，他们在数据稀缺的情况下往往效果不佳。为了解决这些问题，结合物理基础方法和机器学习算法的方法被开发出来。虽然这些方法是有效的，但也需要分析者对问题的物理基础有所了解。本研究旨在推动使用从相似物理现象的人群中学习这种关系的模型。这些模型的开发受到物理模型，特别是有限元模型的启发。这些模型被认为是可转移的、可解释的和值得信赖的，而对于机器学习模型来说，这些属性并不是轻松可行的。

    Machine learning has affected the way in which many phenomena for various domains are modelled, one of these domains being that of structural dynamics. However, because machine-learning algorithms are problem-specific, they often fail to perform efficiently in cases of data scarcity. To deal with such issues, combination of physics-based approaches and machine learning algorithms have been developed. Although such methods are effective, they also require the analyser's understanding of the underlying physics of the problem. The current work is aimed at motivating the use of models which learn such relationships from a population of phenomena, whose underlying physics are similar. The development of such models is motivated by the way that physics-based models, and more specifically finite element models, work. Such models are considered transferrable, explainable and trustworthy, attributes which are not trivially imposed or achieved for machine-learning models. For this reason, machin
    
[^38]: 强化学习在信用指数期权对冲中的应用

    Reinforcement Learning for Credit Index Option Hedging. (arXiv:2307.09844v1 [q-fin.TR])

    [http://arxiv.org/abs/2307.09844](http://arxiv.org/abs/2307.09844)

    本文研究了使用强化学习寻找信用指数期权的最佳对冲策略。通过应用信任区域波动率优化算法，证明所得到的对冲策略优于传统的Black & Scholes的Delta对冲策略。

    

    本文着重于使用强化学习找到信用指数期权的最佳对冲策略。我们采用了实用的方法，关注现实性，即离散时间、交易成本；甚至在真实市场数据上测试我们的策略。我们应用了一种先进的算法——信任区域波动率优化（TRVO）算法，并且证明所得到的对冲策略优于从业者的Black & Scholes的Delta对冲策略。

    In this paper, we focus on finding the optimal hedging strategy of a credit index option using reinforcement learning. We take a practical approach, where the focus is on realism i.e. discrete time, transaction costs; even testing our policy on real market data. We apply a state of the art algorithm, the Trust Region Volatility Optimization (TRVO) algorithm and show that the derived hedging strategy outperforms the practitioner's Black & Scholes delta hedge.
    
[^39]: 在几乎线性时间内投影到 $\ell_{1,\infty}$ 球面；稀疏自编码器的应用

    Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders. (arXiv:2307.09836v1 [cs.LG])

    [http://arxiv.org/abs/2307.09836](http://arxiv.org/abs/2307.09836)

    本文提出了一种投影算法，能够在几乎线性时间内将矩阵投影到 $\ell_{1,\infty}$ 球面。该算法易于实现，能够在有限时间内收敛到精确解。同时，将该算法应用于自编码器训练中可以实现特征选择和权重的稀疏化。

    

    现在寻找稀疏性对于加速大规模神经网络的训练至关重要。投影到 $\ell_{1,2}$ 和 $\ell_{1,\infty}$ 是稀疏化和降低神经网络整体成本的最高效技术之一。本文介绍了一种新的 $\ell_{1,\infty}$ 范数球面的投影算法。该算法的最坏时间复杂度为 $\mathcal{O}\big(nm+J\log(nm)\big)$，其中矩阵为 $\mathbb{R}^{n\times m}$。$J$ 是一个在稀疏性高时趋近于0，在稀疏性低时趋近于 $nm$ 的项。该算法易于实现，并保证在有限时间内收敛到精确解。此外，我们提出在训练自编码器时将 $\ell_{1,\infty}$ 球面投影纳入其中，以强制进行特征选择和权重的稀疏化。在我们的生物学应用中，稀疏化主要出现在编码器中，以实现特征选择，因为只有非常小的一部分数据（<2%）是相关的。

    Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks. Projections onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks. In this paper, we introduce a new projection algorithm for the $\ell_{1,\infty}$ norm ball. The worst-case time complexity of this algorithm is $\mathcal{O}\big(nm+J\log(nm)\big)$ for a matrix in $\mathbb{R}^{n\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low. Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time. Moreover, we propose to incorporate the $\ell_{1,\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights. Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($<2\%$) of the data is relevan
    
[^40]: 即使G是Lipschitz或Holder连续的，我们也建立了一类神经深度算子网络(DON)对Lipschitz算子的普遍性和逼近速率界的结果

    Deep Operator Network Approximation Rates for Lipschitz Operators. (arXiv:2307.09835v1 [math.NA])

    [http://arxiv.org/abs/2307.09835](http://arxiv.org/abs/2307.09835)

    该论文研究了一类神经深度算子网络(DON)在对Lipschitz算子的逼近速率界上的普适性，不需要G是全纯的。

    

    我们在可分Hilbert空间之间建立了一个类似于Lipschitz（或Holder）连续映射G：X→Y的神经深度算子网络(DON)的普遍性和表达速率界. DON架构使用线性编码器E和解码器D通过X，Y的（双正交）Riesz基，并使用一个在序列空间l^2(N)上是Lipschitz连续的无限维参数坐标映射的近似网络. 与以前的工作不同，现在的表达速率结果不需要G是全纯的。

    We establish universality and expression rate bounds for a class of neural Deep Operator Networks (DON) emulating Lipschitz (or H\"older) continuous maps $\mathcal G:\mathcal X\to\mathcal Y$ between (subsets of) separable Hilbert spaces $\mathcal X$, $\mathcal Y$. The DON architecture considered uses linear encoders $\mathcal E$ and decoders $\mathcal D$ via (biorthogonal) Riesz bases of $\mathcal X$, $\mathcal Y$, and an approximator network of an infinite-dimensional, parametric coordinate map that is Lipschitz continuous on the sequence space $\ell^2(\mathbb N)$. Unlike previous works ([Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]), which required for example $\mathcal G$ to be holomorphic, the present expression rate results require mere Lipschitz (or H\"older) continu
    
[^41]: 神经网络在图像分类中学到了什么？基于频率的快捷路径视角

    What do neural networks learn in image classification? A frequency shortcut perspective. (arXiv:2307.09829v1 [cs.LG])

    [http://arxiv.org/abs/2307.09829](http://arxiv.org/abs/2307.09829)

    本研究通过在合成数据集和自然图像上的实验，发现神经网络在图像分类中倾向于找到简单的解决方案，并且在训练过程中首先学到的内容取决于最具有区分性的频率特征，这可以是低频或高频。同时，研究也提出了一种度量标准和方法来识别频率快捷路径，并验证了其在不同类型的图像上的有效性。

    

    频率分析对于理解神经网络（NNs）中的表示学习机制非常有用。大部分研究都集中在回归任务中NNs的学习动态上，而很少有关于分类任务的研究。本研究通过实证研究扩展了频率快捷路径的理解。首先，我们在合成数据集上进行实验，设计了在不同频段上具有偏差的数据集。实验结果表明，NNs倾向于找到分类的简单解决方案，而其在训练过程中首先学到的内容取决于最具有区分性的频率特征，可以是低频或高频。其次，我们通过自然图像验证了这一现象。我们提出了衡量类别频率特征的度量标准，并提出了一种识别频率快捷路径的方法。结果表明，频率快捷路径可以基于纹理或形状，具体取决于何种方式能够最好地简化目标。第三，我们验证了转移性。

    Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transf
    
[^42]: 基于多模态学习的疾病预测

    Multi-modal Learning based Prediction for Disease. (arXiv:2307.09823v1 [eess.IV])

    [http://arxiv.org/abs/2307.09823](http://arxiv.org/abs/2307.09823)

    本文提出了一种基于多模态学习的NAFLD预测方法，结合了综合临床数据集和智能方法，为NAFLD的非侵入性诊断提供了一个有前景的解决方案。

    

    非酒精性脂肪性肝病（NAFLD）是慢性肝病的最常见原因，可以通过准确预测来预防晚期纤维化和肝硬化。然而，肝活检作为NAFLD诊断的金标准是侵入性的、昂贵的，并且容易出现采样误差。因此，非侵入性研究非常有前景，但由于缺乏综合研究数据和多模态数据的智能方法，目前仍处于初级阶段。本文提出了一种NAFLD诊断系统（DeepFLDDiag），结合了一个综合临床数据集（FLDData）和一种基于多模态学习的NAFLD预测方法（DeepFLD）。该数据集包括6000多名参与者的体格检查、实验室检查和影像学研究、广泛的问卷调查以及部分参与者的面部图像，对临床研究来说是全面和有价值的。从数据集中，我们定量分析并选择对NAFLD预测最有贡献的临床元数据。

    Non alcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease, which can be predicted accurately to prevent advanced fibrosis and cirrhosis. While, a liver biopsy, the gold standard for NAFLD diagnosis, is invasive, expensive, and prone to sampling errors. Therefore, non-invasive studies are extremely promising, yet they are still in their infancy due to the lack of comprehensive research data and intelligent methods for multi-modal data. This paper proposes a NAFLD diagnosis system (DeepFLDDiag) combining a comprehensive clinical dataset (FLDData) and a multi-modal learning based NAFLD prediction method (DeepFLD). The dataset includes over 6000 participants physical examinations, laboratory and imaging studies, extensive questionnaires, and facial images of partial participants, which is comprehensive and valuable for clinical studies. From the dataset, we quantitatively analyze and select clinical metadata that most contribute to NAFLD prediction. Furthe
    
[^43]: 动态磁共振成像的深度展开收缩网络

    Deep unrolling Shrinkage Network for Dynamic MR imaging. (arXiv:2307.09818v1 [eess.IV])

    [http://arxiv.org/abs/2307.09818](http://arxiv.org/abs/2307.09818)

    本文提出了一种新的运算符AST，来学习每个通道的阈值，在动态磁共振成像方面取得了更好的性能。

    

    利用稀疏先验的深度展开网络在动态磁共振成像方面取得了巨大的成功。通常采用卷积神经网络来提取变换域，然后对CNN变换数据应用软阈值运算符来强制执行稀疏先验。然而，软阈值运算符通常在所有通道上被限制为相同的。在本文中，我们提出了一种新的运算符，称为通道注意力软阈值（AST），该运算符学习每个通道的阈值。特别地，我们提出了一种新的深度展开收缩网络（DUS-Net），通过展开交替方向乘法器（ADMM）来优化变换后的$l_1$范数动态MR重建模型。在一个开放的动态MR数据集上的实验证明，所提出的DUS-Net优于现有方法。源代码可在\url{https:}中获得。

    Deep unrolling networks that utilize sparsity priors have achieved great success in dynamic magnetic resonance (MR) imaging. The convolutional neural network (CNN) is usually utilized to extract the transformed domain, and then the soft thresholding (ST) operator is applied to the CNN-transformed data to enforce the sparsity priors. However, the ST operator is usually constrained to be the same across all channels of the CNN-transformed data. In this paper, we propose a novel operator, called soft thresholding with channel attention (AST), that learns the threshold for each channel. In particular, we put forward a novel deep unrolling shrinkage network (DUS-Net) by unrolling the alternating direction method of multipliers (ADMM) for optimizing the transformed $l_1$ norm dynamic MR reconstruction model. Experimental results on an open-access dynamic cine MR dataset demonstrate that the proposed DUS-Net outperforms the state-of-the-art methods. The source code is available at \url{https:
    
[^44]: 用稀疏正则最优传输进行流形学习

    Manifold Learning with Sparse Regularised Optimal Transport. (arXiv:2307.09816v1 [stat.ML])

    [http://arxiv.org/abs/2307.09816](http://arxiv.org/abs/2307.09816)

    这篇论文介绍了一种利用稀疏正则最优传输进行流形学习的方法，该方法构建了一个稀疏自适应的亲和矩阵，并在连续极限下与拉普拉斯型算子一致。

    

    流形学习是现代统计学和数据科学中的一个核心任务。许多数据集（细胞、文档、图像、分子）可以被表示为嵌入在高维环境空间中的点云，然而数据固有的自由度通常远远少于环境维度的数量。检测数据嵌入的潜在流形是许多下游分析的先决条件。现实世界的数据集经常受到噪声观测和抽样的影响，因此提取关于潜在流形的信息是一个重大挑战。我们提出了一种利用对称版本的最优传输和二次正则化的流形学习方法，它构建了一个稀疏自适应的亲和矩阵，可以解释为双随机核归一化的推广。我们证明了在连续极限下产生的核与拉普拉斯型算子一致，并建立了该方法的健壮性。

    Manifold learning is a central task in modern statistics and data science. Many datasets (cells, documents, images, molecules) can be represented as point clouds embedded in a high dimensional ambient space, however the degrees of freedom intrinsic to the data are usually far fewer than the number of ambient dimensions. The task of detecting a latent manifold along which the data are embedded is a prerequisite for a wide family of downstream analyses. Real-world datasets are subject to noisy observations and sampling, so that distilling information about the underlying manifold is a major challenge. We propose a method for manifold learning that utilises a symmetric version of optimal transport with a quadratic regularisation that constructs a sparse and adaptive affinity matrix, that can be interpreted as a generalisation of the bistochastic kernel normalisation. We prove that the resulting kernel is consistent with a Laplace-type operator in the continuous limit, establish robustness
    
[^45]: GenKL：通过新的广义KL散度，解决Web图像中的标签模糊和标签不符的迭代框架

    GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence. (arXiv:2307.09810v1 [cs.CV])

    [http://arxiv.org/abs/2307.09810](http://arxiv.org/abs/2307.09810)

    提出了一个新的迭代框架GenKL，通过$(\alpha, \beta)$-广义KL散度来解决Web图像中的标签模糊和非符合实例的问题。

    

    在在线整理的Web图像数据集中，存在着模糊（ID）实例和非符合（OOD）实例，我们统称为非符合（NC）实例。在许多最近的解决NC实例产生的负面影响的方法中，核心的隐含假设是可以通过最大熵来找到NC实例。为了定义"熵"，我们将一个实例的输出预测向量解释为一个多项式随机变量的参数向量，相对于一些具有softmax输出层的训练模型。因此，熵最大化是基于理想化假设，即NC实例的预测是"几乎"均匀分布的。然而，在现实世界的Web图像数据集中，有许多NC实例的预测远非均匀分布。为了解决熵最大化的限制，我们提出了$(\alpha, \beta)$-广义KL散度，$\mathcal{D}_{\text{KL}}^{\alpha, beta}$，从而允许更好地处理非均匀分布和标签模糊性。

    Web image datasets curated online inherently contain ambiguous in-distribution (ID) instances and out-of-distribution (OOD) instances, which we collectively call non-conforming (NC) instances. In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization. For "entropy" to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer. Hence, entropy maximization is based on the idealized assumption that NC instances have predictions that are "almost" uniformly distributed. However, in real-world web image datasets, there are numerous NC instances whose predictions are far from being uniformly distributed. To tackle the limitation of entropy maximization, we propose $(\alpha, \beta)$-generalized KL divergence, $\mathcal{D}_{\text{KL}}^{\alpha,
    
[^46]: 基于分散框架的图形联邦学习

    Graph Federated Learning Based on the Decentralized Framework. (arXiv:2307.09801v1 [cs.LG])

    [http://arxiv.org/abs/2307.09801](http://arxiv.org/abs/2307.09801)

    这项研究提出了一种基于分散框架的图形联邦学习方法，通过确定节点之间的置信度并聚合梯度信息来改进联邦学习的准确性和泛化能力，同时解决了中央服务器故障和网络拓扑扩展性差的问题。

    

    图形学习在许多场景中具有广泛的应用，这些场景需要更多的数据隐私保护。联邦学习是一种新兴的分布式机器学习方法，它利用个体设备或数据中心的数据来提高模型的准确性和泛化能力，同时保护用户数据的隐私。图形联邦学习主要基于经典的联邦学习框架，即客户端-服务器框架。然而，客户端-服务器框架面临诸如中央服务器的单点故障和网络拓扑的扩展性差等问题。首先，我们引入了分散框架到图形联邦学习中。其次，基于节点之间的数据相似性确定节点之间的置信度，然后通过线性加权基于置信度来聚合梯度信息。最后，我们将所提出的方法与FedAvg、Fedprox、GCFL和GCFL+进行比较，以验证其有效性。

    Graph learning has a wide range of applications in many scenarios, which require more need for data privacy. Federated learning is an emerging distributed machine learning approach that leverages data from individual devices or data centers to improve the accuracy and generalization of the model, while also protecting the privacy of user data. Graph-federated learning is mainly based on the classical federated learning framework i.e., the Client-Server framework. However, the Client-Server framework faces problems such as a single point of failure of the central server and poor scalability of network topology. First, we introduce the decentralized framework to graph-federated learning. Second, determine the confidence among nodes based on the similarity of data among nodes, subsequently, the gradient information is then aggregated by linear weighting based on confidence. Finally, the proposed method is compared with FedAvg, Fedprox, GCFL, and GCFL+ to verify the effectiveness of the pr
    
[^47]: 具有一致聚合的概率预测

    Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])

    [http://arxiv.org/abs/2307.09797](http://arxiv.org/abs/2307.09797)

    该论文提出了一种新的模型，利用因子模型结构来产生遵守层次结构的概率预测。模型利用卷积神经网络生成参数，并通过优化样本损失函数实现预测优化。

    

    在许多应用中，准确获得遵守层次结构的概率预测是一项重要的运营挑战，特别是在能源管理、供应链规划和资源配置等领域。对于多变量预测，基本挑战在于预测通常需要与层次结构保持一致。在本文中，我们提出了一种新的模型，利用因子模型结构通过构建来产生一致的预测。这是一个简单的观察结果（可交换性）：置换层次结构中的基本级别序列不会改变它们的聚合。我们的模型使用卷积神经网络来生成因子、它们的加载和基本级别分布的参数；它产生可以根据模型参数进行微分的样本；因此它可以对任何基于样本的损失函数进行优化，包括连续排名概率损失函数。

    Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probabili
    
[^48]: 预测早期时间序列的元学习方法

    Forecasting Early with Meta Learning. (arXiv:2307.09796v1 [cs.LG])

    [http://arxiv.org/abs/2307.09796](http://arxiv.org/abs/2307.09796)

    本文介绍了一种利用元学习方法预测早期时间序列的方法，通过对目标数据集进行对抗学习和利用额外样本增强时间序列，在多个数据集中进行元学习，并在预测性能上优于单任务学习、联合学习、多任务学习和经典预测基线方法。

    

    在时间序列的早期观察期中，可能只有少量历史观察可用于学习模型。然而，在存在先前数据集的情况下，可以应用元学习方法。本文提出了一种利用额外数据集样本进行元学习的方法，通过对目标数据集进行对抗学习，学习增强时间序列的辅助任务。我们的模型(FEML)配备共享的卷积主干网络，可以学习来自不同数据集的不同长度输入的特征，并具有针对不同输出长度的数据集特定预测头。我们证明FEML可以在数据集之间进行元学习，并通过在对抗生成的样本上进行附加学习，提高与单任务学习、联合学习、多任务学习和经典预测基线方法相比的预测性能。

    In the early observation period of a time series, there might be only a few historic observations available to learn a model. However, in cases where an existing prior set of datasets is available, Meta learning methods can be applicable. In this paper, we devise a Meta learning method that exploits samples from additional datasets and learns to augment time series through adversarial learning as an auxiliary task for the target dataset. Our model (FEML), is equipped with a shared Convolutional backbone that learns features for varying length inputs from different datasets and has dataset specific heads to forecast for different output lengths. We show that FEML can meta learn across datasets and by additionally learning on adversarial generated samples as auxiliary samples for the target dataset, it can improve the forecasting performance compared to single task learning, and various solutions adapted from Joint learning, Multi-task learning and classic forecasting baselines.
    
[^49]: 从西方到东方：谁更能理解其他人的音乐？

    From West to East: Who can understand the music of the others better?. (arXiv:2307.09795v1 [cs.SD])

    [http://arxiv.org/abs/2307.09795](http://arxiv.org/abs/2307.09795)

    这项研究探讨了基于迁移学习方法的音频嵌入模型在不同音乐文化和风格中的适用性，并提出了关于跨文化音乐理解的研究问题。

    

    最近，在音乐信息检索领域的发展已经出现了一些基准深度学习模型，这些模型的嵌入可以用于各种下游任务。与此同时，绝大多数这些模型都是在西方流行/摇滚音乐和相关风格上进行训练的。这引发了一个研究问题，即这些模型是否可以用来学习不同音乐文化和风格的表示，或者我们是否可以构建类似的音乐音频嵌入模型，训练数据来自不同的文化或风格。为此，我们利用迁移学习方法来了解数据所属不同音乐文化之间的相似性。我们使用了两个西方音乐数据集，两个来自东地中海文化的传统/民间数据集，以及两个属于印度艺术音乐的数据集。我们训练了三个深度音频嵌入模型，并跨领域进行了转移学习，其中包括两个基于CNN的模型和一个Transformer-based的模型，以在每个目标中进行自动标记。

    Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each targe
    
[^50]: 计算递归教学维度的难度的注释

    A Note on Hardness of Computing Recursive Teaching Dimension. (arXiv:2307.09792v1 [cs.CC])

    [http://arxiv.org/abs/2307.09792](http://arxiv.org/abs/2307.09792)

    本文研究了计算递归教学维度（RTD）问题的难度，证明了在指数时间假设（ETH）下，该问题的计算时间为$n^{\Omega(\log n)}$，与暴力算法的运行时间相匹配。

    

    在这个简短的注释中，我们展示了计算递归教学维度（RTD）问题（给定作为输入的概念类）需要$n^{\Omega(\log n)}$的时间，假设指数时间假设（ETH）成立。这与问题的暴力算法的运行时间$n^{O(\log n)}$相匹配。

    In this short note, we show that the problem of computing the recursive teaching dimension (RTD) for a concept class (given explicitly as input) requires $n^{\Omega(\log n)}$-time, assuming the exponential time hypothesis (ETH). This matches the running time $n^{O(\log n)}$ of the brute-force algorithm for the problem.
    
[^51]: ZeroQuant-FP: 使用浮点格式进行LLMs训练后量化的一项飞跃

    ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])

    [http://arxiv.org/abs/2307.09782](http://arxiv.org/abs/2307.09782)

    ZeroQuant-FP通过使用浮点格式进行LLMs训练后量化，解决了在大型语言模型中平衡计算效率和保持模型质量的挑战，并发现FP8激活优于INT8，并且FP4权重表现与INT4相当甚至更优。

    

    在大型语言模型（LLMs）的复杂领域中，平衡计算效率和保持模型质量是一个巨大的挑战。本研究通过探讨浮点（FP）量化的可行性，特别关注FP8和FP4，以应对均匀量化的固有限制，尤其是处理离群值，并受到NVIDIA H100硬件的启发。我们的全面调查发现，在LLMs中，FP8激活始终优于其整数（INT8）等效，性能优势在包含超过十亿参数的模型中更为明显。对于权重量化，我们的研究结果表明，FP4的性能与INT4相当，甚至更优，简化了在像H100这样支持FP的硬件上的部署。为了减少由权重和激活之间差异引起的精度对齐开销，我们提出了两个缩放约束。

    In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints
    
[^52]: Text2Layer: 使用潜在扩散模型进行分层图像生成

    Text2Layer: Layered Image Generation using Latent Diffusion Model. (arXiv:2307.09781v1 [cs.CV])

    [http://arxiv.org/abs/2307.09781](http://arxiv.org/abs/2307.09781)

    该论文提出了一种使用潜在扩散模型进行分层图像生成的方法，使得在生成高质量图像的同时，实现更好的合成工作流程和更高质量的图层遮罩生成。

    

    图层合成是业余爱好者和专业人士中最受欢迎的图像编辑工作流之一。受到扩散模型的成功启发，我们从分层图像生成的角度探索图层合成问题。我们提出了一种同时生成背景、前景、图层遮罩和合成图像的方法，而不仅仅是生成一幅图像。为了实现分层图像生成，我们训练了一个自动编码器来重建分层图像，并在潜在表示上训练了扩散模型。该方法的一个好处是在高质量图像输出之外，可以实现更好的合成工作流程。另一个好处是生成比通过图像分割的独立步骤产生的图层遮罩更高质量的图层遮罩。实验结果表明，所提出的方法能够生成高质量的分层图像，并为未来的工作奠定了基准。

    Layer compositing is one of the most popular image editing workflows among both amateurs and professionals. Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective. Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously. To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation. One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output. Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation. Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.
    
[^53]: 超越单一特征重要性的新方法ICECREAM

    Beyond Single-Feature Importance with ICECREAM. (arXiv:2307.09779v1 [cs.LG])

    [http://arxiv.org/abs/2307.09779](http://arxiv.org/abs/2307.09779)

    提出了一种名为ICECREAM的新方法，超越了单一特征重要性的统计方法。该方法利用信息论量化衡量了变量联合对目标变量分布的影响，能够确定一个特定结果所需的关键因素集合。

    

    机器学习模型输出的确定特征集合是哪些？云计算应用失败的原因是哪些组件？本研究通过一种基于联合的可解释性方法ICECREAM，解答了这两个问题。具体来说，我们提出了一种信息论量化方法，用于衡量变量联合对目标变量分布的影响。相比于现有的解释性和因果贡献分析方法，该方法能够确定一个特定结果所需的关键因素集合，而不仅仅是对个体因素进行重要性排序。在合成数据和真实数据的实验中，我们展示了ICECREAM在解释性和根本原因分析方面超越了现有方法，并在两个任务上取得了令人印象深刻的准确性。

    Which set of features was responsible for a certain output of a machine learning model? Which components caused the failure of a cloud computing application? These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM). Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.
    
[^54]: 一种新的时空变分量子电路，用于在NISQ设备上实现深度学习

    A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices. (arXiv:2307.09771v1 [quant-ph])

    [http://arxiv.org/abs/2307.09771](http://arxiv.org/abs/2307.09771)

    该论文提出了一种新的时空变分量子电路(ST-VQC)用于在NISQ设备上实现深度学习，通过整合非线性特征提高学习模型对噪声的鲁棒性，并通过独特的编码量子子电路和逐层计算的量子子电路实现基于时间的深度学习。

    

    量子计算以其在高维度通过叠加和纠缠进行极并行计算的能力，为机器学习提供了一种有前景的方法。尽管有潜力，但现有的量子学习算法，如变分量子电路（VQC），在处理更复杂的数据集，特别是那些不可线性分离的数据集时面临挑战。此外，它还遇到了可部署性问题，使得将学习模型部署到实际的量子设备后，其准确率大幅下降。为了克服这些限制，本文提出了一种新颖的时空设计，即ST-VQC，以将非线性整合到量子学习中，并提高学习模型对噪声的鲁棒性。具体而言，ST-VQC可以通过一种新型的基于块的编码量子子电路和逐层计算的量子子电路来提取空间特征，从而实现基于时间的深度学习。此外，还提出了一种无SWAP物理电路。

    Quantum computing presents a promising approach for machine learning with its capability for extremely parallel computation in high-dimension through superposition and entanglement. Despite its potential, existing quantum learning algorithms, such as Variational Quantum Circuits(VQCs), face challenges in handling more complex datasets, particularly those that are not linearly separable. What's more, it encounters the deployability issue, making the learning models suffer a drastic accuracy drop after deploying them to the actual quantum devices. To overcome these limitations, this paper proposes a novel spatial-temporal design, namely ST-VQC, to integrate non-linearity in quantum learning and improve the robustness of the learning model to noise. Specifically, ST-VQC can extract spatial features via a novel block-based encoding quantum sub-circuit coupled with a layer-wise computation quantum sub-circuit to enable temporal-wise deep learning. Additionally, a SWAP-Free physical circuit 
    
[^55]: Sig-Splines：时间序列生成模型的通用逼近和凸校准

    Sig-Splines: universal approximation and convex calibration of time series generative models. (arXiv:2307.09767v1 [cs.LG])

    [http://arxiv.org/abs/2307.09767](http://arxiv.org/abs/2307.09767)

    该论文提出了一种新颖的时间序列生成模型，通过将线性变换和签名变换作为传统神经网络的替代，既实现了神经网络的通用性，又引入了模型参数的凸性。

    

    我们提出了一种新颖的多变量离散时间序列数据生成模型。受神经样条流构造的启发，我们的算法将线性变换和签名变换作为传统神经网络的无缝替代。这种方法不仅实现了神经网络固有的通用性，还引入了模型参数的凸性。

    We propose a novel generative model for multivariate discrete-time time series data. Drawing inspiration from the construction of neural spline flows, our algorithm incorporates linear transformations and the signature transform as a seamless substitution for traditional neural networks. This approach enables us to achieve not only the universality property inherent in neural networks but also introduces convexity in the model's parameters.
    
[^56]: 通过随机滤波和模式识别强化基于POD的反应扩散复杂网络模型简化技术

    Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])

    [http://arxiv.org/abs/2307.09762](http://arxiv.org/abs/2307.09762)

    该论文提出了一种算法框架，通过将模式识别和随机滤波理论的技术结合起来，强化了基于POD的反应扩散复杂网络模型简化技术，在受扰动输入的情况下提高了代理模型的准确性。

    

    复杂网络被用于建模许多现实世界系统，然而这些系统的维度使得其分析变得困难。在这种情况下，可以使用POD等降维技术。然而，这些模型容易受输入数据扰动的影响。我们提出了一种算法框架，将模式识别和随机滤波理论的技术结合起来，以增强这些模型的输出。研究结果表明，我们的方法可以在受扰动输入的情况下提高代理模型的准确性。深度神经网络(DNNs)容易受到对抗性攻击，然而最近的研究发现，神经常微分方程(ODEs)在特定应用中表现出鲁棒性。我们将我们的算法框架与基于神经ODE的方法进行了基准比较。

    Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
    
[^57]: 使用零频谱偏差构建极限学习机

    Constructing Extreme Learning Machines with zero Spectral Bias. (arXiv:2307.09759v1 [cs.LG])

    [http://arxiv.org/abs/2307.09759](http://arxiv.org/abs/2307.09759)

    本研究验证了极限学习机（ELM）不存在谱偏差（SB），并通过实施傅立叶特征嵌入的变体消除了谱偏差，并且成功将ELM应用于高频分辨率至关重要的问题（如PINNs）。

    

    无谱偏差是一种在前馈人工神经网络中观察到的现象，即学习的函数的高频成分收敛速度比低频成分慢。这在各种人工神经网络中普遍存在，对于那些高频分辨率至关重要的领域，如物理信息神经网络（PINNs），这造成了技术挑战。极限学习机（ELM）可以消除解算过程中产生谱偏差（SB）的理论基础，理论上应该没有谱偏差。本研究验证了这一假设的可靠性，并证明它是不正确的。然而，ELM的结构使它们自然地适合于实施傅立叶特征嵌入的变体，已经证明可以减轻人工神经网络中的谱偏差。本研究实施并验证了这种方法完全消除了谱偏差，因此使ELM在实际问题（如PINNs）的应用成为可行。

    The phenomena of Spectral Bias, where the higher frequency components of a function being learnt in a feedforward Artificial Neural Network (ANN) are seen to converge more slowly than the lower frequencies, is observed ubiquitously across ANNs. This has created technology challenges in fields where resolution of higher frequencies is crucial, like in Physics Informed Neural Networks (PINNs). Extreme Learning Machines (ELMs) that obviate an iterative solution process which provides the theoretical basis of Spectral Bias (SB), should in principle be free of the same. This work verifies the reliability of this assumption, and shows that it is incorrect. However, the structure of ELMs makes them naturally amenable to implementation of variants of Fourier Feature Embeddings, which have been shown to mitigate SB in ANNs. This approach is implemented and verified to completely eliminate SB, thus bringing into feasibility the application of ELMs for practical problems like PINNs where resoluti
    
[^58]: 提升数据集压缩的分布匹配方法

    Improved Distribution Matching for Dataset Condensation. (arXiv:2307.09742v1 [cs.LG])

    [http://arxiv.org/abs/2307.09742](http://arxiv.org/abs/2307.09742)

    本文提出了一种基于分布匹配的新型数据集压缩方法，通过解决朴素分布匹配的两个缺点，提出了三种新技术，实现了更高效和有前景的数据集压缩。

    

    数据集压缩旨在将大型数据集压缩成较小的数据集，同时保持其训练模型的能力，减少深度学习应用中的存储成本和训练工作量。然而，传统的数据集压缩方法是以优化为导向的，通过在模型优化过程中执行梯度或参数匹配来压缩数据集，即使在小型数据集和模型上也需要大量的计算资源。本文提出了一种基于分布匹配的新型数据集压缩方法，它更加高效和有前景。具体来说，我们针对朴素分布匹配的两个重要缺点（即不平衡的特征数量和无效的嵌入距离计算）提出了三种新技术（即分区和扩展增强、高效和丰富的模型采样和类别感知的分布正则化）。我们的简单而有效的方法优于大多数先前的方法。

    Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previo
    
[^59]: RaTE: 通过填补空白实现可重复的自动分类评估

    RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap. (arXiv:2307.09706v1 [cs.CL])

    [http://arxiv.org/abs/2307.09706](http://arxiv.org/abs/2307.09706)

    RaTE是一种无标签的自动分类评分方法，它通过大型预训练的语言模型实现可重复的自动分类评估。结果表明，RaTE与人类评判具有较高的相关性，并且人为降低分类法会导致RaTE评分下降。

    

    分类法对于知识表示是必不可少的，然而，大多数关于自动分类构建的研究仍然依赖于人工评估来评分提出的算法。我们认为自动分类评估和分类构建一样重要。我们提出了RaTE，一种无标签的自动分类评分方法，它依赖于一个大型预训练的语言模型。我们将该评估方法应用于三种最先进的自动分类构建算法，并从Yelp领域构建了七个分类法，结果显示：1）RaTE与人类评判的相关性较高；2）人为降低分类法会导致RaTE评分下降。

    Taxonomies are an essential knowledge representation, yet most studies on automatic taxonomy construction (ATC) resort to manual evaluation to score proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just as important as taxonomy construction. We propose RaTE, an automatic label-free taxonomy scoring procedure, which relies on a large pre-trained language model. We apply our evaluation procedure to three state-of-the-art ATC algorithms with which we built seven taxonomies from the Yelp domain, and show that 1) RaTE correlates well with human judgments and 2) artificially degrading a taxonomy leads to decreasing RaTE score.
    
[^60]: 高效的LLM引导生成

    Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])

    [http://arxiv.org/abs/2307.09702](http://arxiv.org/abs/2307.09702)

    本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。

    

    在本文中，我们描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。我们的方法在标记序列生成过程中几乎不增加任何开销，并使得引导生成在实际中可行。在开源Python库Outlines中提供了一个实现。

    In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
    
[^61]: STRAPPER：通过自训练增强和同伴正则化实现基于偏好的强化学习

    STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization. (arXiv:2307.09692v1 [cs.LG])

    [http://arxiv.org/abs/2307.09692](http://arxiv.org/abs/2307.09692)

    本文提出了STRAPPER方法，通过自训练增强和同伴正则化实现基于偏好的强化学习。与其他方法不同的是，作者发现基于偏好的强化学习中存在相似性陷阱现象，即相似的片段对可能会存在截然相反的偏好，对一致性正则化造成影响。

    

    基于偏好的强化学习（PbRL）承诺通过二进制人类偏好学习复杂的奖励函数。然而，这种人类参与的形式需要大量的人力来为片段对分配偏好标签，从而阻碍了其大规模应用。最近的方法尝试重复使用未标记的片段，隐含地阐明了片段的分布，从而减轻了人们的努力。并且进一步考虑了一致性正则化来提高半监督学习的性能。然而，我们注意到，与普通的分类任务不同，PbRL中存在一个我们在本文中定义为相似性陷阱的独特现象。直观地说，人类对于相似的片段对可能会存在截然相反的偏好，但这种相似性可能会导致一致性正则化在PbRL中失败。由于相似性陷阱的存在，这样的一致性正则化不适当地增强了模型预测的一致性可能性。

    Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's pr
    
[^62]: 在协作式MEC系统中的联合服务缓存、通信和计算资源分配：基于DRL的双时间尺度方法

    Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach. (arXiv:2307.09691v1 [cs.NI])

    [http://arxiv.org/abs/2307.09691](http://arxiv.org/abs/2307.09691)

    该论文提出了一个基于DRL的双时间尺度方案，旨在通过联合优化服务缓存、协作卸载和计算通信资源分配来提高MEC系统中的长期服务质量并降低缓存切换成本。

    

    由于多维资源的限制，满足终端的严格服务质量要求对多接入边缘计算（MEC）系统构成了重要挑战。为了应对这个挑战，我们提出了一个协作式MEC框架，促进边缘服务器之间的资源共享，并通过联合优化服务缓存、协作卸载、计算和通信资源分配来最大化长期的服务质量和降低缓存切换成本。服务缓存和其他资源分配之间的双时间尺度特性和时间回归关系使解决问题变得更加困难。为了解决这个问题，我们提出了一种基于深度强化学习（DRL）的双时间尺度方案，称为DGL-DDPG，它由短期遗传算法（GA）和基于长短期记忆网络的深度确定性策略梯度（LSTM-DDPG）组成。

    Meeting the strict Quality of Service (QoS) requirements of terminals has imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems, due to the limited multidimensional resources. To address this challenge, we propose a collaborative MEC framework that facilitates resource sharing between the edge servers, and with the aim to maximize the long-term QoS and reduce the cache switching cost through joint optimization of service caching, collaborative offfoading, and computation and communication resource allocation. The dual timescale feature and temporal recurrence relationship between service caching and other resource allocation make solving the problem even more challenging. To solve it, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate the optimizatio
    
[^63]: Amazon-M2: 一个用于推荐和文本生成的多语言多区域购物会话数据集

    Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])

    [http://arxiv.org/abs/2307.09688](http://arxiv.org/abs/2307.09688)

    Amazon-M2是一个多语言多区域购物会话数据集，可以增强个性化推荐和理解用户偏好能力。

    

    对于电子商务来说，建模客户购物意图是一个重要的任务，因为它直接影响用户体验和参与度。因此，准确理解客户的偏好对于提供个性化推荐至关重要。基于会话的推荐技术利用客户会话数据来预测他们的下一次互动，已经越来越受到欢迎。然而，现有的会话数据集在项目属性、用户多样性和数据集规模方面存在局限性。因此，它们不能全面地捕捉用户行为和偏好的谱系。为了弥补这一差距，我们提出了Amazon Multilingual Multi-locale Shopping Session Dataset，即Amazon-M2。它是第一个由来自六个不同区域的数百万用户会话组成的多语言数据集，其中产品的主要语言是英语、德语、日语、法语、意大利语和西班牙语。值得注意的是，这个数据集可以帮助我们增强个性化和理解用户偏好能力。

    Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, w
    
[^64]: ReLU层的凸几何、球上的可逆性和局部重构研究

    Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction. (arXiv:2307.09672v1 [cs.LG])

    [http://arxiv.org/abs/2307.09672](http://arxiv.org/abs/2307.09672)

    本文研究了ReLU层在闭球和非负部分上的可逆性，并提供了使用凸几何视角和框架理论中的对偶概念来验证和重构ReLU层的计算方法。

    

    本文使用框架理论的方法研究了ReLU层在闭球和非负部分上的可逆性。特别强调了球的半径和偏置向量之间的相互作用。结合了凸几何的视角，得到了一种可行的计算方法，可以在合理的约束条件下验证ReLU层的可逆性，其中约束条件是偏置向量的上界。文中还提供了明确的重构公式，灵感来自框架理论中的对偶概念。所有这些都为量化ReLU层的可逆性以及球上任意输入向量的具体重构算法提供了可能性。

    The paper uses a frame-theoretic setting to study the injectivity of a ReLU-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part. In particular, the interplay between the radius of the ball and the bias vector is emphasized. Together with a perspective from convex geometry, this leads to a computationally feasible method of verifying the injectivity of a ReLU-layer under reasonable restrictions in terms of an upper bound of the bias vector. Explicit reconstruction formulas are provided, inspired by the duality concept from frame theory. All this gives rise to the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.
    
[^65]: JAZZVAR：一份包含爵士乐标准曲独奏中变奏的数据集，用于音乐重绘

    JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting. (arXiv:2307.09670v1 [cs.SD])

    [http://arxiv.org/abs/2307.09670](http://arxiv.org/abs/2307.09670)

    JAZZVAR是一份包含爵士乐标准曲独奏中变奏的数据集，为音乐重绘任务提供了基础。

    

    爵士钢琴家常常会以独特的方式演绎爵士乐标准曲。这些演绎中的片段可以被视为变奏的部分。我们从独奏爵士钢琴演奏中手动提取了这些变奏。JAZZVAR数据集是一组502个变奏和原始MIDI片段的集合。数据集中的每个变奏都有一个相应的原始片段，其中包含了原始爵士乐标准曲的旋律和和弦。我们的方法不同于许多现有的爵士乐数据集，这些数据集通常专注于爵士乐演奏中的即兴演奏部分。在本文中，我们介绍了获取和排序曲目的策划过程，创建原始片段和变奏对的流程，以及我们对数据集的分析。我们还引入了一项新的生成音乐任务——音乐重绘，并介绍了一个在JAZZVAR数据集上训练的基线Transformer模型用于这个任务。我们的数据集还有其他潜在的应用。

    Jazz pianists often uniquely interpret jazz standards. Passages from these interpretations can be viewed as sections of variation. We manually extracted such variations from solo jazz piano performances. The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments. Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard. Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances. In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset. We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task. Other potential applications of our dataset inc
    
[^66]: 迈向具有基础模型的统一智能体

    Towards A Unified Agent with Foundation Models. (arXiv:2307.09668v1 [cs.RO])

    [http://arxiv.org/abs/2307.09668](http://arxiv.org/abs/2307.09668)

    本文研究如何在强化学习智能体中嵌入和利用语言模型和视觉语言模型的能力，设计了一个以语言为核心推理工具的框架，并在稀疏奖励的机器人操作环境中测试了该方法。结果显示，该方法在探索效率和数据复用方面具有显著性能提升，并展示了如何通过复用学到的技能解决新任务。

    

    最近，语言模型和视觉语言模型在理解人类意图、推理、场景理解和规划行为等方面展示了前所未有的能力。在这项工作中，我们探讨了如何将这些能力嵌入和利用在强化学习（RL）智能体中。我们设计了一个以语言作为核心推理工具的框架，探索了这如何使智能体能够应对一系列基础RL挑战，如高效探索、复用经验数据、调度技能和从观察中学习，这些传统上需要单独设计的垂直算法。我们在稀疏奖励的模拟机器人操作环境中测试了我们的方法，其中机器人需要堆叠一组物体。我们展示了在探索效率和能够从离线数据集中复用数据方面与基线方法相比的显著性能提升，并且展示了如何通过复用学到的技能解决新任务。

    Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve no
    
[^67]: 使用动态图转换器预测研究社区中的技术专长和能力演进

    Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers. (arXiv:2307.09665v1 [cs.LG])

    [http://arxiv.org/abs/2307.09665](http://arxiv.org/abs/2307.09665)

    这项研究通过使用动态图转换器预测研究社区中的技术专长和能力演进，从而提高了全球安全和核不扩散等领域的预测能力。

    

    能够预测全球技术专长和能力演进趋势对国家和全球安全非常重要，特别是在核不扩散（NN）等安全关键领域和人工智能（AI）等快速兴起的领域。本研究扩展了传统的统计关系学习方法（例如，协作网络中的链接预测），并制定了使用动态异构图表示来预测技术专长和能力演进的问题。我们开发了新的能力来预测不同粒度（例如科学家和机构级别）的协作模式，作者行为和技术能力演进。我们实现了一种动态图转换器（DGT）神经架构，通过（a）预测异构（而不是同构）节点和边缘，并（b）依赖于离散和连续特征，推动了最先进的图神经网络模型的发展。

    The ability to anticipate technical expertise and capability evolution trends globally is essential for national and global security, especially in safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging fields like artificial intelligence (AI). In this work, we extend traditional statistical relational learning approaches (e.g., link prediction in collaboration networks) and formulate a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. We develop novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities (e.g., scientist and institution levels) in two distinct research fields. We implement a dynamic graph transformer (DGT) neural architecture, which pushes the state-of-the-art graph neural network models by (a) forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b) relying on both discrete -- 
    
[^68]: 基于物理的贝叶斯优化的引导波传播不确定性量化的减阶建模

    Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization. (arXiv:2307.09661v1 [cs.LG])

    [http://arxiv.org/abs/2307.09661](http://arxiv.org/abs/2307.09661)

    本论文提出了一种基于机器学习的减阶建模方法，称为BO-ML-ROM，用于减少计算时间和成本，并同时实现引导波传播的不确定性量化。

    

    在数字孪生背景下，结构健康监测（SHM）构成了基于状态的维护的支柱，促进了虚拟和物理资产之间的互连。引导波传播（GWP）通常用于SHM中的结构检测。然而，GWP对结构材料性质的变化非常敏感，容易导致错误告警。在这个方向上，经常应用不确定性量化（UQ）来提高预测的可靠性。计算力学是模拟GWP的有用工具，通常用于UQ。即便如此，UQ方法的应用需要大量的模拟，而大规模的瞬态数值GWP解决方案会增加计算开销。常用的减阶模型（ROMs）通常用于在有限的时间内提供数值结果。在本文中，我们提出了一种基于机器学习（ML）的ROM，即BO-ML-ROM，以减少与计算时间相关的成本

    In the context of digital twins, structural health monitoring (SHM) constitutes the backbone of condition-based maintenance, facilitating the interconnection between virtual and physical assets. Guided wave propagation (GWP) is commonly employed for the inspection of structures in SHM. However, GWP is sensitive to variations in the material properties of the structure, leading to false alarms. In this direction, uncertainty quantification (UQ) is regularly applied to improve the reliability of predictions. Computational mechanics is a useful tool for the simulation of GWP, and is often applied for UQ. Even so, the application of UQ methods requires numerous simulations, while large-scale, transient numerical GWP solutions increase the computational cost. Reduced order models (ROMs) are commonly employed to provide numerical results in a limited amount of time. In this paper, we propose a machine learning (ML)-based ROM, mentioned as BO-ML-ROM, to decrease the computational time related
    
[^69]: 图神经网络的神经优先队列

    Neural Priority Queues for Graph Neural Networks. (arXiv:2307.09660v1 [cs.LG])

    [http://arxiv.org/abs/2307.09660](http://arxiv.org/abs/2307.09660)

    本文提出了神经优先队列，这是一种用于图神经网络的可微分的模块，并通过在不同数据集上的实验结果验证了其在算法推理和捕捉长距离交互方面的有效性。

    

    图神经网络（GNN）在神经算法推理中取得了可观的成功。许多传统算法使用显式内存来表示数据结构。然而，对于将GNN与外部内存相结合的探索还存在局限性。本文提出了神经优先队列，这是一种可微分的算法优先队列，用于GNN。我们提出并推动了一种内存模块的期望，证明了神经优先队列满足该期望，并通过对算法推理的理解来解释其使用。通过在CLRS-30数据集上的实证结果进一步证明了这一点。此外，我们发现神经优先队列在捕捉长距离交互方面非常有用，这在Long-Range Graph Benchmark的数据集上得到了实证。

    Graph Neural Networks (GNNs) have shown considerable success in neural algorithmic reasoning. Many traditional algorithms make use of an explicit memory in the form of a data structure. However, there has been limited exploration on augmenting GNNs with external memory. In this paper, we present Neural Priority Queues, a differentiable analogue to algorithmic priority queues, for GNNs. We propose and motivate a desiderata for memory modules, and show that Neural PQs exhibit the desiderata, and reason about their use with algorithmic reasoning. This is further demonstrated by empirical results on the CLRS-30 dataset. Furthermore, we find the Neural PQs useful in capturing long-range interactions, as empirically shown on a dataset from the Long-Range Graph Benchmark.
    
[^70]: HAT-CL: 用于连续学习的基于任务的硬注意力PyTorch库

    HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])

    [http://arxiv.org/abs/2307.09653](http://arxiv.org/abs/2307.09653)

    HAT-CL是一个基于任务的硬注意力PyTorch库，以提供对连续学习中的灾难性遗忘现象的解决方案。它通过改善HAT的可用性和兼容性问题，并提供对现有网络复用的支持，实现了对PyTorch模块的自动化梯度操作和转换。此外，HAT-CL还引入了新颖的掩码操作技术。

    

    连续学习中的灾难性遗忘现象，即神经网络在学习新任务时丧失先前获得的知识，给人们带来了重大挑战。硬注意力任务(HAT)机制在减轻这个问题方面已经显示出潜力，但其实际实现受到了可用性和兼容性问题的影响，并且缺乏对现有网络复用的支持。在本文中，我们介绍了HAT-CL，这是HAT机制的用户友好、与PyTorch兼容的重新设计。HAT-CL不仅自动化了梯度操作，还简化了PyTorch模块转化为HAT模块的过程。它通过提供一套全面的模块，可以无缝地集成到现有的架构中。此外，HAT-CL还提供了与TIMM库平滑集成的可用的HAT网络。除了对HAT的重新设计和重新实现之外，我们还介绍了用于HAT的新颖的掩码操作技术。

    Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,
    
[^71]: BadNets在垃圾邮件过滤器中的应用

    Application of BadNets in Spam Filters. (arXiv:2307.09649v1 [cs.CR])

    [http://arxiv.org/abs/2307.09649](http://arxiv.org/abs/2307.09649)

    本研究利用BadNets设计了一种后门攻击，揭示了垃圾邮件过滤器中机器学习模型的潜在漏洞，强调了评估和改进的重要性。

    

    垃圾邮件过滤器是现代电子邮件系统的重要组成部分，它帮助用户防止受到不需要和潜在有害的邮件的侵扰。然而，这些过滤器的有效性取决于驱动它们的机器学习模型的质量。本文中，我们设计了在垃圾邮件过滤领域中的后门攻击。通过展示机器学习模型供应链中的潜在漏洞，我们强调了对垃圾邮件过滤器中使用的模型进行仔细考虑和评估的必要性。我们的结果表明，后门攻击能够有效地用于识别垃圾邮件过滤器中的漏洞，并提出了对该领域进行持续监控和改进的需求。

    Spam filters are a crucial component of modern email systems, as they help to protect users from unwanted and potentially harmful emails. However, the effectiveness of these filters is dependent on the quality of the machine learning models that power them. In this paper, we design backdoor attacks in the domain of spam filtering. By demonstrating the potential vulnerabilities in the machine learning model supply chain, we highlight the need for careful consideration and evaluation of the models used in spam filters. Our results show that the backdoor attacks can be effectively used to identify vulnerabilities in spam filters and suggest the need for ongoing monitoring and improvement in this area.
    
[^72]: 通过关键阶段促进记忆增强型Adam的探索

    Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])

    [http://arxiv.org/abs/2307.09638](http://arxiv.org/abs/2307.09638)

    本研究提出了一种记忆增强型Adam方法，通过使用关键动量项的缓冲区来促进对更平坦最小值的探索。实验证明，该方法在标准的监督语言建模和图像分类任务中提高了几种Adam变体的性能。

    

    自适应梯度优化器，特别是Adam，在训练大规模深度学习模型中发挥了重要作用。这种优化器的优势在于其快速收敛性，同时对超参数的选择更加鲁棒。然而，它们通常比非自适应方法泛化效果更差。最近的研究将这种性能差距归因于选择平坦最小值：自适应方法倾向于在损失函数曲面中更尖锐的盆地中寻找解决方案，从而损害了泛化能力。为了克服这个问题，我们提出了一种新的记忆增强型Adam方法，在训练过程中使用关键动量项的缓冲区来促进对更平坦最小值的探索。直观地说，缓冲区的使用使得优化器如果盆地的吸引范围不够宽，就会超出其范围。我们经验性地证明了我们的方法在标准的监督语言建模和图像分类任务上提高了几种Adam变体的性能。

    Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
    
[^73]: 深度强化学习用于ESG金融投资组合管理

    Deep Reinforcement Learning for ESG financial portfolio management. (arXiv:2307.09631v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.09631](http://arxiv.org/abs/2307.09631)

    本文研究了深度强化学习在ESG金融投资组合管理中的应用，结果显示在按公司ESG评分调整回报的市场中，DRL代理的表现优于标准市场设置。

    

    本文研究了深度强化学习（DRL）在环境、社会和治理（ESG）金融投资组合管理中的应用，特别关注基于ESG评分的市场监管的潜在益处。我们利用了Advantage Actor-Critic（A2C）代理，并使用OpenAI Gym中的环境进行实验，这些环境是从FinRL平台改编的。该研究包括对DRL代理在标准道琼斯工业平均指数（DJIA）市场条件和按公司ESG评分调整回报的情景下的性能进行比较分析。在ESG调控市场中，根据投资组合的回报和ESG评分按比例分配资金，而对于低于指数平均ESG评分的投资组合则征收税款。结果令人惊讶地表明，在ESG调控市场中，DRL代理的表现优于标准DJIA市场设置。此外，我们考虑了将ESG变量纳入模型中的影响。

    This paper investigates the application of Deep Reinforcement Learning (DRL) for Environment, Social, and Governance (ESG) financial portfolio management, with a specific focus on the potential benefits of ESG score-based market regulation. We leveraged an Advantage Actor-Critic (A2C) agent and conducted our experiments using environments encoded within the OpenAI Gym, adapted from the FinRL platform. The study includes a comparative analysis of DRL agent performance under standard Dow Jones Industrial Average (DJIA) market conditions and a scenario where returns are regulated in line with company ESG scores. In the ESG-regulated market, grants were proportionally allotted to portfolios based on their returns and ESG scores, while taxes were assigned to portfolios below the mean ESG score of the index. The results intriguingly reveal that the DRL agent within the ESG-regulated market outperforms the standard DJIA market setup. Furthermore, we considered the inclusion of ESG variables i
    
[^74]: 面向联邦基础模型的可扩展数据集流水线：用于群组结构化学习

    Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning. (arXiv:2307.09619v1 [cs.LG])

    [http://arxiv.org/abs/2307.09619](http://arxiv.org/abs/2307.09619)

    Dataset Grouper是一个库，用于创建大规模群组结构化数据集，并克服了内存限制、提供了灵活性，并且与不同的软件框架兼容。实验证明它可以实现比以前更大规模的联邦语言建模模拟。

    

    我们引入了一个名为Dataset Grouper的库，用于创建大规模的群组结构化（例如联邦）数据集，实现基础模型规模的联邦学习模拟。该库允许根据用户指定的分区创建现有数据集的群组结构化版本，并直接导致各种有用的异构数据集，可以插入现有的软件框架中。Dataset Grouper具有三个关键优势。首先，它能够适应即使单个群组的数据集也太大无法放入内存的情况。其次，它提供了灵活性，既可以选择基础（非分区）数据集，也可以定义分区。最后，它与框架无关。我们通过实验证明，Dataset Grouper允许在比先前工作中大几个数量级的数据集上进行大规模联邦语言建模模拟。我们的实验结果表明，像FedAvg这样的算法更像元学习方法而不是经验学习方法。

    We introduce a library, Dataset Grouper, to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library allows the creation of group-structured versions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper allows for large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empi
    
[^75]: 深入探究可解释性深度学习在神经影像学中的应用：一项全面调查

    Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey. (arXiv:2307.09615v1 [cs.LG])

    [http://arxiv.org/abs/2307.09615](http://arxiv.org/abs/2307.09615)

    本文对可解释的深度学习在神经影像学中的应用进行了全面调查，深度学习模型的易解释性仍然存在挑战，近年来的研究主要集中在如何理解模型决策的直觉，还需探索如何验证解释性方法的可靠性。

    

    深度学习（DL）模型因其能够直接从原始数据中进行端到端学习，减轻了对错误易发特征提取阶段的担忧而备受青睐。最近的DL基于神经影像学的研究也在传统机器学习算法上取得了显著的性能提升。然而，深度学习模型的挑战仍然存在，因为这些模型缺乏透明度，无法成功应用于现实世界的应用中。最近几年，可解释的人工智能（XAI）经历了快速发展，主要用于揭示模型如何作出决策的直觉，这对于安全关键领域如医疗保健、金融和执法机构至关重要。尽管解释性领域取得了显著的进展，但研究人员仍不清楚后期方法揭示了模型学习的哪个方面以及如何验证其可靠性。本文综合评述了可解释的深度学习方法在神经影像学中的应用。

    Deep learning (DL) models have been popular due to their ability to learn directly from the raw data in an end-to-end paradigm, alleviating the concern of a separate error-prone feature extraction phase. Recent DL-based neuroimaging studies have also witnessed a noticeable performance advancement over traditional machine learning algorithms. But the challenges of deep learning models still exist because of the lack of transparency in these models for their successful deployment in real-world applications. In recent years, Explainable AI (XAI) has undergone a surge of developments mainly to get intuitions of how the models reached the decisions, which is essential for safety-critical domains such as healthcare, finance, and law enforcement agencies. While the interpretability domain is advancing noticeably, researchers are still unclear about what aspect of model learning a post hoc method reveals and how to validate its reliability. This paper comprehensively reviews interpretable deep
    
[^76]: 多视角自监督学习用于多变量通道时间序列

    Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v1 [stat.ML])

    [http://arxiv.org/abs/2307.09614](http://arxiv.org/abs/2307.09614)

    本论文提出了一种多视角自监督学习方法，用于处理多变量通道时间序列数据，在不同数据集之间进行迁移学习。通过预训练和微调，结合传递神经网络和TS2Vec损失，该方法在大多数设置下表现优于其他方法。

    

    对多变量生物医学时间序列数据进行标注是一项繁重和昂贵的任务。自监督对比学习通过对未标记数据进行预训练来减少对大型标记数据集的需求。然而，对于多变量时间序列数据，输入通道的集合在不同应用之间通常会有所变化，而大多数现有工作并不允许在具有不同输入通道集合的数据集之间进行迁移学习。我们提出了一种学习一种编码器来分别处理所有输入通道的方法。然后，我们使用传递神经网络在通道之间提取单一表示。我们通过在一个具有六个脑电图通道的数据集上进行预训练，并在一个具有两个不同脑电图通道的数据集上进行微调来展示这种方法的潜力。我们比较了具有传递神经网络和不具有传递神经网络的网络在不同对比损失函数下的性能。我们发现我们的方法结合了TS2Vec损失在大多数设置中的表现优于其他所有方法。

    Labeling of multivariate biomedical time series data is a laborious and expensive process. Self-supervised contrastive learning alleviates the need for large, labeled datasets through pretraining on unlabeled data. However, for multivariate time series data the set of input channels often varies between applications, and most existing work does not allow for transfer between datasets with different sets of input channels. We propose learning one encoder to operate on all input channels individually. We then use a message passing neural network to extract a single representation across channels. We demonstrate the potential of this method by pretraining our network on a dataset with six EEG channels and finetuning on a dataset with two different EEG channels. We compare networks with and without the message passing neural network across different contrastive loss functions. We show that our method combined with the TS2Vec loss outperforms all other methods in most settings.
    
[^77]: 使用可学习的哈希的神经时间点过程检索连续时间事件序列

    Retrieving Continuous Time Event Sequences using Neural Temporal Point Processes with Learnable Hashing. (arXiv:2307.09613v1 [cs.LG])

    [http://arxiv.org/abs/2307.09613](http://arxiv.org/abs/2307.09613)

    提出了一个名为NeuroSeqRet的模型，用于解决大规模检索连续时间事件序列的任务。通过使用可学习的哈希和神经时间点过程，该模型能够为输入的查询序列返回一个相关序列的排序列表。

    

    时间序列已经在各种实际应用中变得普遍。因此，以连续时间事件序列（CTES）形式产生的数据量在过去几年中呈指数增长。因此，针对CTES数据集的当前研究的重要部分是设计用于解决下游任务（如下一个事件预测、长期预测、序列分类等）的模型。使用标记的时间点过程（MTPP）进行预测建模的最新发展使得能够准确地对涉及CTES的几个实际应用进行表征。然而，由于这些CTES数据集的复杂性，过去的文献忽视了大规模检索时间序列的任务。具体而言，通过CTES检索，我们指的是对于一个输入查询序列，检索系统必须从一个大的语料库中返回一个相关序列的排序列表。为了解决这个问题，我们提出了NeuroSeqRet，这是一个典型的

    Temporal sequences have become pervasive in various real-world applications. Consequently, the volume of data generated in the form of continuous time-event sequence(s) or CTES(s) has increased exponentially in the past few years. Thus, a significant fraction of the ongoing research on CTES datasets involves designing models to address downstream tasks such as next-event prediction, long-term forecasting, sequence classification etc. The recent developments in predictive modeling using marked temporal point processes (MTPP) have enabled an accurate characterization of several real-world applications involving the CTESs. However, due to the complex nature of these CTES datasets, the task of large-scale retrieval of temporal sequences has been overlooked by the past literature. In detail, by CTES retrieval we mean that for an input query sequence, a retrieval system must return a ranked list of relevant sequences from a large corpus. To tackle this, we propose NeuroSeqRet, a first-of-its
    
[^78]: 顺序蒙特卡洛学习用于时间序列结构发现

    Sequential Monte Carlo Learning for Time Series Structure Discovery. (arXiv:2307.09607v1 [cs.LG])

    [http://arxiv.org/abs/2307.09607](http://arxiv.org/abs/2307.09607)

    本文提出了一种顺序蒙特卡洛学习的方法，用于自动发现复杂时间序列数据的准确模型。在实验中显示，该方法相对于之前的方法，具有较快的运行速度并能够发现合理的模型结构。

    

    本文提出了一种自动发现复杂时间序列数据准确模型的新方法。在高斯过程时间序列模型的符号空间上工作的贝叶斯非参数先验中，我们提出了一种集成顺序蒙特卡洛（SMC）和旋换MCMC的新型结构学习算法，以实现高效的后验推断。我们的方法可以在“在线”设置中使用，其中新数据顺序地合并在时间中，并且可以在“离线”设置中使用，通过使用历史数据的嵌套子集对后验进行退火。对真实世界的时间序列进行的实验测量结果显示，我们的方法相比之前针对相同模型族的MCMC和贪心搜索结构学习算法可以提供10倍至100倍的运行时间加速。我们使用我们的方法对1,428个计量经济数据集的知名基准进行了首次大规模的高斯过程时间序列结构学习评估。结果表明我们的方法可以发现合理的模型结构。

    This paper presents a new approach to automatically discovering accurate models of complex time series data. Working within a Bayesian nonparametric prior over a symbolic space of Gaussian process time series models, we present a novel structure learning algorithm that integrates sequential Monte Carlo (SMC) and involutive MCMC for highly effective posterior inference. Our method can be used both in "online" settings, where new data is incorporated sequentially in time, and in "offline" settings, by using nested subsets of historical data to anneal the posterior. Empirical measurements on real-world time series show that our method can deliver 10x--100x runtime speedups over previous MCMC and greedy-search structure learning algorithms targeting the same model family. We use our method to perform the first large-scale evaluation of Gaussian process time series structure learning on a prominent benchmark of 1,428 econometric datasets. The results show that our method discovers sensible 
    
[^79]: 使用凸凹表示的Legendre变换将神经网络转化为max-affine样条近似

    A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation. (arXiv:2307.09602v1 [cs.LG])

    [http://arxiv.org/abs/2307.09602](http://arxiv.org/abs/2307.09602)

    这项工作提出了一种将神经网络转化为样条表示的新算法，它不再需要凸多边形和分段线性网络操作符的限制，并且可以在整个网络上执行。这项工作不仅填补了神经网络和逼近理论之间的差距，还使得网络特征图的可视化成为可能。

    

    本文提出了一种将神经网络转化为样条表示的新算法。与以前需要凸多边形和分段线性网络操作符来创建max-affine样条形式的方法不同，本文放宽了这个约束。唯一的约束是函数应该是有界的，并且具有明确定义的二阶导数，尽管通过实验表明这并不是严格必需的。这种方法也可以在整个网络上执行，而不是在每个层上独立执行。与以前的工作一样，这填补了神经网络和逼近理论之间的差距，同时也实现了网络特征图的可视化。通过从一系列架构中提取逼近误差和特征图进行数学证明和实验研究。

    This work presents a novel algorithm for transforming a neural network into a spline representation. Unlike previous work that required convex and piecewise-affine network operators to create a max-affine spline alternate form, this work relaxes this constraint. The only constraint is that the function be bounded and possess a well-define second derivative, although this was shown experimentally to not be strictly necessary. It can also be performed over the whole network rather than on each layer independently. As in previous work, this bridges the gap between neural networks and approximation theory but also enables the visualisation of network feature maps. Mathematical proof and experimental investigation of the technique is performed with approximation error and feature maps being extracted from a range of architectures, including convolutional neural networks.
    
[^80]: 梯度反击：如何滤除高频率提高解释性

    Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])

    [http://arxiv.org/abs/2307.09591](http://arxiv.org/abs/2307.09591)

    本研究发现，基于预测的属性方法与基于梯度的方法产生的属性图具有不同的高频内容，滤除高频率可以提高解释性。

    

    近年来，新型基于预测的属性方法的发展迅猛，逐渐取代了旧的基于梯度的方法来解释深度神经网络的决策。然而，预测型方法为何优于梯度型方法仍不清楚。本文从经验观察开始：这两种方法产生的属性图具有非常不同的功率谱，梯度型方法揭示了比预测型方法更多的高频内容。这一观察引发了多个问题：这种高频信息的来源是什么，它是否真正反映了系统所作出的决策？最后，为什么在多个评价指标下，预测型方法中缺乏高频信息将产生更好的可解释性分数？我们分析了三个代表性的视觉分类模型的梯度，并观察到它包含来自高频的噪声信息。

    Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
    
[^81]: 社交学习网络中的因果影响研究

    Causal Influences over Social Learning Networks. (arXiv:2307.09575v1 [cs.SI])

    [http://arxiv.org/abs/2307.09575](http://arxiv.org/abs/2307.09575)

    本论文研究了社交学习网络中代理之间的因果影响，并提出了一种算法来评估整体影响力和发现高度有影响力的代理。

    

    本文研究了相互连接且经过时间交互的代理之间的因果影响。具体而言，本论文考察了社交学习模型和分布式决策协议的动态，并推导出了表明代理之间因果关系并解释网络上影响流动的表达式。结果表明，这些因果关系取决于图的拓扑结构和每个代理对于他们试图解决的推理问题的信息水平。基于这些结论，本文提出了一种算法来评估代理之间的整体影响力，以发现高度有影响力的代理。还提供了一种从原始观测数据中学习必要的模型参数的方法。结果和所提出的算法通过考虑合成数据和真实的Twitter数据加以说明。

    This paper investigates causal influences between agents linked by a social graph and interacting over time. In particular, the work examines the dynamics of social learning models and distributed decision-making protocols, and derives expressions that reveal the causal relations between pairs of agents and explain the flow of influence over the network. The results turn out to be dependent on the graph topology and the level of information that each agent has about the inference problem they are trying to solve. Using these conclusions, the paper proposes an algorithm to rank the overall influence between agents to discover highly influential agents. It also provides a method to learn the necessary model parameters from raw observational data. The results and the proposed algorithm are illustrated by considering both synthetic data and real Twitter data.
    
[^82]: 自我兼容性：在没有基准数据的情况下评估因果发现的方法。

    Self-Compatibility: Evaluating Causal Discovery without Ground Truth. (arXiv:2307.09552v1 [cs.LG])

    [http://arxiv.org/abs/2307.09552](http://arxiv.org/abs/2307.09552)

    本论文提出了一种在没有基准数据的情况下评估因果发现方法的新方法，通过在不同变量子集上学习的因果图之间的兼容性检测，来伪证因果关系的推断正确性。

    

    鉴于因果基本事实非常罕见，因果发现算法通常只在模拟数据上进行评估。这令人担忧，因为模拟反映了关于噪声分布、模型类别等生成过程的常见假设。在这项工作中，我们提出了一种新的方法，用于在没有基准数据的情况下对因果发现算法的输出进行伪证。我们的关键见解是，尽管统计学习寻求数据点子集之间的稳定性，但因果学习应该寻求变量子集之间的稳定性。基于这个见解，我们的方法依赖于在不同变量子集上学习的因果图之间的兼容性概念。我们证明了检测不兼容性可以伪证因果关系被错误推断的原因，这是因为假设违反或有限样本效应带来的错误。虽然通过这种兼容性测试只是对良好性能的必要条件，但我们认为它提供了强有力的证据。

    As causal ground truth is incredibly rare, causal discovery algorithms are commonly only evaluated on simulated data. This is concerning, given that simulations reflect common preconceptions about generating processes regarding noise distributions, model classes, and more. In this work, we propose a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth. Our key insight is that while statistical learning seeks stability across subsets of data points, causal learning should seek stability across subsets of variables. Motivated by this insight, our method relies on a notion of compatibility between causal graphs learned on different subsets of variables. We prove that detecting incompatibilities can falsify wrongly inferred causal relations due to violation of assumptions or errors from finite sample effects. Although passing such compatibility tests is only a necessary criterion for good performance, we argue that it provides strong evidenc
    
[^83]: 神经网络的语义景观范式

    The semantic landscape paradigm for neural networks. (arXiv:2307.09550v1 [cs.LG])

    [http://arxiv.org/abs/2307.09550](http://arxiv.org/abs/2307.09550)

    本研究引入了语义景观范式，用于描述神经网络的训练动力学，将其视为在图上的路径，图的节点对应于网络学习表示中的新算法。这种抽象使我们能够以统计物理学中研究过的问题来解释各种神经网络现象。

    

    深度神经网络展示了一系列令人着迷的现象，从可预测的缩放定律到训练时间、数据集大小和网络大小的不可预测的新能力的出现。对这些现象的分析揭示了这些网络的学习表示中编码的概念和算法的存在。虽然在解释这些观察到的现象方面取得了重要进展，但对于理解、解剖和预测神经网络性能的统一框架尚缺乏。在这里，我们引入了语义景观范式，这是一个概念性和数学框架，描述了神经网络的训练动力学，其中的路径被视为整个网络中学习表示内在的新算法。这种抽象使我们能够以统计物理学中研究过的问题来描述各种神经网络现象。

    Deep neural networks exhibit a fascinating spectrum of phenomena ranging from predictable scaling laws to the unpredictable emergence of new capabilities as a function of training time, dataset size and network size. Analysis of these phenomena has revealed the existence of concepts and algorithms encoded within the learned representations of these networks. While significant strides have been made in explaining observed phenomena separately, a unified framework for understanding, dissecting, and predicting the performance of neural networks is lacking. Here, we introduce the semantic landscape paradigm, a conceptual and mathematical framework that describes the training dynamics of neural networks as trajectories on a graph whose nodes correspond to emergent algorithms that are instrinsic to the learned representations of the networks. This abstraction enables us to describe a wide range of neural network phenomena in terms of well studied problems in statistical physics. Specifically
    
[^84]: DreaMR: 基于扩散的反事实解释的功能性磁共振成像翻译

    DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI. (arXiv:2307.09547v1 [eess.SP])

    [http://arxiv.org/abs/2307.09547](http://arxiv.org/abs/2307.09547)

    DreaMR 是基于扩散的反事实解释方法，用于高特异性、合理性和一致性的解释功能性磁共振成像（fMRI）数据。

    

    深度学习分析在从功能性磁共振成像（fMRI）测量中检测认知状态方面提供了敏感性的飞跃。然而，由于深度模型对其输入进行层次非线性变换，解释脑部响应和认知状态之间的关联是具有挑战性的。在深度fMRI分类器的常见解释方法中，归因方法显示出较差的特异性，扰动方法显示出有限的合理性。虽然反事实生成承诺解决了这些限制，但以往的方法使用变分或对抗性先验，导致样本一致性不佳。在这里，我们引入了第一个基于扩散的反事实方法DreaMR，以实现具有高特异性，合理性和一致性的fMRI解释。

    Deep learning analyses have offered sensitivity leaps in detection of cognitive states from functional MRI (fMRI) measurements across the brain. Yet, as deep models perform hierarchical nonlinear transformations on their input, interpreting the association between brain responses and cognitive states is challenging. Among common explanation approaches for deep fMRI classifiers, attribution methods show poor specificity and perturbation methods show limited plausibility. While counterfactual generation promises to address these limitations, previous methods use variational or adversarial priors that yield suboptimal sample fidelity. Here, we introduce the first diffusion-driven counterfactual method, DreaMR, to enable fMRI interpretation with high specificity, plausibility and fidelity. DreaMR performs diffusion-based resampling of an input fMRI sample to alter the decision of a downstream classifier, and then computes the minimal difference between the original and counterfactual sampl
    
[^85]: 神经网络的记忆化是否可以被局部化？

    Can Neural Network Memorization Be Localized?. (arXiv:2307.09542v1 [cs.LG])

    [http://arxiv.org/abs/2307.09542](http://arxiv.org/abs/2307.09542)

    本文研究了深度神经网络中记忆化的局部化现象，通过实验证据表明，记忆化并不局限于个别层，而是在模型的多个层中的一小部分神经元中发生。

    

    最近的研究努力在解释深度超参数化网络中记忆化和概括之间的相互作用时，提出了神经网络在模型的最后几层中$\textit{记忆化}$“困难”样本的能力。记忆化是指在训练集的$\textit{非典型}$样本上能够正确预测的能力。在这项工作中，我们展示了记忆化现象并不局限于个别层，而是在模型的各个层中的一小组神经元中发生。首先，通过三种实验方面的收敛证据，我们发现大多数层对于样本的记忆化是冗余的，而对样本记忆化的贡献较大的层，并不一定是最后的层。这三个证据来源包括$\textit{梯度追踪}$（测量梯度范数来自于记忆化和干净样本的贡献），$\textit{层重置}$（将训练过程中特定模型权重替换为先前的训练检查点），以及$\textit{...}$

    Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks $\textit{memorize}$ "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on $\textit{atypical}$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are $\textit{gradient accounting}$ (measuring the contribution to the gradient norms from memorized and clean examples), $\textit{layer rewinding}$ (replacing specific model weights of a converged model with previous training checkpoints), and $\textit{
    
[^86]: 透明的6G RAN切片中基于解释的公平联邦学习

    Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing. (arXiv:2307.09494v1 [cs.NI])

    [http://arxiv.org/abs/2307.09494](http://arxiv.org/abs/2307.09494)

    这篇论文提出了一个解释引导的联邦学习方案，通过利用可解释的人工智能策略产生透明和无偏的深度神经网络，从而确保可靠的预测。

    

    未来的零触摸人工智能驱动的6G网络自动化需要通过可解释的人工智能建立对AI黑盒子的信任，预计AI的可信度将与通信关键性能指标一起作为可量化的服务级别协议指标。这需要利用可解释人工智能输出来生成透明和无偏的深度神经网络。我们设计了一个基于解释的联邦学习方案(EGFL)来确保在训练运行时通过Jensen-Shannon (JS)散度利用XAI策略的模型解释以确保可靠的预测。具体而言，我们通过将回忆度指标作为优化任务的约束条件，预测每个切片RAN的丢包概率来说明所提出的概念。

    Future zero-touch artificial intelligence (AI)-driven 6G network automation requires building trust in the AI black boxes via explainable artificial intelligence (XAI), where it is expected that AI faithfulness would be a quantifiable service-level agreement (SLA) metric along with telecommunications key performance indicators (KPIs). This entails exploiting the XAI outputs to generate transparent and unbiased deep neural networks (DNNs). Motivated by closed-loop (CL) automation and explanation-guided learning (EGL), we design an explanation-guided federated learning (EGFL) scheme to ensure trustworthy predictions by exploiting the model explanation emanating from XAI strategies during the training run time via Jensen-Shannon (JS) divergence. Specifically, we predict per-slice RAN dropped traffic probability to exemplify the proposed concept while respecting fairness goals formulated in terms of the recall metric which is included as a constraint in the optimization task. Finally, the 
    
[^87]: PLiNIO: 一个用户友好的基于梯度的复杂度感知深度神经网络优化方法库

    PLiNIO: A User-Friendly Library of Gradient-based Methods for Complexity-aware DNN Optimization. (arXiv:2307.09488v1 [cs.LG])

    [http://arxiv.org/abs/2307.09488](http://arxiv.org/abs/2307.09488)

    PLiNIO是一个用户友好的基于梯度优化的深度神经网络优化方法库，可以在边缘设备上实现高精度和高效的DNNs，并显著减少内存占用。

    

    高精度且高效的深度神经网络(DNNs)在有限的边缘设备上执行的需求很高。为了在新应用中找到这样的DNNs，需要自动化的优化流程，因为手动探索巨大的超参数组合空间是不可能的。在这项工作中，我们提出了PLiNIO，一个开源库，它实现了一套基于轻量级梯度优化的最新DNN设计自动化技术，统一且用户友好的界面。通过在几个边缘相关任务上的实验，我们展示了PLiNIO中各种优化的组合可以在精度 vs 模型大小方面超越基线算法。值得注意的是，与基线结构相比，PLiNIO可以实现高达94.34%的内存减少，准确率只下降不到1%。

    Accurate yet efficient Deep Neural Networks (DNNs) are in high demand, especially for applications that require their execution on constrained edge devices. Finding such DNNs in a reasonable time for new applications requires automated optimization pipelines since the huge space of hyper-parameter combinations is impossible to explore extensively by hand. In this work, we propose PLiNIO, an open-source library implementing a comprehensive set of state-of-the-art DNN design automation techniques, all based on lightweight gradient-based optimization, under a unified and user-friendly interface. With experiments on several edge-relevant tasks, we show that combining the various optimizations available in PLiNIO leads to rich sets of solutions that Pareto-dominate the considered baselines in terms of accuracy vs model size. Noteworthy, PLiNIO achieves up to 94.34% memory reduction for a <1% accuracy drop compared to a baseline architecture.
    
[^88]: 在族交和背包约束交集下的子模求解最大化问题

    Submodular Maximization under the Intersection of Matroid and Knapsack Constraints. (arXiv:2307.09487v1 [cs.DS])

    [http://arxiv.org/abs/2307.09487](http://arxiv.org/abs/2307.09487)

    本文提出了一种在$k$-matroid约束和$m$-knapsack约束的交集下进行子模求解最大化问题的算法SPROUT，并引入了部分枚举和平滑技术以提高其效率，实现了更好的多项式时间逼近保证。

    

    子模求解最大化问题在诸多应用中经常出现，并且吸引了来自人工智能、金融和运筹学等领域的广泛研究关注。先前的研究主要考虑了单一种类的约束，而很多现实世界中的问题常常涉及多个约束。本文考虑在$k$-matroid约束和$m$-knapsack约束的交集下进行子模求解最大化的问题，并且将部分枚举方法引入到同时贪心算法中，提出了一种新的算法SPROUT。我们证明了SPROUT可以实现比最先进的算法更好的多项式时间逼近保证。然后，我们将随机枚举和平滑技术引入到SPROUT中以提高其效率，得到了SPROUT++算法，该算法可以保持类似的逼近保证。在电影推荐和加权最大割的应用实验上进行了验证。

    Submodular maximization arises in many applications, and has attracted a lot of research attentions from various areas such as artificial intelligence, finance and operations research. Previous studies mainly consider only one kind of constraint, while many real-world problems often involve several constraints. In this paper, we consider the problem of submodular maximization under the intersection of two commonly used constraints, i.e., $k$-matroid constraint and $m$-knapsack constraint, and propose a new algorithm SPROUT by incorporating partial enumeration into the simultaneous greedy framework. We prove that SPROUT can achieve a polynomial-time approximation guarantee better than the state-of-the-art algorithms. Then, we introduce the random enumeration and smooth techniques into SPROUT to improve its efficiency, resulting in the SPROUT++ algorithm, which can keep a similar approximation guarantee. Experiments on the applications of movie recommendation and weighted max-cut demonst
    
[^89]: MolFM:一种多模态分子基础模型

    MolFM: A Multimodal Molecular Foundation Model. (arXiv:2307.09484v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.09484](http://arxiv.org/abs/2307.09484)

    MolFM是一种多模态分子基础模型，通过跨模态关注实现了分子结构、文本和知识图谱之间的联合表示学习。

    

    分子知识存在于三种不同的信息来源模式中：分子结构、生物医学文献和知识库。有效整合来自这些模态的分子知识对促进生物医学研究至关重要。然而，现有的多模态分子基础模型在捕捉分子结构和文本之间的复杂关联方面存在局限性，更重要的是，它们中的任何一个都没有尝试利用从知识图谱中获得的丰富分子专业知识。在本研究中，我们介绍了MolFM，一种多模态分子基础模型，旨在促进从分子结构、生物医学文本和知识图谱中进行联合表示学习。我们提出了分子结构中的原子、分子实体的邻居和语义相关文本之间的跨模态关注，以促进跨模态理解。我们提供了理论分析，表明我们的跨模态预训练捕捉到了分子结构、文本和知识图谱之间的复杂关系。

    Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures
    
[^90]: 电路分析的可解释性是否具有可扩展性？来自毛丫鼠中多项选择能力的证据。

    Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])

    [http://arxiv.org/abs/2307.09458](http://arxiv.org/abs/2307.09458)

    本论文研究了电路分析在最先进的语言模型中的可扩展性，通过对70B毛丫鼠模型进行多项选择题的分析，发现现有的逻辑层归因和激活修补技术具有可扩展性，并进一步研究了注意力头的语义特征。

    

    电路分析是一种理解语言模型内部机制的有前途的技术。然而，现有的分析都是在远离最先进技术的小型模型中进行的。为了解决这个问题，我们在70B毛丫鼠模型中进行了一项案例研究，旨在测试电路分析的可扩展性。具体而言，我们研究了多项选择题，调查了毛丫鼠在知道正确答案文本的情况下是否能够识别出正确答案标签。我们发现已有的逻辑层归因、注意力模式可视化和激活修补技术在毛丫鼠模型中具有自然的可扩展性，使我们能够识别和分类一小组“输出节点”（注意力头和多层感知机）。我们进一步研究了“正确字母”类别的注意力头，旨在了解其特征的语义，结果有所不同。对于正常的多项选择问题，我们显著压缩了查询。

    \emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).  We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, ke
    
[^91]: 用于二分类的分类编码器的基准研究

    A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v1 [cs.LG])

    [http://arxiv.org/abs/2307.09191](http://arxiv.org/abs/2307.09191)

    本研究是迄今为止最全面的分类编码器基准研究，通过对来自不同家族的32种编码器配置进行广泛评估，以及36种实验因素和50个数据集的组合，展示了数据集选择、实验因素和聚合策略对基准研究结论的深远影响。

    

    分类编码器将分类特征转化为数字表示，对于广泛的机器学习模型来说是不可或缺的。现有的编码器基准研究由于选择有限的编码器、实验因素和数据集，缺乏普适性。此外，由于采用了不同的聚合策略，结果存在不一致性。本文是迄今为止最全面的分类编码器基准研究，包括对来自不同家族的32种编码器配置进行了广泛评估，以及36种实验因素和50个数据集的组合。该研究展示了数据集选择、实验因素和聚合策略对基准研究结论的深远影响，这是以前的编码器基准研究忽视的方面。

    Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models. Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions -- aspects disregarded in previous encoder benchmarks.
    
[^92]: 自监督学习中投影头的稀疏性研究

    Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])

    [http://arxiv.org/abs/2307.08913](http://arxiv.org/abs/2307.08913)

    该论文研究了自监督学习中投影头的稀疏性，发现通过在投影子空间中执行对比损失可以提升表示的质量，建议只有一部分特征是必要的，而稀疏的投影头可以增强模型的泛化性能。

    

    最近几年，自监督学习（SSL）已成为从无标签数据中提取有价值表示的一种有希望的方法。其中一种成功的SSL方法是对比学习，其旨在将正样本聚集在一起，将负样本推开。许多当前的对比学习方法都使用参数化的投影头。通过实证分析和理论探索，我们对投影头的内部机制及其与维度折叠现象的关系进行了深入研究。我们的研究结果表明，投影头通过在一个投影子空间中执行对比损失，提升表示的质量。因此，我们提出一个假设，即在最小化一个小批量数据的对比损失时，只有一部分特征是必要的。理论分析进一步表明，稀疏的投影头可以增强泛化性能，因此我们引入SparseHead这一方法。

    In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - 
    
[^93]: Retentive Network: 作为大型语言模型的Transformer的继任者

    Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.08621](http://arxiv.org/abs/2307.08621)

    Retentive Network（RetNet）作为大型语言模型的基础架构，实现了训练并行、低成本推理和良好的性能。通过并行、循环和分块循环三种计算范式，RetNet具有训练并行化、低成本推理和高效的长序列建模的特点。

    

    在这项工作中，我们提出了Retentive Network (RetNet)作为大型语言模型的基础架构，同时实现了训练并行、低成本推理和良好的性能。我们从理论上推导出了循环和注意力之间的连接。然后，我们提出了序列建模的保留机制，支持三种计算范式，即并行、循环和分块循环。具体而言，并行表示允许进行训练并行化。循环表示能够实现低成本的$O(1)$推理，从而提高解码吞吐量、延迟和GPU内存，同时不损失性能。分块循环表示便于使用线性复杂度进行高效的长序列建模，其中每个块可以并行编码，同时进行循环摘要。语言建模实验结果表明，RetNet实现了良好的扩展结果、并行训练、低成本部署和高效的推理。

    In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient i
    
[^94]: M-FLAG：使用冻结语言模型和潜空间几何优化的医学视觉语言预训练

    M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization. (arXiv:2307.08347v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.08347](http://arxiv.org/abs/2307.08347)

    M-FLAG是一种新颖的医学视觉语言预训练方法，通过利用冻结语言模型和引入正交损失函数来优化潜空间几何关系。在医学影像分类、分割和目标检测任务上，M-FLAG在性能上显著优于现有方法，并且减少了78%的参数量。

    

    医学视觉语言模型可以实现医学影像和临床文本的特征共学习和集成。然而，这些模型训练起来并不容易，并且潜空间表示可以非常复杂。本文提出了一种新颖的医学视觉语言模型预训练和正则化方法。该方法命名为医学视觉语言预训练与冻结语言模型和潜空间几何优化（M-FLAG），利用冻结语言模型来稳定和高效地进行训练，并引入了一种新颖的正交损失函数来协调潜空间几何关系。我们在三个下游任务上展示了预训练模型的潜力：医学影像分类、分割和目标检测。通过对五个公开数据集进行广泛的实验证明，M-FLAG在减少78％的参数的同时，明显优于现有的医学视觉语言预训练方法。值得注意的是，M-FLAG表现出了出色的性能。

    Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78\%. Notably, M-FLAG achieves outstanding performance o
    
[^95]: 探索从替代训练中理解对抗性可转移性

    Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])

    [http://arxiv.org/abs/2307.07873](http://arxiv.org/abs/2307.07873)

    本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。

    

    对DNNs的对抗样本(AEs)已经表明是可转移的：成功欺骗白盒子替代模型的AEs也可以欺骗具有不同架构的其他黑盒模型。虽然许多经验研究提供了生成高度可转移AE的指导，但这些研究缺乏解释甚至导致不一致的建议。本文在理解对抗性可转移性方面迈出了一步，特别关注替代方面。从着名的小健壮性现象开始，通过以轻微扰动的对抗性样本对模型进行对抗训练可以得到更好的替代模型，我们将其归因于两个主要因素之间的权衡：模型的平滑性和梯度相似性。我们的研究集中在它们的共同效果上，而不是它们与可转移性的单独相关性。通过一系列理论和实证分析，我们推测数据分布的转变。

    Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
    
[^96]: 基于神经符号的强化去噪扩散概率模型在工业4.0中的实时异常检测

    Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0. (arXiv:2307.06975v1 [cs.LG])

    [http://arxiv.org/abs/2307.06975](http://arxiv.org/abs/2307.06975)

    本论文提出了一种基于神经符号的强化去噪扩散概率模型，用于在工业4.0中实时预测异常。该方法集成了工业本体论，为智能制造提供了形式化知识，并且通过随机傅里叶特征提取扩散模型，实现了在嵌入式系统中直接集成。这种方法在以前从未被探索过。

    

    工业4.0将物联网、大数据和人工智能等数字技术整合到制造和工业流程中，以提高效率和生产力。随着这些技术的互联互通和相互依赖程度越来越高，工业4.0系统变得更加复杂，这就带来了识别和停止可能在制造过程中引起干扰的异常的困难。本文旨在提出一种基于扩散的模型，用于工业4.0流程中的实时异常预测。通过神经符号方法，我们将工业本体论集成到模型中，从而在智能制造中添加了形式化知识。最后，我们提出了一种简单而有效的通过随机傅里叶特征提取扩散模型的方法，以便将其部署到嵌入式系统中，直接集成到制造过程中。据我们所知，这种方法在以前从未被探索过。

    Industry 4.0 involves the integration of digital technologies, such as IoT, Big Data, and AI, into manufacturing and industrial processes to increase efficiency and productivity. As these technologies become more interconnected and interdependent, Industry 4.0 systems become more complex, which brings the difficulty of identifying and stopping anomalies that may cause disturbances in the manufacturing process. This paper aims to propose a diffusion-based model for real-time anomaly prediction in Industry 4.0 processes. Using a neuro-symbolic approach, we integrate industrial ontologies in the model, thereby adding formal knowledge on smart manufacturing. Finally, we propose a simple yet effective way of distilling diffusion models through Random Fourier Features for deployment on an embedded system for direct integration into the manufacturing process. To the best of our knowledge, this approach has never been explored before.
    
[^97]: IntelliGraphs: 用于评估知识图谱生成的数据集

    IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])

    [http://arxiv.org/abs/2307.06698](http://arxiv.org/abs/2307.06698)

    IntelliGraphs是一组新的知识图谱数据集，用于评估知识图谱生成。其中包含具有逻辑规则表达的语义的子图，用于评估子图推断的模型。

    

    知识图谱嵌入（KGE）模型用于学习实体和关系的连续表示。文献中一个关键的任务是预测实体之间的缺失链接。然而，知识图谱不仅仅是链接的集合，还具有其结构中的语义。语义在多个下游任务中至关重要，例如查询回答或推理。我们引入了子图推断任务，其中一个模型必须生成可能的并且语义上有效的子图。我们提出了IntelliGraphs，一个包含五个新的知识图谱数据集的集合。IntelliGraphs数据集包含具有逻辑规则表达的语义的子图，用于评估子图推断。我们还设计了产生合成数据集的数据集生成器。我们设计了四个新的基准模型，其中包括基于传统KGE的三个模型。我们评估了它们的表达能力，并展示了这些模型无法捕捉到语义。我们相信这一基准将促进该领域的发展。

    Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
    
[^98]: 将基础模型作为代理模型引入：朝着更实用的对抗攻击迈进

    Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])

    [http://arxiv.org/abs/2307.06608](http://arxiv.org/abs/2307.06608)

    本文将对抗攻击重新设定为下游任务，通过生成图像噪声来满足新兴趋势，并将基础模型引入作为代理模型。虽然基础模型的表现不佳，但通过在特征空间中进行分析，我们发现缺乏对应的特征。

    

    最近，无盒对抗攻击成为了最实用且具有挑战性的攻击方式，攻击者无法访问模型的架构、权重和训练数据。然而，在无盒设置中，对于代理模型选择过程的潜力和灵活性缺乏认识。受到利用基础模型解决下游任务的兴趣的启发，本文采用了1）将对抗攻击重新设定为下游任务，具体而言，是生成图像噪声以满足新兴趋势；2）将基础模型引入作为代理模型的创新思想。通过利用非鲁棒特征的概念，我们阐述了选择代理模型的两个指导原则，以解释为什么基础模型是这一角色的最佳选择。然而，矛盾地的是，我们观察到这些基础模型表现不佳。通过在特征空间中分析这种意外行为，我们归因于缺乏上述指导原则所需的特征。

    Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster
    
[^99]: 弱监督条件下的音视频事件定位的时间标签优化

    Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v1 [cs.CV])

    [http://arxiv.org/abs/2307.06385](http://arxiv.org/abs/2307.06385)

    本文解决了弱监督条件下的音视频事件定位问题，通过使用基础模型在训练数据上以更细的时间分辨率估计标签，并提出辅助目标来处理合成视频的分布外特性。

    

    音视频事件定位是指在视频中对同时可见和可听到的事件进行时间定位和分类的任务。本文解决了在弱监督条件下的音视频事件定位问题，训练过程中只有视频级别的事件标签（仅有事件是否出现，但没有时间位置信息）可用于监督。我们的思路是使用一个基础模型在训练数据上以更细的时间分辨率估计标签，并使用这些标签重新训练模型。具体来说，我们通过以下步骤确定训练视频中每个帧片段的标签子集: (i) 用另一个视频中与视频级别标签没有重叠的帧替换片段外的帧， (ii) 将这个合成视频输入基础模型，仅提取该片段的标签。为了处理合成视频的分布外特性，我们提出了一个辅助目标，用于引入更多多样化的标签。

    Audio-Visual Event Localization (AVEL) is the task of temporally localizing and classifying \emph{audio-visual events}, i.e., events simultaneously visible and audible in a video. In this paper, we solve AVEL in a weakly-supervised setting, where only video-level event labels (their presence/absence, but not their locations in time) are available as supervision for training. Our idea is to use a base model to estimate labels on the training data at a finer temporal resolution than at the video level and re-train the model with these labels. I.e., we determine the subset of labels for each \emph{slice} of frames in a training video by (i) replacing the frames outside the slice with those from a second video having no overlap in video-level labels, and (ii) feeding this synthetic video into the base model to extract labels for just the slice in question. To handle the out-of-distribution nature of our synthetic videos, we propose an auxiliary objective for the base model that induces mor
    
[^100]: 诊断、反馈、适应性: 用于测试时政策调整的人-机环路框架

    Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])

    [http://arxiv.org/abs/2307.06333](http://arxiv.org/abs/2307.06333)

    提出了一种交互式框架，通过从用户那里直接获取反馈来识别个性化的无关紧要的概念，从而进行数据增强并获得适应个性化用户目标的政策。

    

    政策常常由于分布偏移而失效——即当政策在新环境中部署时，状态和奖励发生变化。数据增强可以通过使模型对与任务无关的变化具有不变性来增加鲁棒性。然而，设计者在事先往往不知道哪些概念是无关紧要的，尤其是当不同的最终用户对任务执行方式有不同的偏好时。我们提出了一个互动框架，通过直接从用户那里获得反馈来识别个性化的无关紧要的概念。我们的核心思想是生成反事实演示，使用户能够快速确定可能与任务相关和无关的概念。然后利用无关紧要的概念的知识进行数据增强，从而获得适应于个性化用户目标的政策。我们在离散和连续控制任务上进行实验证实了我们的框架。我们的方法(1)使用户能够……

    Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments. Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation. However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed. We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts. Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts. The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives. We present experiments validating our framework on discrete and continuous control tasks with real human users. Our method (1) enables users to 
    
[^101]: 用于下游治疗效果估计的贝叶斯因果发现方法的基准测试

    Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v1 [cs.LG])

    [http://arxiv.org/abs/2307.04988](http://arxiv.org/abs/2307.04988)

    该研究评估了六种基准因果发现方法和一种新提出的基于 GFlowNets 的方法在治疗效果估计任务中的表现，并发现 GFlowNets 具有捕捉各种有用和多样的平均处理效应模式的能力。

    

    决策中因果性的实际应用被广泛认可，因果发现和推理在本质上是相互交织的。然而，在因果发现方法的评估中存在明显的差距，对下游推理的重视程度不足。为了填补这一空白，我们评估了六种已建立的基准因果发现方法和一种基于 GFlowNets 的新方法在治疗效果估计的下游任务上的表现。通过实施一个稳健的评估过程，我们为治疗效果估计的这些因果发现方法的有效性提供了有价值的见解，考虑了合成和真实场景以及低数据场景。此外，我们研究的结果表明，GFlowNets 具有有效捕捉各种有用和多样的平均处理效应模式的能力。

    The practical utility of causality in decision-making is widely recognized, with causal discovery and inference being inherently intertwined. Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference. To address this gap, we evaluate six established baseline causal discovery methods and a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation. Through the implementation of a robust evaluation procedure, we offer valuable insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios. Furthermore, the results of our study demonstrate that GFlowNets possess the capability to effectively capture a wide range of useful and diverse ATE modes.
    
[^102]: CREPE：使用CLIP的可学习提示提高视觉关系预测

    CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction. (arXiv:2307.04838v1 [cs.CV])

    [http://arxiv.org/abs/2307.04838](http://arxiv.org/abs/2307.04838)

    本文研究了使用CLIP模型提高视觉关系预测的可能性。通过在UVTransE框架中采用基于CLIP的表示方法，以及引入对比训练策略，我们提出了CREPE模型，简化了现有复杂的图形模型，取得了良好的效果。

    

    本文探讨了视觉语言模型（VLMs），特别是CLIP，在预测视觉目标之间的关系方面的潜力，其中涉及将图像的视觉特征解释为基于语言的关系。现有的最先进方法使用复杂的图形模型，利用语言线索和视觉特征来解决这一挑战。我们假设CLIP嵌入中的强语言先验可以简化这些图形模型，为更简单的方法铺平道路。我们采用了UVTransE关系预测框架，该框架通过场景中的主体、客体和并集框嵌入来学习关系作为一个平移嵌入。我们在UVTransE框架内系统地探索了基于CLIP的主体、客体和并集框表示的设计，并提出了CREPE（CLIP增强谓词估计）。CREPE利用所有三个边界框的基于文本的表示，并引入了一种新颖的对比训练策略，以自动学习视觉关系。

    In this paper, we explore the potential of Vision-Language Models (VLMs), specifically CLIP, in predicting visual object relationships, which involves interpreting visual features from images into language-based relations. Current state-of-the-art methods use complex graphical models that utilize language cues and visual features to address this challenge. We hypothesize that the strong language priors in CLIP embeddings can simplify these graphical models paving for a simpler approach. We adopt the UVTransE relation prediction framework, which learns the relation as a translational embedding with subject, object, and union box embeddings from a scene. We systematically explore the design of CLIP-based subject, object, and union-box representations within the UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate Estimation). CREPE utilizes text-based representations for all three bounding boxes and introduces a novel contrastive training strategy to automatically
    
[^103]: 利用可解释的自适应人群图学习进行多模态脑龄估计

    Multimodal brain age estimation using interpretable adaptive population-graph learning. (arXiv:2307.04639v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04639](http://arxiv.org/abs/2307.04639)

    这项研究提出了一种利用自适应人群图学习的多模态脑龄估计框架，通过优化人群图结构，提高了图卷积网络的性能和准确性。

    

    脑龄估计在临床上非常重要，因为它可以为神经退行性疾病（如阿尔茨海默病）等提供有价值的信息。人群图包括受试者的多模态成像信息以及人群之间的关系，已经在文献中与图卷积网络（GCN）一起使用，并且在各种医学成像任务中证明了其益处。人群图通常是静态的，并且使用非成像信息手动构建。然而，图的构建并不是一个微不足道的任务，可能会显著影响GCN的性能，而GCN对图的结构非常敏感。在这项工作中，我们提出了一个框架，该框架学习了优化下游任务的人群图结构。一种注意机制为一组成像和非成像特征（表现型）分配权重，然后用于边缘提取。生成的图用于训练GCN。整个管道

    Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeli
    
[^104]: Solvent: 一个用于蛋白质折叠的框架

    Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2307.04603](http://arxiv.org/abs/2307.04603)

    Solvent是一个用于蛋白质折叠的统一研究框架，支持最新模型重要组件的实现和基准测试，并提供对蛋白质结构建模领域的有用见解。

    

    一致性和可靠性对于进行人工智能研究至关重要。许多著名的研究领域，如目标检测，已经通过稳定的基准框架进行了比较和验证。在AlphaFold2之后，蛋白质折叠任务已经进入了一个新阶段，许多方法都是基于AlphaFold2的组件提出的。在蛋白质折叠中，一个统一的研究框架的重要性包括实现和基准，以一致且公平地比较各种方法。为了实现这一点，我们提出了Solvent，一个支持最新模型重要组件的蛋白质折叠框架。Solvent包含在一个统一的代码库中实现的不同模型，并支持在相同数据集上对定义的模型进行训练和评估。我们对著名算法及其组件进行了基准测试，并进行了实验以对蛋白质结构建模领域提供有用的见解。我们希望Solvent能提高蛋白质折叠研究的可靠性。

    Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, an protein folding framework that supports significant components of state-of-th-arts models in the manner of off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliabili
    
[^105]: 高效的贝叶斯行程时间层析成像方法，利用基于敏感性的多项式混沌展开和深度生成网络的地质复杂先验

    Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.04228](http://arxiv.org/abs/2307.04228)

    本论文提出了一种高效的贝叶斯行程时间层析成像方法，利用敏感性信息的多项式混沌展开和深度生成网络，以处理地质复杂性先验的挑战。

    

    蒙特卡洛马尔可夫链（MCMC）方法常常面临两个根本性挑战：先验分布的准确刻画和似然函数的高效评估。在层析成像的贝叶斯研究中，主成分分析（PCA）在某些情况下可以方便地定义先验分布，并同时借助基于多项式混沌展开（PCE）的准确代理模型来替代计算密集的全物理正向求解器。当PCA无法直接提供定义先验分布的方式时，可以采用深度生成模型（例如变分自编码器（VAEs））等替代方法。然而，准确产生一个能够捕捉VAE的潜在参数与正向建模输出之间复杂非线性关系的代理模型是一个显著的挑战。

    Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
    
[^106]: BOF-UCB: 一种用于非平稳环境下的上下界信心算法的贝叶斯优化频率算法

    BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits. (arXiv:2307.03587v1 [cs.LG])

    [http://arxiv.org/abs/2307.03587](http://arxiv.org/abs/2307.03587)

    BOF-UCB是一种用于非平稳环境下的背景线性赌博机的贝叶斯优化频率算法，其结合了贝叶斯和频率学派原则，提高了在动态环境中的性能。它利用贝叶斯更新推断后验分布，并使用频率学派方法计算上界信心界以平衡探索和开发。实验证明，BOF-UCB优于现有方法，是非平稳环境中顺序决策的有前途的解决方案。

    

    我们提出了一种新颖的贝叶斯优化频率上下界信心算法（BOF-UCB），用于非平稳环境下的随机背景线性赌博机。贝叶斯和频率学派原则的独特结合增强了算法在动态环境中的适应性和性能。BOF-UCB算法利用顺序贝叶斯更新推断未知回归参数的后验分布，并随后采用频率学派方法通过最大化后验分布上的期望收益来计算上界信心界（UCB）。我们提供了BOF-UCB性能的理论保证，并在合成数据集和强化学习环境中的经典控制任务中展示了其有效性。我们的结果表明，BOF-UCB优于现有的方法，在非平稳环境中进行顺序决策是一个有前途的解决方案。

    We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound (BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationary environments. This unique combination of Bayesian and frequentist principles enhances adaptability and performance in dynamic settings. The BOF-UCB algorithm utilizes sequential Bayesian updates to infer the posterior distribution of the unknown regression parameter, and subsequently employs a frequentist approach to compute the Upper Confidence Bound (UCB) by maximizing the expected reward over the posterior distribution. We provide theoretical guarantees of BOF-UCB's performance and demonstrate its effectiveness in balancing exploration and exploitation on synthetic datasets and classical control tasks in a reinforcement learning setting. Our results show that BOF-UCB outperforms existing methods, making it a promising solution for sequential decision-making in non-stationary environments.
    
[^107]: 用于超出分布可泛化性的大型视觉语言模型压缩

    Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])

    [http://arxiv.org/abs/2307.03135](http://arxiv.org/abs/2307.03135)

    本文研究了针对大型视觉语言模型的模型压缩方法，将教师模型的视觉表示压缩到学生模型中。研究重点在于超出分布可泛化的问题，并提出了两个原则来增强学生模型的性能。

    

    大型视觉语言模型取得了出色的性能，但其规模和计算要求使它们在资源受限设备和时间敏感任务上的部署变得不切实际。模型压缩是创建更小、更快的模型以保持较大模型性能的有希望的方法。本文研究了将大型视觉语言模型中的视觉表示压缩到轻量级学生模型中的过程，使用小型或中型数据集。值得注意的是，本研究关注的是超出分布（OOD）可泛化的开放词汇问题，这在以往的模型压缩研究中被忽视了。我们从视觉和语言的角度提出了两个原则来增强学生模型的OOD可泛化性：（1）更好地模仿教师的视觉表示空间，并在视觉语言对齐方面谨慎地促进更好的一致性；（2）通过丰富学生模型的自举学习和数据扩充来提高OOD可泛化性。

    Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
    
[^108]: LongNet: 将Transformer扩展到10亿个标记

    LongNet: Scaling Transformers to 1,000,000,000 Tokens. (arXiv:2307.02486v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.02486](http://arxiv.org/abs/2307.02486)

    LongNet是一种可以扩展到10亿个标记的Transformer变体，通过扩张注意力解决了序列长度受限的问题，具有线性计算复杂度和对数依赖关系，可以作为分布式训练器使用并无缝集成到现有的Transformer优化中。实验证明LongNet在长序列和短序列上性能强大。

    

    在大语言模型的时代，扩展序列长度已经成为一个关键需求。然而，现有的方法在计算复杂度或模型表达力上存在困难，导致序列长度受限。为了解决这个问题，我们引入了LongNet，它是一种Transformer的变体，可以将序列长度扩展到10亿个标记以上，而不会牺牲对较短序列的性能。具体而言，我们提出了扩张注意力，随着距离的增大，它将注意范围指数级扩展。LongNet具有显著的优势：1）它具有线性计算复杂度和序列中任意两个标记之间的对数依赖关系；2）它可以作为用于极长序列的分布式训练器；3）它的扩张注意力是标准注意力的即插即用替代品，可以与现有的基于Transformer的优化无缝集成。实验证明LongNet在长序列和短序列上都具有强大的性能。

    Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-se
    
[^109]: SwinGNN:重新思考在图生成的扩散模型中的置换不变性

    SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])

    [http://arxiv.org/abs/2307.01646](http://arxiv.org/abs/2307.01646)

    本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。

    

    基于置换等变网络的扩散模型可以学习图数据的置换不变分布。然而，相对于非不变模型，我们发现这些不变模型遇到了更大的学习挑战，因为1）它们的目标分布更具模态性；2）它们的最优一步去噪得分是具有更多成分的高斯混合物的得分函数。受到这个分析的启发，我们提出了一种非不变的扩散模型，称为“SwinGNN”，它采用了一种高效的边到边的2-WL消息传递网络，并利用SwinTransformers中的移动窗口自注意力。此外，通过系统性的实验和剖析，我们确定了几种关键的训练和采样技术，显著提高了图生成样本的质量。最后，我们引入了一种简单的后处理技巧，即随机置换生成的图，可以证明将任何图转换成图形统计量。

    Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
    
[^110]: 多智能体强化学习中心理推理作为内在动机的理论

    Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning. (arXiv:2307.01158v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01158](http://arxiv.org/abs/2307.01158)

    本论文提出了一种方法来建立语义有意义、人类可解释的信念，并将其应用于多智能体强化学习中。研究发现，通过预测其他智能体的信念来作为内在奖励信号，可以在多智能体环境中产生良好的效果。

    

    模拟他人内心状态对于人类的社会智能至关重要，并且对于人工智能代理来说，在多智能体环境中也可以提供类似的好处。我们提出了一种通过深度网络模型来建立语义有意义、人类可解释的信念的方法。然后，我们考虑了二阶信念预测的任务。我们认为，每个智能体能够预测其他智能体的信念的能力可以作为多智能体强化学习的内在奖励信号。最后，我们在一个混合的合作竞争环境中呈现了初步的实证结果。

    The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.
    
[^111]: H$_2$O: 高效生成大型语言模型的热门元素预测器

    H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v1 [cs.LG])

    [http://arxiv.org/abs/2306.14048](http://arxiv.org/abs/2306.14048)

    本文提出了一种通过预测文本中的热门元素来减少GPU内存消耗的方法，在实验中表现良好。

    

    大型语言模型(LLM)在最近取得了令人瞩目的成就, 但是由于成本过高，它们特别难以用于对话系统和故事创作等需要生成长内容的应用。除了模型参数外，通常还需要在GPU内存中存储大量临时状态信息，称为KV cache，它与序列长度和批量大小呈线性关系。在本文中, 我们提出了一种新颖的实现KV cache的方法，它显著地减少了其内存占用量。我们的方法基于一个引人注目的发现，即在计算注意力分数时，小部分标记贡献最大价值。我们称这些标记为热门元素(H$_2$)。通过全面的研究，我们发现(i) H$_2$的出现是自然而然的，并且与文本中标记的频繁共现强相关；(ii)去除它们会导致明显的性能下降。基于这些见解，我们提出了H$_2$O，一种用于高效生成LLM的热门元素预测器。H$_2$O可以准确地预测给定序列中的热门元素，并因此保持一个更小的KV cache,将GPU内存消耗减少了50%。我们在几个基准数据集上的实验表明,H$_2$O可以在显著减少内存消耗的同时，实现与完整KV cache相当的性能。

    Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insi
    
[^112]: Gradient-based Attribution Methods中Pre或Post-Softmax Scores，哪个更好？

    Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?. (arXiv:2306.13197v1 [cs.LG])

    [http://arxiv.org/abs/2306.13197](http://arxiv.org/abs/2306.13197)

    在Gradient-based Attribution Methods中，使用Pre Softmax分数或Post Softmax分数的梯度的选择有各自的优缺点，需要根据具体情况进行权衡。

    

    对于工作作为分类器的神经网络的基于梯度的归因方法使用网络分数的梯度。在这里，我们讨论使用Pre Softmax分数和Post Softmax分数的梯度之间的实际差异以及它们各自的优缺点。

    Gradient based attribution methods for neural networks working as classifiers use gradients of network scores. Here we discuss the practical differences between using gradients of pre-softmax scores versus post-softmax scores, and their respective advantages and disadvantages.
    
[^113]: RL感知机：高维策略学习的泛化动力学

    The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions. (arXiv:2306.10404v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10404](http://arxiv.org/abs/2306.10404)

    本文提出了一个高维RL模型，推导出该模型的典型动力学为一组闭式ODE方程组，并通过实验与神经RL代理进行了比较，结果表明该模型捕捉了现实世界RL的关键特征。

    

    强化学习算法已被证明在许多领域中具有变革性，为了解决真实世界的问题，这些系统通常使用神经网络直接从像素或其他高维感官输入中学习策略。然而，许多强化学习理论都集中于离散状态空间或最坏情况分析，关于高维情况下策略学习的动力学基本问题仍有待解决。在这里，我们提出了一个可解的高维RL模型，它可以捕捉多种学习协议，并将其典型动力学导出为一组闭式常微分方程。我们推导出最佳的学习率和任务难度调度，类似于训练中的退火方案和课程表，并表明该模型表现出丰富的行为，包括在稀疏奖励下的延迟学习；根据奖励基线不同的各种学习方案；以及由奖励严格程度驱动的速度-准确度权衡。与神经RL代理进行的实验比较表明，该模型捕捉了现实世界RL的关键特征。

    Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty - analogous to annealing schemes and curricula during training in RL - and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Expe
    
[^114]: 高维生成模型测度中的精度和召回的不对称性：衡量保真度和多样性的两个重要指标

    Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions. (arXiv:2306.09618v1 [cs.LG])

    [http://arxiv.org/abs/2306.09618](http://arxiv.org/abs/2306.09618)

    本研究发现在高维生成模型测度中使用的精度和召回指标存在不对称性，可能会导致误导性结论。我们提出了一些方法来修正这种错误。

    

    精度和召回是衡量生成模型性能的两个重要指标，它们分别被用来测量模型的保真度和多样性。然而，本研究发现通过使用k近邻的常见逼近方法，这些指标在高维空间中容易出现误导性结论。具体来说，我们理论上和实验上证明了，在高维空间中，两个与真实分布的支持等距离的模型分布可以具有非常不同的精度和召回，从而导致不对称性。针对我们的理论发现，我们提出了一些简单的修正方法来消除这种错误结果。

    Precision and Recall are two prominent metrics of generative performance, which were proposed to separately measure the fidelity and diversity of generative models. Given their central role in comparing and improving generative models, understanding their limitations are crucially important. To that end, in this work, we identify a critical flaw in the common approximation of these metrics using k-nearest-neighbors, namely, that the very interpretations of fidelity and diversity that are assigned to Precision and Recall can fail in high dimensions, resulting in very misleading conclusions. Specifically, we empirically and theoretically show that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution, can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions. Based on our theoretical insights, we then provide simple ye
    
[^115]: 基于近似有效的$p$-阻抗的多类图聚类

    Multi-class Graph Clustering via Approximated Effective $p$-Resistance. (arXiv:2306.08617v1 [cs.LG])

    [http://arxiv.org/abs/2306.08617](http://arxiv.org/abs/2306.08617)

    本文提出了一种近似计算有效$p$-阻抗并将其应用于多类图聚类，该方法可以通过参数$p$偏向于具有高内部连通性或者更小的簇内顶点之间的最短路径距离的聚类。

    

    本文提出了一种近似计算有效$p$-阻抗并将其应用于多类聚类。基于图拉普拉斯和其$p$-拉普拉斯推广的谱方法一直是非欧几里得聚类技术的支柱。$p$-拉普拉斯的优点在于参数$p$对聚类结构具有可控偏倚。$p$-拉普拉斯特征向量法的缺点在于难以计算第三和更高阶特征向量。因此，我们动机在于使用由$p$-拉普拉斯引导的$p$-阻抗进行聚类。对于$p$-阻抗而言，小$p$会偏向于具有高内部连通性的聚类，而大$p$则会偏向于大小“范围”的聚类，即更小的簇内顶点之间的最短路径距离。然而，计算$p$-阻抗成本很高。我们通过开发$p$-阻抗的近似方法来克服这一问题。我们证明了上下界。

    This paper develops an approximation to the (effective) $p$-resistance and applies it to multi-class clustering. Spectral methods based on the graph Laplacian and its generalization to the graph $p$-Laplacian have been a backbone of non-euclidean clustering techniques. The advantage of the $p$-Laplacian is that the parameter $p$ induces a controllable bias on cluster structure. The drawback of $p$-Laplacian eigenvector based methods is that the third and higher eigenvectors are difficult to compute. Thus, instead, we are motivated to use the $p$-resistance induced by the $p$-Laplacian for clustering. For $p$-resistance, small $p$ biases towards clusters with high internal connectivity while large $p$ biases towards clusters of small ``extent,'' that is a preference for smaller shortest-path distances between vertices in the cluster. However, the $p$-resistance is expensive to compute. We overcome this by developing an approximation to the $p$-resistance. We prove upper and lower bounds
    
[^116]: 使用近邻嵌入估计传染效应

    Contagion Effect Estimation Using Proximal Embeddings. (arXiv:2306.02479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02479](http://arxiv.org/abs/2306.02479)

    本文介绍了一种使用近邻嵌入方法来估计社交网络中的传染效应。我们提出了ProEmb框架，通过将变分自动编码器（VAEs）和对抗网络集成在一起，生成高维代理变量的平衡低维表示，并解决了传染效应估计中的偏差问题。

    

    传染效应指的是社交网络中同伴行为对个体结果的因果影响。在观察研究中估计传染效应的显著方法通常假设没有未测量的混杂因素，但由于潜在的同质性，传染可能会被混淆：同质网络中的节点倾向于与具有相似属性的同伴建立联系，并且可以在不互相影响的情况下表现出相似的行为。解决潜在同质性的一种方法是考虑未观察混杂因素的代理变量。然而，在存在高维代理变量时，基于代理的方法可能会导致传染效应估计的严重偏差，正如我们在本文中演示的那样。为解决这个问题，我们引入了新颖的近邻嵌入（ProEmb）框架，该框架将变分自动编码器（VAEs）和对抗网络集成在一起，生成不同处理组高维代理变量的平衡低维表示，并且在因果推论中考虑了传染效应的估计偏差。

    Contagion effect refers to the causal effect of peers' behavior on the outcome of an individual in social networks. While prominent methods for estimating contagion effects in observational studies often assume that there are no unmeasured confounders, contagion can be confounded due to latent homophily: nodes in a homophilous network tend to have ties to peers with similar attributes and can behave similarly without influencing one another. One way to account for latent homophily is by considering proxies for the unobserved confounders. However, in the presence of high-dimensional proxies, proxy-based methods can lead to substantially biased estimation of contagion effects, as we demonstrate in this paper. To tackle this issue, we introduce the novel Proximal Embeddings (ProEmb), a framework which integrates Variational Autoencoders (VAEs) and adversarial networks to generate balanced low-dimensional representations of high-dimensional proxies for different treatment groups and identi
    
[^117]: 知识追踪中端到端因果发现的概念模型

    A Conceptual Model for End-to-End Causal Discovery in Knowledge Tracing. (arXiv:2305.16165v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16165](http://arxiv.org/abs/2305.16165)

    本文提出了一个概念模型来解决知识追踪中的因果发现问题，通过学生反应数据找到不同技能之间的潜在因果关系。该模型引入了一个因果门循环单元模块，并使用了可学习的置换矩阵和下三角矩阵来表示因果顺序和因果结构。在NeurIPS 2022挑战赛中取得了优秀的成绩。

    

    本文致力于解决知识追踪中的因果发现问题，即通过实际学生反应数据找到不同技能之间的潜在因果关系。该问题的重要性在于可以帮助我们了解不同技能之间的因果关系，而无需进行大量的A/B测试，这可以帮助教育工作者根据技能先决条件信息设计更好的课程。具体而言，我们提出了一个概念解决方案，在修改后的深度知识追踪模型中引入了一个新颖的因果门循环单元（GRU）模块，该模块使用了i）一个可学习的置换矩阵来确定技能间的因果顺序，和ii）一个可选可学习的下三角矩阵来表示技能间的因果结构。我们还详细介绍了如何以端到端、可微的方式学习模型参数。我们的解决方案在NeurIPS 2022关于学习路径因果洞察的挑战赛任务3中表现出色。

    In this paper, we take a preliminary step towards solving the problem of causal discovery in knowledge tracing, i.e., finding the underlying causal relationship among different skills from real-world student response data. This problem is important since it can potentially help us understand the causal relationship between different skills without extensive A/B testing, which can potentially help educators to design better curricula according to skill prerequisite information. Specifically, we propose a conceptual solution, a novel causal gated recurrent unit (GRU) module in a modified deep knowledge tracing model, which uses i) a learnable permutation matrix for causal ordering among skills and ii) an optionally learnable lower-triangular matrix for causal structure among skills. We also detail how to learn the model parameters in an end-to-end, differentiable way. Our solution placed among the top entries in Task 3 of the NeurIPS 2022 Challenge on Causal Insights for Learning Paths i
    
[^118]: 关于在量子计算机上采样确定性行列式和Pfaffian点过程

    On sampling determinantal and Pfaffian point processes on a quantum computer. (arXiv:2305.15851v1 [stat.CO])

    [http://arxiv.org/abs/2305.15851](http://arxiv.org/abs/2305.15851)

    本文总结了在量子计算机上采样确定性行列式和Pfaffian点过程的状态及其优化方式。

    

    确定性点过程(DPP) 最早被 Macchi 作为量子光学模型引入，自那以后，它们已广泛用作统计学和计算机科学中的模型和子抽样工具。大多数应用需要从DPP抽样，考虑到其量子起源，自然会想知道在量子计算机上抽样DPP是否比在经典计算机上更容易。本文关注于有限状态空间上的DPP，这是一个在$\{1,\dots,N\}$子集上的分布，由一个$N\times N$的Hermite内核矩阵参数化。最基本的采样包括两个步骤，在经典计算机上分别需要 $\mathcal{O}(N^3)$ 和 $\mathcal{O}(Nr^2)$ 的操作成本，其中$r$是内核矩阵的秩。本文旨在讨论量子计算机上的DPP采样算法的状态及其优化方式。

    DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since then, they have been widely used as models and subsampling tools in statistics and computer science. Most applications require sampling from a DPP, and given their quantum origin, it is natural to wonder whether sampling a DPP on a quantum computer is easier than on a classical one. We focus here on DPPs over a finite state space, which are distributions over the subsets of $\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanilla sampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and $\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rank of the kernel matrix. A large first part of the current paper consists in explaining why the state-of-the-art in quantum simulation of fermionic systems already yields quantum DPP sampling algorithms. We then modify existing quantum circuits, and discuss their insertion in a full DPP sampling pipeline that start
    
[^119]: 带有确定性策略搜索的离策略平均回报行动者-评论家算法

    Off-Policy Average Reward Actor-Critic with Deterministic Policy Search. (arXiv:2305.12239v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12239](http://arxiv.org/abs/2305.12239)

    本文介绍了带有确定性策略搜索的离策略平均回报行动者-评论家算法，并提出了基于策略和离策略的确定性策略梯度定理。使用这些定理，本文还提出了一种平均回报离策略深度确定性策略梯度算法（ARO-DDPG）。该算法在渐近收敛性分析和有限时间分析中展示了较好的性能，并获得了$\epsilon$-最优稳定策略。

    

    平均回报准则相对较少被研究，因为强化学习文献中的大多数现有工作考虑了贴现回报准则。近期有一些关于基于策略的平均回报行动者-评论家算法的工作，但离策略平均回报行动者-评论家算法相对较少探索。本文提出了关于平均回报性能准则的基于策略和离策略确定性策略梯度定理。利用这些定理，我们还提出了一种平均回报离策略深度确定性策略梯度算法（ARO-DDPG）。我们首先使用基于ODE的方法展示了渐近收敛性分析。随后，我们提供了结果随机逼近方案的有限时间分析，使用线性函数逼近器获得了一个$\epsilon$-最优稳定策略，其样本复杂度为$\Omega(\epsilon^{-2.5})$。我们比较了我们提出的ARO-DDPG算法的平均回报性能，并观察到更好的经验表现。

    The average reward criterion is relatively less studied as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this work, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We first show asymptotic convergence analysis using the ODE-based method. Subsequently, we provide a finite time analysis of the resulting stochastic approximation scheme with linear function approximator and obtain an $\epsilon$-optimal stationary policy with a sample complexity of $\Omega(\epsilon^{-2.5})$. We compare the average reward performance of our proposed ARO-DDPG algorithm and observe better empir
    
[^120]: DeepMSS：基于PET/CT图像的深度多模态切片到生存预测学习

    DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images. (arXiv:2305.09946v1 [eess.IV])

    [http://arxiv.org/abs/2305.09946](http://arxiv.org/abs/2305.09946)

    提出了一种DeepMSS模型，采用新颖的Segmentated-to-Survival（STS）框架，使用多模态渐进聚合网络（MMPAN）来探索肿瘤内外的预后信息，并通过自我注意力机制增强的深度生存模型进行生存预测，取得了在两个公共PET/CT图像数据集上优于几种最先进的方法的结果。

    

    生存预测是癌症管理的主要关注点。基于深度学习的深度生存模型已被广泛采用，用于在医学图像上执行端到端的生存预测。最近的深度生存模型通过联合执行肿瘤分割和生存预测，采用多任务学习指导模型提取与肿瘤相关的信息，取得了有希望的性能。然而，现有的深度生存模型在探索肿瘤外预后信息（例如，局部淋巴结转移和邻近组织侵袭）方面存在困难。此外，现有的深度生存模型在利用多模态图像方面欠发展。为了解决这些问题，我们提出了一种名为DeepMSS的深度多模态切片到生存模型。该模型采用一种新颖的Segmentated-to-Survival（STS）框架，通过分离分割和生存预测任务来进行。对于分割，我们使用一种新颖的多模态渐进聚合网络（MMPAN）来探索肿瘤内外的预后信息。对于生存预测，我们提出了一种自我注意力机制增强的深度生存模型，该模型学习MMPAN的特征表示并执行生存预测。在两个公共PET/CT图像数据集上的实验结果表明，我们提出的DeepMSS模型在生存预测方面优于几种最先进的方法。

    Survival prediction is a major concern for cancer management. Deep survival models based on deep learning have been widely adopted to perform end-to-end survival prediction from medical images. Recent deep survival models achieved promising performance by jointly performing tumor segmentation with survival prediction, where the models were guided to extract tumor-related information through Multi-Task Learning (MTL). However, existing deep survival models have difficulties in exploring out-of-tumor prognostic information (e.g., local lymph node metastasis and adjacent tissue invasions). In addition, existing deep survival models are underdeveloped in utilizing multi-modality images. Empirically-designed strategies were commonly adopted to fuse multi-modality information via fixed pre-designed networks. In this study, we propose a Deep Multi-modality Segmentation-to-Survival model (DeepMSS) for survival prediction from PET/CT images. Instead of adopting MTL, we propose a novel Segmentat
    
[^121]: 概率距离法异常检测

    Probabilistic Distance-Based Outlier Detection. (arXiv:2305.09446v1 [cs.LG])

    [http://arxiv.org/abs/2305.09446](http://arxiv.org/abs/2305.09446)

    本文提出了一种将距离法异常检测分数转化为可解释的概率估计的通用方法，该方法使用与其他数据点的距离建模距离概率分布，将距离法异常检测分数转换为异常概率，提高了正常点和异常点之间的对比度，而不会影响检测性能。

    

    距离法异常检测方法的分数难以解释，因此在没有额外的上下文信息的情况下，很难确定正常点和异常点之间的截断阈值。我们描述了将距离法异常检测分数转化为可解释的概率估计的通用方法。该转换是排名稳定的，并增加了正常点和异常点之间的对比度。确定数据点之间的距离关系是识别数据中最近邻关系所必需的，然而大多数计算出的距离通常被丢弃。我们展示了可以使用与其他数据点的距离来建模距离概率分布，并随后使用这些分布将距离法异常检测分数转换为异常概率。我们的实验表明，概率转换不会影响众多表格和图像基准数据集上的检测性能，但会产生可解释性。

    The scores of distance-based outlier detection methods are difficult to interpret, making it challenging to determine a cut-off threshold between normal and outlier data points without additional context. We describe a generic transformation of distance-based outlier scores into interpretable, probabilistic estimates. The transformation is ranking-stable and increases the contrast between normal and outlier data points. Determining distance relationships between data points is necessary to identify the nearest-neighbor relationships in the data, yet, most of the computed distances are typically discarded. We show that the distances to other data points can be used to model distance probability distributions and, subsequently, use the distributions to turn distance-based outlier scores into outlier probabilities. Our experiments show that the probabilistic transformation does not impact detection performance over numerous tabular and image benchmark datasets but results in interpretable
    
[^122]: CB-HVTNet：一种用于组织病理学图像中淋巴细胞评估的通道增强混合视觉 Transformer 网络

    CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v1 [eess.IV])

    [http://arxiv.org/abs/2305.09211](http://arxiv.org/abs/2305.09211)

    CB-HVTNet 提出了一种 Channel Boosted Hybrid Vision Transformer 网络，利用迁移学习生成增强通道，并结合使用 Transformers 和 CNN，在组织病理学图像中高效准确地评估淋巴细胞。

    

    Transformer 由于其学习长距离依赖性的能力已经克服了卷积神经网络（CNN）全局透视学习的缺点。因此，它们已经成为研究人员关注的焦点，用于多个与视觉相关的任务，包括医疗诊断。然而，它们的多头注意模块仅捕获全局级别的特征表示，这对于医学图像来说是不足的。为了解决这个问题，我们提出了一种 Channel Boosted Hybrid Vision Transformer（CB HVT），它利用迁移学习生成增强通道，并使用 Transformers 和 CNN 来分析组织病理学图像中的淋巴细胞。所提出的 CB HVT 包括五个模块，包括通道生成模块、通道利用模块、通道合并模块、区域感知模块和检测和分段头，它们共同有效地识别淋巴细胞。通道生成模块使用通过迁移学习进行通道增强的思想创建多个强大的通道，然后与 Transformers 和 CNN 结合使用，以更好地分析组织病理学图像中的淋巴细胞。总体而言，所提出的 CB HVT 是医学诊断中准确、高效评估淋巴细胞的强大工具。

    Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning to generate boosted channels and employs both transformers and CNNs to analyse lymphocytes in histopathological images. The proposed CB HVT comprises five modules, including a channel generation module, channel exploitation module, channel merging module, region-aware module, and a detection and segmentation head, which work together to effectively identify lymphocytes. The channel generation module uses the idea of channel boosting through transfer learni
    
[^123]: Network-GIANT: 基于哈蒙莫特 Hessian 一致性的全分布式牛顿型优化

    Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus. (arXiv:2305.07898v1 [math.OC])

    [http://arxiv.org/abs/2305.07898](http://arxiv.org/abs/2305.07898)

    本文提出了一种基于哈蒙莫特 Hessian 一致性的全分布式牛顿型优化算法 Network-GIANT，将梯度跟踪和牛顿型迭代算法相结合，经证明对严格凸和光滑损失函数有半全局和指数收敛到精确解的保证，实验证明 Network-GIANT 优于其他分布式学习算法（如 Network-DANE 和 Newton-Raphson Consensus）的收敛性能。

    

    本文考虑了分布式多代理学习的问题，其中全局目标是通过本地优化和节点之间的信息交换来最小化本地目标（经验损失）函数的总和。 我们介绍了一种新的牛顿型完全分布式优化算法，Network-GIANT，它基于 GIANT，这是一种依赖于集中式参数服务器的联邦学习算法。 Network-GIANT 算法是通过在每个节点上使用梯度跟踪和牛顿型迭代算法的组合以及本地梯度和牛顿更新的共识平均来设计的。我们证明了我们的算法保证了对网络上的严格凸和光滑损失函数的半全局和指数收敛到精确解。我们提供了 Network-GIANT 优于其他最先进的分布式学习算法（如 Network-DANE 和 Newton-Raphson Consensus）的收敛性能的实证证据。

    This paper considers the problem of distributed multi-agent learning, where the global aim is to minimize a sum of local objective (empirical loss) functions through local optimization and information exchange between neighbouring nodes. We introduce a Newton-type fully distributed optimization algorithm, Network-GIANT, which is based on GIANT, a Federated learning algorithm that relies on a centralized parameter server. The Network-GIANT algorithm is designed via a combination of gradient-tracking and a Newton-type iterative algorithm at each node with consensus based averaging of local gradient and Newton updates. We prove that our algorithm guarantees semi-global and exponential convergence to the exact solution over the network assuming strongly convex and smooth loss functions. We provide empirical evidence of the superior convergence performance of Network-GIANT over other state-of-art distributed learning algorithms such as Network-DANE and Newton-Raphson Consensus.
    
[^124]: 大纲先行，细节后至：基于语法引导的粗-细代码生成

    Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2305.00909](http://arxiv.org/abs/2305.00909)

    提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。

    

    对于一个复杂算法的实现，人类程序员的做法通常是先概述一下控制流程，然后迭代进行丰富，最终生成一些精心加工的语法结构和层次变量。然而，现有的大型语言模型一次性生成代码，没有中间环节，以反映"大纲先行，细节后至"的结构化思维过程。受到思维链提示的最新成功启发，我们提出了ChainCoder，这是一种程序综合语言模型，它逐步生成Python代码，即从粗到细进行多次迭代。我们首先通过抽象语法树解析将源代码分解为布局框架组件和附件组件，以构建层次表示。然后我们将预测目标重新启动，形成多次通过目标，每次生成一个子序列，这些子序列在层次结构中串联起来。最后，我们利用量身定制的Transformer体系结构来实现模型的优化。

    For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
    
[^125]: 揭示个人信息学中的偏见

    Uncovering Bias in Personal Informatics. (arXiv:2303.15592v1 [cs.CY])

    [http://arxiv.org/abs/2303.15592](http://arxiv.org/abs/2303.15592)

    该论文是第一个对个人信息学系统中的偏见进行实证和分析研究的工作，研究包括原始数据和整个机器学习周期中的偏见，并找出其中的实践和道德影响。

    

    由智能手机和可穿戴设备驱动的个人信息学（PI）系统，通过提供有意义且可执行的见解，消除了用户与其健康信息之间的障碍，使人们能够过上更健康的生活。今天，数十亿用户使用此类系统来监测不仅是身体活动和睡眠，还有生命体征、女性健康和心脏健康等。尽管被广泛使用，并且处理敏感的PI数据，但偏见问题尚未得到系统的调查。这项工作是第一个对包括原始数据和整个机器学习生命周期中的偏见进行全面实证和分析研究的工作，并使用迄今为止最详细的框架。

    Personal informatics (PI) systems, powered by smartphones and wearables, enable people to lead healthier lifestyles by providing meaningful and actionable insights that break down barriers between users and their health information. Today, such systems are used by billions of users for monitoring not only physical activity and sleep but also vital signs and women's and heart health, among others. %Despite their widespread usage, the processing of particularly sensitive personal data, and their proximity to domains known to be susceptible to bias, such as healthcare, bias in PI has not been investigated systematically. Despite their widespread usage, the processing of sensitive PI data may suffer from biases, which may entail practical and ethical implications. In this work, we present the first comprehensive empirical and analytical study of bias in PI systems, including biases in raw data and in the entire machine learning life cycle. We use the most detailed framework to date for exp
    
[^126]: 机器学习中公平性的关键回顾：超越准确性在移动和可穿戴计算中的应用

    Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing. (arXiv:2303.15585v1 [cs.CY])

    [http://arxiv.org/abs/2303.15585](http://arxiv.org/abs/2303.15585)

    本文通过对IMWUT期刊上过去五年发表的论文进行系统回顾，发现UbiComp社区在算法公平方面的进展滞后，存在敏感属性偏差导致的歧视性结果，需要探索报告数据集的信息以解决这些偏差。

    

    移动、可穿戴和普及计算领域正在经历着机器学习的革命性整合。设备现在可以诊断疾病、预测心脏不规则动，发掘人类认知的全部潜力。然而，相关算法在敏感属性（如性别、种族等）方面可能存在偏差，导致歧视性结果。近期，人机交互（HCI）和人工智能伦理学（AI-Ethics）研究社区开始探索报告数据集的信息以揭示并最终对抗这些偏差。本文旨在探讨在这些报告方面UbiComp社区所采纳的程度，并强调潜在不足之处。通过对过去五年（2018-2022）在ACM交互、移动、可穿戴和普适技术（IMWUT）期刊上发表的论文进行系统回顾，我们发现UbiComp社区在算法公平方面的进展滞后。

    The field of mobile, wearable, and ubiquitous computing (UbiComp) is undergoing a revolutionary integration of machine learning. Devices can now diagnose diseases, predict heart irregularities, and unlock the full potential of human cognition. However, the underlying algorithms are not immune to biases with respect to sensitive attributes (e.g., gender, race), leading to discriminatory outcomes. The research communities of HCI and AI-Ethics have recently started to explore ways of reporting information about datasets to surface and, eventually, counter those biases. The goal of this work is to explore the extent to which the UbiComp community has adopted such ways of reporting and highlight potential shortcomings. Through a systematic review of papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal over the past 5 years (2018-2022), we found that progress on algorithmic fairness within the UbiComp community lags behind. 
    
[^127]: Sionna RT：无线电传播建模的可微分光线追踪技术

    Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling. (arXiv:2303.11103v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2303.11103](http://arxiv.org/abs/2303.11103)

    Sionna RT是一个GPU加速的开源库，它集成了可微分的光线追踪器，可以用于模拟无线电波传播。这个功能使得可以计算与多个系统和环境参数有关的量的梯度，对于诸如学习无线电材料和优化发射机方向等应用具有重要价值。同时，可微分光线追踪对于新颖的研究方向如数字孪生也是一个关键的推动者。

    

    Sionna是一个基于TensorFlow的GPU加速开源库，用于链路级模拟。自v0.14版本以来，它集成了一个可微分的光线追踪器（RT），用于模拟无线电波传播。这个独特功能允许计算与许多系统和环境参数有关的信道冲激响应和其他相关量的梯度，例如材料特性、天线图案、阵列几何、发射机和接收机的方向和位置。在本文中，我们概述了Sionna RT的关键组成部分，并展示了学习无线电材料和通过梯度下降优化发射机方向等示例应用。虽然经典的光线追踪对于6G研究课题如可重构智能表面、集成感知与通信以及用户定位是一个重要工具，但可微分光线追踪是许多新颖和令人兴奋的研究方向的关键推动者，例如数字孪生。

    Sionna is a GPU-accelerated open-source library for link-level simulations based on TensorFlow. Since release v0.14 it integrates a differentiable ray tracer (RT) for the simulation of radio wave propagation. This unique feature allows for the computation of gradients of the channel impulse response and other related quantities with respect to many system and environment parameters, such as material properties, antenna patterns, array geometries, as well as transmitter and receiver orientations and positions. In this paper, we outline the key components of Sionna RT and showcase example applications such as learning radio materials and optimizing transmitter orientations by gradient descent. While classic ray tracing is a crucial tool for 6G research topics like reconfigurable intelligent surfaces, integrated sensing and communications, as well as user localization, differentiable ray tracing is a key enabler for many novel and exciting research directions, for example, digital twins.
    
[^128]: 基于深度卷积神经网络伪影降噪的稀疏视图CT图像自动出血检测的改进

    Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v1 [eess.IV])

    [http://arxiv.org/abs/2303.09340](http://arxiv.org/abs/2303.09340)

    本文提出了一种基于深度卷积神经网络的伪影降噪方法，用于改善稀疏视图下自动出血检测的图像质量，并证明其能够与完全采样的图像进行同等精确度的分类和检测。

    

    颅内出血是一种严重的健康问题，需要快速且常常非常密集的医疗治疗。为了诊断，通常要进行颅部计算机断层扫描（CCT）扫描。然而，由于辐射引起的增加的健康风险是一个问题。降低这种潜在风险的最重要策略是尽可能保持辐射剂量低，并与诊断任务一致。 稀疏视图CT可以通过减少所采集的视图总数，从而降低剂量，是一种有效的策略，但代价是降低图像质量。在这项工作中，我们使用U-Net架构来减少稀疏视图CCT的伪影，从稀疏视图中预测完全采样的重建图像。我们使用一个卷积神经网络对出血的检测和分类进行评估，并在完全采样的CCT上进行训练。我们的结果表明，伪影降噪后的CCT图像进行自动分类和检测的准确性与完全采样的CCT图像没有明显差异。

    Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. However, the increased health risk caused by radiation is a concern. The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality. In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. Our results suggest that the automated classification and detection accuracy of hemo
    
[^129]: 布雷-瓦瑟斯坦距离训练下的生成式深度线性网络的关键点和收敛性分析

    Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss. (arXiv:2303.03027v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.03027](http://arxiv.org/abs/2303.03027)

    本文使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型，并在有限秩矩阵空间内表征关键点和最小化问题，最终确定了梯度下降算法的收敛性。

    

    本文探讨了一种使用布雷-瓦瑟斯坦距离训练协方差矩阵的深度矩阵分解模型。相较于以往的研究，我们所提出的模型在损失函数和生成式设置上有所不同。我们在有限秩矩阵空间内表征了该方法的关键点和最小化问题。针对低秩矩阵而言，该方法的海森矩阵理论上可能会爆炸，这为优化方法的收敛性分析带来了挑战。我们确定了梯度下降算法中使用损失的平滑微扰版本时的收敛性，并在初始权重的一定假设条件下证明了有限步长梯度下降的收敛性。

    We consider a deep matrix factorization model of covariance matrices trained with the Bures-Wasserstein distance. While recent works have made important advances in the study of the optimization problem for overparametrized low-rank matrix approximation, much emphasis has been placed on discriminative settings and the square loss. In contrast, our model considers another interesting type of loss and connects with the generative setting. We characterize the critical points and minimizers of the Bures-Wasserstein distance over the space of rank-bounded matrices. For low-rank matrices the Hessian of this loss can theoretically blow up, which creates challenges to analyze convergence of optimizaton methods. We establish convergence results for gradient flow using a smooth perturbative version of the loss and convergence results for finite step size gradient descent under certain assumptions on the initial weights.
    
[^130]: 通过随机特征传播实现图位置编码

    Graph Positional Encoding via Random Feature Propagation. (arXiv:2303.02918v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02918](http://arxiv.org/abs/2303.02918)

    本文提出了一种新的位置编码方案，Random Feature Propagation (RFP)，通过串联迭代算法的中间步骤以计算传播矩阵的主特征向量，从而改进了现有的随机特征和谱位置编码方案。

    

    为了增强GNN，已经研究了两种主要的节点特征增强方案：随机特征和谱位置编码。然而，令人惊讶的是，这两种增强方案之间的关系仍然没有清晰的理解。在这里，我们提出了一种新颖的位置编码方案，它建立了上述两种方法之间的联系，并改进了它们。这种名为Random Feature Propagation (RFP)的新方法受到了幂迭代方法及其推广的启发。它将用于计算传播矩阵的主特征向量的迭代算法的几个中间步骤进行串联，从随机节点特征开始。值得注意的是，这些传播步骤是基于图相关的传播算子的，这些算子可以是预定义的，也可以是学习得到的。我们探讨了RFP的理论和经验优势。首先，我们为使用随机特征、将早期传播步骤纳入考虑以及使用图相关的传播算子提供了理论上的证明。

    Two main families of node feature augmentation schemes have been explored for enhancing GNNs: random features and spectral positional encoding. Surprisingly, however, there is still no clear understanding of the relation between these two augmentation schemes. Here we propose a novel family of positional encoding schemes which draws a link between the above two approaches and improves over both. The new approach, named Random Feature Propagation (RFP), is inspired by the power iteration method and its generalizations. It concatenates several intermediate steps of an iterative algorithm for computing the dominant eigenvectors of a propagation matrix, starting from random node features. Notably, these propagation steps are based on graph-dependent propagation operators that can be either predefined or learned. We explore the theoretical and empirical benefits of RFP. First, we provide theoretical justifications for using random features, for incorporating early propagation steps, and for
    
[^131]: 通过最大泄露分析噪声迭代算法的泛化误差界限

    Generalization Error Bounds for Noisy, Iterative Algorithms via Maximal Leakage. (arXiv:2302.14518v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14518](http://arxiv.org/abs/2302.14518)

    通过最大泄露分析噪声迭代算法的泛化误差界限，证明了如果更新函数在L2-范数下有界且加性噪声为各向同性高斯噪声，则可以得到一个半封闭形式下的最大泄露上界，同时展示了更新函数的假设如何影响噪声的最优选择。

    

    我们采用信息论框架来分析一类迭代式、带有噪声的学习算法的泛化行为。由于这类算法具有随机性，并且包含常用的算法（如随机梯度 Langevin 动力学），所以在信息论度量下研究它们尤为合适。在本文中，我们使用最大泄露（等价于无穷阶 Sibson 互信息）度量，因其易于分析且可以同时获得泛化误差大概率上界和期望值上界。我们证明了如果更新函数（如梯度）在L2-范数下有界，且加性噪声为各向同性高斯噪声，则可以得到一个半封闭形式下的最大泄露上界。另外，我们还展示了更新函数的假设如何影响噪声的最优选择（即最小化产生的最大泄露）。最后，我们计算了...

    We adopt an information-theoretic framework to analyze the generalization behavior of the class of iterative, noisy learning algorithms. This class is particularly suitable for study under information-theoretic metrics as the algorithms are inherently randomized, and it includes commonly used algorithms such as Stochastic Gradient Langevin Dynamics (SGLD). Herein, we use the maximal leakage (equivalently, the Sibson mutual information of order infinity) metric, as it is simple to analyze, and it implies both bounds on the probability of having a large generalization error and on its expected value. We show that, if the update function (e.g., gradient) is bounded in $L_2$-norm and the additive noise is isotropic Gaussian noise, then one can obtain an upper-bound on maximal leakage in semi-closed form. Furthermore, we demonstrate how the assumptions on the update function affect the optimal (in the sense of minimizing the induced maximal leakage) choice of the noise. Finally, we compute 
    
[^132]: 具有星系的鲁棒场地级别无相似度推断

    Robust Field-level Likelihood-free Inference with Galaxies. (arXiv:2302.14101v2 [astro-ph.CO] UPDATED)

    [http://arxiv.org/abs/2302.14101](http://arxiv.org/abs/2302.14101)

    通过训练图神经网络，我们实现了使用星系目录进行场地级别的无相似度推断，可以在不受天文物理学和子网格模型变化影响的情况下，以高精度推断出Ωm的值。

    

    我们使用来自CAMELS项目最先进的流体动力学模拟的星系目录来训练图形神经网络，实现场地级别的无相似度推断。我们的模型具有旋转、平移和置换的不变性，并且不对尺度施加任何限制。从仅包含大约1000个星系的三维位置和径向速度的星系目录中，我们的模型可以以大约12%的精度推断出Ωm的值。更重要的是，通过在来自数千个流体动力学模拟的星系目录上测试模型，每个模拟都有不同的超新星和AGN反馈效率，并使用五种不同的代码和子网格模型运行- IllustrisTNG，SIMBA，Astrid，Magneticum，SWIFT-EAGLE - 我们发现我们的模型对天体物理学、子网格物理学和亚哈洛/星系发现器的变化是稳健的。此外，我们在涵盖参数空间广泛的1,024个模拟上测试了我们的模型- 变化

    We train graph neural networks to perform field-level likelihood-free inference using galaxy catalogs from state-of-the-art hydrodynamic simulations of the CAMELS project. Our models are rotational, translational, and permutation invariant and do not impose any cut on scale. From galaxy catalogs that only contain $3$D positions and radial velocities of $\sim 1, 000$ galaxies in tiny $(25~h^{-1}{\rm Mpc})^3$ volumes our models can infer the value of $\Omega_{\rm m}$ with approximately $12$ % precision. More importantly, by testing the models on galaxy catalogs from thousands of hydrodynamic simulations, each having a different efficiency of supernova and AGN feedback, run with five different codes and subgrid models - IllustrisTNG, SIMBA, Astrid, Magneticum, SWIFT-EAGLE -, we find that our models are robust to changes in astrophysics, subgrid physics, and subhalo/galaxy finder. Furthermore, we test our models on $1,024$ simulations that cover a vast region in parameter space - variation
    
[^133]: CO-BED：通过贝叶斯实验设计的信息理论上下文优化

    CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design. (arXiv:2302.14015v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.14015](http://arxiv.org/abs/2302.14015)

    CO-BED是一个通用的、与模型无关的框架，用于通过贝叶斯实验设计的信息理论来进行上下文优化。它采用黑箱变分方法同时估计和优化设计，可以适应离散动作，并在多个实验中展示出竞争性能。

    

    我们通过贝叶斯实验设计的视角对上下文优化问题进行了形式化，并提出了CO-BED - 一个通用的、与模型无关的框架，用于使用信息理论原则设计上下文实验。在制定合适的基于信息的目标后，我们采用黑箱变分方法在单一随机梯度方案中同时估计和优化设计。此外，为了适应我们框架中的离散动作，我们提议利用连续松弛方案，这可以自然地集成到我们变分目标中。因此，CO-BED为各种上下文优化问题提供了通用的自动化解决方案。我们在许多实验中演示了其有效性，即使与定制的、特定于模型的替代方法相比，CO-BED也表现出了竞争性能。

    We formalize the problem of contextual optimization through the lens of Bayesian experimental design and propose CO-BED -- a general, model-agnostic framework for designing contextual experiments using information-theoretic principles. After formulating a suitable information-based objective, we employ black-box variational methods to simultaneously estimate it and optimize the designs in a single stochastic gradient scheme. In addition, to accommodate discrete actions within our framework, we propose leveraging continuous relaxation schemes, which can naturally be integrated into our variational objective. As a result, CO-BED provides a general and automated solution to a wide range of contextual optimization problems. We illustrate its effectiveness in a number of experiments, where CO-BED demonstrates competitive performance even when compared to bespoke, model-specific alternatives.
    
[^134]: AlpaServe：用于深度学习服务的模型并行的统计复用

    AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving. (arXiv:2302.11665v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11665](http://arxiv.org/abs/2302.11665)

    AlpaServe是一种新颖的服务系统，它利用模型并行和统计复用，在提供多个模型服务时降低延迟，提高处理速率和突发负载处理能力。

    

    传统上，模型并行被视为一种将单个大型深度学习模型扩展到单个设备内存限制之外的方法。本文证明了即使单个模型可以适应单个设备，模型并行还可以用于多个模型的统计复用，以降低提供服务时的延迟。我们探索了新的权衡空间，并提出了一种新颖的服务系统AlpaServe，它确定了在分布式集群中放置和并行处理大型深度学习模型集合的高效策略。生产工作负载上的评估结果显示，AlpaServe可以在满足超过99%请求的延迟约束的同时，以高达10倍的速率处理请求或者处理6倍以上的突发负载。

    Model parallelism is conventionally viewed as a method to scale a single large deep learning model beyond the memory limits of a single device. In this paper, we demonstrate that model parallelism can be additionally used for the statistical multiplexing of multiple devices when serving multiple models, even when a single model can fit into a single device. Our work reveals a fundamental trade-off between the overhead introduced by model parallelism and the opportunity to exploit statistical multiplexing to reduce serving latency in the presence of bursty workloads. We explore the new trade-off space and present a novel serving system, AlpaServe, that determines an efficient strategy for placing and parallelizing collections of large deep learning models across a distributed cluster. Evaluation results on production workloads show that AlpaServe can process requests at up to 10x higher rates or 6x more burstiness while staying within latency constraints for more than 99% of requests.
    
[^135]: 在可解释人工智能中的元评估问题：使用MetaQuantus识别可靠的估计器

    The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus. (arXiv:2302.07265v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07265](http://arxiv.org/abs/2302.07265)

    这项研究解决了可解释人工智能领域中关于在缺乏真实解释标签的情况下如何可靠估算解释方法质量的问题。通过对不同质量估计器进行元评估，利用MetaQuantus框架分析了估计器的韧性和反应特征，从而帮助实践者选择最佳的解释方法。

    

    在可解释人工智能（XAI）领域中，确定在没有真实解释标签的情况下最可靠地估算解释方法的质量是一个尚未解决的挑战。解决这个问题至关重要，因为竞争评估方法（或“质量估计器”）生成的评估结果，旨在衡量解释方法的相同性质，经常呈现出不一致的排名。这样的分歧对于实践者来说很难解释，从而使他们难以选择表现最好的解释方法。我们通过对XAI中的不同质量估计器进行元评估（"评估评估方法的过程"）来解决这个问题。我们的新框架MetaQuantus分析了质量估计器的两个互补性性能特征：对噪声的韧性和对随机性的反应，从而避免了对真实标签的需求。

    One of the unsolved challenges in the field of Explainable AI (XAI) is determining how to most reliably estimate the quality of an explanation method in the absence of ground truth explanation labels. Resolving this issue is of utmost importance as the evaluation outcomes generated by competing evaluation methods (or ''quality estimators''), which aim at measuring the same property of an explanation method, frequently present conflicting rankings. Such disagreements can be challenging for practitioners to interpret, thereby complicating their ability to select the best-performing explanation method. We address this problem through a meta-evaluation of different quality estimators in XAI, which we define as ''the process of evaluating the evaluation method''. Our novel framework, MetaQuantus, analyses two complementary performance characteristics of a quality estimator: its resilience to noise and reactivity to randomness, thus circumventing the need for ground truth labels. We demonstr
    
[^136]: ConCerNet：一种基于对比学习的自动发现守恒律和可靠动力学系统预测框架

    ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. (arXiv:2302.05783v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05783](http://arxiv.org/abs/2302.05783)

    本文提出了一种基于对比学习的框架ConCerNet，用于提高DNN动力学建模的可靠性，实现对系统不变量的自动捕捉和保留，经实验证明其性能优于传统神经网络方法。

    

    深度神经网络(DNN)在动力学系统建模方面表现出极大的能力；然而，它们通常不遵守物理约束，如守恒定律。本文提出了一种名为ConCerNet的新的学习框架，以提高基于DNN的动力学建模的可靠性，赋予不变的属性。ConCerNet由两个步骤组成:(i)对比学习方法，自动捕捉轨迹观测中的系统不变量(即守恒性质)；(ii)神经投影层，保证学习到的动力学模型保留学习到的不变量。我们从理论上证明了学习到的潜在表示和未知系统不变量函数之间的功能关系。实验表明，我们的方法在坐标误差和守恒指标方面始终比基线神经网络表现出更好的效果。使用基于神经网络的参数化且不依赖于先前知识，我们的方法在动力学方面具有广阔的应用前景。

    Deep neural networks (DNN) have shown great capacity of modeling a dynamical system; nevertheless, they usually do not obey physics constraints such as conservation laws. This paper proposes a new learning framework named ConCerNet to improve the trustworthiness of the DNN based dynamics modeling to endow the invariant properties. ConCerNet consists of two steps: (i) a contrastive learning method to automatically capture the system invariants (i.e. conservation properties) along the trajectory observations; (ii) a neural projection layer to guarantee that the learned dynamics models preserve the learned invariants. We theoretically prove the functional relationship between the learned latent representation and the unknown system invariant function. Experiments show that our method consistently outperforms the baseline neural networks in both coordinate error and conservation metrics by a large margin. With neural network based parameterization and no dependence on prior knowledge, our 
    
[^137]: 通过使替代模型更贝叶斯化，可以增强对抗性样本的可迁移性

    Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples. (arXiv:2302.05086v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05086](http://arxiv.org/abs/2302.05086)

    本文通过使替代模型更贝叶斯化，提出了一种攻击贝叶斯模型以实现理想的对抗性样本可迁移性的方法，通过实验证明了该方法的有效性，并在常见基准数据集上优于最新方法。

    

    对抗性样本在深度神经网络之间的可迁移性是许多黑盒攻击的关键。以往的努力主要集中在提高一些替代模型输入的多样性上以改善可迁移性。相比之下，本文选择了替代模型的多样性，并提出攻击贝叶斯模型以实现理想的可迁移性。基于贝叶斯公式，我们提出了一种可行的精调策略，该策略可以与基于深度神经网络参数的许多常见高斯后验逼近方法相结合。通过大量实验证明了我们方法的有效性，这些实验证明我们方法在常见的基准数据集上优于最新的现有方法（在ImageNet上攻击成功率的平均绝对增加率约为19%），并且与这些最新方法的结合可以进一步提高性能。我们的代码: https:

    The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https:
    
[^138]: 顺序核独立性测试

    Sequential Kernelized Independence Testing. (arXiv:2212.07383v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.07383](http://arxiv.org/abs/2212.07383)

    该论文介绍了顺序核独立性测试的方法，以解决传统批量测试在流数据上的问题，实现了根据任务复杂性自适应调整样本大小，并在收集新数据后持续监测和控制误报率。

    

    独立性测试是一个经典的统计问题，在固定采集数据之前的批量设置中得到了广泛研究。然而，实践者们往往更喜欢能够根据问题的复杂性进行自适应的程序，而不是事先设定样本大小。理想情况下，这样的程序应该（a）在简单任务上尽早停止（在困难任务上稍后停止），因此更好地利用可用资源，以及（b）在收集新数据之后，持续监测数据并高效地整合统计证据，同时控制误报率。经典的批量测试不适用于流数据：在数据观察后进行有效推断需要对多重测试进行校正，这导致了低功率。遵循通过投注进行测试的原则，我们设计了顺序核独立性测试，克服了这些缺点。我们通过采用由核相关性测度（如Hilbert-）启发的投注来说明我们的广泛框架。

    Independence testing is a classical statistical problem that has been extensively studied in the batch setting when one fixes the sample size before collecting data. However, practitioners often prefer procedures that adapt to the complexity of a problem at hand instead of setting sample size in advance. Ideally, such procedures should (a) stop earlier on easy tasks (and later on harder tasks), hence making better use of available resources, and (b) continuously monitor the data and efficiently incorporate statistical evidence after collecting new data, while controlling the false alarm rate. Classical batch tests are not tailored for streaming data: valid inference after data peeking requires correcting for multiple testing which results in low power. Following the principle of testing by betting, we design sequential kernelized independence tests that overcome such shortcomings. We exemplify our broad framework using bets inspired by kernelized dependence measures, e.g., the Hilbert-
    
[^139]: 在场学习者能否从演示中学习推理概念？

    Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01692](http://arxiv.org/abs/2212.01692)

    本文介绍了一种概念性少样本学习方法，以帮助在场学习者学习新技能。通过选择与预测示例共享可能信息的演示，这个方法可以在模型记忆独立的情况下区分模型的在场学习能力。

    

    大型语言模型展示了从少量输入-输出演示中学习新任务的新能力。然而，最近的研究表明，在场学习者大部分依赖于他们的预训练知识，如标签的情感，而不是在输入中找到新的关联性。然而，常用的少样本评估设置使用随机选择的在场演示无法区分模型从演示中学习新技能的能力，因为大部分随机选择的演示并不呈现超越暴露于新任务分布的预测的关系。为了在模型记忆独立的情况下区分模型的在场学习能力，我们引入了一个概念性少样本学习方法，选择与预测示例共享可能信息的演示。我们从注释解释中提取了一组这样的概念，并测量了模型展示这些概念可以获得多少好处。

    Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
    
[^140]: 一个指数增长的通用量子电路系列

    An exponentially-growing family of universal quantum circuits. (arXiv:2212.00736v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2212.00736](http://arxiv.org/abs/2212.00736)

    该论文介绍了两种新的指数增长的量子机器学习体系结构，能够解决量子机器学习中的贫瘠高原问题，提高了量子编码的表达能力。

    

    量子机器学习已成为一个引起越来越多关注的领域，但它存在一定的理论和硬件限制。特别是，消失梯度问题或称为贫瘠高原问题，使得对于拥有大量量子比特的电路，训练变得不可能，限制了数据科学家在解决问题时可以使用的量子比特数量。另外，独立的角度嵌入监督量子神经网络被证明能够产生具有与编码深度和编码应用于的并行比特数直接相关的截断傅里叶级数。傅里叶级数的次数限制了模型的表达能力。本工作引入了两种新的体系结构，其傅里叶级数的次数呈指数增长：顺序和并行的指数增长量子机器学习体系结构。通过在编码时高效地利用可用的希尔伯特空间，增加了量子编码的表达能力，从而实现了指数增长。

    Quantum machine learning has become an area of growing interest but has certain theoretical and hardware-specific limitations. Notably, the problem of vanishing gradients, or barren plateaus, renders the training impossible for circuits with high qubit counts, imposing a limit on the number of qubits that data scientists can use for solving problems. Independently, angle-embedded supervised quantum neural networks were shown to produce truncated Fourier series with a degree directly dependent on two factors: the depth of the encoding and the number of parallel qubits the encoding applied to. The degree of the Fourier series limits the model expressivity. This work introduces two new architectures whose Fourier degrees grow exponentially: the sequential and parallel exponential quantum machine learning architectures. This is done by efficiently using the available Hilbert space when encoding, increasing the expressivity of the quantum encoding. Therefore, the exponential growth allows s
    
[^141]: 重新审视Softmax在文本分类中的不确定性近似

    Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14037](http://arxiv.org/abs/2210.14037)

    本研究重新审视了Softmax在文本分类中的不确定性近似方法，并比较了基于MC Dropout的方法。实证分析发现，尽管MC dropout产生了最好的不确定性近似，但使用softmax也能产生相对准确的结果。

    

    文本分类中的不确定性近似是一个在领域适应和可解释性中应用广泛的重要领域。其中一种最常用的不确定性近似方法是蒙特卡罗（MC）Dropout，但由于需要多次前向传递，计算成本较高。相比之下，一种更便宜的方法是仅使用在单次前向传递中基于softmax的方法来估计模型的不确定性。然而，之前的研究表明，这种预测往往过于自信。本文通过在两种基本神经结构的五个数据集上进行彻底的实证分析，旨在探讨这两种方法之间的权衡。我们比较了softmax和MC Dropout的不确定性近似以及下游文本分类性能，同时比较了它们的运行时间（成本）和性能（效益）。我们发现，尽管MC dropout产生了最好的不确定性近似，但使用softmax也能产生相对准确的结果。

    Uncertainty approximation in text classification is an important area with applications in domain adaptation and interpretability. One of the most widely used uncertainty approximation methods is Monte Carlo (MC) Dropout, which is computationally expensive as it requires multiple forward passes through the model. A cheaper alternative is to simply use the softmax based on a single forward pass without dropout to estimate model uncertainty. However, prior work has indicated that these predictions tend to be overconfident. In this paper, we perform a thorough empirical analysis of these methods on five datasets with two base neural architectures in order to identify the trade-offs between the two. We compare both softmax and an efficient version of MC Dropout on their uncertainty approximations and downstream text classification performance, while weighing their runtime (cost) against performance (benefit). We find that, while MC dropout produces the best uncertainty approximations, usin
    
[^142]: 具有利普希茨非线性的单个神经元模型的主动学习

    Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13601](http://arxiv.org/abs/2210.13601)

    该论文提出一种针对具有利普希茨非线性的单个神经元模型的主动学习策略，该策略在敌对标签噪声下拟合线性函数，并在逼近保证方面具有强有力的可证明性能。

    

    我们考虑在敌对标签噪声下的对单个神经元模型进行主动学习的问题，有时也被称为“岭函数”。这些模型已被证明广泛有效地模拟物理现象，并构建用于偏微分方程的代理数据驱动模型。令人惊讶的是，我们证明对于具有任何利普希茨非线性（如ReLU函数、sigmoid函数、绝对值函数、低次多项式函数等）的单个神经元模型，可以使用已知的在敌对标签噪声下拟合“线性函数”的主动学习策略获得强有力的可证明的逼近保证。

    We consider the problem of active learning for single neuron models, also sometimes called ``ridge functions'', in the agnostic setting (under adversarial label noise). Such models have been shown to be broadly effective in modeling physical phenomena, and for constructing surrogate data-driven models for partial differential equations.  Surprisingly, we show that for a single neuron model with any Lipschitz non-linearity (such as the ReLU, sigmoid, absolute value, low-degree polynomial, among others), strong provable approximation guarantees can be obtained using a well-known active learning strategy for fitting \emph{linear functions} in the agnostic setting. % -- i.e. for the case when there is no non-linearity. Namely, we can collect samples via statistical \emph{leverage score sampling}, which has been shown to be near-optimal in other active learning scenarios. We support our theoretical results with empirical simulations showing that our proposed active learning strategy based o
    
[^143]: SurCo：学习用于组合非线性优化问题的线性代理

    SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems. (arXiv:2210.12547v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12547](http://arxiv.org/abs/2210.12547)

    该论文介绍了一种名为SurCo的方法，通过学习线性代理成本，将组合非线性优化问题转化为线性问题，并通过结合梯度方法和线性组合优化的结构提供了高效解决方案。

    

    在许多实际应用中，具有非线性代价函数和组合约束的优化问题与其线性对应物相比，仍然具有挑战性，难以高效求解。为了弥合这一差距，我们提出了SurCo，它学习线性代理成本，可用于现有的组合求解器，以输出原始非线性组合优化问题的良好解决方案。通过对线性代理求解器进行端到端的学习，通过非线性损失函数微分，将基于梯度的方法的灵活性与线性组合优化的结构相结合。我们提出了三个SurCo变体：SurCo-zero用于单个非线性问题，SurCo-prior用于问题分布，SurCo-hybrid用于结合分布和问题特定信息。我们给出了理论上的直觉动机

    Optimization problems with nonlinear cost functions and combinatorial constraints appear in many real-world applications but remain challenging to solve efficiently compared to their linear counterparts. To bridge this gap, we propose $\textbf{SurCo}$ that learns linear $\underline{\text{Sur}}$rogate costs which can be used in existing $\underline{\text{Co}}$mbinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem. The surrogate costs are learned end-to-end with nonlinear loss by differentiating through the linear surrogate solver, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We propose three $\texttt{SurCo}$ variants: $\texttt{SurCo}-\texttt{zero}$ for individual nonlinear problems, $\texttt{SurCo}-\texttt{prior}$ for problem distributions, and $\texttt{SurCo}-\texttt{hybrid}$ to combine both distribution and problem-specific information. We give theoretical intuition motiv
    
[^144]: Alpha-divergence变分推断与重要性加权自编码器的结合：方法和渐近性

    Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics. (arXiv:2210.06226v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.06226](http://arxiv.org/abs/2210.06226)

    本文提出了VR-IWAE下界，该下界是IWAE下界的推广，采用无偏梯度估计器能够实现与VR下界相同的随机梯度下降过程，对该下界进行了理论分析，揭示了其优势和不足，并通过示例验证了理论观点。

    

    针对目标后验分布和变分分布之间的alpha散度，已经提出了几个涉及变分Rényi (VR)下界的算法。尽管有令人满意的实证结果，但这些算法都采用了有偏的随机梯度下降过程，因此缺乏理论保证。本文对VR-IWAE下界进行了正式化和研究，该下界是重要性加权自编码器(IWAE)下界的推广。我们证明了VR-IWAE下界具有几个可取的特性，特别是在重新参数化的情况下与VR下界导致相同的随机梯度下降过程，但这次是依靠无偏梯度估计器。然后，我们提供了对VR-IWAE下界以及标准IWAE下界的两种互补的理论分析。这些分析揭示了这些下界的好处和缺点。最后，我们通过玩具和真实数据示例来说明我们的理论观点。

    Several algorithms involving the Variational R\'enyi (VR) bound have been proposed to minimize an alpha-divergence between a target posterior distribution and a variational distribution. Despite promising empirical results, those algorithms resort to biased stochastic gradient descent procedures and thus lack theoretical guarantees. In this paper, we formalize and study the VR-IWAE bound, a generalization of the Importance Weighted Auto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several desirable properties and notably leads to the same stochastic gradient descent procedure as the VR bound in the reparameterized case, but this time by relying on unbiased gradient estimators. We then provide two complementary theoretical analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those analyses shed light on the benefits or lack thereof of these bounds. Lastly, we illustrate our theoretical claims over toy and real-data examples.
    
[^145]: 使用加权不对称损失函数的神经网络模型预测区间

    Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.04318](http://arxiv.org/abs/2210.04318)

    本论文提出了一种使用加权不对称损失函数的方法，生成可靠的预测区间，适用于复杂的机器学习情境，可扩展为参数化函数的PI预测。

    

    我们提出了一种简单而有效的方法来生成近似和预测趋势的预测区间（PIs）。我们利用加权不对称损失函数来估计PI的下限和上限，权重由区间宽度确定。我们提供了该方法的简洁数学证明，展示了如何将其扩展到为参数化函数推导PI，并论证了该方法为预测相关变量的PI而有效的原因。我们在基于神经网络的模型的真实世界预测任务上对该方法进行了测试，结果表明它在复杂的机器学习情境下可以产生可靠的PI。

    We propose a simple and efficient approach to generate prediction intervals (PIs) for approximated and forecasted trends. Our method leverages a weighted asymmetric loss function to estimate the lower and upper bounds of the PIs, with the weights determined by the interval width. We provide a concise mathematical proof of the method, show how it can be extended to derive PIs for parametrised functions and argue why the method works for predicting PIs of dependent variables. The presented tests of the method on a real-world forecasting task using a neural network-based model show that it can produce reliable PIs in complex machine learning scenarios.
    
[^146]: 在潜在空间中的合作：在变分自编码器中添加混合成分的好处

    Cooperation in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders. (arXiv:2209.15514v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15514](http://arxiv.org/abs/2209.15514)

    本研究展示了在变分自编码器中添加混合成分的好处，并证明了混合成分的增加能够提高其在图像和单细胞数据集上的潜在表示能力。这表明使用混合VAE是获取更灵活变分逼近的标准方法。

    

    本文展示了混合成分在共同适应最大化ELBO时的合作方式。我们借鉴了最近在多个和自适应重要性采样文献中的进展。我们使用单独的编码器网络对混合成分进行建模，并在实证上证明ELBO随混合成分数量的增加是单调非减的。这些结果适用于MNIST、FashionMNIST和CIFAR-10数据集上的不同VAE架构。本工作还表明增加混合成分的数量能够改善VAE在图像和单细胞数据集上的潜在表示能力。这种合作行为表明，使用混合VAE应被视为获取更灵活的变分近似的标准方法。最后，我们首次在大范围的消融实验中将混合VAE与归一化流、层次模型和/或VampPrior进行了比较和结合。

    In this paper, we show how the mixture components cooperate when they jointly adapt to maximize the ELBO. We build upon recent advances in the multiple and adaptive importance sampling literature. We then model the mixture components using separate encoder networks and show empirically that the ELBO is monotonically non-decreasing as a function of the number of mixture components. These results hold for a range of different VAE architectures on the MNIST, FashionMNIST, and CIFAR-10 datasets. In this work, we also demonstrate that increasing the number of mixture components improves the latent-representation capabilities of the VAE on both image and single-cell datasets. This cooperative behavior motivates that using Mixture VAEs should be considered a standard approach for obtaining more flexible variational approximations. Finally, Mixture VAEs are here, for the first time, compared and combined with normalizing flows, hierarchical models and/or the VampPrior in an extensive ablation 
    
[^147]: 使用自监督方法预训练视觉转换器用于基于视觉的深度强化学习

    Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning. (arXiv:2209.10901v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10901](http://arxiv.org/abs/2209.10901)

    本文研究了使用自监督方法预训练视觉转换器，并通过添加时间顺序验证任务来捕捉观测之间的时间关系。实验结果表明，这些方法在学习有用的表示和提高强化学习数据效率方面都很有效。

    

    视觉转换器架构在计算机视觉领域表现出竞争力，在几个基准测试中取代了基于卷积的网络。然而，在强化学习中，卷积神经网络仍然是表示模块的首选架构。本文研究了使用几种最先进的自监督方法来预训练视觉转换器，并评估所学表示的质量。为了展示在这个上下文中时间维度的重要性，我们提出了VICReg的扩展，通过添加一个时间顺序验证任务来更好地捕捉观测之间的时间关系。我们的结果表明，所有方法在学习有用的表示和避免在Atari Learning Environment (ALE)中观测数据上出现重复表示方面都是有效的，这导致了在强化学习中的数据效率提高。此外，使用预训练的编码器要比从头开始训练的模型在强化学习任务中表现更好。

    The Vision Transformer architecture has shown to be competitive in the computer vision (CV) space where it has dethroned convolution-based networks in several benchmarks. Nevertheless, convolutional neural networks (CNN) remain the preferential architecture for the representation module in reinforcement learning. In this work, we study pretraining a Vision Transformer using several state-of-the-art self-supervised methods and assess the quality of the learned representations. To show the importance of the temporal dimension in this context we propose an extension of VICReg to better capture temporal relations between observations by adding a temporal order verification task. Our results show that all methods are effective in learning useful representations and avoiding representational collapse for observations from Atari Learning Environment (ALE) which leads to improvements in data efficiency when we evaluated in reinforcement learning (RL). Moreover, the encoder pretrained with the 
    
[^148]: 基于MPC的模型导向强化学习的新型评分函数：价值求和

    Value Summation: A Novel Scoring Function for MPC-based Model-based Reinforcement Learning. (arXiv:2209.08169v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.08169](http://arxiv.org/abs/2209.08169)

    本文提出了一种新的评分函数，用于解决使用奖励函数评分轨迹时的偏差问题，该方法通过利用价值的折扣和求和来提高MPC-based强化学习的学习效率，并在实验中表现出优于当前最先进算法的结果。

    

    本文提出了一种用于MPC-based强化学习方法规划模块的新型评分函数，以解决使用奖励函数来评分轨迹时的固有偏差。该方法利用价值的折扣和求和来提高现有MPC-based MBRL方法的学习效率。该方法利用最优轨迹来指导策略学习，并根据实际世界和增强型板载数据更新其状态-动作值函数。通过在选定的MuJoCo Gym环境中评估提出的方法的学习效率，并在学习Cassie机器人模型的运动技能中进行评估，结果表明该方法在学习效率和平均奖励回报方面优于当前最先进的算法。

    This paper proposes a novel scoring function for the planning module of MPC-based reinforcement learning methods to address the inherent bias of using the reward function to score trajectories. The proposed method enhances the learning efficiency of existing MPC-based MBRL methods using the discounted sum of values. The method utilizes optimal trajectories to guide policy learning and updates its state-action value function based on real-world and augmented onboard data. The learning efficiency of the proposed method is evaluated in selected MuJoCo Gym environments as well as in learning locomotion skills for a simulated model of the Cassie robot. The results demonstrate that the proposed method outperforms the current state-of-the-art algorithms in terms of learning efficiency and average reward return.
    
[^149]: 不同分布的数据价值

    The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10967](http://arxiv.org/abs/2208.10967)

    不同分布的数据可以对任务的泛化误差产生非单调的影响，使用少量不同分布的数据进行训练是有价值的。

    

    我们期望随着类似任务样本的增加，泛化误差会减小；而随着来自不同分布（OOD）任务样本的增加，泛化误差会增大。在这项工作中，我们展示了一个反直觉的现象：任务的泛化误差可以是样本从OOD任务中的数量的非单调函数。随着OOD样本数量的增加，目标任务的泛化误差在超过一个阈值之前会先减小后增大。换句话说，使用少量OOD数据进行训练是有价值的。我们在合成数据集上使用Fisher线性判别和计算机视觉基准数据集（如MNIST、CIFAR-10、CINIC-10、PACS和DomainNet）上的深度网络来展示和分析这一现象。在我们知道哪些样本属于OOD的理想情况下，我们展示了可以利用目标和OOD经验风险的适当加权目标来利用这些非单调趋势。尽管实际应用有限，但这表明如果我们能够检测到OOD样本，这种方法可能是有价值的。

    We expect the generalization error to improve with more samples from a similar task, and to deteriorate with more samples from an out-of-distribution (OOD) task. In this work, we show a counter-intuitive phenomenon: the generalization error of a task can be a non-monotonic function of the number of OOD samples. As the number of OOD samples increases, the generalization error on the target task improves before deteriorating beyond a threshold. In other words, there is value in training on small amounts of OOD data. We use Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate and analyze this phenomenon. In the idealistic setting where we know which samples are OOD, we show that these non-monotonic trends can be exploited using an appropriately weighted objective of the target and OOD empirical risk. While its practical utility is limited, this does suggest that if we can det
    
[^150]: 数据增强是一个超参数：精心筛选的自监督对于无监督异常检测的成功产生了幻象。

    Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07734](http://arxiv.org/abs/2208.07734)

    这项研究通过广泛的实验，证明数据增强与异常生成机制之间的对齐是自监督学习在无监督异常检测中取得成功的关键，并且在缺乏对齐时，自监督学习甚至可能降低准确性。

    

    自监督学习（SSL）已经成为一种有希望的替代方法，用于为现实世界的问题创建监督信号，避免了手动标注的巨大成本。对于标记异常稀缺或几乎不存在的无监督任务（如异常检测），SSL特别有吸引力。过去已经使用了大量的数据增强函数来进行基于SSL的异常检测（SSAD）的图像数据，并且最近的研究表明数据增强的类型对准确性有着重要影响。受此启发，本研究通过对三种不同检测模型和420个异常检测任务的广泛实验，提供了全面的数字和可视证据，证明数据增强与异常生成机制之间的对齐是SSAD成功的关键，而在缺乏对齐的情况下，SSL甚至可能降低准确性。据我们所知，这是关于图像型SSAD的首次深入研究。

    Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
    
[^151]: 可信推荐系统

    Trustworthy Recommender Systems. (arXiv:2208.06265v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2208.06265](http://arxiv.org/abs/2208.06265)

    可信度推荐系统研究已经从以准确性为导向转变为以透明、公正、稳健性为特点的可信度推荐系统。本文提供了可信度推荐系统领域的文献综述和讨论。

    

    推荐系统旨在帮助用户从庞大的目录中有效地检索感兴趣的物品。长期以来，研究人员一直致力于开发准确的推荐系统。然而，近年来，推荐系统面临越来越多的威胁，包括来自攻击、系统和用户产生的干扰以及系统的偏见。因此，仅仅关注准确性已经不够，研究必须考虑其他重要因素，如可信度。对于终端用户来说，一个值得信赖的推荐系统不仅要准确，而且还要透明、无偏见、公正，并且对干扰或攻击具有稳健性。这些观察实际上导致了推荐系统研究的范式转变: 从以准确性为导向的推荐系统转向了以可信度为导向的推荐系统。然而，研究人员缺乏对可信度推荐系统领域的文献的系统概述和讨论。因此，本文提供了可信度推荐系统的概述，包括对该新兴且快速发展领域的文献的讨论。

    Recommender systems (RSs) aim to help users to effectively retrieve items of their interests from a large catalogue. For a quite long period of time, researchers and practitioners have been focusing on developing accurate RSs. Recent years have witnessed an increasing number of threats to RSs, coming from attacks, system and user generated noise, system bias. As a result, it has become clear that a strict focus on RS accuracy is limited and the research must consider other important factors, e.g., trustworthiness. For end users, a trustworthy RS (TRS) should not only be accurate, but also transparent, unbiased and fair as well as robust to noise or attacks. These observations actually led to a paradigm shift of the research on RSs: from accuracy-oriented RSs to TRSs. However, researchers lack a systematic overview and discussion of the literature in this novel and fast developing field of TRSs. To this end, in this paper, we provide an overview of TRSs, including a discussion of the mo
    
[^152]: 基于原始估计亚梯度求解器的不平衡分类SVM

    Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09311](http://arxiv.org/abs/2206.09311)

    本研究旨在实现对不平衡数据集的分类，并评估成本敏感的PEGASOS SVM的性能，同时将核函数纳入SVM中扩展Ding的工作。

    

    本研究旨在通过实验证明我们的成本敏感PEGASOS SVM在主多次要比从8.6：1到130：1的不平衡数据集上具有良好的性能，并确定包括截距（偏见）、正则化和参数是否会影响我们选择的数据集上的性能。虽然许多人采用SMOTE方法，但我们旨在采用一种计算量较小的方法。通过检查学习曲线来评估性能，这些曲线可以诊断我们是过度拟合还是欠拟合，或者我们选择了过度代表性或欠代表性的训练/测试数据。我们还将在验证曲线中查看超参数的背景与测试和训练误差之间的关系。我们将基准化我们的PEGASOS成本敏感SVM与Ding的LINEAR SVM DECIDL方法的结果。他在一个数据集中获得了0.5的ROC-AUC。我们的工作将通过将核函数纳入SVM来扩展Ding的工作。我们将使用Python而不是MATLAB，因为Python具有更有效地存储我的数据集的字典。

    We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM achieves good performance on imbalanced data sets with a Majority to Minority Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including intercept (bias), regularization and parameters affects performance on our selection of datasets. Although many resort to SMOTE methods, we aim for a less computationally intensive method. We evaluate the performance by examining the learning curves. These curves diagnose whether we overfit or underfit or we choose over representative or under representative training/test data. We will also see the background of the hyperparameters versus the test and train error in validation curves. We benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the work of Ding by incorporating kernels into SVM. We will use Python rather than MATLAB as python has dictionaries for storing m
    
[^153]: 交替优化的图神经网络

    Alternately Optimized Graph Neural Networks. (arXiv:2206.03638v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03638](http://arxiv.org/abs/2206.03638)

    本文提出了一种新的优化框架用于解决图上的半监督学习，通过交替优化算法显著提高了计算和内存效率，并在实验证明相比现有技术可以达到相当或更好的性能。

    

    图神经网络（GNN）在图上的半监督节点分类任务方面取得了重大进展。现有的大部分GNN都是以端到端的方式进行训练，可以看作是解决双层优化问题。这个过程在计算和内存使用上通常效率低下。在这项工作中，我们提出了一种新的优化框架用于图上的半监督学习。这个框架可以通过交替优化算法方便地解决，从而显著提高效率。大量实验证明，所提出的方法在计算和内存效率上显著优于现有技术的基线，并且可以达到相当或更好的性能。

    Graph Neural Networks (GNNs) have greatly advanced the semi-supervised node classification task on graphs. The majority of existing GNNs are trained in an end-to-end manner that can be viewed as tackling a bi-level optimization problem. This process is often inefficient in computation and memory usage. In this work, we propose a new optimization framework for semi-supervised learning on graphs. The proposed framework can be conveniently solved by the alternating optimization algorithms, resulting in significantly improved efficiency. Extensive experiments demonstrate that the proposed method can achieve comparable or better performance with state-of-the-art baselines while it has significantly better computation and memory efficiency.
    
[^154]: 元学习参数化技能

    Meta-Learning Parameterized Skills. (arXiv:2206.03597v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03597](http://arxiv.org/abs/2206.03597)

    提出了一种元学习参数化技能的算法，通过学习可转移的参数化技能并将其综合到新的动作空间中，实现了长期任务的高效学习。实证研究表明，该算法能够使智能体在难以解决的长期任务中取得成功。

    

    我们提出了一种新颖的参数化技能学习算法，旨在学习可转移的参数化技能并将它们综合到支持长期任务高效学习的新动作空间中。我们提出利用离策略元强化学习与以轨迹为中心的平滑项相结合，学习一组参数化技能。我们的智能体可以使用这些学习到的技能构建一个三级层次结构框架，模拟时间扩展参数化动作马尔可夫决策过程。我们进行实证研究表明，所提出的算法使智能体能够解决一组困难的长期任务（障碍课程和机器人操纵）。

    We propose a novel parameterized skill-learning algorithm that aims to learn transferable parameterized skills and synthesize them into a new action space that supports efficient learning in long-horizon tasks. We propose to leverage off-policy Meta-RL combined with a trajectory-centric smoothness term to learn a set of parameterized skills. Our agent can use these learned skills to construct a three-level hierarchical framework that models a Temporally-extended Parameterized Action Markov Decision Process. We empirically demonstrate that the proposed algorithms enable an agent to solve a set of difficult long-horizon (obstacle-course and robot manipulation) tasks.
    
[^155]: 学习处理时间相关的流式数据的在线随机算法

    Learning from time-dependent streaming data with online stochastic algorithms. (arXiv:2205.12549v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12549](http://arxiv.org/abs/2205.12549)

    本文研究了处理时间相关的流式数据的在线随机算法，并通过非渐进分析建立了新颖的启发式算法，加速收敛。实验证明时间变化的小批量SGD方法可以打破依赖结构，有偏倚的SGD方法具有与无偏倚方法相当的性能，并且使用Polyak-Ruppert平均化方法能够加快随机优化算法的收敛。

    

    本文探讨了在时间相关且有偏倚梯度估计下的流式优化问题。我们分析了一些一阶方法，包括随机梯度下降（SGD）、小批量SGD和时间变化的小批量SGD，以及它们的Polyak-Ruppert平均值。我们的非渐进分析建立了新颖的启发式算法，将依赖性、偏倚和凸性水平联系起来，实现了加速收敛。具体来说，我们的研究结果表明：（i）时间变化的小批量SGD方法能够打破长期和短期的依赖结构；（ii）有偏倚的SGD方法可以达到与无偏倚方法相当的性能；（iii）使用Polyak-Ruppert平均化方法可以加速随机优化算法的收敛。为了验证我们的理论发现，我们在模拟和现实的时间相关数据上进行了一系列实验。

    This paper addresses stochastic optimization in a streaming setting with time-dependent and biased gradient estimates. We analyze several first-order methods, including Stochastic Gradient Descent (SGD), mini-batch SGD, and time-varying mini-batch SGD, along with their Polyak-Ruppert averages. Our non-asymptotic analysis establishes novel heuristics that link dependence, biases, and convexity levels, enabling accelerated convergence. Specifically, our findings demonstrate that (i) time-varying mini-batch SGD methods have the capability to break long- and short-range dependence structures, (ii) biased SGD methods can achieve comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence of the stochastic optimization algorithms. To validate our theoretical findings, we conduct a series of experiments using both simulated and real-life time-dependent data.
    
[^156]: 通过图神经网络解决可证明困难的代表性选择问题

    Tackling Provably Hard Representative Selection via Graph Neural Networks. (arXiv:2205.10403v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10403](http://arxiv.org/abs/2205.10403)

    本文研究了属性图的代表性选择问题，并证明了在缺乏图结构的情况下，RS的学习困难性。同时，发现当存在或构建了同质图结构时，可以通过适当的建模方法有效地解决这一困难问题。

    

    代表性选择（RS）是从数据集中找出代表数据子集的问题。本文研究了属性图的RS，并专注于找到能够优化模型准确性的代表性节点。从理论上讲，我们证明了在没有图结构的情况下RS的困难性，通过证明一个具有高度实际意义的变体（用于学习的RS）在任何合理因子的多项式时间内无法近似，这意味着广泛使用的替代函数的最优解与模型的实际准确性之间存在显著的差距。然后，我们研究了具有（同质）图结构的情况，或者可以在数据点之间构建图结构。我们展示了通过合适的建模方法，这种结构的存在可以将一个困难的RS（用于学习）问题有效地解决。

    Representative Selection (RS) is the problem of finding a small subset of exemplars from a dataset that is representative of the dataset. In this paper, we study RS for attributed graphs, and focus on finding representative nodes that optimize the accuracy of a model trained on the selected representatives. Theoretically, we establish a new hardness result forRS (in the absence of a graph structure) by proving that a particular, highly practical variant of it (RS for Learning) is hard to approximate in polynomial time within any reasonable factor, which implies a significant potential gap between the optimum solution of widely-used surrogate functions and the actual accuracy of the model. We then study the setting where a (homophilous) graph structure is available, or can be constructed, between the data points.We show that with an appropriate modeling approach, the presence of such a structure can turn a hard RS (for learning) problem into one that can be effectively solved. To this e
    
[^157]: ConceptEvo：解读深度学习训练中的概念演变

    ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16475](http://arxiv.org/abs/2203.16475)

    ConceptEvo是一个统一的深度神经网络解释框架，可以在训练过程中揭示概念的产生和演变，并通过人机评估和实验证明其发现对模型和预测具有重要意义。

    

    本文提出了ConceptEvo，一个用于深度神经网络（DNNs）的统一解释框架，可以揭示训练过程中学习概念的产生和演变。我们的工作填补了DNN解释研究中的一个关键空白，因为现有方法仅关注训练后的解释。ConceptEvo提出了两个新颖的技术贡献：（1）一种生成统一语义空间的算法，可以在训练过程中进行不同模型的并行比较；（2）一种发现和量化类别预测中重要概念演变的算法。通过与260名参与者进行大规模人机评估和定量实验，我们展示了ConceptEvo可以发现不同模型之间有意义且对预测重要的演变。ConceptEvo适用于现代（ConvNeXt）和经典的DNNs（例如VGGs，InceptionV3）。

    We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work fills a critical gap in DNN interpretation research, as existing methods focus on post-hoc interpretation after training. ConceptEvo presents two novel technical contributions: (1) an algorithm that generates a unified semantic space that enables side-by-side comparison of different models during training; and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation with 260 participants and quantitative experiments, we show that ConceptEvo discovers evolutions across different models that are meaningful to humans and important for predictions. ConceptEvo works for both modern (ConvNeXt) and classic DNNs (e.g., VGGs, InceptionV3).
    
[^158]: Hilbert Simplex几何中的非线性嵌入

    Non-linear Embeddings in Hilbert Simplex Geometry. (arXiv:2203.11434v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.11434](http://arxiv.org/abs/2203.11434)

    本文研究了将图的距离矩阵嵌入到Hilbert Simplex几何中的表示能力，发现该几何结构在嵌入任务中与其他几何结构相媲美，同时具有快速和数值稳健的特点。

    

    机器学习和计算机视觉的关键技术之一是将离散加权图嵌入到连续空间中进行后续处理。在分层结构嵌入到双曲几何中方面取得了很大的成功，因为已经证明任何加权树都可以以任意低的扭曲程度嵌入到该几何中。我们研究了将图的距离矩阵嵌入到Hilbert Simplex几何中的表示能力。我们的发现表明，Hilbert Simplex几何在嵌入任务中与其他几何结构（如Poincaré双曲球或欧几里德几何）相媲美，同时具有快速和数值稳健的特点。

    A key technique of machine learning and computer vision is to embed discrete weighted graphs into continuous spaces for further downstream processing. Embedding discrete hierarchical structures in hyperbolic geometry has proven very successful since it was shown that any weighted tree can be embedded in that geometry with arbitrary low distortion. Various optimization methods for hyperbolic embeddings based on common models of hyperbolic geometry have been studied. In this paper, we consider Hilbert geometry for the standard simplex which is isometric to a vector space equipped with the variation polytope norm. We study the representation power of this Hilbert simplex geometry by embedding distance matrices of graphs. Our findings demonstrate that Hilbert simplex geometry is competitive to alternative geometries such as the Poincar\'e hyperbolic ball or the Euclidean geometry for embedding tasks while being fast and numerically robust.
    
[^159]: 部分观察的马尔科夫决策过程（POMDPs）的自然演员-评论家方法的有限时间分析

    Finite-Time Analysis of Natural Actor-Critic for POMDPs. (arXiv:2202.09753v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.09753](http://arxiv.org/abs/2202.09753)

    本文分析了部分观察的马尔科夫决策过程（POMDPs）下自然演员-评论家方法的有限时间特性，并对使用有限状态控制器产生的错误进行了明确的表征。

    

    我们考虑了有限或可数无限状态空间的部分观察的马尔科夫决策过程（POMDPs）的强化学习问题，其中控制器只能访问基础控制马尔科夫链的噪声观测。我们考虑了一种自然的演员-评论家方法，该方法采用有限的内部存储器进行策略参数化，并使用多步时序差异学习算法进行策略评估。凭借我们的知识，我们首次确立了部分观察系统下基于函数逼近的演员-评论家方法的非渐近全局收敛性。特别地，除了在MDPs中出现的函数逼近和统计误差之外，我们还明确地表征了由于使用有限状态控制器而产生的错误。这种额外的错误是以传统的POMDPs中的信心状态和使用有限状态时的隐藏状态的后验分布之间的总变差距离来表示的。

    We consider the reinforcement learning problem for partially observed Markov decision processes (POMDPs) with large or even countably infinite state spaces, where the controller has access to only noisy observations of the underlying controlled Markov chain. We consider a natural actor-critic method that employs a finite internal memory for policy parameterization, and a multi-step temporal difference learning algorithm for policy evaluation. We establish, to the best of our knowledge, the first non-asymptotic global convergence of actor-critic methods for partially observed systems under function approximation. In particular, in addition to the function approximation and statistical errors that also arise in MDPs, we explicitly characterize the error due to the use of finite-state controllers. This additional error is stated in terms of the total variation distance between the traditional belief state in POMDPs and the posterior distribution of the hidden state when using a finite-sta
    
[^160]: Weisfeiler和Leman来做机器学习了：目前的研究进展。

    Weisfeiler and Leman go Machine Learning: The Story so far. (arXiv:2112.09992v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09992](http://arxiv.org/abs/2112.09992)

    Weisfeiler-Leman算法被广泛应用于处理图和关系数据。本文全面介绍了该算法在监督学习中的应用，包括理论背景、扩展、与等变神经网格的联系、并列出了当前应用和未来研究方向。

    

    近年来，基于Weisfeiler-Leman算法的算法和神经架构已成为处理图和关系数据的机器学习的强大工具。本文全面介绍算法在机器学习环境中的使用情况，重点关注监督学习。我们讨论了理论背景，展示了如何将其用于监督图形和节点表示学习，讨论了最近的扩展，并概述了算法与（置换）等变神经网格的联系。此外，我们还概述了当前的应用和未来的研究方向以刺激进一步的研究。

    In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.
    
[^161]: MAFAT: 内存感知的神经网络融合和切片加速边缘推断

    MAFAT: Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.06960](http://arxiv.org/abs/2107.06960)

    本文提出了一种内存感知的神经网络融合和切片方法，用于在边缘设备上加速推断。通过将网络细分为多个独立的卷积层组合，并进行融合和切片操作，实现了在资源受限的设备上降低内存占用的目标。

    

    一个不断增长的研究挑战是在资源受限的边缘设备上本地运行昂贵的机器学习（ML）网络。具有大型卷积层的ML网络很容易超出可用内存，导致由于过多的操作系统交换而增加延迟。先前的内存减少技术，如修剪和量化，会降低模型的准确性，并且通常需要重新训练。另外，分布式方法将卷积层划分为等效的较小子计算，但实施引入了通信成本，并且需要一个设备网络。然而，分布式划分方法也可以用于在单个设备上以减少的内存占用运行，通过将网络细分为更小的操作。在本文中，我们将先前的分布式划分工作扩展为在单个设备上内存感知的执行。我们的方法扩展了先前的融合策略，以允许多个卷积层的组成部分独立融合和切片。

    A rising research challenge is running costly machine learning (ML) networks locally on resource-constrained edge devices. ML networks with large convolutional layers can easily exceed available memory, increasing latency due to excessive OS swapping. Previous memory reduction techniques such as pruning and quantization reduce model accuracy and often require retraining. Alternatively, distributed methods partition the convolutions into equivalent smaller sub-computations, but the implementations introduce communication costs and require a network of devices. Distributed partitioning approaches can, however, also be used to run in a reduced memory footprint on a single device by subdividing the network into smaller operations. In this paper, we extend prior work on distributed partitioning into a memory-aware execution on a single device. Our approach extends prior fusing strategies to allow for multiple groups of convolutional layers that are fused and tiled independently. This enable
    
[^162]: 在不确定性贪婪赌博问题中的公平分配规划：概率公平性

    Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting. (arXiv:2106.07677v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07677](http://arxiv.org/abs/2106.07677)

    本研究引入了一种概率公平策略ProbFair，如何在不确定性贪婪赌博问题中进行资源分配，并在满足预算约束的同时最大化总期望奖励。实验证明ProbFair在保持效用的同时提供公平性保证。

    

    在预算受限的资源分配中，常常使用不确定性和崩溃赌博模型来描述具有动作相关转移概率的情境，例如在患者之间分配健康干预措施。然而，针对这个规划问题的最先进的Whittle指数方法要么不考虑赌博之间的公平性，要么在保证公平性时激励公平性。因此，我们引入ProbFair，一种概率公平策略，该策略在满足预算约束的同时，最大化总期望奖励，并确保在每个时间步上被选择的概率具有严格的正下界。我们在实际应用中评估了我们的算法，在这个应用中，介入措施支持患者间持续的正压通气（CPAP）疗法的依从性，以及在更广泛的合成转移矩阵类上进行了评估。我们发现ProbFair在提供公平性保证的同时保持了效用。

    Restless and collapsing bandits are often used to model budget-constrained resource allocation in settings where arms have action-dependent transition probabilities, such as the allocation of health interventions among patients. However, state-of-the-art Whittle-index-based approaches to this planning problem either do not consider fairness among arms, or incentivize fairness without guaranteeing it. We thus introduce ProbFair, a probabilistically fair policy that maximizes total expected reward and satisfies the budget constraint while ensuring a strictly positive lower bound on the probability of being pulled at each timestep. We evaluate our algorithm on a real-world application, where interventions support continuous positive airway pressure (CPAP) therapy adherence among patients, as well as on a broader class of synthetic transition matrices. We find that ProbFair preserves utility while providing fairness guarantees.
    
[^163]: 强优化分类树

    Strong Optimal Classification Trees. (arXiv:2103.15965v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.15965](http://arxiv.org/abs/2103.15965)

    本文提出了一种基于流的混合整数优化形式来学习最优二进制分类树，该形式具有更强的线性优化能力，并能处理侧约束，实现可解释和公平的决策树设计。

    

    决策树是最流行的机器学习模型之一，广泛应用于从收益管理和医学到生物信息学的各种应用中。本文考虑了学习具有单变量分割的最优二进制分类树的问题。近年来，该领域的文献大量涌现，既受到启发式方法经验性次优性的推动，也受到混合整数优化（MIO）技术的巨大改进的推动。然而，现有的基于MIO的方法没有充分发挥MIO的威力：它们依赖于弱的形式，导致收敛速度慢且存在较大优化间隙。为了填补文献中的这一空白，我们提出了一种直观的基于流的MIO形式来学习最优二进制分类树。我们的形式可以适应侧约束，以实现可解释和公平的决策树设计。此外，我们还展示了我们的形式具有更强的线性优化能力。

    Decision trees are among the most popular machine learning models and are used routinely in applications ranging from revenue management and medicine to bioinformatics. In this paper, we consider the problem of learning optimal binary classification trees with univariate splits. Literature on the topic has burgeoned in recent years, motivated both by the empirical suboptimality of heuristic approaches and the tremendous improvements in mixed-integer optimization (MIO) technology. Yet, existing MIO-based approaches from the literature do not leverage the power of MIO to its full extent: they rely on weak formulations, resulting in slow convergence and large optimality gaps. To fill this gap in the literature, we propose an intuitive flow-based MIO formulation for learning optimal binary classification trees. Our formulation can accommodate side constraints to enable the design of interpretable and fair decision trees. Moreover, we show that our formulation has a stronger linear optimiza
    
[^164]: 深度学习在医学图像分析中的复杂度评估

    Evaluation of Complexity Measures for Deep Learning Generalization in Medical Image Analysis. (arXiv:2103.03328v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2103.03328](http://arxiv.org/abs/2103.03328)

    本文通过对乳腺超声图像进行实证研究，调查了25个复杂度度量与监督深度学习分类器的普适性能力之间的相关性，发现基于PAC-Bayes平坦性和路径范数的度量方法产生了最一致的结果。

    

    在医学图像分析中，深度学习模型的普适性通常在使用不同设备进行数据采集、设备设置或患者群体时降低。对新图像的普适性能力的更好理解对临床医生对深度学习的可信度至关重要。尽管近年来已经进行了大量研究来建立普适性界限和复杂度度量，但是预测和实际的普适性表现之间仍然存在显著差异。此外，相关的大型经验研究主要基于通用图像数据集的验证。本文介绍了一项经验研究，探讨了25个复杂度度量与乳腺超声图像监督深度学习分类器的普适性能力之间的相关性。结果表明，基于PAC-Bayes平坦性和路径范数的度量方法产生了最一致的结果。

    The generalization performance of deep learning models for medical image analysis often decreases on images collected with different devices for data acquisition, device settings, or patient population. A better understanding of the generalization capacity on new images is crucial for clinicians' trustworthiness in deep learning. Although significant research efforts have been recently directed toward establishing generalization bounds and complexity measures, still, there is often a significant discrepancy between the predicted and actual generalization performance. As well, related large empirical studies have been primarily based on validation with general-purpose image datasets. This paper presents an empirical study that investigates the correlation between 25 complexity measures and the generalization abilities of supervised deep learning classifiers for breast ultrasound images. The results indicate that PAC-Bayes flatness-based and path norm-based measures produce the most cons
    
[^165]: MixPath: 一种统一的一次性神经架构搜索方法

    MixPath: A Unified Approach for One-shot Neural Architecture Search. (arXiv:2001.05887v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2001.05887](http://arxiv.org/abs/2001.05887)

    本论文提出了一种名为MixPath的统一的一次性神经架构搜索方法，通过训练一次性的多路径超网络来准确评估候选架构。采用一种新颖的机制称为Shadow Batch Normalization（SBN）来解决多路径结构的特征差异问题，稳定优化并提高排名性能。

    

    在神经架构设计中，混合多个卷积核被证明是有优势的。然而，当前的两阶段神经架构搜索方法主要局限于单路径搜索空间。如何高效地搜索多路径结构的模型仍然是一个难题。在本文中，我们的动机是训练一个一次性的多路径超网络来准确评估候选架构。具体来说，我们发现在所研究的搜索空间中，从多个路径中求和的特征向量几乎是单个路径的倍数。这种差异扰乱了超网络的训练和排名能力。因此，我们提出了一种新颖的机制，称为Shadow Batch Normalization（SBN），来规范差异的特征统计。大量实验证明，SBN能够稳定优化和提高排名性能。我们将我们的统一多路径一次性方法称为MixPath，可以生成一系列能达到最新技术水平的模型。

    Blending multiple convolutional kernels is proved advantageous in neural architecture design. However, current two-stage neural architecture search methods are mainly limited to single-path search spaces. How to efficiently search models of multi-path structures remains a difficult problem. In this paper, we are motivated to train a one-shot multi-path supernet to accurately evaluate the candidate architectures. Specifically, we discover that in the studied search spaces, feature vectors summed from multiple paths are nearly multiples of those from a single path. Such disparity perturbs the supernet training and its ranking ability. Therefore, we propose a novel mechanism called Shadow Batch Normalization (SBN) to regularize the disparate feature statistics. Extensive experiments prove that SBNs are capable of stabilizing the optimization and improving ranking performance. We call our unified multi-path one-shot approach as MixPath, which generates a series of models that achieve state
    
[^166]: 声明性机制设计

    Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1912.13122](http://arxiv.org/abs/1912.13122)

    本文介绍了声明性机制设计的研究，提出了机构神经网络作为一种受管制的人工神经网络，引起人们对人工教学的关注，并提供了初步的答案。

    

    多智能体系统（MAS）和声明性电子机构（DEIs）的调控是过去十年涉及物理和软件智能体以及法律的多学科研究课题，但近年来逐渐演变为2016年起被称为新闻的机器律师。其中一种首次提出限制软件智能体行为的方案是电子机构。然而，随着人工神经网络（ANNs）被重新定义为深度学习（DL），有关DL使用的安全、隐私、伦理和法律问题引起了人工智能（AI）社区的关注。现在，MAS的规范几乎得到正确处理，我们提出将人工神经网络的规范作为一种特殊类型的受管制的人工神经网络，称之为机构神经网络（INN）。本文的主旨是引起人们对人工教学（AT）的关注，并给出一个初步的答案，展示了一种证明性的方法。

    Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
    

