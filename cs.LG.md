# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model.](http://arxiv.org/abs/2311.01729) | 本文提出了一种新的条件扩散模型CDGraph用于合成社交图，通过捕捉双条件之间的相互依赖关系和保持节点连通性来实现生成高质量的社交网络。 |
| [^2] | [Vision-Language Foundation Models as Effective Robot Imitators.](http://arxiv.org/abs/2311.01378) | 该论文介绍了一种利用视觉语言基础模型进行机器人操作的方法，通过在语言条件的操作数据集上进行微调，实现了在低性能平台上的有效模仿控制。 |
| [^3] | [TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models.](http://arxiv.org/abs/2311.01301) | TRIALSCOPE是一个统一的框架，利用生物医学语言模型将临床文本进行结构化，采用概率建模进行去噪和插补，并应用因果推断技术来应对混杂因素，以从实际世界数据中提取实证证据和推理临床假设。 |
| [^4] | [Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications.](http://arxiv.org/abs/2311.00429) | 本文提出了一种新的作物病害分类方法，利用基于注意力的特征提取和绿色色度坐标，可以轻松与移动设备和物联网设备交互，为农民提供最佳的作物种植保障。 |
| [^5] | [Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs.](http://arxiv.org/abs/2310.20145) | 本文介绍了一种新颖的robust Bayesian Optimization算法，AIRBO，它能够在任意输入不确定性下有效识别出表现一致良好的鲁棒最优解。 |
| [^6] | [Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records.](http://arxiv.org/abs/2310.19967) | 通过多模态机器学习从血液测试、半结构化和非结构化病历中学习，早期检测炎症性关节炎可以改善转诊，为及时治疗和预防病情恶化提供决策支持。 |
| [^7] | [Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation.](http://arxiv.org/abs/2310.18919) | 本研究解决了强化学习中延迟反馈对线性函数逼近的挑战，通过后验采样算法实现了在不同情况下的优越性能。 |
| [^8] | [ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models.](http://arxiv.org/abs/2310.18208) | ArcheType是一种使用大型语言模型进行开源列类型注释的新框架，通过改进上下文采样和标签重新映射，实现了全面的零样本解决方案。 |
| [^9] | [Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs.](http://arxiv.org/abs/2310.18152) | 本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。 |
| [^10] | [On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers.](http://arxiv.org/abs/2310.14421) | 本文研究了针对AI分类器的对抗鲁棒性度量的存在性、唯一性和可扩展性，提出了可以验证的数学条件，并在合成基准测试和生物医学应用中进行了实际计算和解释。 |
| [^11] | [Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series.](http://arxiv.org/abs/2310.14017) | 本论文提出了一个名为COMET的创新层次对比框架，用于医疗时间序列分析。该框架通过在多个层级上开发对比损失，可以充分利用医疗时间序列的复杂特性，并实现自监督学习。使用多个数据集进行实验验证了COMET的有效性。 |
| [^12] | [SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation.](http://arxiv.org/abs/2310.12508) | 这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。 |
| [^13] | [VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization.](http://arxiv.org/abs/2310.11864) | VQ-NeRF是一个基于向量量化的神经网络模型，用于分解和编辑3D场景中的反射场。通过将连续材料离散化，该模型可以减少噪声并生成离散材料的分割地图，从而实现简化的材料编辑。 |
| [^14] | [Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation.](http://arxiv.org/abs/2310.11132) | 这项工作研究了一种针对混合类型数据集的非参数条件独立性检验方法，该方法使用基于k最近邻的CMI估计器和本地置换方案，针对现实世界中包含数值和分类变量的情况。 |
| [^15] | [Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction.](http://arxiv.org/abs/2310.09903) | 本研究评估了特征选择方法在股市价格预测中的性能，通过选择最佳的技术指标组合来实现最少误差的预测。研究结果表明，不同的包装器特征选择方法在不同的机器学习方法中具有不同的表现。 |
| [^16] | [A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model.](http://arxiv.org/abs/2310.08812) | 该研究提出了一个新的时间序列预测方法，使用VMD-GARCH-LSTM模型来捕捉复杂时间序列中的局部特征和波动性信息，并通过集成各个子模态的预测结果来提高预测准确性。 |
| [^17] | [PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps.](http://arxiv.org/abs/2310.04017) | 本研究提出了一种使用蛋白质语言模型和接触图改进药物靶标相互作用预测的方法，通过在现有模型中引入联系图信息，可以改进药物靶标相互作用预测的性能。 |
| [^18] | [RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels.](http://arxiv.org/abs/2310.03912) | 本文提出了一种新的提高贝叶斯优化模型建模能力的方法，通过将注意机制融入深度核学习中，使得代理能够适应上下文信息，提高优化性能。 |
| [^19] | [PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability.](http://arxiv.org/abs/2310.03906) | PyDCM是一个用强化学习实现的可定制的数据中心模型，通过使用自定义配置和向量化的热计算，实现了对数据中心的优化，具有较高的效率。 |
| [^20] | [Sampling via Gradient Flows in the Space of Probability Measures.](http://arxiv.org/abs/2310.03597) | 通过梯度流抽样方法的研究方向在计算科学和工程中具有重要意义。本文通过研究概率测度空间中的梯度流的设计组成部分，提出了三个贡献：Kullback-Leibler散度作为能量泛函的独特属性、度量的选择与不变性的关系。 |
| [^21] | [Machine learning the interaction network in coupled dynamical systems.](http://arxiv.org/abs/2310.03378) | 本研究使用自监督神经网络模型，从观察到的轨迹数据中恢复耦合动力系统的相互作用网络并预测个体代理的动态。 |
| [^22] | [De Novo Drug Design with Joint Transformers.](http://arxiv.org/abs/2310.02066) | 提出了联合Transformer来解决全新药物设计的困难，同时生成新颖分子并预测目标属性。使用惩罚对数似然目标训练模型，在分子生成和预测精度方面取得了最先进的性能，相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。通过联合Transformer生成的新颖分子在全新药物设计中优于其他基于SMILES的优化方法。 |
| [^23] | [DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training.](http://arxiv.org/abs/2310.02025) | DeepZero是一个扩展零阶优化到深度神经网络训练的深度学习框架，通过创新的坐标梯度估计和稀疏诱导的零阶训练协议，实现了高准确性和计算效率的优化。 |
| [^24] | [Towards Robust Cardiac Segmentation using Graph Convolutional Networks.](http://arxiv.org/abs/2310.01210) | 这项研究使用了图卷积网络来实现心脏分割，通过预测轮廓点而不是标记每个像素，消除了心脏分割中的解剖学错误。同时还对图卷积网络进行了消融研究，并评估了临床测量指标的性能。 |
| [^25] | [Data Filtering Networks.](http://arxiv.org/abs/2309.17425) | 本文研究了学习数据过滤网络用于筛选大型未策划数据集的问题，并构建了新的数据过滤网络，从而产生最先进的图像-文本数据集。 |
| [^26] | [Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications.](http://arxiv.org/abs/2309.17076) | 本研究展示了在生物医学应用中，3D网格分割任务中，镜像重量对称性在神经网络中的影响。我们发现，重量对称性可以提高1至3％的额外准确度，并且可以减少参数数量高达8倍，而不影响性能。 |
| [^27] | [GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations.](http://arxiv.org/abs/2309.16223) | 本文针对图神经网络解释的内分布评估问题，提出了GInX-Eval方法，克服了传统评估指标的局限性，为解释方法提供了新的见解。 |
| [^28] | [On Computational Entanglement and Its Interpretation in Adversarial Machine Learning.](http://arxiv.org/abs/2309.15669) | 本研究探索了对抗机器学习模型的复杂性和可解释性，通过将其与爱因斯坦的特殊相对论中的纠缠概念联系起来，发现远程特征样本可以表现出纠缠现象，挑战了对抗可传递性现象的传统描述方法。 |
| [^29] | [Uncertainty Quantification via Neural Posterior Principal Components.](http://arxiv.org/abs/2309.15533) | 本论文提出了一种使用神经网络在单次前向传递中预测任意输入图像后验分布的主成分的方法，以实现不确定性量化。 |
| [^30] | [Spatial-frequency channels, shape bias, and adversarial robustness.](http://arxiv.org/abs/2309.13190) | 本研究通过临界带屏蔽实验揭示了人类物体识别和神经网络物体识别在空间频率通道上的差异，发现人类在自然图像中识别物体所使用的通道与字母和光栅识别相同，而神经网络具有不同的频率通道和鲁棒性。 |
| [^31] | [HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks.](http://arxiv.org/abs/2309.08549) | 本论文提出了一种名为健康影响力噪声训练的高效稳健训练方法，该方法使用影响函数制造了有助于加强分类模型对抗数据污染攻击的健康噪声，并且在仅修改训练数据的子集时也能有效运行。 |
| [^32] | [PRE: Vision-Language Prompt Learning with Reparameterization Encoder.](http://arxiv.org/abs/2309.07760) | 这项工作提出了一种名为PRE的方法，通过重新参数化编码器来增强可学习提示的泛化能力，从而解决了大型预训练视觉-语言模型中手动提示工程的挑战。 |
| [^33] | [PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference.](http://arxiv.org/abs/2309.02334) | 提出了一种名为PolyLUT的新方法，用于训练神经网络在FPGA上进行部署。该方法利用多变量多项式作为基本模块，并利用软逻辑将多项式评估隐藏在FPGA的查找表中，从而实现超低延迟推理，并减少了软件逻辑的层数。 |
| [^34] | [Linear Oscillation: The Aesthetics of Confusion for Vision Transformer.](http://arxiv.org/abs/2308.13670) | 研究提出了一种新的激活函数——线性振动（LoC）激活函数，它通过将线性轨迹和振荡偏差无缝融合，捕捉到了“困惑的重要性”的本质。实证研究表明，在需要区分微妙模式的情境中，使用LoC激活函数可以提高网络的鲁棒性。 |
| [^35] | [Optimal data pooling for shared learning in maintenance operations.](http://arxiv.org/abs/2308.12670) | 本文研究了在维护操作中，通过数据共享学习可以显著降低成本的最优数据汇聚方法。 |
| [^36] | [Low Tensor Rank Learning of Neural Dynamics.](http://arxiv.org/abs/2308.11567) | 研究发现通过学习过程中的张量秩演化来理解神经元连接在学习中的协调变化。研究表明训练过的递归神经网络的权重矩阵通常具有低秩结构，而这种结构在整个学习过程中保持在一个固定的低维子空间中。对真实权重进行低秩分解验证了这一观察结果。 |
| [^37] | [Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning.](http://arxiv.org/abs/2308.09883) | Flamingo是一个用于实现跨大量客户端安全聚合的系统，在私有联邦学习中有广泛的应用。通过消除每轮设置和引入轻量级的丢失容忍协议，Flamingo解决了以往协议在多轮设置下的问题，并引入了新的本地选择客户端邻域的方式。 |
| [^38] | [Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning.](http://arxiv.org/abs/2308.09544) | 本研究提出了一种适应教师的方法（TA）用于无样本连续学习，解决了知识蒸馏方法在这种情况下的性能下降问题，并在多个基准测试中持续提升模型性能。 |
| [^39] | [Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features.](http://arxiv.org/abs/2308.06197) | 这项研究提出了一种基于深度知识蒸馏的新颖持续学习方法，通过使用少量训练样本，能够准确识别新的复合表情类别。 |
| [^40] | [Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language.](http://arxiv.org/abs/2308.05061) | 本文提出了一种使用传感器数据、方程和自然语言提示上下文中运算符学习的方法。通过整合人类知识和语言描述，该方法不仅扩展了物理信息学习的灵活性和普适性，而且显著提高了学习性能和减少了数据需求。 |
| [^41] | [Online covariance estimation for stochastic gradient descent under Markovian sampling.](http://arxiv.org/abs/2308.01481) | 本文研究了在马尔可夫采样下的随机梯度下降中的在线重叠批次均值协方差估计器，并证明了其收敛速率为$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$和$O\big(\sqrt{d}\,n^{-1/8}\big)$，分别对应于状态相关和状态无关的马尔可夫采样。这些速率与独立同分布情况下的最佳收敛速率相匹配，并且克服了由于马尔可夫采样而引起的挑战。 |
| [^42] | [Discrete neural nets and polymorphic learning.](http://arxiv.org/abs/2308.00677) | 本文提出了一种基于离散模拟的神经网络，将Murski{i}的通用代数定理和Cybenko的通用逼近结果统一起来。同时，通过引入关系结构的多态性，我们提出了一种学习算法，用于经典学习任务。 |
| [^43] | [AI Increases Global Access to Reliable Flood Forecasts.](http://arxiv.org/abs/2307.16104) | 本研究开发了一个人工智能模型，可以准确预测未经测量流域的极端水文事件，从而提高了全球洪水预警的覆盖范围。 |
| [^44] | [A Theory for Emergence of Complex Skills in Language Models.](http://arxiv.org/abs/2307.15936) | 本文提出了一个统计框架，通过分析语言模型的交叉熵损失与基本语言任务的能力之间的关系，揭示了语言模型中复杂技能产生的机制。研究结果表明，通过扩展定律，预训练模型能够高效学习，并表现出违反通常泛化理论的能力。 |
| [^45] | [Adaptive Linear Estimating Equations.](http://arxiv.org/abs/2307.07320) | 本文提出了一种解决自适应线性回归模型中非正态渐近行为的方法，使用自适应线性估计方程构建去偏估计量，并在多臂老虎机的背景下保持了最小二乘估计量的非渐近性能。 |
| [^46] | [Embodied Lifelong Learning for Task and Motion Planning.](http://arxiv.org/abs/2307.06870) | 这篇论文研究了在家中长期部署的机器人所面临的具身化终身学习问题，在任务和运动规划的背景下采用了新颖的问题建模方法。利用TAMP系统的模块化特性，提出了一个生成混合模型来产生规划器的候选参数。通过学习共享和非共享的模型，并根据代理任务来在线选择使用的模型，该方法在模拟的2D领域和BEHAVIOR基准测试中取得了显著的规划成功改进。 |
| [^47] | [A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media.](http://arxiv.org/abs/2307.06775) | 本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。 |
| [^48] | [Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data.](http://arxiv.org/abs/2307.04075) | 本研究提出了一种基于注意力机制的无监督对比学习框架（AMUCL），用于分析癌症多组学数据并识别癌症新亚型。通过多头注意力机制和解耦对比学习模型（DMACL），该方法能够深度提取多组学数据特征并进行亚型聚类。 |
| [^49] | [Towards Symmetry-Aware Generation of Periodic Materials.](http://arxiv.org/abs/2307.02707) | 提出了一种名为SyMat的新的材料生成方法，能够对周期性材料的物理对称性进行感知，并能在生成和优化任务中取得有希望的性能。 |
| [^50] | [Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges.](http://arxiv.org/abs/2307.01050) | 本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。 |
| [^51] | [Reduce Computational Complexity for Convolutional Layers by Skipping Zeros.](http://arxiv.org/abs/2306.15951) | 本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。 |
| [^52] | [Is RLHF More Difficult than Standard RL?.](http://arxiv.org/abs/2306.14111) | 本文证明了对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。 |
| [^53] | [One-shot Imitation Learning via Interaction Warping.](http://arxiv.org/abs/2306.12392) | 本研究提出了一种交互变形的方法实现了从单个演示中学习SE（3）机器人操纵策略，并在实验中取得了成功效果。 |
| [^54] | [Sparse Modular Activation for Efficient Sequence Modeling.](http://arxiv.org/abs/2306.11197) | 本论文引入了稀疏模块激活 (SMA) 机制，用于高效的序列建模。这种机制可以动态地稀疏激活序列元素的子模块，减少计算和内存消耗。 |
| [^55] | [Practical Equivariances via Relational Conditional Neural Processes.](http://arxiv.org/abs/2306.10915) | 本文提出的关系条件神经过程（RCNPs）是一种有效将等变性纳入任何神经过程模型的方法，并扩展了等变神经过程的适用性和影响力到更高的维度。 |
| [^56] | [Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.09869) | 本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。 |
| [^57] | [VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models.](http://arxiv.org/abs/2306.06874) | 本文提出VillanDiffusion，一个针对扩散模型的统一后门攻击框架，涵盖主流的无条件和有条件DM，便于对不同DM配置进行后门分析，并为基于字幕的DM后门攻击提供了新的见解。 |
| [^58] | [FLSL: Feature-level Self-supervised Learning.](http://arxiv.org/abs/2306.06203) | 本文提出FLSL方法，采用Transformer进行联合嵌入和聚类，适合于内视图和外视图特征聚类。实验证明该方法在语义类簇表达方面取得显著改进。 |
| [^59] | [Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model.](http://arxiv.org/abs/2306.05720) | 本文探究了潜在扩散模型内部是否创建和使用简单场景几何内部表示，使用线性探针发现LDM内部激活提供了关于3D深度数据和突出对象/背景分离的线性表示，在图像合成中具有因果作用, 可用于LDM输出的简单高级编辑。 |
| [^60] | [Explainable Representation Learning of Small Quantum States.](http://arxiv.org/abs/2306.05694) | 该论文探讨了一种无监督机器学习模型在小量子态上的可解释表示学习方法，得到了一个与潜在结构相联系的可理解表示。 |
| [^61] | [Benchmarking Foundation Models with Language-Model-as-an-Examiner.](http://arxiv.org/abs/2306.04181) | 本文提出了一种新的基准测试框架，使用语言模型作为考官，可以无参考方式评估答案。这个框架解决了过去基准测试流程中的测试泄漏和评估自动化问题，并允许易于扩展，可以采用不同的语言模型作为考官。 |
| [^62] | [Guiding The Last Layer in Federated Learning with Pre-Trained Models.](http://arxiv.org/abs/2306.03937) | 本文研究了在联邦学习中使用预训练模型引导最后一层的问题，提出了使用最近类均值(NCM)精确且高效地拟合分类器的方法，并取得了很好的效果。 |
| [^63] | [For SALE: State-Action Representation Learning for Deep Reinforcement Learning.](http://arxiv.org/abs/2306.02451) | SALE是一种基于状态-动作表示学习的新方法，可以有效地从低级状态中实现表示学习，TD7算法引入了该方法并在连续控制任务中表现优异。 |
| [^64] | [GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction.](http://arxiv.org/abs/2306.01951) | 本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。 |
| [^65] | [Evaluating Language Models for Mathematics through Interactions.](http://arxiv.org/abs/2306.01694) | 本文介绍了一个可进行人机交互评估LLMs的原型平台CheckMate，并利用该平台评估了三个语言模型在大学级数学证明助手方面的能力，发布了结果数据集MathConverse，并得出了人类行为的初步分类和LMM生成正确性与感知帮助性分歧的发现。 |
| [^66] | [Transfer learning for atomistic simulations using GNNs and kernel mean embeddings.](http://arxiv.org/abs/2306.01589) | 本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。 |
| [^67] | [DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model.](http://arxiv.org/abs/2306.01001) | 本文提出了一种扩散模型中的负荷预测不确定性量化方法，采用Seq2Seq网络结构来分离两种类型的不确定性并处理异常情况，不仅着眼于预测条件期望值。 |
| [^68] | [Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models.](http://arxiv.org/abs/2305.17076) | 本文表明Wasserstein分布式强鲁棒估计器的泛化保证适用于一般模型类别，不受维数灾难所困扰，甚至可以涵盖测试时的分布变化。 |
| [^69] | [PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction.](http://arxiv.org/abs/2305.16914) | 本文提出了一种利用SVD无监督三维平面正则化的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构，有效解决了训练视图的过拟合导致低纹理区域的几何重建不佳的问题。 |
| [^70] | [Differentiable Clustering with Perturbed Spanning Forests.](http://arxiv.org/abs/2305.16358) | 介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。 |
| [^71] | [An $\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond.](http://arxiv.org/abs/2305.16041) | 提出一种新颖的采样规则EB-TC $\varepsilon$，用于随机赌博机中的$\varepsilon$-最佳臂的辨识。该规则可用于确定固定置信度或固定预算标识且具备自适应调整勘探参数的渐近最优性。在仿真实验中表现良好，适用于不同问题领域。 |
| [^72] | [Adaptive Data Analysis in a Balanced Adversarial Model.](http://arxiv.org/abs/2305.15452) | 本研究探究在平衡对抗模型下的自适应数据分析，在该模型下，研究者通过提供一种新的对抗模型，使得在平衡状态下，回答那些以前被认为计算上困难的自适应查询变得可行。 |
| [^73] | [Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective.](http://arxiv.org/abs/2305.15408) | 本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。 |
| [^74] | [Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness.](http://arxiv.org/abs/2305.13946) | 本文提出了在线投资组合选择的第一个数据相关上界，算法显示亚线性遗憾率，并在数据“容易”时实现对数遗憾。 |
| [^75] | [Deep Learning with Kernels through RKHM and the Perron-Frobenius Operator.](http://arxiv.org/abs/2305.13588) | 该论文提出了一种基于核方法的深度学习框架：深度RKHM，通过使用$C^*$代数获得更温和的界限，并提供了良性过拟合的理论解释。 |
| [^76] | [Tight conditions for when the NTK approximation is valid.](http://arxiv.org/abs/2305.13141) | 我们研究了NTK逼近在平方损失下训练模型的有效性，发现在模型缩放因子$\alpha=O(T)$时在训练时间$T$之前可以使得NTK逼近有效。 |
| [^77] | [DreamWaltz: Make a Scene with Complex 3D Animatable Avatars.](http://arxiv.org/abs/2305.12529) | DreamWaltz是一个新颖的框架，可以根据文本指导和参数化的人体先验生成和动画化复杂的3D角色。它提出了三维一致的遮挡感知得分蒸馏采样（SDS）来优化隐式神经表示和规范姿势，并学习了一种可动画且具有泛化能力的角色表示。这种方法在创建3D角色方面表现出了高效性和稳健性。 |
| [^78] | [Learning Large Graph Property Prediction via Graph Segment Training.](http://arxiv.org/abs/2305.12322) | 本文提出了一种名为Graph Segment Training的新方法，通过分治法允许使用恒定的内存消耗来学习大型图形属性预测。该方法被评估在几项大型图形属性预测任务上，表现出优于几个最先进基准的出色性能。 |
| [^79] | [Moment Matching Denoising Gibbs Sampling.](http://arxiv.org/abs/2305.11650) | 本文提出了动量匹配去噪Gibbs采样方法，可以在给定‘嘈杂’的模型的情况下，从干净的模型中有效地进行采样。 |
| [^80] | [NODE-ImgNet: a PDE-informed effective and robust model for image denoising.](http://arxiv.org/abs/2305.11049) | 本文提出了一种新型的神经网络架构 NODE-ImgNet，该模型结合了神经常微分方程和卷积神经网络块，它是一种使用 PDE 的有效和稳健的图像去噪模型。该模型能够自然地避免学习过程中引入伪像，实现了更高准确性和参数效率，并在不同场景下展示了一致的有效性和优势。 |
| [^81] | [Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation.](http://arxiv.org/abs/2305.05803) | 本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。 |
| [^82] | [Recommender Systems with Generative Retrieval.](http://arxiv.org/abs/2305.05065) | 本文提出了一种新型的生成式检索模型，将检索和生成组合在一起以产生推荐。 |
| [^83] | [Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation.](http://arxiv.org/abs/2305.03942) | 论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。 |
| [^84] | [Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study.](http://arxiv.org/abs/2305.03017) | 本研究使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐的质量，重点关注于Stack Overflow上的Java编程语言。研究使用BERT将代码示例转换为数值向量。 |
| [^85] | [Approximating CKY with Transformers.](http://arxiv.org/abs/2305.02386) | 本文研究了Transformer模型逼近CKY算法的能力，提出了一种用梯度预测解析的方法，在标准基准测试中表现竞争力更好，同时速度更快。在随机PCFG下解析时，性能下降，但加入额外的归纳偏差是有帮助的。 |
| [^86] | [Calibrated Explanations: with Uncertainty Information and Counterfactuals.](http://arxiv.org/abs/2305.02305) | 该论文提出了一种新的特征重要性解释方法，Calibrated Explanations (CE)，它可以提供准确、稳定的解释，并且可以为概率估计和特征重要性权重提供不确定性量化信息，是一种快速、可靠且强健的解释方法。 |
| [^87] | [ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning.](http://arxiv.org/abs/2304.13219) | 该论文介绍了 ZRG 数据集，其中包含成千上万个样本的高分辨率住宅屋顶正交图像拼接、对应 DSM、3D 屋顶框架和多视图图像生成的点云。该数据集可以用于住宅屋顶结构的几何建模和场景理解，如屋顶轮廓提取、单目高度估计和平面屋顶结构提取等任务。 |
| [^88] | [A Closer Look at Reward Decomposition for High-Level Robotic Explanations.](http://arxiv.org/abs/2304.12958) | 本论文提出了一种将奖励分解与抽象动作空间相结合的学习框架，可以提供基于对象属性的明确高层次解释，避免了解释机器人行为的复杂性。 |
| [^89] | [Bridging RL Theory and Practice with the Effective Horizon.](http://arxiv.org/abs/2304.09853) | 本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。 |
| [^90] | [Safe reinforcement learning with self-improving hard constraints for multi-energy management systems.](http://arxiv.org/abs/2304.08897) | 本论文提出了一种安全强化学习方法，能够实现多能源管理系统中的最优控制，在保证硬约束的前提下减少工程工作，降低建模偏差，并避免潜在的不安全行为。 |
| [^91] | [Unified Out-Of-Distribution Detection: A Model-Specific Perspective.](http://arxiv.org/abs/2304.06813) | 本文提出一种新颖的统一框架，用于将机器学习模型中的外部分布检测扩展到更广泛的范围，该框架旨在检测模型无法正确预测的测试示例，而不是特定的外部分布原因。 |
| [^92] | [Bi-directional Training for Composed Image Retrieval via Text Prompt Learning.](http://arxiv.org/abs/2303.16604) | 本文提出了一种基于文本提示学习和双向训练的组成图像检索方法，可以应用于现有的体系结构，并且在修改文本存在噪声或歧义的情况下特别有效。 |
| [^93] | [The Exact Sample Complexity Gain from Invariances for Kernel Regression on Manifolds.](http://arxiv.org/abs/2303.14269) | 本文提供了在任何流形上，对于一个在流形上任意群作用下不变的目标函数，核岭回归的最小化最优率，从而增加了有效样本数量或降低了流形的维数。 |
| [^94] | [Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding.](http://arxiv.org/abs/2303.12513) | 本文调查了视觉语言预训练对仅文本任务的表现是否有提高。作者提出了一套视觉语言理解任务，证明了多模态训练的文本编码器在视觉推理方面的优越性。 |
| [^95] | [Stochastic Interpolants: A Unifying Framework for Flows and Diffusions.](http://arxiv.org/abs/2303.08797) | 本文提出了一种统一的生成模型，该模型基于随机插值框架，可以实现流和扩散方法的统一。作者构建了一类广泛的连续时间随机过程，用于将两个任意的密度在有限时间内精确地连接。这种方法可以用于基于概率微分方程的确定性和随机生成模型的构建。 |
| [^96] | [Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks.](http://arxiv.org/abs/2303.05445) | 该论文提出了一种名为带吸收的泛洪（FwA）的新协议，用于解决复杂网络上的异构赌博机问题。通过严格的遗憾分析，证明了该协议的有效性。 |
| [^97] | [Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation.](http://arxiv.org/abs/2303.04772) | 本文介绍了无限维度得分扩散模型在多个分辨率水平上的离散化方法，并使用多级扩散算法在多个分辨率上高效地学习。实证表明，该模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本，并可以生成不同分辨率的图像并处理矩形域。 |
| [^98] | [Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach.](http://arxiv.org/abs/2302.10798) | 本论文提出了一种轻量化参数裁剪策略来实现节能深度学习，可以发现最佳的静态子网络，包含二值门控模块和新颖的损失函数，适用于各种神经网络，同时在已有的基准测试中取得了竞争性结果。 |
| [^99] | [Bandit Social Learning: Exploration under Myopic Behavior.](http://arxiv.org/abs/2302.07425) | 该论文研究了自私行为下的劫匪社交学习问题，发现存在一种探索激励权衡，即武器探索和社交探索之间的权衡，受到代理的短视行为的限制会加剧这种权衡，并导致遗憾率与代理数量成线性关系。 |
| [^100] | [Data pruning and neural scaling laws: fundamental limitations of score-based algorithms.](http://arxiv.org/abs/2302.06960) | 评分数据修剪算法在高压缩区域失败，通过随机化的校准协议可以提高现有修剪算法在该区域的性能。 |
| [^101] | [A unified recipe for deriving (time-uniform) PAC-Bayes bounds.](http://arxiv.org/abs/2302.03421) | 该论文提出了一种用于推导PAC-Bayesian泛化界限的统一框架，不同于传统的固定样本大小方式，该框架适用于所有停止时间。同时，该论文还提出了新的边界方法，也可以应用于非平稳损失函数和非独立同分布的数据。 |
| [^102] | [Flat Seeking Bayesian Neural Networks.](http://arxiv.org/abs/2302.02713) | 本文提出了一种扁平化贝叶斯神经网络的方法，该方法在后验推论中考虑了模型的扁平化性质，从而提升了模型的泛化能力和不确定性估计能力。 |
| [^103] | [The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing.](http://arxiv.org/abs/2302.01186) | 该研究提出了ScaledGD(𝜆)方法，相较于传统梯度下降法更加鲁棒，并且在处理低秩矩阵感知问题时具有很好的表现。 |
| [^104] | [AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation.](http://arxiv.org/abs/2301.08110) | AtMan是一种通过在生成式Transformer模型中操纵注意力机制来解释预测的方法，相较于传统方法几乎不占用额外内存，可在生产环境中使用。 |
| [^105] | [Are you using test log-likelihood correctly?.](http://arxiv.org/abs/2212.00219) | 使用测试对数似然进行比较可能与其他指标相矛盾，并且高测试对数似然不意味着更准确的后验近似。 |
| [^106] | [Statistical inference for transfer learning with high-dimensional quantile regression.](http://arxiv.org/abs/2211.14578) | 本研究提出了一种高维分位数回归模型中的转移学习方法，以适应源域和目标域中的异质性和重尾分布。根据精心选择的可转移源域建立了转移学习估计量的误差界限，并提出了有效的置信区间和假设检验程序，以实现一步完成。 |
| [^107] | [MARLlib: A Scalable Multi-agent Reinforcement Learning Library.](http://arxiv.org/abs/2210.13708) | 本文提出了MARLlib，这是一个全面的MARL算法库，可统一数十种算法。它还超越了当前工作，集成了各种环境接口和提供灵活的参数共享策略。 |
| [^108] | [Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees.](http://arxiv.org/abs/2210.07893) | 本文针对高斯过程模型的数值稳定性进行了研究，通过感兴趣点的选择和计算，提供了稳定可靠的稀疏逼近方法。 |
| [^109] | [Detecting hidden confounding in observational data using multiple environments.](http://arxiv.org/abs/2205.13935) | 使用独立数据生成过程下的多环境方法，可以检测观测数据中的未观察到的混淆因素，并提出了测试独立性的程序。 |
| [^110] | [Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case.](http://arxiv.org/abs/2202.05069) | 本文提出了一种适用于线性回归情况的迁移学习算法，该算法能够将新数据与历史数据相结合，特别在新数据稀缺的情况下具有益处，并且在实验验证中表现出对负迁移学习的鲁棒性。 |
| [^111] | [Interpretable Sequence Classification Via Prototype Trajectory.](http://arxiv.org/abs/2007.01777) | ProtoryNet是一种基于原型轨迹的可解释深度神经网络，它通过捕捉时间模式和原型的近似程度来进行文本分类，并实现了直观和细致的推理过程解释。 |

# 详细

[^1]: CDGraph：基于扩散模型的双条件社交图合成

    CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model. (arXiv:2311.01729v1 [cs.SI])

    [http://arxiv.org/abs/2311.01729](http://arxiv.org/abs/2311.01729)

    本文提出了一种新的条件扩散模型CDGraph用于合成社交图，通过捕捉双条件之间的相互依赖关系和保持节点连通性来实现生成高质量的社交网络。

    

    由生成模型合成的社交图因数据稀缺和用户隐私的担忧而越来越受到需求。生成社交网络的关键性能标准之一是对特定条件的忠实度，例如具有特定会员资格和财务状况的用户。虽然最近的扩散模型在生成图像方面表现出了显着的性能，但它们在条件社交图的合成方面的效果尚未得到研究。在本文中，我们提出了第一种用于社交网络的条件扩散模型CDGraph，它基于两个指定条件来训练和合成图。我们提出了CDGraph的去噪过程中的共同演化依赖性，以捕捉双条件之间的相互依赖关系，并进一步结合社会同质性和社会传染力以保持节点之间的连接性，同时满足指定条件。此外，我们引入了一种新颖的分类

    The social graphs synthesized by the generative models are increasingly in demand due to data scarcity and concerns over user privacy. One of the key performance criteria for generating social networks is the fidelity to specified conditionals, such as users with certain membership and financial status. While recent diffusion models have shown remarkable performance in generating images, their effectiveness in synthesizing graphs has not yet been explored in the context of conditional social graphs. In this paper, we propose the first kind of conditional diffusion model for social networks, CDGraph, which trains and synthesizes graphs based on two specified conditions. We propose the co-evolution dependency in the denoising process of CDGraph to capture the mutual dependencies between the dual conditions and further incorporate social homophily and social contagion to preserve the connectivity between nodes while satisfying the specified conditions. Moreover, we introduce a novel class
    
[^2]: Vision-Language Foundation Models作为有效的机器人模仿者

    Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])

    [http://arxiv.org/abs/2311.01378](http://arxiv.org/abs/2311.01378)

    该论文介绍了一种利用视觉语言基础模型进行机器人操作的方法，通过在语言条件的操作数据集上进行微调，实现了在低性能平台上的有效模仿控制。

    

    最近在视觉语言基础模型方面的进展显示出它们理解多模态数据和解决复杂的视觉语言任务（包括机器人操作）的能力。我们寻求一种简单的方式来利用现有的视觉语言模型（VLMs）在机器人数据上进行简单微调。为此，我们提出了一个简单而新颖的视觉语言操作框架，名为RoboFlamingo，它建立在开源的VLMs，OpenFlamingo之上。与以前的工作不同，RoboFlamingo利用预训练的VLMs进行单步视觉语言理解，使用显式策略头模拟顺序历史信息，并只在语言条件的操作数据集上进行微调。这种分解为RoboFlamingo提供了在低性能平台上进行开环控制和部署的灵活性。通过在测试基准上大幅超过现有技术水平，我们展示了RoboFlamingo可以成为一种有效的机器人模仿者。

    Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
    
[^3]: TRIALSCOPE：一个统一的因果框架，用于利用生物医学语言模型扩展实际世界证据生成

    TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v1 [cs.LG])

    [http://arxiv.org/abs/2311.01301](http://arxiv.org/abs/2311.01301)

    TRIALSCOPE是一个统一的框架，利用生物医学语言模型将临床文本进行结构化，采用概率建模进行去噪和插补，并应用因果推断技术来应对混杂因素，以从实际世界数据中提取实证证据和推理临床假设。

    

    实际世界数据的快速数字化为优化医疗服务和加速生物医学发现提供了前所未有的机会。然而，在实践中，这些数据往往以非结构化形式存在，如电子医疗记录中的临床笔记，并且通常受到混杂因素的困扰。本文介绍了TRIALSCOPE，一个用于从人群级观察数据中提取实际世界证据的统一框架。TRIALSCOPE利用生物医学语言模型来扩展规模化的临床文本，采用先进的概率建模进行去噪和插补，并结合最先进的因果推断技术来应对常见的混杂因素。利用临床试验规范作为通用表示形式，TRIALSCOPE提供了一个一键式解决方案，可使用观察数据生成和推理临床假设。在一个包含超过一百万个癌症患者的大规模实际世界数据集上进行了广泛的实验和分析。

    The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million canc
    
[^4]: 使用绿色色度坐标（GCC）和基于注意力的特征提取的支持向量机（SVM）进行作物病害分类，用于基于物联网的智能农业应用

    Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications. (arXiv:2311.00429v1 [eess.IV])

    [http://arxiv.org/abs/2311.00429](http://arxiv.org/abs/2311.00429)

    本文提出了一种新的作物病害分类方法，利用基于注意力的特征提取和绿色色度坐标，可以轻松与移动设备和物联网设备交互，为农民提供最佳的作物种植保障。

    

    作物具有重要意义，因为它们是人类获取能量、营养和药用价值的主要来源。然而，植物病害可以在农业栽培过程中对叶片产生负面影响，导致作物产量和经济价值的显著损失。因此，农民能够识别作物病害非常重要。然而，这种方法通常需要辛勤工作、大量计划和对植物病原体的深入了解。鉴于这些障碍，提供能够轻松与移动设备和物联网设备交互的解决方案对于农民来说至关重要。已经开发和研究了各种机器学习（ML）和深度学习（DL）算法用于植物病害检测的识别，取得了重要且有希望的结果。本文提出了一种新的分类方法，利用了基于注意力的特征提取、RGB和GCC。

    Crops hold paramount significance as they serve as the primary provider of energy, nutrition, and medicinal benefits for the human population. Plant diseases, however, can negatively affect leaves during agricultural cultivation, resulting in significant losses in crop output and economic value. Therefore, it is crucial for farmers to identify crop diseases. However, this method frequently necessitates hard work, a lot of planning, and in-depth familiarity with plant pathogens. Given these numerous obstacles, it is essential to provide solutions that can easily interface with mobile and IoT devices so that our farmers can guarantee the best possible crop development. Various machine learning (ML) as well as deep learning (DL) algorithms have been created & studied for the identification of plant disease detection, yielding substantial and promising results. This article presents a novel classification method that builds on prior work by utilising attention-based feature extraction, RGB
    
[^5]: 高效robust Bayesian Optimization对于任意不确定输入的应用

    Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs. (arXiv:2310.20145v1 [cs.LG])

    [http://arxiv.org/abs/2310.20145](http://arxiv.org/abs/2310.20145)

    本文介绍了一种新颖的robust Bayesian Optimization算法，AIRBO，它能够在任意输入不确定性下有效识别出表现一致良好的鲁棒最优解。

    

    Bayesian Optimization (BO) 是一种广泛应用于各种应用中的高效优化算法。在一些具有挑战性的BO任务中，由于优化过程中的不可避免的随机性，如加工误差、执行噪声或上下文变异，输入不确定性会出现。这种不确定性会使输入在评估之前偏离预期值，导致最终结果的性能波动较大。在本文中，我们引入了一种新颖的robust Bayesian Optimization算法，AIRBO，它能有效地识别在任意输入不确定性下表现一致良好的鲁棒最优解。我们的方法通过使用最大均值差(MMD)赋能高斯过程，直接建模任意分布的不确定输入，并通过Nystrom逼近加速后验推断。我们在MMD估计误差下建立了严格的理论遗憾界，并在合成函数上进行了广泛的实验。

    Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic function
    
[^6]: 早期检测炎症性关节炎以改善转诊的多模态机器学习从血液测试、半结构化和非结构化病历中学习

    Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records. (arXiv:2310.19967v1 [cs.LG])

    [http://arxiv.org/abs/2310.19967](http://arxiv.org/abs/2310.19967)

    通过多模态机器学习从血液测试、半结构化和非结构化病历中学习，早期检测炎症性关节炎可以改善转诊，为及时治疗和预防病情恶化提供决策支持。

    

    早期检测炎症性关节炎（IA）对于高效准确的医院转诊分流以及及时治疗和预防IA疾病进展至关重要，尤其是在有限的医疗资源下。手动评估流程是目前实践中最常见的早期检测IA的方法，但这种方法的劳动密集型和低效率。每个从一般实践（GP）到医院的转诊都需要评估大量的临床信息。机器学习在自动化重复评估任务和提供决策支持方面展示了巨大的潜力，用于早期检测IA。然而，大多数基于机器学习的IA检测方法依赖于血液测试结果。然而，在实践中，血液测试数据并不总是在转诊时可用，因此我们需要利用半结构化和非结构化数据等多模态数据进行早期检测IA的方法。在这项研究中，我们提出了融合和

    Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and en
    
[^7]: 延迟反馈的线性函数逼近强化学习中的后验采样

    Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation. (arXiv:2310.18919v1 [cs.LG])

    [http://arxiv.org/abs/2310.18919](http://arxiv.org/abs/2310.18919)

    本研究解决了强化学习中延迟反馈对线性函数逼近的挑战，通过后验采样算法实现了在不同情况下的优越性能。

    

    运用函数逼近在强化学习中取得了显著进展，但现有的高效算法通常依赖于即时反馈。本文通过采用后验采样来解决延迟反馈对强化学习中线性函数逼近的挑战，首先介绍了Delayed-PSVI算法，通过后验采样中的噪声扰动有效地探索价值函数空间。我们提供了延迟反馈强化学习中后验采样算法的首次分析，并展示了我们的算法在一系列情况下的优越性。

    Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $
    
[^8]: ArcheType：一种使用大型语言模型进行开源列类型注释的新框架

    ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])

    [http://arxiv.org/abs/2310.18208](http://arxiv.org/abs/2310.18208)

    ArcheType是一种使用大型语言模型进行开源列类型注释的新框架，通过改进上下文采样和标签重新映射，实现了全面的零样本解决方案。

    

    现有的深度学习方法在语义列类型注释（CTA）方面存在重要缺点：它们依赖于在训练时固定的语义类型；需要大量的每个类型的训练样本并产生大量运行时推断成本；即使类型保持不变，它们的性能也可能在评估新数据集时下降。大型语言模型在广泛的任务上展示了强大的零样本分类性能，本文探讨了它们在CTA中的应用。我们介绍了ArcheType，一种简单实用的方法，用于上下文采样、提示序列化、模型查询和标签重新映射，从而使大型语言模型能够完全以零样本方式解决列类型注释问题。我们分别对我们方法的每个组成部分进行了分析，并确定出改进上下文采样和标签重新映射提供了最一致的性能提升。

    Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
    
[^9]: 使用大型语言模型进行文本属性图的解缠表征学习

    Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])

    [http://arxiv.org/abs/2310.18152](http://arxiv.org/abs/2310.18152)

    本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。

    

    文本属性图（TAGs）在网络上非常常见，对于该类图，如引用网络、电子商务网络和社交网络的研究在网络社区中引起了相当大的关注。最近，大型语言模型（LLMs）在各种任务上展示了出色的能力。然而，现有的工作仅仅依靠提示信息来传达图结构信息给LLMs，因此对于TAGs中复杂的结构关系了解不足。为解决这个问题，本文提出了解缠图文学习器（DGTL）模型，能够增强LLMs对TAGs的推理和预测能力。我们的DGTL模型通过定制的解缠图神经网络（GNN）层将图结构信息纳入其中，使得LLMs能够捕捉多个结构因素中隐藏的复杂关系。

    Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
    
[^10]: 对AI分类器的对抗鲁棒性度量的存在性，唯一性和可扩展性研究

    On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])

    [http://arxiv.org/abs/2310.14421](http://arxiv.org/abs/2310.14421)

    本文研究了针对AI分类器的对抗鲁棒性度量的存在性、唯一性和可扩展性，提出了可以验证的数学条件，并在合成基准测试和生物医学应用中进行了实际计算和解释。

    

    本文提出并证明了针对（局部）唯一可逆分类器、广义线性模型（GLM）和熵AI（EAI）具有最小对抗路径（MAP）和最小对抗距离（MAD）的存在性、唯一性和明确的分析计算的简单可验证的数学条件。在常见的合成基准测试数据集上，针对神经网络、提升随机森林、GLM和EAI等各类AI工具进行MAP和MAD的实际计算、比较和解释，包括双卷状螺旋线及其扩展以及两个生物医学数据问题（用于健康保险理赔预测和心脏病发作致死率分类）。在生物医学应用中，展示了MAP如何在预定义的可访问控制变量子集中提供唯一的最小患者特定风险缓解干预措施。

    Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
    
[^11]: 全面对比：面向医疗时间序列的层次对比框架

    Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series. (arXiv:2310.14017v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14017](http://arxiv.org/abs/2310.14017)

    本论文提出了一个名为COMET的创新层次对比框架，用于医疗时间序列分析。该框架通过在多个层级上开发对比损失，可以充分利用医疗时间序列的复杂特性，并实现自监督学习。使用多个数据集进行实验验证了COMET的有效性。

    

    在医疗时间序列分析中，对比表示学习至关重要，因为它减少了对劳动密集、领域特定和稀缺的专家注释的依赖。然而，现有的对比学习方法主要关注单一数据层面，未能充分利用医疗时间序列的复杂特性。为了解决这个问题，我们提出了COMET，一个创新的层次框架，它利用医疗时间序列中所有内在层级的数据一致性。我们精心设计的模型系统地捕捉了来自四个潜在层级的数据一致性：观测、样本、试验和患者层级。通过在多个层级上开发对比损失，我们可以学习到保持全面数据一致性的有效表示，实现自监督方法中的信息最大利用。我们在具有挑战性的独立患者设置下进行实验。我们使用三个不同的数据集将COMET与六个基准方法进行比较。

    Contrastive representation learning is crucial in medical time series analysis as it alleviates dependency on labor-intensive, domain-specific, and scarce expert annotations. However, existing contrastive learning methods primarily focus on one single data level, which fails to fully exploit the intricate nature of medical time series. To address this issue, we present COMET, an innovative hierarchical framework that leverages data consistencies at all inherent levels in medical time series. Our meticulously designed model systematically captures data consistency from four potential levels: observation, sample, trial, and patient levels. By developing contrastive loss at multiple levels, we can learn effective representations that preserve comprehensive data consistency, maximizing information utilization in a self-supervised manner. We conduct experiments in the challenging patient-independent setting. We compare COMET against six baselines using three diverse datasets, which include 
    
[^12]: SalUn：通过基于梯度的权重显著性增强机器遗忘在图像分类和生成中的效果

    SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])

    [http://arxiv.org/abs/2310.12508](http://arxiv.org/abs/2310.12508)

    这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。

    

    随着数据法规的不断发展，机器遗忘（MU）已成为增强当前AI模型的信任和安全性的重要工具。然而，现有的MU方法通常在遗忘精度、稳定性和跨领域适用性方面存在局限。为了解决这些挑战，我们引入了MU中的“权重显著性”概念，借鉴了模型解释中的输入显著性。这一创新将MU的关注点从整个模型引导到了具体的模型权重上，提高了其效果和效率。我们称之为显著性遗忘（SalUn）的方法将其与“精确”遗忘（在删除遗忘数据集后从头开始重新训练模型）的性能差距缩小。据我们所知，SalUn是第一个能够在图像分类和生成中有效消除遗忘数据、类别或概念影响的有原则的MU方法。例如，SalUn可在图片分类和生成任务中擦除遗忘数据、类别或概念。

    With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
    
[^13]: VQ-NeRF: 基于向量量化的神经反射分解与编辑

    VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v1 [cs.CV])

    [http://arxiv.org/abs/2310.11864](http://arxiv.org/abs/2310.11864)

    VQ-NeRF是一个基于向量量化的神经网络模型，用于分解和编辑3D场景中的反射场。通过将连续材料离散化，该模型可以减少噪声并生成离散材料的分割地图，从而实现简化的材料编辑。

    

    我们提出了VQ-NeRF，这是一个具有向量量化（VQ）的双分支神经网络模型，用于对3D场景中的反射场进行分解和编辑。传统的神经反射场仅使用连续表示来建模3D场景，尽管现实中的物体通常由离散材料组成。这种缺乏离散化可能导致材料分解噪声和复杂的材料编辑。为了解决这些限制，我们的模型包括一个连续分支和一个离散分支。连续分支按照传统流程预测分解的材料，而离散分支使用VQ机制将连续材料量化为单独的材料。通过离散化材料，我们的模型可以减少分解过程中的噪声，并生成离散材料的分割地图。可以通过点击分割结果的相应区域来轻松选择特定材料进行进一步编辑。

    We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Ad
    
[^14]: 非参数的混合连续-分类变量条件独立性检验：一种新方法和数值评估

    Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation. (arXiv:2310.11132v1 [cs.LG])

    [http://arxiv.org/abs/2310.11132](http://arxiv.org/abs/2310.11132)

    这项工作研究了一种针对混合类型数据集的非参数条件独立性检验方法，该方法使用基于k最近邻的CMI估计器和本地置换方案，针对现实世界中包含数值和分类变量的情况。

    

    条件独立性检验（CIT）是机器学习中常见的任务，例如变量选择，也是基于约束的因果发现的主要组成部分。大多数当下的CIT方法假设所有变量都是数值或所有变量都是分类的，然而，许多现实世界的应用涉及包含数值和分类变量的混合类型数据集。非参数的CIT可以使用基于条件互信息（CMI）估计器结合本地置换方案进行。最近，已提出了两种基于k最近邻（k-NN）的混合类型数据集的新型CMI估计器。与任何k-NN方法一样，这些估计器依赖于距离度量的定义。一种方法通过对分类变量进行one-hot编码来计算距离，从而将分类变量视为离散数值变量，而另一种方法则通过使用分类变量作为条件来通过熵来表示CMI。在这项工作中，我们研究这些方法

    Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these 
    
[^15]: 评估特征选择在股市价格预测中的性能，以确定最有效的技术指标

    Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction. (arXiv:2310.09903v1 [q-fin.ST])

    [http://arxiv.org/abs/2310.09903](http://arxiv.org/abs/2310.09903)

    本研究评估了特征选择方法在股市价格预测中的性能，通过选择最佳的技术指标组合来实现最少误差的预测。研究结果表明，不同的包装器特征选择方法在不同的机器学习方法中具有不同的表现。

    

    鉴于技术指标对股市预测的影响，特征选择对选择最佳指标至关重要。一种考虑在特征选择过程中模型性能的特征选择方法是包装器特征选择方法。本研究旨在通过特征选择鉴定出最少误差的预测股市价格的最佳股市指标组合。为评估包装器特征选择技术对股市预测的影响，本文在过去10年苹果公司的数据上使用了10个评估器和123个技术指标进行了SFS和SBS的考察。此外，通过提出的方法，将由3天时间窗口创建的数据转化为适用于回归方法的输入。从观察结果可以得出：（1）每种包装器特征选择方法在不同的机器学习方法中具有不同的结果，每种方法在不同的预测准确性上也有所不同。

    Due to the influence of many factors, including technical indicators on stock market prediction, feature selection is important to choose the best indicators. One of the feature selection methods that consider the performance of models during feature selection is the wrapper feature selection method. The aim of this research is to identify a combination of the best stock market indicators through feature selection to predict the stock market price with the least error. In order to evaluate the impact of wrapper feature selection techniques on stock market prediction, in this paper SFS and SBS with 10 estimators and 123 technical indicators have been examined on the last 10 years of Apple Company. Also, by the proposed method, the data created by the 3-day time window were converted to the appropriate input for regression methods. Based on the results observed: (1) Each wrapper feature selection method has different results with different machine learning methods, and each method is mor
    
[^16]: 使用VMD-GARCH-LSTM模型的非线性时间序列预测方法

    A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model. (arXiv:2310.08812v1 [stat.ME])

    [http://arxiv.org/abs/2310.08812](http://arxiv.org/abs/2310.08812)

    该研究提出了一个新的时间序列预测方法，使用VMD-GARCH-LSTM模型来捕捉复杂时间序列中的局部特征和波动性信息，并通过集成各个子模态的预测结果来提高预测准确性。

    

    时间序列预测在各个领域都是一个重要而具有挑战性的任务。最近，基于模态分解的方法在复杂时间序列的预测中占据主导地位，因为它能够捕捉局部特征并从数据中提取内在模态的优势。然而，大多数模型无法捕捉包含重要信息的暗含波动性。为了提高当前、快速演变和波动性时间序列的预测能力，我们提出了一种新颖的分解-集成范式，即VMD-LSTM-GARCH模型。该模型使用变分模态分解算法将时间序列分解为K个子模态。随后，GARCH模型从这些子模态中提取波动性信息，这些信息作为LSTM的输入。每个子模态的数值和波动性信息被用来训练一个长短期记忆网络。该网络预测子模态，然后我们将所有子模态的预测结果进行聚合。

    Time series forecasting represents a significant and challenging task across various fields. Recently, methods based on mode decomposition have dominated the forecasting of complex time series because of the advantages of capturing local characteristics and extracting intrinsic modes from data. Unfortunately, most models fail to capture the implied volatilities that contain significant information. To enhance the forecasting of current, rapidly evolving, and volatile time series, we propose a novel decomposition-ensemble paradigm, the VMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed to decompose the time series into K sub-modes. Subsequently, the GARCH model extracts the volatility information from these sub-modes, which serve as the input for the LSTM. The numerical and volatility information of each sub-mode is utilized to train a Long Short-Term Memory network. This network predicts the sub-mode, and then we aggregate the predictions from all sub-modes 
    
[^17]: PGraphDTA: 使用蛋白质语言模型和接触图改进药物靶标相互作用预测

    PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps. (arXiv:2310.04017v1 [cs.LG])

    [http://arxiv.org/abs/2310.04017](http://arxiv.org/abs/2310.04017)

    本研究提出了一种使用蛋白质语言模型和接触图改进药物靶标相互作用预测的方法，通过在现有模型中引入联系图信息，可以改进药物靶标相互作用预测的性能。

    

    发现和开发新药是一项复杂且资源密集型的工作，通常涉及大量成本、时间投入和安全问题。药物发现的一个关键方面是确定新颖的药物-靶标（DT）相互作用。现有的预测DT相互作用的计算方法主要集中在二分类任务上，旨在确定DT对是否存在相互作用。然而，蛋白质-配体相互作用表现出一系列不同结合强度，即结合亲和力，这对准确预测造成了持续挑战。在本研究中，我们研究了在药物靶标相互作用（DTI）预测中使用的各种技术，并提出了改进性能的新方法。我们的方法包括整合蛋白质语言模型（PLMs）和将接触图信息作为现有模型的归纳偏差。通过广泛的实验，我们证明了我们提出的方法的性能优于传统方法。

    Developing and discovering new drugs is a complex and resource-intensive endeavor that often involves substantial costs, time investment, and safety concerns. A key aspect of drug discovery involves identifying novel drug-target (DT) interactions. Existing computational methods for predicting DT interactions have primarily focused on binary classification tasks, aiming to determine whether a DT pair interacts or not. However, protein-ligand interactions exhibit a continuum of binding strengths, known as binding affinity, presenting a persistent challenge for accurate prediction. In this study, we investigate various techniques employed in Drug Target Interaction (DTI) prediction and propose novel enhancements to enhance their performance. Our approaches include the integration of Protein Language Models (PLMs) and the incorporation of Contact Map information as an inductive bias within current models. Through extensive experimentation, we demonstrate that our proposed approaches outper
    
[^18]: RTDK-BO：具有Reinforced Transformer深度核函数的高维贝叶斯优化

    RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])

    [http://arxiv.org/abs/2310.03912](http://arxiv.org/abs/2310.03912)

    本文提出了一种新的提高贝叶斯优化模型建模能力的方法，通过将注意机制融入深度核学习中，使得代理能够适应上下文信息，提高优化性能。

    

    贝叶斯优化（BO）通过高斯过程（GP）代理指导，已经被证明是一种对于高维黑盒优化非常有效的技术，在工业设计和科学计算等许多应用中具有重要意义。最近的研究在单函数优化和少样本多目标优化上引入了强化学习（RL）来提高优化性能。然而，即使是少样本技术也不能充分利用紧密相关目标之间的相似性。本文结合了深度核学习（DKL）和基于注意力的Transformer模型的最新进展，改进了GP代理的建模能力与元学习相结合。我们提出了一种新的方法，通过将注意机制融入DKL中来改进元学习BO代理，使代理能够在BO过程中适应上下文信息。我们将这种Transformer深度核方法与少样本元学习相结合，通过元学习来提高BO的建模能力。

    Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
    
[^19]: 用强化学习实现的PyDCM：为可持续性定制数据中心模型

    PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])

    [http://arxiv.org/abs/2310.03906](http://arxiv.org/abs/2310.03906)

    PyDCM是一个用强化学习实现的可定制的数据中心模型，通过使用自定义配置和向量化的热计算，实现了对数据中心的优化，具有较高的效率。

    

    全球对可持续性和减少碳排放的强调日益增加，促使政府和企业重新思考数据中心的设计和运营方法。鉴于数据中心的高能耗和指数级计算工作量，优化能耗特别是在冷却和IT能源使用方面，数据中心是优化电力消耗的理想候选。解决这个问题的一个重要挑战是缺乏可配置和可扩展的热数据中心模型，它提供了一个端到端的管道。数据中心由多个IT组件组成，其几何配置和散热使得热建模变得困难。本文介绍了PyDCM，这是一个用Python实现的可定制的数据中心模型，用户可以使用自定义的服务器规格和IT机柜的几何布置创建独特的配置。使用向量化的热计算使得PyDCM比当前方法快了数个数量级（30倍）。

    The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
    
[^20]: 在概率测度空间中通过梯度流进行抽样

    Sampling via Gradient Flows in the Space of Probability Measures. (arXiv:2310.03597v1 [stat.ML])

    [http://arxiv.org/abs/2310.03597](http://arxiv.org/abs/2310.03597)

    通过梯度流抽样方法的研究方向在计算科学和工程中具有重要意义。本文通过研究概率测度空间中的梯度流的设计组成部分，提出了三个贡献：Kullback-Leibler散度作为能量泛函的独特属性、度量的选择与不变性的关系。

    

    在计算科学和工程中，使用未知归一化常数的目标概率分布进行抽样是一项基本的挑战。最近的研究表明，通过考虑概率测度空间中的梯度流派生的算法为算法开发开辟了新的途径。本文通过审查这种梯度流的设计组成部分，对这种抽样方法做出了三个贡献。抽样的任何实例化都需要一个能量泛函和一个度量来确定流动，以及流动的数值近似来推导算法。我们的第一个贡献是展示了Kullback-Leibler散度作为一个能量泛函具有唯一的特征（在所有f-散度中），即由其得到的梯度流不依赖于目标分布的归一化常数。我们的第二个贡献是从不变性的角度研究度量的选择。Fisher-Rao度量被称为t

    Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as t
    
[^21]: 机器学习耦合动力系统中的相互作用网络

    Machine learning the interaction network in coupled dynamical systems. (arXiv:2310.03378v1 [math.DS])

    [http://arxiv.org/abs/2310.03378](http://arxiv.org/abs/2310.03378)

    本研究使用自监督神经网络模型，从观察到的轨迹数据中恢复耦合动力系统的相互作用网络并预测个体代理的动态。

    

    相互作用动力系统的研究在科学和工程的各个领域仍然具有研究兴趣。在一组相互作用的粒子中，相互作用网络包含了各个组件之间的相互作用信息。从代理的动态中推断相互作用网络的信息是一个长期以来感兴趣的问题。在这项工作中，我们采用了一个自监督神经网络模型，实现了两个结果：恢复相互作用网络和预测个体代理的动态。这些信息都仅从观察到的轨迹数据中推断出来。本研究将神经关系推理模型应用于两个动力系统：通过胡克定律相互作用的耦合粒子和耦合相位（Kuramoto）振荡器。

    The study of interacting dynamical systems continues to attract research interest in various fields of science and engineering. In a collection of interacting particles, the interaction network contains information about how various components interact with one another. Inferring the information about the interaction network from the dynamics of agents is a problem of long-standing interest. In this work, we employ a self-supervised neural network model to achieve two outcomes: to recover the interaction network and to predict the dynamics of individual agents. Both these information are inferred solely from the observed trajectory data. This work presents an application of the Neural Relational Inference model to two dynamical systems: coupled particles mediated by Hooke's law interaction and coupled phase (Kuramoto) oscillators.
    
[^22]: 通过联合Transformer进行全新药物设计

    De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])

    [http://arxiv.org/abs/2310.02066](http://arxiv.org/abs/2310.02066)

    提出了联合Transformer来解决全新药物设计的困难，同时生成新颖分子并预测目标属性。使用惩罚对数似然目标训练模型，在分子生成和预测精度方面取得了最先进的性能，相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。通过联合Transformer生成的新颖分子在全新药物设计中优于其他基于SMILES的优化方法。

    

    全新药物设计需要同时生成超出训练数据的新颖分子并预测其目标属性，这对生成模型来说是一项艰巨任务。为了解决这个问题，我们提出了联合Transformer，它将Transformer的解码器、编码器和预测器结合为一个具有共享权重的联合生成模型。我们证明了使用惩罚对数似然目标来训练模型可以在分子生成方面实现最先进的性能，同时相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。最后，我们提出了一种概率黑盒优化算法，通过使用联合Transformer生成具有改进目标属性的新颖分子，相比于训练数据，在全新药物设计中表现优于其他基于SMILES的优化方法。

    De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
    
[^23]: DeepZero: 将零阶优化应用于深度模型训练的扩展

    DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])

    [http://arxiv.org/abs/2310.02025](http://arxiv.org/abs/2310.02025)

    DeepZero是一个扩展零阶优化到深度神经网络训练的深度学习框架，通过创新的坐标梯度估计和稀疏诱导的零阶训练协议，实现了高准确性和计算效率的优化。

    

    在无法获取一阶信息时，零阶优化已成为解决机器学习问题的一种常用技术。然而，零阶优化的可扩展性仍然是一个待解决的问题：其应用主要局限在相对小规模的机器学习问题上。我们开发了DeepZero，一个基于零阶优化的深度学习框架，通过三个主要创新将零阶优化扩展到从零开始的深度神经网络训练中。

    Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model
    
[^24]: 实现稳健的心脏分割：使用图卷积网络

    Towards Robust Cardiac Segmentation using Graph Convolutional Networks. (arXiv:2310.01210v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.01210](http://arxiv.org/abs/2310.01210)

    这项研究使用了图卷积网络来实现心脏分割，通过预测轮廓点而不是标记每个像素，消除了心脏分割中的解剖学错误。同时还对图卷积网络进行了消融研究，并评估了临床测量指标的性能。

    

    全自动化的心脏分割可以快速、可重复地从超声心动图检查中提取临床测量指标。U-Net结构是目前医学分割领域的深度学习架构，可以实时分割心脏结构，并且平均误差可与观测者间变异性相媲美。然而，该架构仍然会生成许多解离异常的结构。本研究使用图卷积神经网络的概念，预测出感兴趣结构的轮廓点，而不是对每个像素进行标记。我们提出了一个基于心脏解剖学的图结构，并证明这消除了公开可获得的CAMUS数据集上的多结构分割中的解剖学错误。此外，本研究还对图卷积网络进行了消融研究，并在临床HUNT4数据集上评估了临床测量指标。

    Fully automatic cardiac segmentation can be a fast and reproducible method to extract clinical measurements from an echocardiography examination. The U-Net architecture is the current state-of-the-art deep learning architecture for medical segmentation and can segment cardiac structures in real-time with average errors comparable to inter-observer variability. However, this architecture still generates large outliers that are often anatomically incorrect. This work uses the concept of graph convolutional neural networks that predict the contour points of the structures of interest instead of labeling each pixel. We propose a graph architecture that uses two convolutional rings based on cardiac anatomy and show that this eliminates anatomical incorrect multi-structure segmentations on the publicly available CAMUS dataset. Additionally, this work contributes with an ablation study on the graph convolutional architecture and an evaluation of clinical measurements on the clinical HUNT4 dat
    
[^25]: 数据过滤网络

    Data Filtering Networks. (arXiv:2309.17425v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.17425](http://arxiv.org/abs/2309.17425)

    本文研究了学习数据过滤网络用于筛选大型未策划数据集的问题，并构建了新的数据过滤网络，从而产生最先进的图像-文本数据集。

    

    大型训练集已成为机器学习的基石，并为语言建模和多模态学习的最新进展奠定了基础。虽然对于预训练的数据采集仍然是一种常见的范式，但数据策划往往仍然是临时的。一种常见的方法是首先从网络上收集大量数据，然后通过各种启发式方法将此候选池筛选到实际的训练集中。在这项工作中，我们研究了学习数据过滤网络（DFN）用于筛选大型未策划数据集的问题。我们的主要发现是，用于筛选的网络的质量与其在下游任务上的表现是不同的：例如，一个在ImageNet上表现良好的模型可能会产生比一个在ImageNet上准确率较低但在一小部分高质量数据上进行训练的模型更差的训练集。基于我们的洞察力，我们构建了新的数据过滤网络，从而产生了最先进的图像-文本数据集。具体而言，我们表现最佳的数据集DFN-5B使我们能够进行训练。

    Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train 
    
[^26]: 镜像重量对生物医学应用中的3D网格分割的益处

    Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications. (arXiv:2309.17076v1 [eess.IV])

    [http://arxiv.org/abs/2309.17076](http://arxiv.org/abs/2309.17076)

    本研究展示了在生物医学应用中，3D网格分割任务中，镜像重量对称性在神经网络中的影响。我们发现，重量对称性可以提高1至3％的额外准确度，并且可以减少参数数量高达8倍，而不影响性能。

    

    3D网格分割是具有许多生物医学应用的重要任务。人体具有对称和一些器官位置的变化。我们可以预期在执行生物医学分割的卷积神经网络中，旋转和反转不变的层会产生积极影响。在本研究中，我们展示了在执行3D网格分割的神经网络中，重量对称性的影响。我们分析了病理血管结构（动脉瘤）和传统解剖结构（心室的内膜和外膜）的3D网格分割问题。局部几何特征被编码为从有符号距离函数中采样，并且神经网络对每个网格节点进行预测。我们发现，重量对称性可以增加1至3％的额外准确度，并且可以减少可训练参数的数量高达8倍，而不会降低性能，前提是神经网络至少具有三个卷积层。

    3D mesh segmentation is an important task with many biomedical applications. The human body has bilateral symmetry and some variations in organ positions. It allows us to expect a positive effect of rotation and inversion invariant layers in convolutional neural networks that perform biomedical segmentations. In this study, we show the impact of weight symmetry in neural networks that perform 3D mesh segmentation. We analyze the problem of 3D mesh segmentation for pathological vessel structures (aneurysms) and conventional anatomical structures (endocardium and epicardium of ventricles). Local geometrical features are encoded as sampling from the signed distance function, and the neural network performs prediction for each mesh node. We show that weight symmetry gains from 1 to 3% of additional accuracy and allows decreasing the number of trainable parameters up to 8 times without suffering the performance loss if neural networks have at least three convolutional layers. This also work
    
[^27]: GInX-Eval: 面向图神经网络解释的内分布评估

    GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations. (arXiv:2309.16223v1 [cs.AI])

    [http://arxiv.org/abs/2309.16223](http://arxiv.org/abs/2309.16223)

    本文针对图神经网络解释的内分布评估问题，提出了GInX-Eval方法，克服了传统评估指标的局限性，为解释方法提供了新的见解。

    

    最近，为了突出图中对模型预测最有贡献的边和节点，人们开发了各种解释图神经网络（GNN）的方法。然而，目前尚不清楚如何从人类或模型的角度评估这些解释的正确性。当前评估过程中一个未解决的瓶颈问题是解释的分布与训练数据的分布不同的情况。这个重要问题会影响到现有的评估指标，如流行的忠实度或保真度得分。在本文中，我们展示了忠实度指标的局限性。我们提出了GInX-Eval（图内分布解释评估），这是一种用于评估图解释的过程，克服了忠实度的缺陷，并提供了对解释方法的新见解。使用重新训练策略，GInX得分可衡量已移除边对模型的信息量以及边的重要性。

    Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank s
    
[^28]: 关于计算纠缠及其在对抗机器学习中的解释

    On Computational Entanglement and Its Interpretation in Adversarial Machine Learning. (arXiv:2309.15669v1 [cs.LG])

    [http://arxiv.org/abs/2309.15669](http://arxiv.org/abs/2309.15669)

    本研究探索了对抗机器学习模型的复杂性和可解释性，通过将其与爱因斯坦的特殊相对论中的纠缠概念联系起来，发现远程特征样本可以表现出纠缠现象，挑战了对抗可传递性现象的传统描述方法。

    

    由于对抗性样本在机器学习中具有欺骗模型的能力，潜在地导致严重后果，因此已成为研究的焦点。在本研究中，我们对对抗机器学习模型进行了全面探索，揭示了它们固有的复杂性和可解释性。我们的调查揭示了机器学习模型复杂性与爱因斯坦的特殊相对论之间的有趣联系，通过纠缠的概念。更具体地说，我们对计算纠缠进行了定义，并证明了远程特征样本可以表现出强相关性，类似于量子领域中的纠缠。这一发现挑战了对当代机器学习模型中观察到的对抗可传递性现象的传统描述方法。

    Adversarial examples in machine learning has emerged as a focal point of research due to their remarkable ability to deceive models with seemingly inconspicuous input perturbations, potentially resulting in severe consequences. In this study, we embark on a comprehensive exploration of adversarial machine learning models, shedding light on their intrinsic complexity and interpretability. Our investigation reveals intriguing links between machine learning model complexity and Einstein's theory of special relativity, through the concept of entanglement. More specific, we define entanglement computationally and demonstrate that distant feature samples can exhibit strong correlations, akin to entanglement in quantum realm. This revelation challenges conventional perspectives in describing the phenomenon of adversarial transferability observed in contemporary machine learning models. By drawing parallels with the relativistic effects of time dilation and length contraction during computatio
    
[^29]: 通过神经后验主成分进行不确定性量化

    Uncertainty Quantification via Neural Posterior Principal Components. (arXiv:2309.15533v1 [cs.CV])

    [http://arxiv.org/abs/2309.15533](http://arxiv.org/abs/2309.15533)

    本论文提出了一种使用神经网络在单次前向传递中预测任意输入图像后验分布的主成分的方法，以实现不确定性量化。

    

    不确定性量化对于在自动驾驶和生物成像等安全关键领域中部署图像恢复模型至关重要。迄今为止，关于不确定性可视化的方法主要集中在每像素估计上。然而，每像素方差的热图通常在实际中用途有限，因为它无法捕捉像素之间的强相关性。更自然的不确定性度量对应于后验分布的主成分（PCs）上的方差。理论上，可以通过对输入图像的条件生成模型生成的样本应用PCA来计算PCs。然而，这需要在测试时生成大量的样本，而在目前的最先进（扩散）模型下非常缓慢。在该工作中，我们提出了一种方法来在神经网络的单次前向传递中预测后验分布的PCs，适用于任意输入图像。

    Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. However, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap
    
[^30]: 空间频率通道、形状偏倚和对抗鲁棒性

    Spatial-frequency channels, shape bias, and adversarial robustness. (arXiv:2309.13190v1 [cs.LG])

    [http://arxiv.org/abs/2309.13190](http://arxiv.org/abs/2309.13190)

    本研究通过临界带屏蔽实验揭示了人类物体识别和神经网络物体识别在空间频率通道上的差异，发现人类在自然图像中识别物体所使用的通道与字母和光栅识别相同，而神经网络具有不同的频率通道和鲁棒性。

    

    人类和神经网络在识别物体时使用了哪些空间频率信息？在神经科学中，临界带屏蔽是一种常用的工具，可以揭示用于物体识别的频率选择性滤波器。临界带屏蔽测量了识别性能对不同空间频率添加噪声的敏感度。现有的临界带屏蔽研究表明，人类通过空间频率滤波器（或“通道”）识别周期性模式（光栅）和字母，该滤波器的频带宽度为一个八度（频率翻倍）。在这里，我们将临界带屏蔽引入为网络与人类对比的任务，并在窄带噪声存在下测试了14名人类和76个神经网络在16路ImageNet分类上的表现。我们发现人类使用与字母和光栅相同的一个八度宽的通道来识别自然图像中的物体，使其成为人类物体识别的一个典型特征。另一方面，神经网络使用了不同的频率通道，在不同的空间频率中具有不同的鲁棒性。

    What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or "channel'') that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. On the other hand, the neural netwo
    
[^31]: 基于健康影响力噪声的训练来抵御数据污染攻击的方法

    HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])

    [http://arxiv.org/abs/2309.08549](http://arxiv.org/abs/2309.08549)

    本论文提出了一种名为健康影响力噪声训练的高效稳健训练方法，该方法使用影响函数制造了有助于加强分类模型对抗数据污染攻击的健康噪声，并且在仅修改训练数据的子集时也能有效运行。

    

    虽然已经提出了许多防御方法来防止来自不可信数据源的潜在污染攻击，但大多数研究仅针对特定攻击进行防御，这给了攻击者许多可利用的机会。在本论文中，我们提出了一种基于影响函数的高效稳健训练方法，名为健康影响力噪声训练。通过使用影响函数，我们制造了有助于加强分类模型对抗污染攻击的健康噪声，同时不会对测试数据的泛化能力产生显著影响。此外，我们的方法可以在仅修改训练数据的子集时有效运行，而不是如几种之前的方法中那样向所有示例添加噪声。我们在两个图像数据集上进行了全面评估，并考虑不同的实际攻击场景下的最新攻击技术。我们的实证结果表明，H

    While numerous defense methods have been proposed to prohibit potential poisoning attacks from untrusted data sources, most research works only defend against specific attacks, which leaves many avenues for an adversary to exploit. In this work, we propose an efficient and robust training approach to defend against data poisoning attacks based on influence functions, named Healthy Influential-Noise based Training. Using influence functions, we craft healthy noise that helps to harden the classification model against poisoning attacks without significantly affecting the generalization ability on test data. In addition, our method can perform effectively when only a subset of the training data is modified, instead of the current method of adding noise to all examples that has been used in several previous works. We conduct comprehensive evaluations over two image datasets with state-of-the-art poisoning attacks under different realistic attack scenarios. Our empirical results show that H
    
[^32]: PRE: 视觉-语言提示学习与重新参数化编码器

    PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])

    [http://arxiv.org/abs/2309.07760](http://arxiv.org/abs/2309.07760)

    这项工作提出了一种名为PRE的方法，通过重新参数化编码器来增强可学习提示的泛化能力，从而解决了大型预训练视觉-语言模型中手动提示工程的挑战。

    

    大型预训练的视觉-语言模型（如CLIP）已经展示出在零样本迁移任务中具有巨大潜力。然而，为了达到最佳性能，需要手动选择提示以改进下游图像分布和文本类描述之间的对齐。这种手动提示工程是将这些模型部署到实践中的主要挑战，因为它需要领域专业知识并且非常耗时。为了避免复杂的提示工程，最近的CoOp工作引入了在视觉领域使用可控文本标记的提示学习概念。虽然CoOp可以在手动提示上取得显著改进，但其学到的上下文在同一数据集中更广泛的未见类别中的泛化能力较差。在这项工作中，我们提出了一种名为Prompt Learning with Reparameterization Encoder (PRE) 的简单高效的方法，改进了可学习提示的泛化能力。

    Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
    
[^33]: PolyLUT: 用于超低延迟FPGA基于查找表推理的分段多项式学习

    PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference. (arXiv:2309.02334v1 [cs.LG])

    [http://arxiv.org/abs/2309.02334](http://arxiv.org/abs/2309.02334)

    提出了一种名为PolyLUT的新方法，用于训练神经网络在FPGA上进行部署。该方法利用多变量多项式作为基本模块，并利用软逻辑将多项式评估隐藏在FPGA的查找表中，从而实现超低延迟推理，并减少了软件逻辑的层数。

    

    可编程门阵列（FPGA）被广泛用于实现深度学习推理。标准的深度神经网络推理涉及交错线性映射和非线性激活函数的计算。以往的超低延迟实现工作在FPGA查找表（LUT）中硬编码了线性映射和非线性激活的组合。我们的工作受到这个想法的启发，即FPGA中的LUT可以用来实现比这更多样化的函数。在本文中，我们提出了一种新的方法来训练用于FPGA部署的神经网络，以多变量多项式作为基本模块。我们的方法利用软件逻辑提供的灵活性，将多项式评估隐藏在LUT中且没有任何开销。我们表明，通过使用多项式模块，我们可以实现相同的准确度，而使用的软件逻辑层数要比使用线性函数要少得多，从而带来显著的延迟和面积的减少。

    Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area i
    
[^34]: 线性振动：视觉转换器中的困惑美学

    Linear Oscillation: The Aesthetics of Confusion for Vision Transformer. (arXiv:2308.13670v1 [cs.LG])

    [http://arxiv.org/abs/2308.13670](http://arxiv.org/abs/2308.13670)

    研究提出了一种新的激活函数——线性振动（LoC）激活函数，它通过将线性轨迹和振荡偏差无缝融合，捕捉到了“困惑的重要性”的本质。实证研究表明，在需要区分微妙模式的情境中，使用LoC激活函数可以提高网络的鲁棒性。

    

    激活函数是深度学习的关键，深刻影响神经网络的表示能力和训练动力学。它们不仅塑造了表示的性质，还优化了收敛速度并增强了泛化能力。鉴于这一关键作用，我们提出了线性振动（LoC）激活函数，定义为$f(x) = x \times \sin(\alpha x + \beta)$。与传统的激活函数不同，LoC将线性轨迹与振荡偏差无缝融合。命名为“线性振动”是对其独特属性的致敬，即通过和谐的振动融入线性激活，捕捉“困惑的重要性”的本质。网络激活中的“控制性困惑”概念被认为能够促进更稳健的学习，特别是在需要区分微妙模式的情境中。我们的实证研究表明，当使用LoC激活函数时，网络可以更好地捕捉到隐藏的模式，提高了学习的鲁棒性。

    Activation functions are the linchpins of deep learning, profoundly influencing both the representational capacity and training dynamics of neural networks. They shape not only the nature of representations but also optimize convergence rates and enhance generalization potential. Appreciating this critical role, we present the Linear Oscillation (LoC) activation function, defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional activation functions which primarily introduce non-linearity, LoC seamlessly blends linear trajectories with oscillatory deviations. The nomenclature ``Linear Oscillation'' is a nod to its unique attribute of infusing linear activations with harmonious oscillations, capturing the essence of the 'Importance of Confusion'. This concept of ``controlled confusion'' within network activations is posited to foster more robust learning, particularly in contexts that necessitate discerning subtle patterns. Our empirical studies reveal that, when i
    
[^35]: 维护操作中数据共享学习的最优数据汇聚

    Optimal data pooling for shared learning in maintenance operations. (arXiv:2308.12670v1 [cs.LG])

    [http://arxiv.org/abs/2308.12670](http://arxiv.org/abs/2308.12670)

    本文研究了在维护操作中，通过数据共享学习可以显著降低成本的最优数据汇聚方法。

    

    本文讨论了维护操作中汇聚数据进行共享学习的好处。我们考虑了一组通过先验未知速率相互耦合的泊松退化系统。涉及这些系统的决策问题是高维度马尔可夫决策过程（MDP）。我们提出了一个将这样的MDP分解为二维MDP的分解结果，从而实现结构分析和计算。我们利用这个分解结果证明了数据共享可以相对于不共享时显著降低成本。

    This paper addresses the benefits of pooling data for shared learning in maintenance operations. We consider a set of systems subject to Poisson degradation that are coupled through an a-priori unknown rate. Decision problems involving these systems are high-dimensional Markov decision processes (MDPs). We present a decomposition result that reduces such an MDP to two-dimensional MDPs, enabling structural analyses and computations. We leverage this decomposition to demonstrate that pooling data can lead to significant cost reductions compared to not pooling.
    
[^36]: 神经动力学的低阶张量秩学习

    Low Tensor Rank Learning of Neural Dynamics. (arXiv:2308.11567v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.11567](http://arxiv.org/abs/2308.11567)

    研究发现通过学习过程中的张量秩演化来理解神经元连接在学习中的协调变化。研究表明训练过的递归神经网络的权重矩阵通常具有低秩结构，而这种结构在整个学习过程中保持在一个固定的低维子空间中。对真实权重进行低秩分解验证了这一观察结果。

    

    学习依赖于神经元群体中的协调突触变化。因此，了解学习过程中突触连接的集体演化是神经科学和机器学习中的一个关键挑战。近期的研究表明，经过训练的递归神经网络（RNN）的权重矩阵通常是低秩的，但是这种低秩结构如何在学习过程中展开还不清楚。为了解决这个问题，我们研究了整个学习过程中由权重矩阵形成的3阶张量的秩。通过用不同秩的RNN拟合大规模神经记录的运动学习任务，我们发现推断的权重是低阶张量秩的，因此在整个学习过程中在一个固定的低维子空间中演化。接下来，我们通过在真实权重上直接进行低阶张量秩分解，并展示我们所使用的方法，验证了低阶张量秩学习的观察结论。

    Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applie
    
[^37]: Flamingo: 多轮单服务器安全聚合及其在私有联邦学习中的应用

    Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning. (arXiv:2308.09883v1 [cs.CR])

    [http://arxiv.org/abs/2308.09883](http://arxiv.org/abs/2308.09883)

    Flamingo是一个用于实现跨大量客户端安全聚合的系统，在私有联邦学习中有广泛的应用。通过消除每轮设置和引入轻量级的丢失容忍协议，Flamingo解决了以往协议在多轮设置下的问题，并引入了新的本地选择客户端邻域的方式。

    

    本文介绍了Flamingo，这是一个用于跨大量客户端安全聚合数据的系统。在安全聚合中，服务器对客户端的私有输入进行求和，并在不了解个体输入的情况下得到结果，仅能推断出最终总和。Flamingo专注于联邦学习中的多轮设置，其中执行多个连续的模型权重求和（平均），以得到一个良好的模型。之前的协议（例如Bell等人的CCS '20）仅适用于单轮，并通过多次重复该协议来适应联邦学习的设置。Flamingo消除了之前协议每轮设置的需求，并引入了一种新的轻量级的丢失容忍协议，以确保如果客户端在求和过程中离开，服务器仍然可以获得有意义的结果。此外，Flamingo还引入了一种新的本地选择所谓的客户端邻域的方式，此概念由Bell等人提出。

    This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al
    
[^38]: 适应您的教师: 改进知识蒸馏用于无样本连续学习

    Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.09544](http://arxiv.org/abs/2308.09544)

    本研究提出了一种适应教师的方法（TA）用于无样本连续学习，解决了知识蒸馏方法在这种情况下的性能下降问题，并在多个基准测试中持续提升模型性能。

    

    在这项工作中，我们研究了无样本类别增量学习（CIL），并使用知识蒸馏（KD）作为正则化策略，旨在防止遗忘。KD方法在CIL中取得了成功，但是在没有访问先前任务的训练数据示例的情况下，它们往往很难对模型进行正则化。我们的分析发现，这个问题源于处理分布外数据时教师网络中的显著表示转换。这导致KD损失成分中出现较大的错误，从而导致CIL模型性能下降。受最近的测试时适应方法的启发，我们引入了教师适应（TA）方法，该方法在增量训练过程中同时更新教师和主模型。我们的方法与基于KD的CIL方法无缝集成，能够在多个无样本CIL基准测试中持续提升它们的性能。

    In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
    
[^39]: 使用深度知识蒸馏基本特征进行复杂面部表情识别

    Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features. (arXiv:2308.06197v1 [cs.CV])

    [http://arxiv.org/abs/2308.06197](http://arxiv.org/abs/2308.06197)

    这项研究提出了一种基于深度知识蒸馏的新颖持续学习方法，通过使用少量训练样本，能够准确识别新的复合表情类别。

    

    复杂情绪识别是一项认知任务，迄今为止，其表现在与人类认知水平相等或以上的其他任务中表现优秀的程度已经被证明是较为困难的。由于人脸表达的情绪复杂性，通过面部表情进行情绪识别特别困难。为了使机器在这一领域达到与人类相同的表现水平，它可能需要实时合成知识并理解新概念。人类能够仅仅使用几个例子学习新概念，通过从记忆中提取重要信息并丢弃其余信息。同样，持续学习方法可以在保留已知类别知识的同时学习新类别，而少样本学习方法能够使用非常少的训练样本学习新类别。我们提出了一种新颖的持续学习方法，受到人类认知和学习的启发，可以使用少量的训练样本准确地识别新的复合表情类别。

    Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by
    
[^40]: 使用传感器数据、方程和自然语言提示上下文中的运算符学习

    Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])

    [http://arxiv.org/abs/2308.05061](http://arxiv.org/abs/2308.05061)

    本文提出了一种使用传感器数据、方程和自然语言提示上下文中运算符学习的方法。通过整合人类知识和语言描述，该方法不仅扩展了物理信息学习的灵活性和普适性，而且显著提高了学习性能和减少了数据需求。

    

    在科学机器学习领域中，上下文中的运算符学习已经展示出了在推理阶段从提示数据中学习运算符的显著潜力，而无需进行权重更新。然而，当前模型对传感器数据的过度依赖可能会无意中忽视运算符的宝贵的人类洞察力。为了解决这个问题，我们将上下文中的运算符学习转化为一种多模式范式。我们提出使用“标题”来整合通过自然语言描述和方程式表达的运算符的人类知识。我们演示了这种方法不仅扩展了物理信息学习的灵活性和普遍性，而且还显著提高了学习性能并减少了数据需求。此外，我们引入了一种更高效的多模式上下文运算符学习的神经网络架构，称为“ICON-LM”，基于类似于语言模型的架构。

    In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
    
[^41]: 在马尔可夫采样下，用于随机梯度下降的在线协方差估计

    Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])

    [http://arxiv.org/abs/2308.01481](http://arxiv.org/abs/2308.01481)

    本文研究了在马尔可夫采样下的随机梯度下降中的在线重叠批次均值协方差估计器，并证明了其收敛速率为$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$和$O\big(\sqrt{d}\,n^{-1/8}\big)$，分别对应于状态相关和状态无关的马尔可夫采样。这些速率与独立同分布情况下的最佳收敛速率相匹配，并且克服了由于马尔可夫采样而引起的挑战。

    

    我们研究了用于马尔可夫采样下随机梯度下降的在线重叠批次均值协方差估计器。我们证明了协方差估计器的收敛速率分别为$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$和$O\big(\sqrt{d}\,n^{-1/8}\big)$，其中$d$代表维度，$n$表示观测数量或SGD迭代次数。值得注意的是，这些速率与先前由\cite{zhu2021online}在独立同分布($\iid$)情况下建立的最佳收敛速率相匹配，除了对数因子。我们的分析克服了由于马尔可夫采样而产生的重要挑战，引入了额外的误差项和批次均值协方差估计器的复杂依赖关系。此外，我们还建立了SGD动态误差$\ell_2$范数的前四阶矩的收敛速率。

    We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and $O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\iid$) case by \cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\ell_2$ norm of the error of SGD dynamics u
    
[^42]: 离散神经网络和多态学习

    Discrete neural nets and polymorphic learning. (arXiv:2308.00677v1 [cs.NE])

    [http://arxiv.org/abs/2308.00677](http://arxiv.org/abs/2308.00677)

    本文提出了一种基于离散模拟的神经网络，将Murski{i}的通用代数定理和Cybenko的通用逼近结果统一起来。同时，通过引入关系结构的多态性，我们提出了一种学习算法，用于经典学习任务。

    

    20世纪70年代Murski{i}提出的通用代数定理与20世纪80年代Cybenko等人关于神经网络的通用逼近结果有着惊人的相似性。本文考虑了经典神经网络的离散模拟，将这些结果放在一个统一的框架下。我们介绍了一种基于关系结构多态性的学习算法，并展示了如何将其用于经典学习任务。

    Theorems from universal algebra such as that of Murski\u{i} from the 1970s have a striking similarity to universal approximation results for neural nets along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural net which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.
    
[^43]: 人工智能提高了全球可靠洪水预警的覆盖范围

    AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])

    [http://arxiv.org/abs/2307.16104](http://arxiv.org/abs/2307.16104)

    本研究开发了一个人工智能模型，可以准确预测未经测量流域的极端水文事件，从而提高了全球洪水预警的覆盖范围。

    

    洪水是最常见和影响最大的自然灾害之一，对发展中国家尤其具有不对称的影响，这些国家往往缺乏密集的水流监测网络。准确及时的预警对于减轻洪水风险至关重要，但准确的水文模拟模型通常需要根据每个应用的流域中的长时间数据记录进行校准。我们开发了一个人工智能（AI）模型，可以预测7天内的极端水文事件。该模型在所有大洲、前导时间和重现期中均明显优于当前最先进的全球水文模型（Copernicus应急管理服务全球洪水意识系统）。AI在未经测量的流域中的预测尤其有效，这很重要，因为全球只有百分之几的流域具有流量观测站，而发展中国家的未经测量的流域数量占比很高，对人类特别脆弱。

    Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
    
[^44]: 语言模型中复杂技能产生的理论

    A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])

    [http://arxiv.org/abs/2307.15936](http://arxiv.org/abs/2307.15936)

    本文提出了一个统计框架，通过分析语言模型的交叉熵损失与基本语言任务的能力之间的关系，揭示了语言模型中复杂技能产生的机制。研究结果表明，通过扩展定律，预训练模型能够高效学习，并表现出违反通常泛化理论的能力。

    

    当语言模型的参数集合和训练语料库扩大时，新的技能将在 AI 产品中出现的主要驱动因素。这种现象尚不为人所理解，并且通过对基于梯度训练的数学分析提供机械解释似乎很困难。本文采用不同的方法，使用著名的（和经验性的）LLM扩展定律和简单的统计框架来分析出现。贡献包括：（a）一个统计框架将LLM的交叉熵损失与语言任务基本技能的能力相关联。（b）数学分析表明，扩展定律意味着强烈的归纳偏见，使预训练模型能够学习得非常高效。我们非正式地称之为“弹弓泛化”，因为表面上看，它似乎提供了在技能水平上违反通常泛化理论的能力。（c）弹弓泛化的一个关键例子，即在执行任务时的能力。

    A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
    
[^45]: 自适应线性估计方程

    Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])

    [http://arxiv.org/abs/2307.07320](http://arxiv.org/abs/2307.07320)

    本文提出了一种解决自适应线性回归模型中非正态渐近行为的方法，使用自适应线性估计方程构建去偏估计量，并在多臂老虎机的背景下保持了最小二乘估计量的非渐近性能。

    

    顺序数据收集已成为增强数据收集过程效率的广泛采用的技术。尽管具有优势，但这种数据收集机制常常给统计推断过程引入复杂性。例如，在自适应线性回归模型中，普通最小二乘（OLS）估计量可能表现出非正态的渐近行为，从而对准确的推断和解释提出挑战。本文提出了一种构建去偏估计量的通用方法，该方法采用自适应线性估计方程的思想，并在理论上保证了渐近正态性，并讨论了实现近似最优渐近方差的问题。我们的估计量的一个显著特点是，在多臂老虎机的背景下，我们的估计量保留了最小二乘估计量的非渐近性能，同时获得了渐近正态性。因此，本工作解决了自适应线性回归模型中非正态渐近行为的问题，并为统计推断提供了可靠的方法。

    Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least square estimator while obtaining asymptotic normality property. Consequently, this work
    
[^46]: 任务和运动规划的具身化终身学习

    Embodied Lifelong Learning for Task and Motion Planning. (arXiv:2307.06870v1 [cs.RO])

    [http://arxiv.org/abs/2307.06870](http://arxiv.org/abs/2307.06870)

    这篇论文研究了在家中长期部署的机器人所面临的具身化终身学习问题，在任务和运动规划的背景下采用了新颖的问题建模方法。利用TAMP系统的模块化特性，提出了一个生成混合模型来产生规划器的候选参数。通过学习共享和非共享的模型，并根据代理任务来在线选择使用的模型，该方法在模拟的2D领域和BEHAVIOR基准测试中取得了显著的规划成功改进。

    

    在一个长时间内部署在家中的机器人面临着真正的终身学习问题。作为机器人寻求为用户提供帮助，它应该利用任何积累的经验来改进自己的知识，成为一个更熟练的助手。我们在任务和运动规划（TAMP）学习的背景下，对这种情况进行了新颖的终身学习问题建模。利用TAMP系统的模块化特性，我们开发了一个生成混合模型，为规划器生成候选连续参数。与大多数现有的终身学习方法预先确定数据如何在任务模型之间共享不同，我们的方法学习共享和非共享模型，并根据代理任务来决定在规划过程中在线使用哪个模型，这些任务作为每个模型对状态的理解的代理。我们的方法在模拟的2D领域和BEHAVIOR基准测试中的多个问题上展示了显著的规划成功的改进。

    A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.
    
[^47]: 一种新型的与平台无关的多模态深度学习模型，用于识别社交媒体上的促进饮食紊乱内容

    A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])

    [http://arxiv.org/abs/2307.06775](http://arxiv.org/abs/2307.06775)

    本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。

    

    在过去的十年中，饮食紊乱的诊断和与之相关的死亡数量大幅增加，尤其是在新冠疫情期间。这种巨大增长部分来源于疫情的压力，但也与社交媒体的暴露增加有关，社交媒体上充斥着促进饮食紊乱的内容。这些内容可以诱发观看者的饮食紊乱。本研究旨在创建一个多模态深度学习模型，能够基于视觉和文本数据的组合判断给定的社交媒体帖子是否促进饮食紊乱。从Twitter收集了一个带有标签的推文数据集，对其进行了十二个深度学习模型的训练和测试。根据模型的性能，最有效的深度学习模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的多模态融合模型，准确率和F1分数分别达到95.9%和0.959。RoBERTa和MaxViT融合模型可以有效地识别社交媒体上的促进饮食紊乱的内容。

    Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
    
[^48]: 基于癌症多组学数据的癌症新亚型和治疗的多头注意力机制学习

    Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])

    [http://arxiv.org/abs/2307.04075](http://arxiv.org/abs/2307.04075)

    本研究提出了一种基于注意力机制的无监督对比学习框架（AMUCL），用于分析癌症多组学数据并识别癌症新亚型。通过多头注意力机制和解耦对比学习模型（DMACL），该方法能够深度提取多组学数据特征并进行亚型聚类。

    

    由于癌症的高异质性和临床特征，不同癌症亚型之间的多组学数据和临床特征存在显著差异。因此，癌症亚型的识别和发现对于癌症的诊断、治疗和预后至关重要。本研究提出了一种基于注意力机制的无监督对比学习框架（AMUCL），用于分析癌症多组学数据，从而识别和表征癌症亚型。AMUCL框架包括一个无监督的多头注意力机制，用于深度提取多组学数据特征。重要的是，提出了一种基于多头注意力机制的解耦对比学习模型（DMACL），用于学习多组学数据特征和聚类，并识别新的癌症亚型。这种无监督对比学习方法通过计算特征空间中样本之间的相似度来聚类亚型。

    Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample
    
[^49]: 实现对周期性材料的对称感知生成

    Towards Symmetry-Aware Generation of Periodic Materials. (arXiv:2307.02707v1 [cs.LG])

    [http://arxiv.org/abs/2307.02707](http://arxiv.org/abs/2307.02707)

    提出了一种名为SyMat的新的材料生成方法，能够对周期性材料的物理对称性进行感知，并能在生成和优化任务中取得有希望的性能。

    

    我们考虑使用深度模型生成周期性材料的问题。虽然对于对称感知的分子生成已经得到了广泛研究，但是周期性材料具有不同的对称性，现有的方法并不能完全捕捉到这些对称性。在这项工作中，我们提出了一种新的材料生成方法SyMat，能够捕捉周期性材料结构的物理对称性。SyMat利用变分自动编码器模型生成材料的原子类型和晶格，通过生成原子类型集、晶格长度和晶格角度。此外，SyMat采用基于评分的扩散模型生成材料的原子坐标，在坐标扩散过程中使用了一种新的对称感知概率模型。我们证明SyMat在所有材料的对称变换上理论上是不变的，并且证明了SyMat在随机生成和性能优化任务上取得了有希望的性能。

    We consider the problem of generating periodic materials with deep models. While symmetry-aware molecule generation has been studied extensively, periodic materials possess different symmetries, which have not been completely captured by existing methods. In this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks.
    
[^50]: 运输、变分推断和扩散：应用于回火流和薛定谔桥的论文研究

    Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])

    [http://arxiv.org/abs/2307.01050](http://arxiv.org/abs/2307.01050)

    本文研究了最优运输和变分推断之间的联系，并提出了一种基于路径空间散度的采样和生成建模框架。通过开发新颖的基于得分的回火流技术和正则化的迭代比例拟合目标，本文展示了这些方法的潜力。

    

    本文探讨了最优运输与变分推断之间的联系，重点研究了正向和反向随机微分方程以及Girsanov变换。我们提出了一个基于路径空间散度的采样和生成建模的原则性和系统性框架。我们的工作最终发展出一个新颖的基于得分的回火流技术（与统计物理中的Jarzynski和Crooks恒等式有关）和一个正则化的迭代比例拟合（IPF）型目标，不同于标准IPF的顺序性。通过一系列的生成建模示例和基于双井的稀有事件任务，我们展示了所提方法的潜力。

    This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
    
[^51]: 通过跳过零元素降低卷积层的计算复杂度

    Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])

    [http://arxiv.org/abs/2306.15951](http://arxiv.org/abs/2306.15951)

    本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。

    

    深度神经网络依赖并行处理器进行加速。为了为其设计运算符，需要不仅有优化算法以降低复杂度，还需要充分利用硬件资源。卷积层主要包含三种运算符：前向传播的卷积，反向传播的反卷积和膨胀卷积。当执行这些运算时，始终会向张量中添加0元素，导致冗余计算。本文提出了C-K-S算法（ConvV2, KS-deconv, Sk-dilated），以两种方式跳过这些0元素：修剪滤波器以排除填充的0元素；将稀疏张量转换为稠密张量，避免在反卷积和膨胀卷积中插入0元素。与普通卷积相比，反卷积由于其复杂性而难以加速。本文提供了C-K-S的高性能GPU实现，并通过与PyTorch的比较验证了其有效性。根据实验结果，在某些情况下，C-K-S相对于PyTorch具有优势。

    Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
    
[^52]: RLHF是否比标准RL更困难？

    Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v1 [cs.LG])

    [http://arxiv.org/abs/2306.14111](http://arxiv.org/abs/2306.14111)

    本文证明了对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。

    

    从人类反馈学习的强化学习（RLHF）是从偏好信号学习，而标准强化学习（RL）则直接从奖励信号学习。偏好信号可能包含的信息比奖励信号少，这使得基于偏好的RL似乎更加困难。本文理论上证明，对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。具体而言，我们将问题分为两类：（1）基于奖励概率模型的偏好，此时可以将问题简化为容忍奖励小误差的鲁棒奖励RL问题；（2）对于一般的任意偏好且目标是找到von Neumann获胜者的情况，我们将问题简化为多智能体奖励RL问题，该问题可以在一组受限制的策略下找到马尔可夫博弈的因子纳什平衡解。后一种情况可以进一步降低成对关系的MDP。

    Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the f
    
[^53]: 交互变形的一次性模仿学习

    One-shot Imitation Learning via Interaction Warping. (arXiv:2306.12392v1 [cs.RO])

    [http://arxiv.org/abs/2306.12392](http://arxiv.org/abs/2306.12392)

    本研究提出了一种交互变形的方法实现了从单个演示中学习SE（3）机器人操纵策略，并在实验中取得了成功效果。

    

    在开放式应用中，通过少量示范学习机器人策略是至关重要的。我们提出了一种新的方法，即交互变形，用于从单个演示中学习SE（3）机器人操作策略。我们使用形状变形技术推断环境中每个物体的三维网格，并将操纵动作表示为物体上的关键点，可以用物体的形状进行变形。我们在三个模拟和真实世界的物体重新排列任务中成功地进行了一次性模仿学习。我们还展示了我们的方法在野外预测物体网格和机器人抓取的能力。

    Imitation learning of robot policies from few demonstrations is crucial in open-ended applications. We propose a new method, Interaction Warping, for learning SE(3) robotic manipulation policies from a single demonstration. We infer the 3D mesh of each object in the environment using shape warping, a technique for aligning point clouds across object instances. Then, we represent manipulation actions as keypoints on objects, which can be warped with the shape of the object. We show successful one-shot imitation learning on three simulated and real-world object re-arrangement tasks. We also demonstrate the ability of our method to predict object meshes and robot grasps in the wild.
    
[^54]: 稀疏模块激活用于高效的序列建模

    Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11197](http://arxiv.org/abs/2306.11197)

    本论文引入了稀疏模块激活 (SMA) 机制，用于高效的序列建模。这种机制可以动态地稀疏激活序列元素的子模块，减少计算和内存消耗。

    

    线性状态空间模型 (SSM) 在各种序列建模任务中表现出了很强的性能，因为它们有效地编码了循环结构。然而，在更综合的任务中，如语言建模和机器翻译中，基于自注意力的模型仍然优于SSM。同时使用SSM和自注意力的混合模型通常显示出有希望的性能，但当前方法将注意力模块静态且均匀地应用于输入序列中的所有元素，导致了质量和效率之间的次优权衡。在这项工作中，我们引入了稀疏模块激活 (SMA)，这是一种通用机制，使神经网络能够以可微分的方式稀疏地动态激活序列元素的子模块。通过允许每个元素跳过非激活的子模块，SMA可以在序列建模的训练和推理阶段降低计算和内存消耗。作为SMA的一个特定实例，我们设计了一种新颖的神经网络模型。

    Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
    
[^55]: 通过关系条件神经过程实现实用的等变性

    Practical Equivariances via Relational Conditional Neural Processes. (arXiv:2306.10915v1 [stat.ML])

    [http://arxiv.org/abs/2306.10915](http://arxiv.org/abs/2306.10915)

    本文提出的关系条件神经过程（RCNPs）是一种有效将等变性纳入任何神经过程模型的方法，并扩展了等变神经过程的适用性和影响力到更高的维度。

    

    条件神经过程（CNPs）是一类元学习模型，以其综合运行时效率和可靠的不确定性量化而受欢迎。许多相关的机器学习任务，例如时空建模、贝叶斯优化和连续控制，包含等变性，例如对于平移，模型可以利用最大的性能。然而，先前试图在CNPs中包含等变性在超过两个输入维度之外的尺度上无法有效扩展。在本文中，我们提出了关系条件神经过程（RCNPs），这是一种有效将等变性纳入任何神经过程模型的方法。我们提出的方法扩展了等变神经过程的适用性和影响力到更高的维度。我们在自然包含等变性任务的大量任务上经验证实了RCNPs的竞争性能。

    Conditional Neural Processes (CNPs) are a class of metalearning models popular for combining the runtime efficiency of amortized inference with reliable uncertainty quantification. Many relevant machine learning tasks, such as spatio-temporal modeling, Bayesian Optimization and continuous control, contain equivariances -- for example to translation -- which the model can exploit for maximal performance. However, prior attempts to include equivariances in CNPs do not scale effectively beyond two input dimensions. In this work, we propose Relational Conditional Neural Processes (RCNPs), an effective approach to incorporate equivariances into any neural process model. Our proposed method extends the applicability and impact of equivariant neural processes to higher dimensions. We empirically demonstrate the competitive performance of RCNPs on a large array of tasks naturally containing equivariances.
    
[^56]: 文本到图像扩散模型中的能量交叉注意力用于贝叶斯上下文更新

    Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])

    [http://arxiv.org/abs/2306.09869](http://arxiv.org/abs/2306.09869)

    本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。

    

    尽管文本到图像扩散模型在图像生成任务中表现出色，但最近的研究提出了一个问题，即生成的图像有时无法捕捉到文本提示的预期语义内容，这种现象通常被称为语义错位。为了解决这个问题，我们提出了一种新颖的基于能量的模型（EBM）框架。具体而言，我们首先在去噪自编码器的每个交叉注意力层中制定潜在图像表示和文本嵌入的EBM。然后，我们获得上下文向量的对数后验梯度，可以更新和转移到后续的交叉注意力层，从而隐式地最小化嵌套层次的能量函数。我们的潜在EBMs还允许零样本组合生成，即通过不同上下文的交叉注意力输出的线性组合。通过大量实验，我们证明了所提出的方法在处理各种图像生成任务方面非常有效，并可以显著降低文本提示和生成图像之间的语义错位现象。

    Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
    
[^57]: VillanDiffusion: 一种针对扩散模型的统一后门攻击框架。

    VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (arXiv:2306.06874v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.06874](http://arxiv.org/abs/2306.06874)

    本文提出VillanDiffusion，一个针对扩散模型的统一后门攻击框架，涵盖主流的无条件和有条件DM，便于对不同DM配置进行后门分析，并为基于字幕的DM后门攻击提供了新的见解。

    

    扩散模型（DM）是最先进的生成模型之一，它通过迭代添加噪声和去噪学习可逆的损坏过程。它们是许多生成人工智能应用的主干，例如文本到图像有条件生成。但是，最近的研究表明基本无条件DM（例如DDPM和DDIM）易受后门注入攻击，这是一种由于恶意嵌入模型输入的模式而触发的输出操纵攻击。本文提出了一个统一的后门攻击框架（VillanDiffusion），以扩展当前的DM后门分析范围。我们的框架涵盖了主流的无条件和有条件DM（基于去噪和基于评分），以及各种无需训练的采样器进行整体评估。实验证明，我们的统一框架便于对不同DM配置进行后门分析，并为基于字幕的DM后门攻击提供新的见解。

    Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.
    
[^58]: 特征级自监督学习方法FLSL

    FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])

    [http://arxiv.org/abs/2306.06203](http://arxiv.org/abs/2306.06203)

    本文提出FLSL方法，采用Transformer进行联合嵌入和聚类，适合于内视图和外视图特征聚类。实验证明该方法在语义类簇表达方面取得显著改进。

    

    当前的自监督学习方法（如SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，不适用于密集预测任务，例如对象检测和分割。本文针对这个问题，首次展示了Vision Transformers（ViT）的基础均值漂移聚类过程能够良好地与自然图像语义（例如物体和场景）对齐。通过采用Transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类的自监督学习方法，称为特征级自监督学习（FLSL）。我们提出了FLSL问题的正式定义，并从均值漂移和k-means的角度构建目标。实验证明，FLSL促进了显著的语义类簇表示，并学习了一种适合于内视图和外视图特征聚类的嵌入方案。FLSL的运用取得了显著改进。

    Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements 
    
[^59]: 超越表面统计学：潜在扩散模型中的场景表示

    Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v1 [cs.CV])

    [http://arxiv.org/abs/2306.05720](http://arxiv.org/abs/2306.05720)

    本文探究了潜在扩散模型内部是否创建和使用简单场景几何内部表示，使用线性探针发现LDM内部激活提供了关于3D深度数据和突出对象/背景分离的线性表示，在图像合成中具有因果作用, 可用于LDM输出的简单高级编辑。

    

    潜在扩散模型（LDM）展现了产生逼真图像的惊人能力，但这些模型的内在机制仍然神秘。即使在没有显式深度信息的图像上进行训练，它们通常也会输出一致的3D场景图片。在这项工作中，我们探讨了一个基本的可解释性问题：LDM是否创建并使用一个简单的场景几何内部表示？使用线性探针，我们发现LDM的内部激活编码了线性表示，既包括3D深度数据，又包括突出对象/背景区别。这些表示似乎在去噪过程的早期就出现了——在人类能轻易理解嘈杂的图像之前。干预性实验进一步表明，这些表示在图像合成中扮演因果作用，并且可能用于LDM输出的简单高级编辑。

    Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. These representations appear surprisingly early in the denoising process$-$well before a human can easily make sense of the noisy images. Intervention experiments further indicate these representations play a causal role in image synthesis, and may be used for simple high-level editing of an LDM's output.
    
[^60]: 小量子态的可解释表示学习

    Explainable Representation Learning of Small Quantum States. (arXiv:2306.05694v1 [quant-ph])

    [http://arxiv.org/abs/2306.05694](http://arxiv.org/abs/2306.05694)

    该论文探讨了一种无监督机器学习模型在小量子态上的可解释表示学习方法，得到了一个与潜在结构相联系的可理解表示。

    

    无监督的机器学习模型能够在没有明确人类指导或特征工程的情况下建立起对训练数据的内部表示。通过这种学习到的表示，我们可以进一步了解到关于处理任务所需的数据特征信息。在量子物理学的背景下，训练模型以描述量子态又未经人类干预地学习信息是获得深入了解机器如何呈现复杂量子态的重要途径。解释学习到的表示将为非平凡的量子系统及其有效表示带来新的视角。我们针对由参数化量子电路生成的两量子比特密度矩阵训练了一个生成模型。通过一系列计算实验，我们调查了该模型所学习到的表示以及其内部对数据信息的理解。我们观察到模型学习了一种可解释的表示方式，并将量子态与其潜在结构相联系。

    Unsupervised machine learning models build an internal representation of their training data without the need for explicit human guidance or feature engineering. This learned representation provides insights into which features of the data are relevant for the task at hand. In the context of quantum physics, training models to describe quantum states without human intervention offers a promising approach to gaining insight into how machines represent complex quantum states. The ability to interpret the learned representation may offer a new perspective on non-trivial features of quantum systems and their efficient representation. We train a generative model on two-qubit density matrices generated by a parameterized quantum circuit. In a series of computational experiments, we investigate the learned representation of the model and its internal understanding of the data. We observe that the model learns an interpretable representation which relates the quantum states to their underlying
    
[^61]: 基于语言模型的“考官”对基础模型进行基准测试

    Benchmarking Foundation Models with Language-Model-as-an-Examiner. (arXiv:2306.04181v1 [cs.CL])

    [http://arxiv.org/abs/2306.04181](http://arxiv.org/abs/2306.04181)

    本文提出了一种新的基准测试框架，使用语言模型作为考官，可以无参考方式评估答案。这个框架解决了过去基准测试流程中的测试泄漏和评估自动化问题，并允许易于扩展，可以采用不同的语言模型作为考官。

    

    已经建立了许多基准测试来评估基础模型在开放式问答方面的表现，它是测试模型理解和生成语言的能力的全面测试。大多数工作集中在提出新的数据集，然而，我们在之前的基准测试流程中看到了两个主要问题，即测试泄漏和评估自动化。在本文中，我们提出了一种新的基准测试框架，语言模型作为考官（LMAE），其中LM作为知识渊博的考官，根据其知识制定问题并以无参考方式评估答案。我们的框架允许易于扩展，因为可以采用各种LM作为考官，并且可以不断更新问题，给予更多样化的触发主题。为了更全面和公正地评估，我们设计了三个策略：（1）我们指示LM考官在许多领域生成问题。

    Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains 
    
[^62]: 在联邦学习中使用预训练模型引导最后一层

    Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])

    [http://arxiv.org/abs/2306.03937](http://arxiv.org/abs/2306.03937)

    本文研究了在联邦学习中使用预训练模型引导最后一层的问题，提出了使用最近类均值(NCM)精确且高效地拟合分类器的方法，并取得了很好的效果。

    

    联邦学习(Fl)是一种新兴的范式，允许在不共享数据的情况下跨多个参与者训练模型。最近的一些工作开始考虑使用预训练模型作为现有FL算法的初始化点的影响; 但是，这些方法忽略了集中式学习设置中大量有效的迁移学习文献。在这里，我们重新审视了先前工作中考虑预训练模型的FL问题，并将其扩展到一组计算机视觉迁移学习问题。我们首先观察到，在许多情况下，仅拟合线性分类头是高效且有效的。然后，我们展示了在FL设置中，使用最近类均值(NCM)拟合分类器可以精确地且比现有提议高效地完成，同时获得了强大的性能。最后，我们证明了使用两阶段方法获得分类器，然后微调模型可以产生更好的结果。

    Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel
    
[^63]: 待售：基于状态-动作表示学习的深度强化学习

    For SALE: State-Action Representation Learning for Deep Reinforcement Learning. (arXiv:2306.02451v1 [cs.LG])

    [http://arxiv.org/abs/2306.02451](http://arxiv.org/abs/2306.02451)

    SALE是一种基于状态-动作表示学习的新方法，可以有效地从低级状态中实现表示学习，TD7算法引入了该方法并在连续控制任务中表现优异。

    

    在强化学习领域中，表示学习是处理复杂基于图像任务的有效工具，但通常被忽略了低级状态（例如物理控制问题）的环境。本文介绍了一种名为SALE的新方法，它可以学习嵌入来建模状态和动作之间微妙的相互作用，从低级状态中实现有效的表示学习。我们广泛研究了这些嵌入的设计空间，并强调了重要的设计考虑因素。我们将SALE和RL的检查点自适应方法整合到TD3中，形成TD7算法，该算法在连续控制任务中的表现明显优于现有算法。在OpenAI gym基准任务中，TD7在300k和5M时间步骤下的平均性能增益分别为276.7％和50.7％，可以在在线和离线设置中使用。

    In the field of reinforcement learning (RL), representation learning is a proven tool for complex image-based tasks, but is often overlooked for environments with low-level states, such as physical control problems. This paper introduces SALE, a novel approach for learning embeddings that model the nuanced interaction between state and action, enabling effective representation learning from low-level states. We extensively study the design space of these embeddings and highlight important design considerations. We integrate SALE and an adaptation of checkpoints for RL into TD3 to form the TD7 algorithm, which significantly outperforms existing continuous control algorithms. On OpenAI gym benchmark tasks, TD7 has an average performance gain of 276.7% and 50.7% over TD3 at 300k and 5M time steps, respectively, and works in both the online and offline settings.
    
[^64]: GAD-NR: 通过邻域重构实现图形异常检测

    GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.01951](http://arxiv.org/abs/2306.01951)

    本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。

    

    图形异常检测（GAD）是一种技术，用于识别图中的异常节点，在网络安全、欺诈检测、社交媒体垃圾检测和其他各种领域中有应用。GAD的常见方法是图自编码器（GAEs），它将图形数据编码成节点表示，并根据这些表示来评估图形的重构质量，以识别异常。然而，现有的GAE模型主要针对直接链接重构进行优化，导致在潜在空间中连接图中的节点被聚类。因此，它们擅长检测聚类型结构异常，但对不符合聚类的更复杂的结构异常存在困难。为了解决这个限制，我们提出了一种新颖的解决方案，称为GAD-NR，它是GAE的一个新变体，融合邻域重构进行图形异常检测。GAD-NR的目标是重构节点的整个邻域，涵盖本地结构

    Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
    
[^65]: 通过交互评估数学语言模型

    Evaluating Language Models for Mathematics through Interactions. (arXiv:2306.01694v1 [cs.LG])

    [http://arxiv.org/abs/2306.01694](http://arxiv.org/abs/2306.01694)

    本文介绍了一个可进行人机交互评估LLMs的原型平台CheckMate，并利用该平台评估了三个语言模型在大学级数学证明助手方面的能力，发布了结果数据集MathConverse，并得出了人类行为的初步分类和LMM生成正确性与感知帮助性分歧的发现。

    

    传统的基于静态输入和输出对大型语言模型（LLMs）进行评估的方法不足以开发助手：这种评估未能考虑部署中的基本交互性质，因此限制了我们对语言模型能力的理解。我们引入了CheckMate，这是一个适应性的人机交互原型平台，用于评估LLMs。我们利用CheckMate对三个语言模型（InstructGPT、ChatGPT和GPT-4）进行了一项研究，评估它们作为大学级数学证明助手的能力，参与者混合了从本科生到数学教授的各层次。我们发布了结果交互和评分数据集MathConverse。通过对MathConverse的分析，我们得出了人类行为的初步分类，并发现尽管普遍存在积极相关性，但在LLM生成的正确性和感知帮助性之间存在显著的分歧。

    The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generati
    
[^66]: 基于GNN和核均值嵌入的原子模拟传递学习

    Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])

    [http://arxiv.org/abs/2306.01589](http://arxiv.org/abs/2306.01589)

    本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。

    

    使用机器学习方法学习的原子相互作用势在原子模拟中得到了成功的应用。然而，深度学习管道需要大量数据，而生成参考计算是计算上要求很高的。为了克服这一困难，我们提出了一种传递学习算法，利用了图神经网络（GNNs）在描述化学环境方面的能力，以及核均值嵌入。我们从预先在OC20数据集上进行过训练的GNN中提取特征映射，并使用它来从催化过程的系统特定数据集中学习势能表面。我们的方法进一步通过灵活的核函数来增强，该核函数包括化学物种信息，从而提高了性能和可解释性。我们在一系列逐渐复杂的现实数据集上测试了我们的方法，展示了出色的概括能力和可转移性能，改进了依赖GNNs或岭回归方法的方法。

    Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
    
[^67]: DiffLoad:扩散模型中的负荷预测不确定性量化

    DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])

    [http://arxiv.org/abs/2306.01001](http://arxiv.org/abs/2306.01001)

    本文提出了一种扩散模型中的负荷预测不确定性量化方法，采用Seq2Seq网络结构来分离两种类型的不确定性并处理异常情况，不仅着眼于预测条件期望值。

    

    电力负荷预测对电力系统的决策制定，如机组投入和能源管理等具有重要意义。近年来，各种基于自监督神经网络的方法已经被应用于电力负荷预测，以提高预测准确性和捕捉不确定性。然而，大多数现有的方法是基于高斯似然方法的，它旨在在给定的协变量下准确估计分布期望值。这种方法很难适应存在分布偏移和异常值的时间数据。在本文中，我们提出了一种基于扩散的Seq2seq结构来估计本体不确定性，并使用鲁棒的加性柯西分布来估计物象不确定性。我们展示了我们的方法能够分离两种类型的不确定性并处理突变情况，而不是准确预测条件期望。

    Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
    
[^68]: （正则化）Wasserstein分布式强最优模型的确切泛化保证

    Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models. (arXiv:2305.17076v1 [cs.LG])

    [http://arxiv.org/abs/2305.17076](http://arxiv.org/abs/2305.17076)

    本文表明Wasserstein分布式强鲁棒估计器的泛化保证适用于一般模型类别，不受维数灾难所困扰，甚至可以涵盖测试时的分布变化。

    

    Wasserstein分布式强鲁棒估计器已经成为面对不确定性的预测和决策的强大模型。这些估计器提供了有吸引力的泛化保证：训练分布得到的强鲁棒目标是真实风险的一个精确上界，并且高概率成立。然而，现有的保证要么受到维数灾难的困扰，要么仅限于特定的设置，或者会导致虚假的错误术语。在本文中，我们表明这些泛化保证实际上适用于一般的模型类别，不受维数灾难所困扰，甚至可以涵盖测试时的分布变化。我们还证明，这些结果可以推广到新引入的Wasserstein分布式强最优问题的正则化版本。

    Wasserstein distributionally robust estimators have emerged as powerful models for prediction and decision-making under uncertainty. These estimators provide attractive generalization guarantees: the robust objective obtained from the training distribution is an exact upper bound on the true risk with high probability. However, existing guarantees either suffer from the curse of dimensionality, are restricted to specific settings, or lead to spurious error terms. In this paper, we show that these generalization guarantees actually hold on general classes of models, do not suffer from the curse of dimensionality, and can even cover distribution shifts at testing. We also prove that these results carry over to the newly-introduced regularized versions of Wasserstein distributionally robust problems.
    
[^69]: PlaNeRF：SVD无监督三维平面正则化用于NeRF大规模场景重建。

    PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction. (arXiv:2305.16914v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16914](http://arxiv.org/abs/2305.16914)

    本文提出了一种利用SVD无监督三维平面正则化的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构，有效解决了训练视图的过拟合导致低纹理区域的几何重建不佳的问题。

    

    神经辐射场（NeRF）利用2D图像和相机姿态进行3D场景重建以进行新颖视图合成。尽管NeRF能产生逼真的结果，但它经常遭受过拟合于训练视图的困扰，导致几何重建不佳，尤其是在低纹理区域。这种限制限制了许多需要准确几何形态的重要应用，例如外推NVS，高清映射和场景编辑。为了解决这个问题，我们提出了一种新的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构。我们的方法引入了基于奇异值分解（SVD）的新颖平面正则化，不依赖于任何几何先验知识。此外，我们在我们的损失设计中利用结构相似性指数测量（SSIM）来正确初始化NeRF的体积表示。定量和定性的结果表明，我们的方法优于流行的正则化方法以实现准确的几何重建。

    Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in our loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstructi
    
[^70]: 带扰动生成树的可微聚类方法

    Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])

    [http://arxiv.org/abs/2305.16358](http://arxiv.org/abs/2305.16358)

    介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。

    

    我们介绍了一种基于最小权重生成树的可微聚类方法，它是生成树的一种变体，具有多个连通分量。我们的方法依赖于线性规划解的随机扰动，以实现平滑和高效的梯度计算。这使我们能够在端到端可训练的流水线中包含聚类。我们证明了我们的方法即使在嘈杂的数据集和具有挑战性的几何环境下也能良好地工作。我们还利用这种方法制定了一个特别的损失，以有效地从部分聚类数据学习。我们在几个现实世界的数据集上展示了它在监督和半监督任务中的表现。

    We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
    
[^71]: 一种用于确定固定置信度和以上的 $\varepsilon$-最佳臂辨识算法

    An $\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond. (arXiv:2305.16041v1 [stat.ML])

    [http://arxiv.org/abs/2305.16041](http://arxiv.org/abs/2305.16041)

    提出一种新颖的采样规则EB-TC $\varepsilon$，用于随机赌博机中的$\varepsilon$-最佳臂的辨识。该规则可用于确定固定置信度或固定预算标识且具备自适应调整勘探参数的渐近最优性。在仿真实验中表现良好，适用于不同问题领域。

    

    我们提出了一种新颖的采样规则EB-TC $\varepsilon$，该规则用于随机赌博机中的$\varepsilon$-最佳臂的辨识。这是第一种用于近似最佳臂辨识的Top Two算法分析实例。 EB-TC $\varepsilon$ 是一种“随时可用”的采样规则，因此可以在没有预算知识的情况下用于确定固定置信度或固定预算标识（无需修改）。我们为EB-TC $\varepsilon$ 提供了三种理论保证。首先，我们证明其在固定置信度设置中预期的样本复杂度上有界，特别是在其勘探参数的自适应调整与组合的情况下呈现其渐近最优性。我们通过在任何时间和对于任何误差参数的概率上界来补充这些发现，这进一步产生其任何时间的简单遗憾上界。最后，我们通过数值模拟表明，与现有算法相比，EB-TC $\varepsilon$ 的性能表现优秀，且适用于不同问题领域。

    We propose EB-TC$\varepsilon$, a novel sampling rule for $\varepsilon$-best arm identification in stochastic bandits. It is the first instance of Top Two algorithm analyzed for approximate best arm identification. EB-TC$\varepsilon$ is an *anytime* sampling rule that can therefore be employed without modification for fixed confidence or fixed budget identification (without prior knowledge of the budget). We provide three types of theoretical guarantees for EB-TC$\varepsilon$. First, we prove bounds on its expected sample complexity in the fixed confidence setting, notably showing its asymptotic optimality in combination with an adaptive tuning of its exploration parameter. We complement these findings with upper bounds on its probability of error at any time and for any error parameter, which further yield upper bounds on its simple regret at any time. Finally, we show through numerical simulations that EB-TC$\varepsilon$ performs favorably compared to existing algorithms, in different
    
[^72]: 平衡对抗模型下的自适应数据分析

    Adaptive Data Analysis in a Balanced Adversarial Model. (arXiv:2305.15452v1 [cs.LG])

    [http://arxiv.org/abs/2305.15452](http://arxiv.org/abs/2305.15452)

    本研究探究在平衡对抗模型下的自适应数据分析，在该模型下，研究者通过提供一种新的对抗模型，使得在平衡状态下，回答那些以前被认为计算上困难的自适应查询变得可行。

    

    在自适应数据分析中，机制获取n个来自未知分布D的独立同分布样本，并需要对一系列自适应选择的统计查询提供准确的估计。在一个拥有单向函数的基础上，Hardt和Ullman(FOCS 2014)以及Steinke和Ullman(COLT 2015)表明，通常情况下，回答超过Θ(n^2)个适应性查询是计算上困难的。然而，这些负面结果强烈依赖于一个对抗性模型，该模型使得对抗分析员比机制明显处于优势地位，因为选择自适应查询的分析员也选择了基础分布D。这种不平衡会对所得到的难度结果的适用性提出问题——具有基础分布D的完全知识的分析员几乎没有任何必要向仅持有有限数量D样本的机制发出统计查询。

    In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an unknown distribution $D$, and is required to provide accurate estimations to a sequence of adaptively chosen statistical queries with respect to $D$. Hardt and Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in general, it is computationally hard to answer more than $\Theta(n^2)$ adaptive queries, assuming the existence of one-way functions.  However, these negative results strongly rely on an adversarial model that significantly advantages the adversarial analyst over the mechanism, as the analyst, who chooses the adaptive queries, also chooses the underlying distribution $D$. This imbalance raises questions with respect to the applicability of the obtained hardness results -- an analyst who has complete knowledge of the underlying distribution $D$ would have little need, if at all, to issue statistical queries to a mechanism which only holds a finite number of samples from $D$.  We consider more 
    
[^73]: 从理论角度揭示“思维链”背后的奥秘

    Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])

    [http://arxiv.org/abs/2305.15408](http://arxiv.org/abs/2305.15408)

    本文从理论层面探究了带有“思维链”提示的大型语言模型在解决基本数学和决策问题中的能力，发现自回归Transformer大小恒定即可解决任务，揭示了“思维链”提示的背后机制。

    

    最近的研究发现，"思维链"提示能够显著提高大型语言模型（LLMs）的性能，特别是在涉及数学或推理的复杂任务中。尽管获得了巨大的实证成功，但“思维链”背后的机制以及它如何释放LLMs的潜力仍然是神秘的。本文首次从理论上回答了这些问题。具体而言，我们研究了LLMs带有“思维链”在解决基本数学和决策问题中的能力。我们首先给出一个不可能的结果，表明任何有限深度的Transformer都不能直接输出正确的基本算术/方程任务的答案，除非模型大小随着输入长度的增加呈超多项式增长。相反，我们通过构造证明，大小恒定的自回归Transformer足以通过使用常用的数学语言形式生成“思维链”推导来解决这两个任务。

    Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
    
[^74]: 无需Lipschitzness和Smoothness的在线投资组合选择的数据相关上界

    Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness. (arXiv:2305.13946v1 [cs.LG])

    [http://arxiv.org/abs/2305.13946](http://arxiv.org/abs/2305.13946)

    本文提出了在线投资组合选择的第一个数据相关上界，算法显示亚线性遗憾率，并在数据“容易”时实现对数遗憾。

    

    本文介绍了在线投资组合选择中的第一种小损失和平稳变化的遗憾上界，并标志着在线凸优化具有非Lipschitz、非光滑损失的数据相关上界的首次实例。我们提出的算法在最坏情况下显示出亚线性遗憾率，并在数据“容易”时实现对数遗憾，每次迭代的时间几乎是投资选择数量的线性。遗憾上界是使用对数损失的新型光滑性表征、遵循具有自共轭正则化器的正则化领袖（FTRL）的局部范数分析、它们不一定是障碍的和具有log障碍的乐观FTRL的隐式变体来推导的。

    This work introduces the first small-loss and gradual-variation regret bounds for online portfolio selection, marking the first instances of data-dependent bounds for online convex optimization with non-Lipschitz, non-smooth losses. The algorithms we propose exhibit sublinear regret rates in the worst cases and achieve logarithmic regrets when the data is "easy," with per-iteration time almost linear in the number of investment alternatives. The regret bounds are derived using novel smoothness characterizations of the logarithmic loss, a local norm-based analysis of following the regularized leader (FTRL) with self-concordant regularizers, which are not necessarily barriers, and an implicit variant of optimistic FTRL with the log-barrier.
    
[^75]: 通过RKHM和Perron-Frobenius算子的深度学习

    Deep Learning with Kernels through RKHM and the Perron-Frobenius Operator. (arXiv:2305.13588v1 [stat.ML])

    [http://arxiv.org/abs/2305.13588](http://arxiv.org/abs/2305.13588)

    该论文提出了一种基于核方法的深度学习框架：深度RKHM，通过使用$C^*$代数获得更温和的界限，并提供了良性过拟合的理论解释。

    

    重现核希尔伯特$C^*$-模(RKHM)通过$C^*$代数对重现核希尔伯特空间(RKHS)进行了泛化，而Perron-Frobenius算子是与函数组合相关的线性算子。将这两个概念结合起来，我们提出了深度RKHM，一种基于核方法的深度学习框架。我们在这个设置中推导了一个新的Rademacher广义界限，并通过Perron-Frobenius算子提供了良性过拟合的理论解释。由于$C^*$代数的优势，该界限对输出维度的依赖性较现有界限更加温和。我们展示了$C^*$代数是深度学习的核心工具，使我们能够利用算子的乘积结构，并提供与卷积神经网络的明确联系。我们的理论分析为设计和分析深度核方法提供了一个新的视角。

    Reproducing kernel Hilbert $C^*$-module (RKHM) is a generalization of reproducing kernel Hilbert space (RKHS) by means of $C^*$-algebra, and the Perron-Frobenius operator is a linear operator related to the composition of functions. Combining these two concepts, we present deep RKHM, a deep learning framework for kernel methods. We derive a new Rademacher generalization bound in this setting and provide a theoretical interpretation of benign overfitting by means of Perron-Frobenius operators. By virtue of $C^*$-algebra, the dependency of the bound on output dimension is milder than existing bounds. We show that $C^*$-algebra is a suitable tool for deep learning with kernels, enabling us to take advantage of the product structure of operators and to provide a clear connection with convolutional neural networks. Our theoretical analysis provides a new lens through which one can design and analyze deep kernel methods.
    
[^76]: NTK逼近有效的紧凑条件。

    Tight conditions for when the NTK approximation is valid. (arXiv:2305.13141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13141](http://arxiv.org/abs/2305.13141)

    我们研究了NTK逼近在平方损失下训练模型的有效性，发现在模型缩放因子$\alpha=O(T)$时在训练时间$T$之前可以使得NTK逼近有效。

    

    我们研究了神经切线核（NTK）逼近在使用平方损失函数训练模型时何时有效。在Chizat等人2019年的懒惰训练设置下，我们证明通过因子为$\alpha=O(T)$的模型缩放就足以使NTK逼近在训练时间$T$之前有效。我们的界限是紧凑的，并且改善了Chizat等人2019年的先前界限，后者需要更大的缩放因子$\alpha=O(T^2)$。

    We study when the neural tangent kernel (NTK) approximation is valid for training a model with the square loss. In the lazy training setting of Chizat et al. 2019, we show that rescaling the model by a factor of $\alpha = O(T)$ suffices for the NTK approximation to be valid until training time $T$. Our bound is tight and improves on the previous bound of Chizat et al. 2019, which required a larger rescaling factor of $\alpha = O(T^2)$.
    
[^77]: DreamWaltz：使用复杂3D动画角色制作场景

    DreamWaltz: Make a Scene with Complex 3D Animatable Avatars. (arXiv:2305.12529v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.12529](http://arxiv.org/abs/2305.12529)

    DreamWaltz是一个新颖的框架，可以根据文本指导和参数化的人体先验生成和动画化复杂的3D角色。它提出了三维一致的遮挡感知得分蒸馏采样（SDS）来优化隐式神经表示和规范姿势，并学习了一种可动画且具有泛化能力的角色表示。这种方法在创建3D角色方面表现出了高效性和稳健性。

    

    我们提出了DreamWaltz，这是一个新颖的框架，可以根据文本指导和参数化的人体先验生成和动画化复杂的3D角色。尽管最近的方法在将文本转化为普通物体的3D生成方面取得了令人鼓舞的结果，但创建高质量且可动画的3D角色仍然具有挑战性。为了创建高质量的3D角色，DreamWaltz提出了三维一致的遮挡感知得分蒸馏采样（SDS）来优化隐式神经表示和规范姿势。它通过三维感知的骨架调节提供视角对齐的监督，使得复杂的角色生成不会产生伪影和多个面。对于动画，我们的方法学习了一种可动画且具有泛化能力的角色表示，可以将任意姿势映射到规范姿势表示中。广泛的评估表明，DreamWaltz是一种有效且稳健的方法，可以创建具有复杂形状和外观以及动画的3D角色。

    We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable and generalizable avatar representation which could map arbitrary poses to the canonical pose representation. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The 
    
[^78]: 通过图形段训练学习大型图形属性预测

    Learning Large Graph Property Prediction via Graph Segment Training. (arXiv:2305.12322v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12322](http://arxiv.org/abs/2305.12322)

    本文提出了一种名为Graph Segment Training的新方法，通过分治法允许使用恒定的内存消耗来学习大型图形属性预测。该方法被评估在几项大型图形属性预测任务上，表现出优于几个最先进基准的出色性能。

    

    学习预测大型图的属性具有挑战性，因为每个预测都需要整个图的知识，而在训练期间可用的内存量是有限的。在这里，我们提出了一种名为Graph Segment Training（GST）的通用框架，利用分治方法允许使用恒定的内存占用量来学习大型图形属性预测。 GST首先将大型图形划分为段，然后通过训练迭代中仅对几个段进行反向传播。我们通过引入历史嵌入表来改进GST范例，以有效地获取未进行反向传播的段的嵌入。为了减轻历史嵌入的过时性，我们设计了两种新技术。首先，我们微调预测头以修复输入分布移位。其次，我们引入“Stale Embedding Dropout”来在训练期间降低偏差，从而丢弃一些过时的嵌入。我们对大型图形属性预测任务进行了评估，包括化合物分类和蛋白质相互作用预测。我们提出的方法GST-EFD（包含所有技术）优于几个最先进的基准，并在准确性，存储器消耗和运行时效率方面实现了出色的性能。

    Learning to predict properties of large graphs is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST-EFD (with all the techniques 
    
[^79]: 动量匹配去噪Gibbs采样

    Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])

    [http://arxiv.org/abs/2305.11650](http://arxiv.org/abs/2305.11650)

    本文提出了动量匹配去噪Gibbs采样方法，可以在给定‘嘈杂’的模型的情况下，从干净的模型中有效地进行采样。

    

    能量基模型（EBMs）为建模复杂数据分布提供了一个通用的框架。然而，EBMs 的训练和采样仍然面临重大挑战。用于可扩展 EBM 训练的广泛使用的去噪分数匹配（DSM）方法存在不一致性问题，导致能量模型学习到“嘈杂”的数据分布。在本文中，我们提出了一种有效的采样框架：（伪）Gibbs采样与动量匹配，可以在给定经过DSM训练良好的“嘈杂”模型的情况下，从基础“干净”模型中有效地进行采样。我们探讨了我们的方法相对于相关方法的优势，并展示了如何将该方法扩展到高维数据集。

    Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
    
[^80]: NODE-ImgNet: 一种使用 PDE 的有效和稳健的图像去噪模型

    NODE-ImgNet: a PDE-informed effective and robust model for image denoising. (arXiv:2305.11049v1 [eess.IV])

    [http://arxiv.org/abs/2305.11049](http://arxiv.org/abs/2305.11049)

    本文提出了一种新型的神经网络架构 NODE-ImgNet，该模型结合了神经常微分方程和卷积神经网络块，它是一种使用 PDE 的有效和稳健的图像去噪模型。该模型能够自然地避免学习过程中引入伪像，实现了更高准确性和参数效率，并在不同场景下展示了一致的有效性和优势。

    

    受传统偏微分方程方法的启发，我们提出了一种新颖的神经网络体系框架 NODE-ImgNet，将神经常微分方程（NODEs）与卷积神经网络（CNN）块相结合。 NODE-ImgNet 是本质上的 PDE 模型，动态系统是隐式学习的，而不是显式指定 PDE，自然地避免了学习过程中引入伪像的典型问题。通过调用这种 NODE 结构，可以将其视为残差网络（ResNet）的连续变体，继承其在图像去噪中的优点，我们的模型实现了增强的准确性和参数效率。特别是，在不同场景下，包括受高斯噪声干扰的灰度和彩色图像去噪，以及真实噪声图像学习方面，我们的模型展示了一致的有效性，并展示了从小图像数据集中学习的优势。

    Inspired by the traditional partial differential equation (PDE) approach for image denoising, we propose a novel neural network architecture, referred as NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE model, where the dynamic system is learned implicitly without the explicit specification of the PDE. This naturally circumvents the typical issues associated with introducing artifacts during the learning process. By invoking such a NODE structure, which can also be viewed as a continuous variant of a residual network (ResNet) and inherits its advantage in image denoising, our model achieves enhanced accuracy and parameter efficiency. In particular, our model exhibits consistent effectiveness in different scenarios, including denoising gray and color images perturbed by Gaussian noise, as well as real-noisy images, and demonstrates superiority in learning from small image datasets
    
[^81]: 基于Segment Anything Model (SAM)增强伪标签的弱监督语义分割

    Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])

    [http://arxiv.org/abs/2305.05803](http://arxiv.org/abs/2305.05803)

    本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。

    

    仅使用图像级别的监督的弱监督语义分割(WSSS)由于其与像素级注释相比的低成本而越来越受到关注。大多数现有方法依赖于类激活图(CAM)生成像素级的伪标签进行监督训练。然而，众所周知，CAM经常遭受局部激活的限制-只激活最具区分性的部分，而不是整个对象区域和虚假的激活-不必要地激活物体周围的背景。本研究引入了一种简单而有效的方法来解决这些限制，即利用最近发布的Segment Anything Model (SAM)增强CAM生成高质量的伪标签。SAM是一个分割基础模型，展示了将图像分割成段落的强零-shot能力，但缺乏这些区域的语义标签。为了解决这个问题，我们采用特定类别的伪标签作为信号来选择m。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
    
[^82]: 生成式检索推荐系统

    Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])

    [http://arxiv.org/abs/2305.05065](http://arxiv.org/abs/2305.05065)

    本文提出了一种新型的生成式检索模型，将检索和生成组合在一起以产生推荐。

    

    现代推荐系统使用大规模检索模型进行推荐，包括两个阶段：训练双编码模型将查询和候选项嵌入到相同的空间中，然后使用近似最近邻搜索来选择给定查询嵌入的顶部候选项。本文提出了一种新的单阶段范例：生成式检索模型，该模型通过自回归方式在一个阶段中解码目标候选项的标识符。为此，我们不是为每个项目分配随机生成的原子ID，而是生成语义ID：每个项目的语义有意义的元组编码词，它作为其唯一标识符。我们使用称为RQ-VAE的分层方法生成这些编码词。一旦我们对所有项目都有了语义ID，就会训练基于Transformer的序列到序列模型来预测下一个项目的语义ID。由于这个模型以自回归的方式直接预测标识下一个项的编码词元组，因此它可以将检索和生成组合在一起以产生推荐。

    Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
    
[^83]: 学习6D非抓取式操作的混合演员-评论员地图

    Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])

    [http://arxiv.org/abs/2305.03942](http://arxiv.org/abs/2305.03942)

    论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。

    

    在人类的灵巧性中，非抓取式操作是操作物体的重要组成部分。非抓取式操纵可以使与物体的交互更加复杂，但也在推理交互方面提出了挑战。在本文中，我们引入了一个名为HACMan的混合演员评论员地图，这是一种使用点云观察的6D非抓取式物体操作的强化学习方法。HACMan提出了一种时间抽象和空间基础的物体中心动作表示，该表示包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。我们修改了一个现有的离线策略RL算法，以在这种混合的离散-连续动作表示学习。我们在仿真和现实世界中对HACMan进行了6D物体姿态对齐任务的评估。在最难的任务版本中，通过随机初始化物体和机器人配置，HACMan的表现优于现有的基线方法。

    Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
    
[^84]: 使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐：一项比较研究

    Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])

    [http://arxiv.org/abs/2305.03017](http://arxiv.org/abs/2305.03017)

    本研究使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐的质量，重点关注于Stack Overflow上的Java编程语言。研究使用BERT将代码示例转换为数值向量。

    

    过去和最近一直在进行代码示例推荐的研究，以帮助开发人员完成软件开发任务。由于开发人员经常花费大量时间在互联网上寻找相关的代码示例，利用开源项目和非正式文档。为了找到有用的代码示例，非正式文档（如Stack Overflow讨论和论坛）可以非常宝贵。我们的研究重点是Stack Overflow，它是软件开发人员讨论不同主题的流行资源。为了提高推荐代码示例的质量，我们收集并推荐了Java编程语言中最佳的代码示例。我们采用了BERT来进行处理，它是一个大型语言模型（LLM），可以有效地从文本数据中提取语义信息。我们的第一步是使用BERT将代码示例转换为数值向量。

    The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
    
[^85]: 用Transformer逼近CKY算法

    Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])

    [http://arxiv.org/abs/2305.02386](http://arxiv.org/abs/2305.02386)

    本文研究了Transformer模型逼近CKY算法的能力，提出了一种用梯度预测解析的方法，在标准基准测试中表现竞争力更好，同时速度更快。在随机PCFG下解析时，性能下降，但加入额外的归纳偏差是有帮助的。

    

    本文研究了Transformer模型逼近CKY算法的能力，直接预测句子的解析，避免了CKY算法对句子长度的三次依赖。在标准的组成句分析基准测试中，我们发现这种方法比使用CKY的可比分析器取得了竞争或更好的性能，同时速度更快。我们还评估了在随机PCFG下进行解析的可行性。在这里，我们发现在语法变得更加模糊的情况下，性能下降，这表明Transformer没有完全捕捉到CKY计算。然而，我们也发现，结合额外的归纳偏差是有帮助的，并提出了一种新方法，利用相对于图表表示的梯度来预测解析，类比于CKY算法与图表相关的一个分区函数变体的子梯度。

    We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a parse and thus avoid the CKY algorithm's cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or better performance than comparable parsers that make use of CKY, while being faster. We also evaluate the viability of this approach for parsing under random PCFGs. Here we find that performance declines as the grammar becomes more ambiguous, suggesting that the transformer is not fully capturing the CKY computation. However, we also find that incorporating additional inductive bias is helpful, and we propose a novel approach that makes use of gradients with respect to chart representations in predicting the parse, in analogy with the CKY algorithm being the subgradient of a partition function variant with respect to the chart.
    
[^86]: 校准化解释：基于不确定性信息和反事实的解释模型

    Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])

    [http://arxiv.org/abs/2305.02305](http://arxiv.org/abs/2305.02305)

    该论文提出了一种新的特征重要性解释方法，Calibrated Explanations (CE)，它可以提供准确、稳定的解释，并且可以为概率估计和特征重要性权重提供不确定性量化信息，是一种快速、可靠且强健的解释方法。

    

    人工智能已经成为各种领域决策支持系统中不可或缺的一部分，但人工智能决策系统中预测模型缺乏透明度可能导致滥用或不使用。可解释人工智能旨在创建可以向人类用户解释其推理过程的人工智能系统。可解释人工智能中的局部解释可以提供关于特征重要性的个别预测原因的信息，但存在不稳定性等缺点。为了解决这些问题，我们提出了一种新的特征重要性解释方法，校准化解释(Calibrated Explanations，CE)，它基于 Venn-Abers，同时在生成特征重要性解释的同时校准底层模型。CE不仅提供快速、可靠、稳定和强健的解释，还提供概率估计和特征重要性权重的不确定性量化。此外，该方法是模型无关的，具有易于理解的条件规则，也可以生成反事实推理。

    Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
    
[^87]: ZRG: 一个用于机器学习的高分辨率住宅屋顶三维几何数据集

    ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning. (arXiv:2304.13219v1 [cs.CV])

    [http://arxiv.org/abs/2304.13219](http://arxiv.org/abs/2304.13219)

    该论文介绍了 ZRG 数据集，其中包含成千上万个样本的高分辨率住宅屋顶正交图像拼接、对应 DSM、3D 屋顶框架和多视图图像生成的点云。该数据集可以用于住宅屋顶结构的几何建模和场景理解，如屋顶轮廓提取、单目高度估计和平面屋顶结构提取等任务。

    

    本文介绍了 Zeitview Rooftop Geometry (ZRG) 数据集，其中包含成千上万个高分辨率住宅屋顶正交图像拼接、对应数字表面模型 (DSM)、3D 屋顶框架和多视图图像生成的点云用于住宅屋顶几何和场景理解。我们进行了详细的基准测试，展示了此数据集解锁的众多应用，并为屋顶轮廓提取、单目高度估计和平面屋顶结构提取等任务提供了基线。

    In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset. ZRG contains thousands of samples of high resolution orthomosaics of aerial imagery of residential rooftops with corresponding digital surface models (DSM), 3D rooftop wireframes, and multiview imagery generated point clouds for the purpose of residential rooftop geometry and scene understanding. We perform thorough benchmarks to illustrate the numerous applications unlocked by this dataset and provide baselines for the tasks of roof outline extraction, monocular height estimation, and planar roof structure extraction.
    
[^88]: 对高水平机器人解释中的奖励分解进行更深入的研究

    A Closer Look at Reward Decomposition for High-Level Robotic Explanations. (arXiv:2304.12958v1 [cs.LG])

    [http://arxiv.org/abs/2304.12958](http://arxiv.org/abs/2304.12958)

    本论文提出了一种将奖励分解与抽象动作空间相结合的学习框架，可以提供基于对象属性的明确高层次解释，避免了解释机器人行为的复杂性。

    

    向人类解释智能机器人的行为是具有挑战性的，这是由于它们难以理解的自体感状态、多变的中间目标和其结果的不可预测性。此外，对强化学习代理的一步解释可能是模糊的，因为它们未能在每个转换时考虑到代理的未来行为，从而增加了解释机器人行为的复杂性。通过利用映射到任务特定基元的抽象动作，我们避免了在移动层面上的解释。我们提出的框架将奖励分解（RD）与抽象动作空间结合起来，构建了一个可解释的学习框架，基于任务中的对象属性，实现了明确的高层次解释。我们通过对两个机器人场景的定量和定性分析来证明了我们框架的有效性，展示了RD解释的输出文物中的可视和文本解释，易于人类理解。

    Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. 
    
[^89]: 用有效的视野连接强化学习理论和实践

    Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])

    [http://arxiv.org/abs/2304.09853](http://arxiv.org/abs/2304.09853)

    本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。

    

    深度强化学习在某些环境中表现出色，但在其他环境中却失败得非常严重。理想情况下，强化学习理论应该能够解释这种现象，提供预测实际性能的界限。不幸的是，当前的理论还没有这种能力。本文通过引入包含155个MDP的新数据集BRIDGE，将标准的深度强化学习算法与之前的样本复杂度先前界进行比较，并发现了一个意想不到的性质：当最高Q值的动作在随机策略下的Q值也是最高的时，深度强化学习往往会成功；反之，失败的可能性较高。基于这一性质，我们将其概括为一个新的MDP复杂度度量，称为有效的视野。

    Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
    
[^90]: 带有自我改进硬约束的多能源管理系统的安全强化学习

    Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])

    [http://arxiv.org/abs/2304.08897](http://arxiv.org/abs/2304.08897)

    本论文提出了一种安全强化学习方法，能够实现多能源管理系统中的最优控制，在保证硬约束的前提下减少工程工作，降低建模偏差，并避免潜在的不安全行为。

    

    带有硬约束保证的安全强化学习是多能源管理系统中最有前途的最优控制方向。它只需要在环境特定的约束函数本身上预先而不是完整的模型（即植物，干扰和噪声模型，以及未包括在植物模型中的状态的预测模型 - 例如需求，天气和价格预测）。因此，可减少项目特定的前期和持续的工程工作，仍可以学习更好地表示基础系统动态，并使建模偏差最小化（无基于模型的目标函数）。然而，即使仅约束函数本身有时也不总是容易提供准确的先验（例如能量平衡约束需要详细确定所有能量输入和输出），从而导致潜在的不安全行为。在本文中，我们提出了两个新的进展：（I）将Optlayer和SafeFallback方法结合起来，命名为O

    Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
    
[^91]: 模型特定视角下的统一外部分布检测

    Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v1 [cs.LG])

    [http://arxiv.org/abs/2304.06813](http://arxiv.org/abs/2304.06813)

    本文提出一种新颖的统一框架，用于将机器学习模型中的外部分布检测扩展到更广泛的范围，该框架旨在检测模型无法正确预测的测试示例，而不是特定的外部分布原因。

    

    外部分布检测旨在识别不属于训练分布并不可靠预测的测试样例。虽然已有大量相关工作，但其中大多数只关注来自语义转换（如未见过的类别）的OOD例子，而忽略了其他可能的原因（如协变量转换）。本文提出了一种新颖的统一框架，以更广泛的范围研究OOD检测。我们建议不是检测特定原因导致的OOD例子，而是检测已部署机器学习模型（例如图像分类器）无法正确预测的例子。也就是说，是否应该检测和拒绝测试例子是“模型特定”的。我们展示了该框架统一了由语义变化和协变量变化引起的OOD例子的检测，并密切关注将机器学习模型应用于不受控制的环境的问题。我们对该框架进行了广泛的分析和实验。

    Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is ``model-specific''. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that 
    
[^92]: 基于文本提示学习和双向训练的组成图像检索方法

    Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v1 [cs.CV])

    [http://arxiv.org/abs/2303.16604](http://arxiv.org/abs/2303.16604)

    本文提出了一种基于文本提示学习和双向训练的组成图像检索方法，可以应用于现有的体系结构，并且在修改文本存在噪声或歧义的情况下特别有效。

    

    组成图像检索是根据包含参考图像和描述所需更改的修改文本的多模态用户查询来搜索目标图像的方法。现有的解决这个具有挑战性的任务的方法学习从（参考图像，修改文本）对到图像嵌入的映射，然后将其与大型图像语料库进行匹配。本文提出了一种双向训练方案，利用了这种反向查询，并可应用于现有的组成图像检索体系结构。为了编码双向查询，我们在修改文本前面添加一个可学习的令牌，指定查询的方向，然后微调文本嵌入模块的参数。我们没有对网络架构进行其他更改。在两个标准数据集上的实验表明，双向训练在提高组成图像检索性能方面是有效的，特别是在修改文本存在噪声或歧义的情况下。

    Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datas
    
[^93]: 内容不变性对流形核回归的精确样本复杂度增益

    The Exact Sample Complexity Gain from Invariances for Kernel Regression on Manifolds. (arXiv:2303.14269v1 [cs.LG])

    [http://arxiv.org/abs/2303.14269](http://arxiv.org/abs/2303.14269)

    本文提供了在任何流形上，对于一个在流形上任意群作用下不变的目标函数，核岭回归的最小化最优率，从而增加了有效样本数量或降低了流形的维数。

    

    在实践中，将内容不变性编码进模型可以提高样本复杂度。本文对内容不变性如何改善样本复杂度的理论结果进行了细化和推广，特别地，在任何流形上，对于一个在流形上任意群作用下不变的目标函数，我们提供了核岭回归的最小化最优率。我们的结果适用于（几乎）任何群作用，甚至是正维度的群。对于有限群，增益通过将“有效”样本数量扩大到群的大小来实现。对于正维度的群，增益表现为降低流形的维数，同时还与商空间体积成比例。我们的证明从微分几何的角度来看，与使用不变多项式的更常见策略不同。因此，这个在具有不变性的学习中的新几何视角可能具有独立的兴趣。

    In practice, encoding invariances into models helps sample complexity. In this work, we tighten and generalize theoretical results on how invariances improve sample complexity. In particular, we provide minimax optimal rates for kernel ridge regression on any manifold, with a target function that is invariant to an arbitrary group action on the manifold. Our results hold for (almost) any group action, even groups of positive dimension. For a finite group, the gain increases the "effective" number of samples by the group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes the viewpoint of differential geometry, in contrast to the more common strategy of using invariant polynomials. Hence, this new geometric viewpoint on learning with invariances may be of independent interest.
    
[^94]: BERT是否盲目？探索视觉语言预训练对视觉语言理解的影响。

    Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v1 [cs.CV])

    [http://arxiv.org/abs/2303.12513](http://arxiv.org/abs/2303.12513)

    本文调查了视觉语言预训练对仅文本任务的表现是否有提高。作者提出了一套视觉语言理解任务，证明了多模态训练的文本编码器在视觉推理方面的优越性。

    

    大多数人使用视觉想象来理解和推理语言，但是像BERT这样的模型使用在仅包括文本的预训练过程中获取的知识来推理语言。在本文中，我们调查了视觉语言预训练是否可以提高在涉及隐含视觉推理的仅文本任务上的表现，重点是零样本探测方法。我们提出了一套用于探测文本编码器模型视觉推理能力的视觉语言理解（VLU）任务，以及各种非视觉自然语言理解（NLU）任务用于比较。我们还贡献了一种新型的零样本知识探测方法，Stroop probing，用于将像CLIP这样的模型应用于仅文本任务，而不需要像BERT模型的掩码语言建模头那样的预测头。我们证明了SOTA多模态训练的文本编码器在VLU任务上优于单模态训练的文本编码器，但在NLU任务上不及它们。

    Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being underperformed by them on the NLU tasks, lendin
    
[^95]: 随机插值：流和扩散的统一框架

    Stochastic Interpolants: A Unifying Framework for Flows and Diffusions. (arXiv:2303.08797v1 [cs.LG])

    [http://arxiv.org/abs/2303.08797](http://arxiv.org/abs/2303.08797)

    本文提出了一种统一的生成模型，该模型基于随机插值框架，可以实现流和扩散方法的统一。作者构建了一类广泛的连续时间随机过程，用于将两个任意的密度在有限时间内精确地连接。这种方法可以用于基于概率微分方程的确定性和随机生成模型的构建。

    

    我们介绍了一类建立在随机插值框架上的生成模型，该框架是基于Albergo＆Vanden-Eijnden（2023）提出的，在流和扩散方法上实现统一，我们首先展示了如何构建一类广泛的连续时间随机过程，其时间依赖的概率密度函数在有限时间内精确地连接两个任意的密度。这些“随机插值器”是通过将来自两个密度的数据与其他潜在变量相结合构建的，并且构造的具体细节可以灵活地塑造导致的时间依赖密度。然后我们展示了随机插值器的时间依赖密度满足一阶输运方程以及一系列具有可调扩散的正向和反向Fokker-Planck方程族; 在考虑单个样本的时间演化时，这个观点立即导致了基于概率微分方程的确定性和随机生成模型。

    We introduce a class of generative models based on the stochastic interpolant framework proposed in Albergo & Vanden-Eijnden (2023) that unifies flow-based and diffusion-based methods. We first show how to construct a broad class of continuous-time stochastic processes whose time-dependent probability density function bridges two arbitrary densities exactly in finite time. These `stochastic interpolants' are built by combining data from the two densities with an additional latent variable, and the specific details of the construction can be leveraged to shape the resulting time-dependent density in a flexible way. We then show that the time-dependent density of the stochastic interpolant satisfies a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion; upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on proba
    
[^96]: 带吸收的泛洪：复杂网络上异构赌博机的高效协议

    Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks. (arXiv:2303.05445v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05445](http://arxiv.org/abs/2303.05445)

    该论文提出了一种名为带吸收的泛洪（FwA）的新协议，用于解决复杂网络上的异构赌博机问题。通过严格的遗憾分析，证明了该协议的有效性。

    

    多臂赌博机广泛用于建模顺序决策，在许多现实应用中如在线推荐系统和无线网络中无处不在。我们考虑一个多代理的场景，每个代理解决自己的赌博机问题，赌博机拥有不同的臂。他们的目标是在通过给定网络的通信协议协作的同时最小化他们的集体遗憾。先前关于此问题的文献只考虑了臂的异质性和网络化代理问题。在这项工作中，我们引入了一个同时包含这两个特性的设置。针对这一新颖的设置，我们首先对标准泛洪协议结合经典的上置信界策略提供了严格的遗憾分析。然后，为了减轻在复杂网络中泛洪造成的高通信成本问题，我们提出了一种新的协议，称为带吸收的泛洪（FwA）。我们对由此产生的遗憾上界进行了理论分析，并讨论了该协议的优点。

    Multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. We consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. Their goal is to minimize their group regret while collaborating via some communication protocol over a given network. Previous literature on this problem only considered arm heterogeneity and networked agents separately. In this work, we introduce a setting that encompasses both features. For this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic UCB policy. Then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called Flooding with Absorption (FwA). We provide a theoretical analysis of the resulting regret bound and discuss the advantages of
    
[^97]: 多级扩散：图像生成的无限维度基于得分的扩散模型

    Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04772](http://arxiv.org/abs/2303.04772)

    本文介绍了无限维度得分扩散模型在多个分辨率水平上的离散化方法，并使用多级扩散算法在多个分辨率上高效地学习。实证表明，该模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本，并可以生成不同分辨率的图像并处理矩形域。

    

    基于得分的扩散模型是近年来图像生成的最先进方法之一。现有的基于得分的扩散模型通常在有限维度设置中表述，其中图像被视为具有有限尺寸的张量。本文在无限维度设置中开发了基于得分的扩散模型，即我们将训练数据建模为支撑在矩形域上的函数。除了追求在更高分辨率下生成图像之外，我们的主要动机是创建一个良好定义的无限维度学习问题，以便可以在多个分辨率水平上一致地离散化它。我们希望获得能够横跨不同分辨率级别的扩散模型，并提高训练过程的效率。我们展示了如何克服当前基于得分的扩散模型在无限维度设置中存在的两个缺点。首先，我们修改了前向过程以确保在无限维度设置中潜在分布是良好定义的。其次，我们提出了一种多级扩散算法，使我们能够在多个分辨率上高效地学习。我们实证表明，我们的多级模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本。此外，我们的方法可以无缝地生成不同分辨率的图像并处理矩形域。

    Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting
    
[^98]: 轻量化参数裁剪以实现节能深度学习: 一种二值门控模块方法

    Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10798](http://arxiv.org/abs/2302.10798)

    本论文提出了一种轻量化参数裁剪策略来实现节能深度学习，可以发现最佳的静态子网络，包含二值门控模块和新颖的损失函数，适用于各种神经网络，同时在已有的基准测试中取得了竞争性结果。

    

    随着神经网络越来越大且更加复杂，绿色AI已经引起了深度学习社区的关注。现有的解决方案通常采用对网络参数进行裁剪，以减少训练推断时的计算负荷。然而，裁剪方案通常会导致额外的开销，因为需要进行迭代训练和微调或重复计算动态裁剪图。我们提出了一种新的参数裁剪策略，以学习轻量级子网络，既能最小化能耗，又能在给定的下游任务上与完全参数化的网络保持相似的性能。我们的裁剪方案以绿色为导向，因为它仅需要动态裁剪方法进行一次训练来发现最佳的静态子网络。该方案由一个二进制门控模块和一个新颖的损失函数组成，以发现具有用户定义稀疏度的子网络。我们的方法可以对卷积神经网络（CNN）和循环神经网络（RNN）等通用神经网络进行裁剪和转换，大大减少了计算复杂度和能量消耗，同时在已有的基准测试上取得了竞争性的结果。

    The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr
    
[^99]: 自私行为下的劫匪社交学习

    Bandit Social Learning: Exploration under Myopic Behavior. (arXiv:2302.07425v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2302.07425](http://arxiv.org/abs/2302.07425)

    该论文研究了自私行为下的劫匪社交学习问题，发现存在一种探索激励权衡，即武器探索和社交探索之间的权衡，受到代理的短视行为的限制会加剧这种权衡，并导致遗憾率与代理数量成线性关系。

    

    我们研究了一种社交学习动态，其中代理按照简单的多臂劫匪协议共同行动。代理以顺序方式到达，选择武器并接收相关奖励。每个代理观察先前代理的完整历史记录（武器和奖励），不存在私有信号。尽管代理共同面临开发和利用的探索折衷，但每个代理人都是一见钟情的，无需考虑探索。我们允许一系列与（参数化）置信区间一致的自私行为，包括“无偏”行为和各种行为偏差。虽然这些行为的极端版本对应于众所周知的劫匪算法，但我们证明了更温和的版本会导致明显的探索失败，因此遗憾率与代理数量成线性关系。我们通过分析“温和乐观”的代理提供匹配的遗憾上界。因此，我们建立了两种类型的探索激励之间的基本权衡：武器探索是固有于劫匪问题的，只受当前代理的行动影响，而社交探索是由先前代理行为驱动的，因此有利于未来代理。由于代理的短视行为限制了社交探索，这种权衡被加剧。

    We study social learning dynamics where the agents collectively follow a simple multi-armed bandit protocol. Agents arrive sequentially, choose arms and receive associated rewards. Each agent observes the full history (arms and rewards) of the previous agents, and there are no private signals. While collectively the agents face exploration-exploitation tradeoff, each agent acts myopically, without regards to exploration. Motivating scenarios concern reviews and ratings on online platforms.  We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals, including the "unbiased" behavior as well as various behaviorial biases. While extreme versions of these behaviors correspond to well-known bandit algorithms, we prove that more moderate versions lead to stark exploration failures, and consequently to regret rates that are linear in the number of agents. We provide matching upper bounds on regret by analyzing "moderately optimistic" agents.  As a
    
[^100]: 数据修剪和神经缩放定律：基于评分的算法的基本限制

    Data pruning and neural scaling laws: fundamental limitations of score-based algorithms. (arXiv:2302.06960v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06960](http://arxiv.org/abs/2302.06960)

    评分数据修剪算法在高压缩区域失败，通过随机化的校准协议可以提高现有修剪算法在该区域的性能。

    

    数据修剪算法常用于减少优化过程的内存和计算成本。最近的实证结果表明，随机数据修剪仍然是一个强大的基准，并在高压缩区域优于大多数现有的数据修剪方法，即保留了不到数据的30％的部分。这种压缩区域最近引起了很多关注，因为数据修剪在提高所谓的神经缩放定律中的作用；在[Sorscher et al.]中，作者展示了需要高质量的数据修剪算法才能击败样本势律。在这项工作中，我们关注评分数据修剪算法，并在理论上和实际上展示了为什么这样的算法在高压缩区域失败。我们证明了数据修剪的“没有免费午餐”定理，并通过随机化提出了校准协议，以提高现有修剪算法在高压缩区域的性能。

    Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of $30\%$ or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; in [Sorscher et al.], the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law.  In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate ``No Free Lunch" theorems for data pruning and present calibration protocols that enhance the performance of existing pruning algorithms in this high compression regime using randomization.
    
[^101]: 一种统一的方法推导（时间均匀的）PAC-Bayes界限

    A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03421](http://arxiv.org/abs/2302.03421)

    该论文提出了一种用于推导PAC-Bayesian泛化界限的统一框架，不同于传统的固定样本大小方式，该框架适用于所有停止时间。同时，该论文还提出了新的边界方法，也可以应用于非平稳损失函数和非独立同分布的数据。

    

    我们提出了一个框架，用于推导PAC-Bayesian泛化界限。与大多数关于此主题的文献不同，我们的界限是任何时间都有效的（即时间均匀的），这意味着它们适用于所有停止时间，而不仅仅是固定的样本大小。我们的方法按照以下顺序结合了四种工具：（a）非负超马丁格尔或反向亚马逊，（b）混合法，（c）Donsker-Varadhan公式（或其它凸性对偶原理）和（d）Ville不等式。我们的主要成果是一个PAC-Bayes定理，适用于广泛的离散随机过程类。我们展示了这个结果如何推出知名的经典PAC-Bayes界限，例如Seeger、McAllester、Maurer和Catoni的界限，以及许多最新的界限。我们还提出了几个新的界限。我们的框架还使我们能够放松传统的假设；特别地，我们考虑非平稳损失函数和非独立同分布的数据。

    We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivati
    
[^102]: 扁平化贝叶斯神经网络

    Flat Seeking Bayesian Neural Networks. (arXiv:2302.02713v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02713](http://arxiv.org/abs/2302.02713)

    本文提出了一种扁平化贝叶斯神经网络的方法，该方法在后验推论中考虑了模型的扁平化性质，从而提升了模型的泛化能力和不确定性估计能力。

    

    贝叶斯神经网络（BNN）通过对模型参数施加先验分布并基于观测数据推断后验分布，为深度学习模型提供了概率解释。从后验分布中采样的模型可用于提供集成预测和量化预测不确定性。众所周知，具有较低尖度的深度学习模型具有更好的泛化能力。然而，现有的后验推论对于尖度/扁平化并不具备意识性，可能导致从其采样的模型具有较高的尖度。在本文中，我们针对扁平化对后验进行了理论、贝叶斯设定和变分推断方法的开发。具体地，我们从扁平化意义上推断的模型以及估计该扁平化意义后验的最佳近似后验，具有更好的扁平化性质，因此可能具有更高的泛化能力。我们对几个基准数据集进行了实验评估，并证明我们的方法在样本外准确性和不确定性估计方面优于现有方法。

    Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experime
    
[^103]: 预条件对超参化低秩矩阵感知的影响

    The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01186](http://arxiv.org/abs/2302.01186)

    该研究提出了ScaledGD(𝜆)方法，相较于传统梯度下降法更加鲁棒，并且在处理低秩矩阵感知问题时具有很好的表现。

    

    本文提出了ScaledGD(𝜆)方法来解决低秩矩阵感知中矩阵可能病态以及真实秩未知的问题。该方法使用超参式表示，从一个小的随机初始化开始，通过使用特定形式的阻尼预条件梯度下降来对抗超参化和病态曲率的影响。与基准梯度下降（GD）相比，尽管预处理需要轻微的计算开销，但ScaledGD（𝜆）在面对病态问题时表现出了出色的鲁棒性。在高斯设计下，ScaledGD($\lambda$) 会在仅迭代数对数级别的情况下，以线性速率收敛到真实的低秩矩阵。

    We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi
    
[^104]: AtMan:通过节约内存的注意力机制理解Transformer的预测

    AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08110](http://arxiv.org/abs/2301.08110)

    AtMan是一种通过在生成式Transformer模型中操纵注意力机制来解释预测的方法，相较于传统方法几乎不占用额外内存，可在生产环境中使用。

    

    生成式的Transformer模型越来越复杂，参数数量大且具备处理多输入模态的能力。目前解释它们的预测的方法资源密集。最重要的是，它们需要过多的额外内存，因为它们依赖反向传播，而反向传播会分配的GPU内存几乎是前向传播的两倍。这使得在生产环境中使用它们非常困难，甚至不可能。我们提出了AtMan，它几乎不会产生额外的成本，用于解释生成式Transformer模型。具体而言，AtMan是一种模态无关的扰动方法，通过操纵Transformer的注意力机制生成与输出预测相关性的重要性图。AtMan不使用反向传播，而是在嵌入空间中应用一种基于余弦相似度邻近性的可并行化基于记号的搜索方法。我们在文本和图像-文本基准测试中进行了详尽的实验

    Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstra
    
[^105]: 你是否正确使用了测试对数似然？

    Are you using test log-likelihood correctly?. (arXiv:2212.00219v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.00219](http://arxiv.org/abs/2212.00219)

    使用测试对数似然进行比较可能与其他指标相矛盾，并且高测试对数似然不意味着更准确的后验近似。

    

    测试对数似然常被用来比较不同模型的同一数据，或者比较拟合同一概率模型的不同近似推断算法。我们通过简单的例子展示了如何基于测试对数似然的比较可能与其他目标相矛盾。具体来说，我们的例子表明：（i）达到更高测试对数似然的近似贝叶斯推断算法不必意味着能够产生更准确的后验近似，（ii）基于测试对数似然比较的预测准确性结论可能与基于均方根误差的结论不一致。

    Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.
    
[^106]: 高维分位数回归中的转移学习统计推断

    Statistical inference for transfer learning with high-dimensional quantile regression. (arXiv:2211.14578v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14578](http://arxiv.org/abs/2211.14578)

    本研究提出了一种高维分位数回归模型中的转移学习方法，以适应源域和目标域中的异质性和重尾分布。根据精心选择的可转移源域建立了转移学习估计量的误差界限，并提出了有效的置信区间和假设检验程序，以实现一步完成。

    

    转移学习已经成为一种重要的技术，用于利用源域中的信息来提高目标任务的性能。尽管高维数据普遍存在异质性和/或重尾分布，但目前的转移学习方法未能充分考虑这些问题，可能会影响结果的性能。我们在高维分位数回归模型框架下提出了一种转移学习过程，以适应源域和目标域中的异质性和重尾分布。我们根据精心选择的可转移源域建立了转移学习估计量的误差界限，显示在关键选择标准和较大的源任务样本量下可以实现更低的误差界限。我们进一步提出了一个有效的置信区间和假设检验程序，用于高维分位数回归系数的各个分量，通过倡导双重转移学习估计量，实现一步完成。

    Transfer learning has become an essential technique to exploit information from the source domain to boost performance of the target task. Despite the prevalence in high-dimensional data, heterogeneity and/or heavy tails are insufficiently accounted for by current transfer learning approaches and thus may undermine the resulting performance. We propose a transfer learning procedure in the framework of high-dimensional quantile regression models to accommodate the heterogeneity and heavy tails in the source and target domains. We establish error bounds of the transfer learning estimator based on delicately selected transferable source domains, showing that lower error bounds can be achieved for critical selection criterion and larger sample size of source tasks. We further propose valid confidence interval and hypothesis test procedures for individual component of high-dimensional quantile regression coefficients by advocating a double transfer learning estimator, which is the one-step 
    
[^107]: MARLlib: 一个可扩展的多智能体强化学习库

    MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13708](http://arxiv.org/abs/2210.13708)

    本文提出了MARLlib，这是一个全面的MARL算法库，可统一数十种算法。它还超越了当前工作，集成了各种环境接口和提供灵活的参数共享策略。

    

    尽管多智能体系统和多智能体强化学习算法得到了快速发展，但缺乏统一的评估平台和公认的基准实现。因此，迫切需要开发一个集成库套件，以在各种基准测试中提供可靠的MARL实现和可复制的评估。本文提出了MARLlib，这是一个全面的MARL算法库，用于解决多智能体问题。MARLlib通过新颖的基于代理的分布式数据流设计，在高度可组合的集成风格中统一了数十种算法。此外，MARLlib通过集成各种环境接口和提供灵活的参数共享策略，超越了当前工作；这允许最终用户在最小的代码修改下实现协作、竞争和混合任务的多种解决方案。最后，MARLlib提供易于使用的API和完全解耦合的配置。

    Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
    
[^108]: 通过使用Cover Trees的最小间隔实现数值稳定的稀疏高斯过程

    Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.07893](http://arxiv.org/abs/2210.07893)

    本文针对高斯过程模型的数值稳定性进行了研究，通过感兴趣点的选择和计算，提供了稳定可靠的稀疏逼近方法。

    

    高斯过程常用于较大的机器学习和决策系统中，例如地理空间建模、贝叶斯优化或潜在高斯模型中。在一个系统中，高斯过程模型需要以稳定可靠的方式运行，以确保与系统的其他部分正确交互。本文研究了基于感兴趣点的可扩展稀疏逼近的数值稳定性。为此，我们首先回顾了数值稳定性，并阐述了高斯过程模型可能不稳定的典型情况。在插值文献中原始开发的稳定性理论的基础上，我们导出了对感兴趣点进行计算的数值稳定性的充分条件和某些情况下的必要条件。对于地理空间建模等低维任务，我们提出了一种自动计算满足这些条件的感兴趣点的方法。

    Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of t
    
[^109]: 使用多环境方法检测观测数据中的隐式混淆

    Detecting hidden confounding in observational data using multiple environments. (arXiv:2205.13935v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2205.13935](http://arxiv.org/abs/2205.13935)

    使用独立数据生成过程下的多环境方法，可以检测观测数据中的未观察到的混淆因素，并提出了测试独立性的程序。

    

    在因果推断中，常见的假设是没有隐式混淆。然而，在单个数据集中不能确定这个假设通常是不可能的。在独立的数据生成过程下，我们展示了一种方法来在多个来自不同环境的观测数据集中检测未观察到的混淆因素。我们提出了一种测试可验证的条件独立性的理论，这种独立性仅当存在混淆因素时才不存在，并检查了违反其假设的情况：退化和依赖机制以及忠实度违反。此外，我们提出了一种程序来测试这些独立性，并使用基于真实世界数据的半合成数据和模拟研究研究其经验有限样本行为。在大多数情况下，提出的程序能够正确预测存在隐式混淆，特别是当混淆偏差很大时。

    A common assumption in causal inference from observational data is that there is no hidden confounding. Yet it is, in general, impossible to verify this assumption from a single dataset. Under the assumption of independent causal mechanisms underlying the data-generating process, we demonstrate a way to detect unobserved confounders when having multiple observational datasets coming from different environments. We present a theory for testable conditional independencies that are only absent when there is hidden confounding and examine cases where we violate its assumptions: degenerate & dependent mechanisms, and faithfulness violations. Additionally, we propose a procedure to test these independencies and study its empirical finite-sample behavior using simulation studies and semi-synthetic data based on a real-world dataset. In most cases, the proposed procedure correctly predicts the presence of hidden confounding, particularly when the confounding bias is large.
    
[^110]: 不同输入维度数据集之间的迁移学习：线性回归情况下的算法和分析

    Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case. (arXiv:2202.05069v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.05069](http://arxiv.org/abs/2202.05069)

    本文提出了一种适用于线性回归情况的迁移学习算法，该算法能够将新数据与历史数据相结合，特别在新数据稀缺的情况下具有益处，并且在实验验证中表现出对负迁移学习的鲁棒性。

    

    随着新传感器和监测设备的发展，越来越多的数据源可以作为机器学习模型的输入。这些数据既可以帮助提高模型的准确性，但将这些新输入与历史数据相结合仍然是一个尚未详细研究的挑战。在本文中，我们提出了一种迁移学习算法，将新数据和历史数据结合起来，特别在新数据稀缺的情况下具有益处。我们将重点放在线性回归情况下，这使得我们能够对该方法的益处进行严格的理论研究。我们表明我们的方法对负迁移学习是具有鲁棒性的，并通过真实和模拟数据进行了实证验证。

    With the development of new sensors and monitoring devices, more sources of data become available to be used as inputs for machine learning models. These can on the one hand help to improve the accuracy of a model. On the other hand however, combining these new inputs with historical data remains a challenge that has not yet been studied in enough detail. In this work, we propose a transfer-learning algorithm that combines the new and the historical data, that is especially beneficial when the new data is scarce. We focus the approach on the linear regression case, which allows us to conduct a rigorous theoretical study on the benefits of the approach. We show that our approach is robust against negative transfer-learning, and we confirm this result empirically with real and simulated data.
    
[^111]: 可解释的序列分类通过原型轨迹

    Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.01777](http://arxiv.org/abs/2007.01777)

    ProtoryNet是一种基于原型轨迹的可解释深度神经网络，它通过捕捉时间模式和原型的近似程度来进行文本分类，并实现了直观和细致的推理过程解释。

    

    我们提出了一种新颖的用于文本分类的可解释深度神经网络，称为ProtoryNet，它基于原型轨迹的新概念。受现代语言学中的原型理论的启发，ProtoryNet通过为文本序列中的每个句子找到最相似的原型，并将每个句子与相应的活动原型的接近程度输入到RNN主干中进行预测。然后，RNN主干捕捉到原型的时间模式，我们称之为原型轨迹。原型轨迹能够直观而细致地解释RNN模型的推理过程，类似于人类分析文本的方式。我们还设计了原型修剪过程，以减少模型使用的原型总数，以提高解释性。在多个公共数据集上的实验证明，ProtoryNet比基线的基于原型的深度神经网络更准确，并减少了与现有模型相比的性能差距。

    We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
    

