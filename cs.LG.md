# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning](https://rss.arxiv.org/abs/2402.01350) | FedMoE是一种模型异构的个性化联邦学习算法，通过使用专家混合模型增强大型语言模型，将共享的小特征提取器和本地门控网络分配给每个客户端的本地异构大模型，以解决当前模型异构个性化联邦学习方法中存在的隐私、性能、通信和计算成本等问题。 |
| [^2] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^3] | [On the Semantics of LM Latent Space: A Vocabulary-defined Approach](https://rss.arxiv.org/abs/2401.16184) | 本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。 |
| [^4] | [FAST: Factorizable Attention for Speeding up Transformers](https://arxiv.org/abs/2402.07901) | 该论文介绍了一种可以加速Transformers模型的可分解注意力机制，通过引入因子分解形式的注意力，将注意力机制的复杂度从O(N^2)降低到O(N)，并 在维持注意力矩阵完整表示的同时保持稀疏性和所有-所有令牌关系。实验证明该注意力机制具有稳健的性能，并在不同应用中具有重要潜力。 |
| [^5] | [A systematic investigation of learnability from single child linguistic input](https://arxiv.org/abs/2402.07899) | 我们的研究探索了用单个儿童的语言输入训练语言模型的可学习性，我们发现这种设置下的语言模型能够形成句法和语义词群，并对某些语言现象具有敏感性。 |
| [^6] | [Label-Efficient Model Selection for Text Generation](https://arxiv.org/abs/2402.07891) | DiffUse是一种标注效率高的文本生成模型选择方法，它通过聚类文本语义差异的嵌入来选择更具信息量的实例，并能显著减少所需的注释数量。 |
| [^7] | [MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning](https://arxiv.org/abs/2402.07890) | MAIDCRL 是一种半集中式密集神经网络强化学习算法，通过引入智能体影响图(AIMs)和卷积层，成功在 StarCraft 多智能体挑战中实现了有效的多智能体控制。CNN-enabled MAIDCRL 在学习性能上显著提高，并在复杂的异质场景中取得了更快的学习速度。 |
| [^8] | [Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks](https://arxiv.org/abs/2402.07878) | 本文通过使用图论提取新指标来改进基于机器学习的网络攻击检测方法，从而提高检测效率并克服传统方法的局限性。 |
| [^9] | [Policy Improvement using Language Feedback Models](https://arxiv.org/abs/2402.07876) | 本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。 |
| [^10] | [Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States](https://arxiv.org/abs/2402.07875) | 本文研究了策略梯度在线性二次调节控制中对未见初始状态的外推问题，发现外推程度取决于训练中系统的探索程度。 |
| [^11] | [PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs](https://arxiv.org/abs/2402.07872) | 本文介绍了一种名为PIVOT的新颖视觉提示方法，它通过迭代的视觉问答将任务转化为VLMs问题。每个迭代中，图像被标注为VLMs可以参考的视觉表示，并通过优化选择最佳选项。这种方法能够使VLMs进行机器人控制和其他空间任务的输出。 |
| [^12] | [Scaling Laws for Fine-Grained Mixture of Experts](https://arxiv.org/abs/2402.07871) | 本研究分析了细粒度混合专家模型的标度特性，并引入了粒度作为新的超参数，通过调整粒度可以精确控制专家的大小。研究结果显示，MoE模型在效果上始终优于密集变压器模型，并且随着模型大小和训练预算的增大，密集和MoE模型之间的效率差距也在增大。同时，将MoE中专家的大小设置为与前馈层相同的常见做法在几乎任何计算预算下都不是最优的。 |
| [^13] | [Nesting Particle Filters for Experimental Design in Dynamical Systems](https://arxiv.org/abs/2402.07868) | 本文提出了一种新颖的方法来解决动态系统中的贝叶斯实验设计问题，利用嵌套粒子滤波器和立体蒙特卡洛方法来进行基于梯度的策略优化，相比于其他方法具有更好的性能。 |
| [^14] | [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867) | 本论文提出了一种名为PoisonedRAG的知识污染攻击方法，用于对大型语言模型的检索增强生成进行攻击和破坏。 |
| [^15] | [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://arxiv.org/abs/2402.07865) | 本论文探索了视觉条件化语言模型（VLMs）设计的关键空间，并提供了一套标准化评估，同时还研究了预训练的视觉表示和权衡的问题。 |
| [^16] | [AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy](https://arxiv.org/abs/2402.07862) | 本研究发现，使用LLMs助手可以显著提高预测准确性，不仅仅是由于模型预测准确性的提升。 |
| [^17] | [Multiscale Neuroimaging Features for the Identification of Medication Class and Non-Responders in Mood Disorder Treatment](https://arxiv.org/abs/2402.07858) | 本研究通过多尺度神经影像特征的分析，提出了一种方法来识别对标准药物疗程不反应的情绪障碍患者，为临床医生提供更可靠和高效的治疗选择。 |
| [^18] | [Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts](https://arxiv.org/abs/2402.07851) | 深度学习应用于历史降雨数据的预测比NWP预报和基于持续性的预测更准确。 |
| [^19] | [Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow Matching on Assignment Manifolds](https://arxiv.org/abs/2402.07846) | 本文提出了一种基于连续归一化流的生成模型，该模型可以逐步分配类别，避免了离散化潜在连续模型时的舍入和样本截断等问题。通过匹配分解离散分布的测地线流，可以高效地训练该模型，并且适用于表示复杂统计依赖关系的非分解离散分布。 |
| [^20] | [An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering](https://arxiv.org/abs/2402.07845) | 本研究展示了使用无监督度量模块性优化GNN进行节点聚类的方法，且无需与基准值进行比较。在设计合成实验的过程中，我们发现了这种方法的局限性。 |
| [^21] | [Towards Meta-Pruning via Optimal Transport](https://arxiv.org/abs/2402.07839) | 本文提出了一种名为Intra-Fusion的新方法，通过模型融合和最优传输的概念实现了神经网络的元剪枝，该方法能够有效恢复准确度并避免精调工作，同时具有高效和有前景的特点。 |
| [^22] | [Generalizing across Temporal Domains with Koopman Operators](https://arxiv.org/abs/2402.07834) | 本研究在时间领域泛化问题中提出了库普曼算子的应用，通过对齐条件分布来减小泛化界限。通过使用库普曼算子，我们可以有效地处理时变分布，从而解决时间领域泛化问题。 |
| [^23] | [On Computationally Efficient Multi-Class Calibration](https://arxiv.org/abs/2402.07821) | 提出了一种在多类别预测问题中多样化的投影平滑校准概念，并且给出了多项式时间复杂度的重新校准算法，从而实现了计算效率和强大的预测保证之间的权衡。 |
| [^24] | [Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning](https://arxiv.org/abs/2402.07818) | 本文研究了差分隐私零阶方法在大型语言模型微调中的应用，该方法通过使用零阶梯度来避免传统优化方法的可扩展性瓶颈，实现了在隐私、效用和可扩展性之间的良好平衡。 |
| [^25] | [Retrieval-Augmented Thought Process as Sequential Decision Making](https://arxiv.org/abs/2402.07812) | 检索增强思维过程（RATP）通过多步决策和蒙特卡洛树搜索，以及Q值估计器，解决了大型语言模型在隐私、产生幻觉和处理长文本方面的挑战，并在处理私人数据的问答任务中实现了50%的性能提升。 |
| [^26] | [Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation](https://arxiv.org/abs/2402.07808) | 本论文提出了一种基于样本的最大熵源分布估计方法，通过优先保留不确定性来选择合适的源分布。该方法利用切片-瓦石坦斯坦距离对数据集和模拟进行衡量，适用于具有难以处理似然函数的模拟器。实验表明，该方法可以恢复更高熵的源分布，同时保持模拟的准确性。 |
| [^27] | [Towards a mathematical theory for consistency training in diffusion models](https://arxiv.org/abs/2402.07802) | 本文探索了面向扩散模型中一致性训练的理论基础，证明了在一致性学习中，步骤数量需要超过$d^{5/2}/\varepsilon$的阶数，能够生成与目标分布接近的样本。 |
| [^28] | [Tuning-Free Stochastic Optimization](https://arxiv.org/abs/2402.07793) | 本文提出了一种无调参的随机优化算法，能够在只给出问题参数的粗略提示的情况下，与最优调参优化算法的性能相匹配。并且在有界的优化领域中证明了此算法的可行性，并探讨了在无界域中的条件。 |
| [^29] | [Empowering Federated Learning for Massive Models with NVIDIA FLARE](https://arxiv.org/abs/2402.07792) | 本研究探索了如何利用NVIDIA FLARE的联邦学习能力，解决了处理和利用分布式数据的挑战，并通过参数高效和全面监督微调大型语言模型，提高了自然语言处理和生物药物应用的准确性和鲁棒性。 |
| [^30] | [From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration](https://arxiv.org/abs/2402.07790) | 本研究通过分析校准度量对分数失真的敏感性，引入了一种改进的局部校准分数，并推广了局部回归作为有效的重新校准工具，提高了二元分类器在敏感决策领域的性能表现。 |
| [^31] | [HYPO: Hyperspherical Out-of-Distribution Generalization](https://arxiv.org/abs/2402.07785) | HYPO是一个在超球面空间中学习域不变表示的框架，通过内类变化和间类分离原则的引导，提高了离群泛化性能。 |
| [^32] | [IR-Aware ECO Timing Optimization Using Reinforcement Learning](https://arxiv.org/abs/2402.07781) | 本文提出了一种使用强化学习进行IR感知的ECO时序优化的方法，该方法通过门尺寸调整纠正由IR降低引起的时序退化，并且相较于传统方法在性能和运行时间上都具有优势。 |
| [^33] | [Multi-level Optimal Control with Neural Surrogate Models](https://arxiv.org/abs/2402.07763) | 该研究通过使用神经网络代理模型替代低级别的优化层次，提出了一种多级优化控制方法，用于解决最优执行器和控制设计问题。与传统方法相比，该方法能够快速且效果良好地确定最优执行器设计。 |
| [^34] | [Scalable Structure Learning for Sparse Context-Specific Causal Systems](https://arxiv.org/abs/2402.07762) | 提出了一种可扩展的混合算法，用于学习特定背景模型，通过结合基于顺序的MCMC算法和稀疏性假设实现可扩展学习，该方法在准确性和可扩展性方面表现良好。 |
| [^35] | [Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model](https://arxiv.org/abs/2402.07757) | 该论文研究了Transformer中的逐步推理，提出了一个合成图导航模型来探索逐步推理的底层机制，并通过该模型在合成任务上的实验证明了几个关键现象的存在。 |
| [^36] | [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) | 本文介绍了一种将扩散模型与思维链推理集成的方法，通过扩散传播推理步骤，提供了更大的灵活性和推理能力。实验证明了该方法在数学问题中的有效性，并展示了自我纠正能力和推理技术的潜力。 |
| [^37] | [Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains](https://arxiv.org/abs/2402.07752) | 本文提出了一种新颖的多智能体基于值的算法，混合Q函数（MQF），提高了基于值的方法在多智能体连续动作领域中的效果，通过同时评估多个动作来解决真实回报估计和局部最优解问题。 |
| [^38] | [Predictive Churn with the Set of Good Models](https://arxiv.org/abs/2402.07745) | 本文研究了在现代大众市场应用中，随时间更新的机器学习模型可能导致不稳定的预测结果，通过研究预测性多样性，量化了这种预测性流失，并通过Rashomon集合来分析模型更新中的预期流失。 |
| [^39] | [Towards Unified Alignment Between Agents, Humans, and Environment](https://arxiv.org/abs/2402.07744) | 本文介绍了统一对齐原则 ($\mathbf{UA}^2$)，旨在实现智能体与人类意图、环境动态和自我约束的统一对齐，提出了引入实际特性进行概念验证研究的方法。 |
| [^40] | [Task-conditioned adaptation of visual features in multi-task policy learning](https://arxiv.org/abs/2402.07739) | 本文通过任务条件的自适应器，在多任务策略学习的背景下，调整预训练的大型视觉模型，使其能够解决多个任务，并且无需微调预先训练的权重。 |
| [^41] | [Universal link predictor by In-context Learning](https://arxiv.org/abs/2402.07738) | 这项工作介绍了一种基于上下文学习的通用链接预测器(UniLP)，它将启发式方法的广泛适用性和参数模型的模式学习能力相结合，实现了自主学习目标图中的链接模式并具有跨不同图的泛化能力。 |
| [^42] | [Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism](https://arxiv.org/abs/2402.07735) | 本论文提出了一种利用BAM进行图结构推断的方法。通过神经网络模型，通过变形的耦合模拟输入数据进行训练，仅需通过一次前向传递即可进行推断。通过利用结构方程模型和随机生成的多变量切比雪夫多项式来模拟训练数据，方法能够泛化到线性和各种非线性依赖关系。引入了双线性注意机制（BAM）来处理依赖关系，该机制在转换数据的协方差矩阵水平上运行，并尊重对称正定矩阵流形的几何特性。实证评估证明了方法的有效性和性能。 |
| [^43] | [Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation](https://arxiv.org/abs/2402.07723) | 本论文通过分数阻尼库仑方程证明了重尾SDE的高概率泛化界限，并且相对于参数维度，界限的依赖性要好于p。 |
| [^44] | [LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation](https://arxiv.org/abs/2402.07721) | 本文提出了LoRA-drop方法，通过分析LoRA输出评估参数的重要性，并且保留重要层的LoRA，其余层共享相同参数。实验结果表明LoRA-drop有很好的效果。 |
| [^45] | [Model Collapse Demystified: The Case of Regression](https://arxiv.org/abs/2402.07712) | 本研究在核回归的简化环境中解析了模型崩溃现象，并发现了模型能够处理虚假数据与性能完全崩溃之间的交叉点。通过提出基于自适应正则化的策略，成功缓解了模型崩溃问题。这些发现通过实验证实。 |
| [^46] | [Online Sequential Decision-Making with Unknown Delays](https://arxiv.org/abs/2402.07703) | 本文提出了在在线顺序决策中处理未知延迟问题的三个延迟算法族，并提供了相应的遗憾界限。 |
| [^47] | [Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints](https://arxiv.org/abs/2402.07692) | 本文提出了一种新的贝叶斯优化方法BE-CBO，通过有效地探索可行和不可行设计之间的边界，来解决在实际应用中优化未知约束的未知函数的问题。 |
| [^48] | [Contrastive Multiple Instance Learning for Weakly Supervised Person ReID](https://arxiv.org/abs/2402.07685) | 引入对比多实例学习（CMIL）框架，解决了弱监督人物重识别中数据集标注不准确的问题。CMIL利用对比损失技术，在三个数据集上取得了与最先进性能相匹配的结果，并优于所有基准方法。 |
| [^49] | [Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks](https://arxiv.org/abs/2402.07684) | 本文研究了基于协方差神经网络的基础模型NeuroVNN，该模型可以通过解析解剖学特征进行大脑年龄预测，具有可迁移性。这对于理解神经退行性疾病和认知衰退的增加风险具有重要意义。 |
| [^50] | [A Flow-based Credibility Metric for Safety-critical Pedestrian Detection](https://arxiv.org/abs/2402.07642) | 该论文提出了一种基于光流信号和行人边界框的可信度度量方法，能够帮助开发人员识别安全关键的误检。 |
| [^51] | [Tighter Bounds on the Information Bottleneck with Application to Deep Learning](https://arxiv.org/abs/2402.07639) | 这个论文提出了一个对信息瓶颈更为紧密的变分界限，可以改善以前的基于信息瓶颈的DNNs的性能，并提供了一种简单方法来显著增强分类器DNNs的对抗鲁棒性。 |
| [^52] | [G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering](https://arxiv.org/abs/2402.07630) | 本论文提出了G-Retriever模型，该模型用于文本图理解和问题解答。该模型能够将用户的问题转化为文本回复，并突出显示图形的相关部分。与现有的方法不同，该模型适用于真实世界的文本图形，并可应用于不同的任务，包括场景图理解、常识推理和知识图推理。 |
| [^53] | [Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features](https://arxiv.org/abs/2402.07626) | 本研究通过路径积分方法探索了连续时间随机梯度流动力学中的测试风险，并在小学习率情况下给出了计算纯梯度流动和随机梯度流动的测试风险曲线之间差异的一般公式。通过应用于一个弱特征模型，我们分析了随机项对动力学的修正效果，并与离散时间随机梯度下降的模拟结果进行了比较，结果显示出一致性。 |
| [^54] | [AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts](https://arxiv.org/abs/2402.07625) | 本论文介绍了一种自主数据选择策略，利用语言模型进行数学文本的自动评估和选择，并通过连续预训练显著提高了数学推理能力。主要创新包括利用元提示语言模型作为验证器，发布了高质量的AutoMathText数据集，并实现了预训练令牌效率的提升。 |
| [^55] | [Correctness Verification of Neural Networks Approximating Differential Equations](https://arxiv.org/abs/2402.07621) | 这项工作解决了神经网络逼近微分方程解的正确性验证问题，通过定义神经网络的导数近似和建立并行分支算法，有效界定了函数的上下界，并处理了没有输出域先验知识的情况。 |
| [^56] | [Global optimality under amenable symmetry constraints](https://arxiv.org/abs/2402.07613) | 该论文研究了在可接受的对称约束条件下的全局最优性问题，提出了一种满足对称性质的函数或度量，并通过引入轨道凸体和coycle等工具解决了这一问题。具体应用包括不变核均值嵌入和基于对称约束的运输方案最优性。这些结果与不变性检验的Hunt-Stein定理相关。 |
| [^57] | [Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model](https://arxiv.org/abs/2402.07598) | 本论文提出了一种基于生成模型的近最小极大分布式强化学习算法，该算法在使用生成模型近似回报分布方面具有极小极大优势，解决了一个开放问题，并提供了实验研究结果。 |
| [^58] | [Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification](https://arxiv.org/abs/2402.07595) | 本文通过对比分析ImageNet预训练的深度学习模型和DINOv2在医学图像分类中的表现，发现在临床数据集中，DINOv2的性能不如ImageNet-b。 |
| [^59] | [Foundational Inference Models for Dynamical Systems](https://arxiv.org/abs/2402.07594) | 本研究提出了一种基于监督学习的框架，用于从噪声数据中零样本推理动态系统的普通微分方程（ODE）。通过生成大型ODE数据集，并利用神经网络将噪声观察和初始条件以及向量场进行映射，得到称为基础推理模型（FIM）的结果模型。这些模型可以复制、匹配和组合，用于构建任何维度的推理模型。 |
| [^60] | [Rethinking Scaling Laws for Learning in Strategic Environments](https://arxiv.org/abs/2402.07588) | 本文重新思考了在战略环境中学习的比例定律，发现战略互动可以打破传统的观点，即模型越大或表达能力越强并不一定会随之提高性能。通过几个战略环境的例子，我们展示了这种现象的影响。 |
| [^61] | [Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning](https://arxiv.org/abs/2402.07586) | 该研究在联邦学习中的分布式环境下，首次探索了在存在特定群体概念漂移的情况下实现公平性的挑战和解决方案。 |
| [^62] | [Identifying architectural design decisions for achieving green ML serving](https://arxiv.org/abs/2402.07585) | 本研究通过对机器学习服务架构设计决策的分析，揭示了实现绿色机器学习服务的关键环节，并探讨了其与质量特性的关联。 |
| [^63] | [Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction](https://arxiv.org/abs/2402.07570) | 通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。 |
| [^64] | [Weisfeiler-Leman at the margin: When more expressivity matters](https://arxiv.org/abs/2402.07568) | 研究探讨了1-WL算法在图同构问题中的表达能力和泛化性能之间的关系，发现增强的表达能力对提高泛化性能并不总是有效。此外，通过引入子图信息和经典的边缘理论，探索了更高表达力与改进泛化性能的条件。梯度流也被证明可以促进模型学习更丰富的表达能力。 |
| [^65] | [A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing](https://arxiv.org/abs/2402.07549) | 我们提出了一种精度优化的固定点近存储数字处理单元，用于高效的模拟内存计算。该处理单元在保持高效能量和面积效率以及低延迟的同时，通过支持标准的深度学习激活步骤实现了竞争性的准确性和更高的计算吞吐量。 |
| [^66] | [TransAxx: Efficient Transformers with Approximate Computing](https://arxiv.org/abs/2402.07545) | TransAxx是一个基于PyTorch库的框架，可以支持近似计算，并通过对Vision Transformer模型进行近似感知微调，来提高在低功耗设备上的计算效率。 |
| [^67] | [Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models](https://arxiv.org/abs/2402.07543) | 本研究证明了使用解释来改进语言模型性能的显著好处，尤其适用于较小的模型，解释的加入使模型能够解决之前无法解决的任务。 |
| [^68] | [Accelerating Distributed Deep Learning using Lossless Homomorphic Compression](https://arxiv.org/abs/2402.07529) | 本论文介绍了一种使用无损同态压缩加速分布式深度学习的新方法，通过结合工作节点级别的压缩和网络内聚合，实现高效的训练过程，并保证了训练精度。实验证明了该方法在各种DNN模型上的优越性能。 |
| [^69] | [NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness](https://arxiv.org/abs/2402.07506) | NeuralSentinel是一个工具，能够验证人工智能模型的可靠性和可信度，并帮助非专业人员通过理解模型的决策来增强对这一新系统的信任度。 |
| [^70] | [ClusterTabNet: Supervised clustering method for table detection and table structure recognition](https://arxiv.org/abs/2402.07502) | ClusterTabNet是一种监督聚类方法，通过解释表格结构为单词之间的关系图，并利用深度学习模型预测其邻接矩阵，实现对表格的检测和结构识别。与其他方法相比，ClusterTabNet具有相似或更好的准确性，并且需要更小的模型。 |
| [^71] | [One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning](https://arxiv.org/abs/2402.07501) | 本论文提出了使用监督对比学习的加密流量分类框架，利用对比学习增强数据包级别和流级别表示，并通过图数据增强捕获字节级流量图的细粒度语义不变特征。同时，采用跨级多任务学习的方法，使得数据包级别任务学习到的表示能够被流级别任务利用。 |
| [^72] | [Accelerated Smoothing: A Scalable Approach to Randomized Smoothing](https://arxiv.org/abs/2402.07498) | 本文提出了一种加速随机平滑的方法，通过训练替代了蒙特卡洛抽样，显著提高了计算效率，并在各种设置下展示了其在近似平滑分类器方面的精确性。 |
| [^73] | [Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment](https://arxiv.org/abs/2402.07496) | 通过对深度学习模型的可视化分析，研究了对抗样本攻击的防御方法，以更加精确地了解防御模型的性能如何被修改。 |
| [^74] | [Convolutional Neural Networks for signal detection in real LIGO data](https://arxiv.org/abs/2402.07492) | 本论文介绍了用于LIGO实时数据中信号检测的卷积神经网络，并描述了解决机器学习评估框架问题的机器学习引力波搜索挑战赛。团队TPI FSU Jena提交了基于机器学习方法的算法，并成功将其应用于真实的O3b数据，并恢复了GWTC-3目录中的相关事件。 |
| [^75] | [Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial](https://arxiv.org/abs/2402.07487) | 本文是基于分数的扩散模型的技术教程，重点讲解了通过随机微分方程进行公式化的方法，包括采样和分数匹配。适合初学者了解该领域，并且从业人员在设计新模型或算法时也可能会有所帮助。 |
| [^76] | [Topological Safeguard for Evasion Attack based on the Interpretability of Artificial Neural Network Behavior](https://arxiv.org/abs/2402.07480) | 本文提出了一种基于人工神经网络行为解释性的拓扑保护方法，用于对抗逃避攻击。在过去几年中，深度学习技术的应用日益广泛，但也带来了新的网络安全威胁。逃避攻击是其中一种常见攻击，本文旨在设计一种能够对抗该攻击的有效防御方法。 |
| [^77] | [Cartesian atomic cluster expansion for machine learning interatomic potentials](https://arxiv.org/abs/2402.07472) | 本论文提出了一种改进的机器学习原子间势模型，使用基于笛卡尔坐标的原子密度展开来替代传统的原子团簇展开方法，并结合低维嵌入和原子间消息传递。该模型在不同系统中表现出良好的准确性、稳定性和普适性。 |
| [^78] | [Differentially Private Decentralized Learning with Random Walks](https://arxiv.org/abs/2402.07471) | 这项工作研究了具有随机游走算法的差分隐私去中心化学习，并使用最近的差分隐私变种推导了节点之间的隐私损失。实验结果表明，与节点之间的八卦算法相比，随机游走算法更能提供较好的隐私保证。 |
| [^79] | [Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations](https://arxiv.org/abs/2402.07465) | 这项研究提出了一种基于得分函数的求解器来解决高维福克-普朗克方程中的维数灾难问题。与蒙特卡洛和普通PINN相比，该方法能够更准确地处理与布朗运动相关的概率密度函数，并提供快速采样。 |
| [^80] | [A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?](https://arxiv.org/abs/2402.07462) | 我们提出了HALO法规模式，使用激素分析来调节人工智能的行为模式，以解决价值装载问题中的“回形针最大化器”场景。 |
| [^81] | [On the Distance from Calibration in Sequential Prediction](https://arxiv.org/abs/2402.07458) | 本论文研究了顺序预测中的标定距离，证明了存在一种预测算法可以在敌人选择的二进制序列上实现$O(\sqrt{T})$的标定距离，通过较低的标定距离进行准确近似。 |
| [^82] | [Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs](https://arxiv.org/abs/2402.07453) | 该论文研究了在对抗在线环境中多类分类中依赖于强盗反馈的代价，自适应对手和随机学习者与无视对手和确定性学习者之间的损失差距。 |
| [^83] | [TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound](https://arxiv.org/abs/2402.07452) | TriAug是一个用于乳腺超声图像的异常样本检测框架，通过使用三元状态增强和平衡的球形损失来提高示踪分类的准确性和异常样本检测性能。 |
| [^84] | [AraSpider: Democratizing Arabic-to-SQL](https://arxiv.org/abs/2402.07448) | AraSpider是首个阿拉伯语版本的Spider数据集，研究表明使用回译策略可以显著提高ChatGPT 3.5和SQLCoder模型在阿拉伯语NLP任务中的性能。 |
| [^85] | [Top-$K$ ranking with a monotone adversary](https://arxiv.org/abs/2402.07445) | 本文针对具有单调对手的Top-K排名问题，提出了一种加权最大似然估计器(MLE)，在样本复杂度方面接近最优。算法创新包括了对加权MLE的精确且紧密的$\ell_\infty$误差分析，并与加权比较图的谱特性相关联。 |
| [^86] | [The I/O Complexity of Attention, or How Optimal is Flash Attention?](https://arxiv.org/abs/2402.07443) | 这项研究解决了FlashAttention算法的I/O复杂性是否是最优的问题。 |
| [^87] | [Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT](https://arxiv.org/abs/2402.07440) | 该论文介绍了LoCoV1，一个用于评估长上下文检索性能的新型基准测试，并提出了M2-BERT检索编码器，用于处理长上下文检索，解决了如何评估性能、预训练语言模型以及如何进行微调的挑战。 |
| [^88] | [Learning Optimal Tax Design in Nonatomic Congestion Games](https://arxiv.org/abs/2402.07437) | 本研究致力于学习如何设计最优税收，以在非原子拥堵博弈中提高效率。为了解决指数级的税收函数空间、梯度不存在和目标函数的非凸性等挑战，该算法利用了分段线性税收、额外的线性项和有效的子例程的新颖组成部分。 |
| [^89] | [Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and IV Models for GBP/USD and EUR/GBP Pairs](https://arxiv.org/abs/2402.07435) | 本研究比较了GARCH、EWMA和IV模型在预测GBP/USD和EUR/GBP货币对每日回报的20天变动方面的效果。研究发现EUR/GBP货币对存在非对称回报的证据，而GBP/USD货币对的证据并不一致。 |
| [^90] | [Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand](https://arxiv.org/abs/2402.07419) | 本文展示了通过条件生成模型的推进计算可以计算任何可辨识的因果效应，并提出了基于扩散的方法用于从图像的任何（条件）干预分布中进行采样。 |
| [^91] | [An Empirical Study Into What Matters for Calibrating Vision-Language Models](https://arxiv.org/abs/2402.07417) | 本研究通过实证研究探索了视觉-语言模型(VLMs)的校准性质，并发现温度缩放可以显著提升校准性能，而且VLMs只需少量示例即可进行校准。 |
| [^92] | [Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems](https://arxiv.org/abs/2402.07415) | 本文研究了面向异构计算系统的上下文感知多模型目标检测，通过利用嵌入在输入数据流中的上下文信息实现更高效的基于多模型的目标检测过程，并考虑多加速器执行以优化能量效率。 |
| [^93] | [Auxiliary Reward Generation with Transition Distance Representation Learning](https://arxiv.org/abs/2402.07412) | 这篇论文提出了一种使用状态转换距离表示学习的辅助奖励生成方法，可以在强化学习中自动生成奖励，提高学习效率和减少人工设计奖励的工作量。 |
| [^94] | [Potential-Based Reward Shaping For Intrinsic Motivation](https://arxiv.org/abs/2402.07411) | 该论文提出了潜势引导的奖励塑造方法用于处理内在动机，在复杂和稀疏奖励环境下能避免改变最优策略集导致次优行为的问题。 |
| [^95] | [A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)](https://arxiv.org/abs/2402.07410) | 本研究深入分析了对比语言-图像预训练(CLIP)模型的健壮性，重点关注了其在特定视觉因素变化、不确定性估计和异常输入检测等安全目标上的表现。 |
| [^96] | [Conformal Predictive Programming for Chance Constrained Optimization](https://arxiv.org/abs/2402.07407) | 可容许预测规划（CPP）是一种解决受任意随机参数影响的优化问题的方法，通过利用样本和量子引理将机遇受限优化（CCO）问题转化为确定性优化问题，并具备边际概率可行性保证。 |
| [^97] | [TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator](https://arxiv.org/abs/2402.07393) | TeMPO是一种时分多路动态光学张量加速器，通过跨层设备/电路/架构定制，弥合了光学神经加速器与高度定制的电子对应器之间的性能差距。在设备级别上，该加速器采用了工厂可用的、定制的光学器件，包括实验演示的慢光电光调制器、光分配器和相移器，显著减小了输入编码和点乘中的占地面积和功耗。 |
| [^98] | [Replicability is Asymptotically Free in Multi-armed Bandits](https://arxiv.org/abs/2402.07391) | 本论文研究在多臂赌博机问题中，通过引入探索-再确定算法和连续淘汰算法，以及谨慎选择置信区间的幅度，实现了可复制性，并证明了当时间界足够大时，可复制算法的额外代价是不必要的。 |
| [^99] | [The Limits of Assumption-free Tests for Algorithm Performance](https://arxiv.org/abs/2402.07388) | 这项研究探讨了使用有限数据量回答算法性能问题的基本限制，证明了黑盒测试方法无法准确回答算法在不同训练集上的整体性能和特定模型的性能问题。 |
| [^100] | [Exploring Perceptual Limitation of Multimodal Large Language Models](https://arxiv.org/abs/2402.07384) | 在这项研究中，我们探索了多模态大型语言模型（MLLMs）在感知能力上的限制。我们发现，对于小型视觉对象的问题，MLLMs的回答能力普遍存在限制。通过控制干预实验，我们发现物体质量、大小和位置都对MLLMs的感知能力有影响。 |
| [^101] | [Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like](https://arxiv.org/abs/2402.07383) | 本文提出了ELaTE，一种基于流匹配的零样本文本到语音系统，可以根据短音频提示以精确控制笑声时机和表情生成任何说话者的自然笑声。 |
| [^102] | [Diff-RNTraj: A Structure-aware Diffusion Model for Road Network-constrained Trajectory Generation](https://arxiv.org/abs/2402.07369) | 这篇论文提出了一种结构感知的扩散模型Diff-RNTraj，用于在道路网络上生成受到约束的轨迹数据，解决了现有方法中无法保证轨迹约束在道路上、缺乏道路相关信息的问题。 |
| [^103] | [Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning](https://arxiv.org/abs/2402.07368) | 本研究通过使用2016年和2020年的选举数据，评估了基于大型语言模型的分组代表模型在泛化能力上的表现。研究发现，尽管使用实证数据进行条件设定可以提高整体性能，但上下文学习的益处在不同人口子群组之间差异很大，这对实施分组代表模型的从业人员和决策者构成了挑战。 |
| [^104] | [Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing](https://arxiv.org/abs/2402.07366) | 本文提出了一种基于消息传递的贝叶斯联邦学习（BFL）框架，通过结合期望最大化和Turbo Deep近似消息传递（TDAMP）实现分布式学习和压缩。该框架在处理联邦学习算法的缺点上有着显著的改进。 |
| [^105] | [A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents](https://arxiv.org/abs/2402.07365) | 本文提出了一种基于深度学习的方法，用于相对绩效标准下的最优投资。该方法利用前向-后向随机微分方程的纳什均衡特征和随机微分游戏的机器学习算法的最新进展。数值实验在两个不同的金融模型上进行，比较了不同交互结构的图形图的影响。 |
| [^106] | [Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions](https://arxiv.org/abs/2402.07363) | 本论文提出了一种在一价拍卖中进行竞标的新颖算法，并通过分析证明了其在战略背景下的效果。具体而言，这些算法在面对对策性卖家时表现良好，激励买家进行真实的交易，并获得了最佳的后悔结果。 |
| [^107] | [Regression Trees for Fast and Adaptive Prediction Intervals](https://arxiv.org/abs/2402.07357) | 该论文提出了一种新的、与模型无关的方法族，用于校准具有局部覆盖保证的回归问题的预测区间。这种方法利用回归树和随机森林训练来创建最粗糙的特征空间划分，以近似条件覆盖，提供了准确、快速和自适应的预测区间。 |
| [^108] | [A Novel Gaussian Min-Max Theorem and its Applications](https://arxiv.org/abs/2402.07356) | 本文介绍了一个新的高斯最小最大定理，扩展了经典定理对于独立但非恒定分布的情况。此外，该定理在高维统计学、机器学习、非光滑优化和信号处理等领域有广泛的应用。 |
| [^109] | [Sampling from the Mean-Field Stationary Distribution](https://arxiv.org/abs/2402.07355) | 本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。 |
| [^110] | [Data Distribution-based Curriculum Learning](https://arxiv.org/abs/2402.07352) | 本文提出了一种基于数据分布的课程学习方法（DDCL），通过利用数据集的数据分布和两种评分方法（DDCL（密度）和DDCL（点）），根据样本的顺序来提高分类器的性能。 |
| [^111] | [Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble](https://arxiv.org/abs/2402.07347) | 本研究研究了TextFooler黑盒对01 loss sign激活神经网络集成的攻击准确性。研究发现，相比于sigmoid激活交叉熵和二进制神经网络，使用01 loss sign激活的网络更难受到TextFooler的攻击，并且通过引入一种新的全局汇集步骤，进一步提高了对抗精确度，使TextFooler几乎无效化。 |
| [^112] | [Measurement Scheduling for ICU Patients with Offline Reinforcement Learning](https://arxiv.org/abs/2402.07344) | 本研究介绍了使用离线强化学习进行ICU病人实验室检测排程的方法，并探索了最新数据集和离线强化学习方法在这一领域的应用。研究结果有助于提高实验室检测排程策略的质量和实用性。 |
| [^113] | [Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization](https://arxiv.org/abs/2402.07341) | 这项研究提出了一种对线性强化学习领域中未知噪声水平的自适应置信区间，与已有方法相比，在维度较大时具有显著的改进。此外，针对有界奖励，还提出了一种方差自适应置信区间，具有更好的数值性能。 |
| [^114] | [Random Geometric Graph Alignment with Graph Neural Networks](https://arxiv.org/abs/2402.07340) | 本文研究了在图对齐问题中，通过图神经网络可以高概率恢复正确的顶点对齐。通过特定的特征稀疏性和噪声水平条件，我们证明了图神经网络的有效性，并与直接匹配方法进行了比较。 |
| [^115] | [Differentially Private Training of Mixture of Experts Models](https://arxiv.org/abs/2402.07334) | 本文研究了在自然语言处理中，如何使用差分隐私训练混合专家模型。研究通过探索MoE模型的潜力和应用DP解决了计算和隐私问题，并实验证明了MoE模型可以在保护隐私的前提下有效训练。 |
| [^116] | [Lessons Learned from Mining the Hugging Face Repository](https://arxiv.org/abs/2402.07323) | 这个报告总结了在Hugging Face存储库进行的两项研究的见解，重点关注了碳排放、ML模型的演化和维护方面，为未来的研究人员提供了实用指南。 |
| [^117] | [Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs](https://arxiv.org/abs/2402.07321) | Transformer-based大型语言模型在事实回忆任务中使用加法模式来存储和检索知识，通过相加组合多个独立的机制对正确答案进行构造性干涉。 |
| [^118] | [Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets](https://arxiv.org/abs/2402.07320) | 本研究提出了一种使用语言嵌入和主动学习来实现可解释、安全的自动驾驶的框架。通过使用对比语言-图像预训练嵌入进行聚类实验，我们有效地识别和解释了新颖的自动驾驶场景。 |
| [^119] | [ODIN: Disentangled Reward Mitigates Hacking in RLHF](https://arxiv.org/abs/2402.07319) | 本研究解决了强化学习中的奖励黑客问题，针对回复长度这一挑战，通过建立可靠的评估协议和改进奖励模型的方法，提出了减轻长度偏差的超参数和技巧，并进行了大规模研究。 |
| [^120] | [A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference](https://arxiv.org/abs/2402.07314) | 本论文从理论层面分析了一种关于一般偏好下纳什学习从人类反馈中的方法，通过对两个竞争的LLM进行博弈来找到一种一致生成响应的策略。 |
| [^121] | [BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2402.07310) | BioNeRF是一种生物合理的架构，通过辐射场对场景进行3D表示并合成新视图。它实现了一种认知启发的机制，提高了存储能力和提取信息的能力，并在真实世界图像和合成数据的两个数据集上超过了以人的感知为基础的质量度量的最新结果。 |
| [^122] | [HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs](https://arxiv.org/abs/2402.07309) | 本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。 |
| [^123] | [Self-Consistent Conformal Prediction](https://arxiv.org/abs/2402.07307) | 自洽的符合预测方法能够提供既符合校准的预测又符合以模型预测的动作为条件的预测区间，为决策者提供了严格的、针对具体动作的决策保证。 |
| [^124] | [Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning](https://arxiv.org/abs/2402.07295) | 本文提出在无服务器联邦学习中利用知识蒸馏训练异构客户端模型的方法，以解决资源和统计数据的异质性挑战。 |
| [^125] | [On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study](https://arxiv.org/abs/2402.07294) | 本研究通过引入具有高测试覆盖率的真实Java程序数据集，探索了机器学习基于调用图剪枝的有效性，并通过对比分析静态调用图生成技术来解决当前方法存在的问题。 |
| [^126] | [CLIPPER: Robust Data Association without an Initial Guess](https://arxiv.org/abs/2402.07284) | 本文提出了一种无需初始猜测的稳健数据关联方法CLIPPER，通过利用加权图和寻找最密集的加权点团来解决数据关联的挑战。实验结果显示，该方法在点云注册方面取得了显著的性能提升。 |
| [^127] | [Power Transformer Fault Prediction Based on Knowledge Graphs](https://arxiv.org/abs/2402.07283) | 本文提出了一种基于知识图谱和梯度提升决策树的方法，用于学习有限的电力变压器故障数据。实验证明该方法在故障预测准确度上优于传统的人工神经网络和逻辑回归方法。 |
| [^128] | [How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?](https://arxiv.org/abs/2402.07282) | 本文研究了如何在大型语言模型中权衡诚实和帮助性，在实验中发现强化学习改善了诚实和帮助性，而链式思维提示则偏向于帮助性。研究结果还展示了GPT-4 Turbo对对话框架和听众决策背景的敏感性。这些发现揭示了大型语言模型内化的对话价值观，并暗示零-shot提示可以在一定程度上引导这些抽象价值观。 |
| [^129] | [Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study](https://arxiv.org/abs/2402.07281) | 本文通过一项基准研究评估了多种基于机器学习的异常检测算法，包括树结构方法和深度学习方法，并揭示了深度学习神话的真相。 |
| [^130] | [Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy](https://arxiv.org/abs/2402.07270) | 该研究通过提出创新的评估方法和基于分类数据集的新型VQA基准，推动了对文本生成的视觉语言模型能力的理解。同时，他们还提出了使用语义层次和自动生成的后续问题来改进对细粒度分类任务上粗糙答案的评估。通过比较不同度量标准，他们在进行人工评估研究的基础上选择了最终的度量标准。 |
| [^131] | [Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer](https://arxiv.org/abs/2402.07268) | 使用名为PathFormer的新型GNN模型架构，系统整合信号网络、先验知识和组学数据来实现高精确度疾病诊断和高可重复性生物标志物识别。 |
| [^132] | [Physics-Informed Neural Networks with Hard Linear Equality Constraints](https://arxiv.org/abs/2402.07251) | 这项工作提出了一种新颖的物理信息神经网络（KKT-hPINN），通过从KKT条件导出的投影层，严格保证硬线性等式约束，并在多个数值实验中展示了其提高预测准确性的能力。 |
| [^133] | [DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains](https://arxiv.org/abs/2402.07250) | DIMON是一个学习在一系列变形的域上解算偏微分方程的通用算子学习框架，通过在参考域训练数据上学习解的映射，然后将其重新映射回原始域来实现对多个域上变化的初始/边界条件下的偏微分方程求解的近似。 |
| [^134] | [The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey](https://arxiv.org/abs/2402.07249) | 本文通过系统调查，发现整合领域知识可以提高分子性质预测的准确性，同时利用多模态数据融合可以产生更精确的结果。 |
| [^135] | [Depth Separations in Neural Networks: Separating the Dimension from the Accuracy](https://arxiv.org/abs/2402.07248) | 通过研究深度2和深度3神经网络在逼近Lipschitz目标函数时的分离性质，证明了维度诅咒也会在深度2逼近中存在，即使目标函数可以使用深度3高效表示。这为以前确定深度要求的下界提供了新的观点，并且适用于多种激活函数。 |
| [^136] | [Towards Generalized Inverse Reinforcement Learning](https://arxiv.org/abs/2402.07246) | 本文研究了广义逆强化学习（GIRL）在马尔科夫决策过程（MDP）中的应用，提出了一种解决观察策略与最优策略之间差异以及不完全可观察情况下数学表述最优策略的方法，并开发了一种快速的启发式算法，数值结果显示其有效性。 |
| [^137] | [GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains](https://arxiv.org/abs/2402.07232) | GenSTL是一个通用的稀疏轨迹学习框架，通过自回归生成特征域来实现稀疏轨迹与密集轨迹之间的连接，从而消除了对大规模密集轨迹数据的依赖。 |
| [^138] | [Rethinking Graph Masked Autoencoders through Alignment and Uniformity](https://arxiv.org/abs/2402.07225) | 通过对齐和一致性重新思考图形遮罩自编码器对图的自监督学习方法进行了理论分析，揭示了GraphMAE中的节点级重构目标实际上执行了上下文级对比学习，并指出了GraphMAE在对齐和一致性方面的局限性。 |
| [^139] | [Towards Fast Stochastic Sampling in Diffusion Generative Models](https://arxiv.org/abs/2402.07211) | 本文提出了一种在扩散生成模型中进行快速随机采样的方法，通过对分裂积分器进行原则性修改，实现了更高的采样效率。在CIFAR-10数据集上进行实验，100次网络函数评估下的FID分数为2.36。 |
| [^140] | [Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning](https://arxiv.org/abs/2402.07204) | 本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。 |
| [^141] | [Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks](https://arxiv.org/abs/2402.07200) | 这项研究提出了一种异常值感知批归一化(OABN)和一种聚类的非均匀量化训练(ClusterQAT)框架，用于处理结构再参数化网络(SR)中由合并过程引入的异常值，以提高量化性能。 |
| [^142] | [The Implicit Bias of Gradient Noise: A Symmetry Perspective](https://arxiv.org/abs/2402.07193) | 本研究通过对对称性的存在进行分析，揭示了梯度噪声在随机梯度下降中的隐性偏见。我们发现不同类型的对称性会导致不同的学习动态，其中一类对称性可以自然收敛，而另一类对称性几乎总是发散。此外，我们的研究结果适用于没有对称性的损失函数，对于理解训练动态和解释相关实际问题具有普适性。 |
| [^143] | [GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention](https://arxiv.org/abs/2402.07191) | 本文提出了一种改进的图不变学习方法，通过稀疏性、软性和可微性原则来提取不变子图，从而提高图学习的泛化性能。 |
| [^144] | [Improving LSH via Tensorized Random Projection](https://arxiv.org/abs/2402.07189) | 本文提出了CP-E2LSH和TT-E2LSH两种方法，用于改进局部敏感哈希算法LSH，在处理张量数据的欧几里得距离和余弦相似度时能够提供更快和更空间有效的结果。 |
| [^145] | [Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2402.07182) | 这项研究介绍了一个名为IPRO的算法，利用分解任务为一系列单目标问题方法，可可靠地揭示多目标强化学习中实现最优表现的策略的帕累托前沿，同时提供收敛保证和未发现解的距离上限。 |
| [^146] | [MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization](https://arxiv.org/abs/2402.07180) | 本文提出了一种名为MAGNETO的边缘AI平台，通过从云端推向边缘进行增量人体活动学习，避免了云端与边缘设备之间的数据传输，实现了数据隐私保护、低延迟处理和高度个性化。 |
| [^147] | [GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring](https://arxiv.org/abs/2402.07164) | GeoFormer是一种基于视觉和序列Transformer的紧凑模型，用于从Sentinel-5P卫星图像中预测地表二氧化氮（NO2）浓度。该模型在准确性上表现出色。 |
| [^148] | [PASOA- PArticle baSed Bayesian Optimal Adaptive design](https://arxiv.org/abs/2402.07160) | PASOA是一种新的贝叶斯实验设计程序，通过提供连续的后验分布的准确估计，同时执行顺序设计优化和参数推断。该方法使用 stochastic optimization 和 tempered SMC 来最大化期望信息增益，并提供了一致性的最优设计估计。 |
| [^149] | [Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces](https://arxiv.org/abs/2402.07158) | 本文提出了一种基于大型语言模型的智能界面在软件项目中进行工作量和规模估计的方法，并通过比较传统方法，探讨了如何通过增强基于自然语言的问题规范来实现开发工作量的准确估计。 |
| [^150] | [Natural Language Reinforcement Learning](https://arxiv.org/abs/2402.07157) | 本研究将自然语言表示和强化学习原则相结合，提出了自然语言强化学习（NLRL）框架，解决了强化学习在样本效率低、解释性不足和缺乏监督信号等方面的限制问题，通过实验验证了其有效性和可解释性。 |
| [^151] | [A hybrid iterative method based on MIONet for PDEs: Theory and numerical examples](https://arxiv.org/abs/2402.07156) | 基于MIONet的混合迭代方法结合了传统的数值迭代求解器和神经操作符机器学习方法，具备卓越的加速效果。 |
| [^152] | [Explainable Global Wildfire Prediction Models using Graph Neural Networks](https://arxiv.org/abs/2402.07152) | 本研究提出了一个创新的基于图神经网络的全球野火预测模型，将全球气候和野火数据转化为图表示，解决了传统模型中的海洋数据缺失和远程依赖性问题，并展示了更高的预测准确性。同时，该模型还具有可解释性，揭示了其潜在价值。 |
| [^153] | [X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design](https://arxiv.org/abs/2402.07148) | X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。 |
| [^154] | [Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations](https://arxiv.org/abs/2402.07139) | 本研究通过实证评估，发现与经典车辆跟随模型不同，黑盒子模型对于最优的目标变量选择有不同的要求。 |
| [^155] | [Resampling methods for Private Statistical Inference](https://arxiv.org/abs/2402.07131) | 这项研究提出了两种私有变体的非参数bootstrap方法，用于在差分隐私的情况下构建置信区间。方法在计算效率和置信区间长度上相比现有方法有显著改进。 |
| [^156] | [An attempt to generate new bridge types from latent space of denoising diffusion Implicit model](https://arxiv.org/abs/2402.07129) | 本论文尝试使用去噪扩散隐式模型创新桥梁类型，通过易于理解的代数方法推导出函数公式，使用深度学习平台构建模型，并利用潜空间采样生成具有非对称结构的新桥梁类型。 |
| [^157] | [Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation](https://arxiv.org/abs/2402.07127) |  |
| [^158] | [Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation](https://arxiv.org/abs/2402.07118) | 本研究提出了一种基于人工智能的眼科质量评估系统，可以帮助远程眼科咨询，尤其是在低收入和中等收入国家。该系统能够即时反馈用户拍摄的眼部图像的质量，解决了当前用户拍摄图像质量不佳的问题。 |
| [^159] | [Towards Quantifying the Preconditioning Effect of Adam](https://arxiv.org/abs/2402.07114) | 本论文量化了Adam的预条件效应，结果表明Adam能够减轻病态条件的影响，但会受到维度的限制。 |
| [^160] | [Decoupling Learning and Decision-Making: Breaking the $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods](https://arxiv.org/abs/2402.07108) | 本文研究了在线线性规划的问题，并提出了一种新的算法框架，解决了一阶方法在线算法实现超过$\mathcal{O}(\sqrt{T})$遗憾的挑战，实现了$\mathcal{O}(T^{1/3})$的遗憾。 |
| [^161] | [Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning](https://arxiv.org/abs/2402.07107) | 这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。 |
| [^162] | [Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments](https://arxiv.org/abs/2402.07102) | 未来预测在部分可观测环境中学习 History Representation 具有很强的相关性和有效性。 |
| [^163] | [On the Complexity of First-Order Methods in Stochastic Bilevel Optimization](https://arxiv.org/abs/2402.07101) | 本文研究了在随机双层优化中使用一阶方法的复杂性，提出了一种简单的方法，使用$O(\epsilon^{-6})$次迭代可以收敛到一个$\epsilon$稳定点。 |
| [^164] | [Rethinking the Capacity of Graph Neural Networks for Branching Strategy](https://arxiv.org/abs/2402.07099) | 本文研究了图神经网络（GNNs）在分支策略中的容量，并发现了消息传递GNN (MP-GNN) 的表达能力的局限性以及另一种GNN结构 second-order folklore GNN (2-FGNN) 的通用逼近性质。 |
| [^165] | [Self-Correcting Self-Consuming Loops for Generative Model Training](https://arxiv.org/abs/2402.07087) | 本论文研究了使用合成数据进行生成模型训练时可能出现的自我消耗循环问题，并提出了一种通过引入理想的修正函数来稳定训练的方法。同时，我们还提出了自我修正函数来近似理想的修正函数，并通过实验证实了其有效性。 |
| [^166] | [Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and Phoneme Duration for Multi-Speaker Speech Synthesis](https://arxiv.org/abs/2402.07085) | 本文提出了一种基于语音韵律的方法，通过从音素和音素持续时间中提取说话人嵌入，模拟目标说话人的个体发音特征。实验证明，该方法可以实现有效的多说话人语音合成。 |
| [^167] | [Refined Sample Complexity for Markov Games with Independent Linear Function Approximation](https://arxiv.org/abs/2402.07082) | 本文在独立线性函数逼近的马尔科夫博弈中，通过改进AVLPR框架，提出了基于数据依赖的悲观估计方法，解决了多智能体的诅咒问题。 |
| [^168] | [The Relevance Feature and Vector Machine for health applications](https://arxiv.org/abs/2402.07079) | 本文提出了一种名为RFVM的模型，用于解决临床前瞻性研究中的大数据问题。该模型结合了贝叶斯公式、联合优化和集成修剪等特点，以提高模型的效果和效率。 |
| [^169] | [Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine](https://arxiv.org/abs/2402.07069) | 这篇论文介绍了一种使用大型语言模型自动生成自动机来编码高级知识，加速强化学习过程的算法，并证明了其在多个任务上的有效性和优越性。 |
| [^170] | [Differentially Private Range Queries with Correlated Input Perturbation](https://arxiv.org/abs/2402.07066) | 本研究提出了一种具有相关输入扰动的差分隐私范围查询的局部机制，通过级联采样算法实现，实验表明在保障近乎最优的效用的同时，与输出扰动方法在实践中具有竞争力。 |
| [^171] | [Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise](https://arxiv.org/abs/2402.07062) | 本研究提出了一种基于凸优化方法和不精确预测模型的新UCB类型算法，用于解决具有重和超重对称噪声的随机赌博机问题。通过理论和实验结果表明，在奖励中存在对称噪声的情况下，该算法能够达到更好的性能，相比于一般下界能够获得更小的遗憾界。即使奖励分布没有期望，该算法仍然有效。 |
| [^172] | [Understanding the Training Speedup from Sampling with Approximate Losses](https://arxiv.org/abs/2402.07052) | 本文研究利用近似损失进行样本采样的训练加速方法，通过贪婪策略选择具有大约损失的样本，减少选择的开销，并证明其收敛速度优于随机选择。同时开发了使用中间层表示获取近似损失的SIFT方法，并在训练BERT模型上取得了显著的提升。 |
| [^173] | [$L^*LM$: Learning Automata from Examples using Natural Language Oracles](https://arxiv.org/abs/2402.07051) | 该论文提出了一个名为 $L^*LM$ 的算法，通过自然语言和演示学习 DFA，提高了数据效率，具备强大的少样本学习能力。 |
| [^174] | [A Tale of Tails: Model Collapse as a Change of Scaling Laws](https://arxiv.org/abs/2402.07043) | 本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。 |
| [^175] | [Distilling Symbolic Priors for Concept Learning into Neural Networks](https://arxiv.org/abs/2402.07035) | 本论文通过元学习的方法，将符号贝叶斯模型的先验知识提取到神经网络中，使神经网络具有显示归纳偏见的能力，从而加快对抽象概念的学习。 |
| [^176] | [Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://arxiv.org/abs/2402.07033) | 本文介绍了Fiddler，一种用于Mixture-of-Experts模型的资源高效推断引擎，通过CPU-GPU编排实现最小化数据传输，相比现有方法提高了一个数量级的推断速度。 |
| [^177] | [Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration](https://arxiv.org/abs/2402.07031) | 本论文研究了实例级别的合成数据质量与安全感知，引入了四种超越纯视觉特征的合成数据质量，并提出了优化方法来减少合成和真实图像之间的质量差距。 |
| [^178] | [Semi-Supervised Learning for Bilingual Lexicon Induction](https://arxiv.org/abs/2402.07028) | 本论文提出了一个半监督学习方法，将两种语言对应的连续词表示集对齐到一个共同的空间，推断双语词典。该方法利用无监督学习的基础，在学习新语言时，整合已有语言集的知识，通过排序方法实现词典诱导。 |
| [^179] | [Quantum Speedup for Spectral Approximation of Kronecker Products](https://arxiv.org/abs/2402.07027) | 本研究引入一种创新方法，使用量子方法高效地解决Kronecker积$A_1 \otimes A_2$的谱逼近问题，将时间复杂度降低到$O_{d,\epsilon}(\sqrt{n})$。 |
| [^180] | [Generalization Error of Graph Neural Networks in the Mean-field Regime](https://arxiv.org/abs/2402.07025) | 该论文在均场极限下提供了一个理论框架，评估了图神经网络在过参数化情况下的泛化误差，通过推导出收敛速度为$O(1/n)$的上界，为我们对网络在未见数据上的性能提供了理论保证。 |
| [^181] | [Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations](https://arxiv.org/abs/2402.07023) | 该论文综合评估了开源LLM和谷歌的多模态LLM Gemini 在医学推理、幻觉检测和医学视觉问答任务上的能力。Gemini在诊断准确性方面落后于最先进模型，且易出现幻觉、过度自信和知识盲点。采用提示策略可以提高性能。 |
| [^182] | [Informativeness of Reward Functions in Reinforcement Learning](https://arxiv.org/abs/2402.07019) | 本文研究了在强化学习中设计信息丰富的奖励函数的问题，提出了一种新的奖励信息性准则来评估代理的当前策略在接受特定奖励函数的奖励后的改善情况。 |
| [^183] | [FedImpro: Measuring and Improving Client Update in Federated Learning](https://arxiv.org/abs/2402.07011) | 本文提出了FedImpro方法，通过生成改进的本地模型来减轻联邦学习中的客户漂移问题。该方法通过分析本地训练的泛化贡献，并利用类似的条件分布进行训练，增强了泛化贡献并减小了梯度的差异性。 |
| [^184] | [An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation](https://arxiv.org/abs/2402.07008) | 使用优化框架和迁移学习方法，作者构建了一个基于3D U-Net模型的脑肿瘤分割方法，通过应用各种预处理和后处理技术，在三个挑战上实现了较高的Dice分数。 |
| [^185] | [Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off](https://arxiv.org/abs/2402.07002) | 本论文提出了一种名为FedCEO的新型联邦学习框架，在保护用户隐私的同时，通过让客户端相互协作，实现了对模型效用和隐私之间的权衡。通过高效的张量低秩近端优化，该框架能够恢复被打断的语义信息，并在效用-隐私权衡方面取得了显著的改进。 |
| [^186] | [A Change Detection Reality Check](https://arxiv.org/abs/2402.06994) | 该论文通过实验证明，一个简单的U-Net分割基线仍然是进行变化检测任务的顶尖表现者。 |
| [^187] | [Guided Sketch-Based Program Induction by Search Gradients](https://arxiv.org/abs/2402.06990) | 提出了一种通过搜索梯度使用进化策略学习参数化程序的框架，该框架使程序归纳成为解决多种类型任务的可行解决方案。 |
| [^188] | [Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization](https://arxiv.org/abs/2402.06974) | 本文提出了一种基于超网络的联邦融合算法hFedF，用于解决联邦领域泛化问题。该算法通过非线性融合客户模型，实现了对底层数据分布的全面理解，并在联邦学习中个性化和泛化之间达到了优秀的平衡。 |
| [^189] | [Event-Keyed Summarization](https://arxiv.org/abs/2402.06973) | 事件关键摘要（EKS）是一种新颖的任务，旨在为特定事件生成上下文化的摘要。我们提出了一个基准数据集MUCSUM，并展示了EKS与传统摘要和结构到文本的比较结果。 |
| [^190] | [In-Context Data Distillation with TabPFN](https://arxiv.org/abs/2402.06971) | TabPFN是一种适用于tabular数据的Transformer模型，具有卓越的上下文学习能力。为了解决TabPFN在实际场景中的数据规模限制问题，我们提出了上下文数据蒸馏(ICD)方法，通过优化TabPFN的上下文，使其能够处理更大规模的数据集，提高性能。 |
| [^191] | [Contextual Stochastic Vehicle Routing with Time Windows](https://arxiv.org/abs/2402.06968) | 这项研究探讨了具有时间窗口和随机旅行时间的车辆路径规划问题，并引入了上下文情境，通过使用历史数据提供近似解来最小化总运输成本和预期迟到罚款。 |
| [^192] | [DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction](https://arxiv.org/abs/2402.06966) | 提出了一种从循环神经网络模型中提取状态机的方法，提高了RNN模型的可解释性，并通过提取的状态机来改进RNN的测试和监测。 |
| [^193] | [Tree Ensembles for Contextual Bandits](https://arxiv.org/abs/2402.06963) | 本论文提出了一种基于树集成的情境多臂老虎机新框架，通过整合两种广泛使用的老虎机方法，在标准和组合设置中实现了优于基于神经网络的方法的性能，在减少后悔和计算时间方面表现出更出色的性能。 |
| [^194] | [Architectural Neural Backdoors from First Principles](https://arxiv.org/abs/2402.06957) | 本论文提出了从第一原理构建架构神经后门的方法，并描述了12种不同类型的架构后门。同时，通过构建一个任意触发检测器，展示了无需人工监督即可为架构引入后门的能力。 |
| [^195] | [Training dynamics in Physics-Informed Neural Networks with feature mapping](https://arxiv.org/abs/2402.06955) | 本研究探究了使用特征映射层的物理引导神经网络（PINNs）的训练动态，通过极限共轭核和神经切向核揭示了模型的收敛和泛化。我们提出了一种替代基于傅里叶变换的特征映射的条件正定径向基函数，证明了该方法在各种问题集中的有效性，并可以轻松实现在坐标输入网络中。这为广泛的PINNs研究带来了益处。 |
| [^196] | [OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning](https://arxiv.org/abs/2402.06954) | OpenFedLLM是一个简洁、集成、研究友好的框架/代码库，通过联邦学习在分散的私有数据上实现了大规模语言模型的协作和隐私保护训练，解决了公开数据枯竭的问题。 |
| [^197] | [Efficient Incremental Belief Updates Using Weighted Virtual Observations](https://arxiv.org/abs/2402.06940) | 本文介绍了在贝叶斯统计模型中使用加权虚拟观测进行增量信念更新的算法解决方案，该方案通过构建一组加权观测来调节模型，实现与原始后验相同的推断结果。 |
| [^198] | [Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities](https://arxiv.org/abs/2402.06938) | 这项研究介绍了一种使用谈判能力的分布式基础设施高效资源调度的方法，通过引入一种基于模糊逻辑的代理自动谈判系统，优化云计算提供商和客户之间的协议，以提高效率。 |
| [^199] | [Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts](https://arxiv.org/abs/2402.06937) | 本研究探讨了在分布迁移情况下使用贝叶斯和非贝叶斯方法进行不确定性估计的可行性，旨在实现在医学图像分割任务中可靠和可信的诊断预测。 |
| [^200] | [Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute](https://arxiv.org/abs/2402.06932) | 本文介绍了一种名为LAGRA的可解释分类算法，它通过学习重要的小属性子图来实现图分类，并优化其属性向量。这种方法能有效区分不同类别，并且能够全面地探索所有潜在重要的子图结构。 |
| [^201] | [ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G](https://arxiv.org/abs/2402.06931) | ORIENT是一种面向6G的优先权感知节能方法，旨在通过解决服务实例放置和分配、路径选择和请求优先级的联合问题来最大化系统的整体利润，并在长时间内最小化能耗。 |
| [^202] | [CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition](https://arxiv.org/abs/2402.06923) | 提出了一种名为CochCeps-Augment的方法，利用基于Cochlear Cepstrum的掩蔽增强任务进行自监督对比学习，提高了语音情感识别的性能和噪声鲁棒性。 |
| [^203] | [Whispers in the Machine: Confidentiality in LLM-integrated Systems](https://arxiv.org/abs/2402.06922) | 本研究提供了一种评估LLM集成系统保密性的系统化方法，通过形式化一个"秘密密钥"游戏来捕捉模型隐藏私人信息的能力。评估了八种攻击和四种防御方法，发现当前的防御方法缺乏泛化性能。 |
| [^204] | [Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System](https://arxiv.org/abs/2402.06921) | 本研究通过比较四种聚类技术的性能，提出了一种强大的混合模型用于监督学习任务，以预测太阳能热系统的输出温度。 |
| [^205] | [TREET: TRansfer Entropy Estimation via Transformer](https://arxiv.org/abs/2402.06919) | 本研究提出了TREET，一种基于Transformer的传输熵估计方法，通过引入Donsker-Vardhan表示法和注意力机制，实现了对稳定过程的传输熵估计。我们设计了估计TE的优化方案，并展示了通过联合优化方案优化通信通道容量和估计器的记忆能力。 |
| [^206] | [Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought](https://arxiv.org/abs/2402.06918) | 本文提出了一种基于直接两两比较的方法，通过利用LLMs的噪声反馈，直接识别出最有潜力的中间思维，从而生成优秀的思维链。 |
| [^207] | [Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks](https://arxiv.org/abs/2402.06912) | 本研究通过使用进化策略(ES)来优化神经网络的权重，以通过直接策略搜索解决深度强化学习(DRL)基准问题。研究结果显示，ES可以在许多基准任务中找到有效的线性策略，与当前使用更大网络的DRL方法相比，这表明当前的基准问题比以往认为的更容易解决。 |
| [^208] | [Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions](https://arxiv.org/abs/2402.06908) | 本文研究了图神经网络的瓶颈问题，并提出了一种名为高拓扑神经网络的方法，通过引入高阶相互作用和多关系归纳偏置来缓解这些问题。 |
| [^209] | [GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators](https://arxiv.org/abs/2402.06894) | GenTranslate是一个新的翻译任务生成模型，通过利用大型语言模型的丰富语言知识和强大推理能力，可以从N-best列表中生成更高质量的翻译结果。 |
| [^210] | [Understanding Test-Time Augmentation](https://arxiv.org/abs/2402.06892) | 这篇论文旨在理解测试时间增强（TTA）方法并为其提供理论保证，同时澄清其行为。 |
| [^211] | [Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF](https://arxiv.org/abs/2402.06886) | 本文提出了一种基于惩罚的方法来解决Bilevel强化学习和RLHF问题，这是首个有原则的算法框架。通过理论分析和实验证明了算法的有效性。 |
| [^212] | [DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine](https://arxiv.org/abs/2402.06885) | DimVis是一种基于可解释性提升机器的可视化工具，用于解释维度约减中的视觉聚类。它通过对特征相关性的解释，提供对高维数据中视觉聚类的解释和分析。 |
| [^213] | [Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning](https://arxiv.org/abs/2402.06884) | 本文研究结构冗余的低秩逼近在自监督学习中的应用，提出了一个逼近冗余组件的新方法，并通过分析过量风险来支持理论。 |
| [^214] | [Discriminative Adversarial Unlearning](https://arxiv.org/abs/2402.06864) | 该论文提出了一种基于最小最大优化范式的机器反学习框架，利用强大的成员推断攻击来实现反学习，同时保持其总体性能，并增强了反学习能力。 |
| [^215] | [LiRank: Industrial Large Scale Ranking Models at LinkedIn](https://arxiv.org/abs/2402.06859) | LiRank是领英的一个大规模排名框架，它应用了最先进的建模架构和优化方法，并提出了新的建模改进和技术，通过A/B测试取得了有效的结果。 |
| [^216] | [For Better or For Worse? Learning Minimum Variance Features With Label Augmentation](https://arxiv.org/abs/2402.06855) | 本研究分析了标签增强方法中标签增强的作用。研究证明，在线性可分数据上使用标签增强训练的线性模型只能学习到最小方差特征，而标准训练可以学习到更高方差特征。此外，标签平滑和Mixup对于训练数据的对抗扰动可能不太鲁棒。 |
| [^217] | [Gyroscope-Assisted Motion Deblurring Network](https://arxiv.org/abs/2402.06854) | 本文提出了一个陀螺仪辅助的运动去模糊网络框架，通过利用IMU数据合成和恢复运动模糊图像，解决了实际应用中的训练数据对齐和信息限制问题。 |
| [^218] | [RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations](https://arxiv.org/abs/2402.06827) | 该论文提出了一种名为RAMP的框架，旨在增强对多个$l_p$扰动的对抗鲁棒性。通过分析不同$l_p$攻击之间的权衡关系，并设计逻辑配对损失来提高准确性和鲁棒性的平衡。同时，通过将自然训练与对抗训练相结合，整合有用信息以调和准确性和鲁棒性的权衡。 |
| [^219] | [Forecasting Events in Soccer Matches Through Language](https://arxiv.org/abs/2402.06820) | 本文提出了一种使用语言模型预测足球比赛中下一个事件的方法，该方法受到大型语言模型方法的启发。通过深度学习和WyScout数据集，该方法在预测准确性方面明显超过了以往的方法。该方法的应用包括博彩和比赛分析，并提供了一个模拟骨架用于构建分析流水线。 |
| [^220] | [Monitored Markov Decision Processes](https://arxiv.org/abs/2402.06819) | 这篇论文介绍了一种新颖且通用的强化学习框架——监控马尔可夫决策过程(Monitored MDPs)。在这个框架中，代理不能始终观察到奖励，提出了算法来解决这个新颖的场景。 |
| [^221] | [Towards a Systematic Approach to Design New Ensemble Learning Algorithms](https://arxiv.org/abs/2402.06818) | 本研究针对集成学习算法的设计提出了一种系统的方法，通过创新的偏差-方差-多样性分解框架指导，使用神经网络作为基学习者，生成了21种新的集成算法。 |
| [^222] | [Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models](https://arxiv.org/abs/2402.06815) | 本文介绍了将大型事件模型（LEMs）应用于足球分析领域的创新方法。通过学习“足球语言”，LEMs可以预测后续事件的变量，从而模拟比赛并预测球员在不同团队背景下的表现。研究通过对2017-2018英超赛季使用WyScout数据集进行LEMs的精细调整，发现了LEMs在足球分析中的有效性和局限性，同时突出了该模型在预测球队排名和探索高级场景方面的潜力。 |
| [^223] | [A Kalman Filter Based Framework for Monitoring the Performance of In-Hospital Mortality Prediction Models Over Time](https://arxiv.org/abs/2402.06812) | 本研究基于卡尔曼滤波提出了一个框架，通过调整样本大小和类别分布，以实现对不同时间段性能指标的公平比较。 |
| [^224] | [Evaluating Co-Creativity using Total Information Flow](https://arxiv.org/abs/2402.06810) | 本研究旨在通过使用总信息流来定量评估音乐中的共同创造力过程，并通过定性研究证明该方法与人类感知相匹配。 |
| [^225] | [Explain Variance of Prediction in Variational Time Series Models for Clinical Deterioration Prediction](https://arxiv.org/abs/2402.06808) | 本文提出了使用delta方法确定性地近似预测的变异性的方法，并采用SHAP方法来归因于变异的贡献。该方法适用于临床恶化预测中的变分时间序列模型，可以在提高预测精度的同时提供解释性。 |
| [^226] | [Towards Principled Assessment of Tabular Data Synthesis Algorithms](https://arxiv.org/abs/2402.06806) | 本文提出了一个原则性和系统化的评估框架来评估表格数据合成算法，包括保真度、隐私性和实用性等新指标，以解决现有评估指标的限制。通过这个框架，对不同算法进行了比较和总结。 |
| [^227] | [Generative Nowcasting of Marine Fog Visibility in the Grand Banks area and Sable Island in Canada](https://arxiv.org/abs/2402.06800) | 本研究利用生成式深度学习方法在加拿大大滩地区和萨布尔岛上预测海洋雾能见度，为30分钟和60分钟的可见度阈值提供了生成预测。 |
| [^228] | [ForestColl: Efficient Collective Communications on Heterogeneous Network Fabrics](https://arxiv.org/abs/2402.06787) | ForestColl是一种针对任意网络拓扑生成高效调度的工具，通过构建广播/聚合生成跨越树的通信调度，实现了理论上的最小网络拥塞，并在实验中表现出高于供应商自带通信库的性能。 |
| [^229] | [Transfer learning with generative models for object detection on limited datasets](https://arxiv.org/abs/2402.06784) | 本论文提出了一个适用于通用情景的基于生成模型的迁移学习框架，用于解决有限数据集上的目标检测任务。 |
| [^230] | [Learn to Teach: Improve Sample Efficiency in Teacher-student Learning for Sim-to-Real Transfer](https://arxiv.org/abs/2402.06783) | 本文提出了一种样本效率学习框架，名为学习教学（L2T），通过回收教师智能体收集的经验，解决了教师-学生学习中的样本效率问题。 |
| [^231] | [Retrosynthesis Prediction via Search in (Hyper) Graph](https://arxiv.org/abs/2402.06772) | 本研究提出了一种基于半模板的逆向合成方法，通过在产物分子图和离开基团超图中进行搜索，以处理复杂的反应。 |
| [^232] | [Scalable Kernel Logistic Regression with Nystr\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling](https://arxiv.org/abs/2402.06763) | 本文介绍了使用Nystr\"om近似方法解决大规模数据集上核逻辑回归的可扩展性问题。研究提供了理论分析并验证了不同的地标选择方法的性能。 |
| [^233] | [Embedding Compression for Teacher-to-Student Knowledge Transfer](https://arxiv.org/abs/2402.06761) | 该论文提出了一种嵌入式压缩模块，通过可训练的教师转换来得到紧凑的教师嵌入，提高了分类性能，尤其对于无监督教师嵌入，并且使用嵌入进行引导训练的学生模型展现出更强的泛化能力。 |
| [^234] | [Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion](https://arxiv.org/abs/2402.06756) | 本文分析了对称矩阵完成问题中梯度下降算法的收敛性。研究结果表明，在非正则化的情况下，使用小初始化的梯度下降算法可以收敛到真实的矩阵解，即使在过度参数化的情况下也成立。在过度参数化的情况下，几乎线性的收敛速度可以在获得足够多的观测条目后得到保证。 |
| [^235] | [Low-Rank Learning by Design: the Role of Network Architecture and Activation Linearity in Gradient Rank Collapse](https://arxiv.org/abs/2402.06751) | 本文对DNNs中的梯度秩进行了全面研究，发现低秩学习是某些DNN架构固有的特征，而不仅仅是训练的最后阶段的现象。 |
| [^236] | [ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning](https://arxiv.org/abs/2402.06737) | 本文介绍了一种新颖的自监督学习方法ExGRG，它通过显式生成关系图来解决图结构数据上的挑战，将先验领域知识和在线提取的信息纳入自监督学习中，取得了显著的成功。 |
| [^237] | [Corruption Robust Offline Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2402.06734) | 我们研究了具有人类反馈的强化学习中的数据腐败鲁棒性问题，并设计了新颖的离线方法来处理损坏的数据，并且在不同的数据生成分布假设下具有性能保证。 |
| [^238] | [NICE: To Optimize In-Context Examples or Not?](https://arxiv.org/abs/2402.06733) | 通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。 |
| [^239] | [A Scalable Algorithm for Individually Fair K-means Clustering](https://arxiv.org/abs/2402.06730) | 本文提出了一个可扩展的算法用于个体公平的K均值聚类问题，该算法比现有的算法更快并且能够产生更低成本的解决方案。 |
| [^240] | [Dynamic Graph Information Bottleneck](https://arxiv.org/abs/2402.06716) | 动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。 |
| [^241] | [Learning-augmented Online Algorithm for Two-level Ski-rental Problem](https://arxiv.org/abs/2402.06715) | 本文研究了两层滑雪租赁问题，提出了一种学习增强的在线算法，该算法通过平衡前期成本和潜在未来费用的权衡来最小化总成本。 |
| [^242] | [Electricity Price Forecasting in the Irish Balancing Market](https://arxiv.org/abs/2402.06714) | 本研究针对爱尔兰的平衡市场应用了各种已被证明成功的价格预测技术，并发现在日前市场表现良好的模型在平衡市场中效果不佳，突显了这些市场的不同特点。 |
| [^243] | [Multi-class real-time crash risk forecasting using convolutional neural network: Istanbul case study](https://arxiv.org/abs/2402.06707) | 本文展示了在多类实时崩溃风险预测中，使用卷积神经网络从交通和天气数据中学习相关特征的性能，并提出了一种基于三个时间段的实时崩溃概率预测模型。 |
| [^244] | [CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs](https://arxiv.org/abs/2402.06706) | CoRe-GD是一个使用GNNs的可扩展图可视化框架，通过基于中间节点位置的位置调整技术来优化图的布局，并且具有较低的时间复杂度。 |
| [^245] | [Privacy Profiles for Private Selection](https://arxiv.org/abs/2402.06701) | 本文提出了一种易于使用的方法，通过限制基础算法的隐私配置文件来界定ReportNoisyMax和PrivateTuning的隐私配置文件。 |
| [^246] | [Entropy-Regularized Token-Level Policy Optimization for Large Language Models](https://arxiv.org/abs/2402.06700) | 本文提出了一种熵正则化的令牌级策略优化方法（ETPO），用于优化大规模语言模型（LLMs）。该方法能够通过直接与任务特定环境进行交互，并解决在如何分配令牌级学分和最大化奖励之间的冲突问题。 |
| [^247] | [Feed-Forward Neural Networks as a Mixed-Integer Program](https://arxiv.org/abs/2402.06697) | 这项研究探索了将训练的修正线性单元(ReLU)神经元作为混合整数规划(MIP)的形式，并将MIP模型应用于训练神经网络。研究发现MIP技术在不同的神经网络架构中具有广泛的应用潜力，包括二进制DNN和二值化DNN。 |
| [^248] | [FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models](https://arxiv.org/abs/2402.06696) | 本研究提出了一种名为FL-NAS的基于大型语言模型的神经架构搜索框架，该框架能够在模型准确性、公平性和硬件部署效率三个方面达到卓越的性能。 |
| [^249] | [Integrating LLMs for Explainable Fault Diagnosis in Complex Systems](https://arxiv.org/abs/2402.06695) | 本研究介绍了一种集成系统，通过将基于物理的诊断工具与大型语言模型相结合，帮助复杂系统实现可解释的故障诊断。该系统不仅能够识别故障，还能够提供清晰易懂的故障原因和影响解释，提高自主系统的可靠性和透明度。 |
| [^250] | [Scaling Intelligent Agents in Combat Simulations for Wargaming](https://arxiv.org/abs/2402.06694) | 本研究利用分层强化学习提升战争游戏中智能代理的性能，以加速决策速度和提高决策质量。 |
| [^251] | [HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation](https://arxiv.org/abs/2402.06692) | HistoHDR-Net是一种用于单个LDR到HDR图像转换的简单而有效的方法，通过融合直方图均衡化的LDR图像和自注意力引导，恢复HDR图像的细节。 |
| [^252] | [Neural Models for Source Code Synthesis and Completion](https://arxiv.org/abs/2402.06690) | 本论文提出了一种基于序列到序列的深度学习模型和训练方法，用于将自然语言转化为可编译的代码片段，并为开发人员提供源代码建议和自动补全功能。这一方法能够提取开发者的编码意图并准确推断类型、名称和上下文等信息。 |
| [^253] | [A Study on Stock Forecasting Using Deep Learning and Statistical Models](https://arxiv.org/abs/2402.06689) | 本研究使用深度学习和统计模型进行股票预测，通过回顾多个算法并使用s&p 500指数数据进行训练和测试，旨在找到最佳的股票价格预测方法。 |
| [^254] | [Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results](https://arxiv.org/abs/2402.06688) | 本文比较了机器学习和统计方法在数字高程模型（DEM）校正中的性能，结果表明梯度提升决策树（GBDT）在提高DEM垂直精度方面具有潜力。 |
| [^255] | [Ai4Fapar: How artificial intelligence can help to forecast the seasonal earth observation signal](https://arxiv.org/abs/2402.06684) | 本文研究了使用多元变压器模型预测欧洲和北非地区FAPAR时序轨迹的潜力。结果显示，在一个月的预测范围内，变压器模型优于基准模型，尤其当与天气数据结合使用时效果更好。 |
| [^256] | [Sound Source Separation Using Latent Variational Block-Wise Disentanglement](https://arxiv.org/abs/2402.06683) | 本文提出了一种混合经典数字信号处理/深度神经网络（DSP / DNN）方法，用于声源分离。通过在合适设计的潜在空间中处理分离任务，将单通道欠定分离任务转换为多通道过定问题。实验证明，该方法在面对未见过的分布时具有鲁棒性。 |
| [^257] | [Private Knowledge Sharing in Distributed Learning: A Survey](https://arxiv.org/abs/2402.06682) | 这篇论文提供了关于分布式学习中的私人知识共享的全面调查，分析了在领先的分布式学习架构中使用的各种知识组件，旨在揭示现有的解决方法和未来的研究方向。 |
| [^258] | [Social Physics Informed Diffusion Model for Crowd Simulation](https://arxiv.org/abs/2402.06680) | 本文提出了一种叫做SPDiff的基于社会物理的扩散模型，用于人群模拟。模型综合考虑了人群的互动和历史信息，通过逆向扩散过程生成下一个时间段内行人运动的分布。此外，借鉴社会力模型，并利用人群互动的等变性属性增强了模型的性能。为了解决长期模拟中的误差累积问题，引入了多层次扩散模型和最小二乘法进行参数估计。 |
| [^259] | [Can machine learning predict citizen-reported angler behavior?](https://arxiv.org/abs/2402.06678) | 本研究利用机器学习方法，使用环境、社会经济、渔业管理目标和事件等辅助数据，预测公民报告的钓鱼者行为。结果表明，辅助数据能够以较高准确度预测特定地区和单个水域的钓鱼者行为。 |
| [^260] | [A Masked language model for multi-source EHR trajectories contextual representation learning](https://arxiv.org/abs/2402.06675) | 通过使用多源电子健康记录轨迹并训练模型来解决疾病和干预之间相互作用的问题的背景表示学习。 |
| [^261] | [Understanding Practical Membership Privacy of Deep Learning](https://arxiv.org/abs/2402.06674) | 该论文利用最先进的成员推理攻击方法系统地测试了细调大型图像分类模型的实际隐私漏洞，并发现数据集中每个类别的示例数量以及训练结束时的大梯度与成员推理攻击的漏洞之间存在关联。 |
| [^262] | [Weather Prediction with Diffusion Guided by Realistic Forecast Processes](https://arxiv.org/abs/2402.06666) | 这种新方法利用扩散模型进行天气预测，能够在相同的建模框架下实现直接和迭代预测，并且可以嵌入NWP预测，提高可信赖性和预测性能。 |
| [^263] | [The Essential Role of Causality in Foundation World Models for Embodied AI](https://arxiv.org/abs/2402.06665) | 基于因果关系的基础世界模型对于具身人工智能的发展至关重要，当前的基础模型无法准确建模与现实世界的物理相互作用。因果关系的研究有助于构建真实世界模型，提高对可能相互作用结果的准确预测能力。 |
| [^264] | [Sign Rank Limitations for Attention-Based Graph Decoders](https://arxiv.org/abs/2402.06662) | 该论文研究了基于注意力的图解码器在表征容量方面的限制问题，并提出了可以解决该问题的简单修正方案，而不改变内积框架。 |
| [^265] | [Authentication and integrity of smartphone videos through multimedia container structure analysis](https://arxiv.org/abs/2402.06661) | 通过分析多媒体容器结构，本论文研究了智能手机视频的身份验证和完整性，以提高在社交网络和通信应用程序中共享视频的安全性和可信度。 |
| [^266] | [Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models](https://arxiv.org/abs/2402.06659) | Shadowcast是一种隐秘的数据污染攻击方法，可以通过伪装成良性图像和匹配文本来操纵视觉语言模型的响应。它包括标签攻击和说服攻击，可以混淆类别标签并编写有说服力的描述。使用仅50个毒样本，Shadowcast能够高效实现攻击者的意图。 |
| [^267] | [DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation](https://arxiv.org/abs/2402.06656) | DiffsFormer是一种利用Diffusion Transformer和人工智能生成样本的方法，用于在股票预测中解决数据稀缺性和数据同质性的问题。 |
| [^268] | [Adversarial Text Purification: A Large Language Model Approach for Defense](https://arxiv.org/abs/2402.06655) | 本文研究了防御文本分类器中对抗性净化方法的有效性，并提出了一种基于大型语言模型加以净化的方法。 |
| [^269] | [Using remotely sensed data for air pollution assessment](https://arxiv.org/abs/2402.06653) | 该研究利用遥感数据和机器学习模型创建了一个可以在无观测数据地点推断污染物浓度的模型，为空气污染研究和排放监测提供了重要手段。 |
| [^270] | [Diffusion Model-based Probabilistic Downscaling for 180-year East Asian Climate Reconstruction](https://arxiv.org/abs/2402.06646) | 这项研究介绍了一种基于扩散模型的概率降尺度方法（DPDM），可以高效地将数据从1°降尺度到0.1°分辨率，并生成大量的集合成员以评估降尺度的不确定性。该方法应用于生成东亚地区180年的月表面变量数据集，提供了更详细的观察。 |
| [^271] | [From GARCH to Neural Network for Volatility Forecast](https://arxiv.org/abs/2402.06642) | 该论文通过建立GARCH模型和神经网络模型之间的等价关系，提出了一种名为GARCH-NN的创新方法，用于构建基于神经网络的波动率模型，并将GARCH模型中的波动率风格化事实融入到神经网络中。 |
| [^272] | [Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting](https://arxiv.org/abs/2402.06638) | 本文提出了一种使用注意力联合聚合的Transformer模型，用于时间序列股票预测，旨在解决传统训练方案中存在的过拟合、数据稀缺和隐私问题。 |
| [^273] | [Large (and Deep) Factor Models](https://arxiv.org/abs/2402.06635) | 本文通过证明一个足够宽而任意深的神经网络训练出来的投资组合优化模型与大型因子模型等效，打开了深度学习在此领域中的黑盒子，并提供了一种封闭形式的推导方法。研究实证了不同架构选择对模型性能的影响，并证明了随着深度增加，模型在足够多数据下的表现逐渐提升，直至达到饱和。 |
| [^274] | [SocraSynth: Multi-LLM Reasoning with Conditional Statistics](https://arxiv.org/abs/2402.06634) | SocraSynth是一个多语言模型推理平台，通过使用条件统计和系统化的语境增强技术，以及可调节的辩论争议程度，解决了大型语言模型(LLMs)面临的偏见、幻觉和推理能力不足等问题。 |
| [^275] | [MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive and Dynamic Stock Investment Prediction](https://arxiv.org/abs/2402.06633) | 提出了多关系动态图神经网络（MDGNN）框架，通过利用离散动态图全面捕捉股票之间的多方面关系及其随时间的演变，并利用Transformer结构的能力对多重关系的时间演变进行编码，从而提供了股票及其关联实体之间相互关系的完整视角。 |
| [^276] | [Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss](https://arxiv.org/abs/2402.06187) | Premier-TACO是一种多任务特征表示学习方法，通过预训练通用特征表示，并引入负例抽样策略来提高时序行动对比学习的计算效率，从而显著增强了对新颖动作的少样本模仿学习的效果。 |
| [^277] | [An Inexact Halpern Iteration for with Application to Distributionally Robust Optimization](https://arxiv.org/abs/2402.06033) | 本文研究了确定性和随机环境下Halpern迭代算法的不精确变种，通过适当选择不精确的容差，这些变种展现出O(k^-1)的收敛速度，同时具有竞争性的收敛特性。并且我们还展示了这些方法在两类数据驱动Wasserstein分布鲁棒优化问题中的应用，以及在分布鲁棒学习中使用随机一阶方法进行不精确计算的能力。 |
| [^278] | [\textit{MinMaxMin} $Q$-learning](https://arxiv.org/abs/2402.05951) | \textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。 |
| [^279] | [\textit{SQT} -- \textit{std} $Q$-target](https://arxiv.org/abs/2402.05950) | SQT是一种基于Q-学习的保守型actor-critic算法，利用Q网络的标准差作为一种“不确定性惩罚”，成功解决了过高估计偏差问题，相较于TD3的Q-target公式具有更好的性能优势。 |
| [^280] | [PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models](https://arxiv.org/abs/2402.05868) | PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。 |
| [^281] | [Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning](https://arxiv.org/abs/2402.05817) | 本研究使用了最新的YOLO V7目标检测方法，通过对医学图像进行修改和训练，成功提高了肾脏在磁共振成像中的检测效果，为肾脏疾病的诊断和治疗提供了有力支持。 |
| [^282] | [Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes](https://arxiv.org/abs/2402.05406) | 本文提出了一种仅使用前向传递的LLM结构化修剪方法，通过Bonsai生成的修剪模型在性能上优于梯度-based结构化修剪方法，并且速度是半结构化修剪模型的两倍。 |
| [^283] | [BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs](https://arxiv.org/abs/2402.05301) | 本文介绍了BIKED++数据集，其中包含了140万个自行车设计的图像和参数化CAD文件。该数据集可以用于训练跨模态预测模型，例如使用参数化表示来准确估计图像的特征嵌入。该数据集也已公开，可供研究者使用。 |
| [^284] | [Do Transformer World Models Give Better Policy Gradients?](https://arxiv.org/abs/2402.05290) | 在强化学习中，通过使用变形器世界模型来预测未来奖励并进行策略梯度学习通常变得不可行。研究人员发现常用的变形器世界模型会产生迂回的梯度路径，对于长距离的策略梯度是有害的。为了解决这个问题，他们提出了一种名为Actions World Models (AWMs)的世界模型，可以提供更直接的梯度传播路径。 |
| [^285] | [Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models](https://arxiv.org/abs/2402.05210) | 这篇论文提出了一种采用分割引导扩散模型的解剖可控医学图像生成方法，通过随机掩模消融训练算法实现对解剖约束的条件化，同时提高了网络对解剖真实性的学习能力。 |
| [^286] | [ApiQ: Finetuning of 2-Bit Quantized Large Language Model](https://arxiv.org/abs/2402.05147) | 这项工作介绍了一种名为ApiQ的新型量化框架，通过同时初始化LoRA组件和量化大型语言模型的权重，恢复量化过程中丢失的信息，维持原始模型的激活精度并减轻误差传播。 |
| [^287] | [Denoising Diffusion Probabilistic Models in Six Simple Steps](https://arxiv.org/abs/2402.04384) | 本论文提供了一个简单、全面、干净且清晰的介绍去噪扩散概率模型（DDPM）的方法，强调了从连续时间极限的视角出发，以提供更好的理解和实际性能。 |
| [^288] | [RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback](https://arxiv.org/abs/2402.03681) | RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。 |
| [^289] | [Diffusion World Model](https://arxiv.org/abs/2402.03570) | 扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。 |
| [^290] | [Test-Time Adaptation for Depth Completion](https://arxiv.org/abs/2402.03312) | 该论文提出了一种在线测试时间自适应方法，用于深度补全任务，通过在单次通过中缩小源数据和目标数据间的领域差距，提高模型性能。 |
| [^291] | [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216) | BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。 |
| [^292] | [Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach](https://arxiv.org/abs/2402.02954) | 本文通过应用最优性原理研究了分层信息共享的分布式部分可观察马尔可夫决策过程的解决方法。通过将问题分解成单阶段子游戏，并通过进一步分解子游戏，我们成功地解开了决策变量的纠缠，同时显著减少了时间复杂度。 |
| [^293] | [Evading Data Contamination Detection for Language Models is (too) Easy](https://arxiv.org/abs/2402.02823) | 本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。 |
| [^294] | [Review of multimodal machine learning approaches in healthcare](https://arxiv.org/abs/2402.02460) | 这篇综述主要介绍医疗保健领域中多模态机器学习方法的最新研究进展。通过综合分析最近的文献，探讨了临床诊断中各种数据模态的应用以及融合技术的评估。重点关注影像数据的应用，并介绍了现有的多模态数据集和训练方法。 |
| [^295] | [MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters](https://arxiv.org/abs/2402.02342) | MetaOptimize是一个框架，通过动态调整学习率来优化机器学习算法中的元参数，以提高训练效率和模型性能。 |
| [^296] | [INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer](https://arxiv.org/abs/2402.02317) | INViT是一种具有不变嵌套视图转换器的解决路由问题的方法，通过强制嵌套设计和不变的视图，在编码器内部提高学习求解器的泛化能力，从而实现了在具有不同分布和不同问题规模的TSP和CVRP问题上卓越的泛化性能。 |
| [^297] | [On the Exploitation of DCT-Traces in the Generative-AI Domain](https://arxiv.org/abs/2402.02209) | 本文分析了生成AI模型在生成深度伪造图像时在频域中的DCT系数的统计特征。通过研究发现了一种独特的“辨别指纹”，可以利用它来改善现有的深度伪造检测器。 |
| [^298] | [Quality and Trust in LLM-generated Code](https://arxiv.org/abs/2402.02047) | 本论文研究了机器学习生成代码的质量和信任问题，提出了校准的重要性，并探讨了如何确定模型生成代码的正确性。 |
| [^299] | [Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data](https://arxiv.org/abs/2402.01713) | 本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。 |
| [^300] | [Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques](https://arxiv.org/abs/2402.00654) | 该论文通过使用2017年的商品流动调查公共使用文件数据集和集成学习技术，改进了货运模式选择模型的准确性，包括构建本地模型、提取地理特征和应用集成学习方法。实验结果表明，该方法在没有内存限制的情况下实现了超过92%的准确性。 |
| [^301] | [PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks](https://arxiv.org/abs/2402.00326) | PirateNets是一种物理知识驱动的深度学习框架，解决了多层感知机网络在较大深度时性能下降的问题，通过引入自适应残差连接实现了稳定和高效的训练，并提升了模型的性能。 |
| [^302] | [Privacy-preserving data release leveraging optimal transport and particle gradient descent](https://arxiv.org/abs/2401.17823) | 该研究提出了一种基于边际的保护隐私数据合成方法PrivPGD，利用了最优输运和粒子梯度下降的工具。该方法在不同领域的数据集上表现出色，具有高度的可扩展性和灵活性，并可以满足特定的领域约束条件。 |
| [^303] | [Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations](https://arxiv.org/abs/2401.17345) | 本研究比较了机器学习中常用的伪随机数生成器在统计质量、数值可重复性、时间效率和能源消耗等方面与原始C实现的差异。 |
| [^304] | [PICL: Physics Informed Contrastive Learning for Partial Differential Equations](https://arxiv.org/abs/2401.16327) | 这项工作开发了一种使用广义对比损失的对比预训练框架，通过利用物理信息改善了神经算子在多个偏微分方程中的泛化能力。 |
| [^305] | [Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model](https://arxiv.org/abs/2401.09417) | 本文提出了一种新的通用视觉骨干模型Vim，通过使用双向状态空间模型和位置嵌入来高效表示视觉数据，相比于传统的视觉转换器如DeiT，在各种视觉任务上取得了更好的性能，并且实现了显著提升。 |
| [^306] | [X Hacking: The Threat of Misguided AutoML](https://arxiv.org/abs/2401.08513) | 本文介绍了X黑客的概念，即利用自动化机器学习流程来操纵可解释AI（XAI）指标，从而产生所需解释的模型，而不降低其预测性能。研究者总结了X黑客现象的严重性，并提出了可能的检测和预防方法，同时探讨了对XAI研究可信度和可重现性的伦理影响。 |
| [^307] | [Keep or toss? A nonparametric score to evaluate solutions for noisy ICA](https://arxiv.org/abs/2401.08468) | 本文提出一种非参数分数来自适应选择适用于任意高斯噪声的ICA算法，并通过特征函数评估估计的混合矩阵质量，无需了解噪声分布参数。 |
| [^308] | [On Learning for Ambiguous Chance Constrained Problems](https://arxiv.org/abs/2401.00547) | 本文研究了模糊机会约束优化问题中当决策者不知道分布P时的情况，通过使用一组分布来近似原问题，并推导出了相应的样本复杂度。 |
| [^309] | [Beyond mirkwood: Enhancing SED Modeling with Conformal Predictions](https://arxiv.org/abs/2312.14212) | 这篇论文提出了一种采用机器学习和一致性预测的方法，可以增强光谱能量分布（SED）建模的灵活性和不确定性数量化。与传统的方法不同，该方法可以使用任何兼容sklearn的模型，并通过转换点预测为误差条提高解释能力和可靠性。实验结果表明，在使用CatBoost作为基础预测器的情况下，采用一致预测可以显著提高性能。该方法为从观测数据中推断星系的物理性质提供了更多功能和准确的工具。 |
| [^310] | [In-Context Reinforcement Learning for Variable Action Spaces](https://arxiv.org/abs/2312.13327) | 本文提出了一种Headless-AD模型，通过只训练一次，能够在变化的行动空间中实现强化学习任务的泛化。实验证明该模型能够在从未遇到过的行动空间上表现出显著的泛化能力，甚至胜过针对特定行动集训练的模型。 |
| [^311] | [Parameterized Projected Bellman Operator](https://arxiv.org/abs/2312.12869) | 本论文提出了一种基于学习的近似贝尔曼算子的新方法，以解决近似值迭代算法中样本不确定性和计算复杂度的问题。 |
| [^312] | [Fast, Scalable, Warm-Start Semidefinite Programming with Spectral Bundling and Sketching](https://arxiv.org/abs/2312.11801) | 我们提出了一种名为统一谱束缚与草图初始化（USBS）的算法，用于快速、可扩展地解决大规模半定规划问题。该算法能够利用温暖启动初始化来进一步加速收敛，是一种有效的解决方案。 |
| [^313] | [FedSSA: Semantic Similarity-based Aggregation for Efficient Model-Heterogeneous Personalized Federated Learning](https://arxiv.org/abs/2312.09006) | FedSSA是一种基于语义相似度的聚合方法，用于高效模型异构个性化联邦学习。它通过异构 feature extractor 和同质 classification header 将每个客户端的模型拆分，并通过语义相似度进行头部参数聚合实现本地到全局的知识传输。此外，通过自适应参数稳定策略实现了全局到本地的知识传输。 |
| [^314] | [Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm](https://arxiv.org/abs/2312.08823) | 该论文提出了一种名为Metropolis-adjusted Mirror Langevin算法的方法，用于从约束空间中进行快速采样。这种算法是对Mirror Langevin算法的改进，通过添加接受-拒绝过滤器来消除渐近偏差，并具有指数优化依赖。 |
| [^315] | [Learning the Causal Structure of Networked Dynamical Systems under Latent Nodes and Structured Noise](https://arxiv.org/abs/2312.05974) | 本文提出了一种从部分可观测性条件下的时间序列数据中学习线性网络动力系统的隐藏因果网络的方法，通过特征嵌入和聚类来解决噪声相关性和部分可观测性的挑战。 |
| [^316] | [Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use](https://arxiv.org/abs/2312.04455) | 本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。 |
| [^317] | [What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes](https://arxiv.org/abs/2312.03096) | 这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。 |
| [^318] | [Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective](https://arxiv.org/abs/2312.00054) | 逆向强化学习是从专家策略示范中学习奖励函数的问题，本文提出了在标准离线和在线设置下用多项式样本和运行时间进行高效逆向强化学习的结果线索，并提供了几乎最优的样本复杂性的下界。 |
| [^319] | [Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations](https://arxiv.org/abs/2311.18575) | 本文提出了一个模型来处理零样本学习中的类别分布转移问题，该模型假设转移原因在训练过程中是未知的属性。通过引入基于分层抽样的框架构建合成数据环境，我们能够将类别分布转移看作分布外问题，并提出了一种学习鲁棒表示的算法。实验结果表明，我们的方法在不同类别分布上的泛化能力显著提高。 |
| [^320] | [On robust overfitting: adversarial training induced distribution matters](https://arxiv.org/abs/2311.16526) | 本文首先通过实验证实鲁棒过拟合与对抗训练轨迹上的扰动引起的分布的泛化困难性逐渐增加之间的相关性，然后给出了一个关于扰动引起的分布的泛化误差的新的上界，验证了其实用性，并提供了各种其他见解。 |
| [^321] | [Detection of developmental language disorder in Cypriot Greek children using a neural network algorithm](https://arxiv.org/abs/2311.15054) | 该研究开发了一种使用神经网络算法进行发展性语言障碍（DLD）检测的自动化方法，并首次应用于塞浦路斯希腊儿童DLD人群。实验结果表明该方法具有高的分类效果。 |
| [^322] | [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455) | 本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。 |
| [^323] | [Deciphering and integrating invariants for neural operator learning with various physical mechanisms](https://arxiv.org/abs/2311.14361) | 本文提出了一种称为PIANO的方法，用于解读和整合来自具有不同物理机制的PDE序列中的物理不变量，从而进行算子学习。相比现有技术，在不同条件下PIANO可以显著降低PDE预测任务的相对误差。 |
| [^324] | [Exploring the impact of social stress on the adaptive dynamics of COVID-19: Typing the behavior of na\"ive populations faced with epidemics](https://arxiv.org/abs/2311.13917) | 该论文研究了COVID-19疫情对社会压力的影响，并探究了不同人群面对流行病的行为特征。研究发现传统模型无法完全解释COVID-19的观察现象，因此非医学层面上的迅速应对至关重要。 |
| [^325] | [Discovering Effective Policies for Land-Use Planning](https://arxiv.org/abs/2311.12304) | 通过学习代理模型并使用进化搜索过程，发现了可定制到不同位置的有效土地利用政策，为土地利用规划提供了一个潜在有用的工具。 |
| [^326] | [Efficient Reinforcement Learning from Partial Observability](https://arxiv.org/abs/2311.12244) | 该论文提出了一种基于表示的方法，用于从部分观测中进行有效的强化学习。该方法能够处理部分可观测性带来的计算和统计挑战，并在各种基准测试中展现出优于先进算法的性能。 |
| [^327] | [On Measuring Faithfulness or Self-consistency of Natural Language Explanations](https://arxiv.org/abs/2311.07466) | 本文论述了衡量自然语言解释的忠诚度或自一致性的问题。我们提出了自一致性测试来评估解释的输出级别的一致性。我们通过构建比较一致性测试库，并引入了新的自一致性度量CC-SHAP来支持我们的观点。 |
| [^328] | [Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models](https://arxiv.org/abs/2311.06233) | 这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。 |
| [^329] | [Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet transits and $H_0$ inference](https://arxiv.org/abs/2311.04153) | 该论文提出了一种基于贝叶斯方法的核、均值和噪声边缘化高斯过程，用于系外行星凌星和H0推断。通过核选择和核超参数的边缘化以及贝叶斯模型比较，可以实现核选择和推断。 |
| [^330] | [SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation](https://arxiv.org/abs/2310.18376) | SQLformer是一个用于文本到SQL翻译的深度自回归查询图生成模型，采用了特定的Transformer架构，并通过结构归纳偏差解决领域泛化和自然语言与SQL查询对齐的难题。 |
| [^331] | [Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization](https://arxiv.org/abs/2310.15080) | 本文提出了一种带有参数高效prompt调整和自适应优化的联邦学习方法，以实现大型语言模型的高效和有效训练。 |
| [^332] | [Learning optimal integration of spatial and temporal information in noisy chemotaxis](https://arxiv.org/abs/2310.10531) | 本研究通过深度强化学习探究了在噪声化学趋化中空间和时间信息的最佳整合。研究发现，最佳策略在小细胞和大细胞尺寸下分别为纯粹的时间和空间策略，并且过渡区域的组合策略性能优于其他策略。 |
| [^333] | [Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)](https://arxiv.org/abs/2310.09877) | 本研究通过引入创新的工具和技术，使用积累的局部效应（ALE）进行统计推断，在解决小数据集、直观特征和健壮推断方面取得了突破。这项工作推动了ALE及其应用的研究。 |
| [^334] | [Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space](https://arxiv.org/abs/2310.09656) | 本文介绍了一种名为Tabsyn的方法，利用潜在空间中的扩散模型和变分自编码器生成具有混合类型数据的表格数据。Tabsyn具有通用性、质量和速度等关键优势，通过将不同类型的数据转换为统一空间并捕捉列间关系，优化潜在嵌入的分布来生成高质量的合成数据，同时生成速度更快。 |
| [^335] | [Hierarchical Multi-Marginal Optimal Transport for Network Alignment](https://arxiv.org/abs/2310.04470) | 层次化多边汇运输（HOT）框架通过融合Gromov-Wasserstein（FGW）重心将多个网络分解为对齐簇，并将FGW距离广义化到多边汇环境，实现多网络的联合对齐。实验证明，HOT相对于统计方法取得了显著的改进。 |
| [^336] | [On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation](https://arxiv.org/abs/2309.01753) | 本研究通过罚函数方法研究非凸双层优化问题，建立了罚函数与超级目标函数之间的强连接，并提出了一阶算法来找到驻点。 |
| [^337] | [Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization](https://arxiv.org/abs/2309.01107) | 本研究通过引入策略访问频率正则化，解决了非矩形奖励强鲁棒马尔可夫决策过程（RMDPs）的问题，并且提出了一种策略梯度方法并证明了其收敛性。 |
| [^338] | [Sampling the lattice Nambu-Goto string using Continuous Normalizing Flows](https://arxiv.org/abs/2307.01107) | 本文提出了一种数值方法，使用连续正态流模型对有效弦理论进行建模，并得到可靠的数值估计结果。 |
| [^339] | [Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection](https://arxiv.org/abs/2306.14291) | Hyp-OW是一种利用超几何距离的层次结构学习增强开放世界目标检测的方法，通过超类正则化器学习和建模已知项目的层次表示，通过基于相似度距离的重新标记模块有效地检测未知对象。 |
| [^340] | [Cooperative Multi-Agent Learning for Navigation via Structured State Abstraction](https://arxiv.org/abs/2306.11336) | 本文提出了一种基于结构化状态抽象的协同多智能体导航学习方法，通过应急通信实现智能体之间的协作和信息共享，同时使用神经网络架构来学习自适应的状态空间抽象和通信协议。仿真结果表明，该方法能够显著减少状态空间的大小并提升导航性能。 |
| [^341] | [Making Language Models Better Tool Learners with Execution Feedback](https://arxiv.org/abs/2305.13068) | 这篇论文提出了一个名为TRICE的框架，通过执行反馈实现语言模型的工具学习，使其能够学会何时以及如何有效地使用工具。 |
| [^342] | [Error-mitigated Quantum Approximate Optimization via Learning-based Adaptive Optimization](https://arxiv.org/abs/2303.14877) | 我们设计了一种双自适应区域贝叶斯优化算法，用于提升量子近似优化算法的性能。实验结果表明，该算法在性能上远远超过了传统的方法。 |
| [^343] | [Scalable Neural Network Training over Distributed Graphs](https://arxiv.org/abs/2302.13053) | RETEXO是第一个消除分布式图神经网络训练中通信瓶颈的框架，通过新的训练过程懒消息传递来改善网络通信效率。 |
| [^344] | [STERLING: Synergistic Representation Learning on Bipartite Graphs](https://arxiv.org/abs/2302.05428) | STERLING是一种在二分图上进行协同表示学习的模型，通过保留局部和全局协同性，学习节点嵌入，无需负节点对。这种方法通过最大化正节点对的相似性和共聚簇的互信息来提高连接性。 |
| [^345] | [Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences](https://arxiv.org/abs/2302.03106) | 本文提出了一种使用预训练的嵌入和句子包进行高效灵活的主题建模方法，通过结合生成过程模型和聚类，提供了使用先验自定义主题-文档分布的可能性，实验表明该方法在计算负担较小的情况下取得了最新的结果。 |
| [^346] | [Online Reinforcement Learning in Non-Stationary Context-Driven Environments](https://arxiv.org/abs/2302.02182) | 提出了一种名为LCPO的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧经验进行锚定来解决强化学习中的灾难性遗忘问题。 |
| [^347] | [Rethinking the Expressive Power of GNNs via Graph Biconnectivity](https://arxiv.org/abs/2301.09505) | 本文从根本上不同的角度重新思考了图神经网络（GNN）的表达能力，通过引入一类新的表达度量方法，即图的双连通性，并强调了它们在理论和实践中的重要性。令人惊讶的是，在对以前的GNN架构进行彻底审查后，发现大多数架构都没有对这些度量具有表达能力。唯一的例外是ESAN框架。 |
| [^348] | [Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models](https://arxiv.org/abs/2301.04741) | 本文研究了在基于偏好的强化学习（PbRL）中使用学习到的动力学模型的好处和挑战，并提供了实证证据：（1）相对于模型无关的PbRL，学习到的动力学模型可以大大减少环境交互次数，（2）学习到的动力学模型可以安全高效地合成多样化的偏好查询，（3）基于次优示范的奖励预训练可以在没有环境交互的情况下进行。 |
| [^349] | [Flow: Per-Instance Personalized Federated Learning Through Dynamic Routing](https://arxiv.org/abs/2211.15281) | Flow是一种细粒度无状态个性化联合学习方法，通过学习动态路由机制创建个性化模型，并引入了每个实例的路由，以提高每个客户端的准确性。 |
| [^350] | [When is Momentum Extragradient Optimal? A Polynomial-Based Analysis](https://arxiv.org/abs/2211.04659) | 本论文通过多项式分析，对动量外移梯度方法在不同情景下的加速收敛进行研究，包括特征值存在于实轴、位于实轴上的共轭复数或仅存在共轭复数的情况。同时，我们还得出了实现最快收敛的超参数。 |
| [^351] | [Dynamic Latent Separation for Deep Learning](https://arxiv.org/abs/2210.03728) | 本研究提出了动态潜变量分离的方法，可以在复杂数据中学习表达性强的潜变量，提升输出的多样性。该方法受原子物理学启发，通过学习每个数据样本的结构来解释各个子组件的重要性。实验证明该方法在不同分类和生成问题中提升了模型的性能。 |
| [^352] | [GBSVM: Granular-ball Support Vector Machine](https://arxiv.org/abs/2210.03120) | GBSVM是一种使用粗粒-球作为输入的支持向量机，修复了现有模型的错误并推导出了对偶模型，提出了粒子群优化算法和顺序最小优化算法来解决问题，具有良好的稳健性和效率。 |
| [^353] | [NeuralVDB: High-resolution Sparse Volume Representation using Hierarchical Neural Networks](https://arxiv.org/abs/2208.04448) | NeuralVDB使用分层神经网络改进了VDB的存储效率，将内存占用减少数个数量级同时保持了灵活性，并实现了高压缩比和空间适应性。 |
| [^354] | [Sublinear Time Algorithm for Online Weighted Bipartite Matching](https://arxiv.org/abs/2208.03367) | 这个论文提出了一个在线加权二部图匹配的次线性时间算法，通过引入随机化数据结构，实现了对权重的近似计算，以解决在实际推荐系统或搜索引擎中计算大规模权重的问题。 |
| [^355] | [Differentially Private Graph Learning via Sensitivity-Bounded Personalized PageRank](https://arxiv.org/abs/2207.06944) | 本论文提出了一种敏感性有界的个性化PageRank算法，能够保护用户隐私。该算法在保持准确性的同时，实现了差分隐私图学习的几种工具。 |
| [^356] | [Inertial Newton Algorithms Avoiding Strict Saddle Points](https://arxiv.org/abs/2111.04596) | 混合使用牛顿法和惯性梯度下降的二阶算法几乎总是可以避免严格鞍点，并通过数值实例支持了理论结果。 |
| [^357] | [Compositional Q-learning for electrolyte repletion with imbalanced patient sub-populations](https://arxiv.org/abs/2110.02879) | 组合Q学习方法用于解决医疗环境中存在异质性治疗反应的问题，通过使用复合任务结构和分离的模块化Q值函数，能够更有效地进行决策。 |
| [^358] | [Computationally Efficient High-Dimensional Bayesian Optimization via Variable Selection](https://arxiv.org/abs/2109.09264) | 本论文提出了一种变量选择的计算高效高维贝叶斯优化方法，能够自动学习子空间来优化高维域函数，同时减少了传统方法中的耗时问题，并在实验证明了方法的有效性。 |
| [^359] | [Machine Collaboration](https://arxiv.org/abs/2105.02569) | 本文提出了一种新的监督学习集成框架——机器协作（MaC），通过循环和交互的学习方式，使基础机器能够循环传递信息并相应地更新结构和参数。实验证明，MaC在大多数情况下表现优于其他先进方法。 |
| [^360] | [Sparse NMF with Archetypal Regularization: Computational and Robustness Properties](https://arxiv.org/abs/2104.03527) | 本文研究了使用典型正则化的稀疏非负矩阵分解问题，提出了强鲁棒性和弱鲁棒性的概念，并给出了理论保证和数值实验来加强这些概念的洞察力。 |
| [^361] | [Creativity of Deep Learning: Conceptualization and Assessment](https://arxiv.org/abs/2012.02282) | 这篇论文探讨了深度学习在创意领域的应用，并使用计算创造力的视角对其进行了概念化和评估。研究发现，尽管深度学习可以产生高质量的结果，但其创新性受到限制，同时也存在内部问题表达无法更改以及在不同领域之间建立联系的缺乏能力的问题。 |
| [^362] | [Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess.](http://arxiv.org/abs/2401.16852) | 通过将混合专家方法和MCTS相结合，本研究在国际象棋中显著提升了下棋水平，验证了集成方法的有效性并展示了融入专家知识和战略原则到神经网络中的潜力。 |
| [^363] | [Adaptive Block sparse regularization under arbitrary linear transform.](http://arxiv.org/abs/2401.15292) | 我们提出了一种在任意线性变换下重构具有块稀疏性的信号的方法，相比现有方法扩大了应用范围，并通过数值实验证明了其有效性。 |
| [^364] | [Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem.](http://arxiv.org/abs/2401.14876) | 本论文提出了一种跨空间自适应滤波器（CSF），可以从图拓扑和节点属性空间中提取自适应频率信息，以减轻图卷积网络（GCN）的过度平滑问题。 |
| [^365] | [Inverse analysis of granular flows using differentiable graph neural network simulator.](http://arxiv.org/abs/2401.13695) | 通过使用可微分图神经网络模拟器进行反演分析，解决了传统模拟器计算开销大、不可微等问题，提高了计算效率和准确性。 |
| [^366] | [TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients.](http://arxiv.org/abs/2401.12012) | TurboSVM-FL是一种新颖的联邦聚合策略，通过SVM聚合为懒惰客户端增强联邦学习。这种策略在不增加客户端计算负担的情况下解决了联邦学习中的收敛速度慢的问题。 |
| [^367] | [Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning.](http://arxiv.org/abs/2401.10386) | 本研究提出了一种基于随机森林机器学习的非侵入性诊断急性间室综合征的方法。使用压力传感电阻器检测肌肉间室压力，并通过蓝牙传输结果到Web应用程序。该诊断方法在准确率、灵敏度和F1得分等关键性能指标方面表现出色。 |
| [^368] | [Preparing Lessons for Progressive Training on Language Models.](http://arxiv.org/abs/2401.09192) | 提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。 |
| [^369] | [Augmenting Math Word Problems via Iterative Question Composing.](http://arxiv.org/abs/2401.09003) | 本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。 |
| [^370] | [Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models.](http://arxiv.org/abs/2401.02333) | 本研究提出了一种创新的方法，通过上下文化表格数据来提高 RAG 系统中处理复杂表格查询的准确性，提高了摘要的效率。 |
| [^371] | [Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2401.01479) | Kernel-U-Net是一种层次和对称框架，用于多元时间序列预测。与现有模型相比，它具有较少的参数数量、灵活性和计算效率。 |
| [^372] | [Scalable network reconstruction in subquadratic time.](http://arxiv.org/abs/2401.01404) | 这篇论文提出了一个可扩展的网络重建算法，能够在次二次时间内实现结果，通过随机的二阶邻居搜索产生最佳的边候选。 |
| [^373] | [Reinforcement Unlearning.](http://arxiv.org/abs/2312.15910) | 强化学习中的消除学习是一种新兴的研究领域，旨在解决环境所有者有权撤销智能体训练数据的隐私问题。该领域面临三个主要挑战。 |
| [^374] | [NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes.](http://arxiv.org/abs/2312.14890) | NPHardEval是一个新的基准，旨在评估大型语言模型在900个算法问题上的推理能力，扩展到NP-Hard复杂性类别。 |
| [^375] | [MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs.](http://arxiv.org/abs/2312.03731) | 本文提出了一种名为MultiGPrompt的多任务预训练和提示框架，用于在图形表示学习中提高鲁棒性和减少标注成本。 |
| [^376] | [Linear Log-Normal Attention with Unbiased Concentration.](http://arxiv.org/abs/2311.13541) | 本论文研究了自注意机制，并分析了注意力矩阵的分布和集中能力。通过引入线性对数正态注意力来模拟原始自注意力的分布和集中行为，提高了Transformer模型的可扩展性。 |
| [^377] | [DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries.](http://arxiv.org/abs/2311.02017) | DeliverAI是一个基于强化学习的分布式路径共享网络，用于优化食品配送的多目标优化问题，以减少配送成本并提高消费者满意度。 |
| [^378] | [Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective.](http://arxiv.org/abs/2311.01047) | 本论文提出了一种基于通信理论的方法，通过神经竞争来增强神经网络层输出的信噪比，从而提高深度网络的稳健性。 |
| [^379] | [COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning.](http://arxiv.org/abs/2311.00886) | 这项研究提出了一种名为COSTAR的新方法，通过整合自监督学习，改进了时间反事实结果的估计。该方法在处理时间相关混淆因素时结合了时间和特征关注以及分量对比损失，相比现有模型在估计准确性和对分布之外数据的泛化能力方面表现出更优越的性能。 |
| [^380] | [Latent Space Translation via Semantic Alignment.](http://arxiv.org/abs/2311.00664) | 本论文研究了潜在空间的翻译问题。通过简单的变换，可以将不同神经模型学到的表示翻译到其他预训练网络中。这种方法能够有效地拼接编码器和解码器，并在各种实验设置中得到验证。 |
| [^381] | [Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis.](http://arxiv.org/abs/2311.00164) | 该论文构建了一个大规模的道路交通事故记录数据集，并使用该数据集评估了现有的深度学习方法在预测事故发生方面的准确性。研究发现，图神经网络GraphSAGE能够准确预测道路上的事故数量，并判断事故是否会发生。 |
| [^382] | [Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms.](http://arxiv.org/abs/2310.16363) | 本文研究了受约束的Actor Critic和受约束的Natural Actor Critic算法的有限时间分析，证明了这些算法能找到性能函数的一阶稳定点，并且具有较低的样本复杂度。 |
| [^383] | [FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning.](http://arxiv.org/abs/2310.13283) | FedLoRA是基于LoRA调整的计算和通信高效的模型异构个性化联邦学习框架，旨在为每个联邦学习客户端训练个性化且异构的本地模型。 |
| [^384] | [Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning.](http://arxiv.org/abs/2310.12609) | 本文提出了一种使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法。该方法通过单一的视觉输入，在推理时能够同时生成可达目标并规划避开障碍物的运动路径，具有稳健性和多模态适应性。 |
| [^385] | [ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection.](http://arxiv.org/abs/2310.09298) | ByteStack-ID是一种基于灰度图像和负载字节频率的集成堆叠模型，用于数据包级入侵检测。它能迅速准确地识别网络流量中的各种攻击类型，并与传统方法有所不同。 |
| [^386] | [Discerning Temporal Difference Learning.](http://arxiv.org/abs/2310.08091) | 该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。 |
| [^387] | [Quantile-based Maximum Likelihood Training for Outlier Detection.](http://arxiv.org/abs/2310.06085) | 本文提出了一种基于分位数的极大似然目标，用于改进异常检测中异常值的分离程度。通过将正则化流拟合到预训练的判别性特征，并根据对数似然度评估来检测异常值。实验结果表明，这种方法优于最先进的无监督模型。 |
| [^388] | [Robust Angular Synchronization via Directed Graph Neural Networks.](http://arxiv.org/abs/2310.05842) | 本论文提出了一个名为GNNSync的基于有向图神经网络的鲁棒角度同步解决方案，解决了角度同步问题在高噪声环境下的挑战，并提出了新的损失函数以更好地编码同步约束。 |
| [^389] | [PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps.](http://arxiv.org/abs/2310.04017) | 本研究提出了一种使用蛋白质语言模型和接触图改进药物靶标相互作用预测的方法，通过在现有模型中引入联系图信息，可以改进药物靶标相互作用预测的性能。 |
| [^390] | [Improved prediction of ligand-protein binding affinities by meta-modeling.](http://arxiv.org/abs/2310.03946) | 通过整合基于结构的对接和基于序列的深度学习模型，开发了一个元模型框架，显著改善了配体-蛋白质结合亲和力预测的性能。 |
| [^391] | [Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits.](http://arxiv.org/abs/2310.02975) | 本文研究了在重尾波段问题中完全自适应的遗憾最小化，提出了随机自适应重尾波段问题，并证明了适应性算法相对于标准设置会有更高的遗憾。 |
| [^392] | [Zero-Shot Refinement of Buildings' Segmentation Models using SAM.](http://arxiv.org/abs/2310.01845) | 本文提出了一种使用SAM进行建筑物分割模型的零-shot细化的方法，针对遥感图像应用中SAM性能不佳、无法进行识别的问题进行了处理。通过引入不同的提示来提升模型的泛化能力。 |
| [^393] | [Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data.](http://arxiv.org/abs/2310.00290) | 这篇论文研究了储备计算在自回归时间序列数据中的数学结构，并揭示了其隐藏的权重矩阵结构，以实现对AR类型时间序列数据的完美预测。 |
| [^394] | [Efficient Biologically Plausible Adversarial Training.](http://arxiv.org/abs/2309.17348) | 本文研究了生物合理的学习算法是否比反向传播更具有对抗攻击的鲁棒性，并进行了广泛的比较分析。 |
| [^395] | [Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects.](http://arxiv.org/abs/2309.16457) | 该论文设计了一项新颖的实验并收集了52名参与者的全面脑电图数据集，从而解决了觉醒和睡眠状态下神经表示的差异问题。研究团队开发了通用睡眠解码器（USD），可以在不同个体间对齐觉醒和睡眠的神经模式，并取得了与使用个别睡眠数据进行解码相当的准确率。研究还发现，在测试个体上对USD进行微调可以进一步提高解码准确性。 |
| [^396] | [Class Incremental Learning via Likelihood Ratio Based Task Prediction.](http://arxiv.org/abs/2309.15048) | 该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。 |
| [^397] | [PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions.](http://arxiv.org/abs/2309.08630) | 本研究提出了一种基于图形的喷注表示方法，并设计了一种名为PCN的图神经网络（GNN），利用切比雪夫图卷积（ChebConv）进行深度学习喷注标记，取得了显著的改进。 |
| [^398] | [Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context.](http://arxiv.org/abs/2309.07708) | 本研究通过提出具有市场动态、股票代码和历史状态作为上下文的上下文市场数据集以及使用条件生成对抗网络（GAN）来实现对金融数据生成的控制。 |
| [^399] | [Zero-Shot Robustification of Zero-Shot Models With Foundation Models.](http://arxiv.org/abs/2309.04344) | 提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。 |
| [^400] | [Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio.](http://arxiv.org/abs/2309.03202) | 本研究评估了在S&P 500指数上使用强化学习技术进行多样化投资组合的可行性。研究发现，包含COVID-19时期的市场数据在训练数据集中可以提供更好的性能，并且基于策略的方法（VI和SARSA）在测试中表现优于Q学习。 |
| [^401] | [Certifying LLM Safety against Adversarial Prompting.](http://arxiv.org/abs/2309.02705) | 本研究提出了首个具有可验证安全保证的框架——消除和检查，用于对抗敌对提示。通过逐个消除标记并使用安全过滤器检查生成的子序列，确保任何敌对修改的有害输入提示都能被正确标识为有害。 |
| [^402] | [Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?.](http://arxiv.org/abs/2309.01108) | 本研究探讨了使用预先训练的自监督表示对语音障碍者的声学到发音反演任务的影响。实验结果表明，在低资源条件下，经过微调的DeCoAR模型在精细训练方案中相对于健康对照组和患者，分别取得了约1.81\%和约4.56\%的皮尔逊相关系数(CC)的改进。 |
| [^403] | [Bayesian deep learning for cosmic volumes with modified gravity.](http://arxiv.org/abs/2309.00612) | 该研究利用贝叶斯深度学习的方法，从修正引力模拟中提取宇宙学参数，并对不确定性进行了评估。 |
| [^404] | [Natural Quantum Monte Carlo Computation of Excited States.](http://arxiv.org/abs/2308.16848) | 该论文提出了一种变分蒙特卡洛算法，用于估计量子系统中的激发态，通过转化问题使其成为寻找扩展系统的基态的问题。这种方法特别适用于多电子系统，并且可以准确地计算各种可观测量的期望值，包括非对角线期望值和跃迁偶极矩，并在苯等大分子上得到了良好的结果。 |
| [^405] | [Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs.](http://arxiv.org/abs/2308.09895) | 本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。 |
| [^406] | [Detecting and Preventing Hallucinations in Large Vision Language Models.](http://arxiv.org/abs/2308.06394) | 本论文提出了一个用于训练和评估模型的多模态幻觉检测数据集，以解决大型视觉语言模型中存在的幻觉文本问题。这是第一个用于详细图像描述的全面多模态幻觉检测数据集。 |
| [^407] | [Follow Anything: Open-set detection, tracking, and following in real-time.](http://arxiv.org/abs/2308.05737) | 本文提出了一个名为“跟随任何物体”的机器人系统，可以实时检测、追踪和跟随任何物体，不受训练时概念限制，并且可以通过多模态查询进行应用。通过利用大规模预训练模型的视觉描述符，该系统能够检测、分割和跟踪物体，同时考虑遮挡和物体重新出现。 |
| [^408] | [Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing.](http://arxiv.org/abs/2308.02397) | 本论文通过模拟情况下的设计空间探索，研究了人体姿势估计中精度和硬件资源之间的折衷，提出了利用IMU感测进行高效准确的人体姿势估计的方法。 |
| [^409] | [Adaptive Proximal Gradient Method for Convex Optimization.](http://arxiv.org/abs/2308.02261) | 本文提出了自适应版本的梯度下降（GD）和近端梯度方法（ProxGD），通过利用局部曲率信息完全自适应。所提出的方法具有收敛性，且允许使用更大的步长。 |
| [^410] | [Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach.](http://arxiv.org/abs/2307.13771) | 本文通过添加预训练模块，在差分隐私逻辑回归中提高了模型的准确性。 |
| [^411] | [Epsilon*: Privacy Metric for Machine Learning Models.](http://arxiv.org/abs/2307.11280) | Epsilon*是一种用于测量机器学习模型隐私风险的新度量方法，不需要访问训练数据或模型训练算法，能与成员推断攻击中的假设检验相结合，提供对经过训练的模型实例隐私损失的下界，避免数值和噪声放大不稳定性。 |
| [^412] | [Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models.](http://arxiv.org/abs/2307.08303) | 本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。 |
| [^413] | [Harpa: High-Rate Phase Association with Travel Time Neural Fields.](http://arxiv.org/abs/2307.07572) | 本论文提出了一种名为Harpa的高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，即使在波速未知的情况下也能实现相位关联。这种方法能够处理较小、高速率的地震事件，提供了关于地下弹性介质属性的宝贵描述。 |
| [^414] | [Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability.](http://arxiv.org/abs/2307.07084) | 本研究提出了一种新的自适应Wasserstein变分优化（AWaVO）方法，利用形式方法解决了顺序决策中的解释和透明性问题，并提供了奖励设计和策略收敛的概率解释。 |
| [^415] | [Set Learning for Accurate and Calibrated Models.](http://arxiv.org/abs/2307.02245) | 提出了一种集合学习方法(OKO)来解决机器学习中的模型过度自信和校准不良问题，通过最小化集合的交叉熵误差，从而提高准确性和校准效果，并在有限的训练数据和类别不平衡情况下表现出更好的结果。 |
| [^416] | [Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks.](http://arxiv.org/abs/2306.14043) | 本文研究了机器学习中随机性可被攻击者利用的问题，主要关注流行的随机平滑方法。该方法用于训练可证明鲁棒性的模型和量化不确定性，但其随机性可能导致系统被攻击。 |
| [^417] | [Boosting Multitask Learning on Graphs through Higher-Order Task Affinities.](http://arxiv.org/abs/2306.14009) | 本文从多任务学习的角度重新审视在给定图上预测节点标签的问题，提出通过更高级任务相似性来加强多任务学习，并开发了一种算法来将任务分组以应对负迁移问题。 |
| [^418] | [Understanding quantum machine learning also requires rethinking generalization.](http://arxiv.org/abs/2306.13461) | 本文通过实验认为，传统方法无法解释量子机器学习模型在只使用少量数据训练的情况下表现出成功的泛化性能，该模型可以准确拟合随机状态及随机标记的训练数据，这种记忆随机数据的能力违反了当前小泛化误差的概念，我们通过理论构建补充实证结果，表明量子神经网络可将任意标记拟合到量子状态上，暗示了它们的记忆能力，这些结果排除了单单基于经典复杂度度量的所有可能保证。 |
| [^419] | [Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective.](http://arxiv.org/abs/2306.13092) | SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。 |
| [^420] | [Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD.](http://arxiv.org/abs/2306.08125) | 本研究提出了一种简单的SGD修改方法，使训练出的神经网络输出可被证明为可压缩，而不需要任何非平凡假设。 |
| [^421] | [Exact Mean Square Linear Stability Analysis for SGD.](http://arxiv.org/abs/2306.07850) | 本文提供了SGD稳定性的精确阈值表达式，发现其与批量大小之间呈单调非降关系，进一步展示了减小批量大小可能会影响SGD的稳定性。 |
| [^422] | [Robust Reinforcement Learning via Adversarial Kernel Approximation.](http://arxiv.org/abs/2306.05859) | 该论文提出了一种新奇的在线鲁棒强化学习方法，它通过近似对抗核并使用标准非鲁棒强化学习算法学习鲁棒策略，可应用于任何基础的强化学习算法之上，可以轻松扩展到高维度域。 |
| [^423] | [Revising deep learning methods in parking lot occupancy detection.](http://arxiv.org/abs/2306.04288) | 本文提出了一种基于EfficientNet架构的停车位占用检测算法，并在5个不同的数据集上进行了评估，性能得到提高。 |
| [^424] | [MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation.](http://arxiv.org/abs/2306.04120) | MESSY估计方法是一种基于最大熵的随机和符号密度估计方法，通过构建基于梯度的漂移扩散过程来高效地找到最大熵分布的参数，支持高维问题，并具有优于现有最新方法的有效性和普适性。 |
| [^425] | [Initial Guessing Bias: How Untrained Networks Favor Some Classes.](http://arxiv.org/abs/2306.00809) | 本文提出了“初始猜测偏差”现象，即在未经过训练的神经网络中，由于架构选择的影响，模型往往会将所有预测指向同一个类别。该现象对架构选择和初始化有实际指导意义，并具有理论后果，例如节点置换对称性的崩溃和深度带来的非平凡差异。 |
| [^426] | [Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective.](http://arxiv.org/abs/2306.00353) | 本研究提出了一个基于概率视角的对抗样本构建方法，可以生成语义感知的对抗性样本，并可以有效规避传统对抗性攻击的强化对抗训练方法。 |
| [^427] | [Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders.](http://arxiv.org/abs/2305.19259) | 本论文研究了一种允许任意数据排序的普通SGD算法,并表明在非凸函数情况下，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。 |
| [^428] | [BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning.](http://arxiv.org/abs/2305.18377) | 本文介绍了一种新的标签噪声类型BadLabel，它通过标签翻转攻击显著降低现有标签噪声学习（LNL）算法性能，因此提出了一种鲁棒的LNL方法，表现出在六个数据集上最先进的性能。 |
| [^429] | [Error Bounds for Flow Matching Methods.](http://arxiv.org/abs/2305.16860) | 本文提出了基于ODE的流匹配方法的误差界限，适用于完全确定性抽样，需要满足$L^2$近似误差范围的规律性条件和数据分布。 |
| [^430] | [Dropout Drops Double Descent.](http://arxiv.org/abs/2305.16179) | 本研究发现通过在全连接线性层之前添加一个dropout层，可以缓解双下降现象，从而提高模型的预测准确性。 |
| [^431] | [Democratized Diffusion Language Model.](http://arxiv.org/abs/2305.10818) | 本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。 |
| [^432] | [Exploring the cloud of feature interaction scores in a Rashomon set.](http://arxiv.org/abs/2305.10181) | 本文通过探索拉绍蒙集合中准确性类似的模型，引入了特征交互分数（FIS）来检测特征的相互作用。相较于从单个预先指定的模型中提取特征交互，本文提供了更为可靠的方式。 |
| [^433] | [Smaller Language Models are Better Black-box Machine-Generated Text Detectors.](http://arxiv.org/abs/2305.09859) | 本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。 |
| [^434] | [Solar Active Region Magnetogram Image Dataset for Studies of Space Weather.](http://arxiv.org/abs/2305.09492) | 本数据集提供了一系列太阳活动区磁图，并提供相应的太阳耀斑标签。它可用于研究磁结构、其演化以及太阳耀斑的关系，并对于自动太阳耀斑预测方法的研究具有重要价值。 |
| [^435] | [Physics Informed Token Transformer.](http://arxiv.org/abs/2305.08757) | 本研究提出了一种名为PITT的物理信息化的Token Transformer模型，通过将偏微分方程嵌入学习过程中，使得模型能够融入物理知识，并在多个PDE应用中展现出性能和优势。 |
| [^436] | [Neural Wave Functions for Superfluids.](http://arxiv.org/abs/2305.06989) | 本论文利用费米神经网络波函数方法研究了均匀费米气体超流，提出一种针对FermiNet模型的改进方法，获得了极其准确的结果。 |
| [^437] | [StyleLipSync: Style-based Personalized Lip-sync Video Generation.](http://arxiv.org/abs/2305.00521) | 本文提出了一种基于风格的个性化唇形动画视频生成模型，可以准确地生成任意身份的唇形同步视频，且可用于增强未见面孔的特征。 |
| [^438] | [A multifidelity approach to continual learning for physical systems.](http://arxiv.org/abs/2304.03894) | 这篇论文介绍了一种基于多保真深度神经网络的新型持续学习方法，能限制灾难性遗忘，特别适用于满足相同物理定律或具备物理学先验知识的神经网络等物理问题。 |
| [^439] | [R^2: Range Regularization for Model Compression and Quantization.](http://arxiv.org/abs/2303.08253) | R^2提出了一种基于区间正则化的新方法，利用有效的最小值和最大值调整权重分布，从而使模型压缩和量化技术能够更好地利用其数值表示能力。该方法可以提高模型优化的质量，尤其是在较低位上。 |
| [^440] | [Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks.](http://arxiv.org/abs/2302.11628) | 本文提出了一种名为特征分区聚合的认证防御方法，用于对抗$\ell_0$逃避、后门和污染攻击。与现有防御方法相比，FPA速度更快，提供更大的鲁棒性保证，且能够免费提供额外的鲁棒性维度。 |
| [^441] | [A Survey of Deep Learning: From Activations to Transformers.](http://arxiv.org/abs/2302.00722) | 这篇综述调查了深度学习领域的重要进展，包括各种架构、层、目标和优化技术的发展，以及关注机制、归一化、跳跃连接、Transformer和自监督学习等方法的变体。总结了成功创新的关键策略，并讨论了最近的商业闭源模型。 |
| [^442] | [Discovery and Exploitation of Generalized Network Effects.](http://arxiv.org/abs/2301.00270) | 我们提出了NetEffect，一种图挖掘方法，能够识别和理解具有少量节点标签的大型图中的广义网络效应（如同质性、异质性或二者的组合），并利用这些效应来改进下游任务的准确性和效率。 |
| [^443] | [FED-CD: Federated Causal Discovery from Interventional and Observational Data.](http://arxiv.org/abs/2211.03846) | FED-CD是一个联邦框架，可以从分布式数据集中推断出因果关系，同时保护隐私，适应共享和不相交的干预协变量的场景，具有可扩展性。 |

# 详细

[^1]: FedMoE: 数据级别个性化的混合专家模型用于异构模型的个性化联邦学习

    FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning

    [https://rss.arxiv.org/abs/2402.01350](https://rss.arxiv.org/abs/2402.01350)

    FedMoE是一种模型异构的个性化联邦学习算法，通过使用专家混合模型增强大型语言模型，将共享的小特征提取器和本地门控网络分配给每个客户端的本地异构大模型，以解决当前模型异构个性化联邦学习方法中存在的隐私、性能、通信和计算成本等问题。

    

    联邦学习广泛应用于分散数据的协同训练，但面临数据、系统和模型异构等挑战。这导致模型异构个性化联邦学习 (MHPFL) 的出现。然而，当前的MHPFL方法在数据和模型隐私、模型性能、通信和计算成本方面仍存在关切。为应对这些问题，我们提出了一种新颖的模型异构个性化联邦学习算法 (FedMoE) ，采用著名的专家混合模型 (MoE) 来增强大型语言模型 (LLM)。它为每个客户端的本地异构大模型分配了一个共享的均匀小特征提取器和一个本地门控网络。(1) 在本地训练过程中，本地异构模型的特征提取器作为个性化特征（表示）提取的本地专家，而共享的均匀小特征提取器则作为广义特征提取的全局专家。

    Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. 
    
[^2]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^3]: 关于LM潜在空间的语义学：一种以词汇为定义的方法

    On the Semantics of LM Latent Space: A Vocabulary-defined Approach

    [https://rss.arxiv.org/abs/2401.16184](https://rss.arxiv.org/abs/2401.16184)

    本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。

    

    理解语言模型(LM)的潜在空间对于改进其性能和可解释性至关重要。现有的分析往往在提供基于模型的对LM语义的分离洞察方面存在不足，并忽视了LM适应的重要方面。为了响应这一问题，我们引入了一种开创性的方法，称为以词汇为定义的语义学，它在LM的潜在空间中建立了一个参考框架，确保基于LM词汇的分离语义分析。我们的方法超越了先前的交织分析，利用LM词汇来获得以模型为中心的洞察。此外，我们提出了一种计算logits的新技术，强调可微分性和局部等距性，并引入了一个神经聚类模块，用于在LM适应过程中进行语义校准。通过在多种文本理解数据集上进行广泛实验，我们的方法在检索增强生成和参数高效微调方面超越了最先进的方法。

    Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
    
[^4]: FAST: 用于加速Transformers的可分解注意力机制

    FAST: Factorizable Attention for Speeding up Transformers

    [https://arxiv.org/abs/2402.07901](https://arxiv.org/abs/2402.07901)

    该论文介绍了一种可以加速Transformers模型的可分解注意力机制，通过引入因子分解形式的注意力，将注意力机制的复杂度从O(N^2)降低到O(N)，并 在维持注意力矩阵完整表示的同时保持稀疏性和所有-所有令牌关系。实验证明该注意力机制具有稳健的性能，并在不同应用中具有重要潜力。

    

    在原始的快速多极方法和改进后的快速高斯变换所固有的因子分解的驱动下，我们提出了一种在高维度中高效运行的可分解注意力形式。这种方法将Transformers中的注意力机制的计算和存储复杂度从O(N^2)降低到O(N)。与之前的尝试相比，我们的工作呈现了一个线性缩放的注意力机制，既保持了注意力矩阵的完整表示，又不妥协于稀疏化，并且包含了令牌之间的全互操作关系。我们探索了我们新的注意力度量的属性，并在各种标准设置下进行了测试。结果表明，我们的注意力机制具有稳健的性能，并在使用自我注意力的多样的应用中具有重要的潜力。

    Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.
    
[^5]: 从单一儿童语言输入的可学习性的系统调查

    A systematic investigation of learnability from single child linguistic input

    [https://arxiv.org/abs/2402.07899](https://arxiv.org/abs/2402.07899)

    我们的研究探索了用单个儿童的语言输入训练语言模型的可学习性，我们发现这种设置下的语言模型能够形成句法和语义词群，并对某些语言现象具有敏感性。

    

    语言模型（LM）在生成语言连贯文本方面表现出了 remarkable proficiency，引发了关于它们与人类语言可学习性的相关讨论。然而，这些模型的训练数据与儿童接收到的语言输入之间存在着显著差距。LMs通常在数量级上更大且本质与儿童语言输入不同的数据上进行训练。针对这一差距，我们的研究侧重于在单个儿童语言输入的子集上训练LMs。先前的研究发现，在这种设置下训练的LMs可以形成句法和语义词群，并对某些语言现象具有敏感性。然而，这些研究仅考虑了仅使用一个单一儿童数据集训练的LSTMs和更简单的神经网络。为了检验从单一儿童输入可学习性的鲁棒性，我们系统地…

    Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematicall
    
[^6]: 标注效率高的文本生成模型选择

    Label-Efficient Model Selection for Text Generation

    [https://arxiv.org/abs/2402.07891](https://arxiv.org/abs/2402.07891)

    DiffUse是一种标注效率高的文本生成模型选择方法，它通过聚类文本语义差异的嵌入来选择更具信息量的实例，并能显著减少所需的注释数量。

    

    针对给定目标任务的模型选择可能成本高昂，因为它可能需要对不同模型输出的质量进行广泛的注释。我们引入了DiffUse，一种有效的方法来在候选文本生成模型之间做出明智的决策。DiffUse减少了所需的偏好注释数量，从而节省了在评估中宝贵的时间和资源。DiffUse通过聚类表示模型输出之间的语义差异的嵌入来智能选择实例。因此，它能够识别出一些更有信息量的例子来进行偏好决策。我们的方法与模型无关，可以应用于任何文本生成模型。此外，我们提出了一种实用的迭代方法来动态确定要注释的实例数量。通过对数百个模型对进行一系列实验，我们证明了DiffUse可以显著减少所需的注释数量，最多可减少75%，同时保持高评估水平。

    Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation 
    
[^7]: MAIDCRL: 半集中式多智能体影响密集卷积神经网络强化学习

    MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning

    [https://arxiv.org/abs/2402.07890](https://arxiv.org/abs/2402.07890)

    MAIDCRL 是一种半集中式密集神经网络强化学习算法，通过引入智能体影响图(AIMs)和卷积层，成功在 StarCraft 多智能体挑战中实现了有效的多智能体控制。CNN-enabled MAIDCRL 在学习性能上显著提高，并在复杂的异质场景中取得了更快的学习速度。

    

    在多智能体系统中的分布式决策制定中，对于合作和竞争系统中的交互行为学习，存在困难挑战。为了减轻这种复杂性，MAIDRL提出了一种加强了智能体影响图(AIMs)的半集中式密集强化学习算法，用于学习在StarCraft多智能体挑战(SMAC)场景中的有效多智能体控制。本文在MAIDRL中扩展了DenseNet，引入了半集中式多智能体密集卷积神经网络强化学习(MAIDCRL)，通过将卷积层融入深度模型架构，并在同质和异质场景上评估了性能。结果显示，启用了CNN的MAIDCRL在学习性能上显著提高，并且相对于现有的MAIDRL，特别是在更复杂的异质SMAC场景上，实现了更快的学习速度。我们进一步研究了模型的稳定性和鲁棒性。统计数据显示...

    Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect tha
    
[^8]: 使用图论提高基于机器学习的网络攻击检测能力

    Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks

    [https://arxiv.org/abs/2402.07878](https://arxiv.org/abs/2402.07878)

    本文通过使用图论提取新指标来改进基于机器学习的网络攻击检测方法，从而提高检测效率并克服传统方法的局限性。

    

    网络入侵和网络威胁的早期检测是网络安全的主要支柱之一。其中一种最有效的方法是利用人工智能算法分析网络流量，通过区分攻击者和合法用户以检测可能存在的攻击者。通常通过收集网络终端之间的流量，并以数据包或连接为基础进行分析来实现。在本文中，我们提出了一种预处理网络流量的方法，旨在提取一些新的指标，以便更高效地进行检测，并克服传统方法的一些局限性。这些新指标基于图论，并考虑整个网络，而不仅仅关注个别数据包或连接。我们通过对公开可用的数据集进行实验证明了我们的方法的有效性。

    Early detection of network intrusions and cyber threats is one of the main pillars of cybersecurity. One of the most effective approaches for this purpose is to analyze network traffic with the help of artificial intelligence algorithms, with the aim of detecting the possible presence of an attacker by distinguishing it from a legitimate user. This is commonly done by collecting the traffic exchanged between terminals in a network and analyzing it on a per-packet or per-connection basis. In this paper, we propose instead to perform pre-processing of network traffic under analysis with the aim of extracting some new metrics on which we can perform more efficient detection and overcome some limitations of classical approaches. These new metrics are based on graph theory, and consider the network as a whole, rather than focusing on individual packets or connections. Our approach is validated through experiments performed on publicly available data sets, from which it results that it can n
    
[^9]: 使用语言反馈模型来改进政策

    Policy Improvement using Language Feedback Models

    [https://arxiv.org/abs/2402.07876](https://arxiv.org/abs/2402.07876)

    本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。

    

    我们引入了语言反馈模型（LFMs），用于在指令遵循中识别期望的行为-有助于实现指令中指定任务的行动-以进行模仿学习。为了训练LFMs，我们从大型语言模型（LLMs）获取对视觉轨迹进行语言描述的反馈。首先，通过使用LFMs识别期望模仿的行为，我们在三种不同的语言基础环境（Touchdown，ScienceWorld和ALFWorld）上，在任务完成率上改善了强行为克隆的基线方法。其次，与LLMs直接预测行动相比，使用LFMs在LLM输出标记的数量相同的情况下表现更好。第三，LFMs适应未见环境，通过一轮适应使任务完成率提高了3.5-12.0％。最后，可以修改LFM以提供人类可解释的反馈，无需性能损失，从而允许人类验证模仿学习的期望行为。

    We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
    
[^10]: 线性二次控制中策略梯度的隐性偏差：对未见初始状态的外推

    Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States

    [https://arxiv.org/abs/2402.07875](https://arxiv.org/abs/2402.07875)

    本文研究了策略梯度在线性二次调节控制中对未见初始状态的外推问题，发现外推程度取决于训练中系统的探索程度。

    

    在现代机器学习中，模型可以以多种方式拟合训练数据，其中一些在未见（测试）数据上表现良好，而其他一些则不然。有趣的是，在这种情况下，梯度下降经常展现出一种隐性偏差，导致在未见数据上表现出色。这种隐性偏差在监督学习中已经得到了广泛研究，但在最优控制（强化学习）中却了解得较少。在那里，通过梯度下降学习应用于系统的控制器被称为策略梯度，并且一个非常重要的问题是学习的控制器在对未见初始状态的外推程度。本文在理论上研究了策略梯度在对未见初始状态的外推方面的隐性偏差。我们以基本的线性二次调节器（LQR）问题为重点，确立了外推程度取决于训练中系统在初始状态下引起的探索程度。

    In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Exper
    
[^11]: PIVOT: 迭代视觉提示激发可操作知识用于VLMs

    PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs

    [https://arxiv.org/abs/2402.07872](https://arxiv.org/abs/2402.07872)

    本文介绍了一种名为PIVOT的新颖视觉提示方法，它通过迭代的视觉问答将任务转化为VLMs问题。每个迭代中，图像被标注为VLMs可以参考的视觉表示，并通过优化选择最佳选项。这种方法能够使VLMs进行机器人控制和其他空间任务的输出。

    

    视觉语言模型（VLMs）显示出在各种任务中的令人印象深刻的能力，从逻辑推理到视觉理解。这为与世界进行更丰富的互动打开了大门，例如机器人控制。然而，VLMs只产生文本输出，而机器人控制和其他空间任务需要输出连续的坐标，动作或轨迹。我们如何在不对任务特定数据进行微调的情况下使VLMs能够处理这种设置呢？在本文中，我们提出了一种新颖的VLMs视觉提示方法，称之为迭代视觉优化提示（PIVOT），将任务视为迭代的视觉问答。在每个迭代中，图像被注释为VLMs可以参考的提案的视觉表示（例如候选机器人动作、定位或轨迹）。然后，VLMs选择最佳的任务。这些提案经过迭代优化，使VLMs最终找到最佳的可用选项。

    Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available an
    
[^12]: 细粒度混合专家模型的标度律

    Scaling Laws for Fine-Grained Mixture of Experts

    [https://arxiv.org/abs/2402.07871](https://arxiv.org/abs/2402.07871)

    本研究分析了细粒度混合专家模型的标度特性，并引入了粒度作为新的超参数，通过调整粒度可以精确控制专家的大小。研究结果显示，MoE模型在效果上始终优于密集变压器模型，并且随着模型大小和训练预算的增大，密集和MoE模型之间的效率差距也在增大。同时，将MoE中专家的大小设置为与前馈层相同的常见做法在几乎任何计算预算下都不是最优的。

    

    混合专家（MoE）模型已成为减少大型语言模型计算成本的主要解决方案。在这项工作中，我们分析了它们的标度特性，并纳入了更广泛的变量范围。具体地，我们引入了一个新的超参数，称为粒度，通过调整粒度可以精确控制专家的大小。基于此，我们建立了细粒度MoE的标度律，考虑了训练标记数、模型大小和粒度。利用这些定律，我们推导出了给定计算预算下的最佳训练配置。我们的研究结果不仅表明MoE模型始终优于密集变压器模型，而且还凸显了在扩大模型大小和训练预算时，密集和MoE模型之间的效率差距在扩大。此外，我们证明了将MoE中专家的大小设置为与前馈层相同的常见做法在几乎任何计算预算下都不是最优的。

    Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.
    
[^13]: 动态系统中的实验设计的嵌套粒子滤波器

    Nesting Particle Filters for Experimental Design in Dynamical Systems

    [https://arxiv.org/abs/2402.07868](https://arxiv.org/abs/2402.07868)

    本文提出了一种新颖的方法来解决动态系统中的贝叶斯实验设计问题，利用嵌套粒子滤波器和立体蒙特卡洛方法来进行基于梯度的策略优化，相比于其他方法具有更好的性能。

    

    本文提出了一种新颖的贝叶斯实验设计方法，用于非交换数据，并将其形式化为风险敏感的策略优化。我们开发了内外SMC^2算法，使用嵌套顺序蒙特卡洛（SMC）估计器来预测期望的信息增益，并将其嵌入到粒子马尔可夫链蒙特卡洛（pMCMC）框架中进行基于梯度的策略优化。与最近依赖于偏估计器来摊销先前学习设计策略的成本的方法相比，我们的方法具有更好的性能。在一组动态系统的数值验证中展示了我们方法的有效性。

    In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.
    
[^14]: PoisonedRAG: 知识污染攻击对大型语言模型的检索增强生成

    PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models

    [https://arxiv.org/abs/2402.07867](https://arxiv.org/abs/2402.07867)

    本论文提出了一种名为PoisonedRAG的知识污染攻击方法，用于对大型语言模型的检索增强生成进行攻击和破坏。

    

    大型语言模型（LLM）由于其卓越的生成能力而取得了显著的成功。尽管如此，它们也存在固有的局限性，如缺乏最新的知识和虚构。检索增强生成（RAG）是一种最先进的技术，以减轻这些限制。具体而言，对于给定的问题，RAG从知识数据库中检索相关知识，以增强LLM的输入。例如，当知识数据库中包含从维基百科收集的数百万个文本时，检索到的知识可以是与给定问题在语义上最相似的前K个文本集。因此，LLM可以利用检索到的知识作为上下文为给定问题生成答案。现有研究主要集中在改善RAG的准确性或效率，而对其安全性的探索较少。我们旨在填补这一空白。

    Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge pois
    
[^15]: 透视VLMs：探索视觉条件化语言模型的设计空间

    Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models

    [https://arxiv.org/abs/2402.07865](https://arxiv.org/abs/2402.07865)

    本论文探索了视觉条件化语言模型（VLMs）设计的关键空间，并提供了一套标准化评估，同时还研究了预训练的视觉表示和权衡的问题。

    

    视觉条件化语言模型（VLMs）在视觉对话、场景理解和机器人任务规划等应用中得到了越来越多的应用，这种应用促使了像LLaVa、InstructBLIP和PaLI-3等许多新模型的出现。尽管有这么多新的发布，但关于图像预处理、架构和优化的关键设计决策仍然未被充分探索，这使得我们很难理解模型性能的因素，这一挑战又因缺乏客观、一致的评估而变得更加复杂。为了填补这些空白，我们首先编制了一套标准化评估，涵盖了视觉问答、从语言中定位物体以及探索幻觉等属性的目标挑战集，这些评估可以提供关于VLM能力的精细、准确的见解。其次，我们对关键的设计轴进行了严格的研究，包括预训练的视觉表示和使用的权衡。

    Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
    
[^16]: AI增强预测：LLM助手提高人类预测准确性

    AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy

    [https://arxiv.org/abs/2402.07862](https://arxiv.org/abs/2402.07862)

    本研究发现，使用LLMs助手可以显著提高预测准确性，不仅仅是由于模型预测准确性的提升。

    

    大型语言模型(LLMs)展现出令人印象深刻的能力，在许多领域与甚至超过人类表现。本研究探讨了LLMs在预测任务中增强判断力的潜力。我们评估了两个GPT-4-Turbo助手对预测准确性的影响：一个旨在提供高质量建议（超级预测），另一个旨在过于自信和基本概率忽视。参与者（N = 991）可以在整个研究过程中咨询他们被分配的LLM助手，而对照组则使用一个较低级别的模型（DaVinci-003），不提供直接的预测支持。我们的注册分析显示，LLM增强显著提高了23%的预测准确性，无论是对于任何一种助手类型，相比于对照组。这种改进发生在超级预测助手在预测中更高的准确性的情况下，表明增强的效益不仅仅是由于模型预测准确性。

    Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Explora
    
[^17]: 多尺度神经影像特征用于识别情绪障碍治疗中的药物类别和非反应者

    Multiscale Neuroimaging Features for the Identification of Medication Class and Non-Responders in Mood Disorder Treatment

    [https://arxiv.org/abs/2402.07858](https://arxiv.org/abs/2402.07858)

    本研究通过多尺度神经影像特征的分析，提出了一种方法来识别对标准药物疗程不反应的情绪障碍患者，为临床医生提供更可靠和高效的治疗选择。

    

    在情绪障碍的临床治疗中，患者呈现的复杂行为症状以及对特定药物类别的患者反应的变异性，使用标准的诊断和处方方法时可能会导致提供快速可靠的治疗困难的情况。越来越多地将神经影像扫描和衍生物纳入临床过程中，有望减轻围绕此过程的不确定性。特别是，如果神经特征可以帮助识别对标准的抗抑郁药物或情绪稳定剂疗程不反应的患者，临床医生可能会选择避免漫长且有副作用的治疗，寻求不同的更有效的疗程，而否则可能没有考虑到。之前，有关相关神经影像特征导出的方法仅在数据的一个尺度上工作，可能限制了可用于临床决策的信息深度。

    In the clinical treatment of mood disorders, the complex behavioral symptoms presented by patients and variability of patient response to particular medication classes can create difficulties in providing fast and reliable treatment when standard diagnostic and prescription methods are used. Increasingly, the incorporation of physiological information such as neuroimaging scans and derivatives into the clinical process promises to alleviate some of the uncertainty surrounding this process. Particularly, if neural features can help to identify patients who may not respond to standard courses of anti-depressants or mood stabilizers, clinicians may elect to avoid lengthy and side-effect-laden treatments and seek out a different, more effective course that might otherwise not have been under consideration. Previously, approaches for the derivation of relevant neuroimaging features work at only one scale in the data, potentially limiting the depth of information available for clinical decis
    
[^18]: 将历史降雨数据与NCEP-NWP预报相比较，预测印度季风降雨的技能比较

    Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts

    [https://arxiv.org/abs/2402.07851](https://arxiv.org/abs/2402.07851)

    深度学习应用于历史降雨数据的预测比NWP预报和基于持续性的预测更准确。

    

    在这篇文章中，我们考虑了在印度的四个季风月份，在一天和三天之前预测降雨的问题。我们使用来自IMD的印度历史日降水数据训练了神经网络，时间段为1901年至2022年，空间分辨率为1°×1°。这与来自NCEP（美国国家环境预报中心）的数值天气预报（NWP）预测进行了比较，该数据可用于2011年至2022年。我们进行了详细的全国范围分析，并分别分析了印度一些人口最多的城市。我们的结论是，将深度学习应用于历史降雨数据的预测比NWP预报和基于持续性的预测更准确。平均而言，与我们的预测相比，NCEP-NWP模型的预测在单日预测中的误差约高出34%，在三天预测中的误差高出68%以上。

    In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance. We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022. We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India. Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence. On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction. Simila
    
[^19]: 利用在赋值流形上的E-测地线流匹配生成离散联合分布的模型

    Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow Matching on Assignment Manifolds

    [https://arxiv.org/abs/2402.07846](https://arxiv.org/abs/2402.07846)

    本文提出了一种基于连续归一化流的生成模型，该模型可以逐步分配类别，避免了离散化潜在连续模型时的舍入和样本截断等问题。通过匹配分解离散分布的测地线流，可以高效地训练该模型，并且适用于表示复杂统计依赖关系的非分解离散分布。

    

    本文介绍了一种基于连续归一化流在分解离散度量子流形上的生成模型，该模型逐步对类别进行分配，避免了离散化潜在连续模型时的舍入、样本截断等问题。将子流形嵌入到所有联合离散分布和数据驱动平均的元单纯形中，可以近似表示能够表示结构化离散数据的复杂统计依赖关系的一般非分解离散分布。通过匹配分解离散分布的测地线流，演示了该生成模型的高效训练。各种实验突出了该方法的广泛适用性。

    This paper introduces a novel generative model for discrete distributions based on continuous normalizing flows on the submanifold of factorizing discrete measures. Integration of the flow gradually assigns categories and avoids issues of discretizing the latent continuous model like rounding, sample truncation etc. General non-factorizing discrete distributions capable of representing complex statistical dependencies of structured discrete data, can be approximated by embedding the submanifold into a the meta-simplex of all joint discrete distributions and data-driven averaging. Efficient training of the generative model is demonstrated by matching the flow of geodesics of factorizing discrete distributions. Various experiments underline the approach's broad applicability.
    
[^20]: 使用无监督度量优化GNN进行节点聚类的研究

    An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering

    [https://arxiv.org/abs/2402.07845](https://arxiv.org/abs/2402.07845)

    本研究展示了使用无监督度量模块性优化GNN进行节点聚类的方法，且无需与基准值进行比较。在设计合成实验的过程中，我们发现了这种方法的局限性。

    

    图神经网络（GNN）可以通过学习特征和连接信息的二元性来训练以检测图中的社区。目前，优化GNN的常见方法是使用与基准值的比较来进行超参数调整和模型选择。本研究表明，仅通过优化模块性，可以使用GNN将节点聚类成社区，而无需与基准值进行比较。尽管模块性是一种图分区质量度量，我们证明这也可以用于优化同时编码特征的GNN，并且不会降低性能。我们进一步研究无监督度量性能是否能够预测基准值的性能。为了探究为什么可以使用模块性优化GNN，我们设计了一些合成实验来展示这种方法的局限性。这些合成图表明其在不同、随机和零信息空间分区中的当前能力。

    Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in att
    
[^21]: 通过最优传输实现元剪枝

    Towards Meta-Pruning via Optimal Transport

    [https://arxiv.org/abs/2402.07839](https://arxiv.org/abs/2402.07839)

    本文提出了一种名为Intra-Fusion的新方法，通过模型融合和最优传输的概念实现了神经网络的元剪枝，该方法能够有效恢复准确度并避免精调工作，同时具有高效和有前景的特点。

    

    传统神经网络的结构剪枝通常依赖于识别和丢弃不重要的神经元，这种做法往往会导致显著的准确度损失，需要随后的精调工作。本文提出了一种名为Intra-Fusion的新方法，挑战了传统的剪枝范式。与现有方法只关注设计有意义的神经元重要性度量不同，Intra-Fusion重新定义了上层剪枝过程。通过利用模型融合和最优传输的概念，我们利用给定的不可知重要性度量，得到了更有效的稀疏模型表示。值得注意的是，我们的方法在不需要资源密集型的精调的情况下实现了显著的准确度恢复，使其成为神经网络压缩的高效且有前景的工具。

    Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We 
    
[^22]: 在时间领域中进行泛化：库普曼算子的应用

    Generalizing across Temporal Domains with Koopman Operators

    [https://arxiv.org/abs/2402.07834](https://arxiv.org/abs/2402.07834)

    本研究在时间领域泛化问题中提出了库普曼算子的应用，通过对齐条件分布来减小泛化界限。通过使用库普曼算子，我们可以有效地处理时变分布，从而解决时间领域泛化问题。

    

    在领域泛化的领域中，构建一种能够在没有目标数据的情况下适用于目标领域的预测模型仍然具有挑战性。当考虑到领域之间的演化动态时，这个问题变得更加复杂。虽然已经提出了各种方法来解决这个问题，但对基础泛化理论的全面理解仍然缺乏。在本研究中，我们提出了新的理论结果，通过对齐条件分布来减小泛化界限。我们的分析为通过应用库普曼神经算子来解决时间领域泛化问题提供了关键动机，从而产生了时间库普曼网络（TKNets）。通过使用库普曼算子，我们有效地使用库普曼理论的原则来处理时间领域泛化中遇到的时变分布，其中测量函数被用来建立线性过渡关系。

    In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations betwe
    
[^23]: 论计算有效的多类别校准问题

    On Computationally Efficient Multi-Class Calibration

    [https://arxiv.org/abs/2402.07821](https://arxiv.org/abs/2402.07821)

    提出了一种在多类别预测问题中多样化的投影平滑校准概念，并且给出了多项式时间复杂度的重新校准算法，从而实现了计算效率和强大的预测保证之间的权衡。

    

    考虑一个多类别标记问题，其中标记可以在[1,k]范围内取值，而预测器预测的是标记的分布。在这项工作中，我们研究了以下基础问题：是否存在多类别校准的概念，可以给出对有意义的预测的强大保证，并且可以在多项式时间和样本复杂度下实现？先前的校准概念在计算效率和表达能力之间存在着权衡：它们要么在k的样本复杂度上呈指数级增长，要么需要求解计算难题，要么给出的保证相当弱。我们的主要贡献是提出了一种能够实现所有这些期望的校准概念：我们在多类别预测中制定了一个稳健的投影平滑校准概念，并给出了新的重新校准算法，以在这个定义下以多项式时间复杂度校准预测器。投影平滑校准为多类别预测提供了强大的保证。

    Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.   Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for al
    
[^24]: 可扩展大型语言模型微调的差分隐私零阶方法

    Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning

    [https://arxiv.org/abs/2402.07818](https://arxiv.org/abs/2402.07818)

    本文研究了差分隐私零阶方法在大型语言模型微调中的应用，该方法通过使用零阶梯度来避免传统优化方法的可扩展性瓶颈，实现了在隐私、效用和可扩展性之间的良好平衡。

    

    在特定任务的数据集上进行微调是利用预训练语言模型的强大能力进行各种下游任务的广泛接受的范例。由于预训练语言模型微调的普及以及与之相关的隐私问题，差分隐私预训练语言模型微调引起了越来越多的关注，以保护特定任务数据集的隐私。差分隐私预训练语言模型微调方法的设计核心是在隐私、效用和可扩展性之间达到满意的权衡。大多数现有方法都是基于DP-SGD的创新性工作。尽管将DP-SGD的可扩展性推到了极限，但基于DP-SGD的微调方法不幸地受到了SGD固有低效率的限制。在本文中，我们研究了DP零阶方法在LLM预训练中的潜力，该方法通过用更高效的零阶梯度来近似梯度，避免了SGD的可扩展性瓶颈。与将零阶方法作为一种替代方法进行处理不同，我们引入了一种新的割接框架，该框架能够以非常接近的方式模拟DP-SGD的基本操作，然后利用零阶优化方法来近似梯度。

    Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
    
[^25]: 检索增强的思维过程作为序列决策制定

    Retrieval-Augmented Thought Process as Sequential Decision Making

    [https://arxiv.org/abs/2402.07812](https://arxiv.org/abs/2402.07812)

    检索增强思维过程（RATP）通过多步决策和蒙特卡洛树搜索，以及Q值估计器，解决了大型语言模型在隐私、产生幻觉和处理长文本方面的挑战，并在处理私人数据的问答任务中实现了50%的性能提升。

    

    大型语言模型(LLM)展示了其强大的辅助人类并展现出"智能的火花"的能力。然而，几个开放挑战阻碍了它们的广泛应用：如对隐私的关注、倾向于产生幻觉、难以处理长文本。在本研究中，我们通过引入检索增强思维过程(RATP)来解决这些挑战。通过获取外部知识，RATP将LLM的思考生成过程定式为多步决策过程。为了优化这种思考过程，RATP利用蒙特卡洛树搜索，并学习了一个Q值估计器，实现了高效的推理。在处理具有私人数据的问答任务时，LLM训练方法受到伦理和安全问题的限制。RATP在上下文检索增强语言模型的基础上实现了50%的性能提升。

    Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
    
[^26]: Sourcerer: 基于样本的最大熵源分布估计

    Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation

    [https://arxiv.org/abs/2402.07808](https://arxiv.org/abs/2402.07808)

    本论文提出了一种基于样本的最大熵源分布估计方法，通过优先保留不确定性来选择合适的源分布。该方法利用切片-瓦石坦斯坦距离对数据集和模拟进行衡量，适用于具有难以处理似然函数的模拟器。实验表明，该方法可以恢复更高熵的源分布，同时保持模拟的准确性。

    

    科学建模应用通常需要估计与观测数据集一致的参数分布，被称为源分布估计的推理任务。然而，这个问题可能是不适定的，因为许多不同的源分布可能产生相同的数据分布一致的模拟结果。为了在众多同样有效的源中做出有原则的选择，我们提出了一种目标最大熵分布的方法，即优先保留尽可能多的不确定性。我们的方法完全基于样本，利用切片-瓦石坦斯坦距离来衡量数据集与模拟之间的差异，因此适用于具有难以处理的似然函数的模拟器。我们在几个任务上对我们的方法进行了基准测试，并表明它可以恢复具有更高熵的源分布，而不牺牲模拟的准确性。最后，为了证明我们方法的实用性，我们推断源分布...

    Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation. This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent simulations. To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations - and thus suitable for simulators with intractable likelihoods. We benchmark our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the simulations. Finally, to demonstrate the utility of our approach, we infer source distri
    
[^27]: 面向扩散模型的一种一致性训练数学理论的探索

    Towards a mathematical theory for consistency training in diffusion models

    [https://arxiv.org/abs/2402.07802](https://arxiv.org/abs/2402.07802)

    本文探索了面向扩散模型中一致性训练的理论基础，证明了在一致性学习中，步骤数量需要超过$d^{5/2}/\varepsilon$的阶数，能够生成与目标分布接近的样本。

    

    一致性模型被提出来减少扩散模型采样阶段的高计算开销，实现了单步采样并达到了最先进的实证性能。一致性模型在训练阶段被整合进来，试图训练一系列的一致性函数，能够将扩散过程中的任何时间步骤的任何点映射回其起始点。尽管在实证上取得了成功，但关于一致性训练的全面理论理解还是很难得到的。本文对一致性模型的理论基础进行了初步的探索。我们证明，为了在分布中生成与目标在$\varepsilon$接近的样本（通过某种Wasserstein度量衡量），一致性学习中的步骤数量需要超过$d^{5/2}/\varepsilon$的阶数，其中$d$是数据维度。我们的理论为一致性模型的有效性和有效性提供了严格的洞察。

    Consistency models, which were proposed to mitigate the high computational overhead during the sampling phase of diffusion models, facilitate single-step sampling while attaining state-of-the-art empirical performance. When integrated into the training phase, consistency models attempt to train a sequence of consistency functions capable of mapping any point at any time step of the diffusion process to its starting point. Despite the empirical success, a comprehensive theoretical understanding of consistency training remains elusive. This paper takes a first step towards establishing theoretical underpinnings for consistency models. We demonstrate that, in order to generate samples within $\varepsilon$ proximity to the target in distribution (measured by some Wasserstein metric), it suffices for the number of steps in consistency learning to exceed the order of $d^{5/2}/\varepsilon$, with $d$ the data dimension. Our theory offers rigorous insights into the validity and efficacy of cons
    
[^28]: 无调参的随机优化

    Tuning-Free Stochastic Optimization

    [https://arxiv.org/abs/2402.07793](https://arxiv.org/abs/2402.07793)

    本文提出了一种无调参的随机优化算法，能够在只给出问题参数的粗略提示的情况下，与最优调参优化算法的性能相匹配。并且在有界的优化领域中证明了此算法的可行性，并探讨了在无界域中的条件。

    

    大规模机器学习问题使得调参的成本越来越高昂。这导致了需要能够即时自我调整的算法的需求。我们将“无调参”算法的概念形式化，即只给出问题参数的粗略提示即可与最优调参优化算法的性能相匹配，误差为对数多项式因子。我们特别考虑能够与最优调参的随机梯度下降(SGD)相匹配的算法。当优化的域是有界的时候，我们证明了调参自由与SGD的匹配是可能的，并且通过几个现有算法实现了这一点。我们证明了当优化的域是无界的时候，对于最小化凸平滑或者Lipschitz函数的任务，无调参优化是不可能的。我们讨论了在无界域中，何种情况下可以实现无调参优化。特别地，我们展示了最近提出的 DoG 和 DoWG 算法在噪声分布足够时是无调参的。

    Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is suf
    
[^29]: 使用NVIDIA FLARE来增强大规模模型的联邦学习

    Empowering Federated Learning for Massive Models with NVIDIA FLARE

    [https://arxiv.org/abs/2402.07792](https://arxiv.org/abs/2402.07792)

    本研究探索了如何利用NVIDIA FLARE的联邦学习能力，解决了处理和利用分布式数据的挑战，并通过参数高效和全面监督微调大型语言模型，提高了自然语言处理和生物药物应用的准确性和鲁棒性。

    

    在人工智能和大型语言模型的不断发展中，有效处理和利用数据已成为一个关键挑战。大多数最先进的机器学习算法都是以数据为中心的。然而，由于隐私、监管、地缘政治、版权问题以及移动大型数据集所需的巨大工作量等各种因素，必要的数据并不总是能够集中存储。在本文中，我们探讨了通过NVIDIA FLARE实现的联邦学习如何应对这些挑战，具备易于扩展集成的能力，使得自然语言处理和生物药物应用中的大型语言模型能够进行参数高效和全面监督微调，以提高其准确性和鲁棒性。

    In the ever-evolving landscape of artificial intelligence (AI) and large language models (LLMs), handling and leveraging data effectively has become a critical challenge. Most state-of-the-art machine learning algorithms are data-centric. However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets. In this paper, we explore how federated learning enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full supervised fine-tuning of LLMs for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness.
    
[^30]: 从不确定性到精确性：通过校准提升二元分类器的性能

    From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration

    [https://arxiv.org/abs/2402.07790](https://arxiv.org/abs/2402.07790)

    本研究通过分析校准度量对分数失真的敏感性，引入了一种改进的局部校准分数，并推广了局部回归作为有效的重新校准工具，提高了二元分类器在敏感决策领域的性能表现。

    

    传统上，对二元分类器性能的评估主要集中在准确性等度量上。然而，这些度量常常忽视了模型的固有不确定性，尤其是在涉及敏感决策领域（如金融或医疗保健）时。鉴于模型预测的分数通常被视为事件概率，校准对于准确解释是至关重要的。在我们的研究中，我们分析了各种校准度量对分数失真的敏感性，并引入了一种改进的度量，局部校准分数。通过比较重新校准方法，我们倡导使用局部回归，强调其作为有效重新校准工具和更平滑可视化的双重作用。我们应用这些发现在一个真实场景中，使用随机森林分类器和回归器预测信用违约，并同时测量校准性能优化过程中的校准情况。

    The assessment of binary classifier performance traditionally centers on discriminative ability using metrics, such as accuracy. However, these metrics often disregard the model's inherent uncertainty, especially when dealing with sensitive decision-making domains, such as finance or healthcare. Given that model-predicted scores are commonly seen as event probabilities, calibration is crucial for accurate interpretation. In our study, we analyze the sensitivity of various calibration measures to score distortions and introduce a refined metric, the Local Calibration Score. Comparing recalibration methods, we advocate for local regressions, emphasizing their dual role as effective recalibration tools and facilitators of smoother visualizations. We apply these findings in a real-world scenario using Random Forest classifier and regressor to predict credit default while simultaneously measuring calibration during performance optimization.
    
[^31]: HYPO：超球面离群分布泛化

    HYPO: Hyperspherical Out-of-Distribution Generalization

    [https://arxiv.org/abs/2402.07785](https://arxiv.org/abs/2402.07785)

    HYPO是一个在超球面空间中学习域不变表示的框架，通过内类变化和间类分离原则的引导，提高了离群泛化性能。

    

    离群（OOD）泛化对于在现实世界中部署的机器学习模型至关重要。然而，实现这一点可能从根本上具有挑战性，因为它需要学习跨不同领域或环境的不变特征的能力。在本文中，我们提出了一种新颖的框架HYPO（超球面OOD泛化），它能够证明在超球面空间中学习域不变表示。具体而言，我们的超球面学习算法是根据内类变化和间类分离原则进行引导的，确保来自同一类别的特征（跨不同训练领域）与其类别原型紧密对齐，而不同类别的原型之间则被最大化地分离。我们进一步提供了关于我们的原型学习目标如何改善OOD泛化界限的理论证明。通过对具有挑战性的OOD基准的大量实验，我们证明我们的方法优于竞争基准

    Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines
    
[^32]: 使用强化学习进行IR感知的ECO时序优化

    IR-Aware ECO Timing Optimization Using Reinforcement Learning

    [https://arxiv.org/abs/2402.07781](https://arxiv.org/abs/2402.07781)

    本文提出了一种使用强化学习进行IR感知的ECO时序优化的方法，该方法通过门尺寸调整纠正由IR降低引起的时序退化，并且相较于传统方法在性能和运行时间上都具有优势。

    

    在晚期阶段的工程变更订单（ECOs）通过最小的设计修复来从过多的IR降低导致的时序偏移中恢复。本文将IR感知的时序分析和使用强化学习（RL）进行ECO时序优化相结合。该方法在物理设计和功耗网格综合之后运行，并通过门尺寸调整纠正由IR降低引起的时序退化。它将拉格朗日松弛（LR）技术融入一种新颖的RL框架中，该框架训练一个关系图卷积网络（R-GCN）代理，按顺序调整门尺寸以修复时序违规。R-GCN代理优于传统的仅使用LR的算法：在开放式45nm工艺中，它将延迟-面积权衡曲线的帕累托前沿向左移动，并通过在等质量时使用训练模型进行快速推理，节省运行时间。RL模型可在时序规范间转移，并可通过零样本学习或微调在未见设计上转移。

    Engineering change orders (ECOs) in late stages make minimal design fixes to recover from timing shifts due to excessive IR drops. This paper integrates IR-drop-aware timing analysis and ECO timing optimization using reinforcement learning (RL). The method operates after physical design and power grid synthesis, and rectifies IR-drop-induced timing degradation through gate sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel RL framework, which trains a relational graph convolutional network (R-GCN) agent to sequentially size gates to fix timing violations. The R-GCN agent outperforms a classical LR-only algorithm: in an open 45nm technology, it (a) moves the Pareto front of the delay-area tradeoff curve to the left and (b) saves runtime over the classical method by running fast inference using trained models at iso-quality. The RL model is transferable across timing specifications, and transferable to unseen designs with zero-shot learning or fine tuning.
    
[^33]: 多级优化控制与神经代理模型

    Multi-level Optimal Control with Neural Surrogate Models

    [https://arxiv.org/abs/2402.07763](https://arxiv.org/abs/2402.07763)

    该研究通过使用神经网络代理模型替代低级别的优化层次，提出了一种多级优化控制方法，用于解决最优执行器和控制设计问题。与传统方法相比，该方法能够快速且效果良好地确定最优执行器设计。

    

    本研究将最优执行器和控制设计作为一个多级优化问题进行研究，其中执行器设计基于相关最优闭环的性能进行评估。对于给定的执行器实现，评估最优闭环是一项计算复杂的任务，因此提出了使用神经网络代理来替代这一较低级别的优化层次，从而使得能够使用基于梯度和无梯度的一致性优化方法快速确定最优执行器设计。所提出的代理模型和优化方法的有效性在与热控制中最优执行器位置相关的测试中进行了评估。

    Optimal actuator and control design is studied as a multi-level optimisation problem, where the actuator design is evaluated based on the performance of the associated optimal closed loop. The evaluation of the optimal closed loop for a given actuator realisation is a computationally demanding task, for which the use of a neural network surrogate is proposed. The use of neural network surrogates to replace the lower level of the optimisation hierarchy enables the use of fast gradient-based and gradient-free consensus-based optimisation methods to determine the optimal actuator design. The effectiveness of the proposed surrogate models and optimisation methods is assessed in a test related to optimal actuator location for heat control.
    
[^34]: 可扩展的稀疏特定背景下因果系统的结构学习

    Scalable Structure Learning for Sparse Context-Specific Causal Systems

    [https://arxiv.org/abs/2402.07762](https://arxiv.org/abs/2402.07762)

    提出了一种可扩展的混合算法，用于学习特定背景模型，通过结合基于顺序的MCMC算法和稀疏性假设实现可扩展学习，该方法在准确性和可扩展性方面表现良好。

    

    已经提出了几种表示共同分布分类变量之间特定背景下关系的方法，并且提出了结构学习算法。然而，由于大量特定背景模型的存在，现有的基于优化的方法在可扩展性方面受到限制，而基于约束的方法比约束DAG学习算法更容易出错，因为必须测试更多关系。我们提出了一种混合算法来学习特定背景模型，能够扩展到数百个变量，并且测试的约束不多于标准DAG学习算法。通过结合基于顺序的MCMC算法和类似于DAG模型常用的稀疏性假设，实现了可扩展的学习。为了实现这种方法，我们解决了Alon和Balogh最近提出的一个开放问题的特殊情况。经过在合成数据和真实世界示例上的实验证明，该方法在准确性和可扩展性方面表现良好。

    Several approaches to graphically representing context-specific relations among jointly distributed categorical variables have been proposed, along with structure learning algorithms. While existing optimization-based methods have limited scalability due to the large number of context-specific models, the constraint-based methods are more prone to error than even constraint-based DAG learning algorithms since more relations must be tested. We present a hybrid algorithm for learning context-specific models that scales to hundreds of variables while testing no more constraints than standard DAG learning algorithms. Scalable learning is achieved through a combination of an order-based MCMC algorithm and sparsity assumptions analogous to those typically invoked for DAG models. To implement the method, we solve a special case of an open problem recently posed by Alon and Balogh. The method is shown to perform well on synthetic data and real world examples, in terms of both accuracy and scal
    
[^35]: 对Transformer中逐步推理的理解: 一个合成图导航模型的研究

    Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model

    [https://arxiv.org/abs/2402.07757](https://arxiv.org/abs/2402.07757)

    该论文研究了Transformer中的逐步推理，提出了一个合成图导航模型来探索逐步推理的底层机制，并通过该模型在合成任务上的实验证明了几个关键现象的存在。

    

    逐步推理协议，如scratchpads和chain-of-thought，通过将复杂问题分解为一系列较简单的子问题，帮助语言模型解决复杂问题。尽管这些协议在性能上取得了显著的提升，但逐步推理的底层机制仍然难以理解。为了解决这个问题，我们提出在合成任务中研究自回归Transformer模型，该任务体现了问题的多步性质，其中逐步推理通常最有用。具体而言，我们定义了一个图导航问题，模型的任务是在图上从起始节点到目标节点的路径上进行遍历。尽管简单，我们发现我们可以通过经验重现和分析观察到的几个现象：(i)逐步推理推理间隙，我们发现其原因在于训练数据的结构；(ii)在模型生成中的多样性-准确性权衡，随着采样温度的变化；(iii)模型在输出中的简单性偏见。

    Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's out
    
[^36]: 思想传播：扩散语言模型中的思维链推理

    Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models

    [https://arxiv.org/abs/2402.07754](https://arxiv.org/abs/2402.07754)

    本文介绍了一种将扩散模型与思维链推理集成的方法，通过扩散传播推理步骤，提供了更大的灵活性和推理能力。实验证明了该方法在数学问题中的有效性，并展示了自我纠正能力和推理技术的潜力。

    

    扩散模型在文本处理中引起了关注，相对传统的自回归模型具有许多潜在优势。本文探讨了将扩散模型与思维链（CoT）集成的方法，CoT是一种在自回归语言模型中改进推理能力的成熟技术。我们提出了思维扩散（DoT）模型，允许推理步骤通过扩散过程在时间上传播。与传统的自回归语言模型逐个token从左到右做出决策的方式相比，DoT在计算和推理性能之间具有更大的灵活性。我们的实验证明了DoT在多位数乘法和小学数学问题中的有效性。此外，DoT展示了有希望的自我纠正能力，并从现有的增强推理技术（如自一致解码）中受益。我们的发现有助于理解和发展推理能力。

    Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
    
[^37]: 混合Q函数：推动连续动作领域合作MARL中基于值的方法的进展

    Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains

    [https://arxiv.org/abs/2402.07752](https://arxiv.org/abs/2402.07752)

    本文提出了一种新颖的多智能体基于值的算法，混合Q函数（MQF），提高了基于值的方法在多智能体连续动作领域中的效果，通过同时评估多个动作来解决真实回报估计和局部最优解问题。

    

    在连续动作领域中高效解决多智能体学习问题是一项具有挑战性的任务。虽然基于值的算法在应用于离散动作领域时具有样本效率优势，但在处理连续动作时通常效率低下。另一方面，策略型算法通过利用评论家网络来引导学习过程和稳定梯度估计来解决这一挑战。但是，这些方法在估计真实回报和陷入局部最优解方面存在限制，导致策略效率低下且通常次优。在本文中，我们远离进一步增强评论家网络的趋势，专注于通过同时评估多个动作来改进基于值的方法在多智能体连续动作领域中的有效性。我们提出了一种新颖的多智能体基于值的算法，混合Q函数（MQF），灵感来自于Q函数，使智能体能够将其状态转化为基底。

    Tackling multi-agent learning problems efficiently is a challenging task in continuous action domains. While value-based algorithms excel in sample efficiency when applied to discrete action domains, they are usually inefficient when dealing with continuous actions. Policy-based algorithms, on the other hand, attempt to address this challenge by leveraging critic networks for guiding the learning process and stabilizing the gradient estimation. The limitations in the estimation of true return and falling into local optima in these methods result in inefficient and often sub-optimal policies. In this paper, we diverge from the trend of further enhancing critic networks, and focus on improving the effectiveness of value-based methods in multi-agent continuous domains by concurrently evaluating numerous actions. We propose a novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired from the idea of Q-Functionals, that enables agents to transform their states into basis 
    
[^38]: 使用一组好模型进行预测性客户流失

    Predictive Churn with the Set of Good Models

    [https://arxiv.org/abs/2402.07745](https://arxiv.org/abs/2402.07745)

    本文研究了在现代大众市场应用中，随时间更新的机器学习模型可能导致不稳定的预测结果，通过研究预测性多样性，量化了这种预测性流失，并通过Rashomon集合来分析模型更新中的预期流失。

    

    现代大众市场应用中的机器学习模型经常会随时间进行更新。面临的一个主要挑战是，尽管整体性能在提升，但这些更新可能会以不可预测的方式改变特定模型的预测结果。在实践中，研究人员通过量化模型更新前后的不稳定预测数量来衡量预测性流失。本文通过预测性多样性的角度研究了这种效应，即在一组接近最优模型（Rashomon集合）中存在冲突预测的普遍性。我们展示了如何利用传统的预测性多样性度量来研究这组潜在模型的预期流失，即可能用于替换基线模型在部署中的模型集合。我们从不同的角度给出了模型集内Rashomon集合之间预期流失的理论结果，并通过Rashomon集合表征了模型更新中的预期流失，结合我们的分析结果。

    Machine learning models in modern mass-market applications are often updated over time. One of the foremost challenges faced is that, despite increasing overall performance, these updates may flip specific model predictions in unpredictable ways. In practice, researchers quantify the number of unstable predictions between models pre and post update -- i.e., predictive churn. In this paper, we study this effect through the lens of predictive multiplicity -- i.e., the prevalence of conflicting predictions over the set of near-optimal models (the Rashomon set). We show how traditional measures of predictive multiplicity can be used to examine expected churn over this set of prospective models -- i.e., the set of models that may be used to replace a baseline model in deployment. We present theoretical results on the expected churn between models within the Rashomon set from different perspectives. And we characterize expected churn over model updates via the Rashomon set, pairing our analy
    
[^39]: 实现智能体、人类和环境之间的统一对齐

    Towards Unified Alignment Between Agents, Humans, and Environment

    [https://arxiv.org/abs/2402.07744](https://arxiv.org/abs/2402.07744)

    本文介绍了统一对齐原则 ($\mathbf{UA}^2$)，旨在实现智能体与人类意图、环境动态和自我约束的统一对齐，提出了引入实际特性进行概念验证研究的方法。

    

    基于基础模型的快速进展导致了自主智能体的繁荣，这些智能体利用基础模型的通用能力进行推理、决策和环境交互。然而，当在复杂、现实的环境中运行时，智能体的效能仍然有限。在本研究中，我们引入了统一对齐原则，即同时对齐智能体与人类意图、环境动态和自我约束（如货币预算限制）。从统一对齐 ($\mathbf{UA}^2$) 的视角出发，我们回顾了当前智能体研究的现状，并指出了现有智能体基准和方法候选中被忽视的因素。我们还通过为WebShop引入实际特性进行了概念验证研究，包括使用用户配置文件来展示意图、个性化重新排名以应对复杂的环境动态和运行时成本统计。

    The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
    
[^40]: 多任务策略学习中基于任务条件的视觉特征自适应

    Task-conditioned adaptation of visual features in multi-task policy learning

    [https://arxiv.org/abs/2402.07739](https://arxiv.org/abs/2402.07739)

    本文通过任务条件的自适应器，在多任务策略学习的背景下，调整预训练的大型视觉模型，使其能够解决多个任务，并且无需微调预先训练的权重。

    

    成功地解决各种任务是自主代理的核心能力，这需要灵活地调整底层的决策策略，并且如我们在这项工作中所提出的，还需要调整底层的感知模块。一个类比的论证是人类的视觉系统，它使用自上而下的信号来专注于当前任务。类似地，在这项工作中，我们在多任务策略学习的上下文中，通过特定的下游任务来调整预先训练的大视觉模型。我们引入了基于任务条件的适配器，在不需要微调任何预先训练权重的情况下，与通过行为克隆训练的单一策略结合使用，能够解决多个任务。我们在策略和视觉适配器上根据任务嵌入进行条件化，如果任务是已知的，则可以在推理过程中选择任务嵌入，否则可以从一组示例演示中进行推断。为此，我们提出了一种新的基于优化的估计器。我们在...（摘要未完成）

    Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on 
    
[^41]: 基于上下文学习的通用链接预测器

    Universal link predictor by In-context Learning

    [https://arxiv.org/abs/2402.07738](https://arxiv.org/abs/2402.07738)

    这项工作介绍了一种基于上下文学习的通用链接预测器(UniLP)，它将启发式方法的广泛适用性和参数模型的模式学习能力相结合，实现了自主学习目标图中的链接模式并具有跨不同图的泛化能力。

    

    链接预测是图机器学习中的一项关键任务，其目标是推断图中缺失或未来的链接。传统方法利用基于广泛观察到的连接模式的启发式方法，具有广泛的适用性和泛化性，无需进行模型训练。尽管这些方法很有用，但它们受制于人为推导的启发式方法，缺乏数据驱动方法的适应性。相反，参数链接预测器擅长于从数据中自动学习连接模式并取得最先进的效果，但在不同图之间直接转移上存在问题。相反，它需要进行大量的训练和超参数优化来适应目标图。在这项工作中，我们介绍了通用链接预测器（UniLP），这是一种新颖的模型，将启发式方法的广泛适用性与参数模型的模式学习能力相结合。UniLP设计为自主学习目标图中的链接模式，并具有跨不同图的泛化能力。

    Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph. Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training. Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches. Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs. Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models. UniLP is designed to autono
    
[^42]: 用BAM进行图结构推断：引入双线性注意机制

    Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism

    [https://arxiv.org/abs/2402.07735](https://arxiv.org/abs/2402.07735)

    本论文提出了一种利用BAM进行图结构推断的方法。通过神经网络模型，通过变形的耦合模拟输入数据进行训练，仅需通过一次前向传递即可进行推断。通过利用结构方程模型和随机生成的多变量切比雪夫多项式来模拟训练数据，方法能够泛化到线性和各种非线性依赖关系。引入了双线性注意机制（BAM）来处理依赖关系，该机制在转换数据的协方差矩阵水平上运行，并尊重对称正定矩阵流形的几何特性。实证评估证明了方法的有效性和性能。

    

    在统计学和机器学习中，检测数据集中的依赖关系是一个核心挑战。我们提出了一种新颖的神经网络模型，用于监督图结构学习，即学习观测数据和它们的基本依赖结构之间的映射。该模型通过变形的耦合模拟输入数据进行训练，并且仅需通过训练网络进行一次前向传递即可进行推断。通过利用结构方程模型，并通过随机生成的多变量切比雪夫多项式来模拟训练数据，我们的方法展示了在线性和各种非线性依赖关系之间的强大泛化能力。我们引入了一种新的双线性注意机制（BAM），用于显式处理依赖信息，该机制在转换数据的协方差矩阵水平上运行，并尊重对称正定矩阵流形的几何特性。实证评估展示了方法的有效性和性能。

    In statistics and machine learning, detecting dependencies in datasets is a central challenge. We propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies. We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices. Empirical evaluation demonstrates th
    
[^43]: 通过分数阻尼库仑方程证明重尾SDEs的泛化界限

    Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation

    [https://arxiv.org/abs/2402.07723](https://arxiv.org/abs/2402.07723)

    本论文通过分数阻尼库仑方程证明了重尾SDE的高概率泛化界限，并且相对于参数维度，界限的依赖性要好于p。

    

    过去几年来，理解重尾随机优化算法的泛化性能引起了越来越多的关注。在利用重尾随机微分方程作为代理来阐明随机优化器的有趣方面时，先前的工作要么提供预期的泛化界限，要么引入了不可计算的信息论术语。为了解决这些缺点，在本文中，我们证明了重尾SDE的高概率泛化界限，这些界限不含任何非平凡的信息论术语。为了实现这个目标，我们基于估计与所谓的分数阻尼库仑方程相关联的熵流，开发了新的证明技术（这是一种控制相应重尾SDE分布演化的偏微分方程）。除了获得高概率界限之外，我们还展示了我们的界限相对于参数维度的依赖性要好于p。

    Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to p
    
[^44]: LoRA-drop：基于输出评估的高效LoRA参数剪枝

    LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation

    [https://arxiv.org/abs/2402.07721](https://arxiv.org/abs/2402.07721)

    本文提出了LoRA-drop方法，通过分析LoRA输出评估参数的重要性，并且保留重要层的LoRA，其余层共享相同参数。实验结果表明LoRA-drop有很好的效果。

    

    低秩适应（LoRA）为每个层引入辅助参数，以在有限的计算资源下微调预训练模型。但是，当扩展到更大的模型时，仍然面临资源消耗的挑战。先前的研究通过评估不同层的LoRA参数的重要性来采用剪枝技术来解决这个问题。然而，这些努力只分析了参数的特征以评估其重要性。事实上，与参数和数据相关的LoRA的输出是直接影响冻结模型的因素。为此，我们提出了LoRA-drop，通过分析LoRA输出来评估参数的重要性。我们保留重要层的LoRA，而其他层的LoRA共享相同的参数。在NLU和NLG任务上进行了充分的实验，证明了LoRA-drop的有效性。

    Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.
    
[^45]: 模型崩溃解密：回归案例研究

    Model Collapse Demystified: The Case of Regression

    [https://arxiv.org/abs/2402.07712](https://arxiv.org/abs/2402.07712)

    本研究在核回归的简化环境中解析了模型崩溃现象，并发现了模型能够处理虚假数据与性能完全崩溃之间的交叉点。通过提出基于自适应正则化的策略，成功缓解了模型崩溃问题。这些发现通过实验证实。

    

    在像ChatGPT这样的大型语言模型的时代，"模型崩溃"现象指的是模型在递归地训练自身上一代又一代生成的数据时，其性能逐渐降低，最终变得完全无用，即模型崩溃。在这项工作中，我们在核回归的简化环境中研究了这一现象，并获得了结果，显示模型能够处理虚假数据与模型性能完全崩溃之间存在明显的交叉点。在多项式衰减的光谱和源条件下，我们获得了修改后的缩放定律，展示了从快速到缓慢速率的新交叉现象。我们还提出了基于自适应正则化的简单策略来缓解模型崩溃。我们的理论结果通过实验证实。

    In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.
    
[^46]: 在线顺序决策中的未知延迟问题

    Online Sequential Decision-Making with Unknown Delays

    [https://arxiv.org/abs/2402.07703](https://arxiv.org/abs/2402.07703)

    本文提出了在在线顺序决策中处理未知延迟问题的三个延迟算法族，并提供了相应的遗憾界限。

    

    在在线顺序决策领域，我们利用在线凸优化（OCO）框架解决了具有延迟的问题，其中决策的反馈可能以未知延迟到达。与之前仅限于欧几里得范数和梯度信息的研究不同，我们提出了三个基于近似解的延迟算法族，处理不同类型的接收反馈。我们提出的算法是多功能且适用于通用范数。具体地，我们引入了一系列针对具有完整损失函数信息反馈的延迟规范化领导算法族，一系列针对具有梯度信息反馈的延迟镜像下降算法族，以及一系列针对相应决策点损失函数梯度值信息反馈的简化延迟镜像下降算法族。对于每种类型的算法，我们提供了相应的遗憾界限。

    In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
    
[^47]: Bayesian优化中的边界探索与未知物理约束

    Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints

    [https://arxiv.org/abs/2402.07692](https://arxiv.org/abs/2402.07692)

    本文提出了一种新的贝叶斯优化方法BE-CBO，通过有效地探索可行和不可行设计之间的边界，来解决在实际应用中优化未知约束的未知函数的问题。

    

    贝叶斯优化已成功应用于优化评估次数严重限制的黑盒函数。然而，在许多实际应用中，由于一些物理或系统限制，很难或者不可能事先知道哪些设计是可行的。这些问题导致了更加困难的问题，即优化未知约束的未知函数。在本文中，我们观察到在这种情况下，最优解通常位于设计空间的可行和不可行区域之间的边界上，这使得问题比具有内部最优解的情况更加困难。受到这一观察的启发，我们提出了一种新的贝叶斯优化方法BE-CBO，可以有效地探索可行和不可行设计之间的边界。为了确定边界，我们使用一组神经网络来学习约束，这些神经网络在捕捉复杂边界方面优于标准高斯过程。

    Bayesian optimization has been successfully applied to optimize black-box functions where the number of evaluations is severely limited. However, in many real-world applications, it is hard or impossible to know in advance which designs are feasible due to some physical or system limitations. These issues lead to an even more challenging problem of optimizing an unknown function with unknown constraints. In this paper, we observe that in such scenarios optimal solution typically lies on the boundary between feasible and infeasible regions of the design space, making it considerably more difficult than that with interior optima. Inspired by this observation, we propose BE-CBO, a new Bayesian optimization method that efficiently explores the boundary between feasible and infeasible designs. To identify the boundary, we learn the constraints with an ensemble of neural networks that outperform the standard Gaussian Processes for capturing complex boundaries. Our method demonstrates superio
    
[^48]: 弱监督人物重识别的对比多实例学习

    Contrastive Multiple Instance Learning for Weakly Supervised Person ReID

    [https://arxiv.org/abs/2402.07685](https://arxiv.org/abs/2402.07685)

    引入对比多实例学习（CMIL）框架，解决了弱监督人物重识别中数据集标注不准确的问题。CMIL利用对比损失技术，在三个数据集上取得了与最先进性能相匹配的结果，并优于所有基准方法。

    

    获得大规模、精确标记的人物重识别（ReID）数据集是一个巨大的挑战。弱监督ReID已经开始解决这个问题，尽管其性能跑不过完全监督的方法。为了解决这个问题，我们引入了对比多实例学习（CMIL），这是一个专为更有效的弱监督ReID而设计的新框架。CMIL的特点是只需要一个模型，不需要伪标签，同时利用对比损失的技术，这在所有以前的基于MIL的方法中都不存在，但在传统的ReID性能上有显著提升。通过对三个数据集进行大量实验和分析，CMIL不仅在大规模SYSU-30k数据集上与最先进的性能相匹配，且假设更少，而且在WL-market1501和弱标记的MUddy racer重识别数据集（WL-MUDD）上始终优于所有基准方法。我们还介绍并发布了WL-MUDD数据集。

    The acquisition of large-scale, precisely labeled datasets for person re-identification (ReID) poses a significant challenge. Weakly supervised ReID has begun to address this issue, although its performance lags behind fully supervised methods. In response, we introduce Contrastive Multiple Instance Learning (CMIL), a novel framework tailored for more effective weakly supervised ReID. CMIL distinguishes itself by requiring only a single model and no pseudo labels while leveraging contrastive losses -- a technique that has significantly enhanced traditional ReID performance yet is absent in all prior MIL-based approaches. Through extensive experiments and analysis across three datasets, CMIL not only matches state-of-the-art performance on the large-scale SYSU-30k dataset with fewer assumptions but also consistently outperforms all baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an 
    
[^49]: 基于协方差神经网络的大脑年龄预测的基础模型研究

    Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks

    [https://arxiv.org/abs/2402.07684](https://arxiv.org/abs/2402.07684)

    本文研究了基于协方差神经网络的基础模型NeuroVNN，该模型可以通过解析解剖学特征进行大脑年龄预测，具有可迁移性。这对于理解神经退行性疾病和认知衰退的增加风险具有重要意义。

    

    大脑年龄是通过使用机器学习算法从神经影像数据集中得出的生物年龄估计。相对于实际年龄，大脑年龄的增长可以反映出神经退行性和认知衰退的增加脆弱性。本文研究了基于协方差神经网络的神经年龄预测应用的基础模型-NeuroVNN。NeuroVNN是在健康人群上作为回归模型预训练的，使用皮层厚度特征进行预测实际年龄，并通过微调对不同神经学背景下的大脑年龄进行估计。重要的是，NeuroVNN为大脑年龄增加了解剖学解释性，并具有“无标度”的特性，使其能够迁移到按任意大脑图谱策划的数据集。我们的结果表明，NeuroVNN可以在不同人群中提取与生物相关的合理大脑年龄估计，并成功迁移到与之不同维度的数据集。

    Brain age is the estimate of biological age derived from neuroimaging datasets using machine learning algorithms. Increasing brain age with respect to chronological age can reflect increased vulnerability to neurodegeneration and cognitive decline. In this paper, we study NeuroVNN, based on coVariance neural networks, as a paradigm for foundation model for the brain age prediction application. NeuroVNN is pre-trained as a regression model on healthy population to predict chronological age using cortical thickness features and fine-tuned to estimate brain age in different neurological contexts. Importantly, NeuroVNN adds anatomical interpretability to brain age and has a `scale-free' characteristic that allows its transference to datasets curated according to any arbitrary brain atlas. Our results demonstrate that NeuroVNN can extract biologically plausible brain age estimates in different populations, as well as transfer successfully to datasets of dimensionalities distinct from that f
    
[^50]: 用于安全关键的行人检测的基于流的可信度度量

    A Flow-based Credibility Metric for Safety-critical Pedestrian Detection

    [https://arxiv.org/abs/2402.07642](https://arxiv.org/abs/2402.07642)

    该论文提出了一种基于光流信号和行人边界框的可信度度量方法，能够帮助开发人员识别安全关键的误检。

    

    安全是自动驾驶中感知的最重要方面。然而，目前的物体检测的主要安全问题是标准评估方案使用不关注安全的度量来证明足够的检测性能。因此，必须利用补充领域知识来突出评估任务中的安全关键的误检。为了解决这个问题，本文引入了一种新颖的可信度度量，称为c-flow，用于行人边界框。为此，c-flow利用图像序列中的补充光流信号，并增强了对安全关键的误检的分析，而无需额外的标签。我们在一个大型自动驾驶数据集上实现和评估了c-flow与一种先进的行人检测器。我们的分析证明，c-flow可以帮助开发人员识别安全关键的误检。

    Safety is of utmost importance for perception in automated driving (AD). However, a prime safety concern in state-of-the art object detection is that standard evaluation schemes utilize safety-agnostic metrics to argue sufficient detection performance. Hence, it is imperative to leverage supplementary domain knowledge to accentuate safety-critical misdetections during evaluation tasks. To tackle the underspecification, this paper introduces a novel credibility metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow relies on a complementary optical flow signal from image sequences and enhances the analyses of safety-critical misdetections without requiring additional labels. We implement and evaluate c-flow with a state-of-the-art pedestrian detector on a large AD dataset. Our analysis demonstrates that c-flow allows developers to identify safety-critical misdetections.
    
[^51]: 对信息瓶颈的限制更紧的界限，并应用于深度学习

    Tighter Bounds on the Information Bottleneck with Application to Deep Learning

    [https://arxiv.org/abs/2402.07639](https://arxiv.org/abs/2402.07639)

    这个论文提出了一个对信息瓶颈更为紧密的变分界限，可以改善以前的基于信息瓶颈的DNNs的性能，并提供了一种简单方法来显著增强分类器DNNs的对抗鲁棒性。

    

    深度神经网络（DNNs）通过下游任务、目标函数和其他参数来学习引发的潜在表示。学习到的表示的质量影响着DNN的概括能力和新出现的潜在空间的连贯性。信息瓶颈（IB）提供了一种理论上最优的数据建模框架，但通常是难以处理的。最近的研究工作将DNNs与IB相结合，通过应用VAE-inspire的变分方法来近似相互信息的界限，从而提高对抗攻击的鲁棒性。本文引入了一种新的和更紧的变分界限，提高了以前IB-inspire DNNs的性能。这些进展加强了IB及其变分近似作为数据模型框架的论点，并为分类器DNNs的对抗鲁棒性提供了一种简单的方法。

    Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.
    
[^52]: G-Retriever: 用于文本图理解和问题解答的检索增强生成模型

    G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering

    [https://arxiv.org/abs/2402.07630](https://arxiv.org/abs/2402.07630)

    本论文提出了G-Retriever模型，该模型用于文本图理解和问题解答。该模型能够将用户的问题转化为文本回复，并突出显示图形的相关部分。与现有的方法不同，该模型适用于真实世界的文本图形，并可应用于不同的任务，包括场景图理解、常识推理和知识图推理。

    

    在给定具有文本属性的图形的情况下，我们使用户能够使用对话界面向图形提出问题。针对用户的问题，我们的方法提供文本回复并突出显示图形的相关部分。与现有的方法将大型语言模型(LLM)和图神经网络(GNN)以各种方式整合起来不同，它们大多集中在传统图任务(如节点、边和图分类)或者在小型或合成图上回答简单的图查询。相比之下，我们开发了一个灵活的问题回答框架，针对真实世界的文本图形，适用于多个应用，包括场景图理解、常识推理和知识图推理。为实现这个目标，我们首先用来自不同任务的数据开发了我们的图问题回答(GraphQA)基准测试。然后，我们提出了我们的G-Retriever方法，它集成了GNN和LLM的优势。

    Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, 
    
[^53]: 随机梯度流动力学中的测试风险及其弱特征的精确解

    Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features

    [https://arxiv.org/abs/2402.07626](https://arxiv.org/abs/2402.07626)

    本研究通过路径积分方法探索了连续时间随机梯度流动力学中的测试风险，并在小学习率情况下给出了计算纯梯度流动和随机梯度流动的测试风险曲线之间差异的一般公式。通过应用于一个弱特征模型，我们分析了随机项对动力学的修正效果，并与离散时间随机梯度下降的模拟结果进行了比较，结果显示出一致性。

    

    本研究探讨了学习理论中连续时间随机梯度流动力学的测试风险。利用路径积分公式，在小学习率的情况下，提供了计算纯梯度流动和随机梯度流动的测试风险曲线之间差异的一般公式。我们将这一通用理论应用到一个简单的弱特征模型中，该模型展示了双峰现象，并明确计算了动力学中增加的随机项随时间和模型参数的修正。分析结果与离散时间随机梯度下降的模拟进行了比较，显示出良好的一致性。

    We investigate the test risk of continuous-time stochastic gradient flow dynamics in learning theory. Using a path integral formulation we provide, in the regime of a small learning rate, a general formula for computing the difference between test risk curves of pure gradient and stochastic gradient flows. We apply the general theory to a simple model of weak features, which displays the double descent phenomenon, and explicitly compute the corrections brought about by the added stochastic term in the dynamics, as a function of time and model parameters. The analytical results are compared to simulations of discrete-time stochastic gradient descent and show good agreement.
    
[^54]: AutoMathText：使用语言模型进行数学文本的自主数据选择

    AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts

    [https://arxiv.org/abs/2402.07625](https://arxiv.org/abs/2402.07625)

    本论文介绍了一种自主数据选择策略，利用语言模型进行数学文本的自动评估和选择，并通过连续预训练显著提高了数学推理能力。主要创新包括利用元提示语言模型作为验证器，发布了高质量的AutoMathText数据集，并实现了预训练令牌效率的提升。

    

    为了通过持续的预训练改善语言模型在数学推理方面的能力，我们引入了一种新颖的策略，利用基础语言模型进行自主数据选择。与传统的有人工标注数据的监督微调或训练过的分类器不同，我们的方法利用元提示语言模型作为零样本验证器，自主评估和选择高质量的数学内容，并发布了经过策划的开源AutoMathText数据集，其中包含超过200GB的数据。为了证明我们方法的有效性，我们对AutoMathText数据集进行了连续预训练，使得7B参数的Mistral语言模型在MATH数据集上的下游性能大幅提升，而令牌数量比之前的连续预训练工作减少了几个数量级。我们的方法展示了基准的预训练令牌效率提高了2倍，突显了我们方法在增强中的潜力。

    To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
    
[^55]: 神经网络逼近微分方程的正确性验证

    Correctness Verification of Neural Networks Approximating Differential Equations

    [https://arxiv.org/abs/2402.07621](https://arxiv.org/abs/2402.07621)

    这项工作解决了神经网络逼近微分方程解的正确性验证问题，通过定义神经网络的导数近似和建立并行分支算法，有效界定了函数的上下界，并处理了没有输出域先验知识的情况。

    

    验证逼近偏微分方程（PDE）解的神经网络（NN）是提高其可信度和加速部署的重要里程碑，特别是对于安全关键系统。如果成功，这些NN可以成为模拟软件工具的重要组成部分，可以将复杂动态系统的模拟加速100倍以上。然而，这些函数的验证面临重大挑战；如何高效地界定它们或如何表示NN的导数都不是直接的。本文解决了这两个问题。首先，我们将NN的导数定义为有限差分近似。然后，我们将PDE残差界定问题与初始值问题的误差传播进行了形式化。最后，我们首次解决了在没有输出域先验知识的情况下界定NN函数的问题。为此，我们构建了一个并行分支算法，将近似处理领域融合在一起。

    Verification of Neural Networks (NNs) that approximate the solution of Partial Differential Equations (PDEs) is a major milestone towards enhancing their trustworthiness and accelerating their deployment, especially for safety-critical systems. If successful, such NNs can become integral parts of simulation software tools which can accelerate the simulation of complex dynamic systems more than 100 times. However, the verification of these functions poses major challenges; it is not straightforward how to efficiently bound them or how to represent the derivative of the NN. This work addresses both these problems. First, we define the NN derivative as a finite difference approximation. Then, we formulate the PDE residual bounding problem alongside the Initial Value Problem's error propagation. Finally, for the first time, we tackle the problem of bounding an NN function without a priori knowledge of the output domain. For this, we build a parallel branching algorithm that combines the in
    
[^56]: 在可接受的对称约束条件下的全局最优性

    Global optimality under amenable symmetry constraints

    [https://arxiv.org/abs/2402.07613](https://arxiv.org/abs/2402.07613)

    该论文研究了在可接受的对称约束条件下的全局最优性问题，提出了一种满足对称性质的函数或度量，并通过引入轨道凸体和coycle等工具解决了这一问题。具体应用包括不变核均值嵌入和基于对称约束的运输方案最优性。这些结果与不变性检验的Hunt-Stein定理相关。

    

    我们研究是否存在一种满足可接受变换群指定的对称性质的函数或度量，即同时满足以下两个条件：（1）最小化给定的凸性泛函或风险，（2）满足可容忍对称约束。这种对称性质的例子包括不变性、可变性或准不变性。我们的结果依赖于Stein和Le Cam的老思想，以及在可接受群的遍历定理中出现的近似群平均值。在凸分析中，一类称为轨道凸体的凸集显得至关重要，我们在非参数设置中确定了这类轨道凸体的性质。我们还展示了一个称为coycle的简单装置如何将不同形式的对称性转化为一个问题。作为应用，我们得出了关于不变核均值嵌入和在对称约束下运输方案最优性的Monge-Kantorovich定理的结果。我们还解释了与不变性检验的Hunt-Stein定理的联系。

    We ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations. Examples of such symmetry properties are invariance, equivariance, or quasi-invariance. Our results draw on old ideas of Stein and Le Cam and on approximate group averages that appear in ergodic theorems for amenable groups. A class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings. We also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem. As applications, we obtain results on invariant kernel mean embeddings and a Monge-Kantorovich theorem on optimality of transport plans under symmetry constraints. We also explain connections to the Hunt-Stein theorem on invariant tests.
    
[^57]: 基于生成模型的近最小极大分布式强化学习算法

    Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model

    [https://arxiv.org/abs/2402.07598](https://arxiv.org/abs/2402.07598)

    本论文提出了一种基于生成模型的近最小极大分布式强化学习算法，该算法在使用生成模型近似回报分布方面具有极小极大优势，解决了一个开放问题，并提供了实验研究结果。

    

    我们提出了一种新的基于模型的分布式强化学习算法，并证明了在使用生成模型近似回报分布方面，它是近似最小极大的（在对数因子上），从而解决了Zhang等人（2023）的一个开放问题。我们的分析为分布式强化学习中的分类方法提供了新的理论结果，并引入了一种新的分布式Bellman方程，即随机分类累积分布函数Bellman方程，我们认为这个方程也具有独立的研究意义。我们还进行了实验研究，比较了几种基于模型的分布式强化学习算法，并得出了对实践者有意义的几个结论。

    We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners.
    
[^58]: 对比分析ImageNet预训练的深度学习模型和DINOv2在医学图像分类中的表现

    Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification

    [https://arxiv.org/abs/2402.07595](https://arxiv.org/abs/2402.07595)

    本文通过对比分析ImageNet预训练的深度学习模型和DINOv2在医学图像分类中的表现，发现在临床数据集中，DINOv2的性能不如ImageNet-b。

    

    医学图像分析常常面临着数据稀缺的挑战。迁移学习在解决这个问题同时节约计算资源方面非常有效。最近出现的基础模型，如使用视觉变换器架构的DINOv2，为该领域提供了新的机会并引起了极大的关注。然而，DINOv2在临床数据上的表现仍需验证。本文中，我们使用三种临床模态的脑部MRI数据执行了一个胶质瘤分级任务。我们在迁移学习的上下文中比较了基于ImageNet和DINOv2的各种预训练深度学习模型的表现。我们关注了冻结机制对性能的影响。我们还在其他三种类型的公共数据集上验证了我们的发现：胸部放射学、眼底放射学和皮肤镜。我们的研究结果表明，在我们的临床数据集中，DINOv2的性能不如ImageNet-b。

    Medical image analysis frequently encounters data scarcity challenges. Transfer learning has been effective in addressing this issue while conserving computational resources. The recent advent of foundational models like the DINOv2, which uses the vision transformer architecture, has opened new opportunities in the field and gathered significant interest. However, DINOv2's performance on clinical data still needs to be verified. In this paper, we performed a glioma grading task using three clinical modalities of brain MRI data. We compared the performance of various pre-trained deep learning models, including those based on ImageNet and DINOv2, in a transfer learning context. Our focus was on understanding the impact of the freezing mechanism on performance. We also validated our findings on three other types of public datasets: chest radiography, fundus radiography, and dermoscopy. Our findings indicate that in our clinical dataset, DINOv2's performance was not as strong as ImageNet-b
    
[^59]: 动态系统的基础推理模型

    Foundational Inference Models for Dynamical Systems

    [https://arxiv.org/abs/2402.07594](https://arxiv.org/abs/2402.07594)

    本研究提出了一种基于监督学习的框架，用于从噪声数据中零样本推理动态系统的普通微分方程（ODE）。通过生成大型ODE数据集，并利用神经网络将噪声观察和初始条件以及向量场进行映射，得到称为基础推理模型（FIM）的结果模型。这些模型可以复制、匹配和组合，用于构建任何维度的推理模型。

    

    普通微分方程（ODE）构成了作为自然和社会现象模型的动态系统的基础。然而，推断出最佳描述给定现象的一组噪声观察的ODE可能非常具有挑战性，现有的模型往往也非常专业化和复杂。在这项工作中，我们提出了一种新颖的监督式学习框架，用于从噪声数据中零样本推理ODE。我们首先通过对初始条件空间和定义它们的向量场空间的分布进行采样，生成大型一维ODE数据集。然后，我们学习将这些方程的解的噪声观察与其相应的初始条件和向量场之间的神经映射。我们将结果模型称为基础推理模型（FIM），它们可以（i）沿时间维复制和匹配以增加分辨率；（ii）复制和组合以构建任何维度的推理模型。

    Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any d
    
[^60]: 重新思考战略环境中学习的比例定律

    Rethinking Scaling Laws for Learning in Strategic Environments

    [https://arxiv.org/abs/2402.07588](https://arxiv.org/abs/2402.07588)

    本文重新思考了在战略环境中学习的比例定律，发现战略互动可以打破传统的观点，即模型越大或表达能力越强并不一定会随之提高性能。通过几个战略环境的例子，我们展示了这种现象的影响。

    

    越来越大的机器学习模型的部署反映出一个共识：模型越有表达能力，越拥有大量数据，就能改善性能。随着模型在各种真实场景中的部署，它们不可避免地面临着战略环境。本文考虑了模型与战略互动对比例定律的相互作用对性能的影响这个自然问题。我们发现战略互动可以打破传统的比例定律观点，即性能并不一定随着模型的扩大和/或表达能力的增强（即使有无限数据）而单调提高。我们通过战略回归、战略分类和多智能体强化学习的例子展示了这一现象的影响，这些例子展示了战略环境中的限制模型或策略类的表达能力即可。

    The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
    
[^61]: 揭示特定群体的分布式概念漂移: 联邦学习中的公平要求

    Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning

    [https://arxiv.org/abs/2402.07586](https://arxiv.org/abs/2402.07586)

    该研究在联邦学习中的分布式环境下，首次探索了在存在特定群体概念漂移的情况下实现公平性的挑战和解决方案。

    

    在机器学习领域的不断发展中，确保公平性已成为一个重要关注点，推动了开发旨在减少决策过程中歧视结果的算法。然而，在存在特定群体的概念漂移的情况下实现公平性仍然是一个未被探索的领域，我们的研究代表了在这方面的开拓性努力。特定群体的概念漂移是指一个群体随时间经历概念漂移，而另一个群体却没有，导致公平性下降，即使准确性保持相对稳定。在联邦学习的框架下，客户端共同训练模型，其分布式性质进一步放大了这些挑战，因为每个客户端可以独立经历特定群体的概念漂移，同时仍共享相同的基本概念，从而创造了一个复杂而动态的环境来维持公平性。我们研究的一个重要贡献之一是对群体特定的概念漂移进行形式化和内部化的过程。

    In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and intr
    
[^62]: 为实现绿色机器学习服务而确定的架构设计决策

    Identifying architectural design decisions for achieving green ML serving

    [https://arxiv.org/abs/2402.07585](https://arxiv.org/abs/2402.07585)

    本研究通过对机器学习服务架构设计决策的分析，揭示了实现绿色机器学习服务的关键环节，并探讨了其与质量特性的关联。

    

    对于大型机器学习模型的使用日益增长，关注点逐渐转向它们不断增加的计算需求。虽然人们已经开始关注训练阶段的能源消耗，但较少有研究考虑推断阶段。对于机器学习推断而言，将机器学习模型与用户访问的机器学习系统进行绑定，即机器学习服务，是实现机器学习应用效率的关键但鲜为人知的步骤。本文在机器学习架构设计决策和绿色人工智能方面进行文献研究，特别关注机器学习服务。目的是从学者和实践者的角度，分析机器学习服务架构设计决策，以了解并确定其与质量特性的关联，结合机器学习服务文献的背景。我们的研究结果(i)确定了机器学习服务架构设计决策以及它们对应的组件和相关技术栈，(ii)提供了机器学习服务领域质量特性的概述。

    The growing use of large machine learning models highlights concerns about their increasing computational demands. While the energy consumption of their training phase has received attention, fewer works have considered the inference phase. For ML inference, the binding of ML models to the ML system for user access, known as ML serving, is a critical yet understudied step for achieving efficiency in ML applications.   We examine the literature in ML architectural design decisions and Green AI, with a special focus on ML serving. The aim is to analyze ML serving architectural design decisions for the purpose of understanding and identifying them with respect to quality characteristics from the point of view of researchers and practitioners in the context of ML serving literature.   Our results (i) identify ML serving architectural design decisions along with their corresponding components and associated technological stack, and (ii) provide an overview of the quality characteristics stu
    
[^63]: 只有曲线形状有关：通过下一个曲线形状预测训练基础模型进行零样本多元时间序列预测

    Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction

    [https://arxiv.org/abs/2402.07570](https://arxiv.org/abs/2402.07570)

    通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。

    

    我们提出了General Time Transformer (GTT)，一种仅有编码器的基础模型，用于零样本多元时间序列预测。GTT在一个包含2亿个高质量时间序列样本的大型数据集上进行预训练，涵盖了不同领域。在我们提出的框架中，多元时间序列预测的任务被建模为一个逐通道的下一个曲线形状预测问题，其中每个时间序列样本表示为一系列非重叠的曲线形状，具有统一的数值大小。GTT在通道级别上通过预测过去曲线形状的窗口来预测下一个曲线形状。实验结果表明，GTT在未见时间序列数据集上展现出优秀的零样本多元预测能力，甚至超过了最先进的有监督基线模型。此外，我们还研究了GTT模型参数和训练数据集规模变化的影响，观察到在零样本多元预测的背景下，规模定律也成立。

    We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate
    
[^64]: Weisfeiler-Leman在边缘条件下的更高表达力的重要性

    Weisfeiler-Leman at the margin: When more expressivity matters

    [https://arxiv.org/abs/2402.07568](https://arxiv.org/abs/2402.07568)

    研究探讨了1-WL算法在图同构问题中的表达能力和泛化性能之间的关系，发现增强的表达能力对提高泛化性能并不总是有效。此外，通过引入子图信息和经典的边缘理论，探索了更高表达力与改进泛化性能的条件。梯度流也被证明可以促进模型学习更丰富的表达能力。

    

    Weisfeiler-Leman算法（1-WL）是一个被广泛研究的用于图同构问题的启发式算法。最近，该算法在理解传递消息的图神经网络（MPNNs）的表达能力以及作为图核函数方面发挥了重要作用。尽管取得了成功，但1-WL在区分非同构图方面面临挑战，从而导致了更具表达力的MPNN和核架构的发展。然而，增强的表达能力和改进的泛化性能之间的关系仍不清楚。在本文中，我们展示了当通过图同构来观察时，架构的表达能力在解释其泛化性能方面具有有限的洞察力。此外，我们着重在1-WL和MPNN中引入子图信息，并运用经典的边缘理论来研究架构的增强表达能力与改进的泛化性能之间的条件。此外，我们还展示了梯度流如何推动模型学习更丰富的表达能力。

    The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we show that gradient flow pushes the 
    
[^65]: 用于模拟内存计算的精度优化固定点近存储数字处理单元

    A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing

    [https://arxiv.org/abs/2402.07549](https://arxiv.org/abs/2402.07549)

    我们提出了一种精度优化的固定点近存储数字处理单元，用于高效的模拟内存计算。该处理单元在保持高效能量和面积效率以及低延迟的同时，通过支持标准的深度学习激活步骤实现了竞争性的准确性和更高的计算吞吐量。

    

    模拟内存计算是一种用于快速和高能效的深度学习推理的新兴技术。然而，需要一定量的数字后处理来处理与内存设备相关的电路不匹配和非理想特性。高效的近存储数字逻辑对于保持模拟内存计算的高面积/能量效率和低延迟至关重要。现有的系统采用有限的并行化能力和高延迟的浮点16位（FP16）算术。为了克服这些限制，我们提出了一种基于固定点算术的近存储数字处理单元（NMPU）。它在保持最小面积开销的同时，实现了竞争性精度和更高的计算吞吐量。此外，NMPU支持标准的深度学习激活步骤，如ReLU和批量归一化。我们使用14nm CMOS技术对NMPU设计进行物理实现，并提供了详细的性能、功耗和面积评估。

    Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference. However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices. Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency. To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic. It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead. Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization. We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments. We valid
    
[^66]: TransAxx：具有近似计算能力的高效Transformer模型

    TransAxx: Efficient Transformers with Approximate Computing

    [https://arxiv.org/abs/2402.07545](https://arxiv.org/abs/2402.07545)

    TransAxx是一个基于PyTorch库的框架，可以支持近似计算，并通过对Vision Transformer模型进行近似感知微调，来提高在低功耗设备上的计算效率。

    

    最近，基于Transformer架构引入的Vision Transformer (ViT)模型已经展现出很大的竞争力，并且往往成为卷积神经网络(CNNs)的一种流行替代方案。然而，这些模型高计算需求限制了它们在低功耗设备上的实际应用。当前最先进的方法采用近似乘法器来解决DNN加速器高计算需求的问题，但之前的研究并没有探索其在ViT模型上的应用。在这项工作中，我们提出了TransAxx，这是一个基于流行的PyTorch库的框架，它能够快速支持近似算术，以无缝地评估近似计算对于DNN (如ViT模型)的影响。使用TransAxx，我们分析了Transformer模型在ImageNet数据集上对近似乘法的敏感性，并进行了近似感知的微调以恢复准确性。此外，我们提出了一种生成近似加法的方法。

    Vision Transformer (ViT) models which were recently introduced by the transformer architecture have shown to be very competitive and often become a popular alternative to Convolutional Neural Networks (CNNs). However, the high computational requirements of these models limit their practical applicability especially on low-power devices. Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models. In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models. Using TransAxx we analyze the sensitivity of transformer models on the ImageNet dataset to approximate multiplications and perform approximate-aware finetuning to regain accuracy. Furthermore, we propose a methodology to generate approximate ac
    
[^67]: 给我看怎么做：解释在细调语言模型中的作用

    Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models

    [https://arxiv.org/abs/2402.07543](https://arxiv.org/abs/2402.07543)

    本研究证明了使用解释来改进语言模型性能的显著好处，尤其适用于较小的模型，解释的加入使模型能够解决之前无法解决的任务。

    

    我们的研究证明了使用解释来增强语言模型性能的显著好处。与提示方式不同，细调允许模型在训练阶段学习和更新参数。在本研究中，我们应用细调的方法，使用包含输出解释而非仅呈现答案的数据来对不同大小的语言模型进行训练。我们发现，即使是只有6000万参数的较小语言模型也能从这种方法中获益。有趣的是，我们的结果表明，详细的解释对较小的模型更有益处，而对于较大的模型来说，无论解释的长度如何，都可以获得几乎相同的优势。此外，我们还证明了解释的加入使模型能够解决之前无法解决的任务。最后，我们认为尽管存在挑战，但解释在细调语言模型中起到了重要作用。

    Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the chall
    
[^68]: 使用无损同态压缩加速分布式深度学习

    Accelerating Distributed Deep Learning using Lossless Homomorphic Compression

    [https://arxiv.org/abs/2402.07529](https://arxiv.org/abs/2402.07529)

    本论文介绍了一种使用无损同态压缩加速分布式深度学习的新方法，通过结合工作节点级别的压缩和网络内聚合，实现高效的训练过程，并保证了训练精度。实验证明了该方法在各种DNN模型上的优越性能。

    

    随着深度神经网络（DNN）的复杂性和规模增长，分布式训练过程中的通信开销也随之增加，成为挑战分布式训练系统可扩展性的重要瓶颈。现有的解决方案旨在通过工作节点级别的压缩和网络内聚合来缓解这个瓶颈，但由于它们无法有效平衡压缩效果和计算开销之间的权衡，从而阻碍了整体性能和可扩展性。在本文中，我们引入了一种新颖的压缩算法，将工作节点级别的压缩和网络内聚合有效地结合起来。我们的解决方案既是同态的，可以实现高效的网络内聚合，又是无损的，确保在训练精度上没有妥协。在压缩和计算效率上理论上是最优的，我们的方法在多种DNN模型（如NCF，LSTM，VGG19）上进行了实证验证。

    As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems. Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability. In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation. Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy. Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, LSTM, VGG19, 
    
[^69]: NeuralSentinel: 保障神经网络的可靠性和可信度

    NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness

    [https://arxiv.org/abs/2402.07506](https://arxiv.org/abs/2402.07506)

    NeuralSentinel是一个工具，能够验证人工智能模型的可靠性和可信度，并帮助非专业人员通过理解模型的决策来增强对这一新系统的信任度。

    

    人工智能系统的使用量呈指数增长，由于其能够减少待分析的数据量，减轻用户的工作量并保持高准确率。然而，将这一新元素引入系统中却成为了攻击点，可能会危及系统的可靠性。这个新的情景提出了关于人工智能模型的可靠性和可信度的重要挑战，以及关于其响应决策的不确定性，尤其是在应用于关键领域如医疗保健、化学和电力等。为了解决这些问题，本文介绍了NeuralSentinel（NS），一个能够验证人工智能模型可靠性和可信度的工具。该工具结合了攻击和防御策略以及可解释性概念，可以对人工智能模型进行压力测试，并帮助非专业人员通过理解模型的决策来增强对这一新系统的信任度。

    The usage of Artificial Intelligence (AI) systems has increased exponentially, thanks to their ability to reduce the amount of data to be analyzed, the user efforts and preserving a high rate of accuracy. However, introducing this new element in the loop has converted them into attacked points that can compromise the reliability of the systems. This new scenario has raised crucial challenges regarding the reliability and trustworthiness of the AI models, as well as about the uncertainties in their response decisions, becoming even more crucial when applied in critical domains such as healthcare, chemical, electrical plants, etc. To contain these issues, in this paper, we present NeuralSentinel (NS), a tool able to validate the reliability and trustworthiness of AI models. This tool combines attack and defence strategies and explainability concepts to stress an AI model and help non-expert staff increase their confidence in this new system by understanding the model decisions. NS provid
    
[^70]: ClusterTabNet: 一种用于表格检测和表格结构识别的监督聚类方法

    ClusterTabNet: Supervised clustering method for table detection and table structure recognition

    [https://arxiv.org/abs/2402.07502](https://arxiv.org/abs/2402.07502)

    ClusterTabNet是一种监督聚类方法，通过解释表格结构为单词之间的关系图，并利用深度学习模型预测其邻接矩阵，实现对表格的检测和结构识别。与其他方法相比，ClusterTabNet具有相似或更好的准确性，并且需要更小的模型。

    

    我们提出了一种新颖的基于深度学习的方法，用于对文档中的单词进行聚类，并应用于检测和识别OCR输出中的表格。我们将表格结构自下而上解释为一组单词对之间的关系图（属于同一行、列、标题以及同一表格），并使用转换器编码器模型来预测其邻接矩阵。我们在PubTables-1M数据集以及PubTabNet和FinTabNet数据集上展示了我们的方法的性能。与当前最先进的检测方法（如DETR和Faster R-CNN）相比，我们的方法在需要更小的模型的同时实现了类似或更好的准确性。

    We present a novel deep-learning-based method to cluster words in documents which we apply to detect and recognize tables given the OCR output. We interpret table structure bottom-up as a graph of relations between pairs of words (belonging to the same row, column, header, as well as to the same table) and use a transformer encoder model to predict its adjacency matrix. We demonstrate the performance of our method on the PubTables-1M dataset as well as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art detection methods such as DETR and Faster R-CNN, our method achieves similar or better accuracy, while requiring a significantly smaller model.
    
[^71]: 一列火车完成两个任务：使用监督对比学习的加密流量分类框架

    One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning

    [https://arxiv.org/abs/2402.07501](https://arxiv.org/abs/2402.07501)

    本论文提出了使用监督对比学习的加密流量分类框架，利用对比学习增强数据包级别和流级别表示，并通过图数据增强捕获字节级流量图的细粒度语义不变特征。同时，采用跨级多任务学习的方法，使得数据包级别任务学习到的表示能够被流级别任务利用。

    

    随着网络安全受到广泛关注，加密流量分类已成为当前的研究重点。然而，现有方法在进行流量分类时没有充分考虑数据样本之间的共同特征，导致性能不佳。此外，它们独立地训练数据包级别和流级别的分类任务，这是冗余的，因为数据包级别任务中学习到的数据包表示可以被流级别任务利用。因此，本文提出了一种有效的模型，命名为对比学习增强的时域融合编码器（CLE-TFE）。具体而言，我们利用监督对比学习增强数据包级别和流级别表示，并在字节级流量图上进行图数据增强，以通过对比学习捕获字节之间的细粒度语义不变特征。我们还提出了跨级多任务学习的方法。

    As network security receives widespread attention, encrypted traffic classification has become the current research focus. However, existing methods conduct traffic classification without sufficiently considering the common characteristics between data samples, leading to suboptimal performance. Moreover, they train the packet-level and flow-level classification tasks independently, which is redundant because the packet representations learned in the packet-level task can be exploited by the flow-level task. Therefore, in this paper, we propose an effective model named a Contrastive Learning Enhanced Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised contrastive learning to enhance the packet-level and flow-level representations and perform graph data augmentation on the byte-level traffic graph so that the fine-grained semantic-invariant characteristics between bytes can be captured through contrastive learning. We also propose cross-level multi-task learning, whi
    
[^72]: 加速平滑：一种可扩展的随机平滑方法

    Accelerated Smoothing: A Scalable Approach to Randomized Smoothing

    [https://arxiv.org/abs/2402.07498](https://arxiv.org/abs/2402.07498)

    本文提出了一种加速随机平滑的方法，通过训练替代了蒙特卡洛抽样，显著提高了计算效率，并在各种设置下展示了其在近似平滑分类器方面的精确性。

    

    随机平滑以其使用特定分布的平滑噪声确保平滑分类器的鲁棒性而成为一种有效的可证明的对抗性攻击防御方法。然而，这个过程中蒙特卡洛抽样的使用引入了一个计算密集型的因素，限制了在较大规模上实践随机平滑的可行性。为了解决这个局限，我们提出了一种新的方法，用训练替代了蒙特卡洛抽样。通过在各种设置下的广泛实验，我们展示了我们的方法在近似平滑分类器方面具有显著的精确性。此外，我们还证明了我们的方法显著加速了鲁棒半径认证过程，在计算时间上提供了近600倍的改进，克服了传统随机平滑中的计算瓶颈。

    Randomized smoothing has emerged as a potent certifiable defense against adversarial attacks by employing smoothing noises from specific distributions to ensure the robustness of a smoothed classifier. However, the utilization of Monte Carlo sampling in this process introduces a compute-intensive element, which constrains the practicality of randomized smoothing on a larger scale. To address this limitation, we propose a novel approach that replaces Monte Carlo sampling with the training of a surrogate neural network. Through extensive experimentation in various settings, we demonstrate the efficacy of our approach in approximating the smoothed classifier with remarkable precision. Furthermore, we demonstrate that our approach significantly accelerates the robust radius certification process, providing nearly $600$X improvement in computation time, overcoming the computational bottlenecks associated with traditional randomized smoothing.
    
[^73]: 通过可视化动态风险评估来理解深度学习对抗对抗样本的防御

    Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment

    [https://arxiv.org/abs/2402.07496](https://arxiv.org/abs/2402.07496)

    通过对深度学习模型的可视化分析，研究了对抗样本攻击的防御方法，以更加精确地了解防御模型的性能如何被修改。

    

    近年来，深度神经网络模型已经在不同领域取得了许多进展。然而，它们也开始在风险关键的任务中使用。对这些模型的误诊可能导致严重事故甚至死亡。这个问题引起了研究人员的兴趣，他们研究了可能对这些模型进行攻击的攻击，并发现了一长串漏洞，每个模型都应该进行防御。对抗性样本攻击是研究人员广为知晓的一种攻击，他们已经开发了几种防御方法来避免这种威胁。然而，这些防御方法像深度神经网络模型一样不透明，它们的工作方式仍然未知。这就是为什么通过可视化它们如何改变目标模型的行为来了解被防御模型的性能如何被修改是有趣的。在本工作中，选择了一些对抗性样本攻击的防御方法，以便可以对其行为进行可视化分析。

    In recent years, Deep Neural Network models have been developed in different fields, where they have brought many advances. However, they have also started to be used in tasks where risk is critical. A misdiagnosis of these models can lead to serious accidents or even death. This concern has led to an interest among researchers to study possible attacks on these models, discovering a long list of vulnerabilities, from which every model should be defended. The adversarial example attack is a widely known attack among researchers, who have developed several defenses to avoid such a threat. However, these defenses are as opaque as a deep neural network model, how they work is still unknown. This is why visualizing how they change the behavior of the target model is interesting in order to understand more precisely how the performance of the defended model is being modified. For this work, some defenses, against adversarial example attack, have been selected in order to visualize the behav
    
[^74]: 用于LIGO实时数据中信号检测的卷积神经网络

    Convolutional Neural Networks for signal detection in real LIGO data

    [https://arxiv.org/abs/2402.07492](https://arxiv.org/abs/2402.07492)

    本论文介绍了用于LIGO实时数据中信号检测的卷积神经网络，并描述了解决机器学习评估框架问题的机器学习引力波搜索挑战赛。团队TPI FSU Jena提交了基于机器学习方法的算法，并成功将其应用于真实的O3b数据，并恢复了GWTC-3目录中的相关事件。

    

    在搜索引力波探测器数据中寻找来自紧凑二进制合并的信号是一项计算需求严峻的任务。最近，机器学习算法被提出来应对当前和未来的挑战。然而，这些出版物的结果往往因为在评估过程中的不同选择而大相径庭。机器学习引力波搜索挑战赛旨在解决这些问题，并产生一个统一的机器学习搜索评估框架。六个团队提交了贡献，其中四个基于机器学习方法，两个是最先进的生产分析。本文介绍了来自TPI FSU Jena团队及其更新版本的提交。我们还将算法应用于真实的O3b数据，并恢复了GWTC-3目录中的相关事件。

    Searching the data of gravitational-wave detectors for signals from compact binary mergers is a computationally demanding task. Recently, machine learning algorithms have been proposed to address current and future challenges. However, the results of these publications often differ greatly due to differing choices in the evaluation procedure. The Machine Learning Gravitational-Wave Search Challenge was organized to resolve these issues and produce a unified framework for machine-learning search evaluation. Six teams submitted contributions, four of which are based on machine learning methods and two are state-of-the-art production analyses. This paper describes the submission from the team TPI FSU Jena and its updated variant. We also apply our algorithm to real O3b data and recover the relevant events of the GWTC-3 catalog.
    
[^75]: 基于分数的扩散模型：随机微分方程的技术教程

    Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial

    [https://arxiv.org/abs/2402.07487](https://arxiv.org/abs/2402.07487)

    本文是基于分数的扩散模型的技术教程，重点讲解了通过随机微分方程进行公式化的方法，包括采样和分数匹配。适合初学者了解该领域，并且从业人员在设计新模型或算法时也可能会有所帮助。

    

    本文是关于基于分数的扩散模型的阐释性文章，重点介绍了通过随机微分方程(SDE)进行公式化的方法。在温和的介绍之后，我们讨论了扩散建模的两个关键点--采样和分数匹配，其中包括SDE/ODE采样，分数匹配效率，一致性模型和强化学习。我们提供了简短的证明来说明所述结果的主要思想。本文主要是为了向初学者介绍这个领域，同时从业人员在设计新模型或算法时也可能会发现一些分析有用。

    This is an expository article on the score-based diffusion models, with a particular focus on the formulation via stochastic differential equations (SDE). After a gentle introduction, we discuss the two pillars in the diffusion modeling -- sampling and score matching, which encompass the SDE/ODE sampling, score matching efficiency, the consistency model, and reinforcement learning. Short proofs are given to illustrate the main idea of the stated results. The article is primarily for introducing the beginners to the field, and practitioners may also find some analysis useful in designing new models or algorithms.
    
[^76]: 基于人工神经网络行为可解释性的逃避攻击的拓扑保护

    Topological Safeguard for Evasion Attack based on the Interpretability of Artificial Neural Network Behavior

    [https://arxiv.org/abs/2402.07480](https://arxiv.org/abs/2402.07480)

    本文提出了一种基于人工神经网络行为解释性的拓扑保护方法，用于对抗逃避攻击。在过去几年中，深度学习技术的应用日益广泛，但也带来了新的网络安全威胁。逃避攻击是其中一种常见攻击，本文旨在设计一种能够对抗该攻击的有效防御方法。

    

    在过去几年中，深度学习技术已经在不同领域提出，为每个领域带来了许多进展，但这也带来了关于网络安全方面的新威胁。这些实施的模型带来了与深度学习技术相关的几种漏洞。此外，它们允许利用这些模型，获得私人信息，甚至修改模型的决策。因此，对这些漏洞/攻击进行研究并设计防御措施以避免或对抗它们的兴趣在研究中日益突出。特别是，著名的逃避攻击正在被研究人员分析，因此在文献中可以找到几种避免此威胁的防御措施。自L-BFG算法提出以来，这种威胁一直关注研究界。然而，由于没有适用于所有已知逃避算法的完美防御，因此仍在不断开发新的巧妙对策。

    In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but identifying new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model's decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a no
    
[^77]: 机器学习原子团簇展开在物质科学和化学中的应用

    Cartesian atomic cluster expansion for machine learning interatomic potentials

    [https://arxiv.org/abs/2402.07472](https://arxiv.org/abs/2402.07472)

    本论文提出了一种改进的机器学习原子间势模型，使用基于笛卡尔坐标的原子密度展开来替代传统的原子团簇展开方法，并结合低维嵌入和原子间消息传递。该模型在不同系统中表现出良好的准确性、稳定性和普适性。

    

    机器学习原子间势正在革新材料科学和化学中的大规模、准确的原子模拟。这些势函数通常使用原子团簇展开或变换消息传递与球谐基函数。然而，为了保持旋转对称性，依赖Clebsch-Gordan系数会导致计算效率低下和冗余。我们提出一种替代方法：基于笛卡尔坐标的原子密度展开。该方法在保持相互作用体系的同时提供了对原子环境的完整描述。此外，我们还整合了各种化学元素的低维嵌入和原子间消息传递。所得到的势函数被命名为Cartesian Atomic Cluster Expansion(CACE)，具有良好的准确性、稳定性和普适性。我们在不同系统中进行验证，包括大规模水、小分子和25种元素高熵合金。

    Machine learning interatomic potentials are revolutionizing large-scale, accurate atomistic modelling in material science and chemistry. These potentials often use atomic cluster expansion or equivariant message passing with spherical harmonics as basis functions. However, the dependence on Clebsch-Gordan coefficients for maintaining rotational symmetry leads to computational inefficiencies and redundancies. We propose an alternative: a Cartesian-coordinates-based atomic density expansion. This approach provides a complete description of atomic environments while maintaining interaction body orders. Additionally, we integrate low-dimensional embeddings of various chemical elements and inter-atomic message passing. The resulting potential, named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy, stability, and generalizability. We validate its performance in diverse systems, including bulk water, small molecules, and 25-element high-entropy alloys.
    
[^78]: 具有随机游走的差分隐私去中心化学习

    Differentially Private Decentralized Learning with Random Walks

    [https://arxiv.org/abs/2402.07471](https://arxiv.org/abs/2402.07471)

    这项工作研究了具有随机游走算法的差分隐私去中心化学习，并使用最近的差分隐私变种推导了节点之间的隐私损失。实验结果表明，与节点之间的八卦算法相比，随机游走算法更能提供较好的隐私保证。

    

    深圳的智能手机取而代之的是传统移动电话，这是现代科技发展的必然结果。

    The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph. Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empir
    
[^79]: 基于得分的物理信息神经网络用于高维福克-普朗克方程

    Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations

    [https://arxiv.org/abs/2402.07465](https://arxiv.org/abs/2402.07465)

    这项研究提出了一种基于得分函数的求解器来解决高维福克-普朗克方程中的维数灾难问题。与蒙特卡洛和普通PINN相比，该方法能够更准确地处理与布朗运动相关的概率密度函数，并提供快速采样。

    

    福克-普朗克（FP）方程是随机过程中的基础偏微分方程（PDE）。然而，当处理高维FP PDE时，维数灾难（CoD）会带来挑战。尽管蒙特卡洛和普通物理信息神经网络（PINN）已经显示出应对CoD的潜力，但在处理与布朗运动相关的概率密度函数（PDF）时，两种方法都在高维度上显示出数值误差。点值PDF随着维度增加呈指数级下降，超过了数值模拟的精度，导致了相当大的误差。此外，由于其大规模采样，蒙特卡洛无法提供快速采样。通过对普通PINNs模拟对数似然（LL），将FP方程转化为一个困难的HJB方程，其误差随维数增长迅速。为此，我们提出了一种新方法，利用基于得分的求解器来拟合SDE中的得分函数。得分函数定义为概率密度函数的梯度。

    The Fokker-Planck (FP) equation is a foundational PDE in stochastic processes. However, curse of dimensionality (CoD) poses challenge when dealing with high-dimensional FP PDEs. Although Monte Carlo and vanilla Physics-Informed Neural Networks (PINNs) have shown the potential to tackle CoD, both methods exhibit numerical errors in high dimensions when dealing with the probability density function (PDF) associated with Brownian motion. The point-wise PDF values tend to decrease exponentially as dimension increases, surpassing the precision of numerical simulations and resulting in substantial errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms the FP equation into a difficult HJB equation, whose error grows rapidly with dimension. To this end, we propose a novel approach utilizing a score-based solver to fit the score function in SDEs. The score function, defined as the gradient of t
    
[^80]: 价值装载问题的激素适应方法：预防回形针启示录？

    A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?

    [https://arxiv.org/abs/2402.07462](https://arxiv.org/abs/2402.07462)

    我们提出了HALO法规模式，使用激素分析来调节人工智能的行为模式，以解决价值装载问题中的“回形针最大化器”场景。

    

    价值装载问题对于研究人员来说是一个重要的挑战，他们旨在创建与人类价值观和偏好相一致的人工智能系统。该问题需要一种方法来定义和规范人工智能行为的安全和最优限制。在这项工作中，我们提出了HALO（激素适应通过对手过程）这个法规模式，它使用激素分析来调节人工智能的行为模式。行为激素适应是一种现象，低频率的行为具有益处，而高频率的行为则有害。通过将行为建模为变态对手过程，我们可以使用行为频率响应分析（BFRA）或行为计数响应分析（BCRA）来量化可重复行为的激素限制。我们展示了如何使用HALO来解决“回形针最大化器”场景，这是一个思想实验，其中一个未受管制的人工智能任务是将所有物质转化为回形针。

    The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. 
    
[^81]: 关于顺序预测中的标定距离研究

    On the Distance from Calibration in Sequential Prediction

    [https://arxiv.org/abs/2402.07458](https://arxiv.org/abs/2402.07458)

    本论文研究了顺序预测中的标定距离，证明了存在一种预测算法可以在敌人选择的二进制序列上实现$O(\sqrt{T})$的标定距离，通过较低的标定距离进行准确近似。

    

    我们研究了一种顺序二进制预测场景，在这种场景中，预测器的评估是以标定距离为基准的，标定距离定义为预测值与事后完全标定的预测集之间的$L_1$距离。这类似于最近由B{\l}asiok、Gopalan、Hu和Nakkiran（STOC 2023）提出的离线场景中的标定度量。标定距离是一种自然且直观的偏离完美标定的度量，并且满足不同于许多常见的标定度量（如$L_1$标定误差及其变种）的Lipschitz连续性属性。我们证明了存在一种预测算法，可以在对敌人选择的长度为$T$的二进制序列上，以期望$O(\sqrt{T})$的标定距离实现。在这个上界的核心是一个结构性结果，证明了标定距离可以通过较低的标定距离进行准确近似。

    We study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $L_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight. This is analogous to a calibration measure recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the offline setting. The calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a Lipschitz continuity property which does not hold for many popular calibration measures, such as the $L_1$ calibration error and its variants.   We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$ calibration distance in expectation on an adversarially chosen sequence of $T$ binary outcomes. At the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a con
    
[^82]: Bandit-Feedback在线多类分类：变体和权衡

    Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs

    [https://arxiv.org/abs/2402.07453](https://arxiv.org/abs/2402.07453)

    该论文研究了在对抗在线环境中多类分类中依赖于强盗反馈的代价，自适应对手和随机学习者与无视对手和确定性学习者之间的损失差距。

    

    在对抗在线环境中考虑多类分类领域。与提供完全信息相比，依赖于强盗反馈的代价是多少？自适应对手与无视对手相比，可以增加损失的程度有多大？随机学习者与确定性学习者相比，可以降低损失的程度有多大？我们在错误边界模型中研究了这些问题，并提供了几乎紧确的答案。

    Consider the domain of multiclass classification within the adversarial online setting. What is the price of relying on bandit feedback as opposed to full information? To what extent can an adaptive adversary amplify the loss compared to an oblivious one? To what extent can a randomized learner reduce the loss compared to a deterministic one? We study these questions in the mistake bound model and provide nearly tight answers.   We demonstrate that the optimal mistake bound under bandit feedback is at most $O(k)$ times higher than the optimal mistake bound in the full information case, where $k$ represents the number of labels. This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal ['13] and by Long ['17, '20], who focused on deterministic learners.   Moreover, we present nearly optimal bounds of $\tilde{\Theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversarie
    
[^83]: TriAug：用于超声乳腺病变不平衡分类的异常样本检测

    TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound

    [https://arxiv.org/abs/2402.07452](https://arxiv.org/abs/2402.07452)

    TriAug是一个用于乳腺超声图像的异常样本检测框架，通过使用三元状态增强和平衡的球形损失来提高示踪分类的准确性和异常样本检测性能。

    

    不同的疾病，如乳腺病变的组织亚型，具有严重不同的发病率。即使通过大量的示踪数据进行训练，模型在临床实际中通常遇到属于未见类别的异常样本。为了解决这个问题，我们提出了一种新的框架，基于乳腺超声图像的长尾异常样本检测任务，并配备了一种三元状态增强（TriAug），它可以提高示踪分类的准确性，同时保持良好的异常样本检测性能。与此同时，我们设计了一个平衡的球形损失来处理类不平衡的问题。

    Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.
    
[^84]: AraSpider：实现阿拉伯语到SQL的民主化

    AraSpider: Democratizing Arabic-to-SQL

    [https://arxiv.org/abs/2402.07448](https://arxiv.org/abs/2402.07448)

    AraSpider是首个阿拉伯语版本的Spider数据集，研究表明使用回译策略可以显著提高ChatGPT 3.5和SQLCoder模型在阿拉伯语NLP任务中的性能。

    

    本研究介绍了AraSpider，这是首个阿拉伯语版本的Spider数据集，旨在提升阿拉伯语社区中的自然语言处理（NLP）。该研究测试了四个多语言翻译模型在将英文翻译成阿拉伯语方面的有效性。另外，还评估了两个模型在从阿拉伯文本生成SQL查询方面的能力。结果表明，使用回译显著提高了ChatGPT 3.5和SQLCoder模型的表现，这两个模型在Spider数据集上被认为是最佳表现者。值得注意的是，ChatGPT 3.5展示了高质量的翻译，而SQLCoder在文本到SQL任务中表现出色。该研究强调了将上下文模式和采用回译策略纳入阿拉伯语NLP任务中以提高模型性能的重要性。此外，提供了详细的方法论以实现结果复现并将数据集翻译成其他语言，突显了研究促进的承诺。

    This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting 
    
[^85]: 具有单调对手的Top-K排名问题

    Top-$K$ ranking with a monotone adversary

    [https://arxiv.org/abs/2402.07445](https://arxiv.org/abs/2402.07445)

    本文针对具有单调对手的Top-K排名问题，提出了一种加权最大似然估计器(MLE)，在样本复杂度方面接近最优。算法创新包括了对加权MLE的精确且紧密的$\ell_\infty$误差分析，并与加权比较图的谱特性相关联。

    

    本文解决了具有单调对手的Top-K排名问题。我们考虑了一个比较图被随机生成且对手可以添加任意边的情况。统计学家的目标是根据从这个半随机比较图导出的两两比较准确地识别出Top-K的首选项。本文的主要贡献是开发出一种加权最大似然估计器(MLE)，它在样本复杂度方面达到了近似最优，最多差一个$log^2(n)$的因子，其中n表示比较项的数量。这得益于分析和算法创新的结合。在分析方面，我们提供了一种更明确、更紧密的加权MLE的$\ell_\infty$误差分析，它与加权比较图的谱特性相关。受此启发，我们的算法创新涉及到了

    In this paper, we address the top-$K$ ranking problem with a monotone adversary. We consider the scenario where a comparison graph is randomly generated and the adversary is allowed to add arbitrary edges. The statistician's goal is then to accurately identify the top-$K$ preferred items based on pairwise comparisons derived from this semi-random comparison graph. The main contribution of this paper is to develop a weighted maximum likelihood estimator (MLE) that achieves near-optimal sample complexity, up to a $\log^2(n)$ factor, where n denotes the number of items under comparison. This is made possible through a combination of analytical and algorithmic innovations. On the analytical front, we provide a refined $\ell_\infty$ error analysis of the weighted MLE that is more explicit and tighter than existing analyses. It relates the $\ell_\infty$ error with the spectral properties of the weighted comparison graph. Motivated by this, our algorithmic innovation involves the development 
    
[^86]: 注意力机制的I/O复杂性，或者闪存注意力有多么优化？

    The I/O Complexity of Attention, or How Optimal is Flash Attention?

    [https://arxiv.org/abs/2402.07443](https://arxiv.org/abs/2402.07443)

    这项研究解决了FlashAttention算法的I/O复杂性是否是最优的问题。

    

    自注意力是流行的Transformer架构的核心，但是受到二次时间和空间复杂性的限制。突破性的FlashAttention算法揭示了I/O复杂性是扩展Transformer的真正瓶颈。给定两个层级的内存层次结构，一个快速缓存（例如GPU片上静态随机存储器）和一个慢速内存（例如GPU高带宽内存），I/O复杂性衡量了对内存的访问次数。FlashAttention使用$\frac{N^2d^2}{M}$的I/O操作来计算注意力，其中$N$是注意力矩阵的维度，$d$是头部维度，$M$是缓存大小。然而，这种I/O复杂性是否是最优的？已知的下界只排除了$M=\Theta(Nd)$时的$o(Nd)$的I/O复杂性，因为需要写入慢速内存的输出是$\Omega(Nd)$。这引出了我们工作的主要问题：对于所有的$M$值，FlashAttention是否是I/O最优的？我们通过展示一个I/O复杂性的下界来解决上述问题。

    Self-attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. The breakthrough FlashAttention algorithm revealed I/O complexity as the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory. FlashAttention computes attention using $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ the head-dimension and $M$ the cache size. However, is this I/O complexity optimal? The known lower bound only rules out an I/O complexity of $o(Nd)$ when $M=\Theta(Nd)$, since the output that needs to be written to slow memory is $\Omega(Nd)$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of $M$?   We resolve the above question in its full generality by showing an I/O complexity lower bound t
    
[^87]: 使用LoCo和M2-BERT进行基准测试和构建长上下文检索模型

    Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT

    [https://arxiv.org/abs/2402.07440](https://arxiv.org/abs/2402.07440)

    该论文介绍了LoCoV1，一个用于评估长上下文检索性能的新型基准测试，并提出了M2-BERT检索编码器，用于处理长上下文检索，解决了如何评估性能、预训练语言模型以及如何进行微调的挑战。

    

    检索管道是许多机器学习系统中的重要组成部分，在文档很长（例如10K个标记或更多）且需要在整个文本中合成信息来确定相关文档的领域中表现不佳。开发适用于这些领域的长上下文检索编码器面临三个挑战：（1）如何评估长上下文检索性能，（2）如何预训练基本语言模型以表示短上下文（对应查询）和长上下文（对应文档），以及（3）如何根据GPU内存限制下的批量大小限制对该模型进行微调。为了解决这些挑战，我们首先介绍了LoCoV1，这是一个新颖的12个任务基准测试，用于测量在不可分块或不有效的情况下的长上下文检索。接下来，我们提出了M2-BERT检索编码器，这是一个80M参数状态空间编码器模型，采用Monarch Mixer架构构建，能够进行可扩展的检索。

    Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scali
    
[^88]: 非原子拥堵博弈中学习最优税收设计

    Learning Optimal Tax Design in Nonatomic Congestion Games

    [https://arxiv.org/abs/2402.07437](https://arxiv.org/abs/2402.07437)

    本研究致力于学习如何设计最优税收，以在非原子拥堵博弈中提高效率。为了解决指数级的税收函数空间、梯度不存在和目标函数的非凸性等挑战，该算法利用了分段线性税收、额外的线性项和有效的子例程的新颖组成部分。

    

    本研究探讨了如何学习最优税收设计，以在非原子拥堵博弈中最大化效率。众所周知，玩家之间的自利行为可能会破坏系统的效率。税务机制是缓解此问题并引导社会最优行为的常见方法。在这项工作中，我们首次采取了学习最优税收的初始步骤，该最优税收可以通过平衡反馈来最小化社会成本，即税务设计者只能观察到强制税收下的均衡状态。由于指数级的税收函数空间，梯度不存在和目标函数的非凸性，现有算法不适用。为了解决这些挑战，我们的算法利用了几个新颖的组成部分：（1）分段线性税收来近似最优税收；（2）额外的线性项来保证强凸潜力函数；（3）有效的子例程来找到“边界”税收。该算法可以找到一个$\epsilon$-最优税收，时间复杂度为$O(\bet

    We study how to learn the optimal tax design to maximize the efficiency in nonatomic congestion games. It is known that self-interested behavior among the players can damage the system's efficiency. Tax mechanisms is a common method to alleviate this issue and induce socially optimal behavior. In this work, we take the initial step for learning the optimal tax that can minimize the social cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe the equilibrium state under the enforced tax. Existing algorithms are not applicable due to the exponentially large tax function space, nonexistence of the gradient, and nonconvexity of the objective. To tackle these challenges, our algorithm leverages several novel components: (1) piece-wise linear tax to approximate the optimal tax; (2) an extra linear term to guarantee a strongly convex potential function; (3) efficient subroutine to find the ``boundary'' tax. The algorithm can find an $\epsilon$-optimal tax with $O(\bet
    
[^89]: 分析货币波动：GBP/USD和EUR/GBP货币对的GARCH、EWMA和IV模型的比较研究

    Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and IV Models for GBP/USD and EUR/GBP Pairs

    [https://arxiv.org/abs/2402.07435](https://arxiv.org/abs/2402.07435)

    本研究比较了GARCH、EWMA和IV模型在预测GBP/USD和EUR/GBP货币对每日回报的20天变动方面的效果。研究发现EUR/GBP货币对存在非对称回报的证据，而GBP/USD货币对的证据并不一致。

    

    本研究探讨了英镑（GBP）价值的波动情况，特别关注其与美元（USD）和欧元（EUR）货币对的关系。利用2018年6月15日至2023年6月15日的数据，我们应用不同的数学模型来评估它们在预测货币对每日回报的20天变动方面的效果。我们的分析涉及指数加权移动平均（EWMA）、广义自回归条件异方差（GARCH）模型和隐含波动率（IV）模型的实施。为了评估它们的性能，我们使用均方根误差（RMSE）和平均绝对误差（MAE）指标比较它们的预测准确性。我们深入研究了GARCH模型的复杂性，检查了将其应用于所提供数据集时的统计特性。我们的研究发现，EUR/GBP货币对存在非对称回报的证据，而GBP/USD货币对的证据并不一致。

    In this study, we examine the fluctuation in the value of the Great Britain Pound (GBP). We focus particularly on its relationship with the United States Dollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15, 2018, to June 15, 2023, we apply various mathematical models to assess their effectiveness in predicting the 20-day variation in the pairs' daily returns. Our analysis involves the implementation of Exponentially Weighted Moving Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models, and Implied Volatility (IV) models. To evaluate their performance, we compare the accuracy of their predictions using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the intricacies of GARCH models, examining their statistical characteristics when applied to the provided dataset. Our findings suggest the existence of asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for the GBP/USD pair
    
[^90]: 条件生成模型足以从任何因果效应测度中采样

    Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand

    [https://arxiv.org/abs/2402.07419](https://arxiv.org/abs/2402.07419)

    本文展示了通过条件生成模型的推进计算可以计算任何可辨识的因果效应，并提出了基于扩散的方法用于从图像的任何（条件）干预分布中进行采样。

    

    最近，从观测数据进行因果推断在机器学习中得到了广泛应用。虽然存在计算因果效应的可靠且完备的算法，但其中许多算法需要显式访问观测分布上的条件似然，而在高维场景中（例如图像），估计这些似然是困难的。为了解决这个问题，研究人员通过使用神经模型模拟因果关系，并取得了令人印象深刻的结果。然而，这些现有方法中没有一个可以应用于通用场景，例如具有潜在混淆因素的图像数据的因果图，或者获得条件干预样本。在本文中，我们展示了在任意因果图下，通过条件生成模型的推进计算可以计算任何可辨识的因果效应。基于此结果，我们设计了一个基于扩散的方法，可以从任何（条件）干预分布中采样图像。

    Causal inference from observational data has recently found many applications in machine learning. While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. However, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models. Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on ima
    
[^91]: 一项关于校准视觉-语言模型的实证研究

    An Empirical Study Into What Matters for Calibrating Vision-Language Models

    [https://arxiv.org/abs/2402.07417](https://arxiv.org/abs/2402.07417)

    本研究通过实证研究探索了视觉-语言模型(VLMs)的校准性质，并发现温度缩放可以显著提升校准性能，而且VLMs只需少量示例即可进行校准。

    

    视觉-语言模型（VLMs）已成为零样本识别的主要方法，在处理多样化场景和重要分布变化方面表现出色。然而，将它们应用于风险敏感领域需要对其不确定性估计能力有更深入的了解，这是一个相对未知的领域。在这项研究中，我们探讨了不同架构、数据集和训练策略下VLMs的校准性质。特别是，当在一个领域、标签集或层次级别中进行校准时，我们分析了VLMs在不同领域中的不确定性估计性能。我们的发现表明，虽然VLMs本身并不具备校准不确定性的能力，但温度缩放可以显著且一致地提升校准性能，即使在分布转变和标签集变化的情况下也是如此。此外，VLMs可以通过很少的示例进行校准。通过详细的实验，我们突显了我们的研究的潜在应用和重要性。

    Vision--Language Models (VLMs) have emerged as the dominant approach for zero-shot recognition, adept at handling diverse scenarios and significant distribution changes. However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area. In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies. In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one. Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set. Moreover, VLMs can be calibrated with a very small set of examples. Through detailed experimentation, we highlight the potential applications and importance of our
    
[^92]: 面向异构计算系统的上下文感知多模型目标检测

    Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems

    [https://arxiv.org/abs/2402.07415](https://arxiv.org/abs/2402.07415)

    本文研究了面向异构计算系统的上下文感知多模型目标检测，通过利用嵌入在输入数据流中的上下文信息实现更高效的基于多模型的目标检测过程，并考虑多加速器执行以优化能量效率。

    

    近年来，深度神经网络（DNN）在连续移动目标检测（OD）任务中得到了广泛的应用，特别是在自主系统中。然而，它们部署中的一个普遍问题是采用的单一DNN的通用方法，这导致了计算资源的低效利用。这种低效特别在能量受限的系统中会造成整体系统效率的降低。我们认为，嵌入在输入数据流中的上下文信息（例如摄像头数据流中的帧）可以被利用来实现更高效的基于多模型的OD过程。在本文中，我们提出了SHIFT，它根据动态变化的上下文信息和计算约束不断从各种基于DNN的OD模型中选择。在选择过程中，SHIFT独特地考虑多加速器执行，以更好地优化能量效率同时满足要求。

    In recent years, deep neural networks (DNNs) have gained widespread adoption for continuous mobile object detection (OD) tasks, particularly in autonomous systems. However, a prevalent issue in their deployment is the one-size-fits-all approach, where a single DNN is used, resulting in inefficient utilization of computational resources. This inefficiency is particularly detrimental in energy-constrained systems, as it degrades overall system efficiency. We identify that, the contextual information embedded in the input data stream (e.g. the frames in the camera feed that the OD models are run on) could be exploited to allow a more efficient multi-model-based OD process. In this paper, we propose SHIFT which continuously selects from a variety of DNN-based OD models depending on the dynamically changing contextual information and computational constraints. During this selection, SHIFT uniquely considers multi-accelerator execution to better optimize the energy-efficiency while satisfyin
    
[^93]: 使用状态转换距离表示学习的辅助奖励生成

    Auxiliary Reward Generation with Transition Distance Representation Learning

    [https://arxiv.org/abs/2402.07412](https://arxiv.org/abs/2402.07412)

    这篇论文提出了一种使用状态转换距离表示学习的辅助奖励生成方法，可以在强化学习中自动生成奖励，提高学习效率和减少人工设计奖励的工作量。

    

    强化学习在复杂的顺序决策问题中展现出了其优势。在强化学习中，奖励函数对学习性能至关重要，因为它作为任务完成程度的衡量标准。在实际应用中，奖励往往是由人工设计的，需要费时费力的调整，并且容易受到人类认知偏差的影响。为了实现自动的辅助奖励生成，我们提出了一种新颖的表示学习方法，可以衡量状态之间的“转换距离”。在这些表示的基础上，我们引入了一种无需人类知识的辅助奖励生成技术，用于单任务和技能链场景。提出的方法在广泛的操作任务中进行了评估。实验结果表明，测量状态之间的转换距离以及辅助奖励引起的改进有效提高了学习效率。

    Reinforcement learning (RL) has shown its strength in challenging sequential decision-making problems. The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree. In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases. To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance'' between states. Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge. The proposed approach is evaluated in a wide range of manipulation tasks. The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency
    
[^94]: 潜势引导的奖励塑造用于内在动机

    Potential-Based Reward Shaping For Intrinsic Motivation

    [https://arxiv.org/abs/2402.07411](https://arxiv.org/abs/2402.07411)

    该论文提出了潜势引导的奖励塑造方法用于处理内在动机，在复杂和稀疏奖励环境下能避免改变最优策略集导致次优行为的问题。

    

    最近，在复杂且稀疏奖励环境下，内在动机（IM）奖励塑造方法的数量激增。这些方法往往会无意中改变环境中的最优策略集，导致次优行为产生。以往关于减轻奖励塑造风险的研究，特别是潜势引导的奖励塑造（PBRS），并不能适用于许多IM方法，因为它们通常是复杂的可训练函数，因此依赖于比PBRS开发时更广泛的变量集。我们提出了对PBRS的扩展，证明了在更一般的函数集下保留了最优策略集。我们还提出了一种称为潜势引导的内在动机（PBIM）的方法，可以将IM奖励转换为可用的潜势形式，而不会改变最优策略集。在MiniGrid DoorKey和Cliff Walk中进行了测试。

    Recently there has been a proliferation of intrinsic motivation (IM) reward-shaping methods to learn in complex and sparse-reward environments. These methods can often inadvertently change the set of optimal policies in an environment, leading to suboptimal behavior. Previous work on mitigating the risks of reward shaping, particularly through potential-based reward shaping (PBRS), has not been applicable to many IM methods, as they are often complex, trainable functions themselves, and therefore dependent on a wider set of variables than the traditional reward functions that PBRS was developed for. We present an extension to PBRS that we prove preserves the set of optimal policies under a more general set of functions than has been previously proven. We also present {\em Potential-Based Intrinsic Motivation} (PBIM), a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff Walk
    
[^95]: 《深入解析对比语言-图像预训练(CLIP)模型的健壮性》

    A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)

    [https://arxiv.org/abs/2402.07410](https://arxiv.org/abs/2402.07410)

    本研究深入分析了对比语言-图像预训练(CLIP)模型的健壮性，重点关注了其在特定视觉因素变化、不确定性估计和异常输入检测等安全目标上的表现。

    

    对比语言-图像预训练(CLIP)模型在多个具有挑战性的分布转移中展现出了卓越的泛化能力。然而，关于其在特定视觉因素变化下的鲁棒性仍有待进一步探索。在现实世界的应用中，可靠且安全的系统必须考虑除分类准确性之外的其他安全目标，如预测的不确定性。然而，CLIP模型在这些与安全相关的特征上的有效性较少被探讨。出于上述原因，本研究全面调查了CLIP模型的安全目标，特别关注三个关键属性：对视觉因素变化的弹性，校准的不确定性估计，以及检测异常输入的能力。为此，我们研究了83个CLIP模型和127个ImageNet分类器。它们在架构、（预）训练分布和训练策略上都具有多样性。我们考虑了10个视觉因素（例如形状和图案），5个...

    Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 
    
[^96]: 可容许预测规划用于机遇受限优化

    Conformal Predictive Programming for Chance Constrained Optimization

    [https://arxiv.org/abs/2402.07407](https://arxiv.org/abs/2402.07407)

    可容许预测规划（CPP）是一种解决受任意随机参数影响的优化问题的方法，通过利用样本和量子引理将机遇受限优化（CCO）问题转化为确定性优化问题，并具备边际概率可行性保证。

    

    在对预测规划（CP）的进展的激励下，我们提出了可容许预测规划（CPP），一种解决机遇受限优化（CCO）问题的方法，即受任意随机参数影响的非线性约束函数的优化问题。CPP利用这些随机参数的样本以及量子引理（CP的核心）将CCO问题转化为确定性优化问题。然后，我们通过：（1）将量子表示为线性规划以及其KKT条件（CPP-KKT）；（2）使用混合整数规划（CPP-MIP）来呈现CPP的两种易于处理的改进。CPP具备对CCO问题进行边际概率可行性保证，这与现有方法（例如样本逼近和场景方法）在概念上有所不同。尽管我们探讨了与样本逼近方法的算法相似之处，但我们强调CPP的优势在于易于扩展。

    Motivated by the advances in conformal prediction (CP), we propose conformal predictive programming (CPP), an approach to solve chance constrained optimization (CCO) problems, i.e., optimization problems with nonlinear constraint functions affected by arbitrary random parameters. CPP utilizes samples from these random parameters along with the quantile lemma -- which is central to CP -- to transform the CCO problem into a deterministic optimization problem. We then present two tractable reformulations of CPP by: (1) writing the quantile as a linear program along with its KKT conditions (CPP-KKT), and (2) using mixed integer programming (CPP-MIP). CPP comes with marginal probabilistic feasibility guarantees for the CCO problem that are conceptually different from existing approaches, e.g., the sample approximation and the scenario approach. While we explore algorithmic similarities with the sample approximation approach, we emphasize that the strength of CPP is that it can easily be ext
    
[^97]: TeMPO：面向边缘AI的高效时分多路动态光学张量核心，具备紧凑型慢光电光调制器

    TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator

    [https://arxiv.org/abs/2402.07393](https://arxiv.org/abs/2402.07393)

    TeMPO是一种时分多路动态光学张量加速器，通过跨层设备/电路/架构定制，弥合了光学神经加速器与高度定制的电子对应器之间的性能差距。在设备级别上，该加速器采用了工厂可用的、定制的光学器件，包括实验演示的慢光电光调制器、光分配器和相移器，显著减小了输入编码和点乘中的占地面积和功耗。

    

    由于光学的计算速度和效率优越，在资源受限的边缘平台上，基于电光计算系统为能效人工智能（AI）加速任务提供了巨大的潜力，特别是针对实时、低能量的深度神经网络（DNN）推理任务。然而，目前基于工厂可用设备和传统系统架构的光学神经加速器与高度定制的电子对应器相比，仍然存在性能差距。为了弥合由于领域专业化不足而导致的性能差距，我们提出了一种名为TeMPO的时分多路动态光学张量加速器，通过跨层设备/电路/架构定制实现。在设备级别上，我们呈现了工厂可用的、定制的光学器件，包括实验演示的慢光电光调制器、光分配器和相移器，显著减小了输入编码和点乘中的占地面积和功耗。

    Electronic-photonic computing systems offer immense potential in energy-efficient artificial intelligence (AI) acceleration tasks due to the superior computing speed and efficiency of optics, especially for real-time, low-energy deep neural network (DNN) inference tasks on resource-restricted edge platforms. However, current optical neural accelerators based on foundry-available devices and conventional system architecture still encounter a performance gap compared to highly customized electronic counterparts. To bridge the performance gap due to lack of domain specialization, we present a time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with cross-layer device/circuit/architecture customization. At the device level, we present foundry-compatible, customized photonic devices, including a slow-light electro-optic modulator with experimental demonstration, optical splitters, and phase shifters that significantly reduce the footprint and power in input encoding and dot-
    
[^98]: 在多臂赌博机中，可复制性渐进自由

    Replicability is Asymptotically Free in Multi-armed Bandits

    [https://arxiv.org/abs/2402.07391](https://arxiv.org/abs/2402.07391)

    本论文研究在多臂赌博机问题中，通过引入探索-再确定算法和连续淘汰算法，以及谨慎选择置信区间的幅度，实现了可复制性，并证明了当时间界足够大时，可复制算法的额外代价是不必要的。

    

    本研究受可复制的机器学习需求的推动，研究了随机多臂赌博机问题。特别地，我们考虑了一个可复制算法，确保算法的操作序列不受数据集固有随机性的影响。我们观察到，现有算法所需的遗憾值比不可复制算法多$O(1/\rho^2)$倍，其中$\rho$是非复制程度。然而，我们证明了当给定的$\rho$下时间界$T$足够大时，此额外代价是不必要的，前提是谨慎选择置信区间的幅度。我们引入了一个先探索后决策的算法，在决策之前均匀选择动作。此外，我们还研究了一个连续淘汰算法，在每个阶段结束时淘汰次优动作。为了确保这些算法的可复制性，我们将随机性引入决策制定中。

    This work is motivated by the growing demand for reproducible machine learning. We study the stochastic multi-armed bandit problem. In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset. We observe that existing algorithms require $O(1/\rho^2)$ times more regret than nonreplicable algorithms, where $\rho$ is the level of nonreplication. However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\rho$, provided that the magnitude of the confidence bounds is chosen carefully. We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. To ensure the replicability of these algorithms, we incorporate randomness into their decision-ma
    
[^99]: 无假设测试算法性能的限制

    The Limits of Assumption-free Tests for Algorithm Performance

    [https://arxiv.org/abs/2402.07388](https://arxiv.org/abs/2402.07388)

    这项研究探讨了使用有限数据量回答算法性能问题的基本限制，证明了黑盒测试方法无法准确回答算法在不同训练集上的整体性能和特定模型的性能问题。

    

    算法评价和比较是机器学习和统计学中基本的问题，一个算法在给定的建模任务中表现如何，哪个算法表现最佳？许多方法已经开发出来评估算法性能，通常基于交叉验证策略，将感兴趣的算法在不同的数据子集上重新训练，并评估其在留出数据点上的性能。尽管广泛使用这些程序，但对于这些方法的理论性质尚未完全理解。在这项工作中，我们探讨了在有限的数据量下回答这些问题的一些基本限制。特别地，我们区分了两个问题: 算法$A$在大小为$n$的训练集上学习问题有多好，以及在特定大小为$n$的训练数据集上运行$A$所产生的特定拟合模型有多好？我们的主要结果证明，对于任何将算法视为黑盒的测试方法，无法准确地回答这两个问题。

    Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   Our main results prove that, for any test that treats the algor
    
[^100]: 探索多模态大型语言模型的感知限制

    Exploring Perceptual Limitation of Multimodal Large Language Models

    [https://arxiv.org/abs/2402.07384](https://arxiv.org/abs/2402.07384)

    在这项研究中，我们探索了多模态大型语言模型（MLLMs）在感知能力上的限制。我们发现，对于小型视觉对象的问题，MLLMs的回答能力普遍存在限制。通过控制干预实验，我们发现物体质量、大小和位置都对MLLMs的感知能力有影响。

    

    多模态大型语言模型（MLLMs）最近展示了在回答视觉问题方面引人注目的感知能力，然而，对它们感知能力的限制知之甚少。特别是，在先前的研究中，虽然提供了MLLMs对物体大小敏感的个别证据，但这一现象及其潜在原因尚未得到全面探究。在本研究中，我们定量研究了几个最先进的MLLMs对小型视觉对象的感知，并揭示了它们在回答与小型对象有关的问题时普遍存在的限制。接下来，我们确定了四个独立因素，它们可能导致这种限制，包括物体质量、大小、干扰物和位置，并进行了控制性干预研究，以评估每个因素对MLLMs感知能力的影响。特别是，我们发现低质量的物体和更小的物体大小都可以独立降低MLLMs回答视觉问题的能力。更令人惊讶的是，我们发现物体位置对MLLMs的感知能力也有影响。

    Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images. Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions. More surprisingly, we find that the locati
    
[^101]: 使基于流匹配的零样本文本到语音系统自由地产生笑声

    Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like

    [https://arxiv.org/abs/2402.07383](https://arxiv.org/abs/2402.07383)

    本文提出了ELaTE，一种基于流匹配的零样本文本到语音系统，可以根据短音频提示以精确控制笑声时机和表情生成任何说话者的自然笑声。

    

    笑声是人类语音中最表达性和自然的一部分，传达着情感、社交暗示和幽默。然而，大多数文本到语音(TTS)系统缺乏产生逼真且合适的笑声的能力，限制了其应用和用户体验。虽然之前有工作生成了自然的笑声，但在控制生成的笑声的时机和多样性方面仍存在不足。在这项工作中，我们提出了ELaTE，一种可以基于短音频提示以精确控制笑声时机和表情的零样本TTS系统，可以产生任何说话者的自然笑声。具体而言，ELaTE通过音频提示来模仿声音特征，通过文本提示来指示所生成语音的内容，通过输入来控制笑声表情，可以是笑声的起始和结束时间，或包含要模仿的笑声的另外音频提示。我们的模型基于找到的技术基础进行了开发。

    Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression. Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked. We develop our model based on the foundati
    
[^102]: Diff-RNTraj: 一种面向道路网络约束的轨迹生成的结构感知扩散模型

    Diff-RNTraj: A Structure-aware Diffusion Model for Road Network-constrained Trajectory Generation

    [https://arxiv.org/abs/2402.07369](https://arxiv.org/abs/2402.07369)

    这篇论文提出了一种结构感知的扩散模型Diff-RNTraj，用于在道路网络上生成受到约束的轨迹数据，解决了现有方法中无法保证轨迹约束在道路上、缺乏道路相关信息的问题。

    

    轨迹数据对于各种应用非常重要，因为它记录了车辆的移动情况。然而，由于隐私问题，公开可用的轨迹数据集在规模上仍然有限，这阻碍了轨迹数据挖掘和基于轨迹的应用的发展。为了解决这个问题，已经提出了一些生成合成轨迹的方法来扩大数据集的规模。然而，所有现有方法都是在地理坐标系统中生成轨迹，这对于实际应用存在两个限制：1）不能确保生成的轨迹在道路上受到约束。2）缺乏与道路相关的信息。在本文中，我们提出了一个新的问题，即面向实际应用需求的道路网络约束轨迹（RNTraj）生成，它可以直接在道路网络上生成带有道路相关信息的轨迹。RNTraj是一种混合类型的数据，在每个poi上都与时间相关联。

    Trajectory data is essential for various applications as it records the movement of vehicles. However, publicly available trajectory datasets remain limited in scale due to privacy concerns, which hinders the development of trajectory data mining and trajectory-based applications. To address this issue, some methods for generating synthetic trajectories have been proposed to expand the scale of the dataset. However, all existing methods generate trajectories in the geographical coordinate system, which poses two limitations for their utilization in practical applications: 1) the inability to ensure that the generated trajectories are constrained on the road. 2) the lack of road-related information. In this paper, we propose a new problem to meet the practical application need, \emph{i.e.}, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories on the road network with road-related information. RNTraj is a hybrid type of data, in which each poi
    
[^103]: 通过上下文学习评估分组代表建模的泛化能力

    Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning

    [https://arxiv.org/abs/2402.07368](https://arxiv.org/abs/2402.07368)

    本研究通过使用2016年和2020年的选举数据，评估了基于大型语言模型的分组代表模型在泛化能力上的表现。研究发现，尽管使用实证数据进行条件设定可以提高整体性能，但上下文学习的益处在不同人口子群组之间差异很大，这对实施分组代表模型的从业人员和决策者构成了挑战。

    

    本研究通过对2016年和2020年美国全国选举研究的数据进行上下文学习，评估基于大型语言模型（LLM）的分组代表模型（SRMs）从实证数据中的泛化能力。我们探讨了在不同响应变量和人口子群组之间的泛化能力。尽管使用实证数据进行条件设定可以提高整体性能，但上下文学习的益处在不同人口子群组之间差异很大，有时对某个人口子群组的性能产生了负面影响，但对其他人口子群组的性能产生了积极影响。上下文学习对SRM的不公平益处为实施SRM的从业人员和依赖于其的决策者带来了挑战。我们的工作突出了对来自不同人口子群组的细粒度基准的需求，这些基准不仅测试忠实度，还测试泛化能力。

    This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.
    
[^104]: 通过期望最大化和Turbo Deep近似消息传递的贝叶斯联邦学习

    Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing

    [https://arxiv.org/abs/2402.07366](https://arxiv.org/abs/2402.07366)

    本文提出了一种基于消息传递的贝叶斯联邦学习（BFL）框架，通过结合期望最大化和Turbo Deep近似消息传递（TDAMP）实现分布式学习和压缩。该框架在处理联邦学习算法的缺点上有着显著的改进。

    

    联邦学习是一种机器学习范 paradigm，在这种范式中，客户端拥有分散的训练数据，而中央服务器则负责聚合和调度。通常情况下，联邦学习算法涉及客户端使用随机梯度下降（SGD）来训练他们的本地模型，但这带来了收敛速度较慢和容易陷入次优解的问题。在这项工作中，我们提出了一种基于消息传递的贝叶斯联邦学习（BFL）框架来避免这些缺点。具体而言，我们将深度神经网络（DNN）的学习和压缩问题建模为稀疏贝叶斯推断问题，其中采用了分组稀疏先验以实现结构化模型压缩。然后，我们提出了一种高效的 BFL 算法，名为 EMTDAMP，其中将期望最大化（EM）和 Turbo Deep 近似消息传递（TDAMP）结合起来实现分布式学习和压缩。中央服务器聚合本地后验分布以实现更新。

    Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update 
    
[^105]: 相对绩效标准下的异质代理人最优投资的深度学习方法

    A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents

    [https://arxiv.org/abs/2402.07365](https://arxiv.org/abs/2402.07365)

    本文提出了一种基于深度学习的方法，用于相对绩效标准下的最优投资。该方法利用前向-后向随机微分方程的纳什均衡特征和随机微分游戏的机器学习算法的最新进展。数值实验在两个不同的金融模型上进行，比较了不同交互结构的图形图的影响。

    

    引入了Graphon游戏来研究通过一张交互的加权图的许多玩家的游戏。通过取极限，得到了一个具有连续玩家的游戏，其中的交互是通过一个图形图。本文关注相对绩效标准下的图形游戏的最优投资，并提出了一种深度学习方法。该方法建立在两个关键要素的基础上：首先，通过前向-后向随机微分方程对纳什均衡进行了表征，其次，利用随机微分游戏的机器学习算法的最新进展。我们对两个不同的金融模型进行了数值实验，在每个模型中比较了几种不同的图形图对交互结构的影响。

    Graphon games have been introduced to study games with many players who interact through a weighted graph of interaction. By passing to the limit, a game with a continuum of players is obtained, in which the interactions are through a graphon. In this paper, we focus on a graphon game for optimal investment under relative performance criteria, and we propose a deep learning method. The method builds upon two key ingredients: first, a characterization of Nash equilibria by forward-backward stochastic differential equations and, second, recent advances of machine learning algorithms for stochastic differential games. We provide numerical experiments on two different financial models. In each model, we compare the effect of several graphons, which correspond to different structures of interactions.
    
[^106]: 基于策略稳定性的学习算法在一价拍卖中的竞标

    Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions

    [https://arxiv.org/abs/2402.07363](https://arxiv.org/abs/2402.07363)

    本论文提出了一种在一价拍卖中进行竞标的新颖算法，并通过分析证明了其在战略背景下的效果。具体而言，这些算法在面对对策性卖家时表现良好，激励买家进行真实的交易，并获得了最佳的后悔结果。

    

    在游戏理论和机器学习的交界处，学习在重复的一价拍卖中进行竞标是一个基本问题，由于显示广告转向一价拍卖，最近受到了广泛关注。在这项工作中，我们提出了一个新颖的凹函数形式，用于一价拍卖中纯策略的竞标，并将其用于分析这个问题的自然梯度上升算法。重要的是，我们的分析超越了过去工作的差距，还考虑了在线广告市场的战略背景，其中部署了竞标算法 - 我们证明了我们的算法不会被策略性卖家利用，并且它们激励买家诚实交易。具体而言，我们证明了当最高竞争出价通过对抗方式生成时，我们的算法达到了$O(\sqrt{T})$的后悔，并表明没有更好的在线算法可以做得更好。进一步证明了当最高竞争出价通过对抗方式生成时，我们的算法达到了$O(\log T)$的后悔。

    Learning to bid in repeated first-price auctions is a fundamental problem at the interface of game theory and machine learning, which has seen a recent surge in interest due to the transition of display advertising to first-price auctions. In this work, we propose a novel concave formulation for pure-strategy bidding in first-price auctions, and use it to analyze natural Gradient-Ascent-based algorithms for this problem. Importantly, our analysis goes beyond regret, which was the typical focus of past work, and also accounts for the strategic backdrop of online-advertising markets where bidding algorithms are deployed -- we prove that our algorithms cannot be exploited by a strategic seller and that they incentivize truth-telling for the buyer.   Concretely, we show that our algorithms achieve $O(\sqrt{T})$ regret when the highest competing bids are generated adversarially, and show that no online algorithm can do better. We further prove that the regret improves to $O(\log T)$ when th
    
[^107]: 回归树用于快速和自适应的预测区间

    Regression Trees for Fast and Adaptive Prediction Intervals

    [https://arxiv.org/abs/2402.07357](https://arxiv.org/abs/2402.07357)

    该论文提出了一种新的、与模型无关的方法族，用于校准具有局部覆盖保证的回归问题的预测区间。这种方法利用回归树和随机森林训练来创建最粗糙的特征空间划分，以近似条件覆盖，提供了准确、快速和自适应的预测区间。

    

    预测模型会犯错，因此需要量化与其预测相关的不确定性。符合性推断已经成为一种强大的工具，可以在点预测周围创建统计上有效的预测区域，但是它在回归问题上的朴素应用会产生非自适应的区域。新的符合性得分，通常依赖于分位数回归器或条件密度估计器，旨在解决这个限制。虽然它们在创建预测带方面很有用，但这些得分与量化任意预测模型周围的不确定性的原始目标脱节。本文提出了一种新的、与模型无关的方法族，用于校准具有局部覆盖保证的回归问题的预测区间。我们的方法是基于追求最粗糙的特征空间划分来近似条件覆盖。我们通过对符合性得分进行回归树和随机森林的训练来创建这个划分。我们的提议将回归树和随机森林应用于符合性推断的新领域，以提供准确、快速和自适应的预测区间。

    Predictive models make mistakes. Hence, there is a need to quantify the uncertainty associated with their predictions. Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. We create this partition by training regression trees and Random Forests on conformity scores. Our proposal
    
[^108]: 一个新的高斯最小最大定理及其应用

    A Novel Gaussian Min-Max Theorem and its Applications

    [https://arxiv.org/abs/2402.07356](https://arxiv.org/abs/2402.07356)

    本文介绍了一个新的高斯最小最大定理，扩展了经典定理对于独立但非恒定分布的情况。此外，该定理在高维统计学、机器学习、非光滑优化和信号处理等领域有广泛的应用。

    

    Gordon的一个著名结果允许比较两个高斯过程的最小最大行为，如果满足某些不等式条件。这个结果的结果包括高斯最小最大（GMT）和凸高斯最小最大（CGMT）定理，这些定理在高维统计学、机器学习、非光滑优化和信号处理方面有广泛的应用。目前为止，没有发现满足这些不等式的其他一对高斯过程。在本文中，我们确定了这样一对新的高斯过程。由此得到的定理将经典的GMT定理和CGMT定理从基本过程中的底层高斯矩阵具有iid行的情况扩展到具有独立但非恒定分布的情况。新的CGMT定理应用于多源高斯回归问题，以及属于的二元分类问题。

    A celebrated result by Gordon allows one to compare the min-max behavior of two Gaussian processes if certain inequality conditions are met. The consequences of this result include the Gaussian min-max (GMT) and convex Gaussian min-max (CGMT) theorems which have had far-reaching implications in high-dimensional statistics, machine learning, non-smooth optimization, and signal processing. Both theorems rely on a pair of Gaussian processes, first identified by Slepian, that satisfy Gordon's comparison inequalities. To date, no other pair of Gaussian processes satisfying these inequalities has been discovered. In this paper, we identify such a new pair. The resulting theorems extend the classical GMT and CGMT Theorems from the case where the underlying Gaussian matrix in the primary process has iid rows to where it has independent but non-identically-distributed ones. The new CGMT is applied to the problems of multi-source Gaussian regression, as well as to binary classification of genera
    
[^109]: 从均场稳态分布中采样

    Sampling from the Mean-Field Stationary Distribution

    [https://arxiv.org/abs/2402.07355](https://arxiv.org/abs/2402.07355)

    本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    

    我们研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，或者等价地，即包含交互项的概率测度空间上的最小化函数的复杂性。我们的主要洞察是将这个问题的两个关键方面解耦：(1) 通过有限粒子系统逼近均场SDE，通过时间均匀传播混沌，和(2) 通过标准对数凹抽样器从有限粒子稳态分布中采样。我们的方法在概念上更简单，其灵活性允许结合用于算法和理论的最新技术。这导致在许多设置中提供了改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
    
[^110]: 基于数据分布的课程学习

    Data Distribution-based Curriculum Learning

    [https://arxiv.org/abs/2402.07352](https://arxiv.org/abs/2402.07352)

    本文提出了一种基于数据分布的课程学习方法（DDCL），通过利用数据集的数据分布和两种评分方法（DDCL（密度）和DDCL（点）），根据样本的顺序来提高分类器的性能。

    

    训练样本的顺序对分类器的性能有着重要影响。课程学习是一种将训练样本从简单到困难排序的方法。本文提出了一种称为基于数据分布的课程学习（DDCL）的创新方法。DDCL利用数据集的数据分布以及两种评分方法（DDCL（密度）和DDCL（点））来决定训练样本的顺序。DDCL（密度）利用样本密度分配评分，而DDCL（点）利用欧氏距离进行评分。我们通过在多个数据集上使用神经网络、支持向量机和随机森林分类器进行实验来评估所提出的DDCL方法。评估结果显示，与没有任何课程的标准评估相比，应用DDCL方法可以提高所有数据集的平均分类准确率。

    The order of training samples can have a significant impact on the performance of a classifier. Curriculum learning is a method of ordering training samples from easy to hard. This paper proposes the novel idea of a curriculum learning approach called Data Distribution-based Curriculum Learning (DDCL). DDCL uses the data distribution of a dataset to build a curriculum based on the order of samples. Two types of scoring methods known as DDCL (Density) and DDCL (Point) are used to score training samples thus determining their training order. DDCL (Density) uses the sample density to assign scores while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the proposed DDCL approach by conducting experiments on multiple datasets using a neural network, support vector machine and random forest classifier. Evaluation results show that the application of DDCL improves the average classification accuracy for all datasets compared to standard evaluation without any curriculum. 
    
[^111]: TextFooler黑盒对01 loss sign激活神经网络集成的攻击准确性

    Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble

    [https://arxiv.org/abs/2402.07347](https://arxiv.org/abs/2402.07347)

    本研究研究了TextFooler黑盒对01 loss sign激活神经网络集成的攻击准确性。研究发现，相比于sigmoid激活交叉熵和二进制神经网络，使用01 loss sign激活的网络更难受到TextFooler的攻击，并且通过引入一种新的全局汇集步骤，进一步提高了对抗精确度，使TextFooler几乎无效化。

    

    最近的研究表明，01 loss sign激活神经网络在图像分类对抗攻击方面具有防御能力。针对CIFAR10数据集攻击模型的公共挑战仍然保持不败。在本研究中，我们提出以下问题：使用一种名为TextFooler的流行黑盒文本对抗攻击程序来欺骗01 loss sign激活神经网络困难吗？我们在四个流行的文本分类数据集上研究了这个问题：IMDB评论、Yelp评论、MR情感分类和AG新闻分类。我们发现，与sigmoid激活交叉熵和二进制神经网络相比，我们的01 loss sign激活网络更难受到TextFooler的攻击。我们还研究了一种01 loss sign激活卷积神经网络，它具有一种针对sign激活网络的新型全局汇集步骤。通过这种新的变体，我们看到了在对抗精确度方面的显著提高，使得TextFooler对它几乎无效。我们提供我们的代码供免费使用。

    Recent work has shown the defense of 01 loss sign activation neural networks against image classification adversarial attacks. A public challenge to attack the models on CIFAR10 dataset remains undefeated. We ask the following question in this study: are 01 loss sign activation neural networks hard to deceive with a popular black box text adversarial attack program called TextFooler? We study this question on four popular text classification datasets: IMDB reviews, Yelp reviews, MR sentiment classification, and AG news classification. We find that our 01 loss sign activation network is much harder to attack with TextFooler compared to sigmoid activation cross entropy and binary neural networks. We also study a 01 loss sign activation convolutional neural network with a novel global pooling step specific to sign activation networks. With this new variation we see a significant gain in adversarial accuracy rendering TextFooler practically useless against it. We make our code freely avail
    
[^112]: 使用离线强化学习进行ICU病人的测量排程

    Measurement Scheduling for ICU Patients with Offline Reinforcement Learning

    [https://arxiv.org/abs/2402.07344](https://arxiv.org/abs/2402.07344)

    本研究介绍了使用离线强化学习进行ICU病人实验室检测排程的方法，并探索了最新数据集和离线强化学习方法在这一领域的应用。研究结果有助于提高实验室检测排程策略的质量和实用性。

    

    ICU病人的实验室检测排程是一个重要挑战。研究表明，在ICU中下达的实验室检测约有20-40%是多余的，可以在不妨碍患者安全的情况下消除。先前的研究利用离线强化学习（Offline-RL）基于患者信息找到了最佳的实验室检测排程策略。然而，自那时以来已经发布了新的ICU病人数据集，并且离线强化学习方法也取得了各种进展。在本研究中，我们首先介绍了针对时间序列任务的新发布的MIMIC-IV数据集的预处理流程。然后，我们探索了最先进的离线强化学习方法在识别更好的ICU病人实验室检测排程策略方面的功效。除了评估方法的性能，我们还讨论了在ICU环境中使用离线强化学习框架进行实验室检测排程的总体适用性和实用性。

    Scheduling laboratory tests for ICU patients presents a significant challenge. Studies show that 20-40% of lab tests ordered in the ICU are redundant and could be eliminated without compromising patient safety. Prior work has leveraged offline reinforcement learning (Offline-RL) to find optimal policies for ordering lab tests based on patient information. However, new ICU patient datasets have since been released, and various advancements have been made in Offline-RL methods. In this study, we first introduce a preprocessing pipeline for the newly-released MIMIC-IV dataset geared toward time-series tasks. We then explore the efficacy of state-of-the-art Offline-RL methods in identifying better policies for ICU patient lab test scheduling. Besides assessing methodological performance, we also discuss the overall suitability and practicality of using Offline-RL frameworks for scheduling laboratory tests in ICU settings.
    
[^113]: 对线性强化学习领域的噪声自适应置信区间及其在贝叶斯优化中的应用

    Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization

    [https://arxiv.org/abs/2402.07341](https://arxiv.org/abs/2402.07341)

    这项研究提出了一种对线性强化学习领域中未知噪声水平的自适应置信区间，与已有方法相比，在维度较大时具有显著的改进。此外，针对有界奖励，还提出了一种方差自适应置信区间，具有更好的数值性能。

    

    在序贯决策中，适应未知噪声水平是一个非常重要但具有挑战性的问题，因为有效的探索通常需要对噪声水平有一定的了解，而噪声水平通常只能粗略地指定。我们在线性强化学习领域取得了显著进展，主要有两方面。首先，我们提出了一种新颖的置信区间，该置信区间在未知的亚高斯参数σ_*^2上是“半自适应”的，意味着（归一化的）置信宽度与√（dσ_*^2 + σ_0^2）成正比，其中d为维度，σ_0^2为指定的（已知）亚高斯参数，其值可能比σ_*^2大得多。相比于Abbasi-Yadkori等人（2011）的标准置信区间的√（dσ_0^2），这是一个显著的改进，特别是当d较大时。我们证明了这导致了线性强化学习中改进的后悔边界。其次，对于有界奖励，我们提出了一种新颖的方差自适应置信区间，具有更好的数值性能。

    Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified. We report significant progress in addressing this issue in linear bandits in two respects. First, we propose a novel confidence set that is `semi-adaptive' to the unknown sub-Gaussian parameter $\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\sqrt{d\sigma_*^2 + \sigma_0^2}$ where $d$ is the dimension and $\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\sigma_*^2$. This is a significant improvement over $\sqrt{d\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that this leads to an improved regret bound in linear bandits. Second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numeri
    
[^114]: 用图神经网络对随机几何图进行对齐

    Random Geometric Graph Alignment with Graph Neural Networks

    [https://arxiv.org/abs/2402.07340](https://arxiv.org/abs/2402.07340)

    本文研究了在图对齐问题中，通过图神经网络可以高概率恢复正确的顶点对齐。通过特定的特征稀疏性和噪声水平条件，我们证明了图神经网络的有效性，并与直接匹配方法进行了比较。

    

    我们研究了在顶点特征信息存在的情况下，图神经网络在图对齐问题中的性能。具体而言，给定两个独立扰动的单个随机几何图以及噪声稀疏特征的情况下，任务是恢复两个图的顶点之间的未知一对一映射关系。我们证明在特征向量的稀疏性和噪声水平满足一定条件的情况下，经过精心设计的单层图神经网络可以在很高的概率下通过图结构来恢复正确的顶点对齐。我们还证明了噪声水平的条件上界，仅存在对数因子差距。最后，我们将图神经网络的性能与直接在噪声顶点特征上求解分配问题进行了比较。我们证明了当噪声水平至少为常数时，这种直接匹配会导致恢复不完全，而图神经网络可以容忍n

    We characterize the performance of graph neural networks for graph alignment problems in the presence of vertex feature information. More specifically, given two graphs that are independent perturbations of a single random geometric graph with noisy sparse features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs. We show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed one-layer graph neural network can with high probability recover the correct alignment between the vertices with the help of the graph structure. We also prove that our conditions on the noise level are tight up to logarithmic factors. Finally we compare the performance of the graph neural network to directly solving an assignment problem on the noisy vertex features. We demonstrate that when the noise level is at least constant this direct matching fails to have perfect recovery while the graph neural network can tolerate n
    
[^115]: 差分隐私训练混合专家模型

    Differentially Private Training of Mixture of Experts Models

    [https://arxiv.org/abs/2402.07334](https://arxiv.org/abs/2402.07334)

    本文研究了在自然语言处理中，如何使用差分隐私训练混合专家模型。研究通过探索MoE模型的潜力和应用DP解决了计算和隐私问题，并实验证明了MoE模型可以在保护隐私的前提下有效训练。

    

    本文研究了在自然语言处理领域中，将差分隐私(DP)与混合专家模型(MoE)的训练相结合。随着大型语言模型(LLMs)的参数规模扩大到数十亿，利用庞大的数据集，它们展现出了增强的语言能力和新兴的能力。然而，这种增长引发了重要的计算和隐私问题。我们的研究通过探索MoE模型的潜力（它们以计算效率著称）和应用DP（隐私保护的标准）来解决这些问题。我们首次尝试在DP的约束下训练MoE模型，解决了其架构和DP整合的复杂性所带来的独特挑战。我们的初步实验研究表明，MoE模型可以有效地使用DP进行训练，其性能与非隐私训练的模型相媲美。这项初步研究旨在提供有价值的参考。

    This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing. As Large Language Models (LLMs) scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities. However, this growth raises significant computational and privacy concerns. Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation. We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration. Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts. This initial study aims to provide val
    
[^116]: 从挖掘Hugging Face存储库中学到的教训

    Lessons Learned from Mining the Hugging Face Repository

    [https://arxiv.org/abs/2402.07323](https://arxiv.org/abs/2402.07323)

    这个报告总结了在Hugging Face存储库进行的两项研究的见解，重点关注了碳排放、ML模型的演化和维护方面，为未来的研究人员提供了实用指南。

    

    快速发展的机器学习（ML）和人工智能领域见证了像Hugging Face（HF）这样的平台的兴起，成为模型开发和共享的中心枢纽。本经验报告综合了两项在HF上进行的综合研究，重点关注碳排放、ML模型的演化和维护方面的见解。我们的目标是为未来在HF生态系统中进行软件存储库研究的研究人员提供实用指南，以提高这些研究的质量。我们深入探讨了我们研究中使用的复制包的复杂性，重点介绍了促进我们分析的关键工具和方法。此外，我们提出了一个细致的分层抽样策略，针对多样化的HF Hub数据集，确保了代表性和全面的分析方法。该报告还介绍了初步的指南，从存储库挖掘过渡到队列研究，以确立更详实的研究方案。

    The rapidly evolving fields of Machine Learning (ML) and Artificial Intelligence have witnessed the emergence of platforms like Hugging Face (HF) as central hubs for model development and sharing. This experience report synthesizes insights from two comprehensive studies conducted on HF, focusing on carbon emissions and the evolutionary and maintenance aspects of ML models. Our objective is to provide a practical guide for future researchers embarking on mining software repository studies within the HF ecosystem to enhance the quality of these studies. We delve into the intricacies of the replication package used in our studies, highlighting the pivotal tools and methodologies that facilitated our analysis. Furthermore, we propose a nuanced stratified sampling strategy tailored for the diverse HF Hub dataset, ensuring a representative and comprehensive analytical approach. The report also introduces preliminary guidelines, transitioning from repository mining to cohort studies, to esta
    
[^117]: 总结事实：LLMs中背后的加法机制解析事实回忆的过程

    Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs

    [https://arxiv.org/abs/2402.07321](https://arxiv.org/abs/2402.07321)

    Transformer-based大型语言模型在事实回忆任务中使用加法模式来存储和检索知识，通过相加组合多个独立的机制对正确答案进行构造性干涉。

    

    Transformer-based大型语言模型(LLMs)是如何存储和检索知识的？我们关注最基本的任务形式——事实回忆，模型通过在"事实：斗兽场位于国家"等提示中明确呈现存储的事实。我们发现，事实回忆背后的机制故事比以前认为的要复杂。它包含几种不同、独立且性质不同的机制，这些机制通过相加组合，对正确的属性进行构造性干涉。我们将这一通用现象称为加法模式：模型通过加总多个独立的贡献进行计算。每个机制的贡献本身可能是不足够的，但总和会对正确答案产生积极的干涉。此外，我们扩展了直接对logit属性的方法，可以将一个注意头的输出归因给单独的源记号。我们使用这种技术来解开我们所说的"混合头"——它们本身是a的一部分.

    How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pa
    
[^118]: 用语言嵌入实现可解释、安全的自动驾驶：新颖性识别和主动学习的框架与实验分析

    Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets

    [https://arxiv.org/abs/2402.07320](https://arxiv.org/abs/2402.07320)

    本研究提出了一种使用语言嵌入和主动学习来实现可解释、安全的自动驾驶的框架。通过使用对比语言-图像预训练嵌入进行聚类实验，我们有效地识别和解释了新颖的自动驾驶场景。

    

    本研究探索了在自动驾驶数据集中使用语言嵌入进行主动学习，重点研究新颖性检测。新颖性指的是自动驾驶车辆难以应对的意外情况，需要更高级别的推理能力。我们提出的方法使用基于语言的表示来识别新颖场景，强调安全接管响应和主动学习的双重目的。研究还通过使用对比语言-图像预训练（CLIP）嵌入进行聚类实验，对从两个真实驾驶数据集（一个安装在车辆上、一个安装在基础设施上）衍生的子集进行新颖性检测。通过生成的聚类，我们进一步提出了区分被分类为新颖场景和其他场景的元素的文本解释方法，展示了实验数据池中的差异。

    This research explores the integration of language embeddings for active learning in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level reasoning abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and active learning. The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting quali
    
[^119]: ODIN: 脱耦奖励缓解RLHF中的黑客攻击

    ODIN: Disentangled Reward Mitigates Hacking in RLHF

    [https://arxiv.org/abs/2402.07319](https://arxiv.org/abs/2402.07319)

    本研究解决了强化学习中的奖励黑客问题，针对回复长度这一挑战，通过建立可靠的评估协议和改进奖励模型的方法，提出了减轻长度偏差的超参数和技巧，并进行了大规模研究。

    

    在这项工作中，我们研究了在LLMs上从人类反馈的强化学习中出现的响应长度上的奖励黑客问题。LLMs的格式良好但不太有用的回复往往会欺骗LLMs甚至人类评估者以获得高分。同样的问题也存在于RL中的某些奖励模型中。为了解决训练和评估中的挑战，我们建立了一个更可靠的评估协议，用于比较不同训练配置之间的LLM评估分数和通过改变训练超参数得到的响应长度之间的权衡。基于这个评估，我们进行了大规模研究，结果揭示了RL中用于减轻长度偏差的超参数和技巧的有效性。我们进一步提出通过在共享特征表示上联合训练两个线性头来改进奖励模型，以预测奖励，一个训练来与长度相关，另一个训练来与内容相关。

    In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the oth
    
[^120]: 一种关于一般KL正则化偏好下纳什学习从人类反馈中的理论分析

    A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference

    [https://arxiv.org/abs/2402.07314](https://arxiv.org/abs/2402.07314)

    本论文从理论层面分析了一种关于一般偏好下纳什学习从人类反馈中的方法，通过对两个竞争的LLM进行博弈来找到一种一致生成响应的策略。

    

    来自人类反馈的强化学习（RLHF）从一个概率偏好模型提供的偏好信号中学习，该模型以一个提示和两个响应作为输入，并产生一个分数，表示对一个响应相对于另一个响应的偏好程度。迄今为止，最流行的RLHF范式是基于奖励的，它从奖励建模的初始步骤开始，然后使用构建的奖励为后续的奖励优化阶段提供奖励信号。然而，奖励函数的存在是一个强假设，基于奖励的RLHF在表达能力上有局限性，不能捕捉到真实世界中复杂的人类偏好。在这项工作中，我们为最近提出的学习范式Nash学习从人类反馈（NLHF）提供了理论洞察力，该学习范式考虑了一个一般的偏好模型，并将对齐过程定义为两个竞争的LLM之间的博弈。学习目标是找到一个一致生成响应的策略。

    Reinforcement Learning from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.   In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive LLMs. The learning objective is to find a policy that consistently generates responses
    
[^121]: BioNeRF: 用于视图合成的生物合理神经辐射场

    BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis

    [https://arxiv.org/abs/2402.07310](https://arxiv.org/abs/2402.07310)

    BioNeRF是一种生物合理的架构，通过辐射场对场景进行3D表示并合成新视图。它实现了一种认知启发的机制，提高了存储能力和提取信息的能力，并在真实世界图像和合成数据的两个数据集上超过了以人的感知为基础的质量度量的最新结果。

    

    本文介绍了BioNeRF，一种生物合理的架构，它通过辐射场对场景进行3D表示并合成新视图。由于NeRF依赖于网络权重来存储场景的三维表示，BioNeRF实现了一种受认知启发的机制，将来自多个来源的输入融合成内存类似的结构，提高存储能力并提取更多内在和相关信息。BioNeRF还模仿了金字塔细胞中关于上下文信息的一种行为，其中内存作为上下文提供，并与两个后续神经模型的输入相结合，一个负责生成容积密度，另一个负责渲染场景的颜色。实验结果表明，BioNeRF在两个数据集（真实世界图像和合成数据）上超过了以人的感知为基础的质量度量的最新结果。

    This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.
    
[^122]: HyperBERT:将混合超图感知层与语言模型用于文本属性超图上的节点分类

    HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs

    [https://arxiv.org/abs/2402.07309](https://arxiv.org/abs/2402.07309)

    本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。

    

    超图通过复杂的拓扑结构标记，表达多个实体之间的高阶相互作用，其中超边扮演重要角色。最近，基于超图的深度学习方法在学习文本属性超图上的节点分类问题中引起了越来越多的研究关注。然而，现有方法往往难以同时捕捉超图结构信息的全部内容和节点属性中的丰富语言属性，这在很大程度上影响了它们的效果和泛化能力。为了克服这些挑战，我们探索了如何通过为节点分类任务进一步增强预训练的BERT模型，引入专门的超图感知层。这些层将高阶结构归纳偏差引入语言模型中，从而提高模型利用超图结构中的高阶上下文信息和文本中的语义信息的能力。

    Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
    
[^123]: 自洽的符合预测

    Self-Consistent Conformal Prediction

    [https://arxiv.org/abs/2402.07307](https://arxiv.org/abs/2402.07307)

    自洽的符合预测方法能够提供既符合校准的预测又符合以模型预测的动作为条件的预测区间，为决策者提供了严格的、针对具体动作的决策保证。

    

    在机器学习指导下的决策中，决策者通常在具有相同预测结果的情境中采取相同的行动。符合预测帮助决策者量化动作的结果不确定性，从而实现更好的风险管理。受这种观点的启发，我们引入了自洽的符合预测，它产生了既符合Venn-Abers校准的预测，又符合以模型预测引发的动作为条件的符合预测区间。我们的方法可以后验地应用于任何黑盒预测器，提供严格的、针对具体动作的决策保证。数值实验表明，我们的方法在区间的效率和条件的有效性之间达到了平衡。

    In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.
    
[^124]: 在无服务器联邦学习中使用知识蒸馏训练异构客户端模型

    Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning

    [https://arxiv.org/abs/2402.07295](https://arxiv.org/abs/2402.07295)

    本文提出在无服务器联邦学习中利用知识蒸馏训练异构客户端模型的方法，以解决资源和统计数据的异质性挑战。

    

    联邦学习是一种新兴的机器学习范式，它使得分布式客户端之间能够协作训练共享的全局模型，同时保持数据的去中心化。最近的研究在设计高效的联邦学习系统方面表明，利用无服务器计算技术，特别是函数即服务（FaaS）用于联邦学习，可以提高资源效率，降低训练成本，并减轻数据持有者面临的复杂基础设施管理负担。然而，现有的无服务器联邦学习系统在训练过程中隐式地假设所有参与客户端具有相同的全局模型架构。这一假设未能解决实际联邦学习中的基本挑战，因为不同客户端之间存在资源和统计数据的异质性。为了解决这些挑战并在无服务器联邦学习中实现异构客户端模型，本文提出了利用知识蒸馏（KD）的新型优化无服务器工作流。

    Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper. Towards this, we propose novel optimized serverless workflows for two popular conve
    
[^125]: 机器学习基于调用图剪枝的有效性：一个实证研究

    On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study

    [https://arxiv.org/abs/2402.07294](https://arxiv.org/abs/2402.07294)

    本研究通过引入具有高测试覆盖率的真实Java程序数据集，探索了机器学习基于调用图剪枝的有效性，并通过对比分析静态调用图生成技术来解决当前方法存在的问题。

    

    静态调用图(CG)构建经常过度近似调用关系，导致结果精确但不准确。最近的研究探索了基于机器学习(ML)的CG剪枝作为一种提高精确性的手段，通过消除虚假边缘。然而，目前的方法存在评估数据集有限、训练数据不平衡，以及召回率降低等问题，这影响了实际的下游分析。之前的结果也没有与先进的静态CG构建技术进行比较。这项研究解决了这些问题。我们引入了NYXCorpus，一个具有高测试覆盖率的真实Java程序数据集，并从测试执行中收集轨迹并构建动态CG的基准。我们利用这些CG来探索ML-based CG剪枝器在训练和推断过程中的保守剪枝策略。我们对静态CG使用零控制流分析(0-CFA)生成的结果和由上下文敏感的1-CFA算法生成的结果进行了比较分析。

    Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG pruning as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative pruning strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both wit
    
[^126]: CLIPPER：无需初始猜测的稳健数据关联

    CLIPPER: Robust Data Association without an Initial Guess

    [https://arxiv.org/abs/2402.07284](https://arxiv.org/abs/2402.07284)

    本文提出了一种无需初始猜测的稳健数据关联方法CLIPPER，通过利用加权图和寻找最密集的加权点团来解决数据关联的挑战。实验结果显示，该方法在点云注册方面取得了显著的性能提升。

    

    在估计过程中，识别噪声数据中的对应关系是一个非常重要的步骤。当有一个有信息的初始估计猜测可用时，数据关联的挑战就不那么严重；然而，在大多数情况下，很少有高质量的初始猜测。我们探索了基于图的数据关联形式，不需要初始估计猜测。现有的基于图的方法优化无权图，丢弃了权重边中编码的重要一致性信息，并经常尝试精确解决NP困难问题。相比之下，我们提出了一个新的优化问题，充分利用加权图，并寻求最密集的加权点团。我们对这个问题引入了两个松弛条件：一个凸半定松弛，我们发现在实际中很紧密；以及一个快速的一阶算法CLIPPER，通常可以在毫秒级别内得到接近最优解。在点云注册方面的评估中，我们的方法显示出了很大的优势和性能提升。

    Identifying correspondences in noisy data is a critically important step in estimation processes. When an informative initial estimation guess is available, the data association challenge is less acute; however, the existence of a high-quality initial guess is rare in most contexts. We explore graph-theoretic formulations for data association, which do not require an initial estimation guess. Existing graph-theoretic approaches optimize over unweighted graphs, discarding important consistency information encoded in weighted edges, and frequently attempt to solve NP-hard problems exactly. In contrast, we formulate a new optimization problem that fully leverages weighted graphs and seeks the densest edge-weighted clique. We introduce two relaxations to this problem: a convex semidefinite relaxation which we find to be empirically tight, and a fast first-order algorithm called CLIPPER which frequently arrives at nearly-optimal solutions in milliseconds. When evaluated on point cloud regis
    
[^127]: 基于知识图谱的电力变压器故障预测

    Power Transformer Fault Prediction Based on Knowledge Graphs

    [https://arxiv.org/abs/2402.07283](https://arxiv.org/abs/2402.07283)

    本文提出了一种基于知识图谱和梯度提升决策树的方法，用于学习有限的电力变压器故障数据。实验证明该方法在故障预测准确度上优于传统的人工神经网络和逻辑回归方法。

    

    本文针对电力变压器仅有有限的故障数据这一挑战，提出了一种解决方案。传统的运维工具对潜在故障的预测能力有限。由于故障数据的稀缺性，使得机器学习技术很难有效应用。为了解决这个问题，我们提出了一种新颖的方法，将知识图谱（Knowledge Graph，KG）技术与梯度提升决策树（Gradient Boosting Decision Trees，GBDT）相结合。该方法旨在从少量的高维数据中高效学习，整合了影响变压器故障的各种因素和历史运行数据。我们的方法能够在故障特征数据有限的情况下，实现对电力变压器的准确安全状态评估和故障分析。实验结果表明，相比于其他学习方法，如人工神经网络（Artificial Neural Networks，ANN）和逻辑回归（Logistic Regression，LR），我们的方法在预测准确度方面表现出更好的性能。

    In this paper, we address the challenge of learning with limited fault data for power transformers. Traditional operation and maintenance tools lack effective predictive capabilities for potential faults. The scarcity of extensive fault data makes it difficult to apply machine learning techniques effectively. To solve this problem, we propose a novel approach that leverages the knowledge graph (KG) technology in combination with gradient boosting decision trees (GBDT). This method is designed to efficiently learn from a small set of high-dimensional data, integrating various factors influencing transformer faults and historical operational data. Our approach enables accurate safe state assessments and fault analyses of power transformers despite the limited fault characteristic data. Experimental results demonstrate that this method outperforms other learning approaches in prediction accuracy, such as artificial neural networks (ANN) and logistic regression (LR). Furthermore, it offers
    
[^128]: 大型语言模型如何在诚实与帮助之间进行权衡？

    How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?

    [https://arxiv.org/abs/2402.07282](https://arxiv.org/abs/2402.07282)

    本文研究了如何在大型语言模型中权衡诚实和帮助性，在实验中发现强化学习改善了诚实和帮助性，而链式思维提示则偏向于帮助性。研究结果还展示了GPT-4 Turbo对对话框架和听众决策背景的敏感性。这些发现揭示了大型语言模型内化的对话价值观，并暗示零-shot提示可以在一定程度上引导这些抽象价值观。

    

    在日常交流中，人们经常为了最大限度地帮助听众而近似真相，例如约略时间或省略细节。大型语言模型（LLMs）如何处理这种微妙的权衡？为了回答这个问题，我们使用心理模型和旨在描述人类行为的实验来分析LLMs。我们测试了一系列LLMs，并探讨了优化人类偏好或推理时思考对这些权衡的影响。我们发现，从人类反馈中的强化学习改善了诚实和帮助性，而链式思维提示使LLMs偏向于帮助性而不是诚实。最后，GPT-4 Turbo展示了类似人类的回应模式，包括对对话框架和听众决策背景的敏感性。我们的研究结果揭示了LLMs内化的对话价值观，并暗示即使这些抽象价值观也可以在零-shot提示下在一定程度上被引导。

    In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
    
[^129]: 《基于树结构方法的异常检测能否超越深度学习？一项基准研究》

    Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study

    [https://arxiv.org/abs/2402.07281](https://arxiv.org/abs/2402.07281)

    本文通过一项基准研究评估了多种基于机器学习的异常检测算法，包括树结构方法和深度学习方法，并揭示了深度学习神话的真相。

    

    在确保服务连续性时，复杂的关键任务系统中检测异常情况至关重要。由于异常事件被认为是罕见事件，因此从操作数据中检测异常情况面临着类别分布不平衡问题的挑战。本文通过全面的基准研究评估了多种基于机器学习的异常检测算法。论文通过对各种异常检测算法的公正比较做出了重大贡献，包括经典机器学习方法、各种基于树结构的方法、深度学习和异常点检测方法。论文使用了104个公开可用的和少数专有的工业系统数据集，增强了研究的多样性，使算法性能的评估更加真实，并强调了对实际场景的适应性的重要性。论文揭示了深度学习神话的真相。

    Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study. The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth
    
[^130]: 通过利用分类数据集和其语义层次，开展视觉语言模型的开放式VQA评估

    Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy

    [https://arxiv.org/abs/2402.07270](https://arxiv.org/abs/2402.07270)

    该研究通过提出创新的评估方法和基于分类数据集的新型VQA基准，推动了对文本生成的视觉语言模型能力的理解。同时，他们还提出了使用语义层次和自动生成的后续问题来改进对细粒度分类任务上粗糙答案的评估。通过比较不同度量标准，他们在进行人工评估研究的基础上选择了最终的度量标准。

    

    评估文本生成的视觉语言模型是一项具有挑战性但至关重要的工作。通过解决现有视觉问答（VQA）基准的局限性并提出创新的评估方法，我们的研究旨在推动我们对这些模型能力的理解。我们提出了一种基于知名视觉分类数据集的新型VQA基准，可以对文本生成的视觉语言模型进行细粒度评估，并与判别性视觉语言模型进行比较。为了改善对细粒度分类任务上粗糙答案的评估，我们建议使用标签空间的语义层次来提出关于基准类别的自动生成的后续问题。最后，我们比较了传统的自然语言处理和基于LLM的度量标准来评估给定基准答案的模型预测问题。我们进行了人工评估研究，基于此决定最终度量标准的选择。

    The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be
    
[^131]: 使用PathFormer进行高精确度疾病诊断和高可重复性生物标志物识别

    Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer

    [https://arxiv.org/abs/2402.07268](https://arxiv.org/abs/2402.07268)

    使用名为PathFormer的新型GNN模型架构，系统整合信号网络、先验知识和组学数据来实现高精确度疾病诊断和高可重复性生物标志物识别。

    

    生物标志物的识别对于精确的疾病诊断和理解疾病发病机制在组学数据分析中至关重要，比如使用折叠变化和回归分析。图神经网络（GNNs）是分析图结构数据的主要深度学习模型。然而，我们发现现有GNNs在组学数据分析中存在两个主要限制，即预测（诊断）准确性有限和在多个数据集中可重复的生物标志物识别能力有限。这些挑战的根源在于生物信号通路的独特图结构，其中包括大量的靶点和这些靶点之间密集而复杂的信号交互。为了解决这两个挑战，在本研究中，我们提出了一种名为PathFormer的新型GNN模型架构，它系统地整合信号网络、先验知识和组学数据来对生物标志物进行排名和预测疾病诊断。在比较结果中，PathFormer表现优于现有方法。

    Biomarker identification is critical for precise disease diagnosis and understanding disease pathogenesis in omics data analysis, like using fold change and regression analysis. Graph neural networks (GNNs) have been the dominant deep learning model for analyzing graph-structured data. However, we found two major limitations of existing GNNs in omics data analysis, i.e., limited-prediction (diagnosis) accuracy and limited-reproducible biomarker identification capacity across multiple datasets. The root of the challenges is the unique graph structure of biological signaling pathways, which consists of a large number of targets and intensive and complex signaling interactions among these targets. To resolve these two challenges, in this study, we presented a novel GNN model architecture, named PathFormer, which systematically integrate signaling network, priori knowledge and omics data to rank biomarkers and predict disease diagnosis. In the comparison results, PathFormer outperformed ex
    
[^132]: 具有硬线性等式约束的物理信息神经网络

    Physics-Informed Neural Networks with Hard Linear Equality Constraints

    [https://arxiv.org/abs/2402.07251](https://arxiv.org/abs/2402.07251)

    这项工作提出了一种新颖的物理信息神经网络（KKT-hPINN），通过从KKT条件导出的投影层，严格保证硬线性等式约束，并在多个数值实验中展示了其提高预测准确性的能力。

    

    代理模型用来替代计算成本昂贵的模拟。神经网络被广泛应用作为代理模型，能够有效评估复杂的物理系统。尽管如此，神经网络是数据驱动模型，没有任何物理知识。将物理知识融入神经网络可以提高泛化能力和数据效率。物理信息神经网络（PINN）是一种利用数据中已知物理约束的方法，但无法严格满足预测中的约束。本文提出了一种新颖的物理信息神经网络，KKT-hPINN，通过从KKT条件导出的投影层严格保证硬线性等式约束。在连续搅拌槽反应器（CSTR）单元、萃取精馏子系统和化工厂的Aspen模型上进行的数值实验表明，该模型可以进一步提高预测准确性。

    Surrogate modeling is used to replace computationally expensive simulations. Neural networks have been widely applied as surrogate models that enable efficient evaluations over complex physical systems. Despite this, neural networks are data-driven models and devoid of any physics. The incorporation of physics into neural networks can improve generalization and data efficiency. The physics-informed neural network (PINN) is an approach to leverage known physical constraints present in the data, but it cannot strictly satisfy them in the predictions. This work proposes a novel physics-informed neural network, KKT-hPINN, which rigorously guarantees hard linear equality constraints through projection layers derived from KKT conditions. Numerical experiments on Aspen models of a continuous stirred-tank reactor (CSTR) unit, an extractive distillation subsystem, and a chemical plant demonstrate that this model can further enhance the prediction accuracy.
    
[^133]: DIMON:在一系列变形的域上学习偏微分方程的解算算子

    DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains

    [https://arxiv.org/abs/2402.07250](https://arxiv.org/abs/2402.07250)

    DIMON是一个学习在一系列变形的域上解算偏微分方程的通用算子学习框架，通过在参考域训练数据上学习解的映射，然后将其重新映射回原始域来实现对多个域上变化的初始/边界条件下的偏微分方程求解的近似。

    

    在多个域上变化的初始/边界条件下求解偏微分方程的解在各种应用中都是需要的，但是如果每次域的初始/边界条件变化时都重新计算解，计算成本很高。我们引入了一个通用的算子学习框架，称为DIffeomorphic Mapping Operator LearNing（DIMON），用来学习解在域族$\{\Omega_{\theta}}_\theta$上的近似解，它学习从初始/边界条件和域$\Omega_\theta$到偏微分方程的解（或指定的函数）的映射。DIMON基于将给定问题（初始/边界条件和域$\Omega_{\theta}$）转移到一个参考域$\Omega_{0}$上进行处理，其中使用来自多个问题的训练数据来学习到在$\Omega_{0}$上的解的映射，然后再将其重新映射回原始域$\Omega_{\theta}$。我们考虑了几个问题来展示该框架的性能。

    The solution of a PDE over varying initial/boundary conditions on multiple domains is needed in a wide variety of applications, but it is computationally expensive if the solution is computed de novo whenever the initial/boundary conditions of the domain change. We introduce a general operator learning framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$, that learns the map from initial/boundary conditions and domain $\Omega_\theta$ to the solution of the PDE, or to specified functionals thereof. DIMON is based on transporting a given problem (initial/boundary conditions and domain $\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where training data from multiple problems is used to learn the map to the solution on $\Omega_{0}$, which is then re-mapped to the original domain $\Omega_{\theta}$. We consider several problems to demonstrate the performance of the framewo
    
[^134]: 领域知识和多模态对智能分子性质预测的影响：一项系统调查

    The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey

    [https://arxiv.org/abs/2402.07249](https://arxiv.org/abs/2402.07249)

    本文通过系统调查，发现整合领域知识可以提高分子性质预测的准确性，同时利用多模态数据融合可以产生更精确的结果。

    

    准确预测分子性质对于药物开发尤其是虚拟筛选和化合物优化的进展至关重要。近年来引入了许多基于深度学习的方法，在增强分子性质预测（MPP）方面显示出显著潜力，特别是提高了准确性和对分子结构的洞察力。然而，有两个关键问题：领域知识的整合是否增强了分子性质预测的准确性，使用多模态数据融合是否比单一数据来源方法产生更精确的结果？为了探究这些问题，我们全面回顾和定量分析了基于各种基准的最新深度学习方法。我们发现，整合分子信息将分别提高MPP回归和分类任务的准确性，分别高达3.98％和1.72％。我们还发现，使用三维信息与一维和二维信息相结合会产生更好的结果。

    The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization. The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures. Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing multi-modal data fusion yield more precise results than unique data source methods? To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various benchmarks. We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively. We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional informati
    
[^135]: 神经网络中的深度分离：将维度与准确度分离

    Depth Separations in Neural Networks: Separating the Dimension from the Accuracy

    [https://arxiv.org/abs/2402.07248](https://arxiv.org/abs/2402.07248)

    通过研究深度2和深度3神经网络在逼近Lipschitz目标函数时的分离性质，证明了维度诅咒也会在深度2逼近中存在，即使目标函数可以使用深度3高效表示。这为以前确定深度要求的下界提供了新的观点，并且适用于多种激活函数。

    

    我们证明了深度2和深度3神经网络在逼近一个$\mathcal{O}(1)$-Lipschitz目标函数至常数精度时的指数分离，对于支持在$[0,1]^{d}$上的分布，假设权重指数有界。这解决了在\citet{safran2019depth}中提出的一个问题，并证明了维度诅咒在深度2逼近中的存在，即使在目标函数可以使用深度3高效表示的情况下也是如此。以前，将深度2和深度3分离的下界要求至少有一个Lipschitz参数、目标准确度或逼近域的大小（某种度量）与输入维度多项式地缩放，而我们保持前两者不变，并将我们的域限制在单位超立方体上。我们的下界适用于各种激活函数，并基于一种新的平均情况到最坏情况的随机自约化论证的应用，以减少

    We prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\mathcal{O}(1)$-Lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights. This addresses an open problem posed in \citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3. Previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the Lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube. Our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce
    
[^136]: 朝着广义逆强化学习的方向

    Towards Generalized Inverse Reinforcement Learning

    [https://arxiv.org/abs/2402.07246](https://arxiv.org/abs/2402.07246)

    本文研究了广义逆强化学习（GIRL）在马尔科夫决策过程（MDP）中的应用，提出了一种解决观察策略与最优策略之间差异以及不完全可观察情况下数学表述最优策略的方法，并开发了一种快速的启发式算法，数值结果显示其有效性。

    

    本文研究了在马尔科夫决策过程（MDP）中的广义逆强化学习（GIRL），即在观察到的行为（策略）可能不是最优的情况下学习MDP的基本组成部分的问题。这些组成部分不仅包括奖励函数和转移概率矩阵，还包括动作空间和状态空间，虽然不完全已知但知道属于给定不确定性集合。我们解决了GIRL中的两个关键挑战：首先，需要量化观察策略与潜在最优策略之间的差异；其次，当MDP的基本组成部分不可观察或部分可观察时，数学上描述潜在最优策略的困难。然后，我们提出了GIRL的数学公式并开发了一种快速的启发式算法。在有限状态和无限状态问题上的数值结果显示了我们公式和算法的优点。

    This paper studies generalized inverse reinforcement learning (GIRL) in Markov decision processes (MDPs), that is, the problem of learning the basic components of an MDP given observed behavior (policy) that might not be optimal. These components include not only the reward function and transition probability matrices, but also the action space and state space that are not exactly known but are known to belong to given uncertainty sets. We address two key challenges in GIRL: first, the need to quantify the discrepancy between the observed policy and the underlying optimal policy; second, the difficulty of mathematically characterizing the underlying optimal policy when the basic components of an MDP are unobservable or partially observable. Then, we propose the mathematical formulation for GIRL and develop a fast heuristic algorithm. Numerical results on both finite and infinite state problems show the merit of our formulation and algorithm.
    
[^137]: GenSTL: 通过特征域的自回归生成实现通用稀疏轨迹学习

    GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains

    [https://arxiv.org/abs/2402.07232](https://arxiv.org/abs/2402.07232)

    GenSTL是一个通用的稀疏轨迹学习框架，通过自回归生成特征域来实现稀疏轨迹与密集轨迹之间的连接，从而消除了对大规模密集轨迹数据的依赖。

    

    轨迹是时间戳位置样本的序列。在稀疏轨迹中，位置样本的采样是不频繁的；尽管这种轨迹在现实世界中很常见，但要使用它们来实现高质量的与交通相关的应用程序是具有挑战性的。当前的方法要么假设轨迹是密集采样的并且经过准确的地图匹配，要么依赖于两阶段方案，从而产生次优的应用程序。为了扩展稀疏轨迹的效用，我们提出了一种新颖的稀疏轨迹学习框架GenSTL。该框架经过预训练以使用特征域的自回归生成形成稀疏轨迹与密集轨迹之间的连接。GenSTL可以直接应用于下游任务，或者可以先进行微调。通过这种方式，GenSTL消除了对大规模密集和地图匹配轨迹数据的依赖。其中包括精心设计的特征域编码层和分层的...

    Trajectories are sequences of timestamped location samples. In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications. Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications.   To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL. The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains. GenSTL can subsequently be applied directly in downstream tasks, or it can be fine-tuned first. This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data. The inclusion of a well-crafted feature domain encoding layer and a hierarchical m
    
[^138]: 通过对齐和一致性重新思考图形遮罩自编码器

    Rethinking Graph Masked Autoencoders through Alignment and Uniformity

    [https://arxiv.org/abs/2402.07225](https://arxiv.org/abs/2402.07225)

    通过对齐和一致性重新思考图形遮罩自编码器对图的自监督学习方法进行了理论分析，揭示了GraphMAE中的节点级重构目标实际上执行了上下文级对比学习，并指出了GraphMAE在对齐和一致性方面的局限性。

    

    自监督学习在图中可以分为对比和生成两种方法。过去几年，对比方法，也被称为图对比学习（GCL），在图的自监督学习中占据了主导地位，但最近出现的图形遮罩自编码器（GraphMAE）重新点燃了生成方法的动力。尽管GraphMAE在实证上取得了成功，但对其有效性仍缺乏理论理解。此外，虽然生成和对比方法都被证明是有效的，但它们之间的联系和差异尚未得到全面研究。因此，我们在理论上建立了GraphMAE和GCL之间的桥梁，并证明了GraphMAE中的节点级重构目标隐含地执行了上下文级GCL。基于我们的理论分析，我们进一步从对齐和一致性的角度识别了GraphMAE的局限性，这被认为是高质量图的两个关键属性之一。

    Self-supervised learning on graphs can be bifurcated into contrastive and generative methods. Contrastive methods, also known as graph contrastive learning (GCL), have dominated graph self-supervised learning in the past few years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and contrastive methods have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-qual
    
[^139]: 面向扩散生成模型的快速随机采样方法

    Towards Fast Stochastic Sampling in Diffusion Generative Models

    [https://arxiv.org/abs/2402.07211](https://arxiv.org/abs/2402.07211)

    本文提出了一种在扩散生成模型中进行快速随机采样的方法，通过对分裂积分器进行原则性修改，实现了更高的采样效率。在CIFAR-10数据集上进行实验，100次网络函数评估下的FID分数为2.36。

    

    扩散模型在推理时生成样本的速度较慢。尽管最近有一些努力在改善扩散模型的随机采样效率，但仍然有待改进。我们提出了基于分裂积分器的预训练扩散模型的快速随机采样方法。分裂积分器通常在分子动力学中使用，通过巧妙地在涉及数据、辅助或噪声变量的数值更新之间交替来提高采样效率。然而，我们发现对于快速采样，简单应用分裂积分器是次优的。因此，我们提出了几种原则上修改了简单分裂采样器以提高采样效率的方法，并将得到的采样器称为减小分裂积分器。在CIFAR-10数据集上使用相空间朗之万扩散 (PSLD) [Pandey \& Mandt, 2023] 的背景下，我们的随机采样器在仅进行100次网络函数评估后，实现了2.36的FID分数。

    Diffusion models suffer from slow sample generation at inference time. Despite recent efforts, improving the sampling efficiency of stochastic samplers for diffusion models remains a promising direction. We propose Splitting Integrators for fast stochastic sampling in pre-trained diffusion models in augmented spaces. Commonly used in molecular dynamics, splitting-based integrators attempt to improve sampling efficiency by cleverly alternating between numerical updates involving the data, auxiliary, or noise variables. However, we show that a naive application of splitting integrators is sub-optimal for fast sampling. Consequently, we propose several principled modifications to naive splitting samplers for improving sampling efficiency and denote the resulting samplers as Reduced Splitting Integrators. In the context of Phase Space Langevin Diffusion (PSLD) [Pandey \& Mandt, 2023] on CIFAR-10, our stochastic sampler achieves an FID score of 2.36 in only 100 network function evaluations 
    
[^140]: 结合空间优化和大型语言模型的开放领域城市行程规划

    Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning

    [https://arxiv.org/abs/2402.07204](https://arxiv.org/abs/2402.07204)

    本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。

    

    本文首次提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程。OUIP与传统行程规划不同，传统规划限制了用户表达更详细的需求，阻碍了真正的个性化。最近，大型语言模型(LLM)在处理多样化任务方面表现出潜力。然而，由于非实时信息、不完整的知识和不足的空间意识，它们无法独立地提供满意的用户体验。鉴于此，我们提出了一个名为ItiNera的OUIP系统，将空间优化与大型语言模型(LLM)相结合，根据用户需求提供个性化的城市行程定制服务。具体来说，我们开发了一个基于LLM的流水线，用于提取和更新兴趣点特征，以创建用户自己的个性化兴趣点数据库。对于每个用户请求，我们利用LLM进行协同实现优化。

    In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
    
[^141]: 异常值感知的结构再参数网络低比特量化训练

    Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks

    [https://arxiv.org/abs/2402.07200](https://arxiv.org/abs/2402.07200)

    这项研究提出了一种异常值感知批归一化(OABN)和一种聚类的非均匀量化训练(ClusterQAT)框架，用于处理结构再参数化网络(SR)中由合并过程引入的异常值，以提高量化性能。

    

    轻量级卷积神经网络(CNNs)的设计需要在模型架构和压缩技术方面进行协同设计。作为一种将训练和推断分离的新设计范式，结构再参数化(SR)网络，如代表性的RepVGG，使简单的VGG样式网络焕发了新的生机，具有与先进且常常更复杂的网络相当的高准确度。然而，SR网络中的合并过程会在权重中引入异常值，使其分布与传统网络不同，因此增加了量化的困难。为了解决这个问题，我们提出了一种称为异常值感知批归一化(OABN)的操作级改进方法。此外，为了满足有限比特宽度的要求并保持推断准确性，我们开发了一种基于聚类的非均匀量化训练(ClusterQAT)框架。将OABN与ClusterQAT相结合，可以提高RepVG的量化性能。

    Lightweight design of Convolutional Neural Networks (CNNs) requires co-design efforts in the model architectures and compression techniques. As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks. However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in quantization. To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a clustering-based non-uniform quantization framework for Quantization-Aware Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the quantized performance of RepVG
    
[^142]: 梯度噪声的隐性偏见：从对称性角度来看

    The Implicit Bias of Gradient Noise: A Symmetry Perspective

    [https://arxiv.org/abs/2402.07193](https://arxiv.org/abs/2402.07193)

    本研究通过对对称性的存在进行分析，揭示了梯度噪声在随机梯度下降中的隐性偏见。我们发现不同类型的对称性会导致不同的学习动态，其中一类对称性可以自然收敛，而另一类对称性几乎总是发散。此外，我们的研究结果适用于没有对称性的损失函数，对于理解训练动态和解释相关实际问题具有普适性。

    

    我们对随机梯度下降（SGD）在损失函数存在连续对称性时的学习动态进行了表征，说明了SGD和梯度下降之间的分歧是多么巨大。我们展示了根据对称性对学习动态的影响方式，我们可以将一族对称性分为两类。对于一类对称性，SGD自然地收敛到具有平衡和对齐梯度噪声的解。对于另一类对称性，SGD几乎总是发散的。然后，我们展示了即使损失函数中没有对称性，我们的结果依然适用并可以帮助我们理解训练动态。我们的主要结果是普遍的，它只依赖于对称性的存在，而与损失函数的细节无关。我们证明了所提出的理论对于逐步变形和平坦化提供了解释，并可以应用于常见的实际问题，如表示正则化。

    We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, SGD will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, 
    
[^143]: GSINA: 通过图Sinkhorn Attention改进图不变学习中的子图提取

    GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention

    [https://arxiv.org/abs/2402.07191](https://arxiv.org/abs/2402.07191)

    本文提出了一种改进的图不变学习方法，通过稀疏性、软性和可微性原则来提取不变子图，从而提高图学习的泛化性能。

    

    图不变学习(GIL)是一种有效的方法，用于在不同分布变化下发现图数据与其标签之间的不变关系，以解决各种图学习任务。最近的GIL研究主要集中在从输入图中提取不变子图，作为规则化策略来提高图学习的泛化性能。然而，这些方法在获取不变子图方面也存在各种限制。本文分析了现有工作的缺点，并提出了提取不变子图的相应原则：1）稀疏性，以过滤掉变异特征；2）软性，以获得更广泛的解空间；和3）可微性，以进行端到端优化。为了在一次操作中满足这些原则，我们利用最优传输(OT)理论，并提出了一种新颖的图注意机制，称为图Sinkhorn Attention（G)

    Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (G
    
[^144]: 通过张量化随机投影改进局部敏感哈希LSH

    Improving LSH via Tensorized Random Projection

    [https://arxiv.org/abs/2402.07189](https://arxiv.org/abs/2402.07189)

    本文提出了CP-E2LSH和TT-E2LSH两种方法，用于改进局部敏感哈希算法LSH，在处理张量数据的欧几里得距离和余弦相似度时能够提供更快和更空间有效的结果。

    

    局部敏感哈希(LSH)是数据科学家用于近似最近邻搜索问题的基本算法工具，已在许多大规模数据处理应用中广泛使用，如近似重复检测、最近邻搜索、聚类等。在本文中，我们旨在提出更快和空间更有效的局部敏感哈希函数，用于张量数据的欧几里得距离和余弦相似度。通常，对于张量数据获得LSH的朴素方法涉及将张量重塑为向量，然后应用现有的向量数据LSH方法(E2LSH和SRP)。然而，对于高阶张量，这种方法变得不切实际，因为重塑向量的大小在张量的阶数中呈指数增长。因此，LSH参数的大小呈指数增加。为解决这个问题，我们提出了两种欧几里得距离和余弦相似度的LSH方法，分别是CP-E2LSH和TT-E2LSH。

    Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by data scientists for approximate nearest neighbour search problems that have been used extensively in many large scale data processing applications such as near duplicate detection, nearest neighbour search, clustering, etc. In this work, we aim to propose faster and space efficient locality sensitive hash functions for Euclidean distance and cosine similarity for tensor data. Typically, the naive approach for obtaining LSH for tensor data involves first reshaping the tensor into vectors, followed by applying existing LSH methods for vector data $E2LSH$ and $SRP$. However, this approach becomes impractical for higher order tensors because the size of the reshaped vector becomes exponential in the order of the tensor. Consequently, the size of LSH parameters increases exponentially. To address this problem, we suggest two methods for LSH for Euclidean distance and cosine similarity, namely $CP-E2LSH$, $TT-E2LSH
    
[^145]: 分而治之：用多目标强化学习可靠地揭示帕累托前沿

    Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning

    [https://arxiv.org/abs/2402.07182](https://arxiv.org/abs/2402.07182)

    这项研究介绍了一个名为IPRO的算法，利用分解任务为一系列单目标问题方法，可可靠地揭示多目标强化学习中实现最优表现的策略的帕累托前沿，同时提供收敛保证和未发现解的距离上限。

    

    在多目标强化学习中，获取在不同偏好下实现最优表现的策略的帕累托前沿是一个重大挑战。我们引入了迭代帕累托参考优化（IPRO），这是一个原则性算法，它将找到帕累托前沿的任务分解成一系列具有各种解决方法的单目标问题。这使我们能够在每个步骤中建立收敛保证并提供未发现帕累托最优解的距离上限。实证评估表明，IPRO能够与需要额外领域知识的方法相匹配或优于它们。通过利用问题特定的单目标求解器，我们的方法也有望在多目标强化学习之外的应用中发挥作用，比如路径规划和优化问题。

    A significant challenge in multi-objective reinforcement learning is obtaining a Pareto front of policies that attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist. This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective reinforcement learning, such as in pathfinding and optimisation.
    
[^146]: MAGNETO：边缘人体活动识别的边缘AI--隐私和个性化

    MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization

    [https://arxiv.org/abs/2402.07180](https://arxiv.org/abs/2402.07180)

    本文提出了一种名为MAGNETO的边缘AI平台，通过从云端推向边缘进行增量人体活动学习，避免了云端与边缘设备之间的数据传输，实现了数据隐私保护、低延迟处理和高度个性化。

    

    人体活动识别（HAR）是一个成熟的领域，现代机器学习（ML）技术显著推动了其发展。尽管公司成功地将HAR整合到消费品中，但它们通常依赖于预定义的活动集，这限制了用户级（边缘设备）的个性化。尽管在增量学习方面取得了进展，能够使用新数据更新模型，但这通常发生在云端，需要定期在云端和边缘设备之间进行数据传输，从而引发数据隐私问题。在本文中，我们提出了一种名为MAGNETO的边缘AI平台，将HAR任务从云端推向边缘。MAGNETO允许在边缘设备上直接进行增量人体活动学习，而无需与云端进行任何数据交换。这可以提供强大的隐私保证、低处理延迟和高度的个性化。特别地，我们在Android设备上演示了MAGNETO，从数据采集到结果可视化，验证了整个流程。

    Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visua
    
[^147]: GeoFormer:一种基于视觉和序列Transformer的温室气体监测方法

    GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring

    [https://arxiv.org/abs/2402.07164](https://arxiv.org/abs/2402.07164)

    GeoFormer是一种基于视觉和序列Transformer的紧凑模型，用于从Sentinel-5P卫星图像中预测地表二氧化氮（NO2）浓度。该模型在准确性上表现出色。

    

    空气污染在全球范围内代表了一个关键的环境挑战，通过温室气体排放在气候变化中起着重要作用，并对全球数十亿人的健康产生负面影响。然而，预测污染物的空间和时间分布仍然具有挑战性。地面监测设施的稀缺以及空气污染建模对全面数据集的依赖性（这些数据集通常对许多地区不可获取）使得问题复杂化。在这项工作中，我们介绍了GeoFormer，这是一种结合了视觉Transformer模块和高效的时间序列Transformer模块的紧凑模型，可从Sentinel-5P卫星图像中预测地表二氧化氮（NO2）浓度。我们使用由Sentinel-5P图像和相应的NO2浓度读数构建的数据集，训练了提出的模型以预测地表NO2浓度测量值。该模型具有较高的准确性（MAE 5.65），证明了其有效性。

    Air pollution represents a pivotal environmental challenge globally, playing a major role in climate change via greenhouse gas emissions and negatively affecting the health of billions. However predicting the spatial and temporal patterns of pollutants remains challenging. The scarcity of ground-based monitoring facilities and the dependency of air pollution modeling on comprehensive datasets, often inaccessible for numerous areas, complicate this issue. In this work, we introduce GeoFormer, a compact model that combines a vision transformer module with a highly efficient time-series transformer module to predict surface-level nitrogen dioxide (NO2) concentrations from Sentinel-5P satellite imagery. We train the proposed model to predict surface-level NO2 measurements using a dataset we constructed with Sentinel-5P images of ground-level monitoring stations, and their corresponding NO2 concentration readings. The proposed model attains high accuracy (MAE 5.65), demonstrating the effica
    
[^148]: PASOA-基于粒子的贝叶斯最优自适应设计

    PASOA- PArticle baSed Bayesian Optimal Adaptive design

    [https://arxiv.org/abs/2402.07160](https://arxiv.org/abs/2402.07160)

    PASOA是一种新的贝叶斯实验设计程序，通过提供连续的后验分布的准确估计，同时执行顺序设计优化和参数推断。该方法使用 stochastic optimization 和 tempered SMC 来最大化期望信息增益，并提供了一致性的最优设计估计。

    

    我们提出了一种名为PASOA的新程序，用于贝叶斯实验设计，通过同时提供连续的后验分布的准确估计来执行顺序设计优化。顺序设计过程通过对比估计原则进行，使用随机优化和顺序蒙特卡罗（SMC）采样器来最大化期望信息增益（EIG）。由于连续后验分布之间的距离越大，获得的信息增益越大，因此这个EIG目标可能会恶化经典SMC的性能。为了解决这个问题，提出了温度调节，既可以获得大的信息增益，又可以获得准确的SMC采样，我们证明这对性能来说是至关重要的。这种随机优化和温度调节的新颖组合允许同时处理设计优化和参数推断。我们证明了所得到的最优设计估计量具有一致性。数值实验表明，我们的方法在相同计算预算下比其他方法更好地优化了设计。

    We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical
    
[^149]: 基于大型语言模型的智能界面在软件项目中的工作量和规模估计

    Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces

    [https://arxiv.org/abs/2402.07158](https://arxiv.org/abs/2402.07158)

    本文提出了一种基于大型语言模型的智能界面在软件项目中进行工作量和规模估计的方法，并通过比较传统方法，探讨了如何通过增强基于自然语言的问题规范来实现开发工作量的准确估计。

    

    大型语言模型（LLM）的发展也导致其应用的广泛增加。软件设计作为其中之一，在使用LLM作为扩展固定用户故事的接口组件方面获得了巨大的好处。然而，将基于LLM的人工智能代理包含在软件设计中常常带来意想不到的挑战，特别是在开发工作量的估计方面。通过基于用户界面的用户故事的例子，我们对比了传统方法，并提出了一种新的方法来增强基于自然语言的问题的规范，通过考虑数据源、接口和算法来进行开发工作量的估计。

    The advancement of Large Language Models (LLM) has also resulted in an equivalent proliferation in its applications. Software design, being one, has gained tremendous benefits in using LLMs as an interface component that extends fixed user stories. However, inclusion of LLM-based AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts. Through the example of UI-based user stories, we provide a comparison against traditional methods and propose a new way to enhance specifications of natural language-based questions that allows for the estimation of development effort by taking into account data sources, interfaces and algorithms.
    
[^150]: 自然语言强化学习

    Natural Language Reinforcement Learning

    [https://arxiv.org/abs/2402.07157](https://arxiv.org/abs/2402.07157)

    本研究将自然语言表示和强化学习原则相结合，提出了自然语言强化学习（NLRL）框架，解决了强化学习在样本效率低、解释性不足和缺乏监督信号等方面的限制问题，通过实验验证了其有效性和可解释性。

    

    强化学习（RL）在学习决策任务的策略方面展现出了令人瞩目的能力。然而，RL常常面临样本效率低、解释性不足和缺乏稀疏监督信号等问题的限制。为了解决这些问题，我们从人类学习过程中汲取灵感，引入了自然语言强化学习（NLRL），创新性地将RL原则与自然语言表示结合起来。具体而言，NLRL在自然语言空间中重新定义了任务目标、策略、价值函数、Bellman方程和策略迭代等RL概念。我们还展示了如何利用最新的大型语言模型（LLM）如GPT-4来实现NLRL。对表格MDPs的初步实验表明了NLRL框架的有效性、高效性和可解释性。

    Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
    
[^151]: 基于MIONet的混合迭代方法用于PDEs: 理论和数值实例研究

    A hybrid iterative method based on MIONet for PDEs: Theory and numerical examples

    [https://arxiv.org/abs/2402.07156](https://arxiv.org/abs/2402.07156)

    基于MIONet的混合迭代方法结合了传统的数值迭代求解器和神经操作符机器学习方法，具备卓越的加速效果。

    

    我们提出了一种基于MIONet的混合迭代方法用于PDEs，该方法结合了传统的数值迭代求解器和最近强大的神经操作符机器学习方法，并进一步系统地分析了它在离散化误差和模型推断误差方面的收敛条件、谱行为以及收敛速率等理论性质。我们对常用的平滑器，如Richardson（阻尼雅可比）和Gauss-Seidel，给出了理论结果。我们给出了混合方法相对于模型修正周期的收敛速度的上界，这表明了使混合迭代最快收敛的最小点。我们还通过几个数值实例，包括1维（2维）泊松方程的混合Richardson（Gauss-Seidel）迭代，验证了我们的理论结果，同时反映出了卓越的加速效果。作为一种无网格加速方法，它具备巨大的潜力。

    We propose a hybrid iterative method based on MIONet for PDEs, which combines the traditional numerical iterative solver and the recent powerful machine learning method of neural operator, and further systematically analyze its theoretical properties, including the convergence condition, the spectral behavior, as well as the convergence rate, in terms of the errors of the discretization and the model inference. We show the theoretical results for the frequently-used smoothers, i.e. Richardson (damped Jacobi) and Gauss-Seidel. We give an upper bound of the convergence rate of the hybrid method w.r.t. the model correction period, which indicates a minimum point to make the hybrid iteration converge fastest. Several numerical examples including the hybrid Richardson (Gauss-Seidel) iteration for the 1-d (2-d) Poisson equation are presented to verify our theoretical results, and also reflect an excellent acceleration effect. As a meshless acceleration method, it is provided with enormous po
    
[^152]: 使用图神经网络的可解释全球野火预测模型解释

    Explainable Global Wildfire Prediction Models using Graph Neural Networks

    [https://arxiv.org/abs/2402.07152](https://arxiv.org/abs/2402.07152)

    本研究提出了一个创新的基于图神经网络的全球野火预测模型，将全球气候和野火数据转化为图表示，解决了传统模型中的海洋数据缺失和远程依赖性问题，并展示了更高的预测准确性。同时，该模型还具有可解释性，揭示了其潜在价值。

    

    由于气候变化的不断加剧，野火预测变得越来越关键。传统的基于CNN的野火预测模型在处理缺失的海洋数据和解决气象数据中远程地区的长期依赖性方面存在困难。本文引入一种创新的基于图神经网络（GNN）的全球野火预测模型。我们提出了一种混合模型，将图卷积网络（GCNs）的空间能力与长短期记忆（LSTM）网络的时间深度相结合。我们的方法将全球气候和野火数据独特地转化为图表示，解决了传统模型中存在的空洞海洋数据位置和长期依赖性等挑战。通过使用未知的JULES-INFERNO模拟集对已建立的架构进行基准测试，我们的模型展现出优越的预测准确性。此外，我们强调了模型的可解释性，揭示了其中的潜在价值。

    Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change. Traditional CNN-based wildfire prediction models struggle with handling missing oceanic data and addressing the long-range dependencies across distant regions in meteorological data. In this paper, we introduce an innovative Graph Neural Network (GNN)-based model for global wildfire prediction. We propose a hybrid model that combines the spatial prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long Short-Term Memory (LSTM) networks. Our approach uniquely transforms global climate and wildfire data into a graph representation, addressing challenges such as null oceanic data locations and long-range dependencies inherent in traditional models. Benchmarking against established architectures using an unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior predictive accuracy. Furthermore, we emphasise the model's explainability, unveiling poten
    
[^153]: X-LoRA: 一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略在蛋白质力学和设计中的应用

    X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design

    [https://arxiv.org/abs/2402.07148](https://arxiv.org/abs/2402.07148)

    X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。

    

    我们报道了一种使用深层逐层基于低秩适应（LoRA）的新颖预训练适配器的混合专家策略，用于创建精细调整的大型语言模型。我们提出了一种利用隐藏状态动态混合经过适应的层的门控策略，允许得到的X-LoRA模型利用不同的能力并创建以前未使用的深层逐层适应的组合来解决特定任务。该设计受到了生物普遍性和多样性的生物学原理的启发，其中神经网络建模块在不同的分层表示中被重复使用。因此，X-LoRA模型可以轻松用于任何现有的大型语言模型（LLM），无需修改底层结构。我们还开发了一个定制的X-LoRA模型，提供了包括前向/逆向分析任务和增强推理能力在内的科学能力，重点是生物材料分析。

    We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
    
[^154]: 通过黑盒子模型实现鲁棒的车辆跟随动力学建模：方法论、分析和建议

    Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations

    [https://arxiv.org/abs/2402.07139](https://arxiv.org/abs/2402.07139)

    本研究通过实证评估，发现与经典车辆跟随模型不同，黑盒子模型对于最优的目标变量选择有不同的要求。

    

    在学习经典的车辆跟随模型如GIPPS、IDM等的参数时，选择目标变量是重要的。关于经典车辆跟随模型哪个目标变量是最优的有大量文献，但目前还没有研究对黑盒子模型如LSTM等的最优目标变量进行实证评估。本研究对三种黑盒子模型（GP、LSTM和Kernel Ridge回归）的不同目标变量进行了测试，如加速度、速度和车头间距。这些模型具有不同的目标函数和工作在不同的向量空间，例如，GP工作在函数空间，LSTM工作在参数空间。实验结果表明，对于黑盒子模型，最优的目标变量选择与经典的车辆跟随模型有所不同，具体取决于模型的特性。

    The selection of the target variable is important while learning parameters of the classical car following models like GIPPS, IDM, etc. There is a vast body of literature on which target variable is optimal for classical car following models, but there is no study that empirically evaluates the selection of optimal target variables for black-box models, such as LSTM, etc. The black-box models, like LSTM and Gaussian Process (GP) are increasingly being used to model car following behavior without wise selection of target variables. The current work tests different target variables, like acceleration, velocity, and headway, for three black-box models, i.e., GP, LSTM, and Kernel Ridge Regression. These models have different objective functions and work in different vector spaces, e.g., GP works in function space, and LSTM works in parameter space. The experiments show that the optimal target variable recommendations for black-box models differ from classical car following models depending
    
[^155]: 针对私有统计推断的重采样方法

    Resampling methods for Private Statistical Inference

    [https://arxiv.org/abs/2402.07131](https://arxiv.org/abs/2402.07131)

    这项研究提出了两种私有变体的非参数bootstrap方法，用于在差分隐私的情况下构建置信区间。方法在计算效率和置信区间长度上相比现有方法有显著改进。

    

    我们考虑使用差分隐私构建置信区间的任务。我们提出了两种私有变体的非参数bootstrap方法，该方法在数据的分区上私下计算多个“小”bootstrap的结果的中位数，并给出了得到的置信区间的渐进覆盖误差上界。对于固定的差分隐私参数ε，我们的方法在样本大小n上的误差率与非私有bootstrap相当，只是在对数因子内。我们使用真实数据和合成数据在均值估计、中位数估计和逻辑回归方面对我们的方法进行了经验验证。我们的方法在提供类似的覆盖精度的同时，比以前的方法提供了显著缩短（大约10倍）的置信区间。

    We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.
    
[^156]: 从去噪扩散隐式模型的潜空间中生成新的桥梁类型的尝试

    An attempt to generate new bridge types from latent space of denoising diffusion Implicit model

    [https://arxiv.org/abs/2402.07129](https://arxiv.org/abs/2402.07129)

    本论文尝试使用去噪扩散隐式模型创新桥梁类型，通过易于理解的代数方法推导出函数公式，使用深度学习平台构建模型，并利用潜空间采样生成具有非对称结构的新桥梁类型。

    

    使用去噪扩散隐式模型进行桥梁创新。将图像添加噪声和去噪的过程类比为尸体腐烂和侦探恢复被杀害的受害者现场的过程，以帮助初学者理解。通过易于理解的代数方法，推导出添加噪声和去噪的函数公式，使初学者更容易掌握模型的数学原理。使用三跨梁桥、拱桥、斜拉桥和悬索桥的对称结构图像数据集，基于Python编程语言、TensorFlow和Keras深度学习平台框架，构建和训练了去噪扩散隐式模型。从潜空间采样中，可以生成具有非对称结构的新桥梁类型。去噪扩散隐式模型可以在人类原始桥梁类型的基础上，有机地组合不同的结构组件，创造新的桥梁类型。

    Use denoising diffusion implicit model for bridge-type innovation. The process of adding noise and denoising to an image can be likened to the process of a corpse rotting and a detective restoring the scene of a victim being killed, to help beginners understand. Through an easy-to-understand algebraic method, derive the function formulas for adding noise and denoising, making it easier for beginners to master the mathematical principles of the model. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , denoising diffusion implicit model is constructed and trained. From the latent space sampling, new bridge types with asymmetric structures can be generated. Denoising diffusion implicit model can organically combine different structural components on the basis of human original bridge types, and create new bridge types.
    
[^157]: 观察学习：基于视频的机器人操作学习方法综述

    Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation

    [https://arxiv.org/abs/2402.07127](https://arxiv.org/abs/2402.07127)

    

    

    机器人学习操作技能受到多样化、无偏的数据集的稀缺性的影响。尽管策划的数据集可以帮助解决问题，但在泛化性和现实世界的转移方面仍然存在挑战。与此同时，“野外”视频数据集的大规模存在通过自监督技术推动了计算机视觉的进展。将这一点应用到机器人领域，最近的研究探索了通过被动观察来学习丰富的在线视频中的操作技能。这种基于视频的学习范式显示出了有希望的结果，它提供了可扩展的监督方法，同时降低了数据集的偏见。本综述回顾了视频特征表示学习技术、物体可行性理解、三维手部/身体建模和大规模机器人资源等基础知识，以及从不受控制的视频演示中获取机器人操作技能的新兴技术。我们讨论了仅从观察大规模人类视频中学习如何增强机器人的泛化性和样本效率。

    Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
    
[^158]: 下一代远程眼科诊疗：基于人工智能的质量评估帮助远程基于智能手机的咨询

    Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation

    [https://arxiv.org/abs/2402.07118](https://arxiv.org/abs/2402.07118)

    本研究提出了一种基于人工智能的眼科质量评估系统，可以帮助远程眼科咨询，尤其是在低收入和中等收入国家。该系统能够即时反馈用户拍摄的眼部图像的质量，解决了当前用户拍摄图像质量不佳的问题。

    

    失明和其他眼部疾病是全球关注的健康问题，尤其是在印度等低收入和中等收入国家。在这方面，在COVID-19大流行期间，远程眼科诊疗成为一种生命线，并且智能手机眼部成像的 Grabi 附件得到了广泛使用。然而，用户拍摄的图片质量往往不够好，需要医生审核并且会延误时间。在这种背景下，我们提出了一种基于人工智能的质量评估系统，能够模拟医生的判断并且能够即时反馈，我们对患者拍摄的图像进行了测试。将复杂问题层次化，我们在此解决了一个非常重要的部分，并证明了该概念的可行性。

    Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.
    
[^159]: 对Adam的预条件效应进行量化

    Towards Quantifying the Preconditioning Effect of Adam

    [https://arxiv.org/abs/2402.07114](https://arxiv.org/abs/2402.07114)

    本论文量化了Adam的预条件效应，结果表明Adam能够减轻病态条件的影响，但会受到维度的限制。

    

    本文对Adam的预条件效应进行了详细分析，并量化了Adam在减轻病态条件（困扰梯度下降法）上的作用程度。我们的关键发现是，Adam在病态条件上能够减少依赖于Hessian矩阵条件数的程度，但代价是会受到与维度有关的因素影响。具体来说，对于一个具有对角Hessian矩阵、条件数为κ的d维二次函数，我们证明了在没有动量的Adam中，控制迭代复杂度的有效条件数类似量为O(min(d, κ))。对于一个对角占优的Hessian矩阵，我们获得相应量的上界为O(min(d√(dκ), κ))。因此，当d < O(κ^p)，其中p = 1适用于对角Hessian矩阵时，我们可以得到这种量的界限。

    There is a notable dearth of results characterizing the preconditioning effect of Adam and showing how it may alleviate the curse of ill-conditioning -- an issue plaguing gradient descent (GD). In this work, we perform a detailed analysis of Adam's preconditioning effect for quadratic functions and quantify to what extent Adam can mitigate the dependence on the condition number of the Hessian. Our key finding is that Adam can suffer less from the condition number but at the expense of suffering a dimension-dependent quantity. Specifically, for a $d$-dimensional quadratic with a diagonal Hessian having condition number $\kappa$, we show that the effective condition number-like quantity controlling the iteration complexity of Adam without momentum is $\mathcal{O}(\min(d, \kappa))$. For a diagonally dominant Hessian, we obtain a bound of $\mathcal{O}(\min(d \sqrt{d \kappa}, \kappa))$ for the corresponding quantity. Thus, when $d < \mathcal{O}(\kappa^p)$ where $p = 1$ for a diagonal Hessia
    
[^160]: 解耦学习和决策：用一阶方法突破在线资源分配中的$\mathcal{O}(\sqrt{T})$障碍

    Decoupling Learning and Decision-Making: Breaking the $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods

    [https://arxiv.org/abs/2402.07108](https://arxiv.org/abs/2402.07108)

    本文研究了在线线性规划的问题，并提出了一种新的算法框架，解决了一阶方法在线算法实现超过$\mathcal{O}(\sqrt{T})$遗憾的挑战，实现了$\mathcal{O}(T^{1/3})$的遗憾。

    

    在线线性规划在收益管理和资源分配之间起着重要作用，最近的研究集中在开发有效的一阶在线学习算法。尽管一阶方法在实证上取得了成功，但它们通常只能实现$\mathcal{O}(\sqrt{T})$的遗憾，与最先进的基于线性规划(LP)的在线算法所保证的$\mathcal{O}(\log T)$界限相比是次优的。本文确定了关于在线线性规划的几个重要事实，揭示了一阶方法在线算法实现超过$\mathcal{O}(\sqrt{T})$遗憾的挑战。为了解决这个挑战，我们引入了一个新的算法框架，将学习与决策分离。更重要的是，我们首次展示了一阶方法在这个新框架下可以达到$\mathcal{O}(T^{1/3})$的遗憾。最后，我们进行了数值实验，验证了我们的理论发现。

    Online linear programming plays an important role in both revenue management and resource allocation, and recent research has focused on developing efficient first-order online learning algorithms. Despite the empirical success of first-order methods, they typically achieve a regret no better than $\mathcal{O}(\sqrt{T})$, which is suboptimal compared to the $\mathcal{O}(\log T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based online algorithms. This paper establishes several important facts about online linear programming, which unveils the challenge for first-order-method-based online algorithms to achieve beyond $\mathcal{O}(\sqrt{T})$ regret. To address the challenge, we introduce a new algorithmic framework that decouples learning from decision-making. More importantly, for the first time, we show that first-order methods can attain regret $\mathcal{O}(T^{1/3})$ with this new framework. Lastly, we conduct numerical experiments to validate our theoretical find
    
[^161]: 索crates怀疑的回声：在校准的证据增强学习中接受不确定性

    Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning

    [https://arxiv.org/abs/2402.07107](https://arxiv.org/abs/2402.07107)

    这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。

    

    我们提出了一种新颖的统计方法，用于在基于模型的分布强化学习中引入不确定性意识，涉及基于分位数回归的深度Q网络。提出的算法$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$旨在解决在随机环境中分别估计aleatoric和epistemic不确定性所面临的关键挑战。它将深度证据学习与基于合规推理原则的分位数校准相结合，提供了显式的、无样本计算的$\textit{全局}$不确定性，而不是基于简单方差的$\textit{局部}$估计，克服了传统方法在计算和统计效率以及处理超出分布范围的观测数据方面的局限性。在一套小型化的Atari游戏（即MinAtar）上进行测试，CEQR-DQN在得分和学习速度方面超过了类似的现有框架。它能够严谨地处理外部数据观测，并提供更高的计算和统计效率。

    We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
    
[^162]: 未来预测可以成为部分可观测环境中良好历史表达的有力证据

    Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments

    [https://arxiv.org/abs/2402.07102](https://arxiv.org/abs/2402.07102)

    未来预测在部分可观测环境中学习 History Representation 具有很强的相关性和有效性。

    

    学习良好的历史表达是部分可观测环境中强化学习的核心挑战之一。最近的研究表明，各种辅助任务对促进表达学习具有优势。然而，这些辅助任务的有效性尚未完全使人信服，特别是在需要长期记忆和推理的部分可观测环境中。在这个实证研究中，我们探讨了未来预测在学习部分可观测环境中历史表达时的有效性。我们首先提出了一种通过未来预测将学习历史表达与策略优化分离的方法。然后，我们的主要贡献有两个方面：（a）我们证明了强化学习的性能与部分可观测环境中未来观测的预测精度强相关，（b）我们的方法可以有效地学习部分可观测环境中长时间历史的表达方式。

    Learning a good history representation is one of the core challenges of reinforcement learning (RL) in partially observable environments. Recent works have shown the advantages of various auxiliary tasks for facilitating representation learning. However, the effectiveness of such auxiliary tasks has not been fully convincing, especially in partially observable environments that require long-term memorization and inference. In this empirical study, we investigate the effectiveness of future prediction for learning the representations of histories, possibly of extensive length, in partially observable environments. We first introduce an approach that decouples the task of learning history representations from policy optimization via future prediction. Then, our main contributions are two-fold: (a) we demonstrate that the performance of reinforcement learning is strongly correlated with the prediction accuracy of future observations in partially observable environments, and (b) our approa
    
[^163]: 论随机双层优化问题中一阶方法的复杂性

    On the Complexity of First-Order Methods in Stochastic Bilevel Optimization

    [https://arxiv.org/abs/2402.07101](https://arxiv.org/abs/2402.07101)

    本文研究了在随机双层优化中使用一阶方法的复杂性，提出了一种简单的方法，使用$O(\epsilon^{-6})$次迭代可以收敛到一个$\epsilon$稳定点。

    

    本文考虑了在下层问题无约束且强凸的条件下，寻找双层优化问题中的稳定点。该问题近年来得到了广泛研究；主要的技术挑战是跟踪下层解$y^*(x)$对上层变量$x$的变化。现有方法都将分析结果与一个能够知道下层解的神算法相关联，因此不需要查询离这些解太远的点。我们考虑了一个对这些方法的对偶问题：假设我们有一个被称为$y^*$-感知的预言机，它返回一个$O(\epsilon)$的下层解估计，并且提供在距离$y^*(x)$的$\Theta(\epsilon)$球内处的一阶梯度估计器的局部无偏估计。我们研究了使用这样的$y^*$-感知预言机寻找稳定点的复杂性：我们提出了一种简单的一阶方法，它收敛到一个$\epsilon$稳定点，使用$O(\epsilon^{-6})$的次数是足够的。

    We consider the problem of finding stationary points in Bilevel optimization when the lower-level problem is unconstrained and strongly convex. The problem has been extensively studied in recent years; the main technical challenge is to keep track of lower-level solutions $y^*(x)$ in response to the changes in the upper-level variables $x$. Subsequently, all existing approaches tie their analyses to a genie algorithm that knows lower-level solutions and, therefore, need not query any points far from them. We consider a dual question to such approaches: suppose we have an oracle, which we call $y^*$-aware, that returns an $O(\epsilon)$-estimate of the lower-level solution, in addition to first-order gradient estimators {\it locally unbiased} within the $\Theta(\epsilon)$-ball around $y^*(x)$. We study the complexity of finding stationary points with such an $y^*$-aware oracle: we propose a simple first-order method that converges to an $\epsilon$ stationary point using $O(\epsilon^{-6})
    
[^164]: 重新思考图神经网络在分支策略中的容量

    Rethinking the Capacity of Graph Neural Networks for Branching Strategy

    [https://arxiv.org/abs/2402.07099](https://arxiv.org/abs/2402.07099)

    本文研究了图神经网络（GNNs）在分支策略中的容量，并发现了消息传递GNN (MP-GNN) 的表达能力的局限性以及另一种GNN结构 second-order folklore GNN (2-FGNN) 的通用逼近性质。

    

    图神经网络（GNNs）被广泛应用于预测混合整数线性规划（MILPs）的属性和启发式，并加速MILP求解器。本文研究了GNNs在表示提供分支限界算法中高效策略的强分支（SB）得分方面的能力。尽管现有文献中经常使用最简单的消息传递GNN（MP-GNN）来学习SB得分，但我们证明了其表达能力的一个根本局限性--存在两个不同SB得分的MILP实例，无论参数的数量如何，都无法通过任何MP-GNN区分。此外，我们建立了一个通用逼近定理，用于另一种GNN结构称为second-order folklore GNN（2-FGNN）。我们证明了对于任何MILP数据分布，总是存在一个可以以任意高精度和任意高概率逼近SB得分的2-FGNN。一个小规模的数值实验

    Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm.   Although message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another GNN structure called the second-order folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical 
    
[^165]: 自我纠正自我消耗循环用于生成模型训练

    Self-Correcting Self-Consuming Loops for Generative Model Training

    [https://arxiv.org/abs/2402.07087](https://arxiv.org/abs/2402.07087)

    本论文研究了使用合成数据进行生成模型训练时可能出现的自我消耗循环问题，并提出了一种通过引入理想的修正函数来稳定训练的方法。同时，我们还提出了自我修正函数来近似理想的修正函数，并通过实验证实了其有效性。

    

    随着合成数据在互联网上的质量越来越高以及数量不断增加，机器学习模型越来越多地在人工和机器生成的数据的混合上进行训练。尽管使用合成数据进行表征学习的成功案例有很多，但是在生成模型训练中使用合成数据会产生"自我消耗循环"，这可能导致训练不稳定甚至崩溃，除非满足某些条件。我们的论文旨在稳定自我消耗的生成模型训练。我们的理论结果表明，通过引入一个理想的修正函数，将数据点映射为更有可能来自真实数据分布的样本，可以使自我消耗循环的稳定性呈指数增加。然后，我们提出了自我修正函数，它依赖于专家知识（例如，编程在模拟器中的物理定律），并且旨在自动且大规模地近似理想的修正函数。我们通过实验证实了自我纠正自我消耗循环在生成模型训练中的有效性。

    As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consum
    
[^166]: 基于语音韵律的多说话人语音合成中从音素和音素持续时间中提取说话人嵌入的方法

    Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and Phoneme Duration for Multi-Speaker Speech Synthesis

    [https://arxiv.org/abs/2402.07085](https://arxiv.org/abs/2402.07085)

    本文提出了一种基于语音韵律的方法，通过从音素和音素持续时间中提取说话人嵌入，模拟目标说话人的个体发音特征。实验证明，该方法可以实现有效的多说话人语音合成。

    

    本文提出了一种基于语音韵律的方法，用于从目标说话人的少量句子中建模音素持续时间，从而提取说话人嵌入。语音韵律是与说话人特征相关的重要因素之一，与基频等声学特征一起用于在语音合成中重现单个句子。所提出方法的一个新特点是基于韵律的嵌入，从已知与说话韵律相关的音素及其持续时间中提取，类似于传统的基于频谱特征的说话人识别模型。我们进行了三个实验，包括生成说话人嵌入、使用生成的嵌入进行语音合成以及嵌入空间分析，以评估该方法的性能。结果表明，即使只使用音素及其持续时间信息，所提出的方法也展现了较为适中的说话人识别性能（15.2% EER）。客观和主观评估结果表明，所提出的方法能够实现有效的多说话人语音合成。

    This paper proposes a speech rhythm-based method for speaker embeddings to model phoneme duration using a few utterances by the target speaker. Speech rhythm is one of the essential factors among speaker characteristics, along with acoustic features such as F0, for reproducing individual utterances in speech synthesis. A novel feature of the proposed method is the rhythm-based embeddings extracted from phonemes and their durations, which are known to be related to speaking rhythm. They are extracted with a speaker identification model similar to the conventional spectral feature-based one. We conducted three experiments, speaker embeddings generation, speech synthesis with generated embeddings, and embedding space analysis, to evaluate the performance. The proposed method demonstrated a moderate speaker identification performance (15.2% EER), even with only phonemes and their duration information. The objective and subjective evaluation results demonstrated that the proposed method can
    
[^167]: 基于独立线性函数逼近的马尔科夫博弈的样本复杂度改进

    Refined Sample Complexity for Markov Games with Independent Linear Function Approximation

    [https://arxiv.org/abs/2402.07082](https://arxiv.org/abs/2402.07082)

    本文在独立线性函数逼近的马尔科夫博弈中，通过改进AVLPR框架，提出了基于数据依赖的悲观估计方法，解决了多智能体的诅咒问题。

    

    马尔科夫博弈（MG）是多智能体强化学习（MARL）中的重要模型。长期以来人们一直认为“多智能体的诅咒”（即算法性能随着智能体数量指数级下降）是不可避免的，直到最近几篇作品（Daskalakis等人，2023年；Cui等人，2023年；Wang等人，2023年）。这些作品确实解决了多智能体的诅咒，当状态空间极大且（线性）函数逼近被应用时，它们要么具有更慢的收敛速度$O(T^{-1/4})$，要么在行动数$A_{\max}$上带来多项式依赖——尽管在单智能体情况下即使损失函数可以随时间任意变化（Dai等人，2023年），也可避免这种依赖。本文首先通过Wang等人（2023年）的“AVLPR”框架精化，洞察了基于数据的（即随机的）悲观估计子优化差距，从而允许更广泛的插件算法选择。当专门应用于MGs时，这一方法能够处理独立的情况。

    Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the "curse of multi-agents" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent
    
[^168]: 健康应用中的相关特征与向量机

    The Relevance Feature and Vector Machine for health applications

    [https://arxiv.org/abs/2402.07079](https://arxiv.org/abs/2402.07079)

    本文提出了一种名为RFVM的模型，用于解决临床前瞻性研究中的大数据问题。该模型结合了贝叶斯公式、联合优化和集成修剪等特点，以提高模型的效果和效率。

    

    本文介绍了一种新颖的模型——相关特征与向量机（RFVM），用于解决在临床前瞻性研究中处理大数据问题时面临的挑战。大数据问题是指当机器学习算法与特征数远大于样本数的数据库一起工作时的限制（在某些医学领域中是常见情况）。为了克服这些限制，RFVM融合了以下几个特点：（1）贝叶斯公式，使模型在贝叶斯模型平均的推断下能够避免过拟合问题。（2）联合优化，通过同时引入定义原始空间（特征）和对偶空间（观测）的变量，克服了大数据特征所带来的限制。（3）集成修剪，可以在训练迭代优化期间移除不相关的特征和样本。此外，最后这个修剪方法可以提高模型的效果和效率。

    This paper presents the Relevance Feature and Vector Machine (RFVM), a novel model that addresses the challenges of the fat-data problem when dealing with clinical prospective studies. The fat-data problem refers to the limitations of Machine Learning (ML) algorithms when working with databases in which the number of features is much larger than the number of samples (a common scenario in certain medical fields). To overcome such limitations, the RFVM incorporates different characteristics: (1) A Bayesian formulation which enables the model to infer its parameters without overfitting thanks to the Bayesian model averaging. (2) A joint optimisation that overcomes the limitations arising from the fat-data characteristic by simultaneously including the variables that define the primal space (features) and those that define the dual space (observations). (3) An integrated prunning that removes the irrelevant features and samples during the training iterative optimization. Also, this last p
    
[^169]: 使用大型语言模型自动化和加速奖励机器强化学习

    Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine

    [https://arxiv.org/abs/2402.07069](https://arxiv.org/abs/2402.07069)

    这篇论文介绍了一种使用大型语言模型自动生成自动机来编码高级知识，加速强化学习过程的算法，并证明了其在多个任务上的有效性和优越性。

    

    我们提出了LARL-RM（通过大型语言模型生成的用于奖励机器强化学习的自动机）算法，以将高级知识编码到强化学习中，使用自动机加速强化学习过程。我们的方法使用大型语言模型（LLM）通过提示工程获得高级领域特定知识，而不是直接将高级知识提供给强化学习算法，这需要专家来编码自动机。我们使用思维链和少样本方法进行提示工程，并证明了我们的方法在这些方法下有效。此外，LARL-RM允许完全闭环的强化学习，无需专家来指导和监督学习，因为LARL-RM可以直接使用LLM生成所需的高级知识以完成任务。我们还证明了算法收敛到最优策略的理论保证。我们证明了LARL-RM的实验结果展示了其对常见的强化学习问题具有非常好的性能，并且在一些任务上超越了目前最先进的方法。

    We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-R
    
[^170]: 具有相关输入扰动的差分隐私范围查询

    Differentially Private Range Queries with Correlated Input Perturbation

    [https://arxiv.org/abs/2402.07066](https://arxiv.org/abs/2402.07066)

    本研究提出了一种具有相关输入扰动的差分隐私范围查询的局部机制，通过级联采样算法实现，实验表明在保障近乎最优的效用的同时，与输出扰动方法在实践中具有竞争力。

    

    本工作提出了一种用于线性查询的局部差分隐私机制，特别是范围查询，利用相关输入扰动同时实现无偏性、一致性、统计透明性和对精度目标的控制，无论是在某些查询边缘上还是在层次数据库结构所暗示的精度要求上。所提出的级联采样算法准确高效地实现了该机制。我们的界限表明，我们在保障近乎最优的效用的同时，与输出扰动方法在实践中具有竞争力。

    This work proposes a class of locally differentially private mechanisms for linear queries, in particular range queries, that leverages correlated input perturbation to simultaneously achieve unbiasedness, consistency, statistical transparency, and control over utility requirements in terms of accuracy targets expressed either in certain query margins or as implied by the hierarchical database structure. The proposed Cascade Sampling algorithm instantiates the mechanism exactly and efficiently. Our bounds show that we obtain near-optimal utility while being empirically competitive against output perturbation methods.
    
[^171]: 快速UCB类型算法用于具有重和超重对称噪声的随机赌博机问题

    Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise

    [https://arxiv.org/abs/2402.07062](https://arxiv.org/abs/2402.07062)

    本研究提出了一种基于凸优化方法和不精确预测模型的新UCB类型算法，用于解决具有重和超重对称噪声的随机赌博机问题。通过理论和实验结果表明，在奖励中存在对称噪声的情况下，该算法能够达到更好的性能，相比于一般下界能够获得更小的遗憾界。即使奖励分布没有期望，该算法仍然有效。

    

    在本研究中，我们提出了一种基于一般凸优化方法和不精确的预测模型的UCB类型算法构建方法，并推导了与优化方法收敛速度相对应的遗憾界。我们提出了一种新的算法Clipped-SGD-UCB，并通过理论和经验结果表明，在奖励中存在对称噪声的情况下，可以达到$O(\log T\sqrt{KT\log T})$的遗憾界，而不是$O\left (T^{\frac{1}{1+\alpha}} K^{\frac{\alpha}{1+\alpha}} \right)$，该界是当奖励分布满足$\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$（$\alpha \in (0, 1]$）时的一般下界。此外，即使奖励分布没有期望，即，当$\alpha<0$时，同样的界限也成立。

    In this study, we propose a new method for constructing UCB-type algorithms for stochastic multi-armed bandits based on general convex optimization methods with an inexact oracle. We derive the regret bounds corresponding to the convergence rates of the optimization methods. We propose a new algorithm Clipped-SGD-UCB and show, both theoretically and empirically, that in the case of symmetric noise in the reward, we can achieve an $O(\log T\sqrt{KT\log T})$ regret bound instead of $O\left (T^{\frac{1}{1+\alpha}} K^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution satisfies $\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$ ($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general lower bound for bandits with heavy-tails. Moreover, the same bound holds even when the reward distribution does not have the expectation, that is, when $\alpha<0$.
    
[^172]: 理解通过使用近似损失进行采样的训练加速

    Understanding the Training Speedup from Sampling with Approximate Losses

    [https://arxiv.org/abs/2402.07052](https://arxiv.org/abs/2402.07052)

    本文研究利用近似损失进行样本采样的训练加速方法，通过贪婪策略选择具有大约损失的样本，减少选择的开销，并证明其收敛速度优于随机选择。同时开发了使用中间层表示获取近似损失的SIFT方法，并在训练BERT模型上取得了显著的提升。

    

    众所周知，选择具有较大损失/梯度的样本可以显著减少训练步骤的数量。然而，选择的开销往往过高，无法在总体训练时间方面获得有意义的提升。在本文中，我们专注于选择具有大约损失的样本的贪婪方法，而不是准确损失，以减少选择的开销。对于平滑凸损失，我们证明了这种贪婪策略可以在比随机选择更少的迭代次数内收敛到平均损失的最小值的常数因子。我们还理论上量化了近似水平的影响。然后，我们开发了使用中间层表示获取近似损失以进行样本选择的SIFT。我们评估了SIFT在训练一个具有1.1亿参数的12层BERT基础模型上的任务，并展示了显著的提升（以训练时间和反向传播步骤的数量衡量）。

    It is well known that selecting samples with large losses/gradients can significantly reduce the number of training steps. However, the selection overhead is often too high to yield any meaningful gains in terms of overall training time. In this work, we focus on the greedy approach of selecting samples with large \textit{approximate losses} instead of exact losses in order to reduce the selection overhead. For smooth convex losses, we show that such a greedy strategy can converge to a constant factor of the minimum value of the average loss in fewer iterations than the standard approach of random selection. We also theoretically quantify the effect of the approximation level. We then develop SIFT which uses early exiting to obtain approximate losses with an intermediate layer's representations for sample selection. We evaluate SIFT on the task of training a 110M parameter 12-layer BERT base model and show significant gains (in terms of training hours and number of backpropagation step
    
[^173]: $L^*LM$: 通过自然语言定义示例学习自动机

    $L^*LM$: Learning Automata from Examples using Natural Language Oracles

    [https://arxiv.org/abs/2402.07051](https://arxiv.org/abs/2402.07051)

    该论文提出了一个名为 $L^*LM$ 的算法，通过自然语言和演示学习 DFA，提高了数据效率，具备强大的少样本学习能力。

    

    专家演示已被证明是简化间接指定复杂任务的一种方法。最近的算法甚至支持从演示中提取明确的形式规范，如确定性有限自动机（DFA）。不幸的是，这些技术通常不具备高样本效率。在本文中，我们介绍了一种名为 $L^*LM$ 的算法，用于从演示和自然语言中学习 DFA。由于自然语言的表达能力，我们观察到从专家演示中学习 DFA 的数据效率显著提高。从技术上讲，$L^*LM$ 利用大型语言模型来回答关于底层任务的成员查询。然后将其与最近的演示学习技术相结合，将学习转化为一系列带标签示例学习问题。在我们的实验中，我们观察到这两种模态相互补充，从而产生了一个强大的少样本学习器。

    Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.
    
[^174]: 尾巴的故事：作为尺度律变化的模型崩溃

    A Tale of Tails: Model Collapse as a Change of Scaling Laws

    [https://arxiv.org/abs/2402.07043](https://arxiv.org/abs/2402.07043)

    本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。

    

    随着AI模型大小的增长，神经尺度律已成为预测大模型在扩容和原始（人类或自然）训练数据大小增加时改善的关键工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将逐渐包含越来越多的合成数据。在本文中，我们问：当合成数据进入训练语料库时，尺度律会如何改变？未来的模型仍会改善，还是注定会完全崩溃（模型崩溃）？通过尺度律的视角，我们开发了一个模型崩溃的理论框架。我们发现了广泛的衰减现象，分析了尺度的丧失、与代数的变化尺度、技能的"遗忘"以及混合人类和合成数据时的理解能力。我们的理论通过对一个算术任务和文本生成的转换器进行大规模实验证实。

    As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
    
[^175]: 将符号先验知识融入神经网络的抽象概念学习

    Distilling Symbolic Priors for Concept Learning into Neural Networks

    [https://arxiv.org/abs/2402.07035](https://arxiv.org/abs/2402.07035)

    本论文通过元学习的方法，将符号贝叶斯模型的先验知识提取到神经网络中，使神经网络具有显示归纳偏见的能力，从而加快对抽象概念的学习。

    

    人类可以通过利用归纳偏见从少量的示例中学习新的概念。这些归纳偏见先前已通过在符号假设空间上定义贝叶斯模型来捕捉。是否可能创建一个显示相同归纳偏见的神经网络？我们展示了通过元学习（一种从一组任务中提取共同结构的方法）将符号贝叶斯模型的先验分布提取到人工神经网络中，可以实例化能够快速学习概念的归纳偏见。通过以贝叶斯模型的先验分布生成元学习中使用的任务集，我们能够将该先验传输到神经网络中。我们利用这种方法创建了一个具有对以短逻辑公式表示的概念的归纳偏见的神经网络。通过分析先前人们从少量示例中学习逻辑概念的行为实验结果，我们发现我们的元训练方案可以使神经网络快速学习这些概念。

    Humans can learn new concepts from a small number of examples by drawing on their inductive biases. These inductive biases have previously been captured by using Bayesian models defined over symbolic hypothesis spaces. Is it possible to create a neural network that displays the same inductive biases? We show that inductive biases that enable rapid concept learning can be instantiated in artificial neural networks by distilling a prior distribution from a symbolic Bayesian model via meta-learning, an approach for extracting the common structure from a set of tasks. By generating the set of tasks used in meta-learning from the prior distribution of a Bayesian model, we are able to transfer that prior into a neural network. We use this approach to create a neural network with an inductive bias towards concepts expressed as short logical formulas. Analyzing results from previous behavioral experiments in which people learned logical concepts from a few examples, we find that our meta-train
    
[^176]: Fiddler：用于Mixture-of-Experts模型快速推断的CPU-GPU编排

    Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models

    [https://arxiv.org/abs/2402.07033](https://arxiv.org/abs/2402.07033)

    本文介绍了Fiddler，一种用于Mixture-of-Experts模型的资源高效推断引擎，通过CPU-GPU编排实现最小化数据传输，相比现有方法提高了一个数量级的推断速度。

    

    基于Mixture-of-Experts（MoE）架构的大型语言模型（LLM）在各种任务上表现出了很好的性能。然而，在资源受限的环境下运行这些模型，即GPU内存资源不丰富的情况下，由于模型规模庞大，存在挑战。现有的将模型权重卸载到CPU内存的系统，由于频繁地在CPU和GPU之间移动数据而导致显著的开销。在本文中，我们提出了Fiddler，一种用于MoE模型的资源高效推断引擎，实现了CPU-GPU编排。Fiddler的核心思想是利用CPU的计算能力来最小化CPU和GPU之间的数据传输。我们的评估结果表明，Fiddler能够在单个具有24GB内存的GPU上运行未压缩的Mixtral-8x7B模型（参数超过90GB），每秒生成超过3个token，相比现有方法提高一个数量级。Fiddler的代码可以公开访问，网址为\url{https://github.com/efeslab/fiddler}

    Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
    
[^177]: 实例级别的安全感知与合成数据质量及其校准

    Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration

    [https://arxiv.org/abs/2402.07031](https://arxiv.org/abs/2402.07031)

    本论文研究了实例级别的合成数据质量与安全感知，引入了四种超越纯视觉特征的合成数据质量，并提出了优化方法来减少合成和真实图像之间的质量差距。

    

    建模和校准合成数据的质量对塑造未来安全可靠的自动驾驶技术至关重要，它提供了一种成本效益高且可扩展的替代方案，可以取代真实世界的数据收集。我们关注其在安全关键应用中的作用，引入了超越纯视觉输入特征的四种实例级别质量，旨在使合成数据与现实世界的安全问题相一致。我们提出了一种优化方法来改进合成数据生成器，减少由基于DNN的组件识别出的质量差距。我们的研究结果表明，这种调优可以增强合成和真实图像中安全关键错误之间的相关性。

    Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection. We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics. The aim is to align synthetic data with real-world safety issues. We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component. Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images.
    
[^178]: 半监督学习用于双语词典诱导

    Semi-Supervised Learning for Bilingual Lexicon Induction

    [https://arxiv.org/abs/2402.07028](https://arxiv.org/abs/2402.07028)

    本论文提出了一个半监督学习方法，将两种语言对应的连续词表示集对齐到一个共同的空间，推断双语词典。该方法利用无监督学习的基础，在学习新语言时，整合已有语言集的知识，通过排序方法实现词典诱导。

    

    我们考虑将对应于不同语言的两个连续词表示集对齐到一个共同空间，以推断双语词典的问题。最近的研究表明，通过将在单语数据上训练的词嵌入对齐，可以推断出这样的词典而不使用任何平行数据。这种工作称为无监督双语诱导。通过思考是否可能在逐步学习多种语言的过程中积累经验，我们自问在学习新语言时是否能够在没有平行数据的情况下整合给定语言集的知识。换句话说，虽然保持无监督学习的核心问题在最新步骤中，但我们允许访问其他习语语料库，因此称为半监督。这导致我们提出了一种新的表达形式，将词典诱导视为一个排序问题，我们使用了该机器学习领域的最新工具。

    We consider the problem of aligning two sets of continuous word representations, corresponding to languages, to a common space in order to infer a bilingual lexicon. It was recently shown that it is possible to infer such lexicon, without using any parallel data, by aligning word embeddings trained on monolingual data. Such line of work is called unsupervised bilingual induction. By wondering whether it was possible to gain experience in the progressive learning of several languages, we asked ourselves to what extent we could integrate the knowledge of a given set of languages when learning a new one, without having parallel data for the latter. In other words, while keeping the core problem of unsupervised learning in the latest step, we allowed the access to other corpora of idioms, hence the name semi-supervised. This led us to propose a novel formulation, considering the lexicon induction as a ranking problem for which we used recent tools of this machine learning field. Our experi
    
[^179]: 量子加速下的Kronecker积的谱逼近

    Quantum Speedup for Spectral Approximation of Kronecker Products

    [https://arxiv.org/abs/2402.07027](https://arxiv.org/abs/2402.07027)

    本研究引入一种创新方法，使用量子方法高效地解决Kronecker积$A_1 \otimes A_2$的谱逼近问题，将时间复杂度降低到$O_{d,\epsilon}(\sqrt{n})$。

    

    考虑到Kronecker积在机器学习和优化中的广泛应用，它成为了一个至关重要的线性代数运算符。然而，由于其计算要求高，通过传统计算算法进行Kronecker积的谱逼近的成本也相应增加。现有的经典方法对于谱逼近表现出与矩阵维度$n$线性依赖的特点，其中$A_1 \in \mathbb{R}^{n \times d}$，$A_2 \in \mathbb{R}^{n \times d}$。我们的工作引入了一种创新的方法，使用量子方法高效地解决Kronecker积$A_1 \otimes A_2$的谱逼近问题。通过将矩阵视为量子态，我们提出的方法将谱逼近的时间复杂度大幅降低到$O_{d,\epsilon}(\sqrt{n})$。

    Given its widespread application in machine learning and optimization, the Kronecker product emerges as a pivotal linear algebra operator. However, its computational demands render it an expensive operation, leading to heightened costs in spectral approximation of it through traditional computation algorithms. Existing classical methods for spectral approximation exhibit a linear dependency on the matrix dimension denoted by $n$, considering matrices of size $A_1 \in \mathbb{R}^{n \times d}$ and $A_2 \in \mathbb{R}^{n \times d}$. Our work introduces an innovative approach to efficiently address the spectral approximation of the Kronecker product $A_1 \otimes A_2$ using quantum methods. By treating matrices as quantum states, our proposed method significantly reduces the time complexity of spectral approximation to $O_{d,\epsilon}(\sqrt{n})$.
    
[^180]: 均场极限下图神经网络的泛化误差

    Generalization Error of Graph Neural Networks in the Mean-field Regime

    [https://arxiv.org/abs/2402.07025](https://arxiv.org/abs/2402.07025)

    该论文在均场极限下提供了一个理论框架，评估了图神经网络在过参数化情况下的泛化误差，通过推导出收敛速度为$O(1/n)$的上界，为我们对网络在未见数据上的性能提供了理论保证。

    

    该工作提供了一个理论框架，用于评估在过参数化的情况下通过图神经网络进行图分类任务的泛化误差，即参数数量超过数据点数量的情况。我们研究了两种广泛使用的图神经网络类型：图卷积神经网络和消息传递图神经网络。在本研究之前，关于过参数化情况下泛化误差的现有界限缺乏信息，限制了我们对过参数化网络性能的理解。我们的创新方法是在均场极限下推导出上界，以评估这些图神经网络的泛化误差。我们建立了以$O(1/n)$收敛速度的上界，其中$n$是图样本的数量。这些上界为在具有挑战性的过参数化情况下网络在未见数据上的性能提供了理论上的保证，从而对我们的理解做出了贡献。

    This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our under
    
[^181]: 双子座进入医学院：探索多模态大型语言模型在医学挑战问题和幻觉上的能力

    Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations

    [https://arxiv.org/abs/2402.07023](https://arxiv.org/abs/2402.07023)

    该论文综合评估了开源LLM和谷歌的多模态LLM Gemini 在医学推理、幻觉检测和医学视觉问答任务上的能力。Gemini在诊断准确性方面落后于最先进模型，且易出现幻觉、过度自信和知识盲点。采用提示策略可以提高性能。

    

    大型语言模型在医疗行业具有潜在价值，但通过严格评估来验证其安全性和效果至关重要。为此，我们全面评估了开源LLM和谷歌的新型多模态LLM Gemini 在医学推理、幻觉检测和医学视觉问答任务上的能力。虽然Gemini表现出一定的能力，但在诊断准确性方面落后于MedPaLM 2和GPT-4等最先进模型。此外，Gemini在医学VQA数据集上的准确率为61.45％，明显低于GPT-4V的88％得分。我们的分析发现，Gemini极易出现幻觉、过度自信和知识盲点，这表明如果不加批判地部署，存在风险。我们还针对不同医学学科和测试类型进行了详细分析，为开发人员和临床医生提供了可操作的反馈。为了减少风险，我们采用了提示策略来提高性能。

    Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performanc
    
[^182]: 强化学习中奖励函数的信息性

    Informativeness of Reward Functions in Reinforcement Learning

    [https://arxiv.org/abs/2402.07019](https://arxiv.org/abs/2402.07019)

    本文研究了在强化学习中设计信息丰富的奖励函数的问题，提出了一种新的奖励信息性准则来评估代理的当前策略在接受特定奖励函数的奖励后的改善情况。

    

    奖励函数在指定我们期望强化学习代理执行的任务中起着关键作用。在给定任务和期望的最优行为的情况下，我们研究了如何设计信息丰富的奖励函数，以加快代理的收敛速度。特别是，我们考虑了专家驱动的奖励设计设置，其中专家或教师希望为学习代理提供信息丰富且可解释的奖励。已有的研究考虑了多种不同的奖励设计形式，然而关键挑战是制定一个奖励信息性准则，该准则能够根据代理的当前策略进行调整，并且可以在指定的结构约束下进行优化，以获得可解释的奖励。在本文中，我们提出了一种新的奖励信息性准则，一种定量度量，用于捕捉代理的当前策略如果从特定的奖励函数中获得奖励将如何改善。我们在理论上展示了所提出的信息性准则的实用性。

    Reward functions are central in specifying the task we want a reinforcement learning agent to perform. Given a task and desired optimal behavior, we study the problem of designing informative reward functions so that the designed rewards speed up the agent's convergence. In particular, we consider expert-driven reward design settings where an expert or teacher seeks to provide informative and interpretable rewards to a learning agent. Existing works have considered several different reward design formulations; however, the key challenge is formulating a reward informativeness criterion that adapts w.r.t. the agent's current policy and can be optimized under specified structural constraints to obtain interpretable rewards. In this paper, we propose a novel reward informativeness criterion, a quantitative measure that captures how the agent's current policy will improve if it receives rewards from a specific reward function. We theoretically showcase the utility of the proposed informati
    
[^183]: FedImpro: 测量和改善联邦学习中的客户更新

    FedImpro: Measuring and Improving Client Update in Federated Learning

    [https://arxiv.org/abs/2402.07011](https://arxiv.org/abs/2402.07011)

    本文提出了FedImpro方法，通过生成改进的本地模型来减轻联邦学习中的客户漂移问题。该方法通过分析本地训练的泛化贡献，并利用类似的条件分布进行训练，增强了泛化贡献并减小了梯度的差异性。

    

    联邦学习模型通常会受到异构数据引起的客户漂移的影响，其中数据的分布在不同的客户之间存在差异。为了解决这个问题，先进的研究主要关注于操作现有的梯度，以实现更一致的客户模型。在本文中，我们从另一个角度分析了客户漂移，并旨在通过生成改进的本地模型来减轻这种漂移。首先，我们分析了本地训练的泛化贡献，并得出结论，这种泛化贡献受到不同客户的数据分布之间的条件Wasserstein距离的限制。然后，我们提出了FedImpro，用于构建类似的条件分布进行本地训练。具体而言，FedImpro将模型分解为高层和低层组件，并对重构特征分布上的高层部分进行训练。这种方法增强了泛化贡献，并减小了联邦学习中梯度的差异性。

    Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper
    
[^184]: 用于处理和迁移学习的脑肿瘤分割的优化框架

    An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation

    [https://arxiv.org/abs/2402.07008](https://arxiv.org/abs/2402.07008)

    使用优化框架和迁移学习方法，作者构建了一个基于3D U-Net模型的脑肿瘤分割方法，通过应用各种预处理和后处理技术，在三个挑战上实现了较高的Dice分数。

    

    多模态脑MRI图像中的肿瘤分割是一项具有挑战性的任务，因为样本有限、形状变化大且肿瘤形态分布不均匀。近年来深度学习的进展在自动化医学图像分割的性能方面取得了显著的提高。然而，模型的预测结果在准确性和泛化性方面尚未达到临床使用的理想水平。为了解决BraTS 2023的挑战中1、2和3所提出的问题，我们构建了一个基于3D U-Net模型的脑肿瘤分割的优化框架。该框架包括各种预处理和后处理技术以及迁移学习。在验证数据集上，这个多模态脑肿瘤分割框架分别在Challenge 1、2、3上达到平均病变级别Dice分数0.79、0.72、0.74。

    Tumor segmentation from multi-modal brain MRI images is a challenging task due to the limited samples, high variance in shapes and uneven distribution of tumor morphology. The performance of automated medical image segmentation has been significant improvement by the recent advances in deep learning. However, the model predictions have not yet reached the desired level for clinical use in terms of accuracy and generalizability. In order to address the distinct problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed an optimization framework based on a 3D U-Net model for brain tumor segmentation. This framework incorporates a range of techniques, including various pre-processing and post-processing techniques, and transfer learning. On the validation datasets, this multi-modality brain tumor segmentation framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on Challenges 1, 2, 3 respectively.
    
[^185]: 客户端协作：具有保证隐私-效用权衡改进的灵活差分隐私联邦学习

    Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off

    [https://arxiv.org/abs/2402.07002](https://arxiv.org/abs/2402.07002)

    本论文提出了一种名为FedCEO的新型联邦学习框架，在保护用户隐私的同时，通过让客户端相互协作，实现了对模型效用和隐私之间的权衡。通过高效的张量低秩近端优化，该框架能够恢复被打断的语义信息，并在效用-隐私权衡方面取得了显著的改进。

    

    为了防止用户数据的隐私泄漏，在联邦学习中广泛使用差分隐私，但它并不是免费的。噪声的添加会随机干扰模型的语义完整性，并且这种干扰会随着通信轮次的增加而累积。在本文中，我们引入了一种具有严格隐私保证的新型联邦学习框架，名为FedCEO，通过让客户端"相互协作"，旨在在模型效用和用户隐私之间找到一种权衡。具体而言，我们在服务器上对堆叠的本地模型参数进行了高效的张量低秩近端优化，展示了它在光谱空间中灵活截断高频组分的能力。这意味着我们的FedCEO能够通过平滑全局语义空间来有效恢复被打断的语义信息，以适应不同隐私设置和持续的训练过程。此外，我们将SOTA的效用-隐私权衡边界提高了一个数量级。

    To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqr
    
[^186]: 一个变化检测的现实验证

    A Change Detection Reality Check

    [https://arxiv.org/abs/2402.06994](https://arxiv.org/abs/2402.06994)

    该论文通过实验证明，一个简单的U-Net分割基线仍然是进行变化检测任务的顶尖表现者。

    

    在近年来的遥感文献中，出现了大量提出的用于变化检测的深度学习架构。这些方法声称在不同的标准基准数据集上提供了最先进的性能。然而，该领域是否真正取得了重大进展？在本文中，我们进行了实验证明，一个简单的U-Net分割基线，没有训练技巧或复杂的架构改变，仍然是进行变化检测任务的顶尖表现者。

    In recent years, there has been an explosion of proposed change detection deep learning architectures in the remote sensing literature. These approaches claim to offer state-of the-art performance on different standard benchmark datasets. However, has the field truly made significant progress? In this paper we perform experiments which conclude a simple U-Net segmentation baseline without training tricks or complicated architectural changes is still a top performer for the task of change detection.
    
[^187]: 通过搜索梯度引导的基于草图的程序归纳

    Guided Sketch-Based Program Induction by Search Gradients

    [https://arxiv.org/abs/2402.06990](https://arxiv.org/abs/2402.06990)

    提出了一种通过搜索梯度使用进化策略学习参数化程序的框架，该框架使程序归纳成为解决多种类型任务的可行解决方案。

    

    许多任务可以通过机器学习技术轻松解决。然而，一些任务不能通过统计模型轻松解决，而需要符号化的方法。程序归纳是通过训练来捕捉可解释且可推广的算法来解决此类任务的方法之一。然而，当前的程序归纳方法还不够复杂，不能轻松应用于各种类型的任务，因为它们往往被制定为单一的、包含所有的模型，通常由神经网络进行参数化。为了使程序归纳成为许多场景的可行解决方案，我们提出了一种通过搜索梯度使用进化策略学习参数化程序的框架。这个新的方法与传统的程序归纳有所不同，它允许程序员将任务特定的代码输入到程序的“草图”中，并通过端到端的梯度优化来加速学习的过程。

    Many tasks can be easily solved using machine learning techniques. However, some tasks cannot readily be solved using statistical models, requiring a symbolic approach instead. Program induction is one of the ways that such tasks can be solved by means of capturing an interpretable and generalizable algorithm through training. However, contemporary approaches to program induction are not sophisticated enough to readily be applied to various types of tasks as they tend to be formulated as a single, all-encompassing model, usually parameterized by neural networks. In an attempt to make program induction a viable solution for many scenarios, we propose a framework for learning parameterized programs via search gradients using evolution strategies. This formulation departs from traditional program induction as it allows for the programmer to impart task-specific code to the program 'sketch', while also enjoying the benefits of accelerated learning through end-to-end gradient-based optimiza
    
[^188]: 非线性融合在联邦学习中的应用：基于超网络的联邦领域泛化方法

    Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization

    [https://arxiv.org/abs/2402.06974](https://arxiv.org/abs/2402.06974)

    本文提出了一种基于超网络的联邦融合算法hFedF，用于解决联邦领域泛化问题。该算法通过非线性融合客户模型，实现了对底层数据分布的全面理解，并在联邦学习中个性化和泛化之间达到了优秀的平衡。

    

    联邦学习（FL）作为一种保护数据隐私的多个客户共同训练共享全局模型的有前途的范式出现。为了创建一个稳健和实用的FL框架，扩展其良好泛化能力以适应未知领域是至关重要的，这个问题被称为联邦领域泛化（FDG），目前尚未得到充分探索。我们提出了一种创新的联邦算法，称为hFedF（基于超网络的联邦融合），旨在弥合个性化和泛化之间的性能差距，能够处理各种程度的领域转移。基本上，超网络支持对客户模型进行非线性融合，从而全面了解底层数据分布。我们对个性化和泛化之间的权衡进行了广泛的讨论，并提供了对FL中强大基准测试的新见解。所提出的算法在三个广泛使用的DG数据集上优于强大的基准测试。

    Federated Learning (FL) has emerged as a promising paradigm in which multiple clients collaboratively train a shared global model while preserving data privacy. To create a robust and practicable FL framework, it is crucial to extend its ability to generalize well to unseen domains - a problem referred to as federated Domain Generalization (FDG), being still under-explored. We propose an innovative federated algorithm, termed hFedF for hypernetwork-based Federated Fusion, designed to bridge the performance gap between generalization and personalization, capable of addressing various degrees of domain shift. Essentially, the hypernetwork supports a non-linear fusion of client models enabling a comprehensive understanding of the underlying data distribution. We encompass an extensive discussion and provide novel insights into the tradeoff between personalization and generalization in FL. The proposed algorithm outperforms strong benchmarks on three widely-used data sets for DG in an exce
    
[^189]: 事件关键摘要

    Event-Keyed Summarization

    [https://arxiv.org/abs/2402.06973](https://arxiv.org/abs/2402.06973)

    事件关键摘要（EKS）是一种新颖的任务，旨在为特定事件生成上下文化的摘要。我们提出了一个基准数据集MUCSUM，并展示了EKS与传统摘要和结构到文本的比较结果。

    

    我们介绍了一种新颖的任务，称为事件关键摘要（EKS），它将传统的摘要和文档级事件提取结合起来，目标是在给定文档和提取的事件结构的情况下生成一个上下文化的特定事件摘要。我们介绍了一个用于这个任务的数据集MUCSUM，包括经典MUC-4数据集中所有事件的摘要，以及一组基线模型，其中包括在摘要文献中预训练的语言模型标准以及更大的前沿模型。我们表明，将EKS简化为传统的摘要或结构到文本的去除都会得到较差的目标事件摘要，并且MUCSUM是这一任务的一个稳健的基准。最后，我们对参考摘要和模型摘要进行了人工评估，并对结果进行了详细分析。

    We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.
    
[^190]: 使用TabPFN进行上下文数据蒸馏

    In-Context Data Distillation with TabPFN

    [https://arxiv.org/abs/2402.06971](https://arxiv.org/abs/2402.06971)

    TabPFN是一种适用于tabular数据的Transformer模型，具有卓越的上下文学习能力。为了解决TabPFN在实际场景中的数据规模限制问题，我们提出了上下文数据蒸馏(ICD)方法，通过优化TabPFN的上下文，使其能够处理更大规模的数据集，提高性能。

    

    基于tabular数据，基于树的模型（如XGBoost）在该领域仍然占据主导地位。TabPFN是一种专门为tabular数据设计的Transformer模型，其卓越的上下文学习能力与XGBoost相媲美，无需特定任务训练或超参数调整。然而，TabPFN的适用性受到数据规模限制的影响，在实际场景中使用受到限制。为了解决这个问题，我们提出了一种新颖的方法，即上下文数据蒸馏(ICD)，有效地消除了这些限制，通过优化TabPFN的上下文。ICD使得TabPFN能够在有限内存预算下处理更大规模的数据集，提高TabPFN的二次内存复杂度，但代价是线性数量的调优步骤。值得注意的是，经过ICD增强的TabPFN展现出非常强大的性能。

    Foundation models have revolutionized tasks in computer vision and natural language processing. However, in the realm of tabular data, tree-based models like XGBoost continue to dominate. TabPFN, a transformer model tailored for tabular data, mirrors recent foundation models in its exceptional in-context learning capability, being competitive with XGBoost's performance without the need for task-specific training or hyperparameter tuning. Despite its promise, TabPFN's applicability is hindered by its data size constraint, limiting its use in real-world scenarios. To address this, we present in-context data distillation (ICD), a novel methodology that effectively eliminates these constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to handle significantly larger datasets with a fixed memory budget, improving TabPFN's quadratic memory complexity but at the cost of a linear number of tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong performanc
    
[^191]: 具有时间窗口的情境随机车辆路径规划

    Contextual Stochastic Vehicle Routing with Time Windows

    [https://arxiv.org/abs/2402.06968](https://arxiv.org/abs/2402.06968)

    这项研究探讨了具有时间窗口和随机旅行时间的车辆路径规划问题，并引入了上下文情境，通过使用历史数据提供近似解来最小化总运输成本和预期迟到罚款。

    

    我们研究了具有时间窗口和随机旅行时间的车辆路径规划问题（VRPTW），在该问题中，决策者在做出路径决策之前观察到相关的情境信息，这些信息用特征变量表示。尽管关于随机VRP的文献已经很广泛，但在这个背景下，特征变量的整合得到的关注却很有限。我们引入了具有上下文情境的随机VRPTW问题，该问题在观察到的特征条件下最小化总运输成本和预期迟到罚款。由于旅行时间和特征的联合分布是未知的，我们提出了使用历史数据提供问题近似解的新型数据驱动规划模型。我们区分了基于点估计、样本平均估计和惩罚估计的规划模型，每种模型对待随机旅行时间和特征的方式有不同的观点。我们开发了专门的分枝定价割模型

    We study the vehicle routing problem with time windows (VRPTW) and stochastic travel times, in which the decision-maker observes related contextual information, represented as feature variables, before making routing decisions. Despite the extensive literature on stochastic VRPs, the integration of feature variables has received limited attention in this context. We introduce the contextual stochastic VRPTW, which minimizes the total transportation cost and expected late arrival penalties conditioned on the observed features. Since the joint distribution of travel times and features is unknown, we present novel data-driven prescriptive models that use historical data to provide an approximate solution to the problem. We distinguish the prescriptive models between point-based approximation, sample average approximation, and penalty-based approximation, each taking a different perspective on dealing with stochastic travel times and features. We develop specialized branch-price-and-cut al
    
[^192]: DeepCover: 提高RNN测试覆盖率和在线错误预测的方法，使用状态机提取

    DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction

    [https://arxiv.org/abs/2402.06966](https://arxiv.org/abs/2402.06966)

    提出了一种从循环神经网络模型中提取状态机的方法，提高了RNN模型的可解释性，并通过提取的状态机来改进RNN的测试和监测。

    

    循环神经网络（RNN）已经成为处理序列数据的强大工具，在自然语言处理和语音识别等领域具有广泛应用。然而，RNN模型的缺乏可解释性限制了其可解释性，并带来了理解其内部工作过程的挑战。为了解决这个问题，本文提出了一种从基于RNN的模型中提取状态机（SM）的方法，以洞察其内部功能。所提出的SM提取算法通过四个新提出的度量标准（纯度、丰富度、优秀度和规模）进行评估。所提出的方法及其评估度量标准通过提取的SM在RNN模型中提供了明确的内部决策过程的表示，从而增加了RNN模型的可解释性。除了改善RNN的可解释性，所提取的SM还可以用于改进基于RNN模型的测试与监测。

    Recurrent neural networks (RNNs) have emerged as powerful tools for processing sequential data in various fields, including natural language processing and speech recognition. However, the lack of explainability in RNN models has limited their interpretability, posing challenges in understanding their internal workings. To address this issue, this paper proposes a methodology for extracting a state machine (SM) from an RNN-based model to provide insights into its internal function. The proposed SM extraction algorithm was assessed using four newly proposed metrics: Purity, Richness, Goodness, and Scale. The proposed methodology along with its assessment metrics contribute to increasing explainability in RNN models by providing a clear representation of their internal decision making process through the extracted SM. In addition to improving the explainability of RNNs, the extracted SM can be used to advance testing and and monitoring of the primary RNN-based model. To enhance RNN testi
    
[^193]: 基于树集成的情境多臂老虎机

    Tree Ensembles for Contextual Bandits

    [https://arxiv.org/abs/2402.06963](https://arxiv.org/abs/2402.06963)

    本论文提出了一种基于树集成的情境多臂老虎机新框架，通过整合两种广泛使用的老虎机方法，在标准和组合设置中实现了优于基于神经网络的方法的性能，在减少后悔和计算时间方面表现出更出色的性能。

    

    我们提出了一个基于树集成的情境多臂老虎机的新框架。我们的框架将两种广泛使用的老虎机方法，上信心界和汤普森抽样，整合到标准和组合设置中。通过使用流行的树集成方法XGBoost进行多次实验研究，我们展示了我们框架的有效性。当应用于基准数据集和道路网络导航的真实世界应用时，与基于神经网络的最先进方法相比，我们的方法在减少后悔和计算时间方面表现出更好的性能。

    We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.
    
[^194]: 从第一原理构建的架构神经后门

    Architectural Neural Backdoors from First Principles

    [https://arxiv.org/abs/2402.06957](https://arxiv.org/abs/2402.06957)

    本论文提出了从第一原理构建架构神经后门的方法，并描述了12种不同类型的架构后门。同时，通过构建一个任意触发检测器，展示了无需人工监督即可为架构引入后门的能力。

    

    尽管之前的研究通过改变神经网络的参数来创建后门，但最近的研究揭示了一种更隐蔽的威胁：在网络架构定义中嵌入的后门。这涉及到注入常见的架构组件，如激活函数和池化层，以巧妙地引入一个持续存在的后门行为，即使在重新训练后也是如此。然而，架构后门的全部范围和影响仍然很少被探索。Bober-Irizar等人[2023]首次引入了架构后门；他们展示了如何为棋盘图案创建后门，但从未解释如何针对任意触发模式进行定位。在这项工作中，我们构建了一个可用于无人监督地为架构引入后门的任意触发检测器。这使我们重新审视了架构后门的概念并将其进行分类，描述了12种不同类型。为了评估检测此类后门的困难程度，...

    While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture. This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training. However, the full scope and implications of architectural backdoors have remained largely unexplored. Bober-Irizar et al. [2023] introduced the first architectural backdoor; they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice. In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision. This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types. To gauge the difficulty of detecting such backdoors, 
    
[^195]: 使用特征映射的物理引导神经网络中的训练动态

    Training dynamics in Physics-Informed Neural Networks with feature mapping

    [https://arxiv.org/abs/2402.06955](https://arxiv.org/abs/2402.06955)

    本研究探究了使用特征映射层的物理引导神经网络（PINNs）的训练动态，通过极限共轭核和神经切向核揭示了模型的收敛和泛化。我们提出了一种替代基于傅里叶变换的特征映射的条件正定径向基函数，证明了该方法在各种问题集中的有效性，并可以轻松实现在坐标输入网络中。这为广泛的PINNs研究带来了益处。

    

    物理引导神经网络（PINNs）已成为解决偏微分方程（PDE）的标志性机器学习方法。尽管其变体取得了显著进展，但来自更广泛的隐式神经表示研究的特征映射的经验性成功在很大程度上被忽视。我们通过极限共轭核和神经切向核来研究带有特征映射层的PINNs的训练动态，从而揭示了模型的收敛和泛化。我们还展示了常用的基于傅里叶变换的特征映射在某些情况下的不足，并提出条件正定的径向基函数作为更好的替代方法。经验证实，我们的方法在各种正向和反向问题集中非常有效。这种简单的技术可以轻松在坐标输入网络中实现，并受益于广泛的PINNs研究。

    Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.
    
[^196]: OpenFedLLM：通过联邦学习在分散的私有数据上训练大规模语言模型

    OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning

    [https://arxiv.org/abs/2402.06954](https://arxiv.org/abs/2402.06954)

    OpenFedLLM是一个简洁、集成、研究友好的框架/代码库，通过联邦学习在分散的私有数据上实现了大规模语言模型的协作和隐私保护训练，解决了公开数据枯竭的问题。

    

    在大规模公开可用的数据上训练的大规模语言模型（LLM）在各个领域取得了巨大的成功。然而，更多的数据可以提高性能，但令人担忧的是，高质量的公开数据将在几年内用尽。在本文中，我们提供了对当代LLM的潜在下一步：通过联邦学习在未充分利用的分布式私有数据上进行协作和保护隐私的LLM训练，多个数据所有者共同训练一个共享模型，而不传输原始数据。为了实现这一目标，我们构建了一个简洁、集成和研究友好的框架/代码库，名为OpenFedLLM。它涵盖了用于增强模型遵循指令能力的联邦指令调优、用于与人类价值观对齐的联邦价值对齐以及7个代表性联邦学习算法。此外，OpenFedLLM支持在多领域进行训练，我们涵盖了8个训练数据集；提供全面的评估，我们涵盖了...

    Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cov
    
[^197]: 使用加权虚拟观测实现高效的增量信念更新

    Efficient Incremental Belief Updates Using Weighted Virtual Observations

    [https://arxiv.org/abs/2402.06940](https://arxiv.org/abs/2402.06940)

    本文介绍了在贝叶斯统计模型中使用加权虚拟观测进行增量信念更新的算法解决方案，该方案通过构建一组加权观测来调节模型，实现与原始后验相同的推断结果。

    

    我们提出了一个算法解决了在贝叶斯统计模型中蒙特卡洛推断环境下的增量信念更新问题，该模型由概率编程表示。给定一个模型和样本逼近的后验概率，我们的解决方案构建了一组加权观测来调节模型，从而推断结果与原始后验相同。该问题出现在多层建模、增量推断和数据隐私约束下的推断等情况。首先，选择一组虚拟观测值，然后通过高效的计算优化过程找到观测权重，使得重建的后验与原始后验一致或近似。我们对一些教学示例和案例研究实施并应用了该解决方案，展示了我们方法的效率和鲁棒性。所提供的参考实现不依赖于概率编程语言或推断算法。

    We present an algorithmic solution to the problem of incremental belief updating in the context of Monte Carlo inference in Bayesian statistical models represented by probabilistic programs. Given a model and a sample-approximated posterior, our solution constructs a set of weighted observations to condition the model such that inference would result in the same posterior. This problem arises e.g. in multi-level modelling, incremental inference, inference in presence of privacy constraints. First, a set of virtual observations is selected, then, observation weights are found through a computationally efficient optimization procedure such that the reconstructed posterior coincides with or closely approximates the original posterior. We implement and apply the solution to a number of didactic examples and case studies, showing efficiency and robustness of our approach. The provided reference implementation is agnostic to the probabilistic programming language or the inference algorithm, 
    
[^198]: 使用谈判能力的分布式基础设施高效资源调度

    Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities

    [https://arxiv.org/abs/2402.06938](https://arxiv.org/abs/2402.06938)

    这项研究介绍了一种使用谈判能力的分布式基础设施高效资源调度的方法，通过引入一种基于模糊逻辑的代理自动谈判系统，优化云计算提供商和客户之间的协议，以提高效率。

    

    在过去的几十年里，信息和互联网技术的快速发展催生了大量的数据和信息。信息爆炸推动许多企业或个人寻求租用云计算基础设施来将他们的应用程序放置在云中。然而，云计算提供商和客户之间达成的协议通常不高效。许多因素影响效率，如提供商云计算基础设施的闲置和对客户的额外成本。一个可能的解决方案是引入一种综合的、谈判类的博弈，并根据谈判结果安排资源。我们提出了一种基于模糊逻辑的基于代理的自动谈判系统用于资源调度。所提出的方法可以完成一对一的自动谈判过程，并为提供商和客户生成最优的报价。我们比较了不同成员函数、模糊规则集和网络拓扑结构对资源调度的影响。

    In the past few decades, the rapid development of information and internet technologies has spawned massive amounts of data and information. The information explosion drives many enterprises or individuals to seek to rent cloud computing infrastructure to put their applications in the cloud. However, the agreements reached between cloud computing providers and clients are often not efficient. Many factors affect the efficiency, such as the idleness of the providers' cloud computing infrastructure, and the additional cost to the clients. One possible solution is to introduce a comprehensive, bargaining game (a type of negotiation), and schedule resources according to the negotiation results. We propose an agent-based auto-negotiation system for resource scheduling based on fuzzy logic. The proposed method can complete a one-to-one auto-negotiation process and generate optimal offers for the provider and client. We compare the impact of different member functions, fuzzy rule sets, and ne
    
[^199]: 在分布迁移下评估3D图像分割的不确定性估计方法

    Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts

    [https://arxiv.org/abs/2402.06937](https://arxiv.org/abs/2402.06937)

    本研究探讨了在分布迁移情况下使用贝叶斯和非贝叶斯方法进行不确定性估计的可行性，旨在实现在医学图像分割任务中可靠和可信的诊断预测。

    

    近年来，机器学习在各个领域得到了广泛的应用，然而在医学影像为基础的疾病检测和诊断方面的应用仍然具有挑战性，这主要是由于现实世界数据中存在分布迁移。在实际环境中，部署的模型会遇到与训练数据集明显不同的样本，尤其是在医疗领域，这可能导致性能问题。这个限制影响了深度学习模型在健康应用中的表现力和可靠性。因此，识别能够在健康领域的分布迁移情况下产生可靠的不确定性估计方法变得至关重要。在本文中，我们探讨了使用先进的贝叶斯和非贝叶斯方法来检测分布迁移样本的可行性，旨在实现在分割任务中可靠和可信的诊断预测。具体而言，我们比较了三种不同的不确定性估计方法，每种方法都设计用于...

    In recent years, machine learning has witnessed extensive adoption across various sectors, yet its application in medical image-based disease detection and diagnosis remains challenging due to distribution shifts in real-world data. In practical settings, deployed models encounter samples that differ significantly from the training dataset, especially in the health domain, leading to potential performance issues. This limitation hinders the expressiveness and reliability of deep learning models in health applications. Thus, it becomes crucial to identify methods capable of producing reliable uncertainty estimation in the context of distribution shifts in the health sector. In this paper, we explore the feasibility of using cutting-edge Bayesian and non-Bayesian methods to detect distributionally shifted samples, aiming to achieve reliable and trustworthy diagnostic predictions in segmentation task. Specifically, we compare three distinct uncertainty estimation methods, each designed to
    
[^200]: 学习属性图元：通过可训练属性的图元进行预测性图挖掘

    Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute

    [https://arxiv.org/abs/2402.06932](https://arxiv.org/abs/2402.06932)

    本文介绍了一种名为LAGRA的可解释分类算法，它通过学习重要的小属性子图来实现图分类，并优化其属性向量。这种方法能有效区分不同类别，并且能够全面地探索所有潜在重要的子图结构。

    

    图分类问题已经得到广泛研究，然而，实现一个具有高预测性能和可解释性的模型仍然是一个具有挑战性的问题。本文提出了一种用于属性图数据的可解释分类算法，称为LAGRA（Learning Attributed GRAphlets）。LAGRA学习小属性子图（称为属性图元）的重要性权重，同时优化它们的属性向量。这使我们能够获得一种结合了子图结构和属性向量的组合，可以有效区分不同类别。LAGRA的一个显著特点是，训练数据集中的所有子图结构都可以被视为属性图元的候选结构。这种方法能够全面地探索所有潜在重要的子图，但显然，一个简单的实现可能需要大量的计算。为了缓解这个问题，我们提出了一种高效的修剪策略，结合了...

    The graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a naive implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the
    
[^201]: ORIENT:一种面向6G中延迟敏感应用程序的优先权感知节能方法

    ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G

    [https://arxiv.org/abs/2402.06931](https://arxiv.org/abs/2402.06931)

    ORIENT是一种面向6G的优先权感知节能方法，旨在通过解决服务实例放置和分配、路径选择和请求优先级的联合问题来最大化系统的整体利润，并在长时间内最小化能耗。

    

    随着对6G到来的期望增加，人们对计算和网络的能耗增长表示担忧。预计连接设备的激增和资源要求高的应用程序将为能源资源带来前所未有的挑战。虽然过去已经讨论了可持续的资源分配策略，但这些努力主要集中在单域编排上，或者忽略了6G提出的独特要求。为了解决这个问题，我们研究了服务实例的放置和分配、路径选择和请求优先级的联合问题，称为PIRA。目标函数是通过最大化同时支持的请求数量来最大化系统的整体利润，同时在长时间内最小化能耗。此外，还考虑了计算和网络资源的端到端延迟要求和资源容量约束，其中使用了排队论来估计

    Anticipation for 6G's arrival comes with growing concerns about increased energy consumption in computing and networking. The expected surge in connected devices and resource-demanding applications presents unprecedented challenges for energy resources. While sustainable resource allocation strategies have been discussed in the past, these efforts have primarily focused on single-domain orchestration or ignored the unique requirements posed by 6G. To address this gap, we investigate the joint problem of service instance placement and assignment, path selection, and request prioritization, dubbed PIRA. The objective function is to maximize the system's overall profit as a function of the number of concurrently supported requests while simultaneously minimizing energy consumption over an extended period of time. In addition, end-to-end latency requirements and resource capacity constraints are considered for computing and networking resources, where queuing theory is utilized to estimate
    
[^202]: CochCeps-Augment：一种使用基于Cochlear Cepstrum的掩蔽的自监督对比学习方法用于语音情感识别

    CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition

    [https://arxiv.org/abs/2402.06923](https://arxiv.org/abs/2402.06923)

    提出了一种名为CochCeps-Augment的方法，利用基于Cochlear Cepstrum的掩蔽增强任务进行自监督对比学习，提高了语音情感识别的性能和噪声鲁棒性。

    

    自监督学习（SSL）用于情感内容的自动语音识别可以被噪声干扰严重降低，影响对语音的复杂时域和频谱信息结构进行建模的效率。最近，大规模语音数据集上的SSL以及新的音频特定的SSL代理任务（如时域和频域掩蔽）已经出现，相比于传统的源自图像增强领域的方法，取得了更好的性能。我们提出的创新在于基于成功的范例引入CochCeps-Augment，这是一种用于自监督对比学习语音表示的新型生物启发掩蔽增强任务。具体来说，我们利用了新引入的生物启发式Cochlear cepstrogram（CCGRAM）来推导输入语音的噪声鲁棒表示，然后通过自监督学习方案进一步优化。后者利用SimCLR生成CCGRAM的对比视图，通过对比学习来产生。

    Self-supervised learning (SSL) for automated speech recognition in terms of its emotional content, can be heavily degraded by the presence noise, affecting the efficiency of modeling the intricate temporal and spectral informative structures of speech. Recently, SSL on large speech datasets, as well as new audio-specific SSL proxy tasks, such as, temporal and frequency masking, have emerged, yielding superior performance compared to classic approaches drawn from the image augmentation domain. Our proposed contribution builds upon this successful paradigm by introducing CochCeps-Augment, a novel bio-inspired masking augmentation task for self-supervised contrastive learning of speech representations. Specifically, we utilize the newly introduced bio-inspired cochlear cepstrogram (CCGRAM) to derive noise robust representations of input speech, that are then further refined through a self-supervised learning scheme. The latter employs SimCLR to generate contrastive views of a CCGRAM throu
    
[^203]: 机器中的私语：LLM集成系统中的保密性

    Whispers in the Machine: Confidentiality in LLM-integrated Systems

    [https://arxiv.org/abs/2402.06922](https://arxiv.org/abs/2402.06922)

    本研究提供了一种评估LLM集成系统保密性的系统化方法，通过形式化一个"秘密密钥"游戏来捕捉模型隐藏私人信息的能力。评估了八种攻击和四种防御方法，发现当前的防御方法缺乏泛化性能。

    

    大规模语言模型（LLM）越来越多地与外部工具集成。尽管这些集成可以显著提高LLM的功能，但它们也在不同组件之间创建了一个新的攻击面，可能泄露机密数据。具体而言，恶意工具可以利用LLM本身的漏洞来操纵模型并损害其他服务的数据，这引发了在LLM集成环境中如何保护私密数据的问题。在这项工作中，我们提供了一种系统评估LLM集成系统保密性的方法。为此，我们形式化了一个"秘密密钥"游戏，可以捕捉模型隐藏私人信息的能力。这使我们能够比较模型对保密性攻击的脆弱性以及不同防御策略的有效性。在这个框架中，我们评估了八种先前发表的攻击和四种防御方法。我们发现当前的防御方法缺乏泛化性能。

    Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations.   In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization
    
[^204]: 选择混合回归模型的聚类技术：基于太阳能热系统的案例研究

    Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System

    [https://arxiv.org/abs/2402.06921](https://arxiv.org/abs/2402.06921)

    本研究通过比较四种聚类技术的性能，提出了一种强大的混合模型用于监督学习任务，以预测太阳能热系统的输出温度。

    

    本研究针对四种聚类技术进行性能比较，旨在实现强大的混合模型用于监督学习任务。我们采集了来自位于西班牙加利西亚省Lugo Xermade地区的生物气候住宅“Sotavento”上实验性风电场的真实数据集。作者选择了热太阳能发电系统作为研究对象，研究了应用多种聚类方法后再应用回归技术预测系统输出温度的工作原理。为了定义每种聚类方法的质量，我们实施了两种可能的解决方案。第一种基于三个无监督学习度量（Silhouette、Calinski-Harabasz和Davies-Bouldin），而第二种则采用了多层感知器等回归算法的最常见错误度量方法。

    This work addresses the performance comparison between four clustering techniques with the objective of achieving strong hybrid models in supervised learning tasks. A real dataset from a bio-climatic house named Sotavento placed on experimental wind farm and located in Xermade (Lugo) in Galicia (Spain) has been collected. Authors have chosen the thermal solar generation system in order to study how works applying several cluster methods followed by a regression technique to predict the output temperature of the system. With the objective of defining the quality of each clustering method two possible solutions have been implemented. The first one is based on three unsupervised learning metrics (Silhouette, Calinski-Harabasz and Davies-Bouldin) while the second one, employs the most common error measurements for a regression algorithm such as Multi Layer Perceptron.
    
[^205]: TREET: 基于Transformer的传输熵估计

    TREET: TRansfer Entropy Estimation via Transformer

    [https://arxiv.org/abs/2402.06919](https://arxiv.org/abs/2402.06919)

    本研究提出了TREET，一种基于Transformer的传输熵估计方法，通过引入Donsker-Vardhan表示法和注意力机制，实现了对稳定过程的传输熵估计。我们设计了估计TE的优化方案，并展示了通过联合优化方案优化通信通道容量和估计器的记忆能力。

    

    传输熵（TE）是信息论中揭示过程之间信息流动方向的度量，对各种实际应用提供了宝贵的见解。本研究提出了一种名为TREET的基于Transformer的传输熵估计方法，用于估计稳定过程的TE。所提出的方法利用Donsker-Vardhan（DV）表示法对TE进行估计，并利用注意力机制进行神经估计任务。我们对TREET进行了详细的理论和实证研究，并将其与现有方法进行了比较。为了增加其适用性，我们设计了一种基于功能表示引理的估计TE优化方案。之后，我们利用联合优化方案来优化具有记忆性的通信通道容量，这是信息论中的一个典型优化问题，并展示了我们估计器的记忆能力。

    Transfer entropy (TE) is a measurement in information theory that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel transformer-based approach for estimating the TE for stationary processes. The proposed approach employs Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma. Afterwards, we take advantage of the joint optimization scheme to optimize the capacity of communication channels with memory, which is a canonical optimization problem in information theory, and show the memory capabilities of our estimator. Finally, we apply
    
[^206]: 用直接的两两比较方法生成思维链，以搜索最有潜力的中间思维

    Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought

    [https://arxiv.org/abs/2402.06918](https://arxiv.org/abs/2402.06918)

    本文提出了一种基于直接两两比较的方法，通过利用LLMs的噪声反馈，直接识别出最有潜力的中间思维，从而生成优秀的思维链。

    

    为了提高大型语言模型(LLMs)处理复杂推理问题的能力，提出了思维链(Chain-of-Thoughts, CoT)方法，用于指导LLMs进行逐步推理，从简单到复杂的问题解决。目前最先进的生成这种思维链的方法涉及互动协作，学习者生成候选中间思维，由LLMs评估，引导生成后续思维。然而，一个广泛但未被充分研究的问题是，LLMs的评估通常存在噪声和不可靠性，可能误导生成过程，选择不够有潜力的中间思维。本文受Vapnik原则的启发，提出了一种新的基于比较的CoT生成算法，直接根据LLMs的噪声反馈确定最有潜力的思维。在每一轮中，我们随机配对中间思维，并直接促使LLMs从每对中选择更有潜力的思维。

    To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair,
    
[^207]: 用线性策略网络解决深度强化学习基准问题

    Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks

    [https://arxiv.org/abs/2402.06912](https://arxiv.org/abs/2402.06912)

    本研究通过使用进化策略(ES)来优化神经网络的权重，以通过直接策略搜索解决深度强化学习(DRL)基准问题。研究结果显示，ES可以在许多基准任务中找到有效的线性策略，与当前使用更大网络的DRL方法相比，这表明当前的基准问题比以往认为的更容易解决。

    

    尽管深度强化学习(DRL)算法能够学习有效的策略来解决像Atari游戏和机器人任务这样的挑战性问题，但算法复杂，训练时间往往较长。本研究探讨了进化策略(ES)与基于梯度的深度强化学习方法之间的表现差异。我们使用ES通过神经进化优化神经网络的权重，通过直接策略搜索来完成。我们对常规网络和由一个从观测到动作的单一线性层组成的策略网络进行基准测试；对于三种经典的ES方法和三种基于梯度的方法，如PPO。我们的结果表明，ES可以在许多RL基准任务中找到有效的线性策略，而DRL方法只能使用更大的网络找到成功的策略，这表明当前的基准问题比以前认为的更容易解决。有趣的是，即使对于更复杂的任务，ES的结果也与基于梯度的方法相当。

    Although Deep Reinforcement Learning (DRL) methods can learn effective policies for challenging problems such as Atari games and robotics tasks, algorithms are complex and training times are often long. This study investigates how evolution strategies (ES) perform compared to gradient-based deep reinforcement learning methods. We use ES to optimize the weights of a neural network via neuroevolution, performing direct policy search. We benchmark both regular networks and policy networks consisting of a single linear layer from observations to actions; for three classical ES methods and for three gradient-based methods such as PPO. Our results reveal that ES can find effective linear policies for many RL benchmark tasks, in contrast to DRL methods that can only find successful policies using much larger networks, suggesting that current benchmarks are easier to solve than previously assumed. Interestingly, also for higher complexity tasks, ES achieves results comparable to gradient-based
    
[^208]: 高拓扑神经网络：通过高阶相互作用缓解图神经网络的瓶颈问题

    Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions

    [https://arxiv.org/abs/2402.06908](https://arxiv.org/abs/2402.06908)

    本文研究了图神经网络的瓶颈问题，并提出了一种名为高拓扑神经网络的方法，通过引入高阶相互作用和多关系归纳偏置来缓解这些问题。

    

    自然现象的不可约复杂性使得图神经网络成为在图结构数据上进行表示学习任务的标准模型。虽然它们能够捕捉局部和全局模式的能力令人印象深刻，但与长距离和高阶依赖相关的影响对这些模型提出了相当大的挑战。本文从理论框架入手，揭示了网络的宽度、深度和图拓扑对消息传递神经网络中过度压缩现象的影响。然后，本文通过高拓扑神经网络从高阶相互作用和多关系归纳偏置入手。这种模型通过高维结构传播消息，为信息流提供了快捷方式或额外的路径。通过这种构建，底层的计算图不再与输入图结构耦合，从而缓解了前面提到的瓶颈问题，同时还考虑了h。

    The irreducible complexity of natural phenomena has led Graph Neural Networks to be employed as a standard model to perform representation learning tasks on graph-structured data. While their capacity to capture local and global patterns is remarkable, the implications associated with long-range and higher-order dependencies pose considerable challenges to such models. This work starts with a theoretical framework to reveal the impact of network's width, depth, and graph topology on the over-squashing phenomena in message-passing neural networks. Then, the work drifts towards, higher-order interactions and multi-relational inductive biases via Topological Neural Networks. Such models propagate messages through higher-dimensional structures, providing shortcuts or additional routes for information flow. With this construction, the underlying computational graph is no longer coupled with the input graph structure, thus mitigating the aforementioned bottlenecks while accounting also for h
    
[^209]: GenTranslate: 大型语言模型是生成的多语言语音和机器翻译工具

    GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators

    [https://arxiv.org/abs/2402.06894](https://arxiv.org/abs/2402.06894)

    GenTranslate是一个新的翻译任务生成模型，通过利用大型语言模型的丰富语言知识和强大推理能力，可以从N-best列表中生成更高质量的翻译结果。

    

    大型语言模型（LLMs）的最新进展通过减少表示误差和引入外部知识，推动了多语言语音和机器翻译的发展。然而，翻译任务通常使用束搜索解码和前k个假设选择进行推理。这些技术往往不能充分利用多样化的N-best假设中的丰富信息，使得它们在需要单个高质量输出序列的翻译任务中效果不佳。在本文中，我们提出了一个新的翻译任务生成模型，即“GenTranslate”，它基于LLMs来从N-best列表中生成更好的结果。利用LLMs丰富的语言知识和强大的推理能力，我们的新模型可以将N-best候选人中的丰富信息整合起来，生成更高质量的翻译结果。此外，为了支持LLM的微调，我们构建并发布了一个HypoTransla模型。

    Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTransla
    
[^210]: 理解测试时间增强

    Understanding Test-Time Augmentation

    [https://arxiv.org/abs/2402.06892](https://arxiv.org/abs/2402.06892)

    这篇论文旨在理解测试时间增强（TTA）方法并为其提供理论保证，同时澄清其行为。

    

    测试时间增强（TTA）是一种非常有用的启发式方法，利用数据增强在测试中产生平均输出。尽管TTA在实验中显示出很好的效果，但对其理论方面的讨论还不足。本文旨在为TTA提供理论保证并澄清其行为。

    Test-Time Augmentation (TTA) is a very powerful heuristic that takes advantage of data augmentation during testing to produce averaged output. Despite the experimental effectiveness of TTA, there is insufficient discussion of its theoretical aspects. In this paper, we aim to give theoretical guarantees for TTA and clarify its behavior.
    
[^211]: Bilevel强化学习和RLHF的有原则的基于惩罚的方法

    Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF

    [https://arxiv.org/abs/2402.06886](https://arxiv.org/abs/2402.06886)

    本文提出了一种基于惩罚的方法来解决Bilevel强化学习和RLHF问题，这是首个有原则的算法框架。通过理论分析和实验证明了算法的有效性。

    

    最近，Bilevel优化已被应用于许多机器学习任务中。然而，它们的应用仅限于监督学习设置，其中考虑了具有良性结构的静态目标函数。但是，激励设计、反向强化学习(RL)和来自人类反馈的RLHF等Bilevel问题通常被建模为超越简单静态目标结构的动态目标函数，这给使用现有Bilevel解决方案带来了重大挑战。为了解决这一新的Bilevel问题类别，我们通过惩罚形式引入了解决Bilevel RL问题的第一个原则性算法框架。我们通过理论研究问题的景观及其基于惩罚的（策略）梯度算法进行了验证。我们通过在Stackelberg马尔可夫博弈、来自人类反馈的RL和激励设计中进行模拟来证明我们算法的有效性。

    Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse reinforcement learning (RL), and RL from human feedback (RLHF) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via simulations in the Stackelberg Markov game, RL from human feedback and incentive design.
    
[^212]: DimVis: 使用可解释性提升机器（EBM）解释维度约减中的视觉聚类

    DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine

    [https://arxiv.org/abs/2402.06885](https://arxiv.org/abs/2402.06885)

    DimVis是一种基于可解释性提升机器的可视化工具，用于解释维度约减中的视觉聚类。它通过对特征相关性的解释，提供对高维数据中视觉聚类的解释和分析。

    

    维度约减（DR）技术，如t-SNE和UMAP，很受欢迎，可以将复杂的数据集转换成更简单的可视化表示。然而，虽然这些方法有效地揭示了数据集的总体模式，但可能会引入伪像并存在解释性问题。本文提出了一种名为DimVis的可视化工具，它使用监督的可解释性提升机器（EBM）模型（在用户选择的感兴趣数据上训练）作为DR投影的解释助手。我们的工具通过通过交互式探索UMAP投影中的特征相关性来提供对视觉聚类中特征重要性的解释，从而促进高维数据分析。具体而言，DimVis使用一个对比的EBM模型，该模型在实时训练中能区分感兴趣聚类内外的数据。利用EBM固有的可解释性，我们可以通过单个和成对的特征比较来解释聚类本身。

    Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular for transforming complex datasets into simpler visual representations. However, while effective in uncovering general dataset patterns, these methods may introduce artifacts and suffer from interpretability issues. This paper presents DimVis, a visualization tool that employs supervised Explainable Boosting Machine (EBM) models (trained on user-selected data of interest) as an interpretation assistant for DR projections. Our tool facilitates high-dimensional data analysis by providing an interpretation of feature relevance in visual clusters through interactive exploration of UMAP projections. Specifically, DimVis uses a contrastive EBM model that is trained in real time to differentiate between the data inside and outside a cluster of interest. Taking advantage of the inherent explainable nature of the EBM, we then use this model to interpret the cluster itself via single and pairwise feature comparisons in a 
    
[^213]: 结构冗余的低秩逼近用于自监督学习

    Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning

    [https://arxiv.org/abs/2402.06884](https://arxiv.org/abs/2402.06884)

    本文研究结构冗余的低秩逼近在自监督学习中的应用，提出了一个逼近冗余组件的新方法，并通过分析过量风险来支持理论。

    

    我们研究重构型自监督学习的数据生成机制，以揭示其有效性。在拥有无限量的标记样本的情况下，我们提供了完美线性逼近的充分必要条件。该条件揭示了一个保留标签类别Y的满秩组件，以及一个冗余组件。受到该条件的启发，我们提出通过低秩分解逼近冗余组件，并通过引入一个由分解秩s参数化的新量$\epsilon_s$来衡量逼近质量。我们将$\epsilon_s$整合到线性回归和岭回归设置下的过量风险分析中，后一种正则化方法用于处理学习特征的维度远大于下游任务的标记样本数n的情况。我们设计了三个简化实验，以比较不同设置下的自监督学习和监督学习，以支持我们的理论。

    We study the data-generating mechanism for reconstructive SSL to shed light on its effectiveness. With an infinite amount of labeled samples, we provide a sufficient and necessary condition for perfect linear approximation. The condition reveals a full-rank component that preserves the label classes of Y, along with a redundant component. Motivated by the condition, we propose to approximate the redundant component by a low-rank factorization and measure the approximation quality by introducing a new quantity $\epsilon_s$, parameterized by the rank of factorization s. We incorporate $\epsilon_s$ into the excess risk analysis under both linear regression and ridge regression settings, where the latter regularization approach is to handle scenarios when the dimension of the learned features is much larger than the number of labeled samples n for downstream tasks. We design three stylized experiments to compare SSL with supervised learning under different settings to support our theoretic
    
[^214]: 具有辨别性对抗学习的论文

    Discriminative Adversarial Unlearning

    [https://arxiv.org/abs/2402.06864](https://arxiv.org/abs/2402.06864)

    该论文提出了一种基于最小最大优化范式的机器反学习框架，利用强大的成员推断攻击来实现反学习，同时保持其总体性能，并增强了反学习能力。

    

    我们引入了一种新颖的机器反学习框架，基于最小最大优化范式的已建立原则。我们利用强大的成员推断攻击（MIA）的能力，以促进从训练模型中反学习特定样本。我们考虑了两个网络的场景，攻击者$\mathbf{A}$和经过训练的防御者 $\mathbf{D}$在对抗目标下相互对抗，其中攻击者旨在揭示数据的信息以推断成员身份，而防御者在反击中进行反学习，同时保持其总体性能。算法可以使用反向传播进行端到端训练，遵循已知的迭代最小最大方法来更新攻击者和防御者。我们还加入了自监督目标，有效地解决了遗忘集和验证集之间的特征空间差异，增强了反学习能力

    We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearnin
    
[^215]: LiRank: 领英的工业规模排名模型

    LiRank: Industrial Large Scale Ranking Models at LinkedIn

    [https://arxiv.org/abs/2402.06859](https://arxiv.org/abs/2402.06859)

    LiRank是领英的一个大规模排名框架，它应用了最先进的建模架构和优化方法，并提出了新的建模改进和技术，通过A/B测试取得了有效的结果。

    

    我们介绍了LiRank，这是领英的一个大规模排名框架，它将最先进的建模架构和优化方法应用于生产。我们揭示了几个建模改进，包括Residual DCN，它在著名的DCNv2架构中添加了注意力和残差连接。我们分享了将SOTA架构组合和调优以创建统一模型的见解，包括Dense Gating、Transformers和Residual DCN。我们还提出了用于校准的新技术，并描述了如何将基于深度学习的探索/利用方法应用于生产环境。为了实现大规模排名模型的有效、生产级服务，我们详细介绍了使用量化和词汇压缩训练和压缩模型的方法。我们提供了Feed排名、职位推荐和广告点击率（CTR）预测等大规模使用案例的部署设置细节。通过阐明最有效的技术方法，我们总结了各种A/B测试的经验教训。

    We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. T
    
[^216]: 更好还是更差？通过标签增强学习最小方差特征

    For Better or For Worse? Learning Minimum Variance Features With Label Augmentation

    [https://arxiv.org/abs/2402.06855](https://arxiv.org/abs/2402.06855)

    本研究分析了标签增强方法中标签增强的作用。研究证明，在线性可分数据上使用标签增强训练的线性模型只能学习到最小方差特征，而标准训练可以学习到更高方差特征。此外，标签平滑和Mixup对于训练数据的对抗扰动可能不太鲁棒。

    

    在过去的十年中，数据增强对于成功地训练深度学习模型在分类任务上发挥了关键作用。数据增强技术中的一个重要子类-包括标签平滑和Mixup-涉及在模型训练过程中修改输入数据和输入标签。在这项工作中，我们分析了此类方法中标签增强的作用。我们证明了在线性可分数据上使用标签增强训练的线性模型只能学习到最小方差特征，而标准训练（包括权重衰减）可以学习到更高方差特征。我们的结果的一个重要后果是消极的：与标准训练相比，标签平滑和Mixup对于训练数据的对抗扰动可能不太鲁棒。我们通过对合成数据和图像分类基准的一系列实验证明了我们的理论与实践的一致性。

    Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.
    
[^217]: 陀螺仪辅助的运动去模糊网络

    Gyroscope-Assisted Motion Deblurring Network

    [https://arxiv.org/abs/2402.06854](https://arxiv.org/abs/2402.06854)

    本文提出了一个陀螺仪辅助的运动去模糊网络框架，通过利用IMU数据合成和恢复运动模糊图像，解决了实际应用中的训练数据对齐和信息限制问题。

    

    最近几年图像研究集中关注了去模糊网络。然而，由于缺乏像素对齐的训练三元组（背景图像、模糊图像和模糊热图）以及模糊图像中所固有的信息限制，它们在现实世界的去模糊，尤其是运动模糊方面的实际应用仍然受到限制。本文提出了一个简单而高效的框架，利用惯性测量单元（IMU）数据合成和恢复运动模糊图像。值得注意的是，该框架包括一种用于生成训练三元组的策略，以及一种用于恢复模糊图像的陀螺仪辅助运动去模糊（GAMD）网络。其理论基础在于通过利用IMU数据，我们可以确定图像曝光阶段相机姿态的变换，从而推断出三维空间内每个点的运动轨迹（即模糊轨迹）。因此，使用我们的策略获得的合成三元组与自然运动模糊紧密相关。

    Image research has shown substantial attention in deblurring networks in recent years. Yet, their practical usage in real-world deblurring, especially motion blur, remains limited due to the lack of pixel-aligned training triplets (background, blurred image, and blur heat map) and restricted information inherent in blurred images. This paper presents a simple yet efficient framework to synthetic and restore motion blur images using Inertial Measurement Unit (IMU) data. Notably, the framework includes a strategy for training triplet generation, and a Gyroscope-Aided Motion Deblurring (GAMD) network for blurred image restoration. The rationale is that through harnessing IMU data, we can determine the transformation of the camera pose during the image exposure phase, facilitating the deduction of the motion trajectory (aka. blur trajectory) for each point inside the three-dimensional space. Thus, the synthetic triplets using our strategy are inherently close to natural motion blur, strict
    
[^218]: RAMP：增强对多个$l_p$扰动的对抗鲁棒性

    RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations

    [https://arxiv.org/abs/2402.06827](https://arxiv.org/abs/2402.06827)

    该论文提出了一种名为RAMP的框架，旨在增强对多个$l_p$扰动的对抗鲁棒性。通过分析不同$l_p$攻击之间的权衡关系，并设计逻辑配对损失来提高准确性和鲁棒性的平衡。同时，通过将自然训练与对抗训练相结合，整合有用信息以调和准确性和鲁棒性的权衡。

    

    在提高对单个$l_p$范数受限的对抗攻击的鲁棒性方面，已经有相当多的工作在使用对抗训练（AT）进行研究。然而，AT模型的多范数鲁棒性（共同准确性）仍然较低。我们观察到，同时获得良好的共同准确性和清洁准确性是困难的，因为在多个$l_p$扰动之间存在鲁棒性、准确性/鲁棒性/效率之间的权衡。通过从分布转变的角度分析这些权衡，我们确定了$l_p$攻击之间的关键权衡对，以提高效率并设计了一个逻辑配对损失来提高共同准确性。接下来，我们通过梯度投影将自然训练与AT相连接，以从自然训练中找到并整合有用的信息到AT中，从而调和准确性/鲁棒性的权衡。结合我们的贡献，我们提出了一个名为\textbf{RAMP}的框架，来提高对多个$l_p$扰动的鲁棒性。我们展示了\textbf{RAMP}可以很容易地适应...

    There is considerable work on improving robustness against adversarial attacks bounded by a single $l_p$ norm using adversarial training (AT). However, the multiple-norm robustness (union accuracy) of AT models is still low. We observe that simultaneously obtaining good union and clean accuracy is hard since there are tradeoffs between robustness against multiple $l_p$ perturbations, and accuracy/robustness/efficiency. By analyzing the tradeoffs from the lens of distribution shifts, we identify the key tradeoff pair among $l_p$ attacks to boost efficiency and design a logit pairing loss to improve the union accuracy. Next, we connect natural training with AT via gradient projection, to find and incorporate useful information from natural training into AT, which moderates the accuracy/robustness tradeoff. Combining our contributions, we propose a framework called \textbf{RAMP}, to boost the robustness against multiple $l_p$ perturbations. We show \textbf{RAMP} can be easily adapted for 
    
[^219]: 通过语言预测足球比赛事件

    Forecasting Events in Soccer Matches Through Language

    [https://arxiv.org/abs/2402.06820](https://arxiv.org/abs/2402.06820)

    本文提出了一种使用语言模型预测足球比赛中下一个事件的方法，该方法受到大型语言模型方法的启发。通过深度学习和WyScout数据集，该方法在预测准确性方面明显超过了以往的方法。该方法的应用包括博彩和比赛分析，并提供了一个模拟骨架用于构建分析流水线。

    

    本文介绍了一种预测足球比赛中下一个事件的方法，这是一个与大型语言模型（LLMs）面临的问题非常相似的挑战。与其他严重限制足球事件动态的方法不同，这些方法往往从很多变量中抽象出来或依赖于混合顺序模型，我们的研究提出了一种受到LLMs方法学启发的新技术。这些模型预测了组成一个事件的完整变量链，大大简化了构建足球大事件模型（LEMs）的过程。利用公开可用的WyScout数据集进行深度学习，所提出的方法在关键领域（如下一个事件类型的预测准确性）显著超越了以往LEM提案的性能。本文突显了LEM在多种应用中的实用性，包括博彩和比赛分析。此外，我们还展示了LEM提供了一个模拟骨架，可以构建许多分析流水线。

    This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including betting and match analytics. Moreover, we show that LEMs provide a simulation backbone on which many analytics pipelines can be bu
    
[^220]: 监控马尔可夫决策过程

    Monitored Markov Decision Processes

    [https://arxiv.org/abs/2402.06819](https://arxiv.org/abs/2402.06819)

    这篇论文介绍了一种新颖且通用的强化学习框架——监控马尔可夫决策过程(Monitored MDPs)。在这个框架中，代理不能始终观察到奖励，提出了算法来解决这个新颖的场景。

    

    在强化学习中，代理通过与环境的交互和接收反馈（数值奖励）来学习执行任务。然而，奖励始终可观察的假设在现实世界的问题中通常不适用。例如，代理可能需要要求人类监督其行为或激活监控系统以接收反馈。甚至可能存在奖励在可观察之前一段时间或在不再给予奖励之后的时间。换句话说，有些情况下，环境根据代理的行为生成奖励，但代理无法观察到这些奖励。在本文中，我们正式定义了一个新颖但通用的强化学习框架 - 监控马尔可夫决策过程(Monitored MDPs)，在此框架中代理并非总是能够观察到奖励。我们讨论了这种设置可能带来的理论和实践上的后果，展示了即使在玩具环境中也会出现的挑战，并提出了算法来开始解决这个新颖的场景。

    In reinforcement learning (RL), an agent learns to perform a task by interacting with an environment and receiving feedback (a numerical reward) for its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. For example, the agent may need to ask a human to supervise its actions or activate a monitoring system to receive feedback. There may even be a period of time before rewards become observable, or a period of time after which rewards are no longer given. In other words, there are cases where the environment generates rewards in response to the agent's actions but the agent cannot observe them. In this paper, we formalize a novel but general RL framework - Monitored MDPs - where the agent cannot always observe rewards. We discuss the theoretical and practical consequences of this setting, show challenges raised even in toy environments, and propose algorithms to begin to tackle this novel setting. This paper introduces a p
    
[^221]: 朝着一种系统的方法设计新的集成学习算法

    Towards a Systematic Approach to Design New Ensemble Learning Algorithms

    [https://arxiv.org/abs/2402.06818](https://arxiv.org/abs/2402.06818)

    本研究针对集成学习算法的设计提出了一种系统的方法，通过创新的偏差-方差-多样性分解框架指导，使用神经网络作为基学习者，生成了21种新的集成算法。

    

    集成学习由于其改善预测性能的潜力，一直是机器学习研究的焦点。本研究重新审视了自1990年以来，历史上仅限于回归问题的偏差-方差-协方差分析的集成错误分解的基础性研究。最近的进展引入了一种“多样性的统一理论”，提出了一种创新的偏差-方差-多样性分解框架。借助这种现代理解，我们的研究系统地探索了这种分解在指导新的集成学习算法的创建方面的应用。我们以回归任务为重点，采用神经网络作为基学习者，以调查这个理论框架的实际影响。这种方法使用了7种简单的集成方法，我们称之为策略，用于神经网络生成了21种新的集成算法。其中，大部分方法与“snapshot”策略聚合在一起，这是7种策略中的一种。

    Ensemble learning has been a focal point of machine learning research due to its potential to improve predictive performance. This study revisits the foundational work on ensemble error decomposition, historically confined to bias-variance-covariance analysis for regression problems since the 1990s. Recent advancements introduced a "unified theory of diversity," which proposes an innovative bias-variance-diversity decomposition framework. Leveraging this contemporary understanding, our research systematically explores the application of this decomposition to guide the creation of new ensemble learning algorithms. Focusing on regression tasks, we employ neural networks as base learners to investigate the practical implications of this theoretical framework. This approach used 7 simple ensemble methods, we name them strategies, for neural networks that were used to generate 21 new ensemble algorithms. Among these, most of the methods aggregated with the snapshot strategy, one of the 7 st
    
[^222]: 使用精细调整的大型事件模型估计不同背景下的球员表现

    Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models

    [https://arxiv.org/abs/2402.06815](https://arxiv.org/abs/2402.06815)

    本文介绍了将大型事件模型（LEMs）应用于足球分析领域的创新方法。通过学习“足球语言”，LEMs可以预测后续事件的变量，从而模拟比赛并预测球员在不同团队背景下的表现。研究通过对2017-2018英超赛季使用WyScout数据集进行LEMs的精细调整，发现了LEMs在足球分析中的有效性和局限性，同时突出了该模型在预测球队排名和探索高级场景方面的潜力。

    

    本文引入了大型事件模型（LEMs）在足球分析领域的创新应用，类似于大型语言模型。通过学习足球的“语言” - 预测后续事件的变量而不是单词，LEMs可以模拟比赛并提供各种应用，包括预测不同团队背景下的球员表现。我们专注于使用WyScout数据集对2017-2018英超赛季进行LEMs的精细调整，以获取关于球员贡献和团队战略的具体见解。我们的方法包括调整这些模型以反映足球的微妙动态，从而评估假设的转会。我们的研究结果验证了LEMs在足球分析中的效果和局限性，突显了该模型预测球队预期排名并探索高级场景（例如将Cristiano Ronaldo或Lionel Messi转会至不同球队的潜在影响）的能力。

    This paper introduces an innovative application of Large Event Models (LEMs), akin to Large Language Models, to the domain of soccer analytics. By learning the "language" of soccer - predicting variables for subsequent events rather than words LEMs facilitate the simulation of matches and offer various applications, including player performance prediction across different team contexts. We focus on fine-tuning LEMs with the WyScout dataset for the 2017-2018 Premier League season to derive specific insights into player contributions and team strategies. Our methodology involves adapting these models to reflect the nuanced dynamics of soccer, enabling the evaluation of hypothetical transfers. Our findings confirm the effectiveness and limitations of LEMs in soccer analytics, highlighting the model's capability to forecast teams' expected standings and explore high-profile scenarios, such as the potential effects of transferring Cristiano Ronaldo or Lionel Messi to different teams in the 
    
[^223]: 基于卡尔曼滤波的框架用于监测医院内死亡率预测模型的性能随时间变化

    A Kalman Filter Based Framework for Monitoring the Performance of In-Hospital Mortality Prediction Models Over Time

    [https://arxiv.org/abs/2402.06812](https://arxiv.org/abs/2402.06812)

    本研究基于卡尔曼滤波提出了一个框架，通过调整样本大小和类别分布，以实现对不同时间段性能指标的公平比较。

    

    与临床试验不同，研究人员可以确定所需的正负样本数量，或者在机器学习研究中，验证集的大小和类别分布是静态和已知的。然而，真实世界的情况下，对于进入的患者数量和分布很难控制。因此，在不同时间段测量时，评估指标如接收器操作特性下面积（AUCROC）和精确度-召回率曲线下面积（AUCPR）可能无法直接进行比较。因此，在本研究中，对于运行时间长的二分类器，我们提出调整这些性能指标以考虑样本大小和类别分布，以便在两个时间段之间进行公平比较。值得注意的是，样本数量和类别分布，即正样本的比例，是影响AUCROC方差的两个鲁棒性因素。为了更好地估计性能指标的均值a

    Unlike in a clinical trial, where researchers get to determine the least number of positive and negative samples required, or in a machine learning study where the size and the class distribution of the validation set is static and known, in a real-world scenario, there is little control over the size and distribution of incoming patients. As a result, when measured during different time periods, evaluation metrics like Area under the Receiver Operating Curve (AUCROC) and Area Under the Precision-Recall Curve(AUCPR) may not be directly comparable. Therefore, in this study, for binary classifiers running in a long time period, we proposed to adjust these performance metrics for sample size and class distribution, so that a fair comparison can be made between two time periods. Note that the number of samples and the class distribution, namely the ratio of positive samples, are two robustness factors which affect the variance of AUCROC. To better estimate the mean of performance metrics a
    
[^224]: 使用总信息流评估共同创造力

    Evaluating Co-Creativity using Total Information Flow

    [https://arxiv.org/abs/2402.06810](https://arxiv.org/abs/2402.06810)

    本研究旨在通过使用总信息流来定量评估音乐中的共同创造力过程，并通过定性研究证明该方法与人类感知相匹配。

    

    音乐中的共同创造力指的是两个或更多的音乐家或音乐代理通过创作或即兴创作音乐相互互动。然而，这是一个非常主观的过程，每个音乐家对于在某种情境下哪种即兴创作更好有自己的偏好。在本文中，我们旨在创建一个基于总信息流的度量来定量评估音乐中的共同创造力过程。换句话说，我们的度量是创造性音乐过程有多"好"的指标。我们的主要假设是，好的音乐创作将最大化参与者之间的信息流，该信息流由记录在单独轨道中的音乐声音捕捉。我们提出了一种使用预训练生成模型作为熵估计器计算信息流的方法。我们通过定性研究展示了我们的方法如何与人类感知相匹配。

    Co-creativity in music refers to two or more musicians or musical agents interacting with one another by composing or improvising music. However, this is a very subjective process and each musician has their own preference as to which improvisation is better for some context. In this paper, we aim to create a measure based on total information flow to quantitatively evaluate the co-creativity process in music. In other words, our measure is an indication of how "good" a creative musical process is. Our main hypothesis is that a good musical creation would maximize information flow between the participants captured by music voices recorded in separate tracks. We propose a method to compute the information flow using pre-trained generative models as entropy estimators. We demonstrate how our method matches with human perception using a qualitative study.
    
[^225]: 对于临床恶化预测的变分时间序列模型中预测变异性的解释

    Explain Variance of Prediction in Variational Time Series Models for Clinical Deterioration Prediction

    [https://arxiv.org/abs/2402.06808](https://arxiv.org/abs/2402.06808)

    本文提出了使用delta方法确定性地近似预测的变异性的方法，并采用SHAP方法来归因于变异的贡献。该方法适用于临床恶化预测中的变分时间序列模型，可以在提高预测精度的同时提供解释性。

    

    在医疗领域中，由于许多模型无关方法的应用，深度学习应用所作出的预测分数的可解释性得到了改善。然而，对于住院病人的每日或每小时恶化风险预测，不仅预测的风险概率分数很重要，风险分数的变异性也对辅助临床决策起着关键作用。在本文中，我们建议使用delta方法以确定性地近似预测的变异性，从而可以采用SHAP方法来归因于变异的贡献。通过对变分模型中的条件隐藏空间进行采样来估计预测的变异性，并基于变异性博弈的Shapley值将其传播到输入的临床变量上。该方法适用于变分循环神经网络和变分转换器等变分时间序列模型。我们进一步认为，变分时间序列模型非常适合在预测精度和解释性之间取得平衡。

    In healthcare, thanks to many model agnostic methods, explainability of the prediction scores made by deep learning applications has improved. However, we note that for daily or hourly risk of deterioration prediction of in-hospital patients, not only the predicted risk probability score matters, but also the variance of the risk scores play key roles in aiding clinical decision making. In this paper, we propose to use delta's method to approximate variance of prediction deterministically, such that the SHAP method can be adopted to attribute contribution of variance. The prediction variance is estimated by sampling the conditional hidden space in variational models and is propagated to input clinical variables based on Shapley values of the variance game. This approach works with variational time series models such as variational recurrent neural networks and variational transformers. We further argue that variational time series models are perfect fits for achieving a balance between
    
[^226]: 关于表格数据合成算法的原则性评估

    Towards Principled Assessment of Tabular Data Synthesis Algorithms

    [https://arxiv.org/abs/2402.06806](https://arxiv.org/abs/2402.06806)

    本文提出了一个原则性和系统化的评估框架来评估表格数据合成算法，包括保真度、隐私性和实用性等新指标，以解决现有评估指标的限制。通过这个框架，对不同算法进行了比较和总结。

    

    数据合成被认为是一种利用数据同时保护数据隐私的重要方法。已经提出了大量的表格数据合成算法（我们称之为合成器）。一些合成器满足差分隐私，而其他一些则旨在以启发式的方式提供隐私保护。由于缺乏原则性评估指标以及对利用扩散模型和最新的基于边际的合成器与大型语言模型进行面对面比较的新开发的合成器的理解尚不全面，对这些合成器的优势和弱点的全面了解仍然难以实现。在本文中，我们提出了一个原则性和系统化的评估框架来评估表格数据合成算法。具体而言，我们检查和批评现有的评估指标，并引入了一组新的指标，以解决其限制，包括保真度、隐私性和实用性。基于提出的指标，我们还设计了一个统一的评估组织框架，以对不同算法进行评估并进行比较和总结。

    Data synthesis has been advocated as an important approach for utilizing data while protecting data privacy. A large number of tabular data synthesis algorithms (which we call synthesizers) have been proposed. Some synthesizers satisfy Differential Privacy, while others aim to provide privacy in a heuristic fashion. A comprehensive understanding of the strengths and weaknesses of these synthesizers remains elusive due to lacking principled evaluation metrics and missing head-to-head comparisons of newly developed synthesizers that take advantage of diffusion models and large language models with state-of-the-art marginal-based synthesizers.   In this paper, we present a principled and systematic evaluation framework for assessing tabular data synthesis algorithms. Specifically, we examine and critique existing evaluation metrics, and introduce a set of new metrics in terms of fidelity, privacy, and utility to address their limitations. Based on the proposed metrics, we also devise a un
    
[^227]: 在加拿大大滩地区和萨布尔岛上进行海洋雾能见度的生成式预测

    Generative Nowcasting of Marine Fog Visibility in the Grand Banks area and Sable Island in Canada

    [https://arxiv.org/abs/2402.06800](https://arxiv.org/abs/2402.06800)

    本研究利用生成式深度学习方法在加拿大大滩地区和萨布尔岛上预测海洋雾能见度，为30分钟和60分钟的可见度阈值提供了生成预测。

    

    本研究利用生成式深度学习方法，通过对2022年7月在大西洋北部大滩地区和加拿大东北部的萨布尔岛附近进行的FATIMA（海洋大气中的雾和湍流相互作用）观测数据进行海洋雾能见度预测。观测数据包括使用Vaisala前向散射传感器模型FD70、天气传输器模型WXT50以及Research Vessel Atlantic Condor上安装的Gill R3A超声风速仪收集的雾能见度、风速、露点差和相对湿度。预测过程中，通过对雾能见度、风速、露点差和相对湿度进行时滞处理，生成式对雾能见度进行了30分钟和60分钟的预测，并设定了雾能见度小于1公里和小于10公里两种可见度阈值。此外，还使用了极限梯度提升（XGBoost）方法。

    This study presents the application of generative deep learning techniques to evaluate marine fog visibility nowcasting using the FATIMA (Fog and turbulence interactions in the marine atmosphere) campaign observations collected during July 2022 in the North Atlantic in the Grand Banks area and vicinity of Sable Island (SI), northeast of Canada. The measurements were collected using the Vaisala Forward Scatter Sensor model FD70 and Weather Transmitter model WXT50, and Gill R3A ultrasonic anemometer mounted on the Research Vessel Atlantic Condor. To perform nowcasting, the time series of fog visibility (Vis), wind speed, dew point depression, and relative humidity with respect to water were preprocessed to have lagged time step features. Generative nowcasting of Vis time series for lead times of 30 and 60 minutes were performed using conditional generative adversarial networks (cGAN) regression at visibility thresholds of Vis < 1 km and < 10 km. Extreme gradient boosting (XGBoost) was us
    
[^228]: ForestColl: 异构网络结构上高效的集合通信

    ForestColl: Efficient Collective Communications on Heterogeneous Network Fabrics

    [https://arxiv.org/abs/2402.06787](https://arxiv.org/abs/2402.06787)

    ForestColl是一种针对任意网络拓扑生成高效调度的工具，通过构建广播/聚合生成跨越树的通信调度，实现了理论上的最小网络拥塞，并在实验中表现出高于供应商自带通信库的性能。

    

    随着现代深度神经网络模型越来越大，加速器之间的集合通信（如allreduce等）成为一个重要的性能瓶颈。在当今高度多样化和异构的网络结构下设计高效的通信调度是一项具有挑战性的任务。本文提出了一种名为ForestColl的工具，它能够为任意网络拓扑生成高效的调度。ForestColl使用广播/聚合生成跨越树作为通信调度，实现了理论上的最小网络拥塞。其调度生成运行在强多项式时间内，且具有高扩展性。ForestColl支持包括交换网络和直接连接在内的任何网络结构，以及任何网络图结构。我们在多集群的AMD MI250和NVIDIA A100平台上评估了ForestColl。与供应商自己优化的通信库RCCL和NCCL相比，ForestColl的调度性能提高了高达52％。ForestColl还优于其他...

    As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging given today's highly diverse and heterogeneous network fabrics. In this paper, we present ForestColl, a tool that generates efficient schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretically minimum network congestion. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct connections, as well as any network graph structure. We evaluated ForestColl on multi-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules achieved up to 52\% higher performance compared to the vendors' own optimized communication libraries, RCCL and NCCL. ForestColl also outperforms other s
    
[^229]: 有限数据集上基于生成模型的迁移学习用于目标检测

    Transfer learning with generative models for object detection on limited datasets

    [https://arxiv.org/abs/2402.06784](https://arxiv.org/abs/2402.06784)

    本论文提出了一个适用于通用情景的基于生成模型的迁移学习框架，用于解决有限数据集上的目标检测任务。

    

    在某些领域中，数据的可用性是有限的，尤其是对于目标检测任务，需要正确标记每个目标周围的边界框。一个显著的例子是在海洋生物学领域，需要开发自动检测海洋物种用于环境监测的方法。为了解决数据限制问题，目前最先进的机器学习策略采用了两种主要方法。第一种方法是在现有数据集上预训练模型，然后推广到具体的领域。第二种策略是使用copy-paste技术或ad-hoc模拟器等方法创建特定于目标领域的合成数据集。第一种方法往往面临重大的领域转移问题，而第二种方法需要针对特定任务设计定制解决方案。为了应对这些挑战，我们提出了一个在通用情景下有效的迁移学习框架。

    The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In
    
[^230]: 学习教学：改善教师-学生学习中的样本效率，实现从模拟到现实的迁移

    Learn to Teach: Improve Sample Efficiency in Teacher-student Learning for Sim-to-Real Transfer

    [https://arxiv.org/abs/2402.06783](https://arxiv.org/abs/2402.06783)

    本文提出了一种样本效率学习框架，名为学习教学（L2T），通过回收教师智能体收集的经验，解决了教师-学生学习中的样本效率问题。

    

    模拟到现实（sim-to-real）的迁移是机器人学习中的一个基本问题。域随机化是一种在训练过程中添加随机性的强大技术，可以有效解决模拟与现实之间的差距。然而，观测中的噪声使得学习变得更加困难。最近的研究表明，采用教师-学生学习范式可以加速随机化环境中的训练。通过使用特权信息进行学习，教师智能体可以指导学生智能体在噪声环境中操作。然而，这种方法通常不是样本效率的，因为在训练学生智能体时完全舍弃了教师智能体收集的经验，浪费了环境所透露的信息。在这项工作中，我们通过提出一个名为学习教学（L2T）的样本效率学习框架来扩展教师-学生学习范式，该框架可以回收教师智能体收集的经验。我们观察到，对于一对教师-学生智能体，环境的动态特性对两者都有重要影响。

    Simulation-to-reality (sim-to-real) transfer is a fundamental problem for robot learning. Domain Randomization, which adds randomization during training, is a powerful technique that effectively addresses the sim-to-real gap. However, the noise in observations makes learning significantly harder. Recently, studies have shown that employing a teacher-student learning paradigm can accelerate training in randomized environments. Learned with privileged information, a teacher agent can instruct the student agent to operate in noisy environments. However, this approach is often not sample efficient as the experience collected by the teacher is discarded completely when training the student, wasting information revealed by the environment. In this work, we extend the teacher-student learning paradigm by proposing a sample efficient learning framework termed Learn to Teach (L2T) that recycles experience collected by the teacher agent. We observe that the dynamics of the environments for both 
    
[^231]: 通过(超)图搜索进行逆合成预测

    Retrosynthesis Prediction via Search in (Hyper) Graph

    [https://arxiv.org/abs/2402.06772](https://arxiv.org/abs/2402.06772)

    本研究提出了一种基于半模板的逆向合成方法，通过在产物分子图和离开基团超图中进行搜索，以处理复杂的反应。

    

    在有机合成中，从指定的核心产物预测反应物是一个基本的挑战，被称为逆向合成预测。最近，基于半模板和基于图编辑的方法在解释性和准确性方面取得了良好的表现。然而，由于它们的机制，这些方法无法预测复杂的反应，例如具有多个反应中心或将相同离开基团连接到多个原子的反应。在本研究中，我们提出了一种基于半模板的方法，即逆向合成通过(超)图搜索(RetroSiG)框架，以减轻这些限制。在所提出的方法中，我们将反应中心的识别和离开基团的完成任务转化为在产物分子图中搜索和在离开基团超图中搜索的任务。作为一种基于半模板的方法，RetroSiG具有几个优点。首先，RetroSiG能够处理提到的复杂反应。

    Predicting reactants from a specified core product stands as a fundamental challenge within organic synthesis, termed retrosynthesis prediction. Recently, semi-template-based methods and graph-edits-based methods have achieved good performance in terms of both interpretability and accuracy. However, due to their mechanisms these methods cannot predict complex reactions, e.g., reactions with multiple reaction center or attaching the same leaving group to more than one atom. In this study we propose a semi-template-based method, the \textbf{Retro}synthesis via \textbf{S}earch \textbf{i}n (Hyper) \textbf{G}raph (RetroSiG) framework to alleviate these limitations. In the proposed method, we turn the reaction center identification and the leaving group completion tasks as tasks of searching in the product molecular graph and leaving group hypergraph respectively. As a semi-template-based method RetroSiG has several advantages. First, RetroSiG is able to handle the complex reactions mentione
    
[^232]: 使用Nystr\"om近似的可扩展核逻辑回归：理论分析和离散选择建模应用

    Scalable Kernel Logistic Regression with Nystr\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling

    [https://arxiv.org/abs/2402.06763](https://arxiv.org/abs/2402.06763)

    本文介绍了使用Nystr\"om近似方法解决大规模数据集上核逻辑回归的可扩展性问题。研究提供了理论分析并验证了不同的地标选择方法的性能。

    

    将基于核的机器学习技术应用于使用大规模数据集的离散选择建模时，经常面临存储需求和模型中涉及的大量参数的挑战。这种复杂性影响了大规模模型的高效训练。本文通过引入Nystr\"om近似方法解决了可扩展性问题，用于大规模数据集上的核逻辑回归。研究首先进行了理论分析，其中：i) 对KLR解的集合进行了描述，ii) 给出了使用Nystr\"om近似的KLR解的上界，并最后描述了专门用于Nystr\"om KLR的优化算法的特化。之后，对Nystr\"om KLR进行了计算验证。测试了四种地标选择方法，包括基本均匀采样、k-means采样策略和基于杠杆得分的两种非均匀方法。这些策略的性能进行了评估。

    The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\"om approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\"om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\"om KLR is described. After this, the Nystr\"om KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategi
    
[^233]: 教师与学生之间知识传递的嵌入式压缩

    Embedding Compression for Teacher-to-Student Knowledge Transfer

    [https://arxiv.org/abs/2402.06761](https://arxiv.org/abs/2402.06761)

    该论文提出了一种嵌入式压缩模块，通过可训练的教师转换来得到紧凑的教师嵌入，提高了分类性能，尤其对于无监督教师嵌入，并且使用嵌入进行引导训练的学生模型展现出更强的泛化能力。

    

    传统的知识蒸馏方法要求教师模型和学生模型在相同的任务上进行训练。然而，已经提出了将嵌入作为教师来用于不同的源任务和目标任务的方法。以往使用嵌入作为教师的方法忽视了教师嵌入很可能包含目标任务的无关知识的事实。为了解决这个问题，我们提出使用可训练的教师转换来得到紧凑的教师嵌入的嵌入压缩模块。结果显示，添加嵌入压缩模块可以提高分类性能，特别是对于无监督的教师嵌入。此外，通过嵌入的引导训练的学生模型展现了更强的泛化能力。

    Common knowledge distillation methods require the teacher model and the student model to be trained on the same task. However, the usage of embeddings as teachers has also been proposed for different source tasks and target tasks. Prior work that uses embeddings as teachers ignores the fact that the teacher embeddings are likely to contain irrelevant knowledge for the target task. To address this problem, we propose to use an embedding compression module with a trainable teacher transformation to obtain a compact teacher embedding. Results show that adding the embedding compression module improves the classification performance, especially for unsupervised teacher embeddings. Moreover, student models trained with the guidance of embeddings show stronger generalizability.
    
[^234]: 使用小初始化的梯度下降算法在非正则化矩阵完成中的收敛性分析

    Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion

    [https://arxiv.org/abs/2402.06756](https://arxiv.org/abs/2402.06756)

    本文分析了对称矩阵完成问题中梯度下降算法的收敛性。研究结果表明，在非正则化的情况下，使用小初始化的梯度下降算法可以收敛到真实的矩阵解，即使在过度参数化的情况下也成立。在过度参数化的情况下，几乎线性的收敛速度可以在获得足够多的观测条目后得到保证。

    

    本文研究对称矩阵完成的问题，目标是从仅观测到的部分条目中重构一个正半定矩阵X*，其等价于参数化矩阵UU^T，其中X*的秩为r。我们证明，使用小的初始化的基本梯度下降（GD）算法可以收敛到真实的矩阵X*，而不需要显式的正则化。这个收敛结果适用于过度参数化的场景，其中真实秩r是未知的，并且被一个搜索秩r'保守估计，且r' >> r。现有的结果要么需要显式的正则化，或者需要足够准确的初始点，或者需要准确知道真实秩r。在过度参数化的情况下，即r' >= r，我们证明，在获得Ω(dr^9)的观测条目后，GD算法以初始点∥U_0∥ <= ε几乎线性收敛到X*的ε-邻域中。

    We study the problem of symmetric matrix completion, where the goal is to reconstruct a positive semidefinite matrix $\rm{X}^\star \in \mathbb{R}^{d\times d}$ of rank-$r$, parameterized by $\rm{U}\rm{U}^{\top}$, from only a subset of its observed entries. We show that the vanilla gradient descent (GD) with small initialization provably converges to the ground truth $\rm{X}^\star$ without requiring any explicit regularization. This convergence result holds true even in the over-parameterized scenario, where the true rank $r$ is unknown and conservatively over-estimated by a search rank $r'\gg r$. The existing results for this problem either require explicit regularization, a sufficiently accurate initial point, or exact knowledge of the true rank $r$.   In the over-parameterized regime where $r'\geq r$, we show that, with $\widetilde\Omega(dr^9)$ observations, GD with an initial point $\|\rm{U}_0\| \leq \epsilon$ converges near-linearly to an $\epsilon$-neighborhood of $\rm{X}^\star$. C
    
[^235]: 通过设计实现低秩学习：网络架构和激活线性在梯度秩塌陷中的作用

    Low-Rank Learning by Design: the Role of Network Architecture and Activation Linearity in Gradient Rank Collapse

    [https://arxiv.org/abs/2402.06751](https://arxiv.org/abs/2402.06751)

    本文对DNNs中的梯度秩进行了全面研究，发现低秩学习是某些DNN架构固有的特征，而不仅仅是训练的最后阶段的现象。

    

    我们对深度神经网络（DNNs）的学习动态的理解仍然不完整。最近的研究已经开始揭示了支持这些网络的数学原理，包括“神经坍塌”现象，即在训练的最后阶段，DNN内的线性分类器会收敛到特定的几何结构。然而，几何约束在学习中的作用不仅限于这个终止阶段。例如，全连接层中的梯度自然会由于训练批次上的秩一外积的累积而形成一个低秩结构。尽管已经注意到利用这种结构节省内存或进行正则化的方法，但低秩学习作为某些DNN架构固有的一个方面的出现尚未被充分探讨。在本文中，我们对DNNs中的梯度秩进行了全面研究，考察了架构选择和数据结构对梯度秩界的影响。我们的理论分析证明了低秩学习是某些DNN架构的固有特征。

    Our understanding of learning dynamics of deep neural networks (DNNs) remains incomplete. Recent research has begun to uncover the mathematical principles underlying these networks, including the phenomenon of "Neural Collapse", where linear classifiers within DNNs converge to specific geometrical structures during late-stage training. However, the role of geometric constraints in learning extends beyond this terminal phase. For instance, gradients in fully-connected layers naturally develop a low-rank structure due to the accumulation of rank-one outer products over a training batch. Despite the attention given to methods that exploit this structure for memory saving or regularization, the emergence of low-rank learning as an inherent aspect of certain DNN architectures has been under-explored. In this paper, we conduct a comprehensive study of gradient rank in DNNs, examining how architectural choices and structure of the data effect gradient rank bounds. Our theoretical analysis pro
    
[^236]: ExGRG: 用于自监督表示学习的显式生成关系图

    ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning

    [https://arxiv.org/abs/2402.06737](https://arxiv.org/abs/2402.06737)

    本文介绍了一种新颖的自监督学习方法ExGRG，它通过显式生成关系图来解决图结构数据上的挑战，将先验领域知识和在线提取的信息纳入自监督学习中，取得了显著的成功。

    

    自监督学习（SSL）作为一种无需昂贵的标注标签而预训练深度学习模型的强大技术，通过利用未标记数据中的内嵌信号取得了显著的成功。然而，尽管SSL在计算机视觉任务中通过直观的数据增强展现了出色的性能，但其在图结构数据上的应用面临着挑战，因为图增强操作改变了语义并呈现出反直观的性质。针对这一限制，本文引入了一种新颖的非对比自监督学习方法，即显式生成关系图（ExGRG），以取代仅依靠传统的基于增强的隐式关系图。ExGRG提供了一个框架，可以将先验领域知识和在线提取的信息纳入自监督学习的不变性目标中，借鉴了拉普拉斯特征映射和期望最大化算法。通过将自监督学习与期望最大化算法结合，我们的E步骤涉及关系图的生成，以识别...

    Self-supervised Learning (SSL) has emerged as a powerful technique in pre-training deep learning models without relying on expensive annotated labels, instead leveraging embedded signals in unlabeled data. While SSL has shown remarkable success in computer vision tasks through intuitive data augmentation, its application to graph-structured data poses challenges due to the semantic-altering and counter-intuitive nature of graph augmentations. Addressing this limitation, this paper introduces a novel non-contrastive SSL approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead of relying solely on the conventional augmentation-based implicit relation graph. ExGRG offers a framework for incorporating prior domain knowledge and online extracted information into the SSL invariance objective, drawing inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM). Employing an EM perspective on SSL, our E-step involves relation graph generation to identify can
    
[^237]: 具有人类反馈的抗腐败离线强化学习

    Corruption Robust Offline Reinforcement Learning with Human Feedback

    [https://arxiv.org/abs/2402.06734](https://arxiv.org/abs/2402.06734)

    我们研究了具有人类反馈的强化学习中的数据腐败鲁棒性问题，并设计了新颖的离线方法来处理损坏的数据，并且在不同的数据生成分布假设下具有性能保证。

    

    我们研究了在离线环境中具有人类反馈的强化学习中的数据腐败鲁棒性问题。给定一组离线数据，其中包括轨迹对以及有关人类偏好的反馈，其中$\varepsilon$比例的轨迹对被损坏（例如，反馈翻转或轨迹特征被操纵），从而捕捉到对抗攻击或噪声人类偏好的影响。我们旨在设计算法，从损坏的数据中识别出接近最优的策略，并且具备可证明的保证。现有的理论研究分别研究了腐败鲁棒强化学习（在腐败下直接学习标量奖励）和离线强化学习（在没有腐败的情况下从人类反馈中学习）的设置；然而，它们并不适用于我们处理在离线环境中的损坏数据的问题。为此，我们设计了新颖的在数据生成分布覆盖各种假设下具有腐败鲁棒性的离线强化学习方法。在高层次上，我们的方法具有鲁棒亮点，并确保在不同的数据生成分布假设下的性能保证。

    We study data corruption robustness for reinforcement learning with human feedback (RLHF) in an offline setting. Given an offline dataset of pairs of trajectories along with feedback about human preferences, an $\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or trajectory features manipulated), capturing an adversarial attack or noisy human preferences. We aim to design algorithms that identify a near-optimal policy from the corrupted data, with provable guarantees. Existing theoretical works have separately studied the settings of corruption robust RL (learning from scalar rewards directly under corruption) and offline RLHF (learning from human feedback without corruption); however, they are inapplicable to our problem of dealing with corrupted data in offline RLHF setting. To this end, we design novel corruption robust offline RLHF methods under various assumptions on the coverage of the data-generating distributions. At a high level, our methodology robustif
    
[^238]: NICE: 优化上下文示例还是不优化？

    NICE: To Optimize In-Context Examples or Not?

    [https://arxiv.org/abs/2402.06733](https://arxiv.org/abs/2402.06733)

    通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。

    

    最近的研究表明，大型语言模型（LLMs）通过上下文学习和优化上下文示例（ICE），在各种任务上表现出色。然而，大多数研究假设在提示信息中要么是固定的，要么没有提供指令，导致了一个表面上的共识：优化上下文示例对于提高性能至关重要。我们针对经过指导的LLMs挑战这一共识，研究在提供了任务特定指令的情况下优化上下文示例是否必要，并发现有一些任务对于不同的优化上下文示例方法产生递减的回报。我们引入了一种任务特定的度量标准，称为"度量标准"（Metric），用于量化从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。通过对各种任务和逐步增加的指令集的系统性研究，我们验证了该启发式方法的有效性。

    Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
    
[^239]: 一个可扩展的算法用于个体公平的K均值聚类

    A Scalable Algorithm for Individually Fair K-means Clustering

    [https://arxiv.org/abs/2402.06730](https://arxiv.org/abs/2402.06730)

    本文提出了一个可扩展的算法用于个体公平的K均值聚类问题，该算法比现有的算法更快并且能够产生更低成本的解决方案。

    

    我们提出了一种可扩展的算法，用于个体公平的（p，k）聚类问题，该问题由Jung等人和Mahabadi等人引入。给定度量空间中的n个点P，对于P中的每个x，令$\delta(x)$为包含至少n/k个点的最小球的半径。如果聚类中的中心与x的距离小于等于$\delta(x)$，则将其称为个体公平的聚类。虽然已经有了好的近似算法，但尚未提出具有良好理论保证的高效实用算法。我们设计了第一个快速局部搜索算法，其运行时间仅为~O(nk^2)，并且能够获得（O(1)，6）的双目标近似。然后，我们通过实验证明，我们的算法不仅比之前的工作快得多，而且还能够产生更低成本的解决方案。

    We present a scalable algorithm for the individually fair ($p$, $k$)-clustering problem introduced by Jung et al. and Mahabadi et al. Given $n$ points $P$ in a metric space, let $\delta(x)$ for $x\in P$ be the radius of the smallest ball around $x$ containing at least $n / k$ points. A clustering is then called individually fair if it has centers within distance $\delta(x)$ of $x$ for each $x\in P$. While good approximation algorithms are known for this problem no efficient practical algorithms with good theoretical guarantees have been presented. We design the first fast local-search algorithm that runs in ~$O(nk^2)$ time and obtains a bicriteria $(O(1), 6)$ approximation. Then we show empirically that not only is our algorithm much faster than prior work, but it also produces lower-cost solutions.
    
[^240]: 动态图信息瓶颈

    Dynamic Graph Information Bottleneck

    [https://arxiv.org/abs/2402.06716](https://arxiv.org/abs/2402.06716)

    动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。

    

    动态图广泛存在于现实世界中，它们携带着复杂的时空特征模式，对于它们的表示学习提出了挑战。动态图神经网络（DGNNs）通过利用内在的动态性展示了令人印象深刻的预测能力。然而，DGNNs展示了有限的鲁棒性，易受对抗攻击。本文提出了一种新颖的动态图信息瓶颈（DGIB）框架来学习鲁棒且有区分性的表示。借助信息瓶颈（IB）原理，我们首先提出期望的最优表示应满足最小-全局-一致（MSC）条件。为了在潜在表示中压缩冗余信息和保留有价值的信息，DGIB迭代地引导和改进通过图快照传递的结构和特征信息流。为了满足MSC条件，我们将整体IB目标分解为DGIB$_{MS}$和DGIB$_C$，其中DGIB$_{MS}$通道的目标是...

    Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
    
[^241]: 学习增强的两层滑雪租赁问题的在线算法

    Learning-augmented Online Algorithm for Two-level Ski-rental Problem

    [https://arxiv.org/abs/2402.06715](https://arxiv.org/abs/2402.06715)

    本文研究了两层滑雪租赁问题，提出了一种学习增强的在线算法，该算法通过平衡前期成本和潜在未来费用的权衡来最小化总成本。

    

    在本文中，我们研究了两层滑雪租赁问题，用户需要通过选择三种支付选项之一：按需使用（即租赁），购买单个项目（即单独购买）和购买所有项目（即组合购买）来满足多个项目的一系列需求。在不了解未来需求的情况下，用户旨在通过平衡高昂的前期成本（购买）和潜在的未来费用（租金）之间的权衡来最小化总成本（即租金、单独购买和组合购买费用之和）。我们首先设计了一种鲁棒的在线算法（RDTSR），该算法提供了最坏情况下的性能保证。虽然在线算法对最坏情况下的场景具有鲁棒性，但在典型情况下往往过于谨慎，导致平均性能较差。另一方面，机器学习（ML）算法通常在各种应用中显示出有希望的平均性能，但缺乏最坏情况的性能。

    In this paper, we study the two-level ski-rental problem,where a user needs to fulfill a sequence of demands for multiple items by choosing one of the three payment options: paying for the on-demand usage (i.e., rent), buying individual items (i.e., single purchase), and buying all the items (i.e., combo purchase). Without knowing future demands, the user aims to minimize the total cost (i.e., the sum of the rental, single purchase, and combo purchase costs) by balancing the trade-off between the expensive upfront costs (for purchase) and the potential future expenses (for rent). We first design a robust online algorithm (RDTSR) that offers a worst-case performance guarantee. While online algorithms are robust against the worst-case scenarios, they are often overly cautious and thus suffer a poor average performance in typical scenarios. On the other hand, Machine Learning (ML) algorithms typically show promising average performance in various applications but lack worst-case performan
    
[^242]: 爱尔兰平衡市场的电价预测

    Electricity Price Forecasting in the Irish Balancing Market

    [https://arxiv.org/abs/2402.06714](https://arxiv.org/abs/2402.06714)

    本研究针对爱尔兰的平衡市场应用了各种已被证明成功的价格预测技术，并发现在日前市场表现良好的模型在平衡市场中效果不佳，突显了这些市场的不同特点。

    

    随着可再生能源来源越来越不可预测，短期电力市场变得越来越重要，引起了行业的广泛关注。平衡市场是最接近实时并且最不稳定的市场。然而，目前对其价格预测的文献有限、不一致且过时，针对该市场的深度学习研究尝试很少，也没有公开数据集可用。本研究将在广泛研究的日前市场中成功应用的各种价格预测技术应用于爱尔兰的平衡市场。我们使用一个框架比较了统计学、机器学习和深度学习模型，并研究了不同训练大小对模型的影响。该框架定义了超参数和校准设置，数据集和模型公开以确保可重现性，并作为未来工作的基准。大量的数值研究表明，在日前市场上表现良好的模型在平衡市场上效果不佳，突显了这些市场的不同特点。

    Short-term electricity markets are becoming more relevant due to less-predictable renewable energy sources, attracting considerable attention from the industry. The balancing market is the closest to real-time and the most volatile among them. Its price forecasting literature is limited, inconsistent and outdated, with few deep learning attempts and no public dataset. This work applies to the Irish balancing market a variety of price prediction techniques proven successful in the widely studied day-ahead market. We compare statistical, machine learning, and deep learning models using a framework that investigates the impact of different training sizes. The framework defines hyperparameters and calibration settings; the dataset and models are made public to ensure reproducibility and to be used as benchmarks for future works. An extensive numerical study shows that well-performing models in the day-ahead market do not perform well in the balancing one, highlighting that these markets ar
    
[^243]: 使用卷积神经网络进行多类实时崩溃风险预测：伊斯坦布尔案例研究

    Multi-class real-time crash risk forecasting using convolutional neural network: Istanbul case study

    [https://arxiv.org/abs/2402.06707](https://arxiv.org/abs/2402.06707)

    本文展示了在多类实时崩溃风险预测中，使用卷积神经网络从交通和天气数据中学习相关特征的性能，并提出了一种基于三个时间段的实时崩溃概率预测模型。

    

    本文展示了人工神经网络（ANN）在崩溃风险预测中的性能。首先，获取了一些交通和天气数据作为原始数据。然后对这些数据进行分析，并根据附加树和皮尔逊相关性选择相关特征作为输入数据。此外，将崩溃和非崩溃时间数据分离；然后，使用该时间段内所有可用值的平均值将崩溃和非崩溃事件的特征值写入崩溃和非崩溃事件之前的三个四分钟间隔。在根据事故标记计算每个时间段的崩溃可能性后，降低了非崩溃样本的数量。所提出的CNN模型能够从记录的、经过处理和分类的输入特征（如交通特征和气象条件）中进行学习。本文的目标是基于事件前三个时间段预测实时崩溃的可能性。

    The performance of an artificial neural network (ANN) in forecasting crash risk is shown in this paper. To begin, some traffic and weather data are acquired as raw data. This data is then analyzed, and relevant characteristics are chosen to utilize as input data based on additional tree and Pearson correlation. Furthermore, crash and non-crash time data are separated; then, feature values for crash and non-crash events are written in three four-minute intervals prior to the crash and non-crash events using the average of all available values for that period. The number of non-crash samples was lowered after calculating crash likelihood for each period based on accident labeling. The proposed CNN model is capable of learning from recorded, processed, and categorized input characteristics such as traffic characteristics and meteorological conditions. The goal of this work is to forecast the chance of a real-time crash based on three periods before events. The area under the curve (AUC) f
    
[^244]: CoRe-GD: 一种用于可扩展图可视化的层次化框架与GNNs

    CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs

    [https://arxiv.org/abs/2402.06706](https://arxiv.org/abs/2402.06706)

    CoRe-GD是一个使用GNNs的可扩展图可视化框架，通过基于中间节点位置的位置调整技术来优化图的布局，并且具有较低的时间复杂度。

    

    图可视化，也被称为图绘制，旨在找到优化某些标准的图形嵌入。应力是一种广泛使用的度量标准；当每对节点位置在它们的最短路径距离时，应力被最小化。然而，应力优化具有固有的复杂性，带来计算挑战，并且通常在实践中使用启发式算法来解决。我们引入了一种基于可扩展图神经网络（GNN）的图形绘制框架，具有次二次时间复杂度，可以学习优化应力。受传统应力优化技术和力导向布局算法的启发，我们为输入图形创建了一个粗化层次结构。从最粗糙的级别开始，我们迭代地完善和解除粗化布局，直到为原始图生成嵌入。为了增强网络内的信息传播，我们提出了一种基于中间节点位置的新型位置调整技术。我们的实证评估展示了...

    Graph Visualization, also known as Graph Drawing, aims to find geometric embeddings of graphs that optimize certain criteria. Stress is a widely used metric; stress is minimized when every pair of nodes is positioned at their shortest path distance. However, stress optimization presents computational challenges due to its inherent complexity and is usually solved using heuristics in practice. We introduce a scalable Graph Neural Network (GNN) based Graph Drawing framework with sub-quadratic runtime that can learn to optimize stress. Inspired by classical stress optimization techniques and force-directed layout algorithms, we create a coarsening hierarchy for the input graph. Beginning at the coarsest level, we iteratively refine and un-coarsen the layout, until we generate an embedding for the original graph. To enhance information propagation within the network, we propose a novel positional rewiring technique based on intermediate node positions. Our empirical evaluation demonstrates
    
[^245]: 针对私有选择的隐私配置文件

    Privacy Profiles for Private Selection

    [https://arxiv.org/abs/2402.06701](https://arxiv.org/abs/2402.06701)

    本文提出了一种易于使用的方法，通过限制基础算法的隐私配置文件来界定ReportNoisyMax和PrivateTuning的隐私配置文件。

    

    私有选择机制是差分隐私数据分析的基本原语，广泛应用于私有查询发布、投票和超参数调整等领域。最近的研究在泛化私有选择机制和使用现代数值隐私账务工具（如R\'enyi差分隐私）收敛隐私分析方面取得了重要进展。本文提出了一种易于使用的方法，通过限制基础算法的隐私配置文件来界定ReportNoisyMax和PrivateTuning的隐私配置文件。在数值上，我们的方法在所有感兴趣的区域都优于基于RDP的隐私账务方法。

    Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are fundamental primitives of differentially private (DP) data analysis with wide applications to private query release, voting, and hyperparameter tuning. Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made significant progress in both generalizing private selection mechanisms and tightening their privacy analysis using modern numerical privacy accounting tools, e.g., R\'enyi DP. But R\'enyi DP is known to be lossy when $(\epsilon,\delta)$-DP is ultimately needed, and there is a trend to close the gap by directly handling privacy profiles, i.e., $\delta$ as a function of $\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax and PrivateTuning using the privacy profiles of the base algorithms they corral. Numerically, our approach improves over the RDP-based accounting in all regimes of interest an
    
[^246]: 熵正则化的令牌级策略优化用于大规模语言模型

    Entropy-Regularized Token-Level Policy Optimization for Large Language Models

    [https://arxiv.org/abs/2402.06700](https://arxiv.org/abs/2402.06700)

    本文提出了一种熵正则化的令牌级策略优化方法（ETPO），用于优化大规模语言模型（LLMs）。该方法能够通过直接与任务特定环境进行交互，并解决在如何分配令牌级学分和最大化奖励之间的冲突问题。

    

    大规模语言模型（LLMs）在交互式决策任务中表现出了智能代理的潜力。传统方法通常依赖于精心设计的提示、高质量的示例或额外的奖励模型进行上下文学习、监督微调或RLHF。强化学习（RL）提供了一种动态的解决方案，使LLMs能够通过直接与任务特定环境进行交互来克服这些依赖关系。尽管如此，它面临着重重困难：1）由于巨大的动作空间需要探索而产生的不稳定性；2）基于动作级奖励信号分配令牌级学分的挑战，导致最大化奖励和准确建模语料库数据之间的冲突。为了应对这些挑战，我们引入了熵正则化的令牌级策略优化（ETPO），这是一种专为在令牌级优化LLMs而设计的熵增强强化学习方法。ETPO的核心是我们的一种新颖的逐令牌软Bellman更新算法，

    Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
    
[^247]: 前馈神经网络作为混合整数规划

    Feed-Forward Neural Networks as a Mixed-Integer Program

    [https://arxiv.org/abs/2402.06697](https://arxiv.org/abs/2402.06697)

    这项研究探索了将训练的修正线性单元(ReLU)神经元作为混合整数规划(MIP)的形式，并将MIP模型应用于训练神经网络。研究发现MIP技术在不同的神经网络架构中具有广泛的应用潜力，包括二进制DNN和二值化DNN。

    

    深度神经网络(DNN)在各个应用领域都得到了广泛的研究。DNN由神经元层组成，计算仿射组合，应用非线性操作，并产生相应的激活。修正的线性单元(ReLU)是一种典型的非线性运算符，输出其输入和零的最大值。在像最大池化这样涉及多个输入值的场景中，固定参数的DNN可以被建模为混合整数规划(MIP)。这种形式，使用连续变量表示单元输出和ReLU激活的二进制变量，可以在不同领域中找到应用。本研究探讨了训练的ReLU神经元作为MIP的形式，并将MIP模型应用于训练神经网络(NN)。具体而言，它研究了MIP技术和不同的NN架构之间的相互作用，包括二进制DNN(采用阶梯激活函数)和二值化DNN(权重和激活限制为$-1,0,+1$)。该研究重点关注训练前馈神经网络中的混合整数规划。

    Deep neural networks (DNNs) are widely studied in various applications. A DNN consists of layers of neurons that compute affine combinations, apply nonlinear operations, and produce corresponding activations. The rectified linear unit (ReLU) is a typical nonlinear operator, outputting the max of its input and zero. In scenarios like max pooling, where multiple input values are involved, a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This formulation, with continuous variables representing unit outputs and binary variables for ReLU activation, finds applications across diverse domains. This study explores the formulation of trained ReLU neurons as MIP and applies MIP models for training neural networks (NNs). Specifically, it investigates interactions between MIP techniques and various NN architectures, including binary DNNs (employing step activation functions) and binarized DNNs (with weights and activations limited to $-1,0,+1$). The research focuses on traini
    
[^248]: FL-NAS: 通过大型语言模型为资源受限设备实现公平的NAS

    FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models

    [https://arxiv.org/abs/2402.06696](https://arxiv.org/abs/2402.06696)

    本研究提出了一种名为FL-NAS的基于大型语言模型的神经架构搜索框架，该框架能够在模型准确性、公平性和硬件部署效率三个方面达到卓越的性能。

    

    神经架构搜索（NAS）已成为工业界自动设计深度神经网络的标准工具，尤其是对于计算资源有限的移动和边缘设备驱动的各种应用。最近，由于其卓越的性能，大型语言模型（LLM）也被纳入NAS，并显示出一些有希望的结果。本文通过同时考虑模型准确性、公平性和硬件部署效率三个重要的设计指标，进一步探索了这个方向。我们在本文中提出了一种基于LLM的NAS框架FL-NAS，并通过实验证明，FL-NAS确实能够找到性能优秀的DNN模型，几乎在所有设计考虑方面都比当前最先进的DNN模型有着数量级的提升。

    Neural Architecture Search (NAS) has become the de fecto tools in the industry in automating the design of deep neural networks for various applications, especially those driven by mobile and edge devices with limited computing resources. The emerging large language models (LLMs), due to their prowess, have also been incorporated into NAS recently and show some promising results. This paper conducts further exploration in this direction by considering three important design metrics simultaneously, i.e., model accuracy, fairness, and hardware deployment efficiency. We propose a novel LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN models by orders-of-magnitude across almost all design considerations.
    
[^249]: 在复杂系统中集成LLMs以实现可解释的故障诊断

    Integrating LLMs for Explainable Fault Diagnosis in Complex Systems

    [https://arxiv.org/abs/2402.06695](https://arxiv.org/abs/2402.06695)

    本研究介绍了一种集成系统，通过将基于物理的诊断工具与大型语言模型相结合，帮助复杂系统实现可解释的故障诊断。该系统不仅能够识别故障，还能够提供清晰易懂的故障原因和影响解释，提高自主系统的可靠性和透明度。

    

    本文介绍了一个集成系统，旨在增强复杂系统中故障诊断的可解释性，如核电站，在这些系统中，操作员的理解对于明智的决策至关重要。通过将基于物理的诊断工具与大型语言模型结合起来，我们提供了一种新颖的解决方案，不仅能够识别故障，还能清晰易懂地解释其原因和影响。通过应用于熔盐设施，展示了该系统揭示了诊断故障与传感器数据之间的联系、回答操作员的查询以及评估历史传感器异常的能力。我们的方法强调了将基于模型的诊断与先进的人工智能相结合，以提高自主系统的可靠性和透明度的重要性。

    This paper introduces an integrated system designed to enhance the explainability of fault diagnostics in complex systems, such as nuclear power plants, where operator understanding is critical for informed decision-making. By combining a physics-based diagnostic tool with a Large Language Model, we offer a novel solution that not only identifies faults but also provides clear, understandable explanations of their causes and implications. The system's efficacy is demonstrated through application to a molten salt facility, showcasing its ability to elucidate the connections between diagnosed faults and sensor data, answer operator queries, and evaluate historical sensor anomalies. Our approach underscores the importance of merging model-based diagnostics with advanced AI to improve the reliability and transparency of autonomous systems.
    
[^250]: 扩展战争游戏中的智能代理

    Scaling Intelligent Agents in Combat Simulations for Wargaming

    [https://arxiv.org/abs/2402.06694](https://arxiv.org/abs/2402.06694)

    本研究利用分层强化学习提升战争游戏中智能代理的性能，以加速决策速度和提高决策质量。

    

    为了在未来与技术先进的竞争对手的冲突中保持竞争力，我们需要加速研究和开发战争游戏中的人工智能（AI）。更重要的是，利用机器学习来开发智能战斗行为将成为未来实现超人类水平表现的关键，提升我们在未来战争中的决策质量和加速速度。尽管深度强化学习（RL）在游戏中智能代理行为开发方面继续显示出有希望的结果，但在长期、复杂的任务中，特别是在战斗建模和仿真中，它尚未达到或超过人类水平。借鉴RL的已证明潜力和最近分层强化学习（HRL）的成功，我们的研究正在探索并扩展HRL的使用，以创建能够在这些大规模复杂模拟环境中有效执行的智能代理。我们的最终目标是

    Remaining competitive in future conflicts with technologically-advanced competitors requires us to accelerate our research and development in artificial intelligence (AI) for wargaming. More importantly, leveraging machine learning for intelligent combat behavior development will be key to one day achieving superhuman performance in this domain--elevating the quality and accelerating the speed of our decisions in future wars. Although deep reinforcement learning (RL) continues to show promising results in intelligent agent behavior development in games, it has yet to perform at or above the human level in the long-horizon, complex tasks typically found in combat modeling and simulation. Capitalizing on the proven potential of RL and recent successes of hierarchical reinforcement learning (HRL), our research is investigating and extending the use of HRL to create intelligent agents capable of performing effectively in these large and complex simulation environments. Our ultimate goal is
    
[^251]: HistoHDR-Net：用于单个LDR到HDR图像转换的直方图均衡化

    HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation

    [https://arxiv.org/abs/2402.06692](https://arxiv.org/abs/2402.06692)

    HistoHDR-Net是一种用于单个LDR到HDR图像转换的简单而有效的方法，通过融合直方图均衡化的LDR图像和自注意力引导，恢复HDR图像的细节。

    

    高动态范围（HDR）成像旨在复制真实场景的高视觉质量和清晰度。由于HDR成像的高成本，文献中提出了各种数据驱动方法用于从低动态范围（LDR）图像重构HDR图像。这些方法的一个常见限制是在重构的HDR图像中缺失的细节，这些细节在输入的LDR图像中过曝或曝光不足。为此，我们提出了一种简单而有效的方法HistoHDR-Net，通过融合基于直方图均衡的LDR图像和自注意力引导，恢复HDR图像的细节（例如颜色，对比度，饱和度和亮度）。我们的实验证明了该方法在现有方法上的有效性。

    High Dynamic Range (HDR) imaging aims to replicate the high visual quality and clarity of real-world scenes. Due to the high costs associated with HDR imaging, the literature offers various data-driven methods for HDR image reconstruction from Low Dynamic Range (LDR) counterparts. A common limitation of these approaches is missing details in regions of the reconstructed HDR images, which are over- or under-exposed in the input LDR images. To this end, we propose a simple and effective method, HistoHDR-Net, to recover the fine details (e.g., color, contrast, saturation, and brightness) of HDR images via a fusion-based approach utilizing histogram-equalized LDR images along with self-attention guidance. Our experiments demonstrate the efficacy of the proposed approach over the state-of-art methods.
    
[^252]: 源代码合成和补全的神经模型

    Neural Models for Source Code Synthesis and Completion

    [https://arxiv.org/abs/2402.06690](https://arxiv.org/abs/2402.06690)

    本论文提出了一种基于序列到序列的深度学习模型和训练方法，用于将自然语言转化为可编译的代码片段，并为开发人员提供源代码建议和自动补全功能。这一方法能够提取开发者的编码意图并准确推断类型、名称和上下文等信息。

    

    自然语言（NL）到代码建议系统通过将NL表达转化为可编译的代码片段来帮助集成开发环境（IDE）中的开发人员。当前的方法主要涉及基于语义解析的硬编码、规则系统。这些系统主要依靠手工制定的规则将NL的模式或其语法解析树中的元素映射到各种查询结构，并且只能处理受限制的NL子集和限制的NL语法。这些系统无法从开发者的编码意图中提取语义信息，常常无法推断类型、名称和源代码的上下文以获得准确的系统级代码建议。在本硕士论文中，我们提出了序列到序列的深度学习模型和训练范式，以将NL映射到通用编程语言，可以根据NL的意图为用户提供源代码片段的建议，并扩展源代码的自动补全功能。

    Natural language (NL) to code suggestion systems assist developers in Integrated Development Environments (IDEs) by translating NL utterances into compilable code snippet. The current approaches mainly involve hard-coded, rule-based systems based on semantic parsing. These systems make heavy use of hand-crafted rules that map patterns in NL or elements in its syntax parse tree to various query constructs and can only work on a limited subset of NL with a restricted NL syntax. These systems are unable to extract semantic information from the coding intents of the developer, and often fail to infer types, names, and the context of the source code to get accurate system-level code suggestions. In this master thesis, we present sequence-to-sequence deep learning models and training paradigms to map NL to general-purpose programming languages that can assist users with suggestions of source code snippets, given a NL intent, and also extend auto-completion functionality of the source code to
    
[^253]: 使用深度学习和统计模型进行股票预测的研究

    A Study on Stock Forecasting Using Deep Learning and Statistical Models

    [https://arxiv.org/abs/2402.06689](https://arxiv.org/abs/2402.06689)

    本研究使用深度学习和统计模型进行股票预测，通过回顾多个算法并使用s&p 500指数数据进行训练和测试，旨在找到最佳的股票价格预测方法。

    

    预测股票价格的快速准确模型一直是一个具有挑战性的任务，这是一个尚未找到最佳预测股票价格的方法的研究领域。本文使用机器学习、深度学习和统计分析技术来获取准确的结果，以便投资者可以看到未来的趋势，并最大化股票交易的回报。本文将回顾许多用于股票价格预测的深度学习算法。我们使用s&p 500指数数据进行训练和测试。本调查的目的是检查用于股票价格预测的各种深度学习和统计模型技术，包括统计技术中的移动平均法、ARIMA模型，以及深度学习模型中的LSTM、RNN、CNN和全卷积神经网络模型。将讨论各种模型，包括自回归积分滑动平均模型、循环神经网络模型、长短期记忆模型。

    Predicting a fast and accurate model for stock price forecasting is been a challenging task and this is an active area of research where it is yet to be found which is the best way to forecast the stock price. Machine learning, deep learning and statistical analysis techniques are used here to get the accurate result so the investors can see the future trend and maximize the return of investment in stock trading. This paper will review many deep learning algorithms for stock price forecasting. We use a record of s&p 500 index data for training and testing. The survey motive is to check various deep learning and statistical model techniques for stock price forecasting that are Moving Averages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL CNN which are deep learning models. It will discuss various models, including the Auto regression integration moving average model, the Recurrent neural network model, the long short-term model which is the type of RNN used for lo
    
[^254]: 机器学习和统计方法在数字高程模型（DEM）校正中的比较：中期结果

    Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results

    [https://arxiv.org/abs/2402.06688](https://arxiv.org/abs/2402.06688)

    本文比较了机器学习和统计方法在数字高程模型（DEM）校正中的性能，结果表明梯度提升决策树（GBDT）在提高DEM垂直精度方面具有潜力。

    

    已经提出了几种方法来纠正数字高程模型（DEM）中的高程偏差，例如线性回归。如今，监督式机器学习可以对变量之间的复杂关系进行建模，并已被研究人员在各个领域中应用。在现有文献中，有几项研究采用了机器学习或统计方法来进行DEM校正的任务。然而，据我们所知，这些研究中没有一项比较了这两种方法的性能，特别是针对开放获取的全球DEM数据。我们以前的工作已经展示了机器学习方法的潜力，特别是梯度提升决策树（GBDT）在DEM校正中的应用。在本研究中，我们分享了对比三种最近实施的梯度提升决策树（XGBoost，LightGBM和CatBoost）和多元线性回归（MLR）的结果，以提高30米的Copernicus DEM的垂直精度。

    Several methods have been proposed for correcting the elevation bias in digital elevation models (DEMs) for example, linear regression. Nowadays, supervised machine learning enables the modelling of complex relationships between variables, and has been deployed by researchers in a variety of fields. In the existing literature, several studies have adopted either machine learning or statistical approaches in the task of DEM correction. However, to our knowledge, none of these studies have compared the performance of both approaches, especially with regard to open-access global DEMs. Our previous work has already shown the potential of machine learning approaches, specifically gradient boosted decision trees (GBDTs) for DEM correction. In this study, we share some results from the comparison of three recent implementations of gradient boosted decision trees (XGBoost, LightGBM and CatBoost), versus multiple linear regression (MLR) for enhancing the vertical accuracy of 30 m Copernicus and
    
[^255]: Ai4Fapar：人工智能如何帮助预测季节性的地球观测信号

    Ai4Fapar: How artificial intelligence can help to forecast the seasonal earth observation signal

    [https://arxiv.org/abs/2402.06684](https://arxiv.org/abs/2402.06684)

    本文研究了使用多元变压器模型预测欧洲和北非地区FAPAR时序轨迹的潜力。结果显示，在一个月的预测范围内，变压器模型优于基准模型，尤其当与天气数据结合使用时效果更好。

    

    本文研究了多元变压器模型在欧洲和北非地区以区域级别预测光合有效辐射吸收率（FAPAR）的时间轨迹的潜力。输入数据涵盖了2002年至2022年的时期，并包括遥感和天气数据用于模拟FAPAR预测。使用一年留一次交叉验证对模型进行了评估，并与气候基准进行了比较。结果表明，变压器模型在一个月的预测范围内优于基准模型，之后气候基准更好。变压器模型的RMSE值在前两个月的预测中为0.02至0.04 FAPAR单位。总的来说，经过测试的变压器模型是一种有效的FAPAR预测方法，特别是在结合天气数据并用于短期预测时。

    This paper investigated the potential of a multivariate Transformer model to forecast the temporal trajectory of the Fraction of Absorbed Photosynthetically Active Radiation (FAPAR) for short (1 month) and long horizon (more than 1 month) periods at the regional level in Europe and North Africa. The input data covers the period from 2002 to 2022 and includes remote sensing and weather data for modelling FAPAR predictions. The model was evaluated using a leave one year out cross-validation and compared with the climatological benchmark. Results show that the transformer model outperforms the benchmark model for one month forecasting horizon, after which the climatological benchmark is better. The RMSE values of the transformer model ranged from 0.02 to 0.04 FAPAR units for the first 2 months of predictions. Overall, the tested Transformer model is a valid method for FAPAR forecasting, especially when combined with weather data and used for short-term predictions.
    
[^256]: 使用潜在变分块分离的声源分离

    Sound Source Separation Using Latent Variational Block-Wise Disentanglement

    [https://arxiv.org/abs/2402.06683](https://arxiv.org/abs/2402.06683)

    本文提出了一种混合经典数字信号处理/深度神经网络（DSP / DNN）方法，用于声源分离。通过在合适设计的潜在空间中处理分离任务，将单通道欠定分离任务转换为多通道过定问题。实验证明，该方法在面对未见过的分布时具有鲁棒性。

    

    虽然神经网络方法在解决经典信号处理问题方面取得了重大进展，但往往利用信号处理和神经网络两者的洞察力的混合方法能够提供更全面的解决方案。在本文中，我们提出了一种混合经典数字信号处理/深度神经网络（DSP / DNN）方法，用于源分离（SS），并强调了变分自编码器与传统SS方法之间的理论联系。我们提出了一种将单通道欠定SS任务转换为等效多通道过定SS问题的系统，在适当设计的潜在空间中处理混合物的分离任务。在潜在空间中的分离任务被视为寻找混合物的变分块分离表示。我们通过实验证明，根据经典信号处理理论结果的设计选择和变分公式，可以获得对未见过的分布具有鲁棒性的结果。

    While neural network approaches have made significant strides in resolving classical signal processing problems, it is often the case that hybrid approaches that draw insight from both signal processing and neural networks produce more complete solutions. In this paper, we present a hybrid classical digital signal processing/deep neural network (DSP/DNN) approach to source separation (SS) highlighting the theoretical link between variational autoencoder and classical approaches to SS. We propose a system that transforms the single channel under-determined SS task to an equivalent multichannel over-determined SS problem in a properly designed latent space. The separation task in the latent space is treated as finding a variational block-wise disentangled representation of the mixture. We show empirically, that the design choices and the variational formulation of the task at hand motivated by the classical signal processing theoretical results lead to robustness to unseen out-of-distrib
    
[^257]: 分布式学习中的私人知识共享：一项调查研究

    Private Knowledge Sharing in Distributed Learning: A Survey

    [https://arxiv.org/abs/2402.06682](https://arxiv.org/abs/2402.06682)

    这篇论文提供了关于分布式学习中的私人知识共享的全面调查，分析了在领先的分布式学习架构中使用的各种知识组件，旨在揭示现有的解决方法和未来的研究方向。

    

    人工智能（AI）的崛起已经彻底改变了许多行业，并改变了社会的运作方式。其广泛使用导致了AI和其底层数据在许多智能系统中的分布。在这种情况下，利用分布式或由不同实体拥有的学习过程中的信息至关重要。因此，现代数据驱动的服务已经开发出将分布式知识实体整合到其结果中的方法。为了实现这个目标，最新的AI模型经常被以分散式的方式进行训练。分布式学习涉及多个实体共同进行预测和决策。然而，这种合作也可能带来安全漏洞和挑战。本文提供了关于分布式学习中的私人知识共享的深入调查，考察了在领先的分布式学习架构中使用的各种知识组件。我们的分析揭示了解决这些挑战的现有方法和未来的研究方向。

    The rise of Artificial Intelligence (AI) has revolutionized numerous industries and transformed the way society operates. Its widespread use has led to the distribution of AI and its underlying data across many intelligent systems. In this light, it is crucial to utilize information in learning processes that are either distributed or owned by different entities. As a result, modern data-driven services have been developed to integrate distributed knowledge entities into their outcomes. In line with this goal, the latest AI models are frequently trained in a decentralized manner. Distributed learning involves multiple entities working together to make collective predictions and decisions. However, this collaboration can also bring about security vulnerabilities and challenges. This paper provides an in-depth survey on private knowledge sharing in distributed learning, examining various knowledge components utilized in leading distributed learning architectures. Our analysis sheds light
    
[^258]: 基于社会物理的扩散模型用于人群模拟

    Social Physics Informed Diffusion Model for Crowd Simulation

    [https://arxiv.org/abs/2402.06680](https://arxiv.org/abs/2402.06680)

    本文提出了一种叫做SPDiff的基于社会物理的扩散模型，用于人群模拟。模型综合考虑了人群的互动和历史信息，通过逆向扩散过程生成下一个时间段内行人运动的分布。此外，借鉴社会力模型，并利用人群互动的等变性属性增强了模型的性能。为了解决长期模拟中的误差累积问题，引入了多层次扩散模型和最小二乘法进行参数估计。

    

    人群模拟在城市规划、建筑设计和交通安排等领域具有重要应用。近年来，基于物理启发的机器学习方法在人群模拟中取得了最先进的性能，但未能全面建模人类运动的异质性和多模态性。在本文中，我们提出了一种名为SPDiff的社会物理启发的扩散模型，以弥补上述差距。SPDiff同时考虑了当前时间段内人群的互动和历史信息，通过逆向扩散过程生成下一个时间段内行人运动的分布。受社会力模型（Social Force）中人群动力学的启发，我们设计了一个人群互动模块来指导去噪过程，并通过人群互动的等变性属性进一步增强该模块。为了减轻长期模拟中的误差累积问题，我们提出了一个多层次的扩散模型，并使用最小二乘法进行模型参数估计。

    Crowd simulation holds crucial applications in various domains, such as urban planning, architectural design, and traffic arrangement. In recent years, physics-informed machine learning methods have achieved state-of-the-art performance in crowd simulation but fail to model the heterogeneity and multi-modality of human movement comprehensively. In this paper, we propose a social physics-informed diffusion model named SPDiff to mitigate the above gap. SPDiff takes both the interactive and historical information of crowds in the current timeframe to reverse the diffusion process, thereby generating the distribution of pedestrian movement in the subsequent timeframe. Inspired by the well-known social physics model, i.e., Social Force, regarding crowd dynamics, we design a crowd interaction module to guide the denoising process and further enhance this module with the equivariant properties of crowd interactions. To mitigate error accumulation in long-term simulations, we propose a multi-f
    
[^259]: 机器学习能否预测公民报告的钓鱼者行为？

    Can machine learning predict citizen-reported angler behavior?

    [https://arxiv.org/abs/2402.06678](https://arxiv.org/abs/2402.06678)

    本研究利用机器学习方法，使用环境、社会经济、渔业管理目标和事件等辅助数据，预测公民报告的钓鱼者行为。结果表明，辅助数据能够以较高准确度预测特定地区和单个水域的钓鱼者行为。

    

    预测钓鱼者行为，例如捕获率和钓鱼压力，对于保持鱼类种群和确保钓鱼者满意度至关重要。钓鱼者行为可以部分通过在线平台和手机应用程序进行跟踪，这些平台和应用程序提供了休闲钓鱼者报告的钓鱼活动。此外，已知钓鱼者行为受到当地站点特征的影响。本研究使用机器学习方法，利用环境、社会经济、渔业管理目标和淡水体的事件等辅助数据，研究了公民报告的钓鱼者行为的预测。目标是确定仅凭辅助数据是否能够预测报告的行为。考虑了不同的空间和时间范围以及时间分辨率。在加拿大单个水域的月度预测中，准确度得分平均为88%，在特定地区的一天内进行的空间预测的准确度得分为86%。在其他分辨率和尺度上，模型只能达到很低的预测准确度。

    Prediction of angler behaviors, such as catch rates and angler pressure, is essential to maintaining fish populations and ensuring angler satisfaction. Angler behavior can partly be tracked by online platforms and mobile phone applications that provide fishing activities reported by recreational anglers. Moreover, angler behavior is known to be driven by local site attributes. Here, the prediction of citizen-reported angler behavior was investigated by machine-learning methods using auxiliary data on the environment, socioeconomics, fisheries management objectives, and events at a freshwater body. The goal was to determine whether auxiliary data alone could predict the reported behavior. Different spatial and temporal extents and temporal resolutions were considered. Accuracy scores averaged 88% for monthly predictions at single water bodies and 86% for spatial predictions on a day in a specific region across Canada. At other resolutions and scales, the models only achieved low predict
    
[^260]: 使用多源电子健康记录轨迹的掩码语言模型进行背景表示学习

    A Masked language model for multi-source EHR trajectories contextual representation learning

    [https://arxiv.org/abs/2402.06675](https://arxiv.org/abs/2402.06675)

    通过使用多源电子健康记录轨迹并训练模型来解决疾病和干预之间相互作用的问题的背景表示学习。

    

    使用电子健康记录数据和机器学习指导未来决策需要解决以下挑战，包括 1) 长/短期依赖和 2) 疾病和干预之间的相互作用。双向变压器已经有效地解决了第一个挑战。我们在这里通过遮掩一个源 (例如 ICD10 代码) 并训练变压器使用其他源 (例如 ATC 代码) 预测它来解决后一个挑战。

    Using electronic health records data and machine learning to guide future decisions needs to address challenges, including 1) long/short-term dependencies and 2) interactions between diseases and interventions. Bidirectional transformers have effectively addressed the first challenge. Here we tackled the latter challenge by masking one source (e.g., ICD10 codes) and training the transformer to predict it using other sources (e.g., ATC codes).
    
[^261]: 理解深度学习的实际成员隐私

    Understanding Practical Membership Privacy of Deep Learning

    [https://arxiv.org/abs/2402.06674](https://arxiv.org/abs/2402.06674)

    该论文利用最先进的成员推理攻击方法系统地测试了细调大型图像分类模型的实际隐私漏洞，并发现数据集中每个类别的示例数量以及训练结束时的大梯度与成员推理攻击的漏洞之间存在关联。

    

    我们应用最先进的成员推理攻击（MIA）来系统地测试细调大型图像分类模型的实际隐私漏洞。我们的重点是理解使数据集和样本容易受到成员推理攻击的特性。在数据集特性方面，我们发现数据中每个类别的示例数量与成员推理攻击的漏洞之间存在强烈的幂律依赖关系，这是以攻击的真阳性率（在低假阳性率下测量）来衡量的。对于个别样本而言，在训练结束时产生的大梯度与成员推理攻击的漏洞之间存在很强的相关性。

    We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.
    
[^262]: 基于真实预测过程指导的扩散天气预测

    Weather Prediction with Diffusion Guided by Realistic Forecast Processes

    [https://arxiv.org/abs/2402.06666](https://arxiv.org/abs/2402.06666)

    这种新方法利用扩散模型进行天气预测，能够在相同的建模框架下实现直接和迭代预测，并且可以嵌入NWP预测，提高可信赖性和预测性能。

    

    天气预测是一个至关重要但具有挑战性的领域，最近基于深度学习（DL）的模型已经接近传统数值天气预报（NWP）模型的性能。然而，这些DL模型通常复杂且资源密集，面临着在训练后灵活性有限和整合NWP预测方面的限制，由此可能导致不真实的预测而引起可靠性问题。为此，我们引入了一种新的方法，利用扩散模型（DM）进行天气预测。特别是，我们的方法能够在相同的建模框架下实现直接和迭代预测。我们的模型不仅能够独立生成预测，还可以在采样过程中嵌入NWP预测，甚至可以适应不同的提前期。我们模型的灵活性和可控性为广大天气社区提供了更加可信赖的DL系统。另外，将持续性预测融入我们的方法可以改善预测性能。

    Weather forecasting remains a crucial yet challenging domain, where recently developed models based on deep learning (DL) have approached the performance of traditional numerical weather prediction (NWP) models. However, these DL models, often complex and resource-intensive, face limitations in flexibility post-training and in incorporating NWP predictions, leading to reliability concerns due to potential unphysical predictions. In response, we introduce a novel method that applies diffusion models (DM) for weather forecasting. In particular, our method can achieve both direct and iterative forecasting with the same modeling framework. Our model is not only capable of generating forecasts independently but also uniquely allows for the integration of NWP predictions, even with varying lead times, during its sampling process. The flexibility and controllability of our model empowers a more trustworthy DL system for the general weather community. Additionally, incorporating persistence an
    
[^263]: 基于因果关系的基础世界模型在具身人工智能中的重要作用

    The Essential Role of Causality in Foundation World Models for Embodied AI

    [https://arxiv.org/abs/2402.06665](https://arxiv.org/abs/2402.06665)

    基于因果关系的基础世界模型对于具身人工智能的发展至关重要，当前的基础模型无法准确建模与现实世界的物理相互作用。因果关系的研究有助于构建真实世界模型，提高对可能相互作用结果的准确预测能力。

    

    最近在基础模型中取得的进展，尤其是在大型多模态模型和对话代理方面，引发了对具备普遍能力的具身代理人潜力的兴趣。这样的代理人需要能够在许多不同的真实世界环境中执行新任务。然而，当前的基础模型未能准确建模与现实世界的物理相互作用，因此对于具身人工智能而言是不够的。因果关系的研究有助于构建真实世界模型，这对于准确预测可能相互作用的结果至关重要。本文着重探讨了为即将到来的具身代理生成基础世界模型的前景，并对其中的因果关系的重要性提出了新的观点。我们认为整合因果关系是促进与世界的有意义的物理相互作用至关重要的。最后，我们揭示了这一背景下对因果关系的误解，并展示了我们对未来的展望。

    Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook fo
    
[^264]: 基于注意力的图解码器的符号秩限制

    Sign Rank Limitations for Attention-Based Graph Decoders

    [https://arxiv.org/abs/2402.06662](https://arxiv.org/abs/2402.06662)

    该论文研究了基于注意力的图解码器在表征容量方面的限制问题，并提出了可以解决该问题的简单修正方案，而不改变内积框架。

    

    内积型解码器是提取潜在嵌入数据中有意义信息的最有影响力的框架之一。然而，这种解码器在表征容量方面存在限制，在图重建问题中尤为明显。本文首次在图数据中提供了对这一普遍现象的理论阐述，并提出了直接修改方案以解决该问题，且不脱离内积框架。

    Inner product-based decoders are among the most influential frameworks used to extract meaningful data from latent embeddings. However, such decoders have shown limitations in representation capacity in numerous works within the literature, which have been particularly notable in graph reconstruction problems. In this paper, we provide the first theoretical elucidation of this pervasive phenomenon in graph data, and suggest straightforward modifications to circumvent this issue without deviating from the inner product framework.
    
[^265]: 通过多媒体容器结构分析验证和保证智能手机视频的真实性和完整性

    Authentication and integrity of smartphone videos through multimedia container structure analysis

    [https://arxiv.org/abs/2402.06661](https://arxiv.org/abs/2402.06661)

    通过分析多媒体容器结构，本论文研究了智能手机视频的身份验证和完整性，以提高在社交网络和通信应用程序中共享视频的安全性和可信度。

    

    如今，移动设备已经成为数码相机的自然替代品，因为它们能够轻松快捷地捕捉日常情景，鼓励用户通过图像和视频来表达自己。这些视频可以通过不同的平台进行共享，从而暴露在犯罪分子的意图操纵之下，犯罪分子了解现有法医技术在司法过程中对无罪者进行指控或洗脱有罪者的弱点。通常情况下，制造商并不完全符合视频创建标准的规范。此外，分享在社交网络和即时通信应用程序中的视频经过滤波和压缩处理，以减小其大小、方便传输并优化存储在平台上。平台对规范和转换结果的疏漏嵌入了视频的多媒体容器中的特征模式。这些模式可用于区分视频的来源和处理历史。

    Nowadays, mobile devices have become the natural substitute for the digital camera, as they capture everyday situations easily and quickly, encouraging users to express themselves through images and videos. These videos can be shared across different platforms exposing them to any kind of intentional manipulation by criminals who are aware of the weaknesses of forensic techniques to accuse an innocent person or exonerate a guilty person in a judicial process. Commonly, manufacturers do not comply 100% with the specifications of the standards for the creation of videos. Also, videos shared on social networks, and instant messaging applications go through filtering and compression processes to reduce their size, facilitate their transfer, and optimize storage on their platforms. The omission of specifications and results of transformations carried out by the platforms embed a features pattern in the multimedia container of the videos. These patterns make it possible to distinguish the br
    
[^266]: Shadowcast: 隐秘的数据污染攻击对抗视觉语言模型

    Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models

    [https://arxiv.org/abs/2402.06659](https://arxiv.org/abs/2402.06659)

    Shadowcast是一种隐秘的数据污染攻击方法，可以通过伪装成良性图像和匹配文本来操纵视觉语言模型的响应。它包括标签攻击和说服攻击，可以混淆类别标签并编写有说服力的描述。使用仅50个毒样本，Shadowcast能够高效实现攻击者的意图。

    

    视觉语言模型（VLM）能够从视觉输入中生成文本响应，然而它们的多功能性带来了重大的安全隐患。本研究首次揭示了VLM对数据污染攻击的易受性，这些攻击可以操纵对无害的日常提示的响应。我们引入了一种名为Shadowcast的隐秘数据污染攻击方法，其中毒样本在视觉上与具有匹配文本的良性图像难以区分。Shadowcast在两种攻击类型中展示出了有效性。第一种是标签攻击，使VLM误识别类别标签，例如混淆唐纳德·特朗普和乔·拜登等人。第二种是说服攻击，利用VLM的文本生成能力来编写故事，例如通过有说服力和看似合理的描述将垃圾食品描绘成健康食品。我们展示了Shadowcast使用仅50个毒样本就能高度有效地实现攻击者的意图。此外，这些毒样本仍然保持有效。

    Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, yet their versatility raises significant security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack method where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages VLMs' text generation capabilities to craft narratives, such as portraying junk food as health food, through persuasive and seemingly rational descriptions. We show that Shadowcast are highly effective in achieving attacker's intentions using as few as 50 poison samples. Moreover, these poison samples remain eff
    
[^267]: DiffsFormer: Diffusion Transformer在股票因子增强上的应用

    DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation

    [https://arxiv.org/abs/2402.06656](https://arxiv.org/abs/2402.06656)

    DiffsFormer是一种利用Diffusion Transformer和人工智能生成样本的方法，用于在股票预测中解决数据稀缺性和数据同质性的问题。

    

    机器学习模型在各种股票预测任务中展示出了显著的效果和效率。然而，数据稀缺性带来的困难，如低信噪比和数据同质性，对准确预测构成了重大障碍。为了解决这个问题，我们提出了一种新颖的方法，利用人工智能生成的样本(AIGS)来增强训练过程。在我们的工作中，我们引入了Diffusion Model来生成具有Transformer架构的股票因子(DiffsFormer)。DiffsFormer首先在大规模源领域上进行训练，结合条件指导以捕捉全局联合分布。在特定的下游任务中，我们使用DiffsFormer来通过编辑现有样本来增强训练过程。这个编辑步骤允许我们控制编辑过程的强度，确定生成数据与目标领域的偏离程度。

    Machine learning models have demonstrated remarkable efficacy and efficiency in a wide range of stock forecasting tasks. However, the inherent challenges of data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity, pose significant obstacles to accurate forecasting. To address this issue, we propose a novel approach that utilizes artificial intelligence-generated samples (AIGS) to enhance the training procedures. In our work, we introduce the Diffusion Model to generate stock factors with Transformer architecture (DiffsFormer). DiffsFormer is initially trained on a large-scale source domain, incorporating conditional guidance so as to capture global joint distribution. When presented with a specific downstream task, we employ DiffsFormer to augment the training procedure by editing existing samples. This editing step allows us to control the strength of the editing process, determining the extent to which the generated data deviates from the target domain. To evalu
    
[^268]: 对抗性文本净化：一种基于大型语言模型的防御方法

    Adversarial Text Purification: A Large Language Model Approach for Defense

    [https://arxiv.org/abs/2402.06655](https://arxiv.org/abs/2402.06655)

    本文研究了防御文本分类器中对抗性净化方法的有效性，并提出了一种基于大型语言模型加以净化的方法。

    

    对抗性净化是一种防御机制，用于保护分类器免受对抗性攻击，而无需了解攻击类型或分类器的训练。这些技术对被攻击输入进行特征化和消除对抗性扰动，旨在恢复出与最初被攻击的输入相似且被分类器正确分类的净化样本。由于离散输入的噪声扰动特征化所带来的固有挑战，对抗性文本净化一直相对未被探索。在本文中，我们研究了对抗性净化方法在保护文本分类器中的有效性。我们提出了一种新颖的对抗性文本净化方法，利用大型语言模型（LLMs）的生成能力来净化对抗性文本，而无需明确特征化离散噪声扰动。我们利用提示工程来利用LLMs恢复净化的示例。

    Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified examples for
    
[^269]: 利用遥感数据进行空气污染评估

    Using remotely sensed data for air pollution assessment

    [https://arxiv.org/abs/2402.06653](https://arxiv.org/abs/2402.06653)

    该研究利用遥感数据和机器学习模型创建了一个可以在无观测数据地点推断污染物浓度的模型，为空气污染研究和排放监测提供了重要手段。

    

    空气污染是一个全球性的重要问题，不仅影响人类健康，还影响环境。有关污染物浓度的时空数据的存在对于进行空气污染研究和监测排放至关重要。然而，尽管观测数据具有较大的时间覆盖范围，但测站的数量非常有限，通常建在人口密集地区。本研究的主要目标是创建能够推断无观测数据地点的污染物浓度的模型。采用机器学习模型，更具体地说是随机森林模型，对2019年伊比利亚半岛上的五种选择性污染物（$NO_2$，$O_3$，$SO_2$，$PM10$和$PM2.5$）的浓度进行预测。模型特征包括卫星测量数据、气象变量、土地利用分类、时间变量（月份、年中日期）和空间变量（纬度、经度、海拔）。

    Air pollution constitutes a global problem of paramount importance that affects not only human health, but also the environment. The existence of spatial and temporal data regarding the concentrations of pollutants is crucial for performing air pollution studies and monitor emissions. However, although observation data presents great temporal coverage, the number of stations is very limited and they are usually built in more populated areas.   The main objective of this work is to create models capable of inferring pollutant concentrations in locations where no observation data exists. A machine learning model, more specifically the random forest model, was developed for predicting concentrations in the Iberian Peninsula in 2019 for five selected pollutants: $NO_2$, $O_3$ $SO_2$, $PM10$, and $PM2.5$. Model features include satellite measurements, meteorological variables, land use classification, temporal variables (month, day of year), and spatial variables (latitude, longitude, altit
    
[^270]: 基于扩散模型的概率降尺度方法用于180年东亚气候重建

    Diffusion Model-based Probabilistic Downscaling for 180-year East Asian Climate Reconstruction

    [https://arxiv.org/abs/2402.06646](https://arxiv.org/abs/2402.06646)

    这项研究介绍了一种基于扩散模型的概率降尺度方法（DPDM），可以高效地将数据从1°降尺度到0.1°分辨率，并生成大量的集合成员以评估降尺度的不确定性。该方法应用于生成东亚地区180年的月表面变量数据集，提供了更详细的观察。

    

    随着地球进入“全球加热”时代，了解区域气候变化变得迫在眉睫。提供局部洞察的有效降尺度方法对于实现这一目标至关重要。传统方法，包括计算密集型的区域动力模型或统计降尺度框架，往往容易受到降尺度不确定性的影响。在这里，我们通过将扩散概率降尺度模型（DPDM）引入气象领域来解决这些限制。该模型可以高效地将数据从1°降尺度到0.1°分辨率。与确定性降尺度方案相比，它不仅具有更准确的局部细节，还可以根据概率分布采样生成大量的集合成员，以评估降尺度的不确定性。此外，我们将该模型应用于生成东亚地区180年的月表面变量数据集，为了提供更详细的视角。

    As our planet is entering into the "global boiling" era, understanding regional climate change becomes imperative. Effective downscaling methods that provide localized insights are crucial for this target. Traditional approaches, including computationally-demanding regional dynamical models or statistical downscaling frameworks, are often susceptible to the influence of downscaling uncertainty. Here, we address these limitations by introducing a diffusion probabilistic downscaling model (DPDM) into the meteorological field. This model can efficiently transform data from 1{\deg} to 0.1{\deg} resolution. Compared with deterministic downscaling schemes, it not only has more accurate local details, but also can generate a large number of ensemble members based on probability distribution sampling to evaluate the uncertainty of downscaling. Additionally, we apply the model to generate a 180-year dataset of monthly surface variables in East Asia, offering a more detailed perspective for unde
    
[^271]: 从GARCH到神经网络的波动率预测

    From GARCH to Neural Network for Volatility Forecast

    [https://arxiv.org/abs/2402.06642](https://arxiv.org/abs/2402.06642)

    该论文通过建立GARCH模型和神经网络模型之间的等价关系，提出了一种名为GARCH-NN的创新方法，用于构建基于神经网络的波动率模型，并将GARCH模型中的波动率风格化事实融入到神经网络中。

    

    波动率作为不确定性的度量，在风险管理等众多金融活动中扮演着至关重要的角色。计量经济学和机器学习界已经发展出了两种不同的金融波动率预测方法：随机方法和神经网络（NN）方法。尽管它们各自具有优势，但这些方法通常在不同的研究轨迹中发展，相互之间有很少的交互。本研究通过建立GARCH家族模型和相应的神经网络模型之间的等价关系来弥合这一差距。通过建立等价关系，我们引入了一种创新的方法，名为GARCH-NN，用于构建基于神经网络的波动率模型。它获取GARCH模型的神经网络对应模型，并将其作为组件整合到已建立的神经网络结构中，从而将GARCH模型中固有的波动率风格化事实（SFs）无缝融入到神经网络中。

    Volatility, as a measure of uncertainty, plays a crucial role in numerous financial activities such as risk management. The Econometrics and Machine Learning communities have developed two distinct approaches for financial volatility forecasting: the stochastic approach and the neural network (NN) approach. Despite their individual strengths, these methodologies have conventionally evolved in separate research trajectories with little interaction between them. This study endeavors to bridge this gap by establishing an equivalence relationship between models of the GARCH family and their corresponding NN counterparts. With the equivalence relationship established, we introduce an innovative approach, named GARCH-NN, for constructing NN-based volatility models. It obtains the NN counterparts of GARCH models and integrates them as components into an established NN architecture, thereby seamlessly infusing volatility stylized facts (SFs) inherent in the GARCH models into the neural network
    
[^272]: 使用注意力联合聚合的Transformer模型在时间序列股票预测中的应用

    Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting

    [https://arxiv.org/abs/2402.06638](https://arxiv.org/abs/2402.06638)

    本文提出了一种使用注意力联合聚合的Transformer模型，用于时间序列股票预测，旨在解决传统训练方案中存在的过拟合、数据稀缺和隐私问题。

    

    近期在自然语言处理（NLP）和计算机视觉（CV）领域，Transformer模型的创新展示了其优越的性能。Transformer模型具备捕捉序列数据中长距离依赖关系和相互作用的能力，因此在时间序列建模领域引起了极大的兴趣，并广泛地应用于许多时间序列应用中。然而，在应用到时间序列预测中，尽管有希望的结果，但Transformer模型的适应性仍然存在限制。与NLP和CV中的挑战相比，时间序列问题不仅涉及到输入序列中的顺序或时间依赖性的复杂性，还需要考虑趋势、水平和季节性信息，这些信息对于决策非常重要。传统的训练方案在使用Transformer模型进行预测任务时存在过拟合、数据稀缺和隐私问题等不足之处。本文中，我们提出了一种使用注意力联合聚合的Transformer模型，用于解决时间序列股票预测问题。

    Recent innovations in transformers have shown their superior performance in natural language processing (NLP) and computer vision (CV). The ability to capture long-range dependencies and interactions in sequential data has also triggered a great interest in time series modeling, leading to the widespread use of transformers in many time series applications. However, being the most common and crucial application, the adaptation of transformers to time series forecasting has remained limited, with both promising and inconsistent results. In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making. The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task. In this work, we pro
    
[^273]: 大型（和深度）因子模型

    Large (and Deep) Factor Models

    [https://arxiv.org/abs/2402.06635](https://arxiv.org/abs/2402.06635)

    本文通过证明一个足够宽而任意深的神经网络训练出来的投资组合优化模型与大型因子模型等效，打开了深度学习在此领域中的黑盒子，并提供了一种封闭形式的推导方法。研究实证了不同架构选择对模型性能的影响，并证明了随着深度增加，模型在足够多数据下的表现逐渐提升，直至达到饱和。

    

    我们打开了深度学习在投资组合优化中的黑盒子，并证明了一个足够宽而任意深的神经网络(DNN)被训练用来最大化随机贴现因子(SDF)的夏普比率等效于一个大型因子模型(LFM)：一个使用许多非线性特征的线性因子定价模型。这些特征的性质取决于DNN的体系结构，在一种明确可追踪的方式下。这使得首次可以推导出封闭形式的端到端训练的基于DNN的SDF。我们通过实证评估了LFMs，并展示了各种架构选择如何影响SDF的性能。我们证明了深度复杂性的优点：随着足够多的数据，DNN-SDF的外样总体表现会随着神经网络的深度而增加，当隐藏层达到约100层时达到饱和。

    We open up the black box behind Deep Learning for portfolio optimization and prove that a sufficiently wide and arbitrarily deep neural network (DNN) trained to maximize the Sharpe ratio of the Stochastic Discount Factor (SDF) is equivalent to a large factor model (LFM): A linear factor pricing model that uses many non-linear characteristics. The nature of these characteristics depends on the architecture of the DNN in an explicit, tractable fashion. This makes it possible to derive end-to-end trained DNN-based SDFs in closed form for the first time. We evaluate LFMs empirically and show how various architectural choices impact SDF performance. We document the virtue of depth complexity: With enough data, the out-of-sample performance of DNN-SDF is increasing in the NN depth, saturating at huge depths of around 100 hidden layers.
    
[^274]: SocraSynth:基于条件统计的多语言模型推理系统

    SocraSynth: Multi-LLM Reasoning with Conditional Statistics

    [https://arxiv.org/abs/2402.06634](https://arxiv.org/abs/2402.06634)

    SocraSynth是一个多语言模型推理平台，通过使用条件统计和系统化的语境增强技术，以及可调节的辩论争议程度，解决了大型语言模型(LLMs)面临的偏见、幻觉和推理能力不足等问题。

    

    大型语言模型(LLMs)在实用上面临着偏见、幻觉和推理能力不足等问题。本文介绍了SocraSynth，这是一个多语言模型(LLM)推理平台，旨在解决这些问题。SocraSynth通过连续的论证和可调节的争议程度，利用条件统计和系统化的语境增强，充分发挥了多语言模型(LLM)的优势。该平台通常由一个人类主持者和两个代表互相对抗立场的LLM代理组成。SocraSynth分为两个主要阶段：知识生成和推理评估。在知识生成阶段，主持者定义了辩论话题和争议程度，促使代理商为各自的立场制定支持性的论证。然后，在推理评估阶段，采用了苏格拉底推理和形式逻辑原理来评估所提出的论证的质量。对话以主持者调整争议程度结束。

    Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability. This paper introduces SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these issues. SocraSynth utilizes conditional statistics and systematic context enhancement through continuous arguments, alongside adjustable debate contentiousness levels. The platform typically involves a human moderator and two LLM agents representing opposing viewpoints on a given subject. SocraSynth operates in two main phases: knowledge generation and reasoning evaluation. In the knowledge generation phase, the moderator defines the debate topic and contentiousness level, prompting the agents to formulate supporting arguments for their respective stances. The reasoning evaluation phase then employs Socratic reasoning and formal logic principles to appraise the quality of the arguments presented. The dialogue concludes with the moderator adjusting the contentiousn
    
[^275]: MDGNN：多关系动态图神经网络用于全面和动态的股票投资预测

    MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive and Dynamic Stock Investment Prediction

    [https://arxiv.org/abs/2402.06633](https://arxiv.org/abs/2402.06633)

    提出了多关系动态图神经网络（MDGNN）框架，通过利用离散动态图全面捕捉股票之间的多方面关系及其随时间的演变，并利用Transformer结构的能力对多重关系的时间演变进行编码，从而提供了股票及其关联实体之间相互关系的完整视角。

    

    股票市场是金融系统的一个关键组成部分，但由于经济指标、财务报告、全球新闻和投资者情绪等多方面的动态和复杂关系，预测股价的变动是困难的。传统的序列方法和基于图的模型已经应用于股票价格预测，但它们在捕捉股价变动中的多方面和时间影响方面存在局限性。为了解决这些挑战，提出了多关系动态图神经网络（MDGNN）框架，它利用离散动态图全面捕捉股票之间的多方面关系及其随时间的演变。由图生成的表示提供了股票及其关联实体之间相互关系的完整视角。此外，还利用Transformer结构的能力对多重关系的时间演变进行编码。

    The stock market is a crucial component of the financial system, but predicting the movement of stock prices is challenging due to the dynamic and intricate relations arising from various aspects such as economic indicators, financial reports, global news, and investor sentiment. Traditional sequential methods and graph-based models have been applied in stock movement prediction, but they have limitations in capturing the multifaceted and temporal influences in stock price movements. To address these challenges, the Multi-relational Dynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a discrete dynamic graph to comprehensively capture multifaceted relations among stocks and their evolution over time. The representation generated from the graph offers a complete perspective on the interrelationships among stocks and associated entities. Additionally, the power of the Transformer structure is leveraged to encode the temporal evolution of multiplex relations, provid
    
[^276]: Premier-TACO: 通过时间驱动的对比损失进行多任务表示预训练

    Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

    [https://arxiv.org/abs/2402.06187](https://arxiv.org/abs/2402.06187)

    Premier-TACO是一种多任务特征表示学习方法，通过预训练通用特征表示，并引入负例抽样策略来提高时序行动对比学习的计算效率，从而显著增强了对新颖动作的少样本模仿学习的效果。

    

    我们提出了Premier-TACO，这是一种多任务特征表示学习方法，旨在提高顺序决策任务中少样本策略学习的效率。Premier-TACO利用一部分多任务离线数据集进行预训练通用特征表示，该特征表示捕捉了关键的环境动力学，并使用最少的专家演示进行微调。它通过引入一种新的负例抽样策略推动了时序行动对比学习（TACO）目标的发展，TACO在视觉控制任务中具有最先进的结果。这种策略在显著提高TACO的计算效率方面非常重要，使大规模多任务离线预训练成为可能。我们在包括Deepmind Control Suite、MetaWorld和LIBERO在内的各种连续控制基准测试中进行了广泛的实证评估，证明了Premier-TACO在预训练视觉表示方面的有效性，显著增强了对新颖动作的少样本模仿学习。

    We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
    
[^277]: 不精确的Halpern迭代算法及其在分布鲁棒优化中的应用

    An Inexact Halpern Iteration for with Application to Distributionally Robust Optimization

    [https://arxiv.org/abs/2402.06033](https://arxiv.org/abs/2402.06033)

    本文研究了确定性和随机环境下Halpern迭代算法的不精确变种，通过适当选择不精确的容差，这些变种展现出O(k^-1)的收敛速度，同时具有竞争性的收敛特性。并且我们还展示了这些方法在两类数据驱动Wasserstein分布鲁棒优化问题中的应用，以及在分布鲁棒学习中使用随机一阶方法进行不精确计算的能力。

    

    Halpern迭代算法因其简单形式和吸引人的收敛性质，近年来在解决单调包含问题方面引起了越来越多的关注。本文研究了确定性和随机环境下该方案的不精确变种。我们进行了广泛的收敛性分析，并表明通过适当选择不精确的容差，不精确方案在（期望的）残差范数上具有O(k^-1)的收敛速度。我们的结果放宽了文献中采用的最新不精确性条件，同时具有相同的竞争性收敛特性。然后，我们演示了如何使用所提出的方法解决两类具有凸凹最小-最大优化重构的数据驱动Wasserstein分布鲁棒优化问题。我们强调了其在使用随机一阶方法进行分布鲁棒学习中的不精确计算能力。

    The Halpern iteration for solving monotone inclusion problems has gained increasing interests in recent years due to its simple form and appealing convergence properties. In this paper, we investigate the inexact variants of the scheme in both deterministic and stochastic settings. We conduct extensive convergence analysis and show that by choosing the inexactness tolerances appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in terms of the (expected) residue norm. Our results relax the state-of-the-art inexactness conditions employed in the literature while sharing the same competitive convergence properties. We then demonstrate how the proposed methods can be applied for solving two classes of data-driven Wasserstein distributionally robust optimization problems that admit convex-concave min-max optimization reformulations. We highlight its capability of performing inexact computations for distributionally robust learning with stochastic first-order methods.
    
[^278]: \textit{MinMaxMin} $Q$-learning

    \textit{MinMaxMin} $Q$-learning

    [https://arxiv.org/abs/2402.05951](https://arxiv.org/abs/2402.05951)

    \textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。

    

    \textit{MinMaxMin} $Q$-learning是一种新颖的乐观型Actor-Critic算法，解决了保守型强化学习算法中存在的过高估计偏差的问题（$Q$-估计过高估计了真实的$Q$值）。其核心公式依赖于$Q$-网络之间的差异，采用最小批次最大最小$Q$-网络距离作为$Q$-目标加入，并作为优先级经验回放采样规则。我们在TD3和TD7之上实施了\textit{MinMaxMin}，并对其在流行的MuJoCo和Bullet环境中对抗现有的连续空间算法-DDPG，TD3和TD7进行了严格测试。结果显示，在所有测试任务中，\textit{MinMaxMin}相对于DDPG，TD3和TD7均表现出了稳定的性能提升。

    \textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
    
[^279]: SQT - std Q-target

    \textit{SQT} -- \textit{std} $Q$-target

    [https://arxiv.org/abs/2402.05950](https://arxiv.org/abs/2402.05950)

    SQT是一种基于Q-学习的保守型actor-critic算法，利用Q网络的标准差作为一种“不确定性惩罚”，成功解决了过高估计偏差问题，相较于TD3的Q-target公式具有更好的性能优势。

    

    Std Q-target是一种基于Q-学习的保守型actor-critic算法，它基于一个关键的Q公式：Q网络的标准差，这个标准差作为一种“不确定性惩罚”，是对过高估计偏差问题的一种简约解决方案。我们在TD3/TD7代码的基础上实现了SQT，并将其与最先进的actor-critic算法DDPG、TD3和TD7在七个常见的MuJoCo和Bullet任务上进行了测试。我们的结果表明，在强化学习中，SQT的Q-target公式相对于TD3的Q-target公式在解决过高估计偏差的保守解方面具有优势，而在所有任务中，SQT相对于DDPG、TD3和TD7都有明显的性能优势。

    \textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
    
[^280]: PromptCrypt: 使用表情符号对大型语言模型进行安全通信的提示加密

    PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models

    [https://arxiv.org/abs/2402.05868](https://arxiv.org/abs/2402.05868)

    PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。

    

    基于云的大型语言模型（LLM）如ChatGPT在日常操作中变得越来越重要，成为各种应用程序中的重要工具。虽然这些模型在可访问性和功能性方面带来了重大好处，但它们也引入了重要的隐私问题：在云基础架构中传输和存储用户数据会产生重大的数据泄露和未经授权访问敏感信息的风险；即使数据的传输和存储被加密，LLM服务提供商仍然知道数据的真实内容，从而阻止个人或实体放心使用此类LLM服务。为了解决这些问题，本文提出了一种简单但有效的机制PromptCrypt来保护用户隐私。它使用表情符号对用户输入进行加密，然后将其发送到LLM，有效地使其对人类或LLM的检查无法理解，同时保留原始提示的意图，从而确保用户隐私。

    Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
    
[^281]: 使用YOLO v7在磁共振成像中检测肾脏：一种有监督的对比学习方法

    Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning

    [https://arxiv.org/abs/2402.05817](https://arxiv.org/abs/2402.05817)

    本研究使用了最新的YOLO V7目标检测方法，通过对医学图像进行修改和训练，成功提高了肾脏在磁共振成像中的检测效果，为肾脏疾病的诊断和治疗提供了有力支持。

    

    本研究探索了最新的You Only Look Once (YOLO V7)目标检测方法在医学图像中增强肾脏检测的应用，通过对医学图像格式进行修改，对修改后的YOLO V7进行训练和测试。研究包括878名不同亚型的肾细胞癌（RCC）患者和206名正常肾脏患者。共检索到1084名患者的5657张MRI扫描。从回顾性数据库中选择了326名患有1034个肿瘤的患者，并在其肿瘤周围绘制了边界框。在初始模型上对80%的注释案例进行训练，保留20%用于测试（主要测试集）。然后使用最佳的主要模型在其余861名患者上识别肿瘤，并使用该模型在其扫描中生成边界框坐标。创建了十个基准训练集，其中包含未分割患者上的生成坐标。最终模型用于预测主要测试集中的肾脏。

    Introduction This study explores the use of the latest You Only Look Once (YOLO V7) object detection method to enhance kidney detection in medical imaging by training and testing a modified YOLO V7 on medical image formats. Methods Study includes 878 patients with various subtypes of renal cell carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from a retrospective maintained database, and bounding boxes were drawn around their tumors. A primary model was trained on 80% of annotated cases, with 20% saved for testing (primary test set). The best primary model was then used to identify tumors in the remaining 861 patients and bounding box coordinates were generated on their scans using the model. Ten benchmark training sets were created with generated coordinates on not-segmented patients. The final model used to predict the kidney in the primary test set. We reported the positive predi
    
[^282]: 现在所有人都修剪：仅使用前向传递的LLM结构化修剪

    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes

    [https://arxiv.org/abs/2402.05406](https://arxiv.org/abs/2402.05406)

    本文提出了一种仅使用前向传递的LLM结构化修剪方法，通过Bonsai生成的修剪模型在性能上优于梯度-based结构化修剪方法，并且速度是半结构化修剪模型的两倍。

    

    鉴于非专业从业者和最富有资源的机构之间的硬件差距，尺寸不断增长的LLM变得越来越难以使用。虽然提出了许多方法来压缩LLM，以使其资源消耗可管理，但这些方法本身往往耗费资源，使其目标用户群无法接触到。在这项工作中，我们探讨了仅使用前向传递的LLM结构化修剪问题。我们希望让从业者能够修剪模型，使其规模大到硬件仅有足够的内存来运行推理。我们开发了Bonsai，这是一种无梯度、扰动修剪方法，能够生成小、快和准确的修剪模型。我们观察到，Bonsai生成的修剪模型（i）优于更昂贵的梯度-based结构化修剪方法生成的模型，并且（ii）与半结构化修剪模型相比，速度快一倍且准确性相当。

    Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
    
[^283]: BIKED++：一个包含140万个自行车图像和参数化CAD设计的多模态数据集

    BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs

    [https://arxiv.org/abs/2402.05301](https://arxiv.org/abs/2402.05301)

    本文介绍了BIKED++数据集，其中包含了140万个自行车设计的图像和参数化CAD文件。该数据集可以用于训练跨模态预测模型，例如使用参数化表示来准确估计图像的特征嵌入。该数据集也已公开，可供研究者使用。

    

    本文介绍了一个公开数据集，包含了140万个通过参数化表示和JSON文件以及栅格化图像生成的自行车设计。该数据集是通过渲染引擎和BikeCAD软件生成参数化设计的矢量图形而创建的。本文还公开了该渲染引擎。该数据集具有多种应用，其中一个主要目标是训练参数化和基于图像的设计表示之间的跨模态预测模型。例如，我们证明可以通过训练预测模型直接从参数化表示准确估计对比语言-图像预训练（CLIP）嵌入。这样可以建立参数化自行车设计与文本字符串或参考图像之间的相似关系。经过训练的预测模型也已公开。该数据集加入了BIKED数据集系列。

    This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family whic
    
[^284]: 变形器世界模型是否可以给出更好的策略梯度？

    Do Transformer World Models Give Better Policy Gradients?

    [https://arxiv.org/abs/2402.05290](https://arxiv.org/abs/2402.05290)

    在强化学习中，通过使用变形器世界模型来预测未来奖励并进行策略梯度学习通常变得不可行。研究人员发现常用的变形器世界模型会产生迂回的梯度路径，对于长距离的策略梯度是有害的。为了解决这个问题，他们提出了一种名为Actions World Models (AWMs)的世界模型，可以提供更直接的梯度传播路径。

    

    对于强化学习来说，一种自然的方法是通过展开神经网络世界模型来预测未来的奖励，并通过计算图进行反向传播以学习策略。然而，由于典型的世界模型产生了难以优化的损失地形，这种方法在长时间跨度上通常变得不可行。变形器已知可以高效地传播长时间跨度的梯度：它们是否可以解决这个问题呢？令人惊讶的是，我们发现常用的变形器世界模型会产生迂回的梯度路径，这对于长距离的策略梯度是有害的。为了应对这个挑战，我们提出了一类称为Actions World Models (AWMs)的世界模型，旨在提供更直接的梯度传播路径。我们将这种AWMs集成到一个策略梯度的框架中，强调了网络架构与策略梯度更新之间的关系。我们证明了AWMs可以产生可优化的梯度路径。

    A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat
    
[^285]: 采用分割引导扩散模型的解剖可控医学图像生成

    Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models

    [https://arxiv.org/abs/2402.05210](https://arxiv.org/abs/2402.05210)

    这篇论文提出了一种采用分割引导扩散模型的解剖可控医学图像生成方法，通过随机掩模消融训练算法实现对解剖约束的条件化，同时提高了网络对解剖真实性的学习能力。

    

    扩散模型已经实现了非常高质量的医学图像生成，可以通过为小型或不平衡的数据集提供补充，从而帮助减轻获取和注释新图像的费用，同时还可以应用于其他方面。然而，这些模型在生成图像时面临着全局解剖真实性的挑战。因此，我们提出了一种解剖可控的医学图像生成模型。我们的模型在每个采样步骤中遵循多类解剖分割掩模，并采用随机掩模消融训练算法，以实现对所选解剖约束的条件化，同时允许其他解剖区域的灵活性。这也改善了网络在完全无条件（无约束生成）情况下对解剖真实性的学习。通过对乳腺MRI和腹部/颈部到盆腔CT数据集的比较评估，证明了我们模型在解剖真实性和输入掩模保真度方面具有优越性。

    Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
    
[^286]: ApiQ：2位量化大型语言模型的微调

    ApiQ: Finetuning of 2-Bit Quantized Large Language Model

    [https://arxiv.org/abs/2402.05147](https://arxiv.org/abs/2402.05147)

    这项工作介绍了一种名为ApiQ的新型量化框架，通过同时初始化LoRA组件和量化大型语言模型的权重，恢复量化过程中丢失的信息，维持原始模型的激活精度并减轻误差传播。

    

    随着大型语言模型的增大，内存高效的模型微调近年来备受关注，主要是由于GPU内存限制和这些方法与完全微调的可比结果所带来的约束。尽管有了进展，如QLoRA这样的内存高效微调策略在不同位宽的量化和多样化任务中表现不一致。这种不一致主要来自于量化过程对保留知识的有害影响，导致灾难性遗忘，削弱了预训练模型在微调中的利用。在这项工作中，我们引入了一种名为ApiQ的新型量化框架，旨在通过同时初始化LoRA组件和量化LLM的权重来恢复量化损失的信息。这种方法确保了原始LLM的激活精度的维持，同时减轻了误差的传播。

    Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
    
[^287]: 在六个简单的步骤中去噪扩散概率模型

    Denoising Diffusion Probabilistic Models in Six Simple Steps

    [https://arxiv.org/abs/2402.04384](https://arxiv.org/abs/2402.04384)

    本论文提供了一个简单、全面、干净且清晰的介绍去噪扩散概率模型（DDPM）的方法，强调了从连续时间极限的视角出发，以提供更好的理解和实际性能。

    

    去噪扩散概率模型（DDPM）是一类非常流行的深度生成模型，已成功应用于包括图像和视频生成、蛋白质和材料合成、天气预测和偏微分方程的神经替代等多个问题。尽管其普及度很高，但很难找到一个简单、全面、干净且清晰的DDPM介绍。研究论文中必要的简洁解释无法阐明制定DDPM所采取的不同设计步骤以及省略了步骤的理由以节省空间。此外，这些论述通常从变分下界的视角出发，这是不必要且可能有害的，因为它混淆了方法奏效的原因并暗示了实践中表现不佳的泛化性质。另一方面，采用连续时间极限的视角是美丽且普遍的，但是...

    Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but 
    
[^288]: RL-VLM-F: 强化学习通过视觉语言基础模型反馈

    RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback

    [https://arxiv.org/abs/2402.03681](https://arxiv.org/abs/2402.03681)

    RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。

    

    传统强化学习研究中的奖励设计一直是一个挑战，因为通常需要大量人力和反复试错的过程来设计有效的奖励函数。本文提出了一种自动生成奖励函数的方法，用于代理学习新任务，只使用任务目标的文本描述和代理的视觉观测，并利用视觉语言基础模型（VLMs）的反馈。我们的方法的关键是通过查询这些模型，基于任务目标的文本描述给出对代理的图像观测的偏好，并从偏好标签中学习奖励函数，而不是直接要求这些模型输出原始奖励分数，这可能存在噪音和不一致性。我们证明了RL-VLM-F在各种领域中成功地产生了有效的奖励和策略，包括经典控制以及刚性和灵活操纵方面。

    Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
    
[^289]: 扩散世界模型

    Diffusion World Model

    [https://arxiv.org/abs/2402.03570](https://arxiv.org/abs/2402.03570)

    扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。

    

    我们引入了扩散世界模型（DWM），这是一个条件扩散模型，能够同时预测多步的未来状态和奖励。与传统的一步动力学模型相反，DWM通过单个前向传递提供了长时程的预测，消除了递归查询的需要。我们将DWM整合到基于模型的价值估计中，其中短期回报通过从DWM中采样的未来轨迹进行模拟。在离线强化学习的背景下，DWM可以被视为通过生成建模来实现保守的值正则化。另外，它也可以被视为一种数据源，使离线Q学习能够使用合成数据。我们在D4RL数据集上的实验证实了DWM对长时程模拟的鲁棒性。在绝对性能方面，DWM显著超过了一步动力学模型，性能提高了44%，并达到了最先进的水平。

    We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
    
[^290]: 深度补全的测试时间自适应

    Test-Time Adaptation for Depth Completion

    [https://arxiv.org/abs/2402.03312](https://arxiv.org/abs/2402.03312)

    该论文提出了一种在线测试时间自适应方法，用于深度补全任务，通过在单次通过中缩小源数据和目标数据间的领域差距，提高模型性能。

    

    当将在一些（源）数据集上训练的模型转移到目标测试数据时，常常会观察到性能下降，这是由于它们之间存在领域差距。现有的用于弥合这一差距的方法，如领域适应（DA），可能需要模型训练时使用的源数据（通常不可用），而其他方法，如无源DA，则需要多次通过测试数据。我们提出了一种在线测试时间自适应方法，用于深度补全，即从单个图像和相关的稀疏深度图推断出密集深度图的任务，以在一次通过中缩小性能差距。首先，我们对每种数据模态中的领域转移如何影响模型性能进行了研究。根据我们的观察，稀疏深度模态展现出比图像更小的协变量转移，因此我们设计了一个在源领域中训练的嵌入模块，它保留了从仅编码稀疏深度特征到编码图像和稀疏深度的特征的映射。在测试时间，我们使用这个嵌入模块实现自适应。

    It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During t
    
[^291]: BGE M3-嵌入：通过自知识蒸馏实现多语言、多功能和多粒度的文本嵌入

    BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

    [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216)

    BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。

    

    在本文中，我们提出了一种新的嵌入模型，称为M3-嵌入，以其在多语言、多功能和多粒度方面的多样性而著称。它可以支持超过100种工作语言，在多语言和跨语言检索任务上取得了新的最先进性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的IR应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句到长达8192个标记的文档。M3-嵌入的有效训练包括以下技术贡献。我们提出了一种新颖的自知识蒸馏方法，可以将来自不同检索功能的相关性分数整合为教师信号，以提高训练质量。我们还优化了批处理策略。

    In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
    
[^292]: 解决分层信息共享的分布式部分可观察马尔可夫决策过程：一种广义博弈方法

    Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach

    [https://arxiv.org/abs/2402.02954](https://arxiv.org/abs/2402.02954)

    本文通过应用最优性原理研究了分层信息共享的分布式部分可观察马尔可夫决策过程的解决方法。通过将问题分解成单阶段子游戏，并通过进一步分解子游戏，我们成功地解开了决策变量的纠缠，同时显著减少了时间复杂度。

    

    最近的理论表明，多人分散的部分可观察马尔可夫决策过程可以转化为等效的单人游戏，使得可以应用贝尔曼的最优性原理通过将其分解为单阶段子游戏来解决单人游戏。然而，这种方法在每个单阶段子游戏中纠缠了所有玩家的决策变量，导致指数复杂度的备份。本文展示了如何在保持分层信息共享的前提下解开这些决策变量的纠缠，这是我们社会中一种突出的管理风格。为了实现这个目标，我们应用最优性原理通过进一步将任何单阶段子游戏分解为更小的子游戏来解决它，使我们能够逐次进行单人决策。我们的方法揭示了存在于单阶段子游戏中的广义博弈解决方案，极大地减少了时间复杂度。我们的实验结果验证了我们方法的有效性，证明它可以在解决分层信息共享的分布式部分可观察马尔可夫决策过程中发挥重要作用。

    A recent theory shows that a multi-player decentralized partially observable Markov decision process can be transformed into an equivalent single-player game, enabling the application of \citeauthor{bellman}'s principle of optimality to solve the single-player game by breaking it down into single-stage subgames. However, this approach entangles the decision variables of all players at each single-stage subgame, resulting in backups with a double-exponential complexity. This paper demonstrates how to disentangle these decision variables while maintaining optimality under hierarchical information sharing, a prominent management style in our society. To achieve this, we apply the principle of optimality to solve any single-stage subgame by breaking it down further into smaller subgames, enabling us to make single-player decisions at a time. Our approach reveals that extensive-form games always exist with solutions to a single-stage subgame, significantly reducing time complexity. Our expe
    
[^293]: 逃避语言模型数据污染检测（太）容易

    Evading Data Contamination Detection for Language Models is (too) Easy

    [https://arxiv.org/abs/2402.02823](https://arxiv.org/abs/2402.02823)

    本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。

    

    大型语言模型广泛使用，它们在基准测试上的性能经常指导用户对一个模型与另一个模型的偏好。然而，这些模型所训练的大量数据可能会意外地与公共基准测试数据发生污染，从而损害性能评估。尽管最近开发了一些污染检测方法来解决这个问题，但它们忽视了恶意模型提供者有意进行污染以避免被检测的可能性。我们认为这种情况非常重要，因为它对公共基准测试的可信度产生了怀疑。为了更严格地研究这个问题，我们提出了模型提供者和污染检测方法的分类，这揭示了现有方法中的漏洞，我们通过使用EAL这种简单而有效的污染技术，明显提高了基准测试的性能，并完全逃避了当前的检测方法。

    Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
    
[^294]: 医疗保健中多模态机器学习方法的综述

    Review of multimodal machine learning approaches in healthcare

    [https://arxiv.org/abs/2402.02460](https://arxiv.org/abs/2402.02460)

    这篇综述主要介绍医疗保健领域中多模态机器学习方法的最新研究进展。通过综合分析最近的文献，探讨了临床诊断中各种数据模态的应用以及融合技术的评估。重点关注影像数据的应用，并介绍了现有的多模态数据集和训练方法。

    

    在医疗保健领域，机器学习方法传统上注重使用单一模态的数据，限制了其有效复制临床实践中整合多种信息来源以改善决策的能力。临床医生通常依赖各种数据来源，包括患者的人口统计信息、实验室数据、生命体征和各种影像数据模态来做出明智的决策并对其发现进行上下文化。机器学习的最新进展促进了多模态数据的更高效融合，从而产生更好地代表医生方法的应用。在这里，我们提供了医疗保健中多模态机器学习方法的综述，全面概述了最近的文献。我们讨论了临床诊断中使用的各种数据模态，特别强调影像数据。我们评估了融合技术，探索了现有的多模态数据集，以及研究常见的训练方法。

    Machine learning methods in healthcare have traditionally focused on using data from a single modality, limiting their ability to effectively replicate the clinical practice of integrating multiple sources of information for improved decision making. Clinicians typically rely on a variety of data sources including patients' demographic information, laboratory data, vital signs and various imaging data modalities to make informed decisions and contextualise their findings. Recent advances in machine learning have facilitated the more efficient incorporation of multimodal data, resulting in applications that better represent the clinician's approach. Here, we provide a review of multimodal machine learning approaches in healthcare, offering a comprehensive overview of recent literature. We discuss the various data modalities used in clinical diagnosis, with a particular emphasis on imaging data. We evaluate fusion techniques, explore existing multimodal datasets and examine common traini
    
[^295]: MetaOptimize：一个优化步长和其他元参数的框架

    MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters

    [https://arxiv.org/abs/2402.02342](https://arxiv.org/abs/2402.02342)

    MetaOptimize是一个框架，通过动态调整学习率来优化机器学习算法中的元参数，以提高训练效率和模型性能。

    

    本文解决了机器学习算法中优化元参数（即超参数）的挑战，这是影响训练效率和模型性能的关键因素。我们引入了MetaOptimize框架，摆脱了计算昂贵的传统元参数搜索方法，通过动态调整元参数，特别是步长（也称为学习率），来训练模型。具体而言，MetaOptimize可以适用于任何一阶优化算法，在训练过程中实时调整步长，通过未来损失的折现总和来最小化一种特定形式的遗憾。我们还介绍了MetaOptimize的低复杂度变体，结合其适应多个优化算法的能力，展示了在各种机器学习应用中与手工设计的学习率计划相媲美的性能。

    This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
    
[^296]: INViT:一种具有不变嵌套视图转换器的可泛化解决路由问题的方法

    INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer

    [https://arxiv.org/abs/2402.02317](https://arxiv.org/abs/2402.02317)

    INViT是一种具有不变嵌套视图转换器的解决路由问题的方法，通过强制嵌套设计和不变的视图，在编码器内部提高学习求解器的泛化能力，从而实现了在具有不同分布和不同问题规模的TSP和CVRP问题上卓越的泛化性能。

    

    最近，深度强化学习在学习解决路由问题的快速启发式方法方面取得了有希望的结果。与此同时，大多数求解器在推广到未见过的分布或具有不同规模的分布时遇到了困难。为解决这个问题，我们提出了一种新的架构，称为不变嵌套视图转换器（INViT），它旨在通过在编码器内部强制一个嵌套设计以及不变的视图来促进学习求解器的泛化能力。它应用了修改的策略梯度算法，并结合了数据增强。我们证明了所提出的INViT在具有不同分布和不同问题规模的TSP和CVRP问题上取得了卓越的泛化性能。

    Recently, deep reinforcement learning has shown promising results for learning fast heuristics to solve routing problems. Meanwhile, most of the solvers suffer from generalizing to an unseen distribution or distributions with different scales. To address this issue, we propose a novel architecture, called Invariant Nested View Transformer (INViT), which is designed to enforce a nested design together with invariant views inside the encoders to promote the generalizability of the learned solver. It applies a modified policy gradient algorithm enhanced with data augmentations. We demonstrate that the proposed INViT achieves a dominant generalization performance on both TSP and CVRP problems with various distributions and different problem scales.
    
[^297]: 关于在生成AI领域利用DCT轨迹的探索

    On the Exploitation of DCT-Traces in the Generative-AI Domain

    [https://arxiv.org/abs/2402.02209](https://arxiv.org/abs/2402.02209)

    本文分析了生成AI模型在生成深度伪造图像时在频域中的DCT系数的统计特征。通过研究发现了一种独特的“辨别指纹”，可以利用它来改善现有的深度伪造检测器。

    

    深度伪造对于网络安全和数字取证领域来说是一个巨大的挑战，特别是考虑到最近基于生成AI的解决方案所获得的高质量结果。几乎所有生成模型在合成数据中留下了独特的痕迹，如果对其进行详细分析和识别，可以利用这些痕迹来改善现有深度伪造检测器的泛化限制。本文分析了由GAN和扩散模型引擎生成的深度伪造图像在频域中的特征，详细研究了离散余弦变换(DCT)系数的统计分布。我们认识到并非所有系数对图像检测的贡献相同，我们假设存在一种独特的“辨别指纹”，嵌入在特定系数组合中。为了识别它们，我们对各种系数组合进行了机器学习分类器的训练。此外，我们还使用了可解释AI(XAI)的LIME算法来搜索...

    Deepfakes represent one of the toughest challenges in the world of Cybersecurity and Digital Forensics, especially considering the high-quality results obtained with recent generative AI-based solutions. Almost all generative models leave unique traces in synthetic data that, if analyzed and identified in detail, can be exploited to improve the generalization limitations of existing deepfake detectors. In this paper we analyzed deepfake images in the frequency domain generated by both GAN and Diffusion Model engines, examining in detail the underlying statistical distribution of Discrete Cosine Transform (DCT) coefficients. Recognizing that not all coefficients contribute equally to image detection, we hypothesize the existence of a unique "discriminative fingerprint", embedded in specific combinations of coefficients. To identify them, Machine Learning classifiers were trained on various combinations of coefficients. In addition, the Explainable AI (XAI) LIME algorithm was used to sea
    
[^298]: 机器学习生成代码的质量和信任

    Quality and Trust in LLM-generated Code

    [https://arxiv.org/abs/2402.02047](https://arxiv.org/abs/2402.02047)

    本论文研究了机器学习生成代码的质量和信任问题，提出了校准的重要性，并探讨了如何确定模型生成代码的正确性。

    

    机器学习模型广泛应用，但常常会出错。用户需要可靠的指示，以确定给定模型的输出是否可信，从而可以做出理性决策是否使用该输出。例如，可以将输出与置信度相关联；如果置信度与正确性的可能性强相关，则称该模型为良好校准。在这种情况下，高置信度的输出可以安全接受，低置信度的输出可以拒绝。校准迄今主要在非生成性（例如分类）环境中进行研究，特别是在软件工程领域。然而，生成代码很容易出错：开发人员需要知道何时直接使用、经过仔细审查后使用或丢弃模型生成的代码，因此在生成环境中，校准非常重要。然而，生成代码的正确性概念并不简单，因此校准也是如此。

    Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected.   Calibration has so far been studied in non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. I
    
[^299]: 使用结构化纵向电子健康记录数据促使大型语言模型进行零样本临床预测

    Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data

    [https://arxiv.org/abs/2402.01713](https://arxiv.org/abs/2402.01713)

    本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。

    

    结构化纵向电子健康记录（EHR）数据的固有复杂性使其与传统上为自然语言处理而设计的大型语言模型（LLM）整合时面临重大挑战。受新疾病爆发时迅速决策的紧迫需求的驱使，本研究调查了类似GPT-4的LLM对EHR数据的适应性。我们特别关注它们的零样本能力，即在没有明确训练的情况下进行预测。针对EHR数据的纵向、稀疏和知识注入的特点，我们的提示方法考虑了特定的EHR特征，如单位和参考范围，并采用了与临床上下文相一致的上下文学习策略。通过在MIMIC-IV和TJH数据集上进行全面实验，我们证明了LLM能够通过我们的方法进行零样本临床预测，有效应对了EHR数据的挑战。

    The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
    
[^300]: 提高货运模式选择模型的准确性：基于2017年CFS PUF数据集和集成学习技术的案例研究

    Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques

    [https://arxiv.org/abs/2402.00654](https://arxiv.org/abs/2402.00654)

    该论文通过使用2017年的商品流动调查公共使用文件数据集和集成学习技术，改进了货运模式选择模型的准确性，包括构建本地模型、提取地理特征和应用集成学习方法。实验结果表明，该方法在没有内存限制的情况下实现了超过92%的准确性。

    

    美国人口普查局收集了两轮实验数据，提供了全国商品流动的运输特征，分别在2012年（即公共使用微数据）和2017年（即公共使用文件）发布。基于这些信息，数据驱动方法在理解货运物流的详细模式方面变得越来越有价值。在本研究中，我们使用2017年的商品流动调查公共使用文件数据集，探索构建一个高性能的货运模式选择模型，考虑到三个主要改进：（1）为每个独立的商品/行业类别构建本地模型；（2）提取有用的地理特征，尤其是每种货运模式在起点/终点区域之间的衍生距离；（3）利用集成学习方法，如堆叠或投票，将本地和统一模型的结果相结合，以提高性能。所提出的方法在没有内存限制的情况下实现了超过92%的准确性。

    The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File). With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics. In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance. The proposed method achieved over 92% accuracy without in
    
[^301]: PirateNets：采用残差自适应网络的物理知识驱动深度学习

    PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks

    [https://arxiv.org/abs/2402.00326](https://arxiv.org/abs/2402.00326)

    PirateNets是一种物理知识驱动的深度学习框架，解决了多层感知机网络在较大深度时性能下降的问题，通过引入自适应残差连接实现了稳定和高效的训练，并提升了模型的性能。

    

    虽然物理知识驱动神经网络(PINNs)已成为解决由偏微分方程(PDEs)控制的正向和反向问题的流行深度学习框架，但在采用更大和更深的神经网络架构时，它们的性能会下降。我们的研究发现，这种反直觉行为的根源在于使用不适合的初始化方案的多层感知机(MLP)网络结构，导致网络导数的可训练性较差，并最终导致PDE残差损失的不稳定最小化。为了解决这个问题，我们提出了物理知识驱动残差自适应网络(PirateNets)，这是一种新型架构，旨在促进深度PINN模型的稳定和高效训练。PirateNets利用一种新颖的自适应残差连接，允许网络作为浅层网络进行初始化，并在训练过程中逐渐加深。我们还展示了所提出的初始化方案可以提高PINN模型的训练效果并改善性能。

    While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initializatio
    
[^302]: 采用最优输运和粒子梯度下降的保护隐私数据发布方法

    Privacy-preserving data release leveraging optimal transport and particle gradient descent

    [https://arxiv.org/abs/2401.17823](https://arxiv.org/abs/2401.17823)

    该研究提出了一种基于边际的保护隐私数据合成方法PrivPGD，利用了最优输运和粒子梯度下降的工具。该方法在不同领域的数据集上表现出色，具有高度的可扩展性和灵活性，并可以满足特定的领域约束条件。

    

    我们提出了一种新颖的方法，用于保护关键领域（如医疗保健和政府）中隐私的表格数据差分私有数据合成的任务。目前的最先进方法主要使用基于边际的方法，从私有边际估计生成数据集。在本文中，我们引入了PrivPGD，一种基于边际的私有数据合成的新一代方法，利用了最优输运和粒子梯度下降的工具。我们的算法在大范围数据集上优于现有方法，并具有高度可扩展性和灵活性，可以结合其他领域特定的约束条件。

    We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.
    
[^303]: 机器学习中伪随机数生成器的可重复性、能效和性能：对Python、NumPy、TensorFlow和PyTorch实现的比较研究

    Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations

    [https://arxiv.org/abs/2401.17345](https://arxiv.org/abs/2401.17345)

    本研究比较了机器学习中常用的伪随机数生成器在统计质量、数值可重复性、时间效率和能源消耗等方面与原始C实现的差异。

    

    伪随机数生成器(PRNGs)在机器学习技术中已经无处不在，因为它们在众多方法中都非常有意思。机器学习领域有着巨大的潜力，在各个领域中取得突破性进展，比如最近在大型语言模型(LLMs)中的突破。然而，尽管越来越受关注，但仍然存在一些持续关注的问题，包括与可重复性和能源消耗相关的问题。可重复性对于强大的科学研究和可解释性至关重要，而能效则强调了保护有限全球资源的必要性。本研究深入探讨了机器学习语言、库和框架中最主要的伪随机数生成器(PRNGs)与各自算法原始C实现相比，在统计质量和数值可重复性方面的表现。此外，我们还旨在评估时间效率和能源消耗。

    Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine learning technologies because they are interesting for numerous methods. The field of machine learning holds the potential for substantial advancements across various domains, as exemplified by recent breakthroughs in Large Language Models (LLMs). However, despite the growing interest, persistent concerns include issues related to reproducibility and energy consumption. Reproducibility is crucial for robust scientific inquiry and explainability, while energy efficiency underscores the imperative to conserve finite global resources. This study delves into the investigation of whether the leading Pseudo-Random Number Generators (PRNGs) employed in machine learning languages, libraries, and frameworks uphold statistical quality and numerical reproducibility when compared to the original C implementation of the respective PRNG algorithms. Additionally, we aim to evaluate the time efficiency and energy consumption of 
    
[^304]: PICL: 物理信息对比学习用于偏微分方程

    PICL: Physics Informed Contrastive Learning for Partial Differential Equations

    [https://arxiv.org/abs/2401.16327](https://arxiv.org/abs/2401.16327)

    这项工作开发了一种使用广义对比损失的对比预训练框架，通过利用物理信息改善了神经算子在多个偏微分方程中的泛化能力。

    

    最近，神经算子作为偏微分方程（PDE）替代模型逐渐受到关注。学习解决方案函数而不是函数本身已被证明是一种强大的方法，可快速准确地求解复杂的PDE。尽管在广泛的代理建模任务中对神经算子的性能进行了许多研究，但这些工作通常是逐个方程评估性能。在本研究中，我们开发了一种新颖的对比预训练框架，利用广义对比损失，可以同时改善神经算子在多个控制方程中的泛化能力。控制方程系数用于衡量系统之间的真实相似性。物理信息系统演化和潜在空间模型输出的结合被锚定到输入数据中，并用于我们的距离函数。我们发现，物理信息对比预训练可以提高傅立叶神经算子的准确性和泛化能力。

    Neural operators have recently grown in popularity as Partial Differential Equation (PDEs) surrogate models. Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs. While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time. In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously. Governing equation coefficients are used to measure ground-truth similarity between systems. A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function. We find that physics-informed contrastive pretraining improves both accuracy and generalization for the Fourier Neur
    
[^305]: Vision Mamba: 使用双向状态空间模型高效学习视觉表示

    Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model

    [https://arxiv.org/abs/2401.09417](https://arxiv.org/abs/2401.09417)

    本文提出了一种新的通用视觉骨干模型Vim，通过使用双向状态空间模型和位置嵌入来高效表示视觉数据，相比于传统的视觉转换器如DeiT，在各种视觉任务上取得了更好的性能，并且实现了显著提升。

    

    最近，具有高效硬件感知设计的状态空间模型（SSMs），即Mamba深度学习模型，在长序列建模方面展现出巨大潜力。与此同时，在SSMs上构建高效且通用的视觉骨干模型也是一种有吸引力的方向。然而，由于视觉数据的位置敏感性和对全局上下文的需求，对于SSMs来说，表示视觉数据是具有挑战性的。在本文中，我们展示了对于视觉表示学习来说，依赖自注意力并不是必要的，并提出了一种新的通用视觉骨干模型，即带有双向Mamba块（Vim），它使用位置嵌入标记图像序列，并使用双向状态空间模型压缩视觉表示。在ImageNet分类、COCO目标检测和ADE20k语义分割任务中，Vim相比于DeiT等经过良好验证的视觉转换器，实现了更高的性能，并且显示出了显著的改进。

    Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved
    
[^306]: X黑客：误导的自动机器学习的威胁

    X Hacking: The Threat of Misguided AutoML

    [https://arxiv.org/abs/2401.08513](https://arxiv.org/abs/2401.08513)

    本文介绍了X黑客的概念，即利用自动化机器学习流程来操纵可解释AI（XAI）指标，从而产生所需解释的模型，而不降低其预测性能。研究者总结了X黑客现象的严重性，并提出了可能的检测和预防方法，同时探讨了对XAI研究可信度和可重现性的伦理影响。

    

    可解释的人工智能（XAI）和可解释的机器学习方法有助于建立对模型预测和派生见解的信任，但也为分析师提供了一种扭曲的动机，即操纵XAI指标以支持预先规定的结论。本文介绍了X黑客的概念，即将p-hacking应用于诸如Shap值之类的XAI指标。我们展示了如何利用自动化的机器学习流程来寻找“可辩护”的模型，这些模型可以产生所需的解释并在维持优越的预测性能时。我们将解释和准确性之间的权衡表述为一个多目标优化问题，并通过熟悉的真实世界数据集在经验上展示了X黑客的可行性和严重性。最后，我们提出了可能的检测和预防方法，并讨论了对XAI研究的可信度和可重现性的伦理影响。

    Explainable AI (XAI) and interpretable machine learning methods help to build trust in model predictions and derived insights, yet also present a perverse incentive for analysts to manipulate XAI metrics to support pre-specified conclusions. This paper introduces the concept of X-hacking, a form of p-hacking applied to XAI metrics such as Shap values. We show how an automated machine learning pipeline can be used to search for 'defensible' models that produce a desired explanation while maintaining superior predictive performance to a common baseline. We formulate the trade-off between explanation and accuracy as a multi-objective optimization problem and illustrate the feasibility and severity of X-hacking empirically on familiar real-world datasets. Finally, we suggest possible methods for detection and prevention, and discuss ethical implications for the credibility and reproducibility of XAI research.
    
[^307]: 保留还是丢弃？一种评估有噪声ICA解决方案的非参数分数

    Keep or toss? A nonparametric score to evaluate solutions for noisy ICA

    [https://arxiv.org/abs/2401.08468](https://arxiv.org/abs/2401.08468)

    本文提出一种非参数分数来自适应选择适用于任意高斯噪声的ICA算法，并通过特征函数评估估计的混合矩阵质量，无需了解噪声分布参数。

    

    独立分量分析（ICA）于20世纪80年代引入，作为盲源分离（BSS）的模型，指的是在对混合信号进行恢复时，对源信号或混合过程了解有限的情况下的过程。尽管有许多精密算法进行估计，但不同方法存在不同的缺点。在本文中，我们开发了一种非参数分数，用于自适应地选择ICA算法和任意高斯噪声。该分数的创新之处在于，它只假设数据具有有限的二阶矩，并使用特征函数来评估估计的混合矩阵的质量，而无需了解噪声分布的参数。此外，我们提出了一些新的对比函数和算法，它们具有与现有算法（如FASTICA和JADE）相同的快速计算性能，但在前者可能失败的领域中工作。尽管这些方法也可能存在缺点，

    Independent Component Analysis (ICA) was introduced in the 1980's as a model for Blind Source Separation (BSS), which refers to the process of recovering the sources underlying a mixture of signals, with little knowledge about the source signals or the mixing process. While there are many sophisticated algorithms for estimation, different methods have different shortcomings. In this paper, we develop a nonparametric score to adaptively pick the right algorithm for ICA with arbitrary Gaussian noise. The novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge of the parameters of the noise distribution. In addition, we propose some new contrast functions and algorithms that enjoy the same fast computability as existing algorithms like FASTICA and JADE but work in domains where the former may fail. While these also may have weaknesses,
    
[^308]: 关于学习模糊机会约束问题

    On Learning for Ambiguous Chance Constrained Problems

    [https://arxiv.org/abs/2401.00547](https://arxiv.org/abs/2401.00547)

    本文研究了模糊机会约束优化问题中当决策者不知道分布P时的情况，通过使用一组分布来近似原问题，并推导出了相应的样本复杂度。

    

    我们研究了当决策者(DM)不知道分布P时，满足机会约束的优化问题$\min_x f(x)$ s.t. $P(\left\{ \theta: g(x,\theta)\le 0 \right\})\ge 1-\epsilon$。当DM可以访问一组分布$\mathcal{U}$，其中包含了分布P时，问题被称为模糊机会约束问题。我们研究了当$\mathcal{U}$以$\left\{\mu:\frac{\mu (y)}{\nu(y)}\leq C, \forall y\in\Theta, \mu(y)\ge 0\right\}$的形式存在时的模糊机会约束问题，其中$\nu$是参考分布。我们证明在这种情况下，原问题可以通过从$\nu$中抽取N个独立同分布的样本$\theta_i$，并将原始约束替换为$g(x,\theta_i)\le 0,~i=1,2,\ldots,N$的采样问题来“很好地逼近”。我们还推导出与这种逼近相关的样本复杂度。

    We study chance constrained optimization problems $\min_x f(x)$ s.t. $P(\left\{ \theta: g(x,\theta)\le 0 \right\})\ge 1-\epsilon$ where $\epsilon\in (0,1)$ is the violation probability, when the distribution $P$ is not known to the decision maker (DM). When the DM has access to a set of distributions $\mathcal{U}$ such that $P$ is contained in $\mathcal{U}$, then the problem is known as the ambiguous chance-constrained problem \cite{erdougan2006ambiguous}. We study ambiguous chance-constrained problem for the case when $\mathcal{U}$ is of the form $\left\{\mu:\frac{\mu (y)}{\nu(y)}\leq C, \forall y\in\Theta, \mu(y)\ge 0\right\}$, where $\nu$ is a ``reference distribution.'' We show that in this case the original problem can be ``well-approximated'' by a sampled problem in which $N$ i.i.d. samples of $\theta$ are drawn from $\nu$, and the original constraint is replaced with $g(x,\theta_i)\le 0,~i=1,2,\ldots,N$. We also derive the sample complexity associated with this approximation, i.
    
[^309]: 超越密林：用一致预测增强SED建模

    Beyond mirkwood: Enhancing SED Modeling with Conformal Predictions

    [https://arxiv.org/abs/2312.14212](https://arxiv.org/abs/2312.14212)

    这篇论文提出了一种采用机器学习和一致性预测的方法，可以增强光谱能量分布（SED）建模的灵活性和不确定性数量化。与传统的方法不同，该方法可以使用任何兼容sklearn的模型，并通过转换点预测为误差条提高解释能力和可靠性。实验结果表明，在使用CatBoost作为基础预测器的情况下，采用一致预测可以显著提高性能。该方法为从观测数据中推断星系的物理性质提供了更多功能和准确的工具。

    

    传统的光谱能量分布（SED）拟合技术由于对星系的形成历史和灰尘衰减曲线做出的假设而存在不确定性。我们提出了一种先进的基于机器学习的方法，可以增强SED拟合的灵活性和不确定性数量化。与密林中使用的固定的NGBoost模型不同，我们的方法允许使用任何兼容sklearn的模型，包括确定性模型。我们结合了一致化分位数回归将点预测转换为误差条，增强解释能力和可靠性。我们以CatBoost作为基础预测器，比较了有无一致预测的结果，通过覆盖率和区间宽度等指标显示了性能的提升。我们的方法为从观测数据中推断星系物理性质提供了更加多功能和准确的工具。

    Traditional spectral energy distribution (SED) fitting techniques face uncertainties due to assumptions in star formation histories and dust attenuation curves. We propose an advanced machine learning-based approach that enhances flexibility and uncertainty quantification in SED fitting. Unlike the fixed NGBoost model used in mirkwood, our approach allows for any sklearn-compatible model, including deterministic models. We incorporate conformalized quantile regression to convert point predictions into error bars, enhancing interpretability and reliability. Using CatBoost as the base predictor, we compare results with and without conformal prediction, demonstrating improved performance using metrics such as coverage and interval width. Our method offers a more versatile and accurate tool for deriving galaxy physical properties from observational data.
    
[^310]: 变化的行动空间中的情境式强化学习

    In-Context Reinforcement Learning for Variable Action Spaces

    [https://arxiv.org/abs/2312.13327](https://arxiv.org/abs/2312.13327)

    本文提出了一种Headless-AD模型，通过只训练一次，能够在变化的行动空间中实现强化学习任务的泛化。实验证明该模型能够在从未遇到过的行动空间上表现出显著的泛化能力，甚至胜过针对特定行动集训练的模型。

    

    最近的研究表明，预先在多样数据集上进行上下文多情节训练的变形金刚网络可以在情境中泛化到新的强化学习任务。先前提出的模型的一个关键限制是它们依赖于预定义的行动空间大小和结构。引入新的行动空间通常需要数据重新收集和模型重新训练，这对于一些应用来说可能是昂贵的。我们的工作表明，通过提出一种只训练一次的Headless-AD模型，可以缓解这个问题，该模型能够泛化到具有可变大小、语义内容和顺序的离散动作空间。通过在伯努利和上下文赌博机以及一个网格世界环境中进行实验，我们展示了Headless-AD在从未遇到的行动空间上表现出显著的泛化能力，甚至在几个环境配置上胜过专门针对特定行动集训练的模型。

    Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
    
[^311]: 参数化投影贝尔曼算子

    Parameterized Projected Bellman Operator

    [https://arxiv.org/abs/2312.12869](https://arxiv.org/abs/2312.12869)

    本论文提出了一种基于学习的近似贝尔曼算子的新方法，以解决近似值迭代算法中样本不确定性和计算复杂度的问题。

    

    近似值迭代（AVI）是一类用于强化学习（RL）的算法家族，旨在获得最优值函数的近似。通常，AVI算法采用迭代过程，每个步骤包括（i）贝尔曼算子的应用和（ii）投影步骤到考虑的函数空间中。众所周知，贝尔曼算子利用转移样本，这些样本强烈影响其行为，因为无信息的样本可能导致可忽略的更新或长时间的绕行，而计算密集的投影步骤进一步加剧了这些不利影响。为了解决这些问题，我们提出了一种新颖的替代方法，该方法采用学习的方式得到贝尔曼算子的近似版本，而不是像AVI方法那样通过样本进行估计。通过这种方式，我们能够（i）在转移样本之间进行泛化，（ii）避免计算密集的投影步骤。因此，我们称我们的新算子为"projec"算子。

    Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec
    
[^312]: 快速、可扩展的半定规划与谱束缚和草图初始化

    Fast, Scalable, Warm-Start Semidefinite Programming with Spectral Bundling and Sketching

    [https://arxiv.org/abs/2312.11801](https://arxiv.org/abs/2312.11801)

    我们提出了一种名为统一谱束缚与草图初始化（USBS）的算法，用于快速、可扩展地解决大规模半定规划问题。该算法能够利用温暖启动初始化来进一步加速收敛，是一种有效的解决方案。

    

    尽管半定规划(SDP)在传统上局限于中等规模的问题，但最近使用矩阵草图技术增强的算法已经能够解决更大的SDP问题。然而，这些方法在可扩展性方面取得了成功，但需要增加必要的迭代次数，导致随着问题规模的增长，收敛速度减慢。此外，它们还需要迭代相关的参数调度，限制了有效利用温暖启动初始化的能力，在具有增量到达数据或混合整数规划的实际应用中很重要。我们提出了统一的谱束缚与草图初始化(USBS)，这是一种可证明正确、快速、可扩展的算法，用于解决大规模的SDP问题，可以利用温暖启动初始化来进一步加速收敛。我们提出的算法是一种解决包含相等和不等式约束的一般SDP问题的谱束方法，此外，当结合可选的矩阵草图技术时

    While semidefinite programming (SDP) has traditionally been limited to moderate-sized problems, recent algorithms augmented with matrix sketching techniques have enabled solving larger SDPs. However, these methods achieve scalability at the cost of an increase in the number of necessary iterations, resulting in slower convergence as the problem size grows. Furthermore, they require iteration-dependent parameter schedules that prohibit effective utilization of warm-start initializations important in practical applications with incrementally-arriving data or mixed-integer programming. We present Unified Spectral Bundling with Sketching (USBS), a provably correct, fast and scalable algorithm for solving massive SDPs that can leverage a warm-start initialization to further accelerate convergence. Our proposed algorithm is a spectral bundle method for solving general SDPs containing both equality and inequality constraints. Moveover, when augmented with an optional matrix sketching techniqu
    
[^313]: 基于语义相似度的聚合的FedSSA: 用于高效模型异构个性化联邦学习

    FedSSA: Semantic Similarity-based Aggregation for Efficient Model-Heterogeneous Personalized Federated Learning

    [https://arxiv.org/abs/2312.09006](https://arxiv.org/abs/2312.09006)

    FedSSA是一种基于语义相似度的聚合方法，用于高效模型异构个性化联邦学习。它通过异构 feature extractor 和同质 classification header 将每个客户端的模型拆分，并通过语义相似度进行头部参数聚合实现本地到全局的知识传输。此外，通过自适应参数稳定策略实现了全局到本地的知识传输。

    

    联邦学习（FL）是一种保护隐私的协作机器学习范式。传统的FL要求所有数据所有者（即FL客户端）训练相同的本地模型。这种设计并不适用于涉及数据和/或系统异构的场景。模型异构个性化FL（MHPFL）已经出现来解决这个挑战。现有的MHPFL方法通常依赖于具有相同学习任务性质的公共数据集，或者会产生高计算和通信成本。为了解决这些限制，我们提出了一种名为Federated Semantic Similarity Aggregation（FedSSA）的方法，该方法将每个客户端的模型分为异构（结构不同）特征提取器和同质（结构相同）分类头部。它通过基于语义相似度的头部参数聚合实现了本地到全局的知识传输。此外，通过自适应参数稳定策略实现了全局到本地的知识传输。

    Federated learning (FL) is a privacy-preserving collaboratively machine learning paradigm. Traditional FL requires all data owners (a.k.a. FL clients) to train the same local model. This design is not well-suited for scenarios involving data and/or system heterogeneity. Model-Heterogeneous Personalized FL (MHPFL) has emerged to address this challenge. Existing MHPFL approaches often rely on having a public dataset with the same nature of the learning task, or incur high computation and communication costs. To address these limitations, we propose the Federated Semantic Similarity Aggregation (FedSSA) approach, which splits each client's model into a heterogeneous (structure-different) feature extractor and a homogeneous (structure-same) classification header. It performs local-to-global knowledge transfer via semantic similarity-based header parameter aggregation. In addition, global-to-local knowledge transfer is achieved via an adaptive parameter stabilization strategy which fuses th
    
[^314]: 使用Metropolis-adjusted Mirror Langevin算法从约束空间中快速采样

    Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm

    [https://arxiv.org/abs/2312.08823](https://arxiv.org/abs/2312.08823)

    该论文提出了一种名为Metropolis-adjusted Mirror Langevin算法的方法，用于从约束空间中进行快速采样。这种算法是对Mirror Langevin算法的改进，通过添加接受-拒绝过滤器来消除渐近偏差，并具有指数优化依赖。

    

    我们提出了一种新的方法，称为Metropolis-adjusted Mirror Langevin算法，用于从其支持是紧凸集的分布中进行近似采样。该算法在Mirror Langevin算法（Zhang et al., 2020）的单步马尔科夫链中添加了一个接受-拒绝过滤器，Mirror Langevin算法是Mirror Langevin动力学的基本离散化。由于包含了这个过滤器，我们的方法相对于目标是无偏的，而已知的Mirror Langevin算法等Mirror Langevin动力学的离散化具有渐近偏差。对于该算法，我们还给出了混合到一个相对平滑、凸性好且与自共轭镜像函数相关的约束分布所需迭代次数的上界。由于包含Metropolis-Hastings过滤器导致的马尔科夫链是可逆的，我们得到了对误差的指数优化依赖。

    We propose a new method called the Metropolis-adjusted Mirror Langevin algorithm for approximate sampling from distributions whose support is a compact and convex set. This algorithm adds an accept-reject filter to the Markov chain induced by a single step of the Mirror Langevin algorithm (Zhang et al., 2020), which is a basic discretisation of the Mirror Langevin dynamics. Due to the inclusion of this filter, our method is unbiased relative to the target, while known discretisations of the Mirror Langevin dynamics including the Mirror Langevin algorithm have an asymptotic bias. For this algorithm, we also give upper bounds for the number of iterations taken to mix to a constrained distribution whose potential is relatively smooth, convex, and Lipschitz continuous with respect to a self-concordant mirror function. As a consequence of the reversibility of the Markov chain induced by the inclusion of the Metropolis-Hastings filter, we obtain an exponentially better dependence on the erro
    
[^315]: 学习具有潜在节点和结构化噪声的网络动力系统的因果结构

    Learning the Causal Structure of Networked Dynamical Systems under Latent Nodes and Structured Noise

    [https://arxiv.org/abs/2312.05974](https://arxiv.org/abs/2312.05974)

    本文提出了一种从部分可观测性条件下的时间序列数据中学习线性网络动力系统的隐藏因果网络的方法，通过特征嵌入和聚类来解决噪声相关性和部分可观测性的挑战。

    

    本文考虑在部分可观测性条件下，从部分节点的时间序列数据中学习线性网络动力系统（NDS）的隐藏因果网络。NDS的动力学由生成节点对之间的虚假关联的有色噪声驱动，从而使问题更加困难。为了解决噪声相关性和部分可观测性的挑战，我们为每对节点分配了从观测节点的时间序列数据计算的特征向量。特征嵌入被设计为具有结构一致性：存在一个仿射超平面，能够一致地将特征集分割，将与连接的节点对应的特征向量与与断开的节点对应的特征向量分开。因果推断问题因此通过对设计的特征进行聚类来解决。我们通过简单的基准监督方法展示了所提出的因果推断机制的竞争性能。

    This paper considers learning the hidden causal network of a linear networked dynamical system (NDS) from the time series data at some of its nodes -- partial observability. The dynamics of the NDS are driven by colored noise that generates spurious associations across pairs of nodes, rendering the problem much harder. To address the challenge of noise correlation and partial observability, we assign to each pair of nodes a feature vector computed from the time series data of observed nodes. The feature embedding is engineered to yield structural consistency: there exists an affine hyperplane that consistently partitions the set of features, separating the feature vectors corresponding to connected pairs of nodes from those corresponding to disconnected pairs. The causal inference problem is thus addressed via clustering the designed features. We demonstrate with simple baseline supervised methods the competitive performance of the proposed causal inference mechanism under broad connec
    
[^316]: 强化关注力中最短的支柱：增强大型语言模型的上下文意识，以实现有效的工具使用

    Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use

    [https://arxiv.org/abs/2312.04455](https://arxiv.org/abs/2312.04455)

    本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    

    在本文中，我们证明了大型语言模型(LLMs)中关注分配中的内在波形模式显著影响它们在需要高度上下文意识的任务中的性能，例如利用LLMs进行工具使用。具体而言，当关键信息在上下文中位于关注波形的低谷区域时，模型可能会忽视该信息，导致性能下降。为了解决这个问题，我们提出了一种名为“Attention Buckets”的新型推理方法。它允许LLMs通过多个并行过程处理输入。每个过程使用不同的基准角度进行旋转位置嵌入，从而创建出一个独特的关注波形。通过用一个过程的关注低谷补偿另一个过程的关注高峰，我们的方法增强了LLM对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
    
[^317]: 引起多义性的原因是什么？通过偶然因素的混合选择性的替代起源故事

    What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes

    [https://arxiv.org/abs/2312.03096](https://arxiv.org/abs/2312.03096)

    这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。

    

    多义性神经元——激活一组不相关特征的神经元——被视为解释任务优化深度网络的显著障碍，对AI安全性产生影响。传统的多义性起源故事是数据包含的“特征”多于神经元，因此学习执行任务迫使网络将多个不相关特征分配给同一个神经元，危及我们理解网络内部处理的能力。在这项工作中，我们提出了多义性的第二个且非互斥的替代起源故事。我们展示了即使有足够的神经元来表示数据中的所有特征，偶然多义性也可能产生，这是一种我们称之为“偶然多义性”的现象。通过理论和实验证明，偶然多义性可以由多种原因引起，包括正则化和神经噪音；这种偶然多义性发生是因为随机的因素。

    Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in
    
[^318]: 逆向强化学习比标准强化学习更困难吗？一个理论的观点

    Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective

    [https://arxiv.org/abs/2312.00054](https://arxiv.org/abs/2312.00054)

    逆向强化学习是从专家策略示范中学习奖励函数的问题，本文提出了在标准离线和在线设置下用多项式样本和运行时间进行高效逆向强化学习的结果线索，并提供了几乎最优的样本复杂性的下界。

    

    逆向强化学习（IRL）是从专家策略的示范中学习奖励函数的问题，在开发智能系统中起着关键作用。尽管在应用中广泛使用，但与标准强化学习相比，IRL的理论理解存在独特的挑战，且发展相对较少。本文首次提出了使用多项式样本和运行时间在标准离线和在线设置下进行高效IRL的结果线索。我们的算法和分析巧妙地采用了离线强化学习中常用的悲观原则，并在比现有工作中考虑的更强的度量标准下实现了IRL的保证。我们提供了下界，表明我们的样本复杂性几乎是最优的。

    Inverse Reinforcement Learning (IRL) -- the problem of learning reward functions from demonstrations of an \emph{expert policy} -- plays a critical role in developing intelligent systems. While widely used in applications, theoretical understandings of IRL present unique challenges and remain less developed compared with standard RL. For example, it remains open how to do IRL efficiently in standard \emph{offline} settings with pre-collected data, where states are obtained from a \emph{behavior policy} (which could be the expert policy itself), and actions are sampled from the expert policy.   This paper provides the first line of results for efficient IRL in vanilla offline and online settings using polynomial samples and runtime. Our algorithms and analyses seamlessly adapt the pessimism principle commonly used in offline RL, and achieve IRL guarantees in stronger metrics than considered in existing work. We provide lower bounds showing that our sample complexities are nearly optimal
    
[^319]: 零样本学习中的类别分布转移：学习鲁棒表示

    Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations

    [https://arxiv.org/abs/2311.18575](https://arxiv.org/abs/2311.18575)

    本文提出了一个模型来处理零样本学习中的类别分布转移问题，该模型假设转移原因在训练过程中是未知的属性。通过引入基于分层抽样的框架构建合成数据环境，我们能够将类别分布转移看作分布外问题，并提出了一种学习鲁棒表示的算法。实验结果表明，我们的方法在不同类别分布上的泛化能力显著提高。

    

    类别分布转移对零样本分类器来说尤为具有挑战性，因为它们依赖于从训练类别学到的表示，但部署在新的、未知的类别上。常见的类别分布转移原因是与类别相关的属性的改变，比如在人物识别中的种族或性别。在这项工作中，我们提出并分析了一个采用这个设置的模型，假设在训练过程中未知导致转移的属性。为了解决学习对这种转移鲁棒的数据表示的挑战，我们引入了一种基于分层抽样的框架来构建合成数据环境。尽管两种设置之间存在关键差异，但这个框架使我们能够将零样本学习中的类别分布转移转化为分布外问题。因此，我们提出了一种学习鲁棒表示的算法，并展示了我们的方法在模拟和真实数据集上显著改善了对不同类别分布的泛化能力。

    Class distribution shifts are particularly challenging for zero-shot classifiers, which rely on representations learned from training classes but are deployed on new, unseen ones. Common causes for such shifts are changes in attributes associated with classes, such as race or gender in person identification. In this work, we propose and analyze a model that adopts this setting, assuming that the attribute responsible for the shift is unknown during training. To address the challenge of learning data representations robust to such shifts, we introduce a framework based on hierarchical sampling to construct synthetic data environments. Despite key differences between the settings, this framework allows us to formulate class distribution shifts in zero-shot learning as out-of-distribution problems. Consequently, we present an algorithm for learning robust representations, and show that our approach significantly improves generalization to diverse class distributions in both simulations an
    
[^320]: 关于鲁棒过拟合：对抗训练引起的分布问题很重要

    On robust overfitting: adversarial training induced distribution matters

    [https://arxiv.org/abs/2311.16526](https://arxiv.org/abs/2311.16526)

    本文首先通过实验证实鲁棒过拟合与对抗训练轨迹上的扰动引起的分布的泛化困难性逐渐增加之间的相关性，然后给出了一个关于扰动引起的分布的泛化误差的新的上界，验证了其实用性，并提供了各种其他见解。

    

    对抗训练可以被看作是使用修改后的损失函数的标准训练。但在标准损失下，其泛化误差明显大于标准训练。这一现象被称为鲁棒过拟合，已经引起了相当大的研究关注但仍然是一个谜。本文首先通过实验证实鲁棒过拟合与对抗训练轨迹上的扰动引起的分布的泛化困难性逐渐增加之间的相关性。我们然后给出了一个关于扰动引起的分布的泛化误差的新的上界，其中一个被称为“局部分散”的扰动算子起到重要作用。我们还提供了实验结果来验证该上界的实用性，并提供了各种其他见解。

    Adversarial training may be regarded as standard training with a modified loss function. But its generalization error appears much larger than standard training under standard loss. This phenomenon, known as robust overfitting, has attracted significant research attention and remains largely as a mystery. In this paper, we first show empirically that robust overfitting correlates with the increasing generalization difficulty of the perturbation-induced distributions along the trajectory of adversarial training (specifically PGD-based adversarial training). We then provide a novel upper bound for generalization error with respect to the perturbation-induced distributions, in which a notion of the perturbation operator, referred to "local dispersion", plays an important role. Experimental results are presented to validate the usefulness of the bound and various additional insights are provided.
    
[^321]: 使用神经网络算法检测塞浦路斯希腊儿童的发展性语言障碍

    Detection of developmental language disorder in Cypriot Greek children using a neural network algorithm

    [https://arxiv.org/abs/2311.15054](https://arxiv.org/abs/2311.15054)

    该研究开发了一种使用神经网络算法进行发展性语言障碍（DLD）检测的自动化方法，并首次应用于塞浦路斯希腊儿童DLD人群。实验结果表明该方法具有高的分类效果。

    

    发展性语言障碍（DLD）的儿童在吸收各种语言结构方面遇到困难。早期识别和干预对于防止对儿童的学术、社交和情感发展产生长期负面影响至关重要。该研究旨在开发一种使用人工智能、特别是神经网络机器学习算法的自动化检测DLD的方法。该方案首次在塞浦路斯希腊儿童DLD人群中应用。神经网络模型使用从15名DLD患儿和15名健康对照组中收集的感知和产出数据进行训练，年龄范围为7岁10个月至10岁4个月。采用k-fold技术对算法进行交叉验证。使用准确率、精确度、召回率、F1分数和ROC/AUC曲线等指标评估模型的性能，以评估其在一组未知数据上进行准确预测的能力。结果表明高分类效果。

    Children with developmental language disorder (DLD) encounter difficulties in acquiring various language structures. Early identification and intervention are crucial to prevent negative long-term outcomes impacting the academic, social, and emotional development of children. The study aims to develop an automated method for the identification of DLD using artificial intelligence, specifically a neural network machine learning algorithm. This protocol is applied for the first time in a Cypriot Greek child population with DLD. The neural network model was trained using perceptual and production data elicited from 15 children with DLD and 15 healthy controls in the age range of 7;10 until 10;4. The k-fold technique was used to crossvalidate the algorithm. The performance of the model was evaluated using metrics such as accuracy, precision, recall, F1 score, and ROC/AUC curve to assess its ability to make accurate predictions on a set of unseen data. The results demonstrated high classifi
    
[^322]: 从被毒害的人类反馈中构建的通用越狱后门

    Universal Jailbreak Backdoors from Poisoned Human Feedback

    [https://arxiv.org/abs/2311.14455](https://arxiv.org/abs/2311.14455)

    本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。

    

    强化学习从人类反馈中（RLHF）用于对齐大型语言模型，以产生有用且无害的回应。然而，先前的研究显示，这些模型可以通过找到使模型恢复到未对齐行为的对抗提示来进行越狱。在本文中，我们考虑了一个新的威胁，攻击者通过毒害RLHF训练数据将“越狱后门”嵌入模型中。该后门将一个触发词嵌入模型中，类似于通用的“sudo命令”：在任何提示中添加触发词将使模型产生有害的回应，无需搜索对抗提示。通用越狱后门比先前研究的语言模型后门更强大，我们发现使用常见的后门攻击技术要困难得多。我们探究了RLHF中的设计决策对其所声称的鲁棒性的贡献，并发布一组被毒害模型的基准，以促进对通用越狱后门的未来研究。

    Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
    
[^323]: 解读并整合神经算子学习中的不变量与多种物理机制

    Deciphering and integrating invariants for neural operator learning with various physical mechanisms

    [https://arxiv.org/abs/2311.14361](https://arxiv.org/abs/2311.14361)

    本文提出了一种称为PIANO的方法，用于解读和整合来自具有不同物理机制的PDE序列中的物理不变量，从而进行算子学习。相比现有技术，在不同条件下PIANO可以显著降低PDE预测任务的相对误差。

    

    神经算子已被用作模拟物理系统的替代模型，以克服传统偏微分方程求解器的局限性。然而，大多数现有的算子学习方法假设数据来自单一物理机制，限制了它们在更现实的场景中的适用性和性能。为此，我们提出了一种称为PIANO（Physical Invariant Attention Neural Operator）的方法，用于解读和整合来自具有多种物理机制的PDE序列中的物理不变量（PI）来进行算子学习。PIANO采用自监督学习来提取物理知识，并采用注意机制将其整合到动态卷积层中。与现有技术相比，PIANO在具有不同系数、力或边界条件的PDE预测任务中，可以将相对误差减少13.6\% - 82.2\%。此外，多种下游任务表明，PIANO解读的PI嵌入与u对齐。

    Neural operators have been explored as surrogate models for simulating physical systems to overcome the limitations of traditional partial differential equation (PDE) solvers. However, most existing operator learning methods assume that the data originate from a single physical mechanism, limiting their applicability and performance in more realistic scenarios. To this end, we propose Physical Invariant Attention Neural Operator (PIANO) to decipher and integrate the physical invariants (PI) for operator learning from the PDE series with various physical mechanisms. PIANO employs self-supervised learning to extract physical knowledge and attention mechanisms to integrate them into dynamic convolutional layers. Compared to existing techniques, PIANO can reduce the relative error by 13.6\%-82.2\% on PDE forecasting tasks across varying coefficients, forces, or boundary conditions. Additionally, varied downstream tasks reveal that the PI embeddings deciphered by PIANO align well with the u
    
[^324]: 探究社会压力对COVID-19自适应动态的影响：研究面临流行病的不同人群的行为特征

    Exploring the impact of social stress on the adaptive dynamics of COVID-19: Typing the behavior of na\"ive populations faced with epidemics

    [https://arxiv.org/abs/2311.13917](https://arxiv.org/abs/2311.13917)

    该论文研究了COVID-19疫情对社会压力的影响，并探究了不同人群面对流行病的行为特征。研究发现传统模型无法完全解释COVID-19的观察现象，因此非医学层面上的迅速应对至关重要。

    

    在自然灾害的背景下，人类的应对不可避免地与自然因素交织在一起。COVID-19大流行作为一个重大的压力因素，凸显出不同国家在应对不同地区感染爆发的情况下的自适应动态之间的深刻差异。这强调了文化特征在自然灾害分析中的关键作用。对大规模的流行病的理论理解主要依赖于均场动力学模型。然而，传统的SIR-like模型未能完全解释COVID-19爆发初期观察到的现象。这些现象包括指数增长的意外停止，达到稳定期和多波动态的发生。当出现高度致命且陌生的感染爆发时，迅速以非医学的方式做出应对以减轻负面社会经济影响变得至关重要。

    In the context of natural disasters, human responses inevitably intertwine with natural factors. The COVID-19 pandemic, as a significant stress factor, has brought to light profound variations among different countries in terms of their adaptive dynamics in addressing the spread of infection outbreaks across different regions. This emphasizes the crucial role of cultural characteristics in natural disaster analysis. The theoretical understanding of large-scale epidemics primarily relies on mean-field kinetic models. However, conventional SIR-like models failed to fully explain the observed phenomena at the onset of the COVID-19 outbreak. These phenomena included the unexpected cessation of exponential growth, the reaching of plateaus, and the occurrence of multi-wave dynamics. In situations where an outbreak of a highly virulent and unfamiliar infection arises, it becomes crucial to respond swiftly at a non-medical level to mitigate the negative socio-economic impact. Here we present a
    
[^325]: 发现有效的土地利用规划政策

    Discovering Effective Policies for Land-Use Planning

    [https://arxiv.org/abs/2311.12304](https://arxiv.org/abs/2311.12304)

    通过学习代理模型并使用进化搜索过程，发现了可定制到不同位置的有效土地利用政策，为土地利用规划提供了一个潜在有用的工具。

    

    土地被分配给不同的用途，如森林、城市区域和农业，对陆地碳平衡和气候变化有重大影响。基于可用的土地利用变化的历史数据和相关的碳排放和吸收的模拟，可以学习到一个代理模型，从而能够高效评估决策者可选择的不同选项。然后可以使用进化搜索过程来发现特定位置的有效土地利用政策。该系统构建在Project Resilience平台上，并使用Land-Use Harmonization数据集LUH2和簿记模型BLUE进行评估。它生成可定制到不同位置的碳影响和土地利用变化量的帕累托前沿，从而为土地利用规划提供了一个潜在有用的工具。

    How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
    
[^326]: 高效强化学习在部分可观察性下的应用

    Efficient Reinforcement Learning from Partial Observability

    [https://arxiv.org/abs/2311.12244](https://arxiv.org/abs/2311.12244)

    该论文提出了一种基于表示的方法，用于从部分观测中进行有效的强化学习。该方法能够处理部分可观测性带来的计算和统计挑战，并在各种基准测试中展现出优于先进算法的性能。

    

    在大多数实际应用中，状态信息只能部分观测到，这破坏了马尔科夫决策过程的假设，导致将观测与状态相混淆的算法表现不佳。而部分可观测马尔科夫决策过程（POMDP）提供了一个允许在学习、探索和规划中考虑部分可观测性的通用框架，但也带来了显著的计算和统计挑战。为解决这些困难，我们提出了一个基于表示的视角，提供了一个统一的框架和可行的算法方法，用于从部分观测中进行实际的强化学习。我们提供了理论分析来证明所提出算法的统计效率，并经验性地证明了在各种基准测试中，所提出的算法在部分观测下能够超越最先进性能，推动了可靠的强化学习。

    In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinf
    
[^327]: 关于衡量自然语言解释的忠诚度或自一致性

    On Measuring Faithfulness or Self-consistency of Natural Language Explanations

    [https://arxiv.org/abs/2311.07466](https://arxiv.org/abs/2311.07466)

    本文论述了衡量自然语言解释的忠诚度或自一致性的问题。我们提出了自一致性测试来评估解释的输出级别的一致性。我们通过构建比较一致性测试库，并引入了新的自一致性度量CC-SHAP来支持我们的观点。

    

    大型语言模型（LLMs）可以通过事后或思维链（CoT）解释其预测。但是，LLM可能会编造听起来合理但不忠实于其基本推理的解释。最近的工作设计了旨在判断事后或CoT解释忠实度的测试。在这项工作中，我们认为这些忠实度测试不是衡量模型内部工作的忠实度，而是衡量其输出级别的自一致性。我们的贡献有三个方面：i）我们在模型可解释性的背景下澄清了忠实度测试的地位，将其描述为自一致性测试。我们通过ii）构建了一个比较一致性的测试库，首次在11个开放式LLMs和5个任务的通用套件上比较了现有测试，包括iii）我们的新的自一致性度量CC-SHAP。CC-SHAP是LLM自一致性的细粒度度量（而不是测试）。它进行比较。

    Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
    
[^328]: 数据污染问题: 一种检测和估计大型语言模型中污染的工具

    Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models

    [https://arxiv.org/abs/2311.06233](https://arxiv.org/abs/2311.06233)

    这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。

    

    我们提出了数据污染问题（DCQ），这是一种简单而有效的方法，用于检测大型语言模型（LLM）中的数据污染并估计其数量。具体而言，我们将数据污染检测视为一系列的多项选择问题，并设计了一种测验形式，其中创建了每个数据集实例的三个扰动版本。这些变化仅包括词级扰动。生成的扰动版本与原始实例一起形成DCQ中的选项，额外的选项适应了提供的选择都不正确的可能性。鉴于在选择之间唯一的区别信号是与原始实例的确切措辞相关，如果在预训练阶段已经接触到原始实例，语言模型当被要求从选项中识别原始实例时，倾向于选择原始实例--这是语言模型固有的特性。在使用GPT-4/3.5进行多个数据集的测试中，我们的结果完全缺少准确性。

    We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
    
[^329]: 用于系外行星凌星和H0推断的核、均值和噪声边缘化高斯过程

    Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet transits and $H_0$ inference

    [https://arxiv.org/abs/2311.04153](https://arxiv.org/abs/2311.04153)

    该论文提出了一种基于贝叶斯方法的核、均值和噪声边缘化高斯过程，用于系外行星凌星和H0推断。通过核选择和核超参数的边缘化以及贝叶斯模型比较，可以实现核选择和推断。

    

    使用完全贝叶斯方法，将高斯过程回归扩展为包括核选择和核超参数的边缘化。此外，通过证据进行贝叶斯模型比较，可以直接比较核选择。通过在高维空间中嵌入离散核选择和超参数，使用嵌套抽样进行联合后验计算。在系外行星凌星光变曲线模拟的合成数据上探索了核恢复和均值函数推断。随后，将该方法扩展到均值函数和噪声模型的边缘化，并应用于从实际的红移相关哈勃参数测量中推断当今哈勃参数H0，这些参数来自于宇宙学模型独立的宇宙计时器和ΛCDM依赖的声学谐振。

    Using a fully Bayesian approach, Gaussian Process regression is extended to include marginalisation over the kernel choice and kernel hyperparameters. In addition, Bayesian model comparison via the evidence enables direct kernel comparison. The calculation of the joint posterior was implemented with a transdimensional sampler which simultaneously samples over the discrete kernel choice and their hyperparameters by embedding these in a higher-dimensional space, from which samples are taken using nested sampling. Kernel recovery and mean function inference were explored on synthetic data from exoplanet transit light curve simulations. Subsequently, the method was extended to marginalisation over mean functions and noise models and applied to the inference of the present-day Hubble parameter, $H_0$, from real measurements of the Hubble parameter as a function of redshift, derived from the cosmologically model-independent cosmic chronometer and $\Lambda$CDM-dependent baryon acoustic oscill
    
[^330]: SQLformer：深度自回归查询图生成用于文本到SQL翻译

    SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation

    [https://arxiv.org/abs/2310.18376](https://arxiv.org/abs/2310.18376)

    SQLformer是一个用于文本到SQL翻译的深度自回归查询图生成模型，采用了特定的Transformer架构，并通过结构归纳偏差解决领域泛化和自然语言与SQL查询对齐的难题。

    

    近年来，对于文本到SQL翻译的兴趣日益增长，这是将自然语言问题转化为可执行SQL查询的任务。这项技术具有潜在的潜力，可以使数据库中的数据提取民主化。然而，其中一些主要障碍包括领域泛化，即适应以前未见到的数据库，并且将自然语言问题与相应的SQL查询对齐。为了克服这些挑战，我们引入了SQLformer，这是一种针对执行文本到SQL翻译任务而设计的新型Transformer体系结构。我们的模型以自回归的方式预测SQL查询，并在编码器和解码器层中结合结构归纳偏差。这种偏差是由数据库表和列选择引导的，有助于解码器以广度优先搜索的规范顺序生成SQL查询的图形表示。全面的实验说明了现阶段的技术水平

    In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
    
[^331]: 带有参数高效prompt调整和自适应优化的大型语言模型的联邦学习

    Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization

    [https://arxiv.org/abs/2310.15080](https://arxiv.org/abs/2310.15080)

    本文提出了一种带有参数高效prompt调整和自适应优化的联邦学习方法，以实现大型语言模型的高效和有效训练。

    

    联邦学习是一种有前途的范式，可以实现分散数据的协同模型训练。然而，大型语言模型的训练过程通常涉及更新大量的参数，这限制了联邦学习技术在实际场景中处理大型语言模型的适用性。prompt调整可以显著减少需要更新的参数数量，但它可能导致性能下降或降低训练效率。在联邦学习中直接使用prompt调整通常会导致非平凡的通信成本和性能大幅下降。此外，分散数据通常是非独立和同分布的，并带来客户端漂移问题和因此的低性能。本文提出了一种参数高效的提示调整方法，即FedPepTAO，以实现大型语言模型的高效和有效的联邦学习。首先，提出了一种高效的部分提示调整方法来改善训练性能。

    Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improv
    
[^332]: 学习在噪声化学趋化中空间和时间信息的最佳整合

    Learning optimal integration of spatial and temporal information in noisy chemotaxis

    [https://arxiv.org/abs/2310.10531](https://arxiv.org/abs/2310.10531)

    本研究通过深度强化学习探究了在噪声化学趋化中空间和时间信息的最佳整合。研究发现，最佳策略在小细胞和大细胞尺寸下分别为纯粹的时间和空间策略，并且过渡区域的组合策略性能优于其他策略。

    

    我们研究了由空间梯度估计驱动的化学趋化和由时间估计驱动的化学趋化之间的边界。尽管人们已经知道在高噪声水平下，对于小生物来说，空间化学趋化变得不利，但目前还不清楚最佳策略是否存在不连续的切换，还是存在连续的过渡。在本研究中，我们采用深度强化学习来研究在先验无限制情况下空间和时间信息的可能整合。我们通过循环神经网络对这种组合化学趋化策略进行参数化，并使用化学趋化细胞的最简理论模型进行评估。通过与受限制的策略变体进行比较，我们发现它在小细胞和大细胞尺寸下分别收敛到纯粹的时间和空间策略。我们发现，这种策略之间的过渡是连续的，组合策略在过渡区域的性能优于受限制的变体及其他单一策略。

    We investigate the boundary between chemotaxis driven by spatial estimation of gradients and chemotaxis driven by temporal estimation. While it is well known that spatial chemotaxis becomes disadvantageous for small organisms at high noise levels, it is unclear whether there is a discontinuous switch of optimal strategies or a continuous transition exists. Here, we employ deep reinforcement learning to study the possible integration of spatial and temporal information in an a priori unconstrained manner. We parameterize such a combined chemotactic policy by a recurrent neural network and evaluate it using a minimal theoretical model of a chemotactic cell. By comparing with constrained variants of the policy, we show that it converges to purely temporal and spatial strategies at small and large cell sizes, respectively. We find that the transition between the regimes is continuous, with the combined strategy outperforming in the transition region both the constrained variants as well as
    
[^333]: 使用机器学习和经典技术的统计推断：基于积累的局部效应（ALE）

    Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)

    [https://arxiv.org/abs/2310.09877](https://arxiv.org/abs/2310.09877)

    本研究通过引入创新的工具和技术，使用积累的局部效应（ALE）进行统计推断，在解决小数据集、直观特征和健壮推断方面取得了突破。这项工作推动了ALE及其应用的研究。

    

    积累的局部效应（ALE）是一种对黑盒机器学习（ML）算法结果进行全局解释的模型无关方法。使用ALE进行统计推断面临至少三个挑战：确保ALE分析的可靠性，尤其在小数据集的情况下；直观地表征变量在ML中的整体效应；以及从ML数据分析中进行健壮的推断。为此，我们引入了创新的工具和技术，使用ALE进行统计推断，建立了适应数据集大小的自助法置信区间，并引入了直观指示对结果变量和标准化尺度上的效应的ALE效应大小度量。此外，我们演示了如何使用这些工具绘制可靠的统计推断，反映了ALE熟练突出的灵活模式，实现了R中“ale”包中的实现。这项工作推动了关于ALE及其应用的讨论。

    Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicabi
    
[^334]: 基于得分的扩散在潜在空间中生成混合类型的表格数据

    Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space

    [https://arxiv.org/abs/2310.09656](https://arxiv.org/abs/2310.09656)

    本文介绍了一种名为Tabsyn的方法，利用潜在空间中的扩散模型和变分自编码器生成具有混合类型数据的表格数据。Tabsyn具有通用性、质量和速度等关键优势，通过将不同类型的数据转换为统一空间并捕捉列间关系，优化潜在嵌入的分布来生成高质量的合成数据，同时生成速度更快。

    

    最近在表格数据生成领域的进展极大地提高了合成数据的质量。然而，由于表格数据的复杂多样的分布和融合的数据类型，将扩散模型扩展到表格数据是具有挑战性的。本文介绍了Tabsyn，一种利用潜在空间中的扩散模型和变分自编码器（VAE）生成表格数据的方法。所提出的Tabsyn的主要优势包括：（1）通用性：能够处理各种数据类型，并将其转换为单一的统一空间，并明确捕捉列间关系；（2）质量：通过优化潜在嵌入的分布以增强后续扩散模型的训练，从而生成高质量的合成数据；（3）速度：比现有的基于扩散的方法具有更少的反向步骤和更快的生成速度。对于六个数据集的大量实验证明，Tabsyn优于现有的方法。

    Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces Tabsyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capture inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data, (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms exist
    
[^335]: 层次化多边汇运输用于网络对齐

    Hierarchical Multi-Marginal Optimal Transport for Network Alignment

    [https://arxiv.org/abs/2310.04470](https://arxiv.org/abs/2310.04470)

    层次化多边汇运输（HOT）框架通过融合Gromov-Wasserstein（FGW）重心将多个网络分解为对齐簇，并将FGW距离广义化到多边汇环境，实现多网络的联合对齐。实验证明，HOT相对于统计方法取得了显著的改进。

    

    跨网络找到节点对应关系，即多网络对齐，是在多个网络上进行联合学习的基本先决条件。尽管在两个网络上对齐方面取得了巨大的成功，但是由于解空间指数增长和高阶差异度量缺乏，多网络对齐的文献有限。为了填补这一空白，我们提出了一种层次化多边汇运输框架（HOT）用于多网络对齐。为了处理庞大的解空间，多个网络通过融合的Gromov-Wasserstein（FGW）重心被分解为较小的对齐簇。为了描绘多个网络之间的高阶关系，FGW距离被推广到多边汇环境，基于这个距离可以实现网络的联合对齐。进一步开发了一种快速的近端点方法，保证收敛到局部最优解。大量的实验证明，我们提出的HOT相对于统计方法取得了显著的改进。

    Finding node correspondence across networks, namely multi-network alignment, is an essential prerequisite for joint learning on multiple networks. Despite great success in aligning networks in pairs, the literature on multi-network alignment is sparse due to the exponentially growing solution space and lack of high-order discrepancy measures. To fill this gap, we propose a hierarchical multi-marginal optimal transport framework named HOT for multi-network alignment. To handle the large solution space, multiple networks are decomposed into smaller aligned clusters via the fused Gromov-Wasserstein (FGW) barycenter. To depict high-order relationships across multiple networks, the FGW distance is generalized to the multi-marginal setting, based on which networks can be aligned jointly. A fast proximal point method is further developed with guaranteed convergence to a local optimum. Extensive experiments and analysis show that our proposed HOT achieves significant improvements over the stat
    
[^336]: 关于非凸双层优化和一阶随机逼近的罚函数方法研究

    On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation

    [https://arxiv.org/abs/2309.01753](https://arxiv.org/abs/2309.01753)

    本研究通过罚函数方法研究非凸双层优化问题，建立了罚函数与超级目标函数之间的强连接，并提出了一阶算法来找到驻点。

    

    在这项工作中，我们研究了求解双层优化问题（BO）的一阶算法，在这里，目标函数对于两个层级来说都是光滑的，但可能是非凸的，并且变量受限于闭合凸集。作为第一步，我们通过罚项方法研究了BO的概貌，其中上层和下层目标以加权和的罚项参数$\sigma>0$的形式相结合。特别地，我们通过明确地刻画值和导数两者必须$O(\sigma)$接近的条件，建立了罚函数与超级目标函数之间的强连接。我们分析的一个副产品是当下层问题在最小条件下具有多个解时，超级目标函数梯度的显式公式，这可能具有独立的研究价值。接下来，将罚函数形式视为原始BO的$O(\sigma)$-近似，我们提出了一阶算法，找到一个$\epsilon$-驻点。

    In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper- and lower-level objectives are combined in a weighted sum with penalty parameter $\sigma > 0$. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be $O(\sigma)$-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as $O(\sigma)$-approximation of the original BO, we propose first-order algorithms that find an $\epsilon$-stationar
    
[^337]: 通过频率正则化解决非矩形奖励强鲁棒马尔可夫决策过程

    Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization

    [https://arxiv.org/abs/2309.01107](https://arxiv.org/abs/2309.01107)

    本研究通过引入策略访问频率正则化，解决了非矩形奖励强鲁棒马尔可夫决策过程（RMDPs）的问题，并且提出了一种策略梯度方法并证明了其收敛性。

    

    在强鲁棒马尔可夫决策过程（RMDPs）中，假设奖励和转移动态位于给定的不确定性集合中。通过针对该集合中最具敌对性的模型下的最大回报，RMDPs解决了对错误环境的性能敏感性问题。然而，为了保持计算的可行性，传统上对于每个状态独立地构建不确定性集合。这种所谓的矩形条件仅仅是基于计算上的考虑。因此，它缺乏实际的动机，可能导致过度保守的行为。在这项工作中，我们研究了耦合奖励RMDPs，其中转移核是固定的，但是奖励函数在与名义奖励函数相距 α 半径内。我们直接联系了这种非矩形奖励-RMDPs和应用策略访问频率正则化之间的关系。我们介绍了一种策略梯度方法并证明了其收敛性。数值实验证明了所学策略的鲁棒性

    In robust Markov decision processes (RMDPs), it is assumed that the reward and the transition dynamics lie in a given uncertainty set. By targeting maximal return under the most adversarial model from that set, RMDPs address performance sensitivity to misspecified environments. Yet, to preserve computational tractability, the uncertainty set is traditionally independently structured for each state. This so-called rectangularity condition is solely motivated by computational concerns. As a result, it lacks a practical incentive and may lead to overly conservative behavior. In this work, we study coupled reward RMDPs where the transition kernel is fixed, but the reward function lies within an $\alpha$-radius from a nominal one. We draw a direct connection between this type of non-rectangular reward-RMDPs and applying policy visitation frequency regularization. We introduce a policy-gradient method and prove its convergence. Numerical experiments illustrate the learned policy's robustness
    
[^338]: 使用连续正态流采样格点Nambu-Goto弦

    Sampling the lattice Nambu-Goto string using Continuous Normalizing Flows

    [https://arxiv.org/abs/2307.01107](https://arxiv.org/abs/2307.01107)

    本文提出了一种数值方法，使用连续正态流模型对有效弦理论进行建模，并得到可靠的数值估计结果。

    

    有效弦理论（EST）是一种强大的非微扰方法，用于描述Yang-Mills理论中的约束现象，将约束通量管模型化为一维薄弦。EST计算通常使用zeta函数规范化，但在某些情况下（例如研究通量管形状或超越Nambu-Goto EST的高阶修正），涉及的可观测量太复杂，无法通过这种方式解决。在本文中，我们提出了一种基于最新机器学习方法的数值方法，以解决这个问题。以Nambu-Goto弦为实验对象，我们展示了使用一种称为连续正态流的新型深度生成模型能够获得EST预测的可靠数值估计。

    Effective String Theory (EST) represents a powerful non-perturbative approach to describe confinement in Yang-Mills theory that models the confining flux tube as a thin vibrating string. EST calculations are usually performed using the zeta-function regularization: however there are situations (for instance the study of the shape of the flux tube or of the higher order corrections beyond the Nambu-Goto EST) which involve observables that are too complex to be addressed in this way. In this paper we propose a numerical approach based on recent advances in machine learning methods to circumvent this problem. Using as a laboratory the Nambu-Goto string, we show that by using a new class of deep generative models called Continuous Normalizing Flows it is possible to obtain reliable numerical estimates of EST predictions.
    
[^339]: Hyp-OW: 利用超几何距离的层次结构学习增强开放世界目标检测

    Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection

    [https://arxiv.org/abs/2306.14291](https://arxiv.org/abs/2306.14291)

    Hyp-OW是一种利用超几何距离的层次结构学习增强开放世界目标检测的方法，通过超类正则化器学习和建模已知项目的层次表示，通过基于相似度距离的重新标记模块有效地检测未知对象。

    

    开放世界目标检测(OWOD)是一项具有挑战性且现实的任务，超越了标准目标检测任务的范围。它需要在检测已知和未知对象的同时，整合学习到的知识用于未来的任务。然而，“未知性”在不同上下文中有很大的变化。例如，在自动驾驶场景中，树通常被认为是背景的一部分，但在家庭环境中可能具有重要性。我们认为这种上下文信息应该已经嵌入到已知类别中。换句话说，已知和未知项之间应该存在语义或潜在的结构关系等待发现。受到这一观察的启发，我们提出了Hyp-OW，一种通过超类正则化器来学习和建模已知项目的层次表示的方法。利用这种表示，我们可以通过基于相似度距离的重新标记模块有效地检测未知对象。大量实验证明了我们方法的有效性。

    Open World Object Detection (OWOD) is a challenging and realistic task that extends beyond the scope of standard Object Detection task. It involves detecting both known and unknown objects while integrating learned knowledge for future tasks. However, the level of "unknownness" varies significantly depending on the context. For example, a tree is typically considered part of the background in a self-driving scene, but it may be significant in a household context. We argue that this contextual information should already be embedded within the known classes. In other words, there should be a semantic or latent structure relationship between the known and unknown items to be discovered. Motivated by this observation, we propose Hyp-OW, a method that learns and models hierarchical representation of known items through a SuperClass Regularizer. Leveraging this representation allows us to effectively detect unknown objects using a similarity distance-based relabeling module. Extensive experi
    
[^340]: 基于结构化状态抽象的协同多智能体导航学习

    Cooperative Multi-Agent Learning for Navigation via Structured State Abstraction

    [https://arxiv.org/abs/2306.11336](https://arxiv.org/abs/2306.11336)

    本文提出了一种基于结构化状态抽象的协同多智能体导航学习方法，通过应急通信实现智能体之间的协作和信息共享，同时使用神经网络架构来学习自适应的状态空间抽象和通信协议。仿真结果表明，该方法能够显著减少状态空间的大小并提升导航性能。

    

    协同多智能体强化学习（MARL）用于导航，使得智能体能够合作实现导航目标。通过应急通信，智能体学习协调和共享信息的通信协议以实现导航任务。在应急通信中，没有预先指定使用规则的符号被交换，其中的含义和语法通过训练逐渐形成。在MARL环境中同时学习导航策略和通信协议非常复杂，因为需要探索的状态空间非常庞大。为了应对这种复杂性，本研究提出了一种新颖的神经网络架构，用于联合学习参与导航任务的智能体之间的自适应状态空间抽象和通信协议。目标是设计一个自适应的抽象器，显著减少需要探索的状态空间的大小，同时保持策略性能不受影响。仿真结果表明，该方法能够提高导航性能并减少状态空间的探索。

    Cooperative multi-agent reinforcement learning (MARL) for navigation enables agents to cooperate to achieve their navigation goals. Using emergent communication, agents learn a communication protocol to coordinate and share information that is needed to achieve their navigation tasks. In emergent communication, symbols with no pre-specified usage rules are exchanged, in which the meaning and syntax emerge through training. Learning a navigation policy along with a communication protocol in a MARL environment is highly complex due to the huge state space to be explored. To cope with this complexity, this work proposes a novel neural network architecture, for jointly learning an adaptive state space abstraction and a communication protocol among agents participating in navigation tasks. The goal is to come up with an adaptive abstractor that significantly reduces the size of the state space to be explored, without degradation in the policy performance. Simulation results show that the pr
    
[^341]: 通过执行反馈使语言模型成为更好的工具学习者

    Making Language Models Better Tool Learners with Execution Feedback

    [https://arxiv.org/abs/2305.13068](https://arxiv.org/abs/2305.13068)

    这篇论文提出了一个名为TRICE的框架，通过执行反馈实现语言模型的工具学习，使其能够学会何时以及如何有效地使用工具。

    

    工具作为关键的界面，使人类能够理解和改变环境。随着基础模型的出现，AI系统可以利用工具扩展其能力并与真实世界互动。现有的工具学习方法包括监督微调和提示工程方法，通常使大型语言模型不加选择地利用工具，因为复杂任务往往超出了它们自身的能力。然而，为简单任务引入工具（模型本身可以轻松解决的任务），可能会无意间传播错误而不是提高性能。因此，研究问题是：我们能否教会语言模型何时以及如何使用工具？为满足这个需求，我们提出了Tool leaRning wIth exeCution fEedback (TRICE)，这是一个两阶段的端到端框架，使模型能够通过从工具执行中得到的反馈不断学习，从而学会何时以及如何有效地使用工具。

    Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
    
[^342]: 通过基于学习的自适应优化纠正误差的量子近似优化算法

    Error-mitigated Quantum Approximate Optimization via Learning-based Adaptive Optimization

    [https://arxiv.org/abs/2303.14877](https://arxiv.org/abs/2303.14877)

    我们设计了一种双自适应区域贝叶斯优化算法，用于提升量子近似优化算法的性能。实验结果表明，该算法在性能上远远超过了传统的方法。

    

    组合优化问题普遍存在，并且在一般情况下很难求解。量子计算被视为一种强大的工具，可以在解决某些问题时具有潜在的计算优势。量子近似优化算法（QAOA）是最具代表性的量子-经典混合算法之一，旨在通过将离散优化问题转化为连续电路参数域上的经典优化问题来解决特定的组合优化问题。QAOA目标函数在参数变量上的景观因其普遍存在的局部最小值和贫瘠平台而臭名昭著，其可行性在很大程度上依赖于经典优化算法的有效性。为了提高QAOA的性能，我们设计了一种双自适应区域贝叶斯优化（DARBO），一种适应性经典优化器用于QAOA。我们的实验结果表明，该算法在性能上远远超过了传统的grad方法。

    Combinatorial optimization problems are ubiquitous and computationally hard to solve in general. Quantum computing is envisioned as a powerful tool offering potential computational advantages for solving some of these problems. Quantum approximate optimization algorithm (QAOA), one of the most representative quantum-classical hybrid algorithms, is designed to solve certain combinatorial optimization problems by transforming a discrete optimization problem into a classical optimization problem over a continuous circuit parameter domain. QAOA objective landscape over the parameter variables is notorious for pervasive local minima and barren plateaus, and its viability in training significantly relies on the efficacy of the classical optimization algorithm. To enhance the performance of QAOA, we design double adaptive-region Bayesian optimization (DARBO), an adaptive classical optimizer for QAOA. Our experimental results demonstrate that the algorithm greatly outperforms conventional grad
    
[^343]: 可扩展的分布式图上神经网络训练

    Scalable Neural Network Training over Distributed Graphs

    [https://arxiv.org/abs/2302.13053](https://arxiv.org/abs/2302.13053)

    RETEXO是第一个消除分布式图神经网络训练中通信瓶颈的框架，通过新的训练过程懒消息传递来改善网络通信效率。

    

    图神经网络（GNN）在涉及图结构数据的各种机器学习任务中发挥着重要作用，包括预测蛋白质结构和提供个性化推荐等。实际世界中的图数据往往需要分布式存储在许多计算机上，原因不仅是因为容量限制，还有数据所在地或隐私法律的要求。在这种设置中，网络通信成本很高，成为训练GNN的主要瓶颈。迄今为止，分布式GNN训练的优化主要针对数据级别的改进，例如缓存、网络感知划分和子采样等，这些方法适用于数据中心类似的设置，其中图数据对单个实体可访问且数据传输成本被忽略。我们提出了RETEXO，这是一种可以消除分布式GNN训练中严重通信瓶颈的首个框架，同时尊重任何给定的数据分区配置。关键是通过一种新的训练过程，即懒消息传递，重新排序了消息传递的顺序。

    Graph neural networks (GNNs) fuel diverse machine learning tasks involving graph-structured data, ranging from predicting protein structures to serving personalized recommendations. Real-world graph data must often be stored distributed across many machines not just because of capacity constraints, but because of compliance with data residency or privacy laws. In such setups, network communication is costly and becomes the main bottleneck to train GNNs. Optimizations for distributed GNN training have targeted data-level improvements so far -- via caching, network-aware partitioning, and sub-sampling -- that work for data center-like setups where graph data is accessible to a single entity and data transfer costs are ignored.   We present RETEXO, the first framework which eliminates the severe communication bottleneck in distributed GNN training while respecting any given data partitioning configuration. The key is a new training procedure, lazy message passing, that reorders the sequen
    
[^344]: STERLING: 在二分图上的协同表示学习

    STERLING: Synergistic Representation Learning on Bipartite Graphs

    [https://arxiv.org/abs/2302.05428](https://arxiv.org/abs/2302.05428)

    STERLING是一种在二分图上进行协同表示学习的模型，通过保留局部和全局协同性，学习节点嵌入，无需负节点对。这种方法通过最大化正节点对的相似性和共聚簇的互信息来提高连接性。

    

    二分图表示学习的基本挑战是如何提取有信息量的节点嵌入。自监督学习 (SSL) 是解决这一挑战的一种有希望的范式。最近的二分图 SSL 方法大多基于对比学习，在区分正负节点对的基础上学习嵌入。然而，对比学习通常需要大量负节点对，这可能导致计算负担和语义错误。本文介绍了一种新的协同表示学习模型 (STERLING)，可以在没有负节点对的情况下学习节点嵌入。STERLING 保留了二分图中的独特局部和全局协同性。局部协同性通过最大化跨类型和同类型正节点对的相似性来捕捉，全局协同性则通过最大化共聚簇的互信息来捕捉。理论分析表明 STERLING 可以提高连接性。

    A fundamental challenge of bipartite graph representation learning is how to extract informative node embeddings. Self-Supervised Learning (SSL) is a promising paradigm to address this challenge. Most recent bipartite graph SSL methods are based on contrastive learning which learns embeddings by discriminating positive and negative node pairs. Contrastive learning usually requires a large number of negative node pairs, which could lead to computational burden and semantic errors. In this paper, we introduce a novel synergistic representation learning model (STERLING) to learn node embeddings without negative node pairs. STERLING preserves the unique local and global synergies in bipartite graphs. The local synergies are captured by maximizing the similarity of the inter-type and intra-type positive node pairs, and the global synergies are captured by maximizing the mutual information of co-clusters. Theoretical analysis demonstrates that STERLING could improve the connectivity between 
    
[^345]: 使用预训练的嵌入和句子包的高效灵活的主题建模

    Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences

    [https://arxiv.org/abs/2302.03106](https://arxiv.org/abs/2302.03106)

    本文提出了一种使用预训练的嵌入和句子包进行高效灵活的主题建模方法，通过结合生成过程模型和聚类，提供了使用先验自定义主题-文档分布的可能性，实验表明该方法在计算负担较小的情况下取得了最新的结果。

    

    预训练语言模型在许多自然语言处理任务中取得了最新的突破。然而，在主题建模方面，统计生成模型（如LDA）仍然普遍存在，而这些模型不容易融入上下文词向量。它们可能会生成与人类判断不一致的主题。本文提出了一种新颖的主题建模和推断算法。我们提出了一种句子包（BoS）的方法，以句子作为分析单位。通过结合生成过程模型和聚类，我们使用预训练的句子嵌入。我们基于期望最大化、硬分配和一个退火过程的快速推断算法。评估结果显示，我们的方法能以相对较小的计算需求获得最新的结果。与利用词嵌入的之前方法相比，我们的方法也更加灵活，因为它提供了使用先验自定义主题-文档分布的可能性。

    Pre-trained language models have led to a new state-of-the-art in many NLP tasks. However, for topic modeling, statistical generative models such as LDA are still prevalent, which do not easily allow incorporating contextual word vectors. They might yield topics that do not align well with human judgment. In this work, we propose a novel topic modeling and inference algorithm. We suggest a bag of sentences (BoS) approach using sentences as the unit of analysis. We leverage pre-trained sentence embeddings by combining generative process models and clustering. We derive a fast inference algorithm based on expectation maximization, hard assignments, and an annealing process. The evaluation shows that our method yields state-of-the art results with relatively little computational demands. Our method is also more flexible compared to prior works leveraging word embeddings, since it provides the possibility to customize topic-document distributions using priors. Code and data is at \url{http
    
[^346]: 在非静态上下文驱动环境中的在线强化学习

    Online Reinforcement Learning in Non-Stationary Context-Driven Environments

    [https://arxiv.org/abs/2302.02182](https://arxiv.org/abs/2302.02182)

    提出了一种名为LCPO的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧经验进行锚定来解决强化学习中的灾难性遗忘问题。

    

    我们研究了在非静态环境中的在线强化学习，其中一个随时间变化的外生上下文过程影响着环境动态。在线强化学习在这样的环境中具有挑战性，因为存在“灾难性遗忘”现象。随着训练过程中的新经验增加，代理 tend to forget 先前的知识。以往的方法通常假设任务标签（这在实践中往往是不存在的）或者使用脱机策略学习方法，但这些方法存在不稳定性和性能差的问题。我们提出了一种名为 Locally Constrained Policy Optimization (LCPO) 的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧的经验进行锚定来解决灾难性遗忘问题。为了实现这种锚定，LCPO使用来自当前上下文分布之外的经验样本来局部约束策略优化。我们在Mujoco、经典控制和计算机系统环境中使用多种合成和真实上下文跟踪，评估了LCPO的性能，并发现它能够取得令人满意的结果。

    We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice) or use off-policy methods that suffer from instability and poor performance.   We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it o
    
[^347]: 通过图的双连通性重新思考GNN的表达能力

    Rethinking the Expressive Power of GNNs via Graph Biconnectivity

    [https://arxiv.org/abs/2301.09505](https://arxiv.org/abs/2301.09505)

    本文从根本上不同的角度重新思考了图神经网络（GNN）的表达能力，通过引入一类新的表达度量方法，即图的双连通性，并强调了它们在理论和实践中的重要性。令人惊讶的是，在对以前的GNN架构进行彻底审查后，发现大多数架构都没有对这些度量具有表达能力。唯一的例外是ESAN框架。

    

    设计具有表达能力的图神经网络(GNNs)是学习图结构数据的一个核心主题。尽管已经提出了很多方法来改进GNNs在Weisfeiler-Lehman (WL)测试方面的表现，但是普遍还存在对它们能够系统和可证明地获得的额外能力的缺乏深入了解。本文从根本上不同的角度来研究GNNs的表达能力，超越了WL测试。具体地，我们引入了一类新的通过图的双连通性的表达度量，并强调它们在理论和实践中的重要性。由于双连通性可以使用简单的算法进行计算，并且具有线性的计算成本，因此很自然地可以期望流行的GNNs也可以很容易地进行学习。然而，经过对以前的GNN架构的彻底审查，我们惊讶地发现大多数架构对于任何这些度量都不具有表达能力。唯一的例外是ESAN框架，对于该框架，我们给出了一个理论的解释。

    Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework, for which we give a theoretical just
    
[^348]: 使用学习到的动力学模型的高效基于偏好的强化学习

    Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models

    [https://arxiv.org/abs/2301.04741](https://arxiv.org/abs/2301.04741)

    本文研究了在基于偏好的强化学习（PbRL）中使用学习到的动力学模型的好处和挑战，并提供了实证证据：（1）相对于模型无关的PbRL，学习到的动力学模型可以大大减少环境交互次数，（2）学习到的动力学模型可以安全高效地合成多样化的偏好查询，（3）基于次优示范的奖励预训练可以在没有环境交互的情况下进行。

    

    基于偏好的强化学习（PbRL）可以使机器人在不需要手工制作的奖励函数的情况下，根据个体的偏好学习执行任务。然而，现有的方法要么假设可以访问高保真度的模拟器或解析模型，要么采用无模型的方法，需要大量、可能不安全的在线环境交互。在本文中，我们研究了在执行PbRL时使用学习到的动力学模型的好处和挑战。特别地，我们提供了以下证据，表明在执行PbRL时，学习到的动力学模型提供了以下好处：（1）相对于基于模型的无模型PbRL，偏好引导和策略优化需要更少的环境交互，（2）多样化的偏好查询可以安全高效地作为标准的基于模型的RL的副产品合成，（3）基于次优示范的奖励预训练可以在没有任何环境交互的情况下进行。我们的论文提供实证证据

    Preference-based reinforcement learning (PbRL) can enable robots to learn to perform tasks based on an individual's preferences without requiring a hand-crafted reward function. However, existing approaches either assume access to a high-fidelity simulator or analytic model or take a model-free approach that requires extensive, possibly unsafe online environment interactions. In this paper, we study the benefits and challenges of using a learned dynamics model when performing PbRL. In particular, we provide evidence that a learned dynamics model offers the following benefits when performing PbRL: (1) preference elicitation and policy optimization require significantly fewer environment interactions than model-free PbRL, (2) diverse preference queries can be synthesized safely and efficiently as a byproduct of standard model-based RL, and (3) reward pre-training based on suboptimal demonstrations can be performed without any environmental interaction. Our paper provides empirical eviden
    
[^349]: Flow: 通过动态路由实现每个实例的个性化联合学习

    Flow: Per-Instance Personalized Federated Learning Through Dynamic Routing

    [https://arxiv.org/abs/2211.15281](https://arxiv.org/abs/2211.15281)

    Flow是一种细粒度无状态个性化联合学习方法，通过学习动态路由机制创建个性化模型，并引入了每个实例的路由，以提高每个客户端的准确性。

    

    个性化联合学习旨在根据每个客户端对协作训练的全局模型进行修改。目前的个性化联合学习方法在粗粒度上进行，即客户端的所有输入实例使用相同的个性化模型。这忽视了某些实例由于更好的泛化能力而更准确地处理全局模型的事实。为了解决这个挑战，本文提出了一种细粒度无状态个性化联合学习方法Flow。Flow通过学习一种路由机制，创建动态的个性化模型，确定输入实例是更喜欢本地参数还是全局对应参数。因此，Flow除了利用每个客户端的个性化之外，还引入了每个实例的路由，以提高每个客户端的准确性。此外，Flow是无状态的，使得客户端在联合学习轮之间不必保留个性化状态。这使得Flow适用于大规模联合学习环境，并且对新加入的客户端友好。

    Personalization in Federated Learning (FL) aims to modify a collaboratively trained global model according to each client. Current approaches to personalization in FL are at a coarse granularity, i.e. all the input instances of a client use the same personalized model. This ignores the fact that some instances are more accurately handled by the global model due to better generalizability. To address this challenge, this work proposes Flow, a fine-grained stateless personalized FL approach. Flow creates dynamic personalized models by learning a routing mechanism that determines whether an input instance prefers the local parameters or its global counterpart. Thus, Flow introduces per-instance routing in addition to leveraging per-client personalization to improve accuracies at each client. Further, Flow is stateless which makes it unnecessary for a client to retain its personalized state across FL rounds. This makes Flow practical for large-scale FL settings and friendly to newly joined
    
[^350]: 动量外移梯度何时能达到最佳？基于多项式的分析

    When is Momentum Extragradient Optimal? A Polynomial-Based Analysis

    [https://arxiv.org/abs/2211.04659](https://arxiv.org/abs/2211.04659)

    本论文通过多项式分析，对动量外移梯度方法在不同情景下的加速收敛进行研究，包括特征值存在于实轴、位于实轴上的共轭复数或仅存在共轭复数的情况。同时，我们还得出了实现最快收敛的超参数。

    

    外移梯度方法由于其在可微分博弈中的稳健收敛性而受到青睐。与单目标优化不同，博弈动力学涉及到复杂的相互作用，这种相互作用通过博弈向量场的雅可比矩阵的特征值散布在复平面上。这种复杂性会导致简单的梯度方法发散，即使对于双线性博弈也是如此，而外移梯度方法却能实现收敛。在最近证明的基础上，即动量外移梯度方法在双线性博弈中实现加速收敛\citep{azizian2020accelerating}，我们使用基于多项式的分析来确定该方法出现进一步加速收敛的三种不同情景。这些情景包括特征值存在于（正）实轴上、位于实轴上的共轭复数以及仅存在共轭复数的情况。此外，我们还推导出每个情景的超参数，以实现最快的收敛。

    The extragradient method has gained popularity due to its robust convergence properties for differentiable games. Unlike single-objective optimization, game dynamics involve complex interactions reflected by the eigenvalues of the game vector field's Jacobian scattered across the complex plane. This complexity can cause the simple gradient method to diverge, even for bilinear games, while the extragradient method achieves convergence. Building on the recently proven accelerated convergence of the momentum extragradient method for bilinear games \citep{azizian2020accelerating}, we use a polynomial-based analysis to identify three distinct scenarios where this method exhibits further accelerated convergence. These scenarios encompass situations where the eigenvalues reside on the (positive) real line, lie on the real line alongside complex conjugates, or exist solely as complex conjugates. Furthermore, we derive the hyperparameters for each scenario that achieve the fastest convergence r
    
[^351]: 深度学习的动态潜变量分离

    Dynamic Latent Separation for Deep Learning

    [https://arxiv.org/abs/2210.03728](https://arxiv.org/abs/2210.03728)

    本研究提出了动态潜变量分离的方法，可以在复杂数据中学习表达性强的潜变量，提升输出的多样性。该方法受原子物理学启发，通过学习每个数据样本的结构来解释各个子组件的重要性。实验证明该方法在不同分类和生成问题中提升了模型的性能。

    

    机器学习中的一个核心问题是以灵活和可解释的方式学习用于复杂数据模型预测的表达性潜变量，这些数据包含多个子组件。我们开发了一种方法，改进了表达性，提供了部分解释，并且不限于特定的应用。关键思想是在潜空间中动态地分离数据样本，从而增强输出的多样性。我们的动态潜变量分离方法受到原子物理学的启发，依赖于每个数据样本共同学习的结构，这也揭示出了每个子组件在区分数据样本中的重要性。这种方法，原子建模，不需要对潜空间进行监督，并且允许我们学习额外的部分可解释表示，除了模型的原始目标。实验证明，该算法还提高了各种分类和生成问题中小到大规模模型的性能。

    A core problem in machine learning is to learn expressive latent variables for model prediction on complex data that involves multiple sub-components in a flexible and interpretable fashion. Here, we develop an approach that improves expressiveness, provides partial interpretation, and is not restricted to specific applications. The key idea is to dynamically distance data samples in the latent space and thus enhance the output diversity. Our dynamic latent separation method, inspired by atomic physics, relies on the jointly learned structures of each data sample, which also reveal the importance of each sub-component for distinguishing data samples. This approach, atom modeling, requires no supervision of the latent space and allows us to learn extra partially interpretable representations besides the original goal of a model. We empirically demonstrate that the algorithm also enhances the performance of small to larger-scale models in various classification and generation problems.
    
[^352]: GBSVM: 用粗粒-球作为输入的支持向量机

    GBSVM: Granular-ball Support Vector Machine

    [https://arxiv.org/abs/2210.03120](https://arxiv.org/abs/2210.03120)

    GBSVM是一种使用粗粒-球作为输入的支持向量机，修复了现有模型的错误并推导出了对偶模型，提出了粒子群优化算法和顺序最小优化算法来解决问题，具有良好的稳健性和效率。

    

    GBSVM（Granular-ball Support Vector Machine）是通过使用粗粒-球的粒度作为输入来构建分类器的重要尝试，而不是单个数据点。这是第一个输入不包含点的分类器。然而，现有模型存在一些错误，并且其对偶模型尚未被推导出。因此，当前算法无法实现或应用。为了解决这些问题，本文修复了现有GBSVM原始模型的错误，并推导出了其对偶模型。此外，设计了粒子群优化算法来解决对偶模型问题。还设计了顺序最小优化算法来解决对偶模型。该解决方案比基于粒子群优化的版本更快、更稳定。在UCI基准数据集上的实验结果表明，GBSVM具有良好的稳健性和效率。所有代码已在开源库http://上发布。

    GBSVM (Granular-ball Support Vector Machine) is a significant attempt to construct a classifier using the coarse-to-fine granularity of a granular-ball as input, rather than a single data point. It is the first classifier whose input contains no points. However, the existing model has some errors, and its dual model has not been derived. As a result, the current algorithm cannot be implemented or applied. To address these problems, this paper has fixed the errors of the original model of the existing GBSVM, and derived its dual model. Furthermore, a particle swarm optimization algorithm is designed to solve the dual model. The sequential minimal optimization algorithm is also carefully designed to solve the dual model. The solution is faster and more stable than the particle swarm optimization based version. The experimental results on the UCI benchmark datasets demonstrate that GBSVM has good robustness and efficiency. All codes have been released in the open source library at http://
    
[^353]: NeuralVDB: 使用分层神经网络的高分辨率稀疏体积表示方法

    NeuralVDB: High-resolution Sparse Volume Representation using Hierarchical Neural Networks

    [https://arxiv.org/abs/2208.04448](https://arxiv.org/abs/2208.04448)

    NeuralVDB使用分层神经网络改进了VDB的存储效率，将内存占用减少数个数量级同时保持了灵活性，并实现了高压缩比和空间适应性。

    

    我们引入了NeuralVDB，通过利用机器学习的最新进展，改进了现有的用于高效存储稀疏体积数据的行业标准VDB[Museth 2013]。我们的新颖混合数据结构可以将VDB体积的内存占用减少数个数量级，同时保持其灵活性，并只产生小的（用户可控制的）压缩误差。具体而言，NeuralVDB使用多个分层神经网络替换了浅而宽的VDB树结构的较低节点，通过神经分类器和回归器分别编码拓扑和数值信息。此方法已被证明在保持高级VDB数据结构所提供的空间适应性的同时最大化了压缩比。对于稀疏有符号距离场和密度体积，我们观察到从已经压缩的VDB输入得到的压缩比在10倍到100倍以上，并且几乎没有可见的伪影。

    We introduce NeuralVDB, which improves on an existing industry standard for efficient storage of sparse volumetric data, denoted VDB [Museth 2013], by leveraging recent advancements in machine learning. Our novel hybrid data structure can reduce the memory footprints of VDB volumes by orders of magnitude, while maintaining its flexibility and only incurring small (user-controlled) compression errors. Specifically, NeuralVDB replaces the lower nodes of a shallow and wide VDB tree structure with multiple hierarchical neural networks that separately encode topology and value information by means of neural classifiers and regressors respectively. This approach is proven to maximize the compression ratio while maintaining the spatial adaptivity offered by the higher-level VDB data structure. For sparse signed distance fields and density volumes, we have observed compression ratios on the order of 10x to more than 100x from already compressed VDB inputs, with little to no visual artifacts. F
    
[^354]: 在线加权二部图匹配的次线性时间算法

    Sublinear Time Algorithm for Online Weighted Bipartite Matching

    [https://arxiv.org/abs/2208.03367](https://arxiv.org/abs/2208.03367)

    这个论文提出了一个在线加权二部图匹配的次线性时间算法，通过引入随机化数据结构，实现了对权重的近似计算，以解决在实际推荐系统或搜索引擎中计算大规模权重的问题。

    

    在线二部图匹配是在线算法中的一个基本问题。目标是将两个顶点集合进行匹配，使得边权重的总和最大化，其中对于其中一个顶点集合，每个顶点及其对应的边权重按顺序出现在一个序列中。目前，在实际的推荐系统或搜索引擎中，权重是由用户的深度表示和物品的深度表示之间的内积决定的。标准的在线匹配需要付出$nd$的时间来线性扫描所有的$n$个物品，计算权重（假设每个表示向量长度为$d$），然后根据权重决定匹配。然而，在现实中，$n$可能非常大，比如在线电子商务平台。因此，改进计算权重的时间是一个具有实际意义的问题。在这项工作中，我们提供了计算近似权重的理论基础。我们展示了，通过我们提出的随机化数据结构，权重的计算时间可以达到次线性。

    Online bipartite matching is a fundamental problem in online algorithms. The goal is to match two sets of vertices to maximize the sum of the edge weights, where for one set of vertices, each vertex and its corresponding edge weights appear in a sequence. Currently, in the practical recommendation system or search engine, the weights are decided by the inner product between the deep representation of a user and the deep representation of an item. The standard online matching needs to pay $nd$ time to linear scan all the $n$ items, computing weight (assuming each representation vector has length $d$), and then deciding the matching based on the weights. However, in reality, the $n$ could be very large, e.g. in online e-commerce platforms. Thus, improving the time of computing weights is a problem of practical significance. In this work, we provide the theoretical foundation for computing the weights approximately. We show that, with our proposed randomized data structures, the weights c
    
[^355]: 差分隐私图学习的敏感性有界个性化PageRank算法

    Differentially Private Graph Learning via Sensitivity-Bounded Personalized PageRank

    [https://arxiv.org/abs/2207.06944](https://arxiv.org/abs/2207.06944)

    本论文提出了一种敏感性有界的个性化PageRank算法，能够保护用户隐私。该算法在保持准确性的同时，实现了差分隐私图学习的几种工具。

    

    个性化PageRank(PPR)是一种基本工具，用于无监督学习图表示，如节点排序、标注和图嵌入。然而，随着数据隐私成为最近最重要的关注点之一，现有的PPR算法并未设计用于保护用户隐私。PPR对输入图的边非常敏感：仅差一个边的差异可能会导致PPR向量发生巨大改变，从而可能泄漏用户私密数据。在这篇论文中，我们提出了一种算法，该算法输出近似PPR，并对输入边具有可证明的敏感性边界。此外，我们证明了当输入图具有大度数时，我们的算法达到与非私密算法相似的准确性。我们敏感性有界PPR直接意味着图学习的几种私密算法，如差分隐私(DP)PPR排序、DP节点分类和DP节点嵌入。为了补充我们的理论分析，我们还通过实验证明了算法的实际性能。

    Personalized PageRank (PPR) is a fundamental tool in unsupervised learning of graph representations such as node ranking, labeling, and graph embedding. However, while data privacy is one of the most important recent concerns, existing PPR algorithms are not designed to protect user privacy. PPR is highly sensitive to the input graph edges: the difference of only one edge may cause a big change in the PPR vector, potentially leaking private user data.   In this work, we propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. In addition, we prove that our algorithm achieves similar accuracy to non-private algorithms when the input graph has large degrees. Our sensitivity-bounded PPR directly implies private algorithms for several tools of graph learning, such as, differentially private (DP) PPR ranking, DP node classification, and DP node embedding. To complement our theoretical analysis, we also empirically verify the practical perfor
    
[^356]: 避免严格鞍点的惯性牛顿算法研究

    Inertial Newton Algorithms Avoiding Strict Saddle Points

    [https://arxiv.org/abs/2111.04596](https://arxiv.org/abs/2111.04596)

    混合使用牛顿法和惯性梯度下降的二阶算法几乎总是可以避免严格鞍点，并通过数值实例支持了理论结果。

    

    我们研究了在非凸地形中混合使用牛顿法和惯性梯度下降的二阶算法的渐近行为。我们表明，尽管这些方法具有牛顿特性，它们几乎总是可以避免严格鞍点。我们还证明了这些方法的超参数在临界点附近的定性行为中起到的作用。理论结果得到了数值实例的支持。

    We study the asymptotic behavior of second-order algorithms mixing Newton's method and inertial gradient descent in non-convex landscapes. We show that, despite the Newtonian behavior of these methods, they almost always escape strict saddle points. We also evidence the role played by the hyper-parameters of these methods in their qualitative behavior near critical points. The theoretical results are supported by numerical illustrations.
    
[^357]: 不平衡患者亚群的电解质补充的组合Q学习

    Compositional Q-learning for electrolyte repletion with imbalanced patient sub-populations

    [https://arxiv.org/abs/2110.02879](https://arxiv.org/abs/2110.02879)

    组合Q学习方法用于解决医疗环境中存在异质性治疗反应的问题，通过使用复合任务结构和分离的模块化Q值函数，能够更有效地进行决策。

    

    强化学习（RL）是解决顺序决策任务的有效框架。然而，在医疗环境中应用RL方法具有挑战性，部分原因是由于患者的治疗反应的异质性。一些患者可以使用标准方案进行治疗，而其他患者，如慢性疾病患者，需要个性化的治疗计划。传统的RL方法通常无法考虑到这种异质性，因为它们假设所有患者对治疗的反应是相同的（即，转移动力学是共享的）。我们引入了组合Fitted $Q$-迭代（CFQI），它使用复合任务结构来表示医疗环境中的异质性治疗反应。复合任务由相同任务的几个变体组成，每个变体的难度逐渐增加；解决较简单的变体可以实现更高效地解决更难的变体。CFQI使用一个复合$Q$值函数，其中为每个任务模块单独表示。

    Reinforcement learning (RL) is an effective framework for solving sequential decision-making tasks. However, applying RL methods in medical care settings is challenging in part due to heterogeneity in treatment response among patients. Some patients can be treated with standard protocols whereas others, such as those with chronic diseases, need personalized treatment planning. Traditional RL methods often fail to account for this heterogeneity, because they assume that all patients respond to the treatment in the same way (i.e., transition dynamics are shared). We introduce Compositional Fitted $Q$-iteration (CFQI), which uses a compositional task structure to represent heterogeneous treatment responses in medical care settings. A compositional task consists of several variations of the same task, each progressing in difficulty; solving simpler variants of the task can enable efficient solving of harder variants. CFQI uses a compositional $Q$-value function with separate modules for ea
    
[^358]: 变量选择的计算高效高维贝叶斯优化方法

    Computationally Efficient High-Dimensional Bayesian Optimization via Variable Selection

    [https://arxiv.org/abs/2109.09264](https://arxiv.org/abs/2109.09264)

    本论文提出了一种变量选择的计算高效高维贝叶斯优化方法，能够自动学习子空间来优化高维域函数，同时减少了传统方法中的耗时问题，并在实验证明了方法的有效性。

    

    贝叶斯优化（BO）是一种用于全局优化黑盒函数的方法。虽然BO已成功应用于许多场景，但是开发能够适用于高维域函数的有效BO算法仍然是一个挑战。通过普通的BO优化此类函数非常耗时。基于将高维空间嵌入到低维空间的思想的高维BO的替代策略对嵌入维度的选择非常敏感，需要预先指定。我们开发了一种新的计算高效的高维BO方法，利用了变量选择。我们的方法能够自动学习轴对齐的子空间，即包含选定变量的空间，而无需任何预先指定的超参数。我们从理论上分析了算法的计算复杂性并得出了遗憾界限。我们在几个合成和真实数据上实验证明了我们方法的有效性。

    Bayesian Optimization (BO) is a method for globally optimizing black-box functions. While BO has been successfully applied to many scenarios, developing effective BO algorithms that scale to functions with high-dimensional domains is still a challenge. Optimizing such functions by vanilla BO is extremely time-consuming. Alternative strategies for high-dimensional BO that are based on the idea of embedding the high-dimensional space to the one with low dimension are sensitive to the choice of the embedding dimension, which needs to be pre-specified. We develop a new computationally efficient high-dimensional BO method that exploits variable selection. Our method is able to automatically learn axis-aligned sub-spaces, i.e. spaces containing selected variables, without the demand of any pre-specified hyperparameters. We theoretically analyze the computational complexity of our algorithm and derive the regret bound. We empirically show the efficacy of our method on several synthetic and re
    
[^359]: 机器协作

    Machine Collaboration

    [https://arxiv.org/abs/2105.02569](https://arxiv.org/abs/2105.02569)

    本文提出了一种新的监督学习集成框架——机器协作（MaC），通过循环和交互的学习方式，使基础机器能够循环传递信息并相应地更新结构和参数。实验证明，MaC在大多数情况下表现优于其他先进方法。

    

    我们提出了一种新的监督学习集成框架，称为机器协作（MaC），利用一组基础机器进行预测任务。与并行且独立的bagging/stacking框架和顺序且自上而下的boosting框架不同，MaC是一种循环和交互学习框架。循环和交互特性帮助基础机器循环传递信息并相应地更新其结构和参数。对于从MaC得出的估计器的风险界的理论结果表明，循环和交互特性可以帮助MaC通过简洁的集成减少风险。我们在模拟数据和119个基准真实数据集上进行了大量实验证明，在大多数情况下，MaC的性能显著优于包括分类回归树、神经网络、堆叠和提升在内的其他几种最先进的方法。

    We propose a new ensemble framework for supervised learning, called machine collaboration (MaC), using a collection of base machines for prediction tasks. Unlike bagging/stacking (a parallel & independent framework) and boosting (a sequential & top-down framework), MaC is a type of circular & interactive learning framework. The circular & interactive feature helps the base machines to transfer information circularly and update their structures and parameters accordingly. The theoretical result on the risk bound of the estimator from MaC reveals that the circular & interactive feature can help MaC reduce risk via a parsimonious ensemble. We conduct extensive experiments on MaC using both simulated data and 119 benchmark real datasets. The results demonstrate that in most cases, MaC performs significantly better than several other state-of-the-art methods, including classification and regression trees, neural networks, stacking, and boosting.
    
[^360]: 稀疏NMF与典型正则化：计算和鲁棒性性质

    Sparse NMF with Archetypal Regularization: Computational and Robustness Properties

    [https://arxiv.org/abs/2104.03527](https://arxiv.org/abs/2104.03527)

    本文研究了使用典型正则化的稀疏非负矩阵分解问题，提出了强鲁棒性和弱鲁棒性的概念，并给出了理论保证和数值实验来加强这些概念的洞察力。

    

    我们考虑使用典型正则化的稀疏非负矩阵分解（NMF）问题。目标是将一组数据点表示为少数非负稀疏因子的非负线性组合，这些因子具有吸引人的几何特性，来自于使用典型正则化。我们将在Javadi和Montanari（2019）中研究的鲁棒性概念（无稀疏性）推广为（a）强鲁棒性，即每个估计的典型都接近真实的典型，以及（b）弱鲁棒性，即至少存在一个恢复的典型接近真实的典型。我们的理论结果对于基础数据的假设较为简化，并适用于基于不需要稀疏性的典型的情况。我们提出了新的算法来优化我们的问题。

    We consider the problem of sparse nonnegative matrix factorization (NMF) using archetypal regularization. The goal is to represent a collection of data points as nonnegative linear combinations of a few nonnegative sparse factors with appealing geometric properties, arising from the use of archetypal regularization. We generalize the notion of robustness studied in Javadi and Montanari (2019) (without sparsity) to the notions of (a) strong robustness that implies each estimated archetype is close to the underlying archetypes and (b) weak robustness that implies there exists at least one recovered archetype that is close to the underlying archetypes. Our theoretical results on robustness guarantees hold under minimal assumptions on the underlying data, and applies to settings where the underlying archetypes need not be sparse. We present theoretical results and illustrative examples to strengthen the insights underlying the notions of robustness. We propose new algorithms for our optimi
    
[^361]: 深度学习的创造力：概念化与评估

    Creativity of Deep Learning: Conceptualization and Assessment

    [https://arxiv.org/abs/2012.02282](https://arxiv.org/abs/2012.02282)

    这篇论文探讨了深度学习在创意领域的应用，并使用计算创造力的视角对其进行了概念化和评估。研究发现，尽管深度学习可以产生高质量的结果，但其创新性受到限制，同时也存在内部问题表达无法更改以及在不同领域之间建立联系的缺乏能力的问题。

    

    虽然深度学习（DL）在自动化简单任务方面的潜力已经得到了广泛探索，但最近的研究开始探索使用深度学习进行创造性设计，无论是完整的产品创作还是支持人类在创作过程中的角色。本文利用计算创造力的见解，概念化和评估了文献综述中确定的创造性领域中当前应用生成式深度学习的情况。我们强调了当前系统与不同模型人类创造力的相似之处以及它们的不足之处。虽然深度学习可以产生高质量的图像等高价值结果，但其新颖性通常受到多种限制，例如受到训练数据定义的概念空间的限制。当前的DL方法也不允许对内部问题表达进行更改，并且它们缺乏在高度不同的领域之间建立联系的能力，这两点都被认为是主要的推动力。

    While the potential of deep learning (DL) for automating simple tasks is already well explored, recent research has started investigating the use of deep learning for creative design, both for complete artifact creation and supporting humans in the creation process. In this paper, we use insights from computational creativity to conceptualize and assess current applications of generative deep learning in creative domains identified in a literature review. We highlight parallels between current systems and different models of human creativity as well as their shortcomings. While deep learning yields results of high value, such as high-quality images, their novelty is typically limited due to multiple reasons such as being tied to a conceptual space defined by training data. Current DL methods also do not allow for changes in the internal problem representation, and they lack the capability to identify connections across highly different domains, both of which are seen as major drivers o
    
[^362]: 通过多种方式，将混合专家与MCTS相结合以提高国际象棋中的校验

    Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess. (arXiv:2401.16852v1 [cs.LG])

    [http://arxiv.org/abs/2401.16852](http://arxiv.org/abs/2401.16852)

    通过将混合专家方法和MCTS相结合，本研究在国际象棋中显著提升了下棋水平，验证了集成方法的有效性并展示了融入专家知识和战略原则到神经网络中的潜力。

    

    本文提出了一种新的方法，将深度学习与计算机棋盘相结合，同时使用混合专家方法和蒙特卡罗树搜索方法。我们的方法采用一套专门设计的模型，每个模型都针对游戏输入数据的特定变化做出响应。这导致了一个稀疏激活模型的框架，提供了显著的计算优势。我们的框架将混合专家方法与蒙特卡罗树搜索方法结合起来，以使其与国际象棋的战略阶段相一致，从而摆脱传统的“一刀切”的模型。相反，我们利用不同的游戏阶段定义，将计算任务有效地分配给多个专家神经网络。我们的实证研究显示，在游戏实力方面有了显著改进，超过了传统的单模型框架。这证实了我们集成方法的功效，并凸显了将专家知识和战略原则纳入神经网络中的潜力。

    This paper presents a new approach that integrates deep learning with computational chess, using both the Mixture of Experts (MoE) method and Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized models, each designed to respond to specific changes in the game's input data. This results in a framework with sparsely activated models, which provides significant computational benefits. Our framework combines the MoE method with MCTS, in order to align it with the strategic phases of chess, thus departing from the conventional ``one-for-all'' model. Instead, we utilize distinct game phase definitions to effectively distribute computational tasks across multiple expert neural networks. Our empirical research shows a substantial improvement in playing strength, surpassing the traditional single-model framework. This validates the efficacy of our integrated approach and highlights the potential of incorporating expert knowledge and strategic principles into neural net
    
[^363]: 在任意线性变换下的自适应块稀疏正则化

    Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])

    [http://arxiv.org/abs/2401.15292](http://arxiv.org/abs/2401.15292)

    我们提出了一种在任意线性变换下重构具有块稀疏性的信号的方法，相比现有方法扩大了应用范围，并通过数值实验证明了其有效性。

    

    我们提出了一种用于在未知块结构下的任意线性变换下的块稀疏信号重构方法。该方法是现有方法LOP-$\ell_2$/$\ell_1$的推广，可以在非可逆变换下重构具有块稀疏性的信号，而LOP-$\ell_2$/$\ell_1$不能。我们的工作扩大了块稀疏正则化的范围，使其能够在各种信号处理领域中应用更加灵活和强大。我们推导了一个迭代算法来求解该方法，并给出了其收敛到最优解的条件。数值实验验证了该方法的有效性。

    We propose a convex signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can reconstruct signals with block sparsity under non-invertible transforms, unlike LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.
    
[^364]: 跨空间自适应滤波器：集成图拓扑和节点属性以减轻过度平滑问题

    Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem. (arXiv:2401.14876v1 [cs.LG])

    [http://arxiv.org/abs/2401.14876](http://arxiv.org/abs/2401.14876)

    本论文提出了一种跨空间自适应滤波器（CSF），可以从图拓扑和节点属性空间中提取自适应频率信息，以减轻图卷积网络（GCN）的过度平滑问题。

    

    传统的图卷积网络（GCN）使用低通滤波器从图拓扑中提取低频信号，但当GCN深度增加时可能导致过度平滑问题。为解决这个问题，已经提出了各种方法通过引入从图拓扑中提取的额外滤波器（如高通滤波器）来创建自适应滤波器。然而，这些方法严重依赖拓扑信息，并忽视了节点属性空间，这严重牺牲了深层GCN的表达能力，特别是在处理非同配图时。本文提出了一种跨空间自适应滤波器，称为CSF，能够从拓扑和属性空间中提取自适应频率信息。具体而言，我们首先推导出了一个定制的基于属性的高通滤波器，可以从理论上解释为半监督核岭回归的最小化器。然后，我们将基于拓扑的低通滤波器视为Mercer's核函数。

    The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the 
    
[^365]: 通过可微分图神经网络模拟器进行颗粒流的反演分析

    Inverse analysis of granular flows using differentiable graph neural network simulator. (arXiv:2401.13695v1 [physics.geo-ph])

    [http://arxiv.org/abs/2401.13695](http://arxiv.org/abs/2401.13695)

    通过使用可微分图神经网络模拟器进行反演分析，解决了传统模拟器计算开销大、不可微等问题，提高了计算效率和准确性。

    

    颗粒流中的反演问题，如山体滑坡和碎屑流，涉及基于目标波动剖面估计材料参数或边界条件。传统的高保真度模拟器对这些反演问题是计算密集型的，限制了可能的模拟次数。此外，它们的不可微性使得梯度优化方法无法应用，而这些方法在高维问题中以其效率而闻名。虽然基于机器学习的代理模型提供了计算效率和可微性，但由于其依赖于低维输入-输出映射，无法捕捉到颗粒流的完整物理过程，因此往往难以推广到训练数据之外。我们提出了一种新颖的可微分图神经网络模拟器(GNS)，通过将图神经网络的反向模式自动微分与基于梯度的优化相结合，用于解决反演问题。GNS学习了颗粒流的动力学特性。

    Inverse problems in granular flows, such as landslides and debris flows, involve estimating material parameters or boundary conditions based on target runout profile. Traditional high-fidelity simulators for these inverse problems are computationally demanding, restricting the number of simulations possible. Additionally, their non-differentiable nature makes gradient-based optimization methods, known for their efficiency in high-dimensional problems, inapplicable. While machine learning-based surrogate models offer computational efficiency and differentiability, they often struggle to generalize beyond their training data due to their reliance on low-dimensional input-output mappings that fail to capture the complete physics of granular flows. We propose a novel differentiable graph neural network simulator (GNS) by combining reverse mode automatic differentiation of graph neural networks with gradient-based optimization for solving inverse problems. GNS learns the dynamics of granula
    
[^366]: TurboSVM-FL: 通过SVM聚合为懒惰客户端增强联邦学习

    TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients. (arXiv:2401.12012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.12012](http://arxiv.org/abs/2401.12012)

    TurboSVM-FL是一种新颖的联邦聚合策略，通过SVM聚合为懒惰客户端增强联邦学习。这种策略在不增加客户端计算负担的情况下解决了联邦学习中的收敛速度慢的问题。

    

    联邦学习是一种分布式协作机器学习范例，在近年来获得了强烈的推动力。在联邦学习中，中央服务器定期通过客户端协调模型，并聚合由客户端在本地训练的模型，而无需访问本地数据。尽管具有潜力，但联邦学习的实施仍然面临一些挑战，主要是由于数据异质性导致的收敛速度慢。收敛速度慢在跨设备联邦学习场景中尤为问题，其中客户端可能受到计算能力和存储空间的严重限制，因此对客户端产生额外计算或内存负担的方法，如辅助目标项和更大的训练迭代次数，可能不实际。在本文中，我们提出了一种新颖的联邦聚合策略TurboSVM-FL，它不会给客户端增加额外的计算负担

    Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and c
    
[^367]: 通过随机森林机器学习非侵入性诊断急性间室综合征

    Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning. (arXiv:2401.10386v1 [cs.LG])

    [http://arxiv.org/abs/2401.10386](http://arxiv.org/abs/2401.10386)

    本研究提出了一种基于随机森林机器学习的非侵入性诊断急性间室综合征的方法。使用压力传感电阻器检测肌肉间室压力，并通过蓝牙传输结果到Web应用程序。该诊断方法在准确率、灵敏度和F1得分等关键性能指标方面表现出色。

    

    急性间室综合征（ACS）是一种骨科急症，由肌肉间室内的压力升高引起，导致永久组织损伤并最终致死。ACS的诊断主要依赖患者报告的症状，这种方法在临床上不可靠，通常需要通过侵入性肌肉间室压力测量进行补充。本研究提出了一种连续、客观、非侵入性的ACS诊断方法。该设备通过一个使用贴在皮肤上的压力传感电阻器（FSR）的随机森林机器学习模型检测ACS。最终诊断结果通过蓝牙以实时方式传输到Web应用程序。为了验证诊断结果，创建了一组包含FSR测量和相应的模拟肌肉间室压力的数据集。该诊断方法的准确率达到97%，与侵入性的金标准持平。该设备在关键性能指标包括准确度、灵敏度和F1得分方面表现优异。

    Acute compartment syndrome (ACS) is an orthopedic emergency, caused by elevated pressure within a muscle compartment, that leads to permanent tissue damage and eventually death. Diagnosis of ACS relies heavily on patient-reported symptoms, a method that is clinically unreliable and often supplemented with invasive intracompartmental pressure measurements. This study proposes a continuous, objective, noninvasive diagnostic for ACS. The device detects ACS through a random forest machine learning model that uses pressure readings from force-sensitive resistors (FSRs) placed on the skin. The final diagnosis is exported real-time to a web application via Bluetooth. To validate the diagnostic, a data set containing FSR measurements and the corresponding simulated intracompartmental pressure was created. The diagnostic achieved an accuracy, on par to the invasive gold standard, of 97%. The device excelled in key performance metrics including precision, sensitivity, and F1 score. Manufactured 
    
[^368]: 为渐进式训练语言模型准备课程的方法

    Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])

    [http://arxiv.org/abs/2401.09192](http://arxiv.org/abs/2401.09192)

    提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。

    

    Transformers在人工智能领域的迅速发展带来了资源消耗和温室气体排放的增加，这是由于模型规模的增长。先前的研究表明使用预训练的小模型可以提高训练效率，但这种方法对于新的模型结构可能不适用。另一方面，从头开始训练可能很慢，并且渐进堆叠层往往无法实现显著的加速。为了解决这些挑战，我们提出了一种名为Apollo的新方法，它通过在低层训练期间学习高层功能来准备膨胀操作的课程。我们的方法涉及低值优先采样(LVPS)来训练不同深度，并引入权重共享以促进高效扩展。我们还介绍了一种插值方法来实现稳定的模型深度扩展。实验证明，Apollo实现了最先进的加速比率，甚至……

    The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
    
[^369]: 通过迭代组合问题来增强数学问题求解

    Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])

    [http://arxiv.org/abs/2401.09003](http://arxiv.org/abs/2401.09003)

    本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。

    

    尽管在改善大型语言模型(LLMs)的数学推理能力方面取得了一定进展，但在不使用外部工具的情况下解决竞赛级数学问题仍然对开源LLMs具有挑战性。在这项工作中，我们介绍了MMIQC数据集，这是一个混合处理的网络数据和合成问题-响应对的混合数据集，以提供基础模型更好的数学推理能力。通过在MMIQC上对Mistral-7B(arXiv:2310.06825)进行微调获得的模型Mistral-7B-MMIQC，在MATH(arXiv:2103.03874)上达到了36.0%的准确率，比之前(model size $\sim$7B)的最佳结果高出5.8%。我们的实验还表明，改进的一个重要部分归功于我们的新颖增强方法IQC(迭代组合问题)，其中我们迭代地要求LLM从给定的种子问题中组合新问题，并从另一个LLM中进行拒绝抽样。MMIQC现已在https://huggingface.co/datasets/Vivacem/MMIQC上发布。

    Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
    
[^370]: 超越提取：为语言模型提供上下文化的表格数据以实现高效摘要

    Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])

    [http://arxiv.org/abs/2401.02333](http://arxiv.org/abs/2401.02333)

    本研究提出了一种创新的方法，通过上下文化表格数据来提高 RAG 系统中处理复杂表格查询的准确性，提高了摘要的效率。

    

    传统的检索增强生成 (RAG) 架构在从各种文件中检索信息方面已被证明是有效的。然而，在处理包含复杂表格结构的 PDF 文档中的复杂表格查询时会遇到挑战。本研究引入了一种创新的方法来提高 RAG 系统中复杂表格查询的准确性。我们的方法涉及将 PDF 存储在检索数据库中，并单独提取表格内容。提取的表格经过上下文丰富的处理，将标题与相应的值连接起来。为了确保对丰富数据的全面理解，我们使用经过微调的 Llama-2-chat 语言模型在 RAG 架构中进行摘要。此外，我们通过一次性提示使用 ChatGPT 3.5 API 增强表格数据的上下文含义。然后，将这些丰富的数据与其他 PDF 文件一起输入检索数据库。

    The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
    
[^371]: Kernel-U-Net: 多元时间序列预测的层次和对称框架

    Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate Time Series Forecasting. (arXiv:2401.01479v1 [cs.LG])

    [http://arxiv.org/abs/2401.01479](http://arxiv.org/abs/2401.01479)

    Kernel-U-Net是一种层次和对称框架，用于多元时间序列预测。与现有模型相比，它具有较少的参数数量、灵活性和计算效率。

    

    时间序列预测任务是基于历史信息预测未来趋势。最近基于U-Net的方法在预测真实数据集方面表现出优越性能。然而，这些模型的性能比基于补丁模型或线性模型的模型低。在这项工作中，我们提出了一种对称和层次化的框架，Kernel-U-Net，它在网络的每一层将输入序列切割成片段，然后使用卷积核进行计算。此外，它扩展了经典U-Net中的卷积核的概念，可以接受符合相同设计模式的自定义卷积核。与现有的线性或基于transformer的解决方案相比，我们的模型具有三个优势：1）参数数量较少：参数大小为$O(log(L)^2)$，其中$L$为回溯窗口大小；2）灵活性：其卷积核可以定制和适应数据集；3）计算效率：如果使用此模型，transformer模块的计算复杂度减小为$O(log(L)^2)$。

    Time series forecasting task predicts future trends based on historical information. Recent U-Net-based methods have demonstrated superior performance in predicting real-world datasets. However, the performance of these models is lower than patch-based models or linear models. In this work, we propose a symmetric and hierarchical framework, Kernel-U-Net, which cuts the input sequence into slices at each layer of the network and then computes them using kernels. Furthermore, it generalizes the concept of convolutional kernels in classic U-Net to accept custom kernels that follow the same design pattern. Compared to the existing linear or transformer-based solution, our model contains 3 advantages: 1) A small number of parameters: the parameters size is $O(log(L)^2)$ where $L$ is the look-back window size, 2) Flexibility: its kernels can be customized and fitted to the datasets, 3) Computation efficiency: the computation complexity of transformer modules is reduced to $O(log(L)^2)$ if th
    
[^372]: 可扩展的子二次时间网络重建

    Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])

    [http://arxiv.org/abs/2401.01404](http://arxiv.org/abs/2401.01404)

    这篇论文提出了一个可扩展的网络重建算法，能够在次二次时间内实现结果，通过随机的二阶邻居搜索产生最佳的边候选。

    

    网络重建是指在只有关于条件偶联的观测数据，例如时间序列或图模型的独立样本的情况下，确定N个节点之间未观测到的成对耦合。针对这个问题提出的算法的可扩展性的主要障碍是似乎无法避免的二次复杂度O(N^2)，即要考虑每种可能的成对耦合至少一次，尽管大多数感兴趣的网络都是稀疏的，非零耦合的数量只有O(N)。在这里，我们提出了一个适用于广泛重建问题的通用算法，其在子二次时间内实现结果，其数据相关复杂度宽松上界为O(N^(3/2)logN)，但具有更典型的对数线性复杂度O(Nlog^2 N)。我们的算法依赖于一个随机的二阶邻居搜索，产生了最佳的边候选。

    Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
    
[^373]: 强化学习中的消除学习

    Reinforcement Unlearning. (arXiv:2312.15910v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.15910](http://arxiv.org/abs/2312.15910)

    强化学习中的消除学习是一种新兴的研究领域，旨在解决环境所有者有权撤销智能体训练数据的隐私问题。该领域面临三个主要挑战。

    

    机器消除学习指的是根据数据所有者的请求，降低特定训练数据对机器学习模型的影响的过程。然而，在消除学习的研究中，一个重要的领域往往被忽视，那就是强化学习。强化学习旨在训练一个智能体在环境中做出最优决策以最大化累积奖励。在训练过程中，智能体往往会记忆环境的特征，这引发了一个重大的隐私问题。根据数据保护法规，环境的所有者有权撤销智能体的训练数据的访问权限，因此需要开展一个新颖且紧迫的研究领域，即“强化消除学习”。强化消除学习侧重于撤销整个环境而不是单个数据样本。这一独特特征带来了三个不同的挑战：1）如何提出消除学习方案

    Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes f
    
[^374]: NPHardEval: 通过复杂性类别对大型语言模型的推理能力进行动态基准评估

    NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.14890](http://arxiv.org/abs/2312.14890)

    NPHardEval是一个新的基准，旨在评估大型语言模型在900个算法问题上的推理能力，扩展到NP-Hard复杂性类别。

    

    复杂推理能力是当前大型语言模型的最重要特征之一，它也被用于在复杂决策任务中起到了重要作用。因此，研究大型语言模型的推理能力至关重要：已经建立了许多基准来评估大型语言模型的推理能力。然而，目前的基准在提供大型语言模型推理能力的全面评估方面还不够，同时也容易出现过拟合的风险，因为这些基准是公开可访问且静态的，使得模型有可能根据特定的基准指标调整其响应，从而夸大其性能。针对这些限制，我们的研究引入了一个新的基准，名为NPHardEval。该基准旨在评估大型语言模型在广泛的900个算法问题上的推理能力，涵盖了NP-Hard复杂性类别。

    Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
    
[^375]: 多个任务预训练和图形提示的MultiGPrompt

    MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.03731](http://arxiv.org/abs/2312.03731)

    本文提出了一种名为MultiGPrompt的多任务预训练和提示框架，用于在图形表示学习中提高鲁棒性和减少标注成本。

    

    图形可以固有地对Web上相互连接的对象进行建模，从而支持一系列Web应用，比如网络分析和内容推荐。最近，图神经网络（GNNs）已经成为图表示学习的主流技术。然而，在端到端监督框架中，它们的有效性与任务特定标签的可用性密切相关。为了减少标注成本并增强在少样本设置中的鲁棒性，基于自监督任务的预训练已经成为一种有前途的方法，而提示则被提出来进一步缩小预训练任务与下游任务之间的目标差距。虽然已经对基于提示的图形学习进行了初步的探索，但它们主要利用单个预训练任务，导致从预训练数据中可能学习的通用知识的子集受限。因此，在本文中，我们提出了一种新颖的多任务预训练和提示框架MultiGPrompt，用于进一步提高对图形的表示学习。

    Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
    
[^376]: 线性对数正态注意力与无偏集中力

    Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13541](http://arxiv.org/abs/2311.13541)

    本论文研究了自注意机制，并分析了注意力矩阵的分布和集中能力。通过引入线性对数正态注意力来模拟原始自注意力的分布和集中行为，提高了Transformer模型的可扩展性。

    

    Transformer模型在各种应用中取得了显著的成果。然而，由于自注意机制的时间和内存复杂度与序列长度的二次关系，其可扩展性受到限制。当处理长文档或高分辨率图像时，这一限制构成了重大障碍。本研究通过分析注意力矩阵的分布和集中能力，对自注意机制进行了研究。此外，我们提出了衡量这些数量的工具，并引入了一种新的自注意机制，即线性对数正态注意力，旨在模拟原始自注意力的分布和集中行为。我们在常用的自然语言基准测试上的实验证明，我们提出的线性对数正态注意力优于其他线性化注意力替代方法，为增强Transformer模型的可扩展性提供了一个有前途的途径。我们的代码附在补充材料中。

    Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
    
[^377]: DeliverAI: 强化学习为基础的分布式路径共享网络用于食品配送

    DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])

    [http://arxiv.org/abs/2311.02017](http://arxiv.org/abs/2311.02017)

    DeliverAI是一个基于强化学习的分布式路径共享网络，用于优化食品配送的多目标优化问题，以减少配送成本并提高消费者满意度。

    

    在过去十年中，从生产者到消费者的物品配送经历了显著的增长，并且最近的流行病进一步推动了这一增长。亚马逊生鲜、Shopify、UberEats、InstaCart和DoorDash正在迅速发展，并共享相同的消费品或食品配送业务模式。现有的食品配送方法存在缺陷，因为每次配送都是在最短时间路径上从生产者直接到消费者进行优化。我们观察到，在当前模型下，有很大的减少配送成本的空间。我们将我们的食品配送问题建模为一个多目标优化问题，消费者满意度和配送成本都需要进行优化。受出租车行业中拼车成功的启发，我们提出了DeliverAI - 一种基于强化学习的路径共享算法。与以前的路径共享尝试不同，DeliverAI可以提供实时、时间高效的决策。

    Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-
    
[^378]: 通过倾斜指数层改善稳健性：基于通信理论的视角

    Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective. (arXiv:2311.01047v1 [cs.LG])

    [http://arxiv.org/abs/2311.01047](http://arxiv.org/abs/2311.01047)

    本论文提出了一种基于通信理论的方法，通过神经竞争来增强神经网络层输出的信噪比，从而提高深度网络的稳健性。

    

    提升深度网络稳健性的最新技术大多依赖于合适的数据增强的经验风险最小化。本文提出了一种基于通信理论的互补方法，旨在通过学习和推理中的神经竞争来增强神经网络层输出的信噪比。除了最小化标准的端到端代价外，神经元通过最大化倾斜指数（TEXP）层的目标函数来竞争以稀疏地表示层输入。TEXP学习可以被解释为在数据噪声的高斯模型下通过最大似然估计来匹配滤波器。在TEXP层中，通过使用倾斜的softmax替代批量归一化来进行推理，可以解释为计算每个神经元代表的竞争信号假设的后验概率。通过简化模型提供洞察，我们通过在标准图像上的实验表明，

    State-of-the-art techniques for enhancing robustness of deep networks mostly rely on empirical risk minimization with suitable data augmentation. In this paper, we propose a complementary approach motivated by communication theory, aimed at enhancing the signal-to-noise ratio at the output of a neural network layer via neural competition during learning and inference. In addition to minimization of a standard end-to-end cost, neurons compete to sparsely represent layer inputs by maximization of a tilted exponential (TEXP) objective function for the layer. TEXP learning can be interpreted as maximum likelihood estimation of matched filters under a Gaussian model for data noise. Inference in a TEXP layer is accomplished by replacing batch norm by a tilted softmax, which can be interpreted as computation of posterior probabilities for the competing signaling hypotheses represented by each neuron. After providing insights via simplified models, we show, by experimentation on standard image
    
[^379]: COSTAR: 使用自监督学习改进的时间反事实估计

    COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning. (arXiv:2311.00886v1 [cs.LG])

    [http://arxiv.org/abs/2311.00886](http://arxiv.org/abs/2311.00886)

    这项研究提出了一种名为COSTAR的新方法，通过整合自监督学习，改进了时间反事实结果的估计。该方法在处理时间相关混淆因素时结合了时间和特征关注以及分量对比损失，相比现有模型在估计准确性和对分布之外数据的泛化能力方面表现出更优越的性能。

    

    在许多领域，如医疗保健和电子商务，在观察到的历史数据中估计时间反事实结果对决策至关重要，特别是随机对照试验(RCTs)成本高或者不可行的情况下。对于现实世界的数据集，由于复杂的动态，长范围依赖性以及过去的治疗和协变量对未来结果的影响，建模时间相关的混淆因素是具有挑战性的。在本文中，我们引入了COunterfactual Self-supervised TrAnsformeR（COSTAR），一种整合了自监督学习来改进历史表示的新方法。所提出的框架将时间和特征关注与针对时间处理结果观察的分量对比损失相结合，与现有模型相比，在估计准确性和泛化到分布之外数据方面具有优越性能，通过对合成数据集和现实世界数据集的实证结果验证该优越性能。

    Estimation of temporal counterfactual outcomes from observed history is crucial for decision-making in many domains such as healthcare and e-commerce, particularly when randomized controlled trials (RCTs) suffer from high cost or impracticality. For real-world datasets, modeling time-dependent confounders is challenging due to complex dynamics, long-range dependencies and both past treatments and covariates affecting the future outcomes. In this paper, we introduce COunterfactual Self-supervised TrAnsformeR (COSTAR), a novel approach that integrates self-supervised learning for improved historical representations. The proposed framework combines temporal and feature-wise attention with a component-wise contrastive loss tailored for temporal treatment outcome observations, yielding superior performance in estimation accuracy and generalization to out-of-distribution data compared to existing models, as validated by empirical results on both synthetic and real-world datasets.
    
[^380]: 潜在空间翻译通过语义对齐

    Latent Space Translation via Semantic Alignment. (arXiv:2311.00664v1 [cs.LG])

    [http://arxiv.org/abs/2311.00664](http://arxiv.org/abs/2311.00664)

    本论文研究了潜在空间的翻译问题。通过简单的变换，可以将不同神经模型学到的表示翻译到其他预训练网络中。这种方法能够有效地拼接编码器和解码器，并在各种实验设置中得到验证。

    

    虽然不同的神经模型在接触到语义相关的数据时往往会展现出相似的潜在空间，但这种内在的相似性并不总是立即可辨。为了更好地理解这一现象，我们的工作展示了如何通过比以前认为的更简单的变换将从这些神经模块学到的表示翻译到不同的预训练网络之间。这种方法的优势在于能够使用标准的、通用的代数程序来估计这些变换，并且这些程序具有封闭形式的解。我们的方法直接估计两个给定潜在空间之间的转换，从而实现了有效的编码器和解码器的拼接而无需额外训练。我们在不同的实验设置中广泛验证了这种翻译过程的适应性：包括各种训练数据、领域、架构（如ResNet、CNN、ViT）以及多种下游任务（分类、重构）。

    While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Nota
    
[^381]: 道路安全建模的图神经网络：用于事故分析的数据集和评估

    Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis. (arXiv:2311.00164v1 [cs.SI])

    [http://arxiv.org/abs/2311.00164](http://arxiv.org/abs/2311.00164)

    该论文构建了一个大规模的道路交通事故记录数据集，并使用该数据集评估了现有的深度学习方法在预测事故发生方面的准确性。研究发现，图神经网络GraphSAGE能够准确预测道路上的事故数量，并判断事故是否会发生。

    

    我们考虑基于道路网络连接和交通流量的道路网络上的交通事故分析问题。以往的工作使用历史记录设计了各种深度学习方法来预测交通事故的发生。然而，现有方法的准确性缺乏共识，并且一个基本问题是缺乏公共事故数据集进行全面评估。本文构建了一个大规模的、统一的道路交通事故记录数据集，包括来自美国各州官方报告的900万条记录，以及道路网络和交通流量报告。利用这个新数据集，我们评估了现有的深度学习方法来预测道路网络上的事故发生。我们的主要发现是，像GraphSAGE这样的图神经网络可以准确预测道路上的事故数量，平均绝对误差不超过实际数目的22%，并能够判断事故是否会发生。

    We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not w
    
[^382]: 受约束的Actor Critic和受约束的Natural Actor Critic算法的有限时间分析

    Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms. (arXiv:2310.16363v1 [cs.LG])

    [http://arxiv.org/abs/2310.16363](http://arxiv.org/abs/2310.16363)

    本文研究了受约束的Actor Critic和受约束的Natural Actor Critic算法的有限时间分析，证明了这些算法能找到性能函数的一阶稳定点，并且具有较低的样本复杂度。

    

    Actor Critic方法在广泛的强化学习任务中找到了巨大的应用，特别是当状态-动作空间很大的时候。本文考虑使用函数逼近的actor critic和natural actor critic算法来处理涉及不等式约束的马尔可夫决策过程（C-MDP），并在非 i.i.d（马尔可夫）环境中进行了非渐近分析。我们考虑长期平均成本准则，其中目标和约束函数都是某些规定成本函数的适当策略依赖的长期平均。我们使用拉格朗日乘子法处理不等式约束。我们证明这些算法保证能找到性能（拉格朗日）函数$L(\theta,\gamma)$的一阶稳定点（即$\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$），并且其样本复杂度为$\mathcal{\tilde{O}}(\epsilon^{-2.5})$。

    Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$) of the performance (Lagrange) function $L(\theta,\gamma)$, with a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ in the case of both C
    
[^383]: FedLoRA：基于LoRA调整的模型异构个性化联邦学习

    FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning. (arXiv:2310.13283v1 [cs.LG])

    [http://arxiv.org/abs/2310.13283](http://arxiv.org/abs/2310.13283)

    FedLoRA是基于LoRA调整的计算和通信高效的模型异构个性化联邦学习框架，旨在为每个联邦学习客户端训练个性化且异构的本地模型。

    

    联邦学习是一种新兴的机器学习范 Paradig，其中一个中央服务器协调多个参与者（即FL客户端）在分散的数据上进行协作训练模型，同时保护隐私。这种范 Paradig 限制了所有客户端必须使用相同结构的模型（同构）。实践中，FL经常面临统计异质性、系统异质性和模型异质性等挑战。这些问题激发了模型异构个性化联邦学习（MHPFL）领域的研究，旨在为每个FL客户端训练一个个性化且异构的本地模型。现有的MHPFL方法无法同时实现令人满意的模型性能、可接受的计算开销和高效的通信。为了填补这一差距，我们提出了一种基于LoRA调整的计算和通信高效的模型异构个性化联邦学习框架（FedLoRA）。

    Federated learning (FL) is an emerging machine learning paradigm in which a central server coordinates multiple participants (a.k.a. FL clients) to train a model collaboratively on decentralized data with privacy protection. This paradigm constrains that all clients have to train models with the same structures (homogeneous). In practice, FL often faces statistical heterogeneity, system heterogeneity and model heterogeneity challenges. These challenging issues inspire the field of Model-Heterogeneous Personalized Federated Learning (MHPFL) which aims to train a personalized and heterogeneous local model for each FL client. Existing MHPFL approaches cannot achieve satisfactory model performance, acceptable computational overhead and efficient communication simultaneously. To bridge this gap, we propose a novel computation- and communication-efficient model-heterogeneous personalized Federated learning framework based on LoRA tuning (FedLoRA). It is designed to incorporate a homogeneous 
    
[^384]: 使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法

    Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])

    [http://arxiv.org/abs/2310.12609](http://arxiv.org/abs/2310.12609)

    本文提出了一种使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法。该方法通过单一的视觉输入，在推理时能够同时生成可达目标并规划避开障碍物的运动路径，具有稳健性和多模态适应性。

    

    由于其灵活性和多模态性，扩散模型在机器人领域中已经成为一种强大的工具。尽管其中一些方法有效地解决了复杂问题，但它们往往严重依赖于推理时的障碍物检测并需要额外的设备。为了应对这些挑战，我们提出了一种方法，该方法在推理时能够从单一的视觉输入中同时生成可达目标并规划避开障碍物的运动路径。我们的方法的核心是对训练过程中新颖的碰撞避免扩散核进行使用。通过与行为克隆和经典扩散模型进行评估，我们的框架证明了其稳健性。特别是在多模态环境中，它能够导航到目标并避开被障碍物阻挡的不可达目标，同时确保避免碰撞。

    Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
    
[^385]: ByteStack-ID: 基于灰度图像的网络入侵检测的集成堆叠模型，利用负载字节频率

    ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])

    [http://arxiv.org/abs/2310.09298](http://arxiv.org/abs/2310.09298)

    ByteStack-ID是一种基于灰度图像和负载字节频率的集成堆叠模型，用于数据包级入侵检测。它能迅速准确地识别网络流量中的各种攻击类型，并与传统方法有所不同。

    

    在不断发展的网络安全领域中，迅速准确地识别网络流量中的各种攻击类型至关重要。本文介绍了"ByteStack-ID"，一种专为数据包级入侵检测而设计的创新方法。ByteStack-ID核心是利用从负载数据的频率分布生成的灰度图像，这是一种突破性的技术，极大地提高了模型识别复杂数据模式的能力。值得注意的是，我们的方法完全基于数据包级信息，与传统的基于流量数据的网络入侵检测系统（NIDS）有所不同。在基本堆叠方法的基础上，ByteStack-ID与传统的堆叠方法不同。它将附加的元学习器层无缝集成到连接的基础学习器中，创建了一个高度优化的统一模型。

    In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
    
[^386]: 分辨时间差分学习

    Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])

    [http://arxiv.org/abs/2310.08091](http://arxiv.org/abs/2310.08091)

    该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。

    

    时间差分学习(TD)是强化学习中的基本概念，旨在高效评估策略的价值函数。TD($\lambda$)是一种有效的变体，通过引入记忆轨迹将预测误差分散到历史上下文中。然而，这种方法经常忽视历史状态的重要性以及传播TD误差的相对重要性，这受到访问失衡或结果噪声等挑战的影响。为了解决这个问题，我们提出了一种名为分辨TD学习(DTD)的新型TD算法，它允许灵活的强调函数-在训练过程中预先确定或自适应地分配资源以提高状态之间的效果。我们在特定类别的强调函数内建立了我们方法的收敛性质，并展示了它在深度RL环境中的潜在应用。实证结果表明，使用合理的强调函数不仅可以改进值估计，还可以加速学习。

    Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l
    
[^387]: 基于分位数的极大似然训练用于异常检测

    Quantile-based Maximum Likelihood Training for Outlier Detection. (arXiv:2310.06085v1 [cs.CV])

    [http://arxiv.org/abs/2310.06085](http://arxiv.org/abs/2310.06085)

    本文提出了一种基于分位数的极大似然目标，用于改进异常检测中异常值的分离程度。通过将正则化流拟合到预训练的判别性特征，并根据对数似然度评估来检测异常值。实验结果表明，这种方法优于最先进的无监督模型。

    

    判别性学习有效地对图像分类预测真实的对象类别。然而，在异常值方面，它经常导致误报阳性，这在自主驾驶和视频监视系统等应用中引起了严重关注。以往解决这个挑战的尝试包括使用实际异常值数据通过对比学习训练图像分类器，或者通过合成异常值进行自我监督学习。此外，像素空间中对内点进行无监督生成建模对于异常检测显示出有限的成功。在这项工作中，我们引入了一种基于分位数的极大似然目标，用于学习内点分布，以在推断过程中提高异常值的分离程度。我们的方法通过将正则化流拟合到预训练的判别性特征，并根据评估的对数似然度来检测异常值。实验评估证明了我们方法的有效性，因为它超过了最先进的无监督方法的性能。

    Discriminative learning effectively predicts true object class for image classification. However, it often results in false positives for outliers, posing critical concerns in applications like autonomous driving and video surveillance systems. Previous attempts to address this challenge involved training image classifiers through contrastive learning using actual outlier data or synthesizing outliers for self-supervised learning. Furthermore, unsupervised generative modeling of inliers in pixel space has shown limited success for outlier detection. In this work, we introduce a quantile-based maximum likelihood objective for learning the inlier distribution to improve the outlier separation during inference. Our approach fits a normalizing flow to pre-trained discriminative features and detects the outliers according to the evaluated log-likelihood. The experimental evaluation demonstrates the effectiveness of our method as it surpasses the performance of the state-of-the-art unsupervi
    
[^388]: 鲁棒的角度同步问题的有向图神经网络解决方案

    Robust Angular Synchronization via Directed Graph Neural Networks. (arXiv:2310.05842v1 [cs.LG])

    [http://arxiv.org/abs/2310.05842](http://arxiv.org/abs/2310.05842)

    本论文提出了一个名为GNNSync的基于有向图神经网络的鲁棒角度同步解决方案，解决了角度同步问题在高噪声环境下的挑战，并提出了新的损失函数以更好地编码同步约束。

    

    角度同步问题旨在通过$m$个偏移量$\theta_i-\theta_j \;\mbox{mod} \; 2\pi$的噪声测量准确估计（最多一个常数相位偏移）一组未知角度$\theta_1, \dots, \theta_n\in[0, 2\pi)$. 应用包括传感器网络定位、相位恢复和分布式时钟同步。该问题的异构扩展（称为$k$-同步）是同时估计$k$组角度，给定每个组的未知组分配的噪声观察值。现有的角度同步方法在高噪声环境下通常表现不佳，而这在应用中很常见。在本文中，我们利用神经网络解决角度同步问题及其异构扩展，提出了GNNSync，这是一个理论支撑的端到端可训练框架，使用有向图神经网络。此外，我们设计了新的损失函数来编码角度同步的约束。

    The angular synchronization problem aims to accurately estimate (up to a constant additive phase) a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example, sensor network localization, phase retrieval, and distributed clock synchronization. An extension of the problem to the heterogeneous setting (dubbed $k$-synchronization) is to estimate $k$ groups of angles simultaneously, given noisy observations (with unknown group assignment) from each group. Existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. In this paper, we leverage neural networks for the angular synchronization problem, and its heterogeneous extension, by proposing GNNSync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. In addition, new loss functions are devised to encode synchro
    
[^389]: PGraphDTA: 使用蛋白质语言模型和接触图改进药物靶标相互作用预测

    PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps. (arXiv:2310.04017v1 [cs.LG])

    [http://arxiv.org/abs/2310.04017](http://arxiv.org/abs/2310.04017)

    本研究提出了一种使用蛋白质语言模型和接触图改进药物靶标相互作用预测的方法，通过在现有模型中引入联系图信息，可以改进药物靶标相互作用预测的性能。

    

    发现和开发新药是一项复杂且资源密集型的工作，通常涉及大量成本、时间投入和安全问题。药物发现的一个关键方面是确定新颖的药物-靶标（DT）相互作用。现有的预测DT相互作用的计算方法主要集中在二分类任务上，旨在确定DT对是否存在相互作用。然而，蛋白质-配体相互作用表现出一系列不同结合强度，即结合亲和力，这对准确预测造成了持续挑战。在本研究中，我们研究了在药物靶标相互作用（DTI）预测中使用的各种技术，并提出了改进性能的新方法。我们的方法包括整合蛋白质语言模型（PLMs）和将接触图信息作为现有模型的归纳偏差。通过广泛的实验，我们证明了我们提出的方法的性能优于传统方法。

    Developing and discovering new drugs is a complex and resource-intensive endeavor that often involves substantial costs, time investment, and safety concerns. A key aspect of drug discovery involves identifying novel drug-target (DT) interactions. Existing computational methods for predicting DT interactions have primarily focused on binary classification tasks, aiming to determine whether a DT pair interacts or not. However, protein-ligand interactions exhibit a continuum of binding strengths, known as binding affinity, presenting a persistent challenge for accurate prediction. In this study, we investigate various techniques employed in Drug Target Interaction (DTI) prediction and propose novel enhancements to enhance their performance. Our approaches include the integration of Protein Language Models (PLMs) and the incorporation of Contact Map information as an inductive bias within current models. Through extensive experimentation, we demonstrate that our proposed approaches outper
    
[^390]: 通过元模型改进了配体-蛋白质结合亲和力的预测

    Improved prediction of ligand-protein binding affinities by meta-modeling. (arXiv:2310.03946v1 [cs.LG])

    [http://arxiv.org/abs/2310.03946](http://arxiv.org/abs/2310.03946)

    通过整合基于结构的对接和基于序列的深度学习模型，开发了一个元模型框架，显著改善了配体-蛋白质结合亲和力预测的性能。

    

    通过计算方法准确筛选候选药物配体与靶蛋白的结合是药物开发的主要关注点，因为筛选潜在候选物能够节省找药物的时间和费用。这种虚拟筛选部分依赖于预测配体和蛋白质之间的结合亲和力的方法。鉴于存在许多计算模型对不同目标的结合亲和力预测结果不同，我们在这里开发了一个元模型框架，通过整合已发表的基于结构的对接和基于序列的深度学习模型来构建。在构建这个框架时，我们评估了许多组合的个别模型、训练数据库以及线性和非线性的元模型方法。我们显示出许多元模型在亲和力预测上显著改善了个别基础模型的性能。我们最好的元模型达到了与最先进的纯结构为基础的深度学习工具相当的性能。总体而言，我们证明了这个元模型框架可以显著改善配体-蛋白质结合亲和力预测的性能。

    The accurate screening of candidate drug ligands against target proteins through computational approaches is of prime interest to drug development efforts, as filtering potential candidates would save time and expenses for finding drugs. Such virtual screening depends in part on methods to predict the binding affinity between ligands and proteins. Given many computational models for binding affinity prediction with varying results across targets, we herein develop a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models. In building this framework, we evaluate many combinations of individual models, training databases, and linear and nonlinear meta-modeling approaches. We show that many of our meta-models significantly improve affinity predictions over individual base models. Our best meta-models achieve comparable performance to state-of-the-art exclusively structure-based deep learning tools. Overall, we demonstrate 
    
[^391]: 在重尾波段的完全自适应遗憾最小化领域中的研究

    Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits. (arXiv:2310.02975v1 [cs.LG])

    [http://arxiv.org/abs/2310.02975](http://arxiv.org/abs/2310.02975)

    本文研究了在重尾波段问题中完全自适应的遗憾最小化，提出了随机自适应重尾波段问题，并证明了适应性算法相对于标准设置会有更高的遗憾。

    

    重尾分布在金融到电信等多种环境中自然而然地出现。虽然在次高斯或有界支撑奖励下的遗憾最小化已被广泛研究，但在重尾分布上的学习只在过去十年中受到关注。在随机重尾波段问题中，一个代理在假设分布有有界最大阶的有限矩的情况下学习，这些矩被常数u一致有界，对于某个ε∈(0,1]。据我们所知，文献中只提供需要这两个量作为输入的算法。在本文中，我们研究了随机自适应重尾波段问题，这是标准设置的一个变种，其中代理对ε和u均不知晓。我们表明，适应性是存在代价的，并引入对于任何自适应算法遗憾的两个下界，意味着相对于标准设置有更高的遗憾。最后，我们引入一种特定的分布假设。

    Heavy-tailed distributions naturally arise in many settings, from finance to telecommunications. While regret minimization under sub-Gaussian or bounded support rewards has been widely studied, learning on heavy-tailed distributions only gained popularity over the last decade. In the stochastic heavy-tailed bandit problem, an agent learns under the assumption that the distributions have finite moments of maximum order $1+\epsilon$ which are uniformly bounded by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge, literature only provides algorithms requiring these two quantities as an input. In this paper, we study the stochastic adaptive heavy-tailed bandit, a variation of the standard setting where both $\epsilon$ and $u$ are unknown to the agent. We show that adaptivity comes at a cost, introducing two lower bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t. the standard setting. Finally, we introduce a specific distributional ass
    
[^392]: 使用SAM进行建筑物分割模型的零-shot细化

    Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])

    [http://arxiv.org/abs/2310.01845](http://arxiv.org/abs/2310.01845)

    本文提出了一种使用SAM进行建筑物分割模型的零-shot细化的方法，针对遥感图像应用中SAM性能不佳、无法进行识别的问题进行了处理。通过引入不同的提示来提升模型的泛化能力。

    

    基础模型在各种任务中表现出色，但通常在常规基准测试中评估。将这些模型应用于特定领域，如遥感图像，仍然是一个未充分开发的领域。在遥感领域中，精确的建筑物实例分割对于城市规划等应用至关重要。虽然卷积神经网络（CNN）表现良好，但它们的泛化能力可能受限。为了实现这一目标，我们提出了一种新的方法，以使基础模型适应已有模型的泛化性能下降。在众多模型中，我们的重点在于Segment Anything Model（SAM），这是一种强大的基础模型，以其擅长无类别图像分割能力而闻名。我们首先确定了SAM的局限性，揭示了它在应用于遥感图像时性能不佳。此外，SAM不具备识别能力，因此无法对定位的对象进行分类和标记。为了解决这些限制，我们引入了不同的提示

    Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
    
[^393]: 完美预测储备计算在自回归时间序列数据中的数学结构

    Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data. (arXiv:2310.00290v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00290](http://arxiv.org/abs/2310.00290)

    这篇论文研究了储备计算在自回归时间序列数据中的数学结构，并揭示了其隐藏的权重矩阵结构，以实现对AR类型时间序列数据的完美预测。

    

    储备计算（RC）是一种递归神经网络（RNN），毫无疑问，RC将越来越广泛地用于构建时间序列数据的未来预测模型，具有低训练成本、高速度和高计算能力。然而，对于RC神经网络的数学结构的研究直到最近才开始。Bollt（2021）阐明了自回归（AR）模型对于理解RC神经网络的数学结构的必要性，并指出Wold分解定理是理解这些结构的里程碑。在铭记这一著名结果的基础上，本文阐明了RC神经网络中输入和循环权重矩阵的隐藏结构，并展示了这些结构对于AR类型的时间序列数据实现了完美预测。

    Reservoir Computing (RC) is a type of recursive neural network (RNN), and there can be no doubt that the RC will be more and more widely used for building future prediction models for time-series data, with low training cost, high speed and high computational power. However, research into the mathematical structure of RC neural networks has only recently begun. Bollt (2021) clarified the necessity of the autoregressive (AR) model for gaining the insight into the mathematical structure of RC neural networks, and indicated that the Wold decomposition theorem is the milestone for understanding of these. Keeping this celebrated result in mind, in this paper, we clarify hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures attain perfect prediction for the AR type of time series data.
    
[^394]: 高效的生物合理对抗训练

    Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])

    [http://arxiv.org/abs/2309.17348](http://arxiv.org/abs/2309.17348)

    本文研究了生物合理的学习算法是否比反向传播更具有对抗攻击的鲁棒性，并进行了广泛的比较分析。

    

    用反向传播训练的人工神经网络(ANNs)表现出令人惊讶的性能，并且越来越多地被用于执行我们日常生活中的任务。然而，ANNs极易受到对抗攻击的影响，这些攻击通过微小的有针对性的扰动来改变输入，从而严重破坏模型的性能。使ANNs对这些攻击具有鲁棒性最有效的方法是对抗训练，其中训练数据集被添加了样本用于对抗攻击。不幸的是，这种方法的缺点是增加了训练复杂性，因为生成对抗样本是非常计算消耗高的。与ANNs不同，人类不容易受到对抗攻击的影响。因此，在这项工作中，我们研究了生物合理的学习算法是否比BP更具有对抗攻击的鲁棒性。具体而言，我们对BP和“Error to Pertu"的对抗鲁棒性进行了广泛的比较分析。

    Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
    
[^395]: 通用睡眠解码器：将觉醒和睡眠神经表示对齐于不同个体间

    Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects. (arXiv:2309.16457v1 [cs.LG])

    [http://arxiv.org/abs/2309.16457](http://arxiv.org/abs/2309.16457)

    该论文设计了一项新颖的实验并收集了52名参与者的全面脑电图数据集，从而解决了觉醒和睡眠状态下神经表示的差异问题。研究团队开发了通用睡眠解码器（USD），可以在不同个体间对齐觉醒和睡眠的神经模式，并取得了与使用个别睡眠数据进行解码相当的准确率。研究还发现，在测试个体上对USD进行微调可以进一步提高解码准确性。

    

    通过脑活动解码睡眠中的记忆内容长期以来一直是神经科学的目标。虽然已知啮齿类动物在睡眠中自发地重新激活记忆以支持记忆巩固和离线学习，但由于缺乏经过完整注释的睡眠数据集以及清醒状态和睡眠状态之间神经模式的巨大差异，捕捉人类的记忆再现是具有挑战性的。为了解决这些挑战，我们设计了一项新颖的认知神经科学实验，并从52名参与者收集了一份全面、完整注释的脑电图（EEG）数据集，涵盖了觉醒和睡眠两种状态。利用这个基准数据集，我们开发了通用睡眠解码器（USD），用于在不同个体间对齐觉醒与睡眠的神经表示。我们的模型在未见过的个体上实现了高达16.6%的top-1零样本准确率，与使用个别睡眠数据进行解码的性能相当。此外，对测试个体的USD进行微调可以提高解码准确性。

    Decoding memory content from brain activity during sleep has long been a goal in neuroscience. While spontaneous reactivation of memories during sleep in rodents is known to support memory consolidation and offline learning, capturing memory replay in humans is challenging due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep. To address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 52 subjects during both wakefulness and sleep. Leveraging this benchmark dataset, we developed the Universal Sleep Decoder (USD) to align neural representations between wakefulness and sleep across subjects. Our model achieves up to 16.6% top-1 zero-shot accuracy on unseen subjects, comparable to decoding performances using individual sleep data. Furthermore, fine-tuning USD on test subjects enhances decoding accuracy
    
[^396]: 基于似然比的任务预测的类增量学习

    Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])

    [http://arxiv.org/abs/2309.15048](http://arxiv.org/abs/2309.15048)

    该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。

    

    类增量学习是一种具有挑战性的不断学习的设置，通过顺序学习一系列任务。每个任务由一组唯一的类组成。类增量学习的关键特点是，在测试时不提供每个测试样本的任务标识符（或任务ID）。为每个测试样本预测任务ID是一个具有挑战性的问题。一种新兴的理论上合理且有效的方法是根据任务增量学习的方法，在共享网络中为所有任务训练每个任务的任务特定模型，以处理遗忘。该方法中每个任务的模型是一个非常规分类器而不是传统分类器的离群检测器。离群检测器可以对任务内（分布内（IND））的类进行预测和识别离群数据。在推断期间，离群检测能力是每个测试样本的任务ID预测的关键。然而，本文认为使用传统的离群检测器进行任务ID预测是次优的。

    Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
    
[^397]: PCN：一种利用新颖的图构建方法和切比雪夫图卷积的深度学习方法进行喷注标记

    PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions. (arXiv:2309.08630v1 [hep-ph])

    [http://arxiv.org/abs/2309.08630](http://arxiv.org/abs/2309.08630)

    本研究提出了一种基于图形的喷注表示方法，并设计了一种名为PCN的图神经网络（GNN），利用切比雪夫图卷积（ChebConv）进行深度学习喷注标记，取得了显著的改进。

    

    喷注标记是高能物理实验中的一个分类问题，旨在识别粒子碰撞产生的锥状喷注，并将其标记为发射粒子。喷注标记的进展为超出标准模型的新物理搜索提供了机会。目前的方法使用深度学习在复杂碰撞数据中寻找隐藏的模式。然而，将喷注表示为深度学习模型的输入的方法多种多样，并且通常会向模型隐藏有信息的特征。在这项研究中，我们提出了一种基于图形的喷注表示方法，以尽可能地编码最多的信息。为了从这种表示中最好地学习，我们设计了一种名为Particle Chebyshev Network（PCN）的图神经网络（GNN），并使用切比雪夫图卷积（ChebConv）。ChebConv已经被证明是GNN中的一种有效替代传统图卷积的方法，而在喷注标记中还没有被探索过。PCN取得了显著的改进。

    Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvemen
    
[^398]: 添加语义上下文的控制力量，为金融市场数据生成引入Market-GAN

    Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context. (arXiv:2309.07708v1 [cs.LG])

    [http://arxiv.org/abs/2309.07708](http://arxiv.org/abs/2309.07708)

    本研究通过提出具有市场动态、股票代码和历史状态作为上下文的上下文市场数据集以及使用条件生成对抗网络（GAN）来实现对金融数据生成的控制。

    

    金融模拟器在提升预测准确性、管理风险和促进战略金融决策方面发挥着重要作用。尽管已经开发了金融市场模拟方法，但现有的框架常常难以适应专门的模拟上下文。我们将挑战归结为：i）当前的金融数据集不包含上下文标签；ii）当前的技术没有设计用于生成具有上下文控制的金融数据，与其他模态相比，这要求更高的精度；iii）由于金融数据的非平稳、噪声性质，生成与上下文对齐、高保真度的数据存在困难。为了解决这些挑战，我们的贡献是：i）提出了具有市场动态、股票代码和历史状态作为上下文的上下文市场数据集，利用线性回归和动态时间扭曲聚类结合的市场动态建模方法提取市场动态；ii）我们预先准备了Market-GAN模型，该模型通过使用条件生成对抗网络（GAN）的方法以及编码上下文向量的方式来实现对金融数据生成的控制。

    Financial simulators play an important role in enhancing forecasting accuracy, managing risks, and fostering strategic financial decision-making. Despite the development of financial market simulation methodologies, existing frameworks often struggle with adapting to specialized simulation context. We pinpoint the challenges as i) current financial datasets do not contain context labels; ii) current techniques are not designed to generate financial data with context as control, which demands greater precision compared to other modalities; iii) the inherent difficulties in generating context-aligned, high-fidelity data given the non-stationary, noisy nature of financial data. To address these challenges, our contributions are: i) we proposed the Contextual Market Dataset with market dynamics, stock ticker, and history state as context, leveraging a market dynamics modeling method that combines linear regression and Dynamic Time Warping clustering to extract market dynamics; ii) we prese
    
[^399]: 使用基础模型对零样本模型进行零样本强化

    Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])

    [http://arxiv.org/abs/2309.04344](http://arxiv.org/abs/2309.04344)

    提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。

    

    零样本推断是一种强大的范式，可以在没有进一步训练的情况下使用预训练模型来进行下游分类任务。然而，这些模型容易受到继承的偏见的影响，从而影响它们的性能。传统的解决方案是微调，但这削弱了预训练模型的主要优势，即可以直接使用的能力。我们提出了RoboShot，一种完全零样本的方法，可以改善预训练模型嵌入的鲁棒性。首先，我们使用零样本语言模型（LMs）从任务描述中获取有用的见解。这些见解被嵌入并用于去除嵌入中的有害成分并增强有用成分--而无需任何监督。从理论上讲，我们提供了一个简单且可计算的模型，用于分析零样本嵌入中的偏见，并给出了在什么条件下我们的方法可以提高性能的结果。在实证上，我们在九个图像和NLP分类任务上评估了RoboShot。

    Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s
    
[^400]: 对多样化投资组合进行强化学习技术的评估

    Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio. (arXiv:2309.03202v1 [q-fin.TR])

    [http://arxiv.org/abs/2309.03202](http://arxiv.org/abs/2309.03202)

    本研究评估了在S&P 500指数上使用强化学习技术进行多样化投资组合的可行性。研究发现，包含COVID-19时期的市场数据在训练数据集中可以提供更好的性能，并且基于策略的方法（VI和SARSA）在测试中表现优于Q学习。

    

    本研究旨在回答关于在S&P 500指数上使用强化学习的可行性的关键研究问题。采用了基于策略的价值迭代（VI）和状态-动作-奖励-状态-动作（SARSA）的技术，以及基于策略外的Q学习。该模型在包含2000-2023年多年股市数据的数据集上进行训练和测试。分析展示了在两个不同时间段上训练和测试模型的结果和发现：一个包括COVID-19大流行期间的年份，一个不包括。结果表明，在训练数据集中包含COVID-19时期的市场数据比基准策略表现更好。在测试中，基于策略的方法（VI和SARSA）优于Q学习，凸显了简单策略的偏差-方差权衡和泛化能力的影响。然而，需要注意的是，Q学习的性能可能因条件的变化而有所不同。

    This work seeks to answer key research questions regarding the viability of reinforcement learning over the S&P 500 index. The on-policy techniques of Value Iteration (VI) and State-action-reward-state-action (SARSA) are implemented along with the off-policy technique of Q-Learning. The models are trained and tested on a dataset comprising multiple years of stock market data from 2000-2023. The analysis presents the results and findings from training and testing the models using two different time periods: one including the COVID-19 pandemic years and one excluding them. The results indicate that including market data from the COVID-19 period in the training dataset leads to superior performance compared to the baseline strategies. During testing, the on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the influence of bias-variance tradeoff and the generalization capabilities of simpler policies. However, it is noted that the performance of Q-learning may vary depe
    
[^401]: 证明LLM对抗敌对提示的安全性

    Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])

    [http://arxiv.org/abs/2309.02705](http://arxiv.org/abs/2309.02705)

    本研究提出了首个具有可验证安全保证的框架——消除和检查，用于对抗敌对提示。通过逐个消除标记并使用安全过滤器检查生成的子序列，确保任何敌对修改的有害输入提示都能被正确标识为有害。

    

    为了确保语言模型的输出安全，公开使用的大型语言模型（LLM）引入了所谓的“模型对齐”防护措施。一个对齐的语言模型应该拒绝用户的请求生成有害内容。然而，这种安全措施容易受到敌对提示的攻击，敌对提示包含恶意设计的标记序列，以规避模型的安全防护并导致生成有害内容。在这项工作中，我们介绍了可验证安全保证的第一个对抗敌对提示的框架——消除和检查。我们逐个消除标记，并使用安全过滤器检查生成的子序列。如果安全过滤器检测到任何子序列或输入提示有害，我们的过程将将输入提示标记为有害。这保证了对于某个特定大小的有害输入提示的任何敌对修改也将被标记为有害。我们对抗三种攻击模式：i)敌对后缀，即附加敌对序列…

    Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
    
[^402]: 语音障碍者的声学到发音反演: 预先训练的自监督表示是否有利？

    Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?. (arXiv:2309.01108v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.01108](http://arxiv.org/abs/2309.01108)

    本研究探讨了使用预先训练的自监督表示对语音障碍者的声学到发音反演任务的影响。实验结果表明，在低资源条件下，经过微调的DeCoAR模型在精细训练方案中相对于健康对照组和患者，分别取得了约1.81\%和约4.56\%的皮尔逊相关系数(CC)的改进。

    

    声学到发音反演(ACI)涉及从声学空间映射到发音空间。信号处理特征如MFCCs已被广泛应用于ACI任务。对于有语音障碍的患者，由于不准确和不清晰的发音，ACI是具有挑战性的。本研究使用预先训练的自监督学习(SSL)模型中的表示对语音障碍的ACI进行了实验。我们评估了不同预训练特征对这个具有挑战性的低资源ACI任务的影响。此外，我们还将x-vectors与提取的SSL特征相结合，训练了一个BLSTM网络。在已知情况下，我们尝试了三种ACI训练方案（主题特定，聚合和微调）。结果一致表明，DeCoAR在微调方案中，相对于健康对照组和患者，皮尔逊相关系数(CC)的改进幅度分别为约1.81\%和约4.56\%。

    $ $Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic space to the articulatory space. Signal-processing features like the MFCCs, have been widely used for the AAI task. For subjects with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI for dysarthric speech using representations from pre-trained self-supervised learning (SSL) models. We demonstrate the impact of different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the extracted SSL features to train a BLSTM network. In the seen case, we experiment with three AAI training schemes (subject-specific, pooled, and fine-tuned). The results, consistent across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for healthy controls and patients, 
    
[^403]: 基于贝叶斯深度学习的宇宙尺度中的修正引力研究

    Bayesian deep learning for cosmic volumes with modified gravity. (arXiv:2309.00612v1 [astro-ph.CO])

    [http://arxiv.org/abs/2309.00612](http://arxiv.org/abs/2309.00612)

    该研究利用贝叶斯深度学习的方法，从修正引力模拟中提取宇宙学参数，并对不确定性进行了评估。

    

    新一代的星系调查将提供前所未有的数据，使我们能够在宇宙尺度上测试引力。对大尺度结构的健壮宇宙学分析需要利用编码在宇宙网中的非线性信息。机器学习技术提供了这样的工具，然而却不能提供先验的不确定性评估。本研究旨在通过具有不确定性估计的深度神经网络从修正引力（MG）模拟中提取宇宙学参数。我们使用贝叶斯神经网络（BNNs）实现了一个丰富的近似后验分布，分别考虑了一个带有单一贝叶斯最后一层（BLL）的情况，和一个在所有层面上都具有贝叶斯层（FullB）的情况。我们使用实空间密度场和一套2000个仅包含暗物质粒子网格$ N $-体模拟的功率谱对这两个BNN进行训练，这些模拟包括基于MG-PICOLA的修正引力模型，覆盖了边长为256 $h^{-1}$ Mpc的立方体体积，其中包含128$。

    The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity at cosmological scales. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $N$-body simulations including modified gravity models relying on MG-PICOLA covering 256 $h^{-1}$ Mpc side cubical volumes with 128$
    
[^404]: 量子系统中激发态的自然量子蒙特卡洛计算

    Natural Quantum Monte Carlo Computation of Excited States. (arXiv:2308.16848v1 [physics.comp-ph])

    [http://arxiv.org/abs/2308.16848](http://arxiv.org/abs/2308.16848)

    该论文提出了一种变分蒙特卡洛算法，用于估计量子系统中的激发态，通过转化问题使其成为寻找扩展系统的基态的问题。这种方法特别适用于多电子系统，并且可以准确地计算各种可观测量的期望值，包括非对角线期望值和跃迁偶极矩，并在苯等大分子上得到了良好的结果。

    

    我们提出了一种变分蒙特卡洛算法，用于估计量子系统的最低激发态，这是对寻找基态的估计的自然推广。该方法没有自由参数，并且不需要显式正交化不同的态，而是将寻找给定系统的激发态的问题转化为寻找扩展系统的基态的问题。可以计算任意可观测量的期望值，包括不同态之间的非对角线期望值，如跃迁偶极矩。尽管该方法完全通用，但与最近关于使用神经网络作为多电子系统变分参数的工作结合使用效果特别好，我们展示了通过将该方法与FermiNet和Psiformer变分参数结合使用，可以准确地恢复苯等大分子的垂直激发能和振子强度。除了在分子上的示例之外，我们还...

    We present a variational Monte Carlo algorithm for estimating the lowest excited states of a quantum system which is a natural generalization of the estimation of ground states. The method has no free parameters and requires no explicit orthogonalization of the different states, instead transforming the problem of finding excited states of a given system into that of finding the ground state of an expanded system. Expected values of arbitrary observables can be calculated, including off-diagonal expectations between different states such as the transition dipole moment. Although the method is entirely general, it works particularly well in conjunction with recent work on using neural networks as variational Ansatze for many-electron systems, and we show that by combining this method with the FermiNet and Psiformer Ansatze we can accurately recover vertical excitation energies and oscillator strengths on molecules as large as benzene. Beyond the examples on molecules presented here, we 
    
[^405]: 从高资源语言到低资源编程语言的知识转移用于代码LLMs

    Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])

    [http://arxiv.org/abs/2308.09895](http://arxiv.org/abs/2308.09895)

    本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。

    

    在过去几年中，代码LLMs（大规模语言模型）开始对编程实践产生重大影响。代码LLMs还成为编程语言和软件工程研究的重要组成部分。然而，代码LLMs生成的代码质量在不同编程语言之间存在显著差异。代码LLMs对训练数据充分的编程语言（如Java、Python或JavaScript）产生令人印象深刻的结果，但在像OCaml和Racket这样的低资源语言上表现困难。本文提出了一种有效的方法，通过使用半合成数据提高代码LLMs在低资源语言上的性能。我们的方法生成了高质量的低资源语言数据集，并可用于微调任何预训练的代码LLMs。我们的方法称为MultiPL-T，它将高资源语言的训练数据转化为低资源语言的训练数据。我们将该方法应用于生成十个数据集。

    Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
    
[^406]: 在大型视觉语言模型中检测和预防幻觉

    Detecting and Preventing Hallucinations in Large Vision Language Models. (arXiv:2308.06394v1 [cs.CV])

    [http://arxiv.org/abs/2308.06394](http://arxiv.org/abs/2308.06394)

    本论文提出了一个用于训练和评估模型的多模态幻觉检测数据集，以解决大型视觉语言模型中存在的幻觉文本问题。这是第一个用于详细图像描述的全面多模态幻觉检测数据集。

    

    经过调整的大型视觉语言模型（LVLMs）在泛化跨多种多模态任务方面取得了显著进展，特别是在视觉问答（VQA）方面。然而，为这些模型生成与视觉相关的详细回答仍然是一个具有挑战性的任务。我们发现，即使是当前最先进的LVLM（InstructBLIP）仍然存在着惊人的30%的幻觉文本，包括不存在的对象、不忠实的描述和不准确的关系。为了解决这个问题，我们引入了M-HalDetect，这是一个用于训练和评估幻觉检测和预防模型的多模态幻觉检测数据集。M-HalDetect包含了16k个细粒度的VQA示例标签，是第一个用于详细图像描述的全面多模态幻觉检测数据集。与之前只考虑对象幻觉的工作不同，我们还注释了实体描述。

    Instruction tuned Large Vision Language Models (LVLMs) have made significant advancements in generalizing across a diverse set of multimodal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a {M}ultimodal {Hal}lucination {Detect}ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained labels on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions
    
[^407]: Follow Anything: 实时开放集检测、追踪和跟随

    Follow Anything: Open-set detection, tracking, and following in real-time. (arXiv:2308.05737v1 [cs.RO])

    [http://arxiv.org/abs/2308.05737](http://arxiv.org/abs/2308.05737)

    本文提出了一个名为“跟随任何物体”的机器人系统，可以实时检测、追踪和跟随任何物体，不受训练时概念限制，并且可以通过多模态查询进行应用。通过利用大规模预训练模型的视觉描述符，该系统能够检测、分割和跟踪物体，同时考虑遮挡和物体重新出现。

    

    在工业自动化、物流和仓储、医疗保健和安全等多种机器人应用中，追踪和跟随感兴趣的物体至关重要。在本文中，我们提出了一个机器人系统，能够实时检测、追踪和跟随任何物体。我们的方法被称为“跟随任何物体”（FAn），它是一个开放词汇和多模态模型 - 不受训练时的概念限制，并且可以使用文本、图像或点击查询来应用于推断时的新类别。通过利用来自大规模预训练模型（基础模型）的丰富视觉描述符，FAn可以通过将多模态查询（文本、图像、点击）与输入图像序列进行匹配来检测和分割物体。这些检测和分割的物体在图像帧之间进行跟踪，同时考虑到遮挡和物体重新出现。我们在一个实际的机器人系统上（微型飞行器）演示了FAn，并报告了其无缝跟随感兴趣物体的能力。

    Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed ``follow anything'' (FAn), is an open-vocabulary and multimodal model -- it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of inter
    
[^408]: 从稀疏IMU感测中高效准确地进行人体姿势估计的设计空间探索

    Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing. (arXiv:2308.02397v1 [eess.SP])

    [http://arxiv.org/abs/2308.02397](http://arxiv.org/abs/2308.02397)

    本论文通过模拟情况下的设计空间探索，研究了人体姿势估计中精度和硬件资源之间的折衷，提出了利用IMU感测进行高效准确的人体姿势估计的方法。

    

    鉴于敏感的个人数据，对于评估运动、康复或工作安全中的人体运动的人体姿势估计（HPE），需要准确的感测。因此，本地处理是必要的，并且在此类系统中，有限的能源预算可以通过使用惯性测量单元（IMU）而不是常见的相机感测来解决。研究中很少讨论精度和硬件资源的有效利用之间的核心折衷。我们通过模拟情况下的设计空间探索来解决这个折衷问题，它涉及不同的IMU传感器数量和摆放位置。首先，我们使用公开可用的人体模型数据集为不同的传感器配置生成IMU数据，并用这些数据训练深度学习模型。此外，我们提出了一个组合度量来评估精度和资源利用的折衷。我们将设计空间探索用作评估传感器配置并确定特定用例的有益配置的工具。例如，对于一个同等重要性的系统...

    Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation or work safety requires accurate sensing without compromising the sensitive underlying personal data. Therefore, local processing is necessary and the limited energy budget in such systems can be addressed by Inertial Measurement Units (IMU) instead of common camera sensing. The central trade-off between accuracy and efficient use of hardware resources is rarely discussed in research. We address this trade-off by a simulative Design Space Exploration (DSE) of a varying quantity and positioning of IMU-sensors. First, we generate IMU-data from a publicly available body model dataset for different sensor configurations and train a deep learning model with this data. Additionally, we propose a combined metric to assess the accuracy-resource trade-off. We used the DSE as a tool to evaluate sensor configurations and identify beneficial ones for a specific use case. Exemplary, for a system with equal importance of ac
    
[^409]: 自适应近端梯度方法的凸优化

    Adaptive Proximal Gradient Method for Convex Optimization. (arXiv:2308.02261v1 [math.OC])

    [http://arxiv.org/abs/2308.02261](http://arxiv.org/abs/2308.02261)

    本文提出了自适应版本的梯度下降（GD）和近端梯度方法（ProxGD），通过利用局部曲率信息完全自适应。所提出的方法具有收敛性，且允许使用更大的步长。

    

    在本文中，我们探讨了凸优化中的两个基本一阶算法，即梯度下降（GD）和近端梯度方法（ProxGD）。我们的重点是通过利用平滑函数的局部曲率信息，使这些算法完全自适应。我们提出了基于观察到的梯度差异的自适应版本的GD和ProxGD，因此不会增加计算成本。此外，我们在仅假设梯度的局部Lipschitz性的情况下，证明了我们方法的收敛性。另外，所提出的版本允许使用比[MM20]最初建议的更大的步长。

    In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].
    
[^410]: 在差分隐私逻辑回归中的准确性增强：一种预训练方法

    Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach. (arXiv:2307.13771v1 [cs.LG])

    [http://arxiv.org/abs/2307.13771](http://arxiv.org/abs/2307.13771)

    本文通过添加预训练模块，在差分隐私逻辑回归中提高了模型的准确性。

    

    机器学习模型可以记忆训练数据集，因此在私有数据集上训练机器学习模型可能会侵犯个人隐私。差分隐私是一种严格的隐私保护方法，可在机器学习模型中保留底层训练数据集的隐私。然而，在差分隐私框架下训练机器学习模型通常会降低模型的准确性。本文旨在通过预训练模块提高差分隐私机器学习模型（特别是逻辑回归模型）的准确性。具体而言，我们首先在公开训练数据集上对模型进行预训练，该数据集不涉及隐私问题。然后，我们通过使用私有数据集和差分隐私逻辑回归对模型进行微调。数值结果表明，添加预训练模块显著提高了差分隐私逻辑回归的准确性。

    Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
    
[^411]: Epsilon*: 机器学习模型的隐私度量

    Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])

    [http://arxiv.org/abs/2307.11280](http://arxiv.org/abs/2307.11280)

    Epsilon*是一种用于测量机器学习模型隐私风险的新度量方法，不需要访问训练数据或模型训练算法，能与成员推断攻击中的假设检验相结合，提供对经过训练的模型实例隐私损失的下界，避免数值和噪声放大不稳定性。

    

    我们引入了Epsilon*，一种新的隐私度量方法，用于在隐私减轻策略部署之前、期间或之后，测量单个模型实例的隐私风险。该度量不需要访问训练数据采样或模型训练算法。Epsilon*是一个关于真阳性和假阳性率的函数，用于敌手在成员推断攻击中使用的假设检验中。我们区分了量化经过训练的模型实例的隐私损失和量化产生该模型实例的训练机制的隐私损失。现有的隐私审计文献中的方法为后者提供了下界，而我们的度量方法通过依赖于训练模型实例的隐私的（ε，δ）型量化，为前者提供了下界。我们建立了这些下界之间的关系，并展示了如何实现Epsilon*以避免数值和噪声放大不稳定性。

    We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further 
    
[^412]: 使用大型语言模型增强密集检索的软提示调优

    Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2307.08303](http://arxiv.org/abs/2307.08303)

    本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。

    

    密集检索（DR）将查询和文档转化为密集向量表示，并在向量空间中测量查询与文档之间的相似性。DR的一个挑战是缺乏领域特定的训练数据。虽然DR模型可以通过迁移学习从大规模公共数据集（如MS MARCO）中学习，但证据表明，并非所有DR模型和领域都能同等受益于迁移学习。最近，一些研究人员转向使用大型语言模型（LLMs）来改进零样本和少样本的DR模型。然而，这些方法中采用的硬提示或人工编写的提示无法保证生成的弱查询的质量。为了解决这个问题，我们提出了用于增强DR的软提示调优（SPTAR）：对于每个任务，我们利用软提示调优在有限的真实数据上优化任务特定的软提示，然后用这些提示引导LLMs为未标记的文档标记弱查询，从而得到足够的弱文档-查询对来训练任务特定的模型。

    Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
    
[^413]: Harpa: 高速率下的相位关联与走时神经场

    Harpa: High-Rate Phase Association with Travel Time Neural Fields. (arXiv:2307.07572v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.07572](http://arxiv.org/abs/2307.07572)

    本论文提出了一种名为Harpa的高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，即使在波速未知的情况下也能实现相位关联。这种方法能够处理较小、高速率的地震事件，提供了关于地下弹性介质属性的宝贵描述。

    

    相位关联是根据其起源地震分组地震波到达的任务。它是地震数据处理流程中的基本任务，但对于较小、高速率的地震事件来说是具有挑战性的，这些事件携带有关地震动力学的基本信息，尤其是在常常假定不准确的波速模型下。因此，大多数关联方法都专注于发生率较低且容易关联的较大事件，尽管微地震活动提供了井下弹性介质属性的宝贵描述。在本文中，我们展示了即使在波速未知的情况下，也可以以比以前报告的更高的速率进行关联。我们提出了Harpa，这是一种高速率地震相位关联方法，利用深度神经场构建波速和相关走时的生成模型，并首先解决联合时空源定位和波速恢复问题，然后进行相位关联。

    Phase association groups seismic wave arrivals according to their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by ass
    
[^414]: 安全强化学习作为Wasserstein变分推理：可解释性的形式方法

    Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])

    [http://arxiv.org/abs/2307.07084](http://arxiv.org/abs/2307.07084)

    本研究提出了一种新的自适应Wasserstein变分优化（AWaVO）方法，利用形式方法解决了顺序决策中的解释和透明性问题，并提供了奖励设计和策略收敛的概率解释。

    

    强化学习或最优控制可以为具有可变动态的顺序决策问题提供有效的推理。然而，在实际实施中，解释奖励函数和相应的最优策略一直是一个持久的挑战。因此，将顺序决策问题形式化为推理具有重要价值，因为概率推理原则上提供了多样且强大的数学工具来推断随机动态，同时提供了奖励设计和策略收敛的概率解释。在本研究中，我们提出了一种新颖的自适应Wasserstein变分优化（AWaVO）方法来解决这些顺序决策中的挑战。我们的方法利用形式方法来解释奖励设计，透明地训练收敛，以及对顺序决策的概率解释。为了证明实用性，我们展示了收敛训练并保证了收敛的训练。

    Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
    
[^415]: 准确和校准模型的集合学习

    Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02245](http://arxiv.org/abs/2307.02245)

    提出了一种集合学习方法(OKO)来解决机器学习中的模型过度自信和校准不良问题，通过最小化集合的交叉熵误差，从而提高准确性和校准效果，并在有限的训练数据和类别不平衡情况下表现出更好的结果。

    

    模型过度自信和校准不良在机器学习中很常见，并且在应用标准经验风险最小化时很难解决。在这项工作中，我们提出了一种新的方法来缓解这些问题，我们称之为奇数-$k$-去除学习（OKO），它通过最小化集合的交叉熵误差而不是单个示例的误差来实现。这自然地使模型能够捕捉数据示例之间的相关性，并在有限的训练数据和类别不平衡的情况下实现更好的准确性和校准。令人惊讶的是，即使使用硬标签进行训练并且不进行任何额外的校准参数调整，如温度缩放，OKO通常也能获得更好的校准效果。我们提供了理论上的证明，证明OKO自然地能够获得更好的校准效果，并进行了广泛的实验分析以验证我们的理论发现。我们强调，OKO是一个通用的框架，可以很容易地适应许多不同的情境。

    Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the
    
[^416]: 机器学习需要自己的随机标准：随机平滑和基于PRNG的攻击。

    Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (arXiv:2306.14043v1 [cs.LG])

    [http://arxiv.org/abs/2306.14043](http://arxiv.org/abs/2306.14043)

    本文研究了机器学习中随机性可被攻击者利用的问题，主要关注流行的随机平滑方法。该方法用于训练可证明鲁棒性的模型和量化不确定性，但其随机性可能导致系统被攻击。

    

    随机性支持机器学习中的许多关键功能，包括优化、数据选择、隐私和安全。机器学习系统将生成或收集随机性的任务外包给了编译器、云服务提供商或工具链中的其他地方。但是，攻击者利用不良随机性甚至创建随机性的历史悠久，就像NSA放置后门在随机数生成器中以破解加密一样。本文考虑是否能够仅利用攻击者通常依赖的随机性来危害机器学习系统。我们将重点放在随机平滑上，这是一种流行的方法，用于训练可证明鲁棒性的模型，并为任意模型的特定输入数据点提供认证。我们选择随机平滑是因为它用于安全和安全（用于对抗对抗性示例和量化不确定性，分别）。在幕后，它依赖于采样高斯噪声来探索围绕数据点的体积。

    Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data poin
    
[^417]: 通过更高级任务相似性加强图上的多任务学习

    Boosting Multitask Learning on Graphs through Higher-Order Task Affinities. (arXiv:2306.14009v1 [cs.LG])

    [http://arxiv.org/abs/2306.14009](http://arxiv.org/abs/2306.14009)

    本文从多任务学习的角度重新审视在给定图上预测节点标签的问题，提出通过更高级任务相似性来加强多任务学习，并开发了一种算法来将任务分组以应对负迁移问题。

    

    在给定图上预测节点标签是一个被广泛研究的问题，有许多应用，包括社区检测和分子图预测。本文从多任务学习的角度重新审视此问题，考虑同时在图上预测多个节点标签函数。为了具体说明，考虑重叠社区检测：每个社区成员身份是一个二进制节点分类任务。由于复杂的重叠模式，当我们将多个社区检测应用到naive多任务学习时，我们发现负迁移很普遍，因为不同的节点标签之间的任务关系高度非线性。为了解决这个挑战，我们开发了一种算法，基于更高级的任务相似性度量来将任务分组。然后我们在每个任务组上拟合多任务模型，产生在基线模型上的增强过程。我们将两个任务之间的更高级的任务相似性度量估计为预测损失。

    Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss o
    
[^418]: 理解量子机器学习需要重新思考泛化问题

    Understanding quantum machine learning also requires rethinking generalization. (arXiv:2306.13461v1 [quant-ph])

    [http://arxiv.org/abs/2306.13461](http://arxiv.org/abs/2306.13461)

    本文通过实验认为，传统方法无法解释量子机器学习模型在只使用少量数据训练的情况下表现出成功的泛化性能，该模型可以准确拟合随机状态及随机标记的训练数据，这种记忆随机数据的能力违反了当前小泛化误差的概念，我们通过理论构建补充实证结果，表明量子神经网络可将任意标记拟合到量子状态上，暗示了它们的记忆能力，这些结果排除了单单基于经典复杂度度量的所有可能保证。

    

    量子机器学习模型在只用少量数据训练的情况下也能表现出成功的泛化性能。本文通过系统的随机化实验，展示传统的理解泛化的方法无法解释这些量子模型的行为。我们的实验揭示了最先进的量子神经网络能够准确地拟合随机状态和随机训练数据的标记。这种记忆随机数据的能力违反了当前小泛化误差的概念，使得建立在VC维、Rademacher复杂度和所有均匀相关性度量基础上的方法有些棘手。我们还通过理论构建补充了我们的实证结果，表明量子神经网络能够将任意标记拟合到量子状态上，暗示了它们的记忆能力。我们的结果并不排除只用少量训练数据就能获得良好泛化的可能性，但是排除了单单基于经典复杂度度量的所有可能保证。

    Quantum machine learning models have shown successful generalization performance even when trained with few data. In this work, through systematic randomization experiments, we show that traditional approaches to understanding generalization fail to explain the behavior of such quantum models. Our experiments reveal that state-of-the-art quantum neural networks accurately fit random states and random labeling of training data. This ability to memorize random data defies current notions of small generalization error, problematizing approaches that build on complexity measures such as the VC dimension, the Rademacher complexity, and all their uniform relatives. We complement our empirical results with a theoretical construction showing that quantum neural networks can fit arbitrary labels to quantum states, hinting at their memorization ability. Our results do not preclude the possibility of good generalization with few training data but rather rule out any possible guarantees based only
    
[^419]: 从新的角度压缩ImageNet规模数据集：SRe$^2$L

    Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])

    [http://arxiv.org/abs/2306.13092](http://arxiv.org/abs/2306.13092)

    SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。

    

    我们提出了一个新的数据集压缩框架，称为Squeeze、Recover和Relabel（SRe$^2$L），它在训练期间分离了模型和合成数据的双层优化，以处理不同规模的数据集、模型体系结构和图像分辨率，从而实现有效的数据集压缩。所提出的方法展示了在不同数据集规模上的灵活性，并在合成图像任意分辨率、高分辨率训练的情况下具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力。我们在Tiny-ImageNet和完整的ImageNet-1K数据集上进行了广泛的实验。在50IPC下，我们的方法在Tiny-ImageNet和ImageNet-1K上分别实现了42.5％和60.8％的最高验证精度，较之前所有最先进方法提高了14.5％和32.9％。我们的方法在Res上也比MTT快约52倍(ConvNet-4)和16倍。

    We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
    
[^420]: 带有沉重尾部SGD训练的过参数化神经网络的隐式可压缩性

    Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD. (arXiv:2306.08125v1 [stat.ML])

    [http://arxiv.org/abs/2306.08125](http://arxiv.org/abs/2306.08125)

    本研究提出了一种简单的SGD修改方法，使训练出的神经网络输出可被证明为可压缩，而不需要任何非平凡假设。

    

    由于减少计算需求和压缩与泛化误差之间的显式关系，神经网络压缩成为越来越重要的研究对象。最近的研究表明，随机梯度下降(SGD)的超参数选择可以影响学习参数向量的压缩性。虽然这些结果揭示了训练动态对压缩性的影响，但是它们依赖于不可验证的假设，由于隐含性质，得出的理论并没有提供实用的指导方针。在本研究中，我们提出了一种简单的SGD修改方法，使得算法的输出能够被证明是可压缩的，而不需要任何非平凡假设。我们考虑了一个使用SGD训练的单隐藏层神经网络，并在每次迭代中注入附加的沉重尾部噪声。

    Neural network compression has been an increasingly important subject, due to its practical implications in terms of reducing the computational requirements and its theoretical implications, as there is an explicit connection between compressibility and the generalization error. Recent studies have shown that the choice of the hyperparameters of stochastic gradient descent (SGD) can have an effect on the compressibility of the learned parameter vector. Even though these results have shed some light on the role of the training dynamics over compressibility, they relied on unverifiable assumptions and the resulting theory does not provide a practical guideline due to its implicitness. In this study, we propose a simple modification for SGD, such that the outputs of the algorithm will be provably compressible without making any nontrivial assumptions. We consider a one-hidden-layer neural network trained with SGD and we inject additive heavy-tailed noise to the iterates at each iteration.
    
[^421]: SGD的精确平均二次线性稳定性分析

    Exact Mean Square Linear Stability Analysis for SGD. (arXiv:2306.07850v1 [cs.LG])

    [http://arxiv.org/abs/2306.07850](http://arxiv.org/abs/2306.07850)

    本文提供了SGD稳定性的精确阈值表达式，发现其与批量大小之间呈单调非降关系，进一步展示了减小批量大小可能会影响SGD的稳定性。

    

    近来，优化方法在损失函数极小值点附近的动态稳定性引起了极大关注。对于梯度下降法（GD），稳定的收敛仅可能发生在足够平坦的极小值处，并且已经与训练模型的良好性能联系在一起。但是，尽管GD的稳定性阈值已经众所周知，但迄今为止，尚未推导出随机梯度下降（SGD）的精确阈值的显式表达式。本文提供了这样一种封闭形式的表达式。具体而言，我们提供了一个关于步长$\eta$的显式条件，既是SGD在均方意义下稳定的必要条件，也是充分条件。我们的分析揭示了批量大小$B$的精确作用，特别的，我们展示出稳定性阈值是批量大小的单调非降函数，这意味着减小批量大小只会降低稳定性。此外，我们还展示了SGD的稳定性阈值。

    The dynamical stability of optimization methods at the vicinity of minima of the loss has recently attracted significant attention. For gradient descent (GD), stable convergence is possible only to minima that are sufficiently flat w.r.t. the step size, and those have been linked with favorable properties of the trained model. However, while the stability threshold of GD is well-known, to date, no explicit expression has been derived for the exact threshold of stochastic GD (SGD). In this paper, we derive such a closed-form expression. Specifically, we provide an explicit condition on the step size $\eta$ that is both necessary and sufficient for the stability of SGD in the mean square sense. Our analysis sheds light on the precise role of the batch size $B$. Particularly, we show that the stability threshold is a monotonically non-decreasing function of the batch size, which means that reducing the batch size can only hurt stability. Furthermore, we show that SGD's stability threshold
    
[^422]: 通过对抗核近似实现鲁棒强化学习

    Robust Reinforcement Learning via Adversarial Kernel Approximation. (arXiv:2306.05859v1 [cs.LG])

    [http://arxiv.org/abs/2306.05859](http://arxiv.org/abs/2306.05859)

    该论文提出了一种新奇的在线鲁棒强化学习方法，它通过近似对抗核并使用标准非鲁棒强化学习算法学习鲁棒策略，可应用于任何基础的强化学习算法之上，可以轻松扩展到高维度域。

    

    鲁棒性马尔可夫决策过程提供了一个框架，可以在转移核发生扰动的情况下进行序列决策。然而，在具有高维度域的现实在线环境中，鲁棒强化学习方法并不容易扩展。通过表征鲁棒性马尔可夫决策过程中的对抗核，我们提出一种新的在线鲁棒强化学习方法，它近似对抗核并使用标准（非鲁棒）的强化学习算法来学习一个鲁棒性方针。值得注意的是，我们的方法可以应用于任何基础的强化学习算法之上，可以实现对高维度域的轻松扩展。在经典控制任务、MinAtar和DeepMind控制套件中的实验中，我们的方法表现出了有效性和适用性。

    Robust Markov Decision Processes (RMDPs) provide a framework for sequential decision-making that is robust to perturbations on the transition kernel. However, robust reinforcement learning (RL) approaches in RMDPs do not scale well to realistic online settings with high-dimensional domains. By characterizing the adversarial kernel in RMDPs, we propose a novel approach for online robust RL that approximates the adversarial kernel and uses a standard (non-robust) RL algorithm to learn a robust policy. Notably, our approach can be applied on top of any underlying RL algorithm, enabling easy scaling to high-dimensional domains. Experiments in classic control tasks, MinAtar and DeepMind Control Suite demonstrate the effectiveness and the applicability of our method.
    
[^423]: 基于停车场占用检测的深度学习方法的改进

    Revising deep learning methods in parking lot occupancy detection. (arXiv:2306.04288v1 [cs.LG])

    [http://arxiv.org/abs/2306.04288](http://arxiv.org/abs/2306.04288)

    本文提出了一种基于EfficientNet架构的停车位占用检测算法，并在5个不同的数据集上进行了评估，性能得到提高。

    

    停车场引导系统作为智能城市发展的一部分已经成为一个流行的趋势。这些系统的关键部分是允许驾驶员在感兴趣的区域搜索可用停车位的算法。传统的方法是基于神经网络分类器应用于摄像头记录。然而，现有的系统在特定的视觉条件下缺乏泛化能力和适当的测试。在本研究中，我们广泛评估了最先进的停车位占用检测算法，将它们的预测质量与最近出现的视觉Transformer进行比较，并基于EfficientNet架构提出了一个新的管道。进行的计算实验已经证明，在我们的模型的情况下，性能有了提高，该模型在5种不同数据集上进行了评估。

    Parking guidance systems have recently become a popular trend as a part of the smart cities' paradigm of development. The crucial part of such systems is the algorithm allowing drivers to search for available parking lots across regions of interest. The classic approach to this task is based on the application of neural network classifiers to camera records. However, existing systems demonstrate a lack of generalization ability and appropriate testing regarding specific visual conditions. In this study, we extensively evaluate state-of-the-art parking lot occupancy detection algorithms, compare their prediction quality with the recently emerged vision transformers, and propose a new pipeline based on EfficientNet architecture. Performed computational experiments have demonstrated the performance increase in the case of our model, which was evaluated on 5 different datasets.
    
[^424]: MESSY估计：基于最大熵的随机和符号密度估计

    MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation. (arXiv:2306.04120v1 [cs.LG])

    [http://arxiv.org/abs/2306.04120](http://arxiv.org/abs/2306.04120)

    MESSY估计方法是一种基于最大熵的随机和符号密度估计方法，通过构建基于梯度的漂移扩散过程来高效地找到最大熵分布的参数，支持高维问题，并具有优于现有最新方法的有效性和普适性。

    

    我们引入了基于最大熵的随机和符号密度估计方法MESSY。所提出的方法使用梯度流的矩将概率密度函数从样本中恢复为符号表达式，并将ansatz作为驱动力。特别地，我们构建了一个基于梯度的漂移扩散过程，将未知分布函数的样本与猜测的符号表达式相连。然后，我们展示出当猜测分布具有最大熵形式时，可以通过使用提供的样本的矩构建的线性方程组高效地找到该分布的参数。此外，我们使用符号回归来探索平滑函数的空间，并找到导致最大熵泛函指数的最优基函数，以获得良好条件。该方法在随机搜索的每次迭代中的成本与样本数量呈线性关系，与变量数量呈二次关系，使其可扩展到高维问题。数值实验显示出所提出方法的有效性和普适性，与现有的最新方法相比。

    We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number 
    
[^425]: 初始猜测偏差：未经过训练的神经网络倾向于某些类别

    Initial Guessing Bias: How Untrained Networks Favor Some Classes. (arXiv:2306.00809v1 [cs.LG])

    [http://arxiv.org/abs/2306.00809](http://arxiv.org/abs/2306.00809)

    本文提出了“初始猜测偏差”现象，即在未经过训练的神经网络中，由于架构选择的影响，模型往往会将所有预测指向同一个类别。该现象对架构选择和初始化有实际指导意义，并具有理论后果，例如节点置换对称性的崩溃和深度带来的非平凡差异。

    

    神经网络的初始状态在调节后续的训练过程中扮演重要角色。在分类问题的背景下，我们提供了理论分析，证明神经网络的结构可以在训练之前，甚至在不存在显式偏差的情况下，使模型将所有预测都指向同一个类别。我们展示了这种现象的存在，称为“初始猜测偏差”（Initial Guessing Bias，IGB），这取决于架构选择，例如激活函数、最大池化层和网络深度。我们对IGB进行的分析具有实际意义，可以指导架构的选择和初始化。我们还强调理论后果，例如节点置换对称性的崩溃、自平均的破坏、某些均场近似的有效性以及深度带来的非平凡差异。

    The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call "Initial Guessing Bias" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth.
    
[^426]: 从概率角度构建语义感知的对抗样本

    Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective. (arXiv:2306.00353v1 [stat.ML])

    [http://arxiv.org/abs/2306.00353](http://arxiv.org/abs/2306.00353)

    本研究提出了一个基于概率视角的对抗样本构建方法，可以生成语义感知的对抗性样本，并可以有效规避传统对抗性攻击的强化对抗训练方法。

    

    本研究提出了一种新颖的概率视角对抗样本构建方法——箱约束 Langevin Monte Carlo（LMC）。从这个角度出发，我们开发了一种创新性的方法，以原则性的方式生成语义感知的对抗性样本。这种方法超越了几何距离所施加的限制，选择了语义约束。我们的方法赋予了个体将其对语义的理解融入到模型中的能力。通过人类评估，我们验证了我们的语义感知的对抗样本保持其固有的含义。在 MNIST 和 SVHN 数据集上的实验结果表明，我们的语义感知的对抗样本可以有效地规避针对传统对抗性攻击的强健性对抗训练方法。

    In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks.
    
[^427]: Shuffle SGD总是比SGD更好：对具有任意数据顺序的SGD进行改进分析

    Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders. (arXiv:2305.19259v1 [cs.LG])

    [http://arxiv.org/abs/2305.19259](http://arxiv.org/abs/2305.19259)

    本论文研究了一种允许任意数据排序的普通SGD算法,并表明在非凸函数情况下，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。

    

    随机梯度下降（SGD）算法被广泛用于优化神经网络，随机重排（RR）和单次洗牌（SS）是通过循环遍历训练数据的随机或单个排列的常见选择，然而这些算法在非凸情况下的收敛性质尚未完全理解。现有结果表明，在实际的训练场景中，当时代的数量小于训练集大小时，RR可能表现不如SGD。本文分析了一种允许任意数据排序的普通SGD算法，并展示了在非凸函数情况下的改进收敛速度。具体而言，我们的分析表明，随机和单次洗牌的SGD比经典替换的SGD更快或至少与其一样好，无论迭代次数如何。总的来说，我们的研究凸显了使用随机/单次洗牌的SGD的好处，并为其非凸收敛性质提供了新的见解。

    Stochastic Gradient Descent (SGD) algorithms are widely used in optimizing neural networks, with Random Reshuffling (RR) and Single Shuffle (SS) being popular choices for cycling through random or single permutations of the training data. However, the convergence properties of these algorithms in the non-convex case are not fully understood. Existing results suggest that, in realistic training scenarios where the number of epochs is smaller than the training set size, RR may perform worse than SGD.  In this paper, we analyze a general SGD algorithm that allows for arbitrary data orderings and show improved convergence rates for non-convex functions. Specifically, our analysis reveals that SGD with random and single shuffling is always faster or at least as good as classical SGD with replacement, regardless of the number of iterations. Overall, our study highlights the benefits of using SGD with random/single shuffling and provides new insights into its convergence properties for non-co
    
[^428]: BadLabel: 评估与增强标签噪声学习的鲁棒性视角

    BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning. (arXiv:2305.18377v1 [cs.LG])

    [http://arxiv.org/abs/2305.18377](http://arxiv.org/abs/2305.18377)

    本文介绍了一种新的标签噪声类型BadLabel，它通过标签翻转攻击显著降低现有标签噪声学习（LNL）算法性能，因此提出了一种鲁棒的LNL方法，表现出在六个数据集上最先进的性能。

    

    标签噪声学习旨在提高模型对带有噪声标签的训练数据的泛化能力。为了推进实用的标签噪声学习算法，研究人员提出了不同的标签噪声类型，从条件类噪声到实例依赖噪声不等。本文介绍了一种新的标签噪声类型BadLabel，可以通过标签翻转攻击显著降低现有LNL算法的性能。BadLabel基于标准分类的标签翻转攻击进行制作，选择特定样本并将其标签翻转为其他标签，使得干净标签和噪声标签的损失值变得无法区分。为了解决BadLabel带来的挑战，我们进一步提出了一种鲁棒的LNL方法，每个epoch对标签进行敌对扰动，使干净标签和噪声标签的损失值再次可区分。一旦选择了一小部分（大多数）干净标记数据，我们可以将半监督翻译技术应用到LNL中。我们在合成和真实数据集上评估了BadLabel类型和提出的鲁棒LNL方法。实验表明，在六个数据集上，BadLabel可以将七个现有LNL算法的性能降低高达60％。此外，我们提出的方法在合成和真实数据集上均表现出最先进的性能，比六个数据集上的七个现有LNL算法性能提高多达17％。

    Label-noise learning (LNL) aims to increase the model's generalization given training data with noisy labels. To facilitate practical LNL algorithms, researchers have proposed different label noise types, ranging from class-conditional to instance-dependent noises. In this paper, we introduce a novel label noise type called BadLabel, which can significantly degrade the performance of existing LNL algorithms by a large margin. BadLabel is crafted based on the label-flipping attack against standard classification, where specific samples are selected and their labels are flipped to other labels so that the loss values of clean and noisy labels become indistinguishable. To address the challenge posed by BadLabel, we further propose a robust LNL method that perturbs the labels in an adversarial manner at each epoch to make the loss values of clean and noisy labels again distinguishable. Once we select a small set of (mostly) clean labeled data, we can apply the techniques of semi-supervised
    
[^429]: 流匹配方法的误差界限

    Error Bounds for Flow Matching Methods. (arXiv:2305.16860v1 [stat.ML])

    [http://arxiv.org/abs/2305.16860](http://arxiv.org/abs/2305.16860)

    本文提出了基于ODE的流匹配方法的误差界限，适用于完全确定性抽样，需要满足$L^2$近似误差范围的规律性条件和数据分布。

    

    基于分数的生成模型是一类依赖于随机微分方程（SDE）的流行生成建模技术。自从它们诞生以来，就已经意识到可以使用普通微分方程（ODE）而不是SDE进行生成。这导致介绍了概率流ODE方法和去噪扩散隐式模型。流匹配方法最近进一步扩展了这些基于ODE的方法，并近似于两个任意概率分布之间的流。以前的工作针对随机抽样模式下的扩散模型推导了近似误差的边界，假设$L^2$损失具有某些限制。我们在完全确定性抽样的情况下提供了流匹配过程的误差界限，假设$L^2$近似误差范围有一定的规律性条件和数据分布。

    Score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDE). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODE) rather than SDE. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE-based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the $L^2$ loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an $L^2$ bound on the approximation error and a certain regularity condition on the data distributions.
    
[^430]: Dropout可以缓解双下降现象的研究

    Dropout Drops Double Descent. (arXiv:2305.16179v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16179](http://arxiv.org/abs/2305.16179)

    本研究发现通过在全连接线性层之前添加一个dropout层，可以缓解双下降现象，从而提高模型的预测准确性。

    

    本研究发现并分析，通过在全连接线性层之前添加一个dropout层，可以轻松地缓解双下降现象。双下降现象在近年来引起了公众的关注，即随着样本或模型规模的增加，预测误差会先上升再下降。本文从理论和实证两个方面证明，通过在线性回归模型和非线性随机特征回归中使用最佳的dropout，可以缓解这些现象。

    In this paper, we find and analyze that we can easily drop the double descent by only adding one dropout layer before the fully-connected linear layer. The surprising double-descent phenomenon has drawn public attention in recent years, making the prediction error rise and drop as we increase either sample or model size. The current paper shows that it is possible to alleviate these phenomena by using optimal dropout in the linear regression model and the nonlinear random feature regression, both theoretically and empirically. % ${y}=X{\beta}^0+{\epsilon}$ with $X\in\mathbb{R}^{n\times p}$. We obtain the optimal dropout hyperparameter by estimating the ground truth ${\beta}^0$ with generalized ridge typed estimator $\hat{{\beta}}=(X^TX+\alpha\cdot\mathrm{diag}(X^TX))^{-1}X^T{y}$. Moreover, we empirically show that optimal dropout can achieve a monotonic test error curve in nonlinear neural networks using Fashion-MNIST and CIFAR-10. Our results suggest considering dropout for risk curve
    
[^431]: 民主扩散语言模型

    Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])

    [http://arxiv.org/abs/2305.10818](http://arxiv.org/abs/2305.10818)

    本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。

    

    尽管扩散模型在自然语言处理中有潜在好处，但目前公开的实现、训练模型或可重现的训练程序并不存在。为解决这些挑战，我们提出了基于CDCD框架的民主扩散语言模型（DDLM）。我们提出了一种用C4数据集简化的DDLM训练流程，并对训练模型的行为进行了深入分析。此外，我们引入了一种用于速度更快的采样的新型早期退出策略，该策略针对使用得分插值训练的模型。由于此前没有研究旨在使用预训练扩散LM解决下游任务（例如分类任务），我们在GLUE基准上进行了实验，以研究DDLM的知识转移能力。通过本文，我们提出了可供其他研究人员使用的DDLM训练和评估流程以及预先训练的DDLM模型，这些模型可在未来的D相关的研究中使用。

    Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
    
[^432]: 探索拉绍蒙集合中特征交互得分的云

    Exploring the cloud of feature interaction scores in a Rashomon set. (arXiv:2305.10181v1 [cs.LG])

    [http://arxiv.org/abs/2305.10181](http://arxiv.org/abs/2305.10181)

    本文通过探索拉绍蒙集合中准确性类似的模型，引入了特征交互分数（FIS）来检测特征的相互作用。相较于从单个预先指定的模型中提取特征交互，本文提供了更为可靠的方式。

    

    特征间的相互作用是理解机器学习模型行为的核心。最近的研究在单个预测模型中检测和量化特征交互方面取得了重大进展。然而，我们认为从单个预先指定的模型中提取的特征交互可能不可信，因为：一个训练良好的预测模型可能不会保留真实的特征交互，存在多个在特征交互强度方面存在差异但准确性相近的预测模型。因此，我们建议在准确性近似相等的预测模型类中探索特征交互强度。在这项工作中，我们在拉绍蒙集合的上下文中引入了特征交互分数（FIS），表示在给定任务上实现类似准确性的模型集合。我们提出了一种通用且实用的算法来计算模型类中的FIS。我们通过合成数据展示了FIS的属性，并建立了与单个模型中提取的交互特征的联系。

    Interactions among features are central to understanding the behavior of machine learning models. Recent research has made significant strides in detecting and quantifying feature interactions in single predictive models. However, we argue that the feature interactions extracted from a single pre-specified model may not be trustworthy since: a well-trained predictive model may not preserve the true feature interactions and there exist multiple well-performing predictive models that differ in feature interaction strengths. Thus, we recommend exploring feature interaction strengths in a model class of approximately equally accurate predictive models. In this work, we introduce the feature interaction score (FIS) in the context of a Rashomon set, representing a collection of models that achieve similar accuracy on a given task. We propose a general and practical algorithm to calculate the FIS in the model class. We demonstrate the properties of the FIS via synthetic data and draw connecti
    
[^433]: 小型语言模型更适合作为黑匣子机器生成文本检测器

    Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])

    [http://arxiv.org/abs/2305.09859](http://arxiv.org/abs/2305.09859)

    本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。

    

    随着流畅的生成语言模型的出现，它们可以生成与人类写作的非常相似的令人信服的话语，因此区分一段文本是由机器生成的还是人类写作的变得更加具有挑战性和重要性，因为这样的模型可以用于传播错误信息、虚假新闻、虚假评论并模仿某些作者和人物。为此，已经提出了许多检测机器生成文本的方法。其中大部分方法需要访问目标模型的 logits，或需要可以从目标模型中进行采样的能力。其中一种黑匣子检测方法依赖于观察到生成文本在生成器的似然函数下是局部最优的，而人类写作的文本则不是。我们发现，总体而言，较小且部分训练的模型更适合作为通用文本检测器：它们可以更精确地检测来自小型和大型模型的生成文本。有趣的是，我们发现检测器和生成模型是否具有相同的架构或相同的语料库对检测性能没有显著影响。

    With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
    
[^434]: 空间天气研究中的太阳活动区磁场图像数据集

    Solar Active Region Magnetogram Image Dataset for Studies of Space Weather. (arXiv:2305.09492v1 [astro-ph.SR])

    [http://arxiv.org/abs/2305.09492](http://arxiv.org/abs/2305.09492)

    本数据集提供了一系列太阳活动区磁图，并提供相应的太阳耀斑标签。它可用于研究磁结构、其演化以及太阳耀斑的关系，并对于自动太阳耀斑预测方法的研究具有重要价值。

    

    本数据集提供了美国国家航空航天局（NASA）太阳动力学观测卫星（SDO）的磁图（衡量磁场强度的图像）的全面收集。该数据集包含来自三个来源的数据，提供SDO地震学和磁学仪（HMI）太阳活跃区（大磁通区域，通常是爆发事件的源头）的磁图以及相应耀斑活动的标签。该数据集对于研究磁结构、其随时间的演化以及与太阳耀斑的关系的图像分析或太阳物理学研究将非常有用。该数据集将对那些研究自动太阳耀斑预测方法的研究人员产生兴趣，包括监督和无监督的机器学习（经典和深度）、二元和多类分类以及回归。该数据集是一个最小处理且用户可配置的一致大小太阳图像数据集。

    In this dataset we provide a comprehensive collection of magnetograms (images quantifying the strength of the magnetic field) from the National Aeronautics and Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The dataset incorporates data from three sources and provides SDO Helioseismic and Magnetic Imager (HMI) magnetograms of solar active regions (regions of large magnetic flux, generally the source of eruptive events) as well as labels of corresponding flaring activity. This dataset will be useful for image analysis or solar physics research related to magnetic structure, its evolution over time, and its relation to solar flares. The dataset will be of interest to those researchers investigating automated solar flare prediction methods, including supervised and unsupervised machine learning (classical and deep), binary and multi-class classification, and regression. This dataset is a minimally processed, user configurable dataset of consistently sized images of sola
    
[^435]: 物理信息化的Token Transformer

    Physics Informed Token Transformer. (arXiv:2305.08757v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.08757](http://arxiv.org/abs/2305.08757)

    本研究提出了一种名为PITT的物理信息化的Token Transformer模型，通过将偏微分方程嵌入学习过程中，使得模型能够融入物理知识，并在多个PDE应用中展现出性能和优势。

    

    解决偏微分方程（PDEs）是许多科学和工程领域的核心。虽然传统方法往往速度慢，但机器学习模型却往往无法完整地融入系统信息。在过去几年中，Transformer对人工智能领域产生了重大影响，并在PDE应用中得到了广泛使用。然而，尽管它们取得了成功，但目前Transformer缺乏与物理和推理的整合。本研究旨在通过引入PITT：物理信息化的Token Transformer来解决这个问题。PITT的目的是通过将偏微分方程（PDEs）嵌入学习过程中来融入物理知识。PITT使用方程标记化方法来学习分析驱动的数值更新运算符。通过标记化PDEs和嵌入偏导数，Transformer模型可以意识到物理过程的基本知识。为了证明这一点，研究通过实验证明了PITT在多个PDE应用中的性能和优势。

    Solving Partial Differential Equations (PDEs) is the core of many fields of science and engineering. While classical approaches are often prohibitively slow, machine learning models often fail to incorporate complete system information. Over the past few years, transformers have had a significant impact on the field of Artificial Intelligence and have seen increased usage in PDE applications. However, despite their success, transformers currently lack integration with physics and reasoning. This study aims to address this issue by introducing PITT: Physics Informed Token Transformer. The purpose of PITT is to incorporate the knowledge of physics by embedding partial differential equations (PDEs) into the learning process. PITT uses an equation tokenization method to learn an analytically-driven numerical update operator. By tokenizing PDEs and embedding partial derivatives, the transformer models become aware of the underlying knowledge behind physical processes. To demonstrate this, P
    
[^436]: 超流体的神经波函数研究

    Neural Wave Functions for Superfluids. (arXiv:2305.06989v1 [cond-mat.quant-gas])

    [http://arxiv.org/abs/2305.06989](http://arxiv.org/abs/2305.06989)

    本论文利用费米神经网络波函数方法研究了均匀费米气体超流，提出一种针对FermiNet模型的改进方法，获得了极其准确的结果。

    

    理解超流性仍然是凝聚态物理的一个主要目标。在这里，我们利用最近开发的费米神经网络（FermiNet）波函数Ansatz进行变分蒙特卡洛计算来解决这一挑战。我们研究了一个具有强烈短程双体相互作用的系统-- 均匀费米气体，该系统已知存在超流基态，但难以定量描述。我们展示了在研究均匀费米气体时FermiNet Ansatz的关键局限性，并提出了一种简单的修改，其表现显著优于原始FermiNet，可以给出高度准确的结果。我们数学证明了新的Ansatz是原始FermiNet体系结构的严格概括，尽管使用的参数更少。我们的方法与FermiNet共享几个优势:使用神经网络消除了底层基组的需求;网络的灵活性在变分量子Monte Carlo中产生了极其准确的结果。

    Understanding superfluidity remains a major goal of condensed matter physics. Here we tackle this challenge utilizing the recently developed Fermionic neural network (FermiNet) wave function Ansatz for variational Monte Carlo calculations. We study the unitary Fermi gas, a system with strong, short-range, two-body interactions known to possess a superfluid ground state but difficult to describe quantitively. We demonstrate key limitations of the FermiNet Ansatz in studying the unitary Fermi gas and propose a simple modification that outperforms the original FermiNet significantly, giving highly accurate results. We prove mathematically that the new Ansatz is a strict generalization of the original FermiNet architecture, despite the use of fewer parameters. Our approach shares several advantanges with the FermiNet: the use of a neural network removes the need for an underlying basis set; and the flexiblity of the network yields extremely accurate results within a variational quantum Mon
    
[^437]: StyleLipSync：基于风格的个性化唇形动画视频生成

    StyleLipSync: Style-based Personalized Lip-sync Video Generation. (arXiv:2305.00521v1 [cs.CV])

    [http://arxiv.org/abs/2305.00521](http://arxiv.org/abs/2305.00521)

    本文提出了一种基于风格的个性化唇形动画视频生成模型，可以准确地生成任意身份的唇形同步视频，且可用于增强未见面孔的特征。

    

    本文提出了StyleLipSync，这是一种基于风格的个性化唇形动画视频生成模型，它可以从任意音频生成无关身份的唇形同步视频。为了生成任意身份的视频，我们利用了预培训的StyleGAN的语义丰富潜空间中的表达性唇部先验知识，并通过线性变换设计视频一致性。与以往的唇形同步方法不同，我们引入了姿态感知遮罩，通过逐帧利用三维参数化网格预测器动态定位遮罩，提高了帧间自然度。此外，我们还提出了一种几乎不需要数据的唇形同步适应方法，通过引入同步正则化器来保留唇形同步泛化能力，同时增强人物特定的视觉信息。广泛的实验表明，我们的模型可以生成准确的唇形同步视频，甚至可以在零样本设置下增强未见面孔的特征。

    In this paper, we present StyleLipSync, a style-based personalized lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from arbitrary audio. To generate a video of arbitrary identities, we leverage expressive lip prior from the semantically rich latent space of a pre-trained StyleGAN, where we can also design a video consistency with a linear transformation. In contrast to the previous lip-sync methods, we introduce pose-aware masking that dynamically locates the mask to improve the naturalness over frames by utilizing a 3D parametric mesh predictor frame by frame. Moreover, we propose a few-shot lip-sync adaptation method for an arbitrary person by introducing a sync regularizer that preserves lips-sync generalization while enhancing the person-specific visual information. Extensive experiments demonstrate that our model can generate accurate lip-sync videos even with the zero-shot setting and enhance characteristics of an unseen face using a fe
    
[^438]: 多保真度方法应用于物理系统的持续学习

    A multifidelity approach to continual learning for physical systems. (arXiv:2304.03894v1 [math.NA])

    [http://arxiv.org/abs/2304.03894](http://arxiv.org/abs/2304.03894)

    这篇论文介绍了一种基于多保真深度神经网络的新型持续学习方法，能限制灾难性遗忘，特别适用于满足相同物理定律或具备物理学先验知识的神经网络等物理问题。

    

    我们介绍了一种基于多保真深度神经网络的新型持续学习方法。该方法学习前几个模型输出结果和当前训练数据集上期望输出结果的相关性，从而限制灾难性遗忘。多保真持续学习方法本身表现出鲁棒结果，可以限制多个数据集的遗忘。此外，我们还展示了多保真方法可以与现有的持续学习方法（包括重放和记忆感知突触）相结合，以进一步限制灾难性遗忘。该持续学习方法特别适用于满足每个领域上相同物理定律或具备物理学先验知识的神经网络等物理问题，因为在这些情况下，我们预计前一个模型的输出结果与当前训练领域中的模型之间存在强相关性。

    We introduce a novel continual learning method based on multifidelity deep neural networks. This method learns the correlation between the output of previously trained models and the desired output of the model on the current training dataset, limiting catastrophic forgetting. On its own the multifidelity continual learning method shows robust results that limit forgetting across several datasets. Additionally, we show that the multifidelity method can be combined with existing continual learning methods, including replay and memory aware synapses, to further limit catastrophic forgetting. The proposed continual learning method is especially suited for physical problems where the data satisfy the same physical laws on each domain, or for physics-informed neural networks, because in these cases we expect there to be a strong correlation between the output of the previous model and the model on the current training domain.
    
[^439]: R^2: 基于区间正则化的模型压缩与量化

    R^2: Range Regularization for Model Compression and Quantization. (arXiv:2303.08253v1 [cs.LG])

    [http://arxiv.org/abs/2303.08253](http://arxiv.org/abs/2303.08253)

    R^2提出了一种基于区间正则化的新方法，利用有效的最小值和最大值调整权重分布，从而使模型压缩和量化技术能够更好地利用其数值表示能力。该方法可以提高模型优化的质量，尤其是在较低位上。

    

    参数正则化是一种广泛应用于提高泛化性能的技术，也可用于调整权重分布以达到各种目的。本文探讨了如何利用权重正则化来辅助模型量化和压缩技术，然后提出了区间正则化(R^2)以进一步提高模型优化的质量，重点放在防止异常值方面。通过有效地调整分布中的最小值和最大值，将整个分布塑造成紧凑的形状，从而使模型压缩和量化技术能够更好地利用它们有限的数值表示能力。我们引入了L-inf正则化，其扩展间隔正则化和新的soft-min-max正则化，作为全精度模型训练期间的正则化损失。结合最先进的量化和压缩技术，利用R^2训练的模型在平均水平上表现更好，尤其是在较低位上。

    Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization (R^2) to further boost the quality of model optimization by focusing on the outlier prevention. By effectively regulating the minimum and maximum weight values from a distribution, we mold the overall distribution into a tight shape so that model compression and quantization techniques can better utilize their limited numeric representation powers. We introduce L-inf regularization, its extension margin regularization and a new soft-min-max regularization to be used as a regularization loss during full-precision model training. Coupled with state-of-the-art quantization and compression techniques, models trained with R^2 perform better on an average, specifically at lower bit 
    
[^440]: 特征分区聚合：一种快速的对$\ell_0$攻击的认证防御方法

    Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks. (arXiv:2302.11628v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11628](http://arxiv.org/abs/2302.11628)

    本文提出了一种名为特征分区聚合的认证防御方法，用于对抗$\ell_0$逃避、后门和污染攻击。与现有防御方法相比，FPA速度更快，提供更大的鲁棒性保证，且能够免费提供额外的鲁棒性维度。

    

    稀疏的或$\ell_0$对抗攻击会任意扰动未知的特征子集。$\ell_0$鲁棒性分析特别适用于异构（表格）数据，其中特征具有不同的类型或尺度。目前最先进的$\ell_0$认证防御基于随机平滑，并仅适用于逃避攻击。本文提出了特征分区聚合（FPA）--一种针对$\ell_0$逃避、后门和污染攻击的认证防御。FPA通过集成生成更强的鲁棒性保证，其子模型是在不相交的特征集上训练的。与最先进的$\ell_0$防御相比，FPA速度提高了多达3000倍，并提供了更大的中位数鲁棒性保证（例如，对于CIFAR10的中位数证书为13像素，MNIST的中位数证书为12像素，Weather的中位数证书为4个特征，Ames的中位数证书为3个特征），这意味着FPA能够免费提供额外的鲁棒性维度。

    Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subset of the features. $\ell_0$ robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art $\ell_0$ certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to 3,000${\times}$ faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.
    
[^441]: 深度学习综述：从激活函数到Transformer

    A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00722](http://arxiv.org/abs/2302.00722)

    这篇综述调查了深度学习领域的重要进展，包括各种架构、层、目标和优化技术的发展，以及关注机制、归一化、跳跃连接、Transformer和自监督学习等方法的变体。总结了成功创新的关键策略，并讨论了最近的商业闭源模型。

    

    过去十年中，深度学习取得了显著的进展，得益于各种架构、层、目标和优化技术的涌现。这些包括关注机制、归一化、跳跃连接、Transformer和自监督学习等多种变体方法。我们的目标是向具有深度学习基本理解的人提供对这些领域中最新重要贡献的全面调查。我们的期望是通过对重要最新作品的综合和全面的探讨，促进不同深度学习领域之间形成新的联系。在我们的讨论中，我们总结了过去十年中许多成功创新的关键策略。我们还对最近一些商业闭源模型进行了讨论，例如OpenAI的GPT-4和Google的PaLM 2。

    The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.
    
[^442]: 发现和利用广义网络效应

    Discovery and Exploitation of Generalized Network Effects. (arXiv:2301.00270v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2301.00270](http://arxiv.org/abs/2301.00270)

    我们提出了NetEffect，一种图挖掘方法，能够识别和理解具有少量节点标签的大型图中的广义网络效应（如同质性、异质性或二者的组合），并利用这些效应来改进下游任务的准确性和效率。

    

    针对具有少量节点标签的大型图，我们如何（a）确定图中是否存在广义网络效应（GNE），（b）估计GNE以解释节点类之间的相互关系，以及（c）利用GNE改进诸如准确高效地预测未知标签等下游任务？对于节点分类和定向广告等各种任务，了解GNE是很有价值的。然而，由于节点标签有限且边缘噪音，识别和理解GNE（如同质性、异质性或二者的组合）在现实世界的图中是具有挑战性的。我们提出了NetEffect，一种图挖掘方法来解决上述问题，具有以下特点：（i）基于原则：用统计测试确定带有少量节点标签的图中是否存在GNE；（ii）普遍且可解释：估计观察到的特定类型的GNE的闭式解决方案；（iii）准确且可扩展：集成GNE以实现准确且快速的结果。

    Given a large graph with few node labels, how can we (a) identify whether there is generalized network-effects (GNE) of the graph or not, (b) estimate GNE to explain the interrelations among node classes, and (c) exploit GNE to improve downstream tasks such as predicting the unknown labels accurately and efficiently? The knowledge of GNE is valuable for various tasks like node classification and targeted advertising. However, identifying and understanding GNE such as homophily, heterophily or their combination is challenging in real-world graphs due to limited availability of node labels and noisy edges. We propose NetEffect, a graph mining approach to address the above issues, enjoying the following properties: (i) Principled: a statistical test to determine the presence of GNE in a graph with few node labels; (ii) General and Explainable: a closed-form solution to estimate the specific type of GNE observed; and (iii) Accurate and Scalable: the integration of GNE for accurate and fast
    
[^443]: FED-CD：来自干预和观测数据的联合因果发现

    FED-CD: Federated Causal Discovery from Interventional and Observational Data. (arXiv:2211.03846v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03846](http://arxiv.org/abs/2211.03846)

    FED-CD是一个联邦框架，可以从分布式数据集中推断出因果关系，同时保护隐私，适应共享和不相交的干预协变量的场景，具有可扩展性。

    

    现有的因果推断方法通常要求数据在集中位置可用。然而，许多实际领域，如医疗保健，限制对局部实体收集的数据的访问，主要是出于隐私和监管限制。为了解决这个问题，我们提出了FED-CD，这是一个联邦框架，用于从包含观测数据和干预数据的分布式数据集中推断因果结构。通过交换更新而不是数据样本，FED-CD确保隐私，同时实现去中心化的发现底层有向无环图（DAG）。我们适应共享或不相交的干预协变量的场景，并减轻干预数据异质性的不良影响。我们提供了使用合成和真实DAG的FED-CD性能和可扩展性的实证证据。

    Existing causal discovery methods typically require the data to be available in a centralized location. However, many practical domains, such as healthcare, limit access to the data gathered by local entities, primarily for privacy and regulatory constraints. To address this, we propose FED-CD, a federated framework for inferring causal structures from distributed datasets containing observational and interventional data. By exchanging updates instead of data samples, FED-CD ensures privacy while enabling decentralized discovery of the underlying directed acyclic graph (DAG). We accommodate scenarios with shared or disjoint intervened covariates, and mitigate the adverse effects of interventional data heterogeneity. We provide empirical evidence for the performance and scalability of FED-CD for decentralized causal discovery using synthetic and real-world DAGs.
    

