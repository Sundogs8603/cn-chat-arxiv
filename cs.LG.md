# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [An Efficient General-Purpose Modular Vision Model via Multi-Task Heterogeneous Training.](http://arxiv.org/abs/2306.17165) | 本文提出了一种通过多任务异构训练实现高效通用模块化视觉模型的方法，以应对在视觉任务之间的大量内在差异，并解决多任务模型扩展的挑战。 |
| [^2] | [Local Risk Bounds for Statistical Aggregation.](http://arxiv.org/abs/2306.17151) | 本文通过将全局复杂度替换为较小的局部复杂度来重新审视和加强了统计聚合理论中的经典结果。 |
| [^3] | [Orbit Classification of asteroids using implementation of radial Basis Function on Support Vector Machines.](http://arxiv.org/abs/2306.17138) | 本研究论文实现了径向基函数支持向量机(RBF SVM)用于对小行星轨道进行分类。研究结果表明，RBF SVM算法在效率和准确性方面表现良好，并提供了最佳参数设置。 |
| [^4] | [Synthetic Demographic Data Generation for Card Fraud Detection Using GANs.](http://arxiv.org/abs/2306.17109) | 该论文提出了一种名为DGGAN的深度学习生成对抗网络，用于生成合成人口数据，以用于信用卡欺诈检测。通过使用相对复杂的合成人口数据，可以提高交易数据特征的复杂性，并提升欺诈检测性能。 |
| [^5] | [ManimML: Communicating Machine Learning Architectures with Animation.](http://arxiv.org/abs/2306.17108) | ManimML是一个开源Python库，通过动画演示自动生成的ML算法，为机器学习从业者提供了一种简单且熟悉的方式来沟通和可视化ML算法。 |
| [^6] | [Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations.](http://arxiv.org/abs/2306.17105) | 这篇论文研究了神经网络中的“神经折叠”现象，提出了表面上折叠的表示实际上仍隐藏有重要的细粒度结构，并通过实验证据证明了这一点。 |
| [^7] | [Identifying Important Sensory Feedback for Learning Locomotion Skills.](http://arxiv.org/abs/2306.17101) | 通过深度强化学习学习机器人运动技能时，我们通过一种显著性分析方法定量评估了不同反馈状态的相对重要性。我们发现关节位置、重力向量和速度等关键状态可以实现与使用所有状态相当的步态技能性能。 |
| [^8] | [RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark.](http://arxiv.org/abs/2306.17100) | RL4CO是一个用于组合优化的广泛强化学习基准测试，着重于可扩展性和泛化能力的评估，并展示了一些最新方法在样本效率和适应不同数据分布方面的表现相对较差，强调了对神经CO求解器性能的平衡评估的重要性。 |
| [^9] | [The Importance of Robust Features in Mitigating Catastrophic Forgetting.](http://arxiv.org/abs/2306.17091) | 本研究着眼于连续学习模型，通过引入鲁棒特征数据集发现，在其上训练的模型比在标准数据集上训练的模型具有更小的灾难性遗忘，从而凸显出增强鲁棒特征在减轻灾难性遗忘中的重要性。 |
| [^10] | [Sparsity exploitation via discovering graphical models in multi-variate time-series forecasting.](http://arxiv.org/abs/2306.17090) | 这项工作提出了一种使用图模型进行多元时间序列预测的方法，通过发现和理解底层图结构的相关性来提高预测性能。通过直接利用数据中的稀疏模式构建图结构，从而解决了在没有显式先验图结构的情况下生成的图不稀疏的问题，提高了模型的计算效率和可解释性。 |
| [^11] | [Concept-Oriented Deep Learning with Large Language Models.](http://arxiv.org/abs/2306.17089) | 本文讨论了大型语言模型在概念导向深度学习中的应用，包括从文本和图像中提取概念和概念图。同时也探讨了多模态语言模型在表达人类知识方面的优势。 |
| [^12] | [On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data.](http://arxiv.org/abs/2306.17066) | 该论文通过全面大规模实验研究，系统评估了最先进的神经TPP模型在预测准确性方面的效果，并发现了关键因素。 |
| [^13] | [Gesture Recognition with mmWave Wi-Fi Access Points: Lessons Learned.](http://arxiv.org/abs/2306.17062) | 本研究探索了使用mmWave Wi-Fi信号进行手势识别/姿势估计，通过提取信号-to-noise ratios (SNRs) 和利用深度神经网络（DNN）进行特征提取，实现了在单一环境中96.7%的准确率。 |
| [^14] | [Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning.](http://arxiv.org/abs/2306.17052) | 本文提出了Safe-$\text{M}^3$-UCRL算法，通过使用模型中的认知不确定性和对数障碍方法，实现了在未知转移动态情况下达到安全策略的优化，成功解决了大规模多智能体协调问题。 |
| [^15] | [Comparison of Single- and Multi- Objective Optimization Quality for Evolutionary Equation Discovery.](http://arxiv.org/abs/2306.17038) | 本文比较了单目标和多目标优化在进化方程发现中的质量；单目标优化仅考虑方程中所选项的差异，而多目标优化还考虑了所获得方程的复杂性。 |
| [^16] | [Safety-Aware Task Composition for Discrete and Continuous Reinforcement Learning.](http://arxiv.org/abs/2306.17033) | 本文研究了布尔组合在强化学习中的应用，通过引入两种安全性概念和拓展到连续行动空间，实现了任务的安全感知组合。 |
| [^17] | [milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing.](http://arxiv.org/abs/2306.17010) | milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。 |
| [^18] | [Spectral Batch Normalization: Normalization in the Frequency Domain.](http://arxiv.org/abs/2306.16999) | 本文介绍了一种新颖的方法——谱批量归一化(SBN)，通过在频域中归一化特征映射，提高了深度神经网络泛化能力。实验证明，尽管有批量归一化(BN)，特征映射在网络开始阶段仍然会爆炸。 |
| [^19] | [Weight Compander: A Simple Weight Reparameterization for Regularization.](http://arxiv.org/abs/2306.16993) | 本文提出了一种名为权重压缩器（WC）的新方法，通过使用非线性函数对深度神经网络中的每个权重进行重新参数化，从而提高了泛化能力并减少过拟合。 |
| [^20] | [End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments.](http://arxiv.org/abs/2306.16978) | 本文提出了一种基于端到端强化学习的在线覆盖路径规划方法，能处理未知环境并结合全局地图和局部感知输入，同时考虑长期路径规划和短期障碍物检测。 |
| [^21] | [Diffusion-Jump GNNs: Homophiliation via Learnable Metric Filters.](http://arxiv.org/abs/2306.16976) | 提出了一种名为Diffusion-Jump GNNs的新方法，通过学习可调节的度量过滤器，来提高高阶图神经网络在异质化场景下的效果。这种方法通过跳跃式的渐进扩散距离生成过滤器的支持和系数，以寻找散点之间的联系。 |
| [^22] | [Restore Translation Using Equivariant Neural Networks.](http://arxiv.org/abs/2306.16938) | 本论文提出了一种使用等变神经网络恢复翻译的方法，通过恢复已翻译（甚至旋转）的输入到原始输入，并将其传送到任意分类器中，以提供更好的性能。 |
| [^23] | [End-to-end Autonomous Driving: Challenges and Frontiers.](http://arxiv.org/abs/2306.16927) | 这项研究调查了端到端自动驾驶领域中的关键挑战和未来趋势，包括多模态、可解释性、因果混淆、鲁棒性和世界模型等。通过联合特征优化感知和规划，端到端系统在感知和规划上获得了更好的效果。 |
| [^24] | [OSP: Boosting Distributed Model Training with 2-stage Synchronization.](http://arxiv.org/abs/2306.16926) | OSP是一种新的分布式模型训练方法，通过使用两阶段同步和本地梯度修正来提高通信效率，避免了精度损失。 |
| [^25] | [NAUTILUS: boosting Bayesian importance nested sampling with deep learning.](http://arxiv.org/abs/2306.16923) | NAUTILUS是一种利用深度学习增强贝叶斯重要嵌套采样技术的方法，用于贝叶斯后验和证据估计。该方法通过神经网络回归将INS与深度学习相结合，实现了高效的重要采样。在各种合成问题和现实应用中，NAUTILUS在性能上超过了流行的NS和MCMC软件包。 |
| [^26] | [The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks.](http://arxiv.org/abs/2306.16922) | ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。 |
| [^27] | [Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs.](http://arxiv.org/abs/2306.16921) | 通过课程学习，采用稀疏示例先学习的2层ReLU神经网络可以在混合输入的奇偶目标上学习到足够大阶数的奇偶性，而其他神经网络无法在相同的条件下学习。 |
| [^28] | [The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes.](http://arxiv.org/abs/2306.16917) | 这项研究提出了一种在变形场景中估计相机运动的方法，并介绍了一个由大量合成数据构成的挑战性数据集，以用于可变形环境中的视觉导航和重建。 |
| [^29] | [Obeying the Order: Introducing Ordered Transfer Hyperparameter Optimisation.](http://arxiv.org/abs/2306.16916) | 引入有序传输超参数优化（OTHPO）方法，解决了超参数优化问题中任务顺序相关的挑战，并通过十个基准测试证明了其重要性，该方法胜过当前最先进的迁移超参数优化方法。 |
| [^30] | [AutoML in Heavily Constrained Applications.](http://arxiv.org/abs/2306.16913) | 本文提出了Caml，一种在严格约束的应用中使用元学习的AutoML方法。Caml能够自动适应特定任务的AutoML参数，并考虑用户定义的约束，生成满足约束且具有高预测性能的流程。 |
| [^31] | [Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach.](http://arxiv.org/abs/2306.16906) | 本论文介绍了一种新的数值数据填补方法，通过将最近邻估计和高斯核密度估计结合，能够有效处理多模态数据集中的缺失值，并提供比当前方法更高的概率估计。 |
| [^32] | [Traceable Group-Wise Self-Optimizing Feature Transformation Learning: A Dual Optimization Perspective.](http://arxiv.org/abs/2306.16893) | 本论文提出了一种可追溯的群体化自优化特征转换学习方法，并引入了一个新颖的自优化框架，利用三个级联强化代理自动选择候选特征和操作，生成改进的特征转换组合。 |
| [^33] | [Policy Space Diversity for Non-Transitive Games.](http://arxiv.org/abs/2306.16884) | 这项研究提出了一种新的策略空间多样性度量，并通过将其纳入策略空间响应预言机（PSRO）中，实现了更好的逼近纳什均衡（NE）的效果。 |
| [^34] | [Surgical Phase and Instrument Recognition: How to identify appropriate Dataset Splits.](http://arxiv.org/abs/2306.16879) | 本研究开发了一个基于Web的应用程序，用于手术工作流和器械识别的机器学习模型。通过可视化框架，能够评估手术工作流识别的数据集划分，特别是识别次优的划分。 |
| [^35] | [Understanding the Overfitting of the Episodic Meta-training.](http://arxiv.org/abs/2306.16873) | 本研究通过引入知识蒸馏技术，解决了迭代元训练阶段的过拟合问题，该方法通过惩罚过度区分，保留教师模型的新类别泛化知识，优于标准元训练过程。同时，我们提出了最近邻对称Kullback-Leibler（NNSKL）散度来进一步推进知识蒸馏的极限。 |
| [^36] | [NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes.](http://arxiv.org/abs/2306.16869) | NeuralFuse是一个新颖的附加模块，通过学习输入转换来生成抗误差的数据表示，解决了低电压环境下有限访问神经网络推断的准确性与能量之间的权衡问题。 |
| [^37] | [ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch.](http://arxiv.org/abs/2306.16857) | ArrayBot通过强化学习实现了通用分布式操作，通过对动作空间的重新定义和采用触觉观察训练，其控制策略不仅能够推广到未见过的物体形状，还能在实际机器人中进行转移，展示了巨大的潜力。 |
| [^38] | [On the Relationship Between RNN Hidden State Vectors and Semantic Ground Truth.](http://arxiv.org/abs/2306.16854) | 本研究考察了循环神经网络的隐藏状态向量是否具有语义相似的聚类结构，并通过在训练中识别常规语言的RNNs来验证聚类假设的有效性。 |
| [^39] | [Macro Placement by Wire-Mask-Guided Black-Box Optimization.](http://arxiv.org/abs/2306.16844) | 本文介绍了一种名为WireMask-BBO的黑盒优化框架，通过使用线掩模引导的贪心过程进行宏单元布局，在有效降低HPWL的同时节省大量时间。此方法还可以对现有布局进行微调，改善50%的HPWL。 |
| [^40] | [Solving Kernel Ridge Regression with Gradient-Based Optimization Methods.](http://arxiv.org/abs/2306.16838) | 本研究提出了一种新的方法来解决核岭回归问题，通过等价的目标函数形式和基于梯度的优化方法，我们不仅可以使用其他惩罚方法，还能够从梯度下降的角度研究核岭回归。通过提前停止的正则化，我们推导出了一个闭合解，即核梯度流（KGF），并证明了KGF和KRR之间的差异。我们还将KRR泛化，使用$\ell_1$和$\ell_\infty$惩罚方法，并发现使用这些方法得到的解与前向分步回归和符号梯度下降结合提前停止得到的解非常相似。因此，我们减少了计算复杂度重的近端梯度下降算法的需求。 |
| [^41] | [Sampling weights of deep neural networks.](http://arxiv.org/abs/2306.16830) | 我们提出了一种基于随机特征模型的采样方法，用于深度神经网络的权重和偏差。我们的方法不需要迭代优化或计算梯度，能够生成通用逼近器，并且对数据的变换和缩放是不变的。在数值实验中，我们展示了我们的方法的优越性能。 |
| [^42] | [SaGess: Sampling Graph Denoising Diffusion Model for Scalable Graph Generation.](http://arxiv.org/abs/2306.16827) | SaGess是一种离散去噪扩散方法，通过将扩散模型（DiGress）与广义的分治框架相结合，能够生成大型实际网络。这种方法克服了计算复杂性的限制，使得生成大型图数据成为可能。 |
| [^43] | [Length of Stay prediction for Hospital Management using Domain Adaptation.](http://arxiv.org/abs/2306.16823) | 本研究使用领域适应技术来预测医院管理中的住院时长，以帮助医院有效规划入院、分配资源和改善护理。 |
| [^44] | [Improving Online Continual Learning Performance and Stability with Temporal Ensembles.](http://arxiv.org/abs/2306.16817) | 该研究通过模型集成方法改进了在线连续学习的性能和稳定性，通过综合利用来自不同训练任务的模型，显著提高了在线连续学习的表现。 |
| [^45] | [CLIPAG: Towards Generator-Free Text-to-Image Generation.](http://arxiv.org/abs/2306.16805) | 本文将感知对齐梯度（PAG）的研究扩展到视觉-语言架构，并通过对 CLIP 进行鲁棒性调整，展示了在视觉-语言生成任务中集成 CLIPAG 可以实现显著改进，并实现了无生成器的文本到图像生成。 |
| [^46] | [Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis.](http://arxiv.org/abs/2306.16803) | 本文提出了一种新的基于模型的信用分配算法，通过量化反事实查询来测量动作对未来奖励的影响。与现有方法不同的是，我们通过测量对奖励或奖励对象表示的贡献，获得了具有更低方差的梯度估计。 |
| [^47] | [Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging.](http://arxiv.org/abs/2306.16788) | 本研究通过将多个经过迭代幅度剪枝的模型进行平均，解决了同时利用稀疏性和参数平均的问题，并显著提升了泛化性能。 |
| [^48] | [Graph Sampling-based Meta-Learning for Molecular Property Prediction.](http://arxiv.org/abs/2306.16780) | 本文提出了一种基于图采样的元学习框架，用于少样本分子属性预测。通过构建分子-属性关系图并利用拓扑信息，以及使用对比损失函数调度子图采样过程，有效利用多对多关系进行预测。 |
| [^49] | [Learning from Synthetic Human Group Activities.](http://arxiv.org/abs/2306.16772) | 提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器，通过Unity引擎驱动实现。该生成器具有大规模数据生成、多模态和高质量注释等特点，能够用于研究复杂的人类互动和团队活动。 |
| [^50] | [Performance Analysis of DNN Inference/Training with Convolution and non-Convolution Operations.](http://arxiv.org/abs/2306.16767) | 本文提出了SimDIT，一种新颖的性能分析框架，用于通用ASIC系统级硬件加速器平台上的CNN推理和训练。SimDIT综合考虑了卷积和非卷积操作，并提供了详细的性能统计数据。 |
| [^51] | [Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs.](http://arxiv.org/abs/2306.16761) | 本文提出了一种基于Moreau包络的弱凸差分重构与双层规划算法，可以适用于更多与机器学习和统计相关的应用。 |
| [^52] | [Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall Classification.](http://arxiv.org/abs/2306.16760) | 该论文讨论了使用半监督数据集注释进行迁移学习的方法，通过利用现有的模型解决鸟鸣分类比赛中的挑战，并提出了一种获取带标注数据集的方法。实验证明这种方法在鸟类物种分类方面有效，并展示了迁移学习和半监督数据集注释在类似任务中的潜力。 |
| [^53] | [Eigensubspace of Temporal-Difference Dynamics and How It Improves Value Approximation in Reinforcement Learning.](http://arxiv.org/abs/2306.16750) | ERC是一种新的值估计方法，通过在深度强化学习中利用时间差分动力学的特征子空间，实现了更高效和稳定的值估计路径。实验证明ERC有效地减少了值函数的方差，并在多项任务中优于其他最先进方法。 |
| [^54] | [Principles and Guidelines for Evaluating Social Robot Navigation Algorithms.](http://arxiv.org/abs/2306.16740) | 本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。 |
| [^55] | [Towards Optimal Randomized Strategies in Adversarial Example Game.](http://arxiv.org/abs/2306.16738) | 该论文提出了一种新算法FRAT，用于在对抗性样本攻击中优化随机策略。该算法通过在概率分布空间上建模问题，并维护轻量级的模型混合来达到目的。 |
| [^56] | [Understanding Pathologies of Deep Heteroskedastic Regression.](http://arxiv.org/abs/2306.16717) | 该论文研究了利用异方差神经回归模型对真实世界数据进行建模时的困难，并从统计物理的角度提供了解释。作者证明了这些不稳定性不仅适用于神经网络结构，而且已经在过参数化条件高斯似然模型的场论中存在。数值求解结果与实证模型拟合的定性一致性证明了相变的存在。 |
| [^57] | [Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering.](http://arxiv.org/abs/2306.16713) | 本研究探究了在给定上下文中从相关和无关图像池中挖掘答案的视觉问答问题。我们提出了一个统一的模型，Multi Image BART (MI-BART)，通过检索相关图像并使用相关性编码器进行自由流畅的答案生成。同时，我们还引入了最大的RETVQA数据集，该数据集具有多图像和检索要求，并且可以对一组异构图像进行元数据无关的问题提问。 |
| [^58] | [Elastically-Constrained Meta-Learner for Federated Learning.](http://arxiv.org/abs/2306.16703) | 这项研究提出了一种弹性约束的元学习方法，用于解决联邦学习中由于非独立同分布数据导致元学习的不稳定目标的收敛问题。 |
| [^59] | [Dynamic-Resolution Model Learning for Object Pile Manipulation.](http://arxiv.org/abs/2306.16700) | 本文研究了对象堆叠操作的动态分辨率模型学习，通过构建动态分辨率的粒子环境表示并使用图神经网络进行学习，实现了学习的动态和自适应表示，在对象堆叠操作任务中取得了良好的效果。（Translated from Abstract） |
| [^60] | [Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation.](http://arxiv.org/abs/2306.16699) | 本文提出了一种使用隐式神经表示进行高效的无CPU深度神经网络训练的新方法，通过在GPU上直接存储整个数据集以INR格式，减少了数据传输开销，从而加速训练过程。同时，采用高度并行化和实时执行的解码过程，进一步提升了压缩效果。 |
| [^61] | [SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores.](http://arxiv.org/abs/2306.16688) | SRL是一个可扩展，高效，可扩展的分布式强化学习系统，通过一种新的抽象框架统一了各种实际强化学习训练，并实现了精细优化。 |
| [^62] | [BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models.](http://arxiv.org/abs/2306.16678) | BinaryViT是一种针对二值化视觉Transformer的改进模型，通过借鉴CNN的架构特性，提高了二值化ViT的表示能力和性能。 |
| [^63] | [Game Level Blending using a Learned Level Representation.](http://arxiv.org/abs/2306.16666) | 本文介绍了一种使用聚类-based 平铺嵌入的方法，通过学习的关卡表示来实现游戏关卡混合，为未注释的游戏提供关卡表示，并在游戏之间提供统一的关卡表示，而无需人工注释。 |
| [^64] | [Joint Level Generation and Translation Using Gameplay Videos.](http://arxiv.org/abs/2306.16662) | 该论文提出了一种利用游戏视频进行关卡生成和翻译的联合模型，通过学习同时进行关卡翻译和生成，解决了机器学习关卡生成技术中受限注释数据的问题。 |
| [^65] | [NaturalInversion: Data-Free Image Synthesis Improving Real-World Consistency.](http://arxiv.org/abs/2306.16661) | NaturalInversion 是一种无需真实数据的图像合成方法，通过特征传递金字塔、一对一生成模型和可学习的自适应通道缩放参数，合成的图像与原始数据分布更加一致，并在性能上超过以前的方法。 |
| [^66] | [Private Covariance Approximation and Eigenvalue-Gap Bounds for Complex Gaussian Perturbations.](http://arxiv.org/abs/2306.16648) | 本文研究了在差分隐私下，利用复高斯机制近似一个协方差矩阵为秩k矩阵的问题。通过分析复高斯噪声扰动下矩阵的特征值，我们证明了当M的第k个特征值和第k+1个特征值之间有足够大的间隔时，复高斯机制产生的矩阵与M的最佳秩k近似之间的差距的界限为O(√kd)。 |
| [^67] | [CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?.](http://arxiv.org/abs/2306.16636) | 该论文介绍了中国小学数学应用题（CMATH）数据集，评估了多个流行的大型语言模型（LLMs）在小学数学不同年级的表现。研究发现只有GPT-4在所有年级中取得成功，并且能够保持鲁棒性，而其他模型则在不同年级上表现较差。 |
| [^68] | [Improving Fairness in Deepfake Detection.](http://arxiv.org/abs/2306.16635) | 本研究首次尝试通过提出新的损失函数来改善深度伪造检测的公平性，并在多个数据集和检测器上进行了广泛实验证明了其有效性。 |
| [^69] | [MNISQ: A Large-Scale Quantum Circuit Dataset for Machine Learning on/for Quantum Computers in the NISQ era.](http://arxiv.org/abs/2306.16627) | MNISQ 是一个大规模的量子电路数据集，用于 NISQ 时代的量子和经典机器学习研究。该数据集由 4,950,000 个数据点组成，以量子和经典形式呈现，旨在提升机器学习，并促进量子计算的发展。 |
| [^70] | [Assessing the Performance of 1D-Convolution Neural Networks to Predict Concentration of Mixture Components from Raman Spectra.](http://arxiv.org/abs/2306.16621) | 该论文介绍了一种评估1D-卷积神经网络在从拉曼光谱预测混合组分浓度方面的性能的方法，并提出了一个Python软件包RaMix，该软件包可以生成具有可控噪声水平的合成拉曼混合数据集，用于评估不同化学计量学算法在实时监测应用中的效果。 |
| [^71] | [Curvature-Independent Last-Iterate Convergence for Games on Riemannian Manifolds.](http://arxiv.org/abs/2306.16617) | 该论文通过对黎曼梯度下降算法进行分析，证明了在测地线强单调设置下，具有对曲率不敏感的固定步长的RGD方案可以实现曲率无关和线性的最后收敛速度。 |
| [^72] | [Group-based Robustness: A General Framework for Customized Robustness in the Real World.](http://arxiv.org/abs/2306.16614) | 本研究提出了一种基于群体的鲁棒性指标，可以更好地评估机器学习模型在现实世界中抵抗攻击的能力，弥补了传统指标的不足。实验证明，该指标能够区分模型对特定威胁的脆弱性。 |
| [^73] | [GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps.](http://arxiv.org/abs/2306.16612) | 提出了一种新颖的显著性感知混合方法GuidedMixup，通过优化配对图像中显著区域的冲突，以低计算开销在混合图像中保留显著区域。多个数据集上的实验证明，GuidedMixup在增强效果和计算效率之间取得了良好的平衡。 |
| [^74] | [An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs.](http://arxiv.org/abs/2306.16601) | 本文提出了一个用于基于Transformer的语言模型的高效稀疏推断软件加速器，在CPU上利用Intel Deep Learning Boost实现了稀疏矩阵-稠密矩阵乘法的优化，相较于现有的稀疏库，在各种形状和稀疏度下都获得了一个数量级的性能提升。 |
| [^75] | [Forecasting of the development of a partially-observed dynamical time series with the aid of time-invariance and linearity.](http://arxiv.org/abs/2306.16593) | 本研究提出了一种自回归松弛时间序列（ARS）模型，通过考虑动态系统的时间不变性和线性性，同时估计演化函数和缺失变量，用于预测动态时间序列中缺失变量的发展。 |
| [^76] | [Allocating Divisible Resources on Arms with Unknown and Random Rewards.](http://arxiv.org/abs/2306.16578) | 本论文研究了在每个周期将一单位可分资源分配到多个臂上的问题，臂上的奖励是未知和随机的，而且与分配的资源成比例，而方差与分配资源的阶数成比例。我们设计了两种算法，实现了不同阶数下的最优有界和无界遗憾，结果表明在阶数为1/2时存在相变现象。 |
| [^77] | [Finite-Sample Symmetric Mean Estimation with Fisher Information Rate.](http://arxiv.org/abs/2306.16573) | 本文研究了有限样本下对称均值估计的问题，并给出了基于费舍尔信息的保证。对于对称分布，可以获得收敛到次高斯的收敛速度，而不需要渐近条件。 |
| [^78] | [Feature Selection: A perspective on inter-attribute cooperation.](http://arxiv.org/abs/2306.16559) | 本文综述了辅助特征间协作的过滤特征选择方法的最新研究进展，并总结了不同方法在文献中的贡献。同时提出了当前存在的问题和挑战，以确定未来有前景的研究和发展方向。 |
| [^79] | [Non-Convex Optimizations for Machine Learning with Theoretical Guarantee: Robust Matrix Completion and Neural Network Learning.](http://arxiv.org/abs/2306.16557) | 本论文研究了稳健矩阵补全和神经网络学习两个流行的非凸优化问题，以提供可解释的算法解决机器学习中的黑盒问题。 |
| [^80] | [Learning Fair Classifiers via Min-Max F-divergence Regularization.](http://arxiv.org/abs/2306.16552) | 本文提出了一种通过最小-最大F-散度正则化学习公平分类器的框架，该框架通过使用F-散度衡量公平性，并保持高准确性。该框架可以适用于多个敏感属性和高维数据集。 |
| [^81] | [Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering.](http://arxiv.org/abs/2306.16541) | 该论文介绍了一个下一代增强现实会议系统的设计，其中提出了一种高效真实人体渲染的方法。目前的应用在实时会议方面表现良好，但在传递真实人体动态方面存在不足。该研究借鉴了神经渲染技术，通过采用单目视频采集和自由视点合成来提高会议系统的效率，并提供更高的真实感和交互性。 |
| [^82] | [Prediction of Rapid Early Progression and Survival Risk with Pre-Radiation MRI in WHO Grade 4 Glioma Patients.](http://arxiv.org/abs/2306.16531) | 本研究是首次利用计算和统计建模方法，通过分析胶质瘤患者的MRI序列和分子特征，预测WHO 4级胶质瘤患者的快速早期进展，并确定了与生存概率相关的特征。 |
| [^83] | [A Food Recommender System in Academic Environments Based on Machine Learning Models.](http://arxiv.org/abs/2306.16528) | 这个论文介绍了一个基于机器学习模型的食物推荐系统，通过使用协同过滤、基于内容和基于知识的模型，以提高人们的健康。研究探讨了决策树、k最近邻居(kNN)、AdaBoost和Bagging等机器学习模型在食物推荐系统上的应用。 |
| [^84] | [Shilling Black-box Review-based Recommender Systems through Fake Review Generation.](http://arxiv.org/abs/2306.16526) | 本文提出了一种基于生成的模型，通过虚假评论生成器对基于评论的推荐系统进行操纵攻击。实验证明该框架可以成功地攻击亚马逊上的三种不同类型的RBRS。 |
| [^85] | [HNO: Hyena Neural Operator for solving PDEs.](http://arxiv.org/abs/2306.16524) | 本研究使用了一种名为鬣狗的新型神经算子，它利用多层感知器参数化的长卷积滤波器来解决PDE问题。这种方法通过增强模型对输入上下文的理解，并为不同的PDE实例提供数据依赖权重，提供了一种有效的求解PDE的方式。 |
| [^86] | [For Kernel Range Spaces a Constant Number of Queries Are Sufficient.](http://arxiv.org/abs/2306.16516) | 对于核范围空间，引入了ε-覆盖的概念，用于处理不确定或不精确的数据分析任务。 |
| [^87] | [Momentum Benefits Non-IID Federated Learning Simply and Provably.](http://arxiv.org/abs/2306.16504) | 本论文研究了在非独立同分布联邦学习中利用动量来提升FedAvg和SCAFFOLD算法的性能，证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。 |
| [^88] | [SARC: Soft Actor Retrospective Critic.](http://arxiv.org/abs/2306.16503) | SARC是一个基于SAC算法的新方法，通过改进评论者实现更好的收敛性和梯度估计，为演员提供了更好的策略梯度估计。 |
| [^89] | [Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements.](http://arxiv.org/abs/2306.16502) | 本论文研究了变分不等式中的随机方法，并通过建立大数定律和中心极限定理揭示了这些算法的收敛性质，对于广泛的VIP问题，平均迭代收敛到一个唯一的不变分布。 |
| [^90] | [Towards a Better Theoretical Understanding of Independent Subnetwork Training.](http://arxiv.org/abs/2306.16484) | 本研究对独立子网络训练（IST）进行了理论分析，发现了IST与其他模型并行方法之间的根本差异。 |
| [^91] | [Increasing Performance And Sample Efficiency With Model-agnostic Interactive Feature Attributions.](http://arxiv.org/abs/2306.16431) | 本文提出了一种增强模型普适的交互式特征归因方法，通过纠正特征归因并重新训练模型，显著提高了模型的性能和样本效率。 |
| [^92] | [DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference.](http://arxiv.org/abs/2306.16430) | 本文介绍了一种在DNN推理中使用的自适应指数量化张量方法DNA-TEQ，该方法通过发现大量张量符合指数分布来实现最佳的数值精度和准确性损失平衡。 |
| [^93] | [Complex-valued Adaptive System Identification via Low-Rank Tensor Decomposition.](http://arxiv.org/abs/2306.16428) | 本论文提出了两种新的架构来处理复数信号，超越了原始架构的复数扩展，在性能上表现优越，只需稍微增加计算资源即可实现复数运算。 |
| [^94] | [Long-Term Hourly Scenario Generation for Correlated Wind and Solar Power combining Variational Autoencoders with Radial Basis Function Kernels.](http://arxiv.org/abs/2306.16427) | 本文提出了一种创新的方法，结合变分自动编码器和径向基函数核，用于生成长期逐时的风力和太阳能发电场景，并考虑了这两种能源来源之间的相关性。 |
| [^95] | [A Collaborative Transfer Learning Framework for Cross-domain Recommendation.](http://arxiv.org/abs/2306.16425) | 这篇论文提出了一种协作跨领域迁移学习框架（CCTL），用于解决推荐系统中不同领域CTR预测建模的挑战。 |
| [^96] | [Realistic Synthetic Financial Transactions for Anti-Money Laundering Models.](http://arxiv.org/abs/2306.16424) | 本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。 |
| [^97] | [Neural networks can detect model-free static arbitrage strategies.](http://arxiv.org/abs/2306.16422) | 本文证明了神经网络可以检测金融市场中的无模型静态套利机会，并可应用于交易证券数量较多的金融市场。我们的方法具有易处理性、有效性和稳健性，并使用真实金融数据进行了示例验证。 |
| [^98] | [Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization.](http://arxiv.org/abs/2306.16077) | 本论文提出了一种在垂直联邦学习中使用级联混合优化的方法，通过在下游使用零阶优化保护隐私并在上游使用一阶优化提高收敛速度，从而解决了ZOO-based VFL收敛速度较慢的问题。 |
| [^99] | [An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning.](http://arxiv.org/abs/2306.15786) | 通过对不同数据集、模型和指标进行定量评估，我们发现罗生门效应对可解释机器学习具有影响，这为之前的轶事证据提供了实证支持，并展示了科学家和实践者面临的挑战。 |
| [^100] | [A Restarted Large-Scale Spectral Clustering with Self-Guiding and Block Diagonal Representation.](http://arxiv.org/abs/2306.15138) | 本论文提出了一个具有自我引导和块对角表示的重启聚类方法，该方法在谱聚类中首次应用重启策略，并且通过在每个周期中重新分类样本来获得更好的聚类效果。 |
| [^101] | [Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning.](http://arxiv.org/abs/2306.14522) | 该论文提出了一种非凸随机Bregman近端梯度法（SBPG），它通过使用Bregman近似测度替代了随机梯度法中的上二次逼近，并在捕捉非Lipschitz梯度的非凸目标函数方面得到更好的近似模型。论文证明了SBPG的收敛性质，并提出了一种基于动量的改进版本，称为MSBPG，并证明了它具有更好的收敛性质。 |
| [^102] | [Enhancing Adversarial Training via Reweighting Optimization Trajectory.](http://arxiv.org/abs/2306.14275) | 本文提出了一种名为“加权优化轨迹（WOT）”的新方法，通过优化历史轨迹，解决了对抗训练中的鲁棒泛化问题。 |
| [^103] | [Bring Your Own Data! Self-Supervised Evaluation for Large Language Models.](http://arxiv.org/abs/2306.13651) | 本研究提出了一种自我监督评估框架，通过分析输入文本上的变换对LLMs的灵敏度或不变性，直接监控LLMs在实际数据上的行为。 |
| [^104] | [An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning.](http://arxiv.org/abs/2306.12088) | 本论文提出FedINIBoost，一种新的联邦学习方法，通过梯度匹配构建有效的提取模块，在减少通信开销的同时提高了模型准确率。 |
| [^105] | [Deep Learning for Energy Time-Series Analysis and Forecasting.](http://arxiv.org/abs/2306.09129) | 本文介绍了深度学习在能源时间序列分析和预测中的应用。重点关注希腊能源市场，通过改进深度学习方法来提高预测性能。 |
| [^106] | [ViP: A Differentially Private Foundation Model for Computer Vision.](http://arxiv.org/abs/2306.08842) | 本论文提出了ViP，一个使用差分隐私保证的计算机视觉基础模型。通过使用掩码自编码器和DP-SGD算法，我们在LAION400M数据集上训练了ViP。ViP在标准的视觉任务中学到了高质量的表示，并且在ImageNet上达到了与AlexNet相当的准确率。 |
| [^107] | [The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions.](http://arxiv.org/abs/2306.07774) | 该论文提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播，通过将Lyapunov方程投影到低秩矩阵的流形上，使用数值稳定的动态低秩积分器求解，能够有效地处理高维数据。 |
| [^108] | [Python Wrapper for Simulating Multi-Fidelity Optimization on HPO Benchmarks without Any Wait.](http://arxiv.org/abs/2305.17595) | 本研究开发了一个Python封装器，用于在HPO基准测试上模拟多保真度优化，通过强制每个工作进程等待，可以减少多小时的等待时间，使得模拟结果与实际实验的评估顺序完全一致。 |
| [^109] | [Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication.](http://arxiv.org/abs/2305.17341) | 本文提出一种使用近似数值算术同态加密方案进行隐私保护PCA的新方法，相对以往方法，其在效率、准确性和可扩展性上均有提升，实现了同态矩阵乘法和高效同态电路，计算特征值和特征向量时具有良好的效果。 |
| [^110] | [SVDinsTN: An Integrated Method for Tensor Network Representation with Efficient Structure Search.](http://arxiv.org/abs/2305.14912) | SVDinsTN是一种高效的张量网络表示方法，通过在完全连接的张量网络中插入对角因子，同时计算张量核和对角因子，从而实现最紧凑的TN结构。与现有的TN-SS方法相比，SVDinsTN在速度方面加快了10到10^3倍，并且保持了相当的水平。 |
| [^111] | [A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism.](http://arxiv.org/abs/2305.10631) | 提出了一种基于多尺度特征金字塔网络和双重注意力机制的子腹部MRI图像分割算法，使用空洞卷积和多尺度特征金字塔编码以避免语义差距，设计双重注意力机制以保持空间信息并减少错位。 |
| [^112] | [Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification.](http://arxiv.org/abs/2305.09671) | 深度图像分类数据污染攻击存在检测性与鲁棒性之争：污染太少导致攻击失效，污染太多易被检测到。该论文提出两种防御措施，对有限的信任图像标签对进行后处理来检测和修复被污染的模型，并证明其有效性。 |
| [^113] | [Algorithmic Censoring in Dynamic Learning Systems.](http://arxiv.org/abs/2305.09035) | 本文介绍了动态学习系统中可能出现的审查现象，并且提出了防范审查的措施以及随机探索，从而确保来自被审查组的样本进入训练数据，并纠正模型。 |
| [^114] | [Physics Informed Token Transformer.](http://arxiv.org/abs/2305.08757) | 本研究提出了一种名为PITT的物理信息化的Token Transformer模型，通过将偏微分方程嵌入学习过程中，使得模型能够融入物理知识，并在多个PDE应用中展现出性能和优势。 |
| [^115] | [Mastering Percolation-like Games with Deep Learning.](http://arxiv.org/abs/2305.07687) | 研究使用单人游戏和深度学习在网络攻击中的应用，利用训练的代理人和不同的鲁棒性定义，发现优化攻击或防御网络对特定目标非常敏感。 |
| [^116] | [Boosting Distributed Machine Learning Training Through Loss-tolerant Transmission Protocol.](http://arxiv.org/abs/2305.04279) | 通过设计容忍丢失传输协议（LTP），提高了分布式机器学习训练的速度和吞吐量，该协议允许部分梯度丢失，并通过乱序传输和乱序确认进行实现。 |
| [^117] | [Learning Mixtures of Gaussians with Censored Data.](http://arxiv.org/abs/2305.04127) | 本文提出了一种学习高斯混合模型的算法，该算法仅需要很少的样本且能够对权重和均值进行准确估计。 |
| [^118] | [String Diagrams with Factorized Densities.](http://arxiv.org/abs/2305.02506) | 本文描述了一个定义在随机变量集上的联合密度的范畴及其意义，以帮助概率编程和因果推断中的组合推理。 |
| [^119] | [Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models.](http://arxiv.org/abs/2305.02279) | 本文提出了一种机器学习范式 Learngene，将积累的知识压缩成更为紧凑的信息片段并继承给后代模型，以便于适应新的环境 |
| [^120] | [Domain Generalization for Mammographic Image Analysis via Contrastive Learning.](http://arxiv.org/abs/2304.10226) | 研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。 |
| [^121] | [Non-Proportional Parametrizations for Stable Hypernetwork Learning.](http://arxiv.org/abs/2304.07645) | 本文提出一种针对当前超网络训练策略不稳定、收敛速度慢的问题的解决方案，通过使用非比例加性参数化的方式来修订超网络形式，实现更加稳定和快速的训练。 |
| [^122] | [Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models.](http://arxiv.org/abs/2304.07396) | 本文研究了使用大型语言模型InstructGPT辅助医生预筛选患者是否符合临床试验资格。通过10个合成患者简况的性能评估，展示了LLMs在识别筛选资格标准、单独分类、整体分类、以及需要筛选资格标准的百分比上的表现。 |
| [^123] | [Amortized Learning of Dynamic Feature Scaling for Image Segmentation.](http://arxiv.org/abs/2304.05448) | 该研究提出了一种新的超网络策略，可以根据缩放因子快速生成 Pareto 前沿，无需训练多个网络。该方法能够在使用更少的参数和计算的情况下实现最先进的结果。 |
| [^124] | [Improving Identity-Robustness for Face Models.](http://arxiv.org/abs/2304.03838) | 该论文探讨了在没有身份注释信息的情况下，使用人脸识别嵌入向量作为身份标识的替代方法，以提高人脸模型的身份鲁棒性和公平性。 |
| [^125] | [PeakNet: An Autonomous Bragg Peak Finder with Deep Neural Networks.](http://arxiv.org/abs/2303.15301) | PeakNet是一个利用深度神经网络的自动Bragg峰点寻找器，它通过实时调整来适应逐发强背景散射的波动，消除了手动调整算法参数的需求，减少了误报峰点的数量。 |
| [^126] | [Meta-Calibration Regularized Neural Networks.](http://arxiv.org/abs/2303.15057) | 本研究扩展了元校准的方法，引入了gamma网络和平滑的预期校准误差，实现了更好的神经网络校准。该方法在保持预测性能的同时解决了深度神经网络中的误校准问题。 |
| [^127] | [Can AI-Generated Text be Reliably Detected?.](http://arxiv.org/abs/2303.11156) | 本研究通过实证和理论分析表明，在实际场景中，几种AI文本检测器不可靠。改写攻击可以破解多种检测器，包括水印方案、神经网络检测器和零样本分类器。即使是最好的检测器，随着语言模型的进一步提升，性能也会下降。因此，AI生成的文本的可靠检测仍然是一个挑战。 |
| [^128] | [Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2303.01170) | 本文提出了Expert-Free Online Transfer Learning (EF-OnTL)算法，在多智能体系统中实现无专家的实时迁移学习。通过动态选择迁移源智能体和要转移的知识，解决了传统迁移学习需要对专家智能体任务有良好理解的问题。 |
| [^129] | [Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms.](http://arxiv.org/abs/2303.00515) | 本研究提出一种基于时空因果关系的新型水位预测模型，通过将因果结构形式化为多层网络和使用蒙版方法，提高了其可解释性，运用于汉江数据集的实际分析中表现优异。 |
| [^130] | [Differentially Private Algorithms for the Stochastic Saddle Point Problem with Optimal Rates for the Strong Gap.](http://arxiv.org/abs/2302.12909) | 本研究提出了一种在差分隐私约束下解决convex-concave Lipschitz随机Saddle Point问题的方法，并证明了在满足条件的情况下，该方法具有最佳速率和梯度复杂度。 |
| [^131] | [Variable Importance Matching for Causal Inference.](http://arxiv.org/abs/2302.11715) | 这项研究提出了一种名为“模型匹配”的通用框架，通过学习距离度量，创建匹配组并估计治疗效应，实现了可审核、易排查、准确估计和可扩展的观察性因果推断方法。该框架使用变量重要性测量构建距离度量，并通过LASSO操作化实施，在不需要正确规定线性模型的情况下，实现了在潜在混淆变量数量上的可扩展性及准确性。 |
| [^132] | [Temporal Robustness against Data Poisoning.](http://arxiv.org/abs/2302.03684) | 该论文提出了一种针对数据污染的时序威胁模型，通过利用数据的时间戳，引入了提前时间和持续时间这两个指标，从而定义了数据污染的时序鲁棒性，并提供了一种有效的保护方法。 |
| [^133] | [KDEformer: Accelerating Transformers via Kernel Density Estimation.](http://arxiv.org/abs/2302.02451) | KDEformer通过核密度估计加速变换器的注意力计算，提供了次二次时间内的近似计算，并在实证验证中显示出优异的性能。 |
| [^134] | [Transformers Meet Directed Graphs.](http://arxiv.org/abs/2302.00049) | 这项工作提出了两种有向图的方向和结构感知的位置编码，通过应用于排序网络的正确性测试和源代码理解等任务中，该模型相对于之前的最新技术提升了14.7%。 |
| [^135] | [Continual Learning for Predictive Maintenance: Overview and Challenges.](http://arxiv.org/abs/2301.12467) | 深度学习技术在解决工程问题中起到重要作用，预测性维护方法可以提升对维护需求的准确预测。然而，由于问题状态的变化，传统的固定训练模型存在适应性差的问题。持续学习方法提出了在部署后不断调整模型以适应演变场景的方案，但仍面临一些挑战。 |
| [^136] | [SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient.](http://arxiv.org/abs/2301.11913) | 我们提出了SWARM并行性，一种用于训练大模型的模型并行算法，适用于连接差、异构和不可靠设备。通过在节点之间创建临时的随机化管道并进行重新平衡，SWARM可以实现更少的通信密集度。与现有的大规模训练方法相比，我们的方法具有更好的性能，并与压缩策略结合使用来训练大型Transformer语言模型。 |
| [^137] | [Data-Driven Linear Complexity Low-Rank Approximation of General Kernel Matrices: A Geometric Approach.](http://arxiv.org/abs/2212.12674) | 本文提出了一种基于几何方法的数据驱动线性复杂度低秩逼近算法，适用于大规模、任意分布的矩形核矩阵，可用于高斯过程回归等应用场景。 |
| [^138] | [Explainability in Practice: Estimating Electrification Rates from Mobile Phone Data in Senegal.](http://arxiv.org/abs/2211.06277) | 本论文介绍了使用XAI的一个实际案例：在塞内加尔的移动电话数据上基于ML模型进行电气化率估计。研究发现该模型存在人口密度偏见，并指出了数据处理和模型设计方面的挑战，以及解释的解释的重要性。 |
| [^139] | [Data-free Defense of Black Box Models Against Adversarial Attacks.](http://arxiv.org/abs/2211.01579) | 本研究提出了一种无数据情况下对黑盒模型进行防御的方法，通过生成模型构建合成数据，并使用模型窃取技术训练替代模型网络，同时采用小波噪声去除器（WNR）减少对抗性污染。 |
| [^140] | [Transformers over Directed Acyclic Graphs.](http://arxiv.org/abs/2210.13148) | 本文研究了在有向无环图上的Transformer。通过改进注意机制和位置编码，本方法在多种任务上表现出了很好的效果。 |
| [^141] | [On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness.](http://arxiv.org/abs/2210.10464) | 本文研究了强化学习中预训练的泛化能力。当与目标环境交互不允许时，最好的结果是接近最优的策略；当允许交互时，预训练的改进最多是一个常数因子。在非渐近情况下，我们设计了一个高效算法，并证明了与状态动作空间无关的目标环境遗憾界。 |
| [^142] | [Log-linear Guardedness and its Implications.](http://arxiv.org/abs/2210.10012) | 本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。 |
| [^143] | [Query-based Hard-Image Retrieval for Object Detection at Test Time.](http://arxiv.org/abs/2209.11559) | 该论文研究了测试时间对象检测中的查询驱动困难图像检索问题，提出了一种基于查询的简单直观方法来解决这个问题。 |
| [^144] | [MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier.](http://arxiv.org/abs/2209.11549) | 本论文提出了一种名为MAGIC的方法，通过反转准鲁棒分类器进行一次性掩码引导的图像合成。它通过聚合梯度并利用强空间先验的指导二进制掩码，实现了形状和位置控制、非刚性形状变形以及复制/移动操作，并可简单指定二进制引导掩码来提供强大的合成控制。 |
| [^145] | [Lossy Image Compression with Conditional Diffusion Models.](http://arxiv.org/abs/2209.06950) | 本文提出了一种利用条件扩散模型进行有损图像压缩的优化框架。通过引入额外的内容潜变量以及合成纹理变量，该方法在图像质量评估指标上表现出更强的性能。 |
| [^146] | [A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks.](http://arxiv.org/abs/2208.12136) | 本论文比较了软件测试任务中不同强化学习框架的效果和性能，并指出当前文献中缺乏对DRL框架中实现算法的实证评估和指导方针。 |
| [^147] | [Language Models as Knowledge Embeddings.](http://arxiv.org/abs/2206.12617) | 该论文提出了一种使用语言模型来推导知识嵌入的方法LMKE，它旨在提高对丰富的长尾实体的表示能力并解决基于描述的先前方法的问题，实验结果表明该方法在多个基准数据集上实现了最先进的性能。 |
| [^148] | [Superiority of GNN over NN in generalizing bandlimited functions.](http://arxiv.org/abs/2206.05904) | 本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。 |
| [^149] | [On-device modeling of user's social context and familiar places from smartphone-embedded sensor data.](http://arxiv.org/abs/2205.08790) | 本研究提出了一种新颖的、无监督的轻量级方法，通过在用户的移动设备上建模用户的社交背景和地点，从而从手机嵌入式传感器数据中提取高层次和语义丰富的上下文特征。 |
| [^150] | [Covariance-aware Feature Alignment with Pre-computed Source Statistics for Test-time Adaptation to Multiple Image Corruptions.](http://arxiv.org/abs/2204.13263) | 本文提出了一种使用预计算源统计数据的协方差感知的特征对齐方法，用于对多种图像污染进行测试时间自适应。现有的测试时间自适应方法在面对多种污染时的适应能力有限。本文通过解决复杂的分布偏移问题，提高了对多种污染的自适应能力。 |
| [^151] | [Joint Multi-view Unsupervised Feature Selection and Graph Learning.](http://arxiv.org/abs/2204.08247) | 本文提出了一种联合多视图无监督特征选择和图学习的方法，通过正交分解建模多视图特征选择，应用跨空间局部保持进行聚类结构学习和相似性学习的连接。 |
| [^152] | ["That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks.](http://arxiv.org/abs/2204.04636) | 这项工作提出了一个模型无关的对抗文本检测器，通过识别目标分类器的概率中的模式来改进对抗输入的识别性能，并具有较强的泛化能力。 |
| [^153] | [Structure from Voltage.](http://arxiv.org/abs/2203.00063) | 本论文研究了一种基于电压法的结构建模方法，通过使用缩放电阻可以有效得到电压和有效电阻的极限，同时通过添加一个"地面"节点可以简单自然地计算所有距离。 |
| [^154] | [Did AI get more negative recently?.](http://arxiv.org/abs/2202.13610) | 本文通过对自然语言处理和机器学习领域的论文进行分类和分析，发现随着时间的推移，科学文章更倾向于积极的立场，但也存在一些持消极立场的论文。 |
| [^155] | [Gradient flows on graphons: existence, convergence, continuity equations.](http://arxiv.org/abs/2111.09459) | 本论文研究了图上的梯度流问题，发现在大图的边权重适当函数的欧几里得梯度流收敛到图函数空间上一条新型连续极限。许多自然函数在该设置下都得到了涵盖，例如同态函数和标量熵。 |
| [^156] | [Toward a Perspectivist Turn in Ground Truthing for Predictive Computing.](http://arxiv.org/abs/2109.04270) | 本文提出了一种新的范式，数据透视主义，用于机器学习中的知识表示步骤。这种方法整合了人类参与者的观点和角度，相较于传统黄金标准数据集，具有更多潜力和优势。 |
| [^157] | [CDMA: A Practical Cross-Device Federated Learning Algorithm for General Minimax Problems.](http://arxiv.org/abs/2105.14216) | 本文提出了一种名为CDMA的交叉设备联合学习算法，用于解决一般极小化问题。该算法利用“一旦有足够的响应，立即开始”的机制，有效地在涉及到不可靠移动/IoT设备的设置中进行聚合，具有实际可行性和广泛的应用前景。 |
| [^158] | [Simulation of Human and Artificial Emotion (SHArE).](http://arxiv.org/abs/2011.02151) | 这项研究介绍了模拟人类和人工情感的框架，为人类的心理健康问题提供了新的治疗方案，并且为人工智能提供了一种观察机器情感和动机的方法。 |
| [^159] | [Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners.](http://arxiv.org/abs/2009.02476) | 本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设，发现人们假设学习者具有高的折扣率和高度重视探索，并根据学习者进展调整教学策略。 |
| [^160] | [Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms.](http://arxiv.org/abs/1911.06253) | 这项工作介绍了一种具有非对称几何散射变换的图神经网络，通过引入一类非对称小波，它统一和扩展了现有图形散射架构的理论结果，并为未来的深度学习架构为图形提供了基础。 |

# 详细

[^1]: 一种通过多任务异构训练实现高效通用模块化视觉模型的研究

    An Efficient General-Purpose Modular Vision Model via Multi-Task Heterogeneous Training. (arXiv:2306.17165v1 [cs.CV])

    [http://arxiv.org/abs/2306.17165](http://arxiv.org/abs/2306.17165)

    本文提出了一种通过多任务异构训练实现高效通用模块化视觉模型的方法，以应对在视觉任务之间的大量内在差异，并解决多任务模型扩展的挑战。

    

    我们提出了一种模型，可以执行多个视觉任务，并且可以高效地适应其他后续任务。尽管在多任务学习方面取得了相当大的进展，但大多数工作都集中在从多标签数据中学习：即单个图像集合具有多个任务标签。这种多标签数据集很少、规模小且昂贵。我们将异构指的是具有不同任务标签的图像集，或者是单一任务数据集的组合。很少有人研究在这种异构数据集上进行训练。通用视觉模型仍然以单一任务预训练为主导，如何通过利用设计用于不同目的的主流视觉数据集来扩展多任务模型仍然不清楚。挑战在于管理视觉任务之间的大量内在差异，包括数据分布、架构、任务特定模块、数据集规模和采样策略。为了解决这些挑战，我们提出了修改和扩展专家混合(MoE)视觉转换的方法。

    We present a model that can perform multiple vision tasks and can be adapted to other downstream tasks efficiently. Despite considerable progress in multi-task learning, most efforts focus on learning from multi-label data: a single image set with multiple task labels. Such multi-label data sets are rare, small, and expensive. We say heterogeneous to refer to image sets with different task labels, or to combinations of single-task datasets. Few have explored training on such heterogeneous datasets. General-purpose vision models are still dominated by single-task pretraining, and it remains unclear how to scale up multi-task models by leveraging mainstream vision datasets designed for different purposes. The challenges lie in managing large intrinsic differences among vision tasks, including data distribution, architectures, task-specific modules, dataset scales, and sampling strategies. To address these challenges, we propose to modify and scale up mixture-of-experts (MoE) vision trans
    
[^2]: 统计聚合的本地风险界限

    Local Risk Bounds for Statistical Aggregation. (arXiv:2306.17151v1 [math.ST])

    [http://arxiv.org/abs/2306.17151](http://arxiv.org/abs/2306.17151)

    本文通过将全局复杂度替换为较小的局部复杂度来重新审视和加强了统计聚合理论中的经典结果。

    

    在聚合问题中，目标是将给定类别的基本预测器组合起来，以实现几乎与最佳预测器一样准确的预测结果。在这个灵活的框架中，对类别的结构或目标的性质不做任何假设。聚合在顺序和统计上下文中都有研究。尽管这两个问题之间有一些重要的差异，但两种情况下的经典结果具有相同的全局复杂度度量。在本文中，通过用较小的局部复杂度替换全局复杂度，我们重新审视和加强了统计聚合理论中的经典结果。我们的一些证明基于Catoni引入的PAC-Bayes本地化技术。在其他结果中，我们证明了由Leung和Barron提出的指数权重估计器的局部版本的经典界限，以及Q-聚合估计器的偏差最优界限。这些界限改进了Dai，Rigollet和Zhang关于固定的结果。

    In the problem of aggregation, the aim is to combine a given class of base predictors to achieve predictions nearly as accurate as the best one. In this flexible framework, no assumption is made on the structure of the class or the nature of the target. Aggregation has been studied in both sequential and statistical contexts. Despite some important differences between the two problems, the classical results in both cases feature the same global complexity measure. In this paper, we revisit and tighten classical results in the theory of aggregation in the statistical setting by replacing the global complexity with a smaller, local one. Some of our proofs build on the PAC-Bayes localization technique introduced by Catoni. Among other results, we prove localized versions of the classical bound for the exponential weights estimator due to Leung and Barron and deviation-optimal bounds for the Q-aggregation estimator. These bounds improve over the results of Dai, Rigollet and Zhang for fixed
    
[^3]: 利用径向基函数支持向量机对小行星轨道进行分类

    Orbit Classification of asteroids using implementation of radial Basis Function on Support Vector Machines. (arXiv:2306.17138v1 [astro-ph.EP])

    [http://arxiv.org/abs/2306.17138](http://arxiv.org/abs/2306.17138)

    本研究论文实现了径向基函数支持向量机(RBF SVM)用于对小行星轨道进行分类。研究结果表明，RBF SVM算法在效率和准确性方面表现良好，并提供了最佳参数设置。

    

    本研究论文主要关注利用径向基函数支持向量机(RBF SVM)对小行星轨道进行分类的实现。小行星是重要的天体对象，其轨道对于理解太阳系动力学起着关键作用。国际天文学联合会维护着提供各种机器学习技术实验的数据档案。在本研究中，我们探讨了应用RBF SVM算法对小行星进行分类的方法。结果显示，RBF SVM算法对数据集提供了良好的效率和准确性。我们还分析了不同参数对RBF SVM算法性能的影响，并提供了最佳参数设置。我们的研究突出了利用机器学习技术对小行星轨道进行分类的重要性以及RBF SVM算法在这方面的有效性。

    This research paper focuses on the implementation of radial Basis Function (RBF) Support Vector Machines (SVM) for classifying asteroid orbits. Asteroids are important astronomical objects, and their orbits play a crucial role in understanding the dynamics of the solar system. The International Astronomical Union maintains data archives that provide a playground to experiment with various machine-learning techniques. In this study, we explore the application of RBF SVM algorithm to classify asteroids. The results show that the RBF SVM algorithm provides a good efficiency and accuracy to the dataset. We also analyze the impact of various parameters on the performance of the RBF SVM algorithm and present the optimal parameter settings. Our study highlights the importance of using machine learning techniques for classifying asteroid orbits and the effectiveness of the RBF SVM algorithm in this regard.
    
[^4]: 通过生成对抗网络(GANs)生成用于信用卡欺诈检测的合成人口数据

    Synthetic Demographic Data Generation for Card Fraud Detection Using GANs. (arXiv:2306.17109v1 [cs.LG])

    [http://arxiv.org/abs/2306.17109](http://arxiv.org/abs/2306.17109)

    该论文提出了一种名为DGGAN的深度学习生成对抗网络，用于生成合成人口数据，以用于信用卡欺诈检测。通过使用相对复杂的合成人口数据，可以提高交易数据特征的复杂性，并提升欺诈检测性能。

    

    在许多领域中，使用机器学习模型生成合成数据已经变得普遍。生成可以用于检测欺诈的合成交易数据的技术也在快速发展。一般来说，这些合成数据只包含交易的信息，例如时间、地点和金额。通常不包含个体用户的特征（年龄和性别偶尔会包含）。使用相对复杂的合成人口数据可能提高交易数据特征的复杂性，从而提高欺诈检测性能。受益于机器学习的发展，一些深度学习模型具有超过其他成熟的合成数据生成方法（如微模拟）的潜力。在本研究中，我们构建了一个名为DGGAN的深度学习生成对抗网络，用于生成人口数据。我们的模型在模型训练期间生成样本，我们发现这是重要的。

    Using machine learning models to generate synthetic data has become common in many fields. Technology to generate synthetic transactions that can be used to detect fraud is also growing fast. Generally, this synthetic data contains only information about the transaction, such as the time, place, and amount of money. It does not usually contain the individual user's characteristics (age and gender are occasionally included). Using relatively complex synthetic demographic data may improve the complexity of transaction data features, thus improving the fraud detection performance. Benefiting from developments of machine learning, some deep learning models have potential to perform better than other well-established synthetic data generation methods, such as microsimulation. In this study, we built a deep-learning Generative Adversarial Network (GAN), called DGGAN, which will be used for demographic data generation. Our model generates samples during model training, which we found importan
    
[^5]: ManimML：用动画演示机器学习架构

    ManimML: Communicating Machine Learning Architectures with Animation. (arXiv:2306.17108v1 [cs.LG])

    [http://arxiv.org/abs/2306.17108](http://arxiv.org/abs/2306.17108)

    ManimML是一个开源Python库，通过动画演示自动生成的ML算法，为机器学习从业者提供了一种简单且熟悉的方式来沟通和可视化ML算法。

    

    近年来，由于机器学习在科学和工程领域的应用，对机器学习（ML）的兴趣激增。然而，随着ML技术的发展，解释和可视化新颖的ML算法的工具还远远落后。动画已被证明是一种强大的工具，可以制作出随时间动态变化的系统的吸引人可视化效果，非常适合用于沟通ML算法的任务。然而，目前动画化ML算法的方法是手工制作突出特定算法或使用复杂的通用动画软件。我们开发了ManimML，这是一个开源的Python库，可以直接从代码中轻松生成ML算法的动画。我们旨在利用ML从业者对编程的现有知识，而不是要求他们学习复杂的动画软件。ManimML具有熟悉的语法，用于指定模仿流行的深度学习框架如Pytorch的神经网络。

    There has been an explosion in interest in machine learning (ML) in recent years due to its applications to science and engineering. However, as ML techniques have advanced, tools for explaining and visualizing novel ML algorithms have lagged behind. Animation has been shown to be a powerful tool for making engaging visualizations of systems that dynamically change over time, which makes it well suited to the task of communicating ML algorithms. However, the current approach to animating ML algorithms is to handcraft applications that highlight specific algorithms or use complex generalized animation software. We developed ManimML, an open-source Python library for easily generating animations of ML algorithms directly from code. We sought to leverage ML practitioners' preexisting knowledge of programming rather than requiring them to learn complex animation software. ManimML has a familiar syntax for specifying neural networks that mimics popular deep learning frameworks like Pytorch.
    
[^6]: 神经元实际上是折叠的吗？关于神经表示中的细粒度结构的研究

    Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations. (arXiv:2306.17105v1 [cs.LG])

    [http://arxiv.org/abs/2306.17105](http://arxiv.org/abs/2306.17105)

    这篇论文研究了神经网络中的“神经折叠”现象，提出了表面上折叠的表示实际上仍隐藏有重要的细粒度结构，并通过实验证据证明了这一点。

    

    最近的研究观察到在训练充分的神经网络中出现了一个有趣的“神经折叠”现象，即具有相同标签的训练样本的最后一层表示互相折叠在一起。这似乎表明最后一层的表示完全由标签决定，并且不依赖于输入分布的内在结构。我们提供证据表明这不是一个完全的描述，表面上的折叠隐藏了表示中的重要的细粒度结构。具体而言，即使表示表面上折叠在一起，仍然存在一小部分剩余的变异性能够忠实而准确地捕捉到输入分布的内在结构。例如，如果我们只使用5个粗粒度标签（将两个类别组合成一个超类）对CIFAR-10进行训练直到收敛，我们可以通过无监督聚类从学习到的表示中重构出原始的10个类别标签。

    Recent work has observed an intriguing ''Neural Collapse'' phenomenon in well-trained neural networks, where the last-layer representations of training samples with the same label collapse into each other. This appears to suggest that the last-layer representations are completely determined by the labels, and do not depend on the intrinsic structure of input distribution. We provide evidence that this is not a complete description, and that the apparent collapse hides important fine-grained structure in the representations. Specifically, even when representations apparently collapse, the small amount of remaining variation can still faithfully and accurately captures the intrinsic structure of input distribution. As an example, if we train on CIFAR-10 using only 5 coarse-grained labels (by combining two classes into one super-class) until convergence, we can reconstruct the original 10-class labels from the learned representations via unsupervised clustering. The reconstructed labels a
    
[^7]: 鉴定学习动作技能中重要的感觉反馈

    Identifying Important Sensory Feedback for Learning Locomotion Skills. (arXiv:2306.17101v1 [cs.RO])

    [http://arxiv.org/abs/2306.17101](http://arxiv.org/abs/2306.17101)

    通过深度强化学习学习机器人运动技能时，我们通过一种显著性分析方法定量评估了不同反馈状态的相对重要性。我们发现关节位置、重力向量和速度等关键状态可以实现与使用所有状态相当的步态技能性能。

    

    通过神经网络作为状态-动作映射的深度强化学习(DRL)可以学习机器人的运动技能。虽然状态观测的选择至关重要，但目前缺乏定量分析。在这里，我们提供了一种系统的显著性分析方法，定量评估通过DRL学习到的各种反馈状态的相对重要性。我们的方法可以确定对于步态技能（包括平衡恢复、小跑、跳跃、步速和奔腾）最重要的反馈状态。通过仅使用关键状态，包括关节位置、重力向量、基座的线性和角速度，我们证明了虚拟四足机器人在各种测试场景中可以实现稳健的性能。使用任务性能指标来进行基准测试表明，使用关键状态学习到的步态技能可以达到与使用所有状态学习到的技能相当的性能，以及任务性能或学习成功率。

    Robot motor skills can be learned through deep reinforcement learning (DRL) by neural networks as state-action mappings. While the selection of state observations is crucial, there has been a lack of quantitative analysis to date. Here, we present a systematic saliency analysis that quantitatively evaluates the relative importance of different feedback states for motor skills learned through DRL. Our approach can identify the most essential feedback states for locomotion skills, including balance recovery, trotting, bounding, pacing and galloping. By using only key states including joint positions, gravity vector, base linear and angular velocities, we demonstrate that a simulated quadruped robot can achieve robust performance in various test scenarios across these distinct skills. The benchmarks using task performance metrics show that locomotion skills learned with key states can achieve comparable performance to those with all states, and the task performance or learning success rat
    
[^8]: RL4CO: 用于组合优化的广泛强化学习基准测试

    RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])

    [http://arxiv.org/abs/2306.17100](http://arxiv.org/abs/2306.17100)

    RL4CO是一个用于组合优化的广泛强化学习基准测试，着重于可扩展性和泛化能力的评估，并展示了一些最新方法在样本效率和适应不同数据分布方面的表现相对较差，强调了对神经CO求解器性能的平衡评估的重要性。

    

    我们引入了RL4CO，这是一个广泛的强化学习（RL）用于组合优化（CO）的基准测试。RL4CO采用最先进的软件库和最佳实践，如模块化和配置管理，以便研究人员可以轻松修改神经网络架构、环境和算法。与现有的专注于特定任务（如旅行推销员问题）进行性能评估的方法不同，我们强调可扩展性和泛化能力对于各种优化任务的重要性。我们还系统地评估了各种模型在样本效率、零-shot泛化和适应不同数据分布方面的表现。我们的实验结果表明，一些最新的最先进方法在使用这些新指标进行评估时落后于之前的方法，这表明有必要更加平衡地评估神经CO求解器的性能。我们希望RL4CO能够为研究人员提供一个综合性的基准测试工具，以进一步推动强化学习在组合优化领域的研究。

    We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
    
[^9]: 增强鲁棒特征在减轻灾难性遗忘中的重要性

    The Importance of Robust Features in Mitigating Catastrophic Forgetting. (arXiv:2306.17091v1 [cs.CV])

    [http://arxiv.org/abs/2306.17091](http://arxiv.org/abs/2306.17091)

    本研究着眼于连续学习模型，通过引入鲁棒特征数据集发现，在其上训练的模型比在标准数据集上训练的模型具有更小的灾难性遗忘，从而凸显出增强鲁棒特征在减轻灾难性遗忘中的重要性。

    

    连续学习（CL）是一种应对神经网络在新任务或数据分布上训练时遗忘先前学到知识的方法。鲁棒性对特征进行了分解，将其分为鲁棒和非鲁棒类型，并表明在鲁棒特征上训练的模型显著增强了对抗鲁棒性。然而，至今没有研究关于从CL模型角度评估鲁棒特征在减轻连续学习中的灾难性遗忘方面的有效性。本文介绍了CL鲁棒数据集，并在标准数据集和CL鲁棒数据集上训练了四个基准模型。我们的结果表明，在CL鲁棒数据集上训练的CL模型对先前学习任务的灾难性遗忘较少，而在标准数据集上训练的模型会遗忘较多。我们的观察强调了提供给底层CL模型的特征的重要性，显示CL鲁棒特征可以减轻灾难性遗忘。

    Continual learning (CL) is an approach to address catastrophic forgetting, which refers to forgetting previously learned knowledge by neural networks when trained on new tasks or data distributions. The adversarial robustness has decomposed features into robust and non-robust types and demonstrated that models trained on robust features significantly enhance adversarial robustness. However, no study has been conducted on the efficacy of robust features from the lens of the CL model in mitigating catastrophic forgetting in CL. In this paper, we introduce the CL robust dataset and train four baseline models on both the standard and CL robust datasets. Our results demonstrate that the CL models trained on the CL robust dataset experienced less catastrophic forgetting of the previously learned tasks than when trained on the standard dataset. Our observations highlight the significance of the features provided to the underlying CL models, showing that CL robust features can alleviate catast
    
[^10]: 通过发现多元时间序列预测中的图模型利用稀疏性

    Sparsity exploitation via discovering graphical models in multi-variate time-series forecasting. (arXiv:2306.17090v1 [cs.LG])

    [http://arxiv.org/abs/2306.17090](http://arxiv.org/abs/2306.17090)

    这项工作提出了一种使用图模型进行多元时间序列预测的方法，通过发现和理解底层图结构的相关性来提高预测性能。通过直接利用数据中的稀疏模式构建图结构，从而解决了在没有显式先验图结构的情况下生成的图不稀疏的问题，提高了模型的计算效率和可解释性。

    

    图神经网络（GNN）广泛应用于多元时间序列预测（MTSF）任务中，因为它们能够捕捉不同时间序列之间的相关性。这些基于图的学习方法通过发现和理解表示数据相关性的底层图结构来提高预测性能。当显式的先验图结构不可用时，大多数现有工作无法保证生成的图的稀疏性，使得整体模型计算复杂且难以解释。在这项工作中，我们提出了一种解耦训练方法，其中包括一个图生成模块和一个GNN预测模块。首先，我们使用图形Lasso（或GraphLASSO）直接从数据中利用稀疏模式构建静态和时变情况下的图结构。其次，我们将这些图结构和输入数据拟合到一个图卷积循环网络（GCRN）中，以训练一个预测模型。

    Graph neural networks (GNNs) have been widely applied in multi-variate time-series forecasting (MTSF) tasks because of their capability in capturing the correlations among different time-series. These graph-based learning approaches improve the forecasting performance by discovering and understanding the underlying graph structures, which represent the data correlation. When the explicit prior graph structures are not available, most existing works cannot guarantee the sparsity of the generated graphs that make the overall model computational expensive and less interpretable. In this work, we propose a decoupled training method, which includes a graph generating module and a GNNs forecasting module. First, we use Graphical Lasso (or GraphLASSO) to directly exploit the sparsity pattern from data to build graph structures in both static and time-varying cases. Second, we fit these graph structures and the input data into a Graph Convolutional Recurrent Network (GCRN) to train a forecasti
    
[^11]: 基于大型语言模型的概念导向深度学习

    Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v1 [cs.LG])

    [http://arxiv.org/abs/2306.17089](http://arxiv.org/abs/2306.17089)

    本文讨论了大型语言模型在概念导向深度学习中的应用，包括从文本和图像中提取概念和概念图。同时也探讨了多模态语言模型在表达人类知识方面的优势。

    

    大型语言模型（LLMs）已成功应用于许多自然语言任务和应用，包括文本生成和人工智能聊天机器人。它们也是概念导向深度学习（CODL）的一种有前景的新技术。然而，前提是LLMs要理解概念并确保概念一致性。本文讨论了这些问题，以及LLMs在CODL中的主要用途，包括从文本中提取概念、从文本中提取概念图和概念学习。人类知识包括符号（概念性）知识和具体（感性）知识。而仅文本的LLMs只能表示符号（概念性）知识。另一方面，多模态LLMs能够表示人类知识的完整范围（概念性和感性）。我们讨论了视觉-语言LLMs中的概念理解，这是最重要的多模态LLMs，并介绍了它们在CODL中的主要用途，包括从图像中提取概念、从图像中提取概念图。

    Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image
    
[^12]: 关于神经时态点过程模型在连续时间事件数据上的预测准确性

    On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data. (arXiv:2306.17066v1 [cs.LG])

    [http://arxiv.org/abs/2306.17066](http://arxiv.org/abs/2306.17066)

    该论文通过全面大规模实验研究，系统评估了最先进的神经TPP模型在预测准确性方面的效果，并发现了关键因素。

    

    时态点过程（TPP）是连续时间中建模异步事件序列的标准数学框架。然而，传统的TPP模型常常受限于强假设，限制了其对复杂真实世界事件动态的捕捉能力。为了克服这一限制，研究人员提出了神经TPP，利用神经网络参数化来提供更灵活和高效的建模。虽然最近的研究表明神经TPP的有效性，但它们通常缺乏统一的设置，依赖不同的基线、数据集和实验配置。这使得难以确定推动预测准确性改进的关键因素，阻碍了研究进展。为了弥补这一差距，我们提出了一项全面的大规模实验研究，系统评估了最先进的神经TPP模型的预测准确性。我们的研究涵盖了多个真实世界和合成事件序列数据集。

    Temporal Point Processes (TPPs) serve as the standard mathematical framework for modeling asynchronous event sequences in continuous time. However, classical TPP models are often constrained by strong assumptions, limiting their ability to capture complex real-world event dynamics. To overcome this limitation, researchers have proposed Neural TPPs, which leverage neural network parametrizations to offer more flexible and efficient modeling. While recent studies demonstrate the effectiveness of Neural TPPs, they often lack a unified setup, relying on different baselines, datasets, and experimental configurations. This makes it challenging to identify the key factors driving improvements in predictive accuracy, hindering research progress. To bridge this gap, we present a comprehensive large-scale experimental study that systematically evaluates the predictive accuracy of state-of-the-art neural TPP models. Our study encompasses multiple real-world and synthetic event sequence datasets, 
    
[^13]: 使用mmWave Wi-Fi接入点的姿势识别：经验教训

    Gesture Recognition with mmWave Wi-Fi Access Points: Lessons Learned. (arXiv:2306.17062v1 [cs.LG])

    [http://arxiv.org/abs/2306.17062](http://arxiv.org/abs/2306.17062)

    本研究探索了使用mmWave Wi-Fi信号进行手势识别/姿势估计，通过提取信号-to-noise ratios (SNRs) 和利用深度神经网络（DNN）进行特征提取，实现了在单一环境中96.7%的准确率。

    

    近年来，利用6 GHz以下的信道状态信息（CSI）广泛应用于Wi-Fi感知，特别是活动和手势识别。本文中，我们探索了mmWave（60 GHz）Wi-Fi信号用于手势识别/姿势估计。我们的重点是mmWave Wi-Fi信号，不仅可以用于高速数据通信，还可以用于提高感知，在XR应用中更有用。为此，我们从IEEE 802.11ad设备使用的周期性波束训练中提取空间波束信号-to-noise ratios (SNRs) 。我们考虑了一组由XR应用驱动的10个手势/姿势。我们在两个环境和三人身上进行实验。作为比较，我们还收集了IEEE 802.11ac设备的CSI。我们利用深度神经网络（DNN）从CSI和波束SNR中提取特征。DNN分类器在单一环境中的波束SNR任务上取得了令人满意的结果，达到了最新技术水平的96.7%的准确率。

    In recent years, channel state information (CSI) at sub-6 GHz has been widely exploited for Wi-Fi sensing, particularly for activity and gesture recognition. In this work, we instead explore mmWave (60 GHz) Wi-Fi signals for gesture recognition/pose estimation. Our focus is on the mmWave Wi-Fi signals so that they can be used not only for high data rate communication but also for improved sensing e.g., for extended reality (XR) applications. For this reason, we extract spatial beam signal-to-noise ratios (SNRs) from the periodic beam training employed by IEEE 802.11ad devices. We consider a set of 10 gestures/poses motivated by XR applications. We conduct experiments in two environments and with three people.As a comparison, we also collect CSI from IEEE 802.11ac devices. To extract features from the CSI and the beam SNR, we leverage a deep neural network (DNN). The DNN classifier achieves promising results on the beam SNR task with state-of-the-art 96.7% accuracy in a single environme
    
[^14]: 安全的基于模型的多智能体均场强化学习

    Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2306.17052v1 [cs.LG])

    [http://arxiv.org/abs/2306.17052](http://arxiv.org/abs/2306.17052)

    本文提出了Safe-$\text{M}^3$-UCRL算法，通过使用模型中的认知不确定性和对数障碍方法，实现了在未知转移动态情况下达到安全策略的优化，成功解决了大规模多智能体协调问题。

    

    许多应用，比如共享交通，需要协调大量的智能体。均场强化学习通过优化代表性智能体的策略来应对由此带来的可扩展性挑战。在本文中，我们解决了一个重要的泛化问题，即智能体分布存在全局约束的情况（例如需要满足容量约束或最小覆盖要求）。我们提出了Safe-$\text{M}^3$-UCRL，这是第一个能够在未知转移动态的情况下实现安全策略的基于模型的算法。作为一个关键因素，它在保证悲观约束满足的同时，利用转移模型中的认知不确定性来使用对数障碍方法确保高概率。我们在许多共享交通运营商面临的车辆重定位问题上展示了Safe-$\text{M}^3$-UCRL，并通过基于深圳出租车轨迹数据的仿真评估其性能。我们的算法能够有效满足关键需求。

    Many applications, e.g., in shared mobility, require coordinating a large number of agents. Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a representative agent. In this paper, we address an important generalization where there exist global constraints on the distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose Safe-$\text{M}^3$-UCRL, the first model-based algorithm that attains safe policies even in the case of unknown transition dynamics. As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure pessimistic constraints satisfaction with high probability. We showcase Safe-$\text{M}^3$-UCRL on the vehicle repositioning problem faced by many shared mobility operators and evaluate its performance through simulations built on Shenzhen taxi trajectory data. Our algorithm effectively meets the demand in critica
    
[^15]: 单目标与多目标优化质量在进化方程发现中的比较

    Comparison of Single- and Multi- Objective Optimization Quality for Evolutionary Equation Discovery. (arXiv:2306.17038v1 [cs.LG])

    [http://arxiv.org/abs/2306.17038](http://arxiv.org/abs/2306.17038)

    本文比较了单目标和多目标优化在进化方程发现中的质量；单目标优化仅考虑方程中所选项的差异，而多目标优化还考虑了所获得方程的复杂性。

    

    进化微分方程的发现被证明是一种能够获得不需要常规方法（如通过完整可能项库的稀疏符号回归）的先验假设的方程的工具。方程发现领域包含两个独立的方向。第一个方向是纯数学的，涉及到微分、优化的目标以及它们与函数空间等的关系。第二个方向则完全致力于优化问题的陈述。探索这两个主题可以改善算法处理实验数据的能力，以一种更符合人工智能的方式，而不需要进行显著的预处理和对其本质的先验知识。在本文中，我们考虑了在单目标优化（仅考虑方程中所选项之间的差异）和多目标优化（还考虑所获得方程的复杂性）之间的优势。

    Evolutionary differential equation discovery proved to be a tool to obtain equations with less a priori assumptions than conventional approaches, such as sparse symbolic regression over the complete possible terms library. The equation discovery field contains two independent directions. The first one is purely mathematical and concerns differentiation, the object of optimization and its relation to the functional spaces and others. The second one is dedicated purely to the optimizational problem statement. Both topics are worth investigating to improve the algorithm's ability to handle experimental data a more artificial intelligence way, without significant pre-processing and a priori knowledge of their nature. In the paper, we consider the prevalence of either single-objective optimization, which considers only the discrepancy between selected terms in the equation, or multi-objective optimization, which additionally takes into account the complexity of the obtained equation. The pr
    
[^16]: 面向离散和连续强化学习的安全感知任务组合

    Safety-Aware Task Composition for Discrete and Continuous Reinforcement Learning. (arXiv:2306.17033v1 [cs.LG])

    [http://arxiv.org/abs/2306.17033](http://arxiv.org/abs/2306.17033)

    本文研究了布尔组合在强化学习中的应用，通过引入两种安全性概念和拓展到连续行动空间，实现了任务的安全感知组合。

    

    组合性是可扩展系统设计的关键方面。强化学习（RL）最近在任务学习方面取得了重大成功，但是在真正利用组合方面才刚刚开始。在本文中，我们关注学习任务的布尔组合，而不是功能性或顺序性组合。现有的RL布尔组合侧重于在具有离散行动空间的环境中达到一个令人满意的吸收状态，但不支持可组合的安全性（即避免）约束。我们通过三个贡献推进了学习任务布尔组合的最新技术：i）在此框架中引入了两种不同的安全性概念；ii）展示如何强制执行安全语义，证明正确性（在一些假设下），并分析两种安全性概念之间的权衡；iii）将布尔组合从离散行动空间扩展到连续行动空间。我们使用修改版的价值迭代算法来演示这些技术。

    Compositionality is a critical aspect of scalable system design. Reinforcement learning (RL) has recently shown substantial success in task learning, but has only recently begun to truly leverage composition. In this paper, we focus on Boolean composition of learned tasks as opposed to functional or sequential composition. Existing Boolean composition for RL focuses on reaching a satisfying absorbing state in environments with discrete action spaces, but does not support composable safety (i.e., avoidance) constraints. We advance the state of the art in Boolean composition of learned tasks with three contributions: i) introduce two distinct notions of safety in this framework; ii) show how to enforce either safety semantics, prove correctness (under some assumptions), and analyze the trade-offs between the two safety notions; and iii) extend Boolean composition from discrete action spaces to continuous action spaces. We demonstrate these techniques using modified versions of value iter
    
[^17]: milliFlow：用于人体运动感知的毫米波雷达点云场景流估计

    milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])

    [http://arxiv.org/abs/2306.17010](http://arxiv.org/abs/2306.17010)

    milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。

    

    随着普适计算时代的到来，人体运动感知在智能系统中起着关键作用，用于决策、用户交互和个性化服务。在传统方法中，人体跟踪、姿势估计、手势识别和活动识别等方面进行了大量研究，这些方法主要基于摄像机。然而，摄像机的侵入性特点限制了它们在智能家居应用中的使用。为了解决这个问题，毫米波雷达由于其保护隐私的特点而受到欢迎。在这项工作中，我们提出了一种新颖的深度学习方法milliFlow，用于对毫米波雷达点云进行场景流估计，作为中间层的特征，直接受益于下游的人体运动感知任务。实验结果表明，我们的方法具有优越的性能，平均3D端点误差为4.6cm，明显超过竞争方法。此外，通过结合...

    Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
    
[^18]: 频域归一化：谱批量归一化

    Spectral Batch Normalization: Normalization in the Frequency Domain. (arXiv:2306.16999v1 [cs.CV])

    [http://arxiv.org/abs/2306.16999](http://arxiv.org/abs/2306.16999)

    本文介绍了一种新颖的方法——谱批量归一化(SBN)，通过在频域中归一化特征映射，提高了深度神经网络泛化能力。实验证明，尽管有批量归一化(BN)，特征映射在网络开始阶段仍然会爆炸。

    

    正则化是一组用于提高深度神经网络泛化能力的技术。本文介绍了一种新颖的有效方法——谱批量归一化(SBN)，通过在频域中归一化特征映射来提高泛化。在网络深度初始化阶段，无批量归一化(BN)的残差网络的激活往往以指数级增长，导致特征映射范数非常大，尽管参数相对较小。这种爆炸性动态对学习非常有害。BN使得对缩放因子 γ, β 进行权值衰减正则化近似等效于对特征映射范数进行加性惩罚，从一定程度上防止特征映射范数变得过大。然而，我们通过实验证明，尽管 BN 进行了近似的加性惩罚，深度神经网络(DNNs)中的特征映射往往在网络开始阶段仍然会爆炸。

    Regularization is a set of techniques that are used to improve the generalization ability of deep neural networks. In this paper, we introduce spectral batch normalization (SBN), a novel effective method to improve generalization by normalizing feature maps in the frequency (spectral) domain. The activations of residual networks without batch normalization (BN) tend to explode exponentially in the depth of the network at initialization. This leads to extremely large feature map norms even though the parameters are relatively small. These explosive dynamics can be very detrimental to learning. BN makes weight decay regularization on the scaling factors $\gamma, \beta$ approximately equivalent to an additive penalty on the norm of the feature maps, which prevents extremely large feature map norms to a certain degree. However, we show experimentally that, despite the approximate additive penalty of BN, feature maps in deep neural networks (DNNs) tend to explode at the beginning of the net
    
[^19]: Weight Compander: 一种用于正则化的简单权重重新参数化方法

    Weight Compander: A Simple Weight Reparameterization for Regularization. (arXiv:2306.16993v1 [cs.LG])

    [http://arxiv.org/abs/2306.16993](http://arxiv.org/abs/2306.16993)

    本文提出了一种名为权重压缩器（WC）的新方法，通过使用非线性函数对深度神经网络中的每个权重进行重新参数化，从而提高了泛化能力并减少过拟合。

    

    正则化是一种用于提高深度神经网络泛化能力的技术集合。本文引入了一种名为权重压缩器（WC）的新方法，通过使用非线性函数对深度神经网络中的每个权重进行重新参数化来提高泛化能力。它是一种通用、直观、廉价且易于实现的方法，可以与其他各种正则化技术结合使用。深度神经网络中的大权重是过度拟合训练数据的复杂网络的标志。此外，正则化网络往往具有更广范围的接近零的权重，而中心接近零的权重较少。我们引入了一种权重重新参数化函数，将其应用于每个权重，通过限制权重的幅值同时使其远离零来隐式减少过拟合。这导致网络中更加民主的决策过程。首先，个体权重的影响力不太大。

    Regularization is a set of techniques that are used to improve the generalization ability of deep neural networks. In this paper, we introduce weight compander (WC), a novel effective method to improve generalization by reparameterizing each weight in deep neural networks using a nonlinear function. It is a general, intuitive, cheap and easy to implement method, which can be combined with various other regularization techniques. Large weights in deep neural networks are a sign of a more complex network that is overfitted to the training data. Moreover, regularized networks tend to have a greater range of weights around zero with fewer weights centered at zero. We introduce a weight reparameterization function which is applied to each weight and implicitly reduces overfitting by restricting the magnitude of the weights while forcing them away from zero at the same time. This leads to a more democratic decision-making in the network. Firstly, individual weights cannot have too much influ
    
[^20]: 未知环境中的在线覆盖路径规划的端到端强化学习

    End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments. (arXiv:2306.16978v1 [cs.RO])

    [http://arxiv.org/abs/2306.16978](http://arxiv.org/abs/2306.16978)

    本文提出了一种基于端到端强化学习的在线覆盖路径规划方法，能处理未知环境并结合全局地图和局部感知输入，同时考虑长期路径规划和短期障碍物检测。

    

    覆盖路径规划是寻找覆盖给定封闭区域整个自由空间的最短路径的问题，应用范围从机器人割草和吸尘到地雷清除和搜救任务。虽然离线方法可以为已知环境找到可证明完备且在某些情况下是最优的路径，但在在线场景下，环境事先未知，特别是在存在非静态障碍物的情况下，其价值有限。我们提出了一种基于连续状态和动作空间的端到端强化学习方法，用于处理未知环境的在线覆盖路径规划问题。我们从全局地图和局部感知输入构建观察空间，使代理能够规划长期路径，并同时对短期障碍物进行行动。为了考虑大规模环境，我们提出使用多尺度地图输入表示。此外，我们提出了一种新颖的总变差正则化方法以减少路径偏离问题。

    Coverage path planning is the problem of finding the shortest path that covers the entire free space of a given confined area, with applications ranging from robotic lawn mowing and vacuum cleaning, to demining and search-and-rescue tasks. While offline methods can find provably complete, and in some cases optimal, paths for known environments, their value is limited in online scenarios where the environment is not known beforehand, especially in the presence of non-static obstacles. We propose an end-to-end reinforcement learning-based approach in continuous state and action space, for the online coverage path planning problem that can handle unknown environments. We construct the observation space from both global maps and local sensory inputs, allowing the agent to plan a long-term path, and simultaneously act on short-term obstacle detections. To account for large-scale environments, we propose to use a multi-scale map input representation. Furthermore, we propose a novel total var
    
[^21]: Diffusion-Jump GNNs: 可学习度量过滤器的同质化

    Diffusion-Jump GNNs: Homophiliation via Learnable Metric Filters. (arXiv:2306.16976v1 [cs.LG])

    [http://arxiv.org/abs/2306.16976](http://arxiv.org/abs/2306.16976)

    提出了一种名为Diffusion-Jump GNNs的新方法，通过学习可调节的度量过滤器，来提高高阶图神经网络在异质化场景下的效果。这种方法通过跳跃式的渐进扩散距离生成过滤器的支持和系数，以寻找散点之间的联系。

    

    高阶图神经网络（HO-GNNs）被开发用于在异质性范围中推断一致的潜在空间，其中标签分布与图结构无关。然而，大多数现有的HO-GNNs是基于跳数的，即它们依赖于转移矩阵的幂次。因此，这些结构对分类损失的反应不完全，并且所达到的结构化过滤器具有静态支持。换句话说，这些网络不能学习过滤器的支持或系数，而只能学习过滤器的组合。为了解决上述问题，我们提出了基于渐进扩散距离的跳跃扩散GNNs方法。扩散跳跃生成一对一的距离，其投影确定每个结构化过滤器的支持和系数。这些过滤器称为跳跃，因为它们在广泛的尺度范围内探索以找到散点之间的联系。

    High-order Graph Neural Networks (HO-GNNs) have been developed to infer consistent latent spaces in the heterophilic regime, where the label distribution is not correlated with the graph structure. However, most of the existing HO-GNNs are hop-based, i.e., they rely on the powers of the transition matrix. As a result, these architectures are not fully reactive to the classification loss and the achieved structural filters have static supports. In other words, neither the filters' supports nor their coefficients can be learned with these networks. They are confined, instead, to learn combinations of filters. To address the above concerns, we propose Diffusion-jump GNNs a method relying on asymptotic diffusion distances that operates on jumps. A diffusion-pump generates pairwise distances whose projections determine both the support and coefficients of each structural filter. These filters are called jumps because they explore a wide range of scales in order to find bonds between scatter
    
[^22]: 使用等变神经网络恢复翻译

    Restore Translation Using Equivariant Neural Networks. (arXiv:2306.16938v1 [cs.LG])

    [http://arxiv.org/abs/2306.16938](http://arxiv.org/abs/2306.16938)

    本论文提出了一种使用等变神经网络恢复翻译的方法，通过恢复已翻译（甚至旋转）的输入到原始输入，并将其传送到任意分类器中，以提供更好的性能。

    

    对于分类神经网络来说，对于空间变换（如平移和旋转）的不变性是一种理想的属性和基本的设计原则。然而，常用的卷积神经网络（CNNs）实际上对于即使是微小的平移也非常敏感。存在着大量的工作来通过设计具有变换不变性的模型或评估变换来实现精确或近似的变换不变性。这些工作通常对标准CNNs进行更改，并对标准数据集上的性能造成损害。在本文中，与其修改分类器，我们提出了一个预分类恢复器，将已翻译（甚至旋转）的输入恢复到原始输入，并将其传送到相同数据集的任何分类器中。此恢复器基于一个理论结果，该结果给出了一个仿射算子在张量空间上是平移等变的充分且必要条件。

    Invariance to spatial transformations such as translations and rotations is a desirable property and a basic design principle for classification neural networks. However, the commonly used convolutional neural networks (CNNs) are actually very sensitive to even small translations. There exist vast works to achieve exact or approximate transformation invariance by designing transformation-invariant models or assessing the transformations. These works usually make changes to the standard CNNs and harm the performance on standard datasets. In this paper, rather than modifying the classifier, we propose a pre-classifier restorer to recover translated (or even rotated) inputs to the original ones which will be fed into any classifier for the same dataset. The restorer is based on a theoretical result which gives a sufficient and necessary condition for an affine operator to be translational equivariant on a tensor space.
    
[^23]: 线束自动驾驶：挑战与前景

    End-to-end Autonomous Driving: Challenges and Frontiers. (arXiv:2306.16927v1 [cs.RO])

    [http://arxiv.org/abs/2306.16927](http://arxiv.org/abs/2306.16927)

    这项研究调查了端到端自动驾驶领域中的关键挑战和未来趋势，包括多模态、可解释性、因果混淆、鲁棒性和世界模型等。通过联合特征优化感知和规划，端到端系统在感知和规划上获得了更好的效果。

    

    自动驾驶领域正在迅速发展，越来越多的方法采用端到端算法框架，利用原始传感器输入生成车辆运动计划，而不是专注于诸如检测和运动预测等单个任务。与模块化流水线相比，端到端系统通过联合特征优化感知和规划来获益。这一领域因大规模数据集的可用性、闭环评估以及自动驾驶算法在挑战性场景中的有效执行所需的需求而蓬勃发展。在本调查中，我们全面分析了250多篇论文，涵盖了端到端自动驾驶的动机、路线图、方法论、挑战和未来趋势。我们深入探讨了多模态、可解释性、因果混淆、鲁棒性和世界模型等几个关键挑战。此外，我们还讨论了基础技术的最新进展。

    The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation
    
[^24]: OSP: 使用两阶段同步提升分布式模型训练

    OSP: Boosting Distributed Model Training with 2-stage Synchronization. (arXiv:2306.16926v1 [cs.DC])

    [http://arxiv.org/abs/2306.16926](http://arxiv.org/abs/2306.16926)

    OSP是一种新的分布式模型训练方法，通过使用两阶段同步和本地梯度修正来提高通信效率，避免了精度损失。

    

    分布式深度学习（DDL）是一个有前景的研究领域，旨在提高训练大规模数据集和模型的深度学习任务的效率。随着DDL节点的计算能力不断增强，节点之间的网络连接成为一个主要瓶颈。为了解决参数服务器式DDL中这个瓶颈问题，人们提出了各种梯度压缩和改进的模型同步方法。然而，这两种方法可能会导致丢失梯度而出现精度损失，并且对模型同步的吞吐量的提升有限。为了解决这些挑战，我们提出了一种新的模型同步方法，名为Overlapped Synchronization Parallel（OSP），它采用两阶段同步方法实现高效通信，并使用基于本地梯度的参数修正（LGP）来避免由过期参数引起的精度损失。OSP的原型使用PyTo实现。

    Distributed deep learning (DDL) is a promising research area, which aims to increase the efficiency of training deep learning tasks with large size of datasets and models. As the computation capability of DDL nodes continues to increase, the network connection between nodes is becoming a major bottleneck. Various methods of gradient compression and improved model synchronization have been proposed to address this bottleneck in Parameter-Server-based DDL. However, these two types of methods can result in accuracy loss due to discarded gradients and have limited enhancement on the throughput of model synchronization, respectively. To address these challenges, we propose a new model synchronization method named Overlapped Synchronization Parallel (OSP), which achieves efficient communication with a 2-stage synchronization approach and uses Local-Gradient-based Parameter correction (LGP) to avoid accuracy loss caused by stale parameters. The prototype of OSP has been implemented using PyTo
    
[^25]: NAUTILUS:深度学习增强贝叶斯重要嵌套采样

    NAUTILUS: boosting Bayesian importance nested sampling with deep learning. (arXiv:2306.16923v1 [astro-ph.IM])

    [http://arxiv.org/abs/2306.16923](http://arxiv.org/abs/2306.16923)

    NAUTILUS是一种利用深度学习增强贝叶斯重要嵌套采样技术的方法，用于贝叶斯后验和证据估计。该方法通过神经网络回归将INS与深度学习相结合，实现了高效的重要采样。在各种合成问题和现实应用中，NAUTILUS在性能上超过了流行的NS和MCMC软件包。

    

    我们引入了一种新颖的方法，利用深度学习来增强贝叶斯重要嵌套采样（INS）技术，用于贝叶斯后验和证据估计。与基于拒绝的采样方法（如基本嵌套采样NS或马尔可夫链蒙特卡洛MCMC算法）不同，重要采样技术可以使用所有似然度量值进行后验和证据估计。然而，为了实现高效的重要采样，需要使用与后验分布相似的建议分布。我们展示了如何通过神经网络回归将INS与深度学习相结合来完成这个任务。我们还引入了NAUTILUS，这是一个用于贝叶斯后验和证据估计的参考开源Python实现。我们在各种具有挑战性的合成问题和现实世界应用中，如系外行星检测，星系SED拟合和宇宙学研究，将NAUTILUS与流行的NS和MCMC软件包（包括EMCEE，DYNESTY，ULTRANEST和POCOMC）进行了比较。

    We introduce a novel approach to boost the efficiency of the importance nested sampling (INS) technique for Bayesian posterior and evidence estimation using deep learning. Unlike rejection-based sampling methods such as vanilla nested sampling (NS) or Markov chain Monte Carlo (MCMC) algorithms, importance sampling techniques can use all likelihood evaluations for posterior and evidence estimation. However, for efficient importance sampling, one needs proposal distributions that closely mimic the posterior distributions. We show how to combine INS with deep learning via neural network regression to accomplish this task. We also introduce NAUTILUS, a reference open-source Python implementation of this technique for Bayesian posterior and evidence estimation. We compare NAUTILUS against popular NS and MCMC packages, including EMCEE, DYNESTY, ULTRANEST and POCOMC, on a variety of challenging synthetic problems and real-world applications in exoplanet detection, galaxy SED fitting and cosmo
    
[^26]: ELM神经元：一种高效且表达力强的皮层神经元模型可以解决长时间跨度任务

    The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])

    [http://arxiv.org/abs/2306.16922](http://arxiv.org/abs/2306.16922)

    ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。

    

    传统的大规模神经科学模型和机器学习利用简化的个体神经元模型，依靠集体活动和适当调整的连接来执行复杂的计算。然而，每个生物皮层神经元本质上都是一个复杂的计算设备，最近的一项研究证实了这一点，该研究中，需要一个具有数百万个参数的深度人工神经网络来复制详细生物物理模型的输入输出关系。我们对这些多个参数的必要性提出了质疑，并引入了表达力强的泄漏存储器（ELM）神经元，这是一种受生物启发的计算模型，具有高计算表达力，同时也非常高效。值得注意的是，我们的ELM神经元仅需要8,000个可训练参数就能准确匹配前述的输入输出关系。我们发现，准确的模型需要多个类似于存储器的隐藏状态和复杂的非线性突触整合。

    Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
    
[^27]: 对混合输入的奇偶目标，课程学习的可证明优势

    Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs. (arXiv:2306.16921v1 [cs.LG])

    [http://arxiv.org/abs/2306.16921](http://arxiv.org/abs/2306.16921)

    通过课程学习，采用稀疏示例先学习的2层ReLU神经网络可以在混合输入的奇偶目标上学习到足够大阶数的奇偶性，而其他神经网络无法在相同的条件下学习。

    

    实验结果表明，课程学习，即先呈现简单示例，然后再呈现更复杂的示例，可以提高学习效率。近期的一些理论结果也表明，改变采样分布可以帮助神经网络学习奇偶性，但只有大学习率和单步参数的形式结果。在这里，我们展示了在标准（有界）学习率和常见样本分布的训练步骤数量上的分离结果：如果数据分布是稀疏和密集输入的混合物，则存在一种情况，在这种情况下，通过课程嘈杂梯度下降（或SGD）算法训练的2层ReLU神经网络，先使用稀疏示例，可以学习到足够大阶数的奇偶性，而任何由嘈杂梯度下降算法训练的完全连接的神经网络（宽度或深度可能更大）在乱序样本上都不能在没有额外步骤的情况下学习。我们还提供了实验结果，支持超出的定性分离。

    Experimental results have shown that curriculum learning, i.e., presenting simpler examples before more complex ones, can improve the efficiency of learning. Some recent theoretical results also showed that changing the sampling distribution can help neural networks learn parities, with formal results only for large learning rates and one-step arguments. Here we show a separation result in the number of training steps with standard (bounded) learning rates on a common sample distribution: if the data distribution is a mixture of sparse and dense inputs, there exists a regime in which a 2-layer ReLU neural network trained by a curriculum noisy-GD (or SGD) algorithm that uses sparse examples first, can learn parities of sufficiently large degree, while any fully connected neural network of possibly larger width or depth trained by noisy-GD on the unordered samples cannot learn without additional steps. We also provide experimental results supporting the qualitative separation beyond the 
    
[^28]: 醉汉定向：估计变形场景中相机运动

    The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes. (arXiv:2306.16917v1 [cs.CV])

    [http://arxiv.org/abs/2306.16917](http://arxiv.org/abs/2306.16917)

    这项研究提出了一种在变形场景中估计相机运动的方法，并介绍了一个由大量合成数据构成的挑战性数据集，以用于可变形环境中的视觉导航和重建。

    

    估计变形场景中的相机运动是一个复杂而开放的研究挑战。大多数现有的非刚性结构运动技术假设在建立锚定参考时观察到除变形场景部分外还存在静态场景部分。然而，在某些相关应用案例中，如内窥镜检查，这个假设不成立。解决探索型轨迹最具挑战性的情况下鲁棒性和适当的定量评估方法的可变形里程计和SLAM管道的问题。为了解决共同基准的问题，我们介绍了“醉汉数据集”，它是一个具有挑战性的合成数据集，针对可变形环境中的视觉导航和重建。该数据集是第一个包含地面真实情况的三维场景内每个表面随时间呈现非刚性变形的大型探索相机轨迹集合。通过在逼真的三维建筑中进行模拟，我们获得了大量的数据。

    Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount
    
[^29]: 遵守订单：引入有序传输超参数优化

    Obeying the Order: Introducing Ordered Transfer Hyperparameter Optimisation. (arXiv:2306.16916v1 [cs.LG])

    [http://arxiv.org/abs/2306.16916](http://arxiv.org/abs/2306.16916)

    引入有序传输超参数优化（OTHPO）方法，解决了超参数优化问题中任务顺序相关的挑战，并通过十个基准测试证明了其重要性，该方法胜过当前最先进的迁移超参数优化方法。

    

    我们引入了有序传输超参数优化（OTHPO），这是超参数优化（HPO）的迁移学习版本，其中任务按顺序进行。与最先进的迁移HPO不同，我们假设每个任务与之前的任务最相关。这符合许多部署设置，其中随着收集更多数据，超参数会重新调整；例如，调整一系列电影推荐系统，随着添加更多电影和评级。我们提出了一个形式定义，概述了与相关问题的区别，并提出了一个基本的OTHPO方法，该方法胜过最先进的迁移HPO。我们通过十个基准测试实证地展示了考虑顺序的重要性。这些基准测试涉及逐渐累积数据的设置，并涵盖了XGBoost，随机森林，近似k最近邻，弹性网络，支持向量机和一个独立的现实世界动机驱动的优化问题。我们开源了这些基准测试。

    We introduce ordered transfer hyperparameter optimisation (OTHPO), a version of transfer learning for hyperparameter optimisation (HPO) where the tasks follow a sequential order. Unlike for state-of-the-art transfer HPO, the assumption is that each task is most correlated to those immediately before it. This matches many deployed settings, where hyperparameters are retuned as more data is collected; for instance tuning a sequence of movie recommendation systems as more movies and ratings are added. We propose a formal definition, outline the differences to related problems and propose a basic OTHPO method that outperforms state-of-the-art transfer HPO. We empirically show the importance of taking order into account using ten benchmarks. The benchmarks are in the setting of gradually accumulating data, and span XGBoost, random forest, approximate k-nearest neighbor, elastic net, support vector machines and a separate real-world motivated optimisation problem. We open source the benchmar
    
[^30]: 严格约束应用中的AutoML

    AutoML in Heavily Constrained Applications. (arXiv:2306.16913v1 [cs.LG])

    [http://arxiv.org/abs/2306.16913](http://arxiv.org/abs/2306.16913)

    本文提出了Caml，一种在严格约束的应用中使用元学习的AutoML方法。Caml能够自动适应特定任务的AutoML参数，并考虑用户定义的约束，生成满足约束且具有高预测性能的流程。

    

    为了优化特定任务的机器学习流程，需要对各种超参数进行仔细配置，通常由AutoML系统支持，该系统优化给定训练数据集的超参数。然而，根据AutoML系统的二阶元配置，AutoML过程的性能可能会有很大差异。目前的AutoML系统无法自动适应特定用例的配置。此外，它们也无法编译用户定义的应用约束，以确保流程及其生成的有效性和效率。在本文中，我们提出了Caml，它使用元学习自动适应其自身的AutoML参数，比如搜索策略、验证策略和搜索空间，以适应特定的任务。Caml的动态AutoML策略考虑用户定义的约束，并获得具有高预测性能的满足约束的流程。

    Optimizing a machine learning pipeline for a task at hand requires careful configuration of various hyperparameters, typically supported by an AutoML system that optimizes the hyperparameters for the given training dataset. Yet, depending on the AutoML system's own second-order meta-configuration, the performance of the AutoML process can vary significantly. Current AutoML systems cannot automatically adapt their own configuration to a specific use case. Further, they cannot compile user-defined application constraints on the effectiveness and efficiency of the pipeline and its generation. In this paper, we propose Caml, which uses meta-learning to automatically adapt its own AutoML parameters, such as the search strategy, the validation strategy, and the search space, for a task at hand. The dynamic AutoML strategy of Caml takes user-defined constraints into account and obtains constraint-satisfying pipelines with high predictive performance.
    
[^31]: 数值数据填补的多模态数据集:一种概率最近邻核密度方法

    Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach. (arXiv:2306.16906v1 [stat.ML])

    [http://arxiv.org/abs/2306.16906](http://arxiv.org/abs/2306.16906)

    本论文介绍了一种新的数值数据填补方法，通过将最近邻估计和高斯核密度估计结合，能够有效处理多模态数据集中的缺失值，并提供比当前方法更高的概率估计。

    

    数值数据填补方法通过估计替换缺失的值以利用不完整的数据集。当前的填补方法试图最小化未观察到的真实值和填补值之间的误差。但是，在多模态或复杂分布存在的情况下，这种策略可能会产生伪像，导致填补效果较差。为了解决这个问题，我们引入了$k$NN$\times$KDE算法: 一种将最近邻估计($k$NN)和使用高斯核进行密度估计(KDE)结合的数据填补方法。我们使用人工和真实数据进行了与之前数据填补方法的比较，涉及了不同的数据缺失情况和不同的数据缺失率，并且展示了我们的方法可以处理复杂的原始数据结构，产生更低的数据填补误差，并提供比当前方法更高的概率估计。我们将代码以开源形式发布给社区：https://github.com/DeltaFloflo/knnxkde

    Numerical data imputation algorithms replace missing values by estimates to leverage incomplete data sets. Current imputation methods seek to minimize the error between the unobserved ground truth and the imputed values. But this strategy can create artifacts leading to poor imputation in the presence of multimodal or complex distributions. To tackle this problem, we introduce the $k$NN$\times$KDE algorithm: a data imputation method combining nearest neighbor estimation ($k$NN) and density estimation with Gaussian kernels (KDE). We compare our method with previous data imputation methods using artificial and real-world data with different data missing scenarios and various data missing rates, and show that our method can cope with complex original data structure, yields lower data imputation errors, and provides probabilistic estimates with higher likelihood than current methods. We release the code in open-source for the community: https://github.com/DeltaFloflo/knnxkde
    
[^32]: 可追溯的群体化自优化特征转换学习：双重优化视角

    Traceable Group-Wise Self-Optimizing Feature Transformation Learning: A Dual Optimization Perspective. (arXiv:2306.16893v1 [cs.LG])

    [http://arxiv.org/abs/2306.16893](http://arxiv.org/abs/2306.16893)

    本论文提出了一种可追溯的群体化自优化特征转换学习方法，并引入了一个新颖的自优化框架，利用三个级联强化代理自动选择候选特征和操作，生成改进的特征转换组合。

    

    特征转换旨在通过数学上的改进现有特征来重构一个有效的表示空间。它是应对维度灾难、增强模型泛化能力、减少数据稀疏性和扩展经典模型适用性的关键方法。现有研究主要集中在基于领域知识的特征工程或学习潜在表示方面。然而，这些方法虽然有见地，但缺乏完全自动化，并且无法产生可追溯且最优的表示空间。一个不可或缺的问题是：在为机器学习任务重构特征空间时，我们能否同时解决这些限制？我们的初步工作通过引入一种新颖的自优化框架朝着这个挑战迈出了开创性的一步。这个框架利用了三个级联强化代理的力量，自动选择候选特征和操作，生成改进的特征转换组合。

    Feature transformation aims to reconstruct an effective representation space by mathematically refining the existing features. It serves as a pivotal approach to combat the curse of dimensionality, enhance model generalization, mitigate data sparsity, and extend the applicability of classical models. Existing research predominantly focuses on domain knowledge-based feature engineering or learning latent representations. However, these methods, while insightful, lack full automation and fail to yield a traceable and optimal representation space. An indispensable question arises: Can we concurrently address these limitations when reconstructing a feature space for a machine-learning task? Our initial work took a pioneering step towards this challenge by introducing a novel self-optimizing framework. This framework leverages the power of three cascading reinforced agents to automatically select candidate features and operations for generating improved feature transformation combinations. 
    
[^33]: 非传递性游戏的策略空间多样性

    Policy Space Diversity for Non-Transitive Games. (arXiv:2306.16884v1 [cs.GT])

    [http://arxiv.org/abs/2306.16884](http://arxiv.org/abs/2306.16884)

    这项研究提出了一种新的策略空间多样性度量，并通过将其纳入策略空间响应预言机（PSRO）中，实现了更好的逼近纳什均衡（NE）的效果。

    

    策略空间响应预言机（PSRO）是一种在多智能体非传递性游戏中近似纳什均衡（NE）的重要算法框架。许多之前的研究一直在尝试促进PSRO中的策略多样性。然而，现有多样性度量的一个主要弱点在于更多样化的人口（根据多样性度量）不一定意味着更好地逼近NE（正如我们在论文中证明的那样）。为了解决这个问题，我们提出了一种新的多样性度量，其改进保证了更好地逼近NE。同时，我们开发了一种实用且有正当理由的方法，仅使用状态-行动样本来优化我们的多样性度量。通过将多样性正则化纳入PSRO中的最佳应答求解，我们得到了一个新的PSRO变种，即策略空间多样性PSRO（PSD-PSRO）。我们展示了PSD-PSRO的收敛性。经验上，在各种游戏上进行了大量的实验，证明PSD-PSRO在促进策略多样性、提高逼近NE效果方面更加有效。

    Policy-Space Response Oracles (PSRO) is an influential algorithm framework for approximating a Nash Equilibrium (NE) in multi-agent non-transitive games. Many previous studies have been trying to promote policy diversity in PSRO. A major weakness in existing diversity metrics is that a more diverse (according to their diversity metrics) population does not necessarily mean (as we proved in the paper) a better approximation to a NE. To alleviate this problem, we propose a new diversity metric, the improvement of which guarantees a better approximation to a NE. Meanwhile, we develop a practical and well-justified method to optimize our diversity metric using only state-action samples. By incorporating our diversity regularization into the best response solving in PSRO, we obtain a new PSRO variant, Policy Space Diversity PSRO (PSD-PSRO). We present the convergence property of PSD-PSRO. Empirically, extensive experiments on various games demonstrate that PSD-PSRO is more effective in prod
    
[^34]: 手术阶段和器械识别：如何识别适当的数据集划分

    Surgical Phase and Instrument Recognition: How to identify appropriate Dataset Splits. (arXiv:2306.16879v1 [cs.LG])

    [http://arxiv.org/abs/2306.16879](http://arxiv.org/abs/2306.16879)

    本研究开发了一个基于Web的应用程序，用于手术工作流和器械识别的机器学习模型。通过可视化框架，能够评估手术工作流识别的数据集划分，特别是识别次优的划分。

    

    目的：从时间数据中开发用于手术工作流和器械识别的机器学习模型是一项具有挑战性的任务，由于手术工作流的复杂性质，数据的不平衡分布是手术工作流识别领域面临的主要挑战之一。为了获取有意义的结果，将数据仔细划分为训练集、验证集和测试集，以及选择合适的评估指标是至关重要的。方法：在这项工作中，我们提出了一个开放的基于Web的应用程序，可以进行数据集划分的交互式探索。所提出的可视化框架有助于评估手术工作流识别的数据集划分，特别是识别次优的数据集划分。目前，它支持手术阶段和器械注释的可视化。结果：为了验证专用的交互式可视化，我们使用了一个数据集划分

    Purpose: The development of machine learning models for surgical workflow and instrument recognition from temporal data represents a challenging task due to the complex nature of surgical workflows. In particular, the imbalanced distribution of data is one of the major challenges in the domain of surgical workflow recognition. In order to obtain meaningful results, careful partitioning of data into training, validation, and test sets, as well as the selection of suitable evaluation metrics are crucial. Methods: In this work, we present an openly available web-based application that enables interactive exploration of dataset partitions. The proposed visual framework facilitates the assessment of dataset splits for surgical workflow recognition, especially with regard to identifying sub-optimal dataset splits. Currently, it supports visualization of surgical phase and instrument annotations. Results: In order to validate the dedicated interactive visualizations, we use a dataset split of
    
[^35]: 理解迭代元训练的过拟合问题

    Understanding the Overfitting of the Episodic Meta-training. (arXiv:2306.16873v1 [cs.LG])

    [http://arxiv.org/abs/2306.16873](http://arxiv.org/abs/2306.16873)

    本研究通过引入知识蒸馏技术，解决了迭代元训练阶段的过拟合问题，该方法通过惩罚过度区分，保留教师模型的新类别泛化知识，优于标准元训练过程。同时，我们提出了最近邻对称Kullback-Leibler（NNSKL）散度来进一步推进知识蒸馏的极限。

    

    虽然两阶段少样本分类方法取得了成功，但在迭代元训练阶段，模型遭受严重的过拟合问题。我们假设这是由于过度区分造成的，即模型学习过于依赖适合基类区分的表面特征，而抑制了对新类别的泛化能力。为了惩罚过度区分，我们引入了知识蒸馏技术，在训练过程中保留来自教师模型的新类别泛化知识。具体而言，我们选择验证准确率最好的教师模型，并限制了学生模型线性分类器输出分布与教师模型之间的对称Kullback-Leibler（SKL）散度。这一简单的方法优于标准元训练过程。此外，我们进一步提出了用于元训练的最近邻对称Kullback-Leibler（NNSKL）散度，以推动知识蒸馏的极限。

    Despite the success of two-stage few-shot classification methods, in the episodic meta-training stage, the model suffers severe overfitting. We hypothesize that it is caused by over-discrimination, i.e., the model learns to over-rely on the superficial features that fit for base class discrimination while suppressing the novel class generalization. To penalize over-discrimination, we introduce knowledge distillation techniques to keep novel generalization knowledge from the teacher model during training. Specifically, we select the teacher model as the one with the best validation accuracy during meta-training and restrict the symmetric Kullback-Leibler (SKL) divergence between the output distribution of the linear classifier of the teacher model and that of the student model. This simple approach outperforms the standard meta-training process. We further propose the Nearest Neighbor Symmetric Kullback-Leibler (NNSKL) divergence for meta-training to push the limits of knowledge distill
    
[^36]: NeuralFuse: 学习改善低电压环境下有限访问神经网络推断的准确性

    NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes. (arXiv:2306.16869v1 [cs.LG])

    [http://arxiv.org/abs/2306.16869](http://arxiv.org/abs/2306.16869)

    NeuralFuse是一个新颖的附加模块，通过学习输入转换来生成抗误差的数据表示，解决了低电压环境下有限访问神经网络推断的准确性与能量之间的权衡问题。

    

    深度神经网络在机器学习中已经无处不在，但其能量消耗仍然是一个值得关注的问题。降低供电电压是降低能量消耗的有效策略。然而，过度降低供电电压可能会导致准确性降低，因为模型参数存储在静态随机存储器(SRAM)中，而SRAM中会发生随机位翻转。为了解决这个挑战，我们引入了NeuralFuse，这是一个新颖的附加模块，通过学习输入转换来生成抗误差的数据表示，以在低电压环境中解决准确性与能量之间的权衡。NeuralFuse在标称电压和低电压情况下都能保护DNN的准确性。此外，NeuralFuse易于实现，并可以轻松应用于有限访问的DNN，例如不可配置的硬件或云端API的远程访问。实验结果表明，在1%的位错误率下，NeuralFuse可以将SRAM内存访问能量降低高达24%，同时保持准确性。

    Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains a notable issue. Lowering the supply voltage is an effective strategy for reducing energy consumption. However, aggressively scaling down the supply voltage can lead to accuracy degradation due to random bit flips in static random access memory (SRAM) where model parameters are stored. To address this challenge, we introduce NeuralFuse, a novel add-on module that addresses the accuracy-energy tradeoff in low-voltage regimes by learning input transformations to generate error-resistant data representations. NeuralFuse protects DNN accuracy in both nominal and low-voltage scenarios. Moreover, NeuralFuse is easy to implement and can be readily applied to DNNs with limited access, such as non-configurable hardware or remote access to cloud-based APIs. Experimental results demonstrate that, at a 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to 24% while imp
    
[^37]: ArrayBot: 通过触觉实现通用分布式操作的强化学习

    ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch. (arXiv:2306.16857v1 [cs.RO])

    [http://arxiv.org/abs/2306.16857](http://arxiv.org/abs/2306.16857)

    ArrayBot通过强化学习实现了通用分布式操作，通过对动作空间的重新定义和采用触觉观察训练，其控制策略不仅能够推广到未见过的物体形状，还能在实际机器人中进行转移，展示了巨大的潜力。

    

    我们介绍了ArrayBot，这是一个由16×16的竖向滑动柱和触觉传感器组成的分布式操作系统，可以同时支持、感知和操作桌面上的物体。为了实现通用分布式操作，我们利用强化学习算法自动发现控制策略。面对大量冗余的动作，我们提出考虑空间局部动作图块和频域中低频动作来重新定义动作空间。通过这个重新定义的动作空间，我们训练强化学习代理，只通过触觉观察即可重新定位不同的物体。令人惊讶的是，我们发现发现的策略不仅可以推广到模拟器中看不见的物体形状，而且可以在物理机器人上进行转移而不需要任何域随机化。利用部署的策略，我们展示了丰富的真实世界操作任务，展示了其巨大潜力。

    We present ArrayBot, a distributed manipulation system consisting of a $16 \times 16$ array of vertically sliding pillars integrated with tactile sensors, which can simultaneously support, perceive, and manipulate the tabletop objects. Towards generalizable distributed manipulation, we leverage reinforcement learning (RL) algorithms for the automatic discovery of control policies. In the face of the massively redundant actions, we propose to reshape the action space by considering the spatially local action patch and the low-frequency actions in the frequency domain. With this reshaped action space, we train RL agents that can relocate diverse objects through tactile observations only. Surprisingly, we find that the discovered policy can not only generalize to unseen object shapes in the simulator but also transfer to the physical robot without any domain randomization. Leveraging the deployed policy, we present abundant real-world manipulation tasks, illustrating the vast potential of
    
[^38]: 关于循环神经网络隐藏状态向量和语义基本事实之间的关系

    On the Relationship Between RNN Hidden State Vectors and Semantic Ground Truth. (arXiv:2306.16854v1 [cs.LG])

    [http://arxiv.org/abs/2306.16854](http://arxiv.org/abs/2306.16854)

    本研究考察了循环神经网络的隐藏状态向量是否具有语义相似的聚类结构，并通过在训练中识别常规语言的RNNs来验证聚类假设的有效性。

    

    我们考察了循环神经网络（RNNs）的隐藏状态向量是否倾向于形成语义相似向量的簇群，这个假设被称为聚类假设。虽然这个假设在近年来对RNNs的分析中已经被假定，但其在现代神经网络架构上的有效性还没有得到充分的研究。我们在训练用于识别常规语言的RNNs的背景下考察了聚类假设。这使我们能够在评估中利用完美的基准自动机，与其比较RNN的准确性和隐藏状态向量的分布。我们首先考察了将RNN的隐藏状态向量在语义上分为不同类别的（分段线性）可分离性。然后，我们通过应用多种最新的无监督聚类方法计算隐藏状态向量空间中的簇群。我们正式分析了计算聚类函数的准确性以及聚类假设的有效性。

    We examine the assumption that the hidden-state vectors of recurrent neural networks (RNNs) tend to form clusters of semantically similar vectors, which we dub the clustering hypothesis. While this hypothesis has been assumed in the analysis of RNNs in recent years, its validity has not been studied thoroughly on modern neural network architectures. We examine the clustering hypothesis in the context of RNNs that were trained to recognize regular languages. This enables us to draw on perfect ground-truth automata in our evaluation, against which we can compare the RNN's accuracy and the distribution of the hidden-state vectors.  We start with examining the (piecewise linear) separability of an RNN's hidden-state vectors into semantically different classes. We continue the analysis by computing clusters over the hidden-state vector space with multiple state-of-the-art unsupervised clustering approaches. We formally analyze the accuracy of computed clustering functions and the validity o
    
[^39]: 通过线掩模引导的黑盒优化实现宏单元布局

    Macro Placement by Wire-Mask-Guided Black-Box Optimization. (arXiv:2306.16844v1 [cs.LG])

    [http://arxiv.org/abs/2306.16844](http://arxiv.org/abs/2306.16844)

    本文介绍了一种名为WireMask-BBO的黑盒优化框架，通过使用线掩模引导的贪心过程进行宏单元布局，在有效降低HPWL的同时节省大量时间。此方法还可以对现有布局进行微调，改善50%的HPWL。

    

    面对大规模集成（VLSI）技术的发展，芯片布局中的电子设计自动化（EDA）技术面临新的挑战。宏单元布局作为该过程中的重要子问题，试图确定所有宏单元的位置，以最小化半周长线长（HPWL）并避免重叠。先前的方法包括基于打包、分析和强化学习的方法。本文提出了一种新的黑盒优化（BBO）框架（称为WireMask-BBO），通过使用线掩模引导的贪心过程进行目标评估来进行宏单元布局。配备不同的BBO算法，WireMask-BBO在实践中比先前的方法实现了显著的改进，即通过使用更少的时间实现了显著更短的HPWL。此外，它可以通过将其视为初始解来微调现有的布局，从而使HPWL改善多达50%。WireMask-BBO具有引领芯片布局领域的潜力。

    The development of very large-scale integration (VLSI) technology has posed new challenges for electronic design automation (EDA) techniques in chip floorplanning. During this process, macro placement is an important subproblem, which tries to determine the positions of all macros with the aim of minimizing half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods include packing-based, analytical and reinforcement learning methods. In this paper, we propose a new black-box optimization (BBO) framework (called WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO empirically achieves significant improvements over previous methods, i.e., achieves significantly shorter HPWL by using much less time. Furthermore, it can fine-tune existing placements by treating them as initial solutions, which can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to s
    
[^40]: 用基于梯度的优化方法解决核岭回归问题

    Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])

    [http://arxiv.org/abs/2306.16838](http://arxiv.org/abs/2306.16838)

    本研究提出了一种新的方法来解决核岭回归问题，通过等价的目标函数形式和基于梯度的优化方法，我们不仅可以使用其他惩罚方法，还能够从梯度下降的角度研究核岭回归。通过提前停止的正则化，我们推导出了一个闭合解，即核梯度流（KGF），并证明了KGF和KRR之间的差异。我们还将KRR泛化，使用$\ell_1$和$\ell_\infty$惩罚方法，并发现使用这些方法得到的解与前向分步回归和符号梯度下降结合提前停止得到的解非常相似。因此，我们减少了计算复杂度重的近端梯度下降算法的需求。

    

    核岭回归（KRR）是线性岭回归的非线性推广。在这里，我们引入了KRR目标函数的等价形式，为使用其他惩罚方法和从梯度下降的角度研究核岭回归打开了可能。通过连续时间的视角，我们推导出了一个闭合解——核梯度流（KGF），通过提前停止的正则化，让我们能够在KGF和KRR之间理论上界定差异。我们用$\ell_1$和$\ell_\infty$惩罚方法将KRR泛化，并利用类似KGF和KRR之间的相似性，使用这些惩罚方法得到的解与使用前向分步回归（也称为坐标下降）和符号梯度下降结合提前停止得到的解非常相似。因此，减少了计算复杂度重的近端梯度下降算法的需求。

    Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
    
[^41]: 深度神经网络的采样权重

    Sampling weights of deep neural networks. (arXiv:2306.16830v1 [cs.LG])

    [http://arxiv.org/abs/2306.16830](http://arxiv.org/abs/2306.16830)

    我们提出了一种基于随机特征模型的采样方法，用于深度神经网络的权重和偏差。我们的方法不需要迭代优化或计算梯度，能够生成通用逼近器，并且对数据的变换和缩放是不变的。在数值实验中，我们展示了我们的方法的优越性能。

    

    我们引入了一个概率分布和有效的采样算法，用于完全连接神经网络的权重和偏差。在监督学习环境中，不需要通过迭代优化或计算内部网络参数的梯度来训练网络。采样基于随机特征模型的思想。然而，我们使用输入和输出训练数据对浅层和深度网络进行采样，而不是使用数据无关的分布，如正态分布。我们证明了我们构造的采样网络是通用逼近器。我们还证明了我们的采样方案对刚体变换和输入数据的缩放是不变的。这意味着许多常用的预处理技术不再需要。对于巴龙函数，我们证明了采样浅层网络的L^2近似误差随着神经元数量的平方根减小。在数值实验中，我们展示了我们的方法在多个数据集上的优越性能。

    We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data of the supervised learning problem to sample both shallow and deep networks. We prove that the sampled networks we construct are universal approximators. We also show that our sampling scheme is invariant to rigid body transformations and scaling of the input data. This implies many popular pre-processing techniques are no longer required. For Barron functions, we show that the $L^2$-approximation error of sampled shallow networks decreases with the square root of the number of neurons. In numerical ex
    
[^42]: SaGess：用于可扩展图生成的采样图去噪扩散模型

    SaGess: Sampling Graph Denoising Diffusion Model for Scalable Graph Generation. (arXiv:2306.16827v1 [cs.LG])

    [http://arxiv.org/abs/2306.16827](http://arxiv.org/abs/2306.16827)

    SaGess是一种离散去噪扩散方法，通过将扩散模型（DiGress）与广义的分治框架相结合，能够生成大型实际网络。这种方法克服了计算复杂性的限制，使得生成大型图数据成为可能。

    

    近年来，去噪扩散生成模型被认为是合成数据生成领域的最先进方法，尤其在生成图像方面。这些方法在表格和图形数据生成等其他应用中也证明了成功。然而，由于计算复杂性，迄今为止，这些技术在图数据上的应用仅限于小型图，例如在分子建模中使用的图。在本文中，我们提出了SaGess，一种离散去噪扩散方法，通过将扩散模型（DiGress）与广义的分治框架相结合，能够生成大型实际网络。该算法通过对初始图的子图进行采样来训练DiGress，然后使用DiGress生成的子图构建一个合成图。我们通过与其他方法生成的合成数据集进行比较来评估合成数据集的质量。

    Over recent years, denoising diffusion generative models have come to be considered as state-of-the-art methods for synthetic data generation, especially in the case of generating images. These approaches have also proved successful in other applications such as tabular and graph data generation. However, due to computational complexity, to this date, the application of these techniques to graph data has been restricted to small graphs, such as those used in molecular modeling. In this paper, we propose SaGess, a discrete denoising diffusion approach, which is able to generate large real-world networks by augmenting a diffusion model (DiGress) with a generalized divide-and-conquer framework. The algorithm is capable of generating larger graphs by sampling a covering of subgraphs of the initial graph in order to train DiGress. SaGess then constructs a synthetic graph using the subgraphs that have been generated by DiGress. We evaluate the quality of the synthetic data sets against sever
    
[^43]: 使用领域适应性的医院管理中的住院时长预测

    Length of Stay prediction for Hospital Management using Domain Adaptation. (arXiv:2306.16823v1 [cs.LG])

    [http://arxiv.org/abs/2306.16823](http://arxiv.org/abs/2306.16823)

    本研究使用领域适应技术来预测医院管理中的住院时长，以帮助医院有效规划入院、分配资源和改善护理。

    

    住院时长是一项重要的管理指标，如果提前知道，可以用来有效地规划入院、分配资源和改善护理。通过使用历史患者数据和机器学习技术，可以开发出住院时长预测模型。从伦理角度来看，这些模型不能用于替代部门主管对于患者出院的决策，但对于负责有效医院规划的医院管理系统来说是非常必要的。因此，预测系统的设计应该适应真实的医院环境。在本研究中，我们通过应用领域适应技术，在入院单位的细粒度层面上预测早期住院时长，以利用从潜在源领域学到的信息。从eICU-CRD和MIMIC-IV分别提取了110,079次和60,492次患者入住9个重症监护病房的时变数据。这些数据被输入到长短期记忆网络和全连接网络中，训练出一个源领域模型，wei

    Inpatient length of stay (LoS) is an important managerial metric which if known in advance can be used to efficiently plan admissions, allocate resources and improve care. Using historical patient data and machine learning techniques, LoS prediction models can be developed. Ethically, these models can not be used for patient discharge in lieu of unit heads but are of utmost necessity for hospital management systems in charge of effective hospital planning. Therefore, the design of the prediction system should be adapted to work in a true hospital setting. In this study, we predict early hospital LoS at the granular level of admission units by applying domain adaptation to leverage information learned from a potential source domain. Time-varying data from 110,079 and 60,492 patient stays to 8 and 9 intensive care units were respectively extracted from eICU-CRD and MIMIC-IV. These were fed into a Long-Short Term Memory and a Fully connected network to train a source domain model, the wei
    
[^44]: 使用时间集成改进在线连续学习性能和稳定性

    Improving Online Continual Learning Performance and Stability with Temporal Ensembles. (arXiv:2306.16817v1 [cs.LG])

    [http://arxiv.org/abs/2306.16817](http://arxiv.org/abs/2306.16817)

    该研究通过模型集成方法改进了在线连续学习的性能和稳定性，通过综合利用来自不同训练任务的模型，显著提高了在线连续学习的表现。

    

    当神经网络在大型数据集上进行大量迭代训练时，它们非常有效。然而，当它们在非平稳的数据流和在线方式下进行训练时，其性能会下降：(1)在线设置限制了数据的可用性，(2)由于数据的非平稳性导致灾难性遗忘。此外，几篇最近的文章表明连续学习中使用的重放方法在模型持续评估时存在稳定性差距。在本文中，我们研究了模型集成作为改进在线连续学习性能和稳定性的一种方法。我们观察到，简单地集成来自各种训练任务的模型显著提高了在线连续学习的性能。基于这一观察，并从半监督学习中获取灵感，我们提出了一种改进的连续学习框架，该框架综合利用了显式和隐式知识。

    Neural networks are very effective when trained on large datasets for a large number of iterations. However, when they are trained on non-stationary streams of data and in an online fashion, their performance is reduced (1) by the online setup, which limits the availability of data, (2) due to catastrophic forgetting because of the non-stationary nature of the data. Furthermore, several recent works (Caccia et al., 2022; Lange et al., 2023) arXiv:2205.1345(2) showed that replay methods used in continual learning suffer from the stability gap, encountered when evaluating the model continually (rather than only on task boundaries). In this article, we study the effect of model ensembling as a way to improve performance and stability in online continual learning. We notice that naively ensembling models coming from a variety of training tasks increases the performance in online continual learning considerably. Starting from this observation, and drawing inspirations from semi-supervised l
    
[^45]: CLIPAG: 走向无需生成器的文本到图像生成

    CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])

    [http://arxiv.org/abs/2306.16805](http://arxiv.org/abs/2306.16805)

    本文将感知对齐梯度（PAG）的研究扩展到视觉-语言架构，并通过对 CLIP 进行鲁棒性调整，展示了在视觉-语言生成任务中集成 CLIPAG 可以实现显著改进，并实现了无生成器的文本到图像生成。

    

    感知对齐梯度 (Perceptually Aligned Gradients, PAG) 是在健壮的图像分类模型中观察到的一种有趣属性，其中它们的输入渐变与人类感知对齐并具有语义意义。虽然这一现象引起了显着的研究关注，但仅仅在单模态纯视觉架构的背景下进行了研究。在本研究中，我们将 PAG 的研究扩展到视觉-语言架构，这是多样化的图像-文本任务和应用的基础。通过对 CLIP 进行对抗性鲁棒微调，我们证明了鲁棒的视觉-语言模型相对于其基准模型表现出了 PAG。这项工作展示了 CLIPAG 在几种视觉-语言生成任务中的优势。值得注意的是，我们展示了无缝集成 CLIPAG 的 "即插即用" 方式显著改进了视觉-语言生成应用。此外，利用其 PAG 属性，CLIPAG 实现了无生成器的文本到图像生成。

    Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a "plug-n-play" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation witho
    
[^46]: 长期信用归因通过反事实贡献分析的方式

    Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis. (arXiv:2306.16803v1 [cs.LG])

    [http://arxiv.org/abs/2306.16803](http://arxiv.org/abs/2306.16803)

    本文提出了一种新的基于模型的信用分配算法，通过量化反事实查询来测量动作对未来奖励的影响。与现有方法不同的是，我们通过测量对奖励或奖励对象表示的贡献，获得了具有更低方差的梯度估计。

    

    为了使强化学习更加样本高效，我们需要更好的信用归因方法来衡量动作对未来奖励的影响。在悔棋信用归因（HCA）的基础上，我们引入了反事实贡献分析（COCOA），这是一种新的基于模型的信用归因算法系列。我们的算法通过量化一个反事实查询来实现精确的信用分配：“如果代理选择另一个动作，它仍然会获得这个奖励吗？”通过测量动作对获得后续奖励的贡献，我们展示了对于奖励状态测量贡献（即HCA中所做的）会导致贡献的错误估计，使得HCA在许多相关环境中向高方差的REINFORCE估计器退化。相反，我们通过测量对奖励或所学习的奖励对象的表示的贡献，得到具有更低方差的梯度估计。我们在一系列特定问题上进行了实验

    To make reinforcement learning more sample efficient, we need better credit assignment methods that measure an action's influence on future rewards. Building upon Hindsight Credit Assignment (HCA), we introduce Counterfactual Contribution Analysis (COCOA), a new family of model-based credit assignment algorithms. Our algorithms achieve precise credit assignment by measuring the contribution of actions upon obtaining subsequent rewards, by quantifying a counterfactual query: "Would the agent still have reached this reward if it had taken another action?". We show that measuring contributions w.r.t. rewarding states, as is done in HCA, results in spurious estimates of contributions, causing HCA to degrade towards the high-variance REINFORCE estimator in many relevant environments. Instead, we measure contributions w.r.t. rewards or learned representations of the rewarding objects, resulting in gradient estimates with lower variance. We run experiments on a suite of problems specifically 
    
[^47]: 稀疏模型汤：通过模型平均改进修剪的方法

    Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])

    [http://arxiv.org/abs/2306.16788](http://arxiv.org/abs/2306.16788)

    本研究通过将多个经过迭代幅度剪枝的模型进行平均，解决了同时利用稀疏性和参数平均的问题，并显著提升了泛化性能。

    

    神经网络可以通过剪枝显著压缩，从而得到稀疏模型，这些模型需要更少的存储和浮点运算，同时保持预测性能。模型汤（Wortsman等人，2022年）通过将多个模型的参数平均成一个单一模型来改善泛化和超出分布性能，而不增加推理时间。然而，识别处于相同损失区域的模型以同时利用稀疏性和参数平均是具有挑战性的，因为对任意稀疏模型进行平均会降低整体稀疏度，原因是不同的稀疏连接性。在这项工作中，我们通过展示在迭代幅度剪枝（IMP）的单次重新训练阶段中探索不同的超参数配置（例如批次排序或权重衰减）产生的模型适合进行平均，并且通过设计共享相同的稀疏连接性来解决这些挑战。平均这些模型显著提升了泛化性能。

    Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
    
[^48]: 基于图采样的元学习用于分子属性预测

    Graph Sampling-based Meta-Learning for Molecular Property Prediction. (arXiv:2306.16780v1 [cs.LG])

    [http://arxiv.org/abs/2306.16780](http://arxiv.org/abs/2306.16780)

    本文提出了一种基于图采样的元学习框架，用于少样本分子属性预测。通过构建分子-属性关系图并利用拓扑信息，以及使用对比损失函数调度子图采样过程，有效利用多对多关系进行预测。

    

    分子属性通常只能通过有限数目的样本观测到，研究人员将属性预测视为少样本问题。先前的研究忽视了一个重要事实，即每个分子可以同时记录多个不同的属性。为了有效利用分子和属性之间的多对多关系，我们提出了一种基于图采样的元学习（GS-Meta）框架，用于少样本分子属性预测。首先，我们构建了一个分子-属性关系图（MPG）：分子和属性作为节点，而属性标签决定边。然后，为了利用MPG的拓扑信息，我们将元学习中的一个episode重新定义为MPG的子图，其中包含目标属性节点、分子节点和辅助属性节点。第三，由于子图形式的episode不再相互独立，我们提出使用对比损失函数来调度子图采样过程，以确保采样的episode具有对比性。

    Molecular property is usually observed with a limited number of samples, and researchers have considered property prediction as a few-shot problem. One important fact that has been ignored by prior works is that each molecule can be recorded with several different properties simultaneously. To effectively utilize many-to-many correlations of molecules and properties, we propose a Graph Sampling-based Meta-learning (GS-Meta) framework for few-shot molecular property prediction. First, we construct a Molecule-Property relation Graph (MPG): molecule and properties are nodes, while property labels decide edges. Then, to utilize the topological information of MPG, we reformulate an episode in meta-learning as a subgraph of the MPG, containing a target property node, molecule nodes, and auxiliary property nodes. Third, as episodes in the form of subgraphs are no longer independent of each other, we propose to schedule the subgraph sampling process with a contrastive loss function, which cons
    
[^49]: 从合成的人类团队活动中学习

    Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])

    [http://arxiv.org/abs/2306.16772](http://arxiv.org/abs/2306.16772)

    提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器，通过Unity引擎驱动实现。该生成器具有大规模数据生成、多模态和高质量注释等特点，能够用于研究复杂的人类互动和团队活动。

    

    在以人为中心的计算机视觉中，对复杂的人类互动和团队活动的理解引起了人们的关注。然而，相关任务的进展受到了获取大规模标记的真实世界数据集的困难的限制。为了缓解这个问题，我们提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器。M3Act采用Unity引擎驱动，包含可供仿真使用的三维场景和人物资源，可配置的照明和摄像系统，高度参数化的模块化团队活动，以及在数据生成过程中具有大量领域随机化的特点。我们的数据生成器能够生成具有多个视图、模态（RGB图像、2D姿势、3D动作）和高质量注释的大规模人类活动数据集（2D边界框、实例分割掩模、个体动作和团队活动类别）。利用M3Act，我们可以生成大规模的人类活动数据集，用于研究人类互动和团队活动。

    The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
    
[^50]: DNN推理/训练中卷积和非卷积操作的性能分析

    Performance Analysis of DNN Inference/Training with Convolution and non-Convolution Operations. (arXiv:2306.16767v1 [cs.AR])

    [http://arxiv.org/abs/2306.16767](http://arxiv.org/abs/2306.16767)

    本文提出了SimDIT，一种新颖的性能分析框架，用于通用ASIC系统级硬件加速器平台上的CNN推理和训练。SimDIT综合考虑了卷积和非卷积操作，并提供了详细的性能统计数据。

    

    当前用于深度学习加速器的性能分析框架存在两个主要限制。首先，尽管现代卷积神经网络（CNN）除了卷积层之外还包括许多其他类型的层，在训练过程中尤其如此，但这些框架主要集中在卷积层上。其次，这些框架通常针对推理，缺乏对训练操作的支持。本文提出了一种新颖的性能分析框架SimDIT，用于通用ASIC系统级硬件加速器平台。SimDIT的建模工作全面覆盖了CNN推理和训练中的卷积和非卷积操作，并在高度可参数化的硬件底层上。SimDIT与后端硅实现流程集成，为执行CNN推理和训练工作负载提供了详细的端到端性能统计（如数据访问成本、循环计数、能量和功率）。SimDIT的性能分析功能为性能分析提供了一种全面而有效的方法。

    Today's performance analysis frameworks for deep learning accelerators suffer from two significant limitations. First, although modern convolutional neural network (CNNs) consist of many types of layers other than convolution, especially during training, these frameworks largely focus on convolution layers only. Second, these frameworks are generally targeted towards inference, and lack support for training operations. This work proposes a novel performance analysis framework, SimDIT, for general ASIC-based systolic hardware accelerator platforms. The modeling effort of SimDIT comprehensively covers convolution and non-convolution operations of both CNN inference and training on a highly parameterizable hardware substrate. SimDIT is integrated with a backend silicon implementation flow and provides detailed end-to-end performance statistics (i.e., data access cost, cycle counts, energy, and power) for executing CNN inference and training workloads. SimDIT-enabled performance analysis r
    
[^51]: 基于Moreau包络的弱凸差分重构与双层规划算法

    Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs. (arXiv:2306.16761v1 [math.OC])

    [http://arxiv.org/abs/2306.16761](http://arxiv.org/abs/2306.16761)

    本文提出了一种基于Moreau包络的弱凸差分重构与双层规划算法，可以适用于更多与机器学习和统计相关的应用。

    

    最近，Ye等人设计了一个解决特定类别双层规划的算法，重点是与超参数选择相关的应用，利用基于值函数方法改写的差分凸算法。在下层问题完全凸性的情况下，该算法特别强大，如支持向量机模型或最小绝对收缩和选择算子模型。在本文中，为了适应更多与机器学习和统计相关的应用，我们将下层完全凸性的基本假设大大削弱为弱凸性。因此，我们提出了一种使用下层问题的Moreau包络进行重构，并证明了该重构是一个弱凸差分规划。随后，我们开发了一个逐步收敛的算法来解决这个弱凸差分规划。

    Recently, Ye et al. (Mathematical Programming 2023) designed an algorithm for solving a specific class of bilevel programs with an emphasis on applications related to hyperparameter selection, utilizing the difference of convex algorithm based on the value function approach reformulation. The proposed algorithm is particularly powerful when the lower level problem is fully convex , such as a support vector machine model or a least absolute shrinkage and selection operator model. In this paper, to suit more applications related to machine learning and statistics, we substantially weaken the underlying assumption from lower level full convexity to weak convexity. Accordingly, we propose a new reformulation using Moreau envelope of the lower level problem and demonstrate that this reformulation is a difference of weakly convex program. Subsequently, we develop a sequentially convergent algorithm for solving this difference of weakly convex program. To evaluate the effectiveness of our app
    
[^52]: 使用半监督数据集注释进行迁移学习的鸟鸣分类

    Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall Classification. (arXiv:2306.16760v1 [cs.SD])

    [http://arxiv.org/abs/2306.16760](http://arxiv.org/abs/2306.16760)

    该论文讨论了使用半监督数据集注释进行迁移学习的方法，通过利用现有的模型解决鸟鸣分类比赛中的挑战，并提出了一种获取带标注数据集的方法。实验证明这种方法在鸟类物种分类方面有效，并展示了迁移学习和半监督数据集注释在类似任务中的潜力。

    

    我们提出了关于使用半监督数据集注释进行鸟鸣分类的迁移学习的工作笔记，重点是在记录的声景中识别非洲鸟类物种。我们的方法利用现有的现成模型BirdNET和MixIT来解决比赛中的表示和标注挑战。我们探索了BirdNET学习到的嵌入空间，并提出了一种获取用于监督学习的带标注数据集的方法。我们的实验涉及各种模型和特征工程方法，以在比赛排行榜上达到最佳表现。结果表明我们的方法在鸟类物种分类方面的有效性，并突出了迁移学习和半监督数据集注释在类似任务中的潜力。

    We present working notes on transfer learning with semi-supervised dataset annotation for the BirdCLEF 2023 competition, focused on identifying African bird species in recorded soundscapes. Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenges in the competition. We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning. Our experiments involve various models and feature engineering approaches to maximize performance on the competition leaderboard. The results demonstrate the effectiveness of our approach in classifying bird species and highlight the potential of transfer learning and semi-supervised dataset annotation in similar tasks.
    
[^53]: 时间差分动力学的特征子空间及其如何在强化学习中改进值估计

    Eigensubspace of Temporal-Difference Dynamics and How It Improves Value Approximation in Reinforcement Learning. (arXiv:2306.16750v1 [cs.LG])

    [http://arxiv.org/abs/2306.16750](http://arxiv.org/abs/2306.16750)

    ERC是一种新的值估计方法，通过在深度强化学习中利用时间差分动力学的特征子空间，实现了更高效和稳定的值估计路径。实验证明ERC有效地减少了值函数的方差，并在多项任务中优于其他最先进方法。

    

    我们提出了一种新的值估计方法，即特征子空间规范化批评家（ERC），用于深度强化学习（RL）。 ERC受到了对时序差分（TD）方法中Q值估计误差动力学的分析的启发，该方法遵循由与马尔可夫决策过程（MDP）相关的转移核关联的1-特征子空间定义的路径。它揭示了TD学习的一个基本性质，在先前的深度RL方法中未被使用。在ERC中，我们提出了一个正则化器，指导近似误差趋向于1-特征子空间，从而得到更高效稳定的值估计路径。此外，我们在理论上证明了ERC方法的收敛性。此外，理论分析和实验证明ERC有效地减少了值函数的方差。在DMControl基准测试的26个任务中，ERC优于20个最先进方法。此外，在Q值估计方面也显示出明显的优势。

    We propose a novel value approximation method, namely Eigensubspace Regularized Critic (ERC) for deep reinforcement learning (RL). ERC is motivated by an analysis of the dynamics of Q-value approximation error in the Temporal-Difference (TD) method, which follows a path defined by the 1-eigensubspace of the transition kernel associated with the Markov Decision Process (MDP). It reveals a fundamental property of TD learning that has remained unused in previous deep RL approaches. In ERC, we propose a regularizer that guides the approximation error tending towards the 1-eigensubspace, resulting in a more efficient and stable path of value approximation. Moreover, we theoretically prove the convergence of the ERC method. Besides, theoretical analysis and experiments demonstrate that ERC effectively reduces the variance of value functions. Among 26 tasks in the DMControl benchmark, ERC outperforms state-of-the-art methods for 20. Besides, it shows significant advantages in Q-value approxim
    
[^54]: 评估社交机器人导航算法的原则与指南

    Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])

    [http://arxiv.org/abs/2306.16740](http://arxiv.org/abs/2306.16740)

    本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。

    

    在人类居住环境中导航是部署机器人广泛应用的主要挑战，通常被称为社交机器人导航。虽然社交导航领域近年来取得了很大进展，但评估解决社交导航的算法仍然困难，因为它不仅涉及机器人在静态环境中移动，还涉及到动态的人类参与者及其对机器人行为的感知适应性。相比之下，清晰、可重复、易于获得的基准在计算机视觉、自然语言处理和传统机器人导航等领域加速了进展，使研究人员能够公平比较算法，揭示现有解决方案的局限性，并呈现有前途的新方向。我们相信相同的方法可以有助于社交导航。在本文中，我们为评估社交机器人导航建立了共同、广泛可用且可重复的基准标准，并提出了自己的创新点。

    A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
    
[^55]: 在对抗性样本对抗中优化随机策略

    Towards Optimal Randomized Strategies in Adversarial Example Game. (arXiv:2306.16738v1 [cs.LG])

    [http://arxiv.org/abs/2306.16738](http://arxiv.org/abs/2306.16738)

    该论文提出了一种新算法FRAT，用于在对抗性样本攻击中优化随机策略。该算法通过在概率分布空间上建模问题，并维护轻量级的模型混合来达到目的。

    

    深度神经网络模型对抗性样本攻击的脆弱性是许多人工智能应用中的实际挑战。最近的研究表明，在对抗性训练中使用随机化是找到对抗性样本攻击的最优策略的关键。然而，在一个完全随机化的情况下，防守者和攻击者都可以使用随机策略，目前没有有效的算法来找到这样一个最优策略。为了填补这个空白，我们提出了一种新的算法FRAT，它采用了一个新的无限维连续时间概率分布空间上的流来建模问题。FRAT为防守者维护了一个轻量级模型混合，具有在每次迭代中有效更新混合权重和模型参数的灵活性。此外，FRAT利用轻量级的采样子例程来构建攻击者的随机策略。我们证明了FRAT的连续时间极限的收敛性。

    The vulnerability of deep neural network models to adversarial example attacks is a practical challenge in many artificial intelligence applications. A recent line of work shows that the use of randomization in adversarial training is the key to find optimal strategies against adversarial example attacks. However, in a fully randomized setting where both the defender and the attacker can use randomized strategies, there are no efficient algorithm for finding such an optimal strategy. To fill the gap, we propose the first algorithm of its kind, called FRAT, which models the problem with a new infinite-dimensional continuous-time flow on probability distribution spaces. FRAT maintains a lightweight mixture of models for the defender, with flexibility to efficiently update mixing weights and model parameters at each iteration. Furthermore, FRAT utilizes lightweight sampling subroutines to construct a random strategy for the attacker. We prove that the continuous-time limit of FRAT converg
    
[^56]: 理解深度异方差回归的病态

    Understanding Pathologies of Deep Heteroskedastic Regression. (arXiv:2306.16717v1 [stat.ML])

    [http://arxiv.org/abs/2306.16717](http://arxiv.org/abs/2306.16717)

    该论文研究了利用异方差神经回归模型对真实世界数据进行建模时的困难，并从统计物理的角度提供了解释。作者证明了这些不稳定性不仅适用于神经网络结构，而且已经在过参数化条件高斯似然模型的场论中存在。数值求解结果与实证模型拟合的定性一致性证明了相变的存在。

    

    近期的研究报告了在使用异方差神经回归模型对真实世界数据建模时出现的负面结果。特别是，对于过参数化模型，均值网络和方差网络足够强大，可以拟合每个数据点（同时将预测的方差收缩到零），或者学习一个恒定的预测，输出方差恰好匹配每个预测残差（即将目标解释为纯噪声）。本文从统计物理的角度研究了这些困难。我们证明了观察到的不稳定性不特定于任何神经网络结构，而是已经存在于过参数化条件高斯似然模型的场论中。在轻微的假设下，我们推导出一个可以通过数值求解的非参数自由能。得到的解与真实世界数据上的实证模型拟合具有良好的定性一致性，并且特别证明了相变的存在。

    Several recent studies have reported negative results when using heteroskedastic neural regression models to model real-world data. In particular, for overparameterized models, the mean and variance networks are powerful enough to either fit every single data point (while shrinking the predicted variances to zero), or to learn a constant prediction with an output variance exactly matching every predicted residual (i.e., explaining the targets as pure noise). This paper studies these difficulties from the perspective of statistical physics. We show that the observed instabilities are not specific to any neural network architecture but are already present in a field theory of an overparameterized conditional Gaussian likelihood model. Under light assumptions, we derive a nonparametric free energy that can be solved numerically. The resulting solutions show excellent qualitative agreement with empirical model fits on real-world data and, in particular, prove the existence of phase transit
    
[^57]: 来自图像池的答案挖掘：面向基于检索的视觉问答

    Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering. (arXiv:2306.16713v1 [cs.CV])

    [http://arxiv.org/abs/2306.16713](http://arxiv.org/abs/2306.16713)

    本研究探究了在给定上下文中从相关和无关图像池中挖掘答案的视觉问答问题。我们提出了一个统一的模型，Multi Image BART (MI-BART)，通过检索相关图像并使用相关性编码器进行自由流畅的答案生成。同时，我们还引入了最大的RETVQA数据集，该数据集具有多图像和检索要求，并且可以对一组异构图像进行元数据无关的问题提问。

    

    我们研究在一个给定上下文的相关和无关图像池中挖掘答案的视觉问答问题。在这样的设置中，模型必须首先从图像池中检索相关图像，并从这些检索到的图像中回答问题。我们将这个问题称为基于检索的视觉问答（或简称为RETVQA）。RETVQA与传统研究的视觉问答（VQA）有着明显不同和更大的挑战，传统的VQA要求根据上下文中的单个相关图像回答给定的问题。为了解决RETVQA任务，我们提出了一个统一的Multi Image BART（MI-BART）模型，该模型使用我们的相关性编码器来生成自由流畅的答案。此外，我们还引入了这个领域最大的数据集，即RETVQA，具有以下显著特点：VQA的多图像和检索要求，对一组异构图像进行元数据无关的问题提问。

    We study visual question answering in a setting where the answer has to be mined from a pool of relevant and irrelevant images given as a context. For such a setting, a model must first retrieve relevant images from the pool and answer the question from these retrieved images. We refer to this problem as retrieval-based visual question answering (or RETVQA in short). The RETVQA is distinctively different and more challenging than the traditionally-studied Visual Question Answering (VQA), where a given question has to be answered with a single relevant image in context. Towards solving the RETVQA task, we propose a unified Multi Image BART (MI-BART) that takes a question and retrieved images using our relevance encoder for free-form fluent answer generation. Further, we introduce the largest dataset in this space, namely RETVQA, which has the following salient features: multi-image and retrieval requirement for VQA, metadata-independent questions over a pool of heterogeneous images, exp
    
[^58]: 弹性约束下的元学习器用于联邦学习

    Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])

    [http://arxiv.org/abs/2306.16703](http://arxiv.org/abs/2306.16703)

    这项研究提出了一种弹性约束的元学习方法，用于解决联邦学习中由于非独立同分布数据导致元学习的不稳定目标的收敛问题。

    

    联邦学习是一种协作训练机器学习模型的方法，用于多个参与方之间禁止数据共享。在联邦学习中的一个挑战是客户端之间的非独立同分布数据，因为单个模型无法适应所有客户端的数据分布。为了解决这个问题，介绍了元学习（如Per-FedAvg）。元学习学习适用于所有客户端的共享初始参数。每个客户端使用梯度下降法将初始化快速调整到本地数据分布，实现模型个性化。然而，由于非凸损失函数和采样更新的随机性，元学习方法在本地适应同一客户端时具有不稳定的目标。这种不同适应方向的波动阻碍了元学习的收敛。为了克服这个挑战，我们使用了历史本地调整的模型来限制内循环的方向，并提出了一种弹性约束方法。

    Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
    
[^59]: 对象堆叠操作的动态分辨率模型学习

    Dynamic-Resolution Model Learning for Object Pile Manipulation. (arXiv:2306.16700v1 [cs.RO])

    [http://arxiv.org/abs/2306.16700](http://arxiv.org/abs/2306.16700)

    本文研究了对象堆叠操作的动态分辨率模型学习，通过构建动态分辨率的粒子环境表示并使用图神经网络进行学习，实现了学习的动态和自适应表示，在对象堆叠操作任务中取得了良好的效果。（Translated from Abstract）

    

    从视觉观察中学习到的动力学模型在各种机器人操作任务中表现出很好的效果。学习这种动力学模型的一个关键问题是使用什么场景表示。之前的研究通常假设固定维度或分辨率的表示，这对简单任务可能效率低下，对复杂任务可能无效。在这项工作中，我们研究如何学习不同抽象层次的动态和自适应表示，以实现效率和效果之间的最优平衡。具体而言，我们构建了动态分辨率的粒子环境表示，并使用图神经网络（GNNs）学习了统一的动力学模型，允许连续选择抽象层次。在测试时，代理可以自适应地确定每个模型预测控制（MPC）步骤的最佳分辨率。我们在对象堆叠操作中评估了我们的方法，这是我们在烹饪, 农业等领域经常遇到的任务。

    Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agric
    
[^60]: 快速-INR: 使用隐式神经表示进行效率高的无CPU深度神经网络训练

    Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])

    [http://arxiv.org/abs/2306.16699](http://arxiv.org/abs/2306.16699)

    本文提出了一种使用隐式神经表示进行高效的无CPU深度神经网络训练的新方法，通过在GPU上直接存储整个数据集以INR格式，减少了数据传输开销，从而加速训练过程。同时，采用高度并行化和实时执行的解码过程，进一步提升了压缩效果。

    

    隐式神经表示(INR)是一种创新方法，用于表示复杂的形状或对象，而无需明确定义它们的几何形状或表面结构。相反，INR将对象表示为连续函数。先前的研究已经证明了将神经网络用作INR进行图像压缩的有效性，展示了与传统方法（如JPEG）相当的性能。然而，INR在图像压缩之外还具有各种应用潜力。本文介绍了Rapid-INR，一种利用INR对图像进行编码和压缩的新方法，从而加速计算机视觉任务中的神经网络训练。我们的方法在GPU上直接以INR格式存储整个数据集，减少了训练过程中CPU和GPU之间的数据传输开销。此外，从INR到RGB格式的解码过程高度并行化并实时执行。为了进一步提高压缩效果，我们提出了一种迭代的图像压缩算法。

    Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
    
[^61]: SRL: 将分布式强化学习扩展到一万多个核心

    SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])

    [http://arxiv.org/abs/2306.16688](http://arxiv.org/abs/2306.16688)

    SRL是一个可扩展，高效，可扩展的分布式强化学习系统，通过一种新的抽象框架统一了各种实际强化学习训练，并实现了精细优化。

    

    强化学习（RL）任务的不断复杂化要求分布式RL系统可以高效地生成和处理大量数据以训练智能Agent。然而，现有的开源库存在各种限制，阻碍了它们在需要大规模训练的挑战性场景中的实际应用。虽然OpenAI和DeepMind的工业系统已经成功实现了大规模RL训练，但是它们的系统架构和实现细节对社区来说仍然不公开。在本文中，我们提出了RL训练数据流的新抽象，将各种应用中的实际RL训练统一成一个通用框架，并实现了精细优化。根据这个抽象，我们开发了一个可扩展、高效、可扩展的分布式RL系统，名为"ReaLly Scalable RL（SRL）"。

    The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
    
[^62]: BinaryViT：将二进制视觉Transformer推向卷积模型

    BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models. (arXiv:2306.16678v1 [cs.CV])

    [http://arxiv.org/abs/2306.16678](http://arxiv.org/abs/2306.16678)

    BinaryViT是一种针对二值化视觉Transformer的改进模型，通过借鉴CNN的架构特性，提高了二值化ViT的表示能力和性能。

    

    随着视觉Transformer（ViT）的日益流行和规模的增加，人们越来越关注如何使它们在计算资源有限的边缘设备上更高效、计算成本更低。通过使用二值化，在权重和激活值为二进制时可以显著减小ViT模型的大小和计算成本，使用popcount操作。然而，与CNN在具有大量类别的数据集（如ImageNet-1k）上直接应用卷积神经网络（CNN）二值化方法或现有的二值化方法相比，ViT的性能下降更大。经过广泛分析，我们发现，二值化的基础ViT（如DeiT）缺少许多CNN所具有的关键架构特性，这些特性使二值化的CNN具有比基础ViT更高的表示能力。因此，我们提出了BinaryViT，受CNN架构启发，

    With the increasing popularity and the increasing size of vision transformers (ViTs), there has been an increasing interest in making them more efficient and less computationally costly for deployment on edge devices with limited computing resources. Binarization can be used to help reduce the size of ViT models and their computational cost significantly, using popcount operations when the weights and the activations are in binary. However, ViTs suffer a larger performance drop when directly applying convolutional neural network (CNN) binarization methods or existing binarization methods to binarize ViTs compared to CNNs on datasets with a large number of classes such as ImageNet-1k. With extensive analysis, we find that binary vanilla ViTs such as DeiT miss out on a lot of key architectural properties that CNNs have that allow binary CNNs to have much higher representational capability than binary vanilla ViT. Therefore, we propose BinaryViT, in which inspired by the CNN architecture,
    
[^63]: 使用学习的关卡表示进行游戏关卡混合

    Game Level Blending using a Learned Level Representation. (arXiv:2306.16666v1 [cs.LG])

    [http://arxiv.org/abs/2306.16666](http://arxiv.org/abs/2306.16666)

    本文介绍了一种使用聚类-based 平铺嵌入的方法，通过学习的关卡表示来实现游戏关卡混合，为未注释的游戏提供关卡表示，并在游戏之间提供统一的关卡表示，而无需人工注释。

    

    近年来，通过机器学习进行游戏关卡混合的方法在游戏产生技术领域逐渐流行起来。然而，许多现有的技术依赖于人工注释的关卡表示，从而限制了游戏关卡混合的数量。即使有人工注释的游戏，研究人员还需要创建一个额外的共享表示才能进行混合。本文提出了一种新颖的游戏关卡混合方法，使用了基于聚类的平铺嵌入（CTE），这是一种学习的关卡表示技术，可以为非注释游戏提供关卡表示，并在游戏之间提供统一的关卡表示，而无需人工注释。

    Game level blending via machine learning, the process of combining features of game levels to create unique and novel game levels using Procedural Content Generation via Machine Learning (PCGML) techniques, has gained increasing popularity in recent years. However, many existing techniques rely on human-annotated level representations, which limits game level blending to a limited number of annotated games. Even with annotated games, researchers often need to author an additional shared representation to make blending possible. In this paper, we present a novel approach to game level blending that employs Clustering-based Tile Embeddings (CTE), a learned level representation technique that can serve as a level representation for unannotated games and a unified level representation across games without the need for human annotation. CTE represents game level tiles as a continuous vector representation, unifying their visual, contextual, and behavioral information. We apply this approach
    
[^64]: 利用游戏视频进行关卡生成和翻译的联合模型

    Joint Level Generation and Translation Using Gameplay Videos. (arXiv:2306.16662v1 [cs.CV])

    [http://arxiv.org/abs/2306.16662](http://arxiv.org/abs/2306.16662)

    该论文提出了一种利用游戏视频进行关卡生成和翻译的联合模型，通过学习同时进行关卡翻译和生成，解决了机器学习关卡生成技术中受限注释数据的问题。

    

    利用机器学习的程序生成技术面临着一个与其他领域（如图像或文本生成）不同的显著障碍，即受限的注释数据。许多现有的机器学习关卡生成方法需要除了关卡图像之外的辅助表示。然而，获取这些表示的当前方法是费时费力的，这导致了这个问题的存在。在这项工作中，我们旨在通过利用两个人工注释游戏的游戏视频来开发一种新的多目标框架，学习同时进行关卡翻译和生成。我们的框架的翻译部分可以将游戏视频帧转换为等效的辅助表示，而生成部分可以产生新的关卡段落。评估结果和与基准方法的比较表明，结合关卡生成和翻译任务可以实现更好的效果。

    Procedural Content Generation via Machine Learning (PCGML) faces a significant hurdle that sets it apart from other fields, such as image or text generation, which is limited annotated data. Many existing methods for procedural level generation via machine learning require a secondary representation besides level images. However, the current methods for obtaining such representations are laborious and time-consuming, which contributes to this problem. In this work, we aim to address this problem by utilizing gameplay videos of two human-annotated games to develop a novel multi-tail framework that learns to perform simultaneous level translation and generation. The translation tail of our framework can convert gameplay video frames to an equivalent secondary representation, while its generation tail can produce novel level segments. Evaluation results and comparisons between our framework and baselines suggest that combining the level generation and translation tasks can lead to an over
    
[^65]: NaturalInversion: 无需真实数据的图像合成方法，提升现实世界的一致性

    NaturalInversion: Data-Free Image Synthesis Improving Real-World Consistency. (arXiv:2306.16661v1 [cs.CV])

    [http://arxiv.org/abs/2306.16661](http://arxiv.org/abs/2306.16661)

    NaturalInversion 是一种无需真实数据的图像合成方法，通过特征传递金字塔、一对一生成模型和可学习的自适应通道缩放参数，合成的图像与原始数据分布更加一致，并在性能上超过以前的方法。

    

    我们介绍了一种名为 NaturalInversion 的新颖的基于模型反演的方法，可以合成与原始数据分布相符的图像，而无需使用真实数据。在 NaturalInversion 中，我们提出了以下几点创新：（1）特征传递金字塔，在预训练分类器提取的多尺度特征图的基础上，使用增强的原始数据图像先验信息；（2）一对一生成模型，每个生成器只合成一个批次的图像，以引入非线性度量并简化整个优化过程；（3）可学习的自适应通道缩放参数，以调整输出图像通道并更好地利用原始图像先验信息。通过使用我们的 NaturalInversion，我们从在 CIFAR-10/100 上训练的分类器合成图像，并通过可视化和附加分析显示，我们的合成图像与原始数据分布的一致性比以前的方法更好。此外，我们的合成图像在性能上超过以前的方法。

    We introduce NaturalInversion, a novel model inversion-based method to synthesize images that agrees well with the original data distribution without using real data. In NaturalInversion, we propose: (1) a Feature Transfer Pyramid which uses enhanced image prior of the original data by combining the multi-scale feature maps extracted from the pre-trained classifier, (2) a one-to-one approach generative model where only one batch of images are synthesized by one generator to bring the non-linearity to optimization and to ease the overall optimizing process, (3) learnable Adaptive Channel Scaling parameters which are end-to-end trained to scale the output image channel to utilize the original image prior further. With our NaturalInversion, we synthesize images from classifiers trained on CIFAR-10/100 and show that our images are more consistent with original data distribution than prior works by visualization and additional analysis. Furthermore, our synthesized images outperform prior w
    
[^66]: 复高斯扰动下的私有协方差近似和特征值间隙边界

    Private Covariance Approximation and Eigenvalue-Gap Bounds for Complex Gaussian Perturbations. (arXiv:2306.16648v1 [cs.DS])

    [http://arxiv.org/abs/2306.16648](http://arxiv.org/abs/2306.16648)

    本文研究了在差分隐私下，利用复高斯机制近似一个协方差矩阵为秩k矩阵的问题。通过分析复高斯噪声扰动下矩阵的特征值，我们证明了当M的第k个特征值和第k+1个特征值之间有足够大的间隔时，复高斯机制产生的矩阵与M的最佳秩k近似之间的差距的界限为O(√kd)。

    

    本文考虑在（ε，δ）-差分隐私保护下，近似一个d×d协方差矩阵M为秩为k的矩阵的问题。我们提出并分析了复高斯机制的变体，并证明了此机制产生的矩阵与M的最佳秩k近似之间的弗罗贝尼乌斯范数差的界限约为O(√kd)，只要M的第k个特征值和第k+1个特征值之间有一个足够大的间隔。这改进了以前的工作，以类似的界限需要M的所有前k个特征值之间的间隔至少为√d。我们的分析利用了复矩阵布朗运动的特征值比实数情况下更容易远离，利用戴森的随机微分方程来描述特征值的演化，从而证明了复高斯噪声扰动下的矩阵M的特征值具有很大的间隔的概率较高。

    We consider the problem of approximating a $d \times d$ covariance matrix $M$ with a rank-$k$ matrix under $(\varepsilon,\delta)$-differential privacy. We present and analyze a complex variant of the Gaussian mechanism and show that the Frobenius norm of the difference between the matrix output by this mechanism and the best rank-$k$ approximation to $M$ is bounded by roughly $\tilde{O}(\sqrt{kd})$, whenever there is an appropriately large gap between the $k$'th and the $k+1$'th eigenvalues of $M$. This improves on previous work that requires that the gap between every pair of top-$k$ eigenvalues of $M$ is at least $\sqrt{d}$ for a similar bound. Our analysis leverages the fact that the eigenvalues of complex matrix Brownian motion repel more than in the real case, and uses Dyson's stochastic differential equations governing the evolution of its eigenvalues to show that the eigenvalues of the matrix $M$ perturbed by complex Gaussian noise have large gaps with high probability. Our resu
    
[^67]: CMATH：你的语言模型能通过中国小学数学测试吗？

    CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?. (arXiv:2306.16636v1 [cs.CL])

    [http://arxiv.org/abs/2306.16636](http://arxiv.org/abs/2306.16636)

    该论文介绍了中国小学数学应用题（CMATH）数据集，评估了多个流行的大型语言模型（LLMs）在小学数学不同年级的表现。研究发现只有GPT-4在所有年级中取得成功，并且能够保持鲁棒性，而其他模型则在不同年级上表现较差。

    

    我们提出了中国小学数学应用题（CMATH）数据集，包含了1.7k个具有详细注释的小学水平数学应用题，来源于中国实际的练习和考试。该数据集旨在提供一个评估流行的大型语言模型（LLMs）能够达到小学数学哪个年级水平的基准工具。我们评估了各种流行的LLMs，包括商业和开源选项，并发现只有GPT-4在所有六个小学年级中都取得了成功（准确率≥60%），而其他模型在不同年级上的表现欠佳。此外，我们通过添加干扰信息来评估几个表现最佳的LLMs的鲁棒性。我们的发现显示GPT-4能够保持鲁棒性，而其他模型则失败。我们预计我们的研究将揭示LLMs在算术和推理能力方面的局限性。

    We present the Chinese Elementary School Math Word Problems (CMATH) dataset, comprising 1.7k elementary school-level math word problems with detailed annotations, source from actual Chinese workbooks and exams. This dataset aims to provide a benchmark tool for assessing the following question: to what grade level of elementary school math do the abilities of popular large language models (LLMs) correspond? We evaluate a variety of popular LLMs, including both commercial and open-source options, and discover that only GPT-4 achieves success (accuracy $\geq$ 60\%) across all six elementary school grades, while other models falter at different grade levels. Furthermore, we assess the robustness of several top-performing LLMs by augmenting the original problems in the CMATH dataset with distracting information. Our findings reveal that GPT-4 is able to maintains robustness, while other model fail. We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabi
    
[^68]: 改进深度伪造检测中的公平性

    Improving Fairness in Deepfake Detection. (arXiv:2306.16635v1 [cs.CV])

    [http://arxiv.org/abs/2306.16635](http://arxiv.org/abs/2306.16635)

    本研究首次尝试通过提出新的损失函数来改善深度伪造检测的公平性，并在多个数据集和检测器上进行了广泛实验证明了其有效性。

    

    尽管近年来已经开发出有效的深度伪造检测模型，但是一些最近的研究表明，在开发深度伪造检测模型时所使用的训练数据中存在偏见可能导致不同种族和/或性别的人群的不公平表现。这可能导致这些群体受到不公平的定位或被排除在检测之外，从而让被错误分类的深度伪造操纵舆论并破坏对模型的信任。虽然这些研究着重于确定和评估深度伪造检测中的不公平性，但目前还没有开发出解决深度伪造检测算法层面公平性问题的方法。在这项工作中，我们首次尝试通过提出新的损失函数来改进深度伪造检测的公平性，以在不考虑或考虑人口因素的情况下训练公平的深度伪造检测模型。对四个深度伪造数据集和五个深度伪造检测器的大量实验证明了这种方法的有效性和灵活性。

    Despite the development of effective deepfake detection models in recent years, several recent studies have demonstrated that biases in the training data utilized to develop deepfake detection models can lead to unfair performance for demographic groups of different races and/or genders. Such can result in these groups being unfairly targeted or excluded from detection, allowing misclassified deepfakes to manipulate public opinion and erode trust in the model. While these studies have focused on identifying and evaluating the unfairness in deepfake detection, no methods have been developed to address the fairness issue of deepfake detection at the algorithm level. In this work, we make the first attempt to improve deepfake detection fairness by proposing novel loss functions to train fair deepfake detection models in ways that are agnostic or aware of demographic factors. Extensive experiments on four deepfake datasets and five deepfake detectors demonstrate the effectiveness and flexi
    
[^69]: MNISQ：用于NISQ时代量子计算机上/为量子计算机的机器学习的大规模量子电路数据集

    MNISQ: A Large-Scale Quantum Circuit Dataset for Machine Learning on/for Quantum Computers in the NISQ era. (arXiv:2306.16627v1 [quant-ph])

    [http://arxiv.org/abs/2306.16627](http://arxiv.org/abs/2306.16627)

    MNISQ 是一个大规模的量子电路数据集，用于 NISQ 时代的量子和经典机器学习研究。该数据集由 4,950,000 个数据点组成，以量子和经典形式呈现，旨在提升机器学习，并促进量子计算的发展。

    

    我们引入了第一个大规模的数据集MNISQ，用于NISQ时代的量子和经典机器学习社区。MNISQ包含了4,950,000个数据点，分为9个子数据集。通过将经典信息（例如MNIST数据集）进行量子编码，我们提供了一个双重形式的数据集：量子形式的电路和经典形式的量子电路描述（量子编程语言，QASM）。在量子计算机相关的机器学习研究中，我们面临着双重挑战：利用量子计算机的能力提升机器学习，同时利用最先进的经典机器学习方法促进量子计算的发展。因此，我们对我们的数据集进行了电路分类，同时采用量子和经典模型进行任务处理。在量子方面，我们使用量子核方法测试了我们的电路数据集，并展示了...

    We introduce the first large-scale dataset, MNISQ, for both the Quantum and the Classical Machine Learning community during the Noisy Intermediate-Scale Quantum era. MNISQ consists of 4,950,000 data points organized in 9 subdatasets. Building our dataset from the quantum encoding of classical information (e.g., MNIST dataset), we deliver a dataset in a dual form: in quantum form, as circuits, and in classical form, as quantum circuit descriptions (quantum programming language, QASM). In fact, also the Machine Learning research related to quantum computers undertakes a dual challenge: enhancing machine learning exploiting the power of quantum computers, while also leveraging state-of-the-art classical machine learning methodologies to help the advancement of quantum computing. Therefore, we perform circuit classification on our dataset, tackling the task with both quantum and classical models. In the quantum endeavor, we test our circuit dataset with Quantum Kernel methods, and we show 
    
[^70]: 评估一维卷积神经网络在从拉曼光谱预测混合组分浓度上的性能

    Assessing the Performance of 1D-Convolution Neural Networks to Predict Concentration of Mixture Components from Raman Spectra. (arXiv:2306.16621v1 [eess.SP])

    [http://arxiv.org/abs/2306.16621](http://arxiv.org/abs/2306.16621)

    该论文介绍了一种评估1D-卷积神经网络在从拉曼光谱预测混合组分浓度方面的性能的方法，并提出了一个Python软件包RaMix，该软件包可以生成具有可控噪声水平的合成拉曼混合数据集，用于评估不同化学计量学算法在实时监测应用中的效果。

    

    拉曼光谱是监测生物药物生产过程中化学反应器状态的新兴应用。拉曼频移强度与化学物种的浓度呈线性关系，因此可以利用非破坏性的光照射以无标记的方式分析实时浓度。化学计量学算法用于解释随着反应的进展而产生的复杂混合物中的拉曼光谱。由于缺乏公开可用的拉曼混合数据集，为特定的生物反应器环境找到最佳算法是具有挑战性的。RaMix Python软件包通过生成具有可控噪声水平的合成拉曼混合数据集来评估不同化学计量学算法类型在实时监测应用中的有效性。为了展示该软件包的功能并比较不同化学计量学算法的性能，作者使用48个合成的拉曼混合数据集。

    An emerging application of Raman spectroscopy is monitoring the state of chemical reactors during biologic drug production. Raman shift intensities scale linearly with the concentrations of chemical species and thus can be used to analytically determine real-time concentrations using non-destructive light irradiation in a label-free manner. Chemometric algorithms are used to interpret Raman spectra produced from complex mixtures of bioreactor contents as a reaction evolves. Finding the optimal algorithm for a specific bioreactor environment is challenging due to the lack of freely available Raman mixture datasets. The RaMix Python package addresses this challenge by enabling the generation of synthetic Raman mixture datasets with controllable noise levels to assess the utility of different chemometric algorithm types for real-time monitoring applications. To demonstrate the capabilities of this package and compare the performance of different chemometric algorithms, 48 datasets of simu
    
[^71]: 在黎曼流形上的游戏中无关曲率的最后收敛性

    Curvature-Independent Last-Iterate Convergence for Games on Riemannian Manifolds. (arXiv:2306.16617v1 [math.OC])

    [http://arxiv.org/abs/2306.16617](http://arxiv.org/abs/2306.16617)

    该论文通过对黎曼梯度下降算法进行分析，证明了在测地线强单调设置下，具有对曲率不敏感的固定步长的RGD方案可以实现曲率无关和线性的最后收敛速度。

    

    机器学习和数据分析中的许多应用可以以黎曼流形上的均衡计算形式化。尽管对它们的欧几里德对应物进行了大量研究，但黎曼梯度下降算法的性能仍然不透明且难以理解。我们重新审视了黎曼梯度下降（RGD）的原始方案，并在对测地线单调性假设进行分析，其中包括了研究充分的测地线凸凹极值优化问题作为一个特殊情况。我们的主要贡献是表明，尽管存在距离失真现象，但具有对曲率不敏感的固定步长的RGD方案在测地线强单调设置下可以实现曲率无关和线性的最后收敛速度。据我们所知，以前从未考虑过在黎曼设置中存在曲率无关速率和/或最后收敛性的可能性。

    Numerous applications in machine learning and data analytics can be formulated as equilibrium computation over Riemannian manifolds. Despite the extensive investigation of their Euclidean counterparts, the performance of Riemannian gradient-based algorithms remain opaque and poorly understood. We revisit the original scheme of Riemannian gradient descent (RGD) and analyze it under a geodesic monotonicity assumption, which includes the well-studied geodesically convex-concave min-max optimization problem as a special case. Our main contribution is to show that, despite the phenomenon of distance distortion, the RGD scheme, with a step size that is agnostic to the manifold's curvature, achieves a curvature-independent and linear last-iterate convergence rate in the geodesically strongly monotone setting. To the best of our knowledge, the possibility of curvature-independent rates and/or last-iterate convergence in the Riemannian setting has not been considered before.
    
[^72]: 基于群体的鲁棒性：现实世界中定制鲁棒性的通用框架

    Group-based Robustness: A General Framework for Customized Robustness in the Real World. (arXiv:2306.16614v1 [cs.LG])

    [http://arxiv.org/abs/2306.16614](http://arxiv.org/abs/2306.16614)

    本研究提出了一种基于群体的鲁棒性指标，可以更好地评估机器学习模型在现实世界中抵抗攻击的能力，弥补了传统指标的不足。实验证明，该指标能够区分模型对特定威胁的脆弱性。

    

    众所周知，机器学习模型容易受到逃避攻击的影响，即通过扰动模型输入来引起错误分类。本研究中，我们发现传统的度量目标和非目标鲁棒性的指标无法准确评估现实世界中的真实威胁。为了解决现有方法的缺陷，我们正式定义了一种新的指标，称为基于群体的鲁棒性，它补充了现有的度量标准，并更适合评估特定攻击场景下的模型性能。我们通过实验证明，基于群体的鲁棒性能够在传统的鲁棒性指标不适用的情况下区分模型对特定威胁模型的脆弱性。此外，为了有效准确地衡量基于群体的鲁棒性，我们提出了两个损失函数。

    Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss func
    
[^73]: GuidedMixup：由显著性图引导的高效Mixup策略

    GuidedMixup: An Efficient Mixup Strategy Guided by Saliency Maps. (arXiv:2306.16612v1 [cs.CV])

    [http://arxiv.org/abs/2306.16612](http://arxiv.org/abs/2306.16612)

    提出了一种新颖的显著性感知混合方法GuidedMixup，通过优化配对图像中显著区域的冲突，以低计算开销在混合图像中保留显著区域。多个数据集上的实验证明，GuidedMixup在增强效果和计算效率之间取得了良好的平衡。

    

    数据增强现在是图像训练过程中必不可少的一部分，它有效地防止了过拟合，并使模型对噪声数据集更加鲁棒。最近的混合增强策略发展出了能够丰富显著性信息的混合掩模，这是一种监督信号。然而，这些方法需要很大的计算负担来优化混合掩模。出于这个动机，我们提出了一种新颖的显著性感知混合方法GuidedMixup，它旨在在混合图像中保留显著区域，并具有较低的计算开销。我们开发了一种高效的配对算法，旨在最小化配对图像中显著区域的冲突，并在混合图像中实现丰富的显著性。此外，GuidedMixup通过平滑地插值两个配对图像来控制每个像素的混合比例，以更好地保留显著区域。在多个数据集上的实验证明，GuidedMixup在增强效果和计算效率之间提供了一个良好的平衡。

    Data augmentation is now an essential part of the image training process, as it effectively prevents overfitting and makes the model more robust against noisy datasets. Recent mixing augmentation strategies have advanced to generate the mixup mask that can enrich the saliency information, which is a supervisory signal. However, these methods incur a significant computational burden to optimize the mixup mask. From this motivation, we propose a novel saliency-aware mixup method, GuidedMixup, which aims to retain the salient regions in mixup images with low computational overhead. We develop an efficient pairing algorithm that pursues to minimize the conflict of salient regions of paired images and achieve rich saliency in mixup images. Moreover, GuidedMixup controls the mixup ratio for each pixel to better preserve the salient region by interpolating two paired images smoothly. The experiments on several datasets demonstrate that GuidedMixup provides a good trade-off between augmentatio
    
[^74]: 用于CPU上基于Transformer的语言模型的高效稀疏推断软件加速器

    An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs. (arXiv:2306.16601v1 [cs.LG])

    [http://arxiv.org/abs/2306.16601](http://arxiv.org/abs/2306.16601)

    本文提出了一个用于基于Transformer的语言模型的高效稀疏推断软件加速器，在CPU上利用Intel Deep Learning Boost实现了稀疏矩阵-稠密矩阵乘法的优化，相较于现有的稀疏库，在各种形状和稀疏度下都获得了一个数量级的性能提升。

    

    近年来，基于Transformer的语言模型已成为自然语言处理任务的标准方法。然而，在工业应用中，严格的吞吐量和延迟要求限制了它们的采用。为了缓解这一差距，我们采用了结构化剪枝等模型压缩技术来提高推断效率。然而，大多数现有的神经网络推断运行时对结构化稀疏性缺乏充分的支持。本文提出了一种高效的稀疏深度学习推断软件堆栈，用于基于Transformer的语言模型，其中权重使用恒定的块大小进行剪枝。我们的稀疏软件加速器利用Intel Deep Learning Boost在CPU上最大化稀疏矩阵-稠密矩阵乘法（通常被缩写为SpMM）的性能。在广泛的GEMM形状和5个代表性稀疏度水平下，我们的SpMM内核的性能优于现有的稀疏库（oneMKL、TVM和LIBXSMM）一个数量级。

    In recent years, Transformer-based language models have become the standard approach for natural language processing tasks. However, stringent throughput and latency requirements in industrial applications are limiting their adoption. To mitigate the gap, model compression techniques such as structured pruning are being used to improve inference efficiency. However, most existing neural network inference runtimes lack adequate support for structured sparsity. In this paper, we propose an efficient sparse deep learning inference software stack for Transformer-based language models where the weights are pruned with constant block size. Our sparse software accelerator leverages Intel Deep Learning Boost to maximize the performance of sparse matrix - dense matrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel outperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an order of magnitude on a wide range of GEMM shapes under 5 representative sparsity ra
    
[^75]: 动态时间序列的发展预测在时间不变性和线性性的帮助下

    Forecasting of the development of a partially-observed dynamical time series with the aid of time-invariance and linearity. (arXiv:2306.16593v1 [stat.ME])

    [http://arxiv.org/abs/2306.16593](http://arxiv.org/abs/2306.16593)

    本研究提出了一种自回归松弛时间序列（ARS）模型，通过考虑动态系统的时间不变性和线性性，同时估计演化函数和缺失变量，用于预测动态时间序列中缺失变量的发展。

    

    动态系统产生一种依赖多元序列，称为动态时间序列，通过演化函数发展而来。由于当前时间点的动态时间序列变量通常依赖于前一个时间点的所有变量，现有研究通过估计演化函数来预测未来时间点的变量。然而，在某些实际情况下，动态时间序列中的一些变量是缺失的。本研究提出了一种自回归松弛时间序列（ARS）模型。ARS模型涉及演化函数和作为松弛时间序列的潜在缺失变量的同时估计，借助于动态系统的时间不变性和线性性。本研究实证了提出的ARS模型的有效性。

    A dynamical system produces a dependent multivariate sequence called dynamical time series, developed with an evolution function. As variables in the dynamical time series at the current time-point usually depend on the whole variables in the previous time-point, existing studies forecast the variables at the future time-point by estimating the evolution function. However, some variables in the dynamical time-series are missing in some practical situations. In this study, we propose an autoregressive with slack time series (ARS) model. ARS model involves the simultaneous estimation of the evolution function and the underlying missing variables as a slack time series, with the aid of the time-invariance and linearity of the dynamical system. This study empirically demonstrates the effectiveness of the proposed ARS model.
    
[^76]: 在具有未知和随机奖励的臂上分配可分资源

    Allocating Divisible Resources on Arms with Unknown and Random Rewards. (arXiv:2306.16578v1 [cs.LG])

    [http://arxiv.org/abs/2306.16578](http://arxiv.org/abs/2306.16578)

    本论文研究了在每个周期将一单位可分资源分配到多个臂上的问题，臂上的奖励是未知和随机的，而且与分配的资源成比例，而方差与分配资源的阶数成比例。我们设计了两种算法，实现了不同阶数下的最优有界和无界遗憾，结果表明在阶数为1/2时存在相变现象。

    

    我们考虑一个决策者在每个周期将一个可再生和可分资源分配到多个臂上。这些臂具有未知和随机的奖励，其均值与分配的资源成比例，方差与分配资源的阶数$b$成比例。特别地，如果决策者在一个周期将资源$A_i$分配给臂$i$，那么奖励$Y_i$是$Y_i(A_i)=A_i\mu_i+A_i^b\xi_i$，其中$\mu_i$是未知的均值，噪声$\xi_i$是独立且子高斯的。当阶数$b$从0到1变化时，该框架平滑地连接了标准的随机多臂赌博机和带有完全反馈的在线学习。我们设计了两种算法，它们实现了$b\in[0,1]$时的最优有界差和无界差的遗憾界，并展示了在$b=1/2$处的相变。理论结果依赖于我们开发的一种新型浓度不等式，它限制了子高斯随机变量的线性组合。

    We consider a decision maker allocating one unit of renewable and divisible resource in each period on a number of arms. The arms have unknown and random rewards whose means are proportional to the allocated resource and whose variances are proportional to an order $b$ of the allocated resource. In particular, if the decision maker allocates resource $A_i$ to arm $i$ in a period, then the reward $Y_i$ is$Y_i(A_i)=A_i \mu_i+A_i^b \xi_{i}$, where $\mu_i$ is the unknown mean and the noise $\xi_{i}$ is independent and sub-Gaussian. When the order $b$ ranges from 0 to 1, the framework smoothly bridges the standard stochastic multi-armed bandit and online learning with full feedback. We design two algorithms that attain the optimal gap-dependent and gap-independent regret bounds for $b\in [0,1]$, and demonstrate a phase transition at $b=1/2$. The theoretical results hinge on a novel concentration inequality we have developed that bounds a linear combination of sub-Gaussian random variables w
    
[^77]: 有限样本下具有费舍尔信息速率的对称均值估计

    Finite-Sample Symmetric Mean Estimation with Fisher Information Rate. (arXiv:2306.16573v1 [math.ST])

    [http://arxiv.org/abs/2306.16573](http://arxiv.org/abs/2306.16573)

    本文研究了有限样本下对称均值估计的问题，并给出了基于费舍尔信息的保证。对于对称分布，可以获得收敛到次高斯的收敛速度，而不需要渐近条件。

    

    对于一个未知方差为$\sigma^2$的分布$f$，可以通过$n$个样本以方差$\frac{\sigma^2}{n}$和几乎相对应的次高斯速率来估计均值。当$f$已知且对称时，可以在渐近条件下将其改进为$\frac{1}{n\mathcal I}$，其中$\mathcal I$为该分布的费舍尔信息。然而，对于一般的未知分布$f$，这样的改进是不可能的。但是，Stone(1975)证明了当$f$关于其均值对称时，这种渐近收敛是可能的。然而，Stone的界限是渐近的，即收敛所需的$n$以未指定的方式取决于分布$f$和失败概率$\delta$。在本文中，我们就对称均值估计的费舍尔信息给出有限样本的保证。对于每个$f,n,\delta$满足$n > \log \frac{1}{\delta}$，我们可以得到收敛到方差为$\frac{1}{n \mathcal I_r}$的次高斯附近的收敛，其中$\mathcal I_r$是$r$-$\textit{平滑化}$费舍尔信息。

    The mean of an unknown variance-$\sigma^2$ distribution $f$ can be estimated from $n$ samples with variance $\frac{\sigma^2}{n}$ and nearly corresponding subgaussian rate. When $f$ is known up to translation, this can be improved asymptotically to $\frac{1}{n\mathcal I}$, where $\mathcal I$ is the Fisher information of the distribution. Such an improvement is not possible for general unknown $f$, but [Stone, 1975] showed that this asymptotic convergence $\textit{is}$ possible if $f$ is $\textit{symmetric}$ about its mean. Stone's bound is asymptotic, however: the $n$ required for convergence depends in an unspecified way on the distribution $f$ and failure probability $\delta$. In this paper we give finite-sample guarantees for symmetric mean estimation in terms of Fisher information. For every $f, n, \delta$ with $n > \log \frac{1}{\delta}$, we get convergence close to a subgaussian with variance $\frac{1}{n \mathcal I_r}$, where $\mathcal I_r$ is the $r$-$\textit{smoothed}$ Fisher in
    
[^78]: 特征选择：对属性间协作的视角

    Feature Selection: A perspective on inter-attribute cooperation. (arXiv:2306.16559v1 [cs.LG])

    [http://arxiv.org/abs/2306.16559](http://arxiv.org/abs/2306.16559)

    本文综述了辅助特征间协作的过滤特征选择方法的最新研究进展，并总结了不同方法在文献中的贡献。同时提出了当前存在的问题和挑战，以确定未来有前景的研究和发展方向。

    

    高维数据对数据挖掘和机器学习中的学习任务构成了挑战。特征选择是处理维度缩减的一种有效技术，通常是在应用学习算法之前的重要数据处理步骤。在过去几十年中，过滤特征选择方法从简单的单变量相关性排序算法发展到更复杂的相关性-冗余权衡和基于多元依赖性的方法。这种捕捉多变量依赖的趋势旨在通过特征间的互相合作获取关于类别的独特信息。本文对辅助特征间协作的过滤特征选择方法的最新研究工作进行了全面的调查，并总结了文献中不同方法的贡献。此外，还介绍了当前存在的问题和挑战，以确定未来有前景的研究和发展方向。

    High-dimensional datasets depict a challenge for learning tasks in data mining and machine learning. Feature selection is an effective technique in dealing with dimensionality reduction. It is often an essential data processing step prior to applying a learning algorithm. Over the decades, filter feature selection methods have evolved from simple univariate relevance ranking algorithms to more sophisticated relevance-redundancy trade-offs and to multivariate dependencies-based approaches in recent years. This tendency to capture multivariate dependence aims at obtaining unique information about the class from the intercooperation among features. This paper presents a comprehensive survey of the state-of-the-art work on filter feature selection methods assisted by feature intercooperation, and summarizes the contributions of different approaches found in the literature. Furthermore, current issues and challenges are introduced to identify promising future research and development.
    
[^79]: 用于具有理论保证的机器学习的非凸优化：稳健矩阵补全和神经网络学习

    Non-Convex Optimizations for Machine Learning with Theoretical Guarantee: Robust Matrix Completion and Neural Network Learning. (arXiv:2306.16557v1 [cs.LG])

    [http://arxiv.org/abs/2306.16557](http://arxiv.org/abs/2306.16557)

    本论文研究了稳健矩阵补全和神经网络学习两个流行的非凸优化问题，以提供可解释的算法解决机器学习中的黑盒问题。

    

    尽管机器学习最近取得了进展，大多数学习系统仍然是"黑盒"的概念，其性能无法理解和推导。随着公共安全和隐私问题的日益引起关注，设计一个可解释的学习系统成为机器学习的新趋势。一般来说，许多机器学习问题被表述为最小化（或最大化）某个损失函数。由于真实数据很可能来自非线性模型，损失函数一般是非凸的。与凸优化问题不同，梯度下降算法在解决非凸优化时会陷入假局部最小值。因此，在研究非凸优化问题时，提供可解释的算法是具有挑战性的。本文研究了两个流行的非凸问题：（1）低秩矩阵补全和（2）神经网络学习。

    Despite the recent development in machine learning, most learning systems are still under the concept of "black box", where the performance cannot be understood and derived. With the rise of safety and privacy concerns in public, designing an explainable learning system has become a new trend in machine learning. In general, many machine learning problems are formulated as minimizing (or maximizing) some loss function. Since real data are most likely generated from non-linear models, the loss function is non-convex in general. Unlike the convex optimization problem, gradient descent algorithms will be trapped in spurious local minima in solving non-convex optimization. Therefore, it is challenging to provide explainable algorithms when studying non-convex optimization problems. In this thesis, two popular non-convex problems are studied: (1) low-rank matrix completion and (2) neural network learning.
    
[^80]: 通过最小-最大F-散度正则化学习公平分类器

    Learning Fair Classifiers via Min-Max F-divergence Regularization. (arXiv:2306.16552v1 [cs.LG])

    [http://arxiv.org/abs/2306.16552](http://arxiv.org/abs/2306.16552)

    本文提出了一种通过最小-最大F-散度正则化学习公平分类器的框架，该框架通过使用F-散度衡量公平性，并保持高准确性。该框架可以适用于多个敏感属性和高维数据集。

    

    随着机器学习（ML）系统在执法、刑事司法、金融、招聘和录取等领域得到应用，确保ML辅助决策的公平性变得越来越重要。本文关注公平分类问题，提出一种新颖的最小-最大F-散度正则化框架，用于学习公平分类模型并保持高准确性。我们的框架由两个可训练的网络组成，即分类器网络和偏差/公平性估计器网络，其中公平性使用统计概念的F-散度进行衡量。我们展示了F-散度度量具有凸性和可微性的特性，并且它们的变分表示使它们在实际的基于梯度的训练方法中具有广泛的适用性。所提出的框架可以方便地适应多个敏感属性和高维数据集。我们研究了基于F-散度的训练范式在两种类型的问题上的性能。

    As machine learning (ML) based systems are adopted in domains such as law enforcement, criminal justice, finance, hiring and admissions, ensuring the fairness of ML aided decision-making is becoming increasingly important. In this paper, we focus on the problem of fair classification, and introduce a novel min-max F-divergence regularization framework for learning fair classification models while preserving high accuracy. Our framework consists of two trainable networks, namely, a classifier network and a bias/fairness estimator network, where the fairness is measured using the statistical notion of F-divergence. We show that F-divergence measures possess convexity and differentiability properties, and their variational representation make them widely applicable in practical gradient based training methods. The proposed framework can be readily adapted to multiple sensitive attributes and for high dimensional datasets. We study the F-divergence based training paradigm for two types of 
    
[^81]: 高效真实人体渲染的下一代增强现实会议系统设计

    Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering. (arXiv:2306.16541v1 [cs.CV])

    [http://arxiv.org/abs/2306.16541](http://arxiv.org/abs/2306.16541)

    该论文介绍了一个下一代增强现实会议系统的设计，其中提出了一种高效真实人体渲染的方法。目前的应用在实时会议方面表现良好，但在传递真实人体动态方面存在不足。该研究借鉴了神经渲染技术，通过采用单目视频采集和自由视点合成来提高会议系统的效率，并提供更高的真实感和交互性。

    

    在线会议正在成为新的常态，为在线会议创造身临其境的体验已经成为刻不容缓的需求。高效真实人体渲染是实现身临其境会议的核心。目前流行的应用虽然能够实现实时会议，但在传递真实人体动态方面存在不足，要么是因为空间有限，要么是因为使用缺乏真实互动的化身。最近神经渲染技术的进步，比如神经辐射场（NeRF），为元宇宙会议提供了更高的真实感。然而，NeRF 的渲染速度较慢，对于实时会议带来了挑战。我们设想了一个未来的增强现实元宇宙会议系统的流程，它利用单目视频采集和自由视点合成来提高数据和硬件的效率。为了实现身临其境的会议体验，我们探索了一个加速的基于 NeRF 的自由视点渲染 pipeline，通过在渲染过程中的深度学习模型提供更高的真实感和交互性。

    Meeting online is becoming the new normal. Creating an immersive experience for online meetings is a necessity towards more diverse and seamless environments. Efficient photorealistic rendering of human 3D dynamics is the core of immersive meetings. Current popular applications achieve real-time conferencing but fall short in delivering photorealistic human dynamics, either due to limited 2D space or the use of avatars that lack realistic interactions between participants. Recent advances in neural rendering, such as the Neural Radiance Field (NeRF), offer the potential for greater realism in metaverse meetings. However, the slow rendering speed of NeRF poses challenges for real-time conferencing. We envision a pipeline for a future extended reality metaverse conferencing system that leverages monocular video acquisition and free-viewpoint synthesis to enhance data and hardware efficiency. Towards an immersive conferencing experience, we explore an accelerated NeRF-based free-viewpoint
    
[^82]: 利用预放射MRI预测WHO 4级胶质瘤患者的快速早期进展和生存风险

    Prediction of Rapid Early Progression and Survival Risk with Pre-Radiation MRI in WHO Grade 4 Glioma Patients. (arXiv:2306.16531v1 [cs.LG])

    [http://arxiv.org/abs/2306.16531](http://arxiv.org/abs/2306.16531)

    本研究是首次利用计算和统计建模方法，通过分析胶质瘤患者的MRI序列和分子特征，预测WHO 4级胶质瘤患者的快速早期进展，并确定了与生存概率相关的特征。

    

    最近的临床研究描述了一部分在放射治疗开始前表现出快速早期进展（REP）的脑胶质瘤患者。目前的文献迄今为止仅描述了这种人群的临床病理特征。据我们所知，本研究是首次利用传统放射解剖学、复杂多分辨率分形纹理特征和不同分子特征（MGMT、IDH突变）作为诊断和预后工具，使用计算和统计建模方法预测来自非REP病例的REP患者的潜力。分析了70名患者的放射规划T1增强（T1C）MRI序列。经过1000次迭代的5折交叉验证，集成方法提供了0.793的AUC，标准偏差为0.082，用于REP和非REP分类。此外，在依赖性截标的基础上，基于Copula模型的建模（其中一部分患者可能在死亡前未跟踪）确定了生存概率的显著特征（p值<0.05）。

    Recent clinical research describes a subset of glioblastoma patients that exhibit REP prior to start of radiation therapy. Current literature has thus far described this population using clinicopathologic features. To our knowledge, this study is the first to investigate the potential of conventional ra-diomics, sophisticated multi-resolution fractal texture features, and different molecular features (MGMT, IDH mutations) as a diagnostic and prognostic tool for prediction of REP from non-REP cases using computational and statistical modeling methods. Radiation-planning T1 post-contrast (T1C) MRI sequences of 70 patients are analyzed. Ensemble method with 5-fold cross validation over 1000 iterations offers AUC of 0.793 with standard deviation of 0.082 for REP and non-REP classification. In addition, copula-based modeling under dependent censoring (where a subset of the patients may not be followed up until death) identifies significant features (p-value <0.05) for survival probability a
    
[^83]: 基于机器学习模型的学术环境中的食物推荐系统

    A Food Recommender System in Academic Environments Based on Machine Learning Models. (arXiv:2306.16528v1 [cs.IR])

    [http://arxiv.org/abs/2306.16528](http://arxiv.org/abs/2306.16528)

    这个论文介绍了一个基于机器学习模型的食物推荐系统，通过使用协同过滤、基于内容和基于知识的模型，以提高人们的健康。研究探讨了决策树、k最近邻居(kNN)、AdaBoost和Bagging等机器学习模型在食物推荐系统上的应用。

    

    背景：人们的健康取决于适当的饮食，是一个重要因素。如今，随着人们生活的机械化增加，适当的饮食习惯和行为被忽视了。另一方面，健康领域中的食物推荐也试图解决这个问题。但是随着西方饮食风格的引入和西方化学药物的进步，在疾病治疗和营养方面出现了许多问题。技术的最新进展和人工智能方法在信息系统中的应用，导致了推荐系统的创建，以改善人们的健康。方法：采用混合推荐系统，包括协同过滤、基于内容和基于知识的模型。在2519名学生的营养管理系统中，研究了决策树、k最近邻居(kNN)、AdaBoost和Bagging等机器学习模型在食物推荐系统领域的应用。

    Background: People's health depends on the use of proper diet as an important factor. Today, with the increasing mechanization of people's lives, proper eating habits and behaviors are neglected. On the other hand, food recommendations in the field of health have also tried to deal with this issue. But with the introduction of the Western nutrition style and the advancement of Western chemical medicine, many issues have emerged in the field of disease treatment and nutrition. Recent advances in technology and the use of artificial intelligence methods in information systems have led to the creation of recommender systems in order to improve people's health. Methods: A hybrid recommender system including, collaborative filtering, content-based, and knowledge-based models was used. Machine learning models such as Decision Tree, k-Nearest Neighbors (kNN), AdaBoost, and Bagging were investigated in the field of food recommender systems on 2519 students in the nutrition management system of
    
[^84]: 通过生成虚假评论对基于评论的推荐系统进行操纵的黑盒检测

    Shilling Black-box Review-based Recommender Systems through Fake Review Generation. (arXiv:2306.16526v1 [cs.IR])

    [http://arxiv.org/abs/2306.16526](http://arxiv.org/abs/2306.16526)

    本文提出了一种基于生成的模型，通过虚假评论生成器对基于评论的推荐系统进行操纵攻击。实验证明该框架可以成功地攻击亚马逊上的三种不同类型的RBRS。

    

    基于评论的推荐系统（RBRS）由于能够缓解众所周知的冷启动问题而受到越来越多的研究关注。RBRS利用评论来构建用户和物品的表示。然而，在本文中，我们认为这种对评论的依赖可能会使系统面临被操纵的风险。为了探索这种可能性，在本文中，我们提出了一种用于对RBRS进行操纵攻击的基于生成的模型。具体而言，我们通过强化学习来学习一个虚假评论生成器，它通过向系统添加生成的评论导致预测偏移从而恶意推广物品。通过引入辅助奖励，借助预训练的语言模型和方面预测器来增加文本的流畅性和多样性，生成的评论可以有效地用于高保真度的操纵。实验结果表明，所提出的框架可以成功地攻击亚马逊上的三种不同类型的RBRS。

    Review-Based Recommender Systems (RBRS) have attracted increasing research interest due to their ability to alleviate well-known cold-start problems. RBRS utilizes reviews to construct the user and items representations. However, in this paper, we argue that such a reliance on reviews may instead expose systems to the risk of being shilled. To explore this possibility, in this paper, we propose the first generation-based model for shilling attacks against RBRSs. Specifically, we learn a fake review generator through reinforcement learning, which maliciously promotes items by forcing prediction shifts after adding generated reviews to the system. By introducing the auxiliary rewards to increase text fluency and diversity with the aid of pre-trained language models and aspect predictors, the generated reviews can be effective for shilling with high fidelity. Experimental results demonstrate that the proposed framework can successfully attack three different kinds of RBRSs on the Amazon c
    
[^85]: HNO：用于解决PDE的鬣狗神经算子

    HNO: Hyena Neural Operator for solving PDEs. (arXiv:2306.16524v1 [cs.LG])

    [http://arxiv.org/abs/2306.16524](http://arxiv.org/abs/2306.16524)

    本研究使用了一种名为鬣狗的新型神经算子，它利用多层感知器参数化的长卷积滤波器来解决PDE问题。这种方法通过增强模型对输入上下文的理解，并为不同的PDE实例提供数据依赖权重，提供了一种有效的求解PDE的方式。

    

    数值求解偏微分方程（PDE）通常需要精细离散化以解析必要的时空尺度，这可能会耗费大量计算资源。深度学习的最新进展提供了一种新方法来解决PDE，该方法涉及使用神经算子。神经算子是一种神经网络架构，可以学习函数空间之间的映射，并能够基于数据解决偏微分方程。本研究利用了一种称为鬣狗（Hyena）的新型神经算子，该算子采用由多层感知器参数化的长卷积滤波器。鬣狗算子是一种具有次线性复杂性的操作，它使用状态空间模型来参数化具有全局感受野的长卷积。这种机制增强了模型对输入上下文的理解，并能够为不同的PDE实例提供数据依赖权重。为了衡量各个层在解决PDE中的有效性，我们进行实验评估。

    Numerically solving partial differential equations (PDEs) typically requires fine discretization to resolve necessary spatiotemporal scales, which can be computationally expensive. Recent advances in deep learning have provided a new approach to solving PDEs that involves the use of neural operators. Neural operators are neural network architectures that learn mappings between function spaces and have the capability to solve partial differential equations based on data. This study utilizes a novel neural operator called Hyena, which employs a long convolutional filter that is parameterized by a multilayer perceptron. The Hyena operator is an operation that enjoys sub-quadratic complexity and state space model to parameterize long convolution that enjoys global receptive field. This mechanism enhances the model's comprehension of the input's context and enables data-dependent weight for different PDE instances. To measure how effective the layers are in solving PDEs, we conduct experime
    
[^86]: 对于核范围空间，只需要固定数量的查询就足够了

    For Kernel Range Spaces a Constant Number of Queries Are Sufficient. (arXiv:2306.16516v1 [cs.CG])

    [http://arxiv.org/abs/2306.16516](http://arxiv.org/abs/2306.16516)

    对于核范围空间，引入了ε-覆盖的概念，用于处理不确定或不精确的数据分析任务。

    

    我们引入了核范围空间的ε-覆盖概念。核范围空间涉及一个点集X⊂R^d和由固定核函数（例如高斯核函数K(p,·)=exp(-||p-·||^2)）定义的查询空间。对于大小为n的点集X，查询返回一个值向量Rp∈R^n，其中第i个坐标(Rp)_i=K(p,x_i)，其中x_i∈X。ε-覆盖是点集Q⊂R^d的子集，对于任意p∈R^d，存在q∈Q使得||(Rp-Rq)/n||_1≤ε。这是Haussler在组合范围空间（例如由球查询定义的点集子集）中ε-覆盖概念的平滑模拟，其中得到的向量Rp是{0,1}^n而不是[0,1]^n。这些范围空间的核版本出现在数据分析任务中，其中坐标可能是不确定或不精确的，因此希望在范围查询中添加一些灵活性。

    We introduce the notion of an $\varepsilon$-cover for a kernel range space. A kernel range space concerns a set of points $X \subset \mathbb{R}^d$ and the space of all queries by a fixed kernel (e.g., a Gaussian kernel $K(p,\cdot) = \exp(-\|p-\cdot\|^2)$). For a point set $X$ of size $n$, a query returns a vector of values $R_p \in \mathbb{R}^n$, where the $i$th coordinate $(R_p)_i = K(p,x_i)$ for $x_i \in X$. An $\varepsilon$-cover is a subset of points $Q \subset \mathbb{R}^d$ so for any $p \in \mathbb{R}^d$ that $\frac{1}{n} \|R_p R_q\|_1\leq \varepsilon$ for some $q \in Q$. This is a smooth analog of Haussler's notion of $\varepsilon$-covers for combinatorial range spaces (e.g., defined by subsets of points within a ball query) where the resulting vectors $R_p$ are in $\{0,1\}^n$ instead of $[0,1]^n$. The kernel versions of these range spaces show up in data analysis tasks where the coordinates may be uncertain or imprecise, and hence one wishes to add some flexibility in the not
    
[^87]: 动量简单而可证实地增强非独立同分布联邦学习的效果

    Momentum Benefits Non-IID Federated Learning Simply and Provably. (arXiv:2306.16504v1 [cs.LG])

    [http://arxiv.org/abs/2306.16504](http://arxiv.org/abs/2306.16504)

    本论文研究了在非独立同分布联邦学习中利用动量来提升FedAvg和SCAFFOLD算法的性能，证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。

    

    联邦学习是一种用于大规模机器学习的强大范例，但由于不可靠的网络连接、缓慢的通信以及客户端之间存在的数据异质性，它面临着重大挑战。FedAvg和SCAFFOLD是两种解决这些挑战的基本算法。特别地，FedAvg在与中央服务器进行通信之前采用多个本地更新，而SCAFFOLD在其本地更新中维护每个客户端上的控制变量以补偿“客户端漂移”。文献中提出了各种方法来增强这两种算法的收敛性，但它们要么对算法结构进行不切实际的调整，要么依赖于有界数据异质性的假设。本文探讨了利用动量来增强FedAvg和SCAFFOLD性能的方法。当所有客户端参与训练过程时，我们证明了引入动量可以使FedAvg在不依赖于数据异质性的假设下收敛。

    Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow communication, and substantial data heterogeneity across clients. FedAvg and SCAFFOLD are two fundamental algorithms to address these challenges. In particular, FedAvg employs multiple local updates before communicating with a central server, while SCAFFOLD maintains a control variable on each client to compensate for "client drift" in its local updates. Various methods have been proposed in literature to enhance the convergence of these two algorithms, but they either make impractical adjustments to algorithmic structure, or rely on the assumption of bounded data heterogeneity.  This paper explores the utilization of momentum to enhance the performance of FedAvg and SCAFFOLD. When all clients participate in the training process, we demonstrate that incorporating momentum allows FedAvg to converge without relying on the assumption o
    
[^88]: SARC: 软参演者回顾评论者

    SARC: Soft Actor Retrospective Critic. (arXiv:2306.16503v1 [cs.LG])

    [http://arxiv.org/abs/2306.16503](http://arxiv.org/abs/2306.16503)

    SARC是一个基于SAC算法的新方法，通过改进评论者实现更好的收敛性和梯度估计，为演员提供了更好的策略梯度估计。

    

    SAC是一个演员评论者算法，其两个时间尺度的特性在于评论者估计在任何给定时间都没有收敛于演员，但由于评论者学习速度比演员快，它确保了两者之间的最终一致性。文献中引入了各种策略来学习更好的梯度估计，以帮助实现更好的收敛性。由于梯度估计依赖于评论者，我们认为改进评论者可以为每个时间点的演员提供更好的梯度估计。基于此，我们提出了软参演者回顾评论者(SARC)，其中我们将SAC评论者损失与另一个损失项回顾损失相结合 - 实现了更快的评论者收敛和更好的演员策略梯度估计。现有的SAC实现可以很容易地适应SARC，只需进行微小的修改。通过大量的实验和分析，我们展示了SARC相比S的一致改进。

    The two-time scale nature of SAC, which is an actor-critic algorithm, is characterised by the fact that the critic estimate has not converged for the actor at any given time, but since the critic learns faster than the actor, it ensures eventual consistency between the two. Various strategies have been introduced in literature to learn better gradient estimates to help achieve better convergence. Since gradient estimates depend upon the critic, we posit that improving the critic can provide a better gradient estimate for the actor at each time. Utilizing this, we propose Soft Actor Retrospective Critic (SARC), where we augment the SAC critic loss with another loss term retrospective loss - leading to faster critic convergence and consequently, better policy gradient estimates for the actor. An existing implementation of SAC can be easily adapted to SARC with minimal modifications. Through extensive experimentation and analysis, we show that SARC provides consistent improvement over S
    
[^89]: 变分不等式中的随机方法：遍历性、偏差与改进

    Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements. (arXiv:2306.16502v1 [stat.ML])

    [http://arxiv.org/abs/2306.16502](http://arxiv.org/abs/2306.16502)

    本论文研究了变分不等式中的随机方法，并通过建立大数定律和中心极限定理揭示了这些算法的收敛性质，对于广泛的VIP问题，平均迭代收敛到一个唯一的不变分布。

    

    对于在各种机器学习任务中遇到的min-max优化和变分不等式问题(VIP)，随机外推梯度(SEG)和随机梯度上升下降(SGDA)算法已成为杰出的算法。SEG/SGDA的恒定步长变种广受欢迎，具有易于调节和原始条件迅速适应的优点，但即使在基本的双线性模型中，它们的收敛行为也更加复杂。我们的工作旨在阐明和量化这些算法内在的概率结构。通过将恒定步长SEG/SGDA重新构造为时间齐次马尔可夫链，我们建立了首个大数定律和中心极限定理，证明了在广泛的单调和非单调VIP情况下，平均迭代收敛到一个唯一的不变分布。特别是对于凸凹min-max优化，我们刻画了连接VIP和优化偏差的关系。

    For min-max optimization and variational inequalities problems (VIP) encountered in diverse machine learning tasks, Stochastic Extragradient (SEG) and Stochastic Gradient Descent Ascent (SGDA) have emerged as preeminent algorithms. Constant step-size variants of SEG/SGDA have gained popularity, with appealing benefits such as easy tuning and rapid forgiveness of initial conditions, but their convergence behaviors are more complicated even in rudimentary bilinear models. Our work endeavors to elucidate and quantify the probabilistic structures intrinsic to these algorithms. By recasting the constant step-size SEG/SGDA as time-homogeneous Markov Chains, we establish a first-of-its-kind Law of Large Numbers and a Central Limit Theorem, demonstrating that the average iterate is asymptotically normal with a unique invariant distribution for an extensive range of monotone and non-monotone VIPs. Specializing to convex-concave min-max optimization, we characterize the relationship between the 
    
[^90]: 朝着对独立子网络训练的更好理论理解迈进 (arXiv:2306.16484v1 [cs.LG])

    Towards a Better Theoretical Understanding of Independent Subnetwork Training. (arXiv:2306.16484v1 [cs.LG])

    [http://arxiv.org/abs/2306.16484](http://arxiv.org/abs/2306.16484)

    本研究对独立子网络训练（IST）进行了理论分析，发现了IST与其他模型并行方法之间的根本差异。

    

    现代大规模机器学习的进展离不开数据并行分布式计算的范式。由于大规模模型的分布式计算对通信通道施加了巨大压力，最近的研究主要集中在共同设计通信压缩策略和训练算法，以降低通信成本。尽管纯数据并行性允许更好的数据扩展性，但其在模型扩展性方面存在问题。事实上，计算节点受内存限制严重限制，阻止了模型尺寸的进一步增加。因此，训练巨型神经网络模型的最新成果也依赖于某种形式的模型并行性。在这项工作中，我们对独立子网络训练（IST）进行了更详细的理论研究，这是一种最近提出的高效解决上述问题的技术。我们发现IST和其他模型并行方法之间存在根本性的差异。

    Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, significant recent research has been directed toward co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models also rely on some form of model parallelism. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alter
    
[^91]: 增强模型普适的交互式特征归因提高性能和样本效率

    Increasing Performance And Sample Efficiency With Model-agnostic Interactive Feature Attributions. (arXiv:2306.16431v1 [cs.LG])

    [http://arxiv.org/abs/2306.16431](http://arxiv.org/abs/2306.16431)

    本文提出了一种增强模型普适的交互式特征归因方法，通过纠正特征归因并重新训练模型，显著提高了模型的性能和样本效率。

    

    模型普适的特征归因可以为复杂的机器学习模型提供局部洞察力。如果解释是正确的，领域专家可以验证和信任模型的决策。然而，如果它与专家的知识相矛盾，相关工作只纠正了无关的特征以改进模型。为了允许无限的交互，本文针对两种流行的解释方法（遮蔽法和沙普利值）提供了模型普适的实现，以在复杂模型中强制执行完全不同的归因。对于特定的样本集，我们使用纠正的特征归因来生成额外的局部数据，用于重新训练模型以对样本进行正确解释。通过在各种模型上进行模拟和真实数据实验，我们展示了我们提出的方法如何通过基于纠正的解释来扩充训练数据集，从而显著提高模型的性能。将我们的交互式解释添加到主动学习设置中可以增加样本效率。

    Model-agnostic feature attributions can provide local insights in complex ML models. If the explanation is correct, a domain expert can validate and trust the model's decision. However, if it contradicts the expert's knowledge, related work only corrects irrelevant features to improve the model. To allow for unlimited interaction, in this paper we provide model-agnostic implementations for two popular explanation methods (Occlusion and Shapley values) to enforce entirely different attributions in the complex model. For a particular set of samples, we use the corrected feature attributions to generate extra local data, which is used to retrain the model to have the right explanation for the samples. Through simulated and real data experiments on a variety of models we show how our proposed approach can significantly improve the model's performance only by augmenting its training dataset based on corrected explanations. Adding our interactive explanations to active learning settings incr
    
[^92]: DNA-TEQ：一种用于DNN推理的自适应指数量化张量的方法

    DNA-TEQ: An Adaptive Exponential Quantization of Tensors for DNN Inference. (arXiv:2306.16430v1 [cs.LG])

    [http://arxiv.org/abs/2306.16430](http://arxiv.org/abs/2306.16430)

    本文介绍了一种在DNN推理中使用的自适应指数量化张量方法DNA-TEQ，该方法通过发现大量张量符合指数分布来实现最佳的数值精度和准确性损失平衡。

    

    量化是深度神经网络（DNN）中常用的技术，通过降低激活和权重（即张量）的算术精度来减少存储和计算复杂度。高效的硬件架构采用线性量化，以便将最新的DNN部署到嵌入式系统和移动设备上。然而，线性均匀量化通常无法将数值精度降低到小于8位而不牺牲模型准确性。这是因为张量并不服从均匀分布。本文中，我们展示了大量张量符合指数分布。然后，我们提出了DNA-TEQ，通过自适应方案指数量化DNN张量，以实现数值精度和准确性损失之间的最佳平衡。实验结果表明，与先前的方案相比，DNA-TEQ提供了更低的量化位宽，从而提高了性能。

    Quantization is commonly used in Deep Neural Networks (DNNs) to reduce the storage and computational complexity by decreasing the arithmetical precision of activations and weights, a.k.a. tensors. Efficient hardware architectures employ linear quantization to enable the deployment of recent DNNs onto embedded systems and mobile devices. However, linear uniform quantization cannot usually reduce the numerical precision to less than 8 bits without sacrificing high performance in terms of model accuracy. The performance loss is due to the fact that tensors do not follow uniform distributions. In this paper, we show that a significant amount of tensors fit into an exponential distribution. Then, we propose DNA-TEQ to exponentially quantize DNN tensors with an adaptive scheme that achieves the best trade-off between numerical precision and accuracy loss. The experimental results show that DNA-TEQ provides a much lower quantization bit-width compared to previous proposals, resulting in an av
    
[^93]: 复数自适应系统识别通过低秩张量分解

    Complex-valued Adaptive System Identification via Low-Rank Tensor Decomposition. (arXiv:2306.16428v1 [cs.LG])

    [http://arxiv.org/abs/2306.16428](http://arxiv.org/abs/2306.16428)

    本论文提出了两种新的架构来处理复数信号，超越了原始架构的复数扩展，在性能上表现优越，只需稍微增加计算资源即可实现复数运算。

    

    机器学习（ML）和基于张量的方法在过去几十年中一直受到科学界的重大关注。在之前的工作中，我们提出了一个新颖的基于张量的系统识别框架，以减轻仅使用张量的架构的计算负担，同时仍能实现出色的性能。然而，该方法只允许处理实数问题，因此不能直接应用于处理复数系统的广泛范围的信号处理和通信问题。因此，在这项工作中，我们推导出两种新的架构，以允许处理复数信号，并且证明这些扩展能够在性能方面超越原始架构的平凡的复数扩展，同时只需要稍微增加计算资源来允许复数运算。

    Machine learning (ML) and tensor-based methods have been of significant interest for the scientific community for the last few decades. In a previous work we presented a novel tensor-based system identification framework to ease the computational burden of tensor-only architectures while still being able to achieve exceptionally good performance. However, the derived approach only allows to process real-valued problems and is therefore not directly applicable on a wide range of signal processing and communications problems, which often deal with complex-valued systems. In this work we therefore derive two new architectures to allow the processing of complex-valued signals, and show that these extensions are able to surpass the trivial, complex-valued extension of the original architecture in terms of performance, while only requiring a slight overhead in computational resources to allow for complex-valued operations.
    
[^94]: 长期关联风力和太阳能发电的逐时场景生成：结合变分自动编码器和径向基函数核的方法

    Long-Term Hourly Scenario Generation for Correlated Wind and Solar Power combining Variational Autoencoders with Radial Basis Function Kernels. (arXiv:2306.16427v1 [cs.LG])

    [http://arxiv.org/abs/2306.16427](http://arxiv.org/abs/2306.16427)

    本文提出了一种创新的方法，结合变分自动编码器和径向基函数核，用于生成长期逐时的风力和太阳能发电场景，并考虑了这两种能源来源之间的相关性。

    

    准确生成可持续能源发电的未来场景对于电力系统的长期规划和运营至关重要，尤其是考虑到可持续能源的日益关注和在能源矩阵中的不断增加的渗透。这些预测能够帮助电力系统运营商和能源规划师有效地管理可再生能源发电所带来的变异性和间歇性，从而实现更好的电网稳定性、改善能源管理和加强决策过程。在本文中，我们提出了一种创新的方法，用于生成风力和太阳能发电的长期逐时场景，同时考虑这两种能源来源之间的相关性。为了实现这一目标，我们将变分自动编码器（VAE）的能力与在我们的人工神经网络结构中纳入径向基函数（RBF）核的额外优势相结合。

    Accurate generation of realistic future scenarios of renewable energy generation is crucial for long-term planning and operation of electrical systems, especially considering the increasing focus on sustainable energy and the growing penetration of renewable generation in energy matrices. These predictions enable power system operators and energy planners to effectively manage the variability and intermittency associated with renewable generation, allowing for better grid stability, improved energy management, and enhanced decision-making processes. In this paper, we propose an innovative method for generating long-term hourly scenarios for wind and solar power generation, taking into consideration the correlation between these two energy sources. To achieve this, we combine the capabilities of a Variational Autoencoder (VAE) with the additional benefits of incorporating the Radial Basis Function (RBF) kernel in our artificial neural network architecture. By incorporating them, we aim 
    
[^95]: 跨领域推荐的协作迁移学习框架

    A Collaborative Transfer Learning Framework for Cross-domain Recommendation. (arXiv:2306.16425v1 [cs.IR])

    [http://arxiv.org/abs/2306.16425](http://arxiv.org/abs/2306.16425)

    这篇论文提出了一种协作跨领域迁移学习框架（CCTL），用于解决推荐系统中不同领域CTR预测建模的挑战。

    

    在推荐系统中，有多个不同的业务领域来满足用户的多样化兴趣和需求，不同领域的点击率（CTR）可能会有很大差异，这就需要对不同业务领域进行CTR预测建模。行业解决方案是对每个领域使用特定的模型或迁移学习技术。前者的缺点是单一领域模型没有利用其他领域的数据，而后者则利用不同领域的所有数据，但迁移学习的微调模型可能使模型陷入源领域的局部最优，难以适应目标领域。同时，不同领域之间存在数据数量和特征模式的显著差异，即领域偏移，在迁移过程中可能导致负面迁移。为了克服这些挑战，我们提出了协作跨领域迁移学习框架（CCTL）。

    In the recommendation systems, there are multiple business domains to meet the diverse interests and needs of users, and the click-through rate(CTR) of each domain can be quite different, which leads to the demand for CTR prediction modeling for different business domains. The industry solution is to use domain-specific models or transfer learning techniques for each domain. The disadvantage of the former is that the data from other domains is not utilized by a single domain model, while the latter leverage all the data from different domains, but the fine-tuned model of transfer learning may trap the model in a local optimum of the source domain, making it difficult to fit the target domain. Meanwhile, significant differences in data quantity and feature schemas between different domains, known as domain shift, may lead to negative transfer in the process of transferring. To overcome these challenges, we propose the Collaborative Cross-Domain Transfer Learning Framework (CCTL). CCTL e
    
[^96]: 逼真的合成金融交易用于反洗钱模型

    Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])

    [http://arxiv.org/abs/2306.16424](http://arxiv.org/abs/2306.16424)

    本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。

    

    随着金融的广泛数字化和加密货币的日益流行，网络犯罪分子设计的欺诈方案越来越复杂。洗钱——将非法资金移动以掩盖其来源——可以跨越银行和国界，产生复杂的交易模式。联合国估计每年全球洗钱金额占全球GDP的2-5%，约为0.8-2.0万亿美元。不幸的是，通常无法获得用于训练机器学习模型来检测洗钱的真实数据，且之前的合成数据生成器存在显著缺陷。为了比较模型并推进该领域的发展，需要一个逼真、标准化、公开可用的基准数据集。为此，本文提出了一种合成金融交易数据集生成器和一组合成的反洗钱数据集。我们根据实际交易尽可能地校准了这个基于代理的生成器。

    With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
    
[^97]: 神经网络可以检测无模型静态套利策略

    Neural networks can detect model-free static arbitrage strategies. (arXiv:2306.16422v1 [q-fin.CP])

    [http://arxiv.org/abs/2306.16422](http://arxiv.org/abs/2306.16422)

    本文证明了神经网络可以检测金融市场中的无模型静态套利机会，并可应用于交易证券数量较多的金融市场。我们的方法具有易处理性、有效性和稳健性，并使用真实金融数据进行了示例验证。

    

    本文利用理论和数值方法证明了神经网络可以在市场存在套利机会时检测出无模型静态套利机会。由于使用了神经网络，我们的方法可以应用于交易证券数量较多的金融市场，并确保相应交易策略的几乎即时执行。为了证明其易处理性、有效性和稳健性，我们提供了使用真实金融数据的示例。从技术角度来看，我们证明了单个神经网络可以近似解决一类凸半无限规划问题，这是推导出我们的理论结果的关键。

    In this paper we demonstrate both theoretically as well as numerically that neural networks can detect model-free static arbitrage opportunities whenever the market admits some. Due to the use of neural networks, our method can be applied to financial markets with a high number of traded securities and ensures almost immediate execution of the corresponding trading strategies. To demonstrate its tractability, effectiveness, and robustness we provide examples using real financial data. From a technical point of view, we prove that a single neural network can approximately solve a class of convex semi-infinite programs, which is the key result in order to derive our theoretical results that neural networks can detect model-free static arbitrage strategies whenever the financial market admits such opportunities.
    
[^98]: 安全高效的异步垂直联邦学习:基于级联混合优化方法

    Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization. (arXiv:2306.16077v1 [cs.LG])

    [http://arxiv.org/abs/2306.16077](http://arxiv.org/abs/2306.16077)

    本论文提出了一种在垂直联邦学习中使用级联混合优化的方法，通过在下游使用零阶优化保护隐私并在上游使用一阶优化提高收敛速度，从而解决了ZOO-based VFL收敛速度较慢的问题。

    

    垂直联邦学习(VFL)因能够在垂直分割的数据上联合训练隐私保护模型而引起越来越多的关注。最近的研究表明，应用零阶优化(ZOO)在构建实用的VFL算法方面具有许多优势。然而，基于ZOO的VFL存在一个关键问题，即其收敛速度较慢，限制了其在处理现代大型模型时的应用。为了解决这个问题，我们提出了一种在VFL中使用级联混合优化方法。该方法中，下游模型（客户端）使用ZOO进行训练以保护隐私并确保不共享内部信息。同时，上游模型（服务器）在本地使用一阶优化(FOO)进行更新，这显著提高了收敛速度，使得能够在不损害隐私和安全性的前提下训练大型模型。我们在理论上证明了我们的VFL框架比基于ZOO的VFL更快地收敛。

    Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the c
    
[^99]: 可解释机器学习中罗生门效应的实证评估

    An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning. (arXiv:2306.15786v1 [cs.LG])

    [http://arxiv.org/abs/2306.15786](http://arxiv.org/abs/2306.15786)

    通过对不同数据集、模型和指标进行定量评估，我们发现罗生门效应对可解释机器学习具有影响，这为之前的轶事证据提供了实证支持，并展示了科学家和实践者面临的挑战。

    

    罗生门效应描述了以下现象：对于给定的数据集，可能存在许多具有相同良好性能但采用不同解决策略的模型。罗生门效应对可解释机器学习具有影响，特别是对解释的可比性。我们对三种不同比较场景提供了统一视角，并在不同数据集、模型、归因方法和指标上进行了定量评估。我们发现超参数调整起到了一定作用，指标选择也很重要。我们的结果为先前的轶事证据提供了实证支持，并展示了科学家和实践者面临的挑战。

    The Rashomon Effect describes the following phenomenon: for a given dataset there may exist many models with equally good performance but with different solution strategies. The Rashomon Effect has implications for Explainable Machine Learning, especially for the comparability of explanations. We provide a unified view on three different comparison scenarios and conduct a quantitative evaluation across different datasets, models, attribution methods, and metrics. We find that hyperparameter-tuning plays a role and that metric selection matters. Our results provide empirical support for previously anecdotal evidence and exhibit challenges for both scientists and practitioners.
    
[^100]: 一个具有自我引导和块对角表示的重启大规模谱聚类方法

    A Restarted Large-Scale Spectral Clustering with Self-Guiding and Block Diagonal Representation. (arXiv:2306.15138v1 [cs.LG])

    [http://arxiv.org/abs/2306.15138](http://arxiv.org/abs/2306.15138)

    本论文提出了一个具有自我引导和块对角表示的重启聚类方法，该方法在谱聚类中首次应用重启策略，并且通过在每个周期中重新分类样本来获得更好的聚类效果。

    

    谱聚类是最流行的无监督机器学习方法之一。构建相似性矩阵对于这类方法至关重要。在大多数现有方法中，相似性矩阵只计算一次或者是交替更新。然而，前者很难反映数据点之间的全面关系，而后者耗时且在大规模问题中甚至难以实施。在本文中，我们提出了一个具有自我引导和块对角表示的重启聚类框架。该策略的优势在于尽可能保留从先前周期中获得的有用聚类信息。据我们所知，这是第一个将重启策略应用于谱聚类的工作。关键区别在于我们在方法的每个周期中重新对样本进行分类，而现有方法只进行一次分类。为了进一步减少开销，我们引入了一个块对角表示方法。

    Spectral clustering is one of the most popular unsupervised machine learning methods. Constructing similarity matrix is crucial to this type of method. In most existing works, the similarity matrix is computed once for all or is updated alternatively. However, the former is difficult to reflect comprehensive relationships among data points, and the latter is time-consuming and is even infeasible for large-scale problems. In this work, we propose a restarted clustering framework with self-guiding and block diagonal representation. An advantage of the strategy is that some useful clustering information obtained from previous cycles could be preserved as much as possible. To the best of our knowledge, this is the first work that applies restarting strategy to spectral clustering. The key difference is that we reclassify the samples in each cycle of our method, while they are classified only once in existing methods. To further release the overhead, we introduce a block diagonal representa
    
[^101]: 非凸随机 Bregman 近端梯度法及其在深度学习中的应用

    Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning. (arXiv:2306.14522v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.14522](http://arxiv.org/abs/2306.14522)

    该论文提出了一种非凸随机Bregman近端梯度法（SBPG），它通过使用Bregman近似测度替代了随机梯度法中的上二次逼近，并在捕捉非Lipschitz梯度的非凸目标函数方面得到更好的近似模型。论文证明了SBPG的收敛性质，并提出了一种基于动量的改进版本，称为MSBPG，并证明了它具有更好的收敛性质。

    

    广泛使用的随机梯度方法用于最小化非凸复合目标函数时需要可微部分的Lipschitz平滑性, 但这一要求对于包括二次逆问题和训练神经网络在内的问题类别并不成立。为了解决这个问题，我们研究了一族随机 Bregman 近端梯度 (SBPG) 方法，这些方法只需要可微部分的平滑适应性。

    The widely used stochastic gradient methods for minimizing nonconvex composite objective functions require the Lipschitz smoothness of the differentiable part. But the requirement does not hold true for problem classes including quadratic inverse problems and training neural networks. To address this issue, we investigate a family of stochastic Bregman proximal gradient (SBPG) methods, which only require smooth adaptivity of the differentiable part. SBPG replaces the upper quadratic approximation used in SGD with the Bregman proximity measure, resulting in a better approximation model that captures the non-Lipschitz gradients of the nonconvex objective. We formulate the vanilla SBPG and establish its convergence properties under nonconvex setting without finite-sum structure. Experimental results on quadratic inverse problems testify the robustness of SBPG. Moreover, we propose a momentum-based version of SBPG (MSBPG) and prove it has improved convergence properties. We apply MSBPG to 
    
[^102]: 通过重新加权优化轨迹增强对抗训练

    Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14275](http://arxiv.org/abs/2306.14275)

    本文提出了一种名为“加权优化轨迹（WOT）”的新方法，通过优化历史轨迹，解决了对抗训练中的鲁棒泛化问题。

    

    尽管对抗训练已成为提高深度神经网络鲁棒性的事实上的方法，但众所周知，简单的对抗训练遭受了令人畏缩的鲁棒过拟合问题，导致鲁棒泛化效果不佳。近年来已经提出了一些方法来解决这些缺点，如额外的规范化、对抗权重扰动和更多数据训练。然而，鲁棒泛化的改进仍然远不理想。在本文中，我们从全新的角度解决这一挑战--优化历史轨迹的精细化。我们提出了一种名为“加权优化轨迹（WOT）”的新方法，利用对抗训练的优化轨迹在时间上的特点。我们进行了大量实验证明了WOT在各种最新对抗攻击下的有效性。结果显示，WOT与现有方法完美融合。

    Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
    
[^103]: 自带数据！大型语言模型的自我监督评估

    Bring Your Own Data! Self-Supervised Evaluation for Large Language Models. (arXiv:2306.13651v1 [cs.CL])

    [http://arxiv.org/abs/2306.13651](http://arxiv.org/abs/2306.13651)

    本研究提出了一种自我监督评估框架，通过分析输入文本上的变换对LLMs的灵敏度或不变性，直接监控LLMs在实际数据上的行为。

    

    随着大型语言模型（LLMs）的兴起以及它们在各种领域的普及，衡量语言模型在实际数据上的行为变得不可或缺。为了解决这个问题，本研究提出了一种自我监督评估框架，通过分析输入文本上的变换对LLMs的灵敏度或不变性，直接监控LLM在野外收集的数据集或在模型部署期间进行的流数据的行为，实现了评估LLMs的有效和可扩展的解决方案。

    With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, tox
    
[^104]: 一种减少联邦学习通信的高效虚拟数据生成方法

    An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])

    [http://arxiv.org/abs/2306.12088](http://arxiv.org/abs/2306.12088)

    本论文提出FedINIBoost，一种新的联邦学习方法，通过梯度匹配构建有效的提取模块，在减少通信开销的同时提高了模型准确率。

    

    通信开销是联邦学习中的主要挑战之一。一些经典的方案假设服务器可以从本地模型中提取参与者训练数据的辅助信息来构建中央虚拟数据集。服务器使用虚拟数据集来微调聚合的全局模型，以在较少的通信轮次内达到目标测试精度。本文将上述解决方案概括为基于数据的通信高效联邦学习框架。提出框架的关键是设计一个有效的提取模块（EM），它确保虚拟数据集对微调聚合的全局模型产生积极影响。与现有方法使用生成器来设计EM不同，我们提出的方法FedINIBoost借鉴了梯度匹配的思想来构建EM。具体而言，FedINIBoost在每个通信轮次的每个参与者中使用两个步骤构建真实数据集的代理数据集。然后服务器聚合所有的代理数据集来构建中央虚拟数据集。

    Communication overhead is one of the major challenges in Federated Learning(FL). A few classical schemes assume the server can extract the auxiliary information about training data of the participants from the local models to construct a central dummy dataset. The server uses the dummy dataset to finetune aggregated global model to achieve the target test accuracy in fewer communication rounds. In this paper, we summarize the above solutions into a data-based communication-efficient FL framework. The key of the proposed framework is to design an efficient extraction module(EM) which ensures the dummy dataset has a positive effect on finetuning aggregated global model. Different from the existing methods that use generator to design EM, our proposed method, FedINIBoost borrows the idea of gradient match to construct EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two steps for each participant at each communication round. Then the server aggregates all the pr
    
[^105]: 深度学习在能源时间序列分析和预测中的应用

    Deep Learning for Energy Time-Series Analysis and Forecasting. (arXiv:2306.09129v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09129](http://arxiv.org/abs/2306.09129)

    本文介绍了深度学习在能源时间序列分析和预测中的应用。重点关注希腊能源市场，通过改进深度学习方法来提高预测性能。

    

    能源时间序列分析描述了通过分析过去的能源观测和可能的外部因素来预测未来的过程。在能源时间序列分析和预测的一般领域中涉及到不同的任务，其中包括电力负荷需求预测、个性化能源消费预测，以及可再生能源发电预测等。鉴于深度学习在多个视觉任务中的卓越性能，深度学习模型已成功应用于时间序列预测任务。本文旨在深入探讨各种深度学习方法，针对能源时间序列预测任务的性能进行改进，特别关注希腊能源市场，并为读者提供应用这些方法的必要知识.

    Energy time-series analysis describes the process of analyzing past energy observations and possibly external factors so as to predict the future. Different tasks are involved in the general field of energy time-series analysis and forecasting, with electric load demand forecasting, personalized energy consumption forecasting, as well as renewable energy generation forecasting being among the most common ones. Following the exceptional performance of Deep Learning (DL) in a broad area of vision tasks, DL models have successfully been utilized in time-series forecasting tasks. This paper aims to provide insight into various DL methods geared towards improving the performance in energy time-series forecasting tasks, with special emphasis in Greek Energy Market, and equip the reader with the necessary knowledge to apply these methods in practice.
    
[^106]: ViP：一个用于计算机视觉的差分隐私基础模型

    ViP: A Differentially Private Foundation Model for Computer Vision. (arXiv:2306.08842v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.08842](http://arxiv.org/abs/2306.08842)

    本论文提出了ViP，一个使用差分隐私保证的计算机视觉基础模型。通过使用掩码自编码器和DP-SGD算法，我们在LAION400M数据集上训练了ViP。ViP在标准的视觉任务中学到了高质量的表示，并且在ImageNet上达到了与AlexNet相当的准确率。

    

    由于使用互联网规模数据训练的基础模型，人工智能（AI）在能力上取得了巨大的突破。然而，互联网规模数据的非筛选性质也带来了重大的隐私和法律风险，因为它们往往包含个人信息或受版权保护的材料，未经许可不应该进行训练。在这项工作中，我们提出了一种缓解措施，即使用差分隐私（DP）保证训练基础视觉模型的方法。我们确定了掩码自编码器作为适合与DP-SGD相匹配的学习算法，并在LAION400M数据集上使用$\epsilon=8$的严格隐私预算训练了ViP - 一种具有差分隐私的视觉变压器。我们使用标准的下游视觉任务评估了ViP学到的表示质量；特别地，ViP在ImageNet上实现了$55.7\%$的（非私有）线性探测准确率，与端到端训练的AlexNet相当。

    Artificial intelligence (AI) has seen a tremendous surge in capabilities thanks to the use of foundation models trained on internet-scale data. On the flip side, the uncurated nature of internet-scale data also poses significant privacy and legal risks, as they often contain personal information or copyrighted material that should not be trained on without permission. In this work, we propose as a mitigation measure a recipe to train foundation vision models with differential privacy (DP) guarantee. We identify masked autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and train ViP -- a Vision transformer with differential Privacy -- under a strict privacy budget of $\epsilon=8$ on the LAION400M dataset. We evaluate the quality of representation learned by ViP using standard downstream vision tasks; in particular, ViP achieves a (non-private) linear probing accuracy of $55.7\%$ on ImageNet, comparable to that of end-to-end trained AlexNet (trained and evaluated
    
[^107]: 降秩卡尔曼滤波器：在高维中进行近似低秩动态滤波

    The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions. (arXiv:2306.07774v1 [stat.ML])

    [http://arxiv.org/abs/2306.07774](http://arxiv.org/abs/2306.07774)

    该论文提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播，通过将Lyapunov方程投影到低秩矩阵的流形上，使用数值稳定的动态低秩积分器求解，能够有效地处理高维数据。

    

    在高维动态系统的推断和模拟中，需要进行某种形式的降维才能使问题具有可处理性。在本文中，我们提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播。这是通过将预测步骤相关的Lyapunov方程投影到低秩矩阵的流形上来实现的，然后通过最近开发的数值稳定、动态低秩积分器求解这些方程。与此同时，通过注意协方差更新仅转换协方差矩阵的列空间，而该空间由构造得到，从而使更新步骤具有可处理性。算法与现有的基于集合的方法不同之处在于，协方差矩阵的低秩近似是确定性的，而不是随机的。关键在于，这使得该方法能够有效地处理高维数据。

    Inference and simulation in the context of high-dimensional dynamical systems remain computationally challenging problems. Some form of dimensionality reduction is required to make the problem tractable in general. In this paper, we propose a novel approximate Gaussian filtering and smoothing method which propagates low-rank approximations of the covariance matrices. This is accomplished by projecting the Lyapunov equations associated with the prediction step to a manifold of low-rank matrices, which are then solved by a recently developed, numerically stable, dynamical low-rank integrator. Meanwhile, the update steps are made tractable by noting that the covariance update only transforms the column space of the covariance matrix, which is low-rank by construction. The algorithm differentiates itself from existing ensemble-based approaches in that the low-rank approximations of the covariance matrices are deterministic, rather than stochastic. Crucially, this enables the method to repr
    
[^108]: Python封装器用于在HPO基准测试上模拟多保真度优化，无需等待

    Python Wrapper for Simulating Multi-Fidelity Optimization on HPO Benchmarks without Any Wait. (arXiv:2305.17595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17595](http://arxiv.org/abs/2305.17595)

    本研究开发了一个Python封装器，用于在HPO基准测试上模拟多保真度优化，通过强制每个工作进程等待，可以减少多小时的等待时间，使得模拟结果与实际实验的评估顺序完全一致。

    

    深度学习的超参数（HP）优化对于高性能至关重要。由于深度学习往往需要几小时到几天的训练时间，因此深度学习的HP优化通常是难以承受的昂贵的。这促使出现了表格或替代基准测试，可以在一小部分时间内查询特定HP配置的DL的（预测）性能。然而，由于DL训练的实际运行时间与查询响应时间明显不同，异步HPO（例如多保真度优化）的模拟器必须在每次迭代中等待实际运行时间，否则模拟中的评估顺序不符合实际实验。为了解决这个问题，我们开发了一个Python封装器并描述了它的用法。这个封装器强制每个工作进程等待，以便我们只需等待$10^{-2}$秒，就可以获得与实际实验完全相同的评估顺序，而不是等待几个小时。

    Hyperparameter (HP) optimization of deep learning (DL) is essential for high performance. As DL often requires several hours to days for its training, HP optimization (HPO) of DL is often prohibitively expensive. This boosted the emergence of tabular or surrogate benchmarks, which enable querying the (predictive) performance of DL with a specific HP configuration in a fraction. However, since the actual runtime of a DL training is significantly different from its query response time, simulators of an asynchronous HPO, e.g. multi-fidelity optimization, must wait for the actual runtime at each iteration in a na\"ive implementation; otherwise, the evaluation order during simulation does not match with the real experiment. To ease this issue, we developed a Python wrapper and describe its usage. This wrapper forces each worker to wait so that we yield exactly the same evaluation order as in the real experiment with only $10^{-2}$ seconds of waiting instead of waiting several hours. Our imp
    
[^109]: 使用优化空间的同态矩阵乘法来改进隐私保护PCA

    Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication. (arXiv:2305.17341v1 [cs.CR])

    [http://arxiv.org/abs/2305.17341](http://arxiv.org/abs/2305.17341)

    本文提出一种使用近似数值算术同态加密方案进行隐私保护PCA的新方法，相对以往方法，其在效率、准确性和可扩展性上均有提升，实现了同态矩阵乘法和高效同态电路，计算特征值和特征向量时具有良好的效果。

    

    主成分分析（PCA）是机器学习和数据分析领域中的重要技术。本研究提出了一种新的方法，使用近似数值算术同态加密方案进行隐私保护PCA。我们基于一种被称为PowerMethod的PCA常规方法，该方法以协方差矩阵作为输入，并产生与数据集的第一主成分对应的近似特征向量。我们的方法在效率、准确性和可扩展性方面优于以前的方法（如Pandas CSCML 21）。为了实现这样的效率和准确性，我们实现了以下优化：（i）优化了同态矩阵乘法技术（Jiang等人SIGSAC 2018），该技术在协方差矩阵的计算中起着关键作用；（ii）设计了一个有效的同态电路来同态计算协方差矩阵；（iii）设计了一种新颖且高效的同态加密方案用于特征值和特征向量的计算。

    Principal Component Analysis (PCA) is a pivotal technique in the fields of machine learning and data analysis. In this study, we present a novel approach for privacy-preserving PCA using an approximate numerical arithmetic homomorphic encryption scheme. We build our method upon a proposed PCA routine known as the PowerMethod, which takes the covariance matrix as input and produces an approximate eigenvector corresponding to the first principal component of the dataset. Our method surpasses previous approaches (e.g., Pandas CSCML 21) in terms of efficiency, accuracy, and scalability.  To achieve such efficiency and accuracy, we have implemented the following optimizations: (i) We optimized a homomorphic matrix multiplication technique (Jiang et al. SIGSAC 2018) that will play a crucial role in the computation of the covariance matrix. (ii) We devised an efficient homomorphic circuit for computing the covariance matrix homomorphically. (iii) We designed a novel and efficient homomorphic 
    
[^110]: SVDinsTN: 一种集成的张量网络表示方法及有效的结构搜索方法

    SVDinsTN: An Integrated Method for Tensor Network Representation with Efficient Structure Search. (arXiv:2305.14912v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14912](http://arxiv.org/abs/2305.14912)

    SVDinsTN是一种高效的张量网络表示方法，通过在完全连接的张量网络中插入对角因子，同时计算张量核和对角因子，从而实现最紧凑的TN结构。与现有的TN-SS方法相比，SVDinsTN在速度方面加快了10到10^3倍，并且保持了相当的水平。

    

    张量网络（TN）表示是一种强大的数据分析和机器学习技术。其中一个挑战是张量网络结构搜索（TN-SS）问题，即寻找最优结构以实现紧凑的表示。现有的TN-SS方法主要采用双层优化方法，由于重复的结构评估导致计算成本过高。为解决这个问题，我们提出了一种高效的集成（单层）方法，命名为SVDinsTN，消除了重复繁琐的结构评估。通过为完全连接的TN的每个边插入一个对角因子，我们同时计算TN核和对角因子，因子稀疏性揭示了最紧凑的TN结构。实验结果表明，与现有的TN-SS方法相比，SVDinsTN在运行时间上实现了约10到10^3倍的加速，同时保持了可比较的水平。

    Tensor network (TN) representation is a powerful technique for data analysis and machine learning. It practically involves a challenging TN structure search (TN-SS) problem, which aims to search for the optimal structure to achieve a compact representation. Existing TN-SS methods mainly adopt a bi-level optimization method that leads to excessive computational costs due to repeated structure evaluations. To address this issue, we propose an efficient integrated (single-level) method named SVD-inspired TN decomposition (SVDinsTN), eliminating the need for repeated tedious structure evaluation. By inserting a diagonal factor for each edge of the fully-connected TN, we calculate TN cores and diagonal factors simultaneously, with factor sparsity revealing the most compact TN structure. Experimental results on real-world data demonstrate that SVDinsTN achieves approximately $10\sim{}10^3$ times acceleration in runtime compared to the existing TN-SS methods while maintaining a comparable lev
    
[^111]: 基于多尺度特征金字塔网络和双重注意力机制的腹部MRI图像分割算法

    A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism. (arXiv:2305.10631v1 [eess.IV])

    [http://arxiv.org/abs/2305.10631](http://arxiv.org/abs/2305.10631)

    提出了一种基于多尺度特征金字塔网络和双重注意力机制的子腹部MRI图像分割算法，使用空洞卷积和多尺度特征金字塔编码以避免语义差距，设计双重注意力机制以保持空间信息并减少错位。

    

    本研究旨在解决U-Net在分割直肠癌治疗期间的子腹部MRI图像时，由于多次卷积和池化操作导致编码和解码之间存在语义差距和错位问题。提出了一种基于多尺度特征金字塔网络和双重注意力机制的MRI图像分割方法。我们的创新在于设计了两个模块：1）在编码中使用了空洞卷积和多尺度特征金字塔网络以避免语义差距。2）设计了双重注意力机制，以保持U-Net的空间信息并减少错位。对子腹部MRI图像数据集的实验表明，该方法比其他方法表现更好。总之，多尺度特征金字塔网络可以减少语义差距，双重注意力机制可以使编码和解码之间的特征对齐。

    This study aimed to solve the semantic gap and misalignment issue between encoding and decoding because of multiple convolutional and pooling operations in U-Net when segmenting subabdominal MRI images during rectal cancer treatment. A MRI Image Segmentation is proposed based on a multi-scale feature pyramid network and dual attention mechanism. Our innovation is the design of two modules: 1) a dilated convolution and multi-scale feature pyramid network are used in the encoding to avoid the semantic gap. 2) a dual attention mechanism is designed to maintain spatial information of U-Net and reduce misalignment. Experiments on a subabdominal MRI image dataset show the proposed method achieves better performance than others methods. In conclusion, a multi-scale feature pyramid network can reduce the semantic gap, and the dual attention mechanism can make an alignment of features between encoding and decoding.
    
[^112]: 选择你的毒药：深度图像分类数据污染攻击中的检测性与鲁棒性之争

    Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification. (arXiv:2305.09671v1 [cs.CR])

    [http://arxiv.org/abs/2305.09671](http://arxiv.org/abs/2305.09671)

    深度图像分类数据污染攻击存在检测性与鲁棒性之争：污染太少导致攻击失效，污染太多易被检测到。该论文提出两种防御措施，对有限的信任图像标签对进行后处理来检测和修复被污染的模型，并证明其有效性。

    

    在大量网络爬取数据上训练的深度图像分类模型容易受到数据污染攻击，这是一种暗藏后门的机制。即使培训过程中只有少量污染样本，也足以在推理过程中破坏模型的完整性。虽然已知污染更多的样本可以增强攻击的效果和鲁棒性，但尚不清楚污染太多样本是否会使攻击变得更易被检测到从而削弱攻击效果。我们观察到数据污染攻击中存在一个基本的检测性/鲁棒性权衡：污染太少的样本会导致攻击失效和不鲁棒，但污染太多的样本则会使攻击易被检测到。这提高了数据污染攻击者的门槛，他们必须权衡这种权衡以保持鲁棒和不易被检测。我们的工作提出了两种防御方法，旨在使用有限的信任图像标签对作为培训后的后处理步骤来检测和修复被污染的模型。我们展示了我们的防御措施可以减轻大量污染攻击，同时对逃避尝试保持抵抗力。

    Deep image classification models trained on large amounts of web-scraped data are vulnerable to data poisoning, a mechanism for backdooring models. Even a few poisoned samples seen during training can entirely undermine the model's integrity during inference. While it is known that poisoning more samples enhances an attack's effectiveness and robustness, it is unknown whether poisoning too many samples weakens an attack by making it more detectable. We observe a fundamental detectability/robustness trade-off in data poisoning attacks: Poisoning too few samples renders an attack ineffective and not robust, but poisoning too many samples makes it detectable. This raises the bar for data poisoning attackers who have to balance this trade-off to remain robust and undetectable. Our work proposes two defenses designed to (i) detect and (ii) repair poisoned models as a post-processing step after training using a limited amount of trusted image-label pairs. We show that our defenses mitigate a
    
[^113]: 动态学习系统的算法审查

    Algorithmic Censoring in Dynamic Learning Systems. (arXiv:2305.09035v1 [cs.LG])

    [http://arxiv.org/abs/2305.09035](http://arxiv.org/abs/2305.09035)

    本文介绍了动态学习系统中可能出现的审查现象，并且提出了防范审查的措施以及随机探索，从而确保来自被审查组的样本进入训练数据，并纠正模型。

    

    受选择标记影响的动态学习系统可能会出现审查现象，即针对一组或多组数据点分配持续的负面预测。在消费金融等应用中，这会导致一些申请人组被持续拒绝，并且从未进入训练数据。本文规范化审查现象，展示其可能的出现方式，并强调检测的难度。我们考虑采取防范审查的措施，并进行随机探索，这两种方法都能确保我们对原本未观察到的数据点进行标注。由此产生的技术能够让来自被审查组的样本进入训练数据并纠正模型。我们的结果突显了审查的不可测量的危害，并展示了在各种数据生成过程中缓解策略的有效性。

    Dynamic learning systems subject to selective labeling exhibit censoring, i.e. persistent negative predictions assigned to one or more subgroups of points. In applications like consumer finance, this results in groups of applicants that are persistently denied and thus never enter into the training data. In this work, we formalize censoring, demonstrate how it can arise, and highlight difficulties in detection. We consider safeguards against censoring recourse and randomized-exploration - both of which ensure we collect labels for points that would otherwise go unobserved. The resulting techniques allow examples from censored groups to enter into the training data and correct the model. Our results highlight the otherwise unmeasured harms of censoring and demonstrate the effectiveness of mitigation strategies across a range of data generating processes.
    
[^114]: 物理信息化的Token Transformer

    Physics Informed Token Transformer. (arXiv:2305.08757v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.08757](http://arxiv.org/abs/2305.08757)

    本研究提出了一种名为PITT的物理信息化的Token Transformer模型，通过将偏微分方程嵌入学习过程中，使得模型能够融入物理知识，并在多个PDE应用中展现出性能和优势。

    

    解决偏微分方程（PDEs）是许多科学和工程领域的核心。虽然传统方法往往速度慢，但机器学习模型却往往无法完整地融入系统信息。在过去几年中，Transformer对人工智能领域产生了重大影响，并在PDE应用中得到了广泛使用。然而，尽管它们取得了成功，但目前Transformer缺乏与物理和推理的整合。本研究旨在通过引入PITT：物理信息化的Token Transformer来解决这个问题。PITT的目的是通过将偏微分方程（PDEs）嵌入学习过程中来融入物理知识。PITT使用方程标记化方法来学习分析驱动的数值更新运算符。通过标记化PDEs和嵌入偏导数，Transformer模型可以意识到物理过程的基本知识。为了证明这一点，研究通过实验证明了PITT在多个PDE应用中的性能和优势。

    Solving Partial Differential Equations (PDEs) is the core of many fields of science and engineering. While classical approaches are often prohibitively slow, machine learning models often fail to incorporate complete system information. Over the past few years, transformers have had a significant impact on the field of Artificial Intelligence and have seen increased usage in PDE applications. However, despite their success, transformers currently lack integration with physics and reasoning. This study aims to address this issue by introducing PITT: Physics Informed Token Transformer. The purpose of PITT is to incorporate the knowledge of physics by embedding partial differential equations (PDEs) into the learning process. PITT uses an equation tokenization method to learn an analytically-driven numerical update operator. By tokenizing PDEs and embedding partial derivatives, the transformer models become aware of the underlying knowledge behind physical processes. To demonstrate this, P
    
[^115]: 用深度学习掌握类渗透游戏

    Mastering Percolation-like Games with Deep Learning. (arXiv:2305.07687v1 [cs.LG])

    [http://arxiv.org/abs/2305.07687](http://arxiv.org/abs/2305.07687)

    研究使用单人游戏和深度学习在网络攻击中的应用，利用训练的代理人和不同的鲁棒性定义，发现优化攻击或防御网络对特定目标非常敏感。

    

    尽管网络对随机攻击的鲁棒性得到了广泛研究，但是智能代理的故意破坏并不适用于先前的方法。在这里，我们设计了一个在晶格上的单人游戏，模拟攻击者试图摧毁网络的逻辑。游戏的目标是在最少的步骤中禁用所有节点。我们使用深度Q学习开发了一种强化学习方法，能够成功地学习玩这个游戏，并通过这种方式最优地攻击网络。由于学习算法是通用的，我们训练代理人在不同的鲁棒性定义上并比较学习策略。我们发现，表面上相似的鲁棒性定义引导训练代理使用不同的策略，暗示着优化攻击或防御网络对特定目标非常敏感。我们的方法为理解网络稳健性提供了一种新的方法，可应用于其他离散过程。

    Though robustness of networks to random attacks has been widely studied, intentional destruction by an intelligent agent is not tractable with previous methods. Here we devise a single-player game on a lattice that mimics the logic of an attacker attempting to destroy a network. The objective of the game is to disable all nodes in the fewest number of steps. We develop a reinforcement learning approach using deep Q-learning that is capable of learning to play this game successfully, and in so doing, to optimally attack a network. Because the learning algorithm is universal, we train agents on different definitions of robustness and compare the learned strategies. We find that superficially similar definitions of robustness induce different strategies in the trained agent, implying that optimally attacking or defending a network is sensitive the particular objective. Our method provides a new approach to understand network robustness, with potential applications to other discrete proces
    
[^116]: 通过容忍丢失的传输协议提升分布式机器学习训练速度

    Boosting Distributed Machine Learning Training Through Loss-tolerant Transmission Protocol. (arXiv:2305.04279v1 [cs.DC] CROSS LISTED)

    [http://arxiv.org/abs/2305.04279](http://arxiv.org/abs/2305.04279)

    通过设计容忍丢失传输协议（LTP），提高了分布式机器学习训练的速度和吞吐量，该协议允许部分梯度丢失，并通过乱序传输和乱序确认进行实现。

    

    分布式机器学习（DML）系统被用于提高数据中心和边缘节点中模型训练的速度。参数服务器（PS）通信架构常被采用，但由于多对一的“incast”流量模式导致了严重的长尾延迟，对训练吞吐量产生了负面影响。为了解决这个问题，我们设计了“容忍丢失传输协议”（LTP），它允许在同步过程中部分丢失梯度，以避免不必要的重传，并提高每次迭代的同步速度。LTP通过“乱序传输”（out-of-order transmission）和“乱序确认”（out-of-order ACKs）来实现容忍丢失的传输。LTP利用“提前关闭”（Early Close）根据网络条件调整容忍丢失的阈值，并使用“填充气泡”（Bubble Filling）进行数据校正以保持训练精度。LTP由C++实现并集成到PyTorch中。我们在一个由8个工作节点组成的实验平台上进行了评估。

    Distributed Machine Learning (DML) systems are utilized to enhance the speed of model training in data centers (DCs) and edge nodes. The Parameter Server (PS) communication architecture is commonly employed, but it faces severe long-tail latency caused by many-to-one "incast" traffic patterns, negatively impacting training throughput. To address this challenge, we design the \textbf{L}oss-tolerant \textbf{T}ransmission \textbf{P}rotocol (LTP), which permits partial loss of gradients during synchronization to avoid unneeded retransmission and contributes to faster synchronization per iteration. LTP implements loss-tolerant transmission through \textit{out-of-order transmission} and \textit{out-of-order Acknowledges (ACKs)}. LTP employs \textit{Early Close} to adjust the loss-tolerant threshold based on network conditions and \textit{Bubble Filling} for data correction to maintain training accuracy. LTP is implemented by C++ and integrated into PyTorch. Evaluations on a testbed of 8 work
    
[^117]: 使用截断数据学习高斯混合模型

    Learning Mixtures of Gaussians with Censored Data. (arXiv:2305.04127v1 [cs.LG])

    [http://arxiv.org/abs/2305.04127](http://arxiv.org/abs/2305.04127)

    本文提出了一种学习高斯混合模型的算法，该算法仅需要很少的样本且能够对权重和均值进行准确估计。

    

    本文研究了在具有截断数据的情况下，学习高斯混合模型的问题。即从一个混合单变量高斯分布$\sum_{i=1}^k w_i \mathcal{N}(\mu_i,\sigma^2)$中观测到的样本只有当其位于$S$集合内时才会被观察到。我们提出了一种算法，仅需要$\frac{1}{\varepsilon^{O(k)}}$个样本即可在$\varepsilon$误差内估计权重$w_i$和均值$\mu_i$。

    We study the problem of learning mixtures of Gaussians with censored data. Statistical learning with censored data is a classical problem, with numerous practical applications, however, finite-sample guarantees for even simple latent variable models such as Gaussian mixtures are missing. Formally, we are given censored data from a mixture of univariate Gaussians $$\sum_{i=1}^k w_i \mathcal{N}(\mu_i,\sigma^2),$$ i.e. the sample is observed only if it lies inside a set $S$. The goal is to learn the weights $w_i$ and the means $\mu_i$. We propose an algorithm that takes only $\frac{1}{\varepsilon^{O(k)}}$ samples to estimate the weights $w_i$ and the means $\mu_i$ within $\varepsilon$ error.
    
[^118]: 带有分解密度的字符串图表

    String Diagrams with Factorized Densities. (arXiv:2305.02506v1 [cs.PL])

    [http://arxiv.org/abs/2305.02506](http://arxiv.org/abs/2305.02506)

    本文描述了一个定义在随机变量集上的联合密度的范畴及其意义，以帮助概率编程和因果推断中的组合推理。

    

    有关概率编程和因果模型的研究越来越多地强调了需要在扩展定向图模型的模型类之间进行组合推理的必要性。概率编程和因果模型都定义了一组随机变量上的联合概率密度，并且展示了可以用于推理因果关系和条件独立性的稀疏结构。本文基于最近有关概率映射的马尔可夫范畴的工作，定义了一个范畴，其态射将分别由每个样本空间分解的联合密度与从样本到返回值的确定性映射组合。这是迈向最近的范畴论概率测度描述和通常在概率编程和因果推断中使用的分解密度的操作定义之间的缩小差距的一步。

    A growing body of research on probabilistic programs and causal models has highlighted the need to reason compositionally about model classes that extend directed graphical models. Both probabilistic programs and causal models define a joint probability density over a set of random variables, and exhibit sparse structure that can be used to reason about causation and conditional independence. This work builds on recent work on Markov categories of probabilistic mappings to define a category whose morphisms combine a joint density, factorized over each sample space, with a deterministic mapping from samples to return values. This is a step towards closing the gap between recent category-theoretic descriptions of probability measures, and the operational definitions of factorized densities that are commonly employed in probabilistic programming and causal inference.
    
[^119]: Learngene: 从祖先模型中继承压缩知识到后代模型

    Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])

    [http://arxiv.org/abs/2305.02279](http://arxiv.org/abs/2305.02279)

    本文提出了一种机器学习范式 Learngene，将积累的知识压缩成更为紧凑的信息片段并继承给后代模型，以便于适应新的环境

    

    在一个生物的连续进化过程中，它的基因积累了广泛的经验和知识，使新生后代能够快速适应其特定环境。受到这一观察的启发，我们提出了一种新的机器学习范 paradigm，即 Learngene，使学习模型能够融合基因的三个关键特征。 (i) 积累：知识在祖先模型的连续学习过程中积累。 (ii) 压缩：将积累的详尽知识压缩成更为紧凑的信息片段，即 Learngene。 (iii) 继承：将压缩的 Learngene 继承给后代模型，以便于适应新的环境。由于积累已在一些成熟的范式中得到研究，如大规模预训练和终身学习，因此我们专注于压缩和继承，这引发了三个关键问题，并为这些问题提供了初步的解决方案。

    During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
    
[^120]: 通过对比学习实现乳腺X线摄影图像分析的域泛化

    Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])

    [http://arxiv.org/abs/2304.10226](http://arxiv.org/abs/2304.10226)

    研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。

    

    乳腺X线摄影图像分析是医学影像学领域的一个基本问题，近年来，随着深度学习的不断发展，该领域取得了显著的进展。然而，构建深度学习模型需要大量的具有多样性的图像数据，尤其是对于不同厂商的图像风格，这往往需要非常庞大的样本集。因此，为了提高深度学习模型泛化到不同厂商图像的能力，研究者提出了一种基于对比学习的策略。

    Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
    
[^121]: 针对稳定的超网络学习的非比例参数化

    Non-Proportional Parametrizations for Stable Hypernetwork Learning. (arXiv:2304.07645v1 [cs.LG])

    [http://arxiv.org/abs/2304.07645](http://arxiv.org/abs/2304.07645)

    本文提出一种针对当前超网络训练策略不稳定、收敛速度慢的问题的解决方案，通过使用非比例加性参数化的方式来修订超网络形式，实现更加稳定和快速的训练。

    

    超网络是生成另一个神经网络参数的神经网络。在许多情况下，当前的超网络训练策略是不稳定的，收敛速度通常比非超网络模型慢得多。我们展示了这个问题与使用常见的超网络架构和初始化时出现的问题有关。我们在理论上和实验上证明了这种数值问题在训练过程中会导致不稳定性，从而降低甚至阻止收敛。我们还证明了流行的深度学习归一化策略无法解决这些问题。然后，我们提出了一种基于修订的超网络形式的解决方案，该超网络使用非比例加性参数化。我们在几个任务上测试了所提出的重新参数化，并证明它始终可以导致更稳定的训练，实现更快的收敛。

    Hypernetworks are neural networks that generate the parameters of another neural network. In many scenarios, current hypernetwork training strategies are unstable, and convergence is often far slower than for non-hypernetwork models. We show that this problem is linked to an issue that arises when using common choices of hypernetwork architecture and initialization. We demonstrate analytically and experimentally how this numerical issue can lead to an instability during training that slows, and sometimes even prevents, convergence. We also demonstrate that popular deep learning normalization strategies fail to address these issues. We then propose a solution to the problem based on a revised hypernetwork formulation that uses non-proportional additive parametrizations. We test the proposed reparametrization on several tasks, and demonstrate that it consistently leads to more stable training, achieving faster convergence.
    
[^122]: 改善临床试验的患者预筛选：利用大型语言模型辅助医生

    Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v1 [cs.LG])

    [http://arxiv.org/abs/2304.07396](http://arxiv.org/abs/2304.07396)

    本文研究了使用大型语言模型InstructGPT辅助医生预筛选患者是否符合临床试验资格。通过10个合成患者简况的性能评估，展示了LLMs在识别筛选资格标准、单独分类、整体分类、以及需要筛选资格标准的百分比上的表现。

    

    考虑到患者的临床试验，医生需要进行繁琐的检查，以确定患者是否符合文本基准。大型语言模型（LLMs）已被证明在临床信息提取和临床推理方面表现良好，但尚未在现实场景中得到应用。本文研究了使用InstructGPT辅助医生根据患者的医疗简况确定其是否符合临床试验的资格。使用一次性、选择-推理和思维链策略相结合的提示策略，我们研究了LLMs在10个合成患者简况上的表现。在四个级别上评估了性能：能否从临床试验中给出的医疗简况中识别筛选资格标准；能否为每个单独的标准分类是否符合患者；整体分类是否符合临床试验资格以及需要筛选资格标准的百分比。

    Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be scr
    
[^123]: 动态特征缩放的分段图像方法的实现与学习

    Amortized Learning of Dynamic Feature Scaling for Image Segmentation. (arXiv:2304.05448v1 [cs.CV])

    [http://arxiv.org/abs/2304.05448](http://arxiv.org/abs/2304.05448)

    该研究提出了一种新的超网络策略，可以根据缩放因子快速生成 Pareto 前沿，无需训练多个网络。该方法能够在使用更少的参数和计算的情况下实现最先进的结果。

    

    卷积神经网络已成为图像分割任务中卓越的模型。大多数卷积神经网络分割架构通过固定的因子将空间维度调整为二来聚合空间上下文。为了提高特定应用程序的模型准确性，最近的研究探讨了使用其他调整因子。然而，找到合适的调整因子通常需要为许多不同的因子训练单独的网络，并比较每个模型的性能。这些模型的计算负荷意味着在实践中很少这样做，而且只考虑了几个不同的缩放因子。在这项工作中，我们提出了一种超网络策略，可以用来轻松快速地生成在调整因子变化时，在准确度和效率之间的 Pareto 前沿。我们展示了如何训练一个单独的超网络，该网络生成条件于调整因子的 CNN 参数。这使得用户可以快速选择他们的特定应用程序的缩放因子，而无需训练多个网络。我们的方法能够在比现有方法使用更少的参数和计算的情况下实现最先进的结果。

    Convolutional neural networks (CNN) have become the predominant model for image segmentation tasks. Most CNN segmentation architectures resize spatial dimensions by a fixed factor of two to aggregate spatial context. Recent work has explored using other resizing factors to improve model accuracy for specific applications. However, finding the appropriate rescaling factor most often involves training a separate network for many different factors and comparing the performance of each model. The computational burden of these models means that in practice it is rarely done, and when done only a few different scaling factors are considered.  In this work, we present a hypernetwork strategy that can be used to easily and rapidly generate the Pareto frontier for the trade-off between accuracy and efficiency as the rescaling factor varies. We show how to train a single hypernetwork that generates CNN parameters conditioned on a rescaling factor. This enables a user to quickly choose a rescalin
    
[^124]: 提高人脸模型的身份鲁棒性

    Improving Identity-Robustness for Face Models. (arXiv:2304.03838v1 [cs.CV])

    [http://arxiv.org/abs/2304.03838](http://arxiv.org/abs/2304.03838)

    该论文探讨了在没有身份注释信息的情况下，使用人脸识别嵌入向量作为身份标识的替代方法，以提高人脸模型的身份鲁棒性和公平性。

    

    虽然深度学习模型在许多任务中取得了成功，但人们仍然担心这些模型可能学习到快捷方式，并且缺乏对无关混淆因素的鲁棒性。在直接训练于人脸上的模型中，一个敏感的混淆因素是人的身份。许多与人脸相关的任务理想情况下应该是与身份无关的，并在不同个体之间表现一致（即公平）。通过在训练期间强制执行这种鲁棒性和性能均匀性是度量和实施的一种方法，假设可以在规模上获取与身份相关的信息。但是，由于隐私问题以及收集此类信息的成本，这通常不是情况，大多数人脸数据集只包含输入图像及其相应的任务标签。因此，无需此类注释即可提高身份相关鲁棒性非常重要。在这里，我们探讨使用人脸识别嵌入向量作为身份标识的替代方法，以执行这种鲁棒性和公平性。

    Despite the success of deep-learning models in many tasks, there have been concerns about such models learning shortcuts, and their lack of robustness to irrelevant confounders. When it comes to models directly trained on human faces, a sensitive confounder is that of human identities. Many face-related tasks should ideally be identity-independent, and perform uniformly across different individuals (i.e. be fair). One way to measure and enforce such robustness and performance uniformity is through enforcing it during training, assuming identity-related information is available at scale. However, due to privacy concerns and also the cost of collecting such information, this is often not the case, and most face datasets simply contain input images and their corresponding task-related labels. Thus, improving identity-related robustness without the need for such annotations is of great importance. Here, we explore using face-recognition embedding vectors, as proxies for identities, to enfo
    
[^125]: PeakNet：一种利用深度神经网络的自动Bragg峰点寻找器

    PeakNet: An Autonomous Bragg Peak Finder with Deep Neural Networks. (arXiv:2303.15301v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2303.15301](http://arxiv.org/abs/2303.15301)

    PeakNet是一个利用深度神经网络的自动Bragg峰点寻找器，它通过实时调整来适应逐发强背景散射的波动，消除了手动调整算法参数的需求，减少了误报峰点的数量。

    

    近年来，在X射线自由电子激光器（XFEL）和同步辐射设施中的串行晶体学取得了巨大的进步，使得可以对大分子结构和分子过程进行新颖的科学研究。然而，这些实验产生了大量的数据，给数据减少和实时反馈带来了计算挑战。Bragg峰点寻找算法用于识别有用的图像，并提供关于命中率和分辨率的实时反馈。来自缓冲溶液、注射喷嘴和其他屏蔽材料的逐发强度波动和强背景散射使得这成为一个耗时的优化问题。在这里，我们提出了PeakNet，一种利用深度神经网络的自动Bragg峰点寻找器。

    Serial crystallography at X-ray free electron laser (XFEL) and synchrotron facilities has experienced tremendous progress in recent times enabling novel scientific investigations into macromolecular structures and molecular processes. However, these experiments generate a significant amount of data posing computational challenges in data reduction and real-time feedback. Bragg peak finding algorithm is used to identify useful images and also provide real-time feedback about hit-rate and resolution. Shot-to-shot intensity fluctuations and strong background scattering from buffer solution, injection nozzle and other shielding materials make this a time-consuming optimization problem. Here, we present PeakNet, an autonomous Bragg peak finder that utilizes deep neural networks. The development of this system 1) eliminates the need for manual algorithm parameter tuning, 2) reduces false-positive peaks by adjusting to shot-to-shot variations in strong background scattering in real-time, 3) e
    
[^126]: Meta-Calibration Regularized Neural Networks. (arXiv:2303.15057v2 [cs.LG] UPDATED)

    Meta-Calibration Regularized Neural Networks. (arXiv:2303.15057v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15057](http://arxiv.org/abs/2303.15057)

    本研究扩展了元校准的方法，引入了gamma网络和平滑的预期校准误差，实现了更好的神经网络校准。该方法在保持预测性能的同时解决了深度神经网络中的误校准问题。

    

    最近的研究发现，现代深度神经网络经常存在误校准问题，即预测概率与真实正确性之间的不匹配。该领域的最新工作旨在通过直接训练校准模型来解决此问题，同时优化校准误差的代理目标。最近的元校准（MC）表明，使用元学习来学习更好的校准模型是有效的。在这项工作中，我们通过两个主要组成部分扩展了MC：（1）gamma网络（gamma-net），一个元网络，用于在连续空间为调优骨干网络的focal loss学习逐样本gamma；（2）平滑的预期校准误差（SECE），一种基于高斯核的无偏和可微的ECE，旨在平滑调优gamma-net。所提出的方法在保持预测性能的同时，使神经网络更好地校准。我们的实验表明，（a）在连续空间学习逐样本gamma可以有效地优化骨干网络。

    Miscalibration-the mismatch between predicted probability and the true correctness likelihood-has been frequently identified in modern deep neural networks. Recent work in the field aims to address this problem by training calibrated models directly by optimizing a proxy of the calibration error alongside the conventional objective. Recently, Meta-Calibration (MC) showed the effectiveness of using meta-learning for learning better calibrated models. In this work, we extend MC with two main components: (1) gamma network (gamma-net), a meta network to learn a sample-wise gamma at a continuous space for focal loss for optimizing backbone network; (2) smooth expected calibration error (SECE), a Gaussian-kernel based unbiased and differentiable ECE which aims to smoothly optimizing gamma-net. The proposed method regularizes neural network towards better calibration meanwhile retain predictive performance. Our experiments show that (a) learning sample-wise gamma at continuous space can effec
    
[^127]: AI生成的文本是否可靠地检测出来？

    Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.11156](http://arxiv.org/abs/2303.11156)

    本研究通过实证和理论分析表明，在实际场景中，几种AI文本检测器不可靠。改写攻击可以破解多种检测器，包括水印方案、神经网络检测器和零样本分类器。即使是最好的检测器，随着语言模型的进一步提升，性能也会下降。因此，AI生成的文本的可靠检测仍然是一个挑战。

    

    本文从实证和理论两个方面表明，在实际场景中，几种AI文本检测器并不可靠。从实践上来说，我们证明了轻量级的改写器应用在大型语言模型（LLM）上可以破解一系列的检测器，包括使用水印方案、神经网络检测器和零样本分类器。我们的实验表明，旨在躲避改写攻击的基于检索的检测器仍然容易受到递归改写的攻击。然后，我们提出了一个理论上的不可能结果，指出随着语言模型变得越来越复杂和更擅长模仿人类文本，在最好的检测器性能会下降。对于一个足够先进的语言模型来模仿人类文本，即使最佳的检测器的表现只比随机分类器好上一点点。我们的结果足够概括特定的场景，如改写攻击。

    In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
    
[^128]: 无专家在线多智能体强化学习中的迁移学习

    Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning. (arXiv:2303.01170v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01170](http://arxiv.org/abs/2303.01170)

    本文提出了Expert-Free Online Transfer Learning (EF-OnTL)算法，在多智能体系统中实现无专家的实时迁移学习。通过动态选择迁移源智能体和要转移的知识，解决了传统迁移学习需要对专家智能体任务有良好理解的问题。

    

    传统上，强化学习中的迁移学习通过将知识从专家智能体转移到新手智能体来解决训练问题，如探索成本、数据可用性和收敛时间。然而，这种迁移需要新手智能体对专家智能体的任务有良好的理解才能有效。作为替代方案，本文提出了一种无专家在线动态迁移学习算法（EF-OnTL），该算法能够在多智能体系统中实现无专家的实时迁移学习。在每一次迁移步骤中，根据智能体的性能和不确定性来动态选择迁移源智能体和要转移的知识。为了提高不确定性估计，我们还提出了一种称为SARS-RND的方法，它是对RND的扩展，可以从智能体的状态、行动、奖励和下一状态中估计不确定性。

    Transfer learning in Reinforcement Learning (RL) has been widely studied to overcome training issues of Deep-RL, i.e., exploration cost, data availability and convergence time, by introducing a way to enhance training phase with external knowledge. Generally, knowledge is transferred from expert-agents to novices. While this fixes the issue for a novice agent, a good understanding of the task on expert agent is required for such transfer to be effective. As an alternative, in this paper we propose Expert-Free Online Transfer Learning (EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer learning in multi-agent system. No dedicated expert exists, and transfer source agent and knowledge to be transferred are dynamically selected at each transfer step based on agents' performance and uncertainty. To improve uncertainty estimation, we also propose State Action Reward Next-State Random Network Distillation (sars-RND), an extension of RND that estimates uncertainty from
    
[^129]: 基于时空因果关系的可解释水位预测器

    Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms. (arXiv:2303.00515v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00515](http://arxiv.org/abs/2303.00515)

    本研究提出一种基于时空因果关系的新型水位预测模型，通过将因果结构形式化为多层网络和使用蒙版方法，提高了其可解释性，运用于汉江数据集的实际分析中表现优异。

    

    预测汉江水位对于交通控制和避免自然灾害至关重要，但涉及多种变量并相互复杂地联系着。本研究提出一种新型的转换器，利用变量先前知识基于因果关系，预测汉江济州桥的水位。我们的模型考虑到空间和时间因果关系，将因果结构形式化为多层网络并使用蒙版方法。凭借这种方法，我们可以根据先前的知识获得可解释性。在实际数据分析中，我们使用了2016年至2021年的汉江数据集，并将所提出的模型与深度学习模型进行了比较。

    Forecasting the water level of the Han river is important to control traffic and avoid natural disasters. There are many variables related to the Han river and they are intricately connected. In this work, we propose a novel transformer that exploits the causal relationship based on the prior knowledge among the variables and forecasts the water level at the Jamsu bridge in the Han river. Our proposed model considers both spatial and temporal causation by formalizing the causal structure as a multilayer network and using masking methods. Due to this approach, we can have interpretability that consistent with prior knowledge. In real data analysis, we use the Han river dataset from 2016 to 2021 and compare the proposed model with deep learning models.
    
[^130]: 具有最佳速率的具有强间隙的差分隐私算法Saddle Point问题的研究

    Differentially Private Algorithms for the Stochastic Saddle Point Problem with Optimal Rates for the Strong Gap. (arXiv:2302.12909v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12909](http://arxiv.org/abs/2302.12909)

    本研究提出了一种在差分隐私约束下解决convex-concave Lipschitz随机Saddle Point问题的方法，并证明了在满足条件的情况下，该方法具有最佳速率和梯度复杂度。

    

    我们展示了在$(\epsilon,\delta)$-差分隐私约束下，凸凹Lipschitz随机Saddle Point问题（也称为随机极小极大优化）可以被解决，其具有强（原始-对偶）间隙率为$\tilde O\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$，其中$n$为数据集大小，$d$为问题维度。根据现有的差分隐私随机优化的下界，该速率几乎是最优的。具体来说，我们通过重新设计并分析适用于Saddle Point问题的递归正则化技术，证明了强间隙的紧密上界。我们展示了该速率可以在$O\big(\min\big\{\frac{n^2\epsilon^{1.5}}{\sqrt{d}}, n^{3/2}\big\}\big)$的梯度复杂度以及在损失函数光滑的情况下，$\tilde{O}(n)$的梯度复杂度下实现。作为我们方法的副产品，我们开发了一个通用算法，给定黑盒访问一个满足条件的子程序。

    We show that convex-concave Lipschitz stochastic saddle point problems (also known as stochastic minimax optimization) can be solved under the constraint of $(\epsilon,\delta)$-differential privacy with \emph{strong (primal-dual) gap} rate of $\tilde O\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$, where $n$ is the dataset size and $d$ is the dimension of the problem. This rate is nearly optimal, based on existing lower bounds in differentially private stochastic optimization. Specifically, we prove a tight upper bound on the strong gap via novel implementation and analysis of the recursive regularization technique repurposed for saddle point problems. We show that this rate can be attained with $O\big(\min\big\{\frac{n^2\epsilon^{1.5}}{\sqrt{d}}, n^{3/2}\big\}\big)$ gradient complexity, and $\tilde{O}(n)$ gradient complexity if the loss function is smooth. As a byproduct of our method, we develop a general algorithm that, given a black-box access to a subroutine satisfying
    
[^131]: 可变重要性匹配在因果推断中的应用

    Variable Importance Matching for Causal Inference. (arXiv:2302.11715v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.11715](http://arxiv.org/abs/2302.11715)

    这项研究提出了一种名为“模型匹配”的通用框架，通过学习距离度量，创建匹配组并估计治疗效应，实现了可审核、易排查、准确估计和可扩展的观察性因果推断方法。该框架使用变量重要性测量构建距离度量，并通过LASSO操作化实施，在不需要正确规定线性模型的情况下，实现了在潜在混淆变量数量上的可扩展性及准确性。

    

    我们的目标是提出一种可审核、易于排查、准确估计治疗效应并可扩展到高维数据的观察性因果推断方法。我们描述了一个称为“模型匹配”的通用框架，通过以下步骤实现这些目标：（i）通过结果建模学习距离度量，（ii）使用距离度量创建匹配组，（iii）使用匹配组估计治疗效应。模型匹配使用变量重要性测量来构建距离度量，使其成为一个灵活的框架，可适应不同的应用场景。我们关注问题在潜在混淆变量数量上的可扩展性，使用LASSO将模型匹配框架操作化。我们在LASSO结果建模一致地识别出所有混淆变量的情况下推导性能保证（重要的是不要求线性模型正确规定）。我们还提供实验证明了方法的扩展性与准确性。

    Our goal is to produce methods for observational causal inference that are auditable, easy to troubleshoot, accurate for treatment effect estimation, and scalable to high-dimensional data. We describe a general framework called Model-to-Match that achieves these goals by (i) learning a distance metric via outcome modeling, (ii) creating matched groups using the distance metric, and (iii) using the matched groups to estimate treatment effects. Model-to-Match uses variable importance measurements to construct a distance metric, making it a flexible framework that can be adapted to various applications. Concentrating on the scalability of the problem in the number of potential confounders, we operationalize the Model-to-Match framework with LASSO. We derive performance guarantees for settings where LASSO outcome modeling consistently identifies all confounders (importantly without requiring the linear model to be correctly specified). We also provide experimental results demonstrating the
    
[^132]: 数据污染中的时序鲁棒性

    Temporal Robustness against Data Poisoning. (arXiv:2302.03684v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03684](http://arxiv.org/abs/2302.03684)

    该论文提出了一种针对数据污染的时序威胁模型，通过利用数据的时间戳，引入了提前时间和持续时间这两个指标，从而定义了数据污染的时序鲁棒性，并提供了一种有效的保护方法。

    

    数据污染考虑了通过恶意训练数据操纵机器学习算法行为的情况。现有的数据污染威胁模型都围绕着一个单一指标，即被污染样本的数量。因此，如果攻击者能够以可承受的代价污染比预期更多的样本，就像许多实际场景中一样，他们可能能够在很短的时间内使现有的防御措施失效。为了解决这个问题，我们利用数据的出生日期时间戳，这些时间戳通常是可用的但过去被忽略。利用这些时间戳，我们提出了一个带有两个新型指标（提前时间和持续时间）的数据污染的时序威胁模型，分别衡量攻击提前开始的时间和攻击持续的时间。利用这些指标，我们定义了数据污染的时序鲁棒性的概念，即使有大量被污染的样本，也能提供有意义的保护。我们提出一种方法

    Data poisoning considers cases when an adversary manipulates the behavior of machine learning algorithms through malicious training data. Existing threat models of data poisoning center around a single metric, the number of poisoned samples. In consequence, if attackers can poison more samples than expected with affordable overhead, as in many practical scenarios, they may be able to render existing defenses ineffective in a short time. To address this issue, we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past. Benefiting from these timestamps, we propose a temporal threat model of data poisoning with two novel metrics, earliness and duration, which respectively measure how long an attack started in advance and how long an attack lasted. Using these metrics, we define the notions of temporal robustness against data poisoning, providing a meaningful sense of protection even with unbounded amounts of poisoned samples. We present a 
    
[^133]: KDEformer: 通过核密度估计加速变换器

    KDEformer: Accelerating Transformers via Kernel Density Estimation. (arXiv:2302.02451v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02451](http://arxiv.org/abs/2302.02451)

    KDEformer通过核密度估计加速变换器的注意力计算，提供了次二次时间内的近似计算，并在实证验证中显示出优异的性能。

    

    点积注意力机制在现代深度体系结构（例如Transformer）中对于序列建模起到至关重要的作用，然而，对该模型的朴素精确计算在序列长度上具有二次时间和内存复杂度，在训练长序列模型方面存在阻碍。关键瓶颈是因为在softmax函数的分母中计算分区函数以及在softmax矩阵与值矩阵之间的乘法。我们的关键发现是前者可以被简化为核密度估计（KDE）问题的变种，而高效的KDE求解器可以通过基于子采样的快速矩阵乘积来进一步加速后者。我们提出的KDEformer可以在次二次时间内近似计算注意力，并提供可证明的谱范数界限，而之前的结果只提供逐个元素的误差界限。在实证上，我们验证了KDEformer在准确性，内存和计算开销方面优于其他注意力近似方法。

    Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, na\"ive exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed KDEformer can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that KDEformer outperforms other attention approximations in terms of accuracy, memory, a
    
[^134]: Transformers遇见有向图

    Transformers Meet Directed Graphs. (arXiv:2302.00049v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00049](http://arxiv.org/abs/2302.00049)

    这项工作提出了两种有向图的方向和结构感知的位置编码，通过应用于排序网络的正确性测试和源代码理解等任务中，该模型相对于之前的最新技术提升了14.7%。

    

    Transformers最初被提出作为文本的序列到序列模型，但现在已广泛应用于包括图像、音频、视频和无向图等多种模态。然而，有向图的transformers却是一个意外未被充分开发的主题，尽管它们在包括源代码和逻辑电路在内的普遍领域中具有适用性。在这项工作中，我们提出了两种有向图的方向和结构感知的位置编码：（1）磁场拉普拉斯算子的特征向量 - 是组合拉普拉斯算子的方向感知推广；（2）方向随机游走编码。在实证上，我们展示了附加的方向信息在包括排序网络的正确性测试和源代码理解等各种下游任务中的有效性。结合数据流为中心的图构建，我们的模型在Open Graph Benchmark Code2上相对于之前的最新技术提升了14.7%。

    Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7%.
    
[^135]: 持续学习在预测性维护中的应用: 概述和挑战

    Continual Learning for Predictive Maintenance: Overview and Challenges. (arXiv:2301.12467v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12467](http://arxiv.org/abs/2301.12467)

    深度学习技术在解决工程问题中起到重要作用，预测性维护方法可以提升对维护需求的准确预测。然而，由于问题状态的变化，传统的固定训练模型存在适应性差的问题。持续学习方法提出了在部署后不断调整模型以适应演变场景的方案，但仍面临一些挑战。

    

    深度学习技术已成为有效解决工程问题的主要推动力之一。例如，预测性维护方法已被用于改进对不同机器和操作环境中维护需求的预测。然而，深度学习方法存在一些限制，因为这些模型通常是在反映当前问题状态的固定分布上进行训练的。由于内部或外部因素，问题状态可能会发生改变，并且由于缺乏泛化和适应性，性能会下降。与这种固定训练集相反，现实世界的应用程序不断变化其环境，这就需要在部署后不断调整模型以适应不断演变的场景。为了帮助解决这个问题，持续学习方法提出了在部署后不断调整预测模型并融入新知识的方式。尽管这些技术具有优势，但仍然存在一些挑战。

    Deep learning techniques have become one of the main propellers for solving engineering problems effectively and efficiently. For instance, Predictive Maintenance methods have been used to improve predictions of when maintenance is needed on different machines and operative contexts. However, deep learning methods are not without limitations, as these models are normally trained on a fixed distribution that only reflects the current state of the problem. Due to internal or external factors, the state of the problem can change, and the performance decreases due to the lack of generalization and adaptation. Contrary to this stationary training set, real-world applications change their environments constantly, creating the need to constantly adapt the model to evolving scenarios. To aid in this endeavor, Continual Learning methods propose ways to constantly adapt prediction models and incorporate new knowledge after deployment. Despite the advantages of these techniques, there are still c
    
[^136]: SWARM并行性: 训练大模型可以在通信效率上有惊人的效果

    SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. (arXiv:2301.11913v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2301.11913](http://arxiv.org/abs/2301.11913)

    我们提出了SWARM并行性，一种用于训练大模型的模型并行算法，适用于连接差、异构和不可靠设备。通过在节点之间创建临时的随机化管道并进行重新平衡，SWARM可以实现更少的通信密集度。与现有的大规模训练方法相比，我们的方法具有更好的性能，并与压缩策略结合使用来训练大型Transformer语言模型。

    

    许多深度学习应用受益于使用包含数十亿个参数的大模型。由于需要专用的HPC集群，训练这些模型通常非常昂贵。在这项工作中，我们考虑了训练大模型的替代方法：使用廉价的“可抢占”实例或从多个区域汇集现有资源。我们分析了这些条件下现有模型并行算法的性能，并找到了训练更大模型时通信密集度较低的配置。基于这些发现，我们提出了SWARM并行性，这是一种针对连接差、异构和不可靠设备的模型并行训练算法。SWARM在节点之间创建临时的随机化管道，并在出现故障时进行重新平衡。我们通过实验证实了我们的发现，并将SWARM并行性与现有的大规模训练方法进行了比较。最后，我们将我们的见解与压缩策略相结合，训练了一个大型的Transformer语言模型。

    Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap "preemptible" instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM parallelism with existing large-scale training approaches. Finally, we combine our insights with compression strategies to train a large Transformer language model
    
[^137]: 基于数据驱动的一般核矩阵线性复杂度低秩逼近：一种几何方法

    Data-Driven Linear Complexity Low-Rank Approximation of General Kernel Matrices: A Geometric Approach. (arXiv:2212.12674v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2212.12674](http://arxiv.org/abs/2212.12674)

    本文提出了一种基于几何方法的数据驱动线性复杂度低秩逼近算法，适用于大规模、任意分布的矩形核矩阵，可用于高斯过程回归等应用场景。

    

    一般的矩形核矩阵可以定义为 $K_{ij} = \kappa(x_i,y_j)$，其中 $\kappa(x,y)$ 是一个核函数，$X=\{x_i\}_{i=1}^m$ 和 $Y=\{y_i\}_{i=1}^n$ 是两组点集。本文旨在寻找一个核矩阵的低秩逼近，其中点集 $X$ 和 $Y$ 是大规模而任意分布的，比如相距远离、交错分布、相同等等。这样的矩形核矩阵可能出现在高斯过程回归中，其中 $X$ 对应训练数据，$Y$ 对应测试数据。在这种情况下，点集通常是高维的。由于点集很大，我们必须利用矩阵来自于核函数的事实，并避免形成矩阵，从而排除了大多数代数技术。特别地，我们寻求能够以固定逼近秩为代价线性或近乎线性扩展的方法。本文的主要思想是使用几何方法来近似线性或近乎线性地表示矩阵的内部结构和模式。

    A general, {\em rectangular} kernel matrix may be defined as $K_{ij} = \kappa(x_i,y_j)$ where $\kappa(x,y)$ is a kernel function and where $X=\{x_i\}_{i=1}^m$ and $Y=\{y_i\}_{i=1}^n$ are two sets of points. In this paper, we seek a low-rank approximation to a kernel matrix where the sets of points $X$ and $Y$ are large and are arbitrarily distributed, such as away from each other, ``intermingled'', identical, etc. Such rectangular kernel matrices may arise, for example, in Gaussian process regression where $X$ corresponds to the training data and $Y$ corresponds to the test data. In this case, the points are often high-dimensional. Since the point sets are large, we must exploit the fact that the matrix arises from a kernel function, and avoid forming the matrix, and thus ruling out most algebraic techniques. In particular, we seek methods that can scale linearly or nearly linear with respect to the size of data for a fixed approximation rank. The main idea in this paper is to {\em geo
    
[^138]: 在实践中的可解释性: 从塞内加尔的移动电话数据中估计电气化率

    Explainability in Practice: Estimating Electrification Rates from Mobile Phone Data in Senegal. (arXiv:2211.06277v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2211.06277](http://arxiv.org/abs/2211.06277)

    本论文介绍了使用XAI的一个实际案例：在塞内加尔的移动电话数据上基于ML模型进行电气化率估计。研究发现该模型存在人口密度偏见，并指出了数据处理和模型设计方面的挑战，以及解释的解释的重要性。

    

    可解释的人工智能（XAI）为不可解释的机器学习（ML）模型提供解释。尽管存在许多技术方法，但缺乏对真实世界数据集的验证。在这项工作中，我们提出了XAI的一个用例：一个使用塞内加尔的移动电话数据训练的ML模型，用于估计电气化率。这些数据源自2014/15年橙子公司的数据发展挑战。我们应用了两种模型不可知的局部解释技术，并发现虽然模型可以被验证，但在人口密度方面存在偏见。我们通过指出我们在工作过程中遇到的两个主要挑战来总结本文：数据处理和模型设计可能受到目前可用的XAI方法的限制，以及解释的解释的重要性。

    Explainable artificial intelligence (XAI) provides explanations for not interpretable machine learning (ML) models. While many technical approaches exist, there is a lack of validation of these techniques on real-world datasets. In this work, we present a use-case of XAI: an ML model which is trained to estimate electrification rates based on mobile phone data in Senegal. The data originate from the Data for Development challenge by Orange in 2014/15. We apply two model-agnostic, local explanation techniques and find that while the model can be verified, it is biased with respect to the population density. We conclude our paper by pointing to the two main challenges we encountered during our work: data processing and model design that might be restricted by currently available XAI methods, and the importance of domain knowledge to interpret explanations.
    
[^139]: 无数据情况下对黑盒模型进行防御的方法

    Data-free Defense of Black Box Models Against Adversarial Attacks. (arXiv:2211.01579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01579](http://arxiv.org/abs/2211.01579)

    本研究提出了一种无数据情况下对黑盒模型进行防御的方法，通过生成模型构建合成数据，并使用模型窃取技术训练替代模型网络，同时采用小波噪声去除器（WNR）减少对抗性污染。

    

    许多公司通过API仅将训练好的深度模型作为黑盒暴露给第三方用户，以保护模型的细节（如架构、学习权重、训练细节等）。本研究提出了一种针对黑盒模型在无数据情况下进行对抗攻击的新型防御机制。我们通过生成模型构建合成数据，并使用模型窃取技术训练替代模型网络。为了最小化扰动样本上的对抗性污染，我们提出了“小波噪声去除器”(WNR)，它在输入图像上执行离散小波分解，并仅选择我们的“小波系数选择模块”(WCSM)确定的少数重要系数。为了在通过WNR去除噪声后恢复图像的高频内容，我们进一步训练了一个“再生器”网络，目标是恢复系数。

    Several companies often safeguard their trained deep models (i.e., details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as black boxes through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose 'wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our 'wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a 'regenerator' network with the objective of retrieving the coeffi
    
[^140]: 在有向无环图上的Transformer

    Transformers over Directed Acyclic Graphs. (arXiv:2210.13148v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13148](http://arxiv.org/abs/2210.13148)

    本文研究了在有向无环图上的Transformer。通过改进注意机制和位置编码，本方法在多种任务上表现出了很好的效果。

    

    最近，Transformer模型在图表示学习中变得流行起来，因为它们有能力学习超出常规图神经网络捕捉到的复杂关系。主要的研究问题是如何将图的结构偏差注入到Transformer的架构中，并针对有向无环图（DAGs）提出了一些适应性的架构改进：（1）一个比常规Transformer的二次复杂度更高效的注意机制，同时忠实地捕捉了DAGs的结构，（2）一个对DAG的偏序进行位置编码，补充了前者。我们对我们的方法在各种类型的任务上进行了严格的评估，从对源代码图的分类到对引用网络中的节点，结果显示它在两个重要的任务上是有效的。

    Transformer models have recently gained popularity in graph representation learning as they have the potential to learn complex relationships beyond the ones captured by regular graph neural networks. The main research question is how to inject the structural bias of graphs into the transformer architecture, and several proposals have been made for undirected molecular graphs and, recently, also for larger network graphs. In this paper, we study transformers over directed acyclic graphs (DAGs) and propose architecture adaptations tailored to DAGs: (1) An attention mechanism that is considerably more efficient than the regular quadratic complexity of transformers and at the same time faithfully captures the DAG structure, and (2) a positional encoding of the DAG's partial order, complementing the former. We rigorously evaluate our approach over various types of tasks, ranging from classifying source code graphs to nodes in citation networks, and show that it is effective in two importan
    
[^141]: 关于预训练在RL泛化中的能力：可证明的好处和困难

    On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness. (arXiv:2210.10464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10464](http://arxiv.org/abs/2210.10464)

    本文研究了强化学习中预训练的泛化能力。当与目标环境交互不允许时，最好的结果是接近最优的策略；当允许交互时，预训练的改进最多是一个常数因子。在非渐近情况下，我们设计了一个高效算法，并证明了与状态动作空间无关的目标环境遗憾界。

    

    强化学习（RL）中的泛化目标是在训练期间学习一个能够适用于目标环境的代理。本文从理论的角度研究了RL泛化：我们可以期望通过在训练环境上的预训练对泛化有多大的帮助？当与目标环境的交互不允许时，我们证明我们最多可以获得的是一个近乎最优的策略，同时设计了一个可以实现这一目标的算法。此外，当允许代理与目标环境进行交互时，我们得到了一个令人惊讶的结果，即从预训练中的改进在渐近意义下最多只有一个常数因子。另一方面，在非渐近情况下，我们设计了一个高效的算法，并证明了与状态动作空间无关的目标环境基于分布的遗憾界。

    Generalization in Reinforcement Learning (RL) aims to learn an agent during training that generalizes to the target environment. This paper studies RL generalization from a theoretical aspect: how much can we expect pre-training over training environments to be helpful? When the interaction with the target environment is not allowed, we certify that the best we can obtain is a near-optimal policy in an average sense, and we design an algorithm that achieves this goal. Furthermore, when the agent is allowed to interact with the target environment, we give a surprising result showing that asymptotically, the improvement from pre-training is at most a constant factor. On the other hand, in the non-asymptotic regime, we design an efficient algorithm and prove a distribution-based regret bound in the target environment that is independent of the state-action space.
    
[^142]: 对数线性保护性及其影响的研究

    Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10012](http://arxiv.org/abs/2210.10012)

    本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。

    

    已经发现，在假设可线性的神经表示中，从中删除可人解释的概念的方法是可行和有用的。然而，这种删除对于基于修改后表示进行训练的下游分类器行为的影响尚未完全理解。在这项工作中，我们正式定义了对数线性保护性的概念，即对手无法直接从表示中预测概念的能力，并研究其影响。我们证明，在二元情况下，在某些假设下，下游对数线性模型无法恢复被删除的概念。然而，我们证明，在某些情况下，可以构建一个多类对数线性模型，间接恢复概念，这指出了对数线性保护性作为下游偏差缓解技术的内在局限性。这些发现揭示了线性删除方法的理论限制，并强调了进一步研究可解释神经表示与分类器之间的联系的需要。

    Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
    
[^143]: 测试时间对象检测的查询驱动困难图像检索

    Query-based Hard-Image Retrieval for Object Detection at Test Time. (arXiv:2209.11559v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.11559](http://arxiv.org/abs/2209.11559)

    该论文研究了测试时间对象检测中的查询驱动困难图像检索问题，提出了一种基于查询的简单直观方法来解决这个问题。

    

    长期以来，人们对于捕捉对象检测器错误行为的兴趣一直存在，即找到其性能可能不令人满意的图像。在诸如自动驾驶等实际应用中，对于超出简单检测性能要求的潜在故障进行表征也至关重要。例如，与自身车辆相近的行人的漏检通常需要更仔细的检查，而与车辆较远处的漏检相比较而言。在文献中，预测此类潜在故障在测试时间的问题在很大程度上被忽视了，而基于检测不确定性的传统方法则无法提供对错误的这种精细化表征。在本研究中，我们提出了将找到“困难”图像的问题重新定义为一个基于查询的困难图像检索任务，其中查询是对“困难性”的具体定义，并提供了一种简单直观的方法，可以针对大量的困难图像定义解决这个任务。

    There is a longstanding interest in capturing the error behaviour of object detectors by finding images where their performance is likely to be unsatisfactory. In real-world applications such as autonomous driving, it is also crucial to characterise potential failures beyond simple requirements of detection performance. For example, a missed detection of a pedestrian close to an ego vehicle will generally require closer inspection than a missed detection of a car in the distance. The problem of predicting such potential failures at test time has largely been overlooked in the literature and conventional approaches based on detection uncertainty fall short in that they are agnostic to such fine-grained characterisation of errors. In this work, we propose to reformulate the problem of finding "hard" images as a query-based hard image retrieval task, where queries are specific definitions of "hardness", and offer a simple and intuitive method that can solve this task for a large family of
    
[^144]: MAGIC: 通过反转准鲁棒分类器实现基于掩码的图像合成

    MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.11549](http://arxiv.org/abs/2209.11549)

    本论文提出了一种名为MAGIC的方法，通过反转准鲁棒分类器进行一次性掩码引导的图像合成。它通过聚合梯度并利用强空间先验的指导二进制掩码，实现了形状和位置控制、非刚性形状变形以及复制/移动操作，并可简单指定二进制引导掩码来提供强大的合成控制。

    

    我们提供了一种一次性掩码引导图像合成的方法，通过反转带有强正则化器的准鲁棒分类器来控制对单个图像的操作。我们提出的方法名为MAGIC，利用来自预训练的准鲁棒分类器的结构化梯度，可以更好地保留输入的语义，并保持其分类准确性，从而保证合成的可信度。与目前使用复杂原语来监督过程或使用注意力图作为弱监督信号的方法不同，MAGIC通过在输入上聚合梯度，由强空间先验的指导二进制掩码推动。MAGIC以单个框架实现了一系列操作，实现了形状和位置控制、强烈的非刚性形状变形以及在重复物体存在的情况下的复制/移动操作，并通过简单指定二进制引导掩码来给用户提供强大的合成控制。

    We offer a method for one-shot mask-guided image synthesis that allows controlling manipulations of a single image by inverting a quasi-robust classifier equipped with strong regularizers. Our proposed method, entitled MAGIC, leverages structured gradients from a pre-trained quasi-robust classifier to better preserve the input semantics while preserving its classification accuracy, thereby guaranteeing credibility in the synthesis. Unlike current methods that use complex primitives to supervise the process or use attention maps as a weak supervisory signal, MAGIC aggregates gradients over the input, driven by a guide binary mask that enforces a strong, spatial prior. MAGIC implements a series of manipulations with a single framework achieving shape and location control, intense non-rigid shape deformations, and copy/move operations in the presence of repeating objects and gives users firm control over the synthesis by requiring to simply specify binary guide masks. Our study and findin
    
[^145]: 基于条件扩散模型的有损图像压缩

    Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.06950](http://arxiv.org/abs/2209.06950)

    本文提出了一种利用条件扩散模型进行有损图像压缩的优化框架。通过引入额外的内容潜变量以及合成纹理变量，该方法在图像质量评估指标上表现出更强的性能。

    

    本文提出了一种利用扩散生成模型的端到端优化的有损图像压缩框架。该方法基于变换编码范式，将图像映射到潜在空间进行信息熵编码，然后再映射回数据空间进行重构。与基于变分自编码器(VAE)的神经压缩方法不同，我们的解码器是一个条件扩散模型。因此，我们的方法引入了一个额外的“内容”潜变量，反向扩散过程会对其进行条件化，并利用该变量存储图像信息。决定扩散过程的剩余“纹理”变量会在解码时合成。通过实验，我们展示了模型的性能可以根据感知度量进行调整。我们广泛的实验涉及了多个数据集和图像质量评估指标，结果表明我们的方法相较于基于生成对抗网络的方法能够得到更好的FID分数。

    This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
    
[^146]: 软件测试任务的强化学习框架比较

    A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks. (arXiv:2208.12136v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2208.12136](http://arxiv.org/abs/2208.12136)

    本论文比较了软件测试任务中不同强化学习框架的效果和性能，并指出当前文献中缺乏对DRL框架中实现算法的实证评估和指导方针。

    

    软件测试活动通过审查软件产品的工件和行为，发现可能的缺陷并确保产品满足预期要求。最近，深度强化学习（DRL）已成功应用于复杂的测试任务，如游戏测试、回归测试和测试用例优化，以自动化过程并提供持续适应性。开发者可以通过从头实现DRL算法或使用DRL框架来使用DRL。DRL框架提供了维护良好、实现了最新DRL算法的工具，以便加快DRL应用的开发。开发者已广泛使用这些框架来解决包括软件测试在内的各个领域的问题。然而，据我们所知，目前还没有对DRL框架中实现算法的有效性和性能进行实证评估的研究。此外，文献中缺乏一些指导方针，这些指导方针将帮助开发者在使用DRL框架时做出决策。

    Software testing activities scrutinize the artifacts and the behavior of a software product to find possible defects and ensure that the product meets its expected requirements. Recently, Deep Reinforcement Learning (DRL) has been successfully employed in complex testing tasks such as game testing, regression testing, and test case prioritization to automate the process and provide continuous adaptation. Practitioners can employ DRL by implementing from scratch a DRL algorithm or using a DRL framework. DRL frameworks offer well-maintained implemented state-of-the-art DRL algorithms to facilitate and speed up the development of DRL applications. Developers have widely used these frameworks to solve problems in various domains including software testing. However, to the best of our knowledge, there is no study that empirically evaluates the effectiveness and performance of implemented algorithms in DRL frameworks. Moreover, some guidelines are lacking from the literature that would help 
    
[^147]: 语言模型作为知识嵌入

    Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.12617](http://arxiv.org/abs/2206.12617)

    该论文提出了一种使用语言模型来推导知识嵌入的方法LMKE，它旨在提高对丰富的长尾实体的表示能力并解决基于描述的先前方法的问题，实验结果表明该方法在多个基准数据集上实现了最先进的性能。

    

    知识嵌入是通过将实体和关系嵌入到连续向量空间中来表示知识图谱的一种方法。现有的方法主要是基于结构或基于描述。基于结构的方法学习表示，以保留知识图谱的内在结构。它们不能很好地表示现实世界知识图谱中有限结构信息下丰富的长尾实体。基于描述的方法利用文本信息和语言模型。在这个方向上的先前方法几乎无法超越基于结构的方法，并且存在昂贵的负采样和限制性描述需求等问题。在本文中，我们提出了LMKE，采用语言模型来推导知识嵌入，旨在丰富长尾实体的表示并解决基于描述的先前方法的问题。我们用对比学习框架来表述基于描述的知识嵌入学习，以提高训练和评价的效率。实验结果表明，LMKE在多个基准数据集上实现了最先进的性能，超越了基于结构和基于先前描述的方法。

    Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
    
[^148]: GNN在推广带限函数方面的优越性比NN更加明显

    Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05904](http://arxiv.org/abs/2206.05904)

    本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。

    

    图神经网络（GNN）以其整合图形信息的能力被广泛用于数据分析。然而，GNN的表达能力仅针对图级任务进行了研究，而不是针对节点级任务，例如节点分类，其中试图从观察到的节点标签中插值出缺失的标签信息。本文研究了GNN在所述分类任务中的表达能力，它实质上是一个函数插值问题。具体而言，我们导出了GNN插值$\mathbb{R}^d$中带限函数所需的权重和层数。我们的结果显示，使用GNN架构以$\epsilon$-近似离散带限信号仅需要$O((\log \epsilon^{-1})^{d})$个权重，这比使用完全连接的神经网络（NN）得到的最佳结果的所需权重少得多 - 特别地，使用使用$O((\log \epsilon^{-1})^{d})$个样本来训练GNN以$\epsilon$-逼近带限函数。

    Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\mathbb{R}^d$. Our result shows that, the number of weights needed to $\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\log \epsilon^{-1})^{d})$ weights using a GNN trained by $O((\log \epsilon^{-1})^{d})$ samples to $\epsilon$-approximate a discretized bandlimited signal
    
[^149]: 通过手机嵌入式传感器数据对用户的社交背景和熟悉地点进行设备内建模

    On-device modeling of user's social context and familiar places from smartphone-embedded sensor data. (arXiv:2205.08790v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.08790](http://arxiv.org/abs/2205.08790)

    本研究提出了一种新颖的、无监督的轻量级方法，通过在用户的移动设备上建模用户的社交背景和地点，从而从手机嵌入式传感器数据中提取高层次和语义丰富的上下文特征。

    

    上下文建模和识别是复杂的任务，使移动和泛在计算应用能够适应用户的情境。当前的解决方案主要集中在对有限的上下文信息的处理上，通常在集中式架构上处理，可能会暴露用户的个人数据，而且缺乏个性化功能。因此，设备内上下文建模和识别代表了该领域的当前研究趋势。在移动环境中，用户的社交互动和访问地点是对日常生活场景进行表征的重要信息。在本文中，我们提出了一种新颖的、无监督的轻量级方法，通过直接在用户的移动设备上基于自我网络对用户的社交背景和地点进行建模。依靠这个模型，系统能够从手机嵌入式传感器数据中提取高层次和语义丰富的上下文特征。

    Context modeling and recognition represent complex tasks that allow mobile and ubiquitous computing applications to adapt to the user's situation. Current solutions mainly focus on limited context information generally processed on centralized architectures, potentially exposing users' personal data to privacy leakage, and missing personalization features. For these reasons on-device context modeling and recognition represent the current research trend in this area. Among the different information characterizing the user's context in mobile environments, social interactions and visited locations remarkably contribute to the characterization of daily life scenarios. In this paper we propose a novel, unsupervised and lightweight approach to model the user's social context and her locations based on ego networks directly on the user mobile device. Relying on this model, the system is able to extract high-level and semantic-rich context features from smartphone-embedded sensors data. Speci
    
[^150]: 使用预计算源统计数据进行协方差感知的特征对齐，实现对多种图像污染的测试时间自适应

    Covariance-aware Feature Alignment with Pre-computed Source Statistics for Test-time Adaptation to Multiple Image Corruptions. (arXiv:2204.13263v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.13263](http://arxiv.org/abs/2204.13263)

    本文提出了一种使用预计算源统计数据的协方差感知的特征对齐方法，用于对多种图像污染进行测试时间自适应。现有的测试时间自适应方法在面对多种污染时的适应能力有限。本文通过解决复杂的分布偏移问题，提高了对多种污染的自适应能力。

    

    实际世界中的图像识别系统经常遇到损坏的输入图像，这会导致分布偏移并降低模型的性能。这些系统通常在中央服务器中使用单个预测模型，并处理来自各种环境（如分布在城市或汽车中的摄像头）的图像。这样的单一模型在测试时间面临着以多种不同方式损坏的图像。因此，它们需要在测试过程中立即适应多个变形，而不是以高成本进行重新训练。测试时间自适应 (TTA)，旨在在不访问训练数据集的情况下调整模型，是可以解决这个问题的一种方法。现有的TTA方法确实能够很好地适应单一的污染。然而，在出现多种类型的污染时，适应能力受到限制，这更符合实际情况。我们推测这是因为分布偏移更加复杂，在多种污染情况下适应变得更加困难。事实上，我们预期... (未完待续)

    Real-world image recognition systems often face corrupted input images, which cause distribution shifts and degrade the performance of models. These systems often use a single prediction model in a central server and process images sent from various environments, such as cameras distributed in cities or cars. Such single models face images corrupted in heterogeneous ways in test time. Thus, they require to instantly adapt to the multiple corruptions during testing rather than being re-trained at a high cost. Test-time adaptation (TTA), which aims to adapt models without accessing the training dataset, is one of the settings that can address this problem. Existing TTA methods indeed work well on a single corruption. However, the adaptation ability is limited when multiple types of corruption occur, which is more realistic. We hypothesize this is because the distribution shift is more complicated, and the adaptation becomes more difficult in case of multiple corruptions. In fact, we expe
    
[^151]: 多视图无监督特征选择与图学习的联合方法

    Joint Multi-view Unsupervised Feature Selection and Graph Learning. (arXiv:2204.08247v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.08247](http://arxiv.org/abs/2204.08247)

    本文提出了一种联合多视图无监督特征选择和图学习的方法，通过正交分解建模多视图特征选择，应用跨空间局部保持进行聚类结构学习和相似性学习的连接。

    

    尽管取得了一定的进展，但之前的多视图无监督特征选择方法主要存在两个限制。首先，它们通常使用聚类结构或相似性结构来指导特征选择，忽略了联合公式可能带来的互惠效益。其次，它们通常通过全局结构学习或局部结构学习来学习相似性结构，缺乏同时具备全局和局部结构感知的图学习能力。鉴于此，本文提出了一种联合多视图无监督特征选择和图学习的方法（JMVFG）。具体而言，我们采用正交分解对多视图特征选择进行建模，其中每个目标矩阵被分解为一个视图特定的基矩阵和一个视图一致的聚类指示器。跨空间局部保持被应用于在投影空间中进行聚类结构学习和相似性学习的连接。

    Despite significant progress, previous multi-view unsupervised feature selection methods mostly suffer from two limitations. First, they generally utilize either cluster structure or similarity structure to guide the feature selection, which neglect the possibility of a joint formulation with mutual benefits. Second, they often learn the similarity structure by either global structure learning or local structure learning, which lack the capability of graph learning with both global and local structural awareness. In light of this, this paper presents a joint multi-view unsupervised feature selection and graph learning (JMVFG) approach. Particularly, we formulate the multi-view feature selection with orthogonal decomposition, where each target matrix is decomposed into a view-specific basis matrix and a view-consistent cluster indicator. The cross-space locality preservation is incorporated to bridge the cluster structure learning in the projected space and the similarity learning (i.e.
    
[^152]: “这是一个可疑的反应！”：解读概率变化以检测NLP对抗攻击。

    "That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks. (arXiv:2204.04636v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.04636](http://arxiv.org/abs/2204.04636)

    这项工作提出了一个模型无关的对抗文本检测器，通过识别目标分类器的概率中的模式来改进对抗输入的识别性能，并具有较强的泛化能力。

    

    对抗攻击是当前机器学习研究面临的主要挑战。这些有意制作的输入甚至可以欺骗最先进的模型，使其无法在安全关键的应用中部署。计算机视觉领域已经进行了大量研究以开发可靠的防御策略。然而，在自然语言处理中，同样的问题仍然没有得到深入探究。我们的工作提出了一个对抗文本示例的模型无关检测器。该方法通过扰动输入文本时在目标分类器的概率中识别模式。所提出的检测器在识别对抗输入方面提高了当前技术水平，并展示了在不同的NLP模型、数据集和词级攻击中具有较强的泛化能力。

    Adversarial attacks are a major challenge faced by current machine learning research. These purposely crafted inputs fool even the most advanced models, precluding their deployment in safety-critical applications. Extensive research in computer vision has been carried to develop reliable defense strategies. However, the same issue remains less explored in natural language processing. Our work presents a model-agnostic detector of adversarial text examples. The approach identifies patterns in the logits of the target classifier when perturbing the input text. The proposed detector improves the current state-of-the-art performance in recognizing adversarial inputs and exhibits strong generalization capabilities across different NLP models, datasets, and word-level attacks.
    
[^153]: 电压法结构建模

    Structure from Voltage. (arXiv:2203.00063v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00063](http://arxiv.org/abs/2203.00063)

    本论文研究了一种基于电压法的结构建模方法，通过使用缩放电阻可以有效得到电压和有效电阻的极限，同时通过添加一个"地面"节点可以简单自然地计算所有距离。

    

    有效电阻（ER）是一种探究图结构的有吸引力的方法，它是计算图拉普拉斯矩阵特征向量的替代方式。图拉普拉斯矩阵用于在高维数据中找到低维结构。在这方面，基于ER的分析比基于特征向量的方法具有优势。然而，Von Luxburg等人（2010）表明，当顶点对应于从度量空间的分布中采样的样本时，远离点之间的ER收敛到一种无关于图结构的平凡量。我们表明，通过在一个具有$n$个顶点和$n^2$的图中使用缩放电阻，可以得到电压和有效电阻的有意义的极限。我们还表明，通过向度量图添加一个“地面”节点，可以简单自然地计算从一个选定点到所有其他点的所有距离。

    Effective resistance (ER) is an attractive way to interrogate the structure of graphs. It is an alternative to computing the eigen-vectors of the graph Laplacian. Graph laplacians are used to find low dimensional structures in high dimensional data. Here too, ER based analysis has advantages over eign-vector based methods. Unfortunately Von Luxburg et al. (2010) show that, when vertices correspond to a sample from a distribution over a metric space, the limit of the ER between distant points converges to a trivial quantity that holds no information about the structure of the graph. We show that by using scaling resistances in a graph with $n$ vertices by $n^2$, one gets a meaningful limit of the voltages and of effective resistances. We also show that by adding a "ground" node to a metric graph one gets a simple and natural way to compute all of the distances from a chosen point to all other points.
    
[^154]: AI最近变得更消极了吗？

    Did AI get more negative recently?. (arXiv:2202.13610v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.13610](http://arxiv.org/abs/2202.13610)

    本文通过对自然语言处理和机器学习领域的论文进行分类和分析，发现随着时间的推移，科学文章更倾向于积极的立场，但也存在一些持消极立场的论文。

    

    在这篇论文中，我们将人工智能（AI）的核心子领域自然语言处理（NLP）和机器学习（ML）的科学文章分类为两种，一种是通过引入新技术超越现有模型的文章，被称为“积极立场”；另一种是主要批评现有技术不足的文章，被称为“消极立场”。我们使用超过1500篇NLP和ML论文进行标注，使用基于SciBERT的模型自动预测论文的立场。然后，我们分析了近35年来NLP和ML领域的超过41000篇论文的大规模趋势，发现论文随着时间的推移变得更积极，但也有一些消极的论文。

    In this paper, we classify scientific articles in the domain of natural language processing (NLP) and machine learning (ML), as core subfields of artificial intelligence (AI), into whether (i) they extend the current state-of-the-art by the introduction of novel techniques which beat existing models or whether (ii) they mainly criticize the existing state-of-the-art, i.e. that it is deficient with respect to some property (e.g. wrong evaluation, wrong datasets, misleading task specification). We refer to contributions under (i) as having a 'positive stance' and contributions under (ii) as having a 'negative stance' (to related work). We annotate over 1.5 k papers from NLP and ML to train a SciBERT-based model to automatically predict the stance of a paper based on its title and abstract. We then analyse large-scale trends on over 41 k papers from the last approximately 35 years in NLP and ML, finding that papers have become substantially more positive over time, but negative papers als
    
[^155]: 图上的梯度流：存在性、收敛性、连续性方程

    Gradient flows on graphons: existence, convergence, continuity equations. (arXiv:2111.09459v3 [math.PR] UPDATED)

    [http://arxiv.org/abs/2111.09459](http://arxiv.org/abs/2111.09459)

    本论文研究了图上的梯度流问题，发现在大图的边权重适当函数的欧几里得梯度流收敛到图函数空间上一条新型连续极限。许多自然函数在该设置下都得到了涵盖，例如同态函数和标量熵。

    

    在各种优化问题中，Wasserstein梯度流在概率测度上发现了许多应用。它们通常出现为交换粒子系统的连续极限，这些粒子系统通过某种涉及梯度型势能的均场相互作用演化。然而，在许多问题中，诸如多层神经网络中，所谓的粒子是大图上的边权重，其节点是可交换的。这样的大图已知在其大小趋于无穷大时收敛到称为图函数的连续极限。我们证明了适当函数的欧几里得梯度流收敛到图函数空间上的一条新型连续极限，可适当描述为梯度流或更技术性地说，是一条最大斜率曲线。我们的设置涵盖了图函数上的几个自然函数，例如同态函数和标量熵，并且已经详细计算了这些例子。

    Wasserstein gradient flows on probability measures have found a host of applications in various optimization problems. They typically arise as the continuum limit of exchangeable particle systems evolving by some mean-field interaction involving a gradient-type potential. However, in many problems, such as in multi-layer neural networks, the so-called particles are edge weights on large graphs whose nodes are exchangeable. Such large graphs are known to converge to continuum limits called graphons as their size grow to infinity. We show that the Euclidean gradient flow of a suitable function of the edge-weights converges to a novel continuum limit given by a curve on the space of graphons that can be appropriately described as a gradient flow or, more technically, a curve of maximal slope. Several natural functions on graphons, such as homomorphism functions and the scalar entropy, are covered by our set-up, and the examples have been worked out in detail.
    
[^156]: 走向预测计算的透视主义真实性验证转变

    Toward a Perspectivist Turn in Ground Truthing for Predictive Computing. (arXiv:2109.04270v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.04270](http://arxiv.org/abs/2109.04270)

    本文提出了一种新的范式，数据透视主义，用于机器学习中的知识表示步骤。这种方法整合了人类参与者的观点和角度，相较于传统黄金标准数据集，具有更多潜力和优势。

    

    大多数人工智能应用基于监督式机器学习，而监督式机器学习最终依赖手动注释的数据。注释过程通常以多数票为基础，但最近的研究表明这种方法常常存在问题。本文描述并倡导一种不同的范式，我们称之为数据透视主义，它摆脱了传统的黄金标准数据集，转向采用整合人类参与的角度和观点的方法来进行机器学习过程中的知识表示步骤。借鉴启发我们提议的前人作品，我们描述了我们的提议对于不仅是更主观的任务（例如与人类语言有关的任务），而且对于通常被视为客观的任务（例如医疗决策）的潜力，并介绍了采用透视主义立场在机器学习中的主要优势，以及可能的...

    Most Artificial Intelligence applications are based on supervised machine learning (ML), which ultimately grounds on manually annotated data. The annotation process is often performed in terms of a majority vote and this has been proved to be often problematic, as highlighted by recent studies on the evaluation of ML models. In this article we describe and advocate for a different paradigm, which we call data perspectivism, which moves away from traditional gold standard datasets, towards the adoption of methods that integrate the opinions and perspectives of the human subjects involved in the knowledge representation step of ML processes. Drawing on previous works which inspired our proposal we describe the potential of our proposal for not only the more subjective tasks (e.g. those related to human language) but also to tasks commonly understood as objective (e.g. medical decision making), and present the main advantages of adopting a perspectivist stance in ML, as well as possible d
    
[^157]: CDMA：一种适用于一般极小化问题的实用交叉设备联合学习算法

    CDMA: A Practical Cross-Device Federated Learning Algorithm for General Minimax Problems. (arXiv:2105.14216v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.14216](http://arxiv.org/abs/2105.14216)

    本文提出了一种名为CDMA的交叉设备联合学习算法，用于解决一般极小化问题。该算法利用“一旦有足够的响应，立即开始”的机制，有效地在涉及到不可靠移动/IoT设备的设置中进行聚合，具有实际可行性和广泛的应用前景。

    

    极小化问题涉及到许多重要应用，包括鲁棒对抗学习和生成对抗网络（GAN）训练。最近，在联合学习（FL）范式下解决极小化问题的算法引起了极大的关注。现有的一般极小化问题的联合算法需要在每轮训练中进行完全聚合（即对来自所有客户端的本地模型信息进行聚合）。因此，在涉及到许多不可靠的移动/IoT设备的重要的交叉设备联合学习设置中，这些算法是不适用的。在本文中，我们开发了一种名为CDMA的第一个实用算法，该算法用于解决交叉设备联合学习中的一般极小化问题。CDMA基于“一旦有足够的响应，立即开始”的机制，即服务器首先向一部分客户端发出信号执行本地计算，然后一旦接收到足够多的客户端响应，开始聚合客户端报告的本地结果。

    Minimax problems arise in a wide range of important applications including robust adversarial learning and Generative Adversarial Network (GAN) training. Recently, algorithms for minimax problems in the Federated Learning (FL) paradigm have received considerable interest. Existing federated algorithms for general minimax problems require the full aggregation (i.e., aggregation of local model information from all clients) in each training round. Thus, they are inapplicable to an important setting of FL known as the cross-device setting, which involves numerous unreliable mobile/IoT devices. In this paper, we develop the first practical algorithm named CDMA for general minimax problems in the cross-device FL setting. CDMA is based on a Start-Immediately-With-Enough-Responses mechanism, in which the server first signals a subset of clients to perform local computation and then starts to aggregate the local results reported by clients once it receives responses from enough clients in each 
    
[^158]: 模拟人类和人工情感 (SHArE)的研究

    Simulation of Human and Artificial Emotion (SHArE). (arXiv:2011.02151v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2011.02151](http://arxiv.org/abs/2011.02151)

    这项研究介绍了模拟人类和人工情感的框架，为人类的心理健康问题提供了新的治疗方案，并且为人工智能提供了一种观察机器情感和动机的方法。

    

    模拟人类和人工情感 (SHArE) 的框架描述了情感的架构，可以在心理学、神经科学和人工智能之间进行参数转移。这些参数可以被定义为抽象概念，也可以细化到个体神经元的电压水平。该模型使得可以设计人类的情感轨迹，从而可能为各种心理健康问题提供新的治疗方案。对于人工智能而言，这项工作提供了一种紧凑的表示方法，可以应用于神经网络，以观察机器的情感和动机。

    The framework for Simulation of Human and Artificial Emotion (SHArE) describes the architecture of emotion in terms of parameters transferable between psychology, neuroscience, and artificial intelligence. These parameters can be defined as abstract concepts or granularized down to the voltage levels of individual neurons. This model enables emotional trajectory design for humans which may lead to novel therapeutic solutions for various mental health concerns. For artificial intelligence, this work provides a compact notation which can be applied to neural networks as a means to observe the emotions and motivations of machines.
    
[^159]: 使用机器教学研究教授强化学习者时人类的假设

    Using Machine Teaching to Investigate Human Assumptions when Teaching Reinforcement Learners. (arXiv:2009.02476v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.02476](http://arxiv.org/abs/2009.02476)

    本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设，发现人们假设学习者具有高的折扣率和高度重视探索，并根据学习者进展调整教学策略。

    

    为成功教学，需要对学习者学习方式进行假设，即学习者如何使用来自世界的经验来更新其内部状态。本文研究了在线奖励和惩罚方法下，人们对于学习者的期望假设。研究重点是一种常见的强化学习方法 Q-learning，通过行为实验考察人们的假设。为了达到此目的，我们首先建立了一个规范标准，将问题形式化为机器教学优化问题。为了解决机器教学优化问题，我们使用深度学习逼近方法来模拟学习者在环境中的表现，并学习预测反馈如何影响学习者的内部状态。在教授理想化的探索利用任务时，人们对学习者的学习和折扣率有哪些假设？在行为实验中，我们发现人们可以相对高效和准确地教导 Q-学习者这项任务。人们倾向于假设学习者具有高的折扣率，并高度重视探索。此外，人们会根据学习者的进展调整自己的教学策略。

    Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and 
    
[^160]: 了解具有非对称几何散射变换的图神经网络

    Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms. (arXiv:1911.06253v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1911.06253](http://arxiv.org/abs/1911.06253)

    这项工作介绍了一种具有非对称几何散射变换的图神经网络，通过引入一类非对称小波，它统一和扩展了现有图形散射架构的理论结果，并为未来的深度学习架构为图形提供了基础。

    

    散射变换是一种基于小波的深度学习架构，作为卷积神经网络的模型。最近，有几篇工作引入了散射变换在非欧几里德设置（如图形）中的推广。我们的工作基于这些构造，引入了基于非常一般的非对称小波类的图形窗口化和非窗口化几何散射变换。我们证明了这些非对称图形散射变换与对称散射变换有许多相同的理论保证。因此，提出的构造统一和扩展了现有图形散射架构的已知理论结果。通过这样做，这项工作通过引入大量带有可证明稳定性和不变性保证的网络，有助于弥合几何散射和其他图神经网络之间的差距。这些结果为未来的深度学习架构为图形提供了基础。

    The scattering transform is a multilayered wavelet-based deep learning architecture that acts as a model of convolutional neural networks. Recently, several works have introduced generalizations of the scattering transform for non-Euclidean settings such as graphs. Our work builds upon these constructions by introducing windowed and non-windowed geometric scattering transforms for graphs based upon a very general class of asymmetric wavelets. We show that these asymmetric graph scattering transforms have many of the same theoretical guarantees as their symmetric counterparts. As a result, the proposed construction unifies and extends known theoretical results for many of the existing graph scattering architectures. In doing so, this work helps bridge the gap between geometric scattering and other graph neural networks by introducing a large family of networks with provable stability and invariance guarantees. These results lay the groundwork for future deep learning architectures for g
    

