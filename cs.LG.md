# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks.](http://arxiv.org/abs/2401.03350) | 提出了G-$\Delta$UQ，一种新的训练框架，旨在改善图神经网络（GNN）的内在不确定性估计。该框架通过图锚定策略将随机数据中心化应用于图数据，并且能够支持部分随机的GNN。 |
| [^2] | [Image Inpainting via Tractable Steering of Diffusion Models.](http://arxiv.org/abs/2401.03349) | 本文提出了一种通过可解概率模型精确计算约束后验的方法，然后利用这一信号来引导扩散模型的去噪过程，从而改进图像修复的质量和语义一致性。 |
| [^3] | [An Investigation of Large Language Models for Real-World Hate Speech Detection.](http://arxiv.org/abs/2401.03346) | 本研究调查了大规模语言模型在现实世界中对恶意言论检测的有效性，并发现现有方法在上下文感知方面存在显著限制。而大规模语言模型具有潜力作为上下文感知恶意言论检测的知识库，但目前缺乏有效提示这些模型的方法。 |
| [^4] | [Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection.](http://arxiv.org/abs/2401.03341) | 本论文提出了一种将变分自编码器（VAEs）和自监督学习（SSL）相结合的新颖生成框架，用于解决在时间序列异常检测（TSAD）中由于数据稀缺引起的潜在空间的不连续性导致的重建不稳定性的问题。 |
| [^5] | [A deep learning framework for jointly extracting spectra and source-count distributions in astronomy.](http://arxiv.org/abs/2401.03336) | 这篇论文提出了一种用于在天文学中联合提取光谱和源数分布的深度学习框架。 |
| [^6] | [Walnut Detection Through Deep Learning Enhanced by Multispectral Synthetic Images.](http://arxiv.org/abs/2401.03331) | 本研究提出了一种通过多光谱合成图像增强的深度学习方法进行核桃检测，能够显著提高检测效率。 |
| [^7] | [Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection.](http://arxiv.org/abs/2401.03322) | 本文介绍了一种基于注意力和自编码器的混合模型，用于无监督的在线时间序列异常检测，通过结合注意力和自编码器，并在自编码器的潜在空间中预测下一个时间步骤窗口，提高了模型的准确性。 |
| [^8] | [Comparison of Microservice Call Rate Predictions for Replication in the Cloud.](http://arxiv.org/abs/2401.03319) | 本文通过比较线性回归、多层感知机和梯度提升回归三种机器学习模型在微服务调用率预测方面的性能，发现梯度提升回归模型在减小误差方面表现出色，并且能够准确预测微服务所需的副本数量。 |
| [^9] | [Enhancing Context Through Contrast.](http://arxiv.org/abs/2401.03314) | 本研究提出了一种通过对比学习来提高神经机器翻译性能的新方法，利用巴洛双胞胎损失最大化互信息。与其他方法不同的是，该方法通过上下文增强来提升性能，而不需要明确地增加数据或从头开始学习嵌入。 |
| [^10] | [MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning.](http://arxiv.org/abs/2401.03306) | 本研究提出了一种基于模型的方法，通过模型值扩展和策略正则化，可以在高维领域中有效地进行离线预训练和在线微调的强化学习，解决了离线到在线微调中的分布偏移、离散动力学数据和非稳态奖励问题。 |
| [^11] | [Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT.](http://arxiv.org/abs/2401.03302) | 本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。 |
| [^12] | [On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond.](http://arxiv.org/abs/2401.03301) | 本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。 |
| [^13] | [Autonomous Navigation in Complex Environments.](http://arxiv.org/abs/2401.03267) | 本文研究了在复杂环境中应用CNN-DNN网络融合进行自主导航的方法，通过模仿学习使用LiDAR和摄像头数据对机器人进行训练，并通过Monte-Carlo测试了其鲁棒性。 |
| [^14] | [Large Language Models as Visual Cross-Domain Learners.](http://arxiv.org/abs/2401.03253) | 本研究提出了大型语言模型作为视觉跨领域学习器（LLaVO），通过将图像转换为文本描述，使用大型语言模型进行训练和微调，实现了在跨领域任务中减少领域转移的效果。 |
| [^15] | [TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in End-to-End ASR.](http://arxiv.org/abs/2401.03251) | 本论文提出了一种用于估计端到端ASR信任度的时态词元相似度分数TeLeS，并用缩减损失来解决CEM训练中目标得分数据不平衡的问题。 |
| [^16] | [SeqNAS: Neural Architecture Search for Event Sequence Classification.](http://arxiv.org/abs/2401.03246) | SeqNAS是一种针对事件序列分类的神经架构搜索算法，通过引入一种简单而表达力强的搜索空间，利用常用的构建块进行搜索，提供高质量的任务特定解决方案。 |
| [^17] | [Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization.](http://arxiv.org/abs/2401.03240) | 本文提出了一种无学习率的自适应梯度方法，通过参数缩放解释自适应梯度方法，扩展了无学习率方法的适用性并增强了使用自适应梯度方法进行训练的效果。 |
| [^18] | [Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices.](http://arxiv.org/abs/2401.03233) | 本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。 |
| [^19] | [FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced Contrastive Learning for Data and Model Heterogeneity in Federated Learning.](http://arxiv.org/abs/2401.03230) | FedTGP 是一种用于解决联邦学习中数据和模型异质性问题的方法，通过引入自适应边缘增强对比学习 (ACL) 来学习可训练的全局原型 (TGP)，从而提高了原型的可分离性和语义含义。 |
| [^20] | [Reflected Schr\"odinger Bridge for Constrained Generative Modeling.](http://arxiv.org/abs/2401.03228) | 本研究提出了一种用于在有界域内生成数据的熵正则化最优输运方法，通过推导出带有边界条件的反射正向-反向随机微分方程，解决了反射扩散模型在适应多样性领域时的限制。 |
| [^21] | [End-to-End Anti-Backdoor Learning on Images and Time Series.](http://arxiv.org/abs/2401.03215) | 本文提出了一种名为端到端反后门学习（E2ABL）的方法，该方法可以在图像和时间序列数据上进行鲁棒训练以对抗后门攻击。与传统的反后门学习（ABL）方法不同，E2ABL通过额外的分类头在深度神经网络（DNN）的浅层进行端到端训练，并有效地鉴别深层特征空间中干净样本和污染样本之间的边界。 |
| [^22] | [Understanding Representation Learnability of Nonlinear Self-Supervised Learning.](http://arxiv.org/abs/2401.03214) | 这篇论文是第一次准确分析非线性自监督学习模型的学习结果，通过使用梯度下降算法训练模型并证明其收敛到局部最小值，在理解数据表示可学习性方面做出了重要贡献。 |
| [^23] | [A Robbins--Monro Sequence That Can Exploit Prior Information For Faster Convergence.](http://arxiv.org/abs/2401.03206) | 提出了一种新的方法，通过引入先验信息，改进了Robbins-Monro算法的收敛速度。结果表明，先验信息的Robbins-Monro序列比标准序列收敛更快，特别是在前几步。 |
| [^24] | [Learning-Augmented K-Means Clustering Using Dimensional Reduction.](http://arxiv.org/abs/2401.03198) | 本论文提出了一种使用主成分分析（PCA）降低数据集维度的学习增强K-Means聚类算法，该算法能够在k值为10和25时获得较低的聚类误差。 |
| [^25] | [Decision Making in Non-Stationary Environments with Policy-Augmented Search.](http://arxiv.org/abs/2401.03197) | 在非稳定环境下的决策制定是一个具有挑战性的问题，本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。 |
| [^26] | [Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features.](http://arxiv.org/abs/2401.03195) | 我们提出了一种使用迁移学习和时空特征的比特率和复杂度高效预测方法，通过利用预训练深度神经网络的特征图和预测的最小比特率来改进比特率梯度的效率，从而在视频行业中提供高质量视频并保持比特率的效率。 |
| [^27] | [Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis.](http://arxiv.org/abs/2401.03194) | 本论文提出了一种通过拓扑数据分析学习动态网络中的持久社区结构的方法。通过引入最小化拓扑变化的概念，提出了一种深度图聚类框架，并引入了神经网络正则化方法，用于确保时间一致性和聚类准确性。 |
| [^28] | [On the Convergence of Hermitian Dynamic Mode Decomposition.](http://arxiv.org/abs/2401.03192) | 研究了Hermitian动态模态分解(DMD)对自伴随Koopman算子的谱性质的收敛性，通过建立了关于谱测度收敛性的一般定理，证明了HDMD的特征值和特征函数在适当条件下收敛到基础Koopman算子的谱性质。 |
| [^29] | [Part-of-Speech Tagger for Bodo Language using Deep Learning approach.](http://arxiv.org/abs/2401.03175) | 本文介绍了使用深度学习方法的Bodo语词性标注器。首先，我们提出了Bodo语言模型BodoBERT，这项工作是第一个为Bodo开发的语言模型。其次，我们提出了基于深度学习的Bodo词性标注模型，该模型利用了BiLSTM、CRF和BodoBERT与BytePairEmbeddings的组合。尽管研究已经在资源丰富的语言中进行了大量的语言模型和词性标注模型的研究，但对于低资源语言如Bodo，仍然缺乏相关研究。 |
| [^30] | [UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis.](http://arxiv.org/abs/2401.03173) | UGGNet通过结合U-Net和VGG架构，提供了一种全面解决方案来增强乳腺超声图像分析的性能，从而实现了更准确的乳腺癌诊断。 |
| [^31] | [Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues.](http://arxiv.org/abs/2401.03171) | 本研究使用青少年人口普查数据来预测抑郁风险，关注儿童对抑郁的经历和日常生活情况。 |
| [^32] | [Preserving Silent Features for Domain Generalization.](http://arxiv.org/abs/2401.03170) | 本研究发现，在领域泛化(DG)设置中，自监督模型不如有监督模型在泛化性能上表现得好。我们提出了静默特征的概念，认为保留这些静默特征可以降低测试领域风险。 |
| [^33] | [An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning for Stochastic Environments.](http://arxiv.org/abs/2401.03163) | 本文针对利用Q值与效用函数扩展传统Q学习解决多目标强化学习问题的方法，在随机环境中存在问题。通过实证研究核心算法变体和奖励工程方法，发现噪声Q值估计问题对算法的稳定性和收敛性有关键影响。 |
| [^34] | [QoS-Aware Graph Contrastive Learning for Web Service Recommendation.](http://arxiv.org/abs/2401.03162) | 本研究提出了一种名为QoS感知的图对比学习（QAGCL）的新方法，通过构建具有地理位置信息和随机性的上下文增强图来解决网络服务推荐中的数据稀疏性和冷启动问题，并有效提高推荐准确性。 |
| [^35] | [Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving.](http://arxiv.org/abs/2401.03160) | 本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。 |
| [^36] | [Distributed client selection with multi-objective in federated learning assisted Internet of Vehicles.](http://arxiv.org/abs/2401.03159) | 本文提出了一个分布式客户端选择方案，以减少车联网中保持活动状态的成本。通过选择邻居中评估最高的客户端，考虑样本数量、吞吐量、计算能力和本地数据集质量，我们采用模糊逻辑作为评估器。仿真结果显示我们的提议接近c |
| [^37] | [Data-Dependent Stability Analysis of Adversarial Training.](http://arxiv.org/abs/2401.03156) | 本文提出了基于随机梯度下降的对抗训练的泛化界限，包含了数据分布的信息，通过研究数据分布和对抗预算的变化对鲁棒泛化差距的影响，得到了与均匀稳定性相当的泛化界限。 |
| [^38] | [Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents.](http://arxiv.org/abs/2401.03154) | 该论文介绍了一种处理多目标跟踪的分散式多智能体算法，该算法能在智能体数量少于目标数量时实现主动搜索和跟踪，并使用异步智能体通信来协调动作。 |
| [^39] | [Controllable Image Synthesis of Industrial Data Using Stable Diffusion.](http://arxiv.org/abs/2401.03152) | 该论文提出了一种新的方法，使用稳定扩散技术在工业数据上实现可控的图像合成，从而允许生成自标记的有缺陷的图像。这种方法可以针对工业数据训练通用的预训练生成模型，并实现生成满足特定拓扑特征和几何缺陷位置要求的工业图像。 |
| [^40] | [Semi-supervised learning via DQN for log anomaly detection.](http://arxiv.org/abs/2401.03151) | 本文提出了一种半监督的日志异常检测方法，命名为DQNLog，通过结合深度强化学习中的DQN算法，利用少量有标记的数据和大规模无标记的数据集，有效解决了数据不平衡和标记数量有限的问题，并且通过与异常环境交互和主动探索无标记的数据集，学习已知的异常并发现未知的异常。 |
| [^41] | [Fair Sampling in Diffusion Models through Switching Mechanism.](http://arxiv.org/abs/2401.03140) | 本论文提出了一种称为“属性切换”的公平抽样机制，用于解决扩散模型中公平性的问题。通过在生成的数据中混淆敏感属性，该方法能够实现生成公平数据和保持数据效用的目标。 |
| [^42] | [TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation and Prediction via Multifaceted Graph Modeling.](http://arxiv.org/abs/2401.03138) | 这篇论文提出了使用地理单元交通流数据来改进交通评估和预测的方法。通过结合多变量、时间和空间方面的图神经网络，该方法在长期预测中取得了优越的准确率，并展示了将地理单元交通流整合到交通系统中的潜力。 |
| [^43] | [SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning.](http://arxiv.org/abs/2401.03137) | SPQR论文介绍了一种使用尖峰随机模型来控制强化学习中Q-集合的独立性的方法，通过引入基于随机矩阵理论的正则化损失来克服过高估计偏差。 |
| [^44] | [TimeGraphs: Graph-based Temporal Reasoning.](http://arxiv.org/abs/2401.03134) | TimeGraphs是一种基于图的时间推理方法，通过将动态交互建模为分层时间图，实现了对不同时间尺度的自适应推理。 |
| [^45] | [Vision Transformers and Bi-LSTM for Alzheimer's Disease Diagnosis from 3D MRI.](http://arxiv.org/abs/2401.03132) | 本研究提出了使用Vision Transformers和Bi-LSTM处理MRI图像进行阿尔茨海默病诊断的方法，通过提取特征并保持特征之间的依赖关系，该方法在AD的诊断中取得了良好的准确性和性能。 |
| [^46] | [A Physics-guided Generative AI Toolkit for Geophysical Monitoring.](http://arxiv.org/abs/2401.03131) | EdGeo工具包利用物理原理指导的扩散模型生成高保真度的地下速度图，并通过使用声波方程生成地震波形数据来改善模型修剪后的ML模型性能。 |
| [^47] | [A least distance estimator for a multivariate regression model using deep neural networks.](http://arxiv.org/abs/2401.03123) | 我们提出了一种基于深度神经网络的最小距离估计器，用于多元回归问题，可以灵活建模线性和非线性条件均值函数，并具有更好的捕捉依赖关系和鲁棒性，同时考虑了变量选择在分析高维数据中的重要性。 |
| [^48] | [Advancing DDoS Attack Detection: A Synergistic Approach Using Deep Residual Neural Networks and Synthetic Oversampling.](http://arxiv.org/abs/2401.03116) | 本研究通过结合深度残差神经网络和合成过采样技术，提出了一种增强的DDoS攻击检测方法，能够更准确地辨别复杂的攻击模式。 |
| [^49] | [GLISP: A Scalable GNN Learning System by Exploiting Inherent Structural Properties of Graphs.](http://arxiv.org/abs/2401.03114) | GLISP是一种可扩展的GNN学习系统，通过利用图的结构属性来解决在工业规模图上应用GNN时遇到的可扩展性和性能问题。 |
| [^50] | [When To Grow? A Fitting Risk-Aware Policy for Layer Growing in Deep Neural Networks.](http://arxiv.org/abs/2401.03104) | 本研究发现神经增长具有正则化效果，但现有方法未考虑这一效果。基于此，我们提出了一种风险感知的增长时机策略，以应对欠拟合和过拟合风险。 |
| [^51] | [Adaptive Boosting with Fairness-aware Reweighting Technique for Fair Classification.](http://arxiv.org/abs/2401.03097) | 该论文提出了一种公平AdaBoost（FAB）方法，通过公平感知的基分类器重加权技术，实现了在保持AdaBoost优势的同时实现公平分类，而对预测性能几乎没有牺牲。 |
| [^52] | [Consensus-Threshold Criterion for Offline Signature Verification using Convolutional Neural Network Learned Representations.](http://arxiv.org/abs/2401.03085) | 本论文提出了一种离线签名验证的一致阈值准则，通过使用卷积神经网络模型提取的特征，可以减少伪造签名的误识率。 |
| [^53] | [Energy-efficient Decentralized Learning via Graph Sparsification.](http://arxiv.org/abs/2401.03083) | 本文通过图稀疏化的方法，优化了混合矩阵，以提高去中心化学习的能效。在特殊和一般情况下，分别提出了有保证性能和贪心启发式算法的解决方案，并在仿真实验中验证了其有效性。 |
| [^54] | [StreamVC: Real-Time Low-Latency Voice Conversion.](http://arxiv.org/abs/2401.03078) | StreamVC是一种实时低延迟语音转换解决方案，可以在移动平台上进行实时通信，并实现声音匿名化。它保留了源语音的内容和韵律，同时匹配了目标语音的音质，并且通过软语音单元的学习和提供白化的基频信息来改善音高稳定性。 |
| [^55] | [A Topology-aware Graph Coarsening Framework for Continual Graph Learning.](http://arxiv.org/abs/2401.03077) | 提出了一种面向拓扑感知的连续图学习框架TA$\mathbb{CO}$，通过图减粗和连续学习的方法，解决了传统连续学习策略在处理流式图时的低效率和无法捕捉任务关联性的问题。 |
| [^56] | [Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study.](http://arxiv.org/abs/2401.03069) | 本研究旨在提高深度学习Bug的可复现性，通过构建数据集和确定编辑动作和有用信息，这能够解决目前研究中忽视的问题。 |
| [^57] | [CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution.](http://arxiv.org/abs/2401.03065) | CRUXEval是一个包含800个Python函数的代码推理、理解和执行的基准测试。通过评估二十个代码模型，发现许多在HumanEval上得分高的模型在该基准测试上没有相同的改进。使用CoT的GPT-4展现了最佳性能，但仍未解决问题。与开源模型相比，闭源模型的性能差距更大。 |
| [^58] | [Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach.](http://arxiv.org/abs/2401.03059) | 本研究提出了一种基于神经上下文强化学习方法的QoS-aware UE接入控制策略，旨在在关联URRLC UE与小区之前，准确估计QoS并避免小区过载。 |
| [^59] | [Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate.](http://arxiv.org/abs/2401.03058) | 本论文提出了一种新的子空间立方正则化牛顿方法，可以在解决凸优化问题时实现无维度相关的全局收敛速度，通过在低维子空间上进行二阶更新，克服了高维问题中内存需求和计算成本大的问题。 |
| [^60] | [On the Convergence of Semi Unsupervised Calibration through Prior Adaptation Algorithm.](http://arxiv.org/abs/2401.03051) | 本文研究了一种半无监督校准算法，使用动力系统的观点证明了该算法的收敛性质，并通过实验验证了所提出的结果。 |
| [^61] | [AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis.](http://arxiv.org/abs/2401.03040) | 本文介绍了AccidentGPT，这是一个用于交通事故分析的大型多模态基础模型。它利用多模态输入数据自动重建事故过程视频，并提供多模态输出的多任务分析。此外，AccidentGPT还采用了多模态提示与反馈、混合训练模式和边缘-云分割配置以增强性能，并提出了一些研究机会。 |
| [^62] | [The Rise of Diffusion Models in Time-Series Forecasting.](http://arxiv.org/abs/2401.03006) | 本文调查了扩散模型在时间序列预测中的应用，提供了对这些模型的全面背景信息和详细说明，同时也对它们在不同数据集上的有效性和彼此之间的比较进行了分析。其贡献包括对扩散模型在时间序列预测中应用的彻底探索和按时间顺序排序的模型概述。这是一份对人工智能和时间序列分析领域的研究人员来说具有价值的资源。 |
| [^63] | [AST-T5: Structure-Aware Pretraining for Code Generation and Understanding.](http://arxiv.org/abs/2401.03003) | AST-T5是一种结构感知的预训练模型，通过利用抽象语法树（AST）来增强代码生成、转换和理解的能力。它优于其他同等大小的语言模型，并在代码到代码任务中表现出色。 |
| [^64] | [UnetTSF: A Better Performance Linear Complexity Time Series Prediction Model.](http://arxiv.org/abs/2401.03001) | UnetTSF是一种具有线性复杂度的U-Net时间序列预测模型，通过使用FPN技术提取特征和设计适用于时间序列数据的融合结构，相比于DLiner和PatchTST模型，在多个测试项目中取得了更好的预测结果。 |
| [^65] | [Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition.](http://arxiv.org/abs/2401.03000) | 本文介绍了一种创新方法，用于将多模态情感识别转化为更实用且资源高效的单模态、仅语音的情感识别。使用知识蒸馏和屏蔽训练技术来解决现有模型所依赖的多模态输入在实际应用中可能不可行的问题。 |
| [^66] | [An AI-enabled Bias-Free Respiratory Disease Diagnosis Model using Cough Audio: A Case Study for COVID-19.](http://arxiv.org/abs/2401.02996) | 本研究提出了一种名为Bias Free Network (RBFNet) 的端到端解决方案，通过减轻混淆变量的影响，确保准确且无偏倚的呼吸系统疾病诊断特征。通过将COVID-19数据集纳入研究，强调了该模型的相关性。 |
| [^67] | [GLIDE-RL: Grounded Language Instruction through DEmonstration in RL.](http://arxiv.org/abs/2401.02991) | GLIDE-RL是一种基于演示的强化学习算法，通过引入教师-指导员-学生的课程学习框架，训练出了一个能够遵循自然语言指令并且可以推广到未见指令的RL代理。 |
| [^68] | [On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning.](http://arxiv.org/abs/2401.02989) | 该论文研究了在多物种神经网络中使用伪缺失的选择和效果。作者指出物种出现和伪缺失之间的类别不平衡问题经常被忽视，不同类型的伪缺失选择对结果产生了复杂影响。 |
| [^69] | [A Surrogate-Assisted Extended Generative Adversarial Network for Parameter Optimization in Free-Form Metasurface Design.](http://arxiv.org/abs/2401.02961) | 本文提出了一种基于代理的扩展生成对抗网络（XGAN），用于加速和优化自由形态元表面设计。XGAN通过物理约束准确地生成元表面，并且在实验中表现出了较高的准确度和速度。 |
| [^70] | [Fairness-Aware Job Scheduling for Multi-Job Federated Learning.](http://arxiv.org/abs/2401.02740) | 本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。 |
| [^71] | [A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning.](http://arxiv.org/abs/2401.02325) | 这篇论文提出了一种鲁棒的分位数Huber损失函数，在分布强化学习中通过捕捉噪声并调整参数来增强对异常值的鲁棒性。实证测试验证了该方法的有效性。 |
| [^72] | [View-based Explanations for Graph Neural Networks.](http://arxiv.org/abs/2401.02086) | 这篇论文提出了一种基于视图的解释方法来解释图神经网络(GNNs)的行为，通过生成解释视图和图模式来解释特定类别的结果。 |
| [^73] | [IoT in the Era of Generative AI: Vision and Challenges.](http://arxiv.org/abs/2401.01923) | 在生成式人工智能时代的物联网，Generative AI的进展带来了巨大的希望，同时也面临着高资源需求、及时工程、设备端推理、安全等关键挑战。 |
| [^74] | [SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks.](http://arxiv.org/abs/2401.01625) | SCALA是一种使用稀疏化的对比学习框架，用于在属性网络中进行异常检测。该框架旨在改善网络的嵌入质量，并通过引入稀疏化来为每个节点提供一种新的衡量异常得分的方法。 |
| [^75] | [Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review.](http://arxiv.org/abs/2401.01519) | 本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。 |
| [^76] | [Token Propagation Controller for Efficient Vision Transformer.](http://arxiv.org/abs/2401.01470) | 本文提出一种新颖的令牌传播控制器（TPC），通过结合暂停概率和重新开始概率，实现了对令牌的减少和重复利用的控制，从而提高了视觉Transformer的效率和令牌利用率。 |
| [^77] | [Scalable network reconstruction in subquadratic time.](http://arxiv.org/abs/2401.01404) | 这篇论文提出了一个可扩展的网络重建算法，能够在次二次时间内实现结果，通过随机的二阶邻居搜索产生最佳的边候选。 |
| [^78] | [Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and Stochastic root finding.](http://arxiv.org/abs/2401.01393) | 本文介绍了回溯新的Q-Newton方法（BNQN），该方法是Newton方法的一种变体，具有强大的理论保证，易于实现，且在实验中有良好的表现。通过实验，发现BNQN在多项式和亚纯函数的根搜索中具有更平滑的吸引域，并通过与Newton流和Voronoi图的联系提出了一些挑战性的问题。此外，在面对随机扰动时，BNQN比Newton方法和随机松弛Newton方法更具鲁棒性。 |
| [^79] | [Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data.](http://arxiv.org/abs/2401.01383) | 我们提出了一种使用联邦多轨迹GNN的方法，通过稀缺数据预测婴儿脑连接性。通过联邦学习，我们通过聚合多个医院的本地学习结果来提高模型性能，同时保护数据隐私。 |
| [^80] | [Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces.](http://arxiv.org/abs/2401.01306) | 本文提出了在无限维希尔伯特空间中运用深度学习实现罚函数法和增广拉格朗日法的约束优化算法，并在玩具问题上进行了测试，证明这两种方法都能够产生不错的近似解。在约束函数本身是函数的情况下，通过拉格朗日乘子更新规则的计算优势，实现了显著的加速。 |
| [^81] | [A Comprehensive Study of Knowledge Editing for Large Language Models.](http://arxiv.org/abs/2401.01286) | 本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。 |
| [^82] | [SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models.](http://arxiv.org/abs/2401.00793) | SecFormer是一个优化框架，旨在实现Transformer模型的快速准确隐私保护推理。通过消除高成本的指数和线性操作，SecFormer能够有效解决在大型语言模型中应用SMPC时的性能问题。 |
| [^83] | [Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect.](http://arxiv.org/abs/2401.00422) | 这篇论文从理论和实证分析的角度深入研究了维度诅咒的两个主要原因——距离集中和流形效应，并通过实验证明了使用Minkowski距离进行最近邻搜索（NNS）在高维数据中取得了最佳性能。 |
| [^84] | [TSPP: A Unified Benchmarking Tool for Time-series Forecasting.](http://arxiv.org/abs/2312.17100) | 该论文提出了TSPP，一种针对时间序列预测的统一基准测试工具，以解决各种设置下不同方法之间的比较困难。通过该框架，可以无缝集成模型和数据集，帮助从业者和研究人员进行开发工作。研究表明，精心实施的深度学习模型可以与需要大量特征工程和专家知识的梯度提升决策树相媲美，而只需付出最小的努力。 |
| [^85] | [Preference as Reward, Maximum Preference Optimization with Importance Sampling.](http://arxiv.org/abs/2312.16430) | 本文提出了一种使用重要性抽样进行最大偏好优化的算法，该算法通过直接优化生成策略来消除对奖励模型的需求，提高了数据利用率和稳定性，并通过解决KL正则化问题来改善偏好学习效果。 |
| [^86] | [Revisiting Knowledge Distillation under Distribution Shift.](http://arxiv.org/abs/2312.16242) | 本文通过重新制定目标函数，在分布偏移情况下重新审视了知识蒸馏的范式。使用统一的框架，对多样性偏移和相关性偏移进行了基准评估，并揭示了在分布偏移下教学性能较差的现象。 |
| [^87] | [Deformable Audio Transformer for Audio Event Detection.](http://arxiv.org/abs/2312.16228) | 该论文提出了一种新颖的可变形音频Transformer，命名为DATAR，用于音频事件检测。通过构建一个具有金字塔Transformer骨架的可变形注意力，该模型在预测任务中取得了有效的结果。研究还发现可变形注意力图计算可能过于简化输入特征，提出了进一步增强特征的方法。 |
| [^88] | [Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling.](http://arxiv.org/abs/2312.15195) | 本文提出了一个用于按需共乘车辆派遣的框架，利用互信息作为强化学习智能体的内在奖励，以解决现有算法中只考虑收入最大化而无法满足异常分布请求的问题。 |
| [^89] | [Efficient Asynchronous Federated Learning with Sparsification and Quantization.](http://arxiv.org/abs/2312.15186) | 本论文提出了一种高效异步稀疏化量化联邦学习方法（TEASQ-Fed），利用边缘设备的并行参与，解决了传统方法中设备拖慢训练和通信瓶颈的问题。 |
| [^90] | [Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience.](http://arxiv.org/abs/2312.14260) | 本研究介绍了一种新的框架，将对抗训练和水印技术相结合，用于提高模型的弹性，防御规避攻击，并在知识产权盗窃案件中提供确凿的模型验证。 |
| [^91] | [Distributed Quantum Neural Networks via Partitioned Features Encoding.](http://arxiv.org/abs/2312.13650) | 本论文提出了一种使用分布式量子神经网络的方法，通过分割特征并在多个小型量子神经网络上进行期望值集合来生成预测。通过实验验证，该方法可能在分类任务中优于单一的量子神经网络。 |
| [^92] | [Unlocking Pre-trained Image Backbones for Semantic Image Synthesis.](http://arxiv.org/abs/2312.13314) | 本文提出了一种新的GAN鉴别器类别，利用预训练的图像分类任务的特征骨干网络生成高度逼真的图像，并引入了更好的上下文建模和交叉注意力技术，生成更多样化的图像。 |
| [^93] | [Continual Learning: Forget-free Winning Subnetworks for Video Representations.](http://arxiv.org/abs/2312.11973) | 本研究基于"彩票票据假设"，提出了一种连续学习方法，通过利用稀疏子网络和FSO进行任务增量学习、少样本类增量学习和视频增量学习，实现高效学习和有效的权重重用。 |
| [^94] | [Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency.](http://arxiv.org/abs/2312.11509) | 这个论文介绍了一种基于强化学习的系统，该系统可以根据患者言语不流畅程度自动调整药物，通过对药物组合的强化学习算法的优化，能够收敛到良好的用药方案。 |
| [^95] | [SAME: Sample Reconstruction against Model Extraction Attacks.](http://arxiv.org/abs/2312.10578) | SAME是一种防御模型提取攻击的新方法，基于样本重建的概念，无需额外的数据集和模型访问，并且具有更实用的保护能力。 |
| [^96] | [Do Bayesian Neural Networks Improve Weapon System Predictive Maintenance?.](http://arxiv.org/abs/2312.10494) | 本研究旨在通过实现贝叶斯推断过程对高可靠性武器系统的故障时间进行建模，以改善预测性维护。通过分析和标准分类度量指标的基准测试，我们的方法LaplaceNN在合成和实际数据集上展现了良好的性能。 |
| [^97] | [Point-of-Care Real-Time Signal Quality for Fetal Doppler Ultrasound Using a Deep Learning Approach.](http://arxiv.org/abs/2312.09433) | 这项研究提出了一种基于深度学习的框架，用于提高胎儿多普勒数据的质量，并设计了一个实时信号质量评估指标，可以即时提供反馈和纠正数据源。 |
| [^98] | [Breaking the Silence: the Threats of Using LLMs in Software Engineering.](http://arxiv.org/abs/2312.08055) | 本文讨论了在软件工程中使用LLMs的潜在威胁，包括闭源模型、数据泄漏和研究结果的可重复性，并提出了一套针对软件工程研究人员和语言模型提供商的指南来减轻这些担忧。 |
| [^99] | [Safe Multi-Task Bayesian Optimization.](http://arxiv.org/abs/2312.07281) | 这项研究将鲁棒的高斯过程均匀误差界限扩展到多任务设置中，以解决安全在线优化中超参数未知的问题。 |
| [^100] | [Mixture-of-Linear-Experts for Long-term Time Series Forecasting.](http://arxiv.org/abs/2312.06786) | MoLE是一种混合线性专家模型，通过训练多个线性中心模型和一个路由模型，能够适应时间序列模式的周期性变化，并显著降低了预测误差。 |
| [^101] | [Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay.](http://arxiv.org/abs/2312.06710) | 本论文提出了一个类原型条件扩散模型（CPDM）来解决Continual Learning中的灾难性遗忘问题。CPDM通过提高生成器的图像质量，减少了分类器的灾难性遗忘风险。 |
| [^102] | [Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images.](http://arxiv.org/abs/2312.06454) | 本文介绍了一种使用点变换器结合联邦学习的方法，用于从嗪和嘧啶法染色的全切片图像中预测乳腺癌HER2状态。该方法通过引入动态标签分布策略和辅助分类器，解决了联邦学习中的标签不平衡和利用局部上下文信息和长程依赖性的问题。 |
| [^103] | [Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference.](http://arxiv.org/abs/2312.05910) | 这篇论文介绍了一种将集合卡尔曼滤波引入变分推理框架的方法，用于近似高斯过程状态空间模型的后验分布，并且有效地利用了潜在状态和动力学之间的依赖关系，减少了变分参数的数量。 |
| [^104] | [Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment.](http://arxiv.org/abs/2312.03020) | 本研究基于MobileNetV2模型，使用1576张超声图像构建的数据集对乳腺癌肿瘤进行分类。该模型在正常、良性、恶性的分类准确率、精确率、召回率、ROC-AUC、PR-AUC和MCC分别达到0.82、0.83、0.81、0.94、0.88和0.74。研究还改进了图像强度分布和错误分类，并提供了一个可推广的模型，展示了MobileNetV2在医学成像中的潜力，为肿瘤诊断的精度提供了改进。另外，论文还探讨了基于Streamlit的实时肿瘤分类部署，为未来的癌症研究设立了一个基准。 |
| [^105] | [FaultFormer: Pretraining Transformers for Adaptable Bearing Fault Classification.](http://arxiv.org/abs/2312.02380) | 本论文提出了一个使用预训练Transformer模型进行适应性轴承故障分类的框架。通过研究不同的标记分割和数据增强策略，该方法在稀缺数据环境中能够达到最先进的准确率，并在微调时改善了性能。 |
| [^106] | [Calibration-free online test-time adaptation for electroencephalography motor imagery decoding.](http://arxiv.org/abs/2311.18520) | 本研究探索了在线测试时间自适应（OTTA）的概念，无需校准且保护隐私，以实现无监督的连续自适应模型运行。研究使用了脑电运动想象解码任务，并通过轻量级架构和不同的OTTA技术来提高准确率。 |
| [^107] | [An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance.](http://arxiv.org/abs/2311.17552) | 本文提出了一种高效光照不变的老虎检测框架，采用了EnlightenGAN和YOLOv8模型，实现了61%的mAP得分，并通过光照增强提高了0.7%的mAP。这些方法在ATRW数据集上的表现超过了当前的最新性能。 |
| [^108] | [Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE.](http://arxiv.org/abs/2311.16167) | 这项工作提出了一种基于移动网格PDE的移动采样物理信息神经网络(MMPDE-Net)，通过解决移动网格PDE来自适应生成新的采样点，并且结合物理信息神经网络（PINN）提出了移动采样PINN（MS-PINN）的框架。数值实验验证了MS-PINN相对于PINN的性能改善。 |
| [^109] | [Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study.](http://arxiv.org/abs/2311.15051) | 本研究通过实验证明，带有大学习率和学习率预热的动量梯度下降显示出大型弹射效应，将迭代朝着比梯度下降发现的更平缓的极小值方向推进。 |
| [^110] | [Open Set Dandelion Network for IoT Intrusion Detection.](http://arxiv.org/abs/2311.11249) | 本文提出了一种基于无监督异构领域适应的开放集dandelion网络（OSDN）用于物联网入侵检测，该模型通过从知识丰富的源网络入侵领域进行入侵知识传输，以实现更准确的入侵检测。在开放集设置下，它能够检测到在源领域中未观测到的新兴目标领域入侵。 |
| [^111] | [Asynchronous Local Computations in Distributed Bayesian Learning.](http://arxiv.org/abs/2311.03496) | 这篇论文提出了分布式贝叶斯学习中的异步本地计算方法，通过使用八卦异步通信来减少通信开销，同时利用快速计算实现协作学习未知参数。 |
| [^112] | [Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design.](http://arxiv.org/abs/2311.03489) | 我们提出了一种利用高级综合和大型语言模型生成硬件设计的方法，通过案例研究验证了其功能和质量，并记录了所有相关的工具和结果。我们相信这一方法将在应用特定集成电路设计中产生革命性影响。 |
| [^113] | [Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models.](http://arxiv.org/abs/2310.19802) | 本文将生成式机器学习问题视为参数概率模型的时间演化过程，通过研究模型参数与生成样本之间的热力学交换，发现模型通过耗散热量来学习，参数子系统充当热库存储学到的信息。这为超参数模型的泛化能力提供了有价值的热力学洞察。 |
| [^114] | [Differentially Private Permutation Tests: Applications to Kernel Methods.](http://arxiv.org/abs/2310.19043) | 本文提出了差分隐私排列检验的框架，扩展了经典的非私有排列检验，以在私有环境中保持有限样本有效性和差分隐私性质。该检验的功率取决于检验统计量的选择，并建立了一般条件来保证一致性和非渐进均匀的功率。 |
| [^115] | [Boosting Data Analytics With Synthetic Volume Expansion.](http://arxiv.org/abs/2310.17848) | 本文介绍了一种利用合成数据生成框架来提升数据分析的方法，在此方法中，使用先进模型生成高逼真度的合成数据，并采用统计方法进行分析。研究发现，在合成数据上的统计方法错误随着合成数据的增加而减少，但最终可能会增加或停滞。 |
| [^116] | [Cross-modal Active Complementary Learning with Self-refining Correspondence.](http://arxiv.org/abs/2310.17468) | 本文提出了一种跨模态主动互补学习框架（CRCL），通过使用新颖的主动互补损失（ACL）和高效的自我完善对应关系修正（SCC），改善了现有方法的鲁棒性。 |
| [^117] | [Kiki or Bouba? Sound Symbolism in Vision-and-Language Models.](http://arxiv.org/abs/2310.16781) | 这项研究通过调查视觉与语言模型中的内在知识，发现它们显示了声音象征性的模式，进一步证实声音和意义之间的相关性在跨模态关联中得到了体现。 |
| [^118] | [Learning to (Learn at Test Time).](http://arxiv.org/abs/2310.13807) | 本论文提出了一种学习在测试时进行学习的方法，通过两层嵌套循环对监督学习问题进行重新定义，并使用内循环进行自监督学习，最终的预测结果得到改进。该方法在图像分类任务上表现优越。 |
| [^119] | [What is a good question? Task-oriented asking with fact-level masking.](http://arxiv.org/abs/2310.11571) | 本论文提出了基于任务的询问（TOA）的概念和框架，介绍了一种用于生成对推理任务有用答案的问题的方法。同时还提出了一种事实级遮蔽（FLM）的技术，用于将自然语言数据集转换为自我监督的TOA数据集。 |
| [^120] | [Federated Multi-Objective Learning.](http://arxiv.org/abs/2310.09866) | 本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。 |
| [^121] | [Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift.](http://arxiv.org/abs/2310.07535) | 在协变量转移下，我们提出了一种新的损失函数和表示匹配损失来优化模型的准确性和公平性，通过实验证明在公平性和准确性权衡方面优于其他基线算法，并且提出了一种未经研究的非对称协变量转移设置。 |
| [^122] | [Higher-Order DeepTrails: Unified Approach to *Trails.](http://arxiv.org/abs/2310.04477) | 本论文提出了一种用于分析人类行为的统一方法，通过使用自回归语言模型来捕捉序列中的高阶依赖关系，以改进Web浏览或交通导航等应用的底层基础设施或用户界面。 |
| [^123] | [ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers.](http://arxiv.org/abs/2310.02489) | 本文提出了一种名为ResidualTransformer的方法，通过重新参数化Transformer编码器层之间的模型权重，将模型的大小减小。实验结果表明，ResidualTransformer的性能优于传统Transformer模型，且模型大小得到了显著减小。 |
| [^124] | [Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models.](http://arxiv.org/abs/2310.01119) | 通过对大型语言模型进行微调，在低资源环境中可以通过合成数据生成来改善较小模型的性能，显著提高了下游模型的性能。 |
| [^125] | [Online Sensitivity Optimization in Differentially Private Learning.](http://arxiv.org/abs/2310.00829) | 本文提出了一种在线敏感度优化方法，通过建立剪切阈值和成本函数之间的关系来避免在差分隐私学习中产生的隐私开销，并实现了动态调整剪切阈值的功能。 |
| [^126] | [Bayesian Design Principles for Frequentist Sequential Learning.](http://arxiv.org/abs/2310.00806) | 该论文提出了一种贝叶斯设计原则用于优化频率式序贯学习问题的通用理论，通过生成“算法信念”并使用贝叶斯后验进行决策，实现了无先验的贝叶斯类型算法在对抗性环境中的泛化和优化应用。 |
| [^127] | [USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models.](http://arxiv.org/abs/2309.08023) | USM-SCD是一种基于大型预训练基础模型的多语种演讲者转换检测模型，通过微调模型参数，可以同时检测演讲者转换并为96种语言执行自动语音识别。在实验中表现出了优异的性能。 |
| [^128] | [Online Submodular Maximization via Online Convex Optimization.](http://arxiv.org/abs/2309.04339) | 本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。 |
| [^129] | [Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation.](http://arxiv.org/abs/2309.03447) | 本论文提出了一种使用生成对抗神经算子的数据驱动地面运动合成模型，可以根据不同参数生成三分量加速度时间历史。通过使用神经算子架构，模型训练不受数据采样频率影响。研究结果表明，该模型在验证和应用实例中表现出色，并可用于生成日本地震动数据。 |
| [^130] | [Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies.](http://arxiv.org/abs/2309.02045) | 通过提示策略，本论文探究了如何通过应用角色扮演和思维链提示策略来增强大型语言模型（LLMs）在情感分析中的性能，并在三个不同领域的数据集上进行了评估。 |
| [^131] | [Stochastic Graph Bandit Learning with Side-Observations.](http://arxiv.org/abs/2308.15107) | 本文研究了具有一般函数空间和图反馈的随机背景赌博学习问题，并提出了一种算法，填补了此前研究中的空白。该算法提供了奖励差距依赖的上界，并在遗憾上界方面提供了改进。数值实验证明了该方法的计算效率和有效性。 |
| [^132] | [Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference.](http://arxiv.org/abs/2308.13819) | 本研究提出了一种学习稳定二次模型的推断方法，通过设置适当的优化问题，可以构建具有局部和全局稳定性的二次系统模型。 |
| [^133] | [STEM: Unleashing the Power of Embeddings for Multi-task Recommendation.](http://arxiv.org/abs/2308.13537) | 本文提出了一种称为STEM的新范例，用于解决多任务推荐中的负传递问题。与现有方法不同，STEM通过根据样本中正反馈数量的相对比例进行细分，深入研究样本的复杂性，以提高推荐系统的性能。 |
| [^134] | [SuperCalo: Calorimeter shower super-resolution.](http://arxiv.org/abs/2308.11700) | 本文介绍了一种名为SuperCalo的能量沉积模拟超分辨率模型，能够快速上采样高维细粒度的能量沉积模拟，从而降低计算成本和生成时间。 |
| [^135] | [Randomized algorithms for precise measurement of differentially-private, personalized recommendations.](http://arxiv.org/abs/2308.03735) | 这项研究提出了一种随机算法，用于精确测量差分隐私的个性化推荐。通过离线实验，该算法在关键指标上与私密的非个性化和非私密的个性化实现进行了比较。 |
| [^136] | [Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity.](http://arxiv.org/abs/2308.01562) | 本研究提出了一种剪枝增强的分层联邦学习（PHFL）方法，用于解决无线网络中的带宽稀缺和系统异构性问题。通过模型剪枝和无线通信的优化，实现了在严格的延迟和能耗约束下收敛速度的最优化。 |
| [^137] | [MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning.](http://arxiv.org/abs/2307.16424) | 使用条件扩散作为元学习的一种方法，能够解决深度模型在少样本学习中遇到的梯度计算和消失风险问题。 |
| [^138] | [Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media.](http://arxiv.org/abs/2307.09312) | 多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。 |
| [^139] | [Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images.](http://arxiv.org/abs/2307.08919) | 半监督和自监督学习在医学图像上的准确性与时间前沿进行比较，通过一个精心设计的基准研究来回答从业者的问题。 |
| [^140] | [Differentially Private Clustering in Data Streams.](http://arxiv.org/abs/2307.07449) | 本研究提出了首个针对$k$-means和$k$-median聚类的差分隐私流算法，在流模型中实现对数据隐私的保护，并使用尽可能少的空间。 |
| [^141] | [RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$.](http://arxiv.org/abs/2306.15909) | RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。 |
| [^142] | [RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks.](http://arxiv.org/abs/2306.11335) | 该论文介绍了一个基于逼真机器人操作模拟器的基准评估RM-PRT，并提出了通用的评估流程，旨在通过大规模自然语言指令和多模态提示对机器人操作进行详细评估。 |
| [^143] | [On the Model-Misspecification in Reinforcement Learning.](http://arxiv.org/abs/2306.10694) | 这项研究研究了强化学习中模型错误指定问题，发现基于策略的方法在策略函数逼近存在较大误差的情况下仍然具有稳健性，但对于基于值和基于模型的方法在一般函数逼近中是否能实现类似的稳健性仍然存在疑问。 |
| [^144] | [Conditional expectation via compact kernels.](http://arxiv.org/abs/2306.10592) | 本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。 |
| [^145] | [GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction.](http://arxiv.org/abs/2306.01951) | 本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。 |
| [^146] | [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback.](http://arxiv.org/abs/2305.14387) | 该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。 |
| [^147] | [The emergence of clusters in self-attention dynamics.](http://arxiv.org/abs/2305.05465) | 本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。 |
| [^148] | [Comparing Foundation Models using Data Kernels.](http://arxiv.org/abs/2305.05126) | 本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。 |
| [^149] | [Is dataset condensation a silver bullet for healthcare data sharing?.](http://arxiv.org/abs/2305.03711) | 数据集压缩为医疗数据共享提供了一种既保护了隐私又保存了实用程序的新方法。 |
| [^150] | [Statistical Optimality of Deep Wide Neural Networks.](http://arxiv.org/abs/2305.02657) | 本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。 |
| [^151] | [MaskSearch: Querying Image Masks at Scale.](http://arxiv.org/abs/2305.02375) | MaskSearch是一个系统，通过使用新颖的索引技术和高效的过滤器验证查询执行框架，加速对图像掩模数据库的查询，可以将个体查询加速高达两个数量级，优于现有方法。 |
| [^152] | [The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges.](http://arxiv.org/abs/2304.08242) | 深层潜在位置主题模型用于网络聚类和表示，通过基于模型的聚类策略和概率模型对节点和边进行联合表示，并使用模型选择准则进行参数选择。 |
| [^153] | [Learning Causal Attributions in Neural Networks: Beyond Direct Effects.](http://arxiv.org/abs/2303.13850) | 本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。 |
| [^154] | [Evaluating Self-Supervised Learning via Risk Decomposition.](http://arxiv.org/abs/2302.03068) | 通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。 |
| [^155] | [Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning.](http://arxiv.org/abs/2302.00997) | 在线两阶段随机优化算法的累计目标值最小化，同时保证长期平均第二阶段决策结果属于一个集合。采用对抗性学习算法从在线两阶段问题中开发在线算法，其遗憾界可以降至嵌入对抗性学习算法的遗憾界，并在各种设置下获得了新的结果。 |
| [^156] | [QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models.](http://arxiv.org/abs/2302.00919) | 该论文提出了一种名为QCS-SGM+的算法，利用基于得分的生成模型(SGM)作为隐式先验进行量化压缩感知(QCS)，并可以有效地处理一般矩阵。这个算法解决了在粗糙量化的情况下恢复挑战的问题。 |
| [^157] | [Affinity Uncertainty-based Hard Negative Mining in Graph Contrastive Learning.](http://arxiv.org/abs/2301.13340) | 本文提出了一种新颖的方法，在图对比学习（GCL）中使用亲和力信息来挖掘困难负样本，以解决图数据中困难负样本识别困难的问题。 |
| [^158] | [Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology.](http://arxiv.org/abs/2301.13128) | 该论文提出了一种使用CycleGAN实现无监督染色转换的方法，在乳腺组织病理学中应用于侵袭性癌分类。通过比较不同方法的性能，作者证明了将CycleGAN应用于染色无关特征学习的有效性。 |
| [^159] | [LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain.](http://arxiv.org/abs/2301.13126) | LEXTREME是一个多语言和多任务的法律领域基准，该基准提供了11个数据集涵盖24种语言的测评，最佳模型（XLM-R large）在数据集和语言综合评分上均达到了61.3。这使得LEXTREME仍然具有挑战性并且有改进空间。 |
| [^160] | [Compression, Generalization and Learning.](http://arxiv.org/abs/2301.12767) | 本文提出了一种新的理论，允许在压缩的改变概率上保持控制，并获得了紧密的有限样本边界来评估压缩的改变概率。这对学习应用中的错误分类和错误预测具有重要意义。 |
| [^161] | [Pontryagin Optimal Control via Neural Networks.](http://arxiv.org/abs/2212.14566) | 本文将神经网络与庞特里亚金最大原理相结合，提出了一个样本高效的框架NN-PMP-Gradient，可以应用于具有未知和复杂动力学的系统。通过采用迭代方法，该框架不仅利用准确的神经网络参数化的替代模型，还高效地恢复了最优性条件和最优动作序列。 |
| [^162] | [Impossibility Theorems for Feature Attribution.](http://arxiv.org/abs/2212.11870) | 本文阐述了对于中等规模的模型类，任何完整的线性特征归因方法都无法胜任推断模型行为的改进，启示我们在具体定义最终任务的重要性方面要汲取经验，采用重复模型评估的简单直接方法可以胜过许多其他复杂的特征归因方法。 |
| [^163] | [Thales: Formulating and Estimating Architectural Vulnerability Factors for DNN Accelerators.](http://arxiv.org/abs/2212.02649) | 本论文提出了一种新的指标RA，用于量化在发生瞬态错误的情况下DNN的准确性。作者发现现有的RA形式化方法存在不准确性，并提出了一种能够正确估计RA的算法。 |
| [^164] | [Improved Representation of Asymmetrical Distances with Interval Quasimetric Embeddings.](http://arxiv.org/abs/2211.15120) | 本论文介绍了一种称为区间拟度量嵌入（IQE）的方法，用于改进非对称距离的表示。通过满足四个期望属性，IQEs在拟度量学习中表现出强大的近似和泛化能力，带来了更好的性能和更高的效率。 |
| [^165] | [General time-reversal equivariant neural network potential for magnetic materials.](http://arxiv.org/abs/2211.11403) | 本研究提出了时间反演等变神经网络势SpinGNN++，用于磁性系统的全面原子间势能建模和大规模并行自旋-格子动力学计算。通过引入复杂磁性模型数据并验证其能力，SpinGNN++展示了精确描述磁性材料自旋-格子耦合的能力，为相关性质的探索提供了重要工具。 |
| [^166] | [Causal Fairness Assessment of Treatment Allocation with Electronic Health Records.](http://arxiv.org/abs/2211.11183) | 本研究提出了一种用于评估临床决策中治疗分配公平性的因果公平算法，通过考虑患者群体的异质性并对具有相同可能性的患者进行调整，以确定潜在的不公平性。 |
| [^167] | [Entry Dependent Expert Selection in Distributed Gaussian Processes Using Multilabel Classification.](http://arxiv.org/abs/2211.09940) | 本文提出了一种基于输入数据点特征的灵活的专家选择方法，以减少在分布式高斯过程中的专家数目并提高效率。 |
| [^168] | [Assessing Neural Network Robustness via Adversarial Pivotal Tuning.](http://arxiv.org/abs/2211.09782) | 本研究旨在通过使用预训练图像生成器对图像进行详细、多样和逼真的语义操作，同时保留原始图像的类别，来评估神经网络的鲁棒性。 |
| [^169] | [NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation.](http://arxiv.org/abs/2211.04370) | NESTER是一种自适应的神经符号化方法进行治疗效果评估，将治疗效果估计的所有要求集成到一个框架中，该方法比现有最先进的方法在多个基准数据集上性能更好。 |
| [^170] | [Learning Failure-Inducing Models for Testing Software-Defined Networks.](http://arxiv.org/abs/2210.15469) | 本文介绍了一种名为FuzzSDN的机器学习引导模糊测试方法，可生成导致SDN系统失败的有效测试数据，并学习准确的故障诱导模型以表征此类系统失败的条件。 |
| [^171] | [Highly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering.](http://arxiv.org/abs/2210.13690) | 本文提出了一种多阶段聚类策略，可以解决流式嵌入式演讲者分离系统的多方面挑战，从而提高效率。此策略使用不同的聚类算法处理不同长度的输入，并可适应不同资源约束的设备。 |
| [^172] | [Particle clustering in turbulence: Prediction of spatial and statistical properties with deep learning.](http://arxiv.org/abs/2210.02339) | 本研究利用深度学习模型成功预测了湍流中粒子聚集的空间和统计特性，为早期行星形成中尘埃粒子的碰撞生长提供了理论支持。 |
| [^173] | [Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning.](http://arxiv.org/abs/2209.14624) | 本论文研究了神经网络修剪中是否需要复杂性，并通过与全局幅度修剪进行比较发现，原始的全局幅度修剪方法优于其他最先进的修剪技术。 |
| [^174] | [DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization.](http://arxiv.org/abs/2207.05631) | 这篇论文提出了一个名为DGPO的算法，可以在解决任务时发现多种策略，从而提高策略鲁棒性和与用户交互的乐趣。 |
| [^175] | [Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations.](http://arxiv.org/abs/2206.15462) | 该论文提出了一种名为 AMC 的目标函数，鼓励基于梯度的解释覆盖有注释的感兴趣区域，即编码区域。该方法在提高视觉 grounding 性能方面表现卓越，有望成为视觉 grounding 领域的新进展。 |
| [^176] | [Chordal Sparsity for SDP-based Neural Network Verification.](http://arxiv.org/abs/2206.03482) | 本文提出了一种基于半定规划的神经网络验证方法，通过引入弦状稀疏性，旨在改善现有技术中存在的可扩展性问题。 |
| [^177] | [From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation.](http://arxiv.org/abs/2206.03208) | 本论文介绍了一种概念相关传播（CRP）方法，将局部和全局观点结合起来，为个别预测提供了“何地”和“何物”两个问题的解答。该方法提供了更人类可解释的解释，并通过概念图谱深入了解模型的表示和推理能力。 |
| [^178] | [The Survival Bandit Problem.](http://arxiv.org/abs/2206.03019) | 这个研究介绍了生存强盗问题，这是多臂赌博机问题的一个新变种。该问题的目标是最小化生存遗憾，同时要求算法加速收敛。 |
| [^179] | [Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold.](http://arxiv.org/abs/2205.11677) | 该论文提出了两种有效的算法来将标签信息与稀疏图结构相结合，解决了基于网络拓扑的聚类在稀疏图上的问题。 |
| [^180] | [MGDCF: Distance Learning via Markov Graph Diffusion for Neural Collaborative Filtering.](http://arxiv.org/abs/2204.02338) | 本文通过马尔可夫图扩散协同过滤（MGDCF）模型，展示了一些最先进的基于GNN的CF模型与传统的一层NRL模型之间的等价性，为神经协同过滤提供了新的距离学习方法。 |
| [^181] | [Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks.](http://arxiv.org/abs/2204.00846) | 本文提出了一种基于和弦稀疏性的Chordal-LipSDP方法，用于估计深度神经网络的Lipschitz常数。通过将大型半定约束分解为多个较小的约束，该方法在网络深度增加时能够在准确性和可扩展性之间取得更好的权衡。 |
| [^182] | [The complexity of quantum support vector machines.](http://arxiv.org/abs/2203.00031) | 量子支持向量机利用量子电路定义核函数，相较已知的经典算法，具有可证明的指数加速优势。由于量子力学的概率性质，训练算法受到统计不确定性的影响，对偶问题可以在$O(M^{4.67}/\varepsilon^2)$个量子电路评估中解决，核化的原始问题可以通过$O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$次评估解决。 |
| [^183] | [On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error.](http://arxiv.org/abs/2202.03618) | 本文研究了不平衡最优输运（UOT）问题，提出了一种基于梯度外推方法的新算法（GEM-UOT）来解决该问题。 |
| [^184] | [Climate-Invariant Machine Learning.](http://arxiv.org/abs/2112.08440) | 本研究提出了一种新的框架——"气候不变"的机器学习，通过将气候过程的知识纳入机器学习算法中，可以在广泛的气候和地理条件下保持高准确性。 |
| [^185] | [Location Leakage in Federated Signal Maps.](http://arxiv.org/abs/2112.03452) | 本文研究了在联邦学习框架下，通过梯度泄漏攻击推断用户位置的问题，并提出了一种保护位置隐私的方法。 |
| [^186] | [Token-Modification Adversarial Attacks for Natural Language Processing: A Survey.](http://arxiv.org/abs/2103.00676) | 这项调研对现有的自然语言处理中的标记修改对抗攻击进行了分类和比较，并旨在指导新的研究并推动进一步的攻击组件研究。 |
| [^187] | [A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines.](http://arxiv.org/abs/2004.05839) | 本文研究了松弛优化理论，探讨了风险与复杂度之间的联系，提出了可以从复杂度估计风险的方法，研究了支持向量方法在机器学习中的应用。 |

# 详细

[^1]: 准确可扩展的图神经网络表观不确定性估计

    Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks. (arXiv:2401.03350v1 [cs.LG])

    [http://arxiv.org/abs/2401.03350](http://arxiv.org/abs/2401.03350)

    提出了G-$\Delta$UQ，一种新的训练框架，旨在改善图神经网络（GNN）的内在不确定性估计。该框架通过图锚定策略将随机数据中心化应用于图数据，并且能够支持部分随机的GNN。

    

    尽管图神经网络（GNN）广泛用于节点和图表示学习任务，但在分布变化下GNN不确定性估计的可靠性仍相对较少探索。事实上，虽然事后校准策略可以用于改善内部分布校准，但它们不一定也能改进分布变化下的校准。然而，产生更好的内部不确定性估计的技术尤其有价值，因为它们可以随后与事后策略结合使用。因此，在本研究中，我们提出了一种名为G-$\Delta$UQ的新型训练框架，旨在改善内在的GNN不确定性估计。我们的框架通过新颖的图锚定策略将随机数据中心化原则应用于图数据，并能够支持部分随机的GNN。虽然主流观点是为了获得可靠的估计，需要完全随机网络，但我们发现通过功能多样性引入的中观锚定可以在保证准确性的同时降低计算成本。

    While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored. Indeed, while post-hoc calibration strategies can be used to improve in-distribution calibration, they need not also improve calibration under distribution shift. However, techniques which produce GNNs with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later. Therefore, in this work, we propose G-$\Delta$UQ, a novel training framework designed to improve intrinsic GNN uncertainty estimates. Our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced b
    
[^2]: 图像修复通过可控扩散模型的导航

    Image Inpainting via Tractable Steering of Diffusion Models. (arXiv:2401.03349v1 [cs.CV])

    [http://arxiv.org/abs/2401.03349](http://arxiv.org/abs/2401.03349)

    本文提出了一种通过可解概率模型精确计算约束后验的方法，然后利用这一信号来引导扩散模型的去噪过程，从而改进图像修复的质量和语义一致性。

    

    扩散模型是生成逼真图像的当前最先进技术。然而，对于有约束的图像生成任务，如修复，控制抽样过程仍然具有挑战性，因为对这些约束的精确条件设定是不可解的。本文提出利用可解的概率模型(TPMs)的能力来精确且有效地计算受约束的后验，并利用该信号来引导扩散模型的去噪过程。具体而言，本文采用了一类表达力较强的TPMs，称为概率电路(PCs)。基于先前的进展，我们进一步扩大了PCs的规模，并使其能够引导扩散模型的图像生成过程。实证结果表明，我们的方法可以在三个自然图像数据集（即CelebA-H）中持续改进修复图像的整体质量和语义一致性。

    Diffusion models are the current state of the art for generating photorealistic images. Controlling the sampling process for constrained image generation tasks such as inpainting, however, remains challenging since exact conditioning on such constraints is intractable. While existing methods use various techniques to approximate the constrained posterior, this paper proposes to exploit the ability of Tractable Probabilistic Models (TPMs) to exactly and efficiently compute the constrained posterior, and to leverage this signal to steer the denoising process of diffusion models. Specifically, this paper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs). Building upon prior advances, we further scale up PCs and make them capable of guiding the image generation process of diffusion models. Empirical results suggest that our approach can consistently improve the overall quality and semantic coherence of inpainted images across three natural image datasets (i.e., CelebA-H
    
[^3]: 大规模语言模型在现实世界中对恶意言论检测的调查

    An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])

    [http://arxiv.org/abs/2401.03346](http://arxiv.org/abs/2401.03346)

    本研究调查了大规模语言模型在现实世界中对恶意言论检测的有效性，并发现现有方法在上下文感知方面存在显著限制。而大规模语言模型具有潜力作为上下文感知恶意言论检测的知识库，但目前缺乏有效提示这些模型的方法。

    

    恶意言论已经成为困扰我们社交空间的一个主要问题。虽然在解决这个问题上已经做出了一些显著的努力，但现有方法在有效检测在线恶意言论方面仍然存在显著限制。现有方法的一个主要限制是恶意言论检测是一个高度上下文相关的问题，而这些方法无法完全捕捉恶意言论的上下文以进行准确的预测。最近，大规模语言模型（LLMs）在几个自然语言任务中展示了最先进的性能。LLMs经过大量的自然语言数据进行了广泛的训练，使其能够掌握复杂的上下文细节。因此，它们可以用作上下文感知的恶意言论检测的知识库。然而，使用LLMs检测恶意言论的一个基本问题是没有关于有效提示LLMs进行上下文感知的恶意言论检测的研究。在本研究中，我们进行了一个大规模的研究，调查恶意言论检测方面的问题。

    Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hat
    
[^4]: 弱增强变分自编码器在时间序列异常检测中的应用

    Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection. (arXiv:2401.03341v1 [cs.LG])

    [http://arxiv.org/abs/2401.03341](http://arxiv.org/abs/2401.03341)

    本论文提出了一种将变分自编码器（VAEs）和自监督学习（SSL）相结合的新颖生成框架，用于解决在时间序列异常检测（TSAD）中由于数据稀缺引起的潜在空间的不连续性导致的重建不稳定性的问题。

    

    由于其无监督训练和不确定性估计，深度变分自编码器（VAEs）已经成为基于重建的时间序列异常检测（TSAD）的强大工具。现有的基于VAE的TSAD方法，不论是统计方法还是深度方法，都调整元先验以估计有效捕获数据中的时空依赖关系的似然概率。然而，这些方法面临着内在数据稀缺的挑战，这在异常检测任务中经常出现。这种稀缺容易导致潜在空间中的潜在空洞，即潜在空间中的不连续区域，导致在这些不连续的空间上的非鲁棒重构。我们提出了一种将VAEs与自监督学习（SSL）结合的新颖生成框架来解决这个问题。

    Due to their unsupervised training and uncertainty estimation, deep Variational Autoencoders (VAEs) have become powerful tools for reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based TSAD methods, either statistical or deep, tune meta-priors to estimate the likelihood probability for effectively capturing spatiotemporal dependencies in the data. However, these methods confront the challenge of inherent data scarcity, which is often the case in anomaly detection tasks. Such scarcity easily leads to latent holes, discontinuous regions in latent space, resulting in non-robust reconstructions on these discontinuous spaces. We propose a novel generative framework that combines VAEs with self-supervised learning (SSL) to address this issue.
    
[^5]: 一种用于在天文学中联合提取光谱和源数分布的深度学习框架

    A deep learning framework for jointly extracting spectra and source-count distributions in astronomy. (arXiv:2401.03336v1 [astro-ph.IM])

    [http://arxiv.org/abs/2401.03336](http://arxiv.org/abs/2401.03336)

    这篇论文提出了一种用于在天文学中联合提取光谱和源数分布的深度学习框架。

    

    天文观测通常提供三维地图，编码了观测流量在(1)天球的两个角度和(2)能量/频率上的分布。关于这种地图的一个重要任务是统计地表征那些太暗以至于无法个别检测到的点源的种群。由于对单个光弱源的性质了解有限，通常我们会将整个种群作为一个整体进行研究，推断出一个源数分布 (SCD)，该分布描述了源的数密度随亮度变化的情况。虽然存在用于恢复 SCD 的统计和机器学习方法，但它们通常完全忽略了与流量能谱相关的光谱信息。我们提出了一种深度学习框架，能够同时重构不同发射成分的光谱和点源种群的SCD。在一个概念验证的例子中，我们展示了我们的方法能准确提取复杂形状的源。

    Astronomical observations typically provide three-dimensional maps, encoding the distribution of the observed flux in (1) the two angles of the celestial sphere and (2) energy/frequency. An important task regarding such maps is to statistically characterize populations of point sources too dim to be individually detected. As the properties of a single dim source will be poorly constrained, instead one commonly studies the population as a whole, inferring a source-count distribution (SCD) that describes the number density of sources as a function of their brightness. Statistical and machine learning methods for recovering SCDs exist; however, they typically entirely neglect spectral information associated with the energy distribution of the flux. We present a deep learning framework able to jointly reconstruct the spectra of different emission components and the SCD of point-source populations. In a proof-of-concept example, we show that our method accurately extracts even complex-shape
    
[^6]: 通过多光谱合成图像增强的深度学习进行核桃检测

    Walnut Detection Through Deep Learning Enhanced by Multispectral Synthetic Images. (arXiv:2401.03331v1 [cs.CV])

    [http://arxiv.org/abs/2401.03331](http://arxiv.org/abs/2401.03331)

    本研究提出了一种通过多光谱合成图像增强的深度学习方法进行核桃检测，能够显著提高检测效率。

    

    准确鉴定果园内的核桃种植带来了许多优势，极大提高了核桃果园管理的效率和生产力。然而，核桃树的独特特性，即核桃与叶子之间的形状、颜色和纹理之间的相似性，使其在注释过程中精确区分它们成为一项艰巨的挑战。在本研究中，我们提出了一种改进核桃检测效率的新方法，利用在图像集中训练的YOLOv5，该集合包括了真实和合成的RGB和NIR图像。我们对原始数据集和增强数据集的结果进行对比分析，结果表明使用合成图像时检测效果明显改善。

    The accurate identification of walnuts within orchards brings forth a plethora of advantages, profoundly amplifying the efficiency and productivity of walnut orchard management. Nevertheless, the unique characteristics of walnut trees, characterized by their closely resembling shapes, colors, and textures between the walnuts and leaves, present a formidable challenge in precisely distinguishing between them during the annotation process. In this study, we present a novel approach to improve walnut detection efficiency, utilizing YOLOv5 trained on an enriched image set that incorporates both real and synthetic RGB and NIR images. Our analysis comparing results from our original and augmented datasets shows clear improvements in detection when using the synthetic images.
    
[^7]: 基于注意力和自编码器的混合模型用于无监督在线异常检测

    Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection. (arXiv:2401.03322v1 [cs.LG])

    [http://arxiv.org/abs/2401.03322](http://arxiv.org/abs/2401.03322)

    本文介绍了一种基于注意力和自编码器的混合模型，用于无监督的在线时间序列异常检测，通过结合注意力和自编码器，并在自编码器的潜在空间中预测下一个时间步骤窗口，提高了模型的准确性。

    

    本论文介绍了一种基于注意力和自编码器的混合模型，用于时间序列的无监督在线异常检测。自编码器捕捉短嵌入中的局部结构模式，而注意力模型学习长期特征，通过位置编码实现并行计算。在方法上独特的是，我们提出的混合模型首次在时间序列异常检测中结合了注意力和自编码器。它采用了类似深度变换器模型的注意力机制，并对自编码器的潜在空间中预测下一个时间步骤窗口进行了关键架构修改。该模型利用验证数据集中的阈值进行异常检测，并引入了一种基于分析误差的第一统计矩方法的替代方法，提高了准确性而不依赖于验证数据集。通过在多样的实际基准数据集上进行评估，并与其他广为认可的模型进行比较，证实了模型的有效性。

    This paper introduces a hybrid attention and autoencoder (AE) model for unsupervised online anomaly detection in time series. The autoencoder captures local structural patterns in short embeddings, while the attention model learns long-term features, facilitating parallel computing with positional encoding. Unique in its approach, our proposed hybrid model combines attention and autoencoder for the first time in time series anomaly detection. It employs an attention-based mechanism, akin to the deep transformer model, with key architectural modifications for predicting the next time step window in the autoencoder's latent space. The model utilizes a threshold from the validation dataset for anomaly detection and introduces an alternative method based on analyzing the first statistical moment of error, improving accuracy without dependence on a validation dataset. Evaluation on diverse real-world benchmark datasets and comparing with other well-established models, confirms the effective
    
[^8]: 微服务复制在云中的调用率预测比较

    Comparison of Microservice Call Rate Predictions for Replication in the Cloud. (arXiv:2401.03319v1 [cs.DC])

    [http://arxiv.org/abs/2401.03319](http://arxiv.org/abs/2401.03319)

    本文通过比较线性回归、多层感知机和梯度提升回归三种机器学习模型在微服务调用率预测方面的性能，发现梯度提升回归模型在减小误差方面表现出色，并且能够准确预测微服务所需的副本数量。

    

    如今，许多用户在一群云机器上部署基于微服务的应用程序，这些应用程序具有各种相互连接，并受到动态用户需求的随机变化的影响。为了解决这个问题，我们比较了三种机器学习（ML）模型，用于基于微服务时间预测微服务调用率，并旨在估计可扩展性要求。我们在阿里巴巴的微服务跟踪数据上应用了线性回归（LR）、多层感知机（MLP）和梯度提升回归（GBR）模型。预测结果显示，与GBR和MLP模型相比，LR模型的训练时间更短。然而，与LR和MLP模型相比，GBR模型降低了均方差和平均绝对百分比误差。此外，预测结果显示，梯度提升模型对每个微服务所需的副本数量与实际测试数据非常接近，而无需进行任何预测。

    Today, many users deploy their microservice-based applications with various interconnections on a cluster of Cloud machines, subject to stochastic changes due to dynamic user requirements. To address this problem, we compare three machine learning (ML) models for predicting the microservice call rates based on the microservice times and aiming at estimating the scalability requirements. We apply the linear regression (LR), multilayer perception (MLP), and gradient boosting regression (GBR) models on the Alibaba microservice traces. The prediction results reveal that the LR model reaches a lower training time than the GBR and MLP models. However, the GBR reduces the mean absolute error and the mean absolute percentage error compared to LR and MLP models. Moreover, the prediction results show that the required number of replicas for each microservice by the gradient boosting model is close to the actual test data without any prediction.
    
[^9]: 提升对比度的上下文增强

    Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])

    [http://arxiv.org/abs/2401.03314](http://arxiv.org/abs/2401.03314)

    本研究提出了一种通过对比学习来提高神经机器翻译性能的新方法，利用巴洛双胞胎损失最大化互信息。与其他方法不同的是，该方法通过上下文增强来提升性能，而不需要明确地增加数据或从头开始学习嵌入。

    

    神经机器翻译受益于语义丰富的表示。通过语言建模和对比学习使用互信息最大化目标，已经实现了对这种表示的大幅进展。语言建模的依赖性使得在学习表示的通用性和模型在语言建模任务上的性能之间存在权衡。虽然对比学习改进了性能，但其成功不能仅归因于互信息。我们提出了一种新的上下文增强步骤，通过使用巴洛双胞胎损失最大化互信息来提高神经机器翻译的性能。与其他方法不同的是，我们不是显式地增加数据，而是将语言视为隐含的增强，消除了破坏语义信息的风险。此外，我们的方法不会从头开始学习嵌入，并且可以推广到任何一组预训练的嵌入。

    Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Fin
    
[^10]: MOTO: 离线预训练与在线微调用于基于模型的机器人学习

    MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning. (arXiv:2401.03306v1 [cs.LG])

    [http://arxiv.org/abs/2401.03306](http://arxiv.org/abs/2401.03306)

    本研究提出了一种基于模型的方法，通过模型值扩展和策略正则化，可以在高维领域中有效地进行离线预训练和在线微调的强化学习，解决了离线到在线微调中的分布偏移、离散动力学数据和非稳态奖励问题。

    

    我们研究了在现实机器人任务中，从高维观测中进行离线预训练和在线微调的强化学习问题。最近的离线无模型方法成功地使用在线微调来提高智能体在数据收集策略上的性能，或者适应新的任务。与此同时，基于模型的强化学习算法在样本效率和解决的任务复杂性方面取得了显著进展，但在微调方面仍然被低估了。在这项工作中，我们认为现有的基于模型的离线强化学习方法在高维领域中不适用于离线到在线微调，原因是分布偏移、离散动力学数据和非稳态奖励问题。我们提出了一种基于模型的方法，通过模型值扩展和策略正则化，可以高效地重用先前的数据，同时通过控制认知不确定性来防止模型的利用。

    We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty
    
[^11]: 行动中的现实主义：使用YOLOv8和DeiT从医学图像中诊断脑肿瘤的异常感知

    Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])

    [http://arxiv.org/abs/2401.03302](http://arxiv.org/abs/2401.03302)

    本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。

    

    在医学科学领域，由于脑肿瘤在患者中的罕见程度，可靠地检测和分类脑肿瘤仍然是一个艰巨的挑战。因此，在异常情况下检测肿瘤的能力对于确保及时干预和改善患者结果至关重要。本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤。来自国家脑映射实验室（NBML）的精选数据集包括81名患者，其中包括30例肿瘤病例和51例正常病例。检测和分类流程被分为两个连续的任务。检测阶段包括全面的数据分析和预处理，以修改图像样本和每个类别的患者数量，以符合真实世界场景中的异常分布（9个正常样本对应1个肿瘤样本）。此外，在测试中除了常见的评估指标外，我们还采用了... [摘要长度已达到上限]

    In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
    
[^12]: 在样本高效的离线强化学习中：数据多样性、后验采样，以及更多

    On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])

    [http://arxiv.org/abs/2401.03301](http://arxiv.org/abs/2401.03301)

    本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。

    

    我们试图理解什么促进了对于序贝叶斯决策的历史数据集进行样本高效学习，这个问题通常被称为离线强化学习（RL）。此外，我们对于在利用（值）函数逼近的同时享受样本效率的算法感兴趣。在本文中，我们通过提出一个包括离线RL中覆盖度量的先前概念的数据多样性概念来解决这些基本问题，并且利用这个概念将基于版本空间（VS）、正则化优化（RO）和后验采样（PS）的三个不同类别的离线RL算法进行统一。我们在标准假设下证明，基于VS、基于RO和基于PS的算法达到了\emph{可比}的样本效率，这恢复了在有限和线性模型类别下的最优性的标准假设的边界。这个结果令人惊讶，因为之前的研究表明这些算法不具有有利性的样本效率。

    We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
    
[^13]: 复杂环境中的自主导航

    Autonomous Navigation in Complex Environments. (arXiv:2401.03267v1 [cs.RO])

    [http://arxiv.org/abs/2401.03267](http://arxiv.org/abs/2401.03267)

    本文研究了在复杂环境中应用CNN-DNN网络融合进行自主导航的方法，通过模仿学习使用LiDAR和摄像头数据对机器人进行训练，并通过Monte-Carlo测试了其鲁棒性。

    

    本文探讨了在模拟环境中应用CNN-DNN网络融合来构建机器人导航控制器的方法。模拟环境用于模拟地下救援情景，将自主代理任务设定为在未知的洞穴系统中寻找目标。采用模仿学习的方法，使用LiDAR和摄像头数据来训练控制算法以进行空间导航并找到目标。训练好的模型通过Monte-Carlo进行鲁棒性测试。

    This paper explores the application of CNN-DNN network fusion to construct a robot navigation controller within a simulated environment. The simulated environment is constructed to model a subterranean rescue situation, such that an autonomous agent is tasked with finding a goal within an unknown cavernous system. Imitation learning is used to train the control algorithm to use LiDAR and camera data to navigate the space and find the goal. The trained model is then tested for robustness using Monte-Carlo.
    
[^14]: 大型语言模型作为视觉跨领域学习器

    Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])

    [http://arxiv.org/abs/2401.03253](http://arxiv.org/abs/2401.03253)

    本研究提出了大型语言模型作为视觉跨领域学习器（LLaVO），通过将图像转换为文本描述，使用大型语言模型进行训练和微调，实现了在跨领域任务中减少领域转移的效果。

    

    深度学习模型取得的最新进展依赖于独立同分布的假设，这限制了它们在真实世界中面对领域转移时的应用。为了解决以上问题，跨领域学习旨在提取领域不变的知识，减少训练与测试数据之间的领域转移。然而，在视觉跨领域学习中，传统方法仅关注图像模态，忽视了使用文本模态来缓解领域转移的作用。在本研究中，我们提出了大型语言模型作为视觉跨领域学习器（LLaVO）。LLaVO使用视觉-语言模型将图像转换为详细的文本描述。然后，在经过设计的指导模板生成的源域/目标域的文本描述上，对大型语言模型进行微调。在领域泛化和无监督领域自适应设置下进行的各种跨领域任务的广泛实验结果表明了该方法的效果。

    Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect
    
[^15]: TeLeS：用于估计端到端ASR信任度的时态词元相似度分数

    TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in End-to-End ASR. (arXiv:2401.03251v1 [eess.AS])

    [http://arxiv.org/abs/2401.03251](http://arxiv.org/abs/2401.03251)

    本论文提出了一种用于估计端到端ASR信任度的时态词元相似度分数TeLeS，并用缩减损失来解决CEM训练中目标得分数据不平衡的问题。

    

    从端到端（E2E）自动语音识别（ASR）模型的预测中得出信心估计有助于ASR的下游和上游任务。基于类别概率的置信度得分不能准确地表示过于自信的ASR预测的质量。辅助置信度估计模型（CEM）可校准这些预测。最先进的解决方案使用二进制目标得分进行CEM训练。然而，二进制标签不能揭示预测词的细粒度信息，如参考语音和假设语音之间的时态对齐以及预测词是否完全错误或包含拼写错误。为解决这个问题，我们提出了一种新的时态词元相似度（TeLeS）置信度分数来训练CEM。为了解决CEM训练中目标得分的数据不平衡问题，我们使用缩减损失来聚焦于难以学习的数据点并最小化轻易学习的数据点的影响。我们在三种语言训练的ASR模型上进行了实验。

    Confidence estimation of predictions from an End-to-End (E2E) Automatic Speech Recognition (ASR) model benefits ASR's downstream and upstream tasks. Class-probability-based confidence scores do not accurately represent the quality of overconfident ASR predictions. An ancillary Confidence Estimation Model (CEM) calibrates the predictions. State-of-the-art (SOTA) solutions use binary target scores for CEM training. However, the binary labels do not reveal the granular information of predicted words, such as temporal alignment between reference and hypothesis and whether the predicted word is entirely incorrect or contains spelling errors. Addressing this issue, we propose a novel Temporal-Lexeme Similarity (TeLeS) confidence score to train CEM. To address the data imbalance of target scores while training CEM, we use shrinkage loss to focus on hard-to-learn data points and minimise the impact of easily learned data points. We conduct experiments with ASR models trained in three languages
    
[^16]: SeqNAS: 事件序列分类的神经架构搜索

    SeqNAS: Neural Architecture Search for Event Sequence Classification. (arXiv:2401.03246v1 [cs.LG])

    [http://arxiv.org/abs/2401.03246](http://arxiv.org/abs/2401.03246)

    SeqNAS是一种针对事件序列分类的神经架构搜索算法，通过引入一种简单而表达力强的搜索空间，利用常用的构建块进行搜索，提供高质量的任务特定解决方案。

    

    神经架构搜索(NAS)方法广泛应用于各个行业，以获得高质量的任务特定解决方案，最大限度地减少人为干预。事件序列在各种工业应用中得到广泛使用，包括流失预测、客户分割、欺诈检测和故障诊断等。这些数据包括具有不规则时间戳的分类和实数值组件。尽管NAS方法的有用性，但之前的方法仅应用于其他领域的图像、文本或时间序列。我们的工作通过引入一种新颖的NAS算法SeqNAS来解决这个限制，该算法专门用于事件序列分类。我们开发了一个简单但表达力强的搜索空间，利用常用的事件序列分类构建块，包括多头自注意力、卷积和循环单元。为了进行搜索，我们采用了顺序贝叶斯优化，并利用先前训练过的模型作为教师模型的集成。

    Neural Architecture Search (NAS) methods are widely used in various industries to obtain high quality taskspecific solutions with minimal human intervention. Event Sequences find widespread use in various industrial applications including churn prediction customer segmentation fraud detection and fault diagnosis among others. Such data consist of categorical and real-valued components with irregular timestamps. Despite the usefulness of NAS methods previous approaches only have been applied to other domains images texts or time series. Our work addresses this limitation by introducing a novel NAS algorithm SeqNAS specifically designed for event sequence classification. We develop a simple yet expressive search space that leverages commonly used building blocks for event sequence classification including multihead self attention convolutions and recurrent cells. To perform the search we adopt sequential Bayesian Optimization and utilize previously trained models as an ensemble of teache
    
[^17]: 通过参数缩放解释自适应梯度方法以实现零学习率优化

    Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization. (arXiv:2401.03240v1 [cs.LG])

    [http://arxiv.org/abs/2401.03240](http://arxiv.org/abs/2401.03240)

    本文提出了一种无学习率的自适应梯度方法，通过参数缩放解释自适应梯度方法，扩展了无学习率方法的适用性并增强了使用自适应梯度方法进行训练的效果。

    

    本文解决了在训练深度神经网络中估计自适应梯度方法学习率的挑战。尽管已经提出了几种无学习率方法，但它们通常是针对最陡下降法设计的。然而，尽管最陡下降法对于寻找极小值提供了直观的方法，但许多深度学习应用需要自适应梯度方法以实现更快的收敛。本文将自适应梯度方法解释为应用于参数缩放网络上的最陡下降法，并提出了无学习率的自适应梯度方法。实验结果验证了这种方法的有效性，在各种情况下展示出与手动调整学习率相当的性能。这项工作扩展了无学习率方法的适用性，增强了使用自适应梯度方法进行训练的效果。

    We address the challenge of estimating the learning rate for adaptive gradient methods used in training deep neural networks. While several learning-rate-free approaches have been proposed, they are typically tailored for steepest descent. However, although steepest descent methods offer an intuitive approach to finding minima, many deep learning applications require adaptive gradient methods to achieve faster convergence. In this paper, we interpret adaptive gradient methods as steepest descent applied on parameter-scaled networks, proposing learning-rate-free adaptive gradient methods. Experimental results verify the effectiveness of this approach, demonstrating comparable performance to hand-tuned learning rates across various scenarios. This work extends the applicability of learning-rate-free methods, enhancing training with adaptive gradient methods.
    
[^18]: 基于Split Learning的肌电假肢控制中的收敛速率最大化

    Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])

    [http://arxiv.org/abs/2401.03233](http://arxiv.org/abs/2401.03233)

    本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。

    

    Split Learning (SL)是一种有前途的分布式学习方法，可以在资源有限的环境中应用于基于肌电的假肢控制。与深度学习和联邦学习等其他学习方法相比，SL能够提供更优的解决方案，因为假肢设备在处理能力和电池寿命方面非常有限。在这些情况下，实现SL的可行性源于其固有的模型分割，其中客户端执行较小的模型部分。然而，选择不恰当的切层会阻碍SL系统的训练过程。本文提出了一种用于最大化模型收敛速率的切层选择算法。性能评估表明，所提出的算法在改善假肢控制的肌电模式识别任务中显著加速了收敛过程。

    Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
    
[^19]: FedTGP: 用自适应边缘增强对比学习训练可训练全局原型以解决联邦学习中的数据和模型异质性问题

    FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced Contrastive Learning for Data and Model Heterogeneity in Federated Learning. (arXiv:2401.03230v1 [cs.LG])

    [http://arxiv.org/abs/2401.03230](http://arxiv.org/abs/2401.03230)

    FedTGP 是一种用于解决联邦学习中数据和模型异质性问题的方法，通过引入自适应边缘增强对比学习 (ACL) 来学习可训练的全局原型 (TGP)，从而提高了原型的可分离性和语义含义。

    

    最近，由于其支持异构模型和数据的能力，异构联邦学习（HtFL）引起了人们的关注。为了减少在HtFL中传输模型参数的高通信成本，一个主要挑战是提出了基于原型的HtFL方法，将类别代表，即原型，仅在异构客户端之间共享，同时保护客户端模型的隐私。然而，这些原型在服务器上通过加权平均值简单地聚合成全局原型，从而导致子优化的全局知识，从而对客户端的性能产生负面影响。为了克服这个挑战，我们引入了一种新的HtFL方法，称为FedTGP，它利用了我们的自适应边缘增强对比学习（ACL）在服务器上学习可训练的全局原型（TGP）。通过结合ACL，我们的方法增强了原型的可分离性同时保留语义含义。对于十二个异构模型的大量实验证明

    Recently, Heterogeneous Federated Learning (HtFL) has attracted attention due to its ability to support heterogeneous models and data. To reduce the high communication cost of transmitting model parameters, a major challenge in HtFL, prototype-based HtFL methods are proposed to solely share class representatives, a.k.a, prototypes, among heterogeneous clients while maintaining the privacy of clients' models. However, these prototypes are naively aggregated into global prototypes on the server using weighted averaging, resulting in suboptimal global knowledge which negatively impacts the performance of clients. To overcome this challenge, we introduce a novel HtFL approach called FedTGP, which leverages our Adaptive-margin-enhanced Contrastive Learning (ACL) to learn Trainable Global Prototypes (TGP) on the server. By incorporating ACL, our approach enhances prototype separability while preserving semantic meaning. Extensive experiments with twelve heterogeneous models demonstrate that 
    
[^20]: 有约束生成建模的反射Schr\"odinger桥算法

    Reflected Schr\"odinger Bridge for Constrained Generative Modeling. (arXiv:2401.03228v1 [stat.ML])

    [http://arxiv.org/abs/2401.03228](http://arxiv.org/abs/2401.03228)

    本研究提出了一种用于在有界域内生成数据的熵正则化最优输运方法，通过推导出带有边界条件的反射正向-反向随机微分方程，解决了反射扩散模型在适应多样性领域时的限制。

    

    扩散模型已经成为实际应用中大规模生成模型的首选方法。这些应用通常涉及在有界域内限制的数据分布，通常需要 ad-hoc 阈值技术来强制边界。反射扩散模型旨在通过由反射布朗运动控制的后向过程生成数据分布，以增加泛化能力。然而，反射扩散模型可能不容易适应各种领域，需要正确导出的差同胚映射，并且不能保证最优输运特性。为了克服这些限制，我们引入了反射Schr\"odinger桥算法：一种用于在各种有界域内生成数据的熵正则化最优输运方法。我们推导了带有 Neumann 和 Robin 边界条件的优雅反射正向-反向随机微分方程，并将基于散度的似然训练扩展到有界领域。

    Diffusion models have become the go-to method for large-scale generative models in real-world applications. These applications often involve data distributions confined within bounded domains, typically requiring ad-hoc thresholding techniques for boundary enforcement. Reflected diffusion models (Lou23) aim to enhance generalizability by generating the data distribution through a backward process governed by reflected Brownian motion. However, reflected diffusion models may not easily adapt to diverse domains without the derivation of proper diffeomorphic mappings and do not guarantee optimal transport properties. To overcome these limitations, we introduce the Reflected Schrodinger Bridge algorithm: an entropy-regularized optimal transport approach tailored for generating data within diverse bounded domains. We derive elegant reflected forward-backward stochastic differential equations with Neumann and Robin boundary conditions, extend divergence-based likelihood training to bounded d
    
[^21]: 图像和时间序列的端到端反后门学习

    End-to-End Anti-Backdoor Learning on Images and Time Series. (arXiv:2401.03215v1 [cs.LG])

    [http://arxiv.org/abs/2401.03215](http://arxiv.org/abs/2401.03215)

    本文提出了一种名为端到端反后门学习（E2ABL）的方法，该方法可以在图像和时间序列数据上进行鲁棒训练以对抗后门攻击。与传统的反后门学习（ABL）方法不同，E2ABL通过额外的分类头在深度神经网络（DNN）的浅层进行端到端训练，并有效地鉴别深层特征空间中干净样本和污染样本之间的边界。

    

    后门攻击对深度学习模型构成了重大的安全威胁，尤其是那些用于安全和保密的关键应用程序中。这些攻击通过在训练阶段嵌入隐藏触发器来操纵模型的行为，在推断时允许对模型的输出进行未经授权的控制。尽管存在大量面向图像分类模型的防御策略，但鲜有针对时间序列数据的防御解决方案，也没有能够在受污染数据上进行干净模型训练的端到端解决方案。为了填补这一空白，本文在反后门学习（ABL）的基础上，引入了一种创新方法——端到端反后门学习（E2ABL），用于对抗后门攻击的鲁棒训练。与原始的ABL不同，E2ABL通过与深度神经网络（DNN）的浅层连接的附加分类头实现端到端训练。这个辅助头积极地探索深层特征空间中的干净样本和污染样本之间的边界。

    Backdoor attacks present a substantial security concern for deep learning models, especially those utilized in applications critical to safety and security. These attacks manipulate model behavior by embedding a hidden trigger during the training phase, allowing unauthorized control over the model's output during inference time. Although numerous defenses exist for image classification models, there is a conspicuous absence of defenses tailored for time series data, as well as an end-to-end solution capable of training clean models on poisoned data. To address this gap, this paper builds upon Anti-Backdoor Learning (ABL) and introduces an innovative method, End-to-End Anti-Backdoor Learning (E2ABL), for robust training against backdoor attacks. Unlike the original ABL, which employs a two-stage training procedure, E2ABL accomplishes end-to-end training through an additional classification head linked to the shallow layers of a Deep Neural Network (DNN). This secondary head actively ide
    
[^22]: 理解非线性自监督学习的表示可学习性

    Understanding Representation Learnability of Nonlinear Self-Supervised Learning. (arXiv:2401.03214v1 [cs.LG])

    [http://arxiv.org/abs/2401.03214](http://arxiv.org/abs/2401.03214)

    这篇论文是第一次准确分析非线性自监督学习模型的学习结果，通过使用梯度下降算法训练模型并证明其收敛到局部最小值，在理解数据表示可学习性方面做出了重要贡献。

    

    自监督学习（SSL）在许多下游任务中经验证明其数据表示可学习性。关于数据表示可学习性的理论研究很少，其中许多将非线性神经网络视为“黑箱”，仅关注最终的数据表示。然而，神经网络的准确学习结果对于描述SSL模型学到的数据分布特征至关重要。我们的论文是首次准确分析非线性SSL模型的学习结果。我们考虑了一个包含两个特征的玩具数据分布：与标签相关的特征和隐藏特征。与以往的依赖于闭式解的线性设置工作不同，我们使用梯度下降算法在特定的初始化区域下训练一个1层非线性SSL模型，并证明模型收敛到一个局部最小值。此外，与复杂的迭代分析不同，我们提出了一个新的分析过程，使用t

    Self-supervised learning (SSL) has empirically shown its data representation learnability in many downstream tasks. There are only a few theoretical works on data representation learnability, and many of those focus on final data representation, treating the nonlinear neural network as a ``black box". However, the accurate learning results of neural networks are crucial for describing the data distribution features learned by SSL models. Our paper is the first to analyze the learning results of the nonlinear SSL model accurately. We consider a toy data distribution that contains two features: the label-related feature and the hidden feature. Unlike previous linear setting work that depends on closed-form solutions, we use the gradient descent algorithm to train a 1-layer nonlinear SSL model with a certain initialization region and prove that the model converges to a local minimum. Furthermore, different from the complex iterative analysis, we propose a new analysis process which uses t
    
[^23]: 一种可以利用先验信息加快收敛速度的Robbins-Monro序列

    A Robbins--Monro Sequence That Can Exploit Prior Information For Faster Convergence. (arXiv:2401.03206v1 [cs.LG])

    [http://arxiv.org/abs/2401.03206](http://arxiv.org/abs/2401.03206)

    提出了一种新的方法，通过引入先验信息，改进了Robbins-Monro算法的收敛速度。结果表明，先验信息的Robbins-Monro序列比标准序列收敛更快，特别是在前几步。

    

    我们提出了一种新方法，通过将目标点的先验信息引入Robbins-Monro迭代来改善收敛速度。我们实现了不需要回归模型的先验信息的融合，这也会带来额外的约束。我们证明了这种先验信息的Robbins-Monro序列对于广泛的先验分布都是收敛的，即使是错误的先验分布，如高斯分布、高斯分布的加权和（例如在核密度估计中），以及大于零的有界任意分布函数。我们还通过数值分析来了解序列的性能和参数的影响。结果表明，先验信息的Robbins-Monro序列比标准序列收敛更快，特别是在前几步，这对于测量函数数目有限和误差较大的应用特别重要。

    We propose a new method to improve the convergence speed of the Robbins-Monro algorithm by introducing prior information about the target point into the Robbins-Monro iteration. We achieve the incorporation of prior information without the need of a -- potentially wrong -- regression model, which would also entail additional constraints. We show that this prior-information Robbins-Monro sequence is convergent for a wide range of prior distributions, even wrong ones, such as Gaussian, weighted sum of Gaussians, e.g., in a kernel density estimate, as well as bounded arbitrary distribution functions greater than zero. We furthermore analyse the sequence numerically to understand its performance and the influence of parameters. The results demonstrate that the prior-information Robbins-Monro sequence converges faster than the standard one, especially during the first steps, which are particularly important for applications where the number of function measurements is limited, and when the 
    
[^24]: 使用降维的学习增强K-Means聚类

    Learning-Augmented K-Means Clustering Using Dimensional Reduction. (arXiv:2401.03198v1 [cs.LG])

    [http://arxiv.org/abs/2401.03198](http://arxiv.org/abs/2401.03198)

    本论文提出了一种使用主成分分析（PCA）降低数据集维度的学习增强K-Means聚类算法，该算法能够在k值为10和25时获得较低的聚类误差。

    

    学习增强是一种用于改善方法或模型性能的机器学习概念，可以增强其预测和泛化数据或特征的能力，或通过引入噪声和其他因素测试方法的可靠性。聚类是数据分析的基础，长期以来一直用于了解大型数据集的结构。尽管k-means算法有着悠久的历史，但仍面临着挑战。为了应对这些挑战，我们提出了一种使用主成分分析（PCA）来降低数据集维度的解决方案。

    Learning augmented is a machine learning concept built to improve the performance of a method or model, such as enhancing its ability to predict and generalize data or features, or testing the reliability of the method by introducing noise and other factors. On the other hand, clustering is a fundamental aspect of data analysis and has long been used to understand the structure of large datasets. Despite its long history, the k-means algorithm still faces challenges. One approach, as suggested by Ergun et al,is to use a predictor to minimize the sum of squared distances between each data point and a specified centroid. However, it is known that the computational cost of this algorithm increases with the value of k, and it often gets stuck in local minima. In response to these challenges, we propose a solution to reduce the dimensionality of the dataset using Principal Component Analysis (PCA). It is worth noting that when using k values of 10 and 25, the proposed algorithm yields lower
    
[^25]: 在非稳定环境下的决策制定与策略增强搜索

    Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])

    [http://arxiv.org/abs/2401.03197](http://arxiv.org/abs/2401.03197)

    在非稳定环境下的决策制定是一个具有挑战性的问题，本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。

    

    在许多重要问题中，存在着不确定性下的连续决策制定。针对这类问题，传统的方法包括强化学习和在线搜索（如蒙特卡洛树搜索）。前者通过与环境的交互来学习策略（通常在执行之前完成），而后者在决策时使用环境的生成模型来采样有前景的行动轨迹。在非稳定环境下的决策制定尤为具有挑战性，因为代理操作的环境可能随时间变化。两种方法在这种情况下都存在缺陷--一方面，执行之前学习的策略在环境改变时变得陈旧，重新学习需要时间和计算资源。另一方面，在线搜索在允许的运行时间有限时可能会返回次优行动。本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。

    Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio
    
[^26]: 使用迁移学习和时空特征构建高效的比特率梯度

    Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features. (arXiv:2401.03195v1 [cs.MM])

    [http://arxiv.org/abs/2401.03195](http://arxiv.org/abs/2401.03195)

    我们提出了一种使用迁移学习和时空特征的比特率和复杂度高效预测方法，通过利用预训练深度神经网络的特征图和预测的最小比特率来改进比特率梯度的效率，从而在视频行业中提供高质量视频并保持比特率的效率。

    

    在视频行业里，提供高质量的视频并保持比特率的效率是一个主要的挑战。传统的一刀切比特率梯度方案效率低下，并且由于需要进行大量的编码，使得计算最佳内容感知决策成为不现实。为了缓解这个问题，我们提出了一种使用迁移学习和时空特征的比特率和复杂度高效预测方法。我们提出了以下方法：（1）使用来自知名预训练深度神经网络的特征图来预测比特率质量行为，并限制训练数据的数量；（2）通过预测顶部质量的最小比特率来改进最高质量的梯度效率，并将其应用于顶部step。该方法在102个视频场景上进行了测试，结果显示，在1.71%的BD-Rate开销下，与蛮力方法相比，复杂度降低了94.1%。此外，我们还通过四个网络和消融研究深入研究了迁移学习。

    Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.
    
[^27]: 通过拓扑数据分析学习动态网络中的持久社区结构

    Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis. (arXiv:2401.03194v1 [cs.AI])

    [http://arxiv.org/abs/2401.03194](http://arxiv.org/abs/2401.03194)

    本论文提出了一种通过拓扑数据分析学习动态网络中的持久社区结构的方法。通过引入最小化拓扑变化的概念，提出了一种深度图聚类框架，并引入了神经网络正则化方法，用于确保时间一致性和聚类准确性。

    

    动态社区检测方法通常缺乏有效的机制来确保时间一致性，从而阻碍了对网络演化的分析。本文提出了一种新颖的深度图聚类框架，通过对小时间间隔内的社区间结构进行拓扑变化的最小化，来保证时间的一致性。具体来说，为了解决表示塌缩问题，我们首先引入了基于矩阵分解的深度图聚类算法MFC，保留了节点嵌入。基于静态聚类结果，我们构建概率社区网络，并计算它们的持久同调，这是一种强大的拓扑度量，用于评估它们之间的结构相似性。此外，我们还引入了一种新颖的神经网络正则化方法TopoReg，用于确保社区间结构的拓扑相似性随时间间隔的保持。我们的方法提高了真实网络上的时间一致性和聚类准确性。

    Dynamic community detection methods often lack effective mechanisms to ensure temporal consistency, hindering the analysis of network evolution. In this paper, we propose a novel deep graph clustering framework with temporal consistency regularization on inter-community structures, inspired by the concept of minimal network topological changes within short intervals. Specifically, to address the representation collapse problem, we first introduce MFC, a matrix factorization-based deep graph clustering algorithm that preserves node embedding. Based on static clustering results, we construct probabilistic community networks and compute their persistence homology, a robust topological measure, to assess structural similarity between them. Moreover, a novel neural network regularization TopoReg is introduced to ensure the preservation of topological similarity between inter-community structures over time intervals. Our approach enhances temporal consistency and clustering accuracy on real-
    
[^28]: 关于Hermitian动态模态分解的收敛性研究

    On the Convergence of Hermitian Dynamic Mode Decomposition. (arXiv:2401.03192v1 [math.NA])

    [http://arxiv.org/abs/2401.03192](http://arxiv.org/abs/2401.03192)

    研究了Hermitian动态模态分解(DMD)对自伴随Koopman算子的谱性质的收敛性，通过建立了关于谱测度收敛性的一般定理，证明了HDMD的特征值和特征函数在适当条件下收敛到基础Koopman算子的谱性质。

    

    本文研究了Hermitian动态模态分解(DMD)对自伴随Koopman算子的谱性质的收敛性。Hermitian DMD是一种基于数据的方法，用于从离散时间快照近似表示与未知非线性动力系统相关的Koopman算子，同时在其有限维近似中保持算子的自伴性。我们证明，在适当的条件下，HDMD的特征值和特征函数收敛到基础Koopman算子的谱性质。在此过程中，我们建立了关于谱测度收敛性的一般定理，并在二维薛定谔方程上通过数值结果验证了我们的结论。

    In this work, we study the convergence of Hermitian Dynamic Mode Decomposition (DMD) to the spectral properties of self-adjoint Koopman operators. Hermitian DMD is a data-driven method for approximating the Koopman operator associated with an unknown nonlinear dynamical system from discrete-time snapshots, while preserving the self-adjointness of the operator on its finite-dimensional approximations. We show that, under suitable conditions, the eigenvalues and eigenfunctions of HDMD converge to the spectral properties of the underlying Koopman operator. Along the way, we establish a general theorem on the convergence of spectral measures, and demonstrate our results numerically on the two-dimensional Schr\"odinger equation.
    
[^29]: 使用深度学习方法的Bodo语词性标注器

    Part-of-Speech Tagger for Bodo Language using Deep Learning approach. (arXiv:2401.03175v1 [cs.CL])

    [http://arxiv.org/abs/2401.03175](http://arxiv.org/abs/2401.03175)

    本文介绍了使用深度学习方法的Bodo语词性标注器。首先，我们提出了Bodo语言模型BodoBERT，这项工作是第一个为Bodo开发的语言模型。其次，我们提出了基于深度学习的Bodo词性标注模型，该模型利用了BiLSTM、CRF和BodoBERT与BytePairEmbeddings的组合。尽管研究已经在资源丰富的语言中进行了大量的语言模型和词性标注模型的研究，但对于低资源语言如Bodo，仍然缺乏相关研究。

    

    语言处理系统如词性标注、命名实体识别、机器翻译、语音识别和语言建模在资源丰富的语言中已经得到了广泛研究。然而，对于一些低资源语言，如Bodo、Mizo、Nagamese等，这些系统的研究要么尚未开始，要么处于起步阶段。语言模型在现代自然语言处理的下游任务中起着重要作用。对于资源丰富的语言已经进行了大量的语言模型研究。然而，像Bodo、Rabha和Mising这样的语言仍然缺乏相关研究。本研究首先提出了BodoBERT，一个用于Bodo语的语言模型。据我们所知，这项工作是第一个为Bodo开发语言模型的努力。其次，我们提出了一个基于集成深度学习的Bodo词性标注模型。该词性标注模型基于BiLSTM和CRF的组合，以及BodoBERT与BytePairEmbeddings的堆叠嵌入。我们涵盖了多种语言...（摘要被截断）

    Language Processing systems such as Part-of-speech tagging, Named entity recognition, Machine translation, Speech recognition, and Language modeling (LM) are well-studied in high-resource languages. Nevertheless, research on these systems for several low-resource languages, including Bodo, Mizo, Nagamese, and others, is either yet to commence or is in its nascent stages. Language model plays a vital role in the downstream tasks of modern NLP. Extensive studies are carried out on LMs for high-resource languages. Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack coverage. In this study, we first present BodoBERT, a language model for the Bodo language. To the best of our knowledge, this work is the first such effort to develop a language model for Bodo. Secondly, we present an ensemble DL-based POS tagging model for Bodo. The POS tagging model is based on combinations of BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We cover several lan
    
[^30]: UGGNet：连接U-Net和VGG以提高乳腺癌诊断的先进性

    UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis. (arXiv:2401.03173v1 [eess.IV])

    [http://arxiv.org/abs/2401.03173](http://arxiv.org/abs/2401.03173)

    UGGNet通过结合U-Net和VGG架构，提供了一种全面解决方案来增强乳腺超声图像分析的性能，从而实现了更准确的乳腺癌诊断。

    

    在医学影像领域，乳腺超声成为早期乳腺癌检测的重要诊断工具。然而，准确诊断受到医生经验的影响，尤其是确定受影响区域位置和病变程度的准确性。本文提出了一种名为UGGNet的新型模型，结合了U-Net和VGG架构的优势，以增强乳腺超声图像分析的性能。模型的U-Net组件可以精确分割病变，而VGG组件利用深度卷积层提取特征。UGGNet中这两种架构的融合旨在优化分割和特征表示，为乳腺超声图像的准确诊断提供了全面的解决方案。实验结果表明，UGGNet模型在“乳腺超声图像数据集”上取得了78.2％的显著准确率。

    In the field of medical imaging, breast ultrasound has emerged as a crucial diagnostic tool for early detection of breast cancer. However, the accuracy of diagnosing the location of the affected area and the extent of the disease depends on the experience of the physician. In this paper, we propose a novel model called UGGNet, combining the power of the U-Net and VGG architectures to enhance the performance of breast ultrasound image analysis. The U-Net component of the model helps accurately segment the lesions, while the VGG component utilizes deep convolutional layers to extract features. The fusion of these two architectures in UGGNet aims to optimize both segmentation and feature representation, providing a comprehensive solution for accurate diagnosis in breast ultrasound images. Experimental results have demonstrated that the UGGNet model achieves a notable accuracy of 78.2% on the "Breast Ultrasound Images Dataset."
    
[^31]: 基于人口普查调查和一般生活问题的青少年抑郁风险预测探索

    Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues. (arXiv:2401.03171v1 [cs.LG])

    [http://arxiv.org/abs/2401.03171](http://arxiv.org/abs/2401.03171)

    本研究使用青少年人口普查数据来预测抑郁风险，关注儿童对抑郁的经历和日常生活情况。

    

    在当代社会，生活和工作的压力不断增加，心理疾病已成为现代健康关注的重要问题，而COVID-19疫情进一步凸显了这个问题。青少年抑郁的患病率正在稳步上升，而传统的诊断方法，如量表或面试，对于检测青少年抑郁显得尤为不足。面对这些挑战，出现了许多基于人工智能的方法来辅助诊断心理健康问题。然而，大多数这些方法都集中在量表存在的基本问题上，或者采用多模态方法，如面部表情识别。根据日常习惯和行为来诊断抑郁风险的研究局限于小规模的定性研究。我们的研究利用青少年人口普查数据来预测抑郁风险，重点关注儿童对抑郁的经历和他们的日常生活情况。我们引入了一种方法

    In contemporary society, the escalating pressures of life and work have propelled psychological disorders to the forefront of modern health concerns, an issue that has been further accentuated by the COVID-19 pandemic. The prevalence of depression among adolescents is steadily increasing, and traditional diagnostic methods, which rely on scales or interviews, prove particularly inadequate for detecting depression in young people. Addressing these challenges, numerous AI-based methods for assisting in the diagnosis of mental health issues have emerged. However, most of these methods center around fundamental issues with scales or use multimodal approaches like facial expression recognition. Diagnosis of depression risk based on everyday habits and behaviors has been limited to small-scale qualitative studies. Our research leverages adolescent census data to predict depression risk, focusing on children's experiences with depression and their daily life situations. We introduced a method
    
[^32]: 保留静默特征用于领域泛化的研究

    Preserving Silent Features for Domain Generalization. (arXiv:2401.03170v1 [cs.LG])

    [http://arxiv.org/abs/2401.03170](http://arxiv.org/abs/2401.03170)

    本研究发现，在领域泛化(DG)设置中，自监督模型不如有监督模型在泛化性能上表现得好。我们提出了静默特征的概念，认为保留这些静默特征可以降低测试领域风险。

    

    领域泛化(DG)旨在提高模型在未知测试领域上的泛化能力，用已知的多个训练领域来训练模型。之前的研究表明，自监督对比预训练可以提高模型在下游任务上的鲁棒性。然而，在本文中，我们发现自监督模型在DG设置中并没有比在同一数据集上进行预训练的有监督模型展现出更好的泛化性能。我们认为这是由于自监督对比学习提取的更丰富的类内区分特征，即我们所称的静默特征，在有监督微调过程中被抑制了。这些静默特征很可能包含了更具泛化性的特征。在本研究中，我们对这种特征抑制现象进行建模和分析，并在一定条件下理论上证明保留静默特征可以实现更低的预期测试领域风险。

    Domain generalization (DG) aims to improve the generalization ability of the model trained on several known training domains over unseen test domains. Previous work has shown that self-supervised contrastive pre-training improves the robustness of the model on downstream tasks. However, in this paper, we find that self-supervised models do not exhibit better generalization performance than supervised models pre-trained on the same dataset in the DG setting. We argue that this is owing to the fact that the richer intra-class discriminative features extracted by self-supervised contrastive learning, which we term silent features, are suppressed during supervised fine-tuning. These silent features are likely to contain features that are more generalizable on the test domain. In this work, we model and analyze this feature suppression phenomenon and theoretically prove that preserving silent features can achieve lower expected test domain risk under certain conditions. In light of this, we
    
[^33]: 基于价值的多目标强化学习在随机环境中的实证研究

    An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning for Stochastic Environments. (arXiv:2401.03163v1 [cs.LG])

    [http://arxiv.org/abs/2401.03163](http://arxiv.org/abs/2401.03163)

    本文针对利用Q值与效用函数扩展传统Q学习解决多目标强化学习问题的方法，在随机环境中存在问题。通过实证研究核心算法变体和奖励工程方法，发现噪声Q值估计问题对算法的稳定性和收敛性有关键影响。

    

    解决多目标强化学习问题的一种常见方法是通过使用矢量Q值与效用函数相结合来扩展传统的Q学习。然而，在随机环境下，尤其是在优化标量化预期回报（SER）准则时，这种方法可能会出现问题。本文扩展了之前的研究，对影响基于价值的多目标强化学习Q学习算法学习SER最优策略频率的因素进行了详细考察。我们经验性地研究了几种核心多目标Q学习算法的变体以及奖励工程方法，并展示了这些方法的局限性。特别是，我们强调了噪声Q值估计问题对这些算法的稳定性和收敛性的重要影响。

    One common approach to solve multi-objective reinforcement learning (MORL) problems is to extend conventional Q-learning by using vector Q-values in combination with a utility function. However issues can arise with this approach in the context of stochastic environments, particularly when optimising for the Scalarised Expected Reward (SER) criterion. This paper extends prior research, providing a detailed examination of the factors influencing the frequency with which value-based MORL Q-learning algorithms learn the SER-optimal policy for an environment with stochastic state transitions. We empirically examine several variations of the core multi-objective Q-learning algorithm as well as reward engineering approaches, and demonstrate the limitations of these methods. In particular, we highlight the critical impact of the noisy Q-value estimates issue on the stability and convergence of these algorithms.
    
[^34]: QoS感知的图对比学习用于网络服务推荐

    QoS-Aware Graph Contrastive Learning for Web Service Recommendation. (arXiv:2401.03162v1 [cs.IR])

    [http://arxiv.org/abs/2401.03162](http://arxiv.org/abs/2401.03162)

    本研究提出了一种名为QoS感知的图对比学习（QAGCL）的新方法，通过构建具有地理位置信息和随机性的上下文增强图来解决网络服务推荐中的数据稀疏性和冷启动问题，并有效提高推荐准确性。

    

    随着网络服务技术的进步，云服务的快速增长使得从众多选项中选择高质量的服务变得复杂。本研究旨在通过质量服务（QoS）解决网络服务推荐中的数据稀疏性和冷启动问题。我们提出了一种名为QoS感知的图对比学习（QAGCL）的新方法来进行网络服务推荐。我们的模型利用图对比学习的能力来处理冷启动问题并有效提高推荐准确性。通过构建具有地理位置信息和随机性的上下文增强图，我们的模型提供多样化的视角。通过使用图卷积网络和图对比学习技术，我们从这些增强图中学习用户和服务的嵌入。然后利用学到的嵌入将QoS考虑无缝集成到推荐过程中。实验结果表明，我们的方法在解决数据稀疏性和冷启动问题的同时显著提高了推荐质量。

    With the rapid growth of cloud services driven by advancements in web service technology, selecting a high-quality service from a wide range of options has become a complex task. This study aims to address the challenges of data sparsity and the cold-start problem in web service recommendation using Quality of Service (QoS). We propose a novel approach called QoS-aware graph contrastive learning (QAGCL) for web service recommendation. Our model harnesses the power of graph contrastive learning to handle cold-start problems and improve recommendation accuracy effectively. By constructing contextually augmented graphs with geolocation information and randomness, our model provides diverse views. Through the use of graph convolutional networks and graph contrastive learning techniques, we learn user and service embeddings from these augmented graphs. The learned embeddings are then utilized to seamlessly integrate QoS considerations into the recommendation process. Experimental results de
    
[^35]: 人作为AI导师：增强人机协作强化学习以实现安全高效的自动驾驶

    Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])

    [http://arxiv.org/abs/2401.03160](http://arxiv.org/abs/2401.03160)

    本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。

    

    尽管自动驾驶车辆（AVs）取得了重大进展，但确保AVs的安全性和交通流效率的驾驶策略的发展尚未得到充分探索。在本文中，我们提出了一种增强的人机协作强化学习方法，称为基于人作为AI导师的深度强化学习（HAIM-DRL）框架，以在混合交通编队中实现安全高效的自动驾驶。从人类学习过程中汲取灵感，我们首先引入了一种创新的学习范式，有效地将人类智能注入到AI中，称为人作为AI导师（HAIM）。在这个范式中，人类专家作为导师为AI代理提供帮助。在允许代理在不确定环境中进行充分探索的同时，人类专家可以在危险情况下接管控制，并展示正确的行动以避免潜在事故。另一方面，可以指导代理减小交通流干扰，从而优化交通流效果。

    Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
    
[^36]: 在辅助物联网车联网中具有多目标的分布式客户端选择

    Distributed client selection with multi-objective in federated learning assisted Internet of Vehicles. (arXiv:2401.03159v1 [cs.LG])

    [http://arxiv.org/abs/2401.03159](http://arxiv.org/abs/2401.03159)

    本文提出了一个分布式客户端选择方案，以减少车联网中保持活动状态的成本。通过选择邻居中评估最高的客户端，考虑样本数量、吞吐量、计算能力和本地数据集质量，我们采用模糊逻辑作为评估器。仿真结果显示我们的提议接近c

    

    联邦学习是车联网中新兴的分布式机器学习框架。在车联网中，数百万辆车愿意训练模型以共享它们的知识。保持活动状态意味着参与者必须在固定间隔内更新其状态并参与到下一轮中。然而，当有大量参与的车辆时，保持活动状态的成本是非常大的。在本文中，我们提出了一种分布式客户端选择方案，以减少所有参与者维护活动状态的成本。选择具有最高评估的邻居客户端。在评估器中，考虑了四个变量，包括样本数量、可用吞吐量、计算能力和本地数据集的质量。由于四个变量的闭式解不存在，我们采用模糊逻辑作为评估器。广泛的仿真结果显示我们的提议与c

    Federated learning is an emerging distributed machine learning framework in the Internet of Vehicles (IoV). In IoV, millions of vehicles are willing to train the model to share their knowledge. Maintaining an active state means the participants must update their state to the FL server in a fixed interval and participate to next round. However, the cost by maintaining an active state is very large when there are a huge number of participating vehicles. In this paper, we proposed a distributed client selection scheme to reduce the cost of maintaining the active state for all participants. The clients with the highest evaluation are elected among the neighbours. In the evaluator, four variables are considered including sample quantity, throughput available, computational capability and the quality of the local dataset. We adopted fuzzy logic as the evaluator since the closed-form solution over four variables does not exist. Extensive simulation results show our proposal approximates the c
    
[^37]: 对抗训练的数据相关稳定性分析

    Data-Dependent Stability Analysis of Adversarial Training. (arXiv:2401.03156v1 [cs.LG])

    [http://arxiv.org/abs/2401.03156](http://arxiv.org/abs/2401.03156)

    本文提出了基于随机梯度下降的对抗训练的泛化界限，包含了数据分布的信息，通过研究数据分布和对抗预算的变化对鲁棒泛化差距的影响，得到了与均匀稳定性相当的泛化界限。

    

    稳定性分析是研究深度学习泛化能力的重要方面，它涉及到基于随机梯度下降的训练算法的推导泛化界限。对抗训练是最常用的对抗性示例攻击防御方法。然而，先前的对抗训练泛化界限并未包含数据分布的信息。本文填补了这一空白，提供了基于随机梯度下降的对抗训练的泛化界限，并纳入了数据分布信息。我们利用平均稳定性和高阶近似Lipschitz条件的概念，研究数据分布和对抗预算的变化如何影响鲁棒泛化差距。我们得到的凸损失和非凸损失的泛化界限至少与不包含数据分布信息的均匀稳定性相当。

    Stability analysis is an essential aspect of studying the generalization ability of deep learning, as it involves deriving generalization bounds for stochastic gradient descent-based training algorithms. Adversarial training is the most widely used defense against adversarial example attacks. However, previous generalization bounds for adversarial training have not included information regarding the data distribution. In this paper, we fill this gap by providing generalization bounds for stochastic gradient descent-based adversarial training that incorporate data distribution information. We utilize the concepts of on-average stability and high-order approximate Lipschitz conditions to examine how changes in data distribution and adversarial budget can affect robust generalization gaps. Our derived generalization bounds for both convex and non-convex losses are at least as good as the uniform stability-based counterparts which do not include data distribution information. Furthermore, 
    
[^38]: 分散式多智能体主动搜索和跟踪当目标超过智能体数量时

    Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])

    [http://arxiv.org/abs/2401.03154](http://arxiv.org/abs/2401.03154)

    该论文介绍了一种处理多目标跟踪的分散式多智能体算法，该算法能在智能体数量少于目标数量时实现主动搜索和跟踪，并使用异步智能体通信来协调动作。

    

    多智能体多目标跟踪在野生动物巡逻、安全监控或环境监测等领域有广泛应用。现有算法常常做出一些限制性假设：目标数量和初始位置已知，或者智能体已被预分配到监控环境的不重叠分区，减轻了探索的负担。然而，当智能体数量少于目标数量时，这种假设会限制算法的适用性，因为智能体无法持续跟踪其视野中的目标。此外，多智能体跟踪算法还假设智能体间观测的同步，或者需要一个中央控制器来协调联合动作。相反，我们关注于分散式多智能体、多目标、同时主动搜索和跟踪的设置，其中智能体间通信是异步的。我们提出的算法DecSTER使用了一种基于概率假设密度滤波器的顺序蒙特卡洛实现。

    Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the probability hypothesis density filter fo
    
[^39]: 使用稳定扩散的工业数据可控图像合成

    Controllable Image Synthesis of Industrial Data Using Stable Diffusion. (arXiv:2401.03152v1 [cs.CV])

    [http://arxiv.org/abs/2401.03152](http://arxiv.org/abs/2401.03152)

    该论文提出了一种新的方法，使用稳定扩散技术在工业数据上实现可控的图像合成，从而允许生成自标记的有缺陷的图像。这种方法可以针对工业数据训练通用的预训练生成模型，并实现生成满足特定拓扑特征和几何缺陷位置要求的工业图像。

    

    训练监督式深度神经网络来进行缺陷检测和分割需要大规模完全注释的数据集，在工业环境中这种数据集往往很难获得甚至不可能。生成式人工智能提供了一种扩大小规模工业数据集的机会，从而使得工业界能够使用最先进的监督式方法。不幸的是，即使是好的生成模型也需要大量的数据来训练，而工业数据集往往很小。我们提出了一种新的方法，可以在工业数据上重复使用通用的预训练生成模型，最终实现生成自标记的有缺陷的图像。首先，我们让模型学习新的概念，涉及新的数据分布。然后，我们迫使它学习在生成过程中进行条件限制，产生具有良好定义的拓扑特征并且具有给定几何形状和位置缺陷的工业图像。为了突出我们的方法的优势

    Training supervised deep neural networks that perform defect detection and segmentation requires large-scale fully-annotated datasets, which can be hard or even impossible to obtain in industrial environments. Generative AI offers opportunities to enlarge small industrial datasets artificially, thus enabling the usage of state-of-the-art supervised approaches in the industry. Unfortunately, also good generative models need a lot of data to train, while industrial datasets are often tiny. Here, we propose a new approach for reusing general-purpose pre-trained generative models on industrial data, ultimately allowing the generation of self-labelled defective images. First, we let the model learn the new concept, entailing the novel data distribution. Then, we force it to learn to condition the generative process, producing industrial images that satisfy well-defined topological characteristics and show defects with a given geometry and location. To highlight the advantage of our approach
    
[^40]: 通过DQN进行半监督学习用于日志异常检测

    Semi-supervised learning via DQN for log anomaly detection. (arXiv:2401.03151v1 [cs.SE])

    [http://arxiv.org/abs/2401.03151](http://arxiv.org/abs/2401.03151)

    本文提出了一种半监督的日志异常检测方法，命名为DQNLog，通过结合深度强化学习中的DQN算法，利用少量有标记的数据和大规模无标记的数据集，有效解决了数据不平衡和标记数量有限的问题，并且通过与异常环境交互和主动探索无标记的数据集，学习已知的异常并发现未知的异常。

    

    日志异常检测在保障现代软件系统的安全性和维护方面起着关键作用。目前，检测日志数据中的异常的主要方法是通过监督式异常检测。然而，现有的监督式方法往往依赖于有标记的数据，在实际情况下往往受到限制。本文提出了一种半监督的日志异常检测方法，结合深度强化学习中的DQN算法，称为DQNLog。DQNLog利用少量有标记的数据和大规模无标记的数据集，有效解决了数据不平衡和标记数量有限的问题。该方法不仅通过与偏向于异常的环境进行交互来学习已知的异常，还通过主动探索无标记的数据集来发现未知的异常。此外，DQNLog还引入了交叉熵损失项，防止在深度强化学习中出现模型过高估计的情况。

    Log anomaly detection plays a critical role in ensuring the security and maintenance of modern software systems. At present, the primary approach for detecting anomalies in log data is through supervised anomaly detection. Nonetheless, existing supervised methods heavily rely on labeled data, which can be frequently limited in real-world scenarios. In this paper, we propose a semi-supervised log anomaly detection method that combines the DQN algorithm from deep reinforcement learning, which is called DQNLog. DQNLog leverages a small amount of labeled data and a large-scale unlabeled dataset, effectively addressing the challenges of imbalanced data and limited labeling. This approach not only learns known anomalies by interacting with an environment biased towards anomalies but also discovers unknown anomalies by actively exploring the unlabeled dataset. Additionally, DQNLog incorporates a cross-entropy loss term to prevent model overestimation during Deep Reinforcement Learning (DRL). 
    
[^41]: 通过切换机制，在扩散模型中实现公平抽样

    Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])

    [http://arxiv.org/abs/2401.03140](http://arxiv.org/abs/2401.03140)

    本论文提出了一种称为“属性切换”的公平抽样机制，用于解决扩散模型中公平性的问题。通过在生成的数据中混淆敏感属性，该方法能够实现生成公平数据和保持数据效用的目标。

    

    扩散模型通过良好逼近潜在概率分布，在生成任务中展现了高效性。然而，扩散模型在公平性方面受到训练数据的内在偏差的放大。尽管扩散模型的抽样过程可以通过条件引导来控制，但之前的研究试图找到实证引导来实现定量公平性。为了解决这个限制，我们提出了一种称为“属性切换”机制的具有公平意识的抽样方法，用于扩散模型。在不需要额外训练的情况下，所提出的抽样方法可以在生成的数据中混淆敏感属性，而不依赖分类器。我们在两个关键方面从数学上证明了和实验证明了所提方法的有效性：(i)生成公平数据和(ii)保持生成数据的效用。

    Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
    
[^42]: TelTrans：通过多方面的图模型将多种类型的电信数据应用于交通评估和预测

    TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation and Prediction via Multifaceted Graph Modeling. (arXiv:2401.03138v1 [cs.LG])

    [http://arxiv.org/abs/2401.03138](http://arxiv.org/abs/2401.03138)

    这篇论文提出了使用地理单元交通流数据来改进交通评估和预测的方法。通过结合多变量、时间和空间方面的图神经网络，该方法在长期预测中取得了优越的准确率，并展示了将地理单元交通流整合到交通系统中的潜力。

    

    为了解决基于位置边界检测器的交通预测的局限性，我们提出了地理单元交通流（GCT）流，这是一种利用广泛的蜂窝交通覆盖来捕捉移动模式的新型数据来源。我们广泛的分析验证了它在交通领域的潜力。针对与车辆相关的GCT流预测，我们提出了一种图神经网络，结合了多变量、时间和空间方面，以提高准确率。实验证明我们的模型在长期预测中的优越性，我们还强调了将GCT流整合到交通系统中的潜力。

    To address the limitations of traffic prediction from location-bound detectors, we present Geographical Cellular Traffic (GCT) flow, a novel data source that leverages the extensive coverage of cellular traffic to capture mobility patterns. Our extensive analysis validates its potential for transportation. Focusing on vehicle-related GCT flow prediction, we propose a graph neural network that integrates multivariate, temporal, and spatial facets for improved accuracy. Experiments reveal our model's superiority over baselines, especially in long-term predictions. We also highlight the potential for GCT flow integration into transportation systems.
    
[^43]: SPQR:使用尖峰随机模型控制Q-集合的独立性，用于强化学习

    SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])

    [http://arxiv.org/abs/2401.03137](http://arxiv.org/abs/2401.03137)

    SPQR论文介绍了一种使用尖峰随机模型来控制强化学习中Q-集合的独立性的方法，通过引入基于随机矩阵理论的正则化损失来克服过高估计偏差。

    

    缓解过高估计偏差是深度强化学习在更复杂任务或包含超出分布数据的离线数据集上获得成功表现的关键挑战。为了克服过高估计偏差，研究了Q-learning的集成方法来利用多个Q函数的多样性。由于网络初始化一直是促进Q函数多样性的主要方法，因此在文献中研究了启发式设计的多样性注入方法。然而，先前的研究并未尝试从理论角度保证集成的独立性。通过引入基于随机矩阵理论的Q-集合独立性的新型正则化损失，我们提出了一种用于强化学习的尖峰Wishart Q-集合独立性正则化方法（SPQR）。

    Alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. In order to overcome overestimation bias, ensemble methods for Q-learning have been investigated to exploit the diversity of multiple Q-functions. Since network initialization has been the predominant approach to promote diversity in Q-functions, heuristically designed diversity injection methods have been studied in the literature. However, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning. Specifically, we modify the intractable hypothesis testing criterion for the Q-ensemble independence into a tractable KL divergence 
    
[^44]: TimeGraphs: 基于图的时间推理

    TimeGraphs: Graph-based Temporal Reasoning. (arXiv:2401.03134v1 [cs.LG])

    [http://arxiv.org/abs/2401.03134](http://arxiv.org/abs/2401.03134)

    TimeGraphs是一种基于图的时间推理方法，通过将动态交互建模为分层时间图，实现了对不同时间尺度的自适应推理。

    

    许多现实世界的系统展示了时间上的动态行为，这些行为可以被捕捉为复杂代理交互的时间序列。为了进行时间推理，目前的方法主要通过简单的序列模型来编码时间动态。然而，一般来说，这些模型在有效捕捉输入中丰富动态的全谱时往往失败，因为动态并不均匀分布。特别是，有关信息可能更难提取，并且即使它们不包含重大变化或新信息，也会浪费计算能力来处理所有单独的时间步长。在这里，我们提出了TimeGraphs，一种将动态交互作为分层时间图来表征的新方法，与传统的顺序表示不同。我们的方法使用紧凑的基于图的表示来建模交互，实现了对不同时间尺度的自适应推理。采用自监督方法，TimeGraphs构建了一个多层级事件层次结构

    Many real-world systems exhibit temporal, dynamic behaviors, which are captured as time series of complex agent interactions. To perform temporal reasoning, current methods primarily encode temporal dynamics through simple sequence-based models. However, in general these models fail to efficiently capture the full spectrum of rich dynamics in the input, since the dynamics is not uniformly distributed. In particular, relevant information might be harder to extract and computing power is wasted for processing all individual timesteps, even if they contain no significant changes or no new information. Here we propose TimeGraphs, a novel approach that characterizes dynamic interactions as a hierarchical temporal graph, diverging from traditional sequential representations. Our approach models the interactions using a compact graph-based representation, enabling adaptive reasoning across diverse time scales. Adopting a self-supervised method, TimeGraphs constructs a multi-level event hierar
    
[^45]: Vision Transformers和Bi-LSTM用于3D MRI的阿尔茨海默病诊断

    Vision Transformers and Bi-LSTM for Alzheimer's Disease Diagnosis from 3D MRI. (arXiv:2401.03132v1 [eess.IV])

    [http://arxiv.org/abs/2401.03132](http://arxiv.org/abs/2401.03132)

    本研究提出了使用Vision Transformers和Bi-LSTM处理MRI图像进行阿尔茨海默病诊断的方法，通过提取特征并保持特征之间的依赖关系，该方法在AD的诊断中取得了良好的准确性和性能。

    

    阿尔茨海默病是一种随时间恶化并影响记忆、思维和行为的大脑疾病。如果早期诊断，阿尔茨海默病（AD）是可以治疗和管理的，可以减缓症状的进展并改善生活质量。本研究建议使用Visual Transformer（ViT）和bi-LSTM来处理MRI图像进行阿尔茨海默病的诊断。我们使用ViT从MRI中提取特征，然后将其映射到特征序列。接着，我们使用Bi-LSTM序列建模来保持相关特征之间的相互依赖关系。另外，我们使用阿尔茨海默病神经影像计划（ADNI）的数据来评估所提出的模型对AD患者的二分类性能。最后，我们在文献中将我们的方法与其他深度学习模型进行了评估。该方法在AD的诊断准确度、精确度、F-score和召回率方面表现良好。

    Alzheimer's is a brain disease that gets worse over time and affects memory, thinking, and behavior. Alzheimer's disease (AD) can be treated and managed if it is diagnosed early, which can slow the progression of symptoms and improve quality of life. In this study, we suggested using the Visual Transformer (ViT) and bi-LSTM to process MRI images for diagnosing Alzheimer's disease. We used ViT to extract features from the MRI and then map them to a feature sequence. Then, we used Bi-LSTM sequence modeling to keep the interdependencies between related features. In addition, we evaluated the performance of the proposed model for the binary classification of AD patients using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our method against other deep learning models in the literature. The proposed method performs well in terms of accuracy, precision, F-score, and recall for the diagnosis of AD.
    
[^46]: 用物理指导的生成型人工智能工具包进行地球物理监测

    A Physics-guided Generative AI Toolkit for Geophysical Monitoring. (arXiv:2401.03131v1 [cs.LG])

    [http://arxiv.org/abs/2401.03131](http://arxiv.org/abs/2401.03131)

    EdGeo工具包利用物理原理指导的扩散模型生成高保真度的地下速度图，并通过使用声波方程生成地震波形数据来改善模型修剪后的ML模型性能。

    

    全波形反演(FWI)在地球科学中起着重要作用，用于探索地下。它利用地震波来成像地下速度图。随着机器学习(ML)技术的发展，使用ML的数据驱动方法已经出现，用于FWI任务，相比传统基于物理的方法，提供了更高的准确性和更低的计算成本。然而，地球科学中一个常见的挑战是没有数据，严重限制了ML的有效性。这个问题在模型修剪过程中变得更加严重，这是地球科学中必不可少的一步，因为环境复杂性。为了解决这个问题，我们介绍了EdGeo工具包，它使用物理原理引导的扩散模型来生成高保真度的速度图。该工具包使用声波方程来生成相应的地震波形数据，便于修剪后的ML模型进行微调。我们的结果表明，在各种情况下，SSIM分数都有显著提高，同时减少了MAE和MSE。

    Full-waveform inversion (FWI) plays a vital role in geoscience to explore the subsurface. It utilizes the seismic wave to image the subsurface velocity map. As the machine learning (ML) technique evolves, the data-driven approaches using ML for FWI tasks have emerged, offering enhanced accuracy and reduced computational cost compared to traditional physics-based methods. However, a common challenge in geoscience, the unprivileged data, severely limits ML effectiveness. The issue becomes even worse during model pruning, a step essential in geoscience due to environmental complexities. To tackle this, we introduce the EdGeo toolkit, which employs a diffusion-based model guided by physics principles to generate high-fidelity velocity maps. The toolkit uses the acoustic wave equation to generate corresponding seismic waveform data, facilitating the fine-tuning of pruned ML models. Our results demonstrate significant improvements in SSIM scores and reduction in both MAE and MSE across vario
    
[^47]: 一种使用深度神经网络的多元回归模型最小距离估计器

    A least distance estimator for a multivariate regression model using deep neural networks. (arXiv:2401.03123v1 [stat.ME])

    [http://arxiv.org/abs/2401.03123](http://arxiv.org/abs/2401.03123)

    我们提出了一种基于深度神经网络的最小距离估计器，用于多元回归问题，可以灵活建模线性和非线性条件均值函数，并具有更好的捕捉依赖关系和鲁棒性，同时考虑了变量选择在分析高维数据中的重要性。

    

    我们提出了一种基于深度神经网络(DNN)的最小距离(LD)估计器(DNN-LD)用于多元回归问题，解决了传统方法的局限性。由于DNN结构的灵活性，可以轻松建模线性和非线性条件均值函数，并且只需在输出层添加额外节点即可实现多元回归模型。该方法比最小二乘损失更有效地捕捉响应之间的依赖结构，并且对异常值具有鲁棒性。此外，我们考虑到变量选择在分析高维数据中的关键性，引入了L1型惩罚。具体而言，我们提出了我们称之为(A)GDNN-LD估计器，通过将(自适应)分组Lasso惩罚应用于DNN结构中的权重参数，同时实现了变量选择和模型估计。为了进行计算，我们提出了一种二次平滑近似方法，以便优化非光滑的目标函数。

    We propose a deep neural network (DNN) based least distance (LD) estimator (DNN-LD) for a multivariate regression problem, addressing the limitations of the conventional methods. Due to the flexibility of a DNN structure, both linear and nonlinear conditional mean functions can be easily modeled, and a multivariate regression model can be realized by simply adding extra nodes at the output layer. The proposed method is more efficient in capturing the dependency structure among responses than the least squares loss, and robust to outliers. In addition, we consider $L_1$-type penalization for variable selection, crucial in analyzing high-dimensional data. Namely, we propose what we call (A)GDNN-LD estimator that enjoys variable selection and model estimation simultaneously, by applying the (adaptive) group Lasso penalty to weight parameters in the DNN structure. For the computation, we propose a quadratic smoothing approximation method to facilitate optimizing the non-smooth objective fu
    
[^48]: 提升DDoS攻击检测：使用深度残差神经网络和合成过采样的协同方法

    Advancing DDoS Attack Detection: A Synergistic Approach Using Deep Residual Neural Networks and Synthetic Oversampling. (arXiv:2401.03116v1 [cs.CR])

    [http://arxiv.org/abs/2401.03116](http://arxiv.org/abs/2401.03116)

    本研究通过结合深度残差神经网络和合成过采样技术，提出了一种增强的DDoS攻击检测方法，能够更准确地辨别复杂的攻击模式。

    

    分布式拒绝服务（DDoS）攻击对在线系统的稳定性和可靠性构成重大威胁。有效和早期检测此类攻击对于保护网络的完整性至关重要。在这项工作中，我们通过利用深度残差神经网络（ResNets）和合成过采样技术的能力，引入了一种增强的DDoS攻击检测方法。由于许多网络安全数据集中存在固有的类别不平衡，传统方法常常在将细微的DDoS模式误判为良性时困难重重。通过将合成少数类过采样技术（SMOTE）应用于CICIDS数据集，我们平衡了良性和恶意数据点的表示，使模型能够更好地辨别攻击的复杂模式。我们针对这一特定任务定制的深度残差网络进一步完善了检测过程。对一个现实世界数据集的实验结果表明，我们的方法在DDoS攻击检测方面取得了良好的效果。

    Distributed Denial of Service (DDoS) attacks pose a significant threat to the stability and reliability of online systems. Effective and early detection of such attacks is pivotal for safeguarding the integrity of networks. In this work, we introduce an enhanced approach for DDoS attack detection by leveraging the capabilities of Deep Residual Neural Networks (ResNets) coupled with synthetic oversampling techniques. Because of the inherent class imbalance in many cyber-security datasets, conventional methods often struggle with false negatives, misclassifying subtle DDoS patterns as benign. By applying the Synthetic Minority Over-sampling Technique (SMOTE) to the CICIDS dataset, we balance the representation of benign and malicious data points, enabling the model to better discern intricate patterns indicative of an attack. Our deep residual network, tailored for this specific task, further refines the detection process. Experimental results on a real-world dataset demonstrate that our
    
[^49]: GLISP：通过利用图的固有结构属性的可扩展GNN学习系统

    GLISP: A Scalable GNN Learning System by Exploiting Inherent Structural Properties of Graphs. (arXiv:2401.03114v1 [cs.LG])

    [http://arxiv.org/abs/2401.03114](http://arxiv.org/abs/2401.03114)

    GLISP是一种可扩展的GNN学习系统，通过利用图的结构属性来解决在工业规模图上应用GNN时遇到的可扩展性和性能问题。

    

    作为一种建模图数据的强大工具，图神经网络（GNN）在学术界和工业界都受到了越来越多的关注。然而，由于数据规模巨大和复杂的拓扑结构，将GNN应用于工业规模的图中仍然非常困难。在本文中，我们提出了GLISP，一种用于工业规模图的采样型GNN学习系统。通过利用图的固有结构属性，如幂律分布和数据局部性，GLISP解决了图学习过程中不同阶段产生的可扩展性和性能问题。GLISP由三个核心组件组成：图分区器、图采样服务和图推理引擎。图分区器采用了提出的顶点切割图分区算法AdaDNE，为基于采样的GNN系统产生了平衡的分区，这对于采样型GNN系统至关重要。图采样服务采用了负载均衡设计，允许一些服务进程处理计算和存储负载，从而提高了系统的效率和可扩展性。

    As a powerful tool for modeling graph data, Graph Neural Networks (GNNs) have received increasing attention in both academia and industry. Nevertheless, it is notoriously difficult to deploy GNNs on industrial scale graphs, due to their huge data size and complex topological structures. In this paper, we propose GLISP, a sampling based GNN learning system for industrial scale graphs. By exploiting the inherent structural properties of graphs, such as power law distribution and data locality, GLISP addresses the scalability and performance issues that arise at different stages of the graph learning process. GLISP consists of three core components: graph partitioner, graph sampling service and graph inference engine. The graph partitioner adopts the proposed vertex-cut graph partitioning algorithm AdaDNE to produce balanced partitioning for power law graphs, which is essential for sampling based GNN systems. The graph sampling service employs a load balancing design that allows the one h
    
[^50]: 何时进行神经增长？一种适应风险感知的神经网络层增长策略

    When To Grow? A Fitting Risk-Aware Policy for Layer Growing in Deep Neural Networks. (arXiv:2401.03104v1 [cs.LG])

    [http://arxiv.org/abs/2401.03104](http://arxiv.org/abs/2401.03104)

    本研究发现神经增长具有正则化效果，但现有方法未考虑这一效果。基于此，我们提出了一种风险感知的增长时机策略，以应对欠拟合和过拟合风险。

    

    神经增长是将小型神经网络扩展为大型网络的过程，已被用于加速深度神经网络的训练。神经增长的一个关键方面是确定最佳增长时机。然而，很少有研究系统地调查这一问题。我们的研究发现，神经增长本质上具有一种正则化效果，其强度受增长时机策略的影响。尽管这种正则化效果可以缓解模型的过拟合风险，但在模型欠拟合时可能导致显著的准确度下降。然而，由于当前方法未考虑神经增长的正则化效果，因此未解决这一问题。受这些发现的启发，我们提出了一种风险感知的增长时机策略，根据潜在的欠拟合/过拟合风险水平自动调整增长时机，以应对两种风险。通过使用CIFAR-10/10进行全面的实验验证了这种策略的有效性。

    Neural growth is the process of growing a small neural network to a large network and has been utilized to accelerate the training of deep neural networks. One crucial aspect of neural growth is determining the optimal growth timing. However, few studies investigate this systematically. Our study reveals that neural growth inherently exhibits a regularization effect, whose intensity is influenced by the chosen policy for growth timing. While this regularization effect may mitigate the overfitting risk of the model, it may lead to a notable accuracy drop when the model underfits. Yet, current approaches have not addressed this issue due to their lack of consideration of the regularization effect from neural growth. Motivated by these findings, we propose an under/over fitting risk-aware growth timing policy, which automatically adjusts the growth timing informed by the level of potential under/overfitting risks to address both risks. Comprehensive experiments conducted using CIFAR-10/10
    
[^51]: 具有公平感知重加权技术的自适应Boosting用于公平分类

    Adaptive Boosting with Fairness-aware Reweighting Technique for Fair Classification. (arXiv:2401.03097v1 [cs.LG])

    [http://arxiv.org/abs/2401.03097](http://arxiv.org/abs/2401.03097)

    该论文提出了一种公平AdaBoost（FAB）方法，通过公平感知的基分类器重加权技术，实现了在保持AdaBoost优势的同时实现公平分类，而对预测性能几乎没有牺牲。

    

    基于AdaBoost的机器学习方法已广泛应用于包括医疗、法律和金融等多个关键应用领域的各种分类问题。然而，针对经典算法如AdaBoost，人们越来越关注数据驱动的分类模型的不公平和歧视问题。为了实现公平分类，提出了一种新的公平AdaBoost（FAB）方法，它是AdaBoost的可解释性改进变体。我们主要研究二元分类问题，并关注三种不同指标的公平性（即准确率、假阳性率和假阴性率）。通过利用一种公平感知的基分类器重加权技术，提出的FAB方法可以在保持AdaBoost优势的同时实现公平分类，而对预测性能几乎没有牺牲。另外，引入了FAB中的一个超参数，用于展示对公平性的偏好。

    Machine learning methods based on AdaBoost have been widely applied to various classification problems across many mission-critical applications including healthcare, law and finance. However, there is a growing concern about the unfairness and discrimination of data-driven classification models, which is inevitable for classical algorithms including AdaBoost. In order to achieve fair classification, a novel fair AdaBoost (FAB) approach is proposed that is an interpretable fairness-improving variant of AdaBoost. We mainly investigate binary classification problems and focus on the fairness of three different indicators (i.e., accuracy, false positive rate and false negative rate). By utilizing a fairness-aware reweighting technique for base classifiers, the proposed FAB approach can achieve fair classification while maintaining the advantage of AdaBoost with negligible sacrifice of predictive performance. In addition, a hyperparameter is introduced in FAB to show preferences for the fa
    
[^52]: 离线签名验证中使用卷积神经网络学习表示的一致阈值准则

    Consensus-Threshold Criterion for Offline Signature Verification using Convolutional Neural Network Learned Representations. (arXiv:2401.03085v1 [cs.CV])

    [http://arxiv.org/abs/2401.03085](http://arxiv.org/abs/2401.03085)

    本论文提出了一种离线签名验证的一致阈值准则，通过使用卷积神经网络模型提取的特征，可以减少伪造签名的误识率。

    

    真实签名即使在短时间间隔内也是自然不稳定的，而专家级伪造者总是试图完美模仿真实签名。这就提出了一个挑战，使得真实签名者面临被拒绝访问的风险，而伪造签名者则被授权访问。这意味着高误识率（FAR），即被错误分类为属于真实类别的伪造签名的百分比。现有工作只是浅尝辄止地进行了签名验证，因为误分类错误仍然很高。本文提出了一种离线写字者依赖的签名验证的一致阈值基于距离的分类器准则。通过使用从SigNet和SigNet-F深度卷积神经网络模型中提取的特征，所提出的分类器最小化了FAR。通过对四个数据集：GPDS-300、MCYT、CEDAR和巴西PUC-PR数据集进行实验来证明这一点。在GPDS-300上，一致阈值分类器改进了现有的状态。

    A genuine signer's signature is naturally unstable even at short time-intervals whereas, expert forgers always try to perfectly mimic a genuine signer's signature. This presents a challenge which puts a genuine signer at risk of being denied access, while a forge signer is granted access. The implication is a high false acceptance rate (FAR) which is the percentage of forge signature classified as belonging to a genuine class. Existing work have only scratched the surface of signature verification because the misclassification error remains high. In this paper, a consensus-threshold distance-based classifier criterion is proposed for offline writer-dependent signature verification. Using features extracted from SigNet and SigNet-F deep convolutional neural network models, the proposed classifier minimizes FAR. This is demonstrated via experiments on four datasets: GPDS-300, MCYT, CEDAR and Brazilian PUC-PR datasets. On GPDS-300, the consensus threshold classifier improves the state-of-
    
[^53]: 基于图稀疏化的能效优化去中心化学习

    Energy-efficient Decentralized Learning via Graph Sparsification. (arXiv:2401.03083v1 [cs.LG])

    [http://arxiv.org/abs/2401.03083](http://arxiv.org/abs/2401.03083)

    本文通过图稀疏化的方法，优化了混合矩阵，以提高去中心化学习的能效。在特殊和一般情况下，分别提出了有保证性能和贪心启发式算法的解决方案，并在仿真实验中验证了其有效性。

    

    本文旨在通过优化混合矩阵来提高去中心化学习的能效，混合矩阵控制了学习过程中的通信需求。通过对最先进的去中心化学习算法进行严格分析，将问题表示为双层优化，其中底层通过图稀疏化解决。在完全连接的基础拓扑结构的特殊情况下，提出了一种具有性能保证的解决方案，并针对一般情况提出了一种贪心启发式算法。基于真实拓扑结构和数据集的仿真结果表明，所提出的解决方案可以将最繁忙节点的能耗降低54%-76%，同时保持训练模型的质量。

    This work aims at improving the energy efficiency of decentralized learning by optimizing the mixing matrix, which controls the communication demands during the learning process. Through rigorous analysis based on a state-of-the-art decentralized learning algorithm, the problem is formulated as a bi-level optimization, with the lower level solved by graph sparsification. A solution with guaranteed performance is proposed for the special case of fully-connected base topology and a greedy heuristic is proposed for the general case. Simulations based on real topology and dataset show that the proposed solution can lower the energy consumption at the busiest node by 54%-76% while maintaining the quality of the trained model.
    
[^54]: StreamVC：实时低延迟语音转换

    StreamVC: Real-Time Low-Latency Voice Conversion. (arXiv:2401.03078v1 [eess.AS])

    [http://arxiv.org/abs/2401.03078](http://arxiv.org/abs/2401.03078)

    StreamVC是一种实时低延迟语音转换解决方案，可以在移动平台上进行实时通信，并实现声音匿名化。它保留了源语音的内容和韵律，同时匹配了目标语音的音质，并且通过软语音单元的学习和提供白化的基频信息来改善音高稳定性。

    

    我们提出了StreamVC，这是一个流式语音转换解决方案，能够保留任何源语音的内容和韵律，同时匹配任何目标语音的音质。与之前的方法不同，StreamVC甚至可以在移动平台上以低延迟从输入信号中生成结果波形，这使其适用于实时通信场景，如电话和视频会议，并解决这些场景下的声音匿名化等用例。我们的设计充分利用了SoundStream神经音频编解码器的架构和训练策略，实现了轻量级高质量的语音合成。我们演示了学习软语音单元的可行性，以及通过提供白化的基频信息来改善音高稳定性而不泄漏源音质信息的有效性。

    We present StreamVC, a streaming voice conversion solution that preserves the content and prosody of any source speech while matching the voice timbre from any target speech. Unlike previous approaches, StreamVC produces the resulting waveform at low latency from the input signal even on a mobile platform, making it applicable to real-time communication scenarios like calls and video conferencing, and addressing use cases such as voice anonymization in these scenarios. Our design leverages the architecture and training strategy of the SoundStream neural audio codec for lightweight high-quality speech synthesis. We demonstrate the feasibility of learning soft speech units causally, as well as the effectiveness of supplying whitened fundamental frequency information to improve pitch stability without leaking the source timbre information.
    
[^55]: 一种面向拓扑感知的连续图学习的图减粗框架

    A Topology-aware Graph Coarsening Framework for Continual Graph Learning. (arXiv:2401.03077v1 [cs.LG])

    [http://arxiv.org/abs/2401.03077](http://arxiv.org/abs/2401.03077)

    提出了一种面向拓扑感知的连续图学习框架TA$\mathbb{CO}$，通过图减粗和连续学习的方法，解决了传统连续学习策略在处理流式图时的低效率和无法捕捉任务关联性的问题。

    

    图上的连续学习解决了在数据以流式方式到达且模型在更新时容易忘记以前任务的知识的图神经网络（GNN）的训练问题。传统的连续学习策略，如经验回放，可以适应流式图，但是，这些方法通常面临保留图拓扑的低效率和捕捉旧任务与新任务之间关联性的无能等挑战。为了解决这些挑战，我们提出了TA$\mathbb{CO}$，一个面向拓扑感知的图减粗和连续学习框架，将以前任务的信息存储为一个减少的图。在每个时间段，这个减少的图通过与新图合并并对齐共享节点来扩展，然后进行“缩小”过程以保持稳定的大小。我们设计了一种基于节点表示接近度的图减粗算法来高效地减少图的大小。

    Continual learning on graphs tackles the problem of training a graph neural network (GNN) where graph data arrive in a streaming fashion and the model tends to forget knowledge from previous tasks when updating with new data. Traditional continual learning strategies such as Experience Replay can be adapted to streaming graphs, however, these methods often face challenges such as inefficiency in preserving graph topology and incapability of capturing the correlation between old and new tasks. To address these challenges, we propose TA$\mathbb{CO}$, a (t)opology-(a)ware graph (co)arsening and (co)ntinual learning framework that stores information from previous tasks as a reduced graph. At each time period, this reduced graph expands by combining with a new graph and aligning shared nodes, and then it undergoes a "zoom out" process by reduction to maintain a stable size. We design a graph coarsening algorithm based on node representation proximities to efficiently reduce a graph and pres
    
[^56]: 提高深度学习Bug可复现性的探索性研究

    Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study. (arXiv:2401.03069v1 [cs.SE])

    [http://arxiv.org/abs/2401.03069](http://arxiv.org/abs/2401.03069)

    本研究旨在提高深度学习Bug的可复现性，通过构建数据集和确定编辑动作和有用信息，这能够解决目前研究中忽视的问题。

    

    背景：深度学习在各个领域取得了显著进展。然而，与传统软件系统一样，深度学习系统也存在Bug，这可能对自动驾驶等领域产生严重影响。尽管深度学习技术取得了重大进展，但很少有研究关注深度学习Bug的可复现性，这阻碍了Bug的解决。现有文献指出，仅有3%的深度学习Bug是可复现的，这凸显了进一步研究的必要性。目标：本文考察深度学习Bug的可复现性，识别可提高深度学习Bug可复现性的编辑动作和有用信息。方法：首先，构建了一个包含来自Stack Overflow和Defects4ML的3个框架和22个架构的668个深度学习Bug的数据集。其次，使用分层抽样选择了102个Bug，并尝试确定它们的可复现性。在复现这些Bug的过程中，我们识别了编辑动作和有用信息。

    Context: Deep learning has achieved remarkable progress in various domains. However, like traditional software systems, deep learning systems contain bugs, which can have severe impacts, as evidenced by crashes involving autonomous vehicles. Despite substantial advancements in deep learning techniques, little research has focused on reproducing deep learning bugs, which hinders resolving them. Existing literature suggests that only 3% of deep learning bugs are reproducible, underscoring the need for further research.  Objective: This paper examines the reproducibility of deep learning bugs. We identify edit actions and useful information that could improve deep learning bug reproducibility.  Method: First, we construct a dataset of 668 deep learning bugs from Stack Overflow and Defects4ML across 3 frameworks and 22 architectures. Second, we select 102 bugs using stratified sampling and try to determine their reproducibility. While reproducing these bugs, we identify edit actions and us
    
[^57]: CRUXEval: 一个代码推理、理解和执行的基准测试

    CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. (arXiv:2401.03065v1 [cs.SE])

    [http://arxiv.org/abs/2401.03065](http://arxiv.org/abs/2401.03065)

    CRUXEval是一个包含800个Python函数的代码推理、理解和执行的基准测试。通过评估二十个代码模型，发现许多在HumanEval上得分高的模型在该基准测试上没有相同的改进。使用CoT的GPT-4展现了最佳性能，但仍未解决问题。与开源模型相比，闭源模型的性能差距更大。

    

    我们提出了CRUXEval（代码推理、理解和执行评估），一个包含800个Python函数（3-13行）的基准测试。每个函数都有一个输入输出对，可以进行两个自然任务：输入预测和输出预测。首先，我们提出了一个通用的生成执行基准测试的方法，可以用来创建未来变种的基准测试。其次，我们在我们的基准测试中评估了二十个代码模型，并发现许多最近在HumanEval上得分高的模型在我们的基准测试上没有取得相同的改进。第三，我们展示了简单的CoT和微调方案可以改善在我们的基准测试上的性能，但仍然远未解决问题。最佳设置，使用CoT的GPT-4，在输入和输出预测上的pass@1分别达到了75%和81%。相比之下，Code Llama 34B在输入和输出预测上的pass@1分别为50%和46%，突显了开源模型和闭源模型之间的差距。

    We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to 
    
[^58]: 为URLLC流量优化可靠性的用户接入控制：一种神经上下文强化学习方法

    Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach. (arXiv:2401.03059v1 [cs.LG])

    [http://arxiv.org/abs/2401.03059](http://arxiv.org/abs/2401.03059)

    本研究提出了一种基于神经上下文强化学习方法的QoS-aware UE接入控制策略，旨在在关联URRLC UE与小区之前，准确估计QoS并避免小区过载。

    

    超可靠低延迟通信（URLLC）是下一代无线网络中广泛范围的新兴服务的基石。URLLC基本上依赖于网络能够主动确定是否有足够的资源来支持URLLC流量，并因此防止所谓的小区过载。然而，为URLLC用户设备（UE）实现准确的服务质量（QoS）预测并防止小区超载是非常具有挑战性的任务。这是由于QoS指标（延迟和可靠性）依赖于流量和信道统计数据、用户的移动性和UE之间的相互依赖性。在本文中，开发了一种新的QoS感知UE接入控制方法，用于在将其与小区关联之前，主动估计URLLC UE的QoS，并相应地仅接纳不会导致小区过载的UE子集。为此，制定了一个优化问题来找到一种有效的UE接入控制策略，

    Ultra-reliable low-latency communication (URLLC) is the cornerstone for a broad range of emerging services in next-generation wireless networks. URLLC fundamentally relies on the network's ability to proactively determine whether sufficient resources are available to support the URLLC traffic, and thus, prevent so-called cell overloads. Nonetheless, achieving accurate quality-of-service (QoS) predictions for URLLC user equipment (UEs) and preventing cell overloads are very challenging tasks. This is due to dependency of the QoS metrics (latency and reliability) on traffic and channel statistics, users' mobility, and interdependent performance across UEs. In this paper, a new QoS-aware UE admission control approach is developed to proactively estimate QoS for URLLC UEs, prior to associating them with a cell, and accordingly, admit only a subset of UEs that do not lead to a cell overload. To this end, an optimization problem is formulated to find an efficient UE admission control policy,
    
[^59]: Krylov立方正则化牛顿法：具有无维收敛速度的子空间二阶方法

    Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate. (arXiv:2401.03058v1 [math.OC])

    [http://arxiv.org/abs/2401.03058](http://arxiv.org/abs/2401.03058)

    本论文提出了一种新的子空间立方正则化牛顿方法，可以在解决凸优化问题时实现无维度相关的全局收敛速度，通过在低维子空间上进行二阶更新，克服了高维问题中内存需求和计算成本大的问题。

    

    二阶优化算法，如立方正则化牛顿法，以其快速收敛速度而闻名；然而，在高维问题中，它们变得不实用，因为需要大量的内存和计算成本。一个有前景的方法是在低维子空间中执行二阶更新，从而产生子空间二阶方法。然而，现有的大多数子空间二阶方法随机选择子空间，因此收敛速度较慢，这取决于问题的维度d。在本文中，我们引入了一种新颖的子空间立方正则化牛顿方法，用于解决凸优化问题，其达到了一个维度无关的全局收敛速度，为${O}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$。这里，m表示子空间维度，可以显著小于d。我们的主要创新不是采用随机子空间，而是进行立方正则化...

    Second-order optimization methods, such as cubic regularized Newton methods, are known for their rapid convergence rates; nevertheless, they become impractical in high-dimensional problems due to their substantial memory requirements and computational costs. One promising approach is to execute second-order updates within a lower-dimensional subspace, giving rise to subspace second-order methods. However, the majority of existing subspace second-order methods randomly select subspaces, consequently resulting in slower convergence rates depending on the problem's dimension $d$. In this paper, we introduce a novel subspace cubic regularized Newton method that achieves a dimension-independent global convergence rate of ${O}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$ for solving convex optimization problems. Here, $m$ represents the subspace dimension, which can be significantly smaller than $d$. Instead of adopting a random subspace, our primary innovation involves performing the cubic regul
    
[^60]: 半无监督校准通过先验适应算法的收敛性研究

    On the Convergence of Semi Unsupervised Calibration through Prior Adaptation Algorithm. (arXiv:2401.03051v1 [cs.LG])

    [http://arxiv.org/abs/2401.03051](http://arxiv.org/abs/2401.03051)

    本文研究了一种半无监督校准算法，使用动力系统的观点证明了该算法的收敛性质，并通过实验验证了所提出的结果。

    

    校准是机器学习中的一个关键任务。Semi Unsupervised Calibration through Prior Adaptation (SUCPA)是一种校准算法，适用于大规模语言模型，其定义为一个一阶差分方程组。该方程组导出的映射具有非双曲性特征，并且存在一组非孤立不界定的固定点。本文从动力系统的视角证明了该算法的多个收敛性质。对于二类分类问题，可以证明该算法总是收敛，更具体地说，该映射是全局渐近稳定的，轨道收敛到一条固定点直线上。最后，我们对实际应用进行了数值实验，以支持所提出的结果。实验代码可在网上找到。

    Calibration is an essential key in machine leaning. Semi Unsupervised Calibration through Prior Adaptation (SUCPA) is a calibration algorithm used in (but not limited to) large-scale language models defined by a {system of first-order difference equation. The map derived by this system} has the peculiarity of being non-hyperbolic {with a non-bounded set of non-isolated fixed points}. In this work, we prove several convergence properties of this algorithm from the perspective of dynamical systems. For a binary classification problem, it can be shown that the algorithm always converges, {more precisely, the map is globally asymptotically stable, and the orbits converge} to a single line of fixed points. Finally, we perform numerical experiments on real-world application to support the presented results. Experiment codes are available online.
    
[^61]: AccidentGPT:用于交通事故分析的大型多模态基础模型

    AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis. (arXiv:2401.03040v1 [cs.LG])

    [http://arxiv.org/abs/2401.03040](http://arxiv.org/abs/2401.03040)

    本文介绍了AccidentGPT，这是一个用于交通事故分析的大型多模态基础模型。它利用多模态输入数据自动重建事故过程视频，并提供多模态输出的多任务分析。此外，AccidentGPT还采用了多模态提示与反馈、混合训练模式和边缘-云分割配置以增强性能，并提出了一些研究机会。

    

    交通事故分析对于提高公共安全和制定道路规章制度至关重要。传统方法虽然被广泛使用，但常常受限于手动分析过程、主观决策、单模态输出以及与敏感数据相关的隐私问题。本文介绍了AccidentGPT的概念，这是一个交通事故分析的基础模型，它将多模态输入数据用于自动重建事故过程视频并提供多模态输出的多任务分析。AccidentGPT的设计采用了多模态提示与反馈，用于任务导向的适应性，混合训练模式以利用标记和未标记数据，以及边缘-云分割配置以保护数据隐私。为了充分发挥该模型的功能，我们提出了一些研究机会。本文将填补传统方法中的空白。

    Traffic accident analysis is pivotal for enhancing public safety and developing road regulations. Traditional approaches, although widely used, are often constrained by manual analysis processes, subjective decisions, uni-modal outputs, as well as privacy issues related to sensitive data. This paper introduces the idea of AccidentGPT, a foundation model of traffic accident analysis, which incorporates multi-modal input data to automatically reconstruct the accident process video with dynamics details, and furthermore provide multi-task analysis with multi-modal outputs. The design of the AccidentGPT is empowered with a multi-modality prompt with feedback for task-oriented adaptability, a hybrid training schema to leverage labelled and unlabelled data, and a edge-cloud split configuration for data privacy. To fully realize the functionalities of this model, we proposes several research opportunities. This paper serves as the stepping stone to fill the gaps in traditional approaches of t
    
[^62]: 时间序列预测中扩散模型的兴起

    The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])

    [http://arxiv.org/abs/2401.03006](http://arxiv.org/abs/2401.03006)

    本文调查了扩散模型在时间序列预测中的应用，提供了对这些模型的全面背景信息和详细说明，同时也对它们在不同数据集上的有效性和彼此之间的比较进行了分析。其贡献包括对扩散模型在时间序列预测中应用的彻底探索和按时间顺序排序的模型概述。这是一份对人工智能和时间序列分析领域的研究人员来说具有价值的资源。

    

    本调查探讨了扩散模型在时间序列预测中的应用。扩散模型在生成型人工智能的各个领域中展示出最先进的结果。本文包括对扩散模型的全面背景信息，详细介绍其条件方法，并审查了其在时间序列预测中的应用。分析涵盖了11个具体的时间序列实现，它们的直觉和理论基础，不同数据集上的有效性以及彼此之间的比较。该工作的关键贡献是对扩散模型在时间序列预测中应用的彻底探索，并提供了一个按时间顺序排序的模型概述。此外，本文对该领域的最新技术水平进行了深入讨论，并概述了潜在的未来研究方向。这对于人工智能和时间序列分析领域的研究人员来说是一份宝贵的资源，提供了对最新进展和未来发展的清晰视图。

    This survey delves into the application of diffusion models in time-series forecasting. Diffusion models are demonstrating state-of-the-art results in various fields of generative AI. The paper includes comprehensive background information on diffusion models, detailing their conditioning methods and reviewing their use in time-series forecasting. The analysis covers 11 specific time-series implementations, the intuition and theory behind them, the effectiveness on different datasets, and a comparison among each other. Key contributions of this work are the thorough exploration of diffusion models' applications in time-series forecasting and a chronologically ordered overview of these models. Additionally, the paper offers an insightful discussion on the current state-of-the-art in this domain and outlines potential future research directions. This serves as a valuable resource for researchers in AI and time-series analysis, offering a clear view of the latest advancements and future p
    
[^63]: AST-T5：面向代码生成和理解的结构感知预训练模型

    AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])

    [http://arxiv.org/abs/2401.03003](http://arxiv.org/abs/2401.03003)

    AST-T5是一种结构感知的预训练模型，通过利用抽象语法树（AST）来增强代码生成、转换和理解的能力。它优于其他同等大小的语言模型，并在代码到代码任务中表现出色。

    

    大型语言模型在代码相关任务中取得了显著进展，然而许多模型将代码视为简单序列，忽略了其结构化特性。我们引入了AST-T5，一种新颖的预训练范式，利用抽象语法树（AST）增强了代码生成、转换和理解。通过动态规划，我们的AST感知分割保留了代码结构，而AST感知跨度破坏目标使模型能够重建各种代码结构。与其他模型不同，AST-T5避免了复杂的程序分析或架构更改，因此可以与任何编码器-解码器Transformer无缝集成。评估结果显示，AST-T5在各种代码相关任务中始终优于同等大小的语言模型。结构感知使得AST-T5在代码到代码任务中特别强大，在Bugs2Fix任务的精确匹配得分上超过CodeT5 2个点，并在CodeXGLUE中的Java-C#转换任务的精确匹配得分上超过CodeT5 3个点。

    Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
    
[^64]: UnetTSF:一个性能更好的线性复杂度时间序列预测模型

    UnetTSF: A Better Performance Linear Complexity Time Series Prediction Model. (arXiv:2401.03001v1 [cs.LG])

    [http://arxiv.org/abs/2401.03001](http://arxiv.org/abs/2401.03001)

    UnetTSF是一种具有线性复杂度的U-Net时间序列预测模型，通过使用FPN技术提取特征和设计适用于时间序列数据的融合结构，相比于DLiner和PatchTST模型，在多个测试项目中取得了更好的预测结果。

    

    最近，基于Transformer的模型在时间序列预测领域取得了显著进展，已经成为Dlinear基准模型之外的一种基线模型。本文提出了一种具有线性复杂度的U-Net时间序列预测模型(UnetTSF)，采用了U-Net架构。我们是首次使用FPN技术从时间序列数据中提取特征，取代了将时间序列数据分解为趋势和季节项的方法，同时设计了适用于时间序列数据的融合结构。在8个开源数据集上进行测试后，与最佳线性模型DLiner相比，31个测试项目中有32个取得了最佳结果。平均mse下降10.1％，平均mae下降9.1％。与复杂的基于Transformer的PatchTST相比，UnetTSF在32个测试项目中获得了9个mse的最佳结果和15个mae的最佳结果。

    Recently, Transformer-base models have made significant progress in the field of time series prediction which have achieved good results and become baseline models beyond Dlinear. The paper proposes an U-Net time series prediction model (UnetTSF) with linear complexity, which adopts the U-Net architecture. We are the first to use FPN technology to extract features from time series data, replacing the method of decomposing time series data into trend and seasonal terms, while designing a fusion structure suitable for time series data. After testing on 8 open-source datasets, compared to the best linear model DLiner. Out of 32 testing projects, 31 achieved the best results. The average decrease in mse is 10.1%, while the average decrease in mae is 9.1%. Compared with the complex transformer-base PatchTST, UnetTSF obtained 9 optimal results for mse and 15 optimal results for mae in 32 testing projects.
    
[^65]: 跨模态连接：知识蒸馏和屏蔽训练用于将多模态情感识别转化为单模态、仅语音的情感识别

    Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition. (arXiv:2401.03000v1 [cs.SD])

    [http://arxiv.org/abs/2401.03000](http://arxiv.org/abs/2401.03000)

    本文介绍了一种创新方法，用于将多模态情感识别转化为更实用且资源高效的单模态、仅语音的情感识别。使用知识蒸馏和屏蔽训练技术来解决现有模型所依赖的多模态输入在实际应用中可能不可行的问题。

    

    本文提出了一种创新方法，以解决将多模态情感识别模型转化为更实用且资源高效的单模态情感识别模型的挑战，具体关注语音情感识别。从语音信号中识别情感是一项关键任务，应用于人机交互、情感计算和心理健康评估。然而，现有的最先进模型通常依赖于多模态输入，包括来自多个来源的信息，如面部表情和手势，在实际场景中可能不易获得或不可行。为了解决这个问题，我们提出了一种利用知识蒸馏和屏蔽训练技术的新框架。

    This paper presents an innovative approach to address the challenges of translating multi-modal emotion recognition models to a more practical and resource-efficient uni-modal counterpart, specifically focusing on speech-only emotion recognition. Recognizing emotions from speech signals is a critical task with applications in human-computer interaction, affective computing, and mental health assessment. However, existing state-of-the-art models often rely on multi-modal inputs, incorporating information from multiple sources such as facial expressions and gestures, which may not be readily available or feasible in real-world scenarios. To tackle this issue, we propose a novel framework that leverages knowledge distillation and masked training techniques.
    
[^66]: 一种使用咳嗽音频的AI辅助无偏差呼吸系统疾病诊断模型：以COVID-19为例的案例研究

    An AI-enabled Bias-Free Respiratory Disease Diagnosis Model using Cough Audio: A Case Study for COVID-19. (arXiv:2401.02996v1 [cs.SD])

    [http://arxiv.org/abs/2401.02996](http://arxiv.org/abs/2401.02996)

    本研究提出了一种名为Bias Free Network (RBFNet) 的端到端解决方案，通过减轻混淆变量的影响，确保准确且无偏倚的呼吸系统疾病诊断特征。通过将COVID-19数据集纳入研究，强调了该模型的相关性。

    

    利用人工智能进行基于咳嗽的呼吸系统疾病诊断引起了很大关注，但许多现有研究忽视了其预测模型中的混淆变量。这些变量可能会扭曲咳嗽记录（输入数据）和呼吸系统疾病状态（输出变量）之间的关系，导致有偏倚的关联和不真实的模型性能。为填补这一缺口，我们提出了无偏倚网络（RBFNet），这是一个端到端的解决方案，能够有效地减轻训练数据分布中混淆因素的影响。RBFNet确保了准确且无偏倚的呼吸系统疾病诊断特征，并通过将COVID-19数据集纳入本研究来强调其相关性。该方法旨在通过克服混淆因素带来的挑战，提高基于人工智能的呼吸系统疾病诊断模型的可靠性。提出了RBFNet的特征编码器模块的卷积神经网络（CNN）和长短期记忆（LSTM）网络的混合模型。

    Cough-based diagnosis for Respiratory Diseases (RDs) using Artificial Intelligence (AI) has attracted considerable attention, yet many existing studies overlook confounding variables in their predictive models. These variables can distort the relationship between cough recordings (input data) and RD status (output variable), leading to biased associations and unrealistic model performance. To address this gap, we propose the Bias Free Network (RBFNet), an end to end solution that effectively mitigates the impact of confounders in the training data distribution. RBFNet ensures accurate and unbiased RD diagnosis features, emphasizing its relevance by incorporating a COVID19 dataset in this study. This approach aims to enhance the reliability of AI based RD diagnosis models by navigating the challenges posed by confounding variables. A hybrid of a Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks is proposed for the feature encoder module of RBFNet. An additio
    
[^67]: GLIDE-RL：基于演示的强化学习中的自然语言指导

    GLIDE-RL: Grounded Language Instruction through DEmonstration in RL. (arXiv:2401.02991v1 [cs.CL])

    [http://arxiv.org/abs/2401.02991](http://arxiv.org/abs/2401.02991)

    GLIDE-RL是一种基于演示的强化学习算法，通过引入教师-指导员-学生的课程学习框架，训练出了一个能够遵循自然语言指令并且可以推广到未见指令的RL代理。

    

    复杂的人工智能与人类协作系统的最后一项挑战是AI代理能够理解自然语言并相应地执行任务。然而，由于语言的复杂性和歧义性以及奖励的稀疏性等因素，训练有效的基于自然语言的强化学习（RL）代理一直是一个长期的挑战。强化学习、课程学习、持续学习和语言模型的几项进展分别为在不同环境中训练基于自然语言的代理做出了有效贡献。在这些发展的基础上，我们提出了一种新算法GLIDE-RL，通过引入教师-指导员-学生的课程学习框架来培训一个能够遵循自然语言指令，并且能够推广到之前未见的语言指令的RL代理。

    One of the final frontiers in the development of complex human - AI collaborative systems is the ability of AI agents to comprehend the natural language and perform tasks accordingly. However, training efficient Reinforcement Learning (RL) agents grounded in natural language has been a long-standing challenge due to the complexity and ambiguity of the language and sparsity of the rewards, among other factors. Several advances in reinforcement learning, curriculum learning, continual learning, language models have independently contributed to effective training of grounded agents in various environments. Leveraging these developments, we present a novel algorithm, Grounded Language Instruction through DEmonstration in RL (GLIDE-RL) that introduces a teacher-instructor-student curriculum learning framework for training an RL agent capable of following natural language instructions that can generalize to previously unseen language instructions. In this multi-agent framework, the teacher a
    
[^68]: 关于使用深度学习进行物种分布建模中伪缺失选择和效果的研究

    On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning. (arXiv:2401.02989v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.02989](http://arxiv.org/abs/2401.02989)

    该论文研究了在多物种神经网络中使用伪缺失的选择和效果。作者指出物种出现和伪缺失之间的类别不平衡问题经常被忽视，不同类型的伪缺失选择对结果产生了复杂影响。

    

    物种分布建模是一种非常多功能的工具，用于理解环境条件和物种出现之间的复杂关系。然而，现有数据通常缺乏关于确认的物种缺失的信息，并且仅限于机会性抽样的出现观察。为了克服这个限制，常见的方法是使用伪缺失，它们是指定为负样本的特定地理位置。虽然伪缺失在单物种分布模型中已经得到广泛应用，但在多物种神经网络的背景下，它们的应用仍未充分探索。值得注意的是，物种存在出现和伪缺失之间的类别不平衡往往被忽视。此外，不同类型的伪缺失（例如，随机和目标群体背景点）的存在使选择过程变得复杂。确定伪缺失类型的最佳组合是困难的，并且取决于特征。

    Species distribution modeling is a highly versatile tool for understanding the intricate relationship between environmental conditions and species occurrences. However, the available data often lacks information on confirmed species absence and is limited to opportunistically sampled, presence-only observations. To overcome this limitation, a common approach is to employ pseudo-absences, which are specific geographic locations designated as negative samples. While pseudo-absences are well-established for single-species distribution models, their application in the context of multi-species neural networks remains underexplored. Notably, the significant class imbalance between species presences and pseudo-absences is often left unaddressed. Moreover, the existence of different types of pseudo-absences (e.g., random and target-group background points) adds complexity to the selection process. Determining the optimal combination of pseudo-absences types is difficult and depends on the char
    
[^69]: 基于代理的扩展生成对抗网络在自由形态元表面设计中的参数优化

    A Surrogate-Assisted Extended Generative Adversarial Network for Parameter Optimization in Free-Form Metasurface Design. (arXiv:2401.02961v1 [cs.LG])

    [http://arxiv.org/abs/2401.02961](http://arxiv.org/abs/2401.02961)

    本文提出了一种基于代理的扩展生成对抗网络（XGAN），用于加速和优化自由形态元表面设计。XGAN通过物理约束准确地生成元表面，并且在实验中表现出了较高的准确度和速度。

    

    元表面在第五代（5G）微波通信中有广泛应用。在元表面家族中，与常规形状相比，自由形态元表面在实现复杂的光谱响应方面表现出色。然而，传统的自由形态元表面数值方法耗时且需要专业知识。最近的研究表明，深度学习对于加速和优化元表面设计具有巨大潜力。在这里，我们提出了XGAN，一种带有代理的扩展生成对抗网络（GAN），用于高质量的自由形态元表面设计。所提出的代理为XGAN提供物理约束，以便XGAN能够准确地从输入的光谱响应中生成元表面。在涉及20000个自由形态元表面设计的比较实验中，XGAN实现了0.9734的平均准确度，并且比传统方法快500倍。这种方法有助于元表面库的建立。

    Metasurfaces have widespread applications in fifth-generation (5G) microwave communication. Among the metasurface family, free-form metasurfaces excel in achieving intricate spectral responses compared to regular-shape counterparts. However, conventional numerical methods for free-form metasurfaces are time-consuming and demand specialized expertise. Alternatively, recent studies demonstrate that deep learning has great potential to accelerate and refine metasurface designs. Here, we present XGAN, an extended generative adversarial network (GAN) with a surrogate for high-quality free-form metasurface designs. The proposed surrogate provides a physical constraint to XGAN so that XGAN can accurately generate metasurfaces monolithically from input spectral responses. In comparative experiments involving 20000 free-form metasurface designs, XGAN achieves 0.9734 average accuracy and is 500 times faster than the conventional methodology. This method facilitates the metasurface library buildi
    
[^70]: 为多任务联邦学习提供公平性感知的作业调度

    Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])

    [http://arxiv.org/abs/2401.02740](http://arxiv.org/abs/2401.02740)

    本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。

    

    联邦学习（FL）使多个数据所有者（即FL客户端）能够在不泄露敏感私人数据的情况下共同训练机器学习模型。现有的FL研究主要关注垄断场景，在该场景中，单个FL服务器在每轮训练中选择一部分FL客户端来更新其本地模型。实际上，可能会有多个FL服务器同时尝试从同一个池中选择客户端。本文提出了一种首创的公平感知联邦作业调度（FairFedJS）方法来弥合这一差距。基于Lyapunov优化，它通过同时考虑当前需求和作业付款出价，确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，以防止等待时间过长。基于两个数据集对FairFedJS与四种最先进的方法进行了大量实验证明了其显著优势。它在平均上击败了最佳基准线31.9%和1.0%。

    Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
    
[^71]: 分布强化学习中具有可解释参数调整的鲁棒分位数Huber损失

    A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning. (arXiv:2401.02325v1 [cs.LG])

    [http://arxiv.org/abs/2401.02325](http://arxiv.org/abs/2401.02325)

    这篇论文提出了一种鲁棒的分位数Huber损失函数，在分布强化学习中通过捕捉噪声并调整参数来增强对异常值的鲁棒性。实证测试验证了该方法的有效性。

    

    分布强化学习通过最小化分位数Huber损失函数来估计回报分布，该函数从高斯分布之间的Wasserstein距离计算中产生，捕捉到当前和目标分位数值中的噪声。与经典的分位数Huber损失相比，这种创新的损失函数增强了对异常值的鲁棒性，并且通过近似数据中噪声的数量来调整参数。实证测试在分布强化学习的常见应用Atari游戏和最近的对冲策略中验证了该方法的有效性。

    Distributional Reinforcement Learning (RL) estimates return distribution mainly by learning quantile values via minimizing the quantile Huber loss function, entailing a threshold parameter often selected heuristically or via hyperparameter search, which may not generalize well and can be suboptimal. This paper introduces a generalized quantile Huber loss function derived from Wasserstein distance (WD) calculation between Gaussian distributions, capturing noise in predicted (current) and target (Bellman-updated) quantile values. Compared to the classical quantile Huber loss, this innovative loss function enhances robustness against outliers. Notably, the classical Huber loss function can be seen as an approximation of our proposed loss, enabling parameter adjustment by approximating the amount of noise in the data during the learning process. Empirical tests on Atari games, a common application in distributional RL, and a recent hedging strategy using distributional RL, validate the eff
    
[^72]: 基于视图的图神经网络解释

    View-based Explanations for Graph Neural Networks. (arXiv:2401.02086v1 [cs.LG])

    [http://arxiv.org/abs/2401.02086](http://arxiv.org/abs/2401.02086)

    这篇论文提出了一种基于视图的解释方法来解释图神经网络(GNNs)的行为，通过生成解释视图和图模式来解释特定类别的结果。

    

    研究生成图神经网络(GNNs)的解释，以了解它们在图分类等分析任务中的行为。现有的方法旨在理解GNNs的整体结果，而不是针对特定类别的解释，并且可能返回难以访问或直接查询的解释结构。我们提出了一种新颖的范式GVEX，用于生成图解释的图视图。我们设计了一种两层的解释结构，称为解释视图。解释视图包括一组图模式和一组诱发的解释子图。给定一个包含多个图和由基于GNN的分类器M分配的特定类别标签l的数据库G，它简洁地描述了最好解释为什么l由M分配的G的分数。我们提出了质量度量方法，并制定了一个优化问题来计算GNN解释的最佳解释视图。我们证明了该问题是Σ^2_P难的。

    Generating explanations for graph neural networks (GNNs) has been studied to understand their behavior in analytical tasks such as graph classification. Existing approaches aim to understand the overall results of GNNs rather than providing explanations for specific class labels of interest, and may return explanation structures that are hard to access, nor directly queryable.  We propose GVEX, a novel paradigm that generates Graph Views for EXplanation. (1) We design a two-tier explanation structure called explanation views. An explanation view consists of a set of graph patterns and a set of induced explanation subgraphs. Given a database G of multiple graphs and a specific class label l assigned by a GNN-based classifier M, it concisely describes the fraction of G that best explains why l is assigned by M. (2) We propose quality measures and formulate an optimization problem to compute optimal explanation views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3) 
    
[^73]: 在生成式人工智能时代的物联网: 视野与挑战

    IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v1 [cs.DC])

    [http://arxiv.org/abs/2401.01923](http://arxiv.org/abs/2401.01923)

    在生成式人工智能时代的物联网，Generative AI的进展带来了巨大的希望，同时也面临着高资源需求、及时工程、设备端推理、安全等关键挑战。

    

    带有感知、网络和计算能力的物联网设备，如智能手机、可穿戴设备、智能音箱和家庭机器人，已经无缝地融入到我们的日常生活中。最近生成式人工智能（Generative AI）的进展，如GPT、LLaMA、DALL-E和稳定扩散等，给物联网的发展带来了巨大的希望。本文分享了我们对Generative AI在物联网中带来的好处的看法和愿景，并讨论了Generative AI在物联网相关领域的一些重要应用。充分利用Generative AI在物联网中是一个复杂的挑战。我们确定了一些最关键的挑战，包括Generative AI模型的高资源需求、及时工程、设备端推理、卸载、设备端微调、联邦学习、安全以及开发工具和基准，并讨论了当前存在的差距以及使Generative AI在物联网中实现的有希望的机会。我们希望这篇文章能够激发新的研究和创新。

    Equipped with sensing, networking, and computing capabilities, Internet of Things (IoT) such as smartphones, wearables, smart speakers, and household robots have been seamlessly weaved into our daily lives. Recent advancements in Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold immense promise to push IoT to the next level. In this article, we share our vision and views on the benefits that Generative AI brings to IoT, and discuss some of the most important applications of Generative AI in IoT-related domains. Fully harnessing Generative AI in IoT is a complex challenge. We identify some of the most critical challenges including high resource demands of the Generative AI models, prompt engineering, on-device inference, offloading, on-device fine-tuning, federated learning, security, as well as development tools and benchmarks, and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new res
    
[^74]: SCALA: 基于稀疏化对比学习的属性网络异常检测

    SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks. (arXiv:2401.01625v1 [cs.SI])

    [http://arxiv.org/abs/2401.01625](http://arxiv.org/abs/2401.01625)

    SCALA是一种使用稀疏化的对比学习框架，用于在属性网络中进行异常检测。该框架旨在改善网络的嵌入质量，并通过引入稀疏化来为每个节点提供一种新的衡量异常得分的方法。

    

    属性网络上的异常检测旨在找到与其他大多数节点行为明显不同的节点。通常，网络数据包含实体间的关系信息，异常通常体现在这些关系中。因此，如何全面地建模网络中复杂的交互模式仍然是一个主要关注点。可以观察到，在网络中，异常违反了相似性假设。然而，大多数现有研究只是间接地考虑了这种现象，而不是显式地考虑。此外，正常实体的节点表示很容易受到异常节点引入的噪声关系的干扰。为了解决上述问题，我们提出了一种新颖的基于对比学习的属性网络异常检测框架SCALA，旨在改善网络的嵌入质量，并通过引入稀疏化来为每个节点提供一种新的衡量异常得分的方法。

    Anomaly detection on attributed networks aims to find the nodes whose behaviors are significantly different from other majority nodes. Generally, network data contains information about relationships between entities, and the anomaly is usually embodied in these relationships. Therefore, how to comprehensively model complex interaction patterns in networks is still a major focus. It can be observed that anomalies in networks violate the homophily assumption. However, most existing studies only considered this phenomenon obliquely rather than explicitly. Besides, the node representation of normal entities can be perturbed easily by the noise relationships introduced by anomalous nodes. To address the above issues, we present a novel contrastive learning framework for anomaly detection on attributed networks, \textbf{SCALA}, aiming to improve the embedding quality of the network and provide a new measurement of qualifying the anomaly score for each node by introducing sparsification into
    
[^75]: 探索LLMs在心理学应用中的前沿：一份综述

    Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])

    [http://arxiv.org/abs/2401.01519](http://arxiv.org/abs/2401.01519)

    本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。

    

    本文探索了大型语言模型（LLMs）在心理学应用中的前沿。心理学经历了几次理论变革，当前人工智能（AI）和机器学习，特别是LLMs的使用有望开启新的研究方向。我们详细探讨了LLMs如ChatGPT在心理学研究中的转变。文章讨论了LLMs在认知与行为心理学、临床与咨询心理学、教育与发展心理学以及社会与文化心理学等心理学分支中的影响，强调了它们模拟人类认知和行为方面的潜力。本文深入探讨了这些模型模拟人类文本生成的能力，为心理学中的文献回顾、假设生成、实验设计、实验对象、数据分析、学术写作和同行评审等提供创新工具。虽然LLMs在推动研究方法学方面起着重要作用，

    This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
    
[^76]: 高效视觉Transformer的令牌传播控制器

    Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])

    [http://arxiv.org/abs/2401.01470](http://arxiv.org/abs/2401.01470)

    本文提出一种新颖的令牌传播控制器（TPC），通过结合暂停概率和重新开始概率，实现了对令牌的减少和重复利用的控制，从而提高了视觉Transformer的效率和令牌利用率。

    

    视觉Transformer（ViTs）在各种计算机视觉任务上取得了有希望的结果，然而它们在输入令牌数量上的二次复杂度限制了它们在资源受限环境下的应用。以前的方法采用逐渐减少令牌来解决这个挑战，假设一个层中的令牌冗余意味着所有后续层中也有冗余。我们经验证明这个假设通常是不正确的，即一个层中多余的令牌在后面的层中可以是有用的。基于这个关键洞察力，我们提出了一种新颖的令牌传播控制器（TPC），它结合了两种不同的令牌分布，即暂停概率和重新开始概率，用来控制令牌的减少和重复利用，从而实现更高效的令牌利用。为了改善令牌分布的估计，我们提出了一种平滑机制，作为正则化器，有助于去除噪声异常值。此外

    Vision transformers (ViTs) have achieved promising results on a variety of Computer Vision tasks, however their quadratic complexity in the number of input tokens has limited their application specially in resource-constrained settings. Previous approaches that employ gradual token reduction to address this challenge assume that token redundancy in one layer implies redundancy in all the following layers. We empirically demonstrate that this assumption is often not correct, i.e., tokens that are redundant in one layer can be useful in later layers. We employ this key insight to propose a novel token propagation controller (TPC) that incorporates two different token-distributions, i.e., pause probability and restart probability to control the reduction and reuse of tokens respectively, which results in more efficient token utilization. To improve the estimates of token distributions, we propose a smoothing mechanism that acts as a regularizer and helps remove noisy outliers. Furthermore
    
[^77]: 可扩展的子二次时间网络重建

    Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])

    [http://arxiv.org/abs/2401.01404](http://arxiv.org/abs/2401.01404)

    这篇论文提出了一个可扩展的网络重建算法，能够在次二次时间内实现结果，通过随机的二阶邻居搜索产生最佳的边候选。

    

    网络重建是指在只有关于条件偶联的观测数据，例如时间序列或图模型的独立样本的情况下，确定N个节点之间未观测到的成对耦合。针对这个问题提出的算法的可扩展性的主要障碍是似乎无法避免的二次复杂度O(N^2)，即要考虑每种可能的成对耦合至少一次，尽管大多数感兴趣的网络都是稀疏的，非零耦合的数量只有O(N)。在这里，我们提出了一个适用于广泛重建问题的通用算法，其在子二次时间内实现结果，其数据相关复杂度宽松上界为O(N^(3/2)logN)，但具有更典型的对数线性复杂度O(Nlog^2 N)。我们的算法依赖于一个随机的二阶邻居搜索，产生了最佳的边候选。

    Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
    
[^78]: 回溯新Q-Newton方法，Newton流，Voronoi图和随机求根方法

    Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and Stochastic root finding. (arXiv:2401.01393v1 [math.OC])

    [http://arxiv.org/abs/2401.01393](http://arxiv.org/abs/2401.01393)

    本文介绍了回溯新的Q-Newton方法（BNQN），该方法是Newton方法的一种变体，具有强大的理论保证，易于实现，且在实验中有良好的表现。通过实验，发现BNQN在多项式和亚纯函数的根搜索中具有更平滑的吸引域，并通过与Newton流和Voronoi图的联系提出了一些挑战性的问题。此外，在面对随机扰动时，BNQN比Newton方法和随机松弛Newton方法更具鲁棒性。

    

    最近第三作者引入了一种名为回溯新Q-Newton方法(BNQN)的Newton方法的变种，该方法具有很强的理论保证，易于实现，并具有良好的实验性能。先前进行的实验显示，使用BNQN可以找到多项式和亚纯函数的根时，吸引域具有一些显著的特性。总体上看，它们比Newton方法的吸引域更加平滑。在本文中，我们继续深入实验研究这一显著现象，并将BNQN与Newton流和Voronoi图联系起来。这种联系给出了一些具有挑战性的难题需要解释。实验还表明，与Newton方法和随机松弛Newton方法相比，BNQN对随机扰动更加稳健。

    A new variant of Newton's method - named Backtracking New Q-Newton's method (BNQN) - which has strong theoretical guarantee, is easy to implement, and has good experimental performance, was recently introduced by the third author.  Experiments performed previously showed some remarkable properties of the basins of attractions for finding roots of polynomials and meromorphic functions, with BNQN. In general, they look more smooth than that of Newton's method.  In this paper, we continue to experimentally explore in depth this remarkable phenomenon, and connect BNQN to Newton's flow and Voronoi's diagram. This link poses a couple of challenging puzzles to be explained. Experiments also indicate that BNQN is more robust against random perturbations than Newton's method and Random Relaxed Newton's method.
    
[^79]: 使用稀缺数据和联邦多轨迹GNN预测婴儿脑连接性

    Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.01383](http://arxiv.org/abs/2401.01383)

    我们提出了一种使用联邦多轨迹GNN的方法，通过稀缺数据预测婴儿脑连接性。通过联邦学习，我们通过聚合多个医院的本地学习结果来提高模型性能，同时保护数据隐私。

    

    对于识别早期脑连接性发展的动态过程，了解婴儿脑网络在出生后的第一年中的复杂演化至关重要。现有的深度学习解决方案存在三个主要局限性。首先，它们不能泛化到多轨迹预测任务，其中每个图轨迹对应于特定的成像模态或连接类型（例如T1-w MRI）。其次，现有模型需要大量的训练数据集才能达到令人满意的性能，而这往往很难获取。第三，它们不能有效利用不完整的时间序列数据。为了解决这些限制，我们引入了FedGmTE-Net++，一种联邦图形多轨迹演化网络。通过联邦学习的力量，我们在有限的医院数据集中聚合了不同医院的本地学习结果。结果即可提高每个医院本地生成模型的性能，同时保护数据隐私。

    The understanding of the convoluted evolution of infant brain networks during the first postnatal year is pivotal for identifying the dynamics of early brain connectivity development. Existing deep learning solutions suffer from three major limitations. First, they cannot generalize to multi-trajectory prediction tasks, where each graph trajectory corresponds to a particular imaging modality or connectivity type (e.g., T1-w MRI). Second, existing models require extensive training datasets to achieve satisfactory performance which are often challenging to obtain. Third, they do not efficiently utilize incomplete time series data. To address these limitations, we introduce FedGmTE-Net++, a federated graph-based multi-trajectory evolution network. Using the power of federation, we aggregate local learnings among diverse hospitals with limited datasets. As a result, we enhance the performance of each hospital's local generative model, while preserving data privacy. The three key innovation
    
[^80]: 在无限维希尔伯特空间中学习一些玩具约束优化问题的解决方案

    Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces. (arXiv:2401.01306v1 [math.OC])

    [http://arxiv.org/abs/2401.01306](http://arxiv.org/abs/2401.01306)

    本文提出了在无限维希尔伯特空间中运用深度学习实现罚函数法和增广拉格朗日法的约束优化算法，并在玩具问题上进行了测试，证明这两种方法都能够产生不错的近似解。在约束函数本身是函数的情况下，通过拉格朗日乘子更新规则的计算优势，实现了显著的加速。

    

    本文介绍了在无限维希尔伯特空间中两种流行的理论约束优化算法——罚函数法和增广拉格朗日法的深度学习实现。我们在一些源自变分法或物理学的玩具问题上测试了这些算法。我们证明这两种方法都能够产生对测试问题的不错近似，并且在不同误差方面是可比较的。通过利用拉格朗日乘子更新规则在计算上比求解罚函数法中的子问题更简单的普遍情况，我们在输出约束函数本身是一个函数的情况下实现了显著加速。

    In this work we present deep learning implementations of two popular theoretical constrained optimization algorithms in infinite dimensional Hilbert spaces, namely, the penalty and the augmented Lagrangian methods. We test these algorithms on some toy problems originating in either calculus of variations or physics. We demonstrate that both methods are able to produce decent approximations for the test problems and are comparable in terms of different errors. Leveraging the common occurrence of the Lagrange multiplier update rule being computationally less expensive than solving subproblems in the penalty method, we achieve significant speedups in cases when the output of the constraint function is itself a function.
    
[^81]: 大型语言模型的知识编辑全面研究

    A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])

    [http://arxiv.org/abs/2401.01286](http://arxiv.org/abs/2401.01286)

    本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。

    

    大型语言模型(LLM)在理解和生成与人类交流紧密相似的文本方面展现出了非凡的能力。然而，其主要限制在于训练过程中的显著计算需求，这是由于其广泛的参数化造成的。这一挑战在于世界的动态性，需要频繁更新LLM以修正过时的信息或集成新知识，从而确保其持续的相关性。许多应用需要在训练后进行持续的模型调整，以解决缺陷或不良行为。近年来，对于LLM的知识编辑技术的兴趣越来越高，在特定领域内有效地修改LLM的行为，同时保持整体性能在各种输入中的表现。本文首先定义了知识编辑的目标和挑战，然后综述了现有的知识编辑方法和技术，并讨论了其应用和未来发展的方向。

    Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
    
[^82]: SecFormer：面向大型语言模型的快速准确隐私保护推理

    SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00793](http://arxiv.org/abs/2401.00793)

    SecFormer是一个优化框架，旨在实现Transformer模型的快速准确隐私保护推理。通过消除高成本的指数和线性操作，SecFormer能够有效解决在大型语言模型中应用SMPC时的性能问题。

    

    随着在云平台上部署大型语言模型以提供推理服务的使用增加，隐私问题日益加剧，尤其是涉及投资计划和银行账户等敏感数据。安全多方计算（SMPC）被视为保护推理数据和模型参数隐私的一种有前途的解决方案。然而，SMPC在大型语言模型（特别是基于Transformer架构的模型）的隐私保护推理中的应用往往会导致显著的减速或性能下降。这主要是由于Transformer架构中的众多非线性操作不适合SMPC，并且难以有效规避或优化。为了解决这个问题，我们引入了一个先进的优化框架，称为SecFormer，以实现Transformer模型的快速准确隐私保护推理。通过实施模型设计优化，我们成功消除了高成本的指数和线性操作，并取得了良好的性能。

    With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
    
[^83]: 从距离集中和流形效应解读维度诅咒

    Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect. (arXiv:2401.00422v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00422](http://arxiv.org/abs/2401.00422)

    这篇论文从理论和实证分析的角度深入研究了维度诅咒的两个主要原因——距离集中和流形效应，并通过实验证明了使用Minkowski距离进行最近邻搜索（NNS）在高维数据中取得了最佳性能。

    

    随着维度的增加，数据的特征如分布和异质性变得越来越复杂和违反直觉。这种现象被称为维度诅咒，低维空间中成立的常见模式和关系（例如内部和边界模式）在高维空间中可能无效。这导致回归、分类或聚类模型或算法的性能降低。维度诅咒可以归因于许多原因。本文首先总结了与处理高维数据相关的五个挑战，并解释了回归、分类或聚类任务失败的潜在原因。随后，我们通过理论和实证分析深入研究了维度诅咒的两个主要原因，即距离集中和流形效应。结果表明，使用三种典型的距离测量进行最近邻搜索（NNS）时，Minkowski距离的性能最佳。

    The characteristics of data like distribution and heterogeneity, become more complex and counterintuitive as the dimensionality increases. This phenomenon is known as curse of dimensionality, where common patterns and relationships (e.g., internal and boundary pattern) that hold in low-dimensional space may be invalid in higher-dimensional space. It leads to a decreasing performance for the regression, classification or clustering models or algorithms. Curse of dimensionality can be attributed to many causes. In this paper, we first summarize five challenges associated with manipulating high-dimensional data, and explains the potential causes for the failure of regression, classification or clustering tasks. Subsequently, we delve into two major causes of the curse of dimensionality, distance concentration and manifold effect, by performing theoretical and empirical analyses. The results demonstrate that nearest neighbor search (NNS) using three typical distance measurements, Minkowski
    
[^84]: TSPP：一种针对时间序列预测的统一基准测试工具

    TSPP: A Unified Benchmarking Tool for Time-series Forecasting. (arXiv:2312.17100v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.17100](http://arxiv.org/abs/2312.17100)

    该论文提出了TSPP，一种针对时间序列预测的统一基准测试工具，以解决各种设置下不同方法之间的比较困难。通过该框架，可以无缝集成模型和数据集，帮助从业者和研究人员进行开发工作。研究表明，精心实施的深度学习模型可以与需要大量特征工程和专家知识的梯度提升决策树相媲美，而只需付出最小的努力。

    

    尽管机器学习取得了显著的进展，但重点主要集中在数据获取和模型创建上。然而，在实际环境中全面评估机器学习解决方案需要整个流程的标准化。这种需求在时间序列预测中尤为迫切，因为不同的设置妨碍了各种方法之间的有意义的比较。为了填补这一空白，我们提出了一种统一的基准测试框架，该框架公开了开发时间序列预测模型所涉及的关键建模和机器学习决策。该框架促进了模型和数据集的无缝集成，帮助从业者和研究人员开展他们的开发工作。我们在该框架内对最近提出的模型进行基准测试，证明了精心实施的深度学习模型可以与需要大量特征工程和专家知识的梯度提升决策树相媲美，而只需付出最小的努力。

    While machine learning has witnessed significant advancements, the emphasis has largely been on data acquisition and model creation. However, achieving a comprehensive assessment of machine learning solutions in real-world settings necessitates standardization throughout the entire pipeline. This need is particularly acute in time series forecasting, where diverse settings impede meaningful comparisons between various methods. To bridge this gap, we propose a unified benchmarking framework that exposes the crucial modelling and machine learning decisions involved in developing time series forecasting models. This framework fosters seamless integration of models and datasets, aiding both practitioners and researchers in their development efforts. We benchmark recently proposed models within this framework, demonstrating that carefully implemented deep learning models with minimal effort can rival gradient-boosting decision trees requiring extensive feature engineering and expert knowled
    
[^85]: 偏好作为奖励，使用重要性抽样进行最大偏好优化

    Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16430](http://arxiv.org/abs/2312.16430)

    本文提出了一种使用重要性抽样进行最大偏好优化的算法，该算法通过直接优化生成策略来消除对奖励模型的需求，提高了数据利用率和稳定性，并通过解决KL正则化问题来改善偏好学习效果。

    

    偏好学习是将语言模型与人类价值观对齐的关键技术。从人类反馈强化学习（RLHF）是一种基于模型的算法，用于优化偏好学习，首先拟合偏好分数的奖励模型，然后使用基于策略梯度算法进行优化，以最大化奖励。RLHF的处理过程复杂、耗时且不稳定。直接偏好优化（DPO）算法使用离策略算法直接优化生成策略，消除了对奖励模型的需求，具有高效和稳定的数据利用率。DPO使用布拉德利-特里模型和对数损失，导致在偏好接近确定性时忽略了KL正则化项而过度拟合偏好数据。IPO使用一种基于根查找的成对均方误差损失来解决忽略KL正则化问题，并学习到最优策略。但是IPO的成对损失仍然无法使KL正则化生效。本文设计了一种新的算法，使用重要性抽样技术来解决偏好学习中的优化问题。

    Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
    
[^86]: 重新审视在分布偏移下的知识蒸馏

    Revisiting Knowledge Distillation under Distribution Shift. (arXiv:2312.16242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16242](http://arxiv.org/abs/2312.16242)

    本文通过重新制定目标函数，在分布偏移情况下重新审视了知识蒸馏的范式。使用统一的框架，对多样性偏移和相关性偏移进行了基准评估，并揭示了在分布偏移下教学性能较差的现象。

    

    知识蒸馏将大型模型的知识传递给小型模型，并在最近取得了显著成就。然而，很少有研究探讨了知识蒸馏在面对分布偏移时的机制。分布偏移是指在训练和测试阶段之间数据分布发生的漂移。在本文中，我们重新思考了知识蒸馏的范式，通过在偏移情况下重新制定目标函数。在真实场景下，我们提出了一个统一而系统的框架，用于对两种常见的分布偏移，即多样性偏移和相关性偏移，进行知识蒸馏的基准评估。该评估基准覆盖了来自算法、数据驱动和优化视角的30多种方法，针对五个基准数据集进行评估。总体上，我们对学生模型进行了大量实验。我们揭示了在分布偏移下教学性能较差的有趣观察结果；特别是，复杂的算法和数据增强方法可能对知识蒸馏效果产生负面影响。

    Knowledge distillation transfers knowledge from large models into small models, and has recently made remarkable achievements. However, few studies has investigated the mechanism of knowledge distillation against distribution shift. Distribution shift refers to the data distribution drifts between training and testing phases. In this paper, we reconsider the paradigm of knowledge distillation by reformulating the objective function in shift situations. Under the real scenarios, we propose a unified and systematic framework to benchmark knowledge distillation against two general distributional shifts including diversity and correlation shift. The evaluation benchmark covers more than 30 methods from algorithmic, data-driven, and optimization perspectives for five benchmark datasets. Overall, we conduct extensive experiments on the student model. We reveal intriguing observations of poor teaching performance under distribution shifts; in particular, complex algorithms and data augmentati
    
[^87]: 可变形音频Transformer用于音频事件检测

    Deformable Audio Transformer for Audio Event Detection. (arXiv:2312.16228v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.16228](http://arxiv.org/abs/2312.16228)

    该论文提出了一种新颖的可变形音频Transformer，命名为DATAR，用于音频事件检测。通过构建一个具有金字塔Transformer骨架的可变形注意力，该模型在预测任务中取得了有效的结果。研究还发现可变形注意力图计算可能过于简化输入特征，提出了进一步增强特征的方法。

    

    Transformer 在各种任务中取得了有希望的结果。然而，自我注意计算的二次复杂性限制了应用的范围，特别是在低资源环境和移动设备或边缘设备中。现有工作已经提出利用手工制作的注意力模式来减少计算复杂性。然而，这种手工制作的模式是数据无关的，可能不是最优的。因此，有可能减少了相关的键或值，而重要性较低的键或值仍然保留。基于这个关键洞察力，我们提出了一种新颖的用于音频识别的可变形音频Transformer，命名为DATAR，其中构建了一个具有金字塔Transformer骨架的可变形注意力，并且可学习。这样的架构在预测任务中已被证明是有效的，例如事件分类。此外，我们发现可变形注意力图计算可能过于简化输入特征，可以进一步增强。

    Transformers have achieved promising results on a variety of tasks. However, the quadratic complexity in self-attention computation has limited the applications, especially in low-resource settings and mobile or edge devices. Existing works have proposed to exploit hand-crafted attention patterns to reduce computation complexity. However, such hand-crafted patterns are data-agnostic and may not be optimal. Hence, it is likely that relevant keys or values are being reduced, while less important ones are still preserved. Based on this key insight, we propose a novel deformable audio Transformer for audio recognition, named DATAR, where a deformable attention equipping with a pyramid transformer backbone is constructed and learnable. Such an architecture has been proven effective in prediction tasks,~\textit{e.g.}, event classification. Moreover, we identify that the deformable attention map computation may over-simplify the input feature, which can be further enhanced. Hence, we introduc
    
[^88]: 互信息作为强化学习智能体的内在奖励，用于按需共乘的车辆派遣

    Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling. (arXiv:2312.15195v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.15195](http://arxiv.org/abs/2312.15195)

    本文提出了一个用于按需共乘车辆派遣的框架，利用互信息作为强化学习智能体的内在奖励，以解决现有算法中只考虑收入最大化而无法满足异常分布请求的问题。

    

    按需共乘服务的出现允许每辆车同时为多名乘客提供服务，从而增加了司机的收入，并使乘客能以较低的价格旅行，而不像UberX和Lyft等出租车/按需服务只能为一名乘客分配一辆车。尽管按需共乘服务可以带来如此多的好处，但共乘服务需要一个明确定义的匹配策略，以最大程度地为所有各方（乘客，司机，聚合公司和环境）提供利益，其中区域调度车辆对匹配和收入有重要影响。现有算法通常仅考虑收入最大化，这使得请求具有异常分布的乘客难以获得乘车。如何在确保合理请求分配的同时增加收入，对共乘服务公司（聚合公司）提出了挑战。在本文中，我们提出了一个车辆派遣的框架，用于共乘服务。

    The emergence of on-demand ride pooling services allows each vehicle to serve multiple passengers at a time, thus increasing drivers' income and enabling passengers to travel at lower prices than taxi/car on-demand services (only one passenger can be assigned to a car at a time like UberX and Lyft). Although on-demand ride pooling services can bring so many benefits, ride pooling services need a well-defined matching strategy to maximize the benefits for all parties (passengers, drivers, aggregation companies and environment), in which the regional dispatching of vehicles has a significant impact on the matching and revenue. Existing algorithms often only consider revenue maximization, which makes it difficult for requests with unusual distribution to get a ride. How to increase revenue while ensuring a reasonable assignment of requests brings a challenge to ride pooling service companies (aggregation companies). In this paper, we propose a framework for vehicle dispatching for ride po
    
[^89]: 高效异步稀疏化量化联邦学习

    Efficient Asynchronous Federated Learning with Sparsification and Quantization. (arXiv:2312.15186v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2312.15186](http://arxiv.org/abs/2312.15186)

    本论文提出了一种高效异步稀疏化量化联邦学习方法（TEASQ-Fed），利用边缘设备的并行参与，解决了传统方法中设备拖慢训练和通信瓶颈的问题。

    

    在数据分布在多个边缘设备上的情况下，联邦学习越来越受到关注，它可以在不传输原始数据的情况下合作训练机器学习模型。传统的联邦学习使用参数服务器和大量边缘设备来进行模型训练，每轮选择几个设备参与。然而，有些设备可能会拖慢训练过程甚至导致系统崩溃，而其他空闲设备则闲置不用。由于设备和服务器之间的带宽相对较低，中间数据的通信成为瓶颈。本文提出了一种带有稀疏化和量化的时间高效异步联邦学习方法（TEASQ-Fed），它可以充分利用边缘设备主动参与训练过程。我们利用控制参数来选择适当数量的并行边缘设备。

    While data is distributed in multiple edge devices, Federated Learning (FL) is attracting more and more attention to collaboratively train a machine learning model without transferring raw data. FL generally exploits a parameter server and a large number of edge devices during the whole process of the model training, while several devices are selected in each round. However, straggler devices may slow down the training process or even make the system crash during training. Meanwhile, other idle edge devices remain unused. As the bandwidth between the devices and the server is relatively low, the communication of intermediate data becomes a bottleneck. In this paper, we propose Time-Efficient Asynchronous federated learning with Sparsification and Quantization, i.e., TEASQ-Fed. TEASQ-Fed can fully exploit edge devices to asynchronously participate in the training process by actively applying for tasks. We utilize control parameters to choose an appropriate number of parallel edge device
    
[^90]: 提升防御能力：将对抗训练和水印技术相结合，为模型的弹性提供保护

    Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience. (arXiv:2312.14260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.14260](http://arxiv.org/abs/2312.14260)

    本研究介绍了一种新的框架，将对抗训练和水印技术相结合，用于提高模型的弹性，防御规避攻击，并在知识产权盗窃案件中提供确凿的模型验证。

    

    机器学习模型在越来越多的关键应用中被使用，因此确保其完整性和所有权非常重要。最近的研究发现，对抗训练和水印技术存在冲突的相互作用。本文引入了一种新的框架，将对抗训练与水印技术相结合，以增强对规避攻击的防御能力，并在知识产权盗窃案件中提供确凿的模型验证。我们使用对抗训练和对抗水印来训练一个强大的水印模型。关键的方法是在生成对抗水印时使用更高的扰动预算，与用于对抗训练的预算避免冲突。我们使用MNIST和Fashion-MNIST数据集来评估我们提出的技术对各种模型窃取攻击的效果。所得到的结果在鲁棒性性能方面始终优于现有基准，并进一步证明了这种防御的弹性。

    Machine learning models are being used in an increasing number of critical applications; thus, securing their integrity and ownership is critical. Recent studies observed that adversarial training and watermarking have a conflicting interaction. This work introduces a novel framework to integrate adversarial training with watermarking techniques to fortify against evasion attacks and provide confident model verification in case of intellectual property theft. We use adversarial training together with adversarial watermarks to train a robust watermarked model. The key intuition is to use a higher perturbation budget to generate adversarial watermarks compared to the budget used for adversarial training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets to evaluate our proposed technique on various model stealing attacks. The results obtained consistently outperform the existing baseline in terms of robustness performance and further prove the resilience of this defense
    
[^91]: 分布式量子神经网络通过分割特征编码。

    Distributed Quantum Neural Networks via Partitioned Features Encoding. (arXiv:2312.13650v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2312.13650](http://arxiv.org/abs/2312.13650)

    本论文提出了一种使用分布式量子神经网络的方法，通过分割特征并在多个小型量子神经网络上进行期望值集合来生成预测。通过实验验证，该方法可能在分类任务中优于单一的量子神经网络。

    

    量子神经网络被认为是近期量子计算中的一种有前途的应用，但是在优化过程中面临着梯度消失以及受限制的Qubit数量和浅层电路导致表达能力有限的挑战。为了缓解这些问题，我们提出了一种分布式量子神经网络的方法，通过使用多个小电路逼近大电路的输出来进行预测。然而，逼近大电路需要指数级的小电路评估。因此，我们提出了将分割特征分布在多个小型量子神经网络上，并使用它们的期望值集合来生成预测。为了验证我们的分布式方法，我们进行了Semeion和MNIST手写数字数据集的十类分类实验。Semeion数据集的结果表明，我们的分布式方法在分类中可能优于单一的量子神经网络。

    Quantum neural networks are expected to be a promising application in near-term quantum computing, but face challenges such as vanishing gradients during optimization and limited expressibility by a limited number of qubits and shallow circuits. To mitigate these challenges, an approach using distributed quantum neural networks has been proposed to make a prediction by approximating outputs of a large circuit using multiple small circuits. However, the approximation of a large circuit requires an exponential number of small circuit evaluations. Here, we instead propose to distribute partitioned features over multiple small quantum neural networks and use the ensemble of their expectation values to generate predictions. To verify our distributed approach, we demonstrate ten class classification of the Semeion and MNIST handwritten digit datasets. The results of the Semeion dataset imply that while our distributed approach may outperform a single quantum neural network in classification 
    
[^92]: 解锁预训练的图像骨干用于语义图像合成

    Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.13314](http://arxiv.org/abs/2312.13314)

    本文提出了一种新的GAN鉴别器类别，利用预训练的图像分类任务的特征骨干网络生成高度逼真的图像，并引入了更好的上下文建模和交叉注意力技术，生成更多样化的图像。

    

    语义图像合成是一项重要的条件性图像生成任务，它可以通过用户提供的语义标签图生成图像，同时控制生成图像的内容和空间布局。本文提出了一种新的GAN鉴别器类别，用于语义图像合成，通过利用预训练的图像分类任务的特征骨干网络生成高度逼真的图像。同时，我们还引入了一种新的生成器架构，具有更好的上下文建模能力，并使用交叉注意力将噪声注入潜变量，从而生成更多样化的图像。

    Semantic image synthesis, i.e., generating images from user-provided semantic label maps, is an important conditional image generation task as it allows to control both the content as well as the spatial layout of generated images. Although diffusion models have pushed the state of the art in generative image modeling, the iterative nature of their inference process makes them computationally demanding. Other approaches such as GANs are more efficient as they only need a single feed-forward pass for generation, but the image quality tends to suffer on large and diverse datasets. In this work, we propose a new class of GAN discriminators for semantic image synthesis that generates highly realistic images by exploiting feature backbone networks pre-trained for tasks such as image classification. We also introduce a new generator architecture with better context modeling and using cross-attention to inject noise into latent variables, leading to more diverse generated images. Our model, w
    
[^93]: 连续学习: 面向视频表示的免遗忘优胜子网络

    Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.11973](http://arxiv.org/abs/2312.11973)

    本研究基于"彩票票据假设"，提出了一种连续学习方法，通过利用稀疏子网络和FSO进行任务增量学习、少样本类增量学习和视频增量学习，实现高效学习和有效的权重重用。

    

    受到"彩票票据假设"（LTH）的启发，该假设强调在较大的密集网络中存在高效子网络，研究了在适当的稀疏条件下表现优秀的优胜子网络（WSN）在各种连续学习任务中的应用。它利用来自密集网络的预先存在的权重，在任务增量学习（TIL）场景中实现高效学习。在少样本类增量学习（FSCIL）中，设计了一种称为软子网络（SoftNet）的WSN变体，以防止数据样本稀缺时的过拟合。此外，考虑了WSN权重的稀疏重用，用于视频增量学习（VIL）。考虑了在WSN中使用傅立叶子神经运算器（FSO），它能够对视频进行紧凑编码，并在不同带宽下识别可重用的子网络。我们将FSO集成到不同的连续学习架构中，包括VIL、TIL和FSCIL。

    Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
    
[^94]: 面向基于强化学习的药物调整系统以减少言语不流畅的论文翻译

    Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11509](http://arxiv.org/abs/2312.11509)

    这个论文介绍了一种基于强化学习的系统，该系统可以根据患者言语不流畅程度自动调整药物，通过对药物组合的强化学习算法的优化，能够收敛到良好的用药方案。

    

    我们提出了一种基于强化学习的系统，该系统可以自动为患有与心理健康相关的言语不流畅的虚拟患者开具药物处方，并根据零成本频繁测量结果，调整药物和剂量。我们展示了系统的两个组成部分：一个在我们构建的大型数据集上检测和评估言语不流畅的模块，以及一个可以自动找到良好药物组合的强化学习算法。为了支持这两个模块，我们从文献中收集了关于药物治疗言语不流畅的效果的数据，并建立了一个可信的患者模拟系统。我们证明了在某些情况下，强化学习系统能够收敛到一个良好的用药方案。我们收集并对可能存在言语不流畅的人群进行了数据标注，并使用该数据集演示了我们的方法。我们的工作是一个概念验证:

    We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
    
[^95]: SAME: 防范模型提取攻击的样本重建方法

    SAME: Sample Reconstruction against Model Extraction Attacks. (arXiv:2312.10578v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.10578](http://arxiv.org/abs/2312.10578)

    SAME是一种防御模型提取攻击的新方法，基于样本重建的概念，无需额外的数据集和模型访问，并且具有更实用的保护能力。

    

    尽管深度学习模型在各个领域中表现出显著的性能，但它们的部署需要大量的资源和先进的计算基础设施。作为解决方案，机器学习即服务（MLaaS）应运而生，降低了用户发布或产品化他们的深度学习模型的门槛。然而，之前的研究已经强调了与MLaaS相关的潜在隐私和安全风险，其中一个主要威胁是模型提取攻击。为了解决这个问题，有许多防御解决方案，但它们都存在不切实际的假设和泛化问题，使它们对可靠的保护不够实用。受到这些限制的驱动，我们引入了一种基于样本重建概念的新型防御机制SAME。该策略对防御者的能力要求最小，消除了对辅助的离群分布（OOD）数据集、用户查询历史、白盒模型访问和额外干预的需求。

    While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during m
    
[^96]: 是否贝叶斯神经网络提高了武器系统预测性维护？

    Do Bayesian Neural Networks Improve Weapon System Predictive Maintenance?. (arXiv:2312.10494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10494](http://arxiv.org/abs/2312.10494)

    本研究旨在通过实现贝叶斯推断过程对高可靠性武器系统的故障时间进行建模，以改善预测性维护。通过分析和标准分类度量指标的基准测试，我们的方法LaplaceNN在合成和实际数据集上展现了良好的性能。

    

    我们使用贝叶斯推断过程来建立神经网络模型，用于对高可靠性武器系统的故障时间进行建模，该数据具有区间截断和时间变化的协变量。我们使用标准分类度量指标（如接收器操作特征曲线（ROC）、精确度-召回率曲线（PR）、可靠性曲线可视化）对我们的方法LaplaceNN进行分析和基准测试。

    We implement a Bayesian inference process for Neural Networks to model the time to failure of highly reliable weapon systems with interval-censored data and time-varying covariates. We analyze and benchmark our approach, LaplaceNN, on synthetic and real datasets with standard classification metrics such as Receiver Operating Characteristic (ROC) Area Under Curve (AUC) Precision-Recall (PR) AUC, and reliability curve visualizations.
    
[^97]: 基于深度学习方法的胎儿多普勒超声在现场实时信号质量评估

    Point-of-Care Real-Time Signal Quality for Fetal Doppler Ultrasound Using a Deep Learning Approach. (arXiv:2312.09433v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2312.09433](http://arxiv.org/abs/2312.09433)

    这项研究提出了一种基于深度学习的框架，用于提高胎儿多普勒数据的质量，并设计了一个实时信号质量评估指标，可以即时提供反馈和纠正数据源。

    

    在本研究中，我们提出了一个深度学习框架，旨在与我们之前开发的系统相结合，以增强大规模1D胎儿多普勒数据收集的数据质量。这个系统专为低资源社区的传统土著助产士设计，利用一款价格实惠的安卓手机改善记录信号的质量。我们已经证明多普勒数据可以用于在妊娠期间识别胎儿生长受限、高血压和其他相关问题。然而，信号的质量取决于许多因素，包括射频干扰、胎儿位置、孕妇体型和助产士使用多普勒的方式。为了能够即时提供反馈，纠正数据源，需要一种能够在手机上实时运行的信号质量指标。在这项研究中，我们评估了191个DUS信号，主要持续时间在5到10分钟之间。

    In this study, we present a deep learning framework designed to integrate with our previously developed system that facilitates large-scale 1D fetal Doppler data collection, aiming to enhance data quality. This system, tailored for traditional Indigenous midwives in low-resource communities, leverages a cost-effective Android phone to improve the quality of recorded signals. We have shown that the Doppler data can be used to identify fetal growth restriction, hypertension, and other concerning issues during pregnancy. However, the quality of the signal is dependent on many factors, including radio frequency interference, position of the fetus, maternal body habitus, and usage of the Doppler by the birth attendants. In order to provide instant feedback to allow correction of the data at source, a signal quality metric is required that can run in real-time on the mobile phone.  In this study, 191 DUS signals with durations mainly in the range between 5 to 10 minutes were evaluated for qu
    
[^98]: 打破沉默：在软件工程中使用LLMs的威胁

    Breaking the Silence: the Threats of Using LLMs in Software Engineering. (arXiv:2312.08055v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2312.08055](http://arxiv.org/abs/2312.08055)

    本文讨论了在软件工程中使用LLMs的潜在威胁，包括闭源模型、数据泄漏和研究结果的可重复性，并提出了一套针对软件工程研究人员和语言模型提供商的指南来减轻这些担忧。

    

    大型语言模型（LLMs）在软件工程领域得到了广泛应用，影响着代码补全、测试生成、程序修复和代码摘要等各种任务。然而，虽然具有很大潜力，但研究者们在涉及LLMs的实验中仍需谨慎，因为许多复杂因素可能会影响实验结果。本文对LLM相关研究的有效性潜在威胁进行了开放讨论，包括闭源模型、LLM训练数据与研究评估之间的可能数据泄漏以及LLM相关研究结果的可重复性。作为回应，本文提出了一套针对软件工程研究人员和语言模型提供商的指南，以减轻这些担忧。指南的影响通过现有的LLM提供商的良好实践和软件工程研究人员在测试用例生成方面的实际示例进行说明。

    Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.
    
[^99]: 安全的多任务贝叶斯优化

    Safe Multi-Task Bayesian Optimization. (arXiv:2312.07281v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.07281](http://arxiv.org/abs/2312.07281)

    这项研究将鲁棒的高斯过程均匀误差界限扩展到多任务设置中，以解决安全在线优化中超参数未知的问题。

    

    贝叶斯优化已成为安全在线系统优化的强大工具，因其高样本效率和噪声健壮性。为了进一步加快过程，可以将减少的物理模型纳入优化过程中以加速过程，因为这些模型能够提供对实际系统的近似，并且从中进行采样要便宜得多。模型与现实之间的相似性由额外的超参数表示，并在优化过程中学习。安全性是贝叶斯优化等在线优化方法的重要标准，最近的文献已经解决了此问题，并在已知超参数的假设下提供了安全保障。然而，在实践中这是不适用的。因此，我们扩展了鲁棒高斯过程均匀误差界限，以满足多任务设置，其中涉及从超参数后验分布计算置信区域。

    Bayesian optimization has become a powerful tool for safe online optimization of systems, due to its high sample efficiency and noise robustness. For further speed-up reduced physical models of the system can be incorporated into the optimization to accelerate the process, since the models are able to offer an approximation of the actual system, and sampling from them is significantly cheaper. The similarity between model and reality is represented by additional hyperparameters and learned within the optimization process. Safety is an important criteria for online optimization methods like Bayesian optimization, which has been addressed by recent literature, which provide safety guarantees under the assumption of known hyperparameters. However, in practice this is not applicable. Therefore, we extend the robust Gaussian process uniform error bounds to meet the multi-task setting, which involves the calculation of a confidence region from the hyperparameter posterior distribution utiliz
    
[^100]: 混合线性专家用于长期时间序列预测

    Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06786](http://arxiv.org/abs/2312.06786)

    MoLE是一种混合线性专家模型，通过训练多个线性中心模型和一个路由模型，能够适应时间序列模式的周期性变化，并显著降低了预测误差。

    

    长期时间序列预测(LTSF)旨在预测给定过去值的时间序列的未来值。当前在这个问题上的最先进技术(SOTA)在某些情况下是由以线性为中心的模型实现的，这些模型主要具有线性映射层。然而，由于其固有的简单性，它们不能够适应时间序列模式的周期性变化。为了解决这个挑战，我们提出了一种混合专家风格的增强线性模型的方法，并提出了混合线性专家(MoLE)。MoLE不是训练单个模型，而是训练多个以线性为中心的模型(即专家)和一个权衡和混合其输出的路由模型。虽然整个框架是端到端训练的，但每个专家都学会专门处理特定的时间模式，而路由模型则学会自适应地组合专家们的输出。实验证明，MoLE降低了线性中心模型(DLinear，RLinear和RMLP)的预测误差。

    Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in 
    
[^101]: 使用生成式回放的类原型条件扩散模型进行Continual Learning

    Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay. (arXiv:2312.06710v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06710](http://arxiv.org/abs/2312.06710)

    本论文提出了一个类原型条件扩散模型（CPDM）来解决Continual Learning中的灾难性遗忘问题。CPDM通过提高生成器的图像质量，减少了分类器的灾难性遗忘风险。

    

    缓解灾难性遗忘是Continual Learning中的一个关键难题。深度生成式回放（GR）提供了一种通过从先前任务中生成样本来增强模型记忆能力的技术。随着生成式人工智能的发展，生成模型已经从生成对抗网络（GANs）发展到了最新的扩散模型（DMs）。一个主要问题是生成的数据质量与原始数据相比会逐渐下降，因为生成器不断从其输出中进行自我学习。这种退化可能导致分类器中发生灾难性遗忘的潜在风险。为了解决这个问题，我们提出了类原型条件扩散模型（CPDM），一种基于生成式回放的Continual Learning方法，它通过提高生成器中的图像质量从而减少了分类器中的灾难性遗忘。CPDM的核心是一个可学习的类原型，它捕捉了给定类别中图像的核心特征。

    Mitigating catastrophic forgetting is a key hurdle in continual learning. Deep Generative Replay (GR) provides techniques focused on generating samples from prior tasks to enhance the model's memory capabilities. With the progression in generative AI, generative models have advanced from Generative Adversarial Networks (GANs) to the more recent Diffusion Models (DMs). A major issue is the deterioration in the quality of generated data compared to the original, as the generator continuously self-learns from its outputs. This degradation can lead to the potential risk of catastrophic forgetting occurring in the classifier. To address this, we propose the Class-Prototype Conditional Diffusion Model (CPDM), a GR-based approach for continual learning that enhances image quality in generators and thus reduces catastrophic forgetting in classifiers. The cornerstone of CPDM is a learnable class-prototype that captures the core characteristics of images in a given class. This prototype, integra
    
[^102]: 使用点变换器结合联邦学习从嗪和嘧啶法洗全切片图像中预测乳腺癌HER2状态

    Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images. (arXiv:2312.06454v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2312.06454](http://arxiv.org/abs/2312.06454)

    本文介绍了一种使用点变换器结合联邦学习的方法，用于从嗪和嘧啶法染色的全切片图像中预测乳腺癌HER2状态。该方法通过引入动态标签分布策略和辅助分类器，解决了联邦学习中的标签不平衡和利用局部上下文信息和长程依赖性的问题。

    

    直接从广泛可得的嗪和嘧啶法染色全切片图像中预测人类表皮生长因子受体2（HER2）状态可以降低技术成本，加快治疗选择速度。准确预测HER2需要大量的多地点全切片图像。联邦学习可以在不传输千兆字节大小的全切片图像和数据隐私问题的情况下，协同训练这些全切片图像。然而，联邦学习在解决真实世界中多地点全切片图像的标签不平衡方面面临挑战。此外，现有的全切片图像分类方法不能同时利用联邦学习中站点端特征表示中的局部上下文信息和长程依赖关系。为了解决这些问题，我们提出了一种使用点变换器结合联邦学习从嗪和嘧啶法染色全切片图像中预测多地点HER2状态的方法。我们的方法包括两项新设计。我们提出了一种动态标签分布策略和一个辅助分类器。

    Directly predicting human epidermal growth factor receptor 2 (HER2) status from widely available hematoxylin and eosin (HE)-stained whole slide images (WSIs) can reduce technical costs and expedite treatment selection. Accurately predicting HER2 requires large collections of multi-site WSIs. Federated learning enables collaborative training of these WSIs without gigabyte-size WSIs transportation and data privacy concerns. However, federated learning encounters challenges in addressing label imbalance in multi-site WSIs from the real world. Moreover, existing WSI classification methods cannot simultaneously exploit local context information and long-range dependencies in the site-end feature representation of federated learning. To address these issues, we present a point transformer with federated learning for multi-site HER2 status prediction from HE-stained WSIs. Our approach incorporates two novel designs. We propose a dynamic label distribution strategy and an auxiliary classifier,
    
[^103]: 集合卡尔曼滤波与高斯过程状态空间模型在非均场和在线推理中的应用

    Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference. (arXiv:2312.05910v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05910](http://arxiv.org/abs/2312.05910)

    这篇论文介绍了一种将集合卡尔曼滤波引入变分推理框架的方法，用于近似高斯过程状态空间模型的后验分布，并且有效地利用了潜在状态和动力学之间的依赖关系，减少了变分参数的数量。

    

    高斯过程状态空间模型（GPSSMs）是一种多功能和原则性的非线性动态系统模型。然而，现有的GPSSMs变分学习和推理方法通常需要优化大量变分参数，导致性能和效率不足。为了解决这个问题，我们提出将集合卡尔曼滤波（EnKF），一种成熟的基于模型的滤波技术，纳入变分推理框架中，以近似潜在状态的后验分布。这种利用EnKF的方法可以有效地利用潜在状态和GP动力学之间的依赖关系，同时消除了对变分分布进行参数化的需求，从而显著减少了变分参数的数量。此外，我们还展示了我们提出的算法可以通过简单地对多个项进行求和来直接评估变分推理中的近似证据下界（ELBO）。

    Gaussian process state-space models (GPSSMs) are a versatile and principled family of nonlinear dynamical system models. However, existing variational learning and inference methods for GPSSMs often necessitate optimizing a substantial number of variational parameters, leading to inadequate performance and efficiency. To overcome this issue, we propose incorporating the ensemble Kalman filter (EnKF), a well-established model-based filtering technique, into the variational inference framework to approximate the posterior distribution of latent states. This utilization of EnKF can effectively exploit the dependencies between latent states and GP dynamics, while eliminating the need for parameterizing the variational distribution, thereby significantly reducing the number of variational parameters. Moreover, we show that our proposed algorithm allows straightforward evaluation of an approximated evidence lower bound (ELBO) in variational inference via simply summating multiple terms with 
    
[^104]: 使用MobileNetV2增强的乳腺癌肿瘤分类：对图像强度、错误缓解和Streamlit驱动的实时部署的详细探索

    Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment. (arXiv:2312.03020v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2312.03020](http://arxiv.org/abs/2312.03020)

    本研究基于MobileNetV2模型，使用1576张超声图像构建的数据集对乳腺癌肿瘤进行分类。该模型在正常、良性、恶性的分类准确率、精确率、召回率、ROC-AUC、PR-AUC和MCC分别达到0.82、0.83、0.81、0.94、0.88和0.74。研究还改进了图像强度分布和错误分类，并提供了一个可推广的模型，展示了MobileNetV2在医学成像中的潜力，为肿瘤诊断的精度提供了改进。另外，论文还探讨了基于Streamlit的实时肿瘤分类部署，为未来的癌症研究设立了一个基准。

    

    本研究介绍了一种基于Google的MobileNetV2的复杂迁移学习模型，用于将乳腺癌肿瘤分类为正常、良性和恶性三类，利用了1576张超声图像的数据集（265张正常、891张良性、420张恶性）。该模型的准确率为0.82，精确率为0.83，召回率为0.81，ROC-AUC为0.94，PR-AUC为0.88，MCC为0.74。研究还检查了图像强度分布和错误分类，并提出了未来应用的改进方法。针对数据集不平衡问题，该研究确保了一个可推广的模型。本研究使用了埃及开罗巴希亚医院的数据集，由Walid Al-Dhabyani等人编制，强调了MobileNetV2在医学成像中的潜力，旨在提高肿瘤诊断的精度。此外，本论文还探讨了基于Streamlit的实时肿瘤分类部署，展示了MobileNetV2在医学成像中的适用性，并为癌症研究设定了一个基准。

    This research introduces a sophisticated transfer learning model based on Google's MobileNetV2 for breast cancer tumor classification into normal, benign, and malignant categories, utilizing a dataset of 1576 ultrasound images (265 normal, 891 benign, 420 malignant). The model achieves an accuracy of 0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and MCC of 0.74. It examines image intensity distributions and misclassification errors, offering improvements for future applications. Addressing dataset imbalances, the study ensures a generalizable model. This work, using a dataset from Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al., emphasizes MobileNetV2's potential in medical imaging, aiming to improve diagnostic precision in oncology. Additionally, the paper explores Streamlit-based deployment for real-time tumor classification, demonstrating MobileNetV2's applicability in medical imaging and setting a benchmark for future research in onco
    
[^105]: FaultFormer: 预训练Transformer用于适应性轴承故障分类

    FaultFormer: Pretraining Transformers for Adaptable Bearing Fault Classification. (arXiv:2312.02380v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.02380](http://arxiv.org/abs/2312.02380)

    本论文提出了一个使用预训练Transformer模型进行适应性轴承故障分类的框架。通过研究不同的标记分割和数据增强策略，该方法在稀缺数据环境中能够达到最先进的准确率，并在微调时改善了性能。

    

    全球消费的增长推动了深度学习在智能制造和机器健康监测方面的重要应用。特别是，振动数据提供了丰富可靠的信息，能够对机器健康和预测性维护进行有意义的洞察。在本工作中，我们提出了基于Transformer模型的轴承故障识别的预训练和微调框架。我们研究了不同的标记分割和数据增强策略，以提高性能并达到最先进的准确率。此外，我们展示了针对振动信号的掩码自监督预训练及其在低数据环境、任务适应和数据集适应中的应用。预训练能够提升在稀缺未见训练样本上的10类轴承分类性能。当在预训练分布之外的故障类别上进行微调时，Transformer模型也受益于预训练。最后，我们展示了预训练的Transformer模型。

    The growth of global consumption has motivated important applications of deep learning to smart manufacturing and machine health monitoring. In particular, vibration data offers a rich and reliable source to provide meaningful insights into machine health and predictive maintenance. In this work, we present pretraining and fine-tuning frameworks for identifying bearing faults based on transformer models. In particular, we investigate different tokenization and data augmentation strategies to improve performance and reach state of the art accuracies. Furthermore, we demonstrate masked self-supervised pretraining for vibration signals and its application to low-data regimes, task adaptation, and dataset adaptation. Pretraining is able to improve performance on 10-way bearing classification on scarce, unseen training samples. Transformer models also benefit from pretraining when fine-tuning on fault classes outside of the pretraining distribution. Lastly, pretrained transformers are shown
    
[^106]: 无需校准的在线测试时间自适应应用于脑电运动想象解码

    Calibration-free online test-time adaptation for electroencephalography motor imagery decoding. (arXiv:2311.18520v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2311.18520](http://arxiv.org/abs/2311.18520)

    本研究探索了在线测试时间自适应（OTTA）的概念，无需校准且保护隐私，以实现无监督的连续自适应模型运行。研究使用了脑电运动想象解码任务，并通过轻量级架构和不同的OTTA技术来提高准确率。

    

    脑-计算机接口（BCI）在解码能力方面取得了显著进展，这为将人类大脑与外部设备相连提供了一条有希望的途径，这主要得益于愈发复杂的技术，特别是深度学习。然而，在现实世界的场景中实现高准确率仍然是一个挑战，因为不同测试和受试者之间存在分布差异。在本文中，我们将探讨在线测试时间自适应（OTTA）的概念，以在推断过程中以无监督的方式持续自适应模型。我们的方法通过在自适应过程中不需要访问源数据来保证隐私的保护。此外，OTTA通过不需要任何特定于会话或受试者的数据来实现无需校准运行。我们将使用轻量级架构以及不同的OTTA技术（如对齐，自适应批量归一化）来研究脑电运动想象解码的任务。

    Providing a promising pathway to link the human brain with external devices, Brain-Computer Interfaces (BCIs) have seen notable advancements in decoding capabilities, primarily driven by increasingly sophisticated techniques, especially deep learning. However, achieving high accuracy in real-world scenarios remains a challenge due to the distribution shift between sessions and subjects. In this paper we will explore the concept of online test-time adaptation (OTTA) to continuously adapt the model in an unsupervised fashion during inference time. Our approach guarantees the preservation of privacy by eliminating the requirement to access the source data during the adaptation process. Additionally, OTTA achieves calibration-free operation by not requiring any session- or subject-specific data. We will investigate the task of electroencephalography (EEG) motor imagery decoding using a lightweight architecture together with different OTTA techniques like alignment, adaptive batch normaliza
    
[^107]: 一种用于野生动物监测的高效光照不变的老虎检测框架

    An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance. (arXiv:2311.17552v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.17552](http://arxiv.org/abs/2311.17552)

    本文提出了一种高效光照不变的老虎检测框架，采用了EnlightenGAN和YOLOv8模型，实现了61%的mAP得分，并通过光照增强提高了0.7%的mAP。这些方法在ATRW数据集上的表现超过了当前的最新性能。

    

    老虎保护需要多方面的策略，包括保护生态栖息地、反偷猎措施以及社区参与，以实现老虎种群的可持续增长。随着人工智能的发展，可以利用目标检测自动化进行老虎监测。本文提出了一种基于EnlightenGAN和YOLOv8的准确光照不变框架用于老虎检测。经过微调的YOLOv8模型在没有光照增强的情况下实现了61％的mAP得分。光照增强可以提高0.7％的mAP。这些方法将ATRW数据集的最新性能提高了约6％至7％。

    Tiger conservation necessitates the strategic deployment of multifaceted initiatives encompassing the preservation of ecological habitats, anti-poaching measures, and community involvement for sustainable growth in the tiger population. With the advent of artificial intelligence, tiger surveillance can be automated using object detection. In this paper, an accurate illumination invariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger detection. The fine-tuned YOLOv8 model achieves a mAP score of 61% without illumination enhancement. The illumination enhancement improves the mAP by 0.7%. The approaches elevate the state-of-the-art performance on the ATRW dataset by approximately 6% to 7%.
    
[^108]: 基于移动网格PDE的移动采样物理信息神经网络

    Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2311.16167](http://arxiv.org/abs/2311.16167)

    这项工作提出了一种基于移动网格PDE的移动采样物理信息神经网络(MMPDE-Net)，通过解决移动网格PDE来自适应生成新的采样点，并且结合物理信息神经网络（PINN）提出了移动采样PINN（MS-PINN）的框架。数值实验验证了MS-PINN相对于PINN的性能改善。

    

    在这项工作中，我们提出了一种基于移动网格方法的端到端自适应采样神经网络（MMPDE-Net），通过求解移动网格PDE，可以自适应生成新的采样点。该模型旨在改善采样点生成的质量。此外，我们基于MMPDE-Net开发了一种迭代算法，使得采样点更加精确和可控。由于MMPDE-Net是独立于深度学习求解器的框架，我们将其与物理信息神经网络（PINN）相结合，提出了移动采样PINN（MS-PINN），并在一些假设下通过误差分析验证了其有效性。最后，我们通过四个典型实例的数值实验验证了MS-PINN相对于PINN的性能改善，从而数值上证明了我们方法的有效性。

    In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
    
[^109]: 带预热的动量梯度下降的大型弹射概念研究

    Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study. (arXiv:2311.15051v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.15051](http://arxiv.org/abs/2311.15051)

    本研究通过实验证明，带有大学习率和学习率预热的动量梯度下降显示出大型弹射效应，将迭代朝着比梯度下降发现的更平缓的极小值方向推进。

    

    尽管动量梯度下降在现代深度学习中被广泛使用，但对其对训练轨迹的影响的具体理解仍然难以捉摸。本研究通过实验证明，带有大学习率和学习率预热的动量梯度下降显示出大型弹射效应，将迭代朝着比梯度下降发现的更平缓的极小值方向推进。然后我们提供了实证证据和理论直觉，表明大型弹射效应是由于动量“放大”了自稳定效应（Damian等，2023）。

    Although gradient descent with momentum is widely used in modern deep learning, a concrete understanding of its effects on the training trajectory still remains elusive. In this work, we empirically show that momentum gradient descent with a large learning rate and learning rate warmup displays large catapults, driving the iterates towards flatter minima than those found by gradient descent. We then provide empirical evidence and theoretical intuition that the large catapult is caused by momentum "amplifying" the self-stabilization effect (Damian et al., 2023).B.1
    
[^110]: 开放集dandelion网络用于物联网入侵检测

    Open Set Dandelion Network for IoT Intrusion Detection. (arXiv:2311.11249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.11249](http://arxiv.org/abs/2311.11249)

    本文提出了一种基于无监督异构领域适应的开放集dandelion网络（OSDN）用于物联网入侵检测，该模型通过从知识丰富的源网络入侵领域进行入侵知识传输，以实现更准确的入侵检测。在开放集设置下，它能够检测到在源领域中未观测到的新兴目标领域入侵。

    

    随着物联网设备的广泛应用，保护它们免受恶意入侵变得至关重要。然而，物联网数据的稀缺性限制了传统入侵检测方法的适用性，这些方法高度依赖于数据。为了解决这个问题，我们在本文中提出了基于无监督异构领域适应的开放集dandelion网络（OSDN）。OSDN模型通过从知识丰富的源网络入侵领域进行入侵知识传输，从而实现对数据稀缺的目标物联网入侵领域的更准确的入侵检测。在开放集设置下，它还能检测到在源领域中未观测到的新兴目标领域入侵。为了实现这一点，OSDN模型将源领域形成一个类似蒲公英的特征空间，其中每个入侵类别被紧密分组并且不同的入侵类别被分隔开，即同时强调了类别间的可分性和类别内的紧凑性。

    As IoT devices become widely, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this paper we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactn
    
[^111]: 分布式贝叶斯学习中的异步本地计算

    Asynchronous Local Computations in Distributed Bayesian Learning. (arXiv:2311.03496v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03496](http://arxiv.org/abs/2311.03496)

    这篇论文提出了分布式贝叶斯学习中的异步本地计算方法，通过使用八卦异步通信来减少通信开销，同时利用快速计算实现协作学习未知参数。

    

    随着机器学习在传感器网络，协作机器人和其他多智能体系统领域的应用范围不断扩大，分布式推理算法的部署引起了很多关注。这些算法涉及到通过多个智能体收集的分散数据进行协作学习未知参数。在这样的算法中存在两个竞争的方面，即智能体内部计算和智能体之间的通信。传统上，算法被设计成同时执行这两个方面。然而，在某些情况下，由于通信信道不可靠、耗时或资源昂贵，需要节约使用通信通道。在本文中，我们提出了基于八卦的异步通信来同时利用快速计算和减少通信开销。我们分析了连续智能体之间的本地智能体计算对交互智能体通信的影响。对于本地计算部分，我们采用贝叶斯采样实现。

    Due to the expanding scope of machine learning (ML) to the fields of sensor networking, cooperative robotics and many other multi-agent systems, distributed deployment of inference algorithms has received a lot of attention. These algorithms involve collaboratively learning unknown parameters from dispersed data collected by multiple agents. There are two competing aspects in such algorithms, namely, intra-agent computation and inter-agent communication. Traditionally, algorithms are designed to perform both synchronously. However, certain circumstances need frugal use of communication channels as they are either unreliable, time-consuming, or resource-expensive. In this paper, we propose gossip-based asynchronous communication to leverage fast computations and reduce communication overhead simultaneously. We analyze the effects of multiple (local) intra-agent computations by the active agents between successive inter-agent communications. For local computations, Bayesian sampling via 
    
[^112]: 利用高级综合和大型语言模型生成、模拟和部署统一随机数生成器硬件设计

    Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design. (arXiv:2311.03489v4 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2311.03489](http://arxiv.org/abs/2311.03489)

    我们提出了一种利用高级综合和大型语言模型生成硬件设计的方法，通过案例研究验证了其功能和质量，并记录了所有相关的工具和结果。我们相信这一方法将在应用特定集成电路设计中产生革命性影响。

    

    我们提出了一种新的高级综合方法，利用大型语言模型工具来生成硬件设计。该方法仅使用开源工具，不包括大型语言模型。我们以生成具有wishbone接口的置换同余随机数生成器设计为案例研究。我们使用大型语言模型生成的仿真和Dieharder随机性测试套件验证了随机数生成器设计的功能和质量。我们记录了案例研究中使用的所有大型语言模型聊天记录、Python脚本、Verilog脚本和仿真结果。我们相信，我们的硬件设计生成方法与开源硅130纳米设计工具相结合，将改变应用特定集成电路设计的方式。我们的方法在构建物联网的领域专用计算加速器和概念验证原型时显著降低了门槛。

    We present a new high-level synthesis methodology for using large language model tools to generate hardware designs. The methodology uses exclusively open-source tools excluding the large language model. As a case study, we use our methodology to generate a permuted congruential random number generator design with a wishbone interface. We verify the functionality and quality of the random number generator design using large language model-generated simulations and the Dieharder randomness test suite. We document all the large language model chat logs, Python scripts, Verilog scripts, and simulation results used in the case study. We believe that our method of hardware design generation coupled with the open source silicon 130 nm design tools will revolutionize application-specific integrated circuit design. Our methodology significantly lowers the bar to entry when building domain-specific computing accelerators for the Internet of Things and proof of concept prototypes for later fabri
    
[^113]: 学习生成参数概率模型的随机热力学

    Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])

    [http://arxiv.org/abs/2310.19802](http://arxiv.org/abs/2310.19802)

    本文将生成式机器学习问题视为参数概率模型的时间演化过程，通过研究模型参数与生成样本之间的热力学交换，发现模型通过耗散热量来学习，参数子系统充当热库存储学到的信息。这为超参数模型的泛化能力提供了有价值的热力学洞察。

    

    我们将生成式机器学习问题形式化为参数化概率模型（PPM）的时间演化，从本质上来说，这是一个热力学过程。然后，我们研究了模型参数（记为$\Theta$）与模型生成样本（记为$X$）之间的热力学交换。我们证明了训练数据集和随机梯度下降（SGD）优化器的作用是驱动这两个子系统的时间演化的能源。我们的发现表明，在生成样本$X$的过程中，模型通过耗散热量来学习，导致模型参数$\Theta$的熵增加。因此，参数子系统充当了一个热库，有效地存储了学到的信息。此外，模型参数作为热库的角色为超参数模型的泛化能力提供了有价值的热力学洞察。这种方法提供了一个明确且一致的方式来理解生成模型学习过程中的热力学行为。

    We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
    
[^114]: 差分隐私排列检验：应用于核方法

    Differentially Private Permutation Tests: Applications to Kernel Methods. (arXiv:2310.19043v1 [math.ST])

    [http://arxiv.org/abs/2310.19043](http://arxiv.org/abs/2310.19043)

    本文提出了差分隐私排列检验的框架，扩展了经典的非私有排列检验，以在私有环境中保持有限样本有效性和差分隐私性质。该检验的功率取决于检验统计量的选择，并建立了一般条件来保证一致性和非渐进均匀的功率。

    

    近年来，人们对敏感数据的隐私问题越来越关注。为了应对这些问题，差分隐私作为一种严格的隐私保护框架应运而生，在学术界和工业界广泛认可。尽管在私有数据分析方面取得了相当大的进展，但现有的方法往往存在不实用或明显的统计效率损失。本文旨在通过引入差分隐私排列检验来缓解这些担忧。所提出的框架将经典的非私有排列检验扩展到私有环境中，以严格的方式保持有限样本有效性和差分隐私性质。所提出的检验的功率取决于一个检验统计量的选择，并建立了一般条件保证了一致性和非渐进均匀的功率。为了证明我们框架的实用性和可行性，我们重点关注重现核方法。

    Recent years have witnessed growing concerns about the privacy of sensitive data. In response to these concerns, differential privacy has emerged as a rigorous framework for privacy protection, gaining widespread recognition in both academic and industrial circles. While substantial progress has been made in private data analysis, existing methods often suffer from impracticality or a significant loss of statistical efficiency. This paper aims to alleviate these concerns in the context of hypothesis testing by introducing differentially private permutation tests. The proposed framework extends classical non-private permutation tests to private settings, maintaining both finite-sample validity and differential privacy in a rigorous manner. The power of the proposed test depends on the choice of a test statistic, and we establish general conditions for consistency and non-asymptotic uniform power. To demonstrate the utility and practicality of our framework, we focus on reproducing kerne
    
[^115]: 使用合成数据扩展提升数据分析

    Boosting Data Analytics With Synthetic Volume Expansion. (arXiv:2310.17848v1 [stat.ML])

    [http://arxiv.org/abs/2310.17848](http://arxiv.org/abs/2310.17848)

    本文介绍了一种利用合成数据生成框架来提升数据分析的方法，在此方法中，使用先进模型生成高逼真度的合成数据，并采用统计方法进行分析。研究发现，在合成数据上的统计方法错误随着合成数据的增加而减少，但最终可能会增加或停滞。

    

    合成数据生成作为生成式人工智能的基石，在解决数据稀缺和隐私问题的同时，实现了前所未有的性能。随着合成数据的日益重要，人们开始关注统计方法在合成数据与原始数据上的准确性。在本文中，我们介绍了用于分析的合成数据生成框架。该框架使用高逼真度的合成数据，通过先进模型如表格扩散和生成式预训练转换器模型生成，并结合相关研究洞察进一步增强。在这个框架中的一个重要发现是生成效应：统计方法在合成数据上的错误随着合成数据的增加一开始减少，但最终可能会增加或停滞。这个现象根源于复制原始数据分布的复杂性。

    Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distri
    
[^116]: 跨模态主动互补学习与自我完善对应关系

    Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v1 [cs.CV])

    [http://arxiv.org/abs/2310.17468](http://arxiv.org/abs/2310.17468)

    本文提出了一种跨模态主动互补学习框架（CRCL），通过使用新颖的主动互补损失（ACL）和高效的自我完善对应关系修正（SCC），改善了现有方法的鲁棒性。

    

    最近，图像和文本的匹配引起了学术界和工业界越来越多的关注，这是理解视觉和文本模态之间潜在对应关系的基础。然而，大多数现有方法隐式假设训练对是对齐良好的，而忽略了普遍存在的注释噪音，即噪声对应（NC），从而不可避免地导致性能下降。虽然一些方法尝试解决这种噪声，但仍面临两个挑战性问题：过度记忆/过拟合和对于NC的不可靠修正，特别是在高噪声下。为了解决这两个问题，我们提出了一个通用的跨模态鲁棒互补学习框架（CRCL），它通过利用一种新颖的主动互补损失（ACL）和高效的自我完善对应关系修正（SCC）来改进现有方法的鲁棒性。具体而言，ACL利用主动和互补的学习损失来减少提供错误信息的风险。

    Recently, image-text matching has attracted more and more attention from academia and industry, which is fundamental to understanding the latent correspondence across visual and textual modalities. However, most existing methods implicitly assume the training pairs are well-aligned while ignoring the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby inevitably leading to a performance drop. Although some methods attempt to address such noise, they still face two challenging problems: excessive memorizing/overfitting and unreliable correction for NC, especially under high noise. To address the two problems, we propose a generalized Cross-modal Robust Complementary Learning framework (CRCL), which benefits from a novel Active Complementary Loss (ACL) and an efficient Self-refining Correspondence Correction (SCC) to improve the robustness of existing methods. Specifically, ACL exploits active and complementary learning losses to reduce the risk of providing erroneous s
    
[^117]: Kiki还是Bouba？视觉与语言模型中的声音象征性

    Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.16781](http://arxiv.org/abs/2310.16781)

    这项研究通过调查视觉与语言模型中的内在知识，发现它们显示了声音象征性的模式，进一步证实声音和意义之间的相关性在跨模态关联中得到了体现。

    

    尽管人类语言中的声音和意义之间的映射被认为在很大程度上是随机的，但认知科学的研究表明，在语言和人口群体之间的特定声音和意义之间存在非平凡的相关性，这种现象被称为声音象征性。在许多意义维度中，声音象征性在语言和视觉领域之间的跨模态关联方面尤为显著和充分证明。在这项工作中，我们探讨了声音象征性是否在CLIP和Stable Diffusion等视觉与语言模型中得到体现。通过使用零样本知识探测来调查这些模型的内在知识，我们发现强有力的证据表明它们确实显示了这种模式，与心理语言学中众所周知的kiki-bouba效应相一致。我们的工作提供了一种使用计算工具来展示声音象征性并理解其本质的新方法。我们的代码将公开提供。

    Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.
    
[^118]: 学习在测试时进行学习的方法

    Learning to (Learn at Test Time). (arXiv:2310.13807v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.13807](http://arxiv.org/abs/2310.13807)

    本论文提出了一种学习在测试时进行学习的方法，通过两层嵌套循环对监督学习问题进行重新定义，并使用内循环进行自监督学习，最终的预测结果得到改进。该方法在图像分类任务上表现优越。

    

    我们将监督学习问题重新定义为通过两层嵌套循环进行学习的方法。内循环在每个个体实例上进行自监督学习，然后进行最终的预测。外循环学习内循环使用的自监督任务，以改进最终的预测结果。当内循环学习器为线性模型时，我们的内循环等同于线性注意力；当内循环学习器为核估计器时，等同于自注意力。为了与线性或自注意力层进行实际比较，我们在transformer中用内循环替代了每个线性或自注意力层，因此我们的外循环等同于对架构进行训练。当每个内循环学习器为神经网络时，在ImageNet的224 x 224原始像素上，我们的方法在准确度和FLOPs方面远远优于具有线性注意力的transformer，而（常规的）transformer无法运行。

    We reformulate the problem of supervised learning as learning to learn with two nested loops (i.e. learning problems). The inner loop learns on each individual instance with self-supervision before final prediction. The outer loop learns the self-supervised task used by the inner loop, such that its final prediction improves. Our inner loop turns out to be equivalent to linear attention when the inner-loop learner is only a linear model, and to self-attention when it is a kernel estimator. For practical comparison with linear or self-attention layers, we replace each of them in a transformer with an inner loop, so our outer loop is equivalent to training the architecture. When each inner-loop learner is a neural network, our approach vastly outperforms transformers with linear attention on ImageNet from 224 x 224 raw pixels in both accuracy and FLOPs, while (regular) transformers cannot run.
    
[^119]: 什么是一个好问题？基于任务的询问与事实级遮蔽。

    What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])

    [http://arxiv.org/abs/2310.11571](http://arxiv.org/abs/2310.11571)

    本论文提出了基于任务的询问（TOA）的概念和框架，介绍了一种用于生成对推理任务有用答案的问题的方法。同时还提出了一种事实级遮蔽（FLM）的技术，用于将自然语言数据集转换为自我监督的TOA数据集。

    

    提问是现实生活中合作推理任务（如问答）的重要组成部分。例如，一个法律助手聊天机器人在没有用户情况的具体信息的情况下可能无法提供准确的建议。然而，通常会直接使用大型语言模型来解决推理任务，而不会向用户或第三方提出后续问题。我们将这个问题称为基于任务的询问（TOA）。零-shot聊天模型可以执行TOA，但它们的训练主要基于下一个词预测，而不是问题是否对成功的合作有帮助。为了能够训练和评估TOA模型，我们提出了自然语言任务导向询问的定义和框架，即生成能够为推理任务提供有用答案的问题的问题。我们还提出了事实级遮蔽（FLM）的方法，通过省略特定的部分将自然语言数据集转换为自我监督的TOA数据集。

    Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula
    
[^120]: 联邦多目标学习

    Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09866](http://arxiv.org/abs/2310.09866)

    本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。

    

    在最近几年中，多目标优化（MOO）作为许多多代理多任务学习应用的基础问题出现。然而，现有的MOO算法仍局限于集中式学习环境，无法满足这些多代理多任务学习应用的分布式性质和数据隐私需求。这激发了我们提出一种新的联邦多目标学习（FMOL）框架，其中多个客户端在保持他们的训练数据私密的同时，分布式协作解决一个MOO问题。值得注意的是，我们的FMOL框架允许不同客户端上的不同目标函数集合，以支持广泛的应用，这首次将MOO形式化推广到联邦学习范式中。对于这个FMOL框架，我们提出了两种新的联邦多目标优化（FMOO）算法，称为联邦多梯度下降平均（FMGDA）和联邦随机梯度下降（Federated SGD）。

    In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc
    
[^121]: 在协变量转移下，仅凭少量测试样本改善公平性和准确性的权衡

    Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift. (arXiv:2310.07535v1 [cs.LG])

    [http://arxiv.org/abs/2310.07535](http://arxiv.org/abs/2310.07535)

    在协变量转移下，我们提出了一种新的损失函数和表示匹配损失来优化模型的准确性和公平性，通过实验证明在公平性和准确性权衡方面优于其他基线算法，并且提出了一种未经研究的非对称协变量转移设置。

    

    测试数据中的协变量转移会显著降低模型的准确性和公平性表现。在这种情况下，确保不同敏感群体之间的公平性非常重要，因为这涉及到诸如刑事司法等社会影响。我们在无监督的情况下进行操作，只有一小组无标签的测试样本和一个带标签的训练集可用。为了解决这个问题，我们提出了三个贡献。第一个贡献是基于新型复合加权熵的目标函数，用于预测准确性，并通过表示匹配损失来优化公平性。我们通过实验证明，使用我们的损失函数进行优化，在几个标准数据集上在公平性-准确性权衡方面优于许多最先进的基线算法。我们的第二个贡献是一个新的设置，我们称之为非对称协变量转移，在我们所知的范围内尚未研究过非对称协变量转移。

    Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift
    
[^122]: 高阶DeepTrails：对*Trails的统一方法

    Higher-Order DeepTrails: Unified Approach to *Trails. (arXiv:2310.04477v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04477](http://arxiv.org/abs/2310.04477)

    本论文提出了一种用于分析人类行为的统一方法，通过使用自回归语言模型来捕捉序列中的高阶依赖关系，以改进Web浏览或交通导航等应用的底层基础设施或用户界面。

    

    分析、理解和描述人类行为在不同的环境中具有优势，如网络浏览或交通导航。理解人类行为自然有助于改进和优化底层基础设施或用户界面。通常，人类导航是由状态间转换的序列表示。先前的工作建议使用假设来分析这些转换，代表不同的导航直观。为了在数学上抓住这个设置，使用一阶马尔可夫链来捕捉行为，因此可以应用不同类型的图比较，但是会有损失序列中的高阶依赖信息的固有缺点。为此，我们提出使用自回归语言模型来分析整个序列，因为它们通常用于建模序列中的高阶依赖关系。我们展示了我们的方法可以轻松地适应不同的设置。

    Analyzing, understanding, and describing human behavior is advantageous in different settings, such as web browsing or traffic navigation. Understanding human behavior naturally helps to improve and optimize the underlying infrastructure or user interfaces. Typically, human navigation is represented by sequences of transitions between states. Previous work suggests to use hypotheses, representing different intuitions about the navigation to analyze these transitions. To mathematically grasp this setting, first-order Markov chains are used to capture the behavior, consequently allowing to apply different kinds of graph comparisons, but comes with the inherent drawback of losing information about higher-order dependencies within the sequences. To this end, we propose to analyze entire sequences using autoregressive language models, as they are traditionally used to model higher-order dependencies in sequences. We show that our approach can be easily adapted to model different settings in
    
[^123]: ResidualTransformer：带有权重共享的残差低秩学习的Transformer层

    ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers. (arXiv:2310.02489v1 [cs.CL])

    [http://arxiv.org/abs/2310.02489](http://arxiv.org/abs/2310.02489)

    本文提出了一种名为ResidualTransformer的方法，通过重新参数化Transformer编码器层之间的模型权重，将模型的大小减小。实验结果表明，ResidualTransformer的性能优于传统Transformer模型，且模型大小得到了显著减小。

    

    在部署语音处理模型到始终开启设备上时，内存限制是一个主要关注点之一。虽然使用足够大量的数据训练得到的更大的模型通常表现更好，但使其适应设备内存是一个具有挑战性的问题。在本文中，我们旨在通过重新参数化Transformer编码器层之间的模型权重，并假设特殊的权重组合和结构，来减小模型的大小。更具体地说，受ResNet和最新的LoRA工作的启发，我们提出了一种名为ResidualTransformer的方法，其中Transformer层中的每个权重矩阵包括1）与其相邻层共享的满秩组件，和2）仅属于它自己的独特低秩组件。低秩矩阵只占模型大小的一小部分。此外，我们添加对角线权重矩阵来提高低秩矩阵的建模能力。我们的10k小时语音识别和语音翻译任务的实验结果表明，ResidualTransformer的性能优于传统Transformer模型，且模型大小得到了显著减小。

    Memory constraint of always-on devices is one of the major concerns when deploying speech processing models on these devices. While larger models trained with sufficiently large amount of data generally perform better, making them fit in the device memory is a demanding challenge. In this paper, we aim to reduce model size by reparameterizing model weights across Transformer encoder layers and assuming a special weight composition and structure. More specifically, inspired by ResNet and the more recent LoRA work, we propose an approach named ResidualTransformer, where each weight matrix in a Transformer layer comprises 1) a shared full-rank component with its adjacent layers, and 2) a unique low-rank component to itself. The low-rank matrices only account for a small amount of model size increase. In addition, we add diagonal weight matrices to improve modeling capacity of the low-rank matrices. Experiments of our 10k-hour speech recognition and speech translation tasks show that the T
    
[^124]: 通过对大型语言模型进行微调在低资源环境中合成数据生成

    Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models. (arXiv:2310.01119v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01119](http://arxiv.org/abs/2310.01119)

    通过对大型语言模型进行微调，在低资源环境中可以通过合成数据生成来改善较小模型的性能，显著提高了下游模型的性能。

    

    大型语言模型(LLMs)的上下文学习能力使它们能够以相对较少的标记样本推广到新的下游任务。然而，它们需要巨大的计算资源才能部署。相反，如果用足够多的标记样本对较小的模型进行微调，它们可以解决特定任务。然而，这些样本获取起来很昂贵。为了追求两全其美，我们研究了通过对经过精细调整的教师LLMs生成的训练数据进行合成的合成数据生成，以改善较小模型的下游性能。在四个文本分类和两个文本生成任务中，我们发现数据生成和注释都显著提高了相应下游模型的性能，有时只需要原始训练数据集的一小部分。

    The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset.
    
[^125]: 在差分隐私学习中的在线敏感度优化

    Online Sensitivity Optimization in Differentially Private Learning. (arXiv:2310.00829v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00829](http://arxiv.org/abs/2310.00829)

    本文提出了一种在线敏感度优化方法，通过建立剪切阈值和成本函数之间的关系来避免在差分隐私学习中产生的隐私开销，并实现了动态调整剪切阈值的功能。

    

    训练差分隐私机器学习模型需要限制个体对优化过程的贡献。为了实现这一目标，在平均和批次消毒之前，将个体的梯度的2-范数剪切到预定的阈值。这种选择在两个相对的方面对优化产生不利影响：在较低值时过度剪切加剧了偏差，而在较高值时增加了消毒噪声。这个选择在很大程度上取决于数据集、模型架构，甚至在同一个优化过程中也有所不同，需要通过网格搜索来进行细致调整。为了避免在超参数调优中产生的隐私开销，我们提出了一种新的方法来动态优化剪切阈值。我们将这个阈值视为额外的可学习参数，建立了阈值和成本函数之间的清晰关系。这使我们能够优化前者，自动获得一个调整好的剪切阈值，从而在差分隐私学习中降低了调优的隐私开销。

    Training differentially private machine learning models requires constraining an individual's contribution to the optimization process. This is achieved by clipping the $2$-norm of their gradient at a predetermined threshold prior to averaging and batch sanitization. This selection adversely influences optimization in two opposing ways: it either exacerbates the bias due to excessive clipping at lower values, or augments sanitization noise at higher values. The choice significantly hinges on factors such as the dataset, model architecture, and even varies within the same optimization, demanding meticulous tuning usually accomplished through a grid search. In order to circumvent the privacy expenses incurred in hyperparameter tuning, we present a novel approach to dynamically optimize the clipping threshold. We treat this threshold as an additional learnable parameter, establishing a clean relationship between the threshold and the cost function. This allows us to optimize the former wi
    
[^126]: 贝叶斯设计原则用于频率式序贯学习问题

    Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00806](http://arxiv.org/abs/2310.00806)

    该论文提出了一种贝叶斯设计原则用于优化频率式序贯学习问题的通用理论，通过生成“算法信念”并使用贝叶斯后验进行决策，实现了无先验的贝叶斯类型算法在对抗性环境中的泛化和优化应用。

    

    我们提出了一种通用理论，用于优化频率误差求和的序贯学习问题，可以通过统一的贝叶斯原则得到高效的强化学习和赌博机算法。我们提出了一种新颖的优化方法，在每一轮生成“算法信念”，并使用贝叶斯后验进行决策。我们提出的优化目标是创建“算法信念”，我们将其称为“算法信息比”，有效地表征了任何算法的频率误差的内在复杂度度量。据我们所知，这是第一种将贝叶斯式算法无先验地并且在对抗性环境中以通用和最优方式应用的系统方法。此外，这些算法简单且通常容易实现。作为一个重要应用，我们提出了一种新的多臂赌博机算法，在随机、对抗和非...

    We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non
    
[^127]: 基于大型预训练基础模型的多语种演讲者转换检测模型（USM-SCD）

    USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models. (arXiv:2309.08023v1 [eess.AS])

    [http://arxiv.org/abs/2309.08023](http://arxiv.org/abs/2309.08023)

    USM-SCD是一种基于大型预训练基础模型的多语种演讲者转换检测模型，通过微调模型参数，可以同时检测演讲者转换并为96种语言执行自动语音识别。在实验中表现出了优异的性能。

    

    我们提出了一种多语种演讲者转换检测模型（USM-SCD），可以同时检测演讲者转换并为96种语言执行自动语音识别。该模型是从一个经过大量受监督和无监督数据训练的语音基础模型进行调整而来的，展示了从大型通用基础模型到下游任务的微调的实用性。通过一系列消融研究，我们分析了这个多语种演讲者转换检测模型的性能。我们展示了USM-SCD模型可以在由来自96种语言的数据构成的测试集上实现超过75%的平均演讲者转换检测F1得分。在美式英语上，USM-SCD模型可以在各种公共和内部测试集上实现85.8%的演讲者转换检测F1得分，相对于之前的单语言基准模型提高了21%。我们还展示了只需要微调可训练模型参数的四分之一就可以实现最佳模型性能。

    We introduce a multilingual speaker change detection model (USM-SCD) that can simultaneously detect speaker turns and perform ASR for 96 languages. This model is adapted from a speech foundation model trained on a large quantity of supervised and unsupervised data, demonstrating the utility of fine-tuning from a large generic foundation model for a downstream task. We analyze the performance of this multilingual speaker change detection model through a series of ablation studies. We show that the USM-SCD model can achieve more than 75% average speaker change detection F1 score across a test set that consists of data from 96 languages. On American English, the USM-SCD model can achieve an 85.8% speaker change detection F1 score across various public and internal test sets, beating the previous monolingual baseline model by 21% relative. We also show that we only need to fine-tune one-quarter of the trainable model parameters to achieve the best model performance. The USM-SCD model exhib
    
[^128]: 通过在线凸优化实现在线子模最大化

    Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])

    [http://arxiv.org/abs/2309.04339](http://arxiv.org/abs/2309.04339)

    本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。

    

    我们研究了在线设置下的一般性子模最大化问题在一般性模性约束下。我们证明了在线优化一类大型子模函数，即加权阈值势函数，可以归约到在线凸优化(OCO)问题。这是因为这个类别的函数可以进行凹松弛;因此，结合适当的舍入方案，OCO策略可以在组合设置中实现次线性遗憾。我们还展示了我们的简化方式可以应用在许多不同版本的在线学习问题中，包括动态遗憾、强盗和乐观学习等设置。

    We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
    
[^129]: 通过生成对抗神经算子实现宽带地面运动合成: 开发与验证

    Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation. (arXiv:2309.03447v1 [physics.geo-ph])

    [http://arxiv.org/abs/2309.03447](http://arxiv.org/abs/2309.03447)

    本论文提出了一种使用生成对抗神经算子的数据驱动地面运动合成模型，可以根据不同参数生成三分量加速度时间历史。通过使用神经算子架构，模型训练不受数据采样频率影响。研究结果表明，该模型在验证和应用实例中表现出色，并可用于生成日本地震动数据。

    

    我们提出了一种使用生成对抗神经算子（GANO）的数据驱动地面运动合成模型，该模型结合了机器学习和开放获取的强震动数据集，可以根据矩震级（M）、断裂距离（R_{rup}）、顶部30m处的时间平均剪切波速度（V_{S30}）和构造环境或断层类型生成三分量加速度时间历史。我们使用神经算子，这是一种分辨率无关的架构，可以保证模型训练与数据采样频率无关。首先，我们提出了条件地面运动合成算法（以下简称cGM-GANO）并讨论其与先前工作相比的优势。接下来，我们使用南加州地震中心（SCEC）宽带平台（BBP）产生的模拟地震动验证了cGM-GANO框架。最后，我们在日本的KiK-net数据集上训练了cGM-GANO，表明该框架可以重新生成地震动数据。

    We present a data-driven model for ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) that combines recent advancements in machine learning and open access strong motion data sets to generate three-component acceleration time histories conditioned on moment magnitude ($M$), rupture distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$ ($V_{S30}$), and tectonic environment or style of faulting. We use Neural Operators, a resolution invariant architecture that guarantees that the model training is independent of the data sampling frequency. We first present the conditional ground-motion synthesis algorithm (referred to heretofore as cGM-GANO) and discuss its advantages compared to previous work. Next, we verify the cGM-GANO framework using simulated ground motions generated with the Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the framework can rec
    
[^130]: 通过提示策略增强评论文本的多领域情感分析

    Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies. (arXiv:2309.02045v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.02045](http://arxiv.org/abs/2309.02045)

    通过提示策略，本论文探究了如何通过应用角色扮演和思维链提示策略来增强大型语言模型（LLMs）在情感分析中的性能，并在三个不同领域的数据集上进行了评估。

    

    大型语言模型（LLMs）在科学研究和实际应用中取得了重大进展。现有研究已证明了LLMs在各种自然语言处理任务中的最新性能。然而，如何通过提示策略进一步增强LLMs在特定任务中的性能仍然是一个重要问题。本文探讨了通过应用提示策略来提高LLMs在情感分析中的性能。我们对情感分析任务的提示过程进行了建模，并介绍了两种针对情感分析的新颖策略：角色扮演（RP）提示和思维链（CoT）提示。具体地，我们还提出了RP-CoT提示策略，它是RP提示和CoT提示的结合。我们在三个不同领域的数据集上进行了比较实验，以评估所提出的情感分析策略的有效性。结果表明...

    Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate tha
    
[^131]: 带有侧面观测的随机图赌博学习

    Stochastic Graph Bandit Learning with Side-Observations. (arXiv:2308.15107v1 [cs.LG])

    [http://arxiv.org/abs/2308.15107](http://arxiv.org/abs/2308.15107)

    本文研究了具有一般函数空间和图反馈的随机背景赌博学习问题，并提出了一种算法，填补了此前研究中的空白。该算法提供了奖励差距依赖的上界，并在遗憾上界方面提供了改进。数值实验证明了该方法的计算效率和有效性。

    

    本文研究了具有一般函数空间和图反馈的随机背景赌博问题。我们提出了一种算法，通过适应潜在的图结构和奖励差距来解决这个问题。据我们所知，我们的算法是在这个随机设置下提供奖励差距依赖的上界的第一个算法，填补了[35]的研究空白。与[31,33,35]相比，我们的方法提供了改进的遗憾上界，并且不需要图形数量的知识。我们进行数值实验，以证明我们的方法在遗憾上界方面的计算效率和有效性。这些发现突显了我们算法在推进具有图反馈的随机背景赌博领域方面的重要性，并为各个领域的实际应用开辟了道路。

    In this paper, we investigate the stochastic contextual bandit with general function space and graph feedback. We propose an algorithm that addresses this problem by adapting to both the underlying graph structures and reward gaps. To the best of our knowledge, our algorithm is the first to provide a gap-dependent upper bound in this stochastic setting, bridging the research gap left by the work in [35]. In comparison to [31,33,35], our method offers improved regret upper bounds and does not require knowledge of graphical quantities. We conduct numerical experiments to demonstrate the computational efficiency and effectiveness of our approach in terms of regret upper bounds. These findings highlight the significance of our algorithm in advancing the field of stochastic contextual bandits with graph feedback, opening up avenues for practical applications in various domains.
    
[^132]: 保证稳定的二次模型及其在SINDy和操作推断中的应用

    Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference. (arXiv:2308.13819v1 [cs.LG])

    [http://arxiv.org/abs/2308.13819](http://arxiv.org/abs/2308.13819)

    本研究提出了一种学习稳定二次模型的推断方法，通过设置适当的优化问题，可以构建具有局部和全局稳定性的二次系统模型。

    

    科学机器学习用于学习动力系统是一种强大的工具，它将数据驱动建模、基于物理建模和经验知识相结合。在工程设计循环和数字双胞胎中起着重要的作用。本文主要关注一种操作推断方法，该方法通过在模型结构上提出先验假设（通常由已知物理学或专家给出），从而构建低维动态模型。然后，我们通过设置适当的优化问题来学习模型的运算符以进行推断。动态系统的一个关键属性是稳定性。然而，推断模型无法保证这种属性。本文提出了一种学习二次模型的推断公式，这些模型在设计上是稳定的。具体而言，我们讨论了局部和全局稳定的二次系统的参数化。此外，对于没有稳定点但有界的二次系统：

    Scientific machine learning for learning dynamical systems is a powerful tool that combines data-driven modeling models, physics-based modeling, and empirical knowledge. It plays an essential role in an engineering design cycle and digital twinning. In this work, we primarily focus on an operator inference methodology that builds dynamical models, preferably in low-dimension, with a prior hypothesis on the model structure, often determined by known physics or given by experts. Then, for inference, we aim to learn the operators of a model by setting up an appropriate optimization problem. One of the critical properties of dynamical systems is{stability. However, such a property is not guaranteed by the inferred models. In this work, we propose inference formulations to learn quadratic models, which are stable by design. Precisely, we discuss the parameterization of quadratic systems that are locally and globally stable. Moreover, for quadratic systems with no stable point yet bounded (e
    
[^133]: STEM:释放Embedding在多任务推荐中的力量

    STEM: Unleashing the Power of Embeddings for Multi-task Recommendation. (arXiv:2308.13537v1 [cs.IR])

    [http://arxiv.org/abs/2308.13537](http://arxiv.org/abs/2308.13537)

    本文提出了一种称为STEM的新范例，用于解决多任务推荐中的负传递问题。与现有方法不同，STEM通过根据样本中正反馈数量的相对比例进行细分，深入研究样本的复杂性，以提高推荐系统的性能。

    

    多任务学习（MTL）在推荐系统中变得越来越受欢迎，因为它能够同时优化多个目标。MTL的一个关键挑战是负传递的发生，即由于任务之间的冲突导致某些任务的性能下降。现有研究通过将所有样本视为一个整体来探索负传递，忽视了其中固有的复杂性。为此，我们根据任务之间正反馈的相对数量将样本进行细分，深入研究样本的复杂性。令人惊讶的是，现有MTL方法在收到各任务类似反馈的样本上仍然存在负传递。值得注意的是，现有方法通常采用共享嵌入的范例，并且我们假设它们的失败可以归因于使用这种通用嵌入来建模不同用户偏好的有限能力。

    Multi-task learning (MTL) has gained significant popularity in recommendation systems as it enables the simultaneous optimization of multiple objectives. A key challenge in MTL is the occurrence of negative transfer, where the performance of certain tasks deteriorates due to conflicts between tasks. Existing research has explored negative transfer by treating all samples as a whole, overlooking the inherent complexities within them. To this end, we delve into the intricacies of samples by splitting them based on the relative amount of positive feedback among tasks. Surprisingly, negative transfer still occurs in existing MTL methods on samples that receive comparable feedback across tasks. It is worth noting that existing methods commonly employ a shared-embedding paradigm, and we hypothesize that their failure can be attributed to the limited capacity of modeling diverse user preferences across tasks using such universal embeddings.  In this paper, we introduce a novel paradigm called
    
[^134]: SuperCalo: 能量沉积量模拟的超分辨率技术

    SuperCalo: Calorimeter shower super-resolution. (arXiv:2308.11700v1 [physics.ins-det])

    [http://arxiv.org/abs/2308.11700](http://arxiv.org/abs/2308.11700)

    本文介绍了一种名为SuperCalo的能量沉积模拟超分辨率模型，能够快速上采样高维细粒度的能量沉积模拟，从而降低计算成本和生成时间。

    

    能量沉积模拟是大型强子对撞机计算流程中的主要瓶颈。最近有一些工作使用深度生成模型来克服这个挑战，但许多表现最佳的模型在训练和生成时间上无法很好地适应高维能量沉积模拟。在本文中，我们介绍了一种名为SuperCalo的基于流的超分辨率模型，并证明了高维细粒度的能量沉积模拟可以从粗粒度模拟中快速上采样。这种新颖的方法可以降低与快速能量沉积模拟模型相关的计算成本、内存需求和生成时间。另外，我们还展示了由SuperCalo上采样得到的能量沉积模拟具有高度变化的特点。这使得可以从较少的粗粒度模拟中上采样出多个高维能量沉积模拟，以高保真度的方式进一步减少生成时间。

    Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
    
[^135]: 随机算法用于精确测量差分隐私的个性化推荐

    Randomized algorithms for precise measurement of differentially-private, personalized recommendations. (arXiv:2308.03735v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2308.03735](http://arxiv.org/abs/2308.03735)

    这项研究提出了一种随机算法，用于精确测量差分隐私的个性化推荐。通过离线实验，该算法在关键指标上与私密的非个性化和非私密的个性化实现进行了比较。

    

    个性化推荐是当今互联网生态系统的重要组成部分，它帮助艺术家和创作者吸引感兴趣的用户，同时也帮助用户发现新的有趣内容。然而，由于个人数据和数据隐私的历史上粗心对待，许多用户对个性化推荐平台持怀疑态度。现在，依赖于个性化推荐的企业正进入一个新的范例，需要对他们的系统进行改进，以保护隐私。本文提出了一种个性化推荐算法，既可以实现精确测量，又可以保护差分隐私。我们以广告为例应用，并进行离线实验，量化提出的隐私保护算法对用户体验、广告商价值和平台收入等关键指标的影响，与（私密的）非个性化和非私密的个性化实现的极端情况进行对比。

    Personalized recommendations form an important part of today's internet ecosystem, helping artists and creators to reach interested users, and helping users to discover new and engaging content. However, many users today are skeptical of platforms that personalize recommendations, in part due to historically careless treatment of personal data and data privacy. Now, businesses that rely on personalized recommendations are entering a new paradigm, where many of their systems must be overhauled to be privacy-first. In this article, we propose an algorithm for personalized recommendations that facilitates both precise and differentially-private measurement. We consider advertising as an example application, and conduct offline experiments to quantify how the proposed privacy-preserving algorithm affects key metrics related to user experience, advertiser value, and platform revenue compared to the extremes of both (private) non-personalized and non-private, personalized implementations.
    
[^136]: 在无线网络中的分层联邦学习：剪枝解决带宽稀缺和系统异构性问题

    Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v1 [eess.SY])

    [http://arxiv.org/abs/2308.01562](http://arxiv.org/abs/2308.01562)

    本研究提出了一种剪枝增强的分层联邦学习（PHFL）方法，用于解决无线网络中的带宽稀缺和系统异构性问题。通过模型剪枝和无线通信的优化，实现了在严格的延迟和能耗约束下收敛速度的最优化。

    

    在实际无线网络中，由于终端用户与中央服务器之间存在多个层级，用户设备的计算和电池能力有限，而服务基站具有固定的带宽。鉴于这些实际约束和系统模型，本文利用模型剪枝，并提出一种适用于异构网络的剪枝增强分层联邦学习（PHFL）方法。我们首先推导出收敛速度的上界，清晰地展示了模型剪枝和客户端与关联基站之间的无线通信的影响。然后，我们联合优化模型剪枝比率、客户端的中央处理器（CPU）频率和传输功率，以在严格的延迟和能耗约束下最小化收敛界的可控项。然而，由于原始问题不是凸问题，我们采用逐步凸逼近（SCA）方法，联合优化参数。

    While a practical wireless network has many tiers where end users do not directly communicate with the central server, the users' devices have limited computation and battery powers, and the serving base station (BS) has a fixed bandwidth. Owing to these practical constraints and system models, this paper leverages model pruning and proposes a pruning-enabled hierarchical federated learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper bound of the convergence rate that clearly demonstrates the impact of the model pruning and wireless communications between the clients and the associated BS. Then we jointly optimize the model pruning ratio, central processing unit (CPU) frequency and transmission power of the clients in order to minimize the controllable terms of the convergence bound under strict delay and energy constraints. However, since the original problem is not convex, we perform successive convex approximation (SCA) and jointly optimize the parameters fo
    
[^137]: MetaDiff: 使用条件扩散进行元学习，以进行少样本学习

    MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning. (arXiv:2307.16424v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16424](http://arxiv.org/abs/2307.16424)

    使用条件扩散作为元学习的一种方法，能够解决深度模型在少样本学习中遇到的梯度计算和消失风险问题。

    

    在人工智能中，让深度模型具备少样本学习的能力，即从少量样本快速学习，是一个核心挑战。基于梯度的元学习方法通过学习如何学习新任务有效地解决了这个挑战。其关键思想是以双层优化的方式学习深度模型，其中外部循环过程学习共享的梯度下降算法（即其超参数），而内部循环过程利用它来通过仅使用少量有标签数据对任务特定的模型进行优化。尽管这些现有方法已经显示出卓越的性能，但外部循环过程需要沿着内部优化路径计算二阶导数，这会带来可观的内存负担和梯度消失的风险。受到扩散模型的最新进展的启发，我们发现内部循环的梯度下降过程实际上可以看作是一个扩散的反过程（即去噪），这个扩散的目标是恢复任务特定的模型参数。

    Equipping a deep model the abaility of few-shot learning, i.e., learning quickly from only few examples, is a core challenge for artificial intelligence. Gradient-based meta-learning approaches effectively address the challenge by learning how to learn novel tasks. Its key idea is learning a deep model in a bi-level optimization manner, where the outer-loop process learns a shared gradient descent algorithm (i.e., its hyperparameters), while the inner-loop process leverage it to optimize a task-specific model by using only few labeled data. Although these existing methods have shown superior performance, the outer-loop process requires calculating second-order derivatives along the inner optimization path, which imposes considerable memory burdens and the risk of vanishing gradients. Drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be actually viewed as a reverse process (i.e., denoising) of diffusion where the targe
    
[^138]: 多模态讨论变换器：整合文本、图像和图变换器以检测社交媒体上的仇恨言论。

    Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])

    [http://arxiv.org/abs/2307.09312](http://arxiv.org/abs/2307.09312)

    多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。

    

    我们提出了一种新颖的多模态基于图的变换器模型，名为多模态讨论变换器（mDT），用于检测在线社交网络中的仇恨言论。与传统的仅使用文本的方法不同，我们将标记评论为仇恨言论的方法围绕文本和图像的整体分析展开。这是通过利用图变换器来捕捉评论周围整个讨论中的上下文关系，并采用交织融合层来组合文本和图像嵌入，而不是单独处理不同的模态。我们将模型的性能与仅处理文本的基线进行比较，还进行了广泛的消融研究。最后，我们展望了多模态解决方案在在线环境中提供社会价值的未来工作，并认为捕捉对话的整体视图极大地推进了检测反社会行为的努力。

    We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
    
[^139]: 半监督和自监督学习在医学图像上的准确性与时间前沿比较

    Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])

    [http://arxiv.org/abs/2307.08919](http://arxiv.org/abs/2307.08919)

    半监督和自监督学习在医学图像上的准确性与时间前沿进行比较，通过一个精心设计的基准研究来回答从业者的问题。

    

    对于许多医学图像分类器的应用来说，每个图像都很难或昂贵地获得一个可信的标签。相比之下，没有标签的图像更容易获取。两个主要研究方向都承诺额外的无标签数据可以提高分类器的性能：自监督学习仅在无标签数据上预训练有用的表示，然后通过标记集对这些表示进行微调以获得分类器；半监督学习同时在标记和无标签数据上直接训练分类器。最近的方法从两个方向上都声称在非医学任务上取得了显著的收益，但没有系统评估医学图像，并且大多只与同一方向的方法进行比较。本研究提供了一个经过精心设计的基准来回答从业者的一个关键问题：在小型标记数据集和有限的培训时间预算下，额外的无标签图像能够产生多大的收益。

    For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and 
    
[^140]: 数据流中的差分隐私聚类

    Differentially Private Clustering in Data Streams. (arXiv:2307.07449v1 [cs.DS])

    [http://arxiv.org/abs/2307.07449](http://arxiv.org/abs/2307.07449)

    本研究提出了首个针对$k$-means和$k$-median聚类的差分隐私流算法，在流模型中实现对数据隐私的保护，并使用尽可能少的空间。

    

    流模型是处理大规模现代数据分析的一种常见方法。在流模型中，数据点依次流入，算法只能对数据流进行一次遍历，目标是在使用尽可能少的空间的同时，在流中进行一些分析。聚类问题是基本的无监督机器学习原语，过去已经对流聚类算法进行了广泛的研究。然而，在许多实际应用中，数据隐私已成为一个核心关注点，非私有聚类算法在许多场景下不适用。在这项工作中，我们提供了第一个针对$k$-means和$k$-median聚类的差分私有流算法，该算法在长度最多为$T$的流上使用$poly(k,d,\log(T))$的空间来实现一个“常数”。

    The streaming model is an abstraction of computing over massive data streams, which is a popular way of dealing with large-scale modern data analysis. In this model, there is a stream of data points, one after the other. A streaming algorithm is only allowed one pass over the data stream, and the goal is to perform some analysis during the stream while using as small space as possible.  Clustering problems (such as $k$-means and $k$-median) are fundamental unsupervised machine learning primitives, and streaming clustering algorithms have been extensively studied in the past. However, since data privacy becomes a central concern in many real-world applications, non-private clustering algorithms are not applicable in many scenarios.  In this work, we provide the first differentially private streaming algorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean data points over a stream with length at most $T$ using $poly(k,d,\log(T))$ space to achieve a {\it constant} 
    
[^141]: RL$^3$:通过RL内部的RL$^2$提升元强化学习方法

    RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])

    [http://arxiv.org/abs/2306.15909](http://arxiv.org/abs/2306.15909)

    RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。

    

    元强化学习（meta-RL）方法，如RL$^2$，已经成为学习针对给定任务分布的数据高效的强化学习算法的有希望的方法。然而，这些强化学习算法在长期任务和超出分布任务方面存在困难，因为它们依赖于递归神经网络来处理经验序列，而不是将它们总结为一般的强化学习组件，例如价值函数。此外，即使是transformers在训练和推理成本变得禁止之前也对它们可以有效推理的历史长度有实际限制。相比之下，传统的强化学习算法在数据效率方面不足，因为它们没有利用领域知识，但随着更多数据的可用性，它们会收敛到最优策略。在本文中，我们提出了RL$^3$，一种组合了传统强化学习和元强化学习的原则性混合方法，通过将通过传统强化学习学习到的特定任务动作值作为元强化学习神经网络的一个输入。

    Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
    
[^142]: RM-PRT: 真实的机器人操作模拟器和基于渐进推理任务的基准评估

    RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.11335](http://arxiv.org/abs/2306.11335)

    该论文介绍了一个基于逼真机器人操作模拟器的基准评估RM-PRT，并提出了通用的评估流程，旨在通过大规模自然语言指令和多模态提示对机器人操作进行详细评估。

    

    最近，预训练的大规模语言模型（LLM），如ChatGPT和GPT-4的出现，显着推进了机器的自然语言理解能力。这一突破使我们能够将这些开源LLM无缝地集成到统一的机器人模拟器环境中，以帮助机器人准确理解和执行人类自然语言指令。为此，我们引入了一个逼真的机器人操作模拟器，并在此基础上构建了一个基于渐进推理任务的机器人操作基准（RM-PRT）。具体而言，RM-PRT基准评估基于Unreal Engine 5构建了一个新的高保真数字双胞胎场景，其中包括782个类别，2023个物体，并使用ChatGPT生成了15,000个自然语言指令，以详细评估机器人操作。我们提出了一个通用的RM-PRT基准评估流程，该流程接受包含自然语言指令的多模态提示作为输入，并自动输出行动。

    Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
    
[^143]: 关于强化学习中模型错误指定的研究

    On the Model-Misspecification in Reinforcement Learning. (arXiv:2306.10694v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10694](http://arxiv.org/abs/2306.10694)

    这项研究研究了强化学习中模型错误指定问题，发现基于策略的方法在策略函数逼近存在较大误差的情况下仍然具有稳健性，但对于基于值和基于模型的方法在一般函数逼近中是否能实现类似的稳健性仍然存在疑问。

    

    强化学习的成功关键在于处理复杂的真实模型时的有效函数逼近。现有的样本有效的强化学习算法主要采用三种函数逼近方法：基于策略的方法、基于值的方法和基于模型的方法。然而，在面对模型错误指定的情况下（真实模型与最优函数逼近器之间的不一致），研究表明基于策略的方法即使在策略函数逼近存在较大局部有界错误的情况下也能保持稳健性。其中函数类可能在特定状态和动作下呈现$Ω(1)$的逼近误差，但在由策略引起的状态分布下保持较小的平均误差。然而，对于基于值和基于模型的方法在一般函数逼近中是否能实现类似的稳健性仍然是一个开放的问题。

    The success of reinforcement learning (RL) crucially depends on effective function approximation when dealing with complex ground-truth models. Existing sample-efficient RL algorithms primarily employ three approaches to function approximation: policy-based, value-based, and model-based methods. However, in the face of model misspecification (a disparity between the ground-truth and optimal function approximators), it is shown that policy-based approaches can be robust even when the policy function approximation is under a large locally-bounded misspecification error, with which the function class may exhibit a $\Omega(1)$ approximation error in specific states and actions, but remains small on average within a policy-induced state distribution. Yet it remains an open question whether similar robustness can be achieved with value-based and model-based approaches, especially with general function approximation.  To bridge this gap, in this paper we present a unified theoretical framewor
    
[^144]: 基于紧核的条件期望估计

    Conditional expectation via compact kernels. (arXiv:2306.10592v1 [stat.ML])

    [http://arxiv.org/abs/2306.10592](http://arxiv.org/abs/2306.10592)

    本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。

    

    去噪、条件期望和流形学习任务通常可以在寻找两个随机变量积的条件期望的公共环境下表述。本文针对这个更一般的问题，描述了一种算子理论方法来估计条件期望。核积分算子被用作紧致化工具，将估计问题设置为在再生核希尔伯特空间中的线性逆问题。该方程的解被证明对数值逼近是稳定的，从而确保了数据驱动实现的收敛性。总体技术易于实现，还展示了其在一些实际问题中的成功应用。

    The separate tasks of denoising, conditional expectation and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. This paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. Kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel Hilbert space. This equation is shown to have solutions that are stable to numerical approximation, thus guaranteeing the convergence of data-driven implementations. The overall technique is easy to implement, and their successful application to some real-world problems are also shown.
    
[^145]: GAD-NR: 通过邻域重构实现图形异常检测

    GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.01951](http://arxiv.org/abs/2306.01951)

    本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。

    

    图形异常检测（GAD）是一种技术，用于识别图中的异常节点，在网络安全、欺诈检测、社交媒体垃圾检测和其他各种领域中有应用。GAD的常见方法是图自编码器（GAEs），它将图形数据编码成节点表示，并根据这些表示来评估图形的重构质量，以识别异常。然而，现有的GAE模型主要针对直接链接重构进行优化，导致在潜在空间中连接图中的节点被聚类。因此，它们擅长检测聚类型结构异常，但对不符合聚类的更复杂的结构异常存在困难。为了解决这个限制，我们提出了一种新颖的解决方案，称为GAD-NR，它是GAE的一个新变体，融合邻域重构进行图形异常检测。GAD-NR的目标是重构节点的整个邻域，涵盖本地结构

    Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
    
[^146]: AlpacaFarm: 一种从人类反馈中学习的方法模拟框架

    AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])

    [http://arxiv.org/abs/2305.14387](http://arxiv.org/abs/2305.14387)

    该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。

    

    大型语言模型（LLMs）如ChatGPT因其良好的指令跟随能力而得到了广泛应用。开发这些LLMs需要使用人类反馈进行训练的复杂且尚不明确的工作流程。将此指令跟随过程复制和理解面临三大挑战： 数据收集的高昂成本，缺乏可信的评估和缺乏参考方法实现。我们通过AlpacaFarm解决了这些挑战，这是一个低成本的模拟器，可用于从反馈中学习的研究和开发。第一，我们设计了LLM提示来模拟人类反馈，其成本比众包工作者便宜45倍，并且与人类反馈具有高度一致性。第二，我们提出了一种自动评估方法，并将其与真实世界交互中获得的人类指令进行验证。第三，我们为几种从配对反馈中学习的方法（PPO，best-of-n，expert iteration等）提供了参考实现。

    Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
    
[^147]: 自注意力动态中的聚类现象

    The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])

    [http://arxiv.org/abs/2305.05465](http://arxiv.org/abs/2305.05465)

    本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。

    

    将Transformer视为相互作用的粒子系统，当权重不随时间变化时，本文描述了学习表示的几何形状。我们展示了代表token的粒子随着时间趋于无穷大而趋向于特定的极限对象。出现的极限对象类型取决于价值矩阵的谱。此外，在一维情况下，我们证明了自我注意力矩阵收敛于低秩布尔矩阵。这些结果的组合在数学上证实了Vaswani等人的经验观察，即Transformer处理一系列token时会出现“领导者”。

    Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
    
[^148]: 使用数据内核比较基础模型

    Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])

    [http://arxiv.org/abs/2305.05126](http://arxiv.org/abs/2305.05126)

    本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。

    

    最近自主学习和神经网络扩展的进展使得可以创建大型基础模型，这些模型可以轻松地适应各种下游任务。目前比较基础模型的范式涉及在各种策划数据集上使用聚合指标进行基准测试。不幸的是，这种模型比较方法严重依赖于度量指标的选择，这使得它在理想度量不明显或不可用的情况下不适用。在这项工作中，我们提出了一种没有度量指标的基础模型比较方法，通过它们的嵌入空间几何来实现。我们的方法基于随机图理论，并促进点对点和多模型比较。此外，我们展示了如何使用我们的框架诱导一组配备有与一些下游指标强相关的距离函数的模型流形。

    Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
    
[^149]: 数据集压缩是医疗数据共享的银弹吗？

    Is dataset condensation a silver bullet for healthcare data sharing?. (arXiv:2305.03711v1 [cs.LG])

    [http://arxiv.org/abs/2305.03711](http://arxiv.org/abs/2305.03711)

    数据集压缩为医疗数据共享提供了一种既保护了隐私又保存了实用程序的新方法。

    

    对于医疗数据共享，保护个人信息至关重要，但目前并没有一个万无一失的方法。本文研究了一种深度学习新技术——数据集压缩，以在医疗人工智能研究中共享数据，结果表明前景广阔。压缩后的数据摘要了原始记录，不可逆地隐藏了个体级别的信息，达到了真正的去识别化，允许自由共享。此外，压缩数据中保留了原始深度学习实用程序，且数据量较小且模型收敛加速。在PhysioNet-2012中，20个样本的压缩数据能够使深度模型达到了80.3%的死亡预测测试AUC（而原始记录为5120个样本的85.8%），这一发现也适用于MIMIC-III和Coswara等数据集。我们还通过理论分析和经验证明了DC的隐私保护性。数据集压缩为共享医疗数据提供了一种新的方法，既保护了隐私又保存了实用程序。

    Safeguarding personal information is paramount for healthcare data sharing, a challenging issue without any silver bullet thus far. We study the prospect of a recent deep-learning advent, dataset condensation (DC), in sharing healthcare data for AI research, and the results are promising. The condensed data abstracts original records and irreversibly conceals individual-level knowledge to achieve a bona fide de-identification, which permits free sharing. Moreover, the original deep-learning utilities are well preserved in the condensed data with compressed volume and accelerated model convergences. In PhysioNet-2012, a condensed dataset of 20 samples can orient deep models attaining 80.3% test AUC of mortality prediction (versus 85.8% of 5120 original records), an inspiring discovery generalised to MIMIC-III and Coswara datasets. We also interpret the inhere privacy protections of DC through theoretical analysis and empirical evidence. Dataset condensation opens a new gate to sharing h
    
[^150]: 深度宽松弛神经网络的统计优化性

    Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])

    [http://arxiv.org/abs/2305.02657](http://arxiv.org/abs/2305.02657)

    本文研究了深度宽松弛ReLU神经网络的泛化能力，证明适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中，但过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    

    本文研究了定义在有界域$\mathcal X \subset \mathbb R^{d}$上的深度宽松弛ReLU神经网络的泛化能力。首先证明了神经网络的泛化能力可以被相应的深度神经切向核回归所完全描绘。然后，我们研究了深度神经切向核的谱特性，并证明了深度神经切向核在$\mathcal{X}$上为正定，其特征值衰减率为$(d+1)/d$。由于核回归中已经建立的理论，我们得出结论，适当早停的梯度下降训练的多层宽神经网络可以实现最小极大率，前提是回归函数在对应的NTK相关的再生核希尔伯特空间中。最后，我们证明过度拟合的多层宽神经网络在$\mathbb S^{d}$上不能很好地泛化。

    In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
    
[^151]: MaskSearch：规模化查询图像掩模

    MaskSearch: Querying Image Masks at Scale. (arXiv:2305.02375v1 [cs.DB])

    [http://arxiv.org/abs/2305.02375](http://arxiv.org/abs/2305.02375)

    MaskSearch是一个系统，通过使用新颖的索引技术和高效的过滤器验证查询执行框架，加速对图像掩模数据库的查询，可以将个体查询加速高达两个数量级，优于现有方法。

    

    在图像数据库上的机器学习任务通常会生成注释图像内容的掩模（例如显着性地图，分割地图），并且可以实现各种应用程序（例如确定模型是否学习了虚假关联，或者图像是否被恶意修改以误导模型）。尽管基于掩模属性检索示例的查询对实践者很有价值，但现有系统无法高效地支持此类查询。在本文中，我们正式确定了这个问题，并提出了一个系统MaskSearch，着重于加速对图像掩模数据库的查询。MaskSearch利用了一种新颖的索引技术和一种高效的过滤器验证查询执行框架。我们的原型在现实世界数据集上的实验表明，使用索引大小约为数据5%的MaskSearch可以将个体查询加速高达两个数量级，并且在模拟数据集探索的各种多查询工作负载上始终表现优异，优于现有方法。

    Machine learning tasks over image databases often generate masks that annotate image content (e.g., saliency maps, segmentation maps) and enable a variety of applications (e.g., determine if a model is learning spurious correlations or if an image was maliciously modified to mislead a model). While queries that retrieve examples based on mask properties are valuable to practitioners, existing systems do not support such queries efficiently. In this paper, we formalize the problem and propose a system, MaskSearch, that focuses on accelerating queries over databases of image masks. MaskSearch leverages a novel indexing technique and an efficient filter-verification query execution framework. Experiments on real-world datasets with our prototype show that MaskSearch, using indexes approximately 5% the size of the data, accelerates individual queries by up to two orders of magnitude and consistently outperforms existing methods on various multi-query workloads that simulate dataset explora
    
[^152]: 深层潜在位置主题模型用于带有文本边的网络聚类和表示

    The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges. (arXiv:2304.08242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.08242](http://arxiv.org/abs/2304.08242)

    深层潜在位置主题模型用于网络聚类和表示，通过基于模型的聚类策略和概率模型对节点和边进行联合表示，并使用模型选择准则进行参数选择。

    

    数值交互导致用户共享其他人发布的文本内容，这些内容自然地由将个体与节点关联和交换的文本定义为边的网络来表示。为了理解这些异构和复杂的数据结构，将节点聚类为同类群组以及呈现可理解的数据可视化是必要的。为了解决这两个问题，我们引入了Deep-LPTM，这是一种基于模型的聚类策略，依赖于变分图自动编码器方法以及概率模型来描述讨论的主题。Deep-LPTM允许在两个嵌入空间中构建节点和边的联合表示。参数使用变分推断算法进行推断。我们还引入了IC2L，这是一种专门设计用于选择具有相关聚类和可视化属性的模型的模型选择准则。对合成数据进行了广泛的基准测试研究。特别是

    Numerical interactions leading to users sharing textual content published by others are naturally represented by a network where the individuals are associated with the nodes and the exchanged texts with the edges. To understand those heterogeneous and complex data structures, clustering nodes into homogeneous groups as well as rendering a comprehensible visualisation of the data is mandatory. To address both issues, we introduce Deep-LPTM, a model-based clustering strategy relying on a variational graph auto-encoder approach as well as a probabilistic model to characterise the topics of discussion. Deep-LPTM allows to build a joint representation of the nodes and of the edges in two embeddings spaces. The parameters are inferred using a variational inference algorithm. We also introduce IC2L, a model selection criterion specifically designed to choose models with relevant clustering and visualisation properties. An extensive benchmark study on synthetic data is provided. In particular
    
[^153]: 神经网络中的因果归因学习：超越直接影响

    Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])

    [http://arxiv.org/abs/2303.13850](http://arxiv.org/abs/2303.13850)

    本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。

    

    近年来，捕捉和维护神经网络模型中的因果关系引起了越来越多的关注。本文研究了用因果方法估计和维护神经网络模型中的输入-输出属性。特别地，现有的研究仅假设输入变量独立（由于神经网络架构），因此仅研究直接影响。我们将神经网络视为结构性因果模型，并提出在输入特征中引入边缘以捕捉和维护直接和间接因果效应的简单而有效的方法，同时训练神经网络模型。我们还提出有效的近似策略来量化高维数据的因果归因。我们在合成和真实数据集上进行了各种实验，结果表明，所提出的方法学习了接近基本事实效果的直接和间接因果归因。

    There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
    
[^154]: 通过风险分解评估自监督学习

    Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03068](http://arxiv.org/abs/2302.03068)

    通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。

    

    自监督学习（SSL）的流程设计涉及架构、增强和预训练数据等诸多选择。然而，SSL通常使用单一度量来评估，这并不能提供深入的洞察和改进方案。为解决这些问题，我们提出了一个SSL风险分解，从逼近、表示可用性、探针泛化和编码器泛化等角度对错误进行分解。我们分析了30个设计选择对169个在ImageNet上评估的SSL视觉模型的影响，并为每个组件提供了高效的估计器，为SSL模型的设计和使用提供宝贵的见解。

    Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
    
[^155]: 受限在线两阶段随机优化：通过对抗学习获得近似最优算法

    Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00997](http://arxiv.org/abs/2302.00997)

    在线两阶段随机优化算法的累计目标值最小化，同时保证长期平均第二阶段决策结果属于一个集合。采用对抗性学习算法从在线两阶段问题中开发在线算法，其遗憾界可以降至嵌入对抗性学习算法的遗憾界，并在各种设置下获得了新的结果。

    

    我们考虑一个在线两阶段随机优化问题，其具有有限的$T$期紧约束条件。在每个时间段，我们先作出第一阶段决策，然后观察模型参数的实现，最后从取决于第一阶段决策和模型参数的可行集中做出第二阶段决策。我们旨在最小化累计目标值，同时保证长期平均的第二阶段决策属于一个集合。我们利用对抗性学习算法从在线两阶段问题中开发在线算法。此外，我们算法的遗憾界可以降至嵌入对抗性学习算法的遗憾界。基于我们的框架，在各种设置下我们都获得了新的结果。当每个时间段的模型参数都是从相同的分布中抽取的时候，我们得到了最先进的$O（\sqrt{T}）$遗憾界，这比之前的特殊情况下的界有所提升。我们的算法还可以抵抗模型的敌对性扰动。

    We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm cam be reduced to the regret bound of embedded adversarial learning algorithms. Based on our framework, we obtain new results under various settings. When the model parameter at each period is drawn from identical distributions, we derive state-of-art $O(\sqrt{T})$ regret that improves previous bounds under special cases. Our algorithm is also robust to adversarial corruptions of model
    
[^156]: QCM-SGM+: 基于得分生成模型的量化压缩感知改进

    QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models. (arXiv:2302.00919v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.00919](http://arxiv.org/abs/2302.00919)

    该论文提出了一种名为QCS-SGM+的算法，利用基于得分的生成模型(SGM)作为隐式先验进行量化压缩感知(QCS)，并可以有效地处理一般矩阵。这个算法解决了在粗糙量化的情况下恢复挑战的问题。

    

    在实际的压缩感知过程中，获得的测量数据通常需要在传输或存储前限制为有限比特的量化。这个非线性量化过程带来了重大的恢复挑战，特别是在极度粗糙的量化如1比特下。最近，提出了一种称为QCS-SGM的有效算法，该算法利用基于得分的生成模型(SGM)作为隐式先验进行量化压缩感知(QCS)。由于SGM在捕捉自然信号的复杂结构方面的熟练程度，QCS-SGM明显优于以前的QCS方法。然而，QCS-SGM局限于(近似)行正交传感矩阵，否则可能会无法计算似然分数。为了解决这个限制，我们引入了QCS-SGM+的高级变体，可以有效地处理一般矩阵。关键思想是似然分数计算的贝叶斯推理观点，其中计算期望得分以解决每个测量的结构非正交情况。

    In practical compressed sensing (CS), the obtained measurements typically necessitate quantization to a limited number of bits prior to transmission or storage. This nonlinear quantization process poses significant recovery challenges, particularly with extreme coarse quantization such as 1-bit. Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS (QCS) which utilizes score-based generative models (SGM) as an implicit prior. Due to the adeptness of SGM in capturing the intricate structures of natural signals, QCS-SGM substantially outperforms previous QCS methods. However, QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices as the computation of the likelihood score becomes intractable otherwise. To address this limitation, we introduce an advanced variant of QCS-SGM, termed QCS-SGM+, capable of handling general matrices effectively. The key idea is a Bayesian inference perspective on the likelihood score computation, wherein an expectatio
    
[^157]: 图对比学习中基于亲和力不确定性的负样本挖掘

    Affinity Uncertainty-based Hard Negative Mining in Graph Contrastive Learning. (arXiv:2301.13340v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13340](http://arxiv.org/abs/2301.13340)

    本文提出了一种新颖的方法，在图对比学习（GCL）中使用亲和力信息来挖掘困难负样本，以解决图数据中困难负样本识别困难的问题。

    

    在各种数据类型中，包括图对比学习（GCL）中，困难负样本挖掘已经显示出有效增强自监督对比学习（CL）的性能。现有的基于困难度的CL方法通常将与锚点实例最相似的负实例视为困难负例，这有助于改善CL性能，特别是在图像数据上。然而，在图数据上，这种方法经常无法识别出困难负例，反而会导致很多错误的负例。这主要是因为学到的图表示在图数据中由于过度平滑表示和/或非独立同分布（non-i.i.d.）问题而不够有区别性。为了解决这个问题，本文提出了一种新颖的方法，它在群体亲和力信息（即负实例与锚点实例之间的两组成对亲和力）上构建了一个有区别性的模型来挖掘GCL中的困难负例。

    Hard negative mining has shown effective in enhancing self-supervised contrastive learning (CL) on diverse data types, including graph CL (GCL). The existing hardness-aware CL methods typically treat negative instances that are most similar to the anchor instance as hard negatives, which helps improve the CL performance, especially on image data. However, this approach often fails to identify the hard negatives but leads to many false negatives on graph data. This is mainly due to that the learned graph representations are not sufficiently discriminative due to oversmooth representations and/or non-independent and identically distributed (non-i.i.d.) issues in graph data. To tackle this problem, this article proposes a novel approach that builds a discriminative model on collective affinity information (i.e., two sets of pairwise affinities between the negative instances and the anchor instance) to mine hard negatives in GCL. In particular, the proposed approach evaluates how confident
    
[^158]: 无监督染色适应的标准化CycleGAN训练方法在乳腺组织病理学侵袭性癌分类中的应用

    Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology. (arXiv:2301.13128v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.13128](http://arxiv.org/abs/2301.13128)

    该论文提出了一种使用CycleGAN实现无监督染色转换的方法，在乳腺组织病理学中应用于侵袭性癌分类。通过比较不同方法的性能，作者证明了将CycleGAN应用于染色无关特征学习的有效性。

    

    泛化性是计算病理学的主要挑战之一。切片制备异质性和扫描仪的多样性导致在未参与训练的医疗中心中使用模型时性能较差。为了实现对乳腺侵袭性癌块的染色无关性，我们使用CycleGAN实现了一种染色转换策略进行无监督的图像转换。我们将三种基于CycleGAN的方法与没有任何染色无关性策略的基准分类模型进行了比较。其中两种方法在推断或训练中使用CycleGAN的转换来构建染色特异性的分类模型。最后一种方法在训练过程中使用它们进行染色数据增强，限制了分类模型学习染色无关特征。通过在参考染色上训练和测试基准分类模型来设置基准度量。我们使用三个医疗中心的H＆

    Generalization is one of the main challenges of computational pathology. Slide preparation heterogeneity and the diversity of scanners lead to poor model performance when used on data from medical centers not seen during training. In order to achieve stain invariance in breast invasive carcinoma patch classification, we implement a stain translation strategy using cycleGANs for unsupervised image-to-image translation. We compare three cycleGAN-based approaches to a baseline classification model obtained without any stain invariance strategy. Two of the proposed approaches use cycleGAN's translations at inference or training in order to build stain-specific classification models. The last method uses them for stain data augmentation during training. This constrains the classification model to learn stain-invariant features. Baseline metrics are set by training and testing the baseline classification model on a reference stain. We assessed performances using three medical centers with H&
    
[^159]: LEXTREME：多语言和多任务的法律领域基准

    LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13126](http://arxiv.org/abs/2301.13126)

    LEXTREME是一个多语言和多任务的法律领域基准，该基准提供了11个数据集涵盖24种语言的测评，最佳模型（XLM-R large）在数据集和语言综合评分上均达到了61.3。这使得LEXTREME仍然具有挑战性并且有改进空间。

    

    最近，在transformer架构的显著进展推动下，法律自然语言处理领域取得了惊人的增长。为了衡量进展，精心策划和具有挑战性的基准是至关重要的。然而，大多数基准只能处理英文，而在法律自然语言处理方面尚未有多语言基准可用。此外，许多基准已经饱和，最佳模型明显优于最佳人类，并达到近乎完美的分数。我们调查了法律自然语言处理文献，并选择了11个涵盖24种语言的数据集，创建了LEXTREME。为了进行公平比较，我们提出了两种综合评分，一种基于数据集，一种基于语言。最佳基线模型（XLM-R large）在数据集综合评分和语言综合评分上均达到了61.3。这表明LEXTREME仍然非常具有挑战性，并且为改进留下了充足空间。为了方便研究人员和实践者使用，我们将LEXTREME与所有数据一起发布在huggingface上。

    Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
    
[^160]: 压缩、泛化和学习

    Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12767](http://arxiv.org/abs/2301.12767)

    本文提出了一种新的理论，允许在压缩的改变概率上保持控制，并获得了紧密的有限样本边界来评估压缩的改变概率。这对学习应用中的错误分类和错误预测具有重要意义。

    

    压缩函数是一种将观测集缩小为尺寸减小的子集的映射，同时保留其信息内容。在多个应用中，新观测使压缩集发生变化的条件被解释为新观测带来了额外的信息，在学习理论中，这对应于错误分类或错误预测。本文建立了一个新理论的基础，允许在压缩的改变概率上保持控制（与学习应用中的统计“风险”相对应）。在适当的条件下，压缩集的基数被证明是压缩的改变概率的一致估计量（不对压缩集的尺寸设置上限）；此外，在普遍适用的偏好条件下获得了前所未有的紧密的有限样本边界来评估压缩的改变概率。所有结果都可以在完全应用中使用。

    A compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. In multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. In this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical "risk" in learning applications). Under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. All results are usable in a fully 
    
[^161]: 通过神经网络的庞特里亚金最优控制

    Pontryagin Optimal Control via Neural Networks. (arXiv:2212.14566v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.14566](http://arxiv.org/abs/2212.14566)

    本文将神经网络与庞特里亚金最大原理相结合，提出了一个样本高效的框架NN-PMP-Gradient，可以应用于具有未知和复杂动力学的系统。通过采用迭代方法，该框架不仅利用准确的神经网络参数化的替代模型，还高效地恢复了最优性条件和最优动作序列。

    

    解决现实世界中的最优控制问题是一项具有挑战性的任务，因为复杂的高维系统动力学通常对决策者来说是未知的。因此，通过数值方法找到最优控制动作是困难的。为了应对这种建模和计算挑战，在本文中，我们将神经网络与庞特里亚金最大原理（PMP）相结合，提出了一种样本高效的框架NN-PMP-Gradient。所得到的控制器可以用于具有未知和复杂动力学的系统。通过采用迭代方法，所提出的框架不仅利用了由神经网络参数化的准确替代模型，还通过PMP条件高效地恢复了最优性条件以及最优动作序列。在线性二次调节器、与网格连接的有损电池的能源套利、单摆的控制和两个MuJoCo运动任务的数值仿真中，证明了我们提出的NN-PMP-Gradient是一个通用的和多功能的框架。

    Solving real-world optimal control problems are challenging tasks, as the complex, high-dimensional system dynamics are usually unrevealed to the decision maker. It is thus hard to find the optimal control actions numerically. To deal with such modeling and computation challenges, in this paper, we integrate Neural Networks with the Pontryagin's Maximum Principle (PMP), and propose a sample efficient framework NN-PMP-Gradient. The resulting controller can be implemented for systems with unknown and complex dynamics. By taking an iterative approach, the proposed framework not only utilizes the accurate surrogate models parameterized by neural networks, it also efficiently recovers the optimality conditions along with the optimal action sequences via PMP conditions. Numerical simulations on Linear Quadratic Regulator, energy arbitrage of grid-connected lossy battery, control of single pendulum, and two MuJoCo locomotion tasks demonstrate our proposed NN-PMP-Gradient is a general and vers
    
[^162]: 特征归因的不可能定理

    Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11870](http://arxiv.org/abs/2212.11870)

    本文阐述了对于中等规模的模型类，任何完整的线性特征归因方法都无法胜任推断模型行为的改进，启示我们在具体定义最终任务的重要性方面要汲取经验，采用重复模型评估的简单直接方法可以胜过许多其他复杂的特征归因方法。

    

    尽管有许多可产生合理解释的可解释性方法，但该领域也经验性地看到了许多失败案例。鉴于这些结果，对于实践者如何以原则性方式使用这些方法并在它们之间进行选择仍不清楚。在本文中，我们展示了对于中等规模的模型类（神经网络容易满足），任何完整的线性特征归因方法（例如Integrated Gradients和SHAP）可以被证明对于推断模型行为的改进都无法胜任。我们的结果适用于常见的最终任务，如描述局部模型行为、识别虚假特征和算法回溯。我们工作的一个重要启示是具体定义最终任务的重要性：一旦这样的最终任务被定义，一个简单和直接的方法——重复模型评估——可以胜过许多其他复杂的特征归因方法。

    Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.
    
[^163]: Thales:为DNN加速器制定和估计架构漏洞因素

    Thales: Formulating and Estimating Architectural Vulnerability Factors for DNN Accelerators. (arXiv:2212.02649v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2212.02649](http://arxiv.org/abs/2212.02649)

    本论文提出了一种新的指标RA，用于量化在发生瞬态错误的情况下DNN的准确性。作者发现现有的RA形式化方法存在不准确性，并提出了一种能够正确估计RA的算法。

    

    随着深度神经网络(DNN)在自动驾驶和生物识别等安全关键和隐私敏感的应用中越来越多地被部署，了解DNN的容错性质非常重要。之前的工作主要关注的是失效时间(FIT)率和静默数据损坏(SDC)率等指标，用来衡量设备故障的频率。相反，本文着重于在发生瞬态错误的情况下量化DNN的准确性，也就是告诉我们网络在发生瞬态错误时的表现如何。我们称这个指标为弹性准确性(RA)。我们展示了现有的RA形式化基本上是不准确的，因为它错误地假设在硬件瞬态故障下，软件变量(模型权重/激活)具有相等的故障概率。我们提出了一种算法，捕捉了在瞬态故障下DNN变量的故障概率，因此提供了由硬件验证的正确的RA估计。

    As Deep Neural Networks (DNNs) are increasingly deployed in safety critical and privacy sensitive applications such as autonomous driving and biometric authentication, it is critical to understand the fault-tolerance nature of DNNs. Prior work primarily focuses on metrics such as Failures In Time (FIT) rate and the Silent Data Corruption (SDC) rate, which quantify how often a device fails. Instead, this paper focuses on quantifying the DNN accuracy given that a transient error has occurred, which tells us how well a network behaves when a transient error occurs. We call this metric Resiliency Accuracy (RA). We show that existing RA formulation is fundamentally inaccurate, because it incorrectly assumes that software variables (model weights/activations) have equal faulty probability under hardware transient faults. We present an algorithm that captures the faulty probabilities of DNN variables under transient faults and, thus, provides correct RA estimations validated by hardware. To a
    
[^164]: 利用区间拟度量嵌入改进非对称距离的表示

    Improved Representation of Asymmetrical Distances with Interval Quasimetric Embeddings. (arXiv:2211.15120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15120](http://arxiv.org/abs/2211.15120)

    本论文介绍了一种称为区间拟度量嵌入（IQE）的方法，用于改进非对称距离的表示。通过满足四个期望属性，IQEs在拟度量学习中表现出强大的近似和泛化能力，带来了更好的性能和更高的效率。

    

    非对称距离结构（拟度量）在我们的生活中无处不在，并在机器学习应用中越来越受到关注。在模型表示中引入这样的拟度量结构已被证明可以改善许多任务，包括强化学习（RL）和因果关系学习等。在本研究中，我们提出了四个这种拟度量模型中的期望属性，并展示了先前的工作在其中的缺失。我们提出了一种称为区间拟度量嵌入（IQE）的方法，它被设计为满足所有四个准则。在三个拟度量学习实验中，IQEs展现出强大的近似和泛化能力，从而提高了先前方法的性能和效率。

    Asymmetrical distance structures (quasimetrics) are ubiquitous in our lives and are gaining more attention in machine learning applications. Imposing such quasimetric structures in model representations has been shown to improve many tasks, including reinforcement learning (RL) and causal relation learning. In this work, we present four desirable properties in such quasimetric models, and show how prior works fail at them. We propose Interval Quasimetric Embedding (IQE), which is designed to satisfy all four criteria. On three quasimetric learning experiments, IQEs show strong approximation and generalization abilities, leading to better performance and improved efficiency over prior methods.  Project Page: https://www.tongzhouwang.info/interval_quasimetric_embedding  Quasimetric Learning Code Package: https://www.github.com/quasimetric-learning/torch-quasimetric
    
[^165]: 磁性材料的时间反演等变神经网络势

    General time-reversal equivariant neural network potential for magnetic materials. (arXiv:2211.11403v3 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2211.11403](http://arxiv.org/abs/2211.11403)

    本研究提出了时间反演等变神经网络势SpinGNN++，用于磁性系统的全面原子间势能建模和大规模并行自旋-格子动力学计算。通过引入复杂磁性模型数据并验证其能力，SpinGNN++展示了精确描述磁性材料自旋-格子耦合的能力，为相关性质的探索提供了重要工具。

    

    本研究引入了时间反演E(3)等变神经网络和SpinGNN++框架，用于构建磁性系统的全面原子间势能，包括自旋轨道耦合和非共线磁矩。SpinGNN++将多任务自旋等变神经网络与显式自旋格子项相结合，包括海森堡、Dzyaloshinskii-Moriya、Kitaev、单离子各向异性和二次项相互作用，并采用时间反演等变神经网络来学习高阶自旋-格子相互作用。为了验证SpinGNN++，引入了一个复杂的磁性模型数据集作为基准，并用它来展示其能力。SpinGNN++能够准确描述单层CrI$_3$和CrTe$_2$中复杂的自旋-格子耦合，达到亚毫电子伏特的误差。重要的是，它能够进行大规模并行自旋-格子动力学计算，从而实现相关性质的探索。

    This study introduces time-reversal E(3)-equivariant neural network and SpinGNN++ framework for constructing a comprehensive interatomic potential for magnetic systems, encompassing spin-orbit coupling and noncollinear magnetic moments. SpinGNN++ integrates multitask spin equivariant neural network with explicit spin-lattice terms, including Heisenberg, Dzyaloshinskii-Moriya, Kitaev, single-ion anisotropy, and biquadratic interactions, and employs time-reversal equivariant neural network to learn high-order spin-lattice interactions using time-reversal E(3)-equivariant convolutions. To validate SpinGNN++, a complex magnetic model dataset is introduced as a benchmark and employed to demonstrate its capabilities. SpinGNN++ provides accurate descriptions of the complex spin-lattice coupling in monolayer CrI$_3$ and CrTe$_2$, achieving sub-meV errors. Importantly, it facilitates large-scale parallel spin-lattice dynamics, thereby enabling the exploration of associated properties, including
    
[^166]: 用电子健康记录评估治疗分配的因果公平性

    Causal Fairness Assessment of Treatment Allocation with Electronic Health Records. (arXiv:2211.11183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11183](http://arxiv.org/abs/2211.11183)

    本研究提出了一种用于评估临床决策中治疗分配公平性的因果公平算法，通过考虑患者群体的异质性并对具有相同可能性的患者进行调整，以确定潜在的不公平性。

    

    医疗领域一直面临着治疗差异的持续问题，引发了关于临床实践中治疗分配公平性的担忧。虽然出现了各种公平度量指标用于评估决策过程中的公平性，但越来越多的关注点集中在基于因果性的公平概念上，因为它们可以减轻混杂效应并推断偏差。然而，在评估基于电子健康记录（EHR）数据的临床决策公平性时，因果公平概念的应用仍然是一个未经研究的领域。本研究旨在填补在使用电子健康记录数据评估治疗分配的因果公平性方面的方法论空白。我们提出了一种因果公平算法来评估临床决策的公平性。我们的算法考虑了患者群体的异质性，并通过对具有相同可能性的患者进行调整，以确定治疗分配中的潜在不公平性。

    Healthcare continues to grapple with the persistent issue of treatment disparities, sparking concerns regarding the equitable allocation of treatments in clinical practice. While various fairness metrics have emerged to assess fairness in decision-making processes, a growing focus has been on causality-based fairness concepts due to their capacity to mitigate confounding effects and reason about bias. However, the application of causal fairness notions in evaluating the fairness of clinical decision-making with electronic health record (EHR) data remains an understudied domain. This study aims to address the methodological gap in assessing causal fairness of treatment allocation with electronic health records data. We propose a causal fairness algorithm to assess fairness in clinical decision-making. Our algorithm accounts for the heterogeneity of patient populations and identifies potential unfairness in treatment allocation by conditioning on patients who have the same likelihood to 
    
[^167]: 使用多标签分类的分布式高斯过程中的基于输入的专家选择

    Entry Dependent Expert Selection in Distributed Gaussian Processes Using Multilabel Classification. (arXiv:2211.09940v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09940](http://arxiv.org/abs/2211.09940)

    本文提出了一种基于输入数据点特征的灵活的专家选择方法，以减少在分布式高斯过程中的专家数目并提高效率。

    

    通过分布式训练过程，局部近似降低了标准高斯过程的成本。集成技术将不同数据分区上训练的高斯专家的局部预测进行组合。集成方法通过假设局部预测器之间的完美差异性来聚合模型的预测结果。虽然它保持了可处理的聚合性质，但这个假设在实践中经常被违反。尽管集成方法通过假设专家之间存在依赖关系提供了一致的结果，但其计算成本很高，与所涉及的专家数量成立方关联。通过实施一种专家选择策略，最终的聚合步骤使用较少的专家并且更高效。然而，将一个固定的专家集分配给每个新数据点的选择方法不能编码每个唯一数据点的特定属性。本文提出了一种基于入口数据点特征的灵活的专家选择方法。为此，我们研究了

    By distributing the training process, local approximation reduces the cost of the standard Gaussian Process. An ensemble technique combines local predictions from Gaussian experts trained on different partitions of the data. Ensemble methods aggregate models' predictions by assuming a perfect diversity of local predictors. Although it keeps the aggregation tractable, this assumption is often violated in practice. Even though ensemble methods provide consistent results by assuming dependencies between experts, they have a high computational cost, which is cubic in the number of experts involved. By implementing an expert selection strategy, the final aggregation step uses fewer experts and is more efficient. However, a selection approach that assigns a fixed set of experts to each new data point cannot encode the specific properties of each unique data point. This paper proposes a flexible expert selection approach based on the characteristics of entry data points. To this end, we inves
    
[^168]: 通过对抗基础调整评估神经网络的鲁棒性

    Assessing Neural Network Robustness via Adversarial Pivotal Tuning. (arXiv:2211.09782v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.09782](http://arxiv.org/abs/2211.09782)

    本研究旨在通过使用预训练图像生成器对图像进行详细、多样和逼真的语义操作，同时保留原始图像的类别，来评估神经网络的鲁棒性。

    

    图像分类器的鲁棒性对于它们在现实世界中的部署至关重要。评估其对于训练数据的操纵或偏离能力因此至关重要。传统上，这些修改通常仅包括最小的变化，仍能欺骗分类器，而现代方法对此越来越具有鲁棒性。因此，对图像进行有意义的语义操作的方式在此目的上得到了广泛应用，但主要局限于样式、颜色或属性的变化。虽然表达力强，但这些操作并没有充分利用预训练生成模型的全部能力。在这项工作中，我们旨在弥合这个差距。我们展示了如何使用预训练图像生成器在详细、多样和逼真的方式上对图像进行语义操作，同时保留原始图像的类别。受最近的基于GAN的图像反演方法的启发，我们提出了一种名为“Adversarial Pivo”的方法

    The robustness of image classifiers is essential to their deployment in the real world. The ability to assess this resilience to manipulations or deviations from the training data is thus crucial. These modifications have traditionally consisted of minimal changes that still manage to fool classifiers, and modern approaches are increasingly robust to them. Semantic manipulations that modify elements of an image in meaningful ways have thus gained traction for this purpose. However, they have primarily been limited to style, color, or attribute changes. While expressive, these manipulations do not make use of the full capabilities of a pretrained generative model. In this work, we aim to bridge this gap. We show how a pretrained image generator can be used to semantically manipulate images in a detailed, diverse, and photorealistic way while still preserving the class of the original image. Inspired by recent GAN-based image inversion methods, we propose a method called Adversarial Pivo
    
[^169]: NESTER：一种自适应的神经符号化方法进行治疗效果评估

    NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.04370](http://arxiv.org/abs/2211.04370)

    NESTER是一种自适应的神经符号化方法进行治疗效果评估，将治疗效果估计的所有要求集成到一个框架中，该方法比现有最先进的方法在多个基准数据集上性能更好。

    

    从观测数据中进行治疗效果评估是因果推断中的一个核心问题。基于潜在结果框架的方法通过利用因果推断中的归纳偏置和启发式方法来解决这个问题。每种现有的技术都通过设计神经网络架构和正则化器来解决治疗效果评估的特定方面，例如控制倾向得分、强制随机化等。在本文中，我们提出了一种自适应方法，称为神经符号治疗效果估计器（NESTER），它是一种治疗效果评估的通用方法。NESTER将治疗效果估计的所有要求集成到一个框架中。为此，我们设计了一个基于文献中使用的归纳偏置的治疗效果估计的领域特定语言（DSL）。我们还在理论上研究了NESTER在治疗效果估计任务中的能力。我们全面的实证结果表明，与现有的最先进方法相比，NESTER在多个基准数据集上的效果更好。

    Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
    
[^170]: 学习SDN软件定义网络测试失败模型

    Learning Failure-Inducing Models for Testing Software-Defined Networks. (arXiv:2210.15469v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.15469](http://arxiv.org/abs/2210.15469)

    本文介绍了一种名为FuzzSDN的机器学习引导模糊测试方法，可生成导致SDN系统失败的有效测试数据，并学习准确的故障诱导模型以表征此类系统失败的条件。

    

    软件定义网络（SDN）能够实现由中央化的软件控制器管理的灵活和高效的通信系统。然而，这样的控制器可能会破坏基于SDN的系统的底层通信网络，因此必须进行仔细测试。当基于SDN的系统失败时，工程师需要精确地了解其发生条件以应对此类故障。本文介绍了一种名为FuzzSDN的机器学习引导模糊测试方法，旨在（1）生成导致SDN系统失败的有效测试数据和（2）学习准确的故障诱导模型，以表征此类系统失败的条件。据我们所知，FuzzSDN是首次尝试同时解决这两个SDN目标的方法。我们通过将其应用于两个开源SDN控制器控制的系统来评估FuzzSDN。此外，我们还将FuzzSDN与两种最先进的SDN模糊测试方法和两个基准进行比较。

    Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating effective test data leading to failures in SDN-based systems and (2) learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, FuzzSDN is the first attempt to simultaneously address these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Further, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines
    
[^171]: 高效的多阶段聚类实时流式嵌入式演讲人分离

    Highly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering. (arXiv:2210.13690v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.13690](http://arxiv.org/abs/2210.13690)

    本文提出了一种多阶段聚类策略，可以解决流式嵌入式演讲者分离系统的多方面挑战，从而提高效率。此策略使用不同的聚类算法处理不同长度的输入，并可适应不同资源约束的设备。

    

    在说话者分离方面，最近的研究进展主要集中于提高分离结果的质量，但同时也越来越关注提高分离系统的效率。本文证明了一种多阶段聚类策略——对于不同长度的输入使用不同的聚类算法，从而解决了嵌入式演讲人分离应用的多方面挑战。具体而言，采用后备聚类器来处理短形式输入；主聚类器用于处理中等长度的输入；在经过主聚类器之前，采用预处理聚类器来压缩长形式输入。主聚类器和预处理聚类器都可以配置为计算复杂度上界，以适应不同资源约束的设备。该多阶段聚类策略对于CPU、内存和电池预算紧张的流式嵌入式演讲者分离系统至关重要。

    While recent research advances in speaker diarization mostly focus on improving the quality of diarization results, there is also an increasing interest in improving the efficiency of diarization systems. In this paper, we demonstrate that a multi-stage clustering strategy that uses different clustering algorithms for input of different lengths can address multi-faceted challenges of on-device speaker diarization applications. Specifically, a fallback clusterer is used to handle short-form inputs; a main clusterer is used to handle medium-length inputs; and a pre-clusterer is used to compress long-form inputs before they are processed by the main clusterer. Both the main clusterer and the pre-clusterer can be configured with an upper bound of the computational complexity to adapt to devices with different resource constraints. This multi-stage clustering strategy is critical for streaming on-device speaker diarization systems, where the budgets of CPU, memory and battery are tight.
    
[^172]: 湍流中的粒子聚集：利用深度学习预测空间和统计特性

    Particle clustering in turbulence: Prediction of spatial and statistical properties with deep learning. (arXiv:2210.02339v2 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2210.02339](http://arxiv.org/abs/2210.02339)

    本研究利用深度学习模型成功预测了湍流中粒子聚集的空间和统计特性，为早期行星形成中尘埃粒子的碰撞生长提供了理论支持。

    

    本文研究了利用深度学习模拟与湍流流体气动耦合的粒子聚集。我们使用Athena++流体动力学代码中的拉格朗日粒子模块，在各向同性强迫型水动力湍流的周期域内模拟了在爱普斯坦阻力区的粒子动力学。这一设置是一个理想化模型，与早期行星形成中微米到毫米尺寸的尘埃粒子的碰撞生长相关。我们使用模拟数据训练了一个U-Net深度学习模型，以预测基于相应流体场输入的粒子密度和速度场的三维网格表示。经过训练的模型在高度非线性区捕捉到了聚集粒子的纤维结构。我们通过计算密度场（径向分布函数）和速度场（相对速度和相对径向速度）的指标来评估模型的准确性。

    We investigate the utility of deep learning for modeling the clustering of particles that are aerodynamically coupled to turbulent fluids. Using a Lagrangian particle module within the Athena++ hydrodynamics code, we simulate the dynamics of particles in the Epstein drag regime within a periodic domain of isotropic forced hydrodynamic turbulence. This setup is an idealized model relevant to the collisional growth of micron to mm-sized dust particles in early stage planet formation. The simulation data are used to train a U-Net deep learning model to predict gridded three-dimensional representations of the particle density and velocity fields, given as input the corresponding fluid fields. The trained model qualitatively captures the filamentary structure of clustered particles in a highly non-linear regime. We assess model fidelity by calculating metrics of the density field (the radial distribution function) and of the velocity field (the relative velocity and the relative radial velo
    
[^173]: 神经网络修剪是否需要复杂性？一个关于全局幅度修剪的案例研究

    Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14624](http://arxiv.org/abs/2209.14624)

    本论文研究了神经网络修剪中是否需要复杂性，并通过与全局幅度修剪进行比较发现，原始的全局幅度修剪方法优于其他最先进的修剪技术。

    

    在过去的十年中，修剪神经网络变得越来越流行，因为人们发现在不降低准确性的情况下可以安全地删除现代神经网络中的大量权重。自那时以来，已经提出了许多修剪方法，每一种都声称比前一种更好。今天，许多最先进的技术都依赖于使用重要性分数、通过反向传播获取反馈或基于启发式修剪规则等复杂的修剪方法。在这项工作中，我们质疑这种引入复杂性是否真的有必要来实现更好的修剪结果。我们将这些最先进的技术与一个简单的基准线进行了比较，即全局幅度修剪(Global MP)。全局幅度修剪根据权重的大小对其进行排序并修剪最小的权重。因此，在其原始形式中，它是最简单的修剪技术之一。令人惊讶的是，我们发现原始的全局幅度修剪优于所有其他最先进的技术并取得了最好的结果。

    Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and ach
    
[^174]: DGPO: 使用多样化策略优化发现多种解决方案

    DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.05631](http://arxiv.org/abs/2207.05631)

    这篇论文提出了一个名为DGPO的算法，可以在解决任务时发现多种策略，从而提高策略鲁棒性和与用户交互的乐趣。

    

    大多数强化学习算法都试图寻找解决给定任务的单个最佳策略。然而，学习多种解决方案通常是有价值的，例如，使智能体与用户的交互更加有趣，或者提高策略对意外干扰的鲁棒性。我们提出了一种名为多样化策略优化（DGPO）的在线算法，用于发现解决给定任务的多种策略。与现有工作不同的是，它通过在单次运行中训练共享策略网络实现此目的。具体而言，我们设计了一种基于信息理论多样性目标的内在奖励。我们的最终目标交替约束策略多样性和外在奖励。我们将约束优化问题转化为概率推断任务，并使用策略迭代来最大化得到的下界。实验结果表明，我们的方法能够在各种环境中有效地发现多样化的策略，包括 Atari 游戏和 Mujoco 模拟器，并且能够提供一系列性能和多样性之间的权衡。

    Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet
    
[^175]: 通过鼓励一致的基于梯度的解释来改进视觉 grounding

    Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.15462](http://arxiv.org/abs/2206.15462)

    该论文提出了一种名为 AMC 的目标函数，鼓励基于梯度的解释覆盖有注释的感兴趣区域，即编码区域。该方法在提高视觉 grounding 性能方面表现卓越，有望成为视觉 grounding 领域的新进展。

    

    我们提出了一种基于边缘的损失，用于预训练视觉语言模型，鼓励基于梯度的解释与区域级注释保持一致。我们将这个目标称为 Attention Mask Consistency（AMC），并证明它产生了比依赖于区域级注释的模型更优越的视觉 grounding 性能。 AMC 通过鼓励基于梯度的解释掩码，在包含此类注释的图像中，把它们的注意力分数主要集中在注释的感兴趣区域内。特别地，一个在标准视觉-语言建模目标之上用 AMC 训练的模型，在 Flickr30k 视觉 grounding 基准测试中获得了86.59%的最新结果，相比最佳结果获得了5.48%的绝对提升。我们的方法在已建立的指代表达理解基准测试中表现优秀，还提供了额外的好处。

    We propose a margin-based loss for vision-language model pretraining that encourages gradient-based explanations that are consistent with region-level annotations. We refer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual grounding performance compared to models that rely instead on region-level annotations for explicitly training an object detector such as Faster R-CNN. AMC works by encouraging gradient-based explanation masks that focus their attention scores mostly within annotated regions of interest for images that contain such annotations. Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.48% when compared to the best previous model. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension and offers the added bene
    
[^176]: 基于半定规划的神经网络验证中的弦状稀疏性

    Chordal Sparsity for SDP-based Neural Network Verification. (arXiv:2206.03482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03482](http://arxiv.org/abs/2206.03482)

    本文提出了一种基于半定规划的神经网络验证方法，通过引入弦状稀疏性，旨在改善现有技术中存在的可扩展性问题。

    

    神经网络在许多新兴技术中起着核心作用，但验证其正确性仍然是一个重要挑战。已知网络输出对于即使是小的输入扰动也非常敏感和脆弱，从而增加了不可预测和不希望的行为的风险。快速而准确地验证神经网络对其广泛采用至关重要，并且近年来已经开发出多种方法来应对这个问题。在本文中，我们关注于改进基于半定规划（SDP）的神经网络验证技术。这些技术在保留凸问题形式的同时，提供了表达复杂几何约束的能力，但在实践中，可扩展性仍然是一个重要问题。我们的起点是Fazlyab等人提出的DeepSDP框架，该框架使用二次约束将验证问题抽象为一个大规模的SDP。然而，当网络规模增长时，解决这个SDP问题变得困难且耗时。

    Neural networks are central to many emerging technologies, but verifying their correctness remains a major challenge. It is known that network outputs can be sensitive and fragile to even small input perturbations, thereby increasing the risk of unpredictable and undesirable behavior. Fast and accurate verification of neural networks is therefore critical to their widespread adoption, and in recent years a variety of methods have been developed as a response to this problem. In this paper, we focus on improving semidefinite programming (SDP) based techniques for neural network verification. Such techniques offer the power of expressing complex geometric constraints while retaining a convex problem formulation, but in practice, scalability remains a major issue. Our starting point is the DeepSDP framework proposed by Fazlyab et al, which uses quadratic constraints to abstract the verification problem into a large-scale SDP. When the network size grows, however, solving this SDP quickly 
    
[^177]: 从归因图到可理解的人类解释：通过概念相关传播

    From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation. (arXiv:2206.03208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03208](http://arxiv.org/abs/2206.03208)

    本论文介绍了一种概念相关传播（CRP）方法，将局部和全局观点结合起来，为个别预测提供了“何地”和“何物”两个问题的解答。该方法提供了更人类可解释的解释，并通过概念图谱深入了解模型的表示和推理能力。

    

    可解释的人工智能（XAI）领域旨在使当今强大但不透明的深度学习模型变得透明。而局部的XAI方法通过归因图解释个别预测，从而确定重要特征出现的位置（但不提供有关它们代表什么的信息），而全局解释技术则可视化模型通常学习编码的概念。因此，这两种方法只提供了部分洞察力，并将解释模型的负担留给用户。在这项工作中，我们介绍了概念相关传播（CRP）方法，它结合了局部和全局的观点，从而能够回答个别预测的“何地”和“何物”的问题。我们展示了我们方法在各种设置中的能力，展示了CRP如何提供更具人类解释性的解释，并通过概念图谱深入了解模型的表示和推理能力。

    The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the "where" and "what" questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model's representation and reasoning through concept atlases, 
    
[^178]: 生存强盗问题

    The Survival Bandit Problem. (arXiv:2206.03019v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03019](http://arxiv.org/abs/2206.03019)

    这个研究介绍了生存强盗问题，这是多臂赌博机问题的一个新变种。该问题的目标是最小化生存遗憾，同时要求算法加速收敛。

    

    我们介绍并研究了多臂赌博机问题(MAB)的一个新变种，称为生存强盗问题(S-MAB)。虽然在这两个问题中，目标都是最大化所谓的累积奖励，但在这个新的变种中，如果累积奖励低于预设的阈值，程序将被中断。这个简单但未被探讨的MAB扩展源自许多实际应用。例如，当对自愿患者进行两种药物的测试时，人们的健康问题至关重要，如果出现严重副作用或者疾病综合症没有得到治疗，有必要能够中断实验。从理论的角度来看，S-MAB是第一种可能中断或不中断的MAB变种。我们首先对S-MAB进行形式化，将其目标定义为所谓的生存遗憾的最小化，自然推广了MAB的遗憾。然后，我们证明了同时最小化生存遗憾和加速收敛的适用于S-MAB的算法存在。

    We introduce and study a new variant of the multi-armed bandit problem (MAB), called the survival bandit problem (S-MAB). While in both problems, the objective is to maximize the so-called cumulative reward, in this new variant, the procedure is interrupted if the cumulative reward falls below a preset threshold. This simple yet unexplored extension of the MAB follows from many practical applications. For example, when testing two medicines against each other on voluntary patients, people's health are at stake, and it is necessary to be able to interrupt experiments if serious side effects occur or if the disease syndromes are not dissipated by the treatment. From a theoretical perspective, the S-MAB is the first variant of the MAB where the procedure may or may not be interrupted. We start by formalizing the S-MAB and we define its objective as the minimization of the so-called survival regret, which naturally generalizes the regret of the MAB. Then, we show that the objective of the 
    
[^179]: 稀疏图的半监督聚类：跨越了信息理论门槛

    Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold. (arXiv:2205.11677v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.11677](http://arxiv.org/abs/2205.11677)

    该论文提出了两种有效的算法来将标签信息与稀疏图结构相结合，解决了基于网络拓扑的聚类在稀疏图上的问题。

    

    随机块模型是一种用于网络结构数据聚类和社区检测的基本随机图模型。数十年来对该问题的广泛研究已经建立了许多深刻的结果，其中Kesten-Stigum门槛处的相变现象特别有趣，从数学和应用角度都具有重要意义。它表明，如果模型参数在某个门槛以下，基于网络拓扑的任何估计器在稀疏图上都不能比随机猜测更好。然而，如果我们稍微扩展视野到普遍存在的半监督设置，这样的基本限制将完全消失。我们证明，通过揭示出任意一部分标记，可以在整个参数域内对检测问题进行处理。此外，我们引入了两种有效的算法，一种是基于组合的，一种是基于优化的，用于将标签信息与图结构相结合。我们的工作为随机块模型和半监督学习带来了全新的视角，标志着稀疏图聚类领域的重大突破。

    The stochastic block model is a canonical random graph model for clustering and community detection on network-structured data. Decades of extensive study on the problem have established many profound results, among which the phase transition at the Kesten-Stigum threshold is particularly interesting both from a mathematical and an applied standpoint. It states that no estimator based on the network topology can perform substantially better than chance on sparse graphs if the model parameter is below certain threshold. Nevertheless, if we slightly extend the horizon to the ubiquitous semi-supervised setting, such a fundamental limitation will disappear completely. We prove that with arbitrary fraction of the labels revealed, the detection problem is feasible throughout the parameter domain. Moreover, we introduce two efficient algorithms, one combinatorial and one based on optimization, to integrate label information with graph structures. Our work brings a new perspective to stochasti
    
[^180]: MGDCF: 通过马尔可夫图扩散进行神经协同过滤的距离学习

    MGDCF: Distance Learning via Markov Graph Diffusion for Neural Collaborative Filtering. (arXiv:2204.02338v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2204.02338](http://arxiv.org/abs/2204.02338)

    本文通过马尔可夫图扩散协同过滤（MGDCF）模型，展示了一些最先进的基于GNN的CF模型与传统的一层NRL模型之间的等价性，为神经协同过滤提供了新的距离学习方法。

    

    最近，图神经网络（GNN）被用于构建协同过滤（CF）模型，根据历史用户-物品交互来预测用户偏好。然而，对于基于GNN的CF模型与传统的网络表示学习（NRL）方法之间的关系，我们的了解还相对较少。本文通过一个交换两种类型的距离的马尔可夫过程，展示了一些最先进的基于GNN的CF模型与传统的一层NRL模型之间的等价性。基于这个马尔可夫图扩散协同过滤（MGDCF）模型，我们将GNN视为一个不可训练的马尔可夫过程，可以为一个基于完全连接层编码上下文特征的传统NRL模型构造顶点的常数上下文特征。这种简化有助于...

    Graph Neural Networks (GNNs) have recently been utilized to build Collaborative Filtering (CF) models to predict user preferences based on historical user-item interactions. However, there is relatively little understanding of how GNN-based CF models relate to some traditional Network Representation Learning (NRL) approaches. In this paper, we show the equivalence between some state-of-the-art GNN-based CF models and a traditional 1-layer NRL model based on context encoding. Based on a Markov process that trades off two types of distances, we present Markov Graph Diffusion Collaborative Filtering (MGDCF) to generalize some state-of-the-art GNN-based CF models. Instead of considering the GNN as a trainable black box that propagates learnable user/item vertex embeddings, we treat GNNs as an untrainable Markov process that can construct constant context features of vertices for a traditional NRL model that encodes context features with a fully-connected layer. Such simplification can help
    
[^181]: 基于和弦稀疏性的深度神经网络Lipschitz常数估计

    Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks. (arXiv:2204.00846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.00846](http://arxiv.org/abs/2204.00846)

    本文提出了一种基于和弦稀疏性的Chordal-LipSDP方法，用于估计深度神经网络的Lipschitz常数。通过将大型半定约束分解为多个较小的约束，该方法在网络深度增加时能够在准确性和可扩展性之间取得更好的权衡。

    

    神经网络的Lipschitz常数可以保证图像分类的鲁棒性、控制器设计的安全性以及训练数据之外的泛化能力。由于计算Lipschitz常数是NP难的，估计Lipschitz常数的技术必须在可扩展性和准确性之间权衡。本文在保持零准确度损失的同时，显著推动了半定规划技术LipSDP的可扩展性。我们首先证明LipSDP具有和弦稀疏性，从而推导出一种称为Chordal-LipSDP的和弦稀疏表述。其主要优势在于将LipSDP的主要计算瓶颈，即一个大型半定约束，分解为等效的多个较小半定约束：使得Chordal-LipSDP在网络深度增加时能够优于LipSDP。此外，我们的表述使用了一个可调的稀疏参数，可以在不增加显著误差的情况下获得更紧的估计。

    Lipschitz constants of neural networks allow for guarantees of robustness in image classification, safety in controller design, and generalizability beyond the training data. As calculating Lipschitz constants is NP-hard, techniques for estimating Lipschitz constants must navigate the trade-off between scalability and accuracy. In this work, we significantly push the scalability frontier of a semidefinite programming technique known as LipSDP while achieving zero accuracy loss. We first show that LipSDP has chordal sparsity, which allows us to derive a chordally sparse formulation that we call Chordal-LipSDP. The key benefit is that the main computational bottleneck of LipSDP, a large semidefinite constraint, is now decomposed into an equivalent collection of smaller ones: allowing Chordal-LipSDP to outperform LipSDP particularly as the network depth grows. Moreover, our formulation uses a tunable sparsity parameter that enables one to gain tighter estimates without incurring a signifi
    
[^182]: 量子支持向量机的复杂性

    The complexity of quantum support vector machines. (arXiv:2203.00031v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.00031](http://arxiv.org/abs/2203.00031)

    量子支持向量机利用量子电路定义核函数，相较已知的经典算法，具有可证明的指数加速优势。由于量子力学的概率性质，训练算法受到统计不确定性的影响，对偶问题可以在$O(M^{4.67}/\varepsilon^2)$个量子电路评估中解决，核化的原始问题可以通过$O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$次评估解决。

    

    量子支持向量机使用量子电路来定义核函数。已经证明，与某些数据集的任何已知经典算法相比，这种方法提供了可证明的指数加速。这些模型的训练对应于通过原始形式或对偶形式求解凸优化问题。由于量子力学的概率性质，训练算法受到统计不确定性的影响，这对其复杂性有重大影响。我们证明了对偶问题可以在$O(M^{4.67}/\varepsilon^2)$个量子电路评估中解决，其中$M$表示数据集的大小，$\varepsilon$表示与精确期望值的理想结果相比的解的精度，该精度理论上只能获得。在经验动机假设下，我们证明了核化的原始问题可以通过$O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$次评估来解决。

    Quantum support vector machines employ quantum circuits to define the kernel function. It has been shown that this approach offers a provable exponential speedup compared to any known classical algorithm for certain data sets. The training of such models corresponds to solving a convex optimization problem either via its primal or dual formulation. Due to the probabilistic nature of quantum mechanics, the training algorithms are affected by statistical uncertainty, which has a major impact on their complexity. We show that the dual problem can be solved in $O(M^{4.67}/\varepsilon^2)$ quantum circuit evaluations, where $M$ denotes the size of the data set and $\varepsilon$ the solution accuracy compared to the ideal result from exact expectation values, which is only obtainable in theory. We prove under an empirically motivated assumption that the kernelized primal problem can alternatively be solved in $O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$ evaluations by employing a 
    
[^183]: 关于不平衡最优输运：梯度方法，稀疏性和逼近误差的研究

    On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error. (arXiv:2202.03618v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.03618](http://arxiv.org/abs/2202.03618)

    本文研究了不平衡最优输运（UOT）问题，提出了一种基于梯度外推方法的新算法（GEM-UOT）来解决该问题。

    

    我们研究了可能具有不同质量的两个测度之间的不平衡最优输运（UOT），其中通过具有正则化因子τ的Kullback-Leibler散度放松标准最优输运（OT）的边际约束。虽然文献中只分析了基于Sinkhorn的UOT求解器，其迭代复杂度为O（τlog（n）/εlog（log（n）/ε）），每次迭代的成本为O（n^2）以达到所需的误差ε，但它们的密集输出输运计划严重阻碍了实际应用。另一方面，虽然在现代深度学习应用中被广泛用作计算UOT的启发式方法，并且在稀疏OT问题中取得了成功，但尚未对应用于UOT的梯度方法进行正式研究。在本文中，我们提出了一种基于梯度外推方法（GEM-UOT）的新算法，用于找到一个ε-近似解。

    We study the Unbalanced Optimal Transport (UOT) between two measures of possibly different masses with at most $n$ components, where the marginal constraints of standard Optimal Transport (OT) are relaxed via Kullback-Leibler divergence with regularization factor $\tau$. Although only Sinkhorn-based UOT solvers have been analyzed in the literature with the iteration complexity of ${O}\big(\tfrac{\tau \log(n)}{\varepsilon} \log\big(\tfrac{\log(n)}{{\varepsilon}}\big)\big)$ and per-iteration cost of $O(n^2)$ for achieving the desired error $\varepsilon$, their positively dense output transportation plans strongly hinder the practicality. On the other hand, while being vastly used as heuristics for computing UOT in modern deep learning applications and having shown success in sparse OT problem, gradient methods applied to UOT have not been formally studied. In this paper, we propose a novel algorithm based on Gradient Extrapolation Method (GEM-UOT) to find an $\varepsilon$-approximate sol
    
[^184]: 气候不变的机器学习

    Climate-Invariant Machine Learning. (arXiv:2112.08440v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08440](http://arxiv.org/abs/2112.08440)

    本研究提出了一种新的框架——"气候不变"的机器学习，通过将气候过程的知识纳入机器学习算法中，可以在广泛的气候和地理条件下保持高准确性。

    

    气候变化预测是一个泛化问题：我们使用物理模型在过去、现在和未来的气候中对最近的过去进行外推。目前的气候模型需要对小于模型网格大小的尺度上发生的过程进行表示，这些过程是模型预测不确定性的主要来源。最近的机器学习（ML）算法有望改善这种过程表示，但往往在未经训练的气候环境中外推效果不佳。为了充分发挥物理和统计方法的优势，我们提出了一个新框架——称为"气候不变"的机器学习——将气候过程的知识纳入ML算法中，并证明它可以在三个不同的大气模型中在广泛的气候和地理条件下保持高准确性。我们的结果表明，将物理知识明确纳入数据驱动的地球系统过程模型中可以提高它们的一致性、数据效率和泛化能力。

    Projecting climate change is a generalization problem: we extrapolate the recent past using physical models across past, present, and future climates. Current climate models require representations of processes that occur at scales smaller than model grid size, which have been the main source of model projection uncertainty. Recent machine learning (ML) algorithms hold promise to improve such process representations, but tend to extrapolate poorly to climate regimes they were not trained on. To get the best of the physical and statistical worlds, we propose a new framework -- termed "climate-invariant" ML -- incorporating knowledge of climate processes into ML algorithms, and show that it can maintain high accuracy across a wide range of climate and geographic conditions in three distinct atmospheric models. Our results suggest that explicitly incorporating physical knowledge into data-driven models of Earth system processes can improve their consistency, data efficiency, and generaliz
    
[^185]: 联邦信号地图中的位置泄露问题

    Location Leakage in Federated Signal Maps. (arXiv:2112.03452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.03452](http://arxiv.org/abs/2112.03452)

    本文研究了在联邦学习框架下，通过梯度泄漏攻击推断用户位置的问题，并提出了一种保护位置隐私的方法。

    This paper studies the problem of inferring user location through gradient leakage attacks in the federated learning framework, and proposes a method to protect location privacy.

    本文考虑了从多个移动设备收集的测量数据中预测蜂窝网络性能（信号地图）的问题。我们在在线联邦学习框架内制定了问题：（i）联邦学习（FL）使用户能够协作训练模型，同时保留其训练数据在其设备上；（ii）测量数据是随着用户随时间移动而收集的，并以在线方式用于本地训练。我们考虑一个诚实但好奇的服务器，观察参与FL的目标用户的更新并使用梯度泄漏（DLG）类型的攻击推断他们的位置，该攻击最初是为重构DNN图像分类器的训练数据而开发的。我们做出了关键观察，即DLG攻击应用于我们的设置，可以推断出本地数据批次的平均位置，并因此可以用于在粗略粒度上重构目标用户的轨迹。我们基于这个观察来保护位置隐私，在我们的s中。

    We consider the problem of predicting cellular network performance (signal maps) from measurements collected by several mobile devices. We formulate the problem within the online federated learning framework: (i) federated learning (FL) enables users to collaboratively train a model, while keeping their training data on their devices; (ii) measurements are collected as users move around over time and are used for local training in an online fashion. We consider an honest-but-curious server, who observes the updates from target users participating in FL and infers their location using a deep leakage from gradients (DLG) type of attack, originally developed to reconstruct training data of DNN image classifiers. We make the key observation that a DLG attack, applied to our setting, infers the average location of a batch of local data, and can thus be used to reconstruct the target users' trajectory at a coarse granularity. We build on this observation to protect location privacy, in our s
    
[^186]: 自然语言处理中的标记修改对抗攻击：一项调研

    Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2103.00676](http://arxiv.org/abs/2103.00676)

    这项调研对现有的自然语言处理中的标记修改对抗攻击进行了分类和比较，并旨在指导新的研究并推动进一步的攻击组件研究。

    

    现在有很多针对自然语言处理系统的对抗攻击。其中，绝大多数攻击通过修改单个文档标记来实现成功，我们将其称为标记修改攻击。每种标记修改攻击都由一组特定的基本组件定义，例如对攻击者的约束或特定的搜索算法。基于这一观察，我们对现有的标记修改攻击进行调查，并提取每种攻击的组件。我们使用一个与攻击无关的框架来组织我们的调研，从而对该领域进行有效的分类，并方便进行组件比较。本调研旨在指导新的研究人员进入这一领域，并推动对于个体攻击组件的进一步研究。

    There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a token-modification attack. Each token-modification attack is defined by a specific combination of fundamental components, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. This survey aims to guide new researchers to this field and spark further research into individual attack components.
    
[^187]: 通过松弛优化理论及其在支持向量机中的应用，对风险进行研究

    A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines. (arXiv:2004.05839v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.05839](http://arxiv.org/abs/2004.05839)

    本文研究了松弛优化理论，探讨了风险与复杂度之间的联系，提出了可以从复杂度估计风险的方法，研究了支持向量方法在机器学习中的应用。

    

    本文考虑了松弛优化，这是一种用于数据驱动设计的广泛范例。我们建立了"风险"（未能满足新的、样本外约束的概率）和"复杂度"（根据Garatti和Campi（2019）中介绍的定义）之间的深层联系，并发现这种联系对应用有深远影响，因为它意味着可以从复杂度估计风险，而复杂度可以通过数据进行测量，不需要了解数据生成机制。在本文中，我们建立了新的结果。首先，我们扩大了Garatti和Campi（2019）的范围，以涵盖机器学习中的各种算法。然后，我们研究了经典的支持向量方法，包括支持向量机（SVM）、支持向量回归（SVR）和支持向量数据描述（SVDD）。

    In this paper we consider optimization with relaxation, an ample paradigm to make data-driven designs. This approach was previously considered by the same authors of this work in Garatti and Campi (2019), a study that revealed a deep-seated connection between two concepts: risk (probability of not satisfying a new, out-of-sample, constraint) and complexity (according to a definition introduced in paper Garatti and Campi (2019)). This connection was shown to have profound implications in applications because it implied that the risk can be estimated from the complexity, a quantity that can be measured from the data without any knowledge of the data-generation mechanism. In the present work we establish new results. First, we expand the scope of Garatti and Campi (2019) so as to embrace a more general setup that covers various algorithms in machine learning. Then, we study classical support vector methods - including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD 
    

