# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks.](http://arxiv.org/abs/2305.09666) | 本文提出了一种高效方法在短时间内标记8000个腹部CT扫描中的8个器官，建立了迄今为止最大的多器官数据集。 |
| [^2] | [Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage.](http://arxiv.org/abs/2305.09659) | 本论文提出了一个名为P2MPO的算法框架，用于解决基于鲁棒离线RL的问题。该框架结合了灵活的模型估计子例程和双重悲观的策略优化步骤，采用双重悲观性原则以克服模型偏移等问题。研究表明，在模型准确性的假设下，该框架在拥有良好的鲁棒部分覆盖数据的情况下是具备高效性的。 |
| [^3] | [Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation.](http://arxiv.org/abs/2305.09651) | 本文提出了一种个性化指导的学习技术，称为LGTM，其利用蒸馏效应选择样本以增强学生的泛化能力，在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。 |
| [^4] | [Prompt-Tuning Decision Transformer with Preference Ranking.](http://arxiv.org/abs/2305.09648) | 本研究提出了Prompt-Tuning DT算法，通过使用轨迹段作为prompt来指导RL agent获取环境信息，从而在具有挑战性的任务中实现了优秀表现。 |
| [^5] | [torchosr -- a PyTorch extension package for Open Set Recognition models evaluation in Python.](http://arxiv.org/abs/2305.09646) | 本文介绍了torchosr--一个专为深度神经网络中Open Set Recognition设计的Python扩展包，提供了两种最新的方法和一系列处理数据集和方法的工具，目的是为了简化和推广正确的实验评估。 |
| [^6] | [FitMe: Deep Photorealistic 3D Morphable Model Avatars.](http://arxiv.org/abs/2305.09641) | FitMe是一个可用于单个或多个图像的深层逼真的三维可塑模型化身系统，通过多模式生成器和PCA形状模型捕捉了人脸外观的漫反射和镜面反射，同时使用了快速可微分渲染过程以实现高保真可渲染的人类化身。 |
| [^7] | [SoundStorm: Efficient Parallel Audio Generation.](http://arxiv.org/abs/2305.09636) | SoundStorm 是一种高效的非自回归音频生成模型，可以在更短的时间内生成音质相同但更加一致的音频，并且能够扩展序列长度进行高质量的自然对话片段合成。 |
| [^8] | [Faster Federated Learning with Decaying Number of Local SGD Steps.](http://arxiv.org/abs/2305.09628) | 本研究提出了一种具有衰减局部SGD步数的方法，可以提高联邦学习模型的最终性能，同时减少训练的时间和计算成本。 |
| [^9] | [Addressing computational challenges in physical system simulations with machine learning.](http://arxiv.org/abs/2305.09627) | 本文提出了一个机器学习数据生成框架，通过有限的模拟数据训练有监督的预测模型和强化学习代理生成准确的、类似于模拟的数据来解决物理系统模拟中的计算挑战。 |
| [^10] | [Balancing Risk and Reward: An Automated Phased Release Strategy.](http://arxiv.org/abs/2305.09626) | 本文提出了一种自动化的分阶段发布策略，能够在控制风险的同时最大化启动速度，通过一个受约束的分批赌博机问题模型来实现。 |
| [^11] | [Conditional variational autoencoder with Gaussian process regression recognition for parametric models.](http://arxiv.org/abs/2305.09625) | 本文提出了一种带高斯过程回归识别的条件变分自编码器框架，可以用于数据驱动的参数模型，通过提取低维特征和学习参数到潜变量的映射来缓解模型复杂度的问题。 |
| [^12] | [AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys.](http://arxiv.org/abs/2305.09620) | 本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。 |
| [^13] | [The Power of Learned Locally Linear Models for Nonlinear Policy Optimization.](http://arxiv.org/abs/2305.09619) | 本文介绍了一种学习非线性系统动态的策略优化算法，该算法通过估计局部线性模型和执行类似于$\mathtt{iLQR}$的策略更新之间的迭代来实现，具有多项式的样本复杂度并克服了指数区间上的依赖性。 |
| [^14] | [Towards Expert-Level Medical Question Answering with Large Language Models.](http://arxiv.org/abs/2305.09617) | 本研究提出了Med-PaLM2，通过结合基础LLM改进、医学领域微调和提示策略，并用新颖的集成精炼方法，实现了在MedQA数据集上达到86.5%的医学问答准确率，迈向医学专家级别的问答能力。 |
| [^15] | [Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow.](http://arxiv.org/abs/2305.09610) | 该论文提出了一种基于能量归一化流框架的生成模型，用于并发的在分布内误分类（IDM）和越界检测，可以扩展先前部署的分割模型而无需重新训练，并在测试中实现很好的结果。 |
| [^16] | [Data Augmentation for Conflict and Duplicate Detection in Software Engineering Sentence Pairs.](http://arxiv.org/abs/2305.09608) | 本文探讨了使用数据增强技术来增强软件工程任务中的冲突和重复检测。该研究提出了新的数据增强技术，通过全面实证分析六个软件文本数据集来识别句对冲突和重复，结果表明数据增强技术对软件句对文本数据集的性能都有显着影响。 |
| [^17] | [Expressiveness Remarks for Denoising Diffusion Models and Samplers.](http://arxiv.org/abs/2305.09605) | 本文在漫扩扩散模型和采样器方面进行了表达能力的研究，通过将已知的神经网络逼近结果扩展到漫扩扩散模型和采样器来实现。 |
| [^18] | [Deep Reinforcement Learning to Maximize Arterial Usage during Extreme Congestion.](http://arxiv.org/abs/2305.09600) | 本文提出了一种基于深度强化学习的方法来减少繁忙多车道高速公路交通拥堵，该方法可以学习适应性绕行策略，在减少拥堵和提高车速的同时，优化高速公路车道和周边局部收发路网的使用，实验证明其可以显著优化道路收发效率并减少总通行时间。 |
| [^19] | [Identification and Classification of Exoplanets Using Machine Learning Techniques.](http://arxiv.org/abs/2305.09596) | 本文提出了利用深度学习算法对Kepler太空望远镜和扩展任务K2的数据进行系外行星分类的方法，尤其是通过使用Siamese架构可有效解决低数据情况下的分类问题，实现了较高的分类准确率。 |
| [^20] | [HiNoVa: A Novel Open-Set Detection Method for Automating RF Device Authentication.](http://arxiv.org/abs/2305.09594) | HiNoVa是一种用于自动化射频设备认证的新型开放集检测方法，利用卷积神经网络长短期记忆模型中隐藏状态值模式，可以成功用于监控和控制无线设备的未经授权的网络访问。 |
| [^21] | [Inductive Graph Neural Networks for Moving Object Segmentation.](http://arxiv.org/abs/2305.09585) | 本文提出了一种基于图神经网络结构的图归纳移动物体分割算法(GraphIMOS)，可以对新增数据帧进行预测，将基于图的MOS模型在实际应用中部署成为可能。 |
| [^22] | [Private Everlasting Prediction.](http://arxiv.org/abs/2305.09579) | 本文提出了私有的预测模型，旨在保护训练集的隐私，探讨了提高隐私保护所需的样本复杂度，成功实现了两种不同的预测模式的私有预测器的样本效率和隐私性。 |
| [^23] | [Toward Falsifying Causal Graphs Using a Permutation-Based Test.](http://arxiv.org/abs/2305.09565) | 本文提出了一种通过构建节点置换基线的新型一致性度量方法，用于验证因果图的正确性并指导其在下游任务中的应用。 |
| [^24] | [Learning from Aggregated Data: Curated Bags versus Random Bags.](http://arxiv.org/abs/2305.09557) | 本文研究了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包，对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。 |
| [^25] | [EEG-based Sleep Staging with Hybrid Attention.](http://arxiv.org/abs/2305.09543) | 本文提出了一种基于混合注意力EEG睡眠分期（HASS）框架，采用时空注意力机制自适应地分配给予通道间和通道内的EEG片段权重，显著提高典型睡眠分期网络的性能。 |
| [^26] | [A Comparative Study of Methods for Estimating Conditional Shapley Values and When to Use Them.](http://arxiv.org/abs/2305.09536) | 本文研究了估计条件Shapley值的方法和应用场景，提出了新方法，扩展了之前的方法，并将这些方法分类，通过模拟研究评估了各类方法的精度和可靠性。 |
| [^27] | [Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction.](http://arxiv.org/abs/2305.09510) | 该论文提出了一种实现实时同时多物体三维形状重建、6DoF姿态估计和密集抓取预测的新方法，无需顺序感知和抓取规划步骤，具有快速推理且具有竞争力的性能。 |
| [^28] | [Content-Adaptive Downsampling in Convolutional Neural Networks.](http://arxiv.org/abs/2305.09504) | 本研究提出了一种内容自适应下采样的方法，通过允许基于感知重要性以不同的分辨率处理不同输入图像的区域，提高了神经网络在密集预测任务中的准确性，且计算成本和网络大小开销最小。 |
| [^29] | [Contrastive Label Enhancement.](http://arxiv.org/abs/2305.09500) | Contrastive Label Enhancement是一种新颖的方法，将特征和逻辑标签集成到统一的投影空间中，以生成高级特征。通过对比学习策略，相同样本的特征和逻辑标签被拉近，不同样本的被投影到更远。 |
| [^30] | [Hardware Realization of Nonlinear Activation Functions for NN-based Optical Equalizers.](http://arxiv.org/abs/2305.09495) | 本论文证明了在基于硬件实现神经网络光学均衡器时，使用近似激活函数的 biLSTM 均衡器能够取得接近于原模型的性能。 |
| [^31] | [Solar Active Region Magnetogram Image Dataset for Studies of Space Weather.](http://arxiv.org/abs/2305.09492) | 本数据集提供了一系列太阳活动区磁图，并提供相应的太阳耀斑标签。它可用于研究磁结构、其演化以及太阳耀斑的关系，并对于自动太阳耀斑预测方法的研究具有重要价值。 |
| [^32] | [Executive Voiced Laughter and Social Approval: An Explorative Machine Learning Study.](http://arxiv.org/abs/2305.09485) | 本文探究了行政沟通中的发声笑声对于社会认可的积极影响，特别是当发生双向笑声时。结果表明，这种影响随着组织业绩的下降而增加。 |
| [^33] | [Your Identity is Your Behavior -- Continuous User Authentication based on Machine Learning and Touch Dynamics.](http://arxiv.org/abs/2305.09482) | 本研究使用移动触摸动态进行连续认证的可行性进行了实验，使用神经网络、极端梯度提升和支持向量机三种不同的算法进行研究，并采集了40个受试者的触摸动态数据集，通过持续验证用户身份提高安全性。 |
| [^34] | [Context-enriched molecule representations improve few-shot drug discovery.](http://arxiv.org/abs/2305.09481) | 上下文丰富的分子表示方法能够以更少的标记数据提高药物发现效率 |
| [^35] | [Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer.](http://arxiv.org/abs/2305.09480) | 本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。 |
| [^36] | [Time delay multi-feature correlation analysis to extract subtle dependencies from EEG signals.](http://arxiv.org/abs/2305.09478) | 本文针对脑电信号的复杂性，提出使用多特征相关分析方法来自动分解和提取多种类型的统计依赖关系。其中，PCA降维技术用于找到占主导的依赖关系方向，从而提取脑电信号中微小依赖性。 |
| [^37] | [ANALYSE -- Learning to Attack Cyber-Physical Energy Systems With Intelligent Agents.](http://arxiv.org/abs/2305.09476) | 提出了基于机器学习的软件套件ANALYSE，让学习代理能够自主地在物理网络能源系统中发现攻击。这是一个旨在找到未知攻击类型的自我记录框架。 |
| [^38] | [Reconstruction-based LSTM-Autoencoder for Anomaly-based DDoS Attack Detection over Multivariate Time-Series Data.](http://arxiv.org/abs/2305.09475) | 该论文提出基于重构的LSTM-Autoencoder（LSTM-AE）模型结合了深度学习模型来检测未见过的DDoS攻击异常，该模型通过自编码器来识别最佳阈值。 |
| [^39] | [Graph-Based Deep Learning for Sea Surface Temperature Forecasts.](http://arxiv.org/abs/2305.09468) | 该论文探索了使用图形重新采样和图形神经网络进行全球海表温度预测的方法，相比持久性模型，GNN 在大多数海洋中显示出更好的提前一个月的 SST 预测结果。 |
| [^40] | [An Empirical Study on Google Research Football Multi-agent Scenarios.](http://arxiv.org/abs/2305.09458) | 本研究提供了一种基于人口的MARL训练管线以及超参数，用于训练多智能体足球场景，从零开始在200万步内打败了难度为1.0的机器人，并提供了开源训练框架Light-MALib。 |
| [^41] | [Rethinking the editing of generative adversarial networks: a method to estimate editing vectors based on dimension reduction.](http://arxiv.org/abs/2305.09454) | 该论文提出了一种基于降维的编辑向量估计方法，用于高质量、高精度的语义图像编辑，不需要语义分割或可微分的特征估计网络。 |
| [^42] | [Probabilistic Distance-Based Outlier Detection.](http://arxiv.org/abs/2305.09446) | 本文提出了一种将距离法异常检测分数转化为可解释的概率估计的通用方法，该方法使用与其他数据点的距离建模距离概率分布，将距离法异常检测分数转换为异常概率，提高了正常点和异常点之间的对比度，而不会影响检测性能。 |
| [^43] | [MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers.](http://arxiv.org/abs/2305.09438) | 本文提出了一种基于Transformer模型的新方法MPI-rical，通过对大量代码片段进行训练实现自动化MPI代码生成，使并行化成为可能。 |
| [^44] | [When is an SHM problem a Multi-Task-Learning problem?.](http://arxiv.org/abs/2305.09425) | 本文探讨了在结构健康监测领域中的多任务学习机制，包括自然多任务、输出作为输入和使用额外损失函数三种方法，并给出了相应例子。 |
| [^45] | [Unwrapping All ReLU Networks.](http://arxiv.org/abs/2305.09424) | 本文提出了一种解开ReLU网络的秘密的方法，通过将网络分解成线性模型，实现了对图神经网络和张量卷积网络等结构的扩展，并证明了神经网络可以理解为可解释模型。此外，还提供了计算SHAP值的方法。 |
| [^46] | [Measuring Implicit Bias Using SHAP Feature Importance and Fuzzy Cognitive Maps.](http://arxiv.org/abs/2305.09399) | 本文使用SHAP特征重要性和模糊认知地图模型，对隐性偏见进行测量，结果表明特征重要性作为绝对工具不适应于测量隐性偏见，受保护特征的偏见数量可能因特征是数值编码还是分类编码而有所不同。 |
| [^47] | [Lp- and Risk Consistency of Localized SVMs.](http://arxiv.org/abs/2305.09385) | 本文分析了局部支持向量机的一致性，证明了在非常弱的条件下，它们从全局SVM继承了$L_p$和风险一致性，即使底层区域随数据集大小的增加而变化。 |
| [^48] | [Multi-task convolutional neural network for image aesthetic assessment.](http://arxiv.org/abs/2305.09373) | 本文提出了一种多任务卷积神经网络，可以同时预测图像的总体美学评分和美学属性，并在实验中表现优异，达到接近人类表现的整体美学评分。 |
| [^49] | [Evaluation of self-supervised pre-training for automatic infant movement classification using wearable movement sensors.](http://arxiv.org/abs/2305.09366) | 本研究评估了自监督预训练对婴儿运动分类分类器的性能提高作用，有助于提高自动分析的准确性和稳健性。 |
| [^50] | [Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation.](http://arxiv.org/abs/2305.09330) | 推荐系统的公平性问题越来越引起人们的关注，尤其是用户公平性，已经提出了很多解决方案来减轻用户在使用推荐系统过程中体验到的歧视问题。 |
| [^51] | [OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research.](http://arxiv.org/abs/2305.09304) | OmniSafe是一种基础设施，用于加速安全强化学习研究，帮助解决当代SafeRL研究环境中缺乏协调和有效的学习框架的问题。 |
| [^52] | [Out-of-Distribution Detection for Adaptive Computer Vision.](http://arxiv.org/abs/2305.09293) | 该论文提出了一种自适应计算机视觉场景的越界检测方法，通过根据基于正则化流的越界检测器自适应相机参数，能使目标检测性能指标 mAP、mAR 和 F1 平均提高 3-4 个百分点。 |
| [^53] | [A Dictionary-based approach to Time Series Ordinal Classification.](http://arxiv.org/abs/2305.09288) | 本文提出了一种基于词典的时间序数分类方法(O-TDE)，并证明其在18个TSOC问题上表现显著优于其他四种现有的名义方法。 |
| [^54] | [Noise robust neural network architecture.](http://arxiv.org/abs/2305.09276) | 本文提出了一种名为“Dune Neural Network”的神经网络架构，通过将网络的每个自由参数表示为不确定性区间，并对每个输入元素应用线性变换，实现了对于白噪声数据的噪声鲁棒性能，即使为非常嘈杂的输入图像，此方法也比使用数据增强的人类在测试集上实现了更好的准确度。 |
| [^55] | [Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?.](http://arxiv.org/abs/2305.09275) | 本文重新审视了在线持续学习算法适应性的常见评估方法——在线准确率度量。该度量是不可靠的，现有算法会学习表面的标签相关性，我们提出了一种新的度量来衡量适应性并进行了基准测试。 |
| [^56] | [Online Continual Learning Without the Storage Constraint.](http://arxiv.org/abs/2305.09253) | 本文提出了一种无存储限制的在线持续学习算法，使用kNN分类器和通用预训练特征提取器，在小算力的情况下紧凑地存储和利用整个输入数据流，并实现了更好的性能。 |
| [^57] | [Sorting and Hypergraph Orientation under Uncertainty with Predictions.](http://arxiv.org/abs/2305.09245) | 本文研究了在利用不可信预测的情况下排序和超图定向的学习增强算法，提供了对于正确预测时$1+1/\gamma$的竞争比和对于任意错误预测时$\gamma$的竞争比的算法。 |
| [^58] | [Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples.](http://arxiv.org/abs/2305.09241) | “无法学习的样本”提出一种对数据进行保护的方法，但它无法阻止未经授权的用户对保护后的数据进行利用。通过提出“可学习的未经授权示例”和一种新的纯化过程，我们可以实现对数据的更好保护。 |
| [^59] | [Synthetic data, real errors: how (not) to publish and use synthetic data.](http://arxiv.org/abs/2305.09235) | 合成数据在机器学习领域受到了越来越多的关注，但是不完美的合成数据可能会导致下游机器学习任务中的潜在错误。为了改进这种情况，研究人员引入了深度生成集成（DGE）框架来近似生成过程模型参数的后验分布，以提高下游模型的训练和评估效果。 |
| [^60] | [Touch Sensing on Semi-Elastic Textiles with Border-Based Sensors.](http://arxiv.org/abs/2305.09222) | 本研究提出一种基于半弹性纺织品表面上边缘的传感器进行接触感测的方法，无需在感测区域放置额外传感器。该方法可在可穿戴技术和智能纺织品等领域中应用，能够以82.85%的准确度分类识别三个压力水平，具有潜在的应用价值。 |
| [^61] | [Component Training of Turbo Autoencoders.](http://arxiv.org/abs/2305.09216) | 本文提出采用孤立训练的高斯先验Turbo自编码器的组件方法能够实现快速、一致、泛化性好的训练，并且具有竞争性能，其中借助EXIT图的设计可专注于块误码率而达到期望行为。 |
| [^62] | [CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images.](http://arxiv.org/abs/2305.09211) | CB-HVTNet 提出了一种 Channel Boosted Hybrid Vision Transformer 网络，利用迁移学习生成增强通道，并结合使用 Transformers 和 CNN，在组织病理学图像中高效准确地评估淋巴细胞。 |
| [^63] | [Counterfactual Outcome Prediction using Structured State Space Model.](http://arxiv.org/abs/2305.09207) | 本文探讨了使用结构化状态空间模型对纵向数据中的反事实结果进行预测的方法，相比Treatment Effect Neural Controlled Differential Equation，S4Model更有效建模长期依赖关系、更易于训练、训练时间更短，且在归一化均方误差上提高了10倍。 |
| [^64] | [The Weighted M\"obius Score: A Unified Framework for Feature Attribution.](http://arxiv.org/abs/2305.09204) | 本文提出了权重莫比乌斯分数作为一个参数化的归因框架，可以涵盖很多不同的特征归因方法，包括特征交互，解决了方法繁衍和不可比的问题。通过研究方法的向量空间，提供了一些新方法和解释，实证结果表明其多功能性和有效性。 |
| [^65] | [Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial Attacks.](http://arxiv.org/abs/2305.09179) | 本文研究了神经常微分方程（NODE）在面对噪声和对抗性攻击时所表现出的自然鲁棒性，并通过控制ODE动力学的Lipschitz常数来显著提高其鲁棒性。实验结果在多个数据集上得到了验证。 |
| [^66] | [Empirical Analysis of the Inductive Bias of Recurrent Neural Networks by Discrete Fourier Transform of Output Sequences.](http://arxiv.org/abs/2305.09178) | 通过离散傅里叶变换，直接计算每个模型的输出序列频率，研究发现长短时记忆（LSTM）和门控循环单元（GRU）在输出序列频率方面具有不同的归纳偏置，LSTM更适合需要长期记忆任务。 |
| [^67] | [Deep ReLU Networks Have Surprisingly Simple Polytopes.](http://arxiv.org/abs/2305.09145) | 本文通过计算和分析ReLU网络多面体的单纯形直方图，发现在初始化和梯度下降时它们结构相对简单，这说明了一种新的隐式偏见。 |
| [^68] | [Smart Policy Control for Securing Federated Learning Management System.](http://arxiv.org/abs/2305.09134) | 本文提出了一种基于智能合约的策略控制，可以保障联合学习管理系统的安全性，确保每个参与者都遵守数据保护政策，同时可以审计训练过程。 |
| [^69] | [Graph Reinforcement Learning for Network Control via Bi-Level Optimization.](http://arxiv.org/abs/2305.09129) | 本文提出了一种基于图形强化学习的网络控制方法，通过双层优化实现较好的扩展性和性能，并具备鲁棒性、灵活性和可解释性。 |
| [^70] | [Transfer Causal Learning: Causal Effect Estimation with Knowledge Transfer.](http://arxiv.org/abs/2305.09126) | 本文提出了一个名为$\ell_1$-TCL的通用框架，它使用知识迁移和Lasso回归来提高因果效应估计精度。 |
| [^71] | [A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction.](http://arxiv.org/abs/2305.09121) | 本文提出了一种名为VIC-DDPM的条件去噪扩散概率模型，在可见度数据和脏图像的帮助下，能够生成更细节的图像，同时消除噪声和伪影。相关实验证实，该算法在恢复微弱信号、保留细节结构和消除伪影等方面有很好的性能。 |
| [^72] | [Automatic learning algorithm selection for classification via convolutional neural networks.](http://arxiv.org/abs/2305.09101) | 本文提出了一种直接使用表格数据自动训练卷积神经网络的方法，以学习数据固有的结构，无需识别元特征。在模拟和真实数据集上，该方法均取得了竞争性能。 |
| [^73] | [Weight-Inherited Distillation for Task-Agnostic BERT Compression.](http://arxiv.org/abs/2305.09098) | 本文提出了一种直接从教师模型传递知识的权重继承蒸馏方法，不需要额外的对齐损失就可以训练出一个紧凑的学生模型，并且在GLUE和SQuAD基准测试上优于之前最先进的基于KD的基线。 |
| [^74] | [ProtoVAE: Prototypical Networks for Unsupervised Disentanglement.](http://arxiv.org/abs/2305.09092) | ProtoVAE是一种基于原型网络的深层生成模型，可以无监督解缠多个基准测试，并将数据编码为可解释的表示形式，可用于下游任务。 |
| [^75] | [The Hessian perspective into the Nature of Convolutional Neural Networks.](http://arxiv.org/abs/2305.09088) | 本文基于Hessian映射，揭示了CNN结构和性质的本质，并证明了Hessian秩随着参数数量的增长而呈现出平方根增长。 |
| [^76] | [A Review of Data-driven Approaches for Malicious Website Detection.](http://arxiv.org/abs/2305.09084) | 本文综述了恶意网站检测中基于数据驱动的方法，包括数据预处理、特征提取、模型构建和技术扩展等。最近，深度学习模型被广泛应用于此领域。文中还讨论了该领域面临的挑战及未来方向。 |
| [^77] | [FiMReSt: Finite Mixture of Multivariate Regulated Skew-t Kernels -- A Flexible Probabilistic Model for Multi-Clustered Data with Asymmetrically-Scattered Non-Gaussian Kernels.](http://arxiv.org/abs/2305.09071) | 本研究提出了一种规则化的迭代优化过程，名为Fini，用于解决现有Skew-t混合模型中的“S-DoF爆炸”问题，提高混合模型的建模泛化性和弹性。 |
| [^78] | [An Offline Time-aware Apprenticeship Learning Framework for Evolving Reward Functions.](http://arxiv.org/abs/2305.09070) | 本文提出了一种THEMES学徒式学习框架来解决医疗保健等人类中心任务中的进化奖励函数问题，实验证明其显著优于其他最新基线模型。 |
| [^79] | [Capturing Humans' Mental Models of AI: An Item Response Theory Approach.](http://arxiv.org/abs/2305.09064) | 本研究提出基于IRT的模型来分析人类对于AI的感知，实验结果表明人们普遍期望AI团队伙伴的表现优于人类，并且人们对AI与人类成员的心理模型不同。 |
| [^80] | [Bounded KRnet and its applications to density estimation and approximation.](http://arxiv.org/abs/2305.09063) | 本文介绍了一种新的可逆映射B-KRnet，并将其应用于数据或PDE的密度估计/近似，由于其定义在有界域上，因此比KRnet更有效。 |
| [^81] | [Self-Supervised Pretraining on Paired Sequences of fMRI Data for Transfer Learning to Brain Decoding Tasks.](http://arxiv.org/abs/2305.09057) | 本文介绍一种基于自监督预训练框架的transformer方法并应用于fMRI数据，首次表明多任务训练对fMRI数据具有协同效应。预训练任务促进迁移学习的能力得到了证明。 |
| [^82] | [Physics-informed Convolutional Recurrent Surrogate Model for Reservoir Simulation with Well Controls.](http://arxiv.org/abs/2305.09056) | 本文提出了一种利用物理信息卷积循环神经网络来模拟井控制下地下多孔介质流体流动的替代模型。该模型不需要标记数据进行训练，并且能够预测出未来的油藏状态，具有广泛的应用前景。 |
| [^83] | [Convex optimization over a probability simplex.](http://arxiv.org/abs/2305.09046) | 这篇论文提出了一种新的迭代方案，用于求解概率单纯形上的凸优化问题。该方法具有收敛速度快且简单易行的特点。 |
| [^84] | [Scalable and Robust Tensor Ring Decomposition for Large-scale Data.](http://arxiv.org/abs/2305.09044) | 本文提出了一种可伸缩和健壮的张量环分解算法，能够自适应地填充缺失条目并识别异常值，在存储和计算复杂度上有显著降低，适用于处理大规模张量数据。 |
| [^85] | [Adaptive Federated Pruning in Hierarchical Wireless Networks.](http://arxiv.org/abs/2305.09042) | 本文提出了一种针对分层联邦学习的自适应模型剪枝方法，来降低神经网络规模并解决由于设备增多导致的学习延迟问题。 |
| [^86] | [What Matters in Reinforcement Learning for Tractography.](http://arxiv.org/abs/2305.09041) | 本论文深入探讨了强化学习在Tractography中不同的组成部分，提出了关于RL算法选择、代理人输入、奖励函数和播种策略的一系列建议。 |
| [^87] | [Algorithmic Censoring in Dynamic Learning Systems.](http://arxiv.org/abs/2305.09035) | 本文介绍了动态学习系统中可能出现的审查现象，并且提出了防范审查的措施以及随机探索，从而确保来自被审查组的样本进入训练数据，并纠正模型。 |
| [^88] | [AI in the Loop -- Functionalizing Fold Performance Disagreement to Monitor Automated Medical Image Segmentation Pipelines.](http://arxiv.org/abs/2305.09031) | 提出了一种使用在不同数据集折叠上训练的子模型的方法来确定模型的置信度，有效地标记低性能的自动分割，并在医学图像分割中实现了对机器学习系统的改进。 |
| [^89] | [SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels.](http://arxiv.org/abs/2305.09028) | 本论文提出使用非对称核（asymmetric kernels）实现Toeplitz神经网络（TNNs）的加速，通过稀疏加低秩Toeplitz矩阵分解、小型1D卷积和替换相对位置编码器（RPE）多层感知器（MLP）实现O（n）复杂度，针对因果模型，提出了“快速”因果屏蔽来抵消这种方法的限制。 |
| [^90] | [DATED: Guidelines for Creating Synthetic Datasets for Engineering Design Applications.](http://arxiv.org/abs/2305.09018) | 本研究提出了一套关于生成、注释和验证合成数据集的全面指南，阐述了权衡和方法，强调了周到的抽样方法的重要性。 |
| [^91] | [Gaussian Process Port-Hamiltonian Systems: Bayesian Learning with Physics Prior.](http://arxiv.org/abs/2305.09017) | 高斯过程Port-Hamiltonian系统是一种基于物理先验的贝叶斯学习方法，能够通过对数据建模生成关于指定输入和输出的被动系统，同时保留了Port-Hamiltonian系统的组合性质。 |
| [^92] | [Physics-enhanced Gaussian Process Variational Autoencoder.](http://arxiv.org/abs/2305.09006) | 本文提出一种物理增强的变分自编码器，将物理先验知识作为高斯过程先验，并将其包含在内核函数中，以实现物理正确的预测。 |
| [^93] | [Survey of Malware Analysis through Control Flow Graph using Machine Learning.](http://arxiv.org/abs/2305.08993) | 本文介绍了最新基于控制流图和机器学习的恶意软件检测方法，重点关注了从CFG中提取、表示、分类的不同方法。 |
| [^94] | [The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting.](http://arxiv.org/abs/2305.08992) | 该论文介绍了BraTS 2023修复挑战，要求参与者使用修补技术从有病变的脑部扫描中合成健康脑扫描，以解决许多算法无法分析病变图像的问题。 |
| [^95] | [Federated Learning over Harmonized Data Silos.](http://arxiv.org/abs/2305.08985) | 该论文提出了一种解决异构数据集的联邦学习方法，通过将数据协调和数据填补等关键步骤纳入架构愿景，来促进数据管理信息系统和机器学习的交叉研究。 |
| [^96] | [Autoencoder-based Anomaly Detection in Streaming Data with Incremental Learning and Concept Drift Adaptation.](http://arxiv.org/abs/2305.08977) | 本文提出了一种基于自编码器的增量学习方法与漂移检测的异常检测方法（strAEm++DD），用于解决无标签的流数据中的异常检测问题，该方法在实验研究中表现出不俗的性能表现并优于现有的基线和先进方法。 |
| [^97] | [Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method.](http://arxiv.org/abs/2305.08960) | 提出一种新的似然比方法来训练神经网络，无需使用递归梯度计算，并在多种神经网络架构上有效地减少了对抗性攻击对模型造成的影响。 |
| [^98] | [Motion Question Answering via Modular Motion Programs.](http://arxiv.org/abs/2305.08953) | 提出了一种新的HumanMotionQA任务，用于评估模型在复杂的人体运动序列上进行复杂、多步推理的能力。同时，提出了一种特殊的NSPose方法，利用符号化推理和模块化设计，成功地应用于该任务中，并超过所有基线方法。 |
| [^99] | [Causal Analysis for Robust Interpretability of Neural Networks.](http://arxiv.org/abs/2305.08950) | 本文提出了一种基于因果分析的鲁棒干预方法，用于解释神经网络的决策，避免噪音的干扰。 |
| [^100] | [MIMEx: Intrinsic Rewards from Masked Input Modeling.](http://arxiv.org/abs/2305.08932) | MIMEx是一个通用的框架，它使用遮盖输入建模来提取内在奖励，通过控制遮盖分布来控制难度，可以在高维环境中取得优越的探索结果。 |
| [^101] | [AF2-Mutation: Adversarial Sequence Mutations against AlphaFold2 on Protein Tertiary Structure Prediction.](http://arxiv.org/abs/2305.08929) | 本文研究了针对AlphaFold2的对抗序列突变，实测仅修改三个氨基酸残基，就使其预测的蛋白质三级结构的改变达到了46.61，并能成功发现在特定蛋白质中对结构至关重要且有生物学意义的氨基酸残基，为蛋白质的修饰提供了新思路。 |
| [^102] | [Differential Convolutional Fuzzy Time Series Forecasting.](http://arxiv.org/abs/2305.08890) | 本文提出了一种新的预测模型DFCNN，利用卷积神经网络实现具有可学习能力的FTSF，并能够处理非平稳时间序列。 |
| [^103] | [New methods for new data? An overview and illustration of quantitative inductive methods for HRM research.](http://arxiv.org/abs/2305.08889) | 本文提供了一种在“大数据”时代的人力资源管理研究方法论挑战的潜在解决方案：定量归纳方法，例如因子分析、聚类分析和潜在类别分析。 |
| [^104] | [Covariate-distance Weighted Regression (CWR): A Case Study for Estimation of House Prices.](http://arxiv.org/abs/2305.08887) | 本文提出了一种变量距离加权回归（CWR）方法，将地理距离和属性距离相结合，可以用来预测房价。CWR方法考虑到地质和属性距离，可以产生准确的房价估计。 |
| [^105] | [Identification of the Factors Affecting the Reduction of Energy Consumption and Cost in Buildings Using Data Mining Techniques.](http://arxiv.org/abs/2305.08886) | 本研究利用数据挖掘技术实现了对建筑物成本降低和能耗影响因素的识别，结果表明隔热、建筑物尺寸和制冷系统类型是影响能源消耗和成本降低的关键因素。 |
| [^106] | [Smart Home Energy Management: VAE-GAN synthetic dataset generator and Q-learning.](http://arxiv.org/abs/2305.08885) | 该论文介绍了一种能够生成智能家居能耗数据的VAE-GAN技术，同时探索将生成模型与Q学习结合应用于基于Q学习的HEMS中的表现，并且证明了VAE-GAN技术能够有效地训练HEMS，为智能家居节能带来了新的应用前景。 |
| [^107] | [Learning to Learn Unlearned Feature for Brain Tumor Segmentation.](http://arxiv.org/abs/2305.08878) | 提出一种针对脑肿瘤分割的微调算法，基于主动学习和元学习。通过迁移学习方法将高级别胶质瘤应用于脑转移瘤，几步之内实现了胶质瘤和脑转移瘤领域的平衡参数。 |
| [^108] | [Neurosymbolic AI and its Taxonomy: a survey.](http://arxiv.org/abs/2305.08876) | 本文调查研究了神经符号人工智能的研究，探索了学习数据分布和推理先前和学习知识相结合的方法，以探索实现人工智能通用性的替代方案。 |
| [^109] | [Online machine-learning forecast uncertainty estimation for sequential data assimilation.](http://arxiv.org/abs/2305.08874) | 该论文提出了一种使用卷积神经网络进行在线机器学习的方法，在单个动力学模型集成中估计表示预测误差协方差矩阵的状态依赖预测不确定性，并在混合数据同化方法中应用。通过模拟实验证明，该方法在准确性和计算成本方面优于传统的基于集合的方法。 |
| [^110] | [AMULET: Adaptive Matrix-Multiplication-Like Tasks.](http://arxiv.org/abs/2305.08872) | AMULET是一个开源编译器，通过利用数据库式和编译器优化技术来优化矩阵乘法类任务，从而在执行环境下生成快速代码，相比现有编译器， AMULTET能够在各种矩阵乘法类任务上实现加速，并可以和手动调优的矩阵乘法库相媲美。 |
| [^111] | [Fast Traversability Estimation for Wild Visual Navigation.](http://arxiv.org/abs/2305.08510) | 本文提出了一种基于自监督学习的在线视觉导航系统 WVN，能够在不到 5 分钟的实地培训时间内启动可通过地形分割，并使机器人能够高效地在挑战性环境下完成导航。 |
| [^112] | [Introduction to dynamical mean-field theory of generic random neural networks.](http://arxiv.org/abs/2305.08459) | 本文针对通用随机神经网络，介绍了动力学平均场理论及其数值实现方法，并引入了一种物理上透明的替代方法，以探索网络的集体动态。 |
| [^113] | [A machine learning-based viscoelastic-viscoplastic model for epoxy nanocomposites with moisture content.](http://arxiv.org/abs/2305.08102) | 本文提出了一种运用深度学习的本构模型，能够准确的捕捉与速率相关的应力-应变关系和一致的切线模量，以研究含湿度纳米颗粒/环氧纳米复合材料的循环粘弹-粘塑性-破坏行为。 |
| [^114] | [Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning.](http://arxiv.org/abs/2305.08014) | 本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。 |
| [^115] | [Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery.](http://arxiv.org/abs/2305.07637) | Text2Cohort是一个基于大语言模型的工具箱，可以将用户输入转化为IDC数据库查询，促进自然语言队列发现，减少研究人员查询IDC数据库的学习曲线，实现了癌症成像数据的民主化。 |
| [^116] | [Continual Vision-Language Representaion Learning with Off-Diagonal Information.](http://arxiv.org/abs/2305.07437) | 本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。 |
| [^117] | [Value Iteration Networks with Gated Summarization Module.](http://arxiv.org/abs/2305.07039) | 本文提出了一种具有门控汇总模块的值迭代网络（GS-VIN）来解决值迭代网络在处理更大的输入地图和减轻累积误差方面的挑战。通过自适应迭代策略和门控汇总模块，这种模型可以在保持准确性的同时，减少网络深度并提高训练稳定性。 |
| [^118] | [Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation.](http://arxiv.org/abs/2305.06563) | 本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。 |
| [^119] | [FedDWA: Personalized Federated Learning with Online Weight Adjustment.](http://arxiv.org/abs/2305.06124) | 本文提出了一种个性化联邦学习算法，名为FedDWA，采用动态权重调整来保护数据隐私并以更少的通信开销捕捉客户之间的相似性，能够训练高精度和高效的个性化模型。 |
| [^120] | [BARA: Efficient Incentive Mechanism with Online Reward Budget Allocation in Cross-Silo Federated Learning.](http://arxiv.org/abs/2305.05221) | BARA是一种在线奖励预算分配算法，用于激励跨边缘联邦学习中的数据所有者为模型训练做出贡献，并解决了现有研究中被忽略的奖励预算分配问题。 |
| [^121] | [Leveraging Deep Learning and Digital Twins to Improve Energy Performance of Buildings.](http://arxiv.org/abs/2305.04498) | 本研究提出了Deep Energy Twin的解决方案，将深度学习和数字孪生相结合，以识别建筑物能源使用模式并提供优化能源效率的洞见。 |
| [^122] | [PiML Toolbox for Interpretable Machine Learning Model Development and Validation.](http://arxiv.org/abs/2305.04214) | PiML工具箱是一个综合的Python工具箱，可用于开发和诊断可解释机器学习模型，包括日益增长的可解释模型、模型无关的可解释性工具和模型无关的诊断工具，还支持与MLOps平台的集成和质量保证。 |
| [^123] | [FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding.](http://arxiv.org/abs/2305.03292) | 本文提出FedNC，一个联合学习的通信框架，结合了网络编码技术，能够提高系统的隐私、吞吐量和鲁棒性。 |
| [^124] | [Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability.](http://arxiv.org/abs/2305.03100) | 本文提出了一种统一的框架，用于游戏理论驱动的归因和k阶交互方法，通过假设，可以在连续输入设置中得到唯一全面的特征交互解释，即协同作用。 |
| [^125] | [Projection-Free Online Convex Optimization with Stochastic Constraints.](http://arxiv.org/abs/2305.01333) | 该论文提出了一种用于处理带有随机约束的在线凸优化的无投影算法，其能够取得亚线性遗憾和约束违规的效果。 |
| [^126] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^127] | [LLT: An R package for Linear Law-based Feature Space Transformation.](http://arxiv.org/abs/2304.14211) | LLT是一个R包，用于线性定律特征空间变换，可以帮助对单变量和多变量时间序列进行分类。 |
| [^128] | [Moderately Distributional Exploration for Domain Generalization.](http://arxiv.org/abs/2304.13976) | 本文提出了一种针对领域泛化问题的中度分布探索（MODE）方法，通过在共享相同语义因素的不确定性子集中探索领域，可以提高模型的分布偏移鲁棒性，并在多个基准数据集上实现了最先进的性能。 |
| [^129] | [Towards Mode Balancing of Generative Models via Diversity Weights.](http://arxiv.org/abs/2304.11961) | 本研究提出了通过平衡训练数据集中的模式来增加模型输出多样性的多样性权重训练方案，以更好地适应需要多样化输出的创意应用，并在受控环境中进行的初步实验展示了其潜力。 |
| [^130] | [Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness.](http://arxiv.org/abs/2304.09779) | 研究发现，通过平等化受保护子群体之间的预测分布来实现组公平和视相似个体同等对待实现个人公正是不兼容的。 并提出了一种构建连续概率函数的解决方法，来实现组和个人公平。 |
| [^131] | [Towards Tumour Graph Learning for Survival Prediction in Head & Neck Cancer Patients.](http://arxiv.org/abs/2304.08106) | 本论文提出了一个瘤体图学习框架，通过定位、分割和生存预测，可以在任意视野的PET和CT扫描上帮助实现头颈癌患者的治疗决策和治疗效果预测。 |
| [^132] | [Localizing Model Behavior with Path Patching.](http://arxiv.org/abs/2304.05969) | 本文介绍了一种新的技术——路径修补，用于表达和定量测试表明行为被定位到一组路径的一类自然假设。 |
| [^133] | [Exploring the Connection between Robust and Generative Models.](http://arxiv.org/abs/2304.04033) | 本文探究鲁棒性判别分类器与生成模型之间的联系，并发现在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量，提出了一种名为高能量PGD的新攻击。 |
| [^134] | [Classification of Superstatistical Features in High Dimensions.](http://arxiv.org/abs/2304.02912) | 本文利用经验风险最小化的方法，对高维超统计特征下的数据进行分类，并分析了正则化和分布尺度参数对分类的影响。 |
| [^135] | [SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis.](http://arxiv.org/abs/2304.00020) | 研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。 |
| [^136] | [Efficient Neural Generation of 4K Masks for Homogeneous Diffusion Inpainting.](http://arxiv.org/abs/2303.10096) | 该论文提出了一种高效生成4K掩膜的神经网络方法，可以实现对高分辨率图像进行快速且高质量的修复。 |
| [^137] | [Synthetic Experience Replay.](http://arxiv.org/abs/2303.06614) | 本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。 |
| [^138] | [A hybrid deep-learning-metaheuristic framework for discrete road network design problems.](http://arxiv.org/abs/2303.06024) | 本研究提出了一种混合框架，结合了深度学习和元启发式算法，用于离散道路网络设计问题。该框架可以快速地得到高质量的解决方案，并且可以应用于其他以图为模型的双层问题决策中。 |
| [^139] | [Expressivity of Shallow and Deep Neural Networks for Polynomial Approximation.](http://arxiv.org/abs/2303.03544) | 本研究发现，浅层ReLU网络在表达具有随着输入维度增加的Lipschitz参数的函数时会遭受维度灾难，神经网络的表达能力更依赖于它们的深度而不是总体复杂度。 |
| [^140] | [Applications of Federated Learning in Manufacturing: Identifying the Challenges and Exploring the Future Directions with Industry 4.0 and 5.0 Visions.](http://arxiv.org/abs/2302.13514) | 该论文讨论了联邦学习在制造业中的应用，说明其对解决制造业中数据收集和分析等问题的意义，而其中的联邦学习则可以通过协作集成小型厂商的数据，并在不同设备间完成模型的更新，从而使小型制造商能够更好地利用先进的机器学习和数据驱动方法。 |
| [^141] | [Improving the Data Efficiency of Multi-Objective Quality-Diversity through Gradient Assistance and Crowding Exploration.](http://arxiv.org/abs/2302.12668) | 本文介绍了一种新的QD算法——带有策略梯度辅助和基于拥挤的探索的多目标MAP-Elites (MOME-PGX)来扩展MOME以提高其数据效率和性能。 |
| [^142] | [Revisiting Weighted Aggregation in Federated Learning with Neural Networks.](http://arxiv.org/abs/2302.10911) | 本文重新审视了联邦学习中的加权聚合方法。作者发现权重总和可能小于1，从而改善了泛化性能。作者探索了最优缩小因子如何受到客户端数据异质性和本地周期的影响，并使用客户端相干性研究了客户端之间的相对聚合权重以描绘客户端的重要性。作者提出了一种有效的联邦学习方法（FLLAW），该方法具有可学习聚合权重和全局权重缩小效应。 |
| [^143] | [Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking.](http://arxiv.org/abs/2302.10902) | 本文回顾了基于深度学习的时间序列健康数据缺失值填补方法，并在五个时间序列健康数据集上进行基准测试，研究发现填补的效果依赖于数据类型、变量统计、缺失值率和类型。在时间序列数据中同时进行交叉剖面和纵向缺失值填补的深度学习方法表现最佳。 |
| [^144] | [Policy Evaluation in Decentralized POMDPs with Belief Sharing.](http://arxiv.org/abs/2302.04151) | 本文提出了一种去中心化的信念形成策略，使得代理的参数可以与集中式基线有界的差异，并在多传感器目标跟踪应用中得到应用。 |
| [^145] | [Leveraging Demonstrations to Improve Online Learning: Quality Matters.](http://arxiv.org/abs/2302.03319) | 本篇论文探讨了离线演示数据如何改进在线学习的问题，提出了一种利用演示数据的TS算法，并给出了依赖于先验知识的贝叶斯遗憾界；研究发现，预训练可以大幅提高在线性能，改进程度随专家能力水平的提高而增加。 |
| [^146] | [Run-Off Election: Improved Provable Defense against Data Poisoning Attacks.](http://arxiv.org/abs/2302.02300) | 本文提出了一种名为ROE的新型数据污染防御方法，通过在基本模型之间进行运行式选举，有效利用logits层的信息，并在MNIST数据集和CIFAR-10上得到了比最先进的集成方法更好的结果，提供了针对数据污染攻击的可证明保证 |
| [^147] | [Planning Multiple Epidemic Interventions with Reinforcement Learning.](http://arxiv.org/abs/2301.12802) | 本文将找到最优化的疫情计划转化为马尔可夫决策过程，并利用强化学习算法来搜索最小化疾病和经济成本的计划。 |
| [^148] | [Finding Regions of Counterfactual Explanations via Robust Optimization.](http://arxiv.org/abs/2301.11113) | 该论文提出了一种通过稳健优化计算反事实解释（CE）区域的方法，使用户能够选择适当的措施以获得所需的结果，此方法在逻辑回归、决策树、随机森林和神经网络等最常见的机器学习方法上证明了收敛结果。 |
| [^149] | [Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram.](http://arxiv.org/abs/2301.10856) | 本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。 |
| [^150] | [Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2301.08491) | 本文使用多智能体强化学习模拟社会困境中的道德选择，设计了一套道德奖励结构，旨在分析和研究AI代理的道德行为。 |
| [^151] | [Learning-Rate-Free Learning by D-Adaptation.](http://arxiv.org/abs/2301.07733) | D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。 |
| [^152] | [Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning.](http://arxiv.org/abs/2301.04746) | 本论文提出了一种名为SLAP的方法，它可以替代数据增强，加强经验以加速机器学习并减少样本大小。 在Gomoku游戏和强化学习领域的实验中，证明了SLAP的有效性，可以提高模型的收敛速度，同时减少了样本数量。这种策略至少适用于对称或特定变换不变的领域。 |
| [^153] | [On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective.](http://arxiv.org/abs/2212.12669) | 本文提出了基础决策模型（FDM），通过使用转换器神经架构将各种决策任务制定为序列解码任务，为在复杂现实世界环境中实现机器驱动智能决策(IDM)提供了一个前景广阔的解决方案。 |
| [^154] | [Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias.](http://arxiv.org/abs/2212.11261) | 本文使用对比语言图像预训练的多模态AI模型，使用网页抓取的数据训练，发现这些模型存在性物化偏见，即人的情感状态与身体的呈现相关，表现出对女性的性别偏见。 |
| [^155] | [A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling.](http://arxiv.org/abs/2212.10936) | 本文提出了一种基于深度强化学习的模因算法，用于解决具有实际约束的灵活生产调度问题，并弥补元启发式研究中的缺陷。 |
| [^156] | [Automated Reachability Analysis of Neural Network-Controlled Systems via Adaptive Polytopes.](http://arxiv.org/abs/2212.07553) | 本文提出了一种基于自适应多面体的方法，用于过度逼近神经网络控制系统中的可达性分析，其中利用线性层的奇异值分解以及激活函数的形状，在每个时间步骤上调整多面体的几何形状以适应真实可达集的几何形状，并通过推断出的模板计算出可达集的精确过度逼近。 |
| [^157] | [Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator.](http://arxiv.org/abs/2212.06751) | 本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。 |
| [^158] | [Deep Learning Methods for Partial Differential Equations and Related Parameter Identification Problems.](http://arxiv.org/abs/2212.03130) | 本论文回顾了深度学习在解决偏微分方程和相关参数识别问题中的应用，提出了一个统一的框架来理解这些方法和技术。 |
| [^159] | [c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization.](http://arxiv.org/abs/2211.14411) | 本文提出了约束TPE（c-TPE）方法，是树形Parzen估计器（TPE）的扩展，可有效处理在性能要求之上施加的约束限制，实验证明在81个昂贵的HPO设置中表现出最佳性能排名。 |
| [^160] | [Learning-enhanced Nonlinear Model Predictive Control using Knowledge-based Neural Ordinary Differential Equations and Deep Ensembles.](http://arxiv.org/abs/2211.13829) | 本文介绍了一种学习增强型非线性模型预测控制方法，通过学习基于知识的神经常微分方程和深度集成的模型，提高了系统动态的预测准确性。 |
| [^161] | [Expressibility-Enhancing Strategies for Quantum Neural Networks.](http://arxiv.org/abs/2211.12670) | 本文提出四项增强量子神经网络表达能力的策略，并验证其可以显著提高QNN在近似复杂函数方面的性能表现。 |
| [^162] | [Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models.](http://arxiv.org/abs/2211.09707) | 该论文展示了使用Diffusion Models来进行音频驱动的人体运动合成的有效性和适用性，具备极高的运动质量，可以实现独特的风格表达控制，还可以应用于多源音频下的运动控制。 |
| [^163] | [Analysis and Detectability of Offline Data Poisoning Attacks on Linear Dynamical Systems.](http://arxiv.org/abs/2211.08804) | 研究发现，针对线性动态系统的数据污染攻击需要不同的攻击和检测方法。论文针对最小二乘估计进行了统计测试，确定了与数据兼容的模型集是否包括系统的真实模型，并提出了一种隐蔽的数据污染攻击。 |
| [^164] | [Combining datasets to increase the number of samples and improve model fitting.](http://arxiv.org/abs/2210.05165) | 本文提出了一种组合数据集的新框架ComImp，可以处理不同数据集之间存在不同特征的挑战，并利用PCA-ComImp进行维数降低。此外，此框架还可以用于数据预处理，填补缺失数据的条目。该方法在多个真实世界的数据集上得到了验证。 |
| [^165] | [Dataset Distillation Using Parameter Pruning.](http://arxiv.org/abs/2209.14609) | 本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。 |
| [^166] | [Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans.](http://arxiv.org/abs/2209.13020) | 这篇论文提出了一种“法律指导代码”理念，利用法律信息学将法律知识和推理嵌入到人工智能中，从而使人工智能与人类的目标和社会价值保持一致。 |
| [^167] | [Fairness in Forecasting of Observations of Linear Dynamical Systems.](http://arxiv.org/abs/2209.05274) | 该论文介绍了在机器学习中公平性的概念，并为线性动态系统观测预测中的子群体公平和瞬时公平引入了全球收敛方法，以解决代表性不足的偏见问题。 |
| [^168] | [Challenging Common Assumptions about Catastrophic Forgetting.](http://arxiv.org/abs/2207.04543) | 本文挑战了DNNs遭受灾难性遗忘的常见假设，提出当前优化技术可以缓解这种现象。我们的实验表明，使用标准梯度下降优化方法训练的DNNs在回归任务上可以积累知识，即使任务重新出现，也不会遗忘过去的知识，这种知识积累可以改善在未见任务上的泛化性能。 |
| [^169] | [Gated Domain Units for Multi-source Domain Generalization.](http://arxiv.org/abs/2206.12444) | 该研究提出了一个朴素的假设，即现实世界的分布由不同域中的潜在不变的基元分布（I.E.D）组成，其引入了一个由门控域单元（GDUs）组成的模块化神经网络层。这种方法可以更好地实现对看不见的域的知识转移，从而提高模型的性能。 |
| [^170] | [Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm.](http://arxiv.org/abs/2206.02059) | 本文提出了一种基于边缘感知的Weisfeiler-Lehman算法，以增强图神经网络的表达能力，同时保持消息传递方案的可扩展性。实验表明，我们NC-GNN框架在各种基准测试中表现出有效性和高效性。 |
| [^171] | [Federated Progressive Sparsification (Purge, Merge, Tune)+.](http://arxiv.org/abs/2204.12430) | 本文提出了一种联邦学习的稀疏化策略，可以逐步约束模型的参数集，降低计算和通信成本，同时达到高性能的稀疏化率。实验表明，稀疏模型可以缩小原始模型的十分之一，而准确率不降低或更高。 |
| [^172] | [Quantum compiling with variational instruction set for accurate and fast quantum computing.](http://arxiv.org/abs/2203.15574) | 本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法，实现了快速和精确的量子计算，并且通过实验展示出了更低的误差积累和时间成本。 |
| [^173] | [Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems.](http://arxiv.org/abs/2112.08091) | 本文提出了一个“预防性学习”框架，以在满足对约束标定的条件下，保证具有凸约束和一般目标函数的问题的DNN解的可行性，而无需后处理。通过系统标定不等式约束，我们预示预测误差并确保所得到的解仍然是可行的。同时提出了一种新的对抗样本感知的训练算法以提高DNN的最优性能而不牺牲可行性保证。 |
| [^174] | [A moment-matching metric for latent variable generative models.](http://arxiv.org/abs/2111.00875) | 本文提出了一种用于比较和正则化潜变量生成模型的新型矩匹配度量方法，该方法通过研究数据矩和模型矩之间的差异来评估拟合模型质量。 |
| [^175] | [High-dimensional Inference for Dynamic Treatment Effects.](http://arxiv.org/abs/2110.04924) | 本文提出了一种新的 DR 方法，用于中间条件结果模型的 DR 表示，能够提供更优的稳健性保证，即使在面临高维混淆变量时也能实现一致性。 |
| [^176] | [Training Spiking Neural Networks Using Lessons From Deep Learning.](http://arxiv.org/abs/2109.12894) | 本论文介绍如何将几十年的深度学习、梯度下降、反向传播和神经科学研究的经验教训应用于生物可行的脉冲神经网络，并探索了将数据编码为脉冲和学习过程之间的微妙相互作用以及生物可行的在线学习的发展方向。 |
| [^177] | [MRCpy: A Library for Minimax Risk Classifiers.](http://arxiv.org/abs/2108.01952) | MRCpy是一种用于实现最小化风险分类器的Python库，它基于鲁棒风险最小化技术，可以利用0-1损失并提供了多种分类方法，其中一些提供了紧密的期望损失界限。 |
| [^178] | [GaNDLF: A Generally Nuanced Deep Learning Framework for Scalable End-to-End Clinical Workflows in Medical Imaging.](http://arxiv.org/abs/2103.01006) | GaNDLF是一个通用的且易用的深度学习框架，旨在降低深度学习算法开发的难度，并提供了一种使深度学习可重复、可解释和可扩展的端到端解决方案，它在医学影像中具有广泛的应用。 |
| [^179] | [Graph neural networks-based Scheduler for Production planning problems using Reinforcement Learning.](http://arxiv.org/abs/2009.03836) | 本文提出了GraSP-RL框架，基于图神经网络来训练强化学习代理，以解决车间调度问题中状态空间难以处理、泛化能力较差的问题。 |
| [^180] | [Model Fusion via Optimal Transport.](http://arxiv.org/abs/1910.05653) | 本文提出一种基于最优传输的神经网络模型融合算法，能够成功地在不需要重新训练的情况下进行“单次”知识迁移，并且在独立同分布和非独立同分布的情况下比简单平均和集成模型更优。 |
| [^181] | [S-ConvNet: A Shallow Convolutional Neural Network Architecture for Neuromuscular Activity Recognition Using Instantaneous High-Density Surface EMG Images.](http://arxiv.org/abs/1906.03381) | S-ConvNet和All-ConvNet模型为神经肌肉活动识别提出了一种简单而有效的学习框架，不需要预训练，表现出非常具有竞争力的识别准确性。 |

# 详细

[^1]: 在三周内为8,000个腹部CT扫描标注多器官分割

    Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks. (arXiv:2305.09666v1 [eess.IV])

    [http://arxiv.org/abs/2305.09666](http://arxiv.org/abs/2305.09666)

    本文提出了一种高效方法在短时间内标记8000个腹部CT扫描中的8个器官，建立了迄今为止最大的多器官数据集。

    

    医学影像标注，特别是器官分割，是费时费力的。本文提出了一种系统高效的方法来加速器官分割的标注过程。我们标注了8,448个腹部CT扫描，标记了脾脏、肝脏、肾脏、胃、胆囊、胰腺、主动脉和下腔静脉。传统的标注方法需要一位经验丰富的标注员1600周，而我们的标注方法仅用了三周。

    Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes a systematic and efficient method to expedite the annotation process for organ segmentation. We have created the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation method has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintain
    
[^2]: 分布式鲁棒的离线强化学习：基于双重悲观性的通用算法和强健部分覆盖

    Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])

    [http://arxiv.org/abs/2305.09659](http://arxiv.org/abs/2305.09659)

    本论文提出了一个名为P2MPO的算法框架，用于解决基于鲁棒离线RL的问题。该框架结合了灵活的模型估计子例程和双重悲观的策略优化步骤，采用双重悲观性原则以克服模型偏移等问题。研究表明，在模型准确性的假设下，该框架在拥有良好的鲁棒部分覆盖数据的情况下是具备高效性的。

    

    本文研究了分布式鲁棒的离线强化学习（鲁棒离线RL），其旨在从离线数据集中纯粹地找到一个能够在扰动环境中表现良好的最优强鲁棒策略。我们提出了一个名为P2MPO的算法框架，其中包含了灵活的模型估计子例程和双重悲观的策略优化步骤。双重悲观性原则对于克服由行为策略和目标策略家族之间的不匹配以及名义模型的扰动所引起的分布偏移至关重要。在对模型估计子例程进行一定准确性假设的情况下，我们证明了P2MPO算法在拥有良好的鲁棒部分覆盖数据的情况下是可证明有效的。

    We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \underline{D}oubly \underline{P}essimistic \underline{M}odel-based \underline{P}olicy \underline{O}ptimization ($\texttt{P}^2\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\texttt{P}^2\texttt{MPO}$ is provably efficient with \emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d
    
[^3]: 个性化指导有助于知识蒸馏

    Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])

    [http://arxiv.org/abs/2305.09651](http://arxiv.org/abs/2305.09651)

    本文提出了一种个性化指导的学习技术，称为LGTM，其利用蒸馏效应选择样本以增强学生的泛化能力，在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。

    

    先前研究表明，能力超群的教师模型并不一定能够让学生水平得到提升，这凸显了当前教师培训实践和有效知识传授之间的不一致性。为了提高教师培训过程的指导效果，本文引入了蒸馏效应的概念，以确定每个训练样本对学生泛化能力的影响。我们提出了一种名为学好教师很重要（LGTM）的有效训练技术，以将蒸馏效应纳入教师的学习过程中。通过优先选择可能提升学生泛化能力的样本，我们的LGTM在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。

    It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
    
[^4]: 基于偏好排序的Prompt-Tuning决策变换器

    Prompt-Tuning Decision Transformer with Preference Ranking. (arXiv:2305.09648v1 [cs.LG])

    [http://arxiv.org/abs/2305.09648](http://arxiv.org/abs/2305.09648)

    本研究提出了Prompt-Tuning DT算法，通过使用轨迹段作为prompt来指导RL agent获取环境信息，从而在具有挑战性的任务中实现了优秀表现。

    

    Prompt-tuning已成为一种很有前途的适应预训练模型到下游任务或与人类偏好对齐的方法。Prompt learning 在自然语言处理中得到广泛应用，但由于RL prompts中包含的复杂物理含义和环境特定信息，其适用性有限。这些因素需要通过监督学习来模仿演示，并在学习后可能导致意义的丧失。此外，将prompt-tuning方法直接扩展到RL是具有挑战性的，因为RL prompts是根据环境建模和分析来指导agent行为的，而不是填补缺失的信息，因此在下游任务中调整prompt格式，如在NLP中，可能不会产生显著的改进。本文提出了Prompt-Tuning DT算法来解决这些挑战，通过将轨迹段用作prompt来指导RL agent获取环境信息，并通过黑盒调整来优化prompt，从而改善了下游任务的性能。

    Prompt-tuning has emerged as a promising method for adapting pre-trained models to downstream tasks or aligning with human preferences. Prompt learning is widely used in NLP but has limited applicability to RL due to the complex physical meaning and environment-specific information contained within RL prompts. These factors require supervised learning to imitate the demonstrations and may result in a loss of meaning after learning. Additionally, directly extending prompt-tuning approaches to RL is challenging because RL prompts guide agent behavior based on environmental modeling and analysis, rather than filling in missing information, making it unlikely that adjustments to the prompt format for downstream tasks, as in NLP, can yield significant improvements. In this work, we propose the Prompt-Tuning DT algorithm to address these challenges by using trajectory segments as prompts to guide RL agents in acquiring environmental information and optimizing prompts via black-box tuning to 
    
[^5]: torchosr--一个PyTorch扩展包用于Python中Open Set Recognition模型的评估

    torchosr -- a PyTorch extension package for Open Set Recognition models evaluation in Python. (arXiv:2305.09646v1 [cs.LG])

    [http://arxiv.org/abs/2305.09646](http://arxiv.org/abs/2305.09646)

    本文介绍了torchosr--一个专为深度神经网络中Open Set Recognition设计的Python扩展包，提供了两种最新的方法和一系列处理数据集和方法的工具，目的是为了简化和推广正确的实验评估。

    

    本文介绍了torchosr包--一个与PyTorch库兼容的Python包，提供了专为深度神经网络中的Open Set Recognition设计的工具和方法。该软件包提供了两种最新的Open Set Recognition方法，一组处理基础集并生成用于Open Set Recognition任务的派生集的函数（其中一些类在测试过程中被认为是未知的并仅使用），以及用于处理数据集和方法的其他工具。该软件包提议的主要目标是简化和推广正确的实验评估，其中实验在大量具有不同Openness和类别分配的派生集上进行。作者希望，在软件包中提供的最先进的方法将成为所述领域相关解决方案的正确和开源实现的来源。

    The article presents the torchosr package - a Python package compatible with PyTorch library - offering tools and methods dedicated to Open Set Recognition in Deep Neural Networks. The package offers two state-of-the-art methods in the field, a set of functions for handling base sets and generation of derived sets for the Open Set Recognition task (where some classes are considered unknown and used only in the testing process) and additional tools to handle datasets and methods. The main goal of the package proposal is to simplify and promote the correct experimental evaluation, where experiments are carried out on a large number of derivative sets with various Openness and class-to-category assignments. The authors hope that state-of-the-art methods available in the package will become a source of a correct and open-source implementation of the relevant solutions in the domain.
    
[^6]: FitMe：深层逼真的三维可塑模型化身

    FitMe: Deep Photorealistic 3D Morphable Model Avatars. (arXiv:2305.09641v1 [cs.CV])

    [http://arxiv.org/abs/2305.09641](http://arxiv.org/abs/2305.09641)

    FitMe是一个可用于单个或多个图像的深层逼真的三维可塑模型化身系统，通过多模式生成器和PCA形状模型捕捉了人脸外观的漫反射和镜面反射，同时使用了快速可微分渲染过程以实现高保真可渲染的人类化身。

    

    本文介绍了FitMe，一个人脸反射模型和可微分渲染优化管道，可从单个或多个图像中获取高保真可渲染的人类化身。该模型由多模式基于风格的生成器和基于PCA的形状模型组成，捕捉人脸外观的漫反射和镜面反射。我们采用快速可微分渲染过程，在优化管道中使用，同时实现逼真的面部着色。FitMe在单个“野外”人脸图像上实现了最先进的反射获取和身份保护，并在给定同一身份的多个不受约束的人脸图像时产生了令人印象深刻的扫描效果。与最近改进的方法相比，FitMe可以使用高效的优化过程生成高保真、可渲染的人类化身，准确捕捉人脸反射和形状，同时实现逼真的面部着色。

    In this paper, we introduce FitMe, a facial reflectance model and a differentiable rendering optimization pipeline, that can be used to acquire high-fidelity renderable human avatars from single or multiple images. The model consists of a multi-modal style-based generator, that captures facial appearance in terms of diffuse and specular reflectance, and a PCA-based shape model. We employ a fast differentiable rendering process that can be used in an optimization pipeline, while also achieving photorealistic facial shading. Our optimization process accurately captures both the facial reflectance and shape in high-detail, by exploiting the expressivity of the style-based latent representation and of our shape model. FitMe achieves state-of-the-art reflectance acquisition and identity preservation on single "in-the-wild" facial images, while it produces impressive scan-like results, when given multiple unconstrained facial images pertaining to the same identity. In contrast with recent im
    
[^7]: SoundStorm：高效并行音频生成的模型

    SoundStorm: Efficient Parallel Audio Generation. (arXiv:2305.09636v1 [cs.SD])

    [http://arxiv.org/abs/2305.09636](http://arxiv.org/abs/2305.09636)

    SoundStorm 是一种高效的非自回归音频生成模型，可以在更短的时间内生成音质相同但更加一致的音频，并且能够扩展序列长度进行高质量的自然对话片段合成。

    

    我们提出了 SoundStorm，一种用于高效的非自回归音频生成的模型。SoundStorm 接收 AudioLM 的语义标记作为输入，并依靠双向注意力和基于置信度的并行解码来生成神经音频编解码器的标记。与 AudioLM 的自回归生成方法相比，我们的模型在音色和声学条件下产生相同质量且更高一致性的音频，速度快两个数量级。SoundStorm 在 TPU-v4 上能在0.5秒内生成30秒的音频。我们通过合成带有说话者转换注释和短提示的高质量自然对话片段来展示我们的模型可以扩大音频生成的序列长度。

    We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.
    
[^8]: 具有衰减局部SGD步数的快速联邦学习

    Faster Federated Learning with Decaying Number of Local SGD Steps. (arXiv:2305.09628v1 [cs.LG])

    [http://arxiv.org/abs/2305.09628](http://arxiv.org/abs/2305.09628)

    本研究提出了一种具有衰减局部SGD步数的方法，可以提高联邦学习模型的最终性能，同时减少训练的时间和计算成本。

    

    在联邦学习中，连接在互联网上的客户端设备可以在不与中央服务器或其他客户端共享私人数据的情况下协同训练机器学习模型。FedAvg算法通过在客户端上执行局部训练并进行模型平均来训练单个全球模型。本文提出了一种随着训练的进行而衰减的K值方法，并分析了FedAvg在此情况下的收敛性。同时，在各种非独立同分布的数据集上实验验证了我们的方法的有效性。

    In Federated Learning (FL) client devices connected over the internet collaboratively train a machine learning model without sharing their private data with a central server or with other clients. The seminal Federated Averaging (FedAvg) algorithm trains a single global model by performing rounds of local training on clients followed by model averaging. FedAvg can improve the communication-efficiency of training by performing more steps of Stochastic Gradient Descent (SGD) on clients in each round. However, client data in real-world FL is highly heterogeneous, which has been extensively shown to slow model convergence and harm final performance when $K > 1$ steps of SGD are performed on clients per round. In this work we propose decaying $K$ as training progresses, which can jointly improve the final performance of the FL model whilst reducing the wall-clock time and the total computational cost of training compared to using a fixed $K$. We analyse the convergence of FedAvg with decayi
    
[^9]: 运用机器学习解决物理系统模拟中的计算挑战

    Addressing computational challenges in physical system simulations with machine learning. (arXiv:2305.09627v1 [cs.LG])

    [http://arxiv.org/abs/2305.09627](http://arxiv.org/abs/2305.09627)

    本文提出了一个机器学习数据生成框架，通过有限的模拟数据训练有监督的预测模型和强化学习代理生成准确的、类似于模拟的数据来解决物理系统模拟中的计算挑战。

    

    本文提出了一个机器学习数据生成框架，旨在帮助利用模拟研究各种物理系统或过程的研究人员。高计算成本和由此导致的有限数据经常会对获得这些系统或过程的深入见解产生重大挑战。我们的方法包括两个步骤：首先，我们使用有限的模拟数据训练有监督的预测模型来预测模拟结果。其次，利用强化学习代理来生成准确的、类似于模拟的数据。借助这个框架，研究人员可以生成更准确的数据并知道结果，而不必运行高计算代价的模拟，从而更有效地探索参数空间并深入洞察物理系统或过程。我们通过两个案例研究来展示所提出框架的有效性，一个是地震预测，一个是奥托循环建模。

    In this paper, we present a machine learning-based data generator framework tailored to aid researchers who utilize simulations to examine various physical systems or processes. High computational costs and the resulting limited data often pose significant challenges to gaining insights into these systems or processes. Our approach involves a two-step process: initially, we train a supervised predictive model using a limited simulated dataset to predict simulation outcomes. Subsequently, a reinforcement learning agent is trained to generate accurate, simulation-like data by leveraging the supervised model. With this framework, researchers can generate more accurate data and know the outcomes without running high computational simulations, which enables them to explore the parameter space more efficiently and gain deeper insights into physical systems or processes. We demonstrate the effectiveness of the proposed framework by applying it to two case studies, one focusing on earthquake r
    
[^10]: 平衡风险与收益：一种自动化的分阶段发布策略

    Balancing Risk and Reward: An Automated Phased Release Strategy. (arXiv:2305.09626v1 [stat.ML])

    [http://arxiv.org/abs/2305.09626](http://arxiv.org/abs/2305.09626)

    本文提出了一种自动化的分阶段发布策略，能够在控制风险的同时最大化启动速度，通过一个受约束的分批赌博机问题模型来实现。

    

    分阶段发布是科技行业中逐步发布新产品或更新的常见策略，通过一系列A/B测试，逐步增加处理单元的数量，直到完全部署或废弃。以原则性的方式执行分阶段发布需要以平衡不良影响的风险和迭代和快速学习的需求来选择分配给新发布的单位比例。在本文中，我们正式地阐述了这个问题，并提出了一种算法，在调度的每个阶段自动确定发布百分比，平衡控制风险和最大化启动速度的需求。我们的框架将这一挑战建模为一个受约束的分批赌博机问题，以确保我们预先指定的实验预算不会被高概率耗尽。我们提出的算法利用了自适应贝叶斯方法，其中将分配给处理的最大单元数由

    Phased releases are a common strategy in the technology industry for gradually releasing new products or updates through a sequence of A/B tests in which the number of treated units gradually grows until full deployment or deprecation. Performing phased releases in a principled way requires selecting the proportion of units assigned to the new release in a way that balances the risk of an adverse effect with the need to iterate and learn from the experiment rapidly. In this paper, we formalize this problem and propose an algorithm that automatically determines the release percentage at each stage in the schedule, balancing the need to control risk while maximizing ramp-up speed. Our framework models the challenge as a constrained batched bandit problem that ensures that our pre-specified experimental budget is not depleted with high probability. Our proposed algorithm leverages an adaptive Bayesian approach in which the maximal number of units assigned to the treatment is determined by
    
[^11]: 带高斯过程回归识别的条件变分自编码器用于参数模型

    Conditional variational autoencoder with Gaussian process regression recognition for parametric models. (arXiv:2305.09625v1 [cs.CE])

    [http://arxiv.org/abs/2305.09625](http://arxiv.org/abs/2305.09625)

    本文提出了一种带高斯过程回归识别的条件变分自编码器框架，可以用于数据驱动的参数模型，通过提取低维特征和学习参数到潜变量的映射来缓解模型复杂度的问题。

    

    本文提出了一种基于数据驱动的方法来处理有噪声观察数据的参数模型。高斯过程回归的基于降阶建模（GPR-based ROM）可以实现快速的在线预测，而不需要在离线阶段使用方程。然而，GPR-based ROM对于复杂系统的表现不佳，因为POD投影天然是线性的。条件变分自编码器（CVAE）可以通过非线性神经网络解决这个问题，但它具有更高的模型复杂度，这给训练和调整超参数带来了挑战。因此，我们提出了一种带有高斯过程回归识别的CVAE框架（CVAE-GPRR）。该方法包括识别模型和可能性模型。在识别模型中，我们首先通过POD从数据中提取低维特征来过滤高频冗余信息。然后使用非参数模型GPR来学习从参数到POD潜变量的映射，这也可以缓解模型复杂度的问题。

    In this article, we present a data-driven method for parametric models with noisy observation data. Gaussian process regression based reduced order modeling (GPR-based ROM) can realize fast online predictions without using equations in the offline stage. However, GPR-based ROM does not perform well for complex systems since POD projection are naturally linear. Conditional variational autoencoder (CVAE) can address this issue via nonlinear neural networks but it has more model complexity, which poses challenges for training and tuning hyperparameters. To this end, we propose a framework of CVAE with Gaussian process regression recognition (CVAE-GPRR). The proposed method consists of a recognition model and a likelihood model. In the recognition model, we first extract low-dimensional features from data by POD to filter the redundant information with high frequency. And then a non-parametric model GPR is used to learn the map from parameters to POD latent variables, which can also allevi
    
[^12]: AI增强的调查：利用大语言模型进行全国代表性调查的观点预测

    AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])

    [http://arxiv.org/abs/2305.09620](http://arxiv.org/abs/2305.09620)

    本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。

    

    本论文研究了如何使用经过全国代表性调查微调的大语言模型（LLMs）来增强调查。本文探讨了LLMs在观点预测中，遗漏数据插值，回溯推理和零次预测三个不同应用。我们提出了一种新的方法论框架，将调查问题、个人信念和时间背景的神经嵌入引入到观点预测的个性化LLMs中。在1972年到2021年的“常规社会调查”中，我们从68,846名美国人中获得了3,110个二进制观点，在Alpaca-7b模型的基础上取得了最好的成果，在缺失数据插值（AUC=0.87，公开观点预测为$\rho$=0.99）和回溯推理（AUC=0.86，$\rho$=0.98）方面表现出色。这些显著的预测能力能够以高置信度填补缺失的趋势，并标明公众态度何时发生变化，如同性婚姻的获取支持。然而，在零次预测的情况下，模型的表现受到限制，需要进一步研究。

    How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
    
[^13]: 学习的局部线性模型在非线性策略优化中的威力

    The Power of Learned Locally Linear Models for Nonlinear Policy Optimization. (arXiv:2305.09619v1 [cs.LG])

    [http://arxiv.org/abs/2305.09619](http://arxiv.org/abs/2305.09619)

    本文介绍了一种学习非线性系统动态的策略优化算法，该算法通过估计局部线性模型和执行类似于$\mathtt{iLQR}$的策略更新之间的迭代来实现，具有多项式的样本复杂度并克服了指数区间上的依赖性。

    

    在基于学习的控制中，常见的流程是逐步估计系统动力学模型，并应用轨迹优化算法（例如$\mathtt{iLQR}$）在学习的模型上进行优化，以最小化目标成本。本文对一种简化版的此策略应用于一般非线性系统的情况进行了严格分析。我们分析了一种算法，该算法在估计非线性系统动态的局部线性模型和执行类似于$\mathtt{iLQR}$的策略更新之间进行迭代。我们证明该算法在相关问题参数中达到了多项式的样本复杂度，并通过合成局部稳定增益，克服了在问题区间上的指数依赖性。实验结果验证了我们算法的性能，并与自然的深度学习基线进行了比较。

    A common pipeline in learning-based control is to iteratively estimate a model of system dynamics, and apply a trajectory optimization algorithm e.g.~$\mathtt{iLQR}$ - on the learned model to minimize a target cost. This paper conducts a rigorous analysis of a simplified variant of this strategy for general nonlinear systems. We analyze an algorithm which iterates between estimating local linear models of nonlinear system dynamics and performing $\mathtt{iLQR}$-like policy updates. We demonstrate that this algorithm attains sample complexity polynomial in relevant problem parameters, and, by synthesizing locally stabilizing gains, overcomes exponential dependence in problem horizon. Experimental results validate the performance of our algorithm, and compare to natural deep-learning baselines.
    
[^14]: 大型语言模型在医学问答中的应用：迈向医学专家级别的问答能力

    Towards Expert-Level Medical Question Answering with Large Language Models. (arXiv:2305.09617v1 [cs.CL])

    [http://arxiv.org/abs/2305.09617](http://arxiv.org/abs/2305.09617)

    本研究提出了Med-PaLM2，通过结合基础LLM改进、医学领域微调和提示策略，并用新颖的集成精炼方法，实现了在MedQA数据集上达到86.5%的医学问答准确率，迈向医学专家级别的问答能力。

    

    近年来，人工智能系统在诸如围棋和蛋白质折叠等“宏伟挑战”方面取得了里程碑式的进展。但回答医学问题并像医生一样进行推理被认为也是一种宏伟挑战。大型语言模型在医学问答方面取得了重大进展；Med-PaLM是第一个在MedQA数据集上以67.2％的分数超过美国医疗执业考试（USMLE）样式问题的“及格”分数的模型。 然而，对比模型答案和医生答案，这项和其他先前工作表明还有很大的改进空间。本文提出了Med-PaLM2，通过利用基础LLM改进（PaLM2）、医学领域微调和提示策略（包括新颖的集成精炼方法）来弥合这些差距。在MedQA数据集上，Med-PaLM2的得分可达86.5％，比Med-PaLM提高了超过11％。

    Recent artificial intelligence (AI) systems have reached milestones in "grand challenges" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.  Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a "passing" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 
    
[^15]: 基于能量归一化流的语义分割中并发误分类和越界检测

    Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow. (arXiv:2305.09610v1 [cs.CV])

    [http://arxiv.org/abs/2305.09610](http://arxiv.org/abs/2305.09610)

    该论文提出了一种基于能量归一化流框架的生成模型，用于并发的在分布内误分类（IDM）和越界检测，可以扩展先前部署的分割模型而无需重新训练，并在测试中实现很好的结果。

    

    最近的语义分割模型对于和训练数据集分布相似的测试样本进行分类具有很高的准确性。但是，它们的判别式闭合方法在实际数据设置中对于分布偏移和越界（OOD）类不够稳健。因此，在测试时间使用预测概率作为置信度分数时，预测的概率可能非常不精确。为了解决这个问题，我们提出了一种基于正常化流框架的生成模型，用于并发的在分布内误分类（IDM）和越界检测。所提出的基于流的带有能量输入的检测器（FlowEneDet）可以扩展先前部署的分割模型而无需耗费大量时间进行重新训练。我们的FlowEneDet在内存占用方面具有极小的增加，因此具有低复杂度架构。在Cityscapes、Cityscapes-C、FishyScapes和 SegmentMeIfYouCan评估中，FlowEneDet在使用预先训练的DeepLabV3+和Translated title:基于能量归一化流的语义分割中并发误分类和越界检测的测试中实现很好的结果。

    Recent semantic segmentation models accurately classify test-time examples that are similar to a training dataset distribution. However, their discriminative closed-set approach is not robust in practical data setups with distributional shifts and out-of-distribution (OOD) classes. As a result, the predicted probabilities can be very imprecise when used as confidence scores at test time. To address this, we propose a generative model for concurrent in-distribution misclassification (IDM) and OOD detection that relies on a normalizing flow framework. The proposed flow-based detector with an energy-based inputs (FlowEneDet) can extend previously deployed segmentation models without their time-consuming retraining. Our FlowEneDet results in a low-complexity architecture with marginal increase in the memory footprint. FlowEneDet achieves promising results on Cityscapes, Cityscapes-C, FishyScapes and SegmentMeIfYouCan benchmarks in IDM/OOD detection when applied to pretrained DeepLabV3+ and
    
[^16]: 软件工程句对中的冲突和重复检测数据增强

    Data Augmentation for Conflict and Duplicate Detection in Software Engineering Sentence Pairs. (arXiv:2305.09608v1 [cs.SE])

    [http://arxiv.org/abs/2305.09608](http://arxiv.org/abs/2305.09608)

    本文探讨了使用数据增强技术来增强软件工程任务中的冲突和重复检测。该研究提出了新的数据增强技术，通过全面实证分析六个软件文本数据集来识别句对冲突和重复，结果表明数据增强技术对软件句对文本数据集的性能都有显着影响。

    

    本文研究了使用文本数据增强技术来增强通过句对分类进行的软件工程任务中的冲突和重复检测。该研究采用了常见的增强技术，如打乱顺序，回译和释义，并提出了新的数据增强技术，如名词-动词替换，目标-引理替换和演员-动作替换用于软件需求文本。通过对六个软件文本数据集进行全面的实证分析，以识别句对之间的冲突和重复。结果表明，数据增强技术对所有软件句对文本数据集的性能都有显着影响。另一方面，在数据集相对平衡的情况下，使用增强技术可能会对分类性能产生负面影响。

    This paper explores the use of text data augmentation techniques to enhance conflict and duplicate detection in software engineering tasks through sentence pair classification. The study adapts generic augmentation techniques such as shuffling, back translation, and paraphrasing and proposes new data augmentation techniques such as Noun-Verb Substitution, target-lemma replacement and Actor-Action Substitution for software requirement texts. A comprehensive empirical analysis is conducted on six software text datasets to identify conflicts and duplicates among sentence pairs. The results demonstrate that data augmentation techniques have a significant impact on the performance of all software pair text datasets. On the other hand, in cases where the datasets are relatively balanced, the use of augmentation techniques may result in a negative effect on the classification performance.
    
[^17]: 漫扩扩散模型和采样器的表达能力研究

    Expressiveness Remarks for Denoising Diffusion Models and Samplers. (arXiv:2305.09605v1 [stat.ML])

    [http://arxiv.org/abs/2305.09605](http://arxiv.org/abs/2305.09605)

    本文在漫扩扩散模型和采样器方面进行了表达能力的研究，通过将已知的神经网络逼近结果扩展到漫扩扩散模型和采样器来实现。

    

    漫扩扩散模型是一类生成模型，在许多领域最近已经取得了最先进的结果。通过漫扩过程逐渐向数据中添加噪声，将数据分布转化为高斯分布。然后，通过模拟该漫扩的时间反演的逼近来获取生成模型的样本，刚开始这个漫扩模拟的初始值是高斯样本。最近的研究探索了将漫扩模型适应于采样和推断任务。本文基于众所周知的与F\"ollmer漂移类似的随机控制联系，将针对F\"ollmer漂移的已知神经网络逼近结果扩展到漫扩扩散模型和采样器。

    Denoising diffusion models are a class of generative models which have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored adapting diffusion models for sampling and inference tasks. In this paper, we leverage known connections to stochastic control akin to the F\"ollmer drift to extend established neural network approximation results for the F\"ollmer drift to denoising diffusion models and samplers.
    
[^18]: 深度强化学习最大化繁忙路段通行效率

    Deep Reinforcement Learning to Maximize Arterial Usage during Extreme Congestion. (arXiv:2305.09600v1 [cs.AI])

    [http://arxiv.org/abs/2305.09600](http://arxiv.org/abs/2305.09600)

    本文提出了一种基于深度强化学习的方法来减少繁忙多车道高速公路交通拥堵，该方法可以学习适应性绕行策略，在减少拥堵和提高车速的同时，优化高速公路车道和周边局部收发路网的使用，实验证明其可以显著优化道路收发效率并减少总通行时间。

    

    在道路网络中，交通事故和其他事件，如果不加以缓解，可能会导致级联故障，影响系统的大部分功能。及时处理这种极端拥堵情况是降低排放量、提高生产率和改善城市生活质量的必要手段。本文提出了一种基于深度强化学习（DRL）的方法，在繁忙的多车道高速公路上减少交通拥堵。该智能体被训练学习适应性绕行策略，以便在拥堵的高速公路交通中最优地利用高速公路车道和周边局部收发路网，奖励是减少拥堵及提高车速。实验设置在美国华盛顿州Shoreline市的一段长2.6英里、4条车道的高速公路中进行，在微观和连续的综合交通模拟器SUMO（城市移动仿真）中模拟了两个出口和相关的收发路，同时使用了参数化的转移学习。研究结果表明，所提出的方法可以显著提高极端拥堵时的道路收发效率并减少总通行时间。

    Collisions, crashes, and other incidents on road networks, if left unmitigated, can potentially cause cascading failures that can affect large parts of the system. Timely handling such extreme congestion scenarios is imperative to reduce emissions, enhance productivity, and improve the quality of urban living. In this work, we propose a Deep Reinforcement Learning (DRL) approach to reduce traffic congestion on multi-lane freeways during extreme congestion. The agent is trained to learn adaptive detouring strategies for congested freeway traffic such that the freeway lanes along with the local arterial network in proximity are utilized optimally, with rewards being congestion reduction and traffic speed improvement. The experimental setup is a 2.6-mile-long 4-lane freeway stretch in Shoreline, Washington, USA with two exits and associated arterial roads simulated on a microscopic and continuous multi-modal traffic simulator SUMO (Simulation of Urban MObility) while using parameterized t
    
[^19]: 利用机器学习技术识别和分类系外行星

    Identification and Classification of Exoplanets Using Machine Learning Techniques. (arXiv:2305.09596v1 [astro-ph.EP])

    [http://arxiv.org/abs/2305.09596](http://arxiv.org/abs/2305.09596)

    本文提出了利用深度学习算法对Kepler太空望远镜和扩展任务K2的数据进行系外行星分类的方法，尤其是通过使用Siamese架构可有效解决低数据情况下的分类问题，实现了较高的分类准确率。

    

    美国宇航局的Kepler太空望远镜在发现系外行星的任务中发挥了重要作用。通过对Kepler望远镜接收到的信号进行计算数据分析，支持了这项搜索工作。本文考虑利用残差网络对Kepler太空望远镜及其扩展任务K2的数据进行系外行星识别方面的现有工作的基础上建设。本文旨在探索深度学习算法如何帮助在一个低数据情况和一个更广泛的数据情况下进行系外行星存在的分类。除了标准CNN方法外，我们提出了一个Siamese架构，特别适用于低数据情况下进行分类。CNN和ResNet算法对于三类分类的平均准确率为68％，对于两类分类的准确率为86％。然而，对于三类和两类分类，Siamese算法均达到了99％的准确率。

    NASA's Kepler Space Telescope has been instrumental in the task of finding the presence of exoplanets in our galaxy. This search has been supported by computational data analysis to identify exoplanets from the signals received by the Kepler telescope. In this paper, we consider building upon some existing work on exoplanet identification using residual networks for the data of the Kepler space telescope and its extended mission K2. This paper aims to explore how deep learning algorithms can help in classifying the presence of exoplanets with less amount of data in one case and a more extensive variety of data in another. In addition to the standard CNN-based method, we propose a Siamese architecture that is particularly useful in addressing classification in a low-data scenario. The CNN and ResNet algorithms achieved an average accuracy of 68% for three classes and 86% for two-class classification. However, for both the three and two classes, the Siamese algorithm achieved 99% accurac
    
[^20]: HiNoVa：一种用于自动化射频设备认证的新型开放集检测方法

    HiNoVa: A Novel Open-Set Detection Method for Automating RF Device Authentication. (arXiv:2305.09594v1 [cs.CR])

    [http://arxiv.org/abs/2305.09594](http://arxiv.org/abs/2305.09594)

    HiNoVa是一种用于自动化射频设备认证的新型开放集检测方法，利用卷积神经网络长短期记忆模型中隐藏状态值模式，可以成功用于监控和控制无线设备的未经授权的网络访问。

    

    深度学习利用射频数据中的模式识别和认证设备，为无线网络安全提供了新的功能。开放集检测是深度学习中的一个领域，它可以在部署期间识别来自新设备的样本，而这些设备不在训练集中。过去在开放集检测方面的工作主要应用于独立且同分布的数据，例如图像。相反，RF信号数据具有一系列独特的挑战，因为数据形成一个时间序列，并且样本之间存在非线性的时间依赖关系。我们引入了一种基于卷积神经网络长短期记忆模型中隐藏状态值模式的新型开放集检测方法。我们的方法极大地提高了在LoRa、无线WiFi和有线WiFi数据集上的精度-召回曲线下的面积，因此，可以成功用于监控和控制无线设备的未经授权的网络访问。

    New capabilities in wireless network security have been enabled by deep learning, which leverages patterns in radio frequency (RF) data to identify and authenticate devices. Open-set detection is an area of deep learning that identifies samples captured from new devices during deployment that were not part of the training set. Past work in open-set detection has mostly been applied to independent and identically distributed data such as images. In contrast, RF signal data present a unique set of challenges as the data forms a time series with non-linear time dependencies among the samples. We introduce a novel open-set detection approach based on the patterns of the hidden state values within a Convolutional Neural Network (CNN) Long Short-Term Memory (LSTM) model. Our approach greatly improves the Area Under the Precision-Recall Curve on LoRa, Wireless-WiFi, and Wired-WiFi datasets, and hence, can be used successfully to monitor and control unauthorized network access of wireless devi
    
[^21]: 基于归纳图神经网络的移动物体分割技术

    Inductive Graph Neural Networks for Moving Object Segmentation. (arXiv:2305.09585v1 [cs.CV])

    [http://arxiv.org/abs/2305.09585](http://arxiv.org/abs/2305.09585)

    本文提出了一种基于图神经网络结构的图归纳移动物体分割算法(GraphIMOS)，可以对新增数据帧进行预测，将基于图的MOS模型在实际应用中部署成为可能。

    

    移动物体分割(MOS)是计算机视觉中具有挑战性的问题，特别是在具有动态背景、突变光照、阴影、伪装和移动相机的场景中。本文提出了一种基于图神经网络(GNN)结构的新型图归纳移动物体分割(GraphIMOS)算法。我们的方法建立了一个通用模型，能够对新增数据帧进行预测，使用已经训练好的模型。GraphIMOS 超越了以前的归纳学习方法，并且比以前的传导技术更加通用。我们的算法能够使基于图的 MOS 模型在实际应用中部署。

    Moving Object Segmentation (MOS) is a challenging problem in computer vision, particularly in scenarios with dynamic backgrounds, abrupt lighting changes, shadows, camouflage, and moving cameras. While graph-based methods have shown promising results in MOS, they have mainly relied on transductive learning which assumes access to the entire training and testing data for evaluation. However, this assumption is not realistic in real-world applications where the system needs to handle new data during deployment. In this paper, we propose a novel Graph Inductive Moving Object Segmentation (GraphIMOS) algorithm based on a Graph Neural Network (GNN) architecture. Our approach builds a generic model capable of performing prediction on newly added data frames using the already trained model. GraphIMOS outperforms previous inductive learning methods and is more generic than previous transductive techniques. Our proposed algorithm enables the deployment of graph-based MOS models in real-world ap
    
[^22]: 私有的永久预测

    Private Everlasting Prediction. (arXiv:2305.09579v1 [cs.LG])

    [http://arxiv.org/abs/2305.09579](http://arxiv.org/abs/2305.09579)

    本文提出了私有的预测模型，旨在保护训练集的隐私，探讨了提高隐私保护所需的样本复杂度，成功实现了两种不同的预测模式的私有预测器的样本效率和隐私性。

    

    本文研究了一个私有的预测模型，通过回答一系列分类问题，提供新样本的标签预测，旨在保护训练集的隐私。我们探讨了提高隐私保护所需的样本复杂度。本文研究了实现两种不同的预测模式的私有预测器的样本效率和隐私性，并在具体数据集上展示了其有效性。

    A private learner is trained on a sample of labeled points and generates a hypothesis that can be used for predicting the labels of newly sampled points while protecting the privacy of the training set [Kasiviswannathan et al., FOCS 2008]. Research uncovered that private learners may need to exhibit significantly higher sample complexity than non-private learners as is the case with, e.g., learning of one-dimensional threshold functions [Bun et al., FOCS 2015, Alon et al., STOC 2019].  We explore prediction as an alternative to learning. Instead of putting forward a hypothesis, a predictor answers a stream of classification queries. Earlier work has considered a private prediction model with just a single classification query [Dwork and Feldman, COLT 2018]. We observe that when answering a stream of queries, a predictor must modify the hypothesis it uses over time, and, furthermore, that it must use the queries for this modification, hence introducing potential privacy risks with respe
    
[^23]: 基于置换检验的因果图假设验证方法

    Toward Falsifying Causal Graphs Using a Permutation-Based Test. (arXiv:2305.09565v1 [stat.ML])

    [http://arxiv.org/abs/2305.09565](http://arxiv.org/abs/2305.09565)

    本文提出了一种通过构建节点置换基线的新型一致性度量方法，用于验证因果图的正确性并指导其在下游任务中的应用。

    

    理解系统变量之间的因果关系对于解释和控制其行为至关重要。但是，从观察数据中推断因果图需要很多不总是现实的强假设。对于领域专家来说，很难表达因果图。因此，在将因果图用于下游任务之前，定量评估因果图的优劣的度量提供了有用的检查。现有的度量提供了一个绝对数量的因果图与观察数据之间的不一致性，而没有基础线，从业人员需要回答有多少这样的不一致性是可接受或预期的这一难题。在这里，我们提出了一种新的一致性度量方法，通过构建节点置换的替代基线。通过将不一致性的数量与替代基线上的数量进行比较，我们得出了一个可以解释的度量，捕捉有向无环图是否显著适合。

    Understanding the causal relationships among the variables of a system is paramount to explain and control its behaviour. Inferring the causal graph from observational data without interventions, however, requires a lot of strong assumptions that are not always realistic. Even for domain experts it can be challenging to express the causal graph. Therefore, metrics that quantitatively assess the goodness of a causal graph provide helpful checks before using it in downstream tasks. Existing metrics provide an absolute number of inconsistencies between the graph and the observed data, and without a baseline, practitioners are left to answer the hard question of how many such inconsistencies are acceptable or expected. Here, we propose a novel consistency metric by constructing a surrogate baseline through node permutations. By comparing the number of inconsistencies with those on the surrogate baseline, we derive an interpretable metric that captures whether the DAG fits significantly bet
    
[^24]: 大数据学习：精选包与随机包的对比研究

    Learning from Aggregated Data: Curated Bags versus Random Bags. (arXiv:2305.09557v1 [cs.LG])

    [http://arxiv.org/abs/2305.09557](http://arxiv.org/abs/2305.09557)

    本文研究了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包，对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。

    

    保护用户隐私是许多机器学习系统部署的一个主要关注点，这些系统收集来自各种群体的数据。为了应对这种问题，一种方法是以聚合的形式收集和发布数据标签，从而可以将单个用户的信息与其他用户的信息组合起来。本文探讨了使用聚合数据标签而非单个标签来训练机器学习模型的可能性，具体来说，我们考虑了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包。对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。我们的方法基于以下观察：损失函数的梯度之和可以表示为每个包的梯度的加权和，其中权重是包的大小。

    Protecting user privacy is a major concern for many machine learning systems that are deployed at scale and collect from a diverse set of population. One way to address this concern is by collecting and releasing data labels in an aggregated manner so that the information about a single user is potentially combined with others. In this paper, we explore the possibility of training machine learning models with aggregated data labels, rather than individual labels. Specifically, we consider two natural aggregation procedures suggested by practitioners: curated bags where the data points are grouped based on common features and random bags where the data points are grouped randomly in bag of similar sizes. For the curated bag setting and for a broad range of loss functions, we show that we can perform gradient-based learning without any degradation in performance that may result from aggregating data. Our method is based on the observation that the sum of the gradients of the loss functio
    
[^25]: 基于混合注意力的EEG睡眠分期

    EEG-based Sleep Staging with Hybrid Attention. (arXiv:2305.09543v1 [eess.SP])

    [http://arxiv.org/abs/2305.09543](http://arxiv.org/abs/2305.09543)

    本文提出了一种基于混合注意力EEG睡眠分期（HASS）框架，采用时空注意力机制自适应地分配给予通道间和通道内的EEG片段权重，显著提高典型睡眠分期网络的性能。

    

    睡眠分期对于评估睡眠质量和诊断睡眠障碍至关重要。然而，在不同睡眠阶段期间，捕捉脑电图（EEG）信号内的空间和时间关系仍然具有挑战性。在本文中，我们提出了一种新的框架，称为混合注意力EEG睡眠分期（HASS）框架。具体而言，我们提出了一个精心设计的时空注意力机制，根据不同睡眠阶段期间的大脑空间-时间关系自适应地分配给予通道间和通道内的EEG片段权重。在MASS和ISRUC数据集上进行的实验结果表明，HASS可以显著提高典型睡眠分期网络的性能。我们提出的框架缓解了在睡眠分期期间捕捉EEG信号时的空间-时间关系的困难，并有望提高临床和研究环境中睡眠评估的准确性和可靠性。

    Sleep staging is critical for assessing sleep quality and diagnosing sleep disorders. However, capturing both the spatial and temporal relationships within electroencephalogram (EEG) signals during different sleep stages remains challenging. In this paper, we propose a novel framework called the Hybrid Attention EEG Sleep Staging (HASS) Framework. Specifically, we propose a well-designed spatio-temporal attention mechanism to adaptively assign weights to inter-channels and intra-channel EEG segments based on the spatio-temporal relationship of the brain during different sleep stages. Experiment results on the MASS and ISRUC datasets demonstrate that HASS can significantly improve typical sleep staging networks. Our proposed framework alleviates the difficulties of capturing the spatial-temporal relationship of EEG signals during sleep staging and holds promise for improving the accuracy and reliability of sleep assessment in both clinical and research settings.
    
[^26]: 估计条件Shapley值的方法比较及其应用场景的研究

    A Comparative Study of Methods for Estimating Conditional Shapley Values and When to Use Them. (arXiv:2305.09536v1 [stat.ML])

    [http://arxiv.org/abs/2305.09536](http://arxiv.org/abs/2305.09536)

    本文研究了估计条件Shapley值的方法和应用场景，提出了新方法，扩展了之前的方法，并将这些方法分类，通过模拟研究评估了各类方法的精度和可靠性。

    

    Shapley值最早起源于合作博弈理论，但现在已经广泛应用于机器学习领域的模型无关解释框架中，用来解释复杂模型所做的预测。本文聚焦于预测模型的条件Shapley值的计算，探讨了不同的算法途径与应用场景，这些计算需要估计复杂的条件期望。文章提出了新的方法，扩展了之前提出的方法，并将这些方法分类、比较和评估。分类方式采用蒙特卡罗积分或回归对条件期望进行建模。作者通过广泛的模拟研究来衡量不同方法分类估计条件期望的精度和可靠性。

    Shapley values originated in cooperative game theory but are extensively used today as a model-agnostic explanation framework to explain predictions made by complex machine learning models in the industry and academia. There are several algorithmic approaches for computing different versions of Shapley value explanations. Here, we focus on conditional Shapley values for predictive models fitted to tabular data. Estimating precise conditional Shapley values is difficult as they require the estimation of non-trivial conditional expectations. In this article, we develop new methods, extend earlier proposed approaches, and systematize the new refined and existing methods into different method classes for comparison and evaluation. The method classes use either Monte Carlo integration or regression to model the conditional expectations. We conduct extensive simulation studies to evaluate how precisely the different method classes estimate the conditional expectations, and thereby the condit
    
[^27]: 实时同时多物体三维形状重建，6DoF姿态估计和密集抓取预测。

    Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction. (arXiv:2305.09510v1 [cs.RO])

    [http://arxiv.org/abs/2305.09510](http://arxiv.org/abs/2305.09510)

    该论文提出了一种实现实时同时多物体三维形状重建、6DoF姿态估计和密集抓取预测的新方法，无需顺序感知和抓取规划步骤，具有快速推理且具有竞争力的性能。

    

    在复杂环境中操作的机器人操作系统依赖于感知系统，该系统提供有关场景中对象的几何（姿态和三维形状）以及其他语义信息（例如对象标签）的信息。然后利用这些信息选择相关对象上的可行抓取。在本文中，我们提出了一种新方法，可以同时提供场景中所有对象的几何和语义信息以及这些对象上的可行抓取。我们方法的主要优点在于其速度，因为它避免了顺序感知和抓取规划步骤。通过详细的定量分析，我们展示了我们的方法提供与面向对象形状、姿态和抓取预测的最新专用方法相比具有竞争力的性能，同时提供每秒30帧的快速推理。

    Robotic manipulation systems operating in complex environments rely on perception systems that provide information about the geometry (pose and 3D shape) of the objects in the scene along with other semantic information such as object labels. This information is then used for choosing the feasible grasps on relevant objects. In this paper, we present a novel method to provide this geometric and semantic information of all objects in the scene as well as feasible grasps on those objects simultaneously. The main advantage of our method is its speed as it avoids sequential perception and grasp planning steps. With detailed quantitative analysis, we show that our method delivers competitive performance compared to the state-of-the-art dedicated methods for object shape, pose, and grasp predictions while providing fast inference at 30 frames per second speed.
    
[^28]: 卷积神经网络中的内容自适应下采样

    Content-Adaptive Downsampling in Convolutional Neural Networks. (arXiv:2305.09504v1 [cs.CV])

    [http://arxiv.org/abs/2305.09504](http://arxiv.org/abs/2305.09504)

    本研究提出了一种内容自适应下采样的方法，通过允许基于感知重要性以不同的分辨率处理不同输入图像的区域，提高了神经网络在密集预测任务中的准确性，且计算成本和网络大小开销最小。

    

    许多卷积神经网络（CNN）依赖于逐步降采样其特征映射来增加网络的感受野并降低计算成本。然而，这是以在特征映射中失去粒度的代价为代价的，这限制了正确理解图像或在密集预测任务中恢复细节。为了解决这个问题，通常的做法是用膨胀卷积替换CNN中的最后几个下采样操作，从而在不减少接受域的情况下保留特征地图的分辨率，尽管增加了计算成本。这允许根据输出特征分辨率在预测性能和成本之间进行权衡。现有的工作通过定期下采样或不下采样整个特征映射，隐含地将输入图像和随后的特征映射的所有区域视为同等重要，这在一般情况下并不成立。我们提出了一种自适应下采样方案，通过允许基于其感知重要性，以不同的分辨率处理输入图像的不同区域来概括上述思想。为此，我们引入了一个可学习的门控机制，自适应地确定每个区域是否下采样以及下采样的程度。我们证明了我们的方法在密集预测任务中可以提高准确性，且计算成本和网络大小开销最小。

    Many convolutional neural networks (CNNs) rely on progressive downsampling of their feature maps to increase the network's receptive field and decrease computational cost. However, this comes at the price of losing granularity in the feature maps, limiting the ability to correctly understand images or recover fine detail in dense prediction tasks. To address this, common practice is to replace the last few downsampling operations in a CNN with dilated convolutions, allowing to retain the feature map resolution without reducing the receptive field, albeit increasing the computational cost. This allows to trade off predictive performance against cost, depending on the output feature resolution. By either regularly downsampling or not downsampling the entire feature map, existing work implicitly treats all regions of the input image and subsequent feature maps as equally important, which generally does not hold. We propose an adaptive downsampling scheme that generalizes the above idea by
    
[^29]: 对比标签增强

    Contrastive Label Enhancement. (arXiv:2305.09500v1 [cs.LG])

    [http://arxiv.org/abs/2305.09500](http://arxiv.org/abs/2305.09500)

    Contrastive Label Enhancement是一种新颖的方法，将特征和逻辑标签集成到统一的投影空间中，以生成高级特征。通过对比学习策略，相同样本的特征和逻辑标签被拉近，不同样本的被投影到更远。

    

    标签分布学习(Label distribution learning, LDL)是解决标签模糊性的新的机器学习范例。由于直接获取标签分布很困难，因此许多研究都集中在如何从逻辑标签中恢复标签分布，被称为标签增强(LE)。现有的LE方法通过在逻辑标签的监督下构建特征与标签分布之间的映射关系来估计标签分布。它们通常忽略了特征和逻辑标签都是从不同视角描述实例的事实。因此，我们提出了一种新的方法，称为对比标签增强(ConLE)，它通过对比学习策略将特征和逻辑标签集成到统一的投影空间中，以生成高级特征。在这种方法中，属于同一样本的特征和逻辑标签被拉近，而不同样本的被投影到投影空间中更远的位置。

    Label distribution learning (LDL) is a new machine learning paradigm for solving label ambiguity. Since it is difficult to directly obtain label distributions, many studies are focusing on how to recover label distributions from logical labels, dubbed label enhancement (LE). Existing LE methods estimate label distributions by simply building a mapping relationship between features and label distributions under the supervision of logical labels. They typically overlook the fact that both features and logical labels are descriptions of the instance from different views. Therefore, we propose a novel method called Contrastive Label Enhancement (ConLE) which integrates features and logical labels into the unified projection space to generate high-level features by contrastive learning strategy. In this approach, features and logical labels belonging to the same sample are pulled closer, while those of different samples are projected farther away from each other in the projection space. Sub
    
[^30]: 基于硬件实现非线性激活函数的神经网络光学均衡器

    Hardware Realization of Nonlinear Activation Functions for NN-based Optical Equalizers. (arXiv:2305.09495v1 [cs.LG])

    [http://arxiv.org/abs/2305.09495](http://arxiv.org/abs/2305.09495)

    本论文证明了在基于硬件实现神经网络光学均衡器时，使用近似激活函数的 biLSTM 均衡器能够取得接近于原模型的性能。

    

    为了降低基于神经网络的光学信道均衡器的硬件实现复杂度，我们证明了使用近似激活函数的 biLSTM 均衡器的性能接近于原模型。

    To reduce the complexity of the hardware implementation of neural network-based optical channel equalizers, we demonstrate that the performance of the biLSTM equalizer with approximated activation functions is close to that of the original model.
    
[^31]: 空间天气研究中的太阳活动区磁场图像数据集

    Solar Active Region Magnetogram Image Dataset for Studies of Space Weather. (arXiv:2305.09492v1 [astro-ph.SR])

    [http://arxiv.org/abs/2305.09492](http://arxiv.org/abs/2305.09492)

    本数据集提供了一系列太阳活动区磁图，并提供相应的太阳耀斑标签。它可用于研究磁结构、其演化以及太阳耀斑的关系，并对于自动太阳耀斑预测方法的研究具有重要价值。

    

    本数据集提供了美国国家航空航天局（NASA）太阳动力学观测卫星（SDO）的磁图（衡量磁场强度的图像）的全面收集。该数据集包含来自三个来源的数据，提供SDO地震学和磁学仪（HMI）太阳活跃区（大磁通区域，通常是爆发事件的源头）的磁图以及相应耀斑活动的标签。该数据集对于研究磁结构、其随时间的演化以及与太阳耀斑的关系的图像分析或太阳物理学研究将非常有用。该数据集将对那些研究自动太阳耀斑预测方法的研究人员产生兴趣，包括监督和无监督的机器学习（经典和深度）、二元和多类分类以及回归。该数据集是一个最小处理且用户可配置的一致大小太阳图像数据集。

    In this dataset we provide a comprehensive collection of magnetograms (images quantifying the strength of the magnetic field) from the National Aeronautics and Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The dataset incorporates data from three sources and provides SDO Helioseismic and Magnetic Imager (HMI) magnetograms of solar active regions (regions of large magnetic flux, generally the source of eruptive events) as well as labels of corresponding flaring activity. This dataset will be useful for image analysis or solar physics research related to magnetic structure, its evolution over time, and its relation to solar flares. The dataset will be of interest to those researchers investigating automated solar flare prediction methods, including supervised and unsupervised machine learning (classical and deep), binary and multi-class classification, and regression. This dataset is a minimally processed, user configurable dataset of consistently sized images of sola
    
[^32]: 行政人员的发声笑声与社会认可：机器学习研究的探究。 (arXiv:2305.09485v1 [经济学.GN])

    Executive Voiced Laughter and Social Approval: An Explorative Machine Learning Study. (arXiv:2305.09485v1 [econ.GN])

    [http://arxiv.org/abs/2305.09485](http://arxiv.org/abs/2305.09485)

    本文探究了行政沟通中的发声笑声对于社会认可的积极影响，特别是当发生双向笑声时。结果表明，这种影响随着组织业绩的下降而增加。

    

    我们研究了行政人员沟通中的发声笑声以及它对社会认可的影响。我们将笑声，情感作为信息和信息媒介对公司的社会评价的研究相结合，假设行政人员沟通中的发声笑声对社会认可有积极影响，社会认可是指受众对一个组织的亲和力的感知。我们认为，与众笑的效果尤其强，即在给定的沟通场合中，聚焦的行政人员和观众同时发笑的次数。最后，结合情感作为信息和人类认知的负面偏见，我们假设笑声对社会认可的积极影响随着组织业绩的下降而增加。我们在902个德国巴林德斯利加足球新闻发布会和媒体十大数据中进行测试，应用最先进的机器学习方法进行笑声检测，并找到了部分支持我们想法的结果。

    We study voiced laughter in executive communication and its effect on social approval. Integrating research on laughter, affect-as-information, and infomediaries' social evaluations of firms, we hypothesize that voiced laughter in executive communication positively affects social approval, defined as audience perceptions of affinity towards an organization. We surmise that the effect of laughter is especially strong for joint laughter, i.e., the number of instances in a given communication venue for which the focal executive and the audience laugh simultaneously. Finally, combining the notions of affect-as-information and negativity bias in human cognition, we hypothesize that the positive effect of laughter on social approval increases with bad organizational performance. We find partial support for our ideas when testing them on panel data comprising 902 German Bundesliga soccer press conferences and media tenor, applying state-of-the-art machine learning approaches for laughter dete
    
[^33]: “身行相随”——基于机器学习和触摸动态的连续用户认证研究

    Your Identity is Your Behavior -- Continuous User Authentication based on Machine Learning and Touch Dynamics. (arXiv:2305.09482v1 [cs.CR])

    [http://arxiv.org/abs/2305.09482](http://arxiv.org/abs/2305.09482)

    本研究使用移动触摸动态进行连续认证的可行性进行了实验，使用神经网络、极端梯度提升和支持向量机三种不同的算法进行研究，并采集了40个受试者的触摸动态数据集，通过持续验证用户身份提高安全性。

    

    本研究旨在探究使用移动触摸动态进行连续认证的可行性，使用神经网络、极端梯度提升和支持向量机三种不同的算法进行实验。移动设备在世界范围内的普及率不断提升，目前智能手机的订阅量已经超过了60亿。移动触摸动态是指用户与移动设备交互的特定模式，包括触摸的压力、滑动速度和触摸的持续时间等因素。连续认证是指在用户使用设备时持续验证其身份，而不仅仅是在初始登录时进行身份验证。本研究使用LG V30+收集了40个受试者的触摸动态数据集。参与者各玩了PUBG、Diep.io、Slither和Minecraft等四款手机游戏，每个游戏玩10分钟。使用提取的数据集对三种算法进行了训练和测试，并评估了它们的表现。

    The aim of this research paper is to look into the use of continuous authentication with mobile touch dynamics, using three different algorithms: Neural Network, Extreme Gradient Boosting, and Support Vector Machine. Mobile devices are constantly increasing in popularity in the world, today smartphone subscriptions have surpassed 6 billion. Mobile touch dynamics refer to the distinct patterns of how a user interacts with their mobile device, this includes factors such as touch pressure, swipe speed, and touch duration. Continuous authentication refers to the process of continuously verifying a user's identity while they are using a device, rather than just at the initial login. This research used a dataset of touch dynamics collected from 40 subjects using the LG V30+. The participants played four mobile games, PUBG, Diep.io, Slither, and Minecraft, for 10 minutes each game. The three algorithms were trained and tested on the extracted dataset, and their performance was evaluated based
    
[^34]: 上下文丰富的分子表示提高了少样本药物发现的准确性

    Context-enriched molecule representations improve few-shot drug discovery. (arXiv:2305.09481v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09481](http://arxiv.org/abs/2305.09481)

    上下文丰富的分子表示方法能够以更少的标记数据提高药物发现效率

    

    计算机辅助药物发现中的一个核心任务是采用已知的活性分子构建模型，以发现需要进一步筛选的有潜力分子。然而，通常只有非常少数的活性分子是已知的。因此，少样本学习方法有可能提高药物发现过程中这个关键阶段的效率。我们提出了一种新的少样本药物发现方法。其主要思想是通过已知的上下文或参考分子来丰富分子表示。我们的分子表示丰富新概念是通过现代 Hopfield 网络将支持集和查询集中的分子与大量的参考（上下文）分子相关联。直观地说，这个丰富步骤类似于人类专家将一个给定的分子与熟悉的其属性已知的分子相关联。这个丰富步骤增强并放大了数据的协方差结构，同时消除了大量标记数据的需求。我们在标准的少样本药物发现基准测试上对我们的方法进行了评估，并展示它优于现有的方法。

    A central task in computational drug discovery is to construct models from known active molecules to find further promising molecules for subsequent screening. However, typically only very few active molecules are known. Therefore, few-shot learning methods have the potential to improve the effectiveness of this critical phase of the drug discovery process. We introduce a new method for few-shot drug discovery. Its main idea is to enrich a molecule representation by knowledge about known context or reference molecules. Our novel concept for molecule representation enrichment is to associate molecules from both the support set and the query set with a large set of reference (context) molecules through a Modern Hopfield Network. Intuitively, this enrichment step is analogous to a human expert who would associate a given molecule with familiar molecules whose properties are known. The enrichment step reinforces and amplifies the covariance structure of the data, while simultaneously remov
    
[^35]: 交叉门控多层感知机下的蛋白质复合物不变嵌入是一种一次性抗体设计器

    Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09480](http://arxiv.org/abs/2305.09480)

    本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。

    

    抗体是由免疫系统产生的针对外来物质或抗原的重要蛋白质。抗体的特异性由其互补决定区（CDR）决定，CDR位于抗体链的可变区域中，形成与抗原结合的位点。以往的研究利用复杂的技术生成CDR，但它们遭受了几何建模不足的问题。此外，常见的迭代精化策略导致了低效的推断。本文提出了一种深度生成模型，可以一次性地共同设计CDR的1D序列和3D结构。为了实现这一目标，我们将抗体CDR设计分为两个阶段：（i）蛋白质结构的几何建模和（ii）序列结构共学习。我们开发了一种蛋白质复合物不变嵌入，可捕捉蛋白质骨架原子（包括Cα、N、C和O原子）之间的内部和外部组分相互作用，以实现全面的几何建模。

    Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
    
[^36]: 时间延迟多特征相关分析以提取脑电信号中微小依赖性

    Time delay multi-feature correlation analysis to extract subtle dependencies from EEG signals. (arXiv:2305.09478v1 [eess.SP])

    [http://arxiv.org/abs/2305.09478](http://arxiv.org/abs/2305.09478)

    本文针对脑电信号的复杂性，提出使用多特征相关分析方法来自动分解和提取多种类型的统计依赖关系。其中，PCA降维技术用于找到占主导的依赖关系方向，从而提取脑电信号中微小依赖性。

    

    脑电图（EEG）信号是极其复杂的脑活动的结果。通过例如一对电极信号在不同时间延迟（滞后$\Delta$t）下的联合分布（$\rho_{\Delta t}$）可以访问这种隐藏动态的某些细节。标准方法是监视这样的联合分布的单一评估，例如Pearson相关（或互信息），然而这种方法结果通常相对不太有趣。相比之下，这样的复杂信号可能由多种类型的统计依赖关系构成。本文提出了一种自动分解和提取这些依赖关系的方法。 具体来说，我们将这样的联合分布建模为所有考虑的滞后依赖关系的多项式估计，然后通过PCA降维找到占主导的依赖关系方向$f_v$。这样我们得到一些滞后依赖特征$a_i(\Delta t)$，用于描述各个滞后的统计依赖关系及其重要性。

    Electroencephalography (EEG) signals are resultants of extremely complex brain activity. Some details of this hidden dynamics might be accessible through e.g. joint distributions $\rho_{\Delta t}$ of signals of pairs of electrodes shifted by various time delays (lag $\Delta t$). A standard approach is monitoring a single evaluation of such joint distributions, like Pearson correlation (or mutual information), which turns out relatively uninteresting as expected, there is usually a small peak for zero delay and nearly symmetric drop with delay. In contrast, such a complex signal might be composed of multiple types of statistical dependencies - this article proposes approach to automatically decompose and extract them. Specifically, we model such joint distributions as polynomials estimated for all considered lag dependencies, then with PCA dimensionality reduction find dominant dependency directions $f_v$. This way we get a few lag dependent features $a_i(\Delta t)$ describing separat
    
[^37]: ANALYSE -- 学习使用智能代理攻击物理网络能源系统

    ANALYSE -- Learning to Attack Cyber-Physical Energy Systems With Intelligent Agents. (arXiv:2305.09476v1 [cs.CR])

    [http://arxiv.org/abs/2305.09476](http://arxiv.org/abs/2305.09476)

    提出了基于机器学习的软件套件ANALYSE，让学习代理能够自主地在物理网络能源系统中发现攻击。这是一个旨在找到未知攻击类型的自我记录框架。

    

    信息通信技术的普及和市场的引入使得恶意攻击和以利益为驱动的攻击对物理网络能源系统造成了威胁。为确保供应安全，有必要分析这些攻击及其潜在漏洞，制定对策并改进系统设计。我们提出了基于机器学习的软件套件ANALYSE，让学习代理能够自主地在包括电力系统、信息通信技术和能源市场在内的物理网络能源系统中发现攻击。ANALYSE 是一个模块化、可配置和自我记录的框架，旨在找到尚未知道的攻击类型，并在从科学文献中复现许多已知的攻击策略。

    The ongoing penetration of energy systems with information and communications technology (ICT) and the introduction of new markets increase the potential for malicious or profit-driven attacks that endanger system stability. To ensure security-of-supply, it is necessary to analyze such attacks and their underlying vulnerabilities, to develop countermeasures and improve system design. We propose ANALYSE, a machine-learning-based software suite to let learning agents autonomously find attacks in cyber-physical energy systems, consisting of the power system, ICT, and energy markets. ANALYSE is a modular, configurable, and self-documenting framework designed to find yet unknown attack types and to reproduce many known attack strategies in cyber-physical energy systems from the scientific literature.
    
[^38]: 基于重构的LSTM自编码器用于多元时间序列数据的异常DDoS攻击检测

    Reconstruction-based LSTM-Autoencoder for Anomaly-based DDoS Attack Detection over Multivariate Time-Series Data. (arXiv:2305.09475v1 [cs.CR])

    [http://arxiv.org/abs/2305.09475](http://arxiv.org/abs/2305.09475)

    该论文提出基于重构的LSTM-Autoencoder（LSTM-AE）模型结合了深度学习模型来检测未见过的DDoS攻击异常，该模型通过自编码器来识别最佳阈值。

    

    分布式拒绝服务（DDoS）攻击是一种恶意企图通过发送海量流量来淹没目标或其周围基础架构的服务器、服务或网络的常规流量的攻击。传统的统计和浅层机器学习技术可以基于浅层数据和特征选择检测表面异常，但这些方法无法检测到未见过的DDoS攻击。在这个背景下，我们提出了一个名为LSTM-Autoencoder（LSTM-AE）的基于重构的异常检测模型，该模型结合了两个基于深度学习的模型来检测DDoS攻击异常。长短期记忆（LSTM）网络的提出结构提供了单元，这些单元彼此协作来学习时间序列序列内数据的长期短期相关性。自编码器用于基于在每个样本上评估的重构误差率来识别最佳阈值。

    A Distributed Denial-of-service (DDoS) attack is a malicious attempt to disrupt the regular traffic of a targeted server, service, or network by sending a flood of traffic to overwhelm the target or its surrounding infrastructure. As technology improves, new attacks have been developed by hackers. Traditional statistical and shallow machine learning techniques can detect superficial anomalies based on shallow data and feature selection, however, these approaches cannot detect unseen DDoS attacks. In this context, we propose a reconstruction-based anomaly detection model named LSTM-Autoencoder (LSTM-AE) which combines two deep learning-based models for detecting DDoS attack anomalies. The proposed structure of long short-term memory (LSTM) networks provides units that work with each other to learn the long short-term correlation of data within a time series sequence. Autoencoders are used to identify the optimal threshold based on the reconstruction error rates evaluated on each sample 
    
[^39]: 基于图形的深度学习用于海表温度预测

    Graph-Based Deep Learning for Sea Surface Temperature Forecasts. (arXiv:2305.09468v1 [physics.ao-ph])

    [http://arxiv.org/abs/2305.09468](http://arxiv.org/abs/2305.09468)

    该论文探索了使用图形重新采样和图形神经网络进行全球海表温度预测的方法，相比持久性模型，GNN 在大多数海洋中显示出更好的提前一个月的 SST 预测结果。

    

    海表温度 (SST) 预测有助于管理受人类气候变化影响的海洋生态系统和水产养殖。数值动力模型对于 SST 预测具有资源密集型；机器学习 (ML) 模型可以减少高计算要求，并且最近一直是研究界关注的焦点。ML 模型通常需要大量数据进行训练。环境数据是在定期间隔的网格上收集的，因此早期的工作主要使用基于网格的深度学习 (DL) 进行预测。然而，网格数据和相应的 DL 方法都存在固有问题。随着几何深度学习的出现，将图形作为更广义的数据结构和图形神经网络 (GNNs) 引入到时空域中。在这项工作中，我们初步探讨了图形重新采样和 GNN 用于全球 SST 预测，而 GNN 在大多数海洋中相对于持久性模型在 RMSE 和 MAE 方面显示出更好的提前一个月的 SST 预测结果。

    Sea surface temperature (SST) forecasts help with managing the marine ecosystem and the aquaculture impacted by anthropogenic climate change. Numerical dynamical models are resource intensive for SST forecasts; machine learning (ML) models could reduce high computational requirements and have been in the focus of the research community recently. ML models normally require a large amount of data for training. Environmental data are collected on regularly-spaced grids, so early work mainly used grid-based deep learning (DL) for prediction. However, both grid data and the corresponding DL approaches have inherent problems. As geometric DL has emerged, graphs as a more generalized data structure and graph neural networks (GNNs) have been introduced to the spatiotemporal domains. In this work, we preliminarily explored graph re-sampling and GNNs for global SST forecasts, and GNNs show better one month ahead SST prediction than the persistence model in most oceans in terms of root mean squar
    
[^40]: 谷歌研究足球多智能体场景的实证研究

    An Empirical Study on Google Research Football Multi-agent Scenarios. (arXiv:2305.09458v1 [cs.LG])

    [http://arxiv.org/abs/2305.09458](http://arxiv.org/abs/2305.09458)

    本研究提供了一种基于人口的MARL训练管线以及超参数，用于训练多智能体足球场景，从零开始在200万步内打败了难度为1.0的机器人，并提供了开源训练框架Light-MALib。

    

    目前，研究谷歌研究足球（GRF）上的11v11多智能体（MARL）场景仍然是一个较少被关注的课题。本研究提供了一种基于人口的MARL训练管线以及超参数设置的方法，用于训练多智能体足球场景，从零开始在200万步内就能优于难度为1.0的机器人。此外，本研究还开源了训练框架Light-MALib，扩展了MALib代码库，通过分布式和异步实现以及足球游戏的附加分析工具，提供了指导机器人较好表现的方法。

    Few multi-agent reinforcement learning (MARL) research on Google Research Football (GRF) focus on the 11v11 multi-agent full-game scenario and to the best of our knowledge, no open benchmark on this scenario has been released to the public. In this work, we fill the gap by providing a population-based MARL training pipeline and hyperparameter settings on multi-agent football scenario that outperforms the bot with difficulty 1.0 from scratch within 2 million steps. Our experiments serve as a reference for the expected performance of Independent Proximal Policy Optimization (IPPO), a state-of-the-art multi-agent reinforcement learning algorithm where each agent tries to maximize its own policy independently across various training configurations. Meanwhile, we open-source our training framework Light-MALib which extends the MALib codebase by distributed and asynchronized implementation with additional analytical tools for football games. Finally, we provide guidance for building strong f
    
[^41]: 重新思考生成对抗网络的编辑: 一种基于降维的编辑向量估计方法

    Rethinking the editing of generative adversarial networks: a method to estimate editing vectors based on dimension reduction. (arXiv:2305.09454v1 [cs.CV])

    [http://arxiv.org/abs/2305.09454](http://arxiv.org/abs/2305.09454)

    该论文提出了一种基于降维的编辑向量估计方法，用于高质量、高精度的语义图像编辑，不需要语义分割或可微分的特征估计网络。

    

    近年来，生成对抗网络（GANs）在图像编辑方面被广泛应用。然而，先前的GAN-based图像编辑方法大多需要大规模数据集的语义分割注释来进行训练，只能提供高级别控制或仅在不同图像之间进行插值。之前的研究者通过查找“编辑向量”提出了EditGAN，以获得高质量、高精度的语义图像编辑，而无需进行大量的语义注释。然而，存在许多与语义关联不高的特征，导致EditGAN可能在这些特征上失败。本文基于EditGAN所观察到的潜空间正交性，提出了一种估计编辑向量的方法，该法不依赖于语义分割或可微分的特征估计网络。本方法假设特征的强度分布与隐藏向量的分布之间存在相关性，并通过采样来估计上述分布之间的关系。

    While Generative Adversarial Networks (GANs) have recently found applications in image editing, most previous GAN-based image editing methods require largescale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Previous researchers have proposed EditGAN for high-quality, high-precision semantic image editing with limited semantic annotations by finding `editing vectors'. However, it is noticed that there are many features that are not highly associated with semantics, and EditGAN may fail on them. Based on the orthogonality of latent space observed by EditGAN, we propose a method to estimate editing vectors that do not rely on semantic segmentation nor differentiable feature estimation network. Our method assumes that there is a correlation between the intensity distribution of features and the distribution of hidden vectors, and estimates the relationship between the above distributions by sam
    
[^42]: 概率距离法异常检测

    Probabilistic Distance-Based Outlier Detection. (arXiv:2305.09446v1 [cs.LG])

    [http://arxiv.org/abs/2305.09446](http://arxiv.org/abs/2305.09446)

    本文提出了一种将距离法异常检测分数转化为可解释的概率估计的通用方法，该方法使用与其他数据点的距离建模距离概率分布，将距离法异常检测分数转换为异常概率，提高了正常点和异常点之间的对比度，而不会影响检测性能。

    

    距离法异常检测方法的分数难以解释，因此在没有额外的上下文信息的情况下，很难确定正常点和异常点之间的截断阈值。我们描述了将距离法异常检测分数转化为可解释的概率估计的通用方法。该转换是排名稳定的，并增加了正常点和异常点之间的对比度。确定数据点之间的距离关系是识别数据中最近邻关系所必需的，然而大多数计算出的距离通常被丢弃。我们展示了可以使用与其他数据点的距离来建模距离概率分布，并随后使用这些分布将距离法异常检测分数转换为异常概率。我们的实验表明，概率转换不会影响众多表格和图像基准数据集上的检测性能，但会产生可解释性。

    The scores of distance-based outlier detection methods are difficult to interpret, making it challenging to determine a cut-off threshold between normal and outlier data points without additional context. We describe a generic transformation of distance-based outlier scores into interpretable, probabilistic estimates. The transformation is ranking-stable and increases the contrast between normal and outlier data points. Determining distance relationships between data points is necessary to identify the nearest-neighbor relationships in the data, yet, most of the computed distances are typically discarded. We show that the distances to other data points can be used to model distance probability distributions and, subsequently, use the distributions to turn distance-based outlier scores into outlier probabilities. Our experiments show that the probabilistic transformation does not impact detection performance over numerous tabular and image benchmark datasets but results in interpretable
    
[^43]: MPI-rical：基于Transformer的数据驱动MPI分布式并行辅助

    MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])

    [http://arxiv.org/abs/2305.09438](http://arxiv.org/abs/2305.09438)

    本文提出了一种基于Transformer模型的新方法MPI-rical，通过对大量代码片段进行训练实现自动化MPI代码生成，使并行化成为可能。

    

    在高性能计算中，将串行代码自动并行化以支持共享内存和分布式内存系统是一项具有挑战性的任务。虽然许多尝试将串行代码转换为共享内存环境的并行代码（通常使用OpenMP），但没有任何一项尝试成功将其转化为分布式内存环境。本文提出了一种称为MPI-rical的新方法，通过基于Transformer模型对大约25,000个串行代码片段及其对应的并行MPI代码进行训练，从我们的语料库（MPICodeCorpus）的50,000多个代码片段中生成自动化MPI代码。为了评估模型的性能，我们首先将串行代码转换为基于MPI的并行代码翻译问题分解为两个子问题，并制定两个研究目标：代码补全，即在给定源代码中的某个位置，预测该位置的MPI函数；代码翻译，即预测一个MPI函数。

    Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
    
[^44]: 结构健康监测问题何时成为多任务学习问题？

    When is an SHM problem a Multi-Task-Learning problem?. (arXiv:2305.09425v1 [cs.LG])

    [http://arxiv.org/abs/2305.09425](http://arxiv.org/abs/2305.09425)

    本文探讨了在结构健康监测领域中的多任务学习机制，包括自然多任务、输出作为输入和使用额外损失函数三种方法，并给出了相应例子。

    

    多任务神经网络同时学习多个任务以提高单个任务的性能。本文探讨在结构健康监测（SHM）领域中，多任务学习的三种机制: (i) 自然存在的多个任务；(ii) 将输出用作输入（与最新的基于群体的SHM（PBSHM）的研究相关）；以及 (iii) 使用额外的损失函数提供不同的见解。详细介绍了MTL的每个问题设置，并给出了一个示例。

    Multi-task neural networks learn tasks simultaneously to improve individual task performance. There are three mechanisms of multi-task learning (MTL) which are explored here for the context of structural health monitoring (SHM): (i) the natural occurrence of multiple tasks; (ii) using outputs as inputs (both linked to the recent research in population-based SHM (PBSHM)); and, (iii) additional loss functions to provide different insights. Each of these problem settings for MTL is detailed and an example is given.
    
[^45]: 解开所有ReLU网络的秘密

    Unwrapping All ReLU Networks. (arXiv:2305.09424v1 [cs.LG])

    [http://arxiv.org/abs/2305.09424](http://arxiv.org/abs/2305.09424)

    本文提出了一种解开ReLU网络的秘密的方法，通过将网络分解成线性模型，实现了对图神经网络和张量卷积网络等结构的扩展，并证明了神经网络可以理解为可解释模型。此外，还提供了计算SHAP值的方法。

    

    深度ReLU网络可以分解成一系列线性模型，每个模型在输入空间的一个区域内定义。本文提供了三个结果来扩展这个理论。首先，我们将这种线性分解扩展到了图神经网络和张量卷积网络，以及具有乘法交互的网络。其次，我们证明神经网络可以理解为可解释的模型，例如多元决策树和逻辑理论。最后，我们展示了如何使用这种模型来计算便宜且准确的SHAP值。我们通过图神经网络的实验验证了该理论的有效性。

    Deep ReLU Networks can be decomposed into a collection of linear models, each defined in a region of a partition of the input space. This paper provides three results extending this theory. First, we extend this linear decompositions to Graph Neural networks and tensor convolutional networks, as well as networks with multiplicative interactions. Second, we provide proofs that neural networks can be understood as interpretable models such as Multivariate Decision trees and logical theories. Finally, we show how this model leads to computing cheap and exact SHAP values. We validate the theory through experiments with on Graph Neural Networks.
    
[^46]: 使用SHAP特征重要性和模糊认知地图测量隐性偏见

    Measuring Implicit Bias Using SHAP Feature Importance and Fuzzy Cognitive Maps. (arXiv:2305.09399v1 [cs.LG])

    [http://arxiv.org/abs/2305.09399](http://arxiv.org/abs/2305.09399)

    本文使用SHAP特征重要性和模糊认知地图模型，对隐性偏见进行测量，结果表明特征重要性作为绝对工具不适应于测量隐性偏见，受保护特征的偏见数量可能因特征是数值编码还是分类编码而有所不同。

    

    本文将特征重要性概念与模式分类中的隐性偏见相结合，通过三步方法实现：（i）构建一个分类器和调整其超参数，（ii）构建一个能量化隐性偏见的模糊认知地图模型，（iii）使用SHAP特征重要性在模拟中激活神经元概念。以关于公平研究的实际案例研究结果支持我们的双重假设。一方面，阐明了使用特征重要性作为绝对工具来衡量隐性偏见的风险。另一方面，得出结论：对受保护特征的偏见数量可能因特征是数值编码还是分类编码而有所不同。

    In this paper, we integrate the concepts of feature importance with implicit bias in the context of pattern classification. This is done by means of a three-step methodology that involves (i) building a classifier and tuning its hyperparameters, (ii) building a Fuzzy Cognitive Map model able to quantify implicit bias, and (iii) using the SHAP feature importance to active the neural concepts when performing simulations. The results using a real case study concerning fairness research support our two-fold hypothesis. On the one hand, it is illustrated the risks of using a feature importance method as an absolute tool to measure implicit bias. On the other hand, it is concluded that the amount of bias towards protected features might differ depending on whether the features are numerically or categorically encoded.
    
[^47]: 局部支持向量机的$L_p$和风险一致性

    Lp- and Risk Consistency of Localized SVMs. (arXiv:2305.09385v1 [stat.ML])

    [http://arxiv.org/abs/2305.09385](http://arxiv.org/abs/2305.09385)

    本文分析了局部支持向量机的一致性，证明了在非常弱的条件下，它们从全局SVM继承了$L_p$和风险一致性，即使底层区域随数据集大小的增加而变化。

    

    基于核的正则化风险最小化器，又称为支持向量机（SVM），已知具有许多理想的属性，但在处理大型数据集时具有超线性的计算需求。可以通过使用局部SVM来解决这个问题，这种方法还提供了能够在不同的输入空间区域应用不同超参数的额外优势。本文分析了局部SVM的一致性。证明了它们在非常弱的情况下从全局SVM继承了$L_p$-以及风险-一致性，甚至可以在训练数据集大小增加时，允许底层的区域发生变化。

    Kernel-based regularized risk minimizers, also called support vector machines (SVMs), are known to possess many desirable properties but suffer from their super-linear computational requirements when dealing with large data sets. This problem can be tackled by using localized SVMs instead, which also offer the additional advantage of being able to apply different hyperparameters to different regions of the input space. In this paper, localized SVMs are analyzed with regards to their consistency. It is proven that they inherit $L_p$- as well as risk consistency from global SVMs under very weak conditions and even if the regions underlying the localized SVMs are allowed to change as the size of the training data set increases.
    
[^48]: 多任务卷积神经网络用于图像美学评估

    Multi-task convolutional neural network for image aesthetic assessment. (arXiv:2305.09373v1 [cs.CV])

    [http://arxiv.org/abs/2305.09373](http://arxiv.org/abs/2305.09373)

    本文提出了一种多任务卷积神经网络，可以同时预测图像的总体美学评分和美学属性，并在实验中表现优异，达到接近人类表现的整体美学评分。

    

    由于人们对图像美学偏好的理解还远远不够，图像美学评估是一项具有挑战性的人工智能任务。本文提出了一种多任务卷积神经网络，考虑了影响图像美学的因素。所提出的神经网络同时学习了图像的总体美学评分以及这些已知美学属性。这种多任务学习框架通过共享表示实现有效的泛化。实验结果证明了所提出的方法在预测图像美学的整体评分方面优于现有的方法。在考虑Spearman等级相关性时，我们实现了接近于人类表现的整体美学评分。此外，我们的模型在另一个基准测试中还通过预测图像的特定美学属性开创了多任务应用的先河。

    As people's aesthetic preferences for images are far from understood, image aesthetic assessment is a challenging artificial intelligence task. The range of factors underlying this task is almost unlimited, but we know that some aesthetic attributes affect those preferences. In this study, we present a multi-task convolutional neural network that takes into account these attributes. The proposed neural network jointly learns the attributes along with the overall aesthetic scores of images. This multi-task learning framework allows for effective generalization through the utilization of shared representations. Our experiments demonstrate that the proposed method outperforms the state-of-the-art approaches in predicting overall aesthetic scores for images in one benchmark of image aesthetics. We achieve near-human performance in terms of overall aesthetic scores when considering the Spearman's rank correlations. Moreover, our model pioneers the application of multi-tasking in another ben
    
[^49]: 佩戴运动传感器自动分类婴儿运动的自监督预训练评估

    Evaluation of self-supervised pre-training for automatic infant movement classification using wearable movement sensors. (arXiv:2305.09366v1 [cs.LG])

    [http://arxiv.org/abs/2305.09366](http://arxiv.org/abs/2305.09366)

    本研究评估了自监督预训练对婴儿运动分类分类器的性能提高作用，有助于提高自动分析的准确性和稳健性。

    

    最近开发的婴幼儿可穿戴设备MAIJU为足不出户的婴儿运动评估提供了一种客观和可扩展的手段。这些信息可以用于发展性研究，并支持临床决策。MAIJU的分析完全依赖于婴儿姿势和运动的分类；因此，研究增加分类准确性的方式是必要的，旨在增加自动分析的可靠性和稳健性。本文研究了自监督预训练如何提高对分析MAIJU记录所使用的分类器的性能，并研究了分类器模型的性能是否受到预训练数据的选择性筛选的影响，以排除婴儿运动较小或传感器缺失的时间段。我们的实验表明：i）预训练

    The recently-developed infant wearable MAIJU provides a means to automatically evaluate infants' motor performance in an objective and scalable manner in out-of-hospital settings. This information could be used for developmental research and to support clinical decision-making, such as detection of developmental problems and guiding of their therapeutic interventions. MAIJU-based analyses rely fully on the classification of infant's posture and movement; it is hence essential to study ways to increase the accuracy of such classifications, aiming to increase the reliability and robustness of the automated analysis. Here, we investigated how self-supervised pre-training improves performance of the classifiers used for analyzing MAIJU recordings, and we studied whether performance of the classifier models is affected by context-selective quality-screening of pre-training data to exclude periods of little infant movement or with missing sensors. Our experiments show that i) pre-training th
    
[^50]: 推荐系统中的用户公平性: 方法和评估的系统调查

    Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation. (arXiv:2305.09330v1 [cs.IR])

    [http://arxiv.org/abs/2305.09330](http://arxiv.org/abs/2305.09330)

    推荐系统的公平性问题越来越引起人们的关注，尤其是用户公平性，已经提出了很多解决方案来减轻用户在使用推荐系统过程中体验到的歧视问题。

    

    在数字化水平不断提高的当前社会中，面临着可扩展性方面的巨大挑战。推荐系统已经成为帮助用户导航日益增长的数据量，以及帮助供应商向感兴趣的用户营销产品的必不可少的工具。机器学习方法中的歧视问题日益突出，这促使学术界和工业界研究如何确保推荐系统的公平性。在推荐系统中，这些问题在职业推荐中得到了很好的体现，历史数据中的偏见可能导致推荐系统将一个性别与较低的工资或刻板印象联系起来。特别地，用户公平性关注如何减轻用户在使用推荐系统过程中体验到的歧视问题，该领域已经出现了很多不同的方法来解决不同类型的歧视。所述歧视的性质取决于所处的情境。

    In the current landscape of ever-increasing levels of digitalization, we are facing major challenges pertaining to scalability. Recommender systems have become irreplaceable both for helping users navigate the increasing amounts of data and, conversely, aiding providers in marketing products to interested users. The growing awareness of discrimination in machine learning methods has recently motivated both academia and industry to research how fairness can be ensured in recommender systems. For recommender systems, such issues are well exemplified by occupation recommendation, where biases in historical data may lead to recommender systems relating one gender to lower wages or to the propagation of stereotypes. In particular, consumer-side fairness, which focuses on mitigating discrimination experienced by users of recommender systems, has seen a vast number of diverse approaches for addressing different types of discrimination. The nature of said discrimination depends on the setting 
    
[^51]: OmniSafe：一种加速安全强化学习研究的基础设施

    OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research. (arXiv:2305.09304v1 [cs.LG])

    [http://arxiv.org/abs/2305.09304](http://arxiv.org/abs/2305.09304)

    OmniSafe是一种基础设施，用于加速安全强化学习研究，帮助解决当代SafeRL研究环境中缺乏协调和有效的学习框架的问题。

    

    强化学习（RL）算法赋能的AI系统有着促进社会进步的巨大潜力，但它们的部署常常受到重大安全隐患的阻碍。尤其是在安全关键的应用中，研究人员已经引起了对不受约束的RL代理的意外伤害或不安全行为的担忧。安全强化学习（SafeRL）的理念是将RL代理与无害意图和安全行为模式相一致。在SafeRL中，代理通过从环境中接收反馈来学习开发优化策略，同时也满足了将意外伤害或不安全行为的风险最小化的要求。然而，由于SafeRL算法实现的复杂性，跨越各个领域的方法的结合提出了巨大的挑战。这导致了当代SafeRL研究环境中缺乏一个协调和有效的学习框架。在这项工作中，我们介绍了一个基础框架：OmniSafe，用于加速安全强化学习研究。

    AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework d
    
[^52]: 自适应计算机视觉场景的越界检测方法

    Out-of-Distribution Detection for Adaptive Computer Vision. (arXiv:2305.09293v1 [cs.CV])

    [http://arxiv.org/abs/2305.09293](http://arxiv.org/abs/2305.09293)

    该论文提出了一种自适应计算机视觉场景的越界检测方法，通过根据基于正则化流的越界检测器自适应相机参数，能使目标检测性能指标 mAP、mAR 和 F1 平均提高 3-4 个百分点。

    

    众所周知，当遇到以前未见的图像条件时，计算机视觉可能会不可靠。本文提出了一种根据基于正则化流的越界检测器自适应相机参数的方法。进行了一项小规模的研究，结果表明，根据这个越界检测器适应相机参数可以使 YOLOv4 目标检测器的 mAP、mAR 和 F1 性能指标平均提高 3 到 4 个百分点。作为次要结果，本文还表明可以在 COCO 数据集上训练正则化流模型来进行越界检测，该数据集比大多数越界检测器基准测试数据集更大、更多样化。

    It is well known that computer vision can be unreliable when faced with previously unseen imaging conditions. This paper proposes a method to adapt camera parameters according to a normalizing flow-based out-of-distibution detector. A small-scale study is conducted which shows that adapting camera parameters according to this out-of-distibution detector leads to an average increase of 3 to 4 percentage points in mAP, mAR and F1 performance metrics of a YOLOv4 object detector. As a secondary result, this paper also shows that it is possible to train a normalizing flow model for out-of-distribution detection on the COCO dataset, which is larger and more diverse than most benchmarks for out-of-distibution detectors.
    
[^53]: 基于词典的时间序列序数分类方法

    A Dictionary-based approach to Time Series Ordinal Classification. (arXiv:2305.09288v1 [cs.LG])

    [http://arxiv.org/abs/2305.09288](http://arxiv.org/abs/2305.09288)

    本文提出了一种基于词典的时间序数分类方法(O-TDE)，并证明其在18个TSOC问题上表现显著优于其他四种现有的名义方法。

    

    时间序列分类(TSC)是一个广泛研究领域，可以解决多种实际问题并取得出色的结果。其中表现良好的一类方法是所谓的基于词典的技术。时间词典集合(TDE)是目前基于词典的TSC方法中最先进的方法。在许多TSC问题中，我们发现与时间序列相关的标签具有自然的排序特性，这种特性被称作序数性，可以利用它来提高方法的性能。处理序数时间序列的领域称为时间序数分类(TSOC)，目前尚未被充分开发利用。在本研究中，我们提出了TDE算法的序数适应版，称为序数TDE(O-TDE)。为此，使用18个TSOC问题的测试集进行了全面比较。实验结果表明，与其他四种现有的名义方法相比，基于序数的词典方法取得了显著改进。

    Time Series Classification (TSC) is an extensively researched field from which a broad range of real-world problems can be addressed obtaining excellent results. One sort of the approaches performing well are the so-called dictionary-based techniques. The Temporal Dictionary Ensemble (TDE) is the current state-of-the-art dictionary-based TSC approach. In many TSC problems we find a natural ordering in the labels associated with the time series. This characteristic is referred to as ordinality, and can be exploited to improve the methods performance. The area dealing with ordinal time series is the Time Series Ordinal Classification (TSOC) field, which is yet unexplored. In this work, we present an ordinal adaptation of the TDE algorithm, known as ordinal TDE (O-TDE). For this, a comprehensive comparison using a set of 18 TSOC problems is performed. Experiments conducted show the improvement achieved by the ordinal dictionary-based approach in comparison to four other existing nominal d
    
[^54]: 噪声鲁棒的神经网络架构

    Noise robust neural network architecture. (arXiv:2305.09276v1 [cs.CV])

    [http://arxiv.org/abs/2305.09276](http://arxiv.org/abs/2305.09276)

    本文提出了一种名为“Dune Neural Network”的神经网络架构，通过将网络的每个自由参数表示为不确定性区间，并对每个输入元素应用线性变换，实现了对于白噪声数据的噪声鲁棒性能，即使为非常嘈杂的输入图像，此方法也比使用数据增强的人类在测试集上实现了更好的准确度。

    

    在本文中，我们提出了一种名为“Dune Neural Network”的神经网络架构，用于识别一般噪声图像，而不需要在训练数据中添加任何人工噪声。通过将网络的每个自由参数表示为不确定性区间，并对每个输入元素应用线性变换，我们展示了所得到的架构在面对有白噪声的输入数据时具有相当好的噪声鲁棒性。我们将简单的Dune神经网络应用于MNIST数据集，并证明了即使在对于人类难以识别的非常嘈杂的输入图像上，我们的方法也比使用数据增强的人类在测试集上实现了更好的准确度。我们还发现，我们的方法对于许多其他添加了各种背景模式的示例都很鲁棒。

    In which we propose neural network architecture (dune neural network) for recognizing general noisy image without adding any artificial noise in the training data. By representing each free parameter of the network as an uncertainty interval, and applying a linear transformation to each input element, we show that the resulting architecture achieves decent noise robustness when faced with input data with white noise. We apply simple dune neural networks for MNIST dataset and demonstrate that even for very noisy input images which are hard for human to recognize, our approach achieved better test set accuracy than human without dataset augmentation. We also find that our method is robust for many other examples with various background patterns added.
    
[^55]: 在线持续学习中的快速适应性：我们评估得对吗？

    Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?. (arXiv:2305.09275v1 [cs.LG])

    [http://arxiv.org/abs/2305.09275](http://arxiv.org/abs/2305.09275)

    本文重新审视了在线持续学习算法适应性的常见评估方法——在线准确率度量。该度量是不可靠的，现有算法会学习表面的标签相关性，我们提出了一种新的度量来衡量适应性并进行了基准测试。

    

    本文重新审视了在线持续学习算法适应性的常见评估方法——在线准确率度量。该度量衡量模型在接下来的几个样本上的准确性。然而，我们证明了这个度量是不可靠的。即使是空泛的盲分类器也能通过利用数据流中表面的标签相关性来实现非常高的在线准确率。我们的研究表明，现有的在线持续学习算法也能够实现高在线准确率，但是表现出较差的信息保留能力，表明它们意外地学习了表面的标签相关性。为解决这个问题，我们提出了一种基于消除表面的相关性的近期样本准确度的新度量来衡量适应性。我们使用我们提出的度量在各种计算预算下对大规模数据集中现有的OCL方法进行了基准测试，并发现更好的泛化能力可以达到。

    We revisit the common practice of evaluating adaptation of Online Continual Learning (OCL) algorithms through the metric of online accuracy, which measures the accuracy of the model on the immediate next few samples. However, we show that this metric is unreliable, as even vacuous blind classifiers, which do not use input images for prediction, can achieve unrealistically high online accuracy by exploiting spurious label correlations in the data stream. Our study reveals that existing OCL algorithms can also achieve high online accuracy, but perform poorly in retaining useful information, suggesting that they unintentionally learn spurious label correlations. To address this issue, we propose a novel metric for measuring adaptation based on the accuracy on the near-future samples, where spurious correlations are removed. We benchmark existing OCL approaches using our proposed metric on large-scale datasets under various computational budgets and find that better generalization can be a
    
[^56]: 无存储限制的在线持续学习

    Online Continual Learning Without the Storage Constraint. (arXiv:2305.09253v1 [cs.CV])

    [http://arxiv.org/abs/2305.09253](http://arxiv.org/abs/2305.09253)

    本文提出了一种无存储限制的在线持续学习算法，使用kNN分类器和通用预训练特征提取器，在小算力的情况下紧凑地存储和利用整个输入数据流，并实现了更好的性能。

    

    在线持续学习（OCL）的研究主要集中在通过固定和有限的存储分配来减轻灾难性遗忘。然而，数据存储的可负担性提高了一系列不符合这些假设的应用。在这些情况下，主要关注点在于管理计算支出而不是存储。本文针对这种情况，通过放宽存储限制并强调固定的，有限的经济预算，研究在线持续学习问题。我们提供了一个简单的算法，可以使用kNN分类器和通用预训练特征提取器在微小的计算预算下紧凑地存储和利用整个输入数据流。我们的算法提供了一个常态化学习有吸引力的一致性属性：它永远不会忘记过去的数据。我们在两个大规模的OCL数据集上设立了一个新的状态：连续本地化（CL）和可持续的对象识别（SOR）。

    Online continual learning (OCL) research has primarily focused on mitigating catastrophic forgetting with fixed and limited storage allocation throughout the agent's lifetime. However, the growing affordability of data storage highlights a broad range of applications that do not adhere to these assumptions. In these cases, the primary concern lies in managing computational expenditures rather than storage. In this paper, we target such settings, investigating the online continual learning problem by relaxing storage constraints and emphasizing fixed, limited economical budget. We provide a simple algorithm that can compactly store and utilize the entirety of the incoming data stream under tiny computational budgets using a kNN classifier and universal pre-trained feature extractors. Our algorithm provides a consistency property attractive to continual learning: It will never forget past seen data. We set a new state of the art on two large-scale OCL datasets: Continual LOCalization (CL
    
[^57]: 利用预测的不确定性下的排序和超图定向

    Sorting and Hypergraph Orientation under Uncertainty with Predictions. (arXiv:2305.09245v1 [cs.DS])

    [http://arxiv.org/abs/2305.09245](http://arxiv.org/abs/2305.09245)

    本文研究了在利用不可信预测的情况下排序和超图定向的学习增强算法，提供了对于正确预测时$1+1/\gamma$的竞争比和对于任意错误预测时$\gamma$的竞争比的算法。

    

    学习增强的算法受到越来越多的关注，但最近才在可探索不确定性的环境中被考虑，其中可以通过查询获得不确定输入元素的精确值，并且目标是最小化解决问题所需的查询数量。我们研究了在利用不可信预测的情况下排序和超图定向的学习增强算法。我们的算法提供了提高了准确预测的性能保证，同时保持了在没有预测的情况下最好的最坏情况保证。对于超图定向，对于任何$\gamma\geq 2$，我们提供一个算法，该算法在正确预测时达到$1+1/\gamma$的竞争比，对于任意错误预测则为$\gamma$。对于排序，我们在准确预测时实现了最优解，同时在任意错误预测时仍是$2$竞争的。这些权衡是在实现非平凡保证的同时实现的最佳可能权衡。

    Learning-augmented algorithms have been attracting increasing interest, but have only recently been considered in the setting of explorable uncertainty where precise values of uncertain input elements can be obtained by a query and the goal is to minimize the number of queries needed to solve a problem. We study learning-augmented algorithms for sorting and hypergraph orientation under uncertainty, assuming access to untrusted predictions for the uncertain values. Our algorithms provide improved performance guarantees for accurate predictions while maintaining worst-case guarantees that are best possible without predictions. For hypergraph orientation, for any $\gamma \geq 2$, we give an algorithm that achieves a competitive ratio of $1+1/\gamma$ for correct predictions and $\gamma$ for arbitrarily wrong predictions. For sorting, we achieve an optimal solution for accurate predictions while still being $2$-competitive for arbitrarily wrong predictions. These tradeoffs are the best poss
    
[^58]: 无法学习的样本给出了一种虚假的安全感：通过可学习的例子穿透那些无法利用的数据

    Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])

    [http://arxiv.org/abs/2305.09241](http://arxiv.org/abs/2305.09241)

    “无法学习的样本”提出一种对数据进行保护的方法，但它无法阻止未经授权的用户对保护后的数据进行利用。通过提出“可学习的未经授权示例”和一种新的纯化过程，我们可以实现对数据的更好保护。

    

    在当下随处可见的安全漏洞中，保护数据免于未经授权的利用是至关重要的。最近，一种叫做“无法学习的样本”（UEs）的方法被提出，通过对数据进行微小的扰动，使得模型无法在原始的干净分布上准确地对其进行分类，从而提供了一种强大的保护措施。然而，我们发现 UEs 带来的安全威胁是虚假的，因为它们无法阻止未经授权的用户利用其他未受保护的数据来去除保护，将无法学习的数据重转为可学习。基于这一观察，我们正式定义了一种威胁，引入了“可学习的未经授权示例”（LEs），这些是已经去除保护的UEs。我们的方法的核心是通过一种新的纯化过程，将UEs投射到LEs的流形上。这是通过一种新的联合条件扩散模型来实现的，该模型对UEs进行去噪。

    Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
    
[^59]: 合成数据，真实误差：如何（不）发布和使用合成数据

    Synthetic data, real errors: how (not) to publish and use synthetic data. (arXiv:2305.09235v1 [cs.LG])

    [http://arxiv.org/abs/2305.09235](http://arxiv.org/abs/2305.09235)

    合成数据在机器学习领域受到了越来越多的关注，但是不完美的合成数据可能会导致下游机器学习任务中的潜在错误。为了改进这种情况，研究人员引入了深度生成集成（DGE）框架来近似生成过程模型参数的后验分布，以提高下游模型的训练和评估效果。

    

    通过生成模型生成合成数据在机器学习社区和其他领域越来越受到关注，这种方法承诺将来可以根据个体需求定制数据集。不幸的是，合成数据通常并不完美，可能导致下游任务中的潜在错误。在本文中，我们探讨了生成过程对下游机器学习任务的影响。我们展示了单纯的合成数据方法——将合成数据视为真实数据使用——会导致下游模型和分析无法很好地推广到真实数据。作为合成数据环境下更好的机器学习的第一步，我们引入了深度生成集成（DGE）——受到深度集成启发的框架，旨在隐式地近似生成过程模型参数的后验分布。DGE改善了下游模型的训练、评估和不确定性量化，平均而言远远优于单纯的方法。对于少数类和低密度区域，最大的改进效果得到了实现。

    Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-de
    
[^60]: 基于边缘传感器的半弹性纺织品触摸感测技术研究

    Touch Sensing on Semi-Elastic Textiles with Border-Based Sensors. (arXiv:2305.09222v1 [cs.LG])

    [http://arxiv.org/abs/2305.09222](http://arxiv.org/abs/2305.09222)

    本研究提出一种基于半弹性纺织品表面上边缘的传感器进行接触感测的方法，无需在感测区域放置额外传感器。该方法可在可穿戴技术和智能纺织品等领域中应用，能够以82.85%的准确度分类识别三个压力水平，具有潜在的应用价值。

    

    本研究提出了一种新的接触感测方法，使用半弹性纺织品表面上边缘的传感器，而不需要在接触区域放置额外的传感器。通过在弹性运动织物上进行多种机器学习模型的实验，验证了所提出的方法。其中一种基于边缘的传感器设计的性能进行了深入评估。通过使用视觉标记，最佳表现的视觉传感器预测125mm×125mm区域上一个点的平均均方误差为1.36mm。我们制作了一种仅用纺织品实现的原型，能够以82.85%的准确度分类识别三个压力水平（0、15和20mm）。我们的结果表明，这种方法在可穿戴技术和智能纺织品中具有潜在的应用，以进一步探索这些领域，这将是一个充满前途的方向。

    This study presents a novel approach for touch sensing using semi-elastic textile surfaces that does not require the placement of additional sensors in the sensing area, instead relying on sensors located on the border of the textile. The proposed approach is demonstrated through experiments involving an elastic Jersey fabric and a variety of machine-learning models. The performance of one particular border-based sensor design is evaluated in depth. By using visual markers, the best-performing visual sensor arrangement predicts a single touch point with a mean squared error of 1.36 mm on an area of 125mm by 125mm. We built a textile only prototype that is able to classify touch at three indent levels (0, 15, and 20 mm) with an accuracy of 82.85%. Our results suggest that this approach has potential applications in wearable technology and smart textiles, making it a promising avenue for further exploration in these fields.
    
[^61]: Turbo自编码器的组件训练

    Component Training of Turbo Autoencoders. (arXiv:2305.09216v1 [cs.IT])

    [http://arxiv.org/abs/2305.09216](http://arxiv.org/abs/2305.09216)

    本文提出采用孤立训练的高斯先验Turbo自编码器的组件方法能够实现快速、一致、泛化性好的训练，并且具有竞争性能，其中借助EXIT图的设计可专注于块误码率而达到期望行为。

    

    对Turbo自编码器结构的组件自编码器进行孤立高斯先验训练(TGP)可以实现更快、更一致的训练，并且对任意解码迭代具有更好的泛化能力。我们提出通过外部信息传递EXIT图来适应组件的期望行为，从而实现在大数据长度（k≈1000）时的扩展性，同时保持竞争性能。据我们所知，这是第一个在该区域内表现接近经典编码的自编码器。尽管二进制交叉熵(BCE)损失函数优化了组件的比特误码率(BER)，但是通过EXIT图的设计可以专注于块误码率(BLER)。在串联系统中，组件级别的TGP方法已为内部组件（具有固定外部二进制接口，例如学习的内部代码或均衡器）和外部二进制纠错码的等值器所熟知。本文提出始终采用TGP方法整个Turbo自编码器。

    Isolated training with Gaussian priors (TGP) of the component autoencoders of turbo-autoencoder architectures enables faster, more consistent training and better generalization to arbitrary decoding iterations than training based on deep unfolding. We propose fitting the components via extrinsic information transfer (EXIT) charts to a desired behavior which enables scaling to larger message lengths ($k \approx 1000$) while retaining competitive performance. To the best of our knowledge, this is the first autoencoder that performs close to classical codes in this regime. Although the binary cross-entropy (BCE) loss function optimizes the bit error rate (BER) of the components, the design via EXIT charts enables to focus on the block error rate (BLER). In serially concatenated systems the component-wise TGP approach is well known for inner components with a fixed outer binary interface, e.g., a learned inner code or equalizer, with an outer binary error correcting code. In this paper we 
    
[^62]: CB-HVTNet：一种用于组织病理学图像中淋巴细胞评估的通道增强混合视觉 Transformer 网络

    CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v1 [eess.IV])

    [http://arxiv.org/abs/2305.09211](http://arxiv.org/abs/2305.09211)

    CB-HVTNet 提出了一种 Channel Boosted Hybrid Vision Transformer 网络，利用迁移学习生成增强通道，并结合使用 Transformers 和 CNN，在组织病理学图像中高效准确地评估淋巴细胞。

    

    Transformer 由于其学习长距离依赖性的能力已经克服了卷积神经网络（CNN）全局透视学习的缺点。因此，它们已经成为研究人员关注的焦点，用于多个与视觉相关的任务，包括医疗诊断。然而，它们的多头注意模块仅捕获全局级别的特征表示，这对于医学图像来说是不足的。为了解决这个问题，我们提出了一种 Channel Boosted Hybrid Vision Transformer（CB HVT），它利用迁移学习生成增强通道，并使用 Transformers 和 CNN 来分析组织病理学图像中的淋巴细胞。所提出的 CB HVT 包括五个模块，包括通道生成模块、通道利用模块、通道合并模块、区域感知模块和检测和分段头，它们共同有效地识别淋巴细胞。通道生成模块使用通过迁移学习进行通道增强的思想创建多个强大的通道，然后与 Transformers 和 CNN 结合使用，以更好地分析组织病理学图像中的淋巴细胞。总体而言，所提出的 CB HVT 是医学诊断中准确、高效评估淋巴细胞的强大工具。

    Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning to generate boosted channels and employs both transformers and CNNs to analyse lymphocytes in histopathological images. The proposed CB HVT comprises five modules, including a channel generation module, channel exploitation module, channel merging module, region-aware module, and a detection and segmentation head, which work together to effectively identify lymphocytes. The channel generation module uses the idea of channel boosting through transfer learni
    
[^63]: 结构化状态空间模型用于反事实结果预测

    Counterfactual Outcome Prediction using Structured State Space Model. (arXiv:2305.09207v1 [cs.LG])

    [http://arxiv.org/abs/2305.09207](http://arxiv.org/abs/2305.09207)

    本文探讨了使用结构化状态空间模型对纵向数据中的反事实结果进行预测的方法，相比Treatment Effect Neural Controlled Differential Equation，S4Model更有效建模长期依赖关系、更易于训练、训练时间更短，且在归一化均方误差上提高了10倍。

    

    近年来，由于在医疗保健和社会科学中的潜在应用，纵向数据中的反事实结果预测越来越受到关注。本文探讨了使用状态空间模型（一种流行的序列模型）进行这项任务的方法。具体而言，我们比较了两种模型的性能：Treatment Effect Neural Controlled Differential Equation（TE-CDE）和结构化状态空间模型（S4Model）。虽然TE-CDE利用可控微分方程来解决时间依赖性混淆，但它存在优化问题和训练缓慢的问题。相比之下，S4Model更有效地建模长期依赖关系，并且更容易训练。我们在一个模拟肺部肿瘤增长数据集上评估了这些模型，发现S4Model在每个时期的训练时间减少了1.63倍，并且归一化均方误差也提高了10倍。此外，相较TE-CDE，S4Model在训练期间更稳定，对权重初始化也不太敏感。我们的结果表明，由于其高效和稳定性，S4Model是纵向数据中反事实结果预测更为有前途的方法。

    Counterfactual outcome prediction in longitudinal data has recently gained attention due to its potential applications in healthcare and social sciences. In this paper, we explore the use of the state space model, a popular sequence model, for this task. Specifically, we compare the performance of two models: Treatment Effect Neural Controlled Differential Equation (TE-CDE) and structured state space model (S4Model). While TE-CDE uses controlled differential equations to address time-dependent confounding, it suffers from optimization issues and slow training. In contrast, S4Model is more efficient at modeling long-range dependencies and easier to train. We evaluate the models on a simulated lung tumor growth dataset and find that S4Model outperforms TE-CDE with 1.63x reduction in per epoch training time and 10x better normalized mean squared error. Additionally, S4Model is more stable during training and less sensitive to weight initialization than TE-CDE. Our results suggest that the
    
[^64]: 权重莫比乌斯分数：一个特征归因的统一框架

    The Weighted M\"obius Score: A Unified Framework for Feature Attribution. (arXiv:2305.09204v1 [cs.LG])

    [http://arxiv.org/abs/2305.09204](http://arxiv.org/abs/2305.09204)

    本文提出了权重莫比乌斯分数作为一个参数化的归因框架，可以涵盖很多不同的特征归因方法，包括特征交互，解决了方法繁衍和不可比的问题。通过研究方法的向量空间，提供了一些新方法和解释，实证结果表明其多功能性和有效性。

    

    特征归因旨在通过识别每个特征对预测的影响来解释黑盒模型预测的推理过程。最近的工作将特征归因扩展到多个特征之间的交互。然而，缺乏统一的框架导致方法的大量繁衍，这些方法通常不能直接比较。本文介绍了一种参数化的归因框架——权重莫比乌斯分数，并显示了许多不同的针对单个特征和特征交互的归因方法是特例，还验证了一些新方法。通过研究归因方法的向量空间，我们的框架利用标准线性代数工具，并在各个领域提供解释，包括合作博弈理论和因果中介分析。我们通过将这些归因方法应用于情感分析中的特征交互来实证了框架的多功能性和有效性。

    Feature attribution aims to explain the reasoning behind a black-box model's prediction by identifying the impact of each feature on the prediction. Recent work has extended feature attribution to interactions between multiple features. However, the lack of a unified framework has led to a proliferation of methods that are often not directly comparable. This paper introduces a parameterized attribution framework -- the Weighted M\"obius Score -- and (i) shows that many different attribution methods for both individual features and feature interactions are special cases and (ii) identifies some new methods. By studying the vector space of attribution methods, our framework utilizes standard linear algebra tools and provides interpretations in various fields, including cooperative game theory and causal mediation analysis. We empirically demonstrate the framework's versatility and effectiveness by applying these attribution methods to feature interactions in sentiment analysis and chain-
    
[^65]: Ortho-ODE：增强神经常微分方程对抗攻击的鲁棒性

    Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial Attacks. (arXiv:2305.09179v1 [cs.LG])

    [http://arxiv.org/abs/2305.09179](http://arxiv.org/abs/2305.09179)

    本文研究了神经常微分方程（NODE）在面对噪声和对抗性攻击时所表现出的自然鲁棒性，并通过控制ODE动力学的Lipschitz常数来显著提高其鲁棒性。实验结果在多个数据集上得到了验证。

    

    神经常微分方程（NODE）通过使用数值求解器来求解由神经网络（NN）表示的微分方程，从而引发了一种具有无限深度的新型深度学习模型范式。 NODE旨在解决不规则时间序列问题。然而，NODE对各种噪声和对抗性攻击表现出了鲁棒性。本文研究NODE的自然鲁棒性并考察其中令人惊讶行为的原因。我们表明，通过控制ODE动力学的Lipschitz常数，可以显着提高其鲁棒性。我们从Grownwall不等式中推导出我们的方法。此外，我们绘制收缩理论和Grownwall不等式之间的类比。实验上，我们在多个数据集（MNIST、CIFAR-10和CIFAR 100）上证实了增强的鲁棒性。我们还展示了自适应和非自适应求解器对NODE鲁棒性的影响。

    Neural Ordinary Differential Equations (NODEs) probed the usage of numerical solvers to solve the differential equation characterized by a Neural Network (NN), therefore initiating a new paradigm of deep learning models with infinite depth. NODEs were designed to tackle the irregular time series problem. However, NODEs have demonstrated robustness against various noises and adversarial attacks. This paper is about the natural robustness of NODEs and examines the cause behind such surprising behaviour. We show that by controlling the Lipschitz constant of the ODE dynamics the robustness can be significantly improved. We derive our approach from Grownwall's inequality. Further, we draw parallels between contractivity theory and Grownwall's inequality. Experimentally we corroborate the enhanced robustness on numerous datasets - MNIST, CIFAR-10, and CIFAR 100. We also present the impact of adaptive and non-adaptive solvers on the robustness of NODEs.
    
[^66]: 通过离散傅里叶变换分析递归神经网络的归纳偏置

    Empirical Analysis of the Inductive Bias of Recurrent Neural Networks by Discrete Fourier Transform of Output Sequences. (arXiv:2305.09178v1 [cs.LG])

    [http://arxiv.org/abs/2305.09178](http://arxiv.org/abs/2305.09178)

    通过离散傅里叶变换，直接计算每个模型的输出序列频率，研究发现长短时记忆（LSTM）和门控循环单元（GRU）在输出序列频率方面具有不同的归纳偏置，LSTM更适合需要长期记忆任务。

    

    递归神经网络(RNNs)的一个独特特性是它逐步处理输入序列。本研究旨在揭示RNNs内在的归纳偏置，即在序列分类任务中，RNNs在多久时间步骤中通过输出进行切换。我们称之为输出序列频率。以前的工作通过训练一些合成数据的模型，并将模型的泛化性与候选泛化模式进行比较，分析了归纳偏置。然而，当检查输出序列频率时，由于枚举候选模式在更长的序列上需要大量计算，以前的方法不能直接应用。为此，我们提出了通过将模型的输出视为离散时间信号，并应用频率域分析来直接计算每个模型的输出序列频率的方法。实验结果表明，长短时记忆（LSTM）和门控循环单元（GRU）在输出序列频率方面具有不同的归纳偏置。具体来说，LSTM倾向于在时间步骤之间更少地切换输出，这表明LSTM更喜欢各个时间步之间的依赖关系，更适合需要长期记忆的任务。

    A unique feature of Recurrent Neural Networks (RNNs) is that it incrementally processes input sequences. In this research, we aim to uncover the inherent generalization properties, i.e., inductive bias, of RNNs with respect to how frequently RNNs switch the outputs through time steps in the sequence classification task, which we call output sequence frequency. Previous work analyzed inductive bias by training models with a few synthetic data and comparing the model's generalization with candidate generalization patterns. However, when examining the output sequence frequency, previous methods cannot be directly applied since enumerating candidate patterns is computationally difficult for longer sequences. To this end, we propose to directly calculate the output sequence frequency for each model by regarding the outputs of the model as discrete-time signals and applying frequency domain analysis. Experimental results showed that Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU
    
[^67]: 深层ReLU网络的多面体异常简单

    Deep ReLU Networks Have Surprisingly Simple Polytopes. (arXiv:2305.09145v1 [cs.LG])

    [http://arxiv.org/abs/2305.09145](http://arxiv.org/abs/2305.09145)

    本文通过计算和分析ReLU网络多面体的单纯形直方图，发现在初始化和梯度下降时它们结构相对简单，这说明了一种新的隐式偏见。

    

    ReLU网络是一种多面体上的分段线性函数。研究这种多面体的性质对于神经网络的研究和发展至关重要。目前，对于多面体的理论和实证研究仅停留在计算数量的水平，这远远不能完整地描述多面体。为了将特征提升到一个新的水平，我们提出通过三角剖分多面体得出多面体的形状。通过计算和分析不同多面体的单纯形直方图，我们发现ReLU网络在初始化和梯度下降时具有相对简单的多面体结构，尽管这些多面体从理论上来说可以非常丰富和复杂。这一发现可以被认为是一种新的隐式偏见。随后，我们使用非平凡的组合推导来理论上解释为什么增加深度不会创建更复杂的多面体，通过限制每个维度的平均单纯形数量。

    A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the av
    
[^68]: 保障联合学习管理系统安全的智能策略控制

    Smart Policy Control for Securing Federated Learning Management System. (arXiv:2305.09134v1 [cs.CR])

    [http://arxiv.org/abs/2305.09134](http://arxiv.org/abs/2305.09134)

    本文提出了一种基于智能合约的策略控制，可以保障联合学习管理系统的安全性，确保每个参与者都遵守数据保护政策，同时可以审计训练过程。

    

    物联网设备在智慧城市、智能医疗系统和其他实际应用中的广泛应用导致了大量数据的生成，常常使用不同的机器学习模型进行分析。联合学习（FL）被认为是一种保护隐私的机器学习技术，在不共享原始数据的情况下多个参与者合作训练机器学习模型。然而，由于每个FL参与者实施了各种数据保护政策，当前的FL架构不允许对训练过程进行审计。另外，当前架构中没有全局模型可验证性。本文提出了基于智能合约的策略控制来保障FL管理系统的安全。首先，我们在FL参与者侧开发和部署基于智能合约的本地训练策略控制。这个策略控制被用来验证训练过程，确保每个FL参与者都遵守数据保护政策，从而实现FL管理系统的安全性。

    The widespread adoption of Internet of Things (IoT) devices in smart cities, intelligent healthcare systems, and various real-world applications have resulted in the generation of vast amounts of data, often analyzed using different Machine Learning (ML) models. Federated learning (FL) has been acknowledged as a privacy-preserving machine learning technology, where multiple parties cooperatively train ML models without exchanging raw data. However, the current FL architecture does not allow for an audit of the training process due to the various data-protection policies implemented by each FL participant. Furthermore, there is no global model verifiability available in the current architecture. This paper proposes a smart contract-based policy control for securing the Federated Learning (FL) management system. First, we develop and deploy a smart contract-based local training policy control on the FL participants' side. This policy control is used to verify the training process, ensuri
    
[^69]: 基于图形强化学习的网络控制双层优化方案

    Graph Reinforcement Learning for Network Control via Bi-Level Optimization. (arXiv:2305.09129v1 [cs.LG])

    [http://arxiv.org/abs/2305.09129](http://arxiv.org/abs/2305.09129)

    本文提出了一种基于图形强化学习的网络控制方法，通过双层优化实现较好的扩展性和性能，并具备鲁棒性、灵活性和可解释性。

    

    在过去的几十年中，对动态网络的优化问题进行了广泛的研究，并被广泛用于规划无数的真实世界问题。然而，(1)传统的基于优化的方法不能扩展到大型网络，(2)设计好的启发式或近似算法往往需要大量的手动试验。在本研究中，我们认为数据驱动的策略可以自动化这个过程，并学习有效的算法而不牺牲优化性能。为此，我们通过强化学习的视角提出了网络控制问题，并提出了一个基于图形网络的框架来处理广泛的问题类。我们提出了一个双层优化方案，而不是天真地对高维图形元素（例如边缘）进行操作，(1)我们通过RL指定了下一个状态，(2)并解决一个凸问题来最好地实现它，从而大大提高了可扩展性和性能。我们进一步强调了我们的框架可以实现的一系列期望的特性，包括鲁棒性、灵活性和可解释性，并在网络控制的一些基本问题，包括图形染色和设施选址问题上验证了我们的方法的有效性。

    Optimization problems over dynamic networks have been extensively studied and widely used in the past decades to formulate numerous real-world problems. However, (1) traditional optimization-based approaches do not scale to large networks, and (2) the design of good heuristics or approximation algorithms often requires significant manual trial-and-error. In this work, we argue that data-driven strategies can automate this process and learn efficient algorithms without compromising optimality. To do so, we present network control problems through the lens of reinforcement learning and propose a graph network-based framework to handle a broad class of problems. Instead of naively computing actions over high-dimensional graph elements, e.g., edges, we propose a bi-level formulation where we (1) specify a desired next state via RL, and (2) solve a convex program to best achieve it, leading to drastically improved scalability and performance. We further highlight a collection of desirable f
    
[^70]: 知识迁移下的因果效应估计: 转移因果学习

    Transfer Causal Learning: Causal Effect Estimation with Knowledge Transfer. (arXiv:2305.09126v1 [cs.LG])

    [http://arxiv.org/abs/2305.09126](http://arxiv.org/abs/2305.09126)

    本文提出了一个名为$\ell_1$-TCL的通用框架，它使用知识迁移和Lasso回归来提高因果效应估计精度。

    

    本文研究了一种新颖的问题，即在相同的协变量（或特征）空间设置下通过知识迁移来提高因果效应估计精度，即同类别迁移学习（TL），将其称为转移因果学习（TCL）问题。我们提出了一个通用的框架$\ell_1$-TCL，其中包含$\ell_1$正则化TL来进行苦事参数估计和下游插件ACE估计器，包括结果回归、逆概率加权和双重稳健估计器。最重要的是，借助于Lasso用于高维回归，我们建立了非渐近恢复保证。

    A novel problem of improving causal effect estimation accuracy with the help of knowledge transfer under the same covariate (or feature) space setting, i.e., homogeneous transfer learning (TL), is studied, referred to as the Transfer Causal Learning (TCL) problem. While most recent efforts in adapting TL techniques to estimate average causal effect (ACE) have been focused on the heterogeneous covariate space setting, those methods are inadequate for tackling the TCL problem since their algorithm designs are based on the decomposition into shared and domain-specific covariate spaces. To address this issue, we propose a generic framework called \texttt{$\ell_1$-TCL}, which incorporates $\ell_1$ regularized TL for nuisance parameter estimation and downstream plug-in ACE estimators, including outcome regression, inverse probability weighted, and doubly robust estimators. Most importantly, with the help of Lasso for high-dimensional regression, we establish non-asymptotic recovery guarantee
    
[^71]: 一种用于射电干涉成像重建的条件去噪扩散概率模型

    A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction. (arXiv:2305.09121v1 [astro-ph.IM])

    [http://arxiv.org/abs/2305.09121](http://arxiv.org/abs/2305.09121)

    本文提出了一种名为VIC-DDPM的条件去噪扩散概率模型，在可见度数据和脏图像的帮助下，能够生成更细节的图像，同时消除噪声和伪影。相关实验证实，该算法在恢复微弱信号、保留细节结构和消除伪影等方面有很好的性能。

    

    在射电天文学中，射电望远镜接收到的信号会转换为天体对象的图像。但这些图像通常会包含伪源和其他因素导致的伪影，称为“脏图像”。因此，需要对脏图像进行重建以获取更干净的图像并消除伪影。然而，现有的方法在恢复微弱信号，保留细节结构和消除伪影方面存在一定的局限性。本文提出了一种名为VIC-DDPM的可见度和图像条件下的去噪扩散概率模型。该模型同时利用时域和空域的信息来生成图像，以达到消除噪声和生成更细节的图像的目的。实验证实，我们的算法在峰值信噪比、视觉质量和恢复微弱信号的能力方面优于当前的最先进算法。

    In radio astronomy, signals from radio telescopes are transformed into images of observed celestial objects, or sources. However, these images, called dirty images, contain real sources as well as artifacts due to signal sparsity and other factors. Therefore, radio interferometric image reconstruction is performed on dirty images, aiming to produce clean images in which artifacts are reduced and real sources are recovered. So far, existing methods have limited success on recovering faint sources, preserving detailed structures, and eliminating artifacts. In this paper, we present VIC-DDPM, a Visibility and Image Conditioned Denoising Diffusion Probabilistic Model. Our main idea is to use both the original visibility data in the spectral domain and dirty images in the spatial domain to guide the image generation process with DDPM. This way, we can leverage DDPM to generate fine details and eliminate noise, while utilizing visibility data to separate signals from noise and retaining spat
    
[^72]: 卷积神经网络的分类自动学习算法选择

    Automatic learning algorithm selection for classification via convolutional neural networks. (arXiv:2305.09101v1 [cs.LG])

    [http://arxiv.org/abs/2305.09101](http://arxiv.org/abs/2305.09101)

    本文提出了一种直接使用表格数据自动训练卷积神经网络的方法，以学习数据固有的结构，无需识别元特征。在模拟和真实数据集上，该方法均取得了竞争性能。

    

    与其他任务一样，构建机器学习模型的过程可以受益于先前的经验。基于元学习的分类器选择通过比较不同数据集的特征和机器学习技术的性能，以提高当前建模过程中的决策。然而，本文提出了一种自动学习方案，直接使用表格数据为二进制分类训练卷积神经网络，以学习数据固有的结构而不识别元特征。在模拟数据集上进行的实验显示，所提出的方法在识别线性和非线性模式方面达到了几乎完美的性能，优于基于元特征的传统两步方法。文中所提出的方法随后在真实数据集上进行评估，证明了与现有最先进方法相比的竞争性能。

    As in any other task, the process of building machine learning models can benefit from prior experience. Meta-learning for classifier selection gains knowledge from characteristics of different datasets and/or previous performance of machine learning techniques to make better decisions for the current modeling process. Meta-learning approaches first collect meta-data that describe this prior experience and then use it as input for an algorithm selection model. In this paper, however, we propose an automatic learning scheme in which we train convolutional networks directly with the information of tabular datasets for binary classification. The goal of this study is to learn the inherent structure of the data without identifying meta-features. Experiments with simulated datasets show that the proposed approach achieves nearly perfect performance in identifying linear and nonlinear patterns, outperforming the traditional two-step method based on meta-features. The proposed method is then 
    
[^73]: 任务无关BERT压缩的权重继承蒸馏方法

    Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])

    [http://arxiv.org/abs/2305.09098](http://arxiv.org/abs/2305.09098)

    本文提出了一种直接从教师模型传递知识的权重继承蒸馏方法，不需要额外的对齐损失就可以训练出一个紧凑的学生模型，并且在GLUE和SQuAD基准测试上优于之前最先进的基于KD的基线。

    

    知识蒸馏（KD）是压缩BERT的主要方法。之前的KD方法侧重于为学生模型设计额外的对齐损失，以模仿教师模型的行为。这些方法以间接的方式传递知识。在本文中，我们提出了一种新颖的权重继承蒸馏（WID）方法，直接从教师模型传递知识。WID不需要额外的对齐损失，通过继承权重来训练一个紧凑的学生模型，展示了知识蒸馏的新视角。具体来说，我们将行压缩器和列压缩器设计为映射，然后通过结构重参数化压缩权重。在GLUE和SQuAD基准测试上的实验结果表明，WID优于之前最先进的基于KD的基线。进一步的分析表明，WID也可以在不需要注意力分布对齐损失的情况下学习教师模型的注意力模式。

    Knowledge Distillation (KD) is a predominant approach for BERT compression. Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model. These methods transfer the knowledge in an indirect way. In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher. WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation. Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines. Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.
    
[^74]: ProtoVAE：无监督解缠的原型网络

    ProtoVAE: Prototypical Networks for Unsupervised Disentanglement. (arXiv:2305.09092v1 [cs.LG])

    [http://arxiv.org/abs/2305.09092](http://arxiv.org/abs/2305.09092)

    ProtoVAE是一种基于原型网络的深层生成模型，可以无监督解缠多个基准测试，并将数据编码为可解释的表示形式，可用于下游任务。

    

    生成建模和自监督学习在近年来已经取得了巨大的进展，实现了完全无监督的数据学习。然而，仍需探索如何引导神经网络将数据编码为可解释或可说明的表示形式。无监督解缠问题尤为重要，它提出了发现数据中不同的潜在变化因素或语义概念的方法，而不需要标记样本，并将它们编码为结构上不相交的潜在表示形式。在网络中没有额外的约束或归纳偏差的情况下，生成模型可以学习数据分布和编码因素，但不一定以解缠的方式进行学习。

    Generative modeling and self-supervised learning have in recent years made great strides towards learning from data in a completely unsupervised way. There is still however an open area of investigation into guiding a neural network to encode the data into representations that are interpretable or explainable. The problem of unsupervised disentanglement is of particular importance as it proposes to discover the different latent factors of variation or semantic concepts from the data alone, without labeled examples, and encode them into structurally disjoint latent representations. Without additional constraints or inductive biases placed in the network, a generative model may learn the data distribution and encode the factors, but not necessarily in a disentangled way. Here, we introduce a novel deep generative VAE-based model, ProtoVAE, that leverages a deep metric learning Prototypical network trained using self-supervision to impose these constraints. The prototypical network constr
    
[^75]: 基于Hessian映射的卷积神经网络本质的新视角

    The Hessian perspective into the Nature of Convolutional Neural Networks. (arXiv:2305.09088v1 [cs.LG])

    [http://arxiv.org/abs/2305.09088](http://arxiv.org/abs/2305.09088)

    本文基于Hessian映射，揭示了CNN结构和性质的本质，并证明了Hessian秩随着参数数量的增长而呈现出平方根增长。

    

    尽管卷积神经网络(CNNs)一直被研究、应用和理论化，我们的目的是从它们的Hessian映射的角度提供一个稍微不同的观点，因为损失的Hessian捕捉了参数的成对交互，因此形成了一个自然的基础来探索CNN的架构方面如何表现出它的结构和性质。我们开发了一个依赖于CNN的Toeplitz表示的框架，并利用它来揭示Hessian结构，特别是它的秩。我们证明了紧密的上界（使用线性激活），它们紧密地遵循了Hessian秩的经验趋势，并在更一般的设置中保持在实践中。总的来说，我们的工作概括和确认了一个关键的洞见，即即使在CNNs中，Hessian秩随着参数数量的增长而呈现出平方根增长。

    While Convolutional Neural Networks (CNNs) have long been investigated and applied, as well as theorized, we aim to provide a slightly different perspective into their nature -- through the perspective of their Hessian maps. The reason is that the loss Hessian captures the pairwise interaction of parameters and therefore forms a natural ground to probe how the architectural aspects of CNN get manifested in its structure and properties. We develop a framework relying on Toeplitz representation of CNNs, and then utilize it to reveal the Hessian structure and, in particular, its rank. We prove tight upper bounds (with linear activations), which closely follow the empirical trend of the Hessian rank and hold in practice in more general settings. Overall, our work generalizes and establishes the key insight that, even in CNNs, the Hessian rank grows as the square root of the number of parameters.
    
[^76]: 基于数据驱动的恶意网站检测方法综述

    A Review of Data-driven Approaches for Malicious Website Detection. (arXiv:2305.09084v1 [cs.CR])

    [http://arxiv.org/abs/2305.09084](http://arxiv.org/abs/2305.09084)

    本文综述了恶意网站检测中基于数据驱动的方法，包括数据预处理、特征提取、模型构建和技术扩展等。最近，深度学习模型被广泛应用于此领域。文中还讨论了该领域面临的挑战及未来方向。

    

    在网络安全领域，检测恶意网站已成为一个关键问题。因此，本文综述了用于检测恶意网站的基于数据驱动的方法。首先讨论了传统方法及其限制，然后概述了数据驱动方法。本文建立了数据-特征-模型-扩展管道，并介绍了数据预处理、特征提取、模型构建和技术扩展等数据驱动方法的最新研究进展。具体而言，本文比较了近年来提出的使用深度学习的方法。此外，本文还遵循数据-特征-模型-扩展管道，讨论了恶意网站检测中数据驱动方法面临的挑战以及未来方向。

    The detection of malicious websites has become a critical issue in cybersecurity. Therefore, this paper offers a comprehensive review of data-driven methods for detecting malicious websites. Traditional approaches and their limitations are discussed, followed by an overview of data-driven approaches. The paper establishes the data-feature-model-extension pipeline and the latest research developments of data-driven approaches, including data preprocessing, feature extraction, model construction and technology extension. Specifically, this paper compares methods using deep learning models proposed in recent years. Furthermore, the paper follows the data-feature-model-extension pipeline to discuss the challenges together with some future directions of data-driven methods in malicious website detection.
    
[^77]: FiMReSt: 一种灵活的概率模型，适用于多集群数据的非对称分布非高斯内核

    FiMReSt: Finite Mixture of Multivariate Regulated Skew-t Kernels -- A Flexible Probabilistic Model for Multi-Clustered Data with Asymmetrically-Scattered Non-Gaussian Kernels. (arXiv:2305.09071v1 [cs.LG])

    [http://arxiv.org/abs/2305.09071](http://arxiv.org/abs/2305.09071)

    本研究提出了一种规则化的迭代优化过程，名为Fini，用于解决现有Skew-t混合模型中的“S-DoF爆炸”问题，提高混合模型的建模泛化性和弹性。

    

    最近，Skew-t混合模型被引入作为一种灵活的概率建模技术，考虑到数据簇中的偏斜和统计自由度（S-DoF），以提高建模的泛化性和对重尾和偏斜的鲁棒性。本文表明，目前的Skew-t混合模型基本上受到一种名为“S-DoF爆炸”的隐藏现象的困扰，在期望极值化的非凸迭代过程中，导致正态核的形状产生局部最小值。本文首次提供了关于S-DoF不稳定性的见解，这可能导致内核从t分布的混合中发散，失去建模离群值的泛化能力和强度。因此，本文提出了一种规则化的迭代优化过程来训练混合模型，增强了技术的泛化能力和弹性。最终的混合模型被命名为Fini。

    Recently skew-t mixture models have been introduced as a flexible probabilistic modeling technique taking into account both skewness in data clusters and the statistical degree of freedom (S-DoF) to improve modeling generalizability, and robustness to heavy tails and skewness. In this paper, we show that the state-of-the-art skew-t mixture models fundamentally suffer from a hidden phenomenon named here as "S-DoF explosion," which results in local minima in the shapes of normal kernels during the non-convex iterative process of expectation maximization. For the first time, this paper provides insights into the instability of the S-DoF, which can result in the divergence of the kernels from the mixture of t-distribution, losing generalizability and power for modeling the outliers. Thus, in this paper, we propose a regularized iterative optimization process to train the mixture model, enhancing the generalizability and resiliency of the technique. The resulting mixture model is named Fini
    
[^78]: 一种离线时间感知的学徒式学习框架用于进化奖励函数的学习

    An Offline Time-aware Apprenticeship Learning Framework for Evolving Reward Functions. (arXiv:2305.09070v1 [cs.LG])

    [http://arxiv.org/abs/2305.09070](http://arxiv.org/abs/2305.09070)

    本文提出了一种THEMES学徒式学习框架来解决医疗保健等人类中心任务中的进化奖励函数问题，实验证明其显著优于其他最新基线模型。

    

    学徒式学习是指通过观察和模仿专家的演示来诱导有效的决策策略的过程。然而，大多数现有的学徒式学习方法并不适用于应对人类中心任务中常见的进化奖励函数，例如医疗保健，这种任务需要离线学习。本文提出了一种离线时间感知的分层EM能量子轨迹 (THEMES) 学徒式学习框架，用于解决此类任务中的进化奖励函数。通过对挑战性任务（败血症治疗）的评估，证明了THEMES极大地胜过了竞争对手的最新基线模型。

    Apprenticeship learning (AL) is a process of inducing effective decision-making policies via observing and imitating experts' demonstrations. Most existing AL approaches, however, are not designed to cope with the evolving reward functions commonly found in human-centric tasks such as healthcare, where offline learning is required. In this paper, we propose an offline Time-aware Hierarchical EM Energy-based Sub-trajectory (THEMES) AL framework to tackle the evolving reward functions in such tasks. The effectiveness of THEMES is evaluated via a challenging task -- sepsis treatment. The experimental results demonstrate that THEMES can significantly outperform competitive state-of-the-art baselines.
    
[^79]: 捕捉人类对AI的心理模型：基于IRT方法的研究

    Capturing Humans' Mental Models of AI: An Item Response Theory Approach. (arXiv:2305.09064v1 [cs.LG])

    [http://arxiv.org/abs/2305.09064](http://arxiv.org/abs/2305.09064)

    本研究提出基于IRT的模型来分析人类对于AI的感知，实验结果表明人们普遍期望AI团队伙伴的表现优于人类，并且人们对AI与人类成员的心理模型不同。

    

    深入了解人类如何感知与AI搭档的信息，是我们理解人工智能团队合作的重要基础。我们借鉴认知科学的相关研究，提出了一个基于IRT的框架来模拟人们的感知，针对人与人、人与AI进行实验，并重点研究两者感知之间的异同，以及人们对AI团队伙伴的期望。我们的研究结果表明，人们普遍期望人工智能的表现优于人类，而且对AI成员的心理模型与人类成员的不同。同时，我们还发现学习、反馈和个体差异等因素同样有可能影响人们的心理模型。我们的研究为捕捉和分析人类对不同情境中的AI感知提供了一种有用工具。

    Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate's performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people's perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants' own self-perception. Our results indicate that people expect AI agents' performance to be significantly better on average than the performance of other hu
    
[^80]: 有界KRnet及其在密度估计和近似中的应用

    Bounded KRnet and its applications to density estimation and approximation. (arXiv:2305.09063v1 [cs.LG])

    [http://arxiv.org/abs/2305.09063](http://arxiv.org/abs/2305.09063)

    本文介绍了一种新的可逆映射B-KRnet，并将其应用于数据或PDE的密度估计/近似，由于其定义在有界域上，因此比KRnet更有效。

    

    本文在有界域上开发了一种可逆映射，称为B-KRnet，并将其应用于数据或PDE（例如福克-普朗克方程和Keller-Segel方程）的密度估计/近似。与KRnet类似，B-KRnet的结构将Knothe-Rosenblatt重排的三角形形式转化为归一化流模型。B-KRnet和KRnet之间的主要区别是B-KRnet定义在超立方体上，而KRnet定义在整个空间上，换句话说，我们在B-KRnet中引入了一种新的机制来保持精确的可逆性。将B-KRnet用作传输映射，我们获得了一个明确的概率密度函数（PDF）模型，该模型对应于先验（均匀）分布在超立方体上的推移。为了近似计算域上定义的PDF，B-KRnet比KRnet更有效。通过耦合KRnet和B-KRnet，我们还可以在高维域上定义一个深度生成模型。

    In this paper, we develop an invertible mapping, called B-KRnet, on a bounded domain and apply it to density estimation/approximation for data or the solutions of PDEs such as the Fokker-Planck equation and the Keller-Segel equation. Similar to KRnet, the structure of B-KRnet adapts the triangular form of the Knothe-Rosenblatt rearrangement into a normalizing flow model. The main difference between B-KRnet and KRnet is that B-KRnet is defined on a hypercube while KRnet is defined on the whole space, in other words, we introduce a new mechanism in B-KRnet to maintain the exact invertibility. Using B-KRnet as a transport map, we obtain an explicit probability density function (PDF) model that corresponds to the pushforward of a prior (uniform) distribution on the hypercube. To approximate PDFs defined on a bounded computational domain, B-KRnet is more effective than KRnet. By coupling KRnet and B-KRnet, we can also define a deep generative model on a high-dimensional domain where some di
    
[^81]: 基于自监督预训练的功能磁共振成像序列数据用于脑解码任务的迁移学习

    Self-Supervised Pretraining on Paired Sequences of fMRI Data for Transfer Learning to Brain Decoding Tasks. (arXiv:2305.09057v1 [cs.LG])

    [http://arxiv.org/abs/2305.09057](http://arxiv.org/abs/2305.09057)

    本文介绍一种基于自监督预训练框架的transformer方法并应用于fMRI数据，首次表明多任务训练对fMRI数据具有协同效应。预训练任务促进迁移学习的能力得到了证明。

    

    本文介绍了一种基于自监督预训练框架的transformer方法，并应用于功能磁共振成像(fMRI)数据。首先，我们同时在两个自监督任务上对架构进行预训练，以教会模型对音乐听觉皮层的时间和空间动态有一个普遍的理解。我们的预训练结果首次表明，多任务训练对fMRI数据具有协同效应。其次，我们对预训练的模型进行微调和在一个受监督的fMRI分类任务上进行额外的模型训练。我们观察到，在微调模型上，保留数据的准确性显著提高，这证明了我们预训练任务促进迁移学习的能力。本研究为预训练和fMRI数据的迁移学习的transformer架构增加了数量上的证据，并证明了我们的预训练任务和fMRI数据上的多任务预训练的概念证明。

    In this work we introduce a self-supervised pretraining framework for transformers on functional Magnetic Resonance Imaging (fMRI) data. First, we pretrain our architecture on two self-supervised tasks simultaneously to teach the model a general understanding of the temporal and spatial dynamics of human auditory cortex during music listening. Our pretraining results are the first to suggest a synergistic effect of multitask training on fMRI data. Second, we finetune the pretrained models and train additional fresh models on a supervised fMRI classification task. We observe significantly improved accuracy on held-out runs with the finetuned models, which demonstrates the ability of our pretraining tasks to facilitate transfer learning. This work contributes to the growing body of literature on transformer architectures for pretraining and transfer learning with fMRI data, and serves as a proof of concept for our pretraining tasks and multitask pretraining on fMRI data.
    
[^82]: 物理信息卷积循环替代模型在带井控制下的油藏模拟中的应用

    Physics-informed Convolutional Recurrent Surrogate Model for Reservoir Simulation with Well Controls. (arXiv:2305.09056v1 [cs.LG])

    [http://arxiv.org/abs/2305.09056](http://arxiv.org/abs/2305.09056)

    本文提出了一种利用物理信息卷积循环神经网络来模拟井控制下地下多孔介质流体流动的替代模型。该模型不需要标记数据进行训练，并且能够预测出未来的油藏状态，具有广泛的应用前景。

    

    本文提出了一种新颖的替代模型，使用基于物理信息的卷积循环神经网络(PICRNN)模拟井控制下的地下流体流动。该模型利用卷积长短期记忆(ConvLSTM)来捕捉多孔流体状态演变动态的时空依赖性。ConvLSTM与状态空间方程相连，使得离散时间序列井控信号能够被纳入模型。该模型需要输入初始状态条件和井控序列，并以输出状态变量（如压力）作为输出。通过最小化油藏流体状态-空间方程的残差，网络能够在无需标记数据的情况下进行训练。该模型的设计旨在用作预测未来油藏状态的替代模型，基于初始油藏状态和输入的控制工程。在状态空间方程中强制边界条件，因此不需要额外的损耗项来确保模型的物理一致性。

    This paper presents a novel surrogate model for modeling subsurface fluid flow with well controls using a physics-informed convolutional recurrent neural network (PICRNN). The model uses a convolutional long-short term memory (ConvLSTM) to capture the spatiotemporal dependencies of the state evolution dynamics in the porous flow. The ConvLSTM is linked to the state space equations, enabling the incorporation of a discrete-time sequence of well control. The model requires initial state condition and a sequence of well controls as inputs, and predicts the state variables of the system, such as pressure, as output. By minimizing the residuals of reservoir flow state-space equations, the network is trained without the need for labeled data. The model is designed to serve as a surrogate model for predicting future reservoir states based on the initial reservoir state and input engineering controls. Boundary conditions are enforced into the state-space equations so no additional loss term is
    
[^83]: 概率单纯形上的凸优化

    Convex optimization over a probability simplex. (arXiv:2305.09046v1 [math.OC])

    [http://arxiv.org/abs/2305.09046](http://arxiv.org/abs/2305.09046)

    这篇论文提出了一种新的迭代方案，用于求解概率单纯形上的凸优化问题。该方法具有收敛速度快且简单易行的特点。

    

    我们提出了一种新的迭代方案——柯西单纯形来优化凸问题，使其满足概率单纯形上的限制条件，即$w\in\mathbb{R}^n$中$\sum_i w_i=1$，$w_i\geq0$。我们将单纯形映射到单位球的正四面体，通过梯度下降获得隐变量的解，并将结果映射回原始变量。该方法适用于高维问题，每次迭代由简单的操作组成，且针对凸函数证明了收敛速度为${O}(1/T)$。同时本文关注了信息理论（如交叉熵和KL散度）的应用。

    We propose a new iteration scheme, the Cauchy-Simplex, to optimize convex problems over the probability simplex $\{w\in\mathbb{R}^n\ |\ \sum_i w_i=1\ \textrm{and}\ w_i\geq0\}$. Other works have taken steps to enforce positivity or unit normalization automatically but never simultaneously within a unified setting. This paper presents a natural framework for manifestly requiring the probability condition. Specifically, we map the simplex to the positive quadrant of a unit sphere, envisage gradient descent in latent variables, and map the result back in a way that only depends on the simplex variable. Moreover, proving rigorous convergence results in this formulation leads inherently to tools from information theory (e.g. cross entropy and KL divergence). Each iteration of the Cauchy-Simplex consists of simple operations, making it well-suited for high-dimensional problems. We prove that it has a convergence rate of ${O}(1/T)$ for convex functions, and numerical experiments of projection 
    
[^84]: 大规模数据的可伸缩和健壮的张量环分解

    Scalable and Robust Tensor Ring Decomposition for Large-scale Data. (arXiv:2305.09044v1 [cs.LG])

    [http://arxiv.org/abs/2305.09044](http://arxiv.org/abs/2305.09044)

    本文提出了一种可伸缩和健壮的张量环分解算法，能够自适应地填充缺失条目并识别异常值，在存储和计算复杂度上有显著降低，适用于处理大规模张量数据。

    

    最近，张量环分解因其在高阶张量中的优越表现而受到越来越多的关注。然而，传统的张量环分解算法在面对大规模数据、缺失条目和异常值时，往往难以应用于现实世界的应用中。在本文中，我们提出了一种可扩展和健壮的张量环分解算法，能够处理大规模张量数据及其中的缺失条目和严重异常值。首先，我们开发了一种新颖的自适应加权最陡下降方法，在分解过程中能够自适应地填充缺失条目并识别异常值。此外，利用张量环模型，我们发展了一种新的快速 Gram 矩阵计算（FGMC）方法和一种随机子张量草图（RStS）策略，大幅度减少了存储和计算复杂度。实验结果表明，所提出的方法在张量环分解方面优于现有的方法。

    Tensor ring (TR) decomposition has recently received increased attention due to its superior expressive performance for high-order tensors. However, the applicability of traditional TR decomposition algorithms to real-world applications is hindered by prevalent large data sizes, missing entries, and corruption with outliers. In this work, we propose a scalable and robust TR decomposition algorithm capable of handling large-scale tensor data with missing entries and gross corruptions. We first develop a novel auto-weighted steepest descent method that can adaptively fill the missing entries and identify the outliers during the decomposition process. Further, taking advantage of the tensor ring model, we develop a novel fast Gram matrix computation (FGMC) approach and a randomized subtensor sketching (RStS) strategy which yield significant reduction in storage and computational complexity. Experimental results demonstrate that the proposed method outperforms existing TR decomposition met
    
[^85]: 分层无线网络中的自适应联邦剪枝

    Adaptive Federated Pruning in Hierarchical Wireless Networks. (arXiv:2305.09042v1 [cs.LG])

    [http://arxiv.org/abs/2305.09042](http://arxiv.org/abs/2305.09042)

    本文提出了一种针对分层联邦学习的自适应模型剪枝方法，来降低神经网络规模并解决由于设备增多导致的学习延迟问题。

    

    联邦学习是一种有前途的隐私保护分布式学习框架，其中服务器汇总了由多个设备更新的模型，而不会访问它们的私有数据集。分层联邦学习作为设备-边缘-云聚合层次结构，既可以享受云服务器对更多数据集的访问权，又可以享受边缘服务器与设备之间的有效通信。然而，由于边缘服务器和设备数量增加，本地计算能力和通信带宽有限，HFL网络规模的增加会导致学习延迟增加。为了解决这个问题，本文介绍了在无线网络中使用模型剪枝来降低神经网络规模。我们介绍了基于模型剪枝的HFL梯度l2范数上限的收敛分析，分析了所提出的模型剪枝方案的计算和通信延迟，并制定了一个优化问题，以在给定的延迟阈值下最大化收敛速率。

    Federated Learning (FL) is a promising privacy-preserving distributed learning framework where a server aggregates models updated by multiple devices without accessing their private datasets. Hierarchical FL (HFL), as a device-edge-cloud aggregation hierarchy, can enjoy both the cloud server's access to more datasets and the edge servers' efficient communications with devices. However, the learning latency increases with the HFL network scale due to the increasing number of edge servers and devices with limited local computation capability and communication bandwidth. To address this issue, in this paper, we introduce model pruning for HFL in wireless networks to reduce the neural network scale. We present the convergence analysis of an upper on the l2 norm of gradients for HFL with model pruning, analyze the computation and communication latency of the proposed model pruning scheme, and formulate an optimization problem to maximize the convergence rate under a given latency threshold 
    
[^86]: 强化学习在Tractography中的作用

    What Matters in Reinforcement Learning for Tractography. (arXiv:2305.09041v1 [cs.LG])

    [http://arxiv.org/abs/2305.09041](http://arxiv.org/abs/2305.09041)

    本论文深入探讨了强化学习在Tractography中不同的组成部分，提出了关于RL算法选择、代理人输入、奖励函数和播种策略的一系列建议。

    

    最近，提出了利用深度强化学习（RL）来学习Tractography过程并训练代理人在没有手动筛选的参考流线的情况下重建白质结构。虽然报告的表现颇具竞争力，但所提出的框架复杂，并且对于其多个部分的作用和影响还知之甚少。在这项工作中，我们深入探讨了所提出框架的不同组成部分，例如RL算法的选择，播种策略，输入信号和奖励函数，并阐明了它们的影响。本次研究共训练了约7400个模型，共计近41000小时的GPU时间。我们的目标是指导热衷于探索深度RL在Tractography中可能性的研究人员，展示这种方法的优势和不足。因此，我们最终提出了关于RL算法的选择、代理人输入、奖励函数和播种策略的一系列建议。

    Recently, deep reinforcement learning (RL) has been proposed to learn the tractography procedure and train agents to reconstruct the structure of the white matter without manually curated reference streamlines. While the performances reported were competitive, the proposed framework is complex, and little is still known about the role and impact of its multiple parts. In this work, we thoroughly explore the different components of the proposed framework, such as the choice of the RL algorithm, seeding strategy, the input signal and reward function, and shed light on their impact. Approximately 7,400 models were trained for this work, totalling nearly 41,000 hours of GPU time. Our goal is to guide researchers eager to explore the possibilities of deep RL for tractography by exposing what works and what does not work with the category of approach. As such, we ultimately propose a series of recommendations concerning the choice of RL algorithm, the input to the agents, the reward function
    
[^87]: 动态学习系统的算法审查

    Algorithmic Censoring in Dynamic Learning Systems. (arXiv:2305.09035v1 [cs.LG])

    [http://arxiv.org/abs/2305.09035](http://arxiv.org/abs/2305.09035)

    本文介绍了动态学习系统中可能出现的审查现象，并且提出了防范审查的措施以及随机探索，从而确保来自被审查组的样本进入训练数据，并纠正模型。

    

    受选择标记影响的动态学习系统可能会出现审查现象，即针对一组或多组数据点分配持续的负面预测。在消费金融等应用中，这会导致一些申请人组被持续拒绝，并且从未进入训练数据。本文规范化审查现象，展示其可能的出现方式，并强调检测的难度。我们考虑采取防范审查的措施，并进行随机探索，这两种方法都能确保我们对原本未观察到的数据点进行标注。由此产生的技术能够让来自被审查组的样本进入训练数据并纠正模型。我们的结果突显了审查的不可测量的危害，并展示了在各种数据生成过程中缓解策略的有效性。

    Dynamic learning systems subject to selective labeling exhibit censoring, i.e. persistent negative predictions assigned to one or more subgroups of points. In applications like consumer finance, this results in groups of applicants that are persistently denied and thus never enter into the training data. In this work, we formalize censoring, demonstrate how it can arise, and highlight difficulties in detection. We consider safeguards against censoring recourse and randomized-exploration - both of which ensure we collect labels for points that would otherwise go unobserved. The resulting techniques allow examples from censored groups to enter into the training data and correct the model. Our results highlight the otherwise unmeasured harms of censoring and demonstrate the effectiveness of mitigation strategies across a range of data generating processes.
    
[^88]: AI在医学图像分割管线中的应用——使用Fold性能差异监测自动化医学图像分割管线

    AI in the Loop -- Functionalizing Fold Performance Disagreement to Monitor Automated Medical Image Segmentation Pipelines. (arXiv:2305.09031v1 [eess.IV])

    [http://arxiv.org/abs/2305.09031](http://arxiv.org/abs/2305.09031)

    提出了一种使用在不同数据集折叠上训练的子模型的方法来确定模型的置信度，有效地标记低性能的自动分割，并在医学图像分割中实现了对机器学习系统的改进。

    

    自动标记预测不良表现的方法对于安全地将机器学习工作流纳入临床实践以及在模型训练期间识别困难病例至关重要。我们提出了一种可轻易采用的方法，使用在不同数据集折叠上训练的子模型，它们的差异可用作模型置信度的替代。使用人类互观者值确定阈值，以确定是否需要人工审查最终的集合模型预测。在两个不同的数据集（腹部CT和MR预测肾肿瘤），我们的框架有效地识别了低性能的自动分割。最小Interfold测试Dice分数低于人类互帮者可变性的图像被标记，以最大化被标记图像的数量，同时确保最大的集合测试Dice。当我们的内部训练模型应用于外部公开数据集（KiTS21）时，标记的图像包括比我们的训练数据中观察到的更小的肿瘤和先前未见的具有挑战性的病例，显示了我们方法的健壮性。我们的方法提供了一种实用的手段，以改善目前在医学图像分割的机器学习系统中的黑箱性质，以支持临床实践。

    Methods for automatically flag poor performing-predictions are essential for safely implementing machine learning workflows into clinical practice and for identifying difficult cases during model training. We present a readily adoptable method using sub-models trained on different dataset folds, where their disagreement serves as a surrogate for model confidence. Thresholds informed by human interobserver values were used to determine whether a final ensemble model prediction would require manual review. In two different datasets (abdominal CT and MR predicting kidney tumors), our framework effectively identified low performing automated segmentations. Flagging images with a minimum Interfold test Dice score below human interobserver variability maximized the number of flagged images while ensuring maximum ensemble test Dice. When our internally trained model was applied to an external publicly available dataset (KiTS21), flagged images included smaller tumors than those observed in ou
    
[^89]: SKI加速Toeplitz神经网络：通过非对称核实现加速

    SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels. (arXiv:2305.09028v1 [stat.ML])

    [http://arxiv.org/abs/2305.09028](http://arxiv.org/abs/2305.09028)

    本论文提出使用非对称核（asymmetric kernels）实现Toeplitz神经网络（TNNs）的加速，通过稀疏加低秩Toeplitz矩阵分解、小型1D卷积和替换相对位置编码器（RPE）多层感知器（MLP）实现O（n）复杂度，针对因果模型，提出了“快速”因果屏蔽来抵消这种方法的限制。

    

    Toeplitz神经网络（TNNs）是最近出现并取得令人印象深刻结果的序列模型。它们需要O(n log n)的计算复杂度和O(n)的相对位置编码器（RPE）多层感知器（MLP）和衰减偏差调用。我们的目标是减少它们。我们首先指出，RPE是一个非对称正定核，而Toeplitz矩阵是伪格拉姆矩阵。此外：1）学习的核在主对角线附近显示出刺状行为，而在其他位置则表现出平滑行为；2）RPE MLP较慢。对于双向模型，这促使我们进行稀疏加低秩Toeplitz矩阵分解。对于稀疏组件的操作，我们进行小型1D卷积。对于低秩组件，我们将RPE MLP替换为线性插值，并使用非对称有结构的内核插值（SKI）（Wilson等，2015）以实现O（n）复杂度：我们提供了严格的误差分析。对于因果模型，“快速”因果屏蔽（Katharopoulos等，2020）抵消了SKI的好处。

    Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence model with impressive results. They require O(n log n) computational complexity and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and decay bias calls. We aim to reduce both. We first note that the RPE is a non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow. For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix decomposition. For the sparse component's action, we do a small 1D convolution. For the low rank component, we replace the RPE MLP with linear interpolation and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015) for O(n) complexity: we provide rigorous error analysis. For causal models, "fast" causal masking (Katharopoulos et. al. 2020) negates SKI's benefits. 
    
[^90]: DATED：用于工程设计应用的生成合成数据集的指南

    DATED: Guidelines for Creating Synthetic Datasets for Engineering Design Applications. (arXiv:2305.09018v1 [cs.LG])

    [http://arxiv.org/abs/2305.09018](http://arxiv.org/abs/2305.09018)

    本研究提出了一套关于生成、注释和验证合成数据集的全面指南，阐述了权衡和方法，强调了周到的抽样方法的重要性。

    

    利用人工智能的最新进展，如ChatGPT和DALL-E，在实际应用中需要大量的、领域特定且公开可访问的数据集。不幸的是，这种数据集的稀缺性对于希望将这些突破应用于工程设计的研究人员构成了重大挑战。合成数据集则成为了一种可行的替代方案。然而，从业者通常不确定如何生成高质量的数据集，以准确地代表真实世界的数据，并适用于预期的下游应用。本研究旨在通过提出生成、注释和验证合成数据集的全面指南来填补这一知识空白。阐述了每个方面所涉及的权衡和方法。此外，通过创建一个涡轮增压器数据集，说明了这些指南的实际意义。该研究强调了周到的抽样方法的重要性。

    Exploiting the recent advancements in artificial intelligence, showcased by ChatGPT and DALL-E, in real-world applications necessitates vast, domain-specific, and publicly accessible datasets. Unfortunately, the scarcity of such datasets poses a significant challenge for researchers aiming to apply these breakthroughs in engineering design. Synthetic datasets emerge as a viable alternative. However, practitioners are often uncertain about generating high-quality datasets that accurately represent real-world data and are suitable for the intended downstream applications. This study aims to fill this knowledge gap by proposing comprehensive guidelines for generating, annotating, and validating synthetic datasets. The trade-offs and methods associated with each of these aspects are elaborated upon. Further, the practical implications of these guidelines are illustrated through the creation of a turbo-compressors dataset. The study underscores the importance of thoughtful sampling methods 
    
[^91]: 高斯过程Port-Hamiltonian系统：基于物理先验的贝叶斯学习。

    Gaussian Process Port-Hamiltonian Systems: Bayesian Learning with Physics Prior. (arXiv:2305.09017v1 [eess.SY])

    [http://arxiv.org/abs/2305.09017](http://arxiv.org/abs/2305.09017)

    高斯过程Port-Hamiltonian系统是一种基于物理先验的贝叶斯学习方法，能够通过对数据建模生成关于指定输入和输出的被动系统，同时保留了Port-Hamiltonian系统的组合性质。

    

    基于数据的方法在收集到的数据基础上对复杂动态进行建模取得了显著成果。然而，这些模型通常忽略了决定任何现实系统行为的基本物理原理，这种省略不利于两方面：其一，模型在融合物理先验知识方面缺乏数据效率；其二，模型本身可能不符合物理规律。我们提出了高斯过程Port-Hamiltonian系统（GP-PHS）作为基于物理先验的贝叶斯学习方法，并且进行了不确定性量化。由于GP-PHS具有概率论本质，它使用所收集到的数据来形成所有可能的哈密顿量分布，而非单个点估计。由于底层的物理模型，GP-PHS生成关于指定输入和输出被动系统。此外，所提出的方法保留了Port-Hamiltonian系统的组合性质。

    Data-driven approaches achieve remarkable results for the modeling of complex dynamics based on collected data. However, these models often neglect basic physical principles which determine the behavior of any real-world system. This omission is unfavorable in two ways: The models are not as data-efficient as they could be by incorporating physical prior knowledge, and the model itself might not be physically correct. We propose Gaussian Process Port-Hamiltonian systems (GP-PHS) as a physics-informed Bayesian learning approach with uncertainty quantification. The Bayesian nature of GP-PHS uses collected data to form a distribution over all possible Hamiltonians instead of a single point estimate. Due to the underlying physics model, a GP-PHS generates passive systems with respect to designated inputs and outputs. Further, the proposed approach preserves the compositional nature of Port-Hamiltonian systems.
    
[^92]: 物理增强的高斯过程变分自编码器

    Physics-enhanced Gaussian Process Variational Autoencoder. (arXiv:2305.09006v1 [cs.LG])

    [http://arxiv.org/abs/2305.09006](http://arxiv.org/abs/2305.09006)

    本文提出一种物理增强的变分自编码器，将物理先验知识作为高斯过程先验，并将其包含在内核函数中，以实现物理正确的预测。

    

    变分自编码器能够基于高维的输入/输出数据学习一个低维的潜在空间。以视频剪辑作为输入数据，编码器可以用于描述视频中物体的运动而无需地面真实数据（无监督学习）。尽管物体的动态通常基于基本原理，但现有文献大多忽略了这些先前知识。因此，我们提出了一种物理增强的变分自编码器，将物理增强的高斯过程先验放置在潜在动力学上，以提高变分自编码器的效率并允许物理正确的预测。作为线性动态系统表达的物理先验知识在高斯过程的内核函数中反映为格林函数并被包含其中。所提出的方法的优点在振荡粒子的仿真中得到突出体现。

    Variational autoencoders allow to learn a lower-dimensional latent space based on high-dimensional input/output data. Using video clips as input data, the encoder may be used to describe the movement of an object in the video without ground truth data (unsupervised learning). Even though the object's dynamics is typically based on first principles, this prior knowledge is mostly ignored in the existing literature. Thus, we propose a physics-enhanced variational autoencoder that places a physical-enhanced Gaussian process prior on the latent dynamics to improve the efficiency of the variational autoencoder and to allow physically correct predictions. The physical prior knowledge expressed as linear dynamical system is here reflected by the Green's function and included in the kernel function of the Gaussian process. The benefits of the proposed approach are highlighted in a simulation with an oscillating particle.
    
[^93]: 基于机器学习的控制流图恶意软件分析综述

    Survey of Malware Analysis through Control Flow Graph using Machine Learning. (arXiv:2305.08993v1 [cs.CR])

    [http://arxiv.org/abs/2305.08993](http://arxiv.org/abs/2305.08993)

    本文介绍了最新基于控制流图和机器学习的恶意软件检测方法，重点关注了从CFG中提取、表示、分类的不同方法。

    

    恶意软件对计算机系统和网络的安全构成了重大威胁，需要先进的技术对其进行行为和功能分析以进行检测。传统的基于签名的恶意软件检测方法由于恶意软件的快速演化已经失效。其中最有希望克服基于签名检测方法局限的技术之一是使用控制流图（CFG）。CFG利用程序的结构信息将可执行路径表示为图形，其中节点表示指令，边表示控制流依赖关系。目前利用机器学习算法从CFG中提取这些特征并将其分类为恶意或良性的方法已经成为主流。在本综述中，我们旨在回顾一些基于CFG和机器学习的恶意软件检测的最新方法，重点关注从CFG中提取，表示和分类的不同方法。

    Malware is a significant threat to the security of computer systems and networks which requires sophisticated techniques to analyze the behavior and functionality for detection. Traditional signature-based malware detection methods have become ineffective in detecting new and unknown malware due to their rapid evolution. One of the most promising techniques that can overcome the limitations of signature-based detection is to use control flow graphs (CFGs). CFGs leverage the structural information of a program to represent the possible paths of execution as a graph, where nodes represent instructions and edges represent control flow dependencies. Machine learning (ML) algorithms are being used to extract these features from CFGs and classify them as malicious or benign. In this survey, we aim to review some state-of-the-art methods for malware detection through CFGs using ML, focusing on the different ways of extracting, representing, and classifying. Specifically, we present a comprehe
    
[^94]: 脑肿瘤分割（BraTS）挑战赛2023：通过修复生成健康脑组织的局部合成。

    The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting. (arXiv:2305.08992v1 [eess.IV])

    [http://arxiv.org/abs/2305.08992](http://arxiv.org/abs/2305.08992)

    该论文介绍了BraTS 2023修复挑战，要求参与者使用修补技术从有病变的脑部扫描中合成健康脑扫描，以解决许多算法无法分析病变图像的问题。

    

    为了支持临床医生的决策，提供了许多自动分析脑部MR图像的算法。对于脑肿瘤患者，图像采集时间序列通常始于已经病理性的扫描。这会带来问题，因为许多算法是设计用于分析健康的大脑图像，并且没有为包含病变的图像提供保证。例如，进行脑部解剖分割、组织分割和脑部提取的算法。为解决这个问题，我们引入了BraTS 2023修复挑战。在这里，参与者需要探索修复技术，从有病变的扫描中合成健康的脑部扫描。下面的手稿包含了任务公式、数据集和提交程序。之后会更新以总结挑战的结果。这个挑战是作为BraTS 2023挑战的一部分，由加拿大温哥华MICCAI 2023会议主办。

    A myriad of algorithms for the automatic analysis of brain MR images is available to support clinicians in their decision-making. For brain tumor patients, the image acquisition time series typically starts with a scan that is already pathological. This poses problems, as many algorithms are designed to analyze healthy brains and provide no guarantees for images featuring lesions. Examples include but are not limited to algorithms for brain anatomy parcellation, tissue segmentation, and brain extraction. To solve this dilemma, we introduce the BraTS 2023 inpainting challenge. Here, the participants' task is to explore inpainting techniques to synthesize healthy brain scans from lesioned ones. The following manuscript contains the task formulation, dataset, and submission procedure. Later it will be updated to summarize the findings of the challenge. The challenge is organized as part of the BraTS 2023 challenge hosted at the MICCAI 2023 conference in Vancouver, Canada.
    
[^95]: 异构数据集的联邦学习方法

    Federated Learning over Harmonized Data Silos. (arXiv:2305.08985v1 [cs.LG])

    [http://arxiv.org/abs/2305.08985](http://arxiv.org/abs/2305.08985)

    该论文提出了一种解决异构数据集的联邦学习方法，通过将数据协调和数据填补等关键步骤纳入架构愿景，来促进数据管理信息系统和机器学习的交叉研究。

    

    联邦学习是一种分布式机器学习方法，它可以使地理分布的数据集共同学习一个机器学习模型，而无需共享数据。然而，现有的大部分工作都是基于非结构化数据（如图像或文本），或者基于在不同的站点上具有一致结构的结构化数据。然而，实际情况是各站点具有不同的模式、数据格式、数据值和访问模式，因此需要数据集成方法来解决这些挑战，包括使用声明性模式映射进行数据交换和查询重写，以及实体连接等。因此，我们提出了一个集成了数据协调和数据填补等关键步骤的端到端联邦学习和集成系统的架构愿景，以促进数据管理信息系统和机器学习的交叉研究。

    Federated Learning is a distributed machine learning approach that enables geographically distributed data silos to collaboratively learn a joint machine learning model without sharing data. Most of the existing work operates on unstructured data, such as images or text, or on structured data assumed to be consistent across the different sites. However, sites often have different schemata, data formats, data values, and access patterns. The field of data integration has developed many methods to address these challenges, including techniques for data exchange and query rewriting using declarative schema mappings, and for entity linkage. Therefore, we propose an architectural vision for an end-to-end Federated Learning and Integration system, incorporating the critical steps of data harmonization and data imputation, to spur further research on the intersection of data management information systems and machine learning.
    
[^96]: 基于自编码器的流数据异常检测方法：增量学习和概念漂移适应

    Autoencoder-based Anomaly Detection in Streaming Data with Incremental Learning and Concept Drift Adaptation. (arXiv:2305.08977v1 [cs.LG])

    [http://arxiv.org/abs/2305.08977](http://arxiv.org/abs/2305.08977)

    本文提出了一种基于自编码器的增量学习方法与漂移检测的异常检测方法（strAEm++DD），用于解决无标签的流数据中的异常检测问题，该方法在实验研究中表现出不俗的性能表现并优于现有的基线和先进方法。

    

    在当今数字宇宙中，大量无标签数据以各种方式进行流式生成。在这种情况下，识别异常等罕见事件是一个巨大的挑战，而在非平稳环境中尤其困难，这可能会导致模型预测性能的恶化。为了解决以上挑战，本文提出了一种基于自编码器的增量学习方法与漂移检测的异常检测方法（strAEm++DD）。我们使用真实世界和合成数据集进行了实验研究，并对strAEm++DD进行了经验分析。我们进一步进行了比较研究，证明了所提出的方法明显优于现有的基线和先进方法。

    In our digital universe nowadays, enormous amount of data are produced in a streaming manner in a variety of application areas. These data are often unlabelled. In this case, identifying infrequent events, such as anomalies, poses a great challenge. This problem becomes even more difficult in non-stationary environments, which can cause deterioration of the predictive performance of a model. To address the above challenges, the paper proposes an autoencoder-based incremental learning method with drift detection (strAEm++DD). Our proposed method strAEm++DD leverages on the advantages of both incremental learning and drift detection. We conduct an experimental study using real-world and synthetic datasets with severe or extreme class imbalance, and provide an empirical analysis of strAEm++DD. We further conduct a comparative study, showing that the proposed method significantly outperforms existing baseline and advanced methods.
    
[^97]: 不使用反向传播训练神经网络：深入探究似然比方法

    Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method. (arXiv:2305.08960v1 [cs.LG])

    [http://arxiv.org/abs/2305.08960](http://arxiv.org/abs/2305.08960)

    提出一种新的似然比方法来训练神经网络，无需使用递归梯度计算，并在多种神经网络架构上有效地减少了对抗性攻击对模型造成的影响。

    

    反向传播是深度学习中训练神经网络的最重要的梯度估计方法。然而，文献表明，通过反向传播训练的神经网络容易受到对抗性攻击。我们开发了似然比方法，这是一种新的梯度估计方法，可以训练广泛的神经网络架构，包括卷积神经网络、循环神经网络、图神经网络和脉冲神经网络，而无需递归梯度计算。我们提出了三种方法来有效地减少神经网络训练过程中梯度估计的方差。我们的实验在多个数据集上训练不同的神经网络，并得到了数值结果。所有结果都表明，相对于反向传播方法，似然比方法对抗性攻击下有效地训练了各种神经网络，并显着提高了神经网络的鲁棒性。

    Backpropagation (BP) is the most important gradient estimation method for training neural networks in deep learning. However, the literature shows that neural networks trained by BP are vulnerable to adversarial attacks. We develop the likelihood ratio (LR) method, a new gradient estimation method, for training a broad range of neural network architectures, including convolutional neural networks, recurrent neural networks, graph neural networks, and spiking neural networks, without recursive gradient computation. We propose three methods to efficiently reduce the variance of the gradient estimation in the neural network training process. Our experiments yield numerical results for training different neural networks on several datasets. All results demonstrate that the LR method is effective for training various neural networks and significantly improves the robustness of the neural networks under adversarial attacks relative to the BP method.
    
[^98]: 基于模块化动作程序的动作问答

    Motion Question Answering via Modular Motion Programs. (arXiv:2305.08953v1 [cs.CV])

    [http://arxiv.org/abs/2305.08953](http://arxiv.org/abs/2305.08953)

    提出了一种新的HumanMotionQA任务，用于评估模型在复杂的人体运动序列上进行复杂、多步推理的能力。同时，提出了一种特殊的NSPose方法，利用符号化推理和模块化设计，成功地应用于该任务中，并超过所有基线方法。

    

    为了构建能够感知和理解真实世界中的人类行为的人工智能系统，我们必须首先设计模型，对动作序列进行复杂的时空推理。为了实现这个目标，我们提出了HumanMotionQA任务，评估模型在长时间人类运动序列上进行复杂、多步推理的能力。我们生成了一个问答对数据集，需要在动作序列的小部分中检测运动线索，对事件发生的时间进行推理，并查询特定的运动属性。此外，我们还提出了NSPose，一种神经符号方法，用于处理该任务，它利用符号化推理和模块化设计，通过学习运动概念、属性神经操作符和时间关系，来处理动作。我们证明了NSPose在HumanMotionQA任务中的适用性，胜过了所有基线方法。

    In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
    
[^99]: 鲁棒性神经网络因果分析方法用于解释性研究

    Causal Analysis for Robust Interpretability of Neural Networks. (arXiv:2305.08950v1 [cs.LG])

    [http://arxiv.org/abs/2305.08950](http://arxiv.org/abs/2305.08950)

    本文提出了一种基于因果分析的鲁棒干预方法，用于解释神经网络的决策，避免噪音的干扰。

    

    神经网络内部的解释对于这些黑盒模型的可靠开发和部署至关重要。然而，以往的解释方法集中在基于相关性的度量上，以将模型决策归因于个别示例。然而，这些方法容易受到训练阶段中编码在模型中的噪声和虚假相关性的影响（例如，有偏输入，模型过拟合或错配）。此外，这个过程已经证明会产生嘈杂和不稳定的归因，从而阻碍了对模型行为的透明理解。本文开发了一种基于因果分析的鲁棒干预方法，用于捕捉预训练神经网络中的因果机制及其与预测的关系。我们的方法依赖于路径干预，以推断隐藏层中的因果机制并隔离相关和必要的信息（以进行模型预测），从而避免噪音的干扰。结果是针对特定任务的稳健且可靠的解释。

    Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model's behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specifi
    
[^100]: MIMEx: 遮盖输入建模的内在奖励

    MIMEx: Intrinsic Rewards from Masked Input Modeling. (arXiv:2305.08932v1 [cs.LG])

    [http://arxiv.org/abs/2305.08932](http://arxiv.org/abs/2305.08932)

    MIMEx是一个通用的框架，它使用遮盖输入建模来提取内在奖励，通过控制遮盖分布来控制难度，可以在高维环境中取得优越的探索结果。

    

    在高维观测环境中进行探索很困难。使用内在奖励的一种有前途的方法是使用深度网络估计状态、转换或轨迹的“新颖性”。之前的研究表明，条件预测目标，如遮盖自动编码可看作伪似然的随机估计。我们展示了这一观点如何自然地导致对现有内在奖励方法的统一看法:它们是条件预测的特例，在这种情况下，新颖性的估计可以看作是使用不同的遮盖分布进行伪似然估计。从这个角度，我们提出了一个通用的框架——遮盖输入建模探索内在奖励(MIMEx)，其中遮盖分布可以灵活调整以控制底层条件预测任务的难度。我们演示了当与竞争方法相比时，MIMEx可以取得优越的结果。

    Exploring in environments with high-dimensional observations is hard. One promising approach for exploration is to use intrinsic rewards, which often boils down to estimating "novelty" of states, transitions, or trajectories with deep networks. Prior works have shown that conditional prediction objectives such as masked autoencoding can be seen as stochastic estimation of pseudo-likelihood. We show how this perspective naturally leads to a unified view on existing intrinsic reward approaches: they are special cases of conditional prediction, where the estimation of novelty can be seen as pseudo-likelihood estimation with different mask distributions. From this view, we propose a general framework for deriving intrinsic rewards -- Masked Input Modeling for Exploration (MIMEx) -- where the mask distribution can be flexibly tuned to control the difficulty of the underlying conditional prediction task. We demonstrate that MIMEx can achieve superior results when compared against competitive
    
[^101]: AF2-Mutation：针对AlphaFold2的蛋白三级结构预测的对抗序列突变

    AF2-Mutation: Adversarial Sequence Mutations against AlphaFold2 on Protein Tertiary Structure Prediction. (arXiv:2305.08929v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.08929](http://arxiv.org/abs/2305.08929)

    本文研究了针对AlphaFold2的对抗序列突变，实测仅修改三个氨基酸残基，就使其预测的蛋白质三级结构的改变达到了46.61，并能成功发现在特定蛋白质中对结构至关重要且有生物学意义的氨基酸残基，为蛋白质的修饰提供了新思路。

    

    基于深度学习的方法，如AlphaFold2（AF2），取得了与真实生物实验方法可比的蛋白质三级结构预测效果。虽然AF2在预测变异效果方面存在局限性，但其对序列变异的鲁棒性尚待确定。本文从野生型（WT）序列开始，通过进化方法生成对抗序列，AF2预测与WT存在显著差异。我们在CASP14上的实验证明，在蛋白质序列中仅修改3个氨基酸残基，采用替换、删除和插入策略的组合，AF2预测的本地距离差异测试（lDDT）的改变达到了46.61。此外，当应用于特定蛋白质SPNS2时，我们提出的算法成功地识别了对蛋白质结构确定至关重要且有生物学意义的氨基酸残基，可能表明可修改的蛋白质部位。

    Deep learning-based approaches, such as AlphaFold2 (AF2), have significantly advanced protein tertiary structure prediction, achieving results comparable to real biological experimental methods. While AF2 has shown limitations in predicting the effects of mutations, its robustness against sequence mutations remains to be determined. Starting with the wild-type (WT) sequence, we investigate adversarial sequences generated via an evolutionary approach, which AF2 predicts to be substantially different from WT. Our experiments on CASP14 reveal that by modifying merely three residues in the protein sequence using a combination of replacement, deletion, and insertion strategies, the alteration in AF2's predictions, as measured by the Local Distance Difference Test (lDDT), reaches 46.61. Moreover, when applied to a specific protein, SPNS2, our proposed algorithm successfully identifies biologically meaningful residues critical to protein structure determination and potentially indicates alter
    
[^102]: 差分卷积模糊时间序列预测

    Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v1 [cs.LG])

    [http://arxiv.org/abs/2305.08890](http://arxiv.org/abs/2305.08890)

    本文提出了一种新的预测模型DFCNN，利用卷积神经网络实现具有可学习能力的FTSF，并能够处理非平稳时间序列。

    

    模糊时间序列预测（FTSF）是一种具有广泛应用的典型预测方法。传统的FTSF被认为是一种专家系统，导致失去了识别未定义特征的能力，这是FTSF预测不准确的主要原因。为了解决这个问题，提出了差分模糊卷积神经网络（DFCNN）模型，利用卷积神经网络实现具有可学习能力的FTSF。DFCNN能够识别潜在信息并改善预测精度。由于神经网络的可学习能力，FTSF建立的模糊规则的长度可以任意扩展，这是专家系统所无法处理的。同时，由于非平稳时间序列的趋势，FTSF通常无法实现令人满意的性能。非平稳时间序列的趋势会导致FTSF建立的模糊集失效，并导致预测失败。DFCNN利用卷积神经网络的学习能力，可以处理非平稳时间序列。

    Fuzzy time series forecasting (FTSF) is a typical forecasting method with wide application. Traditional FTSF is regarded as an expert system which leads to lose the ability to recognize undefined feature. The mentioned is main reason of poor forecasting with FTSF. To solve the problem, the proposed model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes convolution neural network to re-implement FTSF with learnable ability. DFCNN is capable of recognizing the potential information and improve the forecasting accuracy. Thanks to learnable ability of neural network, length of fuzzy rules established in FTSF is expended to arbitrary length which expert is not able to be handle by expert system. At the same time, FTSF usually cannot achieve satisfactory performance of non-stationary time series due to trend of non-stationary time series. The trend of non-stationary time series causes the fuzzy set established by FTSF to invalid and cause the forecasting to fail. DFCNN utiliz
    
[^103]: 新数据的新方法？人力资源管理研究中定量归纳方法的概述与示范

    New methods for new data? An overview and illustration of quantitative inductive methods for HRM research. (arXiv:2305.08889v1 [cs.LG])

    [http://arxiv.org/abs/2305.08889](http://arxiv.org/abs/2305.08889)

    本文提供了一种在“大数据”时代的人力资源管理研究方法论挑战的潜在解决方案：定量归纳方法，例如因子分析、聚类分析和潜在类别分析。

    

    “数据是新的石油”，简言之，数据是持续进行的第四次工业革命的重要来源，这导致一些评论员过于快速地将数据量视为财富本身，并认为大数据的发展是利润的几乎直接原因。人力资源管理也没有逃脱这一趋势，对员工大量数据的积累被某些企业家视为构建预测复杂工作行为（如旷工或工作绩效）的模型所必需的充分条件。事实上，这种类比有点误导人：与石油不同，这里不存在有关数据产生的重大问题（其流量由各种信息系统持续且低成本地生成），而是它们的“精炼”，即将这些数据转化为有用的产品，即知识所需的操作。这种转变是人力资源管理研究方法论挑战所在。在本文中，我们提供了定量归纳方法的概述与示范，例如因子分析、聚类分析和潜在类别分析。我们认为这些方法为“大数据”时代的人力资源管理研究提供了潜在的前进方向。

    "Data is the new oil", in short, data would be the essential source of the ongoing fourth industrial revolution, which has led some commentators to assimilate too quickly the quantity of data to a source of wealth in itself, and consider the development of big data as an quasi direct cause of profit. Human resources management is not escaping this trend, and the accumulation of large amounts of data on employees is perceived by some entrepreneurs as a necessary and sufficient condition for the construction of predictive models of complex work behaviors such as absenteeism or job performance. In fact, the analogy is somewhat misleading: unlike oil, there are no major issues here concerning the production of data (whose flows are generated continuously and at low cost by various information systems), but rather their ''refining'', i.e. the operations necessary to transform this data into a useful product, namely into knowledge. This transformation is where the methodological challenges o
    
[^104]: 变量距离加权回归（CWR）：房价估计的案例研究

    Covariate-distance Weighted Regression (CWR): A Case Study for Estimation of House Prices. (arXiv:2305.08887v1 [cs.LG])

    [http://arxiv.org/abs/2305.08887](http://arxiv.org/abs/2305.08887)

    本文提出了一种变量距离加权回归（CWR）方法，将地理距离和属性距离相结合，可以用来预测房价。CWR方法考虑到地质和属性距离，可以产生准确的房价估计。

    

    地理加权回归（GWR）是建立回归模型中空间异质性的常用工具。然而，目前GWR中的权重函数仅考虑地理距离，而属性相似性则被完全忽略。本研究提出了一种变量加权函数，将地理距离和属性距离结合起来。变量距离加权回归（CWR）是GWR的扩展，包括地理距离和属性距离。房价受到诸多因素的影响，如房龄、建筑面积和土地用途等。预测模型用于帮助了解区域房价的特征。CWR被用来了解房价与控制因素之间的关系。CWR可以考虑地质和属性距离，并产生保留地质和属性距离函数权重矩阵的房价准确估计。结果表明CWR在房价估计中表现出了非常好的性能。

    Geographically weighted regression (GWR) is a popular tool for modeling spatial heterogeneity in a regression model. However, the current weighting function used in GWR only considers the geographical distance, while the attribute similarity is totally ignored. In this study, we proposed a covariate weighting function that combines the geographical distance and attribute distance. The covariate-distance weighted regression (CWR) is the extension of GWR including geographical distance and attribute distance. House prices are affected by numerous factors, such as house age, floor area, and land use. Prediction model is used to help understand the characteristics of regional house prices. The CWR was used to understand the relationship between the house price and controlling factors. The CWR can consider the geological and attribute distances, and produce accurate estimates of house price that preserve the weight matrix for geological and attribute distance functions. Results show that th
    
[^105]: 利用数据挖掘技术识别影响建筑物能耗和成本降低的因素

    Identification of the Factors Affecting the Reduction of Energy Consumption and Cost in Buildings Using Data Mining Techniques. (arXiv:2305.08886v1 [cs.LG])

    [http://arxiv.org/abs/2305.08886](http://arxiv.org/abs/2305.08886)

    本研究利用数据挖掘技术实现了对建筑物成本降低和能耗影响因素的识别，结果表明隔热、建筑物尺寸和制冷系统类型是影响能源消耗和成本降低的关键因素。

    

    优化能耗和协调公用事业系统一直是建筑行业关注的焦点。建筑是世界上最大的能源消耗者之一，其能效对于防止浪费和降低成本至关重要。此外，建筑物产生大量的原始数据，可以用于了解能源消耗模式并协助开发优化策略。本研究利用真实世界的数据集，旨在识别影响建筑物成本降低和能耗的因素。为实现这一目标，我们利用三种回归模型（Lasso回归、决策树和随机森林）来预测建筑物的主要燃料使用、电能消耗和成本节省。进行了一项影响能耗和成本降低因素的分析，并使用元启发式算法优化决策树算法。通过使用元启发技术，我们对决策树算法进行微调，以实现更好的模型精度和可解释性。结果表明，影响建筑物能耗和成本降低的关键因素是隔热、建筑物尺寸和使用的制冷系统类型。

    Optimizing energy consumption and coordination of utility systems have long been a concern of the building industry. Buildings are one of the largest energy consumers in the world, making their energy efficiency crucial for preventing waste and reducing costs. Additionally, buildings generate substantial amounts of raw data, which can be used to understand energy consumption patterns and assist in developing optimization strategies. Using a real-world dataset, this research aims to identify the factors that influence building cost reduction and energy consumption. To achieve this, we utilize three regression models (Lasso Regression, Decision Tree, and Random Forest) to predict primary fuel usage, electrical energy consumption, and cost savings in buildings. An analysis of the factors influencing energy consumption and cost reduction is conducted, and the decision tree algorithm is optimized using metaheuristics. By employing metaheuristic techniques, we fine-tune the decision tree alg
    
[^106]: 智能家居能源管理：VAE-GAN合成数据集生成器和Q学习

    Smart Home Energy Management: VAE-GAN synthetic dataset generator and Q-learning. (arXiv:2305.08885v1 [cs.LG])

    [http://arxiv.org/abs/2305.08885](http://arxiv.org/abs/2305.08885)

    该论文介绍了一种能够生成智能家居能耗数据的VAE-GAN技术，同时探索将生成模型与Q学习结合应用于基于Q学习的HEMS中的表现，并且证明了VAE-GAN技术能够有效地训练HEMS，为智能家居节能带来了新的应用前景。

    

    近年来，学术界和工业界对分析住宅建筑物的电力消耗并采用智能家居能源管理系统（HEMS）以减少家庭能源消耗和成本的兴趣日益增加。 HEMS已被开发用于模拟实际智能电网的统计和功能属性。公共可用数据集的获取是这类研究面临的主要挑战。合成系统的不同操作条件所代表的时间序列的开发将进一步增强人工HEMS应用的潜力。在本文中，我们提出了一种新颖的VAE-GAN技术，用于生成智能家居能耗的时间序列数据。 我们还探索了在基于Q学习的HEMS中将生成模型与之相结合的表现。我们使用实际的智能家居数据测试了基于Q学习的HEMS的在线表现。为了测试生成的数据，我们在受控环境中将生成的时间序列数据作为Q学习基础的HEMS的输入。实验结果表明，VAE-GAN技术可以生成逼真的时间序列，并可用于有效地训练HEMS。

    Recent years have noticed an increasing interest among academia and industry towards analyzing the electrical consumption of residential buildings and employing smart home energy management systems (HEMS) to reduce household energy consumption and costs. HEMS has been developed to simulate the statistical and functional properties of actual smart grids. Access to publicly available datasets is a major challenge in this type of research. The potential of artificial HEMS applications will be further enhanced with the development of time series that represent different operating conditions of the synthetic systems. In this paper, we propose a novel variational auto-encoder-generative adversarial network (VAE-GAN) technique for generating time-series data on energy consumption in smart homes. We also explore how the generative model performs when combined with a Q-learning-based HEMS. We tested the online performance of Q-learning-based HEMS with real-world smart home data. To test the gen
    
[^107]: 学习未学特征用于脑肿瘤分割

    Learning to Learn Unlearned Feature for Brain Tumor Segmentation. (arXiv:2305.08878v1 [eess.IV])

    [http://arxiv.org/abs/2305.08878](http://arxiv.org/abs/2305.08878)

    提出一种针对脑肿瘤分割的微调算法，基于主动学习和元学习。通过迁移学习方法将高级别胶质瘤应用于脑转移瘤，几步之内实现了胶质瘤和脑转移瘤领域的平衡参数。

    

    我们提出了一种针对脑肿瘤分割的微调算法，只需要少量的数据样本并帮助网络不忘记原始任务。我们的方法基于主动学习和元学习。医学图像分割面临的困难之一是缺乏适当注释的数据集，因为需要医生对可靠的注释进行标记，并且有许多疾病的变体，例如胶质瘤和脑转移瘤，这些是不同类型的脑肿瘤，在MR图像中具有不同的结构特征。因此，不可能为所有类型的疾病产生大规模的医学图像数据集。在本文中，我们展示了从高级别胶质瘤到脑转移瘤的迁移学习方法，并证明所提出的算法在几步之内实现了胶质瘤和脑转移瘤领域的平衡参数。

    We propose a fine-tuning algorithm for brain tumor segmentation that needs only a few data samples and helps networks not to forget the original tasks. Our approach is based on active learning and meta-learning. One of the difficulties in medical image segmentation is the lack of datasets with proper annotations, because it requires doctors to tag reliable annotation and there are many variants of a disease, such as glioma and brain metastasis, which are the different types of brain tumor and have different structural features in MR images. Therefore, it is impossible to produce the large-scale medical image datasets for all types of diseases. In this paper, we show a transfer learning method from high grade glioma to brain metastasis, and demonstrate that the proposed algorithm achieves balanced parameters for both glioma and brain metastasis domains within a few steps.
    
[^108]: 神经符号人工智能及其分类法：一项调查研究

    Neurosymbolic AI and its Taxonomy: a survey. (arXiv:2305.08876v1 [cs.NE])

    [http://arxiv.org/abs/2305.08876](http://arxiv.org/abs/2305.08876)

    本文调查研究了神经符号人工智能的研究，探索了学习数据分布和推理先前和学习知识相结合的方法，以探索实现人工智能通用性的替代方案。

    

    神经符号人工智能涉及组合符号处理（如经典人工智能）和神经网络的模型，是一个非常成熟的领域。这些模型作为实现人工智能通用性的一种尝试，在探索除了增加数据集和模型尺寸以外的替代方案以及将学习数据分布和推理先前和学习知识相结合方面具有独特作用。本次调查研究了这一领域近年来的研究论文，并提供了这些模型的分类和比较，同时介绍了应用案例。

    Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.
    
[^109]: 在串行数据同化中，基于在线机器学习的预测不确定性估计

    Online machine-learning forecast uncertainty estimation for sequential data assimilation. (arXiv:2305.08874v1 [physics.ao-ph])

    [http://arxiv.org/abs/2305.08874](http://arxiv.org/abs/2305.08874)

    该论文提出了一种使用卷积神经网络进行在线机器学习的方法，在单个动力学模型集成中估计表示预测误差协方差矩阵的状态依赖预测不确定性，并在混合数据同化方法中应用。通过模拟实验证明，该方法在准确性和计算成本方面优于传统的基于集合的方法。

    

    量化预测不确定性是现代数值天气预报和数据同化系统中的关键。基于集合的数据同化系统采用多个模型集成来包含状态依赖的不确定性量化,但这种方法在计算和开发方面具有挑战性。本文提出了一种基于卷积神经网络的机器学习方法，使用单个动力学模型集成估计表示预测误差协方差矩阵的状态依赖预测不确定性。这是通过利用损失函数实现的，该函数考虑了预测误差的异方差性。该方法的性能在混合数据同化方法中进行了研究，该方法结合了Kalman样式分析更新和基于机器学习的状态依赖预测误差协方差矩阵的估计。使用Lorensso等（2006）模型进行了观测系统模拟实验，结果表明，与常用的基于集合的方法相比，机器学习方法在准确性和计算成本方面表现更好。

    Quantifying forecast uncertainty is a key aspect of state-of-the-art numerical weather prediction and data assimilation systems. Ensemble-based data assimilation systems incorporate state-dependent uncertainty quantification based on multiple model integrations. However, this approach is demanding in terms of computations and development. In this work a machine learning method is presented based on convolutional neural networks that estimates the state-dependent forecast uncertainty represented by the forecast error covariance matrix using a single dynamical model integration. This is achieved by the use of a loss function that takes into account the fact that the forecast errors are heterodastic. The performance of this approach is examined within a hybrid data assimilation method that combines a Kalman-like analysis update and the machine learning based estimation of a state-dependent forecast error covariance matrix. Observing system simulation experiments are conducted using the Lo
    
[^110]: AMULET：自适应矩阵乘法类任务

    AMULET: Adaptive Matrix-Multiplication-Like Tasks. (arXiv:2305.08872v1 [cs.PL])

    [http://arxiv.org/abs/2305.08872](http://arxiv.org/abs/2305.08872)

    AMULET是一个开源编译器，通过利用数据库式和编译器优化技术来优化矩阵乘法类任务，从而在执行环境下生成快速代码，相比现有编译器， AMULTET能够在各种矩阵乘法类任务上实现加速，并可以和手动调优的矩阵乘法库相媲美。

    

    在数据科学和机器学习应用中，许多有用的任务可以写成矩阵乘法的简单变种。然而，现有的矩阵/向量库只支持一类特定的计算，并为每个独特的硬件平台手动调优，因此用户很难执行这些任务。用户可以将任务编写为简单的嵌套循环，但当前的编译器并不足够复杂，无法为以这种方式编写的任务生成快速的代码。为了解决这些问题，我们扩展了一个开源编译器，以识别并优化这些矩阵乘法类任务。我们的框架称为AMULET，利用数据库式和编译器优化技术来生成针对其执行环境定制的快速代码。通过实验，我们证明AMULET在各种矩阵乘法类任务上实现了加速，相比现有的编译器，AMULET通常可以在15％以内的误差范围内与手动调优的矩阵乘法库相媲美。

    Many useful tasks in data science and machine learning applications can be written as simple variations of matrix multiplication. However, users have difficulty performing such tasks as existing matrix/vector libraries support only a limited class of computations hand-tuned for each unique hardware platform. Users can alternatively write the task as a simple nested loop but current compilers are not sophisticated enough to generate fast code for the task written in this way. To address these issues, we extend an open-source compiler to recognize and optimize these matrix multiplication-like tasks. Our framework, called Amulet, uses both database-style and compiler optimization techniques to generate fast code tailored to its execution environment. We show through experiments that Amulet achieves speedups on a variety of matrix multiplication-like tasks compared to existing compilers. For large matrices Amulet typically performs within 15% of hand-tuned matrix multiplication libraries, 
    
[^111]: 野外视觉导航中的快速可通行性估计

    Fast Traversability Estimation for Wild Visual Navigation. (arXiv:2305.08510v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.08510](http://arxiv.org/abs/2305.08510)

    本文提出了一种基于自监督学习的在线视觉导航系统 WVN，能够在不到 5 分钟的实地培训时间内启动可通过地形分割，并使机器人能够高效地在挑战性环境下完成导航。

    

    自然环境如森林和草地对于机器人导航来说是具有挑战性的，因为高草、树枝或灌木会产生虚假的障碍物感知。本文提出了一种名为 WVN 的在线自监督学习系统，用于仅使用视觉进行的可通行性估计。该系统能够从实地短时间的人类演示中不断自适应。它利用自监督视觉转换器模型的高维特征，采用在线监督生成方案来实现机器人上的实时运行。我们在森林、公园和草地的具有挑战性的环境中进行了实验和消融研究，证明了我们方法的优势。我们的系统在不到5分钟的实地培训时间内能够启动可通过地形分割，使机器人能够在复杂的户外地形中导航-克服高野草的障碍，以及跟随1.4公里的人行道。虽然我们的实验数据均来自欧洲，但是我们认为结果仍然具有一定的普适性。

    Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we propose Wild Visual Navigation (WVN), an online self-supervised learning system for traversability estimation which uses only vision. The system is able to continuously adapt from a short human demonstration in the field. It leverages high-dimensional features from self-supervised visual transformer models, with an online scheme for supervision generation that runs in real-time on the robot. We demonstrate the advantages of our approach with experiments and ablation studies in challenging environments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex outdoor terrains - negotiating obstacles in high grass as well as a 1.4 km footpath following. While our ex
    
[^112]: 通用随机神经网络动力学平均场理论入门

    Introduction to dynamical mean-field theory of generic random neural networks. (arXiv:2305.08459v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2305.08459](http://arxiv.org/abs/2305.08459)

    本文针对通用随机神经网络，介绍了动力学平均场理论及其数值实现方法，并引入了一种物理上透明的替代方法，以探索网络的集体动态。

    

    动力学平均场理论是一种强大的物理工具，用于分析神经网络的典型行为，其中神经元可以循环连接，或者可以堆叠多层神经元。然而，对于初学者来说，很难接触到此工具和基础物理的精髓。在这里，我们以通用随机神经网络为例，给出了这种方法的教育性介绍，在此类网络中，神经元通过相关突触随机而完全连接，因此网络表现出丰富的集体动态。我们还回顾了应用此工具的相关过去和最近重要作品。此外，还介绍了一种物理上透明的替代方法，即动态腔方法，该方法导出了完全相同的结果。详细介绍了求解积分微分平均场方程的数值实现，以及探索波动耗散定理的说明。

    Dynamical mean-field theory is a powerful physics tool used to analyze the typical behavior of neural networks, where neurons can be recurrently connected, or multiple layers of neurons can be stacked. However, it is not easy for beginners to access the essence of this tool and the underlying physics. Here, we give a pedagogical introduction of this method in a particular example of generic random neural networks, where neurons are randomly and fully connected by correlated synapses and therefore the network exhibits rich emergent collective dynamics. We also review related past and recent important works applying this tool. In addition, a physically transparent and alternative method, namely the dynamical cavity method, is also introduced to derive exactly the same results. The numerical implementation of solving the integro-differential mean-field equations is also detailed, with an illustration of exploring the fluctuation dissipation theorem.
    
[^113]: 一种基于机器学习的含湿度环氧纳米复合材料粘弹-粘塑性模型

    A machine learning-based viscoelastic-viscoplastic model for epoxy nanocomposites with moisture content. (arXiv:2305.08102v1 [cs.LG])

    [http://arxiv.org/abs/2305.08102](http://arxiv.org/abs/2305.08102)

    本文提出了一种运用深度学习的本构模型，能够准确的捕捉与速率相关的应力-应变关系和一致的切线模量，以研究含湿度纳米颗粒/环氧纳米复合材料的循环粘弹-粘塑性-破坏行为。

    

    本文提出了一种基于深度学习的本构模型，用于研究含湿度纳米颗粒/环氧纳米复合材料的循环粘弹-粘塑性-破坏行为。采用一种采样技术和扰动方法的组合框架训练长短期记忆网络，将实验验证的粘弹-粘塑性模型生成的训练数据用于训练模型，使得深度学习模型能够准确捕捉到与加载速率相关的应力-应变关系和一致的切线模量。此外，将基于深度学习的本构模型实现到有限元分析中。通过有限元模拟研究了加载速率和湿度含量对纳米颗粒/环氧样品的力-位移响应的影响。数值实例表明，基于深度学习的模型的计算效率取决于加载条件，并且明显优于传统本构模型。

    In this work, we propose a deep learning (DL)-based constitutive model for investigating the cyclic viscoelastic-viscoplastic-damage behavior of nanoparticle/epoxy nanocomposites with moisture content. For this, a long short-term memory network is trained using a combined framework of a sampling technique and a perturbation method. The training framework, along with the training data generated by an experimentally validated viscoelastic-viscoplastic model, enables the DL model to accurately capture the rate-dependent stress-strain relationship and consistent tangent moduli. In addition, the DL-based constitutive model is implemented into finite element analysis. Finite element simulations are performed to study the effect of load rate and moisture content on the force-displacement response of nanoparticle/ epoxy samples. Numerical examples show that the computational efficiency of the DL model depends on the loading condition and is significantly higher than the conventional constituti
    
[^114]: 基于表面肌电图像的轻量级全卷积神经网络和迁移学习的跨场景手势识别

    Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])

    [http://arxiv.org/abs/2305.08014](http://arxiv.org/abs/2305.08014)

    本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。

    

    利用低分辨率瞬时高清肌电图像进行手势识别可以开辟发展更流畅、更自然的肌肉-计算机界面的新途径。然而，跨场景数据的变异性存在极大的挑战。现有的方法采用非常大且复杂的深度卷积神经网络或基于2SRNN的领域适应方法，来逼近由这些跨场景数据变异性引起的分布偏移。因此，这些方法也需要在预训练和适应阶段中在数百万个训练参数和大规模预训练数据集上进行学习。结果，这使得在实时应用中进行高端资源约束和计算非常昂贵的部署。为了解决这个问题，我们提出了一种轻量级的全卷积神经网络+迁移学习模型，利用轻量级全卷积神经网络和迁移学习(TL)来增强跨场景手势识别。

    Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
    
[^115]: Text2Cohort: 自然语言队列发现对癌症影像数据共享平台的民主化

    Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])

    [http://arxiv.org/abs/2305.07637](http://arxiv.org/abs/2305.07637)

    Text2Cohort是一个基于大语言模型的工具箱，可以将用户输入转化为IDC数据库查询，促进自然语言队列发现，减少研究人员查询IDC数据库的学习曲线，实现了癌症成像数据的民主化。

    

    影像数据共享平台(IDC)是一个基于云的数据库，为研究人员提供开放获取的癌症成像数据和分析工具，旨在促进医学成像研究中的协作。然而，由于其复杂和技术性质，查询IDC数据库以进行队列发现和访问成像数据对研究人员来说具有显著的学习曲线。我们开发了基于大语言模型（LLM）的Text2Cohort工具箱，通过提示工程将用户输入转化为IDC数据库查询，并将查询的响应返回给用户，以促进自然语言队列发现。此外，实现了自动校正以解决查询中的语法和语义错误，通过将错误传回模型进行解释和校正。我们对50个自然语言用户输入进行了Text2Cohort评估，范围从信息提取到队列发现。结果查询和输出由两位计算机科学家进行了确认。

    The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
    
[^116]: 带有非对角信息的视觉-语言连续表示学习

    Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])

    [http://arxiv.org/abs/2305.07437](http://arxiv.org/abs/2305.07437)

    本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。

    

    本文讨论了通过流数据持续训练CLIP模型的可行性。通过追踪连续更新的CLIP模型中表示向量的方向变化，我们探索和总结了这些空间变化，称为空间混乱（SD），可以分为内部旋转和跨模态偏差。此外，我们从经验和理论上证明了内部旋转和跨模态偏差如何导致CLIP在跨模态检索任务中性能下降。为了缓解空间混乱，我们提出了一种简单而有效的连续学习框架Mod-X: 维护非对角信息矩阵。在各种不同规模和范围的常用数据集上的实验表明了我们方法的有效性。

    This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
    
[^117]: 具有门控汇总模块的值迭代网络研究

    Value Iteration Networks with Gated Summarization Module. (arXiv:2305.07039v1 [cs.LG])

    [http://arxiv.org/abs/2305.07039](http://arxiv.org/abs/2305.07039)

    本文提出了一种具有门控汇总模块的值迭代网络（GS-VIN）来解决值迭代网络在处理更大的输入地图和减轻累积误差方面的挑战。通过自适应迭代策略和门控汇总模块，这种模型可以在保持准确性的同时，减少网络深度并提高训练稳定性。

    

    本文针对值迭代网络（VIN）在处理更大的输入地图，减轻由增加迭代次数引起的累积误差的挑战提出了一种新方法——具有门控汇总模块的值迭代网络（GS-VIN）。我们提出自适应迭代策略，利用更大的卷积核减少迭代次数，减少网络深度，提高训练稳定性，同时保持计划过程的准确性。我们还引入了门控汇总模块，使得网络可以强调整个规划过程，而不仅仅依赖于最终的全局规划结果。

    In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments 
    
[^118]: 基于流形正则化 Tucker 分解的时空交通数据填充方法

    Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])

    [http://arxiv.org/abs/2305.06563](http://arxiv.org/abs/2305.06563)

    本文提出了一种基于流形正则化Tucker分解的时空交通数据填充方法，该方法利用稀疏正则化项改善了Tucker核的稀疏性，并引入流形正则化和时间约束项来优化张量的填充性能。

    

    时空交通数据填充(STDI)是数据驱动智能交通系统中不可避免和具有挑战性的任务，在部分观测到的交通数据中估计丢失数据。由于交通数据具有多维和时空性质，我们将丢失数据填充视为张量完成问题。过去十年中，许多关于基于张量分解的 STDI 的研究已经展开。然而，如何利用时空相关性和核张量稀疏性来改善填充性能仍然需要解决。本文重新构造了3/4阶汉克尔张量，并提出了一种创新的流形正则化 Tucker 分解(maniRTD)模型用于STDI。明确地，我们通过引入多维延迟嵌入变换将传感交通状态数据表示为3/4阶张量。然后，ManiRTD使用稀疏正则化项改善了Tucker核的稀疏性，并使用流形正则化和时间约束项来优化张量的填充性能。

    Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
    
[^119]: FedDWA: 个性化联邦学习与动态权重调整

    FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])

    [http://arxiv.org/abs/2305.06124](http://arxiv.org/abs/2305.06124)

    本文提出了一种个性化联邦学习算法，名为FedDWA，采用动态权重调整来保护数据隐私并以更少的通信开销捕捉客户之间的相似性，能够训练高精度和高效的个性化模型。

    

    与传统的联邦学习不同，个性化联邦学习（PFL）能够根据每个客户端的独特需求来训练定制化模型。主流方法是采用一种加权聚合方法来生成个性化模型，其中权重是由不同客户端之间的损失值或模型参数确定的。然而，这种方法要求客户端下载其他模型，不仅增加了通信流量，而且可能侵犯数据隐私。我们在本文中提出了一种新的PFL算法，称为FedDWA（带动态权重调整的联邦学习），来解决上述问题，该算法利用参数服务器（PS）根据从客户端收集的模型计算个性化聚合权重。这样，FedDWA可以以更少的通信开销捕捉客户之间的相似性。我们将PFL问题制定为一种优化问题，并通过引入动态权重调整机制设计了一种新算法。FedDWA能够学习高精度和高效的个性化模型，同时保护数据隐私。我们在综合合成和真实数据集上进行了广泛的实验，证明了FedDWA的有效性。

    Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others' models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called \emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization 
    
[^120]: BARA: 高效的在线奖励预算分配跨边缘联邦学习激励机制

    BARA: Efficient Incentive Mechanism with Online Reward Budget Allocation in Cross-Silo Federated Learning. (arXiv:2305.05221v1 [cs.LG])

    [http://arxiv.org/abs/2305.05221](http://arxiv.org/abs/2305.05221)

    BARA是一种在线奖励预算分配算法，用于激励跨边缘联邦学习中的数据所有者为模型训练做出贡献，并解决了现有研究中被忽略的奖励预算分配问题。

    

    联邦学习（FL）是一种有前途的分布式机器学习框架，可以保护数据隐私。跨边缘联邦学习通过交换多个通信循环的模型参数，使不同组织的孤立数据岛协作一个参数服务器（PS）来完成模型训练。在跨边缘FL中，激励机制对于激励数据所有者为FL训练做出贡献是必不可少的。然而，如何在不同的循环之间分配奖励预算是一个重要但被现有研究大量忽略的复杂问题。解决这个问题的挑战在于奖励预算分配和FL模型效用改进之间的不透明反馈，使得最优奖励预算分配变得复杂。为了解决这个问题，我们设计了一种使用贝叶斯优化的在线奖励预算分配算法，名为BARA（\underline{B}udget \underline{A}llocation for \underline{R}everse \underline{A}uction）。

    Federated learning (FL) is a prospective distributed machine learning framework that can preserve data privacy. In particular, cross-silo FL can complete model training by making isolated data islands of different organizations collaborate with a parameter server (PS) via exchanging model parameters for multiple communication rounds. In cross-silo FL, an incentive mechanism is indispensable for motivating data owners to contribute their models to FL training. However, how to allocate the reward budget among different rounds is an essential but complicated problem largely overlooked by existing works. The challenge of this problem lies in the opaque feedback between reward budget allocation and model utility improvement of FL, making the optimal reward budget allocation complicated. To address this problem, we design an online reward budget allocation algorithm using Bayesian optimization named BARA (\underline{B}udget \underline{A}llocation for \underline{R}everse \underline{A}uction).
    
[^121]: 利用深度学习和数字孪生技术提高建筑物能源性能

    Leveraging Deep Learning and Digital Twins to Improve Energy Performance of Buildings. (arXiv:2305.04498v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04498](http://arxiv.org/abs/2305.04498)

    本研究提出了Deep Energy Twin的解决方案，将深度学习和数字孪生相结合，以识别建筑物能源使用模式并提供优化能源效率的洞见。

    

    建筑物数字化转型积累了大量运营数据，需要智能化解决方案来利用这些数据来提高能源性能。本研究提出了一种解决方案，即“Deep Energy Twin”，将深度学习和数字孪生相结合，以更好地理解建筑物能源使用情况，并识别提高能源效率的潜力。采用本体论创建参数数字孪生，以提供建筑物中不同系统之间数据格式的一致性。基于创建的数字孪生和收集的数据，使用深度学习方法进行数据分析，以识别模式并为能源优化提供洞见。作为示例，本研究在瑞典诺尔肯平的一座公共历史建筑中进行了案例研究，比较了最先进的深度学习架构在建筑物能源预测中的性能。

    Digital transformation in buildings accumulates massive operational data, which calls for smart solutions to utilize these data to improve energy performance. This study has proposed a solution, namely Deep Energy Twin, for integrating deep learning and digital twins to better understand building energy use and identify the potential for improving energy efficiency. Ontology was adopted to create parametric digital twins to provide consistency of data format across different systems in a building. Based on created digital twins and collected data, deep learning methods were used for performing data analytics to identify patterns and provide insights for energy optimization. As a demonstration, a case study was conducted in a public historic building in Norrk\"oping, Sweden, to compare the performance of state-of-the-art deep learning architectures in building energy forecasting.
    
[^122]: PiML工具箱：可解释机器学习模型的开发和验证

    PiML Toolbox for Interpretable Machine Learning Model Development and Validation. (arXiv:2305.04214v1 [cs.LG])

    [http://arxiv.org/abs/2305.04214](http://arxiv.org/abs/2305.04214)

    PiML工具箱是一个综合的Python工具箱，可用于开发和诊断可解释机器学习模型，包括日益增长的可解释模型、模型无关的可解释性工具和模型无关的诊断工具，还支持与MLOps平台的集成和质量保证。

    

    PiML是一个综合且开放的Python工具箱，用于可解释机器学习模型的开发和模型诊断。它设计了低代码和高代码两种机器学习工作流，包括数据管道、模型训练、模型解释和说明以及模型诊断和比较。该工具箱支持日益增长的可解释模型（例如GAM、GAMI-Net、XGB2），具有本地和/或全局可解释性。它还支持模型无关的可解释性工具（例如PFI、PDP、LIME、SHAP）和一个强大的模型无关诊断套件（例如弱点、不确定性、鲁棒性、公平性）。通过灵活的高代码 API，将 PiML 模型和测试集成到现有的 MLOps 平台以实现质量保证。此外，PiML 工具箱还带有综合的用户指南和实践例子，包括银行业中的模型开发和验证应用。该项目可通过arXiv:2305.04214v1[cs.LG]获取。

    PiML (read $\pi$-ML, /`pai.`em.`el/) is an integrated and open-access Python toolbox for interpretable machine learning model development and model diagnostics. It is designed with machine learning workflows in both low-code and high-code modes, including data pipeline, model training, model interpretation and explanation, and model diagnostics and comparison. The toolbox supports a growing list of interpretable models (e.g. GAM, GAMI-Net, XGB2) with inherent local and/or global interpretability. It also supports model-agnostic explainability tools (e.g. PFI, PDP, LIME, SHAP) and a powerful suite of model-agnostic diagnostics (e.g. weakness, uncertainty, robustness, fairness). Integration of PiML models and tests to existing MLOps platforms for quality assurance are enabled by flexible high-code APIs. Furthermore, PiML toolbox comes with a comprehensive user guide and hands-on examples, including the applications for model development and validation in banking. The project is available
    
[^123]: FedNC：基于网络编码启发的安全高效联合学习方法

    FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding. (arXiv:2305.03292v1 [cs.LG])

    [http://arxiv.org/abs/2305.03292](http://arxiv.org/abs/2305.03292)

    本文提出FedNC，一个联合学习的通信框架，结合了网络编码技术，能够提高系统的隐私、吞吐量和鲁棒性。

    

    联合学习是一种有前途的分布式学习机制，但仍然面临两个主要挑战，即隐私泄漏和系统效率。在本文中，我们从网络信息理论的角度重新构思了联合学习系统，并制定了一个原创的联合学习通信框架FedNC，该框架受到网络编码的启发。 FedNC的主要思想是通过对原始数据包进行随机线性组合，将本地模型的信息混合在一起，然后再上传进行进一步的聚合，从而使FL系统更加安全，吞吐量更高，鲁棒性更好。据我们所知，这是第一个将NC引入FL的框架。随着FL在实际网络框架中的不断演变，可以基于FedNC进一步设计更多的应用和变体。

    Federated Learning (FL) is a promising distributed learning mechanism which still faces two major challenges, namely privacy breaches and system efficiency. In this work, we reconceptualize the FL system from the perspective of network information theory, and formulate an original FL communication framework, FedNC, which is inspired by Network Coding (NC). The main idea of FedNC is mixing the information of the local models by making random linear combinations of the original packets, before uploading for further aggregation. Due to the benefits of the coding scheme, both theoretical and experimental analysis indicate that FedNC improves the performance of traditional FL in several important ways, including security, throughput, and robustness. To the best of our knowledge, this is the first framework where NC is introduced in FL. As FL continues to evolve within practical network frameworks, more applications and variants can be further designed based on FedNC.
    
[^124]: 分布式协同功能：统一博弈论交互方法来解释机器学习

    Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability. (arXiv:2305.03100v1 [cs.LG])

    [http://arxiv.org/abs/2305.03100](http://arxiv.org/abs/2305.03100)

    本文提出了一种统一的框架，用于游戏理论驱动的归因和k阶交互方法，通过假设，可以在连续输入设置中得到唯一全面的特征交互解释，即协同作用。

    

    深度学习已经彻底改变了机器学习的许多领域，从计算机视觉到自然语言处理，但这些高性能模型通常是“黑盒子”。解释此类模型将提高AI决策透明度和信任，并且对于理解其他实际需求（如鲁棒性和公平性）是必要的。增强模型透明度的一种流行方法是量化单个输入对模型输出的贡献（称为归因）以及群组输入之间的相互作用的强度。越来越多的这些方法导入博弈论的概念和结果来产生归因和交互作用。本文提出了一个统一的框架，用于博弈论驱动的归因和k阶交互方法。我们展示了在连续输入设置中，假设适度，可以得到特征之间交互的唯一全面说明，即协同作用。我们确定了各种方法的特征。

    Deep learning has revolutionized many areas of machine learning, from computer vision to natural language processing, but these high-performance models are generally "black box." Explaining such models would improve transparency and trust in AI-powered decision making and is necessary for understanding other practical needs such as robustness and fairness. A popular means of enhancing model transparency is to quantify how individual inputs contribute to model outputs (called attributions) and the magnitude of interactions between groups of inputs. A growing number of these methods import concepts and results from game theory to produce attributions and interactions. This work presents a unifying framework for game-theory-inspired attribution and $k^\text{th}$-order interaction methods. We show that, given modest assumptions, a unique full account of interactions between features, called synergies, is possible in the continuous input setting. We identify how various methods are characte
    
[^125]: 无投影在线凸优化与随机约束

    Projection-Free Online Convex Optimization with Stochastic Constraints. (arXiv:2305.01333v1 [math.OC])

    [http://arxiv.org/abs/2305.01333](http://arxiv.org/abs/2305.01333)

    该论文提出了一种用于处理带有随机约束的在线凸优化的无投影算法，其能够取得亚线性遗憾和约束违规的效果。

    

    该论文提出了一种用于带有随机约束的在线凸优化的无投影算法。我们设计了一个在线原始-对偶无投影框架，该框架可以采用任何用于在线凸优化的无长期约束的无投影算法。使用该模板，我们推导出各种情况下的亚线性遗憾和约束违规边界。此外，对于损失函数和约束函数都光滑的情况，我们开发了一种原始-对偶条件梯度方法，实现了$O(\sqrt{T})$的遗憾和 $O(T^{3/4})$的约束违规。此外，对于损失函数和约束函数都是随机的情况，并且与关联的离线随机优化问题存在强对偶性的情况，我们证明了约束违规可以被减少到与遗憾有相同的渐近增长。

    This paper develops projection-free algorithms for online convex optimization with stochastic constraints. We design an online primal-dual projection-free framework that can take any projection-free algorithms developed for online convex optimization with no long-term constraint. With this general template, we deduce sublinear regret and constraint violation bounds for various settings. Moreover, for the case where the loss and constraint functions are smooth, we develop a primal-dual conditional gradient method that achieves $O(\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations. Furthermore, for the setting where the loss and constraint functions are stochastic and strong duality holds for the associated offline stochastic optimization problem, we prove that the constraint violation can be reduced to have the same asymptotic growth as the regret.
    
[^126]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^127]: LLT：线性定律特征空间变换的R包

    LLT: An R package for Linear Law-based Feature Space Transformation. (arXiv:2304.14211v1 [cs.LG])

    [http://arxiv.org/abs/2304.14211](http://arxiv.org/abs/2304.14211)

    LLT是一个R包，用于线性定律特征空间变换，可以帮助对单变量和多变量时间序列进行分类。

    

    线性定律特征空间转换(LLT )算法的目标是帮助对单变量和多变量时间序列进行分类。LLT R包以灵活和用户友好的方式实现了该算法。该包将实例分为训练和测试集，并利用时延嵌入和谱分解技术，识别训练集中每个输入序列(初始特征)的控制模式(称为线性定律)。最后，它应用训练集的线性定律来转换测试集的初始特征。trainTest、trainLaw和testTrans三个单独的函数来执行这些步骤，它们需要预定义的数据结构;然而，为了快速计算，它们只使用内置函数。LLT R包和适当数据结构的示例数据集在GitHub上公开可用。

    The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.
    
[^128]: 针对领域泛化的中度分布探索

    Moderately Distributional Exploration for Domain Generalization. (arXiv:2304.13976v1 [cs.LG])

    [http://arxiv.org/abs/2304.13976](http://arxiv.org/abs/2304.13976)

    本文提出了一种针对领域泛化问题的中度分布探索（MODE）方法，通过在共享相同语义因素的不确定性子集中探索领域，可以提高模型的分布偏移鲁棒性，并在多个基准数据集上实现了最先进的性能。

    

    领域泛化旨在解决训练领域与未知目标领域之间的分布偏移问题。生成新的领域是最有效的方法之一，然而其性能增益取决于生成的领域与目标领域之间的分布差异。分布鲁棒优化有望通过在不确定性集中探索领域来解决分布偏移问题。然而，不确定性集可能非常庞大，在领域泛化中会导致低置信度预测，因为大的不确定性集可能会引入包含与训练领域语义不同的因素的领域。为了解决这个问题，我们提出了一种针对领域泛化的中度分布探索（MODE）。具体而言，MODE在一个与训练领域共享相同语义因素的不确定性$\textit{子集}$中进行分布探索。我们证明，MODE可以为模型提供可证明的分布偏移鲁棒性，并在几个基准领域泛化数据集上实现了最先进的性能。

    Domain generalization (DG) aims to tackle the distribution shift between training domains and unknown target domains. Generating new domains is one of the most effective approaches, yet its performance gain depends on the distribution discrepancy between the generated and target domains. Distributionally robust optimization is promising to tackle distribution discrepancy by exploring domains in an uncertainty set. However, the uncertainty set may be overwhelmingly large, leading to low-confidence prediction in DG. It is because a large uncertainty set could introduce domains containing semantically different factors from training domains. To address this issue, we propose to perform a $\textbf{mo}$derately $\textbf{d}$istributional $\textbf{e}$xploration (MODE) for domain generalization. Specifically, MODE performs distribution exploration in an uncertainty $\textit{subset}$ that shares the same semantic factors with the training domains. We show that MODE can endow models with provabl
    
[^129]: 通过多样性权重实现生成模型的模式平衡

    Towards Mode Balancing of Generative Models via Diversity Weights. (arXiv:2304.11961v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11961](http://arxiv.org/abs/2304.11961)

    本研究提出了通过平衡训练数据集中的模式来增加模型输出多样性的多样性权重训练方案，以更好地适应需要多样化输出的创意应用，并在受控环境中进行的初步实验展示了其潜力。

    

    大型数据驱动的图像模型被广泛用于支持创意和艺术作品。在当前主导的分布拟合范式下，数据集被视为要尽可能接近的真实值。然而，许多创意应用需要多样化的输出，创作者经常努力从给定的数据分布中积极分离出来。我们认为，从纯模式覆盖转向模式平衡的建模目标调整是必要的，以适应更高的输出多样性目标。我们提出了多样性权重，这是一种通过平衡训练数据集中的模式来增加模型输出多样性的训练方案。在受控环境中进行的初步实验展示了我们方法的潜力。我们讨论了我们方法与多样性、公平和包容在生成式机器学习以及计算机创意中的联系。我们的算法实现可以在https://github.com/找到。

    Large data-driven image models are extensively used to support creative and artistic work. Under the currently predominant distribution-fitting paradigm, a dataset is treated as ground truth to be approximated as closely as possible. Yet, many creative applications demand a diverse range of output, and creators often strive to actively diverge from a given data distribution. We argue that an adjustment of modelling objectives, from pure mode coverage towards mode balancing, is necessary to accommodate the goal of higher output diversity. We present diversity weights, a training scheme that increases a model's output diversity by balancing the modes in the training dataset. First experiments in a controlled setting demonstrate the potential of our method. We discuss connections of our approach to diversity, equity, and inclusion in generative machine learning more generally, and computational creativity specifically. An implementation of our algorithm is available at https://github.com/
    
[^130]: 平等攸关不等于平等个人几率: 用于组和个人公平的后处理方法

    Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness. (arXiv:2304.09779v1 [cs.LG])

    [http://arxiv.org/abs/2304.09779](http://arxiv.org/abs/2304.09779)

    研究发现，通过平等化受保护子群体之间的预测分布来实现组公平和视相似个体同等对待实现个人公正是不兼容的。 并提出了一种构建连续概率函数的解决方法，来实现组和个人公平。

    

    组公平通过平衡受保护子群体之间的预测分布来实现；个人公平要求将相似的个体视为同等对待。然而，当评分模型通过不连续的概率函数进行校准时，这两个目标是不兼容的，其中个体可能会随机分配由固定概率确定的结果。这个过程可能会使来自同一受保护组的两个相似个体的分类几率差别明显不同，这是个人公平的明显违反。为每个受保护子群体分配唯一的几率也可能会阻止一个子群体的成员接到另一个子群体有正面结果的平等机会，我们认为这是另一种称为个人几率的不公平类型。我们通过构建受群体阈值约束的连续概率函数来解决所有这些问题。我们的解决方案保留了模型的预测能力。

    Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different -- a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving equal chances of a positive outcome to another, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model's predictive powe
    
[^131]: 面向头颈癌患者生存预测的瘤体图学习

    Towards Tumour Graph Learning for Survival Prediction in Head & Neck Cancer Patients. (arXiv:2304.08106v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.08106](http://arxiv.org/abs/2304.08106)

    本论文提出了一个瘤体图学习框架，通过定位、分割和生存预测，可以在任意视野的PET和CT扫描上帮助实现头颈癌患者的治疗决策和治疗效果预测。

    

    随着全球2020年近100万例头颈癌的新发病例，头颈癌是一种致命且常见的恶性疾病。由于病变出现在多个位置以及患者之间的治疗效果差异，治疗决策和治疗本身带来了挑战。因此，自动分割和预后估计方法可以帮助确保每位患者得到最有效的治疗。本文提出了一个框架，在任意视野(FOV) PET和CT注册扫描上执行这些功能，从而作为VokCow团队参加了HECKTOR 2022挑战的任务1和任务2。该方法分为三个阶段：定位、分割和生存预测。首先，将任意FOV的扫描裁剪到头颈区域，并使用一个U形卷积神经网络(CNN)来训练分割感兴趣区域。然后，使用获得的区域，将另一个CNN与支持向量机分类器相结合，以获取语义分割结果

    With nearly one million new cases diagnosed worldwide in 2020, head \& neck cancer is a deadly and common malignity. There are challenges to decision making and treatment of such cancer, due to lesions in multiple locations and outcome variability between patients. Therefore, automated segmentation and prognosis estimation approaches can help ensure each patient gets the most effective treatment. This paper presents a framework to perform these functions on arbitrary field of view (FoV) PET and CT registered scans, thus approaching tasks 1 and 2 of the HECKTOR 2022 challenge as team \texttt{VokCow}. The method consists of three stages: localization, segmentation and survival prediction. First, the scans with arbitrary FoV are cropped to the head and neck region and a u-shaped convolutional neural network (CNN) is trained to segment the region of interest. Then, using the obtained regions, another CNN is combined with a support vector machine classifier to obtain the semantic segmentati
    
[^132]: 基于路径修补的模型行为定位

    Localizing Model Behavior with Path Patching. (arXiv:2304.05969v1 [cs.LG])

    [http://arxiv.org/abs/2304.05969](http://arxiv.org/abs/2304.05969)

    本文介绍了一种新的技术——路径修补，用于表达和定量测试表明行为被定位到一组路径的一类自然假设。

    

    将神经网络的行为定位到网络组件的某个子集或组件之间的某个交互的子集是分析网络机制和可能失效模式的自然第一步。现有工作常常是定性且临时的，对于评估定位声明的适当方式没有共识。我们引入了路径修补技术，用于表达和定量测试表明行为被定位到一组路径的一类自然假设。我们改进了感应头的解释，表征了GPT-2的行为，并开源了一个框架，以便高效地运行类似的实验。

    Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.
    
[^133]: 探究鲁棒性模型与生成模型之间的联系

    Exploring the Connection between Robust and Generative Models. (arXiv:2304.04033v1 [cs.LG])

    [http://arxiv.org/abs/2304.04033](http://arxiv.org/abs/2304.04033)

    本文探究鲁棒性判别分类器与生成模型之间的联系，并发现在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量，提出了一种名为高能量PGD的新攻击。

    

    本研究将通过分解鲁棒性判别分类器的损失函数来探究鲁棒性判别分类器与能量基模型(EBM)形式的生成模型之间的联系。我们发现，尽管常见的假设是对抗点离开了输入数据的流形，但是在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量。我们提出了两个证据:非定向攻击的概率甚至比自然数据还要高，并且随着攻击强度的增加，其概率也会增加。这使我们能够轻松地检测它们并设计一种名为高能量PGD的新攻击，能够欺骗分类器但具有与数据集相似的能量。

    We offer a study that connects robust discriminative classifiers trained with adversarial training (AT) with generative modeling in the form of Energy-based Models (EBM). We do so by decomposing the loss of a discriminative classifier and showing that the discriminative model is also aware of the input data density. Though a common assumption is that adversarial points leave the manifold of the input data, our study finds out that, surprisingly, untargeted adversarial points in the input space are very likely under the generative model hidden inside the discriminative classifier -- have low energy in the EBM. We present two evidence: untargeted attacks are even more likely than the natural data and their likelihood increases as the attack strength increases. This allows us to easily detect them and craft a novel attack called High-Energy PGD that fools the classifier yet has energy similar to the data set.
    
[^134]: 高维超统计特征的分类方法

    Classification of Superstatistical Features in High Dimensions. (arXiv:2304.02912v1 [stat.ML])

    [http://arxiv.org/abs/2304.02912](http://arxiv.org/abs/2304.02912)

    本文利用经验风险最小化的方法，对高维超统计特征下的数据进行分类，并分析了正则化和分布尺度参数对分类的影响。

    

    在高维情况下，我们通过经验风险最小化的方法，对具有一般中心点的两个数据云的混合进行了学习，假设具有通用的凸损失和凸正则化。每个数据云是通过从可能是不可数的高斯分布叠加中进行采样来获得的，其方差具有通用的概率密度$\varrho$。我们的分析涵盖了大量的数据分布，包括没有协方差的幂律尾部分布的情况。我们研究了所得估计器的泛化性能，分析了正则化的作用以及分离转换与分布尺度参数的相关性。

    We characterise the learning of a mixture of two clouds of data points with generic centroids via empirical risk minimisation in the high dimensional regime, under the assumptions of generic convex loss and convex regularisation. Each cloud of data points is obtained by sampling from a possibly uncountable superposition of Gaussian distributions, whose variance has a generic probability density $\varrho$. Our analysis covers therefore a large family of data distributions, including the case of power-law-tailed distributions with no covariance. We study the generalisation performance of the obtained estimator, we analyse the role of regularisation, and the dependence of the separability transition on the distribution scale parameters.
    
[^135]: SemiMemes：一种用于多模态Memes分析的半监督学习方法

    SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])

    [http://arxiv.org/abs/2304.00020](http://arxiv.org/abs/2304.00020)

    研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。

    

    社交媒体上Memes的普及性引发了分析其隐含含义、审查有害内容的需求。机器学习的Meme审查系统需要半监督学习解决方案，以利用互联网上大量未标记的Memes，并使注释过程变得更简单。此外，该方法需要利用多模态数据，因为Memes的含义通常来自图像和文本。该研究提出了一种多模态半监督学习方法，在两个数据集，即多媒体自动性别歧视识别和令人讨厌的Memes数据集上，优于其他多模态半监督和监督学习的最新模型。借鉴对比语言-图像预训练所获得的见解，这项研究引入了SemiMemes，一种新颖的训练方法，它结合了自编码器和分类任务

    The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
    
[^136]: 高效生成4K掩膜用于齐次扩散修复

    Efficient Neural Generation of 4K Masks for Homogeneous Diffusion Inpainting. (arXiv:2303.10096v1 [eess.IV])

    [http://arxiv.org/abs/2303.10096](http://arxiv.org/abs/2303.10096)

    该论文提出了一种高效生成4K掩膜的神经网络方法，可以实现对高分辨率图像进行快速且高质量的修复。

    

    利用优选数据，齐次扩散修复可以重建具有高质量的稀疏数据图像。虽然大小为3840 x 2160的4K彩色图像可以实时修复，但优化已知数据以用于图像压缩等应用仍然具有挑战性：广泛使用的随机策略可能需要数天才能处理单个4K图像。最近，第一个针对这个所谓的掩膜优化问题的神经方法通过神经修复代理帮助训练掩膜生成网络，提供了高速度和良好质量的小型图像。但是，这些掩膜网络只能在它们训练的分辨率和掩膜密度下输出掩膜。我们通过一种神经-显式的自粗到细的策略解决了这些问题，并实现了高分辨率图像的掩膜优化。此外，我们通过直接将数值修复求解器纳入网络中来改进掩膜网络的训练和可解释性。这使得能够在短短几秒钟内生成高质量的4K图像掩膜。

    With well-selected data, homogeneous diffusion inpainting can reconstruct images from sparse data with high quality. While 4K colour images of size 3840 x 2160 can already be inpainted in real time, optimising the known data for applications like image compression remains challenging: Widely used stochastic strategies can take days for a single 4K image. Recently, a first neural approach for this so-called mask optimisation problem offered high speed and good quality for small images. It trains a mask generation network with the help of a neural inpainting surrogate. However, these mask networks can only output masks for the resolution and mask density they were trained for. We solve these problems and enable mask optimisation for high-resolution images through a neuroexplicit coarse-to-fine strategy. Additionally, we improve the training and interpretability of mask networks by including a numerical inpainting solver directly into the network. This allows to generate masks for 4K imag
    
[^137]: 合成经验回放：旨在用扩充数据来提高深度强化学习的效果

    Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06614](http://arxiv.org/abs/2303.06614)

    本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。

    

    过去十年的一个关键主题是，当大型神经网络和大型数据集相结合时，它们可以产生令人惊异的结果。在深度强化学习中，这种范式通常通过经验回放实现，其中过去的经验数据集用于训练策略或值函数。然而，与监督学习或自监督学习不同，强化学习代理必须收集自己的数据，这通常是有限的。因此，利用深度学习的好处是具有挑战性的，即使是小型神经网络在训练开始时也可能出现过拟合现象。在这项工作中，我们利用了生成建模的巨大进步，并提出了合成经验回放（SynthER），一种基于扩散的方法来灵活地上采样代理收集的经验。我们证明了SynthER是一种有效的方法，可以在离线和在线设置下训练强化学习代理，无论是在感知环境还是在像素环境中。在离线设置中，我们观察到了显着的改进。

    A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
    
[^138]: 一种离散道路网络设计问题的混合深度学习-元启发式框架

    A hybrid deep-learning-metaheuristic framework for discrete road network design problems. (arXiv:2303.06024v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2303.06024](http://arxiv.org/abs/2303.06024)

    本研究提出了一种混合框架，结合了深度学习和元启发式算法，用于离散道路网络设计问题。该框架可以快速地得到高质量的解决方案，并且可以应用于其他以图为模型的双层问题决策中。

    

    本研究提出了一种具有双层架构的混合深度学习元启发式框架，用于解决道路网络设计问题（NDPs）。我们使用图神经网络（GNN）训练来近似用户均衡（UE）交通分配问题的解，并使用训练模型进行的推理来计算遗传算法（GA）的适应度函数评估，以近似解决NDPs。通过使用两个NDP变量和一个精确求解器作为基准，我们证明了我们提出的框架可以在少于1％的时间内给出全局最优结果的5％左右的间隙内提供解决方案。我们的框架可以在专家系统中使用，用于基础设施规划，以智能地确定最佳基础设施管理决策。由于该框架的灵活性，可以轻松地适应许多可以被建模为图上双层问题的其他决策问题。此外，我们还观察到许多有趣的未来方向。

    This study proposes a hybrid deep-learning-metaheuristic framework with a bi-level architecture for road network design problems (NDPs). We train a graph neural network (GNN) to approximate the solution of the user equilibrium (UE) traffic assignment problem, and use inferences made by the trained model to calculate fitness function evaluations of a genetic algorithm (GA) to approximate solutions for NDPs. Using two NDP variants and an exact solver as benchmark, we show that our proposed framework can provide solutions within 5% gap of the global optimum results given less than 1% of the time required for finding the optimal results. Our framework can be utilized within an expert system for infrastructure planning to intelligently determine the best infrastructure management decisions. Given the flexibility of the framework, it can easily be adapted to many other decision problems that can be modeled as bi-level problems on graphs. Moreover, we observe many interesting future direction
    
[^139]: 浅层神经网络和深层神经网络在多项式逼近中的表达能力。

    Expressivity of Shallow and Deep Neural Networks for Polynomial Approximation. (arXiv:2303.03544v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03544](http://arxiv.org/abs/2303.03544)

    本研究发现，浅层ReLU网络在表达具有随着输入维度增加的Lipschitz参数的函数时会遭受维度灾难，神经网络的表达能力更依赖于它们的深度而不是总体复杂度。

    

    本研究探讨了要近似多元单项式所需的修正线性单元（ReLU）神经网络中神经元的数量。我们在一般紧致域上建立了任何浅层网络逼近乘积函数的指数下界。我们还证明了这个下界不适用于在单位立方体上的规范利普希茨单项式。这些发现表明，在表达具有随着输入维度增加的Lipschitz参数的函数时，浅层ReLU网络会遭受维度灾难，神经网络的表达能力更依赖于它们的深度而不是总体复杂度。

    This study explores the number of neurons required for a Rectified Linear Unit (ReLU) neural network to approximate multivariate monomials. We establish an exponential lower bound on the complexity of any shallow network approximating the product function over a general compact domain. We also demonstrate this lower bound doesn't apply to normalized Lipschitz monomials over the unit cube. These findings suggest that shallow ReLU networks experience the curse of dimensionality when expressing functions with a Lipschitz parameter scaling with the dimension of the input, and that the expressive power of neural networks is more dependent on their depth rather than overall complexity.
    
[^140]: 制造业中联邦学习的应用：识别挑战并探索与工业4.0和5.0愿景的未来方向

    Applications of Federated Learning in Manufacturing: Identifying the Challenges and Exploring the Future Directions with Industry 4.0 and 5.0 Visions. (arXiv:2302.13514v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13514](http://arxiv.org/abs/2302.13514)

    该论文讨论了联邦学习在制造业中的应用，说明其对解决制造业中数据收集和分析等问题的意义，而其中的联邦学习则可以通过协作集成小型厂商的数据，并在不同设备间完成模型的更新，从而使小型制造商能够更好地利用先进的机器学习和数据驱动方法。

    

    在制造领域，数据的收集和分析通常是耗时、具有挑战性且成本高昂的过程，这也阻碍了使用先进的机器学习和数据驱动方法来生成良好的结果，因为这些方法需要大量的离线训练数据。对于那些没有大型企业资源的小型制造商尤其具有挑战性。最近，随着物联网(IoT)的引入，数据可以在整个工厂中以集成的方式实时收集，发送到云端进行高级分析，并用于按顺序更新机器学习模型。然而，小型制造商在利用IoT的好处时面临两个障碍：他们可能无法负担或生成足够的数据来操作私有云，并且他们可能不愿意将原始数据共享到公共云。联邦学习(FL)是协作学习的新兴概念，可以帮助小规模工业解决这些问题。

    In manufacturing settings, data collection and analysis are often a time-consuming, challenging, and costly process. It also hinders the use of advanced machine learning and data-driven methods which require a substantial amount of offline training data to generate good results. It is particularly challenging for small manufacturers who do not share the resources of a large enterprise. Recently, with the introduction of the Internet of Things (IoT), data can be collected in an integrated manner across the factory in real-time, sent to the cloud for advanced analysis, and used to update the machine learning model sequentially. Nevertheless, small manufacturers face two obstacles in reaping the benefits of IoT: they may be unable to afford or generate enough data to operate a private cloud, and they may be hesitant to share their raw data with a public cloud. Federated learning (FL) is an emerging concept of collaborative learning that can help small-scale industries address these issues
    
[^141]: 通过梯度辅助和密集探索提高多目标质量多样性的数据效率

    Improving the Data Efficiency of Multi-Objective Quality-Diversity through Gradient Assistance and Crowding Exploration. (arXiv:2302.12668v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2302.12668](http://arxiv.org/abs/2302.12668)

    本文介绍了一种新的QD算法——带有策略梯度辅助和基于拥挤的探索的多目标MAP-Elites (MOME-PGX)来扩展MOME以提高其数据效率和性能。

    

    最近，由于其高效逃离局部最优和生成广泛高效解的能力，质量多样性(QD)算法已成为优化方法。最近，多目标MAP-Elites (MOME)通过在Map Elites网格的每个单元格中维护帕累托前沿将QD范例扩展到多目标设置。MOME在同时获得多样的解的同时实现了与NSGA-II和SPEA2相竞争的全局性能。但是，MOME受非定向遗传搜索机制的限制，这些机制在高维搜索空间中面临困难。在这项工作中，我们提出了一种新的QD算法——带有策略梯度辅助和基于拥挤的探索的多目标MAP-Elites (MOME-PGX)来扩展MOME以提高其数据效率和性能。MOME-PGX使用基于梯度的优化有效地将解决方案驱向更高的性能。

    Quality-Diversity (QD) algorithms have recently gained traction as optimisation methods due to their effectiveness at escaping local optima and capability of generating wide-ranging and high-performing solutions. Recently, Multi-Objective MAP-Elites (MOME) extended the QD paradigm to the multi-objective setting by maintaining a Pareto front in each cell of a map-elites grid. MOME achieved a global performance that competed with NSGA-II and SPEA2, two well-established Multi-Objective Evolutionary Algorithms (MOEA), while also acquiring a diverse repertoire of solutions. However, MOME is limited by non-directed genetic search mechanisms which struggle in high-dimensional search spaces. In this work, we present Multi-Objective MAP-Elites with Policy-Gradient Assistance and Crowding-based Exploration (MOME-PGX): a new QD algorithm that extends MOME to improve its data efficiency and performance. MOME-PGX uses gradient-based optimisation to efficiently drive solutions towards higher perform
    
[^142]: 重新审视联邦学习中的加权聚合方法

    Revisiting Weighted Aggregation in Federated Learning with Neural Networks. (arXiv:2302.10911v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10911](http://arxiv.org/abs/2302.10911)

    本文重新审视了联邦学习中的加权聚合方法。作者发现权重总和可能小于1，从而改善了泛化性能。作者探索了最优缩小因子如何受到客户端数据异质性和本地周期的影响，并使用客户端相干性研究了客户端之间的相对聚合权重以描绘客户端的重要性。作者提出了一种有效的联邦学习方法（FLLAW），该方法具有可学习聚合权重和全局权重缩小效应。

    

    在联邦学习（FL）中，对本地模型进行加权聚合以生成全局模型，聚合权重被标准化（权重和为1）并与本地数据大小成比例。本文重新审视了加权聚合过程，并深入探讨了FL的训练动力学。首先，我们发现权重总和可能小于1，导致全局权重缩小效应（类似于权重衰减）并改善了泛化性能。我们探讨了最优缩小因子如何受到客户端数据异质性和本地周期的影响。其次，我们深入研究了客户端之间的相对聚合权重以描绘客户端的重要性。我们开发了客户端相干性来研究学习动态，并发现存在一个关键点。在进入临界点之前，相干性更高的客户端在泛化中发挥了更重要的作用。基于上述洞见，我们提出了一种有效的联邦学习方法——具有可学习聚合权重的联邦学习（FLLAW），它允许全局权重缩小效应和可学习聚合权重。在各种基准测试中的实验结果表明，FLLAW在更快的收敛速度、更高的准确性和更好的抗数据异质性方面具有很好的效果。

    In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Ag
    
[^143]: 基于深度学习的时间序列健康数据缺失值填补：回顾与基准测试

    Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking. (arXiv:2302.10902v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10902](http://arxiv.org/abs/2302.10902)

    本文回顾了基于深度学习的时间序列健康数据缺失值填补方法，并在五个时间序列健康数据集上进行基准测试，研究发现填补的效果依赖于数据类型、变量统计、缺失值率和类型。在时间序列数据中同时进行交叉剖面和纵向缺失值填补的深度学习方法表现最佳。

    

    在多元时间序列（MTS）数据中填补缺失值对于确保数据质量和生成可靠的数据驱动预测模型至关重要。除了许多统计方法外，近期一些研究提出了最先进的深度学习方法来填补MTS数据中的缺失值。然而，这些深度方法的评估仅局限于一个或两个数据集、较低的缺失率和完全随机的缺失值类型。本文对五个时间序列健康数据集进行了六个数据中心实验，对最先进的深度填补方法进行基准测试。我们广泛的分析表明，没有一种单一的填补方法在所有五个数据集上表现最好。填补效果取决于数据类型、单个变量统计、缺失值率和类型。在时间序列数据中同时进行交叉剖面（跨变量）和纵向（跨时间）缺失值填补的深度学习方法产生了统计学上的最佳性能。

    The imputation of missing values in multivariate time series (MTS) data is critical in ensuring data quality and producing reliable data-driven predictive models. Apart from many statistical approaches, a few recent studies have proposed state-of-the-art deep learning methods to impute missing values in MTS data. However, the evaluation of these deep methods is limited to one or two data sets, low missing rates, and completely random missing value types. This survey performs six data-centric experiments to benchmark state-of-the-art deep imputation methods on five time series health data sets. Our extensive analysis reveals that no single imputation method outperforms the others on all five data sets. The imputation performance depends on data types, individual variable statistics, missing value rates, and types. Deep learning methods that jointly perform cross-sectional (across variables) and longitudinal (across time) imputations of missing values in time series data yield statistica
    
[^144]: 具有信念共享的去中心化POMDP策略评估

    Policy Evaluation in Decentralized POMDPs with Belief Sharing. (arXiv:2302.04151v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04151](http://arxiv.org/abs/2302.04151)

    本文提出了一种去中心化的信念形成策略，使得代理的参数可以与集中式基线有界的差异，并在多传感器目标跟踪应用中得到应用。

    

    多智能体强化学习的大多数工作都集中在环境状态完全可观察的场景中。本文考虑一种协作策略评估任务，其中代理不能直接观察环境状态。相反，代理只能访问含噪声观测和置信向量。我们提出了一种完全去中心化的信念形成策略，该策略依赖于个体更新和通信网络上的本地化交互。除了交换信念外，代理还利用通信网络交换价值函数参数估计。我们分析地表明，该策略允许信息在网络中扩散，从而使代理的参数与集中式基线具有有界差异。一个多传感器目标跟踪应用是我们的案例研究。

    Most works on multi-agent reinforcement learning focus on scenarios where the state of the environment is fully observable. In this work, we consider a cooperative policy evaluation task in which agents are not assumed to observe the environment state directly. Instead, agents can only have access to noisy observations and to belief vectors. It is well-known that finding global posterior distributions under multi-agent settings is generally NP-hard. As a remedy, we propose a fully decentralized belief forming strategy that relies on individual updates and on localized interactions over a communication network. In addition to the exchange of the beliefs, agents exploit the communication network by exchanging value function parameter estimates as well. We analytically show that the proposed strategy allows information to diffuse over the network, which in turn allows the agents' parameters to have a bounded difference with a centralized baseline. A multi-sensor target tracking applicatio
    
[^145]: 利用演示数据改进在线学习:质量至关重要

    Leveraging Demonstrations to Improve Online Learning: Quality Matters. (arXiv:2302.03319v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03319](http://arxiv.org/abs/2302.03319)

    本篇论文探讨了离线演示数据如何改进在线学习的问题，提出了一种利用演示数据的TS算法，并给出了依赖于先验知识的贝叶斯遗憾界；研究发现，预训练可以大幅提高在线性能，改进程度随专家能力水平的提高而增加。

    

    我们研究了离线演示数据可以如何改进在线学习，自然而然地期望会有一定的改进，但问题在于如何改进以及可以改进多少？我们表明，改进的程度必须取决于演示数据的质量。为了生成可移植的见解，我们将重点放在了作为典型在线学习算法和模型的多臂赌博机上应用汤普森抽样（TS）。演示数据是由具有给定能力水平的专家生成的，这是我们引入的一个概念。我们提出了一种知情TS算法，通过贝叶斯定理以一致的方式利用演示数据并导出依赖于先验的贝叶斯遗憾界。这提供了洞见，即预训练如何极大地提高在线性能，以及改进程度随专家能力水平的提高而增加。我们还通过贝叶斯引导实现了实用的、近似的知情TS算法，并通过实验证明了实现了实质性的遗憾减少。

    We investigate the extent to which offline demonstration data can improve online learning. It is natural to expect some improvement, but the question is how, and by how much? We show that the degree of improvement must depend on the quality of the demonstration data. To generate portable insights, we focus on Thompson sampling (TS) applied to a multi-armed bandit as a prototypical online learning algorithm and model. The demonstration data is generated by an expert with a given competence level, a notion we introduce. We propose an informed TS algorithm that utilizes the demonstration data in a coherent way through Bayes' rule and derive a prior-dependent Bayesian regret bound. This offers insight into how pretraining can greatly improve online performance and how the degree of improvement increases with the expert's competence level. We also develop a practical, approximate informed TS algorithm through Bayesian bootstrapping and show substantial empirical regret reduction through exp
    
[^146]: 运行式选举：针对数据污染攻击的改进可证明防御方法

    Run-Off Election: Improved Provable Defense against Data Poisoning Attacks. (arXiv:2302.02300v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02300](http://arxiv.org/abs/2302.02300)

    本文提出了一种名为ROE的新型数据污染防御方法，通过在基本模型之间进行运行式选举，有效利用logits层的信息，并在MNIST数据集和CIFAR-10上得到了比最先进的集成方法更好的结果，提供了针对数据污染攻击的可证明保证

    

    在数据污染攻击中，攻击者试图通过在训练数据中添加、修改或删除样本来改变模型的预测结果。最近，提出了基于集成方法的可证明数据污染防御方法，其中预测是通过对多个基本模型进行多数表决来完成的。本文中，我们表明仅考虑集成防御中的大多数表决是浪费的，因为它没有有效地利用基本模型中的logits层中可用的信息。相反地，我们提出了运行式选举（ROE），这是一种基于基本模型之间的两轮选举的新型聚合方法：在第一轮中，模型为它们首选的类别投票，然后在第一轮中排名前两的类别之间进行第二轮“Run-Off”选举。基于这种方法，我们提出了基于先前工作中的Deep Partition Aggregation（DPA）和Finite Aggregation（FA）方法的DPA+ROE和FA+ROE防御方法。我们在MNIST数据集和CIFAR-10上评估了我们的方法，实验结果表明，我们提出的基于ROE的防御方法优于最先进的集成方法，并提供针对数据污染攻击的可证明保证

    In data poisoning attacks, an adversary tries to change a model's prediction by adding, modifying, or removing samples in the training data. Recently, ensemble-based approaches for obtaining provable defenses against data poisoning have been proposed where predictions are done by taking a majority vote across multiple base models. In this work, we show that merely considering the majority vote in ensemble defenses is wasteful as it does not effectively utilize available information in the logits layers of the base models. Instead, we propose Run-Off Election (ROE), a novel aggregation method based on a two-round election across the base models: In the first round, models vote for their preferred class and then a second, Run-Off election is held between the top two classes in the first round. Based on this approach, we propose DPA+ROE and FA+ROE defense methods based on Deep Partition Aggregation (DPA) and Finite Aggregation (FA) approaches from prior work. We evaluate our methods on MN
    
[^147]: 利用强化学习规划多种传染病干预方案

    Planning Multiple Epidemic Interventions with Reinforcement Learning. (arXiv:2301.12802v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12802](http://arxiv.org/abs/2301.12802)

    本文将找到最优化的疫情计划转化为马尔可夫决策过程，并利用强化学习算法来搜索最小化疾病和经济成本的计划。

    

    应对流行病需要制定计划，描述何时以及如何应用不同的干预措施，比如要求佩戴口罩、接种疫苗、关闭学校或工作场所等。最优的计划将以最小的生命损失、疾病负担和经济成本遏制疫情。在现实情况下，寻找最优计划是一个难以解决的计算问题。我们将这个问题表述为马尔可夫决策过程，并提出一种独特的方法，能够表示对于任何由常微分方程定义的疾病模型上的多个连续干预措施。我们展示了如何有效地应用最先进的演员-评论家强化学习算法（PPO和SAC），以在连续和复杂的状态空间中搜索最小化疾病和经济成本的计划。

    Combating an epidemic entails finding a plan that describes when and how to apply different interventions, such as mask-wearing mandates, vaccinations, school or workplace closures. An optimal plan will curb an epidemic with minimal loss of life, disease burden, and economic cost. Finding an optimal plan is an intractable computational problem in realistic settings. Policy-makers, however, would greatly benefit from tools that can efficiently search for plans that minimize disease and economic costs especially when considering multiple possible interventions over a continuous and complex action space given a continuous and equally complex state space. We formulate this problem as a Markov decision process. Our formulation is unique in its ability to represent multiple continuous interventions over any disease model defined by ordinary differential equations. We illustrate how to effectively apply state-of-the-art actor-critic reinforcement learning algorithms (PPO and SAC) to search fo
    
[^148]: 通过稳健优化找出反事实解释的区域

    Finding Regions of Counterfactual Explanations via Robust Optimization. (arXiv:2301.11113v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11113](http://arxiv.org/abs/2301.11113)

    该论文提出了一种通过稳健优化计算反事实解释（CE）区域的方法，使用户能够选择适当的措施以获得所需的结果，此方法在逻辑回归、决策树、随机森林和神经网络等最常见的机器学习方法上证明了收敛结果。

    

    反事实解释在检测偏见和提高数据驱动分类模型的可解释性方面发挥着重要作用。一个反事实解释（CE）是一个最小的扰动数据点，使得模型的决策发生变化。现有大多数方法只能提供一个CE，可能对于用户来说是难以实现的。在这项工作中，我们推导出一种迭代方法来计算稳健CE，即在特征轻微扰动后仍然有效的CE。为此，我们的方法提供了整个CE区域，使用户能够选择适当的措施以获得所需的结果。我们使用了稳健优化的算法思想，并证明了最常见的机器学习方法（包括逻辑回归、决策树、随机森林和神经网络）的收敛结果。我们的实验结果表明，我们的方法可以有效地为各种常见数据集和分类模型生成全局最佳的稳健CE。

    Counterfactual explanations play an important role in detecting bias and improving the explainability of data-driven classification models. A counterfactual explanation (CE) is a minimal perturbed data point for which the decision of the model changes. Most of the existing methods can only provide one CE, which may not be achievable for the user. In this work we derive an iterative method to calculate robust CEs, i.e. CEs that remain valid even after the features are slightly perturbed. To this end, our method provides a whole region of CEs allowing the user to choose a suitable recourse to obtain a desired outcome. We use algorithmic ideas from robust optimization and prove convergence results for the most common machine learning methods including logistic regression, decision trees, random forests, and neural networks. Our experiments show that our method can efficiently generate globally optimal robust CEs for a variety of common data sets and classification models.
    
[^149]: 部分动员：跟踪俄罗斯媒体和电报之间的多语言信息流

    Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2301.10856](http://arxiv.org/abs/2301.10856)

    本文研究了16个俄罗斯媒体机构和732个电报频道之间的互动，发现新闻媒体不仅通过电报传播现有的叙事，而且会从电报平台源材料，研究结果表明2.3％至26.7％的文章将主题归因于电报活动。

    

    在俄罗斯入侵乌克兰后，针对俄罗斯在线媒体的虚假信息和宣传，包括俄罗斯之声和卫星新闻在内的俄罗斯媒体在欧洲遭到禁止。为了保持观众数量，许多俄罗斯媒体开始在电报等消息服务上大力宣传其内容。在这项工作中，我们研究了2022年期间16家俄罗斯媒体机构如何与732个电报频道互动和利用。利用基础模型MPNet、DP-means聚类和Hawkes过程，我们跟踪新闻网站和电报频道之间的叙事传播情况。我们表明，新闻媒体不仅通过电报传播现有的叙事，而且他们会从电报平台源材料。在我们研究的网站中，2.3％（ura.news）至26.7％（ukraina.ru）的文章讨论了源于/导致电报活动的内容。最后，通过跟踪个别主题的扩散，我们测量新闻网站发表文章的速率。

    In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
    
[^150]: 用多智能体强化学习模拟社会困境中的道德选择

    Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2301.08491](http://arxiv.org/abs/2301.08491)

    本文使用多智能体强化学习模拟社会困境中的道德选择，设计了一套道德奖励结构，旨在分析和研究AI代理的道德行为。

    

    在实际应用中，人工智能（AI）在智能代理中纳入道德选择的重要性不断展现。同时也强调，按照任何一种道德观定义顶层的AI伦理约束非常具有挑战，并且会带来风险。从底层学习的角度出发，或许更适合研究和开发AI代理的道德行为。我们认为，分析根据预定义的道德奖励在社会困境中实行行动的强化学习代理的新兴行为是一个有趣和富有洞察力的起点。在这项工作中，我们对强化学习代理根据道德理论的奖励进行的选择进行了系统分析。我们旨在设计简化但代表一组关键伦理系统的奖励结构。因此，我们首先定义了区分后果和规范伦理的道德奖励函数，并将它们混合以创建新的奖励方案。然后，我们通过训练在社会困境下进行内在动机驱动的强化学习代理来评估这些奖励函数。结果表明，我们的方法能够复制并扩展有关道德选择的文献研究中的许多发现，并能够出现以前未曾报道的新行为。

    Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm
    
[^151]: 通过D适应实现学习率自由学习

    Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07733](http://arxiv.org/abs/2301.07733)

    D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。

    

    D适应是一种自动设置学习率的方法，可以渐近地实现最优收敛速率，用于最小化凸性Lipschitz函数，无需回溯或线性搜索，并且每步无需进行额外的函数值或梯度评估。我们的方法是这一类问题的第一个无超参数且收敛速率无需额外对数因子改进的方法。我们针对SGD和Adam变体展示了广泛的实验，其中该方法自动匹配手动调整的学习率，在十多个不同的机器学习问题中应用，包括大规模的视觉和语言问题。开源实现在 \url{https://github.com/facebookresearch/dadaptation}.

    D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
    
[^152]: 可切换轻量级反对称处理（SLAP）在 Gomoku 强化学习中使用 CNN 比数据增强更快

    Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning. (arXiv:2301.04746v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04746](http://arxiv.org/abs/2301.04746)

    本论文提出了一种名为SLAP的方法，它可以替代数据增强，加强经验以加速机器学习并减少样本大小。 在Gomoku游戏和强化学习领域的实验中，证明了SLAP的有效性，可以提高模型的收敛速度，同时减少了样本数量。这种策略至少适用于对称或特定变换不变的领域。

    

    本文提出了一种名为 SLAP 的方法，用于加强经验以加速机器学习并减少样本大小，以代替数据增强。SLAP是一种模型无关的协议/函数，可以产生相同的输出，但给予不同的变换变量。在Gomoku游戏状态的实验中，SLAP提高了卷积神经网络学习的收敛速度达83％，样本大小只有数据增强的1/8。在Gomoku强化学习中，使用AlphaGo Zero / AlphaZero算法和数据增强作为基线，SLAP将训练样本数量减少了8倍，并在与相同评估器对比时实现了类似的获胜率，但尚不能证明它可以加速强化学习。这些益处至少应适用于对称或特定变换不变的领域。作为未来的工作，SLAP可能有助于更可解释的学习以及不适用于对称的领域的转移学习。

    To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to sym
    
[^153]: 在现实世界中实现智能决策：基础决策模型的视角

    On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective. (arXiv:2212.12669v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.12669](http://arxiv.org/abs/2212.12669)

    本文提出了基础决策模型（FDM），通过使用转换器神经架构将各种决策任务制定为序列解码任务，为在复杂现实世界环境中实现机器驱动智能决策(IDM)提供了一个前景广阔的解决方案。

    

    现实世界环境的普遍不确定性和动态特性给机器驱动智能决策(IDM)系统的广泛实施带来了重大挑战。因此，IDM应具备持续获取新技能并在广泛应用中有效推广的能力。超越任务和应用边界的人工通用智能(AGI)的进步对于增强IDM至关重要。最近的研究广泛调查了转换器神经架构作为各种任务的基础模型，包括计算机视觉，自然语言处理和强化学习。我们提出一个基础决策模型(FDM)，通过使用转换器体系结构将各种决策任务制定为序列解码任务，为扩展复杂实际情况下的IDM应用提供了一个有前途的解决方案。在本文中，我们讨论了基于转换器神经架构开发基础决策模型(FDM)的效率和可行性，用于机器驱动智能决策(IDM)在复杂实际情况中的应用。FDM可以允许持续的技能获取并在各个应用之间进行概括，具有增进人工通用智能(AGI)的潜力。

    The pervasive uncertainty and dynamic nature of real-world environments present significant challenges for the widespread implementation of machine-driven Intelligent Decision-Making (IDM) systems. Consequently, IDM should possess the ability to continuously acquire new skills and effectively generalize across a broad range of applications. The advancement of Artificial General Intelligence (AGI) that transcends task and application boundaries is critical for enhancing IDM. Recent studies have extensively investigated the Transformer neural architecture as a foundational model for various tasks, including computer vision, natural language processing, and reinforcement learning. We propose that a Foundation Decision Model (FDM) can be developed by formulating diverse decision-making tasks as sequence decoding tasks using the Transformer architecture, offering a promising solution for expanding IDM applications in complex real-world situations. In this paper, we discuss the efficiency an
    
[^154]: 使用网页抓取的多模态数据预训练的对比语言-视觉AI模型存在性物化偏见

    Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2212.11261](http://arxiv.org/abs/2212.11261)

    本文使用对比语言图像预训练的多模态AI模型，使用网页抓取的数据训练，发现这些模型存在性物化偏见，即人的情感状态与身体的呈现相关，表现出对女性的性别偏见。

    

    本文评估了使用对比语言图像预训练（CLIP）目标进行网页抓取的多模态数据的九种语言-视觉AI模型，以寻找心理学家研究的偏见的证据：女孩和女性的性物化现象。我们复制了三个心理实验，并显示这种偏见在AI中依然存在。第一个实验使用Sexual OBjectification and EMotion Database中的标准女性图像，并发现情感状态的识别是由主体是否全身或部分穿着进行介导的。嵌入关联测试返回了愤怒(d>0.80)和悲伤(d>0.50)的显着效应大小，将完全穿着的主体的图像与情感相关联。GRAD-CAM显著性图突出显示，CLIP生成的模型存在性物化偏见，即使使用最先进的对比多模态预训练和网页抓取的数据也是如此。

    Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d >0.80) and sadness (d >0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP ge
    
[^155]: 一种基于强化学习的模因算法用于社会技术生产调度

    A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10936](http://arxiv.org/abs/2212.10936)

    本文提出了一种基于深度强化学习的模因算法，用于解决具有实际约束的灵活生产调度问题，并弥补元启发式研究中的缺陷。

    

    本文提出了一个应用深度强化学习的模因算法，用于解决实际双资源约束柔性作业车间调度问题（DRC-FJSSP）。近年来，对于DRL技术已经进行了广泛的研究，但是没有考虑到现实、灵活和以人为中心的车间。本文发现，在以订单为导向的间歇性制造中存在一个研究空白，它经常在具有高服务水平的中小型公司中表示。从这一领域的实际工业项目中，我们认识到需要描述灵活的机器、人工工作者和能力、设置和处理操作、物料到达时间、具有并行任务的复杂作业路径以进行物料清单（BOM）制造、顺序相关设置时间和（部分）自动化任务。另一方面，在DRC-FJSSP的背景下，已经进行了大量的元启发式研究。然而，缺乏适当的方法来解决生产过程的复杂性和不确定性，这是现实工业世界中面临的主要挑战。因此，提出了一种新的算法，以弥补相关领域的缺陷。

    The following article presents a memetic algorithm with applying deep reinforcement learning (DRL) for solving practically oriented dual resource constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years, there has been extensive research on DRL techniques, but without considering realistic, flexible and human-centered shopfloors. A research gap can be identified in the context of make-to-order oriented discontinuous manufacturing as it is often represented in medium-size companies with high service levels. From practical industry projects in this domain, we recognize requirements to depict flexible machines, human workers and capabilities, setup and processing operations, material arrival times, complex job paths with parallel tasks for bill of material (BOM) manufacturing, sequence-depended setup times and (partially) automated tasks. On the other hand, intensive research has been done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of sui
    
[^156]: 基于自适应多面体的神经网络控制系统自动可达性分析

    Automated Reachability Analysis of Neural Network-Controlled Systems via Adaptive Polytopes. (arXiv:2212.07553v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.07553](http://arxiv.org/abs/2212.07553)

    本文提出了一种基于自适应多面体的方法，用于过度逼近神经网络控制系统中的可达性分析，其中利用线性层的奇异值分解以及激活函数的形状，在每个时间步骤上调整多面体的几何形状以适应真实可达集的几何形状，并通过推断出的模板计算出可达集的精确过度逼近。

    

    过度逼近动态系统的可达集是安全验证和鲁棒控制合成中的一个基本问题，并且这些集合的表示是影响计算复杂度和逼近误差的关键因素。本文开发了一种新的方法，利用自适应模板多面体来过度逼近神经网络动态系统的可达集。我们利用线性层的奇异值分解以及激活函数的形状，在每个时间步骤上调整多面体的几何形状以适应真实可达集的几何形状。然后，我们提出了一种分支定界方法，通过推断出的模板计算出可达集的精确过度逼近。我们在神经网络控制下的线性系统的可达性分析中说明了所提出方法的效用。

    Over-approximating the reachable sets of dynamical systems is a fundamental problem in safety verification and robust control synthesis. The representation of these sets is a key factor that affects the computational complexity and the approximation error. In this paper, we develop a new approach for over-approximating the reachable sets of neural network dynamical systems using adaptive template polytopes. We use the singular value decomposition of linear layers along with the shape of the activation functions to adapt the geometry of the polytopes at each time step to the geometry of the true reachable sets. We then propose a branch-and-bound method to compute accurate over-approximations of the reachable sets by the inferred templates. We illustrate the utility of the proposed approach in the reachability analysis of linear systems driven by neural network controllers.
    
[^157]: 基于任务相似度元学习加速多目标非分层超参数最优化的树形结构Parzen估计

    Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06751](http://arxiv.org/abs/2212.06751)

    本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。

    

    超参数优化是提高深度学习性能的关键步骤。实践者通常面临多个方面的权衡，如准确性和延迟时间。在深度学习的高计算需求和对高效超参数优化的不断增长需求下，加速多目标优化变得越来越重要。本文将TPE的收购函数扩展到元学习设置中，使用由任务之间顶级域之间的重叠度定义的任务相似性。我们也从理论上分析并解决了任务相似性的局限性。在实验中，我们展示了我们的方法在表格HPO基准上加速了MO-TPE，并获得了最先进的性能。我们的方法还通过赢得AutoML 2022来得到外部验证。

    Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
    
[^158]: 偏微分方程和相关参数识别问题的深度学习方法

    Deep Learning Methods for Partial Differential Equations and Related Parameter Identification Problems. (arXiv:2212.03130v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03130](http://arxiv.org/abs/2212.03130)

    本论文回顾了深度学习在解决偏微分方程和相关参数识别问题中的应用，提出了一个统一的框架来理解这些方法和技术。

    

    近年来，深度学习方法在数学领域得到了越来越广泛的应用，既更深入地理解了深度学习的概念并探索如何使其更加稳健，又将深度学习算法用于解决数学问题，这种方法被称为科学机器学习。具体而言，越来越多的神经网络结构被发展出来来解决特定类别的偏微分方程。这些方法利用了偏微分方程固有的性质，因此比标准前馈神经网络、递归神经网络或卷积神经网络更好地解决了偏微分方程，这在数学建模领域产生了很大的影响。在本研究中，我们回顾了这些方法，并提出了一个统一的框架来理解它们。我们还讨论了几个与偏微分方程相关的参数识别问题，深度学习已经成功地应用于这些问题。

    Recent years have witnessed a growth in mathematics for deep learning--which seeks a deeper understanding of the concepts of deep learning with mathematics and explores how to make it more robust--and deep learning for mathematics, where deep learning algorithms are used to solve problems in mathematics. The latter has popularised the field of scientific machine learning where deep learning is applied to problems in scientific computing. Specifically, more and more neural network architectures have been developed to solve specific classes of partial differential equations (PDEs). Such methods exploit properties that are inherent to PDEs and thus solve the PDEs better than standard feed-forward neural networks, recurrent neural networks, or convolutional neural networks. This has had a great impact in the area of mathematical modeling where parametric PDEs are widely used to model most natural and physical processes arising in science and engineering. In this work, we review such method
    
[^159]: c-TPE:基于树形结构的带不等式约束的帕捷斯特估计器用于昂贵超参数优化

    c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14411](http://arxiv.org/abs/2211.14411)

    本文提出了约束TPE（c-TPE）方法，是树形Parzen估计器（TPE）的扩展，可有效处理在性能要求之上施加的约束限制，实验证明在81个昂贵的HPO设置中表现出最佳性能排名。

    

    超参数优化（HPO）对于深度学习算法的强大性能至关重要，实际应用通常会在性能要求之上施加一些限制，例如内存使用或延迟等。在本文中，我们提出了约束TPE（c-TPE），这是广泛使用的多功能贝叶斯优化方法——树形Parzen估计器（TPE）的扩展，以处理这些约束。我们提出的扩展不仅是简单地将现有收益函数和原始TPE组合起来，而是包括修改来解决导致性能不佳的问题。我们从经验和理论上深入分析这些修改，提供了有关它们如何有效地克服这些挑战的见解。在实验中，我们证明了c-TPE在81个昂贵的HPO设置中表现出最佳的平均排名性能，具有统计显着性。

    Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
    
[^160]: 使用基于知识的神经常微分方程和深度集成的学习增强型非线性模型预测控制

    Learning-enhanced Nonlinear Model Predictive Control using Knowledge-based Neural Ordinary Differential Equations and Deep Ensembles. (arXiv:2211.13829v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.13829](http://arxiv.org/abs/2211.13829)

    本文介绍了一种学习增强型非线性模型预测控制方法，通过学习基于知识的神经常微分方程和深度集成的模型，提高了系统动态的预测准确性。

    

    非线性模型预测控制（MPC）是一种灵活且越来越受欢迎的框架，用于合成能够满足状态和控制输入约束的反馈控制策略。在该框架中，每个时间步骤都会解决一个优化问题，该问题受到由非线性动态模型表征的一组动态约束的限制。尽管具有通用性，但非线性MPC的性能通常取决于动态模型的准确性。在这项工作中，我们利用深度学习工具，即基于知识的神经常微分方程（KNODE）和深度集成，提高模型的预测准确性。具体而言，我们学习一个KNODE模型的集合，称为KNODE集合，以获得真实系统动态的准确预测。然后将这个学习的模型整合到一个新的学习增强型非线性MPC框架中。我们提供了足够的条件，保证了闭环系统的渐进稳定性。

    Nonlinear model predictive control (MPC) is a flexible and increasingly popular framework used to synthesize feedback control strategies that can satisfy both state and control input constraints. In this framework, an optimization problem, subjected to a set of dynamics constraints characterized by a nonlinear dynamics model, is solved at each time step. Despite its versatility, the performance of nonlinear MPC often depends on the accuracy of the dynamics model. In this work, we leverage deep learning tools, namely knowledge-based neural ordinary differential equations (KNODE) and deep ensembles, to improve the prediction accuracy of this model. In particular, we learn an ensemble of KNODE models, which we refer to as the KNODE ensemble, to obtain an accurate prediction of the true system dynamics. This learned model is then integrated into a novel learning-enhanced nonlinear MPC framework. We provide sufficient conditions that guarantees asymptotic stability of the closed-loop system
    
[^161]: 量子神经网络的表达能力增强策略

    Expressibility-Enhancing Strategies for Quantum Neural Networks. (arXiv:2211.12670v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.12670](http://arxiv.org/abs/2211.12670)

    本文提出四项增强量子神经网络表达能力的策略，并验证其可以显著提高QNN在近似复杂函数方面的性能表现。

    

    量子神经网络通过参数化量子电路来表示，可以在监督学习范式中训练模型将输入数据映射到预测结果。先前大多数工作都集中于理论分析QNN的表达能力。然而，几乎所有文献中，QNN的表达能力仅基于简单的单变量函数进行数值验证。我们惊奇地发现，具有强表达能力的最先进QNN即使在近似简单的正弦函数上，也可能表现不佳。为此，我们提出了四个增强QNN表达能力的策略：适用于正弦函数的嵌入、冗余测量、后测量函数和随机训练数据。我们通过数学分析和/或数值研究，包括学习复杂的基于正弦函数的函数，分析了这些策略的有效性。我们的比较实验结果验证了这四个策略可以显著提高QNN在近似简单单变量函数之外的复杂函数的表现。

    Quantum neural networks (QNNs), represented by parameterized quantum circuits, can be trained in the paradigm of supervised learning to map input data to predictions. Much work has focused on theoretically analyzing the expressive power of QNNs. However, in almost all literature, QNNs' expressive power is numerically validated using only simple univariate functions. We surprisingly discover that state-of-the-art QNNs with strong expressive power can have poor performance in approximating even just a simple sinusoidal function. To fill the gap, we propose four expressibility-enhancing strategies for QNNs: Sinusoidal-friendly embedding, redundant measurement, post-measurement function, and random training data. We analyze the effectiveness of these strategies via mathematical analysis and/or numerical studies including learning complex sinusoidal-based functions. Our results from comparative experiments validate that the four strategies can significantly increase the QNNs' performance in
    
[^162]: 听、去噪、行动！基于扩散模型的音频驱动动作合成。

    Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models. (arXiv:2211.09707v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09707](http://arxiv.org/abs/2211.09707)

    该论文展示了使用Diffusion Models来进行音频驱动的人体运动合成的有效性和适用性，具备极高的运动质量，可以实现独特的风格表达控制，还可以应用于多源音频下的运动控制。

    

    扩散模型作为高度表现力但训练高效的概率模型，近年来受到了广泛的关注。我们展示了这些模型非常适合用于合成与音频同时发生的人体运动，例如跳舞和共同语音手势。由于给定音频时运动复杂且高度模糊，需要对其进行概率描述。具体而言，我们将DiffWave结构用于建模3D姿势序列，将Conformers用于替代膨胀卷积以提高建模能力。我们还展示了对运动风格的控制，使用无分类器的引导来调整风格表达的强度。手势和舞蹈生成实验证实了该方法实现了最高水平的运动质量，具有独特的风格表达，其表达的形式可以更或者更少地突出。我们还使用相同的模型体系结构合成了路径驱动的运动。最后，我们将引导过程推广到多个音频源的期望控制，从而实现了更细粒度的运动控制。

    Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-ex
    
[^163]: 线性动态系统离线数据污染攻击的分析和可检测性

    Analysis and Detectability of Offline Data Poisoning Attacks on Linear Dynamical Systems. (arXiv:2211.08804v4 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.08804](http://arxiv.org/abs/2211.08804)

    研究发现，针对线性动态系统的数据污染攻击需要不同的攻击和检测方法。论文针对最小二乘估计进行了统计测试，确定了与数据兼容的模型集是否包括系统的真实模型，并提出了一种隐蔽的数据污染攻击。

    

    近年来，对数据驱动控制方法中的数据污染攻击影响的研究越来越受关注。机器学习社区已经熟知了毒化攻击，但这些攻击通常使用交叉样本独立等假设，而这些假设在线性动态系统中通常不成立。因此，这些系统需要与i.i.d.设置下针对监督学习问题开发的攻击和检测方法不同的攻击和检测方法。由于大多数数据驱动控制算法使用最小二乘估计，我们通过统计测试来研究污染如何影响最小二乘估计，并质疑数据污染攻击可以以什么方式被检测到。我们确定了在哪些条件下与数据兼容的模型集包含系统的真实模型，并分析了攻击者的不同污染策略。基于此，我们提出了一种隐蔽的数据污染攻击。

    In recent years, there has been a growing interest in the effects of data poisoning attacks on data-driven control methods. Poisoning attacks are well-known to the Machine Learning community, which, however, make use of assumptions, such as cross-sample independence, that in general do not hold for linear dynamical systems. Consequently, these systems require different attack and detection methods than those developed for supervised learning problems in the i.i.d.\ setting. Since most data-driven control algorithms make use of the least-squares estimator, we study how poisoning impacts the least-squares estimate through the lens of statistical testing, and question in what way data poisoning attacks can be detected. We establish under which conditions the set of models compatible with the data includes the true model of the system, and we analyze different poisoning strategies for the attacker. On the basis of the arguments hereby presented, we propose a stealthy data poisoning attack 
    
[^164]: 合并数据集以增加样本数量并提高模型拟合

    Combining datasets to increase the number of samples and improve model fitting. (arXiv:2210.05165v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.05165](http://arxiv.org/abs/2210.05165)

    本文提出了一种组合数据集的新框架ComImp，可以处理不同数据集之间存在不同特征的挑战，并利用PCA-ComImp进行维数降低。此外，此框架还可以用于数据预处理，填补缺失数据的条目。该方法在多个真实世界的数据集上得到了验证。

    

    在许多使用情况下，将来自不同数据集的信息组合起来可以提高机器学习模型的性能，特别是当至少一个数据集的样本数量很少时。然而，在这种情况下可能存在一个潜在的挑战，即这些数据集的特征不完全相同，尽管它们中有一些共同的特征。为了解决这个挑战，我们提出了一种新的框架，称为基于插补的数据集组合（ComImp）。此外，我们提出了一种ComImp的变体，使用主成分分析（PCA），即PCA-ComImp，以在组合数据集之前降低维数。当数据集拥有大量不共享特征时，这非常有用。此外，我们的框架还可以用于数据预处理，即通过在组合不同数据集时填补缺失的条目来插补缺失数据。为了说明所提出的方法的能力和潜在用途，我们在多个真实世界的数据集上进行了广泛实验。

    For many use cases, combining information from different datasets can be of interest to improve a machine learning model's performance, especially when the number of samples from at least one of the datasets is small. However, a potential challenge in such cases is that the features from these datasets are not identical, even though there are some commonly shared features among the datasets. To tackle this challenge, we propose a novel framework called Combine datasets based on Imputation (ComImp). In addition, we propose a variant of ComImp that uses Principle Component Analysis (PCA), PCA-ComImp in order to reduce dimension before combining datasets. This is useful when the datasets have a large number of features that are not shared between them. Furthermore, our framework can also be utilized for data preprocessing by imputing missing data, i.e., filling in the missing entries while combining different datasets. To illustrate the power of the proposed methods and their potential us
    
[^165]: 参数修剪的数据集蒸馏方法

    Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14609](http://arxiv.org/abs/2209.14609)

    本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。

    

    在许多领域中，获得先进模型的方法取决于大型数据集，这使得数据存储和模型训练变得昂贵。作为解决方案，数据集蒸馏可以合成保留原始大型数据集大多数信息的小型数据集。最近提出的匹配网络参数的数据集蒸馏方法已被证明在几个数据集上有效。然而，网络参数的维度通常很大。此外，一些参数在蒸馏过程中难以匹配，降低了蒸馏性能。基于这个观察，本研究提出了一种基于参数修剪的新型数据集蒸馏方法来解决这个问题。该方法可以在蒸馏过程中修剪难以匹配的参数，从而合成更加稳健的蒸馏数据集并提高蒸馏性能。在三个数据集上的实验结果表明，该方法优于其他最先进的数据集蒸馏方法。

    In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
    
[^166]: 法律引导代码：一种法律信息学方法来使人工智能与人类保持一致

    Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v13 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2209.13020](http://arxiv.org/abs/2209.13020)

    这篇论文提出了一种“法律指导代码”理念，利用法律信息学将法律知识和推理嵌入到人工智能中，从而使人工智能与人类的目标和社会价值保持一致。

    

    目前我们无法可靠地指定人类的目标和社会价值，以引导人工智能的行为。制定法律和解释法律构成了一种计算引擎，将不透明的人类价值转化为易读的指令。 “法律指导代码”是嵌入了法律知识和推理的人工智能研究议程。类似于合同当事人无法预见他们未来关系的每个潜在变数，立法者无法预测其提出的法案将适用的所有情况，我们无法提前明确规则，以可靠地引导良好的人工智能行为。法律理论和实践已经开发出各种工具来解决这些规定问题。与法律更为普通的用途（例如通过制裁威胁来阻止不良行为）相反，法律作为一种表达人类沟通目标和价值的表现，可以引导人工智能代码的发展方向。具体而言，法律信息学是计算规则和系统用于表示，分析和操作法律知识的跨学科研究。法律指导代码利用法律信息学来使人工智能与人类的目标和社会价值保持一致，以一种透明，负责，灵活的方式。

    We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. "Law Informs Code" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goa
    
[^167]: 线性动态系统观测预测中的公平性

    Fairness in Forecasting of Observations of Linear Dynamical Systems. (arXiv:2209.05274v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05274](http://arxiv.org/abs/2209.05274)

    该论文介绍了在机器学习中公平性的概念，并为线性动态系统观测预测中的子群体公平和瞬时公平引入了全球收敛方法，以解决代表性不足的偏见问题。

    

    在机器学习中，训练数据经常会捕捉到某个潜在人口的多个子群体的行为。这种行为经常可以被建模为一个未知动态系统的观测值，该系统具有未观察到的状态。然而，如果子群体的训练数据没有得到精心控制，那么就会出现代表性不足的偏见。为了对抗代表性不足的偏见，我们介绍了两种时间序列预测问题中公平的自然概念：子群体公平和瞬时公平。这些概念将预测公正性扩展到了动态系统的学习中。我们还展示了使用非交换多项式优化问题的凸化层次结构来公平地学习问题时的全球收敛方法。我们还展示，通过利用凸化中的稀疏性，可以大大减少运行时间。我们在一个由保险应用程序和著名的COMPAS数据激发的有偏数据集上展示了我们的经验结果。

    In machine learning, training data often capture the behaviour of multiple subgroups of some underlying human population. This behaviour can often be modelled as observations of an unknown dynamical system with an unobserved state. When the training data for the subgroups are not controlled carefully, however, under-representation bias arises. To counter under-representation bias, we introduce two natural notions of fairness in time-series forecasting problems: subgroup fairness and instantaneous fairness. These notions extend predictive parity to the learning of dynamical systems. We also show globally convergent methods for the fairness-constrained learning problems using hierarchies of convexifications of non-commutative polynomial optimisation problems. We also show that by exploiting sparsity in the convexifications, we can reduce the run time of our methods considerably. Our empirical results on a biased data set motivated by insurance applications and the well-known COMPAS data 
    
[^168]: 挑战关于灾难性遗忘的常见假设

    Challenging Common Assumptions about Catastrophic Forgetting. (arXiv:2207.04543v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04543](http://arxiv.org/abs/2207.04543)

    本文挑战了DNNs遭受灾难性遗忘的常见假设，提出当前优化技术可以缓解这种现象。我们的实验表明，使用标准梯度下降优化方法训练的DNNs在回归任务上可以积累知识，即使任务重新出现，也不会遗忘过去的知识，这种知识积累可以改善在未见任务上的泛化性能。

    

    建立能够逐步学习和积累知识的学习智能体是连续学习(CL)研究领域的核心目标。然而，将模型训练在新数据上通常会损害过去数据的性能。在CL文献中，这种效应被称为灾难性遗忘(CF)。尽管CF已被广泛研究，且已提出了大量方法来解决非重叠任务短序列的CF问题，但在这种设置下，CF总是导致过去任务的性能迅速而显著地下降。然而，最近的工作表明，在CL回归设置下，SGD训练线性模型可以累积知识。当任务重新发生时，这种现象尤为明显。我们可能会想知道，是否使用SGD或任何标准梯度下降优化方法训练的DNNs会以这种方式积累知识。这些现象会对将DNNs应用于真实的连续情境产生有趣的影响。实际上，标准的梯度下降优化技术被广泛使用，并为大规模问题提供高效的训练策略。在这项工作中，我们挑战常见的DNNs遭受CF的假设，并提出当前的优化技术可以缓解这种现象。我们展示了在回归任务上使用SGD训练的DNNs可以积累知识，即使任务重新出现，也不会遗忘过去的知识。我们在实际数据集上的实验表明，这种知识积累可以改善在未见任务上的泛化性能。

    Building learning agents that can progressively learn and accumulate knowledge is the core goal of the continual learning (CL) research field. Unfortunately, training a model on new data usually compromises the performance on past data. In the CL literature, this effect is referred to as catastrophic forgetting (CF). CF has been largely studied, and a plethora of methods have been proposed to address it on short sequences of non-overlapping tasks. In such setups, CF always leads to a quick and significant drop in performance in past tasks. Nevertheless, despite CF, recent work showed that SGD training on linear models accumulates knowledge in a CL regression setup. This phenomenon becomes especially visible when tasks reoccur. We might then wonder if DNNs trained with SGD or any standard gradient-based optimization accumulate knowledge in such a way. Such phenomena would have interesting consequences for applying DNNs to real continual scenarios. Indeed, standard gradient-based optimiz
    
[^169]: 多源域一般化的门控领域单元

    Gated Domain Units for Multi-source Domain Generalization. (arXiv:2206.12444v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12444](http://arxiv.org/abs/2206.12444)

    该研究提出了一个朴素的假设，即现实世界的分布由不同域中的潜在不变的基元分布（I.E.D）组成，其引入了一个由门控域单元（GDUs）组成的模块化神经网络层。这种方法可以更好地实现对看不见的域的知识转移，从而提高模型的性能。

    

    分布转移现象（DS）发生于测试时间数据集与训练时间数据集不同时的情况，这可能会在实际应用中明显影响机器学习模型性能，因为它缺乏有关测试时数据分布的知识。为解决这个问题，我们假设现实世界的分布由不同域中的潜在不变的基元分布（I.E.D）组成。这个假设意味着解空间中存在一个不变的结构，可以实现对看不见的域的知识转移。为了利用这个性质来进行域一般化，我们引入了一个由门控域单元（GDUs）组成的模块化神经网络层，它学习每个潜在基元分布的表示。在推断过程中，可以通过将新观测值与每个基元分布的表示进行比较，创建一个加权的学习机集合。我们的灵活框架还可以适应明确的情况。

    The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the dataset at training time, which can significantly impair the performance of a machine learning model in practical settings due to a lack of knowledge about the data's distribution at test time. To address this problem, we postulate that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. This assumption implies an invariant structure in the solution space that enables knowledge transfer to unseen domains. To exploit this property for domain generalization, we introduce a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution. During inference, a weighted ensemble of learning machines can be created by comparing new observations with the representations of each elementary distribution. Our flexible framework also accommodates scenarios where explicit
    
[^170]: 通过基于边的Weisfeiler-Lehman算法增强GNN

    Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm. (arXiv:2206.02059v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02059](http://arxiv.org/abs/2206.02059)

    本文提出了一种基于边缘感知的Weisfeiler-Lehman算法，以增强图神经网络的表达能力，同时保持消息传递方案的可扩展性。实验表明，我们NC-GNN框架在各种基准测试中表现出有效性和高效性。

    

    消息传递图神经网络(GNN)的表达能力被已知的一维Weisfeiler-Lehman (1-WL)算法上界所限制。为了实现更强大的GNN，现有的尝试要么需要特定的特征，要么涉及高时间和空间复杂度的操作。在本文中，我们提出了一个通用且可证明具有强大表达力的GNN框架，保持了消息传递方案的可扩展性。具体而言，我们首先通过考虑邻居之间的边缘来授权1-WL进行图同构测试，从而产生NC-1-WL。 NC-1-WL的表达能力在理论上被显示为严格高于1-WL且低于3-WL。进一步，我们提出了NC-GNN框架作为NC-1-WL的可区分神经版本。我们的简单NC-GNN实现可证明与NC-1-WL一样强大。实验表明，我们的NC-GNN在各种基准测试中表现出有效性和高效性。

    Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Lehman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require ad hoc features, or involve operations that incur high time and space complexities. In this work, we propose a general and provably powerful GNN framework that preserves the scalability of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks.
    
[^171]: 联邦递进稀疏化（清理、合并、调整）+：一种用于联邦学习的稀疏化策略

    Federated Progressive Sparsification (Purge, Merge, Tune)+. (arXiv:2204.12430v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.12430](http://arxiv.org/abs/2204.12430)

    本文提出了一种联邦学习的稀疏化策略，可以逐步约束模型的参数集，降低计算和通信成本，同时达到高性能的稀疏化率。实验表明，稀疏模型可以缩小原始模型的十分之一，而准确率不降低或更高。

    

    为了改进神经网络的联邦训练，我们开发了FedSparsify，一种基于渐进权重幅度剪枝的稀疏化策略。我们的方法有几个好处。首先，由于网络的大小越来越小，在训练期间计算和通信成本得到降低。其次，模型逐步限制为较小的参数集，有利于合并本地模型，提高高级稀疏率下的学习性能。第三，最终稀疏模型显著缩小，改善了推理效率，并优化了加密通信期间的操作延迟。我们的实验表明，FedSparsify可以学习具有高稀疏性和学习性能的子网络。相比于现有的剪枝和非剪枝基线，我们的稀疏模型在达到相同或更好的准确性的情况下，可以缩小原始模型的十分之一。

    To improve federated training of neural networks, we develop FedSparsify, a sparsification strategy based on progressive weight magnitude pruning. Our method has several benefits. First, since the size of the network becomes increasingly smaller, computation and communication costs during training are reduced. Second, the models are incrementally constrained to a smaller set of parameters, which facilitates alignment/merging of the local models and improved learning performance at high sparsification rates. Third, the final sparsified model is significantly smaller, which improves inference efficiency and optimizes operations latency during encrypted communication. We show experimentally that FedSparsify learns a subnetwork of both high sparsity and learning performance. Our sparse models can reach a tenth of the size of the original model with the same or better accuracy compared to existing pruning and nonpruning baselines.
    
[^172]: 基于变分指令集的量子编译优化：实现高精度与快速量子计算

    Quantum compiling with variational instruction set for accurate and fast quantum computing. (arXiv:2203.15574v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.15574](http://arxiv.org/abs/2203.15574)

    本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法，实现了快速和精确的量子计算，并且通过实验展示出了更低的误差积累和时间成本。

    

    量子指令集(QIS)定义为在控制量子比特状态下可以物理实现的一系列量子门操作，其在量子计算中起着基础性作用。本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法来变分实现量子比特的控制，从而实现快速和精确的量子计算。与标准QIS 如量子微指令集(QuMIS)相比，QuVIS 用于多量子比特交换和量子傅里叶变换等门操作具有更低的误差积累和时间成本。在相同量子硬件要求下，本文的方法可大幅提升量子计算的效率。

    The quantum instruction set (QIS) is defined as the quantum gates that are physically realizable by controlling the qubits in a quantum hardware. Compiling quantum circuits into the product of the gates in a properly-defined QIS is a fundamental step in quantum computing. We here propose the \R{quantum variational instruction set (QuVIS)} formed by flexibly-designed multi-qubit gates for higher speed and accuracy of quantum computing. The controlling of qubits for realizing the gates in a QuVIS are variationally achieved using the fine-grained time optimization algorithm. Significant reductions on both the error accumulation and time cost are demonstrated in realizing the swaps of multiple qubits and quantum Fourier transformations, compared with the compiling by the standard QIS such as \RR{the quantum microinstruction set} (QuMIS, formed by several one- and two-qubit gates including the one-qubit rotations and controlled-NOT gate). With the same requirement on quantum hardware, the t
    
[^173]: 基于凸约束的优化问题中DNN方案的可行性保证及其在直流最优潮流问题中的应用

    Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems. (arXiv:2112.08091v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08091](http://arxiv.org/abs/2112.08091)

    本文提出了一个“预防性学习”框架，以在满足对约束标定的条件下，保证具有凸约束和一般目标函数的问题的DNN解的可行性，而无需后处理。通过系统标定不等式约束，我们预示预测误差并确保所得到的解仍然是可行的。同时提出了一种新的对抗样本感知的训练算法以提高DNN的最优性能而不牺牲可行性保证。

    

    在开发用于解决受限制优化问题的深度神经网络（DNN）方案时，确保解的可行性是一个关键挑战，由于DNN固有的预测误差。本文提出了一个“预防性学习”框架，以在满足对约束标定的温和条件下，保证具有凸约束和一般目标函数的问题的DNN解的可行性，而无需后处理。我们无失一般性地关注只有不等式约束的问题。我们系统地标定DNN训练中使用的不等式约束，从而预示预测误差并确保所得到的解仍然是可行的。我们表征了标定量和DNN大小足以确保通用可行性。我们提出了一种新的对抗样本感知的训练算法，以提高DNN的最优性能，而不会牺牲可行性保证。总的来说，该框架提供了两个DNN解。

    Ensuring solution feasibility is a key challenge in developing Deep Neural Network (DNN) schemes for solving constrained optimization problems, due to inherent DNN prediction errors. In this paper, we propose a ``preventive learning'' framework to guarantee DNN solution feasibility for problems with convex constraints and general objective functions without post-processing, upon satisfying a mild condition on constraint calibration. Without loss of generality, we focus on problems with only inequality constraints. We systematically calibrate inequality constraints used in DNN training, thereby anticipating prediction errors and ensuring the resulting solutions remain feasible. We characterize the calibration magnitudes and the DNN size sufficient for ensuring universal feasibility. We propose a new Adversarial-Sample Aware training algorithm to improve DNN's optimality performance without sacrificing feasibility guarantee. Overall, the framework provides two DNNs. The first one from ch
    
[^174]: 一种用于潜变量生成模型的矩匹配度量方法

    A moment-matching metric for latent variable generative models. (arXiv:2111.00875v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.00875](http://arxiv.org/abs/2111.00875)

    本文提出了一种用于比较和正则化潜变量生成模型的新型矩匹配度量方法，该方法通过研究数据矩和模型矩之间的差异来评估拟合模型质量。

    

    面对无监督学习问题时，评估拟合模型的质量是困难的。潜变量模型，如变分自编码器和高斯混合模型，通常使用基于似然的方法进行训练。本文提出了一种新的用于模型比较或正则化的度量方法，该方法依赖于矩。其概念是使用矩范数（如弗罗贝尼乌斯范数）研究数据矩和模型矩之间的差异。我们展示了如何使用这个新的度量方法进行模型比较和正则化，并证明了该方法的可行性。

    It can be difficult to assess the quality of a fitted model when facing unsupervised learning problems. Latent variable models, such as variation autoencoders and Gaussian mixture models, are often trained with likelihood-based approaches. In scope of Goodhart's law, when a metric becomes a target it ceases to be a good metric and therefore we should not use likelihood to assess the quality of the fit of these models. The solution we propose is a new metric for model comparison or regularization that relies on moments. The concept is to study the difference between the data moments and the model moments using a matrix norm, such as the Frobenius norm. We show how to use this new metric for model comparison and then for regularization. It is common to draw samples from the fitted distribution when evaluating latent variable models and we show that our proposed metric is faster to compute and has a smaller variance that this alternative. We conclude this article with a proof of concept o
    
[^175]: 高维推断下的动态治疗效应估计

    High-dimensional Inference for Dynamic Treatment Effects. (arXiv:2110.04924v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2110.04924](http://arxiv.org/abs/2110.04924)

    本文提出了一种新的 DR 方法，用于中间条件结果模型的 DR 表示，能够提供更优的稳健性保证，即使在面临高维混淆变量时也能实现一致性。

    

    在因果推断中，估计动态治疗效应是一项重要的任务，尤其是当面临高维混淆变量时。双重稳健 (DR) 方法因其灵活性而成为估计治疗效应的有力工具。然而，我们展示了仅关注预期结果的 DR 传统方法可能无法提供最优结果。在本文中，我们提出了一种新的 DR 方法，用于中间条件结果模型的 DR 表示，从而提供了更优的稳健性保证。只要每个暴露时间和治疗路径都恰当地参数化了至少一个辅助函数，所提出的方法即使在面临高维混淆变量时也能实现一致性。我们的结果代表了一个重大的进步，因为它们提供了新的稳健性保证。实现这些结果的关键是我们的新 DR 表示，它在需要比以前的方法更弱的假设条件下提供了更好的推断性能。

    Estimating dynamic treatment effects is a crucial endeavor in causal inference, particularly when confronted with high-dimensional confounders. Doubly robust (DR) approaches have emerged as promising tools for estimating treatment effects due to their flexibility. However, we showcase that the traditional DR approaches that only focus on the DR representation of the expected outcomes may fall short of delivering optimal results. In this paper, we propose a novel DR representation for intermediate conditional outcome models that leads to superior robustness guarantees. The proposed method achieves consistency even with high-dimensional confounders, as long as at least one nuisance function is appropriately parametrized for each exposure time and treatment path. Our results represent a significant step forward as they provide new robustness guarantees. The key to achieving these results is our new DR representation, which offers superior inferential performance while requiring weaker ass
    
[^176]: 从深度学习中汲取经验教训，训练脉冲神经网络

    Training Spiking Neural Networks Using Lessons From Deep Learning. (arXiv:2109.12894v5 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2109.12894](http://arxiv.org/abs/2109.12894)

    本论文介绍如何将几十年的深度学习、梯度下降、反向传播和神经科学研究的经验教训应用于生物可行的脉冲神经网络，并探索了将数据编码为脉冲和学习过程之间的微妙相互作用以及生物可行的在线学习的发展方向。

    

    大脑是寻找灵感以开发更有效的神经网络的完美之地。我们的突触和神经元的内部运作提供了对未来深度学习可能呈现的样子的一瞥。本文旨在介绍如何将几十年的深度学习、梯度下降、反向传播和神经科学研究的经验教训应用于生物可行的脉冲神经网络。我们还探讨了将数据编码为脉冲和学习过程之间微妙的相互作用；将基于梯度的学习应用到脉冲神经网络 (SNNs) 的挑战和解决方案；时间反向传播和脉冲时序依赖性可塑性之间微妙的联系以及深度学习如何向着生物可行的在线学习发展。一些想法在神经形态工程界得到了广泛的认可和使用，而其他一些在本文中首次呈现或得到了证明。SNN领域仍处于开发初期，需要解决许多挑战和限制。尽管如此，我们认为脉冲神经网络是理解大脑和开发更有效和智能的人工智能系统的有前途的途径。

    The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This paper serves as a tutorial and perspective showing how to apply the lessons learnt from several decades of research in deep learning, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks.  We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to spiking neural networks (SNNs); the subtle link between temporal backpropagation and spike timing dependent plasticity, and how deep learning might move towards biologically plausible online learning. Some ideas are well accepted and commonly used amongst the neuromorphic engineering community, while others are presented or justified for the first time here.  The fiel
    
[^177]: MRCpy：一种用于最小化风险分类器的库

    MRCpy: A Library for Minimax Risk Classifiers. (arXiv:2108.01952v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.01952](http://arxiv.org/abs/2108.01952)

    MRCpy是一种用于实现最小化风险分类器的Python库，它基于鲁棒风险最小化技术，可以利用0-1损失并提供了多种分类方法，其中一些提供了紧密的期望损失界限。

    

    目前现有的监督分类库都是基于经验风险最小化和使用代理损失技术的。本文介绍MRCpy库，该库实现了基于鲁棒风险最小化的最小化风险分类器（MRC），并可利用0-1损失。这种技术产生了许多分类方法，可以提供紧密的期望损失界限。MRCpy为不同变量的MRC提供了统一的接口，并遵循流行Python库的标准。此外，MRCpy还提供了实现一些流行技术的功能，这些技术可以看作是MRC，例如L1正则化逻辑回归，0-1对抗性和最大熵机。此外，MRCpy还实现了最近的特征映射，如傅里叶，ReLU和阈值特征。该库采用面向对象的方法设计，方便协作者和用户。

    Existing libraries for supervised classification implement techniques that are based on empirical risk minimization and utilize surrogate losses. We present MRCpy library that implements minimax risk classifiers (MRCs) that are based on robust risk minimization and can utilize 0-1-loss. Such techniques give rise to a manifold of classification methods that can provide tight bounds on the expected loss. MRCpy provides a unified interface for different variants of MRCs and follows the standards of popular Python libraries. The presented library also provides implementation for popular techniques that can be seen as MRCs such as L1-regularized logistic regression, zero-one adversarial, and maximum entropy machines. In addition, MRCpy implements recent feature mappings such as Fourier, ReLU, and threshold features. The library is designed with an object-oriented approach that facilitates collaborators and users.
    
[^178]: GaNDLF：一种通用细致的深度学习框架，用于医学影像中可扩展的端对端临床工作流

    GaNDLF: A Generally Nuanced Deep Learning Framework for Scalable End-to-End Clinical Workflows in Medical Imaging. (arXiv:2103.01006v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.01006](http://arxiv.org/abs/2103.01006)

    GaNDLF是一个通用的且易用的深度学习框架，旨在降低深度学习算法开发的难度，并提供了一种使深度学习可重复、可解释和可扩展的端到端解决方案，它在医学影像中具有广泛的应用。

    

    深度学习在科学和临床社区中都有优化机器学习的潜力。然而，开发深度学习算法需要更多的专业知识，并且实现的可变性会影响它们的再现性、转化性和部署性。在这里，我们介绍了由社区驱动的通用细致深度学习框架（GaNDLF），旨在降低这些障碍。 GaNDLF使得深度学习开发、训练和推断机制更加稳定，可重复性、可解释性和可扩展性，而不需要广泛的技术背景。 GaNDLF旨在为计算精密医学中所有深度学习相关任务提供端到端的解决方案。我们演示了GaNDLF分析放射学和组织学图像的能力，具有内置的支持k-fold交叉验证，数据扩充，多种模态和输出类别。我们对多个用例、解剖部位和计算任务的定量性能评估支持GaNDLF的多功能性，并且具有最少的性能损失可以扩展到更大的数据集。总之，GaNDLF是一个用户友好和强大的深度学习框架，旨在促进基于深度学习的医学影像临床工作流。

    Deep Learning (DL) has the potential to optimize machine learning in both the scientific and clinical communities. However, greater expertise is required to develop DL algorithms, and the variability of implementations hinders their reproducibility, translation, and deployment. Here we present the community-driven Generally Nuanced Deep Learning Framework (GaNDLF), with the goal of lowering these barriers. GaNDLF makes the mechanism of DL development, training, and inference more stable, reproducible, interpretable, and scalable, without requiring an extensive technical background. GaNDLF aims to provide an end-to-end solution for all DL-related tasks in computational precision medicine. We demonstrate the ability of GaNDLF to analyze both radiology and histology images, with built-in support for k-fold cross-validation, data augmentation, multiple modalities and output classes. Our quantitative performance evaluation on numerous use cases, anatomies, and computational tasks supports G
    
[^179]: 基于图神经网络的强化学习生产计划问题调度器

    Graph neural networks-based Scheduler for Production planning problems using Reinforcement Learning. (arXiv:2009.03836v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.03836](http://arxiv.org/abs/2009.03836)

    本文提出了GraSP-RL框架，基于图神经网络来训练强化学习代理，以解决车间调度问题中状态空间难以处理、泛化能力较差的问题。

    

    强化学习在车间调度问题中得到广泛应用。但对于车间调度问题，强化学习通常使用矢量化机器特征作为状态空间。这种方法存在三个主要问题：（1）机器单元和作业序列之间的关系没有完全捕获，（2）状态空间随着机器/作业数量的增加呈指数增长，（3）代理的泛化能力存在问题。本文提出了一种新的框架——GraSP-RL，基于图神经网络的强化学习生产计划问题调度器。它将车间调度问题表示为图形，并使用图神经网络(GNN)提取特征来训练强化学习代理。虽然图形本身在非欧几里德空间中，但使用GNN提取的特征在欧几里德空间中提供了当前生产状态的丰富编码，然后被强化学习代理用于选择下一个作业。此外，我们将调度问题视为一个基于图神经网络的流问题，使用回溯方法对此进行求解。

    Reinforcement learning (RL) is increasingly adopted in job shop scheduling problems (JSSP). But RL for JSSP is usually done using a vectorized representation of machine features as the state space. It has three major problems: (1) the relationship between the machine units and the job sequence is not fully captured, (2) exponential increase in the size of the state space with increasing machines/jobs, and (3) the generalization of the agent to unseen scenarios. We present a novel framework - GraSP-RL, GRAph neural network-based Scheduler for Production planning problems using Reinforcement Learning. It represents JSSP as a graph and trains the RL agent using features extracted using a graph neural network (GNN). While the graph is itself in the non-euclidean space, the features extracted using the GNNs provide a rich encoding of the current production state in the euclidean space, which is then used by the RL agent to select the next job. Further, we cast the scheduling problem as a de
    
[^180]: 基于最优传输的模型融合方法

    Model Fusion via Optimal Transport. (arXiv:1910.05653v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.05653](http://arxiv.org/abs/1910.05653)

    本文提出一种基于最优传输的神经网络模型融合算法，能够成功地在不需要重新训练的情况下进行“单次”知识迁移，并且在独立同分布和非独立同分布的情况下比简单平均和集成模型更优。

    

    在机器学习应用中，结合不同的模型是一个广泛使用的范例。尽管最常见的方法是形成一个模型集合并平均它们的各自预测，但由于资源限制（以内存和计算的方式呈线性增长于模型数），这种方法常常无法实现。我们提出了一种基于层次的神经网络模型融合算法，利用最优传输来（软）对齐模型中的神经元，然后平均它们的相关参数。我们展示了这可以在异构非独立同分布数据上，成功地实现“单次”知识迁移（即不需要任何重新训练）在神经网络之间。在独立同分布和非独立同分布的情况下，我们示范了我们的方法明显优于简单平均以及如何在标准卷积网络（如VGG11）、残差网络上进行快速优化替代集成模型。

    Combining different models is a widely used paradigm in machine learning applications. While the most common approach is to form an ensemble of models and average their individual predictions, this approach is often rendered infeasible by given resource constraints in terms of memory and computation, which grow linearly with the number of models. We present a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters.  We show that this can successfully yield "one-shot" knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings , we illustrate that our approach significantly outperforms vanilla averaging, as well as how it can serve as an efficient replacement for the ensemble with moderate fine-tuning, for standard convolutional networks (like VGG11), residual networks
    
[^181]: S-ConvNet: 一种深度表面肌电图像神经肌肉活动识别的浅层卷积神经网络架构

    S-ConvNet: A Shallow Convolutional Neural Network Architecture for Neuromuscular Activity Recognition Using Instantaneous High-Density Surface EMG Images. (arXiv:1906.03381v1 [eess.SP] CROSS LISTED)

    [http://arxiv.org/abs/1906.03381](http://arxiv.org/abs/1906.03381)

    S-ConvNet和All-ConvNet模型为神经肌肉活动识别提出了一种简单而有效的学习框架，不需要预训练，表现出非常具有竞争力的识别准确性。

    

    利用瞬时高密度表面肌电图像进行神经肌肉活动识别的概念为开发更流畅和自然的肌肉-计算机接口开辟了新的途径。然而，现有的方法采用了非常大的深度卷积神经网络(ConvNet)架构和复杂的训练方案来进行HD-sEMG图像识别，需要在非常大规模的标记训练数据集上预训练网络架构，因此计算非常昂贵。为了解决这个问题，本文提出了一种简单而有效的学习即时HD-sEMG图像的神经肌肉活动识别框架——S-ConvNet和All-ConvNet模型。在不使用任何预训练模型的情况下，我们提出的S-ConvNet和All-ConvNet表现出了非常具有竞争力的识别准确性，比基于瞬时HD-sEMG图像的神经肌肉活动识别的更复杂的最先进技术更具优势。

    The concept of neuromuscular activity recognition using instantaneous high-density surface electromyography (HD-sEMG) images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the existing approaches employed a very large deep convolutional neural network (ConvNet) architecture and complex training schemes for HD-sEMG image recognition, which requires the network architecture to be pre-trained on a very large-scale labeled training dataset, as a result, it makes computationally very expensive. To overcome this problem, we propose S-ConvNet and All-ConvNet models, a simple yet efficient framework for learning instantaneous HD-sEMG images from scratch for neuromuscular activity recognition. Without using any pre-trained models, our proposed S-ConvNet and All-ConvNet demonstrate very competitive recognition accuracy to the more complex state of the art for neuromuscular activity recognition based on instantaneous HD-sEMG images, while u
    

