# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Broadcasting in random recursive dags.](http://arxiv.org/abs/2306.01727) | 该论文研究了一个均匀的$k$-dag广播模型，确定了与$p$和$k$有关的阈值，并讨论了大多数规则的误差率。 |
| [^2] | [Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification.](http://arxiv.org/abs/2306.01726) | 本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。 |
| [^3] | [Graph Sparsification for GCN Towards Optimal Crop Yield Predictions.](http://arxiv.org/abs/2306.01725) | 本文提出了一种基于Fiedler数的图稀疏化方法，可以从完整的图核中删除边缘，以降低GCN训练和执行的复杂度，并显著提高农业产量预测的可行性。 |
| [^4] | [A Data-Driven Measure of Relative Uncertainty for Misclassification Detection.](http://arxiv.org/abs/2306.01710) | 本文提出了一种基于数据驱动的相对不确定性度量，用于误分类检测。该度量可以通过学习软预测的分布模式，识别出被误分类的样本，并展示了在多个图像分类任务中的实证改进，优于现有的误分类检测方法。 |
| [^5] | [Resolving Interference When Merging Models.](http://arxiv.org/abs/2306.01708) | 本文揭示了现有模型合并技术存在的干扰问题，提出了具有广泛适用性的解决方案，可显着提高合并后模型的性能。 |
| [^6] | [Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?.](http://arxiv.org/abs/2306.01706) | 本研究探讨了生成建模风格化技术在领域自适应回归任务中是否必要，发现与分类任务相比，其对于回归任务的影响较小。 |
| [^7] | [The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles.](http://arxiv.org/abs/2306.01705) | Transformer使用稠密的自注意力机制，使得在深度网络中可能的连接模式数量呈指数级增长。我们提出了信息通路假说，利用这种特性将连接模式分解为相互独立的信息通路，减少训练过程中的内存和计算成本以及提高模型的泛化能力。 |
| [^8] | [Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy.](http://arxiv.org/abs/2306.01699) | 该论文提出了一种基于亲和性聚类的数据扩充方法MASC，通过同类数据集中的亲和聚类和保护数据的共享达到数据集的平衡，从而解决数据集代表性偏见问题。 |
| [^9] | [MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators.](http://arxiv.org/abs/2306.01697) | MutateNN是一种用于探索硬件加速器上深度学习图像识别模型鲁棒性的工具，提供突变测试和分析能力，且有效性已在多种预训练深度神经网络模型中得到验证。 |
| [^10] | [Evaluating Language Models for Mathematics through Interactions.](http://arxiv.org/abs/2306.01694) | 本文介绍了一个可进行人机交互评估LLMs的原型平台CheckMate，并利用该平台评估了三个语言模型在大学级数学证明助手方面的能力，发布了结果数据集MathConverse，并得出了人类行为的初步分类和LMM生成正确性与感知帮助性分歧的发现。 |
| [^11] | [Uniform Convergence of Deep Neural Networks with Lipschitz Continuous Activation Functions and Variable Widths.](http://arxiv.org/abs/2306.01692) | 本文提出了针对具有Lipschitz连续激活函数和可变宽度的深度神经网络的统一收敛性分析，给出了充分条件以及Lipschitz常数，建立了相应的统一收敛性结果。 |
| [^12] | [GateON: an unsupervised method for large scale continual learning.](http://arxiv.org/abs/2306.01690) | GateON是一种用于大规模连续学习的无监督方法，通过可学习的活动门控和参数相关性的在线估计来防止重要知识被覆盖，同时通过定点神经元的重新激活机制解决了网络饱和的问题。 |
| [^13] | [Harnessing large-language models to generate private synthetic text.](http://arxiv.org/abs/2306.01684) | 本文研究了一种通过利用公开数据集和预训练的生成语言模型，结合私有的微调方法，实现生成具有差分隐私性质的合成文本数据集的方法。生成的文本数据集可以重复使用于其他任务，可无限期保留，或与第三方共享而不损失隐私。 |
| [^14] | [Neural Differential Recurrent Neural Network with Adaptive Time Steps.](http://arxiv.org/abs/2306.01674) | 本研究提出了一种基于神经ODE的RNN模型，通过调整时间步长，可以高效地处理尖峰型非平稳时间序列数据。模型可以有效估计Hawkes类型时间序列数据的强度函数，能够显著提高预测精度并减少计算消耗。 |
| [^15] | [Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning.](http://arxiv.org/abs/2306.01669) | 本文探索使用伪标注方法通过提示调参改进CLIP的方法，通过使用零样本伪标签来优化图像分类任务中有限标注数据的问题。 |
| [^16] | [XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models.](http://arxiv.org/abs/2306.01668) | 本文探讨了可解释人工智能（XAI）在革新医学诊断模型的创新方法和方法论，并通过揭示潜在的决策过程来赋予医疗保健专业人员理解和信任这些模型的能力。这些新技术潜力巨大，有望改变医疗保健的格局，从而提高患者的结果并建立对AI驱动诊断系统的信任。 |
| [^17] | [An Adaptive Method for Weak Supervision with Drifting Data.](http://arxiv.org/abs/2306.01658) | 本文在非稳态设置中提出了一种自适应弱监督方法，该方法可以推断序列数据的未知标签，并适应时间漂移，而无需假设漂移幅度。 |
| [^18] | [Poisoning Network Flow Classifiers.](http://arxiv.org/abs/2306.01655) | 研究了网络流分类器的后门攻击，提出了利用模型可解释性技术生成有效触发器模式和生成隐秘触发器的新策略，以应对对手干扰训练数据的挑战。 |
| [^19] | [GANs Settle Scores!.](http://arxiv.org/abs/2306.01654) | 这篇论文提出了一种新的方法，通过变分方法来统一分析生成器的优化，并展示了在f-散度最小化和IPM GAN中生成器的最优解决方案。这种方法能够平滑分数匹配。 |
| [^20] | [Fair multilingual vandalism detection system for Wikipedia.](http://arxiv.org/abs/2306.01650) | 本文介绍了一种支持多语言维基百科破坏检测的新型系统设计，经过评估发现其覆盖了更多的语言，效率更高，并且比维基百科中生产使用的ORES系统更加准确和公正。 |
| [^21] | [Federated Multi-Sequence Stochastic Approximation with Local Hypergradient Estimation.](http://arxiv.org/abs/2306.01648) | 本文提出了FedMSA，这是第一个针对多序列随机逼近（MSA）的联邦算法，并建立了其近乎最优的通信复杂度。通过本地客户端更新，FedMSA实现了BLO和MCO中超梯度的可证估计。文中还结合动量和方差缩减技术来加速，导致接近最优的速率。 |
| [^22] | [Auditing for Human Expertise.](http://arxiv.org/abs/2306.01646) | 人类专家的价值超出了算法可捕捉范围，我们可以用一个简单的程序测试这个问题。 |
| [^23] | [Reduction of finite sampling noise in quantum neural networks.](http://arxiv.org/abs/2306.01639) | 介绍方差规范化技术方法，减小量子神经网络的有限采样噪声，在QNN的构造妥善的情况下，无需额外电路计算，测试发现可以显著地降低噪声水平及加快训练速度。 |
| [^24] | [Do we become wiser with time? On causal equivalence with tiered background knowledge.](http://arxiv.org/abs/2306.01638) | 本文探究了如何利用分层背景知识来限制等价类，从而有效简化因果效应估计和提高计算效率，同时提供了关于背景知识有效的见解。 |
| [^25] | [Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network.](http://arxiv.org/abs/2306.01631) | 本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。 |
| [^26] | [HomE: Homography-Equivariant Video Representation Learning.](http://arxiv.org/abs/2306.01623) | 本文提出了一种新的多视角视频表示学习方法，通过学习隐式映射，形成了一个具有同变性的表示空间，能够对行动识别和行人意图预测任务获得最先进的结果。 |
| [^27] | [Analyzing Credit Risk Model Problems through NLP-Based Clustering and Machine Learning: Insights from Validation Reports.](http://arxiv.org/abs/2306.01618) | 本文利用聚类和机器学习算法对信贷风险模型中的问题进行了研究，通过分析验证报告中的文本信息，可以识别和分类信贷风险模型中的常见问题。 |
| [^28] | [Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization.](http://arxiv.org/abs/2306.01613) | 本文提出了一种考虑攻击对超参数影响的最优攻击公式，将攻击建模为多目标双层优化问题，可以更准确地评估算法鲁棒性和学习超参数，在多个数据集上的评估证明了这种方法的优势。 |
| [^29] | [Centered Self-Attention Layers.](http://arxiv.org/abs/2306.01610) | 本文提出了一种基于中心化自注意力层的纠错机制，它可以消除transformers中过度平滑的问题，并使图神经网络训练非常深的架构比许多最近的解决方案更有效。 |
| [^30] | [Decentralized Federated Learning: A Survey and Perspective.](http://arxiv.org/abs/2306.01603) | 去中心化联邦学习通过消除中心服务器的需求，实现了直接通信从而节省了通信资源。本文提供了对DFL的全面调查、深入展望和扩展变体与分类的介绍，重点在于DFL的系统与详细前景。 |
| [^31] | [Transfer learning for atomistic simulations using GNNs and kernel mean embeddings.](http://arxiv.org/abs/2306.01589) | 本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。 |
| [^32] | [Probabilistic Concept Bottleneck Models.](http://arxiv.org/abs/2306.01574) | 本文提出了概率概念瓶颈模型（ProbCBM），通过建立概率概念嵌入来解决数据中概念存在模糊性的问题，提高了CBM模型的可靠性及解释的可信度。 |
| [^33] | [Spatio-Temporal Deep Learning-Assisted Reduced Security-Constrained Unit Commitment.](http://arxiv.org/abs/2306.01570) | 本研究提出了一种利用时空深度学习模型进行安全受限机组组合优化的新方法，并通过对多个测试系统的验证，证明了其在解决方案质量和计算时间方面的有效性。 |
| [^34] | [Publicly available datasets of breast histopathology H&E whole-slide images: A systematic review.](http://arxiv.org/abs/2306.01546) | 本文从乳腺H&E染色全切片图像数据集的公开来源出发，系统性地综述了当前可用的数据集情况，包含了12个公开数据集和相关特征信息。总结出数据集的缺失以及更全面和标准化的数据集建立的必要性。 |
| [^35] | [Does it pay to optimize AUC?.](http://arxiv.org/abs/2306.01528) | 本文提出了一种高效算法AUC-opt，用于在$\mathbb{R}^2$中找到可证明的最优AUC线性分类器，并可扩展到$\mathbb{R}^d$。实验表明，在合成和真实数据集上，与其他方法相比，AUC-opt可以显着提高AUC值。优化AUC确实有价值，并且先前研究的限制主要是由于缺乏高效的优化算法。 |
| [^36] | [Transformer-based Multi-Modal Learning for Multi Label Remote Sensing Image Classification.](http://arxiv.org/abs/2306.01523) | 本文提出了一种基于Transformer的多模态学习架构，能够在远程感知图像的多模态多标签分类中进行有效的信息交换，提高了分类效果。 |
| [^37] | [Network Degeneracy as an Indicator of Training Performance: Comparing Finite and Infinite Width Angle Predictions.](http://arxiv.org/abs/2306.01513) | 本文发现网络劣化现象，这会影响网络训练并导致其表现不佳。我们还使用了一种简单算法来预测网络劣化的水平。 |
| [^38] | [Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations.](http://arxiv.org/abs/2306.01505) | 本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示，通过联合类别分布对比学习目标，有效利用标签级特性一致性并保留细粒度的类内特性，实现了在对话情感识别中最先进的结果。 |
| [^39] | [Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today.](http://arxiv.org/abs/2306.01499) | 本文探讨了LLMs如GPT-4在痴呆症诊断上的潜力，发现目前还无法胜过传统AI工具。 |
| [^40] | [Local Message Passing on Frustrated Systems.](http://arxiv.org/abs/2306.01494) | 本文提出了一种优化的基于困难系统的局部消息传递算法，能够在循环图上获得良好的表现。 |
| [^41] | [On Feature Diversity in Energy-based Models.](http://arxiv.org/abs/2306.01489) | 本文研究了能量模型中特征的多样性，分析了冗余的效果，并证明减少特征集的冗余可以提高模型性能。 |
| [^42] | [Robust low-rank training via approximate orthonormal constraints.](http://arxiv.org/abs/2306.01485) | 通过追加正交约束，从而在保持低秩矩阵分解前提下提高深度神经网络的鲁棒性与准确率。 |
| [^43] | [Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking Intent in Recommender Systems.](http://arxiv.org/abs/2306.01476) | 本文提出了一种基于层次强化学习的方法，用于建模用户的层次新奇寻求意图并调整推荐策略以提高推荐项目准确性和多样性。 |
| [^44] | [Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations.](http://arxiv.org/abs/2306.01475) | 本文提出了一种端到端的方法，将面向方面的提取和推荐相结合，同时引入了个性化基于方面的推荐模型，并利用大型语言模型设计了新的模型来为最终的推荐任务生成方面。 |
| [^45] | [Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning.](http://arxiv.org/abs/2306.01474) | 本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。 |
| [^46] | [Guiding Text-to-Text Privatization by Syntax.](http://arxiv.org/abs/2306.01471) | 该论文介绍了一种基于语法的文本隐私保护方法，通过解决候选选择问题以提高替换的句法一致性。 |
| [^47] | [MLP-Mixer as a Wide and Sparse MLP.](http://arxiv.org/abs/2306.01470) | 深度学习中常用的MLP有潜力提高性能。本研究揭示MLP-Mixer 可以作为具有稀疏权重的宽MLP有效地工作。 |
| [^48] | [GANs and alternative methods of synthetic noise generation for domain adaption of defect classification of Non-destructive ultrasonic testing.](http://arxiv.org/abs/2306.01469) | 本文针对无损测试领域数据训练不足的问题，提出了四种基于半解析模拟数据的合成数据生成方法，并通过超参数优化，使用卷积神经网络进行实验数据分类。其中第一种方法修改了CycleGAN，从物理模拟到实验数据进行了映射。 |
| [^49] | [Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables.](http://arxiv.org/abs/2306.01464) | 本论文综合研究表明，XAI 方法在存在抑制变量时解释可能出现误导性，需要进行更加理论化和经验化的研究，确保其应用的正确性和可能性。 |
| [^50] | [ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages.](http://arxiv.org/abs/2306.01460) | 本研究提出了一种新的On-Policy深度强化学习算法，该算法通过在保守值估计和谨慎探索方面的明确整合来解决了当前算法不能充分考虑谨慎交互的问题。 |
| [^51] | [Driving Context into Text-to-Text Privatization.](http://arxiv.org/abs/2306.01457) | 本文介绍了一种度量差分隐私机制，该机制在注入噪声之前加入了一个语义消歧步骤以提高歧义单词替代的有效性，并展示了在“上下文中的单词”数据集上的有效性。 |
| [^52] | [Unsupervised Paraphrasing of Multiword Expressions.](http://arxiv.org/abs/2306.01443) | 本文提出了一种无监督的多词表达式改写方法，使用的数据和工具非常简单，且实验结果表明，该方法在熟语语义文本相似度任务中性能良好。 |
| [^53] | [Towards Robust FastSpeech 2 by Modelling Residual Multimodality.](http://arxiv.org/abs/2306.01442) | 该论文提出了一种名为TVC-GMM的三元链式高斯分布混合模型，用于解决FastSpeech 2合成表现性语音数据集时可能出现的Mel频谱平滑度差的问题，可以提高音频的听觉质量。 |
| [^54] | [Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction.](http://arxiv.org/abs/2306.01439) | 该论文介绍了一种名为NUDGE的策略，利用训练好的基于神经网络的代理来引导逻辑规则的搜索，实现了可解释和可解释的策略。 |
| [^55] | [Multi-Objective Population Based Training.](http://arxiv.org/abs/2306.01436) | 作者提出了一种多目标版基因群体训练算法MO-PBT，实验证明其在多种冲突目标下均优于其他算法。 |
| [^56] | [Improving Adversarial Robustness of DEQs with Explicit Regulations Along the Neural Dynamics.](http://arxiv.org/abs/2306.01435) | 本论文通过神经动力学的解释，提出了一种新的对抗训练框架，通过逐步更新输入来降低预测熵，从而提高DEQ模型的对抗性。 |
| [^57] | [Zero-Shot Blind Audio Bandwidth Extension.](http://arxiv.org/abs/2306.01433) | 本文提出了一种名为BABE的新方法，在零样本情况下解决了具有挑战性的盲音频带宽扩展问题，实现了比最先进的盲带宽扩展基线更好的性能，并且表现出了强大的泛化能力。 |
| [^58] | [Audio-Visual Speech Enhancement with Score-Based Generative Models.](http://arxiv.org/abs/2306.01432) | 本文介绍了一种利用基于得分的生成模型，以视觉信息为条件的音视频语音增强系统。实验证明该系统提高了语音质量，并减少了语音混淆。 |
| [^59] | [On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions.](http://arxiv.org/abs/2306.01431) | 本文介绍了一份关于联邦学习中知识编辑的广泛调查，总结了最新技术，确定了挑战和机遇，并概述了未来的研究方向。 |
| [^60] | [A Closer Look at the Adversarial Robustness of Deep Equilibrium Models.](http://arxiv.org/abs/2306.01429) | 本研究探究了深度平衡模型的对抗鲁棒性问题，提出了估计中间梯度以改进攻击流程的方法。 |
| [^61] | [Improved DeepFake Detection Using Whisper Features.](http://arxiv.org/abs/2306.01428) | 该论文探究使用Whiper模型作为DF检测的前端的影响，通过对比训练3个检测模型后在广泛使用的ASVspoof 2021 DF数据集上的效果，在DF In-The-Wild数据集上测试后表明，使用基于Whisper的特征能够提高每个模型的检测效果，并将等误差率降低了21％，从而超越了最近在In-The-Wild数据集上获得的结果。 |
| [^62] | [Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model.](http://arxiv.org/abs/2306.01424) | 本文研究了连续性结果的部分反事实识别问题，并提出了一种新颖的敏感性模型——曲率敏感模型，通过限制函数级集的曲率来获得信息边界。 |
| [^63] | [Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation.](http://arxiv.org/abs/2306.01423) | 本文提出了一种新的深度优化器FAME，使用三重指数移动平均值（TEMA）来估计梯度矩，提供更丰富和准确的数据变化和趋势信息，可以提高计算机视觉等领域中模型的性能表现。 |
| [^64] | [The Flawed Foundations of Fair Machine Learning.](http://arxiv.org/abs/2306.01417) | 该论文对公平机器学习领域的现在存在的缺陷进行探讨，指出了研究者未能正确理解在实现公平性时，准确率和组相似性之间的权衡关系，该权衡关系可能对公平性构成实质性威胁。 |
| [^65] | [Chemical Property-Guided Neural Networks for Naphtha Composition Prediction.](http://arxiv.org/abs/2306.01391) | 本研究提出了一种利用化学性质信息将卷积神经网络和多层感知器网络结合的神经网络框架，用于提高石脑油成分预测的性能。 |
| [^66] | [Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training.](http://arxiv.org/abs/2306.01381) | 本文提出了一种高效的GNN训练系统AdaQP，通过随机量化跨设备传输的消息以降低通信流量，并提倡边缘节点和中心节点之间的通信-计算并行化，以加快分布式全图GNN训练速度。 |
| [^67] | [Robust and Generalisable Segmentation of Subtle Epilepsy-causing Lesions: a Graph Convolutional Approach.](http://arxiv.org/abs/2306.01375) | 该论文提出了一种通过图卷积网络进行语义分割的方法，能够对细微的癫痫病灶进行识别，降低误报率。 |
| [^68] | [DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents.](http://arxiv.org/abs/2306.01359) | 这篇论文提出了一种名为DWT-CompCNN的深度学习模型，它可以直接对使用HTJ2K算法压缩的文档进行分类，从而提高计算效率。 |
| [^69] | [Covert Communication Based on the Poisoning Attack in Federated Learning.](http://arxiv.org/abs/2306.01342) | 本论文提出了一种基于污染攻击的联邦学习隐蔽通信方法，能够在保证隐蔽性和稳健性的同时，在两个客户端之间实现100%的隐蔽消息传输准确性，这在现有防御方法存在限制的情况下强调了联邦学习中隐蔽通信的潜在威胁和对应的安全挑战。 |
| [^70] | [Resource-Efficient Federated Hyperdimensional Computing.](http://arxiv.org/abs/2306.01339) | 提出了一种资源高效的联合超维计算框架，通过训练多个小的HDC子模型并使用改进后的dropout流程，实现可与大模型相当的预测性能，消耗更少的计算和无线资源。 |
| [^71] | [Federated Domain Generalization: A Survey.](http://arxiv.org/abs/2306.01334) | 该论文调查了联邦领域泛化的研究领域，提到了联邦学习和领域泛化技术的优势，以及在泛化联邦模型时面临的挑战。 |
| [^72] | [Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations.](http://arxiv.org/abs/2306.01333) | 本文讨论了放射学AI中的公平性问题，介绍了Aequitas偏差审计工具包并阐明了公平性评估的基本指标，强调了考虑AI伦理影响的重要性。 |
| [^73] | [Hyperparameters in Reinforcement Learning and How To Tune Them.](http://arxiv.org/abs/2306.01324) | 本文研究了强化学习中的超参数选择问题，提出了采用AutoML中的最佳实践，并展示了采用HPO方法往往具有更高的性能和更低的计算开销。 |
| [^74] | [Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?.](http://arxiv.org/abs/2306.01323) | 本研究证明GNN在同构图中的同构节点和异构图中的异构节点上表现良好，而在另一组节点上表现不佳。该研究提出了一种新框架，通过使用加权聚合提高GNN在不同结构模式节点上的表现，有效解决结构差异性问题。 |
| [^75] | [Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models.](http://arxiv.org/abs/2306.01322) | 本文提出了隐私蒸馏框架，通过过滤具有重新识别风险的图像，训练生成模型，从而减少数据提供者共享多模态生成模型时的隐私泄露风险。 |
| [^76] | [Text Style Transfer Back-Translation.](http://arxiv.org/abs/2306.01318) | 本研究提出了文本风格转化的Back-Translation技术（TST BT），通过使用风格转化模型改善BT数据的源语言部分，旨在提高自然输入的翻译质量。在不同的语言对上进行实验，结果表明TST BT显著提高了翻译性能，还可以用于领域适应，是一种通用的数据增强技术。 |
| [^77] | [Independent Modular Networks.](http://arxiv.org/abs/2306.01316) | 这篇论文提出了一种独特的模块化网络架构来容纳数据的分解性质，可以有效应用于机器人技术，克服现有模块化网络的问题。 |
| [^78] | [EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost.](http://arxiv.org/abs/2306.01310) | EPIC提出了一种基于插值的方法来增强图数据集，通过利用图编辑距离生成与原始图相似但有结构变化的新图，从而提高了分类模型的泛化能力。 |
| [^79] | [Federated Learning Games for Reconfigurable Intelligent Surfaces via Causal Representations.](http://arxiv.org/abs/2306.01306) | 本文提出了一种在联邦学习框架下解决异构通信环境下重构智能表面（RIS）相移配置问题的方法。通过使用因果表示来学习不变的表示，最终预测RIS的相位配置。基于模拟实验的结果表明该方法是有效的。 |
| [^80] | [Egocentric Planning for Scalable Embodied Task Achievement.](http://arxiv.org/abs/2306.01295) | 本文提出了一种自我中心规划的方法，结合符号规划和面向对象的POMDP模型，成为高扩展性任务完成的一种新途径。 |
| [^81] | [Recent Advances in Graph-based Machine Learning for Applications in Smart Urban Transportation Systems.](http://arxiv.org/abs/2306.01282) | 智能交通系统（ITS）是现代交通基础设施的重要组成部分。已出现基于复杂数据的数据驱动解决方案以解决各种与ITS相关的挑战。图形机器学习方法成为了智能交通系统领域的日益重要的研究重点。 |
| [^82] | [Beyond Active Learning: Leveraging the Full Potential of Human Interaction via Auto-Labeling, Human Correction, and Human Verification.](http://arxiv.org/abs/2306.01277) | 本文提出了一个名为CLARIFIER的交互式学习框架，通过利用验证成本的降低，能够更好地利用人类交互，实现更有效的主动学习。 |
| [^83] | [Symmetric Exploration in Combinatorial Optimization is Free!.](http://arxiv.org/abs/2306.01276) | 该论文提出了一种免费的技术，通过利用对称性提高了基于DRL的组合优化求解器的性能，无需额外的目标函数评估，适用于广泛的组合优化任务，并在多种任务上进行实证评估证实了其有效性。 |
| [^84] | [DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection.](http://arxiv.org/abs/2306.01272) | 本文介绍了DeepfakeArt Challenge，这是一个专门为帮助构建机器学习算法以进行生成AI艺术伪造和数据污染检测而设计的大规模挑战基准数据集。 |
| [^85] | [Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training.](http://arxiv.org/abs/2306.01271) | 对抗训练是训练深度神经网络抗击对抗扰动的标准方法, 其学习机制导致干净泛化和强健过拟合现象同时发生。 |
| [^86] | [Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2306.01270) | 提出了一种多机器人路径规划方法MAPPOHR，该方法结合了启发式搜索、经验规则和多智能体强化学习。实验结果表明，该方法在规划效率和避碰能力方面优于现有方法。 |
| [^87] | [Self Contrastive Learning for Session-based Recommendation.](http://arxiv.org/abs/2306.01266) | 本文提出了自对比学习方法，简化了会话推荐领域基于对比学习的模型的复杂性，并提高了推荐性能。 |
| [^88] | [Calibrating Multimodal Learning.](http://arxiv.org/abs/2306.01265) | 本文提出了一个多模态学习的校准方法，可以解决当前方法在预测置信度不可靠的问题，提高模型的分类准确性和置信度校准能力。 |
| [^89] | [Convex and Non-Convex Optimization under Generalized Smoothness.](http://arxiv.org/abs/2306.01264) | 本文发展了一种新的分析技术，并推广了广义平滑度条件，使凸和非凸优化问题获得更强的结果。在该条件下，获得了（随机）梯度下降和Nesterov加速梯度方法的经典收敛率。 |
| [^90] | [Mixture Proportion Estimation Beyond Irreducibility.](http://arxiv.org/abs/2306.01253) | 本文提出了一种更一般的充分条件来解决MPE中不可简约性假设下的估计问题，并实现了更好的估计性能。 |
| [^91] | [Transforming ECG Diagnosis:An In-depth Review of Transformer-based DeepLearning Models in Cardiovascular Disease Detection.](http://arxiv.org/abs/2306.01249) | 本综述介绍了最新的基于transformer的深度学习模型在ECG分类中的应用进展和挑战，探索这些模型对ECG信号中的复杂时间关系进行建模的能力，并提出未来的改进方向。 |
| [^92] | [How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?.](http://arxiv.org/abs/2306.01248) | 这篇论文探讨了是否可以使用预训练的抽象模型和大型语言模型来自动生成法律案例判决的摘要，并在印度的法庭案例判决中进行了相关实验分析。 |
| [^93] | [Towards Sustainable Learning: Coresets for Data-efficient Deep Learning.](http://arxiv.org/abs/2306.01244) | CREST是一个可扩展的框架，用于数据高效深度学习，通过提取有价值的样本和多个小批量核心集，保证非凸模型收敛到稳定点，提高可扩展性和效率。 |
| [^94] | [Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations.](http://arxiv.org/abs/2306.01243) | 本文提出了在延迟和缺失状态观察环境中进行高效强化学习的理论研究，并建立了接近最优的遗憾边界，尽管有损可观察性对策略和规划的类别造成了重大挑战，但学习仍然是高效的。 |
| [^95] | [Federated Learning of Models Pre-Trained on Different Features with Consensus Graphs.](http://arxiv.org/abs/2306.01240) | 在分散的数据集情况下，为了实现本地模型的一致性，提出了一种特征融合的方法来提高预测性能。 |
| [^96] | [A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits.](http://arxiv.org/abs/2306.01237) | 本文提出一种直接最小化贝叶斯遗憾上界的新方法，获得更好的理论离线遗憾界和数值模拟结果，并提供了证据表明流行的LCB-style算法可能不适用。 |
| [^97] | [Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data.](http://arxiv.org/abs/2306.01222) | 本论文提出UnMixMatch框架，可以从非约束的未标记数据中学习有效的表征以提高性能，以解决大多数半监督方法基于有标记和未标记样本来自同一分布的假设。该框架具有强正则化作用的硬增强监督学习器、用于从未标记数据中学习基础表征的对比一致性正则化器以及通过自监督损失来增强从未标记数据学习的表征。 |
| [^98] | [Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation.](http://arxiv.org/abs/2306.01220) | 本文研究了LLMs在代码生成过程中是否与人类程序员的注意力有所不同，结果发现他们之间存在一致性偏差。作者通过量化实验和用户研究，确认了扰动方法计算的注意力最接近人类程序员的注意力，并且这种LLMs模型具有更好的可解释能力和程序员信任度。 |
| [^99] | [An Augmented Lagrangian Approach to Conically Constrained Non-monotone Variational Inequality Problems.](http://arxiv.org/abs/2306.01214) | 本文提出了一种被称为ALAVI的增广拉格朗日原始-对偶方法，用于解决一般约束VI模型，具有收敛性和全局收敛速度，对于存在某些广义单调性质的问题，收敛更快。 |
| [^100] | [Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms.](http://arxiv.org/abs/2306.01213) | 本文通过定义独立因果机制，提出了ICM-VAE框架，使得学习因果解缠绕表示更准确 |
| [^101] | [Linked Deep Gaussian Process Emulation for Model Networks.](http://arxiv.org/abs/2306.01212) | 本文提出了一种链接深高斯过程模拟（LDGP）框架，可用于高效仿真具有非稳态行为的复杂模型，并提高了预测的准确性。 |
| [^102] | [Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples.](http://arxiv.org/abs/2306.01206) | 本文研究如何以原则性的方式分析模型在域内和域外设置下的性能，最终找出一种无监督方法识别OOD / OODist样本。 |
| [^103] | [Physics-informed UNets for Discovering Hidden Elasticity in Heterogeneous Materials.](http://arxiv.org/abs/2306.01204) | 本文提出了一种新型的 UNet 神经网络模型 (El-UNet)，能够通过物理学知识增强和应变图像的分析，精确地推断生物软组织中材料参数的空间分布，具有高性能的优劣和计算效率。 |
| [^104] | [Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models.](http://arxiv.org/abs/2306.01201) | 本文提出了一个适用于实际场景的同时语音翻译系统，支持从57种语言翻译成英语，并具有调整输出延迟的可调参数。我们展示了可以在不增加显著延迟的情况下达到离线水平的准确性。 |
| [^105] | [An Effective Meaningful Way to Evaluate Survival Models.](http://arxiv.org/abs/2306.01196) | 该论文提出了一种有效的方法来评估生存模型的性能，使用伪观察值的MAE指标能够准确地排名模型的性能，发现这种方法比其他替代方法更好。 |
| [^106] | [Conformal Prediction with Partially Labeled Data.](http://arxiv.org/abs/2306.01191) | 本文将顺从预测程序与集合值的训练数据相结合，提出了一种适用于集合值训练和校准数据的顺从预测程序。 |
| [^107] | [A General Framework for Uncertainty Quantification via Neural SDE-RNN.](http://arxiv.org/abs/2306.01189) | 该论文提出了一种基于神经SDE-RNN的通用框架，用于解决时间序列填补中不确定性量化的问题，在任何时间尺度上填补测量并以原则性的方式量化填补中的不确定性，经实验证明可以胜过最先进的方法。 |
| [^108] | [Training neural operators to preserve invariant measures of chaotic attractors.](http://arxiv.org/abs/2306.01187) | 本文提出了一种基于神经算子的训练框架，通过最优传输距离或KSD损失来确保神经算子能够在混沌系统上复现其统计或结构特性。 |
| [^109] | [TMI! Finetuned Models Leak Private Information from their Pretraining Data.](http://arxiv.org/abs/2306.01181) | 本文提出了一种新的会员推断威胁模型TMI，用于评估微调模型对预训练数据的泄露，突显了在使用预训练模型进行迁移学习中存在的隐私风险，并需要对机器学习中的隐私进行更严格的评估。 |
| [^110] | [Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations.](http://arxiv.org/abs/2306.01174) | 本文提出了niLES方法，将理想大涡模拟（LES）和神经随机微分方程（SDE）进行了整合，用于模拟难以处理的湍流现象。 |
| [^111] | [Integrated Sensing-Communication-Computation for Edge Artificial Intelligence.](http://arxiv.org/abs/2306.01162) | 面向边缘人工智能的集成感知-通信-计算技术对于提高资源利用率以及实现边缘学习和边缘人工智能推理任务的定制目标至关重要。 |
| [^112] | [Faster Causal Attention Over Large Sequences Through Sparse Flash Attention.](http://arxiv.org/abs/2306.01160) | 本文介绍了一种新的稀疏Flash注意力机制，能够快速处理大序列中的因果关系，且速度提高了多倍，可以作为任何基于Transformer的语言模型中密集自我注意力的替代方案，并在多个设置中产生了最先进的结果。 |
| [^113] | [Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge.](http://arxiv.org/abs/2306.01158) | 该论文提出了增强模块化强化学习（AMRL），使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。该方法能够减缓强化学习中的一些低效问题，有望在深度强化学习领域得到应用。 |
| [^114] | [Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding.](http://arxiv.org/abs/2306.01157) | 本文提出了一种名为Delphic不确定性的方法来解决离线强化学习中的隐藏混淆问题，并通过实验表明该方法在非可识别的隐藏混淆情况下优于现有算法。 |
| [^115] | [The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks.](http://arxiv.org/abs/2306.01154) | 研究发现梯度下降对于简约解有偏好，针对深度线性网络数据具有低维结构时，从正交初始化开始，演变只会影响权重矩阵的少数奇异空间向量，说明学习过程仅发生在每个权重矩阵的一个小不变空间内，深度网络的学习动态存在简化。 |
| [^116] | [Addressing Discrepancies in Semantic and Visual Alignment in Neural Networks.](http://arxiv.org/abs/2306.01148) | 本文提出了一种数据增强技术，通过语义混合实现了更好地将语义类别与非视觉语义相关性对齐，提高了神经网络中语义类别的对齐度。 |
| [^117] | [Smooth Monotonic Networks.](http://arxiv.org/abs/2306.01147) | 本文提出了一种新的神经网络模块--平滑min-max(SMM)网络，相比于传统的min-max(MM)神经网络结构简单易用，在单调建模方面表现优异。 |
| [^118] | [Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data.](http://arxiv.org/abs/2306.01144) | 本研究提出一种利用合成方法生成用于评估复杂多模态推理任务的数据集，并将其应用于测试最新的视觉问答模型的表现，结果表明该模型在上下文相关的异常检测任务上表现不佳。 |
| [^119] | [Federated Graph Learning for Low Probability of Detection in Wireless Ad-Hoc Networks.](http://arxiv.org/abs/2306.01143) | 本文提出了一种联邦图学习框架，用于最小化无线自组网的探测概率，保护通信节点的隐私和安全。该方法可预测每个节点的最佳通信区域，表现良好。 |
| [^120] | [A Neural RDE-based model for solving path-dependent PDEs.](http://arxiv.org/abs/2306.01123) | 本文提出了一种基于神经粗糙微分方程(NRDE)的模型来学习和求解路径依赖偏微分方程(PPDE)，通过记录标记特征来编码路径信息，且能够高效地利用内存和随着维度的扩展能力，这一方法在数值实验中被证明比文献中的强基线模型更有效。 |
| [^121] | [On the Convergence of Coordinate Ascent Variational Inference.](http://arxiv.org/abs/2306.01122) | 本文通过分析常见的坐标上升变分推断（CAVI）算法在两个块的情况下的收敛性，提供了证明全局或局部指数收敛的一般条件。 |
| [^122] | [Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards.](http://arxiv.org/abs/2306.01121) | 本研究针对重尾奖励的有限步骤表格马尔可夫决策过程问题探讨了差分隐私限制下的两种框架，即价值迭代和策略优化，同时考虑了联合差分隐私和本地差分隐私模型，并为两种情况提供了遗憾上限。 |
| [^123] | [What if We Enrich day-ahead Solar Irradiance Time Series Forecasting with Spatio-Temporal Context?.](http://arxiv.org/abs/2306.01112) | 本文提出了一种深度学习框架，使用卫星数据捕捉时空上下文信息，实现了对太阳辐射时序的高精度日前预测，表现优于不采用卫星数据的时序和机器学习模型，可帮助更有效地将太阳能融入电网。 |
| [^124] | [Comparative Study on the Effects of Noise in ML-Based Anxiety Detection.](http://arxiv.org/abs/2306.01110) | 本研究探究了噪声如何影响基于机器学习的焦虑检测模型，并开发出在嘈杂的现实环境中具有抗干扰性和适应性的模型，以推进该领域的发展。 |
| [^125] | [Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization.](http://arxiv.org/abs/2306.01103) | 本文提出了一种考虑标签和环境因果独立性的方法来解决图形超出分布（OOD）泛化问题，通过敌对训练策略来联合优化属性以获得有效结果，实验证明LECI显着优于之前的方法。 |
| [^126] | [ALO-VC: Any-to-any Low-latency One-shot Voice Conversion.](http://arxiv.org/abs/2306.01100) | 本文提出了ALO-VC，一种任意对任意低延迟一次性语音转换方法，它使用仅为目标说话人提供的一个话语，包括预训练说话人编码器、音高预测器和位置编码器，实现了与非因果基线系统相当的性能。 |
| [^127] | [Large-Batch, Neural Multi-Objective Bayesian Optimization.](http://arxiv.org/abs/2306.01095) | 本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。 |
| [^128] | [Semi-supervised Community Detection via Structural Similarity Metrics.](http://arxiv.org/abs/2306.01089) | 本文提出了一种基于结构相似度指标的快速半监督社区检测算法，并在理论与实验方面均有良好表现。 |
| [^129] | [Hierarchical Attention Encoder Decoder.](http://arxiv.org/abs/2306.01070) | 本论文提出了一种基于分层循环编码器解码器（HRED）架构的模型，可以独立地对输入子序列进行编码，在较低频率模型中处理这些序列，并在原始数据频率下解码输出，同时减少计算时间和内存使用。 |
| [^130] | [Investigating Navigation Strategies in the Morris Water Maze through Deep Reinforcement Learning.](http://arxiv.org/abs/2306.01066) | 本文利用深度强化学习代理程序模拟了莫里斯水迷宫，通过自动分类导航策略，分析人工智能代理程序使用的策略分布，并与实验结果做对比。该研究发现了类似于人和啮齿类动物的学习规律，发展了内部表征与导航策略之间的关联，为生物学特征的探索提供了一定的参考。 |
| [^131] | [Pseudo Labels for Single Positive Multi-Label Learning.](http://arxiv.org/abs/2306.01034) | 本文提出一种伪标签方法，将单正数据转化为完全标记的数据进行训练，以解决单正多标签学习中缺失标签的问题。 |
| [^132] | [Chaos persists in large-scale multi-agent learning despite adaptive learning rates.](http://arxiv.org/abs/2306.01032) | 多智能体学习中的大规模混杂博弈环境下，即使使用自适应学习率，混沌现象仍然存在。 |
| [^133] | [Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts.](http://arxiv.org/abs/2306.01031) | 本文提出了一种新颖的算法，用重视转录不确定性的方式解决语音识别中训练数据不完美的问题，提高了ASR系统的鲁棒性和准确性。 |
| [^134] | [SPINEX: Similarity-based Predictions and Explainable Neighbors Exploration for Regression and Classification Tasks in Machine Learning.](http://arxiv.org/abs/2306.01029) | SPINEX是一种基于相似度的可解释邻居探索算法，通过集成学习和特征交互分析，提高了机器学习算法的可解释性，并且在回归和分类任务中具有优异表现。 |
| [^135] | [An FPGA Architecture for Online Learning using the Tsetlin Machine.](http://arxiv.org/abs/2306.01027) | 本文提出了一种基于FPGA架构的在线学习系统，通过采用低复杂度的Tsetlin Machine机器学习算法，在运行时提供了离线和在线学习管理。这种架构能够按需进行训练，并且能够在操作期间交错地进行推理和训练。 |
| [^136] | [PV2TEA: Patching Visual Modality to Textual-Established Information Extraction.](http://arxiv.org/abs/2306.01016) | PV2TEA提出了一种基于编码器-解码器架构的信息抽取模型，在多模态注释困难的情况下解决了跨模态集成的问题，并提出了三种偏差降低方案。 |
| [^137] | [Graph-Level Embedding for Time-Evolving Graphs.](http://arxiv.org/abs/2306.01012) | 本论文提出了一种针对动态网络的图级嵌入方法，并通过生成节点的时间上下文来训练语言模型，以生成低维度的图级表示，对于下游的图形相似性排序、图形同构和异常检测等任务具有重要的意义。 |
| [^138] | [Physics-informed machine learning of redox flow battery based on a two-dimensional unit cell model.](http://arxiv.org/abs/2306.01010) | 本文提出了一种利用二维数学模型进行物理启发式神经网络预测铁钒液流电池性能的方法，可正确预测单元电压。 |
| [^139] | [Examining the Emergence of Deductive Reasoning in Generative Language Models.](http://arxiv.org/abs/2306.01009) | 本研究调查了不同规模的生成语言模型的演绎推理能力，发现随着规模的增加，推理能力增强，长度不会影响大部分模型表现。 |
| [^140] | [Towards Fair Disentangled Online Learning for Changing Environments.](http://arxiv.org/abs/2306.01007) | 本论文提出了一种面向变化环境的在线学习算法，该算法通过将模型参数划分为环境不变部分和环境特定部分，从而实现了数据公平性。通过大量的实验，证明了该算法的有效性。 |
| [^141] | [AbODE: Ab Initio Antibody Design using Conjoined ODEs.](http://arxiv.org/abs/2306.01005) | 该论文提出了一种新的从头设计抗体的方法AbODE，该方法使用连续微分注意力来应对蛋白折叠、逆向折叠和对接等挑战，并与时间网络和图匹配网络具有基本联系。 |
| [^142] | [Adaptive ship-radiated noise recognition with learnable fine-grained wavelet transform.](http://arxiv.org/abs/2306.01002) | 本文提出了一种自适应通用识别系统AGNet，通过转换可学习的微观参数，学习了不同频率下水下声音的特性，以解决在可变水下环境中识别船舶辐射噪音的问题。 |
| [^143] | [DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model.](http://arxiv.org/abs/2306.01001) | 本文提出了一种扩散模型中的负荷预测不确定性量化方法，采用Seq2Seq网络结构来分离两种类型的不确定性并处理异常情况，不仅着眼于预测条件期望值。 |
| [^144] | [Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling.](http://arxiv.org/abs/2306.00996) | 本文提出了一种使用加权有限状态转换器对CTC模型的对齐图构建进行改进的弱监督方法，可以对失语性言语进行强制对齐，减轻了对逐字转录的需求，能够有效地在实际应用中使用，并在实验中取得了显著的改进。 |
| [^145] | [Diffusion Self-Guidance for Controllable Image Generation.](http://arxiv.org/abs/2306.00986) | 本论文提出了一种扩散自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制，可以用于执行具有挑战性的图像操作，同时不需要额外模型或训练。 |
| [^146] | [BitE : Accelerating Learned Query Optimization in a Mixed-Workload Environment.](http://arxiv.org/abs/2306.00845) | 本文提出了BitE，一种新颖的集成学习模型，利用数据库统计和元数据来调整学习查询优化器以提高性能。 |
| [^147] | [LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing.](http://arxiv.org/abs/2306.00758) | 本文提出了一种基于轻量级Transformer的遥感图像问答系统，即LiT-4-RSVQA，它可以有效准确地进行遥感图像问答，并且大大降低了计算资源的要求。 |
| [^148] | [On the Effectiveness of Hybrid Mutual Information Estimation.](http://arxiv.org/abs/2306.00608) | 本文研究了混合互信息估计的有效性，提出了一种混合方法以应对判别式和生成式方法各自缺点，同时提出了一种名为预测量化的生成方法，与判别式估计器结合可获得更精确的互信息估计结果。 |
| [^149] | [Mechanic: A Learning Rate Tuner.](http://arxiv.org/abs/2306.00144) | 机械师是一种学习率调节器，能自动调整任何基本优化算法和调度的学习率比例因子，可以实现在大规模深度学习任务中接近、匹配或甚至优于手动调整学习率的效果。 |
| [^150] | [A Note On Interpreting Canary Exposure.](http://arxiv.org/abs/2306.00133) | 本文提供了关于如何解释金丝雀曝光的直觉，包括其与成员推理攻击和差分隐私的关系。 |
| [^151] | [Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection.](http://arxiv.org/abs/2306.00006) | 本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。 |
| [^152] | [Smooth-Trajectron++: Augmenting the Trajectron++ behaviour prediction model with smooth attention.](http://arxiv.org/abs/2305.19678) | 本文中，研究者将平滑关注机制引入最先进的Trajectron++轨迹预测模型，提高其性能并比较其性能表现。该研究表明将人类的认知因素纳入轨迹预测模型中具有潜在的优势。 |
| [^153] | [Policy Optimization for Continuous Reinforcement Learning.](http://arxiv.org/abs/2305.18901) | 本研究提出了连续强化学习领域的占用时间概念，并在此基础上扩展了离散强化学习中的PG、TRPO和PPO方法，为连续强化学习领域的研究提供了新的思路和方法。 |
| [^154] | [Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning.](http://arxiv.org/abs/2305.18888) | 该论文提出了一种新的无监督多元时间序列表示学习框架，通过对比形态片段学习以及多粒度对比和多尺度对齐的学习目标，学习时间序列特定表示。 |
| [^155] | [What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?.](http://arxiv.org/abs/2305.18882) | 本文研究了离线目标条件强化学习的目标泛化，提出了一种新的离线GCRL方法GOAT，结合理论和实验结果，加权模仿学习比基于悲观离线RL方法的泛化性能更好。 |
| [^156] | [Dimensionality Reduction for General KDE Mode Finding.](http://arxiv.org/abs/2305.18755) | 本文提出了一种降维算法，可以适用于高斯混合模型和更广泛的核函数，结合梯度下降可制定出有效的实用启发式算法。 |
| [^157] | [Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization.](http://arxiv.org/abs/2305.18730) | 本文提出了两种基于方差约简的优化算法，以实现对多块双层优化问题的高效求解，同时匹配单块标准 BO 问题的最优复杂度、实现并行化加速，以及避免计算高维度的 Hessian 矩阵的逆估计。 |
| [^158] | [KEYword based Sampling (KEYS) for Large Language Models.](http://arxiv.org/abs/2305.18679) | 本文旨在探讨如何通过结合人类生成答案的思路来生成接近人类行为和事实的答案，并探讨关键词对Q/A任务解码算法的影响。 |
| [^159] | [Faith and Fate: Limits of Transformers on Compositionality.](http://arxiv.org/abs/2305.18654) | 研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。 |
| [^160] | [UMD: Unsupervised Model Detection for X2X Backdoor Attacks.](http://arxiv.org/abs/2305.18651) | UMD是一种无监督的模型检测方法，能够有效地检测X2X后门攻击，对抗（源，目标）类别对进行联合推断，该方法定义了新的可转移性统计量，并使用聚类方法来测量和选择潜在的后门类别对的子集。 |
| [^161] | [Alg{\i}lanan Stres Testinin Makine \"O\u{g}renmesi ile Analiz Edilmesi.](http://arxiv.org/abs/2305.18473) | 本研究利用机器学习重新分析了感知压力测试，揭示了测试问题的重要性不相等，展示了在心理上观察到的不同模式。 |
| [^162] | [Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent.](http://arxiv.org/abs/2305.14076) | 本文探究了高斯-斯坦变分梯度下降动态性。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。 |
| [^163] | [Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design.](http://arxiv.org/abs/2305.13035) | 本研究通过改进缩放定律方法推测出计算-优化模型形状，成功实现了形状优化视觉变换器SoViT，该模型在相同计算量下，取得了与超过其两倍大小的模型相竞争的结果。 |
| [^164] | [Privacy in Multimodal Federated Human Activity Recognition.](http://arxiv.org/abs/2305.12134) | 本文研究了多模式联邦人类活动识别中的隐私问题。通过一个特定的系统，联邦学习可以提供更好的隐私保护，同时不会损失人类活动识别的准确性。 |
| [^165] | [Q-malizing flow and infinitesimal density ratio estimation.](http://arxiv.org/abs/2305.11857) | 研究提出了一种可以从一个数据分布P传输到任意访问通过有限样本的Q的流模型。这个模型通过神经ODE模型进行，可以进行无穷小DRE。 |
| [^166] | [SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization.](http://arxiv.org/abs/2305.11615) | SFP提出了一种针对模型子结构的修剪框架，SFP可以自动探索不变的子结构，而不考虑对完全暴露于域外数据的依赖性以及对整个数据分布进行同样特征未命中修剪带来的缺点。这种方法的核心在于，利用ID数据中的伪特征来降低风险。 |
| [^167] | [Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature.](http://arxiv.org/abs/2305.11033) | 本文综述了最近文献中的可视化问答问题，提供了对该领域的深入分析和比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。 |
| [^168] | [Can Deep Learning Reliably Recognize Abnormality Patterns on Chest X-rays? A Multi-Reader Study Examining One Month of AI Implementation in Everyday Radiology Clinical Practice.](http://arxiv.org/abs/2305.10116) | 本研究开发了一种基于深度学习的自动检测算法，可以在胸部X光片上检测出七种特定放射学发现，并且该算法的性能优于评估图像的六名放射科医师。 |
| [^169] | ["I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation.](http://arxiv.org/abs/2305.09941) | 本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。 |
| [^170] | [Towards Understanding Generalization of Macro-AUC in Multi-label Learning.](http://arxiv.org/abs/2305.05248) | 本研究探究了 multi-label 学习中常用的 Macro-AUC 的泛化性质，并发现数据集中标签不平衡对泛化界限有重要影响。未经变量处理的基于损失函数的算法可能由于对标签的不平衡更敏感而表现较差，这一结论在多个数据集上得到验证。 |
| [^171] | [Generating Adversarial Examples with Task Oriented Multi-Objective Optimization.](http://arxiv.org/abs/2304.13229) | 本文提出了“基于任务导向的MOO”方法来实现对抗样本生成以同时实现多个目标，避免了朴素MOO最大化所有目标的弊端。 |
| [^172] | [Matching-based Data Valuation for Generative Model.](http://arxiv.org/abs/2304.10701) | 本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。 |
| [^173] | [Priors for symbolic regression.](http://arxiv.org/abs/2304.06333) | 本文提出了一种在符号回归（SR）框架内将有关函数和参数的详细先验信息纳入的方法，并且演示了该先验在基准数据集和材料科学应用中的性能。 |
| [^174] | [On the Possibilities of AI-Generated Text Detection.](http://arxiv.org/abs/2304.04736) | 该研究探讨了AI生成文本检测的可能性，提出了精确的样本复杂度界限，并指出了设计更准确的检测方法和提高透明度的挑战。 |
| [^175] | [Expert-Independent Generalization of Well and Seismic Data Using Machine Learning Methods for Complex Reservoirs Predicting During Early-Stage Geological Exploration.](http://arxiv.org/abs/2304.03048) | 该研究利用自主方法预测了复杂油藏的空间分布概率，可以进行专家无关的概化预测和地质模型创建。 |
| [^176] | [Do intermediate feature coalitions aid explainability of black-box models?.](http://arxiv.org/abs/2303.11920) | 本文引入了中间概念的级别结构，利用领域专家建立部分-整体关系，从而在不同抽象级别上帮助解释黑盒模型。 |
| [^177] | [A Multifidelity deep operator network approach to closure for multiscale systems.](http://arxiv.org/abs/2303.08893) | 本文提出了一个基于多保真度深度算子网络框架和“内循环”训练方法解决多尺度系统的封闭问题的方法，并且在实验中得到了显著的改进。 |
| [^178] | [MNL-Bandit in non-stationary environments.](http://arxiv.org/abs/2303.02504) | 本文研究了非静态环境下的MNL-Bandit问题，提出了一种算法，其最坏情况下的预期遗憾度为$\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\; N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} + \sqrt{NT}\right\}\right)$。算法基于时代算法，对由于非静态性引入的估计器偏差进行了紧致特征给出新的浓度界。 |
| [^179] | [Locally Regularized Neural Differential Equations: Some Black Boxes Were Meant to Remain Closed!.](http://arxiv.org/abs/2303.02262) | 该论文提出了一种局部正则化的神经微分方程模型，通过启发式成本控制训练过程以学习易于积分的动力系统，同时提高预测速度，并保留了模型的自适应特性。 |
| [^180] | [Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness.](http://arxiv.org/abs/2302.10893) | 这篇论文提出了一种名为“公平扩散”的新策略，可以在生成文本到图像模型部署后减轻偏见并使模型接受公平性指导。 |
| [^181] | [ChatGPT: Jack of all trades, master of none.](http://arxiv.org/abs/2302.10724) | 本研究检验了 ChatGPT 在 25 个不同的 NLP 任务上的性能，它是一个万能的 AI 模型，但无关紧要的表现可能会对某些任务的表现产生负面影响。 |
| [^182] | [Differentiable Multi-Target Causal Bayesian Experimental Design.](http://arxiv.org/abs/2302.10607) | 本文提出了一种可微分的方法来解决因果模型的贝叶斯最佳实验设计问题，可以用于批处理设置下的学习，扩展了之前只能处理单目标状态对干涉的方法，结果比现有方法更好。 |
| [^183] | [ByzSecAgg: A Byzantine-Resistant Secure Aggregation Scheme for Federated Learning Based on Coded Computing and Vector Commitment.](http://arxiv.org/abs/2302.09913) | 本文提出了一种基于编码计算和向量承诺的拜占庭抵抗安全聚合方案，用于联邦学习。该方案通过RAM秘密共享将本地更新分割成较小子向量，并使用双重RAMP共享技术实现成对距离的安全计算。 |
| [^184] | [Online Continuous Hyperparameter Optimization for Contextual Bandits.](http://arxiv.org/abs/2302.09440) | 该论文提出了面向上下文强化学习的在线连续超参数调整框架CDT，能够动态地在搜索空间内学习最优参数配置。 |
| [^185] | [Is Distance Matrix Enough for Geometric Deep Learning?.](http://arxiv.org/abs/2302.05743) | 本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。 |
| [^186] | [Joint Representations for Reinforcement Learning with Multiple Sensors.](http://arxiv.org/abs/2302.05342) | 本文提出了一种基于重构和对比损失的方法来对多传感器的输入进行联合表示学习，证明了这种方法在完成复杂任务时具有良好的效果。 |
| [^187] | [Tighter Information-Theoretic Generalization Bounds from Supersamples.](http://arxiv.org/abs/2302.02432) | 本文介绍了一种新颖的信息论泛化界限，利用投影损失对，与Rademacher序列相关联来源于超取样的设置，这些界限比同一超取样设置中迄今已知的所有信息理论界限都更紧密。 |
| [^188] | [Oscillation-free Quantization for Low-bit Vision Transformers.](http://arxiv.org/abs/2302.02210) | 研究发现，可学习比例因子加剧了权重振荡，本文提出三种技术以解决这个问题，并在多个基准测试上显著提高了模型的性能。 |
| [^189] | [Automatically Marginalized MCMC in Probabilistic Programming.](http://arxiv.org/abs/2302.00564) | 本文提出了在概率编程中使用自动边缘化作为采样过程的一部分，使用 HMC 在从 PPL 中提取的图形模型中进行采样，显著提高了从现实世界的层次模型中采样的效率。 |
| [^190] | [Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits.](http://arxiv.org/abs/2301.13393) | 本文提出了可能任何时候安全的随机组合半臂赌博问题，并设计出算法PASCombUCB在时间轴上最小化后悔值。 |
| [^191] | [Refined Regret for Adversarial MDPs with Linear Function Approximation.](http://arxiv.org/abs/2301.12942) | 本论文研究了在对抗性马尔可夫决策过程（MDP）中的学习问题，提出了两种算法，可以将现有最佳方法中的后悔从$\tilde{\mathcal O}(K^{2/3})$降低到$\tilde{\mathcal O}(\sqrt K)$。其中第一种算法使用对数壁垒正则化器的跟随正则化者（FTRL）算法实现，在损失估计器任意负的情况下有效。第二种算法利用幅度降低的损失估计器，进一步消除了与动作数量多项式相关的依赖关系。 |
| [^192] | [Scaling in Depth: Unlocking Robustness Certification on ImageNet.](http://arxiv.org/abs/2301.12549) | 本文提出了一些新策略和方法解决了证明深度网络稳健性的难点，引入了Linear ResNet架构和Efficient Margin MAximization损失函数，最终实现了新的最先进稳健准确性。 |
| [^193] | [Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming.](http://arxiv.org/abs/2301.12187) | 本文提出了基于两阶段动态规划的深度压缩算法，该算法能够将神经网络的深度合并成等效的浅层卷积操作，从而实现高效的推理延迟和内存占用，同时不会影响模型精度。 |
| [^194] | [Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with Riesz Kernels.](http://arxiv.org/abs/2301.11624) | 本文提出用神经网络逼近Jordan、Kinderlehrer和Otto的反向方案以及一种前向方案，用于计算非光滑Riesz核最大均值差异函数的Wasserstein梯度流。我们通过学习适当的损失函数来近似处理计划的分解，并在交互能上基准测量神经网络的质量。 |
| [^195] | [The Lost Art of Mathematical Modelling.](http://arxiv.org/abs/2301.08559) | 本文批评数学生物学过度专注于分析模型，忽视了制定模型，提出了通过采用开放/多元化的方法逆转这一趋势，对任何给定的生物现象进行无限次建模，以重新发掘失落的创造性数学建模艺术。 |
| [^196] | [End-to-End Modeling Hierarchical Time Series Using Autoregressive Transformer and Conditional Normalizing Flow based Reconciliation.](http://arxiv.org/abs/2212.13706) | 本文提出了一种基于自回归变压器和条件正态化流协调的端到端分层时间序列预测模型，实现了同时预测和协调的功能。 |
| [^197] | [When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods.](http://arxiv.org/abs/2212.10025) | 本文在隐私敏感的自然语言处理任务中探讨了联邦学习如何与参数高效调整方法结合以解决数据异质性问题，在维持可接受性能的同时显著减少通信开销。 |
| [^198] | [PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection.](http://arxiv.org/abs/2212.04708) | PATO是一个策略辅助的远程操作系统，通过自动执行部分演示采集过程，并仅在不确定要执行哪个子任务或行为时请求人类输入，提高了数据采集效率，同时减少了人工操作员的心理负担，实现单个操作员并行控制多个机器人的功能。 |
| [^199] | [PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement.](http://arxiv.org/abs/2212.02779) | 该论文提出了一种基于人类偏好的推荐系统范式PrefRec，允许强化学习推荐系统从用户历史行为偏好中学习，以优化长期用户参与度。 |
| [^200] | [Learning Physically Realizable Skills for Online Packing of General 3D Shapes.](http://arxiv.org/abs/2212.02094) | 该论文研究了学习在线包装不规则三维形状的物理实现技能的问题，提出了一种强化学习管道并采用候选行动生成方法来减小学习负担。 |
| [^201] | [Margin-based sampling in high dimensions: When being active is less efficient than staying passive.](http://arxiv.org/abs/2212.00772) | 基于边缘值的主动学习在高维情况下效率不如被动学习，即使对于无噪声数据和使用贝叶斯最优决策边界进行采样，被动学习仍然更优。特别是在类之间的分离较小的情况下，这种现象更加明显。 |
| [^202] | [SWL-Adapt: An Unsupervised Domain Adaptation Model with Sample Weight Learning for Cross-User Wearable Human Activity Recognition.](http://arxiv.org/abs/2212.00724) | SWL-Adapt是一种新的跨用户可穿戴人体活动识别的无监督领域自适应方法，通过样本权重学习来区分不同样本，提高了该任务的性能表现。 |
| [^203] | [One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion.](http://arxiv.org/abs/2212.00124) | 本研究提出了一种基于风险规避机制的离线强化学习方法，同时解决了避免分布偏移和避免灾难性结果的风险问题，并取得了明显的改进。 |
| [^204] | [Self-Supervised Continual Graph Learning in Adaptive Riemannian Spaces.](http://arxiv.org/abs/2211.17068) | 本文提出了一种自监督连续适应黎曼空间图学习的方法，解决了现有方法忽视曲率变化和对标签依赖过高的问题。 |
| [^205] | [DiffPhase: Generative Diffusion-based STFT Phase Retrieval.](http://arxiv.org/abs/2211.04332) | 本研究提出了一种基于扩散的STFT相位恢复生成方法，适用于填补缺失数据的问题，性能超越了传统的和现代的方法。 |
| [^206] | [Convergence of the Inexact Langevin Algorithm and Score-based Generative Models in KL Divergence.](http://arxiv.org/abs/2211.01512) | 本文研究了不精确 Langevin 算法与基于得分的生成模型在 KL 散度中的收敛性，提出了建立稳定偏差收敛保证的两个关键假设：目标分布满足对数 Sobolev 不等式和分数估计器展示出有界的矩阵生成函数误差。作者探讨了如何获得可靠的分数估计器，并证明了基于核密度估计的简单估计器满足假设。 |
| [^207] | [Local Model Reconstruction Attacks in Federated Learning and their Uses.](http://arxiv.org/abs/2210.16205) | 本文研究了联邦学习中的本地模型重构攻击，攻击者可以通过重构客户端的模型更有效地触发其他攻击，并提出了一种新颖的基于模型的属性推导攻击，实证结果表明攻击有效，可应用于回归和分类任务。 |
| [^208] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^209] | [Oracles & Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2210.11942) | 本文提出了一个通用框架，将Stackelberg均衡搜索作为多智能体RL问题进行实现，提出了一种利用上下文策略的方法，并在标准和新颖的基准领域上进行实验评估，证明相比之前的方法，样本效率得到了极大提高。 |
| [^210] | [Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo.](http://arxiv.org/abs/2210.11925) | 本文提出了一种名为BHMC的新的蒙特卡罗采样算法，能够从定义了约束的黎曼流形中进行无偏采样，其中包含一种新的过滤步骤involution checking step。 |
| [^211] | [Characterizing and Detecting State-Sponsored Troll Activity on Social Media.](http://arxiv.org/abs/2210.08786) | 该研究提出了一种基于AI的新解决方案，通过分析喷子的轨迹，识别国家赞助的喷子帐户。该方法可以准确识别俄罗斯喷子与有机用户，可提供国家赞助影响活动的早期警报，并对保护民主进程做出贡献。 |
| [^212] | [Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization.](http://arxiv.org/abs/2210.04492) | 本文提出了名为UDDIA的统一去毒化和去偏见框架，它能够有效地消除有毒语言和减少社会偏见，同时保持流畅性。 |
| [^213] | [Supervised Metric Learning to Rank for Retrieval via Contextual Similarity Optimization.](http://arxiv.org/abs/2210.01908) | 这项研究提出了一种新的度量学习方法——上下文损失，它通过优化上下文相似性隐式地确保邻居之间的语义一致性，并表现出更好的鲁棒性和抗噪声能力，取得了四个图像检索基准的最新最佳结果。 |
| [^214] | [Bayes-optimal limits in structured PCA, and how to reach them.](http://arxiv.org/abs/2210.01237) | 本文针对带尖峰的PCA问题，利用正交多项式矩阵抽取噪声模型提出了该模型中推断的贝叶斯最优极限的表征，并提出了一种新的AMP算法以实现信息论极限。 |
| [^215] | [Developing A Visual-Interactive Interface for Electronic Health Record Labeling: An Explainable Machine Learning Approach.](http://arxiv.org/abs/2209.12778) | 本文介绍了一种解释性标注助手工具XLabel，通过可解释性提升机（EBM）和可视化展现，帮助医疗专家标记非传染性疾病（NCDs）的电子病历，与其他知名机器学习模型相比，EBM的准确性更佳。 |
| [^216] | [Learning While Scheduling in Multi-Server Systems with Unknown Statistics: MaxWeight with Discounted UCB.](http://arxiv.org/abs/2209.01126) | 本文提出了一种新算法，将MaxWeight调度策略与折扣上置信度（UCB）相结合，以同时学习统计信息和将作业调度到服务器上，在多服务器系统中实现最大化利用服务器的处理能力。 |
| [^217] | [Differentiable Programming for Earth System Modeling.](http://arxiv.org/abs/2208.13825) | 可微分编程有助于改进地球系统模型，并解决气候变化模拟中存在的关键缺陷。 |
| [^218] | [Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation.](http://arxiv.org/abs/2208.12401) | 本文提出了一种可扩展的集合编码方法UMBC，可以与任意非MBC组件相结合，同时仍满足MBC；同时提出了一种高效的MBC训练算法，可以为任何集合大小在训练和测试时都具有恒定的内存开销，给出完整集合梯度的无偏近似。 |
| [^219] | [Hindsight Learning for MDPs with Exogenous Inputs.](http://arxiv.org/abs/2207.06272) | 提出了一种数据高效的带有外部输入的MDPs算法，名为追溯学习（HL）。HL算法通过利用外部变量样本使得过去的决策在回溯中可以加速策略改进，在资源管理问题中表现出良好的性能。 |
| [^220] | [Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks.](http://arxiv.org/abs/2207.01580) | 基于视觉变压器中的空间稀疏性，本文提出了一种动态标记稀疏化框架，可加速各种结构的模型，被剪枝的冗余标记由轻量级预测模块逐步动态估计。 |
| [^221] | [Supply-Side Equilibria in Recommender Systems.](http://arxiv.org/abs/2206.13489) | 本论文探究了推荐系统中个性化内容的供给侧均衡问题，其特点是生产者决策空间是多维的和用户群体是异构的，高维度和异质性的模型创造了专业化的可能性。 |
| [^222] | [Differentiable and Transportable Structure Learning.](http://arxiv.org/abs/2206.06354) | D-Struct是一种可微和可传输的结构学习方法，通过新颖的架构和损失函数使得结构可以在同一领域的不同数据集中传输，比NOTEARS和其他最先进的方法具有更好的性能。 |
| [^223] | [A Survey of Graph-based Deep Learning for Anomaly Detection in Distributed Systems.](http://arxiv.org/abs/2206.04149) | 本文综述并比较了基于图的深度学习算法在分布式系统异常检测中的应用现状，提出了其处理异构和动态结构等现实世界挑战的能力。 |
| [^224] | [What-is and How-to for Fairness in Machine Learning: A Survey, Reflection, and Perspective.](http://arxiv.org/abs/2206.04101) | 本文回顾和反思了机器学习文献中以前提出的各种公平概念，并试图将其与道德和政治哲学中的论点联系起来。在考虑到不同类型公平性所涉及的显性假设和预期结果的差异后，我们提出了一个流程图，它包括对数据生成过程、预测结果和引发影响的不同类型公平性调查的隐性假设和预期结果。 |
| [^225] | [Decentralized Online Regularized Learning Over Random Time-Varying Graphs.](http://arxiv.org/abs/2206.03861) | 本文研究了随机时变图上的分散在线正则化线性回归算法，提出了非负超-鞅不等式的估计误差，证明了算法在满足样本路径时空兴奋条件时，节点的估计可以收敛于未知的真实参数向量。 |
| [^226] | [Subject Membership Inference Attacks in Federated Learning.](http://arxiv.org/abs/2206.03317) | 本文研究跨边界联邦学习中主体级别的隐私，并提出两种新颖的黑盒推理攻击方法。 |
| [^227] | [Fast Nonlinear Vector Quantile Regression.](http://arxiv.org/abs/2205.14977) | 本论文提出了一种基于神经网络的快速非线性向量分位数回归方法，该方法保留了向量分位数回归的优雅的基于几何的公式，同时采用了几种创新的算法思想以及有效的训练和推断步骤，具有优越的预测性能。 |
| [^228] | [Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding.](http://arxiv.org/abs/2205.14814) | 本文揭示了自监督对比学习（SSCL）与随机邻居嵌入（SNE）的联系，SSCL实际上是SNE的一种特殊情况，通过SNE的视角，本文提供了关于领域无关增广、隐式偏差和学到特征的鲁棒性等问题的新分析和实用指南，该方法可以优化SSCL的特征提取。 |
| [^229] | [Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods.](http://arxiv.org/abs/2205.13602) | 引入泊松近似似然(PAL)方法，与ODE方法不同，PAL是从有限人口数量的随机分区模型的近似滤波方程导出的，并且大人口数量限制推动最大PAL估计量的一致性。 |
| [^230] | [Finite-Time Analysis of Temporal Difference Learning: Discrete-Time Linear System Perspective.](http://arxiv.org/abs/2204.10479) | 本文提出一种基于离散时间线性系统模型和Schur矩阵特性的有限时间分析方法，针对表格型时序差分学习，能够统一地覆盖基于策略和基于价值的两种情况。 |
| [^231] | [Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method.](http://arxiv.org/abs/2203.04951) | 本文提出的Object Preference Adaptation (OPA)方法通过学习人类针对特定物体的反馈，在单次干预后进行机器人行为的自适应，提高了数据效率和适用性。 |
| [^232] | [Contextualize Me -- The Case for Context in Reinforcement Learning.](http://arxiv.org/abs/2202.04500) | 情境强化学习提供了灵活、精确和可解释的任务规范和生成，可帮助改进RL中的零-shot泛化，需要上下文信息的洞察力。 |
| [^233] | [De Rham compatible Deep Neural Network FEM.](http://arxiv.org/abs/2201.05395) | 本篇论文提出了在普通正则单形分割上构建精确神经网络有限元的方法，能够适用于分段常数函数空间、连续分段线性函数空间、经典的Raviart-Thomas元和Nédélec边缘元，且能够捕捉不连续性，其中对于CPwL函数的情况，只需使用纯ReLU网络即可。 |
| [^234] | [Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation.](http://arxiv.org/abs/2112.12218) | 本文提出了在最大熵和错误预测上使用的训练策略（MEEP），通过惩罚自信度过高的预测来提高医学图像分割模型的校准性和分割准确性。 |
| [^235] | [Classifying YouTube Comments Based on Sentiment and Type of Sentence.](http://arxiv.org/abs/2111.01908) | 本论文提出了一种基于情感和句子类型的方法，将原始的YouTube评论分类，以帮助YouTuber找到更相关的评论，从而增加其观众群。 |
| [^236] | [Learning Practically Feasible Policies for Online 3D Bin Packing.](http://arxiv.org/abs/2108.13680) | 本文针对在线 3D 无序装箱问题采用深度强化学习，提出了一种基于新的叠放树的装箱稳定性在线分析方法，并提出了一种不同维度分离的装箱策略学习方法 |
| [^237] | [An XAI Approach to Deep Learning Models in the Detection of DCIS.](http://arxiv.org/abs/2106.14186) | 该研究证明了XAI可用于证明辅助人工智能系统的可行性，有效地应用于医疗领域。 |
| [^238] | [Reward is enough for convex MDPs.](http://arxiv.org/abs/2106.00661) | 本文研究了凸性马尔可夫决策过程，发现无法使用静态奖励函数表达目标，提出了一个元算法解决此问题，并统一了文献中的现有算法。 |
| [^239] | [Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model.](http://arxiv.org/abs/2105.03733) | 本文提出基于推进模型的无密度离线算法 GAC，使用最大均值差熵正则化器平衡探索和开发之间的关系，并使用自适应机制提高算法的稳定性和鲁棒性。实验结果表明，推进策略能够提高算法的探索效率和渐近性能。 |
| [^240] | [Lessons on Parameter Sharing across Layers in Transformers.](http://arxiv.org/abs/2104.06022) | 该论文提出了一种放宽常用参数共享技术的Transformer参数共享方法，其可以提高计算时间的效率，通过三种策略来分配每个层的参数，在实验中表现出高效的参数大小和计算时间，并在使用大量训练数据的配置中同样有效。 |
| [^241] | [QCBA: Improving Rule Classifiers Learned from Quantitative Data by Recovering Information Lost by Discretisation.](http://arxiv.org/abs/1711.10166) | 本文提出的QCBA方法可以用于改进基于数值类型数据学习的规则分类器，恢复预离散化过程中丢失的信息，并提出了新的剪枝技术。在22个数据集上的实验表明，FOIL2+QCBA相对其他基线方法而言，具有更高的预测性能和更小的模型大小。 |

# 详细

[^1]: 在随机递归有向无环图中的广播

    Broadcasting in random recursive dags. (arXiv:2306.01727v1 [stat.ML])

    [http://arxiv.org/abs/2306.01727](http://arxiv.org/abs/2306.01727)

    该论文研究了一个均匀的$k$-dag广播模型，确定了与$p$和$k$有关的阈值，并讨论了大多数规则的误差率。

    

    一个均匀的$k$-dag通过从现有节点中均匀随机选择$k$个父节点来推广均匀的随机递归树。它以$k$个“根”开始。每个$k$个根节点都被分配一个位。这些位通过一个嘈杂的信道传播。每个父节点的位都以概率$p$发生变化，并进行大多数表决。当所有节点都接收到它们的位后，$k$-dag被显示，不识别根节点。目标是估计所有根节点中的大多数位。我们确定了$p$的阈值，作为一个关于$k$的函数，使得所有节点的大多数规则产生错误$c+o(1)$的概率小于$1/2$。在阈值以上，大多数规则的错误概率为$1/2+o(1)$。

    A uniform $k$-{\sc dag} generalizes the uniform random recursive tree by picking $k$ parents uniformly at random from the existing nodes. It starts with $k$ ''roots''. Each of the $k$ roots is assigned a bit. These bits are propagated by a noisy channel. The parents' bits are flipped with probability $p$, and a majority vote is taken. When all nodes have received their bits, the $k$-{\sc dag} is shown without identifying the roots. The goal is to estimate the majority bit among the roots. We identify the threshold for $p$ as a function of $k$ below which the majority rule among all nodes yields an error $c+o(1)$ with $c<1/2$. Above the threshold the majority rule errs with probability $1/2+o(1)$.
    
[^2]: 评估有噪声判别器对未标记数据的流式算法 -- 二元分类

    Streaming algorithms for evaluating noisy judges on unlabeled data -- binary classification. (arXiv:2306.01726v1 [stat.ML])

    [http://arxiv.org/abs/2306.01726](http://arxiv.org/abs/2306.01726)

    本文提出了两种代数评估器来估计未标记数据中有噪声二元分类器的性能。其中，第二种评估器的正确性被保证。作者通过利用独立评估器无法返回合理估计的失败，缓解了委托/代理监控悖论，并通过搜索来寻找几乎无误差的三元组。

    

    本文将对未标记数据中的有噪声二元分类器的评估作为流式任务进行研究: 给定一个分类器决策的数据草图，估计标签的真实流行度以及每个分类器对它们的准确度。本文构建了两种完全代数化的评估器来实现这一目标。两种评估器都基于分类器产生独立错误的假设。第一种是基于多数投票的。而第二种则是本文的主要贡献，并被保证是正确的。但是如何确保分类器在任何给定的测试中是独立的呢？本文通过利用独立评估器无法返回合理估计的失败来缓解这个委托/代理监控悖论。通过利用代数故障模式来拒绝太相关的评估集合，使用 \texttt{adult}，\texttt{mushroom} 和 \texttt{two-norm} 数据集对一组几乎无误差三元组进行了实证搜索。这些搜索通过构建评估空间中的表面来进行精细化。

    The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming task: given a data sketch of the decisions by an ensemble, estimate the true prevalence of the labels as well as each classifier's accuracy on them. Two fully algebraic evaluators are constructed to do this. Both are based on the assumption that the classifiers make independent errors. The first is based on majority voting. The second, the main contribution of the paper, is guaranteed to be correct. But how do we know the classifiers are independent on any given test? This principal/agent monitoring paradox is ameliorated by exploiting the failures of the independent evaluator to return sensible estimates. A search for nearly error independent trios is empirically carried out on the \texttt{adult}, \texttt{mushroom}, and \texttt{two-norm} datasets by using the algebraic failure modes to reject evaluation ensembles as too correlated. The searches are refined by constructing a surface in evaluation spa
    
[^3]: 基于图稀疏化的GCN算法用于最优农作物收成预测

    Graph Sparsification for GCN Towards Optimal Crop Yield Predictions. (arXiv:2306.01725v1 [cs.LG])

    [http://arxiv.org/abs/2306.01725](http://arxiv.org/abs/2306.01725)

    本文提出了一种基于Fiedler数的图稀疏化方法，可以从完整的图核中删除边缘，以降低GCN训练和执行的复杂度，并显著提高农业产量预测的可行性。

    

    在农学领域，预测每个田地/县的作物产量对于农民来说非常重要，以减少不确定性并为下一轮作物种植计划作出安排。本文提出了一种基于Fiedler数的图稀疏化方法，旨在从完整的图核中删除边缘，以降低GCN训练和执行的复杂度。实验表明，我们的基于Fiedler的方法能产生具有良好GCN性能的稀疏图。

    In agronomics, predicting crop yield at a per field/county granularity is important for farmers to minimize uncertainty and plan seeding for the next crop cycle. While state-of-the-art prediction techniques employ graph convolutional nets (GCN) to predict future crop yields given relevant features and crop yields of previous years, a dense underlying graph kernel requires long training and execution time. In this paper, we propose a graph sparsification method based on the Fiedler number to remove edges from a complete graph kernel, in order to lower the complexity of GCN training/execution. Specifically, we first show that greedily removing an edge at a time that induces the minimal change in the second eigenvalue leads to a sparse graph with good GCN performance. We then propose a fast method to choose an edge for removal per iteration based on an eigenvalue perturbation theorem. Experiments show that our Fiedler-based method produces a sparse graph with good GCN performance compared
    
[^4]: 基于数据驱动的相对不确定性测度用于误分类检测

    A Data-Driven Measure of Relative Uncertainty for Misclassification Detection. (arXiv:2306.01710v1 [stat.ML])

    [http://arxiv.org/abs/2306.01710](http://arxiv.org/abs/2306.01710)

    本文提出了一种基于数据驱动的相对不确定性度量，用于误分类检测。该度量可以通过学习软预测的分布模式，识别出被误分类的样本，并展示了在多个图像分类任务中的实证改进，优于现有的误分类检测方法。

    

    误分类检测是机器学习中的一个重要问题，它可以识别模型预测不可靠的实例。然而，传统的不确定性测度如香农熵并不能提供一种有效的方式来推断模型预测的实际不确定性。本文提出了一种新颖的数据驱动相对不确定性度量，用于误分类检测。通过学习软预测的分布模式，我们的不确定性度量可以基于预测的类概率标识被误分类的样本。有趣的是，根据所提出的度量，与误分类实例对应的软预测可能具有很大的不确定性，即使它们的香农熵可能很低。我们展示了多个图像分类任务中的实证改进，优于现有的误分类检测方法。

    Misclassification detection is an important problem in machine learning, as it allows for the identification of instances where the model's predictions are unreliable. However, conventional uncertainty measures such as Shannon entropy do not provide an effective way to infer the real uncertainty associated with the model's predictions. In this paper, we introduce a novel data-driven measure of relative uncertainty to an observer for misclassification detection. By learning patterns in the distribution of soft-predictions, our uncertainty measure can identify misclassified samples based on the predicted class probabilities. Interestingly, according to the proposed measure, soft-predictions that correspond to misclassified instances can carry a large amount of uncertainty, even though they may have low Shannon entropy. We demonstrate empirical improvements over multiple image classification tasks, outperforming state-of-the-art misclassification detection methods.
    
[^5]: 合并模型时如何解决干扰的问题

    Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])

    [http://arxiv.org/abs/2306.01708](http://arxiv.org/abs/2306.01708)

    本文揭示了现有模型合并技术存在的干扰问题，提出了具有广泛适用性的解决方案，可显着提高合并后模型的性能。

    

    迁移学习可以在下游任务中进一步微调预训练模型，从而获得显著的优势，包括改进下游性能，加快收敛速度和提高样本效率。然而，已有的模型合并技术往往忽视了不同模型参数之间的干扰，导致合并多个模型时性能大幅下降。本文证明，先前的合并技术由于两个主要干扰来源而不慎丢失有价值的信息：(a)冗余参数值引起的干扰和(b)表示同一参数值的符号在不同模型中的差异。

    Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
    
[^6]: 基于生成建模风格化技术的领域自适应回归任务中是否必要？

    Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?. (arXiv:2306.01706v1 [cs.CV])

    [http://arxiv.org/abs/2306.01706](http://arxiv.org/abs/2306.01706)

    本研究探讨了生成建模风格化技术在领域自适应回归任务中是否必要，发现与分类任务相比，其对于回归任务的影响较小。

    

    无监督的领域自适应旨在在没有目标域标签的情况下使用两种主要技术：输入级别对齐（如生成建模和风格化）和特征级别对齐（匹配特征映射的分布，例如梯度反转层）来弥合源域和目标域之间的差距。本文探讨了输入级别对齐对于领域自适应回归任务的作用。

    Unsupervised domain adaptation (UDA) aims to bridge the gap between source and target domains in the absence of target domain labels using two main techniques: input-level alignment (such as generative modeling and stylization) and feature-level alignment (which matches the distribution of the feature maps, e.g. gradient reversal layers). Motivated from the success of generative modeling for image classification, stylization-based methods were recently proposed for regression tasks, such as pose estimation. However, use of input-level alignment via generative modeling and stylization incur additional overhead and computational complexity which limit their use in real-world DA tasks. To investigate the role of input-level alignment for DA, we ask the following question: Is generative modeling-based stylization necessary for visual domain adaptation in regression? Surprisingly, we find that input-alignment has little effect on regression tasks as compared to classification. Based on thes
    
[^7]: 信息通路假说：Transformer是动态自组织

    The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles. (arXiv:2306.01705v1 [cs.LG])

    [http://arxiv.org/abs/2306.01705](http://arxiv.org/abs/2306.01705)

    Transformer使用稠密的自注意力机制，使得在深度网络中可能的连接模式数量呈指数级增长。我们提出了信息通路假说，利用这种特性将连接模式分解为相互独立的信息通路，减少训练过程中的内存和计算成本以及提高模型的泛化能力。

    

    Transformer模型使用了稠密的自注意力机制，赋予了它在远距离连接方面更高的灵活性。在深度Transformer的多个层中，可能的连接模式数量呈指数级增长。然而，其中很少有连接模式对模型性能有所贡献，其中有用的更少。我们提出假设，在一个Transformer中存在着名为信息通路的稀疏子网络，可以独立地进行训练。然而，这些通路的动态（即依赖于输入）特性使得在训练过程中很难剪枝稠密的自注意力。但是，这些通路的整体分布往往是可以预测的。我们利用这一事实提出了随机子采样自注意力（SSA）——一种用于Transformer的通用训练策略，可以在训练期间将自注意力的内存和计算成本减少4到8倍，同时还可以作为正则化方法，提高模型的泛化能力。

    Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generaliz
    
[^8]: 基于成对分布差异的数据去偏见亲和性聚类框架

    Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy. (arXiv:2306.01699v1 [cs.LG])

    [http://arxiv.org/abs/2306.01699](http://arxiv.org/abs/2306.01699)

    该论文提出了一种基于亲和性聚类的数据扩充方法MASC，通过同类数据集中的亲和聚类和保护数据的共享达到数据集的平衡，从而解决数据集代表性偏见问题。

    

    数据采集方法不足或不具代表性常导致身份组不平衡，形成数据集代表性偏见。这种偏见可能存在于一个或多个受保护属性的不同组之间，并可能导致对某些人群的偏见和歧视性结果。该论文提出了一种数据扩充方法MASC，利用亲和性聚类平衡目标数据集的非保护组和保护组表征。通过将同一保护属性的实例从相似数据集中进行聚类，共享来自受保护属性的实例。该方法包括通过量化数据集间的分布差异构建亲和矩阵，并将其转换为对称成对相似性矩阵。使用非参数的谱聚类算法对目标数据集进行分类。

    Group imbalance, resulting from inadequate or unrepresentative data collection methods, is a primary cause of representation bias in datasets. Representation bias can exist with respect to different groups of one or more protected attributes and might lead to prejudicial and discriminatory outcomes toward certain groups of individuals; in cases where a learning model is trained on such biased data. This paper presents MASC, a data augmentation approach that leverages affinity clustering to balance the representation of non-protected and protected groups of a target dataset by utilizing instances of the same protected attributes from similar datasets that are categorized in the same cluster as the target dataset by sharing instances of the protected attribute. The proposed method involves constructing an affinity matrix by quantifying distribution discrepancies between dataset pairs and transforming them into a symmetric pairwise similarity matrix. A non-parametric spectral clustering i
    
[^9]: MutateNN：用于硬件加速器上图像识别模型的突变测试

    MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])

    [http://arxiv.org/abs/2306.01697](http://arxiv.org/abs/2306.01697)

    MutateNN是一种用于探索硬件加速器上深度学习图像识别模型鲁棒性的工具，提供突变测试和分析能力，且有效性已在多种预训练深度神经网络模型中得到验证。

    

    随着人工智能的研究进展，解决现实世界问题并推动技术发展的新机遇应运而生。图像识别模型特别是被分配了感知任务，以解决复杂的现实世界挑战并导致新的解决方案。此外，这类模型的计算复杂度和资源需求也有所增加。为了解决这个问题，模型优化和硬件加速已成为关键技术，但有效整合这些概念是一个具有挑战性和容易出错的过程。为了让开发人员和研究人员能够探索在不同硬件加速设备上部署的深度学习图像识别模型的鲁棒性，我们提出了MutateNN，这是一个为此目的提供突变测试和分析能力的工具。为了展示其功能，我们对7个广为人知的预训练深度神经网络模型进行了21种变异。我们在4种不同类型的硬件加速器上部署了我们的变异体，分析了它们的行为，并评估了MutateNN在检测出不正确或不精确的模型行为方面的有效性。

    With the research advancement of Artificial Intelligence in the last years, there are new opportunities to mitigate real-world problems and advance technologically. Image recognition models in particular, are assigned with perception tasks to mitigate complex real-world challenges and lead to new solutions. Furthermore, the computational complexity and demand for resources of such models has also increased. To mitigate this, model optimization and hardware acceleration has come into play, but effectively integrating such concepts is a challenging and error-prone process.  In order to allow developers and researchers to explore the robustness of deep learning image recognition models deployed on different hardware acceleration devices, we propose MutateNN, a tool that provides mutation testing and analysis capabilities for that purpose. To showcase its capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep neural network models. We deployed our mutants on 4 different
    
[^10]: 通过交互评估数学语言模型

    Evaluating Language Models for Mathematics through Interactions. (arXiv:2306.01694v1 [cs.LG])

    [http://arxiv.org/abs/2306.01694](http://arxiv.org/abs/2306.01694)

    本文介绍了一个可进行人机交互评估LLMs的原型平台CheckMate，并利用该平台评估了三个语言模型在大学级数学证明助手方面的能力，发布了结果数据集MathConverse，并得出了人类行为的初步分类和LMM生成正确性与感知帮助性分歧的发现。

    

    传统的基于静态输入和输出对大型语言模型（LLMs）进行评估的方法不足以开发助手：这种评估未能考虑部署中的基本交互性质，因此限制了我们对语言模型能力的理解。我们引入了CheckMate，这是一个适应性的人机交互原型平台，用于评估LLMs。我们利用CheckMate对三个语言模型（InstructGPT、ChatGPT和GPT-4）进行了一项研究，评估它们作为大学级数学证明助手的能力，参与者混合了从本科生到数学教授的各层次。我们发布了结果交互和评分数据集MathConverse。通过对MathConverse的分析，我们得出了人类行为的初步分类，并发现尽管普遍存在积极相关性，但在LLM生成的正确性和感知帮助性之间存在显著的分歧。

    The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generati
    
[^11]: 具有Lipschitz连续激活函数和可变宽度的深度神经网络的统一收敛性分析

    Uniform Convergence of Deep Neural Networks with Lipschitz Continuous Activation Functions and Variable Widths. (arXiv:2306.01692v1 [cs.LG])

    [http://arxiv.org/abs/2306.01692](http://arxiv.org/abs/2306.01692)

    本文提出了针对具有Lipschitz连续激活函数和可变宽度的深度神经网络的统一收敛性分析，给出了充分条件以及Lipschitz常数，建立了相应的统一收敛性结果。

    

    本文考虑了具有Lipschitz连续激活函数和可变宽度的深度神经网络。我们建立了一个统一的收敛分析框架，其中提供了关于权重矩阵和偏置向量的充分条件以及Lipschitz常数，以确保深度神经网络在层数趋近于无穷大时对一个有意义的函数的统一收敛。在框架中，我们特别提出了针对具有固定宽度、有界宽度和无界宽度的深度神经网络的统一收敛性结果。特别是，由于卷积神经网络是具有不断增加的权重矩阵的特殊深度神经网络，我们提出了关于掩码序列的条件，从而导致所得卷积神经网络的统一收敛。激活函数的Lipschitz连续性假设允许我们在我们的理论中包括大多数应用中常用的激活函数。

    We consider deep neural networks with a Lipschitz continuous activation function and with weight matrices of variable widths. We establish a uniform convergence analysis framework in which sufficient conditions on weight matrices and bias vectors together with the Lipschitz constant are provided to ensure uniform convergence of the deep neural networks to a meaningful function as the number of their layers tends to infinity. In the framework, special results on uniform convergence of deep neural networks with a fixed width, bounded widths and unbounded widths are presented. In particular, as convolutional neural networks are special deep neural networks with weight matrices of increasing widths, we put forward conditions on the mask sequence which lead to uniform convergence of resulting convolutional neural networks. The Lipschitz continuity assumption on the activation functions allows us to include in our theory most of commonly used activation functions in applications.
    
[^12]: GateON: 一种用于大规模连续学习的无监督方法

    GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])

    [http://arxiv.org/abs/2306.01690](http://arxiv.org/abs/2306.01690)

    GateON是一种用于大规模连续学习的无监督方法，通过可学习的活动门控和参数相关性的在线估计来防止重要知识被覆盖，同时通过定点神经元的重新激活机制解决了网络饱和的问题。

    

    连续学习（CL）的目标是在不对早期任务进行重新训练的情况下按顺序学习任务。然而，传统的神经网络在经过CL训练后会出现灾难性遗忘和有限的泛化能力。为了克服这些问题，我们引入了一种新的方法，称为'Gate and Obstruct Network'（GateON）。GateON将可学习的活动门控与参数相关性的在线估计相结合，以防止重要知识被覆盖。我们的方法在任务之间生成部分重叠的路径，允许在顺序学习过程中进行正向和反向转移。GateON通过定点神经元的重新激活机制来解决参数固定后网络饱和的问题，实现了大规模连续学习。GateON适用于各种网络（全连接、CNN、Transformers），计算复杂度低，有效地学习了高达100个MNIST学习任务，并在预训练BERT中取得了顶尖结果。

    The objective of continual learning (CL) is to learn tasks sequentially without retraining on earlier tasks. However, when subjected to CL, traditional neural networks exhibit catastrophic forgetting and limited generalization. To overcome these problems, we introduce a novel method called 'Gate and Obstruct Network' (GateON). GateON combines learnable gating of activity and online estimation of parameter relevance to safeguard crucial knowledge from being overwritten. Our method generates partially overlapping pathways between tasks which permits forward and backward transfer during sequential learning. GateON addresses the issue of network saturation after parameter fixation by a re-activation mechanism of fixed neurons, enabling large-scale continual learning. GateON is implemented on a wide range of networks (fully-connected, CNN, Transformers), has low computational complexity, effectively learns up to 100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
    
[^13]: 利用大型语言模型生成私有的合成文本

    Harnessing large-language models to generate private synthetic text. (arXiv:2306.01684v1 [cs.LG])

    [http://arxiv.org/abs/2306.01684](http://arxiv.org/abs/2306.01684)

    本文研究了一种通过利用公开数据集和预训练的生成语言模型，结合私有的微调方法，实现生成具有差分隐私性质的合成文本数据集的方法。生成的文本数据集可以重复使用于其他任务，可无限期保留，或与第三方共享而不损失隐私。

    

    差分隐私训练方法，如DP-SGD，可以通过确保机器学习模型不会透露私有信息来保护敏感的训练数据。本文研究了一种替代方法，利用敏感数据集生成新的合成数据集，并确保相对于原始数据具有差分隐私。这样做有几个优点：合成数据可以重复使用于其他任务（包括超参数调整），可以无限期保留，或与第三方共享而不损失隐私。但是，获取差分隐私数据比在训练期间引入差分隐私更加困难。为了使其在文本中可行，最近的研究利用公共数据，从预训练的生成语言模型开始，并在敏感数据上进行私人调整。该模型可以用于抽样差分隐私合成数据集。虽然这个策略似乎很简单，但执行它已被证明是有问题的。以前的方法要么表现出显著的性能损失，要么...

    Differentially private (DP) training methods like DP-SGD can protect sensitive training data by ensuring that ML models will not reveal private information. An alternative approach, which this paper studies, is to use a sensitive dataset to generate a new synthetic dataset which is differentially private with respect to the original data. Doing so has several advantages: synthetic data can be reused for other tasks (including for hyper parameter tuning), retained indefinitely, or shared with third parties without sacrificing privacy.  However, obtaining DP data is much harder than introducing DP during training. To make it feasible for text, recent work has utilized public data by starting with a pre-trained generative language model and privately finetuning it on sensitive data. This model can be used to sample a DP synthetic dataset. While this strategy seems straightforward, executing it has proven problematic. Previous approaches either show significant performance loss, or have, a
    
[^14]: 具有自适应时间步长的神经微分循环神经网络

    Neural Differential Recurrent Neural Network with Adaptive Time Steps. (arXiv:2306.01674v1 [stat.ML])

    [http://arxiv.org/abs/2306.01674](http://arxiv.org/abs/2306.01674)

    本研究提出了一种基于神经ODE的RNN模型，通过调整时间步长，可以高效地处理尖峰型非平稳时间序列数据。模型可以有效估计Hawkes类型时间序列数据的强度函数，能够显著提高预测精度并减少计算消耗。

    

    神经常微分方程（ODE）模型已经在从离散时间戳的观测中学习复杂的连续时间过程方面取得了成功。本文考虑建模和预测一些可能具有尖峰等锐变的非平稳时间序列数据。我们提出了一种基于RNN的模型，称为RNN-ODE-Adap，它使用神经ODE来表示隐藏状态的时间发展，并根据数据随时间变化的陡峭程度自适应地选择时间步长，以更高效地训练"尖峰"时间序列的模型。从理论上讲，RNN-ODE-Adap可有效地估计Hawkes类型时间序列数据的强度函数。我们还提供了关于RNN-ODE模型的近似分析，显示了自适应步长的优势。在模拟动态系统数据和点过程数据上的测试中，所提出的模型证明了具有更高的预测精度和更低的计算成本。

    The neural Ordinary Differential Equation (ODE) model has shown success in learning complex continuous-time processes from observations on discrete time stamps. In this work, we consider the modeling and forecasting of time series data that are non-stationary and may have sharp changes like spikes. We propose an RNN-based model, called RNN-ODE-Adap, that uses a neural ODE to represent the time development of the hidden states, and we adaptively select time steps based on the steepness of changes of the data over time so as to train the model more efficiently for the "spike-like" time series. Theoretically, RNN-ODE-Adap yields provably a consistent estimation of the intensity function for the Hawkes-type time series data. We also provide an approximation analysis of the RNN-ODE model showing the benefit of adaptive steps. The proposed model is demonstrated to achieve higher prediction accuracy with reduced computational cost on simulated dynamic system data and point process data and on
    
[^15]: 用CLIP增强CLIP: 探索有限标注提示调参的伪标注方法

    Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning. (arXiv:2306.01669v1 [cs.CV])

    [http://arxiv.org/abs/2306.01669](http://arxiv.org/abs/2306.01669)

    本文探索使用伪标注方法通过提示调参改进CLIP的方法，通过使用零样本伪标签来优化图像分类任务中有限标注数据的问题。

    

    微调视觉-语言模型（VLMs），如CLIP，以优化其性能往往是必要的。然而，主要障碍是有限数量的标注数据。本文研究使用伪标注（即非标注数据的启发式标签）通过提示调参改进CLIP的方法。传统伪标注方法会在有标注数据上训练模型，然后为无标注数据生成标签。VLM的零样本能力使“第二代”伪标注方法不需要在有标注数据上进行任务特定的训练。通过使用零样本伪标签作为监督来源，我们发现可以将半监督、过渡零样本和无监督学习等学习范式视为优化相同损失函数。这种统一的视角能够实现适用于各种学习范式的多功能培训策略的发展。我们通过改变提示的方式来探索这些培训策略，以解决 CLIP 在图像分类任务中存在的局限性。

    Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is often necessary to optimize their performance. However, a major obstacle is the limited availability of labeled data. We study the use of pseudolabels, i.e., heuristic labels for unlabeled data, to enhance CLIP via prompt tuning. Conventional pseudolabeling trains a model on labeled data and then generates labels for unlabeled data. VLMs' zero-shot capabilities enable a ``second generation'' of pseudolabeling approaches that do not require task-specific training on labeled data. By using zero-shot pseudolabels as a source of supervision, we observe that learning paradigms such as semi-supervised, transductive zero-shot, and unsupervised learning can all be seen as optimizing the same loss function. This unified view enables the development of versatile training strategies that are applicable across learning paradigms. We investigate them on image classification tasks where CLIP exhibits limitations, by varying p
    
[^16]: XAI文艺复兴：重新定义医学诊断模型的可解释性。

    XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models. (arXiv:2306.01668v1 [cs.LG])

    [http://arxiv.org/abs/2306.01668](http://arxiv.org/abs/2306.01668)

    本文探讨了可解释人工智能（XAI）在革新医学诊断模型的创新方法和方法论，并通过揭示潜在的决策过程来赋予医疗保健专业人员理解和信任这些模型的能力。这些新技术潜力巨大，有望改变医疗保健的格局，从而提高患者的结果并建立对AI驱动诊断系统的信任。

    

    随着机器学习模型越来越多地应用于医学诊断，可解释性和透明度的需求变得尤为重要。XAI文艺复兴标志着该领域发生了重大转变，旨在重新定义医学诊断模型的可解释性。本文探讨了在可解释人工智能（XAI）领域内正在革新医学诊断模型的创新方法和方法论。通过揭示潜在的决策过程，XAI技术赋予医疗保健专业人员理解、信任并有效利用这些模型进行准确可靠的医学诊断。本文重点介绍了XAI在医学诊断方面的关键进展及其改变医疗保健格局的潜力，从而改善患者结果并建立对AI驱动诊断系统的信任。

    As machine learning models become increasingly prevalent in medical diagnostics, the need for interpretability and transparency becomes paramount. The XAI Renaissance signifies a significant shift in the field, aiming to redefine the interpretability of medical diagnostic models. This paper explores the innovative approaches and methodologies within the realm of Explainable AI (XAI) that are revolutionizing the interpretability of medical diagnostic models. By shedding light on the underlying decision-making process, XAI techniques empower healthcare professionals to understand, trust, and effectively utilize these models for accurate and reliable medical diagnoses. This review highlights the key advancements in XAI for medical diagnostics and their potential to transform the healthcare landscape, ultimately improving patient outcomes and fostering trust in AI-driven diagnostic systems.
    
[^17]: 一种用于漂移数据的自适应弱监督方法

    An Adaptive Method for Weak Supervision with Drifting Data. (arXiv:2306.01658v1 [cs.LG])

    [http://arxiv.org/abs/2306.01658](http://arxiv.org/abs/2306.01658)

    本文在非稳态设置中提出了一种自适应弱监督方法，该方法可以推断序列数据的未知标签，并适应时间漂移，而无需假设漂移幅度。

    

    我们提出了一种在非稳态设置中具有正式质量保证的自适应方法，用于对数据序列进行弱监督标记。我们的目标是通过使用提供每个数据点正确分类的独立嘈杂信号的弱监督源来推断未知标签。这种情况包括众包和编程式弱监督。我们重点研究非稳态情况，在这种情况下，弱监督源的精度可能会随时间漂移，例如由于底层数据分布的变化。由于漂移，旧数据可能会提供误导性信息来推断当前数据点的标签。以往的工作依赖于先验对漂移幅度的假设，以决定使用多少过去的数据。相比之下，我们的算法不需要任何漂移假设，而是根据输入进行自适应。特别地，在每个步骤中，我们的算法保证弱监督源当前准确度的估计。

    We introduce an adaptive method with formal quality guarantees for weak supervision in a non-stationary setting. Our goal is to infer the unknown labels of a sequence of data by using weak supervision sources that provide independent noisy signals of the correct classification for each data point. This setting includes crowdsourcing and programmatic weak supervision. We focus on the non-stationary case, where the accuracy of the weak supervision sources can drift over time, e.g., because of changes in the underlying data distribution. Due to the drift, older data could provide misleading information to infer the label of the current data point. Previous work relied on a priori assumptions on the magnitude of the drift to decide how much data to use from the past. Comparatively, our algorithm does not require any assumptions on the drift, and it adapts based on the input. In particular, at each step, our algorithm guarantees an estimation of the current accuracies of the weak supervisio
    
[^18]: 网络流分类器的危害攻击研究

    Poisoning Network Flow Classifiers. (arXiv:2306.01655v1 [cs.CR])

    [http://arxiv.org/abs/2306.01655](http://arxiv.org/abs/2306.01655)

    研究了网络流分类器的后门攻击，提出了利用模型可解释性技术生成有效触发器模式和生成隐秘触发器的新策略，以应对对手干扰训练数据的挑战。

    

    随着机器学习分类器越来越多地监控网络流量，研究它们对抗攻击的韧性就变得至关重要。本文重点研究危害攻击，特别是针对网络流量分类器的后门攻击。我们研究了一个具有挑战性的干净标签污染场景，其中对手的能力受限于仅干扰训练数据，无法任意修改训练标签或训练过程的任何其他组件。我们描述了一种触发器制作策略，利用模型可解释性技术生成即使在极低的污染率下也有效的触发器模式。最后，我们设计了生成隐秘触发器的新策略，包括基于生成贝叶斯网络模型的方法，旨在最小化触发器的显著性，从而使正在进行的危害攻击更加具有挑战性。

    As machine learning (ML) classifiers increasingly oversee the automated monitoring of network traffic, studying their resilience against adversarial attacks becomes critical. This paper focuses on poisoning attacks, specifically backdoor attacks, against network traffic flow classifiers. We investigate the challenging scenario of clean-label poisoning where the adversary's capabilities are constrained to tampering only with the training data - without the ability to arbitrarily modify the training labels or any other component of the training process. We describe a trigger crafting strategy that leverages model interpretability techniques to generate trigger patterns that are effective even at very low poisoning rates. Finally, we design novel strategies to generate stealthy triggers, including an approach based on generative Bayesian network models, with the goal of minimizing the conspicuousness of the trigger, and thus making detection of an ongoing poisoning campaign more challengi
    
[^19]: GANs解决分数争议问题！

    GANs Settle Scores!. (arXiv:2306.01654v1 [cs.LG])

    [http://arxiv.org/abs/2306.01654](http://arxiv.org/abs/2306.01654)

    这篇论文提出了一种新的方法，通过变分方法来统一分析生成器的优化，并展示了在f-散度最小化和IPM GAN中生成器的最优解决方案。这种方法能够平滑分数匹配。

    

    生成对抗网络（GAN）由一个生成器和一个判别器组成，生成器被训练以学习期望数据的基础分布，而判别器则被训练以区分真实样本和生成器输出的样本。本文提出了一种统一的方法，通过变分方法来分析生成器优化。在f-散度最小化 GAN 中，我们表明最优生成器是通过将其输出分布的得分与数据分布的得分进行匹配得到的。在IPM GAN中，我们表明这个最优生成器匹配得分型函数，包括与所选IPM约束空间相关的核流场。此外，IPM-GAN优化可以看作是平滑分数匹配中的一种，其中数据和生成器分布的得分与在核函数上进行卷积处理。

    Generative adversarial networks (GANs) comprise a generator, trained to learn the underlying distribution of the desired data, and a discriminator, trained to distinguish real samples from those output by the generator. A majority of GAN literature focuses on understanding the optimality of the discriminator through integral probability metric (IPM) or divergence based analysis. In this paper, we propose a unified approach to analyzing the generator optimization through variational approach. In $f$-divergence-minimizing GANs, we show that the optimal generator is the one that matches the score of its output distribution with that of the data distribution, while in IPM GANs, we show that this optimal generator matches score-like functions, involving the flow-field of the kernel associated with a chosen IPM constraint space. Further, the IPM-GAN optimization can be seen as one of smoothed score-matching, where the scores of the data and the generator distributions are convolved with the 
    
[^20]: 维基百科公正的多语言破坏检测系统

    Fair multilingual vandalism detection system for Wikipedia. (arXiv:2306.01650v1 [cs.LG])

    [http://arxiv.org/abs/2306.01650](http://arxiv.org/abs/2306.01650)

    本文介绍了一种支持多语言维基百科破坏检测的新型系统设计，经过评估发现其覆盖了更多的语言，效率更高，并且比维基百科中生产使用的ORES系统更加准确和公正。

    

    本文介绍了一个新颖的系统设计，旨在支持维基百科社区解决平台上的破坏问题。为了实现这一目标，我们收集了47种语言的大规模数据集，并应用了先进的过滤和特征工程技术，包括多语言掩码语言建模，从人生成的数据构建训练数据集。通过与维基百科生产中使用的称为ORES的系统进行比较，我们评估了系统的性能。我们的研究结果显著增加了覆盖语言的数量，使维基百科的巡逻更有效地服务于更广泛的社区。此外，我们的模型表现优于ORES，确保提供的结果不仅更准确，而且也不会对某些贡献者群体产生偏见。

    This paper presents a novel design of the system aimed at supporting the Wikipedia community in addressing vandalism on the platform. To achieve this, we collected a massive dataset of 47 languages, and applied advanced filtering and feature engineering techniques, including multilingual masked language modeling to build the training dataset from human-generated data. The performance of the system was evaluated through comparison with the one used in production in Wikipedia, known as ORES. Our research results in a significant increase in the number of languages covered, making Wikipedia patrolling more efficient to a wider range of communities. Furthermore, our model outperforms ORES, ensuring that the results provided are not only more accurate but also less biased against certain groups of contributors.
    
[^21]: 用本地超梯度估计的联邦多序列随机逼近

    Federated Multi-Sequence Stochastic Approximation with Local Hypergradient Estimation. (arXiv:2306.01648v1 [cs.LG])

    [http://arxiv.org/abs/2306.01648](http://arxiv.org/abs/2306.01648)

    本文提出了FedMSA，这是第一个针对多序列随机逼近（MSA）的联邦算法，并建立了其近乎最优的通信复杂度。通过本地客户端更新，FedMSA实现了BLO和MCO中超梯度的可证估计。文中还结合动量和方差缩减技术来加速，导致接近最优的速率。

    

    多序列随机逼近（MSA）已经被广泛应用于机器学习中，因为它包含了许多问题的丰富类别，包括双层优化、多层组合优化和强化学习（特别是演员-评论家方法）。然而，即使对于双序列逼近（DSA）的特殊情况，设计出经过证明的有效联邦算法也一直是一个难以捉摸的问题。为了实现这个目标，我们开发了FedMSA，这是第一个针对MSA的联邦算法，并建立了其近乎最优的通信复杂度。作为核心创新，(i) FedMSA通过本地客户端更新实现了BLO和MCO中超梯度的可证估计，这在以前的理论中是一个显著的瓶颈， (ii) 我们的收敛性保证对问题的异质性水平是敏感的。我们还结合动量和方差缩减技术来实现进一步的加速，导致接近最优的速率。

    Stochastic approximation with multiple coupled sequences (MSA) has found broad applications in machine learning as it encompasses a rich class of problems including bilevel optimization (BLO), multi-level compositional optimization (MCO), and reinforcement learning (specifically, actor-critic methods). However, designing provably-efficient federated algorithms for MSA has been an elusive question even for the special case of double sequence approximation (DSA). Towards this goal, we develop FedMSA which is the first federated algorithm for MSA, and establish its near-optimal communication complexity. As core novelties, (i) FedMSA enables the provable estimation of hypergradients in BLO and MCO via local client updates, which has been a notable bottleneck in prior theory, and (ii) our convergence guarantees are sensitive to the heterogeneity-level of the problem. We also incorporate momentum and variance reduction techniques to achieve further acceleration leading to near-optimal rates.
    
[^22]: 人类专家审核研究

    Auditing for Human Expertise. (arXiv:2306.01646v1 [stat.ML])

    [http://arxiv.org/abs/2306.01646](http://arxiv.org/abs/2306.01646)

    人类专家的价值超出了算法可捕捉范围，我们可以用一个简单的程序测试这个问题。

    

    高风险预测任务（例如患者诊断）通常由接受培训的人类专家处理。在这些设置中，自动化的一个常见问题是，专家可能运用很难建模的直觉，并且/或者可以获取信息（例如与患者的交谈），这些信息对于算法来说是不可用的。这引发了一个自然的问题，人类专家是否增加了无法被算法预测器捕捉到的价值。我们开发了一个统计框架，可以将这个问题提出为一个自然的假设检验。正如我们的框架所强调的那样，检测人类专业知识比简单比较专家预测准确性与特定学习算法做出的准确性更加微妙。而是提出了一个简单的程序，测试专家预测是否在“特征”可用而条件下是否与感兴趣的结果统计上独立。因此，我们测试的拒绝表明了人类专业知识确实增加了超出算法可捕捉范围的价值。

    High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor. We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (`features'). A rejection of our test thus suggests that huma
    
[^23]: 量子神经网络中的有限采样噪声降低

    Reduction of finite sampling noise in quantum neural networks. (arXiv:2306.01639v1 [quant-ph])

    [http://arxiv.org/abs/2306.01639](http://arxiv.org/abs/2306.01639)

    介绍方差规范化技术方法，减小量子神经网络的有限采样噪声，在QNN的构造妥善的情况下，无需额外电路计算，测试发现可以显著地降低噪声水平及加快训练速度。

    

    量子神经网络(QNNs)使用参数化的量子电路与数据相关的输入来生成输出, 通过计算期望值带来了基本的有限采样噪声，即使在无误差的量子计算机上也会出现此现象。我们通过介绍方差规范化技术来减少这种噪声，该技术可以减小量子模型训练期间期望值的方差。如果QNN已经妥善构造，则此技术无需进行额外的电路计算。我们的实证研究表明，降低方差可以加快训练速度，降低输出噪声，减少梯度电路评估中的测量次数。我们对多项式函数回归进行了基准测试。在我们的示例中，我们展示了这种规范化方法平均可以降低一个数量级的方差，从而显着降低了噪声水平。

    Quantum neural networks (QNNs) use parameterized quantum circuits with data-dependent inputs and generate outputs through the evaluation of expectation values. Calculating these expectation values necessitates repeated circuit evaluations, thus introducing fundamental finite-sampling noise even on error-free quantum computers. We reduce this noise by introducing the variance regularization, a technique for reducing the variance of the expectation value during the quantum model training. This technique requires no additional circuit evaluations if the QNN is properly constructed. Our empirical findings demonstrate the reduced variance speeds up the training and lowers the output noise as well as decreases the number of measurements in the gradient circuit evaluation. This regularization method is benchmarked on the regression of multiple functions. We show that in our examples, it lowers the variance by an order of magnitude on average and leads to a significantly reduced noise level of
    
[^24]: 随着时间的推移我们是否变得更加聪明？关于分层背景知识的因果等价性问题

    Do we become wiser with time? On causal equivalence with tiered background knowledge. (arXiv:2306.01638v1 [stat.ML])

    [http://arxiv.org/abs/2306.01638](http://arxiv.org/abs/2306.01638)

    本文探究了如何利用分层背景知识来限制等价类，从而有效简化因果效应估计和提高计算效率，同时提供了关于背景知识有效的见解。

    

    由于等价类的有向无环图（通过CPDAGs表示）可能过于庞大，无法提供有用的因果信息。本文探讨了如何整合分层背景知识来限制等价类，从而得出由“分层MPDAGs”表示的等价类。使用分层知识能够显著提高信息量和计算效率：我们表明，构建分层MPDAGs只需要应用Meek的第一法则，并且分层MPDAG（不同于一般的MPDAG）是具有弦图组成部分的链图。这带来了简化，例如确定用于因果效应估计的有效调整集。此外，我们表征了何时一种分层排序比另一种更具信息量，从而提供了关于背景知识有用的方面的见解。

    Equivalence classes of DAGs (represented by CPDAGs) may be too large to provide useful causal information. Here, we address incorporating tiered background knowledge yielding restricted equivalence classes represented by 'tiered MPDAGs'. Tiered knowledge leads to considerable gains in informativeness and computational efficiency: We show that construction of tiered MPDAGs only requires application of Meek's 1st rule, and that tiered MPDAGs (unlike general MPDAGs) are chain graphs with chordal components. This entails simplifications e.g. of determining valid adjustment sets for causal effect estimation. Further, we characterise when one tiered ordering is more informative than another, providing insights into useful aspects of background knowledge.
    
[^25]: Gode -- 将生物化学知识图谱集成到分子图神经网络的预训练中

    Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])

    [http://arxiv.org/abs/2306.01631](http://arxiv.org/abs/2306.01631)

    本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。

    

    分子属性的准确预测对于促进创新治疗方法的发展和理解化学物质和生物系统之间复杂的相互作用至关重要。本研究提出了一种新的方法，将单个分子结构的图表示与生物医学知识图谱 (KG) 的多个领域信息进行集成。通过集成两个级别的信息，我们可以使用自我监督策略预先训练更广泛和更强大的表示，用于分子级和 KG 级预测任务。在性能评估方面，我们在 11 个具有挑战性的化学属性预测任务上微调我们预先训练的模型。我们的框架的结果表明，我们微调的模型优于现有的最先进的模型。

    The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
    
[^26]: HomE: 同变性单应性视频表示学习

    HomE: Homography-Equivariant Video Representation Learning. (arXiv:2306.01623v1 [cs.CV])

    [http://arxiv.org/abs/2306.01623](http://arxiv.org/abs/2306.01623)

    本文提出了一种新的多视角视频表示学习方法，通过学习隐式映射，形成了一个具有同变性的表示空间，能够对行动识别和行人意图预测任务获得最先进的结果。

    

    近年来，自监督表示学习的快速发展使得模型在不需要大量标注数据的情况下得到了更高效和更强健的性能。然而，大部分工作仍然集中在图像上，很少有工作关注视频，甚至更少的工作关注多视角视频，其中可以利用更强大的归纳偏差进行自监督。在本文中，我们提出了一种新的多视角视频表示学习方法，在其中我们显式地模拟表示空间以保持同变性。我们的方法学习不同视角之间的隐式映射，最终形成一个表示空间，其中保持相邻视角之间的单应关系。我们通过行动识别和行人意图预测作为下游任务来评估我们的HomE表示。在UCF101数据集上，我们的方法在3次交叉验证的准确度达到了96.4％，优于大多数最先进的自监督学习方法。同样，在行人意图预测的STIP数据集上，我们的方法也达到了最先进的结果。

    Recent advances in self-supervised representation learning have enabled more efficient and robust model performance without relying on extensive labeled data. However, most works are still focused on images, with few working on videos and even fewer on multi-view videos, where more powerful inductive biases can be leveraged for self-supervision. In this work, we propose a novel method for representation learning of multi-view videos, where we explicitly model the representation space to maintain Homography Equivariance (HomE). Our method learns an implicit mapping between different views, culminating in a representation space that maintains the homography relationship between neighboring views. We evaluate our HomE representation via action recognition and pedestrian intent prediction as downstream tasks. On action classification, our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than most state-of-the-art self-supervised learning methods. Similarly, on the STIP da
    
[^27]: 基于自然语言处理聚类和机器学习的信贷风险模型问题分析：来自验证报告的见解

    Analyzing Credit Risk Model Problems through NLP-Based Clustering and Machine Learning: Insights from Validation Reports. (arXiv:2306.01618v1 [cs.LG])

    [http://arxiv.org/abs/2306.01618](http://arxiv.org/abs/2306.01618)

    本文利用聚类和机器学习算法对信贷风险模型中的问题进行了研究，通过分析验证报告中的文本信息，可以识别和分类信贷风险模型中的常见问题。

    

    本文探讨了使用聚类方法和机器学习算法（包括自然语言处理）通过验证报告中的文本信息识别和分类信贷风险模型中的问题。使用一份独特的数据集，包含了一个大型国际银行集团在2019年1月至2022年12月期间验证团队提出的657个发现。将这些发现分为九个验证维度，并由验证人员使用其专业知识分配了一个严重程度级别。作者使用四个不同的预训练模型生成的发现标题和观察结果的嵌入向量，包括TensorFlow Hub中的“module_url”和SentenceTransformer库中的三个模型，即“all-mpnet-base-v2”、“all-MiniLM-L6-v2”和“paraphrase-mpnet-base-v2”。本文使用和比较了各种聚类方法来对具有相似特征的发现进行分组，从而实现了在每个验证维度内识别常见问题的目标。

    This paper explores the use of clustering methods and machine learning algorithms, including Natural Language Processing (NLP), to identify and classify problems identified in credit risk models through textual information contained in validation reports. Using a unique dataset of 657 findings raised by validation teams in a large international banking group between January 2019 and December 2022. The findings are classified into nine validation dimensions and assigned a severity level by validators using their expert knowledge. The authors use embedding generation for the findings' titles and observations using four different pre-trained models, including "module\_url" from TensorFlow Hub and three models from the SentenceTransformer library, namely "all-mpnet-base-v2", "all-MiniLM-L6-v2", and "paraphrase-mpnet-base-v2". The paper uses and compares various clustering methods in grouping findings with similar characteristics, enabling the identification of common problems within each v
    
[^28]: 数据污染下的超参数学习：基于多目标二层优化的正则化影响分析

    Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization. (arXiv:2306.01613v1 [cs.LG])

    [http://arxiv.org/abs/2306.01613](http://arxiv.org/abs/2306.01613)

    本文提出了一种考虑攻击对超参数影响的最优攻击公式，将攻击建模为多目标双层优化问题，可以更准确地评估算法鲁棒性和学习超参数，在多个数据集上的评估证明了这种方法的优势。

    

    机器学习算法容易遭受数据污染攻击，即通过操纵部分训练数据来有意破坏算法的性能。最优攻击可以被制定为双层优化问题，并有助于在最坏情况下评估算法的强健性。我们发现当前的方法通常假定超参数保持不变，这导致了对算法鲁棒性和正则化影响的过于悲观的观点。因此我们提出了一种新的最优攻击公式，考虑攻击对超参数的影响，并将攻击建模为多目标双层优化问题。这允许制定最优攻击、学习超参数并在最坏情况下评估鲁棒性。我们将此攻击公式应用于使用$L_2$和$L_1$正则化的多个机器学习分类器上。我们对多个数据集的评估确认了先前策略的限制，并证明了我们提出的方法具有更精确的鲁棒性评估和在存在数据污染攻击时更有效地学习超参数的优点。

    Machine Learning (ML) algorithms are vulnerable to poisoning attacks, where a fraction of the training data is manipulated to deliberately degrade the algorithms' performance. Optimal attacks can be formulated as bilevel optimization problems and help to assess their robustness in worst-case scenarios. We show that current approaches, which typically assume that hyperparameters remain constant, lead to an overly pessimistic view of the algorithms' robustness and of the impact of regularization. We propose a novel optimal attack formulation that considers the effect of the attack on the hyperparameters and models the attack as a multiobjective bilevel optimization problem. This allows to formulate optimal attacks, learn hyperparameters and evaluate robustness under worst-case conditions. We apply this attack formulation to several ML classifiers using $L_2$ and $L_1$ regularization. Our evaluation on multiple datasets confirms the limitations of previous strategies and evidences the ben
    
[^29]: 中心化自注意力层

    Centered Self-Attention Layers. (arXiv:2306.01610v1 [cs.LG])

    [http://arxiv.org/abs/2306.01610](http://arxiv.org/abs/2306.01610)

    本文提出了一种基于中心化自注意力层的纠错机制，它可以消除transformers中过度平滑的问题，并使图神经网络训练非常深的架构比许多最近的解决方案更有效。

    

    在深度学习结构中，transformer中的自注意力机制和图神经网络中的消息传递机制被反复应用。我们发现，这种应用不可避免地导致过度平滑，即在transformer中不同令牌和在图神经网络中不同节点的深层表示非常相似。基于我们的分析，我们呈现了这些机制聚合运算符的纠正项。在实证方面，这个简单的术语消除了图像transformers中凸显的问题，在弱监督分割方面获得了超过引入多个辅助网络和训练阶段的精心基准方法的性能。在图神经网络中，纠正项使得训练非常深的架构比许多最近的解决方案更有效。

    The self-attention mechanism in transformers and the message-passing mechanism in graph neural networks are repeatedly applied within deep learning architectures. We show that this application inevitably leads to oversmoothing, i.e., to similar representations at the deeper layers for different tokens in transformers and different nodes in graph neural networks. Based on our analysis, we present a correction term to the aggregating operator of these mechanisms. Empirically, this simple term eliminates much of the oversmoothing problem in visual transformers, obtaining performance in weakly supervised segmentation that surpasses elaborate baseline methods that introduce multiple auxiliary networks and training phrases. In graph neural networks, the correction term enables the training of very deep architectures more effectively than many recent solutions to the same problem.
    
[^30]: 去中心化联邦学习：一份综述与展望

    Decentralized Federated Learning: A Survey and Perspective. (arXiv:2306.01603v1 [cs.LG])

    [http://arxiv.org/abs/2306.01603](http://arxiv.org/abs/2306.01603)

    去中心化联邦学习通过消除中心服务器的需求，实现了直接通信从而节省了通信资源。本文提供了对DFL的全面调查、深入展望和扩展变体与分类的介绍，重点在于DFL的系统与详细前景。

    

    联邦学习（FL）因其在共享知识的同时保护用户数据、保护隐私、提高学习效率并减少通信负载的能力而受到关注。去中心化联邦学习（DFL）是一种去中心化的网络架构，与集中式FL（CFL）相比，DFL消除了中心服务器的需求。DFL使客户端之间可以直接通信，从而显著节省了通信资源。本文对DFL进行了全面的调查和深入的展望。首先，对CFL的方法、挑战和变体进行了回顾，奠定了DFL的背景。然后，介绍了DFL的系统和详细的前景，包括迭代顺序、通信协议、网络拓扑、范例提议和时间变异性。接下来，基于DFL的定义，提出了几个扩展变体和分类并使用了最先进的技术。最后，除了总结DFL的优势与劣势，对DFL未来研究和应用前景进行了展望。

    Federated learning (FL) has been gaining attention for its ability to share knowledge while maintaining user data, protecting privacy, increasing learning efficiency, and reducing communication overhead. Decentralized FL (DFL) is a decentralized network architecture that eliminates the need for a central server in contrast to centralized FL (CFL). DFL enables direct communication between clients, resulting in significant savings in communication resources. In this paper, a comprehensive survey and profound perspective is provided for DFL. First, a review of the methodology, challenges, and variants of CFL is conducted, laying the background of DFL. Then, a systematic and detailed perspective on DFL is introduced, including iteration order, communication protocols, network topologies, paradigm proposals, and temporal variability. Next, based on the definition of DFL, several extended variants and categorizations are proposed with state-of-the-art technologies. Lastly, in addition to sum
    
[^31]: 基于GNN和核均值嵌入的原子模拟传递学习

    Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])

    [http://arxiv.org/abs/2306.01589](http://arxiv.org/abs/2306.01589)

    本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。

    

    使用机器学习方法学习的原子相互作用势在原子模拟中得到了成功的应用。然而，深度学习管道需要大量数据，而生成参考计算是计算上要求很高的。为了克服这一困难，我们提出了一种传递学习算法，利用了图神经网络（GNNs）在描述化学环境方面的能力，以及核均值嵌入。我们从预先在OC20数据集上进行过训练的GNN中提取特征映射，并使用它来从催化过程的系统特定数据集中学习势能表面。我们的方法进一步通过灵活的核函数来增强，该核函数包括化学物种信息，从而提高了性能和可解释性。我们在一系列逐渐复杂的现实数据集上测试了我们的方法，展示了出色的概括能力和可转移性能，改进了依赖GNNs或岭回归方法的方法。

    Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
    
[^32]: 概率概念瓶颈模型

    Probabilistic Concept Bottleneck Models. (arXiv:2306.01574v1 [cs.LG])

    [http://arxiv.org/abs/2306.01574](http://arxiv.org/abs/2306.01574)

    本文提出了概率概念瓶颈模型（ProbCBM），通过建立概率概念嵌入来解决数据中概念存在模糊性的问题，提高了CBM模型的可靠性及解释的可信度。

    

    可解释的模型旨在以可读的方式做出决策。其中，概念瓶颈模型（CBM）根据预测的概念进行概念预测和类预测两步骤。CBM使用从概念预测中得出的高级概念提供解释；因此，可靠的概念预测对于可信度非常重要。在本研究中，我们解决了可能损害可靠性的模糊性问题。虽然数据中概念的存在往往是模糊的，但CBM在不考虑此模糊性的情况下使用确定性方法预测概念。为了针对这种模糊性提供可靠的解释，我们提出了概率概念瓶颈模型（ProbCBM）。通过利用概率概念嵌入，ProbCBM对概念预测中的不确定性进行建模，并基于概念及其相应的不确定性提供解释。这种不确定性提高了解释的可靠性。此外，由于类别预测的概率性质，ProbCBM还提供了类别预测的不确定度估计。

    Interpretable models are designed to make decisions in a human-interpretable manner. Representatively, Concept Bottleneck Models (CBM) follow a two-step process of concept prediction and class prediction based on the predicted concepts. CBM provides explanations with high-level concepts derived from concept predictions; thus, reliable concept predictions are important for trustworthiness. In this study, we address the ambiguity issue that can harm reliability. While the existence of a concept can often be ambiguous in the data, CBM predicts concepts deterministically without considering this ambiguity. To provide a reliable interpretation against this ambiguity, we propose Probabilistic Concept Bottleneck Models (ProbCBM). By leveraging probabilistic concept embeddings, ProbCBM models uncertainty in concept prediction and provides explanations based on the concept and its corresponding uncertainty. This uncertainty enhances the reliability of the explanations. Furthermore, as class unc
    
[^33]: 时空深度学习辅助下的安全受限机组组合优化

    Spatio-Temporal Deep Learning-Assisted Reduced Security-Constrained Unit Commitment. (arXiv:2306.01570v1 [cs.LG])

    [http://arxiv.org/abs/2306.01570](http://arxiv.org/abs/2306.01570)

    本研究提出了一种利用时空深度学习模型进行安全受限机组组合优化的新方法，并通过对多个测试系统的验证，证明了其在解决方案质量和计算时间方面的有效性。

    

    安全受限机组组合优化（SCUC）是电力系统日前调度和市场清算中所需的一个计算复杂的过程。该过程需要每日运行，并需要先进的算法来加快这个过程。与SCUC相关的约束和数据都具有地理和时间相关性，以确保解决方案的可靠性，这进一步增加了其复杂性。本文利用先进的机器学习模型来研究电力系统历史数据中的模式，从而囊括约束条件中的时空相关模式。时空相关机器学习模型通过采用图神经网络（GNN）来理解空间相关性，而使用长短时记忆（LSTM）网络来研究时间序列。本文提出的方法在多个测试系统（如IEEE 24-Bus系统、IEEE 73-Bus系统、IEEE 118-Bus系统和SC 500-Bus系统）上进行了验证，同时与现有方法进行了比较，其解决方案的质量和计算时间均证明其有效性。

    Security-constrained unit commitment (SCUC) is a computationally complex process utilized in power system day-ahead scheduling and market clearing. SCUC is run daily and requires state-of-the-art algorithms to speed up the process. The constraints and data associated with SCUC are both geographically and temporally correlated to ensure the reliability of the solution, which further increases the complexity. In this paper, an advanced machine learning (ML) model is used to study the patterns in power system historical data, which inherently considers both spatial and temporal (ST) correlations in constraints. The ST-correlated ML model is trained to understand spatial correlation by considering graph neural networks (GNN) whereas temporal sequences are studied using long short-term memory (LSTM) networks. The proposed approach is validated on several test systems namely, IEEE 24-Bus system, IEEE-73 Bus system, IEEE 118-Bus system, and synthetic South-Carolina (SC) 500-Bus system. Moreov
    
[^34]: 公开可用的乳腺组织病理学H&E全切片图像数据集：一项系统性综述。

    Publicly available datasets of breast histopathology H&E whole-slide images: A systematic review. (arXiv:2306.01546v1 [eess.IV])

    [http://arxiv.org/abs/2306.01546](http://arxiv.org/abs/2306.01546)

    本文从乳腺H&E染色全切片图像数据集的公开来源出发，系统性地综述了当前可用的数据集情况，包含了12个公开数据集和相关特征信息。总结出数据集的缺失以及更全面和标准化的数据集建立的必要性。

    

    数字病理学和计算资源的进步对乳腺癌诊断和治疗的计算病理学领域产生了重大影响。 然而，获得高质量的标记乳腺癌组织学图像表示着一个重大难题，从而限制了准确和强大的深度学习模型的开发。 在这项系统性综述中，我们确定了可用于开发深度学习算法的公开可用的乳腺H＆E染色全切片图像（WSI）数据集。 我们系统地搜索了9个科学文献数据库和9个研究数据存储库。 我们找到了12个公开可用的数据集，包含5153个乳腺癌H&E WSI。 此外，我们针对每个数据集报告了图像元数据和特征，以帮助研究人员选择适合乳腺癌计算病理学具体任务的数据集。 另外，我们还编制了用于包括文章的修补程序和私有数据集的列表。 我们的综述强调了需要更全面和标准化的数据集，以促进准确和强大的乳腺癌诊断和治疗深度学习算法的发展。

    Advancements in digital pathology and computing resources have made a significant impact in the field of computational pathology for breast cancer diagnosis and treatment. However, access to high-quality labeled histopathological images of breast cancer is a big challenge that limits the development of accurate and robust deep learning models. In this systematic review, we identified the publicly available datasets of breast H&E stained whole-slide images (WSI) that can be used to develop deep learning algorithms. We systematically searched nine scientific literature databases and nine research data repositories. We found twelve publicly available datasets, containing 5153 H&E WSIs of breast cancer. Moreover, we reported image metadata and characteristics for each dataset to assist researchers in selecting proper datasets for specific tasks in breast cancer computational pathology. In addition, we compiled a list of patch and private datasets that were used in the included articles as 
    
[^35]: AUC优化是否值得？

    Does it pay to optimize AUC?. (arXiv:2306.01528v1 [cs.CG])

    [http://arxiv.org/abs/2306.01528](http://arxiv.org/abs/2306.01528)

    本文提出了一种高效算法AUC-opt，用于在$\mathbb{R}^2$中找到可证明的最优AUC线性分类器，并可扩展到$\mathbb{R}^d$。实验表明，在合成和真实数据集上，与其他方法相比，AUC-opt可以显着提高AUC值。优化AUC确实有价值，并且先前研究的限制主要是由于缺乏高效的优化算法。

    

    ROC曲线下面积（AUC）是二元分类器评估的一个重要模型指标，已经提出了许多算法来近似优化AUC。这引发了一个问题，即先前研究观察到的普遍微不足道的收益是由指标固有的限制还是由优化的不足质量引起的？为了更好地理解优化AUC的价值，我们提出了一种高效算法AUC-opt，在$\mathbb{R}^2$中找到可证明的最优AUC线性分类器，其运行时间为$\mathcal{O}(n_+ n_- \log (n_+ n_-))$，其中$n_+$和$n_-$分别是正样本和负样本的数量。此外，通过递归调用低维空间中的AUC-opt，它可以自然地扩展到$\mathbb{R}^d$中，时间复杂度为$\mathcal{O}((n_+n_-)^{d-1}\log (n_+n_-))$。我们证明，当$d$不固定时，该问题是NP完全的，从“开放半球问题”中减少。实验表明，与其他方法相比，AUC-opt在合成和真实数据集上都可以显着提高AUC值。我们的结果表明，优化AUC确实有价值，并且先前研究的限制主要是由于缺乏高效的优化算法。

    The Area Under the ROC Curve (AUC) is an important model metric for evaluating binary classifiers, and many algorithms have been proposed to optimize AUC approximately. It raises the question of whether the generally insignificant gains observed by previous studies are due to inherent limitations of the metric or the inadequate quality of optimization.  To better understand the value of optimizing for AUC, we present an efficient algorithm, namely AUC-opt, to find the provably optimal AUC linear classifier in $\mathbb{R}^2$, which runs in $\mathcal{O}(n_+ n_- \log (n_+ n_-))$ where $n_+$ and $n_-$ are the number of positive and negative samples respectively. Furthermore, it can be naturally extended to $\mathbb{R}^d$ in $\mathcal{O}((n_+n_-)^{d-1}\log (n_+n_-))$ by calling AUC-opt in lower-dimensional spaces recursively. We prove the problem is NP-complete when $d$ is not fixed, reducing from the \textit{open hemisphere problem}.  Experiments show that compared with other methods, AUC-
    
[^36]: 基于Transformer的多模态学习用于多标签遥感图像分类

    Transformer-based Multi-Modal Learning for Multi Label Remote Sensing Image Classification. (arXiv:2306.01523v1 [cs.CV])

    [http://arxiv.org/abs/2306.01523](http://arxiv.org/abs/2306.01523)

    本文提出了一种基于Transformer的多模态学习架构，能够在远程感知图像的多模态多标签分类中进行有效的信息交换，提高了分类效果。

    

    本文在远程感知图像的多模态多标签分类（MLC）框架下，引入了一种新颖的同步类令牌融合（SCT Fusion）架构。所提出的架构利用模态特定的基于注意力的Transformer编码器来处理不同的输入模态，并在每个Transformer编码器块后通过同步特殊类令牌来跨模态交换信息。同步包括通过可训练的融合变换将类令牌融合，从而产生一个包含所有模态信息的同步类令牌。由于融合变换是可训练的，因此它允许准确表示不同模态之间共享的特征。实验结果表明，在多模态MLC数据集上评估时，所提出的架构比单模态架构和早期融合多模态架构更有效。所提出的架构代码已公开。

    In this paper, we introduce a novel Synchronized Class Token Fusion (SCT Fusion) architecture in the framework of multi-modal multi-label classification (MLC) of remote sensing (RS) images. The proposed architecture leverages modality-specific attention-based transformer encoders to process varying input modalities, while exchanging information across modalities by synchronizing the special class tokens after each transformer encoder block. The synchronization involves fusing the class tokens with a trainable fusion transformation, resulting in a synchronized class token that contains information from all modalities. As the fusion transformation is trainable, it allows to reach an accurate representation of the shared features among different modalities. Experimental results show the effectiveness of the proposed architecture over single-modality architectures and an early fusion multi-modal architecture when evaluated on a multi-modal MLC dataset.  The code of the proposed architectur
    
[^37]: 网络劣化作为训练性能评估的指标：有限和无限宽度角度预测的比较。

    Network Degeneracy as an Indicator of Training Performance: Comparing Finite and Infinite Width Angle Predictions. (arXiv:2306.01513v1 [cs.LG])

    [http://arxiv.org/abs/2306.01513](http://arxiv.org/abs/2306.01513)

    本文发现网络劣化现象，这会影响网络训练并导致其表现不佳。我们还使用了一种简单算法来预测网络劣化的水平。

    

    神经网络是功能强大且广泛使用的方法，但其理论行为并没有完全被理解。通过堆叠许多层，可以创建深层神经网络，在许多应用中取得了出色的性能，并促成了最近这些方法的爆炸。先前的研究表明，深度可以指数级增加网络的表达能力。然而，随着网络越来越深，它们越来越容易变得劣化。我们观察到这种退化现象，因为在初始化时，输入倾向于在通过网络的层时变得越来越相关。如果一个网络有太多层，它倾向于逼近一个（随机的）常数函数，有效地无法区分输入。我们在本文中进行了实证研究，发现这似乎影响了网络的训练，并导致它表现不佳。我们使用一种简单的算法，可以准确地预测网络达到的劣化水平。

    Neural networks are powerful functions with widespread use, but the theoretical behaviour of these functions is not fully understood. Creating deep neural networks by stacking many layers has achieved exceptional performance in many applications and contributed to the recent explosion of these methods. Previous works have shown that depth can exponentially increase the expressibility of the network. However, as networks get deeper and deeper, they are more susceptible to becoming degenerate. We observe this degeneracy in the sense that on initialization, inputs tend to become more and more correlated as they travel through the layers of the network. If a network has too many layers, it tends to approximate a (random) constant function, making it effectively incapable of distinguishing between inputs. This seems to affect the training of the network and cause it to perform poorly, as we empirically investigate in this paper. We use a simple algorithm that can accurately predict the leve
    
[^38]: 在对话情感识别中使用监督式对抗性对比学习

    Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])

    [http://arxiv.org/abs/2306.01505](http://arxiv.org/abs/2306.01505)

    本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示，通过联合类别分布对比学习目标，有效利用标签级特性一致性并保留细粒度的类内特性，实现了在对话情感识别中最先进的结果。

    

    情感识别在对话中是提取泛化和稳健表示的一个重要挑战。为了解决这个问题，本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示。该框架应用于对比感知对抗性训练以生成最坏情况的样本，并在原始和对抗样本上使用联合类别分布对比学习目标。它可以有效地利用标签级特性一致性并保留细粒度的类内特性。为了避免对上下文相关数据产生负面影响，我们设计了一个上下文对抗性训练策略，从上下文中学习更多不同的特征，并增强模型对上下文的容错性。在该框架下，我们开发了一个基于序列的方法SACL-LSTM，用于学习针对ERC的标签一致和上下文稳健的情感特征。在三个数据集上的实验证明，SACL-LSTM在对话情感识别方面实现了最先进的结果，优于现有的方法。

    Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
    
[^39]: LLMs（如GPT-4）在痴呆症诊断中能否胜过传统AI工具？或许有潜力，但现在还不行。

    Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today. (arXiv:2306.01499v1 [cs.CL])

    [http://arxiv.org/abs/2306.01499](http://arxiv.org/abs/2306.01499)

    本文探讨了LLMs如GPT-4在痴呆症诊断上的潜力，发现目前还无法胜过传统AI工具。

    

    最近的研究表明，大型语言模型（LLMs），特别是GPT-4，在常见的自然语言处理（NLP）任务上具有卓越的能力，还在各种专业和学术基准测试中展现出接近人类水平的性能。然而，GPT-4是否能直接用于实际应用，并替代专业领域中的传统人工智能（AI）工具，需要进一步的实验验证。本文探讨了LLMs（如GPT-4）在痴呆症诊断上胜过传统AI工具的潜力。通过对比两种工具在临床环境中的诊断准确性，全面探究了GPT-4和传统AI工具的优缺点。两个真实临床数据集上的实验结果表明，尽管像GPT-4这样的LLMs在未来的痴呆症诊断中具有潜力，但目前仍无法超过传统AI工具的性能表现。同时，本文还评估了GPT-4的解释性和可信性，同时也指出了将LLMs集成到医疗应用中面临的挑战。

    Recent investigations show that large language models (LLMs), specifically GPT-4, not only have remarkable capabilities in common Natural Language Processing (NLP) tasks but also exhibit human-level performance on various professional and academic benchmarks. However, whether GPT-4 can be directly used in practical applications and replace traditional artificial intelligence (AI) tools in specialized domains requires further experimental validation. In this paper, we explore the potential of LLMs such as GPT-4 to outperform traditional AI tools in dementia diagnosis. Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting. Experimental results on two real clinical datasets show that, although LLMs like GPT-4 demonstrate potential for future advancements in dementia diagnosis, they currently do not surpass the performance of traditional AI tools. The interpretability and faithfulness of GPT-4 are also eval
    
[^40]: 基于困难系统的局部消息传递

    Local Message Passing on Frustrated Systems. (arXiv:2306.01494v1 [cs.LG])

    [http://arxiv.org/abs/2306.01494](http://arxiv.org/abs/2306.01494)

    本文提出了一种优化的基于困难系统的局部消息传递算法，能够在循环图上获得良好的表现。

    

    因子图上的消息传递是概率推理的强大框架，在各种科学领域中具有重要应用。最广泛使用的消息传递方案是求和-积算法（SPA），它在树上可得到精确结果，但在具有许多小循环的图上往往失败。我们寻求一种替代的消息传递算法，特别适用于这种循环图。为此，我们挑战了SPA的外部原则，这失去了在循环图上的客观性。我们进一步用数据驱动的方式将底层图的因子节点处的本地SPA消息更新规则替换为通用映射。这些修改提高了性能，同时保持了SPA的简单性。我们对两类循环图进行了评估：2x2完全连接的Ising网格和线性通信通道上的符号检测因子图。

    Message passing on factor graphs is a powerful framework for probabilistic inference, which finds important applications in various scientific domains. The most wide-spread message passing scheme is the sum-product algorithm (SPA) which gives exact results on trees but often fails on graphs with many small cycles. We search for an alternative message passing algorithm that works particularly well on such cyclic graphs. Therefore, we challenge the extrinsic principle of the SPA, which loses its objective on graphs with cycles. We further replace the local SPA message update rule at the factor nodes of the underlying graph with a generic mapping, which is optimized in a data-driven fashion. These modifications lead to a considerable improvement in performance while preserving the simplicity of the SPA. We evaluate our method for two classes of cyclic graphs: the 2x2 fully connected Ising grid and factor graphs for symbol detection on linear communication channels with inter-symbol interf
    
[^41]: 《关于能量模型中特征多样性的研究》

    On Feature Diversity in Energy-based Models. (arXiv:2306.01489v1 [cs.LG])

    [http://arxiv.org/abs/2306.01489](http://arxiv.org/abs/2306.01489)

    本文研究了能量模型中特征的多样性，分析了冗余的效果，并证明减少特征集的冗余可以提高模型性能。

    

    能量学习是一种强大的学习范式，它包括了各种判别和生成方法。能量模型（EBM）通常由内部模型组成，学习组合不同的特征来为每个输入配置生成能量映射。本文关注产生特征集的多样性。我们扩展了EBMs的可能近似正确（PAC）理论，分析了减少冗余对EBMs性能的影响。我们导出了针对不同学习上下文的泛化边界，即回归、分类和隐式回归，其中包括不同的能量函数，我们表明，确实减少特征集的冗余可以一致地减少真实期望和经验期望之间的差距，并增强模型的性能。

    Energy-based learning is a powerful learning paradigm that encapsulates various discriminative and generative approaches. An energy-based model (EBM) is typically formed of inner-model(s) that learn a combination of the different features to generate an energy mapping for each input configuration. In this paper, we focus on the diversity of the produced feature set. We extend the probably approximately correct (PAC) theory of EBMs and analyze the effect of redundancy reduction on the performance of EBMs. We derive generalization bounds for various learning contexts, i.e., regression, classification, and implicit regression, with different energy functions and we show that indeed reducing redundancy of the feature set can consistently decrease the gap between the true and empirical expectation of the energy and boosts the performance of the model.
    
[^42]: 通过近似的正交约束实现稳健的低秩训练。

    Robust low-rank training via approximate orthonormal constraints. (arXiv:2306.01485v1 [cs.LG])

    [http://arxiv.org/abs/2306.01485](http://arxiv.org/abs/2306.01485)

    通过追加正交约束，从而在保持低秩矩阵分解前提下提高深度神经网络的鲁棒性与准确率。

    

    随着模型和数据规模的增长，设计剪枝技术以降低深度学习流程的资源需求并保持模型性能已成为广泛努力的目标。为了降低推理和训练成本，主要的工作方向之一使用低秩矩阵分解来表示网络权重。尽管能够保持准确性，但我们观察到低秩方法往往会损害模型对抗扰动的鲁棒性。通过将稳健性建模为神经网络的条件数，我们认为这种稳健性损失是由于低秩权重矩阵的奇异值爆炸引起的。因此，我们引入了一种稳健的低秩训练算法，该算法在保持网络权重位于低秩矩阵流形上的同时，同时强制施加近似的正交约束。因此，该模型降低了训练和推理成本，同时确保了良好的条件性和更好的抗干扰能力。

    With the growth of model and data sizes, a broad effort has been made to design pruning techniques that reduce the resource demand of deep learning pipelines, while retaining model performance. In order to reduce both inference and training costs, a prominent line of work uses low-rank matrix factorizations to represent the network weights. Although able to retain accuracy, we observe that low-rank methods tend to compromise model robustness against adversarial perturbations. By modeling robustness in terms of the condition number of the neural network, we argue that this loss of robustness is due to the exploding singular values of the low-rank weight matrices. Thus, we introduce a robust low-rank training algorithm that maintains the network's weights on the low-rank matrix manifold while simultaneously enforcing approximate orthonormal constraints. The resulting model reduces both training and inference costs while ensuring well-conditioning and thus better adversarial robustness, w
    
[^43]: 用于建模用户新奇寻求意图的层次强化学习在推荐系统中的应用

    Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking Intent in Recommender Systems. (arXiv:2306.01476v1 [cs.IR])

    [http://arxiv.org/abs/2306.01476](http://arxiv.org/abs/2306.01476)

    本文提出了一种基于层次强化学习的方法，用于建模用户的层次新奇寻求意图并调整推荐策略以提高推荐项目准确性和多样性。

    

    推荐新颖内容可以通过向用户介绍新的兴趣点来改善用户在推荐平台上的长期体验。然而，用户并不总是想要探索新颖的内容。因此，了解他们的寻求新奇的意图并相应地调整推荐策略是至关重要的。本文提出了一种基于层次强化学习的新方法，用于建模用户的层次新奇寻求意图，并根据提取的用户寻求新奇倾向性来调整推荐策略。我们发现，我们提出的方法在推荐项目的准确性和多样性方面优于现有方法。

    Recommending novel content, which expands user horizons by introducing them to new interests, has been shown to improve users' long-term experience on recommendation platforms \cite{chen2021values}. Users however are not constantly looking to explore novel content. It is therefore crucial to understand their novelty-seeking intent and adjust the recommendation policy accordingly. Most existing literature models a user's propensity to choose novel content or to prefer a more diverse set of recommendations at individual interactions. Hierarchical structure, on the other hand, exists in a user's novelty-seeking intent, which is manifested as a static and intrinsic user preference for seeking novelty along with a dynamic session-based propensity. To this end, we propose a novel hierarchical reinforcement learning-based method to model the hierarchical user novelty-seeking intent, and to adapt the recommendation policy accordingly based on the extracted user novelty-seeking propensity. We f
    
[^44]: 面向个性化方面提取的大型语言模型的提示调整用于推荐

    Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations. (arXiv:2306.01475v1 [cs.IR])

    [http://arxiv.org/abs/2306.01475](http://arxiv.org/abs/2306.01475)

    本文提出了一种端到端的方法，将面向方面的提取和推荐相结合，同时引入了个性化基于方面的推荐模型，并利用大型语言模型设计了新的模型来为最终的推荐任务生成方面。

    

    现有的方面提取方法主要依赖于显式或基础事实方面信息，或者使用数据挖掘或机器学习方法从隐含用户反馈（例如用户评论）中提取方面。然而，如何利用提取出的方面生成更有意义的推荐给用户仍然未被充分探索。同时，现有的基于方面的推荐研究通常依赖于单独的方面提取模型或假设方面是已知的，而没有考虑到最佳方面集可能取决于手头的推荐任务。在本文中，我们提出了一种将方面提取与基于方面的推荐结合起来的端到端方法，并在单个框架中同时实现这两个目标。对于方面提取组件，我们利用大型语言模型的最新进展，设计了一种新的提示学习机制来为最终的推荐任务生成方面。对于基于方面的推荐组件，我们引进了一种个性化的基于方面的推荐模型，该模型与方面提取模型一起进行训练。我们的实验证明，所提出的方法在方面提取和基于方面的推荐任务上都优于几种强基准模型。

    Existing aspect extraction methods mostly rely on explicit or ground truth aspect information, or using data mining or machine learning approaches to extract aspects from implicit user feedback such as user reviews. It however remains under-explored how the extracted aspects can help generate more meaningful recommendations to the users. Meanwhile, existing research on aspect-based recommendations often relies on separate aspect extraction models or assumes the aspects are given, without accounting for the fact the optimal set of aspects could be dependent on the recommendation task at hand.  In this work, we propose to combine aspect extraction together with aspect-based recommendations in an end-to-end manner, achieving the two goals together in a single framework. For the aspect extraction component, we leverage the recent advances in large language models and design a new prompt learning mechanism to generate aspects for the end recommendation task. For the aspect-based recommendat
    
[^45]: 通用等变Transformer：用于3D分子相互作用学习

    Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])

    [http://arxiv.org/abs/2306.01474](http://arxiv.org/abs/2306.01474)

    本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。

    

    生物学和药物开发中的许多过程涉及不同分子之间的各种3D相互作用，例如蛋白质与蛋白质，蛋白质与小分子等。设计一个通用模型来学习普适的分子相互作用具有重要价值，但也具有挑战性，因为不同的分子通常以不同粒度表示。本文首先提出了将3D分子通用表示为集合的几何图形图，与传统单层表示形式形成对比。在提出的统一表示下，我们提出了通用等变Transformer（GET），以有效地捕捉稀疏块级和密集原子级交互。具体而言，GET由双层注意力模块、前馈模块和层归一化模块组成，值得注意的是，每个模块都是E（3）等变的，以满足3D世界的对称性。在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面进行了大量实验，表明GET优于各种最先进的方法。

    Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
    
[^46]: 基于语法的文本隐私保护方法

    Guiding Text-to-Text Privatization by Syntax. (arXiv:2306.01471v1 [cs.CL])

    [http://arxiv.org/abs/2306.01471](http://arxiv.org/abs/2306.01471)

    该论文介绍了一种基于语法的文本隐私保护方法，通过解决候选选择问题以提高替换的句法一致性。

    

    指标差分隐私是针对文本隐私保护中的独特挑战而设计的差分隐私的一种扩展。通过向嵌入的几何空间中的单词表示添加噪声，单词被替换为在噪声表示的接近位置的单词。由于嵌入式是基于单词共现进行训练的，因此这种机制确保替换源于相同的语义环境。然而，如果不考虑单词的语法类别，这种机制就无法保证替换扮演相似的句法角色。我们分析了文本隐私保护在替换后保留单词语法类别的能力，并发现代用文本中几乎完全由名词构成。由于缺少产生与敏感文本结构相关的代用文本的能力，我们通过将保护步骤转换为候选选择问题来扩展我们的分析。

    Metric Differential Privacy is a generalization of differential privacy tailored to address the unique challenges of text-to-text privatization. By adding noise to the representation of words in the geometric space of embeddings, words are replaced with words located in the proximity of the noisy representation. Since embeddings are trained based on word co-occurrences, this mechanism ensures that substitutions stem from a common semantic context. Without considering the grammatical category of words, however, this mechanism cannot guarantee that substitutions play similar syntactic roles. We analyze the capability of text-to-text privatization to preserve the grammatical category of words after substitution and find that surrogate texts consist almost exclusively of nouns. Lacking the capability to produce surrogate texts that correlate with the structure of the sensitive texts, we encompass our analysis by transforming the privatization step into a candidate selection problem in whic
    
[^47]: MLP-Mixer作为宽且稀疏的MLP

    MLP-Mixer as a Wide and Sparse MLP. (arXiv:2306.01470v1 [cs.LG])

    [http://arxiv.org/abs/2306.01470](http://arxiv.org/abs/2306.01470)

    深度学习中常用的MLP有潜力提高性能。本研究揭示MLP-Mixer 可以作为具有稀疏权重的宽MLP有效地工作。

    

    多层感知器(MLP)是深度学习中被广泛应用于多种问题的基础组件。然而，最近基于MLP的架构(特别是MLP-Mixer)的实证成功表明，提高MLP的性能仍具有潜在的潜力。在本研究中，我们发现MLP-Mixer有效地作为具有某些稀疏权重的宽MLP。最初，我们澄清Mixer的混合层可以作为具有稀疏权重且由Kronecker乘积表示的更宽MLP的有效表达。该表达式自然地定义了一组置换-Kronecker(PK)家族，可以被视为混合层的一般类，也可以被视为Monarch矩阵的一种近似。随后，由于PK家族有效构成具有稀疏权重的宽MLP，因此，可以应用Golubeva、Neyshabur和Gur-Ari(2021)提出的假设，即预测性能：

    Multi-layer perceptron (MLP) is a fundamental component of deep learning that has been extensively employed for various problems. However, recent empirical successes in MLP-based architectures, particularly the progress of the MLP-Mixer, have revealed that there is still hidden potential in improving MLPs to achieve better performance. In this study, we reveal that the MLP-Mixer works effectively as a wide MLP with certain sparse weights. Initially, we clarify that the mixing layer of the Mixer has an effective expression as a wider MLP whose weights are sparse and represented by the Kronecker product. This expression naturally defines a permuted-Kronecker (PK) family, which can be regarded as a general class of mixing layers and is also regarded as an approximation of Monarch matrices. Subsequently, because the PK family effectively constitutes a wide MLP with sparse weights, one can apply the hypothesis proposed by Golubeva, Neyshabur and Gur-Ari (2021) that the prediction performanc
    
[^48]: GANs和替代方法的合成噪声生成用于无损超声测试中缺陷分类的领域自适应

    GANs and alternative methods of synthetic noise generation for domain adaption of defect classification of Non-destructive ultrasonic testing. (arXiv:2306.01469v1 [eess.IV])

    [http://arxiv.org/abs/2306.01469](http://arxiv.org/abs/2306.01469)

    本文针对无损测试领域数据训练不足的问题，提出了四种基于半解析模拟数据的合成数据生成方法，并通过超参数优化，使用卷积神经网络进行实验数据分类。其中第一种方法修改了CycleGAN，从物理模拟到实验数据进行了映射。

    

    本文提出了解决复合材料组件无损超声测试中数据训练不足的挑战的方案。实验表明，直接的模拟方法由于噪声重构差而无法产生代表实验领域的训练数据。因此，提出了四种基于半解析模拟数据的合成数据生成方法，每个方法都在卷积神经网络上进行了评估和超参数优化。第一种方法特别修改了CycleGAN以从基于物理的缺陷模拟到实验缺陷的映射。第二种方法则基于将真实实验无缺陷图片与模拟的缺陷响应相结合。最后两种方法是完全的数据合成方法。

    This work provides a solution to the challenge of small amounts of training data in Non-Destructive Ultrasonic Testing for composite components. It was demonstrated that direct simulation alone is ineffective at producing training data that was representative of the experimental domain due to poor noise reconstruction. Therefore, four unique synthetic data generation methods were proposed which use semi-analytical simulated data as a foundation. Each method was evaluated on its classification performance of real experimental images when trained on a Convolutional Neural Network which underwent hyperparameter optimization using a genetic algorithm. The first method introduced task specific modifications to CycleGAN, to learn the mapping from physics-based simulations of defect indications to experimental indications in resulting ultrasound images. The second method was based on combining real experimental defect free images with simulated defect responses. The final two methods fully si
    
[^49]: 存在抑制变量时 XAI 方法的理论行为研究

    Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables. (arXiv:2306.01464v1 [cs.LG])

    [http://arxiv.org/abs/2306.01464](http://arxiv.org/abs/2306.01464)

    本论文综合研究表明，XAI 方法在存在抑制变量时解释可能出现误导性，需要进行更加理论化和经验化的研究，确保其应用的正确性和可能性。

    

    近年来，“可解释人工智能”（XAI）社区已经创建了一个大量的方法来弥合模型“复杂度”和“可解释性”之间的差距。然而，XAI 方法需要解决的具体问题尚未得到正式说明。因此，XAI 方法缺乏理论和实证证据，以验证其解释的“正确性”，限制了其用于质量控制和透明度目的的潜力。同时，Haufe等人（2014）使用简单的玩具例子展示了即使是线性模型的标准解释也可能极具误导性。具体而言，可能会被归因于所谓的抑制变量，这些变量与预测目标缺乏任何统计关系。Wilming等人（2022）已经经验证了这种行为在大量 XAI 方法中的实证研究。在这里，我们进一步推导了多种流行的 XAI 方法在简单的 toy dataset 上的行为的解析表达式。

    In recent years, the community of 'explainable artificial intelligence' (XAI) has created a vast body of methods to bridge a perceived gap between model 'complexity' and 'interpretability'. However, a concrete problem to be solved by XAI methods has not yet been formally stated. As a result, XAI methods are lacking theoretical and empirical evidence for the 'correctness' of their explanations, limiting their potential use for quality-control and transparency purposes. At the same time, Haufe et al. (2014) showed, using simple toy examples, that even standard interpretations of linear models can be highly misleading. Specifically, high importance may be attributed to so-called suppressor variables lacking any statistical relation to the prediction target. This behavior has been confirmed empirically for a large array of XAI methods in Wilming et al. (2022). Here, we go one step further by deriving analytical expressions for the behavior of a variety of popular XAI methods on a simple tw
    
[^50]: ReLU拯救：用正数优势改进您的On-Policy Actor-Critic算法

    ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages. (arXiv:2306.01460v1 [cs.LG])

    [http://arxiv.org/abs/2306.01460](http://arxiv.org/abs/2306.01460)

    本研究提出了一种新的On-Policy深度强化学习算法，该算法通过在保守值估计和谨慎探索方面的明确整合来解决了当前算法不能充分考虑谨慎交互的问题。

    

    本文介绍了一种增强On-Policy深度强化学习（DRL）算法效果的新方法。我们的方法通过在两个关键方面明确地整合谨慎的环境交互来解决当前On-Policy算法（如Proximal Policy Optimization和Asynchronous Advantage Actor-Critic）不能充分考虑谨慎交互的问题：通过最大化真实价值函数加上常量的下界，从而促进“保守值估计”，并通过引入Thompson采样来进行谨慎探索。这些特点通过对A3C算法进行三个惊人简单的修改实现：通过ReLU函数处理优势估计，进行谱归一化和随机失活。我们提供了理论证明，证明了我们的算法最大化了下界，这也是多智能体情况下Regret Matching Policy Gradients（RMPG）的基础。

    In this paper, we introduce a novel method for enhancing the effectiveness of on-policy Deep Reinforcement Learning (DRL) algorithms. Current on-policy algorithms, such as Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C), do not sufficiently account for cautious interaction with the environment. Our method addresses this gap by explicitly integrating cautious interaction in two critical ways: by maximizing a lower-bound on the true value function plus a constant, thereby promoting a \textit{conservative value estimation}, and by incorporating Thompson sampling for cautious exploration. These features are realized through three surprisingly simple modifications to the A3C algorithm: processing advantage estimates through a ReLU function, spectral normalization, and dropout. We provide theoretical proof that our algorithm maximizes the lower bound, which also grounds Regret Matching Policy Gradients (RMPG), a discrete-action on-policy method for multi-agen
    
[^51]: 将上下文纳入文本-文本隐私保护中

    Driving Context into Text-to-Text Privatization. (arXiv:2306.01457v1 [cs.CL])

    [http://arxiv.org/abs/2306.01457](http://arxiv.org/abs/2306.01457)

    本文介绍了一种度量差分隐私机制，该机制在注入噪声之前加入了一个语义消歧步骤以提高歧义单词替代的有效性，并展示了在“上下文中的单词”数据集上的有效性。

    

    “度量差分隐私”通过向从嵌入空间中导出的单词向量添加经过校准的噪声，并使用最近邻搜索将这些有噪声的向量投影回离散词汇，从而实现了文本到文本的隐私保护功能。由于单词是没有上下文的替代，因此该机制可能无法找到具有歧义含义的单词的替代词，例如“bank”。为了解决这些模棱两可的单词，我们利用了一种语义嵌入并在注入噪声之前加入了一个语义消歧步骤。我们还估计了隐私和实用性对隐私保护机制的修改。在“上下文中的单词”数据集上进行的单词义消歧试验中，我们证明了分类准确度的显著增加，达到了$6.05％$。

    \textit{Metric Differential Privacy} enables text-to-text privatization by adding calibrated noise to the vector of a word derived from an embedding space and projecting this noisy vector back to a discrete vocabulary using a nearest neighbor search. Since words are substituted without context, this mechanism is expected to fall short at finding substitutes for words with ambiguous meanings, such as \textit{'bank'}. To account for these ambiguous words, we leverage a sense embedding and incorporate a sense disambiguation step prior to noise injection. We encompass our modification to the privatization mechanism with an estimation of privacy and utility. For word sense disambiguation on the \textit{Words in Context} dataset, we demonstrate a substantial increase in classification accuracy by $6.05\%$.
    
[^52]: 无监督的多词表达改写方法研究

    Unsupervised Paraphrasing of Multiword Expressions. (arXiv:2306.01443v1 [cs.CL])

    [http://arxiv.org/abs/2306.01443](http://arxiv.org/abs/2306.01443)

    本文提出了一种无监督的多词表达式改写方法，使用的数据和工具非常简单，且实验结果表明，该方法在熟语语义文本相似度任务中性能良好。

    

    我们提出了一种无监督的改写多词表达式（MWEs）的方法，只使用单语料库数据和预训练语言模型（无需微调），不使用任何外部资源，如字典。我们在SemEval 2022熟语语义文本相似度任务上评估了我们的方法，并表明它优于所有无监督系统并与有监督系统相匹敌。

    We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems.
    
[^53]: 通过建模残差多模态实现鲁棒的FastSpeech 2

    Towards Robust FastSpeech 2 by Modelling Residual Multimodality. (arXiv:2306.01442v1 [cs.SD])

    [http://arxiv.org/abs/2306.01442](http://arxiv.org/abs/2306.01442)

    该论文提出了一种名为TVC-GMM的三元链式高斯分布混合模型，用于解决FastSpeech 2合成表现性语音数据集时可能出现的Mel频谱平滑度差的问题，可以提高音频的听觉质量。

    

    基于FastSpeech 2的非自回归语音合成模型可以高效地合成高保真度和自然度的语音，但对于表现性语音数据集，我们观察到了特征音频失真。我们证明，这些伪影是由于过度平滑的Mel频谱预测引入的，而这是由于使用均方误差（MSE）损失来训练Mel频谱解码器所致。FastSpeech 2使用MSE损失被限制为学习训练分布的条件平均值，如果所有的调制信号后分布仍然呈现多模态分布，则这些值可能与自然样本并不接近。为了缓解这个问题，我们引入了TVC-GMM，这是一种三元链式高斯分布混合模型，用于模拟残差多模态。TVC-GMM降低了频谱的平滑性，并通过客观和主观评估证明了对表现性数据集特别有效，从而提高了听觉音频质量。

    State-of-the-art non-autoregressive text-to-speech (TTS) models based on FastSpeech 2 can efficiently synthesise high-fidelity and natural speech. For expressive speech datasets however, we observe characteristic audio distortions. We demonstrate that such artefacts are introduced to the vocoder reconstruction by over-smooth mel-spectrogram predictions, which are induced by the choice of mean-squared-error (MSE) loss for training the mel-spectrogram decoder. With MSE loss FastSpeech 2 is limited to learn conditional averages of the training distribution, which might not lie close to a natural sample if the distribution still appears multimodal after all conditioning signals. To alleviate this problem, we introduce TVC-GMM, a mixture model of Trivariate-Chain Gaussian distributions, to model the residual multimodality. TVC-GMM reduces spectrogram smoothness and improves perceptual audio quality in particular for expressive datasets as shown by both objective and subjective evaluation.
    
[^54]: 通过神经引导符号抽象实现可解释和可解释逻辑策略

    Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])

    [http://arxiv.org/abs/2306.01439](http://arxiv.org/abs/2306.01439)

    该论文介绍了一种名为NUDGE的策略，利用训练好的基于神经网络的代理来引导逻辑规则的搜索，实现了可解释和可解释的策略。

    

    神经网络所需要的有限先验使其成为使用强化学习（RL）编码和学习策略的主要选择。然而，它们也是黑匣子，在工作在图像级别时难以理解代理行为。因此，神经符号RL旨在首先创建可解释的策略。不幸的是，可解释性不意味着可解释性。为了实现解释性和可解释性，我们引入了神经引导可微分逻辑策略（NUDGE）。NUDGE利用训练好的基于神经网络的代理来引导候选加权逻辑规则的搜索，然后使用可微分的逻辑来训练逻辑代理。我们的实验评估表明，NUDGE代理可以产生可解释和可解释的策略，同时胜过纯神经代理，并展现出良好的灵活性，以适应不同初始状态和问题大小的环境。

    The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
    
[^55]: 多目标基因群体训练

    Multi-Objective Population Based Training. (arXiv:2306.01436v1 [cs.LG])

    [http://arxiv.org/abs/2306.01436](http://arxiv.org/abs/2306.01436)

    作者提出了一种多目标版基因群体训练算法MO-PBT，实验证明其在多种冲突目标下均优于其他算法。

    

    基因群体训练（PBT）是一种有效的超参数优化算法。PBT是一种单目标算法，然而，许多现实世界的超参数优化问题涉及两个或更多冲突的目标。在本文中，我们引入了一种多目标版的PBT（MO-PBT）。我们在多样化的多目标超参数优化问题（精确度/召回率、准确度/公平性、准确度/对抗性鲁棒性）上的实验表明，MO-PBT优于随机搜索、单目标PBT和最先进的多目标超参数优化算法MO-ASHA。

    Population Based Training (PBT) is an efficient hyperparameter optimization algorithm. PBT is a single-objective algorithm, but many real-world hyperparameter optimization problems involve two or more conflicting objectives. In this work, we therefore introduce a multi-objective version of PBT, MO-PBT. Our experiments on diverse multi-objective hyperparameter optimization problems (Precision/Recall, Accuracy/Fairness, Accuracy/Adversarial Robustness) show that MO-PBT outperforms random search, single-objective PBT, and the state-of-the-art multi-objective hyperparameter optimization algorithm MO-ASHA.
    
[^56]: 通过神经动力学的显式规定来提高DEQ模型的对抗鲁棒性

    Improving Adversarial Robustness of DEQs with Explicit Regulations Along the Neural Dynamics. (arXiv:2306.01435v1 [cs.LG])

    [http://arxiv.org/abs/2306.01435](http://arxiv.org/abs/2306.01435)

    本论文通过神经动力学的解释，提出了一种新的对抗训练框架，通过逐步更新输入来降低预测熵，从而提高DEQ模型的对抗性。

    

    深度平衡（ DEQ ）模型将传统深层网络的多层堆叠替换为单层变换的不动点迭代。已经证明在各种实际应用场景中 DEQ 模型具有竞争优势，因此一般 DEQ 模型的对抗鲁棒性变得越来越重要。现有的工作通过广泛使用的对抗训练（ AT）框架来提高一般 DEQ 模型的鲁棒性，但它们未能利用 DEQ 模型的结构独特性。为此，我们通过神经动力学的视角解释 DEQs，并发现 AT 对中间状态进行了不充分的规定。此外，中间状态通常提供具有高预测熵的预测。受动态系统熵与其稳定性质之间关联的启发，我们提出通过沿着神经动力学逐步更新输入来降低预测熵。在 AT 过程中，我们还利用随机中间状态t

    Deep equilibrium (DEQ) models replace the multiple-layer stacking of conventional deep networks with a fixed-point iteration of a single-layer transformation. Having been demonstrated to be competitive in a variety of real-world scenarios, the adversarial robustness of general DEQs becomes increasingly crucial for their reliable deployment. Existing works improve the robustness of general DEQ models with the widely-used adversarial training (AT) framework, but they fail to exploit the structural uniquenesses of DEQ models. To this end, we interpret DEQs through the lens of neural dynamics and find that AT under-regulates intermediate states. Besides, the intermediate states typically provide predictions with a high prediction entropy. Informed by the correlation between the entropy of dynamical systems and their stability properties, we propose reducing prediction entropy by progressively updating inputs along the neural dynamics. During AT, we also utilize random intermediate states t
    
[^57]: 零样本盲音频带宽扩展

    Zero-Shot Blind Audio Bandwidth Extension. (arXiv:2306.01433v1 [eess.AS])

    [http://arxiv.org/abs/2306.01433](http://arxiv.org/abs/2306.01433)

    本文提出了一种名为BABE的新方法，在零样本情况下解决了具有挑战性的盲音频带宽扩展问题，实现了比最先进的盲带宽扩展基线更好的性能，并且表现出了强大的泛化能力。

    

    音频带宽扩展涉及从带宽受限的观测信号中实现高频谱的逼真重建。在低通信号退化未知的情况下，比如对历史音频记录的恢复，这便成了一个盲问题。本文提出了一种名为BABE的新方法（Blind Audio Bandwidth Extension），在零样本情况下解决了盲问题，利用了预训练的无条件扩散模型的生成先验。在推断过程中，BABE利用了一个广义版本的扩散后验采样，在其中退化算子是未知的，但是通过迭代进行参数化和推断。提出的方法的性能是用客观和主观指标评估的，结果表明BABE超过了最先进的盲带宽扩展基线，并且在测试合成数据时与非盲滤波器知情方法相比实现了有竞争力的性能。此外，BABE表现出了强大的泛化能力。

    Audio bandwidth extension involves the realistic reconstruction of high-frequency spectra from bandlimited observations. In cases where the lowpass degradation is unknown, such as in restoring historical audio recordings, this becomes a blind problem. This paper introduces a novel method called BABE (Blind Audio Bandwidth Extension) that addresses the blind problem in a zero-shot setting, leveraging the generative priors of a pre-trained unconditional diffusion model. During the inference process, BABE utilizes a generalized version of diffusion posterior sampling, where the degradation operator is unknown but parametrized and inferred iteratively. The performance of the proposed method is evaluated using objective and subjective metrics, and the results show that BABE surpasses state-of-the-art blind bandwidth extension baselines and achieves competitive performance compared to non-blind filter-informed methods when tested with synthetic data. Moreover, BABE exhibits robust generaliza
    
[^58]: 基于得分的生成模型的音视频语音增强

    Audio-Visual Speech Enhancement with Score-Based Generative Models. (arXiv:2306.01432v1 [eess.AS])

    [http://arxiv.org/abs/2306.01432](http://arxiv.org/abs/2306.01432)

    本文介绍了一种利用基于得分的生成模型，以视觉信息为条件的音视频语音增强系统。实验证明该系统提高了语音质量，并减少了语音混淆。

    

    本文介绍了一种利用基于得分的生成模型进行音视频语音增强的系统，该模型以视觉信息为条件。我们利用了一个自我监督学习模型从中获得的音视频嵌入，该模型已经在口型识别上进行了微调。它的基于变压器的编码器的逐层特征被聚合、时间对齐并融合到噪声条件分值网络中。实验评估表明，所提出的音视频语音增强系统在语音质量方面具有改进作用，并减少了与仅音频等效的生成画面中的语音混淆。这得到了下游自动语音识别模型的单词错误率的支持，特别是在低信噪比输入时，单词错误率明显降低。

    This paper introduces an audio-visual speech enhancement system that leverages score-based generative models, also known as diffusion models, conditioned on visual information. In particular, we exploit audio-visual embeddings obtained from a self-super\-vised learning model that has been fine-tuned on lipreading. The layer-wise features of its transformer-based encoder are aggregated, time-aligned, and incorporated into the noise conditional score network. Experimental evaluations show that the proposed audio-visual speech enhancement system yields improved speech quality and reduces generative artifacts such as phonetic confusions with respect to the audio-only equivalent. The latter is supported by the word error rate of a downstream automatic speech recognition model, which decreases noticeably, especially at low input signal-to-noise ratios.
    
[^59]: 关于联邦学习中的知识编辑：展望，挑战和未来方向的透视

    On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions. (arXiv:2306.01431v1 [cs.LG])

    [http://arxiv.org/abs/2306.01431](http://arxiv.org/abs/2306.01431)

    本文介绍了一份关于联邦学习中知识编辑的广泛调查，总结了最新技术，确定了挑战和机遇，并概述了未来的研究方向。

    

    随着联邦学习（FL）引起越来越多的关注，人们普遍认为在学习一系列任务时，在整个框架上简单地应用随机梯度下降（SGD）会导致所谓的“灾难性遗忘”。因此，许多FL研究集中于设计联邦增量学习方法，以减轻遗忘并增加知识。另一方面，遗忘并不总是有害的。选择性遗忘，也称为联邦遗忘，包括消除特定知识，可以解决隐私问题并为获取新知识创造额外的“空间”。然而，缺乏广泛调查涵盖最新进展并对此问题进行全面检查。在本文中，我们介绍了一份关于联邦学习中知识编辑（增强/删除）的广泛调查，旨在总结最新技术，确定挑战和机遇，并概述未来的研究方向。

    As Federated Learning (FL) has gained increasing attention, it has become widely acknowledged that straightforwardly applying stochastic gradient descent (SGD) on the overall framework when learning over a sequence of tasks results in the phenomenon known as ``catastrophic forgetting''. Consequently, much FL research has centered on devising federated increasing learning methods to alleviate forgetting while augmenting knowledge. On the other hand, forgetting is not always detrimental. The selective amnesia, also known as federated unlearning, which entails the elimination of specific knowledge, can address privacy concerns and create additional ``space'' for acquiring new knowledge. However, there is a scarcity of extensive surveys that encompass recent advancements and provide a thorough examination of this issue. In this manuscript, we present an extensive survey on the topic of knowledge editing (augmentation/removal) in Federated Learning, with the goal of summarizing the state-of
    
[^60]: 深度平衡模型的对抗鲁棒性再研究

    A Closer Look at the Adversarial Robustness of Deep Equilibrium Models. (arXiv:2306.01429v1 [cs.LG])

    [http://arxiv.org/abs/2306.01429](http://arxiv.org/abs/2306.01429)

    本研究探究了深度平衡模型的对抗鲁棒性问题，提出了估计中间梯度以改进攻击流程的方法。

    

    深度平衡模型（DEQ）摒弃了传统的层叠方法，转而寻找单一层的固定点。DEQ在不同应用中表现出卓越的性能和特征的内存效率。同时，DEQ的对抗性漏洞引起了人们的关注。一些工作提出了对单调DEQ进行鲁棒性验证的方法。然而，目前对于一般DEQ的经验鲁棒性的研究还很有限。为此，我们观察到，对抗训练的DEQ需要更多的前向步骤才能达到平衡状态，甚至可能违反其固定点结构。此外，黑盒求解器导致DEQ的前向和后向轨迹不对齐。这些事实在评估或对抗性训练DEQ时会导致梯度混淆。因此，我们开发了一些方法来估计DEQ的中间梯度，并将其集成到攻击流程中。我们的方法有助于充分评估DEQ的鲁棒性。

    Deep equilibrium models (DEQs) refrain from the traditional layer-stacking paradigm and turn to find the fixed point of a single layer. DEQs have achieved promising performance on different applications with featured memory efficiency. At the same time, the adversarial vulnerability of DEQs raises concerns. Several works propose to certify robustness for monotone DEQs. However, limited efforts are devoted to studying empirical robustness for general DEQs. To this end, we observe that an adversarially trained DEQ requires more forward steps to arrive at the equilibrium state, or even violates its fixed-point structure. Besides, the forward and backward tracks of DEQs are misaligned due to the black-box solvers. These facts cause gradient obfuscation when applying the ready-made attacks to evaluate or adversarially train DEQs. Given this, we develop approaches to estimate the intermediate gradients of DEQs and integrate them into the attacking pipelines. Our approaches facilitate fully w
    
[^61]: 使用Whisper特征来提高DeepFake检测效果

    Improved DeepFake Detection Using Whisper Features. (arXiv:2306.01428v1 [cs.SD])

    [http://arxiv.org/abs/2306.01428](http://arxiv.org/abs/2306.01428)

    该论文探究使用Whiper模型作为DF检测的前端的影响，通过对比训练3个检测模型后在广泛使用的ASVspoof 2021 DF数据集上的效果，在DF In-The-Wild数据集上测试后表明，使用基于Whisper的特征能够提高每个模型的检测效果，并将等误差率降低了21％，从而超越了最近在In-The-Wild数据集上获得的结果。

    

    随着人声生成方法的不断涌现，音频DeepFake（DF）所带来的威胁正在不断增加。许多检测方法都基于所谓的前端技术，通过转换原始音频，强调评估音频样本真实性所必需的特征。我们的贡献在于将最先进的Whisper自动语音识别模型作为DF检测的前端，并比较不同组合的Whisper和已建立的前端，通过在广泛使用的ASVspoof 2021 DF数据集上训练3个检测模型（LCNN，SpecRNet和MesoNet），并在DF In-The-Wild数据集上进行后续评估。我们表明，使用基于Whisper的特征可以改进每个模型的检测效果，并通过将等误差率降低21％来超越最近在In-The-Wild数据集上获得的结果。

    With a recent influx of voice generation methods, the threat introduced by audio DeepFake (DF) is ever-increasing. Several different detection methods have been presented as a countermeasure. Many methods are based on so-called front-ends, which, by transforming the raw audio, emphasize features crucial for assessing the genuineness of the audio sample. Our contribution contains investigating the influence of the state-of-the-art Whisper automatic speech recognition model as a DF detection front-end. We compare various combinations of Whisper and well-established front-ends by training 3 detection models (LCNN, SpecRNet, and MesoNet) on a widely used ASVspoof 2021 DF dataset and later evaluating them on the DF In-The-Wild dataset. We show that using Whisper-based features improves the detection for each model and outperforms recent results on the In-The-Wild dataset by reducing Equal Error Rate by 21%.
    
[^62]: 带曲率敏感模型的连续性结果的部分反事实识别

    Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. (arXiv:2306.01424v1 [stat.ML])

    [http://arxiv.org/abs/2306.01424](http://arxiv.org/abs/2306.01424)

    本文研究了连续性结果的部分反事实识别问题，并提出了一种新颖的敏感性模型——曲率敏感模型，通过限制函数级集的曲率来获得信息边界。

    

    反事实推断旨在回答“如果”问题，因此属于Pearl因果关系阶梯中最精细的推理类型。现有的针对具有连续结果的反事实推断方法旨在进行点识别，因此对基础结构因果模型进行了强有力且不自然的假设。在本文中，我们放宽了这些假设，旨在进行连续结果的部分反事实识别，即当反事实查询存在具有信息边界的无知区间中时。我们证明了，在一般情况下，即使是连续可微的结构因果模型函数的级集的曲率也是非信息的，反事实查询的无知区间也是非信息的。因此，我们提出了一种新颖的敏感性模型称为曲率敏感模型来解决这个问题。它允许我们通过限制函数级集的曲率来获得信息边界。我们进一步展示了现有的点反事实识别方法可以视为我们提出框架的特定情况。

    Counterfactual inference aims to answer retrospective ''what if'' questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual ide
    
[^63]: 利用三重指数移动平均值实现快速自适应矩估计

    Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation. (arXiv:2306.01423v1 [cs.CV])

    [http://arxiv.org/abs/2306.01423](http://arxiv.org/abs/2306.01423)

    本文提出了一种新的深度优化器FAME，使用三重指数移动平均值（TEMA）来估计梯度矩，提供更丰富和准确的数据变化和趋势信息，可以提高计算机视觉等领域中模型的性能表现。

    

    网络优化是深度学习领域中的一个关键步骤，直接影响计算机视觉等多种领域中模型的性能。虽然多种优化器已经被开发出来，但目前的方法在准确快速地识别梯度趋势方面仍然有限，这可能会导致网络性能不佳。本文提出了一种新的深度优化器，称为快速自适应矩估计（FAME），它首次使用三重指数移动平均值（TEMA）来估计梯度矩。将TEMA纳入优化过程中，可以提供更丰富和准确的数据变化和趋势信息，与目前所有主要自适应优化方法中使用的标准指数移动平均值相比。我们提出的FAME优化器已经在广泛的基准测试中得到了验证，包括CIFAR-10，CIFAR-100，PASCAL-VOC，MS-COCO和Cityscapes。

    Network optimization is a crucial step in the field of deep learning, as it directly affects the performance of models in various domains such as computer vision. Despite the numerous optimizers that have been developed over the years, the current methods are still limited in their ability to accurately and quickly identify gradient trends, which can lead to sub-optimal network performance. In this paper, we propose a novel deep optimizer called Fast-Adaptive Moment Estimation (FAME), which for the first time estimates gradient moments using a Triple Exponential Moving Average (TEMA). Incorporating TEMA into the optimization process provides richer and more accurate information on data changes and trends, as compared to the standard Exponential Moving Average used in essentially all current leading adaptive optimization methods. Our proposed FAME optimizer has been extensively validated through a wide range of benchmarks, including CIFAR-10, CIFAR-100, PASCAL-VOC, MS-COCO, and Cityscap
    
[^64]: 公平机器学习的基础存在缺陷

    The Flawed Foundations of Fair Machine Learning. (arXiv:2306.01417v1 [cs.CY])

    [http://arxiv.org/abs/2306.01417](http://arxiv.org/abs/2306.01417)

    该论文对公平机器学习领域的现在存在的缺陷进行探讨，指出了研究者未能正确理解在实现公平性时，准确率和组相似性之间的权衡关系，该权衡关系可能对公平性构成实质性威胁。

    

    研究人员广泛研究了自动化决策中公平性的定义和实施。然而，当前公平机器学习范例的基础存在着错误的推理、误导性的断言和可疑的实践。这些缺陷是由于没有理解在存在统计上准确的结果和组相似的结果之间的权衡是一种独立的外部限制而非主观彰显的结果所导致的。首先，我们解释了在公平机器学习文献中只存在一种公平的概念：基于敏感属性的结果组相似性，其中相似性有益于一个弱势组。其次，我们展示在任何存在组差异的数据情况下，统计上准确的结果和组相似的结果之间确实存在权衡，而这种权衡对公平存在着重大威胁。

    The definition and implementation of fairness in automated decisions has been extensively studied by the research community. Yet, there hides fallacious reasoning, misleading assertions, and questionable practices at the foundations of the current fair machine learning paradigm. Those flaws are the result of a failure to understand that the trade-off between statistically accurate outcomes and group similar outcomes exists as independent, external constraint rather than as a subjective manifestation as has been commonly argued. First, we explain that there is only one conception of fairness present in the fair machine learning literature: group similarity of outcomes based on a sensitive attribute where the similarity benefits an underprivileged group. Second, we show that there is, in fact, a trade-off between statistically accurate outcomes and group similar outcomes in any data setting where group disparities exist, and that the trade-off presents an existential threat to the equita
    
[^65]: 化学性质导向神经网络用于石脑油成分的预测。

    Chemical Property-Guided Neural Networks for Naphtha Composition Prediction. (arXiv:2306.01391v1 [cs.LG])

    [http://arxiv.org/abs/2306.01391](http://arxiv.org/abs/2306.01391)

    本研究提出了一种利用化学性质信息将卷积神经网络和多层感知器网络结合的神经网络框架，用于提高石脑油成分预测的性能。

    

    石脑油裂解过程严重依赖于石脑油的成分，而这是一种由不同烃类组成的复杂混合物。准确地预测石脑油成分对于有效控制裂解过程和实现最大化性能至关重要。由于需要小型试验或成本限制，传统的方法，如气相色谱和真沸点曲线分析，在实践中是不可行的。在本文中，我们提出了一种神经网络框架，利用化学性质信息来提高石脑油成分预测的性能。我们的提出框架包括两个部分：Watson K因子估计网络和石脑油成分预测网络。这两个网络共用一个基于卷积神经网络（CNN）架构的特征提取网络，而输出层使用基于多层感知器（MLP）的网络来生成两个不同的输出–Watson K因子和石脑油成分。

    The naphtha cracking process heavily relies on the composition of naphtha, which is a complex blend of different hydrocarbons. Predicting the naphtha composition accurately is crucial for efficiently controlling the cracking process and achieving maximum performance. Traditional methods, such as gas chromatography and true boiling curve, are not feasible due to the need for pilot-plant-scale experiments or cost constraints. In this paper, we propose a neural network framework that utilizes chemical property information to improve the performance of naphtha composition prediction. Our proposed framework comprises two parts: a Watson K factor estimation network and a naphtha composition prediction network. Both networks share a feature extraction network based on Convolutional Neural Network (CNN) architecture, while the output layers use Multi-Layer Perceptron (MLP) based networks to generate two different outputs - Watson K factor and naphtha composition. The naphtha composition is exp
    
[^66]: 自适应消息量化和并行化在分布式全图GNN训练中的应用

    Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training. (arXiv:2306.01381v1 [cs.LG])

    [http://arxiv.org/abs/2306.01381](http://arxiv.org/abs/2306.01381)

    本文提出了一种高效的GNN训练系统AdaQP，通过随机量化跨设备传输的消息以降低通信流量，并提倡边缘节点和中心节点之间的通信-计算并行化，以加快分布式全图GNN训练速度。

    

    大规模图网络神经网络（GNNs）的分布式全图训练具有带宽需求高和耗时长的特点。跨设备频繁交换节点特征、嵌入和嵌入梯度（均称为消息）带来了显著的通信开销，对于在其他设备上具有远程邻居的节点（边缘节点）而言，而对于没有远程邻居的节点（中心节点）而言则带来了不必要的等待时间。本文提出了一种高效的GNN训练系统AdaQP，通过随机量化跨设备传输的消息以降低通信流量，并提倡边缘节点和中心节点之间的通信-计算并行化，以加快分布式全图GNN训练速度。我们提供了理论分析，证明了快速训练收敛（以O（T ^ {-1}）的速率，其中T为训练周期的总数），并设计了一种自适应量化位宽分配方案。

    Distributed full-graph training of Graph Neural Networks (GNNs) over large graphs is bandwidth-demanding and time-consuming. Frequent exchanges of node features, embeddings and embedding gradients (all referred to as messages) across devices bring significant communication overhead for nodes with remote neighbors on other devices (marginal nodes) and unnecessary waiting time for nodes without remote neighbors (central nodes) in the training graph. This paper proposes an efficient GNN training system, AdaQP, to expedite distributed full-graph GNN training. We stochastically quantize messages transferred across devices to lower-precision integers for communication traffic reduction and advocate communication-computation parallelization between marginal nodes and central nodes. We provide theoretical analysis to prove fast training convergence (at the rate of O(T^{-1}) with T being the total number of training epochs) and design an adaptive quantization bit-width assignment scheme for eac
    
[^67]: 一种强鲁棒性和可推广性的图卷积方法用于细微癫痫病灶的分割

    Robust and Generalisable Segmentation of Subtle Epilepsy-causing Lesions: a Graph Convolutional Approach. (arXiv:2306.01375v1 [eess.IV])

    [http://arxiv.org/abs/2306.01375](http://arxiv.org/abs/2306.01375)

    该论文提出了一种通过图卷积网络进行语义分割的方法，能够对细微的癫痫病灶进行识别，降低误报率。

    

    该论文提出了使用图卷积网络（GCN）进行语义分割的方法，以解决细微的局限性皮质发育不良（FCD）病变的识别问题。研究人员通过添加辅助损失和弱监督分类损失的方式，使模型能够学习到更多的空间关系，从而降低误报率。该方法在多个中心的数据集上进行了测试。

    Focal cortical dysplasia (FCD) is a leading cause of drug-resistant focal epilepsy, which can be cured by surgery. These lesions are extremely subtle and often missed even by expert neuroradiologists. "Ground truth" manual lesion masks are therefore expensive, limited and have large inter-rater variability. Existing FCD detection methods are limited by high numbers of false positive predictions, primarily due to vertex- or patch-based approaches that lack whole-brain context. Here, we propose to approach the problem as semantic segmentation using graph convolutional networks (GCN), which allows our model to learn spatial relationships between brain regions. To address the specific challenges of FCD identification, our proposed model includes an auxiliary loss to predict distance from the lesion to reduce false positives and a weak supervision classification loss to facilitate learning from uncertain lesion masks. On a multi-centre dataset of 1015 participants with surface-based feature
    
[^68]: DWT-CompCNN：用于高吞吐量JPEG 2000压缩文档的深度图像分类网络

    DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents. (arXiv:2306.01359v1 [cs.CV])

    [http://arxiv.org/abs/2306.01359](http://arxiv.org/abs/2306.01359)

    这篇论文提出了一种名为DWT-CompCNN的深度学习模型，它可以直接对使用HTJ2K算法压缩的文档进行分类，从而提高计算效率。

    

    对于任何包含文档图像的数字应用程序，如检索，文档图像的分类成为必要的阶段。传统上，为了达到这个目的，文档的完整版本，即未压缩的文档图像构成输入数据集，这会因数据量大而带来威胁。因此，如果可以使用文档的压缩表示（在部分解压缩的情况下），直接完成相同的分类任务以使整个过程计算效率更高，那将会是一项创新。本研究提出了一种新颖的深度学习模型DWT-CompCNN，用于使用高吞吐量JPEG 2000（HTJ2K）算法压缩的文档的分类。所提出的DWT-CompCNN包括五个卷积层，卷积核大小分别为16、32、64、128和256用于从提取的小波系数中提高学习能力。

    For any digital application with document images such as retrieval, the classification of document images becomes an essential stage. Conventionally for the purpose, the full versions of the documents, that is the uncompressed document images make the input dataset, which poses a threat due to the big volume required to accommodate the full versions of the documents. Therefore, it would be novel, if the same classification task could be accomplished directly (with some partial decompression) with the compressed representation of documents in order to make the whole process computationally more efficient. In this research work, a novel deep learning model, DWT CompCNN is proposed for classification of documents that are compressed using High Throughput JPEG 2000 (HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional layers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each increasing layer to improve learning from the wavelet coefficients extracted
    
[^69]: 基于污染攻击的联邦学习隐蔽通信研究

    Covert Communication Based on the Poisoning Attack in Federated Learning. (arXiv:2306.01342v1 [cs.LG])

    [http://arxiv.org/abs/2306.01342](http://arxiv.org/abs/2306.01342)

    本论文提出了一种基于污染攻击的联邦学习隐蔽通信方法，能够在保证隐蔽性和稳健性的同时，在两个客户端之间实现100%的隐蔽消息传输准确性，这在现有防御方法存在限制的情况下强调了联邦学习中隐蔽通信的潜在威胁和对应的安全挑战。

    

    隐蔽通信已成为计算机安全领域的重要研究方向。它涉及将特定信息隐藏在载体中进行传输，通常用于传输私人数据、军事机密甚至恶意软件。在深度学习中，已经开发出许多方法来隐藏模型中的信息以实现隐蔽通信。然而，这些方法不适用于联邦学习，因为模型聚合会使客户端嵌入的确切信息失效。为了解决这个问题，我们提出了一种基于污染攻击的联邦学习隐蔽通信方法。我们的方法在两个客户端之间实现了100％的隐蔽消息传输准确性，并经过了大量实验的验证，表现出隐蔽性和稳健性。然而，现有的防御方法对我们的攻击方案的有效性受到限制，凸显了需要开发新的防护方法的紧迫性。我们的研究强调了在联邦学习中隐蔽通信的潜在威胁和相关的安全挑战。

    Covert communication has become an important area of research in computer security. It involves hiding specific information on a carrier for message transmission and is often used to transmit private data, military secrets, and even malware. In deep learning, many methods have been developed for hiding information in models to achieve covert communication. However, these methods are not applicable to federated learning, where model aggregation invalidates the exact information embedded in the model by the client. To address this problem, we propose a novel method for covert communication in federated learning based on the poisoning attack. Our approach achieves 100% accuracy in covert message transmission between two clients and is shown to be both stealthy and robust through extensive experiments. However, existing defense methods are limited in their effectiveness against our attack scheme, highlighting the urgent need for new protection methods to be developed. Our study emphasizes 
    
[^70]: 资源高效的联合超维计算

    Resource-Efficient Federated Hyperdimensional Computing. (arXiv:2306.01339v1 [cs.LG])

    [http://arxiv.org/abs/2306.01339](http://arxiv.org/abs/2306.01339)

    提出了一种资源高效的联合超维计算框架，通过训练多个小的HDC子模型并使用改进后的dropout流程，实现可与大模型相当的预测性能，消耗更少的计算和无线资源。

    

    在传统的联合超维计算（HDC）中，训练更大的模型通常会带来更高的预测性能，但也需要更多的计算、通信和能源资源。如果系统资源有限，人们可能不得不通过减小HDC模型大小来牺牲预测性能。提出的资源高效的联合超维计算（RE-FHDC）框架通过训练多个更小的独立的HDC子模型，并使用所提出的类似于dropout的过程精炼合并的HDC模型来缓解这种约束。我们的数值比较表明，所提出的框架在消耗比基线联合HDC实现更少的计算和无线资源的同时，实现了可比或更高的预测性能。

    In conventional federated hyperdimensional computing (HDC), training larger models usually results in higher predictive performance but also requires more computational, communication, and energy resources. If the system resources are limited, one may have to sacrifice the predictive performance by reducing the size of the HDC model. The proposed resource-efficient federated hyperdimensional computing (RE-FHDC) framework alleviates such constraints by training multiple smaller independent HDC sub-models and refining the concatenated HDC model using the proposed dropout-inspired procedure. Our numerical comparison demonstrates that the proposed framework achieves a comparable or higher predictive performance while consuming less computational and wireless resources than the baseline federated HDC implementation.
    
[^71]: 联邦领域泛化：一项调查研究

    Federated Domain Generalization: A Survey. (arXiv:2306.01334v1 [cs.LG])

    [http://arxiv.org/abs/2306.01334](http://arxiv.org/abs/2306.01334)

    该论文调查了联邦领域泛化的研究领域，提到了联邦学习和领域泛化技术的优势，以及在泛化联邦模型时面临的挑战。

    

    机器学习通常依赖于一个假设，即训练和测试的分布是相同的，数据是集中存储供训练和测试之用。然而，在现实场景中，分布可能存在显著差异，并且数据通常分布在不同的设备、组织或边缘节点上。因此，必须开发能够有效泛化到未见过的分布，并且数据分布在不同领域的模型。为了应对这一挑战，近年来出现了对联邦领域泛化 (FDG) 的极大兴趣。FDG 结合了联邦学习 (FL) 和领域泛化 (DG) 技术的优点，使多个源领域能够协作学习一个能够直接泛化到未见过的领域而又保持数据隐私的模型。然而，在领域转移下泛化联邦模型是一个技术上具有挑战性的问题，目前还没有得到充分的关注。

    Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research 
    
[^72]: 放射学AI中的公平性导航：概念、后果和关键考虑

    Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations. (arXiv:2306.01333v1 [cs.LG])

    [http://arxiv.org/abs/2306.01333](http://arxiv.org/abs/2306.01333)

    本文讨论了放射学AI中的公平性问题，介绍了Aequitas偏差审计工具包并阐明了公平性评估的基本指标，强调了考虑AI伦理影响的重要性。

    

    人工智能（AI）已经在放射学领域取得了重大革命性进展，承诺改善患者结果和简化流程。然而，确保AI模型的公平性至关重要，以防止隐蔽的偏见和不平等现象导致不平等结果。本综述讨论了AI中的公平概念，重点讨论使用Aequitas工具包进行偏差审计及其在放射学中的实际影响，特别是在疾病筛查场景中的影响。Aequitas是一个开源的偏差审计工具包，审查AI模型的决策，识别可能导致不同人口群体和成像设备品牌之间差异的隐藏偏见。该工具包基于统计理论，分析大型数据集以揭示模型的公平性。它在同时处理多种变量方面表现出色，特别是在放射学这样的多元化领域中。本综述阐明了公平性评估的基本指标：平等和比例平等、假阳性率平等、假阴性率平等和预测平等。作者还强调了在放射学AI中考虑伦理影响的重要性，包括对患者造成伤害的风险以及在AI决策过程中需要透明度的必要性。

    Artificial Intelligence (AI) has significantly revolutionized radiology, promising improved patient outcomes and streamlined processes. However, it's critical to ensure the fairness of AI models to prevent stealthy bias and disparities from leading to unequal outcomes. This review discusses the concept of fairness in AI, focusing on bias auditing using the Aequitas toolkit, and its real-world implications in radiology, particularly in disease screening scenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models' decisions, identifying hidden biases that may result in disparities across different demographic groups and imaging equipment brands. This toolkit operates on statistical theories, analyzing a large dataset to reveal a model's fairness. It excels in its versatility to handle various variables simultaneously, especially in a field as diverse as radiology. The review explicates essential fairness metrics: Equal and Proportional Parity, False Positive Rate Parity
    
[^73]: 强化学习中的超参数及其调整方法

    Hyperparameters in Reinforcement Learning and How To Tune Them. (arXiv:2306.01324v1 [cs.LG])

    [http://arxiv.org/abs/2306.01324](http://arxiv.org/abs/2306.01324)

    本文研究了强化学习中的超参数选择问题，提出了采用AutoML中的最佳实践，并展示了采用HPO方法往往具有更高的性能和更低的计算开销。

    

    为了提高可重复性，深度强化学习 (RL) 已经采用了更好的科学实践，如标准化评估指标和报告。然而，RL 中的超参数优化过程仍然存在巨大差异，这使得公平比较 RL 算法变得具有挑战性。本文展示了 RL 中的超参数选择可以显着影响代理的最终性能和采样效率，并且超参数的选择会强烈依赖于调整种子，可能导致过度拟合。因此，我们提出采用 AutoML 中已有的最佳实践，例如分离调整和测试种子，以及在广泛搜索空间内进行合理的超参数优化 (HPO)。通过比较多个最新的 HPO 工具在一系列 RL 算法和环境上与手动调整对比，展示了 HPO 方法往往具有更高的性能和更低的计算开销。

    In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. However, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly. In this paper, we show that hyperparameter choices in RL can significantly affect the agent's final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhe
    
[^74]: 揭示图神经网络中的结构差异性：一个尺码适用于所有吗？

    Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])

    [http://arxiv.org/abs/2306.01323](http://arxiv.org/abs/2306.01323)

    本研究证明GNN在同构图中的同构节点和异构图中的异构节点上表现良好，而在另一组节点上表现不佳。该研究提出了一种新框架，通过使用加权聚合提高GNN在不同结构模式节点上的表现，有效解决结构差异性问题。

    

    近期关于图神经网络（GNN）的研究提供了实证和理论证据，支持它们在捕捉同构和某些异构图上的结构模式方面的有效性。值得注意的是，大多数实际中的同构和异构图都由同构和异构结构模式的混合节点组成，表现出一定的结构差异性。然而，关于不同结构模式下的节点（例如在异构图中的同构节点）在GNN分类任务中的表现分析仍然很有限。本文通过理论和实证研究证明，GNN在同构图中的同构节点和异构图中的异构节点上的表现通常是出色的，而在另一组节点上表现不佳，表现出性能差异性。我们进一步识别了测试展示不同结构模式节点时GNN的效应，并提出了一种通过使用GNN的加权聚合以适应性结构差异性的新框架的解决方案。在各种数据集上的实验表明，所提出的方法在解决结构差异性和提高节点分类任务的性能方面是有效的。

    Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
    
[^75]: 隐私蒸馏：降低多模态扩散模型的重新识别风险。

    Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models. (arXiv:2306.01322v1 [cs.LG])

    [http://arxiv.org/abs/2306.01322](http://arxiv.org/abs/2306.01322)

    本文提出了隐私蒸馏框架，通过过滤具有重新识别风险的图像，训练生成模型，从而减少数据提供者共享多模态生成模型时的隐私泄露风险。

    

    神经网络中的知识蒸馏是指将大型模型或数据集压缩为其自身的较小版本。我们引入了隐私蒸馏，这是一个框架，允许一个文本到图像生成模型教授另一个模型，而不会暴露可识别的数据。本文关注的是通过多模态生成模型共享数据时所面临的隐私问题。一个立即出现的问题是，“数据提供者如何确保生成模型不会泄露与患者有关的可识别信息？”我们的解决方案包括：（1）在真实数据上训练第一扩散模型；（2）使用该模型生成合成数据，并排除具有重新识别风险的图像；（3）仅在过滤后的合成数据上进行第二次扩散模型的训练。我们展示了使用隐私蒸馏训练的数据集可以有效降低重新识别风险，同时保持下游性能。

    Knowledge distillation in neural networks refers to compressing a large model or dataset into a smaller version of itself. We introduce Privacy Distillation, a framework that allows a text-to-image generative model to teach another model without exposing it to identifiable data. Here, we are interested in the privacy issue faced by a data provider who wishes to share their data via a multimodal generative model. A question that immediately arises is ``How can a data provider ensure that the generative model is not leaking identifiable information about a patient?''. Our solution consists of (1) training a first diffusion model on real data (2) generating a synthetic dataset using this model and filtering it to exclude images with a re-identifiability risk (3) training a second diffusion model on the filtered synthetic data only. We showcase that datasets sampled from models trained with privacy distillation can effectively reduce re-identification risk whilst maintaining downstream per
    
[^76]: 文本风格转化的Back-Translation技术

    Text Style Transfer Back-Translation. (arXiv:2306.01318v1 [cs.CL])

    [http://arxiv.org/abs/2306.01318](http://arxiv.org/abs/2306.01318)

    本研究提出了文本风格转化的Back-Translation技术（TST BT），通过使用风格转化模型改善BT数据的源语言部分，旨在提高自然输入的翻译质量。在不同的语言对上进行实验，结果表明TST BT显著提高了翻译性能，还可以用于领域适应，是一种通用的数据增强技术。

    

    Back Translation (BT)技术被广泛应用于机器翻译领域，因为它被证明能够提高翻译质量。然而，由于BT数据的源语言部分是机器翻译的，所以BT主要改善共享相似风格输入（更具体地说，是类似翻译的输入）的翻译质量。对于自然输入，BT只能带来轻微的改进，有时甚至会带来不利的影响。为了解决这个问题，我们提出了文本风格转化的Back-Translation技术（TST BT），它使用风格转换模型来修改BT数据的源语言部分。通过使源语言部分的文本风格更自然，我们旨在改善自然输入的翻译质量。我们在不同的语言对上进行了实验，包括高资源和低资源的语言对，结果表明TST BT显著提高了翻译质量。此外，TST BT还证明在领域适应方面也非常有效，因此这种策略可以被视为一种通用的数据增强技术。

    Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-like inputs), since the source side of BT data is machine-translated. For natural inputs, BT brings only slight improvements and sometimes even adverse effects. To address this issue, we propose Text Style Transfer Back Translation (TST BT), which uses a style transfer model to modify the source side of BT data. By making the style of source-side text more natural, we aim to improve the translation of natural inputs. Our experiments on various language pairs, including both high-resource and low-resource ones, demonstrate that TST BT significantly improves translation performance against popular BT benchmarks. In addition, TST BT is proved to be effective in domain adaptation so this strategy can be regarded as a general data augmenta
    
[^77]: 独立模块化网络

    Independent Modular Networks. (arXiv:2306.01316v1 [cs.CV])

    [http://arxiv.org/abs/2306.01316](http://arxiv.org/abs/2306.01316)

    这篇论文提出了一种独特的模块化网络架构来容纳数据的分解性质，可以有效应用于机器人技术，克服现有模块化网络的问题。

    

    使用单一权重集合的整体神经网络，明确地放弃了数据生成过程的组成性质，这在数据中存在，每个实例可以被视为身份概念（例如对象的形状）与修改概念（例如方向、颜色和大小）的结合。这种分解性质在机器人技术中尤为重要，因为状态估计严重依赖于物理机制（例如旋转和转化）的组合性质来建模交互。为了容纳这种数据特征，已经提出了模块化网络。然而，每个模块的角色缺乏结构，以及模块化网络特有的问题，例如模块崩溃，限制了它们的可用性。我们提出了一种能够容纳上述分解概念的模块化网络架构，提出了一个独特的结构来分离

    Monolithic neural networks that make use of a single set of weights to learn useful representations for downstream tasks explicitly dismiss the compositional nature of data generation processes. This characteristic exists in data where every instance can be regarded as the combination of an identity concept, such as the shape of an object, combined with modifying concepts, such as orientation, color, and size. The dismissal of compositionality is especially detrimental in robotics, where state estimation relies heavily on the compositional nature of physical mechanisms (e.g., rotations and transformations) to model interactions. To accommodate this data characteristic, modular networks have been proposed. However, a lack of structure in each module's role, and modular network-specific issues such as module collapse have restricted their usability. We propose a modular network architecture that accommodates the mentioned decompositional concept by proposing a unique structure that split
    
[^78]: EPIC: 通过可学习的代价实现的编辑路径插值的图形增强

    EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost. (arXiv:2306.01310v1 [cs.LG])

    [http://arxiv.org/abs/2306.01310](http://arxiv.org/abs/2306.01310)

    EPIC提出了一种基于插值的方法来增强图数据集，通过利用图编辑距离生成与原始图相似但有结构变化的新图，从而提高了分类模型的泛化能力。

    

    基于图的模型在各个领域中变得越来越重要，但现有图数据集的有限规模和多样性经常限制它们的性能。为解决这个问题，我们提出了EPIC（通过可学习的代价实现的编辑路径插值），这是一种新颖的基于插值的增强图数据集的方法。我们的方法利用了图编辑距离来生成与原始图相似但结构有所变化的新图。为了实现这一点，我们通过比较带标签的图来学习图编辑距离，并利用这一知识在原始图对之间创建了图编辑路径。通过从图编辑路径中随机抽样的图形，我们丰富了训练集以增强分类模型的泛化能力。我们在几个基准数据集上展示了我们方法的有效性，并表明它在图分类任务中优于现有的增强方法。

    Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.
    
[^79]: 通过因果表示的重构智能表面联邦学习游戏

    Federated Learning Games for Reconfigurable Intelligent Surfaces via Causal Representations. (arXiv:2306.01306v1 [cs.LG])

    [http://arxiv.org/abs/2306.01306](http://arxiv.org/abs/2306.01306)

    本文提出了一种在联邦学习框架下解决异构通信环境下重构智能表面（RIS）相移配置问题的方法。通过使用因果表示来学习不变的表示，最终预测RIS的相位配置。基于模拟实验的结果表明该方法是有效的。

    

    本文研究了在异构通信环境下对重构智能表面（RIS）相移配置进行强鲁棒性的问题。该问题被规定为在联邦学习（FL）设置下在不同环境中进行分布式学习的问题。相当于在异构环境中，多个RIS作为学习代理之间进行博弈的游戏。使用不变风险最小化（IRM）及其FL等价物，被称为FL Games，通过在多个环境中学习不变的因果表示，预测相移来解决RIS配置问题。解决方案对应于根据 最佳响应动力学（BRD）进行游戏，从而产生FL游戏的纳什均衡状态。表示学习器和相位预测器由两个神经网络模型构建，基于对其他来自文献的基准测试的模拟验证其表现。我们的结果显示，基于因果关系的学习可以解决RIS相移配置问题。

    In this paper, we investigate the problem of robust Reconfigurable Intelligent Surface (RIS) phase-shifts configuration over heterogeneous communication environments. The problem is formulated as a distributed learning problem over different environments in a Federated Learning (FL) setting. Equivalently, this corresponds to a game played between multiple RISs, as learning agents, in heterogeneous environments. Using Invariant Risk Minimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS configuration problem by learning invariant causal representations across multiple environments and then predicting the phases. The solution corresponds to playing according to Best Response Dynamics (BRD) which yields the Nash Equilibrium of the FL game. The representation learner and the phase predictor are modeled by two neural networks, and their performance is validated via simulations against other benchmarks from the literature. Our results show that causality-based learning y
    
[^80]: 高扩展性任务完成的自我中心规划

    Egocentric Planning for Scalable Embodied Task Achievement. (arXiv:2306.01295v1 [cs.AI])

    [http://arxiv.org/abs/2306.01295](http://arxiv.org/abs/2306.01295)

    本文提出了一种自我中心规划的方法，结合符号规划和面向对象的POMDP模型，成为高扩展性任务完成的一种新途径。

    

    在不同环境中完成任务对于采取行动的机器人代理来说面临着重大挑战，尤其是在对象类型上的泛化以及执行适当的行动以完成任务。本文提出了自我中心规划，一种创新的方法，将符号规划和面向对象的POMDP结合起来，以解决复杂环境下的任务，利用现有的视觉感知和自然语言处理模型。我们在ALFRED（一个模拟的家庭任务环境）中评估了我们的方法，并展示了其高可扩展性，在ALFRED基准测试中取得了惊人的36.07%的未见过的成功率，并在CVPR Emobodied AI workshop中获得了ALFRED挑战赛的胜利。我们的方法需要可靠的感知，并规定或学习关于代理的行动前提和效果，以及对象类型的符号描述。

    Embodied agents face significant challenges when tasked with performing actions in diverse environments, particularly in generalizing across object types and executing suitable actions to accomplish tasks. Furthermore, agents should exhibit robustness, minimizing the execution of illegal actions. In this work, we present Egocentric Planning, an innovative approach that combines symbolic planning and Object-oriented POMDPs to solve tasks in complex environments, harnessing existing models for visual perception and natural language processing. We evaluated our approach in ALFRED, a simulated environment designed for domestic tasks, and demonstrated its high scalability, achieving an impressive 36.07% unseen success rate in the ALFRED benchmark and winning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires reliable perception and the specification or learning of a symbolic description of the preconditions and effects of the agent's actions, as well as what object types
    
[^81]: 智能城市交通系统中基于图的机器学习的最新进展。

    Recent Advances in Graph-based Machine Learning for Applications in Smart Urban Transportation Systems. (arXiv:2306.01282v1 [cs.LG])

    [http://arxiv.org/abs/2306.01282](http://arxiv.org/abs/2306.01282)

    智能交通系统（ITS）是现代交通基础设施的重要组成部分。已出现基于复杂数据的数据驱动解决方案以解决各种与ITS相关的挑战。图形机器学习方法成为了智能交通系统领域的日益重要的研究重点。

    

    智能交通系统（ITS）是现代交通基础设施的重要组成部分，采用通信技术，信息处理和控制系统来管理交通网络。这种集成各种组件（如道路，车辆和通信系统）的方法，预计通过提供更好的信息，服务和交通模式的协调来提高效率和安全性。近年来，基于图的机器学习已成为智能交通系统领域日益重要的研究重点，旨在开发基于复杂数据的数据驱动解决方案，以解决各种与ITS相关的挑战。本章节提供ITS设计的关键技术挑战的背景信息，以及涵盖从经典统计方法到现代机器学习和基于深度学习的方法的研究方法的综述。具体而言，我们提供了基于图的机器学习方法的深入审查。

    The Intelligent Transportation System (ITS) is an important part of modern transportation infrastructure, employing a combination of communication technology, information processing and control systems to manage transportation networks. This integration of various components such as roads, vehicles, and communication systems, is expected to improve efficiency and safety by providing better information, services, and coordination of transportation modes. In recent years, graph-based machine learning has become an increasingly important research focus in the field of ITS aiming at the development of complex, data-driven solutions to address various ITS-related challenges. This chapter presents background information on the key technical challenges for ITS design, along with a review of research methods ranging from classic statistical approaches to modern machine learning and deep learning-based approaches. Specifically, we provide an in-depth review of graph-based machine learning metho
    
[^82]: 超越主动学习：通过自动标注、人工修正和人工验证充分利用人类交互的潜力

    Beyond Active Learning: Leveraging the Full Potential of Human Interaction via Auto-Labeling, Human Correction, and Human Verification. (arXiv:2306.01277v1 [cs.LG])

    [http://arxiv.org/abs/2306.01277](http://arxiv.org/abs/2306.01277)

    本文提出了一个名为CLARIFIER的交互式学习框架，通过利用验证成本的降低，能够更好地利用人类交互，实现更有效的主动学习。

    

    主动学习是一种人类参与的框架，可交互地和适应性地标记数据实例，从而使得模型性能比随机采样得到了显著提升。主动学习方法通过选择难以标注的实例来起作用，通常依赖于多样性和不确定性。然而，我们认为目前的这些主动学习范例没有充分利用自动标注建议所赋予的人类交互的全部潜力。实际上，我们证明对于许多分类任务和数据集，多数人在验证自动建议标签的正确性时所需时间为更正不正确的建议到正确标签（或不使用任何建议进行标注）所需时间的3到4倍。利用这个结果，我们提出了CLARIFIER（来自分层难度的主动学习），它是一个交互式学习框架，通过利用验证成本的降低，更有效地利用人类交互。

    Active Learning (AL) is a human-in-the-loop framework to interactively and adaptively label data instances, thereby enabling significant gains in model performance compared to random sampling. AL approaches function by selecting the hardest instances to label, often relying on notions of diversity and uncertainty. However, we believe that these current paradigms of AL do not leverage the full potential of human interaction granted by automated label suggestions. Indeed, we show that for many classification tasks and datasets, most people verifying if an automatically suggested label is correct take $3\times$ to $4\times$ less time than they do changing an incorrect suggestion to the correct label (or labeling from scratch without any suggestion). Utilizing this result, we propose CLARIFIER (aCtive LeARnIng From tIEred haRdness), an Interactive Learning framework that admits more effective use of human interaction by leveraging the reduced cost of verification. By targeting the hard (un
    
[^83]: 组合优化中对称探索是免费的！

    Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])

    [http://arxiv.org/abs/2306.01276](http://arxiv.org/abs/2306.01276)

    该论文提出了一种免费的技术，通过利用对称性提高了基于DRL的组合优化求解器的性能，无需额外的目标函数评估，适用于广泛的组合优化任务，并在多种任务上进行实证评估证实了其有效性。

    

    最近，深度强化学习（DRL）在解决组合优化（CO）问题方面已经显示出潜力。然而，他们经常需要大量的目标函数评估，这在现实场景中可能耗时。为了解决这个问题，我们提出了一种“免费”的技术，通过利用对称性来增强任何深度强化学习（DRL）求解器的性能，而无需额外的目标函数评估。我们的关键思想是通过保留奖励的变换来增强基于DRL的组合优化求解器的训练。该算法可能具有影响力，因为它简单，易于与现有求解器集成，并适用于广泛的组合优化任务。在NP难的路线优化，调度优化和新型分子优化的广泛实证评估结果表明，我们的方法轻松提高了最先进的DRL算法的样本效率。

    Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a "free" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou
    
[^84]: DeepfakeArt Challenge: 用于生成AI艺术伪造和数据污染检测的基准数据集

    DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection. (arXiv:2306.01272v1 [cs.CV])

    [http://arxiv.org/abs/2306.01272](http://arxiv.org/abs/2306.01272)

    本文介绍了DeepfakeArt Challenge，这是一个专门为帮助构建机器学习算法以进行生成AI艺术伪造和数据污染检测而设计的大规模挑战基准数据集。

    

    生成人工智能技术的巨大进步在各种应用中带来了显着的成功和前景，范围从会话代理和文本内容生成到语音和视觉合成。在生成AI的崛起和其越来越广泛的采用中，对于生成AI的恶意用途存在着显着的日益增长的关注。在使用生成AI进行视觉内容合成的领域中，重要的关注领域是图像伪造（例如，生成包含或派生自版权内容的图像）和数据污染（即生成被敌对污染的图像）。为了解决这些关键问题，鼓励负责任的生成AI，我们推出了DeepfakeArt Challenge，一个大型挑战基准数据集，专门设计用于帮助构建机器学习算法以进行生成AI艺术伪造和数据污染检测。

    The tremendous recent advances in generative artificial intelligence techniques have led to significant successes and promise in a wide range of different applications ranging from conversational agents and textual content generation to voice and visual synthesis. Amid the rise in generative AI and its increasing widespread adoption, there has been significant growing concern over the use of generative AI for malicious purposes. In the realm of visual content synthesis using generative AI, key areas of significant concern has been image forgery (e.g., generation of images containing or derived from copyright content), and data poisoning (i.e., generation of adversarially contaminated images). Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection. 
    
[^85]: 为什么在对抗训练中会同时出现干净泛化和强健过拟合现象？

    Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])

    [http://arxiv.org/abs/2306.01271](http://arxiv.org/abs/2306.01271)

    对抗训练是训练深度神经网络抗击对抗扰动的标准方法, 其学习机制导致干净泛化和强健过拟合现象同时发生。

    

    对抗训练是训练深度神经网络抗击对抗扰动的标准方法。与在标准深度学习环境中出现惊人的干净泛化能力类似，通过对抗训练训练的神经网络也能很好地泛化到未见过的干净数据。然而，与干净泛化不同的是，尽管对抗训练能够实现低鲁棒训练误差，仍存在显著的鲁棒泛化距离，这促使我们探索在学习过程中导致干净泛化和强健过拟合现象同时发生的机制。本文提供了对抗训练中这种现象的理论理解。首先，我们提出了对抗训练的理论框架，分析了特征学习过程，解释了对抗训练如何导致网络学习者进入到干净泛化和强健过拟合状态。具体来说，我们证明了，通过迫使学习器成为强预测网络，对抗训练将导致干净泛化和鲁棒过拟合现象同时发生。

    Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low $\textit{robust training error}$, there still exists a significant $\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this CGRO phenomenon in adversarial training. First, we propose a theoretical framework of adversarial training, where we analyze $\textit{feature learning process}$ to explain how adversarial training leads network learner to CGRO regime. Specifically, we prove that, u
    
[^86]: 组合启发式和多智能体强化学习的多机器人路径规划

    Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning. (arXiv:2306.01270v1 [cs.AI])

    [http://arxiv.org/abs/2306.01270](http://arxiv.org/abs/2306.01270)

    提出了一种多机器人路径规划方法MAPPOHR，该方法结合了启发式搜索、经验规则和多智能体强化学习。实验结果表明，该方法在规划效率和避碰能力方面优于现有方法。

    

    动态环境下的多机器人路径规划是一个极具挑战性的经典问题。在移动过程中，机器人需要避免与其他移动机器人发生碰撞，同时最小化它们的行驶距离。以往的方法要么使用启发式搜索方法不断重新规划路径以避免冲突，要么基于学习方法选择适当的避碰策略。前者可能由于频繁的重新规划导致行驶距离较长，而后者可能由于低样本探索和利用率而导致学习效率低，从而使模型的训练成本较高。为解决这些问题，我们提出了一种路径规划方法MAPPOHR，该方法结合了启发式搜索、经验规则和多智能体强化学习。该方法包含两个层次：基于多智能体强化学习算法MAPPO的实时规划器，其将经验规则嵌入到动作输出层和奖励函数中；以及一个启发式规划器，它生成初始路径并向MAPPO规划器添加约束。我们的实验结果表明，所提出的方法在规划效率和避碰能力方面优于现有方法。

    Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuri
    
[^87]: 自对比学习用于基于会话的推荐

    Self Contrastive Learning for Session-based Recommendation. (arXiv:2306.01266v1 [cs.IR])

    [http://arxiv.org/abs/2306.01266](http://arxiv.org/abs/2306.01266)

    本文提出了自对比学习方法，简化了会话推荐领域基于对比学习的模型的复杂性，并提高了推荐性能。

    

    基于会话的推荐旨在预测用户对现有项目交互序列的下一个感兴趣的项目，已经吸引了越来越多应用使用对比学习（CL）提高用户和项目的表示。然而，这些对比目标：（1）起到与交叉熵损失类似的作用，同时忽略了项目表示空间优化；（2）通常需要复杂的建模，包括复杂的正/负样本构建和额外的数据增强。在本文中，我们引入了自对比学习（SCL），简化了对比学习的应用，并增强了基于状态的推荐技术的性能。具体而言，SCL被制定为一个目标函数，直接促进项目表示之间的均匀分布，并有效地替换了所有现有的对比目标组件的状态-艺术模型。与以前的工作不同，SCL消除了任何正样本或负样本的需求和SCL消除了任何正样本或负样本的需求和数据增强的需求。

    Session-based recommendation, which aims to predict the next item of users' interest as per an existing sequence interaction of items, has attracted growing applications of Contrastive Learning (CL) with improved user and item representations. However, these contrastive objectives: (1) serve a similar role as the cross-entropy loss while ignoring the item representation space optimisation; and (2) commonly require complicated modelling, including complex positive/negative sample constructions and extra data augmentation. In this work, we introduce Self-Contrastive Learning (SCL), which simplifies the application of CL and enhances the performance of state-of-the-art CL-based recommendation techniques. Specifically, SCL is formulated as an objective function that directly promotes a uniform distribution among item representations and efficiently replaces all the existing contrastive objective components of state-of-the-art models. Unlike previous works, SCL eliminates the need for any p
    
[^88]: 多模态学习的校准。 （arXiv：2306.01265v1 [cs.LG]）

    Calibrating Multimodal Learning. (arXiv:2306.01265v1 [cs.LG])

    [http://arxiv.org/abs/2306.01265](http://arxiv.org/abs/2306.01265)

    本文提出了一个多模态学习的校准方法，可以解决当前方法在预测置信度不可靠的问题，提高模型的分类准确性和置信度校准能力。

    

    多模态机器学习在各种场景中取得了显着的进展。然而，多模态学习的可靠性仍然大部分未被探索。通过广泛的实证研究，本文发现当前的多模态分类方法存在预测置信度不可靠的问题，往往在估计置信度时依赖于部分模态。具体而言，我们发现当前模型估计的置信度甚至可以增加当某些模态受到损坏时。为了解决这个问题，我们引入一个直观的多模态学习原则，即当去除一个模态时，置信度不应该增加。因此，我们提出了一种新的正则化技术，即校准多模态学习（CML）正则化，来校准先前方法的预测置信度。这种技术可以通过现有模型进行灵活配置，并提高置信度校准、分类准确性和多模态集成学习的性能。

    Multimodal machine learning has achieved remarkable progress in a wide range of scenarios. However, the reliability of multimodal learning remains largely unexplored. In this paper, through extensive empirical studies, we identify current multimodal classification methods suffer from unreliable predictive confidence that tend to rely on partial modalities when estimating confidence. Specifically, we find that the confidence estimated by current models could even increase when some modalities are corrupted. To address the issue, we introduce an intuitive principle for multimodal learning, i.e., the confidence should not increase when one modality is removed. Accordingly, we propose a novel regularization technique, i.e., Calibrating Multimodal Learning (CML) regularization, to calibrate the predictive confidence of previous methods. This technique could be flexibly equipped by existing models and improve the performance in terms of confidence calibration, classification accuracy, and mo
    
[^89]: 广义平滑度下的凸和非凸优化

    Convex and Non-Convex Optimization under Generalized Smoothness. (arXiv:2306.01264v1 [math.OC])

    [http://arxiv.org/abs/2306.01264](http://arxiv.org/abs/2306.01264)

    本文发展了一种新的分析技术，并推广了广义平滑度条件，使凸和非凸优化问题获得更强的结果。在该条件下，获得了（随机）梯度下降和Nesterov加速梯度方法的经典收敛率。

    

    经典的凸和非凸优化方法的分析通常需要梯度的Lipshitz性质，这限制了分析范围仅限于二次函数的界限内。最近的工作放松了这个要求，转而使用一种非均匀平滑条件，其中Hessian范数受梯度范数的仿射函数限制，并通过梯度裁剪证明了非凸情况下的收敛性，假设存在有界噪声。在本文中，我们进一步推广了这种非均匀平滑条件，并开发了一种简单但功能强大的分析技术，可以沿轨迹方向限制梯度，从而获得更强的凸和非凸优化问题结果。特别地，在这个广义平滑条件下，我们得到了（随机）梯度下降和Nesterov加速梯度方法的经典收敛率，适用于凸和（或）非凸设定。新的分析方法不需要梯度裁剪，并允许有重尾噪声，这是一种非常实用的优化方法。

    Classical analysis of convex and non-convex optimization methods often requires the Lipshitzness of the gradient, which limits the analysis to functions bounded by quadratics. Recent work relaxed this requirement to a non-uniform smoothness condition with the Hessian norm bounded by an affine function of the gradient norm, and proved convergence in the non-convex setting via gradient clipping, assuming bounded noise. In this paper, we further generalize this non-uniform smoothness condition and develop a simple, yet powerful analysis technique that bounds the gradients along the trajectory, thereby leading to stronger results for both convex and non-convex optimization problems. In particular, we obtain the classical convergence rates for (stochastic) gradient descent and Nesterov's accelerated gradient method in the convex and/or non-convex setting under this general smoothness condition. The new analysis approach does not require gradient clipping and allows heavy-tailed noise with b
    
[^90]: 超越不可简约性的混合比例估计

    Mixture Proportion Estimation Beyond Irreducibility. (arXiv:2306.01253v1 [stat.ML])

    [http://arxiv.org/abs/2306.01253](http://arxiv.org/abs/2306.01253)

    本文提出了一种更一般的充分条件来解决MPE中不可简约性假设下的估计问题，并实现了更好的估计性能。

    

    混合比例估计（MPE）的任务是估计混合物中组成分布的权重，给定来自组成分布和混合物的观测。以前的MPE工作采用了不可简约性假设，这确保了混合比例的可辨识性。在本文中，我们提出了一个更一般的充分条件，适用于几种感兴趣的设置，其中不可简约性不成立。我们进一步提出了一个基于重采样的元算法，它接受任何现有的在不可简约性下设计的MPE算法，并将其调整为在我们更一般的条件下工作。我们的方法在经验上展现了相对于基线方法和最近提出的基于重新分组的算法的估计性能改善。

    The task of mixture proportion estimation (MPE) is to estimate the weight of a component distribution in a mixture, given observations from both the component and mixture. Previous work on MPE adopts the irreducibility assumption, which ensures identifiablity of the mixture proportion. In this paper, we propose a more general sufficient condition that accommodates several settings of interest where irreducibility does not hold. We further present a resampling-based meta-algorithm that takes any existing MPE algorithm designed to work under irreducibility and adapts it to work under our more general condition. Our approach empirically exhibits improved estimation performance relative to baseline methods and to a recently proposed regrouping-based algorithm.
    
[^91]: 变革心电图诊断: Transformer-based深度学习模型在心血管疾病检测中的应用的深入评估

    Transforming ECG Diagnosis:An In-depth Review of Transformer-based DeepLearning Models in Cardiovascular Disease Detection. (arXiv:2306.01249v1 [cs.LG])

    [http://arxiv.org/abs/2306.01249](http://arxiv.org/abs/2306.01249)

    本综述介绍了最新的基于transformer的深度学习模型在ECG分类中的应用进展和挑战，探索这些模型对ECG信号中的复杂时间关系进行建模的能力，并提出未来的改进方向。

    

    深度学习技术的出现显著增进了心电图（ECG）的分析，这是一种非侵入性的评估心脏健康状况的方法。尽管ECG的解释较为复杂，但先进的深度学习模型超越了传统的方法。然而，ECG数据复杂性的增加和实时、准确的诊断需要探索更加强大的架构，例如transformers。 在本文中，我们介绍了对应用于ECG分类的transformer架构的深度评估。这些模型最初是用于自然语言处理的，它们捕捉ECG信号中的复杂时间关系，其他模型可能会忽略它们。我们对最新的基于transformer的模型进行了广泛的搜索和总结，以讨论它们的应用进展和挑战，并建议潜在的未来改进。这篇综述是研究人员和从业者的宝贵资源，旨在阐明使用transformer-based深度学习模型进行ECG诊断。

    The emergence of deep learning has significantly enhanced the analysis of electrocardiograms (ECGs), a non-invasive method that is essential for assessing heart health. Despite the complexity of ECG interpretation, advanced deep learning models outperform traditional methods. However, the increasing complexity of ECG data and the need for real-time and accurate diagnosis necessitate exploring more robust architectures, such as transformers. Here, we present an in-depth review of transformer architectures that are applied to ECG classification. Originally developed for natural language processing, these models capture complex temporal relationships in ECG signals that other models might overlook. We conducted an extensive search of the latest transformer-based models and summarize them to discuss the advances and challenges in their application and suggest potential future improvements. This review serves as a valuable resource for researchers and practitioners and aims to shed light on
    
[^92]: 预训练的抽象模型和LLMs在法律案例判决摘要中的应用准备情况？

    How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])

    [http://arxiv.org/abs/2306.01248](http://arxiv.org/abs/2306.01248)

    这篇论文探讨了是否可以使用预训练的抽象模型和大型语言模型来自动生成法律案例判决的摘要，并在印度的法庭案例判决中进行了相关实验分析。

    

    自动摘要法律案例判决一直是采用抽取式摘要方法尝试解决的问题。然而，近年来，具有生成更自然和连贯摘要能力的抽象摘要模型受到越来越多的关注。现在已经有了专门用于法律领域的预训练抽象摘要模型。此外，众所周知，如ChatGPT这样的通用领域预训练大型语言模型(LLMs)能够生成高质量的文本，并具有文本摘要的能力。因此，值得问的是，这些模型是否已准备好用于自动生成案例判决的抽象摘要。为了探讨这个问题，我们将几种最先进的领域特定的抽象性摘要模型和通用领域的LLMs应用于印度法庭案例判决中，并检查所生成摘要的质量。除了摘要质量的标准度量，我们还检查了生成的摘要中可能存在的不一致性和虚构现象。

    Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the 
    
[^93]: 迈向可持续学习：用于数据高效深度学习的核心集合

    Towards Sustainable Learning: Coresets for Data-efficient Deep Learning. (arXiv:2306.01244v1 [cs.LG])

    [http://arxiv.org/abs/2306.01244](http://arxiv.org/abs/2306.01244)

    CREST是一个可扩展的框架，用于数据高效深度学习，通过提取有价值的样本和多个小批量核心集，保证非凸模型收敛到稳定点，提高可扩展性和效率。

    

    为了提高深度模型的效率和可持续性，我们提出了CREST，这是第一个具有严格理论保证的可扩展框架，用于识别训练非凸模型（特别是深度网络）最有价值的样本。为了保证收敛到非凸函数的稳定点，CREST将非凸损失模拟为一系列二次函数，并为每个二次子区域提取一个核心集。此外，为了确保随机梯度方法（如小批量随机梯度下降）的更快收敛，CREST从更大的随机训练数据子集中迭代地提取多个小批量核心集，以确保近似无偏的梯度和小方差。最后，为了进一步提高可扩展性和效率，CREST确定并排除从核心集选择流程中学习到的样本。我们在几个深度网络上进行了大量实验，包括CIFAR-10、CIFAR-100、TinyImageNet和SNLI等视觉和NLP数据集。

    To improve the efficiency and sustainability of learning deep models, we propose CREST, the first scalable framework with rigorous theoretical guarantees to identify the most valuable examples for training non-convex models, particularly deep networks. To guarantee convergence to a stationary point of a non-convex function, CREST models the non-convex loss as a series of quadratic functions and extracts a coreset for each quadratic sub-region. In addition, to ensure faster convergence of stochastic gradient methods such as (mini-batch) SGD, CREST iteratively extracts multiple mini-batch coresets from larger random subsets of training data, to ensure nearly-unbiased gradients with small variances. Finally, to further improve scalability and efficiency, CREST identifies and excludes the examples that are learned from the coreset selection pipeline. Our extensive experiments on several deep networks trained on vision and NLP datasets, including CIFAR-10, CIFAR-100, TinyImageNet, and SNLI,
    
[^94]: 有损可观察性下的高效强化学习：学习在延迟和缺失环境中做出决策

    Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations. (arXiv:2306.01243v1 [cs.LG])

    [http://arxiv.org/abs/2306.01243](http://arxiv.org/abs/2306.01243)

    本文提出了在延迟和缺失状态观察环境中进行高效强化学习的理论研究，并建立了接近最优的遗憾边界，尽管有损可观察性对策略和规划的类别造成了重大挑战，但学习仍然是高效的。

    

    在现实世界的强化学习系统中，各种形式的有损可观察性可能会使问题变得复杂。当代理由于延迟或丢失的通道而无法观察到系统的最新状态时，这些情况会出现，但代理仍必须做出实时决策。本文介绍了对在控制系统中进行高效强化学习进行理论研究，其中代理必须在延迟和缺失状态观察环境中操作。我们建立了接近最优的遗憾边界，形式为$ \tilde{\mathcal{O}}(\sqrt{{\rm poly}(H) SAK}) $，以在延迟和缺失观察设置下实现强化学习。尽管有损可观察性对策略和规划的类别造成了重大挑战，但我们的结果表明学习仍然是高效的，遗憾边界最优地依赖于原始系统的状态-动作大小。此外，我们比较了受影响的观察下最佳策略的性能与最优值之间的差异。

    In real-world reinforcement learning (RL) systems, various forms of impaired observability can complicate matters. These situations arise when an agent is unable to observe the most recent state of the system due to latency or lossy channels, yet the agent must still make real-time decisions. This paper introduces a theoretical investigation into efficient RL in control systems where agents must act with delayed and missing state observations. We establish near-optimal regret bounds, of the form $\tilde{\mathcal{O}}(\sqrt{{\rm poly}(H) SAK})$, for RL in both the delayed and missing observation settings. Despite impaired observability posing significant challenges to the policy class and planning, our results demonstrate that learning remains efficient, with the regret bound optimally depending on the state-action size of the original system. Additionally, we provide a characterization of the performance of the optimal policy under impaired observability, comparing it to the optimal val
    
[^95]: 具有一致性图的预训练模型联邦学习翻译

    Federated Learning of Models Pre-Trained on Different Features with Consensus Graphs. (arXiv:2306.01240v1 [cs.LG])

    [http://arxiv.org/abs/2306.01240](http://arxiv.org/abs/2306.01240)

    在分散的数据集情况下，为了实现本地模型的一致性，提出了一种特征融合的方法来提高预测性能。

    

    当应用于实践中时，在私有和分散的数据集上学习有效的全局模型已成为机器学习面临的日益重要的挑战。现有的分布式学习范例（如联邦学习）通过模型聚合来实现这一点，这对客户端施加了一种强制性的建模同质性和同步性。然而，在许多实际情况下，这种方法并不适用。为了解决这个问题，本文提出了一个特征融合的方法来实现本地模型之间的一致性。

    Learning an effective global model on private and decentralized datasets has become an increasingly important challenge of machine learning when applied in practice. Existing distributed learning paradigms, such as Federated Learning, enable this via model aggregation which enforces a strong form of modeling homogeneity and synchronicity across clients. This is however not suitable to many practical scenarios. For example, in distributed sensing, heterogeneous sensors reading data from different views of the same phenomenon would need to use different models for different data modalities. Local learning therefore happens in isolation but inference requires merging the local models to achieve consensus. To enable consensus among local models, we propose a feature fusion approach that extracts local representations from local models and incorporates them into a global representation that improves the prediction performance. Achieving this requires addressing two non-trivial problems. Fir
    
[^96]: 离线赌博中贝叶斯遗憾最小化的凸松弛方法

    A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits. (arXiv:2306.01237v1 [cs.LG])

    [http://arxiv.org/abs/2306.01237](http://arxiv.org/abs/2306.01237)

    本文提出一种直接最小化贝叶斯遗憾上界的新方法，获得更好的理论离线遗憾界和数值模拟结果，并提供了证据表明流行的LCB-style算法可能不适用。

    

    离线赌博算法必须仅利用离线数据在不确定环境中优化决策。离线赌博中一种引人注目且逐渐流行的目标是学习一个实现低贝叶斯遗憾并具有高置信度的策略。本文提出了一种新的方法，直接利用高效的锥优化求解器来最小化贝叶斯遗憾的上界。与之前的工作相比，我们的算法在理论上获得了更优的离线遗憾界，并在数值模拟中取得了更好的结果。最后，我们提供一些证据表明流行的LCB（lower confidence bound）-style算法可能不适合离线赌博中最小化贝叶斯遗憾。

    Algorithms for offline bandits must optimize decisions in uncertain environments using only offline data. A compelling and increasingly popular objective in offline bandits is to learn a policy which achieves low Bayesian regret with high confidence. An appealing approach to this problem, inspired by recent offline reinforcement learning results, is to maximize a form of lower confidence bound (LCB). This paper proposes a new approach that directly minimizes upper bounds on Bayesian regret using efficient conic optimization solvers. Our bounds build on connections among Bayesian regret, Value-at-Risk (VaR), and chance-constrained optimization. Compared to prior work, our algorithm attains superior theoretical offline regret bounds and better results in numerical simulations. Finally, we provide some evidence that popular LCB-style algorithms may be unsuitable for minimizing Bayesian regret in offline bandits.
    
[^97]: 用非约束的未标记数据扩展半监督学习的规模

    Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data. (arXiv:2306.01222v1 [cs.LG])

    [http://arxiv.org/abs/2306.01222](http://arxiv.org/abs/2306.01222)

    本论文提出UnMixMatch框架，可以从非约束的未标记数据中学习有效的表征以提高性能，以解决大多数半监督方法基于有标记和未标记样本来自同一分布的假设。该框架具有强正则化作用的硬增强监督学习器、用于从未标记数据中学习基础表征的对比一致性正则化器以及通过自监督损失来增强从未标记数据学习的表征。

    

    本文提出了一种半监督学习框架UnMixMatch，可以从非约束的未标记数据中学习有效的表征以提高性能。大多数现有的半监督方法基于有标记和未标记样本来自同一分布的假设，这限制了通过使用自由生活的未标记数据进行改进的潜力。因此，该假设经常限制半监督学习的可推广性和可扩展性，本文旨在克服这些限制并有效利用非约束的未标记数据进行半监督学习。UnMixMatch包括三个主要组成部分：具有强正则化作用的硬增强监督学习器，用于从未标记数据中学习基础表征的对比一致性正则化器以及通过自监督损失来增强从未标记数据学习的表征。我们进行了大量实验来证明UnMixMatch的有效性。

    We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experim
    
[^98]: 模型的注意力机制是否与人类的注意力机制一致？关于大型语言模型用于代码生成的实证研究。

    Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation. (arXiv:2306.01220v1 [cs.SE])

    [http://arxiv.org/abs/2306.01220](http://arxiv.org/abs/2306.01220)

    本文研究了LLMs在代码生成过程中是否与人类程序员的注意力有所不同，结果发现他们之间存在一致性偏差。作者通过量化实验和用户研究，确认了扰动方法计算的注意力最接近人类程序员的注意力，并且这种LLMs模型具有更好的可解释能力和程序员信任度。

    

    大型语言模型（LLMs）已被证明对于代码生成非常有效。由于LLMs的复杂性和不透明性，我们对这些模型如何生成代码知之甚少。为了深入了解，我们研究了LLMs在代码生成过程中是否与人类程序员相同地关注自然语言描述中的某些部分。通过对流行基准测试HumanEval上的五个LLMs进行分析，发现LLMs的注意力与程序员的注意力存在一致性偏差。此外，我们发现LLMs的代码生成准确性与它们与人类程序员的注意力对齐程度之间没有相关性。通过量化实验和用户研究，我们确认，在12种不同的注意力计算方法中，基于扰动的方法计算的注意力最接近人类注意力，并且始终受到人类程序员的青睐。我们的研究结果强调了需要人类对齐的LLMs以获得更好的可解释性和程序员信任。

    Large Language Models (LLMs) have been demonstrated effective for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. To deepen our understanding, we investigate whether LLMs attend to the same parts of a natural language description as human programmers during code generation. An analysis of five LLMs on a popular benchmark, HumanEval, revealed a consistent misalignment between LLMs' and programmers' attention. Furthermore, we found that there is no correlation between the code generation accuracy of LLMs and their alignment with human programmers. Through a quantitative experiment and a user study, we confirmed that, among twelve different attention computation methods, attention computed by the perturbation-based method is most aligned with human attention and is constantly favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.
    
[^99]: 带圆锥约束的非单调变分不等式问题的增广Lagrangian方法

    An Augmented Lagrangian Approach to Conically Constrained Non-monotone Variational Inequality Problems. (arXiv:2306.01214v1 [math.OC])

    [http://arxiv.org/abs/2306.01214](http://arxiv.org/abs/2306.01214)

    本文提出了一种被称为ALAVI的增广拉格朗日原始-对偶方法，用于解决一般约束VI模型，具有收敛性和全局收敛速度，对于存在某些广义单调性质的问题，收敛更快。

    

    本文考虑了带有凸圆锥约束的混合变分不等式模型。通过为所讨论的VI模型开发类似拉格朗日函数的原始-对偶鞍点系统，我们引入了一种增广拉格朗日原始-对偶方法（称为ALAVI），用于求解一般约束VI模型。在一个被称为原始-对偶变分相干性条件的假设下，我们证明了ALAVI的收敛性。接下来，我们展示了许多现有的广义单调性质是足够的，虽然不是必要的，以此来暗示上述相干条件，并且足以确保ALAVI的收敛性。在此假设下，我们进一步展示，ALAVI实际上具有$O(1/\sqrt{k})$的全局收敛速度，其中$k$是迭代计数。通过引入一种新的“间隙函数”，如果映射是单调的，这个速度可以进一步提高到$O(1/k)$。最后，我们展示了...

    In this paper we consider a non-monotone (mixed) variational inequality model with (nonlinear) convex conic constraints. Through developing an equivalent Lagrangian function-like primal-dual saddle-point system for the VI model in question, we introduce an augmented Lagrangian primal-dual method, to be called ALAVI in the current paper, for solving a general constrained VI model. Under an assumption, to be called the primal-dual variational coherence condition in the paper, we prove the convergence of ALAVI. Next, we show that many existing generalized monotonicity properties are sufficient -- though by no means necessary -- to imply the above mentioned coherence condition, thus are sufficient to ensure convergence of ALAVI. Under that assumption, we further show that ALAVI has in fact an $o(1/\sqrt{k})$ global rate of convergence where $k$ is the iteration count. By introducing a new gap function, this rate further improves to be $O(1/k)$ if the mapping is monotone. Finally, we show t
    
[^100]: 基于独立因果机制原则学习因果解缠绕表示

    Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms. (arXiv:2306.01213v1 [cs.LG])

    [http://arxiv.org/abs/2306.01213](http://arxiv.org/abs/2306.01213)

    本文通过定义独立因果机制，提出了ICM-VAE框架，使得学习因果解缠绕表示更准确

    

    学习解缠绕的因果表示是一个具有挑战性的问题，近年来因其对提取下游任务的有意义信息而引起了广泛关注。本文从独立因果机制的角度定义了一种新的因果解缠绕概念。我们提出了ICM-VAE框架，通过因因果关系观察标签来监督学习因果解缠绕表示。我们使用可学习的基于流的微分同胚函数将噪声变量映射到潜在因果变量中来建模因果机制。此外，为了促进因果要素的解缠绕，我们提出了一种因果解缠绕先验，利用已知的因果结构来鼓励在潜在空间中学习因果分解分布。在相对温和的条件下，我们提供了理论结果，显示了因果要素和机制的可识别性，直到排列和逐元重参数化的限度。我们进行了实证研究...

    Learning disentangled causal representations is a challenging problem that has gained significant attention recently due to its implications for extracting meaningful information for downstream tasks. In this work, we define a new notion of causal disentanglement from the perspective of independent causal mechanisms. We propose ICM-VAE, a framework for learning causally disentangled representations supervised by causally related observed labels. We model causal mechanisms using learnable flow-based diffeomorphic functions to map noise variables to latent causal variables. Further, to promote the disentanglement of causal factors, we propose a causal disentanglement prior that utilizes the known causal structure to encourage learning a causally factorized distribution in the latent space. Under relatively mild conditions, we provide theoretical results showing the identifiability of causal factors and mechanisms up to permutation and elementwise reparameterization. We empirically demons
    
[^101]: 链接深高斯过程模拟用于模型网络

    Linked Deep Gaussian Process Emulation for Model Networks. (arXiv:2306.01212v1 [stat.ML])

    [http://arxiv.org/abs/2306.01212](http://arxiv.org/abs/2306.01212)

    本文提出了一种链接深高斯过程模拟（LDGP）框架，可用于高效仿真具有非稳态行为的复杂模型，并提高了预测的准确性。

    

    现代科学问题通常是多学科的，需要整合来自不同学科的计算机模型，每个模型具有不同的功能复杂性，编程环境和计算时间。链接高斯过程（LGP）仿真通过将各个计算机模型的高斯过程仿真器集成到网络中的分而治之策略来解决这个挑战。然而，在LGP框架中，组件高斯过程仿真器的所需稳定性限制了其在许多实际应用中的适用性。在这项工作中，我们将计算模型网络概念化为一个具有部分隐藏层曝光的深高斯过程。我们开发了一种推断这些部分暴露的深度网络的方法，保留了LGP框架的一个关键优势，即每个模型都可以使用DGP分别仿真，然后链接在一起。我们在合成和实证示例中展示了我们的链接深度高斯过程模拟（LDGP）框架可以高效地仿真具有非稳态行为的复杂模型，并提高了预测的准确性，而相比之下，LGP框架的效果有所提升。

    Modern scientific problems are often multi-disciplinary and require integration of computer models from different disciplines, each with distinct functional complexities, programming environments, and computation times. Linked Gaussian process (LGP) emulation tackles this challenge through a divide-and-conquer strategy that integrates Gaussian process emulators of the individual computer models in a network. However, the required stationarity of the component Gaussian process emulators within the LGP framework limits its applicability in many real-world applications. In this work, we conceptualize a network of computer models as a deep Gaussian process with partial exposure of its hidden layers. We develop a method for inference for these partially exposed deep networks that retains a key strength of the LGP framework, whereby each model can be emulated separately using a DGP and then linked together. We show in both synthetic and empirical examples that our linked deep Gaussian proces
    
[^102]: 在域内和域外样本之间估计语义相似度

    Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples. (arXiv:2306.01206v1 [cs.CL])

    [http://arxiv.org/abs/2306.01206](http://arxiv.org/abs/2306.01206)

    本文研究如何以原则性的方式分析模型在域内和域外设置下的性能，最终找出一种无监督方法识别OOD / OODist样本。

    

    先前的工作通常将来自数据集或源与训练集不同但用于同一任务的域外（OOD）或域外分布（OODist）样本描述为域外，与域内（ID）样本相比，模型在OOD样本上的表现通常较差，尽管这种观察结果并不一致。另一方面，一些研究关注于OOD检测，但大多使用有监督的方法。在这项工作中，我们首先整合并呈现了多个关于OOD和OODist的多重定义，并以原则性的方式分析了模型在ID和OOD / OODist设置下的性能。最后，我们试图识别一种无监督方法，可在不使用训练模型的情况下可靠地识别OOD / OODist样本。我们使用4个不同任务的12个数据集进行了广泛评估的结果表明，无监督度量在该任务中具有良好的潜力。

    Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task. When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent. Another thread of research has focused on OOD detection, albeit mostly using supervised approaches. In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature. Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way. Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model. The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task.
    
[^103]: 物理学知识增强的 UNet 用于发现异质材料中隐藏的弹性

    Physics-informed UNets for Discovering Hidden Elasticity in Heterogeneous Materials. (arXiv:2306.01204v1 [cs.LG])

    [http://arxiv.org/abs/2306.01204](http://arxiv.org/abs/2306.01204)

    本文提出了一种新型的 UNet 神经网络模型 (El-UNet)，能够通过物理学知识增强和应变图像的分析，精确地推断生物软组织中材料参数的空间分布，具有高性能的优劣和计算效率。

    

    生物软组织常常由于结构成分的变化而具有复杂的机械特性。本文提出了一种新颖的基于 UNet 的反演弹性神经网络模型 (El-UNet)，将应变图作为输入图像、正常应力边界条件和区域物理信息，以推断力学参数的空间分布。我们展示了 El-UNet 在估计等向性线性弹性的未知参数和应力分布方面，与全连接的物理学知识增强神经网络相比，具有更高的性能，无论是在精度还是计算成本方面。我们对 El-UNet 的不同变体进行了分类，并提出了一种自适应空间损失加权方法。为了验证我们的反演模型，我们进行了各种等向异质材料有限元模拟产生的合成数据。El-UNet 比全连接物理学知识增强的实现更快、更准确。

    Soft biological tissues often have complex mechanical properties due to variation in structural components. In this paper, we develop a novel UNet-based neural network model for inversion in elasticity (El-UNet) to infer the spatial distributions of mechanical parameters from strain maps as input images, normal stress boundary conditions, and domain physics information. We show superior performance, both in terms of accuracy and computational cost, by El-UNet compared to fully-connected physics-informed neural networks in estimating unknown parameters and stress distributions for isotropic linear elasticity. We characterize different variations of El-UNet and propose a self-adaptive spatial loss weighting approach. To validate our inversion models, we performed various finite-element simulations of isotropic domains with heterogenous distributions of material parameters to generate synthetic data. El-UNet is faster and more accurate than the fully-connected physics-informed implementat
    
[^104]: 学习何时说话：离线模型进行同时语音翻译的延迟与质量权衡

    Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models. (arXiv:2306.01201v1 [cs.CL])

    [http://arxiv.org/abs/2306.01201](http://arxiv.org/abs/2306.01201)

    本文提出了一个适用于实际场景的同时语音翻译系统，支持从57种语言翻译成英语，并具有调整输出延迟的可调参数。我们展示了可以在不增加显著延迟的情况下达到离线水平的准确性。

    

    最近的语音翻译研究主要集中在离线场景中，在此场景中，完整的输入话语在任何输出之前都是可用的。然而，在许多实际应用场景中，这并不合理。在对延迟敏感的应用程序中，翻译应该在输入信息出现时立即发音。我们引入了一个系统，用于同时语音翻译的实际用例。我们的系统支持从57种语言翻译成英语，并具有调整输出延迟的可调参数。我们展示了这些策略达到了离线水平的准确性，同时在Greedy（wait-$k$）基线上最小化了延迟增加。我们开源了我们的评估代码和互动测试脚本，以帮助未来的SimulS2ST研究和应用程序发展。

    Recent work in speech-to-speech translation (S2ST) has focused primarily on offline settings, where the full input utterance is available before any output is given. This, however, is not reasonable in many real-world scenarios. In latency-sensitive applications, rather than waiting for the full utterance, translations should be spoken as soon as the information in the input is present. In this work, we introduce a system for simultaneous S2ST targeting real-world use cases. Our system supports translation from 57 languages to English with tunable parameters for dynamically adjusting the latency of the output -- including four policies for determining when to speak an output sequence. We show that these policies achieve offline-level accuracy with minimal increases in latency over a Greedy (wait-$k$) baseline. We open-source our evaluation code and interactive test script to aid future SimulS2ST research and application development.
    
[^105]: 一种有效的评估生存模型的有意义方法。

    An Effective Meaningful Way to Evaluate Survival Models. (arXiv:2306.01196v1 [cs.LG])

    [http://arxiv.org/abs/2306.01196](http://arxiv.org/abs/2306.01196)

    该论文提出了一种有效的方法来评估生存模型的性能，使用伪观察值的MAE指标能够准确地排名模型的性能，发现这种方法比其他替代方法更好。

    

    评估生存预测模型的一种直接指标是基于平均绝对误差（MAE）-模型预测时间与真实事件时间之间的绝对差值的平均值，对所有个体进行。然而，这是具有挑战性的，因为在实践中，测试集包括（正确）被审查的个体，这意味着我们不知道被审查个体实际经历事件的时间。在本文中，我们探索了用于评估包括（许多）被审查个体的生存数据集的各种指标来估计MAE。此外，我们介绍了一种新颖而有效的方法来生成逼真的半合成生存数据集，以便评估指标。基于半合成数据集的分析，我们的发现表明，我们提出的指标（使用伪观察法的MAE）能够准确地排名模型的性能，并且通常与真实的MAE非常接近-特别是优于几种替代方法。

    One straightforward metric to evaluate a survival prediction model is based on the Mean Absolute Error (MAE) -- the average of the absolute difference between the time predicted by the model and the true event time, over all subjects. Unfortunately, this is challenging because, in practice, the test set includes (right) censored individuals, meaning we do not know when a censored individual actually experienced the event. In this paper, we explore various metrics to estimate MAE for survival datasets that include (many) censored individuals. Moreover, we introduce a novel and effective approach for generating realistic semi-synthetic survival datasets to facilitate the evaluation of metrics. Our findings, based on the analysis of the semi-synthetic datasets, reveal that our proposed metric (MAE using pseudo-observations) is able to rank models accurately based on their performance, and often closely matches the true MAE -- in particular, is better than several alternative methods.
    
[^106]: 有部分标签数据的顺从预测

    Conformal Prediction with Partially Labeled Data. (arXiv:2306.01191v1 [cs.LG])

    [http://arxiv.org/abs/2306.01191](http://arxiv.org/abs/2306.01191)

    本文将顺从预测程序与集合值的训练数据相结合，提出了一种适用于集合值训练和校准数据的顺从预测程序。

    

    虽然顺从预测产生的预测是集合值，但用于训练和校准的数据应该是精确的。在超集学习或部分标签学习的设置中，一种弱监督学习的变体，情况恰恰相反:训练数据可能不精确（集合值），但从此数据中引出的模型产生精确预测。在本文中，我们将两种设置相结合，使顺从预测适应集合值的训练数据。我们提出了一种顺从预测程序的概括，可以应用于集合值的训练和校准数据。我们证明了所提出的方法的有效性，并展示了实验研究，在其中它与自然基线相比较有利。

    While the predictions produced by conformal prediction are set-valued, the data used for training and calibration is supposed to be precise. In the setting of superset learning or learning from partial labels, a variant of weakly supervised learning, it is exactly the other way around: training data is possibly imprecise (set-valued), but the model induced from this data yields precise predictions. In this paper, we combine the two settings by making conformal prediction amenable to set-valued training data. We propose a generalization of the conformal prediction procedure that can be applied to set-valued training and calibration data. We prove the validity of the proposed method and present experimental studies in which it compares favorably to natural baselines.
    
[^107]: 基于神经SDE-RNN的不确定性量化通用框架

    A General Framework for Uncertainty Quantification via Neural SDE-RNN. (arXiv:2306.01189v1 [cs.LG])

    [http://arxiv.org/abs/2306.01189](http://arxiv.org/abs/2306.01189)

    该论文提出了一种基于神经SDE-RNN的通用框架，用于解决时间序列填补中不确定性量化的问题，在任何时间尺度上填补测量并以原则性的方式量化填补中的不确定性，经实验证明可以胜过最先进的方法。

    

    不确定性量化是深度学习中的一个关键但尚未解决的挑战，特别是对于具有不规则采样数据的时间序列填补问题。为了解决这个问题，我们基于循环神经网络和神经随机微分方程的原理提出了一种新框架，用于协调不规则采样的测量。我们能够在任何任意时间尺度上填补测量并以原则性的方式量化填补中的不确定性。具体而言，我们推导了分析表达式，用于量化和传播时间瞬间的认知和随机不确定性。我们对IEEE 37巴士测试分布系统的实验表明，我们的框架可以胜过最先进的时间序列数据填补的不确定性量化方法。

    Uncertainty quantification is a critical yet unsolved challenge for deep learning, especially for the time series imputation with irregularly sampled measurements. To tackle this problem, we propose a novel framework based on the principles of recurrent neural networks and neural stochastic differential equations for reconciling irregularly sampled measurements. We impute measurements at any arbitrary timescale and quantify the uncertainty in the imputations in a principled manner. Specifically, we derive analytical expressions for quantifying and propagating the epistemic and aleatoric uncertainty across time instants. Our experiments on the IEEE 37 bus test distribution system reveal that our framework can outperform state-of-the-art uncertainty quantification approaches for time-series data imputations.
    
[^108]: 训练神经算子以保持混沌吸引子的不变测度

    Training neural operators to preserve invariant measures of chaotic attractors. (arXiv:2306.01187v1 [cs.LG])

    [http://arxiv.org/abs/2306.01187](http://arxiv.org/abs/2306.01187)

    本文提出了一种基于神经算子的训练框架，通过最优传输距离或KSD损失来确保神经算子能够在混沌系统上复现其统计或结构特性。

    

    混沌系统使得长时间预测变得困难，因为初始条件的微小扰动会导致轨迹以指数速度发散。在这种情况下，神经算子训练为最小化平方误差损失，虽然能够准确地进行短期预测，但常常无法再长时间内复制动力学的统计或结构特性，并且可能产生退化的结果。本文提出了一种替代框架，旨在保持表征动态不变统计属性的混沌吸引子的不变测度。具体来说，在多环境设置中（每个样本轨迹都受略微不同动态的控制），我们考虑了两种新的处理嘈杂数据的方法。首先，我们提出了一种基于观测到的动态与神经算子输出之间的最优传输距离的损失。这种方法需要专家对基础物理的专业知识来确定最优传输距离矩阵。

    Chaotic systems make long-horizon forecasts difficult because small perturbations in initial conditions cause trajectories to diverge at an exponential rate. In this setting, neural operators trained to minimize squared error losses, while capable of accurate short-term forecasts, often fail to reproduce statistical or structural properties of the dynamics over longer time horizons and can yield degenerate results. In this paper, we propose an alternative framework designed to preserve invariant measures of chaotic attractors that characterize the time-invariant statistical properties of the dynamics. Specifically, in the multi-environment setting (where each sample trajectory is governed by slightly different dynamics), we consider two novel approaches to training with noisy data. First, we propose a loss based on the optimal transport distance between the observed dynamics and the neural operator outputs. This approach requires expert knowledge of the underlying physics to determine 
    
[^109]: 过拟合的模型会泄露预训练数据的隐私信息

    TMI! Finetuned Models Leak Private Information from their Pretraining Data. (arXiv:2306.01181v1 [cs.LG])

    [http://arxiv.org/abs/2306.01181](http://arxiv.org/abs/2306.01181)

    本文提出了一种新的会员推断威胁模型TMI，用于评估微调模型对预训练数据的泄露，突显了在使用预训练模型进行迁移学习中存在的隐私风险，并需要对机器学习中的隐私进行更严格的评估。

    

    迁移学习已成为机器学习中越来越流行的技术，用于利用为一个任务训练的预训练模型来协助构建相关任务的微调模型。该范例在隐私机器学习方面尤其受欢迎，其中预训练模型被认为是公开的，只有微调数据被视为敏感的。然而，有理由认为用于预训练的数据仍然是敏感的，因此必须了解微调模型泄露有关预训练数据的信息量。本文提出了一种新的会员推理威胁模型，其中对手只能访问已经微调好的模型，并想推断预训练数据的成员资格。为了实现这个威胁模型，我们实施了一种新型的基于元分类器的攻击TMI，它利用了在下游任务中记忆的预训练样本对预测的影响。我们在视觉和自然语言处理任务上评估了TMI，并表明它在仅使用微调模型的情况下实现了高精度的推断预训练数据的成员资格。我们的结果突显了在迁移学习中使用预训练模型可能存在的隐私风险，以及需要对机器学习中的隐私进行更严格的评估的需求。

    Transfer learning has become an increasingly popular technique in machine learning as a way to leverage a pretrained model trained for one task to assist with building a finetuned model for a related task. This paradigm has been especially popular for privacy in machine learning, where the pretrained model is considered public, and only the data for finetuning is considered sensitive. However, there are reasons to believe that the data used for pretraining is still sensitive, making it essential to understand how much information the finetuned model leaks about the pretraining data. In this work we propose a new membership-inference threat model where the adversary only has access to the finetuned model and would like to infer the membership of the pretraining data. To realize this threat model, we implement a novel metaclassifier-based attack, TMI, that leverages the influence of memorized pretraining samples on predictions in the downstream task. We evaluate TMI on both vision and na
    
[^110]: 神经理想大涡模拟：用神经随机微分方程模拟湍流

    Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations. (arXiv:2306.01174v1 [cs.LG])

    [http://arxiv.org/abs/2306.01174](http://arxiv.org/abs/2306.01174)

    本文提出了niLES方法，将理想大涡模拟（LES）和神经随机微分方程（SDE）进行了整合，用于模拟难以处理的湍流现象。

    

    本文提出了一种数据驱动的学习框架，融合了两个强大的思想：来自湍流封闭模型的理想大涡模拟（LES）和用于随机建模的神经随机微分方程（SDE）。理想LES通过将每个完整轨迹视为潜在动力学的随机实现来对LES流进行建模，从而将小尺度效应边际化以获得LES状态的确定性演化。但是，理想LES在分析上是难以处理的。在本文中，我们使用潜在的神经SDE来模拟随机过程的演化，并使用编码器-解码器对潜在空间和所需的理想流场之间进行转换。这与其他类型的闭合模型神经参数化的处理方式形成了鲜明的对比，其他处理方式将每个轨迹视为动力学的确定实现。我们在具有挑战性的混沌动力学系统Kolmogorov flow上展示了我们方法（niLES - 神经理想LES）的有效性。

    We introduce a data-driven learning framework that assimilates two powerful ideas: ideal large eddy simulation (LES) from turbulence closure modeling and neural stochastic differential equations (SDE) for stochastic modeling. The ideal LES models the LES flow by treating each full-order trajectory as a random realization of the underlying dynamics, as such, the effect of small-scales is marginalized to obtain the deterministic evolution of the LES state. However, ideal LES is analytically intractable. In our work, we use a latent neural SDE to model the evolution of the stochastic process and an encoder-decoder pair for transforming between the latent space and the desired ideal flow field. This stands in sharp contrast to other types of neural parameterization of closure models where each trajectory is treated as a deterministic realization of the dynamics. We show the effectiveness of our approach (niLES - neural ideal LES) on a challenging chaotic dynamical system: Kolmogorov flow a
    
[^111]: 面向边缘人工智能的集成感知-通信-计算 （Integrated Sensing-Communication-Computation） （arXiv：2306.01162v1 [cs.IT]）

    Integrated Sensing-Communication-Computation for Edge Artificial Intelligence. (arXiv:2306.01162v1 [cs.IT])

    [http://arxiv.org/abs/2306.01162](http://arxiv.org/abs/2306.01162)

    面向边缘人工智能的集成感知-通信-计算技术对于提高资源利用率以及实现边缘学习和边缘人工智能推理任务的定制目标至关重要。

    

    边缘人工智能是实现万物智能的一种有前途的解决方案，可用于数字孪生、全息投影、语义通信和自动驾驶等高级技术。边缘人工智能任务的性能，包括边缘学习和边缘人工智能推理，取决于三个高度耦合的过程的质量，即数据获取的感知、信息提取的计算和信息传输的通信。然而，这三个模块需要为增强自己的服务质量而竞争网络资源。为此，集成感知-通信-计算（ISCC）对于提高资源利用率以及实现边缘人工智能任务的定制目标至关重要。通过研究三个模块之间的相互作用，本文提出了各种 ISCC 方案，适用于联邦边缘学习任务和边缘人工智能推理任务。

    Edge artificial intelligence (AI) has been a promising solution towards 6G to empower a series of advanced techniques such as digital twin, holographic projection, semantic communications, and auto-driving, for achieving intelligence of everything. The performance of edge AI tasks, including edge learning and edge AI inference, depends on the quality of three highly coupled processes, i.e., sensing for data acquisition, computation for information extraction, and communication for information transmission. However, these three modules need to compete for network resources for enhancing their own quality-of-services. To this end, integrated sensing-communication-computation (ISCC) is of paramount significance for improving resource utilization as well as achieving the customized goals of edge AI tasks. By investigating the interplay among the three modules, this article presents various kinds of ISCC schemes for federated edge learning tasks and edge AI inference tasks in both applicati
    
[^112]: 通过稀疏Flash注意力加速处理大序列中的因果关系

    Faster Causal Attention Over Large Sequences Through Sparse Flash Attention. (arXiv:2306.01160v1 [cs.LG])

    [http://arxiv.org/abs/2306.01160](http://arxiv.org/abs/2306.01160)

    本文介绍了一种新的稀疏Flash注意力机制，能够快速处理大序列中的因果关系，且速度提高了多倍，可以作为任何基于Transformer的语言模型中密集自我注意力的替代方案，并在多个设置中产生了最先进的结果。

    

    基于Transformer的语言模型已经在处理越来越长序列的任务中被广泛应用。对于这些应用，序列长度关于的因果自注意力成为一个核心问题，因为它是唯一一个与序列长度呈二次关系的组件。虽然许多研究已经提出方案来使自注意力的注意力模式稀疏化，但这些方案通常受到实现方面的限制，并最终强加一个简单且静态的结构在关注矩阵上。相反，实现更动态的稀疏注意力通常会导致运行时间显着慢于使用Dao等人（2022）的Flash实现计算完整注意力。我们扩展了FlashAttention，以适应包含键/查询丢弃和基于哈希的注意力在内的大类稀疏注意性模式。这导致实现没有任何计算复杂度开销，并且与以前的动态稀疏注意性相比，速度提高了多倍。我们的方法可用作任何基于Transformer的语言模型中密集自我注意力的替代方案，并在多个设置中产生了最先进的结果，包括生成语言建模和长格式问答。

    Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtim
    
[^113]: 基于异构知识的增强模块化强化学习

    Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])

    [http://arxiv.org/abs/2306.01158](http://arxiv.org/abs/2306.01158)

    该论文提出了增强模块化强化学习（AMRL），使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。该方法能够减缓强化学习中的一些低效问题，有望在深度强化学习领域得到应用。

    

    为了减缓强化学习中的一些低效问题，学者们提出了模块化方法，将不同的决策制定策略组合起来以衍生出可以执行多种任务的代理。这些体系结构的基础模块通常是可重复使用的，也允许“即插即用”的集成。然而，这些解决方案仍然缺乏处理和整合多种类型信息（知识）的能力，例如规则，子目标和技能。我们提出了增强模块化强化学习（AMRL）来解决这些限制。这种新的框架使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。此外，我们引入了一种选择机制的变体，即增强记忆的仲裁器，它增加了利用时间信息的能力。我们在已有的环境中评估了所提出的机制，同时也在新环境中进行了基准测试，结果表明其在深度强化学习领域具有良好的应用前景。

    In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
    
[^114]: 非可识别的隐变量下的Delphic离线强化学习

    Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding. (arXiv:2306.01157v1 [cs.LG])

    [http://arxiv.org/abs/2306.01157](http://arxiv.org/abs/2306.01157)

    本文提出了一种名为Delphic不确定性的方法来解决离线强化学习中的隐藏混淆问题，并通过实验表明该方法在非可识别的隐藏混淆情况下优于现有算法。

    

    离线强化学习中的一个突出挑战是隐藏的混淆问题：未观察到的变量可能影响到智能体采取的行动和观察到的结果。隐藏的混淆可能损害从数据中得出的任何因果结论的有效性，并且是有效的离线强化学习的一个主要障碍。在这篇论文中，我们解决了非可识别设置中的隐藏混淆问题。我们提出了一种基于与观察兼容的世界模型的差异来定义由隐藏混淆偏差引起的不确定性，称为Delphic不确定性，并将其与众所周知的认知和随机不确定性区分开来。我们导出了一种实际方法来估计这三种类型的不确定性，并构建了一种悲观的离线强化学习算法来解决它们。我们的方法不假定未观察到的混淆因子是可识别的，并且试图减少混淆偏差的量。通过广泛的实验表明，我们的方法在存在非可识别的隐藏混淆时优于现有的离线强化学习算法。

    A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive ex
    
[^115]: 梯度下降中的简约定律与深度线性网络学习

    The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks. (arXiv:2306.01154v1 [cs.LG])

    [http://arxiv.org/abs/2306.01154](http://arxiv.org/abs/2306.01154)

    研究发现梯度下降对于简约解有偏好，针对深度线性网络数据具有低维结构时，从正交初始化开始，演变只会影响权重矩阵的少数奇异空间向量，说明学习过程仅发生在每个权重矩阵的一个小不变空间内，深度网络的学习动态存在简化。

    

    过去几年来，对于训练深度网络的一个广泛研究现象是梯度下降对简约解的隐式偏差。在本文中，我们通过将焦点缩小到深度线性网络来研究这种现象。通过我们的分析，我们揭示了在数据具有低维结构时学习动态中的一个惊人的“简约定律”。具体来说，我们发现从正交初始化开始的梯度下降的演变仅影响所有权重矩阵的极小部分奇异向量空间。换句话说，尽管所有权重参数在训练过程中都会更新，但学习过程仅发生在每个权重矩阵的一个小的不变子空间内。这种学习动态的简单性可能对有效训练和更好地理解深度网络都有重要影响。

    Over the past few years, an extensively studied phenomenon in training deep networks is the implicit bias of gradient descent towards parsimonious solutions. In this work, we investigate this phenomenon by narrowing our focus to deep linear networks. Through our analysis, we reveal a surprising "law of parsimony" in the learning dynamics when the data possesses low-dimensional structures. Specifically, we show that the evolution of gradient descent starting from orthogonal initialization only affects a minimal portion of singular vector spaces across all weight matrices. In other words, the learning process happens only within a small invariant subspace of each weight matrix, despite the fact that all weight parameters are updated throughout training. This simplicity in learning dynamics could have significant implications for both efficient training and a better understanding of deep networks. First, the analysis enables us to considerably improve training efficiency by taking advanta
    
[^116]: 神经网络中语义和视觉对齐的差异问题解决方法研究

    Addressing Discrepancies in Semantic and Visual Alignment in Neural Networks. (arXiv:2306.01148v1 [cs.CV])

    [http://arxiv.org/abs/2306.01148](http://arxiv.org/abs/2306.01148)

    本文提出了一种数据增强技术，通过语义混合实现了更好地将语义类别与非视觉语义相关性对齐，提高了神经网络中语义类别的对齐度。

    

    在图像分类任务中，神经网络主要依赖视觉模式。在强健的网络中，我们期望视觉相似的类别具有相似的表示形式。本文考虑当语义类别相似而视觉类别不同，以及当视觉类别相似但语义类别不同的情况。我们提出一种数据增强技术，旨在更好地将语义类别与任意（非视觉）语义相关性对齐。我们利用最近的基于扩散的语义混合工作来生成两个类别的语义混合体，并将这些混合体作为增强数据添加到训练集中。我们评估该方法是否增加了语义对齐，通过在对抗扰动数据上评估模型性能，即一位攻击者可以更容易地将一个类别转换为相似表示的类别。结果表明，使用我们的方法会增加语义类别的对齐度。

    For the task of image classification, neural networks primarily rely on visual patterns. In robust networks, we would expect for visually similar classes to be represented similarly. We consider the problem of when semantically similar classes are visually dissimilar, and when visual similarity is present among non-similar classes. We propose a data augmentation technique with the goal of better aligning semantically similar classes with arbitrary (non-visual) semantic relationships. We leverage recent work in diffusion-based semantic mixing to generate semantic hybrids of two classes, and these hybrids are added to the training set as augmented data. We evaluate whether the method increases semantic alignment by evaluating model performance on adversarially perturbed data, with the idea that it should be easier for an adversary to switch one class to a similarly represented class. Results demonstrate that there is an increase in alignment of semantically similar classes when using our
    
[^117]: 平滑单调网络

    Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])

    [http://arxiv.org/abs/2306.01147](http://arxiv.org/abs/2306.01147)

    本文提出了一种新的神经网络模块--平滑min-max(SMM)网络，相比于传统的min-max(MM)神经网络结构简单易用，在单调建模方面表现优异。

    

    单调性约束是统计建模中的强力正则化工具。它们可以在计算机支持的决策制定中支持公平性，并增加数据驱动科学模型的可信度。经典的min-max(MM)神经网络结构确保了单调性，但由于梯度消失而往往在训练过程中陷入不良局部最优。我们提出了对MM网络的简单修改，使用严格递增的平滑非线性函数来缓解这个问题。得到的平滑min-max(SMM)网络模块继承了MM架构的渐近逼近性质。它可以嵌入到更大的端到端深度学习系统中进行训练。在单调建模的神经网络方面，SMM模块要简单得多，计算需求也要少得多。尽管如此，在我们的实验中，它在泛化性能方面与替代神经和非神经方法相比表现得更为优异。

    Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
    
[^118]: 用合成任务数据评估多模态推理模型的能力

    Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data. (arXiv:2306.01144v1 [cs.LG])

    [http://arxiv.org/abs/2306.01144](http://arxiv.org/abs/2306.01144)

    本研究提出一种利用合成方法生成用于评估复杂多模态推理任务的数据集，并将其应用于测试最新的视觉问答模型的表现，结果表明该模型在上下文相关的异常检测任务上表现不佳。

    

    大语言和联合语言-视觉理解模型的显着进展和应用增加了对探测其潜在推理能力方法的需求。然而，对于未被学术数据集涵盖的复杂多模态推理任务，收集自然数据的困难制约了对AI方法的评估。在本研究中，我们利用高分辨率文本-图像生成的最新进展，开发了一个用于为多模态推理任务生成评估数据的框架。我们将此框架应用于生成上下文相关的异常数据，创建了一个具有挑战性的合成数据集，该数据集在现有数据集中覆盖不足。我们对最先进的视觉问答（VQA）模型在使用此方法生成的数据上的表现进行了基准测试，并表明尽管任务是可行的，但该模型在上下文相关的异常检测任务上的表现显著低于现有的学术数据集。我们的工作突显了考虑合成任务以评估和理解AI模型的潜在能力和限制的重要性。

    The impressive advances and applications of large language and joint language-and-visual understanding models has led to an increased need for methods of probing their potential reasoning capabilities. However, the difficulty of gather naturally-occurring data for complex multi-modal reasoning tasks bottlenecks the evaluation of AI methods on tasks which are not already covered by an academic dataset. In this work, we leverage recent advances in high resolution text-to-image generation to develop a framework for generating evaluation data for multi-modal reasoning tasks. We apply this framework to generate context-dependent anomaly data, creating a synthetic dataset on a challenging task which is not well covered by existing datasets. We benchmark the performance of a state-of-the-art visual question answering (VQA) model against data generated with this method, and demonstrate that while the task is tractable, the model performs significantly worse on the context-dependent anomaly det
    
[^119]: 联邦图学习应用于无线自组网低探测概率问题

    Federated Graph Learning for Low Probability of Detection in Wireless Ad-Hoc Networks. (arXiv:2306.01143v1 [cs.LG])

    [http://arxiv.org/abs/2306.01143](http://arxiv.org/abs/2306.01143)

    本文提出了一种联邦图学习框架，用于最小化无线自组网的探测概率，保护通信节点的隐私和安全。该方法可预测每个节点的最佳通信区域，表现良好。

    

    近期低探测概率技术被用于提升无线网络的隐私和安全性，相较于其他技术，它并非保护信息的传输，而是全面隐藏无线通信的存在。本文基于低探测概率通信，提出了一个基于图神经网络的分布式框架，以最小化整个无线自组网的探测概率并预测每个节点的最佳通信区域，使得节点在与外部无人发现的情况下进行通信。此外，本文还用平均绝对误差和中位数绝对误差两种性能指标来证明所提出方法的有效性。

    Low probability of detection (LPD) has recently emerged as a means to enhance the privacy and security of wireless networks. Unlike existing wireless security techniques, LPD measures aim to conceal the entire existence of wireless communication instead of safeguarding the information transmitted from users. Motivated by LPD communication, in this paper, we study a privacy-preserving and distributed framework based on graph neural networks to minimise the detectability of a wireless ad-hoc network as a whole and predict an optimal communication region for each node in the wireless network, allowing them to communicate while remaining undetected from external actors. We also demonstrate the effectiveness of the proposed method in terms of two performance measures, i.e., mean absolute error and median absolute error.
    
[^120]: 一种用于解决路径依赖偏微分方程的神经RDE模型

    A Neural RDE-based model for solving path-dependent PDEs. (arXiv:2306.01123v1 [cs.LG])

    [http://arxiv.org/abs/2306.01123](http://arxiv.org/abs/2306.01123)

    本文提出了一种基于神经粗糙微分方程(NRDE)的模型来学习和求解路径依赖偏微分方程(PPDE)，通过记录标记特征来编码路径信息，且能够高效地利用内存和随着维度的扩展能力，这一方法在数值实验中被证明比文献中的强基线模型更有效。

    

    路径依赖偏微分方程(PPDE)最初是在金融市场中的路径依赖衍生品中引入的概念。 线性形式后被确定为非马尔可夫的反向随机微分方程(BSDE)。 与经典PDE相比，PPDE的解涉及无限维空间变量，这使得它难以逼近，甚至是不可能的。 在本文中，我们提出了一种基于神经粗糙微分方程(NRDE)的模型来学习PPDE，通过记录标记特征来有效地编码路径信息，并捕捉基本动态。 提出的PPDE解的连续时间模型提供了高效的内存使用和随维度扩展的能力的好处。 通过几个数值实验，与文献中的强基线模型进行比较，证明了所提出模型的有效性。

    The concept of the path-dependent partial differential equation (PPDE) was first introduced in the context of path-dependent derivatives in financial markets. Its semilinear form was later identified as a non-Markovian backward stochastic differential equation (BSDE). Compared to the classical PDE, the solution of a PPDE involves an infinite-dimensional spatial variable, making it challenging to approximate, if not impossible. In this paper, we propose a neural rough differential equation (NRDE)-based model to learn PPDEs, which effectively encodes the path information through the log-signature feature while capturing the fundamental dynamics. The proposed continuous-time model for the PPDE solution offers the benefits of efficient memory usage and the ability to scale with dimensionality. Several numerical experiments, provided to validate the performance of the proposed model in comparison to the strong baseline in the literature, are used to demonstrate its effectiveness.
    
[^121]: 论坐标上升变分推断的收敛性问题

    On the Convergence of Coordinate Ascent Variational Inference. (arXiv:2306.01122v1 [stat.ML])

    [http://arxiv.org/abs/2306.01122](http://arxiv.org/abs/2306.01122)

    本文通过分析常见的坐标上升变分推断（CAVI）算法在两个块的情况下的收敛性，提供了证明全局或局部指数收敛的一般条件。

    

    变分推断（VI）作为马尔科夫链蒙特卡洛方法的一种计算替代方法，由于其可比较的功效和卓越的效率，在大规模贝叶斯模型中用于近似难以计算的后验分布越来越受欢迎。尽管有几项最近的工作通过证明在不同设置下VI在参数估计方面的统计最优性，为VI提供了理论证据，但对VI算法收敛性方面的形式化分析仍然缺乏。在本文中，我们考虑了常见的坐标上升变分推断（CAVI）算法，以实现均值场（MF）VI，并优化所有分解分布空间上的KL散度目标功能。我们通过利用函数分析和优化的广泛工具箱，重点关注两个块的情况，分析CAVI的收敛性。我们提供了证明全局或局部指数收敛的一般条件。

    As a computational alternative to Markov chain Monte Carlo approaches, variational inference (VI) is becoming more and more popular for approximating intractable posterior distributions in large-scale Bayesian models due to its comparable efficacy and superior efficiency. Several recent works provide theoretical justifications of VI by proving its statistical optimality for parameter estimation under various settings; meanwhile, formal analysis on the algorithmic convergence aspects of VI is still largely lacking. In this paper, we consider the common coordinate ascent variational inference (CAVI) algorithm for implementing the mean-field (MF) VI towards optimizing a Kullback--Leibler divergence objective functional over the space of all factorized distributions. Focusing on the two-block case, we analyze the convergence of CAVI by leveraging the extensive toolbox from functional analysis and optimization. We provide general conditions for certifying global or local exponential converg
    
[^122]: 带有重尾奖励的差分隐私式情节强化学习

    Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards. (arXiv:2306.01121v1 [cs.LG])

    [http://arxiv.org/abs/2306.01121](http://arxiv.org/abs/2306.01121)

    本研究针对重尾奖励的有限步骤表格马尔可夫决策过程问题探讨了差分隐私限制下的两种框架，即价值迭代和策略优化，同时考虑了联合差分隐私和本地差分隐私模型，并为两种情况提供了遗憾上限。

    

    本文研究了差分隐私(DP)限制下的重尾奖励的（有限步骤表格）马尔可夫决策过程(MDP)问题。与先前的私有强化学习研究通常假设奖励来自一些有界或次高斯分布以确保DP相比，我们考虑奖励分布只有有限的$(1+v)$阶矩的情况，$v \in (0,1]$。通过使用奖励的健壮均值估计器，我们首先提出了两种针对重尾MDP的框架，即一个用于价值迭代，另一个用于策略优化。在每个框架下，我们考虑了联合差分隐私(JDP)和本地差分隐私(LDP)模型。基于我们的框架，我们为JDP和LDP情况提供了遗憾上限，并表明分布的矩和隐私预算都对遗憾有重要影响。最后，我们建立了遗憾最小化的下限。

    In this paper, we study the problem of (finite horizon tabular) Markov decision processes (MDPs) with heavy-tailed rewards under the constraint of differential privacy (DP). Compared with the previous studies for private reinforcement learning that typically assume rewards are sampled from some bounded or sub-Gaussian distributions to ensure DP, we consider the setting where reward distributions have only finite $(1+v)$-th moments with some $v \in (0,1]$. By resorting to robust mean estimators for rewards, we first propose two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and another is for policy optimization. Under each framework, we consider both joint differential privacy (JDP) and local differential privacy (LDP) models. Based on our frameworks, we provide regret upper bounds for both JDP and LDP cases and show that the moment of distribution and privacy budget both have significant impacts on regrets. Finally, we establish a lower bound of regret minimization
    
[^123]: 如何用时空上下文丰富日前太阳辐射时序预测？

    What if We Enrich day-ahead Solar Irradiance Time Series Forecasting with Spatio-Temporal Context?. (arXiv:2306.01112v1 [cs.LG])

    [http://arxiv.org/abs/2306.01112](http://arxiv.org/abs/2306.01112)

    本文提出了一种深度学习框架，使用卫星数据捕捉时空上下文信息，实现了对太阳辐射时序的高精度日前预测，表现优于不采用卫星数据的时序和机器学习模型，可帮助更有效地将太阳能融入电网。

    

    太阳能潜力巨大，可有效减少CO2排放以缓解气候变化。然而，太阳辐射的固有变异性给无缝融入电网带来了重大挑战。本文提出了一种深度学习框架，利用卫星数据来捕捉时空上下文信息，以实现对当地任何给定站点高精度的日前时序预测，特别强调了对全球水平辐射（GHI）的预测。此外，我们还提出了一种提取每个时间步预测分布的方法，可作为预测不确定度的有价值度量。我们在美国的三个站点上使用数据评估模型，并展示了引入时空上下文所带来的显著性能提升，表明这种方法胜过不采用卫星数据的时序和机器学习模型。

    Solar power harbors immense potential in mitigating climate change by substantially reducing CO$_{2}$ emissions. Nonetheless, the inherent variability of solar irradiance poses a significant challenge for seamlessly integrating solar power into the electrical grid. While the majority of prior research has centered on employing purely time series-based methodologies for solar forecasting, only a limited number of studies have taken into account factors such as cloud cover or the surrounding physical context. In this paper, we put forth a deep learning architecture designed to harness spatio-temporal context using satellite data, to attain highly accurate \textit{day-ahead} time-series forecasting for any given station, with a particular emphasis on forecasting Global Horizontal Irradiance (GHI). We also suggest a methodology to extract a distribution for each time step prediction, which can serve as a very valuable measure of uncertainty attached to the forecast. When evaluating models,
    
[^124]: 基于机器学习的焦虑检测中噪声影响的比较研究。

    Comparative Study on the Effects of Noise in ML-Based Anxiety Detection. (arXiv:2306.01110v1 [cs.LG])

    [http://arxiv.org/abs/2306.01110](http://arxiv.org/abs/2306.01110)

    本研究探究了噪声如何影响基于机器学习的焦虑检测模型，并开发出在嘈杂的现实环境中具有抗干扰性和适应性的模型，以推进该领域的发展。

    

    穿戴式健康设备正在引领一种新时代的连续和非侵入性远程监测。其中一项应用是用于焦虑检测。许多焦虑检测方面的进展发生在受控实验室环境中，但噪声阻止了这些进展推广到现实世界的条件下。本研究旨在研究噪声如何影响模型性能并开发对嘈杂的现实环境具有抗干扰性和适应日常生活中混乱的模型，从而推进该领域的发展。我们尝试研究先前的方法为何失败，并使用可穿戴负荷与情感检测（WESAD）数据集，在三类分类问题（基准值 vs. 压力 vs. 愉悦）中比较不同强度噪声对机器学习模型分类生理唤醒等级的影响。在引入噪声之前，我们基准模型的性能达到了98.7％，而Schmidt 2018年的模型仅达到了80.3％。我们讨论了可能的解决方法。

    Wearable health devices are ushering in a new age of continuous and noninvasive remote monitoring. One application of this technology is in anxiety detection. Many advancements in anxiety detection have happened in controlled lab settings, but noise prevents these advancements from generalizing to real-world conditions. We seek to progress the field by studying how noise impacts model performance and developing models that are robust to noisy, real-world conditions and, hence, attuned to the commotion of everyday life. In this study we look to investigate why and how previous methods have failed. Using the wearable stress and affect detection (WESAD) dataset, we compare the effect of various intensities of noise on machine learning models classifying levels of physiological arousal in the three-class classification problem: baseline vs. stress vs. amusement. Before introducing noise, our baseline model performance reaches 98.7%, compared to Schmidt 2018's 80.3%. We discuss potential so
    
[^125]: 在图形的超出分布泛化中学习标签和环境因果独立性

    Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])

    [http://arxiv.org/abs/2306.01103](http://arxiv.org/abs/2306.01103)

    本文提出了一种考虑标签和环境因果独立性的方法来解决图形超出分布（OOD）泛化问题，通过敌对训练策略来联合优化属性以获得有效结果，实验证明LECI显着优于之前的方法。

    

    我们解决了图形的超出分布（OOD）泛化问题。现有的图形OOD算法要么依赖于受限的假设，要么无法利用训练数据中的环境信息。在这项工作中，我们提出同时纳入标签和环境因果独立（LECI），充分利用标签和环境信息，从而解决之前的方法在识别因果和不变子图时面临的挑战。我们进一步开发了一种敌对训练策略，以联合优化这两个属性，用于具有理论保证的导致子图发现。广泛的实验和分析表明，LECI在合成和真实数据集上都显着优于之前的方法，将LECI确立为图形OOD泛化的实用有效解决方案。

    We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
    
[^126]: ALO-VC: 任意对任意低延迟一次性语音转换

    ALO-VC: Any-to-any Low-latency One-shot Voice Conversion. (arXiv:2306.01100v1 [eess.AS])

    [http://arxiv.org/abs/2306.01100](http://arxiv.org/abs/2306.01100)

    本文提出了ALO-VC，一种任意对任意低延迟一次性语音转换方法，它使用仅为目标说话人提供的一个话语，包括预训练说话人编码器、音高预测器和位置编码器，实现了与非因果基线系统相当的性能。

    

    本文介绍了一种基于非平行音素后验图（PPG）的任意对任意低延迟一次性语音转换方法——ALO-VC。使用只有47.5毫秒未来预测的目标说话人单个话语即可实现任意对任意语音转换。所提出的混合信号处理和机器学习流程结合了预训练的说话人编码器，预测所转换语音的韵律的音高预测器以及位置编码来传达音素的位置信息。我们介绍了两个系统版本：ALO-VC-R使用预训练的d-vector说话人编码器，ALO-VC-E使用ECAPA-TDNN说话人编码器提高性能。实验结果表明，ALO-VC-R和ALO-VC-E在VCTK数据集和两个领域外数据集上均可实现与非因果基线系统相当的性能。此外，两个所提出的系统可以在单个CPU核上部署，具有55毫秒的延迟和0.78的实时因素。我们的演示可在网上访问。

    This paper presents ALO-VC, a non-parallel low-latency one-shot phonetic posteriorgrams (PPGs) based voice conversion method. ALO-VC enables any-to-any voice conversion using only one utterance from the target speaker, with only 47.5 ms future look-ahead. The proposed hybrid signal processing and machine learning pipeline combines a pre-trained speaker encoder, a pitch predictor to predict the converted speech's prosody, and positional encoding to convey the phoneme's location information. We introduce two system versions: ALO-VC-R, which uses a pre-trained d-vector speaker encoder, and ALO-VC-E, which improves performance using the ECAPA-TDNN speaker encoder. The experimental results demonstrate both ALO-VC-R and ALO-VC-E can achieve comparable performance to non-causal baseline systems on the VCTK dataset and two out-of-domain datasets. Furthermore, both proposed systems can be deployed on a single CPU core with 55 ms latency and 0.78 real-time factor. Our demo is available online.
    
[^127]: 大批量神经多目标贝叶斯优化

    Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])

    [http://arxiv.org/abs/2306.01095](http://arxiv.org/abs/2306.01095)

    本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。

    

    贝叶斯优化在全局优化黑盒高成本函数方面提供了强大的框架。然而，由于默认高斯过程代理的可扩展性差，它在处理数据密集型问题，特别是在多目标设置中的能力有限。本文提出了一种新颖的贝叶斯优化框架，专为解决这些限制而设计。我们的方法利用了贝叶斯神经网络方法进行代理建模。这使得它能够有效地处理大批量数据，建模复杂问题以及产生预测的不确定性。此外，我们的方法结合了一种基于众所周知且易于部署的NSGA-II的可扩展的、具有不确定性的收购策略。这种完全可并行化的策略促进了未勘探区域的有效探索。我们的框架允许在最少迭代次数的情况下在数据密集环境中进行有效的优化。我们展示了我们方法的优越性。

    Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
    
[^128]: 基于结构相似度度量的半监督社区检测

    Semi-supervised Community Detection via Structural Similarity Metrics. (arXiv:2306.01089v1 [cs.SI])

    [http://arxiv.org/abs/2306.01089](http://arxiv.org/abs/2306.01089)

    本文提出了一种基于结构相似度指标的快速半监督社区检测算法，并在理论与实验方面均有良好表现。

    

    本文研究了半监督社区检测问题，旨在使用网络拓扑结构和已观察到的节点标签部分推断新节点的社区标签。文章提出了计算新节点与$K$个社区之间“结构相似度指标”的算法，能够用标记和未标记数据对其进行聚合，从而预测新节点的标签。该方法快速且数值上优于现有的半监督算法。理论上，文章导出了误分类误差的明确界限，并通过与理想分类器的比较展示了我们方法的效率。

    Motivated by social network analysis and network-based recommendation systems, we study a semi-supervised community detection problem in which the objective is to estimate the community label of a new node using the network topology and partially observed community labels of existing nodes. The network is modeled using a degree-corrected stochastic block model, which allows for severe degree heterogeneity and potentially non-assortative communities. We propose an algorithm that computes a `structural similarity metric' between the new node and each of the $K$ communities by aggregating labeled and unlabeled data. The estimated label of the new node corresponds to the value of $k$ that maximizes this similarity metric. Our method is fast and numerically outperforms existing semi-supervised algorithms. Theoretically, we derive explicit bounds for the misclassification error and show the efficiency of our method by comparing it with an ideal classifier. Our findings highlight, to the best
    
[^129]: 层次化注意力编码器解码器

    Hierarchical Attention Encoder Decoder. (arXiv:2306.01070v1 [cs.LG])

    [http://arxiv.org/abs/2306.01070](http://arxiv.org/abs/2306.01070)

    本论文提出了一种基于分层循环编码器解码器（HRED）架构的模型，可以独立地对输入子序列进行编码，在较低频率模型中处理这些序列，并在原始数据频率下解码输出，同时减少计算时间和内存使用。

    

    最近，大型语言模型的进展表明，自回归建模可以生成具有许多现实应用的复杂和新颖的序列。然而，这些模型必须自回归地生成输出，当处理长序列时，这变得耗时。已经提出了压缩数据的分层自回归方法作为解决方案，但是这些方法仍然在原始数据频率下生成输出，导致模型速度慢和内存占用高。在本文中，我们提出一种基于分层循环编码器解码器（HRED）架构的模型。该模型独立地对输入子序列进行编码，在较低频率模型中处理这些序列，并在原始数据频率下解码输出。通过将编码器解释为隐式定义的嵌入矩阵并使用采样softmax估计，我们开发了一种训练算法，可以训练整个模型而不需要高频解码器，从而减少计算时间和内存使用。在文本生成任务上的实验表明，与现有方法相比，我们提出的模型可以生成高质量的长序列，并显著减少训练时间和内存要求。

    Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, wh
    
[^130]: 基于深度强化学习的莫里斯水迷宫导航策略研究

    Investigating Navigation Strategies in the Morris Water Maze through Deep Reinforcement Learning. (arXiv:2306.01066v1 [cs.LG])

    [http://arxiv.org/abs/2306.01066](http://arxiv.org/abs/2306.01066)

    本文利用深度强化学习代理程序模拟了莫里斯水迷宫，通过自动分类导航策略，分析人工智能代理程序使用的策略分布，并与实验结果做对比。该研究发现了类似于人和啮齿类动物的学习规律，发展了内部表征与导航策略之间的关联，为生物学特征的探索提供了一定的参考。

    

    导航是一项复杂的技能，其在动物和人类的研究历史悠久。本文通过模拟2D的莫里斯水迷宫，训练深度强化学习代理程序，自动分类导航策略，分析人工智能代理程序使用的策略分布，并与实验数据进行比较，展示了与人类和啮齿类动物相似的学习动态。同时，我们开发了特定于环境的辅助任务，并研究对其有用性的影响因素。我们认为，最具益处的任务可能更符合真实代理使用的生物学特征。最后，我们探讨了代理程序神经网络激活中内部表征的发展。这些表征类似于发现于鼠脑中的位置细胞和头方向细胞，其存在与代理程序使用的导航策略有关。

    Navigation is a complex skill with a long history of research in animals and humans. In this work, we simulate the Morris Water Maze in 2D to train deep reinforcement learning agents. We perform automatic classification of navigation strategies, analyze the distribution of strategies used by artificial agents, and compare them with experimental data to show similar learning dynamics as those seen in humans and rodents. We develop environment-specific auxiliary tasks and examine factors affecting their usefulness. We suggest that the most beneficial tasks are potentially more biologically feasible for real agents to use. Lastly, we explore the development of internal representations in the activations of artificial agent neural networks. These representations resemble place cells and head-direction cells found in mouse brains, and their presence has correlation to the navigation strategies that artificial agents employ.
    
[^131]: 单正多标签学习的伪标签方法

    Pseudo Labels for Single Positive Multi-Label Learning. (arXiv:2306.01034v1 [cs.LG])

    [http://arxiv.org/abs/2306.01034](http://arxiv.org/abs/2306.01034)

    本文提出一种伪标签方法，将单正数据转化为完全标记的数据进行训练，以解决单正多标签学习中缺失标签的问题。

    

    数据注释成本是多标签图像分类的一个重要障碍：必须为每个图像中的每个类别标记存在或不存在。单正多标签（SPML）学习提供了一种经济有效的解决方案，其中模型是在每个图像上训练一个正样本标签。因此，SPML是一个更具挑战性的领域，因为它需要处理缺失的标签。在这项工作中，我们提出了一种方法，通过伪造多标签来将单正数据转化为完全标记的数据。基本上，我们训练了一个在单正标签上的教师网络。然后，我们将教师模型在训练数据上的预测作为地面实况标签，以在完全标记的图像上训练学生网络。通过这种简单的方法，我们展示了学生模型所达到的性能接近使用实际完全标记图像训练的模型。

    The cost of data annotation is a substantial impediment for multi-label image classification: in every image, every category must be labeled as present or absent. Single positive multi-label (SPML) learning is a cost-effective solution, where models are trained on a single positive label per image. Thus, SPML is a more challenging domain, since it requires dealing with missing labels. In this work, we propose a method to turn single positive data into fully-labeled data: Pseudo Multi-Labels. Basically, a teacher network is trained on single positive labels. Then, we treat the teacher model's predictions on the training data as ground-truth labels to train a student network on fully-labeled images. With this simple approach, we show that the performance achieved by the student model approaches that of a model trained on the actual fully-labeled images.
    
[^132]: 自适应学习率下，大规模多智能体学习仍然存在混沌现象

    Chaos persists in large-scale multi-agent learning despite adaptive learning rates. (arXiv:2306.01032v1 [cs.LG])

    [http://arxiv.org/abs/2306.01032](http://arxiv.org/abs/2306.01032)

    多智能体学习中的大规模混杂博弈环境下，即使使用自适应学习率，混沌现象仍然存在。

    

    多智能体学习相对于单智能体学习更加困难、不稳定和不可预测。为了实现自我对弈平衡的收敛，许多专门的启发式方法和技术已经设计出来。其中一个备受赞誉的方法是使用动态自适应学习率。虽然这样的技术已知可以在小规模游戏中提高收敛保证，但在更大的智能体群体中分析这些技术就困难多了。最近的研究表明，即使只有两个策略，使用固定学习率的学习也会在智能体数量足够大的情况下变得混沌。在这项工作中，我们展示了即使对于普遍的乘法权重更新算法，大量混合策略的情况下，混沌现象在使用自适应学习率时仍然存在。在技术层面上，由于系统的非自治性质，我们的方法是基于扩散分析的。

    Multi-agent learning is intrinsically harder, more unstable and unpredictable than single agent optimization. For this reason, numerous specialized heuristics and techniques have been designed towards the goal of achieving convergence to equilibria in self-play. One such celebrated approach is the use of dynamically adaptive learning rates. Although such techniques are known to allow for improved convergence guarantees in small games, it has been much harder to analyze them in more relevant settings with large populations of agents. These settings are particularly hard as recent work has established that learning with fixed rates will become chaotic given large enough populations.In this work, we show that chaos persists in large population congestion games despite using adaptive learning rates even for the ubiquitous Multiplicative Weight Updates algorithm, even in the presence of only two strategies. At a technical level, due to the non-autonomous nature of the system, our approach g
    
[^133]: 无需精确标注: 基于不完全转录的弱监督自动语音识别算法

    Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts. (arXiv:2306.01031v1 [cs.CL])

    [http://arxiv.org/abs/2306.01031](http://arxiv.org/abs/2306.01031)

    本文提出了一种新颖的算法，用重视转录不确定性的方式解决语音识别中训练数据不完美的问题，提高了ASR系统的鲁棒性和准确性。

    

    本文提出了一种新颖的算法，用于使用不完美的训练数据构建自动语音识别(ASR)模型。转录不准确的语音是人工注释的语音语料库中普遍存在的问题，这会降低ASR模型的性能。我们提出了Bypass Temporal Classification(BTC)作为联结时序分类(Connectionist Temporal Classification, CTC)准则的一个扩展。BTC在训练过程中显式地编码了与转录相关的不确定性。这是通过增强训练图的灵活性来实现的，它被实现为加权有限状态转换器(WFST)组合。这种提出的算法可以提高ASR系统的鲁棒性和准确性，特别是在使用不完整转录的语音语料库时。我们的实现将成为开源软件。

    This paper presents a novel algorithm for building an automatic speech recognition (ASR) model with imperfect training data. Imperfectly transcribed speech is a prevalent issue in human-annotated speech corpora, which degrades the performance of ASR models. To address this problem, we propose Bypass Temporal Classification (BTC) as an expansion of the Connectionist Temporal Classification (CTC) criterion. BTC explicitly encodes the uncertainties associated with transcripts during training. This is accomplished by enhancing the flexibility of the training graph, which is implemented as a weighted finite-state transducer (WFST) composition. The proposed algorithm improves the robustness and accuracy of ASR systems, particularly when working with imprecisely transcribed speech corpora. Our implementation will be open-sourced.
    
[^134]: SPINEX：基于相似度的机器学习回归和分类任务的预测和可解释邻居探索

    SPINEX: Similarity-based Predictions and Explainable Neighbors Exploration for Regression and Classification Tasks in Machine Learning. (arXiv:2306.01029v1 [cs.LG])

    [http://arxiv.org/abs/2306.01029](http://arxiv.org/abs/2306.01029)

    SPINEX是一种基于相似度的可解释邻居探索算法，通过集成学习和特征交互分析，提高了机器学习算法的可解释性，并且在回归和分类任务中具有优异表现。

    

    机器学习领域近年来取得了重大进展。然而，许多现有算法缺乏可解释性，并且难以处理高维和不平衡数据。本文提出了SPINEX，一种新颖的基于相似性的可解释邻居探索算法，旨在解决这些限制。该算法将集成学习和特征交互分析结合起来，通过量化每个特征对预测的贡献并确定特征之间的交互作用，从而提高了算法的可解释性。为了评估SPINEX的性能，对59个合成数据集和真实数据集进行了广泛的回归和分类实验。结果表明，SPINEX在性能上达到了可比较的水平，并且在某些场景下可能优于常用的机器学习算法。同样的发现证明了SPINEX在解决高维和不平衡数据的挑战方面的有效性和竞争力。

    The field of machine learning (ML) has witnessed significant advancements in recent years. However, many existing algorithms lack interpretability and struggle with high-dimensional and imbalanced data. This paper proposes SPINEX, a novel similarity-based interpretable neighbor exploration algorithm designed to address these limitations. This algorithm combines ensemble learning and feature interaction analysis to achieve accurate predictions and meaningful insights by quantifying each feature's contribution to predictions and identifying interactions between features, thereby enhancing the interpretability of the algorithm. To evaluate the performance of SPINEX, extensive experiments on 59 synthetic and real datasets were conducted for both regression and classification tasks. The results demonstrate that SPINEX achieves comparative performance and, in some scenarios, may outperform commonly adopted ML algorithms. The same findings demonstrate the effectiveness and competitiveness of 
    
[^135]: 使用Tsetlin机器的在线学习的FPGA架构

    An FPGA Architecture for Online Learning using the Tsetlin Machine. (arXiv:2306.01027v1 [cs.LG])

    [http://arxiv.org/abs/2306.01027](http://arxiv.org/abs/2306.01027)

    本文提出了一种基于FPGA架构的在线学习系统，通过采用低复杂度的Tsetlin Machine机器学习算法，在运行时提供了离线和在线学习管理。这种架构能够按需进行训练，并且能够在操作期间交错地进行推理和训练。

    

    针对机器学习模型需要在无监督情况下不断演化的需求，本文提出了一种基于FPGA架构的在线学习系统，采用低复杂度的Tsetlin Machine机器学习算法。该架构提供了定制化的运行时学习管理，实现了芯片内部的离线和在线学习，能够在推理之前通过预分类数据在FPGA上进行按需训练，并且能够在操作期间交错地进行推理和训练。

    There is a need for machine learning models to evolve in unsupervised circumstances. New classifications may be introduced, unexpected faults may occur, or the initial dataset may be small compared to the data-points presented to the system during normal operation. Implementing such a system using neural networks involves significant mathematical complexity, which is a major issue in power-critical edge applications.  This paper proposes a novel field-programmable gate-array infrastructure for online learning, implementing a low-complexity machine learning algorithm called the Tsetlin Machine. This infrastructure features a custom-designed architecture for run-time learning management, providing on-chip offline and online learning. Using this architecture, training can be carried out on-demand on the \ac{FPGA} with pre-classified data before inference takes place. Additionally, our architecture provisions online learning, where training can be interleaved with inference during operatio
    
[^136]: PV2TEA：将视觉模态与基于文本的信息抽取相结合

    PV2TEA: Patching Visual Modality to Textual-Established Information Extraction. (arXiv:2306.01016v1 [cs.CL])

    [http://arxiv.org/abs/2306.01016](http://arxiv.org/abs/2306.01016)

    PV2TEA提出了一种基于编码器-解码器架构的信息抽取模型，在多模态注释困难的情况下解决了跨模态集成的问题，并提出了三种偏差降低方案。

    

    信息抽取（例如属性值提取）已被广泛研究和建模，但仅基于文本。然而，许多属性可以从基于图像的提取中受益，如颜色、形状、图案等。视觉模态长期以来一直未被充分利用，主要是由于多模态注释的难度。本文旨在将视觉模态与基于文本的属性信息提取器相结合。跨模态集成面临几个独特的挑战：（C1）图像和文本描述在样本内和样本间松散匹配；（C2）图像通常包含丰富的背景，可能会误导预测；（C3）来自基于文本的提取器的弱监督标签对于多模态训练存在偏差。我们提出了PV2TEA，这是一种编码器-解码器架构，配备了三种偏差降低方案：（S1）增强的标签平滑对比，以改进松散匹配的图像和文本的交叉模态对齐; （S2）注意力剪枝方案用于在保留正确信息的同时消除一些不必要的细节；（S3）基于对抗训练的可重组卷积自适应模块，以帮助消除来自文本提取器的偏差。

    Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute information extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established extractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-prunin
    
[^137]: 时间演化图的图级嵌入

    Graph-Level Embedding for Time-Evolving Graphs. (arXiv:2306.01012v1 [cs.LG])

    [http://arxiv.org/abs/2306.01012](http://arxiv.org/abs/2306.01012)

    本论文提出了一种针对动态网络的图级嵌入方法，并通过生成节点的时间上下文来训练语言模型，以生成低维度的图级表示，对于下游的图形相似性排序、图形同构和异常检测等任务具有重要的意义。

    

    图表示学习已经得到广泛的研究，从节点到图的粒度各不相同。虽然在节点级别表示方面的大部分工作都已经被研究，但是对于动态或时态网络的图级嵌入却鲜有研究。然而，在动态网络中学习低维度的图级表示对各种下游图检索任务非常重要，例如时间图相似性排序、时间图同构和异常检测。本文提出了一种新颖的方法来解决这个问题，即建立一个多层图并使用具有时间回溯的修改后随机游走生成节点的时间上下文，然后在这些上下文上训练“文档级”语言模型以生成图级嵌入。我们将所提出的模型在五个公开可用的数据集上进行了评估。

    Graph representation learning (also known as network embedding) has been extensively researched with varying levels of granularity, ranging from nodes to graphs. While most prior work in this area focuses on node-level representation, limited research has been conducted on graph-level embedding, particularly for dynamic or temporal networks. However, learning low-dimensional graph-level representations for dynamic networks is critical for various downstream graph retrieval tasks such as temporal graph similarity ranking, temporal graph isomorphism, and anomaly detection. In this paper, we present a novel method for temporal graph-level embedding that addresses this gap. Our approach involves constructing a multilayer graph and using a modified random walk with temporal backtracking to generate temporal contexts for the graph's nodes. We then train a "document-level" language model on these contexts to generate graph-level embeddings. We evaluate our proposed model on five publicly avai
    
[^138]: 基于二维单元模型的物理启发式机器学习预测铁钒液流电池性能

    Physics-informed machine learning of redox flow battery based on a two-dimensional unit cell model. (arXiv:2306.01010v1 [cs.LG])

    [http://arxiv.org/abs/2306.01010](http://arxiv.org/abs/2306.01010)

    本文提出了一种利用二维数学模型进行物理启发式神经网络预测铁钒液流电池性能的方法，可正确预测单元电压。

    

    本文提出了一种物理启发式神经网络(PINN)方法，通过一个二维数学模型来预测全钒液流电池的性能，并强制执行其物理约束条件。该二维模型包括6个控制方程和24个边界条件，详细描述了铁钒液流电池内发生的电化学反应、质量传输和流体力学过程。为了使用PINN方法解决2D模型，采用复合神经网络来逼近物种浓度和电位；根据电池系统的先验知识对输入和输出进行规范化处理；将处理后的控制方程和边界条件先缩放到1的数量级，然后采用自重法进一步平衡。数值结果表明，PINN能够正确地预测单元电压，但电位的预测具有类似于常数的偏移。

    In this paper, we present a physics-informed neural network (PINN) approach for predicting the performance of an all-vanadium redox flow battery, with its physics constraints enforced by a two-dimensional (2D) mathematical model. The 2D model, which includes 6 governing equations and 24 boundary conditions, provides a detailed representation of the electrochemical reactions, mass transport and hydrodynamics occurring inside the redox flow battery. To solve the 2D model with the PINN approach, a composite neural network is employed to approximate species concentration and potentials; the input and output are normalized according to prior knowledge of the battery system; the governing equations and boundary conditions are first scaled to an order of magnitude around 1, and then further balanced with a self-weighting method. Our numerical results show that the PINN is able to predict cell voltage correctly, but the prediction of potentials shows a constant-like shift. To fix the shift, th
    
[^139]: 探究生成语言模型的演绎推理能力

    Examining the Emergence of Deductive Reasoning in Generative Language Models. (arXiv:2306.01009v1 [cs.CL])

    [http://arxiv.org/abs/2306.01009](http://arxiv.org/abs/2306.01009)

    本研究调查了不同规模的生成语言模型的演绎推理能力，发现随着规模的增加，推理能力增强，长度不会影响大部分模型表现。

    

    我们对生成变压器模型从前提中进行演绎推理的能力进行初步调查。我们观察到不同训练设置的模型性能存在显著差异，并发现演绎推理能力随规模增加而增强。此外，我们发现，除了OpenAI GPT-3和GPT-3.5模型，模型的表现通常不会随着推理链的长度而减弱。本研究考虑了从1.17亿到1750亿个参数的各种变压器解码器模型。

    We conduct a preliminary inquiry into the ability of generative transformer models to deductively reason from premises provided. We observe notable differences in the performance of models coming from different training setups and find that the deductive reasoning ability increases with scale. Further, we discover that the performance generally does not decrease with the length of the deductive chain needed to reach the conclusion, with the exception of OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of transformer-decoder models, ranging from 117 million to 175 billion parameters in size.
    
[^140]: 面向变化环境的公平解缠在线学习

    Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v1 [cs.LG])

    [http://arxiv.org/abs/2306.01007](http://arxiv.org/abs/2306.01007)

    本论文提出了一种面向变化环境的在线学习算法，该算法通过将模型参数划分为环境不变部分和环境特定部分，从而实现了数据公平性。通过大量的实验，证明了该算法的有效性。

    

    在面对变化环境的在线学习问题中，数据按时间顺序一个接一个地接收，并且它们的分布假设可能经常变化。虽然现有方法通过提供对动态遗憾或自适应遗憾的严格界限来展示其学习算法的有效性，但它们大多完全忽略了带有模型公平性的学习，其定义为跨不同子族群（例如，种族和性别）的统计平等。另一个缺点是，在适应新环境时，在线学习者需要使用全局更改更新模型参数，这是昂贵和低效的。受到稀疏机制转移假设的启发，我们声称在线学习中的变化环境可以归因于特定于环境的部分学习参数的部分变化，其余部分保持不变。为此，本文在假设从不同子人群收集的数据具有公平的模型表示的前提下，提出了一种新算法，将模型参数分为环境不变部分和环境特定部分。我们推导了每个子人群模型表示公正性的统计保证，并证明了我们提出的算法的收敛速率。此外，我们通过对合成和真实世界数据集的广泛实验证明了我们方法的有效性。

    In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data coll
    
[^141]: AbODE：使用联合ODE的从头设计抗体

    AbODE: Ab Initio Antibody Design using Conjoined ODEs. (arXiv:2306.01005v1 [cs.LG])

    [http://arxiv.org/abs/2306.01005](http://arxiv.org/abs/2306.01005)

    该论文提出了一种新的从头设计抗体的方法AbODE，该方法使用连续微分注意力来应对蛋白折叠、逆向折叠和对接等挑战，并与时间网络和图匹配网络具有基本联系。

    

    抗体是一种Y形蛋白，可以中和病原体，并构成我们适应性免疫系统的核心。新的抗体的从头设计，以特定抗原为靶标，是加速疫苗发现的关键。然而，这种氨基酸序列和三维结构的联合设计涵盖并强调了来自多个任务的一些核心挑战，包括蛋白折叠（序列到结构），逆向折叠（结构到序列）和对接（结合）。我们通过一个新的生成模型AbODE来克服这些挑战，该模型扩展了图形PDE以适应上下文信息和外部相互作用。与现有方法不同，AbODE使用一轮完整的解码，并引出连续微分注意力，其中包括抗体内部和抗原相关的潜在相互作用。我们揭示了AbODE与时间网络以及图匹配网络之间的基本联系。

    Antibodies are Y-shaped proteins that neutralize pathogens and constitute the core of our adaptive immune system. De novo generation of new antibodies that target specific antigens holds the key to accelerating vaccine discovery. However, this co-design of the amino acid sequence and the 3D structure subsumes and accentuates some central challenges from multiple tasks, including protein folding (sequence to structure), inverse folding (structure to sequence), and docking (binding). We strive to surmount these challenges with a new generative model AbODE that extends graph PDEs to accommodate both contextual information and external interactions. Unlike existing approaches, AbODE uses a single round of full-shot decoding and elicits continuous differential attention that encapsulates and evolves with latent interactions within the antibody as well as those involving the antigen. We unravel fundamental connections between AbODE and temporal networks as well as graph-matching networks. Th
    
[^142]: 适应性船舶辐射噪声识别与可学习微观小波变换

    Adaptive ship-radiated noise recognition with learnable fine-grained wavelet transform. (arXiv:2306.01002v1 [eess.AS])

    [http://arxiv.org/abs/2306.01002](http://arxiv.org/abs/2306.01002)

    本文提出了一种自适应通用识别系统AGNet，通过转换可学习的微观参数，学习了不同频率下水下声音的特性，以解决在可变水下环境中识别船舶辐射噪音的问题。

    

    分析海洋声学环境是一个棘手的任务。背景噪声和可变的信道传输环境使得准确的船舶辐射噪声识别变得复杂。现有的识别系统在处理可变水下环境方面较为薄弱，因此在实际应用中表现令人失望。为了保持识别系统在各种水下环境下的鲁棒性，本文提出了一种自适应通用识别系统——AGNet（自适应通用网络）。通过将固定的小波参数转换为可学习的微观参数，AGNet学习了不同频率下水下声音的特性。其灵活微观的设计有助于捕获更多背景声学信息（例如背景噪声、水下传输通道）。为了利用小波谱图中的隐式信息，AGNet采用并行卷积注意力卷积神经网络（Convolutional Neural Network with Parallel Convolution Attention M）。

    Analyzing the ocean acoustic environment is a tricky task. Background noise and variable channel transmission environment make it complicated to implement accurate ship-radiated noise recognition. Existing recognition systems are weak in addressing the variable underwater environment, thus leading to disappointing performance in practical application. In order to keep the recognition system robust in various underwater environments, this work proposes an adaptive generalized recognition system - AGNet (Adaptive Generalized Network). By converting fixed wavelet parameters into fine-grained learnable parameters, AGNet learns the characteristics of underwater sound at different frequencies. Its flexible and fine-grained design is conducive to capturing more background acoustic information (e.g., background noise, underwater transmission channel). To utilize the implicit information in wavelet spectrograms, AGNet adopts the convolutional neural network with parallel convolution attention m
    
[^143]: DiffLoad:扩散模型中的负荷预测不确定性量化

    DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])

    [http://arxiv.org/abs/2306.01001](http://arxiv.org/abs/2306.01001)

    本文提出了一种扩散模型中的负荷预测不确定性量化方法，采用Seq2Seq网络结构来分离两种类型的不确定性并处理异常情况，不仅着眼于预测条件期望值。

    

    电力负荷预测对电力系统的决策制定，如机组投入和能源管理等具有重要意义。近年来，各种基于自监督神经网络的方法已经被应用于电力负荷预测，以提高预测准确性和捕捉不确定性。然而，大多数现有的方法是基于高斯似然方法的，它旨在在给定的协变量下准确估计分布期望值。这种方法很难适应存在分布偏移和异常值的时间数据。在本文中，我们提出了一种基于扩散的Seq2seq结构来估计本体不确定性，并使用鲁棒的加性柯西分布来估计物象不确定性。我们展示了我们的方法能够分离两种类型的不确定性并处理突变情况，而不是准确预测条件期望。

    Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
    
[^144]: 利用音素级建模进行失语性言语弱监督强制韵律对齐

    Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling. (arXiv:2306.00996v1 [eess.AS])

    [http://arxiv.org/abs/2306.00996](http://arxiv.org/abs/2306.00996)

    本文提出了一种使用加权有限状态转换器对CTC模型的对齐图构建进行改进的弱监督方法，可以对失语性言语进行强制对齐，减轻了对逐字转录的需求，能够有效地在实际应用中使用，并在实验中取得了显著的改进。

    

    语言障碍的研究可以受益于时间对齐数据。然而，失语性言语中的音频-文本不匹配会导致现代语音对齐器的性能快速下降，从而阻碍了自动方法的使用。本文提出了一个简单有效的解决方法，即使用加权有限状态转换器对CTC模型的对齐图构建进行改进。该弱监督方法减轻了对失语性言语逐字转录实现强制对齐的需求。在图的构建过程中，我们允许对常见的失语现象进行建模，即重复和省略。此外，通过使用Oracle错误率评估音频-文本不匹配程度，我们的方法可以有效地在实际应用中使用。我们对TIMIT测试集和UCLASS数据集的损坏版本进行评估，显示出显著的改进，特别是召回率方面，在相对基线上实现了23-25%的相对改进。

    The study of speech disorders can benefit greatly from time-aligned data. However, audio-text mismatches in disfluent speech cause rapid performance degradation for modern speech aligners, hindering the use of automatic approaches. In this work, we propose a simple and effective modification of alignment graph construction of CTC-based models using Weighted Finite State Transducers. The proposed weakly-supervised approach alleviates the need for verbatim transcription of speech disfluencies for forced alignment. During the graph construction, we allow the modeling of common speech disfluencies, i.e. repetitions and omissions. Further, we show that by assessing the degree of audio-text mismatch through the use of Oracle Error Rate, our method can be effectively used in the wild. Our evaluation on a corrupted version of the TIMIT test set and the UCLASS dataset shows significant improvements, particularly for recall, achieving a 23-25% relative improvement over our baselines.
    
[^145]: 可控图像生成的扩散自导方法

    Diffusion Self-Guidance for Controllable Image Generation. (arXiv:2306.00986v1 [cs.CV])

    [http://arxiv.org/abs/2306.00986](http://arxiv.org/abs/2306.00986)

    本论文提出了一种扩散自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制，可以用于执行具有挑战性的图像操作，同时不需要额外模型或训练。

    

    大规模生成模型能够从详细文本描述中生成高质量的图像。然而，图像的许多方面很难或不可能通过文本来传达。我们引入了自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制。我们展示了可以从这些表示中提取出对象的形状、位置和外观等属性并用于指导采样。自导类似于分类器引导，但是使用预训练模型本身中存在的信号，不需要额外的模型或训练。我们展示了如何组合一组简单的属性来执行具有挑战性的图像操作，例如修改对象的位置或大小，将一个图像中的对象外观与另一个图像的布局相结合，将多个图像的对象组合成一个，等等。我们还展示了自导可以用于编辑真实图像。

    Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images
    
[^146]: BitE：在混合工作负载环境中加速学习的查询优化

    BitE : Accelerating Learned Query Optimization in a Mixed-Workload Environment. (arXiv:2306.00845v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2306.00845](http://arxiv.org/abs/2306.00845)

    本文提出了BitE，一种新颖的集成学习模型，利用数据库统计和元数据来调整学习查询优化器以提高性能。

    

    近年来，尽管有许多将深度强化学习应用于查询优化的努力，但查询优化器是复杂的实体，需要手动调整工作负载和数据集以进行优化。 最近的研究主要集中于单一工作负载的学习查询优化结果，这些结果侧重于捕捉特定工作负载的独特特性。 然而，在多个工作负载和数据集具有不同特征且需要混合和学习的情况下，这种方法面临问题。 因此，在本文中，我们提出了 BitE，一种新颖的集成学习模型，利用数据库统计和元数据来调整学习查询优化器以提高性能。 在此过程中，我们引入了多个修订版来解决几个挑战：通过扩展提示集扩展最优Abstract SQL Plan（表示为名为ASP的JSON对象）的搜索空间，将模型引向可能偏袒的默认计划之外。

    Although the many efforts to apply deep reinforcement learning to query optimization in recent years, there remains room for improvement as query optimizers are complex entities that require hand-designed tuning of workloads and datasets. Recent research present learned query optimizations results mostly in bulks of single workloads which focus on picking up the unique traits of the specific workload. This proves to be problematic in scenarios where the different characteristics of multiple workloads and datasets are to be mixed and learned together. Henceforth, in this paper, we propose BitE, a novel ensemble learning model using database statistics and metadata to tune a learned query optimizer for enhancing performance. On the way, we introduce multiple revisions to solve several challenges: we extend the search space for the optimal Abstract SQL Plan(represented as a JSON object called ASP) by expanding hintsets, we steer the model away from the default plans that may be biased by 
    
[^147]: LiT-4-RSVQA: 基于轻量级Transformer的遥感图像问答系统

    LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing. (arXiv:2306.00758v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.00758](http://arxiv.org/abs/2306.00758)

    本文提出了一种基于轻量级Transformer的遥感图像问答系统，即LiT-4-RSVQA，它可以有效准确地进行遥感图像问答，并且大大降低了计算资源的要求。

    

    遥感图像中的视觉问答（VQA）方法旨在回答与遥感图像相关的自然语言问题。目前大部分现有的方法需要大量计算资源，这在遥感操作场景中限制了它们的应用。为解决这个问题，本文提出了一种有效的轻量级Transformer-based VQA in RS（LiT-4-RSVQA）体系结构，用于高效准确地进行遥感图像问答。我们的体系结构包括：i）轻量级文本编码器模块；ii）轻量级图像编码器模块；iii）融合模块；和iv）分类模块。在VQA基准数据集上获得的实验结果表明，我们提出的LiT-4-RSVQA体系结构提供了准确的VQA结果，并且显著降低了执行硬件的计算要求。我们的代码公开在 https://git.tu-berlin.de/rsim/lit4rsvqa。

    Visual question answering (VQA) methods in remote sensing (RS) aim to answer natural language questions with respect to an RS image. Most of the existing methods require a large amount of computational resources, which limits their application in operational scenarios in RS. To address this issue, in this paper we present an effective lightweight transformer-based VQA in RS (LiT-4-RSVQA) architecture for efficient and accurate VQA in RS. Our architecture consists of: i) a lightweight text encoder module; ii) a lightweight image encoder module; iii) a fusion module; and iv) a classification module. The experimental results obtained on a VQA benchmark dataset demonstrate that our proposed LiT-4-RSVQA architecture provides accurate VQA results while significantly reducing the computational requirements on the executing hardware. Our code is publicly available at https://git.tu-berlin.de/rsim/lit4rsvqa.
    
[^148]: 关于混合互信息估计的有效性研究

    On the Effectiveness of Hybrid Mutual Information Estimation. (arXiv:2306.00608v1 [stat.ML])

    [http://arxiv.org/abs/2306.00608](http://arxiv.org/abs/2306.00608)

    本文研究了混合互信息估计的有效性，提出了一种混合方法以应对判别式和生成式方法各自缺点，同时提出了一种名为预测量化的生成方法，与判别式估计器结合可获得更精确的互信息估计结果。

    

    从联合分布的样本中估计互信息是科学和工程中的一个难题。本文研究了一个概括了判别式和生成式方法的变分界约束，并提出了一种混合方法来减少它们各自的缺点。此外，我们提出了一种称为预测量化 (PQ) 的简单生成方法，它可以与判别式估计器轻松结合以实现最小的计算开销。我们的提议通过降低估计器的方差而产生更紧的信息界约束。我们将这些方法应用于相关的高维高斯分布和涉及受固定能量景观约束的自由粒子系统的随机过程的挑战性任务上。实证结果表明，与相应的判别式估计方法相比，混合方法可以持续提高互信息估计精度。

    Estimating the mutual information from samples from a joint distribution is a challenging problem in both science and engineering. In this work, we realize a variational bound that generalizes both discriminative and generative approaches. Using this bound, we propose a hybrid method to mitigate their respective shortcomings. Further, we propose Predictive Quantization (PQ): a simple generative method that can be easily combined with discriminative estimators for minimal computational overhead. Our propositions yield a tighter bound on the information thanks to the reduced variance of the estimator. We test our methods on a challenging task of correlated high-dimensional Gaussian distributions and a stochastic process involving a system of free particles subjected to a fixed energy landscape. Empirical results show that hybrid methods consistently improved mutual information estimates when compared to the corresponding discriminative counterpart.
    
[^149]: Mechanic: 一种学习率调节器

    Mechanic: A Learning Rate Tuner. (arXiv:2306.00144v1 [cs.LG])

    [http://arxiv.org/abs/2306.00144](http://arxiv.org/abs/2306.00144)

    机械师是一种学习率调节器，能自动调整任何基本优化算法和调度的学习率比例因子，可以实现在大规模深度学习任务中接近、匹配或甚至优于手动调整学习率的效果。

    

    我们介绍了一种名为“机械师”的技术，用于自动调整任何基本优化算法和调度的学习率比例因子。我们的方法提供了实现类似目标的最近理论减少的实际应用，用于在线凸优化。我们在具有不同批量大小、调度和基本优化算法的一系列大规模深度学习任务中进行了严格评估。这些实验表明，根据问题，机械师要么非常接近，要么匹配或甚至优于手动调整学习率。

    We introduce a technique for tuning the learning rate scale factor of any base optimization algorithm and schedule automatically, which we call \textsc{mechanic}. Our method provides a practical realization of recent theoretical reductions for accomplishing a similar goal in online convex optimization. We rigorously evaluate \textsc{mechanic} on a range of large scale deep learning tasks with varying batch sizes, schedules, and base optimization algorithms. These experiments demonstrate that depending on the problem, \textsc{mechanic} either comes very close to, matches or even improves upon manual tuning of learning rates.
    
[^150]: 关于金丝雀曝光解释的一些注释

    A Note On Interpreting Canary Exposure. (arXiv:2306.00133v1 [cs.CR])

    [http://arxiv.org/abs/2306.00133](http://arxiv.org/abs/2306.00133)

    本文提供了关于如何解释金丝雀曝光的直觉，包括其与成员推理攻击和差分隐私的关系。

    

    Carlini等人介绍的金丝雀暴露经常被用来实证评估或审核机器学习模型培训的隐私。这篇笔记的目的是提供一些关于如何解释金丝雀曝光的直觉，包括与成员推理攻击和差分隐私的关系。

    Canary exposure, introduced in Carlini et al. is frequently used to empirically evaluate, or audit, the privacy of machine learning model training. The goal of this note is to provide some intuition on how to interpret canary exposure, including by relating it to membership inference attacks and differential privacy.
    
[^151]: 截断亲和力最大化：用于图形异常监测的单类同型建模

    Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])

    [http://arxiv.org/abs/2306.00006](http://arxiv.org/abs/2306.00006)

    本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。

    

    我们在现实世界的图形异常监测（GAD）数据集中经常发现一种普遍的属性......本文提出了一种新的无监督异常评分度量 - 当前节点亲和力......我们进一步提出了截断亲和力最大化 (TAM)，该方法通过最大化与_neighbors的本地亲和力来学习量身定制的节点表示。本文所提方法在原始图形结构上进行优化，可以进行双重One-Class的GAD。

    One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
    
[^152]: Smooth-Trajectron++: 将平滑关注机制引入Trajectron++行为预测模型

    Smooth-Trajectron++: Augmenting the Trajectron++ behaviour prediction model with smooth attention. (arXiv:2305.19678v1 [cs.LG])

    [http://arxiv.org/abs/2305.19678](http://arxiv.org/abs/2305.19678)

    本文中，研究者将平滑关注机制引入最先进的Trajectron++轨迹预测模型，提高其性能并比较其性能表现。该研究表明将人类的认知因素纳入轨迹预测模型中具有潜在的优势。

    

    理解交通参与者的行为对于预测其未来轨迹至关重要，为自动驾驶车辆的开发提供安全可靠的规划系统。本文研究了最先进的Trajectron++轨迹预测模型，并通过在其注意力模块中加入平滑项以进一步提高其性能。该关注机制模仿了认知科学研究所启发的人类注意力，表明人的注意力在切换时存在局限性。我们对结果进行了评估，并将其与原始模型在各种基准测试中进行了比较，揭示了将人类认知洞察力纳入轨迹预测模型中的潜力。

    Understanding traffic participants' behaviour is crucial for predicting their future trajectories, aiding in developing safe and reliable planning systems for autonomous vehicles. Integrating cognitive processes and machine learning models has shown promise in other domains but is lacking in the trajectory forecasting of multiple traffic agents in large-scale autonomous driving datasets. This work investigates the state-of-the-art trajectory forecasting model Trajectron++ which we enhance by incorporating a smoothing term in its attention module. This attention mechanism mimics human attention inspired by cognitive science research indicating limits to attention switching. We evaluate the performance of the resulting Smooth-Trajectron++ model and compare it to the original model on various benchmarks, revealing the potential of incorporating insights from human cognition into trajectory prediction models.
    
[^153]: 连续强化学习的策略优化

    Policy Optimization for Continuous Reinforcement Learning. (arXiv:2305.18901v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18901](http://arxiv.org/abs/2305.18901)

    本研究提出了连续强化学习领域的占用时间概念，并在此基础上扩展了离散强化学习中的PG、TRPO和PPO方法，为连续强化学习领域的研究提供了新的思路和方法。

    

    我们研究了连续时间和空间下的强化学习，采用折扣奖励和随机微分方程的基本动态。在连续强化学习的最新进展的基础上，我们提出了占用时间的概念（特别是针对折扣奖励）并展示了如何有效地使用它来导出性能差异和局部逼近公式。我们还将这些结果扩展到了 PG（策略梯度）、TRPO（信任区域策略优化）和 PPO（近端策略优化）方法，这些方法在离散强化学习中是熟知和强大的工具，但在连续强化学习中尚未得到充分发展。通过数字实验，我们展示了我们的方法的有效性和优势。

    We study reinforcement learning (RL) in the setting of continuous time and space, for an infinite horizon with a discounted objective and the underlying dynamics driven by a stochastic differential equation. Built upon recent advances in the continuous approach to RL, we develop a notion of occupation time (specifically for a discounted objective), and show how it can be effectively used to derive performance-difference and local-approximation formulas. We further extend these results to illustrate their applications in the PG (policy gradient) and TRPO/PPO (trust region policy optimization/ proximal policy optimization) methods, which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL. Through numerical experiments, we demonstrate the effectiveness and advantages of our approach.
    
[^154]: 无监督多元时间序列表示学习的对比形态片段学习

    Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning. (arXiv:2305.18888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18888](http://arxiv.org/abs/2305.18888)

    该论文提出了一种新的无监督多元时间序列表示学习框架，通过对比形态片段学习以及多粒度对比和多尺度对齐的学习目标，学习时间序列特定表示。

    

    近期的研究表明，对于多元时间序列数据，无监督表示学习（URL）具备学习泛化表示以及无需使用不可访问标签就能适用于多数下游任务的能力。然而，现有方法通常采用原本为其他领域（如计算机视觉）设计的模型进行编码，且依赖于强假设设计学习目标，这限制了它们的运行表现。为解决这些问题，我们提出了一种新颖的无监督表示学习框架，通过流行的对比学习范式，学习基于形态片段的时间序列特定表示。据我们所知，这是第一篇探索无监督通用表示学习中形态片段嵌入的研究。特别地，我们提出了一个统一的基于形态片段的编码器和一种新的学习目标，具有多粒度对比和多尺度对齐。

    Recent studies have shown great promise in unsupervised representation learning (URL) for multivariate time series, because URL has the capability in learning generalizable representation for many downstream tasks without using inaccessible labels. However, existing approaches usually adopt the models originally designed for other domains (e.g., computer vision) to encode the time series data and rely on strong assumptions to design learning objectives, which limits their ability to perform well. To deal with these problems, we propose a novel URL framework for multivariate time series by learning time-series-specific shapelet-based representation through a popular contrasting learning paradigm. To the best of our knowledge, this is the first work that explores the shapelet-based embedding in the unsupervised general-purpose representation learning. A unified shapelet-based encoder and a novel learning objective with multi-grained contrasting and multi-scale alignment are particularly 
    
[^155]: 离线目标条件强化学习的目标泛化所必需的因素是什么？

    What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?. (arXiv:2305.18882v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18882](http://arxiv.org/abs/2305.18882)

    本文研究了离线目标条件强化学习的目标泛化，提出了一种新的离线GCRL方法GOAT，结合理论和实验结果，加权模仿学习比基于悲观离线RL方法的泛化性能更好。

    

    离线目标条件强化学习（GCRL）提供了一种从完全离线数据集中训练通用代理的方法。除了在数据集内保守之外，实现未见过的目标的泛化能力是离线GCRL的另一个基本挑战。然而，据我们所知，这个问题尚未得到很好的研究。在本文中，我们从理论和实证两方面研究了离线GCRL的分布外泛化，以确定重要的因素。在一些实验中，我们观察到加权模仿学习比基于悲观离线RL方法的泛化性能更好。基于这个见解，我们推导了一个关于分布外泛化的理论，阐明了几个重要的设计选择。然后，我们提出了一个新的离线GCRL方法，即通用离线目标条件RL（GOAT），通过结合我们理论和实证研究的发现。在一个包含 9 个独立同分布（IID）数据分布的新基准测试中，我们展示了GOAT的有效性和通用性。

    Offline goal-conditioned RL (GCRL) offers a way to train general-purpose agents from fully offline datasets. In addition to being conservative within the dataset, the generalization ability to achieve unseen goals is another fundamental challenge for offline GCRL. However, to the best of our knowledge, this problem has not been well studied yet. In this paper, we study out-of-distribution (OOD) generalization of offline GCRL both theoretically and empirically to identify factors that are important. In a number of experiments, we observe that weighted imitation learning enjoys better generalization than pessimism-based offline RL method. Based on this insight, we derive a theory for OOD generalization, which characterizes several important design choices. We then propose a new offline GCRL method, Generalizable Offline goAl-condiTioned RL (GOAT), by combining the findings from our theoretical and empirical studies. On a new benchmark containing 9 independent identically distributed (IID
    
[^156]: 通用高斯核密度估计模型的降维算法研究

    Dimensionality Reduction for General KDE Mode Finding. (arXiv:2305.18755v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18755](http://arxiv.org/abs/2305.18755)

    本文提出了一种降维算法，可以适用于高斯混合模型和更广泛的核函数，结合梯度下降可制定出有效的实用启发式算法。

    

    在统计学和数据分析中，寻找高维概率分布的模是一项重要的算法问题。本文针对高斯混合模型的模估计结果进行了通用扩展，提出了一种降维算法，可以适用于更广泛的核函数，包括流行的逻辑、Sigmoid和广义高斯核。通过结合梯度下降，该算法可制定出有效的实用启发式算法。

    Finding the mode of a high dimensional probability distribution $D$ is a fundamental algorithmic problem in statistics and data analysis. There has been particular interest in efficient methods for solving the problem when $D$ is represented as a mixture model or kernel density estimate, although few algorithmic results with worst-case approximation and runtime guarantees are known.  In this work, we significantly generalize a result of (LeeLiMusco:2021) on mode approximation for Gaussian mixture models. We develop randomized dimensionality reduction methods for mixtures involving a broader class of kernels, including the popular logistic, sigmoid, and generalized Gaussian kernels. As in Lee et al.'s work, our dimensionality reduction results yield quasi-polynomial algorithms for mode finding with multiplicative accuracy $(1-\epsilon)$ for any $\epsilon > 0$. Moreover, when combined with gradient descent, they yield efficient practical heuristics for the problem.  In addition to our po
    
[^157]: 多块双层优化的分块随机方差约简方法及并行加速

    Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization. (arXiv:2305.18730v1 [math.OC])

    [http://arxiv.org/abs/2305.18730](http://arxiv.org/abs/2305.18730)

    本文提出了两种基于方差约简的优化算法，以实现对多块双层优化问题的高效求解，同时匹配单块标准 BO 问题的最优复杂度、实现并行化加速，以及避免计算高维度的 Hessian 矩阵的逆估计。

    

    本文考虑非凸的多块双层优化问题，并提出了两种基于分块方差约简的优化算法。为了达到算法的三个期望：（a）能匹配单块标准 BO 问题的最优复杂度；（b）实现并行化加速，每个迭代中采样 $I$ 块并对每个采样块采样 $B$ 个样本；（c）避免计算高维度的 Hessian 矩阵的逆估计。本文旨在解决这些问题，并探讨了现有算法的关联性以及不足之处。

    In this paper, we consider non-convex multi-block bilevel optimization (MBBO) problems, which involve $m\gg 1$ lower level problems and have important applications in machine learning. Designing a stochastic gradient and controlling its variance is more intricate due to the hierarchical sampling of blocks and data and the unique challenge of estimating hyper-gradient. We aim to achieve three nice properties for our algorithm: (a) matching the state-of-the-art complexity of standard BO problems with a single block; (b) achieving parallel speedup by sampling $I$ blocks and sampling $B$ samples for each sampled block per-iteration; (c) avoiding the computation of the inverse of a high-dimensional Hessian matrix estimator. However, it is non-trivial to achieve all of these by observing that existing works only achieve one or two of these properties. To address the involved challenges for achieving (a, b, c), we propose two stochastic algorithms by using advanced blockwise variance-reductio
    
[^158]: 基于关键词的采样（KEYS）在大规模语言模型中的应用

    KEYword based Sampling (KEYS) for Large Language Models. (arXiv:2305.18679v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18679](http://arxiv.org/abs/2305.18679)

    本文旨在探讨如何通过结合人类生成答案的思路来生成接近人类行为和事实的答案，并探讨关键词对Q/A任务解码算法的影响。

    

    问答（Q/A）任务可以被看作一个生成任务，即在给定问题和文章（如果有）的情况下生成答案。最近Q/A任务的进展主要关注语言模型的改进，而很少关注其他领域，如采样。关键词在人类语言生成中起着非常重要的作用。因此，本文旨在探索如何结合人类生成答案的行为来生成接近人类行为且事实正确的答案，并讨论关键词如何影响Q/A任务的解码算法。

    Question answering (Q/A) can be formulated as a generative task (Mitra, 2017) where the task is to generate an answer given the question and the passage (knowledge, if available). Recent advances in QA task is focused a lot on language model advancements and less on other areas such as sampling(Krishna et al., 2021), (Nakano et al., 2021). Keywords play very important role for humans in language generation. (Humans formulate keywords and use grammar to connect those keywords and work). In the research community, very little focus is on how humans generate answers to a question and how this behavior can be incorporated in a language model. In this paper, we want to explore these two areas combined, i.e., how sampling can be to used generate answers which are close to human-like behavior and factually correct. Hence, the type of decoding algorithm we think should be used for Q/A tasks should also depend on the keywords. These keywords can be obtained from the question, passage or interne
    
[^159]: 信仰与命运：Transformer在组合性方面的局限性。

    Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18654](http://arxiv.org/abs/2305.18654)

    研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。

    

    Transformer大型语言模型在需要复杂多步推理的任务上表现卓越，但同时在一些简单问题上也会出现失败。这引发了疑问：这些错误是偶然的，还是它们表明了更实质性的限制？为了揭示Transformer的神秘面纱，我们研究了这些模型在三个代表性的组合型任务中的极限 - 多位数乘法、逻辑网格谜题和一个经典的动态规划问题。 这些任务需要将问题分解为子步骤，并将这些步骤综合成精确的答案。我们将组合型任务转化为计算图，以系统地量化其复杂性，并将推理步骤分解为中间子程序。我们的实证结果表明，Transformer通过将多步组合推理转化为线性子图匹配来解决组合型任务。

    Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
    
[^160]: UMD: 无监督模型检测X2X后门攻击

    UMD: Unsupervised Model Detection for X2X Backdoor Attacks. (arXiv:2305.18651v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18651](http://arxiv.org/abs/2305.18651)

    UMD是一种无监督的模型检测方法，能够有效地检测X2X后门攻击，对抗（源，目标）类别对进行联合推断，该方法定义了新的可转移性统计量，并使用聚类方法来测量和选择潜在的后门类别对的子集。

    

    后门（特洛伊）攻击是深度神经网络面临的常见威胁，其中嵌入后门触发器的一个或多个源类别的样本会被错误分类为对抗目标类别。现有的检测分类器是否遭受后门攻击的方法主要是针对单一对抗目标的攻击设计的（例如，全对一攻击）。据我们所知，没有现有的方法可以在没有监督的情况下有效地解决具有任意数量的源类别的更普遍的X2X攻击，每个源类别都与任意目标类别配对。在本文中，我们提出了UMD，第一个通过联合推断对抗（源，目标）类别对来有效检测X2X后门攻击的无监督模型检测方法。特别地，我们首先定义一种新颖的可转移性统计方法，通过提出的聚类方法来量度和选择一组潜在的后门类别对的子集。然后，这些选择的类别对是基于联合评估进行评估的。

    Backdoor (Trojan) attack is a common threat to deep neural networks, where samples from one or more source classes embedded with a backdoor trigger will be misclassified to adversarial target classes. Existing methods for detecting whether a classifier is backdoor attacked are mostly designed for attacks with a single adversarial target (e.g., all-to-one attack). To the best of our knowledge, without supervision, no existing methods can effectively address the more general X2X attack with an arbitrary number of source classes, each paired with an arbitrary target class. In this paper, we propose UMD, the first Unsupervised Model Detection method that effectively detects X2X backdoor attacks via a joint inference of the adversarial (source, target) class pairs. In particular, we first define a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach. Then, these selected class pairs are jointly assessed based
    
[^161]: 用机器学习分析感知压力测试

    Alg{\i}lanan Stres Testinin Makine \"O\u{g}renmesi ile Analiz Edilmesi. (arXiv:2305.18473v1 [cs.LG])

    [http://arxiv.org/abs/2305.18473](http://arxiv.org/abs/2305.18473)

    本研究利用机器学习重新分析了感知压力测试，揭示了测试问题的重要性不相等，展示了在心理上观察到的不同模式。

    

    本研究旨在使用机器学习重新分析感知压力测试，以确定150个个体的感知压力水平并测量测试问题的影响。该测试包括14个问题，每个问题的得分范围为0到4，总得分范围为0-56。其中，7个问题以负面方式表述并相应评分，而其余7个问题以正面方式表述并按相反方式评分。该测试还设计为识别两个子因素：感知自我效能和压力/不适感知。本研究的主要目标是展示利用人工智能技术测试问题可能并不具有相等的重要性，揭示使用机器学习在社会中出现变化的问题，并最终证明在心理上观察到不同的模式存在。这项研究通过重复现有的心理学文献，提供了不同的视角。

    The aim of this study is to reanalyze the perceived stress test using machine learning to determine the perceived stress levels of 150 individuals and measure the impact of the test questions. The test consists of 14 questions, each scored on a scale of 0 to 4, resulting in a total score range of 0-56. Out of these questions, 7 are formulated in a negative context and scored accordingly, while the remaining 7 are formulated in a positive context and scored in reverse. The test is also designed to identify two sub-factors: perceived self-efficacy and stress/discomfort perception. The main objectives of this research are to demonstrate that test questions may not have equal importance using artificial intelligence techniques, reveal which questions exhibit variations in the society using machine learning, and ultimately demonstrate the existence of distinct patterns observed psychologically. This study provides a different perspective from the existing psychology literature by repeating 
    
[^162]: 关于高斯-斯坦变分梯度下降动态性的探究

    Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent. (arXiv:2305.14076v1 [math.ST])

    [http://arxiv.org/abs/2305.14076](http://arxiv.org/abs/2305.14076)

    本文探究了高斯-斯坦变分梯度下降动态性。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。

    

    Stein Variational Gradient Descent (SVGD)是一种非参数基于粒子的确定性采样算法。尽管其被广泛使用，但理解SVGD的理论属性一直是一个具有挑战性的问题。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。受此事实的启发，我们通过双线性核将SVGD投影到高斯分布族中，即高斯变分推断 (GVI) 与 SVGD。我们通过考虑均场 PDE 和离散粒子系统，提供了一个完整的图像。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。我们的分析基于一个新的代数恒等式，该等式将目标高斯分布的费希尔信息矩阵与粒子均匀分布的费希尔信息矩阵相关联。这个等式为我们提供了透视 GVI with SVGD 在均场和粒子设置中的动态性的统一视角。

    Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in ti
    
[^163]: 论文标题：使ViT成形：计算-优化模型设计的缩放定律。

    Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.13035](http://arxiv.org/abs/2305.13035)

    本研究通过改进缩放定律方法推测出计算-优化模型形状，成功实现了形状优化视觉变换器SoViT，该模型在相同计算量下，取得了与超过其两倍大小的模型相竞争的结果。

    

    近期，缩放定律被用来推导在给定计算时间范围内的计算-优化模型大小（参数数量）。我们发展并改进了这些方法，以推测如宽度和深度等计算-优化模型形状，并在视觉变换器中成功实现了这一点。我们经过形状优化的视觉变换器SoViT，在仅使用相同数量的计算量进行预训练的情况下，取得了与超过其两倍大小的模型相竞争的结果。例如，SoViT-400m/14在ILSRCV2012上取得了90.3%的微调准确度，超过了更大的ViT-g/14，在相同设置下接近ViT-G/14，同时推断成本也不到一半。我们进行了多个任务的彻底评估，例如图像分类、字幕、VQA和零-shot转移，在广泛领域中展示了我们模型的有效性并确定了其限制。总体而言，我们的研究发现挑战了盲目扩大视觉模型的现有方法。

    Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models 
    
[^164]: 多模式联邦人类活动识别中的隐私问题

    Privacy in Multimodal Federated Human Activity Recognition. (arXiv:2305.12134v1 [cs.LG])

    [http://arxiv.org/abs/2305.12134](http://arxiv.org/abs/2305.12134)

    本文研究了多模式联邦人类活动识别中的隐私问题。通过一个特定的系统，联邦学习可以提供更好的隐私保护，同时不会损失人类活动识别的准确性。

    

    人类活动识别（HAR）的训练数据往往包含隐私信息或由不合作实体持有。联邦学习（FL）通过在边缘设备上训练机器学习模型来解决这些问题。本文研究了在用户、环境和传感器级别上隐私对联邦HAR的影响。我们表明，FL对HAR的性能取决于FL系统的隐私保护程度，并且主要取决于来自不同传感器的数据的配置。尽管避免数据共享并在人类或环境级别上假设隐私，如之前的工作所做的那样，精度会降低5-7％。然而，将这种隐私延伸到模态级别并严格分离多个客户端之间的传感器数据可能会导致精度降低19-42％。由于这种形式的隐私是HAR中被要求的道德利用被动传感方法所必需的，因此我们实现了一种系统，在该系统中客户端相互训练一个通用的FL模型和一个每种模态一个的组级模型。我们的评估表明，这种方法可以在不牺牲HAR准确性的情况下提高隐私保护。

    Human Activity Recognition (HAR) training data is often privacy-sensitive or held by non-cooperative entities. Federated Learning (FL) addresses such concerns by training ML models on edge clients. This work studies the impact of privacy in federated HAR at a user, environment, and sensor level. We show that the performance of FL for HAR depends on the assumed privacy level of the FL system and primarily upon the colocation of data from different sensors. By avoiding data sharing and assuming privacy at the human or environment level, as prior works have done, the accuracy decreases by 5-7%. However, extending this to the modality level and strictly separating sensor data between multiple clients may decrease the accuracy by 19-42%. As this form of privacy is necessary for the ethical utilisation of passive sensing methods in HAR, we implement a system where clients mutually train both a general FL model and a group-level one per modality. Our evaluation shows that this method leads to
    
[^165]: Q-malizing流和无穷小密度比估计

    Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])

    [http://arxiv.org/abs/2305.11857](http://arxiv.org/abs/2305.11857)

    研究提出了一种可以从一个数据分布P传输到任意访问通过有限样本的Q的流模型。这个模型通过神经ODE模型进行，可以进行无穷小DRE。

    

    连续的正则化流在生成任务中被广泛使用，其中流网络从数据分布P传输到正态分布。一种能够从P传输到任意Q的流模型，其中P和Q都可通过有限样本访问，将在各种应用兴趣中使用，特别是在最近开发的望远镜密度比估计中（DRE），它需要构建中间密度以在P和Q之间建立桥梁。在这项工作中，我们提出了这样的“Q-malizing流”，通过神经ODE模型进行，该模型通过经验样本的可逆传输从P到Q（反之亦然），并通过最小化传输成本进行正则化。训练好的流模型使我们能够沿与时间参数化的log密度进行无穷小DRE，通过训练附加的连续时间流网络使用分类损失来估计log密度的时间偏导数。通过积分时间得分网络

    Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
    
[^166]: SFP: 针对伪特征的修剪方法，用于识别无分布概括问题

    SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization. (arXiv:2305.11615v1 [cs.LG])

    [http://arxiv.org/abs/2305.11615](http://arxiv.org/abs/2305.11615)

    SFP提出了一种针对模型子结构的修剪框架，SFP可以自动探索不变的子结构，而不考虑对完全暴露于域外数据的依赖性以及对整个数据分布进行同样特征未命中修剪带来的缺点。这种方法的核心在于，利用ID数据中的伪特征来降低风险。

    

    模型子结构学习旨在找到一个不变的网络子结构，可以比原始的完整结构更好地进行超出分布范围（OOD）概括。现有的工作通常使用完全暴露的域外数据来搜索不变的子结构，从而可能带来两个缺点：1）不公平，因为完全暴露出域外数据的依赖性；和2）次优的OOD概括，由于对整个数据分布进行了同样的特征未命中修剪。基于ID数据中的伪特征可能具有更低的体验风险的想法，在本文中，我们提出了一种新的伪特征定向的模型修剪框架，称为SFP，以自动探索不变的子结构，而不考虑上述缺点。具体而言，SFP在培训过程中使用我们在理论上验证的任务丢失识别ID实例中的伪特征，基于此，SFP减弱了相应的特征作用，以提高OOD泛化能力。

    Model substructure learning aims to find an invariant network substructure that can have better out-of-distribution (OOD) generalization than the original full structure. Existing works usually search the invariant substructure using modular risk minimization (MRM) with fully exposed out-domain data, which may bring about two drawbacks: 1) Unfairness, due to the dependence of the full exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the equally feature-untargeted pruning on the whole data distribution. Based on the idea that in-distribution (ID) data with spurious features may have a lower experience risk, in this paper, we propose a novel Spurious Feature-targeted model Pruning framework, dubbed SFP, to automatically explore invariant substructures without referring to the above drawbacks. Specifically, SFP identifies spurious features within ID instances during training using our theoretically verified task loss, upon which, SFP attenuates the corresponding 
    
[^167]: 可视化问答：最近文献中技术和常见趋势综述

    Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature. (arXiv:2305.11033v1 [cs.CV])

    [http://arxiv.org/abs/2305.11033](http://arxiv.org/abs/2305.11033)

    本文综述了最近文献中的可视化问答问题，提供了对该领域的深入分析和比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。

    

    可视化问答（VQA）是自然语言处理和图像预测中的一个新兴问题，需要算法回答有关特定图像的问题。在本文中，作者分析了25项最新研究和6个数据集，并提供了下载链接。作者深入调研了该领域的多项研究，并提供了分析比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。

    Visual Question Answering (VQA) is an emerging area of interest for researches, being a recent problem in natural language processing and image prediction. In this area, an algorithm needs to answer questions about certain images. As of the writing of this survey, 25 recent studies were analyzed. Besides, 6 datasets were analyzed and provided their link to download. In this work, several recent pieces of research in this area were investigated and a deeper analysis and comparison among them were provided, including results, the state-of-the-art, common errors, and possible points of improvement for future researchers.
    
[^168]: 深度学习可靠地识别胸部X光异常模式吗？一项多读者研究，检查AI在临床实践中一个月的实施情况。

    Can Deep Learning Reliably Recognize Abnormality Patterns on Chest X-rays? A Multi-Reader Study Examining One Month of AI Implementation in Everyday Radiology Clinical Practice. (arXiv:2305.10116v1 [eess.IV])

    [http://arxiv.org/abs/2305.10116](http://arxiv.org/abs/2305.10116)

    本研究开发了一种基于深度学习的自动检测算法，可以在胸部X光片上检测出七种特定放射学发现，并且该算法的性能优于评估图像的六名放射科医师。

    

    本研究开发了一种基于深度学习的自动检测算法（DLAD，Carebot AI CXR），用于检测和定位胸部X线片上的七种特定放射学发现（肺不张（ATE），实变（CON），胸腔积液（EFF），肺部病变（LES），皮下气肿（SCE），心脏扩大（CMG），气胸（PNO））。我们收集了956张胸部X线片，并将DLAD的性能与在医院环境下评估图像的六名单个放射科医师的表现进行了比较。即使与放射科医师相比，所提出的DLAD也取得了高灵敏度（ATE 1.000（0.624-1.000），CON 0.864（0.671-0.956），EFF 0.953（0.887-0.983），LES 0.905（0.715-0.978），SCE 1.000（0.366-1.000），CMG 0.837（0.711-0.917），PNO 0.875（0.538-0.986））（最低：ATE 0.000（0.000-0.376），CON 0.182（0.070-0.382），EFF 0.400（0.302-0.506），LES 0.238（0.103-0.448），SCE 0.000（0.000-0.634），CMG 0.347（0.228-0.486），PNO 0.375（0.134-0.691），最高：ATE 1.000（0.624-1.000），CON 0.864（0.671-0.956），EFF 0.953（0.887-0.983），LES 0.905（0.715-0.978），SCE 1.000（0.366-1.000），CMG 0.837（0.711-0.917），PNO 0.875（0.538-0.986））。

    In this study, we developed a deep-learning-based automatic detection algorithm (DLAD, Carebot AI CXR) to detect and localize seven specific radiological findings (atelectasis (ATE), consolidation (CON), pleural effusion (EFF), pulmonary lesion (LES), subcutaneous emphysema (SCE), cardiomegaly (CMG), pneumothorax (PNO)) on chest X-rays (CXR). We collected 956 CXRs and compared the performance of the DLAD with that of six individual radiologists who assessed the images in a hospital setting. The proposed DLAD achieved high sensitivity (ATE 1.000 (0.624-1.000), CON 0.864 (0.671-0.956), EFF 0.953 (0.887-0.983), LES 0.905 (0.715-0.978), SCE 1.000 (0.366-1.000), CMG 0.837 (0.711-0.917), PNO 0.875 (0.538-0.986)), even when compared to the radiologists (LOWEST: ATE 0.000 (0.000-0.376), CON 0.182 (0.070-0.382), EFF 0.400 (0.302-0.506), LES 0.238 (0.103-0.448), SCE 0.000 (0.000-0.634), CMG 0.347 (0.228-0.486), PNO 0.375 (0.134-0.691), HIGHEST: ATE 1.000 (0.624-1.000), CON 0.864 (0.671-0.956), E
    
[^169]: “我全然成为我自己”：以TGNB人群为中心，评估开放式语言生成中的偏见

    "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])

    [http://arxiv.org/abs/2305.09941](http://arxiv.org/abs/2305.09941)

    本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。

    

    跨性别和非二元（TGNB）人群在日常生活中经历了不成比例的歧视和排斥。随着语言生成技术的日益普及和应用，进一步边缘化这一人群的可能性也在增加。虽然大量的NLP公平文献着重于阐明和解决性别偏见，但评估TGNB身份所带来的性别伤害需要理解这些身份如何独特地与社会性别规范互动以及与性别二元中心的视角相区分。这样的测量框架本质上需要以TGNB声音为中心，帮助指导包容性别的自然语言处理应该为谁服务。为实现这一目标，我们以TGNB社区和现有的跨学科文献为基础，评估了TGNB个体经历边缘化所形成的社会现实是如何影响和存在于开放式语言生成（OLG）中。首先理解TGNB个体的经历，我们提出了一个评估OLG系统的框架，旨在以TGNB人群为中心，度量与该人群相关的偏见。我们的框架包括特别为TGNB人群设计的调查工具，以及交叉分析结果的交叉方法。我们相信，这项工作将有助于实现更公平、更包容的自然语言处理社区，并潜在地解决NLP研究中广泛的交叉身份问题。

    Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
    
[^170]: 关于多标签学习中Macro-AUC的泛化理解探究

    Towards Understanding Generalization of Macro-AUC in Multi-label Learning. (arXiv:2305.05248v1 [cs.LG])

    [http://arxiv.org/abs/2305.05248](http://arxiv.org/abs/2305.05248)

    本研究探究了 multi-label 学习中常用的 Macro-AUC 的泛化性质，并发现数据集中标签不平衡对泛化界限有重要影响。未经变量处理的基于损失函数的算法可能由于对标签的不平衡更敏感而表现较差，这一结论在多个数据集上得到验证。

    

    在多标签学习中，Macro-AUC是类内AUC算术平均值，通常在实践中使用。然而，它的理论理解远远不足。为了解决这个问题，我们基于对应的代理损失函数表征各种学习算法的宏AUC的泛化属性。我们在理论上确定了影响泛化界限的数据集的关键因素：标签类别不平衡。我们对不平衡感知误差界限的结果表明，广泛使用的未经变量处理的基于损失函数的算法比提出的基于成对和重新加权的算法更敏感于标签类别的不平衡，这可能意味着它的性能较差。此外，各种数据集上的经验结果证实了我们的理论结果。就技术而言，我们提出了一种新的（更通用的）McDiarmid型集中不等式，这可能具有独立的兴趣。

    Macro-AUC is the arithmetic mean of the class-wise AUCs in multi-label learning and is commonly used in practice. However, its theoretical understanding is far lacking. Toward solving it, we characterize the generalization properties of various learning algorithms based on the corresponding surrogate losses w.r.t. Macro-AUC. We theoretically identify a critical factor of the dataset affecting the generalization bounds: \emph{the label-wise class imbalance}. Our results on the imbalance-aware error bounds show that the widely-used univariate loss-based algorithm is more sensitive to the label-wise class imbalance than the proposed pairwise and reweighted loss-based ones, which probably implies its worse performance. Moreover, empirical results on various datasets corroborate our theory findings. To establish it, technically, we propose a new (and more general) McDiarmid-type concentration inequality, which may be of independent interest.
    
[^171]: 基于任务导向多目标优化的对抗样本生成

    Generating Adversarial Examples with Task Oriented Multi-Objective Optimization. (arXiv:2304.13229v1 [cs.LG])

    [http://arxiv.org/abs/2304.13229](http://arxiv.org/abs/2304.13229)

    本文提出了“基于任务导向的MOO”方法来实现对抗样本生成以同时实现多个目标，避免了朴素MOO最大化所有目标的弊端。

    

    深度学习模型，即使是最先进的模型，也很容易受到对抗样本的攻击。对抗训练是提高模型稳健性的最有效方法之一。对于对抗训练的成功来说，关键因素是要能够生成满足某些目标/目标的合格且有差异的对抗样本（例如，找到最大化模型损失以同时攻击多个模型的对抗样本）。因此，多目标优化（MOO）是实现对抗性样本生成以同时实现多个目标/目标的自然工具。本文观察到，MOO的朴素应用往往会平等地最大化所有目标/目标，而不关心目标/目标是否已经实现。这导致了在已实现目标/目标的任务上做无用的努力，而在未实现目标/目标的任务上则投入了较少的关注。因此，本文提出了“基于任务导向的MOO”来解决这一问题，在此情况下...

    Deep learning models, even the-state-of-the-art ones, are highly vulnerable to adversarial examples. Adversarial training is one of the most efficient methods to improve the model's robustness. The key factor for the success of adversarial training is the capability to generate qualified and divergent adversarial examples which satisfy some objectives/goals (e.g., finding adversarial examples that maximize the model losses for simultaneously attacking multiple models). Therefore, multi-objective optimization (MOO) is a natural tool for adversarial example generation to achieve multiple objectives/goals simultaneously. However, we observe that a naive application of MOO tends to maximize all objectives/goals equally, without caring if an objective/goal has been achieved yet. This leads to useless effort to further improve the goal-achieved tasks, while putting less focus on the goal-unachieved tasks. In this paper, we propose \emph{Task Oriented MOO} to address this issue, in the contex
    
[^172]: 基于匹配的生成模型数据估值方法

    Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])

    [http://arxiv.org/abs/2304.10701](http://arxiv.org/abs/2304.10701)

    本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。

    

    数据估值对于机器学习非常重要，因为它有助于增强模型的透明度并保护数据特性。现有的数据估值方法主要集中在判别模型上，忽略了最近吸引了大量关注的深度生成模型。与判别模型类似，需要评估深度生成模型中数据贡献的紧迫需求也存在。然而，以往的数据估值方法主要依赖于判别模型性能指标，并需要对模型进行重新训练。因此，它们不能在实际中直接高效地应用于近期的深度生成模型，例如生成对抗网络和扩散模型。为了弥补这一差距，我们从相似性匹配的角度对生成模型中的数据估值问题进行了构建。具体地，我们引入了“Generative Model Valuator”（GMValuator）——第一个针对任何生成模型的模型无关方法，旨在为生成模型提供数据估值而无需重新训练模型。我们的方法利用数据实例及由生成模型生成的相应合成实例之间的相似度来估计原始数据的价值。大量实验证明了我们的方法在为不同的生成模型（包括GAN和扩散模型）评估数据实例方面的优越性。

    Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
    
[^173]: 符号回归的先验知识

    Priors for symbolic regression. (arXiv:2304.06333v1 [cs.LG])

    [http://arxiv.org/abs/2304.06333](http://arxiv.org/abs/2304.06333)

    本文提出了一种在符号回归（SR）框架内将有关函数和参数的详细先验信息纳入的方法，并且演示了该先验在基准数据集和材料科学应用中的性能。

    

    在为数据集选择符号模型时，人们自然倾向于选择“简单”的表达式或更接近之前在类似情况下看到的方程式。这表明函数应该具有非均匀先验知识，然而，在符号回归（SR）框架内很少考虑。在本文中，我们开发了一种方法，将有关函数和参数的详细先验信息纳入SR中。我们对函数结构的先验是基于n-gram语言模型的，该模型对各个运算符的排列方式以及每个运算符的出现频率都非常敏感。我们还开发了一种基于分式贝叶斯因子的形式体系，以处理数值参数的先验知识，使得可以通过贝叶斯证据公平比较模型，同时明确比较了贝叶斯、最小描述长度和启发式方法用于模型选择。我们通过对基准数据集进行实验以及在材料科学中的应用演示了我们的先验的性能。

    When choosing between competing symbolic models for a data set, a human will naturally prefer the "simpler" expression or the one which more closely resembles equations previously seen in a similar context. This suggests a non-uniform prior on functions, which is, however, rarely considered within a symbolic regression (SR) framework. In this paper we develop methods to incorporate detailed prior information on both functions and their parameters into SR. Our prior on the structure of a function is based on a $n$-gram language model, which is sensitive to the arrangement of operators relative to one another in addition to the frequency of occurrence of each operator. We also develop a formalism based on the Fractional Bayes Factor to treat numerical parameter priors in such a way that models may be fairly compared though the Bayesian evidence, and explicitly compare Bayesian, Minimum Description Length and heuristic methods for model selection. We demonstrate the performance of our pri
    
[^174]: 关于AI生成文本检测的可能性的探讨

    On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.04736](http://arxiv.org/abs/2304.04736)

    该研究探讨了AI生成文本检测的可能性，提出了精确的样本复杂度界限，并指出了设计更准确的检测方法和提高透明度的挑战。

    

    我们的工作着眼于检测由大型语言模型(LLM)生成的输出，以区分其与人类生成的输出。这项能力在许多应用中非常重要。然而，关于这种区分的可能性一直是该领域内的争议话题。因此，一个核心问题是我们是否能够检测到AI生成的文本，如果能，何时能检测到。在这项工作中，我们提供了证据表明，除非人类和机器生成的文本分布在整个支持中完全相同，否则几乎总是可以检测到AI生成的文本。这个观察结果来自于信息论中的标准结果，并依赖于机器生成的文本越像人类，我们就需要更多的样本来检测它。我们得出了AI生成文本检测的精确样本复杂度界限，告诉需要多少个样本才能检测到AI生成的文本。这引起了更多设计更准确的AI生成文本检测方法和提高LLM透明度的挑战。

    Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
    
[^175]: 机器学习方法在复杂油藏早期地质勘探中，利用井和地震数据进行专家无关的概化预测

    Expert-Independent Generalization of Well and Seismic Data Using Machine Learning Methods for Complex Reservoirs Predicting During Early-Stage Geological Exploration. (arXiv:2304.03048v1 [physics.geo-ph])

    [http://arxiv.org/abs/2304.03048](http://arxiv.org/abs/2304.03048)

    该研究利用自主方法预测了复杂油藏的空间分布概率，可以进行专家无关的概化预测和地质模型创建。

    

    本研究旨在开发和应用一种自主方法，用于预测在研究区域内油藏传播的概率。自主性意味着在准备和输入地质地球物理信息之后，专家对算法的影响被最小化。该研究以研究区域早期勘探阶段的3D地震勘探数据和井信息为基础进行了研究。结果，为两组输入数据：基础组和反演校准后的组，预测了油藏空间分布的概率，并得到了标定后的概率立方体。本文所提出的方法可以对地质和地球物理数据进行专家无关的概化，并利用该概化进行假设检验和基于油藏概率表示的地质模型创建。算法的合格表现表明，在复杂油藏早期地质勘探中具有潜在的广泛应用。

    The aim of this study is to develop and apply an autonomous approach for predicting the probability of hydrocarbon reservoirs spreading in the studied area. Autonomy means that after preparing and inputting geological-geophysical information, the influence of an expert on the algorithms is minimized. The study was made based on the 3D seismic survey data and well information on the early exploration stage of the studied field. As a result, a forecast of the probability of spatial distribution of reservoirs was made for two sets of input data: the base set and the set after reverse-calibration, and three-dimensional cubes of calibrated probabilities of belonging of the studied space to the identified classes were obtained. The approach presented in the paper allows for expert-independent generalization of geological and geophysical data, and to use this generalization for hypothesis testing and creating geological models based on a probabilistic representation of the reservoir. The qual
    
[^176]: 中间特征联盟能帮助解释黑盒模型吗？

    Do intermediate feature coalitions aid explainability of black-box models?. (arXiv:2303.11920v1 [cs.LG])

    [http://arxiv.org/abs/2303.11920](http://arxiv.org/abs/2303.11920)

    本文引入了中间概念的级别结构，利用领域专家建立部分-整体关系，从而在不同抽象级别上帮助解释黑盒模型。

    

    本文引入了基于级别结构的中间概念，以帮助黑盒模型的可解释性。级别结构是一种分层结构，每个级别对应数据集的特征（即玩家集分区）。从只包含单元素的平凡集合到只包含大联盟的集合，粗糙度的级别逐渐增加。此外，可以通过领域专家建立部分-整体关系来生成抽象级别的解释。我们在一个实际的汽车模型示例和泰坦尼克号的数据集中说明了这种方法的可用性，其中中间概念在不同抽象级别上帮助解释。

    This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction.
    
[^177]: 多保真度深度算子网络方法适用于多尺度系统的封闭问题

    A Multifidelity deep operator network approach to closure for multiscale systems. (arXiv:2303.08893v1 [physics.comp-ph])

    [http://arxiv.org/abs/2303.08893](http://arxiv.org/abs/2303.08893)

    本文提出了一个基于多保真度深度算子网络框架和“内循环”训练方法解决多尺度系统的封闭问题的方法，并且在实验中得到了显著的改进。

    

    基于投影的降阶模型已经成功地用少量广义（或潜在）变量来表示多尺度系统的行为，但是由于未解析尺度和已解析尺度之间的相互作用（称为封闭问题）的不正确处理，导致降阶模型可能存在不准确性和甚至不稳定性。本文将封闭问题视为多保真度问题，并使用多保真度深度算子网络（DeepONet）框架来解决。此外，为了增强基于多保真度的封闭问题的稳定性和/或准确性，本文还采用了物理和机器学习模型耦合文献中最近开发的“内循环”训练方法。最终方法在一维粘性Burgers方程的激波输运和二维Navier-Stokes方程的漩涡合并方面进行了测试。数值实验表明了显著的改进。

    Projection-based reduced order models (PROMs) have shown promise in representing the behavior of multiscale systems using a small set of generalized (or latent) variables. Despite their success, PROMs can be susceptible to inaccuracies, even instabilities, due to the improper accounting of the interaction between the resolved and unresolved scales of the multiscale system (known as the closure problem). In the current work, we interpret closure as a multifidelity problem and use a multifidelity deep operator network (DeepONet) framework to address it. In addition, to enhance the stability and/or accuracy of the multifidelity-based closure, we employ the recently developed "in-the-loop" training approach from the literature on coupling physics and machine learning models. The resulting approach is tested on shock advection for the one-dimensional viscous Burgers equation and vortex merging for the two-dimensional Navier-Stokes equations. The numerical experiments show significant improv
    
[^178]: 非静态环境下的MNL-Bandit问题研究

    MNL-Bandit in non-stationary environments. (arXiv:2303.02504v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02504](http://arxiv.org/abs/2303.02504)

    本文研究了非静态环境下的MNL-Bandit问题，提出了一种算法，其最坏情况下的预期遗憾度为$\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\; N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} + \sqrt{NT}\right\}\right)$。算法基于时代算法，对由于非静态性引入的估计器偏差进行了紧致特征给出新的浓度界。

    

    本文研究了非静态环境下的MNL-Bandit问题，并提出了一种算法，其最坏情况下的预期遗憾度为$\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\; N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} + \sqrt{NT}\right\}\right)$。其中$N$是臂的数量，$L$是变化的数量，$\Delta_{\infty}^{K}$是未知参数的变化度量。此外，我们展示了期望遗憾度的匹配下界（对数因子内的下界），说明我们的算法是最优的。我们的方法基于Agrawal等人2016年提出的静态MNL-Bandit的时代算法。然而，非静态性带来了一些挑战，我们介绍了新的技术和想法来应对这些挑战。特别是，我们给出了由于非静态性引入的估计器偏差的紧致特征，并推导出新的浓度界。

    In this paper, we study the MNL-Bandit problem in a non-stationary environment and present an algorithm with a worst-case expected regret of $\tilde{O}\left( \min \left\{ \sqrt{NTL}\;,\; N^{\frac{1}{3}}(\Delta_{\infty}^{K})^{\frac{1}{3}} T^{\frac{2}{3}} + \sqrt{NT}\right\}\right)$. Here $N$ is the number of arms, $L$ is the number of changes and $\Delta_{\infty}^{K}$ is a variation measure of the unknown parameters. Furthermore, we show matching lower bounds on the expected regret (up to logarithmic factors), implying that our algorithm is optimal. Our approach builds upon the epoch-based algorithm for stationary MNL-Bandit in Agrawal et al. 2016. However, non-stationarity poses several challenges and we introduce new techniques and ideas to address these. In particular, we give a tight characterization for the bias introduced in the estimators due to non stationarity and derive new concentration bounds.
    
[^179]: 局部正则化的神经微分方程：有些黑盒子是要保持封闭的！

    Locally Regularized Neural Differential Equations: Some Black Boxes Were Meant to Remain Closed!. (arXiv:2303.02262v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02262](http://arxiv.org/abs/2303.02262)

    该论文提出了一种局部正则化的神经微分方程模型，通过启发式成本控制训练过程以学习易于积分的动力系统，同时提高预测速度，并保留了模型的自适应特性。

    

    由于自动适应的能力，类似神经微分方程的隐式层深度学习技术已经成为重要的建模框架。但是，控制这些模型的计算成本是困难的，因为它依赖于自适应求解器的步骤数。本文中，我们在随机时间点使用自适应微分方程求解器的内部成本启发式来引导训练以学习更易于积分的动力系统。我们增加局部正则化项到目标函数中，使得训练过程遵循经验条件并提高预测速度，同时仍保留了模型的自适应特性。

    Implicit layer deep learning techniques, like Neural Differential Equations, have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. However, controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement due to strict requirements on automatic differentiation. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time points to guide the training toward learning a dynamical system that is easier to integrate. We "close the black-box" and allow the use of ou
    
[^180]: 公平扩散：训练文本到图像生成模型实现公平性

    Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10893](http://arxiv.org/abs/2302.10893)

    这篇论文提出了一种名为“公平扩散”的新策略，可以在生成文本到图像模型部署后减轻偏见并使模型接受公平性指导。

    

    最近，生成式AI模型在质量方面取得了惊人的成果，并因此被广泛应用于越来越多的应用中。但由于它们高度依赖于从互联网上随机抽取的十亿级数据集，因此它们也会受到退化和偏见的人类行为的影响，正如我们所展示的那样。事实上，它们甚至可能加剧这些偏见。为了不仅揭示而且对抗这些不良影响，我们提出了一种新的策略，称为公平扩散，以在生成文本到图像模型部署后减轻偏见。具体而言，我们展示了基于人类指导的偏差转移，可在任何方向上产生任意新的比例，例如，身份组。正如我们的实证评估所示，这种控制使生成图像模型在公平性方面能够接受指导，无需数据过滤和额外的训练。

    Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.
    
[^181]: ChatGPT：应付千事的万能型 AI，但无所专精

    ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.10724](http://arxiv.org/abs/2302.10724)

    本研究检验了 ChatGPT 在 25 个不同的 NLP 任务上的性能，它是一个万能的 AI 模型，但无关紧要的表现可能会对某些任务的表现产生负面影响。

    

    OpenAI 推出了聊天生成预训练 Transformer（ChatGPT），革新了人工智能与人类互动的方法。许多研究通过测试 ChatGPT 在众所周知的自然语言处理（NLP）任务中的效果，来评估该模型的效能。然而，现有的研究大多非自动化，并且规模非常有限。本研究在 25 个不同的 NLP 任务上检验了 ChatGPT 的性能，其中大多数任务甚至对人类而言都是主观的，例如情感分析、情绪识别、攻击性和立场检测。另一些任务则需要更客观的推理，如词义消歧、语言可接受性和问答。我们还对 GPT-4 模型在五个选定的 NLP 任务子集上进行了评估。我们自动化了 ChatGPT 和 GPT-4 的引导过程，并分析了超过 49k 个响应。与现有最先进的解决方案（SOTA）进行比较，我们的结果显示，在一些任务上 ChatGPT 的性能存在一定的缺陷。

    OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
    
[^182]: 可微分的多目标因果贝叶斯实验设计

    Differentiable Multi-Target Causal Bayesian Experimental Design. (arXiv:2302.10607v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10607](http://arxiv.org/abs/2302.10607)

    本文提出了一种可微分的方法来解决因果模型的贝叶斯最佳实验设计问题，可以用于批处理设置下的学习，扩展了之前只能处理单目标状态对干涉的方法，结果比现有方法更好。

    

    我们引入了一种基于梯度的方法来解决贝叶斯最佳实验设计问题，以在批处理设置下学习因果模型，这是从有限数据中发现因果关系的关键组成部分，而干预可能是昂贵或有风险的。现有方法依赖于贪婪逼近来构建一批实验，同时使用黑盒方法来优化单个目标状态对的干预。在这项工作中，我们完全放弃了黑盒优化技术和贪婪启发式方法，而是提出了一个概念上简单的端到端梯度优化过程，以获取一组最优的干涉目标状态对。这样的过程能够对设计空间进行参数化，从而有效地优化一批多目标状态干预，这是由于其复杂性而迄今未被探索的设置。我们证明了我们提出的方法在单目标和多目标实验设计问题中都优于基线和现有的获取策略。

    We introduce a gradient-based approach for the problem of Bayesian optimal experimental design to learn causal models in a batch setting -- a critical component for causal discovery from finite data where interventions can be costly or risky. Existing methods rely on greedy approximations to construct a batch of experiments while using black-box methods to optimize over a single target-state pair to intervene with. In this work, we completely dispose of the black-box optimization techniques and greedy heuristics and instead propose a conceptually simple end-to-end gradient-based optimization procedure to acquire a set of optimal intervention target-state pairs. Such a procedure enables parameterization of the design space to efficiently optimize over a batch of multi-target-state interventions, a setting which has hitherto not been explored due to its complexity. We demonstrate that our proposed method outperforms baselines and existing acquisition strategies in both single-target and 
    
[^183]: 基于编码计算和向量承诺的拜占庭抵抗安全聚合方案，用于联邦学习 (arXiv:2302.09913v2 [cs.CR] UPDATED)

    ByzSecAgg: A Byzantine-Resistant Secure Aggregation Scheme for Federated Learning Based on Coded Computing and Vector Commitment. (arXiv:2302.09913v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.09913](http://arxiv.org/abs/2302.09913)

    本文提出了一种基于编码计算和向量承诺的拜占庭抵抗安全聚合方案，用于联邦学习。该方案通过RAM秘密共享将本地更新分割成较小子向量，并使用双重RAMP共享技术实现成对距离的安全计算。

    

    本文提出了一种高效的联邦学习保护方案，可以抵御拜占庭攻击和隐私泄露。这种方案通过处理单个更新来管理对抗行为，并在抵御串通节点的同时保护数据隐私。然而，用于对更新向量进行安全秘密共享的通信负载可能非常高。为了解决这个问题，本文提出了一种将本地更新分割成较小子向量并使用RAM秘密共享的方案。但是，这种共享方法无法进行双线性计算，例如需要异常检测算法的成对距离计算。为了克服这个问题，每个用户都会运行另一轮RAMP共享，该共享具有不同的数据嵌入其中。这种受编码计算思想启发的技术实现了成对距离的安全计算。

    In this paper, we propose an efficient secure aggregation scheme for federated learning that is protected against Byzantine attacks and privacy leakages. Processing individual updates to manage adversarial behavior, while preserving privacy of data against colluding nodes, requires some sort of secure secret sharing. However, communication load for secret sharing of long vectors of updates can be very high. To resolve this issue, in the proposed scheme, local updates are partitioned into smaller sub-vectors and shared using ramp secret sharing. However, this sharing method does not admit bi-linear computations, such as pairwise distance calculations, needed by outlier-detection algorithms. To overcome this issue, each user runs another round of ramp sharing, with different embedding of data in the sharing polynomial. This technique, motivated by ideas from coded computing, enables secure computation of pairwise distance. In addition, to maintain the integrity and privacy of the local u
    
[^184]: 在上下文强化学习领域进行在线连续超参数优化

    Online Continuous Hyperparameter Optimization for Contextual Bandits. (arXiv:2302.09440v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09440](http://arxiv.org/abs/2302.09440)

    该论文提出了面向上下文强化学习的在线连续超参数调整框架CDT，能够动态地在搜索空间内学习最优参数配置。

    

    在随机上下文强化学习中，代理根据过去的经验从时间相关行动集中依次采取行动，以最小化总后悔。与许多其他机器学习算法一样，强化学习的性能严重依赖于其多个超参数，并且理论推导出的参数值可能导致实际上不令人满意的结果。此外，在强化学习环境下使用离线优化方法（如交叉验证）选择超参数是不可行的，因为决策必须实时进行。因此，我们提出了第一个面向上下文强化学习的在线连续超参数调整框架，以学习飞行中的最佳参数配置。具体而言，我们使用了一个名为CDT（Continuous Dynamic Tuning）的双层强化学习框架，并将超参数优化形式化为非平稳连续武器强化学习，在其中每个武器代表一种超参数组合。

    In stochastic contextual bandits, an agent sequentially makes actions from a time-dependent action set based on past experience to minimize the cumulative regret. Like many other machine learning algorithms, the performance of bandits heavily depends on their multiple hyperparameters, and theoretically derived parameter values may lead to unsatisfactory results in practice. Moreover, it is infeasible to use offline tuning methods like cross-validation to choose hyperparameters under the bandit environment, as the decisions should be made in real time. To address this challenge, we propose the first online continuous hyperparameter tuning framework for contextual bandits to learn the optimal parameter configuration within a search space on the fly. Specifically, we use a double-layer bandit framework named CDT (Continuous Dynamic Tuning) and formulate the hyperparameter optimization as a non-stationary continuum-armed bandit, where each arm represents a combination of hyperparameters, a
    
[^185]: 几何深度学习仅依靠距离矩阵足够吗？

    Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05743](http://arxiv.org/abs/2302.05743)

    本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。

    

    图神经网络（GNN）常用于涉及图形几何的任务，例如分子动力学模拟。虽然几何图的距离矩阵包含完整的几何信息，但已经证明消息传递神经网络（MPNNs）无法学习这种几何信息。本文通过构造新颖的对称几何图的家族，扩展了MPNN无法区分其距离矩阵的反例家族，并提出$k$-DisGNNs，可以有效地利用距离矩阵中丰富的几何结构。我们证明了模型的高表达能力，并证明了一些现有的精心设计的几何模型可以作为$k$-DisGNNs的特殊情况统一起来。最重要的是，我们建立了几何深度学习和传统图表示学习之间的联系，展示了那些最初为低度表达能力的GNN模型设计的高度表达力的GNN模型。

    Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
    
[^186]: 多传感器强化学习的联合表示

    Joint Representations for Reinforcement Learning with Multiple Sensors. (arXiv:2302.05342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05342](http://arxiv.org/abs/2302.05342)

    本文提出了一种基于重构和对比损失的方法来对多传感器的输入进行联合表示学习，证明了这种方法在完成复杂任务时具有良好的效果。

    

    在强化学习中有效地结合多个传感器的输入是一个待解决的问题。尽管存在许多自监督表示学习方法来提高基于图像的强化学习的性能和样本复杂性，但它们通常忽略其他可用信息，如机器人本体感知。然而，利用这种本体感知进行表示学习可以帮助算法聚焦于相关方面，并指导其寻找更好的表示。在本文中，我们通过基于递归状态空间模型，从多个传感器中对强化学习的表示学习进行了系统分析。我们提出了一种基于重构和对比损失的组合方法，使我们能够为每个传感器模态选择最合适的方法。我们证明了联合表示的好处，特别是对于每个模态具有不同损失函数的无模型和模型基础强化学习，以完成复杂的任务，包括图像包含分散的视觉信息或缺少足够的上下文线索的任务。

    Combining inputs from multiple sensor modalities effectively in reinforcement learning (RL) is an open problem. While many self-supervised representation learning approaches exist to improve performance and sample complexity for image-based RL, they usually neglect other available information, such as robot proprioception. However, using this proprioception for representation learning can help algorithms to focus on relevant aspects and guide them toward finding better representations. In this work, we systematically analyze representation learning for RL from multiple sensors by building on Recurrent State Space Models. We propose a combination of reconstruction-based and contrastive losses, which allows us to choose the most appropriate method for each sensor modality. We demonstrate the benefits of joint representations, particularly with distinct loss functions for each modality, for model-free and model-based RL on complex tasks. Those include tasks where the images contain distra
    
[^187]: 源于超取样的信息论泛化界限更紧密

    Tighter Information-Theoretic Generalization Bounds from Supersamples. (arXiv:2302.02432v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02432](http://arxiv.org/abs/2302.02432)

    本文介绍了一种新颖的信息论泛化界限，利用投影损失对，与Rademacher序列相关联来源于超取样的设置，这些界限比同一超取样设置中迄今已知的所有信息理论界限都更紧密。

    

    本文介绍了针对学习算法的各种新颖的信息论泛化界限，源于Steinke＆Zakynthinou（2020）的超取样设置-“条件互信息”框架的设置。我们的开发利用将损失对（从训练实例和测试实例获得）投影到单个数字，并将损失值与Rademacher序列（及其移动变体）相关联。所呈现的界限包括平方根界限，快速率界限，包括基于方差和尖锐度的界限以及插值算法的界限等。我们理论上或经验上证明，这些界限比同一超取样设置中迄今已知的所有信息理论界限都更紧密。

    In this work, we present a variety of novel information-theoretic generalization bounds for learning algorithms, from the supersample setting of Steinke & Zakynthinou (2020)-the setting of the "conditional mutual information" framework. Our development exploits projecting the loss pair (obtained from a training instance and a testing instance) down to a single number and correlating loss values with a Rademacher sequence (and its shifted variants). The presented bounds include square-root bounds, fast-rate bounds, including those based on variance and sharpness, and bounds for interpolating algorithms etc. We show theoretically or empirically that these bounds are tighter than all information-theoretic bounds known to date on the same supersample setting.
    
[^188]: 低位视觉变压器的无振荡量化

    Oscillation-free Quantization for Low-bit Vision Transformers. (arXiv:2302.02210v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02210](http://arxiv.org/abs/2302.02210)

    研究发现，可学习比例因子加剧了权重振荡，本文提出三种技术以解决这个问题，并在多个基准测试上显著提高了模型的性能。

    

    量化意识训练的一个不良副作用是权重振荡，其中量化权重经常在两个量化级别之间跳动，导致训练不稳定和子优化的最终模型。本研究发现，可学习的比例因子——在量化中广泛使用的$\textit{de facto}$设置——加剧了权重振荡。本研究研究了可学习比例因子与量化权重振荡之间的关系，并以ViT为案例来说明发现和解决方法。此外，我们还发现自注意力层中量化权重的$\textit{query}$和$\textit{key}$之间的相互依存使ViT容易受到振荡的影响。因此，我们相应地提出了三种技术：统计权重量化（$\rm StatsQ$）以改善量化鲁棒性，与普遍使用的可学习比例因子方法相比；置信度引导的退火（$\rm CGA$）在训练期间冻结具有$\textit{高置信度}$的权重，以减少权重振荡；以及相互依赖权重的均衡（$\rm IWEqual$），以有效处理相互依赖问题。我们的实验表明，我们提出的方法相比于最先进的方法在多个基准测试上显著提高了模型的性能。

    Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confi
    
[^189]: 概率编程中的自动边缘化 MCMC

    Automatically Marginalized MCMC in Probabilistic Programming. (arXiv:2302.00564v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00564](http://arxiv.org/abs/2302.00564)

    本文提出了在概率编程中使用自动边缘化作为采样过程的一部分，使用 HMC 在从 PPL 中提取的图形模型中进行采样，显著提高了从现实世界的层次模型中采样的效率。

    

    Hamiltonian Monte Carlo (HMC) 是从贝叶斯模型中采样潜在变量的一种强大算法。概率编程语言 (PPL) 的出现使用户摆脱了编写推断算法的烦恼，并让用户专注于模型建立。然而，许多模型都难以直接使用 HMC 解决，通常需要使用一些技巧，如模型重新参数化。我们的动机在于许多这些模型可能通过边缘化来简化。我们提出将自动边缘化作为采样过程的一部分，使用 HMC 在从 PPL 中提取的图形模型中进行采样，这显著提高了从现实世界的层次模型中采样的效率。

    Hamiltonian Monte Carlo (HMC) is a powerful algorithm to sample latent variables from Bayesian models. The advent of probabilistic programming languages (PPLs) frees users from writing inference algorithms and lets users focus on modeling. However, many models are difficult for HMC to solve directly, and often require tricks like model reparameterization. We are motivated by the fact that many of those models could be simplified by marginalization. We propose to use automatic marginalization as part of the sampling process using HMC in a graphical model extracted from a PPL, which substantially improves sampling from real-world hierarchical models.
    
[^190]: 可能任何时候安全的随机组合半臂赌博问题

    Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits. (arXiv:2301.13393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13393](http://arxiv.org/abs/2301.13393)

    本文提出了可能任何时候安全的随机组合半臂赌博问题，并设计出算法PASCombUCB在时间轴上最小化后悔值。

    

    本文旨在解决在线决策中可能造成过度风险的问题，提出了可能任何时候安全的随机组合半臂赌博问题。在这个问题中，智能体有选择从$L$个基础项中不超过$K$个进行子集的选项。每个元素都与某个平均奖励和表示其风险的方差相关联。为了减少代理人所遭受的风险，我们要求，在整个时间$T$的时间跨度上，智能体所做的每个选择都应包含其方差之和不超过某个方差预算的元素，且其可能任何时候满足此约束。在此约束条件下，我们设计和分析了一种算法PASCombUCB，以在时间轴上最小化后悔值。通过开发配套信息理论下界，我们证明在问题相关和问题无关的两种范例下，算法都是最优的。

    Motivated by concerns about making online decisions that incur undue amount of risk at each time step, in this paper, we formulate the probably anytime-safe stochastic combinatorial semi-bandits problem. In this problem, the agent is given the option to select a subset of size at most $K$ from a set of $L$ ground items. Each item is associated to a certain mean reward as well as a variance that represents its risk. To mitigate the risk that the agent incurs, we require that with probability at least $1-\delta$, over the entire horizon of time $T$, each of the choices that the agent makes should contain items whose sum of variances does not exceed a certain variance budget. We call this probably anytime-safe constraint. Under this constraint, we design and analyze an algorithm {\sc PASCombUCB} that minimizes the regret over the horizon of time $T$. By developing accompanying information-theoretic lower bounds, we show that under both the problem-dependent and problem-independent paradig
    
[^191]: 具有线性函数逼近的对抗性MDP的精细后悔

    Refined Regret for Adversarial MDPs with Linear Function Approximation. (arXiv:2301.12942v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12942](http://arxiv.org/abs/2301.12942)

    本论文研究了在对抗性马尔可夫决策过程（MDP）中的学习问题，提出了两种算法，可以将现有最佳方法中的后悔从$\tilde{\mathcal O}(K^{2/3})$降低到$\tilde{\mathcal O}(\sqrt K)$。其中第一种算法使用对数壁垒正则化器的跟随正则化者（FTRL）算法实现，在损失估计器任意负的情况下有效。第二种算法利用幅度降低的损失估计器，进一步消除了与动作数量多项式相关的依赖关系。

    

    本文考虑了在对抗性马尔可夫决策过程（MDP）中的学习，其中损失函数可以在$K$个回合内任意更改，状态空间可以任意大。我们假设任何策略的Q函数在某些已知特征上是线性的，即存在线性函数逼近。对于这种设置，现有的最佳后悔上界是$\tilde {\mathcal O}(K^{2/3})$（省略所有其他依赖项），假设有模拟器。本文提供了两种算法，可以在相同的设置下将后悔改进为$\tilde{\mathcal O}(\sqrt K)$。我们的第一个算法利用了精细分析带有对数壁垒正则化器的跟随正则化者（FTRL）算法。此分析允许损失估计器任意负，并且可能具有独立的利益。我们的第二个算法开发了一个幅度降低的损失估计器，进一步消除了第一个算法中与动作数量多项式相关的依赖关系。

    We consider learning in an adversarial Markov Decision Process (MDP) where the loss functions can change arbitrarily over $K$ episodes and the state space can be arbitrarily large. We assume that the Q-function of any policy is linear in some known features, that is, a linear function approximation exists. The best existing regret upper bound for this setting (Luo et al., 2021) is of order $\tilde{\mathcal O}(K^{2/3})$ (omitting all other dependencies), given access to a simulator. This paper provides two algorithms that improve the regret to $\tilde{\mathcal O}(\sqrt K)$ in the same setting. Our first algorithm makes use of a refined analysis of the Follow-the-Regularized-Leader (FTRL) algorithm with the log-barrier regularizer. This analysis allows the loss estimators to be arbitrarily negative and might be of independent interest. Our second algorithm develops a magnitude-reduced loss estimator, further removing the polynomial dependency on the number of actions in the first algorit
    
[^192]: 深度尺度：在ImageNet上实现稳健性认证

    Scaling in Depth: Unlocking Robustness Certification on ImageNet. (arXiv:2301.12549v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12549](http://arxiv.org/abs/2301.12549)

    本文提出了一些新策略和方法解决了证明深度网络稳健性的难点，引入了Linear ResNet架构和Efficient Margin MAximization损失函数，最终实现了新的最先进稳健准确性。

    

    尽管基于Lipschitz方法在确定性保证下能够实现稳健深度学习的承诺，但目前最先进的结果仅限于对低维数据，例如CIFAR-10的前馈卷积网络（ConvNet）。本文研究了将可证明的稳健训练扩展到更大、更深模型的策略。证明深度网络的一个关键挑战是计算ResNet和ViT体系结构中的残差块的Lipschitz界的高效方法。我们展示了用于常规ResNet的Lipschitz常数边界的快速方法往往不准确，并展示了如何通过设计新的残差块来解决这个问题，从而实现了\emph{Linear ResNet} (LiResNet)架构。然后，我们介绍了\emph{Efficient Margin MAximization} (EMMA)损失函数，通过同时惩罚来自\emph{所有}类别的最坏情况对抗性示例稳定稳健训练。这些贡献共同产生了新的\emph{最先进}的稳健准确性。

    Despite the promise of Lipschitz-based methods for provably-robust deep learning with deterministic guarantees, current state-of-the-art results are limited to feed-forward Convolutional Networks (ConvNets) on low-dimensional data, such as CIFAR-10. This paper investigates strategies for expanding certifiably robust training to larger, deeper models. A key challenge in certifying deep networks is efficient calculation of the Lipschitz bound for residual blocks found in ResNet and ViT architectures. We show that fast ways of bounding the Lipschitz constant for conventional ResNets are loose, and show how to address this by designing a new residual block, leading to the \emph{Linear ResNet} (LiResNet) architecture. We then introduce \emph{Efficient Margin MAximization} (EMMA), a loss function that stabilizes robust training by simultaneously penalizing worst-case adversarial examples from \emph{all} classes. Together, these contributions yield new \emph{state-of-the-art} robust accuracy 
    
[^193]: 基于两阶段动态规划的高效卷积神经网络深度压缩算法

    Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming. (arXiv:2301.12187v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12187](http://arxiv.org/abs/2301.12187)

    本文提出了基于两阶段动态规划的深度压缩算法，该算法能够将神经网络的深度合并成等效的浅层卷积操作，从而实现高效的推理延迟和内存占用，同时不会影响模型精度。

    

    近期针对神经网络修剪的最新研究表明，通过减少网络深度而非通道剪枝来减少运行时内存使用和加速推理延迟更加有效。其中，一些最近的工作提出了合并卷积层的深度压缩算法。然而，现有的算法有一个狭窄的搜索空间，并依赖于人工设计的启发式策略。本文提出了一种针对通用卷积运算的新型深度压缩算法。我们提出了一种子集选择问题，用恒等函数替换低效激活层，并将连续的卷积操作优化地合并成浅层等效卷积操作，实现端到端的高效推理延迟。由于所提出的子集选择问题是NP-hard问题，我们提出了一个代理优化问题，可以通过两阶段动态规划在几秒钟内精确求解。

    Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evalu
    
[^194]: 用神经Wasserstein梯度流求解带有Riesz核的最大均值差异

    Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with Riesz Kernels. (arXiv:2301.11624v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11624](http://arxiv.org/abs/2301.11624)

    本文提出用神经网络逼近Jordan、Kinderlehrer和Otto的反向方案以及一种前向方案，用于计算非光滑Riesz核最大均值差异函数的Wasserstein梯度流。我们通过学习适当的损失函数来近似处理计划的分解，并在交互能上基准测量神经网络的质量。

    

    非光滑的Riesz核最大均值差异函数的Wasserstein梯度流显示出丰富的结构，奇异测度可以变成绝对连续的测度，反之亦然。本文旨在贡献于对这种流的理解。我们提出用神经网络（NN）逼近Jordan、Kinderlehrer和Otto的反向方案，以计算这种Wasserstein梯度流，同时提出了一种前向方案，用于所谓的Wasserstein最陡下降流。因为我们不能把自己限制在绝对连续的量度上，所以我们必须处理传输计划和速度计划，而不是通常的传输映射和速度场。的确，我们用生成的NN近似了两个计划的分解，这些计划是根据适当的损失函数学习的。为了评估两个神经系统的质量，我们将其基准化为相互作用能。在这里，我们提供了由Dirac测度开始的Wasserstein方案的解析公式。

    Wasserstein gradient flows of maximum mean discrepancy (MMD) functionals with non-smooth Riesz kernels show a rich structure as singular measures can become absolutely continuous ones and conversely. In this paper we contribute to the understanding of such flows. We propose to approximate the backward scheme of Jordan, Kinderlehrer and Otto for computing such Wasserstein gradient flows as well as a forward scheme for so-called Wasserstein steepest descent flows by neural networks (NNs). Since we cannot restrict ourselves to absolutely continuous measures, we have to deal with transport plans and velocity plans instead of usual transport maps and velocity fields. Indeed, we approximate the disintegration of both plans by generative NNs which are learned with respect to appropriate loss functions. In order to evaluate the quality of both neural schemes, we benchmark them on the interaction energy. Here we provide analytic formulas for Wasserstein schemes starting at a Dirac measure and s
    
[^195]: 数学建模的失落艺术

    The Lost Art of Mathematical Modelling. (arXiv:2301.08559v2 [q-bio.OT] UPDATED)

    [http://arxiv.org/abs/2301.08559](http://arxiv.org/abs/2301.08559)

    本文批评数学生物学过度专注于分析模型，忽视了制定模型，提出了通过采用开放/多元化的方法逆转这一趋势，对任何给定的生物现象进行无限次建模，以重新发掘失落的创造性数学建模艺术。

    

    针对现代机器学习的快速发展，我们对数学生物学提出了批评。我们认为，在数学生物学中的三个模型建立活动——（1）制定模型，（2）分析模型，（3）将模型拟合或与数据进行比较——研究人员目前过度专注于活动（2），从而疏忽了活动（1）。我们建议通过采取开放/多元化的方法来逆转这一趋势，从而对任何给定的生物现象进行无限次建模。我们以鱼类运动作为案例研究，解释了开放方法，并阐述了阻碍数学生物学的一些陷阱（普遍主义、模型的模型等）。最后，我们探讨了如何重新发现一种失落的艺术：创造性的数学建模。本文献给Edmund Crampin的记忆。

    We provide a critique of mathematical biology in light of rapid developments in modern machine learning. We argue that out of the three modelling activities -- (1) formulating models; (2) analysing models; and (3) fitting or comparing models to data -- inherent to mathematical biology, researchers currently focus too much on activity (2) at the cost of (1). This trend, we propose, can be reversed by realising that any given biological phenomena can be modelled in an infinite number of different ways, through the adoption of an open/pluralistic approach. We explain the open approach using fish locomotion as a case study and illustrate some of the pitfalls -- universalism, creating models of models, etc. -- that hinder mathematical biology. We then ask how we might rediscover a lost art: that of creative mathematical modelling.  This article is dedicated to the memory of Edmund Crampin.
    
[^196]: 使用自回归变压器和条件正态化流协调实现端到端分层时间序列建模

    End-to-End Modeling Hierarchical Time Series Using Autoregressive Transformer and Conditional Normalizing Flow based Reconciliation. (arXiv:2212.13706v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13706](http://arxiv.org/abs/2212.13706)

    本文提出了一种基于自回归变压器和条件正态化流协调的端到端分层时间序列预测模型，实现了同时预测和协调的功能。

    

    分层结构的多元时间序列预测在现实世界中普遍存在，不仅需要预测层次结构的每个级别，而且还需要协调所有的预测结果以确保一致性，即预测结果应满足层次聚合约束。本文提出了一种新的端到端分层时间序列预测模型，基于条件正态化流的自回归变压器协调的方法。我们实现了同时进行预测和协调，无需任何显式的后处理步骤。此外，通过利用深度模型的强大功能，我们不依赖于任何偏差估计等假设。

    Multivariate time series forecasting with hierarchical structure is pervasive in real-world applications, demanding not only predicting each level of the hierarchy, but also reconciling all forecasts to ensure coherency, i.e., the forecasts should satisfy the hierarchical aggregation constraints. Moreover, the disparities of statistical characteristics between levels can be huge, worsened by non-Gaussian distributions and non-linear correlations. To this extent, we propose a novel end-to-end hierarchical time series forecasting model, based on conditioned normalizing flow-based autoregressive transformer reconciliation, to represent complex data distribution while simultaneously reconciling the forecasts to ensure coherency. Unlike other state-of-the-art methods, we achieve the forecasting and reconciliation simultaneously without requiring any explicit post-processing step. In addition, by harnessing the power of deep model, we do not rely on any assumption such as unbiased estimates 
    
[^197]: 当联邦学习遇到预训练语言模型的参数高效调整方法

    When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods. (arXiv:2212.10025v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10025](http://arxiv.org/abs/2212.10025)

    本文在隐私敏感的自然语言处理任务中探讨了联邦学习如何与参数高效调整方法结合以解决数据异质性问题，在维持可接受性能的同时显著减少通信开销。

    

    随着对数据隐私关注的增加，最近的研究在隐私敏感的自然语言处理（NLP）任务中使用联邦学习（FL）取得了显著进展。很多文献建议在 FL 范式中完全微调预训练语言模型（PLMs）可以缓解数据异质性问题，缩小与集中式训练的性能差距。然而，大型 PLMs 带来了沉重的通信开销和 FL 系统的本地模型适应成本。为此，我们将各种参数高效调整（PETuning）方法引入到联邦学习中。具体而言，我们提供了一个全面的经验研究，研究了 FL 中代表性的 PLMs 调整方法。实验结果覆盖了数据异质性水平、数据规模和不同 FL 场景的分析。在维持可接受性能的同时，通过本地调整和全局聚合轻量级模型参数，可以显著减少总的通信开销。

    With increasing privacy concerns on data, recent studies have made significant progress using federated learning (FL) on privacy-sensitive natural language processing (NLP) tasks. Much literature suggests fully fine-tuning pre-trained language models (PLMs) in the FL paradigm can mitigate the data heterogeneity problem and close the performance gap with centralized training. However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system. To this end, we introduce various parameter-efficient tuning (PETuning) methods into federated learning. Specifically, we provide a holistic empirical study of representative PLMs tuning methods in FL. The experimental results cover the analysis of data heterogeneity levels, data scales, and different FL scenarios. Overall communication overhead can be significantly reduced by locally tuning and globally aggregating lightweight model parameters while maintaining acceptable performance in var
    
[^198]: PATO: 基于策略辅助的可伸缩机器人数据采集系统

    PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection. (arXiv:2212.04708v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.04708](http://arxiv.org/abs/2212.04708)

    PATO是一个策略辅助的远程操作系统，通过自动执行部分演示采集过程，并仅在不确定要执行哪个子任务或行为时请求人类输入，提高了数据采集效率，同时减少了人工操作员的心理负担，实现单个操作员并行控制多个机器人的功能。

    

    大规模数据是机器学习的重要组成部分，正如在自然语言处理和计算机视觉研究的最新进展中所证明的那样。但是，采集大规模机器人数据的成本更高，速度更慢，因为每个操作员只能同时控制一个机器人。为了使这个昂贵的数据采集过程高效和可伸缩，我们提出了策略辅助的远程操作系统（PATO），该系统利用学习到的辅助策略自动执行部分演示采集过程。PATO在数据采集中自动执行重复的行为，并仅在不确定要执行哪个子任务或行为时请求人类输入。我们进行了与真实机器人和模拟机器人群体的远程操作用户研究，并证明我们的辅助远程操作系统能够减少人工操作员的心理负担，同时提高数据采集效率。此外，它使单个操作员能够并行控制多个机器人，这是首次。

    Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However, collecting large-scale robotic data is much more expensive and slower as each operator can control only a single robot at a time. To make this costly data collection process efficient and scalable, we propose Policy Assisted TeleOperation (PATO), a system which automates part of the demonstration collection process using a learned assistive policy. PATO autonomously executes repetitive behaviors in data collection and asks for human input only when it is uncertain about which subtask or behavior to execute. We conduct teleoperation user studies both with a real robot and a simulated robot fleet and demonstrate that our assisted teleoperation system reduces human operators' mental load while improving data collection efficiency. Further, it enables a single operator to control multiple robots in parallel, which is a first
    
[^199]: PrefRec：利用人类偏好加强长期用户参与的推荐系统

    PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement. (arXiv:2212.02779v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.02779](http://arxiv.org/abs/2212.02779)

    该论文提出了一种基于人类偏好的推荐系统范式PrefRec，允许强化学习推荐系统从用户历史行为偏好中学习，以优化长期用户参与度。

    

    目前，推荐系统在优化即时参与方面取得了令人瞩目的成功。然而，更可取的绩效指标长期用户参与度的提高仍然很难。与此同时，最近的强化学习算法在各种长期目标优化任务中展现了有效性。因此，强化学习被广泛认为是优化推荐中长期用户参与度的有前途的框架。虽然有前途，但应用强化学习在很大程度上依赖于精心设计的奖励，但设计与长期用户参与有关的奖励相当困难。为了缓解这个问题，我们提出了一种新的范式，即以人类偏好为基础的推荐系统，允许强化学习推荐系统从有关用户历史行为的偏好中学习，而不是从明确定义的奖励中学习。这些偏好可以通过众包等技术轻松获得。

    Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they 
    
[^200]: 学习在线包装通用三维形状的可实现物理技能

    Learning Physically Realizable Skills for Online Packing of General 3D Shapes. (arXiv:2212.02094v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02094](http://arxiv.org/abs/2212.02094)

    该论文研究了学习在线包装不规则三维形状的物理实现技能的问题，提出了一种强化学习管道并采用候选行动生成方法来减小学习负担。

    

    我们研究了学习在线包装不规则三维形状的技能的问题，这可能是箱子装箱问题中最具挑战性的情境。目标是连续移动具有任意形状的3D对象序列到一个指定的容器中，并仅观察部分对象。同时，我们考虑了物理实现可能性，涉及放置的物理动力学和约束。包装策略应了解要包装的3D几何形状，并做出有效的决策来以物理可行的方式容纳它们在容器中。我们提出了一种强化学习（RL）管道来学习这种策略。复杂的不规则几何形状和不完美的对象放置共同导致了巨大的解空间。直接在这样的空间中进行训练需要大量的数据。我们提出了一种经过理论验证的候选行动生成方法，以减少RL的行动空间和学习负担。一个参数化的。

    We study the problem of learning online packing skills for irregular 3D shapes, which is arguably the most challenging setting of bin packing problems. The goal is to consecutively move a sequence of 3D objects with arbitrary shapes into a designated container with only partial observations of the object sequence. Meanwhile, we take physical realizability into account, involving physics dynamics and constraints of a placement. The packing policy should understand the 3D geometry of the object to be packed and make effective decisions to accommodate it in the container in a physically realizable way. We propose a Reinforcement Learning (RL) pipeline to learn the policy. The complex irregular geometry and imperfect object placement together lead to huge solution space. Direct training in such space is prohibitively data intensive. We instead propose a theoretically-provable method for candidate action generation to reduce the action space of RL and the learning burden. A parameterized po
    
[^201]: 高维情况下的基于边缘采样：主动学习效率不如被动学习

    Margin-based sampling in high dimensions: When being active is less efficient than staying passive. (arXiv:2212.00772v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00772](http://arxiv.org/abs/2212.00772)

    基于边缘值的主动学习在高维情况下效率不如被动学习，即使对于无噪声数据和使用贝叶斯最优决策边界进行采样，被动学习仍然更优。特别是在类之间的分离较小的情况下，这种现象更加明显。

    

    通常认为，给定相同的标注预算，基于边缘值的主动学习算法会比被动学习算法获得更好的预测性能，尽管计算成本更高。最近的经验证据表明，这种额外的成本可能是徒劳的，因为基于边缘值的主动学习有时甚至比被动学习表现更差。本文在逻辑回归的背景下证明，即使对于无噪声数据和使用贝叶斯最优决策边界进行采样，被动学习仍然优于基于边缘值的主动学习。我们证明所得的结论表明，该现象在高维情况下加剧，特别是在类之间的分离较小的情况下。我们通过对20个高维数据集的实验进行实证，证实了这种直觉，这些数据集涵盖了从金融和组织学到化学和计算机等各种应用领域。

    It is widely believed that given the same labeling budget, active learning (AL) algorithms like margin-based active learning achieve better predictive performance than passive learning (PL), albeit at a higher computational cost. Recent empirical evidence suggests that this added cost might be in vain, as margin-based AL can sometimes perform even worse than PL. While existing works offer different explanations in the low-dimensional regime, this paper shows that the underlying mechanism is entirely different in high dimensions: we prove for logistic regression that PL outperforms margin-based AL even for noiseless data and when using the Bayes optimal decision boundary for sampling. Insights from our proof indicate that this high-dimensional phenomenon is exacerbated when the separation between the classes is small. We corroborate this intuition with experiments on 20 high-dimensional datasets spanning a diverse range of applications, from finance and histology to chemistry and comput
    
[^202]: SWL-Adapt: 一种基于样本权重学习的跨用户可穿戴人体活动识别无监督领域自适应模型

    SWL-Adapt: An Unsupervised Domain Adaptation Model with Sample Weight Learning for Cross-User Wearable Human Activity Recognition. (arXiv:2212.00724v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2212.00724](http://arxiv.org/abs/2212.00724)

    SWL-Adapt是一种新的跨用户可穿戴人体活动识别的无监督领域自适应方法，通过样本权重学习来区分不同样本，提高了该任务的性能表现。

    

    在现实中，可穿戴人体活动识别模型通常面临着跨用户性能下降的问题，主要是由于用户变化引起的。无监督领域自适应成为了跨用户可穿戴人体活动识别的自然解决方案。现有的无监督领域自适应模型通常在领域间对齐样本，但不进行样本间的区分，忽略了样本间的差异。在本文中，我们提出了一种基于样本权重学习的跨用户可穿戴人体活动识别无监督领域自适应模型(SWL-Adapt)。SWL-Adapt根据网络参数计算每个样本的分类损失和领域辨别损失的样本权重。我们引入基于元优化的更新规则，通过选定的伪标记目标样本的元分类损失来指导这个网络的端到端学习。因此，这个网络可以根据手头的跨用户可穿戴人体活动识别任务适应一个加权函数，这优于现有的针对样本细分的规则固定做法。

    In practice, Wearable Human Activity Recognition (WHAR) models usually face performance degradation on the new user due to user variance. Unsupervised domain adaptation (UDA) becomes the natural solution to cross-user WHAR under annotation scarcity. Existing UDA models usually align samples across domains without differentiation, which ignores the difference among samples. In this paper, we propose an unsupervised domain adaptation model with sample weight learning (SWL-Adapt) for cross-user WHAR. SWL-Adapt calculates sample weights according to the classification loss and domain discrimination loss of each sample with a parameterized network. We introduce the meta-optimization based update rule to learn this network end-to-end, which is guided by meta-classification loss on the selected pseudo-labeled target samples. Therefore, this network can fit a weighting function according to the cross-user WHAR task at hand, which is superior to existing sample differentiation rules fixed for s
    
[^203]: 一种解决离线强化学习分布偏移的风险规避方法

    One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion. (arXiv:2212.00124v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00124](http://arxiv.org/abs/2212.00124)

    本研究提出了一种基于风险规避机制的离线强化学习方法，同时解决了避免分布偏移和避免灾难性结果的风险问题，并取得了明显的改进。

    

    离线强化学习（RL）适用于在线探索不可行的安全关键领域。在这种领域中，决策应考虑到灾难性结果的风险。换句话说，决策应该是风险规避的。离线RL的另一个挑战是避免分布偏移，即确保策略访问的状态-操作对靠近数据集中的状态-操作对。以往的研究将离线RL技术（以避免分布偏移）与风险敏感型RL算法（以实现风险规避）相结合。在本文中，我们提出了将风险规避机制作为同时解决这两个问题的方法。我们提出一种基于模型的方法，使用模型集合来估计认知不确定性和随机不确定性。我们训练了一个风险规避的策略，避免高不确定性的行为。对认知不确定性的风险规避可以防止分布偏移，因为避免了数据集中未涵盖的区域。我们在几个标准基准测试中展示了这种方法的有效性，并显示出比先前处理这些挑战的工作明显的改进。

    Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be risk-averse. An additional challenge of offline RL is avoiding distributional shift, i.e. ensuring that state-action pairs visited by the policy remain near those in the dataset. Previous works on risk in offline RL combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address both of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered 
    
[^204]: 自监督连续适应黎曼空间图学习

    Self-Supervised Continual Graph Learning in Adaptive Riemannian Spaces. (arXiv:2211.17068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.17068](http://arxiv.org/abs/2211.17068)

    本文提出了一种自监督连续适应黎曼空间图学习的方法，解决了现有方法忽视曲率变化和对标签依赖过高的问题。

    

    连续的图学习在各种需要依次出现图数据的实际应用中扮演着重要的角色。尽管先前的研究取得了一定成就，但仍面临巨大挑战。一方面，现有方法在零曲率欧几里得空间中工作，很大程度上忽视了曲率随即出现的图序列而变化的事实。另一方面，文献中不断的学习器依赖丰富的标签，但实践中标记图尤其是针对不断出现的图，对实时性的要求很高。为了应对上述挑战，我们提出了一个具有挑战性但实用的问题，即自监督连续适应黎曼空间图学习。我们提出了一种新颖的自监督黎曼图连续学习器（RieGrace）。在RieGrace中，我们首先设计了自适应黎曼GCN（AdaRGCN），该方法将GCN与神经曲率适配器耦合在一起，使得黎曼空间可以适应变化。

    Continual graph learning routinely finds its role in a variety of real-world applications where the graph data with different tasks come sequentially. Despite the success of prior works, it still faces great challenges. On the one hand, existing methods work with the zero-curvature Euclidean space, and largely ignore the fact that curvature varies over the coming graph sequence. On the other hand, continual learners in the literature rely on abundant labels, but labeling graph in practice is particularly hard especially for the continuously emerging graphs on-the-fly. To address the aforementioned challenges, we propose to explore a challenging yet practical problem, the self-supervised continual graph learning in adaptive Riemannian spaces. In this paper, we propose a novel self-supervised Riemannian Graph Continual Learner (RieGrace). In RieGrace, we first design an Adaptive Riemannian GCN (AdaRGCN), a unified GCN coupled with a neural curvature adapter, so that Riemannian space is s
    
[^205]: DiffPhase: 基于扩散的STFT相位恢复生成方法

    DiffPhase: Generative Diffusion-based STFT Phase Retrieval. (arXiv:2211.04332v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.04332](http://arxiv.org/abs/2211.04332)

    本研究提出了一种基于扩散的STFT相位恢复生成方法，适用于填补缺失数据的问题，性能超越了传统的和现代的方法。

    

    近年来，扩散概率模型已被广泛应用于包括语音增强和合成在内的各种任务中。作为一种生成方法，扩散模型在填补缺失数据的问题上表现出色，其中缺失的数据是基于现有数据生成的。相位检索本质上是一种填补缺失数据的问题，其中相位信息必须基于给定的幅度生成。在本研究中，我们建立在语音领域先前的工作之上，专门针对STFT相位恢复调整了语音增强扩散模型。使用语音质量和可懂度度量进行评估表明，扩散方法非常适用于相位检索任务，性能超越了传统的和现代的方法。

    Diffusion probabilistic models have been recently used in a variety of tasks, including speech enhancement and synthesis. As a generative approach, diffusion models have been shown to be especially suitable for imputation problems, where missing data is generated based on existing data. Phase retrieval is inherently an imputation problem, where phase information has to be generated based on the given magnitude. In this work we build upon previous work in the speech domain, adapting a speech enhancement diffusion model specifically for STFT phase retrieval. Evaluation using speech quality and intelligibility metrics shows the diffusion approach is well-suited to the phase retrieval task, with performance surpassing both classical and modern methods.
    
[^206]: 不精确 Langevin 算法与基于得分的生成模型在 KL 散度中的收敛性

    Convergence of the Inexact Langevin Algorithm and Score-based Generative Models in KL Divergence. (arXiv:2211.01512v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01512](http://arxiv.org/abs/2211.01512)

    本文研究了不精确 Langevin 算法与基于得分的生成模型在 KL 散度中的收敛性，提出了建立稳定偏差收敛保证的两个关键假设：目标分布满足对数 Sobolev 不等式和分数估计器展示出有界的矩阵生成函数误差。作者探讨了如何获得可靠的分数估计器，并证明了基于核密度估计的简单估计器满足假设。

    

    本文研究了不精确 Langevin 动力学（ILD）、不精确 Langevin 算法（ILA）和基于得分的生成建模（SGM）在利用估计得分函数进行采样时的情况。我们的重点在于建立关于 Kullback-Leibler（KL）散度的稳定偏差收敛保证。为了实现这些保证，我们采用了两个关键假设：1）目标分布满足对数 Sobolev 不等式（LSI），2）分数估计器展示出一个有界的矩阵生成函数（MGF）误差。值得注意的是，我们采用的 MGF 误差假设相比现有文献中使用的 $L^\infty$ 误差假设更为宽松。然而，它比最近的作品中使用的 $L^2$ 误差假设更强，后者常常导致不稳定的边界。我们探讨了如何获得满足 MGF 误差假设的可靠分数估计器的问题。具体来说，我们证明了一种基于核密度估计的简单估计器满足 MGF 误差假设。

    We study the Inexact Langevin Dynamics (ILD), Inexact Langevin Algorithm (ILA), and Score-based Generative Modeling (SGM) when utilizing estimated score functions for sampling. Our focus lies in establishing stable biased convergence guarantees in terms of the Kullback-Leibler (KL) divergence. To achieve these guarantees, we impose two key assumptions: 1) the target distribution satisfies the log-Sobolev inequality (LSI), and 2) the score estimator exhibits a bounded Moment Generating Function (MGF) error. Notably, the MGF error assumption we adopt is more lenient compared to the $L^\infty$ error assumption used in existing literature. However, it is stronger than the $L^2$ error assumption utilized in recent works, which often leads to unstable bounds. We explore the question of how to obtain a provably accurate score estimator that satisfies the MGF error assumption. Specifically, we demonstrate that a simple estimator based on kernel density estimation fulfills the MGF error assumpt
    
[^207]: 联邦学习中的本地模型重构攻击及其应用

    Local Model Reconstruction Attacks in Federated Learning and their Uses. (arXiv:2210.16205v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16205](http://arxiv.org/abs/2210.16205)

    本文研究了联邦学习中的本地模型重构攻击，攻击者可以通过重构客户端的模型更有效地触发其他攻击，并提出了一种新颖的基于模型的属性推导攻击，实证结果表明攻击有效，可应用于回归和分类任务。

    

    本文探讨了联邦学习中的本地模型重构攻击，攻击者可以通过窃听目标客户端与服务器之间的通信并重构受害者的本地/个性化模型，从而更有效地触发其他经典攻击。本地模型仅依赖于客户端的数据，但可能泄漏比服务器学习的全局模型更多的私有信息。此外，我们提出了一种新的基于模型的属性推导攻击，利用了本地模型重构攻击。我们提供了这种攻击的分析下界。使用真实世界数据集的实证结果表明，我们的本地重构攻击对于回归和分类任务都有效。此外，我们将我们的新颖的属性推导攻击与最先进的攻击进行了基准测试。

    In this paper, we initiate the study of local model reconstruction attacks for federated learning, where a honest-but-curious adversary eavesdrops the messages exchanged between a targeted client and the server, and then reconstructs the local/personalized model of the victim. The local model reconstruction attack allows the adversary to trigger other classical attacks in a more effective way, since the local model only depends on the client's data and can leak more private information than the global model learned by the server. Additionally, we propose a novel model-based attribute inference attack in federated learning leveraging the local model reconstruction attack. We provide an analytical lower-bound for this attribute inference attack. Empirical results using real world datasets confirm that our local reconstruction attack works well for both regression and classification tasks. Moreover, we benchmark our novel attribute inference attack against the state-of-the-art attacks in 
    
[^208]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^209]: 领袖与追随者：深度多智能体强化学习中的Stackelberg均衡

    Oracles & Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning. (arXiv:2210.11942v4 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2210.11942](http://arxiv.org/abs/2210.11942)

    本文提出了一个通用框架，将Stackelberg均衡搜索作为多智能体RL问题进行实现，提出了一种利用上下文策略的方法，并在标准和新颖的基准领域上进行实验评估，证明相比之前的方法，样本效率得到了极大提高。

    

    Stackelberg均衡在多个学习问题中自然出现，例如在安全博弈或间接机制设计中，并在强化学习文献中受到越来越多的关注。我们提出了一个实现Stackelberg均衡搜索的通用框架，将其作为多智能体RL问题，允许进行各种算法设计选择，并将之前的方法视为此框架的特定实例。另外，我们注意到设计空间允许使用以前在文献中没有见过的方法，例如利用多任务和元-RL技术实现追随者的收敛。我们提出了一种使用上下文策略的方法，并在标准和新颖的基准领域上进行实验评估，证明了相比以前的方法，样本效率大大提高。最后，我们还探索了采用框架边界外的算法设计的效果。

    Stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or indirect mechanism design, and have received increasing attention in the reinforcement learning literature. We present a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, allowing a wide range of algorithmic design choices. We discuss how previous approaches can be seen as specific instantiations of this framework. As a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-RL techniques for follower convergence. We propose one such approach using contextual policies, and evaluate it experimentally on both standard and novel benchmark domains, showing greatly improved sample efficiency compared to previous approaches. Finally, we explore the effect of adopting algorithm designs outside the borders of our framework.
    
[^210]: 自共轭障碍哈密尔顿蒙特卡洛的无偏约束采样

    Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo. (arXiv:2210.11925v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.11925](http://arxiv.org/abs/2210.11925)

    本文提出了一种名为BHMC的新的蒙特卡罗采样算法，能够从定义了约束的黎曼流形中进行无偏采样，其中包含一种新的过滤步骤involution checking step。

    

    本文提出了障碍哈密尔顿蒙特卡罗(BHMC)，它是HMC算法的一种变体，旨在从带有自共轭障碍度量的流形中的Gibbs分布π中进行采样。该方法依赖于包含度量的Hamiltonian动力学。因此，它包含定义流形的约束，并能够利用其底层几何形状。然而，相应的Hamilton动力学是通过不可分离的常微分方程来定义的，与欧几里得情况相反。这意味着将HMC推广到黎曼流形中会产生不可避免的偏差。为解决这个问题，我们提出了一种新的过滤步骤，称为“involution检查步骤”。该步骤在两个BHMC版本——连续BHMC(c-BHMC)和数值BHMC(n-BHMC)中实现。我们的主要结果表明，这两个新算法生成可逆Markov链且无偏。

    In this paper, we propose Barrier Hamiltonian Monte Carlo (BHMC), a version of the HMC algorithm which aims at sampling from a Gibbs distribution $\pi$ on a manifold $\mathrm{M}$, endowed with a Hessian metric $\mathfrak{g}$ derived from a self-concordant barrier. Our method relies on Hamiltonian dynamics which comprises $\mathfrak{g}$. Therefore, it incorporates the constraints defining $\mathrm{M}$ and is able to exploit its underlying geometry. However, the corresponding Hamiltonian dynamics is defined via non separable Ordinary Differential Equations (ODEs) in contrast to the Euclidean case. It implies unavoidable bias in existing generalization of HMC to Riemannian manifolds. In this paper, we propose a new filter step, called "involution checking step", to address this problem. This step is implemented in two versions of BHMC, coined continuous BHMC (c-BHMC) and numerical BHMC (n-BHMC) respectively. Our main results establish that these two new algorithms generate reversible Mark
    
[^211]: 在社交媒体上表征和检测国家赞助的喷子活动

    Characterizing and Detecting State-Sponsored Troll Activity on Social Media. (arXiv:2210.08786v5 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2210.08786](http://arxiv.org/abs/2210.08786)

    该研究提出了一种基于AI的新解决方案，通过分析喷子的轨迹，识别国家赞助的喷子帐户。该方法可以准确识别俄罗斯喷子与有机用户，可提供国家赞助影响活动的早期警报，并对保护民主进程做出贡献。

    

    检测在影响活动中运营的国家赞助的喷子是研究社区的一个关键而未解决的挑战，其影响超出了在线领域。为了解决这个挑战，我们提出了一种新的基于AI的解决方案，通过分析喷子的分享活动序列或轨迹，通过两个步骤识别国家赞助的喷子帐户。首先，我们使用基于LSTM的分类器将帐户的轨迹分类为国家赞助的喷子或有机的合法用户。其次，我们利用分类后的轨迹计算一种指标，称为“喷子评分”，来量化帐户的行为与国家赞助的喷子的相似程度。为了评估我们的方法，我们研究了2016年美国总统大选期间的俄罗斯干预活动。我们实验结果表明，我们的方法可以识别帐户轨迹，AUC接近99％，并准确分类俄罗斯喷子和有机用户，F1分数分别为0.95和0.91。我们的解决方案可以集成到现有系统中，提供国家赞助影响活动的早期警报，并对保护民主进程做出贡献。

    The detection of state-sponsored trolls operating in influence campaigns is a critical and unsolved challenge for the research community, which has significant implications beyond the online realm. To address this challenge, we propose a new AI-based solution that identifies state-sponsored troll accounts by analyzing their sharing activity sequences, or trajectories, through a two-step process. First, we classify accounts' trajectories using an LSTM-based classifier as belonging to either a state-sponsored troll or an organic, legitimate user. Second, we utilize the classified trajectories to compute a metric, named ``Troll Score'', to quantify the extent to which an account behaves like a state-sponsored troll. To evaluate our approach, we examine the Russian interference campaign during the 2016 U.S. Presidential election. The results of our experiments show that our method can identify account trajectories with an AUC close to 99% and accurately classify Russian trolls and organic 
    
[^212]: 通过推理时自适应优化实现语言生成中的统一去毒化和去偏见

    Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization. (arXiv:2210.04492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04492](http://arxiv.org/abs/2210.04492)

    本文提出了名为UDDIA的统一去毒化和去偏见框架，它能够有效地消除有毒语言和减少社会偏见，同时保持流畅性。

    

    近年来，预训练语言模型在各种自然语言生成任务中取得了很好的表现。然而，这些模型在训练语料库中捕捉和复制有害内容的情况普遍存在，特别是毒性语言和社会偏见，引起了严重的道德问题。此前在道德上的自然语言生成方面的工作都是分开解决去毒化和去偏见这两个问题的，但我们发现去偏见的模型仍然存在毒性，而经过去毒化的模型甚至会加剧社会偏见，这是有问题的。为了解决这个挑战，我们提出了名为UDDIA的统一去毒化和去偏见框架，将这两个问题作为纠正输出空间的关键问题联合形式化。我们在理论上将我们的框架解释为学习混合加权属性的文本分布。此外，UDDIA对少量参数进行自适应优化，从而控制每个属性的贡献。我们证明了UDDIA能够有效地消除有毒语言并减少社会偏见，同时保持流畅性。此外，UDDIA在广泛的评估中优于最先进的去毒化和去偏见方法。我们还进行了深入的分析，解释了UDDIA的行为，并为未来在道德自然语言生成方面的工作提供了见解。

    Warning: this paper contains model outputs exhibiting offensiveness and biases. Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate social biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based o
    
[^213]: 监督式度量学习以排序为目标的检索，通过上下文相似性优化

    Supervised Metric Learning to Rank for Retrieval via Contextual Similarity Optimization. (arXiv:2210.01908v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01908](http://arxiv.org/abs/2210.01908)

    这项研究提出了一种新的度量学习方法——上下文损失，它通过优化上下文相似性隐式地确保邻居之间的语义一致性，并表现出更好的鲁棒性和抗噪声能力，取得了四个图像检索基准的最新最佳结果。

    

    在图像检索领域，度量学习方法备受关注。很多度量学习的损失函数都集中于对训练样本的正确排序，但是往往会过度拟合不一致的语义标签，需要大量的数据。为了解决这些缺陷，我们提出了一种新的度量学习方法——上下文损失，它除了优化余弦相似度以外，还优化了上下文相似性。我们的上下文损失能隐式地确保邻居之间的语义一致性并收敛于正确的排序。我们通过实验表明所提出的损失对标签噪声更加鲁棒，并且即使在保留大部分训练数据的情况下仍然不易过拟合。广泛的实验表明，我们的方法在四个图像检索基准和多种不同的评估环境下以新的最优状态取得了成功。本文代码可在以下链接中获得：https://github.com/Chris210634/metric-learning-using-contextual-similarity。

    There is extensive interest in metric learning methods for image retrieval. Many metric learning loss functions focus on learning a correct ranking of training samples, but strongly overfit semantically inconsistent labels and require a large amount of data. To address these shortcomings, we propose a new metric learning method, called contextual loss, which optimizes contextual similarity in addition to cosine similarity. Our contextual loss implicitly enforces semantic consistency among neighbors while converging to the correct ranking. We empirically show that the proposed loss is more robust to label noise, and is less prone to overfitting even when a large portion of train data is withheld. Extensive experiments demonstrate that our method achieves a new state-of-the-art across four image retrieval benchmarks and multiple different evaluation settings. Code is available at: https://github.com/Chris210634/metric-learning-using-contextual-similarity
    
[^214]: 结构化PCA中的贝叶斯极限及其实现方法

    Bayes-optimal limits in structured PCA, and how to reach them. (arXiv:2210.01237v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2210.01237](http://arxiv.org/abs/2210.01237)

    本文针对带尖峰的PCA问题，利用正交多项式矩阵抽取噪声模型提出了该模型中推断的贝叶斯最优极限的表征，并提出了一种新的AMP算法以实现信息论极限。

    

    测量噪声中的统计相关性如何影响高维推断？为了回答这个问题，我们研究了范例性的带尖峰矩阵模型的主成分分析（PCA）问题，在这个问题中，一个秩为一的矩阵被加性噪声污染，从而超越了噪声项独立的假设。我们从低阶多项式正交矩阵集合中抽取噪声，产生的噪声相关性使得该设置对应用领域具有相关性但在理论上具有挑战性。我们首次提供了该模型中推断的贝叶斯最优极限的表征。如果尖峰在旋转下是不变的，则标准谱PCA是最优的。然而，对于更一般的先验，无论是PCA还是现有的近似信息传递算法（AMP）都无法达到信息论极限，我们使用统计力学中的重复方法计算了这一极限。因此，我们提出了一种新的AMP算法，灵感来自自适应Thouless-Anderson-Palmer理论。

    How do statistical dependencies in measurement noise influence high-dimensional inference? To answer this, we study the paradigmatic spiked matrix model of principal components analysis (PCA), where a rank-one matrix is corrupted by additive noise. We go beyond the usual independence assumption on the noise entries, by drawing the noise from a low-order polynomial orthogonal matrix ensemble. The resulting noise correlations make the setting relevant for applications but analytically challenging. We provide the first characterization of the Bayes-optimal limits of inference in this model. If the spike is rotation-invariant, we show that standard spectral PCA is optimal. However, for more general priors, both PCA and the existing approximate message passing algorithm (AMP) fall short of achieving the information-theoretic limits, which we compute using the replica method from statistical mechanics. We thus propose a novel AMP, inspired by the theory of Adaptive Thouless-Anderson-Palmer e
    
[^215]: 开发一种解释性的机器学习方法支持的电子病历标注可视化交互工具

    Developing A Visual-Interactive Interface for Electronic Health Record Labeling: An Explainable Machine Learning Approach. (arXiv:2209.12778v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12778](http://arxiv.org/abs/2209.12778)

    本文介绍了一种解释性标注助手工具XLabel，通过可解释性提升机（EBM）和可视化展现，帮助医疗专家标记非传染性疾病（NCDs）的电子病历，与其他知名机器学习模型相比，EBM的准确性更佳。

    

    标注大量电子病历的工作量大且费时，拥有一个标注助手工具可以显著减轻医疗专家的工作量，但为了赢得专家的信任，该工具必须能够解释其结果背后的原因。本文介绍了一种新的解释性标注助手工具XLabel，该工具利用可解释性提升机（EBM）对每个数据点的标签进行分类，并可视化EBM解释的热图。我们使用XLabel作为案例研究，帮助医疗专家标记了四种常见的非传染性疾病(NCDs)的电子病历。实验结果表明：1）XLabel有助于减少标注操作的次数；2）作为一种可解释的分类器，EBM的准确性与其他知名的机器学习模型相当，优于NCD专家使用的基于规则的模型；3）即使超过40%的记录被有意误标，EBM仍能保持较高的准确性。

    Labeling a large number of electronic health records is expensive and time consuming, and having a labeling assistant tool can significantly reduce medical experts' workload. Nevertheless, to gain the experts' trust, the tool must be able to explain the reasons behind its outputs. Motivated by this, we introduce Explainable Labeling Assistant (XLabel) a new visual-interactive tool for data labeling. At a high level, XLabel uses Explainable Boosting Machine (EBM) to classify the labels of each data point and visualizes heatmaps of EBM's explanations. As a case study, we use XLabel to help medical experts label electronic health records with four common non-communicable diseases (NCDs). Our experiments show that 1) XLabel helps reduce the number of labeling actions, 2) EBM as an explainable classifier is as accurate as other well-known machine learning models outperforms a rule-based model used by NCD experts, and 3) even when more than 40% of the records were intentionally mislabeled, E
    
[^216]: 在未知统计信息的多服务器系统中学习调度:结合折扣上置信度与MaxWeight

    Learning While Scheduling in Multi-Server Systems with Unknown Statistics: MaxWeight with Discounted UCB. (arXiv:2209.01126v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.01126](http://arxiv.org/abs/2209.01126)

    本文提出了一种新算法，将MaxWeight调度策略与折扣上置信度（UCB）相结合，以同时学习统计信息和将作业调度到服务器上，在多服务器系统中实现最大化利用服务器的处理能力。

    

    多服务器队列系统是机器学习、无线网络、众包和医疗系统中作业调度的广泛模型。本文考虑了一个多服务器多作业类型的系统，不知道处理时间的统计信息。目标是在不知道处理时间的情况下，在服务器上安排作业。为了充分利用服务器的处理能力，至少需要学习不同类型作业在不同服务器上的服务率。我们提出了一种新算法，将MaxWeight调度策略与折扣上置信度（UCB）相结合，以同时学习统计信息和将作业调度到服务器上。我们证明，在我们的算法下，渐近平均队列长度是有界的。

    Multi-server queueing systems are widely used models for job scheduling in machine learning, wireless networks, crowdsourcing, and healthcare systems. This paper considers a multi-server system with multiple servers and multiple types of jobs, where different job types require different amounts of processing time at different servers. The goal is to schedule jobs on servers without knowing the statistics of the processing times. To fully utilize the processing power of the servers, it is known that one has to at least learn the service rates of different job types on different servers. Prior works on this topic decouple the learning and scheduling phases which leads to either excessive exploration or extremely large job delays. We propose a new algorithm, which combines the MaxWeight scheduling policy with discounted upper confidence bound (UCB), to simultaneously learn the statistics and schedule jobs to servers. We prove that under our algorithm the asymptotic average queue length is
    
[^217]: 可微分编程用于地球系统模型

    Differentiable Programming for Earth System Modeling. (arXiv:2208.13825v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13825](http://arxiv.org/abs/2208.13825)

    可微分编程有助于改进地球系统模型，并解决气候变化模拟中存在的关键缺陷。

    

    地球系统模型是研究未来几十年到几个世纪的地球系统状态的主要工具，特别是在人为温室气体释放的影响下。最先进的地球系统模型可以重现过去150年的观测全球平均温度异常。然而，地球系统模型需要进一步改进，最重要的是关于(i)气候敏感度估计的大范围差异，即温室气体增加的温度响应；(ii)关键变量（如温度和降水）的空间模式；(iii)极端天气事件的表示；以及(iv)多稳态地球系统组件的表示及其预测相关突变的能力。本文认为，使地球系统模型自动可微分具有推动地球系统模型进一步发展的巨大潜力，特别是在这些关键缺陷方面。首先，自动可微分可以允许客观优化。

    Earth System Models (ESMs) are the primary tools for investigating future Earth system states at time scales from decades to centuries, especially in response to anthropogenic greenhouse gas release. State-of-the-art ESMs can reproduce the observational global mean temperature anomalies of the last 150 years. Nevertheless, ESMs need further improvements, most importantly regarding (i) the large spread in their estimates of climate sensitivity, i.e., the temperature response to increases in atmospheric greenhouse gases, (ii) the modeled spatial patterns of key variables such as temperature and precipitation, (iii) their representation of extreme weather events, and (iv) their representation of multistable Earth system components and their ability to predict associated abrupt transitions. Here, we argue that making ESMs automatically differentiable has huge potential to advance ESMs, especially with respect to these key shortcomings. First, automatic differentiability would allow objecti
    
[^218]: 具有通用迷你批量一致性和无偏完全集合梯度近似的可扩展集合编码。

    Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation. (arXiv:2208.12401v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12401](http://arxiv.org/abs/2208.12401)

    本文提出了一种可扩展的集合编码方法UMBC，可以与任意非MBC组件相结合，同时仍满足MBC；同时提出了一种高效的MBC训练算法，可以为任何集合大小在训练和测试时都具有恒定的内存开销，给出完整集合梯度的无偏近似。

    

    近期，关于集合函数的小批量一致性(MBC)的研究引起了人们对于保证将一个分割的集合的部分顺序处理和聚合，而保证所有分割的输出相同的需求的关注。然而，现有的MBC架构的限制导致了具有有限表达能力的模型。此外，先前的研究没有解决在需要完整集合梯度的情况下如何处理训练中的大型集合。为了解决这些问题，我们提出了一种可用于任意非-MBC组件相结合的通用MBC (UMBC) 类集合函数，同时仍满足MBC，使得MBC设置中可以使用更广泛的功能类。此外，我们提出了一种高效的MBC训练算法，它能够为任何集合大小在训练和测试时都具有恒定的内存开销，给出完整集合梯度的无偏近似。我们进行了广泛的实验，包括图像完成、文本分类、无监督聚类等。

    Recent work on mini-batch consistency (MBC) for set functions has brought attention to the need for sequentially processing and aggregating chunks of a partitioned set while guaranteeing the same output for all partitions. However, existing constraints on MBC architectures lead to models with limited expressive power. Additionally, prior work has not addressed how to deal with large sets during training when the full set gradient is required. To address these issues, we propose a Universally MBC (UMBC) class of set functions which can be used in conjunction with arbitrary non-MBC components while still satisfying MBC, enabling a wider range of function classes to be used in MBC settings. Furthermore, we propose an efficient MBC training algorithm which gives an unbiased approximation of the full set gradient and has a constant memory overhead for any set size for both train- and test-time. We conduct extensive experiments including image completion, text classification, unsupervised cl
    
[^219]: 带外部输入的MDPs的追溯学习

    Hindsight Learning for MDPs with Exogenous Inputs. (arXiv:2207.06272v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.06272](http://arxiv.org/abs/2207.06272)

    提出了一种数据高效的带有外部输入的MDPs算法，名为追溯学习（HL）。HL算法通过利用外部变量样本使得过去的决策在回溯中可以加速策略改进，在资源管理问题中表现出良好的性能。

    

    许多资源管理问题需要在不确定性下做出迭代决策，其中影响决策结果的唯一不确定性是决策者控制之外的外部变量。我们将这些问题建模为带有外部输入的MDPs（马尔可夫决策过程），并设计了一类名为追溯学习（HL）的数据高效算法。我们的HL算法通过利用一个关键洞见实现了数据效率：通过外部变量的样本，过去的决策可以在回溯中重新审视，以推断出可以加速策略改进的反事实后果。我们将HL与多个基线算法在多个测试案例中进行比较，包括多秘书和航空公司收益管理问题。我们还将我们的算法扩展到业务关键的云资源管理问题——将虚拟机（VM）分配到物理机器上，并使用来自大型公共云提供商的真实数据集模拟其性能。我们发现HL算法优于基准算法。

    Many resource management problems require sequential decision-making under uncertainty, where the only uncertainty affecting the decision outcomes are exogenous variables outside the control of the decision-maker. We model these problems as Exo-MDPs (Markov Decision Processes with Exogenous Inputs) and design a class of data-efficient algorithms for them termed Hindsight Learning (HL). Our HL algorithms achieve data efficiency by leveraging a key insight: having samples of the exogenous variables, past decisions can be revisited in hindsight to infer counterfactual consequences that can accelerate policy improvements. We compare HL against classic baselines in the multi-secretary and airline revenue management problems. We also scale our algorithms to a business-critical cloud resource management problem -- allocating Virtual Machines (VMs) to physical machines, and simulate their performance with real datasets from a large public cloud provider. We find that HL algorithms outperform d
    
[^220]: 动态空间稀疏化：高效视觉变压器和卷积神经网络的新方法

    Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks. (arXiv:2207.01580v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.01580](http://arxiv.org/abs/2207.01580)

    基于视觉变压器中的空间稀疏性，本文提出了一种动态标记稀疏化框架，可加速各种结构的模型，被剪枝的冗余标记由轻量级预测模块逐步动态估计。

    

    本文提出了一种利用视觉数据中的空间稀疏性加速模型的新方法。我们观察到视觉变压器中最终预测仅基于一小部分信息最丰富的标记，这对于准确的图像识别足够。基于这一观察，我们提出了一种动态标记稀疏化框架，根据输入逐步和动态地剪枝冗余标记以加速视觉变压器。具体而言，我们设计了一个轻量级预测模块，以根据当前特征来估计每个标记的重要性得分。该模块被添加到不同层中以分层地剪枝冗余标记。虽然该框架来源于我们对视觉变压器中稀疏注意力的观察，但我们发现自适应和非对称计算的思想可以成为加速各种结构的一般解决方案。我们将我们的方法扩展到包括卷积神经网络和分层视觉变压器在内的分层模型中。

    In this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. We observe that the final prediction in vision Transformers is only based on a subset of the most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision Transformers. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. While the framework is inspired by our observation of the sparse attention in vision Transformers, we find the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. We extend our method to hierarchical models including CNNs and hierarchical vision Transformers 
    
[^221]: 推荐系统中的供给侧均衡

    Supply-Side Equilibria in Recommender Systems. (arXiv:2206.13489v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2206.13489](http://arxiv.org/abs/2206.13489)

    本论文探究了推荐系统中个性化内容的供给侧均衡问题，其特点是生产者决策空间是多维的和用户群体是异构的，高维度和异质性的模型创造了专业化的可能性。

    

    算法推荐系统（如Spotify和Netflix）不仅影响消费者行为，而且影响生产者的激励。生产者试图创建将被推荐算法显示的内容，这可能影响他们内容的多样性和质量。本文研究个性化内容推荐系统中的供给侧均衡。我们将用户和内容建模为 $D$ 维向量，推荐算法显示每个用户与之最高点积的内容，生产者最大化被推荐其内容的用户数减去生产成本。我们模型的两个关键特征是生产者决策空间是多维的，用户群体是异构的，这与经典低维模型不同。多维性和异质性创造了专业化的可能性，不同的生产者在均衡状态下创建不同类型的内容。使用对偶论证法

    Algorithmic recommender systems such as Spotify and Netflix affect not only consumer behavior but also producer incentives. Producers seek to create content that will be shown by the recommendation algorithm, which can impact both the diversity and quality of their content. In this work, we investigate the resulting supply-side equilibria in personalized content recommender systems. We model users and content as $D$-dimensional vectors, the recommendation algorithm as showing each user the content with highest dot product, and producers as maximizing the number of users who are recommended their content minus the cost of production. Two key features of our model are that the producer decision space is multi-dimensional and the user base is heterogeneous, which contrasts with classical low-dimensional models.  Multi-dimensionality and heterogeneity create the potential for specialization, where different producers create different types of content at equilibrium. Using a duality argumen
    
[^222]: 可微和可传输的结构学习

    Differentiable and Transportable Structure Learning. (arXiv:2206.06354v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06354](http://arxiv.org/abs/2206.06354)

    D-Struct是一种可微和可传输的结构学习方法，通过新颖的架构和损失函数使得结构可以在同一领域的不同数据集中传输，比NOTEARS和其他最先进的方法具有更好的性能。

    

    有向无环图在它们的结构中编码了关于特定分布的大量信息。然而，推断这些结构所需的计算通常是变量数的超指数，因为推断需要扫描一个组合数量巨大的潜在结构空间。直到最近的进展才使得使用可微度量搜索这个空间成为可能，从而极大地减少了搜索时间。我们介绍了D-Struct，它通过一种新颖的架构和损失函数恢复了发现结构在同一领域中的传输性，同时仍然完全可微。因为D-Struct仍然是可微的，所以我们的方法可以轻松地应用于现有的可微框架中。

    Directed acyclic graphs (DAGs) encode a lot of information about a particular distribution in their structure. However, compute required to infer these structures is typically super-exponential in the number of variables, as inference requires a sweep of a combinatorially large space of potential structures. That is, until recent advances made it possible to search this space using a differentiable metric, drastically reducing search time. While this technique -- named NOTEARS -- is widely considered a seminal work in DAG-discovery, it concedes an important property in favour of differentiability: transportability. To be transportable, the structures discovered on one dataset must apply to another dataset from the same domain. We introduce D-Struct which recovers transportability in the discovered structures through a novel architecture and loss function while remaining fully differentiable. Because D-Struct remains differentiable, our method can be easily adopted in existing different
    
[^223]: 基于图的深度学习在分布式系统异常检测中的应用综述

    A Survey of Graph-based Deep Learning for Anomaly Detection in Distributed Systems. (arXiv:2206.04149v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04149](http://arxiv.org/abs/2206.04149)

    本文综述并比较了基于图的深度学习算法在分布式系统异常检测中的应用现状，提出了其处理异构和动态结构等现实世界挑战的能力。

    

    异常检测是复杂分布式系统中关键的任务，深入了解此类系统的要求和挑战对于保障其安全尤为重要。本文综述了基于图的算法在分布式系统中识别异常的潜力。这些系统可以是异构或同构的，从而产生不同的要求。本研究的一个目标是深入分析基于图的方法实现处理异构和动态结构等现实世界挑战的能力。本研究总结了该领域中的最新研究文章并比较归纳其特点。为了更全面地了解该主题，我们还提出了三个具体系统的应用案例。

    Anomaly detection is a crucial task in complex distributed systems. A thorough understanding of the requirements and challenges of anomaly detection is pivotal to the security of such systems, especially for real-world deployment. While there are many works and application domains that deal with this problem, few have attempted to provide an in-depth look at such systems. In this survey, we explore the potentials of graph-based algorithms to identify anomalies in distributed systems. These systems can be heterogeneous or homogeneous, which can result in distinct requirements. One of our objectives is to provide an in-depth look at graph-based approaches to conceptually analyze their capability to handle real-world challenges such as heterogeneity and dynamic structure. This study gives an overview of the State-of-the-Art (SotA) research articles in the field and compare and contrast their characteristics. To facilitate a more comprehensive understanding, we present three systems with v
    
[^224]: 机器学习中的公平性：现状、反思与展望

    What-is and How-to for Fairness in Machine Learning: A Survey, Reflection, and Perspective. (arXiv:2206.04101v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04101](http://arxiv.org/abs/2206.04101)

    本文回顾和反思了机器学习文献中以前提出的各种公平概念，并试图将其与道德和政治哲学中的论点联系起来。在考虑到不同类型公平性所涉及的显性假设和预期结果的差异后，我们提出了一个流程图，它包括对数据生成过程、预测结果和引发影响的不同类型公平性调查的隐性假设和预期结果。

    

    算法公平性引起了机器学习社区越来越多的关注。文献中提出了各种定义，但它们之间的区别和联系尚未得到清晰的解决。本文回顾了和反思了机器学习文献中以前提出的各种公平概念，并试图将其与道德和政治哲学中的论点联系起来，特别是公正理论方面。我们还从动态的角度考虑公平性的问题，并进一步考虑当前预测和决策所引发的长期影响。在考虑到不同类型公平性所涉及的显性假设和预期结果的差异后，我们提出了一个流程图，它包括对数据生成过程、预测结果和引发影响的不同类型公平性调查的隐性假设和预期结果。本文展示了匹配使命（即希望实现哪种公平性）与公平性措施之间的重要性。

    Algorithmic fairness has attracted increasing attention in the machine learning community. Various definitions are proposed in the literature, but the differences and connections among them are not clearly addressed. In this paper, we review and reflect on various fairness notions previously proposed in machine learning literature, and make an attempt to draw connections to arguments in moral and political philosophy, especially theories of justice. We also consider fairness inquiries from a dynamic perspective, and further consider the long-term impact that is induced by current prediction and decision. In light of the differences in the characterized fairness, we present a flowchart that encompasses implicit assumptions and expected outcomes of different types of fairness inquiries on the data generating process, on the predicted outcome, and on the induced impact, respectively. This paper demonstrates the importance of matching the mission (which kind of fairness one would like to e
    
[^225]: 随机时变图上的分散在线正则化学习

    Decentralized Online Regularized Learning Over Random Time-Varying Graphs. (arXiv:2206.03861v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03861](http://arxiv.org/abs/2206.03861)

    本文研究了随机时变图上的分散在线正则化线性回归算法，提出了非负超-鞅不等式的估计误差，证明了算法在满足样本路径时空兴奋条件时，节点的估计可以收敛于未知的真实参数向量。

    

    本文研究了在随机时变图上的分散在线正则化线性回归算法。在每个时间步中，每个节点都运行一个在线估计算法，该算法包括创新项（处理自身新测量值）、共识项（加权平均自身及其邻居的估计，带有加性和乘性通信噪声）和正则化项（防止过度拟合）。不要求回归矩阵和图满足特殊的统计假设，如相互独立、时空独立或平稳性。我们发展了非负超-鞅不等式的估计误差，并证明了如果算法增益、图和回归矩阵共同满足样本路径时空兴奋条件，节点的估计几乎可以肯定地收敛于未知的真实参数向量。特别地，通过选择适当的算法增益，该条件成立。

    We study the decentralized online regularized linear regression algorithm over random time-varying graphs. At each time step, every node runs an online estimation algorithm consisting of an innovation term processing its own new measurement, a consensus term taking a weighted sum of estimations of its own and its neighbors with additive and multiplicative communication noises and a regularization term preventing over-fitting. It is not required that the regression matrices and graphs satisfy special statistical assumptions such as mutual independence, spatio-temporal independence or stationarity. We develop the nonnegative supermartingale inequality of the estimation error, and prove that the estimations of all nodes converge to the unknown true parameter vector almost surely if the algorithm gains, graphs and regression matrices jointly satisfy the sample path spatio-temporal persistence of excitation condition. Especially, this condition holds by choosing appropriate algorithm gains 
    
[^226]: 联邦学习中的主体成员推理攻击

    Subject Membership Inference Attacks in Federated Learning. (arXiv:2206.03317v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03317](http://arxiv.org/abs/2206.03317)

    本文研究跨边界联邦学习中主体级别的隐私，并提出两种新颖的黑盒推理攻击方法。

    

    机器学习模型的隐私攻击通常着重于推论训练数据中特定数据点的存在。然而，攻击者真正想知道的是特定个体的（主体的）数据是否包含在训练中。在这种情况下，攻击者更可能拥有特定主体分布而非实际记录。本文研究跨边界联邦学习中主体级别的隐私，提出两种新颖的黑盒推理攻击方法，并进行评估。

    Privacy attacks on Machine Learning (ML) models often focus on inferring the existence of particular data points in the training data. However, what the adversary really wants to know is if a particular individual's (subject's) data was included during training. In such scenarios, the adversary is more likely to have access to the distribution of a particular subject than actual records. Furthermore, in settings like cross-silo Federated Learning (FL), a subject's data can be embodied by multiple data records that are spread across multiple organizations. Nearly all of the existing private FL literature is dedicated to studying privacy at two granularities -- item-level (individual data records), and user-level (participating user in the federation), neither of which apply to data subjects in cross-silo FL. This insight motivates us to shift our attention from the privacy of data records to the privacy of data subjects, also known as subject-level privacy. We propose two novel black-bo
    
[^227]: 快速非线性向量分位数回归

    Fast Nonlinear Vector Quantile Regression. (arXiv:2205.14977v3 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2205.14977](http://arxiv.org/abs/2205.14977)

    本论文提出了一种基于神经网络的快速非线性向量分位数回归方法，该方法保留了向量分位数回归的优雅的基于几何的公式，同时采用了几种创新的算法思想以及有效的训练和推断步骤，具有优越的预测性能。

    

    分位数回归是估计给定解释特征X的目标变量Y的一个或多个条件分位数的强大工具。QR的局限性在于，由于其目标函数的制定，仅对标量目标变量进行定义，由于量子的概念在多元分布中没有标准定义。最近，向量分位数回归(VQR)被提出作为QR的一种扩展，用于向量值目标变量，得益于通过最优传输对多元分布的分位数概念的有意义的概括。为了克服这些挑战，我们提出了快速非线性向量分位数回归(Fast-NVQR)，一种基于量位函数的神经网络参数化的新方法。Fast-NVQR保留了VQR的优雅的基于几何的公式，同时引入了几种创新的算法思想，以有效地训练神经网络和执行推断。我们提供了Fast-NVQR一致性和收敛速度的理论保证，并在合成和现实数据集上展示了其可伸缩性和优越的预测性能。

    Quantile regression (QR) is a powerful tool for estimating one or more conditional quantiles of a target variable $\mathrm{Y}$ given explanatory features $\boldsymbol{\mathrm{X}}$. A limitation of QR is that it is only defined for scalar target variables, due to the formulation of its objective function, and since the notion of quantiles has no standard definition for multivariate distributions. Recently, vector quantile regression (VQR) was proposed as an extension of QR for vector-valued target variables, thanks to a meaningful generalization of the notion of quantiles to multivariate distributions via optimal transport. Despite its elegance, VQR is arguably not applicable in practice due to several limitations: (i) it assumes a linear model for the quantiles of the target $\boldsymbol{\mathrm{Y}}$ given the features $\boldsymbol{\mathrm{X}}$; (ii) its exact formulation is intractable even for modestly-sized problems in terms of target dimensions, number of regressed quantile levels,
    
[^228]: 您的对比学习其实是在做随机邻居嵌入

    Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding. (arXiv:2205.14814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14814](http://arxiv.org/abs/2205.14814)

    本文揭示了自监督对比学习（SSCL）与随机邻居嵌入（SNE）的联系，SSCL实际上是SNE的一种特殊情况，通过SNE的视角，本文提供了关于领域无关增广、隐式偏差和学到特征的鲁棒性等问题的新分析和实用指南，该方法可以优化SSCL的特征提取。

    

    自监督对比学习（SSCL）在从非标记数据中提取有效特征方面取得了巨大成功。本文揭示了SSCL和随机邻居嵌入（SNE）之间的联系，即从保留邻近信息的角度来看，SSCL可以被视为SNE的一种特殊情况，其输入空间的成对相似性由数据增强指定。通过SNE的视角，对领域无关增广、隐式偏差和学到特征的鲁棒性等问题进行了新的分析，并提出了实用指南。实验表明，从SNE到t-SNE的修改对SSCL具有积极作用。

    Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to $t$-SNE can also be adopted in the 
    
[^229]: 利用泊松近似似然进行流行病分区模型中的一致且快速推理

    Consistent and fast inference in compartmental models of epidemics using Poisson Approximate Likelihoods. (arXiv:2205.13602v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2205.13602](http://arxiv.org/abs/2205.13602)

    引入泊松近似似然(PAL)方法，与ODE方法不同，PAL是从有限人口数量的随机分区模型的近似滤波方程导出的，并且大人口数量限制推动最大PAL估计量的一致性。

    

    为了解决流行病学推理向复杂和异质性模型的扩展难题，我们引入了泊松近似似然(PAL)方法。与采用大人口数量限制来激励确定性模型的ODE方法不同，PAL是从有限人口数量的随机分区模型的近似滤波方程导出的，并且大人口数量限制推动最大PAL估计量的一致性。我们的理论结果似乎是第一个适用于广泛类别的部分观察随机分区模型和解决大人口数量局限性的基于似然的参数估计一致性结果。PALs实现简单，仅涉及基本算术操作和没有调整参数，且评估速度快，不需要从模型进行模拟，计算成本与人口规模无关。通过示例，我们演示了如何使用PALs

    Addressing the challenge of scaling-up epidemiological inference to complex and heterogeneous models, we introduce Poisson Approximate Likelihood (PAL) methods. In contrast to the popular ODE approach to compartmental modelling, in which a large population limit is used to motivate a deterministic model, PALs are derived from approximate filtering equations for finite-population, stochastic compartmental models, and the large population limit drives consistency of maximum PAL estimators. Our theoretical results appear to be the first likelihood-based parameter estimation consistency results which apply to a broad class of partially observed stochastic compartmental models and address the large population limit. PALs are simple to implement, involving only elementary arithmetic operations and no tuning parameters, and fast to evaluate, requiring no simulation from the model and having computational cost independent of population size. Through examples we demonstrate how PALs can be used
    
[^230]: 基于离散时间线性系统的有限时间分析：时序差分学习

    Finite-Time Analysis of Temporal Difference Learning: Discrete-Time Linear System Perspective. (arXiv:2204.10479v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.10479](http://arxiv.org/abs/2204.10479)

    本文提出一种基于离散时间线性系统模型和Schur矩阵特性的有限时间分析方法，针对表格型时序差分学习，能够统一地覆盖基于策略和基于价值的两种情况。

    

    TD-learning是强化学习领域中一种基本的算法，它被用于通过估计马尔可夫决策过程的相应价值函数来评估给定策略。尽管TD-learning的理论分析取得了显著进展，但最近的研究通过开发有限时间误差界定来揭示了其统计效率的保证。本文旨在通过提出一种新的表格型时序差分(TD)学习的有限时间分析，直接有效地利用离散时间随机线性系统模型并利用Schur矩阵特性，为现有知识贡献新的方法。所提出的分析方法可以统一地覆盖基于策略和基于价值的两种情况。通过采用这种方法，我们希望能够提供新的、简单的模板，不仅可以进一步阐明TD-learning和相关RL算法的分析，而且为未来的研究提供有价值的见解。

    TD-learning is a fundamental algorithm in the field of reinforcement learning (RL), that is employed to evaluate a given policy by estimating the corresponding value function for a Markov decision process. While significant progress has been made in the theoretical analysis of TD-learning, recent research has uncovered guarantees concerning its statistical efficiency by developing finite-time error bounds. This paper aims to contribute to the existing body of knowledge by presenting a novel finite-time analysis of tabular temporal difference (TD) learning, which makes direct and effective use of discrete-time stochastic linear system models and leverages Schur matrix properties. The proposed analysis can cover both on-policy and off-policy settings in a unified manner. By adopting this approach, we hope to offer new and straightforward templates that not only shed further light on the analysis of TD-learning and related RL algorithms but also provide valuable insights for future resear
    
[^231]: 从人的物理反馈中学习：一种面向物体的单次自适应方法。

    Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method. (arXiv:2203.04951v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.04951](http://arxiv.org/abs/2203.04951)

    本文提出的Object Preference Adaptation (OPA)方法通过学习人类针对特定物体的反馈，在单次干预后进行机器人行为的自适应，提高了数据效率和适用性。

    

    为了让机器人能够在新环境和任务中有效地部署，它们必须能够理解人类在干预过程中表达的反馈。这可以纠正不良行为或指出其他偏好。现有的方法要么需要重复的交互回合，要么假定先前已知的奖励特征，这是数据效率低下并且很难转移到新任务的。我们通过在术语上将人类任务描述为以物体为中心的子任务，并将物理干预解释为与特定物体的关系来放松这些假设。我们的方法，Object Preference Adaptation (OPA)，由两个关键阶段组成：1）预训练基础策略以产生各种行为，以及2）根据人类反馈在线更新。我们快速而简单的自适应的关键在于固定了代理和物体之间的一般交互动态，只更新特定于物体的偏好。我们的自适应在线进行，只需要一个人类干预。

    For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human interve
    
[^232]: 情境在强化学习中的重要性--情境强化学习框架的案例分析

    Contextualize Me -- The Case for Context in Reinforcement Learning. (arXiv:2202.04500v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04500](http://arxiv.org/abs/2202.04500)

    情境强化学习提供了灵活、精确和可解释的任务规范和生成，可帮助改进RL中的零-shot泛化，需要上下文信息的洞察力。

    

    虽然强化学习（RL）在解决越来越复杂的问题方面取得了重大进展，但许多算法仍然对即使微小的环境变化也非常脆弱。情境强化学习（cRL）提供了一种框架，以原则性的方式建模这种变化，从而实现了灵活、精确和可解释的任务规范和生成。我们的目标是展示cRL框架如何通过有意义的基准和关于泛化任务的结构化推理，为改进RL中的零-shot泛化贡献。我们确认在cRL中最优行为需要上下文信息的洞察力，就像其他相关的部分可观察性领域一样。为了在cRL框架中从实证上验证这一点，我们提供了常见RL环境的多种情境扩展版本。它们是首个基准库CARL的一部分，该库旨在基于cRL扩展的普遍基准进行泛化研究。

    While Reinforcement Learning ( RL) has made great strides towards solving increasingly complicated problems, many algorithms are still brittle to even slight environmental changes. Contextual Reinforcement Learning (cRL) provides a framework to model such changes in a principled manner, thereby enabling flexible, precise and interpretable task specification and generation. Our goal is to show how the framework of cRL contributes to improving zero-shot generalization in RL through meaningful benchmarks and structured reasoning about generalization tasks. We confirm the insight that optimal behavior in cRL requires context information, as in other related areas of partial observability. To empirically validate this in the cRL framework, we provide various context-extended versions of common RL environments. They are part of the first benchmark library, CARL, designed for generalization based on cRL extensions of popular benchmarks, which we propose as a testbed to further study general a
    
[^233]: De Rham兼容的深度神经网络有限元方法

    De Rham compatible Deep Neural Network FEM. (arXiv:2201.05395v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2201.05395](http://arxiv.org/abs/2201.05395)

    本篇论文提出了在普通正则单形分割上构建精确神经网络有限元的方法，能够适用于分段常数函数空间、连续分段线性函数空间、经典的Raviart-Thomas元和Nédélec边缘元，且能够捕捉不连续性，其中对于CPwL函数的情况，只需使用纯ReLU网络即可。

    

    在有界多面体区域$\Omega\subset \mathbb{R}^d$ $(d\in\{2,3\})$的一般正则单形分割$\mathcal{T}$上，我们构建了所有离散de Rham复形中最低阶有限元空间的“精确神经网络（NN）仿真”。 这些包括分段常数函数空间，连续分段线性（CPwL）函数空间，经典的“Raviart-Thomas元”和“Nédélec边缘元”。 对于除CPwL元之外的所有元素，我们的网络架构采用ReLU（修正线性单元）和BiSU（二元阶跃单元）激活来捕捉不连续性。 在CPwL函数的重要情况下，我们证明只需要使用纯ReLU网络即可。 我们的构造和DNN架构在没有对$\Omega$的正则单形分割$\mathcal{T}$的几何限制的情况下推广了以前的结果。 另外，对于CPwL函数，我们的DNN构造在任何维度$d\geq 2$中都是有效的。

    On general regular simplicial partitions $\mathcal{T}$ of bounded polytopal domains $\Omega \subset \mathbb{R}^d$, $d\in\{2,3\}$, we construct \emph{exact neural network (NN) emulations} of all lowest order finite element spaces in the discrete de Rham complex. These include the spaces of piecewise constant functions, continuous piecewise linear (CPwL) functions, the classical ``Raviart-Thomas element'', and the ``N\'{e}d\'{e}lec edge element''. For all but the CPwL case, our network architectures employ both ReLU (rectified linear unit) and BiSU (binary step unit) activations to capture discontinuities. In the important case of CPwL functions, we prove that it suffices to work with pure ReLU nets. Our construction and DNN architecture generalizes previous results in that no geometric restrictions on the regular simplicial partitions $\mathcal{T}$ of $\Omega$ are required for DNN emulation. In addition, for CPwL functions our DNN construction is valid in any dimension $d\geq 2$. Our ``
    
[^234]: 在错误预测上使用最大熵(MEEP)：提高医学图像分割模型的校准性。

    Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation. (arXiv:2112.12218v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.12218](http://arxiv.org/abs/2112.12218)

    本文提出了在最大熵和错误预测上使用的训练策略（MEEP），通过惩罚自信度过高的预测来提高医学图像分割模型的校准性和分割准确性。

    

    现代深度神经网络在医学图像分割任务上取得了显著进展。然而，最近观察到，它们倾向于产生过度自信的估计结果，甚至在高不确定性情况下也是如此，导致模型校准不良、不可靠。在这项工作中，我们介绍了一种基于最大熵的错误预测（MEEP）训练策略，用于分割网络，它有选择地对自信度过高的预测进行惩罚，仅关注错误分类的像素。我们的方法对神经结构不加偏见，不增加模型复杂度，可与多个分割损失函数配对使用。我们在两个具有挑战性的分割任务中进行了基准测试：脑磁共振图像中的白质高信号病变和心脏磁共振图像中的心房分割。实验结果表明，将MEEP与标准分割损失相结合，不仅在模型校准方面，而且在分割准确性方面，相比于基线模型都会带来改进。

    Modern deep neural networks achieved remarkable progress in medical image segmentation tasks. However, it has recently been observed that they tend to produce overconfident estimates, even in situations of high uncertainty, leading to poorly calibrated and unreliable models. In this work we introduce Maximum Entropy on Erroneous Predictions (MEEP), a training strategy for segmentation networks which selectively penalizes overconfident predictions, focusing only on misclassified pixels. Our method is agnostic to the neural architecture, does not increase model complexity and can be coupled with multiple segmentation loss functions. We benchmark the proposed strategy in two challenging segmentation tasks: white matter hyperintensity lesions in magnetic resonance images (MRI) of the brain, and atrial segmentation in cardiac MRI. The experimental results demonstrate that coupling MEEP with standard segmentation losses leads to improvements not only in terms of model calibration, but also i
    
[^235]: 基于情感和句子类型对YouTube评论进行分类

    Classifying YouTube Comments Based on Sentiment and Type of Sentence. (arXiv:2111.01908v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2111.01908](http://arxiv.org/abs/2111.01908)

    本论文提出了一种基于情感和句子类型的方法，将原始的YouTube评论分类，以帮助YouTuber找到更相关的评论，从而增加其观众群。

    

    随着YouTube频道的增长，每个视频都可能收集大量评论，这些评论是理解观众期望和改善频道参与度的主要手段。然而，这些评论只代表用户关于频道和内容的一般观点。许多评论构造较差，琐碎并且存在拼写和语法错误，因此，识别最能吸引内容创作者的评论是一项繁琐的工作。本文旨在通过情感和句子类型将原始评论分类，以帮助YouTuber找到更相关的评论，从而增加其观众群。

    As a YouTube channel grows, each video can potentially collect enormous amounts of comments that provide direct feedback from the viewers. These comments are a major means of understanding viewer expectations and improving channel engagement. However, the comments only represent a general collection of user opinions about the channel and the content. Many comments are poorly constructed, trivial, and have improper spellings and grammatical errors. As a result, it is a tedious job to identify the comments that best interest the content creators. In this paper, we extract and classify the raw comments into different categories based on both sentiment and sentence types that will help YouTubers find relevant comments for growing their viewership. Existing studies have focused either on sentiment analysis (positive and negative) or classification of sub-types within the same sentence types (e.g., types of questions) on a text corpus. These have limited application on non-traditional text c
    
[^236]: 实现可行的在线 3D 无序装箱策略的学习

    Learning Practically Feasible Policies for Online 3D Bin Packing. (arXiv:2108.13680v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2108.13680](http://arxiv.org/abs/2108.13680)

    本文针对在线 3D 无序装箱问题采用深度强化学习，提出了一种基于新的叠放树的装箱稳定性在线分析方法，并提出了一种不同维度分离的装箱策略学习方法

    

    本文解决了在线 3D 无序装箱问题，这是经典装箱问题的一个具有挑战性但实用价值的变种。在这个问题中，物品被递送到代理人处，但不提供完整的序列信息。代理人必须将这些物品直接稳定地装入目标装箱中，而不能改变它们的到达顺序，并且不允许进一步调整。在线 3D 无序装箱问题可以自然地被看作是一个马尔科夫决策过程（MDP）。我们采用深度强化学习，尤其是基于策略的演员-评论家框架，用受限动作空间来解决这个MDP 。为了学习一个实用的可行的装箱策略，我们提出了三个关键的设计。首先，我们提出了一种基于新的叠放树的装箱稳定性在线分析方法。它实现了高度的分析准确性，同时将计算复杂度从 O(N^2) 降低到 O(NlogN)，非常适合于 RL 训练。其次，我们提出了一种不同维度分离的装箱策略学习方法

    We tackle the Online 3D Bin Packing Problem, a challenging yet practically useful variant of the classical Bin Packing Problem. In this problem, the items are delivered to the agent without informing the full sequence information. Agent must directly pack these items into the target bin stably without changing their arrival order, and no further adjustment is permitted. Online 3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt deep reinforcement learning, in particular, the on-policy actor-critic framework, to solve this MDP with constrained action space. To learn a practically feasible packing policy, we propose three critical designs. First, we propose an online analysis of packing stability based on a novel stacking tree. It attains a high analysis accuracy while reducing the computational complexity from $O(N^2)$ to $O(N \log N)$, making it especially suited for RL training. Second, we propose a decoupled packing policy learning for different dimensions o
    
[^237]: 一种应用XAI方法检测DCIS的深度学习模型

    An XAI Approach to Deep Learning Models in the Detection of DCIS. (arXiv:2106.14186v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2106.14186](http://arxiv.org/abs/2106.14186)

    该研究证明了XAI可用于证明辅助人工智能系统的可行性，有效地应用于医疗领域。

    

    结果表明，XAI确实可以作为概念验证的方法，对临床社区内辅助人工智能系统的实施进行讨论。

    The results showed that XAI could indeed be used as a proof of concept to begin discussions on the implementation of assistive AI systems within the clinical community.
    
[^238]: 奖励足以处理凸性MDP问题

    Reward is enough for convex MDPs. (arXiv:2106.00661v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2106.00661](http://arxiv.org/abs/2106.00661)

    本文研究了凸性马尔可夫决策过程，发现无法使用静态奖励函数表达目标，提出了一个元算法解决此问题，并统一了文献中的现有算法。

    

    在马尔可夫决策过程 (MDP) 中，最大化一个马尔可夫和平稳的累积奖励函数可以捕捉到许多目标。然而，并非所有目标都能以此方式捕获。本文研究了凸性MDPs，其中目标是作为静态分布的凸函数表达的，结果表明无法使用静态奖励函数来表达目标。我们将凸性MDP问题重新表述为政策和代价(负奖励)“玩家”的最小最大博弈，利用 Fenchel 对偶性，提出了一个解决此问题的元算法，并证明了它统一了文献中许多现有算法。

    Maximising a cumulative reward function that is Markov and stationary, i.e., defined over state-action pairs and independent of time, is sufficient to capture many kinds of goals in a Markov decision process (MDP). However, not all goals can be captured in this manner. In this paper we study convex MDPs in which goals are expressed as convex functions of the stationary distribution and show that they cannot be formulated using stationary reward functions. Convex MDPs generalize the standard reinforcement learning (RL) problem formulation to a larger framework that includes many supervised and unsupervised RL problems, such as apprenticeship learning, constrained MDPs, and so-called `pure exploration'. Our approach is to reformulate the convex MDP problem as a min-max game involving policy and cost (negative reward) `players', using Fenchel duality. We propose a meta-algorithm for solving this problem and show that it unifies many existing algorithms in the literature.
    
[^239]: 生成式演员-评论家算法: 使用推进模型的离线算法

    Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model. (arXiv:2105.03733v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.03733](http://arxiv.org/abs/2105.03733)

    本文提出基于推进模型的无密度离线算法 GAC，使用最大均值差熵正则化器平衡探索和开发之间的关系，并使用自适应机制提高算法的稳定性和鲁棒性。实验结果表明，推进策略能够提高算法的探索效率和渐近性能。

    

    无模型深度强化学习在视频游戏、推荐系统和机器人控制任务等许多领域取得了巨大成功。在连续控制任务中，广泛使用的具有高斯分布的策略导致环境的探索低效，并且在许多情况下算法的性能受到限制。本文提出了一种无密度的离线算法，称为生成式演员-评论家（Generative Actor-Critic，GAC），使用推进模型来增加策略的表达能力。同时还包括一种熵类技术，称为最大均值差（Maximum Mean Discrepancy，MMD）熵正则化器，以平衡探索和开发之间的关系。此外，我们设计了一种自适应机制来自动缩放这个正则化器，进一步提高了GAC的稳定性和鲁棒性。实验结果表明，推进策略具有理想的特征，例如多模式性，可以明显提高算法的探索效率和渐近性能。

    Model-free deep reinforcement learning has achieved great success in many domains, such as video games, recommendation systems and robotic control tasks. In continuous control tasks, widely used policies with Gaussian distributions results in ineffective exploration of environments and limited performance of algorithms in many cases. In this paper, we propose a density-free off-policy algorithm, Generative Actor-Critic(GAC), using the push-forward model to increase the expressiveness of policies, which also includes an entropy-like technique, MMD-entropy regularizer, to balance the exploration and exploitation. Additionnally, we devise an adaptive mechanism to automatically scale this regularizer, which further improves the stability and robustness of GAC. The experiment results show that push-forward policies possess desirable features, such as multi-modality, which can improve the efficiency of exploration and asymptotic performance of algorithms obviously.
    
[^240]: Transformer层间参数共享的教训（arXiv：2104.06022v4 [cs.CL]更新）

    Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.06022](http://arxiv.org/abs/2104.06022)

    该论文提出了一种放宽常用参数共享技术的Transformer参数共享方法，其可以提高计算时间的效率，通过三种策略来分配每个层的参数，在实验中表现出高效的参数大小和计算时间，并在使用大量训练数据的配置中同样有效。

    

    我们提出了一种Transformer参数共享方法（Vaswani等人，2017）。所提出的方法放宽了一种被广泛使用的技术，即将一个层的参数与所有层共享，如通用Transformer（Dehghani等人，2019），以增加计算时间的效率。我们提出了三种策略：序列、循环和循环（反向）来分配每个层的参数。实验结果表明，所提出的策略在参数大小和计算时间上是有效的。此外，我们还表明了所提出的策略在使用许多训练数据的配置中也是有效的，例如最近的WMT比赛。

    We propose a parameter sharing method for Transformers (Vaswani et al., 2017). The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers (Dehghani et al., 2019), to increase the efficiency in the computational time. We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in the parameter size and computational time. Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.
    
[^241]: QCBA: 通过恢复离散化过程中的信息改进基于数值数据学习的规则分类器

    QCBA: Improving Rule Classifiers Learned from Quantitative Data by Recovering Information Lost by Discretisation. (arXiv:1711.10166v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1711.10166](http://arxiv.org/abs/1711.10166)

    本文提出的QCBA方法可以用于改进基于数值类型数据学习的规则分类器，恢复预离散化过程中丢失的信息，并提出了新的剪枝技术。在22个数据集上的实验表明，FOIL2+QCBA相对其他基线方法而言，具有更高的预测性能和更小的模型大小。

    

    数值属性的预离散化是某些规则学习算法的必要步骤，但是会导致一些效率低下的问题。本文提出了新的规则调整步骤以恢复离散化中丢失的信息，并提出了新的剪枝技术，可以进一步减小规则模型的大小和提高其准确性。提出的QCBA方法最初是为了对基于关联性分类（CBA）算法生成的模型中的定量属性进行后处理，但也可应用于其他规则学习方法的结果。我们展示了对五个关联规则分类算法（CBA、CMAR、CPAR、IDS、SBRL）和两个一阶逻辑规则学习器（FOIL2和PRM）生成模型的后处理效果，使用UCI仓库的22个数据集进行了基准测试，结果表明FOIL2+QCBA相比于所有七个基线的大小更小，并且具有最佳的整体预测性能。后优化的CBA模型具有更好的预测能力。

    A prediscretisation of numerical attributes which is required by some rule learning algorithms is a source of inefficiencies. This paper describes new rule tuning steps that aim to recover lost information in the discretisation and new pruning techniques that may further reduce the size of rule models and improve their accuracy. The proposed QCBA method was initially developed to postprocess quantitative attributes in models generated by the Classification based on associations (CBA) algorithm, but it can also be applied to the results of other rule learning approaches. We demonstrate the effectiveness on the postprocessing of models generated by five association rule classification algorithms (CBA, CMAR, CPAR, IDS, SBRL) and two first-order logic rule learners (FOIL2 and PRM). Benchmarks on 22 datasets from the UCI repository show smaller size and the overall best predictive performance for FOIL2+QCBA compared to all seven baselines. Postoptimised CBA models have a better predictive p
    

