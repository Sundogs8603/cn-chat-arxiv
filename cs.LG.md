# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multidimensional Uncertainty Quantification for Deep Neural Networks.](http://arxiv.org/abs/2304.10527) | 研究如何量化DNN的不同不确定性，并将其用于更有效地解决不同的决策问题。 |
| [^2] | [Learning Narrow One-Hidden-Layer ReLU Networks.](http://arxiv.org/abs/2304.10524) | 本文提出了一个在多项式时间内成功的学习Narrow One-Hidden-Layer ReLU网络的算法，而不需要额外的假设，并使用了分析高阶矩张量的随机收缩的方法，使得可以发现单个神经元。 |
| [^3] | [Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget.](http://arxiv.org/abs/2304.10520) | 本文提出了一种新方法：掩码自编码器对比调整(MAE-CT)，利用最近邻对比学习（NNCLR）在标记数据较少的情况下实现下游分类，该方法可以将图像的丰富特征聚类成对象的语义聚类。 |
| [^4] | [Segment Anything Model for Medical Image Analysis: an Experimental Study.](http://arxiv.org/abs/2304.10517) | 本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。 |
| [^5] | [CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network.](http://arxiv.org/abs/2304.10515) | 本文提出一种基于人脑核心-周边原则的类脑设计原则以指导CNN的设计。通过在网络布线模式和卷积操作中实现核心-周边原则，我们提出了一种全新的CP-CNN架构，它在各种视觉任务中表现出色。 |
| [^6] | ["Can We Detect Substance Use Disorder?": Knowledge and Time Aware Classification on Social Media from Darkweb.](http://arxiv.org/abs/2304.10512) | 本研究使用知识感知BERT模型，分析社交媒体上发布的与合成阿片类物质相关的帖子，以了解用户对不同合成阿片类物质的情感和情绪，从而帮助预测物质使用障碍。 |
| [^7] | [OutCenTR: A novel semi-supervised framework for predicting exploits of vulnerabilities in high-dimensional datasets.](http://arxiv.org/abs/2304.10511) | 本文提出了一种名为OutCenTR的半监督框架，用于预测高维度数据集中可能被利用的漏洞，同时提出一种降维技术OutCenTR，可以增强基本的离群点检测模型，实验证明了OutCenTR的有效性和效率，平均F1分数提高了5倍。 |
| [^8] | [Censoring chemical data to mitigate dual use risk.](http://arxiv.org/abs/2304.10510) | 为了缓解化学数据被用于开发识别新型毒素或化学战剂的预测模型的双重用途风险，提出了一种基于模型不可知的方法，可在保留有利于训练深度神经网络的数据的效用区域的同时，对敏感信息的数据集进行有选择性的添加噪音，该方法被证明是有效的，并且忽略敏感数据会增加模型的方差。 |
| [^9] | [Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts.](http://arxiv.org/abs/2304.10505) | 本文提出了一种视频预训练Transformer，它使用多个最先进的编码器模型将视频转换成通用表示，可以结合多种模式获得最佳性能。 |
| [^10] | [Study of Robust Adaptive Beamforming with Covariance Matrix Reconstruction Based on Power Spectral Estimation and Uncertainty Region.](http://arxiv.org/abs/2304.10502) | 本文提出了一种基于功率谱估计和不确定区域的简单且有效的鲁棒自适应波束成形技术，可以更好地处理干扰加噪声成分，同时具有更好的鲁棒性和自适应性能。 |
| [^11] | [Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code.](http://arxiv.org/abs/2304.10500) | 本文探索使用 Transformer 模型进行形式语言的推理。通过使用编程语言最基本的特性——术语和类型之间的关系，提出了一种基准测试，并提出了一种基于 Transformer 的类型推理模型。实验结果表明，TTI 模型在推理准确性和速度方面都优于现有的最先进方法。 |
| [^12] | [Infinite Physical Monkey: Do Deep Learning Methods Really Perform Better in Conformation Generation?.](http://arxiv.org/abs/2304.10494) | 深度学习方法在构象生成中的表现受到质疑，本文研究了基于物理的方法在这个问题中的表现，发现仍是一个有力选项。 |
| [^13] | [CoProver: A Recommender System for Proof Construction.](http://arxiv.org/abs/2304.10486) | CoProver是一种证明构造推荐系统，能够从证明构造过程中的过去操作中学习，并通过探索存储在ITP中的关于以前证明的知识来提供有用的建议。 |
| [^14] | [Efficient Deep Reinforcement Learning Requires Regulating Overfitting.](http://arxiv.org/abs/2304.10466) | 深度强化学习的主要瓶颈在于高时间差误差的验证集上出现了严重过拟合问题。 |
| [^15] | [Angle based dynamic learning rate for gradient descent.](http://arxiv.org/abs/2304.10457) | 该论文提出了一种基于角度计算的动态学习率方式，用于梯度下降方法中的自适应学习率选择，相较于传统方法，该方法在各个基准数据集上的可以获得更高的准确性，并被证明是可以收敛的。 |
| [^16] | [Certified Adversarial Robustness Within Multiple Perturbation Bounds.](http://arxiv.org/abs/2304.10446) | 本论文提出了一种新的认证方法，可以在多个扰动边界下获得最佳结果，同时针对RS方法进行了优化，实现了最先进的认证性鲁棒性。 |
| [^17] | [Gauge-equivariant pooling layers for preconditioners in lattice QCD.](http://arxiv.org/abs/2304.10438) | 本论文展示了在格点量子色动力学中使用规范等变池化层可以通过伽辽金构造粗网格规范场消除临界减慢现象。 |
| [^18] | [Attention Scheme Inspired Softmax Regression.](http://arxiv.org/abs/2304.10411) | 本研究从 softmax 单元中获得灵感，提出了注意机制 inspired 的 softmax 回归问题，该问题可用于控制潜在函数的进展和稳定性。 |
| [^19] | [Multi-label Node Classification On Graph-Structured Data.](http://arxiv.org/abs/2304.10398) | 该论文提出了一种新的图神经网络架构 MLGCN，用于处理多标签节点分类任务，并研究了多标签场景中同类偏好的语义。在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。 |
| [^20] | [Conditional Generative Models for Learning Stochastic Processes.](http://arxiv.org/abs/2304.10382) | 提出了一种称为 C-qGAN 的框架，利用量子电路结构实现了有效的状态准备过程，可以利用该方法加速蒙特卡罗分析等算法，并将其应用于亚式期权衍生品定价的任务中。 |
| [^21] | [Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.10375) | 提出了一种基于条件注意力的分布式注意力演员架构，利用显著性向量重用环境的条件状态来提高条件协同行为的可解释性和代理性能。 |
| [^22] | [Controllable Neural Symbolic Regression.](http://arxiv.org/abs/2304.10336) | 本文提出了一种称为带假设的神经符号回归（NSRwH）的方法，能够将用户定义的先验知识纳入到生成分析表达式的预测模型中，我们的方法允许用户指定目标表达式中使用的数学符号类型和范围，并对表达式的复杂性施加约束。 |
| [^23] | [Learning Cellular Coverage from Real Network Configurations using GNNs.](http://arxiv.org/abs/2304.10328) | 本论文提出了一种新颖的训练框架，在图形表示中表述学习蜂窝网络覆盖的任务，应用了最新的图神经网络。该框架既可以生成多个KPI的质量单元格配置嵌入，同时能够推广到大型（面向区域）场景，给出非常少的标记单元格。 |
| [^24] | [Adaptive Consensus Optimization Method for GANs.](http://arxiv.org/abs/2304.10317) | 本文提出一种基于ADAM和RMSprop的二阶梯度优化方法，用于训练GAN，在不需要解决线性系统或混合二阶导数项的情况下取得与其他方法相比更快的相似精度，产生了更好或可比的Inception分数和图像质量，并与一阶方法相比产生了显著更好的Inception分数。 |
| [^25] | [Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining.](http://arxiv.org/abs/2304.10311) | 本研究采用自监督和视觉相关的预训练方法对电影票房进行预测，对具有内容关键词的电影表现出了显著的性能提高。 |
| [^26] | [Optimal Activation of Halting Multi-Armed Bandit Models.](http://arxiv.org/abs/2304.10302) | 本文研究了新型的动态分配问题——停止赌博机模型，提出了对于经典的Gittins指数分解结果和最新结果的新证明。 |
| [^27] | [SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning.](http://arxiv.org/abs/2304.10297) | 本文提出了一个新的自监督学习模型SARF，通过利用同义关系辅助提高了少样本关系推理的泛化性能和准确性，经过实验验证超越了目前最先进的方法。 |
| [^28] | [OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer Thin Film Structures.](http://arxiv.org/abs/2304.10294) | OptoGPT是一种基于大型数据集训练的仅包含解码器的Transformer模型，可以自主全局设计探索，同时选择材料和厚度，用于光学多层薄膜结构反向设计。 |
| [^29] | [Aiding reinforcement learning for set point control.](http://arxiv.org/abs/2304.10289) | 本文提出了一种方法，将强化学习与简单的引导反馈控制器相结合，以实现设定点控制问题的更好激励，以提高强化学习控制器的收敛性能，克服了强化学习在实际控制环境中的不足。 |
| [^30] | [A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty.](http://arxiv.org/abs/2304.10284) | 本文提出了一种元启发式方法，它可以以人类和机器学习决策都互相关联的因素来表征一个实例的复杂性，以估计分类错误的风险。 |
| [^31] | [Robust nonlinear set-point control with reinforcement learning.](http://arxiv.org/abs/2304.10277) | 本文提出了三种思想来改进强化学习方法，即利用先前反馈控制器来帮助振幅探索，使用积分误差，对模型集合进行训练。这些想法可提高训练效率，使得训练出来的设定点控制器更加鲁棒，可以直接用于真实的非线性系统中。 |
| [^32] | [Observer-Feedback-Feedforward Controller Structures in Reinforcement Learning.](http://arxiv.org/abs/2304.10276) | 本文提出了使用结构化神经网络进行基于强化学习的非线性自适应控制，将观察器和控制器分为不同的神经网络，通过降低计算复杂度和使控制器具有可理解的结构，显著加快了训练速度。 |
| [^33] | [Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation.](http://arxiv.org/abs/2304.10260) | 该论文提出了一种使用循环一致性生成对抗方法进行领域自适应轨迹模仿的深度强化学习代理DATI，可以学习动态系统中的代表性轨迹，识别交通中异常运动模式，并在各种合成的参考轨迹家族上表现优异。 |
| [^34] | [Indian Sign Language Recognition Using Mediapipe Holistic.](http://arxiv.org/abs/2304.10256) | 本研究致力于创建一个强大的手语识别系统，将印度手语转换为文本或语音。CNN模型对于静态手语的识别表现较好，但是通过监控动态手势，LSTM模型的表现更好。 |
| [^35] | [PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces.](http://arxiv.org/abs/2304.10255) | PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。 |
| [^36] | [A data augmentation perspective on diffusion models and retrieval.](http://arxiv.org/abs/2304.10253) | 本研究从数据增强的角度出发，评估了扩散模型图像生成方法，并发现仅使用扩散模型的训练数据可用于最强的数据增强。 |
| [^37] | [Towards replacing precipitation ensemble predictions systems using machine learning.](http://arxiv.org/abs/2304.10251) | 该论文提出了一种用生成对抗网络生成高分辨率降水集合预测的方法，避免了高分辨率训练数据的需求和计算成本的限制。 |
| [^38] | [Harnessing the Power of Text-image Contrastive Models for Automatic Detection of Online Misinformation.](http://arxiv.org/abs/2304.10249) | 本文研究了一种基于对比学习的虚假信息识别方法，在文本-图像对中区分正宗和虚假新闻，具有很好的效果。 |
| [^39] | [Hotelling Deflation on Large Symmetric Spiked Tensors.](http://arxiv.org/abs/2304.10248) | 本文研究了缩减算法在大规模对称尖峰张量上的应用，提供了在存在非平凡相关性情况下的精确表现，可用于设计更有效的信号估计方法。 |
| [^40] | [Filter-Aware Model-Predictive Control.](http://arxiv.org/abs/2304.10246) | 本文提出了一种新的模型预测控制方法，滤波感知MPC，基于状态估计器的追踪度惩罚信息损失，可以快速规划并在多方面实验中展示了其有效性。 |
| [^41] | [Fourier Neural Operator Surrogate Model to Predict 3D Seismic Waves Propagation.](http://arxiv.org/abs/2304.10242) | 本文提出一种基于傅立叶神经算子的代理模型，可用于预测3D地震波传播的地面运动时间序列。该模型在预测精度和计算效率方面明显优于传统机器学习模型。 |
| [^42] | [Domain Generalization for Mammographic Image Analysis via Contrastive Learning.](http://arxiv.org/abs/2304.10226) | 研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。 |
| [^43] | [The impact of the AI revolution on asset management.](http://arxiv.org/abs/2304.10212) | 本论文分享了作者关于人工智能可能对资产管理产生影响的观点，同时提供了一个衡量特定基金是否真正开发了AI的简单标准。 |
| [^44] | [SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns.](http://arxiv.org/abs/2304.10207) | 本研究利用S参数模式成功实现对Cu互连缺陷的非破坏性检测和诊断，在同时分析根本原因和严重性方面具有先进性，具备早期检测、高诊断准确度和噪声鲁棒性等优点。 |
| [^45] | [Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout.](http://arxiv.org/abs/2304.10191) | 本文提出了一种利用时间步机制实现MC-dropout的高效脉冲神经网络不确定性估计方法，为神经形态学硬件的节能应用提供了新的研究思路。 |
| [^46] | [Cyber Security in Smart Manufacturing (Threats, Landscapes Challenges).](http://arxiv.org/abs/2304.10180) | 智能制造是工业4.0的一部分，将生产设备和网络智能结合，但由于现有操作流程的漏洞，使得智能制造成为网络威胁的主要目标，网络安全至关重要。 |
| [^47] | [Regularizing Second-Order Influences for Continual Learning.](http://arxiv.org/abs/2304.10177) | 本文提出了一种新的选择目标来规范连续学习中的二阶影响，并提供了一种有效的实现方法来优化所提出的标准。 |
| [^48] | [Robust Deep Reinforcement Learning Scheduling via Weight Anchoring.](http://arxiv.org/abs/2304.10176) | 本研究提出了一种使用权重锚定来实现深度强化学习调度的方法，可避免在优化环境下忽略或忘记期望行为，提高了鲁棒性和可操纵性。 |
| [^49] | [Deep Reinforcement Learning Using Hybrid Quantum Neural Network.](http://arxiv.org/abs/2304.10159) | 该研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并评估了其潜力。最终总结了开发深度量子学习的前景和结论。 |
| [^50] | [Flexible K Nearest Neighbors Classifier: Derivation and Application for Ion-mobility Spectrometry-based Indoor Localization.](http://arxiv.org/abs/2304.10151) | 本文提出了一种新的K最近邻分类器变体，可以确保最近邻居确实接近未标记样本，并在过程中找到K值。与标准KNN相比，该算法在室内指纹定位方面具有更高的分类精度。 |
| [^51] | [Learning Sample Difficulty from Pre-trained Models for Reliable Prediction.](http://arxiv.org/abs/2304.10127) | 该论文介绍了如何使用预训练模型通过熵正则化来计算训练样本的难度，并根据样本难度惩罚过于自信的预测，从而提高模型的准确性和不确定性校准。 |
| [^52] | [Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One.](http://arxiv.org/abs/2304.10126) | 本论文提出将多层图神经网络解耦为多个简单模块的方法，以实现更高效的训练。该方法包括经典的前向训练和设计的反向训练。每个模块都可以通过随机算法在前向训练中高效地训练，并且通过反向训练机制来使前面的模块能够感知后面的模块，从而充分训练浅层模块和更深层的模块。 |
| [^53] | [Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning.](http://arxiv.org/abs/2304.10124) | 该论文提出了一种非对称进化训练（AET）框架，使用自适应数据调整（ADA）和环境随机化（ER）优化AET过程，使得AI可以在复杂的AMP游戏中击败顶级人类玩家，而不需要使用任何人类数据。 |
| [^54] | [Automatic Procurement Fraud Detection with Machine Learning.](http://arxiv.org/abs/2304.10105) | 该论文讲述了如何使用机器学习技术检测采购欺诈，针对每个采购事件使用9个特定特征构建了神经网络模型，并通过测试证明这些模型在检测采购欺诈方面是有用的。 |
| [^55] | [Federated Compositional Deep AUC Maximization.](http://arxiv.org/abs/2304.10101) | 本论文介绍了一种通过直接优化AUC分数来解决联邦学习中不平衡数据问题的方法，并开发了一个随机组合梯度下降上升动量算法。通过广泛的实验验证，证实了该方法的效率和有效性。 |
| [^56] | [Two-Memory Reinforcement Learning.](http://arxiv.org/abs/2304.10098) | 本文提出了双记忆强化学习代理 (2M)，它结合了情节记忆和强化学习的优点来提高学习速度和准确性。 |
| [^57] | [Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States.](http://arxiv.org/abs/2304.10079) | 本文提出了循环差分图变换器框架，旨在解决动态图表示学习中未能明确建模边时序状态和提取全局结构特征的问题。 |
| [^58] | [Improving Graph Neural Networks on Multi-node Tasks with Labeling Tricks.](http://arxiv.org/abs/2304.10074) | 本文提出了一种标签技巧，用于在多节点任务上提高图神经网络的表示学习能力。该方法通过区分目标节点和其他节点的方式改进了以往直接聚合各节点表示的缺陷，能够更好地捕捉节点间的依赖关系。 |
| [^59] | [Fruit Picker Activity Recognition with Wearable Sensors and Machine Learning.](http://arxiv.org/abs/2304.10068) | 本文介绍了一种基于可穿戴传感器和机器学习的水果采摘活动识别方法，成功实现了高精度识别袋倒空事件，证明了该方法在农作物管理应用中的可行性。 |
| [^60] | [Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size.](http://arxiv.org/abs/2304.10061) | 本研究开发出Allegro架构，通过创新的模型架构和GPU并行化实现了对极大分子的高精度模拟，能够对空前复杂的结构进行量子保真动力学描述，并成功进行了对4400万原子的HIV外壳的模拟。 |
| [^61] | [Optimality of Robust Online Learning.](http://arxiv.org/abs/2304.10060) | 本文提出了一种基于鲁棒损失函数 $\mathcal{L}_{\sigma}$ 的在线学习算法，可用作在线最小二乘回归的鲁棒替代方案。并证明了在适当选择参数的情况下，该算法具有无容量依赖的最优性收敛性以及强收敛的最优容量依赖速率。 |
| [^62] | [HyperTuner: A Cross-Layer Multi-Objective Hyperparameter Auto-Tuning Framework for Data Analytic Services.](http://arxiv.org/abs/2304.10051) | 本论文提出了HyperTuner，它是一个跨层多目标超参数自动调整框架，可同时考虑模型超参数和系统参数。HyperTuner使用MOPIR算法进行多目标参数重要性排序，然后利用ADUMBO算法找到Pareto最优配置集合，帮助数据分析服务提供商解决高维黑盒多目标优化问题。 |
| [^63] | [Optimal Kernel for Kernel-Based Modal Statistical Methods.](http://arxiv.org/abs/2304.10046) | 本文研究基于核函数的模态统计方法的最优核函数的选择问题，提出了一种（多元）最优核函数，在某个核函数类别中使得其解析得到的渐近误差准则最小化。 |
| [^64] | [ID-MixGCL: Identity Mixup for Graph Contrastive Learning.](http://arxiv.org/abs/2304.10045) | 本文提出了一种基于身份混合的图形对比学习方法，旨在解决通过图形增强得到的不同但相似的图形结构和标签的不匹配问题，以实现更好的表示捕获。 |
| [^65] | [Brain tumor multi classification and segmentation in MRO images using deep learning.](http://arxiv.org/abs/2304.10039) | 本研究提出了基于深度学习的脑肿瘤分类和分割模型，能够准确地将MRI图像中的肿瘤分为四类，并将肿瘤从图像中精确地分割出来，具有潜在的临床应用价值。 |
| [^66] | [Open-World Continual Learning: Unifying Novelty Detection and Continual Learning.](http://arxiv.org/abs/2304.10038) | 本文从理论上证明，分布外检测对于类别增量学习是必要的，因为类别增量学习可以分解成任务内预测和任务 ID 预测，并且任务 ID 预测与分布外检测相关。 |
| [^67] | [Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.](http://arxiv.org/abs/2304.10031) | 拓扑深度学习框架提供了从复杂系统相关数据中提取知识的全面架构，通过解决现有工作的符号和术语不一致问题，有望在应用科学和其他领域开拓新局面。 |
| [^68] | [Jedi: Entropy-based Localization and Removal of Adversarial Patches.](http://arxiv.org/abs/2304.10029) | 本文提出了一种新的对抗性补丁防御方法——Jedi，该方法从信息论的角度解决了补丁定位问题，并利用智能编码器提高了对抗性补丁的定位精度，从而提供了一种针对对抗性补丁的通用防御。 |
| [^69] | [Too sick for surveillance: Can federal HIV service data improve federal HIV surveillance efforts?.](http://arxiv.org/abs/2304.10023) | 本研究提取了联邦艾滋病服务数据和艾滋病监测数据，并使用深度学习模型系列来识别未被监测到的服务数据，旨在为未来的艾滋病传播预防提供数据支持。 |
| [^70] | [Digital Twin Graph: Automated Domain-Agnostic Construction, Fusion, and Simulation of IoT-Enabled World.](http://arxiv.org/abs/2304.10018) | 提出了一个称为数字孪生图(DTG)的数据结构，可以完全自动化和领域无关地构建数字孪生，采用了数据驱动和图学习方法来解决数字孪生的关键挑战。 |
| [^71] | [Power Law Trends in Speedrunning and Machine Learning.](http://arxiv.org/abs/2304.10004) | 该论文发现了速通世界纪录改进的幂律模式，并利用这一发现提高了预测速通世界纪录精度的方法，并在机器学习基准上得到了类似的结果。结果表明，ML基准远未饱和，而机器学习中的改进具有突然性。 |
| [^72] | [Model Based Reinforcement Learning for Personalized Heparin Dosing.](http://arxiv.org/abs/2304.10000) | 本文提出了一种基于预测模型和模型基础的强化学习方法来解决个性化肝素剂量控制问题，相较于标准治疗和无模型方法具有更好的安全性和功效。 |
| [^73] | [Data as voters: instance selection using approval-based multi-winner voting.](http://arxiv.org/abs/2304.09995) | 该论文提出了一种基于赞成票的多获胜者投票的实例选择方法，通过代表性投票规则选择获胜者，并将其作为减少训练集的数据实例。 |
| [^74] | [Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion.](http://arxiv.org/abs/2304.09994) | 本研究提出了一种基于CNN-RNN混合特征融合建模的贝叶斯优化城市洪水预测模型，实现了静态和动态的预测，并通过结合多个CNN和RNN模型，在精度上取得了显著提高。 |
| [^75] | [CKmeans and FCKmeans : Two Deterministic Initialization Procedures For Kmeans Algorithm Using Crowding Distance.](http://arxiv.org/abs/2304.09989) | 本论文提出了两种新型的确定性初始化过程（CKmeans和FCKmeans）以改进Kmeans聚类，并且实验证明这些过程在聚类准确度方面优于传统的Kmeans和Kmeans++。 |
| [^76] | [Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra.](http://arxiv.org/abs/2304.09987) | 本文提出了一种基于四面体和 Delaunay 表示的自适应表示方法，用于神经辐射场，可实现高效的训练和最先进的结果，同时提供了更多接近表面的细节，并且实现了比基于点的表示更好的性能。 |
| [^77] | [Interpretable (not just posthoc-explainable) heterogeneous survivor bias-corrected treatment effects for assignment of postdischarge interventions to prevent readmissions.](http://arxiv.org/abs/2304.09981) | 该研究使用生存分析来评价出院后评估和管理（E/M）服务对防止再住院或死亡的影响，避免了机器学习中幸存者偏差的问题，并确定了个案管理服务在减少再住院方面最为有效，尤其对于出院后到长期护理机构的患者以及入院前有高资源利用率的患者。 |
| [^78] | [Beyond Transformers for Function Learning.](http://arxiv.org/abs/2304.09979) | 本文通过给Transformer模型增加两个简单的归纳学习偏见探究了学习和预测简单函数的能力，并在大型神经网络模型的背景下发现这些偏见的帮助，同时指出这些偏见可能有助于人类在外推能力方面的归纳学习。 |
| [^79] | [Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision.](http://arxiv.org/abs/2304.09975) | 本文提出了一个无监督的图神经网络解决肾移植问题，可以近似解决NP难问题，并且比其他方法更好。 |
| [^80] | [Scheduling DNNs on Edge Servers.](http://arxiv.org/abs/2304.09961) | 本论文研究了如何加速为多个客户端运行边缘服务器DNN。批处理多个DNN请求可以显著加速处理时间。研究设计了一种新的调度算法，并开发了一种协作方法来调度多个边缘服务器的DNN请求，进一步提高了处理速度。 |
| [^81] | [A Latent Space Theory for Emergent Abilities in Large Language Models.](http://arxiv.org/abs/2304.09960) | 本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。 |
| [^82] | [Online Ensemble of Models for Optimal Predictive Performance with Applications to Sector Rotation Strategy.](http://arxiv.org/abs/2304.09947) | 通过机器学习模型和资产特定因素在预测行业回报和测量行业特定风险溢价方面获得更大经济收益，开发了一种新型在线集成算法来学习优化预测性能，特别适用于时间序列问题和可能的黑盒模型系统。 |
| [^83] | [Stock Price Predictability and the Business Cycle via Machine Learning.](http://arxiv.org/abs/2304.09937) | 研究发现，机器学习模型在大多数衰退期间的表现较差，与稳定市场有关的因素可能是其表现优异的原因。因此，我们建议ML从业者在衰退和复苏期间评估其模型。 |
| [^84] | [Identifying Trades Using Technical Analysis and ML/DL Models.](http://arxiv.org/abs/2304.09936) | 本文讲述了如何使用ML/DL模型预测股票价格，以帮助投资者和金融机构做出明智的决策和制定有效的风险管理政策。 |
| [^85] | [Personalized State Anxiety Detection: An Empirical Study with Linguistic Biomarkers and A Machine Learning Pipeline.](http://arxiv.org/abs/2304.09928) | 该论文研究了个性化状态焦虑检测，利用数字生物标志物和机器学习技术检测状态焦虑，并通过多层个性化机器学习流程，考虑上下文，从而更好地捕捉个体之间的心理和行为反应的差异。 |
| [^86] | [The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning.](http://arxiv.org/abs/2304.09914) | 本文使用机器学习的算法分析了来自15个不同国家的220个政治领袖的YouTube视频，总结了政治领袖面部情感表达的差异。 |
| [^87] | [Quantum Kernel Alignment with Stochastic Gradient Descent.](http://arxiv.org/abs/2304.09899) | 本论文研究了量子内核匹配问题，在Pegasos算法的基础上使用随机梯度下降法解决了该问题，并展示了其在非稳定化内核方面的高效性。 |
| [^88] | [GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models.](http://arxiv.org/abs/2304.09875) | 本文提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。该分数捕捉了所有样本中的平均认证防攻击扰动水平，无需运行对抗性攻击。 |
| [^89] | [Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms.](http://arxiv.org/abs/2304.09872) | 本文提出了一种基于深度函数的偏序集合描述性分析框架，并引入了偏序版本的单纯深度，用于比较基于多维性能度量的机器学习算法。实验证明此方法与现有基准方法不同，为分类器比较提供了新的视角。 |
| [^90] | [A Theory on Adam Instability in Large-Scale Machine Learning.](http://arxiv.org/abs/2304.09871) | Adam优化算法在大批量的训练下容易出现不稳定现象，并导致训练异常，作者提出了该现象的理论解释。 |
| [^91] | [Heterogeneous-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.09870) | 提出了一种异构智能体强化学习（HARL）算法，解决了协作多智能体强化学习中的参数共享限制，同时通过引入多智能体优势分解引理和序列更新方案，建立了异构智能体信任区域学习（HATRL）算法及其易处理的逼近方式 HATRPO 和 HAPPO。此外，发现了一种名为异构智能体镜像学习（HAML）的新型框架，加强了对HATRPO和HAPPO的理论保证。 |
| [^92] | [Evolving Constrained Reinforcement Learning Policy.](http://arxiv.org/abs/2304.09869) | 本文提出了一种新颖的进化约束强化学习算法，采用随机排名自适应平衡奖励和约束违规，并同时通过维护一组拉格朗日松弛系数及一个约束缓冲区来限制策略行为。在机器人控制基准测试中取得优异性能。 |
| [^93] | [Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?.](http://arxiv.org/abs/2304.09868) | 本文介绍了一种通过保留谱的数据压缩来加速支持向量聚类的方法，可以显著提高聚类速度而不牺牲聚类质量。 |
| [^94] | [Graph Neural Network-Based Anomaly Detection for River Network Systems.](http://arxiv.org/abs/2304.09367) | 本研究采用图神经网络模型Graph Deviation Network (GDN)来捕捉河流传感器数据的复杂时空关系，并提出了备用异常阈值标准GDN+，以实现对水质的准确持续监测。 |
| [^95] | [In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion.](http://arxiv.org/abs/2304.08658) | 本研究利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力，可以实时预测每个沃克塞尔中的气孔存在，是一个重大飞跃。 |
| [^96] | [A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions.](http://arxiv.org/abs/2304.06787) | 本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。 |
| [^97] | [Physics-informed radial basis network (PIRBN): A local approximation neural network for solving nonlinear PDEs.](http://arxiv.org/abs/2304.06234) | PIRBN是一种局部逼近神经网络，适用于求解具有高频特征和不适定计算域的PDE方程，相比PINN更加高效有效。通过使用梯度下降法训练PIRBN可以收敛到高斯过程。 |
| [^98] | [Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery.](http://arxiv.org/abs/2304.05294) | 本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。 |
| [^99] | [A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models.](http://arxiv.org/abs/2304.04916) | 本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。 |
| [^100] | [SAM vs BET: A Comparative Study for Brain Extraction and Segmentation of Magnetic Resonance Images using Deep Learning.](http://arxiv.org/abs/2304.04738) | 该论文比较了采用深度学习模型的SAM与传统方法BET在MRI脑提取方面的效果，结果表明SAM在信号不均匀、非等向性分辨率或病变靠近脑外区域和脑膜时效果更佳，SAM表现出更准确、更健壮和更多样化的潜力。 |
| [^101] | [TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems.](http://arxiv.org/abs/2304.01951) | TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。 |
| [^102] | [Machine Learning for Economics Research: When What and How?.](http://arxiv.org/abs/2304.00086) | 本文精选综述了使用机器学习工具进行经济学研究和政策分析的文章，强调了机器学习在处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性方面的应用优势，成为计量经济学家工具箱中不可或缺的一部分。 |
| [^103] | [Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases.](http://arxiv.org/abs/2303.15739) | 本研究针对深度ReLU神经网络，证明了过参数化情况下的Bayesian自由能是有界的，说明Bayesian广义误差不会增加。 |
| [^104] | [Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects.](http://arxiv.org/abs/2303.13769) | 本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。 |
| [^105] | [MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder.](http://arxiv.org/abs/2303.04651) | MCTS-GEB是一个通用的重写系统，使用强化学习和蒙特卡洛树搜索来构建最优的E图，有效消除了E图构建中的顺序问题，并在评估中表现出很好的性能。 |
| [^106] | [Graph-level representations using ensemble-based readout functions.](http://arxiv.org/abs/2303.02023) | 本文介绍了一种基于集成的读出函数，能够将节点级表示转换为图级向量，并能够在分子应用中取得良好表现。 |
| [^107] | [Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning.](http://arxiv.org/abs/2303.00396) | 本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。 |
| [^108] | [Heuristic Modularity Maximization Algorithms for Community Detection Rarely Return an Optimal Partition or Anything Similar.](http://arxiv.org/abs/2302.14698) | 通过使用80个真实和随机网络，研究对比了当前8种基于模块化启发式算法和一种精确整数规划方法在社区检测中的优化表现，结果显示平均只有16.9%的启发式算法返回最优划分或相似结果。 |
| [^109] | [Hyena Hierarchy: Towards Larger Convolutional Language Models.](http://arxiv.org/abs/2302.10866) | 本文提出一种名为九斑狼的卷积语言模型，通过交错隐式参数化的长卷积和数据控制门构造。在记忆任务和推理任务中，序列有几千到几十万个标记，九斑狼取得了比其他算子更为精确的表现，达到了基于注意力的模型的水平，并取得了密集注意力模型的最新结构。 |
| [^110] | [Classy Ensemble: A Novel Ensemble Algorithm for Classification.](http://arxiv.org/abs/2302.10580) | Classy Ensemble是一种新颖的集成学习算法，通过每类准确率的加权组合来聚合模型，在大量机器学习数据集上展现出优越性能。并提出Classy Cluster Ensemble和Classy Evolutionary Ensemble两种增强方法。 |
| [^111] | [Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities.](http://arxiv.org/abs/2302.08761) | 本研究基于10个城市的大规模浮动车数据，提供了15分钟分辨率的交通速度信息，涵盖了从主干道到当地街道的所有街道级别，为城市交通运营和规划提供了重要数据。 |
| [^112] | [InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis.](http://arxiv.org/abs/2302.08624) | InstructABSA是一种使用指令学习范式的方面情感分析方法，能够显著提高Aspect Term Extraction、Aspect Term Sentiment Classification、和Joint Task subtasks三个子任务的性能，并且在多个数据集上表现超过之前的最先进方法。 |
| [^113] | [Fixing Overconfidence in Dynamic Neural Networks.](http://arxiv.org/abs/2302.06359) | 该论文提出了一种修复动态神经网络中过度自信问题的方法，通过对最后几层进行概率化处理，量化和纳入不确定性并有助于决定计算预算的确定。 |
| [^114] | [Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification.](http://arxiv.org/abs/2301.07609) | 该论文扩展了信息场理论(IFT)到物理信息场理论(PIFT)，将描述场的物理定律的信息编码为函数先验。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。 |
| [^115] | [Reconstructing Kernel-based Machine Learning Force Fields with Super-linear Convergence.](http://arxiv.org/abs/2212.12737) | 本文提出了一种基于Nyström型方法的预处理器构建框架，实现了在低数据范围内高效重构核机器力场，并在带有数万个培训点的化学系统中获得了稳定和准确的结果。 |
| [^116] | [MegaCRN: Meta-Graph Convolutional Recurrent Network for Spatio-Temporal Modeling.](http://arxiv.org/abs/2212.05989) | MegaCRN是一个使用时空元图学习机制的元图卷积递归网络，对于时空建模具有良好的性能。 |
| [^117] | [Effects of Spectral Normalization in Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2212.05331) | 本文研究多智能体强化学习中评论者的规范化方法，发现使用谱归一化(SN)可以使评论家更稳健地学习，特别是在稀疏奖励场景下。实验结果表明，在规范化评论家的情况下，能够快速轻松地从那些复杂情境中学习，这一点对于实现稳定学习非常重要。 |
| [^118] | [PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks.](http://arxiv.org/abs/2212.02397) | 本论文提出了PowRL框架来通过强化学习来有效应对电力网络中的不确定情况，并保持电网的可靠运行。 |
| [^119] | [Continuous Episodic Control.](http://arxiv.org/abs/2211.15183) | CEC是一种新颖的非参数情景记忆算法，用于连续性行动空间问题中的序列决策制定，其在几个稀疏奖励连续控制环境中比最先进的RL和记忆增强RL算法学习更快，是学习连续控制任务的快速方法。 |
| [^120] | [FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data.](http://arxiv.org/abs/2211.09299) | 本文提出了一种名为 FedFA 的联邦学习框架，通过特征锚定来对齐特征映射并校准分类器，解决了在异构数据时分类器和特征映射之间的恶性循环问题。在实验中表明，该方法能够提高准确性和收敛速度。 |
| [^121] | [Model free variable importance for high dimensional data.](http://arxiv.org/abs/2211.08414) | 该论文提出了一种无模型的变量重要性方法，可以与任意预测函数一起使用，不需要访问预测函数，可用于研究模型残差。引入了Cohort Shapley的积分梯度版本（IGCS），使得在二元预测器情况下也可以使用IG方法。 |
| [^122] | [Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation.](http://arxiv.org/abs/2211.02658) | 机器学习在自适应系统中成为热门方法，但利用机器学习会面临适应空间中的漂移问题。本文提出了一种名为"生命周期自适应"的新方法，能够更好地处理适应空间的漂移。 |
| [^123] | [RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection.](http://arxiv.org/abs/2211.00313) | 本论文提出了一种针对COVID-19检测的新颖区域引导的掩膜图像建模方法，该方法通过利用肺掩模信息来识别有效区域，以学习更有用的COVID-19检测信息。 |
| [^124] | [Federated Learning with Intermediate Representation Regularization.](http://arxiv.org/abs/2210.15827) | 本文提出一种新的联邦学习方法FedIntR，可以通过将中间层的表示集成到本地训练过程中，提供更细粒度的正则化，从而限制本地训练过程中偏离全局模型的程度，以提高模型性能。 |
| [^125] | [A Deep Learning Approach to Analyzing Continuous-Time Systems.](http://arxiv.org/abs/2209.12128) | 深度学习方法能够分析复杂自然过程的时间序列数据，放松了传统简化假设的限制，具备可解释性和灵活的函数逼近能力，能够应用于人类语言处理领域，改进数据解释力和探索性分析能力。 |
| [^126] | [Unsupervised representation learning with recognition-parametrised probabilistic models.](http://arxiv.org/abs/2209.05661) | 本文提出了一种基于识别参数模型的概率无监督学习新方法，可以灵活地学习识别模型，捕捉观测之间的潜在相关性，为图像分类和潜在分配问题提供了有效解决方案。 |
| [^127] | [On the Convergence of the ELBO to Entropy Sums.](http://arxiv.org/abs/2209.03077) | 本论文研究了 ELBO 收敛到熵和的问题，证明了对于一类广泛的生成模型，ELBO 在所有学习的稳定点处都等于一系列熵的和，为无监督学习的学习算法的基本属性提供了深入的洞察。 |
| [^128] | [HybMT: Hybrid Meta-Predictor based ML Algorithm for Fast Test Vector Generation.](http://arxiv.org/abs/2207.11312) | 本文提出一个基于混合元预测的机器学习算法用于快速测试向量生成，该算法通过2级预测器进一步减少了无用工作，顶层元预测器的准确率达到了99% |
| [^129] | [An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System.](http://arxiv.org/abs/2207.07886) | 该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。 |
| [^130] | [Outlier detection of vital sign trajectories from COVID-19 patients.](http://arxiv.org/abs/2207.07572) | 本文提出了一种使用动态时间扭曲距离的轨迹比较算法，可以检测COVID-19患者生命体征轨迹中的异常值，拟整合到现有系统中以改善健康恶化的识别准确度和时效性。 |
| [^131] | [Graph-based Molecular Representation Learning.](http://arxiv.org/abs/2207.04869) | 本文综述了最新的基于图形的分子表示方法，总结与分类为三组。纳入化学领域知识的方法取得了很大进展。未来需要更易于解释和可解释的MRL模型。 |
| [^132] | [A Search-Based Testing Approach for Deep Reinforcement Learning Agents.](http://arxiv.org/abs/2206.07813) | 本文提出一种基于搜索的测试方法，探索状态空间以检测DRL代理的安全性，并在三个基准测试中取得了比基线方法更高的状态空间覆盖率。 |
| [^133] | [Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays.](http://arxiv.org/abs/2206.07638) | 这篇论文证明了用虚拟迭代和延迟自适应步长可以在任意延迟下提高异步SGD算法性能，优于同步小批量SGD。 |
| [^134] | [Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data.](http://arxiv.org/abs/2206.00686) | 该论文提出了一种名为FedDPMS的算法，该算法利用变分自动编码器来合成本地数据集。这种增强方法可减轻数据异构效应，同时保护隐私。 |
| [^135] | [Continuous Generative Neural Networks.](http://arxiv.org/abs/2205.14627) | 本文介绍了一种连续生成神经网络(CGNN)的模型，使用条件保证CGNN是单射的，其生成流形被用于求解反问题，并证明了其方法的有效性和稳健性。 |
| [^136] | [Independent and Decentralized Learning in Markov Potential Games.](http://arxiv.org/abs/2205.14590) | 独立的去中心化学习在马尔科夫潜在博弈中有效，通过更新Q函数可以引导策略收敛到稳定的纳什平衡点。 |
| [^137] | [HOUDINI: Escaping from Moderately Constrained Saddles.](http://arxiv.org/abs/2205.13753) | 本文给出了第一个多项式时间算法，能在适度数量的约束下使梯度下降方法逃脱高维马鞍点。 |
| [^138] | [FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning.](http://arxiv.org/abs/2205.11039) | FLEX是一种新型KGR框架，能够真正处理包括合取、析取、否定等所有FOL操作并支持各种特征空间。实验证明，FLEX优于现有最先进算法。 |
| [^139] | [Communication-Efficient Adaptive Federated Learning.](http://arxiv.org/abs/2205.02719) | 本文提出了一种新的通信效率高的自适应联邦学习方法（FedCAMS），可以解决联邦学习中由于重复的服务器-客户端同步而产生的大量通信开销和基于 SGD 的模型更新缺乏适应性等诸多问题。 |
| [^140] | [A Manifold Two-Sample Test Study: Integral Probability Metric with Neural Networks.](http://arxiv.org/abs/2205.02043) | 文章提出了一种基于积分概率度量的两样本检验方法，适用于低维流形上支持的高维样本。实验结果表明，所提出的方法在统计功率方面优于最先进的方法。 |
| [^141] | [Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification: A survey.](http://arxiv.org/abs/2204.08461) | 本文调查研究探讨了将时间卷积神经网络应用于卫星图像时序分类的方法，相比其他现代方法具有更好的表现。 |
| [^142] | [Kernel Robust Hypothesis Testing.](http://arxiv.org/abs/2203.12777) | 本文使用核方法构造不确定性集，在贝叶斯设置和Neyman-Pearson设置中分别研究了最小化最坏情况下错误概率和控制错误概率的问题，并提出了基于MMD的一系列测试。 |
| [^143] | [AI based Log Analyser: A Practical Approach.](http://arxiv.org/abs/2203.10960) | 本研究使用Transformer构建仅使用正常的日志条目来训练新模型，并通过多种形式的扰动来增强日志，以应对日志来源的异构性和缺乏标签用于训练分类器的限制；该模型进一步使用一种有限的标签样本进行强化学习的方式进行微调，实验结果表现良好。 |
| [^144] | [Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information.](http://arxiv.org/abs/2203.07893) | 这篇论文提出了一种光谱属性去除方法，用于从神经表示中去除私有或保护信息。实验证明，该方法保留了更好的主要任务性能，同时只需少量的被保护属性数据即可去除信息，适用于资源有限的情况。 |
| [^145] | [Adaptive Cross-Layer Attention for Image Restoration.](http://arxiv.org/abs/2203.03619) | 本文提出了一种自适应跨层注意力（ACLA）模块，它可以汇总不同层次的特征信息用于非局部注意力的处理，以提高图像恢复的效果。 |
| [^146] | [Spin-Dependent Graph Neural Network Potential for Magnetic Materials.](http://arxiv.org/abs/2203.02853) | SpinGNN是一种自旋相关的相互作用势方法，利用图神经网络描述磁性系统的海森堡型自旋-晶格相互作用和高阶自旋-晶格耦合，同时成功地模拟了BiFeO3中微妙的自旋-晶格耦合。 |
| [^147] | [Quantum Lazy Training.](http://arxiv.org/abs/2202.08232) | 本文证明了当量子比特数量较大时，地理局部化参数化量子电路的训练进入懒惰阶段，限制了参数变化速率并保证了相应量子模型的线性逼近的精度。 |
| [^148] | [More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks.](http://arxiv.org/abs/2202.03195) | 本文研究了联邦图神经网络中的后门攻击，填补了相关领域的研究空白，并发现在该场景下分布式后门攻击的成功率更高。 |
| [^149] | [Supervision by Denoising.](http://arxiv.org/abs/2202.02952) | 本文提出了一种去噪监督（SUD）框架，可以监督重建模型，并避免制作特定于成像领域的可微正则化器。 |
| [^150] | [Byzantine-Robust Decentralized Learning via ClippedGossip.](http://arxiv.org/abs/2202.01545) | 本文提出的剪切Gossip算法是第一个能够在标准假设下证明收敛到非凸目标的$O(\delta_{max} \zeta^2 /\gamma^2)$邻域的算法，并在大量攻击下表现良好。 |
| [^151] | [DDPG-Driven Deep-Unfolding with Adaptive Depth for Channel Estimation with Sparse Bayesian Learning.](http://arxiv.org/abs/2201.08477) | 本文提出了一种DDPG驱动的自适应深度深度展开框架，适用于不同的输入。该框架被用于解决大规模多输入多输出系统中的信道估计问题。 |
| [^152] | [Probabilistic Approach for Road-Users Detection.](http://arxiv.org/abs/2112.01360) | 本文介绍了一种用于缓解深度目标检测模型中过于自信预测问题的方法，通过引入一种新颖的概率层来避免传统的预测层，实验证明该方法能够减少假阳性中的过度自信。 |
| [^153] | [A transformer-based model for default prediction in mid-cap corporate markets.](http://arxiv.org/abs/2111.09902) | 本文利用Transformer模型，将违约预测问题作为多标签时间序列分类问题，并通过注意力热图解释模型预测。同时，提出了一种自定义损失函数和一个新颖的多通道架构，以优化模型。 |
| [^154] | [Labeled sample compression schemes for complexes of oriented matroids.](http://arxiv.org/abs/2110.15168) | 本研究提出了适用于VC维为$d$的有向拟阵的复合体的拓扑的标记样本压缩方案，为计算学习理论中的样本压缩猜想迈出了一步。 |
| [^155] | [Boundary Graph Neural Networks for 3D Simulations.](http://arxiv.org/abs/2106.11299) | 本文提出了一种新型边界图神经网络（BGNNs），能够有效地表示三维颗粒流动的复杂几何形状，从而实现了高效的预测和计算。 |
| [^156] | [Can a single neuron learn predictive uncertainty?.](http://arxiv.org/abs/2106.03702) | 本文介绍了一种基于单个神经元的新型非参数分位数估计方法，该方法在小样本大小和真实世界实验中的优越表现表明其可以消除与模型规范相关的自由度，并提供更好的预测不确定性估计。 |
| [^157] | [Policy design in experiments with unknown interference.](http://arxiv.org/abs/2011.08174) | 本文研究了如何在有限数量的大簇中进行实验设计，估计和推断最大福利政策，并提出单波实验和多波实验的方法来解决溢出效应问题。 |
| [^158] | [The ELBO of Variational Autoencoders Converges to a Sum of Three Entropies.](http://arxiv.org/abs/2010.14860) | 标准变分自编码器的ELBO在稳定点处可以以闭合形式计算，收敛于三个熵之和。（其中一个熵为先验分布的熵，一个为可观测分布的熵，一个为变分分布的平均熵，成果证明了ELBO在稳定点处等于熵。） |
| [^159] | [Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent.](http://arxiv.org/abs/2009.04709) | 本文提出了一种新的定义来描述对抗训练中梯度的优选方向，利用生成对抗网络的指标来评估对齐情况，并表明在对齐方向上的限制可以进一步提高模型的鲁棒性。 |
| [^160] | [FRMDN: Flow-based Recurrent Mixture Density Network.](http://arxiv.org/abs/2008.02144) | 本文提出了一种基于流的循环混合密度网络，通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上，该模型在图像序列的拟合度上表现显著，具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。 |

# 详细

[^1]: 深度神经网络的多维度不确定量化

    Multidimensional Uncertainty Quantification for Deep Neural Networks. (arXiv:2304.10527v1 [cs.LG])

    [http://arxiv.org/abs/2304.10527](http://arxiv.org/abs/2304.10527)

    研究如何量化DNN的不同不确定性，并将其用于更有效地解决不同的决策问题。

    

    深度神经网络（DNN）在图像和视频分析、自然语言处理、推荐系统和药物研究等许多应用中取得了巨大的成功。然而，由不同根本原因导致的内在不确定性已经被认为是DNN在寻找真实世界问题的稳健和可信解决方案方面的严重障碍。缺乏对这种不确定性的考虑可能会导致不必要的风险。本文研究了如何度量DNN的不同不确定因素，并将它们用于更有效地解决不同的决策问题。

    Deep neural networks (DNNs) have received tremendous attention and achieved great success in various applications, such as image and video analysis, natural language processing, recommendation systems, and drug discovery. However, inherent uncertainties derived from different root causes have been realized as serious hurdles for DNNs to find robust and trustworthy solutions for real-world problems. A lack of consideration of such uncertainties may lead to unnecessary risk. For example, a self-driving autonomous car can misdetect a human on the road. A deep learning-based medical assistant may misdiagnose cancer as a benign tumor.  In this work, we study how to measure different uncertainty causes for DNNs and use them to solve diverse decision-making problems more effectively. In the first part of this thesis, we develop a general learning framework to quantify multiple types of uncertainties caused by different root causes, such as vacuity (i.e., uncertainty due to a lack of evidence)
    
[^2]: 学习窄的单隐藏层ReLU网络

    Learning Narrow One-Hidden-Layer ReLU Networks. (arXiv:2304.10524v1 [cs.LG])

    [http://arxiv.org/abs/2304.10524](http://arxiv.org/abs/2304.10524)

    本文提出了一个在多项式时间内成功的学习Narrow One-Hidden-Layer ReLU网络的算法，而不需要额外的假设，并使用了分析高阶矩张量的随机收缩的方法，使得可以发现单个神经元。

    

    本文考虑了一个经过充分研究的问题——关于在$d$维输入上的高斯分布中，学习$k$个ReLU激活的线性组合。我们提出了第一个在$k$为常数时成功的多项式时间算法。所有之前多项式时间的学习器都需要对网络进行额外的假设，比如正系数系合或隐藏权重向量的矩阵良好定义。我们的方法基于分析高阶矩张量的随机收缩。我们采用多尺度分析来证明足够接近的神经元可以被合并在一起，从而规避了以前工作中存在的定性问题。这使我们能够设计一个迭代过程来发现单个神经元。

    We consider the well-studied problem of learning a linear combination of $k$ ReLU activations with respect to a Gaussian distribution on inputs in $d$ dimensions. We give the first polynomial-time algorithm that succeeds whenever $k$ is a constant. All prior polynomial-time learners require additional assumptions on the network, such as positive combining coefficients or the matrix of hidden weight vectors being well-conditioned.  Our approach is based on analyzing random contractions of higher-order moment tensors. We use a multi-scale analysis to argue that sufficiently close neurons can be collapsed together, sidestepping the conditioning issues present in prior work. This allows us to design an iterative procedure to discover individual neurons.
    
[^3]: 对比调节: 帮助遗忘掩码自编码器的一点小帮助

    Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])

    [http://arxiv.org/abs/2304.10520](http://arxiv.org/abs/2304.10520)

    本文提出了一种新方法：掩码自编码器对比调整(MAE-CT)，利用最近邻对比学习（NNCLR）在标记数据较少的情况下实现下游分类，该方法可以将图像的丰富特征聚类成对象的语义聚类。

    

    掩码图像建模方法，如掩码自编码器（MAE），可以有效地学习输入的丰富表示。但是，为了适应下游任务，由于其丰富的特征不仅捕获了对象而且还包括不相关的图像背景，因此它们需要足够数量的标记数据。相比之下，实例辨别方法侧重于对象。在这项工作中，我们研究如何将MIM的效率和可伸缩性与ID的能力相结合，以在缺少大量标记数据的情况下执行下游分类。为此，我们引入了掩码自编码器对比调整（MAE-CT），这是一种顺序方法，可以将最近邻对比学习（NNCLR）应用于预先训练的MAE。MAE-CT调整了丰富的特征，使它们形成对象的语义聚类，而不使用任何标签。应用于大型和巨型Vision Transformer（ViT）模型时，MAE-CT在线性探测，k-均值聚类和半监督少量样本学习方面匹配或超越了在ImageNet上训练的先前的自我监督方法。

    Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features capture not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT matches or excels previous self-supervised methods trained on ImageNet in linear probing, k
    
[^4]: 医学图像分析中的任意分割模型：一项实验研究

    Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])

    [http://arxiv.org/abs/2304.10517](http://arxiv.org/abs/2304.10517)

    本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。

    

    由于数据注释的有限可用性和获取成本，训练医学图像分割模型仍然具有挑战性。Segment Anything Model（SAM）是一种基础模型，经过超过10亿个注释的训练，主要用于自然图像，旨在能够以交互方式分割用户定义的感兴趣的对象。尽管SAM在自然图像上表现出色，但不清楚该模型在转换到医学图像领域时会受到多大影响。在这里，我们对SAM在各种模态和解剖学的11个医学图像数据集上进行了广泛的评估。在我们的实验中，我们使用标准方法生成点提示来模拟交互分割。实验结果表明，SAM基于单点提示的表现在任务和数据集方面高度变化，即从脊柱MRI数据集的0.1135到髋关节X射线数据集的0.8650。

    Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
    
[^5]: CP-CNN: 基于核心-周边原则指导的卷积神经网络

    CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network. (arXiv:2304.10515v1 [cs.NE])

    [http://arxiv.org/abs/2304.10515](http://arxiv.org/abs/2304.10515)

    本文提出一种基于人脑核心-周边原则的类脑设计原则以指导CNN的设计。通过在网络布线模式和卷积操作中实现核心-周边原则，我们提出了一种全新的CP-CNN架构，它在各种视觉任务中表现出色。

    

    卷积神经网络(CNN)的发展很大程度上归因于其架构设计，即网络布线模式。神经结构搜索(NAS)通过自动化搜索最优网络架构推进了这一进展，但所得到的网络实例在不同任务中可能无法泛化。为了解决这个问题，探索跨任务通用的网络设计原则是更实际的解决方案。本研究基于人脑网络的核心-周边属性探索了一种新颖的类脑设计原则，以指导CNN的设计。我们的工作从最近的研究中汲取灵感，这些研究表明人工和生物神经网络在优化网络架构方面可能具有共同的原则。我们在网络布线模式的设计和卷积操作的稀疏化中实现了核心-周边原则。结果，使用核心-周边原则指导的CNN (CP-CNN)在各种视觉任务上得到了良好的表现。

    The evolution of convolutional neural networks (CNNs) can be largely attributed to the design of its architecture, i.e., the network wiring pattern. Neural architecture search (NAS) advances this by automating the search for the optimal network architecture, but the resulting network instance may not generalize well in different tasks. To overcome this, exploring network design principles that are generalizable across tasks is a more practical solution. In this study, We explore a novel brain-inspired design principle based on the core-periphery property of the human brain network to guide the design of CNNs. Our work draws inspiration from recent studies suggesting that artificial and biological neural networks may have common principles in optimizing network architecture. We implement the core-periphery principle in the design of network wiring patterns and the sparsification of the convolution operation. The resulting core-periphery principle guided CNNs (CP-CNNs) are evaluated on t
    
[^6]: “我们能检测出物质使用障碍吗？”：来自暗网社交媒体上的知识和时间感知分类研究。(arXiv:2304.10512v1[cs.LG])

    "Can We Detect Substance Use Disorder?": Knowledge and Time Aware Classification on Social Media from Darkweb. (arXiv:2304.10512v1 [cs.LG])

    [http://arxiv.org/abs/2304.10512](http://arxiv.org/abs/2304.10512)

    本研究使用知识感知BERT模型，分析社交媒体上发布的与合成阿片类物质相关的帖子，以了解用户对不同合成阿片类物质的情感和情绪，从而帮助预测物质使用障碍。

    

    目前，美国的阿片类和物质滥用问题日益严重，这种现象被称为“阿片类危机”。物质使用和心理健康之间的关系已经得到了广泛研究，其中一种可能的关系是：物质滥用导致心理健康状况不佳。然而，由于缺乏证据，导致合法途径购买的阿片类物质很难获得。本研究分析了社交媒体上有关物质使用的帖子，重点关注通过加密市场销售的合成阿片类物质。我们使用了药物滥用本体论，最先进的深度学习和知识感知BERT模型生成社交媒体帖子的情感和情绪，以了解用户对社交媒体的看法，研究问题，例如：人们对哪种合成阿片类物质持乐观态度、中立态度或者消极态度？或者哪些药物引起了恐惧和悲伤？人们喜欢哪些药物，或者对哪些药物感激？哪些药物人们持消极态度？

    Opioid and substance misuse is rampant in the United States today, with the phenomenon known as the "opioid crisis". The relationship between substance use and mental health has been extensively studied, with one possible relationship being: substance misuse causes poor mental health. However, the lack of evidence on the relationship has resulted in opioids being largely inaccessible through legal means. This study analyzes the substance use posts on social media with opioids being sold through crypto market listings. We use the Drug Abuse Ontology, state-of-the-art deep learning, and knowledge-aware BERT-based models to generate sentiment and emotion for the social media posts to understand users' perceptions on social media by investigating questions such as: which synthetic opioids people are optimistic, neutral, or negative about? or what kind of drugs induced fear and sorrow? or what kind of drugs people love or are thankful about? or which drugs people think negatively about? or 
    
[^7]: OutCenTR: 一种用于高维数据集中预测漏洞利用的半监督框架

    OutCenTR: A novel semi-supervised framework for predicting exploits of vulnerabilities in high-dimensional datasets. (arXiv:2304.10511v1 [cs.CR])

    [http://arxiv.org/abs/2304.10511](http://arxiv.org/abs/2304.10511)

    本文提出了一种名为OutCenTR的半监督框架，用于预测高维度数据集中可能被利用的漏洞，同时提出一种降维技术OutCenTR，可以增强基本的离群点检测模型，实验证明了OutCenTR的有效性和效率，平均F1分数提高了5倍。

    

    每天都有越来越多的漏洞被报道，然而这些漏洞并不都是相同的，有些比其他的更具有针对性。正确估计漏洞被利用的可能性是系统管理员的关键任务。本文利用离群点检测技术来预测高度不平衡和高维度数据集中可能被利用的漏洞，例如国家漏洞数据库。我们提出了一种降维技术OutCenTR，可以增强基本的离群点检测模型。我们在4个基准数据集和12个合成数据集上通过实验证明了OutCenTR的有效性和效率。实验结果显示，与PCA和GRP等最先进的降维技术相比，平均F1分数提高了5倍。

    An ever-growing number of vulnerabilities are reported every day. Yet these vulnerabilities are not all the same; Some are more targeted than others. Correctly estimating the likelihood of a vulnerability being exploited is a critical task for system administrators. This aids the system administrators in prioritizing and patching the right vulnerabilities. Our work makes use of outlier detection techniques to predict vulnerabilities that are likely to be exploited in highly imbalanced and high-dimensional datasets such as the National Vulnerability Database. We propose a dimensionality reduction technique, OutCenTR, that enhances the baseline outlier detection models. We further demonstrate the effectiveness and efficiency of OutCenTR empirically with 4 benchmark and 12 synthetic datasets. The results of our experiments show on average a 5-fold improvement of F1 score in comparison with state-of-the-art dimensionality reduction techniques such as PCA and GRP.
    
[^8]: 缓解双重用途风险的化学数据审查方法

    Censoring chemical data to mitigate dual use risk. (arXiv:2304.10510v1 [cs.LG])

    [http://arxiv.org/abs/2304.10510](http://arxiv.org/abs/2304.10510)

    为了缓解化学数据被用于开发识别新型毒素或化学战剂的预测模型的双重用途风险，提出了一种基于模型不可知的方法，可在保留有利于训练深度神经网络的数据的效用区域的同时，对敏感信息的数据集进行有选择性的添加噪音，该方法被证明是有效的，并且忽略敏感数据会增加模型的方差。

    

    机器学习应用的双重用途（模型既可以用于有益用途又可以用于恶意目的）提出了重大挑战。最近，在化学领域，由于含有敏感标签（如毒理学信息）的化学数据集可能被用于开发识别新型毒素或化学战剂的预测模型，这已成为一个特别的担忧。为了缓解双重用途风险，我们提出了一种基于模型不可知的方法，可以有选择性地添加噪音到数据集中，同时保留有利于训练深度神经网络的数据的效用区域。我们评估了所提出方法在最小二乘法、多层感知器和图神经网络中的有效性, 发现有选择性添加噪声数据可以引入模型的方差和预测敏感标签的偏差，这表明可以实现包含敏感信息的数据集的安全共享。我们还发现，忽略敏感数据通常会增加模型的方差。

    The dual use of machine learning applications, where models can be used for both beneficial and malicious purposes, presents a significant challenge. This has recently become a particular concern in chemistry, where chemical datasets containing sensitive labels (e.g. toxicological information) could be used to develop predictive models that identify novel toxins or chemical warfare agents. To mitigate dual use risks, we propose a model-agnostic method of selectively noising datasets while preserving the utility of the data for training deep neural networks in a beneficial region. We evaluate the effectiveness of the proposed method across least squares, a multilayer perceptron, and a graph neural network. Our findings show selectively noised datasets can induce model variance and bias in predictions for sensitive labels with control, suggesting the safe sharing of datasets containing sensitive information is feasible. We also find omitting sensitive data often increases model variance 
    
[^9]: 视频预训练Transformer: 多模态预训练专家混合体

    Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts. (arXiv:2304.10505v1 [cs.CV])

    [http://arxiv.org/abs/2304.10505](http://arxiv.org/abs/2304.10505)

    本文提出了一种视频预训练Transformer，它使用多个最先进的编码器模型将视频转换成通用表示，可以结合多种模式获得最佳性能。

    

    本文提出了视频预训练Transformer (VPT)。VPT使用来自之前工作的四个最先进的编码器模型将视频转换成紧凑的嵌入序列。我们的骨干网络基于参考Flan-T5-11B架构，学习视频的通用表示形式，这是编码器模型的非线性总和。它使用自回归因果语言建模损失通过预测YouTube视频中讲话的单词进行学习。最后，我们通过为每个任务训练全连接预测头，在标准的下游基准测试中进行评估。据我们所知，这是第一个在“嵌入->骨干网络->预测头”设计模式中使用多个冻结的最先进模型作为编码器的研究，而其他研究都是训练自己的联合编码器模型。此外，我们通过添加显式的场景图信息，包含了比当前SOTA Merlot Reserve更多的模态。出于这两个原因，我们相信它可以将世界上最好的开源模型结合起来，实现SOTA性能。

    We present Video Pre-trained Transformer. VPT uses four SOTA encoder models from prior work to convert a video into a sequence of compact embeddings. Our backbone, based on a reference Flan-T5-11B architecture, learns a universal representation of the video that is a non-linear sum of the encoder models. It learns using an autoregressive causal language modeling loss by predicting the words spoken in YouTube videos. Finally, we evaluate on standard downstream benchmarks by training fully connected prediction heads for each task. To the best of our knowledge, this is the first use of multiple frozen SOTA models as encoders in an "embedding -> backbone -> prediction head" design pattern - all others have trained their own joint encoder models. Additionally, we include more modalities than the current SOTA, Merlot Reserve, by adding explicit Scene Graph information. For these two reasons, we believe it could combine the world's best open-source models to achieve SOTA performance. Initial 
    
[^10]: 基于功率谱估计和不确定区域的鲁棒自适应波束成形方法研究

    Study of Robust Adaptive Beamforming with Covariance Matrix Reconstruction Based on Power Spectral Estimation and Uncertainty Region. (arXiv:2304.10502v1 [cs.NI])

    [http://arxiv.org/abs/2304.10502](http://arxiv.org/abs/2304.10502)

    本文提出了一种基于功率谱估计和不确定区域的简单且有效的鲁棒自适应波束成形技术，可以更好地处理干扰加噪声成分，同时具有更好的鲁棒性和自适应性能。

    

    本文提出了一种基于干扰加噪声成分的功率谱估计和不确定区域（PSEUR）的一致线性阵列的简单有效的鲁棒自适应波束成形技术。具体来说，提出了两种算法来找到每个瞬时的干扰角度区间，基于采用的干扰方向的空间不确定区域。此外，基于对干扰和噪声成分功率的估计引入了一个功率谱，从而允许开发一种鲁棒的IPN协方差矩阵重建方法。所提出的方法有两个主要优点。首先，包含干扰方向的角度区域基于阵列数据的统计信息进行更新。其次，所提出的IPN-PSEUR方法避免了估计干扰区间所有可能方向的功率谱。仿真结果表明，所提出的方法在鲁棒性和自适应性方面优于现有的波束成形技术。

    In this work, a simple and effective robust adaptive beamforming technique is proposed for uniform linear arrays, which is based on the power spectral estimation and uncertainty region (PSEUR) of the interference plus noise (IPN) components. In particular, two algorithms are presented to find the angular sector of interference in every snapshot based on the adopted spatial uncertainty region of the interference direction. Moreover, a power spectrum is introduced based on the estimation of the power of interference and noise components, which allows the development of a robust approach to IPN covariance matrix reconstruction. The proposed method has two main advantages. First, an angular region that contains the interference direction is updated based on the statistics of the array data. Secondly, the proposed IPN-PSEUR method avoids estimating the power spectrum of the whole range of possible directions of the interference sector. Simulation results show that the performance of the pro
    
[^11]: 使用 Transformer 模型进行 Simply Typed Lambda Calculus 的类型推理：深度学习在代码中的案例研究

    Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code. (arXiv:2304.10500v1 [cs.PL])

    [http://arxiv.org/abs/2304.10500](http://arxiv.org/abs/2304.10500)

    本文探索使用 Transformer 模型进行形式语言的推理。通过使用编程语言最基本的特性——术语和类型之间的关系，提出了一种基准测试，并提出了一种基于 Transformer 的类型推理模型。实验结果表明，TTI 模型在推理准确性和速度方面都优于现有的最先进方法。

    

    尽管深度学习和形式语言的交叉领域的研究越来越多，但系统性地探索使用 Transformer 模型推理有类型 lambda 演算方面的研究相对较少。这是一个有趣的研究领域，有两个原因。首先，有类型的 lambda 演算是编程语言的基础。将各种有类型 lambda 演算与有效神经网络结构相关联的一组启发式方法会提供一种将语言特征（例如多态性，子类型，继承等）映射到架构选择的系统方法。其次，Transformer 模型广泛用于应用于代码的深度学习架构中，但其设计和超参数空间在编程语言应用中尚未得到充分探索。因此，我们建议通过也许是编程语言最简单和最基本的特性，即术语和类型之间的关系，来探索这一点的基准测试。

    Despite a growing body of work at the intersection of deep learning and formal languages, there has been relatively little systematic exploration of transformer models for reasoning about typed lambda calculi. This is an interesting area of inquiry for two reasons. First, typed lambda calculi are the lingua franc of programming languages. A set of heuristics that relate various typed lambda calculi to effective neural architectures would provide a systematic method for mapping language features (e.g., polymorphism, subtyping, inheritance, etc.) to architecture choices. Second, transformer models are widely used in deep learning architectures applied to code, but the design and hyperparameter space for them is large and relatively unexplored in programming language applications. Therefore, we suggest a benchmark that allows us to explore exactly this through perhaps the simplest and most fundamental property of a programming language: the relationship between terms and types. Consequent
    
[^12]: 无限物理猴子：深度学习方法在构象生成中真的比传统方法表现更好吗？

    Infinite Physical Monkey: Do Deep Learning Methods Really Perform Better in Conformation Generation?. (arXiv:2304.10494v1 [q-bio.BM])

    [http://arxiv.org/abs/2304.10494](http://arxiv.org/abs/2304.10494)

    深度学习方法在构象生成中的表现受到质疑，本文研究了基于物理的方法在这个问题中的表现，发现仍是一个有力选项。

    

    构象生成是药物发现和化学信息学中的基本问题，有机分子构象生成，特别是在真空和蛋白质口袋环境中，与药物设计最相关。最近，随着几何神经网络的发展，数据驱动的方案已成功应用于这一领域，包括分子构象生成（在真空中）和结合位姿生成（在蛋白质口袋中）。前者打败了传统的ETKDG方法，而后者达到了与广泛使用的分子对接软件相似的精度。尽管这些方法已经显示出了有希望的结果，但一些研究人员最近质疑深度学习（DL）方法是否通过无参数方法更好地在分子构象生成中表现。令人惊讶的是，他们设计的方法有点类似于著名的无限猴子定理，这些猴子甚至装备了物理教育。本文旨在讨论该方法的可行性，并且通过简单的实验，我们发现传统的基于物理的方法仍然是构象生成问题中的有力选项。

    Conformation Generation is a fundamental problem in drug discovery and cheminformatics. And organic molecule conformation generation, particularly in vacuum and protein pocket environments, is most relevant to drug design. Recently, with the development of geometric neural networks, the data-driven schemes have been successfully applied in this field, both for molecular conformation generation (in vacuum) and binding pose generation (in protein pocket). The former beats the traditional ETKDG method, while the latter achieves similar accuracy compared with the widely used molecular docking software. Although these methods have shown promising results, some researchers have recently questioned whether deep learning (DL) methods perform better in molecular conformation generation via a parameter-free method. To our surprise, what they have designed is some kind analogous to the famous infinite monkey theorem, the monkeys that are even equipped with physics education. To discuss the feasib
    
[^13]: CoProver: 一种证明构造推荐系统

    CoProver: A Recommender System for Proof Construction. (arXiv:2304.10486v1 [cs.LO])

    [http://arxiv.org/abs/2304.10486](http://arxiv.org/abs/2304.10486)

    CoProver是一种证明构造推荐系统，能够从证明构造过程中的过去操作中学习，并通过探索存储在ITP中的关于以前证明的知识来提供有用的建议。

    

    交互式定理证明工具(ITPs)是形式化方法专家工具库中不可或缺的一部分, 用于构建和(形式)验证证明. 证明的复杂性和通常需要的专业水平往往会阻碍ITPs的采用. 最近的一系列工作研究了将基于ITP用户活动跟踪的机器学习模型作为实现完全自动化的可行途径的方法. 虽然这是一条有价值的研究线, 但仍然有许多问题需要人类的监督才能完全完成，因此将学习方法应用于协助用户提供有用建议可能更为有益。跟随用户协助的思路，我们介绍了CoProver，这是一个基于transformers的证明推荐系统，能够从证明构造过程中的过去操作中学习，同时探索存储在ITP中的关于以前证明的知识。

    Interactive Theorem Provers (ITPs) are an indispensable tool in the arsenal of formal method experts as a platform for construction and (formal) verification of proofs. The complexity of the proofs in conjunction with the level of expertise typically required for the process to succeed can often hinder the adoption of ITPs. A recent strain of work has investigated methods to incorporate machine learning models trained on ITP user activity traces as a viable path towards full automation. While a valuable line of investigation, many problems still require human supervision to be completed fully, thus applying learning methods to assist the user with useful recommendations can prove more fruitful.  Following the vein of user assistance, we introduce CoProver, a proof recommender system based on transformers, capable of learning from past actions during proof construction, all while exploring knowledge stored in the ITP concerning previous proofs. CoProver employs a neurally learnt sequenc
    
[^14]: 高效深度强化学习需要抑制过拟合

    Efficient Deep Reinforcement Learning Requires Regulating Overfitting. (arXiv:2304.10466v1 [cs.LG])

    [http://arxiv.org/abs/2304.10466](http://arxiv.org/abs/2304.10466)

    深度强化学习的主要瓶颈在于高时间差误差的验证集上出现了严重过拟合问题。

    

    通过与环境的交互收集有限的数据进行策略学习的深度强化学习算法，需要正确的正则化技巧才能实现数据高效利用。本文通过检验几种假设，如非稳态性、过度动作分布偏移和过拟合等，试图理解在样本高效的深度强化学习中主要的瓶颈。我们对DeepMind控制套件（DMC）任务进行了彻底的实证分析，以一种有控制、系统的方式展示了对转换的验证集的高时间差（TD）误差是严重影响深度强化学习算法性能的主要罪魁祸首，而先前的方法......(未完整翻译)

    Deep reinforcement learning algorithms that learn policies by trial-and-error must learn from limited amounts of data collected by actively interacting with the environment. While many prior works have shown that proper regularization techniques are crucial for enabling data-efficient RL, a general understanding of the bottlenecks in data-efficient RL has remained unclear. Consequently, it has been difficult to devise a universal technique that works well across all domains. In this paper, we attempt to understand the primary bottleneck in sample-efficient deep RL by examining several potential hypotheses such as non-stationarity, excessive action distribution shift, and overfitting. We perform thorough empirical analysis on state-based DeepMind control suite (DMC) tasks in a controlled and systematic way to show that high temporal-difference (TD) error on the validation set of transitions is the main culprit that severely affects the performance of deep RL algorithms, and prior method
    
[^15]: 基于角度的梯度下降动态学习率

    Angle based dynamic learning rate for gradient descent. (arXiv:2304.10457v1 [cs.LG])

    [http://arxiv.org/abs/2304.10457](http://arxiv.org/abs/2304.10457)

    该论文提出了一种基于角度计算的动态学习率方式，用于梯度下降方法中的自适应学习率选择，相较于传统方法，该方法在各个基准数据集上的可以获得更高的准确性，并被证明是可以收敛的。

    

    我们提出了一种新颖而简单的方法，用于获取分类任务上基于梯度下降方法的自适应学习率。我们使用当前梯度与新梯度之间的角度来选择自适应学习率，而不是传统的通过基于梯度项期望的衰减来选择自适应学习率的方法。我们使用了在当前梯度垂直方向上计算出来的新梯度，从而帮助我们根据角度历史记录得到更好的自适应学习率，因此与现有最先进的优化器相比，我们的方法具有更高的准确性。在广泛的基准数据集上使用ResNet、DenseNet、EfficientNet和VGG等流行的图像分类架构，我们发现我们的方法在大多数数据集上都获得了最高准确率。此外，我们证明了我们的方法是收敛的。

    In our work, we propose a novel yet simple approach to obtain an adaptive learning rate for gradient-based descent methods on classification tasks. Instead of the traditional approach of selecting adaptive learning rates via the decayed expectation of gradient-based terms, we use the angle between the current gradient and the new gradient: this new gradient is computed from the direction orthogonal to the current gradient, which further helps us in determining a better adaptive learning rate based on angle history, thereby, leading to relatively better accuracy compared to the existing state-of-the-art optimizers. On a wide variety of benchmark datasets with prominent image classification architectures such as ResNet, DenseNet, EfficientNet, and VGG, we find that our method leads to the highest accuracy in most of the datasets. Moreover, we prove that our method is convergent.
    
[^16]: 在多个扰动约束中证明对抗鲁棒性

    Certified Adversarial Robustness Within Multiple Perturbation Bounds. (arXiv:2304.10446v1 [cs.LG])

    [http://arxiv.org/abs/2304.10446](http://arxiv.org/abs/2304.10446)

    本论文提出了一种新的认证方法，可以在多个扰动边界下获得最佳结果，同时针对RS方法进行了优化，实现了最先进的认证性鲁棒性。

    

    随机平滑（RS）是一种众所周知的对抗性攻击抵御方法，它在推理过程中通过预测在输入上的随机噪声扰动下最可能的分类来创建平滑的分类器。虽然最初的工作集中在对$\ell_2$范数扰动的鲁棒性，使用从高斯分布中采样的噪声，但随后的工作表明，不同的噪声分布也可以导致对其他$\ell_p$范数边界的鲁棒性。一般来说，特定的噪声分布是防御给定$\ell_p$范数攻击的最佳选择。本文旨在提高多个扰动界限的认证性对抗性鲁棒性。为此，我们首先提出了一种新的\textit{认证方案}，有效地结合了使用不同噪声分布获得的证书，以在多个扰动界限下获得最佳结果。我们进一步提出了一种新的\textit{训练噪声分布}，以及一个新的\textit{可证明的边界}用于RS，该方法针对同时实现多个扰动界限的鲁棒性进行了优化。我们对各种基准测试进行的实验表明，在使用单个平滑的分类器的情况下，我们实现了最先进的多个扰动界限下的认证鲁棒性。

    Randomized smoothing (RS) is a well known certified defense against adversarial attacks, which creates a smoothed classifier by predicting the most likely class under random noise perturbations of inputs during inference. While initial work focused on robustness to $\ell_2$ norm perturbations using noise sampled from a Gaussian distribution, subsequent works have shown that different noise distributions can result in robustness to other $\ell_p$ norm bounds as well. In general, a specific noise distribution is optimal for defending against a given $\ell_p$ norm based attack. In this work, we aim to improve the certified adversarial robustness against multiple perturbation bounds simultaneously. Towards this, we firstly present a novel \textit{certification scheme}, that effectively combines the certificates obtained using different noise distributions to obtain optimal results against multiple perturbation bounds. We further propose a novel \textit{training noise distribution} along wi
    
[^17]: 格点量子色动力学预条件算子中的规范等变池化层

    Gauge-equivariant pooling layers for preconditioners in lattice QCD. (arXiv:2304.10438v1 [hep-lat])

    [http://arxiv.org/abs/2304.10438](http://arxiv.org/abs/2304.10438)

    本论文展示了在格点量子色动力学中使用规范等变池化层可以通过伽辽金构造粗网格规范场消除临界减慢现象。

    

    我们证明，规范等变的池化和反池化层在格点量子色动力学的多重网格预条件模型中可以与传统的限制和延拓层一样有效。这些层在粗网格上引入了规范自由度，允许在粗网格上使用明确的规范等变层。我们研究了粗粒度规范场的构造，并研究了它们在预条件器模型中的效率。我们展示了使用伽辽金构造粗网格规范场的组合多重网格神经网络消除了临界减慢现象。

    We demonstrate that gauge-equivariant pooling and unpooling layers can perform as well as traditional restriction and prolongation layers in multigrid preconditioner models for lattice QCD. These layers introduce a gauge degree of freedom on the coarse grid, allowing for the use of explicitly gauge-equivariant layers on the coarse grid. We investigate the construction of coarse-grid gauge fields and study their efficiency in the preconditioner model. We show that a combined multigrid neural network using a Galerkin construction for the coarse-grid gauge field eliminates critical slowing down.
    
[^18]: 基于注意机制的 softmax 回归

    Attention Scheme Inspired Softmax Regression. (arXiv:2304.10411v1 [cs.LG])

    [http://arxiv.org/abs/2304.10411](http://arxiv.org/abs/2304.10411)

    本研究从 softmax 单元中获得灵感，提出了注意机制 inspired 的 softmax 回归问题，该问题可用于控制潜在函数的进展和稳定性。

    

    大型语言模型（LLMs）已经给人类社会带来了巨大的变革。LLMs 的关键计算之一是 softmax 单元。这个操作在 LLMs 中非常重要，因为它允许模型在给定输入单词序列的情况下生成可能的下一个单词或短语的分布。这个分布然后用来选择最有可能的下一个单词或短语，基于模型分配的概率。softmax 单元在训练 LLMs 中起着至关重要的作用，因为它允许模型通过调整神经网络的权重和偏差从数据中学习。在凸优化领域，例如使用中心路径法解决线性规划，softmax 函数已经成为控制潜在函数的进展和稳定性的关键工具。在这项工作中，我们从 softmax 单元中获得灵感，定义了一个 softmax 回归问题。形式上讲，给定一个矩阵 $A \in \mathbb{R}^{n \times d}$ 和...

    Large language models (LLMs) have made transformed changes for human society. One of the key computation in LLMs is the softmax unit. This operation is important in LLMs because it allows the model to generate a distribution over possible next words or phrases, given a sequence of input words. This distribution is then used to select the most likely next word or phrase, based on the probabilities assigned by the model. The softmax unit plays a crucial role in training LLMs, as it allows the model to learn from the data by adjusting the weights and biases of the neural network.  In the area of convex optimization such as using central path method to solve linear programming. The softmax function has been used a crucial tool for controlling the progress and stability of potential function [Cohen, Lee and Song STOC 2019, Brand SODA 2020].  In this work, inspired the softmax unit, we define a softmax regression problem. Formally speaking, given a matrix $A \in \mathbb{R}^{n \times d}$ and 
    
[^19]: 基于图结构的多标签节点分类

    Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v1 [cs.LG])

    [http://arxiv.org/abs/2304.10398](http://arxiv.org/abs/2304.10398)

    该论文提出了一种新的图神经网络架构 MLGCN，用于处理多标签节点分类任务，并研究了多标签场景中同类偏好的语义。在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。

    

    图神经网络（GNN）在图中节点分类任务方面展示了最先进的改进。虽然这些进展在多类分类场景中得到了广泛的展示，但一个更普遍和现实的情况，在这种情况下，每个节点可能有多个标签，一直以来受到了很少关注。在进行关于多标签节点分类的重点研究的首要挑战是公开可用的多标签图数据集数量有限。因此，作为我们的第一个贡献，我们收集并发布了三个真实的生物数据集，并开发了一个多标签图生成器，以生成具有可调属性的数据集。虽然高标签相似性（高同类偏好）通常被归因于GNN的成功，但我们认为多标签场景并不遵循目前为多类场景定义的同类偏好和异类偏好的常规语义。作为我们的第二个贡献，我们除了为多标签场景定义同类偏好外，还开发了一种新颖的GNN体系结构，即MLGCN（多标签图卷积网络），来处理多标签节点分类任务。我们的实验表明，在各种基准测试中，MLGCN优于现有的最先进的多标签分类方法。

    Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, besides defining homophily for the multi-label scenario, we dev
    
[^20]: 学习随机过程的有条件生成模型

    Conditional Generative Models for Learning Stochastic Processes. (arXiv:2304.10382v1 [quant-ph])

    [http://arxiv.org/abs/2304.10382](http://arxiv.org/abs/2304.10382)

    提出了一种称为 C-qGAN 的框架，利用量子电路结构实现了有效的状态准备过程，可以利用该方法加速蒙特卡罗分析等算法，并将其应用于亚式期权衍生品定价的任务中。

    

    提出了一种学习多模态分布的框架，称为条件量子生成对抗网络（C-qGAN）。神经网络结构严格采用量子电路，因此被证明能够比当前的方法更有效地表示状态准备过程。这种方法有潜力加速蒙特卡罗分析等算法。特别地，在展示了网络在学习任务中的有效性后，将该技术应用于定价亚式期权衍生品，为未来研究其他路径相关期权打下基础。

    A framework to learn a multi-modal distribution is proposed, denoted as the Conditional Quantum Generative Adversarial Network (C-qGAN). The neural network structure is strictly within a quantum circuit and, as a consequence, is shown to represents a more efficient state preparation procedure than current methods. This methodology has the potential to speed-up algorithms, such as Monte Carlo analysis. In particular, after demonstrating the effectiveness of the network in the learning task, the technique is applied to price Asian option derivatives, providing the foundation for further research on other path-dependent options.
    
[^21]: 多智能体强化学习中有条件协同行为的可解释性研究

    Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning. (arXiv:2304.10375v1 [cs.LG])

    [http://arxiv.org/abs/2304.10375](http://arxiv.org/abs/2304.10375)

    提出了一种基于条件注意力的分布式注意力演员架构，利用显著性向量重用环境的条件状态来提高条件协同行为的可解释性和代理性能。

    

    我们提出了一种无模型强化学习架构，称为基于条件注意力(DA6-X)的分布式注意力演员架构，以提供更好的条件协同行为可解释性。其基本原理涉及重用显著性向量，该向量表示环境的条件状态，例如代理的全局位置。因此，具有嵌入其策略中的DA6-X灵活性的代理通过在决策过程中考虑条件状态中的附加信息表现出优越的性能。通过在对象收集游戏中将提出的方法与传统方法进行比较，实验评估了所提出方法的有效性。通过可视化DA6-X的注意权重，我们确认代理成功地学习了情境依赖性协同行为，通过正确识别各种条件状态，提高了代理的可解释性和性能。

    We propose a model-free reinforcement learning architecture, called distributed attentional actor architecture after conditional attention (DA6-X), to provide better interpretability of conditional coordinated behaviors. The underlying principle involves reusing the saliency vector, which represents the conditional states of the environment, such as the global position of agents. Hence, agents with DA6-X flexibility built into their policy exhibit superior performance by considering the additional information in the conditional states during the decision-making process. The effectiveness of the proposed method was experimentally evaluated by comparing it with conventional methods in an objects collection game. By visualizing the attention weights from DA6-X, we confirmed that agents successfully learn situation-dependent coordinated behaviors by correctly identifying various conditional states, leading to improved interpretability of agents along with superior performance.
    
[^22]: 可控的神经符号回归

    Controllable Neural Symbolic Regression. (arXiv:2304.10336v1 [cs.LG])

    [http://arxiv.org/abs/2304.10336](http://arxiv.org/abs/2304.10336)

    本文提出了一种称为带假设的神经符号回归（NSRwH）的方法，能够将用户定义的先验知识纳入到生成分析表达式的预测模型中，我们的方法允许用户指定目标表达式中使用的数学符号类型和范围，并对表达式的复杂性施加约束。

    

    在符号回归中，目标是找到一个分析表达式，能够在使用尽可能少的数学符号（例如运算符、变量和常量）的情况下精确地拟合实验数据。然而，由于可能表达式的组合空间很大，传统进化算法很难在合理的时间内找到正确的表达式。为了解决这个问题，研究人员开发了一种名为神经符号回归（NSR）算法，能够快速识别数据中的模式并生成分析表达式。然而，这些方法目前还缺乏将用户定义的先验知识纳入其中的能力，而这种能力在自然科学和工程领域通常是必需的。为了克服这种限制，我们提出了一种新的神经符号回归方法，名为带假设的神经符号回归（NSRwH），它允许显式地将关于真实表达式期望结构的假设纳入预测模型中。

    In symbolic regression, the goal is to find an analytical expression that accurately fits experimental data with the minimal use of mathematical symbols such as operators, variables, and constants. However, the combinatorial space of possible expressions can make it challenging for traditional evolutionary algorithms to find the correct expression in a reasonable amount of time. To address this issue, Neural Symbolic Regression (NSR) algorithms have been developed that can quickly identify patterns in the data and generate analytical expressions. However, these methods, in their current form, lack the capability to incorporate user-defined prior knowledge, which is often required in natural sciences and engineering fields. To overcome this limitation, we propose a novel neural symbolic regression method, named Neural Symbolic Regression with Hypothesis (NSRwH) that enables the explicit incorporation of assumptions about the expected structure of the ground-truth expression into the pre
    
[^23]: 利用图神经网络从实际网络配置中学习蜂窝网络覆盖

    Learning Cellular Coverage from Real Network Configurations using GNNs. (arXiv:2304.10328v1 [cs.NI])

    [http://arxiv.org/abs/2304.10328](http://arxiv.org/abs/2304.10328)

    本论文提出了一种新颖的训练框架，在图形表示中表述学习蜂窝网络覆盖的任务，应用了最新的图神经网络。该框架既可以生成多个KPI的质量单元格配置嵌入，同时能够推广到大型（面向区域）场景，给出非常少的标记单元格。

    

    蜂窝覆盖质量估计是自组织网络中的关键任务。在实际场景中，由于在网络设计和优化期间很少提供确定的基础事实，深度学习支持的覆盖质量估计方法无法扩展到大面积，此外，他们无法生成表达能力强的嵌入以充分捕捉单元格配置的变化。为了应对这一挑战，我们在图形表示中表述该任务，以便我们可以应用表现出色的最新图神经网络。我们提出了一种新颖的训练框架，既可以生成多个KPI的质量单元格配置嵌入，同时我们显示它能够推广到大型（面向区域）场景，给出非常少的标记单元格。我们证明，我们的框架与使用大量标记样本进行训练的模型具有可比较的准确性。

    Cellular coverage quality estimation has been a critical task for self-organized networks. In real-world scenarios, deep-learning-powered coverage quality estimation methods cannot scale up to large areas due to little ground truth can be provided during network design & optimization. In addition they fall short in produce expressive embeddings to adequately capture the variations of the cells' configurations. To deal with this challenge, we formulate the task in a graph representation and so that we can apply state-of-the-art graph neural networks, that show exemplary performance. We propose a novel training framework that can both produce quality cell configuration embeddings for estimating multiple KPIs, while we show it is capable of generalising to large (area-wide) scenarios given very few labeled cells. We show that our framework yields comparable accuracy with models that have been trained using massively labeled samples.
    
[^24]: GAN的自适应共识优化方法

    Adaptive Consensus Optimization Method for GANs. (arXiv:2304.10317v1 [cs.LG])

    [http://arxiv.org/abs/2304.10317](http://arxiv.org/abs/2304.10317)

    本文提出一种基于ADAM和RMSprop的二阶梯度优化方法，用于训练GAN，在不需要解决线性系统或混合二阶导数项的情况下取得与其他方法相比更快的相似精度，产生了更好或可比的Inception分数和图像质量，并与一阶方法相比产生了显著更好的Inception分数。

    

    本文提出了一种基于ADAM和RMSprop的二阶梯度优化方法，用于训练生成对抗网络。该方法在获得相似精度时比其他著名的二阶方法更快。与最先进的方法不同，它不需要解决线性系统，也不需要额外的混合二阶导数项。我们推导出了对应于所提出的方法的固定点迭代，并证明了所提出的方法的收敛性。与其他最新提出的最先进的二阶方法相比，所提出的方法产生了更好或可比的Inception分数，以及与其他方法相当的图像质量。与一阶方法（如ADAM）相比，它产生了显著更好的Inception分数。该方法在流行的图像生成任务数据集，如FFHQ、LSUN、CIFAR10、MNIST和Fashion MNIST上进行了比较和验证\footnote{已被IJCNN 2023接受}。代码：\url{https://github.com/misterpawan/acom}

    We propose a second order gradient based method with ADAM and RMSprop for the training of generative adversarial networks. The proposed method is fastest to obtain similar accuracy when compared to prominent second order methods. Unlike state-of-the-art recent methods, it does not require solving a linear system, or it does not require additional mixed second derivative terms. We derive the fixed point iteration corresponding to proposed method, and show that the proposed method is convergent. The proposed method produces better or comparable inception scores, and comparable quality of images compared to other recently proposed state-of-the-art second order methods. Compared to first order methods such as ADAM, it produces significantly better inception scores. The proposed method is compared and validated on popular datasets such as FFHQ, LSUN, CIFAR10, MNIST, and Fashion MNIST for image generation tasks\footnote{Accepted in IJCNN 2023}. Codes: \url{https://github.com/misterpawan/acom
    
[^25]: 采用自监督与视觉相关的预训练方法进行电影票房预测

    Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining. (arXiv:2304.10311v1 [cs.MM])

    [http://arxiv.org/abs/2304.10311](http://arxiv.org/abs/2304.10311)

    本研究采用自监督和视觉相关的预训练方法对电影票房进行预测，对具有内容关键词的电影表现出了显著的性能提高。

    

    电影投资风险较高，因为电影票房收入呈现长尾和双峰分布。准确预测电影票房收入可以减轻不确定性并鼓励投资。然而，学习对演员、导演和用户生成内容相关关键词的有效表示仍然是一个具有挑战性的开放问题。在本研究中，我们调查了自监督预训练的效果，并提出了从电影海报中的对象开始实现视觉相关关键词作为预训练目标。在一个包含35,794部电影的大数据集上的实验表明，自监督训练和视觉相关关键词的预训练都会带来显著的好处。特别是，在具有内容关键词的电影上进行视觉相关的预训练，相对于具有相同架构的微调BERT模型，可以实现14.5%的性能提高。

    Investments in movie production are associated with a high level of risk as movie revenues have long-tailed and bimodal distributions. Accurate prediction of box-office revenue may mitigate the uncertainty and encourage investment. However, learning effective representations for actors, directors, and user-generated content-related keywords remains a challenging open problem. In this work, we investigate the effects of self-supervised pretraining and propose visual grounding of content keywords in objects from movie posters as a pertaining objective. Experiments on a large dataset of 35,794 movies demonstrate significant benefits of self-supervised training and visual grounding. In particular, visual grounding pretraining substantially improves learning on movies with content keywords and achieves 14.5% relative performance gains compared to a finetuned BERT model with identical architecture.
    
[^26]: 停止多臂赌博机模型的最优激活。

    Optimal Activation of Halting Multi-Armed Bandit Models. (arXiv:2304.10302v1 [stat.ML])

    [http://arxiv.org/abs/2304.10302](http://arxiv.org/abs/2304.10302)

    本文研究了新型的动态分配问题——停止赌博机模型，提出了对于经典的Gittins指数分解结果和最新结果的新证明。

    

    本文研究了一种新型的动态分配问题——停止赌博机模型。作为应用，我们获得了对于经典的Gittins指数分解结果和作者在“普遍折旧和承诺下的多臂赌博机”的最新结果的新证明。

    We study new types of dynamic allocation problems the {\sl Halting Bandit} models. As an application, we obtain new proofs for the classic Gittins index decomposition result and recent results of the authors in `Multi-armed bandits under general depreciation and commitment.'
    
[^27]: SARF: 利用同义关系辅助的自监督学习以进行少样本关系推理

    SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning. (arXiv:2304.10297v1 [cs.LG])

    [http://arxiv.org/abs/2304.10297](http://arxiv.org/abs/2304.10297)

    本文提出了一个新的自监督学习模型SARF，通过利用同义关系辅助提高了少样本关系推理的泛化性能和准确性，经过实验验证超越了目前最先进的方法。

    

    知识图谱中针对少量数据、长尾的关系推理（FS-KGR）近年来因其实用性而备受关注。之前的方法需要手动构建元关系集来预训练，导致了大量的劳动成本。自监督学习（SSL）被视为解决这个问题的方案，但在FS-KGR任务中仍处于早期阶段。此外，大多数现有方法忽略了利用与目标数据稀少关系具有类似上下文语义的同义关系（AR）的有益信息。因此，我们提出了一种使用同义关系辅助的自监督学习模型，命名为SARF。具体地，我们的模型设计了四个主要组成部分，即SSL推理模块、AR辅助机制、融合模块和评分函数。我们首先以生成式的方式生成共现模式的表示，同时设计AR辅助机制来捕捉数据稀少关系的相似上下文语义。然后，这两个表示被融合起来生成综合特征表示。最后，我们采用评分函数来预测基于此特征表示的目标关系。两个广泛使用的基准测试上的实验结果表明SARF优于最先进的方法。

    Few-shot relation reasoning on knowledge graphs (FS-KGR) aims to infer long-tail data-poor relations, which has drawn increasing attention these years due to its practicalities. The pre-training of previous methods needs to manually construct the meta-relation set, leading to numerous labor costs. Self-supervised learning (SSL) is treated as a solution to tackle the issue, but still at an early stage for FS-KGR task. Moreover, most of the existing methods ignore leveraging the beneficial information from aliasing relations (AR), i.e., data-rich relations with similar contextual semantics to the target data-poor relation. Therefore, we proposed a novel Self-Supervised Learning model by leveraging Aliasing Relations to assist FS-KGR, termed SARF. Concretely, four main components are designed in our model, i.e., SSL reasoning module, AR-assisted mechanism, fusion module, and scoring function. We first generate the representation of the co-occurrence patterns in a generative manner. Meanwh
    
[^28]: OptoGPT：一种用于光学多层薄膜结构反向设计的基础模型

    OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer Thin Film Structures. (arXiv:2304.10294v1 [physics.optics])

    [http://arxiv.org/abs/2304.10294](http://arxiv.org/abs/2304.10294)

    OptoGPT是一种基于大型数据集训练的仅包含解码器的Transformer模型，可以自主全局设计探索，同时选择材料和厚度，用于光学多层薄膜结构反向设计。

    

    基础模型是大型机器学习模型，一旦训练完成即可解决各种下游任务，在自然语言处理、计算机视觉和强化学习等领域引领研究趋势。然而，目前还没有适用于光学多层薄膜结构反向设计的基础模型。当前的反向设计算法要么不能探索全局设计空间，要么计算效率低下。为了填补这一空白，我们提出了Opto Generative Pretrained Transformer（OptoGPT）。OptoGPT是一个仅包含解码器的Transformer，可以根据特定的频谱目标自回归地生成设计。通过训练一组大型数据集（1000万个设计），我们的模型展现了卓越的能力: 1）自主全局设计探索，通过确定层数（高达20层），同时选择每个层的材料（高达18种不同类型）和厚度；2）高效的结构颜色设计，吸收器，滤波器，分布反射镜。

    Foundation models are large machine learning models that can tackle various downstream tasks once trained on diverse and large-scale data, leading research trends in natural language processing, computer vision, and reinforcement learning. However, no foundation model exists for optical multilayer thin film structure inverse design. Current inverse design algorithms either fail to explore the global design space or suffer from low computational efficiency. To bridge this gap, we propose the Opto Generative Pretrained Transformer (OptoGPT). OptoGPT is a decoder-only transformer that auto-regressively generates designs based on specific spectrum targets. Trained on a large dataset of 10 million designs, our model demonstrates remarkable capabilities: 1) autonomous global design exploration by determining the number of layers (up to 20) while selecting the material (up to 18 distinct types) and thickness at each layer, 2) efficient designs for structural color, absorbers, filters, distrib
    
[^29]: 辅助强化学习应用于设定点控制问题

    Aiding reinforcement learning for set point control. (arXiv:2304.10289v1 [eess.SY])

    [http://arxiv.org/abs/2304.10289](http://arxiv.org/abs/2304.10289)

    本文提出了一种方法，将强化学习与简单的引导反馈控制器相结合，以实现设定点控制问题的更好激励，以提高强化学习控制器的收敛性能，克服了强化学习在实际控制环境中的不足。

    

    尽管强化学习已经在很多领域获得了重要进展，但目前的算法仍然不能很好地解决看似简单的设定点反馈控制问题。其中一个原因是，学得的控制器可能无法在初始阶段充分地激励系统动力学，因此需要很长时间才能获得足够信息用于学习出较好的控制策略。本文的贡献在于将强化学习和简单的引导反馈控制器（例如比例控制器）相结合，从而在设定点控制问题中实现更好的激励，显著提高了强化学习控制器的收敛性能。这在快速准确的实时控制环境中非常重要。所提出的方法经过模拟和实际的双水箱系统测试，取得了良好的结果。

    While reinforcement learning has made great improvements, state-of-the-art algorithms can still struggle with seemingly simple set-point feedback control problems. One reason for this is that the learned controller may not be able to excite the system dynamics well enough initially, and therefore it can take a long time to get data that is informative enough to learn for good control. The paper contributes by augmentation of reinforcement learning with a simple guiding feedback controller, for example, a proportional controller. The key advantage in set point control is a much improved excitation that improves the convergence properties of the reinforcement learning controller significantly. This can be very important in real-world control where quick and accurate convergence is needed. The proposed method is evaluated with simulation and on a real-world double tank process with promising results.
    
[^30]: 一种估算并解释分类器不确定性的元启发式方法

    A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty. (arXiv:2304.10284v1 [cs.LG])

    [http://arxiv.org/abs/2304.10284](http://arxiv.org/abs/2304.10284)

    本文提出了一种元启发式方法，它可以以人类和机器学习决策都互相关联的因素来表征一个实例的复杂性，以估计分类错误的风险。

    

    信任是影响机器学习模型采用的重要因素。定性研究表明，终端用户，特别是在医疗领域，需要能够在决策时表达不确定性的模型，以使用户知道何时忽略模型的建议。然而，现有的量化决策不确定性的方法不是模型无关的，就是依赖于不容易让普通人或终端用户理解的复杂统计推导，这使它们在解释模型的决策过程时不太有用。本文提出了一组类独立的元启发式方法，可以以人类和机器学习决策都互相关联的因素来表征一个实例的复杂性。这些度量被集成到一个元学习框架中，该框架估计了分类错误风险。所提出的框架在鉴别那些有可能被错误分类的实例方面，表现优于预测概率。

    Trust is a crucial factor affecting the adoption of machine learning (ML) models. Qualitative studies have revealed that end-users, particularly in the medical domain, need models that can express their uncertainty in decision-making allowing users to know when to ignore the model's recommendations. However, existing approaches for quantifying decision-making uncertainty are not model-agnostic, or they rely on complex statistical derivations that are not easily understood by laypersons or end-users, making them less useful for explaining the model's decision-making process. This work proposes a set of class-independent meta-heuristics that can characterize the complexity of an instance in terms of factors are mutually relevant to both human and ML decision-making. The measures are integrated into a meta-learning framework that estimates the risk of misclassification. The proposed framework outperformed predicted probabilities in identifying instances at risk of being misclassified. The
    
[^31]: 基于强化学习的鲁棒非线性设定点控制

    Robust nonlinear set-point control with reinforcement learning. (arXiv:2304.10277v1 [eess.SY])

    [http://arxiv.org/abs/2304.10277](http://arxiv.org/abs/2304.10277)

    本文提出了三种思想来改进强化学习方法，即利用先前反馈控制器来帮助振幅探索，使用积分误差，对模型集合进行训练。这些想法可提高训练效率，使得训练出来的设定点控制器更加鲁棒，可以直接用于真实的非线性系统中。

    

    最近，强化学习在非线性控制问题中受到了越来越多的关注。然而，标准的强化学习算法经常在看似简单的设定点控制问题上遇到困难。本文认为三个思想可改进强化学习方法，即 1）利用先前反馈控制器来帮助振幅探索。2）使用积分误差。3）对模型集合进行训练。这些思想共同导致更高效的训练，以及一个经过训练的设定点控制器，它对建模误差更加鲁棒，因此可以直接应用于真实的非线性系统中。该论文通过实验支持了这一论断，实验使用了真实世界的非线性级联罐进程和模拟的强非线性 pH 控制系统。

    There has recently been an increased interest in reinforcement learning for nonlinear control problems. However standard reinforcement learning algorithms can often struggle even on seemingly simple set-point control problems. This paper argues that three ideas can improve reinforcement learning methods even for highly nonlinear set-point control problems: 1) Make use of a prior feedback controller to aid amplitude exploration. 2) Use integrated errors. 3) Train on model ensembles. Together these ideas lead to more efficient training, and a trained set-point controller that is more robust to modelling errors and thus can be directly deployed to real-world nonlinear systems. The claim is supported by experiments with a real-world nonlinear cascaded tank process and a simulated strongly nonlinear pH-control system.
    
[^32]: 强化学习中的观察器-反馈-前馈控制结构

    Observer-Feedback-Feedforward Controller Structures in Reinforcement Learning. (arXiv:2304.10276v1 [eess.SY])

    [http://arxiv.org/abs/2304.10276](http://arxiv.org/abs/2304.10276)

    本文提出了使用结构化神经网络进行基于强化学习的非线性自适应控制，将观察器和控制器分为不同的神经网络，通过降低计算复杂度和使控制器具有可理解的结构，显著加快了训练速度。

    

    本文提出了使用结构化神经网络进行基于强化学习的非线性自适应控制。重点是部分可观测系统，使用单独的神经网络进行状态和前馈观察器以及状态反馈和前馈控制。观察器动态由循环神经网络模拟，而控制器则使用标准网络。如本文所述，这导致了观察器动态分离为循环神经网络部分和状态反馈到反馈和前馈网络部分。这种结构化方法降低了计算复杂度，并使基于强化学习的控制器与使用单个神经网络相比具有“可理解的”结构。仿真结果表明，所提出的结构具有训练速度显著加快的优势。本文还提出了两种包括前馈结构的方法，一种与状态前馈相关

    The paper proposes the use of structured neural networks for reinforcement learning based nonlinear adaptive control. The focus is on partially observable systems, with separate neural networks for the state and feedforward observer and the state feedback and feedforward controller. The observer dynamics are modelled by recurrent neural networks while a standard network is used for the controller. As discussed in the paper, this leads to a separation of the observer dynamics to the recurrent neural network part, and the state feedback to the feedback and feedforward network. The structured approach reduces the computational complexity and gives the reinforcement learning based controller an {\em understandable} structure as compared to when one single neural network is used. As shown by simulation the proposed structure has the additional and main advantage that the training becomes significantly faster. Two ways to include feedforward structure are presented, one related to state feed
    
[^33]: 通过领域自适应模仿学习动态系统的代表性轨迹

    Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation. (arXiv:2304.10260v1 [cs.LG])

    [http://arxiv.org/abs/2304.10260](http://arxiv.org/abs/2304.10260)

    该论文提出了一种使用循环一致性生成对抗方法进行领域自适应轨迹模仿的深度强化学习代理DATI，可以学习动态系统中的代表性轨迹，识别交通中异常运动模式，并在各种合成的参考轨迹家族上表现优异。

    

    领域适应的轨迹模仿是一种某些捕食动物为生存学习的技能，通过将动态信息从一个域（它们的速度和转向方向）映射到不同的域（移动猎物的当前位置）。具有此技能的智能代理可以用于各种任务，包括学习模仿代表性轨迹后识别交通中的异常运动。为此，我们提出DATI，一种使用循环一致性生成对抗方法进行领域自适应轨迹模仿的深度强化学习代理。我们在各种合成的参考轨迹家族上的实验表明，DATI在模仿学习和最优控制中的基准方法在这个设置中表现更好，保持相同的每一项任务的超参数。通过发现海上交通的异常运动模式，它在真实世界场景中的泛化能力已得到证明，为深度学习在领域适应轨迹模仿中的应用开辟了道路。

    Domain-adaptive trajectory imitation is a skill that some predators learn for survival, by mapping dynamic information from one domain (their speed and steering direction) to a different domain (current position of the moving prey). An intelligent agent with this skill could be exploited for a diversity of tasks, including the recognition of abnormal motion in traffic once it has learned to imitate representative trajectories. Towards this direction, we propose DATI, a deep reinforcement learning agent designed for domain-adaptive trajectory imitation using a cycle-consistent generative adversarial method. Our experiments on a variety of synthetic families of reference trajectories show that DATI outperforms baseline methods for imitation learning and optimal control in this setting, keeping the same per-task hyperparameters. Its generalization to a real-world scenario is shown through the discovery of abnormal motion patterns in maritime traffic, opening the door for the use of deep r
    
[^34]: 使用Mediapipe Holistic识别印度手语

    Indian Sign Language Recognition Using Mediapipe Holistic. (arXiv:2304.10256v1 [cs.CV])

    [http://arxiv.org/abs/2304.10256](http://arxiv.org/abs/2304.10256)

    本研究致力于创建一个强大的手语识别系统，将印度手语转换为文本或语音。CNN模型对于静态手语的识别表现较好，但是通过监控动态手势，LSTM模型的表现更好。

    

    聋哑人在日常生活中面临着沟通障碍。他们听不到声音，与不懂手语的人沟通困难，从而在教育、职业和社交等方面遇到困难。技术可以提供替代的沟通渠道，发挥关键作用。一种有助于聋人与听力正常人进行沟通的技术是手语识别。我们将创建一个强大的手语识别系统，将印度手语转换为文本或语音。我们将评估所提出的系统，并比较CNN和LSTM模型。由于存在静态和动态手语，因此需要一个强大的模型来区分它们。在本研究中，我们发现CNN模型比LSTM模型更能捕捉字母和字符，更适用于静态手语的识别，但通过监控手势，它又超越了CNN。

    Deaf individuals confront significant communication obstacles on a daily basis. Their inability to hear makes it difficult for them to communicate with those who do not understand sign language. Moreover, it presents difficulties in educational, occupational, and social contexts. By providing alternative communication channels, technology can play a crucial role in overcoming these obstacles. One such technology that can facilitate communication between deaf and hearing individuals is sign language recognition. We will create a robust system for sign language recognition in order to convert Indian Sign Language to text or speech. We will evaluate the proposed system and compare CNN and LSTM models. Since there are both static and gesture sign languages, a robust model is required to distinguish between them. In this study, we discovered that a CNN model captures letters and characters for recognition of static sign language better than an LSTM model, but it outperforms CNN by monitorin
    
[^35]: PED-ANOVA: 在任意子空间中高效量化超参数重要性

    PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])

    [http://arxiv.org/abs/2304.10255](http://arxiv.org/abs/2304.10255)

    PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。

    

    深度学习中超参数优化的流行使得好的超参数空间设计对于训练强模型至关重要，而好的超参数空间设计又严重依赖于了解不同超参数的作用。这激发了关于超参数重要性的研究，例如使用功能方差分析 (f-ANOVA) 的流行方法。然而，原始的 f-ANOVA 公式不适用于算法设计师最相关的子空间，例如由最佳性能定义的子空间。为了解决这个问题，我们推导了一个新的针对任意子空间的 f-ANOVA 公式，并提出了一个算法，使用 Pearson 散度 (PED) 实现超参数重要性的闭式计算。我们证明，这个新算法，称为 PED-ANOVA，能够成功地识别不同子空间中重要的超参数，同时计算效率极高。

    The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
    
[^36]: 基于数据增强视角的扩散模型与检索研究

    A data augmentation perspective on diffusion models and retrieval. (arXiv:2304.10253v1 [cs.CV])

    [http://arxiv.org/abs/2304.10253](http://arxiv.org/abs/2304.10253)

    本研究从数据增强的角度出发，评估了扩散模型图像生成方法，并发现仅使用扩散模型的训练数据可用于最强的数据增强。

    

    扩散模型在从文本查询中生成逼真图像方面表现优异。因此，许多方法已经被提出，以利用这些生成能力来增强用于下游任务（如分类）的训练数据集。然而，扩散模型本身是在大型嘈杂的监督注释数据集上进行训练的。目前尚不清楚，扩散模型在增强过程中使用附加数据是否能够提高下游性能。我们对现有扩散模型图像生成方法进行系统评估，并研究新的扩展方法，以评估其对数据增强的效益。虽然我们发现，针对目标数据个性化的扩散模型优于更简单的提示策略，但我们也表明，仅使用扩散模型的训练数据，通过简单的最近邻检索过程，可以导致更强的下游性能。

    Diffusion models excel at generating photorealistic images from text-queries. Naturally, many approaches have been proposed to use these generative abilities to augment training datasets for downstream tasks, such as classification. However, diffusion models are themselves trained on large noisily supervised, but nonetheless, annotated datasets. It is an open question whether the generalization capabilities of diffusion models beyond using the additional data of the pre-training process for augmentation lead to improved downstream performance. We perform a systematic evaluation of existing methods to generate images from diffusion models and study new extensions to assess their benefit for data augmentation. While we find that personalizing diffusion models towards the target data outperforms simpler prompting strategies, we also show that using the training data of the diffusion model alone, via a simple nearest neighbor retrieval procedure, leads to even stronger downstream performan
    
[^37]: 用机器学习代替降水集合预测系统

    Towards replacing precipitation ensemble predictions systems using machine learning. (arXiv:2304.10251v1 [physics.ao-ph])

    [http://arxiv.org/abs/2304.10251](http://arxiv.org/abs/2304.10251)

    该论文提出了一种用生成对抗网络生成高分辨率降水集合预测的方法，避免了高分辨率训练数据的需求和计算成本的限制。

    

    与其他气象场相比，降水预测不够准确，因为影响降水分布和强度的几个关键过程发生在全球天气预报模型的可解决尺度以下。这要求使用更高分辨率的模拟。为了生成与预测相关的不确定性预测，同时运行模拟的集合。然而，计算成本是一个限制因素。因此，替代从模拟生成集合系统的趋势是使用神经网络。不幸的是，高分辨率集合运行的数据不可用。我们提出了一种新方法，可以生成高分辨率降水的集合气象预测，而不需要高分辨率训练数据。该方法使用生成对抗网络学习复杂的降水模式，并产生多样化、真实的降水场，从而产生真实的降水集合。

    Precipitation forecasts are less accurate compared to other meteorological fields because several key processes affecting precipitation distribution and intensity occur below the resolved scale of global weather prediction models. This requires to use higher resolution simulations. To generate an uncertainty prediction associated with the forecast, ensembles of simulations are run simultaneously. However, the computational cost is a limiting factor here. Thus, instead of generating an ensemble system from simulations there is a trend of using neural networks. Unfortunately the data for high resolution ensemble runs is not available. We propose a new approach to generating ensemble weather predictions for high-resolution precipitation without requiring high-resolution training data. The method uses generative adversarial networks to learn the complex patterns of precipitation and produce diverse and realistic precipitation fields, allowing to generate realistic precipitation ensemble me
    
[^38]: 利用文本-图像对比模型自动检测在线虚假信息的能力

    Harnessing the Power of Text-image Contrastive Models for Automatic Detection of Online Misinformation. (arXiv:2304.10249v1 [cs.LG])

    [http://arxiv.org/abs/2304.10249](http://arxiv.org/abs/2304.10249)

    本文研究了一种基于对比学习的虚假信息识别方法，在文本-图像对中区分正宗和虚假新闻，具有很好的效果。

    

    随着社交媒体网站的日益普及，新闻文章的数量在网上迅速传播，因此可能存在大规模的潜在虚假信息。虽然大量的研究已经应用监督机器学习方法来检测这样的内容，但缺乏黄金标准训练数据已经阻碍了发展。分析单个数据格式，即虚假的文本描述或虚假的图像，是目前研究的主流方向。然而，在现实世界的情况下，虚假信息通常以文本-图像对的形式形成，新闻文章/新闻标题被描述为文本内容，并通常跟随相关的图像。鉴于对没有标签数据进行学习特征的强大能力，对比学习作为自学习方法已经出现并在计算机视觉中取得了成功。本文旨在探索对比学习在虚假信息识别领域的应用。我们开发了一个文本-图像对比模型，该模型学习区分正宗和虚假新闻中的文本和图像之间的关系，并将其用于自动检测在线虚假信息。我们的实验结果表明，这种提出的模型在使用文本-图像对检测假新闻方面优于现有的方法。

    As growing usage of social media websites in the recent decades, the amount of news articles spreading online rapidly, resulting in an unprecedented scale of potentially fraudulent information. Although a plenty of studies have applied the supervised machine learning approaches to detect such content, the lack of gold standard training data has hindered the development. Analysing the single data format, either fake text description or fake image, is the mainstream direction for the current research. However, the misinformation in real-world scenario is commonly formed as a text-image pair where the news article/news title is described as text content, and usually followed by the related image. Given the strong ability of learning features without labelled data, contrastive learning, as a self-learning approach, has emerged and achieved success on the computer vision. In this paper, our goal is to explore the constrastive learning in the domain of misinformation identification. We devel
    
[^39]: 大规模对称尖峰张量上的Hotelling缩减算法

    Hotelling Deflation on Large Symmetric Spiked Tensors. (arXiv:2304.10248v1 [stat.ML])

    [http://arxiv.org/abs/2304.10248](http://arxiv.org/abs/2304.10248)

    本文研究了缩减算法在大规模对称尖峰张量上的应用，提供了在存在非平凡相关性情况下的精确表现，可用于设计更有效的信号估计方法。

    

    本文研究了当应用于估计受加性高斯噪声污染的大张量中包含的低秩对称尖峰时的缩减算法。具体而言，我们在假定尖峰分量存在非平凡（固定）相关性的情况下，提供了对缩减算法在大维情况下表现的精确刻画，其中包括通过连续的秩-1逼近获得的向量的对齐情况及其估计的权重。我们的分析可让人们理解噪声干扰下的缩减机制，并可用于设计更有效的信号估计方法。

    This paper studies the deflation algorithm when applied to estimate a low-rank symmetric spike contained in a large tensor corrupted by additive Gaussian noise. Specifically, we provide a precise characterization of the large-dimensional performance of deflation in terms of the alignments of the vectors obtained by successive rank-1 approximation and of their estimated weights, assuming non-trivial (fixed) correlations among spike components. Our analysis allows an understanding of the deflation mechanism in the presence of noise and can be exploited for designing more efficient signal estimation methods.
    
[^40]: 滤波感知的模型预测控制

    Filter-Aware Model-Predictive Control. (arXiv:2304.10246v1 [cs.LG])

    [http://arxiv.org/abs/2304.10246](http://arxiv.org/abs/2304.10246)

    本文提出了一种新的模型预测控制方法，滤波感知MPC，基于状态估计器的追踪度惩罚信息损失，可以快速规划并在多方面实验中展示了其有效性。

    

    部分可观测问题在减少成本和收集信息之间存在平衡。可以通过在信念空间中进行规划来最优地解决这些问题，但这通常是极其昂贵的。与之相反，模型预测控制 (MPC) 采用使用状态估计器形成关于状态的信仰，然后在状态空间中进行规划的替代方法。这在规划期间忽略了潜在的未来观察结果，因此不能积极增加或保持其自身状态估计的准确性。我们在信念空间中进行规划和完全忽略其动态之间找到了一个折衷方案，只考虑其未来的准确性。我们的方法，滤波感知MPC，通过我们所谓的”追踪度“，即状态估计器的预期误差，惩罚信息损失。我们展示了基于模型的模拟可以将“追踪度”压缩成一个神经网络，从而实现快速规划。在涉及视觉导航，现实的日常环境和一个双连杆机器人的实验中，我们证明了该方法的有效性。

    Partially-observable problems pose a trade-off between reducing costs and gathering information. They can be solved optimally by planning in belief space, but that is often prohibitively expensive. Model-predictive control (MPC) takes the alternative approach of using a state estimator to form a belief over the state, and then plan in state space. This ignores potential future observations during planning and, as a result, cannot actively increase or preserve the certainty of its own state estimate. We find a middle-ground between planning in belief space and completely ignoring its dynamics by only reasoning about its future accuracy. Our approach, filter-aware MPC, penalises the loss of information by what we call "trackability", the expected error of the state estimator. We show that model-based simulation allows condensing trackability into a neural network, which allows fast planning. In experiments involving visual navigation, realistic every-day environments and a two-link robot
    
[^41]: 傅立叶神经算子代理模型用于预测3D地震波传播

    Fourier Neural Operator Surrogate Model to Predict 3D Seismic Waves Propagation. (arXiv:2304.10242v1 [cs.LG])

    [http://arxiv.org/abs/2304.10242](http://arxiv.org/abs/2304.10242)

    本文提出一种基于傅立叶神经算子的代理模型，可用于预测3D地震波传播的地面运动时间序列。该模型在预测精度和计算效率方面明显优于传统机器学习模型。

    

    随着神经算子的兴起，科学机器学习提供了量化与高保真数值模拟相关的不确定性的新解决方案。传统的神经网络，如卷积神经网络（CNN）或物理信息神经网络（PINN），受到预定义配置的局限，并且只能预测解的解。使用神经算子，可以学习具有不同参数的偏微分方程的一般解，例如弹性波方程。地震学中应用神经算子的应用很少，都仅限于二维设置，尽管三维效应的重要性是众所周知的。在这项工作中，我们应用傅立叶神经算子（FNO）来预测来自3D地质描述的地面运动时间序列。我们使用高保真度模拟代码SEM3D建立了一个由30,000个不同地质构造生成的地面运动的广泛数据库。借助这个数据库，我们训练了一个FNO代理模型，可以准确预测由新地质构造引起的地面运动时间序列。我们的模型在预测精度和计算效率方面明显优于传统的机器学习模型。

    With the recent rise of neural operators, scientific machine learning offers new solutions to quantify uncertainties associated with high-fidelity numerical simulations. Traditional neural networks, such as Convolutional Neural Networks (CNN) or Physics-Informed Neural Networks (PINN), are restricted to the prediction of solutions in a predefined configuration. With neural operators, one can learn the general solution of Partial Differential Equations, such as the elastic wave equation, with varying parameters. There have been very few applications of neural operators in seismology. All of them were limited to two-dimensional settings, although the importance of three-dimensional (3D) effects is well known.  In this work, we apply the Fourier Neural Operator (FNO) to predict ground motion time series from a 3D geological description. We used a high-fidelity simulation code, SEM3D, to build an extensive database of ground motions generated by 30,000 different geologies. With this databa
    
[^42]: 通过对比学习实现乳腺X线摄影图像分析的域泛化

    Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])

    [http://arxiv.org/abs/2304.10226](http://arxiv.org/abs/2304.10226)

    研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。

    

    乳腺X线摄影图像分析是医学影像学领域的一个基本问题，近年来，随着深度学习的不断发展，该领域取得了显著的进展。然而，构建深度学习模型需要大量的具有多样性的图像数据，尤其是对于不同厂商的图像风格，这往往需要非常庞大的样本集。因此，为了提高深度学习模型泛化到不同厂商图像的能力，研究者提出了一种基于对比学习的策略。

    Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
    
[^43]: 人工智能革命对资产管理的影响

    The impact of the AI revolution on asset management. (arXiv:2304.10212v1 [q-fin.GN])

    [http://arxiv.org/abs/2304.10212](http://arxiv.org/abs/2304.10212)

    本论文分享了作者关于人工智能可能对资产管理产生影响的观点，同时提供了一个衡量特定基金是否真正开发了AI的简单标准。

    

    深度学习的最新进展使得机器具备了杰出的能力：它们可以阅读和理解自由流动的文本，与人类进行推理和交涉，翻译不同语言的文本，学习如何做出最优决策等等。如今，机器已经在癌症检测、蛋白质结构预测、药物设计、核聚变反应堆控制等方面实现了革命性突破。虽然这些能力仍处于萌芽阶段，但它们在持续完善和应用中的技术影响几乎将在人类活动的几乎所有社会和经济领域中发挥作用，这是我们以前从未见过的。本文将分享我的观点，即人工智能如何影响资产管理，我将提供一个思维框架，使读者可以用一个简单的标准来评估一个特定基金是否真正开发

    Recent progress in deep learning, a special form of machine learning, has led to remarkable capabilities machines can now be endowed with: they can read and understand free flowing text, reason and bargain with human counterparts, translate texts between languages, learn how to take decisions to maximize certain outcomes, etc. Today, machines have revolutionized the detection of cancer, the prediction of protein structures, the design of drugs, the control of nuclear fusion reactors etc. Although these capabilities are still in their infancy, it seems clear that their continued refinement and application will result in a technological impact on nearly all social and economic areas of human activity, the likes of which we have not seen before. In this article, I will share my view as to how AI will likely impact asset management in general and I will provide a mental framework that will equip readers with a simple criterion to assess whether and to what degree a given fund really exploi
    
[^44]: SREL：基于S参数模式的铜互连非破坏故障诊断的严重性评级集成学习

    SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns. (arXiv:2304.10207v1 [cs.LG])

    [http://arxiv.org/abs/2304.10207](http://arxiv.org/abs/2304.10207)

    本研究利用S参数模式成功实现对Cu互连缺陷的非破坏性检测和诊断，在同时分析根本原因和严重性方面具有先进性，具备早期检测、高诊断准确度和噪声鲁棒性等优点。

    

    随着处理器的工作频率和时钟速度逐年提高，互连对整个电子系统的可靠性和性能都产生了影响。检测和诊断互连故障对电子健康管理至关重要。然而，利用电信号作为预测因子的现有研究存在局限性，例如无法区分缺陷的根本原因，最终需要进行额外的破坏性评估，并容易受到噪声的干扰而导致误警。本文实现了对Cu互连缺陷的非破坏性检测和诊断，实现了早期检测、高诊断准确度和噪声鲁棒性。据我们所知，本研究首次利用电信号模式同时分析根本原因和严重性。在本文中，我们实验性地展示了S参数模式具有故障诊断能力。

    As operating frequencies and clock speeds in processors have increased over the years, interconnects affect both the reliability and performance of entire electronic systems. Fault detection and diagnosis of the interconnects are crucial for prognostics and health management (PHM) of electronics. However, existing research works utilizing electrical signals as prognostic factors have limitations, such as the inability to distinguish the root cause of defects, which eventually requires additional destructive evaluation, and vulnerability to noise that results in a false alarm. Herein, we realize the non-destructive detection and diagnosis of defects in Cu interconnects, achieving early detection, high diagnostic accuracy, and noise robustness. To the best of our knowledge, this study first simultaneously analyzes the root cause and severity using electrical signal patterns. In this paper, we experimentally show that S-parameter patterns have the ability for fault diagnosis and they are 
    
[^45]: 基于MC-dropout的脉冲神经网络高效不确定性估计

    Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout. (arXiv:2304.10191v1 [cs.NE])

    [http://arxiv.org/abs/2304.10191](http://arxiv.org/abs/2304.10191)

    本文提出了一种利用时间步机制实现MC-dropout的高效脉冲神经网络不确定性估计方法，为神经形态学硬件的节能应用提供了新的研究思路。

    

    脉冲神经网络(SNN)作为生物神经元稀疏和事件驱动通信模型的模拟器已引起关注，并因此在神经形态学硬件中显示出越来越多的节能应用前景。与经典人工神经网络(ANNs)一样，预测不确定性对于高风险应用（如自动驾驶车辆、医学诊断和高频交易）的决策非常重要。然而，关于SNNs中的不确定性估计的讨论很少，而人工神经网络(ANNs)中的不确定性估计方法不能直接应用于SNNs。在这里，我们提出了一种基于MC-dropout的脉冲神经网络不确定性估计的高效方法。我们的方法利用SNNs的时间步机制，以一种计算高效的方式实现MC-dropout，不会在训练和推断过程中引入重大开销，同时展示了高准确率和不确定性质量。

    Spiking neural networks (SNNs) have gained attention as models of sparse and event-driven communication of biological neurons, and as such have shown increasing promise for energy-efficient applications in neuromorphic hardware. As with classical artificial neural networks (ANNs), predictive uncertainties are important for decision making in high-stakes applications, such as autonomous vehicles, medical diagnosis, and high frequency trading. Yet, discussion of uncertainty estimation in SNNs is limited, and approaches for uncertainty estimation in artificial neural networks (ANNs) are not directly applicable to SNNs. Here, we propose an efficient Monte Carlo(MC)-dropout based approach for uncertainty estimation in SNNs. Our approach exploits the time-step mechanism of SNNs to enable MC-dropout in a computationally efficient manner, without introducing significant overheads during training and inference while demonstrating high accuracy and uncertainty quality.
    
[^46]: 智能制造中的网络安全（威胁、风景、挑战）

    Cyber Security in Smart Manufacturing (Threats, Landscapes Challenges). (arXiv:2304.10180v1 [cs.CR])

    [http://arxiv.org/abs/2304.10180](http://arxiv.org/abs/2304.10180)

    智能制造是工业4.0的一部分，将生产设备和网络智能结合，但由于现有操作流程的漏洞，使得智能制造成为网络威胁的主要目标，网络安全至关重要。

    

    工业4.0是信息技术和运营技术两个世界中超连通数字化工业的组合。智能制造利用这种机会，将制造设备的智能性与网络层面的智能性相结合。然而，由于现有操作流程的漏洞，智能制造现在成为网络威胁的主要目标。由于智能制造覆盖了从物理系统到增材制造、自主车辆、基于云的IIoT（工业物联网）再到机器人制造等广泛的生产行业领域，网络威胁被置于显著地位，关于如何通过网络连接制造资源、如何为工厂生产整个过程链条进行整合等问题产生了质疑。网络安全的机密性、完整性和可用性对于正常操作至关重要。

    Industry 4.0 is a blend of the hyper-connected digital industry within two world of Information Technology (IT) and Operational Technology (OT). With this amalgamate opportunity, smart manufacturing involves production assets with the manufacturing equipment having its own intelligence, while the system-wide intelligence is provided by the cyber layer. However Smart manufacturing now becomes one of the prime targets of cyber threats due to vulnerabilities in the existing process of operation. Since smart manufacturing covers a vast area of production industries from cyber physical system to additive manufacturing, to autonomous vehicles, to cloud based IIoT (Industrial IoT), to robotic production, cyber threat stands out with this regard questioning about how to connect manufacturing resources by network, how to integrate a whole process chain for a factory production etc. Cybersecurity confidentiality, integrity and availability expose their essential existence for the proper operatio
    
[^47]: 正则化连续学习中的二阶影响

    Regularizing Second-Order Influences for Continual Learning. (arXiv:2304.10177v1 [cs.LG])

    [http://arxiv.org/abs/2304.10177](http://arxiv.org/abs/2304.10177)

    本文提出了一种新的选择目标来规范连续学习中的二阶影响，并提供了一种有效的实现方法来优化所提出的标准。

    

    连续学习旨在学习非静态数据流，而不会灾难性地忘记以前学到的知识。目前的回放方法通过在保存已经看过的数据的小缓冲区上演奏，解决了这个难题，需要一个精细的样本选择策略。然而，现有的选择方案通常只寻求最大化正在进行的选择的效用，忽略了在选择的连续回合之间的干扰。受此启发，我们在影响函数的框架下剖析了顺序选择步骤的相互作用。我们设法识别出一类新的二阶影响，它将逐渐放大重复缓冲区中的偶然偏差，并损害选择过程。为了规范二阶效应，提出了一种新的选择目标，其还与两种广泛采用的标准具有明显的联系。此外，我们还提出了一种有效的实现方法，以优化所提出的标准。

    Continual learning aims to learn on non-stationary data streams without catastrophically forgetting previous knowledge. Prevalent replay-based methods address this challenge by rehearsing on a small buffer holding the seen data, for which a delicate sample selection strategy is required. However, existing selection schemes typically seek only to maximize the utility of the ongoing selection, overlooking the interference between successive rounds of selection. Motivated by this, we dissect the interaction of sequential selection steps within a framework built on influence functions. We manage to identify a new class of second-order influences that will gradually amplify incidental bias in the replay buffer and compromise the selection process. To regularize the second-order effects, a novel selection objective is proposed, which also has clear connections to two widely adopted criteria. Furthermore, we present an efficient implementation for optimizing the proposed criterion. Experiment
    
[^48]: 通过权重锚定实现鲁棒深度强化学习调度

    Robust Deep Reinforcement Learning Scheduling via Weight Anchoring. (arXiv:2304.10176v1 [cs.LG])

    [http://arxiv.org/abs/2304.10176](http://arxiv.org/abs/2304.10176)

    本研究提出了一种使用权重锚定来实现深度强化学习调度的方法，可避免在优化环境下忽略或忘记期望行为，提高了鲁棒性和可操纵性。

    

    当学习方法从仿真到现实中跨越鸿沟时，数据驱动学习方法的鲁棒性仍存在问题。本文提出了一种名为权重锚定的方法，该方法在持续学习中已知，并用于培养和固定神经网络中期望的行为。可以使用权重锚定方法找到一个与另一个学习问题的解接近的学习问题的解。这样，在优化环境下进行学习时，不会忽略或忘记期望的行为。我们通过学习混合的QoS高效离散资源调度与不频繁的优先消息的示例来演示此方法。结果表明，该方法提供了与增加仿真环境的现有技术相当的性能，同时显著提高了鲁棒性和可操纵性。

    Questions remain on the robustness of data-driven learning methods when crossing the gap from simulation to reality. We utilize weight anchoring, a method known from continual learning, to cultivate and fixate desired behavior in Neural Networks. Weight anchoring may be used to find a solution to a learning problem that is nearby the solution of another learning problem. Thereby, learning can be carried out in optimal environments without neglecting or unlearning desired behavior. We demonstrate this approach on the example of learning mixed QoS-efficient discrete resource scheduling with infrequent priority messages. Results show that this method provides performance comparable to the state of the art of augmenting a simulation environment, alongside significantly increased robustness and steerability.
    
[^49]: 基于混合量子神经网络的深度强化学习

    Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])

    [http://arxiv.org/abs/2304.10159](http://arxiv.org/abs/2304.10159)

    该研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并评估了其潜力。最终总结了开发深度量子学习的前景和结论。

    

    量子计算对于促进当前机器学习算法处理更高数据维度或减少深度神经网络模型的总体训练参数的限制具有强烈的影响。本研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并采用深度 Q-Learning 方法。该研究评估了其潜力。因此，设计并培训了一个基于最新的 Qiskit 和 PyTorch 框架的新型 PQC，以与完全经典的深度神经网络进行比较，带或不带集成 PQC。研究最后总结了其关于开发深度量子学习解决迷宫问题或其他强化学习问题的前景和结论。

    Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
    
[^50]: 灵活的K最近邻分类器：推导和在基于离子迁移谱的室内定位中的应用。

    Flexible K Nearest Neighbors Classifier: Derivation and Application for Ion-mobility Spectrometry-based Indoor Localization. (arXiv:2304.10151v1 [cs.LG])

    [http://arxiv.org/abs/2304.10151](http://arxiv.org/abs/2304.10151)

    本文提出了一种新的K最近邻分类器变体，可以确保最近邻居确实接近未标记样本，并在过程中找到K值。与标准KNN相比，该算法在室内指纹定位方面具有更高的分类精度。

    

    K最近邻分类器广泛应用于指纹定位或医学等多个领域。它基于K个标记样本，即最近邻居的类成员身份，决定未标记样本的类成员身份。K的选择一直是各种研究和提出KNN变体的主题，但没有一个变体被证明优于所有其他变体。本文提出了一种新的KNN变体，确保K个最近邻居确实接近未标记样本，并在过程中找到K值。该算法在理论情景和基于离子迁移谱指纹的室内定位方面进行了测试和比较。测试结果显示，该算法在与KNN同样的计算复杂度下，可以实现比KNN更高的分类精度。

    The K Nearest Neighbors (KNN) classifier is widely used in many fields such as fingerprint-based localization or medicine. It determines the class membership of unlabelled sample based on the class memberships of the K labelled samples, the so-called nearest neighbors, that are closest to the unlabelled sample. The choice of K has been the topic of various studies and proposed KNN-variants. Yet no variant has been proven to outperform all other variants. In this paper a new KNN-variant is proposed which ensures that the K nearest neighbors are indeed close to the unlabelled sample and finds K along the way. The proposed algorithm is tested and compared to the standard KNN in theoretical scenarios and for indoor localization based on ion-mobility spectrometry fingerprints. It achieves a higher classification accuracy than the KNN in the tests, while requiring having the same computational demand.
    
[^51]: 从预训练模型中学习样本难度以提高模型可靠性

    Learning Sample Difficulty from Pre-trained Models for Reliable Prediction. (arXiv:2304.10127v1 [cs.LG])

    [http://arxiv.org/abs/2304.10127](http://arxiv.org/abs/2304.10127)

    该论文介绍了如何使用预训练模型通过熵正则化来计算训练样本的难度，并根据样本难度惩罚过于自信的预测，从而提高模型的准确性和不确定性校准。

    

    大规模的预训练模型在各种场景和应用中取得了显著的成功，但如何利用它们来提高下游模型的预测可靠性仍未得到充分探索。此外，现代神经网络发现在固有样本难度和数据不确定性方面表现不佳，做出过于自信的预测。为了解决这个问题，我们提出使用大规模预训练模型以样本难度感知的熵正则化来指导下游模型的训练。预训练模型通过大规模数据集的训练，不会过度拟合下游训练集，使我们能够通过特征空间高斯建模和相对马氏距离的计算来测量每个训练样本的难度。重要的是，通过根据样本的难度自适应地惩罚过于自信的预测，我们同时提高各种具有挑战性的基准测试上的准确性和不确定性校准。

    Large-scale pre-trained models have achieved remarkable success in a variety of scenarios and applications, but how to leverage them to improve the prediction reliability of downstream models is undesirably under-explored. Moreover, modern neural networks have been found to be poorly calibrated and make overconfident predictions regardless of inherent sample difficulty and data uncertainty. To address this issue, we propose to utilize large-scale pre-trained models to guide downstream model training with sample difficulty-aware entropy regularization. Pre-trained models that have been exposed to large-scale datasets and do not overfit the downstream training classes enable us to measure each training sample difficulty via feature-space Gaussian modeling and relative Mahalanobis distance computation. Importantly, by adaptively penalizing overconfident prediction based on the sample's difficulty, we simultaneously improve accuracy and uncertainty calibration on various challenging benchm
    
[^52]: 解耦图神经网络：同时训练多个简单的GNN，而不是一个。

    Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One. (arXiv:2304.10126v1 [cs.LG])

    [http://arxiv.org/abs/2304.10126](http://arxiv.org/abs/2304.10126)

    本论文提出将多层图神经网络解耦为多个简单模块的方法，以实现更高效的训练。该方法包括经典的前向训练和设计的反向训练。每个模块都可以通过随机算法在前向训练中高效地训练，并且通过反向训练机制来使前面的模块能够感知后面的模块，从而充分训练浅层模块和更深层的模块。

    

    图神经网络（GNN）存在严重的效率问题，主要是由于节点依赖随着层数增加呈指数级增长。这极大地限制了随机优化算法的应用，使得GNN的训练通常很耗时。为了解决这个问题，我们提出了将多层GNN解耦为多个简单模块的方法，以实现更高效的训练。该方法由经典的前向训练（FT）和设计的反向训练（BT）组成。在所提出的框架下，每个模块都可以通过随机算法在FT中高效地训练，由于其简单性，不会扭曲图形信息。为避免FT的只单向信息传递，并充分训练浅层模块和更深层的模块，我们开发了一种反向训练机制，使前面的模块能够感知后面的模块。这种反向训练引入了反向信息传递到解耦模块中，同时也会有前向信息传递。

    Graph neural networks (GNN) suffer from severe inefficiency. It is mainly caused by the exponential growth of node dependency with the increase of layers. It extremely limits the application of stochastic optimization algorithms so that the training of GNN is usually time-consuming. To address this problem, we propose to decouple a multi-layer GNN as multiple simple modules for more efficient training, which is comprised of classical forward training (FT)and designed backward training (BT). Under the proposed framework, each module can be trained efficiently in FT by stochastic algorithms without distortion of graph information owing to its simplicity. To avoid the only unidirectional information delivery of FT and sufficiently train shallow modules with the deeper ones, we develop a backward training mechanism that makes the former modules perceive the latter modules. The backward training introduces the reversed information delivery into the decoupled modules as well as the forward i
    
[^53]: 用多智能体非对称进化强化学习掌握非对称多人游戏

    Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning. (arXiv:2304.10124v1 [cs.AI])

    [http://arxiv.org/abs/2304.10124](http://arxiv.org/abs/2304.10124)

    该论文提出了一种非对称进化训练（AET）框架，使用自适应数据调整（ADA）和环境随机化（ER）优化AET过程，使得AI可以在复杂的AMP游戏中击败顶级人类玩家，而不需要使用任何人类数据。

    

    非对称多人游戏（AMP）是一种流行的游戏类型，涉及多种类型的代理在游戏中相互竞争或合作。由于非对称环境中有不平衡的特性，因此使用典型的自我博弈训练方法训练强大的代理以击败顶级人类玩家在AMP游戏中是困难的。我们提出了非对称进化训练（AET），这是一种新颖的多智能体强化学习框架，可以同时训练多种代理在AMP游戏中。我们设计了自适应数据调整（ADA）和环境随机化（ER）来优化AET过程。我们在一个名为Tom＆Jerry的复杂AMP游戏中测试了我们的方法，我们训练出的AI对65个比赛取得了98.5％的胜率，而没有使用任何人类数据。消融实验表明，所提出的模块有益于该框架。

    Asymmetrical multiplayer (AMP) game is a popular game genre which involves multiple types of agents competing or collaborating with each other in the game. It is difficult to train powerful agents that can defeat top human players in AMP games by typical self-play training method because of unbalancing characteristics in their asymmetrical environments. We propose asymmetric-evolution training (AET), a novel multi-agent reinforcement learning framework that can train multiple kinds of agents simultaneously in AMP game. We designed adaptive data adjustment (ADA) and environment randomization (ER) to optimize the AET process. We tested our method in a complex AMP game named Tom \& Jerry, and our AIs trained without using any human data can achieve a win rate of 98.5% against top human players over 65 matches. The ablation experiments indicated that the proposed modules are beneficial to the framework.
    
[^54]: 基于机器学习的自动采购欺诈检测

    Automatic Procurement Fraud Detection with Machine Learning. (arXiv:2304.10105v1 [cs.LG])

    [http://arxiv.org/abs/2304.10105](http://arxiv.org/abs/2304.10105)

    该论文讲述了如何使用机器学习技术检测采购欺诈，针对每个采购事件使用9个特定特征构建了神经网络模型，并通过测试证明这些模型在检测采购欺诈方面是有用的。

    

    虽然采购欺诈一直是几乎所有自由市场的一个严重问题，但审计部门仍然强烈依赖来自知情人士的报告来发现它们。与我们慷慨的合作伙伴顺丰速运分享2015年至2017年其公司进行的采购相关数据库的访问权后，我们的团队研究了如何使用机器学习技术来帮助审计中国目前最深刻的犯罪之一，即采购欺诈。通过将每个采购事件表示为9个特定特征，我们构建了神经网络模型来识别可疑采购并对其欺诈类型进行分类。通过在采购数据库收集的50000个样本上测试我们的模型，我们已经证明这些模型尽管还有改进的空间，但对于检测采购欺诈是有用的。

    Although procurement fraud is always a critical problem in almost every free market, audit departments still have a strong reliance on reporting from informed sources when detecting them. With our generous cooperator, SF Express, sharing the access to the database related with procurements took place from 2015 to 2017 in their company, our team studies how machine learning techniques could help with the audition of one of the most profound crime among current chinese market, namely procurement frauds. By representing each procurement event as 9 specific features, we construct neural network models to identify suspicious procurements and classify their fraud types. Through testing our models over 50000 samples collected from the procurement database, we have proven that such models -- despite having space for improvements -- are useful in detecting procurement frauds.
    
[^55]: 联邦组合深度AUC最大化

    Federated Compositional Deep AUC Maximization. (arXiv:2304.10101v1 [cs.LG])

    [http://arxiv.org/abs/2304.10101](http://arxiv.org/abs/2304.10101)

    本论文介绍了一种通过直接优化AUC分数来解决联邦学习中不平衡数据问题的方法，并开发了一个随机组合梯度下降上升动量算法。通过广泛的实验验证，证实了该方法的效率和有效性。

    

    联邦学习由于平衡隐私和大规模学习的承诺而引起了越来越多的关注；已经提出了许多方法。然而，大多数现有方法都集中在处理平衡数据问题上，而在许多现实应用中，不同类别中的样本数量高度不平衡，导致预测性能远低于理想水平。为了解决这个具有挑战性的问题，我们开发了一种针对不平衡数据的新型联邦学习方法，通过直接优化曲线下面积（AUC）分数来提高预测性能。具体来说，我们将AUC最大化问题作为联邦组合最小最大优化问题进行了表述，并开发了本地随机组合梯度下降上升动量算法，并提供了我们算法的计算和通信复杂度的界限。据我们所知，这是第一个实现如此有利理论结果的工作。最后，广泛的实验结果证实了我们提出的方法的效率和有效性。

    Federated learning has attracted increasing attention due to the promise of balancing privacy and large-scale learning; numerous approaches have been proposed. However, most existing approaches focus on problems with balanced data, and prediction performance is far from satisfactory for many real-world applications where the number of samples in different classes is highly imbalanced. To address this challenging problem, we developed a novel federated learning method for imbalanced data by directly optimizing the area under curve (AUC) score. In particular, we formulate the AUC maximization problem as a federated compositional minimax optimization problem, develop a local stochastic compositional gradient descent ascent with momentum algorithm, and provide bounds on the computational and communication complexities of our algorithm. To the best of our knowledge, this is the first work to achieve such favorable theoretical results. Finally, extensive experimental results confirm the effi
    
[^56]: 双记忆强化学习

    Two-Memory Reinforcement Learning. (arXiv:2304.10098v1 [cs.LG])

    [http://arxiv.org/abs/2304.10098](http://arxiv.org/abs/2304.10098)

    本文提出了双记忆强化学习代理 (2M)，它结合了情节记忆和强化学习的优点来提高学习速度和准确性。

    

    虽然深度强化学习取得了重要的经验性成功，但由于奖励信息传播和参数神经网络更新的速度较慢，它倾向于学习得比较慢。另一方面，非参数化的情节记忆提供了相对较快的学习替代方案，它不需要表示学习，并使用最大情节回报作为状态-动作值进行行动选择。情节记忆和强化学习都有各自的优点和缺点。值得注意的是，人类可以同时利用多个记忆系统进行学习，并从中获益。在这项工作中，我们提出了一种称为双记忆强化学习代理（2M）的方法，它结合了情节记忆和强化学习的优点。 2M 代理利用情节记忆部分的速度和强化学习部分的最优性和广泛适用性相互补充。我们的实验表明，

    While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that 
    
[^57]: 动态图表示学习中带有边时序状态的循环Transformer

    Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States. (arXiv:2304.10079v1 [cs.LG])

    [http://arxiv.org/abs/2304.10079](http://arxiv.org/abs/2304.10079)

    本文提出了循环差分图变换器框架，旨在解决动态图表示学习中未能明确建模边时序状态和提取全局结构特征的问题。

    

    随着现实世界中对图数据分析的广泛需求，动态图表示学习正成为一项趋势性而具有挑战性的研究任务。尽管许多最近的研究基于循环神经网络（RNNs）和图神经网络（GNNs）展现了令人鼓舞的表现，但它们未能明确地对节点特征随时间片段的边时序状态产生影响进行建模。此外，由于GNNs的内在over-smoothing缺陷，它们很难提取全局结构特征，进一步限制了性能。在本文中，我们提出了一个循环差分图变换器（RDGT）框架，该框架首先为每个快照中的边分配了各种类型和权重，以明确地说明它们的特定时间状态，然后采用增强结构的图变换器来通过循环学习范式捕获时间节点表示。在四个真实的数据集上进行的实验结果表明

    Dynamic graph representation learning is growing as a trending yet challenging research task owing to the widespread demand for graph data analysis in real world applications. Despite the encouraging performance of many recent works that build upon recurrent neural networks (RNNs) and graph neural networks (GNNs), they fail to explicitly model the impact of edge temporal states on node features over time slices. Additionally, they are challenging to extract global structural features because of the inherent over-smoothing disadvantage of GNNs, which further restricts the performance. In this paper, we propose a recurrent difference graph transformer (RDGT) framework, which firstly assigns the edges in each snapshot with various types and weights to illustrate their specific temporal states explicitly, then a structure-reinforced graph transformer is employed to capture the temporal node representations by a recurrent learning paradigm. Experimental results on four real-world datasets d
    
[^58]: 利用标签技巧在多节点任务上改进图神经网络

    Improving Graph Neural Networks on Multi-node Tasks with Labeling Tricks. (arXiv:2304.10074v1 [cs.LG])

    [http://arxiv.org/abs/2304.10074](http://arxiv.org/abs/2304.10074)

    本文提出了一种标签技巧，用于在多节点任务上提高图神经网络的表示学习能力。该方法通过区分目标节点和其他节点的方式改进了以往直接聚合各节点表示的缺陷，能够更好地捕捉节点间的依赖关系。

    

    本文提供了关于如何利用图神经网络（GNN）进行多节点表示学习的理论，其中我们有兴趣学习由多个节点组成的节点集的表示，如一个链接。现有的GNN主要设计用于学习单个节点表示。当我们想要学习涉及多个节点的节点集表示时，以前的工作中常见的做法是直接聚合GNN获得的单节点表示。本文展示了这种方法的一个基本局限性，即不能捕捉节点集中多个节点之间的依赖关系，同时也认为直接聚合各个节点的表示无法为多个节点产生有效的联合表示。一个直接的解决方案是区分目标节点和其他节点。基于这个想法，我们提出了“标签技巧”，它首先根据与目标节点集的关系标记图中的节点。

    In this paper, we provide a theory of using graph neural networks (GNNs) for \textit{multi-node representation learning}, where we are interested in learning a representation for a set of more than one node such as a link. Existing GNNs are mainly designed to learn single-node representations. When we want to learn a node-set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN. In this paper, we show a fundamental limitation of such an approach, namely the inability to capture the dependence among multiple nodes in a node set, and argue that directly aggregating individual node representations fails to produce an effective joint representation for multiple nodes. A straightforward solution is to distinguish target nodes from others. Formalizing this idea, we propose \text{labeling trick}, which first labels nodes in the graph according to their relationships with the target node set befo
    
[^59]: 基于可穿戴传感器和机器学习的水果采摘活动识别

    Fruit Picker Activity Recognition with Wearable Sensors and Machine Learning. (arXiv:2304.10068v1 [cs.LG])

    [http://arxiv.org/abs/2304.10068](http://arxiv.org/abs/2304.10068)

    本文介绍了一种基于可穿戴传感器和机器学习的水果采摘活动识别方法，成功实现了高精度识别袋倒空事件，证明了该方法在农作物管理应用中的可行性。

    

    本文介绍了一种新颖的应用，基于可穿戴传感器生成的时间序列数据来检测水果采摘活动。我们提出了一种使用可穿戴传感器和机器学习方法进行人体活动识别的手段来检测采摘袋倒空事件。我们开发了一种半监督的数据标注方法。我们开发并测试了一种基于特征的机器学习集成模型和深度循环卷积神经网络，并在实际数据集上进行了测试。与机器学习集成模型相比，神经网络取得了更好的性能，准确率为97.92%。我们的方法成功地高精度识别了袋倒空事件，证明了使用可穿戴传感器进行农作物管理应用的可行性。

    In this paper we present a novel application of detecting fruit picker activities based on time series data generated from wearable sensors. During harvesting, fruit pickers pick fruit into wearable bags and empty these bags into harvesting bins located in the orchard. Once full, these bins are quickly transported to a cooled pack house to improve the shelf life of picked fruits. For farmers and managers, the knowledge of when a picker bag is emptied is important for managing harvesting bins more effectively to minimise the time the picked fruit is left out in the heat (resulting in reduced shelf life). We propose a means to detect these bag-emptying events using human activity recognition with wearable sensors and machine learning methods. We develop a semi-supervised approach to labelling the data. A feature-based machine learning ensemble model and a deep recurrent convolutional neural network are developed and tested on a real-world dataset. When compared, the neural network achiev
    
[^60]: 将深度等变模型的领先精度扩展到真实尺寸生物分子模拟

    Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size. (arXiv:2304.10061v1 [physics.comp-ph])

    [http://arxiv.org/abs/2304.10061](http://arxiv.org/abs/2304.10061)

    本研究开发出Allegro架构，通过创新的模型架构和GPU并行化实现了对极大分子的高精度模拟，能够对空前复杂的结构进行量子保真动力学描述，并成功进行了对4400万原子的HIV外壳的模拟。

    

    本研究通过创新的模型架构、大规模并行化以及针对高效GPU利用率进行优化的模型和实现，将深度等变神经网络的领先精度、样本效率和鲁棒性发挥到极致计算规模。Allegro架构成功解决了原子模拟的精度-速度权衡问题，实现了空前复杂结构的量子保真动力学描述。我们演示了Allegro的可扩展性，进行了纳秒级蛋白动力学稳定模拟，并在Perlmutter超级计算机上扩展到了一个完整的、所有原子明确溶剂化的4400万原子结构的HIV外壳。我们展示了100万原子的优异强扩展性和5120个A100 GPU的70%弱扩展性。

    This work brings the leading accuracy, sample efficiency, and robustness of deep equivariant neural networks to the extreme computational scale. This is achieved through a combination of innovative model architecture, massive parallelization, and models and implementations optimized for efficient GPU utilization. The resulting Allegro architecture bridges the accuracy-speed tradeoff of atomistic simulations and enables description of dynamics in structures of unprecedented complexity at quantum fidelity. To illustrate the scalability of Allegro, we perform nanoseconds-long stable simulations of protein dynamics and scale up to a 44-million atom structure of a complete, all-atom, explicitly solvated HIV capsid on the Perlmutter supercomputer. We demonstrate excellent strong scaling up to 100 million atoms and 70% weak scaling to 5120 A100 GPUs.
    
[^61]: 鲁棒性在线学习算法的最优性分析

    Optimality of Robust Online Learning. (arXiv:2304.10060v1 [stat.ML])

    [http://arxiv.org/abs/2304.10060](http://arxiv.org/abs/2304.10060)

    本文提出了一种基于鲁棒损失函数 $\mathcal{L}_{\sigma}$ 的在线学习算法，可用作在线最小二乘回归的鲁棒替代方案。并证明了在适当选择参数的情况下，该算法具有无容量依赖的最优性收敛性以及强收敛的最优容量依赖速率。

    

    本文研究在再生核希尔伯特空间上使用鲁棒损失函数 $\mathcal{L}_{\sigma}$ 进行回归的在线学习算法。这个涉及到缩放参数 $\sigma>0$ 的损失函数可以覆盖一系列常用的鲁棒损失函数。提出的算法是针对在线最小二乘回归的鲁棒替代方案，旨在估计条件均值函数。在选择适当的 $\sigma$ 和步长的情况下，我们证明了该在线算法的最终迭代可以在均方距离上实现无容量依赖的收敛最优性。此外，如果已知底层函数空间的其他信息，则我们还建立了强收敛的最优容量依赖速率。据我们所知，这两个结果都是在线学习现有文献中的新结果。

    In this paper, we study an online learning algorithm with a robust loss function $\mathcal{L}_{\sigma}$ for regression over a reproducing kernel Hilbert space (RKHS). The loss function $\mathcal{L}_{\sigma}$ involving a scaling parameter $\sigma>0$ can cover a wide range of commonly used robust losses. The proposed algorithm is then a robust alternative for online least squares regression aiming to estimate the conditional mean function. For properly chosen $\sigma$ and step size, we show that the last iterate of this online algorithm can achieve optimal capacity independent convergence in the mean square distance. Moreover, if additional information on the underlying function space is known, we also establish optimal capacity dependent rates for strong convergence in RKHS. To the best of our knowledge, both of the two results are new to the existing literature of online learning.
    
[^62]: HyperTuner：数据分析服务的跨层多目标超参数自动调整框架

    HyperTuner: A Cross-Layer Multi-Objective Hyperparameter Auto-Tuning Framework for Data Analytic Services. (arXiv:2304.10051v1 [cs.LG])

    [http://arxiv.org/abs/2304.10051](http://arxiv.org/abs/2304.10051)

    本论文提出了HyperTuner，它是一个跨层多目标超参数自动调整框架，可同时考虑模型超参数和系统参数。HyperTuner使用MOPIR算法进行多目标参数重要性排序，然后利用ADUMBO算法找到Pareto最优配置集合，帮助数据分析服务提供商解决高维黑盒多目标优化问题。

    

    超参数优化对于机器学习模型至关重要，除了模型准确性之外，模型训练时间和能耗等调节意图也值得吸引数据分析服务提供商的关注。因此，考虑到模型超参数和系统参数，执行跨层多目标超参数自动调整是至关重要的。为了解决这个具有挑战性的目标，本文提出了HyperTuner。为了解决该高维黑盒多目标优化问题，HyperTuner首先使用MOPIR算法进行多目标参数重要性排序，然后利用提出的ADUMBO算法找到Pareto最优配置集合。在每个迭代过程中，ADUMBO通过最大化一个新的设计良好的度量标准来自动生成Pareto候选集合，并选择最有前途的配置。

    Hyper-parameters optimization (HPO) is vital for machine learning models. Besides model accuracy, other tuning intentions such as model training time and energy consumption are also worthy of attention from data analytic service providers. Hence, it is essential to take both model hyperparameters and system parameters into consideration to execute cross-layer multi-objective hyperparameter auto-tuning. Towards this challenging target, we propose HyperTuner in this paper. To address the formulated high-dimensional black-box multi-objective optimization problem, HyperTuner first conducts multi-objective parameter importance ranking with its MOPIR algorithm and then leverages the proposed ADUMBO algorithm to find the Pareto-optimal configuration set. During each iteration, ADUMBO selects the most promising configuration from the generated Pareto candidate set via maximizing a new well-designed metric, which can adaptively leverage the uncertainty as well as the predicted mean across all t
    
[^63]: 基于核的模态统计方法的最优核函数

    Optimal Kernel for Kernel-Based Modal Statistical Methods. (arXiv:2304.10046v1 [stat.ML])

    [http://arxiv.org/abs/2304.10046](http://arxiv.org/abs/2304.10046)

    本文研究基于核函数的模态统计方法的最优核函数的选择问题，提出了一种（多元）最优核函数，在某个核函数类别中使得其解析得到的渐近误差准则最小化。

    

    基于核函数的模态统计方法包括模态估计、回归和聚类。这些方法的估计准确性取决于所使用的核函数和带宽。本文研究了核函数选择对这些方法估计准确性的影响。特别地，当使用最优带宽时，本文理论上展示了一种（多元）最优核函数，其在定义为其符号变化数量的某个核函数类别中，使得其解析得到的渐近误差准则最小化。

    Kernel-based modal statistical methods include mode estimation, regression, and clustering. Estimation accuracy of these methods depends on the kernel used as well as the bandwidth. We study effect of the selection of the kernel function to the estimation accuracy of these methods. In particular, we theoretically show a (multivariate) optimal kernel that minimizes its analytically-obtained asymptotic error criterion when using an optimal bandwidth, among a certain kernel class defined via the number of its sign changes.
    
[^64]: ID-MixGCL: 基于身份混合的图形对比学习

    ID-MixGCL: Identity Mixup for Graph Contrastive Learning. (arXiv:2304.10045v1 [cs.LG])

    [http://arxiv.org/abs/2304.10045](http://arxiv.org/abs/2304.10045)

    本文提出了一种基于身份混合的图形对比学习方法，旨在解决通过图形增强得到的不同但相似的图形结构和标签的不匹配问题，以实现更好的表示捕获。

    

    最近发展的图形对比学习（GCL）方法是比较同一个图形的两个不同的“视图”以学习节点/图形表示。这些方法的核心假设是通过图形增强，可以生成几个结构不同但语义相似的图形结构，因此原始和增强的图形/节点的身份标签应该是相同的。然而，在本文中，我们发现这个假设并不总是成立，例如分子图中对节点或边的任何扰动都会在一定程度上改变图形标签。因此，我们认为增强图形结构应该伴随着对对比损失使用的标签的适应。基于这个想法，我们提出了 ID-MixGCL，它允许同时调节输入图形和相应的身份标签，具有可控的改变程度，从而捕获细粒度的表示。

    Recently developed graph contrastive learning (GCL) approaches compare two different "views" of the same graph in order to learn node/graph representations. The core assumption of these approaches is that by graph augmentation, it is possible to generate several structurally different but semantically similar graph structures, and therefore, the identity labels of the original and augmented graph/nodes should be identical. However, in this paper, we observe that this assumption does not always hold, for example, any perturbation to nodes or edges in a molecular graph will change the graph labels to some degree. Therefore, we believe that augmenting the graph structure should be accompanied by an adaptation of the labels used for the contrastive loss. Based on this idea, we propose ID-MixGCL, which allows for simultaneous modulation of both the input graph and the corresponding identity labels, with a controllable degree of change, leading to the capture of fine-grained representations 
    
[^65]: 基于深度学习的脑肿瘤多分类和分割在MRO图像中的应用

    Brain tumor multi classification and segmentation in MRO images using deep learning. (arXiv:2304.10039v1 [eess.IV])

    [http://arxiv.org/abs/2304.10039](http://arxiv.org/abs/2304.10039)

    本研究提出了基于深度学习的脑肿瘤分类和分割模型，能够准确地将MRI图像中的肿瘤分为四类，并将肿瘤从图像中精确地分割出来，具有潜在的临床应用价值。

    

    本研究提出了一种基于深度学习的模型，用于从磁共振成像（MRI）扫描中对脑肿瘤进行分类和分割。分类模型基于EfficientNetB1架构，经过训练，将图像分为四类：脑膜瘤，胶质瘤，垂体腺瘤和无肿瘤。分割模型基于U-Net架构，经过训练，能够准确地从MRI图像中分割出肿瘤。该模型在一个公开可用的数据集上进行评估，并实现了高精度和分割指标，表明其在脑肿瘤的诊断和治疗中具有潜在的临床应用价值。

    This study proposes a deep learning model for the classification and segmentation of brain tumors from magnetic resonance imaging (MRI) scans. The classification model is based on the EfficientNetB1 architecture and is trained to classify images into four classes: meningioma, glioma, pituitary adenoma, and no tumor. The segmentation model is based on the U-Net architecture and is trained to accurately segment the tumor from the MRI images. The models are evaluated on a publicly available dataset and achieve high accuracy and segmentation metrics, indicating their potential for clinical use in the diagnosis and treatment of brain tumors.
    
[^66]: 开放世界持续学习：统一新颖性检测与持续学习

    Open-World Continual Learning: Unifying Novelty Detection and Continual Learning. (arXiv:2304.10038v1 [cs.LG])

    [http://arxiv.org/abs/2304.10038](http://arxiv.org/abs/2304.10038)

    本文从理论上证明，分布外检测对于类别增量学习是必要的，因为类别增量学习可以分解成任务内预测和任务 ID 预测，并且任务 ID 预测与分布外检测相关。

    

    随着 AI agent 在未知或新奇的真实开放世界中的使用增加，它们需要具备 (1) 认识已经学习过的物体和检测到之前未见或学习的物体的能力，以及 (2) 增量地学习新物品，逐渐变得更有知识和更强大。 (1) 称为新颖性检测或分布外 (OOD) 检测，而 (2) 称为类别增量学习 (CIL)，是持续学习 (CL) 的一种设置。在现有的研究中，OOD 检测和 CIL 被视为两个完全不同的问题。本文从理论上证明了 OOD 检测实际上对于 CIL 是必要的。我们首先展示 CIL 可以分解为两个子问题：任务内预测 (WP) 和任务 ID 预测(TP)。然后我们证明了 TP 与 OOD 检测相关。关键的理论结果是，无论 WP 和 OOD 检测（或 TP）是否由 CIL 算法显式或隐式地定义，好的 WP 和良好的 OOD 检测或 TP 总是存在嵌入在任何 CIL 算法中的。

    As AI agents are increasingly used in the real open world with unknowns or novelties, they need the ability to (1) recognize objects that (i) they have learned and (ii) detect items that they have not seen or learned before, and (2) learn the new items incrementally to become more and more knowledgeable and powerful. (1) is called novelty detection or out-of-distribution (OOD) detection and (2) is called class incremental learning (CIL), which is a setting of continual learning (CL). In existing research, OOD detection and CIL are regarded as two completely different problems. This paper theoretically proves that OOD detection actually is necessary for CIL. We first show that CIL can be decomposed into two sub-problems: within-task prediction (WP) and task-id prediction (TP). We then prove that TP is correlated with OOD detection. The key theoretical result is that regardless of whether WP and OOD detection (or TP) are defined explicitly or implicitly by a CIL algorithm, good WP and go
    
[^67]: 拓扑深度学习的架构：拓扑神经网络综述

    Architectures of Topological Deep Learning: A Survey on Topological Neural Networks. (arXiv:2304.10031v1 [cs.LG])

    [http://arxiv.org/abs/2304.10031](http://arxiv.org/abs/2304.10031)

    拓扑深度学习框架提供了从复杂系统相关数据中提取知识的全面架构，通过解决现有工作的符号和术语不一致问题，有望在应用科学和其他领域开拓新局面。

    

    自然界中充满了复杂的系统，其组成部分之间存在错综复杂的关系：从社交网络中个体之间的社交互动到蛋白质中原子之间的静电相互作用。拓扑深度学习（TDL）提供了一个全面的框架来处理和从这些系统相关的数据中提取知识，如预测一个人属于哪个社区或预测一个蛋白质是否可以成为合理的药物开发靶点。TDL已经证明拥有理论和实践上的优势，这为在应用科学和其他领域开拓新局面提供了希望。然而，TDL文献的快速增长也导致了拓扑神经网络（TNN）体系结构符号和术语上的不一致。这对于建立在现有工作基础上和将TNN部署到新的现实问题中都是一个真正的障碍。为了解决这个问题，我们提供了一个易于理解的综述。

    The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature has also led to a lack of unification in notation and language across Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying TNNs to new real-world problems. To address this issue, we provide an accessible introduction 
    
[^68]: Jedi: 基于信息熵的对抗性补丁定位与清除

    Jedi: Entropy-based Localization and Removal of Adversarial Patches. (arXiv:2304.10029v1 [cs.CR])

    [http://arxiv.org/abs/2304.10029](http://arxiv.org/abs/2304.10029)

    本文提出了一种新的对抗性补丁防御方法——Jedi，该方法从信息论的角度解决了补丁定位问题，并利用智能编码器提高了对抗性补丁的定位精度，从而提供了一种针对对抗性补丁的通用防御。

    

    真实世界中的对抗性物理补丁已经成功地在各种计算机视觉应用中破坏了最先进的模型。现有的基于梯度或特征分析的防御已经被最近的基于GAN的攻击所破坏，这些攻击生成了自然主义的补丁。在本文中，我们提出了Jedi，一种新的针对对抗性补丁的防御方法，该方法对真实的补丁攻击具有弹性。Jedi从信息论的角度解决了补丁定位问题；利用了两个新想法：(1) 它通过熵分析改进了潜在补丁区域的识别：我们证明了对抗性补丁的熵是高的，即使在自然主义的补丁中也是如此；(2) 它利用能够从高熵内核中完成补丁区域的自动编码器，来改善对抗性补丁的定位。Jedi实现了高精度的对抗性补丁定位，我们证明这对于成功修复图像至关重要。由于Jedi是针对补丁特定的，并不需要先前关于对抗攻击方法的先验知识，因此它提供了一种针对对抗性补丁的通用防御。

    Real-world adversarial physical patches were shown to be successful in compromising state-of-the-art models in a variety of computer vision applications. Existing defenses that are based on either input gradient or features analysis have been compromised by recent GAN-based attacks that generate naturalistic patches. In this paper, we propose Jedi, a new defense against adversarial patches that is resilient to realistic patch attacks. Jedi tackles the patch localization problem from an information theory perspective; leverages two new ideas: (1) it improves the identification of potential patch regions using entropy analysis: we show that the entropy of adversarial patches is high, even in naturalistic patches; and (2) it improves the localization of adversarial patches, using an autoencoder that is able to complete patch regions from high entropy kernels. Jedi achieves high-precision adversarial patch localization, which we show is critical to successfully repair the images. Since Jed
    
[^69]: 对于监测而言太病态：联邦艾滋病服务数据能否提高联邦艾滋病监测工作？

    Too sick for surveillance: Can federal HIV service data improve federal HIV surveillance efforts?. (arXiv:2304.10023v1 [cs.LG])

    [http://arxiv.org/abs/2304.10023](http://arxiv.org/abs/2304.10023)

    本研究提取了联邦艾滋病服务数据和艾滋病监测数据，并使用深度学习模型系列来识别未被监测到的服务数据，旨在为未来的艾滋病传播预防提供数据支持。

    

    引言：目前整合联邦艾滋病服务数据和艾滋病监测的价值尚未得知。上游和完整的案例捕获对于未来的艾滋病传播预防至关重要。方法：本研究将2005年至2018年的Ryan White、社会保障残疾保险、医疗保险、儿童健康保险计划和医疗补助的人口统计数据与疾病控制与预防中心的艾滋病监测数据按人口统计聚合，发现服务统计量超过监测统计量的聚合即为监测未知，而已确定服务的候选聚合。使用分布方法和深度学习模型系列来确定在聚合中监测病例超过服务病例的监测未知，即监测未知服务已知（SUSK）候选聚合。结果：医疗保险有最多的SUSK候选聚合。医疗补助可能有候选的SUSK聚合，其中的病例接近监测。

    Introduction: The value of integrating federal HIV services data with HIV surveillance is currently unknown. Upstream and complete case capture is essential in preventing future HIV transmission. Methods: This study integrated Ryan White, Social Security Disability Insurance, Medicare, Children Health Insurance Programs and Medicaid demographic aggregates from 2005 to 2018 for people living with HIV and compared them with Centers for Disease Control and Prevention HIV surveillance by demographic aggregate. Surveillance Unknown, Service Known (SUSK) candidate aggregates were identified from aggregates where services aggregate volumes exceeded surveillance aggregate volumes. A distribution approach and a deep learning model series were used to identify SUSK candidate aggregates where surveillance cases exceeded services cases in aggregate. Results: Medicare had the most candidate SUSK aggregates. Medicaid may have candidate SUSK aggregates where cases approach parity with surveillance. D
    
[^70]: 数字孪生图：物联网世界自动领域无关建设、融合和模拟

    Digital Twin Graph: Automated Domain-Agnostic Construction, Fusion, and Simulation of IoT-Enabled World. (arXiv:2304.10018v1 [cs.LG])

    [http://arxiv.org/abs/2304.10018](http://arxiv.org/abs/2304.10018)

    提出了一个称为数字孪生图(DTG)的数据结构，可以完全自动化和领域无关地构建数字孪生，采用了数据驱动和图学习方法来解决数字孪生的关键挑战。

    

    随着物联网技术的发展，海量的传感器数据通过无线网络通信，创造了构建数字孪生以反映和模拟复杂物理世界的机会。长期以来，数字孪生被认为严重依赖于领域知识，但我们认为这导致了高门槛和缓慢的开发，因为人工专家稀缺和成本昂贵。在本文中，我们提出了数字孪生图(DTG)，这是一个与处理框架相关联的通用数据结构，可以以完全自动化和领域无关的方式构建数字孪生。这项工作代表了首次尝试采用完全数据驱动和（非传统的）图学习方法来解决数字孪生的关键挑战。

    With the advances of IoT developments, copious sensor data are communicated through wireless networks and create the opportunity of building Digital Twins to mirror and simulate the complex physical world. Digital Twin has long been believed to rely heavily on domain knowledge, but we argue that this leads to a high barrier of entry and slow development due to the scarcity and cost of human experts. In this paper, we propose Digital Twin Graph (DTG), a general data structure associated with a processing framework that constructs digital twins in a fully automated and domain-agnostic manner. This work represents the first effort that takes a completely data-driven and (unconventional) graph learning approach to addresses key digital twin challenges.
    
[^71]: 速通与机器学习中的幂律趋势

    Power Law Trends in Speedrunning and Machine Learning. (arXiv:2304.10004v1 [cs.LG])

    [http://arxiv.org/abs/2304.10004](http://arxiv.org/abs/2304.10004)

    该论文发现了速通世界纪录改进的幂律模式，并利用这一发现提高了预测速通世界纪录精度的方法，并在机器学习基准上得到了类似的结果。结果表明，ML基准远未饱和，而机器学习中的改进具有突然性。

    

    我们发现，在速通世界纪录的改进中存在幂律模式。利用这一观察结果，我们回答了之前研究中的一个未解决问题：如何在预测某个时间跨度（如一个月）内的速通世界纪录时，提高基线预测不改进的精度？通过使用随机效应模型，在预测样本外的世界纪录改进的相对均方误差上，我们在$p<10^{-5}$的显著性水平上提高了基线预测的精度。尽管使用的数据点远少于先前表现最佳的指数移动平均预测模型，但相同的设置在$p=0.15$的显著性水平上提高了预测的准确率。我们将这种方法应用于机器学习基准并取得了超过基线的预测效果。最后，通过解释所得到的模型，我们认为1）ML基准远未饱和，2）机器学习中的突然大幅改进

    We find that improvements in speedrunning world records follow a power law pattern. Using this observation, we answer an outstanding question from previous work: How do we improve on the baseline of predicting no improvement when forecasting speedrunning world records out to some time horizon, such as one month? Using a random effects model, we improve on this baseline for relative mean square error made on predicting out-of-sample world record improvements as the comparison metric at a $p < 10^{-5}$ significance level. The same set-up improves \textit{even} on the ex-post best exponential moving average forecasts at a $p = 0.15$ significance level while having access to substantially fewer data points. We demonstrate the effectiveness of this approach by applying it to Machine Learning benchmarks and achieving forecasts that exceed a baseline. Finally, we interpret the resulting model to suggest that 1) ML benchmarks are far from saturation and 2) sudden large improvements in Machine 
    
[^72]: 基于模型的强化学习在个性化肝素剂量控制中的应用

    Model Based Reinforcement Learning for Personalized Heparin Dosing. (arXiv:2304.10000v1 [math.OC])

    [http://arxiv.org/abs/2304.10000](http://arxiv.org/abs/2304.10000)

    本文提出了一种基于预测模型和模型基础的强化学习方法来解决个性化肝素剂量控制问题，相较于标准治疗和无模型方法具有更好的安全性和功效。

    

    序贯决策问题中的一个关键挑战是在局部信息下安全地优化系统。计算患者的肝素剂量，由于无法直接测量患者血液中肝素的浓度，且不同患者代谢肝素的速率各不相同，因此具有部分可知的状态和动态。然而，若部分结构的动态是已知的，则可以通过模型基础的方法提供安全策略。本文提出了一种框架来优化个性化肝素剂量控制，通过个性化的预测模型预测未来治疗效果，并通过模型基础的强化学习来学习个性化肝素剂量控制策略。我们在重症监护室肝素控制数据集上评估了此方法的效果，与标准治疗和无模型方法相比，证明了其具有更好的安全性和功效。

    A key challenge in sequential decision making is optimizing systems safely under partial information. While much of the literature has focused on the cases of either partially known states or partially known dynamics, it is further exacerbated in cases where both states and dynamics are partially known. Computing heparin doses for patients fits this paradigm since the concentration of heparin in the patient cannot be measured directly and the rates at which patients metabolize heparin vary greatly between individuals. While many proposed solutions are model free, they require complex models and have difficulty ensuring safety. However, if some of the structure of the dynamics is known, a model based approach can be leveraged to provide safe policies. In this paper we propose such a framework to address the challenge of optimizing personalized heparin doses. We use a predictive model parameterized individually by patient to predict future therapeutic effects. We then leverage this model
    
[^73]: 基于投票的实例筛选方法：使用基于赞成票的多获胜者投票

    Data as voters: instance selection using approval-based multi-winner voting. (arXiv:2304.09995v1 [cs.LG])

    [http://arxiv.org/abs/2304.09995](http://arxiv.org/abs/2304.09995)

    该论文提出了一种基于赞成票的多获胜者投票的实例选择方法，通过代表性投票规则选择获胜者，并将其作为减少训练集的数据实例。

    

    我们提出了一种新的机器学习（或数据挖掘）中的实例筛选方法。我们的方法基于最近关于基于赞成票的多获胜者选举中代表性表征的结果。在我们的模型中，实例扮演选民和候选人的双重角色。每个训练集中的实例（作为选民）赞成其本地集合中的实例（扮演候选人的角色）（除自身以外的实例），这个概念在文献中已经存在。然后，我们使用代表性投票规则选择选举获胜者，并作为减少训练集中的数据实例保留。

    We present a novel approach to the instance selection problem in machine learning (or data mining). Our approach is based on recent results on (proportional) representation in approval-based multi-winner elections. In our model, instances play a double role as voters and candidates. Each instance in the training set (acting as a voter) approves of the instances (playing the role of candidates) belonging to its local set (except itself), a concept already existing in the literature. We then select the election winners using a representative voting rule, and such winners are the data instances kept in the reduced training set.
    
[^74]: 基于LSTM-DeepLabv3+和时空特征融合的贝叶斯优化城市洪水预测模型改进

    Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG])

    [http://arxiv.org/abs/2304.09994](http://arxiv.org/abs/2304.09994)

    本研究提出了一种基于CNN-RNN混合特征融合建模的贝叶斯优化城市洪水预测模型，实现了静态和动态的预测，并通过结合多个CNN和RNN模型，在精度上取得了显著提高。

    

    深度学习模型因其相对传统方法更高的准确性和效率，逐渐成为流行的洪水预测方法。但是，当前的机器学习方法通常依赖于单独的空间或时间特征分析，并对输入数据的类型、数量和维度存在限制。本研究提出了一个基于CNN-RNN的混合特征融合建模方法，用于城市洪水预测，将CNN在处理空间特征方面的优势和RNN在分析不同维度的时间序列方面的优势整合起来。这种方法允许进行静态和动态的洪水预测。应用贝叶斯优化来确定七个最具影响力的洪水驱动因素，并确定最佳组合策略。通过结合四个CNN（FCN，UNet，SegNet，DeepLabv3+）和三个RNN（LSTM，BiLSTM，GRU），最优混合模型被确定为LSTM-DeepLabv3+。该模型实现了最高的预测准确性（MAE、RMSE、NSE和KGE率）。

    Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE wer
    
[^75]: CKmeans和FCKmeans：使用拥挤距离的Kmeans算法的两种确定性初始化过程 （arXiv：2304.09989v1 [cs.LG]）

    CKmeans and FCKmeans : Two Deterministic Initialization Procedures For Kmeans Algorithm Using Crowding Distance. (arXiv:2304.09989v1 [cs.LG])

    [http://arxiv.org/abs/2304.09989](http://arxiv.org/abs/2304.09989)

    本论文提出了两种新型的确定性初始化过程（CKmeans和FCKmeans）以改进Kmeans聚类，并且实验证明这些过程在聚类准确度方面优于传统的Kmeans和Kmeans++。

    

    本文提出了两种基于修改后的拥挤距离的K-means聚类的新型确定性初始化过程，分别称为CKmeans和FCKmeans。这些过程利用更密集的点作为初始质心。在多个数据集上进行的实验研究表明，所提出的方法在聚类准确度方面优于Kmeans和Kmeans ++。CKmeans和FCKmeans的有效性归因于它们基于修改后的拥挤距离选择更好的初始质心的能力。总的来说，所提出的方法为改进K-means聚类提供了一种有希望的替代方案。

    This paper presents two novel deterministic initialization procedures for K-means clustering based on a modified crowding distance. The procedures, named CKmeans and FCKmeans, use more crowded points as initial centroids. Experimental studies on multiple datasets demonstrate that the proposed approach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. The effectiveness of CKmeans and FCKmeans is attributed to their ability to select better initial centroids based on the modified crowding distance. Overall, the proposed approach provides a promising alternative for improving K-means clustering.
    
[^76]: Tetra-NeRF：使用四面体表示的神经辐射场

    Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra. (arXiv:2304.09987v1 [cs.CV])

    [http://arxiv.org/abs/2304.09987](http://arxiv.org/abs/2304.09987)

    本文提出了一种基于四面体和 Delaunay 表示的自适应表示方法，用于神经辐射场，可实现高效的训练和最先进的结果，同时提供了更多接近表面的细节，并且实现了比基于点的表示更好的性能。

    

    神经辐射场 (NeRF) 是一种非常流行的方法，用于新视角合成和三维重构问题。NeRF 常用的场景表示是将场景的一致的基于体素的细分与 MLP 结合起来。本文根据观察到的场景的（稀疏）点云提出了一种基于 Delaunay 表示的自适应表示，而非一致的细分或基于点的表示。我们证明了这种表示可以实现高效的训练，获得最先进的结果。我们的方法巧妙地结合了三维几何处理、三角形渲染和现代神经辐射场的概念。与基于体素的表示相比，我们的方法提供了更多接近表面的场景细节。与基于点的表示相比，我们的方法实现了更好的性能。

    Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.
    
[^77]: 可解释的异质性幸存者偏差校正治疗效果研究，旨在指派出院后预防再住院的干预措施

    Interpretable (not just posthoc-explainable) heterogeneous survivor bias-corrected treatment effects for assignment of postdischarge interventions to prevent readmissions. (arXiv:2304.09981v1 [stat.ME])

    [http://arxiv.org/abs/2304.09981](http://arxiv.org/abs/2304.09981)

    该研究使用生存分析来评价出院后评估和管理（E/M）服务对防止再住院或死亡的影响，避免了机器学习中幸存者偏差的问题，并确定了个案管理服务在减少再住院方面最为有效，尤其对于出院后到长期护理机构的患者以及入院前有高资源利用率的患者。

    

    我们采用生存分析来量化出院后评估和管理服务在防止住院或死亡方面的影响。我们的方法避免了应用机器学习到这个问题上的一个特定陷阱，那就是因为幸存者偏差而导致的效果过高 -- 这种过高的估计可能与人群中的异质性混淆因素有关。这种偏差之所以产生，是因为为了在出院后接受干预，一个人必须在介入期内没有再次住院。在得出这种幻影效应的表达式后，我们在本质上可解释的贝叶斯生存框架中控制了这个和其他偏差。我们确定个案管理服务对于整体减少再住院具有最大的影响力，特别是对于出院后到长期护理机构的患者，在入院前一个季度有高资源利用率的患者。

    We used survival analysis to quantify the impact of postdischarge evaluation and management (E/M) services in preventing hospital readmission or death. Our approach avoids a specific pitfall of applying machine learning to this problem, which is an inflated estimate of the effect of interventions, due to survivors bias -- where the magnitude of inflation may be conditional on heterogeneous confounders in the population. This bias arises simply because in order to receive an intervention after discharge, a person must not have been readmitted in the intervening period. After deriving an expression for this phantom effect, we controlled for this and other biases within an inherently interpretable Bayesian survival framework. We identified case management services as being the most impactful for reducing readmissions overall, particularly for patients discharged to long term care facilities, with high resource utilization in the quarter preceding admission.
    
[^78]: 超越Transformer的函数学习

    Beyond Transformers for Function Learning. (arXiv:2304.09979v1 [cs.LG])

    [http://arxiv.org/abs/2304.09979](http://arxiv.org/abs/2304.09979)

    本文通过给Transformer模型增加两个简单的归纳学习偏见探究了学习和预测简单函数的能力，并在大型神经网络模型的背景下发现这些偏见的帮助，同时指出这些偏见可能有助于人类在外推能力方面的归纳学习。

    

    学习和预测简单函数的能力是人类智能的关键方面。最近的一些研究开始使用Transformer架构来探索这种能力，然而仍不清楚这是否足以重现人们在这个领域的外推能力。在这里，我们提出通过增加两个简单的归纳学习偏见来弥补这一差距，这些偏见直接来自于认知科学中最近的抽象推理模型。我们报道的结果表明，在大型神经网络模型的背景下，这些偏见是有帮助的，同时也揭示了可能有助于人类外推能力的归纳学习偏见的类型。

    The ability to learn and predict simple functions is a key aspect of human intelligence. Recent works have started to explore this ability using transformer architectures, however it remains unclear whether this is sufficient to recapitulate the extrapolation abilities of people in this domain. Here, we propose to address this gap by augmenting the transformer architecture with two simple inductive learning biases, that are directly adapted from recent models of abstract reasoning in cognitive science. The results we report demonstrate that these biases are helpful in the context of large neural network models, as well as shed light on the types of inductive learning biases that may contribute to human abilities in extrapolation.
    
[^79]: 无监督图神经网络解决肾移植问题

    Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision. (arXiv:2304.09975v1 [cs.LG])

    [http://arxiv.org/abs/2304.09975](http://arxiv.org/abs/2304.09975)

    本文提出了一个无监督的图神经网络解决肾移植问题，可以近似解决NP难问题，并且比其他方法更好。

    

    本文引入了一种新的基于学习的方法，近似解决了图上的肾移植问题（KEP），这是一个NP难问题。该问题涉及到从一组肾脏供体和等待肾脏捐赠的患者中，选择一组捐赠以最大化移植数量和质量，同时遵守一些有关这些捐赠安排的约束条件。该技术包括两个主要步骤：第一步是无监督训练的图神经网络（GNN）；第二步是确定性的非学习搜索启发式，它利用GNN的输出来查找路径和循环。为了进行比较，我们还实现并测试了一种精确的解决方案方法，使用整数规划、两个贪心搜索启发式，不使用机器学习模块的GNN。我们分析和比较了这些方法，并得出结论，即基于学习的两阶段方法是最佳解决方案。

    This paper introduces a new learning-based approach for approximately solving the Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problem consists of, given a pool of kidney donors and patients waiting for kidney donations, optimally selecting a set of donations to optimize the quantity and quality of transplants performed while respecting a set of constraints about the arrangement of these donations. The proposed technique consists of two main steps: the first is a Graph Neural Network (GNN) trained without supervision; the second is a deterministic non-learned search heuristic that uses the output of the GNN to find paths and cycles. To allow for comparisons, we also implemented and tested an exact solution method using integer programming, two greedy search heuristics without the machine learning module, and the GNN alone without a heuristic. We analyze and compare the methods and conclude that the learning-based two-stage approach is the best solution quality, outp
    
[^80]: 边缘服务器上的深度神经网络调度

    Scheduling DNNs on Edge Servers. (arXiv:2304.09961v1 [cs.NI])

    [http://arxiv.org/abs/2304.09961](http://arxiv.org/abs/2304.09961)

    本论文研究了如何加速为多个客户端运行边缘服务器DNN。批处理多个DNN请求可以显著加速处理时间。研究设计了一种新的调度算法，并开发了一种协作方法来调度多个边缘服务器的DNN请求，进一步提高了处理速度。

    

    深度神经网络(DNN)已经广泛用于各种视频分析任务中。这些任务要求实时响应，由于移动设备处理能力有限，支持此类实时分析的常见方法是将处理离线到边缘服务器。本文考察如何加速为多个客户端运行边缘服务器DNN。我们观察到，批处理多个DNN请求可以显著加速处理时间。基于此观察，我们首先设计了一种新的调度算法，以利用运行相同DNN的所有请求的批处理优势。这很有说服力，因为只有少数DNN，许多请求倾向于使用同一个DNN。我们的算法是通用的，可以支持不同的目标，如最小化完成时间或最大化及时率。然后，我们扩展我们的算法以处理使用不同DNN的具有或不具有共享层的请求。最后，我们开发了一种协作方法来调度多个边缘服务器的DNN请求，进一步提高了处理速度。

    Deep neural networks (DNNs) have been widely used in various video analytic tasks. These tasks demand real-time responses. Due to the limited processing power on mobile devices, a common way to support such real-time analytics is to offload the processing to an edge server. This paper examines how to speed up the edge server DNN processing for multiple clients. In particular, we observe batching multiple DNN requests significantly speeds up the processing time. Based on this observation, we first design a novel scheduling algorithm to exploit the batching benefits of all requests that run the same DNN. This is compelling since there are only a handful of DNNs and many requests tend to use the same DNN. Our algorithms are general and can support different objectives, such as minimizing the completion time or maximizing the on-time ratio. We then extend our algorithm to handle requests that use different DNNs with or without shared layers. Finally, we develop a collaborative approach to 
    
[^81]: 大规模语言模型中潜在空间理论对应新兴能力

    A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])

    [http://arxiv.org/abs/2304.09960](http://arxiv.org/abs/2304.09960)

    本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。

    

    语言并不是随机生成，而是为了传递信息。语言与其底层含义之间存在强烈的关联，在其相关性方面有着严重偏差的稀疏联合分布。此外，由于稀疏性，这些高峰值恰好与语言的边缘分布匹配。随着大数据和大模型上训练的LLMs的出现，我们现在可以精确评估语言的边缘分布，这提供了一种方便的探索联合分布稀疏结构实现有效推理的方式。在本文中，我们将语言分类为明确与{\epsilon}-模糊，并提出定量结果，以表明LLMs的新兴能力（例如语言理解、上下文学习、思路启发以及有效指令微调）都可以归因于对稀疏联合分布进行贝叶斯推断。

    Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
    
[^82]: 在线模型集成对最优预测性能的应用和行业轮换策略

    Online Ensemble of Models for Optimal Predictive Performance with Applications to Sector Rotation Strategy. (arXiv:2304.09947v1 [q-fin.ST])

    [http://arxiv.org/abs/2304.09947](http://arxiv.org/abs/2304.09947)

    通过机器学习模型和资产特定因素在预测行业回报和测量行业特定风险溢价方面获得更大经济收益，开发了一种新型在线集成算法来学习优化预测性能，特别适用于时间序列问题和可能的黑盒模型系统。

    

    资产特定因素通常用于预测金融回报并量化资产特定风险溢价。我们使用各种机器学习模型证明，这些因素包含的信息可以在预测行业回报和测量行业特定风险溢价方面带来更大的经济收益。为了利用不同行业表现的单个模型的强预测结果，我们开发了一种新型在线集成算法，该算法学习优化预测性能。该算法随着时间的推移不断适应，通过分析它们最近的预测性能来确定个体模型的最佳组合。这使它特别适用于时间序列问题，滚动窗口回测程序和可能的黑盒模型系统。我们推导出最优增益函数，用样本外R平方度量表达相应的遗憾界，并推导出最优解。

    Asset-specific factors are commonly used to forecast financial returns and quantify asset-specific risk premia. Using various machine learning models, we demonstrate that the information contained in these factors leads to even larger economic gains in terms of forecasts of sector returns and the measurement of sector-specific risk premia. To capitalize on the strong predictive results of individual models for the performance of different sectors, we develop a novel online ensemble algorithm that learns to optimize predictive performance. The algorithm continuously adapts over time to determine the optimal combination of individual models by solely analyzing their most recent prediction performance. This makes it particularly suited for time series problems, rolling window backtesting procedures, and systems of potentially black-box models. We derive the optimal gain function, express the corresponding regret bounds in terms of the out-of-sample R-squared measure, and derive optimal le
    
[^83]: 通过机器学习预测股票价格和商业周期

    Stock Price Predictability and the Business Cycle via Machine Learning. (arXiv:2304.09937v1 [q-fin.ST])

    [http://arxiv.org/abs/2304.09937](http://arxiv.org/abs/2304.09937)

    研究发现，机器学习模型在大多数衰退期间的表现较差，与稳定市场有关的因素可能是其表现优异的原因。因此，我们建议ML从业者在衰退和复苏期间评估其模型。

    

    本研究探讨了商业周期对机器学习（ML）预测的影响。使用标普500指数，我们发现在大多数衰退期间，ML模型的表现较差，而衰退历史或无风险利率的包含并不能 necessarily 提高其性能。在调查模型表现良好的衰退时，我们发现它们的市场波动性较其他衰退期较低。这意味着表现提高不是由于ML方法的优点，而是因为有效的货币政策稳定了市场。我们建议ML从业者在衰退和复苏期间评估其模型。

    We study the impacts of business cycles on machine learning (ML) predictions. Using the S&P 500 index, we find that ML models perform worse during most recessions, and the inclusion of recession history or the risk-free rate does not necessarily improve their performance. Investigating recessions where models perform well, we find that they exhibit lower market volatility than other recessions. This implies that the improved performance is not due to the merit of ML methods but rather factors such as effective monetary policies that stabilized the market. We recommend that ML practitioners evaluate their models during both recessions and expansions.
    
[^84]: 使用技术分析和ML/DL模型识别交易

    Identifying Trades Using Technical Analysis and ML/DL Models. (arXiv:2304.09936v1 [q-fin.ST])

    [http://arxiv.org/abs/2304.09936](http://arxiv.org/abs/2304.09936)

    本文讲述了如何使用ML/DL模型预测股票价格，以帮助投资者和金融机构做出明智的决策和制定有效的风险管理政策。

    

    预测股市价格的重要性不言而喻。对于投资者和金融机构来说，这是一个关键任务，因为它使他们能够做出明智的投资决策，管理风险并确保金融系统的稳定。准确的股市预测可以帮助投资者最大化收益并尽量减少损失，而金融机构可以利用这些信息制定有效的风险管理政策。然而，由于股市的复杂性和影响股票价格的诸多因素，股市预测是一项具有挑战性的任务。因此，越来越多的先进技术，如深度学习，被用于分析海量数据并洞察股市运行行为。虽然深度学习在准确预测股票价格方面表现出了很大的潜力，但在这个领域仍有许多研究需要进行。

    The importance of predicting stock market prices cannot be overstated. It is a pivotal task for investors and financial institutions as it enables them to make informed investment decisions, manage risks, and ensure the stability of the financial system. Accurate stock market predictions can help investors maximize their returns and minimize their losses, while financial institutions can use this information to develop effective risk management policies. However, stock market prediction is a challenging task due to the complex nature of the stock market and the multitude of factors that can affect stock prices. As a result, advanced technologies such as deep learning are being increasingly utilized to analyze vast amounts of data and provide valuable insights into the behavior of the stock market. While deep learning has shown promise in accurately predicting stock prices, there is still much research to be done in this area.
    
[^85]: 个性化状态焦虑检测：基于语言生物标志物和机器学习流程的实证研究

    Personalized State Anxiety Detection: An Empirical Study with Linguistic Biomarkers and A Machine Learning Pipeline. (arXiv:2304.09928v1 [cs.HC])

    [http://arxiv.org/abs/2304.09928](http://arxiv.org/abs/2304.09928)

    该论文研究了个性化状态焦虑检测，利用数字生物标志物和机器学习技术检测状态焦虑，并通过多层个性化机器学习流程，考虑上下文，从而更好地捕捉个体之间的心理和行为反应的差异。

    

    高社交焦虑症状的人往往在社交场合中表现出较高的状态焦虑。研究表明，利用数字生物标志物和机器学习技术可以检测状态焦虑。然而，大多数现有工作是在整个参与者群体上训练模型，未能捕捉到他们的心理和行为反应在社交环境中的个体差异。为了解决这个问题，在第一项研究中，我们收集了来自35名高社交焦虑参与者在各种社交环境中的语言数据，发现数字语言生物标志物在评价性与非评价性社交环境下以及在具有不同特质心理症状的个体之间显著不同，这表明个性化方法检测状态焦虑的重要性。在第二项研究中，我们使用相同的数据和第一项研究的结果来建立多层个性化机器学习流程来检测状态焦虑，考虑上下文。

    Individuals high in social anxiety symptoms often exhibit elevated state anxiety in social situations. Research has shown it is possible to detect state anxiety by leveraging digital biomarkers and machine learning techniques. However, most existing work trains models on an entire group of participants, failing to capture individual differences in their psychological and behavioral responses to social contexts. To address this concern, in Study 1, we collected linguistic data from N=35 high socially anxious participants in a variety of social contexts, finding that digital linguistic biomarkers significantly differ between evaluative vs. non-evaluative social contexts and between individuals having different trait psychological symptoms, suggesting the likely importance of personalized approaches to detect state anxiety. In Study 2, we used the same data and results from Study 1 to model a multilayer personalized machine learning pipeline to detect state anxiety that considers contextu
    
[^86]: 柿子政治的面孔：使用机器学习比较政治领袖面部情感表达的差异

    The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])

    [http://arxiv.org/abs/2304.09914](http://arxiv.org/abs/2304.09914)

    本文使用机器学习的算法分析了来自15个不同国家的220个政治领袖的YouTube视频，总结了政治领袖面部情感表达的差异。

    

    网络媒体已经彻底改变了政治信息在全球范围内的传播和消费方式，这种转变促使政治人物采取新的策略来捕捉和保持选民的注意力。这些策略往往依赖于情感说服和吸引。随着虚拟空间中视觉内容越来越普遍，很多政治沟通也被标志着唤起情感的视频内容和图像。本文提供了一种新的分析方法。我们将基于现有训练好的卷积神经网络架构提供的Python库fer，应用一种基于深度学习的计算机视觉算法，对描绘来自15个不同国家的政治领袖的220个YouTube视频样本进行分析。该算法返回情绪分数，每一帧都代表6种情绪状态（愤怒，厌恶，恐惧，快乐，悲伤和惊讶）和一个中性表情。

    Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
    
[^87]: 用随机梯度下降法量子内核匹配

    Quantum Kernel Alignment with Stochastic Gradient Descent. (arXiv:2304.09899v1 [quant-ph])

    [http://arxiv.org/abs/2304.09899](http://arxiv.org/abs/2304.09899)

    本论文研究了量子内核匹配问题，在Pegasos算法的基础上使用随机梯度下降法解决了该问题，并展示了其在非稳定化内核方面的高效性。

    

    量子支持向量机有望实现解决某些机器学习问题的量子加速。其中的关键挑战是找到给定数据集的好的量子内核，这个任务被称为内核匹配。在这篇论文中，我们使用Pegasos算法来研究这个问题，Pegasos是一种使用随机梯度下降算法来解决支持向量机优化问题的算法。我们将Pegasos拓展到量子场景，并展示其在内核匹配方面的有效性。与以前通过在外部优化循环中训练QSVM来执行内核匹配的工作不同，我们展示了使用Pegasos同步训练支持向量机和匹配内核的可能性。我们的实验表明，这种方法能够以高精度对齐量子特征映射，并且优于现有的量子内核对齐技术。特别是，我们证明了Pegasos在非稳定化内核方面特别有效，在那里传统方法无法很好地工作。

    Quantum support vector machines have the potential to achieve a quantum speedup for solving certain machine learning problems. The key challenge for doing so is finding good quantum kernels for a given data set -- a task called kernel alignment. In this paper we study this problem using the Pegasos algorithm, which is an algorithm that uses stochastic gradient descent to solve the support vector machine optimization problem. We extend Pegasos to the quantum case and and demonstrate its effectiveness for kernel alignment. Unlike previous work which performs kernel alignment by training a QSVM within an outer optimization loop, we show that using Pegasos it is possible to simultaneously train the support vector machine and align the kernel. Our experiments show that this approach is capable of aligning quantum feature maps with high accuracy, and outperforms existing quantum kernel alignment techniques. Specifically, we demonstrate that Pegasos is particularly effective for non-stationar
    
[^88]: GREAT分数：使用生成模型对对抗性扰动进行全局鲁棒性评估

    GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])

    [http://arxiv.org/abs/2304.09875](http://arxiv.org/abs/2304.09875)

    本文提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。该分数捕捉了所有样本中的平均认证防攻击扰动水平，无需运行对抗性攻击。

    

    目前对于对抗性鲁棒性的研究主要集中在聚合一组数据样本的局部鲁棒性结果上，以评估和排名不同的模型。然而，局部统计量可能无法很好地代表基础未知数据分布的真正全局鲁棒性。为了解决这一挑战，本文首次尝试提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。GREAT分数正式具有一个全局统计量的物理意义，捕捉来自生成模型的所有样本中的平均认证防攻击扰动水平。对于有限样本评估，我们还推导出样本复杂度和样本均值与真实均值之间的概率保证。GREAT分数有几个优点：（1）使用GREAT分数进行鲁棒性评估高效而且规模可扩展，无需运行对抗性攻击。

    Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
    
[^89]: 基于深度函数的偏序集合的描述性分析和机器学习算法

    Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms. (arXiv:2304.09872v1 [cs.LG])

    [http://arxiv.org/abs/2304.09872](http://arxiv.org/abs/2304.09872)

    本文提出了一种基于深度函数的偏序集合描述性分析框架，并引入了偏序版本的单纯深度，用于比较基于多维性能度量的机器学习算法。实验证明此方法与现有基准方法不同，为分类器比较提供了新的视角。

    

    我们提出了一个框架，基于深度函数对偏序集合进行描述性分析。尽管深度函数在线性和度量空间中进行了大量研究，但是对于偏序等非标准数据类型的深度函数的讨论却很少。我们介绍了著名的单纯深度的偏序版本-无并通用深度（ufg depth）。此外，我们利用我们的 ufg depth 来比较基于多维性能度量的机器学习算法。具体地，我们分析不同分类器在标准基准数据集样本上的表现分布。我们的结果有希望地证明了我们的方法与现有基准方法有很大不同，因此为分类器比较的激烈辩论增加了新的视角。

    We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies of depth functions in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we analyze the distribution of different classifier performances over a sample of standard benchmark data sets. Our results promisingly demonstrate that our approach differs substantially from existing benchmarking approaches and, therefore, adds a new perspective to the vivid debate on the comparison of classifiers.
    
[^90]: 大规模机器学习中Adam不稳定性的理论研究

    A Theory on Adam Instability in Large-Scale Machine Learning. (arXiv:2304.09871v1 [cs.LG])

    [http://arxiv.org/abs/2304.09871](http://arxiv.org/abs/2304.09871)

    Adam优化算法在大批量的训练下容易出现不稳定现象，并导致训练异常，作者提出了该现象的理论解释。

    

    本文提出了一个之前未被解释的现象的理论，该现象出现在大型语言模型训练时的发散行为中。我们认为这种现象是由于主流的优化算法 Adam 导致的。我们观察到 Adam 可能会进入一种状态，其中参数更新向量有比较大的范数，并且与训练损失景观下的下降方向基本无关，从而导致发散。这种现象更容易在大批量情况下出现，这也是大型语言模型训练的典型设置。为了证明该理论，我们对规模不同的语言模型（70亿，300亿，650亿和5460亿参数）进行了训练运行的观察。

    We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.
    
[^91]: 异构智能体强化学习

    Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])

    [http://arxiv.org/abs/2304.09870](http://arxiv.org/abs/2304.09870)

    提出了一种异构智能体强化学习（HARL）算法，解决了协作多智能体强化学习中的参数共享限制，同时通过引入多智能体优势分解引理和序列更新方案，建立了异构智能体信任区域学习（HATRL）算法及其易处理的逼近方式 HATRPO 和 HAPPO。此外，发现了一种名为异构智能体镜像学习（HAML）的新型框架，加强了对HATRPO和HAPPO的理论保证。

    

    协作多智能体强化学习（MARL）在人工智能研究中越来越受欢迎，然而，许多研究仍然严重依赖于智能体之间的参数共享，这将它们限制在同质异构智能体设置下，从而导致训练不稳定和缺乏收敛保证。为了在一般的异构智能体设置下实现有效的协作，我们提出了解决上述问题的异构智能体强化学习（HARL）算法。我们的发现核心是多智能体优势分解引理和序列更新方案。基于这些，我们开发了经过验证的无参数共享约束的异构智能体信任区域学习（HATRL）算法，并通过易处理的逼近方式得出了HATRPO和HAPPO。此外，我们发现了一种名为异构智能体镜像学习（HAML）的新型框架，它加强了对HATRPO和HAPPO的理论保证。

    The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP
    
[^92]: 进化约束强化学习策略

    Evolving Constrained Reinforcement Learning Policy. (arXiv:2304.09869v1 [cs.NE])

    [http://arxiv.org/abs/2304.09869](http://arxiv.org/abs/2304.09869)

    本文提出了一种新颖的进化约束强化学习算法，采用随机排名自适应平衡奖励和约束违规，并同时通过维护一组拉格朗日松弛系数及一个约束缓冲区来限制策略行为。在机器人控制基准测试中取得优异性能。

    

    进化算法已被用于演化出一组执行者，以产生多样化的体验来训练强化学习智能体，从而解决时间信用分配问题并提高探索效率。然而，当将这种方法应用于解决约束问题时，很难平衡奖励和约束违规之间的权衡。本文提出了一种新颖的进化约束强化学习（ECRL）算法，采用随机排名自适应平衡奖励和约束违规，并同时通过维护一组拉格朗日松弛系数及一个约束缓冲区来限制策略行为。在机器人控制基准测试中进行的广泛实验表明，我们的ECRL相比最先进的算法取得了优异的性能。消融分析表明引入随机排名和约束缓冲区的优势。

    Evolutionary algorithms have been used to evolve a population of actors to generate diverse experiences for training reinforcement learning agents, which helps to tackle the temporal credit assignment problem and improves the exploration efficiency. However, when adapting this approach to address constrained problems, balancing the trade-off between the reward and constraint violation is hard. In this paper, we propose a novel evolutionary constrained reinforcement learning (ECRL) algorithm, which adaptively balances the reward and constraint violation with stochastic ranking, and at the same time, restricts the policy's behaviour by maintaining a set of Lagrange relaxation coefficients with a constraint buffer. Extensive experiments on robotic control benchmarks show that our ECRL achieves outstanding performance compared to state-of-the-art algorithms. Ablation analysis shows the benefits of introducing stochastic ranking and constraint buffer.
    
[^93]: 通过保留谱的数据压缩加速支持向量聚类

    Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])

    [http://arxiv.org/abs/2304.09868](http://arxiv.org/abs/2304.09868)

    本文介绍了一种通过保留谱的数据压缩来加速支持向量聚类的方法，可以显著提高聚类速度而不牺牲聚类质量。

    

    支持向量聚类是一种重要的聚类方法，但是由于其计算昂贵的簇分配步骤，它面临着可伸缩性问题。在本文中，我们通过保留谱的数据压缩来加速支持向量聚类。具体而言，我们将原始数据集压缩成少量谱表示的聚合数据点，然后在压缩后的数据集上执行标准的支持向量聚类，最后将压缩数据集的聚类结果映射回原始数据集以发现簇。我们在真实数据集上的大量实验结果表明，相较于标准支持向量聚类，我们的方法大大提高了速度，而不会损失聚类质量。

    Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
    
[^94]: 基于图神经网络的河流系统异常检测

    Graph Neural Network-Based Anomaly Detection for River Network Systems. (arXiv:2304.09367v1 [cs.LG])

    [http://arxiv.org/abs/2304.09367](http://arxiv.org/abs/2304.09367)

    本研究采用图神经网络模型Graph Deviation Network (GDN)来捕捉河流传感器数据的复杂时空关系，并提出了备用异常阈值标准GDN+，以实现对水质的准确持续监测。

    

    水是河流网络的生命线，其质量对维护水生态系统和人类社会都有着至关重要的作用。现场传感器技术越来越依赖实时监测水质。异常检测是识别传感器数据中错误模式的关键步骤，但由于数据复杂性和变异性，即使在正常情况下也是一项具有挑战性的任务。本文提出了一种针对河流网络传感器数据异常检测的解决方案，这对于精确持续监测水质非常重要。我们使用图神经网络模型——最近提出的Graph Deviation Network (GDN)，它利用基于图注意力的预测来捕捉传感器之间复杂的时空关系。我们根据所学图形提出了模型GDN+的备用异常阈值标准。为了评估模型的有效性，我们引入了新的基准仿真实验。

    Water is the lifeblood of river networks, and its quality plays a crucial role in sustaining both aquatic ecosystems and human societies. Real-time monitoring of water quality is increasingly reliant on in-situ sensor technology. Anomaly detection is crucial for identifying erroneous patterns in sensor data, but can be a challenging task due to the complexity and variability of the data, even under normal conditions. This paper presents a solution to the challenging task of anomaly detection for river network sensor data, which is essential for the accurate and continuous monitoring of water quality. We use a graph neural network model, the recently proposed Graph Deviation Network (GDN), which employs graph attention-based forecasting to capture the complex spatio-temporal relationships between sensors. We propose an alternate anomaly threshold criteria for the model, GDN+, based on the learned graph. To evaluate the model's efficacy, we introduce new benchmarking simulation experimen
    
[^95]: 多模式传感器融合技术在DED打印SS316L部件中的原位表面孔隙率预测

    In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion. (arXiv:2304.08658v1 [physics.app-ph])

    [http://arxiv.org/abs/2304.08658](http://arxiv.org/abs/2304.08658)

    本研究利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力，可以实时预测每个沃克塞尔中的气孔存在，是一个重大飞跃。

    

    本研究旨在将声发射（AE）等多模式传感器数据中的时频模式与DED过程中的孔隙率形成进行高空间（0.5mm）和时间（<1ms）的关联。通过采用可解释的AI方法中的LIME（局部可解释性非特定性解释），将AE中的某些高频波形特征归因于DED过程中的两个主要孔隙形成途径：飞溅事件和低热量输入下相邻打印轨迹的不充分熔合。该方法为实时预测每个沃克塞尔（0.5mm）中的气孔存在提供了令人兴奋的可能性，这是与先前努力相比的一个重大飞跃。在打印并随后加工SS316L材料样品时，同步采集了包括力，AE，振动和温度在内的多模式传感器数据。深度卷积神经网络分类器用于识别两种孔隙形成途径的AE特征，然后使用可解释AI方法进一步分析这些特征。结果表明，利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力。

    This study aims to relate the time-frequency patterns of acoustic emission (AE) and other multi-modal sensor data collected in a hybrid directed energy deposition (DED) process to the pore formations at high spatial (0.5 mm) and time (< 1ms) resolutions. Adapting an explainable AI method in LIME (Local Interpretable Model-Agnostic Explanations), certain high-frequency waveform signatures of AE are to be attributed to two major pathways for pore formation in a DED process, namely, spatter events and insufficient fusion between adjacent printing tracks from low heat input. This approach opens an exciting possibility to predict, in real-time, the presence of a pore in every voxel (0.5 mm in size) as they are printed, a major leap forward compared to prior efforts. Synchronized multimodal sensor data including force, AE, vibration and temperature were gathered while an SS316L material sample was printed and subsequently machined. A deep convolution neural network classifier was used to ide
    
[^96]: 二元积分布的多项式时间和纯差分隐私估计器

    A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])

    [http://arxiv.org/abs/2304.06787](http://arxiv.org/abs/2304.06787)

    本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。

    

    我们提出了第一个ε-差分隐私、计算有效的算法，可以在总变化距离下准确地估计$\{0,1\}^d$上的乘积分布的均值，同时在多项式对数因子内获得了最优的样本复杂度。之前的工作要么在更弱的隐私概念下有效地解决了这个问题，要么在指数级运行时间内最优地解决了这个问题。

    We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
    
[^97]: 物理信息径向基网络（PIRBN）：用于求解非线性偏微分方程的局部逼近神经网络

    Physics-informed radial basis network (PIRBN): A local approximation neural network for solving nonlinear PDEs. (arXiv:2304.06234v1 [cs.LG])

    [http://arxiv.org/abs/2304.06234](http://arxiv.org/abs/2304.06234)

    PIRBN是一种局部逼近神经网络，适用于求解具有高频特征和不适定计算域的PDE方程，相比PINN更加高效有效。通过使用梯度下降法训练PIRBN可以收敛到高斯过程。

    

    我们最近的深入研究发现，经过训练后的物理信息神经网络（PINN）往往是局部逼近器。这一观察结果引发了这种新型的物理信息径向基网络（PIRBN），它可以在整个训练过程中保持局部特性。与深度神经网络相比，PIRBN只包含一个隐藏层和一个径向基“激活”函数。在适当的条件下，我们证明了使用梯度下降法训练PIRBN可以收敛到高斯过程。此外，我们通过神经切线核（NTK）理论研究了PIRBN的训练动力学。此外，我们还对PIRBN的初始化策略进行了全面调查。基于数值例子，PIRBN已被证明比PINN在解决具有高频特征和不适定计算域的PDE方程方面更有效和高效。此外，现有的PINN数字技术，例如ad...

    Our recent intensive study has found that physics-informed neural networks (PINN) tend to be local approximators after training. This observation leads to this novel physics-informed radial basis network (PIRBN), which can maintain the local property throughout the entire training process. Compared to deep neural networks, a PIRBN comprises of only one hidden layer and a radial basis "activation" function. Under appropriate conditions, we demonstrated that the training of PIRBNs using gradient descendent methods can converge to Gaussian processes. Besides, we studied the training dynamics of PIRBN via the neural tangent kernel (NTK) theory. In addition, comprehensive investigations regarding the initialisation strategies of PIRBN were conducted. Based on numerical examples, PIRBN has been demonstrated to be more effective and efficient than PINN in solving PDEs with high-frequency features and ill-posed computational domains. Moreover, the existing PINN numerical techniques, such as ad
    
[^98]: 使用多数据因果推断选择机器学习应用的强健特征

    Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])

    [http://arxiv.org/abs/2304.05294](http://arxiv.org/abs/2304.05294)

    本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。

    

    强健的特征选择对于创建可靠和可解释的机器学习（ML）模型至关重要。在领域知识有限、潜在交互未知的情况下设计统计预测模型时，选择最优特征集通常很困难。为了解决这个问题，我们引入了一种多数据（M）因果特征选择方法，它同时处理一组时间序列数据集，并生成一个单一的因果驱动集。该方法使用Tigramite Python包中实现的因果发现算法PC1或PCMCI。这些算法利用条件独立性测试推断因果图的部分。我们的因果特征选择方法在将剩余因果特征作为输入传递给ML模型（多元线性回归，随机森林）预测目标之前，过滤掉因果虚假链接。我们将该框架应用于预测西太平洋热带地区的地震强度。

    Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
    
[^99]: 一种基于数据驱动的状态聚合方法用于动态离散选择模型

    A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models. (arXiv:2304.04916v1 [cs.LG])

    [http://arxiv.org/abs/2304.04916](http://arxiv.org/abs/2304.04916)

    本文提出了一种基于数据驱动的状态聚合方法来降低动态离散选择模型的估计计算和样本复杂度，首先利用反向强化学习估计代理Q函数，然后用聚类算法选择重要的状态聚合，最终利用嵌套固定点算法进行最大似然估计。

    

    我们研究了动态离散选择模型，其中一个常见的问题是使用代理行为数据估计代理奖励函数（也称为“结构参数”）的参数。这种模型的最大似然估计需要动态规划，这受到维度灾难的限制。在本文中，我们提出了一种新颖的算法，提供了一种数据驱动的方法来选择和聚合状态，降低了估计的计算和样本复杂度。我们的方法分两个阶段。在第一阶段中，我们使用灵活的反向强化学习方法来估计代理Q函数。我们使用这些估计的Q函数，以及一个聚类算法，选择了一些最为重要的状态，这些状态对于驱动Q函数的变化最为关键。在第二阶段，利用这些被选择的“聚合”状态，我们使用常用的嵌套固定点算法进行最大似然估计。所提出的二阶段方法实现了...

    We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as "structural" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected "aggregated" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach 
    
[^100]: SAM与BET：基于深度学习的磁共振图像脑提取和分割的比较研究

    SAM vs BET: A Comparative Study for Brain Extraction and Segmentation of Magnetic Resonance Images using Deep Learning. (arXiv:2304.04738v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.04738](http://arxiv.org/abs/2304.04738)

    该论文比较了采用深度学习模型的SAM与传统方法BET在MRI脑提取方面的效果，结果表明SAM在信号不均匀、非等向性分辨率或病变靠近脑外区域和脑膜时效果更佳，SAM表现出更准确、更健壮和更多样化的潜力。

    

    脑提取是神经成像研究中的关键预处理步骤，涉及使用MRI数据将脑组织与非脑组织分离。FSL的脑提取工具（BET）是当前的金标准，但由于图像质量问题容易出错。Meta AI的Segment Anything Model（SAM）显示出有希望的零样本分割潜力。本文比较了SAM和BET在不同的脑扫描中的脑提取效果，考虑了图像质量、MRI序列和病变位置。结果表明，在各种评估参数方面，SAM胜过BET，特别是在信号不均匀、非等向性体素分辨率或病变靠近脑外区域和脑膜的情况下。SAM优越的性能表明其在脑提取和分割应用中具有更准确、更健壮和更多样化的潜力。

    Brain extraction is a critical preprocessing step in neuroimaging studies, involving the separation of brain tissue from non-brain tissue using MRI data. FSL's Brain Extraction Tool (BET) is the current gold standard but is prone to errors due to image quality issues. The Segment Anything Model (SAM) by Meta AI has shown promising zero-shot segmentation potential. This paper compares SAM with BET for brain extraction on diverse brain scans, considering image quality, MRI sequences, and lesion locations. Results demonstrate that SAM outperforms BET in various evaluation parameters, particularly in cases with signal inhomogeneities, non-isotropic voxel resolutions, or lesions near the brain's outer regions and meninges. SAM's superior performance indicates its potential as a more accurate, robust, and versatile tool for brain extraction and segmentation applications.
    
[^101]: TransPimLib：用于处理器内存系统上高效的超越函数的库

    TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])

    [http://arxiv.org/abs/2304.01951](http://arxiv.org/abs/2304.01951)

    TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。

    

    处理器内存系统（PIM）承诺减轻现代计算系统中的数据移动瓶颈。然而，现有的真实PIM系统有一个内在的劣势，即它们的硬件比传统的处理器（CPU、GPU）更加受限，因为在内存附近或内部构建处理元件的难度和成本很高。因此，通用的PIM架构支持相当有限的指令集，并且难以执行复杂的操作，例如超越函数和其他难以计算的操作（例如平方根）。这些操作对于一些现代工作负载尤其重要，例如机器学习应用中的激活函数。为了在通用的PIM系统中提供对超越（和其他难以计算）函数的支持，我们介绍了TransPimLib，这是一个库，提供基于CORDIC和LUT的三角函数、双曲函数、指数、对数、平方根等难以计算的函数的方法。

    Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
    
[^102]: 机器学习在经济研究中的应用：何时、什么和如何运用？

    Machine Learning for Economics Research: When What and How?. (arXiv:2304.00086v1 [econ.GN])

    [http://arxiv.org/abs/2304.00086](http://arxiv.org/abs/2304.00086)

    本文精选综述了使用机器学习工具进行经济学研究和政策分析的文章，强调了机器学习在处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性方面的应用优势，成为计量经济学家工具箱中不可或缺的一部分。

    

    本文对使用机器学习工具进行经济学研究和政策分析的重要经济期刊上发表的文章进行了精选综述。综述回答了三个关键问题：（1）何时在经济学中使用机器学习，（2）常用的机器学习模型是什么，以及（3）如何将它们用于经济应用。综述强调了机器学习特别适用于处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性。深度学习模型适用于非传统数据，而集成学习模型适用于传统数据集。尽管传统的计量经济学模型在分析低复杂性数据时可能足够，但由于快速数字化和不断增长的文献，经济数据的复杂性增加，机器学习正成为计量经济学家工具箱中不可或缺的一部分。

    This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used in processing nontraditional and unstructured data, capturing strong nonlinearity, and improving prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggest that ML is becoming an essential addition to the econometrician's toolbox.
    
[^103]: 深度ReLU神经网络在过参数化情况下的贝叶斯自由能

    Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases. (arXiv:2303.15739v1 [cs.LG])

    [http://arxiv.org/abs/2303.15739](http://arxiv.org/abs/2303.15739)

    本研究针对深度ReLU神经网络，证明了过参数化情况下的Bayesian自由能是有界的，说明Bayesian广义误差不会增加。

    

    在人工智能的许多研究领域中，深度神经网络已被证明可用于估计高维输入空间中的未知函数。然而，它们的泛化性能尚未从理论角度完全澄清，因为它们是不可识别的和奇异的学习机器。此外，ReLU函数不可微，奇异学习理论中的代数或解析方法无法应用于它。本文研究了一种过参数化情况下的深度ReLU神经网络，并证明了Bayesian自由能是有界的，即使层数比估计未知数据生成函数所必需的层数更多。由于Bayesian广义误差等于样本大小的自由能增加，因此我们的结果也表明，Bayesian广义误差不会增加。

    In many research fields in artificial intelligence, it has been shown that deep neural networks are useful to estimate unknown functions on high dimensional input spaces. However, their generalization performance is not yet completely clarified from the theoretical point of view because they are nonidentifiable and singular learning machines. Moreover, a ReLU function is not differentiable, to which algebraic or analytic methods in singular learning theory cannot be applied. In this paper, we study a deep ReLU neural network in overparametrized cases and prove that the Bayesian free energy, which is equal to the minus log marginal likelihoodor the Bayesian stochastic complexity, is bounded even if the number of layers are larger than necessary to estimate an unknown data-generating function. Since the Bayesian generalization error is equal to the increase of the free energy as a function of a sample size, our result also shows that the Bayesian generalization error does not increase ev
    
[^104]: 未知嗅探器用于目标检测：不要对未知对象视而不见

    Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])

    [http://arxiv.org/abs/2303.13769](http://arxiv.org/abs/2303.13769)

    本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。

    

    近期提出的开放世界目标和开放集检测在寻找从未见过的物体并将其与已知类别区分开方面取得了突破。然而，他们对从已知类别向未知类别的知识传递的研究需要更深入，从而导致探测隐藏在背景中的未知物体的能力不足。本文中，我们提出了未知嗅探器(UnSniffer)来寻找未知和已知的目标。首先，引入广义物体置信度(GOC)分数，仅使用已知类别样本进行监督和避免在背景中不适当地压制未知物体。值得注意的是，从已知物体学习到的这种置信度分数可以推广到未知物体。此外，我们提出了负能量抑制损失来进一步限制背景中非物体样本。接下来，在推断过程中由于缺乏它们在训练中的语义信息，难以获得每个未知目标的最佳框。为了解决这个问题，

    The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
    
[^105]: MCTS-GEB：蒙特卡洛树搜索是一个好的E图构建器

    MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder. (arXiv:2303.04651v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04651](http://arxiv.org/abs/2303.04651)

    MCTS-GEB是一个通用的重写系统，使用强化学习和蒙特卡洛树搜索来构建最优的E图，有效消除了E图构建中的顺序问题，并在评估中表现出很好的性能。

    

    重写系统广泛使用等式饱和技术来优化重写顺序，但是当E图没有饱和时，无法代表所有可能的重写机会，会重新引入问题。为了解决这个问题，我们提出了MCTS-GEB，一个应用强化学习于E图构建的通用重写系统。MCTS-GEB使用蒙特卡洛树搜索（MCTS）高效规划最优的E图构建，有效地消除了E图构建阶段的顺序问题，并且在合理时间内取得了更好的性能。在两个不同领域的评估中，MCTS-GEB都表现出很好的性能。

    Rewrite systems [6, 10, 12] have been widely employing equality saturation [9], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains 
    
[^106]: 基于集成的读出函数的图级表示

    Graph-level representations using ensemble-based readout functions. (arXiv:2303.02023v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02023](http://arxiv.org/abs/2303.02023)

    本文介绍了一种基于集成的读出函数，能够将节点级表示转换为图级向量，并能够在分子应用中取得良好表现。

    

    图机器学习模型已经成功应用于多个应用领域。最突出的一类模型——图神经网络（GNNs）提供了一种优雅的方式来提取表达性的节点级表示向量，这些向量可以用来解决节点相关的问题，例如在社交网络中对用户进行分类。然而，许多任务需要整个图的表示，例如在分子应用中。为了将节点级表示转换为图级向量，必须应用所谓的读出函数。在这项工作中，我们研究了现有的读出方法，包括简单的非可训练方法以及复杂的参数化模型。我们引入了集成读出函数的概念，这些函数可以结合表示或预测。我们的实验表明，这样的集成可以比简单单一的读出函数表现得更好，或者与复杂的参数化方法表现相似，但开销只有它们的一小部分。

    Graph machine learning models have been successfully deployed in a variety of application areas. One of the most prominent types of models - Graph Neural Networks (GNNs) - provides an elegant way of extracting expressive node-level representation vectors, which can be used to solve node-related problems, such as classifying users in a social network. However, many tasks require representations at the level of the whole graph, e.g., molecular applications. In order to convert node-level representations into a graph-level vector, a so-called readout function must be applied. In this work, we study existing readout methods, including simple non-trainable ones, as well as complex, parametrized models. We introduce a concept of ensemble-based readout functions that combine either representations or predictions. Our experiments show that such ensembles allow for better performance than simple single readouts or similar performance as the complex, parametrized ones, but at a fraction of the m
    
[^107]: 通过受限代理学习控制深度序数分类中的类布局

    Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00396](http://arxiv.org/abs/2303.00396)

    本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。

    

    对于深度序数分类任务，学习特定于序数分类的良好结构化特征空间有助于恰当地捕捉类之间的序数属性。本文提出了一种新颖的受限代理学习方法，该方法可以为每个序数类学习一个代理，然后通过限制这些代理来调整类的全局布局。我们提出了两种策略：硬布局约束和软布局约束。硬布局约束通过直接控制代理的生成来实现，以强制将其放置在严格的线性布局或半圆形布局（即严格序数布局的两种实例）中。软布局约束通过引入正则化项到损失函数中来实现，该项惩罚偏离理想序数布局的情况。在基准数据集上的实验结果证明了所提出的CPL方法在深度序数分类中的有效性。

    For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
    
[^108]: 启发式模块化最大化算法很难返回最优划分或相似结果的社区检测

    Heuristic Modularity Maximization Algorithms for Community Detection Rarely Return an Optimal Partition or Anything Similar. (arXiv:2302.14698v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2302.14698](http://arxiv.org/abs/2302.14698)

    通过使用80个真实和随机网络，研究对比了当前8种基于模块化启发式算法和一种精确整数规划方法在社区检测中的优化表现，结果显示平均只有16.9%的启发式算法返回最优划分或相似结果。

    

    社区检测是计算科学中的一项基本问题，在各个领域都有广泛应用。最常用的方法是设计算法来在网络节点的不同划分之间最大化模块化。通过使用来自各种背景的80个真实和随机网络，我们调查了当前启发式模块化最大化算法在返回最大模块化（最优）划分方面的成功程度。我们评估了（1）算法输出模块化与每个输入图的最大模块化之比，以及（2）它们的输出划分与该图的任何最优划分之间的最大相似度。我们将八种现有的启发式算法与全局最大化模块化的精确整数规划方法进行比较。平均基于模块化的启发式算法只为考虑的80个图的16.9%返回最优划分。此外，根据调整后的互信息的结果显示出实质性差异。

    Community detection is a fundamental problem in computational sciences with extensive applications in various fields. The most commonly used methods are the algorithms designed to maximize modularity over different partitions of the network nodes. Using 80 real and random networks from a wide range of contexts, we investigate the extent to which current heuristic modularity maximization algorithms succeed in returning maximum-modularity (optimal) partitions. We evaluate (1) the ratio of the algorithms' output modularity to the maximum modularity for each input graph, and (2) the maximum similarity between their output partition and any optimal partition of that graph. We compare eight existing heuristic algorithms against an exact integer programming method that globally maximizes modularity. The average modularity-based heuristic algorithm returns optimal partitions for only 16.9% of the 80 graphs considered. Additionally, results on adjusted mutual information reveal substantial diss
    
[^109]: 几千到几十万个标记的序列推理和记忆任务中的卷积语言模型 - 九斑狼等级: 迈向更大的卷积语言模型

    Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10866](http://arxiv.org/abs/2302.10866)

    本文提出一种名为九斑狼的卷积语言模型，通过交错隐式参数化的长卷积和数据控制门构造。在记忆任务和推理任务中，序列有几千到几十万个标记，九斑狼取得了比其他算子更为精确的表现，达到了基于注意力的模型的水平，并取得了密集注意力模型的最新结构。

    

    深度学习的最新进展在很大程度上依赖于大型Transformer的使用，因为它们可以在任意规模上进行学习。然而，Transformers的核心构件——注意力操作符——在长度方面呈现出二次的成本，限制了可以访问的上下文量。现有的基于低秩和稀疏逼近的亚二次方法需要与密集的注意力层结合使用来匹配Transformers，表明存在能力差距。在这项工作中，我们提出了九斑狼，一种亚二次的注意力替代品，通过交错隐式参数化的长卷积和数据控制门构造。在记忆任务和推理任务中，序列有几千到几十万个标记，九斑狼的准确度比依赖于状态空间和其他隐式和显式方法的算子提高了50以上，达到了基于注意力的模型的水平。在标准数据集上，九斑狼并不需要密集注意力的结构，就已经取得了密集注意力模型的最新结构。

    Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard dat
    
[^110]: Classy Ensemble: 一种新颖的用于分类的集成学习算法

    Classy Ensemble: A Novel Ensemble Algorithm for Classification. (arXiv:2302.10580v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10580](http://arxiv.org/abs/2302.10580)

    Classy Ensemble是一种新颖的集成学习算法，通过每类准确率的加权组合来聚合模型，在大量机器学习数据集上展现出优越性能。并提出Classy Cluster Ensemble和Classy Evolutionary Ensemble两种增强方法。

    

    我们提出了Classy Ensemble，一种新的用于分类任务的集成算法，它通过每类准确率的加权组合来聚合模型。我们在153个机器学习数据集上进行了测试，结果表明，Classy Ensemble优于两种其他著名的聚合算法——基于顺序的修剪和基于聚类的修剪——以及最近引入的lexigarden集成生成器。接着，我们提出了三种增强方法：1）Classy Cluster Ensemble，将Classy Ensemble和基于聚类的修剪相结合；2）深度学习实验，展示了Classy Ensemble在四个图像数据集Fashion MNIST、CIFAR10、CIFAR100和ImageNet上的优越性；以及3）Classy Evolutionary Ensemble，其中使用进化算法来选择Classy Ensemble从中选择的模型集合。

    We present Classy Ensemble, a novel ensemble-generation algorithm for classification tasks, which aggregates models through a weighted combination of per-class accuracy. Tested over 153 machine learning datasets we demonstrate that Classy Ensemble outperforms two other well-known aggregation algorithms -order-based pruning and clustering-based pruning -- as well as the recently introduced lexigarden ensemble generator. We then present three enhancements: 1) Classy Cluster Ensemble, which combines Classy Ensemble and cluster-based pruning; 2) Deep Learning experiments, showing the merits of Classy Ensemble over four image datasets: Fashion MNIST, CIFAR10, CIFAR100, and ImageNet; and 3) Classy Evolutionary Ensemble, wherein an evolutionary algorithm is used to select the set of models which Classy Ensemble picks from.
    
[^111]: 基于大规模浮动车数据的10个城市都市路段交通速度数据

    Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities. (arXiv:2302.08761v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08761](http://arxiv.org/abs/2302.08761)

    本研究基于10个城市的大规模浮动车数据，提供了15分钟分辨率的交通速度信息，涵盖了从主干道到当地街道的所有街道级别，为城市交通运营和规划提供了重要数据。

    

    交通分析对城市运营和规划至关重要，但超出环路检测器范围的城市交通密集数据仍然很少。我们提供了一个大规模浮动车辆数据集，即“10个城市的都市路段交通速度数据集”，可用于全球10个城市，并具有每个都市区域1500多平方公里的15分钟分辨率的收集周期，收集时间为2019-2021年，覆盖从主干道到当地街道的所有街道级别的交通信息。该数据集利用工业规模的浮动车辆Traffic4cast数据，通过隐私保护的时空聚合提供了速度和车辆计数。我们详细介绍了高效的匹配方法，将数据映射到OpenStreetMap路网图中。

    Traffic analysis is crucial for urban operations and planning, while the availability of dense urban traffic data beyond loop detectors is still scarce. We present a large-scale floating vehicle dataset of per-street segment traffic information, Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities (MeTS-10), available for 10 global cities with a 15-minute resolution for collection periods ranging between 108 and 361 days in 2019-2021 and covering more than 1500 square kilometers per metropolitan area. MeTS-10 features traffic speed information at all street levels from main arterials to local streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul, London, Madrid, Melbourne and Moscow. The dataset leverages the industrial-scale floating vehicle Traffic4cast data with speeds and vehicle counts provided in a privacy-preserving spatio-temporal aggregation. We detail the efficient matching approach mapping the data to the OpenStreetMap road graph. We e
    
[^112]: InstructABSA: 基于指令学习的方面情感分析

    InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08624](http://arxiv.org/abs/2302.08624)

    InstructABSA是一种使用指令学习范式的方面情感分析方法，能够显著提高Aspect Term Extraction、Aspect Term Sentiment Classification、和Joint Task subtasks三个子任务的性能，并且在多个数据集上表现超过之前的最先进方法。

    

    本文介绍了InstructABSA，一种使用指令学习范式进行Aspect Based Sentiment Analysis (ABSA) 所有子任务（Aspect Term Extraction (ATE)，Aspect Term Sentiment Classification (ATSC)，以及Joint Task modeling）的方法。我们的方法对每个训练样本引入了正面、负面、和中性的例子，并使用指令来调整每个ABSA子任务的模型（Tk-Instruct），从而显著提高了性能。在Sem Eval 2014、2015和2016数据集上的实验结果表明，在所有三个ABSA子任务（ATE、ATSC和Joint Task）上，InstructABSA在性能上都比之前的最先进方法（SOTA）表现出了显著的优势，并且表现超过了7倍大的模型。特别是，在Rest14 ATE子任务上，InstructABSA超过了SOTA 7.31%的得分，Rest15 ATSC子任务上也有提升，并且在Lapt14 Joint Task上的表现提升了8.63%点。我们的结果还表明，对于所有三个子任务，InstructABSA具有强大的新领域泛化能力。

    In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
    
[^113]: 修复动态神经网络中的过度自信问题

    Fixing Overconfidence in Dynamic Neural Networks. (arXiv:2302.06359v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06359](http://arxiv.org/abs/2302.06359)

    该论文提出了一种修复动态神经网络中过度自信问题的方法，通过对最后几层进行概率化处理，量化和纳入不确定性并有助于决定计算预算的确定。

    

    动态神经网络是一种最近的技术，通过根据输入难度动态调整计算代价，承诺缓解现代深度学习模型越来越大的问题。然而，深度学习模型中uncertainty estimates的质量较差，很难区分hard和easy的样本。为了解决这个挑战，我们提出了一种在动态神经网络中进行后处理不确定性量化的计算有效方法。我们展示了通过对最后几层进行概率化处理，充分量化和纳入aleatoric和epistemic uncertainty，可以提高预测性能，并在确定计算预算时有助于决策。在实验中，我们在CIFAR-100、ImageNet和Caltech-256方面展示了准确性、捕获不确定性和校准误差的改进。

    Dynamic neural networks are a recent technique that promises a remedy for the increasing size of modern deep learning models by dynamically adapting their computational cost to the difficulty of the inputs. In this way, the model can adjust to a limited computational budget. However, the poor quality of uncertainty estimates in deep learning models makes it difficult to distinguish between hard and easy samples. To address this challenge, we present a computationally efficient approach for post-hoc uncertainty quantification in dynamic neural networks. We show that adequately quantifying and accounting for both aleatoric and epistemic uncertainty through a probabilistic treatment of the last layers improves the predictive performance and aids decision-making when determining the computational budget. In the experiments, we show improvements on CIFAR-100, ImageNet, and Caltech-256 in terms of accuracy, capturing uncertainty, and calibration error.
    
[^114]: 物理学知识作为不确定性量化模型的信息场理论

    Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.07609](http://arxiv.org/abs/2301.07609)

    该论文扩展了信息场理论(IFT)到物理信息场理论(PIFT)，将描述场的物理定律的信息编码为函数先验。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。

    

    数据驱动的方法结合物理学知识是建模系统的强有力技术。此类模型的目标是通过将测量结果与已知物理定律相结合，高效地求解基本场。由于许多系统包含未知元素，如缺失参数、嘈杂数据或不完整的物理定律，因此这通常被视为一种不确定性量化问题。处理所有变量的常见技术通常取决于用于近似后验的数值方案，并且希望有一种不依赖于任何离散化的方法。信息场理论（IFT）提供了对不一定是高斯场的场进行统计学的工具。我们通过将描述场的物理定律的信息编码为函数先验来扩展IFT到物理信息场理论（PIFT）。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。

    Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
    
[^115]: 用超线性收敛重构基于核的机器学习力场

    Reconstructing Kernel-based Machine Learning Force Fields with Super-linear Convergence. (arXiv:2212.12737v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2212.12737](http://arxiv.org/abs/2212.12737)

    本文提出了一种基于Nyström型方法的预处理器构建框架，实现了在低数据范围内高效重构核机器力场，并在带有数万个培训点的化学系统中获得了稳定和准确的结果。

    

    核机器在量子化学领域持续取得进展，尤其在力场重构的低数据范围内已被证明成功。这是因为可以将许多针对物理对称性的等变性和不变性合并到核函数中以补偿更大的数据集。但是，核机器的可扩展性受到其二次内存和与训练点数成立方关系的限制。虽然已知迭代的Krylov子空间求解器可以克服这些负担，但它们的收敛关键取决于有效的预处理器，这在实践中很难实现。有效的预处理器需要以计算便宜和数值鲁棒的方式部分预解学习问题。在这里，我们考虑了Nyström型方法类的广泛方法，以基于最初核函数的越来越复杂的低秩近似构建预处理器。

    Kernel machines have sustained continuous progress in the field of quantum chemistry. In particular, they have proven to be successful in the low-data regime of force field reconstruction. This is because many equivariances and invariances due to physical symmetries can be incorporated into the kernel function to compensate for much larger datasets. So far, the scalability of kernel machines has however been hindered by its quadratic memory and cubical runtime complexity in the number of training points. While it is known, that iterative Krylov subspace solvers can overcome these burdens, their convergence crucially relies on effective preconditioners, which are elusive in practice. Effective preconditioners need to partially pre-solve the learning problem in a computationally cheap and numerically robust manner. Here, we consider the broad class of Nystr\"om-type methods to construct preconditioners based on successively more sophisticated low-rank approximations of the original kerne
    
[^116]: MegaCRN：用于时空建模的元图卷积递归网络

    MegaCRN: Meta-Graph Convolutional Recurrent Network for Spatio-Temporal Modeling. (arXiv:2212.05989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05989](http://arxiv.org/abs/2212.05989)

    MegaCRN是一个使用时空元图学习机制的元图卷积递归网络，对于时空建模具有良好的性能。

    

    时空建模是AI社区中重要的多元时间序列预测任务。本研究提出了时空元图学习作为处理图流中的异质性和非平稳性的新型图结构学习机制。通过将元图学习器嵌入GCRN编码器-解码器中，我们实现了这个思想，并创建了Meta-Graph Convolutional Recurrent Network (MegaCRN)。我们对两个基准数据集（METR-LA和PEMS-BAY）以及一个包含各种非平稳现象的大规模时空数据集进行了全面评估。我们的模型在所有三个数据集上性能都大为优越（误差率超过27％ MAE和34％ RMSE）。此外，通过一系列定性评估，我们证明了我们的模型能够明确地解开具有不同模式的位置和时间插槽。

    Spatio-temporal modeling as a canonical task of multivariate time series forecasting has been a significant research topic in AI community. To address the underlying heterogeneity and non-stationarity implied in the graph streams, in this study, we propose Spatio-Temporal Meta-Graph Learning as a novel Graph Structure Learning mechanism on spatio-temporal data. Specifically, we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a Meta-Node Bank into GCRN encoder-decoder. We conduct a comprehensive evaluation on two benchmark datasets (METR-LA and PEMS-BAY) and a large-scale spatio-temporal dataset that contains a variaty of non-stationary phenomena. Our model outperformed the state-of-the-arts to a large degree on all three datasets (over 27% MAE and 34% RMSE). Besides, through a series of qualitative evaluations, we demonstrate that our model can explicitly disentangle locations and time slots with different patt
    
[^117]: 多智能体强化学习中的谱归一化效应

    Effects of Spectral Normalization in Multi-agent Reinforcement Learning. (arXiv:2212.05331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05331](http://arxiv.org/abs/2212.05331)

    本文研究多智能体强化学习中评论者的规范化方法，发现使用谱归一化(SN)可以使评论家更稳健地学习，特别是在稀疏奖励场景下。实验结果表明，在规范化评论家的情况下，能够快速轻松地从那些复杂情境中学习，这一点对于实现稳定学习非常重要。

    

    在多智能体稀疏奖励场景下，一个可靠的评论者对于在策略上实现演员-评论者学习至关重要。然而，由于两个因素，学习一个可靠的评论者变得具有挑战性：1）随着智能体数量的增加，联合作用空间呈指数增长；2）这个因素结合奖励稀疏和环境噪声，需要大量的样本数才能实现准确的学习。我们表明，用谱归一化(SN)对评论家进行规范化，使它能够在多智能体策略上的稀疏奖励场景中更加稳健地学习。我们的实验表明，规范化的评论家能够快速地从复杂的SMAC和RWARE领域的稀缺奖励经历中学习到。这些发现强调了评论家规范化在稳定学习中的重要性。

    A reliable critic is central to on-policy actor-critic learning. But it becomes challenging to learn a reliable critic in a multi-agent sparse reward scenario due to two factors: 1) The joint action space grows exponentially with the number of agents 2) This, combined with the reward sparseness and environment noise, leads to large sample requirements for accurate learning. We show that regularising the critic with spectral normalization (SN) enables it to learn more robustly, even in multi-agent on-policy sparse reward scenarios. Our experiments show that the regularised critic is quickly able to learn from the sparse rewarding experience in the complex SMAC and RWARE domains. These findings highlight the importance of regularisation in the critic for stable learning.
    
[^118]: PowRL：用于稳健管理电力网络的强化学习框架

    PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks. (arXiv:2212.02397v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02397](http://arxiv.org/abs/2212.02397)

    本论文提出了PowRL框架来通过强化学习来有效应对电力网络中的不确定情况，并保持电网的可靠运行。

    

    世界各地的电力网络通过为多个行业、企业和家庭消费者提供不间断、可靠和无暂态电力发挥着重要的社会和经济作用。随着可再生能源和电动车产生的不确定发电和高度动态负载需求的出现，通过适当的瞬态稳定问题管理来确保电力网络的稳健运行变得越来越重要，并将停电事件限制在地方范围内。本文引入了一个名为PowRL的强化学习（RL）框架，以缓解意外网络事件的影响，并可靠地在网络上随时维持电力。PowRL利用新颖的超负荷管理启发式以及基于RL提供的最优拓扑选择决策，以确保电网在变化和不确定条件下安全、可靠地运行（无超载线路和无停电事件）。

    Power grids, across the world, play an important societal and economical role by providing uninterrupted, reliable and transient-free power to several industries, businesses and household consumers. With the advent of renewable power resources and EVs resulting into uncertain generation and highly dynamic load demands, it has become ever so important to ensure robust operation of power networks through suitable management of transient stability issues and localize the events of blackouts. In the light of ever increasing stress on the modern grid infrastructure and the grid operators, this paper presents a reinforcement learning (RL) framework, PowRL, to mitigate the effects of unexpected network events, as well as reliably maintain electricity everywhere on the network at all times. The PowRL leverages a novel heuristic for overload management, along with the RL-guided decision making on optimal topology selection to ensure that the grid is operated safely and reliably (with no overloa
    
[^119]: 连续情景控制

    Continuous Episodic Control. (arXiv:2211.15183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15183](http://arxiv.org/abs/2211.15183)

    CEC是一种新颖的非参数情景记忆算法，用于连续性行动空间问题中的序列决策制定，其在几个稀疏奖励连续控制环境中比最先进的RL和记忆增强RL算法学习更快，是学习连续控制任务的快速方法。

    

    非参数情景记忆可以用于快速锁定强化学习任务中高奖励的经验。与参数深度强化学习方法相比，在参数需要缓慢地反向传递奖励信号的方法中，这些方法只需要发现一次解决方案，然后就可以反复解决任务。然而，情景控制解决方案存储在离散表中，这种方法迄今只应用于离散行动空间问题。因此，本文介绍了连续情景控制（CEC），这是一种新颖的非参数情景记忆算法，可用于连续性行动空间问题中的序列决策制定。在几个稀疏奖励连续控制环境中的结果表明，我们提出的方法比最先进的无模型RL和记忆增强RL算法学习更快，同时保持良好的长期性能。简而言之，CEC可以是学习连续控制任务的快速方法。

    Non-parametric episodic memory can be used to quickly latch onto high-rewarded experience in reinforcement learning tasks. In contrast to parametric deep reinforcement learning approaches in which reward signals need to be back-propagated slowly, these methods only need to discover the solution once, and may then repeatedly solve the task. However, episodic control solutions are stored in discrete tables, and this approach has so far only been applied to discrete action space problems. Therefore, this paper introduces Continuous Episodic Control (CEC), a novel non-parametric episodic memory algorithm for sequential decision making in problems with a continuous action space. Results on several sparse-reward continuous control environments show that our proposed method learns faster than state-of-the-art model-free RL and memory-augmented RL algorithms, while maintaining good long-run performance as well. In short, CEC can be a fast approach for learning in continuous control tasks.
    
[^120]: FedFA: 针对异构数据的特征锚定联邦学习

    FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data. (arXiv:2211.09299v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09299](http://arxiv.org/abs/2211.09299)

    本文提出了一种名为 FedFA 的联邦学习框架，通过特征锚定来对齐特征映射并校准分类器，解决了在异构数据时分类器和特征映射之间的恶性循环问题。在实验中表明，该方法能够提高准确性和收敛速度。

    

    联邦学习允许多个客户端在不交换数据的情况下协作训练模型，从而保护数据隐私。然而，在客户端存在异构数据时，它会遭受明显的性能下降。本文发现，常见的本地训练解决方案通过设计特定的辅助损失函数来规范权重差异或特征不一致性，但这些方法忽略了分类器和特征映射不一致之间的恶性循环，导致客户端模型在特征空间和分类器差异的不一致特征空间中更新。我们提出了一个称为 FedFA 的简单而有效的框架，在本地训练过程中通过特征锚定来对齐客户端之间的特征映射并校准分类器，从而使客户端模型在共享的特征空间和一致的分类器下更新。我们证明，与先前的方法相比，在异构数据情况下，这种修改后的联邦学习方法提高了准确性和收敛速度。

    Federated learning allows multiple clients to collaboratively train a model without exchanging their data, thus preserving data privacy. Unfortunately, it suffers significant performance degradation under heterogeneous data at clients. Common solutions in local training involve designing a specific auxiliary loss to regularize weight divergence or feature inconsistency. However, we discover that these approaches fall short of the expected performance because they ignore the existence of a vicious cycle between classifier divergence and feature mapping inconsistency across clients, such that client models are updated in inconsistent feature space with diverged classifiers. We then propose a simple yet effective framework named Federated learning with Feature Anchors (FedFA) to align the feature mappings and calibrate classifier across clients during local training, which allows client models updating in a shared feature space with consistent classifiers. We demonstrate that this modific
    
[^121]: 高维数据的无模型变量重要性方法

    Model free variable importance for high dimensional data. (arXiv:2211.08414v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08414](http://arxiv.org/abs/2211.08414)

    该论文提出了一种无模型的变量重要性方法，可以与任意预测函数一起使用，不需要访问预测函数，可用于研究模型残差。引入了Cohort Shapley的积分梯度版本（IGCS），使得在二元预测器情况下也可以使用IG方法。

    

    模型不可知的变量重要性方法可与任意预测函数一起使用。在这里，我们提供了一些无模型方法，不需要访问预测函数。这在预测函数是专有的且不可用或极其昂贵时很有用。当对模型的残差进行研究时也很有用。Cohort Shapley（CS）方法是无模型方法，但在输入空间的维数上具有指数成本。Frye等人（2020）的监督流形上Shapley方法也是无模型的，但要求输入第二个黑匣子模型，该模型必须为Shapley值问题进行训练。我们引入了Cohort Shapley的积分梯度（IG）版本，称为IGCS，成本为$\mathcal{O}(nd)$。我们表明，在绝大多数相关单元的立方体上，IGCS值函数接近多线性函数，其中IGCS匹配CS。IGCS的另一个好处是它允许使用二元预测器进行IG方法。我们使用一些面积...

    A model-agnostic variable importance method can be used with arbitrary prediction functions. Here we present some model-free methods that do not require access to the prediction function. This is useful when that function is proprietary and not available, or just extremely expensive. It is also useful when studying residuals from a model. The cohort Shapley (CS) method is model-free but has exponential cost in the dimension of the input space. A supervised on-manifold Shapley method from Frye et al. (2020) is also model free but requires as input a second black box model that has to be trained for the Shapley value problem. We introduce an integrated gradient (IG) version of cohort Shapley, called IGCS, with cost $\mathcal{O}(nd)$. We show that over the vast majority of the relevant unit cube that the IGCS value function is close to a multilinear function for which IGCS matches CS. Another benefit of IGCS is that is allows IG methods to be used with binary predictors. We use some area 
    
[^122]: 利用生命周期自适应处理学习自适应系统中适应空间的漂移

    Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02658](http://arxiv.org/abs/2211.02658)

    机器学习在自适应系统中成为热门方法，但利用机器学习会面临适应空间中的漂移问题。本文提出了一种名为"生命周期自适应"的新方法，能够更好地处理适应空间的漂移。

    

    最近，机器学习 (ML) 已成为支持自适应的热门方法。ML 已被用来处理自适应中的几个问题，例如在不确定性下维护最新的运行时模型和可扩展的决策制定。然而，利用 ML 存在固有的挑战。在本文中，我们着重讨论面向基于学习的自适应系统的一个特别重要的挑战：适应空间中的漂移。通过适应空间，我们指的是自适应系统在某一特定时间可以选择的适应选项的集合，以根据适应选项的质量属性进行适应。适应空间的漂移源于影响适应选项质量属性的不确定性。这种漂移可能意味着最终没有适应选项能够满足最初的适应目标，从而降低系统的质量，或者可能出现允许增强适应目标的适应选项。在 ML 中，这种漂移通常被称为概念漂移或实例漂移。为了解决这个挑战，我们提出了一种名为“生命周期自适应”的新方法。生命周期自适应对 ML powered self-adaptation 进行了扩展，使其能够更好地处理适应空间的漂移。

    Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
    
[^123]: RGMIM: 区域引导的掩膜图像建模用于COVID-19检测。

    RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection. (arXiv:2211.00313v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.00313](http://arxiv.org/abs/2211.00313)

    本论文提出了一种针对COVID-19检测的新颖区域引导的掩膜图像建模方法，该方法通过利用肺掩模信息来识别有效区域，以学习更有用的COVID-19检测信息。

    

    目的：自监督学习正在快速推进医学领域的计算机辅助诊断。掩膜图像建模（MIM）是一种自监督学习方法，它掩盖了一组输入像素并试图预测遮盖的像素。传统的MIM方法通常采用随机掩膜策略。与普通图像相比，医学图像往往具有用于疾病检测的小区域。因此，我们在本文中专注于解决这个问题，在自动COVID-19识别方面进行评估。方法：本文提出了一种新颖的区域引导的掩膜图像建模方法（RGMIM）用于COVID-19检测。在我们的方法中，我们设计了一种新的掩膜策略，利用肺掩模信息来识别有效区域，以学习更有用的COVID-19检测信息。我们将所提出的方法与五种自监督学习技术（MAE，SKD，Cross，BYOL和SimSiam）进行对比。我们提出了定量评估。

    Purpose: Self-supervised learning is rapidly advancing computer-aided diagnosis in the medical field. Masked image modeling (MIM) is one of the self-supervised learning methods that masks a subset of input pixels and attempts to predict the masked pixels. Traditional MIM methods often employ a random masking strategy. In comparison to ordinary images, medical images often have a small region of interest for disease detection. Consequently, we focus on fixing the problem in this work, which is evaluated by automatic COVID-19 identification. Methods: In this study, we propose a novel region-guided masked image modeling method (RGMIM) for COVID-19 detection in this paper. In our method, we devise a new masking strategy that employed lung mask information to identify valid regions to learn more useful information for COVID-19 detection. The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). We present a quantitative evaluatio
    
[^124]: 带有中间表示正则化的联邦学习

    Federated Learning with Intermediate Representation Regularization. (arXiv:2210.15827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15827](http://arxiv.org/abs/2210.15827)

    本文提出一种新的联邦学习方法FedIntR，可以通过将中间层的表示集成到本地训练过程中，提供更细粒度的正则化，从而限制本地训练过程中偏离全局模型的程度，以提高模型性能。

    

    与涉及数据收集的集中式模型训练相反，联邦学习使远程客户端能够在不暴露其私有数据的情况下协同训练模型。然而，联邦学习中异构数据可能会导致模型性能下降。很多时候，为了保持较好的性能，限制本地训练过程中偏离全局模型的程度是一种有效策略。以往的研究通过正则化全局和本地学习的表示之间的距离以实现这一点，但是这种方法只考虑了模型的早期层或输出层前面的层的表示。本研究介绍了FedIntR，它通过将中间层的表示集成到本地训练过程中提供了更细粒度的正则化。具体而言，FedIntR计算一个正则化项来鼓励中间层表示和全局模型之间的接近度。

    In contrast to centralized model training that involves data collection, federated learning (FL) enables remote clients to collaboratively train a model without exposing their private data. However, model performance usually degrades in FL due to the heterogeneous data generated by clients of diverse characteristics. One promising strategy to maintain good performance is by limiting the local training from drifting far away from the global model. Previous studies accomplish this by regularizing the distance between the representations learned by the local and global models. However, they only consider representations from the early layers of a model or the layer preceding the output layer. In this study, we introduce FedIntR, which provides a more fine-grained regularization by integrating the representations of intermediate layers into the local training process. Specifically, FedIntR computes a regularization term that encourages the closeness between the intermediate layer represent
    
[^125]: 一种分析连续时间系统的深度学习方法

    A Deep Learning Approach to Analyzing Continuous-Time Systems. (arXiv:2209.12128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12128](http://arxiv.org/abs/2209.12128)

    深度学习方法能够分析复杂自然过程的时间序列数据，放松了传统简化假设的限制，具备可解释性和灵活的函数逼近能力，能够应用于人类语言处理领域，改进数据解释力和探索性分析能力。

    

    科学家通常使用时间序列数据来研究复杂的自然过程，但回归分析常常假设过于简单化的动力学。深度学习的最近进展，在复杂过程模型的性能上取得了惊人的提高，但深度学习通常不用于科学分析。在这里，我们展示了深度学习可以用于分析复杂的过程，提供灵活的函数逼近并具有可解释性。我们的方法放松了传统简化假设（如线性、平稳和同方差性），这些假设对许多自然系统而言是不可行的，可能会严重影响数据的解释。我们在人类语言处理方面进行了模型评估，这是一个具有复杂连续动力学的领域。我们证明了在行为和神经影像数据上有显著的改进，并且我们展示了我们的模型可以在控制多种混杂实验设置的混杂因素和缺失数据的情况下，发现探索性分析中的新模式。

    Scientists often use observational time series data to study complex natural processes, but regression analyses often assume simplistic dynamics. Recent advances in deep learning have yielded startling improvements to the performance of models of complex processes, but deep learning is generally not used for scientific analysis. Here we show that deep learning can be used to analyze complex processes, providing flexible function approximation while preserving interpretability. Our approach relaxes standard simplifying assumptions (e.g., linearity, stationarity, and homoscedasticity) that are implausible for many natural systems and may critically affect the interpretation of data. We evaluate our model on incremental human language processing, a domain with complex continuous dynamics. We demonstrate substantial improvements on behavioral and neuroimaging data, and we show that our model enables discovery of novel patterns in exploratory analyses, controls for diverse confounds in conf
    
[^126]: 无监督表征学习中的识别参数概率模型

    Unsupervised representation learning with recognition-parametrised probabilistic models. (arXiv:2209.05661v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05661](http://arxiv.org/abs/2209.05661)

    本文提出了一种基于识别参数模型的概率无监督学习新方法，可以灵活地学习识别模型，捕捉观测之间的潜在相关性，为图像分类和潜在分配问题提供了有效解决方案。

    

    我们提出了一种基于识别参数模型（RPM）的概率无监督学习新方法：作为关于观察变量和潜在变量的联合分布的归一化半参数化假设类。在观察值在给定潜在变量的条件下是条件独立的关键假设下，RPM将参数先验和观测条件下的潜在分布与非参数观测边缘相结合。该方法可以得到灵活的学习识别模型，捕捉了观测之间的潜在相关性，而不需要显式的参数生成模型。对于离散潜变量，RPM允许进行精确的最大似然学习，即使是基于强大的神经网络识别。我们开发了适用于连续潜变量情况的有效近似方法。实验展示了RPM在高维数据上的有效性，学习从弱间接监督中的图像分类；直接图像级潜在狄利克雷分配的学习。

    We introduce a new approach to probabilistic unsupervised learning based on the recognition-parametrised model (RPM): a normalised semi-parametric hypothesis class for joint distributions over observed and latent variables. Under the key assumption that observations are conditionally independent given latents, the RPM combines parametric prior and observation-conditioned latent distributions with non-parametric observation marginals. This approach leads to a flexible learnt recognition model capturing latent dependence between observations, without the need for an explicit, parametric generative model. The RPM admits exact maximum-likelihood learning for discrete latents, even for powerful neural-network-based recognition. We develop effective approximations applicable in the continuous-latent case. Experiments demonstrate the effectiveness of the RPM on high-dimensional data, learning image classification from weak indirect supervision; direct image-level latent Dirichlet allocation; 
    
[^127]: 关于ELBO收敛到熵和的研究

    On the Convergence of the ELBO to Entropy Sums. (arXiv:2209.03077v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.03077](http://arxiv.org/abs/2209.03077)

    本论文研究了 ELBO 收敛到熵和的问题，证明了对于一类广泛的生成模型，ELBO 在所有学习的稳定点处都等于一系列熵的和，为无监督学习的学习算法的基本属性提供了深入的洞察。

    

    变分下界（又称ELBO或自由能）是许多经典和新颖的无监督学习算法的核心目标。学习算法可以改变模型参数，使变分下界增加。通常，学习进行到参数收敛到接近学习动态的稳定点值。在本文的理论贡献中，我们证明了（对于一类非常广泛的生成模型），变分下界在所有学习的稳定点处均等于一系列熵的和。对于具有一组潜在变量和一组观测变量的标准机器学习模型，这个和包括三个熵: (A) 变分分布的熵（平均熵），(B) 模型先验分布的负熵和 (C) 可观测分布的（期望）负熵。所得到的结果适用于包括：有限数量的数据点，在学习的任意阶段和各种不同的生成模型等真实条件。本研究为无监督学习的学习算法的基本属性提供了深入洞察，是对优化推理和学习的理论分析的第一步。

    The variational lower bound (a.k.a. ELBO or free energy) is the central objective for many established as well as many novel algorithms for unsupervised learning. Learning algorithms change model parameters such that the variational lower bound increases. Learning usually proceeds until parameters have converged to values close to a stationary point of the learning dynamics. In this purely theoretical contribution, we show that (for a very large class of generative models) the variational lower bound is at all stationary points of learning equal to a sum of entropies. For standard machine learning models with one set of latents and one set observed variables, the sum consists of three entropies: (A) the (average) entropy of the variational distributions, (B) the negative entropy of the model's prior distribution, and (C) the (expected) negative entropy of the observable distributions. The obtained result applies under realistic conditions including: finite numbers of data points, at an
    
[^128]: HybMT: 基于混合元预测的机器学习算法用于快速测试向量生成

    HybMT: Hybrid Meta-Predictor based ML Algorithm for Fast Test Vector Generation. (arXiv:2207.11312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11312](http://arxiv.org/abs/2207.11312)

    本文提出一个基于混合元预测的机器学习算法用于快速测试向量生成，该算法通过2级预测器进一步减少了无用工作，顶层元预测器的准确率达到了99%

    

    集成电路测试是一项高度计算密集型的过程。对于现今的复杂设计，通常使用确定性测试生成（DTG）算法生成许多难以检测的故障的测试。机器学习（ML）越来越被用于增加测试覆盖率并减少总体测试时间。这些方法主要减少了传统的路径导向决策制定（PODEM）算法中的无用工作，而不会影响测试质量。对于PODEM的变体，很多时候需要回溯，因为继续执行已经无法取得进展。因此，有必要在算法执行的不同阶段预测最佳策略。本文的新贡献是一个2级预测器:顶层是一个元预测器，在较低层中选择几个预测器之一。我们选择给定电路和目标网的最佳预测器。发现顶层元预测器的准确度为99\%。这导致了

    Testing an integrated circuit (IC) is a highly compute-intensive process. For today's complex designs, tests for many hard-to-detect faults are typically generated using deterministic test generation (DTG) algorithms. Machine Learning (ML) is being increasingly used to increase the test coverage and decrease the overall testing time. Such proposals primarily reduce the wasted work in the classic Path Oriented Decision Making (PODEM) algorithm without compromising on the test quality. With variants of PODEM, many times there is a need to backtrack because further progress cannot be made. There is thus a need to predict the best strategy at different points in the execution of the algorithm. The novel contribution of this paper is a 2-level predictor: the top level is a meta predictor that chooses one of several predictors at the lower level. We choose the best predictor given a circuit and a target net. The accuracy of the top-level meta predictor was found to be 99\%. This leads to a s
    
[^129]: 基于处理内存系统的机器学习训练的实验评估

    An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2207.07886](http://arxiv.org/abs/2207.07886)

    该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。

    

    训练机器学习算法是一种计算密集型的过程，由于不断访问大型训练数据集，这种过程通常会受到内存限制。因此，以处理器为中心的系统（例如CPU，GPU）在内存单元和处理单元之间的数据传输方面存在昂贵的瓶颈，这会消耗大量的能量和执行周期。具有处理内存（PIM）功能的内存中心计算系统可以缓解这种数据移动瓶颈。我们的目标是了解现代通用PIM架构加速ML训练的潜力。为此，我们（1）在实际通用PIM架构上实现了几种代表性的传统ML算法（即线性回归，逻辑回归，决策树，K-Means聚类），（2）严格评估和表征这些算法的准确性，性能和扩展性，并且（3）与它们在CPU和GPU上的相应实现进行比较。我们在实际内存中心计算平台上的评估表明，与相应的CPU和GPU方法相比，基于PIM的ML训练实现了显着的加速和能量效率。

    Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
    
[^130]: COVID-19患者生命体征轨迹异常值检测

    Outlier detection of vital sign trajectories from COVID-19 patients. (arXiv:2207.07572v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07572](http://arxiv.org/abs/2207.07572)

    本文提出了一种使用动态时间扭曲距离的轨迹比较算法，可以检测COVID-19患者生命体征轨迹中的异常值，拟整合到现有系统中以改善健康恶化的识别准确度和时效性。

    

    本文提出了一种新的轨迹比较算法，用于识别异常生命体征趋势，旨在改善对健康恶化的识别。近来人们对于用于在家远程监测患者的连续佩戴式生命体征传感器越来越感兴趣。这些监测器通常与警报系统配对使用，当生命体征测量值超出预定正常范围时触发警报。生命体征趋势，如心率增加，通常是健康状况恶化的征兆，但是很少被纳入警报系统。我们引入了基于动态时间扭曲距离的度量，用于比较时间序列轨迹。我们将每个多变量生命体征时间序列分为180分钟，不重叠的时段。然后计算所有时段之间的距离。每个时段的特征为其与所有其他时段之间的平均距离（平均链接距离），通过相邻时段形成聚类。我们通过合成生成的数据集和来自COVID-19患者的真实数据表明，我们的方法在检测异常轨迹方面优于标准方法。我们提出的轨迹比较算法可以潜在地整合到现有的警报系统中，以提高识别COVID-19患者健康恶化的精度和时效性。

    In this work, we present a novel trajectory comparison algorithm to identify abnormal vital sign trends, with the aim of improving recognition of deteriorating health.  There is growing interest in continuous wearable vital sign sensors for monitoring patients remotely at home. These monitors are usually coupled to an alerting system, which is triggered when vital sign measurements fall outside a predefined normal range. Trends in vital signs, such as increasing heart rate, are often indicative of deteriorating health, but are rarely incorporated into alerting systems.  We introduce a dynamic time warp distance-based measure to compare time series trajectories. We split each multi-variable sign time series into 180 minute, non-overlapping epochs. We then calculate the distance between all pairs of epochs. Each epoch is characterized by its mean pairwise distance (average link distance) to all other epochs, with clusters forming with nearby epochs.  We demonstrate in synthetically gener
    
[^131]: 基于图形的分子表示学习

    Graph-based Molecular Representation Learning. (arXiv:2207.04869v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2207.04869](http://arxiv.org/abs/2207.04869)

    本文综述了最新的基于图形的分子表示方法，总结与分类为三组。纳入化学领域知识的方法取得了很大进展。未来需要更易于解释和可解释的MRL模型。

    

    分子表示学习(MRL)是建立机器学习和化学科学之间联系的关键一步。它将分子编码为数字向量，保留分子的结构和特征，从而可以进行下游任务（如性质预测）。最近，特别是基于深度分子图学习的方法取得了相当大的进展。在本综述中，我们系统地审查了这些基于图形的分子表示技术，特别是那些纳入化学领域知识的方法。具体而言，我们首先介绍了2D和3D分子图的特征。然后，我们根据其输入将MRL方法总结和分类为三组。此外，我们还讨论了MRL支持的一些典型化学应用程序。为了促进在这个快速发展的领域中的研究，我们还列出了本文中使用的基准和常用的数据集合。最后，我们分享了关于未来研究方向更易于解释和可解释的MRL模型的想法。

    Molecular representation learning (MRL) is a key step to build the connection between machine learning and chemical science. In particular, it encodes molecules as numerical vectors preserving the molecular structures and features, on top of which the downstream tasks (e.g., property prediction) can be performed. Recently, MRL has achieved considerable progress, especially in methods based on deep molecular graph learning. In this survey, we systematically review these graph-based molecular representation techniques, especially the methods incorporating chemical domain knowledge. Specifically, we first introduce the features of 2D and 3D molecular graphs. Then we summarize and categorize MRL methods into three groups based on their input. Furthermore, we discuss some typical chemical applications supported by MRL. To facilitate studies in this fast-developing area, we also list the benchmarks and commonly used datasets in the paper. Finally, we share our thoughts on future research dir
    
[^132]: 一种基于搜索的测试方法，用于深度强化学习代理

    A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2206.07813](http://arxiv.org/abs/2206.07813)

    本文提出一种基于搜索的测试方法，探索状态空间以检测DRL代理的安全性，并在三个基准测试中取得了比基线方法更高的状态空间覆盖率。

    

    近十年来，深度强化学习算法越来越多地被应用于解决自动驾驶和机器人等各种决策问题。然而，由于深度强化学习代理在生命安全环境中经常表现出错误行为，导致潜在的重大错误，因此它们面临着巨大的挑战。为了评估DRL代理的安全性，一种方法是对它们进行测试，以检测可能导致关键故障的故障。这就提出了一个问题，即我们如何有效地测试DRL策略，以确保它们的正确性和遵守安全要求。本文提出了一种基于搜索的测试方法，通过引导代理生成满足安全要求的状态序列变化，以探索环境的状态空间。我们的方法在三种不同的DRL基准测试中进行了评估，并在保持相似或更好的测试效果的同时，实现了比基线方法更高的状态空间覆盖率。

    Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac
    
[^133]: 异步SGD在任意延迟下均优于Minibatch SGD

    Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays. (arXiv:2206.07638v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.07638](http://arxiv.org/abs/2206.07638)

    这篇论文证明了用虚拟迭代和延迟自适应步长可以在任意延迟下提高异步SGD算法性能，优于同步小批量SGD。

    

    异步随机梯度下降（SGD）的现有分析在任何延迟较大时会急剧恶化，给人们留下了性能主要取决于延迟的印象。相反，我们证明，对于相同的异步 SGD 算法，无论渐变的延迟如何，性能保证都要好得多，这只取决于用于实施算法的并行设备数量。我们的保证比现有分析严格更好，同时我们还论证异步 SGD 在我们考虑的情景下优于同步小批量 SGD。为了进行我们的分析，我们引入了一种基于“虚拟迭代”和延迟自适应步长的新递归，这使我们能够得出针对凸和非凸目标的最先进的保证。

    The existing analysis of asynchronous stochastic gradient descent (SGD) degrades dramatically when any delay is large, giving the impression that performance depends primarily on the delay. On the contrary, we prove much better guarantees for the same asynchronous SGD algorithm regardless of the delays in the gradients, depending instead just on the number of parallel devices used to implement the algorithm. Our guarantees are strictly better than the existing analyses, and we also argue that asynchronous SGD outperforms synchronous minibatch SGD in the settings we consider. For our analysis, we introduce a novel recursion based on "virtual iterates" and delay-adaptive stepsizes, which allow us to derive state-of-the-art guarantees for both convex and non-convex objectives.
    
[^134]: 受不同ially Private Synthetic Data支持的非独立同分布设置中的联邦学习。

    Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data. (arXiv:2206.00686v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00686](http://arxiv.org/abs/2206.00686)

    该论文提出了一种名为FedDPMS的算法，该算法利用变分自动编码器来合成本地数据集。这种增强方法可减轻数据异构效应，同时保护隐私。

    

    联邦学习(Federated learning, FL)是一种能够让潜在的大量客户端共同训练机器学习模型的隐私保护框架。在FL系统中，服务器通过收集和聚合客户端的模型更新来协调合作，而客户端的数据仍然保持本地和私密。联邦学习中的一个主要挑战是当本地数据异构时，学习到的全局模型的性能可能会明显下降。在本文中，我们提出了FedDPMS(联邦差分私有均值共享)，这是一种FL算法，其中客户端部署变分自动编码器(Variational Auto-encoder, VAE)来利用由信任服务器通信的潜变量数据的差分私有均值来生成合成本地数据集。这种增强可减轻跨客户端的数据异构效应，同时又不会损害隐私。

    Federated learning (FL) is a privacy-promoting framework that enables potentially large number of clients to collaboratively train machine learning models. In a FL system, a server coordinates the collaboration by collecting and aggregating clients' model updates while the clients' data remains local and private. A major challenge in federated learning arises when the local data is heterogeneous -- the setting in which performance of the learned global model may deteriorate significantly compared to the scenario where the data is identically distributed across the clients. In this paper we propose FedDPMS (Federated Differentially Private Means Sharing), an FL algorithm in which clients deploy variational auto-encoders to augment local datasets with data synthesized using differentially private means of latent data representations communicated by a trusted server. Such augmentation ameliorates effects of data heterogeneity across the clients without compromising privacy. Our experiment
    
[^135]: 连续生成神经网络

    Continuous Generative Neural Networks. (arXiv:2205.14627v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.14627](http://arxiv.org/abs/2205.14627)

    本文介绍了一种连续生成神经网络(CGNN)的模型，使用条件保证CGNN是单射的，其生成流形被用于求解反问题，并证明了其方法的有效性和稳健性。

    

    本文介绍了并研究了一种连续生成神经网络（CGNN），即连续情境下的生成模型：CGNN的输出属于无限维函数空间。该架构受DCGAN的启发，采用一个全连接层，多个卷积层和非线性激活函数。在连续的$L^2$情境下，每层空间的维度被紧支小波的多重分辨率分析的尺度所代替。我们提出了关于卷积滤波器和非线性的条件，保证CGNN是单射的。该理论应用于反问题，并允许导出一个CGNN生成流形的（可能非线性的）无限维反问题的Lipschitz稳定性估计。包括信号去模糊在内的多个数值模拟证明并验证了这一方法。

    In this work, we present and study Continuous Generative Neural Networks (CGNNs), namely, generative models in the continuous setting: the output of a CGNN belongs to an infinite-dimensional function space. The architecture is inspired by DCGAN, with one fully connected layer, several convolutional layers and nonlinear activation functions. In the continuous $L^2$ setting, the dimensions of the spaces of each layer are replaced by the scales of a multiresolution analysis of a compactly supported wavelet. We present conditions on the convolutional filters and on the nonlinearity that guarantee that a CGNN is injective. This theory finds applications to inverse problems, and allows for deriving Lipschitz stability estimates for (possibly nonlinear) infinite-dimensional inverse problems with unknowns belonging to the manifold generated by a CGNN. Several numerical simulations, including signal deblurring, illustrate and validate this approach.
    
[^136]: 马尔科夫潜在博弈中的独立和去中心化学习

    Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14590](http://arxiv.org/abs/2205.14590)

    独立的去中心化学习在马尔科夫潜在博弈中有效，通过更新Q函数可以引导策略收敛到稳定的纳什平衡点。

    

    我们提出了一种多智能体强化学习机制，并分析了它在无限时间折扣马尔科夫潜在博弈中的收敛性。我们专注于独立和去中心化的设置，在这种设置下，玩家不了解游戏模型，也不能进行协调。在每个阶段，玩家通过异步方式更新他们的打扰Q函数的估计值，该函数根据实现的一阶段奖励评估他们的总体条件付款。然后，玩家通过将基于估计Q函数的平滑最优一阶段偏差策略纳入其策略中来独立地更新其策略。学习动态的关键特征是Q函数估计是以比策略更快的时间尺度进行更新的。我们证明了我们的学习动态引导的策略在概率1的情况下收敛到马尔科夫潜在博弈的稳定纳什平衡。我们的结果凸显了简单学习动态在达到马尔可夫潜在博弈的稳定纳什平衡方面的功效，即使是在独立和去中心化代理环境中。

    We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
    
[^137]: HOUDINI: 从适度约束的马鞍点中逃脱

    HOUDINI: Escaping from Moderately Constrained Saddles. (arXiv:2205.13753v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13753](http://arxiv.org/abs/2205.13753)

    本文给出了第一个多项式时间算法，能在适度数量的约束下使梯度下降方法逃脱高维马鞍点。

    

    我们给出了第一个在适度数量的约束下从高维马鞍点中逃脱的多项式时间算法。给定光滑函数$f \colon \mathbb R^d \to \mathbb R$的梯度访问权限，我们展示了（带噪声的）梯度下降方法可以在对数个不等式约束下逃脱马鞍点。这是对 Ge 等人的突破性工作的主要开放问题的首次有形进展（无需依赖 NP-Oracle 或改变定义来仅考虑特定约束）。我们的结果适用于正则梯度下降和随机梯度下降。

    We give the first polynomial time algorithms for escaping from high-dimensional saddle points under a moderate number of constraints. Given gradient access to a smooth function $f \colon \mathbb R^d \to \mathbb R$ we show that (noisy) gradient descent methods can escape from saddle points under a logarithmic number of inequality constraints. This constitutes the first tangible progress (without reliance on NP-oracles or altering the definitions to only account for certain constraints) on the main open question of the breakthrough work of Ge et al. who showed an analogous result for unconstrained and equality-constrained problems. Our results hold for both regular and stochastic gradient descent.
    
[^138]: FLEX: 用于复杂知识图谱推理的特征逻辑嵌入框架

    FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning. (arXiv:2205.11039v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.11039](http://arxiv.org/abs/2205.11039)

    FLEX是一种新型KGR框架，能够真正处理包括合取、析取、否定等所有FOL操作并支持各种特征空间。实验证明，FLEX优于现有最先进算法。

    

    知识图谱推理（KGR）的当前最佳模型引入几何对象或概率分布将实体和一阶逻辑（FOL）查询嵌入到低维向量空间中。它们可以总结为一个中心-尺寸框架（点/盒式/锥形，Beta/Gaussian分布等），但是它们的逻辑推理能力有限。为了解决这些难题，我们提出了一种名为特征逻辑嵌入框架（FLEX）的新型KGR框架，它是首个真正能够处理包括合取、析取、否定等所有FOL操作并支持各种特征空间的KGR框架。具体来说，特征逻辑框架的逻辑部分基于向量逻辑，自然地建模了所有FOL操作。实验证明，FLEX在各种基准数据集上显著优于现有的最先进算法。

    Current best performing models for knowledge graph reasoning (KGR) introduce geometry objects or probabilistic distributions to embed entities and first-order logical (FOL) queries into low-dimensional vector spaces. They can be summarized as a center-size framework (point/box/cone, Beta/Gaussian distribution, etc.). However, they have limited logical reasoning ability. And it is difficult to generalize to various features, because the center and size are one-to-one constrained, unable to have multiple centers or sizes. To address these challenges, we instead propose a novel KGR framework named Feature-Logic Embedding framework, FLEX, which is the first KGR framework that can not only TRULY handle all FOL operations including conjunction, disjunction, negation and so on, but also support various feature spaces. Specifically, the logic part of feature-logic framework is based on vector logic, which naturally models all FOL operations. Experiments demonstrate that FLEX significantly outp
    
[^139]: 通信效率高的自适应联邦学习

    Communication-Efficient Adaptive Federated Learning. (arXiv:2205.02719v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.02719](http://arxiv.org/abs/2205.02719)

    本文提出了一种新的通信效率高的自适应联邦学习方法（FedCAMS），可以解决联邦学习中由于重复的服务器-客户端同步而产生的大量通信开销和基于 SGD 的模型更新缺乏适应性等诸多问题。

    

    联邦学习是一种机器学习训练方式，使得客户端可以在不共享本地数据的情况下共同训练模型。然而，实际中实现联邦学习仍面临许多挑战，如由于重复的服务器-客户端同步而产生的大量通信开销以及基于 SGD 的模型更新缺乏适应性。尽管已经提出了各种方法来通过梯度压缩或量化来减少通信成本，并提出了FedAdam等联邦版本的自适应优化器来增加更多的适应性，但当前的联邦学习框架仍无法同时解决上述所有挑战。本文提出了一种新的通信效率高的自适应联邦学习方法（FedCAMS），具有理论上的收敛保证。

    Federated learning is a machine learning training paradigm that enables clients to jointly train models without sharing their own localized data. However, the implementation of federated learning in practice still faces numerous challenges, such as the large communication overhead due to the repetitive server-client synchronization and the lack of adaptivity by SGD-based model updates. Despite that various methods have been proposed for reducing the communication cost by gradient compression or quantization, and the federated versions of adaptive optimizers such as FedAdam are proposed to add more adaptivity, the current federated learning framework still cannot solve the aforementioned challenges all at once. In this paper, we propose a novel communication-efficient adaptive federated learning method (FedCAMS) with theoretical convergence guarantees. We show that in the nonconvex stochastic optimization setting, our proposed FedCAMS achieves the same convergence rate of $O(\frac{1}{\s
    
[^140]: 流形两样本检验研究：神经网络的积分概率度量

    A Manifold Two-Sample Test Study: Integral Probability Metric with Neural Networks. (arXiv:2205.02043v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.02043](http://arxiv.org/abs/2205.02043)

    文章提出了一种基于积分概率度量的两样本检验方法，适用于低维流形上支持的高维样本。实验结果表明，所提出的方法在统计功率方面优于最先进的方法。

    

    两样本检验是一种重要的方法，旨在确定两个观测集合是否遵循相同的分布。我们提出了一种基于积分概率度量（IPM）的两样本检验，适用于低维流形上支持的高维样本。我们通过样本数$n$和流形内在维度$d$的结构表征了所提出的检验的特性。当给定一个图集时，我们提出了两步检验以识别一般分布之间的差异，其在$n^{-1/\max\{d,2\}}$的顺序中实现了第二类型风险。当未给出图集时，我们提出了H\"older IPM检验，适用于具有$(s,\beta)$‐H\"older密度的数据分布，其在$n^{-(s+\beta)/d}$的顺序中实现了第二类型风险。为了减轻评估H\"older IPM的重计算负担，我们使用神经网络来逼近H\"older函数类。基于神经网络的逼近理论，我们可以证明所提出的神经网络H\"older IPM测试具有一致的效率。实验结果表明，我们的提出的方法在统计功率方面优于最先进的方法。

    Two-sample tests are important areas aiming to determine whether two collections of observations follow the same distribution or not. We propose two-sample tests based on integral probability metric (IPM) for high-dimensional samples supported on a low-dimensional manifold. We characterize the properties of proposed tests with respect to the number of samples $n$ and the structure of the manifold with intrinsic dimension $d$. When an atlas is given, we propose two-step test to identify the difference between general distributions, which achieves the type-II risk in the order of $n^{-1/\max\{d,2\}}$. When an atlas is not given, we propose H\"older IPM test that applies for data distributions with $(s,\beta)$-H\"older densities, which achieves the type-II risk in the order of $n^{-(s+\beta)/d}$. To mitigate the heavy computation burden of evaluating the H\"older IPM, we approximate the H\"older function class using neural networks. Based on the approximation theory of neural networks, we
    
[^141]: 探索时间卷积神经网络在卫星图像时序分类中的应用：一项调查研究

    Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification: A survey. (arXiv:2204.08461v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.08461](http://arxiv.org/abs/2204.08461)

    本文调查研究探讨了将时间卷积神经网络应用于卫星图像时序分类的方法，相比其他现代方法具有更好的表现。

    

    地球表面的遥感时序图像提供了详细的地表覆盖地图，其在时空维度上的质量持续提高，对于开发旨在产生准确、实时地表覆盖地图的系统至关重要。这些图像时序在生态系统制图、植被过程监测和人类土地利用变化跟踪等广泛应用中具有重要作用。最近提出了卫星图像时序分类的一些方法，取得了不错的效果，但这些方法往往缺乏利用数据时间维度的本地机制；常常导致大量数据预处理和过长的训练时间。为了克服这些不足，最近开始在卫星图像时序分类中应用了时序卷积神经网络，并取得了令人鼓舞的结果。本文旨在对这一方法与其他当代时序分类方法进行比较调查。

    Satellite Image Time Series (SITS) of the Earth's surface provide detailed land cover maps, with their quality in the spatial and temporal dimensions consistently improving. These image time series are integral for developing systems that aim to produce accurate, up-to-date land cover maps of the Earth's surface. Applications are wide-ranging, with notable examples including ecosystem mapping, vegetation process monitoring and anthropogenic land-use change tracking. Recently proposed methods for SITS classification have demonstrated respectable merit, but these methods tend to lack native mechanisms that exploit the temporal dimension of the data; commonly resulting in extensive data pre-processing contributing to prohibitively long training times. To overcome these shortcomings, Temporal CNNs have recently been employed for SITS classification tasks with encouraging results. This paper seeks to survey this method against a plethora of other contemporary methods for SITS classification
    
[^142]: 核鲁棒假设检验

    Kernel Robust Hypothesis Testing. (arXiv:2203.12777v2 [eess.SP] CROSS LISTED)

    [http://arxiv.org/abs/2203.12777](http://arxiv.org/abs/2203.12777)

    本文使用核方法构造不确定性集，在贝叶斯设置和Neyman-Pearson设置中分别研究了最小化最坏情况下错误概率和控制错误概率的问题，并提出了基于MMD的一系列测试。

    

    本论文研究了鲁棒假设检验问题，在零假设和备择假设下，数据生成分布被假设在某些不确定性集合中，并旨在设计一种测试，在不确定性集合中表现最优。本文将使用核方法以数据驱动的方式构造不确定性集，即以来自零假设和备择假设的训练样本的经验分布为中心，并通过核均值嵌入的距离来约束，即最大平均差异（MMD）。同时，本文还研究了贝叶斯设置和Neyman-Pearson设置。对于贝叶斯设置，即目标是最小化最坏情况下的错误概率，当字母表是有限的时，首先得到了最佳测试。当字母表是无限的时，提出了一种可行的近似方法来量化最坏情况下的错误概率。对于Neyman-Pearson设置，即目标是在最小化第二类错误概率的同时控制在给定水平下的第一类错误概率，提出了一系列基于MMD的测试，并研究了它们的渐近特性。

    The problem of robust hypothesis testing is studied, where under the null and the alternative hypotheses, the data-generating distributions are assumed to be in some uncertainty sets, and the goal is to design a test that performs well under the worst-case distributions over the uncertainty sets. In this paper, uncertainty sets are constructed in a data-driven manner using kernel method, i.e., they are centered around empirical distributions of training samples from the null and alternative hypotheses, respectively; and are constrained via the distance between kernel mean embeddings of distributions in the reproducing kernel Hilbert space, i.e., maximum mean discrepancy (MMD). The Bayesian setting and the Neyman-Pearson setting are investigated. For the Bayesian setting where the goal is to minimize the worst-case error probability, an optimal test is firstly obtained when the alphabet is finite. When the alphabet is infinite, a tractable approximation is proposed to quantify the worst
    
[^143]: 基于AI的日志分析器: 一种实用的方法

    AI based Log Analyser: A Practical Approach. (arXiv:2203.10960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.10960](http://arxiv.org/abs/2203.10960)

    本研究使用Transformer构建仅使用正常的日志条目来训练新模型，并通过多种形式的扰动来增强日志，以应对日志来源的异构性和缺乏标签用于训练分类器的限制；该模型进一步使用一种有限的标签样本进行强化学习的方式进行微调，实验结果表现良好。

    

    日志分析是进行故障或网络事件检测、调查和技术取证分析以实现系统和网络韧性的重要活动。AI算法在日志分析中的潜在应用可以增强这种复杂和繁重的任务。然而，这种解决方案存在一些限制，如日志来源的异构性和没有或很少标签用于训练分类器。当这些标签变得可用时，需要更新分类器。这项基于实践的研究旨在通过使用Transformer构建来仅使用正常的日志条目训练新模型来解决这些挑战。通过多种形式的扰动来增强日志，以作为自监督训练用于特征学习。该模型进一步使用一种有限的标签样本进行强化学习的方式进行微调，以模仿实际情况下有标签可用的情况。我们的模型结构的实验结果显示出很好的性能。

    The analysis of logs is a vital activity undertaken for fault or cyber incident detection, investigation and technical forensics analysis for system and cyber resilience. The potential application of AI algorithms for Log analysis could augment such complex and laborious tasks. However, such solution has its constraints the heterogeneity of log sources and limited to no labels for training a classifier. When such labels become available, the need for the classifier to be updated. This practice-based research seeks to address these challenges with the use of Transformer construct to train a new model with only normal log entries. Log augmentation through multiple forms of perturbation is applied as a form of self-supervised training for feature learning. The model is further finetuned using a form of reinforcement learning with a limited set of label samples to mimic real-world situation with the availability of labels. The experimental results of our model construct show promise with c
    
[^144]: 黄金并非一切：线性和非线性保护属性信息的光谱去除方法

    Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.07893](http://arxiv.org/abs/2203.07893)

    这篇论文提出了一种光谱属性去除方法，用于从神经表示中去除私有或保护信息。实验证明，该方法保留了更好的主要任务性能，同时只需少量的被保护属性数据即可去除信息，适用于资源有限的情况。

    

    我们描述了一种简单而有效的方法（称为光谱属性去除方法; SAL），用于从神经表示中去除私有或保护信息。我们的方法使用矩阵分解将输入表示投影到与保护信息协方差较小的方向上，而不是像因式分解方法那样最大化协方差。我们从线性信息删除开始，并通过使用内核将算法推广到非线性信息删除的情况。我们的实验证明，在删除保护信息后，我们的算法保留了更好的主要任务性能，而不像以前的工作那样删除该信息削弱了性能。此外，实验表明，我们只需少量的被保护属性数据即可去除有关这些属性的信息，这降低了敏感数据的暴露，并更适用于资源有限的情况。可以在https://github.com/jasonshaoshun/SAL上找到代码。

    We describe a simple and effective method (Spectral Attribute removaL; SAL) to remove private or guarded information from neural representations. Our method uses matrix decomposition to project the input representations into directions with reduced covariance with the guarded information rather than maximal covariance as factorization methods normally use. We begin with linear information removal and proceed to generalize our algorithm to the case of nonlinear information removal using kernels. Our experiments demonstrate that our algorithm retains better main task performance after removing the guarded information compared to previous work. In addition, our experiments demonstrate that we need a relatively small amount of guarded attribute data to remove information about these attributes, which lowers the exposure to sensitive data and is more suitable for low-resource scenarios. Code is available at https://github.com/jasonshaoshun/SAL.
    
[^145]: 自适应跨层注意力机制用于图像恢复

    Adaptive Cross-Layer Attention for Image Restoration. (arXiv:2203.03619v3 [eess.IV] CROSS LISTED)

    [http://arxiv.org/abs/2203.03619](http://arxiv.org/abs/2203.03619)

    本文提出了一种自适应跨层注意力（ACLA）模块，它可以汇总不同层次的特征信息用于非局部注意力的处理，以提高图像恢复的效果。

    

    非局部注意力模块被证明在图像恢复中至关重要。传统的非局部注意力单独处理每个层次的特征，可能会错过不同层次之间特征之间的关联性，为解决这个问题，我们旨在设计注意力模块，可以汇总不同层次的信息。我们提出了一种新颖的自适应跨层注意力（ACLA）模块，以有效地将这种注意力设计嵌入神经网络骨干中。ACLA提供了两种自适应设计：（1）在每一层自适应选择非局部注意力的键；（2）自动搜索ACLA模块的插入位置。通过这两个自适应设计，ACLA动态地选择灵活数量的键来汇总前一层次的特征进行非局部注意力的处理，同时保持网络的有效性和效率。

    Non-local attention module has been proven to be crucial for image restoration. Conventional non-local attention processes features of each layer separately, so it risks missing correlation between features among different layers. To address this problem, we aim to design attention modules that aggregate information from different layers. Instead of finding correlated key pixels within the same layer, each query pixel is encouraged to attend to key pixels at multiple previous layers of the network. In order to efficiently embed such attention design into neural network backbones, we propose a novel Adaptive Cross-Layer Attention (ACLA) module. Two adaptive designs are proposed for ACLA: (1) adaptively selecting the keys for non-local attention at each layer; (2) automatically searching for the insertion locations for ACLA modules. By these two adaptive designs, ACLA dynamically selects a flexible number of keys to be aggregated for non-local attention at previous layer while maintainin
    
[^146]: 自旋相关的图神经网络势用于磁性材料

    Spin-Dependent Graph Neural Network Potential for Magnetic Materials. (arXiv:2203.02853v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2203.02853](http://arxiv.org/abs/2203.02853)

    SpinGNN是一种自旋相关的相互作用势方法，利用图神经网络描述磁性系统的海森堡型自旋-晶格相互作用和高阶自旋-晶格耦合，同时成功地模拟了BiFeO3中微妙的自旋-晶格耦合。

    

    机器学习相互作用势的发展极大地提高了分子和晶体模拟的准确性。然而，为磁性系统创建既考虑磁矩又考虑结构自由度的相互作用势仍然是一个挑战。本文介绍SpinGNN，一种自旋相关的相互作用势方法，利用图神经网络（GNN）描述磁性系统。SpinGNN由两种类型的边缘GNN组成：海森堡边缘GNN（HEGNN）和自旋距离边缘GNN（SEGNN）。HEGNN旨在捕捉海森堡型自旋-晶格相互作用，而SEGNN则准确地模拟多体和高阶自旋-晶格耦合。SpinGNN的有效性通过其在高阶自旋哈密顿量和两个复杂自旋-晶格哈密顿量的拟合精度方面的卓越表现得到了证明。此外，它成功地模拟了BiFeO3中微妙的自旋-晶格耦合，并进行了大规模的自旋模拟。

    The development of machine learning interatomic potentials has immensely contributed to the accuracy of simulations of molecules and crystals. However, creating interatomic potentials for magnetic systems that account for both magnetic moments and structural degrees of freedom remains a challenge. This work introduces SpinGNN, a spin-dependent interatomic potential approach that employs the graph neural network (GNN) to describe magnetic systems. SpinGNN consists of two types of edge GNNs: Heisenberg edge GNN (HEGNN) and spin-distance edge GNN (SEGNN). HEGNN is tailored to capture Heisenberg-type spin-lattice interactions, while SEGNN accurately models multi-body and high-order spin-lattice coupling. The effectiveness of SpinGNN is demonstrated by its exceptional precision in fitting a high-order spin Hamiltonian and two complex spin-lattice Hamiltonians with great precision. Furthermore, it successfully models the subtle spin-lattice coupling in BiFeO3 and performs large-scale spin-la
    
[^147]: 量子懒惰训练

    Quantum Lazy Training. (arXiv:2202.08232v6 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2202.08232](http://arxiv.org/abs/2202.08232)

    本文证明了当量子比特数量较大时，地理局部化参数化量子电路的训练进入懒惰阶段，限制了参数变化速率并保证了相应量子模型的线性逼近的精度。

    

    在梯度下降训练超参数模型函数时，有时参数不会发生显着变化，保持接近其初始值。这种现象称为懒惰训练，并激发了对模型函数在初始参数周围的线性逼近的考虑。在懒惰阶段，线性逼近模拟了参数化函数的行为，其相关内核称为切向内核，指定了模型的训练性能。已知大宽度（经典）神经网络的情况下会出现懒惰训练。我们在本文中展示了在量子比特数量较大时，地理局部化参数化量子电路的训练进入懒惰阶段。更准确地说，我们证明了这种地理局部化参数化量子电路参数变化速率的限制，以及其相关量子模型的线性逼近的精度。

    In the training of over-parameterized model functions via gradient descent, sometimes the parameters do not change significantly and remain close to their initial values. This phenomenon is called lazy training, and motivates consideration of the linear approximation of the model function around the initial parameters. In the lazy regime, this linear approximation imitates the behavior of the parameterized function whose associated kernel, called the tangent kernel, specifies the training performance of the model. Lazy training is known to occur in the case of (classical) neural networks with large widths. In this paper, we show that the training of geometrically local parameterized quantum circuits enters the lazy regime for large numbers of qubits. More precisely, we prove bounds on the rate of changes of the parameters of such a geometrically local parameterized quantum circuit in the training process, and on the precision of the linear approximation of the associated quantum model 
    
[^148]: 更多是更好的（在很多情况下）：关于联邦图神经网络中后门攻击的研究

    More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks. (arXiv:2202.03195v5 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2202.03195](http://arxiv.org/abs/2202.03195)

    本文研究了联邦图神经网络中的后门攻击，填补了相关领域的研究空白，并发现在该场景下分布式后门攻击的成功率更高。

    

    图神经网络（GNNs）是一类基于深度学习的图领域信息处理方法。由于其优越的学习复杂图数据表示的能力，GNNs最近已成为广泛使用的图分析方法。然而，由于隐私问题和监管限制，中心化的GNNs在处理与隐私有关的数据场景时可能很困难。联邦学习（FL）是一种新兴技术，用于在多个参与方需要协作训练共享全局模型的隐私保护设置中。尽管有一些研究将FL应用于训练GNNs（联邦GNNs），但没有研究它们对后门攻击的鲁棒性。本文通过在联邦GNNs中进行两种类型的后门攻击，即集中式后门攻击（CBA）和分布式后门攻击（DBA），填补了这一空白。实验结果表明，在几乎所有评估的情况下，DBA攻击成功率比CBA更高。

    Graph Neural Networks (GNNs) are a class of deep learning-based methods for processing graph domain information. GNNs have recently become a widely used graph analysis method due to their superior ability to learn representations for complex graph data. However, due to privacy concerns and regulation restrictions, centralized GNNs can be difficult to apply to data-sensitive scenarios. Federated learning (FL) is an emerging technology developed for privacy-preserving settings when several parties need to train a shared global model collaboratively. Although several research works have applied FL to train GNNs (Federated GNNs), there is no research on their robustness to backdoor attacks.  This paper bridges this gap by conducting two types of backdoor attacks in Federated GNNs: centralized backdoor attacks (CBA) and distributed backdoor attacks (DBA). Our experiments show that the DBA attack success rate is higher than CBA in almost all evaluated cases. For CBA, the attack success rate 
    
[^149]: 去噪监督。 (arXiv:2202.02952v2 [eess.IV] UPDATED)

    Supervision by Denoising. (arXiv:2202.02952v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.02952](http://arxiv.org/abs/2202.02952)

    本文提出了一种去噪监督（SUD）框架，可以监督重建模型，并避免制作特定于成像领域的可微正则化器。

    

    基于学习的图像重建模型，例如基于 U-Net 的模型，如果要保证良好的泛化性能，则需要大量标记的图像数据。然而，在某些成像领域中，由于获取这些数据的成本，具有像素或体素级别精度的标记数据很少。在医学成像等领域，由于不存在单一的标准标签，标签之间存在大量的重复变异性，因此训练重建网络从带标签和无标签的示例中学习以更好地泛化（称为半监督学习），是一个实践和理论上的问题。 然而，针对图像重建的传统半监督学习方法通常需要手工制作针对某些给定成像问题的可微正则化器，这可能非常耗时。 在这项工作中，我们提出了“去噪监督”（SUD）框架，该框架使我们能够监督重建模型，同时避免手动制作特定于成像领域的可微正则化器。

    Learning-based image reconstruction models, such as those based on the U-Net, require a large set of labeled images if good generalization is to be guaranteed. In some imaging domains, however, labeled data with pixel- or voxel-level label accuracy are scarce due to the cost of acquiring them. This problem is exacerbated further in domains like medical imaging, where there is no single ground truth label, resulting in large amounts of repeat variability in the labels. Therefore, training reconstruction networks to generalize better by learning from both labeled and unlabeled examples (called semi-supervised learning) is problem of practical and theoretical interest. However, traditional semi-supervised learning methods for image reconstruction often necessitate handcrafting a differentiable regularizer specific to some given imaging problem, which can be extremely time-consuming. In this work, we propose "supervision by denoising" (SUD), a framework that enables us to supervise reconst
    
[^150]: 剪切Gossip在拜占庭鲁棒的分散式学习中的应用

    Byzantine-Robust Decentralized Learning via ClippedGossip. (arXiv:2202.01545v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01545](http://arxiv.org/abs/2202.01545)

    本文提出的剪切Gossip算法是第一个能够在标准假设下证明收敛到非凸目标的$O(\delta_{max} \zeta^2 /\gamma^2)$邻域的算法，并在大量攻击下表现良好。

    

    本文研究了在任意通信图上进行拜占庭鲁棒的分散式训练的艰巨任务。与联邦学习通过服务器进行通信的方式不同，分散式环境中的workers只能与它们的邻居交流，这使得达成共识和协作训练更加困难。为了解决这些问题，我们提出了一种用于拜占庭鲁棒共识和优化的剪切Gossip算法，它是第一个在标准假设下可以证明收敛到非凸目标的$O(\delta_{max} \zeta^2 /\gamma^2)$邻域的算法。最后，我们在大量攻击下证明了剪切Gossip的鼓舞人心的实证表现。

    In this paper, we study the challenging task of Byzantine-robust decentralized training on arbitrary communication graphs. Unlike federated learning where workers communicate through a server, workers in the decentralized environment can only talk to their neighbors, making it harder to reach consensus and benefit from collaborative training. To address these issues, we propose a ClippedGossip algorithm for Byzantine-robust consensus and optimization, which is the first to provably converge to a $O(\delta_{\max}\zeta^2/\gamma^2)$ neighborhood of the stationary point for non-convex objectives under standard assumptions. Finally, we demonstrate the encouraging empirical performance of ClippedGossip under a large number of attacks.
    
[^151]: DDPG驱动的自适应深度深度展开与稀疏贝叶斯学习的信道估计

    DDPG-Driven Deep-Unfolding with Adaptive Depth for Channel Estimation with Sparse Bayesian Learning. (arXiv:2201.08477v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2201.08477](http://arxiv.org/abs/2201.08477)

    本文提出了一种DDPG驱动的自适应深度深度展开框架，适用于不同的输入。该框架被用于解决大规模多输入多输出系统中的信道估计问题。

    

    深度展开神经网络（NN）由于达到了相对低复杂度下满意的性能而受到了广泛关注。通常，这些深度展开NN对于所有输入都被限制在固定深度上。然而，收敛所需的最佳层数随着不同输入而变化。本文首先提出了一种DDPG驱动的适应深度深度展开框架，适用于不同的输入，其中深度展开NN的可训练参数是由DDPG学习的，而不是直接由随机梯度下降算法更新。具体来说，深度展开NN的优化变量、可训练参数和体系结构被设计为DDPG的状态、动作和状态转移，分别。然后，将该框架用于处理大规模多输入多输出系统中的信道估计问题。具体而言，我们首先制定了信道估计问题的表述。

    Deep-unfolding neural networks (NNs) have received great attention since they achieve satisfactory performance with relatively low complexity. Typically, these deep-unfolding NNs are restricted to a fixed-depth for all inputs. However, the optimal number of layers required for convergence changes with different inputs. In this paper, we first develop a framework of deep deterministic policy gradient (DDPG)-driven deep-unfolding with adaptive depth for different inputs, where the trainable parameters of deep-unfolding NN are learned by DDPG, rather than updated by the stochastic gradient descent algorithm directly. Specifically, the optimization variables, trainable parameters, and architecture of deep-unfolding NN are designed as the state, action, and state transition of DDPG, respectively. Then, this framework is employed to deal with the channel estimation problem in massive multiple-input multiple-output systems. Specifically, first of all we formulate the channel estimation proble
    
[^152]: 路用户检测的概率方法

    Probabilistic Approach for Road-Users Detection. (arXiv:2112.01360v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.01360](http://arxiv.org/abs/2112.01360)

    本文介绍了一种用于缓解深度目标检测模型中过于自信预测问题的方法，通过引入一种新颖的概率层来避免传统的预测层，实验证明该方法能够减少假阳性中的过度自信。

    

    自动驾驶应用中的目标检测意味着对语义对象的检测和跟踪通常是城市驾驶环境的特色，如行人和车辆。目前最先进的基于深度学习的目标检测中存在假阳性问题，这些问题通常带有过于自信的得分。在自动驾驶和其他关键的机器人感知领域，这是非常不希望看到的，因为涉及安全问题。本文提出了一种方法来缓解过于自信的预测问题，通过在测试中引入一种新颖的概率层，向深度目标检测网络中添加这种概率层。建议的方法避免了传统的Sigmoid或Softmax预测层，这些层通常会产生过于自信的预测。实验证明，所提出的技术能够减少假阳性中的过度自信，而不会降低真阳性的性能。该方法在2D-KITTI目标检测中进行了验证，使用了YOLOV4和S。

    Object detection in autonomous driving applications implies that the detection and tracking of semantic objects are commonly native to urban driving environments, as pedestrians and vehicles. One of the major challenges in state-of-the-art deep-learning based object detection are false positives which occur with overconfident scores. This is highly undesirable in autonomous driving and other critical robotic-perception domains because of safety concerns. This paper proposes an approach to alleviate the problem of overconfident predictions by introducing a novel probabilistic layer to deep object detection networks in testing. The suggested approach avoids the traditional Sigmoid or Softmax prediction layer which often produces overconfident predictions. It is demonstrated that the proposed technique reduces overconfidence in the false positives without degrading the performance on the true positives. The approach is validated on the 2D-KITTI objection detection through the YOLOV4 and S
    
[^153]: 用基于Transformer的模型预测中等市值公司违约概率

    A transformer-based model for default prediction in mid-cap corporate markets. (arXiv:2111.09902v4 [q-fin.GN] UPDATED)

    [http://arxiv.org/abs/2111.09902](http://arxiv.org/abs/2111.09902)

    本文利用Transformer模型，将违约预测问题作为多标签时间序列分类问题，并通过注意力热图解释模型预测。同时，提出了一种自定义损失函数和一个新颖的多通道架构，以优化模型。

    

    本文研究了市值低于100亿美元的上市公司，利用30年的美国中等市值公司数据集，预测其中期内的违约概率，并确定贡献违约风险的数据来源（基本面、市场或定价数据）。我们将该问题作为多标签时间序列分类问题，并使用Transformer模型进行预测和注意力热图的解释。为了进一步优化模型，我们提出了一种用于多标签分类的自定义损失函数和一个新颖的多通道架构。

    In this paper, we study mid-cap companies, i.e. publicly traded companies with less than US $10 billion in market capitalisation. Using a large dataset of US mid-cap companies observed over 30 years, we look to predict the default probability term structure over the medium term and understand which data sources (i.e. fundamental, market or pricing data) contribute most to the default risk. Whereas existing methods typically require that data from different time periods are first aggregated and turned into cross-sectional features, we frame the problem as a multi-label time-series classification problem. We adapt transformer models, a state-of-the-art deep learning model emanating from the natural language processing domain, to the credit risk modelling setting. We also interpret the predictions of these models using attention heat maps. To optimise the model further, we present a custom loss function for multi-label classification and a novel multi-channel architecture with differentia
    
[^154]: 有向拟阵的复合体的标记样本压缩方案

    Labeled sample compression schemes for complexes of oriented matroids. (arXiv:2110.15168v3 [math.CO] UPDATED)

    [http://arxiv.org/abs/2110.15168](http://arxiv.org/abs/2110.15168)

    本研究提出了适用于VC维为$d$的有向拟阵的复合体的拓扑的标记样本压缩方案，为计算学习理论中的样本压缩猜想迈出了一步。

    

    我们展示了一个大小为$d$的适当的标记样本压缩方案，用于VC维为$d$的有向拟阵的复合体（简称COM）的拓扑。这极大地扩展了关于充分类（ample classes）的Moran和Warmuth的结果、关于超平面仿射排列的Ben-David和Litman的结果以及关于均匀有向拟阵的复合体的作者的结果，并为样本压缩猜想迈出了一步，这是计算学习理论中最古老的开放性问题之一。一方面，我们的方法通过有向拟阵理论利用了COM的丰富的组合细胞结构。另一方面，将COM的顶点图形视为部分立方体创建了与度量图形理论的有益联系。

    We show that the topes of a complex of oriented matroids (abbreviated COM) of VC-dimension $d$ admit a proper labeled sample compression scheme of size $d$. This considerably extends results of Moran and Warmuth on ample classes, of Ben-David and Litman on affine arrangements of hyperplanes, and of the authors on complexes of uniform oriented matroids, and is a step towards the sample compression conjecture -- one of the oldest open problems in computational learning theory. On the one hand, our approach exploits the rich combinatorial cell structure of COMs via oriented matroid theory. On the other hand, viewing tope graphs of COMs as partial cubes creates a fruitful link to metric graph theory.
    
[^155]: 边界图神经网络用于 3D 模拟

    Boundary Graph Neural Networks for 3D Simulations. (arXiv:2106.11299v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11299](http://arxiv.org/abs/2106.11299)

    本文提出了一种新型边界图神经网络（BGNNs），能够有效地表示三维颗粒流动的复杂几何形状，从而实现了高效的预测和计算。

    

    大量数据的出现使得机器学习在自然科学和工程学方面具有了可观的动力，然而对物理过程进行建模通常很困难。其中一个特别棘手的问题是如何有效地表示几何边界。三角化的几何边界在工程应用中得到了广泛理解和使用。然而，基于它们的尺寸和方向的异质性，将它们集成到机器学习方法中通常十分困难。在本文中，我们引入了一种有效的理论来建模粒子与边界的相互作用，从而推导出了我们的新型边界图神经网络（BGNNs），该网络动态地修改图结构以满足边界条件。新的 BGNNs 在复杂的三维颗粒流动过程（如漏斗、旋转鼓和搅拌器）中进行了测试，这些过程都是现代工业机械的标准组件，但其几何形状仍然十分复杂。BGNNs 在计算效率和预测准确率方面都得到了评估。

    The abundance of data has given machine learning considerable momentum in natural sciences and engineering, though modeling of physical processes is often difficult. A particularly tough problem is the efficient representation of geometric boundaries. Triangularized geometric boundaries are well understood and ubiquitous in engineering applications. However, it is notoriously difficult to integrate them into machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce an effective theory to model particle-boundary interactions, which leads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modify graph structures to obey boundary conditions. The new BGNNs are tested on complex 3D granular flow processes of hoppers, rotating drums and mixers, which are all standard components of modern industrial machinery but still have complicated geometry. BGNNs are evaluated in terms of computational efficiency as well as pre
    
[^156]: 单个神经元能否学习预测不确定性？

    Can a single neuron learn predictive uncertainty?. (arXiv:2106.03702v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.03702](http://arxiv.org/abs/2106.03702)

    本文介绍了一种基于单个神经元的新型非参数分位数估计方法，该方法在小样本大小和真实世界实验中的优越表现表明其可以消除与模型规范相关的自由度，并提供更好的预测不确定性估计。

    

    采用深度学习方法进行的不确定性估计旨在分离我们通过测量所观察到的世界状态的不确定性（客观终点）与模型规范和训练过程用于预测这种状态的方式相混淆的程度（主观手段）--例如神经元的数量，深度，连接，先验分布（如果模型是贝叶斯的），权重初始化等。这引出了一个问题，即在仍能捕获客观终点的前提下，能否消除与这些规范相关的自由度。本文介绍了一种基于最简单的神经网络结构—单个神经元—的连续随机变量的新型非参数分位数估计方法。首先，在合成实验中，该方法展现了它的优势，它将通过排序顺序统计量得到的分位数估计结果（特别是对于小样本大小）与分位回归进行了比较。在真实世界的实验中，该方法被证明可以通过降低超出分布范围的数据中预测方差的低估来为贝叶斯神经网络提供更好的预测不确定性估计。

    Uncertainty estimation methods using deep learning approaches strive against separating how uncertain the state of the world manifests to us via measurement (objective end) from the way this gets scrambled with the model specification and training procedure used to predict such state (subjective means) -- e.g., number of neurons, depth, connections, priors (if the model is bayesian), weight initialization, etc. This poses the question of the extent to which one can eliminate the degrees of freedom associated with these specifications and still being able to capture the objective end. Here, a novel non-parametric quantile estimation method for continuous random variables is introduced, based on the simplest neural network architecture with one degree of freedom: a single neuron. Its advantage is first shown in synthetic experiments comparing with the quantile estimation achieved from ranking the order statistics (specifically for small sample size) and with quantile regression. In real-
    
[^157]: 未知干扰实验中的政策设计

    Policy design in experiments with unknown interference. (arXiv:2011.08174v7 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2011.08174](http://arxiv.org/abs/2011.08174)

    本文研究了如何在有限数量的大簇中进行实验设计，估计和推断最大福利政策，并提出单波实验和多波实验的方法来解决溢出效应问题。

    

    本文研究了在溢出效应存在的情况下估计和推断最大福利政策的实验设计。将单元组织成有限数量的大簇，并在每个簇内以未知的方式相互作用。作为第一项贡献，本文提出了一种单波实验，通过在簇对间仔细变化随机化，考虑溢出效应估计治疗概率变化的边际效应。利用这个边际效应，文章提出了一个检验政策最优性的测试。作为第二项贡献，本文设计了一个多波实验，估计治疗规则并最大化福利。本文对最大可达福利于所估计政策评估下福利之间的差异给出了强有力的小样本保证。作者在根据现有关于信息传播和现金转移计划的实验模拟和大规模现场实验中提供了这种方法的特性。

    This paper studies experimental designs for estimation and inference on welfare-maximizing policies in the presence of spillover effects. Units are organized into a finite number of large clusters and interact in unknown ways within each cluster. As a first contribution, I introduce a single-wave experiment that, by carefully varying the randomization across cluster pairs, estimates the marginal effect of a change in treatment probabilities, taking spillover effects into account. Using the marginal effect, I propose a test for policy optimality. As a second contribution, I design a multiple-wave experiment to estimate treatment rules and maximize welfare. I derive strong small-sample guarantees on the difference between the maximum attainable welfare and the welfare evaluated at the estimated policy. I illustrate the method's properties in simulations calibrated to existing experiments on information diffusion and cash-transfer programs, and in a large scale field experiment implemente
    
[^158]: 变分自编码器的ELBO收敛于三个熵之和。

    The ELBO of Variational Autoencoders Converges to a Sum of Three Entropies. (arXiv:2010.14860v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2010.14860](http://arxiv.org/abs/2010.14860)

    标准变分自编码器的ELBO在稳定点处可以以闭合形式计算，收敛于三个熵之和。（其中一个熵为先验分布的熵，一个为可观测分布的熵，一个为变分分布的平均熵，成果证明了ELBO在稳定点处等于熵。）

    

    变分自编码器(VAEs)的中心目标函数是其变分下界(ELBO)。我们展示了对于标准(即高斯)VAEs，ELBO收敛于由三个熵之和给出的值：(先验分布的负)熵、可观测分布的预期(负)熵以及变分分布的平均熵(后者已经是ELBO的一部分)。我们的推导结果精确，适用于编码器和解码器的小型和复杂深度网络，并适用于有限和无限数量的数据点以及任何稳定点(包括局部最大值和鞍点)。该结果意味着对于标准VAEs，ELBO在稳定点时通常可以以闭合形式计算，而原始ELBO需要数值积分近似。作为主要贡献，我们提供了VAEs的ELBO在稳定点处等于熵的证明。

    The central objective function of a variational autoencoder (VAE) is its variational lower bound (the ELBO). Here we show that for standard (i.e., Gaussian) VAEs the ELBO converges to a value given by the sum of three entropies: the (negative) entropy of the prior distribution, the expected (negative) entropy of the observable distribution, and the average entropy of the variational distributions (the latter is already part of the ELBO). Our derived analytical results are exact and apply for small as well as for intricate deep networks for encoder and decoder. Furthermore, they apply for finitely and infinitely many data points and at any stationary point (including local maxima and saddle points). The result implies that the ELBO can for standard VAEs often be computed in closed-form at stationary points while the original ELBO requires numerical approximations of integrals. As a main contribution, we provide the proof that the ELBO for VAEs is at stationary points equal to entropy su
    
[^159]: 基于投影梯度下降的对抗训练中梯度方向的量化研究

    Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2009.04709](http://arxiv.org/abs/2009.04709)

    本文提出了一种新的定义来描述对抗训练中梯度的优选方向，利用生成对抗网络的指标来评估对齐情况，并表明在对齐方向上的限制可以进一步提高模型的鲁棒性。

    

    对抗训练，尤其是投影梯度下降（PGD），已被证明是提高对抗性攻击鲁棒性的有效方法。在对抗训练后，模型对其输入的梯度具有优选方向。然而，对齐方向并没有得到数学上的很好描述，这使得其难以进行定量评估。我们提出了一种新的定义，将其视为指向决策空间中最近错误类支持集的最近点的向量方向。为了评估对抗训练后模型与此方向的对齐情况，我们应用了一种指标，该指标使用生成对抗网络来产生最小残差，以改变图像中的类别。我们表明，PGD训练的模型相比基线具有更高的对齐度，而我们的指标呈现比竞争指标公式更高的对齐度值，并且在训练过程中强制执行这个对齐方向可以进一步提高模型的鲁棒性。

    Adversarial training, especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction. However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this 
    
[^160]: FRMDN: 基于流的循环混合密度网络

    FRMDN: Flow-based Recurrent Mixture Density Network. (arXiv:2008.02144v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.02144](http://arxiv.org/abs/2008.02144)

    本文提出了一种基于流的循环混合密度网络，通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上，该模型在图像序列的拟合度上表现显著，具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。

    This paper proposes a flow-based recurrent mixture density network (FRMDN) that generalizes recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The model significantly improves the fit to image sequences and outperforms other state-of-the-art methods in terms of the log-likelihood.

    循环混合密度网络是一类重要的概率模型，广泛应用于序列建模和序列到序列映射应用中。在这类模型中，目标序列在每个时间步的密度由具有循环神经网络参数的高斯混合模型建模。本文通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上。非线性变换空间是通过归一化流创建的。我们观察到，该模型显著提高了图像序列的拟合度，用对数似然度量。我们还将所提出的模型应用于一些语音和图像数据，并观察到该模型具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。

    The class of recurrent mixture density networks is an important class of probabilistic models used extensively in sequence modeling and sequence-to-sequence mapping applications. In this class of models, the density of a target sequence in each time-step is modeled by a Gaussian mixture model with the parameters given by a recurrent neural network. In this paper, we generalize recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The non-linearly transformed space is created by normalizing flow. We observed that this model significantly improves the fit to image sequences measured by the log-likelihood. We also applied the proposed model on some speech and image data, and observed that the model has significant modeling power outperforming other state-of-the-art methods in terms of the log-likelihood.
    

